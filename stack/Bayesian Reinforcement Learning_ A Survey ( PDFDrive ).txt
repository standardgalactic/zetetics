Foundations and Trends R
⃝in Machine Learning
Vol. 8, No. 5-6 (2015) 359–483
c⃝2015 M. Ghavamzadeh, S. Mannor, J. Pineau, and
A. Tamar
DOI: 10.1561/2200000049
Bayesian Reinforcement Learning: A Survey
Mohammad Ghavamzadeh
Adobe Research & INRIA
mohammad.ghavamzadeh@inria.fr
Shie Mannor
Technion
shie@ee.technion.ac.il
Joelle Pineau
McGill University
jpineau@cs.mcgill.ca
Aviv Tamar
University of California, Berkeley
avivt@berkeley.edu

Contents
1
Introduction
360
2
Technical Background
367
2.1
Multi-Armed Bandits
. . . . . . . . . . . . . . . . . . . . 367
2.2
Markov Decision Processes . . . . . . . . . . . . . . . . . 370
2.3
Partially Observable Markov Decision Processes . . . . . . 374
2.4
Reinforcement Learning . . . . . . . . . . . . . . . . . . . 376
2.5
Bayesian Learning . . . . . . . . . . . . . . . . . . . . . . 378
3
Bayesian Bandits
385
3.1
Classical Results . . . . . . . . . . . . . . . . . . . . . . . 386
3.2
Bayes-UCB . . . . . . . . . . . . . . . . . . . . . . . . . . 389
3.3
Thompson Sampling . . . . . . . . . . . . . . . . . . . . . 389
4
Model-based Bayesian Reinforcement Learning
396
4.1
Models and Representations . . . . . . . . . . . . . . . . . 396
4.2
Exploration/Exploitation Dilemma
. . . . . . . . . . . . . 400
4.3
Oﬄine Value Approximation
. . . . . . . . . . . . . . . . 401
4.4
Online near-myopic value approximation . . . . . . . . . . 403
4.5
Online Tree Search Approximation . . . . . . . . . . . . . 405
4.6
Methods with Exploration Bonus to Achieve PAC Guarantees411
4.7
Extensions to Unknown Rewards . . . . . . . . . . . . . . 418
ii

iii
4.8
Extensions to Continuous MDPs . . . . . . . . . . . . . . 421
4.9
Extensions to Partially Observable MDPs
. . . . . . . . . 422
4.10 Extensions to Other Priors and Structured MDPs . . . . . 425
5
Model-free Bayesian Reinforcement Learning
427
5.1
Value Function Algorithms
. . . . . . . . . . . . . . . . . 427
5.2
Bayesian Policy Gradient
. . . . . . . . . . . . . . . . . . 436
5.3
Bayesian Actor-Critic
. . . . . . . . . . . . . . . . . . . . 444
6
Risk-aware Bayesian Reinforcement Learning
448
7
BRL Extensions
455
7.1
PAC-Bayes Model Selection . . . . . . . . . . . . . . . . . 455
7.2
Bayesian Inverse Reinforcement Learning . . . . . . . . . . 456
7.3
Bayesian Multi-agent Reinforcement Learning . . . . . . . 458
7.4
Bayesian Multi-Task Reinforcement Learning . . . . . . . . 458
8
Outlook
461
Acknowledgements
464
Appendices
465
A Index of Symbols
466
B Discussion on GPTD Assumptions on the Noise Process
469
References
471

Abstract
Bayesian methods for machine learning have been widely investigated,
yielding principled methods for incorporating prior information into
inference algorithms. In this survey, we provide an in-depth review
of the role of Bayesian methods for the reinforcement learning (RL)
paradigm. The major incentives for incorporating Bayesian reasoning
in RL are: 1) it provides an elegant approach to action-selection (explo-
ration/exploitation) as a function of the uncertainty in learning; and
2) it provides a machinery to incorporate prior knowledge into the al-
gorithms. We ﬁrst discuss models and methods for Bayesian inference
in the simple single-step Bandit model. We then review the extensive
recent literature on Bayesian methods for model-based RL, where prior
information can be expressed on the parameters of the Markov model.
We also present Bayesian methods for model-free RL, where priors are
expressed over the value function or policy class. The objective of the
paper is to provide a comprehensive survey on Bayesian RL algorithms
and their theoretical and empirical properties.
M. Ghavamzadeh, S. Mannor, J. Pineau, and A. Tamar. Bayesian Reinforcement
Learning: A Survey. Foundations and Trends
R
⃝in Machine Learning, vol. 8,
no. 5-6, pp. 359–483, 2015.
DOI: 10.1561/2200000049.

1
Introduction
A large number of problems in science and engineering, from robotics to
game playing, tutoring systems, resource management, ﬁnancial port-
folio management, medical treatment design and beyond, can be char-
acterized as sequential decision-making under uncertainty. Many inter-
esting sequential decision-making tasks can be formulated as reinforce-
ment learning (RL) problems [Bertsekas and Tsitsiklis, 1996, Sutton
and Barto, 1998]. In an RL problem, an agent interacts with a dy-
namic, stochastic, and incompletely known environment, with the goal
of ﬁnding an action-selection strategy, or policy, that optimizes some
long-term performance measure.
One of the key features of RL is the focus on learning a con-
trol policy to optimize the choice of actions over several time steps.
This is usually learned from sequences of data. In contrast to su-
pervised learning methods that deal with independently and iden-
tically distributed (i.i.d.) samples from the domain, the RL agent
learns from the samples that are collected from the trajectories gen-
erated by its sequential interaction with the system. Another impor-
tant aspect is the eﬀect of the agent’s policy on the data collec-
tion; diﬀerent policies naturally yield diﬀerent distributions of sam-
360

361
pled trajectories, and thus, impacting what can be learned from the
data.
Traditionally, RL algorithms have been categorized as being ei-
ther model-based or model-free. In the former category, the agent uses
the collected data to ﬁrst build a model of the domain’s dynamics
and then uses this model to optimize its policy. In the latter case,
the agent directly learns an optimal (or good) action-selection strat-
egy from the collected data. There is some evidence that the ﬁrst
method provides better results with less data [Atkeson and Santa-
maria, 1997], and the second method may be more eﬃcient in cases
where the solution space (e.g., policy space) exhibits more regular-
ity than the underlying dynamics, though there is some disagreement
about this,.
A major challenge in RL is in identifying good data collection strate-
gies, that eﬀectively balance between the need to explore the space of
all possible policies, and the desire to focus data collection towards tra-
jectories that yield better outcome (e.g., greater chance of reaching a
goal, or minimizing a cost function). This is known as the exploration-
exploitation tradeoﬀ. This challenge arises in both model-based and
model-free RL algorithms.
Bayesian reinforcement learning (BRL) is an approach to RL that
leverages methods from Bayesian inference to incorporate information
into the learning process. It assumes that the designer of the system can
express prior information about the problem in a probabilistic distri-
bution, and that new information can be incorporated using standard
rules of Bayesian inference. The information can be encoded and up-
dated using a parametric representation of the system dynamics, in
the case of model-based RL, or of the solution space, in the case of
model-free RL.
A major advantage of the BRL approach is that it provides a prin-
cipled way to tackle the exploration-exploitation problem. Indeed, the
Bayesian posterior naturally captures the full state of knowledge, sub-
ject to the chosen parametric representation, and thus, the agent can
select actions that maximize the expected gain with respect to this
information state.

362
Introduction
Another major advantage of BRL is that it implicitly facilitates reg-
ularization. By assuming a prior on the value function, the parameters
deﬁning a policy, or the model parameters, we avoid the trap of letting
a few data points steer us away from the true parameters. On the other
hand, having a prior precludes overly rapid convergence. The role of
the prior is therefore to soften the eﬀect of sampling a ﬁnite dataset,
eﬀectively leading to regularization. We note that regularization in RL
has been addressed for the value function [Farahmand et al., 2008b]
and for policies [Farahmand et al., 2008a]. A major issue with these
regularization schemes is that it is not clear how to select the regu-
larization coeﬃcient. Moreover, it is not clear why an optimal value
function (or a policy) should belong to some pre-deﬁned set.
Yet another advantage of adopting a Bayesian view in RL is the
principled Bayesian approach for handling parameter uncertainty. Cur-
rent frequentist approaches for dealing with modelling errors in sequen-
tial decision making are either very conservative, or computationally
infeasible [Nilim and El Ghaoui, 2005]. By explicitly modelling the dis-
tribution over unknown system parameters, Bayesian methods oﬀer a
promising approach for solving this diﬃcult problem.
Of course, several challenges arise in applying Bayesian methods to
the RL paradigm. First, there is the challenge of selecting the correct
representation for expressing prior information in any given domain.
Second, deﬁning the decision-making process over the information state
is typically computationally more demanding than directly considering
the natural state representation. Nonetheless, a large array of models
and algorithms have been proposed for the BRL framework, leverag-
ing a variety of structural assumptions and approximations to provide
feasible solutions.
The main objective of this paper is to provide a comprehensive
survey on BRL algorithms and their theoretical and empirical proper-
ties. In Chapter 2, we provide a review of the main mathematical con-
cepts and techniques used throughout this paper. Chapter 3 surveys the
Bayesian learning methods for the case of single-step decision-making,
using the bandit framework. This section serves both as an exposition of
the potential of BRL in a simpler setting that is well understood, but is

363
also of independent interest, as bandits have widespread applications.
The main results presented here are of a theoretical nature, outlin-
ing known performance bounds for the regret minimization criteria.
Chapter 4 reviews existing methods for model-based BRL, where the
posterior is expressed over parameters of the system dynamics model.
Chapter 5 focuses on BRL methods that do not explicitly learn a model
of the system, but rather the posterior is expressed over the solution
space. Chapter 6 focuses on a particular advantage of BRL in dealing
with risk due to parameter-uncertainty, and surveys several approaches
for incorporating such risk into the decision-making process. Finally,
Chapter 7 discusses various extensions of BRL for special classes of
problems (PAC-Bayes model selection, inverse RL, multi-agent RL, and
multi-task RL). Figure 1.1 outlines the various BRL approaches cov-
ered throughout the paper.
An Example Domain
We present an illustrative domain suitable to be solved using the BRL
techniques surveyed in this paper. This running example will be used
throughout the paper to elucidate the diﬀerence between the various
BRL approaches and to clarify various BRL concepts.
Example 1.1 (The Online Shop). In the online shop domain, a retailer
aims to maximize proﬁt by sequentially suggesting products to online
shopping customers. Formally, the domain is characterized by the fol-
lowing model:
• A set of possible customer states, X. States can represent intrinsic
features of the customer such as gender and age, but also dynamic
quantities such as the items in his shopping cart, or his willingness
to shop;
• A set of possible product suggestions and advertisements, A;
• A probability kernel, P, deﬁned below.
An episode in the online shop domain begins at time t = 0, when
a customer with features x0 ∈X enters the online shop. Then, a se-
quential interaction between the customer and the online shop begins,

364
Introduction
-­‐ 
Finite	  state	  controllers	  
-­‐ 
BEETLE	  
	  
-­‐ 
Bayesian	  DP	  
-­‐ 
VOI	  heuris9c	  
	  
-­‐ 
Forward	  search	  
-­‐ 
Bayesian	  sparse	  sampling	  
-­‐ 
HMDP	  
-­‐ 
BFS3	  
-­‐ 
Branch-­‐and-­‐bound	  search	  
-­‐ 
BAMCP	  
	  
-­‐ 
BOSS	  
-­‐ 
BEB	  
-­‐ 
VBRB	  
-­‐ 
BOLT	  
	  
-­‐ 
GPTD	  
-­‐ 
GPSARSA	  
-­‐ 
Bayesian	  Quadrature	  
-­‐ 
Two	  Bayesian	  models	  for	  	  
es9ma9ng	  the	  policy	  gradient	  
	  
-­‐ 
GPTD	  +	  Bayesian	  policy	  gradient	  
Bandits	  	  
(Sec	  3)	  
Model-­‐based	  
BRL	  (Sec	  4)	  
Model-­‐free	  
BRL	  (Sec	  5)	  
	  
Bayes	  UCB	  
Thompson	  sampling	  
	  
	  
	  
Oﬄine	  value	  approximaGon	  
	  
	  
Online	  near-­‐myopic	  value	  approximaGon	  
	  
	  
Online	  tree	  search	  approximaGon	  
	  
	  
	  
	  
	  
	  
ExploraGon	  bonus	  approximaGon	  
	  
	  
	  
	  
Value	  funcGon	  algos	  
	  
	  
Policy	  gradient	  algos	  
	  
	  
	  
Actor-­‐CriGc	  algos	  	  
	  
Bayesian	  
RL	  
Risk	  Aware	  	  
BRL	  (Sec	  6)	  
Bias	  variance	  approximaGon	  
PercenGle	  criterion	  
Min-­‐max	  criterion	  
PercenGle	  measures	  criteria	  
Figure 1.1: Overview of the Bayesian RL approaches covered in this survey.

365
where at each step t = 0, 1, 2, . . . , an advertisement at ∈A is shown
to the customer, and following that the customer makes a decision to
either (i) add a product to his shopping cart; (ii) not buy the product,
but continue to shop; (iii) stop shopping and check out. Following the
customers decision, his state changes to xt+1 (reﬂecting the change in
the shopping cart, willingness to continue shopping, etc.). We assume
that this change is captured by a probability kernel P(xt+1| xt, at).
When the customer decides to check out, the episode ends, and a
proﬁt is obtained according to the items he had added to his cart. The
goal is to ﬁnd a product suggestion policy, x →a ∈A, that maximizes
the expected total proﬁt.
When the probabilities of customer responses P are known in ad-
vance, calculating an optimal policy for the online shop domain is basi-
cally a planning problem, which may be solved using traditional meth-
ods for resource allocation [Powell, 2011]. A more challenging, but real-
istic, scenario is when P is not completely known beforehand, but has
to be learned while interacting with customers. The BRL framework
employs Bayesian methods for learning P, and for learning an optimal
product suggestion policy.
There are several advantages for choosing a Bayesian approach for
the online shop domain. First, it is likely that some prior knowledge
about P is available. For example, once a customer adds a product of
a particular brand to his cart, it is likely that he prefers additional
products of the same brand over those of a diﬀerent one. Taking into
account such knowledge is natural in the Bayesian method, by virtue
of the prior distribution over P. As we shall see, the Bayesian ap-
proach also naturally extends to more general forms of structure in the
problem.
A second advantage concerns what is known as the exploitation–
exploration dilemma: should the decision-maker display only the most
proﬁtable product suggestions according to his current knowledge
about P, or rather take exploratory actions that may turn out to
be less proﬁtable, but provide useful information for future deci-
sions? The Bayesian method oﬀers a principled approach to deal-
ing with this diﬃcult problem by explicitly quantifying the value

366
Introduction
of exploration, made possible by maintaining a distribution over
P.
The various parameter conﬁgurations in the online shop domain
lead to the diﬀerent learning problems surveyed in this paper. In par-
ticular:
• For a single-step interaction, i.e., when the episode terminates
after a single product suggestion, the problem is captured by the
multi-armed bandit model of Chapter 3.
• For small-scale problems, i.e., a small number of products and
customer types, P may be learnt explicitly. This is the model-
based approach of Chapter 4.
• For large problems, a near-optimal policy may be obtained with-
out representing P explicitly. This is the model-free approach of
Chapter 5.
• When the customer state is not fully observed by the decision-
maker, we require models that incorporate partial observability;
see §2.3 and §4.9.
Throughout the paper, we revisit the online shop domain, and spec-
ify explicit conﬁgurations that are relevant to the surveyed methods.

2
Technical Background
In this section we provide a brief overview of the main concepts and
introduce the notations used throughout the paper. We begin with a
quick review of the primary mathematical tools and algorithms lever-
aged in the latter sections to deﬁne BRL models and algorithms.
2.1
Multi-Armed Bandits
As was previously mentioned, a key challenge in sequential decision-
making under uncertainty is the exploration/exploitation dilemma: the
tradeoﬀbetween either taking actions that are most rewarding accord-
ing to the current state of knowledge, or taking exploratory actions,
which may be less immediately rewarding, but may lead to better in-
formed decisions in the future.
The stochastic multi-armed bandit (MAB) problem is perhaps the
simplest model of the exploration/exploitation tradeoﬀ. Originally for-
mulated as the problem of a gambler choosing between diﬀerent slot
machines in a casino (‘one armed bandits’), the stochastic MAB model
features a decision-maker that sequentially chooses actions at ∈A,
and observes random outcomes Yt(at) ∈Y at discrete time steps
367

368
Technical Background
t = 1, 2, 3, . . .. A known function r : Y →R associates the random
outcome to a reward, which the agent seeks to maximize. The out-
comes {Yt(a)} are drawn i.i.d. over time from an unknown probability
distribution P(·|a) ∈P(Y), where P(Y) denotes the set of probability
distributions on (Borel) subsets of Y (see Bubeck and Cesa-Bianchi
[2012] for reference).
Model 1 (Stochastic K-Armed Bandit) Deﬁne a K-MAB to be
a tuple ⟨A, Y, P, r⟩where
• A is the set of actions (arms), and |A| = K,
• Y is the set of possible outcomes,
• P(·|a) ∈P(Y) is the outcome probability, conditioned on action
a ∈A being taken,
• r(Y ) ∈R represents the reward obtained when outcome Y ∈Y is
observed.
A rule that prescribes to the agent which actions to select, or policy, is
deﬁned as a mapping from past observations to a distribution over the
set of actions. Since the probability distributions are initially unknown,
the decision-maker faces a tradeoﬀbetween exploitation, i.e., selecting
the arm she believes is optimal, or making exploratory actions to gather
more information about the true probabilities. Formally, this tradeoﬀ
is captured by the notion of regret:
Deﬁnition 2.1 (Regret). Let a∗∈arg maxa∈A Ey∼P(·|a)
r(y)
 denote
the optimal arm. The T-period regret of the sequence of actions
a1, . . . , aT is the random variable
Regret(T) =
T
X
t=1
h
r
 Yt(a∗)
 −r
 Yt(at)
i
.
Typically, MAB algorithms focus on the expected regret, E
Regret(T)
,
and provide policies that are guaranteed to keep it small in some sense.
As an illustration of the MAB model, let us revisit the online shop
example.
Example 2.1 (Online shop – bandit setting). Recall the online shop do-
main of Example 1.1. In the MAB setting, there is no state information

2.1. Multi-Armed Bandits
369
about the customers, i.e., X = ∅. In addition, each interaction lasts a
single time step, after which the customer checks out, and a proﬁt is
obtained according to his purchasing decision. The regret minimization
problem corresponds to determining which sequence of advertisements
a1, . . . , aT to show to a stream of T incoming customers, such that the
total revenue is close to that which would have been obtained with the
optimal advertisement stream.
In some cases, the decision-maker may have access to some addi-
tional information that is important for making decisions, but is not
captured in the MAB model. For example, in the online shop domain of
Example 2.1, it is likely that some information about the customer, such
as his age or origin, is available. The contextual bandit model (a.k.a.
associative bandits, or bandits with side information) is an extension
of the MAB model that takes such information into account.
The decision-making process in the contextual bandit model is sim-
ilar to the MAB case. However, at each time step t, the decision maker
ﬁrst observes a context st ∈S, drawn i.i.d. over time from a probability
distribution PS(·) ∈P(S), where P(S) denotes the set of probability
distributions on (Borel) subsets of S. The decision-maker then chooses
an action at ∈A and observes a random outcome Yt(at, st) ∈Y, which
now depends both on the context and the action. The outcomes {Yt(a)}
are drawn i.i.d. over time from an unknown probability distribution
P(·|a, s) ∈P(Y), where P(Y) denotes the set of probability distribu-
tions on (Borel) subsets of Y.
Model 2 (Contextual Bandit) Deﬁne a contextual bandit to be a
tuple ⟨S, A, Y, PS, P, r⟩where
• S is the set of contexts,
• A is the set of actions (arms),
• Y is the set of possible outcomes,
• PS(·) ∈P(S) is the context probability,
• P(·|a, s) ∈P(Y) is the outcome probability, conditioned on action
a ∈A being taken when the context is s ∈S,
• r(Y ) ∈R represents the reward obtained when outcome Y ∈Y is
observed.

370
Technical Background
The Markov decision process model, as presented in the following
section, may be seen as an extension of the contextual bandit model to
a sequential decision-making model, in which the context is no longer
i.i.d., but may change over time according to the selected actions.
2.2
Markov Decision Processes
The Markov Decision Process (MDP) is a framework for sequential
decision-making in Markovian dynamical systems [Bellman, 1957, Put-
erman, 1994]. It can be seen as an extension of the MAB framework
by adding the notion of a system state, that may dynamically change
according to the performed actions and aﬀects the outcomes of the
system.
Model 3 (Markov Decision Process) Deﬁne an MDP M to be a
tuple ⟨S, A, P, P0, q⟩where
• S is the set of states,
• A is the set of actions,
• P(·|s, a) ∈P(S) is the probability distribution over next states, con-
ditioned on action a being taken in state s,
• P0 ∈P(S) is the probability distribution according to which the ini-
tial state is selected,
• R(s, a) ∼q(·|s, a) ∈P(R) is a random variable representing the re-
ward obtained when action a is taken in state s.
Let P(S), P(A), and P(R) be the set of probability distributions
on (Borel) subsets of S, A, and R respectively.1 We assume that P,
P0 and q are stationary. Throughout the paper, we use upper-case and
lower-case letters to refer to random variables and the values taken
by random variables, respectively. For example, R(s, a) is the random
variable of the immediate reward, and r(s, a) is one possible realization
of this random variable. We denote the expected value of R(s, a), as
¯r(s, a) =
R r q(dr|s, a).
Assumption A1 (MDP Regularity) We assume that the random
immediate rewards are bounded by Rmax and the expected immediate
1R is the set of real numbers.

2.2. Markov Decision Processes
371
rewards are bounded by ¯Rmax. Note that ¯Rmax ≤Rmax.
A rule according to which the agent selects its actions at each possible
state, or policy, is deﬁned as a mapping from past observations to a
distribution over the set of actions. A policy is called Markov if the
distribution depends only on the last state of the observation sequence.
A policy is called stationary if it does not change over time. A stationary
Markov policy µ(·|s) ∈P(A) is a probability distribution over the
set of actions given a state s ∈S. A policy is deterministic if the
probability distribution concentrates on a single action for all histories.
A deterministic policy is identiﬁed by a mapping from the set of states
to the set of actions, i.e., µ : S →A. In the rest of the paper, we use
the term policy to refer to stationary Markov policies.
The MDP controlled by a policy µ induces a Markov chain Mµ
with reward distribution qµ(·|s) = q
  · |s, µ(s)
 such that Rµ(s) =
R
 s, µ(s)
 ∈qµ(·|s), transition kernel P µ(·|s) = P
  · |s, µ(s)
, and sta-
tionary distribution over states πµ (if it admits one). In a Markov
chain Mµ, for state-action pairs z = (s, a) ∈Z = S × A, we de-
ﬁne the transition density and the initial (state-action) density as
P µ(z′|z) = P(s′|s, a)µ(a′|s′) and P µ
0 (z0) = P0(s0)µ(a0|s0), respectively.
We generically use ξ = {z0, z1, . . . , zT } ∈Ξ, T ∈{0, 1, . . . , ∞}, to de-
note a path (or trajectory) generated by this Markov chain. The prob-
ability (density) of such a path is given by
Pr(ξ|µ) = P µ
0 (z0)
T
Y
t=1
P µ(zt|zt−1).
We deﬁne the (possibly discounted, γ ∈[0, 1]) return of a path as a
function ρ : Ξ →R, ρ(ξ) = PT
t=0 γtR(zt). For each path ξ, its (dis-
counted) return, ρ(ξ), is a random variable with the expected value
¯ρ(ξ) = PT
t=0 γt¯r(zt).2 Here, γ ∈[0, 1] is a discount factor that de-
termines the exponential devaluation rate of delayed rewards.3 The
2When there is no randomness in the rewards, i.e., r(s, a) = ¯r(s, a), then ρ(ξ) =
¯ρ(ξ).
3When γ = 1, the policy must be proper, i.e., guaranteed to terminate, see Put-
erman [1994] or Bertsekas and Tsitsiklis [1996] for more details.

372
Technical Background
expected return of a policy µ is deﬁned by
η(µ) = E
ρ(ξ)
 =
Z
Ξ
¯ρ(ξ) Pr(ξ|µ)dξ.
(2.1)
The expectation is over all possible trajectories generated by policy µ
and all possible rewards collected in them.
Similarly, for a given policy µ, we can deﬁne the (discounted) return
of a state s, Dµ(s), as the sum of (discounted) rewards that the agent
encounters when it starts in state s and follows policy µ afterwards
Dµ(s) =
∞
X
t=0
γtR(Zt) | Z0 =
 s, µ(·|s)
,
with St+1 ∼P µ(·|St). (2.2)
The expected value of Dµ is called the value function of policy µ
V µ(s) = E
Dµ(s)
 = E
" ∞
X
t=0
γtR(Zt)
 Z0 =
 s, µ(·|s)

#
.
(2.3)
Closely related to value function is the action-value function of a policy,
the total expected (discounted) reward observed by the agent when it
starts in state s, takes action a, and then executes policy µ
Qµ(z) = E
Dµ(z)
 = E
" ∞
X
t=0
γtR(Zt)
 Z0 = z
#
,
where similarly to Dµ(s), Dµ(z) is the sum of (discounted) rewards
that the agent encounters when it starts in state s, takes action a, and
follows policy µ afterwards. It is easy to see that for any policy µ, the
functions V µ and Qµ are bounded by ¯Rmax/(1 −γ). We may write the
value of a state s under a policy µ in terms of its immediate reward
and the values of its successor states under µ as
V µ(s) = Rµ(s) + γ
Z
S
P µ(s′|s)V µ(s′)ds′,
(2.4)
which is called the Bellman equation for V µ.
Given an MDP, the goal is to ﬁnd a policy that attains the best
possible values, V ∗(s) = supµ V µ(s), for all states s ∈S. The function
V ∗is called the optimal value function. A policy is optimal (denoted by
µ∗) if it attains the optimal values at all the states, i.e., V µ∗(s) = V ∗(s)

2.2. Markov Decision Processes
373
for all s ∈S. In order to characterize optimal policies it is useful to
deﬁne the optimal action-value function
Q∗(z) = sup
µ Qµ(z).
(2.5)
Further, we say that a deterministic policy µ is greedy with respect
to an action-value function Q, if for all s ∈S and a ∈A, µ(s) ∈
arg maxa∈A Q(s, a). Greedy policies are important because any greedy
policy with respect to Q∗is optimal. Similar to the value function of a
policy (Eq. 2.4), the optimal value function of a state s may be written
in terms of the optimal values of its successor states as
V ∗(s) = max
a∈A

R(s, a) + γ
Z
S
P(s′|s, a)V ∗(s′)ds′

,
(2.6)
which is called the Bellman optimality equation. Note that, similar to
Eqs. 2.4 and 2.6, we may deﬁne the Bellman equation and the Bellman
optimality equation for action-value function.
It is important to note that almost all methods for ﬁnding the
optimal solution of an MDP are based on two dynamic programming
(DP) algorithms: value iteration and policy iteration. Value iteration
(VI) begins with a value function V0 and at each iteration i, generates
a new value function by applying Eq. 2.6 to the current value function,
i.e.,
V ∗
i (s) = max
a∈A

R(s, a) + γ
Z
S
P(s′|s, a)V ∗
i−1(s′)ds′

,
∀s ∈S.
Policy iteration (PI) starts with an initial policy. At each iteration, it
evaluates the value function of the current policy, this process is referred
to as policy evaluation (PE), and then performs a policy improvement
step, in which a new policy is generated as a greedy policy with respect
to the value of the current policy. Iterating the policy evaluation – pol-
icy improvement process is known to produce a strictly monotonically
improving sequence of policies. If the improved policy is the same as
the policy improved upon, then we are assured that the optimal policy
has been found.

374
Technical Background
2.3
Partially Observable Markov Decision Processes
The Partially Observable Markov Decision Process (POMDP) is an
extension of MDP to the case where the state of the system is not
necessarily observable [Astrom, 1965, Smallwood and Sondik, 1973,
Kaelbling et al., 1998].
Model 4 (Partially Observable Markov Decision Process) De-
ﬁne a POMDP M to be a tuple ⟨S, A, O, P, Ω, P0, q⟩where
• S is the set of states,
• A is the set of actions,
• O is the set of observations,
• P(·|s, a) ∈P(S) is the probability distribution over next states, con-
ditioned on action a being taken in state s,
• Ω(·|s, a) ∈P(O) is the probability distribution over possible obser-
vations, conditioned on action a being taken to reach state s where the
observation is perceived,
• P0 ∈P(S) is the probability distribution according to which the ini-
tial state is selected,
• R(s, a) ∼q(·|s, a) ∈P(R) is a random variable representing the re-
ward obtained when action a is taken in state s.
All assumptions are similar to MDPs, with the addition that P(O) is
the set of probability distributions on (Borel) subsets of O, and Ωis
stationary. As a motivation for the POMDP model, we revisit again
the online shop domain.
Example 2.2 (Online shop – hidden state setting). Recall the online
shop domain of Example 1.1. In a realistic scenario, some features of
the customer, such as its gender or age, might not be visible to the
decision maker due to privacy, or other reasons. In such a case, the MDP
model, which requires the full state information for making decisions
is not suitable. In the POMDP model, only observable quantities, such
as the items in the shopping cart, are used for making decisions.
Since the state is not directly observed, the agent must rely on
the recent history of actions and observations, {ot+1, at, ot, . . . , o1, a0}

2.3. Partially Observable Markov Decision Processes
375
to infer a distribution over states. This belief (also called information
state) is deﬁned over the state probability simplex, bt ∈∆, and can be
calculated recursively as:
bt+1(s′) =
Ω(ot+1|s′, at)
R
S P(s′|s, at)bt(s)ds
R
S Ω(ot+1|s′′, at)
R
S P(s′′|s, at)bt(s)dsds′′ ,
(2.7)
where the sequence is initialized at b0 := P0 and the denominator
can be seen as a simple normalization function. For convenience, we
sometimes denote the belief update (Eq. 2.7) as bt+1 = τ(bt, a, o).
In the POMDP framework, the action-selection policy is deﬁned as
µ : ∆(s) →A. Thus, solving a POMDP involves ﬁnding the optimal
policy, µ∗, that maximizes the expected discounted sum of rewards for
all belief states. This can be deﬁned using a variant of the Bellman
equation
V ∗(bt) = max
a∈A
Z
S
R(s, a)bt(s)ds + γ
Z
O
Pr(o|bt, a)V ∗ τ(bt, a, o)
do

.
(2.8)
The optimal value function for a ﬁnite-horizon POMDP can be shown
to be piecewise-linear and convex [Smallwood and Sondik, 1973, Porta
et al., 2006]. In that case, the value function Vt at any ﬁnite plan-
ning horizon t can be represented by a ﬁnite set of linear segments
Γt = {α0, α1, . . . , αm}, often called α-vectors (when S is discrete) or
α-functions (when S is continuous). Each deﬁnes a linear function over
the belief state space associated with some action a ∈A. The value of
a given αi at a belief bt can be evaluated by linear interpolation
αi(bt) =
Z
S
αi(s)bt(s)ds.
(2.9)
The value of a belief state is the maximum value returned by one of
the α-functions for this belief state
V ∗
t (bt) = max
α∈Γt
Z
S
α(s)bt(s) .
(2.10)
In that case, the best action, µ∗(bt), is the one associated with the
α-vector that returns the best value.
The belief as deﬁned here can be interpreted as a state in a particu-
lar kind of MDP, often called the belief-MDP. The main intuition is that

376
Technical Background
for any partially observable system with known parameters, the belief
is actually fully observable and computable (for small enough state
spaces and is approximable for others). Thus, the planning problem is
in fact one of planning in an MDP, where the state space corresponds
to the belief simplex. This does not lead to any computational advan-
tage, but conceptually, suggests that known results and methods for
MDPs can also be applied to the belief-MDP.
2.4
Reinforcement Learning
Reinforcement learning (RL) [Bertsekas and Tsitsiklis, 1996, Sutton
and Barto, 1998] is a class of learning problems in which an agent
(or controller) interacts with a dynamic, stochastic, and incompletely
known environment, with the goal of ﬁnding an action-selection strat-
egy, or policy, to optimize some measure of its long-term performance.
The interaction is conventionally modeled as an MDP, or if the envi-
ronment state is not always completely observable, as a POMDP [Put-
erman, 1994].
Now that we have deﬁned the MDP model, the next step is to solve
it, i.e., to ﬁnd an optimal policy.4 In some cases, MDPs can be solved
analytically, and in many cases they can be solved iteratively by dy-
namic or linear programming (e.g., see Bertsekas and Tsitsiklis [1996]).
However, in other cases these methods cannot be applied because ei-
ther the state space is too large, a system model is available only as
a simulator, or no system model is available at all. It is in these cases
that RL techniques and algorithms may be helpful.
Reinforcement learning solutions can be viewed as a broad class of
sample-based methods for solving MDPs. In place of a model, these
methods use sample trajectories of the system and the agent interact-
ing, such as could be obtained from a simulation. It is not unusual
in practical applications for such a simulator to be available when an
explicit transition-probability model of the sort suitable for use by dy-
namic or linear programming is not [Tesauro, 1994, Crites and Barto,
4It is not possible to ﬁnd an optimal policy in many practical problems. In such
cases, the goal would be to ﬁnd a reasonably good policy, i.e., a policy with large
enough value function.

2.4. Reinforcement Learning
377
1998]. Reinforcement learning methods can also be used with no model
at all, by obtaining sample trajectories from direct interaction with the
system [Baxter et al., 1998, Kohl and Stone, 2004, Ng et al., 2004].
Reinforcement learning solutions can be categorized in diﬀerent
ways. Below we describe two common categorizations that are relevant
to the structure of this paper.
Model-based vs. Model-free Methods: Reinforcement learning
algorithms that explicitly learn a system model and use it to solve
an MDP are called model-based methods. Some examples of these
methods are the Dyna architecture [Sutton, 1991] and prioritized
sweeping [Moore and Atkeson, 1993]. Model-free methods are those
that do not explicitly learn a system model and only use sample
trajectories obtained by direct interaction with the system. These
methods include popular algorithms such as Q-learning [Watkins,
1989], SARSA [Rummery and Niranjan, 1994], and LSPI [Lagoudakis
and Parr, 2003].
Value Function vs. Policy Search Methods: An important
class of RL algorithms are those that ﬁrst ﬁnd the optimal value func-
tion, and then extract an optimal policy from it. This class of RL al-
gorithms contains value function methods, of which some well-studied
examples include value iteration [Bertsekas and Tsitsiklis, 1996, Sut-
ton and Barto, 1998]), policy iteration [Bertsekas and Tsitsiklis, 1996,
Sutton and Barto, 1998]), Q-learning [Watkins, 1989], SARSA [Rum-
mery and Niranjan, 1994], LSPI [Lagoudakis and Parr, 2003], and ﬁtted
Q-iteration [Ernst et al., 2005]. An alternative approach for solving an
MDP is to directly search in the space of policies. These RL algo-
rithms are called policy search methods. Since the number of policies
is exponential in the size of the state space, one has to resort to meta-
heuristic search [Mannor et al., 2003] or to local greedy methods. An
important class of policy search methods is that of policy gradient al-
gorithms. In these algorithms, the policy is taken to be an arbitrary
diﬀerentiable function of a parameter vector, and the search in the
policy space is directed by the gradient of a performance function with

378
Technical Background
respect to the policy parameters [Williams, 1992, Marbach, 1998, Bax-
ter and Bartlett, 2001]). There is a third class of RL methods that use
policy gradient to search in the policy space, and at the same time esti-
mate a value function. These algorithms are called actor-critic [Konda
and Tsitsiklis, 2000, Sutton et al., 2000, Bhatnagar et al., 2007, Pe-
ters and Schaal, 2008, Bhatnagar et al., 2009]). They can be thought
of as RL analogs of dynamic programming’s policy iteration method.
Actor-critic methods are based on the simultaneous online estimation
of the parameters of two structures, called the actor and the critic. The
actor and the critic correspond to conventional action-selection policy
and value function, respectively. These problems are separable, but are
solved simultaneously to ﬁnd an optimal policy.
2.5
Bayesian Learning
In Bayesian learning, we make inference about a random variable X
by producing a probability distribution for X. Inferences, such as point
and interval estimates, may then be extracted from this distribution.
Let us assume that the random variable X is hidden and we can only
observe a related random variable Y . Our goal is to infer X from the
samples of Y . A simple example is when X is a physical quantity and
Y is its noisy measurement. Bayesian inference is usually carried out
in the following way:
1. We choose a probability density P(X), called the prior distri-
bution, that expresses our beliefs about the random variable X
before we observe any data.
2. We select a statistical model P(Y |X) that reﬂects our belief about
Y given X. This model represents the statistical dependence be-
tween X and Y .
3. We observe data Y = y.
4. We update our belief about X by calculating its posterior distri-
bution using Bayes rule
P(X|Y = y) =
P(y|X)P(X)
R P(y|X′)P(X′)dX′ .

2.5. Bayesian Learning
379
Assume now that P(X) is parameterized by an unknown vector of
parameters θ in some parameter space Θ; we denote this as Pθ(X). Let
X1, . . . , Xn be a random i.i.d. sample drawn from Pθ(X). In general,
updating the posterior Pθ(X|Y = y) is diﬃcult, due to the need to
compute the normalizing constant
R
Θ P(y|X′)Pθ(X′)dθ. However, for
the case of conjugate family distributions, we can update the posterior
in closed-form by simply updating the parameters of the distribution.
In the next two sections, we consider three classes of conjugate distribu-
tions: Beta and Dirichlet distributions and Gaussian Processes (GPs).
2.5.1
Beta and Dirichlet Distributions
A simple example of a conjugate family is the Beta distribution, which
is conjugate to the Binomial distribution. Let Beta(α, β) be deﬁned
by the density function f(p|α, β) ∝pα−1(1 −p)β−1 for p ∈[0, 1], and
parameters α, β ≥0. Let Binomial(n, p) be deﬁned by the density
function f(k|n, p) ∝pk(1 −p)n−k for k ∈{0, 1, . . . , n}, and parameters
p ∈[0, 1] and n ∈N. Consider X ∼Binomial(n, p) with unknown
probability parameter p, and consider a prior Beta(α, β) over the un-
known value of p. Then following an observation X = x, the posterior
over p is also Beta distributed and is deﬁned by Beta(α+x, β +n−x).
Now let us consider the multivariate extension of this conjugate
family. In this case, we have the Multinomial distribution, whose con-
jugate is the Dirichlet distribution. Let X ∼Multinomialk(p, N)
be a random variable with unknown probability distribution p =
(p1, . . . , pk). The Dirichlet distribution is parameterized by a count vec-
tor φ = (φ1, . . . , φk), where φi ≥0, such that the density of probability
distribution p = (p1, . . . , pk) is deﬁned as f(p|φ) ∝Qk
i=1 pφi−1
i
. The
distribution Dirichlet(φ1, . . . , φk) can also be interpreted as a prior
over the unknown Multinomial distribution p, such that after observ-
ing X = n, the posterior over p is also Dirichlet and is deﬁned by
Dirichlet(φ1 + n1, . . . , φk + nk).
2.5.2
Gaussian Processes and Gaussian Process Regression
A Gaussian process (GP) is an indexed set of jointly Gaussian ran-
dom variables, i.e., F(x), x ∈X is a Gaussian process if and only

380
Technical Background
if for every ﬁnite set of indices {x1, . . . , xT } in the index set X,
 F(x1), . . . , F(xT )
 is a vector-valued Gaussian random variable. The
GP F may be thought of as a random vector if X is ﬁnite, as a ran-
dom series if X is countably inﬁnite, and as a random function if X
is uncountably inﬁnite. In the last case, each instantiation of F is a
function f : X →R. For a given x, F(x) is a random variable, nor-
mally distributed jointly with the other components of F. A GP F
can be fully speciﬁed by its mean ¯f : X →R,
¯f(x) = E[F(x)]
and covariance k : X × X →R, k(x, x′) = Cov[F(x), F(x′)], i.e.,
F(·) ∼N
  ¯f(·), k(·, ·)
. The kernel function k(·, ·) encodes our prior
knowledge concerning the correlations between the components of F
at diﬀerent points. It may be thought of as inducing a measure of
proximity between the members of X. It can also be shown that k de-
ﬁnes the function space within which the search for a solution takes
place (see Schölkopf and Smola [2002] and Shawe-Taylor and Cristian-
ini [2004] for more details).
Now let us consider the following generative model:
Y (x) = HF(x) + N(x),
(2.11)
where H is a general linear transformation, F is an unknown function,
x is the input, N is a Gaussian noise and independent of F, and Y is the
observable process, modeled as a noisy version of HF. The objective
here is to infer F from samples of Y . Bayesian learning can be applied
to this problem in the following way.
1. We choose a prior distribution for F in the form of a GP, F(·) ∼
N
  ¯f(·), k(·, ·)
. When F and N are Gaussian and independent
of each other, the generative model of Eq. 2.11 is known as the
linear statistical model [Scharf, 1991].
2. The statistical dependence between F and Y is deﬁned by the
model-equation (2.11).
3. We observe a sample in the form of DT = {(xt, yt)}T
t=1.
4. We calculate the posterior distribution of F conditioned on the
sample DT using the Bayes rule. Below is the process to calculate
this posterior distribution.

2.5. Bayesian Learning
381
F(x1)
F(x2)
F(xT )
Y (x1)
Y (x2)
Y (xT )
N(xT )
N(x2)
N(x1)
Figure 2.1: A directed graph illustrating the conditional independencies between
the latent F(xt) variables (bottom row), the noise variables N(xt) (top row), and
the observable Y (xt) variables (middle row), in GP regression (when H = I). All
of the F(xt) variables should be interconnected by arrows (forming a clique), due
to the dependencies introduced by the prior. To avoid cluttering the diagram, this
was marked by the dashed frame surrounding them.
Figure 2.1 illustrates the GP regression setting (when H = I) as a
graphical model in which arrows mark the conditional dependency re-
lations between the nodes corresponding to the latent F(xt) and the
observed Y (xt) variables. The model-equation (2.11) evaluated at the
training samples may be written as
Y T = HF T + N T ,
(2.12)
where F T =
 F(x1), . . . , F(xT )
⊤, Y T = (y1, . . . , yT )⊤, and N T ∼
N(0, Σ). Here [Σ]i,j is the measurement noise covariance between the
ith and jth samples. In the linear statistical model, we then have
F T ∼N(¯f, K), where ¯f =
  ¯f(x1), . . . , ¯f(xT )
⊤and [K]i,j = k(xi, xj)
is a T × T kernel matrix. Since both F T and N T are Gaussian and
independent from each other, we have Y T ∼N(H ¯f, HKH⊤+ Σ).
Consider a query point x, we then have



F(x)
F T
N T


= N








¯f(x)
¯f
0


,


k(x, x)
k(x)⊤
0
k(x)
K
0
0
0
Σ







,
where k(x) =
 k(x1, x), . . . , k(xT , x)
⊤. Using Eq. 2.12, we have the

382
Technical Background
following transformation:



F(x)
F T
Y T


=


1
0
0
0
I
0
0
H
I





F(x)
F T
N T


,
(2.13)
where I is the identity matrix. From Eq. 2.13, we have



F(x)
F T
Y T


= N








¯f(x)
¯f
H ¯f


,


k(x, x)
k(x)⊤
k(x)⊤H⊤
k(x)
K
KH⊤
Hk(x)
HK
HKH⊤+ Σ







.
Using the Gauss-Markov theorem [Scharf, 1991], we know that
F(x)|Y T (or equivalently F(x)|DT ) is Gaussian, and obtain the fol-
lowing expressions for the posterior mean and covariance of F(x) con-
ditioned on the sample DT :
E
F(x)|DT
 = ¯f(x) + k(x)⊤H⊤(HKH⊤+ Σ)−1(yT −H ¯f),
Cov
F(x), F(x′)|DT
 = k(x, x′) −k(x)⊤H⊤(HKH⊤+ Σ)−1Hk(x′),
(2.14)
where yT = (y1, . . . , yT )⊤is one realization of the random vector Y T .
It is possible to decompose the expressions in Eq. 2.14 into input de-
pendent terms (which depend on x and x′) and terms that only depend
on the training samples, as follows:
E
F(x)|DT
 = ¯f(x) + k(x)⊤α,
Cov
F(x), F(x′)|DT
 = k(x, x′) −k(x)⊤Ck(x′),
(2.15)
where
α = H⊤(HKH⊤+ Σ)−1(yT −H ¯f),
C = H⊤(HKH⊤+ Σ)−1H.
(2.16)
From Eqs. 2.15 and 2.16, we can conclude that α and C are suﬃcient
statistics for the posterior moments.
If we set the transformation H to be the identity and assume that
the noise terms corrupting each sample are i.i.d. Gaussian, i.e., N T ∼
N(0, σ2I), where σ2 is the variance of each noise term, the linear sta-
tistical model is reduced to the standard linear regression model. In this

2.5. Bayesian Learning
383
case, the posterior moments of F(x) can be written as
E
F(x)|DT
 = ¯f(x) + k(x)⊤(K + σ2I)−1(yT −¯f)
= ¯f(x) + k(x)⊤α,
Cov
F(x), F(x′)|DT
 = k(x, x′) −k(x)⊤(K + σ2I)−1k(x′)
= k(x, x′) −k(x)⊤Ck(x′),
(2.17)
with
α = (K + σ2I)−1(yT −¯f),
C = (K + σ2I)−1.
(2.18)
The GP regression described above is kernel-based and non-
parametric. It is also possible to employ a parametric representation
under very similar assumptions. In the parametric setting, the GP F
is assumed to consist of a linear combination of a ﬁnite number n of
basis functions ϕi : X →R, i = 1, . . . , n. In this case, F can be written
as F(·) = Pn
i=1 ϕi(·)Wi = φ(·)⊤W , where φ(·) =
 ϕ1(·), . . . , ϕn(·)
⊤is
the feature vector and W = (W1, . . . , Wn)⊤is the weight vector. The
model-equation (2.11) now becomes
Y (x) = Hφ(x)⊤W + N(x).
In the parametric GP regression, the randomness in F is due to W
being a random vector. Here we consider a Gaussian prior over W ,
distributed as W ∼N( ¯w, Sw). By applying the Bayes rule, the pos-
terior (Gaussian) distribution of W conditioned on the observed data
DT can be computed as
E
W |DT
 = ¯w + SwΦH⊤(HΦ⊤SwΦH⊤+ Σ)−1(yT −HΦ⊤¯w),
Cov
W |DT
 = Sw −SwΦH⊤(HΦ⊤SwΦH⊤+ Σ)−1HΦ⊤Sw,
(2.19)
where Φ =
φ(x1), . . . , φ(xT )
 is a n × T feature matrix. Finally, since
F(x) = φ(x)⊤W , the posterior mean and covariance of F can be easily

384
Technical Background
computed as
E

F(x)|DT

= φ(x)⊤¯w
+ φ(x)⊤SwΦH⊤(HΦ⊤SwΦH⊤+Σ)−1(yT −HΦ⊤¯w),
Cov

F(x), F(x′)|DT

= φ(x)⊤Swφ(x′)
−φ(x)⊤SwΦH⊤(HΦ⊤SwΦH⊤+Σ)−1HΦ⊤Swφ(x′).
(2.20)
Similar to the non-parametric setting, for the standard linear regression
model with the prior W ∼N(0, I), Eq. 2.19 may be written as
E
W |DT
 = Φ(Φ⊤Φ + σ2I)−1yT ,
Cov
W |DT
 = I −Φ(Φ⊤Φ + σ2I)−1Φ⊤.
(2.21)
To have a smaller matrix inversion when T > n, Eq. 2.21 may be
written as
E
W |DT
 = (ΦΦ⊤+ σ2I)−1ΦyT ,
Cov
W |DT
 = σ2(ΦΦ⊤+ σ2I)−1.
(2.22)
For more details on GPs and GP regression, see Rasmussen and
Williams [2006].

3
Bayesian Bandits
This section focuses on Bayesian learning methods for regret mini-
mization in the multi-armed bandits (MAB) model. We review classic
performance bounds for this problem and state-of-the-art results for
several Bayesian approaches.
In the MAB model (§2.1), the only unknown quantities are the
outcome probabilities P(·|a). The idea behind Bayesian approaches to
MAB is to use Bayesian inference (§2.5) for learning P(·|a) from the
outcomes observed during sequential interaction with the MAB. Follow-
ing the framework of §2.5, the outcome probabilities are parameterized
by an unknown vector θ, and are henceforth denoted by Pθ(·|a). The
parameter vector θ is assumed to be drawn from a prior distribution
Pprior. As a concrete example, consider a MAB with Bernoulli arms:
Example 3.1 (Bernoulli K-MAB with a Beta prior). Consider a K-MAB
where the set of outcomes is binary Y = {0, 1}, and the reward is
the identity r(Y ) = Y . Let the outcome probabilities be parameter-
ized by θ ∈RK, such that Y (a) ∼Bernoulli[θa]. The prior for each
θa is Beta(αa, βa). Following an observation outcome Y (a) = y, the
posterior for θa is updated to Beta(αa + y, βa + y −1). Also note
that the (posterior) expected reward for arm a is now E [r(Y (a))] =
E [E [Y (a)| θa]] = E [θa] =
αa+y
αa+y+βa+y−1.
385

386
Bayesian Bandits
The principal challenge of Bayesian MAB algorithms, however, is
to utilize the posterior θ for selecting an adequate policy that achieves
low regret.
Conceptually, there are several reasons to adopt a Bayesian ap-
proach for MABs. First, from a modelling perspective, the prior for θ
is a natural means for embedding prior knowledge, or structure of the
problem. Second, the posterior for θ encodes the uncertainty about the
outcome probabilities at each step of the algorithm, and may be used
for guiding the exploration to more relevant areas.
A Bayesian point of view may also be taken towards the perfor-
mance measure of the algorithm. Recall that the performance of a MAB
algorithm is measured by its expected regret. This expectation, how-
ever, may be deﬁned in two diﬀerent ways, depending on how the pa-
rameters θ are treated. In the frequentist approach, henceforth termed
‘frequentist regret’ and denoted Eθ
Regret(T)
, the parameter vector
θ is ﬁxed, and treated as a constant in the expectation. The expec-
tation is then computed with respect to the stochastic outcomes and
the action selection policy. On the other hand, in ‘Bayesian regret’,
a.k.a. Bayes risk, θ is assumed to be drawn from the prior, and the
expectation E
Regret(T)
 is over the stochastic outcomes, the action
selection rule, and the prior distribution of θ. Note that the optimal ac-
tion a∗depends on θ. Therefore, in the expectation it is also considered
a random variable.
We emphasize the separation of Bayesian MAB algorithms and
Bayesian regret analysis. In particular, the performance of a Bayesian
MAB algorithm (i.e., an algorithm that uses Bayesian learning tech-
niques) may be measured with respect to a frequentist regret, or a
Bayesian one.
3.1
Classical Results
In their seminal work, Lai and Robbins [1985] proved asymptotically
tight bounds on the frequentist regret in terms of the Kullback-Leibler
(KL) divergence between the distributions of the rewards of the dif-
ferent arms. These bounds grow logarithmically with the number of

3.1. Classical Results
387
steps T, such that regret is O (ln T). Mannor and Tsitsiklis [2004] later
showed non-asymptotic lower bounds with a similar logarithmic depen-
dence on T. For the Bayesian regret, the lower bound on the regret is
O
√
KT

(see, e.g., Theorem 3.5 of Bubeck and Cesa-Bianchi [2012]).
In the Bayesian setting, and for models that admit suﬃcient statis-
tics, Gittins [1979] showed that an optimal strategy may be found by
solving a speciﬁc MDP planning problem. The key observation here is
that the dynamics of the posterior for each arm may be represented by
a special MDP termed a bandit process [Gittins, 1979].
Deﬁnition 3.1 (Bandit Process). A bandit process is an MDP with two
actions A = {0, 1}. The control 0 freezes the process in the sense that
P(s′ = s|s, 0) = 1, and R(s, 0) = 0. Control 1 continues the process,
and induces a standard MDP transition to a new state with probability
P(·|s, 1), and reward R(s, 1).
For example, consider the case of Bernoulli bandits with a Beta
prior as described in Example 3.1. We identify the state sa of the
bandit process for arm a as the posterior parameters sa = (αa, βa).
Whenever arm a is pulled, the continuation control is applied as fol-
lows: we draw some θa from the posterior, and then draw an outcome
Y ∼Bernoulli[θa]. The state subsequently transitions to an updated
posterior (cf. Example 3.1) s′
a = (αa + Y, βa + Y −1), and a reward of
E [r(Y )] is obtained, where the expectation is taken over the posterior.
The K-MAB problem, thus, may be seen as a particular instance
of a general model termed simple family of alternative bandit processes
(SFABP) [Gittins, 1979].
Model 5 (Simple Family of Alternative Bandit Processes)
A simple family of alternative bandit processes is a set of K ban-
dit processes. For each bandit process i ∈1, . . . , K we denote by si,
Pi(·|si, 1), and Ri(si, 1) the corresponding state, transition probabili-
ties, and reward, respectively. At each time t = 1, 2, . . . , a single bandit
it ∈1, . . . , K that is in state sit(t) is activated, by applying to it the
continuation control, and all other bandits are frozen. The objective
is to ﬁnd a bandit selection policy that maximizes the expected total

388
Bayesian Bandits
γ-discounted reward
E
" ∞
X
t=0
γtRit(sit(t), 1)
#
.
An important observation is that for the K-MAB problem, as de-
ﬁned above, the SFABP performance measure captures an expectation
of the total discounted reward with respect to the prior distribution
of the parameters θ. In this sense, this approach is a full Bayesian
K-MAB solution (cf. the Bayesian regret vs. frequentist regret discus-
sion above). In particular, note that the SFABP performance measure
implicitly balances exploration and exploitation. To see this, recall the
Bernoulli bandits example, and note that the reward of each bandit pro-
cess is the posterior expected reward of its corresponding arm. Since
the posterior distribution is encoded in the process state, future re-
wards in the SFABP performance measure depend on the future state
of knowledge, thereby implicitly quantifying the value of additional
observations for each arm.
The SFABP may be seen as a single MDP, with a state
(s1, s2, . . . , sK) that is the conjunction of the K individual bandit pro-
cess states. Thus, naively, an optimal policy may be computed by solv-
ing this MDP. Unfortunately, since the size of the state space of this
MDP is exponential in K, such a naive approach is intractable. The
virtue of the celebrated Gittins index theorem [Gittins, 1979], is to
show that this problem nevertheless has a tractable solution.
Theorem 3.1. [Gittins, 1979] The objective of SFABP is maximized
by following a policy that always chooses the bandit with the largest
Gittins index
Gi(si) = sup
τ≥1
E
hPτ−1
t=0 γtRi(si(t), 1)
 si(0) = si
i
E
hPτ−1
t=0 γt
 si(0) = si
i
,
where τ is any (past measurable) stopping time.
The crucial advantage of Theorem 3.1, is that calculating Gi may
be done separately for each arm, thereby avoiding the exponential com-
plexity in K. The explicit calculation, however, is technically involved,

3.2. Bayes-UCB
389
and beyond the scope of this survey. For reference see Gittins [1979],
and also Tsitsiklis [1994] for a simpler derivation, and Niño-Mora [2011]
for a ﬁnite horizon setting.
Due to the technical complexity of calculating optimal Gittin’s
index policies, recent approaches concentrate on much simpler algo-
rithms, that nonetheless admit optimal upper bounds (i.e., match the
order of their respective lower bounds up to constant factors) on the
expected regret.
3.2
Bayes-UCB
The upper conﬁdence bound (UCB) algorithm [Auer et al., 2002] is a
popular frequentist approach for MABs that employs an ‘optimistic’
policy to reduce the chance of overlooking the best arm. The algorithm
starts by playing each arm once. Then, at each time step t, UCB plays
the arm a that maximizes < ra > +
q
2 ln t
ta , where < ra > is the average
reward obtained from arm a, and ta is the number of times arm a
has been playing so far. The optimistic upper conﬁdence term
q
2 ln t
ta
guarantees that the empirical average does not underestimate the best
arm due to ‘unlucky’ reward realizations.
The Bayes-UCB algorithm of Kaufmann et al. [2012a] extends the
UCB approach to the Bayesian setting. A posterior distribution over
the expected reward of each arm is maintained, and at each step, the
algorithm chooses the arm with the maximal posterior (1−βt)-quantile,
where βt is of order 1/t. Intuitively, using an upper quantile instead of
the posterior mean serves the role of ‘optimism’, in the spirit of the
original UCB approach. For the case of Bernoulli distributed outcomes
and a uniform prior on the reward means, Kaufmann et al. [2012a]
prove a frequentist upper bound on the expected regret that matches
the lower bound of Lai and Robbins [1985].
3.3
Thompson Sampling
The Thompson Sampling algorithm (TS) suggests a natural Bayesian
approach to the MAB problem using randomized probability match-
ing [Thompson, 1933]. Let Ppost denote the posterior distribution of

390
Bayesian Bandits
θ given the observations up to time t. In TS, at each time step t, we
sample a parameter ˆθ from the posterior and select the optimal action
with respect to the model deﬁned by ˆθ. Thus, eﬀectively, we match the
action selection probability to the posterior probability of each action
being optimal. The outcome observation is then used to update the
posterior Ppost.
Algorithm 1 Thompson Sampling
1: TS(Pprior)
• Pprior prior distribution over θ
2: Ppost := Pprior
3: for t = 1, 2, . . .
do
4:
Sample ˆθ from Ppost
5:
Play arm at = arg maxa∈A Ey∼Pˆθ(·|a) [r(y)]
6:
Observe outcome Yt and update Ppost
7: end for
Recently, the TS algorithm has drawn considerable attention due to
its state-of-the-art empirical performance [Scott, 2010, Chapelle and Li,
2011], which also led to its use in several industrial applications [Grae-
pel et al., 2010, Tang et al., 2013]. We survey several theoretical studies
that conﬁrm TS is indeed a sound MAB method with state-of-the-art
performance guarantees.
We mention that the TS idea is not limited to MAB problems,
but can be seen as a general sampling technique for Bayesian learn-
ing, and has been applied also to contextual bandit and RL problems,
among others [Strens, 2000, Osband et al., 2013, Abbasi-Yadkori and
Szepesvari, 2015]. In this section, we present results for the MAB and
contextual bandit settings, while the RL related results are given in
Chapter 4.
Agrawal and Goyal [2012] presented frequentist regret bounds for
TS. Their analysis is speciﬁc to Bernoulli arms with a uniform prior, but
they show that by a clever modiﬁcation of the algorithm it may also be
applied to general arm distributions. Let ∆a = Eθ
r
 Y (a∗)
−r
 Y (a)

denote the diﬀerence in expected reward between the optimal arm and
arm a, when the parameter is θ.

3.3. Thompson Sampling
391
Theorem 3.2. [Agrawal and Goyal, 2012] For the K-armed stochastic
bandit problem, the TS algorithm has (frequentist) expected regret
Eθ
Regret(T)
 ≤O
  X
a̸=a∗
1
∆2a
2
ln T
!
.
Improved upper bounds were later presented [Kaufmann et al.,
2012b] for the speciﬁc case of Bernoulli arms with a uniform prior.
These bounds are (order) optimal, in the sense that they match the
lower bound of [Lai and Robbins, 1985]. Let KLθ(a, a∗) denote the
Kullback-Leibler divergence between Bernoulli distributions with pa-
rameters Eθ
r(Y (a))
 and Eθ
r(Y (a∗))
.
Theorem 3.3. [Kaufmann et al., 2012b] For any ϵ > 0, there exists a
problem-dependent constant C(ϵ, θ) such that the regret of TS satisﬁes
Eθ
Regret(T)
 ≤(1 + ϵ)
X
a̸=a∗
∆a
  ln(T) + ln ln(T)

KLθ (a, a∗)
+ C(ϵ, θ).
More recently, Agrawal and Goyal [2013a] showed a problem inde-
pendent frequentist regret bound of order O
√
KT ln T

, for the case
of Bernoulli arms with a Beta prior. This bound holds for all θ, and
implies that a Bayesian regret bound with a similar order holds; up to
the logarithmic factor, this bound is order-optimal.
Liu and Li [2015] investigated the importance of having an in-
formative prior in TS. Consider the special case of two-arms and
two-models, i.e., K = 2, and θ ∈{θtrue, θfalse}, and assume that
θtrue is the true model parameter. When Pprior (θtrue) is small, the
frequentist regret of TS is upper-bounded by O
q
T
Pprior(θtrue)

, and
when Pprior (θtrue) is suﬃciently large, the regret is upper-bounded by
O
q
(1 −Pprior (θtrue))T

[Liu and Li, 2015]. For the special case of
two-arms and two-models, regret lower-bounds of matching orders are
also provided in Liu and Li [2015]. These bounds show that an infor-
mative prior, i.e., a large Pprior (θtrue), signiﬁcantly impacts the regret.
One appealing property of TS is its natural extension to cases with
structure or dependence between the arms. For example, consider the
following extension of the online shop domain:

392
Bayesian Bandits
Example 3.2 (Online Shop with Multiple Product Suggestions). Consider
the bandit setting of the online shop domain, as in Example 2.1. Instead
of presenting to the customer a single product suggestion, the decision-
maker may now suggest M diﬀerent product suggestions a1, . . . , aM
from a pool of suggestions I. The customer, in turn, decides whether
or not to buy each product, and the reward is the sum of proﬁts from
all items bought.
Naively, this problem may be formalized as a MAB with the action
set, A, being the set of all possible combinations of M elements from
I. In such a formulation, it is clear that the outcomes for actions with
overlapping product suggestions are correlated.
In TS, such dependencies between the actions may be incorporated
by simply updating the posterior. Recent advances in Bayesian infer-
ence such as particle ﬁlters and Markov Chain Monte-Carlo (MCMC)
provide eﬃcient numerical procedures for complex posterior updates.
Gopalan et al. [2014] presented frequentist high-probability regret
bounds for TS with general reward distributions and priors, and cor-
related actions. Let NT (a) denote the play count of arm a until time
T and let Dˆθ ∈R|A| denote a vector of Kullback-Leibler divergences
Dˆθ (a) = KL
 Pθ(·|a)∥PˆΘ(·|a)
, where θ is the true model in a frequen-
tist setting. For each action a ∈A, we deﬁne Sa to be the set of model
parameters θ in which the optimal action is a. Within Sa, let S′
a be the
set of models ˆθ that exactly match the true model θ in the sense of the
marginal outcome distribution for action a: KL
 Pθ(·|a)∥Pˆθ(·|a)
 = 0.
Moreover, let e(j) denote the j-th unit vector in a ﬁnite-dimensional
Euclidean space. The following result holds under the assumption of
ﬁnite action and observation spaces, and that the prior has a ﬁnite
support with a positive probability for the true model θ (‘grain of
truth’ assumption).
Theorem 3.4. [Gopalan et al., 2014] For δ, ϵ ∈(0, 1), there exists T ∗>
0 such that for all T > T ∗, with probability at least 1 −δ,
X
a̸=a∗
NT (a) ≤B + C(log T),
where B ≡B(δ, ϵ, A, Y, θ, Pprior) is a problem-dependent constant that

3.3. Thompson Sampling
393
does not depend on T and
C(log T) := max
K−1
X
k=1
zk(ak)
s.t.
zk ∈NK−1
+
× {0},
ak ∈A \ {a∗},
k < K,
zi ⪰zk,
zi(ak) = zk(ak),
i ≥k,
∀j ≥1, k ≤K −1 :
min
ˆθ∈S′ak

zk, Dˆθ
 ≥1 + ϵ
1 −ϵ log T,
min
ˆθ∈S′ak
D
zk −e(j), Dˆθ
E
≥1 + ϵ
1 −ϵ log T.
As Gopalan et al. [2014] explain, the bound in Theorem 3.4 accounts
for the dependence between the arms, and thus, provides tighter guar-
antees when there is information to be gained from this dependence.
For example, in the case of selecting subsets of M arms described ear-
lier, calculating the term C(log T) in Theorem 3.4 gives a bound of
O
 (K −M) log T
, even though the total number of actions is
 K
M
.
We proceed with an analysis of the Bayesian regret of TS. Using in-
formation theoretic tools, Russo and Van Roy [2014a] elegantly bound
the Bayesian regret, and investigate the beneﬁts of having an informa-
tive prior. Let pt denote the distribution of the selected arm at time t.
Note that by the deﬁnition of the TS algorithm, pt encodes the poste-
rior distribution that the arm is optimal. Let pt,a(·) denote the posterior
outcome distribution at time t, when selecting arm a, and let pt,a(·|a∗)
denote the posterior outcome distribution at time t, when selecting arm
a, conditioned on the event that a∗is the optimal action. Both pt,a(·)
and pt,a(·|a∗) are random and depend on the prior and on the history
of actions and observations up to time t. A key quantity in the analysis
of Russo and Van Roy [2014a] is the information ratio deﬁned by
Γt = Ea∼pt
Ey∼pt,a(·|a)r(y) −Ey∼pt,a(·)r(y)

q
Ea∼pt,a∗∼pt
KL
 pt,a(·|a∗)∥pt,a(·)
 .
(3.1)
Note that Γt is also random and depends on the prior and on the history
of the algorithm up to time t. As Russo and Van Roy [2014a] explain,

394
Bayesian Bandits
the numerator in Eq. 3.1 roughly captures how much knowing that
the selected action is optimal inﬂuences the expected reward observed,
while the denominator measures how much on average, knowing which
action is optimal changes the observations at the selected action. In-
tuitively, the information ratio tends to be small when knowing which
action is optimal signiﬁcantly inﬂuences the anticipated observations
at many other actions.
The following theorem relates a bound on Γt to a bound on the
Bayesian regret.
Theorem 3.5. [Russo and Van Roy, 2014a] For any T ∈N, if Γt ≤¯Γ
almost surely for each t ∈{1, . . . , T}, the TS algorithm satisﬁes
E
Regret(T)
 ≤¯Γ
q
H(p1)T,
where H(·) denotes the Shannon entropy.
Russo and Van Roy [2014a] further showed that in general, Γt ≤
p
K/2, giving an order-optimal upper bound of O
q
1
2H(p1)KT

.
However, structure between the arms may be exploited to further
bound the information ratio more tightly. For example, consider the
case of linear optimization under bandit feedback where we have
A ⊂Rd, and the reward satisﬁes Ey∼Pθ(·|a) [r(y)] = a⊤θ. In this case, an
order-optimal bound of O
q
1
2H(p1)dT

holds [Russo and Van Roy,
2014a]. It is important to note that the term H(p1) is bounded by
log(K), but in fact may be much smaller when there is an informative
prior available.
An analysis of TS with a slightly diﬀerent ﬂavour was given by Guha
and Munagala [2014], who studied the stochastic regret of TS, deﬁned
as the expected number of times a sub-optimal arm is chosen, where
the expectation is Bayesian, i.e., taken with respect to Pprior(θ). For
some horizon T, and prior Pprior, let OPT(T, Pprior) denote the stochas-
tic regret of an optimal policy. Such a policy exists, and in principle
may be calculated using dynamic programming (cf. the Gittins index
discussion above). For the cases of two-armed bandits, and K-MABs
with Bernoulli point priors, Guha and Munagala [2014] show that the
stochastic regret of TS, labeled TS(T, Pprior) is a 2-approximation of

3.3. Thompson Sampling
395
OPT(T, Pprior), namely TS(T, Pprior) ≤2OPT(T, Pprior). Interestingly,
and in contrast to the asymptotic regret results discussed above, this
result holds for all T.
We conclude by noting that contextual bandits may be approached
using Bayesian techniques in a very similar manner to the MAB algo-
rithms described above. The only diﬀerence is that the unknown vector
θ should now parameterize the distribution over actions and context,
Pθ(·|a, s). Empirically, the eﬃciency of TS was demonstrated in an
online-advertising application of contextual bandits [Chapelle and Li,
2011]. On the theoretical side, Agrawal and Goyal [2013b] study con-
textual bandits with rewards that linearly depend on the context, and
show a frequentist regret bound of ˜O

d2
ϵ
√
T 1+ϵ

, where d is the dimen-
sion of the context vector, and ϵ is an algorithm-parameter that can
be chosen in (0, 1). For the same problem, Russo and Van Roy [2014b]
derive a Bayesian regret bound of order O

d
√
T ln T

, which, up to
logarithmic terms, matches the order of the O

d
√
T

lower bound for
this problem [Rusmevichientong and Tsitsiklis, 2010].

4
Model-based Bayesian Reinforcement Learning
This section focuses on Bayesian learning methods that explicitly main-
tain a posterior over the model parameters and use this posterior to
select actions. Actions can be selected both for exploration (i.e., achiev-
ing a better posterior), or exploitation (i.e., achieving maximal return).
We review basic representations and algorithms for model-based ap-
proaches, both in MDPs and POMDPs. We also present approaches
based on sampling of the posterior, some of which provide ﬁnite sam-
ple guarantees on the learning process.
4.1
Models and Representations
Initial work on model-based Bayesian RL appeared in the control lit-
erature, under the topic of Dual Control [Feldbaum, 1961, Filatov and
Unbehauen, 2000]. The goal here is to explicitly represent uncertainty
over the model parameters, P, q, as deﬁned in §2.2. One way to think
about this problem is to see the parameters as unobservable states of
the system, and to cast the problem of planning in an MDP with un-
known parameters as planning under uncertainty using the POMDP
formulation. In this case, the belief tracking operation of Eq. 2.7 will
396

4.1. Models and Representations
397
keep a joint posterior distribution over model parameters and the true
physical state of the system, and a policy will be derived to select
optimal actions with respect to this posterior.
Let θs,a,s′ be the (unknown) probability of transitioning from state
s to state s′ when taking action a, θs,a,r the (unknown) probability of
obtaining reward r when taking action a at state s, and θ ∈Θ the set
of all such parameters. The belief P0(θ) expresses our initial knowledge
about the model parameters. We can then compute bt(θ), the belief
after a t-step trajectory {st+1, rt, at, st, . . . , s1, r0, a0, s0}. Considering
a single observation (s, a, r, s′), we have
bt+1(θ′) = η Pr(s′, r|s, a, θ′)
Z
S,Θ
Pr(s′, θ′|s, a, θ)bt(θ)dsdθ ,
(4.1)
where η is the normalizing factor. It is common to assume that the
uncertainty between the parameters is independent, and thus compute
bt(θ) = Q
s,a bt(θs,a,s′)bt(θs,a,r).
Recalling that a POMDP can be represented as a belief-MDP (see
§2.3), a convenient way to interpret Eq. 4.1 is as an MDP where the
state is deﬁned to be a belief over the unknown parameters. Theoret-
ically, optimal planning in this representation can be achieved using
MDP methods for continuous states, the goal being to express a policy
over the belief space, and thus, over any posterior over model parame-
ters. From a computational perspective, this is not necessarily a useful
objective. First, it is computationally expensive, unless there are only
a few unknown model parameters. Second, it is unnecessarily hard: in-
deed, the goal is to learn (via the acquisition of data) an optimal policy
that is robust to parameter uncertainty, not necessarily to pre-compute
a policy for any possible parameterization. An alternative approach is
to make speciﬁc assumptions about the structural form of the uncer-
tainty over the model parameters, thereby allowing us to consider an
alternate representation of the belief-MDP, one that is mathematically
equivalent (under the structural assumptions), but more amenable to
computational analysis.
An instance of this type of approach is the Bayes-Adaptive MDP
(BAMDP) model [Duﬀ, 2002]. Here we restrict our attention to MDPs
with discrete state and action sets. In this case, transition probabilities

398
Model-based Bayesian Reinforcement Learning
consist of Multinomial distributions. The posterior over the transition
function can therefore be represented using a Dirichlet distribution,
P(·|s, a) ∼φs,a. For now, we assume that the reward function is known;
extensions to unknown rewards are presented in §4.7.
Model 6 (Bayes-Adaptive MDP) Deﬁne a Bayes-Adaptive MDP
M to be a tuple ⟨S′, A, P ′, P ′
0, R′⟩where
• S′ is the set of hyper-states, S × Φ,
• A is the set of actions,
• P ′(·|s, φ, a) is the transition function between hyper-states, condi-
tioned on action a being taken in hyper-state (s, φ),
• P ′
0 ∈P(S × Φ) combines the initial distribution over physical states,
with the prior over transition functions φ0,
• R′(s, φ, a) = R(s, a) represents the reward obtained when action a is
taken in state s.
The BAMDP is deﬁned as an extension of the conventional MDP
model. The state space of the BAMDP combines jointly the initial
(physical) set of states S, with the posterior parameters on the transi-
tion function Φ. We call this joint space the hyper-state. Assuming the
posterior on the transition is captured by a Dirichlet distribution, we
have Φ = {φs,a ∈N|S|, ∀s, a ∈S × A}. The action set is the same as
in the original MDP. The reward function is assumed to be known for
now, so it depends only on the physical (real) states and actions.
The transition model of the BAMDP captures transitions between
hyper-states. Due to the nature of the state space, this transition func-
tion has a particular structure. By the chain rule, Pr(s′, φ′|s, a, φ) =
Pr(s′|s, a, φ) Pr(φ′|s, a, s′, φ). The ﬁrst term can be estimated by tak-
ing the expectation over all possible transition functions to yield
φs,a,s′
P
s′′∈S φs,a,s′′ . For the second term, since the update of the posterior
φ to φ′ is deterministic, Pr(φ′|s, a, s′, φ) is 1 if φ′
s,a,s′ = φs,a,s′ + 1, and
0, otherwise. Because these terms depend only on the previous hyper-
state (s, φs,a) and action a, transitions between hyper-states preserve
the Markov property. To summarize, we have (note that I(·) is the

4.1. Models and Representations
399
indicator function):
P ′(s′, φ′|s, a, φ) =
φs,a,s′
P
s′′∈S φs,a,s′′ I(φ′
s,a,s′ = φs,a,s′ + 1).
(4.2)
It is worth considering the number of states in the BAMDP. Initially
(at t = 0), there are only |S|, one per real MDP, state (we assume a
single prior φ0 is speciﬁed). Assuming a fully connected state space in
the underlying MDP (i.e., P(s′|s, a) > 0, ∀s, a), then at t = 1 there are
already |S|×|S| states, since φ →φ′ can increment the count of any one
of its |S| components. So at horizon t, there are |S|t reachable states in
the BAMDP. There are clear computational challenges in computing
an optimal policy over all such beliefs.
Computational concerns aside, the value function of the BAMDP
can be expressed using the Bellman equation
V ∗(s, φ) = max
a∈A

R′(s, φ, a) + γ
X
(s′,φ′)∈S′
P ′(s′, φ′|s, φ, a)V ∗(s′, φ′)


= max
a∈A

R(s, a) + γ
X
s′∈S
φa
s,s′
P
s′′∈S φa
s,s′′ V ∗(s′, φ′)

.
(4.3)
Let us now consider a simple example, the Chain problem, which is
used extensively for empirical demonstrations throughout the literature
on model-based BRL.
Example 4.1 (The Chain problem). The 5-state Chain problem [Strens,
2000], shown in Figure 4.1, requires the MDP agent to select between
two abstract actions {1, 2}. Action 1 causes the agent to move to the
right with probability 0.8 (eﬀect “a” in Figure 4.1) and causes the
agent to reset to the initial state with probability 0.2 (eﬀect “b” in
Figure 4.1). Action 2 causes the agent to reset with probability 0.8
(eﬀect “b”) and causes the agent to move to the right with probability
0.2 (eﬀect “a”). The action b has constant reward of +2. Rewards vary
based on the state and eﬀect (“a” and “b”), as shown in Figure 4.1.
The optimal policy is to always choose action 1, causing the agent to
potentially receive +10 several times until slipping back (randomly) to

400
Model-based Bayesian Reinforcement Learning
the initial state. Of course if the transition probabilities and rewards
are not known, the agent has to trade-oﬀexploration and exploitation
to learn this optimal policy.
Figure 4.1: The Chain problem (ﬁgure reproduced from Strens [2000])
4.2
Exploration/Exploitation Dilemma
A key aspect of reinforcement learning is the issue of exploration. This
corresponds to the question of determining how the agent should choose
actions while learning about the task. This is in contrast to the phase
called exploitation, through which actions are selected so as to maximize
expected reward with respect to the current value function estimate.
In the Bayesian RL framework, exploration and exploitation are
naturally balanced in a coherent mathematical framework. Policies are
expressed over the full information state (or belief), including over
model uncertainty. In that framework, the optimal Bayesian policy
will be to select actions based on how much reward they yield, but
also based on how much information they provide about the parame-
ters of the domain, information which can then be leveraged to acquire
even more reward.
Deﬁnition 4.1 (Bayes optimality). Let
V ∗
t (s, φ) = max
a∈A

R(s, a) + γ
Z
S,Φ
P(s′|b, s, a)V ∗
t−1(s′, φ′)ds′dθ′

be the optimal value function for a t-step planning horizon in a
BAMDP.

4.3. Oﬄine Value Approximation
401
Any policy that maximizes this expression is called a Bayes-optimal
policy. In general, a Bayes-optimal policy is mathematically lower than
the optimal policy (Eq. 2.6), because it may require additional actions
to acquire information about the model parameters. In the next section,
we present planning algorithms that seek the Bayes-optimal policy;
most are based on heuristics or approximations due to the computation
complexity of the problem. In the following section, we also review
algorithms that focus on a slightly diﬀerent criteria, namely eﬃcient
(polynomial) learning of the optimal value function.
4.3
Oﬄine Value Approximation
We now review various classes of approximate algorithms for estimat-
ing the value function in the BAMDP. We begin with oﬄine algorithms
that compute the policy a priori, for any possible state and posterior.
The goal is to compute an action-selection strategy that optimizes the
expected return over the hyper-state of the BAMDP. Given the size
of the state space in the BAMDP, this is clearly intractable for most
domains. The interesting problem then is to devise approximate algo-
rithms that leverage structural constraints to achieve computationally
feasible solutions.
Finite-state controllers
Duﬀ[2001] suggests using Finite-State Controllers (FSC) to compactly
represent the optimal policy µ∗of a BAMDP, and then ﬁnding the best
FSC in the space of FSCs of some bounded size.
Deﬁnition 4.2 (Finite-State Controller). A ﬁnite state controller for a
BAMDP is deﬁned as a graph, where nodes represent memory states
and edges represent observations in the form of (s, a, s′) triplets. Each
node is also associated with an action (or alternately a distribution
over actions), which represents the policy itself.
In this representation, memory states correspond to ﬁnite trajecto-
ries, rather than full hyper-states. Tractability is achieved by limiting
the number of memory states that are included in the graph. Given

402
Model-based Bayesian Reinforcement Learning
a speciﬁc ﬁnite-state controller (i.e., a compact policy), its expected
value can be calculated in closed form using a system of Bellman equa-
tions, where the number of equations/variables is equal to |S| × |Q|,
where |S| is the number of states in the original MDP, and |Q| is
the number of memory states in the FSC [Duﬀ, 2001]. The remaining
challenge is to optimize the policy. This is achieved using a gradient
descent algorithm. A Monte-Carlo gradient estimation is proposed to
make it more tractable. This approach presupposes the existence of a
good FSC representation for the policy. In general, while conceptually
and mathematically straight-forward, this method is computationally
feasible only for small domains with few memory states. For many real-
world domains, the number of memory states needed to achieve good
performance is far too large.
Bayesian Exploration Exploitation Tradeoﬀin LEarning (BEETLE)
An alternate approximate oﬄine algorithm to solve the BAMDP is
called BEETLE [Poupart et al., 2006]. This is an extension of the
Perseus algorithm [Spaan and Vlassis, 2005] that was originally de-
signed for conventional POMDP planning, to the BAMDP model. Es-
sentially, at the beginning, hyper-states (s, φ) are sampled from ran-
dom interactions with the BAMDP model. An equivalent continuous
POMDP (over the space of states and transition functions) is solved
instead of the BAMDP (assuming b = (s, φ) is a belief state in that
POMDP). The value function is represented by a set of α-functions over
the continuous space of transition functions (see Eqs. 2.8-2.10). In the
case of BEETLE, it is possible to maintain a separate set of α-functions
for each MDP state, denoted Γs. Each α-function is constructed as a
linear combination of basis functions α(bt) =
R
θ α(θ)b(θ)dθ. In practice,
the sampled hyper-states can serve to select the set of basis functions
θ. The set of α-functions can then be constructed incrementally by
applying Bellman updates at the sampled hyper-states using standard
point-based POMDP methods [Spaan and Vlassis, 2005, Pineau et al.,
2003].
Thus, the constructed α-functions can be shown to be multi-variate
polynomials. The main computational challenge is that the number of

4.4. Online near-myopic value approximation
403
terms in the polynomials increases exponentially with the planning
horizon. This can be mitigated by projecting each α-function into a
polynomial with a smaller number of terms (using basis functions as
mentioned above). The method has been tested experimentally in some
small simulation domains. The key to applying it in larger domains is
to leverage knowledge about the structure of the domain to limit the
parameter inference to a few key parameters, or by using parameter
tying (whereby a subset of parameters are constrained to have the
same posterior).
4.4
Online near-myopic value approximation
We recall from §4.1 that for a planning horizon of t steps, an oﬄine
BAMDP planner will consider optimal planning at |S|t states. In prac-
tice, there may be many fewer states, in particular because some tra-
jectories will not be observed. Online planning approaches interleave
planning and execution on a step-by-step basis, so that planning re-
sources are focused on those states that have been observed during
actual trajectories. We now review a number of online algorithms de-
veloped for the BAMDP framework.
Bayesian dynamic programming
A simple approach, closely related to Thompson sampling (§3.3), was
proposed by Strens [2000]. The idea is to sample a model from the
posterior distribution over parameters, solve this model using dynamic
programming techniques, and use the solved model to select actions.
Models are re-sampled periodically (e.g., at the end of an episode or af-
ter a ﬁxed number of steps). The approach is simple to implement and
does not rely on any heuristics. Goal-directed exploration is achieved
via sampling of the models. Convergence to the optimal policy is achiev-
able because the method samples models from the full posterior over
parameter uncertainty [Strens, 2000]. Of course this can be very slow,
but it is useful to remember that the dynamic programming steps can
be computed via simulation over the sampled model, and do not re-
quire explicit samples from the system. Convergence of the dynamic

404
Model-based Bayesian Reinforcement Learning
programming inner-loop is improved by keeping maximum likelihood
estimates of the value function for each state-action pair. In the bandit
case (single-step planning horizon), this method is in fact equivalent
to Thompson sampling. Recent work has provided a theoretical char-
acterization of this approach, oﬀering the ﬁrst Bayesian regret bound
for this setting [Osband et al., 2013].
Value of information heuristic
The Bayesian dynamic programming approach does not explicitly take
into account the posterior uncertainty when selecting actions, and thus,
cannot explicitly select actions which only provide information about
the model. In contrast, Dearden et al. [1999] proposed to select actions
by considering their expected value of information (in addition to their
expected reward). Instead of solving the BAMDP directly via Eq. 4.3,
the Dirichlet distributions are used to compute a distribution over the
state-action values Q∗(s, a), in order to select the action that has the
highest expected return and value of information. The distribution over
Q-values is estimated by sampling MDPs from the posterior Dirichlet
distribution, and then solving each sampled MDP (as detailed in §2.2)
to obtain diﬀerent sampled Q-values.
The value of information is used to estimate the expected improve-
ment in policy following an exploration action. Unlike the full Bayes-
optimal approach, this is deﬁned myopically, over a 1-step horizon.
Deﬁnition 4.3 (Value of perfect information [Dearden et al., 1999]). Let
V PI(s, a) denote the expected value of perfect information for taking
action a in state s. This can be estimated by
V PI(s, a) =
Z ∞
−∞
Gains,a(x) Pr(qs,a = x)dx,
where
Gains,a(Q∗
s,a) =





E[qs,a2] −q∗
s,a
if a = a1 and q∗
s,a < E[qs,a2],
q∗
s,a −E[qs,a1]
if a ̸= a1 and q∗
s,a > E[qs,a2],
0
otherwise,
assuming a1 and a2 denote the actions with the best and second-best

4.5. Online Tree Search Approximation
405
expected values respectively, and q∗
s,a denotes a random variable repre-
senting a possible value of Q∗(s, a) in some realizable MDP.
The value of perfect information gives an upper bound on the 1-
step expected value of exploring with action a. To balance exploration
and exploitation, it is necessary to also consider the reward of action a.
Thus, under this approach, the goal is to select actions that maximize
E[qs,a] + V PI(s, a).
From a practical perspective, this method is attractive because it
can be scaled easily by varying the number of samples. Re-sampling and
importance sampling techniques can be used to update the estimated
Q-value distribution as the Dirichlet posteriors are updated. The main
limitation is that the myopic value of information may provide only a
very limited view of the potential information gain of certain actions.
4.5
Online Tree Search Approximation
To select actions using a more complete characterization of the model
uncertainty, an alternative is to perform a forward search in the space
of hyper-states.
Forward search
An approach of this type was used in the case of a partially observable
extension to the BAMDP [Ross et al., 2008a]. The idea here is to con-
sider the current hyper-state and build a ﬁxed-depth forward search
tree containing all hyper-states reachable within some ﬁxed ﬁnite plan-
ning horizon, denoted d. Assuming some default value for the leaves of
this tree, denoted V0(s, φ), dynamic programming backups can be ap-
plied to estimate the expected return of the possible actions at the root
hyper-state. The action with highest return over that ﬁnite horizon is
executed and then the forward search is conducted again on the next
hyper-state. This approach is detailed in Algorithm 2 and illustrated
in Figure 4.2. The main limitation of this approach is the fact that for
most domains, a full forward search (i.e., without pruning of the search
tree) can only be achieved over a very short decision horizon, since
the number of nodes explored is O(|S|d), where d is the search depth.

406
Model-based Bayesian Reinforcement Learning
Another limitation is the need for a default value function to be used at
Figure 4.2: An AND-OR tree constructed by the forward search process for the
Chain problem. The top node contains the initial state 1 and the prior over the
model φ0. After the ﬁrst action, the agent can end up in either state 1 or state 2 of
the Chain and update its posterior accordingly. Note that depending on what action
was chosen (1 or 2) and the eﬀect (a or b), diﬀerent parameters of φ are updated as
per Algorithm 2
.
the leaf nodes; this can either be a naive estimate, such as the immedi-
ate reward, maxa R(s, a), or a value computed from repeated Bellman
backups, such as the one used for the Bayesian dynamic programming
approach. The next algorithm we review proposes some solutions to
these problems.
We take this opportunity to draw the reader’s attention to the sur-
vey paper on online POMDP planning algorithms [Ross et al., 2008c],
which provides a comprehensive review and empirical evaluation of a
range of search-based POMDP solving algorithms, including options for
combining oﬄine and online methods in the context of forward search
trees. Some of these methods may be much more eﬃcient than those
presented above and could be applied to plan in the BAMDP model.
Bayesian Sparse Sampling
Wang et al. [2005] present an online planning algorithm that esti-
mates the optimal value function of a BAMDP (Equation 4.3) using
Monte-Carlo sampling. This algorithm is essentially an adaptation of
the Sparse Sampling algorithm [Kearns et al., 1999] to BAMDPs. The
original Sparse Sampling approach is very simple: a forward-search tree
is grown from the current state to a ﬁxed depth d. At each internal

4.5. Online Tree Search Approximation
407
Algorithm 2 Forward Search Planning in the BAMDP.
1: function ForwardSearch-BAMDP(s, φ, d)
2: if d = 0 then
3:
return V0(s, φ)
4: end if
5: maxQ ←−∞
6: for all a ∈A do
7:
q ←R(s, a)
8:
φ′
s,a ←φs,a
9:
for all s′ ∈S do
10:
φ′
s,a,s′ ←φs,a,s′ + 1
11:
q ←q + γ
φs,a,s′
P
s′′∈S φs,a,s′′ ForwardSearch-BAMDP(s′, φ′, d −1)
12:
φ′
s,a,s′ ←φs,a,s′
13:
end for
14:
if q > maxQ then
15:
maxQ ←q
16:
end if
17: end for
18: return maxQ
node, a ﬁxed number C of next states are sampled from a simulator
for each action in A, to create C|A| children. This is in contrast to the
Forward Search approach of Algorithm 2, which considers all possible
next states. Values at the leaves are estimated to be zero and the values
at the internal nodes are estimated using the Bellman equation based
on their children’s values. The main feature of Sparse Sampling is that
it can be shown to achieve low error with high probability in a number
of samples independent of the number of states [Kearns et al., 1999].
A practical limitation of this approach is the exponential dependence
on planning horizon.
To extend this to BAMDP, Bayesian Sparse Sampling introduces
the following modiﬁcations. First, instead of growing the tree evenly
by looking at all actions at each level of the tree, the tree is grown
stochastically. Actions are sampled according to their likelihood of be-
ing optimal, according to their Q-value distributions (as deﬁned by the
Dirichlet posteriors); next states are sampled according to the Dirichlet
posterior on the model. This is related to Thompson sampling (§3.3),

408
Model-based Bayesian Reinforcement Learning
in that actions are sampled according to their current posterior. The
diﬀerence is that the Bayesian Sampling explicitly considers the long-
term return (as estimated from Monte-Carlo sampling of the posterior
over model parameters), whereas Thompson sampling considers just
the posterior over immediate rewards. The speciﬁc algorithm reported
in Wang et al. [2005] proposes a few improvements on this basic frame-
work: such as, in some cases, sampling from the mean model (rather
than the posterior) to reduce the variance of the Q-value estimates in
the tree, and using myopic Thompson sampling (rather than the se-
quential estimate) to decide which branch to sample when growing the
tree. This approach requires repeatedly sampling from the posterior to
ﬁnd which action has the highest Q-value at each state node in the
tree. This can be very time consuming, and thus, so far the approach
has only been applied to small MDPs.
HMDP: A linear program approach for the hyper-state MDP
Castro and Precup [2007] present a similar approach to Wang et al.
[2005]. However, their approach diﬀers on two main points. First, in-
stead of maintaining only the posterior over models, they also main-
tain Q-value estimates at each state-action pair (for the original MDP
states, not the hyper-states) using standard Q-learning [Sutton and
Barto, 1998]. Second, values of the hyper-states in the stochastic tree
are estimated using linear programming [Puterman, 1994] instead of
dynamic programming. The advantage of incorporating Q-learning is
that it provides estimates for the fringe nodes which can be used as con-
straints in the LP solution. There is currently no theoretical analysis to
accompany this approach. However, the empirical tests on small sim-
ulation domains show somewhat better performance than the method
of Wang et al. [2005], but with similar scalability limitations.
Branch and bound search
Branch and bound is a common method for pruning a search tree. The
main idea is to maintain a lower bound on the value of each node in the
tree (e.g., using partial expansion of the node to some ﬁxed depth with
a lower-bound at the leaf) and then prune nodes whenever they cannot

4.5. Online Tree Search Approximation
409
improve the lower-bound (assuming we are maximizing values). In the
context of BAMDPs, this can be used to prune hyper-state nodes in
the forward search tree [Paquet et al., 2005]. The challenge in this case
is to ﬁnd good bounds; this is especially diﬃcult given the uncertainty
over the underlying model. The method has been used in the context
of partially observable BAMDP [Png and Pineau, 2011, Png, 2011] us-
ing a naive heuristic, PD
d=0 γdRmax, where D is the search depth and
Rmax is the maximum reward. The method was applied successfully to
solve simulated dialogue management problems; computational scala-
bility was achieved via a number of structural constraints, including
the parameter tying method proposed by Poupart et al. [2006].
BOP: Bayesian Optimistic Planning
The BOP method [R. Fonteneau, 2013] is a variant on Branch and
bound search in which nodes are expanded based on an upper-bound
on their value. The upper-bound at a root node, s0, is calculated by
building an optimistic subtree, T +, where leaves are expanded using:
xt = arg maxx∈L(T +) P(x)γd, where L(T +) denotes the fringe of the
forward search tree, P(x) denotes the probability of reaching node x
and d denotes the depth of node x.
Theoretical analysis shows that the regret of BOP decreases ex-
ponentially with the planning budget. Empirical results on the classic
5-state Chain domain (Figure 4.1) conﬁrm that the performance im-
proves with the number of nodes explored in the forward search tree.
BAMCP: Bayes-adaptive Monte-Carlo planning
In recent years, Monte-Carlo tree search methods have been applied
with signiﬁcant success to planning problems in games and other do-
mains [Gelly et al., 2012]. The BAMCP approach exploits insights from
this family of approaches to tackle the problem of joint learning and
planning in the BAMDP [Guez et al., 2012].
BAMCP provides a Bayesian adaptation of an algorithm, called Up-
per Conﬁdence bounds applied to Trees (UCT) [Kocsis and Szepesvari,
2006]. This approach is interesting in that it incorporates two diﬀerent
policies (UCT and rollout) to traverse and grow the forward search

410
Model-based Bayesian Reinforcement Learning
tree. Starting at the root node, for any node that has been previously
visited, it uses the UCT criteria
a∗= arg max
a
Q(s, h, a) + c
s
log
 n(s, h)

n(s, h, a)
to select actions. Along the way, it updates the statistics n(s, h) (the
number of times the node corresponding to state s and history h has
been visited in the search tree) and n(s, h, a) (the number of times
action a was chosen in this node); these are initialized at 0 when the
tree is created. Once it reaches a leaf node, instead of using a default
value function (or bound), it ﬁrst selects any untried action (updating
its count to 1) and then continues to search forward using a rollout
policy until it reaches a given depth (or terminal node). The nodes
visited by the rollout policy are not added to the tree (i.e., no n(·)
statistics are preserved).
To apply this to the Bayesian context, BAMCP must select actions
according to the posterior of the model parameters. Rather than sam-
pling multiple models from the posterior, BAMCP samples a single
model Pt at the root of the tree and uses this same model (without
posterior updating) throughout the search tree to sample next states,
after both UCT and rollout actions. In practice, to reduce computa-
tion in large domains, the model Pt is not sampled in its entirety at
the beginning of the tree building process, rather, it is generated lazily
as samples are required.
In addition, to further improve eﬃciency, BAMCP uses learning
within the forward rollouts to direct resources to important areas of
the search space. Rather than using a random policy for the rollouts,
a model-free estimate of the value function is maintained
ˆQ(st, at) ←ˆQ(st, at) + α
 rt + γ max
a
ˆQ(st+1, a) −ˆQ(st, at)
,
and actions during rollouts are selected according to the ϵ-greedy policy
deﬁned by this estimated ˆQ function.
BAMCP is shown to converge (in probability) to the optimal
Bayesian policy (denoted V ∗(s, h) in general, or V ∗(s, φ) when the pos-
terior is constrained to a Dirichlet distribution). The main complexity

4.6. Methods with Exploration Bonus to Achieve PAC Guarantees
411
result for BAMCP is based on the UCT analysis and shows that the
error in estimating V (st, ht) decreases as O

log
 n(st, ht)
/n(st, ht)

.
Empirical evaluation of BAMCP with a number of simulation do-
mains has shown that it outperforms Bayesian Dynamic Programming,
the Value of Information heuristic, BFS3 [Asmuth and Littman, 2011],
as well as BEB [Kolter and Ng, 2009] and SmartSampler [Castro and
Precup, 2010], both of which will be described in the next section. A
good part of this success could be attributed to the fact that unlike
many forward search sparse sampling algorithm (e.g., BFS3), BAMCP
can take advantage of its learning during rollouts to eﬀectively bias the
search tree towards good solutions.
4.6
Methods with Exploration Bonus to Achieve PAC Guar-
antees
We now review a class of algorithms for the BAMDP model that are
guaranteed to select actions such as to incur only a small loss compared
to the optimal Bayesian policy. Algorithmically, these approaches are
similar to those examined in §4.5 and typically require forward sam-
pling of the model and decision space. Analytically, these approaches
have been shown to achieve bounded error in a polynomial number
of steps using analysis techniques from the Probably Approximately
Correct (PAC) literature. These methods are rooted in earlier papers
showing that reinforcement learning in ﬁnite MDPs can be achieved
in a polynomial number of steps [Kearns and Singh, 1998, Brafman
and Tennenholtz, 2003, Strehl and Littman, 2005]. These earlier pa-
pers did not assume a Bayesian learning framework; the extension to
Bayesian learning was ﬁrst established in the BOSS (Best of Sampled
Sets) approach.
The main idea behind many of the methods presented in this section
is the notion of Optimism in the Face of Uncertainty, which suggests
that, when in doubt, an agent should act according to an optimistic
model of the MDP; in the case where the optimistic model is correct,
the agent will gather reward, and if not, the agent will receive valuable
information from which it can infer a better model.

412
Model-based Bayesian Reinforcement Learning
BFS3: Bayesian Forward Search Sparse Sampling
The BFS3 approach [Asmuth and Littman, 2011] is an extension to
the Bayesian context of the Forward Search Sparse Sampling (FSSS)
approach [Walsh et al., 2010]. The Forward Search Sparse Sampling
itself extends the Sparse Sampling approach [Kearns et al., 1999] de-
scribed above in a few directions. In particular, it maintains both lower
and upper bounds on the value of each state-action pair, and uses this
information to direct forward rollouts in the search tree. Consider a
node s in the tree, then the next action is chosen greedily with re-
spect to the upper-bound U(s, a). The next state s′ is selected to be
the one with the largest diﬀerence between its lower and upper bound
(weighted by the number of times it was visited, denoted by n(s, a, s′)),
i.e., s′ ←arg maxs′∼P(·|s,a)
 U(s′) −L(s′)
n(s, a, s′). This is repeated
until the search tree reaches the desired depth.
The BFS3 approach takes this one step further by building the
sparse search tree over hyper-states (s, φ), instead of over simple states
s. As with most of the other approaches presented in this section, the
framework can handle model uncertainty over either the transition pa-
rameters and/or the reward parameters.
Under certain conditions, one can show that BFS3 chooses at most
a polynomial number of sub-optimal actions compared to an policy.
Theorem 4.1.
[Asmuth, 2013]1 With probability at least 1 −δ, the
expected number of sub-ϵ-Bayes-optimal actions taken by BFS3 is at
most BSA(S + 1)d/δt under assumptions on the accuracy of the prior
and optimism of the underlying FSSS procedure.
Empirical results show that the method can be used to solve small
domains (e.g., 5x5 grid) somewhat more eﬀectively than a non-Bayesian
method such as RMAX [Brafman and Tennenholtz, 2003]. Results also
show that BFS3 can take advantage of structural assumptions in the
prior (e.g., parameter tying) to tackle much larger domains, up to 1016
states.
1We have slightly modiﬁed the description of this theorem, and others below, to
improve legibility of the paper. Refer to the original publication for full details.

4.6. Methods with Exploration Bonus to Achieve PAC Guarantees
413
BOSS: Best of Sample Sets
As per other model-based Bayesian RL approaches presented above,
BOSS [Asmuth et al., 2009] maintains uncertainty over the model pa-
rameters using a parametric posterior distribution and incorporates
new information by updating the posterior. In discrete domains, this
posterior is also typically represented using Dirichlet distributions.
When an action is required, BOSS draws a set of sample models
Mi, i = 1, . . . , K, from the posterior φt. It then creates a hyper-model
M#, which has the same state space S as the original MDP, but has
K|A| actions, where aij corresponds to taking action aj ∈A in model
Mi. The transition function for action ai· is constructed directly by
taking the sampled transitions Pi from Mi. It then solves the hyper-
model M# (e.g., using dynamic programming methods) and selects the
best action according to this hyper-model. This sampling procedure
is repeated a number of times (as determined by B, the knownness
parameter) for each state-action pair, after which the policy of the last
M# is retained.
While the algorithm is simple, the more interesting contribution
is in the analysis. The main theoretical result shows that assuming a
certain knownness parameter B, the value at state s when visited by
algorithm A at time t (which we denote V At(st)), is very close to the
optimal value of the correct model (denoted V ∗) with high probability
in all but a small number of steps. For the speciﬁc case of Dirichlet
priors on the model, it can be shown that the number of necessary
samples, f, is a polynomial function of the relevant parameters.
Theorem 4.2.
[Asmuth et al., 2009] When the knownness param-
eter B = maxs,a f
 s, a, ϵ(1 −γ)2,
δ
|S||A|,
δ
|S|2|A|2K
, then with proba-
bility at least 1 −4δ, V At(st) ≥V ∗(st) −4ϵ in all but ξ(ϵ, δ) =
O
 |S||A|B
ϵ(1−γ)2 ln 1
δ ln
1
ϵ(1−γ)

steps.
Experimental results show that empirically, BOSS performs simi-
larly to BEETLE and Bayesian dynamic programming in simple prob-
lems, such as the Chain problem (Figure 4.1). BOSS can also be ex-
tended to take advantage of structural constraints on the state relations
to tackle larger problems, up to a 6 × 6 maze.

414
Model-based Bayesian Reinforcement Learning
Castro and Precup [2010] proposed an improvement on BOSS,
called SmartSampler, which addresses the problem of how many
models to sample. The main insight is that the number of sampled
models should depend on the variance of the posterior distribution.
When the variance is larger, more models are necessary to achieve good
value function estimation. When the variance is reduced, it is suﬃcient
to sample a smaller number of models. Empirical results show that this
leads to reduced computation time and increased accumulated reward,
compared to the original BOSS.
BEB: Bayesian Exploration Bonus
Bayesian Exploration Bonus (BEB) is another simple approach for
achieving Bayesian reinforcement learning with guaranteed bounded
error within a small number of samples [Kolter and Ng, 2009]. The
algorithm simply chooses actions greedily with respect to the following
value function:
˜V ∗
t (s, φ) = max
a∈A
(
R(s, a)+
β
1 + P
s′∈S φs,a,s′ +
X
s′∈S
P(s′|s, φ, a) ˜V ∗
t−t(s′, φ)
)
Here the middle term on the right-hand side acts as exploration bonus,
whose magnitude is controlled by parameter β. It is worth noting that the
posterior (denoted by φ) is not updated in this equation, and thus, the value
function can be estimated via standard dynamic programming over the state
space (similar to BOSS and Bayesian dynamic programming).
The exploration bonus in BEB decays with 1/n (consider n
∼
P
s′∈S φs,a,s′), which is signiﬁcantly faster than similar exploration bonuses
in the PAC-MDP literature, for example the MBIE-EB algorithm [Strehl and
Littman, 2008], which typically declines with 1/√n. It is possible to use this
faster decay because BEB aims to match the performance of the optimal
Bayesian solution (denoted V ∗(s, φ)), as deﬁned in Theorem 4.3. We call this
property PAC-BAMDP. This is in contrast to BOSS where the analysis is
with respect to the optimal solution of the correct model (denoted V ∗(s)).
Theorem 4.3. [Kolter and Ng, 2009] Let At denote the policy followed by the
BEB algorithm (with β = 2H2) at time t, and let st and φt be the correspond-
ing state and posterior belief. Also suppose we stop updating the posterior
for a state-action pair when P
s′∈S φs,a,s′ ≥4H3/ϵ. Then with probability at
least 1 −δ, for a planning horizon of H, we have
V At(st, φt) ≥V ∗(st, φt) −ϵ

4.6. Methods with Exploration Bonus to Achieve PAC Guarantees
415
for all but m time steps, where
m = O
|S||A|H6
ϵ2
log |S||A|
δ

.
It is worth emphasizing that because BEB considers optimality with re-
spect to the value of the Bayesian optimal policy, it is possible to obtain
theoretical results that are tighter (i.e., fewer number of steps) with respect
to the optimal value function. But this comes at a cost, and in particular, there
are some MDPs for which the BEB algorithm, though it performs closely to
the Bayesian optimum, V ∗(s, φ), may never ﬁnd the actual optimal policy,
V ∗(s). This is illustrated by an example in the dissertation of Li [2009] (see
Example 9, p.80), and formalized in Theorem 4.4 (here n(s, a) denotes the
number of times the state-action pair s, a has been observed).
Theorem 4.4. [Kolter and Ng, 2009] Let At denote the policy followed by
an algorithm using any (arbitrary complex) exploration bonus that is upper
bounded by
β
n(s,a)p , for some constants β and p > 1/2. Then there exists
some MDP, M, and parameter, ϵ0(β, p), such that with probability greater
than δ0 = 0.15,
V At(st) < V ∗(st) −ϵ0,
will hold for an unbounded number of time steps.
Empirical evaluation of BEB showed that in the Chain problem (Fig-
ure 4.1), it could outperform a (non-Bayesian) PAC-MDP algorithm in terms
of sample eﬃciency and ﬁnd the correct optimal policy.
VBRB: Variance-Based Reward Bonus
The Variance-based reward approach also tackles the problem of Bayesian
RL by applying an exploration bonus. However in this case, the exploration
bonus depends on the variance of the model parameters with respect to the
posterior [Sorg et al., 2010], i.e.,
˜V ∗
t (s, φ) = max
a∈A
(
R(s, φ, a) + ˆRs,φ,a +
X
s′∈S
P(s′|s, φ, a) ˜V ∗
t−t(s′, φ)
)
,
where the reward bonus ˆRs,φ,a is deﬁned as
βR σR(s,φ,a) + βP
sX
s′∈S
σ2
P (s′|s,φ,a) ,

416
Model-based Bayesian Reinforcement Learning
with
σ2
R(s,φ,a)
=
Z
θ
R(s, θ, a)2b(θ)dθ −R(s, φ, a)2,
(4.4)
σ2
P (s′|s,φ,a)
=
Z
θ
P(s′|s, θ, a)2b(θ)dθ −P(s′|s, φ, a)2,
(4.5)
and βR and βP are constants controlling the magnitude of the exploration
bonus.2
The main motivation for considering a variance-based bonus is that the
error of the algorithm can then be analyzed by drawing on Chebyshev’s in-
equality (which states that with high probability, the deviation of a random
variable from its mean is bounded by a multiple of its variance). The main
theoretical result concerning the variance-based reward approach bounds the
sample complexity with respect to the optimal policy of the true underlying
MDP, like BOSS (and unlike BEB).
Theorem 4.5. [Sorg et al., 2010] Let the sample complexity of state s and
action a be C(s, a) = f
 b0, s, a, 1
4ϵ(1 −γ)2,
δ
|S||A|,
δ
2|S|2|A|2

. Let the internal
reward be deﬁned as ˆR(s, φ, a) =
1
√ρ

σR(s,φ,a) + γ|S|
1−γ
qP
s′ σ2
P (s′|s,φ,a)

with
ρ =
δ
2|S|2|A|2 . Let θ∗be the random true model parameters distributed ac-
cording to the prior belief φ0. The variance-based reward algorithm will follow
a 4ϵ-optimal policy from its current state, with respect to the MDP θ∗, on all
but O
P
s,a C(s,a)
ϵ(1−γ)2
ln 1
δ ln
1
ϵ(1−γ)

time steps with probability at least 1 −4δ.
For the case where uncertainty over the transition model is modelled using
independent Dirichlet priors (as we have considered throughout this section),
the sample complexity of this approach decreases as a function of O(1/√n).
This is consistent with other PAC-MDP results, which also bound the sample
complexity to achieve small error with respect to the optimal policy of the
true underlying MDP. However, it is not as fast as the BEB approach, which
bounds error with respect to the best Bayesian policy.
Empirical results for the variance-based approach show that it is com-
petitive with BEETLE, BOSS and BEB on versions of the Chain problem
that use parameter tying to reduce the space of model uncertainty, and shows
better performance for the variant of the domain where uncertainty over all
state-action pairs is modelled with an independent Dirichlet distribution. Re-
sults also show superior performance on a 4 × 4 grid-world inspired by the
Wumpus domain of Russell and Norvig [2002].
2The approach is presented here in the context of Dirichlet distributions. There
are ways to generalize this use of variance of the reward as an exploration bonus for
other Bayesian priors [Sorg et al., 2010].

4.6. Methods with Exploration Bonus to Achieve PAC Guarantees
417
BOLT: Bayesian Optimistic Local Transitions
Both BEB and the variance-based reward approach encourage exploration
by putting an exploration bonus on the reward, and solving this modiﬁed
MDP. The main insight in BOLT is to put a similar exploration bonus on the
transition probabilities [Araya-Lopez et al., 2012]. The algorithm is simple
and modelled on BOSS. An extended action space is considered: A′ = A × S,
where each action in ζ = (a, σ) ∈A′ has transition probability ˆP(s′|s, ζ) =
E

P(s′|s, a)|s, φ, λη
s,a,σ

, with φ being the posterior on the model parameters
and λη being a set of η imagined transitions λη
s,a,σ = {(s, a, σ), . . . , (s, a, σ)}.
The extended MDP is solved using standard dynamic programming methods
over a horizon H, where H is a parameter of BOLT, not the horizon of the
original problem. The eﬀect of this exploration bonus is to consider an action
ζ that takes the agent to state σ ∈S with greater probability than has
been observed in the data, and by optimizing a policy over all such possible
actions, we consider an optimistic evaluation over the possible set of models.
The parameter η controls the extent of the optimistic evaluation, as well
as its computational cost (larger η means more actions to consider). When
the posterior is represented using standard Dirichlet posteriors over model
parameters, as considered throughout this section, it can be shown that BOLT
is always optimistic with respect to the optimal Bayesian policy [Araya-Lopez
et al., 2012].
Theorem 4.6. [Araya-Lopez et al., 2012] Let At denote the policy followed
by BOLT at time t with η = H. Let also st and φt be the corresponding
state and posterior at the time. Then, with probability at least 1 −δ, BOLT
is ϵ-close to the optimal Bayesian policy
V At(st, φt) < V ∗(st, φt) −ϵ,
for all but e
O( |S||A|η2
ϵ2(1−γ)2 ) = e
O( |S||A|H2
ϵ2(1−γ)2 ) time steps.
To simplify, the parameter H can be shown to depend on the desired
correctness (ϵ, δ).
Empirical evaluation of BOLT on the Chain problem shows that while it
may be outperformed by BEB with well-tuned parameters, it is more robust to
the choice of parameter (H for BOLT, β for BEB). The authors suggest that
BOLT is more stable with respect to the choice of parameter because optimism
in the transitions is bounded by laws of probability (i.e., even with a large
exploration bonus, η, the eﬀect of extended actions ζ will simply saturate),
which is not the case when the exploration bonus is placed on the rewards.
BOLT was further extended, in the form of an algorithm called POT:
Probably Optimistic Transition [Kawaguchi and Araya-Lopez, 2013], where
the parameter controlling the extended bonus, η, is no longer constant. In-
stead, the transition bonus is estimated online for each state-action pair using

418
Model-based Bayesian Reinforcement Learning
a UCB-like criteria that incorporates the notion of the current estimate of
the transition and the variance of that estimate (controlled by a new param-
eter β). One advantage of the method is that empirical results are improved
(compared to BOLT, BEB, and VBRB) for domains where the prior is good,
though empirical performance can suﬀer when the prior is misspeciﬁed. POT
is also shown to have tighter bounds on the sample complexity. However, the
analysis is done with respect to a modiﬁed optimality criteria, called “Prob-
ably Upper Bounded Belief-based Bayesian Planning", which is weaker than
the standard Bayesian optimality, as deﬁned in §4.2.
Table 4.1 summarizes the key features of the online Bayesian RL methods
presented in this section. This can be used as a quick reference to visualize
similarities and diﬀerences between the large number of related approaches. It
could also be used to devise new approaches, by exploring novel combinations
of the existing components.
4.7
Extensions to Unknown Rewards
Most work on model-based Bayesian RL focuses on unknown transition func-
tions and assumes the reward function is known. This is seen as the more in-
teresting case because there are many domains in which dynamics are diﬃcult
to model a priori, whereas the reward function is set by domain knowledge.
Nonetheless a number of papers explicitly consider the case where the reward
function is uncertain [Dearden et al., 1999, Strens, 2000, Wang et al., 2005,
Castro and Precup, 2007, Sorg et al., 2010]. The BAMDP model extends read-
ily to this case, by extending the set of hyper-states to full set of unknown
parameters: S′ ∈S × Θ, with θ = (φ, ϑ), with ϑ representing the prior over
the unknown reward function. The next challenge is to choose an appropriate
distribution for this prior.
The simplest model is to assume that each state-action pair generates a
binary reward, ϑ = {0, 1}, according to a Bernoulli distribution [Wang et al.,
2005, Castro and Precup, 2007]. In this case, a conjugate prior over the reward
can be captured with a Beta distribution (see §2.5.1). This is by far the most
common approach when dealing with bandit problems. In the cases where the
reward function is drawn from a set of discrete values, ϑ = {r1, r2, . . . , rk},
it can be modeled by a Dirichlet prior over those values. However, many
MDP domains are characterized by more complex (e.g., real-valued) reward
functions.
In those cases, an alternative is to assume that the reward is Gaussian
distributed, i.e., ϑ = {µ, σ}. In this case the choice of prior on the stan-
dard deviation σ is of particular importance; a uniform prior could lead the
posterior to converge to σ →0. Following Strens [2000], we can deﬁne the
precision ψ = 1/σ with prior density f(ψ) ∝ψ exp(−ψ2σ2
0/2); this will have

4.7. Extensions to Unknown Rewards
419
Algorithm
Depth
Actions selected
Next-states
Posterior
Solution
Theoretical
in search
considered
sampling
method
properties
Bayesian DP
N/A
∀a ∈A
∀s′ ∈S
Sample 1
Q() solved by DP
MDP opt. →∞
per step
Bounded
expected
regret
Value of
N/A
Information gain
∀s′ ∈S
Sample K
Q() solved by DP
Not known
information
per step
Forward
ﬁxed d
∀a ∈A
∀s′ ∈S
Re-sample
Backups in tree,
Bayes-opt. →∞
search
at node
heuristic at leave
Bayesian
variable d
a ∼E[ ˆR(s, a)]
s′∼ˆ
Pr(s′|s, a, φ)
Re-sample
Backups in tree,
Not known
Sparse
at node
heuristic at leaves
Sampling
HMDP
ﬁxed d
a ∼rand()
s′∼ˆ
Pr(s′|s, a, φ)
Re-sample
Backups in tree,
Not known
at node
Q() by LP at leaves
Branch and
variable d
∀a ∈A, except if
∀s′ ∈S
Re-sample
Backups in tree,
Bayes-opt. →∞
Bound
∃a′U(s, φ, a) <
at node
heuristic at leaves
L(s, φ, a′)
Table 4.1: Online Model-based Bayesian RL methods (DP=Dynamic programming, LP = Linear programming, U=Upper-
bound, L=Lower-bound)

420
Model-based Bayesian Reinforcement Learning
Algorithm
Depth
Actions selected
Next-states
Posterior
Solution
Theoretical
in search
considered
sampling
method
properties
BOP
variable d
arg maxa B(x, a)
∀s′ ∈S
Re-sample
Backups in tree,
Bayes-opt. →∞
at node
1
1−γ at leaves
BAMCP
ﬁxed d
UCT criteria
s′∼ˆ
Pr(s′|s, a, φ)
Sample 1
Rollouts with
Bayes-opt. →∞
per step
ﬁxed policy ˆµ∗
BFS3
variable d
arg maxa
arg maxs′
Re-sample
Backups in tree,
PAC guarantee
U(s, φ, a)
(U(s′) −L(s′))
at node
heuristic at leaves
BOSS
N/A
A × K
∀s′ ∈S
Sample K
Q() solved by DP
PAC-MDP
per step
Smart-
N/A
A × K
∀s′ ∈S
Sample K
Q() solved by DP
PAC-MDP
Sampler
per step,
K = maxs′
var(P (s,a,s′))
ϵ
BEB
N/A
∀a ∈A
∀s′ ∈S
N/A
Q() + reward bonus
PAC-BAMDP
solved by DP
VBRB
N/A
∀a ∈A
∀s′ ∈S
N/A
Q() + variance bonus
PAC-MDP
rewards
solved by DP
BOLT
N/A
A × S
∀s′ ∈S
Create η
Q() solved by DP
PAC-BAMDP
per step
Table 4.1: Continued

4.8. Extensions to Continuous MDPs
421
a maximum at σ = σ0. We can also deﬁne the prior on the mean to be
f(µ) ∝N(µ0, σ2). The parameters (µ0, σ0) capture any prior knowledge about
the mean and standard deviation of the reward function. After n observations
of the reward with sample mean ˆµ and sample variance ˆσ, the posterior dis-
tribution on ψ is deﬁned by: f(ψ) ∝ψn−1 exp
 −ψ2(nˆσ + σ2
0)/2

, and the
posterior on µ is deﬁned by f(µ) ∝N(ˆµ, σ2/n), where σ = 1/ψ. Note that the
posterior on the variance is updated ﬁrst and used to calculate the posterior
on the mean.
Most of the online model-based BRL algorithms presented in Table 4.1
extend readily to the case of unknown rewards, with the added requirement of
sampling the posterior over rewards (along with the posterior over transition
functions). For an algorithm such as BEB, that uses an exploration bonus in
the value function, it is also necessary to incorporate the uncertainty over re-
wards within that exploration bonus. Sorg et al. [2010] deals with the issue by
expressing the bonus over the variance of the unknown parameters (including
the unknown rewards), as in Eq. 4.5.
4.8
Extensions to Continuous MDPs
Most of the work reviewed in this section focuses on discrete domains. In con-
trast, many of the model-free BRL methods concern the case of continuous
domains, where the Bayesian approach is used to infer a posterior distribu-
tion over value functions, conditioned on the state-action-reward trajectory
observed in the past.
The problem of optimal control under uncertain model parameters for con-
tinuous systems was originally introduced by Feldbaum [1961], as the theory
of dual control (also referred to as adaptive control or adaptive dual control).
Several authors studied the problem for diﬀerent classes of time-varying sys-
tems [Filatov and Unbehauen, 2000, Wittenmark, 1995]: linear time invariant
systems under partial observability [Rusnak, 1995], linear time varying Gaus-
sian models with partial observability [Ravikanth et al., 1992], nonlinear sys-
tems with full observability [Zane, 1992], and nonlinear systems with partial
observability [Greenﬁeld and Brockwell, 2003, Ross et al., 2008b].
This last case is closest mathematically to the Bayes-Adaptive MDP model
considered throughout this paper. The dynamical system is described by a
Gaussian transition model (not necessarily linear):
st = gT (st−1, at−1, Vt),
where gT is a speciﬁed function, and Vt ∼N(µv, Σv) is an unknown k-variate
normal distribution with mean vector µv and covariance matrix Σv. Here
the prior distribution on Vt is represented using a Normal-Wishart distribu-
tion [Ross et al., 2008b]. Particle ﬁlters using Monte-Carlo sampling methods

422
Model-based Bayesian Reinforcement Learning
are used to track the posterior over the parameters of this distribution. Plan-
ning is achieved using a forward search algorithm similar to Algorithm 2.
The Bayesian DP strategy (§4.4) and its analysis were also extended to the
continuous state and action space and average-cost setting, under the assump-
tion of smoothly parameterizable dynamics [Abbasi-Yadkori and Szepesvari,
2015]. The main algorithmic modiﬁcation in this extension is to incorporate
a speciﬁc schedule for updating the policy, that is based on a measure of
uncertainty.
Another approach proposed by Dallaire et al. [2009] allows ﬂexibility over
the choice of transition function. Here the transition and reward functions are
deﬁned by:
st = gT (st−1, at−1) + ϵT ,
(4.6)
rt = gR(st, at) + ϵR,
(4.7)
where gT and gR are modelled by (independent) Gaussian Processes (as de-
ﬁned in §2.5.2) and ϵT and ϵR are zero-mean Gaussian noise terms. Belief
tracking is done by updating the Gaussian process. Planning is achieved by a
forward search tree similar to Algorithm 2.
It is also worth pointing out that the Bayesian Sparse Sampling ap-
proach [Wang et al., 2005] has also been extended to the case where the
reward function is expressed over a continuous action space and represented
by a Gaussian process. In this particular case, the authors only considered
Bayesian inference of the reward function for a single state and a single-step
horizon problem (i.e., a bandit with continuous actions). Under these condi-
tions, inferring the reward function is the same as inferring the value function,
so no planning is required.
4.9
Extensions to Partially Observable MDPs
We can extend the BAMDP model to capture uncertainty over the param-
eters of a POMDP (as deﬁned in §2.3) by introducing the Bayes-Adaptive
POMDP (BAPOMDP) model [Ross et al., 2008a, 2011]. Given a POMDP
model, ⟨S, A, O, P, Ω, P0, R⟩, we deﬁne a corresponding BAPOMDP as fol-
lows:
Model 7 (Bayes-Adaptive POMDP) Deﬁne a BAPOMDP M to be a
tuple ⟨S′, A, P ′, P ′
0, R′⟩where
• S′ is the set of hyper-states, S × Φ × Ψ,
• A is the set of actions,
• P ′(·|s, φ, ψ, a) is the transition function between hyper-states, conditioned
on action a being taken in hyper-state (s, φ, ψ),

4.9. Extensions to Partially Observable MDPs
423
• Ω′(·|s, φ, ψ, a) is the observation function, conditioned on action a being
taken in hyper-state (s, φ, ψ),
• P ′
0 ∈P(S×Φ×Ψ) combines the initial distribution over physical states, with
the prior over transition functions φ0 and the prior over observation functions
ψ0,
• R′(s, φ, ψ, a) = R(s, a) represents the reward obtained when action a is
taken in state s.
Here Φ and Ψ capture the space of Dirichlet parameters for the conjugate
prior over the transition and observation functions, respectively. The Bayesian
approach to learning the transition P and observation Ωinvolves starting
with a prior distribution, which we denote φ0 and ψ0, and maintaining the
posterior distribution over φ and ψ after observing the history of action-
observation-rewards. The belief can be tracked using a Bayesian ﬁlter, with
appropriate handing of the observations. Using standard laws of probability
and independence assumptions, we have
P(s′, φ′, ψ′, z|s, φ, ψ, a) =
P(s′|s, a, φ)P(o|a, s′, ψ)P(φ′|φ, s, a, s′)P(ψ′|ψ, a, s′, o).
(4.8)
As in the BAMDP case (Eq. 4.2), P(s′|s, a, φ) corresponds to the ex-
pected transition model under the Dirichlet posterior deﬁned by φ, and
P(φ′|φ, s, a, s′) is either 0 or 1, depending on whether φ′ corresponds to the
posterior after observing transition (s, a, s′) from prior φ. Hence P(s′|s, a, φ) =
φs,a,s′
P
s′′∈S φs,a,s′′
and P(φ′|φ, s, a, s′)
=
I(φ′
s,a,s′
=
φs,a,s′ + 1). Similarly,
P(o|a, x′, ψ) =
ψs′,a,o
P
o′∈O ψs′,a,o and P(ψ′|ψ, s′, a, o) = I(ψ′
s′,a,o = ψs′,a,o + 1).
Also as in the BAMDP, the number of possible hyper-states, (s, φ, ψ),
grows exponentially (by a factor of |S|) with the prediction horizon. What
is particular to the BAPOMDP is that the number of possible beliefs for
a given trajectory, (a0, o1, . . . , at, ot+1), also grows exponentially. In contrast,
given an observed trajectory, the belief in the BAMDP corresponds to a single
hyper-state. In the BAPOMDP, value-based planning requires estimating the
Bellman equation over all possible hyper-states for every belief:
V ∗ bt(s, φ, ψ)

= max
a
n X
s,φ,ψ
R′(s, φ, ψ, a)bt(s, φ, ψ)
+ γ
X
o∈O
P(o|bt−1, a)V ∗ τ(bt, a, o)
o
.
(4.9)
This is intractable for most problems, due to the large number of possible
beliefs. It has been shown that Eq. 4.9 can be approximated with an ϵ-optimal
value function deﬁned over a smaller ﬁnite-dimensional space [Ross et al.,

424
Model-based Bayesian Reinforcement Learning
2011]. In particular, there exists a point where if we simply stop incrementing
the counts (φ, ψ), the value function of that approximate BAPOMDP (where
the counts are bounded) approximates the BAPOMDP within some ϵ > 0. In
practice, that space is still very large.
The Minerva approach [Jaulmes et al., 2005] overcomes this problem by
assuming that there is an oracle who is willing to provide full state identiﬁca-
tion at any time step. This oracle can be used to deterministically update the
counts ψ and φ (rather than keeping a probability distribution over them, as
required when the state is not fully observable). It is then possible to sample a
ﬁxed number of models from the Dirichlet posterior, solve these models using
standard POMDP planning techniques, and sample an action from the solved
models according to the posterior weight over the corresponding model.
The assumption of a state oracle in Minerva is unrealistic for many do-
mains. The approach of Atrash and Pineau [2009] weakens the assumption to
an action-based oracle, which can be called on to reveal the optimal POMDP
action at any time step for the current belief over states. This oracle does
not consider the uncertainty over model parameters and computes its policy
based on the correct parameters. There are some heuristics to decide when to
query the oracle, but this is largely an unexplored question.
An alternative approach for the BAPOMDP is to use the Bayes risk as
a criterion for choosing actions [Doshi et al., 2008, Doshi-Velez et al., 2011].
This framework considers an extended action set, which augments the initial
action set with a set of query actions corresponding to a consultation with an
oracle. Unlike the oracular queries used in Atrash and Pineau [2009], which
explicitly ask for the optimal action (given the current belief), the oracular
queries in Doshi et al. [2008] request conﬁrmation of an optimal action choice:
I think ai is the best action. Should I do ai? If the oracle answers to the
negative, there is a follow-up query: Then I think aj is best. Is that correct?,
and so on until a positive answer is received. In this framework, the goal is
to select actions with smallest expected loss, deﬁned here as the Bayes risk
(cf. the regret deﬁnition in Chapter 2.1):
BR(a) =
X
s,φ,ψ
Q(bt(s, φ, ψ, a))bt(s, φ, ψ) −
X
s,φ,ψ
Q(bt(s, φ, ψ, a∗))bt(s, φ, ψ),
where Q(·) is just Eq. 4.9 without the maximization over actions and a∗is
the optimal action at bt(s, φ, ψ). Because the second term is independent of
the action choice, to minimize the Bayes risk, we simply consider maximizing
the ﬁrst term over actions. The analysis of this algorithm has yielded a bound
on the number of samples needed to ensure ϵ-error. The bound is quite loose
in practice, but at least provides some upper-bound on the number of queries
needed to achieve good performance. Note that this analysis is based on the
Bayes risk criterion that provides a myopic view of uncertainty (i.e., it assumes

4.10. Extensions to Other Priors and Structured MDPs
425
that the next action will resolve the uncertainty over models).
The planning approach suggested by Ross et al. [2011] aims to approxi-
mate the optimal BAPOMDP strategy by employing a forward search similar
to that outlined in Algorithm 2. In related work, Png and Pineau [2011] use a
branch-and-bound algorithm to approximate the BAPOMDP solution. Many
of the other techniques outlined in Table 4.1 could also be extended to the
BAPOMDP model.
Finally, it is worth mentioning that the method of Dallaire et al. [2009],
described in §4.8, is also able to handle continuous partially observable do-
mains by using an additional Gaussian process for the observation function.
4.10
Extensions to Other Priors and Structured MDPs
The work and methods presented above focus on the case where the model
representation consists of a discrete (ﬂat) set of states. For many larger do-
mains, it is common to assume that the states are arranged in a more sophis-
ticated structure, whether a simple clustering or more complicated hierarchy.
It is therefore interesting to consider how Bayesian reinforcement learning
methods can be used in those cases.
The simplest case, called parameter tying in previous sections, corresponds
to the case where states are grouped according to a pre-deﬁned clustering as-
signment. In this case, it is common to aggregate learned parameters according
to this assignment [Poupart et al., 2006, Sorg et al., 2010, Png and Pineau,
2011]. The advantage is that there are fewer parameters to estimate, and thus,
learning can be achieved with fewer samples. The main downside is that this
requires a hand-coded assignment. In practice, it may also be preferable to
use a coarser clustering than is strictly correct in order to improve variance
(at the expense of more bias). This is a standard model selection problem.
More sophisticated approaches have been proposed to automate the pro-
cess of clustering. In cases where the (unknown) model parameters can be as-
sumed to be sparse, it is possible to incorporate a sparseness assumption in the
Dirichlet estimation through use of a hierarchical prior [Friedman and Singer,
1999]. An alternative is to maintain a posterior over the state clustering. Non-
parametric models of state clustering have been considered using a Chinese
Restaurant Process [Asmuth et al., 2009, Asmuth and Littman, 2011]. These
could be extended to many of the other model-based BRL methods described
above. A related approach was proposed for the case of partially observable
MDPs, where the posterior is expressed over the set of latent (unobserved)
states and is represented using a hierarchical Dirichlet process [Doshi-Velez,
2009].
A sensibly diﬀerent approach proposes to express the prior over the space
of policies, rather than over the space of parametric models [Doshi-Velez et al.,

426
Model-based Bayesian Reinforcement Learning
2010]. The goal here is to leverage trajectories acquired from an expert plan-
ner, which can be used to deﬁne this prior over policies. It is assumed that
the expert knew something about the domain when computing its policy. An
interesting insight is to use the inﬁnite POMDP model [Doshi-Velez, 2009]
to specify the policy prior, by simply reversing the role of actions and obser-
vations; a preference for simpler policies can be expressed by appropriately
setting the hyper-parameters of the hierarchical Dirichlet process.
A few works have considered the problem of model-based BRL in cases
where the underlying MDP has speciﬁc structure. In the case of factored
MDPs, Ross and Pineau [2008] show that it is possible to simultaneously
maintain a posterior over the structure and the parameters of the domain.
The structure in this case captures the bipartite dynamic Bayes network that
describes the state-to-state transitions. The prior over structures is main-
tained using an MCMC algorithm with appropriate graph to graph transition
probabilities. The prior over model parameters is conditioned on the graph
structure. Empirical results show that in some domains, it is more eﬃcient to
simultaneously estimate the structure and the parameters, rather than esti-
mate only the parameters given a known structure.
When the model is parametrized, Gopalan and Mannor [2015] use an in-
formation theoretic approach to quickly ﬁnd the set of “probable” parameters
in a pseudo-Bayesian setting. They show that the regret can be exponentially
lower than models where the model is ﬂat. Furthermore, the analysis can be
done in a frequentist fashion leading eventually to a logarithmic regret.
In multi-task reinforcement learning, the goal is learn a good policy over a
distribution of MDPs. A naive approach would be to assume that all observa-
tions are coming from a single model, and apply Bayesian RL to estimate this
mean model. However by considering a hierarchical inﬁnite mixture model
over MDPs (represented by a hierarchical Dirichlet process), the agent is able
to learn a distribution over diﬀerent classes of MDPs, including estimating
the number of classes and the parameters of each class [Wilson et al., 2007].

5
Model-free Bayesian Reinforcement Learning
As discussed in §2.4, model-free RL methods are those that do not explicitly
learn a model of the system and only use sample trajectories obtained by
direct interaction with the system. In this section, we present a family of
value function Bayesian RL (BRL) methods, called Gaussian process temporal
diﬀerence (GPTD) learning, and two families of policy search BRL techniques:
a class of Bayesian policy gradient (BPG) algorithms and a class of Bayesian
actor-critic (BAC) algorithms.
5.1
Value Function Algorithms
As mentioned in §2.4, value function RL methods search in the space of value
functions, functions from the state (state-action) space to real numbers, to ﬁnd
the optimal value (action-value) function, and then use it to extract an optimal
policy. In this section, we focus on the policy iteration (PI) approach and start
by tackling the policy evaluation problem, i.e., the process of estimating the
value (action-value) function V µ (Qµ) of a given policy µ (see §2.4). In policy
evaluation, the quantity of interest is the value function of a given policy,
which is unfortunately hidden. However, a closely related random variable, the
reward signal, is observable. Moreover, these hidden and observable quantities
are related through the Bellman equation (Eq. 2.4). Thus, it is possible to
extract information about the value function from the noisy samples of the
reward signals. A Bayesian approach to this problem employs the Bayesian
methodology to infer a posterior distribution over value functions, conditioned
427

428
Model-free Bayesian Reinforcement Learning
on the state-reward trajectory observed while running a MDP. Apart from the
value estimates given by the posterior mean, a Bayesian solution also provides
the variance of values around this mean, supplying the practitioner with an
accuracy measure of the value estimates.
In the rest of this section, we study a Bayesian framework that uses a
Gaussian process (GP) approach to this problem, called Gaussian process
temporal diﬀerence (GPTD) learning [Engel et al., 2003, 2005a, Engel, 2005].
We then show how this Bayesian policy evaluation method (GPTD) can be
used for control (to improve the policy and to eventually ﬁnd a good or an
optimal policy) and present a Bayesian value function RL method, called
GPSARSA.
5.1.1
Gaussian Process Temporal Diﬀerence Learning
Gaussian process temporal diﬀerence (GPTD) learning [Engel et al., 2003,
2005a, Engel, 2005] is a GP-based framework that uses linear statistical
models (see §2.5.2) to relate, in a probabilistic way, the underlying hidden
value function with the observed rewards, for a MDP controlled by some
ﬁxed policy µ. The GPTD framework may be used with both parametric and
non-parametric representations of the value function, and applied to general
MDPs with inﬁnite state and action spaces. Various ﬂavors of the basic model
yields several diﬀerent online algorithms, including those designed for learn-
ing action-values, providing the basis for model-free policy improvement, and
thus, full PI algorithms.
Since the focus of this section is on policy evaluation, to simplify the no-
tation, we remove the dependency to the policy µ and use D, P, R, and V
instead of Dµ, P µ, Rµ, and V µ. As shown in §2.2, the value V is the result
of taking the expectation of the discounted return D with respect to the ran-
domness in the trajectory and in the rewards collected therein (Eq. 2.3). In
the classic frequentist approach, V is not random, since it is the true, albeit
unknown, value function of policy µ. On the other hand, in the Bayesian ap-
proach, V is viewed as a random entity by assigning it additional randomness
due to our subjective uncertainty regarding the MDP’s transition kernel P
and reward function q (intrinsic uncertainty). We do not know what the true
functions P and q are, which means that we are also uncertain about the true
value function. We model this additional extrinsic uncertainty by deﬁning V
as a random process indexed by the state variable x. In order to separate the
two sources of uncertainty inherent in the discounted return process D, we
decompose it as
D(s) = E

D(s)

+ D(s) −E

D(s)

= V (s) + ∆V (s),
(5.1)
where
∆V (s)
def
= D(s) −V (s)
(5.2)

5.1. Value Function Algorithms
429
is a zero-mean residual. When the MDP’s model is known, V becomes de-
terministic and the randomness in D is fully attributed to the intrinsic ran-
domness in the state-reward trajectory, modelled by ∆V . On the other hand,
in a MDP in which both the transitions and rewards are deterministic but
unknown, ∆V becomes zero (deterministic), and the randomness in D is due
solely to the extrinsic uncertainty, modelled by V .
We write the following Bellman-like equation for D using its deﬁnition
(Eq. 2.2)
D(s) = R(s) + γD(s′),
s′ ∼P(·|s).
(5.3)
Substituting Eq. 5.1 into Eq. 5.3 and rearranging we obtain1
R(s) = V (s) −γV (s′) + N(s, s′),
s′ ∼P(·|s),
where
N(s, s′)
def
= ∆V (s) −γ∆V (s′).
(5.4)
When we are provided with a system trajectory ξ of size T, we write the
model-equation (5.4) for ξ, resulting in the following set of T equations
R(st) = V (st) −γV (st+1) + N(st, st+1),
t = 0, . . . , T −1.
(5.5)
By deﬁning RT =
 R(s0), . . . , R(sT −1)
⊤, V T +1 =
 V (s0), . . . , V (sT )
⊤, and
N T =
 N(s0, s1), . . . , N(sT −1, sT )
⊤, we write the above set of T equations
as
RT = HV T +1 + N T ,
(5.6)
where H is the following T × (T + 1) matrix:
H =


1
−γ
0
. . .
0
0
0
1
−γ
. . .
0
0
...
...
...
...
...
0
0
0
. . .
1
−γ

.
(5.7)
Note that in episodic problems, if a goal-state is reached at time-step T, the
ﬁnal equation in (5.5) is
R(sT −1) = V (sT −1) + N(sT −1, sT ),
(5.8)
and thus, Eq. 5.6 becomes
RT = HV T + N T ,
(5.9)
1Note that in Eq. 5.4, we removed the dependency of N to policy µ and replaced
N µ with N.

430
Model-free Bayesian Reinforcement Learning
V (s0)
V (s1)
V (s2)
V (sT−2)
V (sT−1)
∆V (sT −1)
∆V (sT −2)
∆V (s0)
∆V (s1)
∆V (s2)
R(s0)
R(s1)
R(sT −2)
R(sT −1)
Figure 5.1: A graph illustrating the conditional independencies between the latent
V (st) value variables (bottom row), the noise variables ∆V (st) (top row), and the
observable R(st) reward variables (middle row), in the GPTD model. As in the case
of GP regression, all of the V (st) variables should be connected by arrows, due to
the dependencies introduced by the prior. To avoid cluttering the diagram, this was
marked by the dashed frame surrounding them.
where H is the following T × T square invertible matrix with determinant
equal to 1,
H =


1
−γ
0
. . .
0
0
0
1
−γ
. . .
0
0
...
...
...
...
...
0
0
0
. . .
1
−γ
0
0
0
. . .
0
1


and H−1 =


1
γ
γ2
. . .
γT −1
0
1
γ
. . .
γT −2
...
...
...
...
0
0
0
. . .
1

.
(5.10)
Figure 5.1 illustrates the conditional dependency relations between the latent
value variables V (st), the noise variables ∆V (st), and the observable rewards
R(st). Unlike the GP regression diagram of Figure 2.1, there are vertices
connecting variables from diﬀerent time steps, making the ordering of samples
important. Since the diagram in Figure 5.1 is for the episodic setting, also note
that for the last state in each episode (sT −1 in this ﬁgure), R(sT −1) depends
only on V (sT −1) and ∆V (sT −1) (as in Eqs. 5.8 and 5.9).
In order to fully deﬁne a probabilistic generative model, we also need to
specify the distribution of the noise process N T . In order to derive the noise
distribution, we make the following two assumptions (see Appendix B for a
discussion about these assumptions):
Assumption A2 The residuals ∆V T +1 =
 ∆V (s0), . . . , ∆V (sT )
⊤can be
modeled as a Gaussian process.

5.1. Value Function Algorithms
431
Assumption A3 Each of the residuals ∆V (st) is generated independently
of all the others, i.e., E

∆V (si)∆V (sj)

= 0, for i ̸= j.
By deﬁnition (Eq. 5.2), E

∆V (s)

= 0 for all s. Using Assumption A3, it is
easy to show that E

∆V (xt)2
= Var

D(xt)

. Thus, denoting Var

D(xt)

=
σ2
t , Assumption A2 may be written as ∆V T +1 ∼N
 0, diag(σT +1)

. Since
N T = H∆V T +1, we have N T ∼N(0, Σ) with
Σ = H diag(σT +1)H⊤
(5.11)
=


σ2
0 + γ2σ2
1
−γσ2
1
0
. . .
0
0
−γσ2
1
σ2
1 + γ2σ2
2
−γσ2
2
. . .
0
0
...
...
...
...
...
0
0
0
. . .
σ2
T −2 + γ2σ2
T −1
−γσ2
T −1
0
0
0
. . .
−γσ2
T −1
σ2
T −1 + γ2σ2
T


.
Eq. 5.11 indicates that the Gaussian noise process N T is colored with a tri-
diagonal covariance matrix. If we assume that for all t = 0, . . . , T, σt = σ,
then diag(σT +1) = σ2I and Eq. 5.11 may be simpliﬁed and written as
Σ = σ2HH⊤= σ2


1 + γ2
−γ
0
. . .
0
0
−γ
1 + γ2
−γ
. . .
0
0
...
...
...
...
...
0
0
0
. . .
1 + γ2
−γ
0
0
0
. . .
−γ
1 + γ2


.
Eq. 5.6 (or in case of episodic problems Eq. 5.9), along with the measure-
ment noise distribution of Eq. 5.11, and a prior distribution for V (deﬁned
either parametrically or non-parametrically, see §2.5.2), completely specify a
statistical generative model relating the value and reward random processes.
In order to infer value estimates from a sequence of observed rewards, Bayes’
rule can be applied to this generative model to derive a posterior distribution
over the value function conditioned on the observed rewards.
In the case in which we place a Gaussian prior over V T , both V T and N T
are normally distributed, and thus, the generative model of Eq. 5.6 (Eq. 5.9)
will belong to the family of linear statistical models discussed in §2.5.2. Con-
sequently, both parametric and non-parametric treatments of this model (see
§2.5.2) may be applied in full to the generative model of Eq. 5.6 (Eq. 5.9),
with H given by Eq. 5.7 (Eq. 5.10). Figure 5.2 demonstrates how the GPTD
model described in this section is related to the family of linear statistical
models and GP regression discussed in §2.5.2.
We are now in a position to write a closed form expression for the
posterior moments of V , conditioned on an observed sequence of rewards

432
Model-free Bayesian Reinforcement Learning
Observable
Process
Unknown
Function
Linear Transformation 
H
GPTD
⇠N
!
0, ⌃= H diag(σT )H>"
0
B
B
B
B
B
B
B
B
@
R(s0)
...
R(st)
R(st+1)
...
R(sT −1)
1
C
C
C
C
C
C
C
C
A
=
z
}|
{
2
666664
1
−γ
0
. . .
0
0
0
1
−γ
. . .
0
0
...
...
...
...
...
0
0
0
. . .
1
−γ
0
0
0
. . .
0
1
3
777775
0
B
B
B
B
B
B
B
B
@
V (s0)
...
V (st)
V (st+1)
...
V (sT −1)
1
C
C
C
C
C
C
C
C
A
+
z
}|
{
0
B
B
B
B
B
B
B
B
@
N(s0, s1)
...
N(st, st+1)
N(st+1, st+2)
...
N(sT −1, sT )
1
C
C
C
C
C
C
C
C
A
R(st) = V (st) −γV (st+1) + N(st, st+1)
N(st, st+1)
def
= ∆V (st) −γ∆V (st+1)
∆V (st)
def
= D(st) −V (st)
Y T = HF T + N T
Observable
Process
General Linear
Transformation
Unknown
Function
Gaussian Noise
(independent of F)
GP Regression
⇠N(0, ⌃)
Figure 5.2: The connection between the GPTD model described in §5.1.1 and the
family of linear statistical models and GP regression discussed in §2.5.2.
rT =
 r(s0), . . . , r(sT −1)
⊤, and derive GPTD-based algorithms for value
function estimation. We refer to this family of algorithms as Monte-Carlo
GPTD (MC-GPTD) algorithms.
Parametric Monte-Carlo GPTD Learning:
In the parametric set-
ting, the value process is parameterized as V (s) = φ(s)⊤W , and therefore,
V T +1 = Φ⊤W with Φ =

φ(s0), . . . , φ(sT )

. As in §2.5.2, if we use the prior

5.1. Value Function Algorithms
433
distribution W ∼N(0, I), the posterior moments of W are given by
E

W |RT = rT

= ∆Φ(∆Φ⊤∆Φ + Σ)−1rT ,
Cov

W |RT = rT

= I −∆Φ(∆Φ⊤∆Φ + Σ)−1∆Φ⊤,
(5.12)
where ∆Φ = ΦH⊤. To have a smaller matrix inversion, Eq. 5.12 may be
written as
E

W |RT = rT

= (∆ΦΣ−1∆Φ⊤+ I)−1∆ΦΣ−1rT ,
Cov

W |RT = rT

= (∆ΦΣ−1∆Φ⊤+ I)−1.
(5.13)
Note that in episodic problems H is invertible, and thus, assuming a constant
noise variance, i.e., diag(σT ) = σ2I, Eq. 5.13 becomes
E

W |RT = rT

= (ΦΦ⊤+ σ2I)−1ΦH−1rT ,
Cov

W |RT = rT

= σ2(ΦΦ⊤+ σ2I)−1.
(5.14)
Eq. 5.14 is equivalent to Eq. 2.22 with yT = H−1rT (see the discussion of
Assumption A3).
Using Eq. 5.13 (or Eq. 5.14), it is possible to derive both a batch algo-
rithm [Engel, 2005, Algorithm 17] and a recursive online algorithm [Engel,
2005, Algorithm 18] to compute the posterior moments of the weight vector
W , and thus, the value function V (·) = φ(·)⊤W .
Non-parametric Monte-Carlo GPTD Learning:
In the non-
parametric case, we bypass the parameterization of the value process by plac-
ing a prior directly over the space of value functions. From §2.5.2 with the
GP prior V T +1 ∈N(0, K), we obtain
E

V (s)|RT = rT

= k(s)⊤α,
Cov

V (s), V (s′)|RT = rT

= k(s, s′) −k(s)⊤Ck(s′),
(5.15)
where
α = H⊤(HKH⊤+Σ)−1rT
and
C = H⊤(HKH⊤+Σ)−1H. (5.16)
Similar to the parametric case, assuming a constant noise variance, Eq. 5.16
may be written for episodic problems as
α = (K + σ2I)−1H−1rT
and
C = (K + σ2I)−1.
(5.17)
Eq. 5.17 is equivalent to Eq. 2.18 with yT = H−1rT (see the discussion of
Assumption A3).

434
Model-free Bayesian Reinforcement Learning
As in the parametric case, it is possible to derive recursive updates for α
and C, and an online algorithm [Engel, 2005, Algorithm 19] to compute the
posterior moments of the value function V .
Sparse Non-parametric Monte-Carlo GPTD Learning:
In the
parametric case, the computation of the posterior may be performed online
in O(n2) time per sample and O(n2) memory, where n is the number of basis
functions used to approximate V . In the non-parametric case, we have a new
basis function for each new sample we observe, making the cost of adding the
t’th sample O(t2) in both time and memory. This would seem to make the
non-parametric form of GPTD computationally infeasible except in small
and simple problems. However, the computational cost of non-parametric
GPTD can be reduced by using an online sparsiﬁcation method ([Engel
et al., 2002], [Engel, 2005, Chapter 2]), to a level where it can be eﬃciently
implemented online. In many cases, this results in signiﬁcant computational
savings, both in terms of memory and time, when compared to the non-sparse
solution. For the resulting algorithm, we refer the readers to the sparse
non-parametric GPTD algorithm in [Engel et al., 2005a, Table 1] or [Engel,
2005, Algorithm 20].
5.1.2
Connections with Other TD Methods
In §5.1.1, we showed that the stochastic GPTD model is equivalent to GP
regression on MC samples of the discounted return (see the discussion of
Assumption A3). Engel [2005] showed that by suitably selecting the noise
covariance matrix Σ in the stochastic GPTD model, it is possible to obtain
GP-based variants of LSTD(λ) algorithm [Bradtke and Barto, 1996, Boyan,
1999]. The main idea is to obtain the value of the weight vector W in the
parametric GPTD model by carrying out maximum likelihood (ML) inference
(W is the value for which the observed data is most likely to be generated
by the stochastic GPTD model). This allows us to derive LSTD(λ) for each
value of λ ≤1, as an ML solution arising from some GPTD generative model
with a speciﬁc noise covariance matrix Σ.
5.1.3
Policy Improvement with GPSARSA
SARSA [Rummery and Niranjan, 1994] is a straightforward extension of the
TD algorithm [Sutton, 1988] to control, in which action-values are estimated.
This allows policy improvement steps to be performed without requiring any
additional knowledge of the MDP model. The idea is that under the policy

5.1. Value Function Algorithms
435
µ, we may deﬁne a process with state space Z = S × A (the space of state-
action pairs) and the reward model of the MDP. This process is Markovian
with transition density P µ(z′|z) = P µ(s′|s)µ(a′|s′), where z = (s, a) and
z′ = (s′, a′) (see §2.2 for more details on this process). SARSA is simply the
TD algorithm applied to this process. The same reasoning may be applied
to derive a GPSARSA algorithm from a GPTD algorithm. This is equivalent
to rewriting the model-equations of §5.1.1 for the action-value function Qµ,
which is a function of z, instead of the value function V µ, a function of s.
In the non-parametric setting, we simple need to deﬁne a covariance kernel
function over state-action pairs, i.e., k : Z × Z →R. Since states and actions
are diﬀerent entities, it makes sense to decompose k into a state-kernel ks
and an action-kernel ka: k(z, z′) = ks(s, s′)ka(a, a′). If both ks and ka are
kernels, we know that k is also a kernel [Schölkopf and Smola, 2002, Shawe-
Taylor and Cristianini, 2004]. The state and action kernels encode our prior
beliefs on value correlations between diﬀerent states and actions, respectively.
All that remains is to run the GPTD algorithms (sparse, batch, online) on
the state-action-reward sequence using the new state-action kernel. Action
selection may be performed by the ϵ-greedy exploration strategy. The main
diﬃculty that may arise here is in ﬁnding the greedy action from a large or
inﬁnite number of possible actions (when |A| is large or inﬁnite). This may be
solved by sampling the value estimates for a few randomly chosen actions and
ﬁnd the greedy action among only them. However, an ideal approach would be
to design the action-kernel in such a way as to provide a closed-form expression
for the greedy action. Similar to the non-parametric setting, in the parametric
case, deriving GPSARSA algorithms from their GPTD counterparts is still
straightforward; we just need to deﬁne a set of basis functions over Z and use
them to approximate action-value functions.
5.1.4
Related Work
Another approach to employ GPs in RL was proposed by Rasmussen and Kuss
[2004]. The approach taken in this work is notably diﬀerent from the genera-
tive approach of the GPTD framework. Two GPs are used in Rasmussen and
Kuss [2004], one to learn the MDP’s transition model and one to estimate
the value function. This leads to an inherently oﬄine algorithm. There are
several other limitations to this framework. First, the state dynamics is as-
sumed to be factored, in the sense that each state coordinate evolves in time
independently of all others. This is a rather strong assumption that is not
likely to be satisﬁed in most real problems. Second, it is assumed that the
reward function is completely known in advance, and is of a very special form
– either polynomial or Gaussian. Third, the covariance kernels used are also
restricted to be either polynomial or Gaussian or a mixture of the two, due to

436
Model-free Bayesian Reinforcement Learning
the need to integrate over products of GPs. Finally, the value function is only
modelled at a predeﬁned set of support states, and is solved only for them.
No method is proposed to ensure that this set of states is representative in
any way.
Similar to most kernel-based methods, the choice of prior distribution
signiﬁcantly aﬀects the empirical performance of the learning algorithm.
Reisinger et al. [2008] proposed an online model (prior covariance function) se-
lection method for GPTD using sequential Monte-Carlo methods and showed
that their method yields better asymptotic performance than standard GPTD
for many diﬀerent kernel families.
5.2
Bayesian Policy Gradient
Policy gradient (PG) methods are RL algorithms that maintain a class of
smoothly parameterized stochastic policies

µ(·|s; θ), s ∈S, θ ∈Θ
	
, and
update the policy parameters θ by adjusting them in the direction of an
estimate of the gradient of a performance measure (e.g., Williams [1992],
Marbach [1998], Baxter and Bartlett [2001]). As an illustration of such a
parameterized policy, consider the following example:
Example 5.1 (Online shop – parameterized policy). Recall the online shop do-
main of Example 1.1. Assume that for each customer state s ∈X, and ad-
vertisement a ∈A, we are given a set of n numeric values φ(s, a) ∈Rn that
represent some features of the state and action. For example, these features
could be the number of items in the cart, the average price of items in the
cart, the age of the customer, etc. A popular parametric policy representation
is the softmax policy, deﬁned as
µ(a|s; θ) =
exp

θ⊤φ(s, a)

P
a′∈A exp

θ⊤φ(s, a′)
.
The intuition behind this policy, is that if θ is chosen such that θ⊤φ(s, a) is
an approximation of the state-action value function Q(s, a), then the softmax
policy is an approximation of the greedy policy with respect to Q – which is
the optimal policy.
The performance of a policy µ is often measured by its expected return,
η(µ), deﬁned by Eq. 2.1. Since in this setting a policy µ is represented by its
parameters θ, policy dependent functions such as η(µ) and Pr
 ξ; µ) may be
written as η(θ) and Pr(ξ; θ).
The score function or likelihood ratio method has become the most promi-
nent technique for gradient estimation from simulation (e.g., Glynn [1990],

5.2. Bayesian Policy Gradient
437
Williams [1992]). This method estimates the gradient of the expected return
with respect to the policy parameters θ, deﬁned by Eq. 2.1, using the following
equation:2
∇η(θ) =
Z
¯ρ(ξ)∇Pr(ξ; θ)
Pr(ξ; θ) Pr(ξ; θ)dξ.
(5.18)
In Eq. 5.18, the quantity
u(ξ; θ) = ∇Pr(ξ; θ)
Pr(ξ; θ)
= ∇log Pr(ξ; θ)
(5.19)
=
T −1
X
t=0
∇µ(at|st; θ)
µ(at|st; θ)
=
T −1
X
t=0
∇log µ(at|st; θ)
is called the (Fisher) score function or likelihood ratio of trajectory ξ under
policy θ. Most of the work on PG has used classical Monte-Carlo (MC) to
estimate the (integral) gradient in Eq. 5.18. These methods (in their simplest
form) generate M i.i.d. sample paths ξ1, . . . , ξM according to Pr(ξ; θ), and
estimate the gradient ∇η(θ) using the unbiased MC estimator
c
∇η(θ) = 1
M
M
X
i=1
ρ(ξi)∇log Pr(ξi; θ) = 1
M
M
X
i=1
ρ(ξi)
Ti−1
X
t=0
∇log µ(at,i|st,i; θ).
(5.20)
Both theoretical results and empirical evaluations have highlighted a major
shortcoming of these algorithms: namely, the high variance of the gradient
estimates, which in turn results in sample-ineﬃciency (e.g., Marbach [1998],
Baxter and Bartlett [2001]). Many solutions have been proposed for this prob-
lem, each with its own pros and cons. In this section, we describe Bayesian
policy gradient (BPG) algorithms that tackle this problem by using a Bayesian
alternative to the MC estimation of Eq. 5.20 [Ghavamzadeh and Engel, 2006].
These BPG algorithms are based on Bayesian quadrature, i.e., a Bayesian ap-
proach to integral evaluation, proposed by O’Hagan [1991].3 The algorithms
use Gaussian processes (GPs) to deﬁne a prior distribution over the gradient
of the expected return, and compute its posterior conditioned on the observed
data. This reduces the number of samples needed to obtain accurate (inte-
gral) gradient estimates. Moreover, estimates of the natural gradient as well
as a measure of the uncertainty in the gradient estimates, namely the gradient
covariance, are provided at little extra cost. In the next two sections, we ﬁrst
brieﬂy describe Bayesian quadrature and then present the BPG models and
algorithms.
2We use the notation ∇to denote ∇θ – the gradient with respect to the policy
parameters.
3O’Hagan [1991] mentions that this approach may be traced even as far back as
the work by Poincaré [1896].

438
Model-free Bayesian Reinforcement Learning
5.2.1
Bayesian Quadrature
Bayesian quadrature (BQ) [O’Hagan, 1991], as its name suggests, is a Bayesian
method for evaluating an integral using samples of its integrand. Let us con-
sider the problem of evaluating the following integral4
ζ =
Z
f(x)g(x)dx.
(5.21)
BQ is based on the following reasoning: In the Bayesian approach, f(·) is
random simply because it is unknown. We are therefore uncertain about the
value of f(x) until we actually evaluate it. In fact, even then, our uncer-
tainty is not always completely removed, since measured samples of f(x) may
be corrupted by noise. Modeling f as a GP means that our uncertainty is
completely accounted for by specifying a Normal prior distribution over the
functions, f(·) ∼N
  ¯f(·), k(·, ·)

, i.e.,
E

f(x)

= ¯f(x)
and
Cov

f(x), f(x′)

= k(x, x′),
∀x, x′ ∈X.
The choice of kernel function k allows us to incorporate prior knowledge on the
smoothness properties of the integrand into the estimation procedure. When
we are provided with a set of samples DM =
 xi, y(xi)
	M
i=1, where y(xi) is
a (possibly noisy) sample of f(xi), we apply Bayes’ rule to condition the prior
on these sampled values. If the measurement noise is normally distributed,
the result is a Normal posterior distribution of f|DM. The expressions for the
posterior mean and covariance are standard (see Eqs. 2.14–2.16):
E

f(x)|DM

= ¯f(x) + k(x)⊤C(y −¯
f),
Cov

f(x), f(x′)|DM

= k(x, x′) −k(x)⊤Ck(x′),
(5.22)
where K is the kernel (or Gram) matrix, and [Σ]i,j is the measurement noise
covariance between the ith and jth samples. It is typically assumed that the
measurement noise is i.i.d., in which case Σ = σ2I, where σ2 is the noise
variance and I is the (M × M) identity matrix.
¯
f =
  ¯f(x1), . . . , ¯f(xM)
⊤,
k(x) =
 k(x1, x), . . . , k(xM, x)
⊤,
y =
 y(x1), . . . , y(xM)
⊤,
[K]i,j = k(xi, xj),
C = (K + Σ)−1 .
4Similar to O’Hagan [1991], here for simplicity we consider the case where the
integral to be estimated is a scalar-valued integral. However, the results of this
section can be extended to vector-valued integrals, such as the gradient of the ex-
pected return with respect to the policy parameters that we shall study in §5.2.2
(see Ghavamzadeh et al. [2013]).

5.2. Bayesian Policy Gradient
439
Since integration is a linear operation, the posterior distribution of the
integral in Eq. 5.21 is also Gaussian, and the posterior moments are given by
O’Hagan [1991]
E[ζ|DM] =
Z
E

f(x)|DM

g(x)dx,
Var[ζ|DM] =
ZZ
Cov

f(x), f(x′)|DM

g(x)g(x′)dxdx′.
(5.23)
Substituting Eq. 5.22 into Eq. 5.23, we obtain
E[ζ|DM] = ζ0 + b⊤C(y −¯
f)
and
Var[ζ|DM] = b0 −b⊤Cb,
where we made use of the deﬁnitions
ζ0 =
Z
¯f(x)g(x)dx , b =
Z
k(x)g(x)dx , b0 =
ZZ
k(x, x′)g(x)g(x′)dxdx′.
(5.24)
Note that ζ0 and b0 are the prior mean and variance of ζ, respectively. It is
important to note that in order to prevent the problem from “degenerating
into inﬁnite regress”, as phrased by O’Hagan [1991], we should decompose the
integrand into parts: f (the GP part) and g, and select the GP prior, i.e., prior
mean ¯f and covariance k, so as to allow us to solve the integrals in Eq. 5.24
analytically. Otherwise, we begin with evaluating one integral (Eq. 5.21) and
end up with evaluating three integrals (Eq. 5.24).
5.2.2
Bayesian Policy Gradient Algorithms
In this section, we describe the Bayesian policy gradient (BPG) algo-
rithms [Ghavamzadeh and Engel, 2006]. These algorithms use Bayesian
quadrature (BQ) to estimate the gradient of the expected return with respect
to the policy parameters (Eq. 5.18). In the Bayesian approach, the expected
return of the policy characterized by the parameters θ
ηB(θ) =
Z
¯ρ(ξ) Pr(ξ; θ)dξ
(5.25)
is a random variable because of our subjective Bayesian uncertainty concern-
ing the process generating the return. Under the quadratic loss, the opti-
mal Bayesian performance measure is the posterior expected value of ηB(θ),
E

ηB(θ)|DM

. However, since we are interested in optimizing the performance
rather than in evaluating it, we would rather evaluate the posterior mean of
the gradient of ηB(θ) with respect to the policy parameters θ, i.e.,
∇E

ηB(θ)|DM

= E

∇ηB(θ)|DM

(5.26)
= E
Z
¯ρ(ξ)∇Pr(ξ; θ)
Pr(ξ; θ) Pr(ξ; θ)dξ
DM

.

440
Model-free Bayesian Reinforcement Learning
Gradient Estimation:
In BPG, we cast the problem of estimating the gra-
dient of the expected return (Eq. 5.26) in the form of Eq. 5.21 and use the BQ
approach described in §5.2.1. We partition the integrand into two parts, f(ξ; θ)
and g(ξ; θ), model f as a GP, and assume that g is a function known to us. We
then proceed by calculating the posterior moments of the gradient ∇ηB(θ)
conditioned on the observed data. Ghavamzadeh and Engel [2006] proposed
two diﬀerent ways of partitioning the integrand in Eq. 5.26, resulting in the
two distinct Bayesian models summarized in Table 5.1 (see Ghavamzadeh
et al. [2013], for more details). Figure 5.3 shows how the BQ approach has
been used in each of the two BPG models of Ghavamzadeh and Engel [2006],
summarized in Table 5.1, as well as in the Bayesian actor-critic (BAC) for-
mulation of §5.3.
⇣=
Z z}|{
f(x) g(x) dx
|
{z
}
Modeled
as GP
E
⇥
f(x)|DM
⇤
, Cov
⇥
f(x), f(x0)|DM
⇤
E[⇣|DM] , Var[⇣|DM]
Bayesian Quadrature
r⌘B(✓) =
Z
f
z
}|
{
¯⇢(⇠) r log Pr(⇠; ✓) Pr(⇠; ✓)
| {z }
g
d⇠
BPG Model 1
Modeled as GP
r⌘B(✓) =
Z
f
z}|{
¯⇢(⇠) r log Pr(⇠; ✓) Pr(⇠; ✓)
|
{z
}
g
d⇠
BPG Model 2
Modeled
 as GP
BAC
r⌘(✓) =
Z
dsda ⌫(s; ✓) rµ(a|s; ✓)
|
{z
}
g
f
z
}|
{
Q(s, a; ✓)
Modeled
 as GP
Figure 5.3: The connection between BQ and the two BPG models of Ghavamzadeh
and Engel [2006], and the Bayesian actor-critic (BAC) formulation of §5.3.
It is important to note that in both models ¯ρ(ξ) belongs to the GP part,
i.e., f(ξ; θ). This is because in general, ¯ρ(ξ) cannot be known exactly, even
for a given ξ (due to the stochasticity of the rewards), but can be estimated
for a sample trajectory ξ. The more important and rather critical point is
that Pr(ξ; θ) cannot belong to either the f(ξ; θ) or g(ξ; θ) part of the model,
since it is not known (in model-free setting) and cannot be estimated for a
sample trajectory ξ. However, Ghavamzadeh and Engel [2006] showed that
interestingly, it is suﬃcient to assign Pr(ξ; θ) to the g(ξ; θ) part of the model
and use an appropriate Fisher kernel (see Table 5.1 for the kernels), rather
than having exact knowledge of Pr(ξ; θ) itself (see Ghavamzadeh et al. [2013]
for more details).
In Model 1, a vector-valued GP prior is placed over f(ξ; θ). This induces
a GP prior over the corresponding noisy measurement y(ξ; θ). It is assumed

5.2. Bayesian Policy Gradient
441
Model 1
Model 2
Deter. factor (g)
g(ξ; θ) = Pr(ξ; θ)
g(ξ; θ) =
∇Pr(ξ; θ)
GP factor (f)
f(ξ; θ) = ¯ρ(ξ)∇log Pr(ξ; θ)
f(ξ) = ¯ρ(ξ)
Measurement (y)
y(ξ; θ) = ρ(ξ)∇log Pr(ξ; θ)
y(ξ) = ρ(ξ)
Prior mean of f
E
fj(ξ; θ)
 = 0
E
f(ξ)
 = 0
Prior Cov. of f
Cov
fj(ξ; θ), fℓ(ξ′; θ)

Cov
f(ξ), f(ξ′)

= δj,ℓk(ξ, ξ′)
= k(ξ, ξ′)
Kernel function
k(ξ, ξ′) =
k(ξ, ξ′) =
 1 + u(ξ)⊤G−1u(ξ′)
2
u(ξ)⊤G−1u(ξ′)
E
∇ηB(θ)|DM

Y Cb
BCy
Cov
∇ηB(θ)|DM

(b0 −b⊤Cb)I
B0 −BCB⊤
b or B
(b)i = 1 + u(ξi)⊤G−1u(ξi)
B = U
b0 or B0
b0 = 1 + n
B0 = G
Table 5.1: Summary of the Bayesian policy gradient Models 1 and 2.
that each component of f(ξ; θ) may be evaluated independently of all other
components, and thus, the same kernel function K and noise covariance Σ
are used for all components of f(ξ; θ). Hence for the jth component of f and
y we have a priori
f j =
 fj(ξ1; θ), . . . , fj(ξM; θ)
⊤∼N(0, K),
yj =
 yj(ξ1; θ), . . . , yj(ξM; θ)
⊤∼N(0, K + Σ).
This vector-valued GP model gives us the posterior mean and covariance of
∇ηB(θ) reported in Table 5.1m in which Y =

y⊤
1 ; . . . ; y⊤
n

, C = (K +Σ)−1,
n is the number of policy parameters, and G(θ) is the Fisher information
matrix of policy θ deﬁned as5
G(θ) = E

u(ξ)u(ξ)⊤
=
Z
u(ξ)u(ξ)⊤Pr(ξ; θ)dξ,
(5.27)
where u(ξ) is the score function of trajectory ξ deﬁned by Eq. 5.19. Note that
the choice of the kernel k allows us to derive closed form expressions for b and
b0, and, as a result k is the quadratic Fisher kernel for the posterior moments
of the gradient (see Table 5.1).
5To simplify notation, we omit G’s dependence on the policy parameters θ, and
denote G(θ) as G in the rest of this section.

442
Model-free Bayesian Reinforcement Learning
In Model 2, g is a vector-valued function and f is a scalar valued GP
representing the expected return of the path given as its argument. The
noisy measurement corresponding to f(ξi) is y(ξi) = ρ(ξi), namely, the ac-
tual return accrued while following the path ξi. This GP model gives us the
posterior mean and covariance of ∇ηB(θ) reported in Table 5.1 in which
y =
 ρ(ξ1), . . . , ρ(ξM)
⊤and U =

u(ξ1), . . . , u(ξM)

. Here the choice of ker-
nel k allows us to derive closed-form expressions for B and B0, and as a
result, k is again the Fisher kernel for the posterior moments of the gradient
(see Table 5.1).
Note that the choice of Fisher-type kernels is motivated by the notion
that a good representation should depend on the process generating the data
(see Jaakkola and Haussler [1999], Shawe-Taylor and Cristianini [2004], for a
thorough discussion). The particular selection of linear and quadratic Fisher
kernels is guided by the desideratum that the posterior moments of the gra-
dient be analytically tractable as discussed at the end of §5.2.1.
The above two BPG models can deﬁne Bayesian algorithms for evaluat-
ing the gradient of the expected return with respect to the policy parameters
(see Ghavamzadeh and Engel [2006] and Ghavamzadeh et al. [2013] for the
pseudo-code of the resulting algorithms). It is important to note that comput-
ing the quadratic and linear Fisher kernels used in Models 1 and 2 requires
calculating the Fisher information matrix G(θ) (Eq. 5.27). Consequently,
every time the policy parameters are updated, G needs to be recomputed.
Ghavamzadeh and Engel [2006] suggest two possible approaches for online
estimation of the Fisher information matrix.
Similar to most non-parametric methods, the Bayesian gradient evalua-
tion algorithms can be made more eﬃcient, both in time and memory, by
sparsifying the solution. Sparsiﬁcation also helps to numerically stabilize the
algorithms when the kernel matrix is singular, or nearly so. Ghavamzadeh
et al. [2013] show how one can incrementally perform such sparsiﬁcation in
their Bayesian gradient evaluation algorithms, i.e., how to selectively add a
new observed path to a set of dictionary paths used as a basis for representing
or approximating the full solution.
Policy Improvement:
Ghavamzadeh and Engel [2006] also show how their
Bayesian algorithms for estimating the gradient can be used to deﬁne a
Bayesian policy gradient algorithm. The algorithm starts with an initial set
of policy parameters θ0, and at each iteration j, updates the parameters in
the direction of the posterior mean of the gradient of the expected return
E

∇ηB(θj)|DM

estimated by their Bayesian gradient evaluation algorithms.
This is repeated N times, or alternatively, until the gradient estimate is suﬃ-
ciently close to zero. Since the Fisher information matrix, G, and the poste-
rior distribution (both mean and covariance) of the gradient of the expected

5.2. Bayesian Policy Gradient
443
return are estimated at each iteration of this algorithm, we may make the
following modiﬁcations in the resulting Bayesian policy gradient algorithm at
little extra cost:
• Update
the
policy
parameters
in
the
direction
of
the
natural
gradient, G(θ)−1E

∇ηB(θ)|DM

, instead of the regular gradient,
E

∇ηB(θ)|DM

.
• Use the posterior covariance of the gradient of the expected return
as a measure of the uncertainty in the gradient estimate, and thus,
as a measure to tune the step-size parameter in the gradient update
(the larger the posterior variance the smaller the step-size) (see the
experiments in Ghavamzadeh et al. [2013] for more details). In a similar
approach, Vien et al. [2011] used BQ to estimate the Hessian matrix
distribution and then used its mean as learning rate schedule to improve
the performance of BPG. They empirically showed that their method
performs better than BPG and BPG with natural gradient in terms of
convergence speed.
It is important to note that similar to the gradient estimated by the
GPOMDP algorithm of Baxter and Bartlett [2001], the gradient estimated by
these algorithms, E

∇ηB(θ)|DM

, can be used with the conjugate-gradient
and line-search methods of Baxter et al. [2001] for improved use of gradient
information. This allows us to exploit the information contained in the gradi-
ent estimate more aggressively than by simply adjusting the parameters by a
small amount in the direction of E

∇ηB(θ)|DM

.
The experiments reported in Ghavamzadeh et al. [2013] indicate that the
BPG algorithm tends to signiﬁcantly reduce the number of samples needed
to obtain accurate gradient estimates. Thus, given a ﬁxed number of samples
per iteration, ﬁnds a better policy than MC-based policy gradient methods.
These results are in line with previous work on BQ, for example a work by
Rasmussen and Ghahramani [2003] that demonstrates how BQ, when applied
to the evaluation of an expectation, can outperform MC estimation by orders
of magnitude in terms of the mean-squared error.
Extension to Partially Observable Markov Decision Processes:
The
above models and algorithms can be easily extended to partially observable
problems without any changes using similar techniques as in Section 6 of
Baxter and Bartlett [2001]. This is due to the fact that the BPG framework
considers complete system trajectories as its basic observable unit, and thus,
does not require the dynamic within each trajectory to be of any special form.
This generality has the downside that it cannot take advantage of the Markov
property when the system is Markovian (see Ghavamzadeh et al. [2013] for
more details). To address this issue, Ghavamzadeh and Engel [2007] then

444
Model-free Bayesian Reinforcement Learning
extended their BPG framework to actor-critic algorithms and present a new
Bayesian take on the actor-critic architecture, which is the subject of the next
section.
5.3
Bayesian Actor-Critic
Another approach to reduce the variance of the policy gradient estimates is to
use an explicit representation for the value function of the policy. This class
of PG algorithms are called actor-critic and they were among the earliest to
be investigated in RL [Barto et al., 1983, Sutton, 1984]. Unlike in §5.2 where
we consider complete trajectories as the basic observable unit, in this section
we assume that the system is Markovian, and thus, the basic observable unit
is one step system transition (state, action, next state). It can be shown that
under certain regularity conditions [Sutton et al., 2000], the expected return
of policy µ may be written in terms of state-action pairs (instead of in terms
of trajectories as in Eq. 2.1) as
η(µ) =
Z
Z
dzπµ(z)¯r(z),
(5.28)
where z = (s, a) is a state-action pair and πµ(z) = P∞
t=0 γtP µ
t (z) is a dis-
counted weighting of state-action pairs encountered while following policy µ.
In the deﬁnition of πµ, the term P µ
t (zt) is the t-step state-action occupancy
density of policy µ given by
P µ
t (zt) =
Z
Zt dz0 . . . dzt−1P µ
0 (z0)
tY
i=1
P µ(zi|zi−1).
Integrating a out of πµ(z) = πµ(s, a) results in the corresponding discounted
weighting of states encountered by following policy µ: νµ(s) =
R
A daπµ(s, a).
Unlike νµ and πµ, (1−γ)νµ and (1−γ)πµ are distributions and are analogous
to the stationary distributions over states and state-action pairs of policy µ
in the undiscounted setting, respectively.
The policy gradient theorem ([Marbach, 1998, Proposition 1]; [Sutton
et al., 2000, Theorem 1]; [Konda and Tsitsiklis, 2000, Theorem 1]) states that
the gradient of the expected return, deﬁned by Eq. 5.28, is given by
∇η(θ) =
Z
dsda ν(s; θ)∇µ(a|s; θ)Q(s, a; θ).
(5.29)
In an AC algorithm, the actor updates the policy parameters θ along the
direction of an estimate of the gradient of the performance measure (Eq. 5.29),
while the critic helps the actor in this update by providing it with an esti-
mate of the action-value function Q(s, a; θ). In most existing AC algorithms

5.3. Bayesian Actor-Critic
445
(both conventional and natural), the actor uses Monte-Carlo (MC) techniques
to estimate the gradient of the performance measure and the critic approxi-
mates the action-value function using some form of temporal diﬀerence (TD)
learning [Sutton, 1988].
The idea of Bayesian actor-critic (BAC) [Ghavamzadeh and Engel, 2007]
is to apply the Bayesian quadrature (BQ) machinery described in §5.2.1 to the
policy gradient expression given by Eq. 5.29 in order to reduce the variance in
the gradient estimation procedure (see Figure 5.3 for the connection between
the BQ machinery and BAC).
Similar to the BPG methods described in §5.2.2, in BAC, we place a Gaus-
sian process (GP) prior over action-value functions using a prior covariance
kernel deﬁned on state-action pairs: k(z, z′) = Cov

Q(z), Q(z′)

. We then
compute the GP posterior conditioned on the sequence of individual observed
transitions. By an appropriate choice of a prior on action-value functions,
we are able to derive closed-form expressions for the posterior moments of
∇η(θ). The main questions here are: 1) how to compute the GP posterior
of the action-value function given a sequence of observed transitions? and 2)
how to choose a prior for the action-value function that allows us to derive
closed-form expressions for the posterior moments of ∇η(θ)? The Gaussian
process temporal diﬀerence (GPTD) method [Engel et al., 2005a] described
in §5.1.1 provides a machinery for computing the posterior moments of Q(z).
After t time-steps, GPTD gives us the following posterior moments for Q (see
Eqs. 5.15 and 5.16 in §5.1.1):
bQt(z) = E [Q(z)|Dt] = kt(z)⊤αt,
bSt(z, z′) = Cov [Q(z), Q(z′)|Dt] = k(z, z′) −kt(z)⊤Ctkt(z′),
where Dt denotes the observed data up to and including time step t, and
kt(z) =
 k(z0, z), . . . , k(zt, z)
⊤,
Kt =

kt(z0), kt(z1), . . . , kt(zt)

,
αt = H⊤
t

HtKtH⊤
t + Σt
−1
rt−1 ,
Ct = H⊤
t

HtKtH⊤
t + Σt
−1
Ht .
(5.30)
Using the above equations for the posterior moments of Q, making use of the
linearity of Eq. 5.29 in Q, and denoting g(z; θ) = πµ(z)∇log µ(a|s; θ), the
posterior moments of the policy gradient ∇η(θ) may be written as [O’Hagan,
1991]
E

∇η(θ)|Dt

=
Z
Z
dzg(z; θ)kt(z)⊤αt,
Cov

∇η(θ)|Dt

=
Z
Z2 dzdz′g(z; θ)
 k(z, z′) −kt(z)⊤Ctkt(z′)

g(z′; θ)⊤.

446
Model-free Bayesian Reinforcement Learning
These equations provide us with the general form of the posterior policy gra-
dient moments. We are now left with a computational issue, namely, how to
compute the integrals appearing in these expressions? We need to be able to
evaluate the following integrals:
Bt =
Z
Z
dzg(z; θ)kt(z)⊤,
B0 =
Z
Z2 dzdz′g(z; θ)k(z, z′)g(z′; θ)⊤.
(5.31)
Using the deﬁnitions of Bt and B0, the gradient posterior moments may be
written as
E

∇η(θ)|Dt

= Btαt ,
Cov

∇η(θ)|Dt

= B0 −BtCtB⊤
t .
(5.32)
In order to render these integrals analytically tractable, Ghavamzadeh and
Engel [2007] chose the prior covariance kernel to be the sum of an arbitrary
state-kernel ks and the Fisher kernel kF between state-action pairs, i.e.,
kF (z, z′) = u(z; θ)⊤G(θ)−1u(z′) ,
k(z, z′) = ks(s, s′) + kF (z, z′) ,
(5.33)
where u(z; θ) and G(θ) are respectively the score function and the Fisher
information matrix, deﬁned as6
u(z; θ) = ∇log µ(a|s; θ),
G(θ) = Es∼νµ,a∼µ

∇log µ(a|s; θ)∇log µ(a|s; θ)⊤
(5.34)
= Ez∼πµ 
u(z; θ)u(z; θ)⊤
.
Using the prior covariance kernel of Eq. 5.33, Ghavamzadeh and Engel [2007]
showed that the integrals in Eq. 5.31 can be computed as
Bt = U t ,
B0 = G,
(5.35)
where U t =

u(z0), u(z1), . . . , u(zt)

. As a result, the integrals of the gradient
posterior moments (Eq. 5.32) are analytically tractable (see Ghavamzadeh
et al. [2013] for more details). An immediate consequence of Eq. 5.35 is that,
in order to compute the posterior moments of the policy gradient, we only
need to be able to evaluate (or estimate) the score vectors u(zi), i = 0, . . . , t
and the Fisher information matrix G of the policy.
Similar to the BPG method, Ghavamzadeh and Engel [Ghavamzadeh and
Engel, 2007] suggest methods for online estimation of the Fisher information
matrix in Eq. 5.34 and for using online sparsiﬁcation to make the BAC algo-
rithm more eﬃcient in both time and memory (see Ghavamzadeh et al. [2013]
6Similar to u(ξ) and G deﬁned by Eqs. 5.19 and 5.27, to simplify the notation,
we omit the dependence of u and G to the policy parameters θ, and replace u(z; θ)
and G(θ) with u(z) and G in the sequel.

5.3. Bayesian Actor-Critic
447
for more details). They also report experimental results [Ghavamzadeh and
Engel, 2007, Ghavamzadeh et al., 2013] which indicate that the BAC algo-
rithm tends to signiﬁcantly reduce the number of samples needed to obtain
accurate gradient estimates, and thus, given a ﬁxed number of samples per
iteration, ﬁnds a better policy than both MC-based policy gradient meth-
ods and the BPG algorithm, which do not take into account the Markovian
property of the system.

6
Risk-aware Bayesian Reinforcement Learning
The results presented so far have all been concerned with optimizing the
expected return of the policy. However, in many applications, the decision-
maker is also interested in minimizing the risk of a policy. By risk,1 we mean
performance criteria that take into account not only the expected return, but
also some additional statistics of it, such as variance, Value-at-Risk (VaR),
expected shortfall (also known as conditional-value-at-risk or CVaR), etc. The
primary motivation for dealing with risk-sensitive performance criteria comes
from ﬁnance, where risk plays a dominant role in decision-making. However,
there are many other application domains where risk is important, such as
process control, resource allocation, and clinical decision-making.
In general, there are two sources that contribute to the reward uncer-
tainty in MDPs: internal uncertainty and parametric uncertainty. Internal
uncertainty reﬂects the uncertainty of the return due to the stochastic tran-
sitions and rewards, for a single and known MDP. Parametric uncertainty, on
the other hand, reﬂects the uncertainty about the unknown MDP parameters
– the transition and reward distributions. As a concrete example of this di-
chotomy consider the following two experiments. First, select a single MDP
and execute some ﬁxed policy on it several times. In each execution, the return
may be diﬀerent due to the stochastic transitions and reward. This variability
corresponds to the internal uncertainty. In the second experiment, consider
drawing several MDPs from some distribution (typically, the posterior distri-
1The term risk here should not be confused with the Bayes risk, deﬁned in Chap-
ter 3.
448

449
bution in a Bayesian setting). For each drawn MDP, execute the same policy
several times and compute the average return across the executions. The vari-
ability in the average returns across the diﬀerent MDPs corresponds to the
parametric type of uncertainty. The Bayesian setting oﬀers a framework for
dealing with parameter uncertainty in a principled manner. Therefore, work
on risk-sensitive RL in the Bayesian setting focuses on risk due to parametric
uncertainty, as we shall now survey.
Bias and Variance Approximation in Value Function Estimates
The ﬁrst result we discuss concerns policy evaluation. Mannor et al. [2007]
derive approximations to the bias and variance of the value function of a ﬁxed
policy due to parametric uncertainty.
Consider an MDP M with unknown transition probabilities P,2 a Dirich-
let prior on the transitions, and assume that we have observed n(s, a, s′)
transitions from state s to state s′ under action a. Recall that the pos-
terior transition probabilities are also Dirichlet, and may be calculated as
outlined in §2.5.1.3 Consider also a stationary Markov policy µ, and let P µ
denote the unknown transition probabilities induced by µ in the unknown
MDP M and V µ denote the corresponding value function. Recall that V µ is
the solution to the Bellman equation (2.4) and may be written explicitly as
V µ = (I −γP µ)−1 Rµ, where we recall that Rµ denotes the expected rewards
induced by µ. The Bayesian formalism allows the calculation of the posterior
mean and covariance of V µ, given the observations.
Let ˆP (s′|s, a) = Epost

P (s′|s, a)

denote the expected transition prob-
abilities with respect to their posterior distribution, and let ˆP µ (s′|s) =
P
a µ(a|s) ˆP (s′|s, a) denote the expected transitions induced by µ. We de-
note by ˆV µ = (I −γ ˆP µ)−1Rµ the estimated value function. Also, let e denote
a vector of ones. The posterior mean and covariance of V µ are given in the
following two theorems:
Theorem 6.1. [Mannor et al., 2007] The expectation (under the posterior) of
V µ satisﬁes
Epost [V µ] = ˆV µ + γ2 ˆX ˆQ ˆV µ + γ ˆB + Lbias,
where ˆX =

I −γ ˆP µ−1
, and the vector ˆB and matrix ˆQ are computed
2Following the framework of Chapter 4, we assume that the rewards are known
and only the transitions are unknown. The following results may be extended to
cases where the rewards are also unknown, as outlined in §4.7.
3Note that this is similar to the BAMDP formulation of §4.1, where the hyper-
state encodes the posterior probabilities of the transitions given the observations.

450
Risk-aware Bayesian Reinforcement Learning
according to
ˆBi =
X
a
µ(a|i)2R(i, a)e⊤ˆ
M i,a ˆX·,i,
and
ˆQi,j =
ˆ
Cov
(i)
j,· ˆX·,i
in which
ˆ
Cov
(i) =
X
a
µ(a|i)2 ˆ
M i,a,
where the matrix
ˆ
M i,a is the posterior covariance matrix of P (·|s, a), and
higher order terms
Lbias =
∞
X
k=3
γkE

fk( ˜P)

Rµ,
in which ˜P = P −ˆP, and fk( ˜P) = ˆX

˜P ˆX
k
.
Theorem 6.2. [Mannor et al., 2007] Using the same notations as Theorem 6.1,
the second moment (under the posterior) of V µ satisﬁes
Epost

V µV µ⊤
= ˆV µ ˆV µ⊤+ ˆX

γ2  ˆQ ˆV µRµ⊤+ Rµ ˆV µ⊤ˆQ⊤
+ γ
  ˆBRµ⊤+ Rµ ˆB⊤
+ ˆW

ˆX⊤+ Lvar,
where ˆW is a diagonal matrix with elements
ˆWi,i =
X
a
µ(a|i)2 γV µ⊤+ R(i, a)e
 ˆ
M i,a γV µ + R(i, a)e⊤
,
and higher order terms
Lvar =
X
k,l:k+l>2
γk+lE
h
fk( ˜P)RµRµ⊤fl( ˜P)⊤i
.
It is important to note that except for the higher order terms Lbias and
Lvar, the terms in Theorems 6.1 and 6.2 do not depend on the unknown tran-
sitions, and thus, may be calculated from the data. This results in a second-
order approximation of the bias and variance of V µ, which may be used to
derive conﬁdence intervals around the estimated value function. This approx-
imation was also used by Delage and Mannor [2010] for risk-sensitive policy
optimization, as we describe next.

451
Percentile Criterion
Consider again a setting where n(s, a, s′) transitions from state s to state s′
under action a from MDP M were observed, and let Ppost denote the posterior
distribution of the transition probabilities in M given these observations.
Delage and Mannor [2010] investigate the percentile criterion4 for M, deﬁned
as
max
y∈R,µ
y
s.t.
Ppost
 
E
" ∞
X
t=0
γtR(Zt)
 Z0 ∼P0, µ
#
≥y
!
≥1 −ϵ,
(6.1)
where Ppost denotes the probability of drawing a transition matrix P ′ from the
posterior distribution of the transitions, and the expectation is with respect
to a concrete realization of that P ′. Note that the value of the optimization
problem in (6.1) is a (1−ϵ)-guarantee to the performance of the optimal policy
with respect to the parametric uncertainty.
Unfortunately, solving (6.1) for general uncertainty in the parameters is
NP-hard [Delage and Mannor, 2010]. However, for the case of a Dirichlet
prior, the 2nd order approximation of Theorem 6.1 may be used to derive an
approximately optimal solution.
Let F(µ) denote the 2nd order approximation5 of the expected return
under policy µ and initial state distribution P0 (cf. Theorem 6.1)
F(µ) = P ⊤
0 ˆV µ + γ2P ⊤
0 ˆX ˆQ ˆV µ.
The next result of Delage and Mannor [2010] shows that given enough obser-
vations, optimizing F(µ) leads to an approximate solution of the percentile
problem (6.1).
Theorem 6.3. [Delage and Mannor, 2010] Let N ∗= mins,a
P
s′ n(s, a, s′)
denote the minimum number of state transitions observed from any state
using any action, and ϵ ∈(0, 0.5]. The policy
ˆµ = arg max
µ
F(µ)
is O
 1/
√
ϵN ∗
optimal with respect to the percentile optimization crite-
rion (6.1).
4This is similar to the popular ﬁnancial risk measure Value-at-Risk (VaR). How-
ever, note that VaR is typically used in the context of internal uncertainty.
5Note that the 1st order term is ignored, since it would cancel anyway in the
optimization that follows.

452
Risk-aware Bayesian Reinforcement Learning
Note that, as discussed earlier, F(µ) may be eﬃciently evaluated for every
µ. However, F(µ) is non-convex in µ, but empirically, global optimization
techniques for maximizing F(µ) lead to useful solutions [Delage and Mannor,
2010].
Delage and Mannor [2010] also consider a case where the state transitions
are known, but there is uncertainty in the reward distribution. For general re-
ward distributions the corresponding percentile optimization problem is also
NP-hard. However, for the case of a Gaussian prior, the resulting optimiza-
tion problem is a second-order cone program, for which eﬃcient solutions are
known.
Max-Min Criterion
Consider the percentile criterion (6.1) in the limit of ϵ →0. In this case, we are
interested in the performance under the worst realizable posterior transition
probability, i.e.,
max
µ
min
P ∈Ppost E
" ∞
X
t=0
γtR(Zt)
 Z0 ∼P0, P, µ
#
,
(6.2)
where Ppost denotes the set of all realizable transition probabilities in the
posterior. For the case of a Dirichlet prior, this criterion is useless, as the
set Ppost contains the entire simplex for each state. Bertuccelli et al. [2012]
consider instead a ﬁnite subset6 ˆPpost ∈Ppost, and minimize only over the set
ˆPpost, resulting in the following criterion:
max
µ
min
P ∈ˆ
Ppost
E
" ∞
X
t=0
γtR(Zt)
 Z0 ∼P0, P, µ
#
.
(6.3)
Given a set ˆPpost, the optimization in (6.3) may be solved eﬃciently using
min-max dynamic programming. Thus, the ‘real question’ is how to construct
ˆPpost. Naturally, the construction of ˆPpost should reﬂect the posterior dis-
tribution of transition probabilities. One approach is to construct it from a
ﬁnite number of models sampled from the posterior distribution, reminiscent
of Thompson sampling. However, as claimed in Bertuccelli et al. [2012], this
approach requires a very large number of samples in order to adequately rep-
resent the posterior.
Alternatively, Bertuccelli et al. [2012] propose a deterministic sampling
procedure for the Dirichlet distribution based on sigma-points. In simple
terms, sigma-points are points placed around the posterior mean and spaced
6This is also known as a set of scenarios.

453
proportionally to the posterior variance. The iterative algorithm of Bertuccelli
et al. [2012] consists of two phases. In the ﬁrst phase, the already observed
transitions are used to derive the posterior Dirichlet distribution, for which
an optimal policy is derived by solving (6.3). In the second phase, this policy
is then acted upon in the system to generate additional observations. As more
data become available, the posterior variance decreases and the sigma points
become closer, leading to convergence of the algorithm.
Percentile Measures Criteria
The NP-hardness of the percentile criterion for general uncertainty structures
motivates a search for more tractable risk-aware performance criteria. Chen
and Bowling [2012] propose to replace the individual percentile with a measure
over percentiles. Formally, given a measure ψ over the interval [0, 1], consider
the following optimization problem:
max
µ,y∈F
Z
x
y(x)dψ
s.t.
Ppost
 
E
" T
X
t=0
R(Zt)
 Z0 ∼P0, µ
#
≥y(x)
!
≥x
∀x ∈[0, 1],
(6.4)
where F is the class of real-valued and bounded ψ-integrable functions on the
interval [0, 1], Ppost denotes the probability of drawing a transition matrix P ′
from the posterior distribution of the transitions (as before), and the expec-
tation is with respect to a concrete realization of that P ′. Note that here the
horizon is T and there is no discounting, as opposed to the inﬁnite horizon
discounted setting discussed earlier.
The percentile measure criterion (6.4) may be seen as a generalization of
the percentile criterion (6.1), which is obtained by setting ψ to a Dirac delta
at 1 −ϵ. In addition, when ψ is uniform on [0, 1], (6.4) is equivalent to the
expected value of Theorem 6.1, and when ψ is a delta Dirac at 0, we obtain
the max-min criterion (6.2). Finally, when ψ is a step function, an expected
shortfall (CVaR) criterion is obtained.
Chen and Bowling [2012] introduce the k-of-N family of percentile mea-
sures, which admits an eﬃcient solution under a general uncertainty structure.
For a policy µ, the k-of-N measure is equivalent to the following sampling pro-
cedure: ﬁrst draw N MDPs according to the posterior distribution, then select
a set of the k MDPs with the worst expected performance under policy µ,
and ﬁnally choose a random MDP from this set (according to a uniform dis-
tribution). By selecting suitable k and N, the k-of-N measure may be tuned
to closely approximate the CVaR or max-min criterion.

454
Risk-aware Bayesian Reinforcement Learning
The main reason for using the k-of-N measure is that the above sampling
procedure may be seen as a two-player zero-sum extensive-form game with
imperfect information, which may be solved eﬃciently using counterfactual
regret minimization. This results in the following convergence guarantee:
Theorem 6.4. [Chen and Bowling, 2012] For any ϵ > 0 and δ ∈(0, 1], let
¯T =

1 + 2
√
δ
2 16T 2∆2|I1|2|A|
δ2ϵ2
,
where T∆is the maximum diﬀerence in total reward over T steps. With
probability 1 −δ, the current strategy at iteration T ∗, chosen uniformly
at random from the interval [1, ¯T], is an ϵ-approximation to a solu-
tion of (6.4) when ψ is a k-of-N measure. The total time complexity is
O

(T∆/ϵ)2 |I1|3|A|3N log N
δ3

, where |I1| ∈O (|S|T) for arbitrary reward un-
certainty and |I1| ∈O
 |S|T +1AT 
for arbitrary transition and reward uncer-
tainty.
The exponential dependence on the horizon T in Theorem 6.4 is due to
the fact that an optimal policy for the risk-sensitive criterion (6.4) is not
necessarily Markov and may depend on the complete history. In comparison,
the previous results avoided this complication by searching only in the space of
Markov policies. An interesting question is whether other choices of measure
ψ admit an eﬃcient solution. Chen and Bowling [2012] provide the following
suﬃcient condition for tractability:
Theorem 6.5. [Chen and Bowling, 2012] Let ψ be an absolutely continuous
measure with density function gψ, such that gψ is non-increasing and piece-
wise Lipschitz continuous with m pieces and Lipschitz constant L. A solution
of (6.4) can be approximated with high probability in time polynomial in

|A|, |S|, ∆, L, m, 1
ϵ , 1
δ
	
for (i) arbitrary reward uncertainty with time also
polynomial in the horizon or (ii) arbitrary transition and reward uncertainty
with a ﬁxed horizon.
Note that in line with the previous hardness results, both the CVaR and
max-min criteria may be represented using a non-increasing and piecewise
Lipschitz continuous measure, while the percentile criterion may not.

7
BRL Extensions
In this section, we discuss extensions of the Bayesian reinforcement learn-
ing (BRL) framework to the following classes of problems: PAC-Bayes model
selection, inverse RL, multi-agent RL, and multi-task RL.
7.1
PAC-Bayes Model Selection
While Bayesian RL provides a rich framework for incorporating domain knowl-
edge, one of the often mentioned limitations is the requirement to have cor-
rect priors, meaning that the prior has to admit the true posterior. Of course
this issue is pervasive across Bayesian learning methods, not just Bayesian
RL. Recent work on PAC-Bayesian analysis seeks to provide tools that are
robust to poorly selected priors. PAC-Bayesian methods provide a way to
simultaneously exploit prior knowledge when it is appropriate, while pro-
viding distribution-free guarantees based on properties such as VC dimen-
sion [McAllester, 1999].
PAC-Bayesian bounds for RL in ﬁnite state spaces were introduced by
Fard and Pineau [2010], showing that it is possible to remove the assump-
tion on the correctness of the prior, and instead, measure the consistency of
the prior over the training data. Bounds are available for both model-based
RL, where a prior distribution is given on the space of possible models, and
model-free RL, where a prior is given on the space of value functions. In both
cases, the bound depends on an empirical estimate and a measure of distance
between the stochastic policy and the one imposed by the prior distribution.
The primary use of these bounds is for model selection, where the bounds
455

456
BRL Extensions
can be useful in choosing between following the empirical estimate and the
Bayesian posterior, depending on whether the prior was informative or mis-
leading. PAC-Bayesian bounds for the case of RL with function approximation
are also available to handle problems with continuous state spaces [Fard et al.,
2011].
For the most part, PAC-Bayesian analysis to date has been primarily
theoretical, with few empirical results. However, recent theoretical results on
PAC-Bayesian analysis of the bandit case may provide useful tools for further
development of this area [Seldin et al., 2011a,b].
7.2
Bayesian Inverse Reinforcement Learning
Inverse reinforcement learning (IRL) is the problem of learning the underlying
model of the decision-making agent (expert) from its observed behavior and
the dynamics of the system [Russell, 1998]. IRL is motivated by situations in
which the goal is only to learn the reward function (as in preference elicitation)
and by problems in which the main objective is to learn good policies from the
expert (apprenticeship learning). Both reward learning (direct) and appren-
ticeship learning (indirect) views of this problem have been studied in the last
decade (e.g., Ng and Russell [2000], Abbeel and Ng [2004], Ratliﬀet al. [2006],
Neu and Szepesvári [2007], Ziebart et al. [2008], Syed and Schapire [2008]).
What is important is that the IRL problem is inherently ill-posed since there
might be an inﬁnite number of reward functions for which the expert’s policy
is optimal. One of the main diﬀerences between the various works in this area
is in how they formulate the reward preference in order to obtain a unique
reward function for the expert.
The main idea of Bayesian IRL (BIRL) is to use a prior to encode the
reward preference and to formulate the compatibility with the expert’s policy
as a likelihood in order to derive a probability distribution over the space of
reward functions, from which the expert’s reward function is somehow ex-
tracted. Ramachandran and Amir [2007] use this BIRL formulation and pro-
pose a Markov chain Monte Carlo (MCMC) algorithm to ﬁnd the posterior
mean of the reward function and return it as the reward of the expert. Michini
and How [2012b] improve the eﬃciency of the method in Ramachandran and
Amir [2007] by not including the entire state space in the BIRL inference.
They use a kernel function that quantiﬁes the similarity between states and
scales down the BIRL inference by only including those states that are similar
(the similarity is deﬁned by the kernel function) to the ones encountered by
the expert. Choi and Kim [2011] use the BIRL formulation of Ramachandran
and Amir [2007] and ﬁrst show that using the posterior mean may not be
a good idea since it may yield a reward function whose corresponding op-
timal policy is inconsistent with the expert’s behaviour; the posterior mean

7.2. Bayesian Inverse Reinforcement Learning
457
integrates the error over the entire space of reward functions by including
(possibly) inﬁnitely many rewards that induce policies that are inconsistent
with the expert’s demonstration. Instead, they suggest that the maximum-a-
posteriori (MAP) estimate could be a better solution for IRL. They formulate
IRL as a posterior optimization problem and propose a gradient method to
calculate the MAP estimate that is based on the (sub)diﬀerentiability of the
posterior distribution. Finally, they show that most of the non-Bayesian IRL
algorithms in the literature [Ng and Russell, 2000, Ratliﬀet al., 2006, Neu
and Szepesvári, 2007, Ziebart et al., 2008, Syed and Schapire, 2008] can be
cast as searching for the MAP reward function in BIRL with diﬀerent priors
and diﬀerent ways of encoding the compatibility with the expert’s policy.
Using a single reward function to explain the expert’s behavior might be
problematic as its complexity grows with the complexity of the task being
optimized by the expert. Searching for a complex reward function is often dif-
ﬁcult. If many parameters are needed to model it, we are required to search
over a large space of candidate functions. This problem becomes more severe
when we take into account the testing of each candidate reward function,
which requires solving an MDP for its optimal value function, whose compu-
tation usually scales poorly with the size of the state space. Michini and How
[2012a] suggest a potential solution to this problem in which they partition
the observations (system trajectories generated by the expert) into sets of
smaller sub-demonstrations, such that each sub-demonstration is attributed
to a smaller and less-complex class of reward functions. These simple rewards
can be intuitively interpreted as sub-goals of the expert. They propose a BIRL
algorithm that uses a Bayesian non-parametric mixture model to automate
this partitioning process. The proposed algorithm uses a Chinese restaurant
process prior over partitions so that there is no need to specify the number of
partitions a priori.
Most of the work in IRL assumes that the data is generated by a single
expert optimizing a ﬁxed reward function. However, there are many applica-
tions in which we observe multiple experts, each executing a policy that is
optimal (or good) with respect to its own reward function. There have been
a few works that consider this scenario.
Dimitrakakis and Rothkopf [2011]
generalize BIRL to multi-task learning. They assume a common prior for the
trajectories and estimate the reward functions individually for each trajec-
tory. Other than the common prior, they do not make any additional eﬀorts
to group trajectories that are likely to be generated from the same or sim-
ilar reward functions. Babes et al. [2011] propose a method that combines
expectation maximization (EM) clustering with IRL. They cluster the tra-
jectories based on the inferred reward functions, where one reward function
is deﬁned per cluster. However, their proposed method is based on the as-
sumption that the number of clusters (the number of the reward functions) is

458
BRL Extensions
known. Finally, Choi and Kim [2012] present a nonparametric Bayesian ap-
proach using the Dirichlet process mixture model in order to address the IRL
problem with multiple reward functions. They develop an eﬃcient Metropolis-
Hastings sampler that utilizes the gradient of the reward function posterior to
infer the reward functions from the behavior data. Moreover, after completing
IRL on the behavior data, their method can eﬃciently estimate the reward
function for a new trajectory by computing the mean of the reward function
posterior, given the pre-learned results.
7.3
Bayesian Multi-agent Reinforcement Learning
There is a rich literature on the use of RL methods for multi-agent systems.
More recently, the extension of BRL methods to collaborative multi-agent
systems has been proposed [Chalkiadakis and Boutilier, 2013]. The objec-
tive in this case is consistent with the standard BRL objective, namely to
optimally balance the cost of exploring the world with the expected bene-
ﬁt of new information. However, when dealing with multi-agent systems, the
complexity of the decision problem is increased in the following way: while
single-agent BRL requires maintaining a posterior over the MDP parameters
(in the case of model-based methods) or over the value/policy (in the case
of model-free methods), in multi-agent BRL, it is also necessary to keep a
posterior over the policies of the other agents. This belief can be maintained
in a tractable manner subject to certain structural assumptions on the do-
main, for example that the strategies of the agents are independent of each
other. Alternatively, this framework can be used to control the formation of
coalitions among agents [Chalkiadakis et al., 2010]. In this case, the Bayesian
posterior can be limited to the uncertainty about the capabilities of the other
agents, and the uncertainty about the eﬀects of coalition actions among the
agents. The solution to the Bayes-optimal strategy can be approximated using
a number of techniques, including myopic or short (1-step) lookahead on the
value function, or an extension of the value-of-information criteria proposed
by Dearden et al. [1998].
7.4
Bayesian Multi-Task Reinforcement Learning
Multi-task learning (MTL) is an important learning paradigm and active area
of research in machine learning (e.g., Caruana [1997], Baxter [2000]). A com-
mon setup in MRL considers multiple related tasks for which we are interested
in improving the performance of individual learning by sharing information
across tasks. This transfer of information is particularly important when we
are provided with only a limited number of data with which learn each task.
Exploiting data from related problems provides more training samples for

7.4. Bayesian Multi-Task Reinforcement Learning
459
the learner and can improve the performance of the resulting solution. More
formally, the main objective in MTL is to maximize the improvement over in-
dividual learning, averaged over multiple tasks. This should be distinguished
from transfer learning in which the goal is to learn a suitable bias for a class
of tasks in order to maximize the expected future performance.
Traditional RL algorithms typically do not directly take advantage of in-
formation coming from other similar tasks. However recent work has shown
that transfer and multi-task learning techniques can be employed in RL to
reduce the number of samples needed to achieve nearly-optimal solutions. All
approaches to multi-task RL (MTRL) assume that the tasks share similarity
in some components of the problem such as dynamics, reward structure, or
value function. While some methods explicitly assume that the shared com-
ponents are drawn from a common generative model [Wilson et al., 2007,
Mehta et al., 2008, Lazaric and Ghavamzadeh, 2010], this assumption is more
implicit in others. In Mehta et al. [2008], tasks share the same dynamics and
reward features, and only diﬀer in the weights of the reward function. The
proposed method initializes the value function for a new task using the previ-
ously learned value functions as a prior. Wilson et al. [2007] and Lazaric and
Ghavamzadeh [2010] both assume that the distribution over some components
of the tasks is drawn from a hierarchical Bayesian model (HBM). We describe
these two methods in more detail below.
Lazaric and Ghavamzadeh [2010] study the MTRL scenario in which the
learner is provided with a number of MDPs with common state and action
spaces. For any given policy, only a small number of samples can be generated
in each MDP, which may not be enough to accurately evaluate the policy. In
such a MTRL problem, it is necessary to identify classes of tasks with similar
structure and to learn them jointly. It is important to note that here a task is
a pair of MDP and policy (i.e., a Markov chain) such that all the MDPs have
the same state and action spaces. They consider a particular class of MTRL
problems in which the tasks share structure in their value functions. To allow
the value functions to share a common structure, it is assumed that they are
all sampled from a common prior. They adopt the GPTD value function model
([Engel et al., 2005a], also see §5.1.1) for each task, model the distribution
over the value functions using an HBM, and develop solutions to the following
problems: (i) joint learning of the value functions (multi-task learning), and
(ii) eﬃcient transfer of the information acquired in (i) to facilitate learning
the value function of a newly observed task (transfer learning). They ﬁrst
present an HBM for the case in which all of the value functions belong to the
same class, and derive an EM algorithm to ﬁnd MAP estimates of the value
functions and the model’s hyper-parameters. However, if the functions do not
belong to the same class, simply learning them together can be detrimental
(negative transfer). It is therefore important to have models that will generally

460
BRL Extensions
beneﬁt from related tasks and will not hurt performance when the tasks are
unrelated. This is particularly important in RL as changing the policy at
each step of policy iteration (this is true even for ﬁtted value iteration) can
change the way in which tasks are clustered together. This means that even
if we start with value functions that all belong to the same class, after one
iteration the new value functions may be clustered into several classes. To
address this issue, they introduce a Dirichlet process (DP) based HBM for
the case where the value functions belong to an undeﬁned number of classes,
and derive inference algorithms for both the multi-task and transfer learning
scenarios in this model.
The MTRL approach in Wilson et al. [2007] also uses a DP-based HBM
to model the distribution over a common structure of the tasks. In this work,
the tasks share structure in their dynamics and reward functions. The setting
is incremental, i.e., the tasks are observed as a sequence, and there is no
restriction on the number of samples generated by each task. The focus is not
on joint learning with ﬁnite number of samples, but on using the information
gained from the previous tasks to facilitate learning in a new one. In other
words, the focus in this work is on transfer and not on multi-task learning.

8
Outlook
Bayesian reinforcement learning (BRL) oﬀers a coherent probabilistic model
for reinforcement learning. It provides a principled framework to express the
classic exploration-exploitation dilemma, by keeping an explicit representa-
tion of uncertainty, and selecting actions that are optimal with respect to a
version of the problem that incorporates this uncertainty. This framework is
of course most useful for tackling problems where there is an explicit represen-
tation of prior information. Throughout this survey, we have presented several
frameworks for leveraging this information, either over the model parameters
(model-based BRL) or over the solution (model-free BRL).
In spite of its elegance, BRL has not, to date, been widely applied. While
there are a few successful applications of BRL for advertising [Graepel et al.,
2010, Tang et al., 2013] and robotics [Engel et al., 2005b, Ross et al., 2008b],
adoption of the Bayesian framework in applications is lagging behind the
comprehensive theory that BRL has to oﬀer. In the remainder of this section
we analyze the perceived limitations of BRL and oﬀer some research directions
that might circumvent these limitations.
One perceived limitation of Bayesian RL is the need to provide a prior.
While this is certainly the case for model-based BRL, for larger problems
there is always a need for some sort of regularization. A prior serves as a
mean to regularize the model selection problem. Thus, the problem of speci-
fying a prior is addressed by any RL algorithm that is supposed to work for
large-scale problems. We believe that a promising direction for future research
concerns devising priors based on observed data, as per the empirical Bayes
461

462
Outlook
approach [Efron, 2010]. A related issue is model mis-speciﬁcation and how to
quantify the performance degradation that may arise from not knowing the
model precisely.
There are also some algorithmic and theoretical challenges that we would
like to point out here. First, scaling BRL is a major issue. Being able to solve
large problems is still an elusive goal. We should mention that currently, a
principled approach for scaling up RL in general is arguably missing. Approxi-
mate value function methods [Bertsekas and Tsitsiklis, 1996] have proved suc-
cessful for solving certain large scale problems [Powell, 2011], and policy search
has been successfully applied to many robotics tasks [Kober et al., 2013]. How-
ever, there is currently no solution (neither frequentist nor Bayesian) to the
exploration-exploitation dilemma in large-scale MDPs. We hope that scaling
up BRL, in which exploration-exploitation is naturally handled, may help us
overcome this barrier. Conceptually, BRL may be easier to scale since it allows
us, in some sense, to embed domain knowledge into a problem. Second, the
BRL framework we presented assumes that the model is speciﬁed correctly.
Dealing with model misspeciﬁcation and consequently with model selection
is a thorny issue for all RL algorithms. When the state parameters are un-
known or not observed, the dynamics may stop being Markov and many RL
algorithms fail in theory and in practice. The BRL framework may oﬀer a
solution to this issue since the Bayesian approach can naturally handle model
selection and misspeciﬁcation by not committing to a single model and rather
sustaining a posterior over the possible models.
Another perceived limitation of BRL is the complexity of implementing
the majority of BRL methods. Indeed, some frequentist RL algorithms are
more elegant than their Bayesian counterparts, which may discourage some
practitioners from using BRL. This is, by no means, a general rule. For exam-
ple, Thompson sampling is one of the most elegant approaches to the multi-
armed bandit problem. Fortunately, the recent release of a software library
for Bayesian RL algorithms promises to facilitate this [Castronovo, 2015, Cas-
tronovo et al., 2015]. We believe that Thompson sampling style algorithms
(e.g., Russo and Van Roy [2014b], Gopalan and Mannor [2015]) can pave the
way to eﬃcient algorithms in terms of both sample complexity and computa-
tional complexity. Such algorithms require, essentially, solving an MDP once
in a while while taking nearly optimal exploration rate.
From the application perspective, we believe that BRL is still in its in-
fancy. In spite of some early successes, especially for contextual bandit prob-
lems [Chapelle and Li, 2011], most of the successes of large-scale real appli-
cations of RL are not in the Bayesian setting. We hope that this survey will
help facilitate the research needed to make BRL a success in practice. A con-
siderable beneﬁt of BRL is its ability to work well “out-of-the-box": you only
need to know relatively little about the uncertainty to perform well. More-

463
over, adding complicating factors such as long-term constraints or short-term
safety requirements can be easily embedded in the framework.
From the modelling perspective, deep learning is becoming increasingly
more popular as a building block in RL. It would be interesting to use prob-
abilistic deep networks, such as restricted Boltzman machines as an essential
ingredient in BRL. Probabilistic deep models can not only provide a pow-
erful value function or policy approximator, but also allow using historical
traces that come from a diﬀerent problem (as in transfer learning). We believe
that the combination of powerful models for value or policy approximation
in combination with a BRL approach can further facilitate better exploration
policies.
From the conceptual perspective, the main question that we see as open is
how to embed domain knowledge into the model? One approach is to modify
the prior according to domain knowledge. Another is to consider parametrized
or factored models. While these approaches may work well for particular do-
mains, they require careful construction of the state space. However, in some
cases, such a careful construction defeats the purpose of a fast and robust
“out-of-the-box" approach. We believe that approaches that use causality and
non-linear dimensionality reduction may be the key to using BRL when a
careful construction of a model is not feasible. In that way, data-driven algo-
rithms can discover simple structures that can ultimately lead to fast BRL
algorithms.

Acknowledgements
The authors extend their warmest thanks to Michael Littman, James Finlay,
Melanie Lyman-Abramovitch and the anonymous reviewers for their insights
and support throughout the preparation of this manuscript. The authors wish
to also thank several colleagues for helpful discussions on this topic: Doina
Precup, Amir-massoud Farahmand, Brahim Chaib-draa, and Pascal Poupart.
The review of the model-based Bayesian RL approaches beneﬁted signiﬁcantly
from comprehensive reading lists posted online by John Asmuth and Chris
Mansley. Funding for Joelle Pineau was provided by the Natural Sciences
and Engineering Research Council Canada (NSERC) through their Discovery
grants program, by the Fonds de Québécois de Recherche Nature et Technolo-
gie (FQRNT) through their Projet de recherche en équipe program. Funding
for Shie Mannor and Aviv Tamar were partially provided by the European
Community’s Seventh Framework Programme (FP7/2007-2013) under grant
agreement 306638 (SUPREL). Aviv Tamar is also partially funded by the
Viterbi Scholarship, Technion.
464

Appendices

A
Index of Symbols
Here we present a list of the symbols used in this paper to provide a handy
reference.1
Notation
Deﬁnition
R
set of real numbers
N
set of natural numbers
E
expected value
Var
variance
Cov
covariance
KL
Kullback-Leibler divergence
H
Shannon entropy
N(m, σ2)
Gaussian (normal) distribution with mean m and variance σ2
P
probability distribution
M
model
A
set of actions
K
cardinality of A
T
time horizon
a ∈A
a nominal action
1In this paper, we use upper-case and lower-case letters to refer to random vari-
ables and the values taken by random variables, respectively. We also use bold-face
letters for vectors and matrices.
466

467
Notation
Deﬁnition
Y
set of outcomes (in MAB)
y ∈Y
a nominal outcome
r(y)
reward for outcome y
P(y|a) ∈P(Y)
probability of observing outcome y after taking
action a
a∗
optimal arm
∆a
diﬀerence in expected reward between arms a and a∗
S
set of states (or contexts in a contextual MAB)
s ∈S
a nominal state (context)
PS(s) ∈P(S)
context probability (in contextual MAB)
P(y|a, s) ∈P(Y)
probability of observing outcome y after taking
action a when the context is s (in contextual MAB)
O
set of observations
o ∈O
a nominal observation
P ∈P(S)
transition probability function
P(s′|s, a) ∈P(S)
probability of being in state s′ after taking action
a in state s
Ω(o|s, a) ∈P(O)
probability of seeing observation o after taking
action a to reach state s
q ∈P(R)
probability distribution over reward
R(s, a) ∼q(·|s, a)
random variable of the reward of taking action
a in state s
r(s, a)
reward of taking action a in state s
¯r(s, a)
expected reward of taking action a in state s
Rmax
maximum random immediate reward
¯Rmax
maximum expected immediate reward
P0 ∈P(S)
initial state distribution
bt ∈P(S)
POMDP’s state distribution at time t
τ(bt, a, o)
the information state (belief) update equation
µ
a (stationary and Markov) policy
µ(a|s)
probability that policy µ selects action a in state s
µ∗
an optimal policy
Mµ
the Markov chain induced by policy µ
P µ
probability distribution of the Markov chain induced
by policy µ
P µ(s′|s)
probability of being in state s′ after taking an action
according to policy µ in state s
P µ
0
initial state distribution of the Markov chain induced
by policy µ
qµ
reward distribution of the Markov chain induced by
policy µ

468
Index of Symbols
Notation
Deﬁnition
qµ(·|s)
reward distribution of the Markov
chain induced by policy µ at state s
Rµ(s) ∼qµ(·|s)
reward random variable of the Markov
chain induced by policy µ at state s
πµ ∈P(S)
stationary distribution over states of
the Markov chain induced by policy µ
Z = S × A
set of state-action pairs
z = (s, a) ∈Z
a nominal state-action pair
ξ = {z0, z1, . . . , zT }
a system path or trajectory
Ξ
set of all possible system trajectories
ρ(ξ)
(discounted) return of path ξ
¯ρ(ξ)
expected value of the (discounted)
return of path ξ
η(µ)
expected return of policy µ
Dµ(s)
random variable of the (discounted)
return of state s
Dµ(z)
random variable of the (discounted)
return of state-action pair z
V µ
value function of policy µ
Qµ
action-value function of policy µ
V ∗
optimal value function
Q∗
optimal action-value function
γ
discount factor
α
linear function over the belief simplex
in POMDPs
Γ
the set of α-functions representing the
POMDP value function
k(·, ·)
a kernel function
K
kernel matrix – covariance matrix –
[K]i,j = k(xi, xj)
k(x) =
 k(x1, x), . . . , k(xT , x)
⊤
a kernel vector of size T
I
identity matrix
DT
a set of training samples of size T
ϕi
a basis function (e.g., over states or
state-action pairs)
φ(·) =
 ϕ1(·), . . . , ϕn(·)
⊤
a feature vector of size n
Φ = [φ(x1), . . . , φ(xT )]
a n × T feature matrix
θ
vector of unknown parameters
Θ
a parameter space that the unknown
parameters belong to (θ ∈Θ)

B
Discussion on GPTD Assumptions on the Noise
Process
Assumption A2
The residuals ∆V T +1 =
 ∆V (s0), . . . , ∆V (sT )
⊤can be
modeled as a Gaussian process.
This may not seem like a correct assumption in general, however, in the
absence of any prior information concerning the distribution of the residuals,
it is the simplest assumption that can be made, because the Gaussian distribu-
tion has the highest entropy among all distributions with the same covariance.
Assumption A3
Each of the residuals ∆V (st) is generated independently
of all the others, i.e., E

∆V (si)∆V (sj)

= 0, for i ̸= j.
This assumption is related to the well-known Monte-Carlo (MC) method
for value function estimation [Bertsekas and Tsitsiklis, 1996, Sutton and
Barto, 1998]. MC approach to policy evaluation reduces it into a supervised
regression problem, in which the target values for the regression are samples
of the discounted return. Suppose that the last non-terminal state in the cur-
rent episode is sT −1, then the MC training set is
n st, PT −1
i=t γi−tr(si)
oT −1
t=0 .
We may whiten the noise in the episodic GPTD model of Eqs. 5.9 and 5.10
by performing a whitening transformation with the whitening matrix H−1.
The transformed model is H−1RT = V T + N ′
T with white Gaussian noise
N ′
T = H−1N T ∼N
 0, diag(σT )

, where σT = (σ2
0, . . . , σ2
T −1)⊤. The tth
equation of this transformed model is
R(st) + γR(st+1) + . . . + γT −1−tR(sT −1) = V (st) + N ′(st),
469

470
Discussion on GPTD Assumptions on the Noise Process
where N ′(st) ∼N(0, σ2
t ). This is exactly the generative model we would use
if we wanted to learn the value function by performing GP regression using
MC samples of the discounted return as our target (see §2.5.2). Assuming a
constant noise variance σ2, in the parametric case, the posterior moments are
given by Eq. 2.22, and in the non-parametric setting, α and C, deﬁning the
posterior moments, are given by Eq. 2.18, with yT = (y0, . . . , yT −1)⊤; yt =
PT −1
i=t γi−tr(si). Note that here yT = H−1rT , where rT is a realization of
the random vector RT .
This equivalence uncovers the implicit assumption underlying MC value
estimation that the samples of the discounted return used for regression are
statistically independent. In a standard online RL scenario, this assumption is
clearly incorrect as the samples of the discounted return are based on trajec-
tories that partially overlap (e.g., for two consecutive states st and st+1, the
respective trajectories only diﬀer by a single state st). This may help explain
the frequently observed advantage of TD methods using λ < 1 over the cor-
responding MC (λ = 1) methods. The major advantage of using the GPTD
formulation is that it immediately allows us to derive exact updates for the
parameters of the posterior moments of the value function, rather than wait-
ing until the end of the episode. This point becomes more clear, when later in
this section, we use the GPTD formulation and derive online algorithms for
value function estimation.

References
Y. Abbasi-Yadkori and C. Szepesvari. Bayesian optimal control of smoothly
parameterized systems. In Proceedings of the Conference on Uncertainty
in Artiﬁcial Intelligence, 2015.
P. Abbeel and A. Ng. Apprenticeship learning via inverse reinforcement learn-
ing. In Proceedings of the 21st International Conference on Machine Learn-
ing, 2004.
S. Agrawal and N. Goyal. Analysis of Thompson sampling for the multi-armed
bandit problem. In Proceedings of the 25th Annual Conference on Learning
Theory (COLT), JMLR W&CP, volume 23, pages 39.1 – 39.26, 2012.
S. Agrawal and N. Goyal. Further optimal regret bounds for Thompson sam-
pling.
In Proceedings of the 16th International Conference on Artiﬁcial
Intelligence and Statistics, pages 99–107, 2013a.
S. Agrawal and N. Goyal. Thompson sampling for contextual bandits with
linear payoﬀs. In Proceedings of the 30th International Conference on Ma-
chine Learning (ICML-13), pages 127–135, 2013b.
M. Araya-Lopez, V. Thomas, and O. Buﬀet. Near-optimal BRL using opti-
mistic local transitions. In International Conference on Machine Learning,
2012.
J. Asmuth. Model-based Bayesian Reinforcement Learning with Generalized
Priors. PhD thesis, Rutgers, 2013.
J. Asmuth and M. Littman.
Approaching Bayes-optimality using Monte-
Carlo tree search. In International Conference on Automated Planning and
Scheduling (ICAPS), 2011.
471

472
References
J. Asmuth, L. Li, M. Littman, A. Nouri, and D. Wingate. A Bayesian sampling
approach to exploration in reinforcement learning. In Proceedings of the
Conference on Uncertainty in Artiﬁcial Intelligence, 2009.
K. Astrom. Optimal control of Markov decision processes with incomplete
state estimation. Journal of Mathematical Analysis and Applications, 10:
174–205, 1965.
C. G. Atkeson and J. C. Santamaria. A comparison of direct and model-
based reinforcement learning. In International Conference on Robotics and
Automation (ICRA), 1997.
A. Atrash and J. Pineau. A Bayesian reinforcement learning approach for
customizing human-robot interfaces. In International Conference on Intel-
ligent User Interfaces, 2009.
P. Auer, N. Cesa-Bianchi, and P. Fischer. Finite-time analysis of the multi-
armed bandit problem. Machine Learning, 47(2-3):235–256, 2002.
M. Babes, V. Marivate, K. Subramanian, and M. Littman. Apprenticeship
learning about multiple intentions. In Proceedings of the 28th International
Conference on Machine Learning, pages 897–904, 2011.
A. Barto, R. Sutton, and C. Anderson. Neuron-like elements that can solve
diﬃcult learning control problems. IEEE Transaction on Systems, Man
and Cybernetics, 13:835–846, 1983.
J. Baxter. A model of inductive bias learning. Journal of Artiﬁcial Intelligence
Research, 12:149–198, 2000.
J. Baxter and P. Bartlett. Inﬁnite-horizon policy-gradient estimation. Journal
of Artiﬁcial Intelligence Research, 15:319–350, 2001.
J. Baxter, A. Tridgell, and L. Weaver.
Knightcap: A chess program that
learns by combining TD(λ) with game-tree search. In Proceedings of the
15th International Conference on Machine Learning, pages 28–36, 1998.
J. Baxter, P. Bartlett, and L. Weaver.
Experiments with inﬁnite-horizon
policy-gradient estimation. Journal of Artiﬁcial Intelligence Research, 15:
351–381, 2001.
R. Bellman. Dynamic Programming. Princeton Universty Press, 1957.
D. Bertsekas and J. Tsitsiklis. Neuro-Dynamic Programming. Athena Scien-
tiﬁc, 1996.
L. Bertuccelli, A. Wu, and J. How. Robust adaptive Markov decision pro-
cesses: Planning with model uncertainty. Control Systems, IEEE, 32(5):
96–109, Oct 2012.

References
473
S. Bhatnagar, R. Sutton, M. Ghavamzadeh, and M. Lee. Incremental natural
actor-Critic algorithms. In Proceedings of the Advances in Neural Informa-
tion Processing Systems, pages 105–112, 2007.
S. Bhatnagar, R. Sutton, M. Ghavamzadeh, and M. Lee. Natural actor-critic
algorithms. Automatica, 45(11):2471–2482, 2009.
J. Boyan. Least-squares temporal diﬀerence learning. In Proceedings of the
16th International Conference on Machine Learning, pages 49–56, 1999.
S. Bradtke and A. Barto. Linear least-squares algorithms for temporal diﬀer-
ence learning. Journal of Machine Learning, 22:33–57, 1996.
R. Brafman and M. Tennenholtz. R-max - a general polynomial time algo-
rithm for near-optimal reinforcement learning. Journal of Machine Learn-
ing Research (JMLR), 3:213–231, 2003.
S. Bubeck and N. Cesa-Bianchi. Regret analysis of stochastic and nonstochas-
tic multi-armed bandit problems.
Foundations and Trends in Machine
Learning, 5(1):1–122, 2012. ISSN 1935-8237.
R. Caruana. Multitask learning. Machine Learning, 28(1):41–75, 1997.
P. Castro and D. Precup. Using linear programming for Bayesian exploration
in Markov decision processes. In International Joint Conference on Artiﬁ-
cial Intelligence, pages 2437–2442, 2007.
P. Castro and D. Precup.
Smarter sampling in model-based Bayesian re-
inforcement learning. In Machine Learning and Knowledge Discovery in
Databases, 2010.
M. Castronovo. Bbrl a c++ open-source library used to compare bayesian
reinforcement learning algorithms. https://github.com/mcastron/BBRL/,
2015.
M. Castronovo, D. Ernst, and R. Fonteneau A. Couetoux.
Benchmarking
for bayesian reinforcement learning.
Working paper, Inst. Monteﬁore,
http://hdl.handle.net/2268/185881, 2015.
G. Chalkiadakis and C. Boutilier. Coordination in multiagent reinforcement
learning: A Bayesian approach.
In Proceedings of the 2nd International
Joint Conference on Autonomous Agents and Multiagent Systems (AA-
MAS), 2013.
G. Chalkiadakis, E. Elkinda, E. Markakis, M. Polukarov, and N. Jennings.
Cooperative games with overlapping coalitions. Journal of Artiﬁcial Intel-
ligence Research, 39(1):179–216, 2010.

474
References
O. Chapelle and L. Li. An empirical evaluation of Thompson sampling. In Pro-
ceedings of the Advances in Neural Information Processing Systems, pages
2249–2257, 2011.
K. Chen and M. Bowling. Tractable objectives for robust policy optimization.
In Proceedings of the Advances in Neural Information Processing Systems,
pages 2078–2086, 2012.
J. Choi and K. Kim. Map inference for Bayesian inverse reinforcement learn-
ing. In Proceedings of the Advances in Neural Information Processing Sys-
tems, pages 1989–1997, 2011.
J. Choi and K. Kim. Nonparametric Bayesian inverse reinforcement learning
for multiple reward functions. In Proceedings of the Advances in Neural
Information Processing Systems, pages 305–313, 2012.
R. Crites and A. Barto. Elevator group control using multiple reinforcement
learning agents. Machine Learning, 33:235–262, 1998.
P. Dallaire, C. Besse, S. Ross, and B. Chaib-draa. Bayesian reinforcement
learning in continuous POMDPs with Gaussian processes. In IEEE/RSJ
International Conference on Intelligent Robots and Systems, 2009.
R. Dearden, N. Friedman, and S. J. Russell. Bayesian Q-learning. In AAAI
Conference on Artiﬁcial Intelligence, pages 761–768, 1998.
R. Dearden, N. Friedman, and D. Andre. Model based Bayesian exploration.
In Proceedings of the Conference on Uncertainty in Artiﬁcial Intelligence,
1999.
E. Delage and S. Mannor. Percentile optimization for Markov decision pro-
cesses with parameter uncertainty.
Operations Research, 58(1):203–213,
2010.
C. Dimitrakakis and C. Rothkopf.
Bayesian multi-task inverse reinforce-
ment learning. In Proceedings of the European Workshop on Reinforcement
Learning, 2011.
F. Doshi, J. Pineau, and N. Roy. Reinforcement learning with limited rein-
forcement: using Bayes risk for active learning in POMDPs. In International
Conference on Machine Learning, 2008.
F. Doshi-Velez.
The inﬁnite partially observable Markov decision process.
In Proceedings of the Advances in Neural Information Processing Systems,
2009.
F. Doshi-Velez, D. Wingate, N. Roy, and J. Tenenbaum.
Nonparametric
Bayesian policy priors for reinforcement learning.
In Proceedings of the
Advances in Neural Information Processing Systems, 2010.

References
475
F. Doshi-Velez, J. Pineau, and N. Roy. Reinforcement learning with limited
reinforcement: Using Bayes risk for active learning in POMDPs. Artiﬁcial
Intelligence, 2011.
M. Duﬀ. Monte-Carlo algorithms for the improvement of ﬁnite-state stochas-
tic controllers: Application to Bayes-adaptive Markov decision processes.
In International Workshop on Artiﬁcial Intelligence and Statistics (AIS-
TATS), 2001.
M. Duﬀ.
Optimal Learning: Computational Procedures for Bayes-Adaptive
Markov Decision Processes.
PhD thesis, University of Massachusetts
Amherst, Amherst, MA, 2002.
B. Efron. Large-Scale Inference: Empirical Bayes Methods fro Estimation,
Testing, and Prediction. IMS Statistics Monographs. Cambridge University
Press, 2010.
Y. Engel. Algorithms and Representations for Reinforcement Learning. PhD
thesis, The Hebrew University of Jerusalem, Israel, 2005.
Y. Engel, S. Mannor, and R. Meir. Sparse online greedy support vector regres-
sion. In Proceedings of the 13th European Conference on Machine Learning,
pages 84–96, 2002.
Y. Engel, S. Mannor, and R. Meir.
Bayes meets Bellman: The Gaussian
process approach to temporal diﬀerence learning.
In Proceedings of the
20th International Conference on Machine Learning, pages 154–161, 2003.
Y. Engel, S. Mannor, and R. Meir. Reinforcement learning with Gaussian
processes. In Proceedings of the 22nd International Conference on Machine
Learning, pages 201–208, 2005a.
Y. Engel, P. Szabó, and D. Volkinshtein. Learning to control an octopus arm
with gaussian process temporal diﬀerence methods. In Proceedings of the
Advances in Neural Information Processing Systems, 2005b.
D. Ernst, P. Geurts, and L. Wehenkel. Tree-based batch mode reinforcement
learning. Journal of Machine Learning Research, 6:503–556, 2005.
A. Farahmand, M. Ghavamzadeh, C., and Shie Mannor. Regularized policy
iteration. In Proceedings of the Advances in Neural Information Processing
Systems, pages 441–448, 2008a.
A. Farahmand, M. Ghavamzadeh, C. Szepesvári, and S. Mannor. Regularized
ﬁtted q-iteration: Application to planning. In Recent Advances in Rein-
forcement Learning, 8th European Workshop, EWRL, pages 55–68, 2008b.
M. M. Fard and J. Pineau. PAC-Bayesian model selection for reinforcement
learning. In Proceedings of the Advances in Neural Information Processing
Systems, 2010.

476
References
M. M. Fard, J. Pineau, and C. Szepesvari. PAC-Bayesian policy evaluation for
reinforcement learning. In Proceedings of the Conference on Uncertainty in
Artiﬁcial Intelligence (UAI), 2011.
A. Feldbaum. Dual control theory, parts i and ii. Automation and Remote
Control, 21:874–880 and 1033–1039, 1961.
N. M. Filatov and H. Unbehauen. Survey of adaptive dual control methods. In
IEEE Control Theory and Applications, volume 147, pages 118–128, 2000.
N. Friedman and Y. Singer. Eﬃcient Bayesian parameter estimation in large
discrete domains. In Proceedings of the Advances in Neural Information
Processing Systems, 1999.
S. Gelly, L. Kocsis, M. Schoenauer, M. Sebag, D. Silver, C. Szepesvari, and
O. Teytaud. The grand challenge of computer Go: Monte Carlo tree search
and extensions. Communications of the ACM, 55(3):106–113, 2012.
M. Ghavamzadeh and Y. Engel. Bayesian policy gradient algorithms. In Pro-
ceedings of the Advances in Neural Information Processing Systems, pages
457–464, 2006.
M. Ghavamzadeh and Y. Engel. Bayesian Actor-Critic algorithms. In Pro-
ceedings of the 24th International Conference on Machine Learning, pages
297–304, 2007.
M. Ghavamzadeh, Y. Engel, and M. Valko.
Bayesian policy gradient and
actor-critic algorithms. Technical Report 00776608, INRIA, 2013.
J. Gittins. Bandit processes and dynamic allocation indices. Journal of the
Royal Statistical Society. Series B (Methodological), pages 148–177, 1979.
P. Glynn. Likelihood ratio gradient estimation for stochastic systems. Com-
munications of the ACM, 33:75–84, 1990.
A. Gopalan and S. Mannor. Thompson sampling for learning parameterized
markov decision processes. In Proceedings of the 28th Conference on Learn-
ing Theory (COLT), pages 861–898, 2015.
A. Gopalan, S. Mannor, and Y. Mansour. Thompson sampling for complex
online problems. In Proceedings of the 31st International Conference on
Machine Learning, pages 100–108, 2014.
T. Graepel, J.Q. Candela, T. Borchert, and R. Herbrich. Web-scale Bayesian
click-through rate prediction for sponsored search advertising in Microsoft’s
Bing search engine. In Proceedings of the 27th International Conference on
Machine Learning, pages 13–20, 2010.

References
477
A. Greenﬁeld and A. Brockwell.
Adaptive control of nonlinear stochastic
systems by particle ﬁltering. In International Conference on Control and
Automation, 2003.
A. Guez, D. Silver, and P. Dayan.
Eﬃcient Bayes-adaptive reinforcement
learning using sample-based search. In Proceedings of the Advances in Neu-
ral Information Processing Systems, 2012.
S. Guha and K. Munagala.
Stochastic regret minimization via Thompson
sampling.
In Proceedings of The 27th Conference on Learning Theory,
pages 317–338, 2014.
T. Jaakkola and D. Haussler. Exploiting generative models in discriminative
classiﬁers. In Proceedings of the Advances in Neural Information Processing
Systems, 1999.
R. Jaulmes, J. Pineau, and J. Precup. Active learning in partially observable
Markov decision processes. In European Conference on Machine Learning,
2005.
L. Kaelbling, M. Littman, and A. Cassandra. Planning and acting in partially
observable stochastic domains. Artiﬁcial Intelligence, 101:99–134, 1998.
E. Kaufmann, O. Cappé, and A. Garivier.
On Bayesian upper conﬁdence
bounds for bandit problems. In International Conference on Artiﬁcial In-
telligence and Statistics, pages 592–600, 2012a.
E. Kaufmann, N. Korda, and R. Munos. Thompson sampling: An asymptoti-
cally optimal ﬁnite-time analysis. In Algorithmic Learning Theory, volume
7568 of Lecture Notes in Computer Science, pages 199–213, 2012b.
K. Kawaguchi and M. Araya-Lopez. A greedy approximation of Bayesian rein-
forcement learning with probably optimistic transition model. In Adaptive
Learning Agents 2013 (a workshop of AAAMAS), 2013.
M. Kearns and S. Singh. Near-optimal reinforcement learning in polynomial
time. In International Conference on Machine Learning, pages 260–268,
1998.
M. Kearns, Y. Mansour, and A. Ng. A sparse sampling algorithm for near-
optimal planning in large Markov decision processes. In International Joint
Conference on Artiﬁcial Intelligence, pages 1324–1331, 1999.
J. Kober, D. Bagnell, and J. Peters. Reinforcement learning in robotics: A
survey. International Journal of Robotics Research (IJRR), 2013.
L. Kocsis and C. Szepesvari. Bandit based Monte-Carlo planning. In Proceed-
ings of the European Conference on Machine Learning (ECML), 2006.

478
References
N. Kohl and P. Stone.
Policy gradient reinforcement learning for fast
quadrupedal locomotion. In Proceedings of the IEEE International Con-
ference on Robotics and Automation, pages 2619–2624, 2004.
J. Kolter and A. Ng.
Near-Bayesian exploration in polynomial time.
In
International Conference on Machine Learning, 2009.
V. Konda and J. Tsitsiklis. Actor-Critic algorithms. In Proceedings of the Ad-
vances in Neural Information Processing Systems, pages 1008–1014, 2000.
M. Lagoudakis and R. Parr. Least-squares policy iteration. Journal of Ma-
chine Learning Research, 4:1107–1149, 2003.
T. Lai and H. Robbins. Asymptotically eﬃcient adaptive allocation rules.
Advances in Applied Mathematics, 6(1):4–22, 1985.
A. Lazaric and M. Ghavamzadeh. Bayesian multi-task reinforcement learning.
In Proceedings of the 27th International Conference on Machine Learning,
pages 599–606, 2010.
L. Li. A unifying framework for computational reinforcement learning theory.
PhD thesis, Rutgers, 2009.
C. Liu and L. Li. On the prior sensitivity of Thompson sampling. CoRR,
abs/1506.03378, 2015.
S. Mannor and J. Tsitsiklis.
The sample complexity of exploration in the
multi-armed bandit problem. The Journal of Machine Learning Research,
5:623–648, 2004.
S. Mannor, R. Rubinstein, and Y. Gat. The cross entropy method for fast
policy search. In International Conference on Machine Learning, 2003.
S. Mannor, D. Simester, P. Sun, and J.N. Tsitsiklis. Bias and variance approx-
imation in value function estimates. Management Science, 53(2):308–322,
2007.
P. Marbach. Simulated-Based Methods for Markov Decision Processes. PhD
thesis, Massachusetts Institute of Technology, 1998.
D. McAllester. Some PAC-Bayesian theorems. Machine Learning, 37, 1999.
N. Mehta, S. Natarajan, P. Tadepalli, and A. Fern.
Transfer in variable-
reward hierarchical reinforcement learning. Machine Learning, 73(3):289–
312, 2008.
B. Michini and J. How. Bayesian nonparametric inverse reinforcement learn-
ing.
In Proceedings of the European Conference on Machine Learning,
2012a.

References
479
B. Michini and J. How. Improving the eﬃciency of Bayesian inverse rein-
forcement learning.
In IEEE International Conference on Robotics and
Automation, pages 3651–3656, 2012b.
A. Moore and C. Atkeson. Prioritized sweeping: Reinforcement learning with
less data and less real time. Machine Learning, 13:103–130, 1993.
A. Neu and C. Szepesvári. Apprenticeship learning using inverse reinforce-
ment learning and gradient methods. In Proceedings of the Conference on
Uncertainty in Artiﬁcial Intelligence, 2007.
A. Ng and S. Russell. Algorithms for inverse reinforcement learning. In Pro-
ceedings of the 17th International Conference on Machine Learning, pages
663–670, 2000.
A. Ng, H. Kim, M. Jordan, and S. Sastry. Autonomous helicopter ﬂight via re-
inforcement learning. In Proceedings of the Advances in Neural Information
Processing Systems. MIT Press, 2004.
A. Nilim and L. El Ghaoui. Robust control of markov decision processes with
uncertain transition matrices. Operations Research, 53(5):780–798, 2005.
J. Niño-Mora. Computing a classic index for ﬁnite-horizon bandits. INFORMS
Journal on Computing, 23(2):254–267, 2011.
A. O’Hagan. Bayes-Hermite quadrature. Journal of Statistical Planning and
Inference, 29:245–260, 1991.
I. Osband, D. Russo, and B. Van Roy. (More) eﬃcient reinforcement learning
via posterior sampling. In Proceedings of the Advances in Neural Informa-
tion Processing Systems (NIPS), 2013.
S. Paquet, L. Tobin, and B. Chaib-draa. An online POMDP algorithm for
complex multiagent environments. In International Joint Conference on
Autonomous Agents and Multi Agent Systems (AAMAS), pages 970–977,
2005.
J. Peters and S. Schaal. Natural actor-critic. Neurocomputing, 71(7-9):1180–
1190, 2008.
J. Pineau, G. Gordon, and S. Thrun. Point-based value iteration: an anytime
algorithm for POMDPs. In International Joint Conference on Artiﬁcial
Intelligence, pages 1025–1032, 2003.
S Png. Bayesian Reinforcement Learning for POMDP-based Dialogue Sys-
tems. Master’s thesis, McGill University, 2011.
S. Png and J. Pineau. Bayesian reinforcement learning for POMDP-based
dialogue systems. In ICASSP, 2011.
H. Poincaré. Calcul des Probabilités. Georges Carré, Paris, 1896.

480
References
J. Porta, N. Vlassis, M. Spaan, and P. Poupart. Point-based value iteration
for continuous POMDPs. Journal of Machine Learning Research, 7, 2006.
P. Poupart, N. Vlassis, J. Hoey, and K. Regan. An analytic solution to discrete
Bayesian reinforcement learning. In International Conference on Machine
learning, pages 697–704, 2006.
W. B. Powell.
Approximate Dynamic Programming: Solving the curses of
dimensionality (2nd Edition). John Wiley & Sons, 2011.
M. Puterman. Markov Decision Processes. Wiley Interscience, 1994.
R. Munos R. Fonteneau, L. Busoniu. Optimistic planning for belief-augmented
Markov Decision Processes.
In IEEE Symposium on Adaptive Dynamic
Programming and Reinforcement Learning (ADPRL), 2013.
D. Ramachandran and E. Amir. Bayesian inverse reinforcement learning. In
Proceedings of the 20th International Joint Conference on Artiﬁcial Intel-
ligence, pages 2586–2591, 2007.
C. Rasmussen and Z. Ghahramani. Bayesian Monte Carlo. In Proceedings
of the Advances in Neural Information Processing Systems, pages 489–496.
MIT Press, 2003.
C. Rasmussen and M. Kuss. Gaussian processes in reinforcement learning.
In Proceedings of the Advances in Neural Information Processing Systems.
MIT Press, 2004.
C. Rasmussen and C. Williams. Gaussian Processes for Machine Learning.
MIT Press, 2006.
N. Ratliﬀ, A. Bagnell, and M. Zinkevich.
Maximum margin planning.
In
Proceedings of the 23rd International Conference on Machine Learning,
2006.
R. Ravikanth, S. Meyn, and L. Brown. Bayesian adaptive control of time
varying systems. In IEEE Conference on Decision and Control, 1992.
J. Reisinger, P. Stone, and R. Miikkulainen.
Online kernel selection for
Bayesian reinforcement learning. In Proceedings of the 25th International
Conference on Machine Learning, pages 816–823, 2008.
S. Ross and J. Pineau. Model-based Bayesian reinforcement learning in large
structured domains. In Proceedings of the Conference on Uncertainty in
Artiﬁcial Intelligence, 2008.
S. Ross, B. Chaib-draa, and J. Pineau. Bayes-adaptive POMDPs. In Proceed-
ings of the Advances in Neural Information Processing Systems, volume 20,
pages 1225–1232, 2008a.

References
481
S. Ross, B. Chaib-draa, and J. Pineau. Bayesian reinforcement learning in
continuous POMDPs with application to robot navigation. In IEEE Inter-
national Conference on Robotics and Automation, 2008b.
S. Ross, J. Pineau, S. Paquet, and B. Chaib-draa. Online POMDPs. Journal
of Artiﬁcial Intelligence Research (JAIR), 32:663–704, 2008c.
S. Ross, J. Pineau, B. Chaib-draa, and P. Kreitmann. A Bayesian approach
for learning and planning in partially observable Markov decision processes.
Journal of Machine Learning Research, 12, 2011.
G. Rummery and M. Niranjan. On-line Q-learning using connectionist sys-
tems. Technical Report CUED/F-INFENG/TR 166, Engineering Depart-
ment, Cambridge University, 1994.
P. Rusmevichientong and J. N. Tsitsiklis. Linearly parameterized bandits.
Mathematics of Operations Research, 35(2):395–411, 2010.
I. Rusnak. Optimal adaptive control of uncertain stochastic discrete linear
systems. In IEEE International Conference on Systems, Man and Cyber-
netics, 1995.
S. Russell. Learning agents for uncertain environments (extended abstract).
In Proceedings of the 11th Annual Conference on Computational Learning
Theory, pages 101–103, 1998.
S. Russell and P. Norvig. Artiﬁcial Intelligence: A Modern Approach (2nd
Edition). Prentice Hall, 2002.
D. Russo and B. Van Roy. An information-theoretic analysis of Thompson
sampling. CoRR, abs/1403.5341, 2014a.
D. Russo and B. Van Roy.
Learning to optimize via posterior sampling.
Mathematics of Operations Research, 39(4):1221–1243, 2014b.
L. Scharf. Statistical Signal Processing. Addison-Wesley, 1991.
B. Schölkopf and A. Smola. Learning with Kernels. MIT Press, 2002.
S. Scott. A modern Bayesian look at the multi-armed bandit. Applied Stochas-
tic Models in Business and Industry, 26(6):639–658, 2010.
Y. Seldin, P. Auer, F. Laviolette, J. Shawe-Taylor, and R. Ortner.
PAC-
Bayesian analysis of contextual bandits. In Proceedings of the Advances in
Neural Information Processing Systems, 2011a.
Y. Seldin, N. Cesa-Bianchi, F. Laviolette, P. Auer, J. Shawe-Taylor, and J. Pe-
ters. PAC-Bayesian analysis of the exploration-exploitation trade-oﬀ. In
ICML Workshop on online trading of exploration and exploitation, 2011b.

482
References
J. Shawe-Taylor and N. Cristianini. Kernel Methods for Pattern Analysis.
Cambridge University Press, 2004.
R. Smallwood and E. Sondik. The optimal control of partially observable
Markov processes over a ﬁnite horizon. Operations Research, 21(5):1071–
1088, Sep/Oct 1973.
V. Sorg, S. Sing, and R. Lewis.
Variance-based rewards for approximate
Bayesian reinforcement learning. In Proceedings of the Conference on Un-
certainty in Artiﬁcial Intelligence, 2010.
M. Spaan and N. Vlassis. Perseus: randomized point-based value iteration for
POMDPs. Journal of Artiﬁcial Intelligence Research (JAIR), 24:195–220,
2005.
A. Strehl and M. Littman.
A theoretical analysis of model-based interval
estimation. In International Conference on Machine learning, pages 856–
863, 2005.
A. Strehl and M. Littman. An analysis of model-based interval estimation for
Markov decision processes. Journal of Computer and System Sciences, 74:
1209–1331, 2008.
M. Strens. A Bayesian framework for reinforcement learning. In International
Conference on Machine Learning, 2000.
R. Sutton. Temporal credit assignment in reinforcement learning. PhD thesis,
University of Massachusetts Amherst, 1984.
R. Sutton. Learning to predict by the methods of temporal diﬀerences. Ma-
chine Learning, 3:9–44, 1988.
R. Sutton.
DYNA, an integrated architecture for learning, planning, and
reacting. SIGART Bulletin, 2:160–163, 1991.
R. Sutton and A. Barto. An Introduction to Reinforcement Learning. MIT
Press, 1998.
R. Sutton, D. McAllester, S. Singh, and Y. Mansour. Policy gradient methods
for reinforcement learning with function approximation. In Proceedings of
the Advances in Neural Information Processing Systems, pages 1057–1063,
2000.
U. Syed and R. Schapire. A game-theoretic approach to apprenticeship learn-
ing. In Proceedings of the Advances in Neural Information Processing Sys-
tems, pages 1449–1456, 2008.

References
483
L. Tang, R. Rosales, A. Singh, and D. Agarwal. Automatic ad format selec-
tion via contextual bandits. In Proceedings of the 22nd ACM international
conference on Conference on information & knowledge management, pages
1587–1594. ACM, 2013.
G. Tesauro.
TD-Gammon, a self-teaching backgammon program, achieves
master-level play. Neural Computation, 6:215–219, 1994.
W. Thompson. On the likelihood that one unknown probability exceeds an-
other in view of the evidence of two samples. Biometrika, pages 285–294,
1933.
J. N. Tsitsiklis. A short proof of the gittins index theorem. The Annals of
Applied Probability, pages 194–199, 1994.
N. Vien, H. Yu, and T. Chung. Hessian matrix distribution for Bayesian policy
gradient reinforcement learning. Information Sciences, 181(9):1671–1685,
2011.
T. Walsh, S. Goschin, and M. Littman. Integrating sample-based planning and
model-based reinforcement learning. In Association for the Advancement
of Artiﬁcial Intelligence, 2010.
T. Wang, D. Lizotte, M. Bowling, and D. Schuurmans. Bayesian sparse sam-
pling for on-line reward optimization. In International Conference on Ma-
chine learning, pages 956–963, 2005.
C. Watkins.
Learning from Delayed Rewards.
PhD thesis, Kings College,
Cambridge, England, 1989.
R. Williams. Simple statistical gradient-following algorithms for connectionist
reinforcement learning. Machine Learning, 8:229–256, 1992.
A. Wilson, A. Fern, S. Ray, and P. Tadepalli. Multi-task reinforcement learn-
ing: A hierarchical Bayesian approach. In Proceedings of the International
Conference on Machine Learning, pages 1015–1022, 2007.
B. Wittenmark. Adaptive dual control methods: An overview. In 5th IFAC
symposium on Adaptive Systems in Control and Signal Processing, 1995.
O. Zane. Discrete-time Bayesian adaptive control problems with complete
information. In IEEE Conference on Decision and Control, 1992.
B. Ziebart, A. Maas, A. Bagnell, and A. Dey.
Maximum entropy inverse
reinforcement learning. In Proceedings of the 23rd National Conference on
Artiﬁcial Intelligence - Volume 3, pages 1433–1438, 2008.

