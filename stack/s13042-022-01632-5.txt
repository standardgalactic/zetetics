Vol.:(0123456789)
1 3
International Journal of Machine Learning and Cybernetics (2022) 13:3909–3925 
https://doi.org/10.1007/s13042-022-01632-5
ORIGINAL ARTICLE
A novel multi‑scale and sparsity auto‑encoder for classification
Huiling Wang1 · Jun Sun2   · Xiaofeng Gu2 · Wei Song2
Received: 23 February 2022 / Accepted: 17 August 2022 / Published online: 17 September 2022 
© The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature 2022
Abstract
The inspiration for generating the multi-scale feature representation originates from the basic observation that multi-scale is 
closely related to human visual physiological characteristics. Also, since the increase of hidden layer neurons and the amount of 
data leads to the rise of redundant information, a large amount of calculation makes a model more complex. This paper proposes 
a novel learning method, namely, multi-scale feature consistency regularization and ­L21-norm minimization sparse auto-encoder 
(LR21-MSAE). The multi-scale feature consistency regularization can achieve the latent representations and the visual details 
while retaining multi-scale information. This method ensures that LR21-MSAE can get valid information for better classification 
accuracy. By implementing the ­L21-norm minimization constraint, the LR21-MSAE can adaptively eliminate the potential noise and 
redundant neurons by enforcing some rows and columns of the weight matrix to be reduced to zero. It can reduce the complexity of 
the learning model and promote learning sparsity features to generate a compact network. Moreover, introducing the Wasserstein 
distance in the sparse auto-encoder to measure the difference between the two distributions allows for a more stable training process 
and faster convergence. To complete the test of the LR21-MSAE model, we choose to conduct the experiments on some publicly 
available datasets MNIST, Fashion-MNIST, CIFAR-10, USPS, ISOLET, Pendigits, and Ecoli. We demonstrate the advantages of 
LR21-MSAE, through the experimental results, compared with state-of-the-art feature extraction methods.
Keywords  Feature representation · Auto-encoder · Multi-scale feature · L21-norm · Classification
1  Introduction
Deep learning has received increasing attention due to its 
ability to learn good features in data automatically. It has 
been widely used in all the fields related to feature extrac-
tion, including speech recognition [1, 2], image recognition 
[3, 4], and natural language processing [5, 6]. In supervised 
deep learning, a vast amount of labeled data is required since 
thousands of network parameters need to be learned during 
the training process. However, labeling the data is gener-
ally an extremely time-consuming and labor-intensive work, 
particularly when the amount of data to be labeled is tre-
mendous. Therefore, it is natural that the researchers turn to 
unsupervised learning to handle unlabeled data. Among the 
unsupervised learning methods, the auto-encoder (AE) [7] is 
one of the most widely used models since it has shown obvi-
ous advantages in feature extraction from unlabeled data.
Generally, the purpose of an AE is to reconstruct the 
input data at the output layer. The input data is mapped 
to the hidden layer to capture the latent feature and then 
reconstruct the input data from the hidden feature. How-
ever, the traditional AE has the shortcomings of overfitting 
and feature redundancy, which results in poor classification 
performance. To deal with these issues, many scholars have 
proposed some constraint enforcement models of AEs in 
recent years [8, 9]. Vincent et al. introduced the denoising 
auto-encoder (DAE) [10–12], which learns more robust 
features by locally damaging the input vectors. The sparse 
auto-encoder (SAE) [13–15], proposed by Vincent et al. 
in 2011, achieves a sparse effect and extracts the valuable 
features by suppressing the hidden layer unit from being 
activated. Rifai et al. designed the contractive auto-encoder 
(CAE) [16–18] by using the Frobenius norm of the Jacobian 
matrix of the input data to make the learned features locally 
invariant. In 2014, Makhzani et al. invented the k-sparse 
auto-encoder [19], in which only the highest k activations 
values are reserved in the hidden layer. This method can 
 *	 Jun Sun 
	
sunjun_wx@hotmail.com
1	
School of IoT Engineering, Jiangnan University, 
Wuxi 214122, China
2	
School of Artificial Intelligence and Computer Science, 
Jiangnan University, Wuxi 214122, China

3910
	
International Journal of Machine Learning and Cybernetics (2022) 13:3909–3925
1 3
accelerate the training process and obtain a more accurate 
sparsity. Recently, Liu et al. proposed a large margin auto-
encoder [20], which has enhanced the discriminability of the 
features since samples of different categories are enforced to 
have long distances in hidden feature space.
All the variants mentioned above learn the feature represen-
tation without the scale-space representation, and the complex-
ity of the model is not taken into consideration. Practically, if 
you want to know the overall outline of the observed object 
and see its local information clearly, you must choose different 
distances to view the object. Such multi-scale features of an 
object are quite common in nature and feature learning. Multi-
scale algorithms have achieved a large number of practical 
applications in computer vision in recent years, such as breast 
cancer diagnosis [21], video prediction [22], depth map predic-
tion [23], and image generation [24]. It can make the learning 
system have the characteristic of being robust to the possible 
unknown scale changes by learning multi-scale feature repre-
sentations. Therefore, we can add multi-scale information to 
feature representation learning. Besides, the number of weight 
parameters to be learned grows exponentially with the increase 
of the network neurons’ number and data amount. Too many 
weight parameters in the network will bring enormous compu-
tational complexity and unnecessary storage overhead, so we 
should pay more attention to the sparsity and stability of the 
auto-encoder model.
This paper presents a multi-scale and sparsity auto-
encoder with multi-scale feature consistency regularization 
and ­L21-norm minimization (LR21-MSAE) for unsupervised 
representation learning. LR21-MSAE considers the multi-
scale representation of feature information and the sparsity 
of the auto-encoder, which is different from the traditional 
auto-encoder. First, we input the Laplacian pyramid [25–27] 
as multi-level data into a multi-path auto-encoder network to 
learn latent representations for the Laplacian pyramid and add 
the correlation between the original image-generated features 
and the multi-scale empirical model features as a constraint 
term to learn multi-scale features. Then, LR21-MSAE adds 
­L21-norm [28–31] penalty on the weights and nodes and uses 
a sparsity constraint with Wasserstein distance to discover the 
most relevant features with a stable training. The contribution 
of this work are summarized as follows:
The LR21-MSAE neural network can discover compre-
hensive hierarchical feature representation by adding multi-
scale visual information compared with the basis auto-
encoder. As a result, LR21-MSAE can retain multi-scale 
details and enhance the ability to represent scale informa-
tion, making the training process more stable and effective.
The LR21-MSAE can decompose the characteristic 
details into different scale layers, such as the edge and tex-
ture of the classification images. It can capture the distribu-
tion characteristics of multiple models and improve clas-
sification accuracy with a stable process.
The LR21-MSAE promotes a high-level structured sparsity 
while keeping good classification performance. It can simplify 
the model and get a better generalization ability. Therefore, LR21-
MSAE can realize a considerable trade-off between execution 
efficiency and classification accuracy. At the same time, LR21-
MSAE can discover the most relevant features and produce a more 
compact, more robust, simpler, and more discriminative network.
To evaluate the effectiveness of LR21-MSAE, seven 
benchmark datasets, including MNIST [32], Fashion-
MNIST [33], CIFAR-10 [34], USPS [35], ISOLET [36], 
Pendigits [37], and Ecoli [38], were extensively tested in 
our experiments. LR21-MSAE was then compared with AE, 
SAE, VAE, VLSAE, LR21-SAE, and MSAE to show its 
obvious achievements in improving classification accuracy.
The rest of the paper is organized as follows. In Sect. 2, 
the related work is briefly reviewed, including AE, SAE, and 
Laplacian pyramid. Then, a multi-scale and sparsity auto-
encoder is proposed in Sect. 3. In Sect. 4, the experimental 
results are shown to validate the effectiveness of our pro-
posed method. Finally, some concluding remarks are given 
in the last section, along with possible future work.
2  Related work
2.1  Auto‑encoder (AE)
The basic auto-encoder is composed of a three-layer 
neural network of the input layer, hidden layer, and out-
put layer [39]. In the encoding stage, the input vector 
X = {xi
}n
i=1 ∈Rd is mapped to the hidden representation 
Y = {yi
}n
i=1 ∈Rm using an encoding function f휃 , and then 
in the decoding stage, the hidden layer representation Y 
is mapped to the output vector Z = {zi
}n
i=1 ∈Rd , which 
reconstructs the input data using a decoding function g휃 . 
The general procedure of an AE is shown in Fig. 1. The 
ideal state is to make the input data the same as the output 
data. Therefore, AE is an algorithm that makes the output 
data the same as the input data as much as possible [40]. 
Then the weights parameter w and the bias parameter b of 
the network can be updated by minimizing the reconstruc-
tion error L(x, z) , and L(x, z) = ∑n
i=1 xi −zi
2 denotes the 
reconstruction error between the input matrix and output 
Encoder
Decoder
Error
Input
Resconstruction
Feature code
Fig. 1   The auto-encoder architecture

3911
International Journal of Machine Learning and Cybernetics (2022) 13:3909–3925	
1 3
matrix. Therefore, the parameters of the network can be 
optimized. The corresponding objective function can be 
represented as follows
where xi and zi are the ith dimension of the training samples 
and the output data, respectively, and n is the number of 
input samples. yi = f휃
(xi
) = s(wxi + b) is the encoder map-
ping function and zi = g휃
(yi
) = s(wyi + b) is the decoder 
mapping function. The f  and g are the activation functions 
of encoder and decoder, and common activation functions 
include sigmoid, tanh and relu. The b,b′ are bias vector, 
and w,w′ are weight matrices of the encoder and decoder, 
respectively. w′ is the transpose of w . Then, we can adjust the 
parameters 휃= {w, b, w, b} by minimizing the reconstruc-
tion error term on the gradient descent method.
2.2  Sparse auto‑encoder (SAE)
Sparse auto-encoder adds some sparsity constraints to the 
hidden layer nodes based on the traditional auto-encoder 
[13–15]. The network achieves an effect of sparsity by inhib-
iting the activation of most of the hidden layer neurons. When 
we choose sigmoid as the activation function, and the output 
value is close to 0, we think this unit is inactive. And if tanh 
is selected as the activation functxion, the output value is 
close to − 1, which means this neuron is inactive. To obtain 
the sparsity effect, the SAE uses the KL divergence (Kull-
back–Leibler divergence) to constrain the average activation 
value of the hidden layer neuron output [41] to force it close 
to a given sparsity parameter. KL divergence is an asymmetric 
measure of the distance between two random probability dis-
tributions, and it can be added to the loss function as a penalty. 
Therefore, the objective function of the SAE can be given by
where 훽 controls the sparsity penalty term, which is gener-
ally taken any value between 0 and 1. The KL divergence 
can be expressed as
where 휌 is a sparsity parameter, generally a smaller value 
near to 0, 휌∗
j = 1
n
∑n
i=1 yj
i is the mean activation probability 
of the jth hidden neurons, restricted by the sparsity penalty 
term to close to 휌 . And 
{
yj
i
}n
i=1 is the output of the hidden 
(1)
min휃JAE(휃) = min휃
n
∑
i=1
L(xi, zi
) = min휃
n
∑
i=1
L(xi, g휃
(f휃
(xi
)))
(2)
JSAE(w, b) =
n
∑
i=1
L(xi, zi
) + 훽
m
∑
j=1
KL
(
휌||휌∗
j
)
(3)
m
∑
j=1
KL
(
휌||휌∗
j
)
=
m
∑
j=1
휌log 휌
휌∗
j
+ (1 −휌) log 1 −휌
1 −휌∗
j
layer. That means the activation of hidden neurons must be 
mostly close to 0. Then we can obtain better parameters 휃 by 
the gradient descent algorithm.
2.3  Laplacian pyramid
The Laplacian pyramid has been widely used in image pro-
cessing and communication for its multi-scale and multi-
resolution characteristics. The features such as the edge and 
texture of the image can be decomposed into tower layers 
of different resolutions according to different scales. Then 
the corresponding tower layers of multiple source images 
can be merged separately to obtain the merged pyramid, and 
the fused image can be obtained again finally. Laplacian 
pyramid has been widely used in the multi-scale fusion of 
medical images of different modalities.
The first step of Laplacian pyramid coding is to perform 
Gaussian low-pass filtering and interlace down-sampling on 
the original image G0 to get the image G1 , which is known 
as a “reduced” version of G0 because in the decrease of both 
resolution and sample density. Then, in the same way, we 
form G2 as a reduced version of G1 and so on. Each level of 
the image in the Gaussian pyramid sequence is the previous 
level of image low-pass filtering and then interlaced sam-
pling. The process of averaging between levels is done by 
the function REDUCE as follows
where i and j denote the coordinates of the pixel, N is the 
total number of layers of the pyramid, Cl is the number of 
columns of the lth sub-image level of the Gaussian pyra-
mid, Rl is the number of rows of the lth sub-image level of 
the Gaussian pyramid, 휔 is a Gaussian low-pass filter and 
(s, t) is Gaussian kernel size. Thus the sequence of images 
[G0, G1, … , GN
]
 is called the Gaussian pyramid [44].
After constructing a Gaussian pyramid image sequence 
whose size is gradually halved, the Gaussian pyramid 
expanded by interpolating new node values between the 
given values. Therefore the expanded function applied to 
image Gl of the Gaussian pyramid can yield an image G∗
l that 
is the same as Gl−1 . It can be expressed as
(4)
Gl(i, j) =
2
∑
s=−2
2
∑
t=−2
휔(s, t) ∗Gl−1(2i + s, 2j + t),
1 ≤l ≤N, 0 ≤i ≤Cl, 0 ≤j ≤Rl
(5)
G∗
l (i, j) =4
2
∑
s=−2
2
∑
t=−2
휔(s, t) ∗Gl+1
(
s + i
2 , t + j
2
)
,
1 ≤l ≤N −1, 0 ≤i ≤Cl, 0 ≤j ≤Rl

3912
	
International Journal of Machine Learning and Cybernetics (2022) 13:3909–3925
1 3
where G∗
l (i, j) is the expanded image, and (s + i)∕2 , (t + j)∕2 
are integers. Then each layer image decomposed by Lapla-
cian Pyramid can be expressed as
A Laplacian Pyramid is a series of different images 
[LP0, LP1, … , LPN
]
 . The image LPk at the level k of the 
Laplacian Pyramid is calculated as the difference between 
the blurred image at the level k and the up-sampled version 
of the blurred image at the next level in the Gaussian Pyra-
mid. Laplacian Pyramid can be used to reconstruct the upper 
image from the lower image of the pyramid.
3  Multi‑scale and sparsity auto‑encoder
In this paper, a multi-scale and sparsity auto-encoder is pro-
posed. Specifically, this section can be segmented into four 
subdivisions: (1) Sparsity constraint with Wasserstein dis-
tance. (2) Multi-scale feature consistency regularization. (3) 
­L21-norm sparsity regularization. (4) The proposed LR21-
MSAE method.
3.1  Sparsity constraint with Wasserstein distance
The traditional sparse auto-encoder (SAE) uses the KL 
divergence to constrain the hidden neurons' average acti-
vation value for the purpose of forcing it close to a given 
sparsity parameter. However, the KL divergence suffers 
from gradient explosion during back-propagation because 
of its asymmetry and unbounded results. The JS divergence 
(Jensen-Shannon divergence) [42], as a variation of the KL 
divergence, can compensate for these problems very well. 
The JS divergence is prone to gradient disappearance when 
there is no overlapping part of the two probability distribu-
tions if the JS divergence is a constant and the gradient is 0.
To address the above problems with the similarity meas-
ure, we design the Wasserstein distance [43] with a gradient 
penalty as the regularization term to minimize the difference 
between the average activation distribution 휌∗
j = 1
n
∑n
i=1 yj
i 
and the given sparsity parameter distribution 휌(휌=0.1 ). The 
Wasserstein distance can avoid problems such as gradient 
disappearance and model collapse, and thus can improve the 
stability of model training. The sparsity constraint with the 
Wasserstein distance can be expressed as
(6)
LPl =
{ Gl −G∗
l
1 ≤l ≤N
LPN = GN
l = N
(7)
Jsparse = W(휌∗
j , 휌) =
inf
훾∼Π

휌∗
j ,휌
 E(p,q)∼훾

‖p −q‖

where p and q are random variables subject to the distribu-
tions 휌∗
j and 휌 respectively. Π
(
휌∗
j , 휌
)
 is a set of all the joint 
distributions, with 휌∗
j and 휌 being marginal distributions. 
‖p −q‖ represents the distance of the samples and 
E(p,q)∼훾

‖p −q‖

 is the expected value of the distance.
3.2  Multi‑scale feature consistency regularization
Laplacian pyramids are essentially multi-scale representa-
tions of images, capable of obtaining image detail at each 
scale and capturing discriminative features. Meanwhile, 
higher resolution original images can be reconstructed by 
the inverse generation of the Laplacian pyramid. Inspired 
by this, we input the Laplacian pyramid as multi-scale 
data into a multi-path auto-encoder empirical model to 
reconstruct the original image. The encoding of each layer 
is also connected to the next layer's encoding to reverse the 
generative process of the Laplacian pyramid. The gradient 
calculation for each coding network is based on the recon-
struction error of the image at this level and the next level. 
The architecture of the multi-path auto-encoder empiri-
cal model is shown in Fig. 2. This architecture allows the 
model to notice global image features including shape and 
grey-scale distribution, and local image features including 
edges and textures. It also enables the model to handle 
scale-specific visual details so that the feature representa-
tion obtained is robust to unknown scale variations [27].
With Laplacian pyramid denoted by 
[LP0, LP1, … , LPN
]
 
and the corresponding Gaussian pyramid by 
[G0, G1, … , GN
]
 , the purpose of our multi-path auto-
encoder empirical model is to learn latent representations 
for the Laplacian pyramid, which can be used to recon-
struct the corresponding Gaussian pyramid. Then, we 
represent the encoding and decoding processes by map-
ping functions fk and gk at level k , respectively. The latent 
representation hk is the output of fk . It can be expressed as
Thus, for the multi-path auto-encoder empirical model, 
we have the loss function
To enable the auto-encoder to learn multi-scale fea-
tures, we use the multi-layer auto-encoder as a multi-
scale empirical model, then add to the objective function 
the correlation between the features generated from the 
original image and those from the multi-scale empirical 
(8)
hk =
{
fk
(LPk, hk+1
),
k ≠N
fk
(LPk
),
k = N
(9)
loss1 =
N
∑
k=1
‖‖‖Gk −gk
(hk
)‖‖‖
2
2

3913
International Journal of Machine Learning and Cybernetics (2022) 13:3909–3925	
1 3
model as a constraint term, and employ matrix analysis 
and optimization methods to solve the objective function 
for designing the unsupervised multi-scale feature genera-
tion method. The architecture is displayed in Fig. 2. In this 
way, we turn an auto-encoder model into a multi-scale 
approach, which is capable of dealing with different visual 
details at appropriate scales.
The similarity between the feature y of the auto-encoder 
network and the feature e of the multi-path auto-encoder 
empirical model can be expressed as
where yi is the output of the ith hidden neuron of the auto-
encoder network, and ei is the output of the ith hidden neuron 
output of the multi-path auto-encoder empirical model. Thus 
the multi-scale feature consistency regularization term can 
be expressed as
3.3  L21‑norm sparsity regularization
It is known that an auto-encoder is designed to compress the 
input information and extract the most representative hidden 
features from the data. However, redundant information in 
(10)
loss2 = 1
2m
m
∑
i=1
‖‖yi −ei‖‖
2
2
(11)
Jms = loss1 + loss2 = 1
2N
N
∑
k=1
‖‖‖Gk −gk
(hk
)‖‖‖
2
2 + 1
2m
m
∑
i=1
‖‖yi −ei‖‖
2
2
the auto-encoder structure is easily affected by noise and 
outlier data points, making the model complex and unstable.
From the perspective of auto-encoder network structure, 
removing a neuron and deleting a feature can be seen as an 
almost equivalent problem. Suppose that the weight param-
eters connected to the neuron are close to 0. If the activation 
value of the hidden node is always approximately 0, the node 
is considered inactive and has no effects on the next layer. In 
that case, we can move this neuron from the network. Thus 
we introduce ­L21-norm regularization as
The inner ­L2-norm can promote a dense (non-zero) solu-
tion within the sparsity rows, while the outer ­L1-norm (sum 
of absolute values of matrix elements) enforces sparsity 
in the selected rows. Furthermore, unlike the traditional 
­L1-norm, the ­L21-norm tends to reduce more elements in 
the rows to 0, which means the weight parameters connected 
to useless features will be decreased to 0. Thus, the ­L21-norm 
regularization can achieve a state of row sparsity, which can 
adaptively eliminate potential noise and irrelevant neurons 
in the model, thereby reducing the inherent complexity.
3.4  LR21‑MSAE method
In this section, LR21-MSAE is proposed by integrating spar-
sity constraint with Wasserstein distance, multi-scale feature 
consistency regularization, and ­L21-norm minimization. Fig-
ure 3 shows the structure of LR21-MSAE, which consists of 
(12)
JLR21−wd=‖w‖21
Fig. 2   The architecture of 
multi-scale feature consistency 
regularization

3914
	
International Journal of Machine Learning and Cybernetics (2022) 13:3909–3925
1 3
three layers (an input layer, a hidden layer, and an output layer). 
In such a structure, our regularization terms, i.e., the average 
reconstruction error term JAE , the weight decay term JLR21−wd , 
the sparsity regularization term Jsparse and the multi-scale feature 
consistency regularization term Jms , are considered to achieve a 
better classification performance of the structure. Specifically, 
JAE can be expressed by the mean square error between the 
input matrix and the output matrix. The minimum of JAE can 
effectively ensure that the input matrix is as equal as possible 
to the output matrix and perfectly reconstruct the input matrix. 
The sparsity regularization term Jsparse constrains the hidden 
layer neurons. It achieves its sparsity and stable representation, 
effectively extracting the intrinsic features from a large amount 
of data to reduce the dimensionality while ensuring classifica-
tion accuracy. The weight decay term JLR21−wd introduces the 
­L21-norm of weights into the model and sparsely constrains the 
rows and columns of the weights between the input and hidden 
layer. This regularization can adaptively eliminate the poten-
tial noise and redundant neurons in the structure and reduce the 
complexity of the learning model to prevent overfitting. The 
multi-scale feature consistency regularization term Jms forces 
the model to learn potential information at different scales by 
constraining the features learned by the auto-encoder network 
and the features learned by the multi-path auto-encoder empiri-
cal model, making the model deal with different visual details at 
appropriate scales in the training process and then improving 
classification accuracy. In a word, LR21-MSAE is proposed as 
a multi-scale and sparsity feature learning algorithm that uses 
these four regularization terms described above to constrain 
the hidden layer representation and utilize gradient descent 
better to adjust the weights and bias in this connected network. 
Then, we can use the feature representations extracted from 
the hidden layer as the input to the classifier to implement the 
classification application.
In LR21-MSAE, it is assumed that n training data in the 
d− dimensional space are given, and training samples can 
be expressed by X = {xi
}n
i=1 ∈Rd . LR21-MSAE aims to get 
a hidden representation Y = {yi
}n
i=1 ∈Rm of the network by 
utilizing an appropriate encoding function 
yi = f(xi
) = s(wxi + b) , which maps the input sample xi to 
yi in the l− dimensional hidden layer. And the output data 
Z = {zi
}n
i=1 ∈Rd can be obtained by the decoding function 
zi = g(yi
) = s(wyi + b) , where s(x) =
1
1+e−x is a sigmoid 
function. Note that the parameter 휃= {w, b, w, b} to be 
updated includes weight Matrix w and bias vector b connect-
ing the input layer to the hidden layer, weight Matrix w′ and 
bias vector b′ connecting the hidden layer to the output layer. 
Here, X = [x1, x2, … , xn
] ∈Rd×n denotes the input training 
samples matrix, Y = [y1, x2, … , yn
] ∈Rm×n the hidden rep-
resentation matrix, and Z = [z1, z2, … , zn
] ∈Rd×n the recon-
struction output matrix. Then the total loss function of 
LR21-MSAE is further defined as
where 휆 and 훽 are the parameters to balance the different pen-
alty terms, respectively. The parameters are generally taken any 
value between 0 and 1. According to our preliminary experi-
ments, it is shown that there is no need to multiply the coeffi-
cient before the term of Jms , because the importance of Jms is the 
same as that of JAE . Here, the average reconstruction error term 
JAE= 1
2n
∑n
i=1 xi −zi
2
2 can be expressed by the mean square 
deviation of the input data and output data.
We list the notations used frequently with a brief 
description in Table 1. Then, we use the gradient descent 
method to optimize the objective function in Eq. (13) and 
hence adjust b and w of LR21-MSAE as follows
where 훼 is the learning rate in the gradient descent method. 
The extracted features are input into the softmax classifier 
for the classification application. Therefore, the pseudo-
codes for training LR21-MSAE are shown in Algorithm 1.
(13)
JLR21−MSAE = JAE + 휆JLR21−wd + 훽Jsparse + Jms
(14)
b(l)
i = b(l)
i −훼휕JLR21−MSAE(w, b)
휕b(l)
i
(15)
w(l)
ij = w(l)
ij −훼휕JLR21−MSAE(w, b)
휕w(l)
ij
Fig. 3   The framework of LR21-MSAE for classification

3915
International Journal of Machine Learning and Cybernetics (2022) 13:3909–3925	
1 3
The time complexity of LR21-MSAE is also analyzed in 
this study. We first assume that the number of samples is n , 
and there are d nodes in the input layer, l nodes in the hid-
den layer, d nodes in the output layer, respectively. In each 
iteration of LR21-MSAE, the time complexity of com-
puting the average reconstruction error term JAE and its 
gradient 휕JAE is O(ndl) . The time complexity of computing 
the weight decay term JLR21−wd and its gradient 휕JLR21−wd 
is O(dl) . The time complexity of computing the sparsity 
regularization term Jsparse and its gradient 휕Jsparse is O(ndl) . 
Then the time complexity of computing the Laplacian pyr-
amid regularization term Jms and its gradient 휕Jms is O(ndl) . 
Eventually, the total time complexity of each iteration of 
LR21-MSAE is O(ndl) , which is the accumulation of the 
time complexity of the above four regularizations.
Table 1   Some important notations
Notation
Description
Notation
Description
n
Number of samples
xi
i th input training sample
d
Dimensionality of sample space
yi
The data in hidden layer of the i th training sample
m
Dimensionality of hidden space
zi
Reconstruction data from the i th training sample
w
The weight matrix of encoder
X
Sample matrix of input layer
b
The bias of encoder
Y
Feature matrix of hidden layer
휌
Sparsity parameter
Z
Output matrix of output layer
휌∗
j
The average activation distribution
N
Number of Laplacian pyramid data fusion layer
w′
The weight matrix of decoder
훼
Learning rate in gradient decent method
b′
The bias of decoder
휆,훽
Parameters of objective function terms
Table 2   The specification of the seven datasets
Datasets
#training
#testing
#total
#features
#classes
MNIST
60,000
10,000
70,000
784
10
Fashion-MNIST
60,000
10,000
70,000
784
10
CIFAR-10
50,000
10,000
60,000
3072
10
USPS
7291
2007
9298
256
10
ISOLET
6238
1559
7797
617
26
Pendigits
5496
5496
10,992
16
10
Ecoli
336
200
536
7
8

3916
	
International Journal of Machine Learning and Cybernetics (2022) 13:3909–3925
1 3
4  Experiments
4.1  Experimental configuration
To test the performance of our method LR21-MSAE, we 
carried out some experiments on seven benchmark datasets, 
including MNIST, Fashion-MNIST, CIFAR-10, USPS, ISO-
LET, Pendigits, and Ecoli for object classification, respec-
tively. The last four datasets are from the UCI Machine 
Learning Repository [45]. Details of these seven datasets in 
experiments are summarized in Table 2, and short descrip-
tions of datasets are as follows:
1.	 The MNIST dataset is a modified National Institute of 
Standards and Technology dataset commonly used for 
image recognition. And all pictures are size 28*28 with 
ten different categories. It contains 60,000 grayscale 
training pictures and 10,000 grayscale testing pictures.
2.	 The Fashion-MNIST is a dataset with 60,000 training 
images and 10,000 testing images of fashion products 
with ten different categories: trouser, pullover, dress, 
coat, sandal, shirt, sneaker, bag, ankle boot, and t-shirt/
top. And each image is 28*28 grayscale pixels. Some 
image samples are shown in Fig. 4.
3.	 The CIFAR-10 dataset consists of 60,000 32*32 color 
images in ten classes, and each class has 6,000 pic-
tures. In addition, CIFAR-10 dataset has 50,000 train-
ing images and 10,000 test images. The ten categories 
include airplane, automobile, bird, cat, deer, dog, frog, 
horse, ship, and truck. Figure 5 shows some image sam-
ples of CIFAR-10.
4.	 The USPS dataset is a grayscale US postal handwritten 
digit dataset size 16*16. USPS dataset has 9298 hand-
written digital images in ten categories. And it contains 
7291 training images and 2007 testing images.
5.	 The ISOLET dataset is used to predict which letter-
name was spoken. One hundred fifty participants read 
the alphabet two times in the dataset. These participants 
are divided into five groups of thirty people each. And 
then, five datasets are devised, namely Isolet1, Isolet2, 
Isolet3, Isolet4, and Isolet5. It is worth noting that the 
first four datasets are used for training, and Isolet5 is 
used for testing.
6.	 The Pendigits dataset is short for Pen-Based Recog-
nition of Handwritten Digits Dataset. This dataset is 
introduced by collecting 250 samples from 44 writers 
in total. And few digits are invalid because some writers 
are unfamiliar with the input device, resulting in errone-
ous data. Therefore, we have 7494 samples for training 
and 3498 samples for testing.
7.	 The Ecoli dataset is the Escherichia coli dataset, also 
known as the Protein localization site dataset. This data-
set describes the use of amino acid sequences in cell 
localization sites to classify Escherichia coli proteins. It 
contains 336 E.coli protein data, and seven input vari-
ables represent each data.
We implemented the experiments in MATLAB R2019b 
and conducted them on a GPU server with the following 
configurations: CPU (E5-2620V4*2), memory (32 GB*8), 
solid-state disk (480 GB), hard disk (4 TB), and a GPU 
(GTX 1080Ti*4).
Trouser
pullover
dress
coat
sandal
shirt
sneaker
bag
ankle boot
t-shirt/top
Fig. 4   Some samples of Fashion-MNIST
airplane
automobile
bird
cat
deer
dog
frog
horse
ship
truck
Fig. 5   Some samples of CIFAR-10

3917
International Journal of Machine Learning and Cybernetics (2022) 13:3909–3925	
1 3
To evaluate the effectiveness of LR21-MSAE, we com-
pared it with the following methods:
•	 Auto-encoder with ­L2-norm regularization (AE), which 
is the baseline model [46–48].
•	 Sparse auto-encoder with ­L2-norm regularization (SAE) 
[49].
•	 Variational auto-encoder (VAE) [50].
•	 A new variable learning speed auto-encoder using multi-
scale reconstruction errors and weight update correlation 
(VLSAE) [51].
•	 Sparse auto-encoder with ­L21-norm regularization 
(LR21-SAE).
•	 Auto-encoder with multi-scale feature consistency regu-
larization (MSAE).
4.2  Experiment and analysis
To test the performance of LR21-MSAE, we have conducted 
extensive experiments. We have analyzed the sensitivity, 
convergence, sparsity, and classification performance of the 
proposed LR21-MSAE, the analysis of the Wasserstein dis-
tance, and the runtime comparison.
1.	 Sensitivity of LR21-MSAE
This study should set two coefficients, i.e., 휆 and 훽 , 
to balance each regularization on the objective func-
tion. 휆 is the parameter of the weight decay term, 
and 훽 is the parameter of the sparsity regularization 
Fig. 6   The testing accuracy of LR21-MSAE in (휆, 훽) on a MNIST b USPS c ISOLET

3918
	
International Journal of Machine Learning and Cybernetics (2022) 13:3909–3925
1 3
term. To investigate the sensitivity of LR21-MSAE 
regarding parameters, we set parameters in a particu-
lar range. 휆 and 훽 were all chosen within the ranges of 
{1 × 10e|e = −7, −6, −5, −4, −3, −2, −1, 0, 1, 2} to deter-
mine the best combination of optimal parameters to avoid 
the overflow problem. The testing classification accuracy 
curve of LR21-MSAE in (휆, 훽) on datasets MNIST, USPS, 
and ISOLET are shown in Fig. 6. The coordinates of 휆 
and 훽 offer the variation ranges of these parameters, and 
the color bar displays the accuracy. The parameters of the 
proposed method LR21-MSAE are not very sensitive to 
the specified parametric values, and the model is resistant 
to interference and highly robust. As a result, LR21-MSAE 
can expose a better generalization performance in a range 
of values. Therefore, in the case of uncertain parameters, 
users can randomly select parameters in a specific range. 
As a result, these parameters do not significantly impact 
the final classification accuracy. Figure 6 also shows the 
parameter values of 휆 and 훽 of the optimal classification 
accuracy results. As seen from Fig. 6, the two sets of 휆 
and 훽 , i.e., 휆=0.00001 and 훽=0.1 on the MNIST dataset, 
휆=0.00001 and 훽=0.1 on the USPS dataset, 휆=0.00001 and 
훽=0.0001 on the ISOLET dataset, ensure LR21-MSAE to 
get the highest accuracy on each dataset.
2.	 Convergence of LR21-MSAE
In the subsection, we discuss the effect of the iterations 
and the number of hidden nodes l on the classification accu-
racy. The curve results on ISOLET, USPS, and Pendigits 
datasets are shown in Figs. 7 and 8. It can be observed from 
Fig. 7 that it took only 60–80 iterations for the LR21-MSAE 
to converge. And we also notice that the model with only 
200 hidden neurons can reach the convergence state in 
Fig. 8. Significantly, the proposed LR21-MSAE performed 
better than the other algorithms for all different iterations 
and the most number of hidden neurons with higher classifi-
cation accuracy. Thus, the proposed algorithm LR21-MSAE 
can be applied to some large-scale practical problems.
3.	 Sparsity of LR21-MSAE
Our method utilizes the ­L21-norm regularization to 
enhance the sparsity of the network structure. In this experi-
ment, we compared the distribution of the weights of the 
proposed LR21-MSAE with the AE and SAE to validate 
the LR21-MSAE can learn sparse hidden representation on 
the classification tasks. In this part, we just show weights 
distribution on USPS and ISOLET datasets. And as shown 
0
10
20
30
40
50
60
70
80
90
100
90
91
92
93
94
95
96
97
classification accuracy/%
iterations
LR21-MSAE
 AE
 SAE
 VAE
 VLSAE
 LR21-SAE
 MSAE
(a)
(b)
(c)
0
10
20
30
40
50
60
70
80
90
100
85
86
87
88
89
90
91
92
93
94
95
classification accuracy/%
iterations
 LR21-MSAE
 AE
 SAE
 VAE
 VLSAE
 LR21-SAE
 MSAE
0
10
20
30
40
50
60
70
80
90
100
90
91
92
93
94
95
96
97
98
99
100
classification accuracy/%
iterations
 LR21-MSAE
 AE
 SAE
 VAE
 VLSAE
 LR21-SAE
 MSAE
Fig. 7   The convergence curve of the classification accuracy versus the iterations on datasets a ISOLET b USPS c Pendigits
0
50
100
150
200
250
300
350
400
450
500
40
50
60
70
80
90
100
classification accuracy/%
number of hidden neurons
 LR21-MSAE
 AE
 SAE
 VAE
 VLSAE
 LR21-SAE
 MSAE
(a) 
0
50
100
150
200
250
300
350
400
450
500
60
70
80
90
100
classification accuracy/%
number of hidden neurons
 LR21-MSAE
 AE
 SAE
 VAE
 VLSAE
 LR21-SAE
 MSAE
(b) 
0
50
100
150
200
250
300
350
400
450
500
82
84
86
88
90
92
94
96
98
100
classification accuracy/%
number of hidden neurons
 LR21-MSAE
 AE
 SAE
 VAE
 VLSAE
 LR21-SAE
 MSAE
(c) 
Fig. 8   The convergence curve of the classification accuracy versus the number of hidden neurons on datasets a ISOLET b USPS c Pendigits

3919
International Journal of Machine Learning and Cybernetics (2022) 13:3909–3925	
1 3
in Figs. 9 and 10, overall, we see the method LR21-MSAE 
proposed here outperforms the other two. Our approach can 
obtain a sparser model than AE and SAE methods since 
most weight parameters are constrained to 0. In fact, the 
­L21-norm is used to enhance the sparsity of the rows and 
columns of weights. The weight parameters related to the 
negligible features are reduced to 0 to decrease the complex-
ity of the learning model. This verifies the effectiveness of 
the sparsity strategy by the ­L21-norm regularization, which 
can promote sparsity by eliminating redundant information 
and then obtain higher classification accuracy.
4.	 Classification performance of LR21-MSAE
In this part, we compared the proposed LR21-MSAE with 
AE, SAE, VAE, VLSAE, LR21-SAE, and MSAE to further 
verify our method’s performance. Based on the sensitivity 
and convergence experiment, the parameter values of 휆 and 
훽 , iterations and l have a smooth effect on performance, and 
they should be set to the optimal values for classification. 
To obtain good generalization performance of the model, 
the values of 휆 and 훽 , iterations and l , i.e., 휆= 0.00001 , 
훽= 0.1 , iterations = 500, l = 500 on the MNIST data-
set, 휆= 0.0000001 , 훽= 0.1 , iterations = 500, l = 300 
on the Fashion-MNIST dataset, 휆= 0.00001 , 훽= 0.1 , 
iterations = 100, l = 400 on USPS dataset, 휆= 0.00001 , 
훽= 0.0001 , iterations = 100, l = 450 on ISOLET dataset, 
휆= 0.0000001 , 훽= 0.1 , iterations = 90, l = 400 on Pendigits 
dataset, 휆= 0.000001 , 훽= 0.0001 , iterations = 20, l = 20 on 
Ecoli dataset, enabled LR21-MSAE to obtain the best clas-
sification accuracy on each dataset. And the sigmoid func-
tion was used as the activation function in all experiments.
We repeated the experiments 100 times for each dataset 
using the optimal parameters. In Table 3, we demonstrate the 
classification performances of LR21-MSAE. The bold data 
are the best results in each dataset. As shown, the classifica-
tion performance of the proposed model LR21-MSAE out-
performs AE, SAE, VAE, VLSAE, LR21-SAE, and MSAE 
algorithms in all cases. The classification accuracy of SAE 
is better than baseline AE in most datasets because adding 
-0.3
-0.2
-0.1
0.0
0.1
0.2
0.3
0
1000
2000
3000
4000
5000
6000
Numbers of weights
Weights value
 AE
(a)
-0.3
-0.2
-0.1
0.0
0.1
0.2
0.3
0
2000
4000
6000
8000
10000
Numbers of weights
Weights value
 SAE
(b)
-0.4
-0.2
0.0
0.2
0.4
0
2000
4000
6000
8000
10000
12000
14000
16000
Numbers of weights
Weights value
 LR21-MSAE
(c)
Fig. 9   Weights distribution of AE model, SAE model, and LR21-MSAE model on USPS dataset. a AE weights distribution. b SAE weights dis-
tribution. c LR21-MSAE weights distribution
-0.10
-0.05
0.00
0.05
0.10
0.15
0
1000
2000
3000
4000
5000
Number of weights
Weights value
 AE
(a)
-0.2
-0.1
0.0
0.1
0.2
0.3
0
5000
10000
15000
20000
25000
30000
35000
40000
45000
Number of weights
Weights value
 SAE
(b)
-0.2
-0.1
0.0
0.1
0.2
0.3
0
10000
20000
30000
40000
50000
60000
Number of weights
Weights value
 LR21-MSAE
(c)
Fig. 10   Weights distribution of AE model, SAE model, and LR21-MSAE model on ISOLET dataset. a AE weights distribution. b SAE weights 
distribution. c LR21-MSAE weights distribution

3920
	
International Journal of Machine Learning and Cybernetics (2022) 13:3909–3925
1 3
a sparsity regularization can force the model to extract the 
potential features and improve the accuracy. LR21-SAE 
obtains a superior sparsity model, resulting in the highest 
level structured sparsity, retaining fewer nodes and param-
eters. Therefore, the advantage of LR21-SAE in classifica-
tion accuracy is not apparent. Then the MSAE, which adds 
multi-scale feature consistency regularization term based 
on SAE, can improve the classification accuracy since the 
model can learn the multi-scale feature representation. All 
in all, our method LR21-MSAE has the best classification 
performance among all the compared methods on the tested 
datasets with the given parameter setting.
Figures 11 and 12 illustrate the bar plot of seven algo-
rithms on every category of Fashion-MNIST and CIFAR-
10. The horizontal axis represents every category label, and 
the vertical axis states the average classification accuracy. It 
can be seen that the proposed LR21-MSAE outperforms the 
other six algorithms for most categories.
Figure 13 displays the confusion matrix of different meth-
ods on the USPS dataset. The diagonals of the tables show 
the accuracy of each class, and other values in the table are 
the probabilities of error values. We can see that from the 
confusion matrix in Fig. 13, the proposed LR21-MSAE per-
forms better than the other methods in most cases.
5.	 Analysis of the Wasserstein distance
Several experiments were designed to verify the effect of 
using the Wasserstein distance as a constraint term on the 
average activation. Figure 14 shows the Wasserstein distance 
versus JS divergence and KL divergence when 휌= 0.1 . In 
the figure, the horizontal coordinate is the average activation 
distribution 휌∗
j , and the vertical coordinate is the values of 
KL divergence, JS divergence, and the Wasserstein distance. 
It can be seen that the Wasserstein distance is continuous 
and bounded. Near the given sparsity parameter, the gradient 
of the Wasserstein distance remains 1 or − 1, while the gra-
dient of the JS divergence and KL divergence tends to 0. For 
the gradient propagation optimization methods, the Was-
serstein distance is more conducive to the model training and 
convergence of the training process.
Table 3   The classification accuracies of seven algorithms on seven datasets
Datasets
AE (baseline)
SAE
VAE
VLSAE
LR21-SAE
MSAE
LR21-MSAE
MNIST
95.24 ± 0.15
96.76 ± 0.17
93.02 ± 0.12
98.05 ± 0.14
96.24 ± 0.14
96.88 ± 0.19
98.86 ± 0.11
Fashion-MNIST
85.16 ± 0.21
85.23 ± 0.22
80.54 ± 0.24
85.26 ± 0.22
85.82 ± 0.21
85.88 ± 0.22
86.90 ± 0.20
CIFAR-10
38.56 ± 0.24
39.41 ± 0.22
41.73 ± 0.21
40.96 ± 0.22
42.57 ± 0.20
42.86 ± 0.22
44.80 ± 0.20
USPS
89.87 ± 0.25
93.78 ± 0.22
92.03 ± 0.27
92.32 ± 0.24
93.92 ± 0.20
94.05 ± 0.24
94.99 ± 0.18
ISOLET
94.06 ± 0.25
94.56 ± 0.22
94.31 ± 0.21
94.50 ± 0.22
94.59 ± 0.25
95.12 ± 0.22
95.58 ± 0.20
Pendigits
97.50 ± 0.34
98.43 ± 0.23
98.50 ± 0.24
98.49 ± 0.20
98.60 ± 0.21
98.26 ± 0.20
98.97 ± 0.18
Ecoli
87.50 ± 3.28
88.20 ± 3.05
87.75 ± 3.21
88.08 ± 3.01
88.30 ± 3.29
88.80 ± 3.15
90.10 ± 2.99
Trouser
Pullover
Dress
Coat
Sandal
Shirt
Sneaker
Bag
Ankle boot
T-shirt/top
50
60
70
80
90
100
classification accuracy/%
 AE
 SAE
 VAE
 VLSAE
 LR21-SAE
 MSAE
 LR21-MSAE
Fig. 11   The classification accuracy on each category of Fashion-
MNIST
airplane
automobile
bird
cat
deer
dog
frog
horse
ship
truck
20
30
40
50
60
70
classification accuracy/%
 AE
 SAE
 VAE
 VLSAE
 LR21-SAE
 MSAE
 LR21-MSAE
Fig. 12   The classification accuracy on each category of CIFAR-10

3921
International Journal of Machine Learning and Cybernetics (2022) 13:3909–3925	
1 3
To compare the coding effects of the three different met-
ric distributions, we calculated the average value of the test 
set image coding for different values of 휌 as
Table 4 shows the results of the 휇 for the three metrics on 
the three benchmark datasets for different values of 휌 . We can 
(25)
휇=
1
n × l
n
∑
i=1
l∑
j=1
yj
i
(a)
(b)
(c)
(d)
(e)
(f)
(g)
 
1 
2 
3 
4 
5 
6 
7 
8 
9 
0 
1 0.9638 
0 
0.0028 0.0056 0.0056 
0 
0.0167 
0 
0.0028 0.0028 
2 
0 
0.9621 
0 
0.0114 0.0076 
0 
0.0152 
0 
0.0038 
0 
3 0.0303 
0 
0.8182 0.0253 0.0455 0.0101 0.0051 0.0101 0.0505 0.0051 
4 0.0181 
0 
0.0361 0.8675 
0 
0.0723 
0 
0 
0 
0.006 
5 
0.005 
0.01 
0.035 
0 
0.86 
0 
0.025 
0.005 
0.015 
0.045 
6 0.0313 0.0063 
0 
0.0437 
0.025 
0.8562 
0 
0.0063 0.0125 0.0187 
7 0.0118 
0 
0.0235 
0 
0.0176 0.0235 0.9118 
0 
0.0118 
0 
8 0.0068 
0 
0.0068 0.0068 0.0204 0.0068 
0 
0.898 
0.0136 0.0408 
9 0.0361 0.0181 
0.012 
0.0422 
0.012 
0.0542 
0 
0.006 
0.7952 0.0241 
0 
0 
0.0169 
0 
0 
0.0113 0.0056 
0 
0.0226 0.0169 0.9266 
 
1 
2 
3 
4 
5 
6 
7 
8 
9 
0 
1 
0.961 
0 
0.0028 0.0084 0.0056 
0 
0.0111 
0 
0.0111 
0 
2 
0 
0.9621 
0 
0.0114 0.0076 
0 
0.0152 
0 
0.0038 
0 
3 0.0303 
0 
0.8182 0.0253 0.0505 0.0101 0.0152 0.0101 0.0404 
0 
4 0.0181 
0 
0.0301 0.8675 
0 
0.0602 
0 
0 
0.012 
0.012 
5 
0.005 
0.01 
0.035 
0 
0.85 
0.005 
0.015 
0.005 
0.015 
0.06 
6 0.0375 0.0063 
0 
0.05 
0.025 
0.8375 
0 
0 
0.0187 
0.025 
7 0.0176 
0 
0.0118 
0 
0.0235 0.0059 0.9353 
0 
0.0059 
0 
8 0.0068 
0 
0 
0.0136 
0.034 
0.0068 
0 
0.898 
0.0068 
0.034 
9 0.0301 0.0181 
0.012 
0.0361 0.0241 0.0301 
0 
0.006 
0.8373 
0.006 
0 
0 
0.0169 
0 
0 
0.0113 0.0056 
0 
0.0113 0.0169 0.9379 
 
1 
2 
3 
4 
5 
6 
7 
8 
9 
0 
1 0.9431  0.014 
0.0269 
0.004 
0 
0.004 
0 
0.002 
0 
0.006 
2 
0.002 
0.675 
0.011 
0.179 
0.0011 
0.105 
0 
0.01 
0 
0.0169 
3 
0.013 
0.0121 
0.853 
0.0339 
0 
0.046 
0.001 
0.004 
0 
0.037 
4 
0 
0.14 
0.04 
0.74 
0 
0.069 
0 
0.01 
0 
0.001 
5 
0 
0 
0.001 
0 
0.89 
0 
0.0592 
0.005 
0.0448 
0 
6 
0.002 
0.151 
0.041 
0.134 
0 
0.476 
0 
0.029 
0 
0.167 
7 
0 
0 
0 
0 
0.04 
0 
0.9 
0 
0.06 
0 
8 
0.003 
0.016 
0.004 
0.008 
0.009 
0.02 
0.0061 
0.929 
0.001 
0.0039 
9 
0 
0 
0 
0 
0.021 
0 
0.038 
0.001 
0.94 
0 
0 
0.005 
0.024 
0.055 
0.006 
0 
0.1061 
0 
0.0139 
0 
0.79 
 
1 
2 
3 
4 
5 
6 
7 
8 
9 
0 
1 
0.951 
0.004 
0.026 
0.004 
0 
0.008 
0 
0.001 
0 
0.006 
2 
0.002 
0.736 
0.013 
0.143 
0.001 
0.09 
0 
0.006 
0 
0.009 
3 
0.011 
0.013 
0.864 
0.037 
0 
0.037 
0 
0.006 
0 
0.032 
4 
0 
0.107 
0.031 
0.778 
0 
0.078 
0 
0.006 
0 
0 
5 
0 
0 
0.001 
0 
0.928 
0 
0.043 
0.001 
0.027 
0 
6 
0.003 
0.132 
0.038 
0.098 
0.001 
0.564 
0 
0.026 
0 
0.138 
7 
0 
0 
0 
0 
0.035 
0 
0.928 
0 
0.037 
0 
8 
0.001 
0.014 
0.006 
0.004 
0.007 
0.017 
0.005 
0.943 
0 
0.003 
9 
0 
0 
0 
0 
0.014 
0 
0.046 
0.001 
0.939 
0 
0 
0 
0.021 
0.041 
0.005 
0.003 
0.105 
0 
0.014 
0 
0.811 
 
1 
2 
3 
4 
5 
6 
7 
8 
9 
0 
1 0.9777 
0 
0.0056 
0 
0.0056 
0 
0.0084 
0 
0.0028 
0 
2 
0 
0.9545 0.0038 0.0076 0.0152 
0 
0.0152 
0 
0 
0.0038 
3 0.0101 
0 
0.9141 0.0101 0.0101 0.0101 0.0051 0.0051 0.0303 0.0051 
4 
0.006 
0 
0.006 
0.8916 
0 
0.0663 
0 
0 
0.0241 
0.006 
5 
0.005 
0.005 
0.025 
0 
0.91 
0.01 
0.005 
0.005 
0.005 
0.03 
6 0.0187 
0 
0 
0.0313 
0 
0.9063 
0 
0 
0.0187 
0.025 
7 
0 
0 
0.0176 
0 
0.0118 0.0059 0.9588 
0 
0.0059 
0 
8 
0 
0 
0.0068 0.0068 
0.034 
0 
0 
0.9184 0.0204 0.0136 
9 0.0181 
0 
0.006 
0.0241 
0 
0.0241 
0 
0 
0.9157 
0.012 
0 0.0056 0.0056 0.0056 
0 
0.0056 0.0056 
0 
0.0113 
0.0113 0.9492 
 
1 
2 
3 
4 
5 
6 
7 
8 
9 
0 
1 0.9777 
0 
0.0056 
0 
0.0056 
0 
0.0084 
0 
0.0028 
0 
2 
0 
0.9545 0.0038 0.0076 0.0114 
0 
0.0152 
0 
0 
0.0076 
3 0.0101 
0 
0.9192 
0 
0.0051 0.0152 0.0051 0.0051 0.0354 0.0051 
4 
0.006 
0 
0.006 
0.8855 
0 
0.0663 
0 
0.006 
0.0241 
0.006 
5 
0.005 
0.005 
0.02 
0 
0.905 
0.01 
0.01 
0.005 
0.005 
0.035 
6 0.0125 
0 
0 
0.0313 0.0063 
0.925 
0 
0 
0.0063 0.0187 
7 0.0059 
0 
0.0118 0.0059 0.0118 0.0059 0.9588 
0 
0 
0 
8 
0 
0 
0.0136 
0 
0.034 
0 
0 
0.932 
0.0204 
0 
9 0.0181 
0 
0.012 
0.0361 
0 
0.0181 
0 
0 
0.9036 
0.012 
0 
0 
0.0056 0.0056 
0 
0.0113 0.0056 
0 
0.0056 0.0056 0.9605 
 
1 
2 
3 
4 
5 
6 
7 
8 
9 
0 
1 0.9805 
0 
0.0056 
0 
0.0028 0.0028 0.0056 
0 
0 
0.0028 
2 
0 
0.9659 
0 
0.0014 0.0114 
0 
0.0114 
0 
0 
0 
3 0.0253 
0 
0.9441 0.0001 0.0101 0.0101 0.0051 0.0051 0.0152 0.0051 
4 
0 
0 
0.0081 0.9876 
0 
0.0542 
0 
0 
0.0241 
0.006 
5 
0 
0.005 
0.02 
0 
0.915 
0.015 
0.01 
0.005 
0 
0.03 
6 0.0125 
0 
0 
0.0013 0.0063 0.9187 
0 
0 
0.0125 0.0187 
7 
0 
0 
0.0076 
0 
0.0118 0.0059 0.9647 
0 
0 
0 
8 
0 
0 
0.0068 0.0068 0.0272 0.0136 
0 
0.9252 0.0136 0.0068 
9 
0 
0 
0.012 
0.0041 
0 
0.0181 
0 
0 
0.9277 0.0181 
0 
0 
0.0056 0.0056 0.0056 0.0056 
0 
0 
0.0056 
0.0113 0.9605 
Fig. 13   The confusion matrix of seven models on USPS. a AE confusion matrix. b SAE confusion matrix. c VAE confusion matrix. d VLSAE 
confusion matrix. e LR21-SAE confusion matrix. f MSAE confusion matrix. g LR21-MSAE confusion matrix

3922
	
International Journal of Machine Learning and Cybernetics (2022) 13:3909–3925
1 3
observe from the table that the value of 휇 for the model with 
Wasserstein distance is basically equal to the corresponding 
value of 휌 . It can also be seen that the model with Wasserstein 
distance can make the encoding closer to the given sparsity 
parameter, and thus the regularization effect is better.
Then, we compared the convergence effect of the model 
using different metric distributions. This experiment tested 
models in each iteration and calculated the average accura-
cies on Fashion-MNSIT and CIFAR-10 datasets. The experi-
mental results are illustrated in Fig. 15. As can be seen, 
the model with Wasserstein distance converges the value 
closer to JS divergence and KL divergence in about the first 
25 iterations. However, in the later stage, the average accu-
racy of the model with Wasserstein distance converges very 
quickly, and the value is higher than those of the models 
with JS divergence and KL divergence.
6.	 Comparison of Computational Efficiency
To more intuitively compare the difference between 
the computational time taken by the seven methods in the 
training and testing processes, we graphically visualize 
these data in Fig. 16. It can be easily observed that the 
training time of LR21-MSAE is just slightly longer than 
that of AE, SAE, VAE, LR21-SAE, and MSAE, but mar-
ginally shorter than VLSAE in Fig. 16a, b. Compared to 
AE, SAE, VAE, VLSAE, LR21-SAE, and MSAE, since the 
four constraint terms JAE , JLR21−wd , Jsparse and Jms are all 
considered, LR21-MSAE needs more computational time. 
As shown in Fig. 16c, we can see that the testing time of 
LR21-MSAE and LR21-SAE is significantly shorter than 
that of AE, SAE, VAE, VLSAE, and MSAE. Since the 
LR21-MSAE and LR21-SAE employ the ­L21-norm regu-
larization to constrain the rows and columns of weights, 
the model complexity can be reduced. As the training pro-
cess of the network can be conducted offline in advance, 
our method is obviously superior to other compared algo-
rithms in classification accuracy. Then from the perspec-
tive of the computational complexity and algorithm per-
formance, this method has a specific value in application.
5  Conclusion and future work
This study proposed a novel multi-scale and sparsity auto-
encoder for classification, namely, LR21-MSAE. Firstly, in 
this method, the feature consistency regularization, con-
strained by the learned features and multi-scale features 
on each dataset, can achieve the latent representations 
and provide valid information to the network to improve 
classification accuracy. Then the ­L21-norm regularization 
in LR21-MSAE can adaptively eliminate the potential 
noise and redundant parameters by reducing the weight 
parameters related to negligible features to 0. Besides, 
we proposed to employ Wasserstein distance instead of 
the traditional KL divergence to measure the difference 
between the average activation distribution and the given 
sparsity parameter distribution, which makes the training 
process more stable and the convergence faster in the case 
of the sparsity model. The comparative results on several 
benchmark datasets showed that LR21-MSAE could cap-
ture multi-scale information to improve classification since 
the model is more stable, more compact, and sparser.
To summarize, LR21-MSAE is an effective method due 
to better classification accuracy and a tight network. Further-
more, LR21-MSAE can be used for object classification and 
other tasks, including object tracking, face recognition, and 
person re-identification, achieving better results.
The weakness of this study is that unsupervised learning 
avoids the difficulties of manual annotation but loses the aid 
Fig. 14   The plot of KL divergence, JS divergence, and Wasserstein 
distance when 휌= 0.1
Table 4   Comparison of average values generated by the model using 
different metric distributions on three datasets
휌
KL divergence
JS divergence
Was-
serstein 
distance
Fashion-MNSIT
0.05
0.0489
0.0452
0.0502
0.1
0.0987
0.0969
0.0999
0.2
0.1992
0.1897
0.1998
CIFAR-10
0.05
0.0462
0.0421
0.0500
0.1
0.0974
0.0912
0.0998
0.2
0.1945
0.1901
0.1999
USPS
0.05
0.0472
0.0452
0.0500
0.1
0.0972
0.0935
0.1000
0.2
0.1978
0.1919
0.1998

3923
International Journal of Machine Learning and Cybernetics (2022) 13:3909–3925	
1 3
of sample labels, which causes unsupervised learning's clas-
sification performance to be worse than supervised learn-
ing. Therefore, future work will focus on semi-supervised 
auto-encoders using our proposed LR21-MSAE to reason-
ably make use of labeling information to improve the feature 
extraction capability.
Acknowledgements  This work is partly supported by the National 
Natural Science Foundation of China (Projects numbers: 61673194, 
61672263, 61672265, 62076110, 61673193), the Natural Science 
Foundation of Jiangsu Province (Project number: BK20181341) and 
the national first-class discipline program of Light Industry Technology 
and Engineering (Project number: LITE2018-25).
Declarations 
Conflict of interest  The authors declare that they have no conflict of 
interest.
References
	 1.	 Song ZJ (2020) English speech recognition based on deep learning 
with multiple features. Computing 102(99):1–20. https://​doi.​org/​
10.​1007/​s00607-​019-​00753-0
	 2.	 Byun SW, Lee SP (2021) A study on a speech emotion recogni-
tion system with effective acoustic features using deep learning 
0
10
20
30
40
50
60
70
80
90
100
70
72
74
76
78
80
82
84
86
88
classification accuracy/%
iterations
 with KL divergence
 with JS divergence
 with Wasserstein distance
(a) 
0
10
20
30
40
50
60
70
80
90
100
28
30
32
34
36
38
40
42
44
46
48
classification accuracy/%
iterations
 with KL divergence
 with JS divergence
 with Wasserstein distance
(b) 
Fig. 15   The average classification accuracies of the models with different metric distributions on datasets a Fashion-MNSIT b CIFAR-10
MNIST
Fashion-MNSIT
CIFAR-10
0
2000
4000
6000
AE
SAE
VAE
VLSAE
LR21-SAE
MSAE
LR21-MSAE
training time/s
(a) The training time of seven 
algorithms on MNIST, 
Fashion-MNSIT, and 
CIFAR-10 
Ecoli
Pendigits
ISOLET
USPS
0
20
40
60
80
100
120
AE
SAE
VAE
VLSAE
LR21-SAE
MSAE
LR21-MSAE
training time/s
(b) The training time of seven 
algorithms on USPS, 
ISOLET, Pendigits, and 
Ecoli 
Ecoli
Pendigits
ISOLET
USPS
CIFAR-10
Fashion-MNSIT
MNIST
0.00
0.08
0.16
0.24
0.32
LR21-MSAE
MSAE
LR21-SAE
VLSAE
VAE
SAE
AE
testing time/s
(c) The testing time of seven 
algorithms on the seven 
datasets 
Fig. 16   The training/testing time comparison of seven algorithms on the seven datasets

3924
	
International Journal of Machine Learning and Cybernetics (2022) 13:3909–3925
1 3
algorithms. Appl Sci 11(4):1890–1890. https://​doi.​org/​10.​3390/​
APP11​041890
	 3.	 Kovalev VA, Liauchuk VA, Voynov DM, Tuzikov AV (2021) Bio-
medical image recognition in pulmonology and oncology with the 
use of deep learning. Pattern Recognit Image Anal 31(1):144–162. 
https://​doi.​org/​10.​1134/​S1054​66182​10101​20
	 4.	 Cheok MJ, Omar Z, Jaward MH (2019) A review of hand ges-
ture and sign language recognition techniques. Int J Mach Learn 
Cybern 10(1–3):1–23. https://​doi.​org/​10.​1007/​s13042-​017-​0705-5
	 5.	 Yang BS, Wang LY, Wong DF, Shi SM, Tu ZP (2021) Context-
aware self-attention networks for natural language processing. 
Neurocomputing 458:157–169. https://​doi.​org/​10.​1016/J.​NEU-
COM.​2021.​06.​009
	 6.	 Li R, Zhang X, Li C, Zheng Z, Zhou Z, Geng Y (2021) Keyword 
extraction method for machine reading comprehension based on 
natural language processing. J Phys Conf Ser 1955(1):012072. 
https://​doi.​org/​10.​1088/​1742-​6596/​1955/1/​012072
	 7.	 Bengio Y, Courville A, Pascal V (2013) Representation learning: 
a review and new perspectives. IEEE Trans Pattern Anal Mach 
Intell 35(8):1798–1828. https://​doi.​org/​10.​1109/​TPAMI.​2013.​50
	 8.	 Jia K, Sun L, Gao SH, Song Z, Shi BE (2015) Laplacian auto-
encoders: an explicit learning of nonlinear data manifold. Neu-
rocomputing 106:250–260. https://​doi.​org/​10.​1016/j.​neucom.​
2015.​02.​023
	 9.	 Liu WF, Ma TZ, Tao DP, You JN (2016) HSAE: a Hessian 
regularized sparse auto-encoders. Neurocomputing 187:59–65. 
https://​doi.​org/​10.​1016/j.​neucom.​2015.​07.​119
	10.	 Zhang MH, Yang CL, Yuan Y, Guan Y, Wang SY, Liu QG 
(2021) Multi-wavelet guided deep mean-shift prior for image 
restoration. Signal Process Image Commun 99(9):116449. 
https://​doi.​org/​10.​1016/j.​image.​2021.​116449
	11.	 Lu C, Wang ZY, Qin WL, Ma J (2017) Fault diagnosis of rotary 
machinery components using a stacked denoising autoencoder-
based health state identification. Signal Process 130:377–388. 
https://​doi.​org/​10.​1016/j.​sigpro.​2016.​07.​028
	12.	 Luo S, Zhu L, Althoefer K, Liu H (2017) Knock-Knock: Acoustic 
object recognition by using stacked denoising autoencoders. Neu-
rocomputing 267:18–24. https://​doi.​org/​10.​1016/j.​neucom.​2017.​
03.​014
	13.	 Ozkan S, Kaya B, Akar GB (2019) EndNet: sparse autoencoder 
network for endmember extraction and hyperspectral unmixing. 
IEEE Trans Geosci Remote Sens 57(1):482–496. https://​doi.​org/​
10.​1109/​TGRS.​2018.​28569​29
	14.	 Sun WJ, Shao SY, Zhao R, Yan RQ, Zhang XW, Chen XF (2016) 
A sparse auto-encoder-based deep neural network approach for 
induction motor faults classification. Measurement 89:171–178. 
https://​doi.​org/​10.​1016/j.​measu​rement.​2016.​04.​007
	15.	 Kamimura R, Takeuchi H (2019) Sparse semi-autoencoders to 
solve the vanishing information problem in multi-layered neural 
networks. Appl Intell 49(7):2522–2545. https://​doi.​org/​10.​1007/​
s10489-​018-​1393-x
	16.	 Binbusayyis A, Vaiyapuri T (2021) Unsupervised deep learning 
approach for network intrusion detection combining convolutional 
autoencoder and one-class SVM. Appl Intell 51(10):1–15. https://​
doi.​org/​10.​1007/​S10489-​021-​02205-9
	17.	 Li B, Gong XF, Wang C, Wu RJ, Bian T, Li YM, Wang ZY, 
Luo RS (2021) MMD-encouraging convolutional autoencoder: 
a novel classification algorithm for imbalanced data. Appl Intell 
51(10):1–18. https://​doi.​org/​10.​1007/​S10489-​021-​02235-3
	18.	 Zhang J, Li K, Liang Y, Li N (2017) Learning 3D faces from 
2D images via stacked contractive autoencoder. Neurocomputing 
257:67–78. https://​doi.​org/​10.​1016/j.​neucom.​2016.​11.​062
	19.	 Lan RS, Li ZY, Liu ZB, Gu TL, Luo XN (2018) Hyperspectral 
image classification using k-sparse denoising autoencoder and 
spectral-restricted spatial characteristics. Appl Soft Comput J 
74:693–708. https://​doi.​org/​10.​1016/j.​asoc.​2018.​08.​049
	20.	 Liu WF, Ma TZ, Xie QS, Tao DP, Cheng J (2017) LMAE: a 
large margin Auto-Encoders for classification. Signal Process 
141:137–143. https://​doi.​org/​10.​1016/j.​sigpro.​2017.​05.​030
	21.	 Dong WC, Sun HX, Li Z, Zhang JX, Yang HF (2020) Short-
term wind-speed forecasting based on multiscale mathematical 
morphological decomposition, K-means clustering, and stacked 
denoising autoencoders. IEEE ACCESS 8:146901–146914. 
https://​doi.​org/​10.​1109/​ACCESS.​2020.​30153​36
	22.	 Xu JW, Ni BB, Yang XK (2020) Progressive multi-granularity 
analysis for video prediction. Int J Comput Vis (prepublish). 
https://​doi.​org/​10.​1007/​s11263-​020-​01389-w
	23.	 Wang RP, Cui Y, Song X, Chen K, Fang H (2021) Multi-infor-
mation-based convolutional neural network with attention mecha-
nism for pedestrian trajectory prediction. Image Vis Comput 107. 
https://​doi.​org/​10.​1016/J.​IMAVIS.​2021.​104110
	24.	 Xiao R, Zhang ZL, Wu YY, Jiang PY, Deng J (2021) Multi-scale 
information fusion model for feature extraction of converter trans-
former vibration signal. Measurement 180:109555. https://​doi.​org/​
10.​1016/J.​MEASU​REMENT.​2021.​109555
	25.	 Burt PJ, Adelson EH (1987) The Laplacian pyramid as a compact 
image code. Read Comput Vis 31(4):671–679. https://​doi.​org/​10.​
1109/​TCOM.​1983.​10958​51
	26.	 Nair D, Sankaran P (2020) A modular architecture for high 
resolution image dehazing. Signal Process Image Commun 
92(3):116113. https://​doi.​org/​10.​1016/j.​image.​2020.​116113
	27.	 Zhao QL, Li ZM, Dong JY (2019) Unsupervised representation 
learning with Laplacian pyramid auto-encoders. Appl Soft Com-
put J 85(C):105851–105851. https://​doi.​org/​10.​1016/j.​asoc.​2019.​
105851
	28.	 Gu JY, Wei MT, Guo YY, Wang HX (2021) Common spatial pat-
tern with ­L21-norm. Neural Process Lett 53(5):1–20. https://​doi.​
org/​10.​1007/​S11063-​021-​10567-X
	29.	 Li R, Wang XD, Quan W, Song YF, Lei L (2020) Robust and 
structural sparsity auto-encoder with ­L21-norm minimization. 
Neurocomputing 425:71–81. https://​doi.​org/​10.​1016/j.​neucom.​
2020.​02.​051
	30.	 Liu GQ, Ge HW, Yang JL, Wang SX (2021) Robust semi non-
negative low-rank graph embedding algorithm via the ­L21 
norm. Appl Intell 52(8):8708–8720. https://​doi.​org/​10.​1007/​
S10489-​021-​02837-X
	31.	 Li R, Wang X, Lei L (2019) ­L21-norm based loss function and reg-
ularization extreme learning machine. IEEE Access 7:6575–6586. 
https://​doi.​org/​10.​1109/​ACCESS.​2018.​28872​60
	32.	 MNIST dataset. http://​yann.​lecun.​com/​exdb/​mnist. Accessed 6 
June 2021
	33.	 Fashion-MNIST dataset. https://​github.​com/​zalan​dores​earch/​fashi​
on-​mnist. Accessed 22 June 2021
	34.	 CIFAR-10 dataset. http://​www.​cs.​toron​to.​edu/​~kriz/​cifar.​html. 
Accessed 30 June 2021
	35.	 USPS dataset. http://​www.​gauss​ianpr​ocess.​org/​gpml/​data. 
Accessed 26 June 2021
	36.	 UCI-ISOLET dataset. http://​archi​ve.​ics.​uci.​edu/​ml/​datas​ets/​isolet. 
Accessed 12 July 2021
	37.	 UCI-Pendigits dataset. http://​archi​ve.​ics.​uci.​edu/​ml/​datas​ets/​Pen-​
Based​Recog​nitio​nofHa​ndwri​ttenD​igits. Accessed 6 June 2021
	38.	 UCI-Ecoli dataset. http://​archi​ve.​ics.​uci.​edu/​ml/​datas​ets/​Ecoli. 
Accessed 20 July 2021
	39.	 Zhang GH, Cui DS, Mao SB, Huang GB (2020) Unsupervised fea-
ture learning with sparse Bayesian auto-encoding based extreme 
learning machine. Int J Mach Learn Cybern 11(3):1557–1569. 
https://​doi.​org/​10.​1007/​s13042-​019-​01057-7
	40.	 Chai ZL, Song W, Wang HL, Liu F (2019) A semi-supervised 
auto-encoder using label and sparse regularizations for classifica-
tion. Appl Soft Comput J 77:205–217. https://​doi.​org/​10.​1016/j.​
asoc.​2019.​01.​021

3925
International Journal of Machine Learning and Cybernetics (2022) 13:3909–3925	
1 3
	41.	 Quintanar-Reséndiz AL, Rodríguez-Santos F, Pichardo-Méndez 
JL, Delgado-Gutiérrez G, Ramírez OJ, Vázquez-Medina R (2021) 
Capture device identification from digital images using Kullback–
Leibler divergence. Multim Tools Appl 80(13):19513–19538. 
https://​doi.​org/​10.​1007/​S11042-​021-​10653-1
	42.	 Li YP, Cao WH, Hu WK, Wu M (2020) Abnormality detection for 
drilling processes based on Jensen–Shannon divergence and adap-
tive alarm limits. IEEE Trans Ind Inf 17(9):6104–6113. https://​
doi.​org/​10.​1109/​TII.​2020.​30324​33
	43.	 Takemura S, Takeda T, Nakanishi T, Koyama Y, Hirosaki N 
(2021) Dissimilarity measure of local structure in inorganic crys-
tals using wasserstein distance to search for novel phosphors. Sci 
Technol Adv Mater 22(1):185–193. https://​doi.​org/​10.​1080/​14686​
996.​2021.​18995​55
	44.	 Shuai R, Mu D, Tao Z (2013) Information hiding algorithm based 
on Gaussian pyramid and color field structure. Int J Dig Content 
Technol Appl 7(5):222–229. https://​doi.​org/​10.​4156/​jdcta.​vol7.​
issue5.​27
	45.	 Munoz MA, Villanova L, Baatar D, Smith-Miles K (2018) 
Instance spaces for machine learning classification. Mach Learn 
107(1):109–147. https://​doi.​org/​10.​1007/​s10994-​017-​5629-5
	46.	 Qiang N, Shen XJ, Huang CB, Wu SL, Abeo TA, Ganaa ED, 
Huang SC (2022) Diversified feature representation via deep 
auto-encoder ensemble through multiple activation functions. 
Applied Intelligence 52(9):10591–10603. https://​doi.​org/​10.​1007/​
s10489-​021-​03054-2
	47.	 Cao X, Luo YH, Zhu XY, Zhang LQ, Xu Y, Shen HB, Wang 
TJ, Feng Q (2021) Daeanet: dual auto-encoder attention network 
for depth map super-resolution. Neurocomputing 454:350–360. 
https://​doi.​org/​10.​1016/j.​neucom.​2021.​04.​096
	48.	 Yang DG, Karimi HR, Sun KK (2021) Residual wide-kernel deep 
convolutional auto-encoder for intelligent rotating machinery 
fault diagnosis with limited samples. Neural Netw 141:133–144. 
https://​doi.​org/​10.​1016/j.​neunet.​2021.​04.​003
	49.	 Zhao X, Jia M, Liu Z (2021) Semi-supervised deep sparse auto-
encoder with local and non-local information for intelligent fault 
diagnosis of rotating machinery. IEEE Transactions on Instrumen-
tation and Measurement 70:1–13. https://​doi.​org/​10.​1109/​TIM.​
2020.​30160​45
	50.	 Hou YZ, Zhai JH, Chen JK (2021) Coupled adversarial variational 
autoencoder. Signal Process Image Commun 98(5786):116396. 
https://​doi.​org/​10.​1016/j.​image.​2021.​116396
	51.	 Song W, Li W, Hua ZY, Zhu FX (2021) A new deep auto-encoder 
using multiscale reconstruction errors and weight update correla-
tion. Inf Sci 559:130–152. https://​doi.​org/​10.​1016/J.​INS.​2021.​01.​
064
Publisher's Note  Springer Nature remains neutral with regard to 
jurisdictional claims in published maps and institutional affiliations
Springer Nature or its licensor holds exclusive rights to this article under 
a publishing agreement with the author(s) or other rightsholder(s); 
author self-archiving of the accepted manuscript version of this article 
is solely governed by the terms of such publishing agreement and 
applicable law.

