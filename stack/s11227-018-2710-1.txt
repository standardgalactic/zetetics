The Journal of Supercomputing (2019) 75:7827–7832
https://doi.org/10.1007/s11227-018-2710-1
Implementation of a 3D model heat equation using
fragmented programming technology
Darkhan Akhmed-Zaki1,2 · Danil Lebedev1,2 · Vladislav Perepelkin3,4
Published online: 30 November 2018
© Springer Science+Business Media, LLC, part of Springer Nature 2018
Abstract
Development of efﬁcient numerical programs for large distributed parallel computers
is a challenging problem. Many programming languages, systems and libraries exist
and evolve to help with it, yet the problem is far from being solved. Of interest are
particular application implementations’ studies, which reveal actual capabilities of a
system in the real computation. In this paper, the implementation of an indicative 3D
model heat equation parallel solver using fragmented programming technology and
LuNA system is investigated. A comparative testing with conventional MPI implemen-
tation is presented. The pros and cons of the approach are analyzed for corresponding
applications class.
Keywords Scalable numerical algorithms · Pipelined Thomas algorithm · LuNA
fragmented programming system · Parallel programming automation
1 Introduction
Nowadays, increasingly complex high performance computing (HPC) hardware
imposes increasingly strong demands on application software. HPC clusters often
comprise computing nodes of different performance or memory capacity, and non-
uniform network topology is also common. Some of computing nodes may contain
B Vladislav Perepelkin
perepelkin@ssd.sscc.ru
Danil Lebedev
danil.lebedev.0881@gmail.com
1
Al Farabi Kazakh National University, Almaty, Kazakhstan
2
University of International Business, Almaty, Kazakhstan
3
Institute of Computational Mathematics and Mathematical Geophysics SB RAS, Novosibirsk,
Russia
4
Novosibirsk State University, Novosibirsk, Russia
123

7828
D. Akhmed-Zaki et al.
co-processors, such as GPUs or FPGAs. Programming for such an environment is
complex and error prone, and it requires high skills in system parallel programming.
Such skills are different from the ones an application programmer normally possesses.
In particular, one has to cope with network communications, processes’ and threads’
synchronization, load balancing, interaction with co-processors, etc. Moreover, the
program has to tune to certain hardware conﬁguration, which is generally unknown
until program start.
Under such constraints, an important role is played by programming systems, capa-
ble of dealing with the low-level problems. The more complex problems are, the more
intelligent programming systems have to be. In particular, a system not only has to
“know” the hardware it works with, but also to “understand” application code (and
data) peculiarities. Such knowledge allows the system to reconﬁgure and adapt appli-
cation code to given hardware in order to satisfy performance or other non-functional
requirements. Also, systems hide low-level programming details from the user.
Currently, there are a lot of systems and projects, aimed to ease the programming.
Worth mentioning are PaRSEC [2, 3], Charm++ [4, 5], Chapel [10], X10 [12], UCX
[13] and many others. Despite the big effort involved with their development, the
problem of parallel programming automation is still challenging and far from being
solved. Existence of many research projects indicates the necessity to further investi-
gate the problem. Of interest are particular studies, which reveal capabilities of certain
programming systems and approaches in dealing with real-life computations.
This paper is devoted to such a study. A model 3D equation solver is chosen as an
indicative iterative application on a 3D mesh. Computations mostly consist of solving
tridiagonal equations. The application is implemented with fragmented programming
technology [1], which is being developed in Institute of Computational Mathematics
and Mathematical Geophysics, SB RAS. The technology is aimed at automation of
implementation of numerical applications for multicomputers. The experimentation
is conducted on the computing cluster of Joint Supercomputing Centre of RAS [11].
2 LuNA system
System LuNA (Language for Numerical Algorithms) is an academic project of
ICMMG SB RAS to develop a programming system, capable of parallel execution of
numerical algorithms, represented in a resources-independent coarse-grained explic-
itly parallel form, called fragmented algorithm (FA). As compared to conventional
parallel programming, for example, with use of MPI, LuNA automates data and com-
putations distribution, network data transfer, scheduling of computations and a number
of other tasks, essential to MPI programs development.
FA is basically a potentially inﬁnite bipartite directed acyclic graph of compu-
tational fragments (CF) and data fragments (DF) with arcs denoting informational
dependencies. Input, output or intermediate data of the algorithm are represented by
a set of DFs, each being an immutable aggregated variable (e.g., a subdomain of a
mesh at given time step or iteration). The set of DFs represents data decomposition,
each DF being a mesh domain, an array slice, etc. Computations of the algorithm are
represented by a set of CFs. Each CF is associated with an operation to compute and a
123

Implementation of a 3D model heat equation using…
7829
ﬁnite number of input and output DFs. Once values of input DFs of a CF are available
(computed), the CF may be executed to produce values of its output DFs. Execution
of a FA consists of execution of all its CFs. A FA is deﬁned in LuNA (Language for
Numerical Algorithms) programming language, which is a functional language. The
operations CFs perform are sequential side-effect-free (“pure”) subroutines in C++ (or
other conventional linking-compatible language). FA is interpreted and executed by
LuNA system on a multicomputer, automating CFs and DFs distribution to computing
nodes, transfer of input DFs to CFs, that consume them, scheduling and execution of
CFs in an order, which does not contradict their informational dependencies. LuNA
run-time system also performs dynamic load balancing and a number of other system
functions.
“Under-the-hood” LuNA system only uses scalable distributed algorithms, which
only employ communications between neighboring computing nodes (in sense of net-
work topology), which makes it scalable to potentially unlimited number of computing
nodes, provided the application FA itself is scalable enough.
One of the possibilities fragmented programming technology suggests is the ability
to construct alternative programs which implement the same FA, but possess different
non-functional properties (different behaviors, i.e., fragments distribution to nodes
over time and CFs execution order). LuNA run-time system, for instance, employs
dynamic interpretation of FA in order to implement it, with dynamic resources distri-
bution and CFs scheduling. In some cases, resources distribution and CFs scheduling
can be performed statically and deﬁned using an alternate run-time subsystem called
LuNA framework. Such approach eliminates signiﬁcant amount of run-time over-
head with the price of manual FA behavior speciﬁcation and reduction of program’s
dynamism.
3 Problem formulation and fragmented algorithm
The following model 3D heat equation in the unit cube [6] is considered:
ut  uxx + uyy + uzz
(1)
Initial conditions:
u(x, y, z, 0)  u0(x, y, z)
(2)
Boundary conditions:
u(0, y, z, t)  u(x, 0, z, t)  u(x, y, 0, t)  1
u(1, y, z, t)  u(x, 1, z, t)  u(x, y, 1, t)  1
(3)
To solve Eq. (1) with initial (2) and boundary (3) conditions, the scheme of Douglas
and Rachford [7] is applicable. The basic idea is to deﬁne intermediate steps for a time
step. The ﬁrst step gives a total approximation of the heat equation, and next steps
123

7830
D. Akhmed-Zaki et al.
are correction steps, which improve the numerical stability. Each intermediate step is
solved with Thomas algorithm. The difference scheme is:
un+1/3 −un
τ
 Λ1un+1/3 + Λ2un + Λ3un
un+2/3 −un+1/3
τ
 Λ2

un+2/3 −un
un+1 −un+2/3
τ
 Λ3

un+1 −un
(4)
For parallel implementation of the tridiagonal equation solution, the parallel
pipelined algorithm [8] was employed. It uses spatial domain decomposition for the
set of tridiagonal equations. At ﬁrst, a subset of equations is selected. Once a domain
of each equation is processed, the domain boundary values are transferred to the node,
containing the next domain, and the next subset is processed. The same scheme is
employed for both forward and backward computational steps of the tridiagonal equa-
tion solution.
Douglas and Rachford scheme was used in combination with the pipelined Thomas
algorithm because of the following reasons. Firstly, since all tridiagonal equations for
given dimension are independent, they can be computed using the pipelined Thomas
algorithm in parallel. Secondly, no edges’ exchanges (and therefore communications)
are required between intermediate steps. Thirdly, in [9] authors show applicability
with high efﬁciency of the pipelined Thomas algorithm for one intermediate step of
the scheme of Douglas and Rachford for a part of the whole domain.
The mesh un is decomposed into a 3D grid of domains each being a submesh DF.
Each domain is normally assigned to a computing node, which will store and process
the domain (best for a 3D mesh of computing nodes of corresponding size, but also
good for typical fat tree cluster topologies). Each intermediate step comprises multiple
solutions of the pipelined Thomas algorithm. The total number of tridiagonal equations
to solve is equal to multiplication of domain sizes in two dimensions, perpendicular to
the dimension of the equation. The pipelined Thomas algorithm possesses a parameter,
the portion size. Based on the experimental research, conducted in [9], we choose size
of subsets equal to y domain size, which makes z domain size a number of pipeline
cycles. After all three intermediate steps are ﬁnished, un+1 is obtained. Before passing
to the next time step, border mesh values of u are exchanged between neighboring
domains. Each intermediate or ﬁnal value of each domain corresponds to a DF and is
computed by a CF from other DFs according to informational dependencies.
4 Testing
Performance testing was conducted on MVS10P supercomputer of the Joint Super-
computer Centre of Russian Academy of Sciences [11]. It comprises 2×Xeon
E5-2690 CPU-based computing nodes with 64 GB RAM each. A sequential, MPI,
LuNA and LuNA framework programs were tested. The following parameters, repre-
sentative for such applications, were chosen: mesh size, from 1003 to 7003 with step
123

Implementation of a 3D model heat equation using…
7831
100 (in every dimension); number of processors, from 8 (23) to 216 (63) with step 1
(in each dimension).
Test results are shown in Table 1. It can be seen that hand-coded MPI program
has the best performance. The fragmented algorithm execution with LuNA frame-
work has lower performance, however comparable with MPI program’s performance.
Basic LuNA performance is signiﬁcantly lower. The performance decrease in the
implementations correlates with the increase in ease of programming. LuNA program
development and debugging is the simplest due to the absence of necessity to deal
with communications and resources distribution. LuNA framework program required
behavior (control and resources distribution) to be deﬁned without the need to program
it, while MPI implementation required both deﬁnition and hand-coding of control and
resources distribution. Test results also allow seeing system overhead and scalability
of different implementations of the algorithm.
It can be stated that currently fragmented programming technology and LuNA
system provide means for development and debugging of parallel programs in high
level of abstraction (LuNA language), but in order to achieve good performance,
additional fragmented programming execution tuning has to be performed (behavior
speciﬁcation with LuNA framework).
Table 1 Execution time (in seconds)
NP
Size
2003
3003
4003
5003
6003
7003
Sequential
13
1341.9
5443.0
19,406.0
29,195.5
50,728.4
82,882.3
MPI
23
250.3
950.4
2192.4
4263.9
7273.9
13,072.0
33
192.8
568.4
1264.4
2262.1
3185.2
5297.0
43
196.6
522.6
1027.0
1920.4
2971.7
4437.8
53
169.5
363.8
764.8
1273.6
2040.6
3130.9
63
130.1
346.3
593.1
1016.9
1412.4
2046.1
LuNA
23
2963.08
4557.84
7216.67
11,671.46
18,163.35
28,392.50
33
4935.30
7258.55
10,153.5
13,802.12
18,421.85
24,280.50
43
8222.12
11,776.55
15,928.4
20,830.20
26,620.70
33,428.60
53
2560.43
19,429.94
26,107.8
32,214.55
40,891.55
72236.53
LuNA-fw
23
349.18
1004.40
2632.80
6791.05
9373.10
16,960.20
33
202.28
587.67
1202.88
2462.91
4880.40
7537.10
43
1920.21
1654.13
2899.45
3412.50
4133.35
5576.15
53
1922.18
2554.05
7425.75
2806.90
3354.75
3956.15
63
1536.18
1425.01
2578.61
1894.11
2465.57
2935.53
123

7832
D. Akhmed-Zaki et al.
5 Conclusion
An implementation of a model 3D heat equation solver using Douglas and Rachford
scheme with the fragmented programming technology is considered. Comparative
performance tests of different implementations of the solver are conducted. The
comparative testing of MPI and LuNA implementations shows advantage of MPI
implementation over LuNA in sense of execution time, although LuNA program is
easier to develop due to programming automation, provided by LuNA system. The
semi-automated implementation with LuNA framework is in the middle in both senses
and provides competitive performance for processing of large problems.
The work was supported by Funding Science committee of Ministry of Education
and Science of Kazakhstan, Grants Nos. AP05134651 and BR05236340.
References
1. Malyshkin VE, Perepelkin VA (2011) LuNA fragmented programming system, main functions and
peculiarities of run-time subsystem. In: Proceedings of the 11th International Conference on Parallel
Computing Technologies, LNCS 6873. Springer, pp 53–61
2. Bosilca G, Bouteiller A, Danalis A, Faverge M, Herault T, Dongarra J (2013) PaRSEC: exploiting
heterogeneity to enhance scalability. IEEE Comput Sci Eng 15(6):36–45. https://doi.org/10.1007/978-
3-642-23178-0_5
3. Bosilca G, Bouteiller A, Danalis A, Faverge M, Haidar A, Herault T, Kurzak J, Langou J, Lemarinier P,
Ltaeif H, Luszczek P, YarKhan A, Dongarra J (2011) Flexible development of dense linear algebra algo-
rithms on massively parallel architectures with DPLASMA. In: Proceedings of the Workshops of the
25th IEEE International Symposium on Parallel and Distributed Processing (IPDPS 2011 Workshops).
IEEE, Anchorage, Alaska, USA, 16–20 May 2011, pp 1432–1441
4. Charm++. http://charm.cs.uiuc.edu. Accessed 1 May 2018
5. NAMD: Scalable molecular dynamics library. http://www.ks.uiuc.edu/Research/namd/. Accessed 1
May 2018
6. Tikhonov AN, Samarsky AA (1977) Equations of mathematical physics. M. Nauka, Novosibirsk (in
Russian)
7. Yanenko NN (1967) Method of fractional step for solution of multi-dimensional problems of mathe-
matical physics. M. Nauka, Novosibirsk
8. Sapronov IS, Bykov AN (2009) Parallel pipelined algorithm. Atom 2009, no 44, pp 24–25 (2009) (in
Russian)
9. Akhmed-Zaki DZh, Lebedev DV, Perepelkin VA (2016) Comparison of efﬁciency of parallel imple-
mentation of the tridiagonal SLAE solver: parallel pipelined method, parallel solver. Vestnik KazNU,
mathematics, mechanics, informatics, no 3(91), Almaty, pp 75–85 (in Russian)
10. Chapel. https://chapel-lang.org/. Accessed 1 May 2018
11. Joint Supercomputing Center of Russian Academy of Sciences. http://www.jscc.ru/resources/hpc/.
Accessed 1 May 2018
12. X10 programming language. http://x10-lang.org/. Accessed 1 May 2018
13. UCX. http://www.openucx.org/. Accessed 1 May 2018
123

