Molecular Imaging in Nano MRI

FOCUS SERIES
Series Editor Pierre-Noël Favennec
Molecular Imaging
in Nano MRI
Michael Ting

First published 2014 in Great Britain and the United States by ISTE Ltd and John Wiley & Sons, Inc.
Apart from any fair dealing for the purposes of research or private study, or criticism or review, as
permitted under the Copyright, Designs and Patents Act 1988, this publication may only be reproduced,
stored or transmitted, in any form or by any means, with the prior permission in writing of the publishers,
or in the case of reprographic reproduction in accordance with the terms and licenses issued by the
CLA. Enquiries concerning reproduction outside these terms should be sent to the publishers at the
undermentioned address:
ISTE Ltd
John Wiley & Sons, Inc.
27-37 St George’s Road
111 River Street
London SW19 4EU
Hoboken, NJ 07030
UK
USA
www.iste.co.uk
www.wiley.com
© ISTE Ltd 2014
The rights of Michael Ting to be identified as the author of this work have been asserted by him in
accordance with the Copyright, Designs and Patents Act 1988.
Library of Congress Control Number: 2013956294
British Library Cataloguing-in-Publication Data
A CIP record for this book is available from the British Library
ISSN 2051-2481 (Print)
ISSN 2051-249X (Online)
ISBN 978-1-84821-474-3
Printed and bound in Great Britain by CPI Group (UK) Ltd., Croydon, Surrey CR0 4YY

Table of Contents
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . .
ix
Chapter 1. Nano MRI
. . . . . . . . . . . . . . . . . . . .
1
Chapter 2. Sparse Image Reconstruction
. . . . . .
7
2.1. Introduction . . . . . . . . . . . . . . . . . . . . . . .
7
2.2. Problem formulation . . . . . . . . . . . . . . . . . .
8
2.3. Validity of the observation model in MRFM . . .
9
2.4. Literature review . . . . . . . . . . . . . . . . . . . .
11
2.4.1. Sparse denoising . . . . . . . . . . . . . . . . . .
11
2.4.2. Variable selection . . . . . . . . . . . . . . . . .
12
2.4.3. Compressed sensing . . . . . . . . . . . . . . . .
12
2.5. Reconstruction performance criteria . . . . . . . .
13
Chapter 3. Iterative Thresholding Methods
. . . .
15
3.1. Introduction . . . . . . . . . . . . . . . . . . . . . . .
15
3.2. Separation of deconvolution and denoising . . . .
15
3.2.1. Gaussian noise statistics . . . . . . . . . . . . .
17
3.2.2. Poisson noise statistics . . . . . . . . . . . . . .
19
3.3. Choice of sparse denoising operator in the case
of Gaussian noise statistics
. . . . . . . . . . . . .
20
3.3.1. Comparison to the projected gradient method
23

vi
Molecular Imaging in Nano MRI
3.4. Hyperparameter selection . . . . . . . . . . . . . .
25
3.5. MAP estimators using the LAZE image prior . .
26
3.5.1. MAP1 . . . . . . . . . . . . . . . . . . . . . . . . .
28
3.5.2. MAP2 . . . . . . . . . . . . . . . . . . . . . . . . .
30
3.5.3. Comparison of MAP1 versus MAP2 . . . . . .
31
3.6. Simulation example . . . . . . . . . . . . . . . . . .
33
3.7. Future directions . . . . . . . . . . . . . . . . . . . .
41
Chapter 4. Hyperparameter Selection Using the
SURE Criterion . . . . . . . . . . . . . . . . . . . . . . . .
43
4.1. Introduction . . . . . . . . . . . . . . . . . . . . . . .
43
4.2. SURE for the lasso estimator . . . . . . . . . . . .
44
4.3. SURE for the hybrid estimator . . . . . . . . . . .
45
4.4. Computational considerations . . . . . . . . . . . .
46
4.5. Comparison with other criteria . . . . . . . . . . .
47
4.6. Simulation example . . . . . . . . . . . . . . . . . .
48
Chapter 5. Monte Carlo Approach: Gibbs
Sampling
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
53
5.1. Introduction . . . . . . . . . . . . . . . . . . . . . . .
53
5.2. Casting the sparse image reconstruction
problem in the Bayesian framework . . . . . . . .
54
5.3. MAP estimate using the Gibbs sampler . . . . . .
56
5.3.1. Conditional density of w . . . . . . . . . . . . .
57
5.3.2. Conditional density of a
. . . . . . . . . . . . .
58
5.3.3. Conditional density of sigma2 . . . . . . . . . .
58
5.3.4. Conditional density of σ2 . . . . . . . . . . . . .
60
5.4. Uncertainty in the blur point spread function . .
60
5.5. Simulation example . . . . . . . . . . . . . . . . . .
60
Chapter 6. Simulation Study . . . . . . . . . . . . . . .
65
6.1. Introduction . . . . . . . . . . . . . . . . . . . . . . .
65
6.2. Reconstruction simulation study . . . . . . . . . .
66

Table of Contents
vii
6.2.1. Binary-valued x . . . . . . . . . . . . . . . . . .
67
6.2.2. {0, ±1}-valued x . . . . . . . . . . . . . . . . . .
69
6.3. Discussion . . . . . . . . . . . . . . . . . . . . . . . .
71
Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . .
73
Index
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
77

Introduction
Magnetic resonance imaging (MRI) is a well-established
medical technology that can deliver two-dimensional (2D) or
three-dimensional (3D) reconstructed images of internal body
tissue. A variant, functional MRI (fMRI), has been used to
study brain activity. The contributions of MRI to medicine
were recognized in the 2003 Nobel Prize in Physiology or
Medicine, awarded to P.C. Lauterbur and P. Mansﬁeld.
The spatial resolution of an MRI image is typically in the
millimeter to sub-millimeter range. This is adequate for
visualizing internal body organs, tumors, etc. If we are
interested in visualizing molecules, however, the spatial
resolution needs to be in the nanometer to sub-nanometer
range. Nano MRI is a term that denotes the different
competing methods, all relying on magnetic resonance, that
aim to achieve this level of spatial resolution.
A nano MRI imaging method would have to overcome two
challenges. The ﬁrst challenge is sensitivity to a small signal
in the presence of noise and other imperfections since a single
spin would not emit a strong signal. The second challenge is
to deconvolve the sensor response from the readback image.
These are two challenges that image processing addresses.

x
Molecular Imaging in Nano MRI
Molecular imaging, and, in particular, molecular image
reconstruction involves sparse estimation since most of the
volume to be reconstructed is expected to be empty. Sparse
image reconstruction, however, is not a recent topic: this
inverse
problem
has
been
studied
by
practitioners
in
astronomy, where blur induced by the measurement process
had
to
be
removed
[LUC 94].
More
recently,
sparse
estimation has attracted attention from people interested in
applications such as compressed sensing (CS), where one
tries to reconstruct a signal based on a limited number of
samples [CAN 06].
This book is intended to be an exposition of the image
reconstruction algorithms speciﬁcally targeted for molecular
imaging in nano MRI. Although it tries to be complete in its
treatment of the subject, it is not exhaustive. Much more can
be written on the topic. The organization of work is as
follows. Chapter 1 introduces magnetic resonance force
microscopy (MRFM), a promising candidate in the quest to
achieve MRI on the atomic scale. Chapter 2 introduces the
topic of sparse image reconstruction and brieﬂy reviews the
literature.
Algorithms
developed
with
molecular
image
reconstruction in mind are presented in Chapters 3–5.
Finally, a simulation study is conducted in Chapter 6 to
compare the reconstruction algorithms.

1
Nano MRI
Of all the different methods that have aimed to achieve nanometer to sub-nanometer 3D
reconstructed images, MRFM has made the most progress. The method, however, has its
limitations. The biggest limitation is a very low temperature, in the order of several Kelvin.
Imaging living biological samples under such conditions may not be possible.
MRFM is based on ultra-sensitive force detection using a
submicron magnetic tip on a cantilever. Several protocols
have been used in experiments in order to couple the spin to
the dynamics of the moving cantilever. One such method, the
interrupted oscillating cantilever-driven adiabatic reversal
protocol
(iOSCAR),
was
successfully
used
to
detect
an
isolated electron spin.
A
schematic
diagram
of
an
OSCAR-type
MRFM
experiment for electron spin detection is shown in Figure 1.1.
As shown in the ﬁgure, a submicron ferromagnet is placed on
the tip of a cantilever and positioned close to an unpaired
electron
spin contained within the sample. An applied
radio-frequency (rf) ﬁeld serves to induce magnetic resonance
of the spin when the rf ﬁeld frequency matches the Larmor
frequency. Because the magnetic ﬁeld emanating from the tip
is highly inhomogeneous, magnetic resonance is spatially
conﬁned to a thin bowl-shaped region called the “resonant
slice”.
Molecular Imaging in Nano MRI
© ISTE Ltd 2014. Published by ISTE Ltd and John Wiley & Sons, Inc. 
, Michael Ting. 

2
Molecular Imaging in Nano MRI
Figure 1.1. Schematic of an OSCAR-type MRFM experiment for
electron spin detection
If the cantilever is forced into mechanical oscillation by
positive feedback, the tip motion will cause the position of the
resonant slice to oscillate. As the slice passes back and forth
through an electron spin in the sample, the spin direction will
be cyclically inverted due to an effect called adiabatic rapid
passage [WAG 98]. The cyclic inversion is synchronous with
the cantilever motion and affects the cantilever dynamics by
changing the effective stiffness of the cantilever. Therefore,
the spin–cantilever interaction can be detected by measuring
small shifts in the period of the cantilever oscillation. This
methodology has been successfully used to detect small
ensembles of electron spins [MAM 03a, STI 01], and even a
single spin [RUG 04].
In [MAM 03b], the point spread function (psf) of an MRFM
tip is derived under the following assumptions:
– The tip can be modeled as a point dipole.
– The spins are undergoing cyclic adiabatic inversion (CAI).
– There is no spin–spin coupling.
– Energy-based measurements are taken.

Nano MRI
3
Although
Figure.
1.1
shows
a
horizontal
cantilever,
experiments use a vertical cantilever vibrating in the x
direction. A horizontal cantilever cannot vibrate too closely to
the
sample
surface;
otherwise,
van
der
Waals
and
electrostatic forces will draw the tip onto the surface and
break
the
cantilever
[STO 97].
The
psf
for
a
vertical
cantilever vibrating in the x direction is given by
H(x, y, z) =
⎧
⎨
⎩

G(x,y,z)
G0
2 
1 −

s(x,y,z)
xpk
2
|s(x, y, z)| ≤xpk
0
|s(x, y, z)| > xpk
,
[1.1]
where xpk is the peak amplitude and G0 is a normalizing
constant [MAM 03b]. Let r =

x2 + y2 + z2, then
s(x, y, z) = Bres −Bmag(x, y, z)
G(x, y, z)
,
[1.2]
Bmag(x, y, z) =
3xzm
r5
2
+
3yzm
r5
2
+
m(2z2 −x2 −y2)
r5
+ Bext
2
[1.3]
and G =
∂
∂xBmag, which is
G(x, y, z) =
1
2Bmag(x, y, z)

−90m2x3z2
r12
−90m2xy2z2
r12
+18m2xz2
r10
+ 2

−2mx
r5
−5mx(−x2 −y2 + 2z2)
r7

×

Bext + m(−x2 −y2 + 2z2)
r5
 
.
[1.4]
The parameter set listed in Table 1.1 is used to illustrate
the 3D MRFM psf. Slices of the MRFM psf, which in their 3D

4
Molecular Imaging in Nano MRI
form resemble a trafﬁc cone, are illustrated in Figure 1.2–1.4
at three different z values. As z increases, the psf shrinks in
the x–y plane as well as decreasing in intensity. The 2D MRFM
psf is an x–y slice of the 3D psf. Note that the interior of the
2D MRFM psf is zero; thus, the noiseless MRFM response of
an electric spin consists of two crescents facing each other.
Parameter
Value
Description
Name
Amplitude of external magnetic ﬁeld
Bext
8.817 × 103 G
Value of Bmag in the resonant slice
Bres
104 G
Radius of tip when modeled as a sphere
R0
3 nm
Distance from tip to sample
d
3 nm
Cantilever tip momenta
m
1.923 × 105 emu
Peak cantilever swing
xpk
0.246 nm
Maximum magnetic ﬁeld gradientb
Gmax
406.9 G/nm
aAssuming a spherical tip.
bAssuming optimal sample position.
Table 1.1. Parameters used to illustrate the MRFM psf
Figure 1.2. Slice in the x–y plane at z = 6 nm

Nano MRI
5
Figure 1.3. Slice in the x–y plane at z = 6.361 nm
Figure 1.4. Slice in the x–y plane at z = 6.722 nm

2
Sparse Image Reconstruction
2.1. Introduction
In nano MRI, the readback image is a blurred and noisy
version of the pristine image. The latter term denotes the
object (molecule) of interest. Image reconstruction techniques
have been applied in a variety of ﬁelds: for example, MRI,
astronomy and image deblurring. The particularity of image
reconstruction in nano MRI is that the pristine image is
sparse, something that also holds true for astronomy. By
using an algorithm that takes into account the sparsity of the
pristine image, we can obtain a better reconstructed image.
Making use of a constraint reduces the search space of the
reconstructed image: it is therefore advantageous to use all
possible constraints.
The pristine image will be assumed to be sparse. For
MRFM, in particular, the non-zero image values are assumed
to be non-negative valued for a reason mentioned later in
section 2.3. It might be possible to impose a stricter condition
on the non-zero values: for example, they may come from a
ﬁnite set. This stricter condition will not be explored in this
work. Additive white Gaussian noise will be assumed for the
observation noise.
Molecular Imaging in Nano MRI
© ISTE Ltd 2014. Published by ISTE Ltd and John Wiley & Sons, Inc. 
, Michael Ting. 

8
Molecular Imaging in Nano MRI
There are a number of methods that address sparse image
reconstruction. These methods can be loosely classiﬁed as
either non-Bayesian or Bayesian. The former consists of
methods that do not provide a probabilistic interpretation of
constraints, if they do exist, on the image, noise level, etc. The
latter consists of methods that establish probability prior(s)
on
the
unknown parameters.
Many
algorithms can
be
formulated from a non-Bayesian perspective as well as from a
Bayesian perspective; the two interpretations complement
each other and shed light on the algorithm’s behavior.
2.2. Problem formulation
By enumerating a 2D or 3D array in an order (e.g.,
column- or row-major order), we can equivalently represent a
2D or 3D image by a vector. Without loss of generality,
unnecessarily, denote by x ∈RM the pristine image and by
y ∈RN the observation. Let yi ∼N (hT
i x, σ2), where N (μ, Σ)
denotes
the
Gaussian
distribution
with
mean
μ
and
covariance matrix Σ. The observation model can be written
as
y = Hx + w, w ∼N (0, σ2I),
[2.1]
where H  (h1| . . . |hN)T ∈RN×M represents the system psf.
If H were orthonormal, then [2.1] would degenerate into a
denoising problem. Deﬁne the signal-to-noise ratio (SNR) as:
SNR  10 log10(
Hx
2
2/σ2).
The basic version of the sparse image reconstruction
problem is: given H, σ2 and y, estimate x knowing that it is
sparse. With nano MRI, the image sparsity is in the standard
basis of RM. In other applications where sparsity exists in
another domain, e.g. some wavelet basis, it is possible to
transform the observation model to [2.1]. More advanced
versions of the problem include the case when: (1) σ2 is

Sparse Image Reconstruction
9
unknown; (2) H is approximately known and σ2 is unknown;
or (3) both H and σ2 are unknown.
The distribution of non-zero values of x is an important
aspect in the problem formulation. An image reconstruction
algorithm that uses the correct non-zero distribution would
be expected to perform better than one that does not. In
general, the non-zero xi can assume any non-zero value. In
the case when the non-zero xi comes from a subset of R\{0},
an algorithm that does not make use of this information
would be expected to have poorer performance than the
algorithm that does. In short, the reconstruction algorithm
should make use of hypotheses that match the problem, no
more and no less. This, however, may not always be known a
priori, so a mismatch can occur.
Another salient aspect of the problem is the nature of the
matrix H. For nano MRI, an increased number of observations
can be expected to increase cost or processing time, so ideally
M would be small. On the other hand, in order to achieve good
resolution, N can be expected to be large. If M < N, [2.1] with
σ2 = 0 would be an underdetermined system.
2.3. Validity of the observation model in MRFM
In experiments that use the iOSCAR detection protocol,
the measurements yi are taken according to the schematic
illustrated in Figure 2.1. As can be seen, yi is a ﬁltered
energy statistic. The clock signal used in the generation of
the in-phase and quadrature-phase signals sI(t) and sQ(t),
respectively, comes from the pulses of the rf ﬁeld B1(t). The
sources of noise in the measurements include: phase lock loop
(PLL) noise, interferometer noise and system thermal noise.
The major noise contributor is the PLL, and its phase noise
can be characterized as approximately narrowband Gaussian
around the bandwidth of interest.

10
Molecular Imaging in Nano MRI
LPF
LPF
+
−
interferometer
measurement
PLL
phase
shift
CLK
 (·)2
fs
fs
yi
s(t)
sQ(t)
sI(t)
-90◦
 (·)2
Figure 2.1. Schematic of energy-based measurements for image
reconstruction under iOSCAR
Consider a simpliﬁed analysis of the noise at the output of
Figure 2.1. The assumptions that will be made are: (1) the
noise
variance
in
the
samples
of
the
in-phase
and
quadrature-phase signals is unity, (2) there are no random
spin ﬂips and (3) ignore the effect of the lowpass ﬁlter (LPF),
which introduces correlation across the samples of each
branch. Let Gi, 1 ≤i ≤M denote the independent and
identically distributed (i.i.d.) Gaussian random variables
(r.v’s) Gi ∼N (0, 1). The quadrature-phase lower branch after
M samples equals M
i=1 G2
i
∼χ2
M. Let δi be the response
induced by the iOSCAR protocol: if no spin is present, δi ≡0,
whereas in the presence of a spin, δi is a telegraph signal. Let
Ji ∼N (0, 1) be the noise in the in-phase upper branch, where
Gi, Ji are independent. The upper branch after M samples
equals M
i=1(Ji + δi)2
∼χ2
M(λM), which is a non-central
chi-squared r.v. with non-centrality parameter λM  M
i=1 δ2
i .

Sparse Image Reconstruction
11
As M
→
∞, χ2
M
→d
N (M, 2M), whereas a normal
approximation
for
χ2
M(λ)
as
M
→
∞
is
N (M + λM, 2(M + 2λM)). The difference between the upper
and lower branches then is approximately N (λM, 4(M + λM))
as M
→
∞. In the low SNR regime, M

λM, and
var(yi) ≈4M. This is consistent with [2.1]. In the high SNR
regime, however, var(yi) would not be approximately equal
for all i , so [2.1] would not be a good representation.
2.4. Literature review
A complete review of sparse estimation algorithms is
beyond the scope of this text. Instead, a sampling of some
relevant works is presented in order to highlight useful
learning
and
insight.
The
sparse
image
reconstruction
problem
is
related
to
three
other
extensively
studied
problems: (1) sparse denoising, (2) variable selection in linear
regression and (3) CS.
2.4.1. Sparse denoising
As previously mentioned, a special case occurs when H is
orthonormal. In this case, the sparse reconstruction problem
simpliﬁes into the sparse denoising problem. The latter has
appeared in the context of wavelet regression, where one
would
like
to
estimate
an
unknown
function
in
noise
[JOH 98,
CLY 99].
The
empirical
Bayes
method
in [JOH 98] used the sparse prior
xi|w, ν2 i.i.d.
∼(1 −w)δ(xi) + wφ(xi; 0, ν2).
[2.2]
A later publication by the same authors replaced the
normal
distribution
in
[2.2]
with
a
Laplacian
distribution [JOH 04], leading to
xi|w, a i.i.d.
∼(1 −w)δ(xi) + wγL(xi; a),
[2.3]

12
Molecular Imaging in Nano MRI
where γL(x; a) = (1/2)ae−a|x| is the Laplacian probability
density function (p.d.f.) with shape parameter a. An empirical
Bayes method using [2.3] achieves performance that was
within a constant of the asymptotic minimax error under
certain conditions [JOH 04]. Henceforth, refer to prior [2.3]
as the Laplacian with atom at zero (LAZE) prior. Other p.d.f’s
besides the Laplacian can be used with the same asymptotic
result, so long as the following properties hold true: (1) the
p.d.f. is heavy tailed, (2) it is unimodal and symmetric and (3)
it satisﬁes some regularity conditions.
2.4.2. Variable selection
Variable selection is an old topic in statistics, where we
need to choose the independent variables (covariates) that
are used to explain the observation. A plethora of methods
exist, from subset selection, ridge regression, the lasso and
the elastic net.
2.4.3. Compressed sensing
CS is a newer topic that is very closely related to variable
selection. The former has its genesis in the representation of
a signal in an overcomplete basis. When no noise is present
(σ2
=
0), problem [2.1] reduces to ﬁnding the sparsest
representation of y in terms of the columns of H
P0 : minimize 
x
0 such that y = Hx,
[2.4]
where 
 · 
0 denotes the vector l0 “norm”, deﬁned as 
x
0 
#{i : xi = 0}. The solution to P0 requires an enumerative
approach that is exponential in M. A convex relaxation of P0
that has lower computational complexity is
P1 : minimize 
x
1 such that y = Hx,
[2.5]

Sparse Image Reconstruction
13
where
the
vector
lp
norm
for
p
≥
1
is
deﬁned
as

x
p  (
i |xi|p)1/p. Under certain conditions, the solution of
P1 is equivalent to that of P0.
In traditional CS, the columns of H have low mutual
incoherence. This is not necessarily the case for nano MRI: it
depends on the psf. In the realistic case where noise is
present (σ2 > 0), the equivalence guarantee of P1 to P0 no
longer holds. Under certain conditions, however, P1’s solution
is close to P0’s solution [TRO 07].
Several algorithms that are used in compressed sensing
include
orthogonal
matching
pursuit
(OMP)
[TRO 07,
CAI 11], stagewise OMP [DON 06], and sparse Bayesian
learning (SBL) [WIP 04].
2.5. Reconstruction performance criteria
Given the pristine image x and its reconstruction ˆx, the
reconstruction error can be deﬁned in various ways [TIN 09].
Although the conventional way of assessing reconstruction
error is via the l2 norm, this choice may not provide a good
assessment of how “close” ˆx is to x. As a result, the “detection
error” criterion is used to assess whether ˆx has the same zero
and non-zero locations as x. Finally, the sparsity of ˆx is
assessed with respect to x. The three reconstruction criteria
used for the simulation study are listed in Table 2.1. The
valid range of each criterion is given in the third column,
assuming that x = 0.
To give a sense of what values can be expected of a
reasonably performant reconstructor, the error criteria are
evaluated in the case of the trivial zero reconstructor, where
ˆx = 0, and in the case of the perfect reconstructor, where
ˆx = x. These results are given in Table 2.2.

14
Molecular Imaging in Nano MRI
Description
Deﬁnition
Valid
range
Normalized l2 error norm
x −ˆx2/ x2
[0, ∞)a
Normalized detection error M
i=1 xor(I(xi = 0), I(ˆxi = 0))/x0 [0, M]a
Normalized l0 norm
ˆx0/ x0
[0, M]a
aAssuming that x is non-trivial, i.e. x = 0.
Table 2.1. Criteria used to evaluate the reconstruction performance.
Description
Zero reconstructor Perfect reconstructor
Normalized l2 error norm
1
0
Normalized detection error 1
0
Normalized l0 norm
0
1
Table 2.2. Criteria values for the zero and perfect reconstructor
Thus, while the normalized l2 error norm might have a
valid range of [0, ∞), a good reconstructor should attain a
value in the more restrictive range [0, 1]. A similar reasoning
can be applied to the normalized detection error. The ideal
normalized l0 norm, unity, is not an endpoint of the valid
range.

3
Iterative Thresholding Methods
3.1. Introduction
Application
of
the
Expectation
Maximization
(EM)
algorithm to the sparse image reconstruction problem allows
for
separation
of
the
deconvolution
and
denoising
subproblems. This was noted in [DAU 04] for the case of
Gaussian noise statistics, i.e. [2.1]. This separation principle
is generally stated in the following section; it does not rely on
the assumption of Gaussian noise statistics. The separation
into the deconvolution and denoising subproblems is useful
because it gives intuition to a wide range of iterative
reconstruction methods. This chapter focuses on iterative
reconstruction methods that can be generalized from the EM
algorithm.
3.2. Separation of deconvolution and denoising
From the system
x −→y,
[3.1]
Molecular Imaging in Nano MRI
© ISTE Ltd 2014. Published by ISTE Ltd and John Wiley & Sons, Inc. 
, Michael Ting. 

16
Molecular Imaging in Nano MRI
suppose that there exists an intermediate random variable z
such that, conditioned on z, y is independent of x, i.e.
p(y|z; x) = p(y|z). We now have
x −→z −→y.
[3.2]
In EM terminology, z plays the role of the complete
data [FES 77]. As z is an admissible complete data for p(y; x),
p(y, z; x) = p(y|z)p(z; x), and so log p(y, z; x) = log p(y|z)+
log p(z; x). Consider applying the EM algorithm to obtain the
maximum penalized likelihood estimate (MPLE) estimate of
x, which is
ˆx = argmaxx(log p(y|x) −pen(x)),
[3.3]
where pen(x) is a penalty function imposed on x. From a
Bayesian perspective, pen(x) is considered as the prior, so
[3.3] can be considered as a maximum a priori (MAP)
estimate. This can be seen by letting pen(x) = −log p(x),
where p(x) is the prior on x. With this, the function to be
maximized in the RHS of [3.3] is
log p(y|x) + log p(x) = log p(y, x) = log p(x|y) + log p(y),
[3.4]
leading to
ˆx = argmaxx(log p(x|y) + log p(y)) = argmaxx log p(x|y).
[3.5]
Since log is a monotonically strictly increasing function, the
maximizer of log p(x|y) is also the maximizer of p(x|y).
Let ˆx(n) denote the estimate of x at the nth iteration. At this
point, the Q function of the EM algorithm is
Q(x, ˆx(n)) = Ez[log p(z; x)|y, ˆx(n)] −pen(x) + K,
[3.6]
where K is a constant independent of θ.

Iterative Thresholding Methods
17
Suppose a judicious choice of z can be made such that the
right-hand side (RHS) of [3.6] assumes the form f(x, ˆz(n)) for
some suitable function f(·, ·) and ˆz(n) is an estimator of z at the
nth iteration. Then, a two-step estimation procedure occurs in
each EM iteration. In the nth iteration, the estimate ˆz(n) is
ﬁrst formed; then, the estimate ˆx(n) is formed from
ˆx(n) = argmaxx f(x, ˆz(n)).
[3.7]
The separation principle does not enforce sparsity in ˆx(n).
Sparsity is encouraged by appropriate selection of the penalty
function
pen(x).
While
the
EM
algorithm
ensures
a
monotonic increase in the objective function, it does not
guarantee
convergence
to
a
maximizer
in
general [FES 77, FIG 03]. However, if the likelihood function
is unimodal and certain differentiability criteria are met,
then
the
EM
algorithm
will
converge
to
the
unique
maximum [WU 83].
3.2.1. Gaussian noise statistics
Consider the model given by [2.1]. In [FIG 03], the
separation principle was obtained by selecting z = x + αw1,
where w1 ∼N (0, α2I). Then, y and z are related as follows
y = Hz + w2,
[3.8]
while the noise w2 ∼N (0, σ2I −α2HHT). Although w1 is
white Gaussian noise, w2 is colored. For a square matrix A,
the spectral radius ρ(A)

maxi |λi|, where λi are the
eigenvalues of A. The decomposition above only works if
(α/σ)2 ≤ρ(HHT )−1. Henceforth, without loss of generality,
assume that H is normalized so that ρ(HHT ) = 1. This can be
accomplished by scaling H and x.

18
Molecular Imaging in Nano MRI
Figure 3.1 depicts the decomposition. Estimating θ from z
is a denoising operation, and if w2
were not present,
estimating z from y is a deconvolution operation. In reality,
w2 is indeed present, but it will be seen below that this
decomposition principle still holds.
denoise
deconvolve
w1
H
y
w2
z
θ
Figure 3.1. Decomposition of the deconvolution and
denoising steps for Gaussian statistics
The Q function is
Q(x, ˆx(n)) = f(x, ˆz(n))
[3.9]
with
ˆz(n) = E[z|y, ˆx(n)] = ˆx(n) +
α
σ
2
HT (y −Hˆx(n)),
[3.10]
and f(θ, ·) is a quadratic function. We realize that [3.10] is a
Landweber (LW) iteration: consequently, it can be viewed as
the deconvolution step. The maximization of Q(x, ˆx(n)) can be
regarded
as a denoising step.
The two-step
estimation
procedure that occurs in each EM iteration can thus be
interpreted
as
a
separation
of
the
denoising
and
deconvolution subproblems.
Solving for the maximum of Q(x, ˆx(n)),
ˆx(n+1) = argmaxx

−1
2α2 
x −ˆz(n)
2
2 −pen(x)
	
.
[3.11]

Iterative Thresholding Methods
19
Equations [3.10] and [3.11] can be written more succinctly
as
ˆx(n+1) = D

ˆx(n) + cHT (y −Hˆx(n))

,
[3.12]
where D (·) is a denoising operation that depends on the form
of pen(·) and c = (α/σ)2. Since H is assumed to be normalized
so that ρ(HHT ) = 1, c can be set to 1. While the nano-MRI
problem is formulated in the setting of Gaussian noise
statistics, the separation principle in the case of Poisson noise
statistics is mentioned in section 3.2.2 to illustrate its
generality.
3.2.2. Poisson noise statistics
Consider the case when p(yi; x), 1 ≤i ≤N represents a
sequence of independent Poisson random r.v’s. In particular,
suppose that each yi is an observation from a Poisson r.v. so
that
yi ∼P0(yi; aT
i x),
[3.13]
where P0(n; λ) = e−λλn/n!, n ∈N is the Poisson probability
mass function. This model appears in emission tomography
reconstruction problems [KAY 97], where yi represents the
number of photons/positrons counted at the ith detector, xi
represents the emission density of the ith voxel and aij is the
conditional probability that a photon/positron emitted from
the jth voxel is detected by the ith detector. As a result,
p(y; x) = 
i P0(yi; aT
i x) and the log likelihood is
log p(y; x) = −

i
aT
i x +

i
yi log(aT
i x).
[3.14]
An admissible piece of complete data is {zij}, 1 ≤i ≤N, 1 ≤
j ≤M, where zij is the number of photons/positrons emitted

20
Molecular Imaging in Nano MRI
from the jth voxel and recorded at the ith detector [KAY 97].
Note that yi = 
j zij and zij ∼P0(zij; xjaij). In computing the
Q function of the EM algorithm at the nth iteration, n ≥1, we
obtain
Q(x, ˆx(n)) =

j

−xj

i
aij + log xj

i
ˆz(n)
ij

−pen(x),
[3.15]
where
ˆz(n)
ij
=
yiaij ˆx(n−1)
j

k aikˆx(n−1)
k
[3.16]
and constants independent of x have been dropped from [3.15].
If pen(x) ≡0, a closed-form solution for the maximization of
Q(x, ˆx(n)) is available [KAY 97]
ˆx(n)
j
= ˆx(n−1)
j

i yi · (aij/ 
k aikˆx(n−1)
k
)

i aij
.
[3.17]
The above application of the EM algorithm results in the
same separation principle as previously discussed. Each
iteration of the EM algorithm results in the estimation of the
intermediate variables {zij}1≤i≤N,1≤j≤M, followed by the
estimation
of
x.
As
yi
=

j zij,
the
estimation
of
{zij}1≤i≤N,1≤j≤M can be regarded as a deconvolution step.
3.3. Choice of sparse denoising operator in the case of
Gaussian noise statistics
As previously mentioned, the selection of a certain pen(·)
gives rise to a speciﬁc realization of the denoising operator
D (·) in [3.12]. One example is the lasso estimator, where the
penalty is the l1 norm of x, so pen(x) = β
x
1, β > 0. Let ei
denote the standard unit norm basis vectors in RM; deﬁne the
signum function and soft-thresholding rule as follows.

Iterative Thresholding Methods
21
DEFINITION 3.1.– The signum function is
sgn(x) 
⎧
⎨
⎩
1
x > 0
0
x = 0
−1
x < 0
.
[3.18]
DEFINITION 3.2.– For l ≥0, the soft-thresholding rule is
Ts(x; l)  (x −sgn(x)l)I(|x| > l).
[3.19]
For the lasso estimator, the corresponding D (·) is the
element-wise soft thresholding rule [DAU 04], i.e.
D (x) =

i
Ts(xi; β/2)ei.
[3.20]
Deﬁne the function T(·) to be a shrinkage rule iff T(·) is
anti-symmetric and increasing on (−∞, ∞). A shrinkage rule
that satisﬁes the property that T(x) = 0 iff |x| < t is a
thresholding rule with threshold t. One can verify that the
soft-thresholding rule Ts satisﬁes these conditions.
Inversely, the denoising subproblem can be accomplished
via a denoising operator that is the element-wise application
of a thresholding rule. In this case, one could ask what the
corresponding penalty function would be. Indeed, instead of
applying a ﬁxed denoising operator, one could use empirical
Bayes denoising (EBD), a method successfully applied to
denoise a vector of unknown sparsity [JOH 04]. When used as
the denoising operator D , the iterations [3.12] can be
regarded as an EM-like iteration, but where the penalty
function
(equivalently prior)
on x
is
designed
in each
iteration.
This
can
be
seen
by
applying
the
results
of [HYV 99] to create a prior on x such that the estimate x
formed via EBD can be seen as a MAP estimate. The ﬁrst
step is to recognize that the EBD thresholding rule T(·; φ)

22
Molecular Imaging in Nano MRI
induces a prior ˜p(xi; φ) on xi, where the thresholding rule is
parameterized by φ. Then,
˜p(xi; φ) ∝exp

−1
α2

[T −1(xi; φ) −xi] dxi

.
[3.21]
Let M(θ) 

[T −1(θ; φ) −θ] dθ. For the prior ˜p(·; φ) to exist,
exp(−M(x)/α2)
must
be
integrable
over
R,
as
˜p(x; φ) = exp(−M(x)/α2)/
 ∞
−∞exp(−M(x)/α2)dx. Since the
xi’s are i.i.d. in EBD,
˜p(x; φ) =
M

i=1
˜p(xi; φ).
[3.22]
Note that ˜p(xi; φ) might not bear any resemblance to the
prior on xi used in EBD. However, convergence of these
iterations has not been established.
If the hyperparameter φ were ﬁxed and D (·) takes a
speciﬁc
form,
concrete
results
can be obtained on
the
iterations [3.12]. Consider only thresholding rules with the
property that T(·) strictly increases outside of (−t, t), so that
T(·) is a bijection on R \ (−t, t). Several deﬁnitions are needed
before stating the results from [TIN 09]. With abuse of
notation, let T −1(·) denote the “inverse” of T(·), which will be
discontinuous at 0. Let T †(·) be an extension of T −1(·), deﬁned
as
T †(θ; φ) 
 0
θ = 0
T −1(θ, φ)
θ = 0 ,
[3.23]
and let ST,φ(x)  
1≤i≤M T(xi; φ) ei.
If D (x) = ST,φ(x), α = σ and 
H
2 < 1, the iterations
converge to a stationary point of the objective function

Iterative Thresholding Methods
23
Ψ(x; φ), where
Ψ(x; φ) = 
Hx −y
2
2 +
M

i=1
J1(xi; φ)
[3.24]
and
J1(θ; φ)  2T †(θ; φ)θ −θ2 −2
 s
0
T(ξ; φ) dξ
  
s=T †(θ;φ).
[3.25]
J1(θ; φ) is continuous for all θ ∈R.
As an example, consider the hybrid-thresholding rule,
which is a generalization of the hard- and soft-thresholding
rules. Use of this in the iterative thresholding framework
leads to the hybrid estimator.
DEFINITION 3.3.– For 0 ≤c2 ≤c1, the hybrid-thresholding
rule is
Thy(θ; c1, c2)  (θ −sgn(θ)c2)I(|θ| > c1).
[3.26]
Indeed, Ts(θ; c) = Th(θ; c, c), while the hard-thresholding
rule Th(θ; c)  θI(|θ| > c) = Thy(θ; c, 0), c
≥0. Letting
δc  c1 −c2, the element-wise penalty function is [TIN 09] 1
J1,h(θ)

I(|θ| < δc)[−(θ −I(sgn(θ) ≥0)c1)2 + 2c1c2]
+I(|θ| ≥δt)[2c2|θ| + c2
2].
[3.27]
3.3.1. Comparison to the projected gradient method
Let f(x) be a differentiable function and C a non-empty
closed convex set in RM. The optimization
argminx∈C f(x)
[3.28]
1 In [TIN 09, (20)], J1,h(θ) is wrongly written with sgn(θ) instead of
I(sgn(θ) ≥0).

24
Molecular Imaging in Nano MRI
can be solved via the projected gradient method. Let 
·
I be an
inner product norm and deﬁne the projection of some x ∈RM
onto C as
PC (x)  argminz∈C 
x −z
I .
[3.29]
Consider the sequence
xt+1 = PC (xt −αt∇f(xt))
[3.30]
for αt > 0. There are several convergence results for the
sequence (xt). One of them assumes that f(·) is Lipschitz
continuous
with
Lipschitz
constant
κ
>
0,
e.g.
|f(x) −f(z)| < κ
x −z
 for some norm 
 · 
. In this case, if
there exists  ∈(0, 1) so that  < αt < 2
κ(1 −), the sequence
(xt) from [3.30] converges to a stationary point of f(·).
Let f(x) = 
Hx −y
2
2, so ∇f(x) = 2HT (Hx −y). Since

∇f(x)
2
is unbounded, it is not Lipschitz continuous;
however, it is locally Lipschitz. Then, [3.30] becomes
xt+1 = PC

xt + 2αtHT (y −Hxt)

,
[3.31]
which closely resembles [3.12]. In the place of the denoising
operator D (·) in [3.12], [3.31] has the projection operator PC (·).
Consider
the
non-negative
Landweber
(NnegLW)
estimator, which is [3.31] with PC (x) = 
i xiI(xi ≥0)ei. This
PC (·) projects x ∈RM to the nearest non-negative x in the l2
sense.
In
light
of
the
convergence
result
mentioned
previously,
assuming
that
αt
are
suitably
chosen,
the
iterations will converge to the non-negative x that minimizes
the squared l2 error norm 
Hx −y
2
2. NnegLW ﬁts naturally
within the projected gradient framework of [3.31], whereas
the iterative thresholding framework of section 3.3 is not
applicable since the function f(x) = xI(x > 0) is not a
thresholding rule.

Iterative Thresholding Methods
25
D (·) serves a similar purpose as PC (·) in that it maps its
argument
into
a
more
desirable
space. D ’s
potentially
non-sparse argument is mapped into a vector that is sparser.
In contrast to C , however, the set of sparse images is not
convex. For example, consider SM(q)  {x ∈RM : 
x
0 ≤q}
for 0 ≤q ≤M. For 0 < q < M, SM(q) is not convex. Proximal
methods extend the projected gradient method, and similar
comparisons to the iterative thresholding framework can be
made [COM 11].
3.4. Hyperparameter selection
Recall that φ is the hyperparameter that parameterizes the
thresholding rule T(θ; φ). Since the thresholding rule can be
obtained from the penalty function and vice versa, φ can be
considered to parameterize pen(x) in [3.3]. From the Bayesian
perspective, φ parameterizes the prior on x. The ﬂexibility on
the penalty function/prior is necessary since the sparsity of x
is unknown a priori.
The hyperparameter φ can be estimated by marginalizing
the penalized likelihood over x, thus obtaining
ˆφ = argmaxφ

dx p(y|x; φ)e−pen(x;φ).
[3.32]
In Bayesian terms, by setting pen(x; φ) = −log p(x; φ), as
was done in section 3.2, ˆφ maximizes

dx p(y|x; φ)p(x; φ) =

dx p(y, x; φ) = p(y; φ).
[3.33]
It may be difﬁcult to evaluate the integral in [3.32]. Monte
Carlo methods can be used for this task. With ˆφ, x can be
estimated using the iterations [3.12].

26
Molecular Imaging in Nano MRI
One approach that avoids marginalizing over x is to jointly
estimate x and φ by maximizing the joint p.d.f. p(y, x; φ):
ˆx, ˆφ = argmaxx,φp(y, x; φ).
[3.34]
The criterion being maximized in [3.34], p(y, x; φ), is the
integrand in [3.33]. In general, the estimates ˆx, ˆφ obtained
from [3.34] are not going to be the same as those in the
previous paragraph. A reasonable argument, however, can be
made for [3.34] in the case of the observation model [2.1].
Since p(y, x; φ) = p(y|x; φ)p(x; φ) and p(y|x; φ) is independent
of φ, the estimators ˆx, ˆφ from [3.34] maximize
log p(y|x) + log p(x; φ),
[3.35]
which is similar to [3.3]. The ﬁrst term in [3.35] is the
likelihood of x giving rise to the observation y, whereas the
second term is the likelihood of x given the model parameters
φ. A potential disadvantage of the estimators is that the
second term of [3.35] contains a large number of degrees of
freedom, and so the overall maximization of [3.35] could be
computationally difﬁcult.
3.5. MAP estimators using the LAZE image prior
Consider using the LAZE prior [2.3] for x. Maximizing
[3.35] becomes difﬁcult due to the presence of the delta
function in the LAZE prior. As an alternative, consider the
mixed discrete-continuous version of the LAZE prior obtained
from deﬁning the r.v’s ˜xi and Ii, 1 ≤i ≤M, such that
xi = ˜xiIi. These latter two r.v’s are deﬁned as
Ii =
 0
with probability (1 −w)
1
with probability w
[3.36]
p(˜xi|Ii) =

g0(˜xi)
Ii = 0
g1(˜xi)  γL(˜xi; a)
Ii = 1 ,
[3.37]

Iterative Thresholding Methods
27
where g0(·) is a p.d.f. to be speciﬁed that may or may not
depend on φ. Recall that γL(·; a) is the Laplacian p.d.f. with
shape parameter a. In this section, φ = (a, w). Assume that
the Iis are i.i.d., and the ˜xis conditioned on I are also i.i.d., so
that p(I) = 
i p(Ii) and p(˜x|I) = 
i p(˜xi|Ii). The indicator r.v’s
Ii replace the Dirac delta function in [2.3].
Instead of [3.34], the optimality criterion can be extended
in a natural way to
ˆ˜x, ˆI, ˆφ = argmax
˜x,I,φ
p(y, ˜x, I; φ).
[3.38]
The optimality criterion in [3.38] is erronously cited
in [TIN 09, (7)] as the maximizers of p(˜x, I|y; φ). On the other
hand, p(y, ˜x, I; φ) = p(˜x, I|y; φ)p(y; φ), so the estimators for ˜x
and I in [3.38] can be considered as MAP estimators when φ
is held ﬁxed.
Deﬁne the sets I0  {i : Ii = 0} and I1  I0 = {i : I1 = 0}.
Maximization of p(y, ˜x, I; φ) is equivalent to maximizing
Ψ(˜x, I, φ)  −
Hx −y
2
2
2σ2
+ (M −
I
0) log(1 −w) + 
I
0 log w
+

i∈I1
log
1
2 ae−a|˜xi|

+

i∈I0
log g0(˜xi).
[3.39]
One approach to solving for the maximizer of [3.39] is to
apply a block coordinate maximization [TIN 09], where the
following two steps are alternated until the stopping criterion
is achieved: (1) while holding (˜x, I) constant, solve for φ that
maximizes [3.39] and (2) while holding φ constant, solve for
(˜x, I)
that
maximizes
[3.39].
The
discussion
on
g0(·)
necessarily intervenes again at this point since its form must
be known in order to perform the optimization. Consider two
possibilities for g0(·). In the ﬁrst case, g0(·) = g1(·), which will

28
Molecular Imaging in Nano MRI
give rise to the estimator MAP1. In the second case, g0(·) is a
p.d.f. that satisﬁes the following conditions: (1) |g(x)| < ∞for
all x ∈R, (2) sup g(x) is attained for some x ∈R and (3) g(x) is
independent of φ. The resulting estimator is called MAP2.
3.5.1. MAP1
Since g0(θ) = g1(θ) = γL(θ; a), Ψ(˜x, I, φ) in [3.39] is
Ψ1(˜x, I, φ)

−
Hx −y
2
2
2σ2
+ (M −
I
0) log(1 −w)
+
I
0 log w + M log a
2 −a
˜x
1.
[3.40]
At the t-th step, given (ˆ˜x(t−1), ˆI
(t−1)), the maximizer of Ψ1
for φ can be solved from ∇φΨ1 = 0, which leads to
ˆφ
(t) = (ˆa(t), ˆw(t)) =

M

ˆ˜x(t−1)
1
, 
ˆI
(t−1)
0
M

.
[3.41]
Given φ(t), the iterative thresholding framework based on
the EM algorithm can be applied to maximize Ψ1 with respect
to ˜x and I. The result is the E-step as in [3.10], while the M-
step is
ˆI(n+1)
i
=

I

|ˆz(n)
i
| > ˆa(t)α2 + κ

α, 1−ˆw(t)
ˆw(t)

0 < ˆw(t) ≤1
2
1
1
2 < ˆw(t) ≤1
[3.42]
ˆ˜x(n+1)
i
=

Ts(ˆz(n)
i
; ˆa(t)α2)
ˆI(n+1)
i
= 1
0
ˆI(n+1)
i
= 0
[3.43]

Iterative Thresholding Methods
29
for 1 ≤i ≤M, where κ(α, o) 

2α2 log o. Equations [3.42]
and [3.43] can be combined to produce an overall M-step
of [TIN 09]
ˆx(n+1)
i
=
⎧
⎪
⎨
⎪
⎩
Thy

ˆz(n)
i
; ˆa(t)α2 + κ

α, 1−ˆw(t)
ˆw(t)

, ˆa(t)α2
0 < ˆw(t) ≤1
2
Ts(ˆz(n)
i
; ˆa(t)α2)
1
2 < ˆw(t) ≤1
.
[3.44]
Denote the output of the iterations [3.10], [3.42] and [3.43]
by (ˆ˜x(t), ˆI
(t)). These will be used to calculate ˆφ
(t+1); we cannot,
therefore, use the combined M-step in [3.44]. The MAP1
algorithm is given in algorithm 3.1.
Algorithm 3.1. Iterative thresholding algorithm based on the LAZE
prior: MAP1
Require: H, ˆ˜x(0), ˆI
(0), α
1: t ←0
2: repeat {Use block coordinate optimization}
3:
t ←t + 1
4:
Calculate ˆφ
(t) using [3.41]
5:
n ←0
6:
ˆI
(t,0) ←ˆI
(t−1)
7:
ˆ˜x(t,0) ←ˆ˜x(t−1)
8:
repeat {Use iterative thresholding framework}
9:
Calculate ˆz(t,n) using [3.10]
10:
Calculate ˆI
(t,n+1) and ˆ˜x(t,n+1) using [3.42] and [3.43]
respectively.
11:
n ←n + 1
12:
until stopping condition for ˆ˜x(t,n−1), ˆI
(t,n−1) is satisﬁed
13:
ˆI
(t) ←ˆI
(t,n−1)
14:
ˆ˜x(t) ←ˆ˜x(t,n−1)
15: until stopping condition for ˆφ
(t), ˆI
(t), ˆ˜x(t) is satisﬁed
16: return ˆφ
(t), ˆI
(t), ˆ˜x(t)

30
Molecular Imaging in Nano MRI
3.5.2. MAP2
Due to the constraint (2) on g0(·) in section 3.5,
I1 = {i : xi = 0} w.p. 1.
[3.45]
Consequently, 
I
0 = 
x
0; using this in [3.39] results in
the optimization criterion to maximize being
Ψ2(˜x, I, φ)  −
Hx −y
2
2
2σ2
+ (M −
I
0) log(1 −w)
+ 
I
0 log w + 
x
0 log a
2 −a
x
1 +

i∈I0
log g0(˜xi).
[3.46]
As with MAP1, block coordinate maximization can be
implemented
in
two
substeps.
At
the
t-th
step,
given
(ˆ˜x(t−1), ˆI
(t−1)), the maximizer of Ψ2 for φ can be solved from
∇φΨ2 = 0, leading to
ˆφ
(t) = (ˆa(t), ˆw(t)) =


ˆx(t−1)
0

ˆx(t−1)
1
, 
ˆx(t−1)
0
M

.
[3.47]
Given
φ(t),
the
iterative
thresholding
framework
to
maximize Ψ2 has [3.10] as the E-step. Deﬁne
r(a, w)  2(1 −w) sup g0
wa
[3.48]
and let Lc(f)  {θ : f(θ) = c} denote the level set of f(·) where
c is attained. Lsup g0(g0) is the set of θ that maximizes g0(θ);
from condition (2) in section 3.5, it is non-empty. Let r(t) 

Iterative Thresholding Methods
31
r(ˆa(t), ˆw(t)) for ease of exposition. The M-step is
ˆI(n+1)
i
=

I(|ˆz(n)
i
| > ˆa(t)α2 + κ(α, r(t)))
r(t) ≥1
1
0 ≤r(t) < 1
[3.49]
ˆ˜x(n+1)
i
=

Ts(ˆz(n)
i
; ˆa(t)α2)
ˆI(t)
i
= 1
any element in Lsup g0(g0)
ˆI(t)
i
= 0.
[3.50]
Equations [3.49] and [3.50] are similar to [3.42] and [3.43],
respectively. Instead of the odds of obtaining a non-zero voxel,
(1 −ˆw(t))/ ˆw(t), r(t) is used instead. The latter has an extra
degree of freedom in sup g0. As was the case for MAP1, [3.49]
and [3.50] can be combined to obtain an overall M-step of
ˆx(n+1)
i
=
⎧
⎪
⎨
⎪
⎩
Thy

ˆz(n)
i
; ˆa(t)α2 + κ

α, r(t)
, ˆa(t)α2
r(t) ≥1
Ts(ˆz(n)
i
; ˆa(t)α2)
0 ≤r(t) < 1
.
[3.51]
The iterations [3.47] and [3.51] do not reference ˆ˜x and ˆI. It
is therefore unnecessary to keep track of
ˆI(n)
i
and ˆ˜x(n)
i
separately. The MAP2 algorithm is given in algorithm 3.2,
where g∗ sup g0.
3.5.3. Comparison of MAP1 versus MAP2
The MAP1 estimator can be implemented via a block
coordinate-wise
optimization
that
contains
no
extra
hyperparameter, while MAP2 requires the selection of an
extra hyperparmater g∗= sup g0. It is not clear how this
should
be
optimally
chosen.
The
iterations
for
the
optimization of ˜x and I in the two estimators are similar; the
iterations for MAP1 can be obtained from MAP2 upon setting
g∗= ˆa(t)/2.

32
Molecular Imaging in Nano MRI
Algorithm 3.2. Iterative thresholding algorithm based on the LAZE
prior: MAP2
Require: H, ˆx(0), α, g∗
1: t ←0
2: repeat {Use block coordinate optimization}
3:
t ←t + 1
4:
Calculate ˆφ
(t) using [3.47]
5:
n ←0
6:
ˆx(t,0) ←ˆx(t−1)
7:
repeat {Use iterative thresholding framework}
8:
Calculate ˆz(t,n) using [3.10]
9:
Calculate ˆx(t,n+1) using [3.51]
10:
n ←n + 1
11:
until stopping condition for ˆx(t,n−1) is satisﬁed
12:
ˆx(t) ←ˆx(t,n−1)
13: until stopping condition for ˆφ
(t), ˆx(t) is satisﬁed
14: return ˆφ
(t), ˆx(t)
If the root mean squared (rms) of the non-zero xi were
known a priori, then assuming that the non-zero values come
from a Laplacian distribution, one could solve for the shape
parameter a. The previous remark on the similarity between
the iterations of MAP1 and MAP2 can be used to set
g∗←a/2. The rms of a Laplacian distribution with shape
parameter a equals
√
2/a. As an example, if the rms of the
non-zero xi were equal to 1, then one could set g∗←
√
2.
There is more noticeable difference in the optimization of
the hyperparameter φ. The MAP1 estimate [3.41] can be
written as
ˆφ
(t) =

|I0| + |I1|

i∈I0 |ˆ˜x(t−1)
i
| + 
i∈I1 |ˆ˜x(t−1)
i
|
, |I1|
M

[3.52]

Iterative Thresholding Methods
33
while the MAP2 estimate [3.47] can be written as
ˆφ
(l) =

|I1|

i∈I1 |ˆ˜x(t−1)
i
|
, |I1|
M

.
[3.53]
While the estimator for w is the same, the estimator for a
is different. The MAP2 estimate is the maximum likelihood
(ML) estimate using only the ˆ˜xi in i ∈I1. On the other hand,
the MAP1 estimate is the ML estimate using all ˆ˜xi, even those
i ∈I0. This is contrary to the intent of the parameter a, which
is to model the variance in the non-zero xi via the exponential
p.d.f
3.6. Simulation example
To illustrate the MAP1 and MAP2 reconstructors2, assume
the observation model [2.1] with 2D binary-valued x and the
MRFM psf for H. An SNR of 20 dB is used to generate the
noisy observation y. In addition to MAP1 and MAP2, LW and
NnegLW iterations are shown for comparison purposes. For
MAP1 and MAP2, the iterations are continued until either
one of the following two stopping conditions holds: (1)

ˆx(t) −ˆx(t−1)
2 < 10−7 or (2) 2 × 105 iterations have been
reached. The LW reconstructor shares the same stopping
criterion (1), except that up to 5 × 105
iterations are
permitted. In the results presented below, a binary-valued
(32 × 32) x is generated such that 
x
0 = 8. While the number
of non-zero locations of x is ﬁxed, their locations are
randomly selected from the 14 × 14 center of the 2D image.
This is done for convenience, since, after convolution with the
MRFM psf, the blurred output will ﬁt into the 32 × 32 array.
The reconstructors do not have knowledge of the non-zero
location selection. Figures 3.2 and 3.3 depict x and y,
2 The term estimator and reconstructor are used interchangeably.

34
Molecular Imaging in Nano MRI
respectively. The latter is used for all four reconstructors in
this section. HTy is used as ˆx(0) for the reconstructors.
Figure 3.2. Pristine image x
Figure 3.3. Noisy observation y

Iterative Thresholding Methods
35
The MAP2 reconstructor was run using a test sequence of
thirteen g∗values. These follow a geometric progression, from
1/(4
√
2) to 8
√
2, where each term is
√
2 multiplied by the
previous term. Plots of the three reconstruction performance
criteria – normalized l2 error norm, normalized detection
error, and normalized l0 norm – are given in Figures 3.4–3.6.
Several values of g∗result in MAP2 estimates with the same
reconstruction performance criteria. The MAP2 reconstructor
obtained using g∗= 1/
√
2 is selected for comparison purposes
against the other reconstructors.
Figure 3.4. Normalized l2 error norm versus g∗
for MAP2 reconstructor
The
four
reconstructors’
output
is
displayed
in
Figures 3.7–3.10. Visually, NnegLW and MAP1 look most
similar to x. LW is too noisy, whereas MAP2 has noticeable
background artifacts. Table 3.1 presents the reconstruction
criteria. NnegLW and MAP1 have the better metrics than the
two other reconstructors, in agreement with the visual
assessment.

36
Molecular Imaging in Nano MRI
Figure 3.5. Normalized detection error versus g∗for MAP2
reconstructor
Figure 3.6. Normalized l0 norm versus g∗for MAP2 reconstructor

Iterative Thresholding Methods
37
Figure 3.7. Landweber reconstructor
Figure 3.8. Non-negative Landweber reconstructor

38
Molecular Imaging in Nano MRI
Figure 3.9. MAP1 reconstructor
Figure 3.10. MAP2 reconstructor with g∗= 1/
√
2

Iterative Thresholding Methods
39
Even though MAP2 has poorer reconstruction performance
criteria and has background artifacts, it, along with NnegLW
and MAP1, succeeds in locating the non-zero locations of x.
Figures 3.11–3.13 plot the non-zero ˆxi sorted in ascending
order for NnegLW, MAP1, and MAP2, respectively. Out of
these, MAP1 has the fewest number of spurious non-zero ˆxi.
This helps improve MAP1’s normalized detection error and
normalized
l0
norm,
but
does
not
greatly
affect
the
normalized l2 error norm since the spurious non-zero ˆxi
assume small values. A more extensive comparison will be
left to the simulation study in Chapter 6.
Reconstructor Normalized
Normalized
Normalized
l2 error norm detection error l0 norm
LW
2.1524
127.000
128.000
MAP1
0.0410
3.375
4.375
MAP2
0.1283
29.125
30.125
NNegLW
0.0460
5.250
6.250
Table 3.1. Reconstruction criteria for simulation examples
Figure 3.11. Sorted non-zero ˆxi of the Non-negative
Landweber reconstructor

40
Molecular Imaging in Nano MRI
Figure 3.12. Sorted non-zero ˆxi of the MAP1 reconstructor
Figure 3.13. Sorted non-zero ˆxi of the MAP2
reconstructor with g∗= 1/
√
2

Iterative Thresholding Methods
41
MAP1 and MAP2 are both empirical Bayesian methods. It
is therefore interesting to examine the estimate of their
hyperparameters.
The
actual
sparsity
of
x
is
8/322 = 7.8125 × 10−3. Given n i.i.d. samples x1, . . . , xn from a
Laplacian distribution with shape parameter a, the ML
estimate of a is ˆa = n/ n
i=1 |xi|. Using only the non-zero xi of
x yields ˆa = 1. The hyperparameter estimate for MAP1 is
ˆa ≈127.33, ˆw ≈1.0, whereas, for MAP2 with g∗= 1/
√
2, it is
ˆa ≈21.19, ˆw ≈0.235.
The large value of MAP1’s ˆa can be explained by [3.41]. If
ˆ˜x(t−1) were close to the true x, its l1 norm would be
approximately 8, so ˆa(t) ≈322/8 = 128. MAP1’s estimate
ˆw ≈1.0, on the other hand, seems unintuitive considering
Figure 3.11: this plot indicates that many ˆxi are zero.
However, recall that ˆxi = ˆ˜xi ˆIi. This suggests that many ˆ˜xi are
zero, since all ˆIi
=
1. An observation can be made in
considering
the
hyperparameter
re-estimation
equation
[3.41] and one of the two M-step equations in [3.42]. Once
ˆw(t) > 1/2, subsequent ˆw(u) = 1 for u > t. Ultimately, MAP1’s
ˆw is not a suitable estimate of the sparseness. Since MAP1’s

x
0 = 35, another estimate of ˆw is 35/322 ≈0.0342.
MAP2’s hyperparameter estimates are more reasonable
than those of MAP1, even if they are not close to ideal values
of 7.8125 × 10−3 and 1 for ˆw and ˆa, respectively. Part of this is
due to the large number of non-zero ˆxi, see Figure 3.13
(
x
0 = 241).
3.7. Future directions
The methods in this chapter assume that H and σ2 are
known; however, these may be unknown or partially known.
While it may be possible to estimate H and σ2 beforehand
and use the estimators as the “true” values, this may not be
optimal.

42
Molecular Imaging in Nano MRI
The proposed MAP1 and MAP2 estimators use the LAZE
prior to model x. If the non-zero xi were strictly positive,
however, there would be a mismatch in the prior. Rather than
use a Laplacian p.d.f. to model the non-zero xi, as in [2.3], an
exponential p.d.f. could be used.

4
Hyperparameter Selection Using the
SURE Criterion
4.1. Introduction
The focus of this chapter is not speciﬁcally on a particular
sparse reconstruction method, but rather on the estimate of
its hyperparameter(s). The previous chapter presented two
reconstruction methods where the hyperparameters were
estimated alongside x. This may not always be possible,
especially for a non-Bayesian method. Consider the lasso
estimator, which is known to be a sparse estimator [TIB 96].
Although
it
can
be
modiﬁed
to
become
a
Bayesian
estimator
[PAR 08],
the
modiﬁed
lasso
has
increased
computational complexity. A general, common approach to
select
hyperparameters
for
non-Bayesian
methods
is
cross-validation [HAS 01], which is compute-intensive, and in
the case of lasso, often unstable [PAR 08].
Generally, computing a risk of the form R(x, φ) requires
knowledge of the unknown x. Stein’s unbiased risk estimate
(SURE) is a way to calculate an unbiased estimate of
R(x, φ) [STE 81]. In section 3.4, φ was estimated either by
marginalizing over x or by mating a joint estimation with x in
order to maximize p(y, x; φ). In this chapter, φ is selected as
Molecular Imaging in Nano MRI
© ISTE Ltd 2014. Published by ISTE Ltd and John Wiley & Sons, Inc. 
, Michael Ting. 

44
Molecular Imaging in Nano MRI
the value that minimizes the unbiased estimate of the risk
ˆR(φ), which now only depends on φ. Thus,
ˆφ = argminφ ˆR(φ).
[4.1]
For [4.1] to be practical, however, ˆR(φ) has to be easy to
compute and optimize. Ideally, ˆR(φ) would admit a closed-form
expression.
4.2. SURE for the lasso estimator
The lasso estimator with regularization parameter φ = (β)
has an l1 norm penalty on x, and is deﬁned as
ˆx(y, β)  argminx{
y −Hx
2
2 + β
x
1}.
[4.2]
When the loss function is the l2 error norm, the risk
function of an estimator ˆx(y; φ) is customarily
Rx(x, φ)  EY 
ˆx(Y ; φ) −x
2
2.
[4.3]
Since [4.3] cannot be evaluated for the lasso estimator
using SURE, let μ  Hx, and consider
Rμ(x, φ)  EY 
ˆμ(Y ; φ) −μ
2
2 = EY 
Hˆx(Y ; φ) −Hx
2
2.
[4.4]
Assuming that the columns of H are linearly independent,
SURE for the lasso estimator is
ˆRμ(φ) = σ2 + 1
N 
e(φ)
2
2 + 2σ2
N 
ˆx(φ)
0,
[4.5]
where e(φ)  y −Hˆx(φ) is the reconstruction error [TIN 09,
(23)]. The lasso estimator with its hyperparameter selected via
minimization of the risk ˆRμ(φ) will be called SureLasso.

Hyperparameter Selection Using the SURE Criterion
45
When H = I, ˆR(φ) in [4.5] equals the SURE expression
derived in [DON 95, (11)]. In [NG 99, (4.10), (4.11)], SURE
was derived in the case of a diagonal H and an l1 penalty on
the wavelet coefﬁcients of x. This result can also be obtained
from [4.5].
Other criterion can be used to estimate φ, e. g. generalized
cross-validation (GCV) [TIB 96]. Maximum marginal ML,
however, is not a practical criterion to use unless one resorts
to
Monte
Carlo
methods.
The
integration
required
to
marginalize over x does not admit a closed-form expression.
4.3. SURE for the hybrid estimator
The hybrid estimator was described in section 3.3 starting
from
its
iterative
thresholding
implementation.
More
precisely, it is the estimator that minimizes the cost function

Hx −y
2
2 +
M

i=1
J1,h(xi; φ),
[4.6]
where φ = (t1, t2) and J1,h(·; φ) is deﬁned in [3.27]. Several
deﬁnitions are necessary before citing the SURE result.
DEFINITION
4.1.–
Suppose that x
∈
RM
has r
zero
components, where 1 ≤r ≤M. The permutation matrix
P(x)
∈
RM×M
is said to order the zero and non-zero
components of x if Pdiag(x)PT is a diagonal matrix with its
ﬁrst r diagonal elements equal to zero. For a given x, P(x) is
not unique.
DEFINITION 4.2.– For a matrix A = (aij) ∈Rq×r, let cn be a
sequence of valid row indices, and let dn be a sequence of valid
column indices. There can be no repetition in cn and dn. Then,
the submatrix A[cn, dn] = (αij) is obtained from αij = aci,dj.

46
Molecular Imaging in Nano MRI
Recalling that δc  c1 −c2, let
U(x) 
 diag

rec( x1
2δt), . . . , rec(xm
2δt)

δc > 0
0
δc = 0,
[4.7]
where rect(x) = 1 ⇐⇒|x| ≤1/2 and 0 otherwise. Denote by
G(H)  HT H the Gram matrix of H. Assuming that the
columns of H are linearly independent and G(H) does not
have an eigenvalue of 1/2, SURE for the hybrid estimator is
ˆRμ(φ) = σ2 + 1
N 
e(φ)
2
2 + 2σ2
N tr

K(φ)[K(φ) + L(φ)]−1
[4.8]
where:
K(φ)  (P(ˆx)G(H)P(ˆx))[(r + 1) : M, (r + 1) : M]
[4.9]
L(φ)  −1
2(P(ˆx)U(ˆx)P(ˆx))[(r + 1) : M, (r + 1) : M].
[4.10]
The dependence of ˆx on φ in [4.9] and [4.10] was not made
explicit in order to reduce clutter in the equations.
SURE for the hybrid estimator is more computationally
complex
as
compared
with
SURE
for
lasso,
see
[4.8]
versus [4.5]. To evaluate [4.8], K(φ)(K(φ) + L(φ))−1 would
have to be computed. If ˆx were sparse, (M −r) would be
small,
and
doing
the
matrix
inversion
would
be
less
computationally demanding.
4.4. Computational considerations
An important consideration in the use of SURE to estimate
the hyperparameters is the ease with which ˆx(y; φ) can be
calculated. This also holds for other criteria like GCV, since
the estimator ˆx appears in the hyperparameter optimization
criterion. The lasso enjoys the important advantage of having
a computationally efﬁcient implementation via least angle

Hyperparameter Selection Using the SURE Criterion
47
regression (LARS) [EFR 04] that lets one step through the
estimates in order of decreasing sparsity for the most part.
Criterion [4.5] can consequently be evaluated at various βi. If
the true x is sparse, the minimizer of ˆRμ(β) will be discovered
early in the LARS iterations.
A similarly efﬁcient algorithm that solves for the hybrid
estimator has not yet been discovered. There is only the
iterative thresholding algorithm, which does not leverage the
computation of ˆx(φ) to compute ˆx at another φ = φ. As a
result, a 2D search in t1 and t2 would have to be performed in
order to obtain the minimizer of [4.6].
4.5. Comparison with other criteria
Although σ2 appears in ˆRμ(φ) for the lasso and hybrid
estimators, it does not impact the minimization of the risk
estimator with respect to φ. Dropping σ2 from either [4.5] or
[4.8] results in a form that is reminiscent of the Cp statistic.
Indeed, [4.5] without σ2 corresponds to the Cp statistic
derived in [EFR 04, section 4]. The derivation holds when H
is a matrix whose column vectors are mutually orthogonal, or
more generally, when H satisﬁes the positive cone criterion
[EFR 04, (4.11)]. The linear independence constraint on the
columns of H, necessary for the result in [4.5], is less strict.
An approximation expression for the GCV statistic of the
lasso estimator is [TIB 96]:
GCV(φ) =

e(φ)
2
2/N
[1 −p(φ)/N]2 ,
[4.11]

48
Molecular Imaging in Nano MRI
where, recalling that φ = (β),
p(β)
=
tr

H(HT H + βW†)−1HT 
[4.12]
W
=
diag(|˜xi|)
[4.13]
˜x
=
(HT H + βW†)−1HT y,
[4.14]
and W† is the generalized inverse of W. The GCV criterion
can also be calculated without knowledge of σ2.
4.6. Simulation example
Use of the SURE criterion to select lasso’s hyperparameter
is illustrated assuming the observation model [2.1] under an
SNR of 20 dB. A random binary 2D x is generated, and the
MRFM psf using the same parameters as in section 3.6 is
applied to produce the blurred output. LW and NnegLW
iterations
will
not
be
compared
with
the
SureLasso
reconstructor in this section; for this, refer to the simulation
study in Chapter 6.
Figure 4.1 shows the SURE criterion given by [4.5]
versus the LARS iteration count, where 60 LARS iterations
are used. The estimated risk exhibits a signiﬁcant decrease
between LARS iteration count 1 to 12; thereafter, the
decrease in the risk is slight. In fact, the estimated risk
continues
to
decrease
until
it
achieves
an
“absolute”
minimum at iteration n = 50: see the zoomed-in region,
shown in Figure 4.2. When wanting to select a stopping point
for the regularization, the absolute minimum is not always
the best choice. The lasso reconstructor at higher n would
have more non-zero ˆxis. Instead, we would like to select an n
that has captured the vast majority of the estimated risk
decrease. One simple way of doing so is to continue accepting
the next highest n so long as the estimated risk decrease is
greater than a speciﬁed positive threshold.

Hyperparameter Selection Using the SURE Criterion
49
Figure 4.1. SURE criterion versus LARS iteration count
Figure 4.2. SURE criterion versus LARS iteration
count bigger than 16

50
Molecular Imaging in Nano MRI
In using a threshold of 0.1, the reconstruction at LARS
iteration n = 12 is selected. The lasso reconstructions at
n
=
12 and n
=
50 are shown in Figures 4.3 and 4.4
respectively. There is little difference visually between the
two reconstructions. The reconstruction performance criteria
mentioned in section 2.5 are presented in Table 4.1. For this
particular example, the metrics at n = 12 are all better than
those at n = 50. The biggest differences lie in the normalized
detection error and in the normalized l0 norm, which is
expected.
Figure 4.3. Lasso reconstructor at LARS step 12
LARS iteration Normalized
Normalized
Normalized
l2 error norm detection error l0 norm
12
3.7480
0.500
1.500
50
4.0369
4.250
5.250
Table 4.1. Reconstruction criteria for the lasso reconstructor at
different LARS iteration n
Figures 4.5 and 4.6 show the non-zero
ˆxi sorted in
ascending order for the lasso reconstructor at n = 12 and
n = 50, respectively. The larger n value introduced many

Hyperparameter Selection Using the SURE Criterion
51
small non-zero ˆxi values that are visually indistinguishable
from the background of zero.
Figure 4.4. Lasso reconstructor at LARS step 50
Figure 4.5. Sorted non-zero ˆxi of the lasso
reconstructor at LARS step 12

52
Molecular Imaging in Nano MRI
Figure 4.6. Sorted non-zero ˆxi of the lasso
reconstructor at LARS step 50

5
Monte Carlo Approach: Gibbs
Sampling
5.1. Introduction
Monte Carlo methods have been used in a wide range of
problems in signal and image processing. One of the most
commonly used methods is the Gibbs sampler, a Monte Carlo
Markov chain (MCMC) algorithm that is used to generate
samples from a multivariate p.d.f. that is difﬁcult to sample
directly. The Gibbs sampler can be considered as a specialized
form of the Metropolis–Hastings algorithm. The latter aims
to set up a Markov chain whose distribution converges to the
desired target distribution.
The canonical Gibbs sampling example supposes that
x
∈
RM
is
partitioned
into
P
<
M
partitions,
x
=
(x I1, . . . , x IP ), and that one would like to generate
samples from the distribution f(x). Denote by x(−Ii) the
elements of x not in x Ii, e.g. x(−I1)
= (x I2, . . . , x IP ) and
x(−I2) = (x I1, x I3, . . . , x IP ). Suppose that it is simpler to draw
samples
from
the
conditional
distributions
f(x Ii|x(−Ii)),
1 ≤i ≤P. Let x(t) denote the state of the Markov chain at
time t = 0, 1, . . .. The Gibbs sampler is given by algorithm 5.1.
Molecular Imaging in Nano MRI
© ISTE Ltd 2014. Published by ISTE Ltd and John Wiley & Sons, Inc. 
, Michael Ting. 

54
Molecular Imaging in Nano MRI
Algorithm 5.1. Canonical example of Gibbs sampler
Require: initial state of the Markov chain x(0)
1: t ←0
2: repeat
3:
t ←t + 1
4:
for i = 1 to P do
5:
x Ii(t) ←sample from f(x Ii|x(−Ii)(t −1))
6:
end for
7: until sufﬁcient number of converged samples acquired
8: return last sampled x(t) = (x I1(t), . . . , x IP (t))
When running the Gibbs sampler, indeed any MCMC
algorithm, we are concerned with the number of iterations
required for the Markov chain to approximate the target
distribution sufﬁciently well. A number of initial iterations of
the chain are typically discarded in order to ensure “burn-in”.
At this point, if i.i.d. samples of f(x) are desired, “thinning”
may be required [ROB 05, Chapter 12] since the samples
produced by the Gibbs sampler are correlated. Thinning
involves keeping every Kth sample for some sufﬁciently large
K and discarding the samples in between. This is not
required
when
computing
a
functional
of
the
form

h(x)f(x)dx. Under the appropriate conditions, the Ergodic
theorem can be applied so that n
t=1 h(xt) →E[h(X)] with
probability 1 as n →∞.
5.2. Casting the sparse image reconstruction problem
in the Bayesian framework
In [DOB 09], a mixture of the atom at zero and an
exponential distribution is used to model each r.v. xi. The xis
are assumed to be i.i.d., leading to
xi|w, λ i.i.d.
∼(1 −w)δ(xi) + wγE(xi; a),
[5.1]

Monte Carlo Approach: Gibbs Sampling
55
where γE(x; a)  ae−axI(x > 0) is the exponential p.d.f. The
noise variance σ2 is modeled by a distribution; unlike in
previous chapters, it is assumed to be unknown and a
quantity
to
be
estimated.
In
[DOB 09],
the
Jeffreys
non-informative prior is used for σ2, so that
p(σ2) ∝1
σ2 .
[5.2]
It is possible to introduce spatial correlation in σ2 as well
as spatially varying SNR. To enable a Bayesian treatment of
the image reconstruction problem, priors need to be assigned
for the hyperparameters a, w in [5.1], which will be assumed
to be independent. Non-informative priors are postulated:
a ∼IG (, ), w ∼U (0, 1)
[5.3]
with  > 0 being a small number. IG (α, β) denotes the inverse
gamma distribution with shape parameter α and scale
parameter β, while U (a, b) denotes the uniform distribution
with support (a, b), a < b.
Let Θ  {x, σ2} and Φ  {a, w}. The posterior distribution
of Θ and Φ is
f(Θ, Φ|y, ) ∝f(y|Θ)f(Θ|Φ)f(Φ|).
[5.4]
Assuming that x and σ2 are independent,
f(Θ|Φ) = f(x|a, w)f(σ2),
[5.5]
whereupon [5.4] becomes
f(Θ, Φ|y, ) ∝φ(y; Hx, σ2I)f(x|a, w)f(σ2)f(w)f(a|).
[5.6]

56
Molecular Imaging in Nano MRI
5.3. MAP estimate using the Gibbs sampler
While the full posterior is available in [5.6], only x is of
immediate
interest
in
the
sparse
image
reconstruction
problem [2.1]. The MAP estimate of x is obtained from
ˆx = argmax
x
f(x|y, ),
[5.7]
which is obtained by marginalizing the full posterior over σ2, a
and w. Draws from the full posterior are used to evaluate [5.7].
Let Nb be the burn-in period and Ns be the number of Gibbs
sampler iterations after the burn-in period. In [DOB 09], [5.7]
is implemented as
ˆx ≈
argmax
x∈{xNb+1,...,xNb+Ns}
f(x|y, ).
[5.8]
For [5.8] to produce an estimate that is close to the ˆx from
[5.7], the sequence X
 xNb+1, . . ., xNb+Ns must include
samples close to the true optimum ˆx. Use of the Gibbs
sampler to intelligently explore the search space is a form of
stochastic optimization. Since the full posterior given in [5.6],
not f(x|y, ), is being sampled, a larger Ns may be required
for [5.8] to hold.
The Gibbs sampler for the image reconstruction problem,
given in algorithm 5.2 below, is simply Algorithm 5.1 with
parameters speciﬁc to the image reconstruction problem: x,
its hyperparameters, and σ2 [DOB 09, algorithm 1]. Deﬁne
x(t) to be x at the tth iteration of the Gibbs sampler.
The initial state x(0) can be sampled from the distribution
of x|a, w, where xi|a, w is given by [5.1]. The initial σ2(0) can be
sampled from the Jeffreys non-informative prior [5.2]. Given
the ﬁnite representation of a real number in computing, it is
reasonable to limit the sampled σ2(0) to a reasonable range of

Monte Carlo Approach: Gibbs Sampling
57
values. The conditional densities for the Gibbs sampler used
in lines 4, 5, and 7 of algorithm 5.2 are given in the subsequent
sections.
Algorithm 5.2. Gibbs sampler for the image reconstruction problem
Require: initial state of the Markov chain x(0), initial
variance σ2(0), Nb, Ns
1: t ←0
2: repeat
3:
t ←t + 1
4:
w(t) ←sample from [5.9]
5:
a(t) ←sample from [5.10]
6:
x(t) ←sample from algorithm 5.3
7:
σ2(t) ←sample from [5.19]
8: until sufﬁcient number of converged samples acquired
9: return (x(Nb + 1), . . . , x(Nb + Ns))
In
addition
to
the
convergence
issue
mentioned
in
section 5.1, since the Gibbs sampler for the sparse image
reconstruction problem is used to explore areas of high
posterior probability in the hope that its samples come close
to the true optimum, there is another issue: coverage. In the
case that there are multiple local optima of f(x|y, ), there
needs to be a sufﬁcient number of samples so that all regions
of local optima are sufﬁciently well sampled.
5.3.1. Conditional density of w
Denote by Be (α, β) the beta distribution with parameters α
and β. The conditional density w|Θ, a, y,  is
w|Θ, a, y,  = w|x ∼Be(1 + 
x
0, 1 + M −
x
0).
[5.9]

58
Molecular Imaging in Nano MRI
5.3.2. Conditional density of a
The conditional density a|Θ, w, y,  is
a|Θ, w, y,  = a|x,  ∼IG (
x
0 + , 
x
1 + ).
[5.10]
5.3.3. Conditional density of x
While the density x|σ2, Φ, y,  cannot be expressed in closed
form,
a
closed
form
does
exist
for
the
p.d.f.
of
xi|x(−i), σ2, Φ, y, , where x(−i) is x with xi omitted, i.e.,
x(−i)
 [x1, . . . , xi−1, xi+1, . . . , xM]T . Several deﬁnitions are
required before the result can be stated. Consider the
Gaussian distribution N (μ, σ2) restricted to non-negative
values. The resulting r.v. is a truncated Gaussian; denote its
p.d.f. by φ+(·; μ, σ2). It can be shown that
φ+(x; μ, σ2) =
1
σ φ
 x−μ
σ

Φ
 μ
σ

I(x ≥0)
[5.11]
where φ(·) is the p.d.f. of the standard unit Gaussian r.v., and
Φ(·) is its cumulative distribution function (c.d.f.) Deﬁne ˜xi =
[˜xi,1, . . . , ˜xi,M]T as
˜xi,j 
 0
j = i
xj
j = i
[5.12]
so that ˜xi and x are equal at indices j = i. Finally, let
ei  y −H˜xi.
[5.13]
The p.d.f. of xi conditioned on x(−i), σ2, Φ, y,  is
f(xi|x(−i), σ2, Φ, y, )
∝
(1 −wi)δ(xi) + wiφ+(xi; μi, η2
i )
[5.14]

Monte Carlo Approach: Gibbs Sampling
59
where: η2
i
=
σ2

hi
2
2
[5.15]
μi
=
η2
i
hi, ei
σ2
−1
a

[5.16]
ui
=
w
a φ+(0; μi, η2
i )
[5.17]
wi
=
ui
ui + (1 −w)
[5.18]
The sampling of xt occurs sequentially from x1, ..., xM via
algorithm 5.3 [DOB 09, algorithm 2]. Let x(t, i)  [x1(t), . . . ,
xi(t), xi+1(t −1), . . . , xM(t −1)]T for 1 ≤i ≤M, where xi(t) is
the value of xi at time t. Deﬁne x(t, 0)  x(t −1, M). The same
notation is used also for ˜x(t, i) = [˜xi,1(t), . . . , ˜xi,i(t), ˜xi,i+1(t −
1), . . . , ˜xi,M(t −1)]T .
Algorithm 5.3. Gibbs sampling step for x
Require: x(t −1, M) = x(t, 0), σ2(t), a(t), w(t)
1: for i = 1 to M do
2:
H˜x(t, i) ←Hx(t, i −1) −xi(t −1)hi
3:
e(t, i) ←y −H˜x(t, i)
4:
Calculate [5.15]–[5.18] using e(t, i), σ2(t), a(t), w(t)
5:
Draw xi(t) according to [5.14]
6:
x(t, i) ←[x1(t), . . . , xi(t), xi+1(t −1), . . . , xM(t −1)]T
7:
Hx(t, i) ←H˜x(t, i) + xi(t)hi =
Hx(t, i −1) + [xi(t) −xi(t −1)]hi.
8: end for
9: return x(t, M)
Steps 2 and 7 of the algorithm exist to avoid recalculating
matrix-vector products; only scalar-vector products and vector
operations are needed. An arbitrary precision library may be
needed to calculate ui in [5.17].

60
Molecular Imaging in Nano MRI
5.3.4. Conditional density of σ2
The conditional density σ2|x, Φ, y,  is
σ2|x, Φ, y,  = σ2|x, y ∼IG
N
2 , 
e
2
2
2

[5.19]
5.4. Uncertainty in the blur point spread function
While the Gibbs sampler implementation of the MAP
estimator [5.8] does not require knowing σ2, H is assumed to
be
known.
In
[PAR 12],
H
is
assumed
to
depend
on
λ  (λ1, . . . , λK)T , i.e. H(λ). The λk’s, 1 ≤k ≤K are assumed
to be independent and uniformly distributed around 0. In
particular, each λk ∼U (−Δλk, Δλk), where Δλk > 0. A more
complicated
Gibbs
sampler
is
used,
where
the
Metropolis–Hastings algorithm is used in a sampling step.
5.5. Simulation example
This section is devoted to a simulation example illustrating
the proposed sparse reconstructor. The observation model [2.1]
is assumed, with the MRFM psf used for H. The pristine image
x is a (32×32) 2D random binary image generated in the same
fashion as in previous chapters. An SNR of 20 dB is used to
generate the noisy observation y. A comparison with the other
reconstructors is done in the next chapter.
Denote by ˆxt the estimate ˆx after t samples from the Gibbs
sampler.
The
burn-in
samples
are
discarded
from
consideration: for example, ˆx100 represents the estimate ˆx
after having seen (Nb + 100) samples. Figure 5.1 displays x,
while
Figures
5.2
and
5.3
display
ˆx 1000
and
ˆx 2000,
respectively.
Both
Gibbs
reconstructors
are
visually
indistinguishable. Their reconstruction performance metrics
are given in Table 5.1: the only difference is in the normalized

Monte Carlo Approach: Gibbs Sampling
61
l2 error norm, where ˆx 1000 is slightly better than ˆx 2000. This is
not a contradiction: ˆx is chosen to maximize the posterior,
see [5.8], not the l2 error norm.
Figure 5.1. Pristine image x
Ns
Normalized
Normalized
Normalized
l2 error norm detection error l0 norm
1000 0.0154
0.0
1.0
2000 0.0164
0.0
1.0
Table 5.1. Reconstruction criteria for the Gibbs reconstructor using
Ns = 1,000 and Ns = 2,000 samples
Samples from the posterior [5.6] can be used in other ways.
One example is estimation of the hyperparameters a and w,
or the noise variance σ2, via a sample mean. Conﬁdence
intervals can be calculated without having to resort to
bootstrapping [EFR 79]. Figures 5.4 and 5.5 contain the
histograms of σ2 and w, respectively, drawn from the full
posterior distribution [5.6], using all 2,000 samples (i.e., no
thinning was done). The 95% conﬁdence interval for σ2 is
(3.706 × 10−3, 4.447 × 10−3), whereas the actual σ2 used is

62
Molecular Imaging in Nano MRI
4.236 × 10−3.
The
95%
conﬁdence
interval
for
w
is
(0.00418, 0.01617), whereas the actual fraction of non-zero
values in x is 8/322 ≈0.00781.
Figure 5.2. Gibbs reconstructor using Ns = 1,000 samples. The
optimum is achieved at the 397th sample
Figure 5.3. Gibbs reconstructor using Ns = 2,000 samples. The
optimum is achieved at the 1268th sample

Monte Carlo Approach: Gibbs Sampling
63
Figure 5.4. Histogram of samples σ2 drawn
from the posterior distribution
Figure 5.5. Histogram of samples w drawn
from the posterior distribution

6
Simulation Study
6.1. Introduction
This
chapter
compares
and
contrasts
a
number
of
algorithms previously mentioned using the reconstruction
criteria in section 2.5. The reconstruction algorithms can be
studied under many different scenarios, e.g. varying SNR,
varying degrees of sparseness of the pristine image and
different psf’s. We can also consider mismatches in model
assumptions
in
order
to
assess
the
robustness
of
the
algorithms. For example, all of the algorithms presented
assume white Gaussian noise, but the noise could be spatially
correlated or even correlated with the pristine image. It is
therefore necessary to limit the scope of the simulation study
to a certain extent.
Computation time is an important aspect in evaluating
image reconstruction algorithms. In general, we would expect
a
more
compute-intensive
algorithm
to
have
better
performance. Comparing computation time across different
reconstruction algorithms, however, is not straightforward
since this depends on several factors such as the convergence
criterion, the software/hardware setup and optimizations in
the
source
code.
Due
to
the
different
nature
of
the
Molecular Imaging in Nano MRI
© ISTE Ltd 2014. Published by ISTE Ltd and John Wiley & Sons, Inc. 
, Michael Ting. 

66
Molecular Imaging in Nano MRI
reconstruction algorithms studied, one single convergence
criterion cannot be used. We would expect a stagewise
algorithm such as lasso to be faster than any iterative
thresholding or MCMC algorithm. An iterative thresholding
algorithm can be expected to be faster than an MCMC
algorithm such as the Gibbs sampler.
The simulations in this chapter were executed on a PC
with 8,192 MB of memory and an Intel Core i5-2500K
running Windows 7. The reconstructors were written in
Python and were executed using the 64 bit version of Python
2.7.3. Running Pystone produces a benchmark of 125,389
pystones/s.
6.2. Reconstruction simulation study
In this section, assume that x is a 2D 32 × 32 image and
that the xi are i.i.d. Two possible distributions for the non-
zero xi are considered: (1) the non-zero values of x equal 1
(x is a binary image); and (2) the non-zero values of x come
from {±1} with equal probability. In each of the two cases, two
sparsity levels of x (
x
0 = 8 and 
x
0 = 16) and two different
SNRs (SNR = 20 dB and SNR = 2 dB) are considered. The
simulations use a 2D MRFM psf and assume AWGN.
The reconstruction algorithms applied in this chapter are:
MAP1, MAP2 with g∗
=
1/
√
2, LW iterations, NnegLW
iterations, SureLasso and the Gibb sampler maximizing the
MAP criterion. MAP1, MAP2, LW and NnegLW are run using
the stopping criterion mentioned in section 3.6. SureLasso is
run using 30 LARS iterations. Finally, the Gibbs sampler is
run using Nb
=
300 burn-in samples and Ns
=
1,000
simulation samples.

Simulation Study
67
The number of independent simulations repeated for each
reconstructor is N = 30. Since some reconstruction criteria
results are not normally distributed, the median and median
absolute deviation are computed. The latter is scaled so that
it is a consistent estimator of the standard deviation when
the reconstruction criterion is i.i.d. Gaussian distributed. To
compare two reconstructors, the Mann–Whitney–Wilcoxon
test with continuity correction is used to test for a difference
in their medians.
6.2.1. Binary-valued x
Tables 6.1 and 6.2 list the 20 and 2 dB results, respectively,
for 2D binary-valued x. While NnegLW does not incorporate a
sparse assumption, it nonetheless produces an estimate that
is sparse, although not as sparse as Gibbs or SureLasso. In
the high SNR case, Gibbs has the lowest normalized l2 error
norm (p < 10−6 for both sparsity levels of x). In the low SNR
case, when 
x
0 = 8, NnegLW has a normalized l2 error norm
comparable to Gibbs (p = 0.6973) and lower than MAP2 (p =
3.463 × 10−6). When 
x
0 = 16, NnegLW’s normalized l2 error
norm is smaller than both Gibbs’ and MAP2’s (p < 10−6).
As might be expected, the LW reconstructor is not sparse
for both high and low SNR cases, as can be ascertained from
the normalized detection error and normalized l0 norm. In
the high SNR case, Gibbs perfectly estimates the non-zero
locations of
ˆx in all N
=
30 simulation runs. Gibbs
outperforms the other reconstructors as well in the low SNR
case. In terms of normalized l0 norm, SureLasso and Gibbs
come closest to the ideal value of 1 as compared to the other
reconstructors. Under low SNR, MAP1 for all N
=
30
simulations converges to the trivial 0 estimator. This makes
MAP1 unsuitable for low SNR reconstruction.

68
Molecular Imaging in Nano MRI
Reconstructor Normalized
Normalized
Normalized
Runtime (s)
l2 error norm
detection error
l0 norm
x0 = 8, SNR = 20 dB, N = 30
Gibbs
0.0210 (0.0040)
0.0000 (0)
1.0000 (0)
244.30 (3.25)
LW
2.0807 (0.2306) 127.0000 (0)
128.0000 (0)
178.46 (1.14)
MAP1
0.0540 (0.0114)
4.6875 (1.0193)
5.6875 (1.0193)
9.59 (3.70)
MAP2
0.1160 (0.0087)
32.0625 (1.0193)
33.0625 (1.0193)
97.64 (2.54)
NnegLW
0.0587 (0.0171)
6.5625 (1.2046)
7.5625 (1.2046)
7.71 (2.44)
SureLasso
3.7525 (0.2884)
0.5000 (0.2780)
1.3750 (0.1853)
0.32 (0.02)
x0 = 16, SNR = 20 dB, N = 30
Gibbs
0.0404 (0.0139)
0.0000 (0)
1.0000 (0)
245.39 (2.35)
LW
2.4597 (0.1869)
63.0000 (0)
64.0000 (0)
173.48 (1.51)
MAP1
0.0963 (0.0241)
3.3125 (0.4633)
4.3125 (0.4633)
12.67 (2.47)
MAP2
0.1671 (0.0206)
16.4688 (0.3707)
17.4688 (0.3707)
97.27 (5.15)
NnegLW
0.0974 (0.0189)
3.8125 (0.4633)
4.8125 (0.4633)
10.69 (2.91)
SureLasso
1.9641 (0.4911)
0.8750 (0.1853)
0.9375 (0.4633)
0.33 (0.05)
Table 6.1. Reconstruction criteria median
(median absolution deviation in parentheses) for
2D binary-valued x at SNR=20 dB (high SNR)
Reconstructor Normalized
Normalized
Normalized
Runtime (s)
l2 error norm
detection error
l0 norm
x0 = 8, SNR = 2 dB, N = 30
Gibbs
0.4481 (0.4170)
0.1250 (0.1853)
1.0000 (0)
245.63 (3.03)
LW
17.0702 (1.5936) 127.0000 (0)
128.0000 (0)
177.93 (1.56)
MAP1
1.0000 (0)
1.0000 (0)
0.0000 (0)
1.29 (0.02)
MAP2
0.5696 (0.1108)
7.3750 (0.9266)
8.3750 (0.8339)
8.58 (1.16)
NnegLW
0.4180 (0.0792)
6.4375 (1.0192)
7.4375 (1.0193)
7.44 (2.47)
SureLasso
2.0019 (0.6429)
0.8750 (0.3706)
1.6250 (0.7413)
0.33 (0.05)
x0 = 16, SNR = 2 dB, N = 30
Gibbs
0.9627 (0.1891)
0.8125 (0.2780)
0.8750 (0.0927) 248.66 (3.31)
LW
19.8505 (1.4909)
63.0000 (0)
64.0000 (0)
174.84 (1.03)
MAP1
1.0000 (0)
1.0000 (0)
0.0000 (0)
1.30 (0.03)
MAP2
0.8124 (0.0793)
3.7500 (0.4633)
4.3438 (0.6950)
10.06 (2.72)
NnegLW
0.7098 (0.0863)
3.8750 (0.2780)
4.7813 (0.3243)
9.70 (2.45)
SureLasso
1.6764 (0.5133)
1.0625 (0.1853)
0.7500 (0.5096)
0.32 (0.04)
Table 6.2. Reconstruction criteria median
(median absolution deviation in parentheses) for
2D binary-valued x at SNR=2 dB (low SNR)

Simulation Study
69
There
is
no
model
mismatch
to
any
of
the
six
reconstructors. In the case of MAP1, MAP2 and SureLasso,
where
there
is
no
assumption
of
non-negativity,
these
reconstructors
could
perhaps
have
improved
their
performance
had
they
incorporated
the
non-negativity
assumption. The improvement in NnegLW, which makes use
of the non-negativity assumption, against LW, is dramatic.
The
runtime
statistics
are
consistent
with
prior
expectations. Gibbs has the largest median runtime, followed
by LW. MAP1, MAP2 and NnegLW have median runtimes
that are in the scale of several seconds, whereas SureLasso
has the lowest median runtime.
6.2.2. {0, ±1}-valued x
Assume that xi takes on positive or negative values, and
that the xi are i.i.d. The non-zero xi come from the set {±1}
with equal probability. The purpose of this simulation case is
to test the Gibbs and NnegLW reconstructors under a model
mismatch, since these two reconstructors assume that x is
non-negative. Tables 6.3 and 6.4 list the 20 and 2 dB results,
respectively, for a 2D {0, ±1}-valued x.
Unsurprisingly, Gibbs and NnegLW perform worse than in
the case of binary-valued x. MAP1 and MAP2 have the lowest
normalized l2 error norm in the high- and low-SNR cases,
respectively (p < 10−6). These two reconstructors, however,
have the highest normalized detection error and normalized
l0 norm out of the group of sparse reconstructors.

70
Molecular Imaging in Nano MRI
Reconstructor Normalized
Normalized
Normalized
Runtime (s)
l2 error norm
detection error
l0 norm
x0 = 8, SNR = 20 dB, N = 30
Gibbs
0.8624 (0.1266)
0.7500 (0.1853)
0.3750 (0.1853) 269.39 (7.54)
LW
1.5555 (0.2603) 127.0000 (0)
128.0000 (0)
177.31 (0.82)
MAP1
0.0523 (0.0225)
7.9375 (1.2046)
8.9375 (1.2046)
10.51 (2.56)
MAP2
0.0991 (0.0153)
36.4375 (1.7605)
37.4375 (1.7606)
95.37 (2.18)
NnegLW
0.8218 (0.1072)
2.7500 (0.9266)
2.6250 (1.2973)
3.34 (0.88)
SureLasso
2.9577 (1.3423)
0.9375 (0.6486)
1.6875 (0.8340)
0.44 (0.06)
x0 = 16, SNR = 20 dB, N = 30
Gibbs
0.9487 (0.0810)
0.9375 (0.1853)
0.2500 (0.1853) 271.32 (8.20)
LW
1.5662 (0.2906)
63.0000 (0)
64.0000 (0)
177.33 (3.85)
MAP1
0.0771 (0.0236)
6.1250 (1.0193)
7.1250 (1.0192)
13.55 (3.78)
MAP2
0.1263 (0.0106)
20.7188 (1.3436)
21.7188 (1.3436)
95.34 (2.50)
NnegLW
0.8963 (0.0812)
1.8125 (0.1853)
1.5000 (0.5560)
6.04 (3.99)
SureLasso
1.7973 (0.3994)
1.2500 (0.4633)
1.3438 (0.4170)
0.44 (0.03)
Table 6.3. Reconstruction criteria median (median absolution
deviation in parentheses) for 2D {0, ±1}-valued x at SNR=20 dB
Reconstructor Normalized
Normalized
Normalized
Runtime (s)
l2 error norm
detection error
l0 norm
x0 = 8, SNR = 2 dB, N = 30
Gibbs
0.8786 (0.1359)
0.7500 (0.3707)
0.3125 (0.2780) 267.00 (13.98)
LW
11.7745 (1.7076) 127.0000 (0)
128.0000 (0)
176.89 (3.17)
MAP1
1.0000 (0)
1.0000 (0)
0.0000 (0)
1.29 (0.03)
MAP2
0.4855 (0.0638)
9.8125 (2.0386)
10.8125 (2.0386)
10.31 (4.58)
NnegLW
0.8462 (0.0872)
3.5000 (1.1120)
3.1875 (1.2973)
3.41 (1.10)
SureLasso
2.1039 (1.1806)
1.2500 (0.3707)
1.3750 (1.0193)
0.33 (0.09)
x0 = 16, SNR = 2 dB, N = 30
Gibbs
1.0000 (0.0481)
1.0000 (0.0927)
0.1875 (0.1853) 266.76 (11.94)
LW
11.8527 (1.5794)
63.0000 (0)
64.0000 (0)
176.69 (1.32)
MAP1
1.0000 (0)
1.0000 (0)
0.0000 (0)
1.27 (0.02)
MAP2
0.6493 (0.0552)
5.3438 (0.6023)
6.3125 (0.8803)
9.60 (2.79)
NnegLW
0.9214 (0.0414)
2.2500 (0.7413)
1.8125 (0.8340)
5.12 (2.81)
SureLasso
1.5045 (0.5258)
1.0312 (0.1390)
0.6563 (0.6950)
0.45 (0.05)
Table 6.4. Reconstruction criteria median (median absolution
deviation in parentheses) for 2D {0, ±1}-valued x at SNR=2 dB

Simulation Study
71
Gibbs and SureLasso have the lowest calculated medians
for the normalized detection error; however, the normalized l0
norm suggests that Gibbs’ good normalized detection error
results are due to estimates that approach the trivial 0
estimator. NnegLW, where there is also a model mismatch,
does not exhibit this breakdown behavior. MAP2’s normalized
l0 norm also decreases from the high-to low-SNR case at both
sparsity levels (p
<
10−6). The situation is mixed for
SureLasso: at the higher sparsity level, the normalized l0
norm is the same (p = 0.2451), whereas at the lower sparsity
level, it decreases (p = 5.765 × 10−4).
6.3. Discussion
Choosing an algorithm for the sparse image reconstruction
problem depends on several factors. These can be grouped
into: output, inputs and operating conditions, and algorithmic
complexity/runtime. Having a single performance criterion is
simpler than having several, since, with multiple criteria, a
reconstructor could perform well in one but poorly in another.
Next, the availability or lack of inputs has the ability to alter
performance. In this chapter, MAP1 and MAP2 were run with
the actual noise variance σ2. In reality, this may not be
available.
The
other
reconstructors
do
not
require
σ2.
Knowing the SNR and having prior knowledge on x, e.g. its
sign, could reduce the list of good candidate algorithms to
only one or two candidates. For example, under high SNR
and non-negativity of x, the Gibbs reconstructor performs
very well. Lastly, we generally expect better performance
from higher complexity/runtime. Molecular imaging on a
large molecule would likely impose computing constraints on
the reconstruction algorithm. Making a comparison of the
algorithms subject to a limit on the CPU time, for example,
would be interesting.

Bibliography
[CAI 11] CAI T.T., WANG L., “Orthogonal matching pursuit for
sparse signal recovery with noise”, IEEE Trans. Inform. Theory,
vol. 57, no. 7, pp. 4680–4688, 2011.
[CAN 06] CANDES E.J., ROMBERG J.K., TAO T., “Stable signal
recovery
from
incomplete
and
inaccurate
measurements”,
Communications on Pure and Applied Mathematics, vol. 59,
no. 9, pp. 1207–1223, 2006.
[CLY 99] CLYDE M.A., GEORGE E.I., Empirical Bayes Estimation
in Wavelet Nonparametric Regression, Springer-Verlag, New
York, 1999.
[COM 11] COMBETTES P.L., PESQUET J.-C., “Proximal splitting
methods in signal processing”, Fixed-Point Algorithms for Inverse
Problems in Science and Engineering, pp. 185–212, Springer,
New York, 2011.
[DAU 04] DAUBECHIES I., DEFRISE M., DE MOL C., “An iterative
thresholding algorithm for linear inverse problems with a
sparsity constraint”, Communications on Pure and Applied
Mathematics, vol. 57, no. 11, pp. 1413–1457, 2004.
[DOB 09] DOBIGEON
N.,
HERO
A.O.,
TOURNERET
J.-Y.,
“Hierarchical
Bayesian
sparse
image
reconstruction
with
application to MRFM”, IEEE Transactions on Image Processing,
vol. 18, no. 9, pp. 2059–2070, 2009.
Molecular Imaging in Nano MRI
© ISTE Ltd 2014. Published by ISTE Ltd and John Wiley & Sons, Inc. 
, Michael Ting. 

74
Molecular Imaging in Nano MRI
[DON 95] DONOHO D.L., JOHNSTONE I.M., “Adapting to unknown
smoothness via wavelet shrinkage”, Journal of the American
Statistical Association, vol. 90, no. 423, pp. 1200–1224, 1995.
[DON 06] DONOHO D.L., TSAIG Y., DRORI I., STARCK J.L., Sparse
solution of underdetermined linear equations by stagewise
orthogonal matching pursuit, Report, Stanford University, 2006.
[EFR 79] EFRON B., “Bootstrap methods: another look at the
jackknife”, The Annals of Statistics, vol. 7, no. 1, pp. 1–26, 1979.
[EFR 04] EFRON B., HASTIE T., JOHNSTONE I., et al., “Least angle
regression”, The Annals of Statistics, vol. 32, no. 2, pp. 407–499,
2004.
[FES 77] DEMPSTER A.P., LAIRD N.M., RUBIN D.B., “Maximum
likelihood from incomplete data via the EM algorithm”, Journal
of the Royal Statistical Society. Series B (Methodological), vol. 39,
no. 1, pp. 1–38, 1977.
[FIG 03] FIGUEIREDO M.A.T., NOWAK R.D., “An EM algorithm for
wavelet-based image restoration”, IEEE Transaction on Image
Processing, vol. 12, no. 8, pp. 906–916, 2003.
[HAS 01] HASTIE T., TIBSHIRANI R., FRIEDMAN J., The Elements
of Statistical Learning, Springer, New York, 2001.
[HYV 99] HYVÄRINEN A., “Sparse code shrinkage: denoising of
non-Gaussian data by maximum likelihood estimation”, Neural
Computation, vol. 11, no. 7, pp. 1739–1768, 1999.
[JOH 98] JOHNSTONE I.M., SILVERMAN B.W., Empirical Bayes
approaches to mixture problems and wavelet regression, Report
Stanford University, 1998.
[JOH 04] JOHNSTONE I.M., SILVERMAN B.W., “Needles and straw
in haystacks: empirical Bayes estimates of possibly sparse
sequences”, The Annals of Statistics, vol. 32, no. 4, pp. 1594–1649,
2004.
[KAY 97] KAY
J.,
“The
EM
algorithm
in
medical
imaging”,
Statistical Methods in Medical Research, vol. 6, pp. 55–75, 1997.
[LUC 94] LUCY L.B., “Optimum strategies for inverse problems
in statistical astronomy”, Astronomy and Astrophysics, vol. 289,
no. 3, pp. 983–994, 1994.

Bibliography
75
[MAM 03a] MAMIN
H.J.,
BUDAKIAN
R.,
CHUI
B.W.,
et
al.,
“Detection and manipulation of statistical polarization in small
spin ensembles”, Physical Review Letters, vol. 91, no. 20,
pp. 207604/1–4, 2003.
[MAM 03b] MAMIN H.J., BUDAKIAN R., RUGAR D., Point response
function of an MRFM tip, Report, IBM Almaden, 2003.
[NG 99] NG L., SOLO V., “Optical ﬂow estimation using adaptive
wavelet
zeroing”,
Proceedings
of
the
IEEE
International
Conference on Image Processing, vol. 3, pp. 722–726, 1999.
[PAR 08] PARK T., CASELLA G., “The Bayesian Lasso”, Journal of
the American Statistical Association, vol. 103, no. 482, pp. 681–
686, 2008.
[PAR 12] PARK S.U., DOBIGEON N., HERO A.O., “Semi-blind
sparse image reconstruction with application to MRFM”, IEEE
Transactions on Image Processing, vol. 21, no. 9, pp. 3838–3849,
2012.
[ROB 05] ROBERT C.P., CASELLA G., Monte Carlo Statistical
Methods, 2nd ed., Springer-Verlag, New York, 2005.
[RUG 04] RUGAR D., BUDAKIAN R., MAMIN H.J., et al., “Single
spin detection by magnetic resonance force microscopy”, Nature,
vol. 430, no. 6997, pp. 329–332, 2004.
[STE 81] STEIN C.M., “Estimation of the mean of a multivariate
normal distribution”, The Annals of Statistics, vol. 9, no. 6,
pp. 1135–1151, 1981.
[STI 01] STIPE B.C., MAMIN H.J., YANNONI C.S., et al., “Electron
spin relaxation near a micron-size ferromagnet”, Physical Review
Letters, vol. 87, no. 27, pp. 277602/1–4, 2001.
[STO 97] STOWE
T.D.,
YASUMURA
K.,
KENNY
T.W.,
et
al.,
“Attonewton force detection using ultrathin silicon cantilevers”,
Applied Physics Letters, vol. 71, no. 2, pp. 288–290, 1997.
[TIB 96] TIBSHIRANI R., “Regression shrinkage and selection via
the lasso”, Journal of the Royal Statistical Society, Series B,
vol. 58, no. 1, pp. 267–288, 1996.
[TIN 09] TING
M.,
RAICH
R.,
HERO
A.O.,
“Sparse
image
reconstruction for molecular imaging”, IEEE Transactions on
Image Processing, vol. 18, no. 6, pp. 1215–1227, 2009.

76
Molecular Imaging in Nano MRI
[TRO 07] TROPP
J.A.,
GILBERT
A.C.,
“Signal
recovery from
random measurements via Orthogonal Matching Pursuit”, IEEE
Trans. Inform. Theory, vol. 53, no. 12, pp. 4655–4666, 2007.
[WAG 98] WAGO K., BOTKIN D., YANNONI C.S., RUGAR D.,
“Force-detected electron-spin resonance: adiabatic inversion,
nutation, and spin echo”, Physical Review B, vol. 57, no. 2,
pp. 1108–1114, 1998.
[WIP 04] WIPF D.P., RAO B.D., “Sparse Bayesian learning for
basis selection”, IEEE Trans. Signal Processing, vol. 52, no. 8,
pp. 2153–2164, 2004.
[WU 83] WU C.F.J., “On the convergence properties of the EM
algorithm”, The Annals of Statistics, vol. 11, no. 1, pp. 95–103,
1983.

Index
B, C, D
burn-in, 56, 66
computation time, 65
cross-validation, 43
detection error criterion, 13
G, H, I
generalized cross validation
(GCV), 45–47
Gibbs sampler, 56
hybrid estimator, 23, 45
interrupted OSCAR, 1, 9
L, M, N
lasso estimator, 44
LAZE prior, 12
least angle regression (LARS),
47
magnetic resonance force
microscopy (MRFM), 1
monte carlo markov chain
(MCMC), 53
non-negative Landweber
estimator, 24
P, S
point spread function, 2
Python, 66
Stein’s unbiased risk estimate,
43
SureLasso, 44
Molecular Imaging in Nano MRI
© ISTE Ltd 2014. Published by ISTE Ltd and John Wiley & Sons, Inc. 
, Michael Ting. 

