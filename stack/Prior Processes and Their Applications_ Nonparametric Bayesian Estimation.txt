Springer Series in Statistics
Eswar G. Phadia
Prior Processes 
and Their 
Applications
Nonparametric Bayesian Estimation
 Second Edition 

Springer Series in Statistics
Series editors
Peter Bickel, CA, USA
Peter Diggle, Lancaster, UK
Stephen E. Fienberg, Pittsburgh, PA, USA
Ursula Gather, Dortmund, Germany
Ingram Olkin, Stanford, CA, USA
Scott Zeger, Baltimore, MD, USA

More information about this series at http://www.springer.com/series/692

Eswar G. Phadia
Prior Processes and Their
Applications
Nonparametric Bayesian Estimation
Second Edition
123

Eswar G. Phadia
Department of Mathematics
William Paterson University of New Jersey
WAYNE
New Jersey, USA
ISSN 0172-7397
ISSN 2197-568X
(electronic)
Springer Series in Statistics
ISBN 978-3-319-32788-4
ISBN 978-3-319-32789-1
(eBook)
DOI 10.1007/978-3-319-32789-1
Library of Congress Control Number: 2016940383
© Springer International Publishing Switzerland 2013, 2016
This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part of
the material is concerned, speciﬁcally the rights of translation, reprinting, reuse of illustrations, recitation,
broadcasting, reproduction on microﬁlms or in any other physical way, and transmission or information
storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar methodology
now known or hereafter developed.
The use of general descriptive names, registered names, trademarks, service marks, etc. in this publication
does not imply, even in the absence of a speciﬁc statement, that such names are exempt from the relevant
protective laws and regulations and therefore free for general use.
The publisher, the authors and the editors are safe to assume that the advice and information in this book
are believed to be true and accurate at the date of publication. Neither the publisher nor the authors or
the editors give a warranty, express or implied, with respect to the material contained herein or for any
errors or omissions that may have been made.
Printed on acid-free paper
This Springer imprint is published by Springer Nature
The registered company is Springer International Publishing AG Switzerland

To my
Daughter SONIA
and
Granddaughter ALEXIS


Preface
The foundation of the subject of nonparametric Bayesian inference was laid in two
technical reports: a 1969 UCLA report by Thomas S. Ferguson (later published
in 1973 as a paper in the Annals of Statistics) entitled “A Bayesian analysis
of some nonparametric problems” and a 1970 report by Kjell Doksum (later
published in 1974 as a paper in the Annals of Probability) entitled “Tailfree
and neutral random probabilities and their posterior distributions.” In view of
simplicity with which the posterior distributions were calculated (by updating the
parameters), the Dirichlet process became an instant hit and generated quite an
enthusiastic response. In contrast, Doksum’s approach which was more general
than the Dirichlet process, but restricted to the real line, did not receive the same
kind of attention since the posterior distributions were not easily computable nor
the parameters meaningfully interpretable. Ferguson’s 1974 (Annals of Statistics)
paper gave a simple formulation for the posterior distribution of the neutral to the
right process, and its application to the right censored data was detailed in Ferguson
and Phadia (1979). In fact, it was pointed out in this paper that the neutral to the
right process is equally convenient to handle right censored data as is Dirichlet
process for uncensored data and offers more ﬂexibility. These papers revealed the
advantage of using independent increment processes, and their concrete application
in the reliability theory saw the development of gamma process (Kalbﬂeisch 1978),
extended gamma process (Dykstra and Laud 1981), and beta process (Hjort 1990),
as well as beta-Stacy process (Walker and Muliere 1997a,b). These processes lead
to a class of neutral to the right type processes.
Thus it could rightly be said that, prior to 1974, the subject of nonparametric
Bayesian inference did not exist. The above two papers laid the foundation of this
branch of statistics. Following the publication of Ferguson’s 1973 paper, there was
a tremendous surge of activity in developing nonparametric Bayesian procedures
to handle many inferential problems. During the decades of the 1970s and 1980s,
hundreds of papers were published on this topic. These publications may be
considered as “pioneers” in championing the Bayesian methods and opening a vast
unexplored area in solving nonparametric problems. A review article (Ferguson
et al. 1992) summarized the progress of the two decades. Since then, several new
vii

viii
Preface
prior processes and their applications have appeared in technical publications. Also,
in the last decade, there has been a renewed interest in the applications of variants
of the Dirichlet process in modeling large-scale data [see, e.g., the recent paper by
Chung and Dunson (2011), and references cited therein and a volume of essays
“Bayesian Nonparametric” edited by Hjort et al. (2010)]. For these reasons, there
seems to be a need for a single source of the material published on this topic where
the audience can get exposed to the theory and applications of this useful subject so
that they can apply them in practice. This is the prime motivator for undertaking the
present task.
The objective of this book is to present the material on the Dirichlet process,
its properties, and its various applications, as well as other prior processes that
have been discovered through the 1990s and their applications, in solving Bayesian
inferential problems based on data that may possibly be right censored, sequential,
or quantal response data. We anticipate that it would serve as a one-stop resource
for future researchers. In that spirit, ﬁrst various processes are introduced and their
properties are stated. Thereafter, the focus is to present various applications in
estimation of distribution and survival functions, estimation of density functions and
hazard rates, empirical Bayes, hypothesis testing, covariate analysis, and many other
applications. A major requirement of Bayesian analysis is its analytical tractability.
Since the Dirichlet process possesses the conjugacy property, it has simplicity and
ability to get results in a closed form. Therefore, most of the applications that were
published soon after Ferguson’s paper are based on the Dirichlet process. Unlike
the trend in recent years where computational procedures are developed to handle
large and complex data sets, the earlier procedures relied mostly on developing
procedures in closed forms.
In addition, several new and interesting processes, such as the Chinese restaurant
process, Indian buffet process, and hierarchical processes, have been introduced in
the last decade with an eye toward applications in the ﬁelds outside mainstream
statistics, such as machine learning, ecology, document classiﬁcation, etc. They
have roots in the Ferguson-Sethuraman countable inﬁnite sum representation of the
Dirichlet process and shed new light on the robustness of this approach. They are
included here without going into much details of their applications.
Computational procedures that make nonparametric Bayesian analysis feasible
when closed forms of solutions are impossible or complex are becoming increas-
ingly popular in view of the availability of inexpensive and fast computation power.
In fact, they are indispensable tools in modeling large-scale and high-dimensional
data. There are numerous papers published in the last two decades that discuss them
in great detail and algorithms are developed to simulate the posterior distributions
so that the Bayesian analysis can proceed. These aspects are covered in books by
Ibrahim et al. (2001) and Dey et al. (1998). To avoid duplication, they are not
discussed here. Some newer applications are also discussed in the book of essays
edited by Hjort et al. (2010).
This material is an outgrowth of my lecture notes developed during the week-
long lectures I gave at Zhongshan University in China in 2007 on this topic,
followed by lectures at universities in India and Jordan. Obviously, the choice of

Preface
ix
material included and the style of presentation solely reﬂects my preferences. This
manuscript is not expected to include all the applications, but references are given,
wherever possible for additional applications. The mathematical rigor is limited as
it has already been dealt with in the theoretical book by Ghosh and Ramamoorthi
(2003). Therefore, many theorems and results are stated without proofs, and the
questions regarding existence, consistency, and
convergences are skipped. To
conserve space, numerical examples are not included but referred to the papers
originating those speciﬁc topics. For these reasons, the notations of the originating
papers are preserved as much as possible, so that the reader may ﬁnd it easy to read
the original publications.
The ﬁrst part is devoted to introducing various prior processes, their formulation,
and their properties. The Dirichlet process and its immediate generalizations are
presented ﬁrst. The neutral to the right processes and the processes with independent
increments, which give rise to other processes, are discussed next. They are key
in the development of processes that include beta, gamma, and extended gamma
processes, which are proposed primarily to address speciﬁc applications in the
reliability theory. Beta-Stacy process which generalizes the Dirichlet process is
discussed thereafter. Following that, tailfree and Polya tree processes are presented
which are especially convenient to place greater weights, where it is deemed
appropriate, by selecting suitable partitions in developing the prior. Finally, some
additional processes that have been discovered in recent years (mostly variants of
existing processes) and found to be useful in practice are mentioned. They have
their origin in the Ferguson-Sethuraman inﬁnite sum representation and the manner
in which the weights are constructed. They are collectively called here as Ferguson-
Sethuraman processes.
The second part contains various inferential applications that cover multitudes
of ﬁelds such as estimation, hypothesis testing, empirical Bayes, density estimation,
bioassay, etc. They are grouped according to the inferential task they signify. Since a
major part of efforts have been devoted to the estimation of the distribution function
and its functional, they receive signiﬁcant attention. This is followed by conﬁdence
bands, two-sample problems, and other applications. The third part is devoted to
presenting inferential procedures based on censored data. Heavy emphasis is given
to the estimation of the survival function since it plays an important role in the
survival data analysis. This is followed by other examples which include estimation
procedures in certain stochastic process models.
Ferguson’s seminal paper, and others that followed, has opened up a dormant area
of nonparametric Bayesian inference. During the last four decades, a considerable
attention has been given to this area, and great stride is made in solving many
nonparametric problems and extending some usual approaches (see Müller and
Quintana 2004). For example, in problems where the observations are subjected
to random error, traditionally the errors are assumed to be distributed as normal
with mean zero. Now it is possible to assume them to be having an unknown
distribution whose prior is concentrated around the normal distribution or symmetric
distributions with mean zero and carry out the analysis. Moreover, in many appli-
cations when the prior information tends to nil, the estimators reduce to the usual

x
Preface
maximum likelihood estimators—a desirable property. Obviously, it is impossible
to include all these methods and applications in this manuscript. However, a long list
of references is included for the reader to explore relevant areas of interest further.
Since this book discusses various prior processes, and their properties and
inferential procedures in solving problems encountered in practice and limits deeper
technical details, it is ideal to serve as a comprehensive introduction to the subject
of nonparametric Bayesian inference. It should therefore be accessible to ﬁrst-
time researchers and graduate students venturing into this interesting, fertile, and
promising ﬁeld. As evident by the recent increased interest in using nonparametric
Bayesian methods in modeling data, the ﬁeld is wide open for new entrants. As such,
it is my hope that this attempt will serve the purpose it was intended for, namely,
to make such techniques readily available via this comprehensive but accessible
book. At the least, the reader will gain familiarity with many successful attempts
in solving nonparametric problems from a Bayesian point of view in wide-ranging
areas of applications.
Preface to the Revision
Following the publication of the book in 2013, I have noticed that there has been
continued and intensiﬁed interest in applying nonparametric Bayesian methods in
the analysis of statistical data. Therefore, it is important that I should update the
book to reﬂect the current interest. This is the main rationale for this revision. I have
not only supplemented but expanded the earlier edition with additional material
which would make the book “richer” in the content. As a consequence, I have
reorganized the topics of the ﬁrst part into cohesive but separate chapters. The
second and third parts of the earlier edition (new Chaps. 6 and 7) remain unchanged
as the applications mentioned there were obtained mostly in closed form and have
limited applicability in the present environment of dealing with large and complex
data. Highlights of the improvement in the revised edition are as follows.
The Dirichlet process and its variants are grouped together in Chap. 2. Starting
in 2006, there has been growing interest in developing hierarchical and mixture
models. Accordingly, a new section is added to describe them in more detail.
Implementation of these models in carrying out full Bayesian analysis requires the
knowledge of posterior distributions. Unfortunately, they are not usually in closed
form but are often complicated and intractable—a major hurdle. This makes it
necessary to generate them via simulation for which computational procedures such
as Gibbs sampler, blocked Gibbs sampler, and slice and retrospective sampling are
developed in the literature. These methods are described here and steps of relevant
algorithms provided by the authors are included while discussing speciﬁc models.
A major development that occurred during the last decade was the exploitation
of Sethuraman’s representation of a Dirichlet process in modeling data that included
covariates and spatial data, time series data, dependent groups data, etc., and gave
rise to what is known as dependent (Dirichlet) processes. To reﬂect this development
and continued interest, the material of the earlier edition has been expanded to
include several new processes, thus forming a separate chapter—Chap.3, under
the heading of Ferguson-Sethuraman processes. This chapter not only includes

Preface
xi
dependent processes but also one- and two-parameter Poisson-Dirichlet processes
and a species sampling model.
As mentioned before, the basic processes developed earlier such as neutral to
the right, gamma, beta, and beta-Stacy were essentially based on processes with
independent increments and their associated Levy measures. Therefore, it made
sense to present them cohesively under a single chapter, Chap. 4. The Chinese
restaurant process, Indian buffet process, and stable and kernel beta processes also
ﬁnd place in this chapter. Since a random probability measure may be viewed as
a completely random measure, which in turn can be constructed via the Poisson
process with a speciﬁc Levy measure as its mean measure, some fundamental
deﬁnitions and theorems related to them are also included for the sake of ready
reference. The material of tailfree and Polya tree processes forms Chap.5.
Throughout this revision, I have added additional explanations whenever war-
ranted, including outlines of proofs of major theorems and derivations of basic
processes such as the Dirichlet process, and beta and beta-Stacy processes and
their variants, as well as of processes that are popular in other areas, all for better
understanding the mechanism behind them. Also some further generalization of
these processes have been included. In addition, scores of new references have been
added to the list of references making it easy for interested readers to explore further.
While this book focuses on the fundamentals of nonparametric Bayesian approach,
a recently published book Bayesian Nonparametric Data Analysis by Mull et al.
(2015, Springer), is a good source of Bayesian treatment in modeling and data
analysis and could serve as a complement to the present volume.
I sincerely believe that this expanded version would better serve the readers
interested in this area of statistics.
Acknowledgment
Such tasks as writing a book takes a lot of patience and hard work. My undertaking
was no exception. However, I was fortunate to receive lot of encouragement, advice,
and support on the way.
I had the privilege of support, collaboration, and blessing of Tom Ferguson, the
architect of nonparametricBayesian statistics, which inspired me to explore this area
during the early years of my career. Recent ﬂurry of activity in this area renewed
my interest and prompted me to undertake this task. I am greatly indebted to him.
Jagdish Rustagi brought to my attention in 1970 a prepublication copy of Ferguson’s
seminal 1973 paper which led to my doctoral dissertation at Ohio State University.
I am eternally grateful to him for his advice and support in shaping my research
interests which stayed on track with me for the last 40 years except for a 10-year
stint in administration.
The initial template of the manuscript was developed as lecture notes for
presentation at Zhongshan University in China at the behest of Qiqing Yu of
Binghamton University. I thank him and the Zhongshan University faculty and

xii
Preface
staff for their hospitality. The ﬁnal shape of the manuscript took place during
my sabbatical at the University of Pennsylvania’s Wharton School of Business. I
gratefully thank Edward George and Larry Brown of the Department of Statistics for
their kindness in providing me the necessary facilities and intellectual environment
(and not to forget complimentary lattes) which enabled me to advance my endeavor
substantially. I also take pleasure in thanking Bill Strawderman, for his friendship
of over 30 years, sound advice, and useful discussions during my earlier sabbatical
and frequent visits to Rutgers University campus since then. My sincere thanks go
to anonymous reviewers whose comments and generous suggestions improved the
manuscript. I must have exchanged scores of emails and had countless conversations
with Dr. Eva Hiripi, Editor of Springer, during the last four years. Her patience,
understanding, and helpful suggestions were instrumental in shaping the ﬁnal
products of the ﬁrst and second editions, as well as her decision to publish it
in Springer Statistics Series. My heartfelt thanks go to her. The production staff
at Springer including Ulrike Stricker-Komba and Mahalakshmi Rajendran at SPi
Technologies India Private Ltd including Edita Baronaite did a fantastic job in
detecting missing references and producing the ﬁnal product. They deserve my
thanks.
Since my retirement from WPU, the Department of Statistics at Wharton School,
University of Pennsylvania has been kind enough to host me as visiting scholar to
pursue the revision of the ﬁrst edition. I am very grateful to the faculty and staff of
the department, especially Ed George and Mark Low, for extending their support,
courtesy, and cooperation which enabled me to complete the revision successfully.
I offer my sincere thanks to all of them. I also thank the two anonymous reviewers
for their very complimentary reviews of the revised edition.
This task could not have been accomplished without the support of my institution
in terms of ART awards over a period of number of years and cooperation of my
colleagues. In particular, I thank my colleague Jyoti Champanerker for creating
the ﬂow-chart of Chap. 1. Finally, I thank my wife and companion, Jyotsna my
daughter, Sonia, for their support and my granddaughter, Alexis, who, at her tender
age, provided me happiness and stimulus to keep working on the revision in spite of
my retirement.
Wayne, NJ, USA
Eswar G. Phadia
July 2016

Contents
1
Prior Processes: An Overview ..............................................
1
1.1
Introduction .............................................................
1
1.2
Methods of Construction ...............................................
3
1.3
Prior Processes ..........................................................
7
2
Dirichlet and Related Processes ............................................
19
2.1
Dirichlet Process ........................................................
19
2.1.1
Deﬁnition........................................................
20
2.1.2
Properties........................................................
29
2.1.3
Posterior Distribution ...........................................
36
2.1.4
Extensions and Applications of Dirichlet Process ............
40
2.2
Dirichlet Invariant Process..............................................
43
2.2.1
Properties........................................................
44
2.2.2
Symmetrized Dirichlet Process ................................
45
2.3
Mixtures of Dirichlet Processes ........................................
45
2.3.1
Deﬁnition........................................................
46
2.3.2
Properties........................................................
48
2.4
Dirichlet Mixture Models...............................................
50
2.4.1
Sampling the Posterior Distribution ...........................
53
2.4.2
Hierarchical and Mixture Models ..............................
63
2.4.3
Some Further Generalizations..................................
76
2.5
Some Related Dirichlet Processes......................................
77
3
Ferguson–Sethuraman Processes ..........................................
81
3.1
Introduction .............................................................
81
3.2
Discrete and Finite Dimensional Priors ................................
85
3.2.1
Stick-Breaking Priors PN .a; b/ ................................
85
3.2.2
Finite Dimensional Dirichlet Priors............................
87
3.2.3
Discrete Prior Distributions ....................................
89
3.2.4
Residual Allocation Models....................................
89
xiii

xiv
Contents
3.3
Dependent Dirichlet Processes .........................................
90
3.3.1
Covariate Models ...............................................
94
3.3.2
Spatial Models ..................................................
99
3.3.3
Generalized Dependent Processes ............................. 107
3.4
Poisson–Dirichlet Processes ............................................ 110
3.4.1
One-Parameter Poisson–Dirichlet Process .................... 111
3.4.2
Two-Parameter Poisson–Dirichlet Process .................... 115
3.5
Species Sampling Models .............................................. 119
4
Priors Based on Levy Processes ............................................ 127
4.1
Introduction ............................................................. 127
4.1.1
Nondecreasing Independent Increment Processes ............ 128
4.1.2
Lévy Measures of Different Processes......................... 133
4.1.3
Completely Random Measures................................. 134
4.2
Processes Neutral to the Right.......................................... 137
4.2.1
Deﬁnition........................................................ 138
4.2.2
Properties........................................................ 144
4.2.3
Posterior Distribution ........................................... 146
4.2.4
Spatial Neutral to the Right Process ........................... 156
4.3
Gamma Process ......................................................... 157
4.3.1
Deﬁnition........................................................ 157
4.3.2
Posterior Distribution ........................................... 158
4.4
Extended Gamma Process .............................................. 159
4.4.1
Deﬁnition........................................................ 160
4.4.2
Properties........................................................ 161
4.4.3
Posterior Distribution ........................................... 162
4.5
Beta Process I ........................................................... 164
4.5.1
Deﬁnition........................................................ 166
4.5.2
Properties........................................................ 170
4.5.3
Posterior Distribution ........................................... 171
4.6
Beta Process II .......................................................... 173
4.6.1
Beta Processes on General Spaces ............................. 173
4.6.2
Stable-Beta Process............................................. 179
4.6.3
Kernel Beta Process ............................................ 181
4.7
Beta-Stacy Process ...................................................... 184
4.7.1
Deﬁnition........................................................ 185
4.7.2
Properties........................................................ 188
4.7.3
Posterior Distribution ........................................... 190
4.8
NB Models for Machine Learning ..................................... 192
4.8.1
Chinese Restaurant Process .................................... 193
4.8.2
Indian Buffet Process ........................................... 196
4.8.3
Inﬁnite Gamma-Poisson Process............................... 201

Contents
xv
5
Tailfree Processes ............................................................ 205
5.1
Tailfree Processes ....................................................... 205
5.1.1
Deﬁnition........................................................ 206
5.1.2
Properties........................................................ 207
5.2
Polya Tree Processes.................................................... 208
5.2.1
Deﬁnition........................................................ 209
5.2.2
Properties........................................................ 209
5.2.3
Finite and Mixtures of Polya Trees ............................ 214
5.3
Bivariate Processes...................................................... 216
5.3.1
Bivariate Tailfree Process ...................................... 217
6
Inference Based on Complete Data ........................................ 221
6.1
Introduction ............................................................. 221
6.2
Estimation of a Distribution Function.................................. 222
6.2.1
Estimation of a CDF ............................................ 222
6.2.2
Estimation of a Symmetric CDF ............................... 223
6.2.3
Estimation of a CDF with MDP Prior ......................... 224
6.2.4
Empirical Bayes Estimation of a CDF......................... 224
6.2.5
Sequential Estimation of a CDF ............................... 228
6.2.6
Minimax Estimation of a CDF ................................. 229
6.3
Tolerance Region and Conﬁdence Bands .............................. 230
6.3.1
Tolerance Region ............................................... 230
6.3.2
Conﬁdence Bands............................................... 230
6.4
Estimation of Functionals of a CDF ................................... 232
6.4.1
Estimation of the Mean ......................................... 233
6.4.2
Estimation of a Variance........................................ 234
6.4.3
Estimation of the Median....................................... 235
6.4.4
Estimation of the qth Quantile ................................. 236
6.4.5
Estimation of a Location Parameter ........................... 237
6.4.6
Estimation of P.Z > X C Y/ ................................... 238
6.5
Other Applications ...................................................... 239
6.5.1
Bayes Empirical Bayes Estimation ............................ 239
6.5.2
Bioassay Problem............................................... 241
6.5.3
A Regression Problem .......................................... 243
6.5.4
Estimation of a Density Function .............................. 244
6.5.5
Estimation of the Rank of X1 Among X1; : : : ; Xn ............. 248
6.6
Bivariate Distribution Function......................................... 249
6.6.1
Estimation of F w.r.t. the Dirichlet Process Prior ............. 249
6.6.2
Estimation of F w.r.t. a Tailfree Process Prior................. 249
6.6.3
Estimation of a Covariance..................................... 250
6.6.4
Estimation of the Concordance Coefﬁcient ................... 251
6.7
Estimation of a Function of P .......................................... 253
6.7.1
Dirichlet Process Prior.......................................... 253
6.7.2
Dirichlet Invariant Process Prior ............................... 256
6.7.3
Empirical Bayes Estimation of .P/ .......................... 258

xvi
Contents
6.8
Two-Sample Problems .................................................. 259
6.8.1
Estimation of P.X  Y/ ........................................ 260
6.8.2
Estimation of the Difference Between Two CDFs ............ 261
6.8.3
Estimation of the Distance Between Two CDFs .............. 263
6.9
Hypothesis Testing ...................................................... 264
6.9.1
Testing H0 W F  F0 ............................................ 264
6.9.2
Testing Positive Versus Nonpositive Dependence............. 265
6.9.3
A Selection Problem............................................ 267
7
Inference Based on Incomplete Data ...................................... 269
7.1
Introduction ............................................................. 269
7.2
Estimation of an SF Based on DP Priors............................... 270
7.2.1
Estimation Based on Right Censored Data .................... 270
7.2.2
Empirical Bayes Estimation.................................... 273
7.2.3
Estimation Based on a Modiﬁed Censoring Scheme.......... 274
7.2.4
Estimation Based on Progressive Censoring .................. 275
7.2.5
Estimation Based on Record-Breaking Observations ......... 276
7.2.6
Estimation Based on Random Left Truncation ................ 277
7.2.7
Estimation Based on Proportional Hazard Models............ 277
7.2.8
Modal Estimation ............................................... 278
7.3
Estimation of an SF Based on Other Priors ............................ 279
7.3.1
Estimation Based on an Alternate Approach .................. 279
7.3.2
Estimation Based on Neutral to the Right Processes.......... 281
7.3.3
Estimation Based on a Simple Homogeneous Process........ 283
7.3.4
Estimation Based on Gamma Process ......................... 284
7.3.5
Estimation Based on Beta Process ............................. 285
7.3.6
Estimation Based on Beta-Stacy Process ...................... 286
7.3.7
Estimation Based on Polya Tree Priors ........................ 286
7.3.8
Estimation Based on an Extended Gamma Prior.............. 287
7.3.9
Estimation Assuming Increasing Failure Rate ................ 287
7.4
Linear Bayes Estimation of an SF...................................... 288
7.5
Other Estimation Problems ............................................. 290
7.5.1
Estimation of P.Z > X C Y/ ................................... 290
7.5.2
Estimation of P.X  Y/ ........................................ 291
7.5.3
Estimation of S in Competing Risk Models ................... 292
7.5.4
Estimation of Cumulative Hazard Rates ....................... 295
7.5.5
Estimation of Hazard Rates .................................... 296
7.5.6
Markov Chain Application ..................................... 297
7.5.7
Estimation for a Shock Model ................................. 299
7.5.8
Estimation for a Age-Dependent Branching Process ......... 300

Contents
xvii
7.6
Hypothesis Testing H0 W F  G ........................................ 302
7.7
Estimation in Presence of Covariates .................................. 303
References......................................................................... 309
Author Index...................................................................... 321
Subject Index ..................................................................... 325

Chapter 1
Prior Processes: An Overview
1.1
Introduction
In this section we give an overview of the various processes that have been
developed to serve as prior distributions in the treatment of nonparametric problems
from a Bayesian point of view. We indicate their relationship with each other
and discuss circumstances in which they are appropriate to use and their relative
merits and shortcomings in solving inferential problems. In subsequent sections we
provide more details on each of them and state their properties. To preserve the
historical perspective, they are mostly organized in the order of their discovery and
development. The last two chapters contain various applications based on censored
and uncensored data.
In the Bayesian approach, the unknown distribution function from which the
sample arises is itself considered as a parameter. Thus, we need to construct prior
distributions on the space of all distribution functions, to be denoted by F ./ ;
deﬁned on a sample space , or on all probability measures, … deﬁned on certain
probability space, .X; A/; where A is -ﬁeld of subsets of X . To be more precise,
let X be a random variable deﬁned on some probability space .;  ./ ; Q/ taking
values in .X; A/; and let F ./ denote the space of all distribution functions deﬁned
on the sample space .X; A/:
Consider, for example, the Bernoulli distribution which assigns mass p to 0 and
1  p to 1; 0 < p < 1. In this case the sample space is  D f0; 1g and the space of
all distributions consists of distributions taking jumps of size p at 0 and 1  p at 1 or
F D fF W F .t/ D pIŒt  0 C .1  p/ IŒt  1g, where IŒA is an indicator function
of the set A. Here the random distribution function is characterized by treating p as
random. In this case, a prior on F ./ may then be speciﬁed by simply assigning
a prior distribution to p on …, say uniform, U.0; 1/ or a beta distribution, Be.a; b/
with parameters a > 0; and b > 0: A prior distribution on F ./ or … will be
denoted by P whenever needed.
© Springer International Publishing Switzerland 2016
E.G. Phadia, Prior Processes and Their Applications, Springer Series in Statistics,
DOI 10.1007/978-3-319-32789-1_1
1

2
1
Prior Processes: An Overview
As a second example, consider the multinomial experiment with the sample
space,  D f1; 2; : : : ; kg; In this case, F./ is the space of all distribution functions
corresponding to a .k  1/-dimensional probability simplex Sk D f. p1; p2; : : : ; pk/ W
0  pi  1; Pk
iD1 pi D 1g of probabilities. Then a prior distribution P can be
speciﬁed on F./ by deﬁning a measure on Sk which yields the joint distribution
of . p1; p2; : : : ; pk/, say, the Dirichlet distribution with parameters .˛1; ˛2; : : : ; ˛k/,
where ˛i  0 for i D 1; 2; : : : ; k.
These are examples of ﬁnite dimensional priors. Our concern now is to extend
these formulations to inﬁnite dimension. In such situations the prior is a stochastic
process with parameter space as -algebra of subsets of the underlying space.
While the distribution function F is the parameter of primary interest in
nonparametric Bayesian analysis, at times it is more convenient to discuss the prior
process in terms of a probability measure P instead of the corresponding distribution
function, P.a; b D F .b/  F .a/. However, many of the applications are given in
terms of the distribution function or its functional. The advantage of considering
P is then it is easy to talk about arbitrary space which may include Rk instead of
R1 alone. The Dirichlet process (DP) is deﬁned in this way on an arbitrary space.
Ferguson derives his results for a random probability measure which is a special
case of random measures introduced by Kingman (1967). Random measures are
generated by the Poisson process. They provide a tool to treat priors in a uniﬁed
approach as shown in Hjort et al. (2010). However, such an approach does not
provide any insight into how the priors originated to start with.
Deﬁning a prior for an unknown F on F or for a P on … gives rise to some
theoretical difﬁculties (see, for example, Ferguson 1973). The challenge therefore
is how to circumvent these difﬁculties and deﬁne viable priors. The priors so
deﬁned should have, according to Ferguson (1973), two desirable properties: The
support should be large enough to accommodate all shades of belief; and the
posterior distribution, given a sample should be analytically tractable so that the
Bayesian analysis can proceed. The second desirable property has lead to a search
of priors which are conjugate, i.e., the posterior has the same structure except
for the parameters. This would facilitate posterior analysis since one needs only
to update the parameters of the prior. However, it could also be construed as a
limitation in choice of priors. A balance between the two would be preferable
(Antoniak 1974 adds some more desirable properties). In addition, since the
Bayesian approach involves incorporating prior information to make inferential
procedures more efﬁcient, it may be considered as an extension of the classical
maximum likelihood approach. Therefore, it is natural to expect that the results of
the procedures so developed should reduce to those obtained through the classical
methods when the prior information, reﬂected in parameters of the priors, tends to
nil. It will be seen that this is mostly true, especially in the case of Dirichlet and
neutral to the right processes.
Prior to 1973, the subject area of nonparametric Bayesian inference was non-
existent. Earlier attempts in deﬁning such priors on F can be traced to Dubins
and Freedman (1966) whose methods to construct a random distribution function
resulted in a singular continuous distribution, with probability one. In dealing with

1.2
Methods of Construction
3
a bioassay problem, Kraft and van Eeden (1964) constructs a prior in terms of the
joint distribution of the ordinates of F at certain ﬁxed points of a countable dense
subset of the real line. In Kraft (1964), the author describes a procedure of choosing
a distribution function on the interval [0; 1] which is absolutely continuous with
probability one. Freedman (1963) introduced the notion of tailfree distributions on
a countable space and Fabius (1964) extended the notion to the interval Œ0; 1. But
all these attempts had limited success because either the base was not sufﬁciently
large or the solutions were analytically or computationally intractable.
Ferguson’s landmark paper was the ﬁrst successful attempt in deﬁning a prior
which met the above requirements. Encouraged by his success, several new prior
processes have been proposed in the literature since then to meet speciﬁc needs.
We review them brieﬂy in this chapter and present them formally in subsequent
chapters.
1.2
Methods of Construction
During the earlier period of development, the method of placing a prior on F
or … can broadly be classiﬁed as based essentially on four different approaches.
The ﬁrst one is based on specifying the joint distribution of random probabilities,
and next two are based on different independence properties, and the last one
is based on generating a sequence of exchangeable random variables using the
generalized Polya urn scheme. The ﬁrst three approaches are closely related to
different properties of the Dirichlet distribution [see Basu and Tiwari (1982) for
extensive discussion of these properties]. However, in the last decade or so, several
new processes have been developed which can be constructed via the countable
mixture representation of a random probability measure, also known as the stick-
breaking construction. These are described here informally without going into the
underlying technicalities.
The ﬁrst method introduced by Ferguson (1973) is to deﬁne a family of consistent
ﬁnite dimensional distributions of probabilities of sets of a measurable partition
of a set on an arbitrary space, and then appealing to the Kolmogorov’s extension
theorem. For any positive integer k; let A1; : : : ; Ak be a measurable partition of
X and let ˛ be a nonnegative ﬁnite measure on .X; A/: A random probability
measure P deﬁned on .X; A/ is said to be a Dirichlet process with parameter
˛ if the distribution of the vector .P .A1/ ; : : : ; P .Ak// is Dirichlet distribution,
D .˛ .A1/ ; : : : ; ˛ .Ak// : In symbols it will be denoted as P Ï D .˛/ (In our
presentation, as has been a common practice, we will ignore the distinction between
a random probability P being a Dirichlet process and the Dirichlet process being
a prior distribution for a random probability P on the space …/: This approach
was used in two immediate generalizations: One by Antoniak (1974) who treated
the parameter ˛ itself as random, indexed by u, u having a certain distribution
H and proposed the mixture of Dirichlet processes, i.e., P Ï
R
D .˛u/ dH .u/ :
The other by Dalal (1979a) who treated the measure ˛ as invariant under a ﬁnite

4
1
Prior Processes: An Overview
group of transformations and proposed a Dirichlet Invariant process over a class of
invariant distributions which included, symmetric distributions around a location ;
or distributions having a median at 0:
The remarkable feature of the Dirichlet process (DP) is that it is deﬁned on
abstract spaces and serves as a “base” prior, and is the main source for various
generalizations in many different directions. This makes it possible to generate new
prior processes allowing not only a great deal of ﬂexibility in modeling, but at the
same time are tailored for different statistical problems (see Fig. 1.1). For example,
by treating ˛ itself as a random measure having certain prior distribution, say the
DP, hierarchical models were proposed in Teh et al. (2006); by taking ˛ .X/ as a
positive function instead of a constant, Walker and Muliere (1997a) were able to
generalize the Dirichlet process so that the support included absolutely continuous
distribution functions as well; by writing f .x/ D
R
K.x; u/dG .u/ with a known
kernel K, and taking G Ï D .˛/ ; Lo (1984) was able to place priors on the space
of density functions. Further examples based on countable representation of the DP
are given ahead.
The second method is based on the property of independence of successive
normalized increments of a distribution function F deﬁned on the real line R: It is
based on the Connor and Mosimann (1969) concept of neutrality for k-dimensional
random vectors. For m D 1; 2; : : : consider the sequence of real numbers 1 <
t1 < t2 <    < tm < 1. Doksum (1974) deﬁnes a random distribution
function F as neutral to the right if for all m; the successive normalized increments
F.t1/; .F.t2/  F.t1// =.1  F.t1//; : : :, are independent. This simple requirement
provides a tremendous ﬂexibility in generating priors. Since a distribution function
can be reparametrized as F.t/ D 1exp .Yt/ ; where Yt is a process with indepen-
dent nonnegative increments, the neutral to the right processes can also be viewed
in terms of the processes with independent nonnegative increments. Since the latter
processes are well known, they became the main tool in deﬁning a class of speciﬁc
processes tailored to suit particular applications. Some examples are as follows.
Kalbﬂeisch (1978) deﬁned a gamma process by assuming the increments to
be distributed as the gamma distribution; Dykstra and Laud (1981) proposed
an extended gamma process, by deﬁning a weighted hazard function r .t/ D
R
Œ0;th .s/ dZ .s/ for any positive real valued function h; and Z; a gamma process;
and thus placed priors on the space of hazard functions; by treating the increments
as approximately beta random variables, Hjort (1990) was able to deﬁne a beta
process which places a prior on the space of cumulative hazard functions, and via
the above parametrization, on the CDFs as well; Thibaux and Jordan (2007) deﬁned
a (Hierarchical) beta process by modifying the Levy measure of the beta process;
and Walker and Muliere (1997a) introduced the beta-Stacy process by assuming
the increments to be distributed as beta-Stacy distribution. There are other related
processes as well. They all belong to the family of Levy processes.
The third method is based on a different independence property which corre-
sponds to the tailfree property of the Dirichlet distribution. Let fng be a sequence
of nested partitions of R such that nC1 is a reﬁnement of n, for n D 1; 2; : : : .
Let fBm1; : : : ; Bmkmg denote the partition m: Since the partitions are nested, then

1.2
Methods of Construction
5
for s < m; there is one set in s that contains the set Bmi of m: This set will
be denoted by Bs.mi/: A random probability P is said to be tailfree if the families
˚
P

B1jjB0.1j/

W j D 1; : : : ; k1

; : : : ;
˚
P

BmC1jjBm.mj/

W j D 1; : : : ; kmC1

are inde-
pendent, where B0.1j/ D R. That is, a random probability P is said to be tailfree
if the sets of random variables fP .BjA/ W A 2 n and B 2 nC1g for n D 1; 2; : : :
are independent. Here 0 D R: The random probability P is deﬁned via the joint
distribution of all the random variables P .BjA/ :
The origin of this process goes back to Freedman (1963) and Fabius (1964), but
Doksum (1974) clariﬁed the notion of tailfree and Ferguson (1974) gave a concrete
example, thus formalizing the discussion in the context of a prior distribution.
Tailfree is a misnomer since the deﬁnition does not depend on the tails (Doksum
1974, attributes it to Fabius for pointing out this distinction). Doksum used the
term F-neutral. However, we will use the term tailfree as it has become a common
practice. The Polya tree processes developed more formally by Lavine (1992, 1994)
and Mauldin et al. (1992) are a special case of tailfree processes in which all random
variables are assumed to be independent. Such priors are particularly appropriate
when one wishes to model a random F (Walker et al. 1999) with ﬁxed locations
based on some prior guess of F; say, F0:
As a fourth approach, Blackwell and MacQueen (1973) showed that a prior
process can also be deﬁned by constructing a sequence of exchangeable random
variables via the Polya urn scheme and then appealing to a theorem of de Finetti.
If X1; X2; : : : : is a sequence of exchangeable random variables with a common
distribution P; then for every n and sets A1; : : : ; An 2 A;
P .Xi 2 Ai W i D 1; : : : ; n/ D
Z
n
Y
iD1
P .Ai/ Q .dP/ ;
where Q is known as the de Finetti measure, and serves as a prior distribution of
P: The prior processes (actually measures) discussed here are the different forms
of Q: In particular, they showed that the Dirichlet process can also be deﬁned in
this way. This approach is especially suitable when one is interested in prediction
problems, i.e., in deriving the predictive distribution of XnC1 given X1; : : : ; Xn:
However identiﬁcation of Q is usually a problem.
The Polya urn scheme may be described as follows. Let  D f1; 2; : : : ; kg. We
start with an urn containing ˛i balls of color i, i D 1; 2; : : : ; k. Draw a ball at
random of color i and deﬁne the random variable X1 so that P.X1 D i/ D ˛i,
where ˛i D ˛i=.Pk
iD1 ˛i/. Now replace the ball with two balls of the same color
and draw a second ball. Deﬁne the random variable X2 so that P.X2 D j j X1 D
i/ D .˛j C ıj/=.Pk
iD1 ˛i C 1/, where ıj D 1 if j D i, 0 otherwise. This is the
conditional predictive probability of a future observation. Repeat this process to
obtain a sequence of exchangeable random variables X1; X2; : : :. taking values in :
Blackwell and MacQueen generalize this scheme by taking a continuum of colors
˛. Then a theorem of de Finetti assures us that there exists a probability measure
	 such that the marginal ﬁnite dimensional joint probability distributions under this

6
1
Prior Processes: An Overview
measure is same for any permutation of the variables. This mixing measure is treated
as a prior distribution. In this approach, besides exchangeability all that is needed
essentially is the predictive probability rule to deﬁne a prior.
It is shown later on that this method leads to characterizations of different prior
processes, since once the sequence is constructed by a predictive distribution, the
existence of the prior measure is assured. However the identiﬁcation of that prior
measure is problematic. This approach was adopted by Mauldin et al. (1992) who
used a generalized Polya urn scheme to generate sequences of exchangeable random
variables and based upon them, deﬁned a Polya tree process. Pitman (1996b) gives
other examples.
It is interesting to note that the DP has representation under all of the above
approaches and it is the only prior which can be obtained by any of the above
approaches.
In addition to the above four methods, the countable mixture representation
of a random probability measure has been found recently to be a useful tool in
developing several new processes, some of which are variants of the DP suitable
for handling speciﬁc applications. Note that Ferguson’s primary deﬁnition of the
Dirichlet process with parameter ˛ was in terms of a stochastic process indexed
by the elements of A: His alternative deﬁnition was constructive and described
the Dirichlet process as a random probability measure with a countable sum
representation,
P D
1
X
iD1
piıi;
(1.2.1)
which is a mixture of unit masses placed at random points i’s chosen independently
and identically with distribution F0 D ˛ ./ =˛ .X/ ; and the random weights pi’s are
such that 0  pi  1 and P1
iD1 pi D 1: Ferguson’s weights were constructed
using normalized gamma variates. Because of the inﬁnite sum involved in these
weights it did not, with some exceptions, garner much interest in earlier applications.
Sethuraman (1994) [see also Sethuraman and Tiwari (1982)] remedied this problem
by giving a simple construction, the so-called stick-breaking construction, and the
interest was renewed. His weights are constructed as follows:
p1 D V1 and pi D Vi
i1
Y
jD1
.1  Vj/; i D 1; 2; : : : ; and Vi
iidÏ Be .1; ˛ .X// :
(1.2.2)
In fact a second wave of generalization in the recent years got boost from this
alternative Sethuraman representation and served as an important tool leading to
a dramatic increase in the development of new priors. By varying the ingredients of
this inﬁnite sum representation, several new processes were developed, which we
call Ferguson–Sethuraman processes. Examples are:
If the inﬁnite sum in (1.2.1) is truncated at a ﬁxed or random N < 1; it generates
a class of discrete distribution priors studied by Ongaro and Cattaneo (2004);

1.3
Prior Processes
7
by replacing the parameters .1; ˛ .X// of the beta distribution with real numbers
.ai; bi/ ; i D 1; 2; : : : Ishwaran and James (2001) deﬁned stick-breaking priors; by
indexing i with a covariate x D .x1; : : : ; xk/ ; denoted as ix; MacEachern (1999)
deﬁned a class of dependent DPs which includes spatial and time-varying processes
as well; by replacing the degenerate probability measure ı by a nondegenerate
positive probability measure G; Dunson and Park (2008) introduced kernel DPs.
The above stick-breaking construction as well as the prediction rule based on a
generalized Polya urn scheme proposed by Blackwell and MacQueen (1973) has
been found useful in the development of new processes, two of which are popularly
known as the Chinese restaurant and Indian buffet processes. They have applications
in nontraditional ﬁelds such as word documentation, machine learning, and mixture
models.
A further generalization is also possible. Recall that Ferguson (1973) deﬁned
the DP alternatively by taking a normalized gamma process. This suggests a
natural generalization by deﬁning a random distribution function via a normalized
increasing additive process Z .t/ (or independent increment process) with Z D
limt!1 Z .t/ < 1: Regazzini et al. (2003) pursue this path. Note that Doksum
(1974) used the reparametrization F .t/ D 1eYt; with Yt as an increasing additive
process [Walker and Muliere (1997a), took Yt to be a log-beta process].
A brief exposé of these major processes follows. Details are discussed in
subsequent chapters organized by grouping together related processes.
A recently published chapter by Lijoi and Prünster (2010) provides a uniﬁed
framework for several priors processes in terms of the concept of completely random
measures studied by Kingman (1967), which is a generalization to abstract spaces
of independent increment processes on the real line. They can be generated via
the Poisson process Kingman (1993) by specifying the appropriate mean measure
of the Poisson process. This will be further elaborated in Chap. 4. Lijoi and
Prünster’s formulation is elegant but essentially the same. However, we will stick
with the original approach in which the priors have been constructed by suitable
modiﬁcations of Lévy measures of the processes with independent nonnegative
increments. The rationale being that it provides a historical view of the development
of these processes, and perhaps easy to understand. It also reveals how these
measures came about, for example, in the development of the beta and beta-Stacy
processes, which is not evident by the completely random measures approach.
1.3
Prior Processes
In this section we introduce major processes brieﬂy.
Ferguson’s Dirichlet process is an extension of the k-dimensional Dirichlet
distribution to a process. It essentially met the two basic requirements of a prior
process. It is simple, deﬁned on an arbitrary probability space and belonged to a
conjugate family of priors. Lijoi and Prünster (2010) deﬁne two types of conjugacy:
structural and parametric. In the ﬁrst one, the posterior distribution has the same

8
1
Prior Processes: An Overview
structure as the prior, where as in the second case, the posterior distribution is same
as the prior but only the parameters are updated. Neutral to the right processes are
an example of the ﬁrst kind and the Dirichlet process is an example of the second.
While the conjugacy offers mathematical tractability, it may also be construed as
limiting the class of posterior distributions.
The Dirichlet process has one parameter which is interpretable. If we have
a random sample X D .X1; : : : ; Xn/ from P and P Ï D .˛/ ; then Ferguson
(1973) proved that the posterior distribution, given the sample is again a Dirichlet
process with parameter ˛ C Pn
iD1 ıxi; i.e., PjX Ï D

˛ C Pn
iD1 ıxi

(parametric
conjugacy). Thus it is easy to compute the posterior distribution, by simply updating
the parameter of the prior distribution. This important property made it possible
to derive nonparametric Bayesian estimators of various functions of P, such as
the distribution function, the mean, median, and a number of other quantities, by
simply updating ˛: In fact the parameter ˛ may be considered as representing two
parameters: F0 ./ D ˛ ./ D ˛ ./ =˛ .X/ and M D ˛ .X/ : F0 is interpreted
as prior guess at random function F; or prior mean, and M as prior sample size
or precision parameter indicating how concentrated the F’s are around F0: [Doss
(1985a,b) accentuates this point by constructing a prior on the space of distribution
functions in the neighborhood of F0.] The posterior mean of F is shown to be a
convex combination of the prior guess F0 and the empirical distribution function
Fn. If M ! 0; it reduces to the classical maximum likelihood estimator (MLE) of
F. On the other hand, if M ! 1; it reduces to the prior guess F0: This phenomena
is shown to be true in many estimation problems.
Ferguson (1973) proved various properties and showed their applicability in
solving nonparametric inference problems by giving several illustrative examples.
His initiative set the tone and created a surge in the activity. Numerous papers
were published thereafter describing its utility in treating many of nonparametric
problems from the Bayesian point of view. These applications include sequential
estimation, empirical Bayes estimation, conﬁdence bands, hypothesis testing, and
survival data analysis, to name a few and presented in Chaps. 6 and 7. Dirichlet
process is also neutral to the right process, and is essentially the only process that is
tailfree with respect to every sequence of partitions. It is also the only prior process
such that the distribution of P .A/ depends only upon the number of observations
falling in the set A and not on where they fall. This may be considered as a
weakness of the prior. Also in the predictive distribution of a future observation, the
probabilities of selecting a new or duplicating a previously selected observation do
not depend upon the number of distinct observations encountered thus far. However,
to remedy this deﬁciency a two-parameter Poisson–Dirichlet process (Pitman and
Yor 1997) is developed.
A major deﬁciency though is that its support is conﬁned to discrete probability
measures only. Nevertheless, several recent applications in the ﬁelds of machine
learning, document classiﬁcation, etc., have proved that this deﬁciency is after all
not as serious as previously thought, and on the contrary is useful in modeling such
data. In fact, the Sethuraman representation has unleashed a ﬂood of new processes

1.3
Prior Processes
9
to model various types of data, as indicated later. Thus its popularity has remained
unabated.
While the Dirichlet process has many desirable features and is popular, it was
inadequate in treating certain problems encountered in practice, such as density
estimation, bioassay, problems in reliability theory, etc. Similarly, it is inadequate
in modeling hazard rates and cumulative hazard rates. Therefore several new, and in
some cases extensions, are proposed in the literature as mentioned above. They are
outlined next.
The Dirichlet process is nonparametric in the sense that it has a broad support. In
certain situation, however, Dalal (1979a) saw the need that the prior should account
for some inherent structure present, such as symmetry, in the case of estimation of a
location parameter, or some invariance property. This led him to deﬁne a process
which is invariant, with respect to a ﬁnite group of measurable transformations
G D fg1; : : : ; gkg; gi W X ! X; i D 1; : : : ; k; and which selects an invariant
distribution function with probability one. He calls it a Dirichlet Invariant process
with parameter ˛; a positive ﬁnite measure, and denoted by DGI.˛/. The Dirichlet
process is a special case with the group consisting of a single element, the identity
transformation. The conjugacy property also holds true for the Dirichlet invariant
process. That is, if P Ï DGI.˛/, and X1; : : : ; Xn is a sample of size n from P,
then the posterior distribution of P given X1; : : : ; Xn is DGI.˛ C Pn
iD1 ıg
Xi/, where
ıg
Xi D .1=k/ Pk
iD1 ıgXi. It is found to be useful in solving certain estimation problems
regarding location.
In dealing with the estimation of dose–response curve or estimation based on
the right censored data, if the Dirichlet process prior is assumed, it was found that
the posterior distribution was not a Dirichlet process, but a mixture of Dirichlet
processes. This lead to the development of mixtures of Dirichlet processes (Antoniak
1974). Roughly speaking, the parameter ˛ of the Dirichlet process is treated as
random indexed by u; u having a distribution, say, H: Thus P is said to have
a mixture of Dirichlet processes (MDP) prior, if P Ï
R
D .˛u/ dH .u/ : It has
some attractive properties and is ﬂexible enough to handle purely parametric or
semiparametric models. This has lead to the development of mixture models. In
fact, its applications in modeling high dimensional and complex data have exploded
in recent years (Müller and Quintana 2004; Dunson and Park 2008). Clearly, the
Dirichlet process is a special case of MDP.
Like the Dirichlet process, MDP also has the conjugacy property. Let  D
.
1; : : : ; 
n/ be a sample of size n from P; P Ï
R
U D.˛u/dH.u/; then Pj
Ï
R
U D.˛u C Pn
iD1 ı
i/dH.u/; where H is the conditional distribution of u given :
An important result proved by Antoniak is that if we have a sample from a mixture of
Dirichlet processes and the sample is subjected to a random error, then the posterior
distribution is still a mixture of Dirichlet processes. MDP is shown to be useful in
treating estimation problems in bioassay. However, because of the multiplicities of
observations that we expect in the posterior distribution, explicit expressions for the
posterior distribution are difﬁcult to obtain. Nevertheless, with the development of
computational procedures, this limitation has practically dissipated.

10
1
Prior Processes: An Overview
The Dirichlet process had only one parameter and it was easy to carry out the
Bayesian analysis. However, Doksum (1974) saw it as a limitation and discovered
that if the random P is deﬁned on the real line R, it is possible to deﬁne a more
ﬂexible prior. He introduced a neutral to the right process which is based on
independence of successive normalized increments of F and represents unfolding
of F sequentially. That is, for any partition of the real line, 1 < t1 < t2 <    <
tm < 1; for m D 1; 2; : : :, the successive increments F.t1/; .F.t2/  F.t1// =.1 
F.t1//; : : : are independent. In other words, F is said to be neutral to the right,
if there exist independent random variables V1; : : : ; Vm such that the distribution
of the vector .1  F.t1/; 1  F.t2/; : : : ; 1  F.tm// is same as the distribution of
(V1; V1V2; : : : ; Qm
1 Vi/. Thus the prior can be described in terms of several quantities
providing more ﬂexibility. Furthermore the Dirichlet process deﬁned on the real
line is a neutral to the right process. Doksum proved the conjugacy property with
respect to the data which may include right censored observations as well, i.e., if
the prior is neutral to the right, so is the posterior. However, the expressions for the
posterior distribution are complicated. Ferguson (1974) showed that it is possible to
describe the posterior distribution in simple terms. The neutral to the right process
is found to be especially useful in treating problems in survival data analysis but has
its own weaknesses. Its parameters are difﬁcult to interpret and like the Dirichlet
process, it also concentrates on discrete distribution functions only. However, some
speciﬁc neutral to the right type processes, such as beta and beta-Stacy, have been
since developed which soften the deﬁciency. These processes provide a compromise
between the Dirichlet process and the neutral to the right process. They alleviate the
drawbacks, and at the same time, are more manageable, parameters are interpretable
and they are conjugate with respect to the right censored data.
The neutral to the right process can also be viewed in terms of a process
with independent nonnegative increments (Doksum 1974; Ferguson 1974) via the
reparametrization F .t/ D 1  eYt; where Yt is a process with independent
nonnegative increments (also known as positive Levy process). The DP corresponds
to one of these Yt processes. Thus a prior on F can be placed by using such
processes. This representation is key to the development of a class of neutral to the
right or like neutral to the right processes to suit the needs of different applications.
They are constructed by selecting a speciﬁc independent increment process, such
as, gamma, extended gamma, beta, and log-beta processes. The log-beta process
leads to a beta-Stacy process prior on F which is a neutral to the right process. The
processes with independent nonnegativeincrements are extensively studied and they
have been used successfully in developing priors with appropriate modiﬁcation of
the Lévy measure involved. They all belong to the family of Levy processes. The
advantage in some cases is that a posterior distribution could be described explicitly
having the same structure as the prior, while in other cases only the parameters
needed to be updated. This was demonstrated in Doksum (1974), Ferguson (1974),
and Ferguson and Phadia (1979), and subsequently in other papers (Wild and
Kalbﬂeisch 1981; Hjort 1990; Walker and Muliere 1997a) and was especially shown
to be convenient in dealing with the right censored data.

1.3
Prior Processes
11
While the processes with independent increments mentioned above may be
used to deﬁne priors on the space of all distribution functions, Kalbﬂeisch (1978),
Dykstra and Laud (1981), and Hjort (1990) saw the need to deﬁne priors on
the space of hazard rates and cumulative hazard rates. In view of the above
reparametrization, F may also be viewed in terms of a random cumulative hazard
function. In the discrete case, for an arbitrary partition of the real line, 1 < t1 <
t2 <    < tm < 1; let qj denote the hazard contribution of the interval Œtj1; tj/,
i.e., qj D

F.tj/  F.tj1/

=.1  F.tj1//: Then the cumulative hazard function
Y.t/ is the sum of hazard rates rj’s, Y.t/ D P
tjt  log.1  qj/ D P
tjtrj; and
Y.t/ is identiﬁed as the cumulative hazard rate: Therefore, in covariate analysis
of survival data, Kalbﬂeisch assumed rj to be independently distributed as gamma
distribution and thus was able to deﬁne a gamma process prior on the space of
cumulative hazard rates, which led him to obtain the Bayes estimator for the survival
function, although this was not his primary interest. In fact he was treating the
baseline survival function as a nuisance parameter in dealing with covariate data
under the Cox model and wanted to eliminate it.
Dykstra and Laud (1981) also notes this relationship. However, their interest
being in hazard rates, they deﬁne the hazard rate in a more generalized form,
r.t/ D R t
0 ˇ .s/ dZ .s/ ; ˇ .s/ > 0: By taking Z to be a gamma process, they place a
prior on the space of all hazard rates and call it an extended gamma process. It can
also be used to deal with a distribution function. Its parameters are interpretable.
They show it to be conjugate with respect to the right censored data. But in the case
of exact observations, the posterior turns out to be a mixture of extended gamma
processes and the evaluation of resulting integrals becomes difﬁcult.
Hjort (1990) introduced a different prior process to handle the cumulative
hazard function. Like Kalbﬂeisch, he also deﬁnes the cumulative hazard rate as
the sum of hazard rates in the discrete case (integral in the continuous case). It
is clear that Y D  log.1  F/, and if F is absolutely continuous, then Y is the
cumulative hazard function. To allow the case when the F may not have a density,
he deﬁnes a new general form of the cumulative hazard function H such that
F.t/ D 1  …t
0f1  dH.t/g; where … is the product integral. This creates a problem
in deﬁning a suitable prior on the space of all H’s: Still, he attempts to model
it as an independent increment process and takes the increments to be distributed
approximately as beta distribution. Since the beta distribution lacks the necessary
convolution properties, he had to get around it by deﬁning in terms of “inﬁnitesimal”
increments being beta distributed. Hjort uses this relationship to deﬁne a prior on the
space of all cumulative hazard rates and consequently, on the distribution functions
as it generates a proper CDF. He calls the resulting process a beta process. The beta
process is shown to be conjugate with respect to the data, which may include right
censored observations, and its posterior distribution is easy to compute by updating
the parameters. It covers a broad class of models in dealing with life history data,
including Markov Chain and regression models, and its parameters are accessible
to meaningful interpretation. When B is viewed as a measure of the beta process, it
turns out to be the de Finetti measure of the Indian buffet process.

12
1
Prior Processes: An Overview
By taking Y to be a log-beta process, Walker and Muliere (1997a) proposed a
new prior process on the space of all distribution functions deﬁned on Œ0; 1/; and
called it a beta-Stacy process. The process uses a generalized beta distribution and in
that sense can be considered as a generalization of the beta process. Its parameters
were deﬁned in terms of the parameters of the log-beta process. By taking these
parameters in more general forms they are able to construct a process whose
support includes absolutely continuous distribution functions, thereby extending the
Dirichlet process. In fact it generalizes the Dirichlet process in the sense that it offers
more ﬂexibility and unlike the Dirichlet process, it is conjugate to the right censored
data. It also emerges as a posterior distribution with respect to the right censored data
when the prior is assumed to be a Dirichlet process. It has some additional pluses
as well. Its parameters have reasonable interpretation; it is a neutral to the right
process; and the posterior expectation of the survival function obtained in Susarla
and Van Ryzin (1978a,b) turns out to be a special case.
The random probability measure associated with many of the above processes is
completely random measures (Kingman 1967) on the real line. As the completely
random measures can be constructed via the Poisson process Kingman (1993)
with suitable mean measures, so are these processes. For example, the gamma
process with parameter c > 0; and base measure G0 is generated when the mean
measure is given by  .d
; dp/ D cp1ecpdpG0 .d
/ ; p > 0. Let f.
i; pi/g denote
the points obtained from the Poisson process with mean measure  and deﬁne
i D pi= P1
jD1 pj: Then P D P1
iD1 iı
i is the Dirichlet process with parameters ˛ D
G0 ./ and F0 D ˛1G0. It should be noted that due to normalization, the Dirichlet
process is not a completely random measure since for A1; A2 2 ; P .A1/ and
P .A1/ are not independent but negatively correlated. Beta process with parameter
c > 0; and base measure B0 is generated when the mean measure of Poisson process
is given by  .d
; dp/ D cp1 .1  p/c1 dpB0 .d
/ ; 0 < p < 1:
The tailfree and Polya tree processes are deﬁned on the real line based on a
sequence of nested partitions of the real line and the property of independence
of variables between partitions. Their support includes absolutely continuous
distributions. They are ﬂexible and are particularly useful when it is desired to
give greater weights to the regions where it is deemed appropriate, by selecting
suitable partitions. They possess the conjugacy property. However, unlike the case
of the Dirichlet and other processes, the Bayesian results based on these priors are
strongly inﬂuenced by the partitions chosen. Furthermore, it is difﬁcult to derive
resulting expressions in closed form and the parameters involved are difﬁcult to
interpret adequately. The Dirichlet process is essentially the only process which is
tailfree with respect to every sequence of partitions.
Lavine (1992, 1994) specializes the tailfree process in which all variables
involved, not just variables between partitions, are assumed to be independent
having a beta distribution. This way the expressions are manageable. He names
the resulting process as a Polya Tree process. It is shown that this process preserves
the conjugacy property and for the posterior distribution, one has only to update
the parameters of the beta distributions. The predictive distribution of a future

1.3
Prior Processes
13
observation under the Polya tree prior has a simple form and is easily computable.
Under certain constraints on the parameters, the Polya tree prior reduces to the
Dirichlet process.
Mauldin et al. (1992) propose a different method of constructing priors, via Polya
trees, which is slightly a generalization of Lavine’s approach. Their approach is
to generate sequences of exchangeable random variables based on a generalized
Polya urn scheme. By a de Finetti theorem each such sequence is a mixture of iid
random variables. The mixing measure is viewed as a prior on distribution functions.
It is shown that this class of priors also form a conjugate family which includes
the Dirichlet process and can assign probability one to continuous distributions.
A thorough study of such an approach is carried out in their paper. However the
approach is complicated, and from the practical point of view it is not clear if it
provides any advantage over Lavine’s Polya Tree process.
A broad and useful review of these processes with discussion may be found in
Walker et al. (1999).
In addition to the core processes described above, the Ferguson–Sethuraman
countable mixture representation of the DP, as alluded to in the previous section,
proved to be an important tool in developing a large number of prior processes in
order to address nonparametric Bayesian treatment of models involving different
and complex types of data. Many of these are offshoots of the Dirichlet process but
there are others as well. We describe them brieﬂy here and more detailed treatment
will be given in Chap. 3.
Recall that the Sethuraman’s representation of the Dirichlet process with param-
eter ˛ is P D P1
iD1 piıi where ıi denotes a discrete measure concentrated at
i; i’s are independent and identically distributed according to the distribution
˛=˛ .X/ I and pi (known as random weights) are chosen independently of i as
deﬁned in (1.2.2). Based on this representation, possibilities for developing several
new prior processes by varying the way the weights and locations are deﬁned, seem
natural.
By allowing the possibility of the inﬁnite sum to be truncated at N  1; and
assuming the distribution of Vi in the construction of pi as Be.ai; bi/, ai  0; bi  0;
instead of Be .1; ˛ .X// ; Ishwaran and James (2001) deﬁne a family of priors called
stick-breaking priors. Besides the DP, it includes several other priors as well. By
truncating the sum to a positive integer N; which could be random as well, a class of
discrete prior processes can be generated. Ongaro and Cattaneo (2004) follow this
approach and point out that such processes lack conjugacy property. If ai D a and
bi D b, then we have a process known as two-parameter beta process (Ishwaran and
Zarepour 2000). On the other hand, if ai D 1  a and bi D b C ia, with 0  a < 1
and b > a, then it identiﬁes the 2-parameter Poisson–Dirichlet process described
by Pitman and Yor (1997) (also known as Pitman–Yor process). The process itself
is a two-parameter generalization of the Poisson–Dirichlet distribution derived by
Kingman (1975) as a limiting distribution of decreasing ordered probabilities of a
Dirichlet distribution. Obviously, Dirichlet process is a special case of this process
when a D 1 and b D ˛ ./ ; and when b D 0; we obtain a stable-law process.

14
1
Prior Processes: An Overview
The above generalization allowed truncation of the inﬁnite sum and modiﬁcation
of the weights, but locations i’s and the degenerate measure ı were untouched.
By modifying them as well, several new processes have been proposed with
their newer applications in the ﬁelds as diverse as machine learning, population
genetics, and ecology. Therefore we enlarge the family, and since they all originate
from the countable mixture representations of Ferguson and Sethuraman, they
should rightly be called, as we do, Ferguson–Sethuraman priors, and reserve the
phrase stick-breaking to indicate the process of construction of the weights. Thus,
Ferguson–Sethuraman processes have a basic form of countable mixture of (mostly
unit) masses placed at iid location (mostly) random variables such that the sum of
the weights (need not be constructed as in (1.2.2) is equal to 1 and
P./ D
1
X
iD1
piıi./:
(1.3.1)
To accommodate covariates, the locations i and/or the weights (via Vi’s) are
modiﬁed to depend on vectors of covariates. Let the covariate be denoted by x 2 ;
where  is a subset of k-dimensional Euclidean space Rk: In the single covariate
model, i.e., when k D 1; replace each j by jx; and deﬁne Px ./ D P1
jD1 pjıjx ./ :
That is, each j is replaced by a stochastic process jx; indexed by x 2 : Then the
collection of Px forms a class of random probability measures indexed by x 2 : In
this formulation, the locations of point masses have been indexed by the covariates,
but the weights pj are undisturbed, which could also be made to depend on x. When
weights are constructed as in (1.2.2), this class of priors includes processes such as
the dependent Dirichlet (MacEachern 1999), ordered Dirichlet (Grifﬁn and Steel
2006), kernel stick breaking (Dunson and Park 2008), and local Dirichlet processes
(Chung and Dunson 2011). Gelfand et al. (2005) saw the need to extend the above
deﬁnition of the dependent Dirichlet process by allowing the locations x to be drawn
from random surfaces to create a random spatial process. They named it as Spatial
Dirichlet process. It is essentially a Dirichlet process deﬁned on a space of surfaces.
On the other hand, Rodriguez et al. (2008) replace each random location  in the
above inﬁnite sum representation with a random probability measure drawn from a
Dirichlet process, thus creating a nested affect. They named the resulting process as
nested Dirichlet process. Dunson and Park (2008) replaced the unit measure ı by
a nondegenerate measure in developing their process called the kernel based stick-
breaking process.
In deﬁning the beta process mentioned earlier, Hjort’s (1990) primary interest
was in developing a prior distribution on the space of cumulative hazard functions
and therefore the sample paths were deﬁned on the real line. Thibaux and Jordan
(2007) saw the need to deﬁne the sample paths of the beta process on more general
spaces. So they modify the underlying Levy measure of the beta process and
show that the resulting process serves as a prior distribution over a class of sparse
binary matrices encountered in featural modeling. It is conjugate with respect to
a Bernoulli process and is the de Finetti measure for the Indian buffet process. It

1.3
Prior Processes
15
can also be constructed by the stick-breaking construction method. Teh and Gorur
(2009) generalize the beta process by introducing a stability parameter thereby
incorporating the power-law behavior and name the resulting process as Stable-beta
process. Ren et al. (2011) deﬁne kernel beta process to model covariate data similar
to the dependent DP.
Unfortunately, there are no explicit expressions in closed form for the posterior
distributions in case of the above processes and one has to rely on simulations
methods. For this purpose, simulation algorithms are developed and provided by
the respective authors. The development on this line has seen a tremendous growth
in modeling large and complex data in ﬁelds outside the mainstream statistics and
scores of papers have been published in recent years. Two processes have especially
caught the attention of practitioners in different ﬁelds, and a third to lesser extent.
The Chinese restaurant process (CRP) is a process of generating a sample from
the Dirichlet process and is equivalent to the extended Polya urn scheme introduced
by Blackwell–MacQueen (1973). It is related to the culinary interpretation where
in a stream of N patrons enter a restaurant and are randomly seated on tables.
It describes the marginal distribution of the Dirichlet process in terms of random
partition of patrons determined by K tables they occupy. Samples from the Dirichlet
process are probability measures and samples from the CRP are partitions. Teh et al.
(2006) proposed a further generalization as franchised CRP which corresponds to a
hierarchical Dirichlet process.
The Indian Buffet process (IBP) proposed by Grifﬁths and Ghahramani (2006) is
essentially a process to deﬁne a prior distribution on the equivalence class of sparse
binary matrices (entries of the matrix have binary responses, 0 or 1) consisting of
a ﬁnite number of rows and an unlimited number of columns. Rows are interpreted
as objects (patrons) and columns (tables) as potentially unlimited features. It has
applications in dealing with featural models where the number of features may be
unlimited and need not be known a priori. In contrast to the CRP, here the matrix
may have entries of 1’s in more than one column in each row. It is an iid mixture
of Bernoulli processes with mixing measure the beta process. Thus it can serve as
a prior for probability models involving objects and features encountered in certain
applications in machine learning, such as image processing. It also provides a tool to
handle nonparametric Bayesian models with large number of latent variables. Like
the DP, the IBP also can be constructed by the stick-breaking construction. Further
two and three parameter extensions of the process are proposed in the literature.
A further generalization is proposed by Titsias (2008) where the binary matrices
have been replaced by nonnegative integer valued matrices and a distribution called
inﬁnite gamma-Poisson process is deﬁned as prior on the class of such equivalent
matrices. In this model, the features are allowed to reoccur more than once.
Other processes, some of which are generalizations of the Dirichlet process
and beta process are also mentioned brieﬂy. They include, for example, ﬁnite
dimensional DPs and multivariate DP. Various attempts to extend some of these
processes fruitfully to the bivariate case have remained challenging and mostly
unsuccessful so far. However in one case it is shown that it can be done. A bivariate

16
1
Prior Processes: An Overview
tailfree process (Phadia 2007; Paddock et al. 2003) is constructed on a unit square
and presented in the last section.
Many of the above mentioned processes may be considered as special cases of a
class of models proposed by Pitman (1996b) called species sampling models,
P./ D
1
X
jD1
pjıj./ C
0
@1 
1
X
jD1
pj
1
A Q ./ ,
(1.3.2)
where Q is the probability measure corresponding to a continuous distribution
G; j
iidÏ G; and weights pj’s are constrained by the condition, P1
jD1 pj  1:
However, this model is not popular in statistics.
In summary, from the above description of various processes, it is clear that
each process has its own merits as well as certain limitations. They have been
developed, in some cases, to address speciﬁc needs. Nevertheless, Dirichlet process
and its generalizations have emerged as most useful tools in carrying out non-
parametric Bayesian analysis. Thus, when to use which prior process very much
depends upon what is our objective and what kind of data we have on hand.
Practical considerations, such as incorporation of prior information, mathematical
convenience, computational feasibility, and interpretation of parameters involved
and the results obtained, play critical roles in the decision. Judging from the various
applications that are published in the literature and presented in the applications
chapters, it appears that the Dirichlet process and mixtures of Dirichlet processes
have a substantial edge over the others not withstanding their limitations. However,
this seems to be changing in the current trend towards dealing with Big Data.
Figure 1.1 depicts the connection among the various prior processes.
The development of the above mentioned processes made the nonparametric
Bayesian analysis feasible, but had limitations due to complexities involved in
deriving explicit formulae. Therefore, the attention was focussed in the past on
those applications where the expressions could be obtained in closed form and the
obvious choice was the Dirichlet process. However in recent years a tremendous
progress is made in developing computational methods, such as Gibbs sampler,
importance sampling, slice and retrospective sampling, etc., for simulating the
posterior distributions for implementation of Bayesian analysis which has made it
possible to handle more complex models. And in view of the phenomenal increase in
cheap computing power, the previous limitations have almost dissipated. In fact, the
mixtures of Dirichlet processes have been found to be hugely popular in modeling
high dimensional data encountered in practice. For example, in
addition to the
analysis of survival data, now it is possible as indicated earlier, to implement full
Bayesian analysis in treating covariate models, random effect models, hierarchical
models, wavelet models, etc. The present trend has been to combine parametric

1.3
Prior Processes
17
DP
DIP
MDP
GDP
DMV
NTR
IIP
GP
EGP
PT
TP
MPT
BSP
BP
HBP
HDP
ISR
SSM
DD
FD
CRP
PY
PD
IBP
TBP
DDP
KBP
SDP
Relationship among various processes
Fig. 1.1 An arrow A ! B signiﬁes either B generalizes A; or B originates from A; or A can be
viewed as a particular case of B. Some relations need not be quite precise. A ! B suggests B
can be reached from A via a transformation. Processes in rectangles are identiﬁed as Ferguson-
Sethuraman processes. BP Beta Process, BSP Beta-Stacy Process, CRP Chinese Restaurant
Process, DD Discrete Distributions, DP Dirichlet Process, DIP Dirichlet Invariant Process, MDP
Mixtures of Dirichlet Processes, DDP Dependent Dirichlet Process, DMV Dirichlet Multivariate
Process, EGP Extended Gamma Process, FD Finite Dimensional Process, GDP Generalized
Dirichlet Process, GP Gamma Process, HBP Hierarchical Beta Process, IBP Indian Buffet Process,
IIP Independent Increments Process, ISR Inﬁnite Sum Representation, KBP Kernel Based Process,
NTR Neutral to the right Process, PD Poisson–Dirichlet Process, PT Polya Tree Process, PY
Pitman–Yor (Two-parameter Poisson–Dirichlet Process), SSM Species sampling Model, TBP Two-
parameter Beta Process, TP Tailfree Process
and semiparametric models in modeling such data. Books authored by Dey et al.
(1998), Ibrahim et al. (2001), and Müller et al. (2015) contain numerous examples
and applications, and are a good source to refer if one wants to explore further
from the application angle. See also Favaro and Teh (2013) for an extended list of
references.

Chapter 2
Dirichlet and Related Processes
2.1
Dirichlet Process
The Dirichlet process is an extension of the k-dimensional Dirichlet distribution
to a stochastic process. It is the most popular and extensively used prior in the
nonparametric Bayesian analysis. In fact it is recognized that with its discovery,
Ferguson’s path breaking paper laid the foundation of the ﬁeld of nonparametric
Bayesian statistics. Its success can be traced to its mathematical tractability, simple
and attractive properties, and easy interpretation of its parameters. Dirichlet process
(DP) and its offshoots, such as mixtures of Dirichlet processes (MDPs), hierarchical
Dirichlet process (HDP), and dependent and spatial Dirichlet processes, are most
important and widely used priors in modeling high dimensional and covariate data.
This is made possible due to the development of computational techniques that make
full Bayesian analysis feasible.
A Dirichlet process prior with parameter ˛ D MF0 for a random distribution
function F (or a random probability measure [RPM] P) is a probability measure
on the space of all distribution functions F (space of all RPMs) and is governed
by two parameters: a baseline distribution function F0 that deﬁnes the “location”
or “center” or “prior guess” of the prior, and a positive scalar precision parameter
M which governs how concentrated the prior is around the prior “guess” or baseline
distribution F0. The latter therefore measures the strength of belief in the prior guess.
For large values of M, a sampled F is likely to be closed to F0. For small values of M,
it is likely to put most of its mass on just a few atoms. It is concentrated on discrete
probability distributions. Ferguson deﬁnes the Dirichlet process more broadly in
terms of an RPM. We will follow his lead.
In this section we will present various features of the Dirichlet process: the
original and alternative deﬁnitions; the close relationship between the parameter
˛ and the random probability PI drawing a sample P; the posterior distribution of P
given a random sample from it; procedures for generating samples from the same;
numerous properties and characterization of the DP; and its various extensions.
© Springer International Publishing Switzerland 2016
E.G. Phadia, Prior Processes and Their Applications, Springer Series in Statistics,
DOI 10.1007/978-3-319-32789-1_2
19

20
2
Dirichlet and Related Processes
2.1.1
Deﬁnition
Let P be a probability measure deﬁned on a measurable space .X; A/, where X is a
separable metric space and A D  .X/ is the corresponding -ﬁeld of subsets of X,
and … be a set of all probability measures on .X; A/. In our context P is considered
to be a parameter and (…;  .…// serves as the parameter space. Thus P may be
viewed as a stochastic process indexed by sets A 2 A and is a mapping from …
into Œ0; 1. That is, fP .A/ W A 2 Ag is a stochastic process whose sample functions
are probability measures on .X; A/. P being a probability is a measurable map
from some probability space .;  ./ ; 	/ to the space (…;  .…//. Alternatively,
it means that P .; / is a measurable map from the product space  A into Œ0; 1
such that for every ! 2 ; P .!; / is a probability measure on .X; A/; and for
every set A 2 A; P .; A/ is a measurable function (random variable) on .;  .//
taking values in Œ0; 1. In our treatment we will suppress reference to ! 2 
unless it is required for clarity. The distribution of P is a probability measure on

Œ0; 1A ; 

BA
where 

BA
denotes the -ﬁeld generated by the ﬁeld BA of
Borel cylinder sets in Œ0; 1A. F is a cumulative distribution function corresponding
to P and let F denote the space of all distribution functions.
In his fundamental paper, Ferguson (1973) developed a prior process on the
parameter space (…;  .…/) which he called the Dirichlet process [Blackwell
(1973) and Blackwell and MacQueen (1973) named it as Ferguson prior]. It is
especially convenient since it satisﬁes the two desirable properties mentioned in the
earlier chapter on overview. Because of its simplicity and analytical tractability, the
Dirichlet Process has been widely used despite its limitation that it gives positive
probability to discrete distributions only. However it turns out to be an asset in
certain areas of applications such as modeling grouped and covariate data and
species sampling, as will be seen later in Chap. 3.
Let D.1; : : : ; k/ denote a .k1/-dimensional Dirichlet distribution with density
function given by
f .x1; : : : ; xk1/ D  .1 C    C k/
 .1/     .k/
k1
Y
iD1
xi1
i
 
1 
k1
X
iD1
xi
!k1
;
(2.1.1)
over the simplex: S W
n
.x1; : : : ; xk1/ W xi  0; i D 1; : : : ; k  1; Pk1
iD1 xi  1
o
where
all i are positive real numbers. The Dirichlet distribution has many interesting
properties which lead to the corresponding properties of the Dirichlet process.
For example, its representation in terms of gamma random variables leads to the
alternative deﬁnition of the Dirichlet process. Its tailfree property shows that the
Dirichlet process is a tailfree process. Also, its neutral to the right property shows
that the Dirichlet process is a neutral to the right process. These and many other
properties of the Dirichlet distribution are discussed in great details by Basu and
Tiwari (1982).

2.1
Dirichlet Process
21
Before giving a formal deﬁnition, we must ﬁrst ﬁx the notion of a random
probability measure. Since P is a stochastic process, it can be deﬁned by spec-
ifying the joint distribution of the ﬁnite dimensional vector of random variables
.P .A1/ ; : : : ; P .Am//, for every positive integer m and every arbitrary sequence of
measurable sets A1; : : : ; Am belonging to A, such that Kolmogorov consistency
is satisﬁed. This would then imply that there exists a probability distribution P
on

Œ0; 1A ; 

BA
yielding these ﬁnite dimensional distributions. Since such
a sequence can be expressed in terms of mutually disjoint sets B1; : : : ; Bk; with
[k
iD1Bi D X (by taking intersections of the Ai and their complements), it is sufﬁcient
to deﬁne the joint distribution of .P .B1/ ; : : : ; P .Bk// with P .¿/ D 0 which
meets the consistency condition. Therefore it is sufﬁcient to satisfy the following
condition:
Condition C: If

B0
1; : : : ; B0
k0

and .B1; : : : ; Bk/ are measurable partitions, and
if

B0
1; : : : ; B0
k0

is a reﬁnement of .B1; : : : ; Bk/ with B1 D [r1
jD1B0
j; : : : ; Bk D
[k0
jDrk1C1B0
j, then the distribution of
0
@
r1
X
jD1
P B0
j
 ; : : : ;
k0
X
jDrk1C1
P B0
j

1
A
(2.1.2)
as obtained from the joint distribution of

P

B0
1

; : : : ; P

B0
k0

, is identical to the
distribution of .P .B1/ ; : : : ; P .Bk//.
This condition is shown to be sufﬁcient to validate Kolmogorov consistency
condition, and thus the existence of a measure on

Œ0; 1A ; 

BA
yielding the
given ﬁnite dimensional distribution will be established. Now to deﬁne the DP
which is a measure on

Œ0; 1A ; 

BA
, all that is to be done is to specify the
ﬁnite dimensional joint distribution. This is taken to be the Dirichlet distribution.
We say P is a random probability measure on .X; A/ (i.e., a measurable map
from some probability space .;  ./ ; Q/ into .…; .…//), if the condition C
is satisﬁed; if for any A 2 A; P .A/ is random taking values in Œ0; 1, P .X/ D
1 a.s., and P is ﬁnitely additive in distribution. In this connection it is worth
noting that Kingman (1967) deﬁned a completely random measure (CRM) on an
abstract measurable space .‰;  .‰// as a measure 	 such that for any disjoint sets
A1; A2; : : : 2  .‰/, random variables 	 .A1/, 	 .A1/ ; : : : are mutually independent.
More detailed reference is made later in Sect. 4.1.
The Dirichlet process with parameter ˛, to be denoted as D.˛/, is deﬁned as
follows:
Deﬁnition 2.1 (Ferguson) Let ˛ be a non-null nonnegative ﬁnite measure on
.X; A/. A random probability P is said to be a Dirichlet process on .X; A/ with
parameter ˛ if for any positive integer k, and measurable partition .B1; : : : ; Bk/
of X; the distribution of the vector .P .B1/ ; : : : ; P .Bk// is Dirichlet distribution,
D.˛ .B1/ ; : : : ; ˛ .Bk//.

22
2
Dirichlet and Related Processes
In this deﬁnition, a parallel can be seen of the deﬁnition of a Poisson process (see
Sect. 4.1) on abstract spaces (Kingman 1993) and a gamma process. For example,
recall that a random measure 	 on a measurable space .‰;  .‰// is a gamma
process with parameter ˛ if for any positive integer k and disjoint measurable sets
A1; : : : ; Ak 2  .‰/ ; f	 .Ai/ W i D 1; ::; kg is a family of independent gamma random
variables with mean ˛ .Ai/ ; i D 1; : : : ; k; respectively, and scale parameter unity.
By verifying the Kolmogorov consistency criterion, Ferguson showed the exis-
tence of a probability measure P on the space of all functions from A into Œ0; 1
with -ﬁeld generated by the cylinder sets and with the property that the ﬁnite
dimensional joint distribution of probabilities of sets A1; : : : ; Ak is a Dirichlet
distribution. D.˛/ may thus be considered as a prior distribution on the space …
of probability measures in the sense that each realization of the process yields a
probability measure on .X; A/. Some immediate consequences of this deﬁnition are
as follows:
(a) The Dirichlet process chooses a discrete RPM with probability one. This is true
even when ˛ is assumed to be continuous.
(b) The Dirichlet process is the only process such that for each A 2 A; the posterior
distribution of P.A/ given a sample X1; : : : ; Xn from P depends only on the
number of X’s that fall in A and not where they fall.
(c) Let P Ï D.˛/ and let A 2 A. Then Antoniak (1974) has shown that given
P .A/ D c, the conditional distribution of .1=c/ P restricted to .A; A \ A/ is
a Dirichlet process on .A; A \ A/ with parameter ˛ restricted to A. That is,
for any measurable partition .A1; : : : ; Ak/ of A, the distribution of the vector
.P .A1/ =c; : : : ; P .Ak/ =c/ is Dirichlet, D.˛ .A1/ ; : : : ; ˛ .Ak//.
(d) Let {mI m D 1; 2; : : :g be a nested tree of measurable partitions of (R; B); that
is 1; 2; :: be a nested sequence of measurable partitions such that mC1 is a
reﬁnement of m for each m and [1
0 m generates B. Then the Dirichlet process
is tailfree with respect to every tree of partitions.
(e) Dirichlet process is neutral to the right with respect to every sequence of nested,
measurable ordered partitions.
The parameter, ˛; can in fact be represented by two quantities. The total mass
M D ˛ .X/ and the normalized function ˛ ./ D ˛ ./ =˛ .X/ which may be
identiﬁed with F0, the prior guess at F mentioned earlier in the section. The
parameter M plays a signiﬁcant role. Ferguson gave it the interpretation of prior
sample size. However, some unsavory features have been pointed out in Walker et al.
(1999). It controls the smoothness of F as well as the variability from F0. The prior-
to-posterior parameter update is M ! M C n and F0 ! .MF0 C nFn/ = .M C n/
which is a linear combination of F0 and Fn. When M ! 1, F tends to the prior
guess F0 ignoring the sample information. On the other hand, if M ! 0, the prior
provides no information. However, Sethuraman and Tiwari (1982) have shown that
this interpretation is misleading. Actually in that case F degenerates to a single
random point Y0 selected according to F0. Thus providing deﬁnite information that
F is discrete. This fact about M is used in deﬁning later the generalized Dirichlet

2.1
Dirichlet Process
23
process, where M is replaced by a positive function. For the sake of brevity, we will
write ˛ .a; b/ for ˛ ..a; b//, the ˛ measure of the set .a; b/.
Several alternative representations of the deﬁnition of Dirichlet process have
been proposed in the literature which are described next.
2.1.1.1
Alternative Representations of the Deﬁnition
Gamma Representation The above deﬁnition was given in terms of a stochastic
process indexed by the elements A 2 A. Ferguson also gave an alternative
deﬁnition in terms normalized gamma variables [also true for any normalized
random independent increments (Regazzini et al. 2003)]. Let G be a gamma
process with intensity parameter  ./ D MF0 ./, i.e.,  .A/  G .MF0 .A/ ; 1/,
a gamma distribution with parameters MF0 .A/ and 1. Then F ./ D  ./ = .X/ 
D .MF0/. It is deﬁned in terms of a countable mixture P1
jD1 Pjıj of point masses
at random points (of independent increments of gamma process) with mixing
weights derived from a gamma process. In doing so, he was motivated by the
fact that as the Dirichlet distribution is deﬁnable by taking the joint distribution of
gamma variables divided by their sum, so should the Dirichlet process be deﬁnable
as a gamma process with increments divided by their sum. Let P denote the
probability of an event. Let J1; J2; : : : be a sequence of random variables with
the distribution, P .J1  x1/ D exp fN .x1/g for x1 > 0; and for j D 2; 3; : : :,
P

Jj  xjjJj1 D xj1; : : : ; J1 D x1

D exp
˚
N

xj

 N

xj1

for 0 < xj < xj1;
where N .x/ D ˛ .X/
R 1
x u1eudu for 0 < x < 1. Then the sum Z1 D P1
jD1 Jj
converges with probability one and is a gamma variate with parameters ˛ .X/ and
1, G .˛ .X/ ; 1/. Deﬁne Pj D Jj= P1
iD1 Ji; then Pj  0 and P1
jD1 Pj D 1 with
probability one. Let j’s be iid X-valued random variables with common distribution
˛ ./ and independent of P1; P2; : : :. Then
Theorem 2.2 (Ferguson) The RPM deﬁned by
P.A/ D
1
X
jD1
Pjıj.A/; A 2 A;
(2.1.3)
where ıx is the degenerate probability measure at x, is a Dirichlet process on .X; A/
with parameter ˛.
The key step of the proof involves in showing that for any k and any
arbitrary partition .B1; : : : ; Bk/ of X, the distribution of .P .B1/ ; : : : ; P .Bk// D
1
Z1
P1
jD1 Jjıj.B1/; : : : ; P1
jD1 Jjıj.B1/

is D.˛ .B1/ ; : : : ; ˛ .Bk//. Towards this
end, he shows that for i D 1; : : : ; k, P1
jD1 Jjıj.Bi/
ind
 G .˛ .Bi/ ; 1/ by identifying
them as independent increments of the gamma process [using a theorem from
Ferguson and Klass (1972)], and that their sum is Z1
 G .˛ .X/ ; 1/. This
immediately yields the desired result using a property of the Dirichlet distribution.

24
2
Dirichlet and Related Processes
This representation is counter intuitive. It shows that the DP is a countable
mixture of point masses at randomly selected locations (selected iid ˛/ and decreas-
ing weights are the normalized gamma variates. Further it shows immediately
that P is a discrete probability measure a.s., a fact proved by various authors.
This has inspired the idea of deﬁning a.s. discrete nonparametric priors by way
of normalized completely random measures. Dirichlet process is a normalized
completely random measure. In fact, without normalization one can deal with
random measures themselves (as opposed to random probability measures) which
also yield a.s. discrete random measure priors and are found to be useful in certain
applications. This is so since the random discrete measures can be constructed using
the Poisson processes with speciﬁc Levy measures as their intensity measures.
Levy Measure Representation Since the gamma process is a completely random
measure (CRM) and CRM can be constructed by the Poisson process, Pitman
(1996a) describes the above weights Pj’s by ranking the points of a Poisson process.
Let J
1 > J
2 >    be the points of a Poisson process on .0; 1/ with mean measure
 .x/ D ˛ .X/ x1exdx. Then Pj D J
j = P1
iD1 J
i . Pj’s are in descending order and
the distribution of these weights is the Poisson–Dirichlet distribution, PD .˛ .X//
developed by Kingman (1975) and discussed later in Sect. 3.4.1. Ishwaran and
James (2001) describe them slightly differently. Let k D "1 C    C "k; where each
"i is distributed as exponential distribution with parameter 1, that is, "i
iidÏ exp.1/.
Let N1 stand for the inverse of the Lévy measure of a gamma distribution with
parameter ˛ .X/ ; where N .x/ D ˛ .X/
R 1
x u1eudu, for x > 0. The weights are
N1 
j

normalized by their sum P1
jD1 N1 
j

, a gamma .˛ .X// variable. Then
P./ D
1
X
jD1
N1 
j

P1
jD1 N1 
j
ıj./:
(2.1.4)
This form relies on the random weights constructed using inﬁnitely divisible random
variables.
As a further generalization, Regazzini et al. (2003) introduced a class of
normalized random distribution functions with independent increments. The idea
is to take the random probability distribution as a normalized increment process
F .t/ D Z .t/ =Z; where Z D limt!1 Z .t/ < 1 a.s. Ferguson took Z .t/ to
be a gamma process. Nieto-Barajas et al. (2004) consider the case of normalized
random distribution functions driven by an increasing additive process L, i.e.,
Z .t/ D R K .t; x/ dL .x/ and provide conditions on K and L so that F is a random
probability distribution function a.s.
Stick-Breaking Representation Ferguson’s alternative deﬁnition was based on
weights constructed using gamma variates. An extremely useful and popular
constructive deﬁnition of the DP, known as Stick-breaking representation, is given
by Sethuraman (1994) [see also Sethuraman and Tiwari (1982)] in which the
weights are derived using a beta distribution with parameters 1 and ˛ .X/, denoted

2.1
Dirichlet Process
25
as Be .1; ˛ .X//. His representation of an RPM P having a Dirichlet prior D.˛/ is
P.A/ D
1
X
jD1
pjıj.A/; A 2 A;
(2.1.5)
where j’s are as above,
p1 D V1; and for j  2; pj D Vj…j1
iD1.1  Vi/ with Vj
iidÏ Be .1; ˛ .X// ;
(2.1.6)
and independent of 1; 2; : : : . That is, the weights are generated by the so-called
stick-breaking (SB) construction. Break off a part of a stick of unit length randomly
according to Be .1; ˛ .X// and assign the length of the break part V1 to p1. Next,
from the remaining part 1  V1 of the stick, again break off randomly a V2 fraction
according to Be .1; ˛ .X// and set p2 D V2.1  V1/. Continue this process. At
the jth step break off a Vj fraction according to Be .1; ˛ .X// of the remaining
length of stick Qj1
lD1 .1  Vl/ and set pj D Vj
Qj1
lD1 .1  Vl/ D Vj

1  qj

where
qj D Pj
iD1 pi. This also shows that pj’s go to zero a.s. very fast. This process results
in a sequence of weights p1;p2; : : : of Sethuraman representation. The weights will
be denoted by SBW .˛ .X// and V ’s are known as stick-breaking ratios. In contrast
to Ferguson’s weights, these weights need not be ordered. Ferguson’s weights are
equivalent to these weights rearranged in a decreasing order. That is, Ferguson
weights are order statistics of these weights, p.1/;p.2/; : : : . This construction provides
a simple way to simulate F with D.˛/ prior and also to compute the distribution
of functionals of F such as
R
gdF D P pig .i/ for a measurable function g on
X. As will be seen later, the stick-breaking construction has been found useful in
representing other priors as well.
The SB construction generates a random discrete distribution p D . p1; p2; : : :/
and is of interest to many authors (see Pitman 1996a). The distribution of the vector
p is known as GEM .˛ .X// distribution after Grifﬁths (1980), Engen (1978) and
McCloskey (1965) who contributed to its development (Johnson et al. 1997, calls it
as generalized Engen–McCloskey and credits Grifﬁths for its popularization). It is
discussed later in Sect. 3.4 that if a sequence
˚
p
i

with p
1  p
2     is deﬁned
by ranking the sequence fpig with GEM.˛ .X// distribution, then the sequence
˚
p
i

has a Poisson–Dirichlet distribution with parameter ˛ .X/.
Like the Ferguson’s alternative deﬁnition, Sethuraman representation also reveals
that the random probability P is a countable mixture of point masses at random
locations 1; 2; : : :, chosen according to the distribution ˛ ./. However, the mixing
probabilities p1;p2; : : : are different and may be viewed as a discrete distribution
deﬁned on the set of nonnegative integers with V1; V2; : : : representing indepen-
dent discrete failure rates of this discrete distribution, and each distributed as
Be .1; ˛ .X//. This representation is more amenable to proving various useful
properties of the Dirichlet process and, as will be seen later, it forms the basis for
generating several extensions/generalization of the Dirichlet process.

26
2
Dirichlet and Related Processes
Theorem 2.3 (Sethuraman) P as deﬁned above in (2.1.5) is a Dirichlet process
with parameter ˛.
Its proof is interesting. We need to show ﬁrst that a measure P exists, and second
that its ﬁnite dimensional distributions are Dirichlet distributions.
Let  D .
1; 
2; : : :/ and Y D .Y1; Y2; : : :/ be two sequences of iid random
variables, independent of each other. Let 
i be distributed as beta distribution,
Be .1; ˛ .X//, and Yi distributed as ˛ ./ D ˛ ./ =˛ .X/ ; i D 1; 2; : : : . Let p1 D 
1
and pi D 
i
Qi1
jD1

1  
j

. Now deﬁne a probability function P as
P;Y .A/ D P .A/ D
1
X
iD1
piıYi .A/ ;
A 2 A:
Clearly by deﬁnition, P is random, discrete distribution with probability one, and
is a stochastic process fPA W A 2 Ag. P is a measurable map from .X; A/ into
.…;  .…//.
We need to show that the distribution of P is a Dirichlet measure, D˛ deﬁned on
.…;  .…//. Let  D



1 ; 

2 ; : : :

and Y D

Y
1 ; Y
2 ; : : :

be two sequences such
that 

n D 
nC1 and Y
n D YnC1 for n D 1; 2; : : : . It is easily seen that P satisﬁes the
equality
P;Y .A/ D 
1ıY1 .A/ C .1  
1/ P;Y .A/ ;
A 2 A:
Note that

; Y
has the same distribution as .; Y/ and is independent of .
1; Y1/.
Thus P satisﬁes the following distributional equation:
P
stD 
1ıY1 C .1  
1/ P:
(2.1.7)
From the theory of distributional equation it follows that there exists a distribution
P of P on .…;  .…// satisfying the above equation. His Lemma 3.3 [which is:
Suppose W, U be random variables with W 2 Œ1; 1 and U taking values in linear
space, V also a random variable in the same linear space as U and independent of
.W; U/, and satisfying distributional equation V D U C WV. Then, there is only
one distribution for V] shows that the distribution is unique. Thus the existence of
its distribution is ascertained.
Now the task is to prove that the distribution is the Dirichlet measure D˛.
A measure on .…;  .…// is Dirichlet if its ﬁnite dimensional distributions
are Dirichlet distributions. Therefore consider the partition .B1; : : : ; Bk/ of X
and the corresponding vector of random variables
_
PD .P.B1/; : : : ; P.Bk//. Let
_
DD .ıY1.B1/; : : : ; ıY1.Bk// and ej be a k-dimensional vector with 1 at the jth place
and zero elsewhere. Then it follows that P
_
.D D ej/ D P

Y1 2 Bj

D ˛

Bj

,
j D 1; : : : ; k. From (2.1.7) we see that
_
P satisﬁes the distributional equation
_
P
stD 
1
_
D C .1  
1/
_
P;

2.1
Dirichlet Process
27
where
_
D is independent of 
1 and the k-dimensional random vector
_
P is independent
of (
1;
_
D/. Now let us assume that the distribution of
_
P on the right-hand side is the
Dirichlet distribution D .˛.B1/; : : : ; ˛.Bk// and we know the distribution of
_
D is
Dirichlet D

ej

. Thus given
_
DD ej; the conditional distribution of 
1
_
D C .1  
1/
_
P
by a property of the Dirichlet distribution [which says: Let U and V be two
independent k-dimensional vectors having Dirichlet distributions D ./ and D

ı

,
respectively, where  D .1; : : : ; k/, ı D .ı1; : : : ; ık/ and independent of them,
let W  Be .; ı/, where  D P i and ı D P ıi. Then WU C .1  W/ V 
D

 C ı

] is D

.˛.B1/; : : : ; ˛.Bk// C ej

. Multiplying by the marginal and using
another property [which says: If ˇj D j=; j D 1; : : : ; k, then P ˇjD

 C ej

D
D ./ of the Dirichlet distribution we get D ..˛.B1/; : : : ; ˛.Bk// verifying that this
k-dimensional distribution satisﬁes the functional equation and that the solution is
unique by his Lemma 3.3, thus completing the proof.
The weights in the above representation are related to the Poisson process
(Ishwaran and Zarepour 2003) as follows. Noting that M D ˛ .X/,
P./
DD
1
X
jD1
.ej1=M  ej=M/ıj./;
(2.1.8)
with 0 D 0; ’s as deﬁned above. This can be seen by observing that pj D
ej1=M  ej=M D e"1=M    e"j1=M.1  e"j=M/
DD Qj1
lD1 .1  Vl/ Vj, since
e"1=M Ï Be.M; 1/ and 1  e"1=M Ï Be.1; M/. Ishwaran and Zarepour (2003) give
an interesting discussion of different representations of the weights.
A Limit Representation The DP can also be represented as a limit of ﬁnite
dimensional Dirichlet distribution (Neal 2000; Ishwaran and Zarepour 2003).
Let j’s be iid X-valued random variables with common distribution ˛ ./ and
independent of j’s, let p D . p1; : : : ; pN/ be distributed as symmetric Dirichlet
distribution with parameter .˛ .X/ =N; : : : ; ˛ .X/ =N/ ; i.e., the vector p is assigned
a symmetric Dirichlet prior. In view of a property of the Dirichlet distribution, pi
may also be deﬁned as pi D Yi= PN
jD1 Yj; where Yi are iid G .˛ .X/ =N; 1/. Deﬁne
DN./ D
N
X
jD1
pjıj./:
(2.1.9)
Ishwaran and Zarepour (2003) call DN a ﬁnite dimensional Dirichlet prior. The
authors formally prove that DN converges to the DP with parameter ˛. Thus it
can be used as an approximation to the DP. This construction is simple and a
nice feature here is that the weights are exchangeable. The symmetric Dirichlet
distribution has been previously used by Kingman (1975) in deﬁning the Poisson–
Dirichlet distribution (see Sect. 3.4.1), and Patil and Taillie (1977) in connection
with the size-biased permutation of a vector of probabilities, among others. The

28
2
Dirichlet and Related Processes
connection between Ferguson and Sethuraman weights and their relation to size-
biased permutation are further explored in Sects. 3.4 and 3.5.
Polya Sequence Representation Blackwell and MacQueen (1973) also provide
an alternative characterization of the DP. Let ˛ be a measure as before. Deﬁne a
sequence fXn W n  1g of random variables taking values in X as follows. For every
A 2 A, let
P .X1 2 A/ D ˛ .A/ =˛ .X/ and
P .XnC1 2 AjX1; : : : ; Xn/ D ˛n .A/
˛n .X/ D ˛ .A/ C Pn
iD1ıxi .A/
˛ .X/ C n
;
(2.1.10)
where ˛n D ˛ C Pn
iD1ıxi. A rule specifying the distribution of X1 along with
the conditional distribution of XnC1 given X1; : : : ; Xn for n D 1; 2; : : : is called
a prediction rule for the sequence X1; X2; : : : ; and ˛n .A/ =˛n .X/ as Polya urn
distribution. The sequence fXn W n  1g, called a Polya sequence with parameter
˛; may be viewed as the results of successive draws of balls from a Polya urn
containing ˛ .x/ balls of color x 2 X in which at every stage a ball is drawn at
random, its color is noted and is replaced by two balls of the same color. Given this
prediction rule, they prove the following remarkable result:
Theorem 2.4 (Blackwell and MacQueen) .a/The sequence ˛n ./ =˛n .X/ con-
verges with probability one as n ! 1 to a limiting discrete measure P; .b/ P is
the Dirichlet process with parameter ˛; and .c/ given P, X1; X2; : : : are independent
with distribution P.
The theorem shows that the sequence is exchangeable and that it is a sample
from the DP. Furthermore, it gives us a way to sample from the DP as indicated
later in the chapter. This result has received considerable interest in other areas that
will be described later in Sect. 3.5. Thus the DP can be characterized in terms of its
predictive rule. The predictive distribution in (2.1.10) can also be expressed as
P .XnC1 2 jX1; X2; : : : ; Xn/ D
n
X
iD1
1
˛ .X/ C nıxi ./ C
˛ .X/
˛ .X/ C n˛ ./ :
(2.1.11)
The interpretation of this expression is that XnC1 takes one of the previous n values
with probability 1= .˛ .X/ C n/ and with probability ˛ .X/ = .˛ .X/ C n/, a new
value is picked randomly according to ˛.
The sequence fXn W n  1g so constructed is exchangeable, by which we mean
that for any n  1, the joint distribution of the vector .X1; : : : ; Xn/ is the same as
that of any permutation of the same vector. Therefore, applying a de Finetti theorem
it can be shown (Ghosh and Ramamoorthi 2003) that the mixing distribution in
the theorem turns out to be the Dirichlet process. Basu and Tiwari (1982) have
studied Blackwell–MacQueen deﬁnition in detail. Their paper clears up some
measure theoretical details. Also see an interesting commentary by J. Sethuraman

2.1
Dirichlet Process
29
(“Commentary on a note on the Dirichlet process” in Selected works of Debabrata
Basu, Ed: Anirban DasGupta, Springer, 2011).
The above expression enables us to construct an exchangeable sequence and once
that is done, de Finetti theorem ensures the existence of an RPM providing the
mixing distribution. Generating an exchangeable sequence via a prediction rule is
discussed further in Sect. 3.5.
2.1.1.2
Samples from P
Ferguson deﬁnes what is meant by a sample from an RPM P; by providing the
following deﬁnition:
Deﬁnition 2.5 (Ferguson) Let P be an RPM on .X; A/. X1; : : : ; Xn is said to be
a sample from P, if for any positive integer m and measurable sets A1; : : : ; Am,
C1; : : : ; Cn of X,
PfX1 2 C1; : : : ; Xn 2 CnjP .A1/ ; : : : ; P .Am/ ; P .C1/ ; : : : ; P .Cn/g D
n
Y
jD1
P

Cj

a.s..
This deﬁnition implies that given P .C1/ ; : : : ; P .Cn/ ; the events fX1 2 C1g ; : : : ;
fXn 2 Cng are independent of the rest of the process and are mutually independent,
with PfXj 2 CjjP .C1/ ; : : : ; P .Cn/g D P

Cj

a.s. for j D 1; : : : ; n.
2.1.2
Properties
The Dirichlet process possesses certain interesting features. It is “rich” in the sense
that it is ﬂexible enough to incorporate any prior information or belief; it is closed
in the sense that if the prior is a DP, so is the posterior, given a random sample;
it has parameters which are easily interpretable; it is easy to evaluate expectation
of simple functions with respect to the DP; and it is almost surely discrete. Its
parameter ˛ when expressed in terms of M and F0 has interesting interpretations.
Since E .P .// D ˛ ./ =˛ .X/, one can choose the “shape” of ˛, F0 to reﬂect one’s
prior guess at the shape of the distribution. As the posterior distribution is a linear
combination of F0 and Fn; the sample distribution function, the magnitude of M
represents, in a sense, the degree of conﬁdence in prior guess and treated as if it
were a “prior sample” size. Independent of the shape, one can choose the magnitude
of M to reﬂect his strength of conviction.
On the other hand, its discreteness seems to be inconsistent with the desired
property of “richness” if one wants to place a prior on a class of continuous
distributions. However, Ferguson has proved that the DP is rich in the sense that
there is positive probability that a sample function will approximate as closely as
desired with any ﬁxed distribution which is absolutely continuous with respect to

30
2
Dirichlet and Related Processes
the ﬁnite measure ˛, the parameter of the DP. As will be seen in the next chapter, the
discreteness of the DP turns out to be an asset while carrying out Bayesian analysis
of mixture and hierarchical models, in clustering problems where it is assumed
that each observation may belong to only one distinct cluster, and in application
of species sampling models.
Ferguson proved several important properties of the Dirichlet process and gave a
few applications. Following this, various authors have proved additional properties.
We state these properties without proofs.
There is a closed relationship between the parameter ˛ and random probability
P as the following properties reveal:
1. If ˛ .A/ D 0; then P .A/ D 0 a.s.; if ˛ .A/ > 0, then P .A/ > 0 a.s., and
E .P .A// D ˛ .A/ =˛ .X/.
2. If ˛ is -additive, then so is P, i.e., for a ﬁxed decreasing sequence of
measurable sets An & ¿, then P .An/ ! 0 a.s. as n ! 1. The converse is
also true: If ˛ is not -additive, then neither is P, a.s.
3. If Q is ﬁxed probability measure on .X; A/ such that Q  ˛, then, for any
positive integer m and measurable sets A1; : : : ; Am, and  > 0,
P fjP .Ai/  Q .Ai/j <  for i D 1; : : : ; mg > 0:
It may seem from (1) that ˛ and P are mutually absolutely continuous. But
it is false since the null set outside of which the conclusion of (1) holds may
depend upon the set A. The third property shows that the support of the Dirichlet
process with parameter ˛ contains the set of all probability measures absolutely
continuous with respect to ˛, and that any measure Q not absolutely continuous
with respect to ˛ is not in the support of P.
Two probability measures P1 and P2 are said to be mutually singular, in
symbol P1?P2, if there exists an event A such that P1.A/ D 1 and P2.A/ D 0.
Similarly, by saying that two Dirichlet processes D.˛1/ and D.˛2/ are mutually
singular it is meant that given one sample process P from either D.˛1/ or D.˛2/,
it is possible to identify with probability 1 to which distribution it belongs.
4. Let ˛1 and ˛2 be two nonatomic, non-null ﬁnite measures on .X; A/. If ˛1 ¤ ˛2,
then D.˛1/ ? D.˛2/.
5. Probabilities assigned to two disjoint sets are negatively correlated.
Ferguson demonstrated that various expectations of real valued functions Z’s
deﬁned on .X; A/ can easily be derived.
6. If
R
jZj d˛ < 1, then
R
jZj dP < 1 with probability one and E
R
ZdP D
R
ZdE.P/ D
R
Zd˛.
7. Let Z1 and Z2 be two measurable real valued functions deﬁned on
.X; A/. If
R
jZ1j d˛
<
1,
R
jZ2j d˛
<
1 and
R
jZ1Z2j d˛
<
1,
then E
R
Z1dP
R
Z2dP
D
12= .M C 1/ C 	1	2, where 	i
D
R
Zid˛,
i D 1; 2 and 12 D R Z1Z2d˛  	1	2. If further R
jZ1j2 d˛ < 1 and
R
jZ2j2 d˛ < 1, then Cov.R Z1dP; R Z2dP/ D 12= .M C 1/, and from this we
get Var.R Z1dP/ D 2
0 = .M C 1/, where 2
0 D R Z2
1d˛  	2
1.

2.1
Dirichlet Process
31
8. Let 	 D R ZdP and 	0 D E.	/ D R Zd˛. If R Z4 d˛ < 1, then E .	  	0/3 D
2	3=Œ.M C 1/.M C 2/.
Also
E .	  	0/4 D
6	4 C 3M4
.M C 1/.M C 2/.M C 3/;
where 2 D
R
.Z  	0/2d˛, 	3 D
R
.Z  	0/3d˛, and 	4 D
R
.Z  	0/4d˛.
Some of these properties have been generalized in different directions. Let
g.x1; : : : ; xk/ be a measurable real valued function deﬁned on the k-fold product
space

Xk; Ak
of .X; A/ and symmetric in x1; : : : ; xk. Assume that
Z
Xm j g.x1; : : : ; x1; x2; : : : ; x2; : : : :; xm; : : : ; xm/ j d˛.x1/    d˛.xm/ < 1;
(2.1.12)
for all possible combinations of arguments .x1; : : : ; x1; x2; : : : ;
x2; : : : :;
xm; : : : ; xm/ from all of xi’s distinct .m D k/ to all identical .m D 1/. Note
that the function g vanishes whenever any two coordinates are equal, and
condition (2.1.12) reduces to the simple condition
Z
Xk j g.x1; : : : ; xk/ j d˛.x1/    d˛.xk/ < 1 :
(2.1.13)
An important property that has been used widely in solving some nonpara-
metric Bayesian estimation problems is derived in Yamato (1977a,b, 1984) and
Tiwari (1981, 1988).
9. Under the assumption (2.1.13),
Z
Xk j g.x1; : : : ; xk/ j dP.x1/dP.xk/ < 1 with probability one,
(2.1.14)
and
E
Z
Xk g.x1; : : : ; xk/ dP.x1/    dP.xk/ D
X
C.P imiDk/
kŠŒ˛ .X/†mi
…k
iD1Œimi .mi/Š˛ .X/.k/

Z
X†mi ‰.x/…k
iD1…mi
jD1d˛.xij/;
(2.1.15)
where ‰.x/
D
g.x11; : : : ; x1m1; x21; : : : ; x2m2; : : : ; xkmk; : : : ; xkmk/; s.n/
D
s .s C 1/    .s C n  1/, and the summation P
C.P imiDk/ extends over all
nonnegative integers m1; : : : ; mk such that P imi D k. Taking g to be the
indicator function, one can derive the marginal distribution of a sample
.x1; : : : ; xk/ from P. Yamato (1977a,b) gives examples of various estimable

32
2
Dirichlet and Related Processes
functions as special cases of g. For example, letting g.x1; : : : ; xk/ D x1x2 : : : xk,
he derived the kth moment of the Dirichlet process D.˛/.
Again, using the alternative deﬁnition of the Dirichlet process given by
Ferguson mentioned before, namely P.A/ D P1
jD1 Pjıj.A/, A 2 A, Yamato
(1977a,b, 1984) derived the joint moments of weights.
10. For any positive integers m; and any combination of positive integers r1; : : : ; rm,
such that Pm
iD1 ri D k and M D ˛ .X/,
E
X X
: : :
X
Pr1
j1    Prm
jm

j1¤j2¤:::¤jm
D .r1  1/Š    .rm  1/Š  Mm
M.k/
:
(2.1.16)
As special cases we have
E
0
@
1
X
jD1
P2
j
1
A D 1=.M C 1/; E
0
@X
i¤j
PiPj
1
A D M=.M C 1/;
E
0
@
1
X
jD1
P3
j
1
AD2=Œ.MC1/.MC2/; E
0
@X
i¤j
P2
i Pj
1
A DM=Œ.MC1/.MC2/;
E
0
@ X
i¤j¤k
PiPjPk
1
A D M2=Œ.MC1/.MC2/; E
0
@
1
X
jD1
P2
j
1
A
2
D M.M C 6/=M.4/;
E
0
@
1
X
jD1
P2
j
1
A
3
D M.M2 C 18M C 120/=M.6/; etc.:
11. It is easier to compute the moments of pj’s using the Sethuraman (1994)
representation. Let mi’s be as before. Then, for any combination .m1; : : : ; mn/
of f1; 2; : : : ; ng Tiwari (1981, 1988) proved
E .†p11p1m1p2
21p2
2m2pn
n1pn
nmn/ D …n
iD1Œ.i  1/Š˛ .X/
mi
˛ .X/.n/
;
(2.1.17)
where the summation † is over each pij . j D 1; : : : ; mi, i D 1; : : : ; n/ taking
all mutually distinct values of p1;p2; : : :.
Yamato (1977a,b) also derived similar results using Ferguson’s alternative
deﬁnition. They both use Antoniak’s (1974) result related to the previous
property. For n D 2, the above result was established in Ferguson’s (1973)
paper.
Sethuraman and Tiwari (1982) have investigated the limits of prior dis-
tributions as the parameter ˛ tends to various values. Using the representa-
tion (2.1.5), they have proved the following results.

2.1
Dirichlet Process
33
12. Let f˛rg be a sequence of non-null -additive ﬁnite measures on .X; A/ such
that
˛r .X/ ! 0 and Sup
A
j ˛r .A/  ˛0 .A/ j ! 0 as r ! 1 ; A 2 A;
where ˛r is a probability measure in …. Then D .˛r/
w! ıY0 and D.˛r C
Pn
iD1 ıxi/
w! D
Pn
iD1 ıxi

as r ! 1, where Y0 has the distribution ˛0. This
means that if the total mass ˛ .X/ converges to zero, the Dirichlet process
reduces to a degenerate mass at point Y0 selected according to ˛0 distribution.
Various additional convergence properties of the Dirichlet process may be
found in Ghosh and Ramamoorthi (2003) See also Lo (1983). The distribution
of a linear functional of the Dirichlet process is studied in James (2006).
13. The distribution of 	.P/; the mean of the process, can also be obtained from
Hannum et al. (1981) [see Cifarelli and Regazzini (1979)] who have shown that
Pf
R
g.t/dP.t/  xg D PfTx  0g, where 1 < x < 1 and Tx is a random
variable with characteristic function expf
R
R logŒ1itfg.t/xgd˛.t/g. Using
this result they have shown that when g is odd and ˛ is symmetric about 0,
then the distribution of
R
g.t/dP.t/ is symmetric about 0I and that if P and
Pn are random probability measures on .R; B/ with priors D.˛/ and D.˛n/,
n D 1; 2 : : : , and if ˛n
w! ˛ as n ! 1, then under some mild regularity
conditions R gdPn converges in distribution to R gdP.
14. Let X be a sample of size one from P. Then P .X 2 A/ D ˛ .A/ =˛ .X/ for
A 2 A.
15. Let X Ï P. Then ˛ .X/ D EŒVar.XjP/=VarŒE.XjP/. This property provides
an interpretation of Baye’s rule for 	 D R xdP as a Gauss–Markov estimator
of .1=n/ ˛ .X/ representing the relative precision of the prior mean E.	/ to
the sample mean X, and provides an additional support for the interpretation
of ˛ .X/ as the prior sample size. However this interpretation is questioned by
Walker and Damien (1998).
16. Let P Ï D.˛/ and given P D P, let X  P. Then for any A 2 A; the conditional
distribution of P given P.A/ and X 2 A is same as the conditional distribution
of P given P.A/ (Antoniak 1974). That is knowing X 2 A does not add anything
more to the process.
Multiplicities Since the DP is a discrete distribution, we expect samples from it
to have duplications with probability one. This has some interesting consequences
as ﬁrst pointed out in Antoniak (1974). Let X1; : : : ; Xn be a random sample drawn
from P; and P Ï D.˛/. The marginal distribution of Xi is ˛ ./ =˛ .X/. Normally
we would expect them to be independent and therefore P
˚
Xj D Xi

for any i and
j is likely to be zero. However, this is not the case when P is a Dirichlet process.
Assume ˛ to be nonatomic and consider the case of n D 2. The conditional
distribution of P given X1 is D.˛ C ıX1/ (see Theorem 2.6), which shows that ˛ is
no longer nonatomic but has an atom of point mass 1 at X1. Hence the probability of
X2 D X1 given X1 is in fact 1= .˛ .X/ C 1/ ; independent of X1, and the probability

34
2
Dirichlet and Related Processes
of X2 ¤ X1 is ˛ .X/ = .˛ .X/ C 1/. Proceeding this way, one can argue, based on
Blackwell–MacQueen characterization of the DP, that at the nth stage, probability of
drawing a new distinct value is ˛ .X/ = .˛ .X/ C n  1/. This value is monotonically
decreasing as n increases indicating that the probability of observing a new distinct
value diminishes. Nevertheless it is noted that k D k.n/; the number of distinct
values of observations among a sample of size n increases. Speciﬁcally,
17.
E .k.n// D ˛ .X/
n
X
mD1
1= .˛ .X/ C m  1/ 	 ˛ .X/ Œlog .1 C n=˛ .X// :
(2.1.18)
Hence E .k.n// ! 1 and n ! 1. In fact Korwar and Hollander (1973) have
shown that k.n/
a:s:
! 1 and n ! 1.
Thus, although new distinct values are increasingly rare, there are abundant
of them. Since they are distributed as ˛ ./ =˛ .X/, they provide information to
estimate ˛ ./. On the other hand, the rate at which new distinct values appear
depends on ˛ .X/. It provides information on the magnitude of ˛ .X/ and can be
used to estimate it if unknown (see next property). Also, for a given sample size
n; we would expect many more duplications when ˛ .X/ is very small rather
than when it is very large. Let the distinct values be denoted by X
1 ; : : : ; X
k ;
with their multiplicities n1; : : : ; nk, respectively, so that n1 C    C nk D n.
18. Antoniak (1974) has derived the distribution of k.n/ as
P fk.n/ D kg D an
k˛ .X/k =An .˛ .X// ;
and given k, the joint distribution of multiplicities as
p .n/ D pn;K.n1; : : : ; nkjk/ D ˛ .X/k
˛ .X/.n/
kY
iD1
.ni  1/Š:
(2.1.19)
Here An .z/ D z.n/ D z .z C 1/    .z C n  1/ is the nth degree polynomial in
z and an
k is the integer valued coefﬁcient of the kth term. These are the absolute
values of Sterling numbers of the ﬁrst kind. p .n/ can be thought of as deﬁning
a prior distribution on the vectors n D .n1; : : : ; nk/. See the section on species
sampling models for further discussion on it.
19. Assuming ˛ to be nonatomic and unknown, Korwar and Hollander (1973)
established two interesting results:
(i) k.n/= log n ! ˛ .X/ a.s. as n ! 1; and
(ii) X
1 ; : : : ; X
k.n/ are independent and identically distributed as ˛ ./ =˛ .X/.
Thus ˛ .X/ can be estimated consistently by the quantity k.n/= log n and
the second result leads to a strong law of large numbers for the mean

2.1
Dirichlet Process
35
Pk.n/
iD1 X
i =k.n/. This supports Ferguson’s remark that ˛ .X/ may be interpreted
as the sample size.
Duplication of observations has given rise to two important combinatorial
formulas. Let mi stand for the number of observations in the sample X D
.X1; : : : ; Xn/ which repeat exactly i times, i D 1; 2; : : : ; n. Then Pn
iD1 imi D n
and Pn
iD1 mi D k; the number of distinct observations. Let .X1; : : : ; Xn/ 2
C .m1; : : : ; mn/ be the event that in the sample exactly m1 observations occur
only once and they are the ﬁrst m1 X0s, exactly m2 observations occur in pairs
and they are the next 2m2 X0s; : : :, exactly mn observations occur each n times
and they are the last nmn X0s. For example, m1 D n and mi D 0 for i > 1
means all observations are distinct. If m1 D    D mn1 D 0; mn D 1 means all
observations are identical. Then Antoniak (1974) proved the following result.
20. Let ˛ be nonatomic. Then
P ..X1; : : : ; Xn/ 2 C .m1; : : : ; mn// D
nŠ
…n
iD1imi .miŠ/
˛ .X/
Pn
iD1 mi
˛ .X/.n/
:
(2.1.20)
This formula is discovered independently by Ewens (1972).
21. The marginal (predictive) distribution of XnC1jX1; : : : ; Xn; ˛ is a rescaled
version of the updated ˛ measure, namely ˛
n D .˛ C 1/=.˛ .X/ C n/. It can
be written as a mixture with XnC1 Ï ˛ ./ =˛ .X/ with probability M= .M C n/
and XnC1 set equal to Xi, i D 1; : : : ; n, each with probability 1= .M C n/ and
M D ˛ .X/. Since X1; : : : ; XnjP are iid, they are exchangeable. Therefore one
can rewrite the conditional distribution of XijX1; : : : ; Xi1; XiC1; : : : ; Xn; ˛ as
the mixture with Xi Ï ˛ with probability M= .M C n  1/ and Xi D Xj; j D
1; : : : ; i  1; i C 1; : : : ; n, each with probability 1= .M C n  1/. Because of
the discreteness of the Dirichlet process, we expect some observations to be
repeated. The predictive distribution of a future observation may thus be written
as
XnC1jX1; X2; : : : ; Xn Ï
n
M C n  1
n
k
X
jD1
njıX
j C
M
M C n˛;
(2.1.21)
where as before X
j are k  n distinct observations among n with frequency nj,
respectively. This implies that XnC1 will be a new observation with probability
M= .M C n/ and will coincide with an existing observation with probability
n= .M C n/, both of which do not depend upon k or on frequency nj, important
information is unutilized. This is remedied by the generalization of the Dirichlet
process, namely the two-parameter Poisson–Dirichlet distribution (Pitman and
Yor 1997. See Sect. 3.4.2).
22. The Dirichlet process induces an exchangeable distribution over partitions of N
objects into K classes.

36
2
Dirichlet and Related Processes
2.1.2.1
Characterization
Let C denote the class of all RPMs P such that either (1) P is degenerate at a
given probability distribution P0; (2) P concentrates at a random point; or (3) P
concentrates at two random points. Then Doksum (1974) proved the following
characterizations:
1. If P … C is tailfree (see Chap. 5) with respect to all sequences of nested,
measurable partitions, then P is a Dirichlet process.
2. If P … C is neutral to the right (see Chap. 4) with respect to all sequences of
nested, measurable, ordered partitions, then P is a Dirichlet process.
3. The Dirichlet process is the only process not in C such that for each A 2 A,
the posterior distribution of P.A/ given a sample X1; : : : ; Xn from P, depends
only on the number of X’s that fall in A and not where they fall. That is, if the
posterior distribution of the random probability P given a sample X1; : : : ; Xn from
P depends on the sample only through the empirical distribution function, then P
is a Dirichlet random probability.
4. Lo (1991) gives a different characterization of the Dirichlet process. If the
posterior mean of the random probability P; given a sample X1; : : : ; Xn from
P; is linear in the empirical distribution function, then P is a Dirichlet random
probability. That is, P is a Dirichlet random probability on .X; A/ if and only if
for each n D 1; 2; : : : ; the posterior mean of P given a sample X1; : : : ; Xn is given
by .1  an/Pn C an .1=n/ Pn
iD1ıxi for some an 2 .0; 1/ and some probability Pn
on .X; A/.
This characterization can also be expressed in terms of the predictive probability
based on a sequence of exchangeable random variables X1; : : : ; Xn:
P .XnC1 2 AjX1; : : : ; Xn/ D ˛ .A/ C Pn
iD1ıxi .A/
˛ .X/ C n
D pn
˛ .A/
˛ .X/ C .1  pn/
Pn
iD1ıxi .A/
n
;
(2.1.22)
which is a linear combination of the prior measure ˛ and the empirical distribution
with pn D ˛ .X/ = .˛ .X/ C n/.
2.1.3
Posterior Distribution
All of the above properties are derived for functions of random probability P having
a Dirichlet process prior. Given P, if we have a random sample from P, the following
important property was proved in Ferguson (1973). This property is fundamental
in solving nonparametric Bayesian problems—the motivator for developing the
Dirichlet process in the ﬁrst place. It forms the basis of various applications reported
in Chaps. 6 and 7.

2.1
Dirichlet Process
37
The Dirichlet process is conjugate with respect to exact (uncensored) observa-
tions.
Theorem 2.6 (Ferguson) Let P Ï D.˛/ and given P D P, let X be a random
sample of size one from P, then the marginal distribution of X is ˛ D ˛=˛ .X/, the
normalized measure corresponding to ˛. Also, the posterior distribution of P given
X D x is D.˛ C ıx/, the Dirichlet process prior with updated parameter ˛ C ıx.
If X1; : : : ; Xn is sample of size n from P, then the posterior distribution of P given
X1; : : : ; Xn is D.˛ C Pn
iD1 ıxi/.
Remark 2.7 However, the posterior distribution with respect to right censored
observations is no longer a Dirichlet process but is a mixture of Dirichlet processes,
a beta-Stacy or a neutral to the right process.
The proof of this theorem for the case n D 1; which can be extended to arbitrary
n, depends upon his two propositions. .1/ Let X be sample of size one from P; then
the marginal distribution of X 2 A, A 2 A is ˛ .A/ =˛ .X/; and .2/ for any partition
.B1; : : : ; Bk/ of X and A 2 A, the joint distribution of X 2 A and P .B1/ ; : : : ; P .Bk/
is given by
P fX 2 A; P .B1/  y1; : : : ; P .Bk/  ykg
D
k
X
jD1
˛

Bj \ A

˛ .X/
D

y1; : : : ; ykj˛. j/
1 ; : : : ; ˛. j/
k

;
where D

y1; : : : ; ykj˛. j/
1 ; : : : ; ˛. j/
k

is the distribution function of the Dirichlet
distribution with parameters

˛. j/
1 ; : : : ; ˛. j/
k

with ˛. j/
i
D ˛ .Bi/ if i ¤ j and
˛. j/
i
D ˛ .Bi/ C 1 if i D j. Now to prove that the posterior distribution of
.P .B1/ ; : : : ; P .Bk// given X is the Dirichlet distribution
D .y1; : : : ; ykj˛ .B1/ C ıX .B1/ ; : : : ; ˛ .Bk/ C ıX .Bk// ;
(2.1.23)
it is sufﬁcient to show that this conditional distribution when integrated with respect
to the marginal distribution of X over the set A yields the above distribution. This
follows immediately as
Z
A
D .y1; : : : ; ykj˛ .B1/ C ıX .B1/ ; : : : ; ˛ .Bk/ C ıX .Bk// d˛ .A/ =˛ .X/
D
k
X
jD1
Z
Bj\A
D

y1; : : : ; ykj˛. j/
1 ; : : : ; ˛. j/
k

d˛ .A/ =˛ .X/
D
k
X
jD1
˛

Bj \ A

˛ .X/
D

y1; : : : ; ykj˛. j/
1 ; : : : ; ˛. j/
k

;
(2.1.24)
completing the proof.

38
2
Dirichlet and Related Processes
The implication of this theorem is that in obtaining the posterior distribution,
all one has to do is to update the parameter ˛. It was noted in the characterization
above that the DP is the only process not in C where the posterior distribution of
P.A/ depends upon the number of observations that fall in A and not where they
fall. This makes the DP easy to use, but otherwise not desirable. Lijoi and Prünster
(2010) distinguish between the two types of conjugacy: parametric and structural.
For the parametric, the distribution of the posterior is same as of prior except that
the parameters get updated. The Dirichlet process is an example of this. Whereas
for the second type, the posterior distribution has the same structure as the prior in
the sense that they both belong to the same general class of distributions. Neutral to
the right process is an example of this. The ﬁrst implies the second, but not the other
way around.
An extension of this theorem is proved in Antoniak (1974).
Theorem 2.8 (Antoniak) Let P Ï D.˛/ and 
 be a sample of size one from P.
If X is a random variable such that the conditional distribution of X given P and

 is F .
; /, then the conditional distribution of P given X D x is a mixture of
Dirichlet processes with mixing distribution Hx, which is the conditional distribution
of 
 given X D x; and transition measure ˛u.
; / D ˛ ./ C ı
 ./. In symbols, if
P Ï D.˛/, 
jP Ï P, and XjP; 
 Ï F.
; /; then PjX Ï
R
‚ D.˛ C ı
/dHx.
/.
In applications this model is generally referred to as a mixture of DPs model.
Usually, it is difﬁcult to ﬁnd the posterior distribution Hx.
/. Therefore Gibbs
sampling procedures are developed by which one can sample a posterior distribution
in order to carry out Bayesian analysis. This is described later in this chapter.
2.1.3.1
Sampling a Dirichlet Process
The Dirichlet process is a stochastic process index by the sets of A. Therefore,
to generate a sample from the Dirichlet process with parameter ˛, we need to
assign probability P .A/ for each set A 2 A. In using the Sethuraman representation
P./ D P1
jD1 pjıj./ (2.1.5), we need two things: the realization of weights pj’s
such that pj 2 Œ0; 1 and P1
jD1 pj D 1I and independent of them, locations j’s such
that j
iidÏ ˛ ./ =˛ .X/. There are two approaches to generate the sample. One is
based on the stick-breaking construction described earlier. The other is the extended
Polya urn scheme (Blackwell and MacQueen 1973) which is known outside the
statistical community as the Chinese restaurant process (CRP) presented formally
in Sect. 4.8.1. It involves the process of generating an exchangeable sequence of
random variables. The CRP is obtained by integrating out the P and thus it describes
the marginal distributions in terms of random partitions determined by k tables in a
restaurant. In both the cases, the sample is generated by generating the sequences
˚
pj

and
˚
j

and then using P./ D P1
jD1 pjıj./ to produce a realization of P.
In the ﬁrst approach, we need to generate a sequence
˚
pj; j
1
jD1 taking values
in .Œ0; 1  X/1. Since pj’s are not independent, it is difﬁcult to generate such

2.1
Dirichlet Process
39
a sequence directly. Instead an alternative sequence
˚
Vj; j
1
jD1taking values in
.Œ0; 1  X/1 is generated, where each Vj
iidÏ Be .1; ˛ .X// and j
iidÏ ˛ ./ =˛ .X/.
Then we set p1 D V1 and pj D Vj
Qj1
lD1 .1  Vl/ for j  2. Thus a realization of the
random probability P will be a map from .Œ0; 1  X/1 to ….
For the second approach recall the Polya urn scheme of Blackwell and Mac-
Queen (1973) described earlier in this section. Suppose we have an inﬁnite number
of balls of different colors, denoted by c1; c2; : : : The colors are distributed according
to ˛ D ˛ ./ =˛ .X/. At the ﬁrst step, a ball X1 is drawn at random from this set
according to the distribution ˛; and its color is noted. The ball is replaced along
with an additional ball of the same color. At the .n C 1/th step, either a ball which
is of one of the observed colors is picked with probability n=.˛ .X/ C n/ or a
ball of new color is picked with probability ˛ .X/ =.˛ .X/ C n/. In both cases,
the ball is replaced along with another ball of the same color, and the step is
repeated. Thus a sequence X1; X2; : : :, of random variables is generated where Xi
is a random color from the set of colors fc1; c2; : : :g. Note that we do not need
to know completely the set fckg ahead of the time. We have X1 Ï ˛=˛ .X/ and
XnC1jX1; X2; : : : ; Xn Ï .˛ C Pn
iD1 ıxi/=.˛ .X/ C n/, which as noted earlier, can be
written equivalently as
XnC1jX1; X2; : : : ; Xn Ï
n
X
iD1
1
˛ .X/ C nıxi C
˛ .X/
˛ .X/ C n˛:
(2.1.25)
Some colors, say k  n will be repeated in n draws. Denote the distinct colors
among them by X
1 ; : : : ; X
k and let nj be the number of times the color X
j is
repeated, j D 1; 2; : : : ; k; n1 C    C nk D n. Then the above expression can be
written in terms of k distinct colors, as
XnC1jX1; X2; : : : ; Xn; k Ï
k
X
jD1
nj
˛ .X/ C nıX
j C
˛ .X/
˛ .X/ C n˛:
(2.1.26)
This process is continued indeﬁnitely. It has been interpreted in practical terms
and popularized in culinary metaphor by the catchy name, CRP (attributed to Jim
Pitman and Lester Dubins by Grifﬁths and Ghahramani 2006). The color of .nC1/th
draw being the jth color can be interpreted as seating the .n C 1/th patron at jth
table in a restaurant. It forms the basis of many algorithms to generate posterior
distributions when closed form is unobtainable.
The sequence X1; X2; : : : ; of draws of balls represents incoming patrons at a
Chinese restaurant, distinct colors of balls represent tables with different dishes (one
dish per table), each of unlimited sitting capacity (that is there are inﬁnite many balls
of each color). Each customer sits at a table. The ﬁrst customer sits at a table and
orders randomly a dish for the table according to the distribution ˛ ./ =˛ .X/. The
(nC1/th customer chooses to join previous customers with probability n=.˛ .X/Cn/
or chooses a new table with probability ˛ .X/ =.˛ .X/ C n/ and orders a dish. If he

40
2
Dirichlet and Related Processes
joins previous customers and there are already k tables occupied, then he joins the
jth table (or orders dish X
j / with probability nj=.˛ .X/Cn/; where nj is the number
of customers already occupying the table (or enjoying the dish X
j /; j D 1; 2; : : : ; k
i.e., XnC1 D X
j . If he chooses a new table, he orders a random dish distributed
according to ˛=˛ .X/. This results in the above two expressions.
Patrons are exchangeable as are the random variables X0
i s in the Polya urn
sequence. The probability of a particular sitting arrangement depends only on nj
which is a function of n; and not on the order in which the customers arrive. Thus a
realization of pj is obtained by
pj D lim
n!1
nj .n/
˛ .X/ C n
(2.1.27)
and j
iidÏ ˛ ./ =˛ .X/.
The distinction between the two methods is that in the stick-breaking method
the weights generated are exact, whereas in Polya sequence process they are
approximate. However, in both the methods to sample a P, we need to continue
the process for an inﬁnitely long period which is impossible. Therefore termination
at some suitable stage is employed. One approximate method is to generate a sample
from a ﬁnite dimensional symmetric Dirichlet distribution with parameter ˛ .X/ =N
and use them as weights. That is, let .q1; : : : ; qN/ Ï D .˛ .X/ =N; : : : ; ˛ .X/ =N/
and then deﬁne PN./ D PN
jD1 qjıj./ with j
iidÏ ˛ ./ =˛ .X/. It can be shown that
PN ! P in distribution, as N ! 1.
The above procedures are for sampling from a DP. For the Bayesian analysis to
move forward, we need to sample from the posterior of the DP. If the posterior is
also a DP, as is the case with iid observations, we can adapt the above procedures
by simply updating the parameter ˛. However in many applications, as will be seen
later on in other sections, the posterior under different sampling models turns out
to be complicated and the above procedures are no longer workable. In such cases,
simulation methods are used to generate a sample from the posterior. This topic on
computation will be presented sparingly later on and references will be given for
further exploration.
2.1.4
Extensions and Applications of Dirichlet Process
The remarkable feature of the Dirichlet process, as the chart in Fig. 1 shows, is that it
has led to the development of many extensions/generalizations and/or the Dirichlet
process being a particular case of such processes. It may rightly be considered as a
“base” giving rise to other prior processes. Its relation to various processes discussed
in later sections is as follows.
The Dirichlet process is obviously a particular case of the Dirichlet Invariant and
mixtures of Dirichlet processes introduced in Sects. 2.2 and 2.3, respectively. When

2.1
Dirichlet Process
41
deﬁned on the real line, the DP is also a neutral to the right process discussed in
Sect. 4.2. A certain transformation of the beta process of Sect. 4.5 yields the DP and
the DP is also a particular case of the beta-Stacy process presented in Sect. 4.7.
It is a tailfree process of Sect. 5.1 with respect to every tree of partitions, and
when the parameters of the Polya tree process of Sect. 5.2 are subjected to certain
constraints, it yields the DP. When the discount parameter of the two-parameter
Poisson–Dirichlet process of Sect. 3.4.2 is set to zero, the process reduces to the DP.
The Sethuraman countable mixture representation of the DP was originally used
mainly for proving various properties of the DP. However in recent years, as pointed
out in Sect. 1.3 and further detailed in Chap. 3, its use as an instrument in developing
several related processes has exploded. This representation has four ingredients and
by tempering them, a number of new extensions of the DP have been proposed for
carrying out Bayesian analysis and modeling large and complex data sets.
If the inﬁnite sum P1
iD1 piıi is truncated at a ﬁxed or random N < 1; it
generates a class of ﬁnite discrete distribution priors (Ongaro and Cattaneo 2004). If
the weights deﬁned by a one-parameter beta distribution Be .1; ˛ .X// are replaced
by those deﬁned by a two-parameter beta distribution Be .ai; bi/, a second group
of priors emerged (Ishwaran and James 2001; Pitman and Yor 1997). A third group
of priors are developed to accommodate covariates, by indexing i with a covariate
x D .x1; : : : ; xk/, denoted as ix. This approach is generalized further in different
directions and the resulting priors include processes such as the dependent Dirichlet
(MacEachern 1999), spatial Dirichlet (Gelfand et al. 2005), generalized spatial
Dirichlet (Duan et al. 2007), multivariate spatial Dirichlet (Reich and Fuentes
2007), order-based Dirichlet (Grifﬁn and Steel 2006), and Latent stick-breaking
processes (Rodriguez et al. 2010), to name a few. For the fourth group, a different
type of extension is proposed in which the degenerate probability measure ı is
replaced by a nondegenerate positive probability measure G; called the kernel stick-
breaking Dirichlet process (Dunson and Park 2008). The Sethuraman representation
as well as the predictive distribution based on a generalized Polya urn scheme
proposed by Blackwell and MacQueen (1973) have also been found useful in the
development of new processes, such as the popularly known Chinese restaurant and
Indian buffet processes discussed in Sects. 4.8.1 and 4.8.2, respectively. They have
applications in nontraditional ﬁelds such as word documentation, machine learning,
and mixture models.
A signiﬁcant drawback in application of the above processes is that there are
no convenient expressions for the posterior distributions available and hence the
Bayesian analyses have to rely on simulation methods. For this purpose, however,
fast running algorithms are available in the literature. The volume of literature on
these processes is exhaustive and it is impossible to do a reasonable justice to them
in this book. However, they are brieﬂy introduced in Sect. 2.4.1. References are
given for further exploration if interested.
All these processes belong to a family we call Ferguson–Sethuraman processes,
discussed in the next chapter, to emphasize the importance of the countable mixture
representation of the DP.

42
2
Dirichlet and Related Processes
The above processes may be considered as special cases of a class of models
proposed by Pitman (1996a,b) and called species sampling models,
P./ D
1
X
jD1
pjıj./ C
0
@1 
1
X
jD1
pj
1
A Q ./ ;
(2.1.28)
where Q is the probability measure corresponding to a continuous distribution G;
j
iidÏ G; and weights pj’s are constrained by the condition, P1
jD1 pj  1. However
their use in mainstream statistics is very limited if not absent.
2.1.4.1
Applications of the Dirichlet Process
In view of its conjugacy property, the DP was convenient to use as prior in
nonparametric Bayesian estimation of various parameters and functionals of an
unknown distribution function F. This path was pursued in 1970s and 1980s and
the beauty is that most of these results were obtained in closed form. There is
a vast literature on these topics and we provide a summary of these results in
Chaps. 6 and 7. Nevertheless, extensive references are included for the reader
to pursue further if interested. Among the applications, Bayesian estimation of
a CDF, symmetric CDF, empirical Bayes estimation, sequential estimation, and
minimax estimation of a distribution function are derived in closed form. For
the functionals of a CDF, estimation of the mean, median, variance, qth quantile,
and location parameters have been given. Other applications, such as a regression
problem, estimation of a density function, estimation of covariance and concordance
coefﬁcient, a bioassay problem, and a hypothesis testing problem, are discussed
and end results are given in closed form in Chap. 6. Many of these applications are
also derived for the right censored data in Chap. 7. For example, a nonparametric
Bayesian analog of the popular Kaplan–Meier Product Limit (PL) estimator is
obtained and shown that when the total mass of the prior goes to zero, it reduces
to the PL estimator. Results obtained for other type of sampling, such as modiﬁed
censoring scheme, progressive censoring, and left truncation, are also included in
this chapter.
The current applications of the Dirichlet process are focused on Bayesian
analysis of large and complex data. It is found to be very effective in making
inferences in connection with hierarchical and group data modeling, dynamic
mixtures, spatial modeling, and clustering of observations. In view of the advances
in computational power, close form of results is no longer necessary. This has made
deeper and more sophisticated treatment of data within the reach of data analysts.
This will be evident in Chap. 3.

2.2
Dirichlet Invariant Process
43
2.2
Dirichlet Invariant Process
The support of the Dirichlet process is sufﬁciently broad to accommodate any
prior belief. However, in treating some nonparametric Bayesian inference problems,
Dalal (1979a) realized that the prior information is more structured. For example,
without knowing the speciﬁc form of the underlying distribution, it may be of
interest to estimate the median of a symmetric distribution or test for independence
under a permutation symmetry. In such situations, it is evident that a subset of the
space of all probability measures or distribution functions would be more efﬁcient
to use. Thus there is a justiﬁcation to consider priors deﬁned on a subset of all
distribution functions possessing certain inherent characteristics such as symmetry
about a point, exchangeability, or invariance under a ﬁnite group of transformations.
Dalal (1979a) initiated a study of such priors.
Deﬁnition The Dirichlet Invariant process (DIP) is deﬁned on the same line as the
DP. Let G D fg1; : : : ; gkg be any ﬁnite group of measurable transformations from
a p-dimensional Euclidean space X ! X . A set B 2 X is called G-invariant if
B D gB for all g 2 G; and a ﬁnite non-null measure  is said to be G-invariant if
 .A/ D  .gA/ for all g 2 G and all A 2 X. A measurable partition (A1; : : : ; Am/ of
X is said to be G-invariant if Aj D gAj for all g 2 G and j D 1; : : : ; m.
Deﬁnition 2.9 (Dalal) A G-invariant RPM P is said to be a Dirichlet G-invariant
process if there exists a G-invariant measure ˛ on .X; .X// such that for
every G-invariant measurable partition .A1; : : : ; Am/ of X, the joint distribution of
(P .A1/ ; : : : ; P .Am/) is D.˛ .A1/ ; : : : ; ˛ .Am//. Symbolically P Ï DGI.˛/.
Using the alternative constructive deﬁnition of Ferguson (1973), Dalal proves the
existence of such a process.
If G consists of only one element, namely, the identity transformation, then as
one would expect, the DIG corresponds to the Dirichlet process. Dalal derives
several properties and applies them to estimate a distribution function known to
be symmetric at a known point 
; which will be discussed later in Chap. 6.
Tiwari (1981, 1988) extends the Sethuraman’s (1994) alternative representation
of the Dirichlet process to the DIP. Let ˛ be a G-invariant measure on .X; .X//. Let
. p1;p2; : : : :/ and .1; 2; : : :/ be two independent sequences of iid random variables,
such that i Ï ˛ ./ D ˛ ./ =˛ ./ I p1 D V1, and for j  2; pj D Vj…j1
iD1.1  Vi/
and Vj Ï Be.1; ˛ .//. Then the RPM P given by
P.A/ D
1
X
jD1
pj
1
k
k
X
iD1
ıgij.A/ , A 2 .X/
(2.2.1)
is a DIP with parameter ˛.

44
2
Dirichlet and Related Processes
2.2.1
Properties
Dalal established the following properties corresponding to the properties that were
shown to hold for the Dirichlet process.
1. Let P Ï DGI.˛/; and let A 2 .X/. Then P.A/ D 0 with probability one if and
only if ˛ .A/ D 0.
2. Let P Ï DGI.˛/; and let Z be a real valued measurable function deﬁned on
.X; .X//. If R
jZj d˛ < 1, then R
jZj dP < 1 with probability one and
E R ZdP D R Zd˛=˛ .X/.
Samples from the DIP are deﬁned in the same way as for the Dirichlet process.
3. Let P Ï DGI.˛/ and let X be a sample of size 1 from P. Then for any A 2 .X/,
P .X 2 A/ D P .X 2 gA/ D ˛ .A/ =˛ .X/ for any g 2 G:
4. Let P Ï DGI.˛/ and let X be a sample of size 1 from P. Let B1; : : : ; Bm be a
G-invariant measurable partition of X; and A 2 .X/. Then
P .X 2 A; P .B1/  y1; : : : ; P .Bm/  ym/ D
m
X
iD1
˛ .A \ Bi/
˛ .X/
D .y1; : : : ; ym/

;
where D .y1; : : : ; ym/ D D.y1; : : : ; ymj˛ .B1/ ; : : : ; ˛ .Bi/ C 1; : : : ; ˛ .Bm//.
5. If P Ï DGI.˛/; it is also discrete with probability one, like the Dirichlet process.
6. The main property of the DIP is the following conjugacy property.
Theorem 2.10 (Dalal) Let P Ï DGI.˛/, and X1; : : : ; Xn be sample of size n from
P. Then the posterior distribution of P given X1; : : : ; Xn is DGI.˛ C Pn
iD1 ıg
Xi/,
where ıg
Xi D .1=k/ Pk
jD1 ıgjXi, i D 1; : : : ; n.
Using the alternative deﬁnition of Tiwari (1981, 1988) for the Dirichlet Invariant
process, Yamato (1986, 1987) and Tiwari prove properties of an estimable parameter
' that are similar to the properties 9 and 10 stated for the Dirichlet process, and
extend a weak convergence result (property 12) of the Dirichlet measures to the
Dirichlet Invariant measures as well. When G is generated by g .x/ D x; DGI
gives probability one to the distributions that are symmetric about zero.
Bayesian estimation of a symmetric distribution function and estimation of its
functionals, such as a median, the location parameter, and other functionals are
obtained in closed forms by these authors and are provided in the application
chapters.

2.3
Mixtures of Dirichlet Processes
45
2.2.2
Symmetrized Dirichlet Process
Doss (1984) provides an interesting alternative formulation of the symmetrized
Dirichlet process on the real line R. Let ˛ and ˛C denote the restriction of ˛
to .1; 0/ and .0; 1/, and let F 2 D .˛/ and FC 2 D .˛C/. Then F .t/ D
1
2FC .t/ C 1
2.1  FC .t// has a symmetrized Dirichlet process prior and F is
symmetric about 0. In view of 1  1 correspondence, the construction of a random
distribution on R symmetric about 0 is equivalent to the construction of a random
distribution function on Œ0; 1/. If instead we use F .t/ D 1
2FC .t/ C 1
2F .t//; then
F will not be symmetric but F .0/ D 1
2. Denote its prior as D .˛/. The prior D .˛/
also has many properties similar to the Dirichlet process in terms of having a large
support and meaningful interpretation of the parameters.
While Dalal provides a general framework for the invariant Dirichlet process,
Doss (1984) provides a deeper extension of the above theory in the case of
distributions having a median 
. He further extends the construction of symmetric
Dirichlet priors to symmetric neutral to the right priors as follows. Let F1 and F2 be
two independent neutral to the right distribution functions on Œ0; 1/ and construct
a random distribution function F as F .t/ D 1
2F1 .t/ C 1
2.1  F2 .t//; t 2 R. Then
it is a mixture of two neutral to the right distribution functions labeled as a random
distribution function of “the neutral to the right type” and belongs to the set of all
CDFs with median 0. Note that this can be expressed in terms of two nonnegative
independent increments processes Y1 .t/ and Y2 .t/ via the representation Fi .t/ D
1 exp .Yi .t//, i D 1; 2; t  0. In the estimation of median 
; F is considered as a
nuisance parameter. He uses this representation in deriving the posterior distribution
of 
 given X D x when F is a random distribution neutral to the right type, and the
sample X is obtained from F .x  
/, and F and 
 are assumed to be independent.
2.3
Mixtures of Dirichlet Processes
There are situations where the Dirichlet process is inadequate. For example,
consider the following bioassay problem (Ferguson 1974; Antoniak 1974).
Let F.t/ denote the probability of a positive response of an experimental animal
to a certain drug administered at level t  0. We assume that F.0/ D 0 and that
F.t/ is nondecreasing with limt!1 F.t/ D 1. To learn certain properties of F, we
treat n experimental animals at levels t1; t2; : : : ; tn, and observe independent random
variables Y1; : : : ; Yn, where for i D 1; : : : ; n; Yi is equal to one if the animal given
the dose at level ti shows a positive response, and Yi is equal to zero otherwise.
We may treat this problem from a nonparametric Bayesian approach by choosing
a Dirichlet process prior with parameter ˛ for F, F Ï D.˛/. However, it turns out
that the posterior distribution of F given the data is not a Dirichlet process.
Another example is when the data observed is censored on the right. By this
we mean that we do not observe exact observation X, but instead observe the pair

46
2
Dirichlet and Related Processes
Z D min.X; Y/ and ı D I ŒX  Y, where Y is a censoring variable and ı is an
indicator whether we observe X or Y. The problem is to estimate the unknown
distribution function F from which X is sampled and F Ï D.˛/. Here again the
posterior distribution is not the DP.
For these and other closely related problems, the posterior distribution of F
given the data turns out to be a mixture of Dirichlet processes which provides a
rationale to study the same. Motivated by the fact that the DP does not cover many
of the situations encountered in Bayesian analysis, Antoniak (1974) introduced the
concept of mixtures of DPs. A mixture of Dirichlet processes, roughly speaking, is a
Dirichlet process where the parameter ˛ is itself treated as random having a certain
parametric distribution. The simplest mixture of Dirichlet processes would be the
one that chooses P from D.˛1/ with probability , and from D.˛2/ with probability
1  . In this sense it is a parametric mixture of the Dirichlet processes. It chooses
a continuous distribution with probability one. It is essentially an hierarchical
modeling and allows greater ﬂexibility by having the DP centered around some
known parametric distribution.
The study of MDPs was pursued in detail in Antoniak (1974), where the bioassay
and several other problems are discussed. An important result is contained in Dalal
and Hall (1980). They show that a parametric Bayes model can be approximated
by a nonparametric Bayes model using the MDPs model so that the prior assigns
most of its weight to neighborhoods of parametric model and that any parametric
or nonparametric prior may be approximated arbitrarily closely by a prior which
is a mixture of Dirichlet processes. As will be seen later on these mixtures of
Dirichlet processes are extensively useful in modeling large scale high dimensional
data. It allows one to proceed in a parametric Bayesian fashion. This approach
essentially represents as a compromise between purely parametric and purely
nonparametric models in applications. In this section the main ideas of MDPs,
their important properties, and relevant theorems drawn from Antoniak’s paper are
presented. MDPs have been encountered in many applications in modeling data
involving hierarchical structure, covariate data and spatial data. These applications
and computing aspect for sampling from the posterior distribution to carry out
Bayesian analysis will be discussed in Sect. 3.3. Its application in solving bioassay,
empirical Bayes, and regression problems is covered in Chap. 6.
The mixture of Dirichlet processes should not be confused with the other type
of mixtures mentioned in Sect. 3.3. For example, consider f .x/ D R K .x; u/ dG .u/,
where K .x; u/ is a known kernel and G Ï D.˛/. Here the parametric functions are
mixed with respect to a nonparametric mixing distribution.
2.3.1
Deﬁnition
Before a formal deﬁnition can be given, we need the following deﬁnition of a
transition measure:

2.3
Mixtures of Dirichlet Processes
47
Deﬁnition 2.11 Let .‚;  .‚// and .U;  .U// be two measurable spaces. A
transition measure is a mapping of U   .‚/ into Œ0; 1/ such that
(a) For every u 2 U;
˛ .u; / is a ﬁnite, nonnegative, non-null measure on
.‚;  .‚//.
(b) For every A 2  .‚/ ; ˛ .; A/ is measurable on .U;  .U//.
This differs from the usual deﬁnition of a transition probability since ˛.u; ‚/
need not be identically equal to one. It is needed so that ˛.u; / may serve as a
parameter for the Dirichlet process. Also, instead of ˛.u; / it is convenient to use
the notation ˛u./.
An MDP is deﬁned in the same way as the DP was deﬁned. Recall that in deﬁning
the DP, Ferguson took a ﬁnite dimensional vector of P, .P .A1/ ; : : : ; P .Ak// having
a Dirichlet distribution with parameter .˛ .A1/ ; : : : ; ˛ .Ak//. Now we replace this
parameter vector with a bivariate extension of ˛ vector, .˛ .u; A1/ ; : : : ; ˛ .u; Ak//
and integrate u with respect to its distribution H .u/ to obtain a mixture of DPs.
Formally,
Deﬁnition 2.12 P is said to be a mixture of Dirichlet processes on .‚;  .‚// with
mixing distribution H deﬁned on .U;  .U// and transition measure ˛u, if for any
positive integer k and any measurable partition A1; : : : ; Ak of ‚, we have the random
vector .P.A1/; : : : ; P.Ak// distributed as the mixture
Z
U
D .˛ .u; A1/ ; : : : ; ˛ .u; Ak/// dH.u/;
i.e.,
P .P.A1/  y1; : : : ; P.Ak/  yk/ D
Z
D .y1; : : : ; yk j ˛u.A1/; : : : ; ˛u.Ak// dH.u/
where as before, D.˛1; : : : ; ˛k/ denotes the k-dimensional Dirichlet distribution with
parameters ˛1; : : : ; ˛k. Concisely, P Ï R D.˛u.//dH.u/.
We may consider the index u as a random variable with distribution H, and con-
ditional upon given u, P is a Dirichlet process with parameter ˛u; or symbolically,
u Ï H; Pju Ï D.˛u/. The resulting marginal distribution of P is R
U D.˛u/dH.u/.
As an example, let ˛ .u; A/ D ˛ .A/ C ıu .A/, u Ï H; Pju Ï D.˛ C ıu/, then the
resulting process is an MDP. Also, if A1; : : : ; Ak is any partition of ‚, then
.P.A1/; : : : ; P.Ak// Ï
k
X
iD1
H.Ai/D .˛u.A1/; : : : ; ˛u.Ai/ C 1; : : : ; ˛u.Ak// :
(2.3.1)
A random sample from a mixture of Dirichlet processes is deﬁned in the same
way as for the Dirichlet process.
Deﬁnition 2.13 Let P be a mixture of Dirichlet processes on .‚;  .‚// with
transition measure ˛u and mixing distribution H. 
1; : : : ; 
n is said to be a sample

48
2
Dirichlet and Related Processes
from P, if for any positive integer m and measurable sets A1; : : : ; Am, C1; : : : ; Cn,
Pf
1 2 C1; : : : ; 
n 2 Cnju; P .A1/ ; : : : ; P .Am/ ; P .C1/ ; : : : ; P .Cn/g D
n
Y
jD1
P.Cj/ a.s.
2.3.2
Properties
The following are important characteristics and properties of the MDP:
1. If P 
R
D.˛u.//dH.u/ and 
 is a sample of size one from P, then for any
measurable set A 2  .‚/, the marginal for 
 is
P .
 2 A/ D
Z
U
˛u.A/
˛u.‚/dH.u/
2. Let P  D.˛/ and 
 be a sample of size one from P; and let A 2  .‚/ be
a measurable set such that ˛.A/ > 0. Then the conditional distribution of P
given 
 2 A is a mixture of Dirichlet processes with index space .A;  .‚/ \ A/ ;
transition measure ˛u D ˛ C ıu for u 2 A; and mixing distribution HA ./ D
˛ ./ =˛ .A/. In symbols,
Pj
 2 A 
Z
A
D.˛ C ıu/dHA.u/:
If A D ‚; then it reduces to the Dirichlet process. This should come as no
surprise since, given 
 2 ‚ means no information is given and therefore the
posterior is same as the prior. If instead 
 itself is observed, then the posterior
distribution is not a mixture of DPs, but D.˛ C ı
/ as seen is Sect. 2.1.
3. The MDPs satisfy the conjugacy property. That is, if the prior is an MDP, so is
posterior. However, there is an added complexity now since we have to compute
the posterior distribution H.u/ of u given : For this we need some additional
conditions. This theorem is a special case of the Theorem .16) given below.
Theorem 2.14 (Antoniak) Let  D .
1; : : : ; 
n/ be a sample of size n from P;
P  R
U D.˛u.//dH.u/. Suppose there exists a -ﬁnite, -additive measure 	 on
.‚;  .‚// such that for each u 2 U; .i/ ˛u is -additive and absolutely continuous
with respect to 	; and .ii/ the measure 	 has mass one at each atom of ˛u. Then
Pj 
Z
U
D
 
˛u C
n
X
iD1
ı
i
!
dH.u/;
(2.3.2)
where H is the conditional distribution of u given :

2.3
Mixtures of Dirichlet Processes
49
In applications it is usually difﬁcult to compute H.u/; since  will have
duplications. A partial solution is
H .u/ D
1
˛u.‚/.n/
Qk
iD1˛0
u.

i / mu


i
 C 1.ni1/ dH .u/
R
U
1
˛u.‚/.n/
Qk
iD1˛0u.

i /

mu



i

C 1
.ni1/ dH .u/
;
(2.3.3)
where 

1 ; : : : ; 

k are the k distinct observations, ni is the frequency of 

i , ˛0
u.

i /
denotes the Radon–Nikodym derivative of ˛u./ with respect to 	; and mu



i

D
˛0
u.

i / if 

i is an atom of ˛u, zero otherwise.
It is worth noting that the observations not only affect the each component of the
mixture as one would expect, but also alter the relative weighting of the components
via the conditional distribution of u given .
4. Using the earlier mentioned result .of C .m//, and integrating u 2 B with respect
to H .u/ Antoniak obtained the following result:
Proposition 2.15 Let P 
R
U D.˛u/dH.u/; where ˛u is nonatomic for all u 2 U;
and let  D .
1; : : : ; 
n/ be a sample of size n from P. Then the posterior distribution
of u given .
1; : : : ; 
n/ 2 C .m1; : : : ; mn/ 2  .‚/is determined by
P .u 2 Bj 2 C .m// D
R
B.˛ .u; ‚/
Pn
iD1mi =˛ .u; ‚/.n//dH .u/
R
U.˛ .u; ‚/
Pn
iD1mi =˛ .u; ‚/.n//dH .u/
:
(2.3.4)
Note that m1; : : : ; mn appear here only through their sum which is k .n/ ; the
number of distinct observations in the sample. The implication of assuming ˛ to
be nonatomic is that if ˛ .u; ‚/ D M; a constant independent of u; then the event
 2 C .m/ provides no information about u. But if ˛ is atomic for some or all u; the
posterior distribution of u given  will depend on whether any of the observations
coincide with any of the atoms of ˛u. For further elucidation his paper should be
consulted.
5. An important theorem of his is that if we have a sample from a mixture of
Dirichlet processes and the sample is subjected to a random error, then the
posterior distribution is still a mixture of Dirichlet processes.
Theorem 2.16 (Antoniak) Let P 
R
U D.˛u/dH.u/, 
 be a sample of size one from
P. If X is a random variable such that the conditional distribution of X given P; u and

 is F .
; / ; then the conditional distribution of P given X is a mixture of Dirichlet
processes with mixing distribution Hx, i.e., PjX D x 
R
‚U D.˛u C ı
/dHx.
; u/;
where Hx is the conditional distribution of .
; u/ given X D x.
6. From the applications point of view, the most important property is that if we
have a sample from the Dirichlet process which is distorted by random error, then
the posterior distribution of the process given the distorted sample is a mixture of
Dirichlet processes. This corollary is a special case of the theorem in property 5.

50
2
Dirichlet and Related Processes
Corollary 2.17 Let P  D.˛/ on .‚;  .‚//, and 
 be a sample of size one from
P. If X is a random variable such that the conditional distribution of X given P
and 
 is F .
; /, then the conditional distribution of P given X D x is a mixture of
Dirichlet processes with mixing distribution Hx, which is the conditional distribution
of 
 given X D x, and transition measure ˛u.
; / D ˛ ./ C ı
 ./. In notation, if
P Ï D.˛/, 
jP Ï P; and XjP; 
 Ï F.
; /, then PjX  R
‚ D.˛ C ı
/dHx.
/.
In the last two properties computing the posterior distributions H
 and Hx faces
the same kind of difﬁculties that will be seen with respect to the neutral to the
right priors. Antoniak gives a partial solution. However as noted earlier, in recent
years a great deal of progress has been made in simulating posterior distributions
using computational algorithms. This development has somewhat mitigated the
difﬁculty.
7. Property 2 was proved for a single observation. This was extended by Blum and
Susarla (1977) under certain restrictions. This extension is useful in deriving the
posterior distribution given the right censored data.
Theorem 2.18 (Blum and Susarla) Let P Ï D.˛/, and 
1; : : : ; 
k be a sample
from P. The conditional distribution of P given 
i 2 Ai for i D 1; ::; k and A1 
 A2 
: : : 
 Ak 2  .‚/ and ˛ .A1/ > 0 is a mixture of Dirichlet processes with transition
measure ˛k.u; A/ D ˛ .A/ C Pk
iD1	i .A/ C ıu .A/ for .u; A/ 2 ‚   .‚/ and with
mixing measure 	k; where 	1; : : : ; 	k are deﬁned by 	1 .A/ D ˛ .A \ A1/ =˛ .A1/
for A 2  .‚/ ; and
	l .A/ D ˛

A \ Al \ Ac
l1

˛ .Al/ C l  1
C
l1
X
jD1
˛

A \ Aj \ Ac
j1

˛ Aj
 C j  1
l1
Y
iDj
˛ .Ai/ C i
˛ .AiC1/ C i;
(2.3.5)
for l D 2; : : : ; k; where Ac stands for the complement of A and A0 D ;.
2.4
Dirichlet Mixture Models
The mixture models provide a statistical tool to model and analyze grouped data
consisting of known or unknown number of groups, each with similar characteristics
which can be captured by the assumption of a common underlying distribution. The
distribution is assumed to be nonparametric allowing greater ﬂexibility. Dirichlet
process mixture models are especially useful tools in Machine Learning area; see,
for example, Shahbaba and Neal (2009), Hannah et al. (2011), Wade et al. (2014),
Blei and Frazier (2011) and Blei et al. (2003). In this section, we present some
different kinds of mixture models encountered in practice. To carry out statistical
inference, we need to obtain the posterior distribution which will be undoubtedly
complicated and some simulations will need to be done. For this purpose, some
simulation methods will be described brieﬂy ﬁrst. Following that we will present

2.4
Dirichlet Mixture Models
51
some hierarchical and hierarchical mixture models and computational algorithms
to carry out the necessary simulations. There is an extensive literature on mixture
models based on Sethuraman representation. They are included in the next chapter.
Finally, some generalizations are mentioned in the end.
In nonparametric Bayesian analysis, we usually encounter two types of mixtures.
The ﬁrst is parametric mixture of last section discussed in Antoniak (1974), where
the parameter of the Dirichlet process was considered to be random and the mixing
of Dirichlet processes was done with respect to a parametric distribution. That is,
P ./ D
R
D .˛u/ H .du/, H a distribution function. The RPM so obtained was called
a mixture of Dirichlet processes.
Discreteness of the DP causes problems when the unknown distribution is
known to be continuous and may result in inconsistent estimators (Diaconis and
Freedman 1986). A simple recourse is to introduce a convolution with a known
kernel. Finding that in density estimation the DP is inadequate, Lo (1984) is
the ﬁrst one to follow this path. He considered the second type of mixture—the
kernel mixture of Dirichlet processes—to serve as a prior on the space of density
functions. He (Lo 1984) deﬁnes a kernel representation of the density function as,
f.xjG/ D
R
R K.x; u/G.du/; where G is a distribution function on R. Here K.x; u/ is
a kernel deﬁned on .X  R/ into RC such that for each u 2 R,
R
X K.x; u/dx D 1
and for each x 2 X,
R
R K.x; u/˛.du/ < 1. By treating G to be random with a
D .˛/ prior, a random mixing distribution (density) is deﬁned. Here the mixture is
of parametric families with respect to a nonparametric distribution. By integrating
out G, we get a kernel mixture of the Dirichlet processes. Lo (1984) considers in his
treatment a broad class of kernels, such as histogram, normal with location and/or
scale parameters, symmetric and unimodal densities, and decreasing densities (see
Sect. 6.5.4). Ferguson (1983), West (1992) and Escobar and West (1995) considered
Dirichlet mixture of normals.
The difference between the two types is that the mixing components and mixing
weights are interchanged. To distinguish it from the MDPs introduced by Antoniak,
we will call this mixture as kernel mixture of Dirichlet processes (KMDP) or simply
Dirichlet Process Mixtures (DPM), as is known in certain publications. However
they are related in a way. Consider, for example, the following basic mixture
model: yij
i  f .j
i/ ; 
ijG
iidÏ G; Gj˛  D .˛/. Then Gj Ï D .˛ C P ı
i/ and
GjY Ï
R
D .˛ C P ı
i/ H .jY/ are an MDP with mixing distribution H. Thus the
posterior distribution of G in a mixture model is an MDP.
A typical mixture is the normal mixture where the kernel is taken to be a
normal density (say with known variance), f .x/ D
R
N

xj	; 2
dG .	/ ; where
N .xj	; / is the normal density with mean 	 and variance 2. In general, this type
of model may be stated as f .x/ D
R
p .xj
/ dG .
/, and if we use the Sethuraman
representation of G D P1
iD1piıi; then we have f .x/ D P1
iD1pip .xji/. This is
known as a mixture model.
Ferguson (1983) considered a related but different formulation of the
density function. He modeled it as a countable mixture of normal densities:
f .x/ D P1
iD1piN .xj	i; i/. This formulation has countably inﬁnite number of

52
2
Dirichlet and Related Processes
parameters, . p1; p2; : : : :; 	1; 	2; : : : ; 1; 2; : : :/. It can be written as f .x/
D
R N .xj	; / dG .	; / ; where G is the probability measure on the half plane
f.	; / W  > 0g that gives weight pi to the point i D .	i; i/, i D 1; 2; : : : . While
Lo assumes a Dirichlet process prior for the unknown G; Ferguson deﬁnes a prior
via Sethuraman representation. He deﬁnes a prior distribution for the parameter
vector . p1; p2; : : : :; 	1; 	2; : : : ; 1; 2; : : :/ as follows: vectors . p1; p2; : : : :/ and
.	1; 	2; : : : ; 1; 2; : : :/ are mutually independent; p1; p2; : : :. are the weights
SBW(M) in Sethuraman representation; and .	i; i/ are iid with common gamma-
normal conjugate prior for the two-parameter normal distribution. This shows that G
is a Dirichlet process with parameter ˛ D MG0; where G0 D E .G/ is the conjugate
prior for 	; 2 ; and its inﬁnite sum representation is G D P1
iD1piıi where as
usual . p1; p2; : : : :/ and .1; 2; : : : :/ are independent and i
iidÏ G0.
In both the cases, given a sample x1; : : : ; xn of size n from a distribution with
density f .x/ D R N .xj
/ dG .
/ ; the posterior distribution of G given x1; : : : ; xn is
a mixture of Dirichlet processes . The Bayesian estimate of density function f is
pursued in Sect. 6.5.4.
Normal mixtures also turn up in Escobar (1994) and Escobar and West (1995).
Escobar considered the following model. Let yij	i Ï N .	i; 1/, i D 1; : : : ; n,
	ijG
iidÏ G, 	i and G are unknown. His objective, in contrast to those of Ferguson’s
and Lo’s, is to estimate 	i’s (with the variance being known) based on observed
Yi’s. Escobar also uses a Dirichlet process prior for G.
Escobar and West (1995) describe a normal mixture model for density estimation,
similar to Ferguson’s (1983), but in terms of the predictive distribution of a future
observation. For their model, given

	i; 2
i

, we have independent observations, say
y1; : : : ; yn, such that yij

	i; 2
i

Ï N

	i; 2
i

, i D 1; : : : ; n, and i D

	i; 2
i

are
drawn from some prior distribution G on R  RC. Having observed y1; : : : ; yn, the
objective is to ﬁnd the predictive distribution of the next observation ynC1 which
is a mixture of normals, ynC1jy1; : : : ; yn Ï N

	nC1; 2
nC1

. A usual practice is to
put a parametric prior on the vector  D

	1; : : : ; 	n; 2
1 ; : : : ; 2
n

. However, in a
particular case of

	i; 2
i

D

	i; 2
studied, among others by (West 1992), the
distribution of 	i’s is modeled as Dirichlet process with a normal base measure.
Here the authors assume G Ï D .M; G0/, where G0 is the prior guess taken
to be a bivariate distribution on R  RC. In view of the discreteness of Dirichlet
process prior which induces multiplicities of observations, nC1j1; : : : ; n will have
distribution of the form given in property 21 of Sect. 2.1. They proceed on the line
of Ferguson and derive the conditional distribution of ynC1j1; : : : ; n which is a
mixture of a Student’s t-distribution and n normals N

	i; 2
i

. Following that it
is shown that the unconditional predictive distribution is given by ynC1jy1; : : : ; yn
Ï
R
P .ynC1j/ dP .jy1; : : : ; yn/; see Sect. 2.4.2 for further details.
In the above normal mixture model, the base distribution G0 could be continuous.
However, in certain applications such as modeling group data, we desire to have G0
to be discrete instead. Rather than take it a priori a discrete distribution, it seems
preferable to put another DP prior on G0 itself. This has led to a class of hierarchical
models discussed later. This may be viewed as a third kind of mixture. In all these

2.4
Dirichlet Mixture Models
53
type of models, the posterior distribution is complicated and one has to resort to
simulation process.
2.4.1
Sampling the Posterior Distribution
Density estimation problems described above are typical which reveal how difﬁcult
it is to compute the posterior distribution. Similarly, in models such as hierarchical,
dependent DP, and spatial DP, the posterior distribution is not so simple. Earlier
efforts dealt only with evaluation of posterior mean and variance. To carry out a full
Bayesian analysis, we need to know the posterior distribution. Therefore one has to
resort to simulation. This is accomplished by generating a sequence of values from
the posterior distribution via simulation procedures. Based on these values, one can
compute the various quantities such as moments. In fact a full Bayesian analysis can
be carried out utilizing these simulated values.
However, we need to simulate a sufﬁciently large sequence of these samples so
that the law of large numbers would assure us their consistency. During the last
two decades various simulation methods are proposed and algorithms developed.
They are mostly based on Markov chain Monte Carlo (MCMC) procedures using
the Gibbs sampler. In recent years tremendous progress is made in improving these
methods. Needless to say, there is an extensive literature on sampling methods. For a
good starting point is to consult the papers contained in the book edited by Dey et al.
(1998), Gelfand and Smith (1990) and for further improved methods, see the cross
references in Walker (2007) and Papaspiliopoulos and Roberts (2008). We will ﬁrst
give a general outline of these procedures and later give relevant algorithmic steps
while discussing speciﬁc models.
2.4.1.1
Gibbs Sampler
Basically, suppose we wish to draw a sample from the posterior distribution
p .
1; : : : ; 
kjY/ of vector 
 D .
1; : : : ; 
k/ given observations Y D .y1; : : : ; yn/ : Let
p .
ij
i; Y/ denote the induced full (meaning all variables other than 
i included)
conditional distribution of each component 
i given the other components 
i D
.
1; : : : ; 
i1; 
iC1; : : : ; 
k/ ; for i D 1; : : : ; k, and known information, Y. Since
the joint distribution can be written in terms of full conditional distributions, the
procedure is to start with some initial values 
0 D 
0
1; : : : ; 
0
k
 and generate

1 D 
1
1 ; : : : ; 
1
k
 by drawing 
1
i , one by one, from the updated distribution in
which we replace 
0
i with already drawn 
1
i , until we replace all k 
’s. Order does
not matter. This is the ﬁrst iteration from 
0 D 
0
1; : : : ; 
0
k
 to 
1 D 
1
1 ; : : : ; 
1
k
.
Proceeding this way generates a sequence 
0; 
1; : : :, 
m; : : : from p .
1; : : : ; 
kjY/,
which is a realization of a Markov chain (MC), with transition probability kernel

54
2
Dirichlet and Related Processes
from 
m to 
mC1,
K


m; 
mC1
D
kY
lD1
p.
mC1
l
j
m
j ; j > l; 
mC1
j
; j < l; Y/:
If the posterior is not of a known form, which would be the case if the prior is
not conjugate, one uses the Metropolis–Hastings algorithm. It is a generalization of
the acceptance-rejection rule. However, we have other methods that have proved to
be more efﬁcient.
2.4.1.2
Sampling Strategies
Generally speaking there are two different Gibbs sampling strategies: marginal and
conditional. In the marginal, the strategy is to integrate out analytically the unknown
distribution function G and then exploit the Polya urn characterization of Blackwell
and MacQueen (1973) for drawing samples from the posterior distribution. This
path was initiated by Escobar (1994) and pursued by Escobar and West (1995),
MacEachern (1998) and West et al. (1994), among others. It can be applied to any
mixture model for which the predictive distribution induced by G is known. This
strategy is easily carried out in the case when we have conjugate prior. The process
involves updating components one-at-a-time and is slow and there is tendency for
the process to get stuck at the few distinct values or clusters formed earlier and
new clusters are less frequently formed. In the absence of conjugacy, it is difﬁcult
and Metropolis–Hastings method needs to be used. But there again picking the
right proposal distribution that would enable the Markov chain to converge well
is difﬁcult.
The conditional strategy introduced by Ishwaran and Zarepour (2000, 2003) and
further developed in Ishwaran and James (2001, 2003) is to retain G and exploit
its inﬁnite sum representation. In this approach, the inﬁnite sum is truncated at a
suitable level and Gibbs sampling is carried out from the joint posterior distribution
of weights p’s, atoms ’s, and cluster indicator s’s, deﬁned below. In this way,
simultaneous updating of large subsets of the variables is done while focusing on
ﬁnding appropriate ways for sampling a ﬁnite but large number of atoms of G. It is
readily adaptable to more general stick-breaking measures and to many extensions
of the DP. In addition, theoretically it allows inference for G as well. Under this
approach, a number of procedures have been proposed. They include blocked
Gibbs sampler (Ishwaran and Zarepour 2000; Ishwaran and James 2001), slice
sampling (Neal 2000; Walker 2007), and retrospective sampling (Papaspiliopoulos
and Roberts 2008). In the blocked Gibbs sampling, the inﬁnite sum is truncated
to a positive integer N. The convergence of the Markov chain (MC) is affected by
the way N is chosen. In contrast, the latter two do not require truncation and the
components in the inﬁnite sum representation are added as needed.

2.4
Dirichlet Mixture Models
55
Thibaux (2008) has noted that the Gibbs sampling procedure is too slow for
large scale applications of DP mixtures. Therefore, he develops new Monte Carlo
algorithms which are based on split-and-merge algorithms of Jain and Neal (2004).
2.4.1.3
Marginal Approach
The marginal approach of Escobar (1994) exploits the Polya urn characterization of
the DP in constructing the MCMC using Gibbs sampler. We start with the following
basic mixture model (West et al. 1994). Let
yij
i; 
ind
 f .j
i; / ; 
ijG
iid G; i D 1; : : : ; n; Gj˛  D .˛/ ;
(2.4.1)
where 
i is a vector of parameters associated with index i,  is a vector common to
all i’s, and ˛ D MG0. The functional form of F with density f .j
i; / is assumed to
be known. It may also depend on i as in the case of regression model (for example,
fi .j
i; / D N .Xi
i; 1/). Here  represents a vector of parameters associated with
the distribution f, as was the case of normal mixture with unknown variance (West
1992). In hierarchical model we will have an additional set of hyperparameters, .
For example, if we assume G  D .M; G0/,  D .M; G0/ ; G0 D E .G/ is the base
prior, and M is the precision parameter as before. In such models, we further place
priors on these parameters. However, in our treatment here we suppress  and ,
with the understanding that the following presentation is all conditional on both 
and . Any realization of 
i generated from G yields a set 
 D



1 ; : : : ; 

k

, k  n
of distinct values and is marginally a random sample from G0. Given k, n values of

i are selected from the set 
 according to a uniform multinomial distribution.
In the Polya urn characterization of the DP, we integrate out G and get the joint
distribution of 
’s as
 .d
1; : : : ; d
n/ D G0 .d
1/
n
Y
iD2
8
<
:
i1
X
jD1
1
M C n  1ı
j

d
j

C
M
M C n  1G0 .d
i/
9
=
;
and the conditional predictive distribution as

nj
n1; : : : ; 
1 
k
X
jD1
nj
M C n  1ı

j C
M
M C n  1G0;
(2.4.2)
where 

1 ; : : : ; 

k are k distinct values with frequencies n1; : : : ; nk, respectively,
among 
1; : : : ; 
n1. That is, the posterior distribution of 
n given 
n1; : : : ; 
1 is a
mixture of a discrete distribution with weights on other 
’s, and a known distribution
G0. Since the sequence generated is exchangeable, we can replace 
n in the above
expression by 
i conditional on 
i D .
1; : : : ; 
i1; 
iC1; : : : ; 
n/.

56
2
Dirichlet and Related Processes
Multiplying the above with likelihood f .yij
i/, we obtain the conditional poste-
rior distribution of 
i as

ij
i; y _
k
X
jD1
n
j f

yjj

j

ı

j
C Mf .yij
i/ G0 .
i/
D
k
X
jD1
n
j f

yjj

j

ı

j
C
	
M
Z
f .yij
i/ dG0 .
i/

f .
ijyi; G0/
D
k
X
jD1
q
j ı

j
C q
0f .
ijyi; G0/ ; say,
(2.4.3)
where superscript  represents quantities we get when 
i is excluded from the set
and f .
ijyi; G0/ is the posterior distribution of singleton 
i based on yi alone and
M
R
f .yij
i/ dG0 .
i/ is the marginal of yi. This leads to a Gibbs sampler for 
i
in which it takes one of the previously observed values 

j
with probability pro-
portional to n
j f

yjj

j

or a new value drawn from f .
ijyi; G0/ with probability
proportional to M
R
f .yij
i/ dG0 .
i/. That is,

ij
i; y Ï
(


j
w.p. n
j f

yjj

j

; j D 1; : : : ; k


kC1 w.p.
M
R
f .yij
i/ dG0 .
i/
(2.4.4)
where  is a normalizing constant.
The conditional distribution (2.4.2) clearly suggests that it is not necessary to
generate all n 
0s since some of them are expected to be the same, a consequence
of the DP prior, but only fewer 
0s and identiﬁers s which attach 
 to 
. This
led MacEachern (1994) to suggest an improved approach in which he reduces the
dimension of the state space and hence of the transition kernel, by considering 
0s
instead of 
0s. This was reﬁned in West et al. (1994).
Given k, let si D j if 
i D 

j , j D 1; : : : ; k, so that, given si D j and 
,
yi  f

j

j

. The vector S D .s1; : : : ; sn/ determines grouping of Y D .y1; : : : ; yn/
into k distinct groups or clusters with nj D # fsi D jg ; j D 1; : : : ; k. Let Ij be the set
of indices of observations in group j, i.e., Ij D fi W si D jg and let Yj D fyi W si D jg
be the group of observations in cluster j. Since 

j are a random sample from G0;
the posterior analysis reduces to a collection of k independent analyses, i.e., 

j are
conditionally independent with posterior density




j jY; S; k

 



j jYj; S; k

/
Y
i2Ij
f

yij

j

dG0



j

; j D 1; : : : ; k:
(2.4.5)

2.4
Dirichlet Mixture Models
57
Since 
i  G and G  D .M; G0/, we have conditional distribution

ij
i; si; ki 
ki
X
jD1
n
j
M C n  1ı

j
C
M
M C n  1G0:
(2.4.6)
This shows that 
i can be generated such that with probability proportional to
n
j ; pick 

j
, and with probability M= .M C n  1/ pick a new value distributed
according to G0. Knowledge of 
 is theoretically equivalent to the knowledge of
k, S, and 
. Now as before multiplying both sides by the likelihood, we get the
conditional posterior distribution as

ijY; 
i; si; ki 
ki
X
jD1
qijı

j
C qi0Gi0
(2.4.7)
where
qi0 / Mhi .yi/ ; qij / n
j f

yij

j

; j D 1; : : : ; k;
Gi0 denotes the posterior, namely dGi0 .
i/ / f .yij
i/ dG0 .
i/ and hi .yi/ D
R f .yij
i/ dG0 .
i/ is the marginal density of yi evaluated at the realized datum under
the base prior for 
i. Since si D j iff 
i D 

j
; Eq. (2.4.7) implies
P .si D jjY; 
i; si; ki/ D P


i D 

j
jY; 
i; si; ki

D qij; j D 0; 1; : : : ; k:
(2.4.8)
In other words, si D jjY; 
i; Si; ki means 
i takes value 

j
with probability qij;
j D 1; : : : ; k; si D 0; draw a new 
i from Gi0 of (2.4.7) and this would generate
conﬁguration 
. Now we may successfully sample sets of values k, S and 
 D



1 ; : : : ; 

k

, k will also vary depending upon how many distinct values are among

i. The procedure at any stage m of the algorithm is
(a) Given current 
 (hence k) and S, we generate a new conﬁguration by sequen-
tially sampling indicators one by one, from the posterior distribution (2.4.8),
successively simulating and substituting s1; s2; ::I and mindful that for any index
i such that si D 0; to draw a new 
i from Gi0 of (2.4.7). This will also yield a
new k, the number of clusters. Next we need to pick random 
’s. Ishwaran and
James (2001) suggest that it is simpler to use the current value of 
 from which
to compute current S and then update current 
 given S.
(b) Given k and S; generate a new set of parameters 
 by sampling each new 

j
from the relevant component Q
r2Ijfr

yrj

j

dG0



j

in .2:4:5/.
Successive simulated values will be the realized values of a sample from the
joint posterior distribution  .k; S; 
jY/. Now inference can be based on these
values. Convergence issues are discussed in MacEachern and Müller (1998).

58
2
Dirichlet and Related Processes
Going back to the basic model, when  is unknown and has a prior
distribution  ./, then the posterior conditional distribution of  given the data
is
 .jY; 
; S; k/ /  ./
n
Y
iD1
fi .yij
i; / D  ./
kY
jD1
Y
r2Ij
fr

yrj

j ; 

:
(2.4.9)
Then the above sampling scheme would include an additional step.
(c) Simulate a value of  from this distribution, given the current simulated values
of 
; S, and k; at each step.
When we do not have conjugacy, evaluation of the integral in hi .yi/ becomes
difﬁcult. West et al. (1994) suggested replacing the integral with an average of inte-
grals, average over draws 
0 (in place of 
i/ from the base prior G0. MacEachern and
Müller (1998) pointed out that although this method does provide an approximation
to the posterior, the accuracy is difﬁcult to ascertain. Therefore, they suggest the
so-called no gap algorithm. However Neal (2000) pointed out that this approach is
potentially inaccurate since the posterior distribution M
R
fi .yij
i/ dG0 .
i/ based on
yi alone will be considerably more concentrated than G0, and as a consequence
the process is slower in convergence but also may lead to the wrong stationary
distribution, and is inefﬁcient due to the reduced probability of assigning an
observation to a new cluster.
Interestingly, Neal (2000) expresses this basic model (suppressing / as a ﬁnite
mixture model with k components which is equivalent to the original model as k !
1. It is formulated in terms of grouping variables si; i D 1; : : : ; n, which identiﬁes


j , the value 
i assumes. Let si 2 f1; ::; kg denote the class observation yi belongs
to
yijsi; 
 Ï f



si

sijp Ï Discrete . p1; : : : ; pk/


s Ï G0 and p Ï D
˛
k ; : : : ; ˛
k

;
(2.4.10)
where pD . p1; : : : ; pk/ ; D the Dirichlet distribution and ˛ a positive real number.
By integrating over the mixing proportion p, we can obtain the conditional
distribution
P .si D jjs1; : : : ; si1/ D
n
j C ˛=k
i  1 C ˛ !
n
j
i  1 C ˛ as k ! 1
and P

si ¤ sj for all j < ijs1; : : : ; si1

D
˛
i  1 C ˛ ;
(2.4.11)

2.4
Dirichlet Mixture Models
59
where n
j is the number of s’s among s1; : : : ; si1 that are equal to j. Now if we let

i D 

si , we see that the limit of this model is equivalent to the original DP mixture
model with ˛ as the parameter of the DP.
2.4.1.4
Conditional Approach
Escobar and West and MacEachern’s methods, as pointed out by Ishwaran and
Zarepour (2000), suffer from two limitations: one, by marginalizing G; the Markov
chain tends to mix slowly because the Gibbs sampler uses one coordinator at a time
to update; two, it has the undesirable effect of allowing posterior inference to be
based only on the values of Y. To circumvent these problems, they suggest replacing
the DP prior P by its ﬁnite dimensional approximation PN deﬁned as
PN D
N
X
jD1
pjıj; 1  N < 1;
(2.4.12)
where p’s are random variables such that 0  pj  1 with p1 C    C pN D 1, and
independent of p’s, j
iidÏ H. A key difference here is the weights pj’s need not be
constructed using the SB construction. This will yield the posterior distribution also
with such a representation and reduce the problem to a ﬁnite dimension allowing
the model to be expressed in terms of a ﬁnite number of random variables. This will
make implementation of the Gibbs sampler easier. This is the main idea.
It is similar in spirit to the Neal’s model with one difference. The grouping
variable si in Neal’s model matches 
i with cluster representative 

j . Now the
indicator variable ki associates 
i with j; ki 2 f1; : : : ; Ng. That is, 
i D ki. They
both are, however, related. This can be seen by introducing another identiﬁer tj D h
iff 

j
D h. In that case, tsi D ki. In this approach also, ki will be assumed to
have some discrete distribution. This facilitates the Gibbs sampler to update blocks
of parameters at a time, and generate a sequence . pm; m; km/ ; m  1 of vectors
p D . p1; : : : ; pN/ ;  D .1; : : : ; N/, and k D .k1; : : : ; kn/. From this we can
construct the posterior random measures Pm
N D PN
jD1 p.m/
j
ı.m/
j , which would be
draws from the posterior measure PN .jY/, and based on them statistical inference
can be performed. They named the procedure as blocked Gibbs sampler.
Blocked Gibbs sampler
The model (2.4.1) can equivalently be expressed as
yij; k; 
ind
 f .yijki; / ; i D 1; : : : ; n

60
2
Dirichlet and Related Processes
kijp
iid
N
X
jD1
pjıj
. p; /
ind
  . p/  ./
 Ï  ./ :
(2.4.13)
There are different possibilities to assign a distribution to p; such as a symmetric
Dirichlet distribution with parameter ˛I a truncated beta two-parameter distribution;
or a generalized Dirichlet distribution GD .a; b/ with parameters a D .a1; : : : ; aN/
and b D .b1; : : : ; bN/.
To sample from the posterior distribution PN .jY/, we draw iteratively from the
following conditional distributions:
.jk; Y; /
.kj; p; Y; /
. pjk/
.j; k; Y/ :
(2.4.14)
Each draw of .; k; p; / deﬁnes an RPM P ./ D PN
jD1 pjıj ./, which yields
a draw from the posterior PN .jY/. Note that j
iid H and kjp follow a discrete
distribution. The auxiliary parameter  will have some known distribution. Thus we
need only to assign a prior for the vector p. This is done in such a way that the RPM
PN approximates the DP, P. The algorithm is as follows. Let
˚
k
1 ; : : : ; k
m

be the set
of current distinct values.
(a) Sample j
iid H for each j 2 kn
˚
k
1 ; : : : ; k
m

; and for j D 1; : : : ; m, draw k
j
from the density
H

k
j jk; ; Y

/ H

dk
j

Y
n
iWkiDk
j
o. fyijk
j ; /; j D 1; : : : ; m:
Conjugacy makes the draw exact. Otherwise, Metropolis–Hastings methods
may be used. Hierarchical extensions can be implemented in a straight forward
manner.
(b) Sample ki from
.kij; p; ; Y/
ind

N
X
jD1
pj;iıj ./ ; i D 1; : : : ; n;

2.4
Dirichlet Mixture Models
61
where
. p1;i; : : : ; pN;i/ / . p1 f .yij1; / ; : : : ; pNf .yijN; // :
(c) For p, Ishwaran and Zarepour (2000) propose three different choices for a
prior.
(i) The ﬁrst one is the commonly used approximation of the DP, the symmet-
ric Dirichlet distribution, in which pj˛ Ï D. ˛
N ; : : : ; ˛
N /; ˛ > 0. Sample the
posterior pj˛; k Ï D. ˛
N C n1; : : : ; ˛
N C nM/ for p, where n1; : : : ; nM are the
multiplicities of distinct values in the vector k. This choice is justiﬁed by
noting that it approximates the DP for sufﬁciently large N. It leads to an
easy update for the conditional distribution of p. Selection of N is discussed
in their paper.
(ii) The second one is to assume pj .a; b/ Ï GD .a; b/, a generalized Dirichlet
distribution with parameters a; b. This results in exact draws from the
posterior of SB ratios V
j and then computing pj:
V
j
ind
 Be
0
@aj C nj; bj C
N
X
lDjC1
nl
1
A ; for j D 1; ::; N  1; and
p1 D V
1 ; pj D V
j
j1
Y
iD1

1  V
i

:
(2.4.15)
This procedure requires simulation of N  1 beta random variables and is
very efﬁcient.
(iii) Another possibility is to use truncated beta two-parameter process, in
which case aj D a and bj D b for all j  1, in the above distribution
of V
j ’s. But then one has to be careful in choosing N appropriately.
(d) Conditional for  W Noting that yi D si sample  from the density
 .j; S; Y/ /  .d/
n
Y
iD1
f .yijsi; / :
Slice Sampling
This method introduced by Walker (2007), builds upon Neal’s (2003) slice sampler,
does not marginalize over G and employs the Sethuraman representation of the DP
as well, G  D .M; G0/. However, the beauty is that it does not require truncation
but reduces the inﬁnite sum to a ﬁnite sum by introducing certain latent variables

62
2
Dirichlet and Related Processes
u1; : : : ; un, 0  ui  1, i D 1; : : : ; n; so that


yijfpjg; fjg

D
Z
f .yij
i/ dG .
i/ D
1
X
jD1
pj f .yiji/
(2.4.16)
is augmented to


yi; uijf pjg; fjg

D
1
X
jD1
IŒ pj > ui f .yiji/ :
By integrating out ui’s it reduces to the above expression (2.4.16) again. By
conditioning on ui, the inﬁnite mixture is transformed to a ﬁnite mixture with a
ﬁxed number of components Nui D P1
jD1IŒ pj > ui. Let Ap .u/ D
˚
j W pj > u

. Now
introducing further atom identiﬁers, ki 2 f1; 2; : : :g, i D 1; : : : ; n, the augmented
model becomes


yi; ui; kijfpjg; fjg

D IŒ pki > ui f .yijki/ :
(2.4.17)
Integrating over ui and ki; it again reduces to the original expression (2.4.16). The
joint distribution of fyig; fuig and fkig is then


fyi; ui; kign
iD1jfpjg; fjg

D
n
Y
iD1
IŒ pki > ui f .yijki/ :
(2.4.18)
Note that indicator ki matches yi with atom ki, i.e., ki D j iff yi D j, as before.
To implement the Gibbs sampler, we need to update pj; j; ui, and ki; for which
the key steps are (details may be found in Walker 2007, Müller et al. 2015):
1. Weights pj are updated via the stick-breaking ratios Vj by sampling them from
the updated beta distribution,
Vjj : : :  Be
0
@1 C nj; M C
X
i>j
ni
1
A ;
where nj D Pn
iD1IŒki D j.
2. Atoms j
are sampled from the posterior distribution proportional to
G0 .k/ Q
fiWkiDkgp .yijk/. If there are no ki D j, sample atoms from G0 directly.
3. Latent variables ui are sampled from the (posterior) uniform distribution
U .0; pki/.
4. Indicators ki are sampled from the distribution
P fki D kg / I Œ pk > ui f .yijk/ D I

k 2 Ap .ui/

f .yijk/ :

2.4
Dirichlet Mixture Models
63
Since only a ﬁnite number of components satisfy the constraint pj > ui, the
normalizing constant can easily be evaluated as P
j1Wpj>ui f

yijj

.
By introducing latent variables ui; the process is simpliﬁed. Without them, we
may have to sample an inﬁnite number of ’s, which is impossible. Papaspiliopoulos
and Roberts (2008) also attempt to circumvent this problem by developing a
retrospective sampling procedure, in which ’s are generated retrospectively as the
need arise. However, the procedure is not so simple and therefore is not included
here.
2.4.2
Hierarchical and Mixture Models
In this section we will introduce various hierarchical and mixture models and
processes that have appeared in the literature during the last decade in connection
with modeling group and complex data. Besides hierarchical and mixture processes,
they also include among others, nested and dynamic Dirichlet processes, dependent
and spacial Dirichlet processes, and time-varying Dirichlet processes. They all
exploit the discreteness property of the Dirichlet process to borrow information
across observations as well as groups. It is clear that the Dirichlet process in their
treatment can also be replaced by more general two-parameter Poisson–Dirichlet
and beta processes. Computational methods described in the previous section are
used for simulating posterior distributions to carry out inferential procedures. For
details on computational and inferential procedures, and applications to real and
simulation data, the reader is advised to refer to respective individual papers.
Generally in a nonparametric Bayesian modeling we have y1; : : : ; yn
iid G and
G  P, a certain prior on the distribution function G. That is an iid sample is drawn
from a single realization of P. In mixture modeling, we add an additional layer in
between. We assume yij
i
ind
 F
i, a known parametric distribution, and place a prior
on parameters 
ijG
iid G with G  P. In group data modeling, we replace F
i with a
nonparametric distribution function Gi and assume Gi
iid P, i.e., each observation is
drawn from a different realization of P. It can also be extended to a mixture model
in an obvious manner.
In the previous section we considered the following basic mixture model:
yij
i; 
ind
 f .j
i; / ; 
ijG
iid G; i D 1; : : : ; n; Gj˛  D .˛/ :
(2.4.19)
The parameters 
i’s are conditionally independent given G, and the observations
are conditionally independent given the parameters 
i. A prior is placed on the
nonparametric distribution G of parameters of the mixture model. When the prior
is a DP, the model is referred to as a Dirichlet Process mixture (DPM) model or
Dirichlet mixture Model (DMM).

64
2
Dirichlet and Related Processes
It is obvious that the mixture models need not be restricted to the DP priors.
Other priors such as the gamma process or two-parameter Poisson–Dirichlet process
priors may also be used. In a recent publication, Favaro and Teh (2013) describe
normalized random measure mixture models where the distribution function G
is replaced by a normalized CRM with parameter —a Levy measure, and base
measure 	0.
This model can alternatively be expressed in terms of the stick-breaking (SB)
representation, order reﬂecting convenience for simulation purposes.
GjM; F0  D .MF0/ ; pjM  SBW.M/; kjjp  p;
kjF0
iidÏ F0; yjj.kj; fkg1
kD1/  F

jkj

:
(2.4.20)
Also, G D P1
kD1 pkık and 
j D kj and p D . p1; p2; : : :/. This model can also be
derived as the limit of a sequence of ﬁnite mixture models, where the number of
mixture components tends to inﬁnity, as we saw earlier.
In the above model, the distribution of the parameters of the function f was
assumed to be nonparametric and a prior was placed on it. A different type of mix-
ture is the one which leads to hierarchical models where the parameters of the prior
distributions themselves are considered as random and assigned priors with hyper
parameters. It has a long history of applications in parametric and semiparametric
set ups. Its adaptation to the case of nonparametric (inﬁnite dimensional parameters)
may be framed as follows. In the usual nonparametric Bayesian models we assume

1; : : : ; 
n
iidÏ F and F a Dirichlet process with parameters M and F0. The baseline
distribution F0 is generally taken to be a parametric distribution. However we can
go one step further—M or F0; or both may be treated as random and priors may as
well be assigned to them. For example, we may have for model,

1; : : : ; 
n
iidÏ F; FjM; F0 Ï D .M; F0/ and F0jM; G0 Ï D

M; G0

;
(2.4.21)
each conditionally independent. A hierarchical model is thus basically a prior
distribution over a set of RPMs or distributions whose parameters themselves are
assigned priors. This type of models are referred to as HDP models or in general,
hierarchical models.
2.4.2.1
Group Data Models
In the group data model, we associate with each subgroup an RPM Gj (RPM
P replaced by Gj), and assume that we have variables 
ji drawn from Gj, i.e.,

jijGj
iidÏ
Gj; i D 1; : : : ; nj, j D 1; : : : ; J; the objective being to ﬁnd clusters
that capture certain structure inherent within groups, the number of clusters being
unknown, as well as to link groups in order to borrow information across groups.
For the purpose of borrowing information, one may consider pooling the data into

2.4
Dirichlet Mixture Models
65
one group and assign a single prior in which case the individual characteristics
of the subgroups are lost. The other extreme case is to assign different priors to
each subgroup in which case no information is shared. HDP model is a compromise
between the two extremes in which dependence is implemented by sharing part of
the probability mass across different groups. This is to be accomplished by sharing
clusters among related groups. In such cases, the discreteness of the Dirichlet
process turns out to be an asset. These models are applicable in the cases where the
observations are assumed to be exchangeable within each group and across groups,
groups themselves are exchangeable. They have proved to be especially useful in
genetics and Information Retrieval ﬁelds.
Let G0 be a global RPM. Gj are conditionally independent given G0 and
distributed according to a DP with base measure G0 and concentration parameter
M, i.e., GjjM; G0
ind
Ï D .M; G0/. Thus a reasonable model for group data may be
stated as

jijGj
iidÏ Gj; i D 1; : : : ; nj; j D 1; : : : ; J; GjjM; G0
ind
Ï D

M; G0

:
(2.4.22)
The simplest way to borrow strength across groups is through the base measure
G0 and Sethuraman representation of Gj. G0 may itself be taken as a parametric
distribution and information may be shared through its parameters. But the choice
of such a distribution may be considered as a limitation. Another possibility is to
induce dependency among Gj’s by integrating out the parameters of G0. But they
will not ensure any common atoms for some Gj and thus will not permit sharing of
clusters between groups defeating the purpose on hand. Alternatively, some authors
assume each Gj distributed as group-speciﬁc DP, say, D.M
j ; G0j/ and link the
groups through M
j and/or G0j. In such cases, however, each G0j will have different
sets of atoms and linking of groups via atoms will not be achievable.
2.4.2.2
Hierarchical/Mixture Models
Note that generally, G0 is taken to be a nonatomic measure yielding distinct atoms
with probability one in its SB representation. However, if we want clustering within
groups, it is essential to force G0 to be discrete so that a priori there are ties among
its atoms. One could take it to be a discrete distribution to start with, but that would
be too restrictive. Thus the proposed solution by Teh et al. (2004, 2006) is to not
only force G0 to be discrete by treating G0 itself to be a draw from a DP, D .; H/ ;
but also treat each Gi to be drawn from the same DP, namely D .M; G0/ ; each
Gi thus sharing the atoms of G0. This does not cause any technical problem since
the DP is deﬁned on a general separable metric space. Thus we have the following
model:

jijGj
iidÏ Gj; i D 1; : : : ; nj; GjjM; G0
iidÏ D

M; G0

; G0j; H  D .; H/ :
(2.4.23)

66
2
Dirichlet and Related Processes
This model is called a HDP model for group data. This can easily be extended to
multiple levels of hierarchical modeling.
The hierarchical structure may be extended to mixture models involving group
data, by placing the HDP prior over factors in the mixture model resulting into the
following model:
yjij
ji
ind
Ï F

j
ji

; 
jijGj
iidÏ Gj; i D 1; : : : ; nj; j D 1; : : : ; J;
Gjj

M; G0
 iidÏ D

M; G0

; G0j; H  D .; H/ ;
(2.4.24)
where 
ji is a factor corresponding to a single observation Xji and F is assumed to
be known. As usual all variables are assumed to be conditionally independent. The
parameters
˚

ji
nj
iD1 are likely to assume the atoms
˚


k
1
kD1—which are known as
cluster parameters—of G0 because of the SB representation of Gj (all Gj have the
same set of atoms
˚


k
1
kD1). Here yji and yji0 belonging to the same group share the
same atom 
 of Gj, but also the observations across different groups may share
them as a consequence of the discrete nature of G0. This model is referred to as a
Hierarchical Dirichlet process mixture (HDPM) model.
A formal study of such models in the context of nonparametrics described here
was reported in Teh et al. (2006) where they develop these models. This was
followed by other related extensions such as nested DP (Rodriguez et al. 2008),
dynamic DP (Dunson 2006), time-varying DP (Caron et al. 2007), and hierarchical
dynamic DP (Ren et al. 2008). The interest in these models stems from a need to
model data which is subdivided into a set of groups, as in the case of multi-centric
studies, where often there are group similarities forming group-clusters and groups
themselves may show some clustering tendencies within each group as well, and the
desire is to not only borrow strengths from within groups but also across the groups.
These models fall in the general framework of dependent nonparametric pro-
cesses formulated by MacEachern (1999) and others and discussed later in Chap. 3.
In this formulation F D fFx W x 2 g is deﬁned to be a class of related random
probability measures indexed by x 2 , with the objective to borrow information
across related/dependent RPM to strengthen inference procedures. F is treated as
a stochastic process and the goal in Bayesian analysis is to place a joint distribution
prior on F. In covariate and spatial models  is treated as a continuous space,
usually a subset of ﬁnite dimensional Euclidean space; in the case of sequential
and time-varying models,  is considered as a countable inﬁnite set of time points,
 D ft1; t2; : : : ; g; and in case of group data, it takes a ﬁnitely many values,
 D f1; : : : ; Jg. Clearly the DP falls in this formulation when F has a single
element and the distribution on F is the DP. Thus the dependent nonparametric
processes and DDPs are extensive generalization of the DP to treat collectively
related or dependent RPMs. This approach has generated lot of interest and a
number of models have been proposed in the literature. We will describe some of
them here and others later in the book under the heading Dependent nonparametric
processes.

2.4
Dirichlet Mixture Models
67
The Stick-Breaking Construction of HDPM
The stick-breaking construction for the above models may be framed as follows (Teh
et al. 2006). The SB representation of the base measure G0 and group measures Gj
may be expressed as
G0 D
1
X
kD1
ˇkık; and Gj D
1
X
kD1
jkık;
(2.4.25)
where as usual ˇ’s are the SB weights, ˇ D fˇ1; ˇ2; : : :g  SBW ./ ; k
iidÏ H.
G0 has support at the atoms fkg1
kD1 and so necessarily Gj also has support at those
atoms. Denote j D
˚
j1; j2; : : :

. Weights j are independent given ˇ, since Gj’s
are independent given G0. It can be seen that j’s are related to ˇ ’s in the following
way:
jk D jk
k1
Y
mD1

1  jm

; jk Ï Be
 
Mˇk; M
 
1 
k
X
lD1
ˇl
!!
:
(2.4.26)
In fact, j  D .M; ˇ/, where j and ˇ are viewed as discrete probability
distributions over the positive integers. This may be seen as follows. Any ﬁnite
partition .A1; : : : ; Ak/ of X induces a corresponding partition .N1; : : : ; Nk/ of N such
that Nj D
˚
i W i 2 Aj

, j D 1; : : : ; k. Therefore,
Gj .A1/ ; : : : ; Gj .Ak/  D MG0 .A1/ ; : : : ; MG0 .Ak/
)
0
@X
k2N1
jk; : : : ;
X
k2Nk
jk
1
A  D
0
@M X
k2N1
ˇk; : : : ; M X
k2Nk
ˇk
1
A ;
(2.4.27)
for every ﬁnite partition of N. Thus j  D .M; ˇ/ for each j independently.
Since each 
ji is distributed according to Gj, it takes on the values k with
probability jk. As before for the mixture model, let kji be an indicator variable such
that 
ji D kji. Given kji; we have yji Ï F

jkji

. Thus an equivalent representation
of the HDPM model may be stated in terms of conditional distributions as follows:
ˇj  SBW ./ ; jj.M; ˇ/  D

M; ˇ

; kjijj  j;
kjH Ï H; yjij.kji; fkg1
kD1/ Ï F

jkji

:
(2.4.28)
Chinese Restaurant Franchise
The CRP characterization of the DP is extended to the hierarchical DP where
multiple restaurant share a set of common dishes. The authors name it as

68
2
Dirichlet and Related Processes
Chinese Restaurant Franchise. Here restaurants correspond to groups, customers
to parameters 
ji; ith customer in jth restaurant. Also, let 1; : : : ; K denote K iid
random variables distributed according to H; representing common dishes across
restaurants. Let  jt represent the table-speciﬁc dish (only one) served at table t in
restaurant j,  jt 2 f1; : : : ; Kg, tji be the index of the  jt associated with 
ji and
let kjt be the index of k associated with  jt. That is, 
ji D  jtji and  jt D kjt. In
the Chinese Restaurant franchise metaphor, customer i in restaurant j sits at table
tji whereas table t in restaurant j serves dish kjt. Also, let njtk denote the number of
customers in restaurant j sitting at table t and consuming dish k. Marginal totals
will be denoted by dots. Thus njt denotes the number of customers in restaurant
j at table t, njk denotes the number of customers in restaurant j consuming dish
k; mjk denotes the number of tables in restaurant j serving dish k, mj denotes the
number of tables in restaurant j, mk denotes the number of tables serving dish k,
and ﬁnally, m denotes the total number of tables occupied. By integrating out Gj,
the conditional distribution of 
ji given 
j1; : : : ; 
ji1; M; G0, is obtained as

jij
j1; : : : ; 
ji1; M; G0 
mj
X
tD1
njt
i  1 C M ı jt C
M
i  1 C M G0:
(2.4.29)
This is a mixture with the usual interpretation of a draw from this mixture. Patron

ji will be seated at previously occupied table serving table-speciﬁc dish  jt (i.e.,

ji will take value  jt) with probability njt= .i  1 C M/ and with probability
M= .i  1 C M/ will choose a new table-speciﬁc dish  jt according to the
distribution G0. Integrating out G0 next, the conditional distribution of  jt is
obtained as
 jtj 11;  12; : : : ;  21; : : : ;  jt1; ; H 
K
X
kD1
mk
m C  ık C

m C  H:
(2.4.30)
Table-speciﬁc dish  jt will take value k; k
D
1; : : : ; K with probability
mk= .m C / and will select with probability = .m C / a new dish k0 according
to the distribution H. To obtain samples of 
ji, proceed as follows. For each j and i,
ﬁrst sample 
ji using the ﬁrst expression. If a new sample from G0 is needed, then
use the second expression to obtain a new sample  jt and set 
ji D  jt. It should
be noted that in HDP, the values of the factors are shared within as well as between
groups. The above result describes marginals under a hierarchical DP when G0 and
Gj are integrated out.
For statistical inference, we need to sample the posterior distribution. For this
purpose, the marginal approach is used in which 
ji and  jt are generated using the
Gibbs sampling scheme. As indicated earlier, rather than generating them directly, it
is more efﬁcient (Neal 2000) to sample their index variables tji and kjt and k, from
which 
ji and  jt can be reconstructed. The following sampling scheme is provided
by the authors:

2.4
Dirichlet Mixture Models
69
1. Sampling t. The likelihood of yji is
p

yjijtji; tji D tnew; k

D
K
X
kD1
mk
m C  f
yji
k

yji

C

m C  f
yji
knew

yji

*
where f
yji
knew

yji

D
R
f

yjij

h ./ d is the prior density of yji.
Therefore the conditional distribution of tji is given by
P

tji D tjtji; k

_
(
nji
ji f
yji
kjt

yji

if t is previously used
Mp

yjijtji; tji D tnew; k

if
t D tnew:
If t D tnew, then we sample kjtnew according to *
P

kjtnew D kjt; kjtnew
_
 mkf
yjt
k

yjt

if k is previously used
f
yjt
knew

yjt

if
k D knew .
If njt D 0, then delete the corresponding kjt and by doing this if some mixture
component k becomes unallocated, then delete that mixture component as well.
2. Sampling k. Sample kjt according to:
P kjt D kjt; kjt _
 mkf
yjt
k

yjt

if k is previously used
f
yjt
knew

yjt

if
k D knew .
Two other computation methods and discussion of the relative merits of these
procedures can be found in their paper.
2.4.2.3
Nested Dirichlet Process
In HDP models, the information is shared across groups represented by distributions
G1; : : : ; GJ by sharing the atoms of the base measure G0 which allows us to
identify common clusters across groups. However, it does not reveal any potential
clustering of group distributions with similar characteristics among G1; : : : ; GJ.
Rodriguez et al. (2008) introduced the nested Dirichlet process (nDP) which allow
such clustering. Their motivation was that in multi-centric studies, different centers
may have different outcome distributions but also some clustering among mutually
similar centers may be possible. This is different from clustering of observations
within and across centers discussed by Teh et al. (2006) in their HDP model. One
can potentially cluster centers via having some common parametric distributions or
parameters, but that would be restrictive by the choice of the unknown distributions.
Rodriguez et al. offer an alternative which is more ﬂexible.
Like HDP, nDP is also a hierarchical model involving two levels of DPs: Gj
ind
Ï
D .M; G0/ with G0 assumed to be the DP, D .; H/. That is unlike in the HDP

70
2
Dirichlet and Related Processes
model, here the baseline measure G0 for the ﬁrst level DP is itself a DP instead
of an RPM drawn from it. Think of it as follows. A DP is a prior distribution or
probability measure say, Q; over the space of distributions or probability measures.
Now consider the class of all such Q0s and put a DP prior Q on the space of
Q0s, which is permissible since the original deﬁnition of DP is on any complete
separable metric space under the weak topology. This Q being a DP has a SB
representation, say P1
kD1 ˇkıG
k ; where G
k are members of Q-space and they being
DP measures themselves admit SB representation also. Just as DP is a distribution
on distributions, nDP can be viewed as a distribution on the space of distributions of
distributions, i.e., Gj  D .M; D .; H//. This does not create any problem. This
translates to replacing random atoms in the SB representation of the DP, with RPMs
drawn from a second DP.
Thus the proposed model may be stated as follows. Let yij, i D 1; : : : ; nj be the
observations for different subjects within center j D 1; : : : ; J. Assume subjects are
exchangeable within centers, with yij
iidÏ Fj, j D 1; : : : ; J. The goal is to borrow
information and allow clustering among distributions
˚
FjI j D 1; : : : ; J

. Thus let
fG1; : : : ; GJg be a collection of distributions such that Gj  D .M; D .; H//, and
let Fj .j/ D
R
‚ p .j
; '/ Gj .d
/, where p .j
; '/ is a distribution parametrized
by the ﬁnite dimensional vectors 
 and '. For example, 
D	; ' D 2 and
p .j
; '/ D N

j	; 2
yield a class that is dense on the space of absolutely
continuous distributions (Lo 1984) deﬁned on the real line. The proposed model
is
yij
iidÏ Fj .j/ D
Z
‚
p .j
; '/Gj .d
/ ; Gj  D

M; D .; H/

; j D 1; : : : ; J:
(2.4.31)
The collection fF1; : : : ; FJg is said to have an nDP mixture model. The deﬁnition of
nDP implies the following SB representation:
Gj 
1
X
kD1
ˇkıG
k ; j D 1; : : : ; J; G
k 
1
X
lD1
lkılk; k D 1; 2; : : :
ˇk Ï SBW

M
; lk Ï SBW ./ ; for each k D 1; 2; : : : and lk
iidÏ H:
(2.4.32)
The ﬁrst level of the hierarchy generates a distribution on random probability
measures with atoms corresponding to random distributions G
1; G
2 ; : : : , which are
taken to be nonparametric and are drawn from a common DP, D .; H/.
Since each G
k is a.s. discrete, some Gj and Gj0 will coincide with G
k for some
k; thus forming the cluster of centers. On the other hand, subjects i and i0 from
centers j and j0 will be clustered together if and only if Gj D Gj0 D G
k and 
ij D

i0j0 D lk for some l. Thus the model highlights clustering of similar distributions
at the ﬁrst level and then clustering of observations takes place at the second level
only across already clustered together distributions. In comparison with HDP it is

2.4
Dirichlet Mixture Models
71
noted that in HDP, one draw from a DP is used as the baseline measure G0. This
implies that fG1; : : : ; GJg share the same atoms (of G0/; but assign them different
weights. Therefore, P

Gj D Gj0
D 0 under HDP and clustering occurs only at the
level of the observations. Under nDP, clustering is induced on both observations
and distributions. This means that the different distributions have either the same
atoms with the same weights if they belong to the same cluster of distributions or
completely different atoms and weights if they don’t.
It also induces a random partition of a set of random distributions. Since a
priori P

Gj D Gj0jH

D
1
1CM > 0; the model induces clustering in the space
of distributions and thus creates a collection of dependent random distributions. For
A 2 A; Gj .A/ and Gj0 .A/ are random variables with correlation coefﬁcient between
them given by corr

Gj .A/ ; Gj0 .A/ jH

D
1
1CM and the correlation coefﬁcient
between two draws is
corr


ij; 
i0j0
D
(
1
1C if j D j
0
1
.1CM/.1C/ if j ¤ j
0:
(2.4.33)
It would be seen from this that as M ! 1, corrGj .A/ ; Gj0 .A/ jH is 0, each
distribution in the collection is assigned to a distinct atom. That is, distributions
become independent a priori. On the other hand, as M ! 0; the correlation goes
to 1 implying the a priori probability of assigning all of the distributions to the same
atom G
k goes to 1.
For computing posterior distributions, the authors follow the truncation approx-
imation method (Ishwaran and Zarepour 2000) in which they truncate the inﬁnite
sums in Gj and G
k to K and L; respectively. This method is closely related to the
method proposed by Ishwaran and James (2001, 2003). The main steps of their
computation algorithm are as follows:
1. Sample the center indicators &j (&j D k if Gj D G
k /, j D 1; : : : ; J from a
multinomial distribution with probabilities
P

&j D kj : : :

_ ˇk
n
Y
iD1
L
X
lD1
lkp

yijjlk; '

:
2. Sample the atom indicators ij (ij D l if 
ij D lk/, j D 1; : : : ; J and i D
1; : : : ; nj from another multinomial distribution with probabilities
P

ij D lj : : :

_ l;&jp

yijjl&j; '

:
3. Sample ˇk by generating its SB ratios from Be

1 C mk; M C PK
sDkC1 ms

, k D
1; : : : ; K  1, and setting the last component as one, where mk is the number of
distributions assigned to component k.

72
2
Dirichlet and Related Processes
4. Sample kl by generating its SB ratios from Be

1 C nlk;  C PL
sDlC1 nls

, l D
1; : : : ; L  1; and setting the last component as one, where nlk is the number of
observations assigned to atom l of distribution k.
5. Sample atoms lk from the posterior distribution
p .lkj : : :/ _
Y
fi;jj&jDk;ijDlg
p

yijjlk; '

h .lk/ ;
where h .lk/ is the density corresponding to the base measure H.
6. Sample ' from its full conditional distribution.
p .'j : : : :/ _
JY
jD1
nj
Y
iD1
p

yijjij;&j; '

p .'/ :
If the concentration parameters M and  are also unknown, they can be sampled
as well.
It is clear that nesting can not only be done with respect to DP only. (Jordan
2010) remarked that we can in a similar fashion deﬁne a nested beta process as
B  BeP.P1
kD1 ˇ
k ıB
k /, where B
k D P1
lD1 pklı!kl. This deﬁnes a random measure
B which is a collection of atoms, each of which is a beta process. This may be better
suited for document modeling.
2.4.2.4
Dynamic Mixture and Hierarchical Dirichlet Models
In practical applications often the data is collected in sequential manner over time
and it is assumed that a time evolution exists between adjacent data groups. To
incorporate this type of time-dependence, Dunson (2006), Ren et al. (2008), and
others have introduced what is known as dynamic models that is described now.
The data consists of J groups collected sequentially as fy1iI i D 1; : : : ; n1g,
: : : ; fyJiI i D 1; : : : ; nJg, each yji distributed independently according to a known
distribution F

j
ji

, 
ji
iidÏ Gj, j D 1; 2; : : : ; J. To account for dependency among
Gj’s, Dunson (2006) (see also Müller et al., 2004) proposed a Bayesian dynamic
mixture Dirichlet process (DMDP) in which Gj shares features with Gj1 but
additionally some innovation may also occur. He introduced this dependence by
modeling
Gj D .1  j1/Gj1 C j1Hj1 D wj1G1 C wj2H1 C    C wjjHj1;
(2.4.34)
where wjl
D
l1
Qj1
mDl .1  m/, l
D
1; : : : ; j are weights on the different
components in the mixture with 0 D 1 and Hl is the innovation distribution,
l D 1; : : : ; j  1 taken each to be a DP. As can be seen, this model randomly
reduces the probability attached to the atoms of Gj1 by a factor of .1  j1/

2.4
Dirichlet Mixture Models
73
and adding new atoms drawn from the nonatomic base distribution of Hj1. j1
controls the amount of the expected change from j1 to j; with j1 ! 0 signifying
no change. By assuming G1 to be drawn independently from D .M; G0/, where G0
is nonatomic, and Hl  D

M
l ; H0l

, l D 1; : : : ; j  1, Dunson derives a formula
for the correlation coefﬁcient between Gj and Gj1 and gives details of posterior
computation via MCMC algorithm.
This model has the drawback that mixture components can only be added over
time thus resulting in more components at later times. Also by assuming Hl 
D

M
l ; H0l

, one will end up in adding more and more atoms at each stage. Ren
et al. (2008) combined the dynamic modeling as well as hierarchical structure and
proposed an extension of the HDP model called the dynamic hierarchical Dirichlet
Process (dHDP) which has dynamic mixture model and HDP model each as a
special case. The rationale for this extension is as follows. The HDP model shares
“statistical strength” across different groups of data through sharing the same set
of atoms (may refer to as discrete parameters), but the mixing weights associated
with different atoms are varied. Furthermore, it is assumed that the data groups
are exchangeable. However in certain applications such as the above or seasonal
market data analysis, data is collected in a time sequential manner which justiﬁes
introduction of dynamic models to reﬂect this important temporal information. In
such a scenario, the group data exchangeability assumption is obviously no longer
valid. Therefore, they modiﬁed Dunson’s model as follows:
Gj D .1  wj1/Gj1 C wj1Hj1; j D 2; : : : ; J;
(2.4.35)
where G1

D .M; G0/, the innovation distribution Hj1 is drawn from
D

M
j ; G0

instead of D

M
j ; H0j

and wj1  Be

aw. j1/; bw. j1/

. This way,
Gj is modiﬁed from Gj1 by the introduction of a new Hj1 and the variable wj1
controls the probability of innovation.By choosing the prior for Hj1 as D

M
j ; G0

they eliminate the prospect of adding new atoms at every stage, as was the case in
Dunson’s model, and the same atoms of G0 are used but weights differ. Additionally
to ensure G0 is discrete, a hierarchy is introduced by assuming G0 to be drawn from
D .; H/.
The distribution G0 has SB representation P1
kD1 ˇkık, where k are the global
atoms drawn iid H and fˇkg1
kD1 are SBW ./ deﬁned as usual. Hence we can rewrite
G1 D P1
kD1 1kık, H1 D P1
kD1 2kık; : : : ; HJ1 D P1
kD1 Jkık, where the
weights j are independent given ˇ and are shown to be distributed in Teh et al.
(2006) as jjM
j ; ˇ  D

M
j ; ˇ

. When substituted in the above expression, Gj
can be written as
Gj D
j1
Y
lD1
.1  wl/G1 C
j1
X
lD1
(
j1
Y
mDlC1
.1  wm/!lHl
)
D wj1G1 C wj2H1 C    C wjjHj1;
(2.4.36)

74
2
Dirichlet and Related Processes
where wjl D wl1
Qj1
mDl .1  wm/, l D 1; : : : ; j with w0 D 1. For each wj D

wj1; : : : ; wjj

; Pj
lD1 wjl D 1. wj is the prior probability that the data in group j
will be drawn from the mixture distribution of G1; H1; : : : ; Hj1. If all wj D 0, then
all the groups share the same distribution G1 and the model reduces to a Dirichlet
mixture model. On the other hand, if all wj D 1, the model reduces to the HDP.
Therefore, this model is more general and encompasses both DP and HDP models.
The correlation coefﬁcient of the distributions between two successive groups is
computed as, for j D 2; ::; J,
Corr

Gj1; Gj

D
Pj1
lD1
wjlwj1l
1CM
l
 M
l CC1
C1
Pj
lD1
w2
jl
1CM
l  M
l CC1
C1
 1
2 Pj1
lD1
w2
j1l
1CM
l  M
l CC1
C1
 1
2
:
Similar to the alternate form of the HDPM expressed in terms of conditional
probabilities, dHDP can also be stated as
ˇj  SBW ./ ; jj.M
j ; ˇ/  D

M
j ; ˇ

; wjjawj; bwj  Be

awj; bwj

;
rjijwj  wj; zjij1Wj; rji  rji; kjH  H; yjij.zji; fkg1
kD1/ Ï F

jzji

;
(2.4.37)
where rji is a variable to indicate which mixture distribution is taken from 1Wj D
fkgj
kD1 to draw the observation yji and zji is the parameter component indicator.
A modiﬁed block Gibbs sampler (Ishwaran and James 2001) is used to carry out
the posterior computation as follows:
1. Generate wl from the updated conditional distribution
Be
0
@aw C
J
X
jDlC1
nj;lC1; bw C
J
X
jDlC1
l
X
hD1
njh
1
A ;
where njh D PNj
iD1 I

rji D h

. For simplicity, awj and bwj are taken as aw and bw,
respectively, for all j.
2. Generate lk from the updated conditional distribution
Be
 
M
l ˇk C A; M
l
 
1 
k
X
lD1
ˇl
!
C B
!
;
where
A D
J
X
jD1
Nj
X
iD1
I

rji D l; zji D k

; B D
J
X
jD1
Nj
X
iD1
K
X
k0DkC1
I

rji D l; zji D k0
:

2.4
Dirichlet Mixture Models
75
3. Update indicator variables rji and zji by generating samples from multinomial
distributions
P

rji D lj : : :

_ wl1
j1
Y
mDl
.1  wm/ lzji
zji1
Y
qD1

1  lq

:p

xjijzji

; l D 1; : : : ; J;
P

zji D kj : : :

_ rji;k
k1
Y
k0D1

1  rji;k0
p

xjijk

; k D 1; : : : ; K:
These posterior distributions are appropriately normalized—the ﬁrst such that
Pj
lD1 P

rji D l

D 1 and the second by a constant PK
kD1 P

zji D k

.
The remaining variables in the above expression are sampled in the same way as
in HDP model (Teh et al. 2006).
2.4.2.5
Time-Varying Dirichlet Process
Caron et al. (2007) introduce a class of time-varying Dirichlet process mixtures
which ensure that at each time point the random distribution follows a DPM model.
Thus the distribution of observations evolves over time instead of being ﬁxed.
Suppose that at each discrete time point t D 1; 2; : : :, we have a sample of n
observations denoted by yt D fyt1; : : : ; ytng which are iid samples from Ft ./ D
R
‚ f .j
/ dGt .
/, where f .j
/ is the pdf and Gt is a mixing distribution assumed
to be, Gt  D .M; G0/, and has the following inﬁnite sum representation Gt D
P1
kD1 ˇktıkt, where ˇkt’s are SBW.M/ ; and kt
iidÏ G0. That is, it is the following
hierarchical model:
GtjM; G0  D

M; G0

; 
ktjGt
iidÏ Gt; yktj
kt Ï f .j
kt/ :
(2.4.38)
We may also integrate out Gt and state
kt
iidÏ G0 and yktj
kt Ï f

j
kt

;
(2.4.39)
where 
kt are distinct values among kt’s.
This model is based on a simple generalized Polya urn model adopted by
changing the number and locations of clusters over time. However, they are
computationally difﬁcult to implement.
There are other dynamic models discussed in the literature.

76
2
Dirichlet and Related Processes
2.4.2.6
Probit Stick-Breaking Processes
If in the SB representation of an RPM P; we replace Vj  Be .1; M/ by Vj D
ˆ

vj

and vj  N

	; 2
, we say that P follows a probit SB process with baseline
distribution F0 and shape parameters 	 and , denoted by PSBP(	; ; F0/. That is,
the beta distribution of Vj is replaced by a probit model. PSBP has been discussed
by Rodriguez and Dunson (2011), nested DP by Rodriguez et al. (2008), and local
DP by Chung and Dunson (2011). The support of PSBP with respect to the topology
of pointwise convergence is the set of absolutely continuous measures with respect
to the baseline measure F0.
2.4.3
Some Further Generalizations
The Dirichlet process used in the above mixture and hierarchical models may be
replaced with its two-parameter generalization, PD .˛; 
/ to gain more ﬂexibility.
This is found to be useful in areas such as natural language modeling and image
processing. This aspect is pursued by Teh and Jordan (2010), where further material
may be found. Similarly the DP can be replaced with an RPM of the form (1.3.2)
encountered in species sampling models developed by Pitman (1995, 1996a; 1996b).
Since the DP is a normalized gamma CRM, it gives a clue to replace the same with
a normalized CRM. Mixture models with normalized random measure priors are
presented in Favaro and Teh (2013). In fact, there is a growing interest in recent
years in exploring mixture models in modeling large scale data sets as they offer
more ﬂexibility and at the same time are computationally becoming feasible.
Parallel to the HDP, Thibaux and Jordan (2007) proposed a hierarchical beta
process to be discussed in Sect. 4.6. In features modeling (Sect. 4.8), we have vectors
Zi’s of binary responses zik D 1 or 0, k D 1; 2; : : : according as whether the feature
is present or not in a subject. Thus, the vectors Zi’s are distributed as Bernoulli
processes with parameter B, and B is assumed to have a beta process prior with
parameters c and B0. In symbols, Zi
iidÏ BeP .B/ and B Ï BP.c; B0/. In modeling
documents by the set of words or phrases they contain, assume that the documents
are classiﬁed into K categories, A1; : : : ; AK and Zij associated with the jth document
in ith category, i D 1; : : : ; K, is a vector of binary responses with pi
! being the
probability (speciﬁc to the ith category) of feature ! (say a word) being present.
Then the hierarchical beta process model may be expressed as Zij Ï BeP .Bi/,
j D 1; : : : ; ni, Bi Ï BP.ci; B/; i D 1; : : : ; K and B Ï BP.c; B0/; subject to certain
conditions on the variables and parameters involved.
Remark 2.19 Alternative to hierarchical modeling, empirical Bayes approach also
offers some advantage. Instead of putting a prior on the baseline distribution F0, it
may be estimated from the (past) data itself. This is done in the empirical Bayes
approach in various applications discussed in Chaps. 6 and 7. This approach has
some merit over the hierarchical modeling methods in the sense that the data

2.5
Some Related Dirichlet Processes
77
itself guides the value(s) of unknown parameter(s) as opposed to assuming certain
arbitrary priors. In the empirical Bayes applications of the Dirichlet process, M and
F0 were estimated consistently using the past data and the analysis proceeded. The
author is unaware if any attempts have been made in estimating the parameters of
PD .˛; 
/.
In the kernel mixtures, usually the normal distribution with mean 0 and variance
2 is the preferred choice of the kernel. But this may be extended to include other
types of functions. James (2006) explores this path in his paper.
2.5
Some Related Dirichlet Processes
In applications, some variants of the DP are introduced in the literature. Brieﬂy, they
are as follows.
Dirichlet-Multinomial Process
In the context of Bayesian inference for sampling from a ﬁnite population, Lo
(1986) deﬁnes a ﬁnite dimensional process. Assume that F Ï D.˛/ and given F,
X1; : : : ; XN is an iid sample from F. The marginal distribution of X1; : : : ; XN is then
symmetric and is a function of N and ˛. A point process N ./ D PN
jD1ıXj ./ deﬁned
on .R; B/ is called a Dirichlet -multinomial process with parameters .N; ˛/ if for
any k and any partition .B1; : : : ; Bk/ of R, the random vector .N .B1/ ; : : : ; N .Bk//
is a Dirichlet-multinomial with parameters .NI ˛ .B1/ ; : : : ; ˛ .Bk//. Alternatively it
may also be stated as follows. Let F0 be a distribution function, M > 0 a real
number, and N a positive integer. A point process N ./ on .R; B/ is said to be
a Dirichlet -multinomial process with parameters .M; F0; N/ if for any k and any
partition .B1; : : : ; Bk/ of R, the random vector .N .B1/ ; : : : ; N .Bk// ; given F has
a conditional multinomial distribution with parameters .NI F .B1/ ; : : : ; F .Bk// and
F Ï D.˛/ on R, with ˛ .R/ D M and F0 ./ D ˛ ./ =M. He shows that this process is
conjugate and the Dirichlet process is the limit of the Dirichlet -multinomial process
as N ! 1. It also appears in connection with Bayesian bootstrap (Lo 1987).
Dirichlet Multivariate Process
In the process of dealing with nonparametric Bayesian estimation in a competing
risks model, Salinas-Torres et al. (2002) introduced a multivariate version of the
Dirichlet process called Dirichlet Multivariate process as follows.

78
2
Dirichlet and Related Processes
Let .X;A/ be a measurable space and ˛1; : : : ; ˛k be ﬁnite, non-null, and
nonnegative measures deﬁned on .X; A/. Let  D .1; : : : ; k/, P1; : : : ; Pk be
mutually independent random elements deﬁned on a suitable probability space.
Suppose further that  has a singular Dirichlet distribution D .˛1 .X/ ; : : : ; ˛k .X//,
and Pj Ï D ˛j
, j D 1; : : : ; k. Set P D P
1; : : : ; P
k
 D .1P1; : : : ; kPk/.
Then P is said to be a Dirichlet multivariate (k-variate) process with parameter
˛ D .˛1; : : : ; ˛k/. Also, P
1; : : : ; P
k are subprobability measures and Pk
jD1P
j is a
probability measure on .X; A/. They show that the posterior distribution is obtained
simply by updating the parameters of the priors. They also derive some weak
convergence results of P and use it in deriving nonparametric Bayesian estimators
in competing risks models (to be described in Sect. 7.5.3).
Generalized Dirichlet Process
Hjort (1990) deﬁnes a prior for the distribution function F via its cumulative hazard
function H. Let H be a beta process prior with parameters c./ and H0./, that is
symbolically, H  Befc./; H0./g, and consider the random CDF F.t/ D 1…
Œ0;tf1
dH.s/g. Then EŒF.t/ D F0 .t/ D 1 …
Œ0;tf1dH0.s/g as shown in Sect. 4.5. It is then
noted in Hjort (1990) that F is a Dirichlet process with parameter F0 ./ and  is a
positive constant chosen so that c.s/ D F0Œs; 1/. Thus in this case F is identiﬁed
as a generalized Dirichlet process with two parameters, c./ and F0./. This fact is
the key idea in developing the beta-Stacy (Walker and Muliere 1997a) process. Hjort
also notes that if H  Befc./; H0./g, then B D  log .1  F/ is a Lévy process
(independent nonnegative increments process) with Lévy representation
E.e
B.t// D . …
jWtjtE.1  Sj/
/ exp


Z 1
0
f .c.s/ C 
/   .c.s//g

c.s/dH0c.s/;
(2.5.1)
where  .x/ is the digamma function 
0 .x/ = .x/. Various properties of the
Dirichlet process developed in Sect. 2.1 and applications may be investigated for
the generalized Dirichlet process as well.
Bernstein–Dirichlet Prior
Petrone (1999) introduces a class of prior distributions on the space FŒ0; 1 of
distribution functions F deﬁned on the closed interval Œ0; 1. This is done via
constructing a Bernstein polynomial. Given a function F on the closed interval Œ0; 1,

2.5
Some Related Dirichlet Processes
79
a Bernstein polynomial of order k of F is deﬁned as
B.xI k; F/ D
k
X
jD0
F
	 j
k

  
k
j
!
xj .1  x/kj :
(2.5.2)
If F is a random distribution function on Œ0; 1, and k is also random, then clearly,
so is the polynomial B.xI k; F/. As k ! 1, B.xI k; F/ ! F .x/ at each point
of continuity x of F. Its derivative can be written as a mixture of beta densities
Pk
jD1wj;kBe.x W j; k  j  1/ where
wj;k D F
	 j
k

 F
	j  1
k

; j D 1; 2; : : : ; k; F .0/ D 0;
(2.5.3)
wj;k  0 and Pk
jD1wj;k D 1. The mixture is a probability density. By randomizing k
and the weights wj;k of the mixture, a prior on the space of densities on Œ0; 1 can be
constructed. A probability measure induced by B is called a Bernstein prior and its
construction is described. For example, if k and F are dependent, given k, F can be
chosen as a Dirichlet process prior with parameter ˛k. If they are independent, then
a joint distribution for the pair .k; F/ may be assigned on the corresponding product
space. If in this latter case, F Ï D .˛/ ; she calls such a prior as Bernstein–Dirichlet
prior. It is shown to have full support and it can also select an absolutely continuous
distribution function with a continuous and smooth derivative.

Chapter 3
Ferguson–Sethuraman Processes
3.1
Introduction
In this chapter, we describe brieﬂy several new processes which have their origin
in Ferguson (1973) and Sethuraman (1994) countable sum mixture representations
of the Dirichlet process. Some of them have garnered signiﬁcant interest in
many ﬁelds outside the mainstream statistics including machine learning, ecology,
population genetics, document classiﬁcation, etc. Thus it is safe to say that they have
revolutionized the nonparametric Bayesian approach to modeling and statistical
inference.
Ferguson’s formal deﬁnition of the DP, while elegant and concise had limited
applicability. Given a random sample, the posterior is also a DP and all we had to do
was to update the parameter. However, if the sample was distorted or right censored,
the posterior was no longer a DP but a mixture of DPs as noted earlier. Similarly,
it does not give any clue as to how to perform Bayesian analysis if we wanted to
incorporate covariates, or have spatial data, or are dealing with complex mixture
models for which the posterior does not have any simple form. His alternative
deﬁnition deﬁned the DP as a countable mixture of point masses where the weights
were derived via normalized increments of a gamma process. But since it required
evaluation of an inﬁnite sum, it was not conducive for practical applications.
Sethuraman (1994) showed that the weights can be constructed without having
to evaluate an inﬁnite sum via the stick-breaking process involving a series of
beta random variables and showed that the two representations are equivalent in
distribution:
P./ D
1
X
jD1
pjıj./
dD
1
X
jD1
Pjıj./;
(3.1.1)
© Springer International Publishing Switzerland 2016
E.G. Phadia, Prior Processes and Their Applications, Springer Series in Statistics,
DOI 10.1007/978-3-319-32789-1_3
81

82
3
Ferguson–Sethuraman Processes
where Pj’s and pj’s are weights in Ferguson and Sethuraman representations,
respectively, deﬁned earlier in Sect. 2.1.1, independent of i
iid H. This remarkable
result has had far reaching impact on Bayesian analysis of complex models as
will be seen later in this chapter. However, the inﬁnite sum poses a problem in
applications. Ishwaran and Zarepour (2000) provided a partial solution to this
problem by truncating the sum to a ﬁnite number of terms N as an approximation
to the DP, which makes sense since the weights pj’s decay rapidly. This idea of
truncation was cast into a broader formulation by Ishwaran and James (2001) who
deﬁne a larger class of priors, PN .a; b/ D PN
jD1 p
j ıj./; called stick-breaking
priors, where N could be ﬁnite or inﬁnite, and the weights constructed via the stick-
breaking construction, but with one difference—the stick-breaking ratios Vj were
taken to be independent beta random variables with parameters ai  0 and bi  0.
When N is ﬁnite, VN is taken to be one so that PN
jD1 p
j D 1. Besides the DP, this
class includes some well-known processes to be described later.
Ferguson and Sethuraman representations provide a clue to consider a more gen-
eral class of random probability measures (RPMs). Let P be a random probability
measure deﬁned on .X; A/ as
P./ D
1
X
jD1
pjıj./; pj  0; j D 1; 2; : : : ;
1
X
jD1
pj D 1;
(3.1.2)
a countable mixture of unit masses placed at (random) points 1; 2; : : : with random
weights, p1; p2; : : : ; which need not be constructed according to the stick-breaking
construction (Ferguson’s weights were not). We call P a Ferguson–Sethuraman
process. (Note that we could replace 1 in the above deﬁnition by N, 1  N  1
as in the class PN; but except for some ﬁnite dimensional priors, most of the
well-known priors anyway are deﬁned when N D 1. Also, the mass at j need
not be restricted to unity (with corresponding rebalance of weights), but such
generalizations may not have much utility in practice. Therefore it seems hardly
worthwhile to consider such generality.) Discrete random measures may also be
deﬁned with the condition P1
jD1pj < 1.
The phrase, stick-breaking is generally used in connection with the construction
of weights and goes back to several authors cited by Ishwaran and James, and is used
to signify the method of construction of the weights. However, to include random
discrete probability distributions (j are not random) and vectors p D . p1; p2; : : :/ of
proportions (of species in a population), or normalized increments of independent
increment processes (say a gamma process), where the p’s need not be constructed
using the stick-breaking construction and yet have the above representation, I
enlarge the family and call it as Ferguson–Sethuramanprocesses, and save the stick-
breaking phrase to indicate the method of construction. This terminology seems to
me as more appropriate. The remarkable feature is that it not only encompasses
the PN .a; b/ family but also includes many more additional processes. They all
are discrete and some have SB construction representation as well. The manner in

3.1
Introduction
83
which the weights are assigned or constructed and locations are deﬁned, determine
different processes, as will be seen in this chapter.
A Ferguson–Sethuraman process has four essential ingredients: inﬁnite sum;
random weights pj’s; (random) locations i’s; and unit masses at i’s. By varying
these ingredients, various distributions have emerged to serve as priors. In fact,
most of the prior processes currently deﬁned in the literature belong to this family.
They include a class of discrete priors such as the Dirichlet-multivariate process;
multi-parameter priors such as the beta two-parameter process and two-parameter
Poisson–Dirichlet process; covariate processes such as the Dependent Dirichlet
processes; and hierarchical and mixture processes such as the Kernel based stick-
breaking processes. These and other processes are the subject matter of this chapter.
Two immediate types of Ferguson–Sethuraman processes are apparent. First,
the sum in the above representation could be truncated at a positive integer N;
and second, the weights pi’s may be constructed using distributions that are more
general than the one-parameter beta distribution used in Sethuraman representation.
By truncating the sum to a ﬁnite number of terms, it yields a class of discrete
distribution priors (see, for example, Ishwaran and James 2001; Ongaro and
Cattaneo 2004) and provides a way to approximate the Dirichlet process for
implementing MCMC algorithm in generating a sample from the Dirichlet process.
This in turn has facilitated analysis of complex hierarchical and mixture models in
practice.
Recall that Ferguson’s countable mixture representation uses normalized incre-
ments of the gamma process to construct the weights pj’s. This could be generalized
further by replacing them with normalized increments of completely random
measures (see, for example, Regazzini et al. 2003; Favaro and Teh 2013). On the
other hand, the Sethuraman representation is based on a series of beta Be .1; M/
random variables. This suggests a natural generalization by taking a more ﬂexible
distribution of stick-breaking ratios Vi; the beta distribution with parameters ai and
bi, i.e., Vi Ï Be .ai; bi/ ; ai > 0; bi > 0; i D 1; 2; : : :. By varying the values of
ai and bi; Ishwaran and James (2001) have shown that a large class of priors can
thus be constructed which includes some well-known processes such as one- and
two-parameter Poisson–Dirichlet and stable processes. However, except for some
special cases, there does not seem to be any meaningful interpretation of these
parameters. During the last few years, Sethuraman’s construction has further turned
up in some unexpected prior processes in the ﬁelds outside statistics. They include
the Chinese restaurant process (CRP) and the Indian buffet process (IBP) (Grifﬁths
and Ghahramani 2006).
The third and fourth types of Ferguson–Sethuraman processes involve the iid
locations i’s and the unit mass attached to each of them. By making the locations
j depend upon covariates, MacEachern (1999) has shown that it is possible to carry
out the Bayesian analysis of regression type models. He calls the resulting process a
Dependent Dirichlet process. Several authors have further generalized this approach
by making j and/or pj dependent upon auxiliary information and proposed spatial
Dirichlet process (Gelfand et al. 2005), generalized spatial Dirichlet process (Duan
et al. 2007), order-based dependent Dirichlet process (Grifﬁn and Steel 2006),

84
3
Ferguson–Sethuraman Processes
multivariate spatial Dirichlet process (Reich and Fuentes 2007), and latent stick-
breaking process (Rodriguez et al. 2010), to name a few. Finally, by replacing
the unit mass at j with a ﬁnite nondegenerate measure, Dunson and Park (2008)
introduced a kernel based Dirichlet process and have shown that it is feasible to
carry out the Bayesian analysis of more complex models.
Many of the above generalizations may be considered as special cases of a
discrete RPM studied by Pitman (1996a,b) to model species sampling problems in
ecology and population genetics, where the data arise from a discrete distribution.
His species sampling model is deﬁned as
P./ D
1
X
jD1
pjıj./ C
0
@1 
1
X
jD1
pj
1
A Q ./ ,
(3.1.3)
where Q is a probability measure corresponding to a nonatomic distribution H; j
iidÏ
H; and the weights pj’s are constrained by the conditions, pj  0 and P1
jD1 pj  1.
In the context of population genetics, it is considered that a population consists of
an inﬁnite number of species identiﬁed by 1;2; : : : ; and pj represent the proportion
of the j-th species encountered in a sample drawn from a large population. Clearly,
if P1
jD1 pj D 1; it reduces to the model (2.1.5). However, this model is not popular
in mainstream statistics.
The weights pj’s have some interesting features. Ferguson’s weights were
constructed using a gamma process and they were in decreasing order, p1 > p2 >
   . Sethuraman weights were constructed using beta random variables and they
need not be in any particular order. On the other hand, if p D . p1; p2; : : :/ is viewed
as a vector of probabilities, the joint distribution of pi’s or of p determined by beta
random variables Vi’s is known as the GEM distribution named after McCloskey
(1965), Engen (1978), and Grifﬁths (1980) who introduced it in the context of
ecology and population genetics (see Johnson et al. 1997). Also, when interpreted
as a probability model in ecology, it is known as Engen’s (1975) model. The
distribution of ranked permutation p D

p.1/; p.2/; : : :

of p is the Poisson–Dirichlet
distribution (Kingman 1975). Further connections of these weights are discussed
below.
The manner in which the four ingredients are speciﬁed determine the type
of Ferguson–Sethuraman process generated. In this chapter various processes as
identiﬁed by their originators are described. The selection of these processes was
guided by their novelty, variety, and breadth, and not by their importance. Also the
discussion is limited to a general introduction suggesting further consultation to the
relevant papers for details, computational methods, and illustrative examples. The
material of this chapter is broadly grouped under the headings:
(a) Discrete and ﬁnite dimensional priors
(b) Dependent Dirichlet Processes
(c) Poisson–Dirichlet processes
(d) Species sampling models

3.2
Discrete and Finite Dimensional Priors
85
3.2
Discrete and Finite Dimensional Priors
3.2.1
Stick-Breaking Priors PN .a; b/
Motivated by the Sethuraman representation and construction of two-parameter
Poisson–Dirichlet process, Ishwaran and James (2001) deﬁne a broad class of priors,
PN .a; b/ ; called stick-breaking (SB) priors, as follows. An RPM P is said to a stick-
breaking prior if it has the following a.s. representation:
P./ D
N
X
jD1
pjıj./; 1  N  1;
(3.2.1)
where
p1 D V1; pi D Vi
i1
Y
jD1
.1  Vj/; i  2;
Vi
ind
 Be .ai; bi/ ; i D 1; 2; : : : ; .a; b/ D .a1; a2; : : : ; b1; b2; : : :/ ;
(3.2.2)
and independent of Vi’s, i
iid H; H nonatomic. It is a special case of Pitman’s
species sampling model (3.1.3), the weights being constructed via the SB con-
struction. In order that the family PN .a; b/ is well deﬁned, we have to ensure that
PN
jD1 pj D 1 a.s. In the case of N < 1; .a; b/ D .a1; : : : ; aN1; b1; : : : ; bN1/ and
VN must necessarily be 1. To check that it is well deﬁned in the case of N D 1;
Ishwaran and James (2001) provide the following condition that must be satisﬁed.
1
X
kD1
pk D 1 a.s. iff
1
X
kD1
E .log .1  Vk// D 1 or equivalently
1
X
kD1
log
	
1 C ak
bk

D 1:
(3.2.3)
The expectation and variance of P can easily we calculated, based on indepen-
dence of Vi; and are as follows (Grifﬁn and Steel 2006).
E .PN.B// D E
0
@
N
X
jD1
pj
1
A E

ıj.B/

D H .B/ ; B 2 A
Var .PN.B// D H .B/ .1  H .B//
N
X
iD1
ai .ai C 1/
.ai C bi/ .ai C bi C 1/
Y
j<i
bj

bj C 1


aj C bj
 
aj C bj C 1
:
If we assume ai D 1 and bi D M for all i; and N D 1; we recover the variance
formula for the Dirichlet process: Var .PN.B// D H .B/ .1  H .B// = .M C 1/. The

86
3
Ferguson–Sethuraman Processes
stick-breaking priors have been used in certain models, such as order-based SB
processes, spatial processes, and latent SB processes, discussed later in the chapter.
As stated earlier, by varying the parameters of the beta distribution leads to the
following processes. A two-parameter generalization is introduced by Ishwaran and
Zarepour (2000) by replacing the distribution Be.1; ˛/ of Vi in the Sethuraman
representation of the DP, with Be.a; b/; a > 0; b > 0; that is, if ai D a and bi D b
for all i  1. They call it a beta two-parameter process. It is discussed in connection
with the approximation of the DP by truncating the inﬁnite sum (3.1.2) to a ﬁnite
N terms in order to implement MCMC algorithm in ﬁtting certain nonparametric
hierarchical and mixture models. They also suggest that this process may be suitable
for ﬁnite mixture modeling since the number of distinct sample values tend to match
with the number of mixture components. On the other hand, if we set ai D 1  ˛
and bi D 
 C i˛; with 0  ˛ < 1; 
 > ˛, it gives rise to a two-parameter
Poisson–Dirichlet distribution denoted as PD.˛; 
/ (Pitman and Yor 1997). By
setting ˛ D 0 and 
 D 	 in PD.˛; 
/; the Dirichlet process D .	/ is recovered;
PD.˛; 0/ yields a stable-law process; and PD.0; 
/ yields the (one parameter)
Poisson–Dirichlet distribution (Kingman 1975). One- and two-parameter Poisson–
Dirichlet distributions are discussed in greater detail later as they have emerged as
useful distributions with different applications. Other than these special cases, any
meaningful interpretation of parameters .ai; bi/ ; i D 1; 2; : : : of PN .a; b/ is not
clear if not difﬁcult.
Similar to the Polya urn characterization of the DP, Polya urn characterization
of the family P1 .a; b/ may be obtained as follows. Suppose we have a sample of
size n from the above process, that is, 
ijP
iidÏ P, i D 1; 2; : : : ; n; P Ï P1 .a; b/.
In addition, assume that i
iidÏ H, H nonatomic. The sample will have some ties. Let


1 ; : : : ; 

k be the k distinct observations among the sample and n1; : : : ; nk be their
multiplicities, respectively, so that n1 C  Cnk D n. Integrating out P; the marginal
distribution of 
i’s will satisfy the following prediction rule (Pitman 1995)

nC1j
1; : : : ; 
n Ï
k
X
iD1
ni  a
b C n ı

i C b C ka
b C n H:
(3.2.4)
From this it is clear (Pitman 1995, 1996a,b) that the process can be characterized
in terms of a generalized Polya urn scheme. Given 
1; 
2; : : : ; 
n; choose 
nC1
at the .n C 1/-th step to be a new observation drawn from H with probabil-
ity .b C ka/ = .b C n/ and equal to a previous observation 

i
with probability
.ni  a/ = .b C n/ ; i D 1; 2; : : : ; k. Its special case, a D 0 and b D ˛ ./
corresponds to the Dirichlet process with parameter ˛ and yields the Blackwell and
MacQueen (1973) predictive rule.

3.2
Discrete and Finite Dimensional Priors
87
3.2.2
Finite Dimensional Dirichlet Priors
Alternate to the SB priors, a class of ﬁnite dimensional priors may be deﬁned by
truncating the sum in (3.1.2) above to a ﬁnite N < 1; i.e., as P./ D PN
jD1 pjıj./;
where the weights need not be constructed via SB construction. If the random
weights are deﬁned as above, then the vector pN D . p1; : : : ; pN/ has generalized
Dirichlet distribution GD .a; b/ ; with density given by
 N1
Y
iD1
 .ai C bi/
 .ai/  .bi/
! N1
Y
jD1
p
aj1
j
pbN11
N

N2
Y
jD1
 
1 
j
X
kD1
pk
!bj.ajC1CbjC1/
;
(3.2.5)
and it constitutes a conjugate prior to multinomial sampling. However if the
random weights are deﬁned as having a Dirichlet distribution, pN Ï D .a1; : : : ; aN/ ;
it can also be represented as having GD .a; b/ with a D .a1; : : : ; aN1/ and
b D
PN
kD2ak; PN
kD3ak; : : : ; aN

. Thus P deﬁnes a class of priors named as ﬁnite
dimensional Dirichlet priors by Ishwaran and Zarepour (2000). In particular, if
the parameters of the Dirichlet distribution taken as a1 D    D aN D ˛=N for
some ˛ > 0 (symmetric Dirichlet distribution), then a D .˛=N; : : : ; ˛=N/. P is
called a ﬁnite dimensional Dirichlet prior with parameter ˛. This symmetric prior
has been used by other authors (for example, Kingman 1975 and Patil and Taillie
1977) in constructing prior distributions. The ﬁnite dimensional priors may also be
constructed by truncating the inﬁnite sum at N and discarding all terms beyond N
and setting pN D 1  PN1
kD1 pk.
Another example of ﬁnite dimensional Dirichlet priors is where the weights
are constructed using the gamma random variables. Let pj D Yj=Y; where Yj
iidÏ
G .˛=N; 1/ ; j D 1; : : : ; N; Y D PN
kD1Yk, and independent of j, j
iidÏ H; and deﬁne
PN ./ D
N
X
kD1
Yk
Y ık ./ :
(3.2.6)
It is a good approximation of the D .˛H/ since for a ﬁxed value of D .1; : : : ; N/ ;
PNj  D .˛F0 .; // ; where F0 .; / D
1
N
PN
kD1 ık ./ is the empirical measure
of 1; : : : ; N. It can also be expressed as a mixture of DPs because F0 .; / is a
random measure. Conditioning on ; and then integrating we see that
PN ./
dD
Z
  
Z
D .˛F0 .; // dH .1/    dH .N/ :
In fact Ishwaran and Zarepour (2000) prove a stronger result that for any real
valued measurable function g; PN .g/
D! P .g/ ; where P D D .˛H/. An equivalent

88
3
Ferguson–Sethuraman Processes
representation of PN is
PN ./ D
N
X
kD1
Yk
Y ık ./
DD
N
X
jD1
(
Vj
j1
Y
iD1
.1  Vi/
)
ıj ./
(3.2.7)
where Vj
ind
 Be .1 C ˛=N; ˛ .1  j=N// for j < N and VN D 1.
If we take a D ˛=N, N  n and b D ˛ > 0 in (3.2.4), it would yield

nC1j
1; : : : ; 
n Ï
k
X
iD1
ni C ˛=N
b C n
ı

i C ˛

1  k
N

b C n
H;
(3.2.8)
and would represent a sample from PN, an example of PN .a; b/.
Muliere and Tardella (1998) introduced an approximation to the DP by introduc-
ing what they call -Dirichlet Process for which, given  > 0, they terminate the
inﬁnite sum at N deﬁned as N D inf
n
n 2 N W Pn
jD1 pj  1  
o
. Now the RPM is
deﬁned as
P./ D
N
X
jD1
pjıj./ C
0
@1 
N
X
jD1
pj
1
A ıNC1./:
That is the remaining weight is placed on the .N C 1/-th atom. An alternative is to
distribute the remaining weight on all N atoms. They show that
SupA2A fjP .A/  P.A/jg  :
Thus as  ! 0; -DP converges to the usual DP. They also show that N 
Poisson .M log /. Rather than truncate the inﬁnite sum in Sethuraman represen-
tation by a ﬁnite N as in Ishwaran and Zarepour (2000, 2003), these authors let it
guide by the amount of closeness from a particular distribution of interest is desired
in advance.
Finite dimensional Dirichlet priors serve as an approximation to D .˛/ and is
used in computation of posterior distributions. Finite dimensional Dirichlet pro-
cesses, such as Dirichlet-multinomial and Dirichlet-multivariate, were introduced
towards the end of last chapter. They were deﬁned via appropriate modiﬁcation of
the basic deﬁnition of the DP. Here they are deﬁned via truncation of the inﬁnite
sum representation.
Blocked Gibbs sampler discussed in Sect. 2.4 can be applied for posterior
computations in a mixture model (2.4.1) under the prior PN .a; b/ with minor
changes. In this efforts an interesting observation of Sethuraman (see Paisley et al.
2010) could prove a boon. He has shown that one could sample  Ï Be.a; b/

3.2
Discrete and Finite Dimensional Priors
89
according to the following stick-breaking construction:
 D
1
X
iD1
Vi
i1
Y
jD1

1  Vj

I ŒYi D 1
(3.2.9)
with Vi
iid Beta .1; a C b/ and Yi
iid Bernoulli .a= .a C b// and where I Œ is the
indicator function.
3.2.3
Discrete Prior Distributions
It is clear that the integer N itself may be considered as a discrete random variable
having a speciﬁc distribution. In this case, Ongaro and Cattaneo (2004) present a
unifying general class of discrete prior distributions …d as follows. Let H be a
nonatomic probability measure on .X;A/. An RPM P belongs to this class …d if it
can be represented as P D PN
iD1piıi where pi; and i are independent, pi having
a speciﬁed distribution, i
iidÏ H and N being an extended value positive integer
or a random variable. As a particular case, given N they take the vector pN D
. p1; : : : ; pN/ distributed as an arbitrary distribution on the .N  1/-dimensional
simplex
SN D
(
pNW
N
X
iD1
pi D 1; pi  0; i D 1; : : : ; N
)
:
They prove some of the properties which are similar to the ones that hold for the
Dirichlet process. However, it does not satisfy the conjugacy property and therefore
to include the posterior distributions, they show how to create an enlarged family
embedding the class of priors.
3.2.4
Residual Allocation Models
The weights in the ﬁnite case, when viewed as a vector of proportions
pND . p1; : : : ; pN/, have been found quite useful in ecology (see, for example,
Patil and Taillie 1977). One particular model involving these weights seems to
appear very frequently. It is called the residual allocation model (RAM) which is
also known as the stick-breaking model. Let pN be as above and deﬁne the residual

90
3
Ferguson–Sethuraman Processes
fractions as follows:
v1 D p1; v2 D
p2
1  p1
; v3 D
p3
1  p1  p2
; : : : ; vN D
pN
1  p1      pN1
D 1:
(3.2.10)
A random probability pN is said to be a RAM if v1; : : : ; vN1 have independent dis-
tributions with P .0 < vi < 1/ D 1, i D 1; 2; : : : N  1 and P .vN D 1/ D 1. In the
case N D 1; it is necessary to have P .limn!1 .1  p1  p2      pn/ D 0/ D 1.
Examples of RAM are (Patil and Taillie 1977)
1. The symmetric Dirichlet distribution with parameters N and ˛ > 0 is an obvious
RAM with vi Ï Be .˛; .N  i/ ˛/.
2. Engen’s (1975) model is also a RAM with vi
iidÏ Be .1; ˛/, ˛ > 0.
3. Size-biased permutation (deﬁned later) of pN having a symmetric Dirichlet
distribution with parameter ˛ is another RAM with vi Ï Be .˛ C 1; .N  i/ ˛/.
Finite sum representation is also used as a tool to approximate the Dirichlet
process in carrying out computational algorithms.
3.3
Dependent Dirichlet Processes
As is well known, the DPM models are one of the most versatile tools in modeling
data. They are a generalization of ﬁnite mixture models to the case which allow
inﬁnite number of mixture components. One of the assumption is that each sample
is generated independently from the same DP. However in practice this assumption
may be called into question or considered to be an unrealistic constraint as the
samples may come from different DPs. HDP provides a partial solution by assuming
them to be drawn independently from a common super (parent) DP. However this
is not sufﬁcient when dealing with sequential or time-varying data, where RPMs at
contiguous points of observation are assumed to be related. This and other reasons
prompt us to consider a family F of nonparametric RPMs or random distribution
functions, Fx; deﬁned on a common measurable space .X;A/, and indexed by x 2 
for some space ; such that they are mutually dependent. It will be denoted as
F D fFx W x 2 g. It is recognized as a stochastic process deﬁned on the space of
probability measures over the domain . This makes it possible to share information
among Fx, across x.
There is an advantage to treat RPMs Fx collectively in making inference. For
example, while dealing with data which consist of subgroups, it is reasonable to
assume that each subgroup has a distinct but related RPM, say the distributions of
treatment outcomes in related health facilities in a multicenter clinical trial. In this
case  has ﬁnite number of elements. In sequential time-varying data,  may have
countably inﬁnite number of values ft1; t2; : : :g. In regression and random effect
models,  would be a set of covariates; and in spatial setting  may be a subset of

3.3
Dependent Dirichlet Processes
91
d-dimensional Euclidean space, Rd. Models based on such collection of RPMs are
collectively termed as dependent nonparametric models. They offer a compromise
between two extreme choices: one in which all Fx are assumed to be identical and
two, all are distinct and independent.
Earlier reference to dependent Dirichlet processes (DDP) may be found in
Cifarelli and Regazzini (1979) and Muliere and Petrone
(1993) who deﬁned
dependent nonparametric models across related RPMs; by introducing a regression
for the base measure of marginal DP distributed RPM Fx, i.e., Fx  D .M; F0x/.
Muliere and Petrone assumed a speciﬁc form for F0x; F0x D N

ˇx; 2
.
MacEachern (1999) introduced a general concept to deﬁne DDP as an extension
of the DP model to the class of dependent RPMs, F. His motivation was that the
traditional linear models are inappropriate for several reasons. One, the conditional
distribution of response variable, given a particular explanatory variable need not
follow a parametric distribution. Two, the usual assumption of error term following
a normal distribution is not always justiﬁable and excludes distributions with other
potential shapes. Compounding the problem is the assumption that it is independent
of the explanatory variables. Consequently, he makes a case for a nonparametric
approach in accommodating covariates with the error term distribution evolving
with the changes in explanatory variables. Recall that in the context of survival
data analysis, the most popular nonparametric approach to incorporate covariates
is through the Cox model, and its Bayesian analog was introduced in Kalbﬂeisch
(1978) who assumed a gamma process prior for the baseline distribution and
averaged it out in estimating the regression parameters (see Sect. 4.3).
In Bayesian approach to such modeling, it is desired to put a prior on F which
allows us to borrow strength across index x, such as learning across various studies.
It represents a generalization of placing a prior on a single member Fx. Learning
across studies can also be achieved by employing hierarchical models, or in a
hierarchical setup for group data, where the RPM for each subgroup is assumed
to have been drawn from a single parent prior. However the present approach
provides far greater ﬂexibility. A subclass of models are termed as dependent
Dirichlet processes (DDPs) when marginally each Fx is assumed to have a DP prior.
Exception to this includes models when Fx may have other priors such as SB process
priors or beta process priors.
Among the desirable properties of a prior on F, indicated succinctly in Chung
and Dunson (2011), are (i) dependence should increase between Fx and Fx0, as
x and x0 gets closer; (ii) expressions for the mean, variance for each Fx and
covariance should be simple and easily interpretable; (iii) Fx marginally should be
a DP; and (iv) posterior computation should be able to be carried out. Most of the
models presented here achieve these properties, although (iv) is usually difﬁcult to
implement. However, adequate simulation procedures are available to address this
challenge.
MacEachern’s approach is to use the Sethuraman representation of a DP to
accommodate covariate, x 2 ; where   Rd is known as the covariate space.

92
3
Ferguson–Sethuraman Processes
Recall that under this representation, an RPM F can be expressed as F D P1
jD1 pjıj,
where the location atoms 0
js and weights pj’s are deﬁned as usual with mass
parameter M; and base measure F0 deﬁned on .X; A/. The key idea behind his
approach is to introduce dependence across Fx by assuming the distribution of
point masses dependent across levels of x besides being independent across j.
Accordingly, we replace each j by jx; and deﬁne Fx D P1
jD1 pjıjx. That is, each j
is replaced by a realization of the stochastic process jx, which, for example, could
be a Gaussian Process (GP), indexed by x 2 . Thus dependence is introduced
by linking Fx through dependent location point masses. In this formulation, the
locations of point masses have been indexed by the covariates, but the weights pj
are undisturbed. The distribution on F is now termed as a Dependent Dirichlet
process. It is governed by three parameters: M, F0, and a stationary stochastic
process  which has Fx as marginal for each x. The parameters F0 and M may
or may not depend on the covariate x. If they do, the marginal distribution of Fx will
be a Dirichlet process with parameters Mx and F0x. If they do not,  will generally
be a stationary stochastic process with continuous path and index set .
A simple application of the DDP model is in modeling the residual structure in
the linear model. In the traditional linear model yi D xiˇCi; the errors are assumed
to be drawn iid from a normal distribution with mean 0 and variance 2; which does
not allow them to evolve with the covariate x. In the DDP modeling, i’s are assumed
to be independent and each i Ï Fxi. This approach of inducing dependence is
used in the development of ANOVA DDP models (DeIorio et al. 2004) where
they considered the index x as a categorical variable to model an ANOVA type
dependence of point masses; of spatial models (Gelfand et al. 2005; Duan et al.
2007) in which x is replaced with a stochastic process of random variables where
each random variable is associated with a location in some given region; and in
constructing latent stick-breaking process (Rodriguez et al. 2010). Having only the
locations depend on x constitutes the ﬁrst kind of extension.
The second kind of extension is to have the random weights pj also vary with x.
This can be accomplished by replacing the individual SB ratios Vj, used in building
up pj; by stochastic processes Vjx, x 2 . In doing so, note that the total mass M will
have to be allowed to depend on x as well, since the distribution of Vj involves the
parameter M. This will yield a modiﬁed parameter M. This approach of inducing
dependence through the weights is used in developing time-varying DDP (Nieto-
Barajas et al. 2008); and in order-based SB process priors (Grifﬁn and Steel 2006).
In addition, Grifﬁn and Steel (2006) use permutation of Vj and Reich and Fuentes
(2007) supplement Vj with kernels, as discussed below.
Among the properties of the DDP model, it is pointed out that the prior
distribution on .Fx1; : : : ; Fxd/ has full support as long as the stochastic process 
is rich; the random marginal distribution follows a DP for each x 2 I the RPMs
Fx are continuous in x, the feature that provides evolving distributions along with
changes in covariate; it encompasses a wide spectrum of inference ranging from
a nearly parametric .Mx ! 1/ to an inference showing a strong dependence
between distribution near x; and ﬁnally such models are computationally amenable.

3.3
Dependent Dirichlet Processes
93
By construction, Fx’s are discrete. To extend the models to absolutely continuous
distributions, they all follow the usual recipe of supplementing the DDP model with
a continuous distribution G on unobserved covariate, x. Let F .y/ D
R
Fx .y/ dG .x/.
Thus, although each of Fx is discrete, integrating over a covariate with G produces
a continuous F.
It is clear that in place of the DP, other Ferguson–Sethuraman processes, such as
SB priors (Ishwaran and James 2001) or Pitman–Yor process (Pitman and Yor 1997)
may also be used, affording a great ﬂexibility in modeling.
In this section, we will now describe several dependent nonparametric models.
DeIorio et al. (2004) propose a dependent nonparametric model that describes
dependence across related random distributions Fx in F in an ANOVA-type set up.
Nieto-Barajas et al. (2012) proposed a time-series DDP model, in which dependence
of RPMs is introduced over time while still marginally at each time point the RPM is
assumed to be a DP. Grifﬁn and Steel (2006) propose order-based SB prior models
for continuous covariates in which dependence is introduced by allowing weights
and locations to depend upon covariates indirectly. By allowing the locations x to be
drawn from random surfaces, Gelfand et al. (2005) create random spatial processes.
Duan et al. (2007) modify Gelfand et al. model so that the surface selection can
vary with the choice of locations. Rodriguez et al. (2010) follow a different route.
They no longer build different distributions Fx for each possible value of the index
space, but instead construct a stochastic process where observations at different
locations are dependent and have a common unknown marginal distribution. They
call the resulting process a latent stick-breaking process. Chung and Dunson (2011)
deﬁne a new generalization of the DP called the Local Dirichlet Process to allow
predictor dependence which enables them to borrow information from neighboring
components. Dunson and Park (2008) incorporate covariates through a kernel
mixture, ﬁrst used by Lo (1984) in density estimation. Savitsky and Paddock (2013)
extend this approach and develop a DDP model for repeated measure data which is
not included here.
All of the above models were deﬁned based on Sethuraman representation and
dependence was induced through weights and location atoms. Rao and Teh (2009)
follow a completely different route and provide a general framework to construct
DDP on arbitrary spaces by way of normalized gamma processes. It is well known
that the DP is a normalized completely random measure (CRM) and that a CRM
can be constructed via the Poisson process (PP) (Kingman 1967). Therefore it
offer an alternative approach to the popular SB construction of the DP. Noting
this connection, Lin et al. (2010) give a construction of dependent processes based
on compound PPs. In particular, they develop a dynamic process they call as
Markov Chain DP which is shown to be suitable as a prior in the case of evolving
mixture models. But their approach is for the DDP models which is subsumed by
a generalized procedure developed by Foti et al. (2012) for the construction of
dependent processes based on thinning PPs. The nice thing is that this approach
is not restricted to DDP only but can be used for all models that can be represented
as CRMs. For example, IBP can be described in terms of a mixture of Bernoulli

94
3
Ferguson–Sethuraman Processes
processes where the mixing measure, the beta process (Thibaux and Jordan 2007),
is a CRM.
The list obviously is not exhaustive and models are included for their novelty
and variety. The respective authors provide adequate rationale for developing these
models and describe procedures for carrying out analyses with practical examples.
Needless to say that these extensions of the DP offer a great deal of ﬂexibility in
modeling statistical data.
Computational methods to simulate the posterior distribution which would
unable us to carry out Bayesian inference are fairly well crafted and have become
common tools during the last decade or two. Therefore, we will mention them brieﬂy
in passing and let the reader explore them for details in the original papers where the
models were introduced. To facilitate this, we retain the original notations as much
as possible.
3.3.1
Covariate Models
3.3.1.1
ANOVA DDP Model
For the case x being a categorical variate, DeIorio et al. (2004) propose a dependent
nonparametric model that describes dependence across related random distribution
functions Fx in F in an ANOVA-type setup. They use the DDP model to introduce
dependence in such a way that marginally each random distribution Fx follows a
DP. Therefore, each Fx has Sethuraman representation of a countable mixture of
point masses. Now the strategy is to assume ANOVA model for these location
point masses. The model developed can alternatively be considered as a mixture
of ANOVA models with a DP prior on the unknown mixing measure. Here the
categorical covariate x could be a k-dimensional vector belonging to the covariate
space . As an example consider the case, when x is a bivariate variable, say,
x D .v; w/ ; v D 1; : : : ; V and w D 1; : : : ; W. Let Fx D P1
kD1 pkıkx and assume the
following additive structure on the locations xk:
xk  .v;w/k D mk C Avk C Bwk
(3.3.1)
with A1k D B1k  0, mk
iid p0
m ./ ; Avk
iid p0
Av ./, and Bwk
iid p0
Bw ./, mutually
independent across k; v, and w. This is a familiar random effect model in the analysis
of variance setup. The weights in the Fx are independent of x and are computed
as usual by the SB construction with mass parameter M. The base measure F0 is
a convolution of p0
m, p0
Av, and p0
Bw, which, for example, could be taken as normal
distributions with respective means 	m; 	Av, and 	Bw and variances 2
m, 2
A, and 2
B.
Dependence between two random distribution functions Fx and Fx0 is induced when
x and x0 share a common main effect. They refer to this probability model over F as
ANOVA DDP model and denote it as fFx W x 2 g  ANOVA DDP .M; F0/.

3.3
Dependent Dirichlet Processes
95
As in the traditional ANOVA model, the structural relationships are deﬁned
here through the additive structure and the level of dependence is governed by the
variances of p0’s. Marginally, for each x D .v; w/, Fx is distributed as DP with mass
M and base measure F0
x. This model allows us to incorporate the notion of main
effects and interactions, and adapts the interpretation of the traditional ANOVA
model of normal means to the model with unknown random distributions.
The above model can be extended to a k-dimensional categorical covariate x D
.x1; : : : ; xk/ with main effects and interactions, in a straightforward manner and Fx
need not be univariate. Other extensions are also indicated.
To counter the effect of DP being a discrete measure, they also introduce a
kernel mixture to the ANOVA DDP model, similar to the kernel mixtures described
elsewhere. A typical such model would be
yij.xi D x/  Hx .yi/ where Hx .y/ D
Z
f .yj
/ dFx .
/
and fFx W x 2 g  ANOVA DDP .M; F0/ ;
(3.3.2)
where the kernel f .yj
/ D N .yj	; S/, with the covariance matrix S is typically
used.
For posterior inference it is convenient to view this model as a mixture of
ANOVA models, with mixing coefﬁcients following a DP prior. Thus, let di denote
a design vector to select the appropriate ANOVA model corresponding to xi and
˛h D fmh; A2h; : : : ; AVh; B2h; : : : ; BWhg so that xh D ˛hdi for x D xi. With the
above speciﬁed parameters of prior base measure

p0
m; p0
Av; p0
Bw

, the model can be
rewritten as
yij.xi D x/  Hx .yi/ ; Hx .y/ D
Z
N .yj˛di; S/ dF .˛/ ; F Ï D .M; F0/ ;
(3.3.3)
or for posterior simulation, by introducing latent variables ˛i
yi D ˛idi C i; ˛i Ï F; F Ï D .M; F0/ and i Ï N.0; S/:
(3.3.4)
Let ˛
1 ; : : : ; ˛
k be k  n distinct elements among ˛1; : : : ; ˛n with nj multiplicities
of ˛
j ; j D 1; : : : ; k. Deﬁne si D j iff ˛i D ˛
j , the cluster identiﬁer, and let j D
fi W si D jg, and  stand for some known, or possibly unknown, hyperparameters of
F0 and S. To implement the posterior distribution simulation, the marginal approach
discussed in Sect. 2.4 is used. The conjugate nature of p0 simplify the simulation.
It can be implemented by reformulating the above model as a mixture of ANOVA
models. That is, the data yi are drawn from a mixture of ANOVA models with a DP
prior on the unknown mixing distribution. Now the MCMC scheme used in the case
of DP mixture models (for example, MacEachern 1998) indicated earlier can also

96
3
Ferguson–Sethuraman Processes
be extended for posterior simulation in ANOVA DDP models. The steps for Gibbs
sampler are
1. Resample si from
P .si D jjsi; y; ; S/ _
(
n
j p .yijsi D j; si; yi; ; S/ ; j D 1; : : : ; k
M R N .yi W ˛idi; S/ dF .˛/
j D k C 1;
where ai denotes all elements of vector a except the i-th element ai;
n
j D
 nj  1 if j D si
nj
if j ¤ si;
and k is the number of clusters with ˛i removed. If n
si
D 0; relabel the
remaining clusters j D 1; : : : ; k D k  1. After sampling si, set
k D

k
if
si  k
k C 1 if si D k C 1:
2. Resample ˛
j from its posterior distribution given by
p

˛
j js; y; ; S

_
2
4Y
i2j
N

yi W ˛
j di; S

3
5 p0 
˛
j j

:
3. Resample , if unknown, conditional on the current values of s; ˛, and k, as per
standard posterior simulation for the DP mixture models.
This model can be extended to accommodate other features. For example, one
could consider a non-linear model or introduce hierarchy in the model. Similarly, it
can be extended to the data involving repeated measures.
In this model the weights of the point masses do not depend on the covariate x.
More complex models can be constructed by allowing its dependence.
3.3.1.2
Time-Series DDP
Nieto-Barajas et al. (2012) introduced another variation of DDP model in which
dependence of RPMs is induced over time while still marginally at each time point
the RPM is assumed to be a DP. The dependence is induced by deﬁning multivariate
beta distributions for the SB ratio Vj’s. This model is suitable to serve as a prior
for a time series of RPMs. In this case  consists of a ﬁnite number of values,
t1; : : : ; tJ, which without loss of generality, we take here as 1; : : : ; J. A time-series
model is proposed by deﬁning a joint probability distribution on the collection
F D fFt W t D 1; : : : ; Tg of RPMs representing time-varying distributions. This is

3.3
Dependent Dirichlet Processes
97
achieved by making the weights in the Sethuraman representation of Ft vary with
time, but dependent, while the atoms remain the same over time. This allows them to
specify different strengths of dependence at different stages. This model is different
from the ANOVA DDP model where locations change but weights remain the same
across t.
Let Ft D P1
jD1 pjtıj, where pjt are weights speciﬁc to Ft and j are point
masses assumed to be common across all t and distributed according to F0. Now
the dependence between Ft and FtC1 is implemented by introducing a sequence of
latent binomial random variables
zjtjVjt  Bin

mjt; Vjt

; Vj1  Be .1; M/ ;
(3.3.5)
and for t > 1; replacing the distribution of Vjt with
Vjtjzjt1  Be

1 C zjt1; M C mjt1  zjt1

; t D 2; : : : ; T:
(3.3.6)
The joint probability model for fF1; : : : ; FT g is referred to as time-series DDP
model and written as fF1; : : : ; FT g  tsDDP .M; F0; m/, where m D
˚
mjt
1;T
jD1;tD1.
The marginal distribution of Vjt  Be .1; M/ does not change. This together with
j
iid F0, imply that Ft  D .M; F0/. So Ft remains unchanged as a DP. The role of
latent variables zjt is to introduce dependence between pairs Vjt and VjtC1. Here the
parameter mjt governs the level of dependence, larger the value of mjt, greater is the
dependence. In fact the correlation between Vjt and VjtC1 is given by
corr

Vjt; VjtC1

D
mjt
1 C M C mjt
and is controlled by mjt. As mjt ! 1, the correlation is 1, and Vjt D VjtC1 with
probability 1 and results in equal weights. Doing so for all j and t; we get all RPMs
Ft’s as identical with probability 1. On the other hand, if all mjt ! 0, we get Vjt and
VjtC1 as independent.
The authors also derive correlation between Ft and FtC1 and discuss its interpre-
tation. From this it could be concluded that even if Vjt is independent of VjtC1; for
all j, Ft is still correlated with FtC1 due to common atoms ’s they share. In special
case mjt D mt; the correlation between Ft .B/ and FtC1 .B/ for B 2 A simplify to
corr .Ft .B/ ; FtC1 .B// D
1 C M
1 C 2M .2 C M C mt/ = .2 C M C 2mt/:
The simulation of posterior distribution can be performed via the usual MCMC
scheme. To sample from the posterior distribution of Ft, a collapsed Gibbs sampler
which is similar to the blocked Gibbs sample of Ishwaran and James (2001) is used,
in which the process is expressed as Sethuraman representation but truncated at
ﬁnite number of terms N, and hence it approximates the posterior distribution of

98
3
Ferguson–Sethuraman Processes
the process. In contrast, the collapsed Gibbs sampler, however, does not involve
truncation and is more efﬁcient. It is similar to the retrospective sampling procedure
developed by Papaspiliopoulos and Roberts (2008) and is based on marginalizing
nonallocated point masses in the SB representation of Ft.
3.3.1.3
Order-Based Stick-Breaking Prior
In contrast to DeIorio et al. (2004), Grifﬁn and Steel (2006) propose models for
continuous covariates in which dependence between the distributions is induced
by allowing both weights and locations to depend upon covariate indirectly. The
rationale is that since an RPM is deﬁned as a function of V and  via the SB
construction, it is natural for a Fx deﬁned at a point x to make V and  also depend
on x. However, instead of having Vj ’s and j ’s depend directly on x, they induce
dependence by ranking elements of V and  by an ordering  which depends upon
x such that the distributions for similar covariate values are associated with similar
ordering and thus will be close. At any covariate x, the random distribution used
ﬁrst is the Stick-breaking prior, but later their main focus is the DP prior, since it
is not easy to deﬁne ranking for an inﬁnite sequence. They deﬁne an order-based
stick-breaking prior on the covariate space  by a sequence
˚
aj; bj
N
jD1 (where N is
potentially inﬁnite), a centering distribution F0; and a stochastic process f .x/gx2
such that the following holds:
˚
1 .x/ ; : : : ; n.x/ .x/


 f1; : : : ; Ng for some n .x/  NI i .x/ D j .x/ iff i D j:
 .x/ D
˚
1 .x/ ; : : : ; n.x/ .x/

with n .x/  N; is referred to as the ordering at x:
The random variables j’s and Vj’s are independent, j
iidÏ F0; and Vj Ï Be

aj; bj

.
The ordering  associates each pair

j; Vj

with a position in the covariate space
and the ordering is deﬁned through ranking of them by their distance from x, with
the smallest distance ﬁrst. In applications they use a Poisson point process Z with
intensity  to generate the ordering in combination with the permutation. The same
distribution over the space  is obtained if i .x/ D i for all x 2  and i D 1; : : : ; N.
The random distribution at x 2  is deﬁned as
Fx D
n.x/
X
jD1
pj .x/ ıj.x/;
(3.3.7)
where pj .x/
D
Vj.x/
Q
i<j

1  Vi.x/

and for ﬁnite n .x/ ; pn.x/ .x/
D
Q
i<n.x/

1  Vi.x/

. They show E ŒFx .B/ D F0 .B/ for B 2 A. Variance of
Fx .B/ and correlation between Fx1 .B/ and Fx2 .B/ are also derived. If aj D 1 and
bj D M and that N D 1; we recover a DP at any x 2  if n .x/ D 1. This
subclass of processes is called order-based dependent Dirichlet processes .DDP/
with parameter M; base distribution F0; and stochastic process f .x/gx2. Since the

3.3
Dependent Dirichlet Processes
99
DP produces a discrete distribution, the kernel (usually taken as a normal kernel)
mixture of DPs model provides a vehicle to generate an absolutely continuous
distribution. This model can be used when x is time, space, or a covariate.
Mixtures of order-based dependent Dirichlet processes are also introduced in
three different settings: curve ﬁtting, modeling volatility in time-series data, and in
spatial modeling. In the case of curve ﬁtting, models used for density estimation (Lo
1984; Escobar and West 1995) can be extended by replacing the DP by DDP.
As one would expect, there are difﬁculties in simulating the posterior distribution.
The authors use the truncation method of Ishwaran and James (2001). An additional
step required is to sample the point process Z and the intensity parameter  in
which Z is also truncated. In contrast to these authors, DeIorio et al. use the Gibbs
sampler for the posterior distribution marginalized over the parameters V’s, and
where needed over the parameters ’s as well. The updating of parameters follows
from that of the DP described in Ishwaran and James (2001). Alternatively, one
can use the methods proposed in Walker (2007) and Papaspiliopoulos and Roberts
(2008) which alleviates the necessity of truncation.
Reich and Fuentes (2007) extend the stick-breaking prior to the multivariate
spatial setting and use it to analyze wind ﬁeld data. They use bivariate normal priors
for the location j; and similar to Grifﬁn and Steel have the weights pj vary spatially,
but instead of permuting Vj’s, they introduce a series of kernel functions to allow
the masses to change with space. That is they replace Vj .x/ in the stick-breaking
construction by a kernel wj .x/ Vj. This is similar to the approach of Dunson and
Park (2008) discussed elsewhere.
3.3.2
Spatial Models
3.3.2.1
Spatial Dirichlet Process
Earlier F was considered as a collection of random probability measures indexed
by x; which was treated as a covariate and  a covariate space. For spatial models,
x is replaced with a stochastic process of random variables where each random
variable is associated with a location in some given region. Modeling in such spatial
context is usually carried out by assuming a stationary Gaussian process for the
spatial process. The parameters of the Gaussian process are unknown and hence the
process is random. For the Bayesian analog, parameters are assigned certain prior
distributions, otherwise the procedure is deemed to be a ﬁnite dimensional model.
Gelfand et al. (2005) instead replace the Gaussian process and allow the locations
x to be drawn from random surfaces to create a random spatial process. They
developed a nonparametric, non-stationary spatial process called the spatial DP,
and then extend it to a spatial DP mixture. To replace the GP, a stochastic process
of random variables, Y .s/, where each is associated with a location s in a given
region D 
 Rd is needed. As in the case of Gaussian process, the stochastic
process is speciﬁed by its arbitrary ﬁnite dimensional distribution of the vector

100
3
Ferguson–Sethuraman Processes
.Y .s1/ ; : : : ; Y .sk// ; where s.k/ D .s1; : : : ; sk/ is a set of k locations in D. This
resulting process is obviously non-stationary and the joint distribution need not be
normal.
Gelfand et al. start developing the model for point-referenced data assuming
the data collected as a sample from a realization of random process YD

fY .s/ W s 2 Dg at the speciﬁc locations .s1; : : : ; sk/, and that we have n replications
of measurements at these locations. Thus the data is of form fyt .s1/ ; : : : ; yt .sk/gT,
t D 1; : : : ; n. To mirror the GP, they deﬁne a random process F such that marginally
at each s 2 D, F .Y .s// is a DP (think as FY.s/ instead of Fx). So to develop the
spatial DP, they start with a DP with parameters, M; and F0 which is assumed to
be continuous, frequently a stationary Gaussian. A random distribution function
arising from a D .M; F0/ has the usual representation P1
jD1 pjıj where pj’s are the
weights and j are iid F0; and independent of pj’s. (j could be vectors in which
case we obtain a multivariate DP.) Now to model Y’s, atoms j are replaced with
a realization of a random process j;D D
˚
j .s/ W s 2 D

; :j D 1; 2; : : : while
leaving the weights undisturbed. The resulting random process F for YD has the
form P1
jD1 pjıj;D and the construction is referred to as a spatial DP model. This
means that for s.k/ as above, F induces a ﬁnite dimensional RPM F.k/ on the space
of distribution functions for the vector fYt .s1/ ; : : : ; Yt .sk/g. Because the weights
pj’s are independent of locations s 2 D; we have F.k/ Ï D

M; F.k/
0

, where F.k/
0
is the k-variate distribution for fYt .s1/ ; : : : ; Yt .sk/g induced by F0. For example,
if F0 is a Gaussian process, then F.k/
0
is a k-dimensional normal distribution. The
representation of F indicates that it is a non-stationary process, centered around
a stationary process F0. Note the same surface is used for each realization of j;D
from F0. Covariance structure is thus incorporated thereby pooling information from
nearby locations.
In comparison with MacEachern (2000), the distinction highlighted is that there
the distributions are dependent such that at each index value the distribution is
univariate DP. Here, F induces a random distribution F .Y .s// for each s, hence
the set FD  fF .Y .s// W s 2 Dg will be a DDP. (It is possible to get a richer
spatial DP by allowing Vj in pj as well to depend on s; parallel to a more general
DDP.) The advantage here is that by the proximity of j;D’s, the random marginal
distribution F .Y .si// of Y .si/ and F

Y

sj

of Y

sj

given F are such that the
difference between them tends to zero as
si  sj
 ! 0. This way information from
neighboring locations can be utilized. They show
E .Y .s/ jF/ D
1
X
jD1
pjj .s/ ; V .Y .s/ jF/ D
1
X
jD1
pj2
j .s/ 
2
4
1
X
jD1
pjj .s/
3
5
2

3.3
Dependent Dirichlet Processes
101
and the covariance of a pair of sites
Cov

Y .s/ ; Y.s0/jF

D
1
X
jD1
pjj .s/ j

s0

2
4
1
X
jD1
pjj .s/
3
5
2
4
1
X
jD1
pjj

s0
3
5 :
To overcome the discreteness of F; they follow the usual practice and propose
a kernel mixture type model in which a pure error process is mixed with respect
to F to create a random process F that has continuous support. The above model
can further be used for prediction of a random realization of the process at locations
where YD is not observed. Properties of these processes are investigated.
To simulate the posterior distribution, the Gibbs sampling procedure is used.
3.3.2.2
Generalized Spatial Dirichlet Process
Duan et al. (2007) argue that the above model uses the same set of weights pj’s thus
inducing common surface selection for all locations in the collection, which may not
be appropriate in certain situation. Therefore they introduce a random distribution
function for the spatial effects such that the surface selection can vary with the
choice of locations and so can the joint selection of surfaces for the n-locations.
The marginal distribution of the effect at each site still comes from a DP. Thus their
model is a generalization of the above model in which pj’s are also made to vary with
locations. They call it as a generalized spatial Dirichlet Process. Here an RPM F on
the space of surfaces over D is deﬁned such that its ﬁnite dimensional distributions
is speciﬁed as follows. For any collection of sets A1; : : : ; Ak of A
P .Y .s1/ 2 A1; : : : ; Y .sk/ 2 Ak/ D
1
X
i1D1
  
1
X
ikD1
pi1;:::;ikıi1.s1/ .A1/ : : : ıik.sk/ .Ak/ ;
(3.3.8)
where ij stands for i

sj

, j D 1; : : : ; k, j are iid F0, and the weights fpi1;:::;ikg,
independent of locations, represent the site-speciﬁc joint selection probabilities
having a distribution deﬁned in inﬁnite dimensional simplex
S D
8
<
:pi1;:::;ik  0I
1
X
i1D1
  
1
X
ikD1
pi1;:::;ik D 1
9
=
; :
(3.3.9)
The weights must also satisfy the consistency condition
pi1;:::;ik1;ikC1;:::;in D pi1;:::;ik1;;ikC1;:::;in 
1
X
jD1
pi1;:::;ik1;j;ikC1;:::;in

102
3
Ferguson–Sethuraman Processes
and a continuity property. The authors compute the ﬁrst two moments of Y .s/ and
the covariance of Y .si/ and Y sj
 conditional on the realized distribution F. They
use this generalized process to model a random effect model of the type Y .s/ D
m .s/ C ˇ .s/ C  .s/, where m .s/ is a constant term and  .s/ is a Gaussian pure
random error term with mean zero and variance 2.
3.3.2.3
Latent Stick-Breaking Process
In the previous model, we considered a class F of RPMs Fx deﬁned for each value
x of the index space D and proposed priors on F. Rodriguez et al. (2010) follow a
different route. They construct stochastic processes on D  Rd and propose prior
distributions on these processes having constant marginals at each point x 2 D, but
allow samples from it to be dependent. That is, different distributions Fx for each
possible value of the index space is no longer built, but instead construct a stochastic
process where observations (random variables) at different locations are dependent
but have a common unknown marginal distribution. In other words instead of having
marginal distribution of the process change at each location, they assume a constant
marginal Fx (independent of x) across the index space, but allow samples from it
to be dependent. The resulting process is called a latent stick-breaking process. It
induces a partition of the index space.
Thus the goal in the case of univariate is to construct a stochastic process
YD  fY .s/ W s 2 Dg ; D 
 R1 such that, marginally, Y .s/ jF  F for all s 2 D,
for some unknown F, but cov .Y .s/ ; Y .s0/ jF/ ¤ 0. It is sufﬁcient for the purpose
to provide a ﬁnite dimensional joint distribution of the vector .Y .s1/ ; : : : ; Y .sk//.
For this purpose, they deﬁne a stochastic process U D fU .s/ W s 2 Dg with uniform
marginals deﬁned, and then deﬁne Y .s/ D F .U .s//, where F is a discrete
probability distribution and F is its generalized inverse. This clearly shows that
Y .s/  F for any given s and Y .s/ and Y .s0/ are correlated. To choose the process
U and a prior F; they consider independent sequences fz .s/ W s 2 Dg ; fVlgL
lD1
and flgL
lD1. Here the latent Gaussian process z is taken such that marginally
z .s/  N .0; 1/ for all s 2 D and corr .z .s/ ; z .s0// D  .s; s0/. Then U .s/ is
taken as ˆ .z .s//, where ˆ is the cumulative distribution of the standard normal
variate. Sequences fVlgL
lD1 and flgL
lD1 are used to deﬁne a random distribution
F D PL
lD1 plıl; where
pl D Vl
Y
k<l
.1  Vk/; Vl  Be .al; bl/ for l < L and VL D 1:
(3.3.10)
However, as a departure from the usual practice, they impose order restriction in
constructing the sequence of atoms flgL
lD1, as follows. 1  F0, and for l > 1; l 
F0l, where F0l is the restriction of F0 to the set Sl D f W  > l1g, i.e., F0l .B/ D
F0 .B \ Sl/ =F0 .Sl/ for any measurable set B 2 A. This order restriction allows
them to generate skewed mixing distributions. Now for any ﬁnite set of locations

3.3
Dependent Dirichlet Processes
103
s1; : : : ; sn in D, the joint distribution is given by
P

Y .s1/ D l1; : : : ; Y .sn/ D lnj fVlgL
lD1 and flgL
lD1

D P

z .s1/ 2 Œˆ1 .l11/ ; ˆ1 .l1//; : : : ; z .sn/ 2 Œˆ1 .ln1/ ; ˆ1 .ln//

;
(3.3.11)
where l D 1  Q
kl.1  Vk/ is the proportion of the unit stick assigned to the ﬁrst
l atoms, with 0 D 0. The prior distribution induced this way on F is called a latent
stick-breaking process (LaSBP), and has parameters .falgL
lD1 ; fblgL
lD1 ; F0; /. It is
written as
YjF  F and F  LaSBPL.falgL
lD1 ; fblgL
lD1 ; F0; /:
(3.3.12)
Here the joint distribution reﬂects closeness between Y .s/ and Y .s0/ as opposed to
between the distributions at s and s0, as is the case in DDP.
In Gelfand et al. (2005) a random distribution function F was constructed for the
(whole) process YD and it induced the distribution F .Y .s// at a point s 2 D. Here a
single F is constructed for all points in D. Moreover, replication of observations is
not required.
For inference purposes, an MCMC algorithm is developed in which al D a and
bl D b are set. For the case L < 1, a blocked Gibbs sampler (Ishwaran and James
2001) is used and in case of L D 1, a retrospective sampler of Papaspiliopoulos
and Roberts (2008) is used. The algorithm proceeds by sequentially updating the
parameters involved.
The hybrid DP (hDP) models proposed by Petrone et al. (2009) is related to
LaSBP. The authors also use Gaussian copula processes to build dependence across
locations. Their model is as follows. Let Yi  fY .s/ W s 2 Dg, D 
 R1; i D 1; : : : ; n
be random curves deﬁned on D and consider the following model: Yi D i Ci; i D
1; : : : ; n , where i are independent realization of a Gaussian process GP

0; 2I

,
or Yiji  GP

i; 2
, where the mean functions i are speciﬁed so as to borrow
strength in the estimation by introducing dependence across i’s. They are assumed
to be sampled from a probability measure G. Thus we have a mixture of Gaussian
processes
Y1; : : : ; YnjG 
Z
GP

j
; 2
dG .
/ :
(3.3.13)
For the random curve 
  f
 .s/ W s 2 Dg in RD with probability measure G; denote
by Gs the ﬁnite k-dimensional distribution of .
 .s1/ ; : : : ; 
 .sk// at coordinates
.s1; : : : ; sk/. It is assumed that the random curves are observed at the same
coordinates so that the available data are Yi D .Yi .s1/ ; : : : ; Yi .sk// ; i D 1; : : : ; n;
with Yi

sj

D 
i

sj

C i

sj

. Let 
i D .
i .s1/ ; : : : ; 
i .sk//. Then the ﬁnite

104
3
Ferguson–Sethuraman Processes
dimensional characterization of the above model is
Yij
i
ind
 Nk


i; 2Ik

and 
ijGs
iid Gs;
(3.3.14)
where Nk .; / is the k-variate normal distribution and Ik is the k-dimensional identity
matrix. After integrating out 
’s, the ﬁnite dimensional characterization can be
stated as, YijGx
iid R Nk


; 2Ik

dGs .
/, a mixture of normal densities. Now the
problem becomes familiar—a prior is assigned to Gs.
The authors choose a ﬁnite dimensional discrete SB prior (Ishwaran and James
2001) for G which has representation GN D PN
jD1 pjı

j , with p D . p1; : : : ; pN/ Ï
D .˛1; : : : ; ˛N/ and independent of pj’s, 
’s are iid curve atoms in RD distributed
according to G0, a nonatomic distribution on RD. Using the corresponding k-
dimensional representation, we have
YijGx
iid
N
X
jD1
pjNk



j ; 2Ik

;
(3.3.15)
a ﬁnite dimensional mixture of k-variate normal distribution, a familiar mixture
model. Note, here 

j D



j .s1/ ; : : : ; 

j .sk/
 iid G0s. If the Dirichlet distribution
is taken to be symmetric with parameters, ˛i D ˛=N, it is clear that in limit as
N ! 1; the prior G tends to the DP. Extending to the functional case, it can be
said that the RPM GN deﬁned on RD has a functional DP. If we take N D 1, then
G1 represents the Sethuraman representation of the DP and in that case pj’s are
deﬁned as usual using beta random variables and the resulting prior is identiﬁed as
functional DP with parameters ˛ and G0 as usual.
The authors thus deﬁne a prior on the space of probability measures for a
random curve by allowing local clustering. They discuss several models, called
hybrid Dirichlet mixture models for functional data, using this prior and discuss
their performances with real and simulated data sets.
However, in LaSBP scalar atoms are used instead of process realizations, which
is simpler.
Details of multivariate extension and mixtures of LaSBP along with various
properties are available in their paper. For computation purposes, an MCMC
algorithm is developed and steps are given for updating the coordinates from full
conditionals of the parameters. Application of the above model with practical
examples is illustrated in the paper.
3.3.2.4
Local Dirichlet Process
Chung and Dunson (2011) deﬁne a new generalization of the DP called the Local
Dirichlet Process to allow predictor dependence which enables them not only
to borrow information from neighboring components, but also yields marginal

3.3
Dependent Dirichlet Processes
105
DPs. It is a prior distribution on a collection of RPMs indexed by predictors.
It allows local adoptability while preserving many of the desirable properties of
the DDP, (1)–(4) mentioned in the introduction. The RPM at a given predictor
value is constructed using weights and atoms of the SB construction located in a
neighborhood of the predictor, thus inducing dependence. Their strategy is ﬁrst to
deﬁne predictor-dependent SB priors. Deﬁne the global sets of locations, weights
and atoms, respectively, as follows. Y D fyi W i D 1; 2; : : :g ; V D fVi W i D 1; 2; : : :g
and D fi W i D 1; 2; : : :g, where yi
iid H; a ﬁnite positive measure, Vi
iid Be .1; M/,
and i
iid
F0, and H is a probability measure on .0;  .0// ; 0 Lebesgue
measurable subset of Euclidean space Rd. Let   0 be a given predictor
space. For a given x 2 , let D 
x denote a  -neighborhood of x 2 . That
is D 
x
D fy W d .x; y/ <  ; y 2 0g, where  > 0, and d .; / W   0 ! RC
is a given metric. Assume that H

D 
x

> 0. Also, for x 2 ; let Lx be a
predictor-dependent set indexing the locations belonging to D 
x deﬁned by Lx D
fi W d .x; yi/ <  ; i D 1; 2; : : :g. Deﬁne a set of local random components,
Y .x/ D fyi W i 2 Lxg ; V .x/ D fVi W i 2 Lxg and  .x/ D fi W i 2 Lxg :
(3.3.16)
This implies that the sets V .x/ and  .x/ contain weights and atoms that are assigned
to the locations Y .x/ in D 
x . Using the local components Y .x/ ; V .x/, and  .x/
deﬁne
Fx D
N.x/
X
lD1
pl .x/ ıl.x/ with pl .x/ D Vl.x/
l1
Y
jD1
1  Vj.x/
 ;
(3.3.17)
where N .x/ is the cardinality of Lx and l .x/ is the l-th ordered index in Lx. Under
the above setup and assumption, they prove (their Lemma 1) that for all x 2 ,
N .x/ D 1 and PN.x/
lD1 pl .x/ D 1 almost surely.
Thus Fx is well deﬁned. Given M; F0; H;  and choice of metric d .; /, the
above formulation deﬁnes a predictor-dependent stick-breaking prior (SBP) for
F D fFx W x 2 g deemed as local Dirichlet Process (lDP), i.e., an lDP prior with
parameters M; F0; H,  is assigned to F. Symbolically, F D fFx W x 2 g Ï
lDP .M; F0; H;  /. They prove that marginally, Fx is a DP, Fx Ï D .M; F0/.
The parameter  controls the size of neighborhood D 
x . If x and x0 are close
to each other, their respective neighborhoods might overlap and Fx and Fx0 may
share some common atoms resulting in corresponding dependence of Fx and Fx0.
The amount of dependence will decrease if x and x0 are farther apart and may even
tend to Fx and Fx0 being independent. While each Fx is a DP, lDP prior approach
allows it to vary with predictors and borrow information across local regions of
prediction space. Due to the discreteness property of each Fx, the lDP will induce
local clustering of subjects according to their predictor values.

106
3
Ferguson–Sethuraman Processes
The authors compute the correlation coefﬁcient between Fx .B/ and Fx0 .B/
for B 2 A and discuss some properties. For computational purposes, an MCMC
algorithm based on the blocked Gibbs sampler (Ishwaran and James 2001) is
developed and the necessary steps for sampling from the relevant conditional
posterior distributions are provided. The paper also contains an application of the
lDP to an epidemiologic study.
3.3.2.5
Kernel Based Stick-Breaking Processes
A different way of incorporating the covariates is through the kernel mixture ﬁrst
introduced by Lo (1984) in relation to density estimation (see Sect. 2.5.3). Dunson
and Park (2008) follow this line of extension. In the problem of density estimation,
Lo (1984) deﬁnes a random density function by setting f .y/ D R k .y; u/ dF .u/,
where k .y; u/ is a known kernel and F is taken to be a random distribution function.
By assuming F Ï D .M; F0/ ; he places a prior on the space of density functions.
These type of kernel mixture models are dense (in L1 norm) in the space of
absolutely continuous distributions. Now the covariate can be accommodated via
letting f .yjx/ D R k .y; u/ dFx .u/, where Fx is chosen according to the Dependent
Dirichlet process discussed above.
Motivated by this possibility, a further more complex generalization is proposed
by Dunson and Park (2008) in which, in addition to the locations and mixing
weights depending on covariate x, they replace the point mass at j by a nonde-
generate probability measure Gj. Dependence is induced through weighted mixture
of independent probability measures. That is, they construct predictor-dependent
RPMs via the SB construction in which the weights are expressed as a kernel
multiplied by beta random variables. Consider a sequence of mutually independent
random components
˚
j; Vj; GjI j D 1; 2; : : :

, where for each j, j Ï F0 is a
location parameter, Vj Ï Be

aj; bj

deﬁnes a probability weight, and Gj Ï Q
a probability measure, all deﬁned on appropriate spaces. For a bounded kernel
K W Rp  Rp ! Œ0; 1 ; let Ujx

j; Vj

D K

x; j

Vj for all x 2 .
Then a kernel stick-breaking process is deﬁned as
Fx D
1
X
jD1
Ujx

j; Vj
 Y
i<j
.1  Uix .i; Vi// Gj:
(3.3.18)
This representation may be recognized as a covariate dependent mixture of an
inﬁnite sequence of base probability measures with Gj located at j. By the argument
of Ishwaran and James (2001), it can be concluded that Fx is well deﬁned. The usual
weights in SB construction get relatively higher proportion of probability at earlier
locations. By replacing them with K x; j
 Vj; allows weights to be rebalanced by
proper choice of K so that j close to x gets relatively higher proportion of SB
probability. This way Fx and Fx0 will have similar probabilities allocated if x and x0
are close. In this manner, the kernel SB process accommodates dependence.

3.3
Dependent Dirichlet Processes
107
There are some obvious special cases. For example, if the kernel K .x; / D 1 for
all .x; / 2   X, so that
Fx  F D
1
X
jD1
Vj
Y
i<j
.1  Vi/ Gj:
(3.3.19)
Then if Gj Ï D .M; F0/, independently for each j, we get a stick-breaking mixture
of DPs as a prior for F; and if Gj D ıj, j Ï F0, then F is assigned a stick-breaking
prior as deﬁned by Ishwaran and James (2001). If in addition to K .x; / D 1;
aj D 1 and bj D M for all j D 1; 2; : : :, the prior for F is a DP mixture of DPs.
Conditional on the sequence
˚
j; VjI j D 1; 2; : : :

, expectation and variance of Fx;
and covariance are derived. The authors discuss further properties of the process,
clustering and prediction rules. For posterior computation they develop a conditional
approach relying on a combination of MCMC algorithm that uses retrospective
sampling and generalized Polya urn sampling steps.
3.3.3
Generalized Dependent Processes
All of the models presented in the previous sections were developed using the
Sethuraman representation of the DP and different modiﬁcations were implemented
in its SB construction to obtain various nonparametric dependent processes models.
Realizing that the DP can also be constructed as a normalized gamma process, Rao
and Teh (2009) pursue an alternate approach in developing DDP. Following this
and also noting that the DP is a normalized completely random measure (CRM)
(Kingman 1967) that can be constructed using a Poisson process (it may help to
review Sect. 4.1 ﬁrst), Lin et al. (2010) give a different construction of dependent
Dirichlet processes based on compound PPs. In particular they constructed a Markov
DP. Their approach, however, is for the DDP model which is subsumed by a
generalized procedure developed by Foti et al. (2012) for the construction of
dependent processes based on thinning PPs. The nice thing is that this approach
is not restricted to DDP only but can be used for all models that can be represented
as CRMs. For example, IBP can be described in terms of a mixture of Bernoulli
processes where the mixing measure, the beta process (Thibaux and Jordan 2007) is
a CRM. This means that Foti et al. method can also be used to construct a family of
dependent beta processes (Ren et al. 2011). This interesting alternate approach for
developing dependent processes will now be presented in this section.
3.3.3.1
Spatial Normalized Gamma Processes
Rao and Teh (2009) provide a general framework to construct DDP on arbitrary
spaces by way of normalized gamma processes. The idea is to deﬁne a gamma

108
3
Ferguson–Sethuraman Processes
process G over an extended space, slice up the space into different regions and
associate a DP with each region. Then deﬁne the DP by normalizing the restriction
of the gamma process on the associated region. This produces a set of dependent
DPs. Dependence is controlled by the amount of overlap among the regions. Since
the gamma process is a completely randomized measure, it can be constructed by
a Poisson process. Its realization can be expressed as an inﬁnite sum of atomic
measures with random weighted point masses, based on the mean measure of the
Poisson process.
Let .‚;  .‚// be a measure space and let T be an index space. We construct
the set FT D fFt W t 2 Tg of dependent random measures over .‚;  .‚// such that
each Ft is marginally a DP as follows. Let  be an auxiliary space and for each
t 2 T; let t   be a measurable set. Consider the product space   ‚ and
deﬁne a gamma process G on the product space with base measure ˛ itself deﬁned
on the product space   ‚. Let ˛t .d
/ D
R
t ˛ .dx; d
/ be the restriction of ˛
on the set t. ˛t is then a measure on ‚ for each t 2 T. Since the restriction of a
gamma process is also a gamma process, deﬁne Gt .d
/ D
R
t G .dx; d
/, which is
a gamma process with base measure ˛t. Now deﬁne Ft D Gt=Gt .‚/. Then clearly,
Ft  D .˛t/. The resulting set FT D fFt W t 2 Tg of dependent DPs is called Spatial
normalized gamma processes. The degree of dependence of Dirichlet processes, Fs
and Ft depends upon the amount of overlapping of gamma processes, Gs and Gt.
As an example, consider T D  D R1, the real line, and t D .t  L; t C L/ ;
interval of length 2L; for L > 0. Also, let M be a concentration parameter and
F0, the base distribution deﬁned on ‚: The base measure of the gamma process G is
˛ .dx; d
/ D dxMF0 .d
/ =2L and yields ˛t D MF0 .d
/. Therefore, Ft  D .MF0/.
Each atom in G has a time point y and a time-span Œy  L; y C L and therefore will
appear in the DP Ft as long as t 2 Œy  L; y C L. Thus two DPs Fs and Ft will share
more atoms if s and t are close to each other and no atoms if js  tj > 2L. In fact, the
dependence between Fs and Ft is a function of js  tj and they will be independent
if js  tj > 2L.
Although FT is a collection of inﬁnite number of DPs, in practice we will come
across only a ﬁnite number of observations at times say t1; : : : ; tn.
Like the models covered earlier, this construction also yields marginal DPs.
Chung and Dunson’s (2009) model come close to this model where the depen-
dence is introduced through spatially overlapping regions and use SB construction
instead of normalized gamma processes. For posterior computation the authors
give a MCMC procedure involving Gibbs sampler, and also Metropolis–Hastings
procedures.
3.3.3.2
Dependent CRMs and Processes
Since the DP is a normalized CRM, and that a CRM can be constructed via the
Poisson process (PP), Lin et al. (2010) give a different construction of dependent
processes based on compound PPs instead of gamma processes. In particular, they
develop a dynamic process they call as Markov Chain DP which is shown to be

3.3
Dependent Dirichlet Processes
109
suitable as a prior in the case of evolving mixture models such that the DP at each
step is generated by modifying the previous one to include new information and
delete the information no longer useful. But their approach is still for the DDP model
which is subsumed by a generalized procedure developed by Foti et al. (2012) for
the construction of dependent processes based on thinning PPs.
Recall that a CRM deﬁned on the space .‚;  .‚// with Levy measure  can
be represented by a PP with mean measure  on the space ‚  RC. If  has
inﬁnite mass, then there are inﬁnite number of points of the PP. Then the CRM
with Levy measure  can be represented as 	 D P1
kD1 kı
k. In the same vein,
dependent nonparametric processes can also be described by a PP deﬁned on the
augmented space   ‚  RC, where  is an auxiliary space in addition to ‚ being
a parameter space and RC the space of masses of points of the PP. Foti et al. (2012)
use this framework as a basis for dealing with two covariate dependent models,
namely the gamma and beta processes, and show that it improves prediction power
in nonparametric Bayesian approach to modeling.
The augmented PP for the dependent processes may be written as follows: Let
… D f.xi; 
i; i/g1
iD1 be a PP on the space ‚RC with mean measure described
by the positive Levy measure  .dx; d
; d/. While the theory holds for this form
of Levy measure, we consider the homogeneous case where  .dx; d
; d/ D
G .dx; d
/ 0 .d/. That is, the masses attached to atoms are independent of the
location of atoms in   ‚. In this case the CRM corresponding to dependent
processes can be represented as:
	 D
1
X
kD1
kı.xk;
k/
(3.3.20)
on the space   ‚. Let T be some covariate space and let fpx W T ! Œ0; 1gx2 be
a collection of functions induced by x 2 . For deﬁning a family of measures 	t
dependent on t 2 T; one proceeds as follows: For each point .xk; 
k; k/ 2 …, deﬁne
a set of Bernoulli random variables
˚
rt
k

t2T taking value 1 with probability pxk .t/,
zero otherwise. The variable rt
k serves as an identiﬁer and indicates whether the
atom k in the global measure is included in the local measure 	t at covariate value
t or not. By the marking theorem of PPs, the resulting thinned PP can be written as
…t D
˚
.xk; 
k; k/ jrt
k D 1
1
kD1 with mean measure
 .A; d
; d/ D
Z
A
px .t/  .dx; d
; d/ for A 2  ./ ;
(3.3.21)
and the corresponding inﬁnite sum representation of 	t is
	t D
1
X
kD1
rt
kkı
k:
(3.3.22)

110
3
Ferguson–Sethuraman Processes
	t is a CRM deﬁned on ‚ that varies with t 2 T and has levy measure
	t .d
; d/ D
Z

px .t/  .dx; d
; d/ :
(3.3.23)
It is named as thinned CRM. The thinning function px .t/ can take many forms
including a bounded kernel and it controls the dependency between 	t and 	t0.
As an example in the case of a single-location thinned CRM they take px .t/ D
f .jx  tj/, where T D  and f W  ! Œ0; 1 a unimodal function. Just as a DP can
be constructed by normalizing a gamma process, the authors note that a DDP can
be constructed by normalizing a thinned gamma process. If  D T and the thinning
probability given by pxk .t/ D R 1
0
I Œjt  xkj  l f .l/ dl for some distribution f ./
over window size l, then after normalization, it yields the spatial normalized gamma
process. Another example of this approach is the kernel beta process developed
by Ren et al. (2011) and presented in the next chapter, Sect. 4.6.3. Thus it is clear
that different Levy measures and thinning functions give rise to different dependent
processes.
3.4
Poisson–Dirichlet Processes
The Dirichlet process was deﬁned as an RPM P on quite a general and arbitrary
measurable space X, and … was the collection of all such measures. Its countably
inﬁnite mixture representation was expressed as P D P1
jD1 pjıj. In contrast, this
section deals with a random discrete probability distribution p D . p1; p2; : : :/
deﬁned on a countably inﬁnite set and let … denote here the collection of such
discrete probability distributions on N or some translation of N. … may also be
considered as a collection of vectors p or a set of sequences . p1; p2; : : :/ of real
numbers subject to restrictions pi  0; i D 1; 2; : : : ; and P 1
iD1pi D 1. The
interest is in ﬁnding the joint distribution of the components of vector p or their
permutations, which arise naturally in many ﬁelds of applications. For example,
pi’s may be considered as the proportion of species in a population encountered in
ecology or in study of abundances of genes in population genetics, and the interest
is in their representation in the order of dominance among a sample drawn from the
population.
Recall that earlier we have dealt with a sequence p D . p1; p2; : : :/ ; where the
pi’s were constructed via the stick-breaking construction. Namely, p1 D v1; pn D
vn
Qn1
iD1 .1  vi/, n  2 and vi are independent and identically distributed on Œ0; 1.
We shall identify this sequence as SB sequence. It seems to have ﬁrst appeared in
McCloskey (1965). If in particular, vi
iidÏ Be.1; /, then the sequence is said to
have a GEM ./ distribution. In which case, pi’s were weights in the Sethuraman
representation. Now we deﬁne two additional sequences, which are permutations
of p.

3.4
Poisson–Dirichlet Processes
111
The ﬁrst one is a rank-ordered permutation of p in which pi’s are arranged in
descending order, and is denoted by p D

p.1/; p.2/; : : :

. Recall that the weights in
Ferguson’s alternative deﬁnition (see Sect. 2.1) of an RPM P form such an ordered
sequence. The second is a size-biased permutation of p obtained sequentially as
follows. Pick an element pi of p at random and set Qp1 D pi D pi.1/ (i.e.,
Qp1 D pi with probability pi). Remove pi.1/. Now pick pi at random from the
remaining components and set Qp2 D pi D pi.2/ ( i.e., Qp2 D piIŒ pi ¤ Qp1 with
probability piIŒ pi ¤ Qp1= .1  Qp1/). Continue this way. Then the resulting sequence
Qp D . Qp1; Qp2; : : :/ is said to be a size-biased random permutation of p. Formally,
Deﬁnition 3.1 Let p D . p1; p2; : : :/ be a sequence of real numbers subject to
restrictions pi  0; i D 1; 2; : : : ; and P 1
iD1pi D 1 and deﬁne a new sequence
Qp D . Qp1; Qp2; : : :/ as follows:
P . Qp1 D pi/ D pi; i D 1; 2; : : : and for k  2;
P

QpkC1 D pjjQp1; : : : ; Qpk; p

D pjIŒ pj ¤ Qpi; for all 1  i  k
.1  Qp1  Qp2; : : : ; Qpk/
; j D 1; 2; : : : :
(3.4.1)
Then Qp D . Qp1; Qp2; : : :/ is known as size-biased permutation of p D . p1; p2; : : :/.
The objective of this section is to describe the relationship between these
sequences and to introduce the distribution of p known as Poisson–Dirichlet dis-
tribution (Kingman 1975) involving one parameter, and its two-parameter extension
(Pitman and Yor 1997) which is also a two-parameter generalization of the Dirichlet
process. These distributions may be considered as priors on …; a subset of …
such that p1  p2     . These distributions may also be constructed by the
stick-breaking construction and thus are termed as Ferguson–Sethuraman type
distributions (atoms are treated as ﬁxed known constants).
3.4.1
One-Parameter Poisson–Dirichlet Process
In quest for the distribution of p, Kingman (1975) points out that it is impossible to
choose at random a p which is invariant under permutation of that set. Therefore his
approach is to consider ﬁrst the class of ﬁnite dimensional probability distributions
pN D . p1; p2; : : : ; pN/ and then letting N increase indeﬁnitely. He shows that
under appropriate conditions, the vector pN, rearranged in decreasing order, has
a nondegenerate limiting distribution involving one parameter and named it as a
Poisson–Dirichlet distribution. He gives an interesting motivator—the problem of
“heaps,” which may be described as follows.
Suppose we have a heap of N items, I1; I2; : : : ; IN stacked up on the desk.
Periodically, we seek an item which is Ik with probability pk and is searched through
the heap, starting at the top. After its use, it is placed back on the top of the heap and

112
3
Ferguson–Sethuraman Processes
a subsequent search starts. All searches are assumed to be independent. The process
is repeated, every time the item after use is being placed at the top. Eventually,
the system will stabilize and items will have been stacked up in the order of their
popularity, the most sought after item will tend to be placed on the top, second most
popular item will be placed next, and so on. This is essentially the rearrangement of
pi’s in decreasing order.
It also has ecological applications where pi in the random vectors p
D
. p1; p2; : : :/ may represent the proportion of i-th species i in an inﬁnite population,
and presumably, there are unlimited species. It is desired to ﬁnd the distribution of
the random vector p D

p.1/; p.2/; : : :

, where p.1/  p.2/;     and where p.k/ for
k D 1; 2; : : :, represents the proportion of k-th dominant species k encountered in
a sample draw.
Another way to look at the ordered values p.1/  p.2/;     is to recognize
them as ordered normalized independent increments of a gamma process with shape
parameter . That is if Y.i/ denotes the normalized and ordered size of jump of a
gamma process, then . p.1/; p.2/; : : : ; / may be considered same as .Y.1/; Y.2/; : : : ; /.
Or the sequence may also be viewed in terms of a Poisson process as follows. Let
T1 > T2 >    be the points of a Poisson process on .0; 1/ with mean measure
x1exdx. Let Pi D Ti= .P Ti/ ; i D 1; 2; : : : . Then P1 > P2 >    . Kingman
has shown that the ranked permutation vector p does converge in distribution, but
unfortunately its limiting distribution is intractable.
Let pN
D
. p1; : : : ; pN/ be a vector such that pi
 0; i
D
1; 2; : : : ; N
and PN
iD1pi D 1. The usual prior distribution taken for pN is D .˛1; : : : ; ˛N/,
the Dirichlet distribution with parameter vector .˛1; : : : ; ˛N/, on the .N  1/-
dimensional simplex. However, Kingman notes that if in particular all ˛’s are the
same and equal to ˛; then pi’s have an exchangeable symmetric distribution D
.˛; : : : ; ˛/, and E. pi/ D N1. When ˛ is large, the distribution tends to degenerate
at

N1; : : : :; N1
, while small values of ˛ indicate pj’s to be small but there is a
high probability that a few may not be small. In that case, what is the distribution
of pN? These observations led him to consider the asymptotic case resulting in a
distribution called Poisson–Dirichlet distribution deﬁned as:
Deﬁnition 3.2 (Kingman) The distribution of an inﬁnite sequence . p1; p2; : : :/ of
real numbers with p1  p2     > 0 and P1
iD1pi D 1; a.s. depends only on a single
parameter  and is called Poisson–Dirichlet distribution, denoted as PD ./.
It is difﬁcult to derive PD ./ directly. Therefore, consider the N-dimensional
vector pN having a symmetric Dirichlet distribution with parameter ˛ and then
passing to the limit N ! 1, ˛ ! 0 such that N˛ ! , it can be shown that
pN has a limiting distribution.
Theorem 3.3 (Kingman) For  > 0; there is a probability measure P on …
with the following property. For each N, the ﬁnite dimensional random vector
pN D . p1; p2; : : : ; pN/ subject to pi  0, i D 1; 2; : : : ; N and PN
iD1pi D 1, having
symmetric distribution D .˛; ˛; : : : ; ˛/ and N˛ ! , as N ! 1 and ˛ ! 0, then

3.4
Poisson–Dirichlet Processes
113
for any n; the distribution of the random vector

p.1/; p.2/; : : : ; p.n/

converges to the
n-dimensional marginal of P.
This theorem exhibits the limiting joint distribution of order statistics p.1/ 
p.2/;    . Thus PD ./ deﬁnes a prior on … just as the Dirichlet process is a
prior on …. Unlike the DP, however, the ﬁnite dimensional distribution of PD ./
is difﬁcult to describe explicitly. It is clear from the above that the decreasing order
statistics of the D .˛1; ˛2; : : : ; ˛n/ provide an approximation to those of PD ./ as
long as n is large and ˛i are small with their sum ˛1 C˛2 C  C˛n being close to .
Following Patil and Taillie (1977), Kingman (1993) gives a simple construction
of PD ./ in terms of size-biased sampling. Brieﬂy, let the random probability vector
pN D . p1; p2; : : : ; pN/ have the symmetric Dirichlet distribution D .˛; ˛; : : : ; ˛/.
Draw components of pN by size-biased sampling so that pN can be rearranged
as a vector qN with components expressed as (see the next theorem), q1 D v1,
qi D vi
Qi1
jD1
1  vj
, i  2 and vi’s are beta random variables with appropriate
parameters. Now taking the limit N ! 1 and ˛ ! 0 so that N˛ ! ; it is shown
that vi
iidÏ Be .1; / and that the rank order permutation q D

q.1/; q.2/; : : :

, has the
PD ./ distribution. This construction forms the basis of two-parameter Poisson–
Dirichlet distribution in which the distribution of SB ratios is replaced by more
general beta distribution.
Interestingly, the random sequences p; p, and Qp are in a way related and the stick-
breaking representation plays a critical role as the following theorem attest (Pitman
1996a).
Theorem 3.4 (McCloskey) Let Qp be a size-biased permutation of a sequence of
random variables P1 > P2 >    > 0 with P Pi D 1. Then Qpj can be represented as
Qpj D vj
j1
Y
iD1
.1  vi/
(3.4.2)
for a sequence of iid random variables vj if and only if the sequence fPig has
PD ./ distribution for some  > 0. The random variables vj then have a common
distribution Be .1; /.
This kind of representation is further explored in Perman et al. (1992) and forms
the basis of the deﬁnition of the two-parameter Poisson–Dirichlet distribution. They
provide formulae which explain why the size-biased permutation of the normalized
jumps of a subordinator can be represented, in certain cases, by a stick-breaking
scheme deﬁned by independent beta random variables.
The above theorem implies the following.
Corollary 3.5 Let a sequence f pg with p1  p2     be deﬁned by ranking f pg
having GEM ./ distribution. Then
(i) f pg has PD ./ distribution, and
(ii) f pg is a size-biased permutation of f pg.

114
3
Ferguson–Sethuraman Processes
Connection of these sequences with the residual allocation scheme was high-
lighted by Patil and Taillie (1977). As observed there, the rank-ordered permutation
of Engen’s model (in which residual fractions are iid Be .1; / ;  > 0) is equal in
distribution to the Kingman’s limit. And the size-biased permutation of Kingman’s
limit equals Engen’s model. Thus the two are permutations of each other and that
Engen’s model itself is invariant under the size-biased permutation.
These sequences when interpreted as weights or atom masses of Ferguson
and Sethuraman representation of the DP, the relationship among them may be
summarized loosely as
p
rank-perm
!
p.F-weights Ï PD/
SB-perm
!
Qp.S-weights Ï GEM/
rank-perm
!
.F-weights Ï PD/;
where F- and S-weights are weights in Ferguson and Sethuraman representations.
Recall that as a result of Polya urn characterization representation, the con-
ditional distribution of XnC1jX1; : : : ; Xn was expressed as in (2.1.26) and it was
indicated that limn!1

nj=n

D
pj the weight of j-th atom in Sethuraman
representation. This along with the above connection of p and Qp was expressed in a
different light by Pitman (1996a) as follows.
Theorem 3.6 (Pitman) Let F Ï D .MF0/ and F0 nonatomic. Let pi denote the
magnitude of i-th largest atom of F and i be its location in X. Locations are a.s.
distinct. Let X
j denote the j-th distinct value observed in a sample .X1; X2; : : :/ from
F and p
j D F.fX
j g/ the size of atom of F at X
j . Then a.s.
F D
X
i
piıi D
X
p
j ıX
j ;
(3.4.3)
where
(i) .p1; p2; : : :/ has PD .M/ distribution;
(ii) the i are iid .F0/, independently of .p1; p2; : : :/;
(iii)

p
1; p
2; : : :

is a size-biased permutation of .p1; p2; : : :/;
(iv)

p
1; p
2; : : :

has GEM .M/ distribution; and
(v) the X
j are iid .F0/, independently of

p
1; p
2; : : :

.
The key observation to be made here is that the permutation of weights carries
the corresponding locations along as well. This correspondence was exploited in
simulation methods discussed earlier in the last chapter.
The vector p is said to be invariant under size-biased permutation (ISBP) if the
sequence Qp obtained by size-biased permutation of p has the same ﬁnite dimensional
distributions as that of p. Characterization of such random distributions that are
ISBP is obtained by Pitman (1996b).

3.4
Poisson–Dirichlet Processes
115
McCloskey’s result in this regard is that for any sequence p with p1 D v1, pn D
vn
Qn1
iD1 .1  vi/, n  2 and vi are independent and identically distributed on Œ0; 1,
then p is ISBP if vi
iidÏ Be.1; /;   0; i  1. The “only if” part was established by
Pitman (1996b).
A sample of size n drawn from PD ./ will have ties. It will be composed of m1
singletons (i.e., belonging to distinct species), m2 pairs, m3 triplets, and so on, so
that Pn
jD1jmn D n. Let m D .m1; m2; : : : ; mn/. Then probability of observing such
a sample is given by
P .m D m/ D
nŠ ./
 .n C /
n
Y
jD1
	 mj
jmjmjŠ

;
(3.4.4)
and is known as Ewens’s (1972) sampling formula derived in the context of genetics.
This formula was independently discovered by Antoniak (1974) (see Eq.(2.1.20)).
This formula is generalized when sampling is from a two-parameter Poisson–
Dirichlet distribution and is given in Eq. (3.4.7).
The predictive distribution of a future observation from PD ./ is a special case
of the one given for the two-parameter Poisson–Dirichlet distribution below.
3.4.2
Two-Parameter Poisson–Dirichlet Process
As noted in Sect. 2.1 (property 21) the predictive distribution produced by the
Dirichlet process selects a new observation with probability M= .M C n/ and
coincides with one of the previous observations with probability n= .M C n/. These
probabilities do not depend on K or frequency nj missing out on some valuable
information. Inclusion of this information is achievable via the two-parameter
Poisson–Dirichlet distribution (also know as Pitman–Yor process) developed by
Pitman and Yor (1997) (see also, Perman et al. 1992, and Pitman 1995, 1996a,b)
as an extension of one-parameter Poisson–Dirichlet distribution. It is a probability
distribution over the set of decreasing sequences of positive numbers adding to 1.
It is also a two-parameter generalization of the Dirichlet process, along with other
generalizations mentioned earlier. The parameter  is replaced by two parameters:
a discount parameter ˛, and a concentration parameter 
, such that 0  ˛ < 1
and 
 > ˛ and the distribution is denoted by PD.˛; 
/. The discount parameter
˛ governs the power-law behavior which makes this process more suitable than the
Dirichlet process for many applications. If ˛ D 0; then PD.0; 
/ is the Dirichlet
process D .
/; and if 
 D 0, it yields a random probability whose weights are
based on a stable law with index 0 < ˛ < 1. It may be constructed using the
stick-breaking construction in the same way as the one-parameter Poisson–Dirichlet
Distribution mentioned above, where the iid variables vi having the distribution
Be.1; / are replaced by independent variables Vi such that Vi Ï Be.1  ˛; 
 C i˛/,
for i D 1; 2; : : :. Consonant with the constructive deﬁnition of PD ./, Pitman and

116
3
Ferguson–Sethuraman Processes
Yor (1997) give the following deﬁnition of PD .˛; 
/ in terms of independent beta
random variables.
Deﬁnition 3.7 (Pitman and Yor) For 0  ˛ < 1 and 
 > ˛, let a probability
P
;˛ govern independent random variables Vk with Vk Ï beta .1  ˛; 
 C k˛/ and
set Qp1 D V1, Qpn D Vn
Qn1
kD1.1  Vk/, n  2. Further let p1  p2     be the
ranked values of Qp1; Qp2; : : :. Then the Poisson–Dirichlet distribution with parameters
.˛; 
/, denoted as PD .˛; 
/, is deﬁned to be the P
;˛ distribution of the sequence
. p1; p2; : : :/.
Thus PD .˛; 
/ is a two-parameter prior distribution on …. This deﬁnition
reveals that the sequence .Qp1; Qp2; : : :) obtained by the size-biased random permu-
tation of . p1; p2; : : :/ is a simple Engen’s (1975) residual allocation model.
As parallel to the case of Poisson–Dirichlet distribution, it is shown that under
P
;˛ governing .V1; V2; : : :/ ; the sequence . p1; p2; : : :/ is such that p1 > p2 >   
and P1
nD1 pn D 1 a.s. and .Qp1; Qp2; : : :/ is a size-biased permutation of . p1; p2; : : :/.
It is further concluded that if . p1; p2; : : :/ is any sequence having PD .˛; 
/
distribution with 0  ˛ < 1 and 
 > ˛ and Qp a size-biased permutation of p and
Vn deﬁned as Vn D Qpn=.Qpn C QpnC1 C    /; then the sequences p, Qp and .V1; V2; : : :/
have the same distributions as those in the deﬁnition. Vn D Qpn=.1 Qp1; : : : ; Qpn1/
can also be viewed as residual fractions.
Thus as in the case of one-parameter Poisson–Dirichlet distribution, it is shown
here that the joint distribution of ranked values of size-biased picks is the Poisson–
Dirichlet distribution; and that the size-biased permutation of rank-ordered elements
of p having this distribution produces a sequence of pi’s derived by the residual
allocation scheme; also the random weights in inﬁnite mixture representation of a
RPM have similar interpretation as in one-parameter Poisson–Dirichlet distribution.
As PD .˛; 
/ is a two-parameter generalization of the Dirichlet process, it offers
more ﬂexibility and may offer tremendous advantage over the Dirichlet process
in some data modeling. Unlike the DP, it is not a normalized completely random
measure since the variable Vk depends on k. Various applications discussed in
Chaps. 6 and 7 may be reworked with replacing the Dirichlet process there with
PD .˛; 
/. Its application in hierarchical modeling is discussed in Teh and Jordan
(2010).
Pitman and Yor also give a Poisson process characterization which is essentially
a generalization of the same given for the DP mentioned in Sect. 2.1. However,
its complexity makes it unfavorable for practical use as was the case for the DP.
Whereas, the SB construction is easy to use and as such it is rightly regarded as a
tremendous innovation (Ishwaran and James 2001).
Characterization Suppose we have a random sample from an unknown RPM P
having a PD .˛; 
/ distribution, i.e., XijP
iidÏ P; i D 1; 2; : : : ; n; P Ï PD .˛; 
/. The
sample will have some ties. Let X
1 ; : : : ; X
K be the K distinct observations among

3.4
Poisson–Dirichlet Processes
117
the sample and n1; : : : ; nK be their multiplicities, respectively. Integrating out P, the
marginal distribution of Xi’s will satisfy the following prediction rule (Pitman 1995).
XnC1jX1; X2; : : : ; Xn Ï
K
X
iD1
ni  ˛

 C n ıX
i C 
 C K˛

 C n H;
(3.4.5)
where X
i
iidÏ H, H nonatomic. From this it is clear that the Poisson–Dirichlet
process can be characterized in terms of a generalized Polya urn scheme. Given
X1; X2; : : : ; Xn, choose XnC1 at the .n C 1/-th step equal to one of the previous
observation X
i with probability .ni  ˛/ = .
 C n/ ; i D 1; 2; : : : ; K; and as a new
observation with probability .
 C K˛/ = .
 C n/. Thus the probability that XnC1
will be a new distinct observation depends upon the number of clusters K and
is monotonically increasing in K. ˛ serves as the moderating parameter for this
dependence. The distinct values X
1 ; : : : ; X
K may be treated as clusters. Since the
probability of XnC1 belonging to the i-th cluster is proportional to its size ni; and
if ni is small the effect of ˛ will be signiﬁcant and will tend to keep small clusters
relatively small. On the other hand, if the cluster size is large, ˛ being less than 1
will have negligible effect and large clusters will continue to grow. Its special case,
˛ D 0 and 
 D ˛ ./ corresponds to the Dirichlet process with parameter ˛ and
yields the Blackwell–MacQueen (1973) predictive rule.
The formula for pn;K in the case of PD .˛; 
/ is derived by Pitman (1995) as
p .n/ D pn;K.n1; : : : ; nKjK/ D
QK1
iD1 .
 C i˛/
.
 C 1/.n1/
K
Y
jD1
.1  ˛/.nj1/ ;
(3.4.6)
where as before, s.n/ D .s C n/=.s/ D s.s C 1/ : : : :.s C n  1/. When ˛ D 0; it
reduces to the formula presented in the section on the DP (property 18).
This probability may also be expressed alternatively. Suppose in applications
to population genetics, a sample of size n is classiﬁed in terms of the number of
different species, mi  0 consisting of i animals, i D 1; : : : ; n; represented in the
sample, then Pn
iD1mi D K and Pn
iD1imi D n. This is same as in the context of
partitioning n integers, where mi represents the number of cells in the partition which
contains i integers, i D 1; : : : ; n. Then we get (Pitman 1995)
P .m D m/ D nŠ
QK1
iD1 .
 C i˛/
.
 C 1/.n1/ Qn
iD1miŠ
n
Y
jD1
 
.1  ˛/. j1/
jŠ
!mj
;
(3.4.7)
known as Pitman’s sampling formula. This is a two-parameter extension of Ewens’s
(1972) formula (3.4.4). When ˛ ! 0, we get Ewen’s formula, which was also
discovered by Antoniak (1974) as given in Sect. 2.1, property 20.

118
3
Ferguson–Sethuraman Processes
Updating Rule Recall that in the case of the DP, given a sample X1; X2; : : : ; Xn
from F, and F Ï D .˛/, the conditional distribution of F was simply D .˛n/ and the
updating rule was to change the parameter ˛ to ˛n D ˛CPn
iD1ıXi. Similarly, Pitman
provides the following updating rule for PD .˛; 
/. Given a sample Y1; Y2; : : : ; Yn
from P and P Ï PD .˛; 
/, the posterior random measure may be approximated by
a ﬁnite sum
P.jY/ D
K
X
jD1
p
j ıY
j ./ C p
KC1Q ./ ,
(3.4.8)
where Y
1 ; : : : ; Y
K are as before K distinct observations with multiplicities
n1; : : : ; nK; respectively,

p
1; : : : ; p
K; p
KC1

Ï D .n1  ˛; : : : ; nK  ˛; 
 C K˛/
(3.4.9)
which is independent of random measure Q, and Q Ï PD .˛; 
 C K˛/.
An in-depth investigation of PD .˛; 
/ has been carried out by Pitman and Yor
(1997). Also related papers by Perman et al. (1992) and Pitman (1995, 1996b) shed
more light on this distribution.
Applications of the Poisson–Dirichlet distribution in hierarchical modeling,
approximation to the posterior distribution given a sample from it, and computa-
tional aspects using Gibbs sampling algorithm, are discussed in Ishwaran and James
(2001).
Gibbs Sampler for PD .˛; / Consider the hierarchical model:
XijYi;'
ind
  .XijYi;'/ ; i D 1; : : : ; n
YijP
iid P; P  PD .˛; 
/ ; '   .'/ :
Integrating out P; one can create a marginalized model
XijYi;'
ind
  .XijYi;'/ ; i D 1; : : : ; n
.Y1; : : : ; Yn/   .Y1; : : : ; Yn/
'   .'/ ;
where  .Y1; : : : ; Yn/ denotes the joint distribution for Y D .Y1; : : : ; Yn/ deﬁned by
the underlying Polya urn scheme. Letting g. p/ to be any positive integrable function
on .…;  .…// ; Ishwaran and James proved a generalization of Lo’s (1984) result
as follows:
Z
…
g .P/ P .dPjX/ D
Z
Xn
Z
…
g .P/ P .dPjY/  .dYjX/ ;
(3.4.10)

3.5
Species Sampling Models
119
where
 .dYjX/ D
 .dY/ R Qn
iD1 .XijYi;'/  .d'/
R R Qn
iD1 .XijYi;'/  .d'/  .dY/
(3.4.11)
and  .Y/ is the joint distribution of Y determined by the generalized Polya urn
scheme given above, and integrations are deﬁned over appropriate domains. The
posterior distribution P .dPjY/ was given above (3.4.8).
Then the marginal approach for Gibbs sampling algorithm for PD .˛; 
/ is as
follows: To draw values from the posterior distribution  .Y; 'jX/ of the above
model, we draw iteratively from the conditional distributions of
YijYi; '; X; i D 1; : : : ; n;
'jY; X:
(3.4.12)
(a) The conditional distributions are given by
P fYi 2 jYi; '; Xg D
m
X
jD1
q
j ıY
j ./ C q
0P fYi 2 j'; Xig ;
where
q
0 / .
 C ˛m/
Z
 .XijY; '/ H .dy/ ; q
j / .n
j  ˛/ XijY
j ; ' ; j D 1; : : : ; m;
and
˚
Y
1 ; : : : ; Y
m

are distinct values in Yi with Y
j having frequency n
j , j D
1; : : : ; m.
(b) draw from the density of 'jY; X
 .'jY; X/ /  .d'/
n
Y
iD1
 .XijYi; '/ :
For the conditional approach, one can use the Blocked Gibbs sampler described
earlier in Sect. 2.4 with some obvious modiﬁcations.
3.5
Species Sampling Models
While discussing mixture and hierarchical models earlier, it was pointed out that the
discreteness of the RPM P under the DP prior served as an advantage in Bayesian
analysis of such models. Another area where discreteness also plays a natural role is
in ecology and population genetics dealing with sampling from a large population of
individuals of various species. This connection was explored in a series of papers by

120
3
Ferguson–Sethuraman Processes
Jim Pitman and his colleagues. That development was named as species sampling
models that we brieﬂy describe in this section.
We have seen earlier that if X1; : : : ; Xn is sample of size n from P and P 
D .˛/, then Ferguson (1973) proved that the conditional distribution of XnC1 given
X1; : : : ; Xn is given by
P .XnC1 2 jX1; : : : ; Xn/ D ˛ ./ C Pn
iD1ıxi ./
˛ .X/ C n
D ˛n ./
˛n .X/;
(3.5.1)
where ˛n D ˛ C Pn
iD1ıXi. Its connection to the Polya urn scheme was mentioned
in Sect. 2.1. Note that here the above prediction rule (3.5.1) for a new observation
emerged as a consequence of iid sampling from P having the DP prior. The other
way is also true as the Blackwell and MacQueen theorem (Theorem 2.4) shows: the
prediction rule (3.5.1) characterizes the DP process.
Blackwell and MacQueen (1973), start with a sequence fXn W n  1g of random
variables taking values in X, along with the following prediction rule
P .X1 2 / D ˛ ./ =˛ .X/ and
P .XnC1 2 jX1; : : : ; Xn/ D ˛n ./
˛n .X/;
(3.5.2)
and call the sequence as a Polya sequence with parameter ˛. With this tool, they
proved their fundamental theorem (see Sect. 2.1) which says that
(a) The sequence ˛n ./ =˛n .X/ converges with probability one as n ! 1 to a
limiting discrete measure P;
(b) P is the Dirichlet process with parameter ˛; and
(c) given P, X1; X2; : : : are independent with distribution P.
The conditional probability (3.5.2) is known as the predictive rule for the Polya
urn sequence. The key point here is that this predictive rule not only produces an
exchangeable sequence but also identiﬁes the underlying RPM, as the DP. This
interesting result raises a question: are their other type of schemes and predictive
rules which lead to an exchangeable sequence, and do they characterize the
underlying RPM? Pitman (1996b) and Hansen and Pitman (2000) investigated this
question and developed sufﬁcient and necessary conditions in terms of a symmetric
nonnegative function called exchangeable partition probability function (EPPF).
This question has also been explored by Ishwaran and Zarepour (2003) and based on
a modiﬁed urn scheme they provide a sufﬁcient condition to ensure exchangeability
of the resulting sequence, and as a consequence, the limit of the sequence is a
discrete RPM. Thus, in a way they generalize Blackwell and MacQueen theorem.
They assume ˛ to be nonatomic, although it was not necessary for the Polya
sequence.
Since P is discrete a.s., there will be ties among the sample drawn from it
with probability one. Denote the k .n/ D k distinct values (distinct species when

3.5
Species Sampling Models
121
sampling from a large population) among the sample X1; : : : ; Xn by X
1 ; : : : ; X
k.n/; in
the order they appear and n1n; : : : ; nk.n/n their respective frequencies. This could be
thought of as if the distinct values creating a partition of sample labels f1; 2; : : : ; ng
into k classes or clusters Cj D
n
i W Xi D X
j
o
, with cluster sizes n1n; : : : ; nk.n/n. Thus
the conditional distribution can be written as
P .XnC1 2 jX1; X2; : : : ; Xn/ D
k
X
jD1
njn
˛ .X/ C nıX
j ./ C
˛ .X/
˛ .X/ C n˛ ./ ;
(3.5.3)
where ˛ ./ D ˛ ./ =˛ .X/ as before. Note that the point masses associated with X
j
depend exclusively on the frequency of X
j and not on the value of X
j , nor on the
frequencies of other distinct values X
i .
Thus, the rule (3.5.3) can be written in a more general form in terms of the masses
which are functions of the cluster sizes. For some nonatomic distribution G0; let
P .X1 2 / D G0 ./ ; and
P .XnC1 2 jX1; X2; : : : ; Xn; k .n/ D k/ D
k
X
jD1
pj .nn/ ıX
j ./ C pkC1 .nn/ G0 ./ ;
(3.5.4)
where nn D

n1n; : : : ; nk.n/n

and pj .n/ D P

XnC1 D X
j jnn D n

, 1  j 
k .n/ with n D .n1; : : : ; nk/, a vector of frequencies of X
j ’s, pj .n/  0 and
Pk.n/C1
jD1
pj .n/ D 1. Equation (3.5.4) is known as the prediction rule (PR). Given
that n observations results in the vector n; pj .n/ here is the probability that the next
observation is the j-th species already observed for 1  j  k .n/, and pkC1 .n/ is the
probability that it is a new species. pj .n/ plays a special role in the PR and is some
symmetric function of ﬁnite sequences of frequencies n known as the predictive
probability function (PPF). It provides a clue that a sequence of functions pj .n/
with the above constraints determine the distribution of the sequence fXng via the
above predictive rules. For example, in the case of Polya sequence,
pj .n/ D
nj
˛ .X/ C nI Œ1  j  k C
˛ .X/
˛ .X/ C nI Œj D k C 1 ;
(3.5.5)
with n D Pk
jD1 nj; and in the case of two-parameter Poisson–Dirichlet distribution
PD .˛; 
/,
pj .n/ D nj  ˛
n C 
 I Œ1  j  k C 
 C k˛
n C 
 I Œ j D k C 1 :
(3.5.6)
Starting with an exchangeable sequence and a predictive rule in general form
of (3.5.4), Pitman (1996b) proved the following theorem which characterizes the

122
3
Ferguson–Sethuraman Processes
general form of RPM G, essentially generalizing the Blackwell–MacQueen theorem
proved for the Polya urn sequence.
Theorem 3.8 (Pitman) Let fXng be an exchangeable sequence subject to a predic-
tion rule of the form (3.5.4) and let Gn denote the conditional distribution of XnC1
given X1; X2; : : : ; Xn deﬁned by (3.5.4). Then (1) Gn
a:s:
! G (in total variation norm)
where
G D
X
j
PjıX
j C
0
@1 
X
j
Pj
1
A G0;
(3.5.7)
and where Pj is the frequency of j-th species to appear, i.e., Pj D limn!1

nj=n

;
(2) the X
j
iidÏ G0 and independent of Pj; and (3) given G; .X1; X2; : : :/ is a sample
from G.
The key differences in relation to the Blackwell and MacQueen result are that
here exchangeability is assumed to start with unlike in Blackwell and MacQueen
theorem where it is a consequence, and further the RPM G is not identiﬁed here as
the DP as is done in the Blackwell and MacQueen paper.
In the context of species sampling, suppose that a random sample X1; X2; : : : is
drawn from a large population of individuals of various species, where Xi represents
species of the i-th individual drawn, and the j-th distinct species to appear is denoted
by X
j . When governed by a prediction rule, Pitman (1996b) calls this sequence as
a species sampling sequence (SSS).
Deﬁnition 3.9 An exchangeable sequence fXng is called a species sampling
sequence (SSS) if the PR (3.5.4) holds true and G0 is a nonatomic distribution.
In view of the above theorem, it follows that fXng is a SSS iff fXng is a sample
from a random distribution function G that admits a representation of the form
G D
X
j
Pjıj C
0
@1 
X
j
Pj
1
A G0;
(3.5.8)
for some sequence of random variables
˚
Pj

such that Pj  0 for all j and P
j Pj  1
a.s., and some sequence j
iidÏ G0, independent of Pj. This shows that the PR of an
SSS characterizes an RPM. Such a setup with a random distribution G of the above
form and a sample .X1; X2; : : :/ from G is called a species sampling model (SSM).
Pj can be interpreted as the relative frequency of the j-th species in a population and
j as tag assigned to it. G has an atom Pj at j for each j such that Pj > 0 and rest of
the mass distributed according to G0. The model is said to be proper if P
j Pj D 1

3.5
Species Sampling Models
123
a.s. so that
G D
X
j
Pjıj D
X
j
PjıX
j
(3.5.9)
where Pj and X
j are as in (3.5.7) deﬁned in terms of a sample from G.
In general, the RPM G can be deﬁned directly by specifying the weights subject
to the condition that they add to one, and the distribution of point masses. It can
alternatively deﬁned by specifying the sequence
˚
pj .n/ W 1  j  k .n/ C 1

of its
PPF as indicated above in (3.5.5) and (3.5.6).
A third alternative to characterize an SSM is through the prior on the sequence
of random partitions (Pitman 1996b; Hansen and Pitman 2000). Suppose fXng is an
exchangeable sequence admitting a prediction rule of type (3.5.4) and let …n be the
partition of f1; 2; : : : ; ng generated by the duplicate values of .X1; : : : ; Xn/. Then for
each n and 1  j  k .n/ ; pj .n/ and pkC1 .n/ are almost surely some functions of
…n, the induced random partition of f1; 2; : : : ; ng, with nn D

n1; n2; : : : ; nk.n/

, a
vector of cluster sizes, is also exchangeable. (Gnedin and Pitman (2007) show that
any PPF with weights that are functions of the cluster size only must be essentially
of form pj _ nj.) Therefore, it sufﬁces to specify the probability of nn for all possible
values of nn; deﬁning a prior p .nn/ over the set N D [1
kD1Nk. This prior is known
as exchangeable partition probability function (EPPF) and must satisfy Kolmogorov
consistency criteria,
p .n/ D
k.n/C1
X
jD1
p

n jC
for all n 2N and p .1/ D 1;
(3.5.10)
where n jC denotes n with the j-th component incremented by 1. Thus p W N !
Œ0; 1.
Converse is also true. Pitman (his Proposition 13) shows that any function
regarded as EPPF satisfying (3.5.10), and  a diffused probability distribution, there
is a unique distribution for an SSS fXng such that p in (3.5.10) is the EPPF of fXng
and  is the distribution of X1. Thus he [see also Hansen and Pitman (2000)] gives
the following characterization of an exchangeable sequence.
Theorem 3.10 (Pitman) Given  a diffused measure and a sequence of functions
˚
pj .n/

deﬁned on N satisfying pj .n/  0 and Pk.n/C1
jD1
pj .n/ D 1, let fXng govern
by the prediction rule (3.5.4). The sequence is exchangeable iff there exists p .n/
deﬁned on N such that pj .n/ D p

n jC
=p .n/ holds. Then fXng is a sample from
G and EPPF of fXng is the unique nonnegative symmetric function p .n/ such that
pj .n/ D p

n jC
=p .n/ holds and p .1/ D 1.

124
3
Ferguson–Sethuraman Processes
PPF can obviously be deﬁned via EPPF as pj .n/ D p

n jC
=p .n/. The PPF for
the PD .˛; 
/ distribution is
XnC1jX1; X2; : : : ; Xn D
k.n/
X
jD1
njn  ˛

 C n ıX
j .xnC1/ C 
 C k .n/ ˛

 C n
G0 .xnC1/ ;
and EPPF reduces to
p .n/ D
 .
 C 1/
.
 C k .n/ ˛/  .
 C n/
k.n/
Q
jD1
f.
 C k .n/ ˛/g 

njn  ˛

 .1  ˛/ :
(3.5.11)
EPPF for the DP given in Sect. 2.1 can be obtained by setting ˛ D 0 in the above
expression.
Ishwaran and Zarepour (2003) also provide a sufﬁcient condition to ensure
exchangeability of a sequence in terms of the probabilities of selecting new values or
choosing a previously sampled value under a modiﬁed Polya urn scheme. Consider
a Polya urn sequence X1; X2; : : : with parameter  whose distinct values in order
of appearance are X
1 ; X
2 ; : : : with frequencies n1; n2; : : :, respectively, generated by
the following predictive rule:
P .X1 2 / D  ./
P .XnC1 2 jX1; X2; : : : ; Xn/ D
k.n/
X
jD1
qj .n/
Pk.n/
iD0qi .n/
ıxi ./ C
q0 .n/
Pk.n/
iD0qi .n/
 ./ ;
(3.5.12)
where k .n/ denotes the number of distinct values among the ﬁrst n members
of the sequence, qj .n/, i
D
0; : : : ; k .n/, are symmetric functions of n
D

n1; n2; : : : ; nk.n/

. Their sufﬁciency condition is:
Condition: For each n  1; qi .n/ D  .nn;i/ and q0 .n/ D  0 .nn;0/, where  
and  0 are some ﬁxed nonnegative real valued functions; and for each partition n of
f1; : : : ; ng,
k.n/
X
iD0
qi .n/ D  .n/ > 0
for  some ﬁxed real valued function.
Then it is shown that if this condition holds, then the sequence X1; X2; : : : is
exchangeable. Among their examples are .1/ sequence of iid random variables
are obviously exchangeable with q0 .n/ D 1 and qi .n/ D 0, i D 1; : : : ; k .n/;
.2/ Blackwell–MacQueen sequence correspond to q0 .n/ D  .X/ ; qi .n/ D ni and
Pk.n/
iD0qi .n/ D  .X/ C n; .3/ the two-parameter Poisson–Dirichlet process with
parameters 
 and ˛ corresponds to q0 .n/ D 
 C k .n/ ˛ and qi .n/ D ni  ˛;

3.5
Species Sampling Models
125
where 0  ˛ < 1 and 
> ˛. In this case Pk.n/
iD0qi .n/ D 
 C nI and
.4/ ﬁnite dimensional Dirichlet priors. Let N > 1 be a positive integer and let
q0 .n/ D 
 .1  n=N/ I fn < Ng and qi .n/ D ni C 
=N, where 
 > 0. Then
Pk.n/
iD0qi .n/ D 
 C n. This leads to the generalization of Blackwell–MacQueen
theorem: If we have a sequence deﬁned by the prediction rule in (3.5.12), then there
exists a discrete RPM P such that the conclusion of Blackwell–MacQueen theorem
holds true, but P need not be a DP.

Chapter 4
Priors Based on Levy Processes
4.1
Introduction
The Dirichlet process was deﬁned on an arbitrary space X and involved only
one parameter, ˛: Thus, with speciﬁcation of ˛, the prior is completely identiﬁed
and the Bayesian analysis can proceed. However, it can also be construed as a
limitation. Doksum (1974) introduced a signiﬁcant generalization of the Dirichlet
process on the real line, R; by deﬁning a class of priors called neutral to the
right (NTR) processes. It is deﬁned in a distinctly different manner. The Dirichlet
process was deﬁned in terms of the joint distribution of probabilities of sets which
formed a measurable partition of X. The neutral to the right process is based on
the independence of successive normalized increments of a distribution function
F. It has close connection with the processes with independent nonnegative (will
be dealing with only nonnegative) increments and can be deﬁned in terms of an
independent increment process. In fact it is shown that F is neutral to the right
if and only if the process Yt D  log .1  F .t// is a nondecreasing process with
independent increments. This allows us to specify a wide choice of family of NTR
prior processes, one for each independent increment process. Several processes
discussed in this chapter belong to this family of Levy processes.
By writing H.t/ D  log .1  F .t// ; Kalbﬂeisch (1978) considered a class
H of cumulative hazard functions (CHFs) and deﬁned an independent increment
process as a prior over this class (and consequently on F) in which the increments
were taken to be distributed as gamma distribution, resulting in a gamma process
prior. To allow the case when F may not have a density, Hjort (1990) deﬁnes
the CHF as A.t/ D
R
Œ0;t
dF.s/
FŒs;1/ and considered the class of CHFs. He deﬁnes
a prior over this class via the independent increment process (IIP) and assumes
independent increments to be distributed approximately as beta distribution. The
resulting prior process was named as beta process. Walker and Mallick (1997a)
deﬁne a class of priors known as beta-Stacy processes directly over F by taking
a particular independent increment process which they call as log-beta process,
© Springer International Publishing Switzerland 2016
E.G. Phadia, Prior Processes and Their Applications, Springer Series in Statistics,
DOI 10.1007/978-3-319-32789-1_4
127

128
4
Priors Based on Levy Processes
whose increments are distributed as beta-Stacy distribution. If F is an NTR, then
irrespective of the deﬁnition of CHF, both H and A turn out to be independent
increment processes. In the beta process, the focus was on the cumulative integral
of the sample realization of the process. Thibaux and Jordan (2007) found it useful
to take the realizations themselves in deﬁning a beta-Bernoulli process which was
then followed by stable-beta and kernel beta processes. It is interesting to note that
some of these processes can be constructed by the SB construction as will be noted.
It is well known that if the ﬁxed points of discontinuities and the deterministic
part of an independent increment process are removed then it can be characterized
by its Levy measure. This makes it convenient to study the NTR processes via their
Levy measures. A further generalization is possible and useful. Instead of viewing
priors as processes on the real line, they can be studied on abstract spaces via the
completely random measures (CRMs) introduced by Kingman (1967), and the fact
that such measures can be constructed using the Poisson processes. This approach
was espoused in Lijoi and Prünster (2010). The advantage is that it provides a
uniﬁed theory. However, it does not provide any insight into their origination. For
this reason, original derivations will be described here.
Since the neutral to the right processes as well as other processes discussed here
is intimately connected to independent increment processes it is instructive to review
brieﬂy the independent increment processes ﬁrst. For the same reason, the CRM,
Poisson process, and related theorems are useful to review brieﬂy before using
them in describing various processes. Following these short digression, several prior
processes along with their properties and posterior distributions, will be presented in
more details. Since the neutral to the right process plays a central role, it is described
ﬁrst and more space is devoted to it. Thereafter, gamma, extended gamma, beta,
beta-Stacy, and Indian buffet processes are presented.
4.1.1
Nondecreasing Independent Increment Processes1
A nondecreasing process with independent increments (also called additive process,
positive Lévy process, and subordinator) is a continuous time version of the sum of
independent random variables. It is deﬁned as fYtgt2T where T is an interval, mostly
taken as .0; 1/ ; and the increments of fYtg are nonnegative and independent. It
plays an important role in nonparametric Bayesian analysis. Ferguson’s (1973,1974)
alternative deﬁnition of the Dirichlet process, which is contrary to one’s intuition,
is described through an independent increment process and the corresponding Lévy
measure. As will be seen later on that several other prior processes also emerge from
1Part of the material of this and the next two subsections is based on Ferguson (1974), Ferguson
and Phadia (1979), and Ferguson’s unpublished notes which clarify and provide further insight into
the description of the posterior processes neutral to the right. I am grateful to Tom Ferguson for
passing on his notes to me which helped in developing these sections.

4.1
Introduction
129
nondecreasing processes with independent increments (and their associated Lévy
measures). In fact all that is needed is an inﬁnitely divisible random variable and
a baseline distribution function (Doksum 1974). Besides the processes neutral to
the right, they include beta, beta-Stacy, and gamma processes. Another important
advantage is that it can be used in describing the posterior distributions and in
treating inference problems involving right censored data. In view of this, we
ﬁrst brieﬂy discuss the stochastic processes with independent increments (rigorous
treatment of which may be found in any standard textbook on probability) and
connect them with the above mentioned processes.
A stochastic process Yt with t 2 R has independent increments if for every
positive integer n and for every real numbers t0 < t1 <    < tn the increments
Yt1  Yt0; Yt2  Yt1; : : : ; Ytn  Ytn1 are stochastically independent. We consider
below only processes with independent increments for which the sample paths Yt
are nondecreasing with probability one. Thus each increment Yt  Ys for t > s
is nonnegative with probability one. The process may have at most countable
number of ﬁxed points of discontinuities. Let St be the height of the jump at t
which is nonnegative with probability one and may have any distribution without
disturbing the independence of the increments. Thus the processes with independent
increments have two components: one component is the process in which positive
increments (jumps) and the location points are both random, where as in the second
component, only the jumps are random but the locations are ﬁxed. This may happen
a priori as a part of the prior process or as a result of a sampled observation.
However, for a nondecreasing process with independent increments without ﬁxed
points of discontinuity, the distributions of the increments are known to be restricted
to the class of inﬁnitely divisible laws, which may be worthwhile to review brieﬂy
ﬁrst.
Inﬁnitely Divisible Laws
A random variable X is said to be inﬁnitely divisible, and its law is said to be an
inﬁnitely divisible distribution, if for every positive integer n it can be represented
as a sum
X D Xn1 C Xn2 C    C Xnn;
where Xn1; Xn2; : : : ; Xnn are independent and identically distributed random vari-
ables. Here we are mainly concerned with the case where the inﬁnitely divisible
random variable is nonnegative. In such a case, the one-sided Laplace transform of
the distribution exists and has a very simple form. (See, for example, Feller 1966,
Chap. XIII.7.) Its logarithm form for 
  0 is
 .
/ D log E e
X D 
b C
Z 1
0
.e
z  1/dN.z/;
(4.1.1)

130
4
Priors Based on Levy Processes
where the location parameter b  0, and N is a measure, called the Lévy measure,
on the open interval .0; 1/ such that
Z 1
0
zdN.z/ < 1 and
Z 1
1
dN.z/ < 1:
(4.1.2)
Some simple well-known examples of a nonnegative inﬁnitely divisible random
variable are
1. The Poisson random variable X with parameter  deﬁned on 0; c; 2c; : : : ; c > 0;
and having a probability mass function P.X D nc/ D en=nŠ, n D 0; 1; 2; : : :,
has the log Laplace transform  .
/ D .e
c  1/ which is (4.1.1) with b D 0
and Lévy measure that assigns mass  to the point c.
2. The gamma random variable X deﬁned on .0; 1/ with density
f.x/ D .˛/1ˇ˛ex=ˇx˛1I.0;1/.x/; ˛ > 0 and ˇ > 0;
has the log Laplace transform  .
/ D ˛ log.1Cˇ
/ which may be represented
in the form of Eq. (4.1.1) as
 .
/ D ˛
Z 1
0
.e
z  1/ez=ˇz1dz
with b D 0 and the Lévy measure as dN.z/ D ˛ez=ˇz1dz . This measure gives
inﬁnite mass to the positive axis but does satisfy conditions (4.1.2).
3. The random variable X D  log Y; where Y Ï Be.˛; ˇ/; has the Log-Beta
distribution with density
f.x/ D .˛ C ˇ/
.˛/.ˇ/e˛x.1  ex/ˇ1I.0;1/.x/,
and the log Laplace transform of X;
 .
/ D log .˛ C ˇ/ C log .˛ C 
/  log .˛ C ˇ C 
/  log .u/;
which may be represented in the form of Eq. (4.1.1) as
 .
/ D
Z 1
0
.e
z  1/e˛z.1  eˇz/
.1  ez/z
dz
with b D 0 (see Ferguson 1974, Lemma 1). Again the corresponding Lévy
measure
dN.z/ D e˛z.1  eˇz/
.1  ez/z
dz
is not ﬁnite yet satisﬁes conditions (4.1.2).

4.1
Introduction
131
Other examples include the negative binomial distribution, and the completely
asymmetric stable distributions with characteristic exponent less than unity.
Sampling an ID Distribution
In applications, we need to sample an ID distribution. Bondesson (1982) seems to
be the ﬁrst one to develop an algorithm to generate random variables from an ID
distribution given the Levy measure in its Laplace transform form. His procedure is
as follows. Suppose we want to sample X from an ID distribution F. In that case F
has Laplace transform without the deterministic part as
.s/ D exp
Z 1
0
.esz  1/dN.z/

;
where the Levy measure satisﬁes N..1; 1// < 1 and
R 1
0 zdN.z/ < 1: If
 D N..0; 1// < 1; deﬁne a d.f. G .x/ D 1N..0; x/: Let  .s/ be the Laplace
transform of G and   Poisson ./ : Then .s/ D expf. .s/  1/g is the
Laplace transform of random variable X D P
iD1 Yi; where Yi are iid with d.f. G
and independent of : This suggests a method of simulating F: However, when  is
not ﬁnite, this method does not work and Bondesson develops a more general and
ﬂexible method. Let fZ .u/ W u > 0g be a family of nonnegative independent random
variables and Ti; i D 1; 2; : : : be the points in increasing order in an independent
Poisson point process with rate  on RC; and set X D P1
iD1 Z .Ti/ : It is then shown
that the Laplace transform of X is given by
.s/ D E Œexp fsXg D exp


Z 1
0
. .sI u/  1/du

;
(4.1.3)
where  .sI u/ D E Œexp fsZ .u/g with Z .u/  H .zI u/. Changing the order of
integration, the above expression can be written as
.s/ D exp


Z 1
0
.esz  1/
	Z 1
0
H .dzI u/ du


:
(4.1.4)
Now suppose that for every u > 0 we can ﬁnd a suitable distribution H .zI u/ on RC
and a  such that

Z 1
0
H .dzI u/ du D N .dz/ :
(4.1.5)
Then one can simulate points Ti of a Poisson ./ process and then values Z .Ti/ from
the distribution functions H .xI Ti/ and set X D P1
iD1 Z .Ti/ : Then X will have the
desired ID distribution. In particular, we may deﬁne H .zI u/ D H .z=g .u// where

132
4
Priors Based on Levy Processes
g .u/ is a nonnegative function. By taking H to be degenerate at 1; he shows that it
is possible to ﬁnd the function g so that  R 1
0
H .dzI u/ du D N .dz/ is satisﬁed. An
obstacle is that in practice it may be difﬁcult to identify the distribution H:
Damien et al. (1995) have also developed an algorithm to generate approximate
random variables from an ID distribution based on the Levy measure, N ./ in
its Laplace transform, and random variables approximating stable distributions.
Their motivation is derived from the fact that the characteristic function of any ID
distribution can be expressed as the limit of Poisson-type characteristic functions.
Thus any ID random variable may be expressed as a weighted sum of Poisson
random variables. The weights are derived from the Levy measure. The method
is illustrated by applying it to the gamma process, the Dirichlet process, and simple
homogeneous process (Ferguson and Phadia 1979). This method is used by Damien
et al. (1995) in simulating ID random variables for the extended gamma process
Dykstra and Laud (1981), discussed later. We state here their algorithm in the
general case and apply to speciﬁc processes later during their discussions.
1. Generate x1; : : : ; xn
iidÏ .1=k/ d
 .x/ ; where k D R 1
0
d
 .x/ and 
 .x/ is a ﬁnite
measure.
2. Generate yi Ï Poisson .k .1 C xi/ =nxi/ ; i D 1; : : : ; n;
3. Deﬁne z D Pn
iD1xiyi:
They prove that the characteristic function of z converges as n ! 1 to the
characteristic function of an ID distribution and thus z serves as a random variable
whose distribution approximates an ID distribution.
Wolpart and Ickstadt (1998) suggest an exact method called Inverse Levy
measure to simulate an IID on the interval Œ0; T :
Nondecreasing Inﬁnitely Divisible Processes
Let Yt be an almost surely nondecreasing process with independent increments and
no ﬁxed points of discontinuity. As noted above, it is known that all increments
Yt  Ys, s < t are inﬁnitely divisible. Therefore, we may write the log Laplace
transform of the increment Yt  Y0 as
 t.
/ D b.t/
 C
Z 1
0
.e
z  1/dNt.z/;
(4.1.6)
where Nt.z/ is a Lévy measure depending on t. (If Nt does not depend on t, then the
process is known as homogeneous.) It may also be written as dNt.z/ D  .dt; dz/ ; 
being a ﬁnite measure.
Here b.t/ is easily seen to be continuous and nondecreasing.

4.1
Introduction
133
4.1.2
Lévy Measures of Different Processes
As mentioned before, the Lévy measure Nt.z/ plays a critical role and identiﬁes
different processes, discussed in subsequent sections, as follows:
(i) For the gamma process with shape parameter  .t/ ; a right continuous function
on RC and intensity parameter  > 0; the Lévy measure is given by dNt.s/ D
.t/esds=s:
(ii) For the simple homogeneous process (Ferguson and Phadia 1979), the Lévy
measure turns out to be dNt.s/ D .t/dN.s/, where N is any measure on the
interval .0; 1/ such that R 1
0
z
1CzdN.z/ < 1:
(iii) The log-beta process (Walker and Muliere 1997a) with parameters ˛ ./ ; a
right continuous measure, and ˇ ./ ; a positive function, both deﬁned on the
interval Œ0; 1/; has the Lévy measure
dNt.s/ D
ds
.1  es/
Z t
0
exp .s .ˇ .u/ C ˛ fug// d˛c .u/ ;
(4.1.7)
where ˛c./ is the continuous part of ˛ ./ :
(iv) The Lévy measure of the beta process A.t/ (Hjort 1990) with parameters A0.t/
and c ./ is given by
dNt.s/ D
Z t
0
c.u/s1.1  s/c.u/1dA0c.u/ds;
(4.1.8)
where A0c.z/ is the continuous part of A0.t/ and c ./ is a piecewise continuous
nonnegative function on Œ0; 1/: Unlike the others mentioned above, this Lévy
measure is concentrated on the interval Œ0; 1 :
(v) The Lévy measure
dNt.s/ D
ds
.1  es/
Z t
0
c.u/ exp.sc.u/G.u; 1// dGc.u/
(4.1.9)
deﬁnes a beta-Stacy process (Walker and Muliere 1997a) with parameters
G ./ and c ./ ; where Gc./ is the continuous part of the right continuous
distribution function G ./ ; and c ./ is a positive function on Œ0; 1/:
(vi) For speciﬁc parameters of the beta-Stacy process such that, G is continuous
and c.u/ D c a constant, then
dNt.s/ D
ds
.1  es/
Z t
0
c exp .sc.1  G.u/// dG.u/;
(4.1.10)

134
4
Priors Based on Levy Processes
which upon simpliﬁcation reduces to
dNt.s/ D esc.e.1G.t//  1/
s.1  es/
ds;
(4.1.11)
which is the Levy measure for the Dirichlet process given in Ferguson (1974).
(vii) In connection with the Indian Buffet process, Thibaux and Jordan (2007)
introduced the Hierarchical Beta process where the Lévy measure of the beta
process was taken without cumulative and modiﬁed for continuous A0 as
 .du; ds/ D c .u/ s1 .1  s/c.u/1 A0 .du/ ds
on   Œ0; 1 ; and where c ./ is a positive function on , a probability space,
known as concentration function, and A0 is a ﬁxed base measure on :
The NTR and other processes can be described on an abstract space in terms of
the CRMs introduced by Kingman (1967). The advantage is that it allows us to treat
these processes in a uniﬁed manner. Since CRMs are constructed using the Poisson
process on abstract spaces, this approach offers another method of constructing prior
processes as will be seen later in connection with the dependent processes. A major
disadvantage is that it does not provide any insight into how such processes arise.
Nevertheless, it is instructive to view this approach as well. For this purpose, the
following brief introduction to the CRM and Poisson process is taken from Kingman
(1967, 1993).
4.1.3
Completely Random Measures
All of the above mentioned prior processes were developed by taking different
forms of independent increment processes and their Levy measures. The notion
of independent increment process has been generalized by Kingman (1967) in
deﬁning a CRM thereby making it possible to consider abstract spaces and not
just the real line. He studied certain properties of the same and proved that such
measures are almost surely discrete. Lijoi and Prünster (2010) recast the processes
with independent increments in a more general and elegant manner in terms of
CRMs tying together a number of neutral to the right type processes.
Deﬁnition 4.1 (Kingman) Let … be a space of ﬁnite measures 	 on .X; A/
such that 	 .A/ < 1 for all bounded sets A 2 A; and  .…/ be the
corresponding -algebra. A measure 	 is said to a CRM if, 	 .A/ is a random
variable deﬁned on some probability space .; F; Q/ into .…;  .…// for all
A 2 A such that for any pair-wise disjoint sets A1; : : : ; Ak in A; the random
variables 	 .A1/ ; : : : ; 	 .Ak/ are mutually independent. That is, random measures
assigned to disjoint subsets are independent.

4.1
Introduction
135
A CRM 	 is a stochastic process with index set A; also known as the parameter
set. The whole distribution over 	 is determined once the distributions of random
variables 	 .A/ are given for all A 2 A. The distribution of 	 can be derived in
terms of its characteristic function. The CRMs are discrete with probability one.
It is a generalization of the independent increment process on the real line, since
	 ..0; t/ on the real line may be viewed as a stochastic process with independent
increments.
Kingman described a way to construct a CRM using the nonhomogeneous
Poisson process (Kingman 1993). The Poisson process is deﬁned on an abstract
space S as a random set of points such that the number of points in disjoint sets is
independent Poisson variates. Formally,
Deﬁnition 4.2 (Kingman) Let .S;  .S/ ; 	/ be a measure space, let … be a random
subset of S and let N .A/ D card .A \ …/ ; A 2 S: Then … is said to be a Poisson
process with measure 	 if
(i) whenever for pair-wise disjoint members A1; A2; : : : ; Ak; k
>
1
of
 .S/ ; N .A1/ ; N .A2/ ; : : : ; N .Ak/ are independent.
(ii) if 	 .A/ < 1; then N .A/ Ï Poisson .	 .A// ; and if 	 .A/ D 1; then N .A/ D
1 a.s.
Note that … is a Poisson process with mean measure 	 if and only if N is a CRM.
In conformity with the deﬁnition of a stochastic process, … is a mapping from some
probability space .ˆ;  ./ ; P/ into S1 of all countable subsets of S: Thus N .A/ is
a function from  ! f0; 1; : : : ; 1g : For a Poisson process to exist, it is necessary
that the measure 	 must be nonatomic. N is ﬁnitely additive. To ensure it to be -
additive, it is necessary that 	 can be expressed as the sum of a countable number
of  -ﬁnite measures. If so, then there exists a Poisson process … on S with measure
	 and … is almost certainly countable.
Theorem 4.3 (Superposition Theorem, Kingman) Let …1; …2; : : : be a count-
able collection of independent Poisson processes on S and let …n have mean
measure 	n for each n: Then their superposition … D [1
nD1…n is a Poisson process
with mean measure 	 D P1
nD1 	n:
Suppose that with each point ! 2 …; we associate a random variable X!; called
mark of !; taking value in some space ˆ. The distribution of X! may depend on !
but otherwise independent of other points of …: The pair .!; X!/ is then a random
point ! of the product space S  ˆ: The totality of such points forms a random
countable subset … D f.!; X!/ W ! 2 …g of S  ˆ: Then a fundamental result is
the following marking theorem:
Theorem 4.4 (Marking Theorem, Kingman) The random subset … is a Poisson
process on S  ˆ with mean measure 	 given by
	 .D/ D
Z Z
.!;x/2D
	 .d!/ p .!; dx/ ;
(4.1.12)

136
4
Priors Based on Levy Processes
where p .!; / is a probability distribution depending upon ! and deﬁned on ˆ such
that for any set C 
 ˆ; p .; C/ is a measurable function on S:
Kingman’s excellent book Poisson Processes is a good source for further details.
If the measure of the Poisson process has inﬁnite mass, it generates an inﬁnite
number of points. Thus a CRM can be expressed as 	 D P1
iD1 piı!i, where !i are
atoms of measure 	 and pi are the weights attached to the atoms.
As in processes with independent increments, the measure 	 may be described as
having (apart from the deterministic component) two components: 	1 representing
the part with ﬁxed points of discontinuity and 	2 representing random points of
discontinuities. Write
	 D 	1 C 	2 D
X
i1
Siıci C
X
i1
Jiıi;
(4.1.13)
where Si are random masses at ﬁxed points ci of discontinuities, are nonnegative and
mutually independent, and are also independent of 	2. The jumps in 	1 will occur
at sample observations, whether the prior process may or may not have them. With
the ﬁxed discontinuity part removed, 	 will admit Levy representation
log E e
	2.0;t D
Z 1
0
.e
 z  1/d Nt.z/;
(4.1.14)
where d Nt.z/ is the Levy measure, at times written as d Nt.z/ D  .dz; dt/ D
t .dz/ ˛ .dt/ ; where ˛ is a measure on .X; A/ and  is a transition kernel on
X  B

RC
: If t D ; then the distribution of jumps in 	2 is independent
of their location and both  and 	2 are termed homogeneous; otherwise they are
nonhomogeneous. Then 	2 and subsequently 	 can be constructed by using the
Poisson process with mean measure . 	2 can be characterized in terms of the
distribution of random points set f.Ji; i/gi1 as a Poisson process with intensity
measure given by the Levy measure : It is often convenient to specify the Levy
measure of 	2 and the distribution of mass at ﬁxed points of discontinuities
separately rather than specifying the full Levy measure of 	: Some examples of
Levy measures of 	2 leading to well-known processes are: the mean measure
 .dz; ds/ D .z/s1esdsdz; s > 0
(4.1.15)
produces the gamma process with shape parameter  .t/ and intensity parameter ;
 .dz; ds/ D c .z/ s1 .1  s/c.z/1 A0 .dz/ ds; 0 < s < 1
(4.1.16)
results in the beta process with parameters .c ./ ; A0/ where c ./ is a positive
function known as concentration function, and A0 is a ﬁxed base measure on :
Random probability measures can also be obtained by normalizing completely
random measures. For example, consider the points f.qi; i/g obtained from a
Poisson process with mean measure  used in the gamma process. Now deﬁne

4.2
Processes Neutral to the Right
137
a random probability measure P D P
i1
piıi; where pi D qi= P
i1
qi: Then P is a
Dirichlet process with Levy measure given by
dNt.s/ D s1 .1  es/1 esc.e.1G.t//  1/ds;
(4.1.17)
which can also be written as
 .dt; ds/ D s1 .1  es/1 esc.e.1G.t//  1/G.dt//ds:
(4.1.18)
where c > 0 is a constant and G .t/ is a continuous base distribution function. Note
that P is not a CRM, since P .A1/ and P .A2/ for disjoint sets A1 and A2 are not
independent but are negatively correlated.
For this line of development, see Lijoi and Prünster (2010). It is clear from
the above that different forms of Levy measure  deﬁne different processes as
observed in their paper. A good account of deﬁning discrete nonparametric priors
by normalizing CRMs is given in a recent article (Favaro and Teh 2013). In view of
the similarity of the CRM approach to the one presented below, we prefer to stick
with the our approach for two reasons. One, it provides a historical perspective of the
development of these processes, and perhaps easy to understand. Two, it also reveals
how these measures came about, which is not clear by the CRMs approach. This is
evident, for example, in the development of the beta and beta-Stacy processes.
4.2
Processes Neutral to the Right
The Dirichlet process has a single parameter, ˛; and with its speciﬁcation the prior
is completely identiﬁed. As noted earlier, Doksum (1974) saw it as a limitation and
introduced a general method of deﬁning a broad class of priors called neutral to the
right processes. They are constructed on the basis of independence of successive
normalized increments of the distribution function F. They are closely connected
to the processes with independent increments and can be deﬁned in terms of
an independent increment process. If the ﬁxed points of discontinuities and the
deterministic part of an independent increment process are removed then it can be
characterized by its Levy measure. Thus the NTR processes may be studied in terms
of their Levy measures. This approach was found to be critical in developing various
other prior processes as will be seen later on.
The neutral to the right process can be speciﬁed by four quantities instead of a
single parameter ˛ of the DP: ﬁxed points of discontinuities, distributions of jumps,
continuous deterministic part, and continuous Levy measure. Thus the process
allows a large amount of freedom to the statistician in choosing a prior. On the other
hand, in contrast to ˛; it is difﬁcult to interpret these parameters in terms of prior
belief. Like the DP, it also select a discrete distribution function with probability
one. But unlike the DP, it is conjugate with respect to the data that may include right

138
4
Priors Based on Levy Processes
censored observations. However, the posterior distribution is not in a close form and
usually difﬁcult to compute. It can be characterized in terms of where the number
of observations fall relative to the point of evaluation of F: It has clear advantage
over the tailfree processes (to be formally deﬁned later on) in that it may be deﬁned
independently of any partition points. This class of priors is sufﬁciently broad and
as will be seen later that besides the Dirichlet process, it includes processes such
as gamma, simple homogeneous, and beta-Stacy processes which are found to be
useful in speciﬁc applications. It can be extended to spatial neutral to the right
processes in the same way the DP was extended to form the class of dependent
DPs (see Sect. 3.3). When we say F is neutral to the right, we also mean that a prior
distribution deﬁned on the space .F;  .F// is neutral to the right. Doksum’s paper
is the main source of the following material.
4.2.1
Deﬁnition
A random probability measure P is deﬁned in the same way as in the case of the DP.
However, Doksum assumes P to be -ﬁnite, i.e., the distribution of P .An/ tends to
a degenerate distribution at 0 as n ! 1: If fAng is a decreasing sequence of sets
An; then it is equivalent to limn!1 P .An/ D 0 a.s. Let P be a random probability
measure on .R; B/ ; where B D  .R/ ; and let F .t/ D P ..1; t/ denote the
corresponding random distribution function. Essentially, P and F (as well as their
distributions) are said to be neutral to the right if for all partitions 1 D t0 < t1 <
   < tk < tkC1 D 1 of R, and k a positive integer, the normalized increments of F,
F.t1/; ŒF.t2/  F.t1/=Œ1  F.t1/; : : : ; ŒF.tk/  F.tk1/=Œ1  F.tk1/
(4.2.1)
are independent: In other words, a random distribution function F.t/ on the real line
is neutral to the right if for every t1 and t2 with t1 < t2,
1  F.t2/
1  F.t1/ and fF.t/ W t < t1 g are independent.
That is the proportion of mass F assigns to the subinterval .t2; 1/ of the interval
.t1; 1/ is independent of what F.t/ does to the left of t1. The fractions in (4.2.1)
are the hazard contributions of F of respective intervals and are known as residual
fractions. See Sect. (3.2.4).
Similarly, F is said to be neutral to the left if the ratios
F.tk/; ŒF.tk/  F.tk1/=F.tk/; : : : ; ŒF.t2/  F.t1/=F.t2/

4.2
Processes Neutral to the Right
139
are independent. Because in (4.2.1) the denominator may be zero with positive
probability, the following formal deﬁnition is preferred:
Deﬁnition (Doksum) A random distribution function F on (R; B/ is said to be
neutral to the right if for all m D 1; 2; : : : ; and all sequences t1 < t2 <    < tm of
real numbers, there exists independent nonnegative random variables V1;V2; : : : ; Vm
such that the distribution of the vector .F.t1/; : : : ; F.tm// is same as the distribution
of (V1;1.1  V1;/ .1  V2;/ ; : : : ; 1Qm
iD1 .1  Vi//: Or equivalently, if there exist
independent nonnegative random variables V1;V2; : : : ; Vm such that the distribution
of .1  F.t1/; 1  F.t2/; : : : ; 1  F.tm//
is the same as the distribution of (V1;
V1V2; : : : ; Qm
1 Vi/ .
By solving the set of equations
1  F.ti/ D
iY
1
Vi for i D 1; : : : ; m;
for Vi, we ﬁnd (deﬁning t0 D 1 so that F.t0/ D 0)
Vi D
1  F.ti/
1  F.ti1/ for i D 1; : : : ; m.
Thus, if the difﬁculties entailed in dividing zero by zero are ignored, a process
neutral to the right may be deﬁned to be one for which the ratios (4.2.1) are
stochastically independent.
Doksum deﬁnes a sample from a random distribution function F in the same way
as Ferguson (1973) did (see Sect. 2.1).
Viewing it differently and writing 1  F.t/ D eH.t/, a prior can be speciﬁed on
the space fH .t/g of cumulative hazard functions, by assigning ﬁnite dimensional
distributions to the hazard contributions q1; : : : ; qk of F, where for any partition
1 < t1 <; : : : ; < tk < 1 of the real line,
qj D F.tj/  F.tj1/ = 1  F.tj1/ D 1  Vj;
(4.2.2)
is the hazard contribution of the j-th interval (Kalbﬂeisch 1978). Assigning inde-
pendent prior distributions to qj’s, subject to some consistency requirement, results
in the process neutral to the right. In the case of Dirichlet process, qj’s have
independent beta distributions. Hjort (1990) develops a prior process called the beta
process (see Sect. 4.5), for H .t/ even in the case when F may not have a density.
Walker and Muliere (1997a) have shown its usefulness in models related to survival
event history data analysis.

140
4
Priors Based on Levy Processes
The deﬁnition may also be stated in terms of random probability measures as
was done with the Dirichlet process. For any measurable partition B1; B2; : : : ; Bm,
m  1 of the real line, with Bi D .ti1; ti; i D 1; : : : ; m  1; t0 D 1 and
Bm D .tm1; 1/; neutral to the right and tailfree processes are deﬁned in terms
of independence properties of P .B1/ ; : : : ; P .Bm/. P is said to be neutral to
the right if there exist independent nonnegative random variables V1;V2; : : : ; Vm
such that the vector (P .B1/ ; : : : ; P .Bm// has the same joint distribution as
.V1; V2 .1  V1/ ; : : : ; Vm
Qm1
iD1 .1  Vi//: If V1; V2; : : : are taken to be independent
with Vi  Be .˛i; ˇi/ and ˇi D Pm
jDiC1 ˛j; then it yields the Dirichlet process (see
property 5 below). Thus the process neutral to the right generalizes the Dirichlet
process and provides much more ﬂexibility, and a rich class of models can be
developed by specifying different distributions for the variables Vi’s. However it is
still deﬁned on the real line and therefore difﬁcult to extend to higher dimensions.
The NTR process is deﬁned on the real line. However, as pointed out by Doksum,
the concept of neutrality can also be deﬁned for random probabilities on abstract
measurable spaces. This concept of a neutral random probability is an extension
of Connor and Mosimann (1969) concept of neutrality for k-dimensional random
vectors to a process as is the DP an extension of k-dimensional Dirichlet distribution
to a process.
Let fm W m D 0; 1; : : :g denote a sequence of nested, measurable partition of X
with 0 D fXg and m D fAm1; : : : ; Amkmg: We further assume that the partition is
ordered from left to right by which we mean the set Ami is left of (comes before) Amj
for all i < j: Now the deﬁnition may be stated as follows:
Deﬁnition 4.5 P is neutral with respect to the sequence fmg of nested measurable,
ordered partitions of X if for each m  1; there exist nonnegative independent
random variables Vm1; : : : ; Vmkm with Vmkm D 1 and
.P .Am1/ ; : : : ; P .Amkm//
dD .Vm1; Vm2 .1  Vm1/ ; : : : ; Vmkm
km1
Q
jD1

1  Vmj

/:
For the DP, we had only one partition and the random probability vector had the
ﬁnite dimensional Dirichlet distribution. The term neutral refers to independence
properties of sets within partitions, namely for each m  1; the random variables
P .Am1/ ; P

Am2jAc
m1

; P .Am3j .Am1 [ Am2/c/ ; : : : ; P .AmkmjAmkm/ D 1
are independent. The interpretation is as one moves to the right, the relative random
probability assigned to the next set is independent of the corresponding relative
random probability assigned to the other sets in the partition. P is neutral since
we can deﬁne random variables Vmi D P .AmijAmi [ : : : [ Amkm/ : We see a clear
parallel to the deﬁnition of F being neutral on the real line.

4.2
Processes Neutral to the Right
141
4.2.1.1
Alternate Representation of NTR
The random distribution function F, neutral to the right, may also be viewed
alternatively (Doksum 1974; Ferguson 1974) in terms of a process with independent
increments.
Let Yt D  log.1  F.t// where Yt D C1 if F.t/ D 1, so that
F.t/ D 1  eYt.
(4.2.3)
Here the equality is to be understood as in distribution. Then the process Yt has inde-
pendent increments, since for any partition t1 < t2 <    < tm of R, the increments
Yt1; Yt2  Yt1; : : : ; Ytm  Ytm1 correspond to the independent normalized increments
in F. It can be interpreted as a random integrated hazard function. Furthermore,
since F.t/ is assumed to be a distribution function a.s., it is nondecreasing a.s., right
continuous a.s., limt!1 F.t/ D 0 a.s., and limt!C1 F.t/ D 1 a.s. Translating
these properties in terms of the Yt, we may state the following alternative deﬁnition
of a process neutral to the right:
Deﬁnition 4.6 (Doksum) Let Yt be a process with independent increments, non-
decreasing a.s., right continuous a.s., limt!1 Yt D 0 a.s., and limt!C1 Yt D 1
a.s. Then (4.2.3) deﬁnes a random distribution function neutral to the right. (Yt is
allowed to be C1 with positive probability for ﬁnite t.)
Thus, F.t/ may be decomposed using the above representation. The process
Yt has at most countably many ﬁxed points of discontinuity at say, t1; t2; : : : in
some order. Let S1; S2; : : : represent the random heights of the jumps at t1; t2; : : :
respectively. Then S1; S2; : : : are independent nonnegative, possibly inﬁnite-valued,
random variables with densities, say f1; f2; : : : with respect to some convenient
measure. The jumps fSjg are also independent of the rest of the process, so with
the jumps removed let
Zt D Yt 
X
j
SjIŒtj;1/.t/.
(4.2.4)
This process has independent increments, is nondecreasing a.s., and has no ﬁxed
points of discontinuity. Therefore, Zt has inﬁnitely divisible increments and its
moment generating function (MGF) has Lévy formula
log E e
Zt D 
b.t/ C
Z 1
0
.e
 z  1/d Nt.z/;
(4.2.5)
where b is nondecreasing and continuous with b.t/ ! 0 as t ! 1, and where Nt
is a continuous Lévy measure, that is,
1. for every Borel set B, Nt.B/ is continuous and nondecreasing,
2. for every real t, Nt./ is a measure on the Borel subsets of .0; 1/;
3. R 1
0 .z= .1 C z//d Nt.z/ ! 0 as t ! 1.

142
4
Priors Based on Levy Processes
Thus, going back to the processes neutral to the right, they can be speciﬁed by
giving four things:
(a) M D ft1; t2; : : : ; g the set of ﬁxed points of discontinuity
(b) f D ff1; f2; : : : ; g the densities (or distributions) of the jumps there
(c) b.t/ the continuous deterministic part, and
(d) Nt.B/ the continuous Lévy measure.
The function b corresponds to the continuous deterministic part of the process
Yt. If there are no ﬁxed points of discontinuity and if the Lévy measure vanishes
identically, then F.t/ is the ﬁxed nonrandom distribution function
F.t/ D 1  eb.t/.
On the other hand, if b  0, then Yt and hence F.t/ increases only in jumps a.s.,
so that F is discrete with probability one. In general, except for the fact that an F
may have a continuous nonrandom part as a mixture, processes neutral to the right
do not avoid the drawback, noted for the Dirichlet process, of choosing discrete
probability measures with probability one. The above four quantities enable us to
describe the posterior distribution as will be seen next. We take b  0 and thus
a neutral to the right process is characterized by M; f, and Nt: We will denote in
symbols, F  NTR.M; f;Nt/.
It is difﬁcult to interpret these parameters as was possible in the case of the DP.
Walker and Damien (1998) offer a partial solution and provide a general method for
specifying the prior mean and variance of an NTR, and as a consequence, provide
interpretation of the parameters involved. They do so by relating the mean and
variance of the random distribution function in terms of the Levy measure of the
process. Let S.t/ D 1 F.t/: Using the Levy representation without the ﬁxed points
of discontinuity, it can be seen that
	 .t/ D  log EŒS.t/ D
Z 1
0
.1  ez/ dNt .z/ and
 .t/ D  log EŒS2.t/ D
Z 1
0

1  e2z
dNt .z/ :
Now the task is to ﬁnd a Levy measure Nt ./ which satisfy these two equations with
prior speciﬁcation of 	 and : They consider the Levy measures of type
dNt .z/ D
dz
.1  ez/
Z t
0
ezˇ.s/d˛ .s/ ;
where ˛ is a ﬁnite measure and ˇ ./ is a nonnegative function. It will be seen in a
later section that this type of Levy measure characterizes a new process, the beta-
Stacy process, which covers many of the known NTR processes. For example, the
DP arises when ˇ .t/ D ˛ .t; 1/ and the simple homogeneous process (Ferguson

4.2
Processes Neutral to the Right
143
and Phadia 1979) arises when ˇ is constant. Walker and Damien (1998) prove the
existence of ˛ ./ and ˇ ./ such that
	 .t/ D
Z 1
0
Z t
0
ezˇ.s/d˛ .s/ dz and
 .t/ D
Z 1
0
Z t
0
 1  exp .2z/
1  exp .z/

ezˇ.s/d˛ .s/ dz:
(4.2.6)
If ˇ is constant, then we have 	 .t/ D ˛ .t/ =ˇ and  .t/ D Œ.1 C 2ˇ/ = .1 C ˇ/
Œ˛ .t/ =ˇ allowing us to interpret the parameters ˛ and ˇ in terms of 	 and :
The above alternate representation (4.2.3) shows a close connection between an
NTR process and a nondecreasing IIP through the cumulative hazard function. The
CHF is deﬁned as H.t/ D  log.1  F.t// and if the F does not have a Density,
then A .t/ D
R t
0
dF.s/
F.s/; where F D 1  F: If F has an NTR prior, then the prior
distribution of A or H turns out to be an IIP, albeit with different Levy measures, A
and H; respectively. In exploring this connection, it is sufﬁcient to restrict attention
to independent increment processes when b  0 and there are no ﬁxed points
of discontinuities. These are the NTR priors which give mass one to all discrete
distribution and such priors can be described fully in terms of their Levy measures.
This line is pursued in Dey et al. (2003) and the authors prove some interesting
properties on the way. In particular,
Proposition 4.7 (Dey et al.) The following are equivalent for a prior … on F:
(1) … is neutral to the right
(2) the process fH.t/ W t > 0g induced by the mapping F ! H .t/ has independent
increments.
(3) the process fA.t/ W t > 0g induced by the mapping F ! A .t/ has independent
increments.
The Levy measure dNt .z/ can alternatively expressed as  .dt; dz/ : Thus many
NTR processes can be deﬁned by taking different forms of : One convenient way
is to express it in terms of Levy measures A and H of cumulative hazard processes
arising via A and H in which case A is deﬁned on the space .0; 1/  .0; 1/ ; since
the increment in A is restricted to the interval .0; 1/ as will be seen later on while
discussing the beta process, and H on .0; 1/  .0; 1/ : One general form of A
may be considered as A .ds; du/ D a .s; u/ A0 .ds/ du; 0 < s < 1; 0 < u < 1;
where A0 is an increasing right continuous function such that A0 .t/  A0 .t/  1
and a .s; u/ is such that
R 1
0 ua .s; u/ du < 1 for all s: By taking a speciﬁc form
of A .dz; ds/ D c .z/ s1 .1  s/c.z/1 A0 .dz/ ds; where c ./ is a positive function
known as concentration function, and A0 is a ﬁxed base measure on ; Hjort (1990)
introduced the beta process prior (see Sect. 4.5) and showed its usefulness in a
variety of applications. Similarly, by deﬁning
H .dz; ds/ D .1  exp .s//1 c .z/ exp.sc .z/ F .z//dzH0 .ds/ ; 0 < z; s < 1;

144
4
Priors Based on Levy Processes
where H0 is a ﬁxed measure, Walker and Muliere (1997a) introduced the beta-Stacy
process prior on F; which generalizes the DP and is deﬁned as an NTR process
with H as above. The corresponding independent increment process was called a
log-beta process. Beta and log-beta give rise to similar NTR priors via A1 and H1:
The relationship between the two priors is stated explicitly in Dey et al. (2003) as
Proposition 4.8 A is the distribution of .s; u/ ! .s;  ln .1  u// under H:
Conversely, H is the distribution of .s; u/ ! .s; 1  eu/ under A:
The representation (4.2.3) of a CDF in terms of an IIP has facilitated the
development of several new processes. Kalbﬂeisch (1978) used a gamma process
to place a prior on survival function S which he considered as a nuisance parameter
in dealing with a regression problem. In order to construct a prior for hazard rates
 .t/ ; Dykstra and Laud (1981) deﬁne the extended gamma process, generalizing the
gamma process, where the independent increments of the Yt process were taken to be
gamma distributed with scale parameters taken to be the corresponding increments
of a continuous function ˛ .t/ treated as parameter of the extended gamma process.
Then the process was deﬁned as a convolution of another right continuous function
ˇ .t/ ; which was treated as a second parameter
4.2.2
Properties
Various properties of the neutral to the right process were highlighted in Doksum
(1974).
1. The following three conditions are equivalent:
(i) F is neutral to the right.
(ii) For all t1 <    < tk; k  1 there exist independent random variables
V1; : : : ; Vk such that .F.t1/; F.t2/F.t1/; : : : ; F.tk/F.tk1// and (V1;V2.1
V1/; : : : ; Vk
Qk1
1
.1  Vi// have the same distribution.
(iii) F is tailfree with respect to every sequence of nested ordered partitions
of the form m
D fAm;1; : : : ; Am;kmg with Am;i
D .tm;i; tm;iC1, i D
1; : : : ; kmI 1 D tm;1 <    < tm;kmC1 D 1, m D 1; 2; : : :
2. The following connection is seen between a random distribution function and
a particular stochastic process which is exploited in developing several prior
processes.
Theorem 4.9 F.t/ is a random distribution function neutral to the right if and only
if it has the same distribution as of process 1  eY.t/ for some a.s. nondecreasing,
a.s. right continuous, independent increments process with limt!1 Y.t/ D 0 a.s.
and limt!1 Y.t/ D 1 a:s:
As noted by Doksum, this theorem shows that the Dirichlet random distribution
function F is not the only random distribution function that is neutral to the

4.2
Processes Neutral to the Right
145
right. In fact, different random distribution functions which are neutral to the right
may be constructed corresponding to the processes with independent nonnegative
increments and their Laplace transforms. He gives some examples in his paper and
other examples, presented later, include gamma, beta, and beta-Stacy processes.
In view of this property, one can compute E.F.t// by using E.F.t// D 1 
E

eY.t/
:
3. If a random distribution function F.t/ is neutral to the right and if  log.1F.t//
has no nonrandom part, then F is discrete with probability one. This follows from
theorem in property 2 and a property of independent increment processes.
4. If F is neutral to the right and neutral to the left, then F is a Dirichlet process (on
R) or a limit of Dirichlet processes or processes concentrated on two nonrandom
points.
5. In the above deﬁnition of the neutral to the right process, if Vi’s are chosen to be
Be.˛i; ˇi/ such that ˇi D P
jiC1˛i; then it reduces to the Dirichlet process on R
as noted before.
6. If P … C (see characterization of the DP) is neutral with respect to all sequences
of nested, measurable, ordered partitions, then P is a Dirichlet process.
7. Dirichlet process is neutral to the right process with respect to every sequence
of nested, measurable ordered partitions of R. This can be seen as follows.
For each m D 1; 2; : : :consider the sequence of nested partitions, fmg D
fAm1; : : : ; Amkmg denoting the ordered partition m of R. We need to show that for
each m, there exists independent family of random variables Vm1;Vm2; : : : ; Vmkm
such that the joint distribution of the vector .P.Am1/; : : : ; P.Amkm// has the
same distribution as of .Vm1; Vm2.1  Vm1/; : : : ; Vmkm
km1
Q
jD1
.1  Vmj//, namely
the Dirichlet distribution with parameter ˛. For this it would be sufﬁcient
to show that P.Ami/ and Vmi
i1
Q
jD1
.1  Vmj/ have the same distribution, namely
Be.˛.Ami/; ˛.R/  Pi
jD1˛.Amj//; for some i; 1  i  km: To see this deﬁne
Vm1; D P.Am1/; Vm2; D P.Am2jAc
m1/; Vm3 D P.Am3jAc
m1 \ A c
m2/; : : :
and so on. Now take each of them distributed independently as beta distribution
with parameter
.˛.Am1/; ˛.R/  ˛.Am1//;
0
@˛.Am2/; ˛.R/ 
2
X
jD1
˛.Amj/
1
A ;
.˛.Am3/; ˛.R/ 
3
X
jD1
˛.Amj//; etc:;

146
4
Priors Based on Levy Processes
respectively. Continuing in this way, it can be seen that P.Ami/ D Vmi
Qi1
jD1.1 
Vmj/, for i D 2; : : : ; km. Now using the properties of the beta distribution, it can
be seen that
Vmi
i1
Y
jD1
.1  Vmj/ Ï Be
0
@˛.Ami/; ˛.R/ 
iX
jD1
˛.Amj/
1
A :
8. Neutral to the right processes satisfy the structural conjugacy property:
Theorem 4.10 (Doksum) Let X1; : : : ; Xn be a sample from F which may include
right censored observations: If F is a random distribution function neutral to the
right, then the posterior distribution of F given the data is also neutral to the right.
Doksum proved the conjugacy property in a complicated way in terms of ﬁnding
the posterior distributions of variables Vj’s given the sample, and then extending to
F. Recall that the posterior distribution of the Dirichlet process was simple—all you
had to do was to update its parameter, ˛: Since the neutral to the right process has
many more parameters, it would be difﬁcult to ﬁnd the posterior in a similar way.
Doksum gives the description of it, but it is complicated. Ferguson (1974) provided
an alternative description of the posterior which is much simpler.
Description of the posterior distribution is given next.
4.2.3
Posterior Distribution
In the case of the Dirichlet process (also mixtures of Dirichlet processes and
Dirichlet invariant process), the conjugacy was parametric and therefore it was
easy to describe the posterior, which remained the same as the prior but with an
updated parameter ˛ to ˛ C Pn
iD1 ıXi. Similarly for the NTR process, the posterior
distribution of F is NTR with updated parameters. However, it is not that simple to
update parameters in this case. Since there are other processes in the class of neutral
to the right that are included in the later sections, it seems instructive to report
the description in some what more detailed manner. As noted earlier, Doksum’s
description of the posterior distribution is in terms of the posterior distributions
of the normalized increments Vj’s and is complicated. Ferguson (1974) (and later
Ferguson and Phadia 1979) gives an explicit and simpler description and shows
that as the uncensored data are handled conveniently by the Dirichlet process, the
censored data can also be handled with ease by neutral to the right processes.
Heuristically it can be described as follows. Let t1 < t2 <    < tk represent
a large number of partition points. The distribution of .F.t1/; : : : ; F.tk// is related
to .Yt1; : : : ; Ytk/, which may be described even more simply through the variables
.Z1; : : : ; Zk/ where Zi D Yti  Yti1 represents the i-th increment of Yt process, and
Yt0  0. In writing the joint distribution of the Zi and an observation X from F,
we approximate by reducing the information about X to that of knowing into which

4.2
Processes Neutral to the Right
147
interval .tj1; tj X falls. Then, if fi.zi/ denotes the density of Zi , the joint density of
Z1; : : : ; Zk and X may be written as
f .z1; : : : ; zk; x/ D
 kY
iD1
fi.zi/
!
P.X 2 .tj1; tj j Z D z/
D
 kY
iD1
fi.zi/
!
.F.tj/  F.tj1//
D
 kY
iD1
fi.zi/
!
e Pj1
iD1 zi .1  e zj/
D
 j1
Y
iD1
fi.zi/e zi
!
fj.zj/.1  e zj/
0
@
kY
iDjC1
fi.zi/
1
A .
(4.2.7)
The posterior density of Z1; : : : ; Zk given X 2 .tj1; tj is then proportional to this
quantity. From this, we see that given X 2 .tj1; tj the Zi are still independent (and
hence the posterior distribution of F should still be neutral), and that the distributions
of the increments to the right of tj have not changed (a rationale to coin the word
“tailfree”), while the distributions of the increments to the left of tj1 are changed
by multiplying the density by ez and renormalizing. If there is a prior ﬁxed point of
discontinuity at x, the posterior density of the jump at x is obtained by multiplying
the prior density by .1  e z/ and normalizing. This can be extended for a sample
of size n, in part as follows.
We have to be mindful of three cases: what happens to the increments of intervals
in which no observation fall, intervals before and after an observation; what happens
to the distribution of jumps at ﬁxed points of discontinuities; and what happens to
the jumps at points other than the points of discontinuities. The ﬁrst one is easy to
handle and was answered in Doksum’s paper.
Theorem 4.11 (Doksum 1974) The posterior distribution of an increment Z D
YtYs of an interval .s; t in which no observations fall is obtained by multiplying the
prior density of Z by er z and renormalizing, where r is the number of observations
among the sample of size n that are greater than t.
Thus the posterior distribution of an increment of an interval depends only upon
the number of observations beyond the interval and not where they fall. This is
similar to the case of the DP where the posterior distribution depended on only the
number of observations belonging to the interval and not where they were located.
For the second case, if x is a prior ﬁxed point of discontinuity, then the posterior
density of the jump Z in Yt at x is obtained by multiplying the prior density of Z
by e rz.1  e z/m and normalizing, where r is the number of observations greater
than x, and m is the number of observations equal to x. That is, if gx .z/ is the prior

148
4
Priors Based on Levy Processes
density of the jump Z in Yt at x; then the posterior density g
x .z/ is given by
g
x .z/ D
e rz.1  e z/mgx .z/
R
e rs.1  e s/mgx .s/ ds:
(4.2.8)
There is a problem in extending this to the case if x is not a prior ﬁxed point of
discontinuity. If x is a point at which one or more observations fell, then the posterior
distribution of Yt may have a ﬁxed point of discontinuity at x even if the prior did not
have ﬁxed point discontinuity at x. On the other hand, there may be no change in the
posterior, as is the case when x is in a region where b.t/ increases but Nt.R/ does not,
since then it is known that the observation X D x arose from the nonrandom part of
the distribution. The general case is a mixture of these two cases. It is sufﬁcient to
state the theorem for a sample of size one, since larger samples may be handled by
an application of Theorem 4.10.
Deﬁne for each Borel set B  Œ0; 1/ a measure 	B./ on the Borel subsets of R
to satisfy
	B..1; t/ D b.t/IB.0/ C
Z
B
.1  ez/dNt.z/:
(4.2.9)
Note that 	B  	Œ0;1/ , so that the Radon–Nikodym derivative
B.t/ D
d	B
d	Œ0;1/
.t/
(4.2.10)
exists for all B.
Theorem 4.12 (Ferguson) Let F.t/ be a process neutral to the right and let X be
a sample of size 1 from F. If x is not a prior ﬁxed point of discontinuity, then the
posterior distribution of the jump S in Yt at x, given X D x is given by
Hx.s/ D Œ0;s.x/.
(4.2.11)
Thus we summarize the above observations as a theorem for sample size one.
Following Hjort (1990) and Walker and Muliere (1997a), we write dNt .z/ D
dz
R
.0;t a .z; s/ ds; where a .z; s/ is some nonnegative function.
Theorem 4.13 Let F be a random distribution function neutral to the right with
parameters, M; f D
˚
fj
k
jD1 and Nt .z/ ; and let X be a sample of size 1 from F: Then
the posterior distribution of F given X is again neutral to the right with updated
parameters M; fD
n
f 
j
ok
jD1 and N
t .z/ as described below. In symbols; If F 
NTR.M; f;Nt/, then FjX  NTR.M; f;N
t /; where dN
t .z/ D dz
R
.0;t a .z; s/ ds:
Let x be a real number.

4.2
Processes Neutral to the Right
149
(i) Given X D x, and x D ti for some i, M D M; fj .s/ changes to
f 
j .s/ D
8
<
:
esfj .s/
if tj < x
 .1  es/ fi .s/ if tj D x
fj .s/
if tj > x;
(4.2.12)
a.s; z/ D
 a.s; z/ez if z  x
a.s; z/
if z > x;
(4.2.13)
where  is the normalizing constant.
(ii) Given X D x, and x ¤ ti for any i, an additional point of discontinuity is added
to the set M; M D M [ fxg ; with fj .s/ changes to
f 
j .s/ D
 ezfj .s/ if tj < x;
fj .s/
if tj > x;
f 
x .s/ D  .1  ez/ a.s; z/; 0 < s < 1;
(4.2.14)
a.s; z/ as in .i/ and f 
x .s/ is the density of new jump S at x:
Ferguson and Phadia (1979) extended the above result for right censored data
since the NTR is more amenable to deal with censored data than the DP. The
posterior distribution of F given a censored observation is as follows:
Theorem 4.14 (Ferguson and Phadia) Let F be a random distribution function
neutral to the right, X be a sample of size one from F; and let x be a real number.
(a) The posterior distribution of F given X > x is neutral to the right; the
posterior distribution of an increment y to the right of x is the same as the
prior distribution; the posterior distribution of an increment to the left of or
including x is found by multiplying the prior density by ey and normalizing.
(b) The posterior distribution of F given X  x is neutral to the right; the posterior
distribution of an increment y to the right of or including x is the same as the
prior distribution; the posterior distribution of an increment to the left of x is
found by multiplying the prior density by ey and normalizing.
The proof is straightforward. Consider an arbitrary partition of the real line
1 D t0 < t1 <    < tj1 < tj D x < tjC1 <    < tn < 1; and deﬁne
Wi D YtiC1Yti for i D 0; : : : ; j2 (with Yt0  0 ), Wj1 D Y
tj Ytj1; Wj D YtjY
tj ;
and for i D j C 1; : : : ; n; Wi D Yti  Yti1: Under the prior distribution, W0s
are independent with their joint density, with respect to some convenient product
measure, can be written as
fW .w0;w1; : : : ; wn/ D
n
Y
iD0
fWi .wi/ :

150
4
Priors Based on Levy Processes
Given F; the probability that X > x is
1  F .x/ D eYx D ePj
iD0wi;
and therefore
fW .w0;w1; : : : ; wnjX > x/ _
jY
iD0
ewifWi .wi/
n
Y
iDjC1
fWi .wi/ :
This shows that the W’s are independent a posteriori and hence the posterior
distribution of F is neutral to the right. Similarly, for the case X  x it can be
seen that
fW .w0;w1; : : : ; wnjX > x/ _
j1
Y
iD0
ewifWi .wi/
n
Y
iDj
fWi .wi/ ;
from which the theorem follows.
For the general case of sample size n which may include censored data, Ferguson
and Phadia derive the posterior distribution of F and give compact formulas in terms
of the posterior MGF of the process Yt. The idea was to be able to compute posterior
moments—obviously this was before the advent of simulations. The formulas are
given in Sect. 3.3.2 and illustrated with the calculation involved in two speciﬁc cases
where the Yt process is assumed to be homogeneous so that the Levy measure is of
type Nt ./ D  .t/ N ./ ; where  .t/ is continuous nondecreasing function and N is
any measure, both on .0; 1/ :
For the uncensored data case, the result is as follows. Let Mt .
/ D Ee
Yt denote
the MGF; u1 < u2 <    < uk be the distinct ordered values among the sample
of n observations x1; : : : ; xn with ı1; : : : ; ık denoting the number of uncensored
observations at u1; : : : ; uk, respectively, so that Pk
1ıi D nI hj D Pk
iDjC1ıi denote
the number of xi greater than ujI and j.t/ denotes the number of ui less than or equal
to t; M
t .
/ denotes the MGF of Yt, M
t .
/ D lims!t Ms.
/; for s < tI and ﬁnally,
let Gu .s/ denotes the prior distribution of the jump in Yt at u and Hu .s/ its posterior
distribution, given X D u for a single observation. Then we have
Theorem 4.15 (Ferguson and Phadia) Let F be a random distribution function
neutral to the right, and let X1; : : : ; Xn, be a sample of size n from F. Then the
posterior distribution of F given the data is neutral to the right, and Yt has posterior
MGF
Mt.
 j data/ D Mt.
 C hj.t//
Mt.hj.t//

j.t/
Y
iD1
"
M
ui.
 C hi1/
M
ui.hi1/
 Cui.
 C hi; ıi/
Cui.hi; ıi/

Mui.hi/
Mui.
 C hi/
#
;
(4.2.15)

4.2
Processes Neutral to the Right
151
where, if u is not a prior ﬁxed point of discontinuity of Yt;
Cu.˛; ˇ/ D
 Œcc R 1
0
e˛z.1  e z/ˇ1d Hu.z/ if ˇ  1
1
if ˇ D 0;
(4.2.16)
while, if u is a prior ﬁxed point of discontinuity of Yt; then d Hu.z/ D .1 
e z/d Gu.z/:
Now it is easy to evaluate posterior moments of F: For example, the posterior
expectation of the survival function S D 1  F is obtained by plugging 
 D 1
in the above expression, E.S.t/jdata/ D Mt.1jdata/. However, the difﬁculty is still
encountered in ﬁnding the posterior distribution Hu .s/ of the jump at the point of
discontinuity. Nevertheless, it is shown that in the case of homogeneous processes
this is easy to do.
For the case of homogeneous processes, there are no prior ﬁxed points of
discontinuities, b.t/  0 and the Lévy measure of Yt has the simple form, dNt.z/ D
.t/d N.z/: Thus the above formula simpliﬁes. Yt D  log.1  F.t// has the MGF
Mt.
/ D exp

.t/
Z 1
0
.e
 z  1/d N.z/

.
(4.2.17)
The posterior distribution of the jump S in Yt at a point x that is not a prior ﬁxed
point of discontinuity is given by
Hx.s/ D
Z s
0
.1  e z/d N.z/=
Z 1
0
.1  e z/d N.z/,
(4.2.18)
independent of x and of .t/.
For the sample of n observations, the posterior MGF turns out to be
Mt.
 j data/ D
j.t/
Y
iD1
"
M
ui.
 C hi1/
M
ui.hi1/
 Cui.
 C hi; ıi/
Cui.hi; ıi/

Mui.hi/
Mui.
 C hi/
#
Mt.
 C hj.t//
Mt.hj.t//
;
(4.2.19)
where
Cu.˛; ˇ/ D
Z 1
0
e˛z.1  e z/ˇ1d Hu.z/:
(4.2.20)
For the gamma process prior, substitute d N.z/ D ezz1dz and for a simple
homogeneous process, d N.z/ D ez .1  ez/1 dz; in the above formula (see
Ferguson and Phadia 1979).
In view of the complication of evaluating the posterior distribution in general,
and the need to simulate the same, the above description is not helpful and alternate
descriptions are given in Hjort (1990) and Walker and Damien (1998) in terms of

152
4
Priors Based on Levy Processes
the set of ﬁxed points of discontinuities and the Levy measure. This will allow
simulation of increments of intervals in which no observation fall, the simulation
of jump sizes at ﬁxed points of discontinuities, and the posterior form of the Levy
measure. To carry out the simulation, procedures are developed in Damien et al.
(1995), Damien et al. (1996), Bondesson (1982), and Wolpart and Ickstadt (1998),
which are described in the sections dealing with these processes.
In practical applications, we need to simulate the jumps at ﬁxed points of
discontinuities and the increments which have ID distribution. The simulation
procedures are discussed next.
4.2.3.1
Simulation of the Posterior Process
As has been recognized so far that except for the DP, the posterior distribution for
the NTR process, and processes that follow, is difﬁcult to handle. For this reason,
most of the papers published early in 1970s and 1980s concentrated on getting
estimates, mainly the posterior mean, in closed form and because of this limitation,
the priors developed were limited in their applications to carry out the full Bayesian
analysis. This changed with the pioneering paper of Damien et al. (1995) in which
they developed simulation procedures for generating the posterior distribution thus
obviating the need for applications limited to estimation alone. Since then it has
become routine in application of nonparametric Bayesian methodology in analyzing
data in various ﬁelds including the so called big data. Bayesian analysis in covariate
and spatial data modeling was presented in the previous chapter. Here we describe
brieﬂy how the simulation of posterior distribution can be carried out for the case
when the prior is an NTR process. This would be instructive since the other priors
included in subsequent sections belong to the NTR family. For details the original
papers can be referred.
If F is NTR, it can be written as F .t/ D 1  eYt and Yt is a nondecreasing
process with independent increments. Yt corresponds to a CRM and hence it can
be constructed using the Poisson process methodology (Kingman 1967, 1993).
This approach is suggested in Wolpart and Ickstadt (1998). Alternate approach
is to recognize that an increment consists of sum of two parts: the jump part
and the continuous part. They are independent by virtue of the process deﬁned.
Thus they can be generated separately. The jump components are independent
random variables and hence they can be simulated by standard rejection algorithms
once their prior densities are speciﬁed. The random variates corresponding to the
continuous part is known to have an ID distribution and their simulation is achieved
via the procedure of sampling an ID distribution described earlier.
Damien et al. (1995) use the theory developed in Ferguson and Phadia (1979)
in terms of the posterior MGF to simulate this part of the process. As indicated
earlier, their motivation arises from the fact that the characteristic function of any
ID distribution can be expressed as the limit of Poisson-type characteristic functions.
Thus in principle any ID random variable can be expressed as the inﬁnite weighted
sum of Poisson-type random variables. However, how to identify the weights is not

4.2
Processes Neutral to the Right
153
clear. Therefore, they replace the weights by suitable functions of random variables
derived from the Levy measure of the process. Their algorithm to generate random
variables approximating ID distributions described earlier is as follows:
1. Generate x1; : : : ; xn
iidÏ .1=k/ d
 .x/ ; where k D
R 1
0
d
 .x/ and 
 .x/ is a ﬁnite
measure. (d
 .x/ D x .1 C x/1 dN .x/ if the Levy form of the MGF is available.)
2. Generate yi Ï Poisson .k .1 C xi/ =nxi/ ; i D 1; : : : ; n;
3. Deﬁne z D Pn
iD1xiyi:
Their detail steps for generating a random variate from the distribution of an
increment of the process are as follows:
(a) Given g.w/ the distribution of jump at a ﬁxed point of discontinuity, generate
w Ï g.w/ via Gibbs sampler or rejection algorithm.
(b) Let d
 .x/ D f.x/dx: Generate a sample x1; : : : ; xn of size n from the density
f.x/:
(c) Evaluate the normalizing constant k D
R 1
0
f.x/dx:
(d) Simulate n Poisson variates y1; : : : ; yn with parameter i D k .1 C xi/ =nxi:
(e) Set z D Pn
iD1xiyi:
(f) Set v D w C z  v is a random variate from the desired distribution of the
increment in question.
For example, in the case of the simple homogeneous process the steps (a)–(c) are
as follows. Steps (d)–(f) are self-evident.
(a) the distribution of jump W is given by
g .w/ / exp f . C s/ wg :
(b) f .z/ / exp f . C s/ zg z= .1 C z/ .1  exp .z// ; where s is the number of
observations to the right of interval whose increment is under consideration.
(c) the normalizing constant k can be evaluated by numerical calculations since the
numerator of f .z/ is an exponential density.
See their paper for a numerical example.
Simulation of the posterior process is also given in Walker and Damien (1998)
paper and based on the description given above of the same (see Theorem 12).
Ignoring the deterministic part, an NTR process is described by three quantities:
The set M D ft1; t2; : : :g of ﬁxed points of discontinuities, f1; f2; : : : ; the densities
(or distributions) of the jumps there, and Nt./ the continuous Lévy measure.
The Levy measure is usually taken as dNt.z/ D dz
R
.0;t a .z; s/ ds: For example,
beta-Stacy process with parameters ˛ ./ and ˇ ./ arises when
a .z; s/ ds D .1  exp .s//1 ŒexpŒzˇ .s/d˛ .s/ :
(4.2.21)
In view of the properties of NTR, the jumps at ti’s and the continuous increments
are all independent and need to be simulated. To simulate the posterior distribution

154
4
Priors Based on Levy Processes
given a single observation X (can be extended easily to any sample size), we note
that if X is not one of the ﬁxed points of discontinuity and X > x; the set M does
not change and the jumps at ﬁxed points of discontinuities can be sampled from
the posterior densities of the form _ .1  exp .s// .es/	 ;  and 	 nonnegative
integers, given in the description of the posterior distribution above. This can be
done via the Gibbs sampler or Metropolis–Hastings procedure.
If X D x is not a prior ﬁxed point of discontinuity, then an additional point
of discontinuity is added. Since the increments are inﬁnitely divisible, it needs
to be sampled separately for which a general algorithm was described earlier.
Similarly for the increment where no observation fall will have an ID distribution
and can be handled as mentioned before. Walker and Damien (1998) give details
of necessary steps. Their approach is based on the fact that the distribution of
an ID random variable is the same as the distribution of
R
.0;1/ zdP .z/ ; where P
is a Poisson process with mean measure dz R
I a .z; s/ ds; I being the interval of
the increment and a ./ being the updated form of a ./. Procedures pertaining to
speciﬁc processes are given in respective sections dealing with those processes.
4.2.3.2
Characterization
As mentioned in Sect. 2.1 that if X1; X2; : : : is an exchangeable sequence of random
variables deﬁned on .0; 1/ ; then from a theorem of De Finetti, there exists a
random distribution function F conditional on which X1; X2; : : :
iidÏ F; and a De
Finetti measure 	 on FC; known as prior for F; such that for any n the joint
distribution of X1; X2; : : : ; Xn is
P .X1 2 A1; : : : ; Xn 2 An/ D
Z
n
Y
iD1
F .Ai/ 	 .dF/ :
(4.2.22)
Lo’s (1991) characterization of the Dirichlet process is: If the posterior mean of the
random probability P; given a sample X1; : : : ; Xn from P; is linear in the empirical
distribution function, then P is a Dirichlet random probability. When expressed in
terms of the predictive probability based on a sequence of exchangeable random
variables X1; : : : ; Xn;
P .XnC1 2 AjX1; : : : ; Xn/ D 'n
 n
X
iD1
ıxi .A/
!
;
(4.2.23)
a function of the number of previous observations falling in the set A: In a similar
spirit, Walker and Muliere (1999) proved the following characterization of the
process neutral to the right. They drew their inspiration from W.E. Johnson’s (see
Zabel 1982) characterization of the Dirichlet distribution, namely the conditional
probability of an outcome belonging to cell k is a function of number of previous

4.2
Processes Neutral to the Right
155
observations that fell in cell k: Let X1; X2; : : : be a sequence of random variables
with each Xi deﬁned on .0; 1/ ; Xi Ï F; such that
P .XnC1 > tjX1 ; : : : ; Xn/ D
tY
0

1  dH

s; N fsg ; NC .s/

;
(4.2.24)
where H ./ is the cumulative hazard function of F; N fsg denote the number of Xi
equal to s; NC .s/ the number of Xi greater than s, i D 1; : : : ; n; and
dH

s; N fsg ; NC .s/
 
1  dH

s; N fsg C 1; NC .s/

D dH

s; N fsg ; NC .s/ C 1
 
1  dH

s; N fsg ; NC .s/

(4.2.25)
for all s > 0; and Qt
0 represents a product integral. Then the sequence is
exchangeable with de Finetti measure a process neutral to the right.
Another characterization is also given by Dey, Ericson, and Ramamoorthi (2003).
For a discrete distribution function F with S D 1  F; the hazard rate is deﬁned
as F fsg =S .s/ ; and the corresponding cumulative hazard function as H ./ D
P
s./ŒF fsg =S .s/: Now let F0 be the set of all distribution functions deﬁned
on the set f1; 2; : : :g : Then for F 2 F0 and … an NTR on F0; we have
E .S . j// D E
	
S .j  1/ 
S . j/
S .j  1/

D E .S .j  1// E
	
S . j/
S .j  1/

:
Therefore,
E
	
S . j/
S .j  1/

D
E .S . j//
E .S .j  1//:
That is E .HF/ D HE.F/: This relationship leads to the following characterization of
NTR on F0: Let … be a prior on F0 such that for all i  1; S .i/ =S .i  1/ 2 .0; 1/
a.s. Then it is NTR if and only if for all n;
E…jX .HF/ D HE…jX.F/ and for any k; E…jX>k .HF/ D HE…jX>k.F/;
where the expectation is with respect to the posterior distribution. It is conjectured
by them that a similar characterization holds in general for F:
Doksum had noted that for the NTR process prior, the posterior distribution
of S .t/ depends upon the number of observations less than t and the number of
observations greater than t and not where they fall, a property shared by the DP.
Dey et al. show that this property essentially characterizes the NTR process.

156
4
Priors Based on Levy Processes
4.2.4
Spatial Neutral to the Right Process
For accommodating a covariate in nonparametric Bayesian analysis, MacEachern
(1999) deﬁned a family fFx W x 2 g of Dirichlet processes, where  is a subset of
Rd; and called them Dependent Dirichlet processes (covered in Sect. 3.3). Note that
marginally, each one of them is a DP. If we want to extend this concept to the neutral
to the right processes, it is not clear how this can be done directly. James (2006)
approaches this problem through the cumulative hazard function of F: Suppose we
have random elements .T; X/ deﬁned on Œ0.1   having a distribution F.dt; dx/:
The alternative deﬁnition F.t/ D 1  eY.t/ of NTR does not provide any help.
However, in dealing with event history data, Kalbﬂeisch (1978) and Hjort (1990)
work directly with cumulative hazard function ƒ deﬁned as ƒ .dt/ D F.dt/=S .t/ ;
where S .t/ D 1  F .t/ is the corresponding survival function. When F is a NTR
process, ƒ is a Levy process or a CRM, and when ﬁxed points of discontinuities
are removed, admits a representation (see section on CRMs) with Levy measure
dNt .s/ D  .ds; dt/ D t .ds/ ƒ0.dt/ say, where t .ds/ is the distribution of jump
in Levy process at location t and ƒ0 can be obtained from the initial guess F0 of F:
Now by extending ƒ to a completely random hazard measure ƒ .dt; dx/ on the space
Œ0.1  ; James (2006) deﬁnes a spatial neutral to the right process on Œ0.1  
as
F .dt; dx/ D S .t/ ƒ .dt; dx/ :
(4.2.26)
Its Levy measure
 .ds; dt; dx/ D t .ds/ ƒ0 .dt; dx/ ;
(4.2.27)
where ƒ0 .dt; dx/
D
F0.dt; dx/=S0 .t/ is chosen based on the initial guess
speciﬁcation of F0.dt; dx/: Clearly, the marginal yields an NTR process. This
deﬁnition can be extended to include prior ﬁxed points of discontinuities.
If
t .ds/ ƒ0 .dt; dx/ D c .s/ s1 .1  s/c.s/1 dsƒ0 .dt; dx/ ; 0 < s < 1;
for c .s/ a positive function on Œ0; 1 yields a natural extension of Hjort’s (1990)
beta cumulative hazard process to beta processes on Œ0.1  ; and thus deﬁnes
what James calls beta-neutral distributions on Œ0.1  : This extension facilitates
the implementation of the neutral to the right mixture models on the same line as
was done for Dirichlet mixture models in Sect. 2.4. He also derives the posterior
distribution of spatial NTR process given the data .t1; x1/ ; : : : ; .tn; xn/ from F: See
his paper for details.

4.3
Gamma Process
157
4.3
Gamma Process
As noted earlier, a neutral to the right process F may be viewed in terms of a process
with nonnegative independent increments Yt via the representation F.t/ D 1  eYt.
This approach offered many possibilities that have been exploited. When F is
continuous, H .t/ D  log .1  F.t// is the cumulative hazard function. This shows
the possibility of utilizing independent increment processes as priors for H .t/ :
Kalbﬂeisch (1978) was the ﬁrst one to explore this possibility in the context of
Bayesian analysis of survival data. Writing a survival function S .t/ D eH.t/; it is
clear that H .t/ can be modeled as an independent increment process. This is done
by choosing a gamma process prior, which has increments distributed independently
as gamma distribution. However, his interest was in analyzing a regression model,
the Cox model. It is expressed as S.t/ D 1F.t/ D exp
˚
H.t/eˇW
; where W is a
vector of covariates and ˇ is the vector of regression coefﬁcients and exp fH.t/g D
P .T  tjW D 0/ is the baseline distribution. Since his objective was to estimate the
regression parameter ˇ; H was considered to be a nuisance parameter. Estimation
of ˇ proceeded by determining the marginal posterior distribution of data, having H
eliminated. Later Wild and Kalbﬂeisch (1981) extended the work of Ferguson and
Phadia (1979) to incorporate covariates (see Sect. 7.7).
4.3.1
Deﬁnition
Let G.˛; ˇ/ denote the gamma distribution with shape parameter ˛ > 0 and scale
parameter ˇ > 0. Let ˛ .t/ ; t  0 be an increasing left continuous function such
that ˛ .0/ D 0.
Deﬁnition 4.16 Let Zt; t  0 be a stochastic process such that (i) Z0 D 0, (ii) Zt
has independent increments in non-overlapping intervals; and (iii) for t > s, the
increment Zt  Zs is distributed as G.c.˛ .t/  ˛ .s//; c/, where c > 0 is a constant.
Then Zt is said to be a gamma process with parameters, c˛ .t/ and c, and denoted as
G.c˛ .t/ ; c/:
It is clear that ˛ .t/ is the mean of the process and c is a precision parameter.
Sample paths are a.s. increasing. It is a special case of the independent nonnegative
increments process with log MGF given by
log EŒexpf
Ztg D 
b .t/ C
Z 1
0
.e
 s  1/d Nt.s/;
(4.3.1)
where the Lévy measure has the form d Nt.s/ D ˛.t/s1ecsds and b .t/  0: Other
properties of the gamma process are well known. Recently, Thibaux (2008) gives
two interesting size-biased constructions of the gamma process.

158
4
Priors Based on Levy Processes
Dykstra and Laud (1981) present a more general approach and develop an
Extended Gamma Process, which is described next.
4.3.2
Posterior Distribution
Assume H Ï G.cH0 .t/ ; c/; where H0 is the prior guess at H. In deriving the
posterior distribution we have to be concerned about the prior ﬁxed points of
discontinuities inherent in the processes with independent increments. Kalbﬂeisch
got around it by assuming H0 to be absolutely continuous, in which case there are
no prior ﬁxed points of discontinuities. He shows that the posterior distribution of
H.t/ is again an independent increments process.
His approach is similar to the one described for the neutral to the right process
in the previous section. For an arbitrary partition of the real line, 1 D t0 <
t1 < t2 <    < tm D 1; let qj denote the hazard contribution of the interval
Œtj1; tj/: Then the cumulative hazard function H.ti/ is the sum of hazard rates rj’s,
H.ti/ D Pi
jD1  log.1  qj/ D Pi
jD1rj; or ri D H.ti/  H.ti1/: Clearly, H is
nondecreasing and by assigning independent distributions to qj’s, a neutral to the
right prior emerges for F. Let ri Ï G .c .H0.ti/  H0.ti1// ; c/ ; i D 1; 2; : : : ; m:
Then by this construction, a gamma process emerges as a prior on the space of
cumulative hazard functions H.t/. It is clear that like the Dirichlet and neutral to the
right processes, the gamma process also yields a random H which is discrete with
probability one.
Given a sample from F, the posterior distribution is derived by identifying the
posterior distributions of the increments, a strategy used by Doksum (1974). Here
it is illustrated for the sample size one. Repeated application of this procedure
yields the solution for any sample size. Now for an observation X D x such
that x 2 Œti1; ti/; the hazard rate ri is the sum of three independent components:
U; the increment to the left of x, J; the jump at x; and V; the increment to the
right of x. U and V are gamma variables. The distribution of V and subsequent
increments in H remain unchanged. While the distribution of U and all increments
prior to x have gamma distribution with scale parameter changed from c to
c C 1 (or by c C eˇW relative to the observation if the regression model is
considered). Thus rj Ï G

c

H0.tj/  H0.tj1/

; c C 1

; j D 1; : : : ; i  1; and
U Ï G .c .H0.x/  H0.ti1// ; c C 1/ : The posterior distribution of the jump J turns
out to be a distribution with density function
fJ .s/ D esc  es.cC1/
s

log
 cC1
c

(4.3.2)
and MGF
MJ .
/ D log ..c C 1  
/ = .c  
// = log ..c C 1/ = .c// :
(4.3.3)

4.4
Extended Gamma Process
159
Putting together all the independent variables, the posterior distribution of H given
X D x is derived. The extension to the sample size n is obvious (see his paper for
details). Clearly the distribution, as one would expect, does not have a closed form.
The conjugacy of Poisson and gamma distributions extend to processes as
well in the same way as the conjugacy of multinomial to the DP and Bernoulli
process to the beta process. Let the measure B Ï G.c; B0/; where c is a constant
concentration parameter and B0 the base measure. If X1; : : : ; Xn
iidÏ Poisson .B/ ;
then the posterior distribution of BjX1; : : : ; Xn is again a G.c C n; Bn/; where
Bn D
c
cCnB0 C
1
cCn
Pn
iD1 Xi:
Damien et al. (1995) give a procedure to simulate the posterior distribution of
the gamma process with parameter  .t/ and scale parameter c, viewed as a process
neutral to the right. Recall that the MGF of gamma process is (Ferguson and Phadia
1979)
Mt .
/ D exp

 .t/
Z 1
0

e
z  1

eczz1dz

:
To use the steps described earlier, we need the following to generate an ID
increment:
(a) the distribution of jump W is given by
g .w/ / w1 exp f .c C s/ wg f1  exp .w/g ;
where s is the number of observations to the right of the interval of the increment
under consideration.
(b) f .z/ / exp f .c C s/ zg = .1 C z/ :
(c) the normalizing constant k can be evaluated by numerical calculations since the
numerator of f .z/ is an exponential density.
4.4
Extended Gamma Process
Apart from the Gamma process, the prior processes discussed so far were con-
structed mainly for the purpose of treating a cumulative distribution function (CDF),
F. The Dirichlet process and its variants were constructed on an arbitrary space of
probability measures, and tailfree (to be presented in Sect. 5.1) and processes neutral
to the right were constructed on the space of distribution functions deﬁned on the
real line. These priors are inadequate if one is interested in density functions, or
hazard rates which play an equally important role in the study of life history data.
In fact, in the context of reliability theory, hazard rates and cumulative hazard rates
play a central role. This led Dykstra and Laud (1981) to investigate the problem
of placing a prior on the collection of hazard rates. In their treatment, they use
processes with independent increments by treating a random hazard rate as a mixture

160
4
Priors Based on Levy Processes
of gamma processes. A by-product of this approach is that it places a prior on
absolutely continuous distribution functions instead of discrete distributions. The
Bayes estimators derived with respect to this prior under the usual loss function also
turn out to be absolutely continuous. These priors are deﬁned on the real line but are
not neutral to the right processes and therefore the results of Doksum (1974) and
Ferguson and Phadia (1979) are not directly applicable.
4.4.1
Deﬁnition
Let F be a left continuous CDF with F.x/ D 0 for x  0; S.x/ D 1  F.x/; H.x/ D
 ln S.x/. If r.t/ is a right continuous function such that H.x/ D
R
Œ0;x/ r.t/dt, then r.t/
is known as the hazard rate. Let ˛.t/; t  0 be a nondecreasing, left continuous real
valued function such that ˛.0/ D 0I ˇ.t/; t  0 be a positive, right continuous real
valued function bounded away from zero with left-hand limits existing; and ﬁnally,
let Z .t/ ; t  0 be a gamma process with independent increments corresponding to
the shape parameter ˛.t/. It is assumed WLOG that this process has nondecreasing,
left continuous sample paths.
Deﬁnition 4.17 (Dykstra and Laud) Let Z .t/ 2 G.˛ .t/ ; 1/: Then a stochastic
process deﬁned by
r.t/ D
Z
Œ0;t/
ˇ.s/dZ.s/
(4.4.1)
is said to be an extended gamma process and denoted by r.t/ v .˛ ./ ; ˇ .//.
..˛ ./ ; ˇ .// is also known as weighted gamma process or a mixture of
gamma processes. Obviously, if r.t/ is random, then correspondingly, F.x/ D
1  expf R
Œ0;x/ r.t/dtg will also be random. Its sample paths are nondecreas-
ing/nonincreasing with probability one and can be used as random hazard rates.
Thus the extended gamma process serves directly as a prior on the class of
increasing/decreasing hazard rate (IHR/DHR) functions. Its Levy formula for the
Laplace transform is
Mr.t/ .
/ D exp
Z 1
0

e
z  1

dNt .z/

(4.4.2)
and
dNt .z/ D
Z t
0
z1eˇ.s/zd˛ .s/

dz:
(4.4.3)

4.4
Extended Gamma Process
161
From Doksum (1974), F.x/ will be neutral to the right only if H.x/ D
R
Œ0;x/ r.t/dt
has independent increments. Clearly, even though r.t/ has independent increments,
H.x/ will not, and hence the distributional results of Doksum are not applicable.
Ammann (1984, 1985) generalizes this approach by recasting the hazard rate as a
function of the sample paths of nonnegative processes with independent increments
which include an increasing component as well as a decreasing component. This
way he is able to deﬁne a broad class of priors over a space of absolutely continuous
distributions that include IFR, DFR, and U-shaped failure rate survival functions.
4.4.2
Properties
1. The MGF of hazard rate is Mr.t/ .
/ D exp
n

R
Œ0;t/ log.1  ˇ.s/
/d˛.s/
o
:
2. 	 .r.t// D E (r.t// = R
Œ0;t/ ˇ.s/d˛.s/:
3. 2 .r.t// D Var .r.t// D
R
Œ0;t/ ˇ2.s/d˛.s/:
4. The marginal and joint survival functions are as follows:
Theorem 4.18 (Dykstra and Laud) If the hazard rate r .t/ has the prior distribu-
tion .˛ ./ ; ˇ .//, then the marginal survival function of an observation X is given
by
S .t/ D P .X  t/ D exp


Z
Œ0;t/
log.1 C ˇ.s/ .t  s//d˛.s/

(4.4.4)
and the joint survival function given n observations X1; : : : ; Xn is
S .t1; : : : ; tn/ D P .X1  t1; : : : ; Xn  tn/
D exp
(

Z
Œ0;t/
log
 
1 C ˇ.s/
n
X
iD1
.ti  s/C
!
d˛.s/
)
;
(4.4.5)
where aC D max.a; 0/
It is easy to derive the posterior distribution given X  x. However, it has the
same difﬁculty as for the neutral to the right processes when X D x:
5. In the case of the Dirichlet process, the parameter F0 was interpreted as prior
guess at the unknown F; and M as the concentration parameter or weight attached
to the prior guess. Likewise, by deﬁning 	 .t/ and 2 .t/ as nondecreasing
functions, the authors feel it reasonable to interpret 	 .t/ as the best guess of the
hazard rate and 2 .t/ as a measure of uncertainty or variation in the hazard rate
at the point t: Then, if 	; 2 and ˛ are assumed to be differentiable, parameters

162
4
Priors Based on Levy Processes
˛ ./ and ˇ ./ may be speciﬁed suitably in terms of 	 ./ and 2 ./ as follows:
ˇ .t/ D
	d2 .t/
dt


	d	 .t/
dt

and
d˛ .t/
dt
D
d	 .t/
dt
2

	d2 .t/
dt

:
(4.4.6)
4.4.3
Posterior Distribution
The conjugacy property for this prior holds only in the case of right censored data.
For the exact observations, the posterior distribution turns out to be a mixture of
extended gamma processes.
Theorem 4.19 (Dykstra and Laud) Let the prior over the hazard rates be
.˛ ./ ; ˇ .//: Then the posterior over the hazard rates,
(i) given m censored observations of the form X1  x1; X2  x2; : : : ; Xm  xm, is
.˛ ./ ; ˇ .// where
ˇ .t/ D
ˇ .t/
1 C ˇ .t/  Pm
iD1 .xi  t/C I
(4.4.7)
(ii) given m observations of the form X1 D x1; : : : ; Xm D xm; is a mixture of
extended gamma processes.
P.r .t/ 2 BjX1 D x1; : : : ; Xm D xm/
D
R
Œ0;xm/   
R
Œ0;x1/
Qm
iD1ˇ .zi/ ‰ .BI Q/ Qm
iD1dŒ˛ C Pm
jDiC1 I.xj;1/ .zi/
R
Œ0;xm/   
R
Œ0;x1/
Qm
iD1ˇ .zi/ Qm
iD1Œd˛ C Pm
jDiC1 I.xj;1/ .zi/
;
(4.4.8)
where ‰ .BI Q/ denotes the probability of the set B 2 B under a stochastic process
distributed as Q D .˛ C Pm
iD1 I.xi;1/; ˇ/
The effect of censored observations is thus to decrease the slope of the sample
paths to the left of the censoring points while leaving it unchanged to the right of
censoring points.
The posterior distribution with respect to exact observations is somewhat compli-
cated. However, the methods of Kalbﬂeisch (1978) and Ferguson and Phadia (1979)
may be used to express it in terms of MGFs.
Damien, Laud, and Smith (1996) give a Monte Carlo method that approximate
random increments of the posterior process. However, they conclude that an alter-
native method given by Bondesson (1982), discussed earlier, is more efﬁcient. Let

4.4
Extended Gamma Process
163
di be the number of exact observations in the i-th interval and fıi be the prior density
of the increment ıi whose Laplace transform is given in the Levy form above. For
the purpose of simulating the posterior density f 
ıi ; they choose g.u/ D eu and
show that  D ˛ .si/  ˛ .si1/, and H.x/ is given by 1  1 R si
si1 eˇ.s/xd˛ .s/ :
Simulation of H creates no problem since it is a mixture of exponentials. To simulate
ıi; their algorithm is
(a) Simulate n exponential variates with parameter  D ˛ .si/  ˛ .si1/ :
(b) Deﬁne n-vector T D .T1; : : : ; Tn/ ; whose elements are the cumulative sums of
exponential variates of step (a). These two steps correspond to simulation from
the Poisson process with intensity parameter :
(c) Sample n independent variates w1; : : : ; wn with distribution proportional to ˛ .s/
restricted to the interval .si1; si:
(d) Sample n exponential variates Z1; : : : ; Zn with parameter ˇ .wi/ eTi; where Ti is
the i-th element of T; corresponds to simulation from H .zI Ti/ :
(e) Deﬁne X D Pn
jD1 Zj  X will have the required ID distribution fıi:
(f) To sample ıi use the rejection algorithm to determine whether X can be retained
as a realization from the density f 
ıi :
(g) Repeat the above steps until X is accepted.
4.4.3.1
Poisson Point Process
In deﬁning the Dirichlet process, Ferguson (1973) was motivated by the fact that
the Dirichlet distribution is conjugate with respect to sampling from a multinomial
distribution. Lo (1982) recognized that the gamma distribution is conjugate with
respect to sampling from a Poisson distribution. So like the Dirichlet process, it
should be possible to deﬁne a gamma process to solve inference problems related
to the Poisson point process from a nonparametric Bayesian point of view. Lo
showed that this is possible via the weighted gamma process introduced above, and
established the following conjugacy property:
Theorem 4.20 (Lo 1982) If the prior distribution of the intensity measure  of a
Poisson point process is .˛; ˇ/; then given a sample N1; : : : ; Nn of size n from a
Poisson point process with intensity measure ; the posterior distribution of  is
.˛ C Pn
jD1 Nj; ˇ= .1 C nˇ//:
4.4.3.2
Weighted Distribution Function
In the Bayesian analysis of weighted sampling models (where the probability of
including an observation in the sample is proportional to a weighting function),
Lo (1993b) shows that the normalized weighted gamma process can be used as
a conjugate prior for the sampling from a weighted distribution. The weighted

164
4
Priors Based on Levy Processes
distribution is deﬁned as
F .dxjG/ D w .x/ G .dx/
R
w .x/ G .dx/;
(4.4.9)
where w .x/ is a known weight function, 0
<
w .x/
<
1; and G is the
unknown parameter. The normalized weighted gamma process is deﬁned as
 ./ D r .t/ =r .C1/ and is denoted by .˛ ./ ; ˇ .//; where ˛ and ˇ are
shape and weight parameters, respectively. Suppose that we have a random sample
X1; : : : ; XnjG
iidÏ F .dxjG/ ; i.e., the probability of including an observation in the
sample is proportional to the weight function w. Then it is shown that if the prior for
G is .˛; 1=w/; then the posterior distribution of GjX is  
˛ C Pn
1ıxi; 1=w

:
4.5
Beta Process I
As noted in the previous two sections, the hazard rates and cumulative hazard rates
play an important role in reliability theory. However, it is not easy to place a prior
on them. While the neutral to the right processes were ﬂexible, they were unwieldy
in practical applications. Nevertheless, as discussed in Sect. 4.3, Kalbﬂeisch (1978)
used a speciﬁc independent increment process—the gamma process, as prior for the
cumulative hazard function (leading to a neutral to the right process on F). Dykstra
and Laud (1981) followed him by treating the hazard rate as a mixture of gamma
processes and constructed a prior on the collection of the hazard rates discussed in
the last section.
Hjort (1990) follows a different approach. Note that when F is absolutely
continuous, G D  ln .1  F/ is the cumulative hazard function. However, to
accommodate the case when F does not have a density, he chooses to deal with
the cumulative hazard rate, which is deﬁned as the sum of hazard rates in the
discrete-time framework (and a limit argument in the continuous case), each having
an independent beta distribution, and develops a new class of prior processes called
Beta processes, thereby placing a prior on the space of cumulative hazard rates.
In his development the focus was on cumulative realization of the samples.
However, by considering the sample realizations itself, Thibaux and Jordan (2007)
found its usefulness in modeling data consisting of unlimited number of features
where the beta process is used as a parameter for the Bernoulli process. Seizing on
this useful application Paisley et al. (2010) provided a stick-breaking construction
parallel to that of the Dirichlet process. Later Paisley et al. (2012) connected this
construction to the Poisson process and proved some additional properties.
Hjort’s construction is based on viewing the cumulative hazard rate as a
stochastic process. Clearly, by construction it is a nondecreasing process having
independent increments. Thus his approach is parallel to that of Doksum (1974),
Ferguson (1974), and Ferguson and Phadia (1979) in which the distribution
function was reparametrized in terms of the cumulative hazard function via the

4.5
Beta Process I
165
representation F .t/ D 1  eYt; t  0; with Yt being a nondecreasing process with
independent increments. This representation facilitated in developing expressions
for the posterior distribution of F as well as for the posterior MGF of Yt. Hjort
follows a similar path which allows him to deal not only with censored data but more
complex models such as Markov Chains and regression models as well. His focus,
however, is on the cumulative hazard rates and for reasons described below, the
formulas of Ferguson and Phadia do not translate directly to his case. He derives his
motivation from the discrete time case and treats the continuous time as a limiting
case. We will, however, consider only the continuous time case.
The idea is as follows. Let the hazard rate be denoted by h .t/ D
F
0.t/
FŒt;1/ D
dF.t/
FŒt;1/,
t  0, and the cumulative hazard function H .t/ D
R t
0 h .s/ ds; where dF .t/ is an
increment at t of F. To permit the deﬁnition of cumulative hazard function when F
has no density, a more general form of the deﬁnition of H; which is valid when F is
absolutely continuous as well, is used.
H.t/ D
Z
Œ0;t
dF.s/
FŒs; 1/;
F.t/ D 1  …
Œ0;tf1  dH.t/g;
(4.5.1)
where …
Œ0;t denotes the product integral over the interval Œ0; t. But this creates a
problem: the increments of H cannot exceed one, i.e., 0  dH .t/ D H .t/H .t/ 
1 for all t: This excludes certain independent increments processes (for example, the
gamma process whose increments may exceed 1 resulting in F being greater than
1). This suggests that the increments dH should have a distribution deﬁned on the
interval Œ0; 1 and the Lévy measure of the independent increments process restricted
to this interval: A natural choice is the beta distribution. But this distribution does
not have the additive property and therefore, the distribution of the increments of
H is only approximately beta distribution over any ﬁnite interval, however, small
the length of the interval might be. With all these considerations in mind, the space
of all cumulative hazard rates restricted to a subspace H, which results F to be a
proper distribution function. To place a prior on H is to assign probability to every
ﬁnite set of increments HŒtj; tj1/ in Kolmogorov consistent way. This is done, and
the existence (Hjort 1990, Theorem 3.1) is proved of a nonnegative, independent
increments process H ./ ; the beta process, whose paths a.s. fall in H. (Hjort denotes
the members of H by A ./ : However, we will use H ./ instead since we would be
focusing only on H:/ Thus H as function of F is a mapping from F to H. If F has
a neutral to the right prior, then the distribution of both G and H turn out to be an
independent increment process. A formal discussion on this relationship is given in
Ghosh and Ramamoorthi (2003).
It is worth noting the following distinction. Recall that earlier F.t/ was expressed
as F.t/ D 1eYt, and Yt was a nondecreasing process with independent increments
and with a countable number of ﬁxed points of discontinuities. If the discontinuities
are removed, then the process is nondecreasing with independent increments and
hence has a simpler Lévy representation which was exploited in Ferguson and
Phadia paper. Hjort deals with the Yt process itself. In this sense the beta process

166
4
Priors Based on Levy Processes
may be viewed as a process leading to a neutral to the right process on F. However,
it is designated as H.t/ process to reﬂect the role of the cumulative hazard rate and it
highlights the distinction. Recall that Kalbﬂeisch (1978) also assigned a prior to the
cumulative hazard function, but he used a gamma process since his focus was not on
the distribution function per se. In the gamma process, the increments are assumed to
be independent gamma random variables and in view of the convolution properties
of the gamma distribution, it worked well. Furthermore, by assuming the baseline
cumulative hazard function, H0 to be absolutely continuous, Kalbﬂeisch avoided
the problem of prior ﬁxed points of discontinuities. Here, the distribution of the
jumps at ﬁxed points of discontinuities are taken as independent beta distributions
and the increments (inﬁnitesimally small) of the process as independently but
approximately beta distributed.
It is shown that the beta process so deﬁned has the usual desirable properties: it
has broad support, it is ﬂexible, it has the conjugacy property with respect to the data
which may include censored data, its parameters have natural interpretations, the
formulas can be expressed in closed forms and updating the parameters for posterior
distribution is easily accomplished. The posterior distribution can be expressed
in terms of what happens to the increments, before, after, and at the observation
x. A particular transformation of the Dirichlet process leads to a special case of
the beta process. In addition, its applications to nonhomogeneous models such as
Markov Chain, competing risks, and covariate models are pointed out. Damien et al.
(1996) provide detailed steps for implementation of the beta process in practice.
Hjort’s very detailed and comprehensive paper contains many useful results and
applications and may be worthwhile to review. In the next section we present an
extension of the beta process on arbitrary spaces in which the cumulative hazard
function H is replaced by a ﬁnite measure in the same way that the distribution
function F was replaced by a random probability measure P while developing prior
distributions in earlier sections.
4.5.1
Deﬁnition
Let H0 be a cumulative hazard with a ﬁnite number of jumps taking place at t1; t2; : : :
and let c./ be a piecewise continuous, nonnegative function on Œ0; 1/. In a time-
discrete model, let X be a random variable taking values in  D f0; 1; 2 : : :g.
(This set can be generalized to the set containing 0; b; 2b; : : : for any arbitrary
positive constant b.) Then h.x/ D PfX D xjX  xg for x D 0; 1; 2 : : : and
H.x/ D Px
sD0 h.s/. Thus, h.x/ D H.x/  H.x/ represents an increment in H.t/
(and h0 in H0/ at t D x. Now let
h.x/ Ï Befc.x/h0.x/; c.x/.1  h0.x//g:
(4.5.2)

4.5
Beta Process I
167
In the time-continuous case, with dH.x/ representing an inﬁnitesimal increment
in H.x/ as well as the size of a jump at tj; j  1, let
dH.x/ Ï Befc.x/dH0.x/; c.x/.1  dH0.x//g:
(4.5.3)
These two cases lead to the deﬁnition of H.t/ viewed as a process with independent
increments and having a speciﬁc Lévy representation. The advantage now is that
E.h.x// D h0.x/ D dH0.x/; and E.H.x// D H0.x/; the prior guesses of h.x/
and H.x/; respectively, and Var.h.x// D h0.x/.1  h0.x//=Œc.x/ C 1 as the prior
“uncertainty.” A formal deﬁnition is as follows:
Deﬁnition 4.21 (Hjort) An independent nonnegative increment process H; also
known as (positive) Levy process, is a beta process with parameters c./ and H0./,
symbolically,
H  Befc./; H0./g;
(4.5.4)
if the following holds: For t  0; 
  0, H has a Lévy representation with MGF
given by
Mt .t/ D log E

e
H.t/
D
X
tjt
log E

e
Sj

Z 1
o

1  es

dLt .s/ ;
(4.5.5)
where Sj D Hftjg D H.tj/  H.t
j /  Befc.tj/H0ftjg; c.tj/.1  H0ftjg/g, fLtI t  0g
is a continuous Lévy measure having the form
dLt.s/ D
Z t
0
c.z/s1.1  s/c.z/1dH0;c.z/ds
for t  0; and 0 < s < 1;
(4.5.6)
and H0;c .t/ D H0.t/  P
tjt
H0ftjg is H0 with its jumps removed. (Note that the
Levy measure can also be alternatively written as H0;c .dt; ds/ D c.t/s1.1 
s/c.t/1dH0;c.t/ds:/
Here, H0 can be interpreted as a prior guess at the cumulative hazard and
c.t/ as a measure of strength in the prior guess (playing the role, respectively, of
F0 D ˛ and M in the Dirichlet process). Thus by deﬁnition, the beta process has
independent increments and at ﬁxed points of discontinuity, each increment has a
beta distribution. The Lévy measure is concentrated on the interval Œ0; 1 instead of
the interval Œ0; 1/.
The existence of beta process is guaranteed by proving ﬁrst the existence of such
a Levy process.
Theorem 4.22 (Hjort) Let H0
2 H be continuous and c ./ be a piecewise
continuous, nonnegative function. then there exists a Levy process H; whose paths

168
4
Priors Based on Levy Processes
a.s. fall in H and whose Levy representation is
log E

e
H.t/
D 
Z 1
o

1  es

dLt .s/ ;
(4.5.7)
where
dLt.s/ D
Z t
0
c.z/s1.1  s/c.z/1dH0.z/ds
for t  0; and 0 < s < 1;
(4.5.8)
Proof Here are the main steps of his proof.
For each n; consider the interval . i1
n ; i
n and deﬁne independent random
variables Xni Ï Be .ani; bni/ ; where ani D cniH0. i1
n ; i
n , bni D cni  ani; and
cni D c
 i 1
2
n

; the value of function c ./ at the midpoint of the interval. Further let
H0.0/ D 0; and deﬁne Hn.t/ D P
i
n t Xni; t  0: Thus Hn represents the sum of
independent random variables. Then it is easy to see that as n ! 1;
E .Hn.t// D
X
i
n t
H0
	i  1
n
; i
n

! H0.t/I and
Var .Hn.t// D
X
i
n t
H0
	i  1
n
; i
n
 	
1  H0
	i  1
n
; i
n

= .cni C 1/
!
Z t
0
dH0.s/ .1  dH0.s//
c .s/ C 1
:
(4.5.9)
Next we need to show that the sequence fHn.t/g converges in distribution to a
Levy process H with the aforementioned properties. The standard technique is to
show that the ﬁnite dimensional distributions of the sequence fHn.t/g converges
properly and that the sequence is tight in the space D of all right continuous
functions on Œ0; 1/ with left-limits, and equipped with the Skorohod topology. This
would imply that the sequence fHn.t/g converges in distribution to a random element
of D: Hjort ﬁrst restricts the space to D Œ0; R ; R > 0 and shows that the sequence
converges to an element of D Œ0; R ; say HŒR; for each R: Then takes HŒR to be the
Œ0; R restriction of the Levy process H on Œ0; 1/ with the Levy measure as given
in the deﬁnition. As a ﬁnal step he shows that H indeed lies in H by noting that its
restriction HŒR on D Œ0; R is closed with respect to the Skorohod topology and that
Hn certainly lies in HŒR for all large n: Thus it follows from Billingsley (1968)’s
Theorem 2.1 that H lies in H with probability 1.
These steps may seem routine, but the work involved is far from simple. A crucial
step involved is in showing the convergenceof ﬁnite dimensional distributions. With

4.5
Beta Process I
169
this in mind, we ﬁrst show
log E .exp .
Hn.t/// !
	Z 1
0

e
s  1

dLt .s/

D
1
X
mD1
.1/m 
m
mŠ
Z t
0
Z 1
o
c .z/ sm1 .1  s/c.z/1 dsdH0.z/:
(4.5.10)
For ﬁx t; the left-hand side is equal to
log E
0
@Y
i
n t
exp .
Xni/
1
A D
X
i
n t
log E .exp .
Xni// :
(4.5.11)
Expanding the exponential on the right-hand side and evaluating the expectations
term by term with respect to the beta distribution, we get
LHS D
X
i
n t
log
"
1 C
1
X
mD1
.1/m 
m
mŠ
 .cni/  .ani C m/
 .ani/  .cni C m/
#
;
(4.5.12)
where  ./ denotes the usual gamma function.
Now expanding the log function and ignoring the higher order terms as they can
be shown to go to 0 as n ! 1; and noting that ani
cni D H0. i1
n ; i
n; we have
LHS D
X
i
n t
1
X
mD1
.1/m 
m
mŠ
.ani C m  1/ : : : .ani C 1/
.cni C m  1/ : : : .cni C 1/ H0
	i  1
n
; i
n

D
1
X
mD1
.1/m 
m
mŠ
2
4X
i
n t
.ani C m  1/ : : : .ani C 1/
.cni C m  1/ : : : .cni C 1/ H0
	i  1
n
; i
n
3
5
!
1
X
mD1
.1/m 
m
mŠ
Z t
0
 .m/  .c .z/ C 1/
 .c .z/ C m/
dH0.z/; as n ! 1
(4.5.13)
which is the right side of (4.5.10).
Likewise it can be shown that for any interval (aj1; aj;
log E
0
@exp
0
@
k
X
jD1

jHn.aj1; aj
1
A
1
A !
k
X
jD1
	Z 1
0

e
js  1

dL.aj1;aj .s/

(4.5.14)
implying that the ﬁnite dimensional distributions of fHng converges properly.

170
4
Priors Based on Levy Processes
4.5.2
Properties
Some of the properties of the beta process are as follows:
1. EŒH.t/ D P
tjt ESj C H0;c.t/ D H0.t/:
2. varŒH.t/ D P
tjt VarSj C
R t
0
dH0;c.s/
c.s/C1 D
R t
0
dH0.s/.1dH0.s//
c.s/C1
.
In Ferguson and Phadia Yt was expressed as Yt D  log.1  F.t// and it
was shown that the property of nonnegative independent increments is preserved
passing from prior to posterior distribution. Here instead we have H.t/ D
 log.1  F.t// and there is a connection between the two. H.t/ is a nonnegative
independent increment process if and only if Yt is. Hence the property of
independent increments is preserved in H as well passing from prior to posterior
distributions. However, the formulas turn out to be different.
3. A prior for the distribution function is neutral to the right if and only if the
corresponding cumulative hazard rate is an independent nonnegative increment
process with Lévy measure concentrated on Œ0; 1.
4. The conjugacy property also holds for the beta process.
Theorem 4.23 (Hjort) Let H  Befc./; H0./g as deﬁned above. Then, given
a random sample which may include right censored observations; the posterior
distribution is given by
Hj data  Be
(
c./ C R ./ ;
Z ./
0
c.s/dH0.s/ C dN.s/
c.s/ C R.s/
)
(4.5.15)
where R.t/ D Pn
iD1IŒXi  t; the number of observations available at time t and
N.t/ stands for the number of uncensored observations less than or equal to t.
As was the case with the processes neutral to the right, the posterior process
contains ﬁxed points of discontinuities at uncensored points even though the prior
may not.
The posterior distribution of a jump at t is
H ftg j data Ï Befc.t/H0 ftgCdN .t/ , c.t/.1H0 ftg/CR .t/dN .t/g:
(4.5.16)
Therefore, in describing the posterior distribution care must be taken. Hjort gives
the description in his theorem (Hjort 1990, Theorem 4.1) which is reproduced in
the next subsection, and is similar to Ferguson (1974) and Ferguson and Phadia’s
(1979) theorems.
5. Let H  Befc./; H0./g and let F.t/ D 1  …
Œ0;tf1  dH.t/g: Then EŒF.t/ D
F0.t/ D 1  …
Œ0;tf1  dH0.t/g; independent of c ./ : Then for k any given positive
constant and choosing c .s/ D kF0Œs; 1/, it is shown that F is a Dirichlet process
with parameter kF0 ./ ; In fact, F may be considered as a generalized Dirichlet
process with two parameters, c ./ and F0 ./ :

4.5
Beta Process I
171
6. Muliere and Walker (1997) have shown that the beta process may also be viewed
as a Polya tree process (see Sect. 5.2).
7. MacEachern (1999) introduced Dependent Dirichlet processes by considering a
family fFx W x 2 g of random distributions, where x was labeled as a covariate
and thus it made possible to extend nonparametric Bayesian analysis to models
involving covariates and auxiliary variables. Similar extension can be considered
for hazard function. James (2006) follows this line and deﬁnes what he calls
spatial neutral to the right processes described earlier.
4.5.3
Posterior Distribution
The beta process being an independent increment process, the theory developed for
Yt in the case of NTR can be adopted here. This is what precisely is done here,
but formulas turn out to be different. In stating the description of the posterior
distribution (needed for simulation), Hjort considers a slightly more general case.
He assumes as prior a process with independent nonnegative increments with ﬁxed
points of discontinuities in order, M D ft1; : : : ; tkg ; f D
˚
fj
k
jD1 ; where fj is the prior
density of jump Sj at tj; j D 1; : : : ; k and Lévy measure dLt.s/ D ds
R t
0 a.s; z/dz on
0 < s < 1; t  0 where a.s; z/ is some continuous nonnegative function such
that
R 1
0 sdLt.s/ < 1: In the case of beta process priors, fj’s are beta densities,
a.s; z/ D c.z/s1.1s/c.z/1dG .s/ ; where G is a continuous nondecreasing function
with G .0/ D 0; and G D H0: (See Theorem 4.12 in Sect. 4.2.)
Theorem 4.24 (Hjort) Given H, let X be a sample of size one from the correspond-
ing distribution function F, and H be a nonnegative independent increment process
with parameters M; f, a.s; z/ and H0: Let x 2 R: Then the posterior distribution of
H is again a nonnegative independent increment process with parameters updated
as follows. Here  is the normalizing constant.
(i) Given X > x, fj .s/ changes to
f 
j .s/ D
 .1  s/fj .s/ if tj  x;
fj .s/
if tj > x
and a.s; z/ gets multiplied by .1  s/ for only z  x: M D M:
(ii) Given X D x, and x D ti for some i, fj .s/ changes to
f 
j .s/ D
8
<
:
.1  s/fj .s/ if tj < x
sfi .s/
if tj D x
fj .s/
if tj > x
and a.s; z/ gets multiplied by .1  s/ for only z  x: M D M:

172
4
Priors Based on Levy Processes
(iii) Given X D x, and x ¤ ti for any i, fj .s/changes to
f 
j .s/ D
 .1  s/fj .s/ if tj < x;
fj .s/
if tj > x
and a.s; z/ gets multiplied by .1  s/ for only z  x; and an additional point of
discontinuity is added to the set M at x; M D M [ fxg ; with density of the jump S
at x, f 
x .s/ D ksa.s; z/; 0 < s < 1:
The general case of size n, which may include right censored data, can be handled
by repeated application of the above theorem. However, as indicated in .iii/ a new
point is added for every uncensored observation, say ur not among tj’s, and hence
we need to specify the density of the jumps at these new points of discontinuity
(assuming no ﬁxed points of discontinuity to start with). This is done and stated in
Hjort’s Theorem 4.2, which resembles Theorem 4 of Ferguson and Phadia (1979).
The density of the jump at ur is given by
f 
r .s/ D snr.1  s/mra .s; ur/ ;
(4.5.17)
where nr is the number of uncensored observations at ur and mr is the number of
observations greater than ur: The density of the jump at tj; a prior ﬁxed point is given
by
f 
j .s/ D snj.1  s/mjfj .s/ ;
(4.5.18)
where nj is the number of uncensored observations at tj and mj is the number of
observations greater than tj: In Ferguson and Phadia, the posterior distribution was
given in terms of the MGF and precise formulas were worked out in two speciﬁc
cases (see Sect. 7.3).
Damien et al. (1996) tailor the general method developed in Damien et al.
(1995) for NTR processes to the present case and provide a technique to simulate
the posterior distribution, which can then be used to carry out the analysis. Their
approach is to discretize the time axis and differs from Hjort’s method in that the
distribution of the increments is not approximated. The distribution of the jump
components at ﬁxed points of discontinuities has beta posterior distribution given
in (4.5.18) and can be generated by the standard rejection algorithm. To generate the
posterior distribution of the continuous component of an increment of the process,
they develop an algorithm to generate approximately random variates from the
posterior process using the Levy formula for its MGF and proceed as follows. When
the ﬁxed points of discontinuities are removed, the distribution of the process has
Levy measure
dL
t .s/ D
Z t
0
fc.z/ C R .z/g s1.1  s/c.z/CR.z/1 c.z/dH0;c.z/
c.z/ C R .z/

ds;
(4.5.19)

4.6
Beta Process II
173
where R .z/ is the number of observations greater than or equal to z: Now divide the
time axis into intervals and let j denote the j-th interval and deﬁne
sdLj .s/ D s
Z tj
tj1
c.z/s1.1  s/c.z/1dH0c.z/ds
(4.5.20)
and let z denote a random variate from the normalizable measure dH0c restricted to
j: Then it is shown that s Ï Be .1; c.z// is a random variate from sdLj .s/ :
Now the algorithm to generate random variates approximating the beta process
increments can be stated as follows:
1. Generate sD s
1; : : : ; s
n
 from sdLj using the above result;
2. Generate yi Ï Poisson

k=ns
i

; i D 1; : : : ; n; where k D
R 1
0 sdLj.s/I
3. Now the sample for the j-th increment is H
j D Pn
iD1s
i yi:
They prove that as n ! 1; H
j tends in distribution to a variate having an
inﬁnitely divisible distribution. Details are given in their paper. They also illustrate
their technique by reworking the example of Ferguson and Phadia (1979) and
comparing the results with the results of Hjort (1990).
Various applications of the beta process in statistical inference problems, such as
estimation of survival function, semiparametric regression models, Markov Chain,
and dynamic Bayesian estimation discussed in his paper, are dealt with in the
Applications Chaps. 6 and 7.
For the analysis of Cox model based on interval censored data and assuming a
discretized beta process model, see Sinha (1997) and Ibrahim et al. (2001).
Hjort’s treatment is extended in two different directions. Kim (1999) allows for
more general censoring scheme leading to a multiplicative intensity model (Aalen
1978). On the other hand, James (2006) associates a new variable x 2 X to the
cumulative hazard and proposed a class of neutral to the right processes called
spatial neutral to the right processes on RC  X; discussed earlier.
4.6
Beta Process II
4.6.1
Beta Processes on General Spaces
While Hjort’s primary interest was in placing a prior over a class of cumulative
hazard functions, it was natural for him to deﬁne the beta process based on the
independent increment process and taking the parameters c and H0 as nondecreasing
functions. The focus was on the cumulative integral of the sample realizations of
the process, whereas in certain applications (such as word occurrence in document
corpora), the attention is on the realizations themselves. This makes it necessary to
deﬁne the beta process on more general spaces and need not be restricted to the real
line only.

174
4
Priors Based on Levy Processes
In Sect. 2.1 we alluded to the Chinese restaurant process (CRP) (see Sect. 4.8.1
for details) which produces an exchangeable sequence of random variables and the
DP serves as its underlying mixing De Finetti measure. Similarly, the IBP (to be
discussed in Sect. 4.8.2) generates an exchangeable sequence of binary matrices and
the question would be, what is the underlying De Finetti measure for this sequence?
This served as a motivation for Thibaux and Jordan (2007) to propose the beta
process presented in this section and show that it is that measure.
As in the case of the Dirichlet process, greater ﬂexibility is achieved by replacing
Hjort’s cumulative process with a random measure B; with parameters c; a positive
function and B0; a ﬁnite base measure, and the Levy measure without the integral.
This is exactly what is done in Thibaux and Jordan (2007). The resulting random
measure is also discrete and enjoys some similarities with the Dirichlet process
in its construction. Thus many of the extensions of the Dirichlet process can also
be formulated for the beta process. The authors also deﬁne a hierarchical set up
for factorial modeling which is an analog of the hierarchical Dirichlet process.
This makes it possible to consider models in which features are shared among a
number of groups. For this general case of the beta process, we use slightly different
notations to distinguish from the earlier section. Thereafter we will present a three-
parameter generalization of the beta process developed by Teh and Gorur (2009) to
take into account the power-law behavior. Similar to the stick-breaking construction
of the Dirichlet process, we will also give stick-breaking construction of the beta
processes. Thibaux (2008) deﬁnes the geometric process and shows that the beta
process is also conjugate with respect to the geometric process.
Let .;  .// be a probability space. Recall that a positive random measure
Q deﬁned on  is said to be an independent increment process (or a completely
random measure Sect. 4.1) if for any pair-wise disjoint sets A1; : : : ; Ak; k > 1 of ,
the measures Q .A1/ ; : : : ; Q .Ak/ are independent random variables. As stated there,
this is a generalization of independent increment processes on abstract spaces. Now
the beta process is redeﬁned as follows:
Deﬁnition 4.25 (Thibaux and Jordan) An independent increment process with
positive increments deﬁned on .;  .// is said to be a beta process B with
parameters c ./ and B0; denoted by B Ï BP .c; B0/ if its Lévy measure for
continuous B0 is given by
 .d!; dp/ D B0 .d!/ c .!/ p1 .1  p/c.!/1 dp
(4.6.1)
on   Œ0; 1 ; where c ./ is a positive function on , known as the concentration
function, and B0 is a ﬁxed base measure on :
As a function of p;  is a degenerate beta distribution. When B0 is discrete of the
form B0 D P1
iD1 qiı!i; where ı!i is a unit point mass at !i 2 ; then B has atoms
at the same locations B D P1
iD1 piı!i; but pi  Be .c .!i/ qi; c .!i/ .1  qi// ; which

4.6
Beta Process II
175
necessarily imply 0  qi  1: B0 can also be expressed as ˛G0 with ˛ the mass
parameter and G0 a smooth base distribution function. ˛ controls the overall mass
of the process and G0 controls the random locations of atoms.
For the case B0 is continuous (nonatomic) such that B0 ./ D ; and c a positive
scalar (can be extended to c, a positive function with extra work), Paisley et al.
(2010) have shown that the beta process B can be derived as a limit of BK; as K !
1; where BK D PK
kD1 pkı!k; with
pk
iid Be
c
K ; c

1  
K

; and !k
iid 1
 B0:
This can be seen as follows. When the K-vector of p’s is integrated out and taking
the limit K ! 1, the marginal distribution produces the two-parameter extension
of the IBP (Ghahramani et al. 2007) which can be shown to have the beta process
as its underlying De Finetti mixing measure. Recall that the Dirichlet process was
shown to be the underlying De Finetti measure for the CRP. Teh et al. (2007) (see
Sect. 4.8.1) proposed a slightly different construction for the IBP in which they take
Be
 c
K ; 1

instead, which simplify the derivation. This construction, however, does
not extend to the two-parameter generalization of the IBP).
To sample a random B is to draw a set of points .!i; pi/ 2   Œ0; 1 from a
Poisson process with mean measure : Since  has inﬁnite mass, there are inﬁnite
number of points. Thus B may be represented as an inﬁnite sum B D P1
iD1 piı!i. It
is a realization of the beta process and is similar to the inﬁnite sum representation of
the Dirichlet process. It shows that like the Dirichlet process, B is also discrete. This
raises a question. Can B be constructed by the stick-breaking construction that was
possible in the case of the Dirichlet process? This has been answered afﬁrmatively
in Paisley et al. (2010).
4.6.1.1
Stick-Breaking Construction
This construction is similar to the one obtained by Sethuraman (1994) for the
Dirichlet process discussed earlier. In their construction, Paisley et al. (2010) use
a special stick-breaking representation of the beta distribution (credited to Jayaram
Sethuraman), namely, X Ï Be .a; b/ can be sampled according to the following
stick-breaking scheme:
X D
1
X
iD1
Vi
i1
Y
jD1

1  Vj

I ŒYi D 1 with Vi
iid Be .1; a C b/ and Yi
iid Bernoulli
	
a
a C b

;
(4.6.2)

176
4
Priors Based on Levy Processes
and where I ŒYi D 1 is the indicator function. Steps for the construction of the beta
process B are
For i D 1; 2; : : : select Ci
iid Poisson ./ I
for k D 1; 2; : : : ; Ci draw V.l/
ij
iid Be .1; c/ and !ik
iid 1
 B0I
set B D
1
X
iD1
Ci
X
jD1
V.i/
ij
i1
Y
lD1

1  V.l/
ij

ı!ij:
(4.6.3)
Here as stated before, c > 0 scalar and B0 is nonatomic ﬁnite base measure. B
can also be expressed as
C1
X
jD1
V1jı!1j C
1
X
iD2
Ci
X
jD1
VijeTijı!ij where Tij
iid Be .i  1; c/ :
Then it has been shown that B  BP .c; B0/ :
Unlike in the case of the Dirichlet process, here each summand consists of a
sum of random number Ci (drawn according to a Poisson distribution) of atoms,
each atom drawn according to 1
 B0 and weighted according to the stick-breaking
scheme. That is, at each round i; Ci is drawn from Poisson ./ : For this round,
round speciﬁc stick i is used. The mass associated with each atom in the i-th round
is equal to the i-th break (discarding the previous i1 breaks) from this stick, where
the stick-breaking weights follow a Be .1; c/ stick-breaking process. In other words,
all atoms !ij are chosen according to 1
 B0: Identify the Ci atoms in the i-th round by
!i1; !i2; : : : ; !iCi: Now corresponding to each of these atoms, take a different stick
and use a Be .1; c/ stick-breaking process. Discard the ﬁrst i  1 cuts and use the
i-th cut as weight for the atom !ij: Superscript denotes the cuts up to the i-th round.
Since the number of prior cuts increases with each round, the attached weights
decrease stochastically as was the case in the stick-breaking construction of the
Dirichlet process. The weights are controlled by the parameter c: As c decreases,
they get smaller more rapidly since smaller and smaller fractions of each stick
remains to yield the weights; if c increases, the weights decay more gradually. The
expected weight on an atom in round i is shown to be equal to c.i1/= .1 C c/i : The
number of atoms at each round is controlled by :
In a later publication, Paisley et al. (2012) have shown that the above stick-
breaking construction can be derived from the characterization of the beta process as
a Poisson process with the mean measure . Let 1j D V1j and ij D Vij exp

Tij

for i > 1; where Tij Ï G .i  1; c/ : Let Bi D PCi
jD1 ijı!ij; and ﬁnally, B D P1
iD1 Bi:
Noting that Ci
iid Poisson ./ ; for each Bi the set of atoms
˚
!ij

form a Poisson
process with mean measure B0: Each !ij is marked with a ij 2 Œ0; 1 that has

4.6
Beta Process II
177
probability measure i (to be indicated below). It is shown that each Bi has an
underlying Poisson process …i D
˚
!ij; ij

; on Œ0; 1 with mean measure B0i:
Then by the superposition theorem (Kingman 1993) (see Sect. 4.1), … D [1
iD1…i is
a Poisson process with mean measure  D B0  P1
iD1 i: Thus B has an underlying
Poisson process with mean measure   i determines the probability distribution
of ij which has density 1 .d/ D c .1  /c1 d for i D 1: The probability
distribution of ij for i > 1 does not have a closed form. However, the authors have
shown that the sum turn out to be a simple expression:
1
X
iD1
i D c1 .1  /c1 :
(4.6.4)
Thus,
 .d!; d/ D B0 .d!/ c1 .1  /c1 d:
(4.6.5)
In the above treatment c was taken to be a scalar. It can be extended to the general
case of function c ./ considered by Hjort (1990) as long as B0 is -ﬁnite. The idea is
to partition the set  as the union of sets Dk with  D [kDk such that B0 .Dk/ < 1;
for each k D 1; 2; : : : . Now for each Dk construct the Poisson process as above with
mean measure
Dk .d!; d/ D B0 .d!/ c .!/ 1 .1  /c.!/1 d for ! 2 Dk
(4.6.6)
and apply the superposition theorem of the Poisson process. Each round of con-
struction adds Poisson .B0 .Dk// new atoms !.k/
ij
2 Dk drawn iid from B0=B0 .Dk/ :
For each of these atoms, the weight .k/
ij
is the i-th cut of a Be

1; c

!.k/
ij

stick-
breaking process. Now the union of these processes yield the beta process in the
general case.
4.6.1.2
Hierarchical Distribution
Just as the Dirichlet process has been found to be useful in hierarchical modeling,
so has been the beta process. Thibaux and Jordan (2007) give an example in
which they model a document by the set of words it contains. They assume that
document Xi;j; i D 1; : : : ; nj and j D 1; : : : ; n; is generated by including each word

 independently with a probability pj

 speciﬁc to category j: These probabilities
form a discrete measure Pj over the space of words and a beta process prior
BP

cj; B

is placed on Pj ’s. In turn, B itself is generated as a realization of a beta
process with parameters c0 and B0: Thus we have an hierarchical model as follows:
Xi;j  Pj; Pj  BP

cj; B

and B  BP .c0; B0/.

178
4
Priors Based on Levy Processes
The beta process is shown to be conjugate with respect to the Bernoulli process
(encountered in connection with feature data modeling and the IBP discussed below)
deﬁned as follows:
Deﬁnition 4.26 (Thibaux and Jordan) Let B be a measure on : An independent
increment process Z on .;  .// is said to be a Bernoulli process with parameter
(hazard measure) B; denoted by Z Ï BeP .B/ if its Lévy measure is given by
	 .d!; dp/ D B .d!/ ı1.dp/:
If B is continuous, then Z is a Poisson process, Z
D
PN
iD1 ı!i where
N Ï Poisson .B .// and !i’s are independently and identically distributed as
B ./ =B ./ : That is a realization of Bernoulli process is a collection of atoms
distributed according to B ./ =B ./ ; each of unit mass or simply a Poisson process
in which weights are either 0 or 1. If B is discrete of the form B D P1
iD1 piı!i;
then Z D P1
iD1 qiı!i; where the atoms are as in B and qi are independent Bernoulli
random variables taking value 1 with probability pi:
4.6.1.3
Posterior Distribution of B
The conjugacy property of the beta process with respect to independent
Bernoulli processes may be used to derive the posterior distribution of B. Let
Z1; : : : ; Zn be a set of independent Bernoulli processes with hazard measure B,
ZijB Ï BeP.B/; i D 1; : : : ; n; and Bjc; B0 Ï BP .c; B0/ ; then the posterior is
BjZ;c; B0ÏBP

c C n;
c
cCnB0 C
1
cCn
Pn
iD1 Zi

which resembles expression (4.5.15).
To generate posterior beta processes, we note the following. The posterior
distribution of BjZ reveals that it is the sum of two independent beta processes.
Say, for n D 1; F1  BP

c C 1;
1
cC1Z1

and G1  BP

c C 1;
c
cC1B0

: This makes
it possible to sample B by ﬁrst sampling Z1 which is a Poisson process with mean
measure B0; then sampling F1 and G1: Let Z1 D PK1
iD1 ı!i: Since the base measure
for F1 is discrete, using the earlier discussion of discrete base measure, we get
F1 D PK1
iD1 piı!i; where pi  Be .1; c/ :G1 can further be decomposed into F2 and
G2: Proceeding inductively, we get for any n; B
dD bBn C Gn where bBn D Pn
iD1 Fi.
At each stage the expected mass of Fn gets accumulated where as the remaining
expected mass, E .Gn .// D
c
cCn decreases to 0: Thus limn!1bBn
dD B and bBn can
be used as an approximation to B: This leads to the following iterative algorithm at
any stage n given by the authors:
Sample Kn  Poisson

c
cCn1

;
Sample Kn new locations !j from 1
 B0 independently,
Sample weight of !j; pj  Be .1; c C n  1/ independently,
Set bBn D bBn1 C PKn
iD1 piı!i:

4.6
Beta Process II
179
Conditional Distribution
The prediction rule based on independent Bernoulli processes drawn from B; and
after eliminating B may be stated as
ZnC1jZn; c; B0 Ï BeP
 
c
c C nB0 C
1
c C n
n
X
iD1
Zi
!
D BeP
 
c
c C nB0 C
K
X
kD1
mk
c C nı!
k
!
;
(4.6.7)
where K is the number of distinct atoms among Z1; : : : ; Zn with atom !
k having mk
frequency. Therefore to sample ZnC1jZn an atom is added at !
k with probability
mk= .c C n/and a Poisson .c= .c C n// number of new atoms are added at new
locations drawn independently from B0=: When c is a constant and B0 continuous,
this can be seen as the two-parameter version of the IBP establishing that the beta
process is the de Finetti mixture measure of the IBP.
4.6.1.4
Geometric Process
Thibaux (2008) deﬁnes a geometric process with hazard measure B; denoted by
X  GeoP .B/ ; as the Levy process with Levy measure given by
 .d!; d/ D .1  B .d!//
1
X
kD1
ık.dp/B .d!/k :
This means that each atom piı!i of B; X has an atom Niı!i; where Ni

Geometric.1  pi/ : It is noted that the beta process is also conjugate to the
geometric process, and the posterior distribution of B is given by
BjX1; : : : ; Xn  BP
 
cn; c
cn
B0 C 1
cn
n
X
iD1
Xi
!
; where cn D c C n C
n
X
iD1
Xi:
4.6.2
Stable-Beta Process
Parallel to the three-parameter generalization of the Dirichlet process, namely
the Pitman–Yor process, Teh and Gorur (2009) generalize the beta process by
introducing a stability parameter thereby incorporating the power-law behavior.
Their motivation is to deﬁne the underlying de Finetti measure of the IBP with
the power-law behavior extending the fact of beta process being the underlying de
Finetti measure of the two-parameter IBP. Thus the new measure is a generalization
of the beta process. This three-parameter generalization was named as a stable-beta
process in view of the fact that it generalizes both the stable (Perman et al. 1992)

180
4
Priors Based on Levy Processes
process and the beta process. It has no ﬁxed point atoms and its Levy measure is
given by
 .d!; dp/ D
 .1 C c/
 .1  /  .c C /p1 .1  p/cC1 dpB0 .d!/ ; 0 < p < 1;
where c >  is a concentration parameter, 0  
< 1 is the stability
parameter and B0 is the base measure. The stability parameter governs the power-
law behavior of the stable-beta process. When  D 0; the process reduces to a
standard two-parameter beta process. It can be shown that E .B .A// D B0 .A/ ; and
Var .B .A// D ..1  / = .1 C c// B0 .A/ for A 2 . See Broderick et al. (2012) for
further elucidation and characterization of the beta process as a CRM.
Stick-Breaking Construction
The size-biased ordering of atoms leads to the following stick-breaking construction
procedure for the stable beta process, where  D B0 ./:
For i D 1; 2; : : : select Ci  Poisson
	
  .1 C c/  .i  1 C c C /
 .i C c/  .c C /

I
For k D 1; 2; : : : ; Ci draw ik  Be .1  ; i  1 C c C / and !ik
iid 1
 B0I
Set B D
1
X
iD1
Ci
X
kD1
ikı!ik:
Posterior Distribution
Since the beta process is a conjugate prior of Bernoulli process, one can easily derive
the posterior distribution which is still a beta process but not a stable-beta process,
since it will include atoms generated by the observed data points. Let Z1; : : : ; Zn
be iid Bernoulli processes with parameter B; a stable-beta process. Since we expect
some repetitions among the atoms, this can further be expressed in terms of distinct
atoms with their respective distributions. Let !
1 ; : : : ; !
K be K distinct atoms with
!
k occurring mk times among Z1; : : : ; Zn; with the distribution Fnk; k D 1; 2; : : : ; K:
Then the posterior distribution of BjZ1; : : : ; Zn is a beta process with the updated
Levy measure given by
n .d!; dp/ D
 .1 C c/
 .1  /  .c C /p1 .1  p/cCnC1 dpB0 .d!/ ;
(4.6.8)

4.6
Beta Process II
181
and the distribution of mass at !
k ; k D 1; 2; : : : ; K given by
Fnk .dp/ D
 .n C c/
 .mk  / .n  mk C c C /pmk1 .1  p/cCnmkC1 dp;
(4.6.9)
which is simply a beta distribution, Be .mk  ; n  mk C c C / :
Conditional and Joint Distribution
The conditional distribution of ZnC1jZ1; : : : ; Zn can also be obtained easily by noting
that
P .ZnC1 .d!/ D 1jZ1; : : : ; Zn/ D E ŒB .d!/ jZ1; : : : ; Zn D
Z 1
0
pn .d!; dp/
(4.6.10)
D  .1 C c/  .n C c C /
 .n C 1 C c/  .c C /B0 .d!/ :
(4.6.11)
This shows that since ZnC1 is a random Bernoulli process, it can be sampled
from a Poisson process deﬁned on the space n
˚
!
1 ; : : : ; !
K

with mean measure
.1Cc/.nCcC/
.nC1Cc/.cC/B0: It will have Poisson

 .1Cc/.nCcC/
.nC1Cc/.cC/

number of new atoms,
each independently and identically distributed according to
1
 B0: Multiplying
successively the conditional probabilities of Zi given the previous ones, we obtain
the joint probability distribution of Z1; : : : ; Zn eliminating B as
f .Z1; : : : ; Zn/ D exp
 

n
X
iD1
 .1 C c/  .i  1 C c C /
 .i C 1 C c/  .c C /
!

K
Y
iD1
 .mk  /  .n  mk C c C /  .1 C c/
 .1  /  .c C /  .n C c/
B0

!
k

:
(4.6.12)
4.6.3
Kernel Beta Process
In the last chapter covariant dependent Dirichlet processes were constructed so that
nonparametric Bayesian analysis of covariate data can be carried out. There an RPM
having a DP prior was attached to each covariate. In the same way, Ren, Wang,
Dunson, and Carin (2011) construct a family of dependent beta processes attached
to each covariate in a covariate space. They call the resulting process as the kernel

182
4
Priors Based on Levy Processes
beta process (KBP). This may be considered as an example of the general approach
developed by Foti et al. (2012) for constructing dependent processes, presented in
the last chapter, Sect. 3.3.
In the IBP, we have a vector Zn of ones and zeros representing whether a
particular feature (patron partakes the dish) is present or not in the n-th individual.
Zn is a Bernoulli process with parameter B; Zn Ï BeP .B/ ; where B is a beta
process with parameters c and base measure B0; B Ï BP .c; B0/ : Like in the case
of DP where an RPM F was associated with a covariate x 2 ;  a covariate
space, here also a beta process is attached to the covariate x; say Bx and let the
set B D fBx W x 2 g. In the DDP, the RPM Fx was constructed in terms of the
Sethuraman representation and the covariate was associated with the location atom
 or weight p: Parallel to that here the inﬁnite sum representation of Bx is constructed
by appealing to the Poisson process construction of a CRM.
Since B is a beta process with Levy measure
 .d; d!/ D c .!/ 1 .1  /c.!/1 dB0 .d!/ ;
(4.6.13)
it can be constructed using the PP with mean measure  and has representation
B D
1
X
iD1
iı!i;
(4.6.14)
where .!i; i/ 2   Œ0; 1 are points of the PP. Since the points are generated
via Poisson(/ distribution and  D
R

R
Œ0;1 .d; d!/ D 1; there are countably
inﬁnite number of points. If Zn is the n-th vector drawn from a BeP .B/ ; then
Zn D
1
X
iD1
bniı!i; bni Ï Bernoulli .i/ :
The data corresponds to the set fZngN
nD1 where the indicator bni is utilized to
represent whether the feature !i 2  is present .bni D 1/ or not .bni D 0/ in Zn:
When B is integrated out, it yields conditional probability for fZng that corresponds
to the IBP.
In the above beta-Bernoulli construction, the same B is used for generation of all
Zn’s implying that the probability i of presence of feature !i remains the same for
each n. Now to incorporate covariates, we associate covariate xn 2  with Zn; and
denote the set of covariates as fxng : Now a condition is imposed that if samples n
and n0 have similar covariates xn and xn0, then it is more likely that they will possess
a similar subset of the features f!ig I if the covariates are distinct, it is less probable
that features will be shared.
Thus the set B will be generalized as
B D fBx W x 2 g D
1
X
iD1
iı!i; !i Ï B0;
(4.6.15)

4.6
Beta Process II
183
where i D fi .x/ W x 2 g is a stochastic process from  ! Œ0; 1 independent
of f!ig ; i.e. B is a set of dependent beta processes Bx D P1
iD1i .x/ ı!i: This is a
general set up and governs various special cases. For example i .x/ can be used to
model predictor-dependent functions in probit, logit, or kernel SB processes. One
such special case is the KBP.
The standard practice is to adjust the weights i .x/ according to the distance
between the value of the covariate x and a set of nearby ﬁxed locations x0 drawn
according to some measure H on  and the distance reﬂected in the kernel K .; / W
 ! Œ0; 1 : In the kernel SB process discussed earlier we had, !i
iidÏ B0; i .x/ D
Vi .x/ Q
k<i .1  Vk .x// ; Vj .x/ D jK .x; x0/ ; where j 2 ‰ is a feature dependent
measure of proximity between x and x0: Another example is the Gaussian kernel
where,  D R; and
K

x; x0
D exp
n

x  x02o
;
(4.6.16)
 a positive real valued random variable distributed according to a measure Q
deﬁned on RC: In this case the revised Levy measure is deﬁned as
x D H dx0 Q .d/  .d; d!/ ;
(4.6.17)
where  .d; d!/ is the Levy measure associated with the beta process.
Thus the dependent beta process can be expressed as Bx D P1
iD1iK

x; x0
i

iı!i:
Here iK

x; x0
i

serves as a Foti et al. (2012) identiﬁer rx0
i indicating whether the
atom i in the global measure is included in the local measure Bx at covariate value
x or not. We supplemented the CRM and PP with covariance space and dependent
measures are now deﬁned on an extended product space   ‰  RC  ; and
the mean measure of the PP is given by x: Therefore draws from the augmented
PP would yield measures Bx: If  D fx1; : : : ; xkg ; then B will be a k-dimensional
vector. This covariance dependent beta process can be used in the feature model to
deﬁne the covariance dependent feature model. With Znx distributed as BeP .Bx/ ;
it generalizes the beta-Bernoulli model, where Znx D P1
iD1bnxiı!i; with bnxi Ï
Bernoulli

iiK

x; x0
i

: This can be expressed equivalently as bnxi D z.1/
xi z.2/
xi ; with
z.1/
xi Ï Bernoulli

iK

x; x0
i

and z.2/
xi Ï Bernoulli.i/ : Thus the process evolves as
a 2-step generalization of the beta-Bernoulli process.
For any Borel set A 2 B; when B is drawn from the KBP and for covariates x
and x0; the authors derive
E .Bx .A// D B0 .A/ E .Kx/
Cov .Bx .A/ ; Bx0 .A// D E .KxKx0/
Z
A
B0 .d!/ .1  B0 .d!//
c .!/ C 1
 cov .KxKx0/
Z
A
B2
0 .d!/

184
4
Priors Based on Levy Processes
where E .Kx/
D
R
‰K .x; x0/ H .dx0/ Q .d/ : If K .x; x0/
D
1 for x
2
; E .Kx/ D E .KxKx0/ D 1; cov .KxKx0/ D 0 and the above results reduces to the
original hierarchical beta process and IBP.
4.7
Beta-Stacy Process
Alternative to the Dirichlet process and processes neutral to the right, Walker
and Muliere (1997a) introduced a new stochastic process, called the beta-Stacy
process, which places a prior on the space F of all distributions functions deﬁned
on Œ0; 1/ (extension to the whole real line is trivial): It is derived by using a
particular independent non-negative increment process Z , namely the log-beta
process deﬁned below as opposed to the gamma process in Kalbﬂeisch (1978) and
the beta process in Hjort (1990). As in the case of beta process, they also consider in
their construction, discrete and continuous cases separately. But unlike in the beta
process they take the increments to be distributed as beta-Stacy distribution with
parameters, ˛ and ˇ and with density given by
f .y/ D
1
B .˛; ˇ/y˛ .x  y/ˇ1
x˛Cˇ1
I.0;x/ .y/
for 0 < x  1;
(4.7.1)
where B .˛; ˇ/ is the usual beta function. If x D 1, the density reduces to that of
a beta distribution. They give interpretation of the parameters in terms of the mean
and variance of F: A key feature is that it gives positive probability to continuous
distribution functions.
The beta-Stacy process generalizes the Dirichlet process in two respects: it has
a broader support so that more ﬂexible prior information may be represented; and
unlike the Dirichlet process, it is conjugate to the right censored data. The parameter
of the Dirichlet process M; a positive constant, is replaced by a positive function
c ./ : Thus a special case of beta-Stacy process yields the Dirichlet process. Also,
when the prior process is assumed to be Dirichlet, the posterior distribution given the
right censored data turns out to be a beta-Stacy process. It is worth noting that while
the Dirichlet process is deﬁned by the joint distribution of probabilities of sets of
any ﬁnite measurable partition of X, the beta-Stacy process is deﬁned on the interval
Œ0; 1/ via the independent nonnegative increment process Z and the representation
of the neutral to the right process. Thus the beta-Stacy process belong to the class
of neutral to the right processes.

4.7
Beta-Stacy Process
185
4.7.1
Deﬁnition
First we deﬁne the log-beta process. Let ˛ ./ (˛ .0/ D 0 ) be a right continuous
measure and ˇ ./ be a positive function, both deﬁned on Œ0; 1/: Let ft1; t2; : : :g be
a countable set of discontinuities of ˛ ./ and deﬁne a continuous measure ˛c .t/ D
˛ .t/  P
tjt ˛ ˚tj
. The log-beta process is deﬁned as follows:
Deﬁnition 4.27 (Walker and Muliere) A stochastic process Z is a log-beta process
on .Œ0; 1/; BC/, with parameters ˛ ./ and ˇ ./, if Z is an independent nonnegative
increment process with log-Laplace transform
log E

e
Z.t/
D
X
tjt
log E

e
Sj

Z 1
o

1  ev

dNt .v/ ;
(4.7.2)
where Sj is the size of the jump at tj; 1  exp

Sj

v Be

˛
˚
tj

; ˇ.tj/

; and the
Lévy measure is given by,
dNt .v/ D
1
.1  ev/
Z t
o
exp.v .ˇ .s/ C ˛ fsg// d˛c .s/ dv;
v > 0:
(4.7.3)
In contrast to the beta process, here the Levy measure is not restricted to .0; 1/
interval and instead of jumps themselves, their functions are distributed as beta
distribution.
To deﬁne the beta-Stacy process in terms of the parameters c ./ and G; G a
distribution function to be interpreted as a prior guess of F (similar to F0 in the
Dirichlet process), we require the following.
Let c./ be a positive function, G 2 F be a right continuous function with a
countable set of discontinuities at ft1; t2; : : :g, and Gc.t/ D G.t/  P
tjt G
˚
tj

so
that Gc.t/ is continuous. Let Z be an independent nonnegative increments process
with log Laplace transform
Mt .
/ D log E

e
Z.t/
D
X
tjt
log E

e
Sj

Z 1
o

1  ev

dNt .v/ ;
(4.7.4)
where 1  exp

Sj

v Be

c.tj/G
˚
tj

; c.tj/GŒtj; 1/

and the Lévy measure given
by, for v > 0;
dNt .v/ D
1
.1  ev/
Z t
o
exp.vc .s/ G.s; 1//c .s/ dGc .s/ dv:
(4.7.5)

186
4
Priors Based on Levy Processes
Deﬁnition 4.28 (Walker and Muliere) Let Z be an independent nonnegative
increment process as deﬁned above. Then F is a beta-Stacy process on .Œ0; 1/; BC/
with parameters c ./ and G, denoted by F 2 S .c./; G/ ; if for all t  0; F .t/ D
1  eZ.t/:
Thus we desire a process which has increments distributed (inﬁnitesimally
speaking) as beta-Stacy distribution and also that F is a distribution function with
probability 1. The existence of such a process relies on two steps. First to show the
existence of the log-beta process and then the existence of F such that F 2 F with
probability 1. The proof is on the line of Hjort’s (1990) proof for the existence of
beta process.
Theorem 4.29 (Walker and Muliere) Let G 2 F be continuous and let c ./ be a
piecewise continuous positive function. Then
(i) There exists an independent nonnegative increment process Z with Levy
representation given by
log E

e
Z.t/
D 
Z 1
o

1  ev

dNt .v/ ;
(4.7.6)
where
dNt .v/ D
1
.1  ev/
Z t
o
exp.vc .s/ GŒs; 1//c .s/ dG .s/ dv:
(4.7.7)
(ii) If F .t/ D 1  exp .Z .t// ; then F 2 F with probability 1:
Proof The following are the main steps of their proof:
(i) For i D 1; : : : ; n deﬁne ani D cniGŒ i1
n ; i
n/ , bni D cniGŒ i
n; 1/; and cni D
c
 i 1
2
n

: Let Xni Ï Be .ani; bni/ ; Wni D  log .1  Xni/ ; and let Zn .0/ D 0 and
Zn.t/ D P
i
n t Wni; t  0: We wish to show that the sequence fZn.t/g converges
in distribution properly to a Levy process Z with the aforementioned properties.
We have, upon evaluating the expectation,
log E .exp .
Zn.t/// D
X
i
n t
log E .exp .
Wni//
D
X
i
n t
log  .ani C bni/  .bni C 
/
 .bni/  .ani C bni C 
/:
(4.7.8)

4.7
Beta-Stacy Process
187
Ferguson (1974) has shown that using the formula  .x/ D x1 .x C 1/ ;
the above term involving gamma functions can be written as
k1
Y
mD0
.ani C bni C 
 C m/ .bni C m/
.ani C bni C m/ .bni C 
 C m/   .ani C bni C k/  .bni C 
 C k/
 .bni C k/  .ani C bni C 
 C k/;
(4.7.9)
and further using Sterling’s formula  .x/ Ï .2x/1=2 .x=e/x for large x; it can
be shown that the term involving gamma functions tends to 1 as k ! 1: Thus
log E .exp .
Zn.t/// D
X
i
n t
1
X
mD0
log .ani C bni C 
 C m/ .bni C m/
.ani C bni C m/ .bni C 
 C m/
D
X
i
n t
Z 1
0

e
  1
 exp .bni/ .1  exp .ani//
 .1  exp .//
d;
(4.7.10)
where the second equality follows from the Levy representation
log

   D
Z 1
0

e  1
 e

d;
(4.7.11)
for the moment generating function of the negative exponential distribution
with parameter  [see Ferguson (1974)’s Lemma 1]. Noting that
X
i
n t
exp .bni/ .1  exp .ani// ! 
Z t
0
c .s/ exp .c .s/ GŒs; 1// dG .s/ ;
(4.7.12)
as n ! 1; it follows that
log E .exp .
Zn.t/// !
Z 1
0

e
  1

.1  exp .//
Z t
0
c .s/ exp .c .s/ GŒs; 1// dG .s/ d
D
Z 1
0

e
  1

dNt ./ :
(4.7.13)
Likewise it can be shown that for any interval (aj1; aj;
log E
0
@exp
0
@
k
X
jD1

jZn.aj1; aj
1
A
1
A !
k
X
jD1
	Z 1
0

e
j  1

dN.aj1;aj ./

;
(4.7.14)

188
4
Priors Based on Levy Processes
implying that the ﬁnite dimensional distributions of fZng converge properly.
Now following the arguments used for the beta process, the existence of the
process Z can be concluded:
(ii) To show that such a process F exists; we proceed as follows. Let Yn1 D Xn1
and Ynk D Xnk
Qk1
jD1

1  Xnj

; for k > 1: Then the joint distribution of
.Yn1; : : : ; Ynm/ is G .an1; bn1; : : : ; anm; bnm/ : Letting Fn .t/ D P
k
n t Ynk and
Fn .0/ D 0; we have
 log .1  Fn .t// D 
X
k
n t
log .1  Xnk/ D
X
k
n t
Wnk D Zn.t/;
implying that fFng is a discrete time beta-Stacy process and Fn D 1 
exp .Zn.t// : The sequence fFng converges in distribution to F. F 2 F with
probability 1 since R 1
0
dG .s/ =GŒs; 1/ D 1: Further details may be found in
their paper.
Note that in deﬁning the neutral to the right process, F was reparametized in
terms of Yt; a non-negative independent increment process. Here, essentially, Yt is
taken to be the log-beta process. Thus the beta-Stacy process is an NTR process.
The parameters ˛ and ˇ of the log-beta process are related, under certain
condition, to the parameters of the beta-Stacy process c ./ and G as follows:
G.t/ D 1 
Y
tkt
	
1 
˛ ftkg
ˇ .tk/ C ˛ ftkg

exp
	

Z t
o
d˛c .s/
ˇ .s/ C ˛ fsg

and c .t/ D ˇ .t/ =GŒt; 1/. ˛ and ˇ can be recovered from c ./ and G via
˛ .t/ D
Z t
o
c .s/ dGc .s/ C
X
tjt
c

tj

G
˚
tj

; ˇ .t/ D c .t/ GŒt; 1/:
As noted above, there is a connection between the beta process and beta-Stacy
process. This connection was explicitly stated in Dey et al. (2003) as follows. The
prior … is a beta-Stacy process with parameters .D; F0/ if and only if it is a beta
process prior with parameters .C; H0/ ; where C D D.1  F0/ and H0 D H .F0/ :
4.7.2
Properties
Here are some of the properties of the beta-Stacy process.
1. If we take c./ D c a constant (D ˛

RC
/ and G ./ D ˛ ./ =˛

RC
continuous,
then dNt .v/ reduces to
dNt .v/ D
dv
.1  ev/
Z t
o
exp.vcG.s; 1//cdGc .s/ D ev˛.0;1/
v.1  ev/

ev˛.t/  1

dv;

4.7
Beta-Stacy Process
189
which is the Lévy measure for the Dirichlet process (Ferguson 1974) with
parameter ˛; and thus F.t/ D 1  eZ.t/ is a Dirichlet process viewed as neutral
to the right process. Here the generality is gained by taking the parameter c as a
positive function instead of a constant.
2. If we replace 1  ev by v in the Lévy measure for log-beta process and assume
˛ to be continuous, upon integrating out v; it can be shown that
log E

e
Z.t/
D 
Z t
o
log.1 C 
=ˇ.s//d˛.s/;
which characterizes the extended gamma process (Dykstra and Laud 1981).
3. The Lévy measure for the beta process dLt .s/ ; with support .0; 1/ ; can be
obtained via a simple transformation of the Lévy measure of log-beta process,
with ˛ assumed to be continuous,
dLt .s/ D
1
1  sdNt . log.1  s// :
(4.7.15)
4. If H is a beta process and dZ D  log .1  dH/ ; then F.t/ D 1  eZ.t/ is a
beta-Stacy process.
5. By taking 
 D 1 in the MGF (4.7.4), the prior mean can be seen to be
E .F .t// D 1 
Y
tkt
	
1 
G ftkg
G.tk; 1/

exp
	

Z t
o
dGc .s/ =G.s; 1/

D 1 
Y
Œ0;t
	
1  dG .s/
G.s; 1/

D G .t/ .
(4.7.16)
The second equality is in the product integral notation.
6. The conjugacy property of the beta-Stacy process with respect to the data which
may possibly include censored observations, is stated in the following theorem:
Theorem 4.30 (Walker and Muliere) Let X1; : : : ; Xn be a random sample, possi-
bly with right censoring, from an unknown distribution function F on Œ0; 1/ and let
F v S .c./; G/ : Then the posterior distribution of F is again a beta-Stacy process
with parameter c./ and G; where
G .t/ D 1 
Y
Œ0;t

1  c .s/ dG .s/ C dN .s/
c .s/ GŒs; 1/ C R .s/

;
(4.7.17)
c.t/ D c .t/ GŒt; 1/ C R .t/  Nftg
GŒt; 1/
;
(4.7.18)
and where as before, N ./ is the counting process for uncensored observations and
R .t/ D Pn
i IŒXi  t.

190
4
Priors Based on Levy Processes
This generalizes Susarla and Van Ryzin (1976) result where F was assumed to
have a Dirichlet process prior.
7. A similar conjugacy result, parallel to that for the gamma and beta processes
holds for the log-beta process as well.
Theorem 4.31 (Walker and Muliere) Given a sample of size n from F with a log-
beta process prior with parameters ˛ .t/ and ˇ .t/, then the posterior distribution is
also a log-beta process with parameters updated as ˛ .t/ C N .t/ and ˇ .t/ C R .t/ 
Nftg:
8. The posterior mean, which is the Bayes estimate of F .t/ under the weighted
quadratic loss function, is given in Chap. 6 and is the same estimator as obtained
by Hjort (1990) for the beta process.
4.7.3
Posterior Distribution
In view of the fact that the beta-Stacy process is an NTR process, the description
given earlier for the posterior distribution of an NTR is equally valid here and is
similar to the one given for the beta process. Again we have sets M of ﬁxed points
of discontinuities, f D
˚
fj

j1 of associated densities of jump at tj; and Levy measure
of the form dNt.z/ D .R t
0 a.z; s/ds/dz: The beta-Stacy process with parameters ˛ ./
and ˇ ./ arises when a.z; s/ds D ezˇ.s/d ˛.s/=.1  ez/. The updated parameters
are M; f and a.z; s/ as described in Theorem 4.12. If, however, X > x (as would
be in the case of censored observation) the parameters are M D M;
f 
j .s/ D
 esfj .s/ if tj  x
fj .s/
if tj > x;
(4.7.19)
and a.z; s/ D
 a.z; s/es if s  x
a.z; s/
if s > x;
(4.7.20)
where  is the normalizing constant.
This approach would allow one to carry out full Bayesian analysis via simula-
tion.
1. The densities to be sampled corresponding to the jumps points of M are of form
f 
j .z/
_
.1  ez/ e	z with integers ; 	  0
(4.7.21)

4.7
Beta-Stacy Process
191
and
f 
j .z/
_
.1  ez/ a.z; s/; with integer  > 0:
(4.7.22)
These can be sampled via Gibbs sampler algorithm.
2. The random variable Z corresponding to the continuous increment of the
interval Œa; b/ is inﬁnitely divisible and therefore the algorithm developed by
Damien et al. (1995) can be used to sample its density as described earlier in
connection with ID distributions. However, Walker and Damien (1998) introduce
an alternative approach based on the fact Z
dD
R 1
0
zdP ./ ; where P ./ is a
Poisson process with mean measure dz
R
Œa;b/ a.z; s/ds: It is similar to the method
of Bondesson (1982) described earlier. Details may be found in their paper along
with a numerical example in which they rework Kaplan and Meier (1958) data
and compare the results with those of Ferguson and Phadia (1979).
4.7.3.1
Characterization
Recall that for the Dirichlet process we had 	.t/ D EŒF.t/ D F0.t/ and VarŒF.t/ D
F0.t/.1F0.t//=.MC1/ and therefore we may set the parameter of the DP as ˛ ./ D
MF0./. The neutral to the right process was described in terms of the stochastic
process with Lévy measure Nt./. As 	.t/ and VarŒF.t/ characterizes the Dirichlet
process, Walker and Damien (1998) deﬁne two functions 	.t/ and VarŒF.t/ in terms
of the Lévy measure that characterizes the neutral to the right process, and more
generally, the beta-Stacy process. Note that S.t/ D 1  F.t/ D eZ.t/; EŒS.t/ D
EŒeZ.t/. So with no ﬁxed points of discontinuity, consider functions
	.t/ D  log EŒS.t/ D  log EŒeZ.t/ D
Z 1
0
.1  ez/d Nt.z/
(4.7.23)
and
.t/ D  log EŒS2.t/ D
Z 1
0
.1  e2z/d Nt.z/;
(4.7.24)
where Nt./ is a Lévy measure as before. Also, since .EŒS.t//2 < EŒS2.t/ <
EŒS.t/; 	 and  satisfy 0 < 	.t/ < .t/ < 2	.t/. Thus to characterize the process,
it is required to ﬁnd Nt satisfying these two equations. They consider Lévy measures
of the type
dNt.z/ D .1  ez/1
Z t
0
ezˇ.s/d ˛.s/dz;
(4.7.25)

192
4
Priors Based on Levy Processes
where ˇ./ is a nonnegative function and ˛ ./ is a ﬁnite measure and show that this
type of Nt characterizes the beta-Stacy process and covers many neutral to the right
type processes. In particular, the Dirichlet process arises when ˇ.t/ D ˛.t; 1/, and
the simple homogeneous process (Ferguson and Phadia 1979) emerges when ˇ is
constant.
They prove the existence of such ˛ ./ and ˇ ./ which satisfy
	.t/ D
Z 1
0
Z t
0
ezˇ.s/d ˛.s/dz and .t/ D
Z 1
0
Z t
0
.1  e2z/
.1  ez/ ezˇ.s/d ˛.s/dz:
(4.7.26)
If ˇ is constant, then 	 and  are related to ˛ ./ and ˇ through 	.t/ D ˛ .t/ =ˇ
and .t/ D c˛ .t/ =ˇ where c D .1 C 2ˇ/=.1 C ˇ/. In general, they discuss a
meaningful way to choose ˛ ./ and ˇ ./. It is important to note that this method
allows one to specify the mean and variance for F.t/ which in general is not possible
for the neutral to the right processes.
4.8
NB Models for Machine Learning
The aim of machine learning is to develop computational programs which would
improve performance given the observed data. To facilitate this task, the search is
for generative probabilistic models for discrete data, such as text corpora, which
would allow the practitioner to detect hidden structures, patterns, or clusters.
Nonparametric Bayesian methods provide a rich tools kit for such tasks and have
been increasingly becoming popular in such exploration, see, for example, Shahbaba
and Neal (2009), Hannah et al. (2011), Wade et al. (2014), Blei and Frazier (2011)
and Blei et al. (2003). These authors have shown the beneﬁt of using Dirichlet
process mixtures. However, it is clear that other processes presented in this book
and illustrated their use at various places, may also be used.
Generally, the use of mixture models assumes that one latent cause is associated
with each observation or data point. This assumption can be quite restrictive (Titsias
2008). As an alternative to multinomial representation underlying mixture models,
factorial models which associates each data point a set of latent variables seems
to be more preferable in featural modeling. An added advantage is that we do not
need to know the cardinality of features a priori. Besides the CRP mentioned earlier
in Sect. 2.1, Indian buffet and inﬁnite gamma-Poisson processes in particular have
garnered a lot of interest from outside the statistical community, and found useful in
a wide range of interesting applications. These processes, along with the Bayesian
nonparametric hierarchical modeling (as is related to machine learning) have proved
to be indispensable tools in solving problems in the areas such as information
retrieval and word segmentation.

4.8
NB Models for Machine Learning
193
The CRP is a marginal distribution in the case of DP prior and it is characterized
as a prior distribution over partitions of N objects distributed in K classes. This
can be conveniently represented by an N  K matrix, each row having a single
entry of one and rest of the entries of zeros. In featural models, the columns may
serve as features. To permit the possibility of each object or subject (data point)
may have multiple features, Grifﬁths and Ghahramani (2006) proposed a process
called the IBP, which is essentially a distribution over a class of equivalent binary
matrices. Titsias (2008) saw this as a limitation. To deal with features having
multiple occurrences, a natural generalization would be to replace each non-zero
entry with an entry of positive integer. Titsias (2008) followed this extension and
developed a process called inﬁnite gamma-Poisson process. It is interesting to note
that the conjugacy of the DP to multinomial sampling results in the CRP, conjugacy
of beta process to Bernoulli process yields the IBP, and conjugacy of gamma process
to Poisson process sampling yields the inﬁnite gamma-Poisson process.
These processes are found to be useful in representing featural models where
potentially there are unlimited number of features, i.e. each data point is associated
with a set of possibly unlimited number of features. Thibaux (2008) discusses
these models in the context of machine learning applications, and develops efﬁcient
sampling procedures which are shown to be faster than Gibbs sampling methods.
Our objective in this section is limited to presenting the developmental aspects of
these processes.
4.8.1
Chinese Restaurant Process
The CRP is a process of generating a sample from the Dirichlet process and is
equivalent to the extended Polya urn scheme introduced by Blackwell–MacQueen
and discussed in Sect. 2.1.1. In the restaurant analogy, it is a sequential process
of assigning N customers to K tables, K potentially unbounded, each assignment
being independent. This is same thing as allocating N objects to K classes or cells
independently, or grouping N objects into K clusters. In machine learning and other
applications it is generally assumed that K is unbounded. A formal derivation of the
process given by Grifﬁths and Ghahramani (2011) is as follows. The strategy is to
consider ﬁrst the allocation when K is ﬁnite and then take the limit K ! 1.
Let the vector c D .c1; : : : ; cN/ represent allocations of these N objects, with
ci D k 2 f1; : : : ; Kg indicating the assignment of object i to class k with probability,
say pk  0; such that p1 C    C pK D 1. That is P .ci D k/ D pk; k D 1; : : : ; K: Let
pD .p1; : : : ; pK/ : The joint distribution of c is
p .cjp/ D p .c1; : : : ; cNjp/ D
N
Y
iD1
p .cijpi/ D
K
Y
kD1
pnk
k ;
(4.8.1)

194
4
Priors Based on Levy Processes
where nk D PN
iD1 I Œci D k is the number of objects assigned to class k; k D
1; : : : ; K:
In Bayesian analysis, p may be treated as a parameter vector and it is cus-
tomary to assign a conjugate prior, namely the Dirichlet distribution with param-
eters ˛1; : : : ; ˛K; D .˛1; : : : ; ˛K/. Since we are going to allow K to be potentially
unbounded, it is the usual practice to take the Dirichlet distribution to be symmetric
with parameter vector .˛=K; : : : ; ˛=K/ ; where ˛ > 0 is a positive measure. By
integrating out the p vector, we get the distribution of c as
p .cj˛/ D
QK
kD1 

nk C ˛
K

  ˛
K
K
 .˛/
 .N C ˛/:
Expanding the gamma function and using the recursion relation  .x/
D
.x  1/  .x  1/ ; we can write  .nk C ˛=K/ D
nk1
Q
jD1
.j C ˛=K/ .˛=K/  .˛=K/ :
Cancelling the like terms, the above expression simpliﬁes to
p .cj˛/ D
 ˛
K
KC
KC
Y
kD1
nk1
Y
jD1
.j C ˛=K/
 .˛/
 .N C ˛/;
(4.8.2)
where KC is the number of classes with at least one object, and the indices are
reordered so that nk > 0 for all k  KC: There KN possible assignment values
of c: Assuming that the assignment is made randomly, the probability of any one
particular set of assignment would be 1=KN which goes to zero as K tends to 1:
Since N is ﬁnite and KC  N; p .cj˛/ ! 0 as K ! 1: For this reason, the
authors deﬁne a distribution over equivalence classes of assignment vectors instead
of the vectors themselves. This deﬁnes a joint distribution for all class assignments
c in which individual class assignments are not independent, but are exchangeable
with the probability of an assignment vector remains the same when the indices
of the objects are permuted. For example, for N D 4; assignments .1; 2; 1; 3/ and
.2; 1; 2; 3/ produce the same 3 classes except for the labelling of classes. Therefore
consider the distribution of c over the equivalence class of class assignments. Now
suppose that we partition N objects into KC classes but have K D K0 C KC labels
to assign to those subsets. Then there are KŠ=K0Š assignment vectors c that belong
to the same equivalence class, Œc; and probability of each of them will be the same.
Therefore, adding over them results in the distribution of assignment vectors over
the class of equivalence as
p .Œcj˛/ D KŠ
K0Š
 ˛
K
KC
KC
Y
kD1
nk1
Y
jD1

j C ˛
K

 .˛/
 .N C ˛/:

4.8
NB Models for Machine Learning
195
Now taking the limit as K ! 1 and noting that KŠ=.K0ŠKKC/ ! 1; we get
lim
K!1 p .Œcj˛/ D ˛KC
KC
Y
kD1
.nk  1/Š
 .˛/
 .N C ˛/;
(4.8.3)
which serves as a prior over class assignments for an inﬁnite mixture model.
The above formula can also be derived using the distribution of Polya urn
sequence (see Sect. 2.1.1) c1; c2; : : : ; with parameter ˛. Recall that the conditional
distribution of ci given c1; : : : ; ci1 is given by
cijc1; : : : ; ci1 Ï ˛ ./ C Pn
iD1 ıci ./
˛ C i  1
:
Therefore the joint distribution of c1; : : : ; cN is
p .c1; : : : ; cN/ D ˛ .c1/
˛
N
Y
iD2
.˛ C Pi1
jD1 ıcj/ .ci/
.˛ C i  1/
D
2
4˛ .c1/
N
Y
iD2
0
@˛ C
i1
X
jD1
ıcj
1
A .ci/
3
5 
" N
Y
iD1
1
˛ C i  1
#
:
(4.8.4)
The second product can be seen to be  .˛/ = .N C ˛/ : We expect ties among
the Nc’s. Let the K distinct c’s be denoted by c
1; : : : ; c
K with c
k having nk ties,
k D 1; : : : ; K as before. Noting that ˛

c
i

D ˛=K for all i D 1; : : : ; K; the ﬁrst
term in the second line of the above equation becomes
˛ .c1/
N
Y
iD2
0
@˛ C
i1
X
jD1
ıcj
1
A .ci/ D
KC
Y
kD1
˛
K
nk1
Y
jD1
 ˛
K C j

:
(4.8.5)
Putting two terms together, we get the expression (4.8.2) for the joint distribution of
c1; : : : ; cN derived earlier.
The Chinese restaurant analogy described before is as follows. Customers or
patrons enter the restaurant one after another and each choose a table at random. The
ﬁrst customer chooses the ﬁrst table with probability ˛=˛ D 1: The next customer
joins the occupied table with probability 1= .˛ C 1/ and chooses a second table
with probability ˛= .˛ C 1/ : The (n C 1/-th customer chooses to join previous
customers with probability n=.˛ C n/ or chooses a new table with probability
˛=.˛ C n/; n D 1; : : : ; N: If he joins previous customers and there are already
m tables occupied, then he joins the k-th table; k D 1; 2; : : : ; m, with probability
proportional to the number of customers already occupying the table, that is, with
probability nk=.˛ C n/; where nk is the number of customers sitting at that table.

196
4
Priors Based on Levy Processes
This deﬁnes the conditional probability of the (n C 1/-th customer occupying the
k-th table. The process continues until all N customers are seated occupying K
tables. This is same as allocating N objects (patrons) to K cells (tables) in a
sequential manner. The joint distribution of this allocation results in the same above
two expressions.
Patrons are exchangeable as are the random variables X0
is in the Polya urn
sequence. The probability of a particular sitting arrangement depends only on nk
and not on the order in which they arrive and sit. As n increases, there is a greater
probability that the next patron will choose an existing table rather than a new table.
After N steps, the output of the CRP is a partition of N customers across K tables,
or partitioning of N balls in K distinct colors, or simply, partitioning of integers
f1; 2; : : : ; Ng into K distinct sets, and CRP is the induced distribution over the
partitions. Partitions are realizations of the CRP. The DP induces an exchangeable
distribution over partitions. The expected number of tables K occupied by ﬁrst N
customers is PN
iD1Œ˛= .˛ C i  1/ (Antoniak 1974) and tend to inﬁnity along with
N: That is, as N ! 1; the number of partitions tend to inﬁnity as well.
The CRP is obtained by integrating out the random probability measure drawn
from the Dirichlet process and thus it describes the marginal distributions in terms
of random partitions determined by K tables in a restaurant. Thus the DP may be
viewed as the de Finetti measure of the CRP. Samples from the Dirichlet process
are probability measures and samples from the CRP are partitions. Teh et al. (2006)
proposed a further generalization as franchised CRP which corresponds to a hier-
archical Dirichlet process in which the base distribution F0 of the Dirichlet process
is itself considered as having a Dirichlet process prior with hyper parameters, say,
M and G0: This was presented in Sect. 2.4.2 as Chinese Restaurant Franchise. In
the CRP franchise, we have a group of restaurant serving a common set of dishes.
Patrons stream in, sit at a table in one of the restaurants and share a common dish
from the global menu across restaurants. This process describes marginals under a
hierarchical DP when Gj and G0 are integrated out.
4.8.2
Indian Buffet Process
In the CRP each object (patron) can possess only one feature (choose one table or
order one dish), and was represented having a single entry of one in each row of
the N  K matrix and rest of the entries as zeros, where rows represent objects and
columns as features. However, in certain applications such as factorial or featural
modeling, each object may possess an unlimited number of features, therefore a
generalization of the above model is desired. This leads to the development of the
Indian Buffet process proposed by Grifﬁths and Ghahramani (2006). The catchy
phrase is coined by the authors as a culinary metaphor in view of the similarity with
the CRP. It is claimed that Indian restaurants in London offer buffet with almost
unlimited number of dishes. In their analogy, patrons visiting the restaurants are
objects and dishes they choose are features. In contrast to the CRP, here a patron

4.8
NB Models for Machine Learning
197
may choose any number of dishes and the number N of patrons is ﬁxed. It can be
viewed as a factorial analog of the CRP.
The IBP is essentially a process to deﬁne a prior distribution on the equivalence
class of sparse binary matrices (entries of the matrices have binary responses)
consisting of a ﬁnite number of rows and an unlimited number of columns.
Thus it can serve as a prior for probability models involving objects and features
encountered in certain applications in machine learning, such as image processing.
It also provides a tool to handle nonparametric Bayesian models with large number
of latent variables. For such factorial models, a set of latent Bernoulli variables
are associated with each data point. The advantage is that one need not know the
cardinality of features beforehand. Since the rows are exchangeable, they form an
exchangeable sequence. It is shown that the underlying mixing de Finetti measure
for such a sequence is the beta process, as the DP is for the CRP. The IBP
also provides an algorithm to sample beta processes. An expanded review of the
process and its applications can be found in their subsequent paper (Grifﬁths and
Ghahramani 2011).
Let Z be a binary response matrix of N rows and an unlimited number of
columns. Its elements zik denote the fact that object i possess k-th feature, i D
1; : : : ; N and k D 1; 2; : : : ; and takes on values 1 or 0 according to whether the
feature is present or not. The task is to derive the probability distribution of a random
matrix Z.
The approach adopted in deriving the probability distribution of Z is to start with
a ﬁnite number of columns K and then consider the limit as K tends to inﬁnity. Let
	k denote the probability that an object possesses k-th feature and that the features
are generated independently. Thus, zik Ï Ber(	k/; k D 1; 2; : : : ; K for each i D
1; 2; : : : ; N: Under this model and given  D .	1; 	2; : : : ; 	K/; the probability of
Z D z is given by
P.Z D zj—/ D
K
Y
kD1
N
Y
iD1
P.zikj	k/ D
K
Y
kD1
	mk
k .1  	k/Nmk;
(4.8.6)
where mk D PN
iD1 zik is the number of objects possessing feature k. Assigning a
beta prior, Be. ˛
K ; 1/ to each 	k; where ˛ is a strength parameter of the IBP yields
P.Z D z/ D
K
Y
kD1
˛
K 

mk C ˛
K

 .N  mk C 1/


N C 1 C ˛
K

:
(4.8.7)
This distribution depends only on mk and not on the order of the columns. That
is the probability remains unchanged if the columns are permuted. Permutation of
columns is an equivalence relation on the set of N  K matrices.
An analogy with customer-dishes is obvious. Patrons stream in one by one and
taste dishes. If the i-th patron tastes k-th dish, then zik D 1I otherwise zik D 0:
Thus mk is the number of patrons tasting k-th dish. If the interest is only on what

198
4
Priors Based on Levy Processes
dishes are tasted and not in the order in which they are tasted, then the interest might
be on the probability of observing any matrix Qz in the equivalence class of z. This
probability is shown to be
P.Z D Qz/ D
KŠ
Q2N1
hD0 KhŠ
P.Z D z/;
(4.8.8)
where Kh denotes the number of columns having the full history h; h
D
0; 1; : : : ; 2N1 (total number of histories is 2N), a typical history being .z1k; : : : ; zNk/
with zik D 0 or 1; and K0 being the number of features for which mk D 0: This is
a distribution over the equivalence classes obtained by partitioning a set of binary
matrices according to column-permutation.
In order to deﬁne a distribution over inﬁnite dimensional binary matrices, the
authors take into account how customers choose dishes and deﬁne left-ordered
binary matrices. A typical left-ordered binary matrix is generated from Z by ﬁrst
accumulating to the left all columns of Z for which z1k D 1; i.e. all dishes tried
by the ﬁrst patron: Next put together all columns on the left for which z2k D 1;
and so on. Equivalence classes are deﬁned with respect to these matrices and the
probability of producing a speciﬁc matrix belonging to the equivalence class by this
process, as K ! 1 and Kh held ﬁxed, is shown to be
P.Z D Qz/ D
˛KC
Q2N1
hD0 KhŠ
expf˛HNg
KC
Y
kD1
.N  mk/Š .mk  1/Š
NŠ
;
(4.8.9)
where KC D P2N1
hD0 Kh; the number of features for which mk > 0 (thus K D K0 C
KC/ and HN D PN
jD1
1
j ; the N-th harmonic number. So this distribution represents
a prior with parameter ˛ over the binary matrices with N rows and an unlimited
number of columns in the same way as the Dirichlet process is a prior over the class
of all probability measures, and PD ./ is the prior over all discrete probability
distributions p with ordered components p1 > p2 >    .
A random draw from the Dirichlet process can be obtained by any one of the
methods mentioned in Sect. 2.1. Likewise, the above probability distribution can be
derived from the IBP with parameter ˛; as follows. The derivation by the stick-
breaking construction is mentioned thereafter.
In the IBP, N customers enter a restaurant sequentially which has the choice of
inﬁnitely many dishes arranged in a line. The ﬁrst customer starts at the left and
tastes the number of dishes according to the Poisson .˛/ : The (n C 1/-th customer
n D 1; : : : ; N moves along the buffet sampling dishes according to the popularity,
and tasting dish k with probability mk=n; where mk is the number of previous
customers who have tasted the same dish k. In addition, he samples a number of
new dishes, not tasted before by any customer, according to the Poisson .˛=n/ : The
selection of different dishes by different customers can be indicated by a binary
matrix Z:

4.8
NB Models for Machine Learning
199
The probability of any particular matrix generated by the IBP is then shown to
be
P.Z D z/ D
˛KC
QN
nD1K.n/Š
expf˛HNg
KC
Y
kD1
.N  mk/Š .mk  1/Š
NŠ
;
(4.8.10)
where K.n/ is the number of new dishes sampled by the n-th customer and KC is
the number of dishes for which mk > 0. These matrices are not in the left-ordered
form. Therefore an adjustment is made by multiplying this probability by the factor
QN
nD1K.n/Š=Q2N1
hD0 KhŠ yielding the desired probability of Eq. (4.8.9) (see Grifﬁths
and Ghahramani 2006 for further details).
4.8.2.1
Stick-Breaking Construction of IBP
To sample a binary matrix from the distribution of Z we need 	k’s (similar to pi’s
in the Sethuraman representation of the Dirichlet process). But since we do not care
for the ordering of columns, it is sufﬁcient to generate ordered 	k’s . These ordered
	k’s are given in terms of beta random variables 
k’s. Let 	.1/ > 	.2/ >    > 	.K/
be a decreasing reordering of 	1; 	2; : : : ; 	K; where a Be. ˛
K ; 1/ prior is placed on
each 	k: As K ! 1; Teh et al. (2007) construct a stick-breaking representation
(slightly different from the earlier SB construction) of the IBP as follows. Let

k
iidÏ Be.˛; 1/I
	k D 
k	k1 D
kY
lD1

l;
(4.8.11)
and 
k is independent of 	1; 	2; : : : ; 	k1; k D 1; 2; : : : K: This construction may
be viewed in terms of breaking a stick of unit length. At the ﬁrst stage, cut the stick
at point 
1 chosen randomly according to Be.˛; 1/; and discard the cut piece and
label the length of the remaining part of stick as 	1: At the second stage, cut the
remaining part of the stick at point 
2 Ï Be.˛; 1/ relative to the current length of
the stick, and discard the cut piece. Label the length of the remaining part of stick
as 	2: Continue this process.
The connection of this process to the one for the Dirichlet process is as follows.
If at stage k, we denote the length of the discarded piece as pk; then we have
pk D .1  
k/	k1 D .1  
k/
k1
Y
lD1

l:
(4.8.12)
Now making a change of variable Vk D 1  
k; we have Vk
iidÏ Be.1; ˛/ and setting
pk D Vk
Qk1
lD1 .1  Vl/ ; pk’s turn out to be the weights in stick-breaking construction
of the Dirichlet process.

200
4
Priors Based on Levy Processes
As pointed out by them, in both constructions, the weights are obtained as the
lengths of sticks. In Dirichlet process, the weights pk are the lengths of discarded
pieces, whereas in IBP, the weights 	k are the lengths of sticks remaining. Thus in
the Dirichlet process construction, the pk’s necessarily add to 1 but need not have
any order among them. In contrast, the 	k need not add to 1, but are in decreasing
order. This duality between the Dirichlet process and the IBP may be exploited for
further extensions of the IBP mirroring the extensions of the Dirichlet process. For
example, Pitman–Yor (1997) extension of the Dirichlet process may be adapted to
the IBP by taking 
k Ï Be.˛ C k; 1  / and 	k D Qk
lD1
l:
A two-parameter generalization of IBP is derived by Ghahramani et al. (2007) by
assigning a Be

˛ˇ
K ; ˇ

(instead of Be
 ˛
K ; 1

/ prior to each 	k: In the metaphor of
Indian buffet, the ﬁrst customer starts at left of buffet lay out and samples Poisson.˛/
dishes. The n-th customer tastes any dish k from previously sampled dishes by
mk > 0 customers with probability mk= .ˇ C n  1/ ; and in addition tastes Poisson
.˛ˇ= .ˇ C n  1// new dishes. The parameter ˛ reﬂects the average number of
dishes tried (have features) by the customers. The expected overall total number
of dishes (features) tried is a function of ˇ and it increases as ˇ increases (for ﬁxed
n/. Therefore, the authors interpret ˇ as the feature repulsion parameter.
Teh and Gorur (2009) introduce a three-parameter generalization of the IBP
with power-law behavior similar to the stable beta process. The parameters are
˛; ˇ; and  such that ˛ > 0 and ˇ >  and  2 Œ0; 1/: In the context of
Indian buffet, the ﬁrst customer tries Poisson .˛/ dishes; the n-th customer tries
previously tried dish k.mk > 0/ with probability .mk  /= .ˇ C n  1/ ; k D
1; 2; : : : ; KC and tries a number of new dishes according to Poisson .˛r/ ;where
r D . .1 C ˇ/  .n  1 C ˇ C // = . .n C ˇ/  .ˇ C // : This is similar to
the Pitman–Yor process (Sect. 3.4.2) where  was subtracted from the number of
customers seated around each table and added to the prospect of sitting at a new
table. The mass parameter ˛ controls the total number of dishes tried by the patrons,
concentration parameter ˇ controls the number of customers that will try each dish,
and the stability parameter  controls the power-law behavior of the process.
It is shown that the total number of dishes tried by n customers is O .n/ and the
proportion of dishes tried by m customers is asymptotically O

m1
: However,
the number of dishes each customer tries is simply a Poisson .˛/ distributed random
variable. When  D 0; it reduces to the two parameter IBP. This is akin to
Pitman–Yor process discussed in the last chapter which generalized the Dirichlet
process by the introduction of an additional discount parameter : The authors
also give the stick-breaking construction for this process which is the same as for
the two parameter IBP and show that their power-law IBP is a good model for
word occurrences in document corpora. For such a model, let n be the number of
documents in a corpus and let Zi.f
g/ D 1 if word type 
 appears in document i;
and 0 otherwise. Let 	.f
g/ be the appearance probability of word type 
 among
the documents. Now assume a stable-beta prior on 	 and each document modeled
as a conditionally independent Bernoulli process draw. The joint distribution of the

4.8
NB Models for Machine Learning
201
word appearance Z1; : : : ; Zn; integrating out 	 results in the IBP joint probability
given above.
IBP is connected to the beta process B (discussed in Sect. 4.6), in the same way
the CRP is connected to the Dirichlet process. It is an iid mixture of the Bernoulli
processes with mixing measure the beta process. Here again customers and dishes
are identiﬁed with objects and features, respectively. Let Zi be a binary row vector of
Z; i D 1; : : : ; n: Let B be the beta process with parameter c ./ ; a positive function
and B0; a ﬁxed base measure, and given B; let Zi be distributed as the Bernoulli
process with parameter B (Sect. 4.6). That is, B Ï BP .c; B0/ and ZijB Ï BeP .B/ ;
for i D 1; : : : ; n are independent Bernoulli draws from B. Integrating out B, we have
the marginal predictive distribution as
ZnC1jZ1; : : : ; Zn; c; B0 Ï BeP
 
c
c C nB0 C
K
X
kD1
mk
c C nı!k
!
;
(4.8.13)
where mk is the number of customers among n having tried dish k and !k stands
for a patron selects dish k. Its interpretation in the terminology of IBP is as follows.
Suppose B0 is continuous (if not adjustment needs to be made as indicated earlier)
and c is constant such that  D B0 ./ is ﬁnite. Since Z1 Ï BeP .B0/ and B0 is
continuous, Z1 is a Poisson process .B0/ ; and the total number of features of Z1 is
Z1 ./ Ï P ./ : That is the ﬁrst customer will taste Poisson ./ number of dishes.
For the .n C 1/-th customer, ZnC1 is sum of two components: U the number of
dishes already tasted by n customers, and V the number of new dishes he will taste.
U Ï BeP.P
k
mk
cCnı!k/ and V Ï BeP.
c
cCnB0/:U will have mass
mk
cCn at !k i.e. he will
taste dish k already tried by previous customers with probability
mk
cCn; k D 1; : : : ; K;
and will taste additionally Poisson
 c
cCn

number of new dishes.
Here the underlying Dirichlet/multinomial structure of the CRP is replaced by
beta/Bernoulli structure. For an application to document classiﬁcation problem,
their paper should be consulted.
Broderick et al. (2013) give an excellent account of combinatorial stochastic
representations for the feature modeling, which includes the beta process and the
IBP, thus extending the similar representations for the partition modeling which
include the Dirichlet process and the CRP.
4.8.3
Inﬁnite Gamma-Poisson Process
In both the CRP and IBP, we are dealing with binary matrices with each row
having a single entry of one in the case of CRP and multiple entries of ones in
the case of IBP, rest of the entries being zeros. In practice this may be inefﬁcient to
model the generation mechanism of data such as images. Titsias (2008) proposed
a model based on gamma-Poisson distribution, which may be seen as an obvious
generalization of IBP in which multiple repetition or occurrences of the same feature

202
4
Priors Based on Levy Processes
is allowed. That is an entry of one is replaced by an entry of positive integer.
In restaurant analogy this means that a patron may go for the same dish, say
chicken curry or nan, more than once. His approach in deriving the probability
distribution over such matrices is similar to the IBP, which is to consider ﬁrst
K to be ﬁnite and then taking the limit as K goes to inﬁnity. This produces the
distribution over the equivalence classes of non-negative integer valued matrices,
which is equivalent to the distribution over partitions of objects by the DP, using
Ewen’s (1972) distribution.
Now given K features, Z is an N  K matrix with non-negative integer valued
entries zik: Now assume zik  Poisson .	k/ with 	k as a feature speciﬁc parameter,
k D 1; 2; : : : ; K; for each i D 1; 2; : : : ; N. Thus
P.Z D zj—/ D
K
Y
kD1
N
Y
iD1
Poisson.zikj	k/ D
K
Y
kD1
	mk
k exp .N	k/
QN
iD1zikŠ
;
(4.8.14)
where mk D PN
iD1 zik is the number of objects possessing feature k. Further assume
	k  G. ˛
K ; 1/: Integrating out parameters f	kg ; we get
P.Z D zj˛/ D
K
Y
kD1


mk C ˛
K


 ˛
K

.N C 1/mkC ˛
K QN
iD1zikŠ
;
which shows that the columns of Z are independent. Note that this distribution is
exchangeable since reordering rows of Z does not change the probability. Since
the columns are independent, the expectation of sum of all elements of Z is
K PN
iD1 E .zik/ D ˛N; since
E .zik/ D
1
X
zikD0
zikNB
	
zikI ˛
K ; 1
2

D ˛
K ;
where NB .zikI r; p/ denotes a negative binomial distribution with parameters r > 0
and 0 < p < 1: The expectation of sum of all elements of Z is independent of K
and as K increases to inﬁnity, the matrix Z gets sparser. ˛ controls the sparsity of
the matrix and may be treated as sparsity parameter. This can alternatively derived
by letting Ti  Poisson ./ ; the vector
.zi1; : : : ; ziK/ 
 
Ti
zi1; : : : ; ziK
! K
Y
kD1

zik
k ; i D 1; : : : ; N;
.
1; : : : ; 
K/  D
 ˛
K ; : : : ; ˛
K

; and   G .˛; 1/ : Integrating out 
0s and  yield
the above probability. This process generates ﬁrst a gamma random variable and
multinomial parameters and then samples the rows of Z independently by using the
Poisson-multinomial pair.

4.8
NB Models for Machine Learning
203
Now in order to deﬁne a distribution over inﬁnite dimensional non-negative
integer valued matrices, the author for the same reason as in the IBP considers a
class of equivalence matrices and shows that
P.Z D Qz/ D
KŠ
PcN1
hD0 KhŠ
K
Y
kD1


mk C ˛
K


 ˛
K

.N C 1/mkC ˛
K QN
iD1zikŠ
;
(4.8.15)
where c is a sufﬁciently large integer such that zik  c  1 holds, and h D
.z1k; : : : ; zNk/ as the integer number associated with column k that is expressed in
a numerical system with basis c: Now taking the limit K ! 1 and using the same
strategy used in the case of Dirichlet-multinomial pair, the author show that
P.Z D zj˛/ D
1
PcN1
hD0 KhŠ
˛KC
.N C 1/mC˛
KC
Y
kD1
.mk  1/Š
QN
iD1zikŠ
;
(4.8.16)
where m D PKC
kD1 mk: This probability deﬁnes an exchangeable joint distribution
over non-negative integer valued matrices with inﬁnitely many columns in a left-
ordered form.
Similar to the cases of CRP and IBP, the above distribution can be derived from
a stochastic process that constructs the matrix Z sequentially as the data arrive one
by one in a ﬁxed order. However, an additional step is required here at each stage
compared to the IBP. It is not enough to sample the number of dishes tasted alone,
but also how frequently each dish is tasted. This is accomplished by using the Ewens
(1972) distribution.
At the start, all features are unrepresented. Draw an integer number r1 from the
negative binomial distribution NB.r1I ˛; 1
2/ which has the mean value ˛: It is the
total number of feature occurrences for the ﬁrst data point. Now given r1; select
randomly a partition .z1k; : : : ; z1K1/ of r1 such that z1k C    C z1K1 D r1; and 1 
K1  r1 according to Ewens distribution given by
p .z1k; : : : ; z1K1/ D ˛K1
 .˛/
 .r1 C ˛/
r1Š
QK1
jD1z1j
sY
iD1
1
a.1/
i Š
;
where s is the number of distinct integers in the set fz1k; : : : ; z1K1g and a.1/
i
is the
multiplicity of integer i in the partition .z1k; : : : ; z1K1/ : At the n-th stage, let Kn1
denote the number of represented features so far. For each k  Kn1; draw znk
according to NB.znkI mk;
n
nC1/ which has mean value mk
n ; where mk D Pn1
iD1 zik
is the popularity measure of the k-th feature. Having sampled all the represented
features, draw rn Ï NB.rnI ˛;
n
nC1/ of unrepresented features, and partition it by
drawing from the Ewens’ distribution as was done at stage 1. This process produces

204
4
Priors Based on Levy Processes
the distribution
P.Z D zj˛/ D
1
QN
nD1
Qrn
iD1a.n/
i Š
˛KC
.N C 1/mC˛
KC
Y
kD1
.mk  1/Š
QN
nD1znkŠ
;
(4.8.17)
where {a.n/
i g are the integer-multiplicities for the n-th data point. Note that this
distribution does not have the same form as (4.8.16) since it depends on the order
the data arrives. Like in the case of IBP, if we consider only the left-ordered class of
matrices generated then we obtain the distribution (4.8.16).
Titsias gives an MCMC algorithm for sampling the posterior distribution of Z
given the data, using mainly the Gibbs type sampling from conditional posterior
distributions.

Chapter 5
Tailfree Processes
In Chap. 1 it was indicated that there is a third method of constructing priors for
a random probability measure P, which is based on an independence property of
a sequence of nested partitions of the real line R. In this chapter we present such
priors called tailfree and Polya tree processes and their properties, and point out
their advantages and shortcomings in practical applications. In addition, a bivariate
extension of the Polya tree process is also presented in Sect. 5.3.
5.1
Tailfree Processes
In view of the limitations of the Dirichlet process that it selects a discrete probability
distribution with probability one, efforts were focused to discover some alternatives.
One of them, the tailfree processes offer some hope. Like the neutral to the right
processes, they are also deﬁned on the real line. Earlier attempts for constructing
tailfree processes can be traced to Freedman (1963) and Fabius (1964, 1973) but
Doksum (1974) clariﬁed the notion of tailfree and Ferguson (1974) gave a concrete
example, thus formalizing the discussion in the context of a prior. As mentioned
earlier, Tailfree is a misnomer since the deﬁnition does not depend on the tails and
Doksum used the term F-neutral. However, we will use the term tailfree as it has
become a common practice. They are deﬁned on the real line based on a sequence
of nested partitions of the real line and the property of independence of variables
between partitions. Their support includes absolutely continuous distributions. They
are ﬂexible and are particularly useful when it is desired to give greater weights to
the regions where it is deemed appropriate, by selecting suitable partitions. They
possess the conjugacy property. However, unlike the case of the Dirichlet and
other processes, the Bayesian results based on these priors are strongly inﬂuenced
by the partitions chosen. Additional shortcomings are pointed out in property 8.
Furthermore, it is difﬁcult to derive resulting expressions in closed form and the
© Springer International Publishing Switzerland 2016
E.G. Phadia, Prior Processes and Their Applications, Springer Series in Statistics,
DOI 10.1007/978-3-319-32789-1_5
205

206
5
Tailfree Processes
parameters involved are difﬁcult to interpret adequately. The Dirichlet process is
essentially the only process which is tailfree with respect to every sequence of
partitions. Computations of tailfree processes are generally more difﬁcult than those
of the Dirichlet priors.
5.1.1
Deﬁnition
In describing the tailfree processes, we follow Ferguson (1974). Let fmI m D
1; 2; : : :g be a tree of nested measurable partitions of (R; B); that is, 1; 2; : : : be
a sequence of measurable partitions such that mC1 is a reﬁnement of m for each
m, and [1
0 m generates B. Simplest form of partitions are when mC1 is obtained
by splitting each set of the partition m into two pieces.
Deﬁnition 5.1 (Ferguson) The distribution of a random probability P on (R; B) is
said to be tailfree with respect to {m} if there exists a family of nonnegativerandom
variables fVm;BI m D 1; 2; : : : ; B 2 mg such that
(1) the families fV1;BI B 2 1g, fV2;BI B 2 2g,...are independent, and
(2) for every m D 1; 2; : : :, if Bj 2 j; j D 1; 2; : : : ; m is such that B1  B2; : : : ; 
Bm, then P .Bm/ D …m
jD1Vj;Bj.
Another simple tailfree process is when the sequence of partitions is deﬁned
by splitting randomly the right most set at each level and then deﬁning the random
variables Vm;B’s by the conditional probability P BmjjBm1;j.m1/
 where Bm1;j.m1/
refers to the set in m1 containing Bmj. The sequence may be constructed by a stick-
breaking method. Let 
1, 
2, ...be iid uniform random variables deﬁned on .0; 1/.
Take a stick of unit length and cut a piece of length p1 D 
1. Of the remaining
length 1  p1, cut a piece of length p2 D 
2.1  p1/. Continue this process. Here
0 D .0; 1; 1 D f.0; p1; . p1; 1g ; 2 D f.0; p1; . p1; p2; . p2; 1g : : : .
An important special case of tailfree processes on .0; 1 is when the partitions
points are taken as the dyadic rationals.
The Dyadic Tailfree Process
Ferguson (1974) constructed a dyadic tailfree process on the interval .0; 1 relative
to the sequence fmg where m is the set of all dyadic intervals of length 1=2m; m D
f..i  1/=2m; i=2mI i D 1; : : : ; 2mg ; m D 1; 2; : : : . The intervals are expressed in
binary notations as follows. Let :12 : : : m denote the binary expansion of the
dyadic rational Pm
jD1j2j, where each j is zero or one. Thus B0 D .0; 1=2; B1 D
.1=2; 1; B00 D .0; 1=4; B01 D .1=4; 1=2, etc. A set B 2 m is of the form
B12:::m D .:12 : : : m; :12 : : : m C 2m. The random probability P is deﬁned
via the joint distribution of all the random variables, P .B12:::m/. Let Y; 0  Y  1
denote P .B0/ and 1  Y denote P .B1/, that is, Y and 1  Y are the probabilities

5.1
Tailfree Processes
207
of events X 2 B0 and X 2 B1, respectively. Next let Y0 2 Œ0; 1 denote the
conditional probability P .B00jB0/ ; Y1 D P .B10jB1/, so that P .B00/ D YY0 and
P .B01/ D Y.1  Y0/; P .B10/ D .1  Y/Y1, etc. Following this pattern, use Y12:::m
with m D 0 for Vm;B, and 1  Y12:::m with m D 1 for Vm;B for a set B 2 m. Then
P .B/ is the product of all the variables associated with the path in the tree leading
to B from .0; 1. Thus
P .B/ D
0
@
m
Y
jD1;jD0
Y12:::j1
1
A
0
@
m
Y
jD1;jD1
.1  Y12:::j1/
1
A ;
(5.1.1)
where for j D 1; Y12:::j1 stands for Y. For example, P

. 1
2; 5
8

D .1  Y/ Y1Y10.
The Y random variables are taken so that they are independent between different
layers of partitions. Thus the choice of distributions of Y’s should be such that
P.B12:::m 0000
„ƒ‚…
j terms
:::/ D P.B12:::m/ Q
j Y12:::m 0000
„ƒ‚…
j terms
::: : : :
a:s:
! 0.
When P is extended to be deﬁned over the algebra of sets generated by the dyadic
intervals, P will be -additive. Finally, it is extended in the usual manner to a unique
probability deﬁned on the class of Borel sets on .0; 1. The distribution of P will be
tailfree with respect to the sequence of partitions, {m}. If all the Y’s are chosen
mutually independent with Y12:::m Ï Be .˛12:::m0; ˛12:::m1/, for some suitable
nonnegative real numbers ˛’s, the process yields a Polya tree process, discussed in
the next section.
5.1.2
Properties
1. The Dirichlet process is tailfree with respect to every sequence of nested
measurable partitions (Doksum 1974). This can be seen as follows.
For each m D 1; 2; : : : let fAm1; : : : ; Amkmg denote the partition m of R
such that m is a reﬁnement of m1. We need to show that for each m, there
exists independent family of random variables fZ1iI i D 1; 2; : : : ; k1g : : : fZmiI i D
1; 2; : : : ; kmg, such that the distribution of the vector .P.Am1/; : : : ; P.Amkm// is the
same as that of .Qm
jD1 Zj1; : : : ; Qm
jD1 Z jkm/, namely, the Dirichlet distribution. But
for any i, i D 1; 2; : : : ; km, P.Ami/ has a Be.˛.Ami/; ˛.R/  ˛.Ami//. Therefore
we have to show that there exist random variables Z’s such that Qm
jD1 Z ji is also
distributed as Be.˛.Ami/; ˛.R/  ˛.Ami//. For this purpose, we deﬁne Z ji D
P.AjijAj1. ji// as independent beta distributed random variables with parameter
.˛.Aji/; ˛.Aj1. ji//˛.Aji//, for j D 1; 2; : : : ; m, where Aj1. ji/ is some set of the
partition j1 which contains Aji of j and A0 D R. Now taking the product of
these variables, and using the properties of beta random variables, it can be seen
that Qm
jD1 Zji Ï Be.˛.Ami/; ˛.R/  ˛.Ami// as was to be shown.

208
5
Tailfree Processes
2. Tailfree processes are particularly useful when it is desired to give greater
weights to the regions where it is deemed appropriate, by selecting suitable
partitions in the construction of the prior.
3. Tailfree processes are conjugate. Ferguson’s (1974) Theorem 2.2 is
Theorem 5.2 If the distribution of P is tailfree with respect to the sequence of
partition {m}, and if X1; : : : Xn is a sample from P, then the posterior distribution
of P given X1; : : : Xn is also tailfree with respect to {m}.
The posterior distribution of the V’s in the deﬁnition, given X1; : : : Xn can
easily be calculated.
4. Besides being difﬁcult in applications, the main drawback is that the points of
subdivision play a strong role in the posterior distributions. Thus the behavior of
estimates will depend upon the type of partitions used in describing the process.
5. It is easy to check that a distribution function F is neutral to the right if and only
if F is tailfree with respect to every sequence of partition m such that mC1 is
obtained from m by splitting the right most interval .tm, 1/ into two pieces .tm,
tmC1 and .tmC1, 1/.
5.2
Polya Tree Processes
The Polya tree is a tailfree process in which all (not just between partitions) variables
are assumed to be independent. The original idea was contained in Ferguson
(1974). But it was Lavine (1992, 1994) who deﬁned it formally and studied in
detail to serve as a prior for an unknown distribution function. Since the Dirichlet
process is also a tailfree process, the Polya tree process may be considered as an
intermediate between the Dirichlet process and a tailfree process. It has advantage
over the Dirichlet process since, with proper choice of parameters, it can select
continuous and absolutely continuous distributions with probability one. Thus
unlike the Dirichlet process, it could serve as a potential candidate to put priors over
density functions. On the other hand, it has advantage over tailfree processes since it
provides a greater tractability. It also has the conjugacy property with respect to the
right censored data which is not true for the Dirichlet process. With enlarged base,
it is a generalization of the Dirichlet process (albeit on the real line) as prior and
thus has potential to replace the Dirichlet process as prior in various applications.
Mauldin et al. (1992) considered its multivariate analog and instead of beta they
deal with the Dirichlet distribution. We will however limit ourselves to Lavine’s
formulation. As in the case of the Dirichlet process, Lavine also deﬁnes mixtures
of Polya trees for certain applications, and ﬁnite or partially speciﬁed Polya trees
for computational feasibility. Some recent activity of their use in modeling data is
mentioned in the end. Paddock et al. (2003) indicate multidimensional extensions
of the Polya tree. They also propose randomized Polya trees to soften the effect of
partition points

5.2
Polya Tree Processes
209
5.2.1
Deﬁnition
Let E D f0; 1g, E0 D ;; Em be the m-fold product E   E; E D [1
0 Em and EN
be the set of inﬁnite sequences of elements of E. Let X be a separable measurable
space, 0 D X and … D fmI m D 0; 1; : : :g be a separating binary tree of partitions
of X; that is, let 0, 1, ...be a sequence of partitions such that [1
0 m generates
the measurable sets and that every B 2 mC1 is obtained by splitting some B‘2 m
into two pieces. Let B; D X and, for all  D 1    m 2 E, let B0 and B1
be the two pieces into which B splits. Degenerate splits are allowed, for example,
B D B0[;. Here the partition m may be viewed as a generalization of Ferguson’s
set m of dyadic intervals of length 2m mentioned in the previous section.
Deﬁnition (Lavine) A random probability measure P on X is said to have a Polya
tree distribution, or a Polya tree prior, with parameter (…; A), written P v PT.…; A/,
if there exist nonnegative numbers A D f˛:  2 E and random variables Y D fY:
 2 E such that the following hold:
(i) all the random variables in Y are independent;
(ii) for every  2 E, Y has a Beta distribution with parameters ˛0 and ˛1;
(iii) for every m D 1; 2; : : : and every  2 Em,
P(B1m) =
	
…m
jD1IjD0Y1j1

 	
…m
jD1IjD1

1  Y1j1

,
(5.2.1)
where the ﬁrst term in the products, i.e., j D 1 is interpreted as Y; or as 1  Y;.
Random variables 
1; 
2; : : : are said to be a sample from P if, given P, they are
iid with distribution P.
The Y’s have the following interpretation: For any i D 1; 2; : : : ; Y; and 1  Y;
are, respectively, the probabilities that 
i 2 B0 and 
i 2 B1, and  D 0, and for
 ¤ 0; Y and 1  Y are the conditional probabilities that 
i 2 B0 and 
i 2 B1,
respectively, given that 
i 2 B. Polya trees are shown to be conjugate and therefore
can easily be updated. The new Polya tree has the same structure but the parameters
are altered.
The new updated Polya tree gives the distribution of Pj
i. When 
i is observed
exactly there are inﬁnitely many ˛’s to update. If 
i is observed to be in one of the
sets, say, Bı, there are only ﬁnitely many to update.
5.2.2
Properties
Most of these properties, unless speciﬁed otherwise, are established by Lavine
(1992, 1994).

210
5
Tailfree Processes
1. The Dirichlet process is a special case of Polya trees. A Polya tree is a Dirichlet
process if, for every  2 E, ˛ D ˛0 C ˛1. Many of the properties proved for
the Dirichlet process may be extended to the Polya tree process as well.
2. Muliere and Walker (1997) show that the Polya tree priors generalize the beta
process when viewed at an increment level. For any increment s > 0, let B0 D
Œ0; s/ and B1 D Œs; 1/. Now partition B1 into B10 D Œs; 2s/ and B11 D
Œ2s; 1/. Continue partitioning the right partition. For m > 1, let B1m0 D
Œms; .m C 1/s/ and B1m1 D Œ.m C 1/s; 1/ where i D 1 for all i D
1; 2; : : : ; m. Let G be a measure on  and set ˛m10 D m1G .B1m0/ and
˛m11 D m1G .B1m1/,where m1 D  ..m  1=2/s/ for some positive
function  ./. Now deﬁne a sequence of independent beta random variables
Ym Ï Be.˛m10; ˛m11/, and let Xm D Ym…m1
jD1 .1  Yj/. Finally set A .0/ D 0
and A .t/ D P
mst Ym. Then A can be considered as a beta process (Hjort
1990) with parameter c ./ D  ./ GŒ; 1/ and A0 ./ D
R ./
0 dG.s/=GŒs; 1/
viewed at an incremental level of s (Walker and Muliere 1997b; Muliere and
Walker 1997). The corresponding distribution function results from F.t/ D 1 
…mst .1  Ym/ D P
jst Xj.
3. With proper choice of parameters, Polya trees assign probability 1 to the set
of all continuous distributions. Kraft (1964), Ferguson (1974), and Mauldin
et al. (1992) give sufﬁcient conditions (see the next property) for a random
probability measure P to be continuous or absolutely continuous with proba-
bility one. This would make Polya trees to be appropriate candidates to place
priors on density functions, and since the posterior distributions for this priors
are obtained by simply updating the parameters, they would make Bayesian
estimation of densities feasible, which was not possible with the Dirichlet
prior because of discreteness. Lo (1984) used kernel mixture of the Dirichlet
processes to place priors on densities (see Sect. 6.5.4).
4. As noted in the last section, Ferguson (1974) constructed a simple dyadic
tailfree process P on the interval (0; 1 with respect to fmg where m D
f..i  1/ =2m; i=2mI i D 1; : : : ; 2mg ; m D 1; 2; : : :. . As before, let :12 : : : :m
denote the binary expansion of dyadic rational Pm
jD1 j:2j, where j is zero
or one, and for short, write m D 1    m. If all the Y random variables
deﬁned there are taken to be mutually independent distributed as Ym1 Ï
Be

˛m10; ˛m11

, for nonnegative real numbers ˛’s, then the posterior dis-
tributions of Y variables given a sample of size n have the same structure, with
Ym1 Ï Be

˛m10 C r; ˛m11 C n  r

, where r is the number of observations
falling in the interval (:m10; :m11 and n  r falling in the interval (
:m11; :m11 C 2m. He points out that the choice of ˛’s have the following
consequences.
(a) ˛m D
1
2m yields a P which is discrete with probability 1 (Blackwell 1973)
(b) ˛m D 1 yields a P which is continuous singular with probability 1 (Dubins
and Freedman 1966)
(c) ˛m D m2 yields a P which is absolutely continuous with probability 1
(Kraft 1964).

5.2
Polya Tree Processes
211
Thus the selections of ˛’s have consequences. They affect the rate at which
the updated predictive distribution moves from the prior distribution to the
sample distribution, and how closely the distribution of P is concentrated about
its mean (see below).
5. Polya tree CDF can be made uniformly close to a given CDF with high
probability. Also, using a Polya tree its probability density function can be made
close to a given density function which is impossible for the Dirichlet process
since it is a discrete measure. That is, Lavine shows for a given probability
measure Q with density q, for any  > 0 and  2 .0; 1/, there exists a Polya
tree P with density p such that P .essSup
 jlog . p .
/ =q .
//j < / > .
6. Polya trees have an advantage that for some sampling situations where the
posterior turns out to be a mixture of Dirichlet processes when a Dirichlet prior
is used, Polya trees lead just to a single Polya tree.
7. It allows one to specify a prior which places greater weight on the subsets of real
line where it is deemed appropriate, by selecting suitable partitions accordingly.
8. It enables one to design partitions so that the random distribution centers around
a desired known distribution.
9. A major drawback is that the partition … plays an essential role in developing
inferential procedures (Dirichlet processes are the only tailfree processes in
which … does not) and thus the conclusions are contaminated by the type of
partition used. Second, because of the ﬁxed partitioning, discontinuities are
introduced in the predictive distributions. In fact Ferguson (1974) had pointed
that a PT can be used to construct a density with respect to the Lebesgue
measure that exists with probability one, but it will have discontinuities at
all partition points with probability 1. They will have more dramatic effect in
higher dimensions.
To soften the effect of partition points however, Paddock et al. (2003)
proposed an interesting new strategy. They introduced randomized Polya trees
in which the partition points are no longer taken to be ﬁxed but are random.
The randomness is implemented by fudging the bifurcation points via auxiliary
random variables, one per each level of the tree, in such a way that the endpoints
of subintervals are no longer ﬁxed dyadic rationals but are random close to
the dyadic rationals. Thus the subintervals at each level are no longer equal
in length. Now the random variables Y are deﬁned in the usual manner. This
scheme can be implemented for Polya trees with any partition points and not
necessarily restricted to dyadic rationals. It induces smoothness in the resulting
distribution.
Another sour point is that the Polya tree is limited in its extension to higher
dimension, although extension to bivariate is not so bad and is included in this
chapter later on.
10. Recall that in the case of Dirichlet process, the parameter ˛ was a ﬁnite measure
representing prior guess F0 at the unknown distribution function F via the
relation ˛ ./ D MF0 ./ and M D ˛ .R/. Therefore, E ŒF ./ D F0 ./ D
˛ ./ =˛ .R/. Similarly in the case of Polya tree, we can deﬁne a probability
measure ˇ D E ŒP by deﬁning ˇ .B/ D E ŒP .B/ for any measurable set B.

212
5
Tailfree Processes
Lavine does this by deﬁning ﬁrst for any  2 E; ˇ .B/ and then extending it
to any measurable set B.
ˇ .B/ D E
"	
…m
jD1IjD0Y1j1

  
…m.1
jD1IjD1
Y1j1/
!#
D
…m
jD1IjD0
˛j10
˛j10 C ˛j11
…m
jD1IjD1
˛j11
˛j10 C ˛j11
(5.2.2)
deﬁnes ˇ on the elements of [m and since the latter generates measurable sets,
ˇ is thus extended to the measurable sets.
Now P Œ
i 2 B
D
E ŒP Œ
i 2 B jP
D
E ŒP ŒB
D
ˇ .B/. Thus the
unconditional distribution of an observation 
i is given for  D m 2 E, as
P Œ
i 2 B D E ŒP ŒB D E ŒP.B1/P.B12jB1/    P.BjB1::::m1/
D
˛1
˛0 C ˛1
  
˛m
˛m10 C ˛m11
:
(5.2.3)
Polya trees are conjugate and so for the posterior distribution we need only
to update the parameters which can be done easily. For example, Y; is the
probability that 
 2 B0 and is a beta random variable. Therefore the conditional
distribution of Y; given 
 is a beta distribution with one of the parameters of
the beta distribution will have increased by one. If 
 2 B for some  2 E, the
scheme of updating follows the same rule. However, the difference is that if 
 is
observed exactly, then inﬁnitely many ˛’s get updated. On the other hand, if we
know only that 
 2 B, only ﬁnitely many ˛’s need to be updated. To overcome
the problem of updating inﬁnitely many parameters, he suggests two possible
recourses. One is to take ˛’s large enough . m2/ at level m and stop at some
level M (say of order log2 n ); other is to consider only ﬁnitely many levels of a
Polya tree (see below).
11. The predictive probability is easy to compute (Muliere and Walker 1997).
Suppose we are given the data  D .
1; : : : ; 
n/, then
P


nC1 2 Bmjdata

D
˛1 C n1
˛0 C ˛1 C n   
˛m C nm
˛m10 C ˛m11 C nm1
;
(5.2.4)
where n is the number of observations among 
’s in B.
12. Dråghici and Ramamoorthi (2000) give conditions for the prior and posterior
Polya tree processes to be mutually continuous, and mutually singular.
13. Construction of a Polya Tree: If a prior guess of ˇ, say, continuous ˇ0 is
available, Lavine gives a construction of Polya tree such that 
1 Ï ˇ0 . Choose
B0 and B1 such that ˇ0.B0/ D ˇ0.B1/ D 1
2. Then for every  2 E, choose B0
and B1 to satisfy ˇ0 .B0 j B/ D ˇ0 .B1 j B/ D 1
2, and so on.

5.2
Polya Tree Processes
213
If X D R and ˇ0 has a CDF G, the elements of m can be taken to be the
intervals {G1.k=2m/; G1..k C 1/=2m/g for k D 0; : : : 2m  1.
In the case of censored observations, suppose we know only ‚1
>

1; : : : ; ‚k > 
k. WLOG assume 
1 < 
2 < : : : < 
k. Then … may be chosen
so that B1 D .
1; 1/; B11 D .
2; 1/; : : : ; B11:::1 D .
k; 1/. Then Pjdata 
PT(…, A) where now ˛
1 D ˛1 Ck; ˛
11 D ˛11 Ck1; : : : ; ˛
11:::1 D ˛11:::1 C1.
Although the posterior distribution is a single Polya tree, it is a mixture of
Dirichlet processes.
The parameters ˛’s may be chosen according to how quickly the updated
predictive distribution moves away from the prior predictive distribution. If ˛’s
are large it will be close to the prior and if ˛’s are small it will be close to sample
distribution, a behavior found in the Dirichlet process. If ˛0 D ˛1, then the
beta distribution is symmetric. Large values of ˛0 D ˛1 will make P smooth
as noted in Ferguson (1974). The choice of ˛’s also governs how closely the
distribution of P is concentrated around its mean. See Lavine (1992, 1994) for
further discussion.
14. An anonymous reviewer has brought to my attention a paper of Paddock
et al. (2003) in which it is indicated how the Polya tree can be extended to
multidimensional. As an application, the authors give computational scheme
to simulate conditional predictive distribution of a vector of random variables
X1; : : : ; Xk given XkC1; : : : ; Xm; m > k based on Polya tree prior which is
restricted to a ﬁnite level n. The procedure is like the Gibbs sampler. A
bivariate Polya tree (Phadia 2007) is presented in the next section and Bayesian
estimators with respect to this prior are given in Chaps. 6 and 7.
Since for exact observations we have to update inﬁnitely many ˛’s, Polya trees
may not be suitable unless ﬁnite Polya trees are used. For the right censored data we
have more choices—Polya tree priors, along with beta, beta-Stacy and neutral to the
right processes are preferable over the Dirichlet process. However, the construction
of Polya tree priors where the partitions are based on observed data should be a
cause for concern.
Lavine provides examples of calculations involved in using Polya tree priors in
place of the Dirichlet priors. He describes the posterior distribution when Polya
trees are used to model the errors in regression models Yi D  .Xi; ˇ/ C i, where
Xi is a known vector of covariates, ˇ is an unknown vector of parameters and  is
a known function of Xi and ˇ, and i are independent with unknown distribution
P; P Ï PT

…ˇ; Aˇ

. He also reworks Antoniak’s (1974) empirical Bayes problem
in which a mixture of Dirichlet processes prior is replaced by a mixture of Polya
trees prior, and shows how posteriors can be computed via the Gibbs sampler thus
demonstrating advantages of this substitution.

214
5
Tailfree Processes
Characterization
Walker and Muliere (1997b) give the following characterization. Let rkt for k D
1; 2; : : : ; m represent the number of observations in B1k (where B1m D Bmt/
given that there are nj observations in Bmj (for j D 1; : : : ; t).
F v PT .…; A/ if and only if there exist nonnegative numbers AD .˛0; ˛1; : : :/
such that, for all m D 1; 2; : : :, and t 2 f1; : : : ; 2mg, and nonnegative integers
n1; : : : ; nt,
P Œ
nC1 2 Bmtjn1 2 Bm1; : : : ; nt 2 Bmt
D
˛1 C r1t
˛0 C ˛1 C n
˛12 C r2t
˛10 C ˛11 C r1t
  
˛m C rmt
˛m10 C ˛m11 C rm1t
;
(5.2.5)
where ˛1m is written as ˛m.
5.2.3
Finite and Mixtures of Polya Trees
Just as Antoniak (1974) (see Sect. 2.3) deﬁned mixtures of Dirichlet processes
by indexing the parameter of the Dirichlet process ˛ with a variable 
 having a
parametric distribution, so also the mixtures of Polya trees are deﬁned.
Deﬁnition 5.3 (Lavine) The distribution of a random probability P is said to be a
mixture of Polya trees if there exists a random variable U with distribution H, known
as the mixing distribution, and for each u, Polya trees with parameters f…u; Aug such
that PjU D u Ï PT f…u; Aug.
Thus for any measurable set B 2  .X/ ; P .P 2 B/ D R P .P 2 Bju/H .du/.
Obviously if H is degenerate at a point, then the mixture reduces to a single Polya
tree. For the posterior distribution, we not only need to update Au but also H must
be updated just as the case was in mixtures of Dirichlet processes. The mixtures of
Polya trees produce a smoothening effect and thus the role of partition may not be
that critical.
In practical applications such as generating a sample from the Dirichlet process
using the inﬁnite sum Sethuraman representation, the truncation was necessary to
proceed with the Bayesian analysis (Sect. 2.4.1, and Ishwaran and James 2001).
Similarly, in using the Polya Tree prior, truncation is recommended by Lavine
(1994). This is possible by choosing ˛’s appropriately so that they increase rapidly
towards the end of the tree. Simpliﬁcation is achieved by terminating and updating
the partition … up to some level M and the resulting Polya trees PT (…M, AM)
are termed by Lavine as ﬁnite or partially speciﬁed Polya trees (see Lavine for
formal deﬁnition). Mauldin et al. (1992) also deﬁne ﬁnite Polya trees. Lavine offers
guidance on how this can be done to a desired accuracy. For example, up to level M,
deﬁne random distribution G according to the Polya tree scheme and there after

5.2
Polya Tree Processes
215
it may be deﬁned either as uniform distribution or use G0, the base distribution,
restricted to this set. Hanson and Johnson (2002) recommend M to be of order
Ï log2 n as a rule of thumb for sample size n.
Since the base of Polya tree priors include absolutely continuous distributions,
it is found to be favorable over the Dirichlet process. Consider the general linear
model Z D Xˇ C , where X is a vector of covariates, ˇ is a vector of regression
coefﬁcients, and  is the error term. Traditionally the practice is to assume the error
term to be distributed as a parametric distribution, typically normal distribution
with mean zero. The nonparametric Bayesian approach is to assume the error term
having an unknown distribution, and a prior is placed on the unknown distribution
[see Antoniak (1974) for example] centered around a base distribution which may
be taken as normal with mean zero. There are several papers along this line using
different priors.
Walker and Mallick (1997b) use a ﬁnite Polya tree prior for the random
errors in a hierarchical generalized linear model centered around a known base
probability measure (by taking partitions to coincide with the percentiles of the
corresponding distribution function) and ﬁnd this approach to be more appropriate
than a parametric approach. They extend this approach to an accelerated failure
time model (Walker and Mallick 1999) where the error term is assumed to have
a Polya tree prior and show how to implement MCMC procedure and give an
application to survival data. Procedure to simulate a random probability measure
P from PT .…; A/ is also indicated in their paper. This is done by ﬁrst generating
a ﬁnite set of beta random variables and deﬁning the random measure PM by
P .B1M/ for each 1    M according to (5.2.3). Then one of the 2M sets is picked
according to the random weights P .B1M/ and then a uniform random variate is
taken from this set. If one of the set chosen happens to be an extreme set, then the
random variate is chosen according to the base measure G0 restricted to this set. ˛’s
are chosen such that they increase rapidly down towards level M. Details may be
found in their paper.
Hanson and Johnson (2002) argue that in practice it may be difﬁcult to specify
a single centering/base distribution G0. Therefore, they recommend modeling the
error term in a linear model as a mixture of Polya trees. A mixture of Polya tree
distribution G is speciﬁed by allowing parameters of the centering distribution
G0 and/or the family of real numbers ˛’s to be random. That is, GjU; C Ï
PT .…u; Ac/ ; U Ï fu .u/ ; C Ï fC .c/. They consider mixtures of Polya trees in
which the partition is constructed by a parametric family of probability distributions
with variance U. The effect of taking mixtures is to smooth out the partitions of a
simple Polya tree. Hanson (2006) further justiﬁed the efﬁciency of using mixtures
of Polya trees alternative to using parametric models and provided computational
strategies to carry out the analysis and illustrated them by considering several
examples.
There are numerous papers published since 2006 demonstrating the utility of
using mixtures of Polya trees just like mixtures of the Dirichlet process, in modeling
regression models and certain types of large and complex data. Computational
procedures are demonstrated with real data and efﬁciency of such methods is

216
5
Tailfree Processes
discussed. For example, they are used in reliability and survival analysis (Hanson
2007), and multivariate mixtures of Polya trees are used for modelling ROC data
(Hanson et al. 2008). For an introduction and some applications, the reader is
referred to a paper by Christensen et al. (2008).
To soften the effect of partition points that play part in inferential outcomes,
Paddock et al. (2003) devised a randomized Polya tree, as mentioned earlier. The
paper also contains procedure to simulate observations from predictive distribution
and illustrative examples.
5.3
Bivariate Processes
Bivariate extensions of the prior processes are not so easy. In the non-Bayesian
context, several attempts have been made to extend the Kaplan–Meier (1958) PL
estimator to the case of bivariate survival function but encountered problems. In
some cases the estimators of a distribution function (or a survival function) obtained
are not proper distribution functions (or of survival functions), while in other cases,
no explicit or simple forms are possible. Dabrowska (1988) constructed an analogue
of the PL estimator which is consistent but is not a proper survival function as it
assigns negative mass to certain regions. For other efforts, see the references in
Dabrowska (1988). However there is some hope in the Bayesian approach.
Ferguson’s (1973) Dirichlet process was deﬁned on an arbitrary space of
probability measures. This made it easy in extending the Dirichlet process to
higher dimensions in a straightforward manner. See, for example, Ferguson (1973),
Dalal and Phadia (1983), Phadia and Susarla (1983), among others, who assign a
Dirichlet process prior for an unknown bivariate distribution function deﬁned on
R2 D R  R or R2
C D RC  RC in addressing some estimation problems. In dealing
with the estimation of a survival function, Tsai (1986) follows a slightly different
path. He places a Dirichlet process prior with parameter ˛, on .R; B/, where
R D RC f0; 1g and B D Bf; f0g ; f1g ; f0; 1gg , B is a Borel ﬁeld on RC and
˛ is a non-null ﬁnite measure on .R; B/. Salinas-Torres et al. (2002) generalize
Tsai’s approach by taking the second coordinate with values in f1; : : : ; kg. In the
context of survival data, Pruitt (1992) shows that the Bayes estimator of a bivariate
survival function with Dirichlet prior could be inconsistent. This point was further
discussed in Ghosh et al. (2006).
On the other hand, all the processes belonging to the class of processes neutral
to the right are deﬁned on the real line and their extension to the bivariate case is
difﬁcult and remained unexplored. However, there is a renewed interest and several
attempts have been made in recent years. See for example, Walker and Muliere
(2003), Ghosh et al. (2006), Bulla et al. (2007, 2009), Paddock et al. (2003) and
Phadia (2007). Recall that in the univariate case the Bayes estimator with respect to
the Dirichlet process prior and more generally, with respect to the neutral to the right
processes, converges to the PL estimator. Recognizing this fact, Ghosh et al. (2006)
approach this problem by developing a natural generalization of the beta process to

5.3
Bivariate Processes
217
the bivariate case and derive an estimator using an approach which is labelled as
“essential approach” in view of it not using full likelihood.
Bulla et al. (2007) approach the same problem from a different angle. They use a
reinforced process derived from the Generalized Polya urn scheme in constructing
a bivariate prior on the space of distribution functions deﬁned on the product space
f1; 2; : : :g  f1; 2; : : :g. Thus this approach may be suitable when the Bayesian
prediction of future behavior of a bivariate observation based on past observations
is of interest. They extend their approach to the estimation of a multivariate survival
function in Bulla et al. (2009). Yang et al. (2008) use mixtures of Polya trees in
nonparametric estimation of bivariate density based on interval censored data.
Walker and Muliere (2003) considered a different model. Suppose we have data
from two distributions which are known to be closed but otherwise unknown. Their
closeness is modelled by deﬁning a parameter  D corr.F1 .A/ ; F2 .A//  0 for
every set A in the domain. They describe a bivariate Dirichlet process model for
' .F1; F2/ in which marginal distributions for F1 and F2 are taken to be the same
Dirichlet distributions and show how to ﬁnd their posterior distributions. Their prior
utilizes the Dirichlet-multinomial point process introduced by Lo (1986) (Sect. 2.5).
The difﬁculty in describing the posterior completely is pointed out.
In contrast, it is relatively easy to construct bivariate tailfree and Polya tree
processes. Mauldin et al. (1992) constructed one such process in terms of a prior
guess of the unknown distribution. On the other hand, taking a cue from Lavine
(1992), Mauldin et al. (1992) and Ferguson (1974), Phadia (2007) proposed a two-
dimensional extension of Ferguson’s (1974) dyadic tailfree process and showed
that given a random sample, the posterior distribution is also tailfree. It is then
used in deriving bivariate estimators of a distribution (survival) function which
are included in Chaps. 6 and 7. This extension is presented here. An anonymous
reviewer has brought to my attention a paper of Paddock et al. (2003) in which
a similar construction is suggested for Euclidean space Rk. Here more details are
presented in the case of bivariate process.
5.3.1
Bivariate Tailfree Process
Recall the deﬁnition of a tailfree process presented in Sect. 5.1. The distribution of
a random probability P on (R; B) is said to be tailfree with respect to a sequence
of nested partitions {m} if 9 a family of nonnegative random variables fVm;BI m D
1; 2; : : : ; B 2 mg such that (1) the families fV1;BI B 2 1g; fV2;BI B 2 2g; : : :
are independent, and (2) for every m D 1; 2; : : :, if Bj 2 j, j D 1; 2; : : : ; m is
such that B1  B2; : : : ;  Bm, then P .Bm/ D …m
jD1Vj;Bj. Thus to describe a bivariate
tailfree process we need two things: a sequence of nested partitions … D fmg
and a set of variables Vi ’s. The construction is similar to Ferguson’s (1974) dyadic
tailfree process of Sect. 5.1 except that each set in m is split into four instead of two
at the .m C 1/th level, and deals with Dirichlet rather than beta distributions. For

218
5
Tailfree Processes
analytical convenience a unit square X D .0; 1  .0; 1 is taken in Phadia (2007)
and a sequence of partitions is deﬁned as follows.
The unit square .0; 1  .0; 1 is denoted by B0. It is subdivided into
four symmetric subsquares B1; B2; B3; B4 of size 1=2 and they are iden-
tiﬁed with sufﬁxes 1; 2; 3, and 4 as those starting from the bottom left
end side and moving in a clock-wise direction. Thus B1
D
.0; 1
2 
.0; 1
2; B2
D
.0; 1
2  . 1
2; 1; B3
D
. 1
2; 1  . 1
2; 1, and B4
D
. 1
2; 1 
.0; 1
2. Each subsquare is further divided into four symmetric subsquares,
B11; : : : ; B14; : : : ; B41; : : : ; B44, each of size 1=4 and the process is continued. Now
let 0 D fB0g; 1 D fB1; B2; B3; B4g; 2 D fB11; : : : ; B14; : : : ; B41; : : : ; B44g, ....,
m D fBc1c2;:::cm; where ci D 1; 2; 3; 4 for i D 1; 2; : : : ; m g, m D 1; 2; : : : . m will
have 4m1 4-tuple subsquares of size 2m. Note that Bc1  Bc1c2  Bc1c2c3  : : : .
Thus … D fmI m D 0; 1; : : :g forms a sequence of nested partitions.
The subsquares Bc1c2;:::cm may be identiﬁed by their bottom left end cor-
ners taken to be dyadic rationals .r; s/, which can be expressed in terms of
binary expansion of Pm
jD1 j:2j with j D 0 or 1 as .:e1e2 : : : :em; :e
0
1e
0
2 : : : :e
0
m/,
where ei and e
0
i take values 0 or 1. For example, the bottom left end cor-
ner of B32 is
 1
2; 3
4

D .0:10; 0:11/. To further identify the four subsquares
Bc1c2;:::cm1; Bc1c2;:::cm2; Bc1c2;:::cm3, and Bc1c2;:::cm4 of Bc1c2;:::cm with their bottom left
end corners .:e1e2 : : : :em; :e
0
1e
0
2 : : : :e
0
m/, we place at the blank places .0; 0/ for
cmC1 D 1; .0; 1/ for cmC1 D 2; .1; 1/, for cmC1 D 3 and .1; 0/ for cmC1 D 4.
For example: B231 is the set corresponding to a square with bottom left end corner
.0:010; 0:100/.
The collection of these squares forms a dense set in .0; 1  .0; 1. It can be
identiﬁed as follows. Let E D f1; 2; 3; 4g and Ek be the set of sequences of numbers
i 2 E, of length k denoted by ck D c1c2; : : : ck. Let E D [kEk be the set of all
sequences of 1; 2; 3, and 4 of ﬁnite lengths. We shall denote the elements of E by
. Thus n is a partition consisting of sets of the form B where  2 En and let
fB1; B2; B3; B4g be a further partition of B. Thus [kEk generates 

.0; 12
.
Now we proceed to deﬁne the family of random variables Vm;B. Then the random
probability P will be deﬁned via these independent families. Set
fV1;B D Zc1 D P.Bc1jB0/ for B D Bc1 2 1; c1 D 1; 2; 3; 4:g; fV2;B D Zc2 D
P.Bc2jBc1/ for B D Bc2 2 2; ci D 1; 2; 3; 4; i D 1; 2g; : : : ; fVm;B D Zcm D
P.BcmjBcm1/ for B D Bcm 2 m; ci D 1; 2; 3; 4 for i D 1; 2; : : : ; mg; : : : : : :.
We take these families to be independent between levels, m D 1; 2; : : : . This
way for an arbitrary set B 2 m, P.B/ D P.Bc1c2;:::cm/, is the product of all
the variables associated with the path in the tree from Œ0; 12 to Bc1c2;:::cm so that
P.Bc1c2;:::cm/ D Qm
iD1Zc1c2;:::;ci; ci 2 f1; 2; 3; 4g; i D 1; 2; : : : ; m. For example,
if B2143 2 4; P.B2143/ D Z2Z21Z214Z2143. The random probability P is deﬁned
through the joint distribution of P.Bc1c2;:::cm/ by assigning suitable distributions to
Z’s. Since the sets Bcm are decreasing to an empty set , we should therefore have
P.Bcm ii : : : :i
„ƒ‚…/ going to 0 for i 2 f1; 2; 3; 4g. Thus the choice of distributions of Z’s
should be such that P.Bcm ii : : : :i
„ƒ‚… : : :/ D P.Bcm/ Q Zcm ii : : : :i
„ƒ‚… : : :
a:s:
! 0.

5.3
Bivariate Processes
219
When P is extended to be deﬁned over the algebra of sets generated by the
squares, P will be -additive. Finally, it is extended in the usual manner to a unique
probability deﬁned on the class of Borel sets on .0; 1  .0; 1. This will yield a
random probability P on ..0; 12; 

.0; 12
/. The distribution of P will be tailfree
with respect to the sequence of partitions, …. Now Theorem 5.2 is applicable.
Thus if X1; X2; : : : Xn is a sample from P, then the posterior distribution of P given
X1; X2; : : : ; Xn is also tailfree w.r.t. {m}. The unit square may be replaced by
.0; T  .0; T for a ﬁnite T.
Recall that in the deﬁnition of a Polya tree prior (Lavine 1992), all (between as
well as within partitions) the pairs of variables Z’s are assumed to be independent
each having a beta distribution. Similarly, in the bivariate case we may assume the
4m1, 4-tuple vectors

Zcm1;1; : : : ; Zcm14

at level m to be mutually independent
each having a Dirichlet distribution with parameters

˛cm1;1; : : : ; ˛cm14

for some
nonnegative real numbers ˛’s. Although we take here the Dirichlet distribution in
place of beta distribution, for any speciﬁc values of c1c2; : : : cm; Zcm will have a beta
distribution. For example, Z2143  Be.˛2143; ˛2141 C ˛2142 C ˛2144/. To ensure that
P.Bcm/ Q Zcm ii : : : :i
„ƒ‚… : : :
a:s:
! 0, we place the following condition on ˛’s.
1
X
jD0
cm ii : : : :i
„ƒ‚…
j
 ˛cm ii : : : :i
„ƒ‚…
j
l
cm ii : : : :i
„ƒ‚…
j
D 1;
i; l 2 f1; 2; 3; 4g and
cm ii : : : :i
„ƒ‚…
j
D
4
X
lD1
˛cm ii : : : :i
„ƒ‚…
j
l:
(5.3.1)
The posterior distributions of the Zvectors is again independent each with
a Dirichlet distribution and the parameters ˛’s of the posterior distributions get
updated.
.Zcm1; Zcm2; Zcm3; Zcm4/ j X1; X2; : : : Xn  D.˛cm1 C Ncm1; : : : ; ˛cm4 C Ncm4/;
(5.3.2)
where Ncmi D the number of Xj’s that fall in the set Bcmi.
By appropriate choice of all the parameters f˛c1g; f˛c1c2g; : : :, it is possible,
like in the univariate case, to obtain tailfree processes that are discrete, that
are continuous singular, or that are absolutely continuous with probability one
(Ferguson 1974). The application of using Polya tree priors in Bayesian estimation
is discussed in the next two chapters.

Chapter 6
Inference Based on Complete Data
6.1
Introduction
In the statistical problem of nonparametric Bayesian analysis we have a random
probability P belonging to … and having a particular prior distribution. Given P D
P, we also have a random sample X1; : : : ; Xn, which are iid P taking values in .
Based on the sample, our objective is to estimate a function, .P/ of P with respect
to a certain loss function. Most of the applications presented in this part use the
Dirichlet process prior or its variants—Dirichlet Invariant process and mixtures of
Dirichlet processes. In Chap. 7, we use other priors such as the neutral to the right
processes which are more suited while dealing with censored data, but obviously
they are applicable in the uncensored data case as well.
In this chapter, ﬁrst we will deal primarily with estimation problems and
thereafter we will present hypothesis testing and other applications brieﬂy. We
will consider the distribution function (CDF) or its functionals. Since the Dirichlet
process prior is conjugate, the strategy will be to obtain ﬁrst the Bayes estimate of 
for the no-sample problem whenever possible and then to update the parameter(s) of
the prior to obtain Bayes estimator for any sample size n. Through out this chapter
we assume that we have a random sample X1; : : : ; Xn from an unknown distribution
function F (corresponding to P) deﬁned on the real line R. In the case of two-
sample problem, we will have a second sample Y1; : : : ; Yn from another distribution
function, say, G deﬁned on R. Both samples will be assumed to be independent. The
loss functions used are a weighted integral squared error loss, denoted by L1 for the
distribution function and a squared error loss L2 for its functionals, where
L1.F;bF/ D
Z
.F.t/ 
^
F.t//2dW.t/I
L2.';
^'/ D .' 
^'/2;
(6.1.1)
with W being a given weight function or a ﬁnite measure on .R; B/.
© Springer International Publishing Switzerland 2016
E.G. Phadia, Prior Processes and Their Applications, Springer Series in Statistics,
DOI 10.1007/978-3-319-32789-1_6
221

222
6
Inference Based on Complete Data
Through out this and the next chapter, we will denote the samples by bold
letters, such as X D .X1; : : : ; Xn/, the sample distribution by bFn.t/ and the Bayes
estimator with respect to the Dirichlet prior D.˛/, by bF˛. Additionally, we let
_˛ ./ D ˛ ./ =˛ .R/, M D ˛ .R/ and pn D ˛ .R/ =.˛ .R/ C n/. In some applications,
we use X instead of R.
The topics in this chapter are organized as follows:
1. Estimation of a distribution function.
2. Tolerance region and Conﬁdence bands.
3. Estimation of functionals of a distribution function.
4. Some other applications.
5. Bivariate distribution function.
6. Estimation of a function of P.
7. Two-sample problems.
8. Hypothesis Testing.
Under these headings, applications to sequential estimation, empirical Bayes, lin-
ear Bayes, minimax estimation, bioassay, and other applications will be presented.
The beauty of the Dirichlet process is that most of the results are in closed forms.
Also, once the no-sample problem is solved, all that is needed to solve the problem
for any sample size is to update the parameter of the Dirichlet process. This strategy
is used repeatedly. Needless to say that many of the problems discussed here could
also be solved by using other priors, such as processes neutral to the right, Polya
trees, and beta-Stacy, although closed form of the results may not be guaranteed.
6.2
Estimation of a Distribution Function
In this section the Bayesian estimation of a distribution function with respect to
the Dirichlet process and related prior processes is presented. Also included are the
empirical Bayes, sequential and minimax estimation procedures.
6.2.1
Estimation of a CDF
Let X1; : : : ; Xn
iidÏ
F, F deﬁned on .R; B/. The objective is to estimate F
based on X under the loss function L1 and prior D.˛/. For each t, F.t/ 
Be.˛.1; t; ˛.t; 1//. The risk is given by E.L.F;bF// D R E.F.t/bF .t//2dW.t/.
The Bayes estimate of F for the no-sample problem is the posterior mean bF.t/ D
E.F.t// D F0.t/ D ˛.1; t=˛ .R/, where the expectation is taken with respect to
D.˛/. By Theorem 2.6 of Sect. 2.1, we have FjX Ï D.˛ C Pn
iD1 ıXi/. Therefore,
for a sample of size n, the expectation is taken with respect to D.˛ CPn
iD1 ıXi/, and

6.2
Estimation of a Distribution Function
223
the Bayes estimator is bF.t/ D E.F.t/ j X1; : : : ; Xn/ obtained as (Ferguson 1973)
bF˛ .t/ D bF.t j X1; : : : ; Xn/ D ˛.1; t C Pn
iD1 ıXi.1; t
˛ .R/ C n
D pn  F0.t/ C .1  pn/  bFn.t/, say,
(6.2.1)
wherebFn.t/ D 1
nPn
iD1 ıXi.1; t, the empirical distribution function of the sample.
Thus the Baye’s rule bF˛ may be interpreted as a mixture of the prior guess F0 and
the empirical distribution function with respective weights, pn and 1  pn. At the
same time, F0 can be interpreted as the “center” around which the Bayes estimate
resides. Robustness of this estimator is discussed in Hannum and Hollander (1983).
Remark 6.1 M D ˛ .R/ may be interpreted as a precision parameter or the prior
sample size (Ferguson 1973). As ˛ .R/ ! 1, bF˛ reduces to the prior guess
F0 at F. On the other hand, if ˛ .R/ ! 0, the Baye’s estimator reduces to the
sample distribution function and hence it could be said that it corresponds to
providing no information. However, Sethuraman and Tiwari (1982) take issue with
this interpretation. For ﬁnite ˛0 and ˛k; k  1 on R, they show that if, along with the
sequence ˛k .R/ ! 0, we have SupA j˛k .A/  ˛0 .A/j ! 0 as k ! 1, A 2 B, then
D.˛k/ ! ıY0, where Y0 has the distribution ˛0. This means that the information
about P is that it is a probability measure concentrated at a particular point in R; and
the point is selected according to ˛0. This is a deﬁnite information about P and its
discreteness.
6.2.2
Estimation of a Symmetric CDF
In the previous section, F was an arbitrary distribution function. Suppose now we
wish to estimate F which is symmetric about a known point . This suggests that
the space of all distribution functions be restricted to the symmetric distributions
only. So assume F to be distributed according to the Dirichlet Invariant process
(Sect. 2.2), that is, F Ï DGI.˛/, G D fe; gg with e.x/ D x; g.x/ D 2  x. Then the
Bayes estimate of F under the loss function L1 is (Dalal 1979a)
bF˛.t j X1; : : : ; Xn/ D ˛.1; t C 1
2
Pn
iD1.ıXi.1; t C ı2Xi.1; t/
˛ .R/ C n
D pn  F0.t/ C .1  pn/  bFsn.t/;
(6.2.2)
where bFsn.t/ is -symmetrized version of the empirical distribution. This is an
analog of the Bayes estimator OF˛.

224
6
Inference Based on Complete Data
6.2.3
Estimation of a CDF with MDP Prior
Let G .
/ stand for a random distribution function selected from a mixture of
Dirichlet processes (Sect. 2.3) with index space U D R, parameter space ‚ D R,
observation space also R and mixing distribution H. That is G 2 R
R D.˛u/dH.u/
and let 
1; : : : ; 
n
iidÏ G, and given 
i let Xi1; : : : ; Ximi be a sample of size mi from
F
i .x/ ; i D 1; : : : ; n. This is the so-called mixture model presented in Sect. 2.4.
The Bayes estimate of G under the L1 loss function is given by bG
D
E .Gj
1; : : : ; 
n/ if 
i’s are observed directly, and bG D
E .GjXi1; : : : ; Ximi/ if
Xij’s are observed. One can use the formula given under property 3 or 4 of
Sect. 2.3 to evaluate the former, and the formula given under property 5 to evaluate
the latter. Antoniak (1974) has illustrated computational procedures by taking
examples of small sample of size n D 2. As an example, he takes the transition
measure ˛u ./ =˛ .R/ D N.u; 2/, mixing distribution H D N

0; 2
and sampling
distribution F
 D N.
; 2/, and obtains the expression for bG in a closed form. For
larger sample size, the evaluations are difﬁcult. However, computational algorithms
reported in Kuo (1986b) Dey et al. (1998), Ibrahim, Chen, and Sinha (2001), and
simulation procedures discussed in Sect. 2.4 have mitigated the problem to some
extent.
In an effort to compromise between parametric and purely nonparametric
models, Doss (1994) investigates prior distributions for F which give most of their
mass to a “small neighborhood” of an “entire” parametric family. In other words,
he considers the situation where a parametric family H
, 
 2 ‚  Rp is speciﬁed.
Thus, a prior on F is placed as follows. First choose 
 according to some prior ,
then choose F from D .MH
/ with speciﬁed M > 0. This leads to the mixture of
Dirichlet processes priors, F 2
R
DMH
  .d
/. While this formulation encounters
the same computational difﬁculties, it allows him to consider a more general set up
when instead of exact values of Xi.Ï F/, it is only known that Xi 2 Ai  R. Thus we
may have Ai D fxig if Xi is an exact observation, and Ai D .ci; 1/ if Xi is censored
on the right by ci. The task is to obtain the posterior distribution of F given the data.
Doss develops an algorithm for generating a random distribution function from this
conditional posterior distribution.
6.2.4
Empirical Bayes Estimation of a CDF
In the Sect. 6.2.1 we derived the Bayesian estimator of F assuming a Dirichlet
process prior with parameter ˛. It was assumed there that ˛ is known via a known
prior guess F0 of F, and the total mass M. If this is not the case, we need to estimate
F0 or M or both. This can be done via the empirical Bayes (EB) approach which
is described now. The efﬁcacy of the empirical Bayes estimator is judged by a
criterion called “asymptotic optimality”: An empirical Bayes estimator is said to
be asymptotically optimal relative to a class of Dirichlet process priors if the Bayes

6.2
Estimation of a Distribution Function
225
risk of the EB estimator given ˛ converges to the Bayes risk of the Bayes estimator
for all ˛. This being a weak criterion, generally, the rate of convergence is also
indicated.
Since EŒF .t/ D F0 .t/ and var.F.t// D F0 .t/ .1  F0 .t// = .M C 1/, the
parameter ˛ is then expressed as ˛ ./ D MF0 ./, which provides interpretation
of M as a “precision” or “accuracy” or “uncertainty” parameter and speciﬁcation
of F0 implies that the random distribution function is centered around F0. For this
reason, it is felt that the empirical Bayes method, where the sample data is used for
identifying F0, is better rather than specifying some arbitrary F0; whose validation
may or may not be ascertained.
In the empirical Bayes framework, we are currently at the .n C 1/th stage of
an experiment, and information is available not only from the current stage, but
also from the n previous stages. Thus we have a sequence of pairs .Pi; Xi/; i D
1; 2; : : : ; nC1 of independent random elements, where Pi’s are probability measures
on .R; B/ having a common Dirichlet process prior D.˛/. Given Pi D P; Xi D
.Xi1; Xi2; : : : ; Ximi/ is a random sample of size mi from P. The task is to estimate the
distribution function corresponding to P at the .n C 1/th stage or its functional. The
strategy is to use the information provided by the previous n stages in estimating
the parameters of the prior at the .n C 1/th stage. This approach will be used
in estimating the distribution function, the mean, and in general, any estimable
parameters of degree 2 or 3.
Remark 6.2 In many hierarchical modeling, the parameters at intermediate stages
are assumed to have certain distributions with some hyper parameters. This is ﬁne
if there are valid justiﬁcations for such assignments. However, in absence of such
information it is judged that the empirical Bayes methods may offer better solution
since here the observed data itself is used to provide information on the unknown
parameters.
Let F1; F2; : : : ; FnC1 be n C 1 distribution functions on the real line, and for
i D 1; 2; : : : ; n C 1, let Xi D .Xi1; Xi2; : : : ; Ximi/ be a sample of size mi from
Fi. We assume each Fi to have a common Dirichlet process prior, D.˛/. Our
prior information is incorporated through F0 and M. As before bFj.t/ is the sample
distribution function of Xj and pj D ˛ .R/ =.˛ .R/ C mj/ , j D 1; : : : ; n C 1. The
Bayes estimator of FnC1 at the .n C 1/th stage with respect to the prior D.˛/ and
the loss function
L.FnC1;eFnC1/ D
Z 
FnC1.t/  eFnC1.t/
2 dW.t/;
(6.2.3)
based on X1; : : : ; XnC1 is from Sect. 6.2.1 (suppressing the dependence on ˛/,
eFnC1.t/ D pnC1F0 C .1  pnC1/bFnC1.t/:
(6.2.4)

226
6
Inference Based on Complete Data
The Bayes risk of eFnC1.t/ with respect to D.˛/, denoted by r.˛/
D
ED.˛/EFnC1ŒL.FnC1;eFnC1/, is (Korwar and Hollander 1976)
r.˛/ D r.eFnC1; ˛/ D
pnC1
˛ .R/ C 1
Z
F0.t/.1  F0.t//dW.t/ D
pnC1
˛ .R/ C 12;
(6.2.5)
where 2 D
R
x2dF0.x/ .
R
x dF0.x//2 is the variance of F0.
If F0 and M are known, r.˛/ can be evaluated completely. In the EB approach,
we are able to estimate these parameters from the previous data and adjust this
estimator, resulting in what is known as an empirical Bayes estimator.
Korwar and Hollander (1976) considered the case of ˛ when M is known but
F0 is unknown. They estimated F0.t/ by the average of ﬁrst n sample distributions,
1
n
Pn
jD1 bFj.t/, substituted this in (6.2.4) and proposed the following empirical Bayes
estimator of FnC1:
FnC1.t/ D pnC1
1
n
n
X
jD1
bFj.t/ C .1  pnC1/bFnC1.t/.
(6.2.6)
They evaluated the Bayes risk of FnC1.t/ as
r.FnC1; ˛/ D EŒL.FnC1; FnC1/ D r.˛/
0
@1 C pnC1
n2
n
X
jD1
1
1  pj
1
A ;
(6.2.7)
where the expectation is taken with respect to D.˛/ as well as X1; : : : ; XnC1. When
the samples are of same size as m, r.FnC1; ˛/ reduces to r.˛/Œ1 C ˛ .R/ =mn.
Clearly, as n ! 1, r.FnC1; ˛/ ! r.˛/ for any ˛. Thus they concluded that
the estimator is asymptotically optimal and established the rate of convergence
0.n1/. Zehnwirth (1981) relaxed the assumption of M known in the case of equal
sample size and estimated M in a clever way (to be described below) by the F-ratio
statistic ±n in one-way analysis of variance based on X1; : : : ; Xn and showed that
the resulting estimator of FnC1 is also asymptotically optimal with the same rate of
convergence, 0.n1/.
Note that the estimators of F0 proposed by Korwar and Hollander and Zehnwirth
were based only on the past data, but not the current data. Ghosh et al. (1989)
modiﬁed these estimators to include the current data as well. Thus, it gives greater
weight to the current data in estimating FnC1.t/ than that in the Hollander and
Korwar and Zehnwirth estimators, and yields smaller risk than those estimators.
When ˛ .R/ is known, their proposed empirical Bayes estimator of FnC1.t/ turns
out to be
eF
nC1.t/ D pnC1bF0.t/ C .1  pnC1/bFnC1.t/,
(6.2.8)

6.2
Estimation of a Distribution Function
227
where bF0.t/ D PnC1
jD1 .1  pj/bFj.t/= PnC1
jD1 .1  pj/, and the Bayes risk is
r.eF
nC1; ˛/ D r.˛/
"
1 C
pnC1
PnC1
jD1 .1  pj/
#
;
(6.2.9)
which converges to r.˛/ as n ! 1, and hence it is asymptotically optimal.
Comparing the risks of estimators with and without the use of the current data, it can
be veriﬁed that r.eF
nC1; ˛/  r.˛/  r.FnC1; ˛/  r.˛/ and hence an improvement
is achieved by using the estimator eF
nC1.t/ over FnC1. In fact if the sample sizes are
equal, r.eF
nC1; ˛/  r.˛/ D .n=.n C 1//Œr.FnC1; ˛/  r.˛/.
Observe that eF
nC1.t/ is a linear combination PnC1
jD1 w
j bFj.t/, with w
j D pnC1.1
pj/=
PnC1
jD1 .1  pj/

, j D 1; : : : ; n, and w
nC1 D pnC1.1pnC1/=.PnC1
jD1 .1pj//C
.1pnC1/. Clearly PnC1
jD1 w
j D 1. This gives a clue for them to show that indeed the
Bayes risk of eF
nC1 is smaller than the Bayes risk of any other estimator of the form
PnC1
jD1 wjbFj with PnC1
jD1 wj D 1. By taking different choices of wj we can see that this
class includes the following estimators. The choice of wj D pm=n . j D 1; : : : ; n/
and wnC1 D 1  pm, with m1 D    D mn D m and pm D ˛ .R/ =.˛ .R/ C m/,
leads to Korwar and Hollander (1976) estimator FnC1.t/. Another possible choice
of wj D 1=.nC1/ for j D 1; : : : ; nC1 which leads to the estimator PnC1
jD1 bFj=.nC1/
of FnC1. Also, the usual MLE estimator of FnC1 is bFnC1 which is obtained when
wnC1 D 1, and w1 D    D wn D 0.
When ˛ .R/ is unknown, Zehnwirth (1981) proposed an estimator for ˛ .R/ based
on a one-way ANOVA table using the past data X1; : : : ; Xn for equal sample size m
at each stage and proved its consistency
m=.1  ±n/ ! ˛ .R/ in probability as n ! 1:
(6.2.10)
Ghosh et al. (1989) provide an improvementover his estimator by including XnC1
as well in the ±n statistic. Let Xj D Pmj
iD1 Xji=mj be the mean of the sample values
at jth stage and X D PnC1
jD1 mjXj = PnC1
jD1 mj denote the overall mean. Deﬁne
MSW D
nC1
X
jD1
mj
X
iD1
.Xji  Xj/2
,nC1
X
jD1
.mj  1/ and MSB D
nC1
X
jD1
.Xj  X/2 =n;
(6.2.11)
the usual within and between mean squares, respectively. Simple evaluations
involving the Dirichlet process yield
E .MSW/ D .˛ .R/ =.˛ .R/ C 1// 2
(6.2.12)
E .MSB/ D .˛ .R/ =.˛ .R/ C 1// 2 C : 2=.n.˛ .R/ C 1//;
(6.2.13)

228
6
Inference Based on Complete Data
where  D PnC1
jD1 mj  PnC1
jD1 m2
j = PnC1
jD1 mj. They proposed the following estimator
of ˛ .R/.
O˛1 .R/ D maxf0; .MSB =MSW  1/ n =g,
(6.2.14)
which is shown to be strongly consistent under some mild conditions. [Note that the
Zehnwirth’s (1981) estimator of ˛ .R/ is based only on X1; : : : ; Xn and had assumed
m1 D    D mnC1]. Substituting this estimator of ˛ .R/ in pj D .1 C mj˛1 .R//1
they revise the estimate for F0 as
eF0.t/ D
( PnC1
jD1 .1  Opj/bFj.t/= PnC1
jD1 .1  Opj/; if O˛1 .R/ ¤ 0
PnC1
jD1 bFj.t/ =.n C 1/;
if O˛1 .R/ D 0.:
(6.2.15)
Finally for the case ˛ .R/ unknown, Ghosh et al. (1988) utilizing these estimators
proposed beFnC1 as an improved empirical Bayes estimator of FnC1, where
beFnC1.t/ D OpnC1eF0.t/ C .1  OpnC1/bFnC1.t/,
(6.2.16)
and proved the asymptotic optimality of this estimator.
6.2.5
Sequential Estimation of a CDF
Ferguson (1982) derives the sequential estimator of F with respect to the loss
function L1 and prior D.˛/, with F0 D ˛ taken as a speciﬁed distribution function
on R. Then as noted earlier, E.F/ D F0 and the posterior distribution of F, given
the sample X1; : : : ; Xn from F, is D..M C n/ OF˛/, where bF˛, as before, is the Bayes
estimator of F under L1, and the minimum Bayes risk is obtained as
Z
Var.F.x/jX/dW.x/ D .1=.M C n C 1//
Z
˛
bF˛.x/.1  bF˛.x//dW.x/:
(6.2.17)
In sequential estimation we need a stopping rule and a terminal estimator. It is
enough to ﬁnd a stopping rule, since once we have the stopping rule, the terminal
Bayes estimator is bF˛ itself. Ferguson provides the k-stage look ahead rule which, at
each stage stops or continues according to whether the rule is optimal among those
taking at most k more observations stops or continues. There is a positive cost c > 0
to look for each additional observation. After observing X1; : : : ; Xn, the 1-stage look
ahead rule calls for stopping after the ﬁrst n observations for which
Z
˛
bF˛.1  bF˛/dW  c.M C n C 1/2:
(6.2.18)

6.2
Estimation of a Distribution Function
229
Clearly the left-hand side is bounded above by W.R/=4 and the right-hand side
increases with n. Ferguson argues that the 1-stage look-ahead rule eventually calls
for stopping and bounds on the maximum sample size can be found and provides
justiﬁcation for the optimality of this rule.
Ferguson also discusses the sequential estimation of the mean 	 D R xdF.x/
under the squared error loss function L2. Let 	0 D R xdF0.x/, the prior estimate of
the mean and Xn D .1=n/ Pn
1Xi. The minimum conditional Bayes risk is given by
Var.	jX1; : : :Xn/ D 2
n =.M C n C 1/, where 2
n is the variance of the distribution
bFn, which can be expressed as
2
n D
Z
.x  	n/2dbFn.x/ D
	
M2
0 C ns2
n C
Mn
M C n.xn  	0/2

=.M C n/;
(6.2.19)
with 2
0 as the variance of F0 and ns2
n D Pn
1.xi  xn/2. Then the 1-stage rule is to
stop after the ﬁrst n observations for which 2
n  c.M C n C 1/2. He also provides
further some modiﬁed stopping rules and discusses their usage.
Sequential approach from the Bayesian point of view is also used by Hall (1976,
1977) in treating search problems with random overlook probabilities having a
Dirichlet or a mixture of Dirichlet processes priors. Clayton and Berry (1985) treat
one-armed bandit problem and Clayton (1985) a sequential testing problem for the
mean of a population.
6.2.6
Minimax Estimation of a CDF
One of the ﬁrst non-Bayesian application of the Dirichlet process was contained in
Phadia (1971), where a sequence of Dirichlet process priors was used in deriving
the minimax estimator of an unknown F based on a sample of size n. The technique
used was ﬁrst to ﬁnd an equalizer rule given by
bFmx.t/ D
pn=2 C Pn
iD1 ıXi.1; t
pn C n
;
and a sequence of least favorable priors D.˛k/ was deﬁned, where ˛k was taken
to be a ﬁnite measure giving equal weight pn=2 to points ˙k, k a nonnegative
real number. Then it was shown that the Bayes risk of the Bayes estimator with
respect to D.˛k/ converges to the risk of the above equalizer rule as k ! 1. Thus
it was established that the above estimator was minimax under the L1 loss (minimax
estimators for other loss functions were also obtained and in particular it was shown
that the sample distribution function was minimax under a weighted quadratic loss
function). However, at Ferguson’s suggestion the results were simpliﬁed by taking
a sequence of beta distribution priors (Phadia 1973).

230
6
Inference Based on Complete Data
6.3
Tolerance Region and Conﬁdence Bands
In this section we ﬁrst present the Tolerance region discussed by Ferguson in his
1973 paper, and then the construction of a conﬁdence band as proposed by Breth
(1978) and others.
6.3.1
Tolerance Region
Ferguson (1973) treats the problem of deriving a tolerance region from a decision
theoretic approach. Suppose we want to estimate the qth quantile tq of an unknown
distribution F on the real line by an upper tolerance point a under the loss function
L. p; a/ D pP..1; a/ C qI.a;1/.tq/;
(6.3.1)
where p is a constant, 0 < p < 1. If tq is known exactly, L is minimized by choosing
a D tq. But if tq is not known precisely, then we need to minimize the Bayes risk
with respect to the D.˛/ given by
E.L. p; a// D pu C q
Z q
0
.M/
.uM/..1  u/M/ zuM1.1  z/.1u/M1dz;
(6.3.2)
where u represents F0.a/, and M D ˛ .R/ as before. Let u D f. p; q; M/ denote the
point at which the minimum occurs. Then the Bayes rule for the no-sample problem
is given by a D f. p; q; ˛ .R//th quantile of F0. For a sample X1; : : : ; Xn of size n,
the Bayes rule therefore is given by
Oan.X/ D f. p; q; ˛.R/ C n/th quantile of bFn.
(6.3.3)
6.3.2
Conﬁdence Bands
In the classical theory, conﬁdence bands .FL; FU/ for an unknown distribution
function F are constructed for a given conﬁdence level 1  , such that
P .FL  F  FU/ D 1  . Here F is considered to be ﬁxed while FL and FU
are random, they being functions of ordered sample values. In the Bayesian context,
it is the other way around—F is considered to be random and FL and FU are ﬁxed
and determined in terms of the prior and posterior probabilities. Breth (1978) and
Neath and Bodden (1997) treat this problem.

6.3
Tolerance Region and Conﬁdence Bands
231
Recursive Method
Breth deﬁnes Bayesian conﬁdence bands as follows.
Deﬁnition 6.3 (Breth) Suppose F Ï D.˛/. Then if PfFL.t/  F.t/  FU.t/ for
all t g D 1 .2/ is a prior (posterior) probability, the functions FL.t/and FU.t/
constitute the boundaries for a ﬁxed region within which the random distribution
function lies with prior (posterior) probability 1 .2/. .FL.t/; FU.t//are deﬁned to
be a pair of Bayesian conﬁdence bands for the random distribution function F with
prior (posterior) probability 1 .2/.
Let m be a ﬁxed positive integer and for i D 1; 2; : : : ; m deﬁne ui and vi such
that ui < vi for all i and 0 D u0  u1      um < 1; 0 < v1  v2     
vmC1 D 1. Further, let I .x/ D 1 if x  0 and 0 otherwise, and J .x/ D 1 if x
> 0 and 0 otherwise. For 1 D t0 < t1 < t2 <    < tm < tmC1 D 1, deﬁne
FL.x/ D Pm
iD1 .ui  ui1/ I .x  ti/ and FU.x/ D v1 C Pm
iD1 .viC1  vi/ J .x  ti/.
Also, for a > 0, let ˛ .R/ D a C 1 > 0 and ˛ .t/ =˛ .R/ be a distribution function.
It is clear that PfFL.t/  F.t/  FU.t/ for all tg D Pfuj  F.tj/  vj for
j D 1; : : : ; mg. Therefore, to be able to calculate the probabilities of this type, it
sufﬁces to be able to calculate general rectangular probabilities (Steck 1971) over
the ordered Dirichlet distribution, since .F.t1/; : : : ; F.tm//  D.a1; : : : ; am;:amC1/
with aj D ˛

tj

 ˛

tj1

; j D 1; : : : ; m. To calculate the boundaries with respect
to the posterior probability, replace ˛ by ˛ D ˛ C nbFn, where bFn is the sample
distribution function for the sample of size n.
It should be noted that as in the classical theory, there are many pairs of Bayesian
conﬁdence bands for F with the same probability content 1  , say. In practice, a
particular pair must be chosen to express quantitative conﬁdence in F.
Breth (1978) uses recursive methods for computing Pfuj  F.tj/  vj
for
all jg for ﬁxed numbers fujg, fvjg, and ftjg when F is a Dirichlet process, and
gives details on calculations that are needed in practical applications. In a follow-up
paper he (Breth 1979) discusses construction of Bayesian conﬁdence intervals for
quantiles and the mean, and also treats Bayesian tolerance intervals. The complexity
in numerical calculation is evident. If ˛ is not stipulated a priori, it can be estimated
(see Korwar and Hollander 1973).
In this connection it is worth mentioning that in non-Bayesian context, Phadia
(1974) constructed the best invariant one- and two-sided conﬁdence bands for an
unknown continuous distribution function F. They were invariant under the group
G of transformations g.y1; : : : ; yn/ D ..y1/; : : : ; .yn//, where  is a continuous,
strictly increasing function from R onto R. The conﬁdence bands were step functions
taking jumps at the ordered sample values. For a given conﬁdence level, the values
of jumps were calculated as a minimization problem using Steck’s (1971) result.

232
6
Inference Based on Complete Data
Simulation Method
Neath and Bodden (1997) also constructed .1/100 % Bayesian conﬁdence bands
FL and FU by using a simulation method. Let P be a random probability measure
having a mixture of Dirichlet processes as prior distribution. In other words, 
 Ï G,
Pj
 Ï D.˛
/. Let F be a distribution function corresponding to P. Given a random
sample from F, ﬁrst a value of 
 is obtained from the posterior distribution GX of

, given X: The posterior distribution of F given the data and 
 is D.˛
 C Pn
1ıxi/.
However, this distribution is analytically intractable. Therefore, in constructing the
conﬁdence bands, they treat simulated sample of distribution functions F1; : : : ; FN
as the actual distributions and choose FL and FU such that
1
N
N
X
iD1
IfFL.t/  Fi.t/  FU.t/; t 2 Rg  1  :
(6.3.4)
In choosing the “best” bounds, the following two criteria are used:
.i/ minfmax
t ŒFU.t/  FL.t/g and (ii) min
Z
ŒFU.t/  FL.t/dW.t/

:
(6.3.5)
The minimum is taken over all functions FL and FU such that (6.3.4) is satisﬁed.
For the process of choosing FL and FU, they give two algorithms and discuss their
implementation, and also provide a numerical example to illustrate the procedure.
Bayesian Bootstrap Method
Hjort (1985), on the other hand, uses a Bayesian bootstrap method to construct
conﬁdence intervals for a function 
 .F/ of an unknown F. Let F Ï D .˛/ and
X1; : : : ; Xn
iidÏ F. Then FjX Ï D.˛CPn
1ıxi/ which can be written as D.MF0CnbFn/.
Let G .t/ D P.
 .F/  tjbFn/. We need to ﬁnd 
L and 
U such that P.
L  
 .F/ 

U/ D 1  2, say. Thus G1 ./ and G1 .1  / are the natural choices for 
L
and 
U, respectively with G1 . p/ D inf ft W G .t/  pg. Now a large set of values of

 .F/ can be generated via Monte Carlo and then G can be approximated, permitting
the calculation of G1 . p/.
6.4
Estimation of Functionals of a CDF
In this section we discuss various applications in which Bayesian estimators of
certain functionals such as the mean, median, and variance are derived using the
Dirichlet process priors.

6.4
Estimation of Functionals of a CDF
233
6.4.1
Estimation of the Mean
The Bayesian estimation of the mean 	 D R xdP.x/ with respect to the Dirichlet
process prior and under the squared error loss L2 was considered in Ferguson (1973).
It is assumed that ˛ has a ﬁnite ﬁrst moment. The Bayes rule for the no-sample
problem is the mean of 	, say, 	0 which, by property 6 of Sect. 2.1.2, is
^	 D
ED.˛/
R xdP.x/ D R xd˛.x/=˛ .R/ D 	0. The Bayes rule for a sample of size n
therefore is obtained by updating the parameter ˛ to ˛ C Pn
iD1 ıXi, and is given by
O	˛n.X/ D .˛ .R/Cn/1
Z
xd.˛.x/C
n
X
iD1
ıXi.x// D pn	0 C.1pn/Xn;
(6.4.1)
where Xn D 1
n
Pn
iD1 Xi is the sample mean. The Bayes estimator thus is like OF˛,
a convex combination of the prior guess at 	, namely 	0, and the sample mean.
As ˛ .R/ ! 0, O	˛n ! Xn, and as ˛ .R/ ! 1, O	n ! 	0. The Bayes risk of O	n
is r.˛/ D pn2= .˛ .R/ C n/. Alternatively, the estimator can also be obtained by
taking g.x/ D x in property 9 of Sect. 2.1.2. More generally, let Z be a measurable
real valued function deﬁned on .R; B/ and 
 D R ZdP. If P Ï D.˛/ and 
0 D
R Zd˛=˛ .R/ < 1, then the Bayes estimator of 
 under the loss L2 is given by
^

˛n.X/ D pn
0 C .1  pn/1
n
n
X
iD1
Z.Xi/:
(6.4.2)
Yamato (1984) showed that the mean 	 is distributed symmetrically about a
constant 
 if the measure ˛ is symmetric about 
 and
R
jxj d˛.x/ < 1.
If ˛ and ˛ .R/ are unknown, the empirical Bayes method can be used.
6.4.1.1
Empirical Bayes Estimation of the Mean
This can be dealt with in the same manner as the distribution function in Sect. 6.2.4
and the same notations will be used here as well. The Bayes estimator with respect
to D.˛/ at the .n C 1/th stage is given by
O	˛ D pnC1	0 C .1  pnC1/
mnC1
X
iD1
XnC1;i=mnC1:
(6.4.3)
The Bayes risk of O	˛ is given by r.˛/ D pnC12=.˛ .R/ C 1/. For the empirical
Bayes approach, 	0 is estimated from the ﬁrst n samples by Korwar and Hollander
(1976) and the resulting estimator O	n has the Bayes risk as r. O	n; ˛/ D .1 C
˛ .R/ = PnC1
iD1 mi/r.˛/. Ghosh et al. (1988) estimate 	0 from the past as well as
current sample data as O	0 D PnC1
jD1 .1  pj/Xj= PnC1
jD1 .1  pj/ and plugs in O	˛ of

234
6
Inference Based on Complete Data
(6.4.3). The resulting estimator is denoted as O	nC1 and its Bayes risk is
r. O	nC1; ˛/ D r.˛/ C p2
nC12=
nC1
X
jD1
.1  pj/:
(6.4.4)
They have shown that O	nC1 is asymptotically optimal and has a smaller Bayes risk
than the estimator proposed by Korwar and Hollander (1976). Again, if ˛ .R/ is
unknown, it can be estimated as indicated in Sect. 6.2.4.
In the context of a ﬁnite population of size N, Binder (1982) considered the
task of Bayes estimation of the population mean PN
iD1 Xi=N, where X1; : : : ; XN are
population values, by assuming that there is a super population P with prior D.˛/,
and given P, these values are iid P.
Ghosh et al. (1989) have also considered the empirical Bayes estimation of the
ﬁnite population distribution function. Tiwari and Lahiri (1989) have treated the
Bayes and empirical Bayes estimation of variances from stratiﬁed samples and
studied the risk performance of the empirical Bayes estimators.
6.4.2
Estimation of a Variance
Consider now the task of estimating the variance of an unknown probability
distribution P. If ˛ has a ﬁnite second moment, then the variance of P deﬁned by
Var P D
Z
x2dP.x/ 
	Z
xdP.x/

2
;
(6.4.5)
is a random variable. Ferguson (1973) obtained the Bayes estimator under the
squared error loss L2 assuming the Dirichlet process prior. The Bayes estimator
of Var P for the no-sample problem is the posterior mean
E Var P D E
Z
x2dP.x/  E
	Z
xdP.x/

2
D .2
0 C 	2
0/ 
	
2
0
˛ .R/ C 1 C 	2
0

D
˛ .R/
˛ .R/ C 12
0 ;
(6.4.6)
where 	0 is as deﬁned above in Sect. 6.4.1 and 2
0 D
R
x2d˛.x/=˛ .R/  	2
0 is the
variance of F0.

6.4
Estimation of Functionals of a CDF
235
For a sample of size n, the Bayes rule is therefore obtained by replacing the
parameter ˛ by ˛ C Pn
iD1 ıXi . After some simpliﬁcation and rearrangement, we get
the Bayes estimator of Var P as
O2
n .X/ D
˛ .R/ C n
˛ .R/ C n C 1Var. OF˛/
D
˛ .R/ C n
˛ .R/ C n C 1. pn2
0 C .1  pn/ s2
n C pn.1  pn/.	0  Xn/2/
D
˛ .R/ C n
˛ .R/ C n C 1
 
pn2
0 C .1  pn/
 
pn
1
n
n
X
iD1
.Xi  	0/2 C .1  pn/ s2
n
!!
;
(6.4.7)
where s2
n D 1
n
Pn
iD1.Xi  Xn/2. The last equality expresses O2
n as a mixture of three
different estimates of the variance, as noted by Ferguson.
If the prior sample size ˛ .R/ ! 0, keeping F0 ﬁxed, O2
n converges to the estimate
1
nC1
Pn
iD1.Xi  Xn/2. This estimate is the best invariant or minimax estimator of the
variance of a normal distribution under the loss .VarP  O2/2=.VarP/2.
6.4.3
Estimation of the Median
Next consider the problem of estimation of the median 
 deﬁned as 
D
med P. Ferguson (1973) derived the Bayes estimator under the absolute error loss,
L.
; O
/ D j
  O
j. 
 is unique with probability one and thus a well deﬁned random
variable. Under this loss function, any median of the distribution of 
 is a Bayes
estimator of 
. For the Dirichlet process prior with parameter ˛, Ferguson points
out that any median of the distribution of 
 is a median of the expectation of P, and
conversely,
med .dist: med P/ D med EP:
(6.4.8)
Thus any number t satisfying
˛..1; t//
˛ .R/
 1
2  ˛..1; t/
˛ .R/
(6.4.9)
is a Bayes estimate of 
 with respect to the prior D.˛/ and absolute error loss.
With F0.t/ D ˛..1; t/=˛ .R/ ; the Bayes estimate for the no-sample problem is
O
 D median of F0 and for the sample of size n, it is
O
˛n D median of bF˛;
(6.4.10)
where bF˛ is the Bayes estimate of F derived in Sect. 6.2.1.

236
6
Inference Based on Complete Data
Doss (1985a,b) also considers the problem of estimating the median but in a
different nonparametric Bayesian framework. Let X1; : : : ; Xn be a random sample
with distribution F
, where F
.x/ D F.x  
/ for some F that has median 0. F
is assumed to be unknown and the problem is to estimate 
. Rather than placing
a prior on F, he chooses F and FC from D.˛/ and D.˛C/, respectively, and
deﬁnes F .t/ D .F .t/ C FC .t// =2, where ˛ and ˛C are the restriction of ˛
to the intervals .1; 0/ and .0; 1/, respectively. Then, F is a random distribution
function such that F .0/ D 1
2 .but not symmetric, although E .F .t// D F0/. In fact F
so deﬁned is a mixture of two Dirichlet processes. Let D.˛/ denote its distribution.
Let ˛ D MF0, where F0 is a distribution function with median zero and for
simplicity, no mass at zero. He places a prior on the pair .F; 
/ by assuming F and

 independent, F Ï D .˛/ and 
 having an arbitrary distribution . Given 
 and F,
let X be a sample from F .x  
/. Assume that F0 has continuous density f0. Then,
Doss obtains the marginal posterior distribution of 
 given X as
d .
jX/ D  .X/ …Œ f0 .Xi  
/‰ .X;
/ d .
/ ;
(6.4.11)
where ‰1 .X;
/ D .M=2 C nbFn .
//.M=2 C n.1  bFn .
///;bFn the sample
distribution function, … represents the product taken over the distinct Xi and  .X/
is a normalizing constant.
Using the posterior distribution one can ﬁnd the Bayes estimate of 
. The
estimator is essentially a convex combination of the maximum likelihood estimator
with respect to F0 and the sample median, with mixing weights depending on the
sample values. Doss shows that the Bayes estimator is consistent only if the true
distribution of Xj is discrete. He also derives the posterior distribution of 
 in the
case of F being a “neutral to the right type” distribution discussed in Sect. 4.2.
6.4.4
Estimation of the qth Quantile
Ferguson (1973) extends the estimation of the median to the qth quantile of P,
denoted by tq: P..1, tq//  q  P..1, tq/, for 0 < q < 1. The qth quantile of
P Ï D.˛/ is unique with probability 1, so that tq is a well-deﬁned random variable.
He considers the following loss function:
L.tq;Otq/ D p.tq  Otq/
if tq  Otq
D .1  p/.tq  Otq/
if tq < Otq;
(6.4.12)

6.4
Estimation of Functionals of a CDF
237
for some p; 0 < p < 1. For this loss, any pth quantile of the distribution of tq is a
Bayes estimate of tq. The distribution of tq is
Pftq  tg D PfF.t/ > qg D
Z 1
q
.M/
.uM/..1  u/M/ zuM1.1  z/.1u/M1dz;
(6.4.13)
where M D ˛ .R/ and u D ˛..1; t/=˛ .R/ D F0.t/. Setting this expression equal
to p and solving the resulting equation for t, Ferguson obtains the pth quantile of
tq. For ﬁxed p; q, and M, let this equation deﬁne a function u. p; q; M/. The Bayes
estimate of tq for the no-sample problem is the uth quantile of F0,
Otq D u. p; q; ˛ .R//th quantile of F0;
(6.4.14)
and for the sample of size n, it is
Otq.X/ D u. p; q; .˛.R/ C n//th quantile of bF˛:
(6.4.15)
If p and q are both 1
2, this reduces to the estimate of the median, since u. 1
2; 1
2; M/ D
1
2 for all M.
Doss (1985a,b) extends his own results of estimating the median to the estimation
of quantiles as well, and discusses their properties.
6.4.5
Estimation of a Location Parameter
Considered the following model for sample observations. Let Y D C", where  is
the location parameter and ", the error term. Assume that  and " are independent.
The objective is to estimate  based on a random sample Y1; : : : ; Yn from an -
symmetric distribution function F. That is F is assumed to be symmetric about
, but otherwise  and F are unknown. If " Ï G and G Ï D .MF0/, where F0
could be a standard normal distribution, then E .G/ D F0 and hence the errors
are generated by a distribution in the neighborhood of F0. With M
large the
neighborhood becomes concentrated around F0. Thus, Dalal (1979b) argues that the
model can be interpreted from a robustness perspective as well. Let  be distributed
according to a prior distribution , the group of transformations G D fe; gg with
e.x/ D x; g.x/ D 2x , and ˛ be a -symmetric non-null ﬁnite measure on .R; B/.
Given , he assumes F to be distributed according to the Dirichlet Invariant process,
DGI.˛/ and obtains a Bayes estimate O.y/ D Ejy./ of , where the expectation
is taken with respect to the conditional distribution .jy/ of  given y averaged
over F. However, O.y/ is not in a closed form and he encounters computational
difﬁculties which is illustrated by an example consisting of 2 observations.

238
6
Inference Based on Complete Data
Let ˛ D MF0, and assume that F0 has a density f0, and that we have a sample
of size one, Y1 Ï F with F Ï DGI .˛/. Then E

F

D F0 and the marginal
conditional distribution of Y1 given  is F0. Since  is a prior distribution of , the
conditional density of j Y1 D y1 Ï f0 .y1/ =
R
f0 .x/ d .x/.
If we have a second observation y2, then we run into difﬁculty since the
distribution of Y2jy1; ; F Ï F. But Fjy1;  Ï DGI

˛ C 1
2ıy1 C 1
2ı2y1

which
results in a distribution of Y2jy1;  as a combination of continuous and discrete
parts with point discrete masses at y1 and 2  y1. Thus the evaluation of the
posterior distribution of jy2; y1 gets complicated. The above argument is extended
to the case of n observations and shown that if  is absolutely continuous, the
posterior distribution of jy is a mixture of absolutely continuous and discrete
probabilities. The mixing weights depend upon not only on the distinct observations
but also on their multiplicities. The discrete component concentrates its mass on the
points

yi C yj

=2, i ¤ j. However, the computational techniques lately developed
involving simulation should make it simpler to compute the posterior distribution.
This and other aspects of Bayesian estimation of a location parameter are
discussed in his paper in detail.
Doss (1985a) also discusses this model, but instead of errors drawn from a
symmetric distribution, he takes them to be drawn from an F which has median
0, but otherwise unknown, and it is desired to estimate . He places priors on the
pair .F; / and computes the marginal posterior distribution of  and takes the
mean of the distribution as the estimate of . In a follow-up paper (Doss 1985b)
he discusses consistency issues and shows that the Bayes estimates are consistent if
the distribution of errors is discrete, otherwise they can be inconsistent.
6.4.6
Estimation of P.Z > X C Y/
Zalkikar, Tiwari, and Jammalamadaka (1986) considered the problem of estimation
of the parameter
 .F/ D P.Z > X C Y/ D
Z 1
1
Z 1
1
S.x C y/dF.x/dF.y/;
(6.4.16)
where it is assumed that the random variables X; Y, and Z are independent from
the same distribution function F, and S D 1  F. This problem is encountered
in reliability theory where it is desired to test whether new is better than used.
Assume F Ï D.˛/ and the squared error loss L2. Based on a random sample

6.5
Other Applications
239
X D .X1; : : : ; Xm/ from F, they derived the Bayes estimator as follows:
b .F/ D
M C m
.M C m/.3/
h
2.M C m C 1/ C bF˛.0/
C.M C m/
Z 1
1
S˛.2y/dbF˛.y/ C .M C m/2.bF˛/

;
(6.4.17)
where a.k/ D a.a C 1/ : : : .a C k  1/ and S˛ D 1  bF˛.
When M ! 0, the estimator reduces to an estimator which is asymptotically
equivalent to the U-statistic,
Um D
1
m.m  1/.m  2/
X
IŒXi > Xj C Xk;
(6.4.18)
where the summation is taken over all m.m  1/.m  2/ distinct triplets .i; j; k/,
1  i; j; k  m.
By using the earlier mentioned technique (Sect. 6.2.4), they obtain an empirical
Bayes estimate at the .nC1/th stage utilizing the past as well as current data, which
is also shown to be asymptotically optimal with the rate of convergence 0.n1/.
6.5
Other Applications
There are many other applications that have not been covered here. For exam-
ple, Lo (1988) has studied the Bayesian bootstrap estimation of a functional

.PI X1; : : : ; Xn/, where the variables X1; : : : ; Xn are iid P, with P having the prior
D.˛/. He has provided large sample Bayesian bootstrap probability intervals for the
mean, variance, and conﬁdence bands for the distribution function, the smoothed
density, and smoothed rate function. In a subsequent paper (Lo, 1988), he also
considered a Bayesian bootstrap for a ﬁnite population.
Dirichlet process priors have also been used for bandits problem by Clayton
(1985).
We do, however, present a few interesting applications.
6.5.1
Bayes Empirical Bayes Estimation
In a general empirical Bayes setting (or mixture models), we have n unobservable
independent random variables 
i, i D 1; 2; : : : ; n from an unknown distribution G,
and associated with each 
i, we have a random variable Xi chosen independently
from a distribution with density function fi.xj
i/, i D 1; 2; : : : ; n. The problem is
to estimate 
i’s or G itself. A common procedure is to obtain ﬁrst an estimator Gn

240
6
Inference Based on Complete Data
of G from the data X1; : : : ; Xn, and then estimate 
i as the Bayes estimate with
respect to the prior Gn. In the Bayesian approach to the empirical Bayes problem,
G itself is to be considered random with a prior distribution. Berry and Christensen
(1979) followed this route assuming the Dirichlet prior D(˛/ for G. Antoniak (1974)
had shown that the posterior distribution of G is a mixture of Dirichlet processes
with parameter ˛ C Pn
iD1 ı
i and mixing distribution H.jX/. Thus, the posterior
distribution of G given X in symbols is
GjX Ï
Z
D
 
˛ C
n
X
iD1
ı
i
!
dH.jX/:
(6.5.1)
If we have unconditional marginal distribution of , then dH.jX/ can be
expressed as
dH.jX/ D
n
Y
jD1
fj

xjj
j

dH ./
,0
@
Z
n
Y
jD1
fj

xjj
j

dH ./
1
A :
(6.5.2)
However, even in the simple case where fi.xj
/ is a binomial distribution
with parameter 
, Berry and Christensen (1979) found it difﬁcult to evaluate and
recommended some approximations. By using a lemma of Lo (1984), Kuo (1986a,b)
was able to express the Bayes estimator of 
i under the loss Pn
iD1.
i 
^

/2 in a
concise form as a ratio of two n-dimensional integrals as follows:
^

i D E.
ijX/ D
R
: : :
R
Rn.
i
Qn
iD1fi.xij
i//Qn
iD1.˛ C Pi1
jD1 ı
j/.d
i/
R
: : :
R
Rn.Qn
iD1fi.xij
i//Qn
iD1.˛ C Pi1
jD1 ı
j/.d
i/
(6.5.3)
for all i D 1; 2; : : : ; n. Still it is hard to evaluate these integrals. She overcomes
this problem by decomposing each of the multidimensional integrals as a weighted
average of products of one dimensional integrals and approximating each of the
weighted averages by an importance sampling Monte Carlo method. She illustrates
the computation in detail with a numerical example.
This model has been discussed in Escobar and West (1995) and Escobar (1994).
Lavine (1994) generalizes the approach by using a Polya tree prior for G and
shows how the posterior distribution can be computed via the Gibbs sampler and
demonstrates the advantages of using mixtures of Polya trees over mixtures of
Dirichlet processes.

6.5
Other Applications
241
6.5.2
Bioassay Problem
The goal of the bioassay problem is to assess the dose–response relationship in a
population. In particular, one is interested in the estimation of the distribution of
tolerance level to a drug administered to subjects at various dose levels. In order
to determine an effective dose, one needs to collect data at different dose levels
and their effect on the subjects in mitigating the condition for which the drug is
administered. The impact of the drug on subjects is represented by a CDF, F.t/,
deﬁned on Œ0; 1/ and represents the proportion of the population that would respond
to dose t. This distribution is often known as dose–response curve in the ﬁeld of
bioassay.
Suppose a stimulus is administered to nj subjects at dose level tj with positive
response in rj subjects, j D 1; 2; : : : ; L. Let F .t/ represent the probability of getting
positive response at dose level t. Thus rj, j D 1; 2; : : : ; L are independent, each
being a binomial random variable with parameters nj and F.tj/. Based on such
quantal response data, the task is to estimate the response curve F nonparametrically
from a Bayesian approach. This problem was ﬁrst considered by Kraft and van
Eeden (1964) who used a dyadic tailfree process as prior. The computations are
difﬁcult and were illustrated in the case of only three dose levels in their paper.
Ramsey (1972) uses a Dirichlet process prior and obtains the modal estimates of
F by maximizing the ﬁnite dimensional joint density of the posterior distribution
which is not a Dirichlet.
Ferguson and Phadia (1979) noted that the bioassay problem may be considered
as a censored sampling problem in which bioassay positive responses are observa-
tions censored on the left (since they could have responded to the drug at t
i but were
observed at ti only), and non-responses (failures) are observations censored on the
right. Thus if all positive responses were considered as the real observations, they
can be taken care of by updating the parameter of the Dirichlet prior. They showed
(see Sect. 7.2.8) that the application of Ramsey’s formulas when all observations are
failures and Dirichlet process updated for real observations yield the modal estimate
of F [see Eq. 7:2:21]. It is also noted that in the case of all failures, Ramsey’s modal
estimate has a simple closed form (Ramsey’s estimator was not) and is essentially
given by the Susarla–Van Ryzin estimator of the survival function S D 1  F.
Antoniak (1974) also assumes the Dirichlet process prior and worked out an exact
solution in the case of two dose levels and showed that the posterior distribution
leads to a mixture of Dirichlet processes. For example, if there is only one dose at
t1, F.t1/ has a beta distribution, Be .˛.0; t1; ˛ .t1; 1// and the posterior distribution
would be Be .˛.0; t1 C r1; ˛ .t1; 1/ C n1  r1/, and therefore, the Bayes estimator
under the integrated squared error loss will be the mean of this distribution, OF .t1/ D
.˛.0; t1 C r1/ = .˛ .0; 1/ C n1/. This is deceptively simple. For two dose levels at
t1 < t2 it starts to get complicated. Antoniak worked out the details and produced

242
6
Inference Based on Complete Data
the following estimator:
OF .t1/ D
r2
X
iD0
n1r1
X
jD0
aij
ˇ1 C r1 C i
M C n1 C n2
;
(6.5.4)
OF .t2/ D
r2
X
iD0
n1r1
X
jD0
bij
ˇ1 C ˇ2 C n1 C r2  j
M C n1 C n2
;
(6.5.5)
and for other values of t, OF .t/ is obtained by the linear interpolation. Here
aij D bij=
r2
X
iD0
n1r1
X
jD0
bij and
(6.5.6)
bij D
 
n1  r1
j
! 
r2
i
!
  .ˇ1 C r1 C i/  .ˇ2 C n1  r1 C r2  i  j/  .ˇ3 C n2  r2 C j/
 .ˇ1/  .ˇ2/  .ˇ3/
;
(6.5.7)
with ˇ1 D ˛.0; t1, ˇ2 D ˛.0; t2, and ˇ3 D ˛.t2; 1/. For the general case, the
expressions are complicated and involve multiple integrals.
Bhattacharya (1981) develops procedures to compute ﬁnite dimensional distribu-
tions of the posterior distribution of a Dirichlet prior. Taking a lead from Ferguson
and Phadia (1979), Ammann (1984) writes F .t/ D 1  exp .H .t// and assumes
H to be a process with independent increments with no deterministic component.
He then derives the posterior distribution of H.t/ in terms of Laplace transforms.
However, the expressions are no simpler.
In view of these difﬁculties, Kuo (1988) proposed a linear Bayes estimate of
F which is a Bayes rule in the space generated by r1; : : : ; rL and 1. She derives the
estimator by point-wise minimization of the loss function
R
.F
^
F/2dW at each dose
level. At any point t which is not a dose level, the estimate is deﬁned by the linear
interpolation of estimates at the two adjacent dose levels. Her result is as follows.
Let cov (r/ denote the covariance matrix of r1; : : : ; rL, and let D

i; tj

denote the covariance matrix with the ith column replaced by the column

cov

r1; F

tj

; : : : ; cov

rL; F

tj
T. Also, let M D ˛Œ0; 1/, F0 .t/ D ˛ .t/ =M
and C be a class of decision rules which are linear combinations of r1; : : : ; rL and 1.
Then with F Ï D .˛/, the Bayes rule in this class at each dose level tj, j D 1; 2 : : : ; L
is given by
OF

tj

D F0

tj

C
L
X
iD1
ni
^
i. j/Œri=ni  F0 .ti/;
(6.5.8)

6.5
Other Applications
243
and at t; tj < t < tjC1
OF .t/ D F0

tjC1

 F0 .t/
F0

tjC1

 F0

tj
 OF

tj

C
F0 .t/  F0

tj

F0

tjC1

 F0

tj
 OF

tjC1

;
(6.5.9)
where
^
i . j/ D jD i; tj
 j=jcov (r/j:
Kuo also shows that OF

tj

is an asymptotically unbiased and consistent estimator
of F

tj

. As M ! 0, OF

tj

! rj=nj and as M ! 1; OF

tj

! F0

tj

. She points out
that at times the estimator may not be monotone and if monotonicity is essential,
one can use the pool-adjacent-violators algorithm (Barlow 1972, pp. 13–18) for
obtaining the desired result.
In the case of M and F0 unknown, empirical Bayes method of Sect. 6.2.4 may be
used.
6.5.3
A Regression Problem
In the bioassay problem, the objective was to estimate the dose–response curve.
Antoniak (1974) points out that a similar problem that arises in regression problems
can also be handled in the same way. Let G be a distribution function on Œ0; 1 and
assume that G Ï D .˛/. At chosen points 0 D t0 < t1 <  < tk  1, assume that we
have samples Xl D .Xl1; : : : ; Xlml/ from F.xjG .tl//, l D 1; 2; : : : ; k, and based on
these samples, our aim is to make inferences about the parameters G .tl/. Since G has
a Dirichlet process prior, the joint distribution of (G(t1/; G.t2/G.t1/; : : : ; 1G.tk//
is a Dirichlet distribution with parameters .˛.t1/; ˛.t2/  ˛.t1/; : : : ; ˛ .1/  ˛.tk//.
He points out that the observations for different values of l will not be generally
independent and thus the calculations become complex. He illustrates them by
taking an example with k D 2. Note that in the bioassay problem, the observations
available at each value of l were from a binomial distribution, where as in the
regression problem, they arise from some known distribution.
Consider the general linear model Z D X˛C, where X is a vector of covariates,
ˇ is a vector of regression coefﬁcients, and  is the error term. Traditionally the
practice is to assume the error term to be distributed as a parametric distribution,
typically normal distribution with mean zero. The nonparametric Bayesian approach
is to assume the error term having an unknown distribution, and a prior is placed
on the unknown distribution centered around a base distribution which may be
taken as normal with mean zero. There are several papers along this line using
different priors. Since the base of a Polya tree prior includes absolutely continuous
distributions, it is found to be favorable over the Dirichlet process.
As stated in Sect. 5.2, Lavine (1994) considers the model Yi D ' .Xi; ˇ/ C i,
where ' is a known function, Xi is a known vector of covariates, ˇ is an unknown
vector of regression parameters with prior density f, and the i are independent with

244
6
Inference Based on Complete Data
unknown distribution P. Assuming Pjˇ Ï PT.…ˇ; Aˇ/, he derives the posterior
distribution of ˇ and shows that the posterior distribution of Pjˇ is PT.…ˇ; AˇjY1
' .X1; ˇ/ ; : : : ; Yn  ' .Xn; ˇ//.
Walker and Mallick (1997b) use a ﬁnite Polya tree prior for the error distribution
in a hierarchical generalized linear model centered around a known base probability
measure, whereas Hanson and Johnson (2002) recommend modeling the error
distribution as a mixture of Polya trees. These approaches were mentioned in
Sect. 5.2.
6.5.4
Estimation of a Density Function
The nonparametric Bayesian density function estimation may be viewed as an
application of the mixtures of Dirichlet processes.
Let X1; : : : ; Xn be a sample of size n from a density function f.x/ with respect
to some ﬁnite measure on R. Based on X D .X1; : : : ; Xn/, consider the problem of
estimating f.x/ at some ﬁxed point x, or some functional of f.x/, such as the mean
R x f.x/dx. For the Bayesian treatment, we need to assign a prior on the space of
all density functions and be able to handle the posterior distribution analytically. In
order that the posterior distribution is manageable, it would be preferable to ﬁnd a
conjugate family of priors. This is known to be difﬁcult. Lo (1984) approaches this
problem by using a kernel representation of the density function, and assigning a
Dirichlet prior to its mixing distribution G. His results are presented here.
Let G be a distribution function on R and ˛ a ﬁnite measure on .R; B/. Let
K.x; u/ represent a kernel deﬁned on .XR/ into RC such that for each u 2 R,
R
X K.x; u/dx D 1 and for each x 2 X, R
R K.x; u/˛.du/ < 1. (Lo takes X and
R to be Borel subsets of Euclidean spaces). The posterior distribution of GjX has
been obtained by Antoniak (1974) as indicated earlier. For each G 2 F, deﬁne
f.xjG/ D R
R K.x; u/G.du/, then f.jG/ is a kernel representation of the density
function f and G is known as a mixing distribution. Lo deﬁnes a prior distribution
for random f by letting G to be a random distribution with Dirichlet process prior
D .˛/. This way the broad support for the prior on the space of G is extended
to the broad support for the prior on the space of all density functions. Since
G Ï D.˛/, it can be seen that for each x 2 X, the marginal density of X is
f0.x/ D R
F f.xjG/D˛.dG/ D R
R K.x; u/˛.du/=˛.R/. Now the posterior distribution
of G given the data X can be seen to be
P.G 2 BjX/ D
R
B
Qn
iD1
R
R K.xi; ui/G.dui/D˛.dG/
R
F
Qn
iD1
R
R K.xi; ui/G.dui/D˛.dG/;
(6.5.10)

6.5
Other Applications
245
for all B 2 F. By repeated application of his lemma (interchanging the order of
integration),
Z
F
Z
R
h.u; G/G.du/D˛.dG/ D
Z
R
Z
F
h.u; G/D˛Cıu.dG/˛.du/=˛.R/;
(6.5.11)
he shows that
P.G 2 BjX/ D
R
Rn D˛CP ıui.B/	n;k;˛.du/
R
Rn 	n;k;˛.du/
;
(6.5.12)
where
	n;K;˛.C/ D
Z
C
n
Y
iD1
K.xi; ui/
n
Y
iD1
0
@˛ C
i1
X
jD1
ıuj
1
A .dui/
(6.5.13)
for C 2 Bn, du D Qn
iD1dui and u 2 Rn. For any measurable function g, this leads to
E.g.G/jX/ D
R
Rn g.G/D˛CP ıui.dG/	n;k;˛.du/
R
Rn 	n;k;˛.du/
:
(6.5.14)
Now, by taking g.G/ D f.xjG/ and simplifying, the posterior expectation Of.xjG/
of f.xjG/ is derived as
Of˛.xjG/ D E. f.xjG/jX/ D pnf0.x/ C .1  pn/Ofn.x/;
(6.5.15)
which is a convex combination of prior guess f0.x/ deﬁned above, and a quantity
Ofn.x/, to be deﬁned below, which mirrors the sample distribution function, but is
complicated.
Let k D k ./ denote the number of cells Ci in the partition  of f1; 2; : : : ; mg I
cardinality of the ith cell being mi i D 1; : : : ; kI gi .u/, i D 1; : : : ; m, are m positive
or ˛-integrable functions;
'./ D
kY
iD1
8
<
:.mi  1/Š
Z
R
Y
l2Ci
gl .u/ ˛.du/
9
=
; ;
(6.5.16)
and ﬁnally let w./ D './=P
'./. Then it is shown Ofn.x/ to be
Ofn.x/ D 1
n
X

w./
k
X
iD1
mi
( R
R K.x; u/Q
l2CiK.xl; u/˛.du/
R
R
Q
l2CiK.xl; u/˛.du/
)
;
(6.5.17)

246
6
Inference Based on Complete Data
where the summation is taken over all partitions  of f1; 2; : : : ; mg. Ofn serves as a
Bayes estimate under the loss function L. f; Of/ D
R
j f.xjG/ Of.xjG/j2W.dx/, where
W is a known weight function.
The choice of the kernel K and the parameter ˛ of the prior is critical, Lo gives
several examples of K.x; u/ and ˛ and computes the Bayes estimators. Among them
include kernels of type histogram, normal with location and/or scale parameters,
symmetric and unimodal densities, decreasing densities, etc. If K is chosen to reﬂect
the histogram model, the estimator reduces to the usual Bayes estimates of cell
probabilities. Kuo’s (1986b) Monte Carlo method may be adapted to carry out the
calculations. Lavine (1992) uses mixtures of Polya trees in density estimation.
Ghorai and Susarla (1982) considered an empirical Bayes approach to the above
problem. Assuming ˛ .R/ to be known, they obtained an estimator of f0.x/ D
R
R K.x; u/˛.du/=˛.R/ based on previous n copies and substituted in the Bayesian
estimator Of .xjG/ at the .n C 1/th stage. Under certain conditions, they prove the
asymptotic optimality of the resulting estimator.
A different formulation of the density function was considered by Ferguson
(1983). He modeled it as a countable mixtures of normal densities: f .x/ D
P1
iD1pih .xj	i; i/ where h .xj	; / is the normal density with mean 	 and
variance 2. This formulation has countably inﬁnite number of parameters,
. p1; p2; : : : ; 	1; 	2; : : : ; 1; 2; : : :/. Since the interest is in estimating f .x/ at a
point x, and not in estimating the parameters themselves, it can be written as
f .x/ D R h .xj	; / dG .	; /, where G is the probability measure on the half
plane f.	; / W  > 0g that gives weight pi to the point .	i; i/, i D 1; 2; : : : .
While Lo assumes a Dirichlet process prior for the unknown G, Ferguson deﬁnes
a prior via the Sethuraman representation of G. He deﬁnes the prior distribution
for the parameter vector . p1; p2; : : : ; 	1; 	2; : : : ; 1; 2; : : :/ as follows: vectors
. p1; p2; : : :/ and .	1; 	2; : : : ; 1; 2; : : :/ are independent; p1; p2; : : :. are the weights
with parameter M in Sethuraman representation; and i D .	i; i/ are iid with
common gamma-normal conjugate prior for the two-parameter normal distribution.
This shows that G is a Dirichlet process with parameter ˛
D MG0, where
G0 D E .G/ is the conjugate prior for 	; 2, and its inﬁnite sum representation
is G D P1
iD1piıi where as usual . p1; p2; : : :/ and .1; 2; : : :/ are independent and
i
iidÏ G0. Now given a sample x1; : : : ; xn of size n from a distribution with density
f .x/ D R h .xj/ dG ./, the posterior distribution of G given x1; : : : ; xn has been
obtained by Antoniak (1974) as mixture of Dirichlet processes
Gjx1; : : : ; xn Ï
Z
  
Z
D .˛ C nGn/ dH .1; : : : ; njx1; : : : ; xn/ ;
with nGn D Pn
iD1ıi. H .1; : : : ; njx1; : : : ; xn/ is the posterior distribution of
1; : : : ; n given x1; : : : ; xn. Since E .D .˛ C nGn// D .MG0 C nGn/ = .M C n/,
E .G ./ jx1; : : : ; xn/ D pnG0 ./ C.1 pn/
R
: : :
R
Gn ./ dH .1; : : : ; njx1; : : : ; xn/ ;
(6.5.18)

6.5
Other Applications
247
and
Of .x/ D E . f .x/ jx1; : : : ; xn/ D pn f0.x/ C .1  pn/Ofn.x/;
(6.5.19)
where pn D M= .M C n/ as before,
f0.x/ D E . f .x// D
1
X
iD1
E . pi/ Eh .xj .	i; i// D Eh .xj	; / ;
and Ofn.x/ is given by
Ofn.x/ D 1
n
n
X
iD1
Z
  
Z
h .xji/ dH .1; : : : ; njx1; : : : ; xn/ :
(6.5.20)
Following Lo, Ofn .x/ can be written as a ratio h .x; x1; : : : ; xn/ =h .x1; : : : ; xn/, where
h .x1; : : : ; xn/ D
1
M.n/
Z
  
Z  n
Y
nD1
h .xiji/
!
n
Y
nD1
d
0
@MG0 C
i1
X
jD1
ıj
1
A .i/;
(6.5.21)
and computations are carried out by Kuo’s (1986b) Monte Carlo method.
Normal mixtures also turn up in Escobar (1994) and Escobar and West (1995).
Escobar’s set up is as follows. Let Yij	i Ï N .	i; 1/, 	ijG
iidÏ G, 	i and G are
unknown. In contrast to Ferguson’s and Lo’s objectives, his objective is to estimate
	i’s (with the variance being known to be 1) based on observed Yi’s and using a
nonparametric Bayesian approach. Escobar also uses a DP prior for G. When G is
known the Bayesian estimator is the posterior mean
E .	ijYi/ D
R
	i .Yi  	i/ dG .	i/
R
 .Yi  	i/ dG .	i/ ;
(6.5.22)
where  is the density of the standard normal distribution function. When G is
unknown, empirical Bayes methods are typically used. Instead Escobar uses a DP
prior for G. Antoniak has shown that if the DP prior is used for G, then the posterior
distribution of 	i is a mixture of DPs. Thus it was computationally difﬁcult. Kuo
(1986b) and Lo (1984) developed Monte Carlo integration algorithms, but Escobar
points out that they are inefﬁcient since they do not sample values conditionally
based on the data. Therefore, he introduces new a Gibbs sampler like method that
remedied this problem.
Escobar and West (1995) describe a normal mixture model, similar to Ferguson’s
(1983), in terms of the predictive distribution of a future observation. For their
model, given

	i; 2
i

, we have a random sample, say Y1; : : : ; Yn, such that
Yij

	i; 2
i

Ï N

	i; 2
i

, i D 1; : : : ; n, and the objective is to ﬁnd the predictive

248
6
Inference Based on Complete Data
distribution of the next observation YnC1jY1; : : : ; Yn which is a mixture of normals,
YnC1jY1; : : : ; Yn Ï N

	nC1; 2
nC1

. A usual practice is to put a parametric prior
on vector  D

	1; : : : ; 	n; 2
1 ; : : : ; 2
n

. Ferguson models the common prior for
i D

	i; 2
i

as a DP prior. Thus the data is considered as coming from a Dirichlet
mixture of normals in contrast to Antoniak where the DP processes were mixed
with respect to a parametric distribution H .
/ ; ˛
 Ï H .
/. A particular case of

	i; 2
i

D

	i; 2
has been studied (see West 1992) in which the 	i’s distribution
is modeled as DP with a normal base measure.
In view of the discreteness of DP prior which induces multiplicities of obser-
vations, nC1j1; : : : ; n will have distribution of the form given in property 21
of Sect. 2.1.2. Then they proceed on the line of Ferguson, derive the conditional
distribution of YnC1j1; : : : ; n which is a mixture of a Student’s t-distribution
and n normals N

	i; 2
i

, and then it is shown that the unconditional predictive
distribution is given by YnC1jY1; : : : ; Yn Ï
R
P .YnC1j/ dP .jY1; : : : ; Yn/. Since the
evaluation of P.jY1; : : : ; Yn/ is difﬁcult even in small samples, they use Monte
Carlo approximation using extensions of the iterative technique developed by
Escobar (1994).
6.5.5
Estimation of the Rank of X1 Among X1; : : : ; Xn
Let X1; : : : ; Xn
iidÏ F. The problem of estimating the rank order G of X1 among
X1; : : : ; Xn based on the knowledge of r .< n/ observed values of X1; : : : ; Xr was
considered from a Bayesian point of view by Campbell and Hollander (1978).
WLOG assume that x1; : : : ; xr are the ﬁrst r values in the sample. Let K; L, and
M denote the number of observations among X1; : : : ; Xn that are less than, equal to,
and greater than X1, respectively. Then the rank order G of X1 is taken as the average
value of the ranks that would be assigned to the L values tied at X1, in the ascending
order, i.e., G D 1
L
PL
iD1.K C i/ D K C .L C 1/=2.
Let K
0; L
0 , and M
0 be deﬁned, respectively, as the corresponding numbers of
observations among x1; : : : ; xr. Then the rank order G
0 of x1 among x1; : : : ; xr is
given by G
0 D K
0 C .L
0 C 1/=2. Given x1; : : : ; xr, the problem is to estimate G
which is clearly a function of K; L, and M. Assuming F Ï D.˛/, Campbell and
Hollander obtained the posterior mean,
^
G D E.G jx1; : : : ; xr/ D G
0 C .n  r/f˛
0..1; x1// C 1
2˛
0.fx1g/=˛
0.R/g;
(6.5.23)
where ˛
0 D ˛ C Pr
iD1 ıxi:
^
G depends on x1; : : : ; xr only through G
0 and x1. In
comparison, the non-Bayesian estimators are given by GF D G
0 C .n  r/F.x1/ in
the case of a known continuous function F, and GU D G
0 C.n1/G
0=.nr/, when
F is unknown.

6.6
Bivariate Distribution Function
249
6.6
Bivariate Distribution Function
Ferguson’s (1973) deﬁnition of the Dirichlet process on an arbitrary space of
probability measures makes it amenable for its extension to higher dimensions in
a straight forward manner. In presenting the applications of Dirichlet process in
bivariate situation, we will be concerned with the distribution and survival functions
deﬁned on R2 D R  R and a ﬁnite non-null measure ˛ on (R2; B2/ where B2
represents the -ﬁeld of Borel subsets of R2.
Let P be a random probability measure on (R2; B2/ and F.x; y/ be the corre-
sponding bivariate distribution function. Assume that we have a random sample
.X; Y/ D .X1; Y1/; : : : ; .Xn; Yn/ from F.x; y/. Then the Bayesian estimators are
presented ﬁrst for the distribution function F and then for its functionals.
6.6.1
Estimation of F w.r.t. the Dirichlet Process Prior
For the Bayesian estimation of F.x; y/, we assume that F has a Dirichlet process
prior with parameter ˛. As in the univariate case, we take the weighted loss function
L.F;
^
F/ D
R
R2.F 
^
F/2dW, where W now is a nonnegative weight function on R2.
The Bayesian estimator of F.x; y/ with respect to the Dirichlet process prior and
the loss function L is a direct extension of Ferguson’s Bayesian estimator in one-
dimension, and is given by
^
F˛.x; y/ D ˛..1; x  .1; y/ C Pn
iD1 ı.Xi;Yi/..1; x  .1; y/
˛ .R2/ C n
D pn
˛..1; x  .1; y/
˛ .R2/
C .1  pn/ 1
n
n
X
iD1
ı.Xi;Yi/..1; x  .1; y/:
(6.6.1)
Empirical Bayes estimation of F.x; y/ when ˛ ./ is unknown but ˛ R2 is known
can be carried out as in the univariate case. Also, following Zehnwirth’s (1981) lead,
an estimator for unknown ˛ R2 was developed in Dalal and Phadia (1983) and was
used when ˛ R2 is assumed to be unknown.
6.6.2
Estimation of F w.r.t. a Tailfree Process Prior
In Chap. 5, the tailfree processes were introduced and their properties as well as
the bivariate extension (Phadia 2007) were discussed. Here the Bayes estimator of
F with respect to the bivariate tailfree process prior is derived under the weighted
squared error loss function. If x and y are binary rationals, then the estimate can be

250
6
Inference Based on Complete Data
written as a ﬁnite sum; if either x or y is not a binary rational, then the estimate
involves an inﬁnite sum.
In view of the conjugacy property of tailfree processes, it is sufﬁcient to derive
the estimate for the no-sample problem. Then for a sample of size n, all we have to
do is to update the parameters (see Sect. 5.3). Consider, for example, .x, y/ D . 1
2; 3
4/.
Following the notation of Sect. 5.3,
bF
	1
2; 3
4

D E

F
	1
2; 3
4


D EŒP.B1/ C P.B21/ C P.B23/
D EŒZ1 C Z2Z21 C Z2Z23
D EŒZ1 C Z21 C Z23
D ˛1
1
C ˛2
2
˛21
21
C ˛2
2
˛23
23
;
where cm D ˛cm1 C ˛cm2 C ˛cm3 C ˛cm4 and cm D c1c2: : :cm.
On the other hand, if .x, y/ D . 1
3; 1
2/, say, then
bF
	1
3; 1
2

D E

F
	1
3; 1
2


D EŒP.B11 [ B12/ C P.[2
iD1 [2
jD1 .B13ij [ B14ij/ C : : :
D E
2
4Z11 C Z12 C
2
X
iD1
2
X
jD1
.Z13ij C Z14ij/ C : : :
3
5
D ˛11
11
C ˛12
12
C
2
X
iD1
2
X
jD1
	˛13ij
13ij
C ˛14ij
14ij

C : : : :
Now given a sample X; all we have to do is to update the ˛’s.
6.6.3
Estimation of a Covariance
The covariance of P is deﬁned for .x; y/ 2 R2 by the formula
Cov P =
Z
xydP 
Z
xdP
Z
ydP:
(6.6.2)
Assuming the squared error loss L2 and P Ï D.˛/, Ferguson (1973) derived its
Bayesian estimator. For the no-sample problem we have
E.Cov P/ =
˛.R2/
˛.R2/ C 112;
(6.6.3)

6.6
Bivariate Distribution Function
251
where 12 is the covariance of E.P/ given by 12 D ŒR xyd˛.x; y/  	1	2=˛.R2/,
	1 D R xd˛.x; y/=˛.R2/, and 	2 D R yd˛.x; y/=˛.R2/. Now for the sample of size
n, we update ˛ and obtain the Bayes estimate as
1
CovP˛ D
˛.R2/ C n
˛.R2/ C n C 1. pn12 C .1  pn/s12 C pn.1  pn/.	1  Xn/.	2  Yn//;
(6.6.4)
where s12 D n1Pn
iD1.Xi  Xn/.Yi  Yn/ is the sample covariance. This is again a
mixture of three relevant quantities.
6.6.4
Estimation of the Concordance Coefﬁcient
The problem of estimation of concordance coefﬁcient in a bivariate distribution
was treated in Dalal and Phadia (1983). Let .X; Y/ and .X
0; Y
0/ be two independent
observations from a joint distribution function F.x; y/: A quantity of interest is  D
Pf.XX
0/.Y Y
0/ > 0g, which is related to Kendall’s  D E.sign/.XX
0/.Y Y
0/,
by the equation  D 2  1. It is used as a measure of the dependence between X
and Y as well as a measure of the degree of concordance among observations from
F.x; y/. Let
T1 D f

x; y; x0; y0
W

x  x0
.y  y0/ > 0g and
T2 D f

x; y; x0; y0
W

x  x0
.y  y0/ D 0g:
(6.6.5)
Since F is allowed to be discrete, a slight modiﬁcation of , namely,
 D PFf.X  X
0/.Y  Y
0/ > 0g C 1
2  PFf.X  X
0/.Y  Y
0/ D 0g
(6.6.6)
is preferred. The rationale is that the tied pairs are evenly distributed among
concordants .X  X
0/.Y  Y
0/ > 0 and discordants .X  X
0/.Y  Y
0/ < 0. When
X and Y are independent,  D 0, and its estimator serves as a statistic to test the
hypothesis of independence of X and Y. Now,
 D F D
Z 	
IT1 C 1
2  IT2

d.F.x; y/F.x0; y0//:
(6.6.7)
Assuming F Ï D.˛/, and ˛ deﬁned on (R2; B2/, the Bayes estimator of  for
the no-sample problem is given by
b˛0 D ED.˛/.F/ D
Z 	
IT1 C 1
2  IT2

dED.˛/.F.x; y/F.x0; y0/:
(6.6.8)

252
6
Inference Based on Complete Data
Let ˛ D MQ and let G be a CDF corresponding to the measure Q. Applying
Theorem 4 of Ferguson (1973) in evaluating ED.˛/.F.x; y/F.x0; y0// and simplifying
we get,
b˛0 D
M
M C 1G C
1
2.M C 1/,
(6.6.9)
where G D PGŒ.X  X
0/.Y  Y
0/ > 0 C 1
2PGŒ.X  X
0/.Y  Y
0/ D 0.
When X and Y are independent, G D 1
2, and therefore, b˛0 D 1
2 also.
Now for the case of n observations, .X1; Y1/; : : : ; .Xn; Yn/  F.x; y/, the posterior
distribution of F given the data is again a Dirichlet process with the parameter ˛
updated as ˛ C Pn
iD1 ı.Xi;Yi/, which can be rewritten as
˛ C
n
X
iD1
ı.Xi;Yi/ D .M C n/
"
M
M C nQ C
1
M C n
n
X
iD1
ı.Xi;Yi/
#
D .M C n/Q, say.
(6.6.10)
If G is a CDF corresponding to Q, then G D pnG C .1  pn/bGn, where
bGn is the empirical CDF based on the n observations .xi; yi/; i D 1; 2; : : : ; n, and
pn D M= .M C n/. Hence the Bayesian estimator is given by,
b˛n D
M C n
M C n C 1
Z 	
IT1 C 1
2  IT2

d. pnG C .1  pn/bGn/
 . pnG C .1  pn/bGn/ C
1
2.M C n C 1/
D
M C n
M C n C 1Œp2
nG C 2pn.1  pn/.G; bGn/ C .1  pn/2b
Gn
C
1
2.M C n C 1/,
(6.6.11)
where
b
Gn D 1
n2
n
X
i;jD1
	
IŒ.xx0/.yy0/>0 C 1
2IŒ.xx0/.yy0/D0

;
(6.6.12)
and .G; bGn/ D 1
n
Pn
iD1 G.xi; yi/ with
G.xi; yi/ D

PGŒ.X  xi/.Y  yi/ > 0 C 1
2PGŒ.X  xi/.Y  yi/ D 0

:
(6.6.13)

6.7
Estimation of a Function of P
253
Here G and b
Gn can be interpreted as the natural estimates of the coefﬁcient
of concordance for the idealized model, and the sample and a single observation,
respectively; whereas G.xi; yi/ is the theoretical concordance probability of the
pair .xi; yi/.
The authors evaluated explicitly the Bayesian estimator for two interesting mod-
els, namely the bivariate normal and Gumbel’s bivariate exponential distribution.
They extended the above result to the empirical Bayes estimate of  with M
known, and used Zehnwirth’s (1981) technique to estimate M, when M is unknown.
In both cases, they showed that the estimates are asymptotically optimal with rate
of convergence 0.n1/.
6.7
Estimation of a Function of P
The examples of Sects. 6.4 and 6.6 can be generalized to any measurable function
.P/ of P. Let Xk denote the product space. A real valued function ' W … ! R
is said to be estimable with kernel h if there exists a statistics h .X1; : : : ; Xk/ such
that .P/ D
R
Xkh .x1; : : : ; xk/ Qk
iD1dP .xi/. The degree of an estimable parameter
.P/ is the least sample size for which there is such an h (Zacks 1971, p. 151).
The Bayes and empirical Bayes estimation of estimable parameters of degree 1
and 2 under the loss function L2 and with respect to the Dirichlet and Dirichlet
Invariant processes as priors were investigated by Yamato (1977a,b), Tiwari (1981)
and Tiwari and Zalkikar (1985). Their results are as follows.
6.7.1
Dirichlet Process Prior
Based on a random sample X from P, the Bayesian estimator O of  under L2 loss
is given by the posterior mean E ..P/ j X1; : : : ; Xn/. In particular, suppose .P/ D
h.P/ and P Ï D .˛/, where
h.P/ D
Z
Xk h.x1; : : : ; xk/dP.x1/    dP.xk/;
(6.7.1)
and h is a symmetric measurable function from k into R satisfying
Z
Xk jh.x1; : : : ; xk/jd˛.x1/    d˛.xm/ < 1;
(6.7.2)

254
6
Inference Based on Complete Data
where as before, ˛./ D ˛ ./ =˛ .R/. Under a further assumption concerning the
second moment of h with respect to ˛m, m  k, namely
Z
Xm jh.x1; : : : ; x1; x2; : : : ; x2; : : : ; xm; : : : ; xm/ j2 d˛.x1/d˛.xm/ < 1;
(6.7.3)
for all possible combinations of arguments .x1; : : : ; x1; x2; : : : ; x2; : : : ; xm; : : : ; xm/,
m  k, from all distinct .m D k/ to all identical .m D 1/, the Bayes estimator of
h.P/ with respect to D.˛/ for the no-sample problem is O0
h;˛ D ED.˛/ .h.P//, and
for the sample X1; : : : ; Xn it is
On
h;˛ D ED.˛CPn
iD1 ıxi/; .h.P// D O0
h;˛CPn
iD1 ıXi:
(6.7.4)
Thus using this expression and property 9 of Sect. 2.1.2, Yamato (1977a,b) and
Tiwari (1981) derived the following result. Based on a sample X1; : : : ; Xn, the Bayes
estimator of h.P/ with respect to the prior D.˛/ and loss L2 is given by
On
h;˛ D
X
C.P imiDk/
kŠŒ˛ .X/ C n†mi
…k
iD1Œimi .mi/ŠŒ˛ .X/ C n.k/

Z
X†mi
h.x11; : : : ; x1m1; x21; : : : ; x2m2; : : : ; xk1; ::; xkmk/…k
iD1…mi
jD1dbF˛.xij/;
(6.7.5)
where bF˛./ D pn˛ ./ C .1  pn/bFn ./ is the Bayes estimator of F corresponding to
P. Sethuraman and Tiwari (1982) showed that On
h;˛ ! Oh; Pn
iD1 ıxi as ˛ .X/ ! 0.
Also, if h.x1; : : : ; xk/ is such that it vanishes whenever two coordinates are equal,
then
Oh; Pn
iD1 ıxi D n.n  1/    .n  k C 1/
n.k/
Uh;n ;
(6.7.6)
where Uh;n is the usual U-statistic based on the sample X1; : : : ; Xn. Yamato (1977b)
has proved that the asymptotic distribution of O0
h; Pn
iD1 ıxi is the same as that of Uh;n .
Using the above result and based on a sample X, the Bayes estimators with
respect to D.˛/ of estimable functions of degree 1 and 2, namely 1 .P/ D
R
h.x/dP.x/ and 2 .P/ D
R
h.x; y/dP.x/dP.y/ are obtained in Tiwari and Zalkikar
(1985, 1991a) as
O1 .P/ D pn
Z
h.x/d˛.x/ C .1  pn/
n
n
X
iD1
h.Xi/
(6.7.7)

6.7
Estimation of a Function of P
255
and
O2 .P/ D
M C n
M C n C 1

p2
n
Z
h.x; y/d˛.x/d˛.y/
C2pn.1  pn/
n
n
X
iD1
Z
h.x; xi/d˛.x/ C .1  pn/2
n2
X
i¤j
h.Xi; Xj/
9
=
; .
(6.7.8)
From these two expressions, the Bayes estimators of parameters such as the
mean, variance, covariance and the probability that X is stochastically smaller than
Y can be derived. Explicit expressions were given earlier.
Tiwari and Zalkikar also extended Dalal and Phadia’s (1983) result for the
Bayes and empirical Bayes estimators of the concordance coefﬁcient to a general
parameter of degree 2, namely
& D
Z
h.x; yI x0; y0/dP.x; y/dP.x0; y0/;
(6.7.9)
where h.x; yI x
0; y
0/ is a real valued function deﬁned on .R4; B4/, where B4 stands
for the corresponding Borel sets of R4. The Bayes estimator of & with respect to the
Dirichlet process prior deﬁned on .R2; B2/ is given by
O&˛ D
M C m
M C m C 1Œp2
m&˛ C 2pm.1  pm/&.˛; Fm/ C .1  pm/2&Fm
C
1
M C m C 1
(
pm
Z
h.x; yI x; y/d˛.x; y/ C .1  pm/
m
m
X
iD1
h.Xi; YiI Xi; Yi/
)
;
(6.7.10)
where &˛ D
R
h.x; yI x0; y0/d˛.x; y/d˛.x0; y0/, &Fm D
1
m2
Pm
iD1
Pm
jD1 h.Xi; YiI Xj; Yj/,
&.˛; Fm/ D 1
m
Pm
iD1 &˛ .Xi; Yi/, and &˛ .x; y/ D
R
h.x; yI x0; y0/d˛.x0; y0/.
Note that by taking h.x; yI x0; y0/ D IT1 C 1
2IT2 where
T1 D f.x; yI x0; y0/ W .x  x0/.y  y0/ > 0g
(6.7.11)
and
T2 D f.x; yI x0; y0/ W .x  x0/.y  y0/ D 0g;
(6.7.12)
the results of Dalal and Phadia (1983) can be obtained.

256
6
Inference Based on Complete Data
6.7.2
Dirichlet Invariant Process Prior
Yamato (1986, 1987) carried out similar estimation procedures using the Dirichlet
Invariant process with parameter ˛ and under the same loss L2. Let ˛ D MQ and
M D ˛ .X/ and assume the same ﬁnite group G D fg1; : : : ; gkg of transformations
as used in Dalal (1979a).
In particular, if we take G Dfe; gg with e.x/ D x; g.x/ D 2  x, for x 2 R and
 a constant, and h.x/ D IŒx  t, then F D P..1; t/ and its Bayes estimate
yields
OF
˛ .t/ D pnF0.t/ C .1  pn/ OF
n .t/;
(6.7.13)
where, F0.t/ D Q..1; t/ and OF
n .t/ is -symmetrized version of the empirical
distribution,
OF
n .t/ D 1
2n
n
X
iD1
ıXi..1; t/ C ı2Xi..1; t/:
(6.7.14)
OF
˛ is identical to the one obtained by Dalal (1979a).
He (Yamato 1987) also using the alternative deﬁnition of the Dirichlet Invariant
process generalizes the above treatment to an arbitrary degree s of estimable
parameters in one sample case. As an example of this result, the Bayes estimate
of 1 under L2 loss is obtained as
O
1˛ D pn
Z
h.x/dQ.x/ C .1  pn/
nk
n
X
iD1
k
X
jD1
h.gjXi/;
(6.7.15)
wherein 1
nk
Pn
iD1
Pk
jD1 h.gjXi/ is the G-invariant U-statistic based on kernel h.
Similarly, the Bayesian estimator for an estimable parameter of degree 2, '2 is
obtained. Assume that
Z
X2h.x; y/dQ.x/dQ.y/ < 1;
Z
X
h.x; gx/dQ.x/ < 1 for any g 2 G;
(6.7.16)

6.7
Estimation of a Function of P
257
and let X1; : : : ; Xn be a sample from P, P Ï DGI.˛/. Then the Bayes estimate of
'2 under L2 loss is (Yamato 1986, 1987)
O
2˛ D
M C n
M C n C 1
2
4p2
n
Z
X2h.x; y/dQ.x/dQ.y/ C 2pn.1  pn/
nk
n
X
iD1
k
X
jD1

Z
X
h.x; gjXi/dQ.x/ C .1  pn/2
n2k2
X
i1;i2
X
j1;j2
h.gj1Xi1; gj2Xi2/
3
5
C
1
k.M C n C 1/
2
4pn
k
X
jD1
Z
X
h.x; gjx/dQ.x/ C .1  pn/
nk
X
i
X
j1;j2
h.gj1Xi; gj2Xi/
3
5 :
(6.7.17)
If we let M go to zero, the above estimator reduces to
O
2
D
1
n.n C 1/k2
X
j1;j2
"X
i1;i2
h.gj1Xi1; gj2Xi2/ C
X
i
h.gj1Xi; gj2Xi/
#
;
(6.7.18)
and if we replace Dirichlet Invariant with Dirichlet process, clearly the estimator
reduces to
^
'
2D D
1
n.n C 1/
"X
i1;i2
h.Xi1; Xi2/ C
X
i
h.Xi; Xi/
#
:
(6.7.19)
For illustrative purposes, Yamato takes several different forms of h.x; y/ and
derives the Bayes estimates of the resulting parameters. For example, if we take
h.x; y/ D jxyj and XDR, then 
 D
R
R2jxyjdP.x/dP.y/ is the coefﬁcient of mean
difference of the distribution P. On the other hand, if h.x; y/ D .x1  y1/.x2  y2/=2
with x D .x1; x2/ and y D .y1; y2/, then

 D
Z
R2x1x2dP.x1; x2/ 
Z
R
x1dP.x1; x2/
Z
R
x2dP.x1; x2/
(6.7.20)
is the covariance of the distribution P.
In another example, he takes h.x; y/ D  ...x1  y1/.x2  y2// with x D .x1; x2/
and y D .y1; y2/, and  D IŒt > 0. Then 
 D 2Pf.X1  X2 /.Y1  Y2/ > 0g  1
is a measure of the correlation between .X1; Y1/ and .X2; Y2/ or concordance.
Taking G Dfe; gg with e.x1; x2/ D .x1; x2/; g.x1; x2/ D .x2; x1/, for .x1; x2/ 2
R2, he derives the Bayes estimate. When M ! 0, this estimator reduces to the

258
6
Inference Based on Complete Data
non-Bayesian estimator (Randles and Wolf 1979), namely
^

 D
1
n.n C 1/
2
4
# of fpairs .i; j/ W .Xi  Xj /.Yi  Yj/ > 0, 1  i < j  ng
C# of fpairs .i; j/ W .Xi  Yj /.Yi  Xj/ > 0, 1  i < j  ng
C#fi W Xi D Yi ; 1  i  ng  n:
3
5
(6.7.21)
6.7.3
Empirical Bayes Estimation of .P/
Earlier empirical Bayes estimation results derived for F.t/ by Korwar and Hollander
(1976), Hollander and Korwar (1976), and for P.X  Y/ by Phadia and Susarla
(1979) under D .˛/ prior were reported. Tiwari and Zalkikar (1985, 1991a) gener-
alize these results by replacing the indicator function of the sets .1; x and ŒX 
Y by arbitrary measurable functions h.x/ and h.x; y/. Speciﬁcally, the empirical
Bayes estimation of estimable parameters of degree one and two of an unknown
probability measure on .R; B/ is treated, and asymptotically optimal results with
rate of convergence 0.n1/ of these estimators were established. In proving these
results they used the Sethuraman (1994) representation for the Dirichlet process.
The Bayesian estimator of 1 based on a sample XnC1 of size m at the .n C 1/th
stage was obtained earlier as
O1˛ D pm0 C .1  pm/UnC1,
(6.7.22)
where 0 D
R
hd ˛; pm D M=.M C nm/ and UnC1 D 1
m
Pm
jD1 h.XnC1;j/.
To estimate 1 at the .n C1/th stage on the basis of .X1; : : : ; XnC1/, we may use
the techniques of Sect. 6.2.4 to estimate ﬁrst 0 from the previous n copies and M by
Zehnwirth’s approach. Substituting these estimates, the empirical Bayes estimator
of 1 at the .n C 1/th stage is given by
O˛1;nC1 D Opm
n
X
iD1
Ui
n C .1  Opm/UnC1;
(6.7.23)
where, for the samples Xi; i D 1; 2; : : : ; n, Ui D 1
m
Pm
jD1 h.Xij/; Opm D b
Mn=.b
Mn C
m/; b
Mn D max.0; m.±n  1/1/, and ±n is the F-ratio statistics in one-way ANOVA
table based on the observations X1; : : : ; Xn. For this estimator, the asymptotic
optimality relative to ˛ with rate of convergence O.n1/ is also established.
Similarly they consider the empirical Bayes estimation of 2 with h.x; y/ D 0
whenever x D y. If M is known, the empirical Bayes estimator of 2 at the .nC1/th

6.8
Two-Sample Problems
259
stage is
O˛2;nC1 .P/ D
M C m
M C m C 1
"
p2
m
M C 1
M
n
X
iD1
U2i
n C 2pm.1  pm/
n
X
kD1
n
X
iD1
UnC1;i;k
mn
C.1  pm/2
X
1 j¤km
h.XnC1;j; XnC1;k/
m2
3
5 ;
(6.7.24)
where for the ith sample, Xi D .Xi;1; : : : ; Xi;m/; i D 1; 2; : : : ; n and,
U2i D
1
m.m  1/
X
1 j¤km
h.Xi;j; Xi;k/,
(6.7.25)
UnC1;i;k D 1
m
m
X
jD1
h.XnC1;k; Xi;j/; i D 1; 2; : : : ; n.
(6.7.26)
Under the assumption that R h2.x; y/d˛.x/d˛.y/ exists and is ﬁnite, it is shown
that the sequence f O2;nC1g is asymptotically optimal relative to ˛ with the rate of
convergence O.n1/ .
The empirical Bayes estimation of 2.P/ is also considered in Ghosh (1985) and
exact Bayes risk for Bayes and empirical Bayes estimators are computed. It is shown
that Dalal and Phadia (1983) result for the estimation of concordance coefﬁcient can
be obtained as a special case of his result.
Again for the case when M is unknown, Tiwari and Zalkikar (1985, 1991a)
use Zehnwirth’s estimate for M and established similar result (See their paper for
details) and also obtain the empirical Bayes estimator for & [see (6.7.9)] and proved
its asymptotic optimality with rate of convergence O.n1/.
Remark 6.4 Asymptotic optimality of the empirical Bayes estimators of variance
and the mean deviation about the mean of P can be derived from the above result by
taking h.x; y/ D 1
2.x  y/2 and j x  y j, respectively.
Similar empirical Bayes treatment is also given in Ghosh et al. (1988), where the
main idea was to use past as well as the current data in estimating the parameters of
the Dirichlet process prior. It is shown that by doing this, we get improved estimators
in terms of smaller risks.
6.8
Two-Sample Problems
Suppose we have two independent samples, X1; : : : ; Xn1 from F and Y1; : : : ; Yn2
from G. In this section we consider the Bayesian estimation of certain functionals
of F and G with respect to the Dirichlet priors D.˛1/ and D.˛2/, respectively.

260
6
Inference Based on Complete Data
6.8.1
Estimation of P.X  Y/
Ferguson (1973) derived the Bayesian estimator of  D P.X  Y/ D R FdG under
the squared error loss L2. Let F Ï D.˛1/ and independently, G Ï D.˛2/. Then
for the no-sample problem the estimate of  is given by 0 D E./ D R F0dG0
where F0 D E.F/ and G0 D E.G/, and the expectation is taken with respect to the
Dirichlet priors. Given the samples X1; : : : ; Xn1 Ï F and Y1; : : : ; Yn2 Ï G, we update
the estimate 0 and obtain the Bayesian estimate as O D R bF˛1dbG˛2, where bF˛1 and
bG˛2 are Bayes estimators of F and G, respectively, as obtained in Equation (6.2.1).
Simplifying further we get
b˛1˛2.X; Y/ D p1n1p2n20 C p1n1.1  p2n2/ 1
n2
n2
X
iD1
F0.Yi/
C .1  p1n1/p2n2
1
n1
n1
X
iD1
G0.Xi/ C .1  p1n1/.1  p2n2/ 1
n1n2
U;
(6.8.1)
where p1n1
D
˛1.R/= .˛1.R/ C n1/; p2n2
D ˛2.R/= .˛2.R/ C n2/, and U D
Pn2
jD1
Pn1
iD1 I.1;Yj.Xi/ is the Mann–Whitney statistic. When both ˛1.R/ and ˛2.R/
tend to zero, b˛1˛2 reduces to the usual nonparametric estimate .1=.n1n2//U.
Hollander and Korwar (1976) extend this estimator to the empirical Bayes
estimator. Assume that ˛1 and ˛2 are unknown except for ˛1.R/ and ˛2.R/ which are
speciﬁed, and that we have n copies of data available from the ﬁrst n stages and are
required to estimate  at the .n C 1/th stage. As in one sample case, they estimate
˛1 and ˛2 from the ﬁrst n-stage data Xi D .Xi1; : : : ; Xin1/ and Yi D .Yi1; : : : ; Yin2/
for i D 1; 2; : : : ; n and propose the following estimator:
b˛1˛2n.X; Y/ D p1n1p2n2
1
n2n1n2
n
X
jD1
n2
X
kD1
n
X
iD1
n1
X
lD1
I.1;Yjk.Xil/
C p1n1.1  p2n2/
1
nn1n2
n2
X
kD1
n
X
iD1
n1
X
lD1
I.1;YnC1k.Xil/
C .1  p1n1/p2n2
1
n1
n1
X
lD1
8
<
:1  1
nn2
n
X
jD1
n2
X
kD1
I.1;XnC1l.Yjk/
9
=
;
C .1  p1n1/.1  p2n2/ 1
n1n2
n1
X
lD1
n2
X
kD1
I.1;YnC1k.XnC1l/:
(6.8.2)

6.8
Two-Sample Problems
261
Finally, they show that b˛1˛2n is asymptotically optimal with respect to ˛1 and
˛2. Clearly, when ˛1.R/ and ˛2.R/ are also unknown, they could be estimated as
indicated earlier, and the above estimator may be adjusted accordingly.
6.8.2
Estimation of the Difference Between Two CDFs
A measure of the difference between two distributions functions F and G, is
deﬁned by
d.F; G/ D
Z
.F .t/  G .t//2 d
	F .t/ C G .t/
2

;
(6.8.3)
which is somewhat difﬁcult to handle. However, if the distributions are continuous
on R, then it can be written as
d.F; G/ D 4
3 
Z
G .t/ dF2 .t/ C
Z
F .t/ dG2 .t/

:
(6.8.4)
Based on two independent samples, X1; : : : ; Xn1 from F and Y1; : : : ; Yn2 from G,
Yamato (1975) considered the problem of Bayesian estimation of d.F; G/ under the
squared error loss L.d.F; G/, Od.F; G// D (d.F; G/  Od.F; G//2. In order to use the
latter version of the deﬁnition, he deﬁnes linearized Dirichlet process as priors for F
and G which are assumed to be continuous. Following Doksum (1972), he deﬁnes a
linearized Dirichlet process as follows. For reals a < b, consider the partition  of
.a; b/, a D t1 < t2; : : : ; < tk D b and denote the norm of the partition as kk D
max
1ik1jtiC1  tij. Let ˛ be a ﬁnite measure on .R; B/ with support .a; b/. Let H0
be a realization of the Dirichlet process with parameter ˛ such that H0.a/ D 0 and
H0.b/ D 1 with probability one. Given the partition , the joint distribution of the
corresponding increments of the distribution function has a Dirichlet distribution.
With this formulation, he deﬁnes a linearized Dirichlet process as follows:
Deﬁnition 6.5 (Yamato) H is said to be a linearized Dirichlet process with parame-
ter ˛ and partition , when H is linear between the points.t1; H0.t1//; : : : ; .tk; H0.tk//
and H0.ti/; i D 1; 2; : : : ; k are the realization of the Dirichlet process with parameter
˛ having support .a; b/ and partition , with a D t1 and b D tk.
Assume F and G as independent linearized Dirichlet processes with parameters
˛1 and ˛2, respectively, and partition . Then under the squared error loss, the Bayes
estimate is given by the posterior mean,
EŒd.F; G/jX1; : : : ; Xn1 and Y1; : : : ; Yn2:
(6.8.5)

262
6
Inference Based on Complete Data
To evaluate this expectation, he deﬁnes (pseudo Bayesian estimators)
bF˛1n1 .t/ D pn1F0 .t/ C .1  pn1/bFn1 .t/ ;
bG˛2n2 .t/ D pn2G0 .t/ C .1  pn2/ bGn2 .t/ ;
(6.8.6)
on the interval .a; b), with bF˛1n1 .t/ D bG˛2n2 .t/ D 0 for t  a, and bF˛1n1 .t/ D
bG˛2n2 .t/ D 1 for t  b, with probability one, where pn1 D ˛1 .R/ = .˛1 .R/ C n1/,
pn2 D ˛2 .R/ = .˛2 .R/ C n2/, bFn1 and bGn2 are the empirical distribution functions
of the samples X and Y, respectively, F0 .t/ D ˛1 .t/ =˛1 .R/ and G0 .t/ D
˛2 .t/ =˛2 .R/. Denoting by bF˛1n1; and bG˛2n2;, the linearized versions of bF˛1n1 and
bG˛2n2, respectively, on the partition , he evaluates the above expectation obtaining
the Bayesian estimator of d.F; G/ on the interval .a; b) as
Od˛1˛2.F; G/ D 4
3 
˛1 .R/ C n1
˛1 .R/ C n1 C 1
Z b
a
bG˛2n2;.t/dbF2
˛1n1;.t/

1
˛1 .R/ C n1 C 1
 2
3
Z b
a
bG˛2n2;.t/dbF˛1n1;.t/
C1
3
k1
X
1
bG˛2n2.tiC1/ŒbF˛1n1.tiC1/  bF˛1n1.ti/
)
(6.8.7)

˛2 .R/ C n2
˛2 .R/ C n2 C 1
Z b
a
bF˛1n1;.t/dbG2
˛2n2;.t/

1
˛2 .R/ C n2 C 1
 2
3
Z b
a
bF˛1n1;.t/dbG˛2n2;.t/
(6.8.8)
C1
3
k1
X
1
bF˛1n1.tiC1/ŒbG˛2n2.tiC1/  bG˛2n2.ti/
)
:
(6.8.9)
Finally, taking the limit kk ! 0, the above estimator reduces to
Od˛1˛2.F; G/ D 4
3 
1
˛
1 C 1
Z b
a
bG˛2n2.t/dbF˛1n1.t/ C ˛
1
Z b
a
bG˛2n2.t/dbF2
˛1n1.t/


1
˛
2 C 1
Z b
a
bF˛1n1.t/dbG˛2n2.t/ C ˛
2
Z b
a
bF˛1n1.t/dbG2
˛2n2.t/

;
(6.8.10)
where ˛
1 D ˛1 .R/ C n1 and ˛
2 D ˛2 .R/ C n2.
This estimator is derived on the basis of a particular prior distribution with the
interval .a; b/ as its support. In general, when F and G are continuous, the author
proposes an estimator of d.F; G/ as Od.F; G/ with the range of integrals replaced in

6.8
Two-Sample Problems
263
the above formula by 1 to 1. By letting ˛1 .R/ and ˛2 .R/ tend to zero, we get a
non-Bayesian estimator of d.F; G/.
It should be noted that the above formula (6.8.10) for the difference between
two distribution is valid if and only if F and G are continuous. For this reason,
the author used the linearized Dirichlet processes as priors in deriving the Bayes
estimate, and passing through the limit of the Bayes estimate yielded the above
estimator Od˛1˛2.F; G/ (with the integrals R 1
1/. However, the author argues that if
we deﬁne the difference as the above quantity regardless of the distribution functions
being continuous or not, and assign Dirichlet priors to them, the direct computation
will show that the resulting estimate is equal to the above estimate.
6.8.3
Estimation of the Distance Between Two CDFs
When F and G are continuous distribution functions, the horizontal distance
between F and G is deﬁned as  .x/ D G1 .F .x//  x, for a real number x.
Hollander and Korwar (1982) consider a one-sample problem where G is assumed
to be known and only a random sample of size n from F is available to estimate .
Although F is continuous, they assume F Ï D.˛/. Under the loss function L1, the
Bayes estimator for the no-sample problem is found by minimizing the integrand of
E.L.;
^
// D
Z
E..x/ 
^
.x//2dW.x/
(6.8.11)
yielding
^
0.x/ D E..x// D EfG1F.x/g  x. For a sample of size n from F, the
Bayesian estimator is obtained simply by updating ˛.
If G is assumed to be an exponential distribution, G.x/ D 1ex; x > 0;  > 0,
then
^
0 is
^
0.x/ D
1
  B.˛0; ˇ
0/ 
Z 1
0
1
X
jD1
y˛0Cj1.1  y/ˇ01
j
dy  x
D 1
 
1
X
jD1
B.˛0 C j; ˇ0/
j  B.˛0; ˇ0/  x;
(6.8.12)
where ˛0 D ˛..1; x/; ˇ0 D ˛..x; 1// D ˛ .R/  ˛0. Now for a sample of size n
from F the Bayes estimator is the above expression
^
0.x/ with ˛0 and ˇ0 replaced
by ˛ D ˛0 C Pn
iD1 ıXi and ˇ D ˛ .R/ C n  ˛, respectively, their updated
versions.

264
6
Inference Based on Complete Data
6.9
Hypothesis Testing
In applications of the Dirichlet process prior so far, we have discussed mainly
the estimation of an unknown distribution function F or a parameter ' which is
a function of the unknown probability measure P. Ferguson (1973) pointed out
the difﬁculty of using the Dirichlet Process prior in hypothesis testing problems.
However, Susarla and Phadia (1976) were able to show how such problems can be
handled. The idea was to replace the usual 0  1 loss with a smoother loss function
based on a known weight function W. Thus their approach to the problem of the
hypothesis testing was from a decision theoretic point of view—a ﬁrst as far as we
know. Their method can be extended to treat multiple decision theoretic problems
as well. This is described now.
6.9.1
Testing H0 W F  F0
Let X D .X1; : : : ; Xm/ be a random sample from the distribution function F and
let F0 be a known distribution function. Consider the problem of testing hypothesis
H0 W F  F0 against the alternative H1 W F — F0 when the loss function L is given by
L.F; a0/ D
Z
.F  F0/CdW
and
L.F; a1/ D
Z
.F  F0/dW;
(6.9.1)
where L.F; ai/ indicates the loss incurred when action ai (deciding in favor of Hi)
is taken for i D 0; 1; W is a known weight function, aC D maxfa; 0g and a D
 minfa; 0g for any real number a. Assume F Ï D.˛/. Let ı. X/ D Pftaking
action a0 j Xg. Then the Bayes risk of ı against D.˛/ is
rm.ı; ˛/ D
Z
E Œ L.F; a0/  L.F; a1/ j X ı.X/ d Qm.X/ C E Œ L.F; a1/;
(6.9.2)
where Qm is the unconditional distribution of X and the expectation is taken with
respect to D.˛/. Hence a Bayes rule against D.˛/ which minimizes the above risk
is given by ım.X/ D IŒm.X/  0 where m.X/ D
R
E Œ F.u/  F0.u/ j X dW.u/
and the minimum Bayes risk is
r
m.˛/ D
Z
Œm.X/0
m.X/ d Qm.X/ C E Œ L.F; a1/:
(6.9.3)
If ˛ is known, m.X/ can be easily evaluated since for each u, F.u/jX D x Ï
Be.˛.1; u C Pm
iD1 IŒXi  u; ˛.u; 1/ C Pm
iD1 IŒXi > u/.

6.9
Hypothesis Testing
265
When ˛ is unknown, we can use the empirical Bayes method. Let ˛.R/ D 1 and
assume the usual set up for the empirical Bayes estimation with sample size mi at
the ith stage. Then an empirical Bayes rule at the .n C 1/th stage is given by
nC1.XnC1/ D Pfaccepting a0 j X1; : : : ; Xn, XnC1g;
(6.9.4)
Let bn.XnC1/ be an estimate based on (X1; : : : ; Xn) of mnC1.XnC1/
D
R
E Œ FnC1.u/  F0.u/ j XnC1 dW.u/ given by
bn.xnC1/C
Z
F0dW D
Z f O˛.1; uN C PmnC1
iD1 IŒXnC1;i  u gdW.u/
.1 C mnC1/
;
(6.9.5)
where O˛.1; uN D n1 Pn
jD1 m1
j
Pmj
iD1ŒXj;i  u. Let n .xnC1/ D IŒbn.XnC1/ 
0, and let rnC1.n/ denote the risk of using n to decide about FnC1. Then it is proved
that rnC1.n/  r
mnC1.˛/  n 1
2 .
When ˛ .R/ is unknown, they estimate it by a consistent estimator
O˛ .R/ D
.log mn/1f# of distinct observations in Xng (see property 19 of Sect. 2.1.2). Let 
n
be the rule obtained by substituting this estimator in n with
bn.XnC1/ C
Z
F0dW D
Z f O˛.R/ O˛.1; uN C PmnC1
iD1 ŒXnC1;i  u gdW.u/
. O˛.R/ C mnC1/
:
(6.9.6)
When ˛ is nonatomic and mnC1 ! 1 as n ! 1, it is shown that rnC1.
n / 
r
mnC1.˛/ D O..mnC1/1.minflog mn; ng/1=2/.
In addition, they have shown that some of these procedures are component-wise
admissible and have also discussed the extension of their results to the multiple
action problem.
6.9.2
Testing Positive Versus Nonpositive Dependence
In the bivariate distribution case, we come across the problem of testing positive
dependence versus nonpositive dependence. Let F.x; y/ be a bivariate distribution
function deﬁned on (R2; B2) with marginal CDFs FX.x/ and FY.y/, respectively.
The objective is to test the following hypotheses:
H0 W F.x; y/  FX.x/FY.y/ for all .x; y/ in R2
H1 W F.x; y/ < FX.x/FY.y/ for all .x; y/ in R2;
(6.9.7)

266
6
Inference Based on Complete Data
under the loss function
L.F; a0/ D
Z
.F.x; y/  FX.x/FY.y//dW.x; y/
L.F; a1/ D
Z
.F.x; y/  FX.x/FY.y//CdW.x; y/;
(6.9.8)
where the actions a0 and a1 are to accept H0 and H1, respectively, W is a
known weight function on R2. For given observations .x; y/, denote by 
.x; y/ the
probability of taking action a0. Then Dalal and Phadia (1983) have shown that the
Bayes rule against D.˛/ is given by

.x; y/ D IŒn.x;y/;
(6.9.9)
where
n.x; y/ D EŒL.F; a0/  L.F; a1/ j .X; Y/
D
Z
ŒE.F.x0; y0/  FX.x0/FY.y0// j .X; Y/dW.x0; y0/:
(6.9.10)
Here the expectation is taken with respect to the posterior Dirichlet process
with parameter ˛ C Pn
iD1 ı.xi:yi/. Let ˛ D MQ, G0 be a CDF corresponding to
Q, G D pnG0 C .1  pn/bGn, where bGn is the empirical CDF based on the n
observations .xi; yi/; i D 1; 2; : : : ; n, and pn D M= .M C n/. Then the integrand
can be evaluated as
MG0.x0; y0/ C Pn
iD1 ı.xi:yi/..1; x0  .1; y0/
M C n
 G.x0; y0/ C MG
X.x0/G
Y.y0/
M C n C 1
;
(6.9.11)
and hence n.x; y/ can be evaluated. As in the case of estimating the concordance
coefﬁcient above, the empirical Bayes solution can be carried out here as well when
˛ is not known, with M known or unknown.
6.9.2.1
Testing the Hypothesis H0 W F  G Against the Alternative
H1 W F — G
An analog of the test discussed in Sect. 6.9.1 in a two-sample situation is to test
the hypothesis H0 W F  G against the alternative H1 W F — G. This topic is
covered more generally in Sect. 7.6 based on randomly right-censored samples. Its
application to the uncensored data as a special case is obvious and therefore it will
not be presented here.

6.9
Hypothesis Testing
267
6.9.3
A Selection Problem
Consider the following selection problem. We are given k samples, Xi
D
.Xi1; : : : ; Xiki/ distributed according to Fi, i
D
1; 2; : : : ; k, and a sample
Y D .Y1; : : : ; Yn/ known to have come from one of the k distributions. The problem
is to ﬁnd from which one. Antoniak (1974) considered this problem and provided
a Bayes solution. Let X D N to be a set of nonnegative integers and  .N/ be
the corresponding -algebra generated by the singleton sets. Assume that for
i D 1; 2; : : : ; k, Fi Ï D .˛i/. For technical reasons, each ˛i is taken to be a discrete
measure with the same support and deﬁned on  .N/ with ˛i D .˛i0; ˛i1; : : :/ and
˛i .fjg/ D ˛ij, j˛ij D P1
jD0 ˛ij. Let j be the prior probability that the sample
Y1; : : : ; Yn came from Fj, j D 1; 2; : : : ; k. Let L .i; j/ be the associated loss function
in deciding Y as coming from Fi when in fact it is from Fj. The goal is to seek
a nonrandomized decision rule which minimizes the expected loss. First note that
FijXi Ï D ˛
i
, where ˛
i D ˛
i0; ˛
i1; : : :, with ˛
ij D ˛ij C mij, and mij is the
number of Xi’s equal to j, j D 0; 1; : : :. . The Bayes risk ri is given by
ri.; ˛/ D
k
X
jD1
L .i; j/ P . jjY1; : : : ; Yn/ D
k
X
jD1
L .i; j/
jP .Yj j/
Pk
jD1 jP .Yj j/
;
(6.9.12)
where
P .Yj j/ D
1
Y
lD0
˛.kl/
jl
˛.n/
l
;
a.n/ D a.a C 1/    .a C n  1/ ; n > 0;
(6.9.13)
and kl is the number of Y’s equal to l. The Bayes decision rule selects s, where
rs D min ri. For the 0  1 loss and uniform prior j D 1=k, the Bayes decision rule
is to choose s for which P .Yjs/ D maxj P .Yj j/.

Chapter 7
Inference Based on Incomplete Data
7.1
Introduction
Most common form of incomplete data is when the observations are censored
on the right. Therefore, in this chapter we will be dealing mainly with the right
censored data, although estimators based on other sampling schemes will also
be presented. A typical problem in the analysis of right censored data may be
described as follows. We have a random sample X D .X1; : : : ; Xn/ from an unknown
distribution function F deﬁned on RC D .0; 1/, and let Y1; : : : ; Yn be nonnegative
random variables deﬁned on RC also or positive real numbers (to be speciﬁed in
the sequel). We do not observe Xi’s directly but only via Zi D min.Xi; Yi/ and
ıi D IŒXi  Yi; i D 1; 2; : : : ; n. Based on .Z; ı/ D f.Zi; ıi/gn
iD1, we are required
to make various inferences about F or its function, but mainly the survival function
(SF), S.t/ D 1F.t/. No distributional assumptions are made regarding F, and thus
the procedures reported here may be considered as nonparametric. In the context of
survival analysis, Xi’s are known as “uncensored,” “real,” or “exact” observations
while Yi’s are right “censoring” variables. ıi indicates whether the i-th observation
is real or censored.
This problem is encountered in many applications such as industrial, competing
risks, life testing, life tables (Kaplan and Meier 1958), biomedical research,
and survival data analysis (Gross and Clark 1975). However, its application and
usefulness in the analysis of survival data arising in clinical research have received
wide attention. In the non-Bayesian context, the problem was ﬁrst considered by
Kaplan and Meier (1958) who developed two estimators for estimating S. One of
them, known as the product limit (PL) estimator is
bSPL.u/ D NC.u/
n
n
Y
jD1
	NC.Zj/ C j
NC.Zj/

IŒıiD0;Zju
;
(7.1.1)
© Springer International Publishing Switzerland 2016
E.G. Phadia, Prior Processes and Their Applications, Springer Series in Statistics,
DOI 10.1007/978-3-319-32789-1_7
269

270
7
Inference Based on Incomplete Data
with multiplicities j at Zj, j D 1; : : : ; n, and NC.u/ D Pn
iD1IŒZi > u, has been
widely popular and studied extensively. It is also used in solving other problems
encountered in estimation, prediction, hypothesis testing, etc.
We present here the Bayesian approach. Since the parameter of interest here is
the distribution function F itself, it will be considered as random and a prior deﬁned
over the space of all distribution functions, FC will be assigned to F.
Due to its analytical tractability, the Dirichlet process is used extensively as a
prior for F in statistical inference problems presented here. But unlike in the case of
complete data, the posterior distribution given the right censored data is a mixture
of Dirichlet processes. However, when viewed as a neutral to the right process, the
Dirichlet process is structurally conjugate with respect to the right censored data.
As such, it is shown that for the right censored data, neutral to the right processes
are more suitable as priors. They cover the Dirichlet process as well as beta-Stacy
process, among others, and their treatment is considered in more detail and formulas
are presented.
As in the previous chapter, estimation of the distribution (survival) function is of
prime interest and therefore it is considered ﬁrst in Sect. 7.2 assuming the Dirichlet
process prior. Also estimation of the survival function under different censoring
schemes is presented. In Sect. 7.3, the estimation of the survival function based on
other types of priors is considered. In a slight digression, a linear Bayes estimation
of the survival function is presented in Sect. 7.4. Various other estimation problems
are included in Sect. 7.5. Section 7.6 deals with a hypothesis testing problem and
ﬁnally, estimation of the survival function incorporating covariates is presented in
the last section (Sect. 7.7).
7.2
Estimation of an SF Based on DP Priors
In this section the pioneering work of Susarla and Van Ryzin (1976) in deriving the
Bayesian estimator of a survival function with respect to the Dirichlet process is
presented. Empirical Bayes and Bayesian estimators under various other sampling
schemes are also considered. Interestingly they all have similar forms and are
generalization of the non-Bayesian PL estimator.
7.2.1
Estimation Based on Right Censored Data
Based on the data .Z; ı/, we shall obtain in this section the Bayesian estimation of
the survival function S.t/, under the loss function L1 used earlier
L1.S;bS/ D
Z 1
0
.S.t/ bS .t//2W.t/;
(7.2.1)

7.2
Estimation of an SF Based on DP Priors
271
where W./ is a known weight function on RC. It is assumed that X1; : : : ; Xn are
iid F, and Y1; : : : ; Yn are independent with Yi distributed as Gi, i D 1; 2; : : : ; n.
Assume also that Y1; : : : ; Yn are independent of (F, X1; : : : ; Xn ). Susarla and Van
Ryzin’s strategy was to take care of real observations ﬁrst by noting that the posterior
distribution of F given these observations is again the Dirichlet process with updated
parameter of the DP. Then the censored observations were dealt with the updated
parameter.
Observe that .Z; ı/ are invariant for any permutation of observed pairs .ı1,
Z1/; : : : ; .ın, Zn/. Hence without loss of generality we can (and we shall) rearrange
these observations as .1, Z1/; : : : ; .1, Zk/; .0, ZkC1/; : : : ; .0, Zn/. Thus Z1; : : :,
Zk are uncensored observations and ZkC1; : : :, Zn are censored observations. The
uncensored observations are taken care of by replacing the parameter ˛ by ˛k D
˛ C Pk
iD1 ıZi. Among the censored observations, let Z.kC1/; : : :, Z.m/ denote the
distinct ordered observations with multiplicities j at Z. j/, j D k C 1; : : : ; m, so
that Pm
jDkC1j D n  k. Then, the Bayes estimator of S.u/ given the data is the
conditional expectation of S.u/ given .0, ZkC1/; : : : ; .0, Zn/, where the expectation
is now performed with respect to the posterior distribution of F given .1, Z1/; : : : ; .1,
Zk/ which is D.˛k/. Thus, we have
bS˛.u/ D ED.˛k/fS.u/ j .0; ZkC1/; : : : ; .0; Zn/g.
(7.2.2)
After establishing several intermediate steps for the moments of the conditional
distribution of S.u/, Susarla and Van Ryzin derived the following Bayes estimator
of the survival function S.u/ for u in the interval Z.l/  u < Z.lC1/; l D k; : : : ; m,
with Z.k/ D 0 and Z.mC1/ D 1:
bS˛.u/ D ˛.u; 1/ C NC.u/
˛.RC/ C n
lY
jDkC1
˛ŒZ. j/; 1/ C NC.Z. j// C j
˛ŒZ. j/; 1/ C NC.Z. j//
,
(7.2.3)
where NC.u/ is the number of observations greater than u. Alternatively, it can be
written as
bS˛.u/ D ˛.u; 1/ C NC.u/
˛.RC/ C n
n
Y
jD1
	˛ŒZj; 1/ C NC.Zj/ C j
˛ŒZj; 1/ C NC.Zj/

IŒıiD0;Zju
:
(7.2.4)
Several observations are in order here.
Remarks
1. This estimator is also in the product form and looks like the PL estimator of
Kaplan and Meier (1958). But unlike the PL estimator it is deﬁned everywhere.
2. Like the PL estimator it has jumps only at the uncensored observations.

272
7
Inference Based on Incomplete Data
3. Unlike the PL estimator, it is not constant between two uncensored observations
but reﬂects the contribution of the prior information. It is a smoother version of
the PL estimator.
4. As ˛.RC/ ! 0, the estimator reduces to the PL estimator.
5. If there are no censored observations (i.e., all Gi ’s are degenerate at 1) this
estimator reduces to the Bayes estimator given by Ferguson (1973) restricted
to RC.
For a ﬁxed u, the conditional distribution of F.u/ given .ı; Z/ is a mixture of
beta distributions. Also, for a ﬁxed u such that Z.l/  u < Z.lC1/, with l D k; : : : ; m,
Z.k/ D 0 and Z.mC1/ D 1, the conditional p-th moment of S.u/ given the data is
shown to be
E Œ Sp.u/ j .ı; Z/ D
p1
Y
sD0
 ˛.u; 1/ C s C NC.u/
˛.RC/ C s C n

lY
jDkC1
˛ŒZ. j/; 1/ C s C NC.Z. j// C j
˛ŒZ. j/; 1/ C s C NC.Z. j//

;
(7.2.5)
where the inside product is treated as one if u < Z.kC1/.
This gives a clue that the conditional distribution of F given .ı; Z/ is a mixture
of Dirichlet processes, as indicated by Susarla and Van Ryzin (1976).
In fact Blum and Susarla (1977) conﬁrmed the above conjecture by proving that
the posterior distribution of S, given the censored observations, is indeed a mixture
of Dirichlet processes and indicated the transition and mixing measures.
Theorem 7.1 (Blum and Susarla) Let Gj be absolutely continuous or discrete
distribution for j D 1; : : : ; n and let Y1; : : : ; Yn be independent of (F, X1; : : : ; Xn ).
Then the posterior distribution of P given .Z; ı/ is a mixture of Dirichlet processes
with transition measure ˇ ./ C Pk1
jD1	j ./ C I .u/ and mixing measure 	k, where
ˇ .B/ D ˛ .B/ C Pnk
jD1IB

Zj

, ˇ .ŒZnkC1; 1//	1 .B/ D ˇ .B \ ŒZnkC1; 1// and
	l .B/ D ˇ .B \ ŒZnkCl; ZnkCl1//
ˇ .ŒZnkC1; 1// C l  1

l1
X
jD1
ˇ

B \ ŒZnkCj; ZnkCj1/

ˇ .ŒZnkC1; 1// C j  1
l1
Y
iDj
ˇ .ŒZnkCi; 1// C i
ˇ .ŒZnkCiC1; 1// C i
(7.2.6)
for l D 2; : : : ; k.

7.2
Estimation of an SF Based on DP Priors
273
If we assume that Gi’s are identical to G, that is, Yi Ï G; i D 1; 2; : : : ; n, and
that G is a ﬁxed unknown continuous distribution on RC, then in equation (7.2.4)
j D 1 for all j and the Bayes estimatorbS˛ becomes
bS˛.u/ D ˛.u; 1/ C NC.u/
˛.RC/ C n
n
Y
jD1
	˛ŒZj; 1/ C NC.Zj/ C 1
˛ŒZj; 1/ C NC.Zj/

IŒıiD0;Zju
:
(7.2.7)
This representation of bS˛ is used in proving asymptotic properties of the Bayes
estimator. It is shown (Susarla and Van Ryzin 1978b,c) that bS˛ is almost surely
consistent with a convergencerate of O

log n=n
1
2

, and convergesweakly to a mean
zero Gaussian process. They also give an expression for the covariance matrix.
In the above formulation, Xi and Yi are assumed to be independent. If they are
allowed to be dependent, the marginal distribution of X may not be identiﬁable.
Nevertheless, a Bayesian treatment of the problem is possible and has been carried
out by Phadia and Susarla (1983) by assuming a Dirichlet process prior for the joint
distribution of .X; Y/. This will be further discussed in Sect. 7.5.3.
7.2.2
Empirical Bayes Estimation
Susarla and Van Ryzin (1978a) also considered the empirical Bayes approach in
estimating the survival function. For simplicity we consider the case of sample size
one only. The general case is straightforward. Thus, at each of .n C 1/stages
we have one observation and ˛ is unknown, but ˛.RC/ is known. As in the
earlier sections for complete data, ˛ is estimated from n previous stages by O˛n and
substituted in the Bayes estimator at the .n C 1/ stage. Thus under the weighted
squared error loss function, the empirical Bayes estimator of SnC1.u/ is given by
bSnC1.u/ D
(
.1 C O˛n.u; 1//
for u < ZnC1
 O˛n.u; 1/

O˛n.ZnC1;1/CIŒınC1D0
O˛n.ZnC1;1/

for u  ZnC1
;
(7.2.8)
where  D

1 C ˛.RC/
1 and
O˛n.u; 1/
˛.RC/
D
1
nGj.u/
n
X
jD1
IŒZj > u:
(7.2.9)
Under some mild conditions, it is proved that the above estimator is asymptotically
optimal with rate of convergence O.n1/.

274
7
Inference Based on Incomplete Data
However, because of Gj.u/ in the denominator, their estimator was not monotone
in that the estimator was increasing between two censored observations and hence
was not a proper survival function. Phadia (1980) proposed a slightly modiﬁed
estimator which did not have this undesirable property and at the same time, it
was also asymptotically optimal with the same rate of convergence O.n1/. His
estimator is given by the same estimator bSnC1.u/ as above, but the estimator O˛n is
replaced by
O˛n.u; 1/
˛.RC/
D NC.u/
n
n
Y
jD1
	NC.Zj/ C 1 C c
NC.Zj/ C c

ŒıjD0; Zju
;
(7.2.10)
where c is a positive constant. By a suitable choice of c, a desired level of
smoothness in the empirical Bayes estimator may be obtained. It was suggested that
a value of 1=2 to 5 for c to be reasonable, but may depend on some other optimal
criterion.
7.2.3
Estimation Based on a Modiﬁed Censoring Scheme
In extending the results of Susarla and Van Ryzin (1976) to a more general class
of priors, namely, the processes neutral to the right developed by Doksum (1974),
Ferguson and Phadia (1979) considered a modiﬁed sampling scheme in which the
censored observations were classiﬁed as “exclusive” if X > x and “inclusive” if
X  x. Assume that the observational data has three forms, m1 real observations
X1 D x1; : : : ; Xm1 D xm1, m2 “exclusive censoring” Xm1C1 > xm1C1; : : : ; Xm1Cm2 >
xm1Cm2, and m3 “inclusive” censoring Xm1Cm2C1  xm1Cm2C1; : : : ; Xm1Cm2Cm3 
xm1Cm2Cm3, variables where m1 C m2 C m3 D n, the sample size. The former type is
the customary way of deﬁning censoring and is the only type considered in Kaplan
and Meier (1958) and Susarla and Van Ryzin (1976). In addition, Ferguson and
Phadia assumed the censoring points as given constants and not random variables
as assumed by Susarla and Van Ryzin (1976). (Lo 1993a, Lemma 7.1, shows that
the distinction is immaterial as long as the distributions of censored variables are
independent of F) Under this sampling scheme, they derived the posterior mean.
Let u1
<
u2
<
: : :
<
uk be the distinct ordered values among
x1; : : : ; xnI ı1; : : : ; ık
are number of uncensored observations at u1; : : : ; uk,
respectively; 1; : : : ; k denote the number of “exclusive” censoring at u1; : : : ; uk,
respectively; 	1; : : : ; 	k denote the number of “inclusive” censoring at u1; : : : ; uk,
respectively, so that Pk
1ıi D m1, Pk
1i D m2, Pk
1	i D m3, hj D Pk
iDjC1.ıi C
i C 	i/ denote the number of xi greater than ujI and j.t/ denotes the number of
ui less than or equal to t. The vectors u; ı; 	;  will be referred to as the data. If

7.2
Estimation of an SF Based on DP Priors
275
F Ï D.˛/, then the posterior expectation of the survival function S.t/ is
E.S.t/jdata/ D ˛.t; 1/ C hj.t/
˛.R/ C n
j.t/
Y
iD1
.˛Œui; 1/ C hi1/.˛.ui; 1/ C hi C i/
.˛.ui; 1/ C hi/.˛Œui; 1/ C hi C i C ıi/:
(7.2.11)
The Susarla-Van Ryzin formula is really the above formula with all 	i’s equal to
zero, since in that case hi1 D hi C i C ıi for i D 1; : : : ; k.
7.2.4
Estimation Based on Progressive Censoring
There is a broad class of experiments in which the Zj’s are observed sequentially, and
cost and/or time considerations often entail termination of experimentation before
all Zj’s have been observed. For example, a study may be curtailed at the k.D k.n//-
th smallest order statistics Z.k/, 1  k  n, and then in effect, the statistician has at
his disposal only the data
f.ı
i ; Z.i//; 1  i  k; Z.r/ > Z.k/; r D k C 1; : : : ng;
(7.2.12)
where ı
i D 0 or 1 according as Z.i/ is a true survival time or censoring time.
Statistical procedure based on this type of data which is referred to as progres-
sively censoring scheme and is treated by Tiwari et al. (1988). Assuming that the
ﬁrst l, 1  l  k; Z.i/’s are uncensored observations and proceeding as in Sect. 7.2.1
gives
bSk.u/ D E fS.u/ j .0; ZlC1/; : : : ; .0; Zk/, 1  l  k, Z.r/ > Z.k/; r D k C 1; : : : ng;
(7.2.13)
where the expectation is taken with respect to D.˛ C Pl
iD1 ıZ.i//. From Blum
and Susarla (1977) one may observe that the conditional distribution of S.u/ j
.0; ZlC1/; : : : ; .0; Zk/, 1  l  k, Z.r/ > Z.k/; r D k C 1; : : : n, is a mixture of
Dirichlet processes (see also Tiwari et al. 1988). Hence, from Blum and Susarla
(1977) or Gardiner and Susarla (1981, 1983) the Bayes estimator bSk in (7.2.13)
becomes
bSk.u/ D ˛.u; 1/ C NC.u/ C .n  k/IŒZ.k/ > u
˛.RC/ C n

kY
jD1
 
˛.Z. j/; 1/ C NC 
Z. j/

C .n  k/ C 1
˛.Z. j/; 1/ C NC 
Z. j/

C .n  k/
!I
h
ı
j D0; Z. j/u
i

	˛.Z.k/; 1/ C .n  k/
˛.Z.k/; 1/

Œ Z.k/u
:
(7.2.14)

276
7
Inference Based on Incomplete Data
Note that by setting k.n/ D n in (7.2.14) yields Susarla and Van Ryzin (1976)
estimator (7.2.4) with j D 1 for all j. Also, if there are no observed censoring times
in the data of (7.2.12), estimator (7.2.14) reduces to
bS.u/ D
 ˛.u; 1/ C NC.u/ C .n  k/IŒZ.k/ > u
˛.RC/ C n


	˛.Z.k/; 1/ C .n  k/
˛.Z.k/; 1/

IŒ Z.k/u
;
(7.2.15)
which in turn may be viewed as a generalization of Ferguson’s (1973) estimator

˛.u; 1/ C NC.u/

=

˛.RC/ C n

, when censoring is absent and k.n/ D n. For
u  Z.k/, formula (7.2.14) yields the (Kaplan and Meier 1958) Product-Limit
estimator, which itself reduces to the empirical survival function in the absence of
censoring.
7.2.5
Estimation Based on Record-Breaking Observations
In certain industrial experiments one observes only the successive minima and the
number of trials required to obtain the next minima. The objective is to estimate
the survival function based on such data. Tiwari and Zalkikar (1991b) treated
this problem and obtained the Bayes estimator for the survival function using the
Dirichlet process prior.
Let X1; : : : ; Xn be iid random variables from a continuous distribution function
F deﬁned on RC D .0; 1/ and let S.t/ be the corresponding survival function.
The data is observed sequentially and can be represented as Y1; K1; Y2; K2; : : :,
where Yi’s are successive minima and Ki’s are the number of trials required to
obtain a subsequent minimum. In harmony with the survival data, this data can be
reformulated as follows. Let Z1 D X1 and Zi D minfZi1; Xig and ıi D IŒXi < Zi1,
for i D 2; 3; : : : . Clearly the pair Zi and ıi are neither independent nor have the
same identical distribution. Let i denote the multiplicities of Zi (corresponding to
ıi D 0). Then, using an approach similar to the one used in Susarla and Van Ryzin
(1976), Tiwari and Zalkikar obtain the Bayes estimator of the survival function S
with respect to D.˛/ and under the weighted squared error loss as
bS˛.u/ D ˛.u; 1/ C NC.u/
˛.RC/ C n
n
Y
jD1
	˛ŒZj; 1/ C NC.Zj/ C j
˛ŒZj; 1/ C NC.Zj/

IŒıjD0; Zju=j
:
(7.2.16)
By taking the limit ˛.RC/ ! 0, it is shown that the above estimator reduces to
the nonparametric maximum likelihood estimate of S obtained by Samaniego and
Whitaker (1988). Weak convergence of the above estimator is also established.

7.2
Estimation of an SF Based on DP Priors
277
Finally, considering the usual empirical Bayes setup of Sect. 6.2.4, they derive an
empirical Bayes estimator and show that it is asymptotically optimal.
7.2.6
Estimation Based on Random Left Truncation
In most of the applications, we encounter censoring on the right. However, in
Tiwari and Zalkikar (1993) the authors consider left truncation and derive the Bayes
estimator for the survival function. Under the random left truncation model, it is
assumed that we have independent random variables X1; : : : ; Xn and T1; : : : ; Tn from
continuous distribution functions F and G, respectively, and we observe the pairs
.Xi; Ti/; i D 1; 2; : : : n only if Xi  Ti, for all i, otherwise nothing is observed. Since
G is continuous, Ti’s are distinct. Regardless of F being continuous, they assume
F 2 D.˛/, and obtain the Bayes estimator of the survival function S as follows:
bS.u/ D ˛.u; 1/ C n.Sn.u/  Gn.u//
˛.RC/
n
Y
iWTi<u
˛.Ti; 1/ C n.Sn.T
i /  Gn.T
i //
˛.Ti; 1/ C n.Sn.T
i /  Gn.Ti//
;
(7.2.17)
where
Sn.u/ D .1=n/
n
X
jD1
ŒXj > u; Gn.u/ D .1=n/
n
X
jD1
ŒTj > u;
and G.u/ D 1  G.u/; G.u/ being the left-sided limit at u. As ˛.RC/ ! 0, it is
shown that the limiting Bayes estimator is a rescaled PL estimator above the smallest
truncating observation T.1/. Below T.1/, the sample does not provide any information
and hence the Bayes estimator reduces to the limit of the prior guess. The weak
convergence of the above estimator and a numerical example are furnished.
7.2.7
Estimation Based on Proportional Hazard Models
Ghorai (1989) derives the Bayes estimator of the survival function assuming
the proportional hazard model. Let SX and SY denote the survival functions of
uncensored variable X and censored variable Y, respectively. Then, under the
proportional hazard model, it is assumed that SY D Sˇ
X for some ˇ > 0. ˇ is
known as the censoring parameter. Since E .ıi/ D P .X  Y/ D .1 C ˇ/1 D 
,
say, 
 is the expected proportion of uncensored observations. Since X’s and Y’s are
assumed to be independent and Z D min.X; Y/, SZ .t/ D P.Z > t/ D .SX .t//1Cˇ
or SX .t/ D .SZ .t//
. He assumes a priori SZ .t/ Ï D.˛/ and 
 Ï Be .a; b/, a beta
distribution with parameters a and b, and that SZ .t/ and 
 are independent. Then

278
7
Inference Based on Incomplete Data
the posterior distributions are SZ .t/ j .Z; ı/ Ï D.˛ C Pn
iD1 ıZi/ and 
j .Z; ı/ Ï
Be .a C Nu; b C n  Nu/ D Be .a; b/, say, where Nu D Pn
iD1 ıi. Since SZ .t/ and

 are independent a priori, they can be seen to be so a posterior as well. Now the
Bayes estimator of SX .t/ under L1 loss function is
bSX .t/ D EŒ.SZ .t//
 j .Z; ı/ D EBe.a;b/
h
ED.˛CPn
iD1 ıZi/

S
Z .t/ j
i
:
(7.2.18)
Appealing to the moments of the Dirichlet process, the quantity inside the square
brackets is
ED.˛CPn
iD1 ıZi /

S
Z .t/ j

D


˛

RC
C n

 .˛ .RC/ C n C 
/


˛ .t; 1/ C NC .Z1/ C 

 .˛ .t; 1/ C NC .Z1//
 ;
(7.2.19)
where, as before, NC.u/ D Pn
iD1 IŒZi > u. However, explicit evaluation of this
expression is difﬁcult and therefore some approximations by expanding the ratios
of gamma functions are provided. Final evaluation of the estimator bSX is then
proceeded by taking the expectation of this quantity with respect to Be .a; b/.
Ghorai proves some asymptotic properties of this estimator, in particular, almost
sure consistency and weak convergence to a Gaussian process.
7.2.8
Modal Estimation
There is difﬁculty in deﬁning the mode of an inﬁnite dimensional distribution. Like
Ramsey (1972), Ferguson and Phadia (1979) avoid this difﬁculty by restricting
attention to ﬁnite dimensional subsets of the variables. With t1; : : : ; tk as arbitrary
points, they deﬁne the modal estimate of F.t/ as the modal value of F.t/ in the
joint distribution of .F.t/; F.t1/; : : : ; F.tk//, where t1; : : : ; tk are arbitrary points
that contain all exclusive censoring points. So assume that t1; : : : ; tk are arbitrary
points containing t and all exclusive censoring points. Further assume that they are
arranged in an increasing order and that F Ï D.˛/. ˛ is assumed to give positive
mass to every open interval. Thus the vector . p1; p2 : : : ; pkC1/ D .F.t1/; F.t2/ 
F.t1/; : : : ; 1F.tk// has a Dirichlet distribution with parameters ˇi D ˛.ti/˛.ti1/
for i D 1; : : : ; k C 1 with ˛.t0/ D 0, ˛.tkC1/ D ˛.R/, and ˇi > 0, for
i D 1; : : : ; k C 1. The authors take the density of the vector . p1; p2 : : : ; pk/ with
respect to the measure d D Qk
iD1dpi=QkC1
iD1 pi where pkC1 D 1  Pk
iD1pi, over the
simplex
Sk D
(
. p1; p2 : : : ; pk/ W pi  0 for i D 1; : : : ; k, and
k
X
iD1
pi  1
)
:
(7.2.20)

7.3
Estimation of an SF Based on Other Priors
279
The prior density of . p1; p2 : : : ; pk/ over Sk with respect to d is proportional to
QkC1
iD1 pˇi
i . With this formulation they prove the following. Let ˛ be such that it gives
positive mass to every open interval. Then the posterior modal (with respect to )
estimate of S, given the data is
bS.t/ D ˛.t; 1/ C hj.t/
˛.R/ C n
j.t/
Y
iD1
.˛.ui; 1/ C hi C i/
.˛.ui; 1/ C hi/
;
(7.2.21)
where ui’s are distinct observations in the sample, j.t/ denotes the number of ui less
than or equal to t and hj denotes the number of observations greater than uj.
The above formula reveals that the estimate depends only on the censoring points
among t1; : : : ; tk, and is thus independent of the choice of t1; : : : ; tk provided all
exclusive censoring points are included.
Remark 7.2 It is clear from all of the above results that the Bayes estimator of S
with respect to a Dirichlet process prior under various sampling schemes turn out
to be a version of Susarla-Van Ryzin estimator; and when the prior information
tends to nil via ˛ .R/ ! 0, they reduce to the nonparametric MLE, namely, the PL
estimator, as the Bayesian approach envisages. This may be construed as another
attractive feature of the Dirichlet process.
7.3
Estimation of an SF Based on Other Priors
In this section, Bayesian estimators of a survival function with respect to other
priors, such as processes neutral to the right, beta, gamma, and beta-Stacy, are
presented. They have similar form as for the case of the Dirichlet process prior,
and in many cases the estimators are a different version of the Susarla-Van Ryzin
estimator. In case of the processes neutral to the right, a posterior moment generating
function (MGF) is given. From these, the estimators for the case of uncensored
data can easily be recovered. Also, an alternate approach of placing a prior via
subsurvival functions is presented.
7.3.1
Estimation Based on an Alternate Approach
A different approach is adopted by Tsai (1986). He considers the joint distribution
of .Z; ı/ and assigns a Dirichlet process prior with parameter ˛ (to be described
below) to the pair. He obtains the Bayes estimators of subsurvival functions under
the weighted squared error loss function and then combines the two estimators to
produce an estimator for the survival function, which is then shown to be Bayes
under a slightly different loss function. This approach does not require independence

280
7
Inference Based on Incomplete Data
between the Xi’s and Yi’s. In fact he does not even deﬁne censoring variables Yi’s.
Instead he assumes the data available to be of the form .Zi; ıi/ where ıi D 1 if
Zi; D Xi and ıi D 0 if Zi < Xi for i D 1; : : : ; n, the pairs .Zi; ıi/ are independent,
and X1; : : : ; Xn
iidÏ S. Furthermore, it is clear that .Zi; ıi/’s need not be identically
distributed. The independence-like assumption makes the marginal distribution of
X identiﬁable, and the estimate of S consistent. Since the marginal distribution is
not Dirichlet under this assumption, the resulting estimator is distinct from that of
Susarla and Van Ryzin. Here are some details.
Tsai places a Dirichlet process prior with parameter ˛, on .R; B/, where
R D RC f0; 1g and B D B  f; f0g ; f1g ; f0; 1gg, B is a Borel ﬁeld on RC and
˛ is a non-null ﬁnite measure on .R; B/. Then, based on a random sample of size
n, the Bayes estimators
^
Su and
^
Sc of subsurvival functions Su .t/ D P.Z > t; ı D 1/
and Sc .t/ D P.Z > t; ı D 0/, respectively, are derived under the loss function
L.S;
^
S/ D R 1
0 .S 
^
S/2dW :
^
Su .t/ D ˛ ..t; 1/ ; f1g/ C Pn
iD1 IŒZi > t; ıi D 1
˛ .R/ C n
(7.3.1)
and
^
Sc .t/ D ˛ ..t; 1/ ; f0g/ C Pn
iD1 IŒZi > t; ıi D 0
˛ .R/ C n
:
(7.3.2)
To unify the discrete and continuous cases of S, he follows Kalbﬂeisch and
Prentice (1980, pp. 7–9) and deﬁnes
ƒ .t/ D 
Z tC
0
dS .u/
S .u/
(7.3.3)
and
 .ƒ/ .t/ D lim
k!1
kY
iD1
f1  Œƒ .ui/  ƒ .ui1/g ;
(7.3.4)
where 0 D u0 < u1 <    < uk D t, the integral and differential operators are
Riemann–Stieltjes operators, and the limit k ! 1 is taken as uk D ukuk1 ! 0.
From the above,
S .t/ D  .ƒ/ .t/ D exp
I t
0
dS .u/
S .u/
 Y
st
	
1  S .s/
S .s/

;
(7.3.5)
where the integral is over the intervals of points less than t for which S is continuous,
and S .s/ D S .s/  S

sC
.

7.3
Estimation of an SF Based on Other Priors
281
Thus a self-consistent (Efron 1967) estimator
^
S of S is obtained as
^
S .t/ D 
0
@
Z tC
0
d
^
Su .u/
^
Su .u/ C
^
Sc .u/
1
A .t/
D 
 

Z tC
0
d.˛ ..u; 1/ ; f1g/ C Pn
iD1 IŒZi > u; ıi D 1/
˛ .Œu; 1/; f0; 1g/ C Pn
iD1 IŒZi  u
!
.t/ :
(7.3.6)
Then it is shown that
^
S is the Bayes estimator of S under the loss function
L.S;
^
S/ D
Z 1
0
Œ1 .S/ .t/  1.
^
S/ .t/2dW .t/ ;
(7.3.7)
where 1 denotes the inverse operator of . He proves the estimator to be strongly
consistent and derives the weak convergence results.
When ˛ ..t; 1/ ; f0g/ D 0 and ˛ ..t; 1/ ; f1g/ D ˛ .t; 1/, then
^
S reduces to a
version of the Susarla-Van Ryzin estimator under certain conditions.
Salinas-Torres et al. (2002) generalize this approach in the context of k competing
risks. Suppose the risk set is f1; : : : ; kg and let  be a subset of f1; : : : ; kg. Then
they derive a Bayes estimator for the marginal survival function S by the above
approach and show that the resulting estimator is also consistent and weakly
convergent. It is discussed later in this chapter.
7.3.2
Estimation Based on Neutral to the Right Processes
In all of the applications discussed in the previous section, the Dirichlet process
prior was used. In this section, we describe the results when a neutral to the right
process is used as prior in the estimation of a survival function. This prior being
conjugate with respect to the right censored data, the posterior distribution given the
data is also neutral to the right. This result of Doksum (1974) was extended to the
case of two types of censoring in Ferguson and Phadia (1979).
Since the prior distribution of F may give positive probability to the event that
F has a jump at a ﬁxed point, it is useful in such problems to generalize earlier
treatments to allow two types of censoring: “inclusive” and “exclusive.” In this case,
the description of the posterior distribution of F turns out to be simpler than that in
the case of uncensored data. In fact, the posterior distribution is the same as in
Doksum’s except that the jump at the point x does not have to be treated differently.
The increment at x is treated as if it were to the left of x in the case of exclusive
censoring and to the right of x for inclusive censoring.

282
7
Inference Based on Incomplete Data
Thus the nonparametric Bayesian estimation problem based on right censored
data can be conveniently carried out by using processes neutral to the right as prior
processes. As a particular case, Susarla and Van Ryzin’s result is derived. A slight
deviation from earlier treatment is considered here in that the censoring variables
yi’s are assumed to be ﬁxed constants rather than random variables. However, as
noted in Sect. 7.2.3, Lo (1993a, Lemma 7.1), has shown that the results hold even in
the case when yi’s are assumed to be random with distributions Gi, as long as F and
Gi’s are independent.
Complete description of the posterior distribution of F for application to the
censored data in which two types of censoring is considered was presented for a
sample of size one in Sect. 4.2, Theorem 7.3.
For the general case, as mentioned there, it is easy to work with the MGF,
Mt .
/ D Ee
Yt, where Yt is a process with nonnegative independent increments.
The vectors u; ı; 	;  as deﬁned in Sect. 7.2.3 will be referred to as the data. We
will use the same notation as before. We also use M
t .
/ to denote the MGF of Y
t ,
M
t .
/ D lims!t Ms.
/, for s < t. Gu .s/ denotes the prior distribution of the jump
in Yt at u, and Hu .s/ its posterior distribution, given X D u for a single observation.
Then the following result was obtained.
Theorem 7.3 (Ferguson and Phadia) Let F be a random distribution function
neutral to the right, and let X1; : : : ; Xn, be a sample of size n from F, yielding data
u; ı; 	; . Then the posterior distribution of F given the data is neutral to the right,
and Yt has posterior MGF
Mt.
 j data/ D Mt.
 C hj.t//
Mt.hj.t//

j.t/
Y
iD1
"
M
ui.
 C hi1/
M
ui.hi1/
 Cui.
 C hi C i; ıi/
Cui.hi C i; ıi/

Mui.hi/
Mui.
 C hi/
#
;
(7.3.8)
where, if u is a prior ﬁxed point of discontinuity of Yt,
Cu.˛; ˇ/ D
Z 1
0
e˛z.1  e z/ˇd Gu.z/
(7.3.9)
while, if u is not a prior ﬁxed point of discontinuity of Yt,
Cu.˛; ˇ/ D
 R 1
0
e˛z.1  e z/ˇ1d Hu.z/
if ˇ  1
1
if ˇ D 0:
(7.3.10)
Now it is easy to evaluate posterior moments of F. For example the posterior
expectation of the survival function S is obtained by plugging 
 D 1 in the above
expression, E.S.t/jdata/ D Mt.1jdata/. However, the difﬁculty is encountered in
ﬁnding the posterior distribution Hu .s/ of the jump at the point of discontinuity.

7.3
Estimation of an SF Based on Other Priors
283
Nevertheless, it is shown that in the case of homogeneous processes this is easy to
do. This was illustrated in two speciﬁc cases (Ferguson and Phadia 1979).
7.3.3
Estimation Based on a Simple Homogeneous Process
In the ﬁrst case, let Yt be a simple homogeneous process with the MGF
Mt .
/ D exp

.t/
Z 1
0
.e
 z  1/ez .1  ez/1 d z

;
(7.3.11)
where  is assumed to be continuous. Then, with the above data scheme and
notations, it is shown that
E.S.t/jdata/ D e.t/=.hj.t/C/

j.t/
Y
iD1

exp

 .ui/
hi1  hi
.hi1 C / .hi C /


.hi C i C /
.hi C i C ıi C /;
(7.3.12)
which is the Bayes estimator under the weighted squared error loss function.
If we have the knowledge of S0.t/, the prior guess of the survival function, then
.t/ D  log S0.t/. Substituting this in the above formula, we get
E.S.t/jdata/ D S0.t/=.hj.t/C/

j.t/
Y
iD1

S0.t/

hi1hi
.hi1C/.hiC/


.hi C i C /
.hi C i C ıi C /:
(7.3.13)
Further if S0.t/ > 0 for all t, we have as  ! 0,
E.S.t/jdata/ !
(
Qj.t/
iD1
hiCi
hiCiCıi
for t < uk
S0.t/
S0.uk/
Qk
iD1
hiCi
hiCiCıi for t  uk
;
(7.3.14)
where .hk C k/ = .hk C k C ık/ is to be treated as 1 if it is 0=0. This is a maximum
likelihood estimator. If there are no censored observations, i D 0, hi C ıi D hi1
and the estimator reduces to the sample distribution function.

284
7
Inference Based on Incomplete Data
7.3.4
Estimation Based on Gamma Process
A second case is when the independent increments of the process Yt have gamma
distributions with parameters  .t/ (assumed to be continuous) and . In this case
the MGF takes a simpler form
Mt .
/ D
	

 C 

.t/
D exp

.t/
Z 1
0
.e
 z  1/ezz1d N.z/

:
(7.3.15)
The posterior mean of the survival function given the above scheme of data turns
out to be
E.S.t/jdata/ D Mt.1jdata/ D
	
hj.t/ C 
hj.t/ C  C 1

.t/

j.t/
Y
iD1
"	.hi1.t/ C / .hi.t/ C  C 1/
.hi1.t/ C  C 1/ .hi.t/ C /

.ui/
 G .hi C i C  C 1; ıi/
G .hi C i C ; ıi/

;
(7.3.16)
where
G .˛; ˇ/ D
ˇ1
X
iD0
 
ˇ  1
i
!
.1/i log
	˛ C i C 1
˛ C i

:
(7.3.17)
Note that E.S.t// D Mt.1/ D


C1
.t/. Thus, if we have a prior guess at S.t/,
say, S0.t/, we can choose .t/ such that .= . C 1//.t/ D S0.t/, for all t and for a
ﬁxed . For further observations and the effect of  on the behavior of this estimate,
see Ferguson–Phadia paper.
In the case of gamma process prior, Ghorai (1981) derived an empirical Bayes
estimator of the survival function. Here  ./ plays a role similar to ˛ ./ in the
Dirichlet process. He assumes  to be known, estimates  ./ from the previous
nstages, and proceeds to determine the empirical Bayes estimator of S at the
.n C 1/-the stage on the line of Susarla and Van Ryzin (1978a) and Phadia (1980).
For simplicity he also considers the case of sample size one. His estimator of S at
the .n C 1/-the stage turns out to be the above estimator in which  is replaced by

7.3
Estimation of an SF Based on Other Priors
285
its estimator
^ and can be simpliﬁed as
bSnC1.u/ D
8
ˆˆˆˆ<
ˆˆˆˆ:
. C1
C2/
^ .u/
if
u < ZnC1
.

C1/
^ .u/ 
.C1/2
.C2/
^.ZnC1/
if
u  ZnC1; ınC1 D 0
.

C1/
^ .u/ 
.C1/2
.C2/
^.ZnC1/
if
u  ZnC1; ınC1 D 1 ,
(7.3.18)
where  D ln
 C2
C1

 ln
 C1


and
^ .u/ is such that
	

 C 1

^ .u/
D 1 C NC.u/
1 C n
n
Y
jD1
	NC.Zj/ C 2
NC.Zj/ C 1

ŒıjD0; Zju
:
:
(7.3.19)
Ghorai showed that the sequence of empirical Bayes estimators is asymptotically
optimal with rate of convergence O.n1/.
7.3.5
Estimation Based on Beta Process
As noted in Sect. 4.5, the cumulative hazard function A .t/ D
R
Œ0;tdF .s/ =FŒs; 1/
is related to the distribution function via the correspondence F .t/
D
1 
Q
Œ0;t f1  dA .s/g, where … is the product integral. Suppose A has a beta process
prior with parameters c ./ and A0 as deﬁned in Sect. 4.5. Suppose we have, as before
X1; : : : ; Xn
iidÏ F, y1; : : : ; yn as censoring times and we observe Zi D min fXi; yig and
ıi D IŒXi  yi. Then, using the posterior distribution of A given the data .Z; ı/ and
applying the relevant formulas, Hjort (1990) derives the following Bayes estimator
of S .t/ under L1 loss.
^
S .t/ D E .S .t/ jdata/ D
Y
Œ0;t

1  c .s/ dA0 .s/ C dN .s/
c .s/ C M .s/

;
(7.3.20)
where N ./ is the counting process for uncensored observations, dN .t/ D N ftg, the
number of uncensored observations at t, and M .t/ D Pn
i IŒZi  t], the number of
observations surviving at t. As c ./ ! 0,
^
S .t/ tends to the PL estimator. It should
be noted that here the prior is placed on the cumulative hazard function and not
on the survival function itself. On the other hand, if F has a beta-Stacy prior with
parameters c ./ and G, Muliere and Walker (1997) obtain the same estimator as
shown below.

286
7
Inference Based on Incomplete Data
7.3.6
Estimation Based on Beta-Stacy Process
Assume a beta-Stacy prior (see Sect. 4.7) with parameters c ./ and G for F. Then
given a random sample from F, with possible right censored observations, the Bayes
estimate of S .t/ with L1 loss function is given by Muliere and Walker (1997)
^
S .t/ D E .S .t/ jdata/ D
Y
Œ0;t

1  c .s/ dG .s/ C dN .s/
c .s/ GŒs; 1/ C M .s/

;
(7.3.21)
where N ./ and M .t/ are as deﬁned above. The PL estimator is obtained if c ./ ! 0.
7.3.7
Estimation Based on Polya Tree Priors
Muliere and Walker (1997) present the estimation of a survival curve using the
posterior predictive distribution of a future observation. Assume that … and A,
as described in Sect. 5.2 are given and F Ï PT .…; A/. Let 
1; 
2; ::jF
iidÏ F.
The posterior predictive distribution based on exact observations was given earlier
[expression (5.2.4)] as
P


nC1 2 Bmjdata

D
˛1 C n1
˛0 C ˛1 C n   
˛m C nm
˛m10 C ˛m11 C nm1
;
(7.3.22)
where n is the number of observations in B and k D 1 : : : k. Note that if we let
˛0 + ˛1 D ˛ for all , it reduces to (˛

Bm

C nm/=

˛

RC
C n

as one would
obtain for the Dirichlet process. For the censored data, it is shown that the posterior
predictive distribution of a future observation is given by
P


nC1 2 Bmjdata

D
˛1 C n1
˛0 C ˛1 C n   
˛m C nm
˛m10 C ˛m11 C nm1  m1
;
(7.3.23)
where  is the number of observations that are censored in B. If we take ˛ measure
such that ˛ D ˛ .B/ for all , then this estimator is essentially the same as Susarla-
van Ryzin estimator obtained using the Dirichlet process, since for such ˛’s, the
Polya tree distribution reduces to the Dirichlet process as was noted in Ferguson
(1974).
For the Polya tree …, their construction uses the distinct censoring points, say,
t1 < t2 <    < tk, and splits the right-hand side interval [ti; 1/ into two intervals
Œti; tiC1/ and ŒtiC1; 1/, for i D 1; 2; : : : ; k. The construction of intervals to the left of
these partitions remains arbitrary. Obviously the point of contention is the unsavory
fact of using data in constructing the prior. They suggest some partial remedy to
overcome this undesirable situation.

7.3
Estimation of an SF Based on Other Priors
287
Neath (2003) also uses Polya tree distributions for statistical modeling of
censored data.
7.3.8
Estimation Based on an Extended Gamma Prior
In contrast to placing a prior on the space of survival functions, if the hazard rate r.t/
is assumed to be distributed a priori as an extended gamma process with parameters
˛ and ˇ, ..˛ ./ ; ˇ .// (see Sect. 4.4), and S.x/ D expf
R
Œ0;x/ r.t/dtg, then Dykstra
and Laud (1981) has shown that OS˛ˇ.t/ given below is the Bayes estimator of
S.t/ D P .XnC1  tjX1 D x1; : : : ; Xn D xn/, the conditional survival function of a
future observation given n current observations, under the usual L1 loss function.
OS˛ˇ.t/ D exp


Z
Œ0;1/
log.1 C ˇ.s/ .t  s/C/d˛.s/

 .ˇ/=.ˇ/;
(7.3.24)
where
 .ˇ/ D
Z
Œ0;xn/
  
Z
Œ0;x1/
n
Y
iD1
ˇ .zi/
n
Y
iD1
d
2
4˛ C
n
X
jDiC1
I.xj;1/
3
5 .zi/ ;
(7.3.25)
with ˇ .s/ D
ˇ.s/
h
1Cˇ.s/.ts/C/
i and ˇ .t/ D ˇ .t/ =

1 C ˇ .t/ 
mP
iD1
.xi  t/C

.
7.3.9
Estimation Assuming Increasing Failure Rate
If the hazard rate (also known as failure rate) r.t/ is known to be increasing
(nondecreasing), a different approach is proposed in Padgett and Wei (1981). In
this case they propose a constant jump process prior on the space of all increasing
failure rates. The process consists of constant jumps of size c at times Ti > 0; i D
1; 2; : : :, where Ti are arrival times of a Poisson process fN .t/ W t > 0g with intensity
parameter . With respect to this prior and under the usual respective loss functions,
the Bayesian estimates of the survival function, S.x/ D expf
R
Œ0;x/ r.t/dtg, failure
rate function, and the density function, based on right censored observations, are
obtained by them. Although the derivation is straightforward, the expressions do
not turn out to be simple. See their paper for details.

288
7
Inference Based on Incomplete Data
7.4
Linear Bayes Estimation of an SF
In Bayesian estimation of a survival curve S, Zehnwirth (1985) takes a different
approach. In assuming the Dirichlet process or a neutral to the right process as
prior, it is tacitly assumed that the hazard contributions of nonoverlapping intervals
are independent. Zehnwirth argues that this may not be the case in practice. In his
paper he obtains a Bayesian estimator of S by estimating the hazard contributions
between successive censoring points by linear Bayes rule. In doing so, tractability
and simplicity are gained but his estimator turns out to be only an approximate
Bayes estimator of the survival curve. Moreover, he takes the loss function as point-
wise squared error at distinct censoring points which is different from the usual
weighted squared error loss function. It may be reasonable if the censoring points
are assumed to be ﬁxed, as is the case in some clinical studies where the trials are
monitored at regular intervals.
Again as in the right censored data model, let Z.1/, Z.2/; : : : ; Z.m/ be the distinct
ordered censored observations among a sample of n observations. Let N.u/ D
Pn
jD1ŒZj
 u, NC.u/ D Pn
jD1ŒZj
> u, and j stand for the number of
censored observations at Z. j/; Pm
jD1 j D n. Further denote N. j/ D N.Z. j// for
j D 1; 2; : : : ; m. Let p.bja/ D PŒXi  b j Xi  a D S.b/=S.a/ if PŒXi  a > 0,
otherwise p.bja/ D 0. p.bja/ represents the conditional probability of surviving up
to point b given that the object has survived up to point a.
For any u 2 RC, let Z.l/  u  Z.lC1/, l D 0; 1; : : : ; m with Z.0/ D 0, Z.mC1/ D
1, N.0/ D n, and 0 D 0. Consider the partition Œ0; Z.1//, ŒZ.2/, Z.3//; : : : ; ŒZ.l/; u/
of Œ0; u/. Then S.u/ can be written as
S.u/ D p.ujZ.l//
l1
Y
jD0
p.Z. jC1/jZ. j//:
(7.4.1)
Zehnwirth now estimates each p.bja/ by a linear Bayes rule (i.e., linear rule that
best approximates the Bayes rule) by minimizing the risk
R.S;bS/ DE

.au;l C bu;l
NC.u/
N.l/  l
 p.ujZ.l///2

C
l1
X
jD0
E

.aj C bj
N. j C 1/
N. j/  j
 p.Z. jC1/jZ. j///2

;
(7.4.2)

7.4
Linear Bayes Estimation of an SF
289
over all au;l; bu;l; aj and bj for j D 0; 1; : : : ; l1. The linear Bayes estimator for S.u/
thus obtained for Z.l/  u  Z.lC1/, l D 0; 1; : : : ; m, is
bS.u/ D
 NC.u/ C f.ujZ.l//g.ujZ.l/; /
N.l/  l C f.ujZ.l//


l1
Y
jD0
	N. j C 1/ C f.Z. jC1/jZ. j//g.Z. jC1/jZ. j//
N. j/  j C f.Z. jC1/jZ. j//

(7.4.3)
where
f.bja/ D EŒ p.bja/.1  p.bja/
VarŒ p.bja/
and
g.bja/ D EŒ p.bja/:
(7.4.4)
Here f.bja/ may be interpreted as the number of individuals at risk at a and
f.bja/g.bja/ as the number of survivors up to b among them.
So far no assumption is made regarding a prior for F. If F is assumed to
be a neutral to the right process, then given data the posterior distribution of
F is also neutral to the right. In this case f.bja/ and g.bja/ may be evaluated
as follows. Note that for any two disjoint intervals Œ0; a/ and [a; b/, the survival
probability satisﬁes p.bj0/ D p.aj0/p.bja/. By the independence property of
neutral to the right processes, this expression can be written as EŒSr.b/=Sr.a/ D
EŒSr.b/=EŒSr.a/ for any r, a positive integer. This yields
g.bja/ D S1.b/=S1.a/
and
f.bja/ D S1.b/=S1.a/  S2.b/=S2.a/
S2.b/=S2.a/  S2
1.b/=S2
1.a/;
(7.4.5)
where S1 and S2 are the ﬁrst and second moments of S, S1 .u/ D E .S .u//, and
S2 .u/ D E

S2 .u/

.
Now substituting these quantities in (7.4.3), the linear Bayes estimator for S.u/
reduces to
bS.u/ D
(
NC.u/ C f.ujZ.l//S1.u/=S1.z
.l//
N.l/  l C f.ujZ.l//
)

l1
Y
jD0
 
N. j C 1/ C f.z. jC1/jz. j//S1.z
. jC1//=S1.z
. j//
N. j/  j C f.z. jC1/jz. j//
!
:
(7.4.6)

290
7
Inference Based on Incomplete Data
On the other hand, if F
Ï D.˛/, then S.u/ has a Be.˛Œu; 1/; ˛Œ0; u//
distribution and therefore,
S1.u/ D ˛Œu; 1/
˛.RC/
and
S2.u/ D ˛Œu; 1/.˛Œu; 1/ C 1/
˛.RC/.˛.RC/ C 1/ :
(7.4.7)
This implies f.bja/ D ˛Œa; 1/. Substituting this in the above expression yields
the Bayes estimator of Susarla and Van Ryzin (1976). Other neutral to the right
processes such as gamma or simple homogeneous processes may be used to evaluate
S1 and S2 yielding different linear estimates of S. In fact, besides the above
independence assumption, all we need is the ﬁrst two moments of S to compute
this estimator.
7.5
Other Estimation Problems
In this section, we describe Bayesian solutions to some other estimation problems
that have appeared in the literature.
7.5.1
Estimation of P.Z > X C Y/
Let X; Y, and Z be independent and identically distributed as F, which is deﬁned on
RC. Consider the problem of estimating the probability  .F/ given by
 .F/ D P.Z > X C Y/ D
Z 1
0
Z 1
0
S.x C y/dS.x/dS.y/:
(7.5.1)
Assume F Ï D.˛/ and the squared error loss function L2. Based on a random
sample of right censored data .Z; ı/ of size n, Zalkikar et al. (1986) derived the
Bayes estimator of  as
^
 .S/ D .M C n/2
.M C n/.3/


Z 1
0
OS˛.2y/d OS˛.y/ C .M C n/. OS˛/

;
(7.5.2)
where OS˛ is the Bayes estimator of S with respect to the Dirichlet process prior.
When M ! 0, OS˛ ! OSPL, the PL estimator of the survival function, therefore the
estimator reduces to
^
 .S/ D n2
n.3/


Z 1
0
OSPL.2y/d OSPL.y/ C n. OSPL/

:
(7.5.3)

7.5
Other Estimation Problems
291
The empirical Bayes estimator is also derived using the procedure discussed in
Sect. 6.2.4.
7.5.2
Estimation of P.X  Y/
The Bayesian estimator of  D P.X  Y/ D
R
FdG based on two independent
samples, X1; : : : ; Xn from F and Y1; : : : ; Yn from G (need not be of the same size)
under the squared error loss was derived by Ferguson (1973). He assumed F
Ï
D.˛1/ and independently, G Ï D.˛2/ . Based on samples X and Y, he obtained the
estimator as
^
 D
R bF˛1dbG˛2, where bF˛1 and bG˛2 are Bayes estimators of F and G,
respectively. Its treatment from the empirical Bayes point of view was considered
by Hollander and Korwar (1976), as indicated earlier.
Its extension to the case of right censored data was carried out in Phadia and
Susarla (1979) as follows. Assume that we have U1; : : : ; Un
iidÏ H1,and V1; : : : ; Vn
iidÏ
H2, the censoring variables. All random variables are assumed to be mutually
independent. We observe the pairs (Si; Ti/, i D 1; 2; : : : ; n, where Si D min.Xi; Ui/
and Ti D min.Yi; Vi/, and pairs .ıi; i/, where ıi D IŒXi  Ui, and i D IŒYi 
Vi i D 1; 2; : : : ; n. Based on the data fS; ı; T; 
g we want to estimate . In the
context of censored data, it is easy to handle if F and G are considered as right
sided distribution functions. That is F.t/ D P.X > t/ and G.t/ D P.Y > t/. Then
 D P.X  Y/ D R .1  F/d.1  G/ D  R GdF. Therefore the Bayes estimate
of  under the squared error loss is given by
^
 D
R
E.G/dE.F/ D 
R bG˛2dbF˛1
where the conditional expectation is taken with respect to the posterior distributions,
and bF˛1 and bG˛2 are the Bayes estimators of F and G, respectively, derived earlier.
Again when ˛ ./ and ˛ .R/ are unknown, the empirical Bayes methods can be
used. This was done by the authors. For simplicity, they took the sample of size one
at each stage and proposed the following estimator at the .n C 1/-th stage (based on
n previous stages, each of sample size one):
^
nC1D  R Mn
1 bG ^
˛2dbF ^
˛1 where
^˛1 and
^˛2 are given by
^˛1.u/ D NC
1 .u/
n
n
Y
iD1
"
NC
1 .Si/ C 2
NC
1 .Si/ C 1
#ŒıiD0;Siu
(7.5.4)
and
^˛2.u/ D NC
2 .u/
n
n
Y
iD1
"
NC
2 .Ti/ C 2
NC
2 .Ti/ C 1
#ŒiD0;Tiu
;
(7.5.5)

292
7
Inference Based on Incomplete Data
where NC
1 .u/ D Pn
iD1IŒSi > u, NC
2 .u/ D Pn
iD1IŒTi > u, and fMng is a suitable
sequence converging to 1 as n ! 1. Unlike in the uncensored case, the integral
in the estimator here had to be restricted to the interval .1; Mn/ to overcome
divergence of some integrals arising in the bounds. They also discuss the choice of
the sequence fMng. Note also that
^˛1 and
^˛2 do not depend on H1 and H2. Finally,
the asymptotic optimality of the estimator is established and explicit expression for
the rate of convergence of 0

n1
is derived.
7.5.3
Estimation of S in Competing Risk Models
Consider the situation in which there are two competing causes of death labeled
1 and 2. With each cause of death i, i D 1; 2, associate a random variable Ti
representing the time of death if i were only the cause of death. Then in practice,
one observes only the min.T1; T2/ and the cause of death 1 or 2. Based on this type
of data, one needs to estimate the survival function S.x; y/ D P.X > x; Y > y/
corresponding to a random probability P deﬁned on

R2
C; B2
C

, and B2
C is the Borel
ﬁeld deﬁned on R2
C. Note that unlike in the right censored data case discussed in the
previous sections, X and Y are not assumed to be independent.
Phadia and Susarla (1983) treated this problem and obtained the Bayes estimator
of S.x; y/ with respect to a bivariate Dirichlet process prior and under the weighted
squared error loss function.
Let .X1;Y1/ ; : : : ; .Xn;Yn/ be a random sample from the unknown distribution
function F .x; y/ deﬁned on R2
C D f.0; 1/  .0; 1/g. The data consists of .Zi; ıi/,
where Zi D min.Xi;Yi/ and ıi D IŒXi  Yi, i D 1; 2; : : : ; n. Let P be a Dirichlet
process D .˛/ deﬁned on R2
C with parameter ˛, where ˛ is a nonnegative ﬁnite
measure on

R2
C; B2
C

. The loss function used is
R
R2
C.S
^
S/2dW, where W is a known
weight function on R2
C. Suppose among the data there are k distinct zi’s and without
loss of generality assume them to be ordered so that 0 < z1 <  < zk < 1. Further
let i and 	i be the number of censored and uncensored observations at zi, i.e.,
i D #
˚
jj min.Xj; Yj/ D zi and Xj > Yj

, 	i D #
˚
jj min.Xj; Yj/ D zi and Xj  Yj

.
Then the Bayes estimator of S.s; t/ is obtained as
^
S .s; t/ D EŒS .s; t/ j .Z; ı/
D
1
˛ R2
C
 C n
˚
˛ ..s; 1/  .t; 1// C NC .max .s; t// C P
r
r

;
(7.5.6)

7.5
Other Estimation Problems
293
where the summation is taken over all r such that min .s; t/ < zr < max .s; t/,
NC .u/ D P
fiWzi>ug .i C 	i/ D # of observations > u;
(7.5.7)

r D
8
<
:
r˛0
s .Zr/ if s > t
	r˛0
t .Zr/ if s < t
0
if s D t
;
(7.5.8)
and
˛0
s .Zr/ D lim
&0
˛ .fX > s; Zr   < Y  Zrg/
˛ .fX > Y; Zr   < Y  Zrg/
˛0
t .Zr/ D lim
&0
˛ .fY > t; Zr   < X  Zrg/
˛ .fX  Y; Zr   < X  Zrg/;
(7.5.9)
whenever the limits exist.
It should be noted that the Bayes estimator is a proper survival function. P
r
r in
the numerator of the Bayes formula represents a quantity which may be considered
as a sum of “conditional” probabilities each weighed by the number of ties at the
point of conditioning. If we take s D t, then it reduces to a 2-dimensional analogue
of the Bayes estimator obtained by Ferguson (1973). If we set t D 0, we get the
Bayes estimator of the marginal survival function S.s; 0/.
This result can be extended to the case of competing risks models where there
are three or more competing (dependent) causes of failure and we observe only the
life time of the component and the cause of failure.
As an example, the authors compute the Bayes estimator by taking ˛ measure to
be continuous with density,
˛0 .x; y/ D
 ˇ .ˇ C / e.ˇC/y if 0  x < y
 .ˇ C / e.ˇC/x if 0  y < x;
(7.5.10)
which is a special case of Freund’s bivariate exponential distribution (Johnson and
Kotz 1970), for ˇ;  > 0. Then by straightforward substitution and simpliﬁcation,
the Bayes estimator for s > t is obtained as
^
S .s; t/ D
1
n C 1
(
e.ˇC/s Œ1 C  .s  t/ C NC .s/ C
X
t<zr<s
re.ˇC/.zrs/
)
:
(7.5.11)
A similar expression can be obtained for s < t. For t D s,
^
S .s; 1/ D
.n C 1/1 ˚
˛ ..s; 1/  .s; 1// C NC .s/

.
In the above treatment the posterior distribution of the joint survival function
was not derived. Neath and Samaniego (1996) shaped this problem in the general
framework of a multiple decrement model, and in the bivariate case obtained the

294
7
Inference Based on Incomplete Data
posterior distribution of S given the data. However, their approach is different and
employ the special feature of the identiﬁed minima in updating the parameter ˛.
They introduce a new random variable  having a distribution
P f 2 Bg D P fU 2 BjZg ;
(7.5.12)
for any set B 2 B2
C, and where U represents a complete observation .X; Y/, and Z,
the identiﬁed minimum of the pair. That is,  represents the unobserved complete
realization from P. They prove that if P Ï D .˛/, then the posterior distribution
of P given Z is a mixture of Dirichlet processes with random parameter measure
˛ C ı. They extend this result to the sample of size n, for the case when the sample
is drawn from a continuous distribution and ˛ is a continuous measure. Finally the
limiting posterior distribution is derived showing that as the sample size grows to
inﬁnity, the posterior distribution becomes degenerate. In a subsequent paper (Neath
and Samaniego 1997) they handle the case of sample being drawn from a discrete
distribution and ˛ being a discrete measure.
In the above two papers, the authors do not assume X and Y to be independent.
In contrast, if the components are assumed to be independent, Salinas-Torres et al.
(2002) propose a different approach in estimating the survival function based on a
subset of failure caused by using Peterson (1977) formula of expressing the survival
function as a function of subsurvival functions. It is essentially the extension of
Tsai’s (1986) approach, where he considers two competing risks and uses Peterson’s
formula in deriving a self-consistent estimator and shows that it is Bayes with
respect to a certain loss function (see Sect. 7.3.1). The approach of Salinas-Torres
et al. is presented here brieﬂy.
Consider the competing risks model with k competing causes of system’s failure.
Let Xj denote the failure time of the j-th component and its (marginal) survival
function be denoted by Sj .t/ D P

Xj > t

, j D 1; ::; k. Let Z D min ( X1; : : : ; Xk/
and S
j .t/ D P .Z > t; ı D j/, be the subsurvival function of the j-th component,
j D 1; :::; k. Then if ı D j, Z D Xj and S .t/ D P .Z > t/ D Pk
jD1S
j .t/. Let  be a
nonempty subset of f1; 2; : : : ; kg and denote its complement by c. Corresponding
to , let S
 .t/ D P .Z > t; ı 2 / and S .t/ D P.min
j2 Xj > t/ be the subsurvival
and survival functions, respectively. Peterson’s formula for S .t/ as a function of
subsurvival functions is
S .t/ D ' S
 ./ ; S
c ./ I t ;
for
t  min tS; tSc
 ;
(7.5.13)
where
' .F ./ ; G ./ I t/ D exp
H t
0
dF .s/
F .s/ C G .s/
 Y
t
F .sC/ C G .sC/
F .s/ C G .s/ ;
(7.5.14)
tS D sup ft W S .t/ > 0g, and
H t
0 is the integral over the union of intervals of
points less than t for which F ./ is continuous. Q
t indicates the product over the

7.5
Other Estimation Problems
295
set fs  t W s is a jump point of Fg. Let the loss function be
L

S;bS
D
R 1
0
k S bS k2 dW .t/ ;
(7.5.15)
where kk stands for the usual norm, S D S
1; : : : ; S
k
, and bS D

bS
1; : : : ;bS
k

is an estimator of S. Suppose we have a sample of size n and let Z1; : : : ; Zn be
the minima of observations. Further assume that among them ; Z.1/ <; : : : ; < Z.m/
are m distinct ordered minima. Subsurvival functions S
 .t/ are estimated by its
natural estimator,bS
 .t/ D .1=n/ Pn
iD1IŒZi > t; ı 2  andbS .t/ D bS
 .t/ CbS
c .t/.
Combining the various estimates, Salinas-Torres et al. prove the following result.
Suppose the vector function .˛1 .s; 1/ ; : : : ; ˛k .s; 1// in s is continuous on .0; t/,
t > 0 and S .t/ and Sc .t/ have no common points of discontinuities, then, for
t  Z.m/,
bS .t/ D '

bS
;bS
cI t

D bS .t/ k .t/ exp

1
˛ .R/ C n
 X
j2c
R t
0
d˛j .s; 1/
bS .s/
(7.5.16)
is the Bayes estimator of S .t/ under the above loss function, where nj
D
Pn
iD1IŒZi  Z. j/, dj D Pn
iD1IŒZi D Z. j/; ıi D 1, j D 1; : : : ; m and
k .t/ D
Y
iWZ.i/t
Pk
jD1˛j

Z.i/; 1

C ni  di
Pk
jD1˛j

Z.i/; 1

C ni
:
(7.5.17)
They also establish strong consistency and weak convergence of the estimator.
When k D 2,  D f1g and c D f2g, it reduces to the usual right censored data
model. In this case, ˛1 .t; 1/ C ˛2 .t; 1/ D ˛ .t; 1/ for each t, and the product
bS .t/ k .t/ is analogous to Tsai’s (1986) equation (3.2). It is also similar to the
Susarla–VanRyzin estimator derived in Sect. 7.1. Likewise, if ˛j .s; 1/ ! 0, for
j D 1; 2, the estimator reduces to the PL estimator.
7.5.4
Estimation of Cumulative Hazard Rates
So far we have been dealing with the estimation of a survival function. In this section
we describe the nonparametric Bayesian estimation of a cumulative hazard rate
obtained in Hjort (1990) by assuming a beta process prior of Sect. 4.5. Let X Ï F
taking values in the discrete time scale f0; b; 2b; : : :g. without loss of generality
we take b D 1. For j D 0; 1; 2 : : : , let f . j/ D P fX D jg ; F. j/ D P fX  jg D
Pj
iD0 f.i/; h. j/ D PfX D jjX  jg D f . j/ = .1  F. j//, and cumulative hazard
rate H. j/ D Pj
iD0 h.i/. Let X1; : : : ; Xn
iidÏ F be a random sample subjected to right
censorship, and thus the data consists of Zi D min fXi; yig and ıi D IŒXi  yi, yi

296
7
Inference Based on Incomplete Data
being censoring time for the i-th individual, i D 1; : : : ; n. Let N be the counting
process of uncensored observations and M the number-at-risk process given by
N . j/ D
n
X
iD1
I
"
Zi  j and ıi D 1I and M . j/ D
n
X
iD1
IŒZi  j
#
; j  0:
(7.5.18)
Treating H as a stochastic process with independent summands and having a beta
process Befc./; H0./g prior, h. j/ is distributed as Befc. j/h0. j/; c. j/.1  h0. j//g.
Then as noted before, E.h. j// D h0. j/ D dH0. j/ is the prior guess of h. j/ and
E.H. j// D H0. j/, and Var.h. j// D h0. j/.1  h0. j//=Œc. j/ C 1 as the prior
“uncertainty.” The posterior distribution of H (discrete time version of property 4
of Sect. 4.5) is
Hjdata Ï Be

c C M;
X cdH0 C dN
c C M

;
(7.5.19)
and the nonparametric Bayesian estimator of H under the weighted quadratic loss is
given by
bH . j/ D E .Hjdata/ D
n
X
iD0
c .i/ h0 .i/ C dN .i/
c .i/ C M .i/
:
(7.5.20)
Similarly in the time-continuous case, it is shown (not to minimize the efforts
required) that the analysis leads to the estimator,
bH .t/ D
Z t
0
c .s/ dH0.s/ C dN .s/
c .s/ C M .s/
:
It is worth noting that as c ./ ! 0, these estimators reduce to the usual
nonparametric Nelson–Aalen estimator and as c ./ ! 1, it simply reduces to the
prior guess H0—properties observed earlier for the distribution functions.
7.5.5
Estimation of Hazard Rates
Assume that the hazard rate r .t/ has an extended gamma process prior (see Sect. 4.4)
 .˛ ./ ; ˇ .//. Given a sample of n observations, the posterior distributions of r .t/
was given in expression (4.4.7) for censored and in expression (4.4.8) for exact
observations.

7.5
Other Estimation Problems
297
The posterior mean of r.t/ based on m observations (censored and exact) is
^r .t/ D
R
Œ0;xm/   
R
Œ0;x1/
R
Œ0;t/
Qm
iD0ˇ .zi/ Qm
iD0d
h
˛ C Pm
jDiC1 I.xj;1/
i
.zi/
R
Œ0;xm/   
R
Œ0;x1/
Qm
iD1ˇ .zi/ Qm
iD1d
h
˛ C Pm
jDiC1 I.xj;1/
i
.zi/
:
(7.5.21)
Clearly,
^r .t/ is the Bayes estimator under the loss function
R
Œ0;1/ .r.t/
^r .t/
2
dW .t/ subject to the condition
Z
Œ0;1/
Z
Œ0;t/
ˇ2.s/d˛.s/dW .t/ < 1:
(7.5.22)
For the computational purposes, they show that the multi-dimensional integrals
in
^r .t/ can be reduced to a type that involves only one-dimensional integrals.
The Bayesian estimator of the survival function derived under the assumption
r.t/ v .˛ ./ ; ˇ .// is given in Sect. 7.3.8.
7.5.6
Markov Chain Application
Hjort (1990) extends the results of Sect. 7.5.4 to the case of nonhomogeneous
Markov Chain and obtains Bayesian estimators of transition probabilities and
cumulative hazard rates. Let X D fX .r/ W r D 0; 1; 2; : : : :g be a Markov chain with
state space f1; : : : ; kg and transition probabilities from i to j,
pij.r; s/ D P fX .s/ D jjX .r/ D ig ; 0  r  s; i; j D 1; : : : ; k:
(7.5.23)
The treatment in the last subsection corresponds to the state f1; 2g and the possible
transitions being only from state 1 to 2. Also, h . j/ corresponds to one-step
probabilities hij .s/ D pij.s  1; s/ and the cumulative hazard rate from i to j is
Hij .s/ D Ps
rD1 hij .r/, s  1. The data now available is of the form X observed
up to and including time t, and let Xt D fX .r/ W r D 0; 1; 2; : : : ; tg collected on
n individuals moving around in the state space independently of each other, each
with transition probability pij. Let the data be represented as follows. X.l/
t.l/ D
˚
X.l/ .r/ W r D 0; 1; : : : ; t .l/

, l D 1; 2; : : : ; n,
dNij .r/ D
n
X
lD1
IŒX.l/ .r  1/ D i; X.l/ .r/ D j;
(7.5.24)

298
7
Inference Based on Incomplete Data
and
Mi .r/ D
n
X
lD1
IŒX.l/ .r  1/ D i; r  t .l/; r  1:
(7.5.25)
Here, Mi .r/ is the number of individuals at risk in state i just before time r and are
subject to transition probability hij .r/ to one of k  1 states, j ¤ i, or may remain in
state i with probability hii .r/ D 1  Pk
j¤i;jD1 hij .r/. Mi .r/ does not include those
that had X.l/ .r  1/ D i, but were censored before r. The increments dNij .r/ add up
to counting processes Nij, and Nij .s/ counts the number of transitions i to j observed
in the time interval .0; s.
Assume a prior distribution for the k.k  1/ cumulative hazard rates Hij
which speciﬁes that its summands are independent and that k rows of h .r/ are
independently distributed according to a Dirichlet distribution with parameters
ci.r/h0i1.r/; : : : ; ci.r/h0ik.r/ for the i-th row. Then, E.hij .r// D h0ij .r/ and there-
fore, H0ij .s/ D Ps
rD1 h0ij .r/ is the prior guess at Hij. Then the nonparametric
Bayesian estimator for hij .r/ with respect to a quadratic loss is the posterior mean
Ohij .r/ D E

hij .r/ jdata

D ci.r/h0i1.r/ C dNij .r/
ci.r/ C Mi .r/
;
(7.5.26)
and for Hij .s/ is
bHij .s/ D
s
X
rD1
ci.r/h0i1.r/ C dNij .r/
ci.r/ C Mi .r/
; s  1:
(7.5.27)
Similarly, the Bayes estimator of waiting time distribution Fi for state i deﬁned
as Fi .s/ D PfX leaves i before time sg is obtained as
b
Fi .s/ D 1 
sY
rD1

1  ci.r/dH0;i.r/ C dNi .r/
ci.r/ C Mi .r/

:
(7.5.28)
Similar analysis in the time-continuous case leads to the estimator
bHij .t/ D
Z t
0
cij.s/dH0;ij.s/ C dNij .s/
cij.s/ C Mi .s/
:
(7.5.29)
The estimate of waiting time distribution
Gi .Œs; t/ D PfX .u/  i; u 2 Œs; t jX .s/ D ig D
Y
Œs;t
f1  dHi.u/g for  t;
(7.5.30)

7.5
Other Estimation Problems
299
is given by
bGi.Œs; t/ D
Y
Œs;t

1  cidH0;i C dNi
ci C Mi

;
(7.5.31)
where Ni D P
j¤i Nij and H0;i D P
j¤i H0;ij.
7.5.7
Estimation for a Shock Model
A problem of Bayesian analysis of shock models and wear processes using the
Dirichlet process prior was studied in Lo (1981). Suppose a device is subject to
shocks occurring randomly according to a Poisson process N D fN.t/I t 2 R} with
intensity parameter . The i-th shock inﬂicts a random amount Xi, i D 1; 2 : : : of
damage on the device. The Xi’s are assumed to be iid F deﬁned on RC. The process
is observed until a ﬁxed time T. Thus we have for the data, N.T/ the number of
shocks occurring during the time interval Œ0; T, and X1; : : : ; XN.T/, the amounts of
damages. The task is to estimate the survival probability S.t/ that the device survives
beyond time t 2 Œ0, T]. Lo considers the nonparametric Bayesian estimation
approach to this problem, and obtains the Bayes estimator for S.t/ assuming the
pair .; F/ to be independent random variables and placing a gamma G.; 
/ and a
Dirichlet process D(˛/ priors on  and F, respectively. He also derives the posterior
distribution of .; F/ given the process N.t/ up to time T, denoted by NT and
XT D .X1; : : : ; XN.T//, and shows that it has again the same structure as the prior
but with parameters updated. Also,  and F turn out to be independent as one would
expect. Symbolically,
.; F/jNT; XT  G . C N.T/; 
 C T/  D
0
@˛ C
N.T/
X
iD1
ıXi
1
A :
(7.5.32)
Now for any f, a real valued integrable function, the conditional expecta-
tion of f.; F/, given the data, i.e., EŒ f.; F/jNT; XT can be computed. In
particular, he deals with the Bayesian estimator of survival probability S.t/ D
P1
kD0Pket.t/k=kŠ, where Pk is the probability that the device survives k shocks
during the time interval Œ0; t, known as the capacity or threshold of the device.
Then the Bayes estimator of S under the loss function L.S;
^
S/ D R
R.S.t/ 
^
S.t//2dW.t/ is obtained as
^
S.t/ D EŒS.t/jNT; XT D
1
X
kD0
tk
kŠE

PkjNT; XTEŒetkjNT; XT

:
(7.5.33)

300
7
Inference Based on Incomplete Data
Using the fact that Pk depends on F only, and the fact that  and F are
independent under the posterior distribution, the estimator reduces to
^
S.t/ D
1
X
kD0
tk
kŠ
. C N.T/ C k/
. C N.T//
	
1

 C T C t

k 	

 C T

 C T C t

CN.T/
EŒPkjNT; XT:
(7.5.34)
This expression is evaluated in closed form for three particular cases of Pk. They
are (1) Pk D PfX1 C : : : C Xk  yjN.t/ D kg, the sum of the damages does not
exceed the capacity or threshold; (2) Pk D Qk
iD1F.yi/, the threshold values changes
after each shock; and (3) Pk D ŒF.y/k, the ﬁxed threshold model.
Lo (1982) also treats the problem of estimation of the intensity parameter 
of a nonhomogeneous Poisson point process based on a random sample from the
process. Assuming a weighted gamma distribution G.˛; ˇ/ as prior for , and given
a random sample N1; : : : ; Nn of n functions from this process, he shows that the pos-
terior distribution of  is again a gamma distribution G.˛ CPn
jD1 Nj; ˇ= .nˇ C 1//.
Kim (1999) on the other hand considers a more general model called the
multiplicative intensity model. A stochastic process N.t/ deﬁned on the time interval
Œ0; T is called a counting process if the sample paths are right continuous step
functions with N.0/ D 0 and having a ﬁnite number of jumps, each of size one.
It is called a multiplicative intensity model if the cumulative intensity process has
certain form. Kim derives nonparametric inference procedures for such a model.
Lo’s result may be considered as a special case of Kim’s treatment. Further Kim
adopts a semi-martingale approach instead of the Lévy measure approach for the
independent increment processes used.
7.5.8
Estimation for a Age-Dependent Branching Process
An interesting application of the Dirichlet process is given in Johnson et al. (1979)
for the Bayesian estimation of the distributions of offsprings and life-lengths in the
context of a Bellman–Harris branching process. It is based on the family tree up
to time T. Start with a population of one ancestor. Using their notation, with every
offspring x, associate one nonnegative random variable tx, the life-length, and a point
process nx, the reproduction of x. Assume the pairs .nx; tx/ as iid with probability
distribution P  G, where nx Ï P, known as offspring distribution, and tx Ï G,
the life-length distribution of the individual in the process. P is taken as a discrete
distribution
˚
pj
1
jD0, pj  0 for all j and P1
jD0pj D 1, and G a distribution function
on .0; 1/. The authors derive Bayesian estimators of P and G and give explicit
expressions as shown below.

7.5
Other Estimation Problems
301
Assume P and G to be independent having Dirichlet processes D .˛1/ and D .˛2/,
with ﬁnite non-null measures ˛1 and ˛2 as priors, respectively. The support of ˛1 is
restricted to N D f0; 1; 2; : : :g and the loss function assumed is,
L..P; G/ ; .bP; bG// D a1
1
X
jD0
W1 . j/ . pj  Opj/2 C a2
Z 1
0
.G .t/  bG .t//2dW2 .t/ ;
(7.5.35)
where a1; a2  0 and W1 and W2 are known weight functions on N and .0; 1/,
respectively. For data we have Nl .T/ D # of splits of size l in Œ0; T, l D 0; 1; 2; : : : ,
Dt1; : : : ; Dtn age at death of n individuals who died in Œ0; T, and St1; : : : ; Stm survival
times of m individuals who survived time T. Based on this data, Bayes estimators of
P and G, assuming them to be independent of each other, are obtained. As one would
expect, they have similar expression as the Bayes estimator of survival function
obtained in Sect. 7.1.
1. The Bayes estimator of P under the above loss function is given by bP D
˚
Opj

,
where
Opj D ˛1 . j/ C Nl .T/ I Œ j D l
˛1 .N/ C P1
lD0Nl .T/ :
(7.5.36)
2. The conditional distribution of PjNl .T/ ; l 2 N is D

˛1 C P1
lD0Nl .T/ ıl

.
3. Assuming ˛2 to be nonatomic, the Bayes estimator of G under the above loss
function is given by bG, where
1  bG .t/ D ˛2.t; 1/ C NC.t/
˛2.RC/ C m C n
kY
jD1
 
˛2.S
tj; 1/ C N.S
tj /
˛2.S
tj; 1/ C N.S
tj /  
j
!I
h
Stjt
i
;
(7.5.37)
where NC.t/ D # of deaths and survival times greater than t,N.t/ D # of
deaths and survival times greater than or equal to t, S
t1; : : : ; S
tk are the k distinct
observations among St1; : : : ; Stm, and 
j are their multiplicities.
Remark 7.4 As ˛1 .N/ ! 0, Opj ! MLE of pj. The Bayes estimator under the
squared error loss of the mean M of the offspring distribution P is given by
b
M D
P1
lD0l˛1 .flg/ C P1
lD0lNl .T/
˛1 .N/ C P1
lD0Nl .T/
:
(7.5.38)
Remark 7.5 The estimator 1bG .t/ looks similar to the Susarla-Van Ryzin estimator
of the survival function, but two vital differences were noted by the authors. In
that estimator, the total sample size n which includes censored and uncensored
observations is a known constant and ﬁxed ahead of the sampling. Here n and m
are random variables. Second, in that treatment, censoring times were taken to be

302
7
Inference Based on Incomplete Data
independent of the survival times. Here the censoring random variables associated
with St1; : : : ; Stm are not independent of the random life-times Dt1; : : : ; Dtn.
The authors also treat the case when P and G are not independent, and the prior
for the pair .P; G/ is taken to be the Dirichlet process deﬁned on the product space
NRC, and point out that the estimator Opj in this case not only depends on the splits
nx but also on the life-lengths and survival times.
7.6
Hypothesis Testing H0 W F  G
Earlier in Sect. 6.9.1, hypothesis testing relative to the null hypothesis H0 W F  F0
against the alternative H1 W F — F0 was considered from a decision theoretic point
of view. In this section we consider its two-sample analog when the data is right
censored. The case of uncensored data can easily be handled as a special case.
In the non-Bayesian context, Gehan (1965) had obtained procedures to test the
hypothesis H0 W F D G against one- and two-sided alternatives for the censored
data. His test statistic was a natural extension of Wilcoxon–Mann–Whitney statistic.
Efron (1967) produced an alternative test statistic as an improvement over Gehan’s
statistic. Phadia and Susarla (1979) used a decision theoretic approach for this
testing problem. The statistic that emerged is quite different from those of Gehan
(1965) and Efron (1967).
Using the same notations as earlier in the two-sample problem (see Sect. 7.5.2),
we want to test the hypothesis H0 W F  G against H1 W F — G based on the data
fS; ı; T; 
g. F and G are considered as right sided distribution functions. The loss
function used here is an appropriate modiﬁcation of that used in one sample case
(see Sect. 6.9). L..F; G/; a0/ D R .F  G/CdW and L..F; G/; a1/ D R .F  G/dW,
where L..F; G/; ai/ indicates the loss when action ai (deciding in favor of Hi) is
taken for i D 0; 1; W is a weight function, aC D maxfa; 0g and a D  minfa; 0g
for any a 2 R, as before. Assume F and G to have Dirichlet process priors with
parameters ˛1 and ˛2, respectively. Let n D Pf taking action a0 j data g. Then the
Bayes rule with respect to these priors is given by the test statistic
n D IŒ .˛1; ˛2/  0;
(7.6.1)
where
 .˛1; ˛2/ D E Œ L..F; G/; a0/  L..F; G/; a1/ j data
D
Z
.bF˛1.u/  bG˛2.u//dW.u/;
(7.6.2)

7.7
Estimation in Presence of Covariates
303
and bF˛1 and bG˛2 are the usual Bayes estimators of F and G derived earlier. The
minimum Bayes risk against the Dirichlet process priors is
r
n .˛1; ˛2/ D inf
n .˛1; ˛2; n/ D EŒIŒ .˛1; ˛2/  0 .˛1; ˛2/ C E ŒL..F; G/; a1/;
(7.6.3)
which can easily be evaluated. When ˛1 and ˛2 are unknown, the empirical Bayes
method of earlier sections was recommended. In that case the test statistics n is
replaced at the .n C 1/-stage by
^
nC1 with  replaced by
^
 nC1 given by
^
 nC1 ./ D
R
.bF^˛1.u/  bG^˛2.u//dW.u/, where as before,
^˛1 and
^˛2 given in (7.5.4) may be
used. It is shown that the above test statistic is asymptotically optimal with the rate
of convergence 0.n
1
2 /.
Damien and Walker
(2002) present a different approach for comparing two
treatment effects in which the Bayes Factor is used.
7.7
Estimation in Presence of Covariates
Most of the analysis presented for the censored data can be extended to incorporate
regressor variables. To accommodate covariates in the analysis of right censored
survival data, a common practice in the non-Bayesian context is to use the Cox’s
model. Kalbﬂeisch (1978) and Wild and Kalbﬂeisch (1981) initiated this approach
in the Bayesian framework
Suppose we have positive survival times Xi’s distributed according to a distribu-
tion function F and is associated with a set of covariates W
T
i D .Wi1; : : : ; Wik/, a
transpose of the column vector. The Cox’s model, also known as the proportional
hazard model, is expressed as S .tI W/ D S0 .t/exp.ˇW/, where ˇ is a row vector of
regression coefﬁcients and S0 is the baseline survival function, or in terms of hazard
rates, as  .tjw/ D 0 .t/ exp .ˇW/. The main interest in covariate data analysis
centers around the estimation and hypothesis testing of ˇ, and in such cases, S0 may
be regarded as a nuisance parameter. On the other hand, one may be interested in
the estimation of S0 itself.
As stated in Chap. 4, Kalbﬂeisch considered these problems in a Bayesian
framework. Writing S0 .t/ D eH.t/, it is immediately clear that H .t/ may be viewed
as a nondecreasing process with independent nonnegative increments. Thus with
H .0/ D 0, and as t ! 1, H .t/ ! 1, the theory of the nondecreasing processes
with independent increments can be used. For the task of covariate analysis,
Kalbﬂeisch treats H .t/ as a nuisance parameter having a certain prior distribution
and carried out the estimation of ˇ by determining the marginal distribution of
observations as a function of ˇ having H .t/ eliminated. For the prior it was
convenient to choose speciﬁcally a gamma process with parameters cH0 .t/ and c,
so that E .H .t// D H0 .t/ and Var .H .t// D H0 .t/ =c. The parameter H0 serves as

304
7
Inference Based on Incomplete Data
a prior guess at H, and c the precision parameter. To avoid difﬁculties of prior ﬁxed
points of discontinuities, he assumed H0 to be absolutely continuous. The gamma
process is easy to handle in deriving the posterior distribution of H .t/ given the
observations, and thus one can easily compute E

eƒ.t/jobservations

for a ﬁxed ˇ.
Following the paper of Ferguson and Phadia (1979), Wild and Kalbﬂeisch (1981)
placed the above approach in a more general setting by using the processes neutral
to the right as priors for H .t/ instead of the gamma process. By proposing a simple
adjustment in the derivations given by Ferguson and Phadia, they were able to
extend Ferguson and Phadia’s treatment to the regression analysis problem. If we
let Yt D  log S .t/ and Y0t D  log S0 .t/, then Yt D Y0teˇW. Since eˇW is treated
as nonrandom, it is easy to see that if F0 D 1  S0 is neutral to the right, so
is F D 1  S. Now the theorems of Ferguson and Phadia are applicable with
the following variations. The posterior density of an increment in Y0t is obtained
by multiplying the prior density, say, dG.y/ with exp.yeˇW/ and normalizing;
likewise the distribution of jump at x should be adjusted. Similarly, if the given
observation is censored, the posterior distribution of an increment changes only to
the left of x and is found by multiplying the prior density by exp.yeˇW/ (instead
of exp.y/) and renormalizing.
The full description of the posterior distribution for the sample size n D 1 can be
reformulated as follows:
Theorem 7.6 (Wild and Kalbﬂeisch) Let F0 be a random distribution neutral to
the right. Then given an observation X D x or X > x from F, the posterior
distribution of F0 is also neutral to the right.
For X D x,
(i) the posterior distribution of an increment in Y0t to the right of x is the same as
the prior distribution.
(ii) An increment Y0t  Y0s for s < t left of x with a prior density dG .y/ has the
posterior density proportional to exp.yeˇW/dG .y/.
(iii) There is a jump discontinuity J D Y0x  Y
0x at x, in the posterior whether there
was one in the prior or not. If the prior density of J is assumed to be dGx .s/,
then it has the posterior density proportional to (1  exp.seˇW//dGx .s/.
For the case X > x,
(i) the posterior distribution of an increment in Y0t to the right of x is the same as
the prior distribution.
(ii) The posterior distribution of an increment Y0t  Y0s for s < t  x has the
description same as in (iii) above.
For the general case of sample size n > 1, it is convenient to derive a formula
for the posterior MGF. It is essentially the same as in Ferguson and Phadia except
that now with the adjustments mentioned above, instead of counting the number of
observations at a point or beyond a point, we count the exponential scores eˇWi.

7.7
Estimation in Presence of Covariates
305
Let u1; : : : ; uk be the distinct values among x1; : : : ; xn ordered so that u1 <
u2 < : : : < uk. Let C .i/ and D .i/ , respectively, be the sets of labels of censored
and uncensored observations at ui, R .i/ be the set of labels of all observations
that are greater than ui, Gui .s/ be the prior distribution of a jump, J in Y0t at
ui, Hui .s/ be the posterior distribution of J given that a failure occurred at ui,
Mt.
/ D E .exp .Y0t
// denote the MGF of Y0t, and M
t .
/ denote the MGF of
Y
t , M
t .
/ D lims!t Ms.
/, s < t. Then we have
Theorem 7.7 (Wild and Kalbﬂeisch) Let F0 be a random distribution function
neutral to the right, and let X1; : : : ; Xn, be a sample of independent observations
such that Xi is distributed according to Fi D 1  .1  F0 .t//exp.ˇWi/, where Wi is
the associated vector of covariates for Xi. Then the posterior distribution of F0 given
the data is neutral to the right, and Y0t has posterior MGF
Mt.
 j data/ D Mt.
 C hj.t//
Mt.hj.t//

j.t/
Y
iD1
"
M
ui.
 C hi1/
M
ui.hi1/
 Cui.
 C hi C i; di/
Cui.hi C i; di/

Mui.hi/
Mui.
 C hi/
#
;
(7.7.1)
where hi D P
l2R.i/ exp .ˇWl/, i D P
l2C.i/ exp .ˇWl/ and di is the number of
observations in D .i/.
If ui is a prior ﬁxed point of discontinuity of Y0t, then
Cui.˛; di/ D
Z 1
0
e˛z Y
l2D.i/

1  exp

zeˇWl
d Gui.z/;
(7.7.2)
where the product is taken over di observations in D .i/ I while if ui is not a prior
ﬁxed point of discontinuity of Y0t, then
Cui.˛; di/ D
8
<
:
1
if di D 0
R 1
0
e˛zd Hui.z/
if di D 1
R 1
0
e˛zQf1  exp

zeˇWl
gd Hu.z/ if di > 1
;
(7.7.3)
where the product is taken over di1 observations. (Note that one observation is
needed to generate a ﬁxed point of discontinuity at ui (see Ferguson and Phadia
1979).

306
7
Inference Based on Incomplete Data
In application, difﬁculties are encountered in evaluating the posterior distribution
Hu of a jump at u where a single observation fell. However, as noted earlier, for
certain speciﬁc processes neutral to the right it is relatively simple. The gamma
process prior is one such process and Wild and Kalbﬂeisch evaluate the Bayes
estimator of F0 in this particular case. For the gamma process prior, the independent
increments of the process Y0t have gamma distributions with shape parameter  .t/
and intensity parameter . Since for this homogeneous process there are no prior
ﬁxed points of discontinuities, we need to consider only the second part of the above
formula. For this case, an application of Theorem 5 of Ferguson and Phadia for the
gamma process yields
Cui.˛ C 1; di/
Cui.˛; di/
D
8
ˆˆ<
ˆˆ:
1
if di D 0
log ˛C1C exp.ˇW/
˛C1
= log ˛C exp.ˇW/
˛
if di D 1
R 1
0
e.˛C1/zQ
l2D.i/f1exp.zeˇWl/gz1d z
R 1
0
e˛zQ
l2D.i/f1exp.zeˇWl/gz1d z
if di > 0:
(7.7.4)
Further noting that the MGF of the gamma process is Mt.
/ D .= . C 
//.t/,
and putting the various quantities together in the formula for the posterior MGF, they
obtain an expression for the Bayes estimator similar to (3.15) in Ferguson–Phadia
paper.
E.1  F.t/jdata/ D Mt.1jdata/ D
	
hj.t/ C ﬁ
hj.t/ C ﬁC 1

‚.t/

j.t/
Y
iD1
"	.hi1 C / .hi C  C 1/
.hi1 C  C 1/ .hi C /

.ui/
 Cui .hi C i C  C 1; di/
Cui .hi C i C ; di/

:
(7.7.5)
The difference is in the quantities
hi D
X
l2R.i/
exp .ˇWl/ ;
i D
X
l2C.i/
exp .ˇWl/ ;
(7.7.6)
and the ratio of Cui’s (as deﬁned above) is used in place of the ratios of 'G’s.
Burridge (1981) extends the above analysis to group data. Clayton (1991) devel-
ops computational procedures for the results. His model assumes the increments
in the cumulative hazard to be nonnegative and independent in disjoint intervals
and uses the gamma process to model the baseline cumulative hazard function. This
approach has the disadvantage of highly discrete and independent hazards in disjoint
intervals. Sinha (1998) presents an analysis of using a correlated prior process for
the baseline hazard.

7.7
Estimation in Presence of Covariates
307
In his paper on Beta process, Hjort (1990) extends the covariate analysis
by recasting the Cox model in terms of hazard functions as 1  dAi.s/
D
f1  dA.s/gexp.ˇwi/, where A is the cumulative hazard function and dAi.s/ is the haz-
ard function of the i-th individual. By assuming ˇ known and A  Be fc ./ ; A0.s/g,
he shows that the posterior distribution of A given the data is a process with
independent increments and is distributed again like a beta process between jumps.
But at jumps the distribution is somewhat complicated. Using this approach, he
derives the Bayes estimator for A under the weighted squared error loss. His work
parallels the work of Wild and Kalbﬂeisch (1981) but the difference is that these
authors found it necessary to assume the covariates to be constant in time, whereas
in his derivation they can be time-dependent.

References
Aalen, O. O. (1978). Nonparametric inference for a family of counting processes. Annals of
Statistics, 6, 701–726.
Ammann, L. P. (1984). Bayesian nonparametric inference for quantal response data. Annals of
Statistics, 12, 636–645.
Ammann, L. P. (1985). Conditional Laplace transforms for Bayesian nonparametric inference in
reliability theory. Stochastic Processes and Their Applications, 20, 197–212.
Antoniak, C. (1974). Mixtures of Dirichlet processes with applications Bayesian nonparametric
problems. Annals of Statistics, 2, 1152–1174.
Barlow, R. E., Barthalomew, D. J., Bremner, J. M., & Brunk, H. D. (1972). Statistical inference
under order restrictions. New York: Wiley.
Basu, D., & Tiwari, R. C. (1982). A note on the Dirichlet process. In G. Kallianpur, R. Krishnaiah,
& J. K. Ghosh (Eds.) Statistics and probability: Essays in honor of C. R. Rao (pp. 89–103).
Berry, D. A., & Christensen, R. (1979). Empirical Bayes estimation of a binomial parameter via
mixtures of Dirichlet process. Annals of Statistics, 7, 558–568.
Bhattacharya, P. K. (1981). Posterior distribution of a Dirichlet process from quantal response data.
Annals of Statistics, 1, 356–358.
Billingsley, P. (1968). Convergence of probability measures. New York: Wiley.
Binder, D. A. (1982). Nonparametric Bayesian models for samples from ﬁnite populations.
Journal of the Royal Statistical Society B, 44, 388–393.
Blackwell, D. (1973). Discreteness of Ferguson selections. Annals of Statistics, 1 , 356–358.
Blackwell, D., & MacQueen, J. B. (1973). Ferguson distributions via Polya urn schemes. Annals
of Statistics, 9 , 803–811.
Blei, D. M., & Frazier, P. I. (2011). Distant dependent Chinese restaurant processes. Journal of
Machine Learning Research, 12 , 2461–2488.
Blei, D. M., Ng, A. Y., & Jordan, M. I. (2003). Latent Dirichlet allocation. Journal of Machine
Learning Research, 3 , 993–1022.
Blum, J., & Susarla, V. (1977). On the posterior distribution of a Dirichlet process given randomly
right censored observations. Stochastic Processes and Their Applications, 5 , 207–211.
Bondesson, L. (1982). On simulation from inﬁnitely divisible distributions. Advances in Applied
Probability, 14, 855–869.
Breth, M. (1978). Bayesian conﬁdence bands for a distribution function. Annals of Statistics, 6 ,
649–657.
Breth, M. (1979). Nonparametric Bayesian interval estimation. Biometrika, 66, 641–644.
Broderick, T., Jordan, M. L., & Pitman, J. (2012). Beta processes, Stick-breaking and power laws.
Bayesian Analysis, 7, 439–476.
© Springer International Publishing Switzerland 2016
E.G. Phadia, Prior Processes and Their Applications, Springer Series in Statistics,
DOI 10.1007/978-3-319-32789-1
309

310
References
Broderick, T., Jordan, M. L., & Pitman, J. (2013). Cluster and feature modeling from combinatorial
stochastic processes. Statistical Science, 28(3), 289–312.
Bulla, P., Muliere, P., & Walker, S. (2007). Bayesian nonparametric estimation of a bivariate
survival function. Statistica Sinica, 17, 427–444.
Bulla, P., Muliere, P., & Walker, S. (2009). A Bayesian nonparametric estimator of a multivariate
survival function. Journal of Statistical Planning and Inference, 139, 3639–3648.
Burridge, M. (1981). Empirical Bayes analysis of survival data. Journal of the Royal Statistical
Society B, 43, 65–75.
Campbell, G., & Hollander, M. (1978). Rank order estimation with the Dirichlet prior. Annals of
Statistics, 6(1), 142–153.
Caron, F., Davy, M., & Doucet, A. (2007). Generalized Polya urn for time-varying Dirichlet process
mixtures. In Proceedings of the Conference on Uncertainty in Artiﬁcial Intelligence (Vol. 23).
Christensen, R., Hanson, T., & Jara, A. (2008). Parametric nonparametric statistics: An introduc-
tion to mixtures of ﬁnite Polya trees. Annals of Statistics, 62, 296–306.
Chung, Y., & Dunson, D. B. (2011). The local Dirichlet process.
Annals of the Institute of
Statistical Mathematics, 63 , 59–80.
Cifarelli, D. M., & Regazzini, E. (1979). Considerazioni generali sull’impostazione bayesiana di
problemi non parametrici, Rivista di matematica per le Scienze Economicje e Socialli, 2, Part I
39–52, Part II 95–111.
Clayton, M. K. (1985). A Bayesian nonparametric sequential test for the mean of a population.
Annals of Statistics, 13 1129–1139.
Clayton, M. K. (1991). A Monte Carlo method for Bayesian inference in frailty models.
Biometrika, 47, 467–485.
Clayton, M. K., Berry, D. (1985). Bayesian nonparametric bandits. Annals of Statistics, 13, 1523–
1534.
Connor, R. J., & Mosimann, J. E. (1969). Concept of independence for proportions with a
generalization of the Dirichlet distribution. Journal of the American Statistical Association,
64 , 194–206.
Dabrowska, D. M. (1988). Kaplan-Meier estimate on the plane. Annals of Statistics, 15, 1475–
1489.
Dalal, S. R. (1979a). Dirichlet invariant processes and applications to nonparametric estimation of
symmetric distribution functions. Stochastic Processes and Their Applications, 9, 99–107.
Dalal, S. R. (1979b). Nonparametric and Robust Bayes estimation of location. In Optimizing
methods in statistics (pp. 141–166). New York: Academic.
Dalal, S. R., & Hall, G. J. (1980). On approximating parametric Bayes models by nonparametric
Bayes models. Annals of Statistics, 8, 664–672.
Dalal, S. R., & Phadia, E. G. (1983). Nonparametric Bayes inference for concordance in Bivariate
distributions. Communications in Statistics - Theory & Methods, 12(8), 947–963.
Damien, P., Laud, P. W., & Smith, A. F. M. (1995). Random variate generation form inﬁnitely
divisible distributions with applications to Bayesian inference. Journal of the Royal Statistical
Society B, 57, 547–64.
Damien, P., Laud, P. W., & Smith, A. F. M. (1996). Implementation of Bayesian non-parametric
inference based on beta processes. Scandinavian Journal of Statistics, 23, 27–36.
Damien, P., & Walker, S. (2002). A Bayesian nonparametric comparison of two treatments.
Scandinavian Journal of Statistics, 29, 51–56.
DeIorio, M., Müller, P., Rosner, G .L., & MacEachern, S. N. (2004). An anova model for dependent
random measures. Journal of the American Statistical Association, 99, 205–215.
Dey, J., Erickson, R. V., & Ramamoorthi, R. V. (2003). Some aspects of neutral to right priors.
International Statistical Review, 71(2), 383–401.
Dey, D., Müller, P., & Sinha, D. (Eds.). (1998). Practical nonparametric and semiparametric
Bayesian statistics. Lecture notes in statistics. New York: Springer.
Diaconis, P., & Freedman, D. A. (1986). On inconsistent of Bayes estimates of location. Annals of
Statistics, 14, 68–87.

References
311
Doksum, K. A. (1972). Decision theory for some nonparametric models. Proceedings of the Sixth
Berkeley symposium on Mathematical Statistics and Probability, Vol. I: Theory of Statistics
(pp. 331–343).
Doksum, K. A. (1974). Tailfree and neutral random probabilities and their posterior distribu-
tions.Annals of Probability, 2 , 183–201.
Doss, H. (1984). Bayesian estimation in the symmetric location problem. Zeitschrift für
Wahrscheinlichkeitstheorie und verwandte Gebiete, 68, 127–147.
Doss, H. (1985a). Bayesian nonparametric estimation of the median: Part I: computation of the
estimates. Annals of Statistics, 13, 1432–1444.
Doss, H. (1985b). Bayesian nonparametric estimation of the median: Part II: Asymptotic properties
of the estimates. Annals of Statistics, 13, 1445–1464.
Doss, H. (1994). Bayesian nonparametric estimation for incomplete data via successive substitu-
tion sampling. Annals of Statistics, 22, 1763–1786.
Dråghici, L., & Ramamoorthi, R. V. (2000). A note on the absolute continuity and singularity of
Polya tree priors and posteriors. Scandinavian Journal of Statistics, 27, 299–303.
Duan, J. A., Guindani, M., & Gelfand, A. E. (2007). Generalized spatial Dirichlet process model.
Biometrika, 94, 809–825.
Dubins, L. E., & Freedman, D. A. (1966). Random distribution functions. Proceedings of the Fifth
Berkeley Symposium on Mathematical Statistics and Probability (Vol. 2, pp. 183–214).
Dunson, D. B. (2006). Bayesian dynamic modeling of latent trait distributions. Biostatistics, 7(4),
551–568.
Dunson, D. B., & Park, J. H. (2008). Kernel Stick-breaking processes. Biometrika, 95, 307–323.
Dykstra, R. L., & Laud, P. (1981). A Bayesian nonparametric approach to reliability. Annals of
Statistics, 9 , 356–367.
Engen, S. (1975). A note on the geometric series as a species frequency model. Biometrika, 62,
697–699.
Engen, S. (1978). Stochastic Abundance Models with emphasis on biological communities and
species diversity. London: Chapman and Hall.
Ewens, W. J. (1972). The sampling theory of selectively neutral alleles. Theoretical Population
Biology, 3, 87–112.
Escobar, M. D. (1994). Estimating normal means with a Dirichlet process prior. Journal of the
American Statistical Association, 89, 268–277.
Escobar, M. D., & West, M. (1995). Bayesian density estimation and inference using mixtures.
Journal of the American Statistical Association, 90, 577–588.
Efron, B. (1967). The two sample problem with censored data. In Proceedings of the Fifth Berkeley
Symposium on Mathematical Statistics and Probability (Vol. 4, pp. 831–853).
Fabius, J. (1964). Asymptotic behavior of Bayes estimates. Annals of Mathematical Statistics, 35 ,
846–856.
Fabius, J. (1973). Neutrality and Dirichlet distributions. In Transactions of the 6th Prague
Conference on Information Theory, Statistical Decision Functions and Random Processes
(pp. 175–181).
Favaro, S., & Teh, Y. W. (2013). MCMC for normalized random measure mixture models.
Statistical Science, 28, 335–359.
Feller, W. (1966). An introduction to probability theory and its applications (Vol. II). New York:
Wiley.
Ferguson, T. S. (1973). A Bayesian analysis of some nonparametric problems. Annals of Statistics,
1 , 209–230.
Ferguson, T. S. (1974). Prior distributions on spaces of probability measures. Annals of Statistics,
2, 615–629.
Ferguson, T. S. (1982). Sequential estimation with Dirichlet process priors. In S. Gupta & J. Berger
(Eds.). Statistical decision theory and related topics III (Vol. 1 , pp. 385–401).
Ferguson, T. S. (1983). Bayesian density estimation by mixtures of normal distributions. In H.
Rizvi & J. S. Rustagi (Eds.). Recent advances in statistics (pp. 287–302). New York: Academic.

312
References
Ferguson, T. S., & Klass, M. J. (1972). A representation of independent increment processes
without Gaussian components. Annals of Mathematical Statistics, 43 , 1634–1643.
Ferguson, T. S., & Phadia, E. G. (1 979). Bayesian nonparametric estimation based on censored
data. Annals of Statistics, 7 , 163–186.
Ferguson, T. S., Phadia, E. G., & Tiwari, R. C. (1992 ). Bayesian nonparametric inference. In M.
Ghosh & P. K. Pathak (Eds.). Current issues in statistical inference: Essays in honor of D.
Basu. IMS lecture notes-monograph series (Vol. 17, pp. 127–150).
Foti, N. J., Futoma, J. D., Rockmore, D. N., & Williamson, S. (2012). A unifying representation
for a class of dependent random measures. arXiv:1211.475v1[stat.ML]
Freedman, D. A. (1963). On the asymptotic behavior of Bayes estimates in the discrete case. Annals
of Mathematical Statistics, 34 , 1386–1403.
Gardiner, J. C., & Susarla, V. (1981). A nonparametric estimator of the survival function under
progressive censoring. In J. Crowley & R. A. Johnson (Eds.). Survival analysis. IMS lecture
notes-monograph series (Vol. 2, pp. 26–40).
Gardiner, J. C., & Susarla, V. (1983). Weak convergence of a Bayesian nonparametric estimator of
the survival function under progressive censoring. Statistics and Decision, 1, 257–263.
Gehan, E. A. (1965). A generalized Wilcoxon test for comparing arbitrarily singly-censored
samples. Biometrika, 52, 203–213.
Gelfand, A. E., & Smith, A. F. M. (1990). Sampling-based approaches to calculating marginal
densities. Journal of the American Statistical Association, 85, 398–409.
Gelfand, A. E., Kottas, A., & MacEachern, S. N. (2005). Bayesian nonparametric spatial modeling
with Dirichlet process mixing. Journal of the American Statistical Association, 100, 1021–
1035.
Ghosh, J. K., & Ramamoorthi, R. V. (2003). Bayesian nonparametric .Springer series in statistics.
New York: Springer.
Ghosh, J. K., Hjort, N. L., Messan, C., & Ramamoorthi, R. V. (2006). Bayesian bivariate survival
estimation. Journal of Statistical Planning and Inference, 136, 2297–2308.
Ghosh, M. (1985). Nonparametric empirical Bayes estimation of certain functionals. Communica-
tions in Statistics - Theory & Methods, 14(9), 2081–2094.
Ghosh, M., Lahiri, P., & Tiwari, R. C. (1989). Nonparametric empirical Bayes estimation of the
distribution and the mean. Communications in Statistics - Theory & Methods , 18(1), 121–146.
Ghorai, J. K. (1981). Empirical Bayes estimation of a distribution function with a gamma process
prior. Communications in Statistics - Theory & Methods, A10(12), 1239–1248.
Ghorai, J. K. (1989). Nonparametric Bayesian estimation of a survival function under the
proportional hazard model. Communications in Statistics - Theory & Methods, A18(5), 1831–
1842.
Ghorai, J. K., & Susarla, V. (1982). Empirical Bayes estimation of probability density function
with Dirichlet process prior. In W. Grossmann, et al. (Eds.). Probability and statistical inference
(pp. 101–114). Dordrecht: D. Reidel Publishing Company.
Gnedin, A., & Pitman, J. (2007). Poisson representation of a Ewens fragmentation process.
Combinatorics, Probability and Computing, 16, 819–827.
Grifﬁn, J. E., & Steel, M. F. J. (2006). Order-based dependent Dirichlet processes. Journal of the
American Statistical Association, 101, 179–194.
Grifﬁths, R. C. (1980). Allele frequencies in multidimensional Wright-Fisher models with a
general symmetric mutation structure. Theoretical Population Biology, 17(1), 51–70.
Grifﬁths, T. L., & Ghahramani, Z. (2006). Inﬁnite latent feature models and the Indian buffet
process. In Advances in neural information processing systems (Vol. 18). Cambridge, MA:
MIT.
Ghahramani, Z., Grifﬁths, T. L., & Sollich, P. (2007). Bayesian nonparametric latent feature models
(with discussion and rejoinder). In J. M. Bernado, et al. (Eds.). Bayesian statistics (Vol. 8).
Oxford, UK: Oxford University Press.
Grifﬁths, T. L., & Ghahramani, Z. (2011). The Indian buffet process: An introduction and review.
Journal of Machine Learning Research, 12, 1185–1224.

References
313
Gross, A. J., & Clark, V. A. (1975). Survival distributions. Reliability applications in biomedical
sciences. New York: Wiley.
Hall, G. J., Jr. (1976). Sequential search with random overlook probabilities. Annals of Statistics,
4, 807–816.
Hall, G. J., Jr. (1977). Strongly optimal policies in sequential search with random overlook
probabilities. Annals of Statistics, 5, 124–135.
Hannah, L. A., Blei, D. M., & Powell, W. B. (2011). Dirichlet process mixtures of general linear
models. Journal of Machine Learning Research, 12, 1923–1953.
Hannum, R. C., & Hollander, M. (1983). Robustness of Ferguson’s Bayes estimator of a
distribution function. Annals of Statistics, 11, 632–639, 1267.
Hannum, R. C., Hollander, M., & Langberg, N. A. (1981). Distributional results for random
functionals of a Dirichlet process. Annals of Probability, 9, 665–670.
Hansen, B., & Pitman, J. (2000). Prediction rules for exchangeable sequences related to species
sampling. Statistics & Probability Letters, 46, 251–256.
Hanson, T. E. (2006). Inference for mixtures of ﬁnite Polya tree models. Journal of the American
Statistical Association, 101 , 1548–1565.
Hanson, T. E. (2007). Polya trees and their use in reliability and survival analysis. In Encyclopedia
of statistcs in quality and reliability (pp. 1385–1390). New York: Wiley.
Hanson, T. E., Branscum, A., & Gardner, I. (2008). Multivariate mixtures of Polya trees for
modelling ROC data. Statistical Modelling, 8, 81–96.
Hanson, T. E., & Johnson, W. O. (2002). Modeling regression error with a mixture of Polya trees.
Journal of the American Statistical Association, 97 , 1020–1033.
Hjort, N. L. (1985). Bayesian Nonparametric Bootstrap Conﬁdence Intervals. NSF-and LCS-
Technical Report, Department of Statistics, Stanford University.
Hjort, N. L. (1990). Nonparametric Bayes estimators based on Beta processes in models for life
history data. Annals of Statistics, 18(3), 1259–1294.
Hjort, N. L., Homes, C., Müller, P., & Walker, S. G. (2010). Bayesian nonparametrics. Cambridge
series in statistical and probabilistic mathematics. Cambridge: Cambridge University Press.
Hollander, M., & Korwar, R. M. (1976). Nonparametric empirical Bayes estimation of the
probability that X  Y. Communications in Statistics - Theory & Methods, A5(14), 1369–1383.
Hollander, M., & Korwar, R. M. (1982). Nonparametric Bayesian estimation of the horizontal
distance between two populations. In Nonparametric statistical inference (Vol. 1). New York:
North Holland.
Ishwaran, H., & James, L. F. (2001). Gibbs sampling methods for Stick-breaking priors. Journal
of the American Statistical Association, 96 , 161–173.
Ishwaran , H., & James, L. F. (2003). Generalized weighted Chinese restaurant processes for
species sampling mixture models. Statistica Sinica, 13, 1211–1235.
Ishwaran , H., & Zarepour, M. (2000). Markov chain Monte Carlo in approximate Dirichlet and
beta two-parameter process hierarchical models. Biometrika, 87, 371–390.
Ishwaran , H., & Zarepour, M. (2003). Exact and approximate sum representations for the Dirichlet
process. Canadian Journal of Statistics, 30 , 269–283.
Ishwaran , H., & Zarepour, M. (2003). Random probability measures via Polya sequences:
Revisiting the Blackwell-MacQueen Urn scheme. airXiv:Math/0309041v1.
Ibrahim, J. L., Chen, M., & Sinha, D. (2001). Bayesian survival analysis . New York: Springer.
Jain, S., & Neal, R. (2004). A split-merge Markov chain monte Carlo procedure for the Dirichlet
process mixture model. Journal of Computational and Graphical Statistics, 13, 158–182.
James, L. F. (2006). Poisson calculus for spatial neutral to the right processes. Annals of Statistics,
34 , 416–440.
Johnson, N. L., & Kotz, S. (1970). Distributions in statistics-continuous multivariate distributions.
New York: Wiley.
Johnson, N. L., Kotz, S., & Balkrishnan, N. (1997). Multivariate Ewens distribution. In Discrete
multivariate distributions (Chap. 41, pp. 232–246). New York: Wiley.
Johnson, R. A., Susarla, V., & Van Ryzin, J. (1979). Bayesian non-parametric estimation for age-
dependent branching processes. Stochastic Processes and Their Applications, 9, 307–318.

314
References
Jordan, M. I. (2010). Hierarchical models, nested models and completely random measures. In M.-
H. Chen, D. Dey, P. Mueller, D. Sun, & K. Ye (Eds.), Frontiers of statistical decision making
and Bayesian analysis: In honor of James O. Berger. New York: Springer.
Kalbﬂeisch, J. D. (1978). Nonparametric Bayesian analysis of survival data. Journal of the Royal
Statistical Society B, 40, 214–221.
Kalbﬂeisch, J. D., & Prentice, R. L. (1980). The statistical analysis of failure time data. New York:
Wiley.
Kaplan, E. L., & Meier, P. (1958). Nonparametric estimation from incomplete observations.
Journal of the American Statistical Association, 53, 457–481.
Kim, Y. (1999). Nonparametric Bayesian estimators for counting processes. Annals of Statistics,
27, 562–588.
Kingman, J. F. C. (1967). Completely random measures. Paciﬁc Journal of Mathematics, 21, 59–
78.
Kingman, J. F. C. (1975). Random discrete distributions. Journal of the Royal Statistical Society
B, 75, 1–22.
Kingman, J. F. C. (1993). Poisson processes. Oxford: Clarendon Press.
Korwar, R. M. & Hollander, M. (1973). Contributions to the theory of Dirichlet processes. Annals
of Probability, 1 , 705–711.
Korwar, R. M., & Hollander, M. (1976). Empirical Bayes estimation of a distribution function.
Annals of Statistics, 4 , 581–588.
Kraft, C. H. (1964). A class of distribution function processes which have derivatives. Journal of
Applied Probability, 1, 385–388.
Kraft, C. H., & van Eeden, C. (1964). Bayesian bioassay. Annals of Mathematical Statistics, 35 ,
886–890.
Kuo, L. (1986a). A note on Bayes empirical Bayes estimation by means of Dirichlet processes.
Statistics & Probability Letters , 4, 145–150.
Kuo, L. (1986b). Computations of mixtures of Dirichlet processes. SIAM Journal on Scientiﬁc
Computing, 7, 60–71.
Kuo, L. (1988). Linear Bayes estimators of the potency curve in bioassay. Biometrika, 75 , 91–96.
Lavine, M. (1992). Some aspects of Polya tree distributions for statistical modelling. Annals of
Statistics, 20 , 1222–1235.
Lavine, M. (1994). More aspects of Polya trees for statistical modelling. Annals of Statistics, 22 ,
1161–1176.
Lijoi, A. , & Prünster, I. (2010). Models beyond the Dirichlet process. In N. L. Hjort, et al.
(Eds.). Bayesian nonparametrics. Cambridge series in statistical and probabilistic mathematics
(pp. 80–136).
Lin, D., Grimson, E., & Fisher, J. (2010). Construction of dependent Dirichlet processes based on
Poisson processes. In Neural Information Processing Systems.
Lo, A. Y. (1981). Bayesian nonparametric statistical inference for shock models and wear
processes. Scandinavian Journal of Statistics, 8, 237–242.
Lo, A. Y. (1982). Bayesian nonparametric statistical inference for Poisson point processes.
Zeitschrift für Wahrscheinlichkeitstheorie und verwandte Gebiete, 59, 55–66.
Lo, A. Y. (1983). Weak convergence for Dirichlet processes. Sankhya, 45, 105–111.
Lo, A. Y. (1984). On a class of Bayesian nonparametric estimates; I. Density estimates. Annals of
Statistics, 12 , 351–357.
Lo, A. Y. (1986). Bayesian statistical inference for sampling a ﬁnite population. Annals of
Statistics, 14, 1226–1233.
Lo, A. Y. (1987). A large sample study of the Bayesian bootstrap. Annals of Statistics, 15(1), 360–
375.
Lo, A. Y. (1988). A Bayesian bootstrap for a ﬁnite population. Annals of Statistics, 16, 1684–1695.
Lo, A. Y. (1991). A characterization of the Dirichlet process.Statistics & Probability Letters, 12,
185–187.
Lo, A. Y. (1993a). A Bayesian bootstrap for censored data. Annals of Statistics, 21, 100–123.
Lo, A. Y. (1993b). A Bayesian method for weighted sampling. Annals of Statistics, 21, 2138–2148.

References
315
MacEachern, S. N. (1998). Computational methods for mixture of Dirichlet process models. In
D. Dey, P. Müller, & D. Sinha (Eds.). Practical nonparametric and semiparametric Bayesian
statistics (pp. 23–44).
MacEachern, S. N. (1999). Dependent nonparametric processes. In ASA Proceedings of the Section
on Bayesian Statistical science. Alexandria: American Statistical Association.
MacEachern, S. N., & Müller, P. (1998). Estimating mixtures of Dirichlet process models. Journal
of Computational and Graphical Statistics, 7, 223–238.
Mauldin, R. D., Sudderth, W. D., & Williams, S. C. (1992). Polya trees and random distributions.
Annals of Statistics, 20 , 1203–1221.
McCloskey, J. W. (1965). A Model for the Distribution of Individuals by Species in an Environment,
unpublished Ph.D. thesis, Michigan State University.
Muliere, P., & Petrone, S. (1993). A Bayesian predictive approach to sequential search for an
optimal dose: parametric and nonparametric models. Journal of the Italian Statistical Society,
2, 349–364.
Muliere, P., & Tardella, L. (1998). Approximating distributions of random functionals of Ferguson-
Dirichlet priors. Canadian Journal of Statistics, 26, 283–297.
Muliere, P., & Walker, S. (1997). A Bayesian non-parametric approach to survival analysis using
Polya trees. Scandinavian Journal of Statistics, 24, 331–340.
Müller, P., & Quintana, F. A. (2004). Nonparametric Bayesian data analysis. Statistical Science,
19, 95–110.
Müller, P., Quintana, F. A., Jara, A., & Hanson, T. (2015). Bayesian nonparametric data analysis.
New York: Springer.
Neal, R. M. (2000). Markov chain sampling methods for Dirichlet process mixture models. Journal
of Computational and Graphical Statistics, 9 , 249–265.
Neal, R. M. (2003). Slice sampling. Annals of Statistics, 31 , 705–767.
Neath, A. A., & Bodden, K. (1997). Bayesian nonparametric conﬁdence bounds for a distribution
function. Journal of Statistical Computation and Simulation, 59, 147–160.
Neath, A. A. (2003). Polya tree distributions for statistical modeling of censored data. Journal of
Applied Mathematics and Decision Sciences, 7(3), 175–186.
Neath, A. A., & Samaniego, F. J. (1996). On Bayesian estimation of the multiple decrement
function in the competing risks problem. Statistics & Probability Letters, 31, 75–83.
Neath, A. A., & Samaniego, F. J. (1997). On Bayesian estimation of the multiple decrement
function in the competing risks problem, II. Statistics & Probability Letters, 35, 345–354.
Nieto-Barajas, L. E., Prunster, I., & Walker, S. G. (2004). Normalized random measures driven by
increasing additive processes. Annals of Statistics, 32 , 2343–2360.
Nieto-Barajas, L. E., Müller, P., Ji, Y., Lu, Y., & Mills, G. B. (2012). A time-series DDP for
functional proteomics proﬁles. Biometrics, 68, 859–868.
Ongaro, A., & Cattaneo, C. (2004). Discrete random probability measures: A general framework
for nonparametric Bayesian inference. Statistics & Probability Letters, 67, 33–45.
Paddock, S., Ruggeri, F., Lavine, M., & West, M. (2003). Randomised Polya tree models for
nonparametric Bayesian inference. Statistica Sinica, 13, 443–460.
Padgett, W. J., & Wei, L. J. (1981). A Bayesian nonparametric estimator of survival probability
assuming increasing failure rate. Communications in Statistics - Theory & Methods, A10(1),
49–63.
Paisley, J., Blei, D. M., & Jordan, M. I. (2012). Stick-breaking beta processes and the Poisson
process. In Proceedings of the 15th International Conference on Artiﬁcial Intelligence and
Statistics, La Palma, Canary Islands.
Paisley, J., Zaas, A., Woods, C. W., Ginsburg, G. S., & Carin, L. (2010). A stick-breaking
construction of the beta process. In Proceedings of the 27th International Conference on
Machine Learning, Haifa.
Papaspiliopoulos, O., & Roberts, G. O. (2008). Retrospective Markov chain Monte Carlo methods
for Dirichlet process hierarchical models. Biometrika, 95 , 169–186.
Patil, G. P., & Taillie, C. (1977). Diversity as a concept and its implications for random
communities. Bulletin International Statistical Institute, 47, 497–515.

316
References
Peterson, A. V. (1977). Expressing the Kaplan-Meier estimator as a function of empirical
subsurvival functions. Journal of the American Statistical Association, 72, 854–858.
Perman, M., Pitman, J., & Yor, M. (1992). Size-biased sampling of Poisson point processes and
excursions. Probability Theory and Related Fields, 92, 21–39.
Petrone, S. (1999). Random Bernstein polynomials. Scandinavian Journal of Statistics, 26, 373–
393.
Petrone, S., Guindani, M., & Gelfand, A. E. (2009). Hybrid Dirichlet mixture models for functional
data. Journal of the Royal Statistical Society B, 71, 75–782.
Phadia, E. G. (1971). Minimax Estimation of a Cumulative Distribution Function. Technical Report
71-1, Division of Statistics, The Ohio State University.
Phadia, E. G. (1973). Minimax estimation of a cumulative distribution function. Annals of
Statistics, 1, 1149–1157.
Phadia, E. G. (1974). Best invariant conﬁdence bands for a continuous cumulative distribution
function. Australian Journal of Statistics, 16(3), 148–152.
Phadia, E. G. (1980). A note on empirical Bayes estimation of a distribution function based on
censored data. Annals of Statistics, 8(1), 226–229.
Phadia, E. G. (2007). On bivariate tailfree processes. In Proceedings of the 56th Session of the
International Statistical Institute, Lisbon (2007) (electronic version)
Phadia, E. G., & Susarla, V. (1983). Nonparametric Bayesian estimation of a survival curve with
dependent censoring mechanism. Annals of the Institute of Statistical mathematics , 35, 389–
400.
Phadia, E. G., & Susarla, V. (1979). An empirical Bayes approach to two-sample problems with
censored data. Communications in Statistics - Theory & Methods, A8(13), 1327–1351.
Pitman, J. (1995). Exchangeable and partially exchangeable random partitions. Probability Theory
and Related Fields, 102, 145–158.
Pitman, J. (1996a). Some developments of the Blackwell-MacQueen urn scheme. In T. S. Ferguson,
L. S. Shapley & J. B. MacQueen (Eds.). Statistics, Probability and Game Theory. Papers in
Honor or David Blackwell (pp. 245–267). Hayward, CA: IMS.
Pitman, J. (1996b). Random discrete distributions invariant under size-biased permutation.
Advances in Applied Probability, 28, 525–539.
Pitman, J., & Yor, M. (1997). The two-parameter Poisson-Dirichlet distribution derived from a
stable subordinator. Annals of Probability, 25, 855–900.
Pruitt, R. C. (1992). An inconsistent Bayes estimate in bivariate survival curve analysis. Statistics
and Probability Letters, 15(3), 177–180.
Ramsey, F. L. (1972). A Bayesian approach to bioassay. Biometrics, 28, 841–858.
Randles, R. H., & Wolf, D. A. (1979). Introduction to the Theory of Nonparametric Statistics. New
York: Wiley.
Rao, V., & Teh, Y. W. (2009). Spatial normalized gamma processes. In Neural Information
Processing Systems, 2009.
Regazzini, E., Lijoi, A. , & Prunster, I. (2003). Distributional results for means of normalized
random measures with independent increments. Annals of Statistics, 31, 560–585.
Reich, B. J., & Fuentes, M. (2007). A multivariate semiparametric Bayesian spatial modeling
framework for hurricane surface wind ﬁelds. Annals of Applied Statistics, 1, 240–264.
Ren, L., Dunson, D., & Carin, L. (2008). Dynamic hierarchical Dirichlet process. In Proceedings
of the International Conference on Machine Learning, Helsinki.
Ren, L., Wang, Y., Dunson, D. , & Carin, L. (2011). The kernel Beta process. In Neural Information
Processing Systems, 2011.
Rodriguez, A. , & Dunson, D. B. (2011). Nonparametric Bayesian models through probit stick-
breaking processes. Bayesian Analysis, 6(1), 145–177.
Rodriguez, A., Dunson, D. B. , & Gelfand, A. E (2008). The nested Dirichlet process. Journal of
the American Statistical Association, 103, 1131–1154.
Rodriguez, A., Dunson, D. B. , & Gelfand, A. E. (2010). Latent stick-breaking processes. Journal
of the American Statistical Association, 105, 647–659.

References
317
Salinas-Torres, V. H., Pereira, C. A. B. , & Tiwari, R. C. (2002). Bayesian nonparametric estimation
in a series system or a competing-risks model. Nonparametric Statistics, 14, 449–458.
Samaniego, F. J. , & Whitaker, L. R. (1988). On estimating population characteristics from record-
breaking observations. II. Nonparametric results. Naval Research Logistics, 35, 221–236.
Savitsky, T. D. , & Paddock, S. M. (2013). Bayesian nonparametric hierarchical modeling for
multiple membership data in grouped attendance interventions. Annals of Applied Statistics, 7,
1074–1094.
Sethuraman, J. (1994). A constructive deﬁnition of the Dirichlet process prior. Statistica Sinica, 2,
639–650.
Sethuraman, J. , & Tiwari, R. C. (1982). Convergence of Dirichlet measures and the interpretation
of their parameter. In S. Gupta & J. Berger (Eds.). Statistical decision theory and related topics
III (Vol. 1, pp. 305–315).
Shahbaba, B., & Neal, R. M. (2009). Non-linear models using Dirichlet process mixtures. Journal
of Machine Learning Research, 10, 1829–1850.
Sinha, D. (1997). Time-discrete beta-process model for interval-censored survival data. Canadian
Journal of Statistics, 25, 445–456.
Sinha, D. (1998). Posterior likelihood methods for multivariate survival data. Biometrics, 54, 1463–
1474.
Steck, G. P. (1971). Rectangle probabilities for uniform order statistics and the probability that
the empirical distribution function lies between two distributions. Annals of Mathematical
Statistics, 42, 1–11.
Susarla, V , & Phadia, E. G. (1976). Empirical Bayes testing of a distribution function with
Dirichlet process priors. Communications in Statistics - Theory & Methods, A5(5), 455–469.
Susarla, V. , & Van Ryzin, J. (1976). Nonparametric Bayesian estimation of survival curves from
incomplete observations. Journal of the American Statistical Association, 71, 897–902.
Susarla, V. , & Van Ryzin, J. (1978a). Empirical Bayes estimation of a distribution (survival)
function from right-censored observations. Annals of Statistics, 6, 740–754.
Susarla, V., & Van Ryzin, J. (1978b). Large sample theory for a Bayesian nonparametric survival
curve estimator based on censored samples. Annals of Statistics, 6, 755–768.
Susarla, V., & Van Ryzin, J. (1978c). Addendum to large sample theory for a Bayesian nonpara-
metric survival curve estimator based on censored samples. Annals of Statistics, 8, 693.
Teh, Y. W. , & Gorur, D. (2009). Indian buffet processes with power-law behavior. In Advances in
neural information processing systems (Vol. 22).
Teh, Y. W., Gorur, D., & Ghahramani, Z. (2007). Sick-breaking construction for the Indian buffet
process. In M. Meila & X. Shen (Eds.). Proceedings of the International Conference on
Artiﬁcial Intelligence and Statistics (Vol. 11, pp. 556–63). Brookline, MA: Microtone.
Teh, Y. W. , & Jordan, M. I. (2010). Hierarchical Bayesian nonparametric models with applications.
In N. L. Hjort, et al. (Eds.). Bayesian nonparametrics. Cambridge series in statistical and
probabilistic mathematics.
Teh, Y. W., Jordan, M. I, Beal, M. J., & Blei, D. M. (2004). Hierarchical Dirichlet processes. In
Advances in neural information processing systems, Vol. 17. Cambridge, MA: MIT Press.
Teh, Y. W., Jordan, M. I, Beal, M. J. , & Blei, D. M. (2006). Hierarchical Dirichlet processes.
Journal of the American Statistical Association, 101, 1566–1581.
Thibaux, R. (2008). Nonparametric Bayesian Models for Machine Learning. Ph.D. dissertation,
Department of Statistics, University of California, Berkeley.
Thibaux, R. , & Jordan, M. I. (2007). Hierarchical beta processes and the Indian buffet process.
In M. Meila and X. Shen (Eds.). Proceedings of the International Conference on Artiﬁcial
Intelligence and Statistics (Vol. 11, pp. 564–571). Brookline, MA: Microtone.
Titsias, M. K. (2008). The inﬁnite Gamma-Poisson feature model. Advances in Neural Information
Processing Systems, 20, 97–104.
Tiwari, R. C. (1981). A Mathematical Study of the Dirichlet Process. Ph.D. dissertation, Depart-
ment of Statistics, Florida State University.
Tiwari, R. C. (1988). Convergence of the Dirichlet Invariant measures and the limits of Bayes
estimates. Communications in Statistics - Theory & Methods, 17(2), 375–393.

318
References
Tiwari, R. C., Jammalamadaka, S. R., & Zalkikar , J. N. (1988). Bayes and empirical Bayes
estimation of survival function under progressive censoring. Communications in Statistics -
Theory & Methods, A17(10), 3591–3606.
Tiwari, R. C., & Lahiri, P. (1989). On Robust Bayes and empirical Bayes estimation of means and
variances from stratiﬁed samples. Communications in Statistics: Theory and Methods, 18(3),
913–926.
Tiwari, R. C., & Zalkikar, J. N. (1985). Empirical Bayes estimation of functionals of unknown
probability measures. Communications in Statistics - Theory & Methods, 14, 2963–2996.
Tiwari, R. C. , & Zalkikar, J. N. (1991a). Empirical Bayes estimate of certain estimable parameters
of degree two. Calcutta Statistical Association Bulletin, 34, 179–188.
Tiwari, R. C., & Zalkikar, J. N. (1991b). Bayesian inference of survival curve from record-breaking
observations: Estimation and asymptotic results. Naval Research Logistics, 38, 599–609.
Tiwari, R. C., & Zalkikar, J. N. (1993). Nonparametric Bayesian estimation of survival function
under random left truncation. Journal of Statistical Planning and Inference, 35, 31–45.
Tsai, W. Y. (1986). Estimation of survival curves from dependent censorship models via a
generalized self-consistent property with nonparametric Bayesian estimation application.
Annals of Statistics, 14, 238–249.
Wade, S., Dunson, D. B., Petrone, S. , & Trippa, L. (2014). Improving prediction from Dirichlet
process mixtures via enrichment. Journal of Machine Learning Research, 15, 1041–1071.
Walker, S. G. (2007). Sampling the Dirichlet mixture model with slices. Communications in
Statistics - Simulation and Computation, 36, 45–54.
Walker, S. G. , & Damien, P. (1998). A full Bayesian nonparametric analysis involving a neutral to
the right process. Scandinavian Journal of Statistics, 25, 669–680.
Walker, S. G., Damien, P., Laud, P., & Smith, A. F. M. (1999). Bayesian nonparametric inference
for random distributions and related functions. Journal of the Royal Statistical Society B, 61,
485–527.
Walker, S. G. , & Mallick, B. K. (1997). A note on the scale parameter of the Dirichlet Process.
Canadian Journal of Statistics, 25, 473–479.
Walker, S. G. , & Mallick, B. K. (1997). Hierarchical generalized linear models and frailty models
with Bayesian nonparametric mixing. Journal of the Royal Statistical Society B, 59, 845–860.
Walker, S. G. , & Mallick, B. K. (1999). Semiparametric accelerated life time models. Biometrics,
55, 477–483.
Walker, S. G. , & Muliere, P. (1997a). Beta-Stacy processes and a generalization of the Polya-urn
scheme. Annals of Statistics, 25(4), 1762–1780.
Walker, S. G. , & Muliere, P. (1997b). A characterization of Polya tree distributions. Statistics &
Probability Letters, 31, 163–168.
Walker, S. G., & Muliere, P. (1999). A characterization of a neutral to the right prior via an
extension of Johnson’s sufﬁcientness postulate. Annals of Statistics, 27(2), 589–599.
Walker, S. G., & Muliere, P. (2003). A bivariate Dirichlet process. Statistics & Probability Letters,
64, 1–7.
West, M. (1992). Modelling with mixtures (with discussion). In J. M. Bernardo, J. 0. Berger, A.
P. Dawid, & A. F. M. Smith (Eds.). Bayesian statistics (Vol. 4, pp. 503–524). Oxford: Oxford
University Press.
West, M., Müller, P. , & Escobar, M. D. (1994). Hierarchical priors and mixture models, with
applications in regression and density estimation. In A. F. M. Smith & P. R. Freeman (Eds.).
Aspects of uncertainty: A tribute to D.V. Lindley (pp. 363–386). New York: Wiley.
Wild, C. J. , & Kalbﬂeisch, J. D. (1981). A note on a paper by Ferguson and Phadia. Annals of
Statistics, 9, 1061–1065.
Wolpart, R. L., & Ickstadt, K. (1998). Poisson/gamma random ﬁeld models for spatial statistics.
Biometrika, 85, 251–267.
Yamato, H. (1975). A Bayesian estimation of a measure of the difference between two continuous
distributions. Reports of the Faculty of Science Kagoshima University (Mathematics, Physics
and Chemistry), 8, 29–38.

References
319
Yamato, H. (1977a). Relations between limiting Bayes estimates and the U-statistics for estimable
parameters of degree 2 and 3. Communications in Statistics - Theory & Methods, A6, 55–66.
Yamato, H. (1977b). Relations between limiting Bayes estimates and the U-statistics for estimable
parameters. Journal of the Japan Statistical Society, 7, 57–66.
Yamato, H. (1984). Characteristic functions of means of distributions chosen from a Dirichlet
process. Annals of Probability, 12, 262–267.
Yamato, H. (1986). Bayes Estimates of estimable parameters with a Dirichlet Invariant process.
Communications in Statistics - Theory & Methods, 15(8), 2383–2390.
Yamato, H. (1987). Nonparametric Bayes estimates of estimable parameters with a Dirichlet
invariant process and invariant U-statistics. Communications in Statistics - Theory & Methods,
16(2), 525–543.
Yang, M., Hanson, T. , & Christensen, R. (2008). Nonparametric Bayesian estimation of a bivariate
density with interval censored data. Computational Statistics & Data Analysis, 52(12), 5202–
5214.
Zabel, S. L. (1982). W. E. Johnson’s “sufﬁcientness” postulate. Annals of Statistics, 10, 1091–1099.
Zacks, S. (1971). The theory of statistical inference. New York: Wiley.
Zalkikar, J. N., Tiwari, R. C., & Jammalamadaka, S. R. (1986). Bayes and empirical Bayes
estimation of the probability that Z > X C Y. Communications in Statistics – Theory &
Methods, 15(10), 3079–3101.
Zalkikar, J. N., Tiwari, R. C., & Jammalamadaka, S. R. (1986). Bayes and empirical Bayes
estimation of the probability that Z > X C Y. Communications in Statistics - Theory &
Methods, 15(10), 3079–3101.
Zehnwirth, B. (1981). A note on the asymptotic optimality of the empirical Bayes distribution
function. Annals of Statistics, 9, 221–224.
Zehnwirth, B. (1985). Nonparametric Linear Bayes estimation of survival curves from incomplete
observations. Communications in Statistics - Theory & Methods, 14(8), 1769–1778.

Author Index
A
Ammann, L.P., 161, 242
Antoniak, C., 2, 3, 9, 22, 24, 32–35, 38, 45,
46, 49–51, 115, 117, 213–215, 224,
241, 244, 247
B
Balkrishnan, N., 27
Barlow, R.E., 243
Barthalomew, D. J., 243
Basu, D., 3, 20, 28, 29
Beal, M.J., 93, 107, 174
Berry, D.A., 229, 240
Bhattacharya, P. K., 242
Binder, D.A., 234
Blackwell, D., 5, 7, 15, 20, 28, 34, 38, 41, 54,
86, 117, 120, 122, 124, 125, 210
Blei, D.M., 50, 192
Blum, J., 50, 272, 275
Bodden, K., 230, 232
Breth, M., 230, 231
Bulla, P., 216, 217
Burridge, M., 306
C
Cattaneo, C., 13, 41, 83, 89
Chen, M., 224
Christensen, R., 216, 240
Chung, Y., 14, 76, 91, 93, 104, 108
Cifarelli, D.M., 33, 91
Clark, V.A., 269
Clayton, M.K., 229, 239, 306
Connor, R. J., 4, 140
D
Dabrowska, D.M., 216
Dalal, S.R., 3, 9, 43–46, 216, 223, 237, 249,
251, 255, 256, 259, 266
Damien, P., 33, 132, 142, 143, 151–154, 159,
162, 166, 172, 191, 303
Dey, D., 17, 53, 143, 144, 155, 188, 224
Doksum, K.A., 4, 5, 7, 10, 36, 127, 129,
137–141, 144, 146, 147, 155, 158,
160, 161, 164, 205, 207, 261, 274,
281
Doss, H., 8, 45, 224, 236–238
Dråghici, L., 212
Dubins, L.E., 2, 39, 210
Dunson, D.B., 7, 9, 14, 41, 66, 72, 73, 76, 84,
91, 93, 99, 104, 106, 108, 181
Dykstra, R. L., 4, 11, 132, 144, 158–162, 164,
189, 287
E
Efron, B., 281, 302
Engen, S., 25, 84, 90, 114, 116
Escobar, M.D., 51, 52, 54, 55, 59, 99, 240, 247,
248
Ewens, W.J, 35, 115, 117, 203
F
Fabius, J., 3, 5, 205
Feller, W., 129
Ferguson, T.S., 2, 3, 5–8, 10, 13, 14, 17,
19–25, 28–30, 32, 35, 37, 41, 43,
45, 47, 51, 52, 111, 114, 120, 128,
130, 132–134, 139, 141, 142, 146,
© Springer International Publishing Switzerland 2016
E.G. Phadia, Prior Processes and Their Applications, Springer Series in Statistics,
DOI 10.1007/978-3-319-32789-1
321

322
Author Index
148–152, 157, 159, 160, 162–165,
170, 172, 173, 187, 189, 191,
192, 206, 208–211, 213, 216, 217,
219, 223, 228–230, 233–235, 237,
241, 246–250, 252, 260, 264, 272,
274, 276, 278, 282–284, 286, 293,
304–306
Freedman, D.A., 3, 5, 51, 205, 210
G
Gardiner, J.C., 275
Gehan, E.A., 302
Gelfand, A.E., 14, 41, 83, 92, 93, 99, 100,
103
Ghahramani, Z., 15, 39, 83, 175, 193, 196–200
Ghorai, J.K., 246, 277, 278, 284, 285
Ghosh, J.K., 216
Ghosh, M., 28, 33, 165, 216, 226–228, 233,
234, 259
Gorur, D., 174, 200
Grifﬁths, T.L., 15, 25, 39, 83, 84, 193, 196,
197, 199
Gross, A.J., 269
H
Hall, G.J. Jr., 46, 229
Hannum, R.C., 33, 223
Hanson, T., 215, 216, 244
Hjort, N.L., 2, 4, 10, 11, 78, 127, 133, 139,
143, 148, 152, 156, 164, 165, 167,
168, 170, 171, 173, 177, 184, 190,
210, 232, 285, 295, 297, 307
Hollander, M., 34, 223, 226, 227, 231, 233,
234, 248, 258, 260, 263, 291
Homes, C., 2
I
Ibrahim, J.L., 17, 173, 224
Ickstadt, K, 132, 152
Ishwaran, H., 7, 13, 24, 27, 41, 54, 57, 59, 61,
71, 74, 82, 83, 85–88, 93, 97, 99,
103, 104, 106, 107, 116, 118, 120,
124, 214
J
James, L.F., 7, 13, 24, 33, 41, 54, 57, 71, 74,
77, 82, 83, 85, 93, 97, 99, 103, 104,
106, 107, 116, 118, 156, 171, 173,
214
Jammalamadaka, S.R., 238
Johnson, N.L., 293
Johnson, R.A., 25, 84, 215, 244, 293, 300
Jordan, M.I., 4, 14, 72, 76, 94, 107, 116, 128,
134, 164, 174, 177, 178
K
Kalbﬂeisch, J.D., 4, 10, 11, 91, 127, 139, 144,
156–158, 162, 164, 166, 184, 280,
303–307
Kaplan, E.L., 191, 269, 271, 274, 276
Kim, Y., 173, 300
Kingman, J.F.C., 2, 7, 12, 21, 22, 24, 84, 86,
87, 93, 107, 111–114, 128, 134–136,
177
Korwar, R.M., 34, 226, 227, 231, 233, 234,
258, 260, 263, 291
Kotz, S., 293
Kraft, C.H., 3, 210, 241
Kuo, L., 240, 242, 243, 246, 247
L
Langberg, N.A., 33
Laud, P.W., 4, 11, 32, 132, 144, 152, 158–162,
189, 287
Lavine, M, 5, 12, 13, 208, 209, 211–214, 217,
219, 240, 243, 246
Lijoi, A., 7, 38
Lo, A.Y., 4, 36, 51, 70, 77, 93, 99, 106, 163,
210, 217, 239, 240, 244, 246, 247,
274, 282, 299, 300
M
MacEachern, S.N., 7, 14, 41, 54, 56–59, 66,
83, 91, 95, 100, 156, 171
MacQueen, J.B., 5, 7, 15, 20, 28, 38, 41, 54,
86, 117, 120
Mallick, B.K., 215, 244
Mauldin, R.D., 5, 6, 13, 208, 210, 214, 217
McCloskey, J.W., 25, 84, 110, 113–115
Meier, P., 42, 191, 216, 269, 271, 274, 276
Messan, C., 216
Mosimann, J.E., 4, 140
Muliere, P., 4, 7, 10, 12, 78, 88, 91, 127, 133,
139, 144, 148, 154, 171, 184–186,
189, 190, 210, 212, 214, 216, 217,
285, 286
Müller, P., 9, 17, 57, 58, 62, 72
N
Neath, A.A., 230, 232, 287, 293, 294

Author Index
323
O
Ongaro, A., 6, 13, 41, 83, 89
P
Padgett, W.J., 287
Park, J.H., 7, 9, 14, 41, 84, 93, 99, 106
Patil, G.P., 27, 87, 89, 90, 113, 114
Pereira, C.A.B., 77, 216, 281, 294, 295
Perman, M., 113, 115, 118, 179
Peterson, A.V., 294
Petrone, S., 78, 91, 103
Phadia, E.G., 10, 16, 128, 132, 143, 146,
149, 151, 152, 157, 159, 160, 162,
164, 170, 172, 173, 191, 192, 213,
216–218, 229, 231, 241, 242, 249,
251, 255, 258, 259, 264, 266, 273,
274, 278, 281, 283, 284, 291, 292,
302, 304–306
Pitman, J., 6, 8, 13, 16, 24, 25, 35, 39, 41,
42, 76, 84, 86, 93, 111, 113–118,
120–123, 179, 200
Prentice, R. L., 280
Prünster, I., 7, 38, 128, 134, 137
R
Ramamoorthi, R.V., 28, 33, 155, 165, 212
Ramsey, F.L., 241, 278
Randles, R.H., 258
Regazzini, E., 7, 23, 24, 33, 83, 91
S
Salinas-Torres, V.H., 77, 216, 281, 294,
295
Samaniego, F.J., 276, 293, 294
Sethuraman, J., 6, 22, 24, 32, 81, 175, 223,
254, 258
Sinha, D., 173, 224, 306
Smith. A.F.M., 162
Sollich, P., 175, 200
Steck, G. P., 231
Sudderth, W. D., 5, 6, 13, 208, 210, 214, 217
Susarla, V., 12, 50, 190, 216, 246, 258, 264,
270–276, 284, 290–292, 302
T
Taillie, C., 27, 87, 89, 90, 113, 114
Teh, Y.W., 4, 15, 17, 64–67, 69, 73, 75, 76, 83,
93, 107, 116, 137, 174, 175, 179,
196, 199, 200
Thibaux, R., 4, 14, 55, 76, 94, 107, 128, 134,
157, 164, 174, 177, 179, 193
Tiwari, R.C., 3, 6, 20, 22, 24, 28, 31, 32, 43,
44, 223, 234, 238, 253–255, 258,
259, 275–277
Tsai, W.Y., 216, 279, 280
V
van Eeden, C., 3
van Ryzin. J., 12, 190, 270–274, 276, 284, 290
W
Walker, S.G., 4, 5, 7, 10, 12, 13, 33, 54, 62, 78,
99, 127, 133, 139, 142–144, 148,
151, 153, 154, 184, 191, 210, 212,
214–217, 244, 285, 286, 303
Wei, L.J., 287
West, M., 51, 52, 54–56, 58, 59, 99, 240, 247,
248
Whitaker, L.R., 276
Wild, C.J., 10, 157, 303, 304, 306, 307
Williams, S.C., 6, 13, 208, 210, 214, 217
Wolf, D.A., 258
Wolpart, R.L., 132, 152
Y
Yamato, H., 31, 32, 44, 233, 253–257, 261
Yang, M., 217
Yor, M., 8, 13, 35, 41, 86, 93, 111, 115, 116,
118, 200
Z
Zacks, S., 253
Zalkikar, J.N., 238, 253–255, 258, 259, 276,
277, 290
Zarepour, M., 13, 27, 54, 59, 61, 71, 82, 86–88,
120, 124
Zehnwirth, B., 226, 227, 288

Subject Index
A
Asymptotic optimality, 224, 228, 246, 259, 292
B
Bayes empirical Bayes, 239–240
Bayes estimator of
concordant coefﬁcient, 42, 251–253, 255,
259, 266
covariance, 42, 242, 250–251, 255, 257,
273
cumulative hazard function, 11
density function, 42, 52, 239–240,
244–248, 287
distribution function, 1, 2, 8, 42, 44, 51,
63, 160, 163–164, 216, 221–229,
231–234, 236, 239, 247, 249, 263,
264, 270, 277, 291, 300
estimable functions, 254
hazard rate, 11, 159–160, 287, 295–298
location parameter, 42, 44, 237–238
mean, 8, 190, 233–234, 241, 255, 261
median, 235–236
modal, 241, 278–279
q-th quantile, 42, 230, 231, 236–237
survival function, 270–290
symmetric distribution function, 44, 237
variance, 234–235
Bayes risk, 225–230, 233, 234, 259, 264, 267,
303
Binary matrix, 198, 199
Bioassay problem, 3, 9, 42, 45, 241–243
C
Competing risk models, 292–295
Conﬁdence bands, 8, 222, 230–232, 239
Conjugacy, 7–8, 10, 12, 13, 38, 42, 44, 48, 54,
58, 60, 89, 146, 159, 162, 163, 166,
170, 178, 189, 190, 193, 205, 208,
250
Cox model, 11, 91, 157, 173, 307
D
Distribution
Bernoulli, 1, 14, 76, 89, 93–94, 107,
128, 159, 164, 178–183, 193, 200,
201
beta distribution, 1, 7, 11, 12, 24, 26, 41,
62, 76, 83, 86, 96, 113, 127, 139,
145, 146, 164–167, 169, 174, 175,
181, 184, 209, 212, 213, 217, 219,
229, 241, 272
bivariate, 52, 216, 222, 249–253, 265
Dirichlet, 2–4, 13, 19–79, 86, 87, 90, 104,
112, 113, 116, 140, 145, 154, 163,
194, 207, 217, 219, 231, 243, 261,
278, 298
gamma distribution, 4, 23, 24, 127,
157–159, 163, 166, 300, 306
GEM, 25, 84, 110, 113, 114
log-beta distribution, 7, 10, 12, 127, 130,
133, 144, 184–186, 188–190
mixing, 28, 29, 38, 46–51, 75, 95, 102, 214,
224, 240, 244
multinomial, 55, 71, 77
Poisson distribution, 2, 7, 12, 22, 24,
27, 93, 98, 107, 108, 110–119,
128, 130–136, 152–154, 159, 163,
175–182, 191–193, 198, 200–202,
287, 299, 300
symmetric, 4, 43, 44, 112, 223, 237, 238
© Springer International Publishing Switzerland 2016
E.G. Phadia, Prior Processes and Their Applications, Springer Series in Statistics,
DOI 10.1007/978-3-319-32789-1
325

326
Subject Index
E
Engen’s model, 114
Estimable functions, 254
Estimation
based on covariates, 303–307
Bayes empirical Bayes, 239–240
concordance coefﬁcient, 251–253
covariance, 250–251
empirical Bayes, 224–228, 233–234,
258–259, 273–274
linear Bayes, 288–290
location parameter, 237–238
maximum likelihood, 8, 227, 229, 236,
279, 301
mean, 233–234
median, 235–236
minimax, 42, 222, 229, 235
mode, 278–279
quantiles, 236–237
sequential, 228–229
shock model, 299–300
variance, 234–235
Ewen’s formula, 117
F
Function
cummulative hazard, 4, 11, 14, 78, 127,
139, 143, 155–158, 164–166, 285,
295–296, 306, 307
cumulative distribution, 20, 102, 159,
222–229, 232–239, 241, 252, 266
density, 4, 42, 51, 52, 106, 158, 159, 206,
208, 210, 211, 239, 244–248, 287
distribution, 222–229, 249–251
random distribution, 1, 2, 7, 24, 45, 79, 94,
100, 103, 106, 122, 138, 139, 141,
142, 144–146, 148–150, 154, 224,
225, 231, 282, 305
survival, 11, 12, 144, 151, 156, 157, 161,
173, 216, 217, 241, 249, 269–302
Functionals of p; 253–259
G
Group of transformations, 3, 43, 237
H
Hazard rate, 9, 11, 144, 155, 158–162,
164–166, 170, 287, 295–298
Hierarchical models, 4, 30, 64, 76, 91, 119
Hypothesis testing, 8, 42, 221, 264–267,
302–303
K
Kernel, 4, 7, 15, 41, 46, 51, 53, 56, 77, 93, 95,
99, 101, 110, 128, 136, 181–184,
210, 244, 246, 253, 256
kernel-based, 14, 17, 83, 84, 106–107
L
Loss function
integrated squared error, 241
squared error, 221, 229, 233, 234, 238, 249,
250, 260, 261, 273, 276, 279, 283,
288, 290–292, 301, 307
weighted, 249
M
Markov Chain, 11, 53, 54, 93, 108, 165, 166,
173, 297–299
Measure
Lévy, 4, 7, 10, 14, 24, 64, 109, 110,
128, 130–134, 136, 137, 141–143,
150–153, 156, 157, 165, 167, 168,
170–172, 174, 178–180, 182, 183,
185, 189, 191, 300
probability, 1–3, 5–7, 12, 14, 16, 19–24,
30, 33, 41–43, 52, 66, 70, 78, 79, 82,
84, 89, 99, 103–106, 112, 136–138,
140, 142, 166, 177, 196, 198, 205,
209, 210, 215, 216, 223, 232, 244,
246, 249, 258, 264
random, 2, 4, 7, 12, 21, 22, 24, 59, 64, 72,
76, 82, 87, 93, 107, 108, 118, 128,
134–137, 174, 215
P
Permutation
rank ordered, 111, 114, 116
size-biased, 27, 90, 111, 113, 114, 116
Polya
generalized urn scheme, 3, 6, 7, 41, 75, 86,
107, 117, 119, 217
sequence, 28, 40, 120, 121
tree, 5, 6, 12, 13, 17, 41, 171, 208–217,
219, 240, 243, 244, 246, 286–287
urn scheme, 5, 13, 15, 38, 39, 118, 120,
124, 193
Predictive
distribution, 5, 6, 8, 12, 28, 35, 41, 52, 54,
55, 115, 201, 211, 213, 216, 247,
286
rule, 28, 86, 117, 120, 121, 124

Subject Index
327
Processes
age-dependent branching, 300–302
Bernoulli process, 14, 15, 128, 164,
178–183, 193, 200
Bernstein process, 78–79
beta-neutral process, 156
beta process, 4, 7, 11–15, 41, 63, 72, 76,
78, 91, 94, 107, 109, 110, 127, 128,
133, 134, 136, 139, 143, 156, 159,
164–184, 193, 197, 200, 201, 210,
216, 285, 295, 307
beta-Stacy process, 4, 10, 12, 17, 41,
127, 133, 137, 138, 144, 145, 153,
184–192, 270, 286
bivariate processes, 216–219
bivariate tailfree process, 217–219
Chinese restaurant process, 15, 17, 38, 83,
174, 193–196
Dirichlet dependent process, 90–110
Dirichlet invariant process, 43–45
Dirichlet process, 19–42
extended gamma process, 4, 11, 132, 144,
158–164, 189, 287, 296
Ferguson-Sethuraman processes, 6, 14, 17,
41, 81–125
Gamma process, 157–159
generalized Dirichlet process, 78
hierarchical Dirichlet process, 15, 66, 174
Indian buffet process, 7, 11, 14, 15, 17, 41,
83, 128, 134, 196–201
linearized Dirichlet process, 261, 263
local Dirichlet process, 14, 93, 104–106
log-beta process, 7, 10, 12, 127, 133, 144,
185, 186, 188–190
mixtures of Dirichlet processes, 9, 16, 17,
19, 40, 45–50, 146, 214, 244
multivariate Dirichlet process, 77–78, 83,
88
neutral to the right process, 2, 4, 8, 10, 12,
17, 20, 22, 37, 41, 45, 127, 128,
134, 137–156, 158–161, 164–166,
170, 171, 173, 184, 188, 191,
192, 205, 216, 221, 270, 281–283,
288–290
non-negative independent increments
process, 188
Pitman-Yor process, 13, 17, 93, 115, 179,
200
point process, 77, 98, 99, 131, 163, 217,
300
Poisson-Dirichlet process, 8, 13, 17, 41, 64,
83–85, 110–119, 124
Polya tree process, 5, 6, 12, 13, 17, 41, 171,
205, 208–216
simple homogeneous process, 133, 142,
153, 192, 283, 290
stick-breaking process, 76, 81, 92, 93,
102–104, 106–107, 176
tailfree process, 5, 12, 16, 17, 20, 41, 138,
140, 205–219, 241, 249–250
two-parameter beta process, 13, 180
two-parameter Poisson-Dirichlet process,
8, 17, 63, 64, 83, 115–119, 124
Progressive censoring, 42, 275–276
Proportional hazard, 277–278
R
Regression problem, 46, 144, 243–244
Residual allocation model, 89–90
Residual fractions, 114, 116
Right censored data, 9–12, 42, 50, 129, 149,
162, 172, 184, 208, 213, 269–273,
281, 288, 290–292, 295
S
Sized-biased permutation, 27, 90, 111, 113,
114, 116
T
Tolerance region, 222, 230–232
Two-sample problem, 222, 259–263, 302

