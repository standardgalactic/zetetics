RICE UNIVERSITY
Very Large Scale Bayesian Machine Learning
by
Zhuhua Cai
A THESIS SUBMITTED
IN PARTIAL FULFILLMENT OF THE
REQUIREMENTS FOR THE DEGREE
Doctor of Philosophy
ApPROVED, T HESIS C OMMITTEE:
..IdfIl.-zjil'ong
Associate Professor of Electrical and
Computer Engineering
RiceUniversity
d
~~
Associate Professor of Computer Science
Rice University
Houston, Texas
June, 2014

ABSTRACT
Very Large Scale Bayesian Machine Learning
by
Zhuhua Cai
This thesis aims to scale Bayesian machine learning (ML) to very large datasets.
First, I propose a pairwise Gaussian random ﬁeld model (PGRF) for high dimen-
sional data imputation. The PGRF is a graphical, factor-based model. Besides its
high accuracy, the PGRF is more eﬃcient and scalable than the Gaussian Markov
random ﬁeld model (GMRF). Experiments show that the PGRF followed by the lin-
ear regression (LR) or support vector machine (SVM) reduces the RMSE by 10% to
45% compared with the mean imputation followed by the LR or SVM. Furthermore,
the PGRF scales the imputation to very large datasets distributed in a 100-machine
cluster that could not be handled by the GMRF or Gaussian methods at all.
Unfortunately, the PGRF model is hard to implement – approximately 18000
lines of Hadoop code and 4 months of work in distributed debugging and running.
To reduce the huge amount of human eﬀort, I designed a database system called
SimSQL. SimSQL supports rich analytical methods such as Bayesian ML, and scales
such methods to terabytes of data distributed over 100 machines. SimSQL enlarges
the analysis power of relational database systems, and at the same time keeps merits
such as declarative language, transparent optimization and automatic parallelization.
SimSQL builds upon the MCDB uncertainty database, and allows the deﬁnition of
recursive stochastic tables. SimSQL is an ideal platform for Markov chain simulations

or iterative algorithms such as PageRank.
To show SimSQL’s performance, I introduce an objective benchmark that com-
pares SimSQL with Giraph, GraphLab and Spark on ﬁve Bayesian ML problems.
The results show that SimSQL provides the best programmability and competitive
performance. To run a general Bayesian ML model, SimSQL takes 1X less code than
Spark, 6X less than GraphLab, and 12X less code than Giraph, while its time cost is
within 5X slowdown in the worst case compared with Giraph and GraphLab.
In brief, I consider both modeling and inference for large scale Bayesian ML.
The goals for both sides are the same: scaling Bayesian ML to very large datasets,
achieving better performance and reducing time cost in design, implementation and
execution of ML algorithms.

Acknowledgments
I have spent ﬁve years on this dissertation. It would not be ﬁnished without assistance
and support from many individuals.
First, I would like to thank my adviser, Prof. Christopher M. Jermaine for his help
throughout my time at Rice University. His patience and trust have been driving me
towards success since the very beginning of my graduate studies, and his insights and
vision make this journey much easier. Currently, my research has received signiﬁcant
interest from many research communities. Especially, SimSQL and its open source
release acknowledge my contribution to the area of big data analytics. I owe these
successes to my adviser. I would also like to thank Prof. Lin Zhong and Prof. Luay
K. Nakhleh for serving on my committee and providing their helpful feedback on this
thesis. Similarly, I would like to thank Prof. Peter J. Haas in IBM Research and Dr.
Dionysios Logothetis in Telefonica Research for their guidance.
Second, I would like to thank many workmates and friends at Rice University,
including Subi Arumugam, Anna Drummond, Zekai Gao, Shangyu Luo, Xu Liu, Risa
Myers, Niketan Pansare, Letao Qi, Luis Leopoldo Perez, Zografoula Vagena, Yiting
Xia and Chaoran Yang. Their help improved my research skills, widened my vision
and brought me ﬁve years of exciting and entertaining life. Especially, I would like
to thank Zografoula Vagena for improving my engineering, research and living skills.
I owe her too much, and will always remember her generous help.
Last, I would like to thank my wife Fei Jiang and my parents for their advice,
support and aﬀection. Without them, this thesis would not exist.

Contents
Abstract
ii
Acknowledgments
iv
List of Illustrations
x
List of Tables
xiii
1 Introduction
1
1.1
Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1
1.2
My Solution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3
1.3
Statistical ML: A Primer . . . . . . . . . . . . . . . . . . . . . . . . .
4
1.4
PGRF for Big Data Imputation . . . . . . . . . . . . . . . . . . . . .
5
1.5
SimSQL for Bayesian ML
. . . . . . . . . . . . . . . . . . . . . . . .
7
1.6
An Objective Benchmark for Bayesian ML . . . . . . . . . . . . . . .
10
1.7
Thesis Organization . . . . . . . . . . . . . . . . . . . . . . . . . . . .
11
2 High-Dimensional Data Imputation
13
2.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
14
2.2
The PGRF Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
18
2.2.1
Basic Factor Model . . . . . . . . . . . . . . . . . . . . . . . .
18
2.2.2
Priors on the Model Parameters . . . . . . . . . . . . . . . . .
20
2.3
Markov Chain Monte Carlo
. . . . . . . . . . . . . . . . . . . . . . .
20
2.3.1
MCMC Basics . . . . . . . . . . . . . . . . . . . . . . . . . . .
21
2.3.2
Sampling Unknown Data Values . . . . . . . . . . . . . . . . .
21
2.3.3
Updating the Factor Means
. . . . . . . . . . . . . . . . . . .
24
2.3.4
Updating the Factor Variances and Covariances . . . . . . . .
25

vi
2.4
Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
29
2.4.1
Overview
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
29
2.4.2
Implementation and Hardware . . . . . . . . . . . . . . . . . .
29
2.4.3
Model Quality Experiments . . . . . . . . . . . . . . . . . . .
30
2.4.4
Computational Eﬃciency . . . . . . . . . . . . . . . . . . . . .
36
2.5
Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
37
2.5.1
Old Methods
. . . . . . . . . . . . . . . . . . . . . . . . . . .
38
2.5.2
Maximum Likelihood Imputation . . . . . . . . . . . . . . . .
39
2.5.3
Multiple Imputation
. . . . . . . . . . . . . . . . . . . . . . .
40
2.5.4
Missing Values Are Not Missing Completely at Random
. . .
41
2.5.5
Large Scale Imputation . . . . . . . . . . . . . . . . . . . . . .
41
2.6
Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
42
3 SimSQL for Large Scale Bayesian ML
44
3.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
44
3.2
Bayesian Linear Regression . . . . . . . . . . . . . . . . . . . . . . . .
48
3.2.1
The Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
48
3.2.2
Inference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
49
3.2.3
Learning the Model . . . . . . . . . . . . . . . . . . . . . . . .
49
3.3
Latent Dirichlet Allocation . . . . . . . . . . . . . . . . . . . . . . . .
52
3.3.1
Model and Inference
. . . . . . . . . . . . . . . . . . . . . . .
52
3.3.2
Specifying the Gibbs Sampler in SimSQL . . . . . . . . . . . .
55
3.4
Specifying an Analysis . . . . . . . . . . . . . . . . . . . . . . . . . .
57
3.5
Implementation Overview
. . . . . . . . . . . . . . . . . . . . . . . .
59
3.5.1
SimSQL Language
. . . . . . . . . . . . . . . . . . . . . . . .
59
3.5.2
SimSQL Kernel . . . . . . . . . . . . . . . . . . . . . . . . . .
61
3.5.3
SimSQL Compiler . . . . . . . . . . . . . . . . . . . . . . . . .
66
3.5.4
Logical Optimizer . . . . . . . . . . . . . . . . . . . . . . . . .
79

vii
3.6
SIMSQL EXECUTION ENGINE . . . . . . . . . . . . . . . . . . . .
81
3.6.1
Overview
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
81
3.6.2
Tuple Bundles . . . . . . . . . . . . . . . . . . . . . . . . . . .
82
3.6.3
The VG Wrapper Operation . . . . . . . . . . . . . . . . . . .
83
3.7
Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
85
3.7.1
Overview
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
86
3.7.2
Benchmarks Run . . . . . . . . . . . . . . . . . . . . . . . . .
87
3.7.3
Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
89
3.8
Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
92
3.8.1
Parallel programming language
. . . . . . . . . . . . . . . . .
92
3.8.2
Database system
. . . . . . . . . . . . . . . . . . . . . . . . .
93
3.8.3
MapReduce . . . . . . . . . . . . . . . . . . . . . . . . . . . .
93
3.8.4
Large scale machine learning . . . . . . . . . . . . . . . . . . .
95
3.8.5
Markov Chain Monte Carlo
. . . . . . . . . . . . . . . . . . .
97
3.9
Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
97
4 Benchmark for Large Scale Bayesian ML
99
4.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
99
4.2
Experimental Overview . . . . . . . . . . . . . . . . . . . . . . . . . .
101
4.2.1
Platforms Tested . . . . . . . . . . . . . . . . . . . . . . . . .
101
4.2.2
ML Models Considered . . . . . . . . . . . . . . . . . . . . . .
102
4.2.3
Balancing Performance and Ease-Of-Use . . . . . . . . . . . .
102
4.2.4
Experimental Platform . . . . . . . . . . . . . . . . . . . . . .
103
4.3
Platforms Evaluated
. . . . . . . . . . . . . . . . . . . . . . . . . . .
103
4.3.1
Spark
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
103
4.3.2
SimSQL . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
104
4.3.3
GraphLab . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
104
4.3.4
Giraph . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
105

viii
4.3.5
Versions Tested . . . . . . . . . . . . . . . . . . . . . . . . . .
105
4.4
Gaussian Mixture Model . . . . . . . . . . . . . . . . . . . . . . . . .
105
4.4.1
Spark Implementation
. . . . . . . . . . . . . . . . . . . . . .
106
4.4.2
SimSQL Implementation . . . . . . . . . . . . . . . . . . . . .
108
4.4.3
GraphLab Implementation . . . . . . . . . . . . . . . . . . . .
110
4.4.4
Giraph Implementation . . . . . . . . . . . . . . . . . . . . . .
111
4.4.5
Experiments and Results . . . . . . . . . . . . . . . . . . . . .
112
4.4.6
Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
113
4.5
The Bayesian Lasso . . . . . . . . . . . . . . . . . . . . . . . . . . . .
117
4.5.1
Spark Implementation
. . . . . . . . . . . . . . . . . . . . . .
117
4.5.2
SimSQL Implementation . . . . . . . . . . . . . . . . . . . . .
118
4.5.3
GraphLab Implementation . . . . . . . . . . . . . . . . . . . .
119
4.5.4
Giraph Implementation . . . . . . . . . . . . . . . . . . . . . .
120
4.5.5
Experiments and Results . . . . . . . . . . . . . . . . . . . . .
121
4.5.6
Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
121
4.6
Hidden Markov Model . . . . . . . . . . . . . . . . . . . . . . . . . .
123
4.6.1
Spark Implementation
. . . . . . . . . . . . . . . . . . . . . .
124
4.6.2
SimSQL Implementation . . . . . . . . . . . . . . . . . . . . .
125
4.6.3
GraphLab Implementation . . . . . . . . . . . . . . . . . . . .
127
4.6.4
Giraph Implementation . . . . . . . . . . . . . . . . . . . . . .
127
4.6.5
Experiments and Results . . . . . . . . . . . . . . . . . . . . .
128
4.6.6
Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
130
4.7
Latent Dirichlet Allocation . . . . . . . . . . . . . . . . . . . . . . . .
131
4.7.1
Experiments and Results . . . . . . . . . . . . . . . . . . . . .
133
4.7.2
Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
134
4.8
Gaussian Imputation . . . . . . . . . . . . . . . . . . . . . . . . . . .
135
4.8.1
Experiments and Results . . . . . . . . . . . . . . . . . . . . .
136
4.8.2
Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
137

ix
4.9
Summary of Findings . . . . . . . . . . . . . . . . . . . . . . . . . . .
137
5 Conclusion
141
5.1
Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
141
5.2
Lessons Learned . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
142
5.3
Future Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
144
Bibliography
146

Illustrations
2.1
Example, a ﬁve-dimensional PGRF model. Factors are represented
by boxes, variables by circles. The set Ψ in this example speciﬁes four
correlations to model: {f(1, 3), (1, 5), (3, 5), (4, 5)}. Thus, there are
four bivariate Gaussian factors connecting these four pairs of
dimensions. Since dimension 2 has no correlations, a univariate
Gaussian factor is attached to it.
. . . . . . . . . . . . . . . . . . . .
19
2.2
Prediction accuracy when varying the fraction of missing values. . . .
33
2.3
Prediction accuracy when varying the number of correlations in Ψ. . .
34
2.4
Cumulative density function of the distribution of the strength of the
observed correlations for the data sets. . . . . . . . . . . . . . . . . .
35
2.5
Running time as a function of compute cluster size; data size per
machine remains constant. . . . . . . . . . . . . . . . . . . . . . . . .
36
3.1
Initializing a and b (σ2
0 = 10). . . . . . . . . . . . . . . . . . . . . . .
50
3.2
Updating the variance σ2.
. . . . . . . . . . . . . . . . . . . . . . . .
50
3.3
Updating the value of a (σ2
0 = 10).
. . . . . . . . . . . . . . . . . . .
51
3.4
Updating the value of b (σ2
0 = 10).
. . . . . . . . . . . . . . . . . . .
51
3.5
Initializing Θ
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
54
3.6
Initializing w
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
55
3.7
Updating Θ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
56
3.8
Updating w . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
57
3.9
Updating Ψ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
58

xi
3.10 Syntax for an LDA analysis
. . . . . . . . . . . . . . . . . . . . . . .
58
3.11 The computing stack of SimSQL. . . . . . . . . . . . . . . . . . . . .
60
3.12 An example of nested subquery. . . . . . . . . . . . . . . . . . . . . .
61
3.13 Overview of simulation query processing in SimSQL.
. . . . . . . . .
63
3.14 The graph used to determine the optimal frame cut. . . . . . . . . . .
65
3.15 An example of the semantic tree.
. . . . . . . . . . . . . . . . . . . .
66
3.16 Determining whether a recursive query plan is valid. . . . . . . . . . .
68
3.17 The typechecker tree for the query in Figure 3.12
. . . . . . . . . . .
69
3.18 The steps to unnest the query in Figure 3.12 . . . . . . . . . . . . . .
71
3.19 Template plan for creating a random table. . . . . . . . . . . . . . . .
75
3.20 Template plans for LDA . . . . . . . . . . . . . . . . . . . . . . . . .
76
3.21 Frame to be optimized for execution. . . . . . . . . . . . . . . . . . .
77
3.22 Attribute renaming in preoptimization. . . . . . . . . . . . . . . . . .
78
3.23 Transformation to drop and recreate a random attribute in SimSQL.
80
3.24 Running time (HH:MM) per iteration for GMM and LDA inference.
Iteration zero is the initialization. . . . . . . . . . . . . . . . . . . . .
88
3.25 Avg running time per iteration (HH:MM), changing the number of
parallel simulations while keeping the (number of data points) × (the
number of simulations) constant.
. . . . . . . . . . . . . . . . . . . .
89
3.26 Parallel coordinates plot depicting sampled LDA θ vectors for three
documents, iterations 50-60. The top ten topics for each document
are shown, sorted in order of importance.
. . . . . . . . . . . . . . .
91
4.1
GMM MCMC implementation; lines of code and average time per
iteration. Time in parens is for the initialization/setup. “SV” is a
short for super vertex. Format is HH:MM:SS or MM:SS. ∗I was
actually unable to run GraphLab at 100 machines, and the closest to
100 machines that I could get was 96 machines.
. . . . . . . . . . . .
114

xii
4.2
Bayesian Lasso MCMC implementation; lines of code and average
time per iteration. Format is HH:MM:SS or MM:SS. “SV” is a short
for super vertex. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
121
4.3
HMM results. Time in parens is for the initialization/setup. Format
is HH:MM:SS or MM:SS. . . . . . . . . . . . . . . . . . . . . . . . . .
129
4.4
LDA results. Time in parens is for the initialization/setup. Format is
HH:MM:SS or MM:SS. . . . . . . . . . . . . . . . . . . . . . . . . . .
134
4.5
Gaussian imputation results. Time in parens is for the
initialization/setup. Format is HH:MM:SS or MM:SS.
. . . . . . . .
136
4.6
Average time per iteration (and start-up time).
. . . . . . . . . . . .
139

Tables

1
Chapter 1
Introduction
This thesis is about scaling Bayesian machine learning (ML) to very large datasets.
I consider both modeling and inference for large scale Bayesian ML. First, I propose
a novel Bayesian model called pairwise Gaussian random ﬁeld (PGRF) for the im-
putation problem. The PGRF performs very well in terms of accuracy, eﬃciency
and scalability on very large datasets that are distributed in a 100-machine cluster.
Second, I design a parallel database system called SimSQL that scales Markov chain
simulations and Bayesian ML into terabytes of data in a 100-machine cluster. Novel
techniques such as the strong data independence and declarative language make Sim-
SQL an ideal platform for large scale Bayesian ML. Furthermore, I introduce an
objective benchmark that compares SimSQL with prevailing parallel ML platforms.
The results show that SimSQL has the best programmability and competitive per-
formance (and in some cases the best) among the benchmarked systems.
1.1
Motivation
Continuously growing data places unprecedented challenges on modern data analysis
for both modeling and inference. From statisticians to biologists and social scientists,
people are designing new models and implementing new systems for timely and reli-
able data analysis. The Bayesian approach has been proved to be an important data
analysis technique in a range of domains including science, engineering, medicine,

2
bioinformatics, etc. Unfortunately, despite its simplicity and elegance, the Bayesian
approach is not widely applied to very large datasets due to challenges in scalability
and computational eﬃciency. The challenges manifest themselves as follows:
Current Bayesian models do not scale. As the data scales, people tend to ap-
ply rich models to eliminate the bias of simple models, and to extract strong patterns
from the dataset. However, rich models, especially models with a large number of
strongly correlated variables, are computationally hard to parallelize [1, 2]. For exam-
ple, the Gaussian distribution, a fundamental model in statistics, is computationally
expensive for modeling high-dimensional data [3]. Without computational eﬃciency,
a statistically preferred model may not even be run in a distributed cluster.
Parallel Bayesian inference is hard to code. The design and implementa-
tion of distributed/parallel Bayesian ML algorithms are hard to achieve. “Big data”
has driven the development of an ecosystem of distributed computing that includes
distributed ﬁle systems [4, 5], key-value stores [6, 7], parallel computing frameworks
[8, 5], and high-level language abstraction systems[9, 10]. However, coding, debugging
and running the distributed/parallel algorithms in such systems can be frustratingly
diﬃcult, especially for a statistician whose expertise is in model design instead of
parallel programming and system engineering.
Parallel Bayesian ML systems are hard to ﬁnd.
A number of systems
[11, 12, 13, 1, 14] have been proposed for large scale Bayesian ML. However, while
the level of engineering associated with these systems is impressive, the actual sci-
ence undertaken has been almost nonexistent. Experiments in this area are typically
uninformative. Frequently, a new system is compared with itself; or it selects some
simple but not representative test cases to show its performance. Such experiments
will then conﬁrm what they were engineered to show. What is missing is any sort

3
of meaningful exploration of the limits and relative merits of various approaches, so
that data engineers can have useful guidelines about which system to choose when
considering a real problem.
Due to such challenges, the Bayesian ML is less widely used in large scale datasets
compared with other classical methods such as linear regression [15], k-nearest neigh-
bors [16], vector space model [17], decision tree [18], etc.
1.2
My Solution
I aim to scale Bayesian ML to very large datasets. To achieve this goal, ﬁrst, a model
design should consider computational eﬃciency and statistic properties. Second, the
inference algorithm should be implemented in an easy, fast and reliable way.
The thesis makes three contributions to this ﬁeld:
• I propose the pairwise Gaussian random ﬁeld (PGRF) model [19] for big data
imputation. The PGRF is used as an example to show that computational prop-
erties such as eﬃciency and parallelism are signiﬁcant in addition to statistical
properties such as accuracy in big data analysis.
• I design the SimSQL database system [12] for large scale data analytics. Sim-
SQL provides a declarative query language for model speciﬁcation, and it auto-
matically compiles, optimizes and runs parallel inference algorithms. SimSQL
natively supports Bayesian ML and Markov chain simulations.
• I introduce a careful benchmark [20] to thoroughly compare and explore the
strengths and weaknesses of several big data platforms.
In the remainder of this chapter, I ﬁrst introduce some background explaining
Bayesian ML, and then give a short introduction for each subproblem.

4
1.3
Statistical ML: A Primer
In statistical ML, one ﬁrst postulates a generative, statistical model for a data set,
characterized by a distribution function f(X|θ). Here, X is the data set, and θ is the
unseen model parameters and hidden variables. Learning is the process of choosing a
value ˆθ for θ so that ˆθ describes well how the model could have produced X. Learning
is typically quite expensive computationally, especially if the data set is very large.
Hence platforms that can support parallel and/or distributed ML are attractive.
I focus on learning relatively complex, hierarchical models, which are often char-
acterized by the Bayesian approach to ML. In this approach, the user expresses his
or her prior belief about the model by supplying a prior distribution f(θ) over θ.
The goal is to understand something about the posterior distribution for θ, written
as f(θ|X). The posterior relates to the prior via Bayes’ Rule:
f(θ|X) ∝f(θ)f(X|θ).
The simplest, most universal, and most common way to analyze the posterior is
to draw a set of samples from it via Markov chain Monte Carlo (MCMC) [21]. To
apply MCMC to the problem, a Markov chain is deﬁned over all possible values for
θ. This Markov chain is a random walk over a graph, where each vertex in the graph
is a possible value for θ, and an edge between two possible values ˆθ1 and ˆθ2 for θ is
labeled with a weight describing the likelihood of transition from ˆθ1 to ˆθ2 during the
walk. The graph is deﬁned so that observing the current state after an inﬁnite-length
walk is equivalent to drawing a sample ˆθ from the target f(θ|X).
In practice, a common way is to use the “Gibbs sampler” [21], a special case of
MCMC algorithm. Gibbs sampling generates samples from a high dimensional distri-
bution function P(X) by simulating a Markov chain. In theory, the chain converges

5
to be static after a “burn in” period, and the current state of the chain can be col-
lected as a sample from the target distribution. Assuming that the set of parameters
to sample is X = {X1, X2, ..., Xd}, Gibbs sampling works as follows:
1. Choose some initial values for all the parameters in X.
2. Iterate over all the parameters in X, sampling the value of one parameter Xi
from the conditional distribution P(Xi|X∖Xi).
3. Run step 2 for many iterations until the chain converges.
The Gibbs sampler will be used throughout this thesis when learning a model is
required.
1.4
PGRF for Big Data Imputation
Imputation is a classic problem in statistics [22, 23, 24, 25, 26, 27, 28, 29], and it
can be the ﬁrst step in big data analysis, such as data cleaning and preprocessing.
Many statistical methods such as mean imputation [30] and regression imputation
[26, 27] have been proposed. In such methods, one common choice is the (multi-
dimensional) Gaussian distribution [31] or its modiﬁed version the Gaussian Markov
random ﬁeld (GMRF) [3]. The Gaussian distribution has many advantages including
the fact that it is by far the simplest mechanism for modeling covariance. However,
Gaussian imputation is computationally expensive on very large datasets.
As a response, I propose a very simple, factor-based model for “Gaussian-like”
imputation called the pairwise Gaussian random ﬁeld (PGRF). Unlike the Gaussian
[31] or GMRF models [3], a PGRF does not require a covariance or correlation matrix
as input. Instead, it takes as input a set of p (dimension, dimension) pairs which the

6
user suspects to be strongly correlated or anti-correlated.
These pairs deﬁne the
graphical structure of the model, with a simple Gaussian factor associated with each
pair. Using this structure, the PGRF performs simultaneous inference and imputation
of X. A separate step is not needed to estimate a sparse inverse covariance matrix.
The key beneﬁt of the method is its big win in terms of eﬃciency, where the time
complexity for inference is approximately linear with p. Typically, p << m2 (Σ has
m2 entries).
The PGRF has several advantages over the Gaussian distribution or Gaussian
Markov random ﬁeld (GMRF):
• Lower complexity: the cost of learning and applying the model per iteration
is Θ(n(m+p)), which is much better than Θ(nm2), the time cost for computing
the covariance matrix from a dataset.
• Parallelization: the whole algorithm can be parallelized easily in a framework
such as MapReduce.
• High Utility: the accuracy of high-dimensional regression and classiﬁcation
can be improved using the PGRF, particularly when the model is inferred jointly
with the task.
My experiments show that the time cost is 50 minutes per iteration to classify 1M
documents with a 104 dictionary size where 50% of values are missing, and this time
cost stays constant while both the data and machine scale linearly. Furthermore, the
PGRF followed by linear regression (LR) or support vector machine (SVM) reduces
the RMSE by 10% to 45% compared with mean imputation followed by the LR or
SVM.

7
My PGRF implementation has 18000 lines of Java source code on Hadoop. Such
large code is not unique; it is generally hard to implement parallel codes for Bayesian
ML. To address the problem, I introduce the SimSQL database system.
1.5
SimSQL for Bayesian ML
In general, Bayesian ML and MCMC methods are hard to implement in the context
of big data, for several reasons:
• It is diﬃcult to implement parallel codes from scratch using a language such
as Java/C++/C due to the diﬃculty of handling details such as deadlock, job
scheduling, etc.
• R/MATLAB/SAAS [32, 33, 34] can be easily used to specify the Bayesian ML,
but their performance is low.
Techniques such as message-passing [35] and
SNOW [36] could scale up such languages but are too low-level for data engi-
neers to easily use.
• Various scalable ML codes or libraries [37, 38, 39, 40, 41, 42, 43] have been
proposed for specialized problems on speciﬁc parallel architectures. However,
they are not general enough to express the arbitrary Bayesian models.
• Parallel frameworks such as MapReduce/Spark [8, 11] are suitable for writing
parallel programs, but it is too hard to specify Bayesian ML models using such
tools. My PGRF implementation requires 18000 lines of Java code with 11
speciﬁc MapReduce jobs.
• Analytic databases on top of MapReduce [10, 44, 45, 46, 47] or massively par-
allel processing (MPP) architectures [48, 49, 50] do not support Bayesian ML

8
as well. Such systems can handle simple analytic tasks such as aggregation but
not tasks such as MCMC. Combining analytic databases with external process-
ing does not scale; e.g., the I/O between the runtime and the database engine
becomes a bottleneck; integration among diﬀerent systems can cause fault tol-
erance problems.
In response to this, I describe the SimSQL database system. SimSQL takes a
relational approach to large scale Bayesian ML. Speciﬁcally, the database engine is
leveraged to run the compiled and optimized ML code. One of the novelties is that
SimSQL provides a pure declarative language (SQL) to specify the iterative/recursive
process of MCMC. The language not only hides the system details such as network
communication and deadlock, but also the logic of data ﬂow such as aggregation and
scalar functions.
SimSQL extends the Monte Carlo database system (MCDB) [48], a prototype
SQL-based database supporting stochastic analytics.
Both databases allow users
to deﬁne stochastic tables in addition to ordinary database tables. In a stochastic
table, some entries are random variables with associated probability distributions (not
necessarily in closed form) and others are ordinary, deterministic data values. When
a query is issued that includes stochastic tables, both systems use pseudo-random
number generation techniques to instantiate each stochastic data value. Queries can
be run over the resulting database instance (also called possible world) many times
in a Monte Carlo fashion to obtain an empirical distribution of the query results.
SimSQL contains two extensions over MCDB. First, SimSQL allows stochastic
tables to be used to parametrize the processes that stochastically generate other
stochastic tables. This allows SimSQL to implement hierarchical Bayesian models
such as latent Dirichlet allocation (LDA) [51]. Second, SimSQL allows both versioning

9
and recursive deﬁnitions of stochastic database tables. For example, stochastic table
A can be used to parametrize the stochastic generation of table B, which in turn can
be used to parametrize the stochastic generation of a second version of A, and so on.
With these two extensions, SimSQL allows user to deﬁne a directed acyclic graph
(DAG) of stochastic tables, where each vertex corresponds to a versioned stochastic
table, and the directed edges represent the dependency among versioned stochastic
tables. This functionality makes SimSQL an ideal platform for specifying Bayesian
ML and massive stochastic agent-based simulations.
Compared with previous methods for performing large scale Bayesian ML, Sim-
SQL has following advantages:
• A declarative query interface gives SimSQL a surprisingly useful set of abstrac-
tions for Bayesian ML. It is natural to group a class of random variables or
parameters to a stochastic database table.
• The VG function abstraction allows the deﬁnition of arbitrary statistical func-
tions to realize stochastic tables. It is very easy to specify the posterior distri-
bution of a parameter inside a VG function for the MCMC processing.
• Implicit parallelism in SimSQL allows one to write SQL without worrying about
how the code will be run in clusters. SimSQL automatically compiles, optimizes,
conﬁgures and executes the code at scale in the parallel framework MapReduce.
• The high performance of SimSQL makes it a competitive platform for large
scale Bayesian ML and MCMC simulation.
With the collaboration of SimSQL team members [12], I have implemented and
released SimSQL under the Apache license [52]. Readers can refer to [52] for enough

10
information on how to launch SimSQL on Amazon EC2 within about 30 minutes.
The system consists of an SQL query compiler with approximately 50,000 lines of
Java code, a cost-based query optimizer with approximately 11,000 lines of Prolog
code, and a runtime written in around 40,000 lines of Java and C/C++ on top of
Hadoop. Furthermore, I have extensively tested SimSQL as both a database system
and a data analytics platform on hundreds of queries such as TPC-H [53] and a dozen
Bayesian models such as LDA [51].
1.6
An Objective Benchmark for Bayesian ML
A number of systems [35, 54, 12, 11, 13, 1, 55] have been proposed to provide general-
purpose programming and systems support for coding and running large scale ML,
and some systems such as Spark [11] and GraphLab [1] have already been commer-
cialized. All such systems are advertised to have good performance and user-friendly
interfaces. For example, it is reported that Spark run jobs 100X faster than MapRe-
duce with writing 2 −5X less code.
Unfortunately, there is little objective knowledge regarding the pros and cons of
the various platforms for implementing and running large scale ML problems. Most
studies focus on simple analytics problems, such as k-means [11] or logistic regres-
sion [11], or on problems that are very low-dimensional or have only modest CPU
requirements relative to data set size. However, such problems are not universally rep-
resentative, especially considering the fact that data engineers may design arbitrary
models for domain speciﬁc problems. Currently, the literature lacks a benchmark that
objectively surveys and compares diﬀerent systems with respect to their architecture,
engineering principles, performance, programmability, and applicability.
As a response, I describe an objective benchmark that compares SimSQL with

11
three other prevailing large scale ML platforms regarding ﬁve representative ML prob-
lems that cover the general tasks such as classiﬁcation, regression, feature selection
and imputation. The benchmark should be helpful for data engineers who must se-
lect a platform and implement the parallel code given a statistical model, and for
system designers who must choose problems and benchmark their systems. Using my
benchmark, users can balance ease of implementation with performance for diﬀerent
platforms, while a system designer needs only to implement the codes for my selected
problems and compare their results with my numbers. Note that the benchmark is
not designed for users who just want to use a parallel library, and have no concern
on eﬀorts to implement a code.
The contributions of this benchmark include:
• Four platforms are reviewed for their architecture, system design, program-
ming/query interface and applicability.
• A benchmark is proposed that considers both the performance and programma-
bility of selected platforms.
• The benchmark is coded and run on the four platforms.
• Results are given that are useful for understanding the merits and drawbacks
of each platform.
1.7
Thesis Organization
This thesis is organized as follows. Chapter 2 introduces the PGRF model as well as
its associated parallel inference algorithm. Chapter 3 presents the SimSQL database
system, focusing on the compiler and the ML runtime design. Chapter 4 gives the

12
details of my benchmark design, the comparison results and the limitations/merits of
each platform. Finally, I conclude with some discussion of future work in the area of
big ML in Chapter 5.

13
Chapter 2
High-Dimensional Data Imputation
In this chapter, I consider the problem of imputation in very high-dimensional data
with an arbitrary covariance structure. The modern solution to this problem is the
Gaussian Markov random ﬁeld (GMRF) [3]. However, utilizing a GMRF requires ac-
cess to a sparsiﬁed, inverse covariance matrix Σ−1 for the data. Computing Σ−1 using
even state-of-the-art methods is very costly, as it typically requires ﬁrst estimating
Σ from the data (at a O(nm2) cost for m dimensions and n data points) and then
performing a regularized inversion of the estimated Σ, which is also very expensive.
This is impractical for even moderately-sized, high-dimensional data sets.
I propose a very simple alternative to the GMRF called the pairwise Gaussian
random ﬁeld or PGRF for short. The PGRF is a graphical, factor-based model. Un-
like traditional Gaussian or GMRF models, a PGRF does not require a covariance or
correlation matrix as input. Instead, a PGRF takes as input a set of p (dimension,
dimension) pairs which the user suspects to be strongly correlated or anti-correlated.
This set of pairs deﬁnes the graphical structure of the model, with a simple Gaus-
sian factor associated with each of the p (dimension, dimension) pairs. Using this
structure, it is easy to perform simultaneous inference and imputation of the model.
The key beneﬁt of the approach is that the time required for the PGRF to perform
inference is approximately linear with respect to p; where p will typically be much
smaller than the number of entries in a m × m covariance or precision matrix.

14
2.1
Introduction
Imputation of missing values is one of the most classic problems in statistics and
machine learning [22, 23, 28].∗In this problem, I assume n data points of the form
X = ⟨x1, x2, ..., xn⟩with xi ∈ℜm, where each xi is sampled from a distribution with
unknown probability density function f. Then, X is censored to produce X′, where
each x′
i,j in X′ either takes the value xi,j, or the value ∅, which denotes that xi,j is
missing from X′. The standard assumption is that X is censored without looking at
the data; that is, the data value itself has no bearing on whether it is censored, so
f(xi,j|x′
i,j = ∅) = f(xi,j). While this assumption typically does not hold in reality,
it simpliﬁes the problem, and methods developed under this assumption work quite
well in practice.
The imputation problem is then deﬁned as the problem of re-constructing X from
X′. Often, this is done by trying to infer f by analyzing X′. Then, each xi can be
reconstructed by sampling from (or ﬁnding the most likely value of) f(xi|x′
i).
In practice, one of the most common choices for f is the Gaussian or multi di-
mensional normal distribution (denoted by fN). The Gaussian distribution has many
advantages, e.g., it is by far the simplest mechanism for modeling covariance, despite
that it can be problematic for some applications (consider, for example, categorical or
binary data). But this inconvenience is easily handled in a variety of ways, including
using probit-like [56] techniques.
Problem. Unfortunately, a signiﬁcant problem with Gaussian imputation is that
it can be prohibitively expensive computationally for high-dimensional data. There
are two primary reasons for this expense.
∗Classically, there has been a distinction between imputation and multiple imputation; in this
chapter I use the former term to refer to both problems.

15
First, the covariance matrix Σ contains m2 entries; even evaluating fN(xi|x′
i, µ, Σ)
takes O(m3) time. Given that m may exceed 104 in practice, this is problematic.
The cubic complexity leads in practice to the use of the Gaussian Markov random
ﬁeld (GMRF) [3], which can be seen as a generalization of the Gaussian distribution
that accepts the inverse covariance matrix Σ−1 (also known as the precision matrix)
rather than the covariance matrix as a parameter. The GMRF is much more compu-
tationally eﬃcient when Σ−1 is sparse. Using methods such as Markov Chain Monte
Carlo (MCMC) [21], it is possible to sample from fGMRF(xi|x′
i, µ, Σ−1) in time that is
roughly linear with the number of non-zero entries in Σ−1 [57]. This is a huge improve-
ment over the Gaussian distribution, and the utility of the GMRF has motivated a
lot of recent work in estimating a sparse precision matrix from a data set [58, 59, 60].
These methods typically employ an optimization framework that uses a regularization
term to enforce sparsity [61]. However, this does not alleviate the problem entirely.
It is still very expensive to estimate Σ−1 from a data set. Methods for estimating Σ−1
often begin with the standard estimator for Σ:
1
N
P
i(xi −µ)(xi −µ)T, which requires
O(nm2) time. If n >> m and m itself is large, this is likely not feasible.
Second, the expense of utilizing a Gaussian model arises from the fact that I do
not actually have access to X but X′, which contains the censored ∅values. Using
only those known data values (x′
i,j ̸= ∅) in X′ can be problematic, since the resulting
estimate for Σ may not be positive deﬁnite. A standard method for handling this
is to perform imputation concurrently with estimation of Σ using methods like the
Expectation Maximization (EM) [62] or MCMC. However, both re-estimation of Σ
and re-imputation incurs a Θ(nm2) cost or greater at each iteration.
One state of the art method [60] suggests ﬁrst estimating Σ as described above
(that is, ignoring missing values) and then using a method such as QUIC [58] to

16
compute a regularized estimate of Σ−1 from Σ. But again, even estimating Σ in this
way requires O(nm2) time, not to mention the task of estimating Σ−1 from Σ.
Solution. So far, I have shown that the main challenge of Gaussian imputation
lies on its computation of complete covariance matrix. One can argue fairly easily
that no existing method for Gaussian imputation is a reasonable choice for use with
a moderately-sized data set having m on the order of 104 or larger.
In response to this, I propose a very simple, factor-based model for “Gaussian-
like” imputation called the pairwise Gaussian random ﬁeld or PGRF for short. Unlike
traditional Gaussian or GMRF models, a PGRF does not require a covariance or cor-
relation matrix as input. Instead, a PGRF takes as input a set of p (dimension,
dimension) pairs which the user suspects to be strongly correlated or anti-correlated.
This set of pairs deﬁnes the graphical structure of the model, with a simple Gaus-
sian factor associated with each of the p (dimension, dimension) pairs. Using this
structure, it is easy to perform simultaneous inference and imputation of the model.
The key beneﬁt of the approach is that the time required for the PGRF to perform
inference is approximately linear with respect to p; where p will typically be much
smaller than the number of entries in a m × m covariance or precision matrix.
This method requires that p (dimension, dimension) pairs are speciﬁed, but ﬁnding
an appropriate set of pairs is much easier than estimating Σ or Σ−1. Domain knowl-
edge or even simple methods suﬃce, e.g, one can use Fisher Z-transformation [63] on
a small sample of the data set to provide a rough estimate of the m2 correlations, and
then select the top few correlated (dimension, dimension) pairs.
Contribution. The main contributions of this chapter are as follows:
1. The PGRF model is proposed and applied to the imputation problem. Com-
pared with the Gaussian or GMRF methods, the cost of learning and applying

17
the model is Θ(n(m + p)), which is much better than Θ(nm2).
2. An MCMC algorithm for learning the model and imputing missing values is pre-
sented. One novel aspect of this algorithm is that it handles the normalization
problem in Markov random ﬁeld by using a Monte Carlo integration method.
3. An experimental study veriﬁes the utility of the model. Rather than focusing on
the ability of the model to recover missing data—which is questionable, since
imputation itself is typically not the ﬁnal goal in practice—my experimental
study focuses on the utility of the PGRF model for imputation in the context
of a variety of high-dimensional ML tasks. I show that PGRF results in much
higher accuracy, particularly when the model is inferred jointly with a classiﬁer.
4. One very important ﬁnding is perhaps not surprising but often underscored: to
get the most out of imputation, it is better to simultaneously perform imputa-
tion, learn the distribution f, and also simultaneously learn the model consum-
ing the imputation. For example, if imputation is undertaken for a classiﬁcation
model, a single learning algorithm with two tasks provides more utility than a
two stage impute-then-learn strategy. Fortunately, this is easy with the PGRF
model, as I show in the experimental section.
The next section of the chapter describes the PGRF model.
In Section 2.3 I
describe a Markov Chain Monte Carlo algorithm for simultaneous imputation and
inference.
The next section describes a set of experiments, where I examine the
utility of the PGRF model for use in a variety of supervised learning tasks. Section
2.5 considers related work, and Section 3.9 gives a short conclusion.

18
2.2
The PGRF Model
2.2.1
Basic Factor Model
I utilize a simple, factor-based Markov random ﬁeld. The user speciﬁes a set Ψ of
(dimension, dimension) pairs.
Each pair of data dimensions (j, k) for j < k are
conditionally independent unless (j, k) ∈Ψ. For such a (j, k) pair, the PGRF model
allows for an explicit correlation by utilizing the following factor:
ψj,k(xi) = fN
Ã 

xi,j
xi,k

|


µj
µk

,


σ2
j
σj,k
σj,k
σ2
k


!
In this expression, fN(.) is the two-dimensional normal distribution.
In addition, if a data dimension is not mentioned in Ψ, then it has a simple
Gaussian factor taking the form:
ψj(xi) = Normal(xi,j|µj, σ2
j)
Let Ψ denote the set of dimensions that do not appear in any (j, k) pair in Ψ. Then
the PDF associated with the factor model is simply:
fPGRF(xi|Ω) = 1
Z


m
Y
j∈Ψ
ψj(xi)



Y
(j,k)∈Ψ
ψj,k(xi)


Thus, each dimension that is not mentioned as being part of any explicitly-
represented correlation is given a Gaussian factor to control its distribution, and
in addition there is one Gaussian factor controlling the joint distribution of each pair
of dimensions for which an explicitly correlation has been requested.†
†While I have chosen the normal distribution, there is nothing special about the normal that
renders it the only applicable choice. In fact, the normal could be replaced with any distribution for
which a bivariate version exists, such as the gamma distribution, for example [64].

19
1
2
3
4
5
 1 3
!
 1 5
!
 3 5
!
 4 5
!
 2
Figure 2.1 : Example, a ﬁve-dimensional PGRF model. Factors are represented by
boxes, variables by circles. The set Ψ in this example speciﬁes four correlations to
model: {f(1, 3), (1, 5), (3, 5), (4, 5)}. Thus, there are four bivariate Gaussian factors
connecting these four pairs of dimensions. Since dimension 2 has no correlations, a
univariate Gaussian factor is attached to it.
The full set of parameters Ωfor the factor model is:
{µi, σ2
i |i ∈{1...m}} ∪{σj,k|(j, k) ∈Ψ}
Note that in fPGRF, Z is a normalization term that ensures that fPGRF(xi|Ω) is
in fact a proper probability distribution. Thus, Z takes the form:
Z =
Z
x

Y
j∈Ψ
ψj(x)



Y
ψj,k∈Ψ
ψj,k(x)

dx
As an example, ﬁve-dimensional PGRF model is depicted in Figure 2.1.
Note that fPGRF(xi|Ω) bears a strong resemblance to the multivariate normal
distribution with the key diﬀerence that only a (potentially) small set of correlations
Ψ are used. As intimated in the previous section, one can choose a relatively small
size for Ψ (linear with the number of dimensions m) based upon domain knowledge,
or based upon a small sample of the data, and thus sidestep the problem of computing

20
all of the m2 entries in the full covariance matrix (in the case of a classical, Gaussian
model) or producing a sparse precision matrix (in the case of a GMRF).
2.2.2
Priors on the Model Parameters
While there is nothing particular to the PGRF model that is inherently Bayesian—
likelihood-based approaches can be used in conjunction with the model, for example—
in the rest of the chapter I assume that the model will be used in Bayesian fashion.
This has the advantage of allowing for simultaneous inference of the missing data in
X′ as well as the parameters of the distribution function fPGRF.
In my Bayesian approach, the generative process I utilize for the set of parameters
Ωis:
(1) For j ∈{1...m}:
1. σ2
j ∼InvGamma(1, 1)
2. µj ∼Normal(c1, c2)
(2) For (j, k) ∈Ψ:
1. σj,k ∼Uniform(−
q
σ2
jσ2
k,
q
σ2
jσ2
k)
And thus fΩ(Ω) becomes:
P(Ω) =
Ã m
Y
j=1
InvGamma(σ2
j|.) × Normal(µj|.)
!
×

Y
(j,k)∈Ψ
Uniform(σj,k| −
q
σ2
jσ2
k,
q
σ2
jσ2
k)


2.3
Markov Chain Monte Carlo
When I utilize the PGRF in practice, I am given the set of correlations Ψ to explicitly
model, as well as a censored data set X′; the task is then to simultaneously learn a

21
PGRF model (that is, to learn the complete set of parameters Ω) and to impute the
missing values in X′. Since I will describe a Bayesian approach, in practice this means
inferring the distribution:
f(X, Ω|X′) =f(X′, X, Ω)
f(X′)
=f(X′|X)fPGRF(X|Ω)fΩ(Ω)
f(X′)
∝f(X′|X)fPGRF(X|Ω)fΩ(Ω)
=







fPGRF(X|Ω)fΩ(Ω) if X consistent with X′
0 otherwise
(2.1)
Here “consistent with” means that xi,j = x′
i,j where x′
i,j ̸= ∅. As I discuss now, I can
characterize this distribution using a relatively simple MCMC algorithm.
2.3.1
MCMC Basics
I will approximate f(X, Ω|X′) by drawing samples from the distribution using a Gibbs
sampler. The Gibbs sampler algorithm has been described in the section 1.3.
Gibbs sampler requires the conditional, posterior distribution for each of the values
of interest. In my case, the values of interest contain (1) each xi,j where x′
i,j = ∅, (2)
each µj, (3) each σj, and (4) each σj,k. I give these derivations now.
2.3.2
Sampling Unknown Data Values
Sampling xi,j using Equation 1 (speciﬁcally, generating xi,j ∼f(xi,j|{µj′}, {σ2
j′, {σj′,k},
X without xi,j)) is trivial in the case where x′
i,j ̸= ∅, since the value chosen must be
consistent with x′
i,j; in this case, I keep xi,j ﬁxed at x′
i,j.
It is nearly as easy when j ∈Ψ since xi,j will have no correlations and will be
normally distributed. I simply generate xi,j as xi,j ∼Normal(xi,j|µj, σ2
j).

22
In the case where j is not in Ψ (hence, dimension j is correlated with one or more
additional dimensions) things are a bit more interesting. In this case, I know that:
f(xi,j|.) ∝


Y
(j′,k)∈Ψ s.t. j′=j∨k=j
ψj′,k(xi)


Recall that ψj′,k(xi) simply evaluates a two-dimensional, normal PDF. For notational
convenience, I will subsequently ignore the fact that j can appear as either the ﬁrst
or second dimension listed in a (dimension, dimension) pair, and re-write this as:
f(xi,j|.) ∝

Y
(j,k)∈Ψ
ψj,k(xi)


This function is very easy to compute, hence I could apply an algorithm such as
rejection sampling to produce xi,j ∼f(xi,j|.). However, rejection sampling is often
expensive, and upon inspection it is clear that f(xi,j|.) is in fact a normal distribu-
tion. This is easy to see: since ψj,k is normal, then ψj,k conditioned upon another
dimension in xi (besides j) is also normal. Since every normal distribution over xi,j
is an exponentiated quadratic function, when I multiply together these conditional
distributions, I am exponentiating the sum of the quadratic functions. Hence, the
resulting product will be an exponentiated quadratic function of xi,j, which is normal.
To compute the parameters of this normal distribution, a bit of arithmetic and
elementary linear algebra are necessary. First, consider the following precision matrix:



σ2
j
σj,k
σj,k
σ2
k



−1
Let aj,k denote the upper left-hand entry in this matrix, and let bj,k denote the upper
right-hand entry in the matrix.
Conditioning the two-dimensional normal factors
upon the known values for xi,k where k ̸= j, I can then re-write the expression for

23
f(xi,j|.) as:
f(xi,j|.) ∝
Y
(j,k)∈Ψ
p
(aj,k)
√
2π e−
µ
xi,j−µj+
bj,k
aj,k (xi,k−µk)
¶2
aj,k
2
∝
Y
(j,k)∈Ψ
e−
µ
xi,j−µj+
bj,k
aj,k (xi,k−µk)
¶2
aj,k
2
=
Y
(j,k)∈Ψ
e−(xi,j−µi,j,k)
2
aj,k
2
for:
µi,j,k = µj −bj,k
aj,k
(xi,k −µk)
This can be re-written as:
f(xi,j|.) ∝
Y
(j,k)∈Ψ
e−(x2
i,j−2xi,jµi,j,k+µ2
i,j,k)aj,k
2
= e−P
(j,k)∈Ψ (x2
i,j−2xi,jµi,j,k+µ2
i,j,k)aj,k
2
= eαx2
i,j+βxi,j+γ
where:
α = −
X
(j,k)∈Ψ
aj,k
2
β =
X
(j,k)∈Ψ
aj,kµi,j,k
Since each normal distribution is an exponentiated quadratic function of this form, I
have:
f(xi,j|.) = Normal
µ−β
2α , −1
2α
¶
Thus, it is extremely simple and eﬃcient to sample from f(xi,j|.); the computational
complexity is linear in the number of dimensions that correlates with dimension k.

24
2.3.3
Updating the Factor Means
Updating the each of the µj values is more diﬃcult, because all of the various µj
values appear in the Z term. Fortunately, upon examination, it becomes clear that
even though µj appears in Z, Z is constant over varying values of µj, and thus I can
actually ignore Z when updating each µj. This can be shown mathematically, but the
informal reasoning is as follows. When each of the normal factors are evaluated over a
particular value for x, in every case the only terms in which either x or µj appear are
diﬀerences of the form (xi,j −µj). Because all possible x values are considered during
the integration, changing a particular µj by adding ϵ to it cannot then change the
value of the integral—the various (xi,j −µj) diﬀerences will still evaluate to exactly
the same set of values when all of the possible x values are integrated over.
Since I can ignore Z, there are two cases that fall out of Equation 2.1 when
updating µj. Either j ∈Ψ, or it is not. In the case where j ∈Ψ, µj appears only in
the one, uni-dimensional Gaussian factor associated with dimension j, and I have:
f(µj|{µj′ s.t. j′ ̸= j},{σ2
j′}, {σj′,k}, X) ∝fN(µj|c1, c2)fN(1
n
X
i
xi,j|µj, 1
nσ2
j)
This is proportional to:
e
−
(µj−c1)2
2c2
−
n( 1
n
P
i xi,j−µj)2
2σ2
j
∝e
−
µ2
j
2c2 +
c1µj
c2 −
nµ2
j
2σ2
j
+(
P
i xi,j)µj
σ2
j
=eαµ2
j+βµj
where α = −σ2
j + nc2
2σ2
jc2
and β = c1σ2
j + (P
i xi,j) c2
σ2
jc2
Then (as in the sampling of xi,j) I have:
f(µj|.) = Normal
µ−β
2α , −1
2α
¶

25
In the second case, j is not in Ψ.
Here, µj can participate in many diﬀerent
bivariate correlation factors. In this case, I have:
f(µj|{µj′ s.t. j′ ̸= j}, {σ2
j′}, {σj′,k}, X) ∝
fN(µj|c1, c2) ×
Y
(j,k)∈Ψ
fN(µj|1
n
X
i
xi,j + bj,k
aj,k
Ã
1
n
X
i
xi,j −µk
!
,
1
naj,k
)
with aj,k and bj,k deﬁned as before.
Deﬁne µj,k so that:
f(µj|{µj′ s.t. j′ ̸= j},{σ2
j′}, {σj′,k}, X) ∝fN(µj|c1, c2)
Y
(j,k)∈Ψ
fN(µj|µj,k,
1
naj,k
)
This is proportional to:
exp

−µ2
j
2c2
+ c1µj
c2
−µ2
j
X
(j,k)∈Ψ
naj,k
2
+ µj
X
(j,k)∈Ψ
naj,kµj,k


Which I again re-write as:
eαµ2
j+βµj
where α = −
1 + c2n P
(j,k)∈Ψ aj,k
2c2
and β =
c1 + c2n P
(j,k)∈Ψ aj,kµj,k
c2
Using these values, I can again update f(µj|.) as:
f(µj|.) = Normal
µ−β
2α , −1
2α
¶
2.3.4
Updating the Factor Variances and Covariances
Updating each σ2
j and σj,k is more challenging, because here the normalization term
Z actually changes with these values. Thus, when updating these values via Gibbs
sampling, I cannot ignore the Z term, and it must be evaluated.

26
I will do this using Monte Carlo integration, taking advantage of the fact that
the function being integrated over is actually my own factor-based model, hence it is
easy to draw samples from such a model, using the machinery of Section 2.3.2. This
makes Monte Carlo integration a natural choice.
I start by considering the update of σ2
j. The basic idea is to break the set of
factors into two subsets: those that reference dimension j, and those that do not.
I will sample from the distribution induced by the ﬁrst set, and then evaluate the
obtained samples over the second set to approximate the integral.
To do this, ﬁrst let Ψj denote the set of all dimensions which (a) are not included in
Ψ−{j}, and (b) are not referenced by any pair of dimensions in {(j′, k) ∈Ψ s. t. j′ ̸=
j ∧k ̸= j}. In other words, Ψj is the set of all dimensions that are left uncovered by
any factor once I remove all factors that reference dimension j.
Then I re-write Z as:
Z =
Z
x
f1(j, x)f2(j, x)dx
where:
f1(j, x) =

Y
j′̸=j∈Ψ
ψj′(x)

×


Y
ψj′,k∈Ψ s.t. j′̸=j∧k̸=j
ψj′,k(x)

×

Y
j′∈Ψj
fN
ÃX
i
xi,j′
n ,
X
i
x2
i,j′
n
−(
X
i
xi,j′
n )2
!

Thus, f1(j, x) is essentially the original factor model, evaluated at x, with all factors
involving j removed, and any dimensions that are left uncovered (those in Ψj) now
covered by a new, normal factor, whose mean and variance are computed from the

27
observed data. Then f2(j, x) is deﬁned as:
f2(j, x) =

Y
j′∈Ψj
fN
ÃX
i
xi,j′
n ,
X
i
x2
i,j′
n
−(
X
i
xi,j′
n )2
!

−1
×







ψj(x) if j ∈Ψ
³Q
(j,k)∈Ψ ψj,k(x)
´
otherwise
Once I have decomposed the integrand in this way, I know from basic Monte Carlo
theory that I can produce an unbiased estimate of the value of an integral of the
form
R
j,x f1(x)f2(j, x)dx (up to a constant factor) by drawing a set of samples S =
⟨s1, s2, ...sns⟩from the distribution g(s) ∝f1(j, s), and then computing the estimate:
ˆZ(σ2
j) = 1
ns
X
si∈S
f2(j, s)
Further, since f1(j, x) is actually an instance of the PGRF model (again, at least
up to a constant factor) which does not depend upon the value of σ2
j, it is easy to
sample the set S from it, using exactly the machinery of Section 2.3.2.
It is then an easy matter to derive the conditional posterior for σ2
j that can be
utilized during Gibbs sampling as:
f(σ2
j|{µj′},{σ2
j′ s.t. j′ ̸= j}, {σj′,k}, X) ∝
1
ˆZ(σ2
j)n×
Y
xi∈X
Ã







ψj(xi) if j ∈Ψ
1 otherwise
×
µ Y
(j,k)∈Ψ
ψj,k(xi)
¶!
×
InvGamma(σ2
j)
I now consider the update of σj,k, which can be handled similarly. This time,
let Ψj,k denote the set of all dimensions which are not referenced by any pair of
dimensions in Ψ −{(j, k)}. In other words, Ψj,k is the set of all dimensions that are

28
left uncovered by any factor once I remove the factor (if present) that references both
dimension j and dimension k. Note that I do not need to worry about Ψ since I do
not worry about σj,k unless dimensions j and k are covered by a correlation. If they
are covered by a correlation, then I know that they cannot be present in Ψ.
Similarly to last time, I re-write Z as:
Z =
Z
x
f3(j, k, x)f4(j, k, x)dx
where:
f3(j, k,x) =

Y
j′∈Ψ
ψj′(x)

×


Y
(j′,k′)∈(Ψ−{(j,k)})
ψj′,k′(x)

×

Y
j′∈Ψj,k
fN
ÃX
i
xi,j′
n ,
X
i
x2
i,j′
n
−(
X
i
xi,j′
n )2
!

f4(j, k, x) is deﬁned as:
f4(j, k, x) =

Y
j′∈Ψj,k
fN
ÃX
i
xi,j′
n ,
X
i
x2
i,j′
n
−(
X
i
xi,j′
n )2
!

−1
× ψj,k(x)
As before, I can produce an unbiased estimate of the value of the integral by
drawing a set of ns samples S from g(s) ∝f3(j, k, s), and then computing
ˆZ(σj,k) = 1
ns
X
si∈S
f4(j, k, s)
The conditional posterior for σj,k is then:
f(σj,k|{µj′}, {σ2
j′},{σj′,k′ s.t. (j′, k′) ̸= (j, k)}, X) ∝
1
ˆZ(σj,k)n ×
Ã Y
xi∈X
ψj,k(xi)
!
× Uniform(σj,k| −
q
σ2
jσ2
k,
q
σ2
jσ2
k)

29
2.4
Evaluation
2.4.1
Overview
In this section, I evaluate the PGRF model. There are two questions that I wanted
to answer:
1. Does the model provide useful results?
2. Is the model reasonable computationally?
To answer the ﬁrst question, I will consider the utility of values imputed by the
PGRF model for solving a set of supervised learning tasks. If the PGRF model is
useful, then a predictive model should have much lower error using the values imputed
by the PGRF model, as opposed to a naive imputation methodology.
To answer the second question, I implemented the PGRF model in a cluster-based
environment. I consider the ability of the MCMC model learning algorithm to scale
out as I add more and more data.
2.4.2
Implementation and Hardware
I implemented the PGRF Gibbs sampler described in this chapter in Java using
Hadoop. My implementation consists of around 18, 000 lines of Java source code. My
parallelization of the Gibbs sampler is quite straightforward. The Gibbs sampling
takes place in a set of “super-cycles”. In a super cycle, ﬁrst, all of the xi values
are partitioned around the cluster. In a super-cycle, all of the missing xi,j values
in x′
i are re-sampled 2 × log(number of missing values in x′
i) times in a circular
fashion. Since there are no dependencies when imputing the missing values in any
xi and xj pair for i ̸= j, the imputation is embarrassingly parallelizable. Once all

30
of the missing values in each xi have been updated, I then update all of the values
in Ω. The parallelization here is a bit more intricate, as all of the values in Ωare
not independent, so it is necessary to take care in order to avoid conﬂicts. I use an
independent-set based partitioning to account for this [65].
All of my experiments were conducted using a rented Amazon EC2 cluster. I used
the (relatively inexpensive) Amazon EC2 “high CPU extra large” instance type; the
computational power of one of these virtual machines is now somewhat less than that
of a typical desktop machine). The cluster having one machine costs 7 cents per hour
(Amazon EC2 spot pricing).
2.4.3
Model Quality Experiments
Setup. To evaluate the utility of the PGRF model, I focus on the ability of the
model to perform an imputation that is useful for a classiﬁcation/regression task. I
deliberately eschew a direct study of “imputation accuracy” because in my opinion,
reports of imputation accuracy (measured in terms of the RMSE between the imputed
value and the actual value, for example) are in the best case mostly useless, and in
the worst case harmful since they paint an inaccurate picture of the utility of the
underlying method. In practice, imputation is almost always performed with some
end goal in mind; typically, the goal is recovering lost or missing data to perform
a predictive task or learn some sort of statistical model. Rarely if ever is there a
concern about the individual data values that are imputed; the concern is how useful
the imputed values are. If data can be imputed with high accuracy and yet that
accuracy does not result in a better ability of a learned predictive model to perform
its task, there is no reason to perform the imputation. Conversely, if the imputation
accurately is relatively low and yet the utility for assisting with the task at hand is

31
high, the imputation is in fact quite useful.
Keeping this in mind, on a handful of high-dimensional classiﬁcation/regression
tasks, I learn and evaluate the following six models:
1. A Bayesian linear regression model [66], learned with mean imputation (that is,
I simply replace missing values with the mean value for that dimension).
2. A Bayesian linear regression model, learned jointly with PGRF-based imputa-
tion (that is, the regression parameters, the parameters in Ωand the missing
values are incorporated into the Gibbs sampler).
3. A Gaussian support vector machine (SVM), learned with mean imputation.
4. A Gaussian SVM, learned with data imputed by PGRF beforehand.
5. A linear SVM, learned with mean imputation.
6. A linear SVM, learned with PGRF imputation beforehand.
The classiﬁcation/regression tasks that I performed were as follows:
1. Predicting whether a 20-newsgroups document comes from alt.talk.religion (a
+1 label) or not (a −1 label). To do this, I computed the 10,000 most common
words across the 20,000 documents in the data set, and used those to convert
each document into a tf-idf vector with 10,000 dimensions.
2. Predicting a stock price. Each row in the Stocks data set consists of 499 stock
prices collected over two days, day d and day d + 1; thus the data has 998
dimensions. There are approximately 20 years of data in the set. The prediction
task is using those stock prices to predict the price of a 500th stock price on day
d + 1. 8.35% of this data are missing without artiﬁcially discarding any values.

32
3. Predicting a river ﬂow level. The Rivers data set consists of river discharge
levels for 4,442 rivers in the state of California, and was obtained from the
USGS web site. Flow levels for 12,054 days were collected. The task is using
ﬂow from 4,441 rivers to predict the ﬂow of the 4,442rd river. 48.6% of this
data are missing without artiﬁcially discarding any values.
4. Predicting the year of publication of a NIPS paper. The Nips data set consists
of 1,740 ML papers published over 13 years.
As the 20-newsgroups data, I
computed the 10,000 most common words across the documents in the data set,
used those to convert each document into a tf-idf vector with 10,000 dimensions.
On each of these data sets, I considered diﬀerent fractions of missing values (de-
noted using c) and diﬀerent numbers of correlations in Ψ. In each data set, I test
10% missing, 30% missing, 50% missing, and so on, up to 90% missing (note that
in the case of the River data, more than 48.6% of the data are already missing, so I
do not test numbers below that). For each data point, the fraction of missing values
is sampled from a Beta( 1
1−c −1, 1) distribution, with mean c. Masked values within
each data point are selected at random using Poisson sampling.
In each case, I train on 70% of the data, and then test the remaining 30%.
Results. In Figure 2.2 I plot, for each of the four data sets and the six diﬀerent
combination learning algorithms/imputation strategies, the average RMSE of the
predicting as a function of the fraction of the missing values. In the experiments
depicted in all of these plots, the top 2,000 correlations are used; these are determined
using a sampling-based algorithm.
In Figure 2.3 (a)-(c), I plot, for each of the three diﬀerent predictive learning
algorithms (used in conjunction with the PGRF model) the RMSE as a function of

33
0
0.1
0.3
0.5
0.7
0.9
0.3
0.35
0.4
0.45
0.5
0.55
0.6
The fraction of missing values
RMSE
LR
LR with imputation
Gaussian SVM
Linear SVM
Gaussian SVM with imputation
Linear SVM with imputation
(a) 20-newsgroups
0
0.1
0.3
0.5
0.7
0.9
0
2
4
6
8
10
12
14
16
18
20
The fraction of missing values
RMSE
 
 
Linear SVM with imputation
LR
Gaussian SVM
Gaussian SVM with imputation
Linear SVM
LR with imputation
(b) Stocks
0
0.1
0.3
0.5
0.7
0.9
0
1000
2000
3000
4000
5000
6000
7000
The fraction of missing values
RMSE
 
 
Gaussian SVM
Gaussian SVM with imputation
Linear SVM
LR
LR with imputation
Linear SVM with imputation
(c) Rivers
0
0.1
0.3
0.5
0.7
0.9
3
3.1
3.2
3.3
3.4
3.5
3.6
3.7
The fraction of missing values
RMSE
 
 
LR
LR with imputation
Gaussian SVM
Linear SVM
Gaussian SVM with imputation
Linear SVM with imputation
(d) Stocks
Figure 2.2 : Prediction accuracy when varying the fraction of missing values.
the number of correlations that are used when learning the model. In each of these
plots, the fraction of missing data is 50%.
Discussion. The most striking ﬁnding is that in every case, the linear regression
model joint with the PGRF model was strongly dominant. In a sense, this is actu-
ally to be expected, because this method performs imputation simultaneously with
learning the model rather than a separate step. This allows the imputation model to

34
0
500
1000
1500
2000
2500
3000
3500
4000
0
0.5
1
1.5
2
The number of correlations
RMSE
 
 
rmse of Stock × 10
rmse of 20newsgroups
rmse of California River × 103
rmse of Nips × 10
(a) Linear regression
0
500
1000
1500
2000
2500
3000
3500
4000
0
0.2
0.4
0.6
0.8
1
1.2
1.4
The number of correlations
RMSE
 
 
rmse of Stock × 10
rmse of 20newsgroups
rmse 0f California River × 104
rmse of Nips × 10
(b) SVM with a linear kernel
0
500
1000
1500
2000
2500
3000
3500
4000
0
0.2
0.4
0.6
0.8
1
1.2
The number of correlations
RMSE
 
 
rmse of Stock × 10
rmse of 20newsgroups
rmse of California River × 104
rmse of Nips × 10
(c) SVM with a Gaussian kernel
0
5
10
15
20
25
30
35
40
0
2
4
6
8
10
12
14
16
18
Super Cycles
σy
2
 
 
California River × 107
Stock × 101
20newsgroups × 10−1
Nips × 100
(d) Regression prediction variance.
Figure 2.3 : Prediction accuracy when varying the number of correlations in Ψ.
be tailored to the speciﬁc prediction task at hand, increasing accuracy signiﬁcantly.
This underscores a very important point: undertaking imputation as a separate pre-
processing task (which is what I did with the two SVM-based options) may not be
worthwhile; at the very least, it signiﬁcantly dampens the utility of the imputation.
Instead, a method such as Expectation Maximization or MCMC that performs simul-
taneous imputation and model estimation should be strongly preferred.

35
0
0.2
0.4
0.6
0.8
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
σ2
i σ2
j
σ2
ij
cdf
Empirical CDF
 
 
20newsgroups
Stock
California River
Nips
Figure 2.4 : Cumulative density function of the distribution of the strength of the
observed correlations for the data sets.
In fact, the only data set where the two-step imputation-then-learn was particu-
larly useful was the Rivers data set, which had a strong covariance structure. Figure
2.4 plots the CDF of the various correlations in each data set, and clearly shows
that the Stocks and Rivers data sets have strong correlations. Not only does this
explain that imputation-then-learn performed best on the Rivers data set, but it also
explains Figure 2.3(a). For both the Stocks and Rivers data sets, adding additional
correlations to Ψ reduces the RMSE signiﬁcantly, up until about 1,000 correlations
total. Adding more correlations has much less signiﬁcant eﬀect on the other two data
sets, both of which lack a strong correlation structure.
A relatively surprising ﬁnding is that the Bayesian linear regression—even using
simple, mean-based imputation—did a better job than the SVM-based learners. The
partial explanation is that three of the four tasks were regression tasks, where an SVM
might not be the best choice, particularly vis-a-vis linear regression. But surprisingly
I found that even in the classiﬁcation task on the 20-newsgroups data set, a carefully-

36
1
3
5
10
20
50
100
0
1000
2000
3000
4000
5000
6000
7000
8000
The number of nodes in the cluster
The time per super cycle (seconds)
 
 
Imputation
Markov random field
total
Figure 2.5 : Running time as a function of compute cluster size; data size per machine
remains constant.
designed regression model with regularization was a better choice.
2.4.4
Computational Eﬃciency
Setup. My experiment here is quite straightforward. On an n-node Hadoop cluster,
I replicate the 20-newsgroups data set n times, with 50% of the data values missing.
Thus, for an n-node cluster, there are 2n × 108 total values in X, with n × 108 values
missing. I considered various cluster sizes, testing n in 1 to 100; at 100, there are
then 20 billion individual values in X, and 10 billion of them must be imputed. For
each cluster size, I measured the total time required per super-cycle.
Results. The time per cycle is plotted as a function of cluster size in Figure 2.5. To
put these times into context, it is necessary to know how many super-cycles must be
run. Figure 2.3(d) plots the variance of the prediction made by a linear regression
joint with the imputation as a function of the number of super-cycles for all the

37
datasets.
Discussion. As one would expect how embarrassingly parallelizable the imputation
is, the time required for the imputation per super-cycle stays constant as I add ma-
chines and data, no matter the size of the cluster. The time required to update the
PGRF model actually drops as the size of the cluster increases, since the size of the
model stays ﬁxed while the additional computational power speeds the learning.
It is reasonable to consider how expensive all of this actually is. In the case of
the 10,000 dimensional 20-newsgroups data set, I require around one hour per super-
cycle, and from Figure 2.3(d) I see that 10 super-cycles are required. This is around
$.70 of compute time per machine, or instance of the 20-newsgroups data set. In
contrast, running a Bayesian linear regression on the same setup requires about $.80
of compute time per machine. Thus, employing the PGRF model roughly doubles the
cost compared to Bayesian linear regression alone, and results in an RMSE reduction
of between 10% (in the Nips data) and 45% (in the rivers data). For most applications,
this RMSE reduction should be signiﬁcant enough to justify the additional cost.
Finally, note that in the Nips data set, the variance of the regression model drops
signiﬁcantly but then actually increases past ten super-cycles. One might explain
this by the fact that this was the most diﬃcult prediction task, causing the MCMC
algorithm to have a more diﬃcult time reaching the stationary distribution.
2.5
Related Work
Imputation is a classical problem in statistics [67, 68].
For decades, a variety of
simple methods had been used that either deleted the cases with missing values or
edited the missing values with preset values [67]. Such methods are problematic,
e.g., introducing bias [26]. As a result, a variety of imputation methods [26] have

38
been introduced since the origin of modern imputation techniques marked by Rubin’s
theoretical framework described in a pioneer paper [68].
2.5.1
Old Methods
Case Deletion. Among the old methods, case deletion, also named complete-case
analysis or listwise deletion [67, 69] simply deletes cases with missing values. The
method reduces the eﬀective sample size, and thus decreases the power of the later
analysis. Available-case analysis or pairwise deletion [70], a special case deletion, uses
diﬀerent sets of data for diﬀerent analysis. However, when data are not missing at
random, case deletion introduces bias [26, 71].
Reweighting. In cases when missing values are not missing completely at random,
using reweighting [23] after the case deletion may reduce bias. In this method, the
estimates are computed based on the weighted observations. The weights of observa-
tions are derived from the probabilities estimated by methods such as logistic regres-
sion [72]. Reweighting applies to univariate cases, but is problematic for arbitrary or
multivariate patterns due to repeated weight estimation.
Mean Imputation. Mean imputation [30] replaces the missing value with the mean
of the variable from the observations.
It does not change the sample mean but
decreases its variance [23]; furthermore, it weakens any correlations that involve this
variable, which makes it unattractive for multivariate analysis.
Hot-deck Imputation. Hot-deck imputation, in a simple case, may randomly select
a similar record for a missing value. Another related method is “last observation
carried forward” (LOCF) [25, 73] which ﬁrst sorts a dataset based on some variables
and then imputes a missing value by the most recent observation in the order. The
method partially preserves the distribution shape of the variable [74], but distorts

39
correlations in multivariate cases [26, 75]. By contrast, cold deck imputation selects
the substituted values from pre-processed data or another dataset [76].
Regression Imputation. Regression Imputation, also named conditional mean im-
putation [26, 27], creates a regression model between the variable that needs impu-
tation and the other variables. However, the model overestimates the correlations
among variables. As a result, Stochastic regression [28] adds noise variance to the
regression model, and therefore has less bias.
2.5.2
Maximum Likelihood Imputation
The following subsections describe the state of the art methods including maximum
likelihood imputation (ML Estimation) [22] and multiple imputation [23]. Both pro-
vide higher accuracy and power than simple imputation methods [67].
ML estimation ﬁrst assumes that a random model generates the dataset and then
identiﬁes the parameters Θ by maximizing the probability of the model.
A sim-
ple form of ML estimation, called observed-data likelihood [22], ﬁrst learns Θ from
the observations, and then uses Θ for imputation. A more general method is the
expectation-maximization (EM ) algorithm [62, 23, 24, 77]. EM iteratively and alter-
natively imputes the data according to the model and learns the model by imputed
data. PGRF follows a similar idea but uses MCMC [78] in Bayesian inference. The
EM algorithm applies to many other data analysis problems such as multilevel lin-
ear models [79, 80], latent class analysis [81], factor analysis [82] and ﬁnite-mixture
models [83].
ML estimation is highly eﬃcient for imputation, and the variance of the imputed
data approaches the theoretical lower bound achieved by any unbiased estimator
[84].
However, MI estimation rests on some assumptions.
First, it assumes that

40
the observations are large enough that the ML estimates are approximately unbiased
and normally distributed [26]. Second, the selection of a parametric model can be
challenging. A wrong model could cause standard errors and test statistics to be
very misleading [85]. Finally, the ML estimate assumes that the missing values are
missing at random [86, 87]. When this is not true, it would increase the diﬃculties
of modeling [87, 88].
2.5.3
Multiple Imputation
Multiple Imputation (MI ) [23, 26] imputes a dataset multiple times in order to sim-
ulate the uncertainty of missing values. Speciﬁcally, MI analysis has three stages:
imputation, analysis and pooling. The imputation phase generates multiple imputed
datasets by imputation methods such as stochastic regression. In the second stage,
the desired analysis is performed on each dataset separately, while the third stage
aggregates the analysis results such as parameter estimates and standard errors.
Such imputation serves an important purpose and requires special algorithms [24].
Like ML estimation, it assumes a statistical process that models observations and
missing values. Given this process, samples are drawn from P(Xmis|Xobs, ˆΘ), where
ˆΘ is an estimate from observations. When the same model is used for imputation
and analysis, MI produces answers similar to those with ML under the same model
[26]. Furthermore, both ˆΘ and Xmis have multiple copies in the imputation process.
Unlike ML, MI treats ˆΘ and
ˆ
Xmis as random rather than ﬁxed. As a result, it is
easy to put MI in Bayesian approach. However, MI has similar problems as with ML
estimation; e.g., it relies on large-sample approximations, appropriate model selection
[23, 24] and assumption that data are missing at completely random [26, 89].

41
2.5.4
Missing Values Are Not Missing Completely at Random
Other than the methods described so far, methodologists have proposed a number
of imputation models that work beyond the assumption that data are missing at
completely random, i.e., the selection model [28, 67, 29] and pattern mixture model
[90]. In the selection model, users ﬁrst specify distributions for the data as well as the
distribution for individual data to be missing; the pattern mixture model classiﬁes the
data into groups by their missing pattern and estimates the growth model separately
within each pattern. As for how these models speciﬁcally work, I recommend the
survey paper [26].
2.5.5
Large Scale Imputation
While the aforementioned methods and their statistical properties have been well
studied [28, 26, 23], their computational properties, i.e., the algorithm complexity
and parallelization, have generally been neglected. Admittedly, simple methods such
as mean imputation can be easily parallelized, and thus still enjoy use in practice
[91]. Nevertheless, as the data scales, sophisticated imputation methods such as ML
estimation and MI are more attractive for richer analytics. The selection of methods
depend on statistical properties as well as computational properties.
The Gaussian Markov random ﬁled (GMRF) [3] has been proposed to be the
solution to improve the scalability of Gaussian distribution. The pioneering work [61]
uses regularization to estimate sparse precision matrices. A number of recent eﬀorts
[58, 59, 92, 60] have been proposed to estimate sparse precision matrices eﬃciently
and some of these are explicitly geared towards handling missing data. However, these
methods do not alleviate the problem entirely. It is still very expensive to estimate
Σ−1 from a data set.

42
The PGRF diﬀers from these eﬀorts in that it sidesteps the computation of co-
variance, while modeling the arbitrary set of correlations from the input graphical
structure and imputing the missing data simultaneously. The algorithm complexity
is linear with the amount of missing data, as well as the magnitude of the graph struc-
ture. My experimental study focused on the utility of the PGRF model for performing
high-dimensional imputation in conjunction with a supervised learning problem. The
most relevant work in the literature is the paper of Loh and Wainwright [93] who
consider the problem of regression with missing values using an EM algorithm.
2.6
Conclusion
This chapter considered the problem of imputation for high-dimensional data. I pro-
posed a simple graphical, factor-based model called the pairwise Gaussian random
ﬁeld or PGRF. A PGRF model does not require a covariance matrix as input. In-
stead, it takes as input a set of p strongly correlated or anti-correlated (dimension,
dimension) pairs. These pairs deﬁne the graphical structure of the model, with a
simple Gaussian factor associated with each pair. Using this structure, it is easy to
perform simultaneous inference and imputation of the model. I described an MCMC
algorithm for learning the model and performing imputation.
A set of experiments evaluate the utility of the PGRF model for use in conjunc-
tion with several prediction tasks. In every case, when a PGRF model was learned
simultaneously with a regularized linear regression model, it signiﬁcantly increased
the accuracy of the predictions made by the model.
One very interesting ﬁnding was that if imputation is to be used for a task such as
classiﬁcation or regression, for maximum accuracy it is necessary to simultaneously
learn the imputation model, perform the imputation, and learn the predictive model.

43
If these subtasks are separated, it represents a lost opportunity to tailor the imputa-
tion to the predictive model and compromises accuracy. This argues quite strongly
in favor of imputation models (such as the PGRF) that work within frameworks such
as EM and MCMC, which are amenable to simultaneous imputation and predictive
learning.

44
Chapter 3
SimSQL for Large Scale Bayesian ML
This chapter describes the SimSQL system, which allows for SQL-based speciﬁcation,
simulation, and querying of database-valued Markov chains, i.e., chains whose value
at any time step comprises the contents of an entire database. SimSQL extends the
earlier Monte Carlo database system (MCDB) [48], which permitted Monte Carlo
simulation of static database-valued random variables. Like MCDB, SimSQL uses
user-speciﬁed “VG functions” to generate the simulated data values that are the
building blocks of a simulated database. The enhanced functionality of SimSQL is
enabled by the ability to parametrize VG functions using stochastic tables, so that one
stochastic database can be used to parametrize the generation of another stochastic
database, which can parametrize another, and so on. Other key extensions include
the ability to explicitly deﬁne recursive versions of a stochastic table and the ability
to execute the simulation in a MapReduce environment. I focus on applying SimSQL
to very large scale Bayesian ML.
3.1
Introduction
The SimSQL system allows for SQL-based speciﬁcation and large-scale, distributed
simulation of database-valued Markov chains, i.e., chains whose value at any time
step comprises the contents of an entire database. SimSQL employs many ideas in
the Monte Carlo database system (MCDB) [48], a prototype, SQL-based database

45
system designed to support stochastic analytics. Both SimSQL and MCDB allow
users to deﬁne—in addition to ordinary database tables—so-called stochastic database
tables. In a stochastic table, some entries are random variables with associated prob-
ability distributions (not necessarily known in closed form) and others are ordinary,
deterministic data values. Conceptually, when an SQL query is issued that includes
stochastic tables, both SimSQL and MCDB use pseudo-random number generation
techniques to instantiate each stochastic data value. Queries can be run over the
resulting database instance (also called a “possible world”); this process is repeated
many times in a Monte Carlo fashion to obtain an empirical distribution of the query
results. Both SimSQL and MCDB instantiate and process many possible worlds si-
multaneously and eﬃciently by sharing ﬁxed costs using a “tuple bundle” approach.
SimSQL contains the basic functionality of the original MCDB system, but also
has some signiﬁcant extensions. This chapter focuses most closely on two unique capa-
bilities to SimSQL that are not in MCDB and which signiﬁcantly increase SimSQL’s
applicability vis-a-vis the earlier system.
1. SimSQL allows data in one stochastic tables to parametrize the sophistical gen-
eration of other stochastic tables. This allows SimSQL to implement hierarchical
stochastic models such as latent Dirichlet allocation (LDA) [51].
2. SimSQL allows both versioning and recursive deﬁnitions of stochastic tables.
For example, data in stochastic table A can be used to parametrize the stochastic
generation of table B, which in turn can be used to parametrize the stochastic
generation of a second version of table A, and so on.
MCDB merely allows generation of sample realizations of a given stochastic database
D, i.e., a single static database-valued random variable. However, the foregoing ex-

46
tensions enable SimSQL to generate realizations of a database-valued Markov chain
D[0], D[1], D[2], . . .. That is, the stochastic mechanism that generates a realization
of the ith database state D[i] may explicitly depend on the prior state D[i −1].
Possible applications of this powerful functionality include Bayesian ML and massive
stochastic agent-based simulations. In this chapter I focus on ML applications.
Scalable Bayesian Machine Learning in SimSQL. This chapter gives several
examples of how SimSQL can support Bayesian ML over large data sets. In Bayesian
inference, it is assumed that a parametrized stochastic process generated the data.
The parameters are uncertain, and the goal is to combine prior beliefs about the
parameters with the observed data to infer the “posterior distribution” of the pa-
rameters. Depending upon the application, these parameters may describe clusters
in the data, regression coeﬃcients, or latent topics in text. Often, the “posterior dis-
tribution” can be very diﬃcult to describe analytically, so the most common analysis
approach is to sample from the posterior distribution using MCMC simulation.
SimSQL’s database-oriented approach gives a surprisingly appropriate set of ab-
stractions for Bayesian ML. In most ML problems, a few classes of variables or param-
eters are needed to be inferred. These might be latent variables (such as the identity
of a cluster that produced a particular data point) or model parameters (such as the
covariance matrices in a Gaussian mixture model). It is quite natural to group the
members of such a class together and store them in a database table.
MCMC inference is natural in SimSQL because of the VG function abstraction
that it borrows from MCDB. A VG function is a (nearly) arbitrary, (possibly) user-
deﬁned pseudo-random function that generates samples of uncertain data values;
these samples are then used to construct sample realizations of stochastic tables. The
sampling mechanism of a VG function depends on input tables as parameters. It is

47
very easy to embed the sampling of the posterior distribution of a variable or param-
eter inside of a VG function, which is needed exactly by many MCMC algorithms.
Furthermore, SQL is a very useful language for preparing complex parametrization
of VG functions. This resulting MCMC codes are small and easy to understand “in
the large”—that is, the speciﬁcation for how variables ﬁt together and parametrize
each other is often remarkably simple, even for complex models.
Perhaps the most important advantage of running MCMC in SimSQL is that one
can write SQL without worrying about how the code will be parallelized across many
machines. SimSQL automatically takes care of the optimization and parallelization.
SimSQL takes as input a speciﬁcation written entirely in SimSQL’s dialect of SQL,
then optimizes it and executes the actual simulation in parallel on a Hadoop cluster,
allowing for scalable processing.
Highlights.
• This chapter describes how SimSQL’s dialect of SQL makes it very easy to
declaratively specify Markov chain simulations.
• SQL codes for three Bayesian inference problems are given.
• Details are given on how SimSQL compiles, optimizes, and runs Markov chain
simulations.
• Experiments show that SimSQL has reasonable performance for running large-
scale, Markov chain simulations.

48
3.2
Bayesian Linear Regression
To demonstrate the use of SimSQL for large scale Bayesian analysis, I implement
MCMC inference for a very simple Bayesian linear regression model using SimSQL’s
dialect of SQL.
3.2.1
The Model
Suppose I have a large database table myTable(x,y) of n rows. My goal is to learn
two values a and b such that for an arbitrary observation x, I can predict y via the
formula y ≈ax + b. Although this is a toy problem—in practice, with such a simple
problem one is unlikely to use Bayesian methods—it makes a good ﬁrst example.
To apply Bayesian methodology to such a problem, an analyst will ﬁrst posit a
stochastic generative process for the data:
1. a ∼Normal(0, σ2
0).
2. b ∼Normal(0, σ2
0).
3. σ2 ∼InvGamma(1, 1).
4. For each i in {1...n}, yi ∼Normal(axi + b, σ2).
Here InvGamma denotes the inverse gamma distribution.
I assume that the observed value yi is normally distributed—with variance σ2—
around the predicted value axi + b. The unknown parameters a and b are themselves
treated as random variables and have normal “priors” with common variance σ2
0
(known as constant “hyperparameter”). The term “prior” in Bayesian parlance refers
to the probability distribution I initially assume to produce the parameter values. The
priors reﬂect previous knowledge or belief, or perhaps are chosen for computational

49
convenience. My beliefs about the prior distribution are updated upon observation
of the data in myTable, leading to a posterior distribution on the parameters; this
posterior distribution is the ultimate object of my study. Choosing an appropriate
generative process and set of priors is something of a “black art” and is beyond the
scope of the paper [31].
A stochastic generative process results in a probability density function (PDF) for
the random data and parameters. In my example, I have:
P(a, b, σ2, {yi}|{xi}) = Normal(a|0, σ2
0) × Normal(b|0, σ2
0)
× InvGamma(σ2|1, 1) ×
Y
i
Normal(yiaxi + b, σ2)
3.2.2
Inference
My goal is to estimate the parameters a, b, and σ2 after seeing the data in myTable(x,y),
i.e., the posterior distribution P
¡
a, b, σ2|{xi, yi}
¢
. Here, a Gibbs sampler is used to
sample the values for these parameters. The Gibbs sampler algorithm has been de-
scribed in the section 1.3. In my example, I can use Bayes’ rule together with the
PDF in (3.1) to derive the following conditional distributions:
P(a|.) ∝Normal(a|0, σ2
0) × e(2σ2)−1 P
i{2(yi−b)xia−x2
i a2}
P(b|.) = Normal
³
b
¯¯¯
P
i(yi −axi)
(σ2/σ2
0) + n ,
¡ 1
σ2
0
+ n
σ2
¢−1´
P(σ2|.) = InvGamma
³
σ2 ¯¯¯ 1 + n
2, 1 +
P
i(yi −axi −b)2
2
´
3.2.3
Learning the Model
A Gibbs sampler for this model must repeatedly update the values for a, b and σ2 in
a recursive fashion. The order to update these three parameters is arbitrary; I choose
to ﬁrst initialize a and b, and then update σ2, a and b in sequence.

50
CREATE TABLE tableA[0] (aValue) AS
WITH sampledVal AS Normal (SELECT * FROM VALUES (0, 10))
SELECT * FROM sampledVal;
CREATE TABLE tableB[0] (bValue) AS
WITH sampledVal AS Normal (SELECT * FROM VALUES (0, 10))
SELECT * FROM sampledVal;
Figure 3.1 : Initializing a and b (σ2
0 = 10).
CREATE TABLE variance[i] (s2Value) AS
WITH sampledVal AS InvGamma (
(SELECT 1 + dataNum/2 FROM
metaData),
(SELECT 1 + 0.5 *SUM ((y-aValue*x-bValue)*(y-aValue*x-bValue))
FROM myData, tableA[i], tableB[i])
)
SELECT sampledVal.value FROM sampledVal;
Figure 3.2 : Updating the variance σ2.
Initialization. I ﬁrst initialize the parameters a and b and store each in one-row
tables tableA(aValue) and tableB(bValue). In SimSQL, a stochastic table may
have a version number, which corresponds to a step of a Markov chain simulation
or more generally to a level of recursion. In my case, tableA[0] denotes the initial
version of tableA, and the SQL statement “CREATE TABLE tableA[0] (aValue) AS”
in Figure 3.1 describes precisely how this table is to be created. Speciﬁcally, the VG
function Normal accepts a parameter table and outputs a result table of normally
distributed samples, which is called table sampledVal; the table-valued output of
this VG function is then loaded into tableA[0] via the “SELECT *” subquery.
Updating the parameters. The code to update σ2, a and b—given in Figures 3.2

51
CREATE TABLE tableA[i] (aValue) AS
WITH sampledVal AS SampleA (
(SELECT * FROM VALUES (0, 10)),
(SELECT 1/(2*var.s2Value) FROM variance[i-1] as var),
(SELECT SUM (2*(y-bValue)*x), SUM(x*x) FROM myTable, tableB[i-1])
)
SELECT sampledVal.value FROM sampledVal;
Figure 3.3 : Updating the value of a (σ2
0 = 10).
CREATE TABLE tableB[i] (bValue) AS
WITH sampledVal as Normal (
(SELECT SUM(y-aValue*x)/(var.s2Value/10+dataNum)
FROM variance[i-1] AS var, myData, tableA[i], metaData
GROUP BY var.s2Value, dataNum),
(SELECT (1.0 / (1.0 / 10 + dataNum / var.s2Value))
FROM metaData, variance[i-1] AS var)
)
SELECT sampledVal.value FROM sampledVal;
Figure 3.4 : Updating the value of b (σ2
0 = 10).
through 3.4—is similar to the code in Figure 3.1, with a few notable diﬀerences. First,
the versioning variable i replaces the hard-coded 0; so that the SimSQL compiler
knows that variance, tableA and tableB are recursively deﬁned. Second, note the
references to variance[i-1] in Figures 3.3 and 3.4, and the reference to tableB[i-1]
in Figure 3.3. In this way, these stochastic table deﬁnitions reference “older” versions
of stochastic tables. Next, the code in Figure 3.3 uses the SampleA VG function. Since
P(a|.) has a non-standard distribution function, a library VG function cannot be used

52
and the user-deﬁned SampleA are used instead; this VG function encapsulates a user-
implemented “rejection” sampler [31] for P(a|.). The three subqueries compute the
statistics as inputs the rejection sampler: the ﬁrst (trivially) computes the parameter
vector (0, σ2
0), the second computes 1/(2σ2), and the third computes the quantities
P
i 2(yi −bxi) and P
i x2
i needed in the exponent that appears in the deﬁnition of
P(a|.). The situation is a bit simpler in Figures 3.2 and 3.4 in that the standard
InvGamma and Normal VG functions are used.
3.3
Latent Dirichlet Allocation
I now turn to a more realistic example, which is called LDA model [51]. LDA has many
applications, from document classiﬁcation to dimensionality reduction to document
indexing.
For reasons of exposition, I perform the inference using a “full” Gibbs
sampler, as opposed to the “collapsed” sampler [94].
3.3.1
Model and Inference
LDA models a document as a bag of words where each word is generated by ﬁrst
selecting a latent topic from a document-speciﬁc topic distribution and then generat-
ing the word from a topic-speciﬁc word distribution. The topic distribution for each
document is uncertain and has a Dirichlet prior. The Dirichlet family of “distribu-
tions over distributions” is the multivariate generalization of the beta distribution
over [0, 1]; for k ≥2 and parameter vector α = (α1, α2, . . . , αk), the PDF is given by
Dirichlet(x1, x2, . . . , xk|α) =







Γ
¡Pk
i=1 αi
¢
Qk
i=1 Γ(αi)
Qk
i=1 xαi−1
i
if xi ≥0 and Pk
i=1 xi = 1;
0
otherwise,

53
where each αi is positive and Γ is the standard gamma function. The word distribution
for each topic is also uncertain, with a “symmetric” Dirichlet prior in which α1 =
· · · = αk. With a slight abuse of notation, I use a symbol such as α to denote both a
positive constant and the vector (α, α, . . . , α).
Consider a dataset of n documents with nj words in the jth document and m
distinct words overall. Assuming that t topics are present in the data and ﬁxing
hyperparameters α and β, the generative process underlying LDA is formally as:
1. For i ∈{1...t}: Ψi ∼Dirichlet(β)
2. For j ∈{1...n}:
(a) Θj ∼Dirichlet(α)
(b) zj ∼Multinomial(nj, Θj)
(c) For k ∈{1...t}: wj,k ∼Multinomial(zj,k, Ψk)
In step (1), I ﬁrst produce a matrix Ψ with t rows and m columns; the row Ψi is a
list of probabilities, where Ψi,d is the probability that topic i will produce word d.
Ψi is sampled from a Dirichlet(β) distribution. I subsequently use Ψ∗,d to denote the
column listing the probability that each topic will produce word d.
Next, I produce (for each document) a probability vector Θj, where Θj,k is the
probability that a word in document j is generated from topic k. For document k,
I sample zj from a multinomial distribution with parameters nj and Θj, so that zj,k
records the number of words added to document j by topic k.
Finally, for document j, the quantity wj is a matrix having t rows and m columns.
wj,k,d tells us how many copies of word d were contributed to document j by topic
k.
Let wj,k denote the kth row in the wj matrix, and let wj,∗,d denote the dth

54
CREATE TABLE theta[0] (docID, topicID, prob) AS
FOR EACH d IN documents
WITH NewProbs AS Dirichlet(
SELECT topicID, alpha FROM
topics, hyperparameters)
SELECT d.docID, np.outID, np.probability
FROM NewProbs AS np;
Figure 3.5 : Initializing Θ
column in this matrix. The kth row wj,k is sampled from a multinomial distribution
with parameters zj,k and Ψk. After this process, the jth document in the corpus
(represented as word frequencies) is δj = P
k wj,k. As discussed in Section 3.4, a
typical goal of an LDA analysis is learning the posterior distribution of the Θj vectors.
My generative process is mathematically equivalent to the one described in [51]; it
diﬀers slightly in that it is matrix- and vector-oriented, which leads to a simpler and
more intuitive Gibbs sampler. It can be shown that the iterated sequence of updates
required by the Gibbs sampler is as:
Θj ∼Dirichlet(α + zj)
(3.1)
wj,∗,d ∼Multinomial(δj,d, Θj × Ψ∗,d)
(3.2)
Ψi ∼Dirichlet(β +
X
j
wj,i)
(3.3)
Here α + zj denotes a vector addition and Θj × Ψ∗,d denotes the item-by-item mul-
tiplication of the two vectors.
I assume that the latter vector is normalized to a
probability vector, if needed, prior to multinomial sampling. There is no need to
explicitly sample zj, because zj,k can always be computed as P
d wj,k,d.

55
CREATE TABLE w[0] (docID, wordID, topicID, count) AS
FOR EACH dw IN wordInDoc
WITH TC AS Multinomial (
(SELECT tm.topicID, tm.probability FROM theta[0] AS tm
WHERE tm.docID = dw.docID),
(SELECT dw.count))
SELECT dw.docID, dw.wordID, tc.outID, tc.count
FROM TC AS tc;
Figure 3.6 : Initializing w
3.3.2
Specifying the Gibbs Sampler in SimSQL
To describe the above Gibbs sampler in SimSQL, I assume four database tables.
The ﬁrst two contain the topic and document identiﬁers, and the third contains a
single row with the positive, real-valued Dirichlet hyperparameters α and β (recall
my assumption of symmetric Dirichlet priors). The fourth table stores the number of
times that each unique word appears in each document.
topics(topicID)
documents(docID)
words(wordID)
hyperparameters(alpha,beta)
% hyperparameters
wordInDoc(docID,wordID,count) % words in documents
In addition, I have three stochastic tables; one (psi) to store the Ψ matrix, one
(theta) to store the Θ matrix, and one (w) to store the various w matrices:
psi[i](topicID,wordID,prob)
theta[i](docID,topicID,prob)

56
CREATE TABLE theta[i] (docID, topicID, prob) AS
FOR EACH d IN documents
WITH NewProbs AS Dirichlet
(SELECT pw.topicID, sum(pw.count) + hyperparameters.alpha
FROM w[i-1] as pw, topics AS t, hyperparameters
WHERE pw.docID = d.docID AND pw.topicID = t.topicID
GROUP BY pw.topicID, hyperparameters.alpha)
SELECT d.docID, np.outID, np.prob FROM NewProbs AS np;
Figure 3.7 : Updating Θ
w[i](docID,topicID,wordID,count)
Initialization. I initialize the Θ and w matrices as shown in Figures 3.5 and 3.6.
These codes are similar to the code in the previous section, with one big diﬀerence:
the FOR EACH loop. Consider Figure 3.5. This code scans the documents table (“FOR
EACH d IN documents”) and, for each d encountered, invokes the Dirichlet VG
Function via the WITH statement. The output schema for this VG function has two
attributes: outID, the topic identiﬁer, and probability, the word-production prob-
ability associated with that topic. The ﬁnal SELECT query assembles the output from
the VG function into a temporary table. All of the temporary tables created over all
of the d values are then UNIONed to form theta[0]. Next, as shown in Figure 3.6,
w is initialized by sampling, for each unique word in each document, the number of
times that each topic produces the word, using the Multinomial VG function.
Updating the parameters. Figure 3.7 displays the SQL code for updating Θ. It
is very similar to the initialization code in Figure 3.5; the key diﬀerence is in the
parametrization of the Dirichlet VG function, which is based on the formula in

57
CREATE TABLE w[i] (docID, wordID, topicID, count) AS
FOR EACH dw IN wordInDoc
WITH TC AS Multinomial(
(SELECT tm.topicID, wpt.prob * tm.prob
FROM psi[i-1] AS wpt, theta[i] AS tm
WHERE wpt.wordID = dw.wordID AND wpt.topicID = tm.topicID AND
tm.docID = dw.docID),
(SELECT dw.count))
SELECT dw.docID, dw.wordID, tc.outID, tc.count FROM TC AS tc;
Figure 3.8 : Updating w
(3.1). Similarly, the code to update w—shown in Figure 3.8—is very similar to the
initialization code in Figure 3.6. Here the diﬀerence lies in the parametrization of
the Multinomial VG function that allocates the number of appearances of a word in
a document among the diﬀerent topics. The parameter representing the probability
that a given word in the document is associated with a given topic is computed as
the product of the probability of the topic in the document and the probability that
the topic produces the word, as required by (3.2).
3.4
Specifying an Analysis
I have shown how to specify a database-valued Markov chain with the SimSQL syntax.
Denote by D = (D[0], D[1], · · · , D[M]) the ﬁrst M +1 states of such a chain and recall
that the overall goal of my analysis is to study the (perhaps conditional) distribution
of a query Q = Q(D). Figure 3.10 shows a typical query. MCI is the number of
parallel chains (also called “possible worlds”). This query runs the MCMC algorithm

58
CREATE TABLE psi[i] (topicID, wordID, prob) AS
FOR EACH t IN topics
WITH NewProbs AS Dirichlet
(SELECT pw.wordID, sum(pw.count) + hyperparameters.beta
FROM w[i] AS pw, hyperparameters
WHERE pw.topicID = t.topicID
GROUP BY pw.wordID, hyperparameters.beta)
SELECT t.topicID, np.outID, np.probability FROM NewProbs AS np;
Figure 3.9 : Updating Ψ
SET MCI = 3;
SELECT * FROM theta[50];
CREATE MATERIALIZED VIEW thetaLog[i:30,..,50] AS
SELECT * FROM theta[i];
Figure 3.10 : Syntax for an LDA analysis
for 51 iterations, and records the theta table from 30th to 50th iteration using 21
materialized views. Two issues are discussed as follows.
First, how large is M? In theory, the algorithm converges after a burn-in period.
Our interest centers on the states of the chain from the burn-in period to the iteration
M. In Figure 3.10, I suppose M equals the largest index in the query (M = 50),
and the burn-in time is 20. However, users generally do not know these two values.
How to determine both values automatically will be addressed in future work. Note
that not all the simulations are convergent, e.g., a genetic mutation simulation or a
random walk in Graphs [95].
Second, how to use the parallel chains (many “possible worlds”)? Conceptually,

59
MCDB ﬁrst generates from the input VG function N samples, which are used to
obtain a resulting empirical distribution. Similarly, N parallel chains are generated
in SimSQL; for problems such as random walk [95], combining N parallel chains
can lead to an empirical distribution of the result. However, for a problem such as
LDA, this method is problematic. Assume 3 topics exist in the LDA dataset, and
the query in Figure 3.10 returns the following topic vectors for the ﬁrst document,
i.e., ⟨0.8, 0.1, 0.1⟩, ⟨0.1, 0.8, 0.1⟩, and ⟨0.1, 0.1, 0.8⟩.
Since topics are symmetric in
LDA, these vectors can be possibly the same.
Estimating the mean by ˆτj(N) =
(1/N) PN
i=1 Θ(i)
j [M] can be a wrong approach, where Θ(i)
j [M] is the value of θj[M] in
the ith chain. In the current version, the parallel chains are independent with each
other. The cross-interactions among the chains will be considered in future work.
3.5
Implementation Overview
Figure 3.11 shows the computing stack of SimSQL. The next few sections of this chap-
ter describe the technique details, mainly focusing on how SimSQL handles Markov
chain simulation queries.
3.5.1
SimSQL Language
The language supported by SimSQL is very close to the classical SQL, but provides
the functionalities for stochastic analytics.
DDL. The data deﬁne language (DDL) in SimSQL supports stochastic tables,
simulation models and VG functions in addition to tables, (materialized) views and
functions. The format for deﬁning tables, (materialized) views and functions follows
the SQL/92 standard; the format for stochastic tables and VG functions is exempliﬁed
in [48, 12]; the format for simulation models is presented in Section 3.2; the format

60
 !"#!$
%&'%()*)+#,-+,!
).,&/+$*012&'&3!"
4.'1&$!"
5678&/+$*012&'&3!"
9:!/-2&.#*9#,&#!
Figure 3.11 : The computing stack of SimSQL.
for simulation models is also presented in Section 3.2. Tables and materialized views
have the same representation, where attributes, statistics and references are saved in
the catalog. Views and stochastic tables can be deﬁned by using tables/materialized
views, or by recursively using views and stochastic tables; both are saved as SQL
macros. Simulation models are not saved directly but inferred from the deﬁnitions of
the stochastic tables involved.
DML. The data manipulation language (DML) in SimSQL allows users to query
(stochastic) tables, (materialized) views or stochastic tables in a Markov chain by
the SELECT statement. However, like other analytical platforms such as Hive [10] and
Spark [11], SimSQL does not support ﬁne-grained updates such as INSERT or UPDATE
or transactions. SimSQL could delete a relation or replace the directory of a relation
but it can not delete individual tuples.
Nested subqueries. Subqueries can appear in all SQL clauses: SELECT, WHERE,
FROM, HAVING, functions or VG functions, and they can span multiple levels. A typical
example is that subqueries correlating with the FOR EACH loop can parameterize the
VG functions; another one is shown in Figure 3.12, which ﬁnds “the words that appear
in all the documents.” How to decorrelate and unnest subqueries will be discussed
later.

61
SELECT wd1.wordID FROM wordInDoc AS wd1 WHERE NOT EXISTS (
SELECT * FROM documents AS d WHERE NOT EXISTS (
SELECT * FROM wordInDoc AS wd2
WHERE wd2.wordID = wd1.wordID AND wd2.docID = d.docID));
Figure 3.12 : An example of nested subquery.
Other language features.
The SimSQL language has some other features.
SimSQL supports all standard SQL operators, such as aggregations, arithmetical
expressions, Boolean expressions, constant-value tables, table valued functions, etc.
A Boolean expression is considered as a special arithmetical expression (valued as
0 or 1). Subqueries are also counted as math expressions. Furthermore, SimSQL
has many frequently used build-in (VG) functions, such as power(.), normal(.),
and it allows users to deﬁne their own UDFs and VG functions in Java and C++
respectively.
3.5.2
SimSQL Kernel
The SimSQL kernel works as the controller for the system.
Given a DDL query,
the kernel calls the parser to compile it, and updates the meta-store accordingly.
Given a DML query (SELECT statement), if it does not reference any stochastic tables
in a simulation model, the kernel calls the compiler, logical/physical optimizer and
execution engine one after another to run it; otherwise the query references stochastic
tables in a simulation model, and some quite unique techniques are leveraged.

62
Supporting Huge Plans
Markov chain simulation queries are unique in that the query plans are huge. For
example, running one iteration of the MCMC algorithm for the Gaussian Mixture
Model (GMM) (see the experimental section of the chapter) requires executing around
100 relational operators, so dozens of iterations require thousands of operators. One
approach to run such a simulation would be to execute those thousands of operators
as a single plan. However, there are problems with this approach. For example, the
cost-based optimization of such a huge plan is diﬃcult because of the lack of reliable
statistics over intermediate results. Estimating such statistics during optimization
fails after the ﬁrst few levels of the plan, due to the exponential propagation of the
estimation errors [96].
A Frame-based Approach
I therefore perform incremental (re-) optimization and execution. My approach is to
partition the computation into sub-plans, called frames, which loosely correspond to
the levels of recursive speciﬁcation of the simulation. Each frame is optimized and
executed in sequence: the output of one frame is provided as input to the next one.
Dividing the computation into a sequence of frames provides an opportunity to collect
precise statistics for each frame and use them when processing the next one. Most
important, materializing the output of each frame makes it possible to checkpoint
the computation. In addition to supporting recovery, checkpoints are crucial because
they provide the ability to inspect intermediate states of a computation during a post-
analysis phase. For example, imagine that after the completion of an 100 iteration
MCMC process, a user wants to analyze the state of the Markov Chain at the 20th
iteration. Using a nearby checkpoint, the system can eﬃciently process that request.

63
 !"#!$
%&"'!"
("&)!*+,-
("&)!./0
123/4&$5%67'/4&$
89-/)/:!"
;<!4,-/2#
;#3/#!
("&)!5
=-&-'
>
%$&#5
?!-&@&-&
%$&#
8,-9,-5
=-&-'
 !" #$
#%&'(
A!)9$&-!
%$&#./0
A!)9$&-!
%$&#./0
B,!"7
Figure 3.13 : Overview of simulation query processing in SimSQL.
Compiling And Executing A Simulation
The process for running a Markov chain simulation query, depicted in Figure 3.13, is
as follows:
1. The SimSQL compiler parses the speciﬁcation and creates a number of template
plans that will be strung together to form the simulation. Template plans are
so-named because they are the unoptimized building blocks from which a frame
is constructed—they serve as templates for various parts of the frame. At a
minimum, the system creates a template plan for each SQL code snippet that
speciﬁes the initial version of a stochastic table, and a template plan for each
SQL code snippet that deﬁnes an intermediate stochastic table.
2. In a cost-based fashion, the set of template plans deﬁning the intermediate
tables are partitioned via a “frame cut” into two subsets. This partitioning
determines where one frame ends and another begins, and is done so as to
minimize the amount of data that needs to be materialized at the end of each

64
frame.
3. The template plans for the initialization are appended with all of the template
plans up to and including those that are part of the cut. This deﬁnes the ﬁrst
frame, which is then optimized.
4. After optimization, the deterministic parts of the ﬁrst frame are identiﬁed, and
the frame is run. As the frame is run, the deterministic parts are materialized.
5. While the result of running the frame is written out, statistics describing the
resulting output tables are computed.
6. The next frame is created and optimized. The optimizer has the option of decid-
ing (in cost-based fashion) to integrate the materialized, deterministic “views”
from Step 4 into the plan for the frame.
7. The resulting plan for the frame is run.
8. Steps (5) through (7) are repeated until completion.
Frame Cut
As described earlier, a huge simulation plan is partitioned into frames, where some
stochastic tables in each frame are checkpointed for the next frame. If this is done
naively and very large tables parametrize the next frame, the size of the checkpoint
may be huge. Thus, SimSQL chooses to end each frame at a “narrow” point in the
computation, where the stochastic tables needed to parametrize the next frame take
little space. The following algorithm computes the smallest checkpoint possible:
1. Given the template plans, a logical plan with two iterations is generated. This
logical plan is optimized to obtain statistics for each stochastic table.

65
È
É
Ê
Ë
Ì
Í
Î
Ï
Ð
Î
Ñ
É
Ê
Ë
Ì
Í
Ò
Ó
Ê
É
Ê
Ë
Ì
Í
È
É
Ê
Í
Î
Ï
Ð
Î
Ñ
É
Ê
Í
Ò
Ó
Ê
É
Ê
Í
Ô
Õ
Figure 3.14 : The graph used to determine the optimal frame cut.
2. An acyclic directed graph G = (V, E) is deﬁned, where V consists of stochastic
tables with indices i-1 and i, and E encodes the dependencies among them.
3. The source and sink are added. For each table A ∈V , if A references tables with
indices i-2, a direct edge from source to A is added; similarly, if A is referenced
by tables with indices i+1, a directed edge from A to sink is added.
4. I assign the weight +∞to each edge e ∈E and the weight (cost) from Step 1
to each node v ∈V . My problem is then transformed to a max-ﬂow/min-cut
problem [97] with vertex capacities, and I apply the Edmonds-Karp algorithm.
Figure 3.14 shows the graph used to compute the LDA frame cut. The assigned
weights (costs for materialization) are displayed for each stochastic table. After apply-
ing the Edmonds-Karp algorithm to compute the minimum cut, I ﬁnd (as expected)
the optimal frame cut is at {psi[i-1], theta[i]}. Thus, these two tables will be
checkpointed and will serve as input into every frame.
In a short summary, the kernel works as the scheduler for the whole system.
The kernel processes diﬀerent queries by using diﬀerent strategies. Especially if the
input is a simulation query, it ﬁrst partitions the huge plan into frames by calling a
minimum-cut algorithm, and then treats each frame as a standard query plan.

66
 !"#!$%&$'()*+&,(-()./&0.-12#&3.4/(5
3.4/(&!67(#.
-.#(+&&87(8.9":
.88)"4'8(&%";8+&9126<=>&82?"6<=>&?)24:
@A0&BCDE&3.4/(F0(G()(-6(
)(G()(-6(+&126'#(-8;
-.#(+&1
H"87&!8.8(#(-8
-.#(+ I(JK)24;
 L,&@'-68"2-+&=")"67/(85
 !B%BD3&!8.8(#(-85
L,&@'-68"2-
-.#(+&=")"67/(8
?.).;%";8+& ;'4M'()*5
;(/(68&/";8+&91N126<=> -?N2'8<=> -?N?)24:
G)2#&/";8+&9I(JK)24; C!&-?:
 !B%BD3&!8.8(#(-85
;(/(68&/";8+&9?JNJ2)1<=>&;'#O?JN62'-8P&Q 7*?()?.).#(8();N4(8.:
G)2#&/";8+&9J9":&.;&?J> 7*?()?.).#(8();:
J7()(&?)(1"6.8(+&9?JN82?"6<= R 8N82?"6<=:
S)2'?F4*&/";8+&9?JNJ2)1<=> 7*?()?.).#(8();N4(8.:
Figure 3.15 : An example of the semantic tree.
3.5.3
SimSQL Compiler
The SimSQL compiler performs multiple tasks: query parsing, semantic checking,
query unnesting, plan generation, and pre-optimization.
Query Parsing
The query parser parses a query into a semantic tree, which records the semantic
information of this query.
For example, Figure 3.15 shows the semantic tree for
the query in the Figure 3.7. The root of a tree records the query type as well as
the references to its inner data structure; a non-root node is an SQL expression, a
Boolean expression or a math expression. Note that a subquery is a math expression
as well as a wrapper for a SELECT statement.
Semantic Checking
Semantic checking has two goals. The ﬁrst is to check if the query is semantically cor-
rect. The second is to create a tree for use in type checking, which will be considered

67
subsequently.
The checks of semantic correctness that are performed are fairly standard, with
an additional check for correctness of recursive table deﬁnitions. Supposing A is a
recursive table, the check ensures that (1) the schema of a stochastic table A[i]
is consistent, (2) each stochastic table has exactly one recursive deﬁnition, (3) a
stochastic table with a constant index A[const] does not reference a stochastic table
with a varying index B[i] and (4) at least one stochastic table with a constant index
is deﬁned. The current implementation assumes that a stochastic table A with index
i can only reference another stochastic table B with index i-1 or i; however, lifting
that constraint is straightforward.
The system also ensures that every stochastic table can be derived from stochastic
tables with constant indices:
1. First a graph is generated whose nodes are the stochastic tables with indexes i
and i-1, and whose edges represent the dependence relationships among them.
See, for example, Figure 3.16(a) for LDA.
2. As in Figure 3.16(b), edges among stochastic tables with index i are copied to
the corresponding tables with index i-1.
3. If all the stochastic tables with index i can be accessed, then this check is
passed.
As the other goal of semantic checking, the type-checker tree is derived from a
semantic tree, but is more ﬂat. A type-checker node has all the associated infor-
mation for a query block (deﬁned as the query itself or a subquery), such as the
output attributes, tables, subqueries, children, parent, etc. Among such information,
the correlated reference list records the correlated (table, attribute) pairs,

68
3%042'
(5"()%042'
/$0%042'
3%0'
(5"()%0'
/$0%0'
(a)
3%042'
(5"()%042'
/$0%042'
3%0'
(5"()%0'
/$0%0'
(b)
Figure 3.16 : Determining whether a recursive query plan is valid.
which is used for query unnesting. Note that in the ﬁgure, a dotted rectangle denotes
that a typechecker node has a pointer to the associated root/subquery node in the
semantic tree.
Subquery Unnesting
As indicated by [98, 99, 100, 101, 102, 103, 104, 99, 105, 106], subquery unnesting is
generally understood to provide eﬃcient query processing. There are two solutions
for this problem. First, the magic-decorrelation approach considers a subquery as
a function, and saves the result of the function as a temporary table; unnesting is
achieved by joining the temporary table with outer tables [104]. Second, the Apply
operator is used by Microsoft SQL Server, where the subqueries are removed by apply
removal under a set of rules [107, 99]. Other methods exist, but lack generality. For
example, WinMagic [105, 106] unnests subqueries with aggregates, and the Bypass
operator [108, 109] applies to scalar queries with disjunctive predicates.

69
 !"#$%#$&#'(
 !"#$%#$&#')
 !"#$%#$&#'*
+,(-+.',/0
,-,.$/0
Figure 3.17 : The typechecker tree for the query in Figure 3.12
SimSQL extends the magic-decorrelation approach [104]. The magic-decorrelation
can be summarized as follows. Given a query, it ﬁrst builds the tree structure for the
query. Each node in the tree corresponds to a query block such as Select-Project-Join
(SPJ), Aggregate, Union or Intersection.
Second, the magic-decorrelation rewrite
rule works in a top-down fashion. For each node in the tree, if the current node is
correlated and can be decorrelated, it ﬁrst ﬁgures out the set of input variables in
the correlated tables (Feed stage). The Feed stage produces a Correlated Input (CI)
Box and a Decorrelated Output (DCO) Box, both of which are positioned between
the current node and its children in the query graph. The CI box is used to join
the result of subquery with the current node, while the correlations are pushed down
to the DCO box and are removed in the following Absorb stage. The details can be
referenced in [104].
SimSQL adopts a bottom-up approach to magic-decorrelation instead of a top-
down approach. For clarity, I ﬁrst describe how SimSQL unnests a standard database
query in Figure 3.12. The typechecker tree for this query is shown in Figure 3.17,
where the inner data structure is ignored. The black arrows show the edges in the
tree, and the dotted lines represent the correlations among query blocks. The steps
for unnesting this query are shown in Figure 3.18. First, the inner-most query block is
unnested by using a view named temp view3. Note that two attributes wd1.wordID

70
and d.docID reference the outer two blocks respectively. As a result, both tables
wordInDoc and document are pushed down to join with the table WordInDoc in
the inner-most block. If the inner-most block is considered as a function, the view
temp view3 saves the result for each referenced pair (d.docID, wd1.wordID). Simi-
larly, the referenced attribute from the outer-most block with its associated functional
result for the second level block is represented by a view temp view2. Since the pred-
icate between second level and third level is negative, I use an anti-join to ﬁlter out
the pairs (d.docID, wd1.wordID) that are accepted by the inner-most block. Fi-
nally, another anti-join is used to ﬁlter out the tuples that are accepted by the second
level block. Note that in the real implementation, all these steps are executed in
logical level, and SimSQL language does not support semi-join, duplicate-removal or
anti-join.
Compared with the original magic-decorrelation [104], my method has the follow-
ing extensions. First, I use a bottom-up approach, and that is easier to understand.
Second, my typechecker tree is much simpler than the Query Graph Model (QGM) in
the original paper. Third, the correlated relations are pushed down directly into inner
blocks, instead of projecting the correlated attributes followed by a duplicate-removal
operator. Intuitively, my method is less eﬃcient than the original one; however, our
logical optimizer could remove the redundant operators, which is like the apply re-
moval in the Microsoft SQL Server [107, 99].
Given a general nested query, my algorithm works as follows.
1. A typechecker tree is generated in the stage of semantic checking. Each type-
checker node corresponds to a query block and has the information for the
nested (to-and-from) correlations.

71
CREATE
temp view3(docID, wordID, count, d doc ID, wd1 wordID) AS
SELECT wd2.*, d.docID, wd1.wordID
FROM wordInDoc AS wd1, document AS d, wordInDoc AS wd2
WHERE wd2.wordID = wd1.wordID AND wd2.docID = d.docID;
CREATE
temp view2(docID, wd1 wordID) AS
SELECT d.*, wd1.wordID
FROM (wordInDoc AS wd1, document AS d) AntiJoin
temp view3 ON
d.docID =
temp view3.d doc ID AND wd1.wordID =
temp view3.wd1 wordID
SELECT wd1.wordID FROM wordInDoc AS wd1 AntiJoin
temp view2 ON
wd1.wordID =
temp view2.wd1 wordID
Figure 3.18 : The steps to unnest the query in Figure 3.12
2. The nodes in the typechecker tree are processed in an anti-topological (bottom-
up) order. All the leaves are unnested ﬁrst and the root node is last.
3. Given the current node in the typechecker tree, suppose Q is the set of ta-
bles/views listed in its from clause, P is the set of tables/views in its ancestors
that are correlated by the current node or its descendants, and R is the set of
temporary views generated for unnesting its children. If an element in P or Q
is a subquery, a separate temporary view is generated accordingly. In addition,
I use F to denote the temporary view to generate for the current query block.
The process for unnesting the current query block is as follows.
(a) I let elements in P Cartesian product with each other. If the current block
has an aggregate or a GROUP-BY clause, a duplicate removal is put on the
result of this Cartesian product. This corresponds to the Feed stage of the
magic-decorrelation approach, and I use view1 to denote the result of this

72
step.
(b) The view view1 is pushed down to the FROM clause of the current block.
If the current block has a COUNT aggregate, then a left-outer-join is used
instead to address the well-known COUNT-zero bug problem. I use view2
to denote the result of this step.
(c) The predicate of the current block is transformed into a conjunctive normal
form (CNF). Each element e in this CNF has the form A1 ∨A2 ∨· · · An ∨
Q1 ∨Q2 ∨· · · Qm, where Ai is a (negative) predicate without subqueries
and Qj is a (negative) predicate with subqueries. However, the current
version of SimSQL only accepts following cases: 1) e has no subqueries, 2)
e has subqueries but all the predicates with such subqueries are negative,
and 3) e is a positive predicate with a subquery. Accordingly, the WHERE
predicate is divided into two parts, one with subqueries and the other
without subqueries. The former is used to link the temporary views in
R, while the latter replaces the old WHERE predicate. For each element
e with subqueries, if e is a single positive predicate, I use a semi-join
to link view2 with the associated view in R; otherwise, e has the form
constant ∨¬Q1 ∨¬Q2 · · · ∨¬Qm. In the latter case, e is transformed to
¬(¬constant∧(Q1∧Q2∧· · ·∧Qm)). I ﬁrst let the associated views in this
predicate Cartesian product with each other, and then use an anti-join to
link view2 with the result of this Cartesian product. Note that view2
changes when iterating diﬀerent element e. I use view3 to denote the
result of this step.
(d) After processing the temporary views from the children, the new WHERE
predicate is put on top of view3. If the current block has an aggregate

73
or a GROUP-BY clause, all the attributes associated with relations in P are
added to the GROUP-BY list. If a COUNT aggregate exists, then the COUNT-
zero bug is addressed here. Furthermore, all the attributes referenced by
the current block or its descendant are added to the projection list. I use
F to denote the temporary view for the current block.
So far, I have described my approach by extending magic-decorrelation. However,
magic-decorrelation considers a subquery as a deterministic function, and this does
not apply to a stochastic table, where a VG function produces a stochastic result. To
address the problem, I generate a primary key for the outer FOR EACH loop, and this
operator is called the seed. The result of this seed operator is pushed down in the
Feed stage. The primary key generated in this way will be kept until the temporary
view for the subquery is joined back to the outer block. The details will be described
in next section.
Plan Generation
The query unnesting generates a set of temporary views and a SELECT statement,
which correspond to the subqueries and outer-most query block respectively. Given
these temporary views and SELECT statement, the next task is to translate the SELECT
statement to a directed acyclic graph of logical operators.
1. Translate the FROM clause to a chain of Cartesian products. A table is trans-
lated to a table-scan; a (temporary)-view is translated to a sub-plan by calling
this algorithm recursively; a stochastic-table is translated to another sub-plan
by a separate algorithm described later.
If the sub-plan for a (temporary)-
view/stochastic-table exists in the memory, then its root is linked to the Carte-

74
sian products instead of regenerating the sub-plan.
2. Translate the WHERE clause to a ﬁlter operator.
3. Translate the aggregates and GROUP BY to an aggregate operator if exists.
4. Translate the arithmetic-expression/renaming to a scalar operator.
5. Translate the SELECT list to a projection operator.
6. Link such operators together with the projection operator as the root.
Note that in step 1), a separate algorithm is needed for translating stochastic
tables. The algorithm works as follows (also depicted in Figure 3.19).
1. Plans Q1prm to QNprm instantiate the tables to parametrize the VG function,
and Qouter instantiates the “outer table” that corresponds to the tuples from
the FOR EACH statement of a stochastic table deﬁnition.
2. The Seed operator appends an integer (the “seed”) to every tuple from Qouter.
3. The bottom joins add the seeds to each parameter stream.
4. The actual VG function is run by the VGWrapper operator.
5. The result of VGWrapper is joined back with the outer stream.
A bit of explanation must be provided. Seeds play three roles here. First, they
seed the pseudorandom number generators for VG functions. Second, they are used as
keys to match tuples from parameter streams with corresponding tuples in the outer
table stream. Finally, seeds are used to join the output tuples of the VG function

75
Seed 
 
 
VGWrapper 
VG Function 
Seed 
Qouter 
Q1prm 
QNprm 
͙͘ 
Figure 3.19 : Template plan for creating a random table.
with the tuples in the outer table stream. Here, the last two roles extend the magic-
decorrelation approach in the case of stochastic functions. Furthermore, both Seed
and VGWrapper are the new operators that are added to the relational database to
support stochastic analytics. The details are described in Section 3.6.3.
However, the above algorithm works only for non-simulation queries. If a SELECT
statement or its referenced views reference a stochastic table in a Markov chain sim-
ulation model, my strategy is diﬀerent. The process is highlighted as eight steps in
section 3.5.2, and some details are discussed in the next subsection.
Plan Generation for Markov Chain Simulations
A simulation query makes plan generation much harder. For example, given a query
such as SELECT * FROM theta[100], if I recursively use the previous algorithm, the
plan would be huge. Furthermore, many queries may exist simultaneously to query
the model. For eﬃciency, performance and reliability, I have proposed the frame-

76










 
!
"
#
$
%
&
'
(
#
)
*
+
+
,
*
+
+
,
-
.
/
0
1
2
3
$

4
4

$
5
6
7
8
9
:
;
<
=
:
>
8
?
(a)
@
A
A
B
C
D
E
F
G
H
H
I
F
J
K
L
M
L
N
O
P
A
Q
R
S
T
U
V
W
X
H
U
Y
Z
[
\
\
]
[
\
\
]
H
F
U
X
F
^
_
_
`
N
^
P
^
M
a
b
c
N
Q
L
d
c
e
f
g
h
g
i
(b)
j
k
k
l
m
n
o
p
q
r
r
s
p
t
u
v
w
v
x
y
z
k
{
|
}
~










r







x

z

w



x
{
v






r
p


p












(c)








 
¡
¡
¢

£
¤
¥
¦
§
¨
©
ª
«
¨
¬
¦
­
¡
®
¯
°
¯
±
²
³
´
µ

¶
·
¸
¹
µ
º
»
¼
¬
¦
¬
½
¾
¥
©
¼
§
¨
ª
©
¿
À
À
Á
¿
À
À
Á
Â
Ã
Ä
Å
Æ
Ç
¢
Æ
 
°
¯
³
(d)
Figure 3.20 : Template plans for LDA
based approach, which is highlighted as eight steps in section 3.5.2. This section
describes the generation of template plans and frame in the step 1, 3 and 6.
A template plan is simply an executable, unoptimized query plan. A template plan
is generated by the same algorithm described earlier; however, some table-scans could

77
Ö
×
×
Ø
Ù
Ú
Û
Ü
Ý
Þ
Þ
ß
Ü
à
á
â
ã
ä
å
æ
ç
è
å
é
ã
ê
Þ
ë
ì
í
ì
î
ï
ð
ñ
ò
Ü
ó
ô
õ
ö
ò
÷
ø
ù
ù
ú
ñ
í
ì
ð
û
ü
ß
û
Ý
í
ì
ð
ø
ù
ù
ú
ý
þ
é
ã
é
ÿ
 
â
æ
þ
ä
å
ç
æ
Ö
×
×
Ø
Ù
Ú
Û
Ü
Ý
Þ
Þ
ß
Ü
à
!
å
ÿ
å
þ
"
ã
×
ä
ê
ñ
í
ì
ð
ó
ò
÷
ë
û
ò
Þ
ì
÷
ë
é
#
#
ý
þ
é
ã
é
ÿ
 
â
æ
þ
ä
å
ç
æ
Þ
Ü
ì
ò
Ü
ø
ù
ù
ú
ø
ù
ù
ú
Ö
×
×
Ø
Ù
Ú
Û
Ü
Ý
Þ
Þ
ß
Ü
à
!
å
ÿ
å
þ
"
ã
×
ä
ê
ñ
í
ì
ð
û
ò
Þ
ì
÷
ë
Þ
Ü
ì
ò
Ü
é
#
#
ý
þ
é
ã
é
ÿ
 
â
æ
þ
ä
å
ç
æ
ø
ù
ù
ú
û
ü
ß
û
Ý
í
ì
$
ï
ð
Þ
ë
ì
í
ì
ð
%
&
'
(
)






Figure 3.21 : Frame to be optimized for execution.
not be executed, since they read stochastic tables A[i] or A[const] that do not exist
in the system. There are three types of template plans: baseline, intermediate, and
branch template plans. Baseline plans correspond to stochastic table deﬁnitions with
constant indices. In the LDA example, theta[0] and w[0] are compiled into baseline
plans. Intermediate plans correspond to stochastic table deﬁnitions that recursively
depend upon other stochastic tables, and are themselves used to parametrize other

78
 
 !"#$
$ !"#%
% !"#&
 !"#'
' !"#&
 
 !"# 
 !"# 
 !"# 
 !"# 
 !"# (
Figure 3.22 : Attribute renaming in preoptimization.
stochastic tables. In the LDA example, the theta[i], w[i], and psi[i] deﬁnitions
are compiled into three intermediate template plans.
Figure 3.20 shows the four
template plans for the LDA example. Branch plans correspond to SQL queries that
are run on top of stochastic tables to extract data or perform analysis.
As template plans are generated, the next task is to integrate the template plans
into an executable frame. The ﬁrst frame is produced by starting with the baseline
tables, and then attaching (as speciﬁed by the SQL) all subsequent template plans up
to and including those plans in the optimal cut. Subsequent frames are produced by
assuming the results of the optimal cut as input, and then including all of the template
plans in a full iteration. An example frame for LDA is shown in Figure 3.21.
Preoptimization
The plans from the plan generation stage are executable. However, my goal is ﬁrst
to optimize the plan. Therefore, some preoptimization steps are necessary. First, a
set of statistics are attached to the plan for describing the properties of the relations,

79
VG functions and SimSQL system. The ﬁrst category includes the number of unique
tuples, the type and number of unique values for each attribute, and the list of
stochastic attributes; the VG function statistics include the type and size of output
attributes, the value of bundles per tuple, the stochastic input and output attributes;
and the last one is the global statistics, including the number of possible worlds, the
seed size, the isPres size and the weights for diﬀerent type of logical operators. The
second step is to rename the attributes in their lineage. The goal is to reduce the total
number of attributes in the plan while avoiding conﬂicts. For example, Figure 3.22
shows the renaming of attribute A in its lineage. A is kept in the left branch, while
it is changed to A′ in the right branch. The last step is to remove the redundancy in
the plan, including redundant renaming, projection and scalar operators. After these
three steps, the plan will be written out as a Prolog program, which will be optimized
by the logical optimizer.
3.5.4
Logical Optimizer
The logical optimizer optimizes a plan by graph reordering and transformation. Un-
fortunately, space precludes us from describing the extended SimSQL’s query opti-
mizer in detail. Brieﬂy, the optimizer uses an A*-inspired search to optimize the plan.
The cost of a plan is a weighted, linear combination of the total number of bytes input
to and output from each intermediate plan operator. Weights on the various operator
types capture the importance of each operator in query cost estimation and are calcu-
lated using linear regression. SimSQL’s optimizer has at its disposal all of the usual
transformations (such as join re-orderings and push-down of selections) and also a
list of SimSQL-speciﬁc transformations. An example of an SimSQL transformation
is shown in Figure 3.23, where a random attribute called count is dropped and later

80
topics 
theta [i + 1 ] 
psi [i] 
Seed 
w [i] 
Seed 
psi [i - 1] 
wordInDoc 
docID, wordID, topicID, count 
 
VGWrapper 
 
theta [i] 
Scalar Function 
docs 
count 
 
Aggregate 
 
Seed 
hyperparameters 
docID, wordID,  
topicID 
 
(a) Plan before transformation.
topics 
theta [i + 1 ] 
psi [i] 
Seed 
w [i] 
Seed 
psi [i - 1] 
wordInDoc 
docID, wordID, topicID, count 
 
VGWrapper 
 
theta [i] 
Scalar Function 
docs 
count 
 
Aggregate 
 
Seed 
hyperparameters 
ʋ 
docID, wordID,  
topicID 
 
w [i] 
VGWrapper 
 
ɷ 
Seed 
Scalar Function 
(b) Plan after transformation.
Figure 3.23 : Transformation to drop and recreate a random attribute in SimSQL.
re-created using the seed. Because random attributes are implemented as an array
of random values, carrying them around can be quite expensive; thus, dropping and
re-creating a random attribute may be desirable.
Another task for the logical optimizer is to ﬁnd and materialize the deterministic
plans that repeat in iterative frames. An optimized frame may contain deterministic
sub-plans, which are sub-plans of the query plan containing no VGWrapper operators,
and where the initial input comes only from deterministic database tables.
The
output from such a plan can be re-used. To identify the deterministic sub-plans I
employ a simple bottom-up algorithm. The algorithm visits each operator based on
its topological ordering. An operator belongs to a deterministic plan if (1) it is a
deterministic database table scan or (2) all its descendants belong to a deterministic

81
plan. Following standard methods used to incorporate materialized views into query
plans [110], I give the optimizer the opportunity of re-using a deterministic sub-plan
by introducing a plan transformation.
3.6
SIMSQL EXECUTION ENGINE
This section of the chapter describes various aspects of the SimSQL execution engine.
3.6.1
Overview
The SimSQL execution engine is built on top of Hadoop, and written in both Java and
C++. The ultimate output for the SimSQL compiler and optimizer (and the input
into the SimSQL execution engine) is a speciﬁcation for the query plan to be run, in
a SimSQLspeciﬁc dataﬂow language that describes all of the relational operations in
the plan, as well as how they ﬁt together. The SimSQL execution engine compiles this
plan into a set of MapReduce jobs, and then, taking into account the dependencies
among them, it runs them one at a time. To run a particular MapReduce job, SimSQL
ﬁrst “writes” (using a special macro-expansion facility) a Java code for the desired
map task and a Java code for the the desired reduce task. These Java codes contain
logic that is speciﬁcally tailored to the task at hand. For example, if the goal is to
execute a relational join, the generated Java might contain (on the map side) specially
generated code to run selection predicates over the two input relations, project away
un-needed attributes, and compute hash keys over the input records. The generated
Java code for the reduce side might contain code for the join predicate, as well as
code for the ﬁnal projection and any arithmetic or other functions that must be run
to produce the ﬁnal output.
Once the Java code for the task is generated, it is
compiled and then executed. The reason that I employ this sort of dynamic code

82
generation is to bypass the high CPU cost associated with interpreting, at runtime,
data structures describing how to process the records dynamically; instead, I allow an
optimizing compiler to generate byte code that does this far more eﬃciently. Others
have explored this idea in the past few years (see, for example, the work of Neumann
[111]). While such dynamic code generation has a cost, namely, the time required to
generate and compile the code, in a MapReduce environment the additional fraction
of a second is a small price to pay when one considers that the fastest MapReduce
jobs take tens of seconds to run.
3.6.2
Tuple Bundles
Unlike in most applications of Hadoop, the data processed by SimSQL conform to a
SimSQL-speciﬁc binary format instead of text-based format. The important point to
note is that the SimSQL format incorporates the MCDB “tuple bundle” trick [48].
Conceptually, SimSQL repeats the desired simulation N times to produce N mutually
independent runs; in fact, the simulation is executed only once. This is made possible
through SimSQL’s use of tuple bundles, a concept in MCDB. Rather than running
the simulation N times, a single set of tuple bundles that encodes each of the N
independent possible worlds ﬂows through a single query plan.
A tuple bundle t with schema S is, logically speaking, an array of N tuples, all
having schema S. An attribute att is constant if t[i].att = c for some ﬁxed value
c and i = 1, 2, · · · , N, otherwise it is stochastic. In my SimSQL implementation, the
N tuples making up a single tuple bundle are stored in the obvious way: constant
attributes are represented using a singleton value, and stochastic attributes are stored
as arrays of values. A special stochastic attribute called isPres may also be present
in the tuple bundle. The value of t[i].isPres equals true if and only if the tuple

83
bundle has a constituent tuple that appears in the ith possible world.
3.6.3
The VG Wrapper Operation
Most of SimSQL’s MapReduce implementations of relational operations (the various
joins such as the natural join, anti-join, and semi-join, de-duplication, selection, and so
on) are straightforward. However, one operation that deserves a bit more explanation
is SimSQL’s VGWrapper operation.
A VGWrapper operation is part of a template
plan, as shown in Figure 3.19 (this was discussed in the previous section), and the
VGWrapper is tasked with parameterizing and executing a particular VG function.
Every VG function in SimSQL is a C++ class with (at least) four methods:
ClearParams, TakeParams, TakeSeed, and OutputVals. ClearParams is called to
let the VG function know that it is about to be parameterized. After ClearParams,
TakeParams is called repeatedly to parameterize the VG function. When the VGWrapper
is ready to use the function to produce random data, TakeSeed is called with the seed
for the VG function’s PRNG. The VGWrapper then repeatedly calls OutputVals to ob-
tain random values from the VG function. When a null is eventually returned by
the VG function (indicating that it is done producing values for the current possible
world), the VGWrapper will then (optionally) call ClearParams to indicate that it
wants to re-parameterize the function for the next trial (if the parameterization is
constant across worlds, then no re-parameterization is needed). The VGWrapper then
calls TakeSeed, followed by a sequence of calls to OutputVals, until a null is once
again returned. This process is completed until all worlds have been computed.
To demonstrate how the VGWrapper invokes a VG function, consider a Multinomial
VG function and the w[i] stochastic table from the previous LDA example. Imagine
that there are four tuple bundles, encoding three possible worlds, that are used to

84
parameterize the function:
(tID: 0, prob: ⟨0.1, 0.2, 0.1 ⟩, isPres: ⟨T, T, T ⟩)
(tID: 1, prob: ⟨0.4, 0.6, 0.5 ⟩, isPres: ⟨F, T, F ⟩)
(tID: 2, prob: ⟨0.5, 0.2, 0.4 ⟩, isPres: ⟨T, F, T ⟩)
(count: 14)
Here, a ⟨⟩pair indicates an array of values, with each value being applicable to
one of the three possible worlds. In my example, the VGWrapper will begin with the
ﬁrst possible world, calling ClearParams to indicate the start of a parameterization,
followed by three calls to TakeParams, giving (0, 0.1), (2, 0.5), and (14) in sequence.
The VGWrapper then calls TakeSeed, passing along the seed associated with the cur-
rent dw value (see Figure 3.8) from the outer table stream (Qouter in Figure 3.19).
The VGWrapper then repeatedly calls OutputVals to obtain all output tuples for the
ﬁrst possible world. The VGWrapper then calls ClearParams and parameterizes the
second possible world with (0, 0.2), (1, 0.6), and (14), and obtains the result with
calls to OutputVals. Finally, it parameterizes the third possible world with (0, 0.1),
(2, 0.4) and (14), obtains the result.
Note that I made the somewhat non-obvious decision to reseed the VG function’s
PRNG for each and every possible world. This is in contrast to MCDB, where the VG
function is seeded only once, and then the seed is used for all of the possible worlds.
The reason that SimSQL re-seeds for each possible world is so that I can handle the
presence of the isPres attribute in the outer input table with some eﬃciency. The
stream of tuples from the outer input table supplies the seeded FOR EACH tuples
to the VGWrapper. If one of these tuples has a false value in isPres, it means that
the outer tuple has been removed from the database (perhaps via an earlier selection)
and logically, it should not exist. A simple way to handle the non-existence would

85
be to have the VGWrapper ignore it, knowing that it will be enforced by the top-level
join in Figure 3.19. Although this solution is correct, it might be costly, since the VG
function will be invoked needlessly.
Another solution would be to avoid running the VG function for possible worlds
where isPres is false. This must be done with care. As discussed in the previous
section, the optimizer is free to drop (project away) random attributes, and then
generate them again later via a second execution of the VGWrapper operator. This
transformation is depicted in Figure 3.23, and is useful if carrying around random
data is more expensive than re-generating it. If the VGWrapper is invoked once, and
then a subsequent operation removes an outer stream tuple from a possible world,
the sequence of calls to the VG function will change the second time around, because
that possible world will not be parameterized nor will OutputVals be called for that
possible world. If the VG function was parameterized only once, at initialization, then
the second use of the VGWrapper will produce diﬀerent, inconsistent results because
it will not be run for certain possible worlds.
Thus, in SimSQL the seed associated with a tuple from the outer table stream is
not given directly to the VG function. Rather, it is used to seed an additional PRNG
that is internal to the VGWrapper, and which is used to re-seed the VG function for
each possible world via a call to TakeSeed. If isPres is false for any possible world,
the new seed is discarded. That way, a possible world is always assigned the same
seed, regardless of changes to the isPres attribute.
3.7
Evaluation
This section shows the result of SimSQL with version 0.0. The goal is not to show
that SimSQL can be used to implement the fastest distributed MCMC simulation

86
possible—it would be unreasonable to think that a simulation coded using 47 lines of
SQL (as with LDA) could possibly outperform a distributed simulation custom coded
in Java or C. Rather, I wish to give some evidence that a simulation implemented and
run on the SimSQL platform can have reasonable performance. “Reasonable” here
might be a time that is within an order of magnitude (or so) of a special-purpose Java
or C version. A 10X slowdown might be a big performance hit, but (in my view) it is
more than balanced by the fact a distributed Java or C version might require several
hundred times as much code and utilization of many diﬀerent tools.
3.7.1
Overview
SimSQL consists of an SQL query compiler and query un-nester (approximately
50, 000 lines of Java code), a cost-based query optimizer (approximately 11, 000 lines
of Prolog code), and a runtime (written in around 40, 000 lines of Java and C/C++)
that is built upon Hadoop. I use Amazon EC2 for all of my runs. The machines used
are High-Memory Quadruple Extra Large Instance types, which have 8 relatively
low-end virtual cores, two disks, and 68GB of RAM per machine.
My high-level goal is examining SimSQL’s feasibility. Along these lines, I wish
to answer the questions: How well does SimSQL scale as the amount of data and
computing resources increases? Can SimSQL run a distributed simulation quickly
and (even more importantly) in a cost eﬀective manner? To answer this question,
I perform an experiment where the amount of data per compute node in a cluster
is held constant, but an ever-increasing number of nodes are used to perform the
simulation. If SimSQL scales perfectly, the time to run each simulation will stay
constant as the number of nodes is increased.
I focus on two particular simulations: the MCMC simulation for the LDA model

87
of Section 3.3 and a more complicated MCMC simulation for learning a Bayesian
Gaussian mixture model (GMM). All SQL code listed in the chapter is compiled and
run without modiﬁcation. The LDA simulation compiles to a query plan with 45
operations in the baseline iteration, and 39 operations in the other iterations. The
GMM simulation has 40 operations in the baseline iteration, and 87 operations in the
other iterations.
For the LDA model, I use the 20-newsgroups dataset [112] in my experiments.
Since this dataset has only 16,330 documents, I create a database of (almost) arbi-
trary size by randomly picking two documents from the corpus and merging them—
this gives us
¡2×16330
2
¢
distinct “documents”.
While this corpus of semi-simulated
documents may not be the best testbed for evaluating the LDA model itself, I am
interested in evaluating the SimSQL system, and this data should suﬃce. In my ex-
periments, I use 100 topics and a dictionary of 20,000 words. For the GMM model, I
use synthetically-generated ten dimensional data. The data are themselves generated
by sampling from a Gaussian mixture model.
3.7.2
Benchmarks Run
Scalability. To test scalability, I perform the following experiment. For the two
models, I ﬁrst choose a per-machine data size n, such that across experiments, when
performing a simulation using m machines, the total amount of data processed is
always n × m, no matter the number of nodes. In the case of LDA, n is 250,000
documents. In the case of the GMM, n is 1,000,000 data points. For each run, 10
diﬀerent simulations are performed at the same time, so that 10 parallel Markov
Chains are simulated. I run the simulations for the two models using 5 nodes, 10
nodes, 20 nodes, 50 nodes, and 100 nodes. Figure 3.24 displays the results.

88
LDA Inference
Nodes
Iteration Number
0
1
2
3
4
5
5
2:22
2:33
2:28
2:25
2:29
2:22
10
2:33
2:35
2:30
2:28
2:32
2:30
20
2:34
2:27
2:30
2:29
2:30
2:34
50
2:44
2:39
2:37
2:41
2:52
2:44
100
3:24
3:16
3:17
3:18
3:13
3:04
Gaussian Mixture Model Inference
Nodes
Iteration Number
0
1
2
3
4
5
5
0:06
0:54
0:53
0:54
0:53
0:55
10
0:06
0:55
0:53
0:53
0:52
0:53
20
0:06
0:57
0:56
0:57
0:56
0:56
50
0:08
1:10
1:07
1:04
1:01
1:02
100
0:08
1:13
1:13
1:13
1:12
1:12
Figure 3.24 : Running time (HH:MM) per iteration for GMM and LDA inference.
Iteration zero is the initialization.
Work Sharing.
To test the ability of the system to share work across possible
worlds, I ﬁx the number of nodes to be exactly 20. Then, for the two models, I
vary n as follows. Let N be the number of Markov Chains that are simulated in
parallel. For LDA, I ﬁx n × N to be 20 × 10 × 250, 000, and for the GMM, I ﬁx
n×N to be 20×10×1, 000, 000, and then I perform ﬁve runs of the simulation, using
N = {1, 5, 10, 20}. Thus, if N = 10, the run is identical to the scalability experiment

89
Model
Number of parallel worlds
20
10
5
1
LDA
2:11
2:28
3:27
4:20
GMM
0:51
0:57
1:27
1:53
Figure 3.25 : Avg running time per iteration (HH:MM), changing the number of
parallel simulations while keeping the (number of data points) × (the number of
simulations) constant.
with m = 20. But with N = 20, half the amount of data per machine is used, and
if N = 1, ten times the amount of data per machine is used. Figure 3.25 gives the
results.
3.7.3
Discussion
The system does appear to scale. The running time is more or less constant when
the total data processed is held constant at n × m. That said, not all of the rows
in Figure 3.24 are identical for diﬀering cluster sizes. Much of the increase in time
is due to the fact that as the size of the problem increases, it becomes impossible
for the system to pipeline expensive operations, such as joins. For example, in my
Amazon cluster, I have approximately 6GB of RAM available for operation-speciﬁc,
auxiliary data structures, for each Hadoop mapper and reducer. As I move from 10 to
20 machines (and 10 million to 20 million data points) it becomes impossible to buﬀer
a copy of the entire data set in an 6GB hash table to run a join, and so certain join
switch from single-pass pipelined joins to full sort-merge joins, and the time increases
slightly. As I move from 50 to 100 machines, I ﬁnd that it is not always possible to
store meta-data about all 100 million data points (such as the PRNG seed assigned to

90
each) in RAM, and again I see an increase in running time. That said, in both cases,
at 100 machines, none of the varying-sized tables can be buﬀered in RAM, and so I
would expect a more constant running time as the cluster size grows even larger.
Although the system appears to scale, it is reasonable to ask whether a running
time of two or three hours per iteration is practical. To many practitioners of Bayesian
machine learning, who have long been used to month-long simulation runs, the answer
is aﬃrmative. Additionally, I assert that the absolute time taken per iteration is
perhaps not that important for a cloud-based platform. If the underlying architecture
scales, the running time can be driven down dramatically at no additional cost by
simply renting more machines. Speciﬁcally, my 100-node cluster costs around $14
per hour (14 cents per machine per hour), and can run a GMM iteration in about 70
minutesor a bit more than $15 per iteration. I could instead rent 200 machines, at a
cost of $28 per hour, and as long as the running time per iteration is cut approximately
in half, I will still spend $15 per iteration, but would now have to wait only half as long
for each iteration to ﬁnish. True, increasing cluster size past a certain point produces
diminishing returns. In my Hadoop-based system it is impossible to push the per-
iteration running time down below a few minutes (each GMM iteration requires 36
MapReduce jobs, which will always take at least seconds each, even in a small cluster).
But in general, if the simulation takes too long, one can just rent a larger cluster.
Perhaps a more reasonable question is: Is the cost of learning a model reasonable?
First, consider how many iterations are required for convergence. Burn-in periods
of 1, 000 or more iterations are common in the machine learning literature, but I
question whether this is always a necessity. All of the standard Bayesian models with
which I am familiar-including LDA, GMM, linear regression, conditional random
ﬁelds, and matrix factorization become quite stable after 100 iterations or so. For

91
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0
 1
 2
 3
 4
 5
 6
 7
 8
 9
Probability
Topics
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0
 1
 2
 3
 4
 5
 6
 7
 8
 9
Probability
Topics
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0
 1
 2
 3
 4
 5
 6
 7
 8
 9
Probability
Topics
Figure 3.26 : Parallel coordinates plot depicting sampled LDA θ vectors for three
documents, iterations 50-60. The top ten topics for each document are shown, sorted
in order of importance.
example, in Figure 3.26 I plot the Θ vectors obtained in iterations 50-60 for three
diﬀerent documents from a run of LDA on the 20-newsgroups data set. There is little
ﬂuctuation, indicating stability in the model.
If I assume 100 iterations are desired, then at $14 per iteration to handle 100
million data points (in the case of the GMM simulation), I have a cost of $14 per

92
million data points. Considering that I am running 10 simulations in parallel at that
cost,1 and that running a single simulation cuts the cost by a factor of ﬁve (see Table
3.25), I could run a single MCMC inference at the cost of around $3 per million data
points.
This is certainly not inexpensive, perhaps 10× to 100× more expensive than a
special-purpose, C or Java implementation, such as the LDA code written by Smola
and Narayanamurthy [113].
That said, programmers who can write high-quality
distributed or parallel C/Java code are very expensive themselves (and quite rare!),
and so I argue that the fact that I obtain a cost of $3 per million data points with
only 63 lines of SQL makes my approach look very attractiveespecially when the goal
is fast prototyping of a particular inference algorithm.
3.8
Related Work
This section gives readers an overview of tools and systems for the data analysis,
including programming languages, database systems, and the MapReduce parallel
framework [8], as well as its extensions [5, 114, 1].
3.8.1
Parallel programming language
Programming languages such as C, Java and Python [115] are still in widespread use
for data analysis. However, it is inconvenient to process large scale Bayesian ML in
this way. Some high-level languages such as R [32], MATLAB [33] and SAAS [34]
do provide the abstractions for complex data analysis; however, such languages do
not have good support for scalability and fault tolerance. Many parallel program-
ming languages/APIs/models [116, 117, 118, 119, 120, 36] have been proposed to
scale and to optimize the data processing on speciﬁc/generic hardware/architecture.

93
Ananth Grama’s textbook [121] gives a good introduction for all kinds of principles
and techniques, including message passing, MPI [117], OpenMP [118], etc. However,
these techniques are too low-level for many programmers to use. Furthermore, many
of them do not support fault tolerance well. For example, message-passing requires
mastery of knowledge in parallel computing, and it is designed for a “tightly” con-
nected cluster [35]; SNOW [36], the popular parallel package designed for R, requires
speciﬁc operations for distributing and processing data.
3.8.2
Database system
Database systems have been used extensively for data analysis [122, 123, 124, 125].
Edgar Codd’s pioneering paper [126] led to the origin of the relational database sys-
tems such as System R [127] (later known as DB2 [98]) and Oracle [124], which use set-
oriented relational algebra to manage and analyze data. Such systems have evolved
for more than thirty years, and numerous techniques such as indexing and transac-
tions have been introduced. However, to support rich analytics such as Bayesian ML,
engineers need to write external programs to interact with the database. Not only
is I/O overhead problematic for big data, but also the integration among diﬀerent
systems is not easy. Some researchers have made eﬀorts toward putting analytics into
relational database [48, 49, 50], such as standard deviation, linear regression, vector/-
matrix operations, and user-deﬁned functions, etc. However, such functionalities are
not appropriate for Bayesian ML.
3.8.3
MapReduce
The driving force behind large scale ML originally came from Internet companies,
which manage and analyze web-scale data.
Popular parallel frameworks include

94
MapReduce [8], Hadoop [5] (an open source version of MapReduce) and Dryad [128].
Underneath such frameworks are storage systems such as Google ﬁle system (GFS
[4]), the Hadoop distributed ﬁle system (HDFS [129]) or their extensions such as
Bigtable [6] or HBase [7]. Above the frameworks are high level language abstractions,
such as Sawzall [9], FlumeJava [130] and DryadLINQ [131]. A signiﬁcant advantage
of such systems is their ﬁne-grained fault tolerance model: failed tasks or slow nodes
can be handled via replication. Furthermore, users control the schema “on the read”.
However, a deep understanding of issues such as data partition and job conﬁgura-
tion is generally a prerequisite for using such systems. Even FlumeJava [130] and
DryadLINQ [131] require users to be familiar with APIs such as map and join.
Recently, many data warehouses have been built upon MapReduce. For example,
Pig [132] and Jaql [133] translates dataﬂow into MapReduce jobs, and Hive [10],
HadoopDB [44], Tenzing [45], Scope [46] and Cheetah [47] support dialects of SQL.
However, MapReduce has lower performance than a parallel database when applied
to structured data [134]. To achieve better performance, several database engines are
built on top of storage systems directly instead of MapReduce, e.g., Google’s Dremel
[135], PowerDrill [136] and Cloudera’s Impala [137]. The problems supported by such
systems are within the scope of those supported by the relational database systems.
To solve complex ML problems, engineers need to write an iterative dataﬂow
of parallel jobs following the API of these platforms [128, 133, 132, 11, 138, 139].
The dataﬂow is similar with the database query plan. As a result, traditional query
optimization [140, 141, 142, 143, 144] and pipelining techniques [145, 146] are applied
in such systems. Similarly, SimSQL has a cost-based optimizer which uses an A*-
inspired search algorithm to optimize the plan. Furthermore, it uses pipelining to
optimize the physical plan and ﬁgure out the set of logical operators to make a

95
physical job. Also, the dataﬂow in SimSQL is transparent to a user. Haloop [147],
on the other hand, oﬀers an iterative MapReduce interface for writing iterative or
recursive algorithms. However, like MapReduce [8] or Pregel [114], its programming
interface is low-level.
3.8.4
Large scale machine learning
The demand for large scale data analysis has driven many other systems that are
designed speciﬁcally for iterative algorithms, machine learning and data mining.
I ﬁrst turn to the distributed/parallel graph processing.
Pregel [114], Giraph
[13] (an open source version of Pregel), HAMA [55] as well its extensions [148, 149]
are graph-based, distributed computing platforms that follow the bulk synchronous
parallel (BSP) model [150]. Similarly, GraphLab and its extensions [1, 2, 151] provide
the vertex-centric programming model for parallel graph processing; however, they
diﬀer from Pregel due to an asynchronous computation model. Asynchronicity allows
that one needs only consider the computation required to update the state of that
vertex but ignores the remainder of the computation. Both Pregel and GraphLab
have been successfully applied to a range of ML problems, such as PageRank [152],
LDA [51], etc. Other similar graph processing or database systems include BPGL
[153] and Kineograph [154]. However, like MapReduce [8], all such systems provide a
low-level programming interface, which gives ﬂexibility to control the processing but
is cumbersome for the machine learning and data mining problems.
Spark [11] is another important computing system for big data analytics. Like
FlumeJava [130], Spark provides langauge integrated programming interfaces in-
cluding Java, Scala and Python. Furthermore, Spark utilizes Resilient Distributed
Datasets (RDDs), which provide a fault-tolerant, distributed memory abstraction.

96
It is argued in [11] that Spark could provide much better performance than Hadoop
MapReduce. Based on Spark, GraphX [155] takes a database-tuple-like approach, and
represents a distributed graph as tabular data-structures. However, Spark-Python is
not stable and has low performance [20]. Furthermore, it is hard for engineers to
conﬁgure and tune Spark’s performance in clusters.
Third, some researchers combine MapReduce with the statistical software such
as R, SAS or MATLAB. For example, Ricardo [35] and RHIPE [156] integrate R
and Hadoop. However, both systems require the programmer to identify and handle
scalability of diﬀerent components of an algorithm [54]. In addition to R or MATLAB,
some other domain-speciﬁc languages (DSL) have been proposed, such as Cumulon
[157], SystemML [54], OptiML [14], ScalOps [158] and MLbase [159]. Cumulon and
SystemML express ML algorithms in a high level language supporting linear algebra,
and compile it into MapReduce jobs; OptiML translates DSL to CUDA code for
GPUs; ScalOps leverages Hyracks [160] runtime to process the recursive query plan
translated from DSL. MLbase’s query itself does not have a speciﬁc algorithm, but
relies on its execution engine to select speciﬁc ML algorithms. Among such systems,
SystemML is the most relevant with SimSQL [12], while SystemML is targeted for
linear algebra such as R, and SimSQL is designed for machine learning in databases.
Fourth, a variety of libraries are provided for big ML based on MapReduce or
other parallel frameworks [37, 38, 39, 40, 41, 42, 43]. Apache Mahout [37] provides
a collection of Hadoop-based libraries for recommendation, classiﬁcation, clustering,
etc. Pegasus [38] is another Hadoop-based library which maps graph mining problems
into parallel matrix-vector multiplications in MapReduce. Spark also provides MLlib
libraries [161] to support classiﬁcation, linear regression, SVM, etc. However, all such
libraries are not generic enough to support Bayesian ML.

97
3.8.5
Markov Chain Monte Carlo
Perhaps the work closest to SimSQL in the database literature is the paper of Deutsch,
Koch, and Milo on languages for specifying queries over Markov chains [21]. However,
SimSQL’s goal is to prototype a distributed platform for large scale Bayesian MCMC,
while Deutch et al. are concerned with how to answer queries over a Markov chain
without actually simulating the chain many times. The setting that they consider
is somewhat restrictive compared to my own (probabilistic databases with c-tables
[162], or repair-keys [163]), and it is not immediately apparent how this work applies
to common distributions such as the Dirichlet that accept real, vector-valued inputs
and produce correlated, real, vector-valued outputs. MCMC for model inference over
database data has been previously considered [164, 165]. In [164], the MCMC loop
takes place within an external module. In [165], eﬃcient SQL-based implementations
of two MCMC algorithms were described. However, the work is not meant to provide
general-purpose MCMC simulation.
Jigsaw [166] is a system that uses VG functions to perform stochastic, what-if
analysis. Jigsaw focuses not on systems issues, but instead on the problem of opti-
mization over stochastic models; most of the technical innovation in the Jigsaw system
is directed towards alleviating Monte Carlo overheads by decreasing the number of
VG function invocations required when a stochastic simulation must be repeated for
many diﬀerent parameter conﬁgurations during the optimization.
3.9
Conclusion
I have described the functionality, design and implementation of the SimSQL system,
which allows for SQL-based speciﬁcation, simulation, and querying of database-valued

98
Markov chains. The key advantage of the SimSQL system is that it allows a program-
mer to specify Markovian simulations of relatively high complexity in only a few dozen
lines of SQL code; these simulations are then automatically distributed and run in
parallel over a cluster of machines. As a proof-of-concept I described in detail the
SQL speciﬁcation of two real world Bayesian inference algorithms: MCMC inference
for linear regression and for LDA. I also discussed possible SQL speciﬁcations of anal-
ysis tasks over the results of the performed simulation. My experiments, using the
LDA and GMM models, show that SimSQL has reasonable performance and scales
well when running distributed Markov chain simulations.

99
Chapter 4
Benchmark for Large Scale Bayesian ML
I describe an extensive benchmark of platforms available to a user who wants to run
a machine learning (ML) algorithm over a very large data set, but cannot ﬁnd an
existing implementation and thus must “roll her own” ML code. I have carefully
chosen ﬁve ML implementation tasks that involve learning relatively complex, hier-
archical models. I completed those tasks on four diﬀerent computational platforms,
and using 70, 000 hours of Amazon EC2 compute time, I carefully compared running
times, tuning requirements, and ease-of-programming of each.
4.1
Introduction
Many platforms have been proposed to scale distributed/parallel machine learning
(ML) codes to very large datasets, including OptiML [14], GraphLab [1, 2], SystemML
[54], and SimSQL [12]. MLBase [159] and ScalOps [158] also address the problem,
though the most recent published descriptions indicate that these systems are more
immature. Other systems such as Pregel [114], Giraph [13], Hama [55], Spark [11],
Ricardo [35], Nyad [167], and DryadLinq [131] may not have been developed only for
ML, but count it as an important application.
I describe an objective benchmark of some of the platforms available to a user
who wants to run a speciﬁc ML inference algorithm over a large data set, but cannot
ﬁnd an existing implementation and thus must “roll her own” ML code. Given the

100
wide variety of ML models, this will not be an uncommon occurrence.∗I draw a
distinction between a user who wants to implement and apply a brand new ML code,
and someone who just wants to use a code, and focus on the former. The implementor
will want to balance ease of implementation with performance, whereas an end user
has little concern for the eﬀort required to engineer the code and will be happy with
an intricately constructed C and MPI code as long as it is fast and easy to use.†
My contributions. My speciﬁc contributions are:
(1) I shed some light on the relative merits of some (quite diﬀerent) platforms for
implementing large-scale ML algorithms. My results will surprise many readers.
(2) Second, I demonstrate what a scientiﬁc study of a platform for writing large-scale
ML codes might look like. I have carefully chosen a set of tasks that involve learning
relatively complex, hierarchical statistical models. I have purposely avoided simple,
convex models whose parameters can be optimized using easily-implemented tech-
niques such as gradient descent [168]. Their simplicity means they beneﬁt relatively
little from the abstractions provided by the platforms I consider.
(3) Finally, I hope that my eﬀorts will grow into a widely used, standard benchmark
for this sort of platform. In the future, an implementor of a new or existing platform
need only implement these codes and compare with my numbers.
∗For example, I consider ﬁve standard Bayesian ML algorithms in this study, and it appears that
only the collapsed LDA inference algorithm [51] is available as part of an existing package, and even
then I am aware of no “non-collapsed” Gibbs sampler implementation (See Section 4.7).
†My focus on giving an implementor an idea of what platform to use, and I do not consider
benchmarking ML libraries that are not meant as a platform for writing new codes, such as Mahout
[37] and MADLib [43].

101
4.2
Experimental Overview
4.2.1
Platforms Tested
The platforms I evaluate in the chapter are:
Spark [11], which embodies the MapReduce/data-ﬂow approach.
I choose Spark
because it purports to be both higher performance and easier to use than Hadoop.
SimSQL [12], which is a parallel, relational database that supports an SQL-based
approach to running large-scale MCMC simulations.
GraphLab [1], which supports a graph-based abstraction for writing distributed ma-
chine learning codes. For large-scale MCMC, one can map the various components
of θ to nodes in a graph, with edges representing statistical dependencies.
Giraph [13], which is a graph-based platform that exempliﬁes the BSP approach,
widely known to be used extensively by Facebook.
These four were chosen as representative of various approaches. Spark exempliﬁes
the dataﬂow/MapReduce approach, SimSQL the database approach, GraphLab the
graph-based approach, and Giraph the BSP approach. My choices to a certain extent
were motivated with our pre-existing familiarity with the platform. Familiarity is
desirable, since it allows one to avoid errors that lead to bad performance. Other
options certainly exist. For example, I could have used a commercial parallel database
system; these have been used for statistical ML for a long time [169], and I could
have even borrowed some of MADLib’s existing infrastructure. Or, instead of Spark,
I could have used something like Pig [132].

102
4.2.2
ML Models Considered
I use each of these platforms to implement MCMC codes for learning the following
ﬁve models. I chose them as being representative of the sort of hierarchical, statistical
models that one might want to learn over a large data set:
(1) A Gaussian mixture model (GMM) for data clustering.
(2) The Bayesian Lasso [66], a Bayesian regression model.
(3) A hidden Markov model (HMM) for text.
(4) Latent Dirichlet allocation (LDA), a model for text mining.
(5) A Gaussian model for missing data imputation.
4.2.3
Balancing Performance and Ease-Of-Use
Since I focus on evaluating the platforms from the point-of-view of a user-implementor,
my study focuses on both performance and programmability. These are frequently at
odds—since these platforms can all execute arbitrary Java/C++/Python code, there
is always a temptation to bypass the platform to gain high performance. Thus, I
adopted a simple guideline when considering how to implement the MCMC simula-
tions I study in this chapter. I ﬁrst attempt to implement each code in the “purest”
way possible; that is, the way in which was most in-keeping with the conceptual design
of the platform being evaluated. For example, in a graph, this means mapping each
variable or data point to a vertex, and using the graph to manage the computation,
avoiding the use of mechanisms such as synchronized global variables.
Unfortunately, there were cases where such a “clean” implementation would not
work. In such situations, I made things work by writing code that takes some of the
burden of managing the data and computation away from the platform.

103
4.2.4
Experimental Platform
All of the experiments were performed using Amazon EC2 m2.4xlarge machines,
running Ubuntu (each machine had eight virtual cores, two disks, and 68 GB of RAM).
In all, I used approximately 70,000 hours of compute time, considering debugging,
tuning and testing. I was initially concerned about variability in EC2 performance
from day-to-day and machine-to-machine. When I tested the same MCMC simulation
on ﬁve diﬀerent days using ﬁve diﬀerent compute clusters, I found that the standard
deviation in per-iteration running time was only 32 seconds (out of 27 minutes on
average) and so I decided that such variations were insigniﬁcant.
4.3
Platforms Evaluated
4.3.1
Spark
Spark [11] is an open source cluster computing system designed for large scale data
analytics.
Spark utilizes Resilient Distributed Datasets (RDDs), which provide a
fault-tolerant, distributed memory abstraction. RDDs are built by running coarse-
grained transformations over either a data set or other existing RDDs, and they are
calculated by various actions. Spark provides fault tolerance by tracking data lineage,
so it can rebuild lost data partitions easily. The designers of Spark argue that since
RDDs allow for in-memory computations without concern for lost data, they can
facilitate very fast performance.
To us, one of the most attractive aspects of Spark is that it provides a Python-
based programming interface.
Python is now one of the most popular languages
for ML-oriented programming (along with other high-level languages such as R and
Matlab), and I was eager to evaluate a Python-based platform.

104
4.3.2
SimSQL
SimSQL [12] is a distributed, relational database system, whose design has been aug-
mented to support stochastic analytics. Speciﬁcally, SimSQL has native support for
a special type of user-deﬁned function known as a variable generation (VG) function.
VG functions are randomized, table-valued functions. Using SimSQL’s dialect of SQL,
it is possible to deﬁne a random database table whose contents are constructed from
one or more VG function invocations, as well as from deterministic data stored within
the database. Random table deﬁnitions in SimSQL’s SQL dialect can be mutually
recursive. Hence one can deﬁne, in SQL, MCMC simulations.
SimSQL is written mostly in Java, with a Prolog query optimizer. SimSQL VG
functions are written in C++.
SimSQL compiles SQL into Java classes that are
executed as Hadoop MapReduce jobs.
4.3.3
GraphLab
GraphLab [1] is a graph-based distributed computing platform written in C++. The
so-called “graph-parallel” abstraction used by GraphLab is a useful abstraction for
ML, since many ML inference algorithms are naturally centered around variables/data
(which map to vertices in a graph) and the statistical relationships between them
(which map to edges). GraphLab is unique in that its computational model is pull-
based and asynchronous. Each vertex in the graph constantly requests data from
its neighbors in order to update its own state. Asynchronicity is the single most
deﬁning (and unique) characteristic of the GraphLab approach, and allows for a very
appealing computational model, since when writing the code associated with a vertex,
one need only consider the computation required to update the state of that vertex;
one can more or less ignore the remainder of the computation.

105
4.3.4
Giraph
Giraph [13] is also a graph-based, distributed computing platform. It is written in
Java and runs on top of Hadoop. It is often viewed as an open-source version of Pregel
[114]. Giraph diﬀers fundamentally from GraphLab in that its model is push-based
and synchronous, using the so-called bulk synchronous parallel (BSP) model. Giraph
divides a graph algorithm into a sequence of super-steps. In each super-step, every
vertex executes the same user-deﬁned compute function in parallel, where the vertex
receives messages from other vertices in super-step i−1, accesses and updates its local
state, and then sends its messages to the other vertices for super-step i + 1. Vertices
can work together to perform tasks such as aggregation in a tree-based fashion using
combiners to speed the computation.
4.3.5
Versions Tested
I tested version 0.1 of SimSQL, version 2.2 of GraphLab, version 1.0.0 of Giraph.
My GMM and Bayesian Lasso implementation ran on version 0.7.3 of Spark and the
other models on version 0.8.0 of Spark.
4.4
Gaussian Mixture Model
I begin my experimental evaluation by considering a classical data clustering model:
the Gaussian mixture model (GMM). A GMM views a data set as being produced
by a set of K Gaussian (multi-dimensional normal) distributions; the kth Gaussian
is parameterized by a mean vector µk and a covariance matrix Σk, and has an as-
sociated probability πk. To produce the jth point in the data set, the model ﬁrst
selects a Gaussian by generating a sampled vector cj vector from a Multinomial(π, 1)

106
distribution (cj,k is then a 1 if and only if the k Gaussian was used to produce the
data point xj, and zero otherwise). The data point itself is then sampled from the
Gaussian indicated by cj. I put a Dirichlet(α) prior on π, a Normal(µ0, Λ−1
0 ) prior on
each µk, and an InvWishart(v, Ψ) prior on each Σk. The goal during learning is then
to discover all of the unseen cj values, as well as all of the Gaussian parameters.
Let p(i)
j
denote the unit-length vector whose kth entry is proportional to π(i)
k ×
Normal
³
xj|µ(i)
k , Σ(i)
k
´
. That is, it gives the posterior probability that point k came
from the jth cluster, given the parameters at the ith MCMC iteration. A Markov
chain to learn the desired, posterior distribution can be derived as:
µ(i)
k ∼Normal
³ µ
Λ0 + n
³
Σ(i−1)
k
´−1¶−1
×
Ã
Λ0µ0 +
³
Σ(i−1)
k
´−1 X
j
c(i−1)
j,k
xj
!
,
µ
Λ0 + n
³
Σ(i−1)
k
´−1¶−1´
Σ(i)
k ∼InvWish
Ã
n + v, Ψ +
X
j
c(i−1)
j,k
(xj −µ(i)
k )(xj −µ(i)
k )T
!
π(i)
k ∼Dirichlet
Ã
α +
X
j
c(i−1)
j
!
c(i)
j
∼Multinom
³
p(i)
j , 1
´
The task is to write a distributed code that simulates this chain.
4.4.1
Spark Implementation
I begin by creating a RDD named data, which is read and parsed from data in
permanent storage (HDFS in my example):
lines = sc.textFile("hdfs://master:54310/data.txt")
data=lines.map(parseLine).cache()

107
The cache() function asks the system to store RDD in the memory, which can
accelerate subsequent visits and operations on data.
Next, I compute the hyper-parameters µ and Λ−1
0
as the observed mean and
dimensional variance of the data:
num = data.count()
hyper mean = data.reduce(add)/num
hyper cov diagonal = data.map(lambda x:square(x-hyper mean)).reduce(add)/num
numpy.fill diagonal(hyper cov, hyper cov diagonal)
Based on those parameters, I initialize the model (πi, µi and Σi) for each cluster:
c model = sc.parallelize(range(0, K)).map(lambda x:
(x, (mvnrnd(hyper mean, hyper cov),
invWishart(hyper cov, len(hyper mean)+2))))
.collectAsMap()
pi = np.zeros(K, float)
pi.fill(1.0/K)
Here, mvnrnd(.) and invWishart(.) are user-deﬁned functions that call PyGSL
library functions. collectAsMap() is a built-in function that transforms the RDD to
a Python dictionary.
Finally, I come to the main loop of the program, which consists of three MapRe-
duce jobs. The ﬁrst dominates the overall runtime:
c agg = data.map(lambda x: sample mem(x, pi, c model))
.reduceByKey(lambda (x1, y1, z1), (x2, y2, z2):(x1+x2, y1+y2, z1+z2))
On the Map side, the output of the function sample mem(x, pi, c model) is a tuple
(k, (1, x, sq x)), where k is the membership of data point x and sq x is the

108
matrix ˆΣ = (x −µk)(x −µk)T, µk is the mean of kth Gaussian component. On the
Reduce side, tuples with the same membership are aggregated into a single tuple.
The second job is Map-only.
This job samples µk and Σk for each Gaussian
component in parallel:
c model = c agg.mapValues(lambda (c num, x sum, sq sum):
updateModel(c num, x sum, sq sum, len(hyper mean)+2,
hyper mean, hyper cov)).collectAsMap()
The third job collects the number of data points assigned to each cluster, which is
then used to update π:
c num = c agg.mapValues(lambda (c num, x sum, sq sum): c num).collectAsMap()
pi = sample dirichlet(c num)
4.4.2
SimSQL Implementation
The simulation is implemented in SimSQL using a database schema with four random
tables that correspond to the four classes of variables listed above:
clus means[i](clus id,dim id, dim value)
clus covas[i](clus id, dim id1, dim id2,dim value)
clus prob[i](clus id, prob)
membership[i](data id, clus id)
The data to be processed are stored in a database table:
data(data id, dim id, data val)
as are the entries in the α vector (the hyperparameter for the Dirichlet prior on π):
cluster(clus id, pi prior)

109
In addition, several views are used to store the other hyperparameters, which are
computed empirically from the data. For example, the vector µo is computed as the
mean of the entire data set:
create view mean prior(dim id, dim val) as
select dim id, avg(data val) from data group by dim id;
Aside from this, the entire SimSQL code consists of (1) initialization codes for the
ﬁrst three random tables, (2) recursive deﬁnitions for all four random tables, and (3)
a C++ implementation of the multinomial membership VG function, which is used
to update membership[i] (the other VG functions are all library functions).
As an example, consider the following initialization:
create table clus prob[0] (clus id, prob) as
with diri res as Dirichlet(select clus id, pi prior from cluster)
select diri res.out id, diri res.prob from diri res;
This code uses the hyperparameters stored in the cluster table to parameterize the
Dirichlet VG function, which then outputs the value of π(0) as a set of (k, π(0)
k ) pairs,
that are then stored in the clus prob[0] table.
Here is an example of a recursive deﬁnition:
create table clus prob[i](clus id, prob) as
with diri res as Dirichlet
(select cmem.clus id, cmem.count num+cluster.pi prior as diri para
from (select cm.clus id as clus id, count(cm.data id) as count num
from membership[i-1] as cm
group by clus id) as cmem, cluster

110
where cmem.clus id = clus.clus id)
select diri res.out id, diri res.prob from diri res;
This code parameterizes the Dirichlet distribution by performing the required α +
P
j c(i−1)
j
computation, in order to re-sample the selection probability for each of the
clusters. This computation requires that I compute the number of times that each
data point is assigned to each cluster, which is done via SQL aggregation.
4.4.3
GraphLab Implementation
I deﬁne three types of vertices: data vertices, cluster vertices, and the mixture-
proportion vertex.
There is a one-to-one mapping between data points and data
vertices. The clusters are put into a separate cluster vertex, and the vector of mixing
proportions π is maintained by the mixture-proportion vertex. The cluster vertices
and data vertices deﬁne a complete, bipartite graph and the mixture-proportion ver-
tex is connected to each of the data vertices.
Each Gibbs sampler iteration is implemented using GraphLab’s gather-apply-
scatter abstraction.
In the gather phase, each node “gathers” the state of all of
its neighbors. In my implementation, the jth data vertex exports a view of itself that
contains three values: the data point xj, the vector cj identifying the cluster that
produced the jth data point; and the matrix ˆΣj = (xj −µ1(cj))(xj −µ1(cj))T. Here,
1(cj) returns the identity of the entry in cj having the value “1”. This is stored as a
triple ⟨cj, xj, ˆΣj⟩. This triple is “gathered” by all of the nodes attached to the data
vertex. The mixture-proportion vertex sums up all of the cj vectors as it examines
the triples associated with each data vertex, and the kth cluster vertex sums up all
of the xj values (and all of the Σj values) for those data vertices having a 1 in the
kth position in cj.

111
Likewise, each data vertex “gathers” the state of the model from the mixture-
proportion vertex (where it receives the vector π) and from each of the cluster vertices
(where it receives the cluster mean vector µk and cluster covariance matrix Σk).
Next comes the apply phase, where each one of the vertices re-samples its own
state. Each data vertex re-samples cj, the mixing-proportion vertex re-samples π,
and the cluster vertices re-sample the cluster mean and covariance matrix.
In the scatter phase, each vertex signals all of its adjacent vertices, letting them
know that the apply phase has completed.
4.4.4
Giraph Implementation
The graph I use for Giraph is identical to the GraphLab graph. However, as I will
describe, the Giraph programming model is more oriented towards message-passing,
as opposed to computation over a predeﬁned set of edges. These messages can be
constructed case-by-case, whereas in GraphLab a vertex must simple export a single
view of its internals to the rest of the world.
The actual learning computation begins with an initialization of the cluster-related
parameters ⟨µ(0)
k , Σ(0)
k , π(0)
k ⟩stored in the each cluster vertex followed by an initializa-
tion of the membership of each data point c(0)
j
stored in each data vertex.
After initialization, the cluster-membership vertex sends the current value of πk
to the kth cluster vertex, so K messages are sent in all. Each cluster vertex receives
this mixing-proportion message and broadcasts the triple ⟨µk, Σk, πk⟩to the whole
system. The jth data vertex receives these K messages, and samples its membership
cj. It computes the matrix ˆΣj = (xj −µ1(cj))(xj −µ1(cj))T. (Again, 1(cj) returns the
identity of the entry in cj having the value “1”.) It then sends the double ⟨xj, ˆΣj⟩
to the cluster vertex indicated by cj. Each cluster aggregates those messages, and

112
resamples its posterior covariance Σk and mean µk. Each cluster counts the number
of points assigned to it, and sends this count in a message to the cluster-membership
vertex, which updates π.
I attempted to optimize the computation as much as possible. To save the memory,
I do not record the edges between the data vertices and the cluster vertices explicitly;
I instead use broadcast to communicate from the cluster vertices to data vertices, and
I use a naming scheme allowing each data vertex to send a message to the appropriate
cluster vertex without the system recording the edge. Giraph’s combiner functionality
is used to reduce communication and increase load balancing during aggregation.
Numerical Computing in Java. One signiﬁcant issue is the diﬃculty of ﬁnding
an appropriate Java-language statistical/numerical software package. I went with the
Mallet library [170] due to its completeness. I found Mallet to be slow for problems
requiring high-dimensional linear algebra. A library such as JBLAS that wraps a
high-performance C linear algebra package might be faster, but passing the Java/C
barrier can be expensive, and such solutions are generally not “plug and play.”
4.4.5
Experiments and Results
I tuned each code and platform in order to obtain the fastest per-iteration running
time possible. I then created a synthetic data set having ten dimensions, generated
using a mixture of ten Gaussians, and used each platform to learn these Gaussians.
For each platform, I ran the GMM inference code on three compute clusters of diﬀerent
sizes: ﬁve machines, 20 machines, and 100 machines. The amount of data per machine
was kept constant at ten million data points, so (for example) the 100 machine cluster
was used to perform GMM inference over one billion data points.
Furthermore, I created a second data set with 100 dimensions. I then ran an ex-

113
periment where the amount of data are constant at 1 million data points per machine.
I performed inference for this data set only on the ﬁve-machine cluster.
Note that each platform is running exactly the same MCMC simulation (with
only minor diﬀerences in the initialization and order of the updates in each iteration).
Thus, it is not informative to examine the actual models learned.
The results are shown in Figure 4.1 (a). In this ﬁgure I show, for each experiment,
the average per-iteration running time, computed over the ﬁrst ﬁve iterations of the
MCMC simulation. In parens, I give the time required to initialize the simulation
(this typically includes choosing the initial cluster parameters and perhaps doing the
initial assignment of data points to clusters). This is a one-time cost. “Fail” means
that while the code could be executed on a small problem size, it could not be run at
the scale required by the experiment via any reasonable amount of tuning.
4.4.6
Discussion
SimSQL, Spark and Giraph: No Signiﬁcant Diﬀerences. Putting aside for a
moment the failure of Giraph to scale to 100 machines or handle the 100-dimensional
problem, there is not a signiﬁcant diﬀerence in the non-GraphLab runtimes depicted
in Figure 4.1 (a). The exception is SimSQL at 100 dimensions, which has a per-
iteration time that is twice that of Spark. The reason for this is that the GMM
simulation must aggregate one (xj −µ)(xj −µ)T matrix (which is a 10,000 entry
matrix for 100-dimensional data) for each data point. In SimSQL, this is performed
using a costly GROUP BY, which is slower than the Spark matrix/vector operations.
Java vs. Python. I was curious as to whether there was a performance hit using
Python with Spark as opposed to Java with Spark.
I chose Python because the
Spark + Python interface is very nice, leading to beautiful codes, and Python is now

114
(a) GMM: Initial Implementations
10 dimensions
100 dimensions
code size
5 machines
20 machines
100 machines
5 machines
SimSQL
197
27:55 (13:55)
28:55 (14:38)
35:54 (18:58)
1:51:12 (36:08)
GraphLab
661
Fail
Fail
Fail
Fail
PySpark
236
26:04 (4:10)
37:34 (2:27)
38:09 (2:00)
47:40 (0:52)
Giraph
2131
25:21 (0:18)
30:26 (0:15)
Fail
Fail
(b) GMM: Alternative Implementations
10 dimensions
100 dimensions
code size
5 machines
20 machines
100 machines
5 machines
Spark (Java)
737
12:30 (2:01)
12:25 (2:03)
18:11 (2:26)
6:25:04 (36:08)
GraphLab (SV)
681
6:13 (1:13)
4:36 (2:47)
6:09 (1:21)∗
33:32 (0:42)
(c) GMM: Super Vertex Implementations
10 dimensions, 5 machines
100 dimensions, 5 machines
w/o SV
with SV
w/o SV
with SV
SimSQL
27:55 (13:55)
6:20 (12:33)
1:51:12 (36:08)
7:22 (14:07)
GraphLab
Fail
6:13 (1:13)
Fail
33:32 (0:42)
Spark (Python)
26:04 (4:10)
29:12 (4:01)
47:40 (0:52)
47:03 (2:17)
Giraph
25:21 (0:18)
13:48 (0:03)
Fail
6:17:32 (0:03)
Figure 4.1 : GMM MCMC implementation; lines of code and average time per itera-
tion. Time in parens is for the initialization/setup. “SV” is a short for super vertex.
Format is HH:MM:SS or MM:SS. ∗I was actually unable to run GraphLab at 100
machines, and the closest to 100 machines that I could get was 96 machines.

115
commonly used for statistical/numerical computing (NumPy and PyGSL, which I
used, are quite popular, for example). However, Spark + Python uses Py4J, which
uses sockets to send data back and forth between a Python interpreter and the JVM.
This may be slow.
Thus, I re-implemented the Spark GMM MCMC code in Java with Mallet, and
ran the experiments once again. The results are shown in Figure 4.1 (b). For the
ten-dimensional inference problem, Java takes around 50% of the Python time. But
for the 100-dimensional problem, it is more than eight times slower, presumably due
to the cost of 100-dimensional Java linear algebra.
I re-visit this in Section 10 of the chapter, where I compare Spark Java vs. Spark
Python on the LDA learning task.
GraphLab, Giraph and Super Vertex Codes. My GraphLab implementation
failed every attempt to run it at the scale required by the experiments. Why? In
a typical “hand-coded” implementation of the GMM inference algorithm, a program
would store the current model in RAM, and then cycle through all of the data points
in sequence, updating the cluster membership for each. In GraphLab, there is no
notion of pinning a single copy of the model in RAM and cycling through the data
points. Instead, the data vertex associated with each node must gather a copy of the
model via the links in the graph, and the user cannot control this process. In practice,
GraphLab seems to simultaneously materialize one 50KB copy of the model for each
data point, which quickly exhausts the available memory and the computation fails.
How can one get around this problem? One method I considered was to move the
model out of the graph, eschew the gather-apply-scatter mechanism, and perform the
aggregation needed to update the model with one of GraphLab’s distributed aggre-
gation facilities. The issue is that such an implementation would not use GraphLab’s

116
graph abstraction in any meaningful way.
In the end, I settled on the idea of combining a large numbers (hundreds of
thousands) of data points together to form “super vertices” (I use 8000 super vertices
on my 100 machine cluster). The gather phase in this implementation changes in that
there is only one copy of the model obtained for the entire super vertex. Further, the
view of the super vertex exported to its neighbors becomes an array of ⟨k, nk, µk, Σk⟩
tuples, with one tuple for each value of k in 1...K. Here, nk is the number of points in
the super vertex assigned to cluster k, µk is the sum over xj where xj has been assigned
to cluster k, and Σk is computed similarly from a sum over (xj −µ(i)
k )(xj −µ(i)
k )T
for those points assigned to cluster k. A cluster vertex in the gather phase collects
only the one triple assigned to it by each super vertex. In the apply phase, the super
vertex then runs through each of its data points, in sequence.
In addition to radically reducing the memory requirements, this has the additional
beneﬁt of distributing most of the heavy-duty aggregation to the super vertices, in-
stead of at the cluster vertices.
The result is very fast, as shown in Figure 4.1 (b).
A similar “super vertex”
construction was a necessary part of each one of the GraphLab implementations
described in the chapter; without it, none of my GraphLab codes would run.
Super Vertex Codes on Other Platforms. Naturally, GraphLab is not the only
platform that can beneﬁt from the super vertex construction.
By grouping data
points together and handling them as a single unit in “hand-coded” C++ (Sim-
SQL/GraphLab), Java (in Giraph), or Python (Spark) I can often realize signiﬁcant
speedups. In Figure 4.1 (c) which shows the running times obtained through GMM
super vertex codes on each of the platforms. SimSQL in particular can be made to
run extremely fast using a super vertex construction; the 100-dimensional GMM im-

117
plementation ran in a time that was only 20% of its nearest competitor (GraphLab).
4.5
The Bayesian Lasso
The Bayesian Lasso [66] is a well-known regularized Bayesian linear regression for-
mulation and associated MCMC sampler. Let the data set D consist of a set of n
⟨x, y⟩pairs. The model consists of a p-dimensional vector of regression coeﬃcients β,
where the (centered) response ˜y associated with the data point x is assumed to be
generated as ˜y ∼Normal(β · x, σ2), and σ2 has an inverse-gamma prior.
To simplify the MCMC simulation used to learn the model, a set of p auxiliary
variables τ 2
1 , τ 2
2 , ... are introduced that control the variance of the various regression
coeﬃcients as they are updated. Let ˜y be the centered response vector and X denote
the matrix of regressors constructed from D. Then the MCMC simulation for learning
the model is as follows:
1/(τ (i)
j )2 ∼InvGaussian(
s
λ2(σ(i−1))2
(β(i−1)
j
)2 , λ2)
β(i) ∼Normal
³
(A(i))−1XT ˜y, σ2(A(i))−1´
, where
A(i) = XTX + (D(i)
τ )−1 and
D(i)
τ = diag
³
(τ (i)
1 )2, (τ (i)
2 )2, ...
´
(σ(i))2 ∼InvGamma(1 + n + p
2
,
2 + P
⟨x,y⟩∈D (˜y −β(i) · x)
2 + P
j(β(i)
j )2/(τ (i)
j )2
2
)
4.5.1
Spark Implementation
In the Spark implementation, I ﬁrst create a RDD data by reading the input ﬁles
from HDFS, and use the RDD to compute the centered response ˜y:

118
data = lines.map(parseData).cache()
y sum = data.map(lambda (id, (x, y)): y).sum()
y avg = y sum/data.count() data = data.map(lambda (id,(x, y)): (id, (x, y-y avg)))
Next, the Gram matrix and the quantity vector are computed:
XX = data.flatMap(lambda (id, (x, y)): computePairSum(x, y)).reduceByKey(add)
XY = data.flatMap(lambda (id, (x, y)): computeXYSum(x, y)).reduceByKey(add)
Most of the code of the main loop of the Gibbs sampler is run locally, since most
of the computation required by the loop is quite modest for data of low to medium
dimensionality (up to a few thousand dimensions).
Only one actual MapReduce
job is needed per iteration. Speciﬁcally, I must compute P
⟨x,y⟩∈D (˜y −β · x)2 in a
distributed fashion in order to parameterize the inverse-gamma distribution needed
to compute (σ(i))2. The code is:
remain sum1 = data.map(lambda (id,(x, y)): computeRemainSquare(x, y, beta)).sum()
4.5.2
SimSQL Implementation
The SimSQL implementation begins by creating three materialized views that will be
used repeatedly, in each iteration of the MCMC simulation. The three materialized
views include (1) the Gram matrix computed over X, (2) the centered response vector
˜y, and (3) the quantity ||yi × xi||1. The most expensive of these views to actually
compute is the Gram matrix XTX.
Aside from those three materialized views, the SimSQL implementation of the
Bayesian Lasso utilizes three random tables: (1) the beta[i] table which stores the
β(i) vector as a set of tuples, (2) the sigma[i] table (with one tuple) that stores the ith
value of (σ(i))2, and (3) the tau[i] that stores one tuple for each 1/(τ (i)
j )2 value. These

119
CREATE TABLE statements for these three tables have a very close correspondence with
the three diﬀerent MCMC update steps described above. Consider tau[i]:
CREATE TABLE tau[i](rigid, tau Value) AS
FOR EACH r IN regressor IDs
WITH IG AS InvGaussian(
(SELECT sqrt((pr1.lambda*pr1.lambda*s.sigma)/(b.bet*b.bet))
FROM prior pr1, sigma[i] s, beta[i-1] b
WHERE b.rigid = r.grid),
(SELECT (pr2.lambda * pr2.lambda)
FROM prior pr2))
SELECT r.grid, (1.0 / IG.out) FROM IG;
Here, the table prior has a single tuple that gives all of the prior hyperparameters.
4.5.3
GraphLab Implementation
Again, I use a super vertex-based implementation. The ith data vertex in the graph
is made up of a large number of data points, forming a matrix Xi and vector yi. The
jth model vertex consists of the inverse of the auxiliary variable 1/τ 2
j . I use one more
vertex to store the regression coeﬃcient vector β and the variance σ2. This vertex
sits in the center of the graph structure.
I begin using GraphLab’s map reduce vertices method to obtain a couple of
invariant statistics. While using an operation that measures the (non-existent!) global
state of an asynchronous system might be problematic during the actual Markov chain
simulation, it is a nice way to collect statistics before the simulation begins. I use one
MapReduce to calculate the Gram matrix XTX and to center the response. Based
on y, the second MapReduce calculates XT ˜y and stores it as a global variable.

120
Next I use GraphLab’s gather-apply-scatter abstraction to simulate the required
Markov Chain. The gather phase collects ⟨βj, σ2⟩for the j-th model vertex from the
center vertex. The center vertex collects the vector of auxiliary variables 1/τ 2 from
the model vertices, as well as P
⟨x,y⟩∈D (˜y −β · x)2 from the data vertices.
The apply phase then updates the center vertex by sampling new β and σ2 val-
ues, and updates each model vertex by sampling 1/(τj)2 from the inverse Gaussian
distribution based on σ2 and βj.
4.5.4
Giraph Implementation
My implementation uses three types of graph vertices: data vertices, dimensional
vertices and a model vertex, which are used to store the dataset, collect required
statistics such as the Gram matrix XTX, and update the model (β, σ2 and each τ 2
j ),
respectively. A dimensional vertex is associated with each data dimension.
The actual learning computation begins with the computation of XTX and XT ˜y.
First, each data vertex computes xTx and sends this quantity along with y to the
other two types of vertices. The jth dimensional vertex collects the statistics for the
jth row of the Gram matrix, while the model vertex computes ˜y. The Gram matrix is
saved to the model vertex based on the messages passed from the dimensional vertices,
while each data vertex computes xT ˜y, and sends it to the model vertex. The model
vertex ﬁrst initializes each 1/τ 2
j , σ2, and β. The model vertex then broadcasts β to
the data vertices so that they can compute (˜y −β · x)2. The model vertex obtains
the sum of these values over all data vertices, and then updates each 1/τ 2
j , σ2, and
β. The updated β is then re-broadcast to all of the data vertices and the process is
repeated. I make use of Giraph’s combiner and aggregator facilities wherever possible
to speed the computation and reduce communication.

121
Bayesian Lasso
lines of code
5 machines
20 machines
100 machines
SimSQL
100
7:09 (2:40:06)
8:04 (2:45:28)
12:24 (2:54:45)
GraphLab (SV)
572
0:36 (0:37)
0:26 (0:35)
0:31 (0:50)
Spark (Python)
168
0:55 (1:26:59)
0:59 (1:33:13)
1:12 (2:06:30)
Giraph
1871
Fail
Fail
Fail
Giraph (SV)
1953
0:58 (1:14)
1:03 (1:14)
2:08 (6:31)
Figure 4.2 : Bayesian Lasso MCMC implementation; lines of code and average time
per iteration. Format is HH:MM:SS or MM:SS. “SV” is a short for super vertex.
4.5.5
Experiments and Results
To test these four implementations, I created a synthetic data set having 103 regres-
sor dimensions and a one-dimensional response. As in GMM experiments, I held
the number of data points per machine constant at 105 and tested compute clusters
consisting of ﬁve, 20, and 100 machines. The results are shown in Figure 4.2. Gi-
raph was unable to run without implementing the simulation using the super vertex
construction.
4.5.6
Discussion
SimSQL, Spark, and Long Initialization Times. Note the substantial perfor-
mance gap between SimSQL and Spark on one hand and GraphLab and Giraph on
the other, considering the time to run the initialization code. Both of the former sys-
tems require between one and three hours for initialization/startup, while GraphLab
takes under one minute. Normally, MCMC initialization time is not particularly im-

122
portant, since initialization is run only once. But in the case of the Bayesian Lasso,
the simulation converges very quickly, magnifying the importance of the initialization.
SimSQL’s relatively slow performance can be explained by its lack of support for
vector and matrix operations. Consider the initialization phase of the simulation,
which requires computing the Gram matrix XTX over the data set. In SimSQL, the
computation is performed as an aggregate-GROUP BY query, with one group for every
one of the one million entries in the Gram matrix.
In contrast, consider GraphLab’s super vertex code.
The ith super vertex in
the GraphLab implementation has several thousand data points, which are stored in
C++ as a matrix Xi. This super vertex locally computes XT
i Xi using a fast matrix
multiplication. At the same time, a similar computation is taking place all over the
compute cluster. The resulting few thousand, million-entry matrices are then sent to
a central location and added together.
In the latter case, most of the computational work is done in a high-performance
linear algebra library. In the former, it is done using the relational engine. This
accounts for the huge diﬀerence.
SimSQL’s Relatively High Per-Iteration Times. SimSQL is also slow on a per-
iteration bases. SimSQL takes about ten times as long as Spark, 20 times as long as
GraphLab, and ﬁve times as long as Giraph, per iteration. This means that SimSQL
would spend around ﬁve hours to initialize and run twelve iterations, as opposed to
two hours 15 minutes for Spark. Here again, SimSQL seems to suﬀer greatly from
the fact that each xi is stored as one thousand tuples rather than a single vector, and
A−1XT ˜y must be computed using set-oriented aggregates.

123
4.6
Hidden Markov Model
The next model I consider is a hidden Markov model (HMM) for text. This model
and the associated MCMC simulation are somewhat more intricate than the previous
two.
Assume that xj is the ordered list or vector of words that make up the jth doc-
ument. Each word xj,k in xj is assumed to be produced by one of K hidden states.
Hidden state s has an associated probability vector Ψs, where Ψs,w is the probability
that state s would produce word w. Since the model is sequential, there is also a
state-to-state transition probability vector δs associated with state s. Thus, δs,s′ is
the probability of transitioning from state s to state s′ when one moves to the next
word in the document.
Let yj be the vector of n state assignments associated with the jth document. To
generate yj using the HMM, I imagine that the start state yj,1 for document j is ﬁrst
selected by sampling from a Categorical(δ0) distribution (since there does not exist a
zeroth state, δ0 is used to control the start state of each document). Then, for each k
in 2...n, Pr[yj,k+1|yj,k] = δyj,k,yj,k+1. Finally, I generate the ith document by sampling
each xj,k from a Categorical(Ψyj,k) distribution. I put a Dirichlet(α) prior on each δs,
and a Dirichlet(β) prior on each Ψs.
To learn this model, I utilize a simulation that updates every other state assign-
ment in a given step. Speciﬁcally, if the current step i of the simulation is even and

124
k is even, or if the current step ith is odd and k is odd, I have:
Pr[y(i)
j,k = s] ∝δ(i−1)
0,s
× Ψ(i−1)
s,xj,k × δ(i−1)
s,y(i−1)
j,k+1 if k = 1
∝δ(i−1)
y(i−1)
j,k−1,s × Ψ(i−1)
s,xj,k if k ends the document
∝δ(i−1)
y(i−1)
j,k−1,s × Ψ(i−1)
s,xj,k × δ(i−1)
s,y(i−1)
j,k+1 otherwise.
Otherwise, the state assignment does not change in iteration i:
y(i)
j,k = y(i−i)
j,k
The other updates are as follows. Let f(w, s) = P
j,k one(xj,k = w and y(i)
j,k = s),
where one() returns 1 if the Boolean argument is true and 0 otherwise. Similarly,
g(s) = P
j one(y(i)
j,0 = s) and h(s, s′) = P
j,k one(y(i)
j,k = s and y(i)
j,k+1 = s′). Then:
Ψ(i)
s ∼Dirichlet(β + ⟨f(1, s), f(2, s), f(3, s)...⟩)
δ(i)
0 ∼Dirichlet(α + ⟨g(1), g(2), g(3)...⟩)
δ(i)
s
∼Dirichlet(α + ⟨h(s, 1), h(s, 2), h(s, 3)...⟩)
4.6.1
Spark Implementation
I describe my document-based Spark implementation, where Spark is asked to man-
age and aggregate data at the document, rather than the word level. This Spark
implementation begins by creating a RDD called d w seq. This RDD stores, for each
document, the document identiﬁer and its associated list of words. Then, a trans-
formation is applied to randomly initialize the states, so the list of words in each
document in d w seq is replaced by a list of (word, state) pairs. In the following
code snippets, I use d id and s id to refer to the the document identiﬁer and state
identiﬁer, and state size to store the total number of states:

125
d w seq = lines.map(parseDoc).cache()
d w s seq = d w seq.mapValues(lambda w seq: init state(w seq, state size))
Now comes the main loop of Gibbs sampler.
In each iteration, I ﬁrst sample
the state transition matrix δ by using two jobs. The ﬁrst job computes, for each
state s (used as the key), the total number of state transitions (s, s′) for each target
state s′ that occurred in the previous iteration. These aggregates are then used to
parameterize a MapReduce job that updates the δ values:
h = d w s seq.flatMap(lambda (d id, w s seq): comp h(w s seq, state size))
.reduceByKey(lambda p1,p2: addStateCount(p1, p2, state size))
delta = h.mapValues(lambda h s: sample delta(h s, state size)).collectAsMap()
Similarly, I use two jobs to sample Ψ. One more MapReduce job applies a self-
transformation of d w s seq to the states:
d w s seq = d w s seq.mapValues(lambda w s seq: update state(w s seq, delta, psi))
Note that update state(.) is a user-deﬁned function that alternatively updates the
states for the even words and the odd words.
4.6.2
SimSQL Implementation
The word-based SimSQL HMM relies on four random tables: states[i], starts[i],
trans[i], and emits[i]. These four tables store all of the y(i)
j
vectors, the δ0 vector,
all of the δ(i)
s
vectors, and all of the Ψ(i)
s
vectors, respectively.
At the heart of the SimSQL implementation is the states[i] table, with schema
(docID, prevPos, curPos, nextPos, prevStateID, curStateID). This table stores
the current set of state assignments. If i is even, then for each (j, k) pair where k is

126
even, there will be one tuple in this table that stores:
(j, k −1, k, k + 1, y(i)
j,k, y(i)
j,k+1)
If i is odd, then there will be one such tuple for each odd k.
To update each tuple to create the (i + 1)th version of the states table, I would
update the above tuple to produce:
(j, k −2, k −1, k, y(i+1)
j,k−1, y(i+1)
j,k
)
All of the new tuples produced in this way from states[i] then constitute the new
contents of states[i+1].
Performing this update requires (a) setting y(i+1)
j,k
in the new tuple to be equal to
y(i)
j,k in the old tuple, and (b) sampling a new value of y(i+1)
j,k−1 using the Categorical()
VG function. Performing this sampling in turn requires having access to three types
of data, in addition to the tuple being updated: (1) the value for y(i)
j,k−2, (2) all of
transition probabilities out of state y(i)
j,k−2, and (3) the probability that each and every
possible state would emit word xj,k−1. Gathering all of these values together actually
requires a six-table join to parameterize the Categorical VG function.
The reader may wonder why I included all three of prevPos, curPos, and nextPos
in each tuple in states[i].
Why not simply include curPos?
The reason is a
quirk in the SimSQL implementation.
The six-table join required to parameter-
ize the Categorical VG function requires join predicates of the form t1.curPos =
t2.curPos + 1. The SimSQL optimizer has problems with this sort of equality pred-
icate, implementing it ineﬃciently as a cross-product. My way around this was to
explicitly store nextPos, so the join could be written as t1.curPos = t2.nextPos,
which is handled eﬃciently as an equi-join.

127
4.6.3
GraphLab Implementation
The graph I use for the HMM simulation contains only two types of vertices. Each
data (super) vertex contains the xj vectors for a large number of documents as well
as the corresponding state assignment vectors. In addition, there is one state vertex
associated with each of the K hidden states.
The state vertex for state s stores
the word emission probability vector Ψs and the state-to-state transition probability
vector δs. This graph is complete and bipartite.
The GraphLab computation begins by initializing the state assignment for each
word by performing a transform vertices operation over all the data vertices. Then
the gather-apply-scatter computation begins. In the gather phase, the data vertices
collect Ψs and δs from all the state vertices, while each state vertex s computes
f(w, s), g(s), h(s, s′) based on the words whose state assignments are also s from the
data vertices. The apply phase simply updates y, Ψs and δs according to the rules
described above.
4.6.4
Giraph Implementation
Similarly, two types of vertices are used in the Giraph implementation. Since (unlike
for GraphLab) I did not implement only a super vertex version of the Giraph simu-
lation, depending upon the level of granularity, a data vertex could correspond to a
word, a document, or a set of documents. Just as in the GraphLab implementation,
a state vertex maintains Ψs and δs.
Consider the simple, word-based Giraph implementation, where each data vertex
stores only a word.
In this case, there is an edge between the vertex associated
with each xj,k and the vertices associated with both xj,k−1 and xj,k+1. To begin the
computation, the data vertices randomly initialize each yj,k, and then send yj,k to

128
their neighbors. All the data vertices then record their neighbor’s states yj,k−1 and
yj,k+1, and then send a word-count pair ⟨xj,k, 1⟩and a state-count pair ⟨yj,k+1, 1⟩to
the (yj,k)th state vertex, where they are aggregated. To facilitate fast aggregation, I
use the Giraph combiner mechanism. Next, the state vertices update their emission
and transition probability vectors based on the collected statistics. Both probability
vectors are broadcast back to the data vertices for updating their states.
4.6.5
Experiments and Results
To test the four implementations, I created a synthetic document database. To cre-
ate each “document”, I choose two newsgroup postings from the ubiquitous 20 news-
groups data set and concatenate the postings end-on-end. Since there are 20,000
posts in this data set, it was possible to create up to 400 million diﬀerent synthetic
documents in this way. I used a dictionary size of 10 thousand words and used K = 20
diﬀerent states. The average document in the database was 210 words in length. As
before, I tested the ability of each implementation to run on various compute cluster
sizes, and I kept the amount of data on each machine constant. I used 2.5 million
documents on each machine.
In my ﬁrst experiment, I tested “word-based” HMM implementations built on
top of Giraph, SimSQL, and Spark. Here “word-based” corresponds to the simulation
where each word (with its associated hidden state) is individually pushed through the
system. Since only SimSQL was able to handle this simulation (and SimSQL took
more than eight hours per iteration), I only ran this simulation on ﬁve machines. The
results are shown at the left in Figure 4.3 (a). Note the “NA” for the number of
lines of code for Spark. I could not get Spark to perform the required self-join of the
set state assignments with itself without failing. Since this join is a prerequisite for

129
(a) HMM: Word-based and document-based implementations
Word-based, 5 machines
Document-based, 5 machines
lines of code
running time
lines of code
running time
SimSQL
131
8:17:07 (10:51:32)
123
3:42:40 (20:44)
Spark (Python)
NA
Fail
214
4:21:36 (27:36)
Giraph
1717
Fail
1470
11:02 (7:03)
(b) HMM: Super Vertex Implementations
lines of code
5 machines
20 machines
100 machines
Giraph
1735
2:27 (1:12)
2:44 (1:52)
3:12 (2:56)
GraphLab
681
20:39 (16:28)
Fail
Fail
Spark (Python)
215
3:45:58 (11:02)
4:01:02 (13:04)
Fail
SimSQL
136
2:05:12 (1:44:45)
2:05:31 (1:44:36)
2:19:10 (2:04:40)
Figure 4.3 : HMM results. Time in parens is for the initialization/setup. Format is
HH:MM:SS or MM:SS.
running a“word-based” HMM, I did not implement the rest of the algorithm.
I also tested a “document-based” HMM implementation built on Giraph, SimSQL,
and Spark; the experiment was run using ﬁve machines. In this implementation, the
re-sampling of all of the hidden states for an entire document is handled as a group,
and hand-coded in Java (Giraph), Python (Spark), or C++ (SimSQL). For all of the
platforms, this has three main beneﬁts. First, the state of the model (all of the Ψ(i)
s
and δ(i)
s
vectors) need only be associated with each document, and not with each word
individually. Second, the platform itself need not link up all of the adjacent words

130
and states in order to re-sample each y(i)
j,k. This linking can be done internally, within
the user-supplied code. On all platforms except for SimSQL, the statistics necessary
to update all of the Ψ(i)
s
and δ(i)
s
vectors can be output at once on a per-document
basis. The results are shown at the right in Figure 4.3 (a).
Finally, I wrote a “super vertex” HMM implementation for all four platforms
where a large number of documents are grouped together, and the hidden states
associated with all of the words for all of the documents are updated together. The
results obtained by running the HMM super vertex implementation for each of the
four platforms are shown in Figure 4.3 (b).
4.6.6
Discussion
The Giraph HMM Simulation Is Really Fast. This much is obvious; I consider
some of the reasons now.
Giraph’s Speed Relative to SimSQL. Even though the SimSQL super vertex
implementation groups a large number of documents together, and then generates
all of the y(i)
j,k vales associated with each word in each document in the group via a
single C++ VG function invocation, all of those generated values must be output by
the VG function as tuples. SimSQL must then aggregate all of the output tuples to
compute the f(.), g(.), and h(.) functions required by the simulation. This is very
time-consuming. True, the process could likely be sped up considerably by doing pre-
aggregation within the VG function (a similar tactic was used to make the SimSQL
GMM super vertex simulation the fastest of all of the platforms) but this is a bit
awkward because it requires encoding all of the output y(i)
j,k values plus all of the
aggregates as a single output table. Giraph, on the other hand, need not output all
of the y(i)
j,k values; they are stored internally, within the super vertex.

131
Comparing GraphLab and Giraph.
Why was Giraph so much faster than
GraphLab, and why does it scale better?
I tackle the second question ﬁrst. All failures were memory-related. It is not that
the data set itself is too large for GraphLab to handle. My compute cluster has 7× the
RAM needed to store the corpus. In practice, however, I was constantly struggling to
overcome memory problems with GraphLab. It appears that this is related to the lack
of control that a GraphLab programmer has over the way that data are materialized
and moved around. Consider the problem of aggregating all of the data required
to compute f(w, s), g(s), and h(s, s′) for a particular s—this aggregation is going
to happen at each of the 20 graph vertices that correspond to the 20 hidden HMM
states. Each super vertex will produce around 10MB of data that counts the number
of times that each particular state-to-state transition happens as well as the number
of times that each word is associated to each state. If this set of counts arrives at a
state vertex from each of the 10,000 super vertices, I could easily end up having to
materialize 100GB of data, killing the computation.
Giraph provides a richer programming interface that allowed us to sidestep some
of the more serious computational and memory-related problems that are associated
with mapping the simulation to a graph. For example, consider the aggregate compu-
tation described above. Giraph features graph-based aggregation functionality (with
combiners) that oﬀers a far faster (and safer) mechanism for gathering the required
statistics.
4.7
Latent Dirichlet Allocation
Latent Dirichlet Allocation (LDA) is a model for text mining. It views each word in
a document as being produced by a latent topic, so that if a topic t produces the kth

132
word in document j (denoted as wj,k), then Pr[wj,k = ω] = φt,ω, where φt represents
the word distribution for topic t. Further, each document has a topic distribution,
where the distribution for document j is a vector θj. Let zj,k indicate which topic
produced word k in document j; then Pr[zj,k = t] = θj,t. A Dirichlet(α) prior is put
on each θj vector, and a Dirichlet(β) parameter is put on each φt vector.
Gibbs sampling is the typical way that an LDA model is learned. The “collapsed”
LDA Gibbs sampler is standard. Here “collapsed” means that one or more variables
have been integrated out in the derivation of the Gibbs sampler. I choose, however,
to include the non-collapsed sampler in my benchmark for two reasons. First, it is
more interesting as a benchmark because it is a bit more complicated, including more
parameters that must be computed. Second, there is the issue of correctness. It is
very challenging to parallelize the collapsed LDA Gibbs sampler correctly because of
the complex correlation structure that the collapsing induces among the updates to
the various zj vectors. Most parallel/distributed LDA Gibbs samplers ignore these
correlations and update the vectors in parallel, disregarding the eﬀect of the concur-
rent updates of the zj vectors. I am uncomfortable with benchmarking an aggressive
(and somewhat questionable) computational trick.
This simulation corresponding to the non-collapsed sampler is then deﬁned as
follows. Let f(j, t) = P
k one(z(i)
j,k = t) and let g(t, ω) = P
j,k one(wj,k = ω and z(i)
j,k =
t). Then:
Pr[z(i)
j,k = t] ∝θ(i−1)
j,t
× φ(i−1)
t,wj,k
θ(i)
j
∼Dirichlet(α + ⟨f(j, 1), f(j, 2), f(j, 3), ...⟩)
φ(i)
t
∼Dirichlet(β + ⟨g(t, 1), g(t, 2), g(t, 3), ...⟩)
Lacking space, I do not describe the LDA implementations.

133
4.7.1
Experiments and Results
I used the document database used in the HMM experiments, a dictionary size of
10,000 words, and a model size of 100 topics.
The ﬁrst thing that I wanted to do was to implement a “pure”, word-based LDA.
In this implementation, the wj,k and z(i)
j,k values are managed as individual elements
by the underlying compute platform. However, since neither Spark nor Giraph were
able to handle the word-based HMM simulation (and GraphLab was unable to handle
a similar simulation for the relatively simple GMM inference problem), it seemed
unlikely that I would be able to develop a word-based LDA for any of those platforms.
Thus, I only implemented this version of the LDA simulation for SimSQL. The result
of running this simulation on ﬁve machines (again with 2.5 million documents per
machine) is shown in Figure 4.4 (a).
I next implemented a document-based LDA, where all of the z(i)
j,k values are re-
sampled on a per-document basis. In SimSQL, these values still need to be output
(and aggregated) as individual tuples, but the VG function for re-sampling all of the
z(i)
j,k values for a single document need be parameterized only once. In Spark, I output
only a single z(i)
j
vector for a document, which should result in an even more eﬃcient
sampler than the one written in SimSQL. Giraph should be more eﬃcient still, since
they never need to output any z(i)
j,k values; these can be maintained internally in
memory within a graph vertex. Results are shown in Figure 4.4 (a).
Finally I implemented a super vertex version of the code for each of the four
platforms. The results are shown in Figure 4.4 (b).

134
(a) LDA: Word-based and document-based implementations
Word-based, 5 machines
Document-based, 5 machines
lines of code
running time
lines of code
running time
SimSQL
126
16:34:39 (11:23:22)
129
4:52:06 (4:34:27)
Spark (Python)
NA
NA
188
≈15:45:00 (≈2:30:00)
Giraph
NA
NA
1358
22:22 (5:46)
(b) LDA: Super Vertex Implementations
lines of code
5 machines
20 machines
100 machines
Giraph
1406
18:49 (2:35)
20:02 (2:46)
Fail
GraphLab
517
39:27 (32:14)
Fail
Fail
Spark (Python)
220
≈3:56:00 (≈2:15:00)
≈3:57:00 (≈2:15:00)
Fail
SimSQL
117
1:00:17 (3:09)
1:06:59 (3:34)
1:13:58 (4:28)
Figure 4.4 : LDA results. Time in parens is for the initialization/setup. Format is
HH:MM:SS or MM:SS.
4.7.2
Discussion
Everyone Fails Except for SimSQL. The LDA simulation is quite similar to the
HMM simulation, particularly the document-based and super vertex versions. The
big diﬀerence is that since there are 100 topics, the size of the model that must be
learned (and the size of the set of statistics computed) is around ﬁve times as large as
in the case of the HMM. This appears to makes the task a bit more diﬃcult, especially
for Giraph. My Giraph LDA implementation ran about ten times longer than the
HMM implementation, and, perhaps most signiﬁcantly, failed to run at all on 100

135
machines. SimSQL was the only platform that was able to run the LDA simulation
on 100 machines and 250 million documents.
4.8
Gaussian Imputation
The last model I consider is the most complicated of the ﬁve models, though not the
most diﬃcult model to handle computationally: a GMM modiﬁed to impute missing
values in the data. The MCMC simulation to learn the GMM and simultaneously use
it to impute the missing null values is similar to the GMM simulation, but with one
additional step where I must estimate the censored values. For the jth data point, let
x1
j denote the vector of censored values from xj and let x2
j denote the vector of un-
censored values. Then x1
j • x2
j is equivalent to xj, with the dimensions “scrambled”
so that the ﬁrst few dimensions were the ones that were censored. Let µj denote
mean vector of the cluster that (according to c(i−1)
j
) produced the jth data point,
and let Σj denote the corresponding covariance matrix. Re-arrange and partition the
dimensions of µj and Σj so that it is the case that:
x1
j • x2
j ∼Normal








µ1
j
µ2
j


,


Σ11
j Σ12
j
Σ21
j Σ22
j








I add an additional step to the GMM simulation. For each x1
j:
(x1
j)(i) ∼Normal
¡
µ1
j + Σ12
j (Σ22
j )−1(x2
j −µ2
j), Σ11
j −Σ12
j (Σ22
j )−1Σ21
j
¢
As was the case with LDA, space precludes us from describing each of my Gaussian
imputation implementations in detail.

136
Gaussian Imputation
lines of code
5 machines
20 machines
100 machines
Giraph
2274
28:43 (0:19)
31:23 (0:18)
Fail
GraphLab (Super vertex)
1197
6:59 (3:41)
6:12 (8:40)
6:08 (3:03)
Spark (Python)
294
1:22:48 (3:52)
1:27:39 (4:03)
1:29:27 (4:27)
SimSQL
182
28:53 (14:29)
30:41 (15:30)
39:33 (22:15)
Figure 4.5 :
Gaussian imputation results.
Time in parens is for the initializa-
tion/setup. Format is HH:MM:SS or MM:SS.
4.8.1
Experiments and Results
I used the ten-dimensional data from the GMM experiments, but performed a bit
of additional post-processing. For each data point, I took a sample p ∼Beta(1, 1),
which gave us a probability p of between 0 and 1, with a 0.5 average. Each of the ten
attribute values within the data point was then censored by ﬂipping a synthesized
coin which came up heads with probability p. If a heads was observed, the value
was replaced with a ∅. In this way, 50% of the attribute values in the data set were
censored.
As before, I kept the amount of data constant at 10 million data points per
machine, and tested the per-iteration running times at ﬁve, 20, and 100 machines.
The results are shown in Figure 4.5.

137
4.8.2
Discussion
Spark’s Increase in Running Time. What struck us as remarkable here was how
these results look almost exactly the same to the GMM results, with the exception
of a very signiﬁcant running time increase for Spark. The reason may be that in
the case of the GMM, I can store all of the data in memory using the cache()
function. However, in the imputation model, the actual data set changes constantly
as imputation is being performed.
4.9
Summary of Findings
Giraph and GraphLab.
I begin by discussing the two graph-based platforms:
Giraph and GraphLab.
On the positive side, they are both quite fast—typically faster than Spark and
SimSQL. However, both Giraph and GraphLab suﬀer greatly from memory-related
diﬃculties, and for that reason could not be made to run on the largest, most com-
plicated problems. The data sets were not too large to ﬁt into RAM. My largest data
set was around 1TB in size, which ﬁts comfortably in the 7TB of aggregate RAM
of my 100 machine compute cluster. In fact, my experiments demonstrate how the
constant mantra that “memory is the new disk” must be applied carefully. True,
there are few problems (even “big data” problems) where the raw data cannot ﬁt into
RAM, but ML inference problems typically require the computation of large sets of
complicated statistics, and the computation can temporarily blow up the size of the
data by orders of magnitude.
Giraph scaled better than GraphLab, seemingly because Giraph’s BSP-based
model maps nicely to the bipartite structure of the models tested. Had I tested models

138
with a sparser dependency graph which more closely matched GraphLab’s targeted
applications, results may have been diﬀerent. Moreover, Giraph’s synchronous pro-
gramming model includes mechanisms (such as combiners for performing distributed,
in-graph aggregation) that can be used to overcome some of the memory-related prob-
lems that plagued the graph-based platforms. The downside is that Giraph had large
and complex codes. GraphLab codes were small and elegant, especially considering
the language is C++.
SimSQL. SimSQL was often—though not always—slower than Giraph and GraphLab.
SimSQL’s purely tuple-oriented approach can hurt its performance, because (for ex-
ample) a 1,000 by 1,000 matrix is pushed through the system as a set of one million
tuples. This can result in some very long compute times—see the section on the
Bayesian Lasso (though, interestingly, SimSQL’s Gram matrix computation times
were only a bit slower than Spark’s). It seems that this is an issue that needs to be
addressed.
On the positive side, SimSQL was consistently able to perform computations that
none of the other three platforms could run, and was the only platform that never
failed to run any of the computations tested. All of the other platforms were clearly
at or beyond their limit on the largest problems, whereas SimSQL appeared ready
to scale up further. This robustness is due to the fact that under the hood, SimSQL
looks a lot like a parallel database system—it is well-understood how to scale such a
system.
Spark. On the positive side, Spark codes (particularly those written in Python) are
incredibly short and beautiful. Spark’s succinctness rivals that of SimSQL’s SQL
codes, though many users will ﬁnd Spark codes to be preferable, since they are im-
perative/functional.

139
LDA Spark Java Implementation
lines of code
5 machines
20 machines
100 machines
377
9:47 (0:53)
19:36 (1:15)
Fail
Figure 4.6 : Average time per iteration (and start-up time).
On the negative side, Spark was slower than the two graph-based platforms. I
was a bit worried these results were related to my choice of Python instead of Scala
or Java. I had already tested both Python and Java GMM implementations, but
just to be sure, I tested a Java Spark LDA implementation as well. The results are
shown in Figure 4.6. The speed is much better than the Python implementation, but
I could still not get Spark to run the LDA inference algorithm on 100 machines. The
implementation failed on 20 machines after 18 iterations as well.
Spark in general required a lot of tuning and experimentation to get things to
work on large and/or complicated problems. There was some disagreement among
the authors of this chapter as to why. One explanation is that Spark relies greatly
upon techniques such as “lazy evaluation” for speed and job scheduling, which looks
a lot like pipelining in a database system.
However, database systems often use
statistical information to decide when and how to pipeline. Without this, it is easy to
make bad decisions, and in general, I spent a lot of time tuning Spark, doing things
such as forcing RDDs to disk. Perhaps the ultimate solution is to make Spark—
and other dataﬂow systems—work more like a database system, carefully planning
computational choices such as RDD materialization and pipelining using cost models.
Final Thoughts. In the end, the “correct” platform choice is highly problem- and
user-dependent, depending on whether ease of implementation, ease of use, or perfor-

140
mance are most important. Spark + Python was (in my opinion) the most attractive
platform simply in terms of ease-of-coding, though it was challenging to get it to work
on the larger and more complicated problems. GraphLab was fast on smaller prob-
lems, but it did not scale well. This might be because none of the models I considered
naturally map to a graph. Giraph, on the other hand, which is really more of a BSP
platform than a graph platform, did very well, though memory was an issue on the
largest problems. SimSQL was most attractive on the largest and most complicated
problems.
Finally, the problems I studied (by design) tended to skew to the more compli-
cated end of the spectrum: hierarchical Bayesian models considered are more complex
than least-squares linear regression or k-means, which are the ML methods used most
widely in practice. The models we considered require a lot of CPU and data movemen-
t/communication. Had I considered simpler problems, or those that map naturally to
a graph (for example, labeling the nodes in a Markov random ﬁeld where the model
parameters are already known), the results might have been diﬀerent.

141
Chapter 5
Conclusion
In this chapter, I ﬁrst summarize the contributions of this work, then describe some
learnt lessons, and ﬁnally indicate some potential future work following this disserta-
tion.
5.1
Contributions
This dissertation makes the following contributions to scale Bayesian ML to very large
datasets:
PGRF. A pairwise Gaussian random ﬁeld (PGRF) model, the ﬁrst graphical and
factor-based model for high dimensional data imputation is proposed. PGRF is a
product of the compromise between statistical properties and computational eﬃ-
ciency in the case of large scale datasets. PGRF has advantages over the Gaussian
Markov random ﬁeld (GMRF) model and high dimensional Gaussian distribution
for its computation eﬃciency, and over the mean imputation for its higher accuracy.
My results show that the PGRF followed by the linear regression (LR) or support
vector machine (SVM) reduces the RMSE by 10% to 45% compared with the mean
imputation followed by the LR or SVM. It scales the imputation to datasets with 104
dimensions and 106 data points that could not be handled by GMRF or Gaussian
methods. The paper describing this in detail is published in ICDM ’2013 [19].
SimSQL.
SimSQL represents one of the most novel techniques for large scale

142
Bayesian ML. SimSQL allows programmers to specify their simulations with hun-
dreds of lines of SQL code, and automatically optimizes and parallelizes the running
of such programs reliably in the cluster. Our idea signiﬁcantly enlarges the power
of relational databases, but also frees analysts from a heavy workload of writing, de-
bugging, and deploying their programs. All of such functionalities are achieved by
extending the MCDB uncertainty database with the recursive stochastic tables and
user-deﬁned “VG functions.” All the SimSQL functionalities described in this disser-
tation are implemented and published in [12], and its current version V0.3 is released
under the Apache license [52].
Benchmark. An objective benchmark that compares SimSQL with Giraph, GraphLab
and Spark on ﬁve general Bayesian machine learning algorithms is provided. The
benchmark shows that SimSQL has the best programmability, while its performance
is competitive, and in some cases superior to the other platforms for very large scale
Bayesian ML. To run a general Bayesian ML model, SimSQL takes 1X less code than
Spark, 6X less than GraphLab, and 12X less code than Giraph, while its time cost
is within 5X slowdown in the worst case compared with the memory-based systems
such as Giraph and GraphLab. The results provide insights for data engineers who
need to select a platform and implement their ML code. The paper about this is
published in [20].
5.2
Lessons Learned
Computation matters. Many Bayesian models have nice statistical properties but
are computationally hard to learn. Examples include the Gaussian distribution and
kernel methods. The problem can be addressed in diﬀerent layers. One solution is
to use higher performance hardware such as more CPUs and larger memory; another

143
solution is to optimize the performance using some system techniques such as logical
and physical optimizations; a third one is to accelerate the inference by using some
statistical or mathematical methods; the last one is to weaken the model by adding
more assumptions or to change the model in some cases.
Practically, these four
methods are used jointly and frequently.
Combining subtasks into a joint task helps. Many ML problems can be divided
into multiple stages, and diﬀerent models are applied accordingly. If a joint model is
proposed that links diﬀerent stages, the performance is generally better than treating
diﬀerent subproblems separately. Our PGRF is an example to show that combining
the imputation and learning of the predictive model together gives more accuracy for
both the imputation and prediction.
Human eﬀort is expensive.
ML experts put much emphasis on performance
including statistical properties as well as the time cost for inference. However, human
eﬀorts are generally neglected. For example, a 5X slowdown in SimSQL can be much
more beneﬁcial than a distributed Java or C version that requires several hundreds
of times as much code, much more development time and a much higher level of
programming expertise.
Data independence beneﬁts us. As a database design principle, data indepen-
dence has been proved to be valuable for 30 years. It means that in system design,
each higher level of functionalities is immune to changes of the lower level of archi-
tectures. With data independence, backends can be changed between a single com-
putation and distributed computation or between CPU and GPU, while the higher
query interfaces or languages stay the same; new physical or logical optimization
strategies can be added as plug-ins; users are liberated from the physical or logical
technique details to focus on the applications. However, this is not accepted by the

144
ML practitioners, e.g., people wrote hundreds of ML papers on GPU in ICML or
KDD conferences. SimSQL proves that with a few changes, relational databases are
ideal platforms for large scale Bayesian ML.
Combining human with machine intelligence works. The concept of machine
learning means that machines could learn information or knowledge from data. Ma-
chines are better at redundant work, such as managing and searching information in
the data. However, with a bit of creative help from humans, the machine could work
better. For example, super-vertex based implementation for LDA has better perfor-
mance than “natural” word-based LDA; artiﬁcially selecting k random data points
as centroids accelerates the Gibbs sampler better than initializing the centroids from
hyper distributions.
No single platform addresses all of the ML problems. Our benchmark demon-
strates that the “correct” platform choice is highly problem- and user-dependent,
depending on whether ease of implementation, ease of use, or performance are most
important. For example, a graph platform such as Giraph or GraphLab may be more
appropriate for strongly correlated graphical models such as the Markov random
ﬁeld; Spark Python is attractive for data engineers who prefer Python programming
to SQL. Further, the large scale ML problem still does not have a ﬁnal solution.
Depending on applications and users, building new platforms is still on the way.
5.3
Future Work
Despite the fact that many algorithms/platforms have been proposed, large scale
machine learning is still at its early stage.
This section describes some potential
future work in this area that could extend this dissertation.
Applying Bayesian ML to domain-speciﬁc problems.
As shown in chapter

145
2, the Bayesian methods such as PGRF successfully address practical large scale
domain-speciﬁc problems. Such problems widely exist in business intelligence, online
social networks/advertising, geo-social networks, network optimization, privacy and
security, e-commerce and crowdsourcing systems. Applying the Bayesian methods to
such domain problems continues to be promising.
SimSQL can be faster. As indicated by chapter 4, SimSQL does not perform well
on some cases, e.g., the Bayesian Lasso and LDA. This problem can be addressed
by some system-oriented techniques. For example, one ongoing project with SimSQL
is to add native support for vectors and matrices all through its architecture. It is
expected that with this support, SimSQL could be 10X faster for running a group
of ML problems where matrices and vectors are used to represent the data. Another
potential extension is to add non-MapReduce physical operators for small scale data.
When the optimizer detects that the dataset is small, the engine will run such physical
operators. This method makes SimSQL faster by reducing time in launching and
cleaning MapReduce jobs.
SimSQL can be more friendly. SimSQL currently supports the SQL query in-
terface. However, it is easy to add other interfaces by compiling them into SQL or
Prolog-based logical plans. For example, one ongoing project is to design a BUGS-
like language to specify the Markov chain. Furthermore, new language features can
be added. One project is to add the union operator, so that SimSQL could support
strongly correlated models such as linear regression and matchbox [171].
As described earlier, SimSQL has been released under the Apache license. With
more people to use this system, I expect that more research interests and system
challenges will appear, and more extensions will be added in the future.

146
Bibliography
[1] Y. Low, J. Gonzalez, A. Kyrola, D. Bickson, C. Guestrin, and J. M. Hellerstein,
“Graphlab: A new framework for parallel machine learning,” in UAI, pp. 340–
349, 2010.
[2] J. E. Gonzalez, Y. Low, H. Gu, D. Bickson, and C. Guestrin, “Powergraph: Dis-
tributed graph-parallel computation on natural graphs,” in Proceedings of the
10th USENIX Conference on Operating Systems Design and Implementation,
OSDI’12, (Berkeley, CA, USA), pp. 17–30, USENIX Association, 2012.
[3] H. Rue and L. Held, Gaussian Markov Random Fields: Theory and Applica-
tions (Chapman & Hall/CRC Monographs on Statistics & Applied Probability).
Chapman and Hall/CRC, 1 ed., Feb. 2005.
[4] S. Ghemawat, H. Gobioﬀ, and S.-T. Leung, “The google ﬁle system,” in SOSP,
pp. 29–43, 2003.
[5] Hadoop.
Open
Source
Implementation
of
MapReduce.
http://hadoop.apache.org/.
[6] F. Chang, J. Dean, S. Ghemawat, W. C. Hsieh, D. A. Wallach, M. Burrows,
T. Chandra, A. Fikes, and R. E. Gruber, “Bigtable: A distributed storage
system for structured data,” in Proceedings of the 7th USENIX Symposium on
Operating Systems Design and Implementation - Volume 7, OSDI ’06, (Berkeley,
CA, USA), pp. 15–15, USENIX Association, 2006.

147
[7] HBASE. https://hbase.apache.org/.
[8] J. Dean and S. Ghemawat, “Mapreduce: simpliﬁed data processing on large
clusters,” in OSDI04: PROCEEDINGS OF THE 6TH CONFERENCE ON
SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTA-
TION, USENIX Association, 2004.
[9] R. Pike, S. Dorward, R. Griesemer, and S. Quinlan, “Interpreting the data:
Parallel analysis with sawzall,” Sci. Program., vol. 13, pp. 277–298, Oct. 2005.
[10] A. Thusoo, J. S. Sarma, N. Jain, Z. Shao, P. Chakka, S. Anthony, H. Liu,
P. Wyckoﬀ, and R. Murthy, “Hive: A warehousing solution over a map-reduce
framework,” Proc. VLDB Endow., vol. 2, pp. 1626–1629, Aug. 2009.
[11] M. Zaharia, M. Chowdhury, T. Das, A. Dave, J. Ma, M. McCauley, M. J.
Franklin, S. Shenker, and I. Stoica, “Resilient distributed datasets: A fault-
tolerant abstraction for in-memory cluster computing,” in Proceedings of the
9th USENIX Conference on Networked Systems Design and Implementation,
NSDI’12, (Berkeley, CA, USA), pp. 2–2, USENIX Association, 2012.
[12] Z. Cai, Z. Vagena, L. Perez, S. Arumugam, P. J. Haas, and C. Jermaine, “Sim-
ulation of database-valued markov chains using simsql,” in Proceedings of the
2013 ACM SIGMOD International Conference on Management of Data, SIG-
MOD ’13, (New York, NY, USA), pp. 637–648, ACM, 2013.
[13] Giraph, https://giraph.apache.org/.
[14] A. K. Sujeeth, H. Lee, K. J. Brown, H. Chaﬁ, M. Wu, A. R. Atreya, K. Oluko-
tun, T. Rompf, and M. Odersky, “Optiml: an implicitly parallel domainspeciﬁc

148
language for machine learning,” in in Proceedings of the 28th International Con-
ference on Machine Learning, ser. ICML, 2011.
[15] C. M. Bishop, “Pattern recognition and machine learning,” in Springer, pp. 179–
220, 2006.
[16] N. S. Altman, “An Introduction to Kernel and Nearest-Neighbor Nonparametric
Regression,” The American Statistician, vol. 46, no. 3, pp. 175–185, 1992.
[17] G. Salton, A. Wong, and C. S. Yang, “A vector space model for automatic
indexing,” Commun. ACM, vol. 18, pp. 613–620, Nov. 1975.
[18] B. Efron, “Why Isn’t Everyone a Bayesian?,” The American Statistician,
vol. 40, pp. 1–5, Feb. 1986.
[19] Z. Cai, C. M. Jermaine, Z. Vagena, D. Logothetis, and L. L. Perez, “The pair-
wise gaussian random ﬁeld for high-dimensional data imputation,” in ICDM,
pp. 61–70, 2013.
[20] Z. Cai, Z. Gao, S. Luo, L. Perez, Z. Vagena, and C. Jermaine, “A comparison
of platforms for implementing and running very large scale machine learning al-
gorithms,” in Proceedings of the 2014 ACM SIGMOD International Conference
on Management of Data, SIGMOD ’14, ACM, 2014.
[21] D. Deutch, C. Koch, and T. Milo, “On probabilistic ﬁxpoint and markov chain
query languages,” in Proceedings of the Twenty-ninth ACM SIGMOD-SIGACT-
SIGART Symposium on Principles of Database Systems, PODS ’10, (New York,
NY, USA), pp. 215–226, ACM, 2010.

149
[22] R. J. A. Little and D. B. Rubin, Statistical Analysis with Missing Data. New
York, NY, USA: John Wiley & Sons, Inc., 1986.
[23] D. B. Rubin, Multiple Imputation for Nonresponse in Surveys. Wiley, 1987.
[24] J. L. Schafer, Analysis of incomplete multivariate data. Monographs on statistics
and applied probability ; 72., Chapman & Hall, 1997.
[25] P. Lane, “Handling drop-out in longitudinal clinical trials: a comparison of
the LOCF and MMRM approaches.,” Pharmaceutical statistics, vol. 7, no. 2,
pp. 93–106, 2008.
[26] J. L. Schafer and J. W. Graham, “Missing data: our view of the state of the
art,” Psychological Methods, vol. 7, pp. 147–177, 2002.
[27] J. L. Schafer and N. Schenker, “Inference with Imputed Conditional Means,”
Journal of the American Statistical Association, vol. 95, pp. 144–154, 2000.
[28] C. K. Enders, Applied Missing Data Analysis. New York: Guilford Press, 2010.
[29] B. Muth´en, T. Asparouhov, A. M. Hunter, and A. F. Leuchter, “Growth mod-
eling with nonignorable dropout: alternative analyses of the STAR*D antide-
pressant trial.,” Psychological methods, vol. 16, pp. 17–33, Mar. 2011.
[30] R. L. Brown, “Eﬃcacy of the indirect approach for estimating structural equa-
tion models with missing data: A comparison of ﬁve methods,” Structural Equa-
tion Modeling, vol. 1, pp. 287–316, 1994.
[31] C. M. Bishop, “Pattern recognition and machine learning,” in Springer, pp. 78–
113, 2006.

150
[32] The R Project for Statistical Computing, http://www.r-project.org/.
[33] The Language of Technical Computing, http://www.mathworks.com/.
[34] SAS Software, http://www.sas.com.
[35] S. Das, Y. Sismanis, K. S. Beyer, R. Gemulla, P. J. Haas, and J. McPherson,
“Ricardo: integrating r and hadoop,” in SIGMOD Conference, pp. 987–998,
2010.
[36] L. Tierney, A. J. Rossini, N. Li, and H. Sevcikova, “snow: Simple network of
workstations,” in http://cran.r-project.org/web/packages/snow/.
[37] Apache mahout, http://mahout.apache.org/.
[38] U. Kang, C. E. Tsourakakis, and C. Faloutsos, “Pegasus: A peta-scale graph
mining system implementation and observations,” in Proceedings of the 2009
Ninth IEEE International Conference on Data Mining, ICDM ’09, (Washington,
DC, USA), pp. 229–238, IEEE Computer Society, 2009.
[39] B. Catanzaro, N. Sundaram, and K. Keutzer, “Fast support vector machine
training and classiﬁcation on graphics processors,” in Proceedings of the 25th
International Conference on Machine Learning, ICML ’08, (New York, NY,
USA), pp. 104–111, ACM, 2008.
[40] C. T. Chu, S. K. Kim, Y. A. Lin, Y. Yu, G. R. Bradski, A. Y. Ng, and K. Oluko-
tun, “Map-reduce for machine learning on multicore,” in NIPS (B. Sch¨olkopf,
J. C. Platt, and T. Hoﬀman, eds.), pp. 281–288, MIT Press, 2006.
[41] H. P. Graf, E. Cosatto, L. Bottou, I. Durdanovic, and V. Vapnik, “Parallel
support vector machines: The cascade svm,” in NIPS, 2004.

151
[42] E. Y. Chang, H. Bai, and K. Zhu, “Parallel algorithms for mining large-scale
rich-media data,” in Proceedings of the 17th ACM International Conference on
Multimedia, MM ’09, (New York, NY, USA), pp. 917–918, ACM, 2009.
[43] J. M. Hellerstein, C. R´e, F. Schoppmann, D. Z. Wang, E. Fratkin, A. Gorajek,
K. S. Ng, C. Welton, X. Feng, K. Li, and A. Kumar, “The madlib analytics
library: Or mad skills, the sql,” Proc. VLDB Endow., vol. 5, pp. 1700–1711,
Aug. 2012.
[44] A. Abouzeid, K. Bajda-Pawlikowski, D. Abadi, A. Silberschatz, and A. Rasin,
“Hadoopdb: An architectural hybrid of mapreduce and dbms technologies for
analytical workloads,” Proc. VLDB Endow., vol. 2, pp. 922–933, Aug. 2009.
[45] B. Chattopadhyay, L. Lin, W. Liu, S. Mittal, P. Aragonda, V. Lychagina,
Y. Kwon, and M. Wong, “Tenzing a sql implementation on the mapreduce
framework,” PVLDB, vol. 4, no. 12, pp. 1318–1327, 2011.
[46] J. Zhou, N. Bruno, M.-C. Wu, P.-˚A. Larson, R. Chaiken, and D. Shakib, “Scope:
parallel databases meet mapreduce,” VLDB J., vol. 21, no. 5, pp. 611–636, 2012.
[47] S. Chen, “Cheetah: A high performance, custom data warehouse on top of
mapreduce,” Proc. VLDB Endow., vol. 3, pp. 1459–1468, Sept. 2010.
[48] R. Jampani, F. Xu, M. Wu, L. L. Perez, C. Jermaine, and P. J. Haas, “Mcdb: A
monte carlo approach to managing uncertain data,” in Proceedings of the 2008
ACM SIGMOD International Conference on Management of Data, SIGMOD
’08, (New York, NY, USA), pp. 687–700, ACM, 2008.
[49] N. R. Alur, P. J. Haas, D. Momiroska, P. Read, N. H. Summers, V. Totanes,

152
, and C. Zuzarte, “Db2 udbs high function business intelligence in e-business,
ibm redbook series,” 2002.
[50] J. Cohen, B. Dolan, M. Dunlap, J. M. Hellerstein, and C. Welton, “Mad skills:
New analysis practices for big data,” PVLDB, vol. 2, no. 2, pp. 1481–1492,
2009.
[51] D. M. Blei, A. Y. Ng, and M. I. Jordan, “Latent dirichlet allocation,” in NIPS,
pp. 601–608, 2001.
[52] SimSQL, http://cmj4.web.rice.edu/SimSQL/SimSQL.html.
[53] TPCH, http://www.tpc.org/tpch/default.asp.
[54] A. Ghoting, R. Krishnamurthy, E. Pednault, B. Reinwald, V. Sindhwani,
S. Tatikonda, Y. Tian, and S. Vaithyanathan, “Systemml: Declarative machine
learning on mapreduce,” in Proceedings of the 2011 IEEE 27th International
Conference on Data Engineering, ICDE ’11, (Washington, DC, USA), pp. 231–
242, IEEE Computer Society, 2011.
[55] S. Seo, E. J. Yoon, J. Kim, S. Jin, J.-S. Kim, and S. Maeng, “Hama: An eﬃ-
cient matrix computation with the mapreduce framework,” in Proceedings of the
2010 IEEE Second International Conference on Cloud Computing Technology
and Science, CLOUDCOM ’10, (Washington, DC, USA), pp. 721–726, IEEE
Computer Society, 2010.
[56] J. H. Aldrich and F. D. Nelson, Linear Probability, Logit, and Probit Models.
Newbury Park, CA: Sage Publications, 1984.

153
[57] H. Rue, “Fast sampling of gaussian markov random ﬁelds,” Journal of the Royal
Statistical Society: Series B (Statistical Methodology), vol. 63, no. 2, pp. 325–
338, 2001.
[58] C.-J. Hsieh, M. A. Sustik, I. S. Dhillon, and P. Ravikumar, “Sparse inverse co-
variance matrix estimation using quadratic approximation,” Advances in Neural
Information Processing Systems, vol. 24, pp. 2330–2338, 2011.
[59] C.-J. Hsieh, I. Dhillon, P. Ravikumar, and A. Banerjee, “A divide-and-conquer
method for sparse inverse covariance estimation,” in NIPS 25, pp. 2339–2347,
2012.
[60] M. Kolar and E. P. Xing, “Consistent covariance selection from data with miss-
ing values.,” in ICML, icml.cc / Omnipress, 2012.
[61] J. Friedman, T. Hastie, and R. Tibshirani, “Sparse inverse covariance estimation
with the graphical lasso.,” Biostatistics (Oxford, England), vol. 9, pp. 432–441,
July 2008.
[62] A. P. Dempster, N. M. Laird, and D. B. Rubin, “Maximum likelihood from
incomplete data via the em algorithm,” JOURNAL OF THE ROYAL STATIS-
TICAL SOCIETY, SERIES B, vol. 39, no. 1, pp. 1–38, 1977.
[63] O. J. Dunn and V. Clark, “Correlation coeﬃcients measured on the same in-
dividuals,” Journal of the American Statistical Association, vol. 64, no. 325,
pp. 366–377, 1969.
[64] F. Chatelain, J.-Y. Tourneret, J. Inglada, and A. Ferrari, “Bivariate gamma
distributions for image registration and change detection.,” IEEE Transactions
on Image Processing, vol. 16, no. 7, pp. 1796–1806, 2007.

154
[65] J. Gonzalez, Y. Low, A. Gretton, and C. Guestrin, “Parallel gibbs sampling:
From colored ﬁelds to thin junction trees.,” in AISTATS (G. J. Gordon, D. B.
Dunson, and M. Dudk, eds.), vol. 15 of JMLR Proceedings, pp. 324–332,
JMLR.org, 2011.
[66] T. Park and G. Casella, “The bayesian lasso,” JASA, vol. 103, no. 482, pp. 681–
686, 2008.
[67] C. K. Enders, “Analyzing longitudinal data with missing values.,” Rehabilita-
tion psychology, vol. 56, pp. 267–288, Nov. 2011.
[68] D. B. Rubin, “Inference and missing data,” Biometrika, vol. 63, pp. 581–590,
1976.
[69] J. L. Peugh and C. K. Enders, “Missing Data in Educational Research: A
Review of Reporting Practices and Suggestions for Improvement,” REVIEW
OF EDUCATIONAL RESEARCH, vol. 74, pp. 525–556, Jan. 2004.
[70] W. Wothke, “Longitudinal and multi-group modeling with missing data,” in
Modeling longitudinal and multilevel data:
Practical issues, applied approaches, and speciﬁc examples (T. D. Little, K. U.
Schnabel, and J. Baumert, eds.), Mahwah, NJ:
Lawrence Erlbaum Associates, 2000.
[71] J. W. Graham, “Maximizing the usefulness of data obtained with planned miss-
ing value patterns: An application of maximum likelihood procedures,” Multi-
variate Behavioral Research, vol. 31, pp. 197–218, 1996.
[72] F. Harrell, Regression Modeling Strategies. Springer, corrected ed., Jan. 2001.

155
[73] A. M. Wood, I. R. White, and S. G. Thompson, “Are missing outcome data ad-
equately handled? A review of published randomized controlled trials in major
medical journals.,” Clinical trials (London, England), vol. 1, no. 4, pp. 368–376,
2004.
[74] W. G. Madow, H. Nisselson, and I. Olkin, Imcomplete data in sample surveys,
vol. 1. Academic Press, 1983.
[75] G. Liu and A. L. Gould, “Comparison of alternative strategies for analysis
of longitudinal trials with dropouts.,” Journal of Biopharmaceutical Statistics,
vol. 12, pp. 207–226, 2002.
[76] R. R. Andridge and R. J. A. Little, “A Review of Hot Deck Imputation for
Survey Non-response,” International Statistical Review, vol. 78, pp. 40–64, Apr.
2010.
[77] G. McLachlan and T. Krishnan, The EM algorithm and extensions. Wiley series
in probability and statistics, Wiley, 2. ed ed., 2008.
[78] W. R. Gilks, S. Richardson, and D. Spiegelhalter, Markov Chain Monte Carlo
in practice: interdisciplinary statistics, vol. 2. Chapman & Hall/CRC, 1995.
[79] R. I. Jennrich and M. D. Schluchter, “Unbalanced Repeated-Measures models
with structured covariance matrices,” Biometrics, vol. 42, no. 4, pp. 805–820,
1986.
[80] N. M. Laird and J. H. Ware, “Random-eﬀects models for longitudinal data.,”
Biometrics, vol. 38, pp. 963–974, Dec. 1982.

156
[81] C. C. Clogg and L. A. Goodman, “Latent structure analysis of a set of mul-
tidimensional contingency tables.,” J. Am. Stat. Assoc., vol. 79, pp. 762–771,
1984.
[82] D. Rubin and D. Thayer, “EM algorithms for ML factor analysis,” Psychome-
trika, vol. 47, pp. 69–76, Mar. 1982.
[83] D. Titterington, A. Smith, and E. Makov, Statistical Analysis of Finite Mixture
Distributions. Chichester: Wiley, 1985.
[84] D. R. Cox and D. V. Hinkley, Theoretical Statistics. London: Chapman & Hall,
1974.
[85] A. Satorra and P. M. Bentler, “Corrections to test statistics and standard errors
in covariance structure analysis,” in Latent variables analysis (A. von Eye and
C. C. Clogg, eds.), pp. 399–419, Thousands Oaks, CA: Sage, 1994.
[86] L. M. Collins, J. L. Schafer, and C. M. Kam, “A comparison of inclusive and re-
strictive strategies in modern missing-data procedures,” Psychological Methods,
vol. 6, pp. 330–351, 2001.
[87] K. H. Yuan and P. M. Bentler, “Three Likelihood-Based Methods for Mean
and Covariance Structure Analysis with Nonnormal Missing Data,” Sociological
Methodology, vol. 30, pp. 165–200, 2000.
[88] S. L. Zeger, K.-Y. Liang, and P. S. Albert, “Models for Longitudinal Data: A
Generalized Estimating Equation Approach,” Biometrics, vol. 44, pp. 1049–
1060, Dec. 1988.

157
[89] R. J. Glynn, N. M. Laird, and D. B. Rubin, “Multiple Imputation in Mixture
Models for Nonignorable Nonresponse with Follow-ups,” Journal of the Amer-
ican Statistical Association, vol. 88, pp. 984–993, 1993.
[90] D. Hedeker and R. D. Gibbons, “Applications of randomeﬀects pattern-mixture
models for missing data in longitudinal studies. psychol methods,” 1997.
[91] K. A. Bollen and P. J. Curran, A Structural Equation Perspective. Mathematics,
Jan. 2006.
[92] B. Rolfs, B. Rajaratnam, D. Guillot, I. Wong, and A. Maleki, “Iterative thresh-
olding algorithm for sparse inverse covariance estimation,” in Advances in Neu-
ral Information Processing Systems 25, pp. 1583–1591, 2012.
[93] P.-L. Loh and M. J. Wainwright, “High-dimensional regression with noisy and
missing data: Provable guarantees with non-convexity,” in NIPS, pp. 2726–
2734, 2011.
[94] I. Porteous, D. Newman, A. Ihler, A. Asuncion, P. Smyth, and M. Welling,
“Fast collapsed gibbs sampling for latent dirichlet allocation,” in Proceedings
of the 14th ACM SIGKDD International Conference on Knowledge Discovery
and Data Mining, KDD ’08, (New York, NY, USA), pp. 569–577, ACM, 2008.
[95] L. Lov´asz, “Random walks on graphs: A survey,” in Combinatorics, Paul Erd˝os
is Eighty (D. Mikl´os, V. T. S´os, and T. Sz˝onyi, eds.), vol. 2, pp. 353–398,
Budapest: J´anos Bolyai Mathematical Society, 1996.
[96] Y. E. Ioannidis and S. Christodoulakis, “Optimal histograms for limiting worst-
case error propagation in the size of join results,” ACM Trans. Database Syst.,
vol. 18, pp. 709–748, Dec. 1993.

158
[97] T. Cormen, C. Leiserson, R. Rivest, and C. Stein, Introduction to Algorithms.
McGraw-Hill, 2003.
[98] DB2, http://www-01.ibm.com/software/data/db2/.
[99] M. Elhemali, C. A. Galindo-Legaria, T. Grabs, and M. M. Joshi, “Execution
strategies for sql subqueries,” in Proceedings of the 2007 ACM SIGMOD Inter-
national Conference on Management of Data, SIGMOD ’07, (New York, NY,
USA), pp. 993–1004, ACM, 2007.
[100] W. Kim, “On optimizing an sql-like nested query,” ACM Trans. Database Syst.,
vol. 7, pp. 443–469, Sept. 1982.
[101] R. A. Ganski and H. K. T. Wong, “Optimization of nested sql queries revisited.,”
in SIGMOD Conference (U. Dayal and I. L. Traiger, eds.), pp. 23–33, ACM
Press, 1987.
[102] U. Dayal, “Of nests and trees: A uniﬁed approach to processing queries that
contain nested subqueries, aggregates, and quantiﬁers,” in VLDB, pp. 197–208,
1987.
[103] L.-Y. Yuan, ed., 18th International Conference on Very Large Data Bases, Au-
gust 23-27, 1992, Vancouver, Canada, Proceedings, Morgan Kaufmann, 1992.
[104] P. Seshadri, H. Pirahesh, and T. Y. C. Leung, “Complex query decorrelation,”
in Proceedings of the Twelfth International Conference on Data Engineering,
ICDE ’96, (Washington, DC, USA), pp. 450–458, IEEE Computer Society, 1996.
[105] C. Zuzarte, H. Pirahesh, W. Ma, Q. Cheng, L. Liu, and K. Wong, “Winmagic:
Subquery elimination using window aggregation,” in Proceedings of the 2003

159
ACM SIGMOD International Conference on Management of Data, SIGMOD
’03, (New York, NY, USA), pp. 652–656, ACM, 2003.
[106] S. Bellamkonda, R. Ahmed, A. Witkowski, A. Amor, M. Zait, and C.-C. Lin,
“Enhanced subquery optimizations in oracle,” Proc. VLDB Endow., vol. 2,
pp. 1366–1377, Aug. 2009.
[107] C. A. Galindo-Legaria and M. Joshi, “Orthogonal optimization of subqueries
and aggregation,” in SIGMOD Conference, pp. 571–581, 2001.
[108] A. Kemper, G. Moerkotte, K. Peithner, and M. Steinbrunn, “Optimizing dis-
junctive queries with expensive predicates,” SIGMOD Rec., vol. 23, pp. 336–
347, May 1994.
[109] M. Brantner, N. May, and G. Moerkotte, “Unnesting scalar sql queries in the
presence of disjunction.,” in ICDE (R. Chirkova, A. Dogac, M. T. zsu, and
T. K. Sellis, eds.), pp. 46–55, IEEE, 2007.
[110] J. Goldstein and P.-A. Larson, “Optimizing queries using materialized views:
A practical, scalable solution,” SIGMOD Rec., vol. 30, pp. 331–342, May 2001.
[111] T. Neumann, “Eﬃciently compiling eﬃcient query plans for modern hardware,”
Proc. VLDB Endow., vol. 4, pp. 539–550, June 2011.
[112] 20Newsgroups Dataset, “Available at: http://people.csail.mit.edu/jrennie/20newsgroups/.
[113] A. Smola and S. Narayanamurthy, “An architecture for parallel topic models,”
Proc. VLDB Endow., vol. 3, pp. 703–710, Sept. 2010.
[114] G. Malewicz, M. H. Austern, A. J. C. Bik, J. C. Dehnert, I. Horn, N. Leiser, and
G. Czajkowski, “Pregel: a system for large-scale graph processing,” in SIGMOD

160
Conference, pp. 135–146, 2010.
[115] B. Venners, “The making of python,” Artima Developer, 2007.
[116] J. Shen, J. Fang, H. J. Sips, and A. L. Varbanescu, “An application-centric
evaluation of opencl on multi-core cpus,” Parallel Computing, vol. 39, no. 12,
pp. 834–850, 2013.
[117] W. Gropp, E. L. Lusk, N. E. Doss, and A. Skjellum, “A high-performance,
portable implementation of the mpi message passing interface standard.,” Par-
allel Computing, vol. 22, no. 6, pp. 789–828, 1996.
[118] A. Amritkar, D. Tafti, R. Liu, R. Kufrin, and B. M. Chapman, “Openmp
parallelism for ﬂuid and ﬂuid-particulate systems,” Parallel Computing, vol. 38,
no. 9, pp. 501–517, 2012.
[119] M. Frigo, C. E. Leiserson, and K. H. Randall, “The implementation of the cilk-5
multithreaded language,” in PLDI, pp. 212–223, 1998.
[120] M. Schmidberger, M. Morgan, D. Eddelbuettel, H. Yu, L. Tierney, and U. Mans-
mann, “State of the art in parallel computing with r,” Journal of Statistical
Software, vol. 31, pp. 1–27, 8 2009.
[121] V. Kumar, Introduction to Parallel Computing. Boston, MA, USA: Addison-
Wesley Longman Publishing Co., Inc., 2nd ed., 2002.
[122] H. Garcia-Molina, J. D. Ullman, and J. Widom, Database Systems: The Com-
plete Book. Upper Saddle River, NJ, USA: Prentice Hall Press, 2 ed., 2008.
[123] E. F. Codd, “The capabilities of relational database management systems,”
IBM Research Report, San Jose, California, vol. RJ3132, 1981.

161
[124] Oracle, http://www.oracle.com/.
[125] History of MySQL, MySQL 5.1 Reference Manual. MySQL AB., 2011.
[126] E. F. Codd, “A relational model of data for large shared data banks,” Commun.
ACM, vol. 13, no. 6, pp. 377–387, 1970.
[127] D. D. Chamberlin, M. M. Astrahan, M. W. Blasgen, J. N. Gray, W. F. King,
B. G. Lindsay, R. Lorie, J. W. Mehl, T. G. Price, F. Putzolu, P. G. Selinger,
M. Schkolnick, D. R. Slutz, I. L. Traiger, B. W. Wade, and R. A. Yost, “A
history and evaluation of system r,” Commun. ACM, vol. 24, pp. 632–646, Oct.
1981.
[128] M. Isard, M. Budiu, Y. Yu, A. Birrell, and D. Fetterly, “Dryad: Distributed
data-parallel programs from sequential building blocks,” in Proceedings of the
2Nd ACM SIGOPS/EuroSys European Conference on Computer Systems 2007,
EuroSys ’07, (New York, NY, USA), pp. 59–72, ACM, 2007.
[129] K. Shvachko, H. Kuang, S. Radia, and R. Chansler, “The hadoop distributed
ﬁle system,” in Proceedings of the 2010 IEEE 26th Symposium on Mass Storage
Systems and Technologies (MSST), MSST ’10, (Washington, DC, USA), pp. 1–
10, IEEE Computer Society, 2010.
[130] C. Chambers, A. Raniwala, F. Perry, S. Adams, R. R. Henry, R. Bradshaw, and
N. Weizenbaum, “Flumejava: Easy, eﬃcient data-parallel pipelines,” SIGPLAN
Not., vol. 45, pp. 363–375, June 2010.
[131] Y. Yu, M. Isard, D. Fetterly, M. Budiu, U. Erlingsson, P. K. Gunda, and J. Cur-
rey, “Dryadlinq: A system for general-purpose distributed data-parallel com-

162
puting using a high-level language,” in Proceedings of the 8th USENIX Con-
ference on Operating Systems Design and Implementation, OSDI’08, (Berkeley,
CA, USA), pp. 1–14, USENIX Association, 2008.
[132] A. F. Gates, O. Natkovich, S. Chopra, P. Kamath, S. M. Narayanamurthy,
C. Olston, B. Reed, S. Srinivasan, and U. Srivastava, “Building a high-level
dataﬂow system on top of map-reduce: The pig experience,” Proc. VLDB En-
dow., vol. 2, pp. 1414–1425, Aug. 2009.
[133] K. S. Beyer, V. Ercegovac, R. Gemulla, A. Balmin, M. Y. Eltabakh, C.-C.
Kanne, F. zcan, and E. J. Shekita, “Jaql: A scripting language for large scale
semistructured data analysis.,” PVLDB, vol. 4, no. 12, pp. 1272–1283, 2011.
[134] A. Pavlo, E. Paulson, A. Rasin, D. J. Abadi, D. J. DeWitt, S. Madden, and
M. Stonebraker, “A comparison of approaches to large-scale data analysis,” in
Proceedings of the 2009 ACM SIGMOD International Conference on Manage-
ment of Data, SIGMOD ’09, (New York, NY, USA), pp. 165–178, ACM, 2009.
[135] S. Melnik, A. Gubarev, J. J. Long, G. Romer, S. Shivakumar, M. Tolton, and
T. Vassilakis, “Dremel: Interactive analysis of web-scale datasets,” Commun.
ACM, vol. 54, pp. 114–123, June 2011.
[136] A. Hall, O. Bachmann, R. B¨ussow, S. Ganceanu, and M. Nunkesser, “Processing
a trillion cells per mouse click,” PVLDB, vol. 5, no. 11, pp. 1436–1446, 2012.
[137] https://github.com/cloudera/impala.
[138] Q. Ke, M. Isard, and Y. Yu, “Optimus: A dynamic rewriting framework for
data-parallel execution plans,” in Proceedings of the 8th ACM European Con-

163
ference on Computer Systems, EuroSys ’13, (New York, NY, USA), pp. 15–28,
ACM, 2013.
[139] R. S. Xin, J. Rosen, M. Zaharia, M. J. Franklin, S. Shenker, and I. Stoica,
“Shark: Sql and rich analytics at scale,” in Proceedings of the 2013 ACM SIG-
MOD International Conference on Management of Data, SIGMOD ’13, (New
York, NY, USA), pp. 13–24, ACM, 2013.
[140] R. Avnur and J. M. Hellerstein, “Eddies: Continuously adaptive query process-
ing,” in Proceedings of the 2000 ACM SIGMOD International Conference on
Management of Data, May 16-18, 2000, Dallas, Texas, USA (W. Chen, J. F.
Naughton, and P. A. Bernstein, eds.), pp. 261–272, ACM, 2000.
[141] T. Urhan, M. J. Franklin, and L. Amsaleg, “Cost-based query scrambling for
initial delays,” SIGMOD Rec., vol. 27, pp. 130–141, June 1998.
[142] Y. E. Ioannidis, “Query optimization,” ACM Comput. Surv., vol. 28, no. 1,
pp. 121–123, 1996.
[143] N. Kabra and D. J. DeWitt, “Eﬃcient mid-query re-optimization of sub-optimal
query execution plans,” SIGMOD Rec., vol. 27, pp. 106–117, June 1998.
[144] S. Chaudhuri, “An overview of query optimization in relational systems,” in
Proceedings of the Seventeenth ACM SIGACT-SIGMOD-SIGART Symposium
on Principles of Database Systems, PODS ’98, (New York, NY, USA), pp. 34–
43, ACM, 1998.
[145] C. Chekuri, W. Hasan, and R. Motwani, “Scheduling problems in parallel
query optimization,” in Proceedings of the Fourteenth ACM SIGACT-SIGMOD-

164
SIGART Symposium on Principles of Database Systems, PODS ’95, (New York,
NY, USA), pp. 255–265, ACM, 1995.
[146] W. Hong, “Exploiting inter-operation parallelism in xprs,” SIGMOD Rec.,
vol. 21, pp. 19–28, June 1992.
[147] Y. Bu, B. Howe, M. Balazinska, and M. D. Ernst, “Haloop: Eﬃcient iterative
data processing on large clusters,” Proc. VLDB Endow., vol. 3, pp. 285–296,
Sept. 2010.
[148] N. T. Bao and T. Suzumura, “Towards highly scalable pregel-based graph pro-
cessing platform with x10,” in Proceedings of the 22Nd International Confer-
ence on World Wide Web Companion, WWW ’13 Companion, (Republic and
Canton of Geneva, Switzerland), pp. 501–508, International World Wide Web
Conferences Steering Committee, 2013.
[149] Z. Cai, D. Logothetis, and G. Siganos, “Facilitating real-time graph mining,” in
Proceedings of the Fourth International Workshop on Cloud Data Management,
CloudDB ’12, (New York, NY, USA), pp. 1–8, ACM, 2012.
[150] L. G. Valiant, “A bridging model for parallel computation,” Commun. ACM,
vol. 33, pp. 103–111, Aug. 1990.
[151] Y. Low, D. Bickson, J. Gonzalez, C. Guestrin, A. Kyrola, and J. M. Hellerstein,
“Distributed graphlab: A framework for machine learning and data mining in
the cloud,” Proc. VLDB Endow., vol. 5, pp. 716–727, Apr. 2012.
[152] J. Cho, H. Garcia-Molina, and L. Page, “Eﬃcient crawling through url order-
ing,” Comput. Netw. ISDN Syst., vol. 30, pp. 161–172, Apr. 1998.

165
[153] D. Gregor and A. Lumsdaine, “The parallel bgl: A generic library for dis-
tributed graph computations,” in In Parallel Object-Oriented Scientiﬁc Com-
puting (POOSC, 2005.
[154] R. Cheng, J. Hong, A. Kyrola, Y. Miao, X. Weng, M. Wu, F. Yang, L. Zhou,
F. Zhao, and E. Chen, “Kineograph: Taking the pulse of a fast-changing and
connected world,” in Proceedings of the 7th ACM European Conference on Com-
puter Systems, EuroSys ’12, (New York, NY, USA), pp. 85–98, ACM, 2012.
[155] R. S. Xin, J. E. Gonzalez, M. J. Franklin, and I. Stoica, “Graphx: A resilient
distributed graph system on spark,” in First International Workshop on Graph
Data Management Experiences and Systems, GRADES ’13, (New York, NY,
USA), pp. 2:1–2:6, ACM, 2013.
[156] S. Guha, “Rhipe:
R and hadoop integrated processing environment,” in
http://ml.stat.purdue.edu/rhipe.
[157] B. Huang, S. Babu, and J. Yang, “Cumulon: Optimizing statistical data anal-
ysis in the cloud,” in Proceedings of the 2013 ACM SIGMOD International
Conference on Management of Data, SIGMOD ’13, (New York, NY, USA),
pp. 1–12, ACM, 2013.
[158] M. Weimer, T. Condie, R. Ramakrishnan, et al., “Machine learning in scalops,
a higher order cloud computing language,” in BigLearn, vol. 9, pp. 389–396,
2011.
[159] T. Kraska, A. Talwalkar, J. C. Duchi, R. Griﬃth, M. J. Franklin, and M. I.
Jordan, “Mlbase: A distributed machine-learning system.,” in CIDR, 2013.

166
[160] V. Borkar, M. Carey, R. Grover, N. Onose, and R. Vernica, “Hyracks: A ﬂexi-
ble and extensible foundation for data-intensive computing,” in Proceedings of
the 2011 IEEE 27th International Conference on Data Engineering, ICDE ’11,
(Washington, DC, USA), pp. 1151–1162, IEEE Computer Society, 2011.
[161] MLlib, http://spark.apache.org/docs/latest/mllib-guide.html.
[162] T. J. Green and V. Tannen, “Models for incomplete and probabilistic informa-
tion,” in Proceedings of the 2006 International Conference on Current Trends
in Database Technology, EDBT’06, (Berlin, Heidelberg), pp. 278–296, Springer-
Verlag, 2006.
[163] C. Koch, “On query algebras for probabilistic databases,” SIGMOD Rec.,
vol. 37, pp. 78–85, Mar. 2009.
[164] M. Wick, A. McCallum, and G. Miklau, “Scalable probabilistic databases with
factor graphs and mcmc,” Proc. VLDB Endow., vol. 3, pp. 794–804, Sept. 2010.
[165] D. Z. Wang, M. J. Franklin, M. Garofalakis, J. M. Hellerstein, and M. L. Wick,
“Hybrid in-database inference for declarative information extraction,” in Pro-
ceedings of the 2011 ACM SIGMOD International Conference on Management
of Data, SIGMOD ’11, (New York, NY, USA), pp. 517–528, ACM, 2011.
[166] O. A. Kennedy and S. Nath, “Jigsaw: Eﬃcient optimization over uncertain
enterprise data,” in Proceedings of the 2011 ACM SIGMOD International Con-
ference on Management of Data, SIGMOD ’11, (New York, NY, USA), pp. 829–
840, ACM, 2011.
[167] D. G. Murray, F. McSherry, R. Isaacs, M. Isard, P. Barham, and M. Abadi,
“Naiad: A timely dataﬂow system,” in Proceedings of the Twenty-Fourth ACM

167
Symposium on Operating Systems Principles, SOSP ’13, (New York, NY, USA),
pp. 439–455, ACM, 2013.
[168] C. M. Bishop, “Pattern recognition and machine learning,” in Springer, pp. 240–
245, 2006.
[169] C. Ordonez and P. Cereghini, “Sqlem: Fast clustering in sql using the em
algorithm,” in Proceedings of the 2000 ACM SIGMOD International Conference
on Management of Data, SIGMOD ’00, (New York, NY, USA), pp. 559–570,
ACM, 2000.
[170] A.
McCallum,
“Mallet:
A
machine
learning
for
language
toolkit,”
http://mallet.cs.umass.edu/, 2002.
[171] D. H. Stern, R. Herbrich, and T. Graepel, “Matchbox:
large scale online
bayesian recommendations,” in WWW, pp. 111–120, 2009.

