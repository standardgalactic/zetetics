Applied Intelligence
https://doi.org/10.1007/s10489-022-04389-0
Unsupervised image-to-image translation via long-short
cycle-consistent adversarial networks
Gang Wang1
· Haibo Shi1 · Yufei Chen2 · Bin Wu3
Accepted: 5 December 2022
© The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2022
Abstract
Cycle consistency conducts generative adversarial networks from aligned image pairs to unpaired training sets and can
be applied to various image-to-image translations. However, the accumulation of errors that may occur during image
reconstruction can affect the realism and quality of the generated images. To address this, we exploit a novel long and
short cycle-consistent loss. This new loss is simple and easy to implement. Our dual-cycle constrained cross-domain image-
to-image translation method can handle error accumulation and enforce adversarial learning. When image information is
migrated from one domain to another, the cycle consistency-based image reconstruction constraint should be constrained
in both short and long cycles to eliminate error accumulation. We adopt the cascading manner with dual-cycle consistency,
where the reconstructed image in the first cycle can be cast as the new input to the next cycle. We show a distinct
improvement over baseline approaches in most translation scenarios. With extensive experiments on several datasets, the
proposed method is superior to several tested approaches.
Keywords GAN · Image-to-image translation · Dual learning · Cycle consistency
1 Introduction
Image-to-image translation aims to learn the mapping
between one visual domain to another and plays an
important role in many image processing and computer
vision applications, including image segmentation [54],
 Bin Wu
bwu@shufe-zj.edu.cn
 Gang Wang
gwang.cv@gmail.com
Haibo Shi
shihaibo@sufe.edu.cn
Yufei Chen
yufeichen@tongji.edu.cn
1
Institute of Data Science and Statistics, School of Statistics
and Management, Shanghai University of Finance and
Economics, Shanghai, 200433, China
2
CAD Research Center, College of Electronics and Information
Engineering, Tongji University, Shanghai, 201804, China
3
Shanghai University of Finance and Economics Zhejiang
College, Jinhua, 321013, China
anomaly detection [36], image super-resolution [48], neural
style transfer [14, 34], and medical image analysis [3, 46].
Traditional image-to-image translation approaches require
a large number of aligned training data sets. When paired
images are provided, the mapping model can be constructed
with a conditional generative model [13, 44] or a regression
method [20] in supervised learning. However, the paired
data is expensive to collect. To this end, numerous methods
have successfully translated images via cycle consistency
constraints [16, 47, 54] and shared latent space [25] in
unsupervised or unpaired settings without available paired
samples.
Due to the shape and texture information typically hav-
ing multiple variations across domains, previous approaches
achieve varying image translation results. Mapping local
texture can be implemented in the style transfer models,
however they fail to address large shape transformations.
Despite the tremendous results of existing generative adver-
sarial networks, the number of image-to-image translation
applications always encounters the issue of unrealistic arti-
facts. Although the hypothesis in cycle consistent GANs
is a one-to-one mapping between the input image domain
and the output visual domain, and it is enforced at a pixel
level without losing any information, we discovered that the
quality of the generated images is unsatisfactory since the

G. Wang et al.
learned models have been trained on a single cycle con-
straint, which is under-constrained. In other words, it is easy
to accumulate errors in the progress of image reconstruc-
tion, and the errors may impact the realism and quality of
the generated images. In addition, existing models cannot
obtain the expected translation results for both preserv-
ing and changing the shape without modifying the network
architecture and tuning the hyper-parameter settings. It is
worth noting that a common issue in recent methods is mode
collapse, where quite a few real images are represented in
the output.
This work focuses on two major problems in image-
to-image translation with the cycle-consistent adversarial
network when the reconstruction error accumulates in the
training. The first problem is the perceptual realism of
one-to-one mapping, and the second one is modeling the
training strategy to eliminate the mode collapse. To this
end, we customize an effective unsupervised image trans-
lation approach employing a Long-Short Cycle consistent
adversarial network for Image-to-image Translation (named
LSCIT). Precisely, the key idea is to present a dual-cycle
consistency constraint, which not only depends on the short
cycle consistency but also requires an additional long cycle
consistency in a dual learning framework. In the case of
the long-short cycle consistency, the output from the short
cycle part can be cast as a new augmented input into the
next long cycle. On the one hand, LSCIT with reconstruc-
tion loss achieved from the extra cycle can enforce the
training. On the other hand, dual-cycle consistency is self-
augmented since it feeds the inter result as the augmented
data to the next step. Figure 1 depicts the architectures with
single short-cycle consistency and the proposed approach
with long-short-cycle consistency. The proposed method
is inspired by cycle-consistent adversarial networks and
dual learning. Two image generators G and ˆG (e.g., label-
to-photo and photo-to-label) have been trained simultane-
ously by minimizing the reconstruction losses in both short
and long cycle consistency. Importantly, long-short cycle
consistency guides image translation by preventing gen-
erators from mode collapse and unrealistic hallucinations.
Moreover, we improve the implementation of our previous
short work [43] and provide more detailed experimental
analysis, such as albation studies, comparison experiments,
and model selection.
Our contributions are summarized as follows:
–
We propose a long-short cycle consistent loss, and
apply it to adversarial network for unsupervised or
unpaired image-to-image translation. Our dual-cycle
consistency constraint can restrain error accumulation
and guide the adversarial training.
–
We introduce a multi-step decay training strategy in the
long cycle consistency. It can be used to maintain the
stability in image-to-image translation.
–
We report a systematic comparison of our LSCIT
and other variants on perceptual realism. Extensive
experiments on image-to-image translation tasks show
that it is superior to several prior models.
The remaining parts of thispaper are organized as follows.
We review related work in Section 2 and present our
method in Section 3. Section 4 details the implementation
of the framework. The experimental results are reported in
Section 5. We conclude our work in the last section.
2 Related work
Generative Adversarial Networks (GANs) have emerged
for image generation tasks including Boltzmann machines
[37], auto-encoders [10, 42], and variational auto-encoder
(VAE) [18]. In recent years, the development of GANs
[7] has given a powerful push to the progress of image
synthesis. The key component of GANs is an adversarial
discriminator, which enforces the generated images to
be indistinguishable from real ones. The generator and
discriminator, formulated as a two-player mini-max game,
are trained simultaneously. A variety of works have been
developed to exploit the capability of GANs for a wide
range of image generation tasks. Soon after, conditional
generative adversarial networks (cGANs) [30] extend
Fig. 1 (a) Cycle consistency. (b) Long-short cycle consistency. Generators G : X →Y and ˆG : Y →X

Unsupervised image-to-image translation via long-short cycle-consistent...
GANs by using additional information such as class labels
to condition both generator and discriminator, and then
cGANs can generate specified images.
Supervised Image-to-Image Translation. The goal of
image-to-image translation is to learn a mapping from the
input domain to the target domain, where the supervised
image-to-image translation needs a training set that must
be aligned between input and output images. Several
vision tasks [21, 26] with paired samples can be solved
by supervised image-to-image translation approaches. The
pix2pix [13] framework is based on conditional generative
adversarial networks and combines the adversarial losses
using the L1 loss. To extend pix2pix to solve high-resolution
photo-realistic image generation, pix2pixHD [44] has been
developed to synthesize high-resolution visually desirable
outputs with conditional GANs. Moreover, BicycleGAN
[55] has been presented to learn multi-modal image-
to-image translation on paired data samples. Here, our
approach is unsupervised without aligning labels.
Unsupervised Image-to-Image Translation. Generally, it
is expensive to construct paired images for supervised
image-to-image translation. To address the limitation,
several approaches have been proposed to relate the
input and output domains (e.g., X and Y in Fig. 1)
without aligning. CycleGAN [54] builds on the pix2pix
method and models an unpaired image translation approach
with cycle consistency. Meanwhile, DiscoGAN [16],
DualGAN [47], and UNIT [25] develop the same idea for
two cross-domain image-to-image translations in different
viewpoints. To generate multi-modal outputs, MUNIT [12]
and DRIT++ [22] are designed to learn diverse image
translations. To get the attentional image information,
AttentionGAN [29] and UGATIT [15] guide the generative
model to focus on more interesting regions with the
attention module. In particular, attention-guided image-
to-image translation introduces unsupervised attention to
individual instances without translating the background
to guide the adversarial training of the generators and
discriminators. Similarly, instance-aware GAN [31] is used
to incorporate the instance content and improve multi-
instance transfiguration. In addition, StarGAN [4] and
ComboGAN [1] have been introduced for multi-domain
image-to-image translation, and they can decrease the
size of all generators across multiple domains. MCGAN
[35] introduces an end-to-end multi-constraint generative
adversarial model for unpaired image-to-image translation.
An energy-based model in the latent space of a pre-trained
auto-encoder is used in LETIT [51]. Use of contrastive
learning enables CUT [33] to address the unpaired image-
to-image translation issue. FSeSim [52] presents a self-
supervised learning approach to explicitly learn spatially-
correlative maps for various image-to-image translations.
DiscoGAN, DualGAN, MUNIT, DRIT++, StarGAN, and
UGATIT are cycle-consistency-based models, while CUT
and FSeSim use a one-sided framework without cycle-
consistency. Instead of pursuing diversity, in this work, our
approach aims to learn as-realistic-as-possible image-to-
image translation from the unpaired training set.
Dual learning [8], like cycle consistency, aims to leverage
monolingual data in machine translation more effectively,
where monolingual data can be cast as parallel bilingual
data, similar to paired training data in image-to-image
translation. The assumption in dual learning is that any
translation task consists of a dual-task, i.e., the primal-
task (e.g., photo-to-label translation) and the dual-task
(e.g., label-to-photo translation). In general, the above two
tasks behave as a loop to estimate the unpaired sentences.
Then, Luo et al. [27] extended dual learning to semantic
image segmentation. Inspired by this work, DualGAN [47]
introduces dual learning to learn a mapping from an input
image to an output image, where two GANs, i.e., primal-
GAN and dual-GAN, are designed for the primal-task and
the dual-task. Our method is also inspired by dual learning.
Unlike the above methods, our framework not only contains
primal and dual tasks but also considers additional serial
processing.
Cycle consistency is widely used in computer vision
tasks such as matching and alignment [11, 49, 53], and its
core idea is to apply transitivity as a constraint to regularize
structured data. Importantly, CycleGAN [54] introduces a
cycle consistency loss to force both generators GX→Y and
ˆGY→X, as shown in Fig. 1(a), to be consistent with each
other. In general, the cycle-consistent loss in domain X
and domain Y can be calculated by L1 loss ∥ˆG(G(x)) −
x∥1 and ∥G( ˆG(y)) −y∥1, respectively. Note that the idea
of cycle-consistent GAN is very similar to dual learning.
Cycle consistency constraints are widely used in computer
vision tasks [6, 23]. Precisely, cycle-in-cycle GAN has
been applied for image super-resolution [48] and keypoint-
guided image generation [38]. However, the origin cycle
consistency in CycleGAN has several problems, since the
pixel level cycle consistency causes unrealistic outputs.
One solution is that Nizan and Tal [32] take advantage of
collaboration between various GANs to break the reliance
on cycle consistency. While our approach builds on dual
learning with long-short cycle consistency, unlike cycle-in-
cycle GANs with more customized generators in the image
super-resolution framework, we use two generators, G and
ˆG, in both primal-task and dual-task.
3 Methodology
In this paper, we aim to train a mapping function Gx→y that
maps samples from a source image domain X to a target
image domain Y without using paired samples. As shown in

G. Wang et al.
Fig. 1(b), the proposed framework includes two generators
Gx→y and ˆGy→x and two discriminators DX and DY . With
long and short-consistent adversarial networks, we extend
vanilla cycle consistency to dual-cycle consistency, where
cycle consistency can guide image-to-image translation by
minimizing reconstruction loss between the real image and
the output sample, and dual-cycle consistency can guide
image translation by preventing generators from mode
collapse and unrealistic hallucinations.
3.1 Framework overview
Our full LSCIT is illustrated in Fig. 1(b). There are
two domains: X for source images and Y for target
images. The LSCIT learns by generating one domain
from another by GAN, helping the network to learn the
intermediate representation shared between the source and
target domains and the reconstruction capability upon the
intermediate representation. Precisely, an input sample x
should be translated into an output y as realistic as possible
via GAN. More specifically, the LSCIT model includes two
GANs for the two domain representations, where each GAN
consists of a generator to translate one image from one
domain to another. In addition, a discriminator is used to
distinguish between synthesized images and real images,
which can guide the synthesized image towards a real one.
Different from the single-cycle consistency, as shown
in Fig. 1(a), we design a cascade framework including a
dual-cycle consistency constraint, where the intermediate
generated images ˜Xy→x and ˜Y x→y are used to constrain the
reconstruction error in the short cycle, while the last output
ˆXy→x and ˆY x→y are applied to evaluate the reconstruction
in the long cycle (see Fig. 2).
3.2 LSCIT learning
Given two sets of unpaired images {xi}N
i=1
∈
X and
{yj}M
j=1 ∈Y sampled from domains X and Y, where G
denotes the forward generator X →Y in the primal-task
and ˆG denotes the backward generator Y →X in the dual-
task. Importantly, the primal discriminator DX is used to
distinguish between translated images and real images {x}
of domain X, and the dual discriminator DY is applied to
discriminate between fake output and {y} of domain Y.
3.2.1 Full objective
Our proposed GAN model for unsupervised image-to-image
translation consists of three loss terms: adversarial loss
LGAN for aligning the distribution of synthesized image
to the distribution of target image, short cycle consistency
loss Lshort for constraining the intermediate cross-domain
mappings to be a one-to-one mapping, and long cycle
consistency loss, Llong for constraining the encouraged
reconstruction to maintain training stability. Finally, the full
objective function for generators {G, ˆG} and discriminators
{DX, DY } in both primal-task and dual-task is expressed as:
L = LGAN + λshortLshort + λlongLlong,
(1)
where λshort and λlong are trade-off parameters which control
the relative importance of the objectives. Now, the model is
constrained by the proposed dual-cycle consistency. In the
ablation study, we compare our long-short cycle consistency
against the Lshort alone, Llong alone, and illustrate that the
proposed consistency is more effective.
3.2.2 Adversarial loss
To ensure the generated fake images look indistinguishable,
we apply an adversarial loss inherited from GAN [7] to
make both generators learn the cross-domain translations:
G and ˆG. For the primal-GAN, the adversarial loss can be
expressed as:
LGAN(G, DY , X, Y) = Ey∼pdata(y)[log DY (y)]
+Ex∼pdata(x)[log(1 −DY (G(x)))], (2)
where the goal of G is to synthesize the image that towards
to image from the target domain Y as-similar-as possible,
while DY aims to recognize the real and fake one. Similarly,
for the dual-GAN, we write the adversarial loss as:
LGAN( ˆG, DX, Y, X) = Ex∼pdata(x)[log DX(x)]
+Ey∼pdata(y)[log(1 −DX( ˆG(x)))]. (3)
The total adversarial loss is the sum of cross-domain
translations, which is written as:
LGAN = LGAN(G, DY , X, Y) + LGAN( ˆG, DX, Y, X),
(4)
where the quality of the output fake samples improves as
the generator and discriminator play the mini-max game
to reach the Nash equilibrium of the training procedure
[7]. Moreover, the generators are set to not only trick the
discriminators but also to be close to the real image at
the pixel level. Here, we choose the L1 to calculate the
pixel-level loss in the generators.
3.2.3 Short cycle consistency loss
By minimizing the adversarial loss, generators G and ˆG are
trained to translate input images into G(x) and ˆG(y). In
theory, the produced outputs are identically distributed as
the target domains Y and X respectively. In other words, the
discriminators cannot distinguish between the output and
the real ones. However, there is no guarantee that generators
can map an individual input to the desired output. To

Unsupervised image-to-image translation via long-short cycle-consistent...
Fig. 2 Top: The architecture with (a) single-cycle consistency and (b) dual-cycle consistency. Bottom: Detailed network architectures in our
proposed framework, including generator, encoder, decoder, and discriminator
alleviate this problem, we designed a dual-cycle consistency
to regularize the primal and dual generators.
For each input image x sampled from domain X, as
shown in Fig. 1(b), the short cycle consistency of the primal-
task should be able to map x back to its origin domain, i.e.,
x →G(x) →ˆG(G(x)) = ˜x, and the similar procedure for
the dual-task is expressed as y →ˆG(y) →G( ˆG(y)) = ˜y.
To make the learned mapping functions cycle-consistent,
we calculate reconstruction loss between x versus ˜x and y
versus ˜y, respectively, and then short cycle consistency loss
can be written as:
Lshort = Ex∼pdata(x)[∥ˆG(G(x)) −x∥1]
+Ey∼pdata(y)[∥G( ˆG(y)) −y∥1],
(5)
where the first term denotes the forward cycle consistency,
and the latter term is the backward cycle consistency.
3.2.4 Long cycle consistency loss
The short cycle consistency loss can be used to guide the
generators to learn to map input images to the desired
outputs with simple color mappings quickly in both primal
and dual tasks. However, for single-cycle consistency, more
constraints are required to force the learned generators
to produce an output distribution that matches the target
domain. We add a cascading reconstruction loss to
regularize the generators. Precisely, for each output image
˜x generated from the short cycle consistency procedure, the
long cycle consistency of the primal-task should be able
to map ˜x back to its origin image, i.e., ˜x →G(˜x) →
ˆG(G(˜x)) = ˆx. Similarly, for each output image ˜y in the dual-
task, we express it as ˜y →ˆG( ˜y) →G( ˆG( ˜y)) = ˆy. Then the
long cycle consistency loss can be expressed as:
Llong = Ex∼padata(x)[∥ˆG(G(˜x)) −x∥1]
+Ey∼padata(y)[∥G( ˆG( ˜y)) −y∥1] ,
(6)
where ˜x ∈X, and ˜x ∈Y. Here, the L1 loss is used
to measure the reconstruction error, since it can force the
generated images to match the domain distribution.
4 Implementation
4.1 Network architecture
Figure 2 shows the network architecture in LSCIT, where
generators G and
ˆG are constructed with an identical
network architecture. The generator network consists of an

G. Wang et al.
encoder, residual blocks, and a decoder, where the encoder
includes two convolutional layers with a stride of 2 (s = 2)
and 3 × 3 filters for down-sampling, the residual blocks
contain several layers of ResNet and more blocks (e.g.,
9 blocks) for higher-resolution training images, and the
decoder consists of two transposed convolutional layers
with a stride of 2 (s = 2) and 3 × 3 filters for up-sampling.
We use instance normalization [41] and RELU activation
in the generator and discriminator. For discriminators, the
70 × 70 PatchGAN [24] architecture is used to identify
whether overlapping patches are fake or real at the patch
level instead of the full image since high-frequency features
(e.g., texture and style) can be captured effectively by
patch configuration. In other words, dual-cycle consistency
loss enforces the preservation of global and low-frequency
information while PatchGAN-like discriminators DX and
DY are used to capture local high-frequency information.
4.2 Implementation details
To avoid gradients vanishing and get a more realistic output,
we use a least-squares loss Lls [28] rather than the sigmoid
cross-entropy loss in vanilla GANs. To be more specific,
we train generator G and discriminator DY to minimize
Lls(G) = Ex[(DY (G(x)) −1)2] and minDY Lls(DY ) =
Ey[(DY (y)−1)2]+Ex[DY (G(x))2]. Similarly, we can train
generator ˆG and discriminator DX by minimizing Lls( ˆG) =
Ey[(DX( ˆG(y)) −1)2] and minDX Lls(DX) = Ex[(D(x) −
1)2] + EY [DX( ˆG(y))2].
For the hyperparameters in objective function (1), we set
λshort =10. Our LSCIT is trained from scratch with a learning
rate of 0.0002 for 200 epochs, where the learning rate
decays to zero linearly after 100 epochs. The Adam solver
[17] with momentum term β1 = 0.5 and β2 = 0.999 is used
to train on all datasets. The batch size is set to 1. The number
of discriminator iterations per generator iteration nD is set
to 50. All input images are 256 × 256 without cropping.
4.3 Multi-step decay training
We introduce a two-step technique to train LSCIT, in which
we set λshort = 10 and λlong = 1 for the first 100 epochs, and
then fix λshort and decay λlong over the next 100 epochs, using
λlong = [·, ·] for short. For example, λlong =[1, 0.1] indicates
LSCIT learning with a two-step decay training strategy. In
our experiments, we show the qualitative and quantitative
results by adopting different multi-step decay parameters.
5 Experiments
To evaluate the effectiveness of our LSCIT in image-to-
image translation, we set up several experiments, including
a comparison of our LSCIT against recent models and an
ablation study for different variants of our method. All models
are trained on NVIDIA GeForce GTX Titan XP GPUs.
5.1 Datasets and evaluation metrics
We evaluate the capability of LSCIT on the following
datasets.
Cityscapes: photo-to-labels. 2975 images and 500 images
are collected from the Cityscapes Dataset [5] for training
and testing, respectively.
Facades: photo-to-labels. 400 images and 100 images are
collected from the CMP Facades Dataset [40] for training
and testing, respectively.
CUHK Face Sketch: sketch-to-face. We selected 188
student faces from the CUHK Face Sketch Database [45],
which contains 150 images for training. In preprocessing,
we center-crop these origin images to 256 × 256.
Day-Night: day-to-night. 90 images and 10 images are
collected from [19] for training and testing, respectively, and
are resized to 256 × 256.
Oil-Chinese Paintings: 1177 oil paintings and 1175
Chinese paintings are collected from the oil-Chinese
painting dataset [47] for training, and 47 images for testing.
The first four datasets contain ground-truth data, so we
can use them to evaluate the method quantitatively.
Evaluation Metrics: We use the FCN-score [13] and
the semantic segmentation metrics including per-pixel
accuracy, per-class accuracy, and mean class Intersection-
Over-Union (Class IOU) [5] to evaluate the labels-to-photo
task and the photo-to-labels task on Cityscapes and Facades,
respectively. Importantly, FID (Fr´echet Inception Distance)
[9] is used to measure the distance between the output
distribution and the target distribution. The lower the FID
values, the better the realism of the translated images. We
use LPIPS (Learned Perceptual Image Patch Similarity) [50]
to evaluate the similarity between each generated image and
its real input, where higher LPIPS scores mean further/more
different, while lower means more similar. Precisely, we use
the labeled data to compute the LPIPS scores, where we
measure the LPIPS distance between the generated image
and its corresponding labeled image.
5.2 Baseline models
We compare our LSCIT with various cycle consistency-
based unsupervised models, including DualGAN [47],
CycleGAN [54], StarGAN [4], MUNIT [12], UGATIT [15],
and DRIT++ [22]. In addition, we also chose a supervised
model, BicycleGAN [55], and a current self-supervised
FSeSim [52], for our comparison experiments.
DualGAN [47] uses the WGAN [2] loss and the
RMSProp solver [39] to guide the translation between

Unsupervised image-to-image translation via long-short cycle-consistent...
source and target domains. We selected the official
implementation based on the Tensorflow framework. We set
the number of epochs to 45 on all datasets.
CycleGAN [54] introduces the vanilla cycle consistency
to regularize the mapping between two different domains,
X and Y. Its network consists of two down-sampling
convolution blocks, nine residual blocks, two up-sampling
deconvolution blocks, and four discriminator layers. Note
that the identity loss and dropout defined in [54] are not used
in our experiments.
BicycleGAN [55] is a supervised model for learning the
mapping with paired samples. Here, we train BicycleGAN
on four datasets with ground-true data: Cityscapes, Facades,
CUHK Face Sketch, and Day-Night, and select the first
random output as the results. Note that the BicycleGAN
results are not directly comparable to those of the other
approaches as the paired data is used.
StarGAN [4] is a framework for multi-domain image-to-
image translation in one model, and it can decrease the size
of all generators across multiple domains.
MUNIT [12] is introduced to generate various multi-
modal outputs for an input sample. Assuming that the image
representation can be embedded in a content and style code
that is decoupled. In addition, AdaIN is used in the decoder,
and the network consists of a multi-scale discriminator.
UGATIT [15] uses the attention module and adaptive
layer-instance normalization to guide the mapping in the
image translation. Moreover, UGATIT consists of two
implementation versions: full and light. For the limited GPU
memory (less than 12GB), we chose the light version in our
experiments.
DRIT++ [22] aims to generate multi-modal images for
the purpose of pursuing realism and diversity. Here, we
use it to evaluate the realism with only one random output
on sketch-to-face and day-to-night tasks. The number of
iterations is set to 1200.
FSeSim [52] ultilies a learned spatially-correlative map
to calculate spatially-correlative losses, in which the fixed
self-similarity of features is extracted from selected pre-
trained networks.
All comparison methods have been trained with the
default hyperparameters in the released public implementa-
tions.
5.3 Ablation study
To evaluate the effectiveness of the proposed long-short
cycle consistency loss in (1), we set up the ablation study
to compare our LSCIT against ablations of the full loss
function, including GAN + short cycle consistency alone
Lshort with λshort = 10, GAN + long cycle consistency
alone Llong with λlong = 10, and GAN + long-short cycle
consistency Lshort + Llong with multi-step decay training
strategy λlong = [1, 0.1], as the method is motivated by
cycle consistency and dual learning.
Figure 3 shows the comparison results against the
ablations of the LSCIT full objective on Cityscapes,
Facades, CHUK Face, and Day-Night datasets. Each
column in left and right groups indicates input sample,
ground-truth (GT), generated image only with Lshort,
generated image only with Llong, and generated image with
both Lshort and Llong, respectively. We can see that the
Fig. 3 Ablation Study on Cityscapes (1st row), Facades (2nd row),
CHUK Face (3rd row), and Day-Night (last row) datasets. Each col-
umn in the group represents an input sample, ground-truth (GT),
generated image with only Lshort, generated image with only Llong,
and generated image with both Lshort and Llong

G. Wang et al.
dual-cycle consistency Lshort + Llong can get the better
generations.
In Table 1, we report the comparisonresults against the abla-
tions of our full objective on the Cityscapes and Facades.
Removing extra long cycle consistency loss degrades
results. Meanwhile, removing inner short cycle consistency
loss also degrades results, especially photo-to-labels on
Cityscapes. Note that the Llong alone on Cityscapes photo-
to-labels incurs mode collapse, but dual-cycle gets better
results in most test cases than the Lshort one by combining
the inner short and extra long cycle consistency.
Table 2 reports comparison results against ablations of
our full objective using FID and LPIPS on CUHK Face and
Day-Night datasets. Lshort + Llong with a multi-step decay
training strategy achieves the best LPIPS on three data sce-
narios, while Llong alone gets the best FID scores. Totally,
Tables 1 and 2 illustrate that the long-short cycle consistency
and the multi-step decay training strategy are effective.
5.4 Experimental results
5.4.1 Qualitative evaluations
Using the labeled datasets with ground truth, we first
evaluate the method for several image translation tasks:
labels↔photo, sketch↔face, and day↔night, and then we
show more applications using image-to-image translation,
such as style transfer: oil↔Chinese paintings.
Results on Cityscapes. Figure 4 shows the image
translation comparison results from baselines and our
LSCIT on Cityscapes labels↔photo. Image semantic
segmentation results are synthesized by the image-to-
image translation methods in the top two rows of Fig. 4.
From the qualitative results, we observe that DualGAN
obtains similar wrong segmentation results on all inputs
since the mode collapse disturbs the generation. The
supervised BicycleGAN reports good results but with
Table 1 Ablation study: classification performance of photo→labels and FCN-scores of labels→photo for different variants of our method on
the Cityscapes and Facades datasets
Data
Lsh
Llo
[·, ·]
Per-pixel acc.
Per-class acc.
Class IOU
Cityscapes photo→labels
✓
54.72±11.03
21.19±04.12
13.98±3.33
✓
8.42±1.07
5.29±0.94
1.96±0.21
✓
✓
42.06±7.38
13.77±2.73
8.02±2.01
✓
✓
54.47±12.17
21.93±4.29
14.98±3.68
✓
✓
✓
55.08±12.60
23.20±5.12
15.74±4.22
Cityscapes labels→photo
✓
49.80±11.88
6.90±2.05
4.40±1.51
✓
39.53±6.30
4.89±1.19
2.81±0.66
✓
✓
49.09±9.88
5.92±1.23
3.35±0.68
✓
✓
49.11±12.69
7.00±2.19
4.49±1.63
✓
✓
✓
48.76±12.10
6.85±2.13
4.41±1.57
Facades photo→labels
✓
17.98±6.34
11.41±1.21
4.44±1.05
✓
26.76±7.22
11.08±1.79
5.66±1.19
✓
✓
31.05±7.45
14.63±1.86
7.61±1.35
✓
✓
49.81±10.75
29.98±6.89
19.13±5.75
✓
✓
✓
50.06±11.43
30.36±7.49
19.74±6.29
Facades labels→photo
✓
42.63±9.60
12.59±2.12
7.32±1.75
✓
44.28±8.37
12.63±2.22
7.59±1.67
✓
✓
45.40±0.72
13.03±1.75
8.05±1.41
✓
✓
47.36±10.36
13.80±2.24
8.31±2.02
✓
✓
✓
48.13±10.01
13.85±2.37
8.41±2.00
Lsh denotes the short cycle consistency loss, Llo denotes the long cycle consistency loss, [·, ·] denotes the multi-step decay training. The bold
entries found in Tables are significant for readers to get the best experimental results

Unsupervised image-to-image translation via long-short cycle-consistent...
Table 2 Ablation study: generation performance of photo→labels and labels→photo using FID and LPIPS for different variants of our method
on the CUHK Face and Day-Night datasets
Data
Lsh
Llo
[·, ·]
FID
LPIPS
CUHK Face sketch→face
✓
106.11
0.203±0.061
✓
74.04
0.179±0.050
✓
✓
83.88
0.183±0.049
✓
✓
97.17
0.206±0.059
✓
✓
✓
78.83
0.182±0.055
CUHK Face face→sketch
✓
41.19
0.204±0.054
✓
37.04
0.202±0.052
✓
✓
46.47
0.215±0.052
✓
✓
37.66
0.202±0.051
✓
✓
✓
38.01
0.202±0.051
Day Night day →night
✓
348.97
0.619±0.056
✓
283.00
0.635±0.070
✓
✓
363.84
0.710±0.038
✓
✓
330.83
0.642±0.087
✓
✓
✓
293.76
0.600±0.081
Day Night night →day
✓
304.48
0.605±0.089
✓
370.49
0.693±0.058
✓
✓
307.97
0.617±0.057
✓
✓
371.61
0.584±0.081
✓
✓
✓
299.77
0.550±0.098
Lsh denotes the short cycle consistency loss, Llo denotes the long cycle consistency loss, [·, ·] denotes the multi-step decay training. The bold
entries found in Tables are significant for readers to get the best experimental results
jagged feelings. We can see that our LSCIT is able to get
satisfactory segmentation results. For labels↔photo (see
the 3rd-4th rows in Fig. 4), CycleGAN and BicycleGAN
produce slightly better photos from labels on Cityscapes,
but StarGAN, MUNIT, and DRIT++ generate some low-
quality results. Some pedestrians are not generated well
from the given labels by CycleGAN. Conversely, LSCIT
with dual-cycle consistency is capable of preserving global
information and generating more realistic pedestrians. Note
that DualGAN still gets wrong results as occurring mode
collapse.
Results on Facades. Figure 4 reports labels↔photo tasks
on the architectural Facades dataset. For photo-to-labels,
the trained CycleGAN with cycle consistency gets bad
segmentation labels as it faces mode collapse. DualGAN
generates slightly worse labels, while BicycleGAN uses
paired data to map Facades’ photos to labels very well.
MUNIT also occurs mode collapse, while UGATIT and
DRIT++ achieve good outputs. Here, our LSCIT gets
better, more realistic results. All methods can generate
desirable architectural facades, where LSCIT can learn
the better global architecture of buildings, such as the
roof.
Results on CUHK Face. Figure 5 shows that almost all
methods generate more realistic faces, including realistic
facial features and skin color, and sketches, except for
StarGAN, MUNIT, and DRIT++. CycleGAN produces
slightly strange results on several generated faces. For
example, some outputs contain a bright spot on their
forehead. DualGAN produces more visually appealing
results, although mixed with some background color.
BicycleGAN produces some blurry results. As well, we can
see that the results translated by LSCIT generally contain
fewer artifacts than those from comparison approaches.
Results on Day-Night. For day→night task, as shown
in Fig. 5, it is actually hard to define a true night scene
from a day image, hence we can see all baseline methods
generate desirable night images. Nevertheless, the detailed
information in the generated images can be evaluated
visually, and our LSCIT can preserve more information
in realism. In particular, more artifacts are produced. For
instance, the generated mountains and trees are not realistic.
Interestingly, results from our LSCIT look more realistic
with higher quality.
Results on Unlabeled Samples. Figure 6 shows the
style transfer task: oil↔Chinese paintings. In this section,

G. Wang et al.
Fig. 4 Qualitative evaluations and comparisons against baseline
models of image translation results on Cityscapes and Facades
datasets. Two input samples are shown in each dataset, the top
two rows denote photo→labels, and the bottom two rows denote
labels→photo. From left to right, each column indicates the input sam-
ple, ground truth (GT), DualGAN [47], CycleGAN [54], BicycleGAN
[55], StarGAN [4], MUNIT [12], UGATIT [15], DRIT++ [22], and
our LSCIT. Note that BicycleGAN [55] is a supervised model
we evaluate LSCIT variants by using different two-stage
parameters λlong = [0, 1], λlong = [0, 0.1], and λlong =
[1, 0.1], where our method can generate realistic results. All
of the results show that our method can be used to process
cross-domain style transfers.
5.4.2 Quantitative evaluations
Classification performance of photo-to-labels and FCN-
scores of the opposite mapping (labels-to-photo) using
per-pixel accuracy, per-class accuracy, and Class IOU for
different approaches on Cityscapes and Facades datasets
is reported in Table 3, where bold font indicates the best,
and the underline indicates the second best. In both cases,
LSCIT outperforms the baselines. Note that DualGAN gets
the best per-pixel accuracy on Cityscapes labels-to-photo,
while the qualitative results, as shown in Fig. 4, actually tell
us that DualGAN might occur mode collapse. Comparing
the supervised BicycleGAN, we observe that our LSCIT
can catch similar or even better accuracies. We use the
light version of the UGATIT to evaluate the generations,
and it gets good performances due to the attention module
and adaptive normalization. Moreover, we also use FID and
average LPIPS distance to measure the realism of generated
images on Cityscapes and Facades, as reported in Table 3,
where the lower scores indicate the better generation.
FSeSim achieves good performance on Facades from photo
to labels, while our approach obtains better results on
Facades from labels to photo. Importantly, adopting a multi-
step decay training strategy and dual-cycle consistency

Unsupervised image-to-image translation via long-short cycle-consistent...
Fig. 5 Qualitative evaluations and comparisons against baseline
models of image translation results on CUHK Face and Day-Night
datasets. Two input samples are shown in each dataset. The top
two rows denote photo→labels, and the bottom two rows denote
labels→photo. From left to right, each column indicates the input sam-
ple, ground truth (GT), DualGAN [47], CycleGAN [54], BicycleGAN
[55], StarGAN [4], MUNIT [12], UGATIT [15], DRIT++ [22], and
our LSCIT. Note that BicycleGAN [55] is a supervised model
can obtain better results in most scenarios. For instance,
LSCIT gets the best accuracy on Cityscapes photo-to-labels
and the second best accuracy on Facades labels-to-photo.
Table 4 assesses the performance of the sketch↔face
photo tasks on the CUHK Face Sketch Database. Here,
we use FID and LPIPS distance to evaluate the translation
models. Note that we focus on evaluating the similarity
between the generated fake image and its real image
(ground-truth). Thus, lower LPIPS scores and FID values
indicate the better quality of the produced images. Our
approach achieves the lowest FID score and LPIPS value
in the sketch↔face mapping, with CycleGAN as the next
best performing approach. DRIT++ produces unsatisfactory
results with the highest mean LPIPS score as it has been
designed for diversity multi-modal image translation, while
BicycleGAN, MUNIT, and UGATIT get good FID values
and slightly better mean LPIPS values than DRIT++.
Importantly, by comparing the results, our LSCIT obtains
better results in most scenarios than the baselines on these
two image-to-image translation tasks.

G. Wang et al.
Fig. 6 Image-to-image
translation results from variants
of λlong with a multi-step decay
training strategy on Oil-Chinese
paintings. The top two rows:
oil→Chinese paintings. The
bottom two rows: Chinese→oil
paintings
5.5 Multi-step decay training strategy effectiveness
To achieve better quality and more realistic results, we
propose a multi-step technique to train the proposed LSCIT.
In the first step, we fix λshort = 10, and then change λlong
over the next step. In this experiment, we setup two cases
without a multi-step decay training strategy, λlong = 0.1
and λlong = 1, and three settings: λlong = [0, 0.1], λlong =
[0, 1], and λlong = [1, 0.1]. Figure 7 shows the comparison
results on the Cityscapes, Facades, CUHK Face, and Day-
Night datasets using FID and LPIPS scores. We can see that
the multi-step decay training strategy improves the image-
to-image translation performance. In addition, Fig. 6 shows
the synthesized style paintings of LSCIT using the multi-
step decay training strategy. Different styles are generated
with the training setup. The qualitative and quantitative
results obtained by adopting different multi-step decay
parameters illustrate the effectiveness.
5.6 Model selection
The present long-short cycle consistency is with dual cycles
in the loss function. Here, we set up an experiment to
evaluate the model with more cycles. Figure 8 shows the
comparison with three cycles. The results of the model with
three cycles get slightly higher FID and LPIPS scores than
our original two cycles. Three cycles do not help improve
performance, but they do add more training times. Thus,
there is a balance between performance and efficiency in the
proposed dual-cycle consistency.
6 Conclusion
Unsupervised
or
unpaired
image-to-image
translation
approaches are able to learn to map corresponding image
regions. However, the generated images look unrealistic as

Unsupervised image-to-image translation via long-short cycle-consistent...
Table 3 Comparison study: We compared the generation performance of photo→labels and labels→photo using FCN-scores, FID, and LPIPS to
baseline models on the Cityscapes and Facades datasets
Data
Model
Per-pixel acc.
Per-class acc.
Class IOU
FID
LPIPS
Cityscapes photo →labels
DualGAN [47]
39.31±8.16
14.61±3.73
8.52±2.28
239.08
0.464±0.030
CycleGAN [54]
54.72±11.03
21.19±4.12
13.98±3.33
108.55
0.343±0.058
BicycleGAN [55]
69.30±9.95
24.44±5.05
17.62±4.41
169.39
0.279±0.071
StarGAN [4]
49.25±10.19
17.78±3.74
11.58±3.02
144.63
0.381±0.059
MUNIT [12]
50.24±12.22
16.17±3.99
9.75±3.16
121.19
0.444±0.057
UGATIT [15]
65.53±12.84
24.49±4.96
17.68±4.31
90.46
0.260±0.077
DRIT++ [22]
53.34±11.30
17.80±4.11
11.88±3.16
86.84
0.409±0.059
FSeSim [52]
15.96±5.59
5.97±1.39
2.2±0.58
334.96
0.628±0.035
LSCIT (Ours)
66.24±9.56
25.01±4.97
17.46±4.13
75.95
0.309±0.070
Cityscapes labels →photo
DualGAN [47]
56.89±10.83
5.95±1.28
3.90±1.01
244.08
0.476±0.047
CycleGAN [54]
49.80±11.88
6.90±2.05
4.40±1.51
82.09
0.367±0.064
BicycleGAN [55]
53.44±12.10
8.28±2.43
5.29±1.77
86.22
0.346±0.060
StarGAN [4]
53.05±11.29
6.46±1.81
4.27±1.38
211.53
0.405±0.064
MUNIT [12]
52.34±12.86
8.23±2.41
5.11±1.81
75.22
0.386±0.066
UGATIT [15]
49.62±12.09
7.16±2.28
4.50±1.58
56.36
0.371±0.064
DRIT++ [22]
47.12±11.79
6.09±1.78
3.64±1.14
107.83
0.350±0.068
FSeSim [52]
50.29±12.20
7.29±2.37
4.66±1.72
71.68
0.385±0.067
LSCIT (Ours)
52.82±13.66
8.36±2.61
5.42±1.93
59.47
0.320±0.067
Facades photo →labels
DualGAN [47]
22.99±8.05
12.39±3.27
6.45±2.09
292.96
0.605±0.049
CycleGAN [54]
23.98±9.25
10.48±2.89
5.58±2.09
280.07
0.776±0.089
BicycleGAN [55]
52.64±10.01
32.35±6.54
20.83±5.52
283.45
0.476±0.082
StarGAN [4]
56.26±9.91
35.97±7.34
24.07±6.56
269.85
0.397±0.080
MUNIT [12]
22.79±12.50
11.30±1.99
3.53±1.82
397.23
0.745±0.082
UGATIT[15]
54.86±11.23
31.35±8.50
21.44±7.12
98.58
0.436±0.096
DRIT++ [22]
50.76±10.95
29.05±7.92
18.92±6.33
130.69
0.466±0.081
FSeSim [52]
56.58±11.15
35.53±7.84
24.19±6.9
147.06
0.402±0.091
LSCIT (ours)
50.06±11.43
30.36±7.49
19.74±6.29
112.18
0.462±0.084
Facades labels →photo
DualGAN [47]
38.11±9.56
11.10±1.59
6.21±1.44
207.15
0.517±0.070
CycleGAN [54]
43.05±8.80
12.06±01.86
7.17±1.58
138.02
0.485±0.061
BicycleGAN [55]
47.98±9.33
14.17±2.28
8.63±1.90
123.12
0.448±0.069
StarGAN [4]
44.14±9.71
13.52±2.93
7.91±2.21
210.84
0.478±0.069
MUNIT [12]
46.58±11.60
13.19±2.68
7.65±2.11
137.47
0.491±0.071
UGATIT [15]
47.06±10.52
14.04±3.06
8.53±2.68
108.68
0.478±0.070
DRIT++ [22]
46.26±11.07
13.73±2.61
8.19±2.42
130.98
0.444±0.074
FSeSim [52]
48.32±9.59
13.37±1.85
8.10±1.68
210.02
0.482±0.077
LSCIT (ours)
48.13±10.01
13.85±2.37
8.41±2.00
128.04
0.445±0.072
The bold font indicates the best, and the underline indicates the second best
a result of error accumulation and mode collapse. In this
paper, we present a dual-cycle consistency dual learning for
unsupervised image-to-image translation. We assume that
more information in the reconstruction procedure of the
cycle-consistent model might be lost, so we add an extra
long cycle consistency to constrain the objective. From the
extensive cross-domain image-to-image translation experi-
ments and the ablation study, we demonstrate that the pro-
posed model outperforms the baselines and gets significant
improvements in the quality of translated images.

G. Wang et al.
Table 4 Comparison study: generating performance using FID and LPIPS for our method against baseline models on CUHK Face and Day-Night
datasets
Data
Model
FID
LPIPS
CUHK Face sketch →face
DualGAN [47]
118.89
0.214±0.046
CycleGAN [54]
106.11
0.203±0.061
BicycleGAN [55]
126.48
0.206±0.039
StarGAN [4]
237.97
0.498±0.031
MUNIT [12]
223.11
0.323±0.046
UGATIT [15]
71.99
0.175±0.051
DRIT++ [22]
108.15
0.229±0.048
FSeSim [52]
103.71
0.172±0.050
LSCIT (Ours)
78.83
0.182±0.055
CUHK Face face →sketch
DualGAN [47]
54.45
0.214±0.052
CycleGAN [54]
41.19
0.204±0.054
BicycleGAN [55]
118.73
0.353±0.043
StarGAN [4]
124.31
0.329±0.047
MUNIT [12]
128.41
0.342±0.052
UGATIT [15]
59.46
0.255±0.057
DRIT++ [22]
57.23
0.303±0.121
FSeSim [52]
94.33
0.275±0.042
LSCIT (Ours)
38.01
0.202±0.051
Day Night day →night
DualGAN [47]
285.08
0.671±0.067
CycleGAN [54]
348.97
0.619±0.056
BicycleGAN [55]
322.27
0.660±0.072
StarGAN [4]
281.86
0.638±0.056
MUNIT [12]
270.15
0.672±0.063
UGATIT [15]
290.90
0.641±0.067
DRIT++ [22]
311.45
0.652±0.069
FSeSim [52]
365.74
0.697±0.052
LSCIT (Ours)
293.76
0.600±0.081
Day Night night →day
DualGAN [47]
323.93
0.695±0.062
CycleGAN [54]
304.48
0.605±0.089
BicycleGAN [55]
314.50
0.586±0.071
StarGAN [4]
384.90
0.630±0.087
MUNIT [12]
300.50
0.593±0.048
UGATIT [15]
312.80
0.639±0.093
DRIT++ [22]
307.13
0.603±0.059
FSeSim [52]
201.46
0.336±0.049
LSCIT (Ours)
299.77
0.550±0.098
The bold font indicates the best, and the underline indicates the second best
However, our proposed dual-cycle consistency would
be limited by the following two aspects. a) In a cycle-
framework, we still need a cycle-consistency loss, which
would affect the performance of our approach. It would be
a good motivation to introduce the idea to the one-sided
framework. b) The proposed dual-cycle consistency strikes
a balance between performance and efficiency; adding more
cycles does not improve performance further but does result
in more training and prediction runtime. Thus, it should be
modeled as an efficient approach in future work.

Unsupervised image-to-image translation via long-short cycle-consistent...
Fig. 7 Comparison results on Cityscapes, Facades, CUHK Face, and Day-Night datasets using FID and LPIPS scores with different λlong
Fig. 8 Comparison with more cycles on Day-Night, CUHK Face, and Facades datasets using FID and LPIPS scores

G. Wang et al.
Acknowledgments This work was supported by the National Natural
Science Foundation of China under Grant No. 61703260, and partially
supported by the National Natural Science Foundation of China under
Grant No. 62173252.
References
1. Anoosheh A, Agustsson E, Timofte R, Van Gool L (2018)
Combogan: unrestrained scalability for image domain translation.
In: Proceedings of the IEEE conference on computer vision and
pattern recognition workshops, pp 783–790
2. Arjovsky M, Chintala S, Bottou L (2017) Wasserstein genera-
tive adversarial networks. In: Proceedings of the international
conference on machine learning, pp 214–223
3. Chen C, Wang G (2021) Iosuda: an unsupervised domain
adaptation with input and output space alignment for joint optic
disc and cup segmentation. Appl Intell 51(6):3880–3898
4. Choi Y, Choi M, Kim M, Ha JW, Kim S, Choo J (2018) Stargan:
unified generative adversarial networks for multi-domain image-
to-image translation. In: Proceedings of the IEEE conference on
computer vision and pattern recognition, pp 8789–8797
5. Cordts M, Omran M, Ramos S, Rehfeld T, Enzweiler M,
Benenson R, Franke U, Roth S, Schiele B (2016) The cityscapes
dataset for semantic urban scene understanding. In: Proceedings of
the IEEE conference on computer vision and pattern recognition,
pp 3213–3223
6. Engin D, Genc¸ A, Kemal Ekenel H (2018) Cycle-dehaze:
enhanced cyclegan for single image dehazing. In: Proceedings of
the IEEE conference on computer vision and pattern recognition
workshops, pp 825–833
7. Goodfellow I, Pouget-Abadie J, Mirza M, Xu B, Warde-Farley
D, Ozair S, Courville A, Bengio Y (2014) Generative adversarial
nets. In: Advances in neural information processing systems,
pp 2672–2680
8. He D, Xia Y, Qin T, Wang L, Yu N, Liu TY, Ma WY (2016)
Dual learning for machine translation. In: Advances in neural
information processing systems, pp 820–828
9. Heusel M, Ramsauer H, Unterthiner T, Nessler B, Hochreiter S
(2017) Gans trained by a two time-scale update rule converge
to a local nash equilibrium. In: Advances in neural information
processing systems, pp 6626–6637
10. Hinton GE, Salakhutdinov RR (2006) Reducing the dimensional-
ity of data with neural networks. Science 313(5786):504–507
11. Huang QX, Guibas L (2013) Consistent shape maps via
semidefinite programming. In: Computer graphics forum. Wiley
online library, vol 32, pp 177–186
12. Huang X, Liu MY, Belongie S, Kautz J (2018) Multimodal
unsupervised image-to-image translation. In: Proceedings of the
european conference on computer vision (ECCV), pp 172–189
13. Isola P, Zhu JY, Zhou T, Efros AA (2017) Image-to-image
translation with conditional adversarial networks. In: Proceedings
of the IEEE conference on computer vision and pattern
recognition, pp 1125–1134
14. Karras T, Laine S, Aila T (2019) A style-based generator
architecture for generative adversarial networks. In: Proceedings
of the IEEE conference on computer vision and pattern
recognition, pp 4401–4410
15. Kim J, Kim M, Kang H, Lee K (2019) U-gat-it: unsupervised
generative attentional networks with adaptive layer-instance
normalization for image-to-image translation. In: Proceedings of
the international conference on learning representations
16. Kim T, Cha M, Kim H, Lee JK, Kim J (2017) Learning to discover
cross-domain relations with generative adversarial networks. In:
Proceedings of the 34th international conference on machine
learning. JMLR. org, vol 70, pp 1857–1865
17. Kingma DP, Ba J (2014) Adam: a method for stochastic
optimization. In: Proceedings of the international conference on
learning representations
18. Kingma DP, Welling M (2013) Auto-encoding variational bayes.
In: Proceedings of the international conference on learning
representations
19. Laffont PY, Ren Z, Tao X, Qian C, Hays J (2014) Transient
attributes for high-level understanding and editing of outdoor
scenes. ACM Trans Grap (TOG) 33(4):149
20. Larsson G, Maire M, Shakhnarovich G (2016) Learning repre-
sentations for automatic colorization. In: European conference on
computer vision. Springer, pp 577–593
21. Ledig C, Theis L, Husz´ar F, Caballero J, Cunningham A,
Acosta A, Aitken A, Tejani A, Totz J, Wang Z et al
(2017) Photo-realistic single image super-resolution using a
generative adversarial network. In: Proceedings of the IEEE
conference on computer vision and pattern recognition, pp 4681–
4690
22. Lee HY, Tseng HY, Mao Q, Huang JB, Lu YD, Singh M,
Yang MH (2020) Drit++: Diverse image-to-image translation via
disentangled representations. Int J Comput Vis 128(10):2402–
2417
23. Li C, Deng C, Wang L, Xie D, Liu X (2019) Coupled cyclegan:
unsupervised hashing network for cross-modal retrieval. In:
Proceedings of the AAAI conference on artificial intelligence,
33(1), pp 176–183
24. Li C, Wand M (2016) Precomputed real-time texture synthesis
with markovian generative adversarial networks. In: European
conference on computer vision. Springer, pp 702–716
25. Liu MY, Breuel T, Kautz J (2017) Unsupervised image-to-
image translation networks. In: Advances in neural information
processing systems, pp 700–708
26. Long J, Shelhamer E, Darrell T (2015) Fully convolutional
networks for semantic segmentation. In: Proceedings of the IEEE
conference on computer vision and pattern recognition, pp 3431–
3440
27. Luo P, Wang G, Lin L, Wang X (2017) Deep dual learning
for semantic image segmentation. In: Proceedings of the
IEEE international conference on computer vision, pp 2718–
2726
28. Mao X, Li Q, Xie H, Lau RY, Wang Z, Paul Smolley S (2017)
Least squares generative adversarial networks. In: ICCV, pp 2794–
2802
29. Mejjati YA, Richardt C, Tompkin J, Cosker D, Kim KI (2018)
Unsupervised attention-guided image-to-image translation. In:
Advances in neural information processing systems, pp 3693–
3703
30. Mirza M, Osindero S (2014) Conditional generative adversarial
nets. arXiv:1411.1784
31. Mo S, Cho M, Shin J (2018) Instagan: instance-aware image-to-
image translation. In: Proceedings of the international conference
on learning representations
32. Nizan O, Tal A (2019) Breaking the cycle–colleagues are all you
need. In: Proceedings of the IEEE/CVF conference on computer
vision and pattern recognition, pp 7860–7869
33. Park T, Efros AA, Zhang R, Zhu JY (2020) Contrastive learning
for unpaired image-to-image translation. In: European conference
on computer vision. Springer, pp 319–345
34. Richardson E, Alaluf Y, Patashnik O, Nitzan Y, Azar Y, Shapiro
S, Cohen-Or D (2021) Encoding in style: a stylegan encoder
for image-to-image translation. In: Proceedings of the IEEE/CVF
conference on computer vision and pattern recognition, pp 2287–
2296

Unsupervised image-to-image translation via long-short cycle-consistent...
35. Saxena D, Kulshrestha T, Cao J, Cheung SC (2022) Multi-
constraint adversarial networks for unsupervised image-to-image
translation. IEEE Trans Image Process
36. Schlegl T, Seeb¨ock P, Waldstein SM, Langs G, Schmidt-erfurth
U (2019) f-anogan: Fast unsupervised anomaly detection with
generative adversarial networks. Med Image Anal 54:30–44
37. Smolensky P (1986) Information processing in dynamical
systems: foundations of harmony theory. Tech Rep, Colorado Univ
at Boulder Dept of computer science
38. Tang H, Xu D, Liu G, Wang W, Sebe N, Yan Y (2019) Cycle
in cycle generative adversarial networks for keypoint-guided
image generation. In: Proceedings of the 27th ACM international
conference on multimedia, pp 2052–2060
39. Tieleman T, Hinton G (2012) Lecture 6.5-rmsprop: divide
the gradient by a running average of its recent magnitude.
COURSERA: Neural Netw Mach Learn 4(2):26–31
40. Tyleˇcek R, ˇS´ara R (2013) Spatial pattern templates for recognition
of objects with regular structure. In: German conference on pattern
recognition. Springer, pp 364–374
41. Ulyanov D, Vedaldi A, Lempitsky V (2016) Instance normaliza-
tion: the missing ingredient for fast stylization. arXiv:1607.08022
42. Vincent P, Larochelle H, Bengio Y, Manzagol PA (2008) Extract-
ing and composing robust features with denoising autoencoders.
In: Proceedings of the 25th international conference on machine
learning, pp 1096–1103
43. Wang G, Shi H, Chen Y (2021) Self-augmentation with dual-
cycle constraint for unsupervised image-to-image generation. In:
Proceedings of the IEEE 33rd international conference on tools
with artificial intelligence, pp 886–890
44. Wang TC, Liu MY, Zhu JY, Tao A, Kautz J, Catanzaro B (2018)
High-resolution image synthesis and semantic manipulation with
conditional gans. In: Proceedings of the IEEE conference on
computer vision and pattern recognition, pp 8798–8807
45. Wang X, Tang X (2008) Face photo-sketch synthesis and
recognition. IEEE Trans Pattern Anal Mach Intell 31(11):1955–
1967
46. Wolterink JM, Dinkla AM, Savenije MH, Seevinck PR, van den
Berg CA, Iˇsgum I (2017) Deep mr to ct synthesis using unpaired
data. In: International workshop on simulation and synthesis in
medical imaging. Springer, pp 14–23
47. Yi Z, Zhang H, Tan P, Gong M (2017) Dualgan: unsupervised dual
learning for image-to-image translation. In: Proceedings of the
IEEE international conference on computer vision, pp 2849–2857
48. Yuan Y, Liu S, Zhang J, Zhang Y, Dong C, Lin L (2018) Unsu-
pervised image super-resolution using cycle-in-cycle generative
adversarial networks. In: Proceedings of the IEEE conference on
computer vision and pattern recognition workshops, pp 701–710
49. Zach C, Klopschitz M, Pollefeys M (2010) Disambiguating visual
relations using loop constraints. In: 2010 IEEE computer society
conference on computer vision and pattern recognition. IEEE,
pp 1426–1433
50. Zhang R, Isola P, Efros AA, Shechtman E, Wang O (2018) The
unreasonable effectiveness of deep features as a perceptual metric.
In: Proceedings of the IEEE conference on computer vision and
pattern recognition, pp 586–595
51. Zhao Y, Chen C (2021) Unpaired image-to-image translation
via latent energy transport. In: Proceedings of the IEEE/CVF
conference on computer vision and pattern recognition, pp 16418–
16427
52. Zheng C, Cham TJ, Cai J (2021) The spatially-correlative loss for
various image translation tasks. In: Proceedings of the IEEE/CVF
conference on computer vision and pattern recognition, pp 16407–
16417
53. Zhou T, Krahenbuhl P, Aubry M, Huang Q, Efros AA (2016)
Learning dense correspondence via 3d-guided cycle consistency.
In: Proceedings of the IEEE conference on computer vision and
pattern recognition, pp 117–126
54. Zhu JY, Park T, Isola P, Efros AA (2017) Unpaired image-to-
image translation using cycle-consistent adversarial networks. In:
Proceedings of the IEEE international conference on computer
vision, pp 2223–2232
55. Zhu JY, Zhang R, Pathak D, Darrell T, Efros AA, Wang
O, Shechtman E (2017) Toward multimodal image-to-image
translation. In: Advances in neural information processing
systems, pp 465–476
Publisher’s note Springer Nature remains neutral with regard to
jurisdictional claims in published maps and institutional affiliations.
Springer Nature or its licensor (e.g. a society or other partner) holds
exclusive rights to this article under a publishing agreement with the
author(s) or other rightsholder(s); author self-archiving of the accepted
manuscript version of this article is solely governed by the terms of
such publishing agreement and applicable law.
Gang Wang is presently an Associate Professor with the Institute of
Data Science and Statistics, School of Statistics and Management,
Shanghai University of Finance and Economics, Shanghai, China.
He received his Ph.D. degree in Computer Science and Technology
from Tongji University in 2016. His current research interests include
computer vision, pattern recognition, machine learning, and data
analysis.
Haibo Shi received his Ph.D. degree in pattern recognition and
intelligent system from Tongji University, Shanghai, China. He is
currently an Assistant Professor with the Institute of Data Science and
Statistics, School of Statistics and Management, Shanghai University
of Finance and Economics, Shanghai, China. His research interests
cover deep reinforcement learning and computational neuroscience.
Yufei Chen is presently an Associate Professor in the CAD
Research Center, College of Electronic and Information Engineering,
Tongji University, Shanghai 201804, China. She was a Post-doctoral
Researcher in Control Science and Engineering of Tongji University
from 2010 to 2012. She received her Ph.D. degree from Tongji
University in 2010. She was also a Guest Researcher in Fraunhofer
Institute for Computer Graphics Research, Germany from 2008 to
2009. Her research topics include image processing, medical image
analysis, and data mining.
Bin Wu received his Ph.D. degree in software engineering from the
East China Normal University in 2010. He is currently an Associate
Professor of Shanghai University of Finance and Economics Zhejiang
College. His main research interests are theoretical and applied
machine learning, notably deep learning, for automatic reasoning and
computer vision.

