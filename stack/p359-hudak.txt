Conception, 
Evolution, 
and Application 
of Functional 
Programming 
Languages 
PAUL HUDAK 
Yale University, 
Department 
of Computer Science, New Haven, Connecticut 
06520 
The foundations 
of functional 
programming 
languages are examined from both historical 
and technical 
perspectives. 
Their evolution 
is traced through several critical 
periods: early 
work on lambda calculus and combinatory 
calculus, Lisp, Iswim, FP, ML, and modern 
functional 
languages such as Miranda’ 
and Haskell. 
The fundamental 
premises on which 
the functional 
programming 
methodology 
stands are critically 
analyzed with respect to 
philosophical, 
theoretical, 
and pragmatic 
concerns. Particular 
attention 
is paid to the 
main features that characterize 
modern functional 
languages: higher-order 
functions, 
lazy evaluation, 
equations and pattern 
matching, 
strong static typing and type inference, 
and data abstraction. 
In addition, 
current research areas-such 
as parallelism, 
nondeterminism, 
input/output, 
and state-oriented 
computations-are 
examined with 
the goal of predicting 
the future development 
and application 
of functional 
languages. 
Categories and Subject Descriptors: 
D.l.l 
[Programming 
Techniques]: 
Applicative 
(Functional) 
Programming; 
D.3.2 [Programming 
Languages]: 
Language 
Classifications-applicative 
languages; data-flow 
languages; nonprocedural 
languages; very 
high-level 
languages; F.4.1 [Mathematical 
Logic 
and Formal 
Languages]: 
Mathematical 
Logic-lambda 
calculus and related systems; K.2 [History 
of Computing]: 
Software 
General Terms: Languages 
Additional 
Key Words and Phrases: Data abstraction, 
higher-order 
functions, 
lazy 
evaluation, 
referential 
transparency, 
types 
INTRODUCTION 
The earliest programming 
languages were 
developed with one simple goal in mind: to 
provide a vehicle through 
which one could 
control the behavior of computers. Not sur- 
prisingly, 
the early languages reflected the 
structure of the underlying 
machines fairly 
well. 
Although 
at first 
blush 
that 
goal 
seems eminently 
reasonable, the viewpoint 
quickly changed for two very good reasons. 
First, it became obvious that what was easy 
for a machine 
to reason about was not 
necessarily easy for a human being to rea- 
son about. Second, as the number of differ- 
1 Miranda 
is a trademark 
of Research Software 
Ltd. 
ent kinds of machines increased, the need 
arose for a common language with which to 
program all of them. 
Thus 
from 
primitive 
assembly 
lan- 
guages (which were at least a step up from 
raw machine code) there grew a plethora of 
high-level 
programming 
languages, begin- 
ning with 
FORTRAN 
in the 1950s. The 
development 
of these languages 
grew so 
rapidly 
that by the 1980s they were best 
characterized 
by grouping them into fami- 
lies that reflected a common computation 
model or programming 
style. Debates over 
which 
language or family 
of languages is 
best will undoubtedly 
persist for as long as 
computers need programmers. 
Permission 
to copy without 
fee all or part of this material 
is granted provided 
that the copies are not made or 
distributed 
for direct commercial 
advantage, the ACM copyright 
notice and the title of the publication 
and its 
date appear, and notice is given that copying is by permission 
of the Association 
for Computing 
Machinery. 
To 
copy otherwise, 
or to republish, 
requires a fee and/or specific permission. 
0 1989 ACM 0360-0300/89/0900-0359 
$01.50 
ACM Computing Surveys, Vol. 21, No. 3, September 1989 

360 
. 
Paul Hudak 
CONTENTS 
INTRODUCTION 
Programming Language Spectrum 
Referential Transparency and Equational 
Reasoning 
Plan of Study 
1. EVOLUTION 
OF FUNCTIONAL 
LANGUAGES 
1.1 Lambda Calculus 
1.2 Lisp 
1.3 Iswim 
1.4 APL 
1.5 FP 
1.6 ML 
1.7 SASL, KRC, and Miranda 
1.8 Dataflow Languages 
1.9 Others 
1.10 Haskell 
2. DISTINGUISHING 
FEATURES OF MODERN 
FUNCTIONAL 
LANGUAGES 
2.1 Higher Order Functions 
2.2 Nonstrict Semantics (Lazy Evaluating) 
2.3 Data Abstraction 
2.4 Equations and Pattern Matching 
2.5 Formal Semantics 
3. ADVANCED FEATURES AND ACTIVE 
RESEARCH AREAS 
3.1 Overloading 
3.2 Purely Functional Yet Universal I/O 
3.3 Arrays 
3.4 Views 
3.5 Parallel Functional Programming 
3.6 Caching and Memoization 
3.7 Nondeterminism 
3.8 Extensions to Polymorphic-Type Inference 
3.9 Combining Other Programming Language 
Paradigms 
4. DISPELLING 
MYTHS ABOUT FUNCTIONAL 
PROGRAMMING 
5. CONCLUSIONS 
REFERENCES 
The class of functional, 
or applicative, 
programming 
languages, in which compu- 
tation 
is carried out entirely 
through 
the 
evaluation 
of expressions, is one such fam- 
ily of languages, and debates over its merits 
have been quite lively in recent years. Are 
functional 
languages 
toys? 
Or are they 
tools? Are they artifacts 
of theoretical 
fan- 
tasy or of visionary 
pragmatism? 
Will they 
ameliorate 
software woes or merely com- 
pound them? Whatever 
answers we might 
have for these questions, we cannot ignore 
the significant 
interest current researchers 
have in functional 
languages and the im- 
pact they have had on both the theory and 
pragmatics 
of programming 
languages in 
general. 
Among the claims made by functional 
language advocates are that programs can 
be written 
quicker, 
are more concise, are 
higher level (resembling 
more closely tra- 
ditional 
mathematical 
notation), 
are more 
amenable to formal reasoning and analysis, 
and can be executed more easily on parallel 
architectures. 
Of course, many of these fea- 
tures touch 
on rather 
subjective 
issues, 
which is one reason why the debates can be 
so lively. 
This paper gives the reader significant 
insight 
into the very essence of functional 
languages and the programming 
method- 
ology that they support. 
It starts with 
a 
discussion of the nature of functional 
lan- 
guages, followed by an historical 
sketch of 
their development, 
a summary of the dis- 
tinguishing 
characteristics 
of modern func- 
tional 
languages, 
and 
a discussion 
of 
current research areas. Through 
this study 
we will put into perspective both the power 
and weaknesses of the functional 
program- 
ming paradigm. 
A Note to the Reader: This paper as- 
sumes a good understanding 
of the funda- 
mental 
issues in programming 
language 
design and use. To learn more about mod- 
ern functional 
programming 
techniques, 
including 
the important 
ideas behind rea- 
soning about functional 
programs, refer to 
Bird and Wadler [1988] or Field and Har- 
rison [1988]. To read about how to imple- 
ment 
functional 
languages, 
see Peyton 
Jones 
[ 19871 (additional 
references 
are 
given in Sections 1.8 and 5). 
Finally, 
a comment on notation: 
Unless 
otherwise stated, all examples will be writ- 
ten in Haskell, 
a recently 
proposed func- 
tional 
language 
standard 
[Hudak 
and 
Wadler 19881. Explanations 
will be given 
in square brackets [ ] as needed.’ 
‘Since 
the Haskell 
Report 
is relatively 
new, some 
minor changes to the language may occur after this 
paper has appeared. An up-to-date 
copy of the Report 
may be obtained from the author. 
ACM Computing Surveys, Vol. 21, NO. 3, September 1989 

Functional 
Programming 
Languages 
361 
Programming 
Language 
Spectrum 
Imperative 
languages are characterized 
as 
having 
an implicit 
state that is modified 
(i.e., side effected) by constructs (i.e., com- 
mands) in the source language. As a result, 
such languages generally 
have a notion 
of 
sequencing 
(of the commands) 
to permit 
precise 
and 
deterministic 
control 
over 
the state. Most, including 
the most pop- 
ular, 
languages 
in 
existence 
today 
are 
imperative. 
As an example, 
the assignment 
state- 
ment is a (very common) command, since 
its effect is to alter the underlying 
implicit 
store so as to yield a different 
binding 
for 
a particular 
variable. 
The 
begin . . . end 
construct 
is the prototypical 
sequencer of 
commands, 
as are the well-known 
goto 
statement 
(unconditional 
transfer 
of con- 
trol), 
conditional 
statement 
(qualified 
se- 
quencer), and while loop (an example of a 
structured 
command). 
With 
these simple 
forms, we can, for example, compute the 
factorial 
of the number X: 
n:= x; 
a := 1; 
while n>O do 
begin a := a*n; 
n := n-l 
end; 
After execution 
of this program, the value 
of a in the implicit 
store will contain 
the 
desired result. 
In 
contrast, 
declarative 
languages 
are 
characterized 
as having 
no implicit 
state, 
and thus the emphasis is placed entirely 
on 
programming 
with expressions (or terms). 
In particular, 
functional 
languages are dec- 
larative languages whose underlying 
model 
of computation 
is the function 
(in contrast 
to, for example, the relation that forms the 
basis for logic programming 
languages). 
In a declarative 
language state-oriented 
computations 
are accomplished by carrying 
the state around explicitly 
rather than im- 
plicitly, 
and looping 
is accomplished 
via 
recursion 
rather than by sequencing. 
For 
example, the factorial of x may be computed 
in the functional 
language Haskell by 
fat x 1 
where fat n a 
= if n>O then fat (n-l) 
(a*n) 
else a 
in which the formal parameters n and a are 
examples of carrying 
the state around ex- 
plicitly, 
and the recursive 
structure 
has 
been arranged so as to mimic as closely as 
possible the looping 
behavior 
of the pro- 
gram given earlier. 
Note that the condi- 
tional 
in this program 
is an expression 
rather than command; that is, it denotes a 
value (conditional 
on the value of the pred- 
icate) 
rather 
than 
a sequencer 
of com- 
mands. Indeed the value of the program is 
the desired factorial, 
rather than it being 
found in an implicit 
store. 
Functional 
(in general, declarative) 
pro- 
gramming is often described as expressing 
what is being computed rather than how, 
although 
this is really a matter of degree. 
For example, the above program may say 
less about how factorial 
is computed than 
the imperative 
program given earlier, but 
is perhaps not as abstract as 
fat x 
where fat n 
= if n==O then 1 
else n*fac (n-l) 
[== 
is the infix 
operator 
for equality], 
which appears very much like the mathe- 
matical definition 
of factorial 
and is indeed 
a valid functional 
program. 
Since most languages have expressions, 
it is tempting 
to take our definitions 
liter- 
ally and describe functional 
languages via 
derivation 
from conventional 
programming 
languages: 
Simply 
drop 
the assignment 
statement 
and 
any 
other 
side-effecting 
primitives. 
This approach, of course, is very 
misleading. 
The result of such a derivation 
is usually 
far less than satisfactory, 
since 
the purely 
functional 
subset of most im- 
perative 
languages 
is 
hopelessly 
weak 
(although 
there are important 
exceptions, 
such as Scheme [Rees and Clinger 19861). 
Rather than saying what functional 
lan- 
guages don’t have, it is better to character- 
ize them by the features they do have. For 
modern 
functional 
languages, 
those fea- 
ACM Computing 
Surveys, Vol. 21, No. 3, September 
1989 

362 
. 
Paul Hudak 
tures include 
higher-order 
functions, 
lazy 
evaluation, 
pattern 
matching, 
and various 
kinds of data abstraction-all 
of these fea- 
tures will 
be described 
in detail 
in this 
paper. Functions 
are treated as first-class 
objects, are allowed to be recursive, higher 
order, and polymorphic, 
and in general are 
provided 
with mechanisms 
that ease their 
definition 
and use. Syntactically, 
modern 
functional 
languages 
have an equational 
look in which functions 
are defined using 
mutually 
recursive 
equations 
and pattern 
matching. 
This discussion suggests that what is im- 
portant 
is the 
functional 
programming 
style, in which the above features are man- 
ifest and in which side effects are strongly 
discouraged but not necessarily eliminated. 
This is the viewpoint 
taken, for example, 
by the ML community 
and to some extent 
the Scheme community. 
On the other hand, 
there is a very large contingency 
of purists 
in the functional 
programming 
community 
who believe 
that 
purely 
functional 
lan- 
guages are not only sufficient 
for general 
computing needs but are also better because 
of their “purity”. 
At least a dozen purely 
functional 
languages exist along with their 
implementations.3 
The main property 
that 
is lost when side effects are introduced 
is 
referential 
transparency; 
this loss in turn 
impairs equational 
reasoning, as described 
below. 
Referential 
Transparency 
and Equational 
Reasoning 
The emphasis on a pure declarative 
style of 
programming 
is perhaps the hallmark 
of 
the functional 
programming 
paradigm. The 
term referentially 
transparent 
is often used 
to describe this style of programming, 
in 
which “equals can be replaced by equals”. 
For example, consider the (Haskell) expres- 
sion 
. . . x+x . . . 
wherex=fa 
The function 
application 
(f a) may be sub- 
stituted 
for any free occurrence of x in the 
3 This situation 
forms an interesting 
contrast with the 
logic programming 
community, 
where Prolog is often 
described as declarative 
(whereas Lisp is usually not), 
and there are very few pure logic programming 
lan- 
guages (and even fewer implementations). 
Unlike 
many 
developments 
in computer 
science, functional 
languages have main- 
tained the principles 
on which they were 
4 In all fairness, 
there are logics for reasoning 
about 
imperative 
programs, such as those espoused by Floyd, 
Hoare, Dijkstra, 
and Wirth. 
None of them, however, 
exploits 
any notion of referential 
transparency. 
ACM Computing 
Surveys, Vol. 21, NO. 3, September 
1989 
scope created by the where expression, such 
as in the subexpression 
x+x. 
The same 
cannot generally 
be said of an imperative 
language, where we must first be sure that 
no assignment 
to x is made in any of the 
statements intervening 
between the initial 
definition 
of x and one of its subsequent 
uses.4 In general this can be quite a tricky 
task, for example, 
in the case in which 
procedures are allowed to induce nonlocal 
changes to lexically 
scoped variables. 
Although 
the notion of referential 
trans- 
parency may seem like a simple idea, the 
clean equational 
reasoning that it allows is 
very powerful, 
not only for reasoning 
for- 
mally about programs but also informally 
in writing 
and debugging programs. A pro- 
gram in which 
side effects are minimized 
but not eliminated 
may still benefit 
from 
equational 
reasoning, 
although 
naturally 
more care must be taken when applying 
such reasoning. 
The degree of care, how- 
ever, may be much higher than we might 
think 
at first: 
Most languages that allow 
minor forms of side effects do not minimize 
their locality lexically-thus 
any call to any 
function 
in any module might conceivably 
introduce 
a side effect, in turn invalidating 
many applications 
of equational 
reasoning. 
The perils of side effects are appreciated 
by the most experienced 
programmers 
in 
any language, although 
most are loathe to 
give them up completely. 
It remains 
the 
goal of the functional 
programming 
com- 
munity to demonstrate that we can do com- 
pletely 
without 
side 
effects, 
without 
sacrificing 
efficiency 
or modularity. 
Of 
course, as mentioned 
earlier, 
the lack of 
side effects is not all there is to the func- 
tional programming 
paradigm. As we shall 
soon see, modern functional 
languages rely 
heavily on certain other features, most no- 
tably higher-order 
functions, 
lazy evalua- 
tion, and data abstraction. 
Plan of Study 

Functional 
Programming 
Languages 
l 
363 
founded to a surprising 
degree. Rather than 
changing 
or compromising 
those 
ideas, 
modern functional 
languages are best clas- 
sified as embellishments 
of a certain set of 
ideals. It is a distinguishing 
feature of mod- 
ern functional 
languages that they have so 
effectively 
held on to pure mathematical 
principles 
in a way shared by very few other 
languages. 
Because of this, we can learn a great deal 
about 
functional 
languages 
simply 
by 
studying 
their 
evolution. 
On the 
other 
hand, such a study 
may fail to yield 
a 
consistent 
treatment 
of any one feature 
that 
is common 
to most functional 
lan- 
guages, for it will be fractured into its man- 
ifestations 
in each of the languages as they 
were historically 
developed. For this reason 
I have taken a three-fold 
approach to our 
study: 
First, 
Section 
1 provides 
an historical 
sketch of the development 
of functional 
languages. Starting 
with the lambda calcu- 
lus as the prototypical 
functional 
language, 
it 
gradually 
embellishes 
it 
with 
ideas 
as they were historically 
developed, lead- 
ing eventually 
to a reasonable 
technical 
characterization 
of 
modern 
functional 
languages. 
Next, 
Section 
2 presents 
a detailed 
discussion 
of four important 
concepts- 
higher-order 
functions, 
lazy 
evaluation, 
data abstraction 
mechanisms, 
and equa- 
tions/pattern 
matching-which 
are critical 
components 
of all modern functional 
lan- 
guages and are best discussed as indepen- 
dent topics. 
Section 3 discusses more advanced ideas 
and outlines 
some critical 
research areas. 
Then to round out the paper, Section 4 puts 
some of the limitations 
of functional 
lan- 
guages into perspective by examining 
some 
of the myths that have accompanied their 
development. 
1. EVOLUTION 
OF FUNCTIONAL 
LANGUAGES 
1.1 Lambda 
Calculus 
The development 
of functional 
languages 
has been influenced 
from time to time by 
many sources, but none is as paramount 
nor as fundamental 
as the work of Church 
[1932-1933, 
19411 on the lambda calculus. 
Indeed 
the 
lambda 
calculus 
is usually 
regarded as the first functional 
language, 
although 
it was certainly 
not thought 
of as 
programming 
language at the time, given 
that there were no computers on which to 
run the programs. 
In any case, modern 
functional 
languages can be thought 
of as 
(nontrivial) 
embellishments 
of the lambda 
calculus. 
It is often thought 
that the lambda cal- 
culus also formed the foundation 
for Lisp, 
but this in fact appears not to be the case 
[McCarthy 
19781. The 
impact 
of the 
lambda calculus on early Lisp development 
was minimal, 
and it has only been very 
recently that Lisp has begun to evolve more 
toward lambda calculus ideals. On the other 
hand, Lisp had a significant 
impact on the 
subsequent development 
of functional 
lan- 
guages, as will be discussed in Section 1.2. 
Church’s work was motivated 
by the de- 
sire to create a calculus (informally, 
a syn- 
tax for terms and set of rewrite 
rules for 
transforming 
terms) 
that 
captured 
one’s 
intuition 
about the behavior 
of functions. 
This approach is counter to the considera- 
tion of functions 
as, for example, sets (more 
precisely, 
sets of argument/value 
pairs), 
since the intent was to capture the compu- 
tational aspects of functions. 
A calculus is 
a formal way for doing just that. 
Church’s 
lambda calculus was the first 
suitable 
treatment 
of the computational 
aspects of functions. 
Its type-free 
nature 
yielded a particularly 
small and simple cal- 
culus, and it had one very interesting 
prop- 
erty, capturing 
functions 
in their 
fullest 
generality: 
Functions 
could be applied to 
themselves. In most reasonable theories of 
functions 
as sets, this is impossible, 
since 
it requires the notion 
of a set containing 
itself, resulting 
in well-known 
paradoxes. 
This ability of self-application 
is what gives 
the lambda calculus 
its power. It allows 
us to gain the effect of recursion 
without 
explicitly 
writing 
a recursive 
definition. 
Despite this powerful 
ability, 
the lambda 
calculus 
is consistent 
as a mathematical 
system-no 
contradictions 
or paradoxes 
arise. 
Because of the relative importance 
of the 
lambda 
calculus 
to the development 
of 
functional 
languages, I will describe it in 
ACM Computing 
Surveys, Vol. 21, No. 3, September 
1989 

364 
l 
Paul Hudak 
detail in the remainder of this section, using 
modern notational 
conventions. 
1.1.7 Pure Untyped Lambda Calculus 
The abstract 
syntax 
of the pure untyped 
lambda calculus (a name chosen to distin- 
guish 
it from 
other 
versions 
developed 
later) 
embodies 
what 
are called 
lambda 
expressions, defined by5 
x E Id 
Identifiers 
e E Exp 
Lambda expressions 
where e ::= x 1 el e2 I Xx.e 
Expressions 
of the form Xx.e are called 
abstractions 
and of the form 
(el e,) are 
called applications. 
It is the former 
that 
captures the notion 
of a function 
and the 
latter that captures the notion 
of applica- 
tion of a function. 
By convention, 
applica- 
tion is assumed to be left associative, 
so 
that (ei ez e3) is the same as ((ei en) e3). 
The rewrite rules of the lambda calculus 
depend on the notion of substitution 
of an 
expression ei for all free occurrences of an 
identifier 
x in an expression 
e2, which we 
write as [el/x]e2.6 Most systems, including 
both the lambda calculus and predicate cal- 
culus, that use substitution 
on identifiers 
must be careful 
to avoid name conflicts. 
Thus, although the intuition 
behind substi- 
tution 
is strong, its formal definition 
can 
be somewhat tedious. 
To understand 
substitution, 
we must 
first understand 
the notion of the free vari- 
ables of an expression e, which we write as 
fu(e) 
and define by the following 
simple 
rules: 
b(x) 
= 1x1 
fu(el e2) = fukl) 
U fuk2) 
fu(Xx.e) 
= fu(e) - (x) 
6 The notation 
d E D means that d is a typical element 
of the set D, whose elements may be distinguished 
by 
subscripting. 
In the case of identifiers, 
we assume that 
each xi is unique; that is, xi # zj if i # j. The notation 
d ::= altl ) ah2 ] . . ] altn is standard BNF syntax. 
6 In denotational 
semantics the notation 
e[u/x] is used 
to denote the function 
e’ that is just like e except that 
e’ x = U. Our notation 
of placing the brackets in front 
of the expression 
is to emphasize 
that 
[u/x]e 
is a 
syntactic 
transformation 
on the expression 
e itself. 
We say that x is free in e iff x E fu(e). 
The substitution 
[el/x]e2 
is then defined 
inductively 
by 
Wxlk2 
e3) = ([edxle2)([edxl4 
hXj.e2, 
ifi=j 
hX,..[edxile2, 
if i # j and xi $ fu(eJ 
hXk.[el/xi]([xklxjle2), 
otherwise, 
where k f i, k # j, 
and xk 4 fu (el) U fu (e2) 
The last rule is the subtle one, since it is 
where a name conflict 
could occur and is 
resolved by making 
a name change. The 
following 
example 
demonstrates 
applica- 
tion of all three rules: 
[y/x]((Xy.x)(Xx.x)x) 
= (Xz.y)(Xx.x)y 
To complete the lambda calculus, we de- 
fine three simple rewrite 
rules on lambda 
expressions: 
(1) ol-conversion 
(renaming): 
. 
Xxi.e H Xxj.[xj/xi]e, 
where xj 4 fu(e). 
(2) p-conversion 
(application): 
0-3 
k2 H 
k2lxh. 
(3) q-conversion: 
Xx.(e x) H e, 
if x 4 fu(e). 
These rules, together 
with 
the standard 
equivalence 
relation 
rules for reflexivity, 
symmetricity, 
and transitivity, 
induce 
a 
theory of convertibility 
on the lambda cal- 
culus, which can be shown to be consistent 
ACM Computing Surveys, Vol. 21, No. 3, September 1989 

Functional 
Programming 
Languages 
as a mathematical 
system.7 
The 
well- 
known 
Church-Rosser 
theorem 
[Church 
and Rosser 19361 (actually 
two theorems) 
is what embodies the strongest 
form of 
consistency 
and has to do with a notion of 
reduction, which is the same as convertibil- 
ity 
but 
restricted 
so that 
P-conversion 
and 
v-conversion 
only 
happen 
in 
one 
direction: 
l 
365 
sion. Obviously, 
we would 
like for that 
value to be unique; and we would like to be 
able to find 
it whenever 
it exists. 
The 
Church-Rosser 
theorems 
give us positive 
results for both of these desires. 
1.1.2 Church-Rosser 
Theorems 
Church-Rosser 
Theorem 
I 
(1) ,&reduction: 
(2) s-reduction: 
Xx.(e x) * 
e, 
if x 4 fu(e). 
We write e, & e2 if e2 can be derived from 
zero or more /3- or v-reductions 
or cu-con- 
versions; 
in other words 5 
is the reflex- 
ive, 
transitive 
closure 
of + 
including 
a-conversions. 
Similarly, 
& is the reflexive, 
transitive 
closure of w. In summary, 
* 
captures the notion of reducibility, 
and 45 
captures the notion of intraconvertibility. 
Definition 
A lambda expression 
is in normal form if 
it cannot be further 
reduced using p- or 
q-reduction. 
Note that some lambda expressions have 
no normal form, such as 
One consequence 
of this result is that 
how we arrive 
at the normal 
form does 
not matter; that is, the order of evaluation 
is irrelevant 
(this 
has important 
conse- 
quences for parallel 
evaluation 
strategies). 
The question then arises as to whether 
or 
not it is always possible to find the normal 
form (assuming 
it exists). We begin with 
some definitions. 
(Xx. b x)1 ox. (x x)), 
Definition 
where the only possible reduction 
leads to 
an identical 
term, and thus the reduction 
process is nonterminating. 
Nevertheless, 
the normal 
form appears 
to be an attractive 
canonical 
form for a 
term, has a clear sense of finality 
in a 
computational 
sense, and is what we intu- 
itively 
think 
of as the value of an expres- 
A normal-order 
reduction 
is a sequential 
reduction in which, whenever there is more 
than 
one reducible 
expression 
(called 
a 
reder), the leftmost 
one is chosen first. In 
contrast, an applicative-order 
reduction is a 
sequential 
reduction 
in which the leftmost 
innermost 
redex is chosen first. 
Church-Rosser 
Theorem 
II 
If e. & el and e, is in normal form, then 
there exists a normal-order 
reduction from 
e. to el. 
7 The lambda calculus as we have defined it here is 
what Barendregt 
[1984] calls the XKq-calculus 
and is 
slightly 
more general than Church’s 
original 
XK-cal- 
culus (which did not include p-conversion). 
Further- 
more, Church originally 
showed the consistency 
of the 
XI-calculus 
[Church 
19411, an even smaller subset (it 
only allowed abstraction 
of x from e if x was free in 
e). We will ignore the subtle differences 
between these 
calculi-our 
version 
is the one most often discussed 
in the literature 
on functional 
languages. 
If e. & e, then there exists an e2 such that 
e. A e2 and el + e2.’ 
In other words, if e. and el are intracon- 
vertible, then there exists a third term (pos- 
sibly the same as e. or el) to which they 
can both be reduced. 
Corollary 
No lambda expression can be converted to 
two distinct 
normal forms (ignoring 
differ- 
ences due to a-conversion). 
a Church and Rosser’s original proofs of their theorems 
are rather long, and many have tried to improve 
on 
them since. The shortest proof I am aware of for the 
first theorem is fairly 
recent and aptly due to Rosser 
119821. 
ACM Computing Surveys, Vol. 21, No. 3, September 1989 

366 
l 
Paul Hudak 
This is a very satisfying 
result; it says 
that if a normal form exists, we can always 
find it; that is, just use normal-order 
reduc- 
tion. To see why applicative-order 
reduc- 
tion is not always adequate, consider the 
following 
example: 
Applicative-order 
reduction 
(Xx. y)((Xx. 
x x) (Xx. x x)) 
4 (Xx. y)((Xx. 
x x)(Xx. x Lx)) 
a* 
Normal-order 
reduction 
(Xx.y) ((Xxxx) 
(Xx.xx)) 
*Y 
We will 
return 
to the trade-offs 
between 
normal- 
and applicative-order 
reduction 
in Section 
2.2. For now we simply 
note 
that the strongest completeness 
and con- 
sistency 
results 
have been achieved with 
normal-order 
reduction. 
In actuality, 
one of Church’s 
(and oth- 
ers’) motivations 
for developing the lambda 
calculus 
in the first place was to form a 
foundation 
for all of mathematics 
(in the 
way that, for example, set theory is claimed 
to provide 
such a foundation). 
Unfortu- 
nately, all attempts 
to extend the lambda 
calculus sufficiently 
to form such a foun- 
dation failed to yield a consistent 
theory. 
Church’s 
original 
extended 
system 
was 
shown inconsistent 
by the Kleene-Rosser 
paradox [Kleene and Rosser 19351; a sim- 
pler inconsistency 
proof 
is embodied 
in 
what is known as the Curry paradox [Ros- 
ser 19821. The only consistent systems that 
have been derived from the lambda calculus 
are much too weak to claim as a foundation 
for mathematics, 
and the problem remains 
open today. 
These inconsistencies, 
although 
disap- 
pointing 
in a foundational 
sense, did not 
slow down research on the lambda calculus, 
which turned out to be quite a nice model 
of functions 
and of computation 
in general. 
The Church-Rosser 
theorem 
was an ex- 
tremely 
powerful 
consistency 
result for a 
computation 
model, and in fact rewrite sys- 
tems completely 
different 
from the lambda 
calculus are often described as “possessing 
the 
Church-Rosser 
property” 
or 
even 
anthropomorphically 
as being 
Church- 
Rosser. 
1.1.3 Recursion, X-Definability, 
and 
Church’s Thesis 
Another 
nice property 
of the lambda cal- 
culus is embodied in the following 
theorem: 
Fixpoint 
Theorem 
Every lambda expression e has a fixpoint 
e’ 
such that (e e’) & e’. 
Proof. 
Take e ’ to be (Y e), where Y, 
known as the Y combinator, 
is defined by 
Y = Xf.(Xx.f (x x))(hx.f (x x)) 
Then we have 
(Ye) = (Xx.e(x x))(Ax.e(x 
x)) 
= e((Xx.e(x 
x))(Xx.e(x 
x))) 
= e(Y e) 
This 
surprising 
theorem 
(and equally 
surprising 
simple proof) is what has earned 
Y the name “paradoxical 
combinator”. 
The 
theorem is quite significant-it 
means that 
any recursive function 
may be written 
non- 
recursively 
(and nonimperatively)., 
To see 
how, consider a recursive function f defined 
by 
fF 
. . . f . . . 
This could be rewritten 
as 
f = (Xf. ... f .**)f 
where the inner 
occurrence 
of f is now 
bound. This equation essentially 
says that 
f is a fixpoint 
of the lambda expression 
(Xf. * * * f e. s). But that is exactly what Y 
computes for us, so we arrive at the follow- 
ing nonrecursive 
definition 
for f: 
f = Y(Xf. 
-0. f .**) 
As a concrete example, the factorial 
func- 
tion 
fat = Xn. 
if (n = 0) then 1 else (n * fac(n - 1)) 
can be written 
nonrecursively 
as 
fat = Y(hfac. 
An. 
if (n = 0) then 1 else (n * fac(n - 1))) 
ACM Computing 
Surveys, Vol. 21, No. 3, September 
1989 

Functional 
Programming 
Languages 
l 
367 
The ability 
of the lambda 
calculus 
to 
simulate recursion in this way is the key to 
its power and accounts for its persistence 
as a useful model of computation. 
Church 
recognized this power, and is perhaps best 
expressed in his now famous thesis: 
Church’s 
Thesis 
Effectively 
computable functions 
from posi- 
tive integers 
to positive 
integers are just 
those definable in the lambda calculus. 
This 
is quite a strong claim. Although 
the notion of functions 
from positive inte- 
gers to positive 
integers can be formalized 
precisely, the notion of effectively 
comput- 
able cannot; thus no proof can be given for 
the thesis. It gained support, however, from 
Kleene 
[1936] who in 1936 showed that 
X-definability 
was precisely 
equivalent 
to 
Godel and Herbrand’s 
notions of recursive- 
ness. Meanwhile, 
Turing 
[1936] had been 
working 
on his now famous Turing 
ma- 
chine, and in 1937 [Turing 
19371 he showed 
that Turing 
computability 
was also pre- 
cisely equivalent 
to X-definability. 
These 
were quite satisfying 
results.’ 
The 
lambda 
calculus 
and the Turing 
machine were to have profound impacts on 
programming 
languages and computational 
complexity,1° 
respectively, 
and computer 
science in general. This influence was prob- 
ably much greater than Church or Turing 
could have imagined, which is perhaps not 
surprising 
given that 
computers 
did not 
even exist yet. 
In parallel 
with the development 
of the 
lambda 
calculus, 
Schonfinkel 
and Curry 
were busy founding 
combinatory 
logic. It 
was Schonfinkel 
[ 19241 who discovered the 
surprising 
result that any function 
could be 
‘Much 
later 
Post 
[1943] and Markov 
[1951] pro- 
posed two other 
formal 
notions 
of effective 
com- 
putability; 
these also were shown to be equivalent 
to X-definability. 
lo Although 
the lambda calculus and the notion of h- 
definability 
predated the Turing 
machine, complexity 
theorists 
latched 
onto the Turing 
machine 
as their 
fundamental 
measure of decidability. 
This is probably 
because of the appeal of the Turing 
machine 
as a 
machine, 
giving 
it more credibility 
in the emerging 
arena of electronic 
digital 
computers. 
See Trakhten- 
brot [1988] for an interesting 
discussion of this issue. 
expressed as the composition 
of only two 
simple functions, 
K and S. Curry 
119301 
proved the consistency 
of a pure combina- 
tory calculus, 
and with 
Feys [Curry 
and 
Feys 19581 elaborated the theory consider- 
ably. Although 
this work deserves as much 
attention 
from a logician’s point of view as 
the lambda calculus, and in fact its origins 
predate that of the lambda calculus, we will 
not pursue it here since it did not contribute 
directly 
to the development 
of functional 
languages 
in the way that 
the lambda 
calculus did. On the other hand, the com- 
binatory 
calculus 
was eventually 
to play 
a surprising 
role in the implementation 
of functional 
languages, 
beginning 
with 
Turner 
[1979] and summarized 
in Peyton 
Jones [1987, Chapter 161. 
Another 
noteworthy 
attribute 
of the 
lambda calculus is its restriction 
to func- 
tions of one argument. 
That it suffices to 
consider only such functions 
was first sug- 
gested by Frege in 1893 [van Heijenoort 
19671 and independently 
by Schonfinkel 
in 
1924. This restriction 
was later exploited 
by Curry and Feys [ 19581, who used the 
notation (f x y) to denote ((f 3~) y), which 
previously 
would have been written f (x, y). 
This notation 
has become known as cur- 
rying, and f is said to be a curried function. 
As we will see, the notion 
of currying 
has 
carried over today as a distinguishing 
syn- 
tactic characteristic 
of modern functional 
languages. 
There are several variations 
and embel- 
lishments of the lambda calculus. They will 
be mentioned in the discussion of the point 
at which 
functional 
languages 
exhibited 
similar characteristics. 
In this way we can 
clearly 
see the relationship 
between 
the 
lambda calculus and functional 
languages. 
1.2 Lisp 
A discussion of the history of functional 
languages would certainly 
be remiss if it 
did not include a discussion of Lisp, begin- 
ning with McCarthy’s 
seminal work in the 
late 1950s. 
Although 
lambda calculus 
is often con- 
sidered 
as the 
foundation 
of Lisp, 
by 
McCarthy’s 
[ 19781 own account the lambda 
calculus actually played a rather small role. 
ACM Computing Surveys, Vol. 21, No. 3, September 1989 

368 
l 
Paul Hudak 
Its main impact came through 
McCarthy’s 
desire to represent functions 
anonymously, 
and Church’s 
h-notation 
was what 
he 
chose: A lambda abstraction 
written 
Xx.e in 
lambda calculus would be written 
(lambda 
(x) e) in Lisp. 
Beyond that, the similarity 
wanes. For 
example, rather than use the Y combinator 
to express recursion, 
McCarthy 
invented 
the conditional 
expression” 
with which re- 
cursive functions 
could be defined explicitly 
(and, arguably, 
more intuitively). 
As an 
example, the nonrecursive 
factorial 
func- 
tion given in the lambda calculus in Section 
1.1.3 would be written 
recursively 
in Lisp 
in the following 
way: 
(define fat (n) 
(if (= n 0) 
t* n Vat (- n 1))) )) 
This 
and other 
ideas were described 
in 
two landmark 
papers in the early 1960s 
[McCarthy 
1960; 19631 that inspired work 
on Lisp for many years to come. 
McCarthy’s 
original 
motivation 
for de- 
veloping 
Lisp was the desire for an alge- 
braic 
list-processing 
language 
for use in 
artificial 
intelligence 
research. 
Although 
symbolic 
processing 
was a fairly 
radical 
idea at the time, his aims were quite prag- 
matic. One of the earliest attempts 
at de- 
signing such a language was suggested by 
McCarthy 
and resulted 
in FLPL 
(FOR- 
TRAN-compiled 
list processing language), 
implemented 
in 1958 on top of the FOR- 
TRAN 
system on the IBM 704 [Gelernter 
et al. 19601. During 
the next 
few years 
McCarthy 
designed, 
refined, 
and imple- 
mented Lisp. His chief contributions 
dur- 
ing this period were the following: 
(1) The conditional 
expression and its use 
in writing 
recursive functions. 
(2) The use of lists and higher-order 
oper- 
ations over lists such as mapcar. 
(3) The central idea of a cons cell and the 
use of garbage collection 
as a method 
of reclaiming 
unused cells. 
I1 The conditional 
in FORTRAN 
(essentially 
the only 
other programming 
language in existence at the time) 
was a statement, 
not an expression, 
and was for con- 
trol, not value-defining, 
purposes. 
(4) The use of S-expressions 
(and abstract 
syntax 
in general) 
to represent 
both 
program and data.” 
All four of these features are essential in- 
gredients 
of any Lisp implementation 
to- 
day; 
the 
first 
three 
are 
essential 
to 
functional 
language 
implementations 
as 
well. 
A simple example of a typical 
Lisp defi- 
nition 
is the following 
one for mapcar: 
(define mapcar (fun 1st) 
(if (null 1st) 
nil 
(cons (fun (car 1st)) 
(mapcar fun (cdr 
1st))) 1) 
This example demonstrates all of the points 
mentioned 
above. Note that the function 
fun is passed as an argument 
to mapcar. 
Although 
such higher-order 
programming 
was very well known 
in lambda calculus 
circles, it was certainly 
a radical departure 
from FORTRAN 
and has become one of 
the most important 
programming 
tech- 
niques in Lisp and functional 
programming 
(higher order functions 
are discussed more 
in Section 
2.1). The primitive 
functions 
cons, car, cdr, and null are the well-known 
operations 
on lists whose names are still 
used today. 
cons creates a new list cell 
without 
burdening 
the user with 
explicit 
storage management; 
similarly, 
once that 
cell is no longer needed a “garbage collec- 
tor” will come along and reclaim it, again 
without 
user involvement. 
For example, 
since mapcar constructs 
a new list from an 
old one, in the call 
(mapcar fun (cons a (cons b nil))) 
the list (cons a (cons b nil)) 
will become 
garbage after the call and will automatically 
be reclaimed. Lists were to become the par- 
adigmatic 
data structure 
in Lisp and early 
functional 
languages. 
The definition 
of mapcar in a modern 
functional 
language such as Haskell would 
appear 
similarly, 
except 
that 
pattern 
matching 
would be used to destructure 
the 
I2 Interestingly, 
McCarthy 
[1978] claims that it was 
the read and print 
routines 
that 
influenced 
this 
notation 
most. 
ACM Computing 
Surveys, Vol. 21, No. 3, September 
1989 

Functional 
Programming 
Languages 
369 
list: 
mapcar fun [ ] 
= [I 
mapcar fun (xxs) 
= fun x : mapcar fun xs 
[ [ ] is the null list and : is the infix operator 
for cons; also note that function 
application 
has higher 
precedence 
than 
any 
infix 
operator.] 
McCarthy 
was also interested 
in design- 
ing a practical 
language, and thus Lisp had 
many 
pragmatic 
features-in 
particular, 
sequencing, the assignment statement, and 
other primitives 
that induced side effects 
on the store. Their 
presence undoubtedly 
had much to do with early experience with 
FORTRAN. 
Nevertheless, 
in his early pa- 
pers, McCarthy 
emphasized the mathemat- 
ical elegance of Lisp, and in a much later 
paper his student Cartwright 
demonstrated 
the 
ease with 
which 
one could 
prove 
properties 
about 
pure 
Lisp 
programs 
[ Cartwright 
19761. 
Despite its impurities, 
Lisp had a great 
influence 
on functional 
language develop- 
ment, and it is encouraging 
to note that 
modern Lisps (especially Scheme) have re- 
turned 
more to the purity 
of the lambda 
calculus 
rather 
than 
the ad hocery that 
plagued the Maclisp 
era. This 
return 
to 
purity 
includes the first-class 
treatment 
of 
functions 
and the lexical scoping of identi- 
fiers. Furthermore, 
the preferred 
modern 
style of Lisp 
programming, 
such as es- 
poused by Abelson 
et al. [ 19851, can be 
characterized 
as being predominantly 
side- 
effect free. And, finally, 
note that Hender- 
son’s [1980] Lispkit 
Lisp is a purely func- 
tional 
version 
of Lisp that uses an infix, 
algebraic syntax. 
1.2.1 Lisp in Retrospect 
Before continuing 
the historical 
develop- 
ment it is helpful 
to consider some of the 
design decisions McCarthy 
made and how 
they would be formalized 
in terms of the 
lambda calculus. It may seem that condi- 
tional expressions, for example, are an ob- 
vious feature to have in a language, but that 
only reflects our familiarity 
with 
modern 
high-level 
programming 
languages, most of 
which have them. In fact the lambda cal- 
culus version of the factorial example given 
in the previous 
section used a conditional 
(not to mention 
arithmetic 
operators), yet 
most readers probably 
understood 
it per- 
fectly and did not object to the departure 
from precise lambda calculus syntax. 
The effect of conditional 
expressions can 
in fact be achieved in the lambda calculus 
by encoding 
the true and false values as 
functions, 
as well as by defining 
a function 
to emulate the conditional: 
true = hx.Ay.x 
false = Xx.Ay.y 
cond = Xp.Xc.Xa.(p c a) 
In other words, (cond p c a) = (if p then c 
else a). One can then define, for example, 
the factorial 
function 
by 
fuc = An. cond (= n 0) 1 (* n( fuc (- n 1))) 
where = is defined by 
(=nn) 
*true 
(= n m) + false, 
if m # n 
where m and n range over the set of integer 
constants. 
However, 
I am still cheating 
a 
bit by not explaining 
the nature 
of the 
objects -, *, 0, 1, and so on, in pure lambda 
calculus terms. It turns out that they can 
be represented in a variety of ways, essen- 
tially using functions to simulate the proper 
behavior, 
just as for true, false, and the 
conditional 
(for the details, 
see Church 
[1941]). In fact any conventional 
data or 
control 
structure 
can be simulated 
in the 
lambda calculus; if this were not the case, 
it would 
be difficult 
to believe Church’s 
thesis. 
Even if McCarthy 
knew of these ways to 
express things in the lambda calculus (there 
is reason to believe that he did not), effi- 
ciency concerns might have rapidly led him 
to consider 
other 
alternatives, 
especially 
since FORTRAN 
was the only high-level 
programming 
language with which anyone 
had any experience. 
In particular, 
FOR- 
TRAN 
functions 
evaluated 
their 
argu- 
ments 
before entering 
the body 
of the 
function, 
resulting 
in what is often called a 
strict, 
or call-by-value, 
evaluation 
policy, 
corresponding 
roughly to applicative-order 
reduction in the lambda calculus. With this 
strategy extended to the primitives, 
includ- 
ing the conditional, 
we cannot easily define 
ACM Computing 
Surveys, Vol. 21, No. 3, September 
1989 

370 
l 
Paul Hudak 
recursive 
functions. 
For example, 
in the 
above definition 
of factorial 
all three argu- 
ments to cond would be evaluated, includ- 
ingfac (- n l), resulting in nontermination. 
Nonstrict 
evaluation, 
corresponding 
to 
the normal-order 
reduction that is essential 
to the lambda calculus 
in realizing 
recur- 
sion, was not very well understood 
at the 
time-it 
was not at all clear how to imple- 
ment it efficiently 
on a conventional 
von 
Neumann 
computer-and 
we would have 
to wait another 20 years or so before such 
an implementation 
was even attempted.13 
The 
conditional 
expression 
essentially 
allowed 
one to invoke 
normal-order, 
or 
nonstrict, 
evaluation 
selectively. 
Stated 
another 
way, 
McCarthy’s 
conditional, 
although 
an expression, 
was compiled into 
code that essentially 
controlled 
the reduc- 
tion 
process. Most 
imperative 
program- 
ming languages today that allow recursion 
do just that, and thus even though 
such 
languages are often referred 
to as strict, 
they 
all rely 
critically 
on at least one 
nonstrict 
construct: 
the conditional. 
1.2.2 Lambda Calculus with Constants 
The conditional 
expression is actually only 
one example of very many primitive 
func- 
tions that were included 
in Lisp. Rather 
than explain them in terms of the lambda 
calculus by a suitable encoding 
(i.e., com- 
pilation), 
it is perhaps better to extend the 
lambda calculus formally 
by adding a set of 
constants along with a set of what are usu- 
ally called &rules, which state relationships 
between constants 
and effectively 
extend 
the basis set of a-, p-, and a-reduction 
rules. 
For example, the reduction rules for = given 
earlier 
(and repeated below) 
are &rules. 
This new calculus, often called the lambda 
calculus 
with 
constants, 
can be given 
a 
precise abstract syntax: 
x E Id 
Identifiers 
c E Con 
Constants 
e E Exp 
Lambda expressions 
where e ::= x 1 c 1 el e2 1 Xx.e 
In On the other 
hand, the call-by-name 
evaluation 
strategy 
invented 
in ALGOL 
had very 
much of a 
normal-order 
reduction 
flavor. See Wadsworth 
[1971] 
and Wegner 
[1968] for early 
discussions 
of these 
issues. 
ACM Computing 
Surveys, Vol. 21, No. 3, September 
1989 
for which various &rules apply, such as the 
following: 
(= 0 0) + True 
(= 0 1) + False 
(+ 0 0) * 
0 
(+ 0 1) * 
1 
(+ 27 32) + 59 
(If True el e2) * 
e, 
(If False el e2) + e2 
(Car (Cons el e2)) * 
el 
(Cdr (Cons el e2)) * 
e2 
where =, +, 0, 1, If, True, False, Cons, and 
so on, are elements of Con. 
The above rules can be shown to be a 
conservative 
extension 
of the lambda cal- 
culus, a technical 
term that in our context 
essentially 
means that convertible 
terms in 
the original 
system are still 
convertible 
in the new, and (perhaps more importantly) 
inconvertible 
terms in the original 
system 
are still inconvertible 
in the new. In gen- 
eral, care must be taken when introducing 
d-rules, since all kinds of inconsistencies 
could arise. For a quick and dirty example 
of inconsistency, 
we might define a primi- 
tive function 
over integers 
called broken 
with the following 
d-rules: 
(broken 0) * 
0 
(broken 0) 4 
1 
which immediately 
implies that more than 
one normal 
form exists for some terms, 
violating 
the first Church-Rosser 
theorem 
(see Section 1.1). 

Functional 
Programming 
Languages 
l 
371 
As a more subtle example, suppose we 
define the logical relation 
Or by 
(Or True e) 4 True 
(Or e True) 4 True 
(Or False False) + Fake 
Although 
these rules form a conservative 
extension 
of the lambda calculus, 
a com- 
mitment 
to evaluate 
either 
of the argu- 
ments may lead to nontermination 
(even if 
the other argument 
may reduce to True). 
In fact, it can be shown that with the above 
rules there does not exist a deterministic 
sequential 
reduction 
strategy 
that 
will 
guarantee that the normal form True will 
be found for all terms having such a normal 
form, and thus the second Church-Rosser 
property 
is violated. 
This version of Or is 
often called the parallel or, since a parallel 
reduction 
strategy is needed to implement 
it 
properly 
(and 
with 
which 
the 
first 
Church-Rosser 
theorem will at least hold). 
Alternatively, 
we could define a sequential 
or by 
significant 
syntactic 
and semantics 
ideas 
[Landin 
19661. Iswim, according to Landin, 
“can be looked on as an attempt to deliver 
Lisp from its eponymous 
commitment 
to 
lists, its reputation 
for hand-to-mouth 
stor- 
age allocation, 
the hardware dependent fla- 
vor of its pedagogy, its heavy bracketing, 
and its compromises with tradition.” 
When 
all is said and done, the primary 
contri- 
butins 
of Iswim, 
with 
respect 
to 
the 
development 
of functional 
languages, are 
the following: 
(1) Syntactic 
innovations 
(4 
(b) 
The abandonment 
of prefix syntax 
in favor of infix. 
The introduction 
of let and where 
clauses, including 
a notion 
of si- 
multaneous and mutually 
recursive 
definitions. 
(c) 
(Or True e) + True 
(Or False e) + e 
The use of an off-side rule based 
on indentation 
rather than sepa- 
rators (such as commas or semi- 
colons) to scope declarations 
and 
expressions. 
For example 
(using 
Haskell syntax), the program frag- 
ment 
which 
can 
be shown 
to 
satisfy 
both 
e where f x = x 
Church-Rosser 
theorems. 
ab=l 
1.3 lswim 
Historically 
speaking, Peter Landin’s 
work 
in the mid 1960s was the next significant 
impetus 
to the functional 
programming 
paradigm. Landin’s 
work was deeply influ- 
enced by that of Curry 
and Church. 
His 
early 
papers 
discussed 
the 
relationship 
between 
lambda 
calculus 
and both 
ma- 
chines and high-level 
languages 
(specifi- 
cally ALGOL 
60). Landin 
[1964] discussed 
how one could mechanize the evaluation 
of 
expressions 
through 
an abstract 
machine 
called the SECD machine; in Landin 
[ 19651 
he formally 
defined a nontrivial 
subset of 
ALGOL 60 in terms of the lambda calculus. 
cannot be confused with 
e where f x = x a 
b=l 
and is equivalent 
to what might 
otherwise be written 
as 
e where {f x = x; a b = 1) 
It is apparent from his work that Landin 
regarded highly the expressiveness and pu- 
rity of the lambda calculus and at the same 
time recognized its austerity. 
Undoubtedly 
as a result of this work, in 1966 Landin 
introduced 
a language (actually 
a family of 
languages) 
called Iswim 
(for If You See 
What I Mean), which included a number of 
(2) Semantic innovations 
(4 
An emphasis on generality. 
Landin 
was half serious in hoping that the 
Iswim 
family 
could 
serve as the 
“next 700 programming 
languages.” 
Central to his strategy was the idea 
of defining 
a syntactically 
rich lan- 
guage in terms of a very small but 
expressive core language. 
(b) 
An emphasis on equational 
reason- 
ing (i.e., the ability to replace equals 
with equals). This elusive idea was 
backed up with four sets of rules for 
reasoning 
about expressions, 
dec- 
larations, 
primitives, 
and problem- 
oriented extensions. 
ACM Computing 
Surveys, Vol. 21, No. 3, September 
1989 

372 
. 
Paul Hudak 
(c) 
The SECD 
machine 
as a simple 
abstract 
machine 
for 
executing 
functional 
programs. 
We can think 
of Landin’s 
work as ex- 
tending the lambda calculus with constants 
defined in the last section so as to include 
more primitives, 
each with its own set of 6- 
rules, but more importantly 
let and where 
clauses, for which it is convenient 
to intro- 
duce the syntactic category of declarations: 
e E Exp 
Expressions 
where e ::= . + . ] e where dl . . . d, 
1 let dI . . . d, in e 
d E Decl 
Declarations 
where d ::= x = e 
1 xx1 --. xn=e 
and for which we then need to add some 
axioms (i.e., reduction 
rules) to capture the 
desired semantics. Landin proposed special 
constructs 
for defining 
simultaneous 
and 
mutually 
recursive definitions, 
but we will 
take a simpler and more general approach 
here: We assume that 
a block of decla- 
rations 
dI 
-. . d, always 
is potentially 
mutually 
recursive-if 
it isn’t, 
our rules 
still work: 
(let d, . . . d, in e) 
+ (e where dI - - . d,) 
(xx1 a.+ x,=e) 
* 
(x = Xx1.Xx2. . . - Xx,.e) 
(e where x1 = el) 
3 (Xx,.e)( YXxl.el) 
(e where (x, = ei) . . . (x, = e,)) 
=a (Xxl.e)(YXxl.el) 
where x2 = (Xxl.ez)(YXrl.e,) 
x, = (Xx1. e,)(YXxl.el) 
These rules are semantically 
equivalent 
to 
Landin’s, 
but they avoid the need for a 
tupling 
operator 
to handle mutual 
recur- 
sion and they use the Y combinator 
(de- 
fined in Section 1.1) instead of an iterative 
unfolding 
step. 
We will 
call this resulting 
system the 
recursive lambda calculus with constants, or 
just recursive lambda calculus. 
Landin’s 
emphasis on expressing 
what 
the desired result is, as opposed to saying 
how to get it, and his claim that Iswim’s 
declarative14 style of programming 
was bet- 
ter than 
the incremental 
and sequential 
imperative 
style were ideas to be echoed by 
functional 
programming 
advocates to this 
day. On the other hand, it took another 10 
years before 
interest 
in functional 
lan- 
guages was to be substantially 
renewed. 
One of the reasons is that there were no 
decent implementations 
of Iswim-like 
lan- 
guages around; this reason, in fact, was to 
persist into the 1980s. 
1.4 APL 
Iverson’s 
[1962] 
APL, 
although 
not 
a 
purely 
functional 
programming 
language, 
is worth mentioning 
because its functional 
subset is an example 
of how we could 
achieve 
functional 
programming 
without 
relying 
on lambda 
expressions. 
In fact, 
Iverson’s design of APL was motivated 
out 
of his desire to develop an algebraic pro- 
gramming language for arrays, and his orig- 
inal work used an essentially 
functional 
notation. 
Subsequent development 
of APL 
resulted in several imperative 
features, but 
the underlying 
principles 
should 
not be 
overlooked. 
APL was also unique in its goal of suc- 
cinctness, 
in that 
it used a specially 
de- 
signed alphabet 
to represent 
programs- 
each letter corresponding 
to one operator. 
That APL became popular 
is apparent 
in 
the fact that 
many 
keyboards, 
both 
for 
typewriters 
and computer 
terminals, 
car- 
ried the APL alphabet. Backus’ FP, which 
came after APL, was certainly 
influenced 
by the APL philosophy, 
and its abbreviated 
publication 
form also used a specialized 
alphabet 
(see the example in Section 1.5). 
In fact FP has much in common with APL, 
the primary 
difference being that FP’s fun- 
damental 
data structure 
is the sequence, 
whereas APL’s is the array. 
It is worth 
noting 
that recent work on 
APL 
has revived 
some of APL’s 
purely 
functional 
foundations. 
The most notable 
I4 Landin 
actually 
disliked 
the term 
“declarative,” 
preferring 
instead “denotative.” 
ACM Computing 
Surveys, Vol. 21, No. 3, September 
1989 

Functional 
Programming 
Languages 
l 
373 
work is that of Tu [Tu 1986; Tu and Perlis 
19861, who designed a language called FAC, 
for Functional 
Array 
Calculator 
(presum- 
ably a take off on Turner’s 
KRC, 
Kent 
Recursive 
Calculator). 
FAC 
is a purely 
functional 
language that adopts most of the 
APL syntax 
and programming 
principles 
but also has special features to allow pro- 
gramming 
with 
infinite 
arrays; naturally, 
lazy evaluation 
plays a major role in the 
semantics of the language. Another 
inter- 
esting approach is that of Mullin 
[1988]. 
1.5 FP 
Backus’ FP was one of the earliest func- 
tional 
languages to receive widespread 
at- 
tention. 
Although 
most of the features in 
FP are not found in today’s modern func- 
tional 
languages, 
Backus’ 
[ 19781 Turing 
Award lecture was one of the most influ- 
ential 
and now most-often 
cited papers 
extolling 
the functional 
programming 
par- 
adigm. It not only stated quite eloquently 
why functional 
programming 
was “good” 
but also quite vehemently 
why traditional 
imperative 
programming 
was 
Ubad,?.15 
Backus’ coined the term “word-at-a-time 
programming” 
to capture 
the essence of 
imperative 
languages, 
showed 
how such 
languages were inextricably 
tied to the von 
Neumann 
machine, and gave a convincing 
argument 
why such languages 
were not 
going to meet the demands of modern soft- 
ware development. 
That this argument was 
being made by the person who is given the 
most credit for designing FORTRAN 
and 
who also had significant 
influence 
on the 
development 
of ALGOL 
led substantial 
weight to the functional 
thesis. The expo- 
sure given to Backus’ paper was one of the 
best things that could have happened to the 
field of functional 
programming, 
which at 
the time was certainly 
not considered main- 
stream. 
Despite the great impetus Backus’ paper 
gave to functional 
programming, 
it is inter- 
esting 
to note that 
in the same paper 
Backus also said that languages based on 
lambda calculus would have problems, both 
i5 Ironically, 
the Turing 
Award was given to Backus 
in a large part because of his work on FORTRAN. 
in implementation 
and expressiveness, 
be- 
cause the model was not suitably 
history 
sensitive (more specifically, 
it did not han- 
dle large data structures 
such as databases 
very easily). With 
regard to implementa- 
tion, this argument is certainly 
understand- 
able because it was not 
clear 
how 
to 
implement 
the notion of substitution 
in an 
efficient 
manner 
nor was it clear how to 
structure data in such ways that large data 
structures could be implemented 
efficiently 
(both of these issues are much better under- 
stood today). 
With 
regard to expressive- 
ness, that 
argument 
is still 
a matter 
of 
debate today. In any case, these problems 
were the motivation 
for Backus’ Applica- 
tive State Transition 
(AST) 
Systems, in 
which state is introduced 
as something 
on 
which purely functional 
programs interact 
with in a more traditional 
(i.e., imperative) 
way. 
Perhaps more surprising, 
and an aspect 
of the paper that 
is usually 
overlooked, 
Backus 
had this 
to say about 
lambda- 
calculus based systems: 
. 
An FP system is founded on the use of a fixed set 
of combining 
forms called functional 
forms. 
. . In 
contrast, a lambda-calculus 
based system is founded 
on the use of the lambda expression, 
with an asso- 
ciated 
set of substitution 
rules for variables, 
for 
building 
new functions. 
The 
lambda 
expression 
(with 
its substitution 
rules) is capable of defining 
all possible 
computable 
functions 
of all possible 
types and of any number of arguments. 
This free- 
dom and power has its disadvantages 
as well as its 
obvious advantages. 
It is analogous to the power of 
unrestricted 
control statements in conventional 
lan- 
guages: with unrestricted 
freedom comes chaos. If 
one constantly 
invents new combining 
forms to suit 
the occasion, as one can in the lambda calculus, one 
will 
not become familiar 
with 
the style or useful 
properties 
of the few combining 
forms 
that 
are 
adequate for all purposes. 
Backus’ 
argument, 
of course, was in the 
context of garnering support for FP, which 
had a small set of combining 
forms that 
were claimed to be sufficient 
for most pro- 
gramming applications. 
One of the advan- 
tages of this approach is that each of these 
combining 
forms could be named with par- 
ticular brevity, 
and thus programs become 
quite compact-this 
was exactly 
the ap- 
proach taken by Iverson in designing APL 
ACM Computing Surveys, Vol. 21, No. 3, September 1989 

374 
l 
Paul Hudak 
(see Section 1.4). For example, an FP pro- 
gram for inner product looks like 
Def IP = (/+) 0 (ax) 
0 Trans 
where /, 0, and LY are combining 
forms 
called insert, compose, and apply-to-all, 
re- 
spectively. In a modern functional 
language 
such as Haskell this would be written 
with 
slightly 
more verbosity 
as 
ip 11 12 = fold1 (+) 0 (map2 (*) 11 12) 
[In Haskell an infix operator such as + may 
be passed as an argument 
by surrounding 
it in parentheses.] 
Here fold1 is the equiv- 
alent of insert (/), and map2 is a two-list 
equivalent 
of apply-to-all 
(a), thus elimi- 
nating the need for Trans. These functions 
are predefined 
in Haskell, 
as they are in 
most modern functional 
languages, for the 
same reason that Backus argues-they 
are 
commonly used. If they were not, they could 
easily be defined by the user. For example, 
we may wish to define an infix composition 
operator for functions, 
the first of which is 
binary, as follows: 
(f.0.g) 
XY =f(gxy) 
[Note how infix 
operators may be defined 
in Haskell; operators are distinguished 
lex- 
ically by being nonalphabetic.] 
With 
this 
we can reclaim much of FP’s succinctness 
in defining 
ipp: 
lp = fold1 (+) 0 .o. map2 (*) 
[Recall that in Haskell, 
function 
applica- 
tion has higher precedence than infix 
op- 
erator application.] 
It is for this reason, 
together with the fact that FP’s specializa- 
tion precluded 
the generality 
afforded by 
user-defined 
higher order functions 
(which 
is all that combining 
forms are), that mod- 
ern functional 
languages did not follow the 
FP style. As we shall soon see, certain other 
kinds of syntactic 
sugar became more pop- 
ular instead (such as pattern matching, 
list 
comprehensions, 
and sections). 
Many extensions 
to FP have been pro- 
posed over the years, including 
the inclu- 
sion of strong typing and abstract datatypes 
[Guttag et al. 19811. In much more recent 
work, Backus et al. [1986] have designed 
the language 
FL, which 
is strongly 
(al- 
though 
dynamically) 
typed and in which 
higher 
order functions 
and user-defined 
datatypes 
are allowed. 
Its 
authors 
still 
emphasize the algebraic style of reasoning 
that is paramount 
in FP, although 
it is 
also given a denotational 
semantics that is 
probably 
consistent 
with 
respect to the 
algebraic semantics. 
1.6 ML 
In the mid 197Os, at the same time Backus 
was working 
on FP at IBM, 
several re- 
search 
projects 
were 
underway 
in 
the 
United Kingdom 
that related to functional 
programming, 
most notably work at Edin- 
burgh. There Gordon et al. [ 19791 had been 
working 
on 
a proof-generating 
system 
called LCF for reasoning 
about recursive 
functions, 
in particular 
in the context 
of 
programming 
languages. The system con- 
sisted of a deductive 
calculus 
called PPX 
(polymorphic 
predicate 
calculus) 
together 
with an interactive 
programming 
language 
called 
ML, 
for 
metalanguage 
(since 
it 
served as the command language for LCF). 
LCF is quite interesting 
as a proof sys- 
tem, but its authors 
soon found that ML 
was also interesting 
in its own right, and 
they proceded to develop it as a stand-alone 
functional 
programming 
language [Gordon 
et al. 19781. That it was, and still is, called 
a functional 
language is actually 
somewhat 
misleading, 
since it has a notion 
of refer- 
ences that are locations that can be stored 
into and read from, much as variables are 
assigned and read. Its I/O system also in- 
duces side effects and is not referentially 
transparent. 
Nevertheless, 
the style of pro- 
gramming 
that it encourages is still func- 
tional, and that is the way it was promoted 
(the same is somewhat 
true for Scheme, 
although to a lesser extent). 
More recently 
a standardization 
effort 
for ML has taken place, in fact taking some 
of the good ideas of Hope [Burstall 
et al. 
19801 (such as pattern 
matching) 
along 
with it, yielding a language now being called 
Standard 
ML, 
or 
SML 
[Milner 
1984; 
Wikstrom 
19881. 
ML is a fairly 
complete language-cer- 
tainly 
the most practical 
functional 
lan- 
guage at the time it appeared-and 
SML is 
even more so. It has higher order functions, 
ACM Computing 
Surveys, Vol. 21, No. 3, September 
1989 

Functional 
Programming 
Languages 
l 
375 
a simple I/O facility, 
a sophisticated 
mod- 
ule system, and even exceptions. But by far 
the most significant 
aspect of ML 
is its 
type system, which is manifested in several 
ways: 
(1) 
(2) 
(3) 
(4) 
It is strongly and statically 
typed. 
It uses type inference to determine the 
type of every expression, instead of re- 
lying on excplicit 
type declarations. 
It allows 
polymorphic 
functions 
and 
data structures; 
that is, functions 
may 
take arguments 
of arbitrary 
type if in 
fact the function 
does not depend on 
that 
type 
(similarly 
for data struc- 
tures) . 
It has user-defined 
concrete and ab- 
stract datatypes (an idea actually 
bor- 
rowed from Hope and not present in 
the initial 
design of ML). 
ML was the first language to use type in- 
ference as a semantically 
integrated 
com- 
ponent 
of the language, and at the same 
time 
its 
type 
system 
was richer 
than 
any previous 
statically 
typed language in 
that 
it permitted 
true polymorphism. 
It 
seemed that the best of two worlds 
had 
been achieved-not 
only is making explicit 
type declarations 
(a sometimes burdensome 
task) not required, but in addition 
a pro- 
gram that successfully 
passes the type in- 
ferencer is guaranteed not to have any type 
errors. Unfortunately, 
this idyllic picture is 
not completely 
accurate 
(although 
it is 
close), as we shall soon see. 
We shall next discuss the issue of types 
in a foundational 
sense, thus continuing 
our plan of elaborating 
the lambda calculus 
to match the language features being dis- 
cussed. This will 
lead us to a reasonable 
picture of ML’s Hindley-Milner 
type sys- 
tem, the rich polymorphic 
type system that 
was mentioned 
above and that was later 
adopted 
by every other 
statically 
typed 
functional 
language, 
including 
Miranda 
and Haskell. 
Aside from the type system, 
the two most novel features in ML are its 
references and modules, which are also cov- 
ered in this section. 
Discussion 
of ML’s 
data abstraction 
facilities 
will be postponed 
until Section 2.3. 
1.6.1 Hindley-Milner 
Type System 
We can introduce 
types 
into 
the pure 
lambda calculus by first introducing 
a do- 
main of basic types, say BasTyp, as well as 
a domain of derived types, say Typ, and 
then 
requiring 
that 
every expression 
be 
tagged with a member of Typ, which we do 
by superscripting, 
as in e’. The derived 
type r2 + 71 denotes the type of all func- 
tions from values of type 72 to values of 
type TV, and thus a proper application 
will 
have the form e;P-r’ eF)T1. Modifying 
the 
pure lambda calculus in this way, we arrive 
at the pure typed lambda calculus: 
bg 
Basic types 
TE 
Derived types 
where T ::= b 1 71 + 72 
xT E Id 
Typed identifiers 
e E Exp 
Typed lambda expressions 
where e ::= x7 
e;2-Tle;z)T1 
1 ~Xx’2.e’l)‘*-rl 
for which 
we then provide 
the following 
reduction 
rules: 
(1) Typed-oc-conversion: 
(xx;1 . eT) w (Xx;l . [x;‘/xi1]e7), 
where xi’ 4 fu(e’). 
(2) Typed-P-conversion: 
((xxrz . e;l)e;l) 
et3 [e;2/xT2]e;‘. 
(3) Typed-s-converson: 
Xx” . (e’*x’l) 
-3 eT*, if x7’ @ fu(e’?). 
To preserve type correctness, 
we assume 
that the typed identifiers 
xrl and x12, where 
71 # TV, are distinct 
identifiers. 
Note then 
how every expression 
in our new calculus 
carries with it its proper type, and thus type 
checking is built in to the syntax. 
Unfortunately, 
there is a serious problem 
with this result: Our new calculus has lost 
the power of X-definability. 
In fact, every 
term in the pure typed lambda calculus can 
be shown to be strongly normalizable, mean- 
ing each has a normal 
form, and there 
is an effective procedure for reducing each 
of them to its normal form [Fortune 
et al. 
ACM Computing 
Surveys, Vol. 21, No. 3, September 
1989 

376 
l 
Paul Hudak 
19851. Consequently, 
we can only compute 
a rather 
small subset of functions 
from 
integers to integers-namely, 
the extended 
polynomials 
[Barendregt 
19841 .I6 
The reader can gain some insight 
into 
the problem by trying 
to write the defini- 
tion of the Y combinator-that 
paradoxical 
entity 
that 
allowed 
us to express recur- 
sion-as 
a typed term. The difficulty 
lies 
in properly 
typing 
self-application 
(recall 
the discussion in Section l.l), 
since typing 
(ee) requires 
that 
e have both the type 
72 + pi and r2, which cannot be done within 
the structure 
we have given. It can, in fact, 
be shown that the pure typed lambda cal- 
culus has no fixpoint 
operator. 
Fortunately, 
there is a clever way to solve 
this dilemma. 
Instead 
of relying 
on self- 
application 
to implement 
recursion, simply 
add to the calculus 
a family 
of constant 
fixpoint 
operators similar to Y, only typed. 
To do this, we first move into the typed 
lambda calculus with constants, in which a 
domain of constants Con is added as in the 
(untyped) 
lambda calculus with constants. 
We then include 
in Con a typed fixpoint 
operator of the form Y, for every type 7, 
where 
Then for each fixpoint 
operator Y, we add 
the &rule: 
Typed- Y-conversion: 
( Y7eT+r)T * 
(e’“( 
Y,eT’T)T)T 
The reader may easily verify that type con- 
sistency is maintained 
by this rule. 
By ignoring the type information, 
we can 
see that the above &rule corresponds to the 
conversion 
(Yf) w (f (Yf )) in the untyped 
case, and the same trick for implementing 
recursion 
with 
Y as discussed in Section 
1.1 can be used here, thus regaining 
X- 
definability. 
For example, 
a nonrecursive 
definition 
of the typed factorial 
function 
would be the same as the untyped 
version 
I6 On the bright side, some researchers view the strong 
normalization 
property 
as a feature, 
since it means 
that all programs are guaranteed to terminate. 
Indeed 
this property 
forms the basis of much of the recent 
work on using constructive 
type theory as a foundation 
for programming 
languages. 
ACM 
Computing 
Surveys, 
Vol. 
21, No. 
3, September 
1989 
given earlier, except that Y, would be used 
where 7 = Int + Int. 
In addition to this calculus, we can derive 
a typed recursive lambda calculus with con- 
stants in the same way that we did in the 
untyped 
case, except that 
instead of the 
unrestricted 
Y combinator 
we use the typed 
versions defined above. 
At this point our type system is about on 
par with that of a strongly 
and explicitly 
typed language such as Pascal or Ada. We 
would, however, like to do better. As men- 
tioned in Section 1.6, the designers of ML 
extended the state of the art of type systems 
in two significant 
ways: 
l They permitted 
polymorphic 
functions. 
l They used type inference to infer types 
rather 
than requiring 
that they be de- 
clared explicitly. 
As an example of a polymorphic 
function, 
consider 
map :: (a + 
b) -3 [a] + 
[b] 
mwf[l=[l 
mapf(x:xs) 
=fx:mapfxs 
The first line is a type signature 
that de- 
clares the type of map; it can be read as 
“for all types a and b, map is a function 
that takes two arguments, 
a function 
from 
a into b and a list of elements of type a, 
and returns a list of elements of type b.” 
[Type signatures are optional in Haskell; 
the type system is able to infer them auto- 
matically. 
Also note that 
the type con- 
structor 
+ 
is right 
associative, 
which 
is 
consistent 
with 
the left 
associativity 
of 
function 
application.] 
Therefore, 
map can be used to map 
square down a list of integers, or head down 
a list of lists, and so on. In a monomorphic 
language such as Pascal, one would have to 
define a separate function 
for each of these. 
The advantage of polymorphism 
should be 
clear. 
One way to achieve polymorphism 
in our 
calculus is to add a domain of type variables 
and extend the domain 
of derived types 
accordingly: 
b E BasTyp 
Basic types 
v E TypId 
Type variables 
7 E TYP 
Derived types 
where 7 ::= b I u I ~14 
72 

Functional 
Programming 
Languages 
Thus, for example, u -+ 7 is a proper type 
and can be read as if the type variable was 
universally 
quantified: 
“for all types u, the 
type u + 7.” To accommodate this we must 
change the rule for P-conversion 
to read 
377 
signatures 
present. The use of such poly- 
morphism 
however, is limited 
to the scope 
in which 
map was defined. For example, 
the program 
(2) 
silly map f g 
where f 
:: Int -+ Int 
g 
:: Char ---) Char 
map :: (a + 6) + bl + PI 
silly m f g = (m f num-list, 
m g char-list) 
Typed-P-conversion 
with 
type 
vari- 
ables: 
(a) 
((XxTz.ez) @ ([e;a/xrs]e;‘)‘l 
(b) 
((xx”.e;l)ep) 
@ ([~2/ul([e~/x”le;l))‘~ 
where substitution 
on type variables is de- 
fined in the obvious way. Note that this 
rule implies the validity 
of expressions 
of 
the form (ey+T1e2) TV. Similar 
changes are 
required to accommodate expressions such 
as (e ;-“e;) 
“. 
But, alas, now that type variables 
have 
been introduced, 
it 
is no longer 
clear 
whether a program is properly typed-it 
is 
not built in to the static syntactic structure 
of the calculus. In fact, the type-checking 
problem for this calculus is undecideable, 
being a variant 
of a problem 
known 
as 
partial polymorphic 
type inference [Boehm 
1985; Pfenning 
19881. 
Rather than trying to deal with the type- 
checking problem directly, we might go one 
step further 
with 
our calculus 
and try to 
attain ML’s lack of a requirement 
for ex- 
plicit typing. We can achieve this by simply 
erasing all type annotations 
and then trying 
to solve the seemingly 
harder problem 
of 
inferring 
types of completely 
naked terms. 
Surprisingly, 
it is not known whether 
an 
effective type inference algorithm 
exists for 
this calculus, even though the problem of 
partial polymorphic 
type inference, known 
to be undecidable, 
seems as if it should be 
easier. 
Fortunately, 
Hindley 
[ 19691 and Milner 
[1978] 
independently 
discovered 
a re- 
stricted 
polymorphic 
type system that is 
almost as rich as that provided by our cal- 
culus and for which type inference is decid- 
able. In other words, there exist certain 
terms in the calculus presented above that 
one could argue are properly 
typed but 
would 
not be allowed 
in the Hindley- 
Milner 
system. The system still allows pol- 
ymorphism, 
such as exhibited 
by map de- 
fined earlier, and is able to infer the type 
of functions 
such as map without 
any type 
[(el, e2) is a tuple] results in a type error, 
since map is passed as an argument 
and 
then instantiated 
in two different 
ways; 
that is, once as type (Int -+ Int) + [Int] + 
[Int] 
and once as type (Char + Char) + 
[Char] + [Char]. If map were instantiated 
in several ways within 
the scope in which 
it was defined or if m were only instantiated 
in one way within 
the function 
silly, there 
would have been no problem. 
This example demonstrates 
a fundamen- 
tal limitation 
to the Hindley-Milner 
type 
system, but in practice 
the class of pro- 
grams that the system rejects is not large 
and is certainly 
smaller than that rejected 
by any existing type-checking 
algorithm 
for 
conventional 
languages in that, if nothing 
else, it allows polymorphism. 
Many other 
functional 
languages have since then incor- 
porated what amounts to a Hindley-Milner 
type 
system, 
including 
Miranda 
and 
Haskell. It is beyond the scope of this ar- 
ticle to discuss the details of type inference, 
but the reader may find good pragmatic 
discussions in Hancock 
[1987] and Damas 
and Milner 
[1982] and a good theoretical 
discussion in Milner 
[ 19781. 
As a final comment we point out that an 
alternative 
way to gain polymorphism 
is to 
introduce 
types as values and give them at 
least some degree of first-class 
status (as 
we did earlier for functions); 
for example, 
allowing 
them to be passed as arguments 
and returned 
as results. Allowing 
them to 
be passed as arguments only (and then used 
in type annotations 
in the standard way) 
results in what is known as the polymorphic 
or second-order 
typed 
lambda 
calculus. 
Girard [1972] and Reynolds [1974] discov- 
ered and studied this type system indepen- 
dently, 
and 
it 
appears 
to 
have 
great 
expressive power. It turns out, however, to 
be essentially 
equivalent 
to the system we 
ACM Computing 
Surveys, Vol. 21, No. 3, September 
1989 

378 
l 
Paul Hudak 
developed earlier and has the same diffi- 
culties with respect to type inference. It is, 
nevertheless, 
an active area of current 
re- 
search (see Cardelli and Wegner [ 19851 and 
Reynolds [ 19851 for good summaries of this 
work). 
1.6.2 ML ‘s References 
A reference is essentially 
a pointer to a cell 
containing 
values of a particular 
type; ref- 
erences are created by the (pseudo)function 
ref. For example, 
ref 5 evaluates 
to an 
integer reference-a 
pointer 
to a cell that 
is allowed to contain 
only integers and in 
this case having 
initial 
contents 
5. The 
contents 
of a cell can be read using the 
prefix operator !. Thus if x is bound to ref 
5 then !r returns 5. 
The cell pointed 
to by a reference may 
be updated via assignment using the infix 
operator :=. Continuing 
with the above ex- 
ample, x := 10, although an expression, has 
the side effect of updating 
the cell pointed 
to by x with the value 10. Subsequent eval- 
uations of !x will then return 10. Of course, 
to make the notion 
of subsequent 
well- 
defined, 
it is necessary to introduce 
se- 
quencing 
constructs; 
indeed ML even has 
an iterative 
while construct. 
References in ML amount to assignable 
variables 
in a conventional 
programming 
language and are only notable in that they 
can be included 
within 
a type structure 
such as Hindley-Milner’s 
and can be rele- 
gated to a minor role in a language that is 
generally proclaimed 
as being functional. 
A 
proper 
treatment 
of references 
within 
a 
Hindley-Milner 
type system can be found 
in Tofte [1988]. 
1.6.3 Modules 
Modules 
in ML are called structures 
and 
are essentially 
reified 
environments. 
The 
type of a structure 
is captured in its signa- 
ture and contains all of the static properties 
of a module that are needed by some other 
module that might use it. The use of one 
module by another 
is captured by special 
functions 
called functors 
that map struc- 
tures to new structures. 
This capability 
is 
sometimes called a parameterized 
module 
or generic package. 
For example, a new signature called SIG 
(think of it as a new type) may be declared 
by 
signature SIG = 
sig 
val 
x : int 
val succ : int + int 
end 
in which the types of two identifiers 
have 
been declared, x of type int and succ of type 
int + int. The following 
structure 
S (think 
of it as an environment) 
has the implied 
signature SIG defined above: 
structure S = 
struct 
val n = 5 
val succ x = x+1 
end 
If we then define the following 
functor 
F 
(think of it as a function 
from structures to 
structures): 
functor F( 2’: SIG) = 
struct 
valy= 
TX+ 
1 
val add2 x: = T.succ(Z’.su~~(x)) 
end 
then the new structure 
declaration 
structure U = F(S) 
is equivalent 
to having written 
structure 
U = 
struct 
valy=x+l 
val add2 x = succ(succ(~)) 
val x: = 5 
val succ x = x+1 
end 
except that the signature 
of U does not 
include bindings 
for x and succ (i.e., they 
are hidden). 
Although 
seemingly 
simple, 
the 
ML 
module facility 
has one very noteworthy 
feature: Structures 
are (at least partially) 
first 
class in that 
functor 
take them as 
arguments 
and return 
them as values. A 
more conservative 
design (such as adopted 
in Miranda 
and Haskell) 
might require all 
modules to be named, thus relegating them 
to second-class status. Of course, this first- 
class treatment 
has to be ended somewhere 
if type checking is to remain effective, and 
ACM Computing 
Surveys, Vol. 21, No. 3, September 
1989 

Functional 
Programming 
Languages 
l 
379 
in ML that is manifested 
in the fact that 
structures 
can be passed to functors 
only 
(e.g., they cannot be placed in lists), the 
signature 
declaration 
of a functor’s 
argu- 
ment is mandatory 
in the functor 
declara- 
tion, and functors themselves are not first 
class. 
It is not clear whether this almost-first- 
class treatment 
of structures 
is worth the 
extra complexity, 
but the ML module facil- 
ity is certainly 
the most sophisticated 
of 
those found in existing functional 
program- 
ming languages, and it is achieved with no 
loss of static type-checking 
ability, 
includ- 
ing the fact that modules may be compiled 
independently 
and later linked via functor 
application. 
1.7 SASL, KRC, and Miranda 
At the same time ML and FP were being 
developed, David Turner, 
first at the Uni- 
versity 
of St. Andrews 
and later at the 
University 
of Kent, was busy working 
on 
another 
style of functional 
languages re- 
sulting 
in a series of three languages that 
characterize 
most faithfully 
the modern 
school of functional 
programming 
ideas. 
More than any other researcher, 
Turner 
[ 1981,1982] argued eloquently 
for the value 
of lazy evaluation, 
higher order functions 
and the use of recursion 
equations 
as a 
syntactic 
sugaring for the lambda calculus. 
Turner’s 
use of recurrence 
equations 
was 
consistent with Landin’s argument 10 years 
earlier, as well as Burge’s [1975] excellent 
treatise 
on recursive 
programming 
tech- 
niques 
and 
Burstall 
and 
Darlington’s 
[ 19771 work on program 
transformation. 
But the prodigious use of higher order func- 
tions 
and lazy evaluation, 
especially 
the 
latter, was something 
new and was to be- 
come a hallmark 
of modern functional 
pro- 
gramming techniques. 
In 
the 
development 
of 
SASL 
(St. 
Andrews 
Static Language) 
[Turner 
19761, 
KRC (Kent Recursive Calculator) 
[Turner 
19811, and 
Miranda17 
[Turner 
19851, 
Turner concentrated 
on making things eas- 
ier on the programmer, 
and thus he intro- 
duced various sorts of syntactic 
sugar. In 
I’ Miranda 
is one of the few (perhaps the only) func- 
tional languages to be marketed commercially. 
particular, 
using SASL’s syntax for equa- 
tions gave programs a certain mathematical 
flavor, since equations were deemed appli- 
cable 
through 
the 
use of guards 
and 
Landin’s 
notion 
of the off-side 
rule was 
revived. For example, this definition 
of the 
factorial 
function 
fat n = 1, 
n=O 
= n * fac(n-l), 
n>O 
looks a lot like the mathematical 
version 
fat n = 
{ 
1 
ifn=O 
n * fac(n - 1) 
if n > 0 
(In Haskell this program would be written 
with slightly 
different 
syntax as 
fat n(n==O= 
1 
1 C-0 
= n*fac(n-1) 
More on equations 
and pattern 
matching 
may be found in Section 2.4.) 
Another 
nice aspect of SASL’s 
equa- 
tional style is that it facilitates 
the use of 
higher 
order functions 
through 
currying. 
For example, if we define 
addxy=x+y 
then “add” 1 is a function 
that adds 1 to its 
argument. 
KRC is an embellishment 
of SASL pri- 
marily through 
the addition 
of ZF expres- 
sions (which 
were intended 
to resemble 
Zemelo-Frankel 
set abstraction 
and whose 
syntax 
was originally 
suggested by John 
Darlington), 
as well as various other short- 
hands for lists (such as [a . . b] to denote 
the list of integers from a to b, and [a . .] to 
denote the infinite 
sequence starting 
with 
a). For example (using Haskell syntax), 
[ x*x 1 x c [l . . loo], odd(x) ] 
is the list of squares of the odd numbers 
from 1 to 100 and is similar to 
(x2 1 x E (1, 2, . . . , 100) A odd(x)} 
except that the former is a list, the latter is 
a set. In fact Turner 
used the term set 
abstraction as synonymous 
with ZF expres- 
sion, but in fact both 
terms 
are some- 
what misleading 
since the expressions 
ac- 
tually denote lists, not true sets. The more 
ACM Computing Surveys, Vol. 21, No. 3, September 1989 

380 
l 
Paul Hudak 
popular 
current 
term is list comprehen- 
sion,” 
which 
is what 
is used in the re- 
mainder 
of this paper. As an example of 
the power of list comprehensions, 
here is 
a concise 
and perspicuous 
definition 
of 
quicksort: 
4s[ 1 
=[I 
qs (x:xs) = qs [y 1 ytrs,y<3c 
] ++ [x] ++ 
PIY I Y-wY>=xl 
[+-t is the infix append operator.] 
Miranda 
is in turn an embellishment 
of 
KRC, primarily 
in its treatment 
of types: 
It is strongly typed, using a Hindley-Milner 
type system, and it allows user-defined 
con- 
crete and abstract datatypes (both of these 
ideas were presumably 
borrowed from ML; 
see Sections 1.6 and 1.6.1). One interesting 
innovation 
in syntax in Miranda 
is its use 
of sections 
(first 
suggested 
by Richard 
Bird), which are a convenient 
way to con- 
vert partially 
applied infix 
operators 
into 
functional 
values. For example, the expres- 
sions (+), (x+), and (+x) correspond to the 
functions f, g, and h, respectively, 
defined 
by 
fxy=x+y 
i?Y 
= x+y 
hy 
=y+x 
In part because of the presence of sections, 
Miranda 
does not 
provide 
syntax 
for 
lambda 
abstractions. 
(In 
contrast, 
the 
Haskell 
designers 
chose to have lambda 
abstractions 
and thus chose not to have 
sections.) 
Turner 
was perhaps the foremost 
pro- 
ponent of both higher-order 
functions 
and 
lazy evaluation, 
although 
the ideas origi- 
nated 
elsewhere. 
Discussion 
of both 
of 
these topics is delayed until 
Sections 2.1 
and 2.2, respectively, 
where they are dis- 
cussed in a more general context. 
1.8 Dataflow 
Languages 
In the area of computer architecture, 
begin- 
ning 
predominantly 
with 
the 
work 
of 
Dennis’ 
and 
Misuras 
[1974] 
the 
early 
1970s there arose the notion 
of dataflow, 
a computer 
architecture 
organized 
solely 
” A term popularized 
by Philip Wadler. 
around the data dependencies in a program, 
resulting 
in high degrees of parallelism. 
Since data dependencies 
were paramount 
and artificial 
sequentiality 
was objectiona- 
ble, the languages designed to support such 
machines were essentially 
functional 
lan- 
guages, although 
historically 
they 
have 
been called dataflow languages. In fact they 
do have a few distinguishing 
features, typ- 
ically 
reflecting 
the idiosyncrasies 
of the 
dataflow 
architecture 
(just as imperative 
languages reflect the von Neumann 
archi- 
tecture): They are typically 
first order (re- 
flecting 
the 
difficulty 
in 
constructing 
closures in the dataflow 
model), strict (re- 
flecting the data-driven 
mode of operation 
that was most popular and easiest to imple- 
ment), and in certain 
cases do not even 
allow recursion 
(reflecting 
Dennis’ original 
static dataflow 
design, rather than, for ex- 
ample, 
Arvind’s 
dynamic 
tagged-token 
model [Arvind 
and Gostelow 1977; Arvind 
and Kathail 
19811). A good summary 
of 
work 
on 
dataflow 
machines, 
at 
least 
through 
1981, can be found in Treleaven 
et 
al. [1982]; 
more 
recently, 
see Vegdahl 
[1989]. 
The two most important 
dataflow 
lan- 
guages developed 
during 
this 
era were 
Dennis et al.‘s Val [Ackerman 
and Dennis 
1979; McGraw 
19821, and Arvind 
and 
Gostelow’s 
Id [1982]. More recently, 
Val 
has evolved into 
SISAL 
[McGraw 
et al. 
19831 and Id into 
Id Nouveau 
[Nikhiel 
et al. 19861. The former has retained much 
of the strict 
and first-order 
semantics 
of 
dataflow languages, whereas the latter has 
many 
of the features 
that 
characterize 
modern functional 
languages. 
Keller’s 
FGL 
[Keller 
et al. 19801 and 
Davis’ DDN [Davis 19781 are also notable 
developments 
that accompanied a flurry of 
activity 
on dataflow 
machines at the Uni- 
versity of Utah in the late 70’s. Yet another 
interesting 
dataflow 
language is Ashcroft 
and Wadge’s Lucid 
[Ashcroft 
and Wadge 
1976a, 197613; Wadge and Ashcroft 
19851 
[McGraw 
et al. 19831 whose distinguishing 
feature is the use of identifiers 
to represent 
streams of values (in a temporal, 
dataflow 
sense), thus allowing 
the expression 
of it- 
eration 
in a rather 
concise manner. 
The 
authors also developed an algebra for rea- 
soning about Lucid programs. 
ACM Computing Surveys, Vol. 21, No. 3, September 1989 

Functional 
Programming 
Languages 
381 
1.9 Others 
In the late 1970s and early 1980s a surpris- 
ing number of other modern functional 
lan- 
guages appeared, most in the context 
of 
implementation 
efforts. 
These 
included 
Hope at Edinburgh 
University 
[Burstall 
et 
al. 19801, FEL at Utah [Keller 
19821, Lazy 
ML 
(LML) 
at 
Chalmers 
[Augustsson 
19841, Alfl at Yale [Hudak 
19841, Ponder 
at Cambridge 
[Fairbairn 
19851, Orwell 
at 
Oxford [Wadler and Miller 
19881, Daisy at 
Indiana 
[Johnson 
N.d.] 
Twentel 
at the 
University 
of Twente 
[Kroeze 19871, and 
Tui at Victoria 
University 
[Boutel 19881. 
Perhaps the most notable of these lan- 
guages was Hope, 
designed 
and imple- 
mented by Rod Burstall, 
David MacQueen, 
and Ron Sannella at Edinburgh 
University 
[ 19801. Their 
goal was “to produce a very 
simple programming 
language which 
en- 
courages 
the 
production 
of 
clear 
and 
manipulable 
programs.” 
Hope is strongly 
typed, allows polymorphism, 
but requires 
explicit 
type declarations 
as part 
of all 
function 
definitions 
(which also allowed a 
useful form of overloading). 
It has lazy lists 
but otherwise is strict. It also has a simple 
module facility. 
But perhaps the most sig- 
nificant 
aspect of Hope is its user-defined 
concrete datatypes and the ability 
to pat- 
tern match against them. ML, in fact, did 
not originally 
have these features; 
they 
were borrowed from Hope in the design of 
SML. 
To quote Bird and Wadler 
[1988], this 
proliferation 
of functional 
languages was “a 
testament 
to the vitality 
of the subject,” 
although 
by 1987 there existed so many 
functional 
languages that there truly was a 
Tower of Babel, and something 
had to be 
done. The funny 
thing was, the semantic 
underpinnings 
of these 
languages 
were 
fairly consistent, 
and thus the researchers 
in the field had very little 
trouble 
under- 
standing each other’s programs, so the mo- 
tivation 
within 
the research community 
to 
standardize 
on a language was not high. 
Nevertheless, 
in September 1987 a meet- 
ing was held at the FPCA Conference 
in 
Portland, 
Oregon, to discuss the problems 
that this proliferation 
of languages was cre- 
ating. There was a strong consensus that 
the general use of modern, nonstrict 
func- 
tional 
languages was being hampered 
by 
the lack of a common language. Thus it was 
decided that a committee should be formed 
to design such a language, providing 
faster 
communication 
of new 
ideas, 
a stable 
foundation 
for real applications 
develop- 
ment, and a vehicle through 
which 
other 
people would be encouraged to learn and 
use functional 
languages. The result of that 
committee’s 
effort was a purely functional 
programming 
language 
called 
Haskell 
[Hudak 
and Wadler 
19881, named after 
Haskell 
B. 
Curry, 
and 
described 
in 
Section 1.10. 
1.10 Haskell 
Haskell is a general-purpose, 
purely func- 
tional 
programming 
language 
exhibiting 
many of the recent innovations 
in func- 
tional 
(as well 
as other) 
programming 
language research, including 
higher order 
functions, 
lazy 
evaluation, 
static 
poly- 
morphic 
typing, 
user-defined 
datatypes, 
pattern matching, and list comprehensions. 
It is also a very complete language in that 
it has a module facility, 
a well-defined 
func- 
tional I/O system, and a rich set of primi- 
tive 
datatypes, 
including 
lists, 
arrays, 
arbitrary 
and fixed precision 
integers, and 
floating-point 
numbers. in this sense Has- 
kell represents 
both the culmination 
and 
solidification 
of many years of research on 
functional 
languages-the 
design was in- 
fluenced by languages as old as Iswim and 
as new as Miranda. 
Haskell also has several interesting 
new 
features; most notably, 
a systematic 
treat- 
ment of overloading, 
an orthogonal 
ab- 
stract 
datatype 
facility, 
a universal 
and 
purely functional 
I/O system, and, by anal- 
ogy to list comprehensions, 
a notion 
of 
array comprehensions. 
Haskell is not a small language. The de- 
cision to emphasize certain 
features such 
as pattern 
matching 
and user-defined 
da- 
tatypes and the desire for a complete and 
practical language that includes such things 
as I/O and modules necessitates 
a some- 
what large design. The Haskell Report also 
provides a denotational 
semantics for both 
the static and dynamic behavior of the lan- 
guage; it is considerably 
more complex than 
ACM Computing 
Surveys, Vol. 21, No. 3, September 
1989 

382 
. 
Paul Hudak 
the simple semantics defined in Section 2.5 
for the lambda calculus, but then again one 
wouldn’t 
really 
want 
to program 
in as 
sparse a language as the lambda calculus. 
Will Haskell become a standard? Will it 
succeed as a useful programming 
language? 
Only time will tell. As with any other lan- 
guage development, 
it is not only the qual- 
ity of the design that counts but also the 
ready availability 
of good implementations 
and the backing from vendors, government 
agencies, and researchers alike. At this date 
it is too early to tell what role each of these 
factors will play. 
I will end our historical 
development 
of 
functional 
languages here, without 
elabo- 
rating 
on the details of Haskell 
just yet. 
Those 
details 
will 
surface 
in significant 
ways in the next section, where the most 
important 
features of modern function 
lan- 
guages are discussed, and in the following 
section, 
where more advanced 
ideas and 
active research areas are discussed. 
2. DISTINGUISHING 
FEATURES 
OF 
MODERN 
FUNCTIONAL 
LANGUAGES 
Recall that I chose to delay detailed 
dis- 
cussion of four distinguishing 
features 
of 
modernfunctionallanguages-higher-order 
functions, 
lazy evaluation, 
data abstrac- 
tion 
met hanisms, 
and equations/pattern 
matching. 
Now that we have completed our 
study of the historical 
development 
of func- 
tional 
languages, 
we can return 
to those 
features. Most of the discussion will center 
on how the 
features 
are manifested 
in 
Haskell, ML, and Miranda. 
2.1 Higher Order Functions 
If functions 
are treated as first-class 
values 
in a language-allowing 
them to be stored 
in data structures, 
passed as arguments, 
and returned 
as results-they 
are referred 
to as higher-order 
functions. 
I have not said 
too much about the use of higher 
order 
functions 
thus far, although 
they exist in 
most of the functional 
languages that I have 
discussed, including 
of course the lambda 
calculus. Their use has in fact been argued 
in many circles, including 
ones outside of 
functional 
programming, 
most notably the 
Scheme community 
[Abelson et al. 19851. 
The 
main 
philosophical 
argument 
for 
higher-order 
functions 
is that functions 
are 
values just like any others, so why not give 
them the same first class status? But there 
are also compelling 
pragmatic 
reasons for 
wanting 
higher-order 
functions. 
Simply 
stated, the function 
is the primary 
abstrac- 
tion mechanism over values; thus facilitat- 
ing the use of functions 
increases the use 
of that kind of abstraction. 
As an example of a higher-order 
function, 
consider the following: 
twicefx=f(fx) 
which takes its first argument, 
a function 
f, and applies it twice to its second argu- 
ment, X. The syntax used here is important: 
twice as written 
is curried, 
meaning 
that 
when applied to one argument it returns a 
function 
that then takes one more argu- 
ment, the second argument 
above. For ex- 
ample, the function 
add2. 
add2 = twice succ 
where succ x = n+l 
is a function 
that will add 2 to its argument. 
Making 
function 
application 
associate to 
the left facilitates 
this mechanism, 
since 
(twice succ X) is equivalent 
to ((twice succ) 
x), so everything 
works out just fine. 
In modern 
functional 
languages 
func- 
tions can be created in several ways. One 
way is to name them using equations, 
as 
above; another 
way is to create them di- 
rectly as lambda abstractions, thus render- 
ing them 
nameless, 
as in the 
Haskell 
expression 
\x + x+1 
[in lambda calculus this would be written 
Xx.x + 11, which is the same as the successor 
function 
succ defined above. add2 can then 
be defined more succinctly 
as 
add2 = twice (Lx + x+1) 
From a pragmatic viewpoint, 
we can un- 
derstand the use of higher-order 
functions 
by analyzing 
the use of abstraction 
in gen- 
eral. As known from introductory 
program- 
ming, a function 
is an abstraction 
of values 
over some common behavior 
(an expres- 
sion). Limiting 
the values over which the 
abstraction 
occurs to nonfunctions 
seems 
ACM Computing 
Surveys, Vol. 21, No. 3, September 
1989 

Functional 
Programming 
Languages 
383 
unreasonable; 
lifting 
that 
restriction 
re- 
sults 
in higher-order 
functions. 
Hughes 
makes a slightly 
different 
but equally com- 
pelling 
argument 
in Hughes [1984] where 
he emphasizes the importance 
of modular- 
ity in programming 
and argues convinc- 
ingly that higher-order 
functions 
increase 
modularity 
by serving as a mechanism 
for 
glueing program fragments together. That 
glueing property 
comes not just from the 
ability 
to compose functions 
but also from 
the ability 
to abstract over functional 
be- 
havior as described above. 
A.s an example, suppose in the course of 
program construction 
we define a function 
to add together 
the elements of a list as 
follows: 
sum [] 
= 0 
sum(x:xs) = add x (sum xs) 
Then suppose we later define a function 
to 
multiply 
the elements of a list as follows: 
prod [ ] 
= 1 
prod(x:xs)= 
mu1 x (prod xs) 
But now we notice a repeating pattern and 
anticipate 
that we might see it again, so we 
ask ourselves if we can possibly 
abstract 
the common behavior. 
In fact, this is easy 
to do: We note that add/mu1 and O/l are 
the variable elements in the behavior, 
and 
thus we parameterize 
them; 
that 
is, we 
make them formal parameters, 
say f and 
init. 
Calling 
the new function 
fold, the 
equivalent 
of sum/prod will be “fold f init”, 
and thus we arrive at 
(fold f init) 
[ ] 
= init 
(fold f init)(x:xs) 
= f x ((fold f init) xs) 
where the parentheses around “fold f init” 
are used only for emphasis, and are other- 
wise superfluous. 
From this we can derive new definitions 
for sum and product: 
sum = fold add 0 
prod = fold mu1 1 
Now that 
the fold abstraction 
has been 
made, many other useful functions 
can be 
defined, even something 
as seemingly un- 
related as append: 
append xs ys = fold (:) ys xs 
[An infix 
operator 
may be passed as an 
argument 
by enclosing 
it in parentheses; 
thus (:) is equivalent 
to \x y + x : y.] This 
version of append simply replaces the [ ] at 
the end of the list xs with the list ys. 
It is easy to verify that the new defini- 
tions are equivalent 
to the old using simple 
equational 
reasoning 
and induction. 
It is 
also important 
to note that in arriving 
at 
the main abstraction 
we did nothing 
out of 
the ordinary-we 
just apply classical data 
abstraction 
principles 
in as unrestricted 
a 
way as possible, 
which 
means allowing 
functions 
to be first-class 
citizens. 
2.2 Nonstrict 
Semantics 
(Lazy Evaluation) 
2.2.1 Fundamentals 
The normal-order 
reduction 
rules of the 
lambda calculus 
are the most general in 
that 
they 
are guaranteed 
to produce 
a 
normal 
form 
if in fact one exists 
(see 
Section 1.1). In other words, they result in 
termination 
of the rewriting 
process most 
often, and the strategy lies at the heart of 
the Church-Rosser 
theorem. Furthermore, 
as argued earlier, 
normal-order 
reduction 
allows recursion 
to be emulated 
with the 
Y combinator, 
thus giving the larnbda cal- 
culus the most powerful 
form 
of effec- 
tive computability, 
captured 
in Church’s 
Thesis. 
Given all this, it is quite natural to con- 
sider using normal-order 
reduction 
as the 
computational 
basis for a programming 
language. Unfortunately, 
normal-order 
re- 
duction, implemented 
naively, is hopelessly 
inefficient. 
To see why, consider this simple 
normal-order 
reduction 
sequence: 
(Xx. (+ x x))(* 
5 4) 
* 
(+ (* 5 4)(* 
5 4)) 
* 
(+ 20 (* 5 4)) 
* 
(+ 20 20) 
4 
40 
Note that the multiplication 
(* 5 4) is done 
twice. In the general case, this could be an 
arbitrarily 
large computation, 
and it could 
potentially 
be repeated as many times as 
there are occurrences of the formal param- 
eter (for this reason an analogy 
is often 
drawn between normal-order 
reduction and 
call-by-name 
parameter 
passing in Algol). 
ACM Computing 
Surveys, Vol. 21, No. 3, September 
1989 

384 
l 
Paul Hudak 
In practice 
this can happen quite often, 
reflecting 
the simple fact that results are 
often shared. 
One solution 
to this problem is to resort 
to something 
other than normal-order 
re- 
duction, 
such as applicative-order 
reduc- 
tion, which for the above term yields the 
following 
reduction 
sequence: 
(Ax. (+ x x))(* 
5 4) 
4 (Xx. (+ x x)) 20 
=$ (+ 20 20) 
* 
40 
Note that the argument is evaluated before 
the P-reduction 
is performed 
(similar 
to 
a call-by-value 
parameter-passing 
mecha- 
nism), and thus the normal form is reached 
in three steps instead of four, with no re- 
computation. 
The problem with this solu- 
tion is that it requires the introduction 
of 
a special reduction 
rule to implement 
re- 
cursion (such as gained through 
the &rule 
for McCarthy’s 
conditional), 
and further- 
more there are examples for which it does 
more work than 
normal-order 
reduction. 
For example, consider 
Applicative 
order 
normal order 
(Xx. l)(* 
5 4) 
(Xx. l)(* 
5 4) 
* 
(Xx. 1) 20 
*l 
*l 
or even worse (repeated from Section 1.1): 
Applicative 
order 
(Xx. l)((Xx. 
x x)(Xx. x x)) 
- 
(Xx. l)((Xx. 
x x)(Xx. x x)) 
=+ 
Normal order 
(Xx. l)((hr. 
x x)(Xx. x z)) 
*l 
which in the applicative-order 
case does not 
terminate. 
Despite these problems, most of 
the early functional 
languages, 
including 
pure Lisp, FP, ML, Hope, and all of the 
dataflow 
languages used applicative-order 
semantics.” 
In addition 
to overcoming 
the 
efficiency 
problem of normal-order 
reduc- 
tion, applicative-order 
reduction 
could be 
implemented 
with 
relative 
ease using the 
call-by-value 
compiler technology 
that had 
been developed for conventional 
imperative 
programming 
languages. 
Nevertheless, 
the appeal of normal-order 
reduction 
cannot be ignored. Returning 
to 
lambda calculus basics, we can try to get to 
the root of the efficiency 
problem, 
which 
seems to be the following: 
Reduction 
in the 
lambda calculus 
is normally 
described as 
string reduction, 
which precludes any pos- 
sibility 
of sharing. 
If instead we were to 
describe it as a graph reduction 
process, 
perhaps sharing 
could be achieved. 
This 
idea was first 
suggested by Wadsworth 
in his Ph.D. 
dissertation 
in 1971 [1971, 
Chapter 41, in which he outlined 
a graph- 
reduction 
strategy 
that 
used pointers 
to 
implement 
sharing. Using this strategy re- 
sults in the following 
reduction 
sequence 
for the first example given earlier: 
which takes the same number of steps as 
the applicative-order 
reduction 
sequence. 
We 
will 
call 
an 
implementation 
of 
normal-order 
reduction 
in which recompu- 
tation 
is avoided lazy evaluation 
(another 
term often used is call by need). Its key 
feature is that arguments 
in function 
calls 
are evaluated at most once. It possesses the 
full power of normal-order 
reduction 
while 
I9 Actually 
this is not quite true-most 
implementa- 
tions of these languages use an applicative-order 
re- 
duction 
strategy 
for the top-level 
redices only, thus 
yielding 
what is known as a weak head normal form. 
This strategy turns out to be easier to implement 
than 
complete applicative-order 
reduction 
and also permits 
certain 
versions 
of the Y combinator 
to be imple- 
mented without 
special rules. See Burg [1975] for an 
example of this using Landin’s 
SECD machine. 
ACM Computing Surveys, Vol. 21, No. 3, September 1989 

Functional 
Programming 
Languages 
l 
385 
being more efficient 
than applicative-order 
reduction in that “at most once” sometimes 
amounts to no computation 
at all. 
Despite the appeal of lazy evaluation 
and 
this 
apparent 
solution 
to the efficiency 
problem, it took a good 10 years more for 
researchers to discover ways to implement 
it efficiently 
compared to conventional 
pro- 
gramming 
languages. 
The chief problem 
centered on the difficulty 
in implementing 
lazy graph-reduction 
mechanisms 
on con- 
ventional 
computers, 
which 
seem to be 
better 
suited 
to call-by-value 
strategies. 
Simulating 
the unevaluated 
portions 
of 
the graph in a call-by-value 
environment 
amounts 
to 
implementing 
closures, 
or 
“thunks” 
efficiently, 
which have some in- 
herent, nontrivial 
costs [Bloss et al. 19811. 
It is beyond the scope of this paper to 
discuss the details of these implementation 
concerns; see Peyton-Jones 
[1987] for an 
excellent summary. 
Rather than live with conventional 
com- 
puters, we could alternatively 
build special- 
ized graph-reduction 
or dataflow hardware, 
but so far this 
has not resulted 
in any 
practical, 
much less commercially 
avail- 
able, machines. Nevertheless, 
this work is 
quite promising, 
and good summaries 
of 
work in this area can be found in articles 
by Treleaven 
et al. [1982] and Vegdahl 
[1984], 
both 
of which 
are reprinted 
in 
Thakkar 
[ 19871. 
2.2.2 Expressiveness 
Assuming that we can implement 
lazy eval- 
uation efficiently 
(current 
practice is con- 
sidered acceptably good), we should return 
to the question 
of why we want it in the 
first place. Previously 
we argued on philo- 
sophical 
grounds-it 
is the most general 
evaluation 
policy-but 
is lazy evaluation 
useful to programmers 
in practice? 
The 
answer is an emphatic 
yes, which 
I will 
show via a twofold argument. 
First, lazy evaluation frees a programmer 
from concerns about evaluation 
order. The 
fact is, programmers 
are generally 
con- 
cerned about the efficiency 
of their 
pro- 
grams, and thus they prefer not evaluating 
things that are not absolutely necessary. As 
a simple example, suppose we may need to 
know the greatest common divisor of b and 
c in some computation 
involving 
2. In a 
modern functional 
language we might write 
fax 
where a = gcd b c 
without 
worrying 
about a being evaluated 
needlessly-if 
in the computation 
of f a n 
the value of a is needed, it will be computed; 
otherwise 
it will 
not. If we were to have 
written 
this program in Scheme, for exam- 
ple, we might try 
(let ( (a kcd b c)) 1 
(fax)) 
which will always evaluate a. Knowing 
that 
f doesn’t always need that value and being 
concerned about efficiency, 
we may decide 
to rewrite this as 
(let ( (a (delay &cd b cl)) 1 
(fax)) 
which requires modifying 
f so as to force its 
first argument. Alternatively, 
we could just 
write (f b c x), which requires modifying f 
so as to compute the gcd internally. 
Both 
of these solutions 
are severe violations 
of 
modularity 
and arise out of the program- 
mer’s concern about evaluation 
order. Lazy 
evaluation 
eliminates that concern and pre- 
serves modularity. 
The second argument for lazy evaluation 
is perhaps the one more often heard: the 
ability to compute with unbounded “infinite” 
data structures. 
The idea of lazily evaluat- 
ing data structures 
was first proposed by 
Vuillemin 
[ 19741, but similar 
ideas were 
developed 
independently 
by 
Henderson 
and Morris 
[1976] and Friedman and Wise 
[1976]. In a later series of papers, Turner 
[1981, 19821 provided 
a strong argument 
for using lazy lists, especially 
when com- 
bined with 
list comprehensions 
(see Sec- 
tion 1.7) and higher order functions 
(see 
Section 2.1). Aside from Turner’s 
elegant 
examples, Hughes [1984] presented an ar- 
gument based on facilitating 
modularity 
in 
programming 
where, along with higher or- 
der functions, 
lazy evaluation 
is described 
as a way to “glue” 
pieces of programs 
together. 
ACM Computing 
Surveys, Vol. 21, No. 3, September 
1939 

306 
. 
Paul Hudak 
The primary 
power of lazily 
evaluated 
data structures 
comes from its use in sep- 
arating data from control. The idea is that 
a programmer 
should be able to describe a 
specific 
data structure 
without 
worrying 
about how it gets evaluated. 
Thus, for ex- 
ample, we could describe the sequence of 
natural 
numbers 
by the following 
simple 
program: 
nats = 0: map succ nats 
or alternatively 
by 
numsfrom n = n:numsfrom 
(n+l) 
nats 
= numsfrom 0 
These are examples of “infinite 
lists”, 
or 
streams, and in a language that did not 
support 
lazy evaluation 
would 
cause the 
program to diverge. With 
lazy evaluation 
these data structures 
are only evaluated 
as they are needed, on demand. For exam- 
ple, we could define a function 
that filters 
out only those elements satisfying 
a pro- 
perty p : 
filter p (X : xs) 
= if (p X) then (x: rest) else rest 
where rest = filter p xs 
in which case “filter 
p nats” could be writ- 
ten knowing 
that the degree of the list’s 
computation 
will be determined 
by its con- 
text-that 
is, the consumer of the result. 
Thus 
filter 
has no operational 
control 
within 
it and can be combined with other 
functions 
in a modular way. For example, 
we could compose it with 
a function 
to 
square each element in a list: 
map (\x-+x*x) 
. filter p 
[. is the infix 
composition 
operator.] 
This 
kind of modular 
result, 
in which 
data is 
separated from control, 
is one of the key 
features of lazy evaluation. 
Many more ex- 
amples of this kind may be found in Hughes 
[1984]. 
2.3 Data Abstraction 
Independently 
of the development 
of func- 
tional languages there has been considera- 
ble work on data abstraction 
in general and 
on strong 
typing, 
user-defined 
datatypes 
ACM Computing 
Surveys, Vol. 21, Nq. 3, September 
1989 
and type checking 
in particular. 
Some of 
this work has also taken on a theoretical 
flavor, not only in foundational 
mathemat- 
ics where logicians 
have used types to re- 
solve many famous paradoxes, but also in 
formal semantics where types aid our un- 
derstanding 
of programming 
languages. 
Fueling the theoretical 
work are two sig- 
nificant 
pragmatic advantages of using data 
abstraction 
and strong 
typing 
in 
one’s 
programming 
methodology. 
First, data ab- 
straction 
improves 
the modularity, 
secu- 
rity, and clarity of programs. Modularity 
is 
improved 
because we can abstract 
away 
from implementation 
(i.e., representation) 
details; security is improved because inter- 
face violations 
are automatically 
prohib- 
ited, and clarity 
is improved 
because data 
abstraction 
has an almost self-documenting 
flavor. 
Second, strong static typing helps in de- 
bugging since we are guaranteed 
that if a 
program compiles successfully no error can 
occur at run time due to type violations. 
It 
also leads to more efficient 
implementa- 
tions, since it allows us to eliminate 
the 
most run-time 
tag bits and type testing. 
Thus there is little performance 
penalty for 
using data abstraction 
techniques. 
Of course, these issues are true for any 
programming 
language, and for that reason 
a thorough 
treatment 
of types and data 
abstraction 
is outside the scope of this sur- 
vey; the reader may find an excellent 
sum- 
mary in Cardelli 
and Wegner [1985]. The 
basic idea behind the Hindley-Milner 
type 
system was discussed in Section 1.6.1. I will 
concentrate 
in this section 
on how data 
abstraction 
is manifest 
in modern 
func- 
tional languages. 
2.3.1 Concrete Datatypes 
As mentioned 
earlier, there is a strong ar- 
gument for wanting 
language features that 
facilitate 
data abstraction, 
whether 
or not 
the language 
is functional. 
In fact such 
mechanisms 
were first 
developed 
in the 
context 
of imperative 
languages 
such as 
Simula, Clu, and Euclid. It is only natural 
that they be included 
in functional 
lan- 
guages. ML, as I mentioned, 
was the first 
functional 
language to do this, but many 
others soon followed suit. 

Functional 
Programming 
Languages 
l 
387 
In this section I will 
describe concrete 
(or algebraic) 
datatypes 
as well 
as type 
synonyms. 
I will use Haskell 
syntax, 
but 
the ideas are essentially 
identical 
(at least 
semantically) 
to those used in ML 
and 
Miranda. 
New algebraic datatypes may be defined 
along with 
their 
constructors 
using data 
declarations, 
as in the following 
definitions 
of lists and trees: 
data List a 
= Nil 
/ Cons a (List a) 
data Tree b 
= Empty 1 Node 6 (List (Tree b)) 
The identifiers 
List and Tree are called type 
constructors, and the identifiers 
a and b are 
called type variables, which 
are implicity 
universally 
quantified 
over the scope of the 
data declaration. 
The identifiers 
Nil, Cons, 
Empty, and Node are called data construc- 
tors, or just 
constructors, 
with 
Nil 
and 
Empty 
being nullary 
constructors. 
[Note 
that both type constructors 
and data con- 
structors 
are capitalized, 
so that the latter 
can be used without 
confusion 
in pattern 
matching 
(as discussed in the next section) 
and to avoid confusion 
with type variables 
(such as a and b in the above example).] 
List and Tree are called type construc- 
tors since they construct 
types from other 
types. For example, Tree Ints is the type of 
trees of integers. 
Reading 
from the data 
declaration 
for Tree, we see then that a tree 
of integers is either Empty or a Node con- 
taining 
an integer and a list of more trees 
of integers. 
We can now see that the previously 
given 
type signature for map, 
map :: (a * b) - [al ---f PI 
is equivalent 
to 
map :: (a-+b)+(Lista)-+(Listb) 
That is, [. . .] in a type expression 
is just 
syntax for application 
of the type construc- 
tor List. Similarly, 
we can think of + as an 
infix type constructor 
that creates the type 
of all functions 
from its first argument 
(a 
type) to its second (also a type). 
[A useful property 
to note is the con- 
sistent 
syntax 
used in 
Haskell 
for ex- 
pressions 
and types. Specifically, 
if Ti is 
the type of expression 
or pattern 
ci, then 
the expressions 
\e1+e2, 
[e,], and (e1,e2) 
have the types Tl+T2, 
[T,], and (Tl,T2), 
respectively.] 
Instances 
of these new types are built 
simply 
by using 
the constructors. 
Thus 
Empty is an empty tree, and Node 5 Empty 
is a very simple tree of integers with one 
element. The type of the instance 
is in- 
ferred via the same type inference 
mecha- 
nism 
that 
infers 
types 
of polymorphic 
functions, 
as described previously. 
Defining 
new concrete datatypes is fairly 
common not only in functional 
languages 
but also in imperative 
languages, although 
the polymorphism 
offered by modern func- 
tional 
languages 
makes it all the more 
attractive. 
Type synonyms are a way of creating new 
names for types, such as in the following: 
type Intree 
= Tree Ints 
type Flattener 
= Intree -+ [Ints] 
Note that Intree is used in the definition 
of 
Flattener.Type 
synonyms do not introduce 
new types (as data declarations 
do) but 
rather are a convenient 
way to introduce 
new names (i.e., synonyms) 
for existing 
types. 
2.3.2 Abstract Datatypes 
Another 
idea in data abstraction 
originat- 
ing in imperative 
languages is the notion of 
an abstract datatype (ADT) 
in which 
the 
details of the implementation 
of a datatype 
are hidden from the users of that type, thus 
enhancing 
modularity 
and security. 
The 
traditional 
way to do this is exemplified 
by ML’s 
ADT 
facility 
and emulated 
in 
Miranda. 
Although 
the Haskell 
designers 
chose a different 
approach to ADTs 
(de- 
scribed below), the following 
example of a 
queue ADT 
is written 
as if Haskell 
had 
ML’s 
kind 
of ADTs, 
using the keyword 
abstype: 
abstype Queue a = Q [a] 
where first (Q us) = last us 
isempty 
(Q [ 1) = True 
isempty 
(Q as) = False 
The main point is that the functions 
first, 
isempty, and so on, are visible in the scope 
ACM Computing 
Surveys, Vol. 21, No. 3, September 
1989 

388 
l 
Paul Hudak 
of the abstype declaration, 
but the con- 
structor 
Q, including 
its type, is not. Thus 
a user of the ADT 
has no idea whether 
queues are implemented 
as lists (as shown 
here) or some other data structure. 
The 
advantage of this, of course, is that one is 
free to change the representation 
type with- 
out fear of breaking 
some other code that 
uses the ADT. 
2.3.3 Haskell’s Orthogonal Design 
In Haskell a rather different 
approach was 
taken to ADTs. The observation 
was made 
that the main difference between a concrete 
and abstract datatype 
was that the latter 
had a certain degree of information 
hiding 
built into it. So instead of thinking 
of ab- 
stract datatypes and information 
hiding as 
going hand in hand, the two were made 
orthogonal 
components 
of the language. 
More specifically, 
concrete datatypes were 
made the only data abstraction 
mechanism, 
and to that 
an expose declaration 
was 
added to control 
information 
hiding, 
or 
visibility. 
For example, 
to get the effect of the 
earlier definition 
of a queue, we would write 
expose Queue, first, isempty 
from data Queue a = Q [a] 
first (Q as) = last as 
isempty (Q [ ] = True 
isempty (Q us) = False 
Since Q is not explicitly 
listed in the expose 
declaration, 
it becomes hidden 
from the 
user of the ADT. 
The advantage of this approach to ADTs 
is more flexibility. 
For example, suppose we 
also wish to hide isempty or perhaps some 
auxiliary 
function 
defined 
in the nested 
scope. This is trivially 
done with the or- 
thogonal 
design but is much harder with 
the ML design as described so far. Indeed, 
to alleviate this problem the ML designers 
provided 
an additional 
construct, 
a local 
declaration, 
with which one can hide local 
declarations. 
Another 
advantage of the or- 
thogonal 
design is that the same mecha- 
nism can be used at the top level of a 
module to control visibility 
of the internals 
of the module to the external 
world. 
In 
other words, the expose mechanism is very 
ACM Computing 
Surveys, Vol. 21, No. 3, September 
1989 
general and can be nested. Haskell uses a 
conservative 
module system that relies on 
this capability. 
A disadvantage 
of the orthogonal 
ap- 
proach 
is that 
if the most typical 
ADT 
scenario only requires hiding the represent- 
ative type, the user will 
have to think 
through the details in each case rather than 
having 
the hiding 
done automatically 
by 
using abstype. 
2.4 Equations 
and Pattern Matching 
One of the programming 
methodology 
at- 
tributes 
that is strongly 
encouraged in the 
modern school of functional 
programming 
is the use of equational 
reasoning 
in the 
design and construction 
of programs. The 
lack of side effects accounts for the primary 
ability 
to apply equational 
reasoning, 
but 
there are syntactic 
features that can facili- 
tate it as well. Using equations 
as part of 
the syntax is the most obvious of these, but 
along 
with 
that 
goes 
pattern 
matching 
whereby 
one can write 
several equations 
when defining 
the same function, 
only one 
of which is presumably applicable in a given 
situation. 
Thus 
modern 
functional 
lan- 
guages have tried to maximize 
the expres- 
siveness of pattern matching. 
At first 
blush, 
equations 
and pattern 
matching 
seem fairly 
intuitive 
and rela- 
tively 
innocuous. 
Indeed, we have already 
given 
many 
examples 
that 
use pattern 
matching 
without 
having said much about 
the details 
of how it works. But in fact 
pattern 
matching 
can have surprisingly 
subtle effects on the semantics 
of a lan- 
guage and thus 
should 
be carefully 
and 
precisely defined. 
2.4.1 Pattern Matching Basics 
Pattern matching should actually be viewed 
as the primitive 
behavior of a case expres- 
sion, which has the general form 
case e of 
pat1 + el 
pat2 + e2 
patn -+ en 
Intuitively, 
if the structure 
of e matches 
pati, then the result of the case expression 

Functional 
Programming 
Languages 
l 
389 
is ei. A set of equations 
of the form 
fpatl 
= el 
f pat2 = e2 
f patn = en 
can then be thought 
of as shorthand 
for 
f=\x+caserof 
pat1 -+ el 
pat2 ---, e2 
patn + en 
Despite this translation, 
for convenience 
I 
will use the equational 
syntax 
in the re- 
mainder of this section. 
The question 
to be asked first is just what 
the pattern-matching 
process consists of. 
For example, what exactly are we pattern 
matching against? One idea that seems rea- 
sonable is to allow one to pattern 
match 
against 
constants 
and 
data 
structures. 
Thus, fat can be defined by 
fat 
0 = 1 
’ 
n = n*fac(n-1) 
[The tick mark in the second equation 
is 
an abbreviation 
for fat.] But note that this 
relies on a top-to-bottom 
reading of the 
program, 
since (fat 
0) actually 
matches 
both equations. We can remove this ambi- 
guity by adding a guard (recall the discus- 
sion in Section 1.7), which in Haskell looks 
like the following: 
fat 0 
’ 
= 1 
n ] n>O = n*fac(n-1) 
As we have already demonstrated 
through 
several examples, it is also reasonable to 
pattern match against lists: 
length [] 
= 0 
(x: xs) = 1 + length xs 
and for that 
matter 
any data structure, 
including 
user-defined 
ones: 
data Tree2 a 
= Leaf a ] Branch (Tree2 a) (Tree2 a) 
fringe (Leaf x) 
= [x] 
9 
(Branch 
left right) 
= fringe left ++ fringe right 
where ++ is the infix append operator. 
Another 
possibility 
that seems desirable 
is the ability 
to repeat formal parameters 
on the left-hand 
side to denote that argu- 
ments in those positions 
must have the 
same value, as in the second line of the 
following 
definition: 
member x [ ] 
= False 
, 
x (x : xs) = True 
, 
x (y : xs) = member x xs 
[This is not legal Haskell syntax, since such 
repetition 
of identifiers 
is not allowed.] 
Care must, however, 
be taken with 
this 
approach since in something like 
alleq [x, x, x] = True 
, 
Y 
= False 
it is not clear in what order the elements of 
the list are to be evaluated. For example, if 
they are evaluated left to right then “alleq 
[I, 2, bot]“, where bot is any nonterminat- 
ing computation, 
will return False, whereas 
with a right to left order the program will 
diverge. One solution 
that would at least 
guarantee consistency 
in this approach is 
to insist that all three positions 
are evalu- 
ated so that the program diverges if any of 
the arguments diverge. 
In general the problem of what gets eval- 
uated, and when, is perhaps the most subtle 
aspect of reasoning about pattern matching 
and suggests that the pattern-matching 
al- 
gorithm be fairly simple so as not to mislead 
the user. Thus in Haskell the above repe- 
tition 
of identifiers 
is disallowed-equa- 
tions must be linear-but 
some functional 
languages allow it (e.g., Miranda 
and Alfl 
[Hudak 19841). 
A particularly 
subtle version of this prob- 
lem is captured in the following 
example. 
Consider these definitions: 
data Silly a = Foo a ] Other 
bar (Foo x) = 0 
’ 
Other 
= 1 
Then a call “bar bot” will diverge, since bar 
must be strict in order that it can distin- 
guish between the two kinds of arguments 
that it might receive. But now consider this 
small modification: 
data Silly a = Foo a 
bar (Foo x) = 0 
ACM Computing 
Surveys, Vol. 21, No. 3, September 
1989 

390 
l 
Paul Hudak 
Now a call bar bot seems like it should 
return 
0, since bar need not be strict-it 
can only receive one kind of argument and 
thus does not need to examine it unless it 
is needed in computing 
the result, which in 
this case it does not. In Haskell a mecha- 
nism is provided 
so that either semantics 
may be specified. 
Two useful discussions on the subject of 
pattern matching 
can be found in Augusts- 
son [1985] and Wadler [1987a]. 
2.4.2 Connecting Equations 
Let us now turn to the more global issue of 
how the individual 
equations are connected 
together as a group. As mentioned 
earlier, 
one simple way to do this is give the equa- 
tions a top-to-bottom 
priority, 
so that in 
fat 0 = 1 
’ 
n = n * fac(n-1) 
the second equation 
is tried only after the 
first one has failed. This 
is the solution 
adopted in many functional 
languages, in- 
cluding Haskell and Miranda. 
An alternative 
method is to insist that 
the equations 
be disjoint, 
thus rendering 
the order irrelevant. 
One significant 
moti- 
vation for this is the desire to reason about 
the applicability 
of an equation 
indepen- 
dently of the others, thus facilitating 
equa- 
tional reasoning. The question is, HOW can 
one guarantee disjointness? 
For equations 
without 
guards, the disjointness 
property 
can be determined 
statically; 
that is, by just 
examining 
the 
patterns. 
Unfortunately, 
when unrestricted 
guards are allowed, the 
problem 
becomes 
undecidable, 
since 
it 
amounts to determining 
the equality 
of ar- 
bitrary 
recursive 
predicates. 
This in turn 
can be solved by resolving 
the guard dis- 
jointness 
at run time. On the other hand, 
this solution 
ensures correctness 
only for 
values actually 
encountered 
at run time, 
and thus 
the programmer 
might 
apply 
equational 
reasoning erroneously 
to as yet 
unencountered 
values. 
The two ideas could also be combined by 
providing 
two different 
syntaxes for joining 
equations. 
For example, 
using the hypo- 
thetical keyword else (not valid in Haskell): 
sameShallowStructure 
[a] 
[c] 
= True 
[a&] [c,d] = True 
else 
7 
x 
Y 
= False 
The first two equations would be combined 
using a disjoint 
semantics; 
together 
they 
would 
then be combined 
with 
the third 
using a top-to-bottom 
semantics. Thus the 
third equation acts as an “otherwise” 
clause 
in the case that the first two fail. A design 
of this sort was considered for Haskell early 
on, but the complexities 
of disjointness, 
especially 
in the context 
of guards, were 
considered 
too great and the design was 
eventually 
rejected. 
2.4.3 Argument Order 
In the same sense that it is desirable 
to 
have the order of equations 
be irrelevant, 
it is desirable to have the order of argu- 
ments be irrelevant. 
In exploring 
this pos- 
sibility, 
consider first the functions f and’g 
defined by 
fll=l 
fZx=Z 
gll=l 
gx2=2 
which differ only in the order of their ar- 
guments. Now consider to what “f 2 bot” 
should evaluate. Clearly the equations for f 
are disjoint, 
clearly the expression 
matchs 
only the second equation, 
and since we 
want a nonstrict 
language it seems the an- 
swer should clearly be 2. For a compiler to 
achieve this it must always evaluate 
the 
first argument to f first. 
Now consider the expression 
“g bot 2”- 
by the same argument 
given above the re- 
sult should also be 2, but now the compiler 
must be sure always to evaluate the second 
argument to g first. Can a compiler always 
determine 
the correct 
order in which 
to 
evaluate its arguments? 
To help answer that quation, 
first con- 
sider this intriguing 
example (due to Berry 
[ 19781): 
fOlx=l 
flrO=Z 
fx01=3 
ACM Computing 
Surveys, Vol. 21, No. 3, September 
1989 

Functional 
Programming 
Languages 
l 
391 
Clearly 
these equations 
are disjoint. 
So 
what is the value of “f 0’1 bot”? The desired 
answer is 1. And what is the value of “f 1 
bot O”? The desired answer is 2. And what 
is the value of “f bot 0 l”? 
The desired 
answer is 3. But now the most difficult 
question is In what order should the argu- 
ments be evaluated? 
If we evaluate 
the 
third one first, then the answer to the first 
question 
cannot be 1. If we evaluate the 
second one first, then the answer to the 
second question cannot be 2. If we evaluate 
the first one first, then the answer to the 
third question cannot be 3. In fact there is 
no sequential 
order that will 
allow us to 
answer these three questions 
the way we 
would like-some 
kind of parallel 
evalua- 
tion is required. 
This subtle problem is solvable in several 
ways, but they all require 
some kind 
of 
compromise-insisting 
on a parallel 
(or 
pseudoparallel) 
implementation, 
rejecting 
certain seemingly 
valid programs, making 
equations more strict than one would like, 
or giving 
up 
on the 
independence 
of 
the order of evaluation 
of arguments. 
In 
Haskell 
the last solution 
was chosen- 
perform 
a left-to-right 
evaluation 
of the 
arguments-because 
it presented the sim- 
plest semantics 
to the user, which 
was 
judged to be more important 
than making 
the order of arguments irrelevant. 
Another 
way to explain this is to think of equations 
as syntax for nested lambda expressions, in 
which case one might not expect symmetry 
with respect to the arguments anyway. 
2.5 Formal Semantics 
Simultaneously 
with 
work on functional 
languages Scott, Strachey, and others were 
busy establishing 
the foundations 
of deno- 
tational 
semantics, 
now the most widely 
used tool for describing the formal seman- 
tics of programming 
languages. There was 
a close connection 
between this work and 
functional 
languages primarily 
because the 
lambda calculus served as one of the sim- 
plest programming 
languages with enough 
useful properties to make its formal seman- 
tics interesting. 
In particular, 
the lambda 
calculus 
had a notion 
of self-application, 
which implies that certain domains had to 
contain their own function 
spaces. That is, 
it required a domain D that was a solution 
to the following 
domain equation: 
D=D+D 
At first this seems impossible-surely 
there 
are more functions 
from D into D than 
there are elements in D-but 
Scott [1970] 
was able to show that indeed such domains 
existed, as long as one was willing to restrict 
the allowable 
functions 
in certain 
(quite 
reasonable) ways and by treating 
= as an 
isomorphism 
rather 
than 
an 
equality. 
Scott’s work served as the mathematical 
foundation 
for Strachey’s work [Milne 
and 
Stracheg 19761 on the denotational 
seman- 
tics of programming 
languages; see [Stoy 
19791 and [Schmidt 
19851 for thorough 
treatments. 
Denotational 
semantics 
and functional 
programming 
have close connections, 
and 
the functional 
programming 
community 
emphasizes the importance 
of formal 
se- 
mantics in general. For completeness 
and 
to 
show 
how 
simple 
the 
denotational 
semantics 
of a functional 
language 
can 
be, we give the semantics of the recursive 
lambda 
calculus 
with 
constants 
defined 
in Section 2.3. 
Bas = Int + Bool + . . . Basic values 
D 
= Bas + (D + D) 
Denotable 
values 
Env = Id + D 
Environments 
8: Exp -+ Env + D 
3: Con + D 
gI[xi)enu= 
enuI[xj 
8I[cjenv 
= 3T[cl 
~?(jele2]enu = (271[el J/env)(Z?[epjenv) 
BI[Xx.ejenu 
= Au.Z?[ejenu[u/x] 
kF[e where x1 = e,; .‘. . ; X, = e,l)env 
= 8I[ejenv’ 
where 
env’ = fix Aenv’.env[(Z[e,Denv’)/xl, 
This semantics is relatively 
simple, but 
in moving to a more complex language such 
as Miranda 
or Haskell 
the semantics can 
become significantly 
more complex due to 
the many syntactic 
features that make the 
languages convenient 
to use. In addition, 
ACM Computing 
Surveys, Vol. 21, No. 3, September 
1989 

392 
l 
Paul Hudak 
one must state precisely the static seman- 
tics as well, including 
type checking 
and 
pattern-matching 
usage. This complexity 
is 
managed in the Haskell 
Report 
by first 
translating 
Haskell 
into a kernel which is 
only slightly 
more complex than the above. 
3. ADVANCED 
FEATURES 
AND ACTIVE 
RESEARCH 
AREAS 
Some of the most recent ideas in functional 
language design are new enough that they 
should be regarded as on-going 
research. 
Nevertheless, 
many 
of them 
are sound 
enough to have been included 
in current 
language designs. In this section we will 
explore a variety 
of such ideas, beginning 
with 
some of the innovative 
ideas in the 
Haskell design. Some of the topics have a 
theoretical 
flavor, such as extensions of the 
Hindley-Milner 
type system; some have a 
pragmatic 
flavor, such as expressing 
non- 
determinism, 
efficient 
arrays, and I/O; and 
some involve the testing of new application 
areas, such as parallel and distributed 
com- 
putation. 
All in all, studying 
these topics 
should 
provide 
insight 
into 
the goals of 
functional 
programming 
as well as some of 
the problems in achieving 
those goals. 
3.1 Overloading 
The kind 
of polymorphism 
exhibited 
by 
the Hindley-Milner 
type system is what 
Strachey 
called parametric 
polymorphism 
to distinguish 
it from another kind that he 
called ad hoc polymorp.lzi.sm or overloading. 
The two can be distinguished 
in the follow- 
ing way: A function 
with parametric 
poly- 
morphism 
does not care what type certain 
of its arguments 
have and thus it behaves 
identically 
regardless of the type. In con- 
trast, a function 
with ad hoc polymorphism 
does care and in fact may behave differently 
for different 
types. Stated another way, ad 
hoc polymorphism 
is really just a syntactic 
device for overloading 
a particular 
func- 
tion name or symbol with more than one 
meaning. 
For example, the function 
map defined 
earlier exhibits 
parametric 
polymorphism 
and has typing 
map :: (a + b) - [a] -+ [b] 
Regardless of the kind of list given to map 
it behaves in the same way. In contrast, 
consider the function 
+, which we normally 
wish to behave differently 
for integer and 
floating point numbers and not at all (i.e., 
be a static 
error) 
for nonnumeric 
argu- 
ments. Another 
common 
example 
is the 
function 
== (equality), 
which certainly 
be- 
haves 
differently 
when 
comparing 
the 
equality 
of two numbers versus, say, two 
lists. 
Ad hoc polymorphism 
is normally 
(and 
I suppose 
appropriately) 
treated 
in 
an 
ad hoc manner. Worse, there is no accepted 
convention 
for doing 
this; 
indeed 
ML, 
Miranda, 
and Hope all do it differently. 
Recently, however, a uniform treatment 
for 
handling 
overloading 
was discovered inde- 
pendently 
by Kaes [ 19881 (in trying to gen- 
eralize ML’s 
ad hoc equality 
types) and 
Wadler 
and Blott 
[1989] (as part of the 
process of defining 
Haskell). 
Below I will 
describe the solution as adopted in Haskell; 
details may be found in Wadler and Blott 
[ 19891. 
The basic idea is to introduce 
a notion of 
type classes that 
capture 
a collection 
of 
overloaded operators in a consistent 
way. 
A class declaration 
is used to introduce 
a 
new type class and the overloaded operators 
that must be supported by any type that is 
an instance of that class. An instance dec- 
laration 
declares that a certain type is an 
instance 
of a certain 
class, and thus in- 
cluded in the declaration 
are the definitions 
of the overloaded operators instantiated 
on 
the named type. 
For example, say that we wish to overload 
+ and negate on types Int and Float. To do 
so, we introduce 
a new type class called 
Num: 
class Num a where 
(+) 
:: a+a-+a 
negate :: a -3 a 
This 
declaration 
may be read “a type a 
belongs to the class Num if there are (over- 
loaded) functions 
+ and negate, of the ap- 
propriate 
types, defined on it.” 
We may then declare Int and Float to be 
instances of this class, as follows: 
instance Num Int where 
x+Y 
= addInt x y 
negate x = negateInt x 
ACM Computing 
Surveys, Vol. 21, No. 3, September 
1989 

Functional 
Programming 
Languages 
9 
393 
instance Num Float where 
x+Y 
= addFloat x y 
negate x = negateFloat n 
[note 
how 
infix 
operators 
are defined; 
Haskell’s 
lexical syntax prevents ambigui- 
ties] 
where 
addInt, 
negateInt, 
addFloat, 
and negateFloat 
are assumed in this case 
to be predefined 
functions 
but in general 
could be any user-defined 
function. 
The 
first declaration 
above may be read “Int is 
an instance of the class Num as witnessed 
by these definitions 
of + and negate.” 
Using type classes we can thus treat over- 
loading 
in a consistent, 
arguably 
elegant, 
way. Another 
nice feature 
is that 
type 
classes naturally 
support a notion of inher- 
itance. For example, we may define a class 
Eq by 
class Eq a where 
(==) :: a + a + Boo1 
Given this class, we would certainly 
expect 
all members of the class Num, say, to have 
== defined on them. Thus the class decla- 
ration for Num could be changed to 
class Eq a * Num a where 
(+I 
::a+a+a 
negate :: a + a 
which can be read as “only members of the 
class Eq may be members of the class Num, 
and a type a belongs to the class Num if. . . 
(as before).” 
Given this class declaration, 
instance declarations 
for Num must include 
a definition 
of == as in 
instance Num Int where 
x+Y 
= addInt x y 
negate n = negateInt x 
X”Y 
= eqInt x y 
The Haskell 
Report uses this inheritance 
mechanism to define a very rich hierarchi- 
cal numeric 
structure 
that reflects 
fairly 
well a mathematician’s 
view of numbers. 
The 
traditional 
Hindley-Milner 
type 
system is extended 
in Haskell 
to include 
type classes. The resulting 
type system is 
able to verify that the overloaded operators 
do have the appropriate 
type. It is however, 
possible (but not likely) 
for ambiguous sit- 
uations to arise, which in Haskell result in 
type error but can be reconciled 
explicitly 
by the user (see Hudak and Wadler [1988] 
for the details). 
3.2 Purely Functional 
Yet Universal 
I/O 
To many the notion 
of I/O conjures 
an 
image of state, side effects, and sequencing. 
Is there any hope at achieving purely func- 
tional yet universal 
and of course efficient 
I/O? Suprisingly, 
the answer is yes. Per- 
haps even more surprising 
is that over the 
years there have emerged not one but two 
seemingly very different 
solutions: 
The lazy stream model, in which temporal 
events are modeled as lists, whose lazy 
semantics mimics the demand-driven 
be- 
havior of processes. 
The continuation 
model in which 
tem- 
porality 
is modeled via explicit 
contin- 
uations. 
Although 
papers have been written 
ad- 
voacting 
both solutions, 
and indeed they 
are very different 
in style, the two solutions 
turn out to be exactly equivalent 
in terms 
of expressiveness; in fact, there is an almost 
trivial 
translation 
from one to the other. 
The Haskell I/O system takes advantage of 
this fact and provides a unified 
framework 
that supports both styles. The specific I/O 
operations available in each style are iden- 
tical-what 
differs is the way they are ex- 
pressed-and 
thus programs in either style 
may be combined 
with a well-defined 
se- 
mantics. 
In addition, 
although 
certain 
of 
the primitives 
rely on nondeterministic 
be- 
havior in the operating 
system, referential 
transparency 
is still retained internal 
to a 
Haskell program. 
In this 
section 
the two styles will 
be 
described as they appear in Haskell, 
to- 
gether with the translation 
of one in terms 
of the other. Details of the actual Haskell 
design may be found in Hudak and Wadler 
[1988], and a good discussion of the trade- 
offs between the styles, including 
examples, 
may be found 
in Hudak 
and Sundaresh 
[ 19881. ML, by the way, uses an imperative, 
referentially 
opaque, form of I/O (perhaps 
not surprising 
given the presence of refer- 
ences); Miranda 
uses a rather 
restricted 
form of the stream model; and Hope uses 
the continuation 
model but with 
a strict 
(i.e., call-by-value) 
semantics. 
To begin, we can paint 
an appealing 
functional 
view of a collection 
of programs 
executing within 
an operating system (OS) 
ACM Computing 
Surveys, Vol. 21, No. 3, September 
1989 

394 
l 
Paul Hudak 
as shown in Figure 1. With this view pro- 
grams are assumed to communicate 
with 
the OS via messages-programs 
issue re- 
quests to the OS and receive responses from 
the OS. 
Ignoring 
for now the OS itself as well as 
the merge and split operations, 
a program 
can be seen as a function 
from a stream 
(i.e., list) of responses to a stream of re- 
quests. Although 
the above picture is quite 
intuitive, 
this latter description 
may seem 
counterintuitive-how 
can a program 
re- 
ceive a list of responses before it has gen- 
erated any requests? But remember that we 
are using a lazy (i.e., nonstrict) 
language, 
and thus the program 
is not obliged 
to 
examine any of the responses before it is- 
sues its first request. This application 
of 
lazy evaluation 
is in fact a very common 
style of programming 
in functional 
lan- 
guages. 
Thus a Haskell program engaged in I/O 
is required to have type Behavior, 
where 
type Behavior = [Response] -+ [Request] 
(Recall from Section 2.3 that [Response] is 
the type consisting 
of lists of values of type 
Response.) The main operational 
idea is 
that the nth response is the reply of the 
operating system to the nth request. 
For simplicity, 
we will assume that there 
are only two kinds of requests and three 
kinds of responses, as defined below: 
data Request 
= ReadFile’ Name 
1 WriteFile’ Name Contents 
data Response 
= Success 1 Return Contents 
1 Failure ErrorMsg 
type Name 
= String 
type Contents = String 
type ErrorMsg= String 
[This is a subset of the requests available 
in Haskell.] 
As an example, given this request list, 
1 . . . , WriteFile fname sl, Readfile fname, 
. . . 1 
and the corresponding 
response list, 
[ . . . , Success, Return ~2, . . . ] 
then sl == 
52, unless there 
were some 
intervening 
external effect. 
In contrast, 
the continuation 
model is 
normally 
characterized 
by a set of transac- 
tions. Each transaction 
typically 
takes a 
success continuation 
and a failure contin- 
uation as arguments. 
These continuations 
in turn are simply functions 
that generate 
more transactions. 
For example, 
data Transaction 
= ReadFile’ 
Name FailCont RetCont 
1 WriteFile’ 
Name Contents 
FailCont SuccCont 
type FailCont 
= ErrorMsg + Transaction 
type RetCont 
= Contents + Transaction 
Type SuccCont Transaction 
[In Haskell 
the transactions 
are actually 
provided as functions 
rather than construc- 
tors; see below.] 
The special transaction 
Done 
represents 
program 
termination. 
These 
declarations 
should 
be compared 
with 
those for the stream 
model 
given 
earlier. 
Returning 
to the simple example given 
earlier, the request and response list are no 
longer separate entities since their effect is 
interwoven 
into the continuation 
structure, 
yielding 
something 
like this: 
WriteFile’ fname sl exit 
(ReadFile’ fname exit 
(\s2 + . . .) ) 
where exit errmsg = Done 
in which case, as before, we would expect 
sl == s2 in the absence of external effects. 
This is essentially 
the way I/O is handled 
in Hope. 
Although 
these two styles seem very dif- 
ferent, there is a simple translation 
of the 
continuation 
model into the stream model. 
In Haskell, 
instead 
of defining 
the new 
datatype Transaction, 
a set of functions 
is 
defined that accomplishes 
the same task 
but that is really 
stream transformers 
in 
ACM Computing 
Surveys, Vol. 21, No. 3, September 
1989 

Functional 
Programming 
Languages 
l 
395 
Stream 01 
requests 
Stream of 
responses 
Figure 1. Functional I/O. 
the request/response 
style. In other words, 
input and output. Because the merge itself 
the type Transaction 
should be precisely 
is implemented 
in the operating 
system, 
the type Behavior 
and should not be a new 
referential 
transparency 
is retained within 
datatype at all. Thus we arrive at 
a Haskell program. 
readFile 
:: Name + 
FailCont 
+= FailCont 
---) RetCont 
-+ Behavior 
writeFile 
:: Name + Contents ---$ FailCont 
+ SuccCont 
---* Behavior 
done 
:: 
Behavior 
type FailCont 
= ErrorMsg 
+ Behavior 
type RetCont 
= Contents 
-+ Behavior 
type SuccCont = 
Behavior 
readFile name fail succ resps = 
(ReadFile name) : case (head resps) of 
Return contents --, succ contents 
(tail resps) 
Failure msg 
-+ fail msg (tail resps) 
writeFile 
name contents fail succ resps = 
(WriteFile 
name contents) 
: case (head resps) of 
Success 
+ succ (tail resps) 
Failure msg -+= fail msg (tail resps) 
done resps = [ ] 
This pleasing and very efficient 
translation 
allows 
us to write 
Haskell 
programs 
in 
either style and mix them freely. 
The complete design, of course, includes 
a fairly rich set of primitive 
requests besides 
ReadFile and WriteFile, 
including 
a set of 
requests for communicating 
through chan- 
nels, which include things such as standard 
Another useful aspect of the Haskell de- 
sign is that it includes a partial 
(but rigor- 
ous) specification 
of the behavior of the OS 
itself. For example, it introduces the notion 
of an agent that consumes data on out- 
put, channels 
and produces data on input 
channels. The user is then modeled as an 
agent 
that 
consumes 
standard 
output 
and 
ACM Computing 
Surveys, Vol. 21, No. 3, September 
1989 

396 
l 
Paul Hudak 
produces standard 
input. 
This 
particular 
agent is required to be strict in the standard 
output, 
corresponding 
to the notion 
that 
the user reads the terminal 
display before 
typing at the keyboard. No other language 
design that I am aware of has gone this far 
in specifying 
the I/O system with this de- 
gree of precision; it is usally left implicit 
in 
the specification. 
It is particularly 
impor- 
tant in this context 
however, because the 
proper semantics 
relies critically 
on how 
the OS consumes and produces the request 
and response lists. 
To conclude this section I will show two 
Haskell programs that prompt the user for 
the name of a file, read and echo the file 
name, and then look up and display 
the 
contents of the file on standard output. The 
first 
version 
uses the stream model, the 
second the continuation 
model. 
[The operator !! is the list selection opera- 
tor; thus xs !! n is the nth element in the 
list xs.] 
translated 
almost 
verbatim 
into 
a func- 
tional language. The main philosophy 
is to 
treat the entire 
array as a single entity 
defined declaratively 
rather than as a place 
holder of values that is updated incremen- 
tally. This, in fact, is the basis of the APL 
philosophy 
(see Section 
1.4), and some 
researchers 
have concentrated 
on com- 
bining 
functional 
programming 
ideas with 
those from APL 
[Tu 1986; Tu and Perlis 
19861. The reader may find good general 
discussions 
of arrays 
in functional 
pro- 
gramming 
languages 
in 
Wadler 
[ 19861, 
Hudak 
[1986a], and Wise [1987]. In the 
remainder 
of this section 
I will 
describe 
Haskell’s 
arrays, 
which 
originated 
from 
some ideas in Id Nouveau 
[Nikhil 
et al. 
19861. 
Haskell has a family of multidimensional 
nonstrict 
immutable 
arrays whose special 
interaction 
with 
list comprehensions 
pro- 
vides a convenient 
array comprehension 
syntax for defining 
arrays monolithically. 
main resps = 
[ AppendChannel “stdout” “please type a filename\CR\“, 
if (resps!!l == Success) then (ReadChannel “stdin”), 
AppendChannel “stdout” fname, 
if (resps!!3 == Success) then (ReadFile fname), 
AppendChannel “stdout” (case resps !! 4 of 
Failure msg 
---f “can’t open” 
Return file-contents 
-+ file-contents) 
] where fname = case resps !! 2 of 
Return user-input --+ get-line user-input 
main = appendchannel “stdout” “please type a filename\CR\” 
exit 
(readchannel “stdin” exit (\user-input 
-+ 
appendchannel “stdout” fname exit 
(readFile fname (\msg + appendchannel “stdout” “can’t open” exit done) 
(\contents + 
appendchannel “stdout” contents exit done)) 
where fname = get-line user-input)) 
exit msg = done 
3.3 Arrays 
As an example, 
here is how to define a 
As it turns 
out, arrays can be expressed 
vector of squares of the integers 
from 1 
rather nicely in a functional 
language, and 
to n. 
. 
in fact all of the arguments 
about mathe- 
a = array (1, n) [ (i, i*i) 1 i + [l . . n] ] 
matical 
elegance fall in line when using 
arrays. This is especially 
true of program 
The first argument 
to array is a tuple of 
development 
in 
scientific 
computation, 
bounds, and thus 
this 
array 
has size n 
where textbook matrix algebra can often be 
and is indexed 
from 1 to n. The second 
ACM Computing 
Surveys, Vol. 21, No. 3, September 
1989 

Functional 
Programming 
Languages 
l 
397 
argument is a list of index/value 
pairs and 
is written 
here as a conventional 
list com- 
prehension. 
The ith element of an array a 
is written 
a ! i, and thus in the above case 
we have that a ! i = i*i. 
There are several useful semantic prop- 
erties of Haskell’s arrays. First, they can be 
recursive-here 
is an example of defining 
the 
first 
n numbers 
in 
the 
Fibonacci 
sequence: 
fib = array (0, n) 
( I@, 11, (1,1) I ++ 
[ (i, fib!&l)+fib! 
(i-2)) 
1 i c [2 . . n] ] ) 
This example demonstrates how we can use 
an array as a cache, which in this case turns 
an exponentially 
poor algorithm 
into 
an 
efficient 
linear one. 
Another important 
property is that array 
comprehensions 
are constructed 
lazily, and 
thus the order of the elements in the list is 
completely 
irrelevant. 
For example, we can 
construct 
an m-by-n matrix 
using a wave- 
front recurrence, where the north and west 
borders are 1 and each other element is 
the sum of its north, northwest, 
and west 
neighbors, as follows: 
a = array ((lm), 
(l,n)) 
( [ ((1,lM) 1 
++ 
[ ((i,l),l) 
( i c 
[2.. m] ] ++ 
1 ((l,i),l) 
lj + 12.. nl I ++ 
[ ((i,j), a!(&1,;) 
+ a!(i,j-1) 
+ a!(i-l,j-1)) 
lic[2.. 
ml,j-P..nl 
I) 
The elements in this result can be accessed 
in any order-the 
demand-driven 
effect of 
lazy evaluation 
will 
cause the necessary 
elements to be evaluated in an order con- 
strained 
only by data dependencies. 
It is 
this property that makes array comprehen- 
sions so useful in scientific 
computation, 
where 
recurrence 
equations 
express 
the 
same kind of data dependencies. In imple- 
menting 
such recurrences 
in FORTRAN 
we must be sure that the elements are eval- 
uated in an order consistent 
with the de- 
pendencies-lazy 
evaluation 
accomplishes 
that for us. 
On the other hand, although elegant, ar- 
ray comprehensions 
may be difficult 
to im- 
plement 
efficiently. 
There 
are two main 
difficulties: 
the standard problem of over- 
coming the inefficiencies 
of lazy evaluation 
and the problem of avoiding the construc- 
tion of the many intermediate 
lists that the 
second argument to array seems to need. A 
discussion of ways to overcome these prob- 
lems is found 
in Anderson 
and Hudak 
[ 19891. Alternative 
designs for functional 
arrays and their implementations 
may be 
found in Aasa et al. [1987], Holmstrom 
[ 19831, Hughes [1985a], and Wise [ 19871. 
Another 
problem 
is that array compre- 
hensions are not quite expressive enough 
to capture all behaviors. The most conspic- 
uous example of this is the case in which 
an array is being used as an accumulator, 
say in building 
a histogram, 
and thus one 
actually 
wants an incremental 
update ef- 
fect. Thus 
in Haskell 
a function 
called 
accumArray 
is provided 
to capture 
this 
kind of behavior 
in a way consistent 
with 
the monolithic 
nature of array comprehen- 
sions (similar 
ideas are found in Steele et 
al. [1986] and Wadler 
[1986]). It is not, 
however, clear that this is the most general 
solution to the problem. An alternative 
ap- 
proach is to define an incremental 
update 
operator on arrays, but then even nastier 
efficiency 
problems 
arise, since (concep- 
tually at least) the updates involve copying. 
Work on detecting when it is safe to imple- 
ment 
such 
updates 
destructively 
has 
resulted in at least one efficient 
implemen- 
tation 
[Bloss 1988; Bloss and Hudak N.d.; 
Hudak 
and 
Bloss 
19851, although 
the 
analysis itself is costly. 
Nevertheless, 
array 
comprehensions 
have the potential 
for being very useful, 
and many interesting 
applications 
have al- 
ready been programmed 
using them (see 
Hudak and Anderson 
[1988] for some ex- 
amples). It is hoped that future 
research 
will 
lead to solutions 
to the remaining 
problems. 
3.4 Views 
Pattern 
matching 
(see Section 2.4) is very 
useful in writing 
compact and readable pro- 
grams. Unfortunately, 
knowledge 
of the 
concrete representation 
of an object is nec- 
essary before pattern 
matching 
can be in- 
voked, which seems to be at odds with the 
notion of an abstract datatype. To reconcile 
this conflict 
Wadler 
[1987] introduced 
a 
notion of views. 
ACM Computing 
Surveys, Vol. 21, No. 3, September 
1989 

398 
l 
Paul Hudak 
A view 
declaration 
introduces 
a new 
algebraic datatype, just like a data decla- 
ration, but in addition 
establishes 
an iso- 
morphism 
between the values of this new 
type and a subset of the values of an exist- 
ing algebraic datatype. For example, 
data Complex 
= Rectangular 
Float Float 
view Complex 
= Polar 
Float Float 
where toView (Rectangular 
x y) 
= Polar (sqrt (x**2+y**2)) 
(arctan 
( y/x 1) 
fromview 
(Polar r t) 
= Rectangular 
(r*(cos t)) 
(r*(sin 
t)) 
[Views are not part of Haskell, but as with 
abstract datatypes we will use Haskell syn- 
tax, here extended with the keyword view.] 
Given the datatype Complex, we can read 
the view declaration 
as, “One view of Com- 
plex contains only Polar values; to translate 
from a Rectangular 
to a Polar value, use 
the function 
toView; to translate 
the other 
way, use fromview.” 
Having 
declared this view, we can now 
use pattern matching 
using either the Rec- 
tangular constructor 
or the Polar construc- 
tor; similarly, 
new objects of type Complex 
can be built with either the Rectangular 
or 
Polar constructors. 
They have precisely the 
same status, and the coercions 
are done 
automatically. 
For example, 
rotate (Polar r t) angle = Polar r (t+angle) 
As the example 
stands, 
objects of type 
Complex 
are concretely 
represented 
with 
the Rectangular 
constructor, 
but this deci- 
sion could be reversed by making Polar the 
concrete constructor 
and Rectangular 
the 
view, without 
altering 
any of the functions 
that manipulate 
objects of type Complex. 
Whereas 
traditionally 
abstract 
data 
types are regarded as hiding the represen- 
tation, 
with views we can reveal as many 
representations 
(zero, one, or more) as are 
required. 
As a final example, consider this defini- 
tion of Peano’s view of the natural 
number 
subset of integers: 
view Integer = Zero 1 Succ Integer 
where fromView 
Zero = 0 
’ (Succ n) 1 n>=O 
=n+l 
toView 0 = Zero 
’ n 1 n>O = succ (n-l) 
With this view, 7 is viewed as equivalent 
to 
succ (Succ (Succ (Succ (Succ (Succ (Succ 
Zero)))))) 
Note that fromView 
defines a mapping of 
any finite element of Peano into an integer, 
and toView 
defines the inverse mapping. 
Given this view, we can write definitions 
such as 
fat Zero 
= 1 
’ (Succ n) = (Succ n) * (fat n) 
which 
is very useful 
from an equational 
reasoning 
standpoint, 
since it allows 
us 
to use an abstract representation 
of inte- 
gers without 
incurring 
any performance 
overhead-the 
view declarations 
provide 
enough 
information 
to map all 
of the 
abstractions 
back into concrete implemen- 
tations at compile time. 
On the other hand, perhaps the most 
displeasing 
aspect of views is that an im- 
plicit 
coercion is taking place, which may 
be confusing to the user. For example, in 
case (Foo a b) of 
Fooxy+exp 
we cannot be sure in exp that a==x 
and 
b==y. 
Although 
views were considered 
in 
an initial 
version 
of Haskell, 
they were 
eventually 
discarded, 
in a large part be- 
cause of this problem. 
3.5 Parallel 
Functional 
Programming 
An often-heralded 
advantage of functional 
languages is that parallelism 
in a functional 
program is implicit; 
it is manifested 
solely 
through data dependencies and the seman- 
tics of primitive 
operators. This is in con- 
trast 
to 
more 
conventional 
languages, 
where explicit 
constructs 
are typically 
used 
to invoke, synchronize, 
and in general co- 
ordinate 
the concurrent 
activities. 
In fact, 
as discussed earlier, many functional 
lan- 
guages were developed simultaneously 
with 
work on highly parallel dataflow and reduc- 
tion machines, and such research continues 
today. 
In most of this 
work, 
parallelism 
in 
a functional 
program 
is detected by the 
system and allocated 
to processors auto- 
matically. 
Although 
in certain constrained 
ACM Computing 
Surveys, Vol. 21, No. 3, September 
1989 

Functional 
Programming 
Languages 
l 
399 
classes of functional 
languages the mapping 
of process to processor can be determined 
optimally 
[Chen 1986; Delosme and Ipsen 
19851, in the general case the optimal strat- 
egy is undecidable, 
so heuristics 
such as 
load balancing 
are often used instead. 
But what if a programmer 
knows a good 
(perhaps optimal) 
mapping 
strategy for a 
program executing on a particular 
machine, 
but the compiler 
is not smart enough to 
determine 
it? And even if the compiler 
is 
smart enough, how does one reason about 
such behavior? 
We could argue that the 
programmer should not be concerned about 
such details, but that is a difficult 
argument 
to make to someone whose job is precisely 
to invent such algorithms. 
To meet these needs, various researchers 
have designed extensions to functional 
lan- 
guages, resulting 
in what 
I like to call 
parafunctional 
programming 
languages. 
The extensions 
amount to a metalanguage 
(e.g., annotations) 
to express the desired 
behavior. Examples include annotations 
to 
control 
evaluation 
order 
[Burton 
1984; 
Darlington 
and 
While 
1987; Sridharan 
19851, prioritize 
tasks, and map processes 
to processors 
[Hudak 
1986c; Hudak 
and 
Smith 
1986; Keller 
and Lindstrom 
19851. 
Similar work has taken place in the Prolog 
community 
[Shapiro 19841. In addition, 
re- 
search has resulted in formal operational 
semantics 
for 
such 
extensions 
[Hudak 
198613; Hudak 
and Anderson 
19871. In 
the remainder 
of this section one kind of 
parafunctional 
behavior 
will 
be demon- 
strated-that 
of mapping program to ma- 
chine (based on the work in Hudak [1986c] 
and Hudak and Smith [1986]). 
The fundamental 
idea behind process-to- 
processor mapping 
is quite simple. Con- 
sider the 
expression 
el+e2. 
The 
strict 
semantics of + allows the subexpressions 
el and e2 to be executed in parallel-this 
is an example of what is meant by saying 
that the parallelism 
in a functional 
program 
is implicit. 
But suppose now that we wish 
to express precisely 
where (i.e., on which 
processor) 
the subexpressions 
are to be 
evaluated; 
we may do so quite simply by 
annotating 
the subexpressions 
which 
ap- 
propriate 
mapping information. 
An expres- 
sion annotated 
in this 
way is called 
a 
mapped expression, which has the following 
form: 
exp on proc 
[on is a hypothetical 
keyword, 
and is not 
valid 
Haskell] 
which 
intuitively 
declares 
that exp is to be computed on the processor 
identified 
by proc. The expression 
exp is 
the body of the mapped expression 
and 
represents the value to which the overall 
expression 
will 
evaluate 
(and thus 
can 
be any of expression. 
including 
another 
mapped expression). 
The expression 
proc 
must evaluate to a processor id. Without 
loss of generality 
the processor ids, or pids, 
are assumed to be integers, 
and there is 
some predefined 
mapping from those inte- 
gers to the physical processors they denote. 
Returning 
now to the example, we can 
annotate the expression (el+e2) as follows: 
(el on 0) + (e2 on 1) 
where 0 and 1 are processor ids. Of course, 
this static mapping is not very interesting. 
It would be nice, for example, if we were 
able to refer to a processor relative to the 
currently 
executing 
one. We can do this 
through 
the use of the reserved identifier 
self, which when evaluated returns the pid 
of the currently 
executing processor. Using 
self we can now be more creative. For ex- 
ample, suppose we have a ring of n proces- 
sors that are numbered 
consecutively; 
we 
may then rewrite the above expression as 
(el on left self) + (e2 on right self) 
where left pid = mod (pid-1) 
rz 
right pid = mod (pid+l) 
n 
[mod x y computes 
x modulo y.], which 
denotes the computation 
of the two sub- 
expressions 
in parallel 
on the two neigh- 
boring 
processors, 
with 
the 
sum being 
computed on self. 
To see that it is desirable to bind self 
dynamically, 
consider 
that one may wish 
successive invocations 
of a recursive call to 
be executed on different 
processors-this 
is 
not easily expressed with 
lexically 
bound 
annotations. 
For example, consider the fol- 
lowing 
list-of-factorials 
program, 
again 
using a ring of processors: 
(map fat [2,3,4]) on 0 
where map f [] 
= [I 
f (xxs) 
= f x : ((map f xs) on 
(right self)) 
ACM Computing 
Surveys, Vol. 21, No. 3, September 
1989 

400 
’ 
Paul Hudak 
Note that 
the recursive 
call to map is 
mapped onto the processor to the right of 
the current 
one, and thus the elements 2, 
6, and 24 in the result list are computed on 
processors 0, 1, and 2, respectively. 
Parafunctional 
programming 
languages 
have been shown to be adequate in express- 
ing a wide range of deterministic 
parallel 
algorithms 
clearly 
and concisely 
[Hudak 
1986c; Hudak and Smith 19861. It remains 
to be seen, however, whether the pragmatic 
concerns that motivate these kinds of lan- 
guage extensions 
persist, 
and if they do, 
whether or not compilers can become smart 
enough to perform the optimizations 
auto- 
matically. 
Indeed these same questions can 
be asked about other language extensions, 
such as the memoization 
techniques 
dis- 
cussed in the next section. 
3.6 Caching 
and Memoization 
Consider 
this 
simple 
definition 
of the 
Fibonacci function: 
fibO=l 
’ 
l=l 
’ 
n = fib (n-l) 
+ fib (n-2) 
Although 
simple, it is hopelessly inefficient. 
We could rewrite 
it in one of the classic 
ways, but then the simplicity 
and elegance 
of the original 
definition 
is lost. Keller and 
Sleep [ 19861 suggest an elegant alternative: 
Provide syntax for expressing the caching 
or memoization 
of selected functions. 
For 
example, the syntax might take the form of 
a declaration 
that 
precedes the function 
definition, 
as in 
memo f’ib using cache 
fibO=l 
’ 
l=l 
’ 
n = fib (n-l) 
+ fib (n-2) 
which would be syntactic 
sugar for 
fib = cache fib1 
where fib1 0 = 1 
’ 
l=l 
’ 
n = fib (n-l) 
+ fib (n-2) 
The point 
is that cache is a user-defined 
function 
that specifies a strategy for cach- 
ing values of fib. For example, 
to cache 
values 
in 
an 
array, 
we might 
define 
cache by 
cache fn = \n + (array (0,max) 
[(i&z i) 1 it[O..max]]) 
! n 
where we assume max is the largest argu- 
ment to which fib will be applied. Expand- 
ing out the definitions 
and syntax yields 
fib n = (array (0,max) 
where k$lj 
i HO..maxll) ! n 
’ 
l=l 
’ 
n = fib (n-1) 
+ fib (n-2) 
which is exactly the desired result.” 
As a 
methodology 
this is very nice, since librar- 
ies of useful caching 
functionals 
may be 
produced and reused to cache many differ- 
ent functions. 
There are limitations 
to the 
approach, as well as extensions, all of which 
are described in Keller and Sleep [1986]. 
One of the limitations 
of this approach 
is that in general it can only be used with 
strict functions, 
and even then the expense 
of performing 
equality 
checks on, for ex- 
ample, list arguments can be expensive. As 
a solution 
to this problem. 
Hughes intro- 
duced the notion 
of lazy memo-functions, 
in which the caching strategy uses an iden- 
tity test (EQ in Lisp terminology) 
instead 
of an equality test [Hughes 1985131. Such a 
strategy 
can no longer be considered 
as 
syntactic 
sugar, since an identity 
predicate 
is not something 
normally 
provided 
as a 
primitive 
in functional 
languages because 
it is implementation 
dependent. Neverthe- 
less, if built 
into a language lazy memo- 
functions 
provide a very efficient 
(constant 
time) caching mechanism 
and allow very 
elegant solutions to a class of problems not 
solved by Keller and Sleep’s strategy: those 
involving 
infinite 
data structures. 
For ex- 
ample, consider this definition 
of the infi- 
nite list of ones: 
ones = 1 : ones 
Any reasonable 
implementation 
will 
rep- 
resent this as a cyclic list, thus consuming 
constant 
space. Another 
common idiom is 
the use of higher-order 
functions 
such as 
” This is essentially 
the same solution as the one given 
in Section 3.3. 
ACM Computing Surveys, Vol. 21, No. 3, September 
1989 

Functional 
Programming 
Languages 
l 
401 
map: 
twos = map (\x+2*x) 
ones 
But now note that only the cleverest 
of 
implementations 
will 
represent 
this as a 
cyclic list, since map normally 
generates a 
new list cell on every recursive call. By lazy 
memoizing map, however, the cyclic list will 
be recovered. To see how, note that the first 
recursive 
call to map will be “map (\x+ 
2*x) (tail ones)“-but 
(tail ones) is identi- 
cal to ones (remember that ones is cyclic), 
and thus map is called with 
arguments 
identical to the first call. Thus the old value 
is returned, and the cycle is created. Many 
more practical 
examples are discussed in 
Hughes [ 1985b]. 
The interesting 
thing about memoization 
in general is that it begins to touch on some 
of the limitations 
of functional 
languages- 
in particular, 
the inability 
to side effect 
global objects such as caches-and 
solu- 
tions such as lazy memo-functions 
repre- 
sent useful compromises. 
It remains to be 
seen whether more general solutions can be 
found 
that 
eliminate 
the need for these 
special-purpose 
features. 
3.7 Nondeterminism 
Most 
programmers 
(including 
the 
very 
idealistic 
among us) admit 
the need for 
nondeterminism, 
despite the semantic dif- 
ficulties 
it introduces. 
It seems to be an 
essential 
ingredient 
of real-time 
systems, 
such as operating 
systems and device con- 
trollers. 
Nondeterminism 
in 
imperative 
languages is typically 
manifested 
by run- 
ning 
in 
parallel 
several 
processes that 
are side effecting 
some global state-the 
nondeterminism 
is thus 
implicit 
in the 
semantics 
of the language. 
In functional 
languages, nondeterminism 
is manifested 
through the use of primitive 
operators such 
as amb or merge-the 
nondeterminism 
is 
thus made explicit. 
Several papers have 
been published 
on the use of such primi- 
tives in functional 
programming, 
and it 
appears quite reasonable to program con- 
ventional 
nondeterministic 
applications 
using them [Henderson 
1982; Stoye 19851. 
The problem is, once introduced, 
nondeter- 
minism 
completely 
destroys 
referential 
transparency, 
as we shall see. 
By way of introduction, 
McCarthy 
[ 19631 
defined a binary nondeterministic 
operator 
called amb having the following 
behavior: 
amb(el, I) 
= el 
amb(l., 
e2) = e2 
amb(e,, e2) = either e, or e2, 
chosen nondeterministically 
The operational 
reading of amb(e,, e2) is 
that e, and e2 are evaluated in parallel, and 
the one that completes first is returned 
as 
the value of the expression. 
To see how referential 
transparency 
is 
lost, consider this simple example: 
(amb 1 2) + (amb 1 2) 
Is the answer 2 or 4? Or is it perhaps 3? 
The possibility 
of the answer 3 indicates 
that referential 
transparency 
is lost-there 
does not appear to be any simple syntactic 
mechanism for ensuring that we could not 
replace equals for equals in a misleading 
way. Shortly, 
we will discuss possible solu- 
tions to this problem, 
but for now let us ’ 
look at an example using this style of non- 
determinism. 
Using amb, we can easily define things 
such as merge that 
nondeterministically 
merge two lists, or streams:21 
merge as bs = amb 
(if (as == [ 1) then bs else 
(head as: merge (tail as) bs)) 
(if (bs == [ 1) then as else 
(head bs: merge as (tail bs))) 
which 
then can be used, for example, 
in 
combining 
streams of characters from dif- 
ferent computer terminals: 
process 
(merge 
term1 
term2) 
” Note that this version of merge: 
merge [ ] bs = bs 
merge as [ ] = as 
merge (a:as) (b: bs) 
= amb (a : merge as (b:bs)) 
(b : merge (a:as) 
bs) 
is not correct, since merge I bs evaluates to I, whereas 
we would like it to be bs ++ I, 
which 
in fact the 
definition 
in the text yields. 
ACM Computing Surveys, Vol. 21, No. 3, September 1989 

402 
l 
Paul Hudak 
Using this as a basis, Henderson 
[1982] 
show how many operating system problems 
can be solved in a pseudofunctional 
lan- 
guage. Hudak 
[1986a] uses nondetermin- 
ism of a slightly 
different 
kind to emulate 
the parallel updating 
of arrays. 
Although 
satisfying 
in the sense of being 
able to solve real-world 
kinds of nondeter- 
minism, these solutions 
are dissatisfying 
in 
the way they destroy referential 
transpar- 
ency. One might argue that the situation 
is 
at least somewhat better than the conven- 
tional imperative 
one in that the nondeter- 
minism is at least made explicit, 
and thus 
one could induce extra caution 
when rea- 
soning about those sections of a program 
exhibiting 
nondeterministic 
behavior. 
The 
only problem with this is that determining 
which sections of a program are nondeter- 
ministic 
may be difficult-it 
is not a lexical 
property, 
but rather a dynamic 
one, since 
any function 
may call a nondeterministic 
subfunction. 
At least two solutions 
have been pro- 
posed to this problem. 
One, proposed by 
Burton 
[1988], is to provide a tree-shaped 
oracle as an argument 
to a program from 
which nondeterministic 
choices may be se- 
lected. By passing the tree and its subtrees 
around explicitly, 
referential 
transparency 
can be preserved. ‘The problem 
with 
this 
approach is that carrying the oracle around 
explicitly 
is cumbersome 
at best. On the 
other 
hand, 
functional 
programmers 
al- 
ready carry around 
a greater amount 
of 
state to avoid problems with side effects, so 
perhaps the extra burden is not too great. 
Another 
(at least partial) 
solution 
was 
proposed by Stoye [1985] in which 
all of 
the nondeterminism 
in a program is forced 
to be in one place. Indeed to some extent 
this was the solution 
adopted in Haskell, 
although 
for somewhat 
different 
reasons. 
The problem with this approach is that the 
nondeterminism 
is not eliminated 
com- 
pletely but rather centralized. 
It allows rea- 
soning 
equationally 
within 
the 
isolated 
pieces but 
not within 
the collection 
of 
pieces as a whole. Nevertheless, 
the isola- 
tion (at least in Haskell) 
is achieved syn- 
tactically, 
and thus it is easy to determine 
when equational 
reasoning is valid. A gen- 
eral discussion 
of these issues is found in 
Hudak and Sundaresh 
[1988]. 
An interesting 
variation 
on these two 
ideas is to combine them-centralize 
the 
nondeterminism 
and then use an oracle to 
define it in a referentially 
transparent 
way. 
Thus the disadvantages of both approaches 
would seem to disappear. 
In any case, I should point out that none 
of these solutions 
makes reasoning 
about 
nondeterminism 
any easier, they just make 
reasoning about programs easier. 
3.8 Extensions 
to Polymorphic-Type 
Inference 
The Hindley-Milner 
type system has cer- 
tain limitations; 
an example 
of this was 
given in Section 
1.6.1. Some success has 
been achieved in extending 
the type system 
to include other kinds of data objects, but 
surprisingly 
little success has been achieved 
at removing 
the fundamental 
limitations 
while still retaining 
the tractability 
of the 
type inference 
problem. 
It is a somewhat 
fragile system but is fortunately 
expressive 
enough to base practical 
languages on it. 
Nevertheless, 
research 
continues 
in this 
area. 
Independently 
of type inference, consid- 
erable research is underway 
on the expres- 
siveness of type systems in general. The 
most obvious thing to do is allow types to 
be first class, thus allowing abstraction 
over 
them in the obvious way. Through 
gener- 
alizations 
of the type system it is possible 
to model 
such things 
as parameterized 
modules, inheritance, 
and subtyping. 
This 
area has indeed taken on a character of its 
own; a good summary of current work may 
be found in Cardelli 
and Wegner 
[1985] 
and Reynolds [ 19851. 
3.9 Combining 
Other Programming 
Language 
Paradigms 
A time-honored 
tradition 
in programming 
language design is to come up with hybrid 
designs that combine the best features of 
several different 
paradigms, and functional 
programming 
language 
research 
has not 
escaped that tradition. 
I will 
discuss two 
such hybrids here, although 
others exist. 
The 
first 
hybrid 
is combining 
logic 
programming 
with 
functional 
program- 
ming. The “logical 
variable 
permits” 
two- 
way matching 
(via unification) 
in logic 
ACM 
Computing 
Surveys, 
Vol. 
21, No. 
3, September 
1989 

Functional 
Programming 
Languages 
e 
403 
programming 
languages, as opposed to the 
one-way matching 
(via pattern 
matching) 
in functional 
languages, 
and thus seems 
like a desirable feature to have in a lan- 
guage. Indeed its declarative nature fits well 
with the ideals of functional 
programming. 
The 
integration 
is, however 
not easy- 
many proposals have been made yet none 
are completely 
satisfactory, 
especially 
in 
the 
context 
of higher 
order 
functions 
and 
lazy 
evaluation. 
See Degroot 
and 
Lindstrom 
[1985] for a good summary 
of 
results. 
The second area reflects an attempt 
to 
combine the state-oriented 
behavior of im- 
perative languages in a semantically 
clean 
way. The same problems arise here as they 
do with 
nondeterminism-simply 
adding 
the assignment statement means that equa- 
tional reasoning must always be qualified, 
since it is difficult 
to determine whether or 
not a deeply nested call is made to a func- 
tion that induces a side effect. There are, 
however, some possible solutions 
to this, 
most notably work by Gifford and Lucassen 
[Gifford 
and Lucassen 1986; Lucassen and 
Gifford 
19881 in which effects are captured 
in the type system. In Gifford’s 
system it is 
possible to determine 
from a procedure’s 
type whether or not it is side-effect free. It 
is not currently 
known, however, whether 
such a system can be made into a type 
inference system in which type declarations 
are not required. This is an active area of 
current research. 
4. Dispelling 
Myths About Functional 
Programming 
To gain further 
insight 
into the nature of 
functional 
languages, it is helpful 
to dis- 
cuss, with 
the hope of dispelling, 
certain 
myths that have arisen over the years. 
Myth 
1, that functional 
programming 
is 
the antithesis 
of conventional 
imperative 
programming, 
is largely 
responsible 
for 
alienating 
imperative 
programmers 
from 
functional 
languages. But, in fact, there is 
much in common between the two styles of 
programming, 
which I make evident by two 
simple arguments. 
Consider first that one of the key evolu- 
tionary 
characteristics 
of high-level 
imper- 
ative programming 
languages has been the 
use of expressions to denote a result rather 
than a sequence of statements 
to put to- 
gether a result imperatively 
and in piece- 
meal. 
Expressions 
were 
an 
important 
feature of FORTRAN, 
had more of a math- 
ematical flavor, and freed the programmer 
of low-level 
operational 
detail (this burden 
was of course transferred 
to the compiler). 
From FORTRAN 
expressions, to functions 
in Pascal, to expression oriented program- 
ming style in Scheme-these 
advances are 
all on the same evolutionary 
path. Func- 
tional programming 
can be seen as carrying 
this evolution 
to its logical conclusion- 
everything 
is an expression. 
The second argument 
is based on an 
analogy 
between 
functional 
(i.e., 
side- 
effect-free) 
programming 
and structured 
(i.e., goto-less) programming. 
The fact is, it 
is hard to imagine 
doing without 
either 
goto’s or assignment 
statements, until one 
is shown what to use in their place. In the 
case of goto, one uses instead structured 
commands, in the case of assignment state- 
ments, one uses instead lexical binding and 
recursion. 
As an example, this simple program frag- 
ment with goto’s 
x := init; 
i := 0; 
loop: x := f(x, i); 
i := i+l; 
if id0 
got0 loop; 
can be rewritten 
in a structured 
style as 
x := init; 
i := 0; 
while i<lO 
being x := f(x, i); 
i := i+l 
end; 
In capturing 
this disciplined 
use of goto, 
arbitrary 
jumps into or out of the body of 
the block now cannot be made. Although 
this can be viewed as a constraint, 
most 
people feel that the resulting 
disciplined 
style of programming 
is clearer, easier to 
maintain, 
and so on. 
More discipline 
is evident here than just 
the judicious 
use of goto. Note in the orig- 
inal program 
fragment 
that x and i are 
assigned to exactly once in each iteration 
of the loop; the variable 
i, in fact, is only 
being used to control the loop termination 
ACM Computing 
Surveys, Vol. 21, No. 3, September 
1989 

404 
l 
Paul Hudak 
criteria, and the final value of x is intended 
as the value computed 
by the loop. This 
disciplined 
use of assignment 
can be cap- 
tured 
by the 
following 
assignment-free 
Haskell program: 
loop init 0 
where loop x i = if i-40 
then (loop (fx 
i) (i+l) 
else x 
Functions 
(and procedures) 
can in fact be 
thought 
of as a disciplined 
use of goto and 
assignment-the 
transfer 
of control to the 
body of the function 
and the subsequent 
return 
capture 
a disciplined 
use of goto, 
and the formal-to-actual 
parameter binding 
captures a disciplined 
use of assignment. 
By inventing 
a bit of syntactic 
sugar to 
capture the essence of tail recursion, 
the 
above program could be rewritten 
as 
let x = init 
i=O 
in while i-40 
[this syntactic 
sugar is not found in Has- 
begin next x = f(x, i) 
next i = i+l 
end 
result x 
kell, although 
some other functional 
(es- 
pecially 
dataflow) 
languages have similar 
features, 
including 
Id, Val, 
and Lucid] 
where the form “next x = . . .” is a construct 
(next is a keyword) 
used to express what 
the value of x will be on the next iteration 
of the loop. Note the similarity 
of this 
program to the structured 
one given earlier. 
In order to enforce 
a disciplined 
use of 
assignment properly, 
we can constrain 
the 
syntax so that only one next statement 
is 
allowed for each identifier 
(stated another 
way, this constraint 
means that it is a triv- 
ial matter to convert 
such programs 
into 
the tail recursive form shown earlier). If we 
think 
of “next 
x” and “next 
i” as new 
identifiers 
(just as the formal parameters 
of loop can be thought of as new identifiers 
for every call to loop), 
then 
referential 
transparency 
is preserved. Functional 
pro- 
gramming advocates argue that this results 
in a better style of programming, 
in much 
the same way that structured 
programming 
advocates argue for their cause. 
Thus 
the 
analogy 
between 
goto-less 
programming 
and 
assignment-free 
pro- 
gramming 
runs deep. When Dijkstra 
first 
introduced 
structured 
programming, 
much 
of 
the 
programming 
community 
was 
aghast-how 
could one do without 
goto? 
But as people programmed in the new style, 
it was realized that what was being imposed 
was a discipline 
for good programming 
not 
a police state to inhibit 
expressiveness. Ex- 
actly the same can be said of side-effect- 
free programming, 
and its advocates hope 
that as people become more comfortable 
programming 
in the functional 
style, they 
will appreciate the good sides of the disci- 
pline thus imposed. 
When viewed in this way functional 
lan- 
guages can be seen as a logical step in the 
evolution 
of imperative 
languages-thus, 
of 
course, rendering 
them nonimperative. 
On 
the other hand, it is exactly this purity 
to 
which some programmers 
object, and one 
could argue that just as a tasteful 
use of 
goto here or there is acceptable, 
so is a 
tasteful 
use of a side effect. Such small 
impurities 
certainly 
shouldn’t 
invalidate 
the functional 
programming 
style and thus 
may be acceptable. 
Myth 
2 is that functional 
programming 
languages are toys. The first step toward 
dispelling 
this myth is to cite examples of 
efficient 
implementations 
of functional 
languages, of which there now exist several. 
The Alfl 
compiler 
at Yale, for example, 
generates code that is competitive 
with that 
generated by conventional 
language com- 
pilers [Young 19881. Other notable compi- 
ler efforts 
include 
the LML 
compiler 
at 
Chalmers 
University 
[Augustsson 
19841, 
the Ponder compiler at Cambridge Univer- 
sity [Fairbairn 
19851, and the ML compi- 
lers developed at Bell Labs and Princeton 
[Appel and MacQueen 19871. 
On the other hand, there are still 
in- 
herent 
inefficiencies 
that 
cannot 
be ig- 
nored. 
Higher-order 
functions 
and lazy 
evaluation 
certainly 
increase 
expressive- 
ness, but in the general case the overhead 
of, for example, the dynamic storage man- 
agement necessary to support them cannot 
be eliminated. 
The second step toward 
dispelling 
this 
myth amounts to pointing 
to real applica- 
tions development 
in functional 
languages 
ACM Computing 
Surveys, Vol. 21, No. 3, September 
1989 

Functional 
Programming 
Languages 
l 
405 
including 
real-world 
situations 
involving 
nondeterminism, 
databases, 
parallelism, 
and so on. Atlhough 
examples of this sort 
are not plentiful 
(primarily 
because of the 
youth 
of the field) 
and are hard to cite 
(since papers are not usually written 
about 
applications), 
they do exist. For example, 
(1) 
(2) 
(3) 
(4) 
(5) 
(6) 
The dataflow 
groups at MIT 
and the 
functional 
programming 
groups at Yale 
have written 
numerous functional 
pro- 
grams for scientific 
computation. 
So 
have two national labs: Los Alamos and 
Lawrence Livermore. 
MCC has written 
a reasonably 
large 
expert system (EMYCIN) 
in SASL. 
At least one company is currently 
mar- 
keting a commercial 
product 
(a CAD 
package) that 
uses a lazy functional 
language. 
A group at IBM uses a lazy functional 
language for graphics and animation. 
The LML 
(lazy ML) compiler at Chal- 
mers was written 
almost 
entirely 
in 
LML, 
and the new Haskell 
compilers 
at both Glasgow and Yale are being 
written 
in Haskell. 
GEC Hirst 
Research Lab is using a 
program 
for designing 
VLSI 
circuits 
that was written 
by some researchers 
at Oxford 
using 
a lazy 
functional 
language. 
There 
are other examples. 
In particular, 
there are many Scheme and Lisp programs 
that are predominantly 
side effect free and 
could properly 
be thought 
of as functional 
programs. 
Myth 3, that functional 
languages cannot 
deal with 
state, is often 
expressed as a 
question: 
How can someone program in a 
language that does not have a notion 
of 
state? The answer, of course, is that we 
cannot, 
and in fact functional 
languages 
deal with 
state very nicely, 
although 
the 
state is expressed explicitly 
rather 
than 
implicitly. 
So the issue is more a matter of 
how one expresses state and manipulations 
of it. 
State in a functional 
program is usually 
carried 
around 
explicitly 
in one of two 
ways: (1) in the values of bound variables 
of functions, 
in which case it is updated by 
making a recursive call to the function 
with 
new values as arguments, 
or (2) in a data 
structure 
that is updated nondestructively 
so that, at least conceptually, 
the old value 
of the data structure 
remains 
intact 
and 
can be accessed later. Although 
declarative 
and referentially 
transparent, 
this treat- 
ment of state can present problems to an 
implementation, 
but it is certainly 
not a 
problem 
in expressiveness. 
Furthermore, 
the implementation 
problems 
are not as 
bad as they first seem, and recent work has 
gone a long way toward solving them. For 
example, a compiler can convert tail recur- 
sions into loops and “single-threaded” 
data 
structures into mutable ones. 
It turns out that, with respect to expres- 
siveness, one can use higher-order 
func- 
tions not only to manipulate 
state but also 
to make it appear implicit. 
To see how, 
consider 
the 
imperative 
program 
given 
earlier: 
x := init; 
i := 0; 
loop: X := f(x, i); 
i := i + 1; 
if (i < 10) got0 loop; 
We can model the implicit 
state in this 
program as a pair (xval, ival) and define 
several functions 
that 
update 
this 
state 
in a way 
that 
mimics 
the 
assignment 
statements: 
x (xval, ival) xval’ = (xval’, 
ival) 
i (xval, ival) ival’ = (xval, ival’) 
x’(x,i)=x 
i’ (x, i) = i 
const u s = u 
We will 
take advantage 
of the fact that 
these functions 
are defined in curried form. 
Note how x and i are used to update the 
state, and x ’ and i ’ are used to access the 
state. For example, the following 
function, 
when applied to the state, will increment 
i: 
\s-+is((i’s)+l) 
For expository 
purposes we would like to 
make the state as implicit 
as possible, and 
thus we express the result as a composition 
of higher-order 
functions. 
To facilitate 
this 
and to make the result look as much like 
the original 
program as possible, we define 
the following 
higher-order 
infix 
operators 
ACM Computing 
Surveys, Vol. 21, No. 3, September 
1989 

406 
l 
Paul Hudak 
and functions”: 
f:=g 
=\s-+fs(gs) 
f;g 
=\s-g(fs) 
got0 f = f 
f+‘g=\s-+fs+gs 
f<‘g=\s-+fs<gs 
if’ 
p c = \s + 
(if 
(p s) then 
(c s) else s) 
[I am cheating 
slightly 
here in that ; is a 
reserved operator ‘in Haskell and thus can- 
not really be redefined in this way.] 
Given these definitions, 
we can now write 
the following 
functional 
(albeit contrived) 
version 
of the imperative 
program 
given 
earlier: 
x := const init; 
i := const 0; 
loop where 
loop = x := f; 
i := i’ +’ const 1; 
if’ 
(i’ C’ const 
10) (got0 loop) 
This result is rather disquieting-it 
looks 
very much like the original 
imperative 
pro- 
gram. Of course, we worked hard to make 
it that way, which 
in the general case is 
much harder to do and it is certainly 
not 
the recommended way to do functional 
pro- 
gramming. 
Nevertheless 
it exemplifies 
the 
power and flexibility 
of higher-order 
func- 
tions-note 
how they are used here both to 
manipulate 
the state and to implement 
the 
goto (where in particular 
the definition 
of 
loop is recursive, since the goto implements 
a loop). 
5. Conclusions 
This paper presented functional 
program- 
ming in its many shapes and forms. Al- 
though 
it only touched on the surface of 
many issues, it is hoped that enough of a 
foundation 
has been given that researchers 
can explore particularly 
interesting 
topics 
in more depth and programmers 
can learn 
to use functional 
languages in a variety 
of 
applications. 
I said little 
about 
how to implement 
functional 
languages, 
primarily 
because 
doing that subject justice 
would probably 
“It 
is interesting 
to note that 
:=, const, and goto 
correspond 
precisely 
to the combinators 
S, K, and I, 
and ; is almost the combinator 
B, but is actually 
CR. 
double the size of this paper. The interes- 
ted reader can refer to by far the best 
single reference to sequential 
implementa- 
tions, Peyton Jones’ [ 19871, as well as other 
techniques 
that 
have recently 
appeared 
viable [Bloss et al. 1988; Burn et al. 1988; 
and Fairbairn 
and Wray 
19871. Parallel 
implementations 
have taken a variety 
of 
forms. On commercial 
machines the state 
of the art on parallel 
graph reduction 
im- 
plementations 
may be found in Goldberg 
and Hudak 
[1988] and Goldberg 
[1.988a, 
1988b]. The latest on special-purpose 
par- 
allel graph reducers can be found in Peyton 
Jones et al. [ 19871 and Watson and Watson 
[1987]. For a different 
kind of implemen- 
tation see Hudak and Mohr [1988]. Refer- 
ences to dataflow 
machines were given in 
Section 1.8. 
ACKNOWLEDGMENTS 
Thanks 
to the Lisp and Functional 
Programming 
Research Group at Yale, which has inspired 
much of 
my work and served as my chief sounding board. A 
special thanks 
is also extended 
to the Haskell 
Com- 
mittee, through 
which I learned more than I care to 
admit. In addition, 
the following 
people provided 
val- 
uable comments 
on early 
drafts 
of the paper: 
Kei 
Davis, 
Alex Ferguson, 
John 
Launchbury, 
and Phil 
Wadler 
(all from the University 
of Glasgow); 
Juan 
Guzman, 
Siau-Cheng 
Khoo, 
Amir 
Kishon, 
Raman 
Sundaresh, and Pradeep Varma (all from Yale); David 
Wise 
(Indiana 
University); 
and three 
anonymous 
referees. 
I also wish to thank 
my funding 
agencies: The 
Department 
of Energy under grant FG0266ER25012, 
the Defense Advanced 
Research Projects Agency un- 
der grant N00014-88-K-0573, 
and the National 
Sci- 
ence Foundation 
under grant DCR-8451415. 
Without 
their generous support none of this work would have 
been possible. 
In writing 
this survey paper I have tried to acknowl- 
edge accurately 
the significant 
technical 
contributions 
in each of the areas of functional 
programming 
that I 
have covered. 
Unfortunately, 
that is a very difficult 
task, and I apologize 
in advance 
for any errors 
or 
omissions. 
REFERENCES 
AMA, 
A., 
HOLMSTROM, 
S., 
AND 
NILSSON, 
C. 
1987. 
An efficiency 
comparison 
of some repre- 
sentations 
of purely functional 
arrays. Tech. Rep. 
33. Programming 
Methodology 
Group, Chalmers 
University 
of Technology. 
ABELSON, H., 
SUSSMAN, 
G. J., 
AND 
SUSSMAN, 
J. 
1985. 
Structure 
and Interpretation 
of Computer 
ACM Computing Surveys, Vol. 21, No. 3, September 1989 

Functional 
Programming 
Languages 
l 
407 
Programs. 
The MIT 
Press, Cambridge, 
Mass., 
and McGraw-Hill 
New York. 
ACKERMAN, W. B., AND DENNIS, J. B. 1979. 
VAL- 
A value-oriented 
algorithmic 
language prelimi- 
nary reference manual. Laboratory 
for Computer 
Science MIT/LCS/TR-218, 
MIT. 
ANDERSON, S., AND HUDAK, P. 1989. 
Efficient 
com- 
pilation 
of Haskell 
array cqmprehensions. 
Tech. 
Rep. 
YALEU/DCS/RR693. 
Yale 
University, 
Department 
of Computer 
Science. 
APPEL, A. W., AND MACQUEEN, D. B. 1987. 
A stan- 
dard ML compiler. 
In Proceedings 
of the 2987 
Functional 
Programming 
Languages 
and Com- 
puter 
Architecture 
Conference 
(Sept. 
1987). 
Springer-Verlag 
LNCS 274, IFIP pp. 301-324. 
ARVIND AND GOSTELOW, K. P. 1977. 
A computer 
capable 
of exchanging 
processors 
for time. 
In 
Proceedings IFIP Congress, pp. 849-853. 
ARVIND AND GOSTELOW, K. P. 1982. 
The U-inter- 
preter. Computer 15, 2, 42-50. 
ARVIND AND KATHAIL, V. 1981. 
A multiple processor 
da.ta flow machine that supports generalized pro- 
cedures. In Proceedings of the 8th Annual 
Sym- 
posium on Computer Architecture. 
Vol. 9, No. 3. 
ACM SIGARCH, 
pp. 291-302. 
ASHCROFT, E. A., 
AND WADGE, 
W. 
W. 
1976a. 
Lucid-A 
formal system for writing 
and proving 
programs. SIAM J. Comput. 5, 3,336-354. 
ASHCROFT, E. A., AND WADGE, W. W. 1976b. 
Lucid, 
a 
nonprocedural 
language 
with 
iteration. 
Commun. ACM 20, 519-526. 
AUGUSTSSON, L. 1984. 
A compiler 
for Lazy ML. In 
Proceedings 
1984 ACM Conference on LISP and 
Functional 
Programming 
(August). 
ACM, 
pp. 
218-227. 
AUGUSTSSON, L. 1985. 
Compiling pattern-matching. 
In Functional 
Programming 
Languages and Com- 
puter Architecture. 
Springer-Verlag 
LNCS 
201, 
pp. 368-381. 
BACKUS, J. 1978. 
Can programming 
be liberated 
from the von Neumann 
style? A functional 
style 
and its algebra of programs. 
Commun. ACM 21, 
8,613-641. 
BACKUS, J., WILLIAMS, J. H., AND WIMMERS, E. L. 
1986. 
FL 
language 
manual 
(preliminarv 
ver- 
sion). 
Tech. 
Rep.-RJ 
5339 (54809). Computer 
Science, IBM 
Almaden 
Research Center. Alma- 
den, CA. 
BARENDREGT, H. P. 1984. 
The Lambda 
Calculus, 
Its Syntax 
and Semantics. 
Revised 
ed. North- 
Holland, 
Amsterdam. 
BERRY, 
G. 
1978. 
Sequentialite 
de 
l’bvaluation 
formelle 
des X-expressions. 
In Proceedings 
3-e 
Colloque International 
sur la Programmation. 
BIRD, R., AND WADLER, P. 1988. 
Introduction 
to 
Functional 
Programming. 
Prentice 
Hall, 
Engle- 
wood Cliffs, N.J. 
BLOSS, A. 
1988. 
Path 
analysis: 
Using 
order-of- 
evaluation 
information 
to optimize 
lazy func- 
tional languages. Ph.D. Dept. Computer 
Science, 
dissertation, 
Yale Univ. 
BLOSS, A., AND HUDAK, P. 1987. 
Path semantics. In 
Proceedings 
of Third 
Workshop 
on the Mathe- 
matical 
Foundations 
of Programming 
Language 
Semantics. 
Springer-Verlag 
LNCS 
(Tulane 
Univ., April 1987), 298, 476-489. 
BLOSS, A., HUDAK, P., AND YOUNG, J. 1988. 
Code 
optimizations 
for lazy evaluation. 
Lisp and Sym- 
bolic Computation: 
An International 
Journal 
1, 
147-164. 
BOEHM, H.-J. 1985. 
Partial 
polymorphic 
type infer- 
ence is undecidable. 
In Proceedings of 26th Symp- 
soium on Foundations 
of Computer Science. IEEE 
pp. 339-345. 
BOUTEL, B. E. 1988. 
Tui language manual. 
Tech. 
Rep. CSD-8-021. Victoria 
University 
of Welling- 
ton, Department 
of Computer 
Science. 
BURGE, W. H. 1975. 
Recursive Programming 
Tech- 
niques. Addison-Wesley, 
Reading, Mass. 
BURN, G. L., PEYTON JONES, S. L., AND ROBSON, 
J. D. 1988. 
The spineless G-machine. 
In Pro- 
ceedings 1988 ACM Conference on Lisp and Func- 
tional Programming 
(Salt Lake Citv, Utah). ACM 
SIGPLAN/SIGACT;SIGART, 
244-258. 
BURSTALL, R. M., MACQUEEN, D. B., AND SANNELLA, 
D. T. 1980. 
HOPE: An experimental 
applicative 
language. In The 1980 LISP Conference. Stanford 
University, 
Santa Clara Univ. The USP Co., pp. 
136-143. 
BURTON, F. W. 1984. 
Annotations 
to control 
paral- 
lelism 
and reduction 
order 
in the distributed 
evaluation 
of functional 
programs. ACM 
Trans. 
Program. Lang. Syst. 6, 2,159-174. 
BURTON, F. W. 1988. 
Nondeterminism 
with 
refer- 
ential 
transparency 
in functional 
programming 
languages. Comput. J. 3I,3,243-247. 
CARDELLI, L., AND WEGNER, P. 1985. 
On under- 
standing 
types, 
data 
abstraction, 
and 
poly- 
morphism. ACM Comput. Surv. 17,4, 471-522. 
CARTWRIGHT, R. 1976. 
A practical 
formal semantic 
definition 
and verification 
system for typed Lisp. 
Tech. Rep. AIM-296. 
Stanford 
Artificial 
Intelli- 
gence Laboratory. 
CHEN, M. C. 1986. 
Transformations 
of parallel 
pro- 
grams in crystal. 
In Information 
Processing 
‘86, 
Elsevier North-Holland, 
New York, pp. 455-462. 
CHURCH, A. 1932-1933. 
A set of postulates 
for the 
foundation 
of logic. Ann. Math. 
2, 33-34, 346- 
366,839-864. 
CHURCH, A. 1941. 
The Calculi of Lambda Conversion. 
Princeton 
University 
Press, Princeton, 
N.J. 
CHURCH, A., AND ROSSER, J. B. 1936. 
Some prop- 
erties of conversion. 
Trans. Am. Math. 
SOC. 39, 
472-482. 
CURRY, H. B., AND FEYS, R. 1958. 
Combinatoty 
Logic. Vol. 1. North-Holland, 
The Netherlands. 
DAMAS, L., AND MILNER, 
R. 1982. 
Principle 
type 
schemes for functional 
languages. 
In 9th ACM 
Symposium 
on Principles 
of Programming 
Lan- 
guages. ACM. 
DARLINGTON, J., AND WHILE, L. 1987. 
Controlling 
the behavior 
of functional 
language systems. In 
ACM Computing 
Surveys, Vol. 14, No. 3, September 
1989 

408 
Proceedings 
of 1987 Functional 
Programming 
Languages 
and Computer 
Architecture 
Confer- 
ence. Springer-Verlag 
LNCS 274, pp. 278300. 
DAVIS, A. L. 1978. 
The architecture 
and system 
method of DDM-1: 
A recursively 
structured 
data 
driven machine. In Proceedings 5th Annual Sym- 
posium on Computer Architecture. 
IEEE, ACM. 
DEGROOT, D., AND LINDSTROM, G. 1985. 
Functional 
and Logic Programming. 
Prentice-Hall, 
Engle- 
wood Cliffs, N.J. 
DELOSME, J.-M., AND IPSEN, I. C. F. 1985. 
An illus- 
tration 
of a methodology 
for the construction 
of 
efficient 
systolic 
architectures 
in VLSI. 
In Pro- 
ceedings 2nd International 
Symposium 
on VLSI 
Technology, 
Systems, 
and Applications, 
ITRI, 
NSC pp. 268-273. 
DENNIS, J. B., AND MISUNAS, D. P. 1974. 
A prelim- 
inary architecture 
for a basic dataflow 
processor. 
GOLDBERG, B., AND HUDAK, P. 1988. 
Implementing 
functional 
programs on a hypercube multiproces- 
sor. In Proceedings of 3rd Conference on Hyper- 
cube Concurrent 
Computers 
and Applications. 
ACM. 
GORDON, M. J., MILNER, 
R., AND WADSWORTH, 
C. P. 1979. 
Edinburgh 
LCF. 
Springer-Verlag 
LNCS 78, Berlin. 
GORDON, M., MILNER, R., MORRIS, L., NEWEY, M., 
AND WADSWORTH, C. 1978. 
A metalanguage 
for 
interactive 
proof in LCF. In Conference Record of 
the 5th Annual ACM Symposium on Princiules 
of 
Programming 
Languages. ACM, pp. 119-130. 
GUTTAG, 
J., 
HORNING, 
J., 
AND 
WILLIAMS, 
J. 
1981. 
FP with data abstraction 
and strong typ- 
ing. In Proceedings 
of the 1981 Conference 
on 
Functional 
Programming 
Languages 
and Com- 
puter Architecture. 
ACM, pp. 11-24. 
In Proceedings of the 2nd Annual 
Symposium 
on 
HANCOCK, P. 1987. 
Polymorphic 
type-checking. 
In 
Computer Architecture. 
ACM, IEEE, pp. 126-132. 
The Implementation 
of Functional 
Programming 
FAIRBAIRN, J. 1985. 
Design and implementation 
of 
Languages, S. L. Peyton Jones, Ed. Prentice-Hall 
a simple typed 
language based on the lambda 
International, 
Englewood 
Cliffs, N.J., Chapters 8 
calculus. Ph.D. dissertation, 
Univ. of Cambridge. 
and 9. 
Available 
as Computer 
Laboratory 
TR No. 75. 
HENDERSON, 
P. 
1980. 
Functional 
Programming: 
FAIRBAIRN, J., AND WRAY, S. 1987. 
Tim: A simple, 
Application 
and Implementation. 
Prentice-Hall, 
lazy abstract 
machine to execute supercombina- 
Englewood 
Cliffs, N.J. 
tors. In Proceedings of 1987 Functional 
Program- 
HENDERSON, P. 1982. 
Purely 
functional 
operating 
ming 
Languages 
and 
Computer 
Architecture 
systems. 
In Functional 
Programming 
and Its 
Conference. 
Springer 
Verlag 
LNCS 
274, pp. 
Applications: 
An 
Advance 
Course. 
Cambridge 
34-45. 
University 
Press, pp. 177-192. 
FIELD, A. J., AND HARRISON, P. G. 1988. 
Functional 
HENDERSON, P. AND MORRIS, L. 1976. 
A lazy eval- 
Programming. 
Addison-Wesley, 
Workingham, 
uator. In 3rd ACM Symposium 
on Principles 
of 
England. 
Programming 
Languages. ACM, pp. 95-103. 
FORTUNE, S., LEIVANT, 
D., AND O’DONNELL, 
M. 
HINDLEY, R. 1969. 
The principle 
type scheme of an 
1985. 
The expressiveness 
of simple and second- 
object in combinatory 
logic. Trans. Amer. Math. 
order type structures. 
J. ACM 30, 1, 151-185. 
SOC. 146, 29-60. 
FRIEDMAN, 
D. P., AND WISE, D. S. 1976. 
Cons 
HOLMSTROM, 
S. 1983. 
How to handle 
large data 
should not evaluate 
its arguments. 
In Automata, 
structures 
in functional 
languages. In Proceedings 
Languages and Programming, 
Edinburgh 
Univer- 
of SERC/Chalmers 
Workshop on Declarative 
Pro- 
sity Press, pp. 257-284. 
gramming 
Languages. SERC. 
GELERNTER, H., HANSEN, J. R., AND GERBERICH, 
HUDAK, P. 1984. 
ALFL 
Reference Manual 
and Pro- 
C. L. 1960. 
A FORTRAN-compiled 
list process- 
grammer’s 
Guide. 2nd ed. Res. Rep. YALEU/ 
ing language. J. ACM 7, 2, 87-101. 
DCS/RR-322. 
Yale University. 
GIFFORD, 
D. 
K., 
AND 
LUCASSEN, 
J. 
M. 
1986. 
HUDAK, P. 1986a. 
Arrays, 
non-determinism, 
side- 
Integrating 
functional 
and imperative 
program- 
effects, and parallelism: 
a functional 
perspective. 
ming. In Proceedings 
1986 ACM 
Conference 
on 
In Proceedings of the Santa Fe Graph Reduction 
Lisp and Functional 
Programming. 
ACM 
SIG- 
Workshop 
(Los Alamos/MCC). 
Springer-Verlag 
PLAN/SIGACT/SIGART, 
pp. 28-38. 
LNCS 279, pp. 312-327. 
GIRARD, J.-Y. 
1972. 
Interpretation 
Fonctionelle 
et 
Elimination 
des Coupures 
dans 1’Arithmetique 
d’Ordre 
Superieur. 
Ph.D. dissertation, 
Univ. 
of 
Paris. 
GOLDBERG, B. 1988a. 
Buckwheat: 
Graph reduction 
on a shared memory multiprocessor. 
In Proceed- 
ings 1988ACM 
Conference on Lisp and Functional 
Programming 
(Salt 
Lake 
City, 
Utah, 
August 
1988) ACM SIGPLAN/SIGACT/SIGART. 
GOLDBERG, B. 1988b. 
Multiprocessor 
execution 
of 
HUDAK, P. 198613. Denotational 
semantics of a para- 
functional 
programming 
language. Int. J. Parallel 
Program. 
15, 2, 103-125. 
HUDAK, 
P. 
1986. 
Para-functional 
programming. 
Computer 19,8, 60-71. 
HUDAK, P., AND ANDERSON, S. 1987. 
Pomset inter- 
pretations 
of parallel 
functional 
programs. 
In 
Proceedings 
of 1987 Functional 
Programming 
Languages 
and Computer 
Architecture 
Confer- 
ence. Springer 
Verlag LNCS 274, pp. 234-256. 
functional 
programs. Ph.D. dissertation, 
Dept. of 
HUDAK, P., AND ANDERSON, S. 1988. 
Haskell 
solu- 
Computer 
Science, Yale Univ. Available 
as Tech. 
tions to the language session problems at the 1988 
Rep. YALEU/DCS/RR-618. 
Salishan 
high-speed 
computing 
conference. 
ACM Computing Surveys, Vol. 14, No. 3, September 1989 

409 
Tech. Rep. YALEU/DCS/RR-627. 
Department 
of Computer 
Science, Yale University. 
HUDAK, P., AND MOHR, E. 1989. 
Graphinators 
and 
the dualitv 
of SIMD 
and MIMD. 
In Proceedings 
1988 ACM 
Conference 
on Lisp and Functional 
Programming 
(Salt 
Lake 
City, 
Utah, 
August). 
ACM SIGPLAN/SIGACT/SIGART. 
HUDAK, P., AND SUNDARESH, R. 1988. 
On the ex- 
pressiveness 
of purely 
functional 
I/O 
systems. 
Tech. Rep. YALEU/DCS/RR-665. 
Department 
of Computer 
Science. Yale University. 
HUDAK, P., AND SMITH, L. 1986. 
Para-functional 
programming: 
A 
paradigm 
for 
programming 
multiprocessor 
systems. In 22th ACM Symposium 
on Principles 
of Programming 
Languages. ACM, 
pp. 243-254. 
HUDAK, P., AND WADLER, P. Eds. 1988. 
Report on 
the Functional 
Programming 
Language Haskell. 
Tech. Rep. YALEU/DCS/RR656. 
Department 
of 
Computer 
Science, Yale University. 
HUGHES, 
J. 1984. 
Why 
functional 
programming 
matters. 
Tech. Rep. 16. Programming 
Method- 
ology Group, Chalmers University 
of Technology. 
HUGHES, J. 1985a. 
An efficient 
implementation 
of 
purely 
functional 
arrays. 
Tech. Rep. Program- 
ming Methodology 
Group, Chalmers 
University 
of Technology. 
HUGHES, J. 198513. Lazy memo-functions. 
In Func- 
tional 
Programming 
Languages 
and Computer 
Architecture. 
Springer-Verlag 
LNCS 
201, pp. 
129-146. 
IVERSON, K. 1962. 
A Programming 
Language. Wiley, 
New York. 
JOHNSON, S. D. 1988. 
Daisy Programming 
Manual. 
Tech. Rep. Indiana University 
Computer Science 
Department. 
KAES, S. 1988. 
Parametric 
polymorphism. 
In Pro- 
ceedings of the 2nd Eupropean 
Symposium 
on 
Programming. 
Springer-Verlag 
LNCS 300. 
KELLER, 
R. M. 
1982. 
FEL 
programmer’s 
guide. 
AMPS TR 7. University 
of Utah. 
KELLER, 
R. 
M., 
AND 
LINDSTROM, 
G. 
1985. 
Approaching 
distributed 
database 
implementa- 
tions through 
functional 
programming 
concepts. 
In International 
Conference on Distributed 
Sys- 
tems. IEEE. 
KELLER, R. M., AND SLEEP, R. 1986. 
Applicative 
caching. ACM Trans. Program. Lang. Syst. 8, 1, 
88-108. 
KELLER, 
R. M., JAYARAMAN, B., ROSE, D., AND 
LINDSTROM, G. 1980. 
FGL programmer’s 
guide. 
AMPS Tech. Memo 1. Department 
of Computer 
Science, University 
of Utah. 
KLEENE, S. C. 1936. 
X-definability 
and recursive- 
ness. Duke Math. J. 2, 340-353. 
KLEENE, S. C., AND ROSSER, J. B. 1935. 
The incon- 
sistency of certain 
forms of logic. Ann. Math. 2, 
36,630-636. 
KROEZE, H. J. 198661987. 
The TWENTEL 
system 
(version 
1). Tech. Rep. Department 
of Computer 
Science, University 
of Twente, 
The Netherlands. 
LANDIN, P. J. 1964. 
The mechanical 
evaluation 
of 
expressions. 
Comput. J. 6, 4, 308-320. 
LANDIN, 
P. J. 1965. 
A correspondence 
between 
ALGOL 
60 and 
Church’s 
lambda 
notation. 
Commun. ACM 8,89-101, 
158-165. 
LANDIN, P. J. 1966. 
The next 700 programming 
lan- 
guages. Commun. ACM 9, 3, 157-166. 
LUCASSEN, 
J. 
M., 
AND GIFFORD, 
D. 
K. 
1988. 
Polymorphic 
effect 
systems. 
In Proceedings 
of 
15th ACM Symposium on Principles 
of Program- 
ming Z,anguages. ACM, pp. 47-57. 
MARKOV, A. A. 1951. 
Teoriya 
algorifmov 
(Theory 
of algorithms). 
Trudy 
Mat. 
Inst. 
Steklou 
38, 
176-189. 
MCCARTHY, J. 1960. 
Recursive functions 
of symbolic 
expressions 
and their computation 
by machine, 
Part I. Commun. ACM 3,4, 184-195. 
MCCARTHY, 
J. 1963. 
A basis for a mathematical 
theory 
of computation. 
In Computer 
Program- 
ming and Formal 
Systems. North-Holland, 
The 
Netherlands, 
pp. 33-70. 
MCCARTHY, J. 1978. 
History 
of Lisp. In Preprints 
of 
Proceedings 
of ACM 
SZGPLAN 
History 
of Pro- 
gramming 
Languages 
Conference. 
SIGPLAN 
Notices, Vol. 13, pp. 217-223. 
MCGRAW, J. R. 1982. 
The VAL language: Descrip- 
tion and analysis. TOPLAS, 
4, 1, 44-82. 
MCGRAW, J., ALLAN, S., GLAUERT, J., AND DOBES, I. 
1983. 
SISAL: Streams and Iteration 
in a Single- 
Assignment 
Language, 
Language 
Reference 
Manual. 
Tech. Rep. M-146. Lawrence 
Livermore 
National 
Laboratory. 
MILNE, R. E., AND STRACHEY, C. 1976. 
A Theory of 
Programming 
Language 
Semantics. 
Chapman 
and Hall, London, and John Wiley, New York. 
MILNER, R. A. 1978. 
A theory of type polymorphism 
in programming. 
J. Comput. 
Syst. Sci. 17, 3, 
348-375. 
MILNER, R. 1984. 
A proposal 
for Standard 
ML. In 
Proceedings 
1984 ACM Conference on LISP and 
Functional 
Programming. 
ACM, pp. 184-197. 
MULLIN, L. R. 1988. 
A mathematics 
of arrays. Ph.D 
dissertation, 
Computer 
and Information 
Science 
and CASE Center, Syracuse University. 
NIKHIL, 
R. S., PINGALI, K., AND ARVIND. 1986. 
Id 
nouveau. Computation 
Structures 
Group Memo 
265. Laboratory 
for Computer 
Science, Massa- 
chusetts Institute 
of Technology. 
PEYTON JONES, S. L. 1987. 
The Implementation 
of 
Functional 
Programming 
Languages. 
Prentice- 
Hall International, 
Englewood 
Cliffs, N.J. 
PEYTON JONES, S. L., CLACK, C., SALKILD, J., AND 
HARDIE, M. 
GRIP-A 
high-performance 
archi- 
tecture for parallel 
graph reduction. 
In Proceed- 
ings of 1987 Functional 
Programming 
Languages 
and Computer Architecture 
Conference. Springer- 
Verlag LNCS 274, pp. 98-112. 
PFENNING, F. 1988. 
Partial 
polymorphic 
type infer- 
ence and higher-order 
unification. 
In Proceedings 
1988 ACM 
Conference 
on Lisp and Functional 
ACM Computing Surveys, Vol. 14, No. 3, September 1989 

410 
Programming 
(Salt Lake City, Utah). ACM SIG- 
PLAN/SIGACT/SIGART, 
pp. 153-163. 
POST, E. L. 
Formal 
reductions 
of the general com- 
binatorial 
decision 
problem. 
Am. J. Math. 
65, 
197-215. 
REES, J., AND CLINGER, W. Eds. 1986. 
The revised 
report 
on the 
algorithmic 
language 
Scheme. 
SIGPLAN 
Notices 21, 12, 37-79. 
REYNOLDS, J. C. 1974. 
Towards 
a theory 
of type 
structure. 
In 
Proceedings 
of Colloque 
sur 
la 
Programmation. 
Springer-Verlag 
LNCS 
19, pp. 
408-425. 
REYNOLDS, J. C. 1985. 
Three 
approaches 
to type 
structure. 
In Mathematical 
Foundations 
of Soft- 
ware Deuelopment, 
Springer-Verlag 
LNCS 
185, 
pp. 97-138. 
ROSSER, J. B. 1982. 
Highlights 
of the history 
of the 
lambda-calculus. 
In Proceedings I982 ACM Con- 
ference on LISP 
and Functional 
Programming. 
ACM, pp. 216-225. 
SCHMIDT, ,D. A. 1985. 
Detecting 
global 
variables 
in 
denotational 
specifications. 
ACM 
Trans. 
Program. Lung. Syst. 7, 2, 299-310. 
SCH~NFINKEL, 
M. 
1924. 
Uber 
die bausteine 
der 
mathematischen 
logik. Muthematische 
Annalen 
92, 305. 
SCOTT, D. S. 1970. 
Outline of a mathematical 
theory 
of computation. 
Programming 
Research 
Group 
PRG-2, Oxford 
University. 
SHAPIRO, E. 1989: 
Systolic Programming: 
A Para- 
digm 
of Parallel 
Processing. 
Department 
of 
Applied 
Mathematics 
Tech. Rep. CS84-21, The 
Weizmann 
Institute 
of Science. 
SRIDHARAN, N. S. 1985. 
Semi-applicative 
program- 
ming: An example. Tech. Rep. BBN Laboratories. 
STEELE, JR., 
L. 
G., AND HILLIS, 
D. 
W. 
1986. 
Connection 
machine 
lisp: Fine-grained 
parallel 
symbolic 
processing. 
In Proceedings 
1986 ACM 
Conference on Lisp and Functional-Programming 
(Cambridge. 
Mass.). ACM SIGPLAN/SIGACT/ 
SIGART,pp. 
279-297. 
STOY, J. E. 1977. 
Denotational 
Semantics: The Scott- 
Strachey 
Approach 
to Programming 
Language 
Theory. The MIT 
Press, Cambridge, 
Mass. 
STOYE, W. 1985. 
A New Scheme for Writing 
Func- 
tional 
Operating 
Systems. Tech. Rep. 56. Com- 
puter Laboratory, 
University 
of Cambridge. 
THAKKAR, 
S. S. Ed. 1987. 
Selected 
Reprints 
on 
Dataflow 
and Reduction Architectures. 
The Com- 
puter Society Press, Washington, 
DC. 
TOFTE, M. 1988. 
Operational 
semantics 
and poly- 
morphic type inference. 
Ph.D. dissertation, 
Dept. 
Computer 
Science, Univ. of Edinburgh 
(CST-52- 
88). 
TRAKHTENBROT, B. A. 1988. 
Comparing 
the Church 
and Turing approaches: Two prophetic 
messages. 
Tech. 
Rep. 98/88. Eskenasy 
Institute 
of Com- 
puter Science, Tel-Aviv 
University. 
TRELEAVEN, P. C., BROWNBRIDGE, D. R., AND HOP- 
KINS, 
R. P. 1982. 
Data-driven 
and demand- 
driven computer architectures. 
Comput. Suru. 14, 
1,93-143. 
Tu, 
H-C. 
1988. 
FAC: 
Functional 
array 
calculator 
and its application 
to APL and functional 
pro- 
gramming. 
Ph.D. dissertation, 
Dept. Computer 
Science, 
Yale 
Univ. 
Available 
as Res. Rep. 
YALEU/DCS/RR-468. 
Tu, H-C., AND PERLIS, A. J. 1986. 
FAC: A functional 
APL language. IEEE Software 3, 1, 36-45. 
TURING, A. M. 1936. 
On computable 
numbers with 
an application 
to 
the 
entscheidungsproblem. 
Proc. London Math. SOC. 42, 230-265. 
TURING, A. M. 1937. 
Computability 
and X-defina- 
bility. 
J. Symbolic Logic 2, 153-163. 
TURNER, D. A. 1976. 
SASL language manual. Tech. 
Rep. Univ. St. Andrews. 
TURNER, D. A. 1979. 
A new implementation 
tech- 
inque 
for applicative 
languages. 
Softw. 
Pratt. 
Exper. 9,31-49. 
TURNER, D. A. 1981. 
The semantic 
elegance of ap- 
plicative 
languages. 
In Proceedings 
of the 1981 
Conference 
on Functional 
Programming 
Lan- 
guages and Computer 
Architecture. 
ACM, 
pp. 
85-92. 
TURNER, D. A., 1982. 
Recursion 
equations as a pro- 
gramming 
language. In Functional 
Programming 
and Its Applications: 
An Advanced Course. Cam- 
bridge University 
Press, New York, pp. l-28. 
TURNER, D. A. 1985. 
Miranda: 
A non-strict 
func- 
tional language with polymorphic 
types. In Func- 
tional 
Programming 
Languages 
and Computer 
Architecture. 
Springer-Verlag 
LNCS 
201, pp. 
1-16. 
VAN HEIJENOORT, J. 1967. 
From 
Frege to Godel. 
Harvard 
University 
Press, Cambridge, 
Mass. 
VEGDAHL, S. R. 1984. 
A survey of proposed archi- 
tectures for the execution of functional 
languages. 
IEEE 
Trans. Comput. C-23,12, 
1050-1071. 
VUILLEMIN, J. 1974. 
Correct and optimal implemen- 
tations 
of recursion 
in a simple 
programming 
language. J. Comput. Syst. Sci. 9, 3. 
WADGE, W. W., AND ASHCROFT, E. A. 1985. 
Lucid, 
the Dataflow 
Programming 
Language. Academic 
Press, London. 
WADLER, P. 1986. 
A new array operation. 
In Work- 
shop on Graph Reduction 
Techniques, 
Springer- 
Verlag LNCS 279. 
WADLER, P. 1987a. 
Efficient 
compilation 
of pattern- 
matching. 
In The Implementation 
of Functional 
Programming 
Languages, S. L. Peyton Jones, Ed. 
Prentice-Hall 
International, 
Englewood 
Cliffs, 
N.J., Chapter 5. 
WADLER, P. 1987. 
Views: A way for pattern-match- 
ing to cohabit with data abstraction. 
Tech. Rep. 
34. Programming 
Methodology 
Group, Chalmers 
Univ. 
of Technology, 
March 
1987. Preliminary 
version 
appeared in-the 
Proceedings 
of the 14th 
ACM Symposium 
on Principles 
of Programming 
Languages (January 
1987). 
ACM Computing Surveys, Vol. 14, No. 3, September 1989 

411 
WEGNER, P. 1968. 
Programming 
Languages, Infor- 
mation 
Structures, 
and Machine 
Organization. 
McGraw-Hill, 
New York. 
WIKSTR~M, 
A. 1988. 
Standard 
ML. 
Prentice-Hall, 
Englewood 
Cliffs, N.J. 
WISE, D. 1987. 
Matrix 
algebra and applicative 
pro- 
gramming. In Proceedings of 1987 Functional 
Pro- 
gramming 
Languages and Computer Architecture 
Conference, 
Springer 
Verlag 
LNCS 
274, pp. 
134-153. 
YOUNG, J. 1988. 
The Semantic 
Analysis 
of Func- 
tional Programs: Theory and Practice. Ph.D. dis- 
sertation, 
Dept. Computer 
Science, Yale Univ., 
130-142. 
WADLER, P., AND BLOTT, S. 1989. 
How to make ad 
hoc polymorphism 
less ad hoc. In Proceedings of 
16th ACM Symposium 
on Principles 
of Progmm- 
ming Languages. ACM, pp. 60-76. 
WADLER, P., AND MILLER, Q. 1988. 
An Introduction 
to Orwell. 
Tech. 
Rep. Programming 
Research 
Group, Oxford University. 
(First version, 
1985.) 
WADSWORTH, C. P. 1971. 
Semantics and pragmatics 
of the lambda calculus. Ph.D. dissertation, 
Oxford 
Univ. 
WATSON, P., AND WATSON, I. 1987. 
Evaluating 
functional 
programs on the FLAGSHIP 
machine. 
In Proceedings 
of 1987 Functional 
Programming 
Languages 
and Computer 
Architecture 
Confer- 
ence. Springer-Verlag 
LNCS 274, pp. 80-97. 
Received May 1988; final revision accepted May 1989. 
ACM Computing Surveys, Vol. 14, No. 3, September 1989 

