Optimal Direct Policy Search
Tobias Glasmachers and J¨urgen Schmidhuber
IDSIA, University of Lugano
6928, Manno-Lugano, Switzerland
{tobias,juergen}@idsia.ch
Abstract. Hutter’s optimal universal but incomputable AIXI agent mod-
els the environment as an initially unknown probability distribution-
computing program. Once the latter is found through (incomputable)
exhaustive search, classical planning yields an optimal policy. Here we
reverse the roles of agent and environment by assuming a computable
optimal policy realizable as a program mapping histories to actions. This
assumption is powerful for two reasons: (1) The environment need not be
probabilistically computable, which allows for dealing with truly stochas-
tic environments, (2) All candidate policies are computable. In stochastic
settings, our novel method Optimal Direct Policy Search (ODPS) identi-
ﬁes the best policy by direct universal search in the space of all possible
computable policies. Unlike AIXI, it is computable, model-free, and does
not require planning. We show that ODPS is optimal in the sense that
its reward converges to the reward of the optimal policy in a very broad
class of partially observable stochastic environments.
1
Introduction
Reinforcement learning (RL) algorithms are often categorized into model-based
and model-free approaches. Model-based methods typically learn a model of the
environment and its dynamics, to be used for decision making in a second step.
Advantages include potentially ﬂexible adaptation to new tasks expressed by
diﬀerent reward functions within the same environmental dynamics. Disadvan-
tages include the high sample complexity of model learning: typically, many in-
teractions with the environment are needed to estimate the underlying dynamic
process with suﬃcient certainty.
Value function approaches do not require a full predictive model; instead
they compress knowledge about the environment into a single number per state
or state-action-pair, to model the only task-speciﬁc quantity of interest in most
RL settings, namely, expected future reward. Since single number estimation is
often more eﬃcient and the value function is a suﬃcient statistics for learning
the optimal policy, such methods can often reduce the sample complexity.
Model-free algorithms search for good policies without ever building a model
of the environment. A particular branch of model-free methods is direct policy
search [4], which includes policy gradient algorithms or evolution strategies ap-
plied to a ﬁtness function based on the reward of an episode or a ﬁxed time

2
T. Glasmachers and J. Schmidhuber
frame. Such algorithms are relatively blind to the particularities of the environ-
ment (e.g., state transitions) and the reward signal (they only care for accu-
mulated reward). One of the advantages of this blindness is that they are not
aﬀected by classical problems such as partial observability. Direct policy search
is particularly well suited for complex environments solvable by relatively simple
policies.
All of the above paradigms can be found in nature at various levels of com-
plexity. Higher-developed animals, in particular humans, use anticipation and
planning to solve many complex tasks, which to some extent requires a model of
the environment. On the other hand, ﬁndings in neuroscience studies have been
connected to value-function learning [8]. Many simpler animals exhibit complex
but inﬂexible patterns of basically pre-programmed behavior, which can be un-
derstood as the result of evolutionary direct policy search.
For most complex tasks in a human-dominated and dynamic world, model-
based approaches are intuitively expected to excel in the long run. However,
when confronted with highly unfamiliar situations, humans are able to resort
to trial and error strategies that often allow them to act better than chance,
until having collected enough experience to build a suﬃciently good predictive
model enabling them to resort to the familiar planning paradigm. Thus, direct
policy search may have its place as a bootstrapping method even in elaborate
value-function or model-based approaches.
Optimal planning is hard, but identifying a computational model of an arbi-
trarily complex but computable environment is even harder. The AIXI model [1]
does it in a provably optimal way, identifying the correct environmental model
through a minimal number of interactions with the environment, but this re-
quires a continual and exhaustive and computationally intractable systematic
search in all of program space, necessarily ignoring all issues of computational
complexity. (Note, however, recent serious eﬀorts to scale down AIXI for realistic
applications [9].)
The main contribution of this paper is a theoretical, conceptual one. While
direct policy search is sometimes viewed as a ‘last resort’ heuristic, here we show
that one can use it as a general and asymptotically optimal RL method for
all episodic POMDP problems, asymptotically on the same level as the model-
based AIXI scheme, but, unlike AIXI, incrementally computable (although not
necessarily practically feasible). It covers any other behavior-generating searcher,
such as realistically downscaled AIXI variants, by systematically enumerating all
such algorithms. In fact, many direct policy search algorithms can trivially be
understood as (non-optimal) approximations of ours.
2
Reinforcement Learning in POMDPs
We consider an agent interacting with an environment in discrete time steps
t ∈N. In each time step the environment (including the agent) is in a Markov
state st ∈S. Let D(X) denote the space of probability distributions over a

Optimal Direct Policy Search
3
measure space X. In each time step the agent perceives an observation ot ∈O
of the state st, described by ot ∼Ωst ∈D(O).
The agent performs an action at ∈A, which results in a transition to state
st+1. The dynamics of the system are described by st+1 ∼Pst,at ∈D(S). After
acting the agent obtains a task-speciﬁc feedback in terms of a scalar reward
signal rt+1 ∈R, possibly depending on st, at, and st+1. Having arrived in the
new state st+1, it also obtains the next observation ot+1, and the process starts
over.
By ht = (o1, a1, r1, . . . , ot−1, at−1, rt−1, ot) we denote the history at time t.
Let H denote the space of all possible histories. Then a (behavior) policy is a
description of how the agent acts in its environment, expressed as a mapping
π : H →D(A). We collect all such mappings in the set Π of policies.
Assume that the subset T ⊂S of terminating states is non-empty. We denote
the time of termination by the random variable T. We call a task episodic if the
suprema of expectation and variance of T are ﬁnite: sup
n
Eπ[T]
 π ∈Π
o
< ∞
and sup
n
Eπ[(T −Eπ[T])2]
 π ∈Π
o
< ∞. These prerequisites corresponds to
the standard technical assumption of ergodicity.
2.1
MDPs and POMDPs
The underlying Markov decision process (MDP) is described by the tuple (S, T,
A, S, P, R), where S ∈D(S) is the distribution of start states, Ps,a denotes the
probability distribution of transitioning from state s to the next state s′ ∼Ps,a
when taking action a, and R(s, a, s′) ∈D(R) is the distribution of rewards
obtained for this transition. In what follows we make the standard assumptions
that S and A are ﬁnite, and that for each combination of s, a, and s′ expectation
and variance of R(s, a, s′) are ﬁnite. We are only interested in episodic tasks.
Now, a partially observable Markov decision process (POMDP) is a tuple
(S, T, A, O, S, P, Ω, R), where Ω(as deﬁned above) stochastically maps states to
observations. We assume that O is ﬁnite. POMDPs are a rather general class
of environments for reinforcement learning. Most classical RL algorithms are
restricted to MDPs, because their learning rules heavily rely on the Markov
property of the state description. However, in many realistic tasks observations
are naturally stochastic and/or incomplete, such that the environment has to be
treated as a POMDP.
2.2
Objectives of Learning
The goal of learning a policy π in an episodic task is usually deﬁned to maximize
future expected reward ρ = Eπ hPT −1
t=1 rt
i
. However, in what follows we leave
the choice of the objective function ρ open and just require the form ρ(π) =
Eπ[η(hT )], where the ‘success’ function η as a function of the full episode hT
has ﬁnite expectation and bounded variance. This is a reasonable assumption

4
T. Glasmachers and J. Schmidhuber
automatically fulﬁlled for the future expected reward, since expectation and
variance of both T and Rs,a,s′ are bounded.
Note that there is an important conceptual diﬀerence between the functions
η and ρ: While η(hT ) is a quantity measurable via interaction with the environ-
ment, evaluating the objective function ρ(π) requires knowledge of the POMDP,
which is assumed to be opaque to the reinforcement learner.
2.3
Direct Search in Policy Space
Direct policy search is a class of model-free reinforcement learning algorithms.
A direct policy search algorithm searches a space Π of policies π by means
of direct search, for example, an evolutioary algorithm. Such search schemes
are particularly suitable for learning in POMDP environments, because they
make no assumptions about observations being Markovian, or treatable as nearly
Markovian. The search is direct in the sense that it relies solely on evaluations
of the objective function, here on the stochastic version η, often stabilized by
averaging over multiple episodes.
3
Optimal Direct Policy Search
The aim of the present paper is to lift direct policy search for stochastic RL
tasks to the level of universal search [2] with a Turing complete programming
language, encompassing all possible computable policies. Thus, our approach is
closely connected to deterministic universal searchers [2, 5], but lifted to the class
of stochastic RL tasks. In contrast to universally optimal [1] approaches such as
the G¨odel machine [6] we do not require the concept of proof search.
The basic idea of our approach amounts to systematically applying the enu-
merable set of computable policies to the task in a scheme that makes the statis-
tical evaluation of each such policy more and more reliable over time. At the same
time we make sure that the fraction of time spent on exploitation (in contrast
to systematic exploration) tends to one. This combination allows us to derive
an optimality theorem stating that the reward obtained by our agent converges
towards the optimal reward.
Consider a Turing machine that receives the current history ht as input on
its tape and outputs an action by the time it halts. This can be achieved by
initializing the tape with a default distribution over actions, which may or may
not be changed by the program.
The basic simple idea of our algorithm is a nested loop that simultaneously
makes the following quantities tend to inﬁnity: the number of programs consid-
ered, the number of trials over which a policy is averaged, the time given to each
program. At the same time, the fraction of trials spent on exploitation converges
towards 1.
Letting the number of programs go to inﬁnity allows for ﬁnding policies en-
coded by programs with arbitrarily high indices. At the same time, each program

Optimal Direct Policy Search
5
is given more and more but always ﬁnite execution time, circumventing the halt-
ing problem in the style of universal search [2]. Averaging the reward over more
and more episodes makes these estimates arbitrarily reliable, which is essential
to prove optimality. Finally, letting the fraction of episodes used for exploita-
tion in online mode tend to one makes sure that the overall performance of the
algorithm tends to the supremum of the performances of all computable policies.
To specify the optimal direct policy search algorithm we need the following
notation:
• Let p : N →P be an enumeration of all programs P of the Turing machine
in use, with pi denoting the i-th program.
• Let πc
p denote the policy obtained by running program p for c steps. This
policy, understood as a mapping from the history ht ∈H to a distribution
over actions, is implemented as follows: We write a ‘default’ distribution
over actions and the history ht onto the tape of the Turing machine. Then
we run program pi for c steps, or less, if it halts by itself. During this time
the program may perform arbitrary computations based on the history, and
overwrite the distributions over actions (which can, e.g., be represented as a
soft-max decision). We also write πc
i instead of πc
pi for program pi.
• Let (Nn)n∈N, (Cn)n∈N, (En)n∈N, and (Xn)n∈N be sequences of natural
numbers, all tending to inﬁnity:
• Nn denotes the number of programs considered in epoch n,
• Cn denotes the time given to each program,
• En denotes the number of episodes over which the reward of each policy
is averaged,
• Xn denotes the number of episodes for exploitation in online mode.
We need the following easily satisﬁable technical conditions on these se-
quences: (a) lim
n→∞Nn/En = 0, (b) lim
n→∞(Nn · En)/Xn = 0.
Algorithm 1: Optimal Direct Policy Search (ODPS)
for n ∈N do
vn ←−∞; bn ←0;
for i ∈{1, . . . , Nn} do
w ←0;
for e ∈{1, . . . , En} do
perform one episode according to πCn
i
, resulting in hT ;
w ←w + η(hT );
end
Ri ←w/En;
if Ri > vn then vn ←Ri; bn ←i;
end
if online-mode then perform Xn episodes according to πCn
bn ;
end

6
T. Glasmachers and J. Schmidhuber
With these conventions, the pseudo-code of Optimal Direct Policy Search (ODPS)
is given in Algorithm 1. The algorithm operates in epochs, each starting with an
exploration phase during which a number of candidate policies is systematically
evaluated. ODPS has two modes: In exploration mode it evaluates more and
more powerful policies in each epoch, without ever exploiting its ﬁndings. At
the end of each episode, after the direct policy search is stopped, it assumes to
have found the best so far solution for later exploitation phases. In online mode,
ODPS is actually exploiting the policies it ﬁnds, achieving arbitrarily close to
optimal performance in the limit. But exploitation phases are still interleaved
with exploration phases, to make sure no good policy is missed by chance.
4
Formal Optimality
The following theorems formalize optimality of ODPS:
Theorem 1. Consider the inﬁnite sequence of episodes executed by Algorithm 1
in online-mode, and let ηj denote the success η(hT ) of the j-th episode. Let
Tn = Pn
k=1 NkEk + Xk denote the number of episodes at the end of epoch n. By
¯ηn =
1
Tn
PTn
j=1 ηj we denote the success averaged over the ﬁrst n epochs. Then
it holds ¯ηn
Pr
−−−−→
n→∞sup

ρ(πc
p)
 p ∈P, c ∈N
	
, where
Pr
→denotes convergence in
probability.
Theorem 2. Consider Algorithm 1 either in online or in exploration mode. Let
bn be the index of the best program found in epoch n. Then it holds ρ(πCn
bn )
Pr
−−−−→
n→∞
sup

ρ(πc
p)
 p ∈P, c ∈N
	
, where
Pr
→denotes convergence in probability.
Proof (Proof of Theorem 2). We deﬁne ρ∗= sup

ρ(πc
p)
 p ∈P, c ∈N
	
. Fix
constants ε > 0 and δ > 0. We have to show that there exists n0 ∈N such that
for all n > n0 it holds Pr(ρ∗−ρ(πCn
bn ) > ε) < δ.
Let R(n)
i
denote the average success of policy πCn
i
during the exploration
phase of epoch n, and let ρ(n)
i
= ρ(πCn
i
) denote the corresponding objective
function value. Furthermore, let ρ∗
n = max

ρ(n)
i
)
 i ∈{1, . . . , Nn}
	
denote
the best achievable objective function value in epoch n. We have E[R(n)
i
] =
E[η(hT ) | πCn
i
] = ρ(n)
i
and Var(R(n)
i
) ≤σ2
En , where σ2 is a bound on the variance
of η. This allows us to estimate
Pr

∃i ∈{1, . . . , Nn} s.t.
R(n)
i
−ρ(n)
i
 ≥ε
4

≤Nn · Pr
R(n)
i
−ρ(n)
i
 ≥ε
4

for all i ∈{1, . . . , Nn}
≤Nn
En
· 16σ2
ε2
≤δ

Optimal Direct Policy Search
7
for all n > n1. The ﬁrst step follows from the union bound, the second one from
Chebyshev’s inequality. According to property (a) there exists n1 ∈N such that
for all n > n1 we have Nn
En <
δε2
16σ2 , which implies the last step for this choice
of n1. Together with R(n)
bn ≥R(n)
i
this implies
ρ(πCn
bn ) ≥R(n)
bn −ε
4 ≥R(n)
i
−ε
4 ≥ρ∗
n −ε
2 ,
for all i ∈{1, . . . , Nn} and n > n1, and with probability of at least (1 −δ).
Per construction we have limn→∞ρ∗
n = ρ∗. Thus, there exists n2 ∈N such
that for all n > n2 it holds ρ∗−ρ∗
n < ε/2. With n0 = max{n1, n2} it follows
that ρ(πCn
bn ) > ρ∗−ε holds for all n > n0 with probability of at least (1 −δ).
Proof (Proof of Theorem 1). Property (b) guarantees that ˜ηn −ρ(πCn
bn )
Pr
−−−−→
n→∞0,
where ˜ηn is the average performance during epoch n. It is easy to see that this
also implies ¯ηn −ρ(πCn
bn )
Pr
−−−−→
n→∞0. Now we apply Theorem 2, which proves the
assertion.
5
Discussion
Theorems 1 and 2 formally establish the asymptotic optimality of ODPS for all
ﬁnite episodic POMDP problems, as outlined in the previous sections. In explo-
ration mode, the algorithm ﬁnds arbitrarily close to optimal policies. In online
mode, it exploits its previously learned knowledge, resulting in asymptotically
optimal performance.
An interesting aside is that the ODPS algorithm, together with a simulation
of the POMDP, is a program itself and is thus available as a subroutine in
some of the programs p ∈P. This self-reference is neither helpful nor does it
pose any problem to the procedure. In particular, the ODPS algorithm does not
reason about itself, and in contrast to the G¨odel machine [7] does not attempt
to optimize its own search procedure.
Let us play a number of variations on the theme. For example, we may ask for
the best policy that computes its action in a priori limited time. This scenario
is more realistic than allowing the agent to take arbitrary long (but ﬁnite) time
to make its decisions. The restricted time requirement simpliﬁes the problem at
hand considerably. Note that any program terminating after at most n ∈N time
steps can be expressed with at most n instructions, limiting the search space to
a ﬁnite subset. Thus, all epochs can in principle search the full set of programs,
and the sequence Nn disappears from the algorithm.
We do not claim that ODPS is a practical algorithm for solving real problems
eﬃciently. How to scale it down? The probably most important step is to insert
prior bias by giving small indices in the enumeration pi to programs believed to
be likely problem solvers. In many cases such programs would be known to halt
in advance, which is why the progressively increasing sequence in computation

8
T. Glasmachers and J. Schmidhuber
time Cn could be set to inﬁnity, enabling us to transfer information from early
episodes to later ones in a straight-forward manner, such that the number of
evaluations in episode n can be reduced from En to En −En−1. The language
P can even be restricted to a non-Turing complete (typically ﬁnite) subset [3].
6
Experimental Test
The goal of our experiment is demonstrate the typical behavior of ODPS. We use
a non-trivial POMDP environment, but in order to render the search practical
we rely on a non-Turing complete encoding of policies tailored to the task.
The agent is confronted with the 49-state POMDP depicted in Figure 1. Its
states are organized into a 6 × 8 grid, plus the single terminating state on the
right. The two possible start states, each with probability 1/2, are the two circled
states at the top left. There are exactly two possible observations O = {0, 1}
and two actions A = {a1, a2}.
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
Fig. 1. Illustration of the 49-state POMDP used in
the experiments. The two circled states in the top
left are selected as start states with equal proba-
bility. The number in each circle is the observation
the agent perceives. The two actions a1 and a2 have
opposite eﬀects in the gray and white background
rows: In the gray rows, a1 moves right and a2 moves
left, where each of the two possible arrows to the
left and to the right are followed with equal prob-
ability. Actions are unrelibale, having the opposite
eﬀect in 30% of the cases. In the white rows the
roles of actions are reversed. The goal is to reach
the terminating state as quickly as possible. This
is expressed by the reward uniformly distributed in
the interval [−1, 0] for all states.
The mapping from states
to observations is determinis-
tic; the symbol observed in
each state is given in the ﬁg-
ure. Note that the agent ob-
serves only a single bit of infor-
mation per state, which makes
partial observability a serious
problem.
The eﬀects of actions are
highly stochastic. In rows with
gray
background
action
a1
moves the agent to the right,
and action a2 to the left, but
the eﬀects of the actions are
reversed in 30% of the cases.
In rows with white background
the roles of a1 and a2 are re-
versed. Furthermore, there are
two possible destination states
for each step, which are cho-
sen with equal probability. In
each time step the agent re-
ceives a reward drawn from the
uniform distribution on the in-
terval [−1, 0]. Thus, the goal of
the agent is to reach the termi-
nating state as quickly as pos-
sible.

Optimal Direct Policy Search
9
The reason for using this seemingly over-complex POMDP is that direct
policy search is particularly well suited for this type of task, because the opti-
mal policy is relatively simple. It depends only on the last three observations
(ot−2, ot−1, ot) in the form π(0, 0, 1) = π(0, 1, 0) = π(1, 0, 0) = π(1, 1, 1) = a1
and π(0, 0, 0) = π(0, 1, 1) = π(1, 0, 1) = π(1, 1, 0) = a2. This encoding of the op-
timal policy is much simpler than an encoding of its value function in the MDP
formulation, not to speak of the diﬃculties when learning in the corresponding
POMDP.
0
20
40
60
80
100
0
−5
−10
−15
−20
−25
Fig. 2. The graph shows the best ex-
ploration performance vn (dotted curve),
the performance during exploitation (solid
curve), and the total average performance
(dashed curve), over the course of the ﬁrst
100 episodes. The vertical line at episode 13
marks the point from where on the optimal
policy p128 is available.
Consequently we encode policies
as tables, mapping tuples of most
recent obversations to actions. We
enumerate the possible values of no,
the number of previous observations
taken into account; for each size we
systematically enumerate all possible
combinations of actions. There are 2no
such observations, and 2(2no) tabu-
lar policies. In early interactions when
there are fewer observations available
than processed by the policy, the ob-
servation vector is padded with ze-
ros accordingly. This encoding scheme
is not unique, which is typical when
encoding programs. For example, p1
and p3 both always execute action a1, in the form p1(·) = a1, in contrast to
p3(0) = a1, p3(1) = a1.
We ran ODPS with the settings Nn = 10n, En = ⌈n log(n + 1)⌉, and Xn =
10E3
n for 100 epochs. These sequences fulﬁll properties (a) and (b), such that
Theorems 1 and 2 hold when the number of epochs is unlimited. In the above
encoding the optimal policy happens to appear for the ﬁrst time as p128. Thus,
it is in the search space from epoch 13 on.
We monitored diﬀerent quality indicators summarized in Figure 2. We can
observe a number of typical behaviors from the graph. In the ﬁrst iterations
the number En of episodes per evaluation is too small, resulting in a huge gap
between the dotted and the solid curve (overﬁtting). After about epoch 20 the
exploitation performance becomes stable. The same holds for the identiﬁcation
of the best policy. Although this policy is available from episode 13 on, it is
not identiﬁed reliably until epoch 20, after which the number En of averages
is large enough. Thus, ODPS in exploration mode is reliably successful roughly
from epoch 20 on. The dotted curve indicates the total averaged performance
in online mode and converges only slowly towards the optimum. This is because
in each epoch a large number of episodes is allocated to systematic exploration,
during which the average performance is around −25 (not shown in the graph).

10
T. Glasmachers and J. Schmidhuber
7
Conclusion
We introduced a class of direct policy search methods that in theory can op-
timally solve any ﬁnite, stochastic, episodic POMDP problem. Our results are
extremely general in the sense that they make very few assumptions on the
underlying MDP. In particular, our approach treats stochasticity naturally and
does not need the assumption of a deterministically or probabilistically com-
putable environment. Our only restriction is that the optimal policy must be
relevant to machine learning, in the weak sense that it is a computable function
of the history.
Our ODPS algorithm is not necessarily practical, but most direct policy
search schemes are closely related (typically replacing systematic search with
heuristics, and the search space of all computable policies with a restricted sub-
space), and we outlined how to scale it down to realistic applications, describing
an illustrative experiment with a highly stochastic POMDP.
Still, this work remains conceptual in spirit, and, in the authors’ point of
view, serves the sole purpose of providing a solid theoretical understanding of
the power of direct policy search. Our work shows the principal ability of direct
policy search to solve almost arbitrary problems optimally, and thus provides a
solid theoretical justiﬁcation for its use.
References
1. Marcus Hutter. Universal Artiﬁcial Intelligence: Sequential Decisions based on Al-
gorithmic Probability. Springer, Berlin, 2004.
2. L. Levin. Universal sequential search problems. Problems of Information Transmis-
sion, 9(3):265–266, 1973.
3. Tom Schaul and J¨urgen Schmidhuber.
Towards Practical Universal Search.
In
Proceedings of the Third Conference on Artiﬁcial General Intelligence, Lugano, 2010.
4. J. Schmidhuber. Sequential decision making based on direct search. In R. Sun and
C. L. Giles, editors, Sequence Learning: Paradigms, Algorithms, and Applications.
Springer, 2001. Lecture Notes on AI 1828.
5. J. Schmidhuber. Optimal Ordered Problem Solver. Machine Learning, 54:211–254,
2004.
6. J. Schmidhuber.
G¨odel machines: Fully Self-Referential Optimal Universal Self-
Improvers. In B. Goertzel and C. Pennachin, editors, Artiﬁcial General Intelligence,
pages 119–226, 2006.
7. J. Schmidhuber. Ultimate Cognition `a la G¨odel . Cognitive Computation, 1(2):177–
193, 2009.
8. W. Schultz, P. Dayan, and P. R. Montague. A neural substrate of prediction and
reward. Science, 275(5306):1593, 1997.
9. J. Veness, K. S. Ng, M. Hutter, and D. Silver. A Monte Carlo AIXI Approximation.
Technical Report 0909.0801, arXiv, 2009.

