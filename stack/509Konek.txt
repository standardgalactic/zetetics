Probabilistic Knowledge and Cognitive Ability
Jason Konek
University of Kent
On the Bayesian view, belief is not an on-off matter. Opinions are gradable.
An agent might be more conﬁdent that it will rain in London this after-
noon than in Leeds, nearly certain that the exams she left on her desk
yesterday did not grade themselves over the evening, and so forth.1 And
these opinions pin down different truth-value estimates for different prop-
ositions. (We follow Bruno de Finetti and Richard Jeffrey in thinking of
propositions as variables that take the value 1 at worlds where they are
true, and 0 where false. Truth-value estimates are estimates of the value, 0
or 1, that the proposition takes at the actual world.) For example, if you
are more conﬁdent that it will rain in London (call this proposition
‘LONDON’) than in Leeds (call this proposition ‘LEEDS’), then your state
of opinion “pins down” a higher truth-value estimate for LONDON than
LEEDS in the following sense: every assignment of truth-value estimates
that rationalizes, or makes sense of, your opinions attaches a higher value
(a larger truth-value estimate) to LONDON than to LEEDS.2
Thanks to Joanna Burch-Brown, Chris Burr, Steve Campbell, Dmitri Gallow, Allan Gib-
bard, Martha Gibson, Pavel Janda, Jim Joyce, Ben Levinstein, Sarah Moss, Samir Okasha,
Richard Pettigrew, Patrick Shirreff, Dan Singer, Dennis Stampe, Rohan Sud, Mike Titel-
baum, audiences at University of Wisconsin at Madison and the Munich Center for Math-
ematical Philosophy, as well as anonymous referees for the Philosophical Review for helpful
comments and advice. I was supported by the ERC Starting Grant “Epistemic Utility
Theory: Foundations and Applications” during my work on this essay.
1. I follow many authors in using ‘opinions’ as a catch-all to refer any type of doxastic
attitude whatsoever; see, by way of comparison, Kaplan 2010 and Joyce 2010. So, for
example, full beliefs, comparative beliefs ( judgments of the form X is more likely than
Y ), independence judgments, and so forth, all count as opinions, on this view.
2. Kraft, Pratt, and Seidenberg (1959) and Scott (1964) provide representation
Philosophical Review, Vol. 125, No. 4, 2016
DOI 10.1215/00318108-3624754
q 2016 by Cornell University
509
Downloaded from http://read.dukeupress.edu/the-philosophical-review/article-pdf/125/4/509/462964/509Konek.pdf by WIKIPEDIA LIBRARY user on 03 July 2023

When an agent’s opinions about propositions X 1; X 2, . . . are so
rich and speciﬁc that they pin down a single estimate xi (a real number
between 0 and 1 inclusive) of the truth-value of each Xi, we say that he or
she has precise degrees of belief, or credences, for X 1; X 2, . . . . Having a precise
credence xi for a proposition Xi, then, does not require having a full
belief about the objective probability of Xi, or explicitly judging that xi is
the best estimate of Xi’s truth-value, or anything of the sort. (Neither does
it preclude this.) It simply requires having a doxastic state, or state of
opinion, with a particular property, namely, the property of being ratio-
nalizable only by sets of truth-value estimates that say xi is the best esti-
mate of Xi’s truth-value.3
When an agent has precise credences, we call the function c that
maps each proposition Xi to its corresponding truth-value estimate xi his
orher credence function, or credal distribution.4 When they exist, these credal
values, c ðX iÞ ¼ xi (the values assigned to X 1; X 2, . . . by his or her cre-
dence function), serve as numerical measures of how conﬁdent someone
in the corresponding doxastic state can be said to be of the X i, where
c ðX iÞ ¼ 0 and c ðX iÞ ¼ 1 represent minimal and maximal conﬁdence,
respectively. So, for example, if you have a credence of 0.99 that the
exams on your desk did not grade themselves over the evening, then
0.99 serves as a numerical measure of how conﬁdent you are in that
theorems that specify exactly when an agent’s comparative beliefs “pin down” precise
truth-value estimates for all comparable propositions, in the sense of singling out a unique
set of truth-value estimates that rationalize (represent) those beliefs.
3. Some Bayesians take credences to be sui generis doxastic attitudes: degreed
beliefs with precise strengths; see, by way of comparison, Pettigrew 2013a. On this
view, an agent who has only comparative beliefs, for example (opinions of the form X is
more likely than Y )—a rather different type of doxastic attitude—does not count as having
precise credences, regardless of how rich and speciﬁc those comparative beliefs happen
to be.Thiscontrasts starkly withtheviews ofSavage (1954), Jeffrey (1965), andKrantz et al.
(1971). The account of credences outlined here is more permissive. Any agent whose
opinions are rationalizable by a unique set of truth-value estimates—whether he or she
has sui generis doxastic attitudes of the sort Pettigrew envisions, or comparative beliefs,
or some combination of the two—counts as having precise credences.
4. Most authors treat ‘credal distribution’ as a slightly more general term than
‘credence function’, but use the two interchangeably when additional precision is
unnecessary. I will follow suit. On this usage, an agent’s credence function c : F ! ½0; 1
is deﬁned on the full algebra F of propositions (closed under negation and disjunction)
that he or she has opinions about. An agent’s credal distribution, on the other hand, can
refer to either (i) his or her total credence function c (see, by way of comparison, Moss
2013 and Titelbaum 2015), or (ii) the restriction of c to some contextually salient partition
of interest, X ¼ {X 1; . . . ; X n} , F (see, by way of comparison, Joyce 2005, 2009).
J A S O N K O N E K
510
Downloaded from http://read.dukeupress.edu/the-philosophical-review/article-pdf/125/4/509/462964/509Konek.pdf by WIKIPEDIA LIBRARY user on 03 July 2023

proposition. You are ninety-nine times as conﬁdent that they are still
sitting on your desk ungraded as not. We will engage in the useful ﬁction,
from here on out, that the agents under consideration have perfectly
precise credences.
Credences have a range of epistemically laudable properties, just
like full beliefs. Just as full beliefs are evaluable on the basis of their truth,
for example, credences are evaluable on the basis of their accuracy. An
agent’s credence in a proposition X is more accurate the more conﬁ-
dence he or she invests in X if X is true (that is, the closer c (X ) is to 1),
and the less conﬁdence he or she invests in X if X is false (that is, the
closer c (X ) is to 0). Accuracy is a matter of getting close to the truth, in
this sense.5 Just as full beliefs capture more or less appropriate responses
to the available evidence, so too do credences. If you are playing poker
in a casino, you should (probably) have something like a credence of
0.0001 that you will be dealt a royal ﬂush, in light of your evidence, rather
than a credence of 0.9999 (unless you know that the game is rigged, or
something of the sort); the former is a more appropriate response to your
evidence than the latter. And just as full beliefs are produced by more or
less reliable mechanisms, so too are credences. Wishful thinking, for
example, tends to produce fairly inaccurate credences, and so it is a
fairly unreliable credence-producing mechanism.
Sarah Moss (2013) argues that credences share even more in com-
mon with full beliefs than many Bayesian epistemologists have thought
(see Moss 2013, 2). In particular, she argues that credences can constitute
knowledge in much the way that full beliefs can.6 Just as your belief that
smoking causes cancer might constitute knowledge, so too might your
extremely low credence that prayer prevents cancer. Moss calls this latter
sort of knowledge “probabilistic knowledge.” The aim of this essay is to
develop a novel brand of objective Bayesianism—a theory that says, for
5. We will make this characterization more precise in section 4.2 and section 5.2
using epistemic scoring rules or inaccuracy scores. A scoring rule I maps credence functions c
and worlds w (consistent truth-value assignments) to nonnegative real numbers, Iðc; wÞ.
Iðc; wÞ measures how inaccurate c is if w is actual. Different scores capture different ways
of valuing closeness to the truth.
6. Following Moss and others, I use ‘constitutes’ to refer to (as Moss puts it) the
metaphysically innocuous relation that obtains between your doxastic and epistemic
states (see, by way of comparison, Moss 2013, 12n11). This is just the relation that holds
when, for example, a full belief “amounts to,” “counts as,” or “rises to the level of ”
knowledge.
Probabilistic Knowledge and Cognitive Ability
511
Downloaded from http://read.dukeupress.edu/the-philosophical-review/article-pdf/125/4/509/462964/509Konek.pdf by WIKIPEDIA LIBRARY user on 03 July 2023

any body of evidence, which credences to have, given that evidence7—
designed to take us some way toward securing probabilistic knowledge.
In section 1, I rehearse a few reasons for countenancing probabi-
listic knowledge. In section 2, I propose two necessary conditions on prob-
abilistic knowledge—an antiluck condition and an ability condition—
and explore the relationship between the two. I argue that, for the pur-
pose at hand—that is, designing helpful formal tools meant to help us
secure probabilistic knowledge—we ought to focus on the ability con-
dition. In sections 3–4, I search for a way to sift credal states that satisfy
this ability condition from ones that do not. I argue that a particular
“summary statistic” of credal states can help; the smaller this statistic,
the greater the extent to which its accuracy is a product of cognitive
ability. In section 5, I examine a popular objective Bayesian principle
for choosing prior (preexperiment) credences in various contexts of
inquiry: the maximum entropy principle, or MaxEnt. I use this summary
statistic to evaluate whether MaxEnt delivers skillfully produced poste-
rior (postexperiment) credences, and hence good candidates for proba-
bilistic knowledge. In section 6, I describe a novel objective Bayesian
principle: the maximum sensitivity principle, or MaxSen. I argue that
MaxSen yields better candidates for probabilistic knowledge than MaxEnt.
It yields credences whose accuracy is, to the greatest extent possible,
the product of cognitive ability. The upshot: MaxSen takes us some way
toward securing probabilistic knowledge. In section 7, I explore what
MaxSen teaches us about the nature of cognitive ability and probabilistic
knowledge. In section 8, I summarize the preceding discussion. In section
9, I address two pressing concerns. Finally, in section 10, I discuss limi-
tations of the MaxSen principle.
1. Probabilistic Knowledge
Suppose that Amy is waiting on a package. She calls the post ofﬁce to ﬁnd
out whether it was delivered this morning. The receptionist checks the
driver’s morning itinerary and says:
(1)
It might have been.
(2)
More likely than not.
(3)
It probably was.
7. More cautiously, objective Bayesian theories prescribe adopting certain ‘prior’
(preexperiment) credences relative to certain types of ‘prior’ evidence, and perhaps
contexts of inquiry or decision.
J A S O N K O N E K
512
Downloaded from http://read.dukeupress.edu/the-philosophical-review/article-pdf/125/4/509/462964/509Konek.pdf by WIKIPEDIA LIBRARY user on 03 July 2023

To a ﬁrst approximation, (1) calls for giving not-too-low credence to the
proposition that her package was delivered (credence above some fairly
minimal, contextually determined threshold, perhaps).8 (2) calls for
giving it more credence than its negation. (3) calls for high credence.9
Suppose that Amy does what’s called for. She responds in one of these
ways. Then we might describe her new state of opinion as follows:
(4)
Amy thinks that her package might have been delivered.
(5)
Amy thinks that it’s more likely than not that her package was
delivered.
(6)
Amy thinks that her package was probably delivered.
And if the receptionist’s testimony is reliable, the following seem appro-
priate too:
(7)
Amy knows that her package might have been delivered.
(8)
Amy knows that it’s more likely than not that her package was
delivered.
(9)
Amy knows that her package was probably delivered.
This raises a puzzle. On the one hand, Amy seems to acquire new
knowledge. On the other hand, she seems to lack any full beliefs that
might constitute this knowledge.10 None of her old full beliefs (prior to
updating on the receptionist’s testimony) seem like good candidates.
And (1)–(3) seem not to call for any new full beliefs. They obviously do
not call for full belief that the package was in fact delivered. Neither do
they call for any particular full belief about chance, for example, full
belief that:
(10)
There is a not-too-low chance it was delivered.
(11)
There is a higher chance it was delivered than not.
(12)
There is a fairly high chance it was delivered.
8. See Swanson 2006, sec. 2.2.2, for an account of ‘might’ along these lines.
9. Assume that Amy’s prior evidence is fairly typical: she has no special reason to
think that the receptionist is being intentionally deceptive, or anything of the sort.
10. By ‘full belief ’ I mean the categorical doxastic attitude of accepting a proposition
as true. Full beliefs are the theoretical loci of traditional epistemology: they are true or
false, justiﬁed or unjustiﬁed, reliably produced or not, constitute knowledge or not, and
so forth. Some epistemologists maintain that rational full beliefs are held with certainty,
for example, Levi (1980). Many do not. Likewise, some epistemologists are eliminativists
about full belief, for example, Jeffrey (1970). But many are not, for example, Kyburg
(1961), Foley (1992), and Leitgeb (2013). I take no stand on these issues here.
Probabilistic Knowledge and Cognitive Ability
513
Downloaded from http://read.dukeupress.edu/the-philosophical-review/article-pdf/125/4/509/462964/509Konek.pdf by WIKIPEDIA LIBRARY user on 03 July 2023

After all, Amy knows that the chance that the package was delivered
earlier this morning is currently either 0 or 1.11 And a chance of 0 is
inconsistent with any of (10)–(12). So if the receptionist’s testimony
justiﬁed full belief in any of (10)–(12), then it would also justify (together
with her background knowledge) full belief that the chance is 1! She
could conclude that there is no chance—zilch—that her package was
not delivered.
But her evidence justiﬁes no such thing. For similar reasons, it
does not justify any particular full belief about epistemic probability.12
Hence the conundrum: Amy lacks any full beliefs that might constitute
her new knowledge.
The way out of such conundrums, Moss suggests, is to counte-
nance probabilistic knowledge: credences can constitute knowledge, just as
full beliefs can.
Countenancing probabilistic knowledge not only allows us to
make sense of run-of-the-mill knowledge ascriptions like (7)–(9), it
also allows us to make sense of some puzzling ascriptions of scientiﬁc
knowledge (of special interest for us). For example, consider the most
recent assessment report (AR5) of the Intergovernmental Panel on
11. Most accounts of chance satisfy what List and Pivato (2015) call the chance-future
desideratum: the true chance distribution at any time assigns only 0 or 1 to past events.
Notably, though, Meacham (2005, 2010) rejects this. See List and Pivato (2015) for further
discussion.
12. Epistemic probabilities, very roughly, provide a measure of the unique rational
credence to have in a proposition relative to a particular body of evidence. To see that
(1)–(3) do not call for full belief in a proposition about epistemic probability (if such
probabilities exist), consider the following scenario. Amy has conclusive evidence about
whether her package was delivered or not—perhaps a well-informed mathematician told
her that it was delivered if and only if a particular mathematical proposition is true. And
she knows that her evidence is conclusive. She knows that a perfectly rational agent with
her evidence would have either credence 0 or 1 in the mathematical proposition, and
accordingly have either credence 0 or 1 that her package was delivered. But Amy cannot
tell which way her evidence points, so to speak, due to her uncertainty about the math-
ematical proposition. In that case, when the receptionist utters one of (1)–(3), she might
reasonably use this information to weigh the two hypotheses about the valence of her
evidence and arrive at some middling credence about whether her package was delivered.
But she should not fully believe that the evidential probability takes some middling value.
She knows it is either 0 or 1 (her evidence is conclusive). See Eriksson and Hajek (2007,
206–7) for further discussion.
J A S O N K O N E K
514
Downloaded from http://read.dukeupress.edu/the-philosophical-review/article-pdf/125/4/509/462964/509Konek.pdf by WIKIPEDIA LIBRARY user on 03 July 2023

Climate Change (IPCC). According to AR5, the majority of climate sci-
entists think the following:
(13)
The Antarctic ice sheet might suffer abrupt and irreversible loss,
even if anthropogenic emissions of greenhouse gases are stopped
(IPCC 2013, 18).
(14)
Warming is more likely than not to exceed 48C above preindustrial
levels by 2100 if we fail to take additional measures to curb green-
house gas emissions (ibid., 18).
(15) It is very likely that heat waves will occur more often and last longer
as surface temperature rises over the twenty-ﬁrst century (ibid., 10).
And if the IPCC’s climate models are reliable, the following seem appro-
priate too:
(16)
Those scientists know that the Antarctic ice sheet might suffer
abrupt and irreversible loss, even if anthropogenic emissions of
greenhouse gases are stopped.
(17) Those scientists know that warming is more likely than not to exceed
48C above preindustrial levels by 2100 if we fail to take additional
measures to curb greenhouse gas emissions.
(18) Those scientists know that it is very likely that heat waves will occur
more often and last longer as surface temperature rises over the
twenty-ﬁrst century.
(16)–(18) are puzzling for much the same reason that (7)–(9) are. Inqui-
ry into the climate system typically leads researchers to adjust their degrees
of belief, or credences, that various climatic events will or will not occur, as
well as their credences about the chances of those events, the effects of
different possible interventions, and so on. But it does not always yield
decisive enough data to license new full beliefs about those matters. Nev-
ertheless, such inquiry often seems to deliver new knowledge—the sort of
knowledge described in (16)–(18). Scientiﬁc inquiry seems to be a
knowledge-producing enterprise, even if not always a full-belief-produ-
cing enterprise (as it often enough is not).13 Countenancing probabilis-
tic knowledge allows us to make sense of this fact.
13. Of course, scientiﬁc inquiry often does lead researchers to adopt new full beliefs.
And sometimes these full beliefs constitute knowledge. But even when inquiry only leads
researchers to adjust their degrees of belief or credences, it still plausibly delivers new
knowledge, evidenced by the appropriateness of (16)–(18).
Probabilistic Knowledge and Cognitive Ability
515
Downloaded from http://read.dukeupress.edu/the-philosophical-review/article-pdf/125/4/509/462964/509Konek.pdf by WIKIPEDIA LIBRARY user on 03 July 2023

The probabilistic knowledge thesis—the thesis that credences can
amount to knowledge, just as full beliefs can—has a range of other moti-
vations too, Moss argues. Consider, for example, the following probabi-
listic Gettier case (adapted from Zagzebski 1996, 285–86; see Moss 2013,
10, for a similar case):14
Suppose that Mary has very good but not perfect eyesight. She walks into
the house with her daughter, glances across the living room at the man
slouched in the chair by the ﬁre, and whispers, “Quiet, honey. Daddy is
probably sleeping.” But Mary misidentiﬁes the man in the chair. It is not
her husband, Bob, but his similar-looking brother, Rob (whom Mary
thought was out of the country). Luckily, though, Mary’s high credence
that Bob is sleeping in the living room is accurate. Bob, it turns out, is
dozing in the other living room chair, along the far wall, just out of view.
In some sense, Mary has just the right credences. Her high credence that
Bob is sleeping in the living room is appropriate, or justiﬁed, in light of her
evidence, namely, the appearance of a Bob-ish looking man slouched in
the chair by the ﬁre. And her high credence is fairly accurate, since Bob is
in fact sleeping in the living room. (Recall, accuracy is a matter of getting
close to the truth in the sense of lending high credence to truths and low
credence to falsehoods.) Nevertheless, that credence is ﬂawed, in some
way. It is ﬂawed in just the same way, it seems, that Smith’s belief in the
original Gettier case is ﬂawed. Smith’s belief that the man who will get the
job has ten coins in his pocket is appropriate, or justiﬁed, in light of his
evidence (reliable testimony that Jones will get the job and a clear view of
the contents of Jones’s pockets). And Smith’s belief is true (since Smith
himself will get the job and happens to have ten coins in his pocket). But
Smith’s belief, just like Mary’s high credence, is true (accurate) primarily
by luck.15
The best explanation of the epistemic incorrectness in these two
cases, Moss argues, is that Mary’s high credence and Smith’s belief both
fail to constitute knowledge. One could attempt to explain the epistemic
incorrectness in the two cases by positing that the absence of luck is a
primitive epistemic virtue. But it would be better to identify some positive
14. See also Greco and Turri 2013, sec. 6.
15. Since Nozick 1981, epistemologists typically treat lucky true belief as true belief
that violates some appropriate modal condition, for example, sensitivity, or safety. For an
introduction to the literature on epistemic luck, see Pritchard 2005, 2008.
J A S O N K O N E K
516
Downloaded from http://read.dukeupress.edu/the-philosophical-review/article-pdf/125/4/509/462964/509Konek.pdf by WIKIPEDIA LIBRARY user on 03 July 2023

virtue that Mary’s high credence and Smith’s belief both lack, in just the
way the probabilistic knowledge thesis does:
I.
Constituting knowledge is an epistemic virtue.
II.
Epistemic luck undermines knowledge.
III.
Both Mary’s high credence and Smith’s belief are accurate primarily
by luck.
C.
Both Mary’s high credence and Smith’s belief fail to constitute knowl-
edge, and so lack a certain (positive) epistemic virtue.
To summarize, the thesis that credences can amount to knowledge, just as
full beliefs can, does important theoretical work. It allows us to make
sense of run-of-the-mill knowledge ascriptions like (7)–(9), as well as
ascriptions of scientiﬁc knowledge like (16)–(18). And it allows us to
give a satisfying, uniﬁed explanation of the epistemic incorrectness in
both qualitative and probabilistic Gettier cases. Moss argues that it does
other important work as well, for example, it allows us to develop more
plausible knowledge norms for action and decision. And we could go on.
It does even more.16 On top of that, Moss dispels some prima facie serious
concerns about probabilistic knowledge, for example, that accepting it
forces us to reject some truisms about knowledge: that it is factive, safe,
and (perhaps) sensitive. If this is all correct, it provides compelling reason
to countenance probabilistic knowledge.
My aim now is to identify necessary conditions on probabilistic
knowledge (section 2). Then we will develop formal tools for choosing
credences that help to ensure those conditions are met, at least in an
interesting range of contexts of inquiry (sections 3–6). If successful,
these tools will yield good candidates for probabilistic knowledge. They
will take us some way toward securing such knowledge. Finally, we will
16. For example, probabilistic knowledge plausibly helps us sort genuine learning
experiences from “mere cognitive disturbances.” To illustrate, suppose an agent has an
experience E that makes her certain of a proposition D (and nothing else). When does E
count as a genuine learning experience, involving the acquisition of new evidence, as
opposed to a pathological episode of some sort—a “mere cognitive disturbance”? (Get-
ting this question right is important. Different inferences are warranted in the two cases.)
Fans of E ¼ K will say: exactly when E involves her coming to know D. The same question
arises for nondogmatic learning experiences. Suppose an agent has an experience E 0 that
induces a Jeffrey-shift, that is, sets her credences over a partition in a particular way. Again
you might ask: When does E 0 count as a genuine learning experience, involving the acqui-
sition of new evidence, as opposed to a “mere cognitive disturbance” set ? One hypothesis:
exactly when those new credences (the direct result of the Jeffrey-shift) constitute prob-
abilistic knowledge.
Probabilistic Knowledge and Cognitive Ability
517
Downloaded from http://read.dukeupress.edu/the-philosophical-review/article-pdf/125/4/509/462964/509Konek.pdf by WIKIPEDIA LIBRARY user on 03 July 2023

draw out some general lessons about the nature of probabilistic knowl-
edge (section 7).
2. Cognitive Ability
2.1. The Relationship between Ability and Luck
Pritchard (2010, 2012) defends an antiluck virtue epistemology. On this
view, knowledge is belief that satisﬁes two conditions: an antiluck condi-
tion and an ability condition.
ANTILUCK CONDITION. Knowledge is incompatible with luck, in the sense
of being safe: if one knows, then one’s true belief could not easily
have been false (see Pritchard 2010, 52).
ABILITY CONDITION.17 Knowledge requires cognitive ability, in the sense
that if one knows, then one’s cognitive success (the truth of one’s belief)
is the product of one’s cognitive ability (see Pritchard 2012, 248).18
Knowledge-ﬁrst theorists will be pessimistic about analyzing knowledge in
this way. But they might, nevertheless, agree that the antiluck and ability
conditions specify important properties of knowledge, properties that
make knowledge valuable. Similarly, we might suggest that suitably modi-
ﬁed antiluck and ability conditions specify important properties of prob-
abilistic knowledge, even if they do not provide the building blocks of an
analysis.
PROBABILISTIC ANTILUCK CONDITION. Probabilistic knowledge is incompat-
ible with luck, in the sense of being safe: if some of your credences
constitute knowledge, then they could not easily have been wildly
inaccurate.
17. Of course, one might understand the ability condition in a more or less demand-
ing way. For example, epistemic internalists might be inclined toward the view that the
truth of one’s belief is the product of one’s cognitive ability only if one possesses reﬂec-
tively accessible epistemic support for that belief. Epistemic externalists, alternatively,
might insist that a wide range of reliable belief-formation processes manifest cognitive
ability even in the absence of reﬂectively accessible support. We take a broadly externalist
approach in what follows. And we focus largely on one speciﬁc type of cognitive ability (see
section 2.2, especially n. 22). Nevertheless, our discussion is pertinent for both internalists
and externalists. It teaches us an important lesson about the nature of cognitive ability
even if it does not, as the internalist maintains, tell the whole story about what cognitive
ability consists in.
18. Pritchard endorses a slightly weaker ability condition, namely, that one’s cogni-
tive success must be to a signiﬁcant degree creditable to one’s cognitive ability. See, by way of
comparison, Pritchard 2012, 273.
J A S O N K O N E K
518
Downloaded from http://read.dukeupress.edu/the-philosophical-review/article-pdf/125/4/509/462964/509Konek.pdf by WIKIPEDIA LIBRARY user on 03 July 2023

PROBABILISTIC ABILITY CONDITION. Probabilistic knowledge requires cog-
nitive ability, in the sense that if some of your credences constitute
knowledge, then their success (the accuracy of one’s credences) is
the product of cognitive ability.
Typically, these conditions run together (Sosa 2007, 28–29).19 For
example, typically if an election forecaster has a high credence that
candidate A will beat candidate B, and his or her credence is not only
accurate but accurate because of his or her cognitive ability or skill (skill
in assessing the current polling data, extracting the right general lessons
from previous elections, and so on), then it will be safe as well. It could not
easily have been inaccurate. Suppose the election is, in fact, proceeding
fairly normally, or typically. There is no cockamamy plot to steal the elec-
tion for B, foiled at the last minute, or anything of the sort. So his or her
data (polling data, data about election dynamics, and so forth) could not
easily have been wildly misleading. Then, in view of his or her skill at
assessing that data, his or her high credence that A will beat B could
only be inaccurate (A loses to B) if whole districts miraculously ﬂip par-
ties, or something similar. And worlds in which whole districts miracu-
lously ﬂip parties are distant possibilities. So his orher credence could not
easily have been inaccurate. Indeed, the antiluck and ability conditions
seem so intimately related that some epistemologists presuppose that,
once spelled out correctly, one will entail the other.20
But this is a mistake. As intimately related as they are, it would be
wrong to suppose that one entails the other. The antiluck and ability
conditions impose logically independent epistemic demands on our
full beliefs, as Pritchard (2012, 249) emphasizes. The same is true of
credences. Your credences can be safe, but not accurate because skillfully
produced. And they can be accurate because skillfully produced, but not
safe. Imagine, for example, that the forecaster’s circumstances are not
normal. A large portion of the pro-candidate-A electorate could easily
have had soporiﬁc drugs slipped into their coffee the morning of the
election. But the goons tasked with delivering the drugs happened to slip
on the ice and break their vials (or something similarly outlandish). Then
our forecaster’s credence is accurate because skillfully produced, but not
safe. It could easily have been wildly inaccurate (because the goons could
easily have not slipped).
19. See also Greco and Turri 2013, sec. 6.
20. See Sosa 1999. For additional discussion, see Pritchard 2012, 248–49.
Probabilistic Knowledge and Cognitive Ability
519
Downloaded from http://read.dukeupress.edu/the-philosophical-review/article-pdf/125/4/509/462964/509Konek.pdf by WIKIPEDIA LIBRARY user on 03 July 2023

Alternatively, imagine that our election forecaster has no clue
how to assess polling data. But a crafty benefactor intent on building
up his or her self-conﬁdence is tracking his or her credence (by monitor-
ing his or her predictions and so on) and paying off voters so as to make
that credence accurate. Then it is accurate, and moreover safe, but not
accurate because skillfully produced.
So neither the probabilistic antiluck condition nor the probabi-
listic ability condition entails the other. Still, they hang together in a way
that can and ought to guide our search for helpful formal tools, tools
meant to help us secure probabilistic knowledge. In normal circum-
stances, we can mitigate dependence on luck—make our credences
safe—by reasoning skillfully. We can reason from our evidence in just
the right (skillful) way, so that its character is paramount for explaining
our success (the accuracy of our credences). And if we do, then normally
we could not easily fail (arrive at highly inaccurate credences). The
reason: normally our evidence could not easily be highly misleading.
The lesson: formal tools designed to yield skillfully produced cre-
dences will typically turn out to be doubly valuable. They will help ensure
that both the ability and antiluck conditions are satisﬁed. In contrast,
when these conditions do not hang together in the normal way—when
reasoning skillfully is not enough to mitigate dependence on luck—savvy
prior construction will likely not help however we proceed. Formal tools
for selecting credences simply will not, however well designed, stave off
safety-undermining goons with drugs.
We will focus on the probabilistic ability condition then. Our aim:
develop a new kind of objective Bayesianism designed to deliver cre-
dences that are not only reasonably likely to be accurate, in a wide
range of contexts of inquiry, but whose accuracy is, to the greatest extent
possible, a product of cognitive ability. In normal circumstances, these
credences will be safe as well. This is the best available means, it seems, to
the end of securing credences that are eligible candidates for constituting
probabilistic knowledge.21
21. Given my focus, we will forgo further discussion of the probabilistic antiluck
condition, and any pressing concerns, for example, concerns about what could possi-
bly ground the relevant (contextually sensitive) threshold, that is, the threshold below
which the accuracy of your credences cannot drop, across some range of nearby worlds, if
those credences are to amount to knowledge. Nota bene: If Moss’s approach is correct,
these concerns are moot. She explores general factivity, antiluck (safety), and sensitivity
conditions on knowledge (qualitative and probabilistic) that avoid postulating such a
threshold (Moss 2013, 17–20).
J A S O N K O N E K
520
Downloaded from http://read.dukeupress.edu/the-philosophical-review/article-pdf/125/4/509/462964/509Konek.pdf by WIKIPEDIA LIBRARY user on 03 July 2023

2.2. An Account of Cognitive Ability
What is it for the accuracy of an agent’s credences to be the product of
cognitive ability? One tempting proposal: it is just for them to be pro-
duced by some reliable credence-forming mechanism, and for their accu-
racy to be explained by the exercising of that mechanism. Suppose, for
example, that a doctor examines a patient, acquires a large mass of clini-
cal data (the results of blood tests, a lumbar puncture, and so forth), and
uses it, together with her prior data (information about which symptoms
correlate with which diseases, and so forth), to arrive at a high credence
that the patient has multiple sclerosis (MS). Suppose that her high cre-
dence is rather accurate (the patient does have MS), and her reasoning
process is reliable. Suppose ﬁnally that the fact that she reasoned so
reliably from all of this data explains why her credence is accurate to
the particular degree that it is. (It is not the case that, despite her impec-
cable reasoning, her credence is accurate simply by a stroke of luck.)
Then the accuracy of her high credence is the product of cognitive ability,
on this proposal.
Unfortunately, this proposal cannot be correct. Reasoning that
manifests cognitive skill often produces equivocal, inaccurate credences.
So such reasoning need not be particularly reliable. It need not tend to
produce highly accurate credences. Suppose that our doctor has almost
no access to the patient, and so cannot gather much clinical data. Perhaps
the patient locked herself in her ﬂat, and all the doctor can do is talk to
her through the door. Our doctor might well spread her credence fairly
evenly over a range of mutually exclusive hypotheses about the patient’s
illness in a case like this. And such credences are bound to be fairly
inaccurate (after all, they are spread fairly evenly over mostly false hypoth-
eses). Still, those credences are plausibly the product of cognitive ability.
They are (more or less) the same credences that any skilled, sane doctor
would have. The upshot: in a wide range of cases, skillful reasoning
produces fairly equivocal, inaccurate credences. So such reasoning is
not highly reliable. (It is not antireliable either, of course.) High reli-
ability is too much to demand of skill-manifesting credence-forming
mechanisms.
Instead, we should say that having cognitive skill is a matter of reason-
ing in such a way that your evidence explains the accuracy (or inaccuracy) of
your credences to the greatest degree possible (even if reasoning in this way fails
to give you a positively high chance of securing accurate credences,
for example, because your evidence is too scant, unspeciﬁc, and so
Probabilistic Knowledge and Cognitive Ability
521
Downloaded from http://read.dukeupress.edu/the-philosophical-review/article-pdf/125/4/509/462964/509Konek.pdf by WIKIPEDIA LIBRARY user on 03 July 2023

forth).22 The accuracy of your credences is the result of cognitive skill or
ability, on this view, exactly to the extent that the reasoning process that
produces them makes evidential factors (how weighty, speciﬁc, mislead-
ing, and so forth, your evidence is) comparatively important for explain-
ing that accuracy, and makes nonevidential factors comparatively
unimportant.23
This account rightly recognizes that cognitive skill is valuable not
because it always provides you with a high chance of securing highly
accurate credences (it does not, particularly when evidence is scant);
rather, cognitive skill is valuable because it makes extraneous, nonevi-
dential factors—factors that have no bearing on the character of your
evidence—irrelevant to your chance of securing such credences. When
you reason from your evidence in a way that manifests cognitive skill,
your chance of success (securing accurate credences) depends not on
whether some nonevidential factor turns out this way or that but on
whether your evidence is weighty or ﬂimsy, speciﬁc or unspeciﬁc, mis-
leading or not, and so forth.
In addition to respecting this insight about the value of cognitive
skill, it also explains our considered judgments about whether such skill is
in play in a range of cases. Suppose, for example, that a middle-aged man
comes into the emergency room with chest pain. Two doctors, Jim and
Betsy, observe him. They both have the same prior evidence, let’s imagine
(a brief description of the man’s symptoms, together with background
medical knowledge). And that prior evidence fails to discriminate
between various competing hypotheses, for example, that the patient’s
chest pain is caused by a heart attack, that it is caused by hyperventilation,
and so forth. Moreover, both Betsy’s and Jim’s prior credences (their
credences prior to acquiring new clinical data) are consistent with the
22. For the purposes of our discussion, we will understand cognitive skill narrowly as
skill at securing accurate credences. This is not to deny, however, that the exercising of per-
ceptual skills, motor skills, and so forth, involve the manifesting of cognitive skill, under-
stood more broadly, as they surely do.
23. Posterior credences are a product of both an agent’s prior credences and his or
her updating rule. So arriving at posteriors skillfully requires having the right sort of
priors/updating rule pair. I will assume, though, that the agents we consider update by
conditionalization, for the sorts of reasons outlined in Greaves and Wallace 2006, Leitgeb
and Pettigrew 2010, and Easwaran 2013 (at least when responding to traditional, dogmat-
ic learning experiences, that is, learning with certainty). They do not update by repeated
instances of MaxEnt (see sec. 5), as proposed by Jon Williamson (2010, chap. 4), or some
alternative policy. So I will suppress talk of updating rules in what follows.
J A S O N K O N E K
522
Downloaded from http://read.dukeupress.edu/the-philosophical-review/article-pdf/125/4/509/462964/509Konek.pdf by WIKIPEDIA LIBRARY user on 03 July 2023

constraints imposed by their prior evidence, let’s stipulate. But Jim’s cre-
dences also reﬂect a hunch. He “feels it in his bones” that the patient’s
chest pain is a result of hyperventilation, and so lends much more cre-
dence to that hypothesis than to the others. Betsy, on the other hand,
spreads her credence much more evenly over all of the relevant
hypotheses.
To be clear, Jim is not recognizing some ineffable signs that Betsy is
missing (in the way that Dr. House might). If he were, he would have more
evidence than Betsy, contra our assumption (even if he could not express
that evidence to anyone). The bias in Jim’s prior reﬂects a “mere hunch.”
Importantly, concentrated priors like Jim’s are rather resilient with
respect to a wide range of data.24 Learning something new does not alter
them much (for many new data items). Betsy’s prior, in contrast, is much
more malleable, much more prone to change in the face of new data. So
when Betsy runs her diagnostic tests and updates her prior, she revises her
credences quite a bit. In contrast, when Jim updates on the same clinical
data, he revises his credences fairly minimally.
Suppose that Betsy and Jim are both successful. Their updated
credences are fairly accurate. They both end up concentrating most of
their credence on the hyperventilation hypothesis, and the patient is, in
fact, suffering chest pain as a result of hyperventilation. But Jim is a little
closer to certain. His credences are a little more concentrated. So Jim’s
posterior is a little more accurate than Betsy’s. Still, his accuracy seems to
be less a product of cognitive skill than Betsy’s and more a product of his
lucky hunch.
The current account explains this judgment. Because Jim’s prior is
biased strongly in favor of the hyperventilation hypothesis (reﬂecting his
hunch), it is fairly resilient with respect to new data. As a result, his posterior
accuracy depends a great deal on his prior accuracy. His high degree of pos-
terior accuracy is explained, in no small part, by the fact that his prior
credences were quite accurate (his hunch was correct). This is constitu-
tive, on the proposed account, of failure to manifest cognitive skill.
Skilled reasoning, on this view, mitigates the explanatory relevance of
24. A distribution p is resilient with respect to a datum D to the extent that the result
of conditioning p on D, that is, pD ðÞ ¼ p ð j DÞ, is close to p. In section 5.2, I introduce
one particularly attractive divergence between distributions: Crame´r-von Mises distance,
C. If we use Crame´r-von Mises distance as our measure of “closeness” when explicating
resilience, then we can say: a distribution p is resilient with respect to a datum D to the
extent that Cðp; pDÞ is close to zero.
Probabilistic Knowledge and Cognitive Ability
523
Downloaded from http://read.dukeupress.edu/the-philosophical-review/article-pdf/125/4/509/462964/509Konek.pdf by WIKIPEDIA LIBRARY user on 03 July 2023

nonevidential factors, such as fortuitous prior accuracy, so that they are
no more relevant than required by one’s prior evidence.25
If this is right, then we can recast the probabilistic ability condition
as follows:
PROBABILISTIC ABILITY CONDITION*. Probabilistic knowledge requires cog-
nitive ability, in the following sense: if some of your credences con-
stitute knowledge, then your evidence explains their accuracy to the
greatest degree possible.
The hope now is to ﬁnd some way of sifting credal states that satisfy this
ability condition from ones that do not. My plan is to identify a particular
“summary statistic” that tracks the amount of cognitive ability that a cre-
dal state manifests. I will then use this statistic to develop a novel kind of
objective Bayesianism that will help us to secure credences that are not
only reasonably likely to be accurate, at least in a wide range of contexts of
inquiry, but whose accuracy is, to the greatest extent possible, a product
of cognitive ability. Such credences are particularly good candidates for
probabilistic knowledge.
3. Chance and Explanation
To identify this summary statistic, we need to say a bit more about how
cognitive skill manages to make evidential factors (how weighty, speciﬁc,
misleading, and so forth, your evidence is) comparatively important for
explaining the accuracy of your credences, and make nonevidential fac-
tors (lucky hunches, and so forth) comparatively unimportant.
Foreshadowing a bit (in an imprecise but illustrative way), here is
what we will ﬁnd: Past facts help to explain future events just in case they
also help to explain the chances of those events at intervening times (sup-
posing those chances are deﬁned).26 So, if wiggling the past fact does not
make the chance of the future event wiggle, so to speak, then the past fact
does not explain the future event. This is, more or less, how cognitive skill
works its magic. It makes it so that wiggling (or altering, in a Lewisian
25. Prior evidence E can require a factor F to be relevant to explaining posterior
accuracy to at least degree k in the following sense: F is relevant to at least degree k for
every prior p in the set of distributions C consistent with the constraints imposed by E.
26. More carefully, past facts help to causally explain future events in the sense of
ﬁguring into the full, complete causal explanation of those future events, just in case they also
help to explain the chances of those events at intervening times (supposing those chances
are deﬁned). See the General Argument below in this section for further discussion.
J A S O N K O N E K
524
Downloaded from http://read.dukeupress.edu/the-philosophical-review/article-pdf/125/4/509/462964/509Konek.pdf by WIKIPEDIA LIBRARY user on 03 July 2023

sense) certain past facts—for example, the fact that a hunch of yours
turned out to be spot-on (or wildly off)—does not wiggle (or alter) the
chance of a certain future event—for example, arriving at accurate cre-
dences after performing an experiment, gathering some data, and updat-
ing on that data.27 And in that case, the past fact (about prior accuracy)
plausibly does not explain the future event (attaining posterior accuracy).
The character of your data—how weighty it is, how speciﬁc it is, how
misleading it is, and so forth—explains it instead. And this is just what is
required for the future event to be the product of cognitive skill, on my view.
To ﬁll in the details of this story—about how cognitive skill makes
evidential factors important for explaining accuracy and makes nonevi-
dential factors unimportant—start by considering an instructive case
involving practical skill:
MARS ROVER. The new Mars rover is beginning its descent through Mars’s
atmosphere. At the outset, it is blind. A bulky heat shield blocks its
sensors. But before long, it will eject the heat shield, release its super-
sonic parachute, and slow down. At that point, its sensors will make
various readings, and it will maneuver its way to the landing site. If the
rover is equipped with a guidance program that makes it skilled at
landing, then its chance of success (touching down close to the target)
would be (more or less) the same (or invariant), whether it happens to
emerge from its initial (blind) descent directly above the landing site,
or one-half mile to the north, or three-quarters of a mile to the north-
east. Its skill will makeitsothat(withinreasonablebounds)factsabout
its initial proximity to the site—facts that it has no information about
during the blind part of its descent (nonevidential factors)—have
little to no impact on its chance of success. As a result, the rover’s
initial proximity to the landing site (within reasonable bounds) will
plausibly be irrelevant for explaining why that chance of success is
what it is. In turn, it seems, it will also be irrelevant for explaining why
the rover achieves whatever actual degree of success that it does.
Facts about the rover’s initial proximity to the landing site (that is, its
proximity upon emerging from initial descent) are not reﬂected in its
prior (pre-heat-shield ejection) evidence; they do nothing to explain the
character of that evidence (the rover is blind during its initial descent).
27. An alteration of one event is, to a ﬁrst approximation, another event that differs
slightly in when or how it occurs. See Lewis 2000, 188. Altering one event C fails to alter
another E if the following is true: had any one of a range of alterations C1; . . . ; Cn of C
occurred, E would have occurred just the same.
Probabilistic Knowledge and Cognitive Ability
525
Downloaded from http://read.dukeupress.edu/the-philosophical-review/article-pdf/125/4/509/462964/509Konek.pdf by WIKIPEDIA LIBRARY user on 03 July 2023

Initial proximity is, in this sense, a nonevidential factor. The rover’s skill
at landing makes this sort of nonevidential factor irrelevant twice over.28
It makes it irrelevant for explaining why the rover’s chance of success is
what it is (evidenced by the invariance of that chance across changes in
initial proximity). It also makes it irrelevant for explaining why the rover
is actually successful to the degree that it is (why it actually touches down
close to the target or not). The two are intertwined. The rover’s skill seems
to make initial proximity irrelevant for explaining actual success precisely
by making it irrelevant for explaining the chance of success. And the
reason, plausibly, that the rover’s skill can operate in this way is this: no
past fact (for example, about initial proximity) can help explain why the
rover actually lands close to the target without explaining why its chance
of landing close to the target is what it is.
This phenomenon is quite general. Chances are explanatory foci. Or
more carefully: chances are causal-explanatory foci.29 A past fact F par-
tially causally explains a future event E if and only if F partially explains
why the chance of E takes the values that it does at intervening times
(supposing the chance of E is deﬁned).30 (All the events of interest to
us—touching down near a landing site, diagnosing a patient’s medical
28. This is so whether or not conditions on Mars are kind enough for the rover to
have a positively high chance of success (landing near the target). If the conditions on
Mars are massively unpredictable, and the landing task massively complex, then our
skilled rover might have a low chance of landing within, say, 0.5 miles of its landing site.
(The Curiosity rover, for example, which was extremely skilled at landing, touched down
1.5 miles from its landing site.) Our rover might have a much lower chance than it would
if it blindly and unskillfully rocketed due north during initial descent (if it happens to
be approaching the landing site directly from the south), burning resources before its
sensors even come online. Still, its skill at landing makes nonevidential factors irrelevant
(or nearly irrelevant) for explaining why it achieves the degree of success that it does in
fact achieve.
29. For our purposes, it will sufﬁce if the possible outcomes of some interesting range
of experiments are chancy, as the theoretical hypotheses we typically take to explain those
outcomes posit. In that case, the events of interest for us—achieving a particular degree of
(posterior) accuracy after conditionalizing on the outcome of an experiment—are also
chancy. And in that case, our summary statistic will turn out to be a good guide to the
amount of cognitive ability that a credal state manifests, and useful for helping us secure
probabilistic knowledge.
30. A fact F about the past causally explains a future event E by explaining how or why
some past event C causally inﬂuences E. Perhaps facts are proper causal relata, as Bennett
(1988) and Mellor (1995) maintain. In that case, we might simply say: A past fact F (par-
tially) causes a future event E if and only if F (partially) causes the chance of E to take the
values that it does at intervening times (supposing the chance of E is deﬁned). But we
hope to remain neutral about the nature of causal relata. So our original formulation
J A S O N K O N E K
526
Downloaded from http://read.dukeupress.edu/the-philosophical-review/article-pdf/125/4/509/462964/509Konek.pdf by WIKIPEDIA LIBRARY user on 03 July 2023

condition, and so forth—have causal explanations. So we can safely
restrict our attention in this way.)
The General Argument that chances are causal-explanatory foci
(summed up informally following the formal presentation) goes as fol-
lows:
1.
No-Skipping-Intervening-Times Thesis. Suppose that F is a fact
about the history of some (closed) causal system S at a time
t 2 prior to the current time t, and event E occurs in the
history of S at a time t þ after t. (So t intervenes between t2
and tþ.) Then F helps to causally explain E (in the sense of
ﬁguring into the full, complete causal explanation of E) if
and only if F helps to explain one of the following:31
(i)
why some causally relevant part of S (that is, some part
of S whose state at t inﬂuences E) is in the state that it
is in at t;
(ii) why the causal laws governing S (the laws that underwrite
facts about causal inﬂuence in S) are what they are.
2.
Chance-Determination Thesis. The chance of E at t is deter-
mined—and hence explained—by the states of the causally
relevant parts of S at t, together with the causal laws govern-
ing S. So F helps to causally explain why the chance of E is
what it is at t if and only if F helps to explain either (i) or (ii).
C.
Chances as Causal-Explanatory Foci Thesis. F helps to causally
explain E if and only if F helps to explain why the chance of
E is what it is at t.
Shorter version: past facts F causally explain future events E if and only if
they explain how or why events at intervening times inﬂuence E; but facts
about such intervening events are precisely what determine the chance of
E at intervening times; so chances are causal-explanatory foci; past facts F
causally explain future events E if and only if they explain why the chance
of E is what it is at intervening times.
The General Argument provides us with good pro tanto reason to
think that chances are causal-explanatory foci quite generally. Of course,
wins the day. Whether or not past facts causally promote future events, they surely causally
explain future events, by explaining why their causal ancestors were the way they were.
31. The General Argument is modeled on Joyce 2007, 200–202.
Probabilistic Knowledge and Cognitive Ability
527
Downloaded from http://read.dukeupress.edu/the-philosophical-review/article-pdf/125/4/509/462964/509Konek.pdf by WIKIPEDIA LIBRARY user on 03 July 2023

a full defense would come with various qualiﬁcations.32 But we will not
attempt such a defense here. It will sufﬁce for our purposes if the General
Argument motivates the following more restricted thesis:
RESTRICTED CHANCE-EXPLANATION THESIS. In some important contexts of
inquiry, both evidential factors (how weighty, speciﬁc, and so forth,
your evidence is) and nonevidential factors (lucky hunches, and so
forth) help to explain why you actually arrive at accurate (or inaccu-
rate) postexperiment credences if and only if they help to explain
why your chance of arriving at such credences is what it is.
Recherche´ concerns about the General Argument will not call the
Restricted Chance-Explanation Thesis into question. So we will hence-
forth assume that chances are explanatory foci, at least in the restricted range
of contexts under consideration. Past facts (for example, about the for-
tuitous accuracy of your preexperiment credences) help to explain
future events (for example, settling on accurate postexperiment cre-
dences) just in case they also help to explain the chances of those events
at intervening times.
32. For example, if some event C causally inhibits E, but E nevertheless occurs, then
according to the No-Skipping-Intervening-Times Thesis, any past fact F that helps to
explain why C occurred also partially causally explains E (in virtue of explaining some
part of the total causal backstory of E ). Consider a concrete example. Imagine that
immediately prior to t, the chance of a particular particle P decaying at t is 0.01 (rather
low). Nevertheless, P does in fact decay at t. Now imagine that some past fact F partially
explains why P’s chance of decaying is 0.01, by explaining some past event C that inhibits
decay. Then according to the No-Skipping-Intervening-Times Thesis, F partially causally
explains why P decays at t (since F explains C and C features in the total causal backstory
of P’s decaying). This, you might think, shows that the No-Skipping-Intervening-Times
Thesis is false, at least without further qualiﬁcation.
We might respond as follows. The practice of providing explanations involves answer-
ing contextually salient questions. What this example shows is that in some explanatory
contexts, facts like F will not ﬁgure into a relevant answer to the why-question under
discussion. But this is true of nearly any putative explanans. What is important is this: in
some contexts of inquiry, facts like F do ﬁgure into the answer to the salient why-question.
For example, if an inquisitive child asks, “Why did P decay, rather than spontaneously
turning into a butterﬂy?” it would be perfectly appropriate to respond, “Well, because of F
and a number of other factors, P had some chance of decaying, albeit a small one. But P
had no chance at all of turning into a butterﬂy.” The upshot is that F is one of the causal-
explanatory factors available to ﬁgure into answers about why P decayed, even if it in fact
turns out not to be relevant in typical explanatory contexts. So it is no decisive mark
against the No-Skipping-Intervening-Times Thesis that it counts F as part of the full,
complete causal explanation of why P decayed.
J A S O N K O N E K
528
Downloaded from http://read.dukeupress.edu/the-philosophical-review/article-pdf/125/4/509/462964/509Konek.pdf by WIKIPEDIA LIBRARY user on 03 July 2023

This feature of chance—that it acts as an explanatory focal
point—might seem exotic. But it is actually quite banal. An example
will help:
ASTHMA DRUG. You have pretty bad asthma. Your doctor recommends a
new drug, BreatheEZ. Happily, it clears right up. A few months later,
you stumble across a report in the Journal of Asthma Studies. The pre-
vious clinical trials of BreatheEZ were ﬂawed. In more recent trials, it
failed to demonstrate a statistically signiﬁcant increase in positive
health outcomes compared to placebo. This, you think, is good evi-
dence that taking BreatheEZ failed to affect, or explain in any way, the
chance that the patients in those studies had of recovering. If it had,
you reason, then the new, improved clinical trials would have dem-
onstrated a statistically signiﬁcant increase in health outcomes.
Now suppose that you think that you are just like those patients in all
of the relevant respects and conclude that taking BreatheEZ does not
explain why your chance of recovery was what it was. Then it would be
natural to infer that it fails to explain why you actually recovered. Maybe
moving to a new city with different allergens, or something of the sort,
explains your recovery. Whatever the right story is, though, BreatheEZ is no
part of it. This inference is mediated by the chances-as-explanatory-foci
thesis. No past fact (the fact that you took BreatheEZ) can help explain
one’s success (recovering from asthma) without explaining why one’s
chance of success was what it was.
We will now attempt to leverage this fact about chance—that
chances are explanatory foci—to identify a “summary statistic” of credal
states that will help us sift ones that satisfy the probabilistic ability con-
dition from ones that do not. Here is how we will proceed. First, we will
look for a statistic that helps us sort out, for any given credal state, what
explains why its chance of success (posterior accuracy) is what it is, in any
experimental context. Does it reﬂect some sort of hunch that goes
beyond the available prior evidence? A hunch that explains why it has a
particularly high (or low) chance of attaining a particularly high (or low)
degree of posterior (postexperiment) accuracy? Or are those chance
facts explained, rather, by the character of the available experimental
evidence: how weighty it is, how speciﬁc it is, and so forth?
If we ﬁnd such a statistic, we will be off to the races. It will help us
sort out which factors explain why any prior (preexperiment) credal state
has the chance that it does of attaining any particular degree of posterior
(postexperiment) accuracy. And because chances are explanatory foci,
Probabilistic Knowledge and Cognitive Ability
529
Downloaded from http://read.dukeupress.edu/the-philosophical-review/article-pdf/125/4/509/462964/509Konek.pdf by WIKIPEDIA LIBRARY user on 03 July 2023

this will enable us to sort out which factors explain why that credal state
actually attains the degree of posterior accuracy that it does. And this is
exactly what determines whether such accuracy is the product of cogni-
tive ability. So we will be able to sift credal states that satisfy the probabi-
listic ability condition from ones that do not.
4. Cognitive Ability and Objective Expected Accuracy
4.1. An Example: Scientiﬁc Inquiry
To have a concrete case in front of us, while searching for our ability-
tracking summary statistic, imagine the following: A microbiologist
designs and performs an experiment to adjudicate between competing
theoretical hypotheses H 1; . . . ; H n; for example, hypotheses about
whether and how overexpression of a certain gene causes chromosomal
instability in breast tumors.33 She has prior (preexperiment) credences
for H 1; . . . ; H n; which reﬂect both her prior evidence—information
about past patients, chromosomal instability in other sorts of tumors,
and so on—as well her personal inductive hunches and quirks. And
she will soon acquire new experimental data, which she will use to update
those prior credences, to arrive at new, better-informed posterior (post-
experiment) credences.
For precision, assume that our agent has opinions about prop-
ositions in an atomic Boolean algebra F;34 which means that (i) F is
closed under negation and disjunction, and (ii) every proposition in
F can be expressed as a disjunction of atoms (the logically strongest
elements of F).35 We can think of each atom w of F as the “possible
world” in which all of the propositions X in F that w entails are true,
and all other propositions Y in F are false.36 Let W be the set of all such
33. Gene expression is the transcription and translation process that turns genetic
information into protein. A gene is overexpressed when that process goes awry, yielding a
surplus of protein.
34. F need not be ﬁnite. In some examples, though, we will assume that it is, simply
for expositional ease.
35. The atoms w of F are the logically strongest elements of F in the following sense:
for every X [ F; w either entails X or :X, and X entails w only if w ¼ X or X ¼ Y & :Y
for some Y [ F:
36. Thinking of atoms w of F as possible worlds in this way is metaphysically innoc-
uous. The point is simply that such w function as doxastically possible worlds for the agent.
They are the ﬁnest-grained descriptions of the way the world could be from his or her
perspective.
J A S O N K O N E K
530
Downloaded from http://read.dukeupress.edu/the-philosophical-review/article-pdf/125/4/509/462964/509Konek.pdf by WIKIPEDIA LIBRARY user on 03 July 2023

atoms or “worlds.”37 Let c : F ! ½0; 1 be the real-valued credence func-
tion (or credal distribution) that measures how conﬁdent our micro-
biologist is in propositions in F. And let ch : F ! ½0; 1 be the true
chance function, that is, the real-valued function that speciﬁes the true
chances of propositions in F (immediately prior to conducting the
experiment).38
Assume also that our agent satisﬁes some fairly uncontroversial
epistemic norms. Her credences are probabilistically coherent.39 She
updates by conditionalization, so that, when she acquires a new datum
D (and nothing more), she adopts a posterior credence function c 0 : F !
½0; 1 that satisﬁes c 0ðX Þ ¼ c ðX j DÞ for all X [ F.40 And she obeys the
Principal Principle. So she treats chance as an epistemic expert. When she
learns that chance’s probability for X at t is x, she straightaway adopts x
as her new credence for X.41
37. If you prefer to model propositions as sets of worlds, let W be the set of all possi-
ble worlds, and let F be a sigma-algebra of subsets of W (closed under complement
and union).
38. On many accounts of chance, for example, the propensity theories of Fetzer
(1982, 1983) and Gillies (2000), the true chance function is deﬁned only on a rather
restricted set of propositions. On other accounts, even when the chances are deﬁned, they
are often imprecise (modelable by a set of probability functions). I restrict our attention,
however, to algebras F for which the true chances are both deﬁned and precise.
39. See Joyce 1998, 2009, Predd et al. 2009, and Schervish, Seidenfeld, and Kadane
2009 for an accuracy-dominance argument that rational credence functions must be
probabilistically coherent, that is, must satisfy the laws of ﬁnitely additive probability:
1.
c ð`Þ ¼ 1
2.
c ð’Þ # c ðX Þ
3.
c ðX 1 _ . . . _ X nÞ ¼ c ðX 1Þ þ · · · þ c ðX nÞ for any pairwise incompatible
propositions X 1; . . . ; X n
Axiom 1 says that you must invest full conﬁdence in tautologies. Axiom 2 says that you
must invest at least as much conﬁdence in any proposition X as you do a contradiction.
Axiom 3 says that the amount of conﬁdence that you invest in a disjunction X 1 _ . . . _ X n
of pairwise incompatible propositions X 1; . . . ; X n must be the sum of your degrees of
conﬁdence in each of the Xi.
40. When c ðDÞ . 0, c ðX j DÞ is just c ðX & DÞ=c ðDÞ. For an expected accuracy argu-
ment for conditionalization, see Greaves and Wallace 2006, Leitgeb and Pettigrew 2010,
and Easwaran 2013.
41. For ease of exposition, I focus on a version of the Principal Principle that Petti-
grew (2016, 137) calls the extended temporal principle:
ETP. If an agent has a credence function c : F ! ½0; 1, then rationality requires
that c ðX j T chÞ ¼ ch ðXÞ for all propositions X in F, and all possible current
Probabilistic Knowledge and Cognitive Ability
531
Downloaded from http://read.dukeupress.edu/the-philosophical-review/article-pdf/125/4/509/462964/509Konek.pdf by WIKIPEDIA LIBRARY user on 03 July 2023

Finally, make a couple of additional assumptions about the prop-
ositions in our microbiologist’s algebra. She has credences about the
competing theoretical hypotheses H 1; . . . ; H n that she hopes to adjudi-
cate between, which we suppose are pairwise incompatible and jointly
exhaustive. She has credences about the different possible experimental
outcomes she might observe, or the new data she might receive,
D1; . . . ; Dm (also pairwise incompatible and jointly exhaustive). And
these two sets of propositions are related. Each hypothesis Hi speciﬁes
chances for the possible data items D1; . . . ; Dm. To have a way of talking
about them, let chi be the function that would specify the chances if Hi
were true. So the chance that our microbiologist’s experiment would
yield outcomes D1; . . . ; Dm if Hi were true is chiðD1Þ; . . . ; chiðDmÞ, respec-
tively. (This notation will be helpful shortly.)
Our question is this: What can we say about the extent to which the
character of our microbiologist’s evidence explains the accuracy of her
posterior credal state (and hence the extent to which her credences
satisfy the probabilistic ability condition)? Do other factors play a signiﬁ-
cant explanatory role? In particular, do her prior credences reﬂect some
sort of hunch that goes beyond her prior evidence (as Dr. Jim’s did), so
that her posterior accuracy depends, in large part, on fortuitous prior
accuracy?
First, we will outline the answer. Then we will unpack it. The an-
swer, brieﬂy, is this: If her objective expected posterior accuracy would be the
same (invariant), regardless of whether her prior credences reﬂect a
particularly accurate hunch or not, then that hunch plausibly plays no
role in explaining why she has a particularly high (or low) chance of
attaining a particularly high (or low) degree of posterior accuracy. And if
the accuracy (or inaccuracy) of her hunch does not explain why her
chances are what they are, then it simply cannot explain why she attains
the actual degree of posterior accuracy that she does. Chances, after all,
are explanatory foci.
The upshot: if her objective expected posterior accuracy would
be the same, whether her prior credences reﬂect a particularly accurate
hunch or not, then the accuracy of that hunch is plausibly irrelevant for
chance functions ch such that c ðT chÞ . 0, where Tch is the proposition that
the current chance function is ch.
See Pettigrew 2016 for a chance-dominance argument for ETP, as well as discussion of
alternative deference to chance principles.
J A S O N K O N E K
532
Downloaded from http://read.dukeupress.edu/the-philosophical-review/article-pdf/125/4/509/462964/509Konek.pdf by WIKIPEDIA LIBRARY user on 03 July 2023

explaining why she attains whatever actual degree of posterior accuracy
that she does. Rather, her success or failure on that front is explained
primarily by the character of her evidence: how weighty it is, how speciﬁc
it is, how misleading it is, and so forth. And this is just what is required for
the accuracy of her posterior credences to be the product of cognitive
ability.
To unpack this answer, make two observations.
4.2. Observation 1: Accuracy versus Expected Accuracy
First, we can distinguish between the actual accuracy of our microbi-
ologist’s prior and posterior credences—roughly, how close they are to
the actual truth-values—and the objective expected posterior accuracy of
her credences. A bit of background will help illuminate what objective
expected accuracy is and why it is important. Expectations are
estimates.42 More carefully, if V is some (real-valued) variable—for
example, the annual rainfall in New York, the amount of money in
your bank account, the degree of inaccuracy of a microbiologist’s cre-
dences—then Expp ðVÞ ¼ P
w[WpðwÞ·VðwÞ is the best estimate of V,
according to the probability function p, where VðwÞ is the value that V
takes in world w (the annual rainfall in New York in w, and so forth).43
Objective expectations are the estimates determined by objective prob-
ability functions—the true chance function, in particular. So, for
example, if ch is the true chance function, then its best estimate of the
annual rainfall R in New York is just the objective expected value of
R; Expch ðRÞ ¼ P
w[Wch ðwÞ·R ðwÞ:
Similarly, chance’s best estimate of how successful our microbi-
ologist will be—how accurate her credences will be after performing the
experiment, observing the outcome, and updating on her new data—is
just her objective expected posterior accuracy. Imagine, for example, that Hi is
the true theoretical hypothesis. So chi is the true chance function ( just
prior to performing the experiment). And the chance of the experiment
producing outcomes D1; . . . ; Dm is chi ðD1Þ; . . . ; chi ðDmÞ, respectively.
Now note that updating (conditioning) her prior credence function c
on these bits of possible new evidence yields different posterior cre-
42. See for example Jeffrey 2004, chap. 4.
43. Variables V : W ! R are functions from worlds to real numbers that model
(measurable) quantities of interest, for example, the annual rainfall in New York (in
inches), or the length of Russell’s yacht (in feet).
Probabilistic Knowledge and Cognitive Ability
533
Downloaded from http://read.dukeupress.edu/the-philosophical-review/article-pdf/125/4/509/462964/509Konek.pdf by WIKIPEDIA LIBRARY user on 03 July 2023

dence functions, cD1; . . . ; cDm. And each of these credence functions is (or
at least could be) accurate to a different degree. So not only does chance
have views about which bit of evidence our microbiologist will receive—
given by chi ðD1Þ; . . . ; chi ðDmÞ—it also has views about how likely she is to
attain various different degrees of posterior accuracy; views that are sum-
med up by chance’s best estimate of her success, that is, her objective
expected posterior accuracy.
To keep track of these degrees of accuracy, and to say explicitly
what our microbiologist’s objective expected posterior accuracy is, let’s
introduce some notation. First, at any world w, let “c 0” denote the result
of updating (conditioning) our microbiologist’s prior credence function
c on the experimental data that she receives in w. For example, if she
receives data Dj in w (that is, w implies Dj), then “c 0” denotes cDj at w. (So
“c 0” is a nonrigid designator.) Next, let Iðc 0; wÞ be a nonnegative
real number that (in some as of yet unspeciﬁed way) measures the
inaccuracy of c 0 at w. So if our microbiologist receives data Dj in w, then
Iðc 0; wÞ ¼ IðcDj; wÞ:
Interpret I as follows. If Iðc 0; wÞ ¼ 0, then c 0 is minimally inac-
curate at w. Inaccuracy increases as Iðc 0; wÞ grows larger. Finally, let Ic 0 :
W ! R be the function that maps any world w to c 0’s measure of inaccu-
racy at that world, Iðc 0; wÞ. (See section 5.2 for extended discussion of
inaccuracy measures.)
Now we can specify the objective expected posterior accuracy
of our microbiologist’s credences: ExpchiðI c 0Þ ¼ P
Dj[{D1; ... ;Dm}chi ðDjÞ·
P
w[Djchi ðw j DjÞ·IðcDj; wÞ. This is chi’s best estimate of how successful
she will be. It summarizes all of the information that chi sees as relevant
to what degree of posterior accuracy she will achieve. The closer
ExpchiðIc 0Þ is to zero, the more likely she is (all things considered) to be
successful, according to chi (that is, to end up with accurate postexper-
iment credences).
4.3. Observation 2: Counterfactual Independence as a Guide to Explanatory
Irrelevance
Second observation: Chance’s best estimate of how successful our micro-
biologist will be—or rather, how that estimate behaves across a range of
counterfactual scenarios—is a good guide to what is relevant (and not
relevant) for explaining why her chance of success is what it is. Imagine,
for example, that she could perform her experiment using instrument
A, B, or C. If chance’s best estimate of how successful she will be—her
J A S O N K O N E K
534
Downloaded from http://read.dukeupress.edu/the-philosophical-review/article-pdf/125/4/509/462964/509Konek.pdf by WIKIPEDIA LIBRARY user on 03 July 2023

objective expected posterior accuracy—would be the same whether she
opted for A, B, or C, then which instrument she uses plausibly plays
no role in explaining why that expectation is what it is. Counterfactual
independence provides good (but defeasible) evidence of explanatory
irrelevance.44
Moreover, expectations are typically good proxies for whole distri-
butions (at least for the purposes of sorting out what does and does not
help to explain one’s chance of success). In many experimental contexts,
if one’s expected accuracy would stay the same, were the value of some
variable to change (for example, which instrument our microbiologist
uses), then one’s entire distribution of chances over accuracy hypotheses
(hypotheses about what degree of posterior accuracy one achieves)
would stay the same.45 So we can say: which instrument she uses—A, B,
or C—plausibly plays no role in explaining why her chance of attaining
any particular degree of posterior accuracy is what it is. (We could, if we
were so inclined, avoid using expectations as proxies. But doing so would
be messy and would provide no additional insight. So the game is not
worth the candle.)46
44. Counterfactual independence provides only defeasible evidence of explanatory
irrelevance, however. Imagine, for example, that our microbiologist uses high-quality
growth media A for her cell culture. If she had used lower-quality growth media B or C,
however, her lab-mate would have added a supplement that made it functionally equiv-
alent to A. In that case, using A helps to (causally) explain why her expected accuracy is
what it is, even though that expectation would be the same whether she opted for A, B,
or C. I ignore cases of this sort, involving preemption or trumping, in what follows. Such
cases are outside the domain of applicability of the tools developed here.
45. Letting expected accuracy go proxy for the entire distribution of chances over accu-
racy hypotheses is often harmless. Speciﬁcally, when your prior credences over candidate
chance functions take the form of a beta distribution (a very ﬂexible class of prior distri-
butions; see n. 53), and those chance functions themselves are binomial distributions
(which will be the case, for example, when the trials of your experiment are identical and
independent, and yield a sequence of “successes” and “failures”), then the distribution
over accuracy hypotheses determined by any such chance function can be approximated
by an exponential distribution. And it is easy to show that the Crame´r-von Mises distance
(to take one example) between any two exponential distributions is bounded by the
difference between their means, or expected values. So objective expected posterior accu-
racy is invariant across a range of values for some variable only if the entire distribution of
chances over posterior accuracy hypotheses is invariant.
46. We could, in principle, directly examine how much one’s chance distri-
bution over accuracy hypotheses would vary across a range of values for some variable
of interest (using an appropriate metric on the space of probability distributions, such as
Crame´r-von Mises distance; see sec. 6). This would allow us to avoid the detour through
objective expected accuracy altogether.
Probabilistic Knowledge and Cognitive Ability
535
Downloaded from http://read.dukeupress.edu/the-philosophical-review/article-pdf/125/4/509/462964/509Konek.pdf by WIKIPEDIA LIBRARY user on 03 July 2023

More generally, if chance’s best estimate of your success—your
objective expected posterior accuracy—would be the same, regardless of
what value some variable V takes, then V plausibly plays no role in explain-
ing why your chance of attaining any particular degree of posterior accu-
racy is what it is.
Now recall our initial question. We wanted to know whether our
microbiologist’s prior credences reﬂect some sort of hunch that goes
beyond her prior evidence; a hunch that explains why she has a particu-
larly high (or low) chance of attaining a particularly high (or low) degree
of posterior accuracy; a hunch that, as a result, helps explain why she
attains whatever actual degree of posterior accuracy that she does. We
now have the tools to sort this out.
4.4. Explaining Posterior Accuracy
Suppose our microbiologist’s objective expected posterior accuracy
would be the same, regardless of whether her prior credences reﬂect a
particularly accurate hunch or not; regardless whether the variable of
interest for us—prior accuracy—takes a particularly high or low value.
Put differently, suppose it would be the same, regardless of whether
the true hypothesis turns out to be Hi, one that her prior credence
function c initially centers a great deal of probability mass around (in
which case c has a high degree of prior accuracy), or whether it turns
out to be Hj, one that c centers little mass around (in which case c has a
lower degree of prior accuracy).
More succinctly, suppose:
INVARIANCE. ExpchiðI c 0Þ < Expchj ðI c 0Þ for all Hi and Hj with 1 # i; j # n:
Then we can say whether c reﬂects a particularly accurate hunch or not
(whether c attains a particularly high or low degree of prior accuracy) is
irrelevant for explaining why its chance of attaining any particular degree
of posterior accuracy is what it is.
Finally, recall that chances are explanatory foci. No past fact (for
example, facts about prior accuracy) can explain a future event (attain-
ing a particular degree of posterior accuracy) without (partially) ex-
plaining why that event had the chance of occurring that it did. So,
since fortuitous prior accuracy is irrelevant for explaining why c’s chance
of posterior accuracy is what it is, it is also irrelevant for explaining why
c attains whatever actual degree of posterior accuracy that it does.
J A S O N K O N E K
536
Downloaded from http://read.dukeupress.edu/the-philosophical-review/article-pdf/125/4/509/462964/509Konek.pdf by WIKIPEDIA LIBRARY user on 03 July 2023

Our microbiologist’s posterior credences, then, are more like
Dr. Betsy’s than Dr. Jim’s. Their accuracy is more the product of cognitive
skill than lucky hunch. The reason: her prior credences have a special
property. Their objective expected posterior accuracy is invariant across
theoretical hypotheses. This allows us to tell a principled story about
what does and does not explain her success (or failure) at the end of
the day, her high (or low) degree of posterior accuracy. Her posterior
accuracy (or inaccuracy) is not explained by fortuitous prior accuracy
(or lack thereof). It is not explained by the accuracy (or inaccuracy) of
a spot-on (or wildly off) priorhunch. The reason: No such hunch helps to
explain why her chance of attaining a particularly high (or low) degree of
posterior accuracy is what it is. (If it did, then wiggling the accuracy of that
hunch by wiggling which theoretical hypothesis is true would also wiggle
her objective expected posterior accuracy.) And chances are explanatory
foci. So no such hunch helps to explain why she actually attains a particu-
larly high (or low) degree of posterior accuracy. Rather, her success (or
failure) on this front is explained primarily by the character of her evidence.
This answer to our initial question has various moving parts. But
once we get a grip on them, the point is simple. Consider an analogy. If
chance’s best estimate of how many games Ghana will win in the World
Cup—Ghana’s objective expected win total—would be the same, regard-
less of what color socks they decide to wear, then sock color plausibly plays
no role in explaining why their chance of winning one, or two, or three
games, and so forth, is what it is. And chances are explanatory foci. So sock
color also plays no role in explaining why they actually win the number of
games that they do.
Conversely, if Ghana’s objective expected win total would differ
depending, for example, on how many days of rest the team receives in
the weeks leading up to the World Cup, then rest time plausibly does play
a role in explaining why their chance of winning one, or two, or three
games, and so forth, is what it is. And again chances are explanatory foci.
So rest time also plays some role in explaining why they actually win the
number of games that they do.
Similarly, if chance’s best estimate of how successful our micro-
biologist’s credences will be—her objective expected posterior accu-
racy—would be the same, regardless of how accurate a hunch those
credences happen to encode, then that hunch plausibly plays no role
in explaining why her chance of being accurate to degree x, y, or z is what it
is. And chances are explanatory foci. So having a spot-on (or wildly off)
hunch also plays no role in explaining why she actually attains the degree
Probabilistic Knowledge and Cognitive Ability
537
Downloaded from http://read.dukeupress.edu/the-philosophical-review/article-pdf/125/4/509/462964/509Konek.pdf by WIKIPEDIA LIBRARY user on 03 July 2023

of posterior accuracy that she does. Our microbiologist’s actual degree of
posterior accuracy is, rather, explained primarily by the character of her
evidence: how weighty it is, how speciﬁc it is, whether or not it is mislead-
ing, and so forth. And this is just what is required for the accuracy of her posterior
credences to be the product of cognitive ability.
We now have the “summary statistic” of credal states that we have
been looking for; the statistic that will help us sift credal states that satisfy
the probabilistic ability condition from ones that do not. It is variance in
objective expected posterior accuracy across theoretical hypotheses. (We will say
more about how to measure variance in section 6.) The smaller this sta-
tistic, the greater the extent to which posterior accuracy is the product of
cognitive ability.
My aim now is to use this statistic to help us secure nice posterior
credences—credences that are not only reasonably likely to be accurate
but whose accuracy is the product of cognitive ability—in a wide range of
inference problems. Inference problems, of the sort familiar from classical
statistics, arise in structured but nonetheless fairly ubiquitous contexts of
inquiry (in the sciences, in engineering, and so forth): we have compet-
ing hypotheses we wish to adjudicate between; we will soon acquire new
(experimental) data that we can use (together with our prior data) to
help us so adjudicate; and we know enough about the data-generating
process (the experiment) to know which data items could be produced,
and what the chances of receiving each of the various possible data items
would be, were these different theoretical hypotheses true (typically as a
result of careful experimental design). The formal tools developed here
will take us some way toward securing probabilistic knowledge in many of
these important contexts of inquiry.
It is worth noting, before we move on, that the central examples in
sections 5 and 6 will involve a particularly simple type of inference prob-
lem: ﬂipping a coin in order to adjudicate between competing hypoth-
eses about its bias. But one might wonder: What does this tell us about the
more complicated inference problems that arise in the sciences, and so
forth? The answer: a surprising amount. To the extent that the formal
tools developed here help secure probabilistic knowledge in simple, coin-
ﬂipping problems, they will also help us secure such knowledge in any
inference problem that involves performing a ﬁxed number of identical
and independent trials of an experiment with only ﬁnitely many possible
outcomes. Quite a lot of inference problems in the sciences, engineering,
and so forth, are like this (though, of course, many are not). See section
10 for further discussion.
J A S O N K O N E K
538
Downloaded from http://read.dukeupress.edu/the-philosophical-review/article-pdf/125/4/509/462964/509Konek.pdf by WIKIPEDIA LIBRARY user on 03 July 2023

5. The Maximum Entropy Method
5.1. A Case Study
Consider an agent with opinions about propositions in an atomic Bool-
ean algebra F. She is planning to perform an experiment aimed at
adjudicating between competing theoretical hypotheses, H 1; . . . ; H n
(pairwise incompatible and jointly exhaustive). And her experiment
might yield data items, D1; . . . ; Dm (also pairwise incompatible and jointly
exhaustive). Suppose that F contains H 1; . . . ; H n and D1; . . . ; Dm.
According to one popular brand of objective Bayesianism—the
maximum entropy principle, or MaxEnt—our agent should take her prior
information into account in inquiry as follows:
.
First, she should summarize that prior information by a system
of constraints on prior credence functions b : F ! ½0; 1, for
example,
–
b ðH iÞ $ b ðH jÞ (that is, Hi is at least as probable as Hj),
–
0:6 # b ðH iÞ # 0:9 (that is, Hi is probable to at least degree
0.6 and at most 0.9), and
–
b ðH i j DÞ $ b ðH j j D 0Þ (that is, Hi conditional on D is at
least as probable as Hj conditional on D 0),
which we represent by a set C of credence functions, namely, the
set of all minimally rational credence functions, b (that is, b that
satisfy Probabilism, the Principal Principle, and so forth), con-
sistent with those constraints.
.
Second, she should adopt some prior credence function c :
F ! ½0; 1 or other that maximizes Shannon entropy over C.47
47. If the credence functions c in C are deﬁned over only ﬁnitely many theoretical
hypotheses, then their “entropy” or “uninformativeness,” on the MaxEnt approach, is
measured by their Shannon entropy,
H ðcÞ ¼ 2
X
1#i#n
c ðH iÞ log ðc ðH iÞÞ:
If they are deﬁned over uncountably many theoretical hypotheses, parameterized by
u1; . . . ; un, then it is standardly measured by their differential entropy,
h ðcÞ ¼ 2
ðþ1
21
· · ·
ðþ1
21
f ðx1; . . . ; xnÞ log ð f ðx1; . . . ; xnÞÞdxn . . . dx1;
where f is c’s joint probability density function for u1; . . . ; un. See n. 52 for more infor-
mation about probability density functions.
Probabilistic Knowledge and Cognitive Ability
539
Downloaded from http://read.dukeupress.edu/the-philosophical-review/article-pdf/125/4/509/462964/509Konek.pdf by WIKIPEDIA LIBRARY user on 03 July 2023

(Under a broad range of conditions, this “MaxEnt prior” will
turn out to be unique.)48
If she has no prior information about H 1; . . . ; Hn at all—her evidence
fails to constrain her prior credences altogether—then the maximum
entropy distribution is just the uniform distribution.49 So MaxEnt agrees
with Laplace’s Principle of Indifference:
POI. If an agent has no information about hypotheses H 1; . . . ; H n, and so
no reason to think that any one is more or less probable than any
other, then he or she ought to be equally conﬁdent in each: c ðH iÞ ¼
c ðH jÞ for all Hi and Hj.
Indeed, we can think of MaxEnt as generally making the same sort of
recommendation as POI. POI says that in one special case—when your
evidence imposes no constraints on your credences—those credences
should be uniform. MaxEnt says that in all cases your credences should
be as close to uniform as possible, while satisfying the constraints imposed
by your prior evidence.50
E. T. Jaynes offered an information-theoretic rationale for
MaxEnt. Shannon entropy, Jaynes thought, is uniquely reasonable as a
48. Since Shannon entropy H is a continuous, strictly convex, real-valued function,
there will be a unique MaxEnt prior whenever C is closed (that is, contains all of its limit
points), bounded (that is, is a subset of a ball of ﬁnite radius), and convex (that is, is closed
under mixtures). This will be the case, for example, when the agent’s prior evidential
constraints specify a (closed) range of expected values for some number of variables—the
expected annual rainfall in New York is between thirty and sixty inches (inclusive); the
expected price of Tesla’s stock at the end of the quarter is between 210 and 230 (inclu-
sive); and so forth. When there is no unique MaxEnt prior—either because (i) many
priors maximize entropy, or (ii) none do—objective Bayesians such as Jon Williamson
(2010) recommend either (iii) adopting some prior or other that maximizes entropy (in
the former case), or (iv) adopting some prior or other with “sufﬁciently high” entropy (in
the latter case), where what counts as sufﬁciently high depends on pragmatic
considerations.
49. More carefully, when our evidence provides no constraints on prior credences
c : F ! ½0; 1, and there exists a countably additive uniform distribution on F, then the
MaxEnt distribution is just the uniform distribution. In certain contexts, however, the
MaxEnt prior exists while a countably additive uniform prior does not. It is well known, for
example, that if F is an inﬁnite-dimensional space, then there is no Lebesgue measure on
F, and hence, no analogue of the standard uniform distribution on F. But there is often a
MaxEnt prior on such F. See, for example, Furrer, Aberg, and Renner 2011.
50. If c uniquely maximizes Shannon entropy over C, then it is as close as possible to
the uniform distribution u in the following sense: for any other b [ C, the Kullback-Leibler
divergence of u from b, DKL ðb; uÞ ¼ P
wb ðwÞ log ðb ðwÞ=u ðwÞÞ, is greater than the Kullback-
Leibler divergence of u from c.
J A S O N K O N E K
540
Downloaded from http://read.dukeupress.edu/the-philosophical-review/article-pdf/125/4/509/462964/509Konek.pdf by WIKIPEDIA LIBRARY user on 03 July 2023

measure of the informativeness of a prior. So “the maximum-entropy
distribution may be asserted for the positive reason that it is . . . maximally
maximally noncommittal with regard to missing information,” he con-
cluded (Jaynes 1957, 623). But many ﬁnd this argument uncompelling.51
We will explore, brieﬂy, whether MaxEnt has an alternative rationale—
whether it delivers credences that are eligible candidates for constituting
probabilistic knowledge.
Imagine that a bookie hands you and your friend a coin, and offers
you a bet: win $w if it comes up heads; lose $l if not. Neither of you have
any prior evidence about the coin’s bias, that is, the chance that it will
come up heads. But you are allowed to ﬂip the coin for a bit—fourteen
times, perhaps—in order to gather some new evidence before deciding
whether or not to take the bet. You adopt the maximum entropy prior,
which given your dearth of evidence is just the uniform distribution u. So
you spread your credence evenly over the various competing hypotheses
about the bias of the coin—hypotheses of the form B ¼ x (read: the
coin’s bias B is x).52
Your friend, in contrast, adopts a more concentrated prior cre-
dence function b (for example, a beta distribution that centers most of
its probability density around the hypothesis that the coin’s bias is 5/7;
see ﬁg. 1).53 Your friend’s prior credences, as a result (like Dr. Jim’s
in section 2), are rather resilient with respect to a wide range of data.
51. See, for example, Seidenfeld 1986 and Joyce 2009, 284–85.
52. More carefully, your credences about the coin’s bias B are given by u ðs # B #
tÞ ¼
Ð t
s f ðxÞdx for 0 # s , t # 1; where f is the uniform probability density function f,
that is, the function that assigns the same probability density to each hypothesis,
B ¼ x ð0 # x # 1Þ, namely, f ðxÞ ¼ 1.
A distribution u’s density function f, intuitively, centers more probability density on
hypotheses that u sees as more plausible. The probability that u assigns to the true hypoth-
esis being in some set S is equal to the volume under f on that region S (which is higher the
more mass f attaches to hypotheses in S).
53. Beta distributions b are parameterized by two quantities, a and b. These “shape
parameters” determine which hypotheses the distribution b focuses its probability mass
on. The “concentration parameter,” a þ b, corresponds roughly to how “peaked” b is
around its mean. The larger (smaller) a is compared to b, the more b focuses probability
mass on chance hypotheses B ¼ x with x < 1 (x < 0). Beta distributions are fairly com-
putationally tractable and form a very ﬂexible class of distributions. In fact, any prior can
be approximated by a mixture of beta distributions (Walley 1996, 9). And beta distri-
butions have nice dynamic properties as well. For example, they continue to be beta
distributions when updated on various sorts of data (they are conjugate priors to the bino-
mial, negative binomial, and geometric likelihood functions). For these reasons, I will
focus on them in many of my examples.
Probabilistic Knowledge and Cognitive Ability
541
Downloaded from http://read.dukeupress.edu/the-philosophical-review/article-pdf/125/4/509/462964/509Konek.pdf by WIKIPEDIA LIBRARY user on 03 July 2023

Flipping the coin a few times and observing its outcome does not alter
them much.54 In contrast, your prior credences (like Dr. Betsy’s) are
much more malleable, much more prone to change in the face of new
data. When you ﬂip the coin and observe its outcome, you revise your
credences quite a bit.
The upshot: your friend’s success—whatever degree of posterior
accuracy she actually ends up with after ﬂipping the coin and observing its
outcome—seems to be more a product of her lucky (or unlucky) hunch
(namely, that the coin’s bias is approximately 5/7) than yours. It is less a
product of cognitive skill.
Our summary statistic bears this out, as we will demonstrate short-
ly. Here is what we will show. Chance’s best estimate of how successful u
will be—u’s objective expected posterior accuracy—varies fairly mini-
mally across hypotheses about the coin’s bias: much less than b’s. It
would take nearly the same value, regardless of how accurate a hunch u
happens to encode (regardless of whether the true hypothesis ends up
being one that makes u have a relatively high degree of prior accuracy or
not). So having a spot-on (or wildly off) hunch about the coin’s bias plays
less of a role in explaining why your chance of securing particularly accu-
rate (or inaccurate) posterior credences is what it is. Consequently, it
plays less of a role in explaining why you actually end up with particularly
Figure 1.
The uniform distribution u and more concentrated beta distribution b (with
shape parameters a ¼ 10 and b ¼ 4Þ over hypotheses of the form B ¼ x
54. See n. 62 for an illustration of b’s resilience.
J A S O N K O N E K
542
Downloaded from http://read.dukeupress.edu/the-philosophical-review/article-pdf/125/4/509/462964/509Konek.pdf by WIKIPEDIA LIBRARY user on 03 July 2023

accurate (or inaccurate) posterior credences. The accuracy of your pos-
terior credences is explained, to a much greater degree, by the character
of your evidence: how many times you ﬂip the coin (how weighty your
evidence is), whether the frequency of heads in your sequence of coin
ﬂips ends up being close to the coin’s true bias (how misleading your
evidence is), and so forth. And this is just what is required for the accuracy
of your posterior credences to be more a product of cognitive skill than
your friend’s.
For concreteness, let’s measure (in)accuracy by an epistemic scoring
rule or inaccuracy score. An inaccuracy score is a function I, which maps
credence functions c and worlds w to nonnegative real numbers, Iðc; wÞ.
Iðc; wÞ measures how inaccurate c is if w is actual. If Iðc; wÞ equals zero,
then c is minimally inaccurate (maximally close to the truth) at w. Inac-
curacy increases as Iðc; wÞ grows larger.
Reasonable inaccuracy scores satisfy a range of constraints (see
Joyce 1998, 2009, and Pettigrew 2016). But instead of detailing these
constraints, we will simply focus our attention on a particularly attractive
inaccuracy measure: the continuous ranked probability score (CRPS). Read-
ers that are uninterested in the details of the CRPS (which are fairly
formal), or the reasons for focusing on it, can skip ahead to section 5.3.
5.2. Continuous Ranked Probability Score
The continuous ranked probability score, as detailed in, for example,
Hersbach 2000 and Gneiting and Raftery 2007, provides one way of
measuring the inaccuracy of continuous credal distributions. Your credal
distribution is continuous when you have credences about variables (for
example, the annual rainfall in New York) that can take uncountably
many values (rather than credences about, for example, a ﬁnite number
of propositions, which take only two values: 1 at worlds where they are
true, and 0 where false).55 So, in particular, if you have credences about
55. Strictly speaking, a distribution p : F ! ½0; 1 over hypotheses about the values
of variables V1; . . . ; Vn is called continuous when its cumulative distribution function
(CDF) P : F ! ½0; 1 is continuous. The CDF P of p is deﬁned by P ðxÞ ¼ p ðV # xÞ and
speciﬁes the probability (according to p ) that V takes a value less than or equal to x. (The
notion of continuity in play here is the standard topological notion. The relevant topology
on F is just the topology generated by modeling the atoms of F, which take the form
V1 ¼ x1 & . . . & Vn ¼ xn, as points x1; . . . ; xn
h
i in Rn, and lifting the standard open ball
topology on Euclidean n-space.) Some authors reserve the term for distributions with
absolutely continuous CDFs.
Probabilistic Knowledge and Cognitive Ability
543
Downloaded from http://read.dukeupress.edu/the-philosophical-review/article-pdf/125/4/509/462964/509Konek.pdf by WIKIPEDIA LIBRARY user on 03 July 2023

the bias of a coin (a variable that can take any of the uncountably many
real values between 0 and 1)—as you do in the case at hand—then your
credal distribution is continuous.
According to the CRPS, the inaccuracy at a world w of a continuous
credal distribution p over hypotheses about the value of a variable V is
given by:
where VðwÞ is the value of V at w, and 1 is the indicator function, deﬁned
as follows:56
:
The remainder of section 5.2 is devoted to unpacking this deﬁnition, and
to exploring the various desirable properties of the CRPS. But ﬁrst, a bit of
background.
The CRPS falls out of a very general framework for thinking about
accuracy. Pettigrew (2013a, 899) introduces the framework as follows:
The idea is that the epistemic utility [accuracy] of a credence function at a
world ought to be given by its ‘proximity’ to the credence function that is
‘perfect’ or ‘vindicated’ at that world—that is, the credence function that
perfectly matches whatever it is that credence functions strive to match.
Thus, we need to identify, for each world, the ‘perfect’ or ‘vindicated’
credence function; and we need to identify the correct measure of
distance from each of these credence functions to another credence
function.
The CRPS takes the perfect or vindicated credence function at w, y w, to
be the omniscient credence function, that is, the credence function that
centers all of its probability density on the hypothesis that describes the
true value of V at w, VðwÞ (for example, the hypothesis that describes
the true bias of the coin at w). In addition, it measures the proximity
of one credence function p to another q by the Crame´r-von Mises distance
between the two: Cðp; qÞ ¼
Ð 1
21 j P ðxÞ 2 Q ðxÞ j 2 dx. The proximity of p
to q, on this view, is a function of the area between their cumulative distri-
bution functions (CDFs), P and Q (counting regions of smaller divergence
56. When we say that VðwÞ ¼ x is the value of V at w, this is shorthand for the follow-
ing: the atom w entails the truth of V ¼ x and the falsehood of V ¼ y for all y – x.
J A S O N K O N E K
544
Downloaded from http://read.dukeupress.edu/the-philosophical-review/article-pdf/125/4/509/462964/509Konek.pdf by WIKIPEDIA LIBRARY user on 03 July 2023

between P and Q for less, and regions of greater divergence for more;
see ﬁg. 2). The CDF P of a distribution p over hypotheses about the
value of a variable V is deﬁned by P ðxÞ ¼ p ðV # xÞ; and speciﬁes the
probability (according to p ) that V takes a value less than or equal to x.
So to recap, the CRPS says that accuracy at a world w is a matter of
getting close to the truth at that world, in the sense of pinning down estimates
that are close to perfect or vindicated at w, where (i) closeness is measured
by Crame´r-von Mises distance, and (ii) the (alethically) perfect estimates
at w are those given by the omniscient credence function, y w, which
centers all of its probability density on the hypothesis that describes the
true value of V at w, namely, VðwÞ.
With these two components in place—a notion of perfection or
vindication for credence functions, and a reasonable measure of distance
between credence functions—it is easy to see why the CRPS takes the
form that it does. We just need one more observation: The cumulative
distribution function Yw of the perfect, or vindicated, distribution y w
over hypotheses about V is equal to
The reason is simple: Yw ðxÞ ¼ y w ðV # xÞ speciﬁes the probability, ac-
cording to y w, that V takes a value less than or equal to x. And y w assigns
probability 1 to the hypothesis that describes the true value of V at w, that
is, VðwÞ, and probability 0 to all other hypotheses. This means inter alia
Figure 2.
The Crame´r-von Mises distance between beta distributions p and q is given
by the average squared difference between their CDFs, P and Q (shaded area)
Probabilistic Knowledge and Cognitive Ability
545
Downloaded from http://read.dukeupress.edu/the-philosophical-review/article-pdf/125/4/509/462964/509Konek.pdf by WIKIPEDIA LIBRARY user on 03 July 2023

that it assigns probability 1 to the proposition that V takes a value less
than or equal to x for any x greater than or equal to VðwÞ. Shorter:
Yw ðxÞ ¼ 1 if x $ VðwÞ. Likewise, y w assigns probability 0 to the prop-
osition that V takes a value less than or equal to x for any x strictly less
than VðwÞ. Shorter: Yw ðxÞ ¼ 0 if x , VðwÞ.
This gives us the CRPS. The inaccuracy of a continuous credal
distribution p at a world w, according to the CRPS, is given by its prox-
imity—measured by Crame´r-von Mises distance—to the perfect, or vin-
dicated, credal distribution at w, y w. And the Crame´r-von Mises distance
from p to y w is just Iðp; wÞ ¼
Ð 1
21 j P ðxÞ 2 1ðx $ V ðwÞÞ j 2 dx.
The continuous ranked probability score is an attractive measure
of accuracy for a variety of reasons. It is extensional : The accuracy of a
credal distribution over hypotheses about the value of a variable V is a
function exclusively of (i) the various estimates it encodes—of the truth-
values of those hypotheses, and of V itself—and (ii) the actual values of
those quantities.57 How justiﬁed that credal distribution is, in light of
some body of evidence, how reliably produced it is, and so forth, makes
no difference to its accuracy, according to the CRPS, except insofar as
57. It is worth noting that two distributions p and q might do equally well estimating
the truth-values of hypotheses about the value of V, intuitively, and yet have different
inaccuracy scores according to the CRPS, in virtue of encoding different estimates for
V itself. For example, let V be the bias of the coin, and p and q be distributions that spread
their probability mass uniformly over the ranges [0.3, 0.4] and [0.4, 0.5], respectively.
That is, the density function that deﬁnes p; f p; attaches the same positive probability mass
( ¼ 10) to any hypothesis V ¼ x with x [ ½0:3; 0:4, and probability mass 0 to every other
hypothesis. Similarly for q (mutatis mutandis). Now suppose that the true bias of the coin
is 0.6. The value of V at the actual world w is 0.6. Then p and q seem to do equally well
estimating the truth-values of hypotheses about the value of V. They each spread their
probability mass uniformly over sets of equal Lebesgue measure—sets containing only
false hypotheses—and consequently attach no probability mass to the true hypothesis,
V ¼ 0.6. Yet, p’s best estimate of V; Expp ðV Þ ¼ 0:35, is further from V’s true value, 0.6,
than q’s best estimate, Expq ðV Þ ¼ 0:45: The CRPS treats this fact as relevant to the accu-
racy of p and q : Iðp; wÞ < 0:233 and Iðq; wÞ < 0:133.
This is a feature of the CRPS, not a bug. It is precisely to yield accuracy assessments
that are “sensitive to distance” in this way—assessments that “give credit” to a distribution
for determining a best estimate (expectation) that is close to the true value of the variable
by “assigning high probabilities to values near but not identical to the one materializ-
ing”—that statisticians sometimes opt for scores like the CRPS, rather than the predictive
deviance score, or the ignorance score, and so forth (Gneiting and Raftery 2007, 365–67).
This is precisely the reason that statisticians such as Gneiting and Raftery deem it “more
compelling to deﬁne scoring rules directly in terms of predictive cumulative distribution
functions,” as the CRPS does, rather than in terms of predictive densities (Gneiting and
Raftery 2007, 365).
J A S O N K O N E K
546
Downloaded from http://read.dukeupress.edu/the-philosophical-review/article-pdf/125/4/509/462964/509Konek.pdf by WIKIPEDIA LIBRARY user on 03 July 2023

they help to determine how close its estimates are to the actual values of
those quantities. The CRPS is truth directed: Moving your credences uni-
formly closer to the truth (while remaining coherent) always improves
accuracy.58 It is strictly proper: Any (coherent) continuous credal distri-
bution expects itself to be more accurate than any other distribution
(Gneiting and Raftery 2007, 367).59 Perhaps most telling, it is a natural
extension of the Brier score, Bðp; wÞ ¼ ð1=nÞ·Pn
i¼1 j p ðH iÞ 2 w ðH iÞ j 2—a
paradigmatically reasonable scoring rule for discrete distributions—to
the space of continuous distributions (see, by way of comparison, Joyce
2009, sec. 12; and Gneiting and Raftery 2007, 365).60 It also yields the
correct verdict about comparative accuracy in those cases where obviously
correct answers are to be had.61
5.3. Case Study Continued
The bookie hands you and your friend the coin. You adopt the uniform
prior credal distribution u over hypotheses about the coin’s bias B.
Your friend adopts a more concentrated prior credal distribution b (see
ﬁg. 3). Now imagine that the true bias of the coin is 5/7. The value of B
at the actual world w is 5/7. Imagine also that when you ﬂip the coin
your allotted fourteen times, it comes up heads ten times and tails four.
58. More carefully, the CRPS is truth directed in the following sense: if p and q are
both probabilistically coherent, continuous distributions over hypotheses about the
value of a variable V, and (i) j p ðV [ SÞ 2 w ðV [ SÞ j # j q ðV [ SÞ 2 w ðV [ SÞ j for all
S # R, and (ii) j p ðV [ S 0Þ 2 w ðV [ S 0Þ j , j q ðV [ S 0Þ 2 w ðV [ S 0Þ j for some S 0 # R,
then (iii) Iðp; wÞ , Iðq; wÞ. This is weaker than the notion of truth-directedness found
in Joyce 2009, 269, which applies unrestrictedly, not simply to coherent distributions.
But the CRPS is not deﬁned for incoherent distributions, and so should not be expected
to satisfy Joyce’s more general truth-directedness constraint.
59. For more on propriety, which goes under various names in the literature, see the
discussion of cogency in Oddie 1997, sec. 3; the discussion of self-recommendation in
Greaves and Wallace 2006, sec. 3; and the discussion of propriety in Joyce 2009, sec. 8.
60. A distribution p : F ! ½0; 1 over hypotheses about the values of variables
V1; . . . ; Vn is called discrete when its cumulative distribution function P : F ! ½0; 1 is
somewhere discontinuous (see n. 55 for additional detail). If, for example, you have
credences about a ﬁnite number of propositions, which can take only two values (1 at
worlds where they are true, and 0 where false), then your credal distribution is discrete.
61. For example, if p and q are beta distributions with the same mean x, but increas-
ing variance, then p is more accurate (less inaccurate) than q at a world w where the true
value of V is x, according to the CRPS: Iðp; wÞ , Iðq; wÞ. Similarly, if p and q are beta
distributions with the same variance, but their means—x and y, respectively—are such
that z ,x ,y, then p is more accurate (less inaccurate) than q at a world w 0 where the true
value of V is z : Iðp; w 0Þ , Iðq; w 0Þ.
Probabilistic Knowledge and Cognitive Ability
547
Downloaded from http://read.dukeupress.edu/the-philosophical-review/article-pdf/125/4/509/462964/509Konek.pdf by WIKIPEDIA LIBRARY user on 03 July 2023

So your new data D is perfectly nonmisleading. The true bias of the coin
is exactly the frequency of heads in D, namely, 5/7.
When you and your friend conditionalize on your new data D, the
two of you arrive at the posterior credence functions u 0 ¼ uD and b 0 ¼ bD,
respectively (see ﬁg. 4). The result: your friend is more successful.
Her posterior is more accurate than yours: Iðu 0; wÞ ¼ 0:028 . 0:020 ¼
Iðb 0; wÞ. But her accuracy is less a product of cognitive ability than yours.
The reason, informally, is that your friend’s credences are rather
resilient with respect to a wide range of data. Not only do they in fact
remain relatively unchanged by updating on D (evident from ﬁgs. 3
and 4), but they would have remained similarly unchanged, even if the
character of her evidence had been a bit different: had she observed eight
or nine heads, rather than ten, for example.62 As a result, the accuracy of
her credences seems to have relatively little to do with the character of her
evidence. It is explained primarily by the (fortuitous) accuracy of her prior
credences. The true bias of the coin, namely, 5/7, happened to be equal
to the mean of your friend’s rather concentrated prior credence func-
tion b. So the hypothesis describing that true bias (5/7) happened to be in
the center of the neighborhood where b lumps nearly all of its probability
density. Shorter: her hunch—that the coin’s bias is approximately 5/7—
happened to be correct. And her posterior is accurate, in no small part,
because her prior happened to be so accurate (her hunch was correct).
This is constitutive, on my view, of failure to manifest cognitive skill
or ability. Your degree of posterior accuracy is explained, on the other
hand, primarily by the character of your evidence: the fact that it is fairly
weighty (fourteen tosses, rather than only two or three), the fact that it is
maximally nonmisleading (the frequency of heads in your data sample
matches the coin’s true bias), and so on.
Our summary statistic—variance in objective expected posterior
accuracy across theoretical hypotheses—bears this out. It predicts that
the accuracy of your posterior credences is, to a much greater extent than
your friend’s, the product of cognitive ability. The reason: u’s objective
expected posterior accuracy varies fairly minimally across hypotheses
about the bias of the coin, much less than b’s.
62. The Crame´r-von Mises distance between b and bD is fairly small: only about 0.002.
The Crame´r-von Mises distance between u and uD, in contrast, is 0.068 (thirty-four times
bigger). Had your friend observed eight or nine heads, rather than ten, the distance
between her prior and posterior would have been 0.006 and 0.016, respectively. Yours
would have been 0.046 and 0.033 (eight times and two times bigger, respectively).
J A S O N K O N E K
548
Downloaded from http://read.dukeupress.edu/the-philosophical-review/article-pdf/125/4/509/462964/509Konek.pdf by WIKIPEDIA LIBRARY user on 03 July 2023

To see this, let’s simply calculate and compare the objective expect-
ed posterior accuracy of u and b, respectively, relative to each hypothesis,
B ¼ x, about the bias of the coin. Assume that the trials of your exper-
iment—the tosses of the coin—are independent and identically distributed.
That is, the chance of getting heads on any toss i, given that you get a
heads on toss j, is just the unconditional chance of getting a heads on
toss i (tosses are independent). And the unconditional chance of getting
a heads on any toss i is the same as the unconditional chance of getting a
Figure 4.
uD and bD
Figure 3.
u and b
Probabilistic Knowledge and Cognitive Ability
549
Downloaded from http://read.dukeupress.edu/the-philosophical-review/article-pdf/125/4/509/462964/509Konek.pdf by WIKIPEDIA LIBRARY user on 03 July 2023

heads on any other toss j (tosses are identically distributed). In that case,
each hypothesis, B ¼ x, about the bias of the coin fully speciﬁes chances
for all of the possible outcomes you might observe: every possible ﬁnite
sequence of heads and tails. In particular, if the bias B of the coin is x, then
the chance of your fourteen ﬂips resulting in any sequence with k heads
and 14 2 k tails is x k·ð1 2 xÞ142k. For example, let S be the proposition
H 1 & . . . & H k & T kþ1 & . . . & T 14 (heads on the ﬁrst k tosses, tails on
the next 14 2 k tosses), and let chx be the function that would specify the
chances if B ¼ x were true. Then supposing the bias B of the coin is x,
the chance of S is as follows:
chx ðSÞ
¼ chx ðH 1 & . . . & T 14Þ
¼ chx ðH 1 j H 2 & . . . & T 14Þ·
chx ðH 2 j H 3 & . . . & T 14Þ . . . chx ðT 14Þ ðAxioms of ProbabilityÞ
¼ chx ðH 1Þ . . . chx ðT 14Þ
ðIndependenceÞ
¼ x kð1 2 xÞ142k
ðIdentically DistributedÞ
And the chance of your fourteen ﬂips resulting in some sequence or other
with k heads and 14 2 k tails—call this proposition D k—is
chx ðDkÞ ¼
14
k
 
!
x kð1 2 xÞ142k:
The propositions D0; . . . ; D14 are the possible new data items you might
receive (and are mutually exclusive and jointly exhaustive). They capture
all of the information you might learn—how many times the coin came
up heads and tails, respectively—that is relevant to adjudicating between
hypotheses about the bias of the coin. (The order of heads and tails, and
so on, is irrelevant. The number of heads is what statisticians call a “suf-
ﬁcient statistic.”)
Now recall that updating (conditioning) your prior credence
function u on these bits of new evidence yields different posterior cre-
dence functions, uD0; . . . ; uD14, each of which is (or at least could be)
accurate to a different degree. And since chance has views about which
of these data items you will receive, it also has views about how likely you
are to attain these different degrees of posterior accuracy.
J A S O N K O N E K
550
Downloaded from http://read.dukeupress.edu/the-philosophical-review/article-pdf/125/4/509/462964/509Konek.pdf by WIKIPEDIA LIBRARY user on 03 July 2023

These views are summed up by u’s objective expected posterior
accuracy—chance’s best estimate of how successful u will be—which we
can now calculate as follows:63
ExpchxðIu 0Þ
ð1Þ
¼
X
14
k¼0
X
w[Dk
chx ðwÞIðuDk; wÞ
ð2Þ
The mathematical details are not terribly important. What is important is
that we have a well-motivated, concrete way of measuring chance’s best
estimate of how successful u will be if the bias B of the coin is x. So we can
examine how much this best estimate—u’s objective expected posterior
accuracy—varies across hypotheses about the coin’s bias. And, as it turns
out, it varies fairly minimally, as ﬁgure 5 shows. This tells us something
important about what explains the accuracy of your posterior credences,
and in turn, the amount of cognitive ability that your credal state manifests.
The story—of what explains your success and why—is familiar by
now. Suppose chance’s best estimate of how successful your credences
will be—your objective expected posterior accuracy—would be the
same, regardless of how accurate a hunch those credences happen to
encode. That is, suppose your prior credence function u satisﬁes
INVARIANCE. ExpchxðIu 0Þ ¼ ExpchyðIu 0Þ for all 0 # x; y # 1:
(The uniform prior credence function u does not perfectly satisfy INVARI-
ANCE, in the case at hand. Other prior credence functions, as we will see in
section 6, vary less across hypotheses about the coin’s bias. But u does
63. Line 4 assumes that the coin’s bias is not itself a chancy matter, in the following
sense: chx attaches zero probability to any world w such that B ðwÞ – x.
Probabilistic Knowledge and Cognitive Ability
551
Downloaded from http://read.dukeupress.edu/the-philosophical-review/article-pdf/125/4/509/462964/509Konek.pdf by WIKIPEDIA LIBRARY user on 03 July 2023

come pretty close: ExpchxðI u 0Þ is roughly 0.065 for all 0 # x # 1.) Then
the accuracy of that hunch is (more or less) irrelevant for explaining why
your chance of being accurate to any particular degree x, y, or z is what it is.
And chances are explanatory foci. So having a spot-on (or wildly off)
hunch also plays no role in explaining why you actually attain the degree
of posterior accuracy that you do (namely, Iðu 0; wÞ < 0:028). The accu-
racy of your posterior credences is explained, rather, by the character of
your evidence: the fact that it is fairly weighty (fourteen tosses, rather than
only two or three), the fact that it is maximally nonmisleading (the fre-
quency of heads in your data, namely, 5/7, matches the coin’s true bias),
and so on. And this is just what is required for the accuracy of your pos-
terior credences to be the product of cognitive ability.
Your situation mirrors the Mars rover’s in many ways. Suppose that
chance’s best estimate of how successful the rover will be—its objective
expected distance from the landing site—would be the same, regardless
of whether it happens to emerge from its initial (blind) descent directly
above the site, or one-half mile to the north, or three-quarters of a mile to
the northeast, and so forth. (Even the fanciest Mars rover does not perfectly
satisfy this supposition. But in virtue of the actual Mars rover’s skill at
landing, it comes pretty close.) Then its initial proximity to the landing
site (a nonevidential factor) is (more or less) irrelevant for explaining
why its chance of landing within one, or two, or three miles of that site,
and so forth, is what it is. And chances are explanatory foci. So initial
proximity is also (more or less) irrelevant for explaining why it attains
Figure 5.
The objective expected posterior accuracy of u across hypotheses B ¼ x
J A S O N K O N E K
552
Downloaded from http://read.dukeupress.edu/the-philosophical-review/article-pdf/125/4/509/462964/509Konek.pdf by WIKIPEDIA LIBRARY user on 03 July 2023

the actual degree of success that it does—why it actually lands close to
its target (or not).
Your friend, however—the one who adopts the more biased beta
prior b—is in a different boat. Her prior’s objective expected posterior
accuracy varies rather signiﬁcantly across hypotheses about the coin’s
bias (see ﬁg. 6). So the fact that she happened to have such a high
chance of success (such a low chance of posterior inaccuracy) is
explained, in no small part, by the fact that the true bias of the coin,
namely, 5/7, happened to be in the center of the neighborhood where
b lumps the majority of its probability density. It is explained, in no small
part, by the fact that her hunch—that the coin’s bias is approximately
5/7—happens to be correct. Having a spot-on hunch, then, also plays
a signiﬁcant role in explaining why she actually attains the degree of
posterior accuracy that she does (namely, Iðb 0; wÞ < 0:020).
Her situation is like a less skilled rover’s. A less skilled rover might
emerge from its initial descent directly above its landing site. And its
landing strategy might give it a high chance of success in that case. So it
might land close to its target. Still, unlike the skilled Mars rover, its objec-
tive expected degree of success varies signiﬁcantly across changes in
initial proximity. How close it happens to be to the landing site after
emerging from its initial (blind) descent makes a big difference to its
chance of success. The upshot: nonevidential factors (facts about initial
proximity) are relevant for explaining why its chance of success is what
Figure 6.
The objective expected posterior accuracy of b across hypotheses B ¼ x
Probabilistic Knowledge and Cognitive Ability
553
Downloaded from http://read.dukeupress.edu/the-philosophical-review/article-pdf/125/4/509/462964/509Konek.pdf by WIKIPEDIA LIBRARY user on 03 July 2023

it is. In turn, they are relevant for explaining why it is successful to the
degree that it is (why it lands close to its target or not).
This all serves to highlight an important virtue of MaxEnt. It deliv-
ers posterior credences whose accuracy is, to a large extent, the product of
cognitive ability. In this respect, it outperforms other brands of objective
Bayesianism that recommend more concentrated priors. Unlike those
other brands of objective Bayesianism, the accuracy of MaxEnt’s posterior
credences is explained primarily by facts about the evidence that they
were conditionalized on—how weighty it is, how speciﬁc it is, how mis-
leading it is, and so forth—rather than, for example, the fact that its
recommended prior credences encode a spot-on (or wildly off) hunch
(a nonevidential factor). And this is just what is required for posterior
accuracy to be the product of cognitive ability.
So MaxEnt does seem to recommend credal states that are at
least reasonably eligible candidates for constituting probabilistic knowl-
edge (at least in simple inference problems); the sorts of states in which
we might be plausibly said to know, for example,
(19)
that the coin might come up heads on the next toss,
(20)
that it’s more likely than not that the coin’s bias is between 0.6 and
0.8, or
(21)
that the coin’s bias is probably greater than 0.5.
But MaxEnt does not deliver credences that are maximally eligible candi-
dates for constituting probabilistic knowledge, I will argue. An alternative
objectivist method, the MaxSen method, delivers better candidates.
6. The MaxSen Method
Once more, suppose that our agent has opinions about propositions in
an atomic Boolean algebra F. She designs an experiment to adjudicate
between competing theoretical hypotheses, H 1; . . . ; H n (pairwise incom-
patible and jointly exhaustive). And her experiment might yield any of
the possible data items, D1; . . . ; Dm (also pairwise incompatible and joint-
ly exhaustive). Suppose that F contains H 1; . . . ; H n and D1; . . . ; Dm.
Finally, let chi be the function that would specify the chances if Hi were
true. So the chance that the experiment would yield outcomes D1; . . . ; Dm
if H i were true is given by chi ðD1Þ; . . . ; chi ðDmÞ; respectively.
According to the maximum sensitivity principle, or MaxSen, our agent
should take her prior information into account as follows:
J A S O N K O N E K
554
Downloaded from http://read.dukeupress.edu/the-philosophical-review/article-pdf/125/4/509/462964/509Konek.pdf by WIKIPEDIA LIBRARY user on 03 July 2023

.
First, she should summarize that prior information by a system
of constraints on prior credence functions, b : F ! ½0; 1, for
example,
–
b ðH iÞ $ b ðH jÞ (that is, Hi is at least as probable as Hj),
–
0:6 # b ðH iÞ # 0:9 (that is, Hi is probable to at least degree
0.6 and at most 0.9), and
–
b ðH i j DÞ $ b ðH j j D 0Þ (that is, Hi conditional on D is at
least as probable as Hj conditional on D 0),
which we represent by a set C of credence functions, namely, the
set of all minimally rational credence functions, b (that is, b that
satisfy Probabilism, the Principal Principle, and so forth), con-
sistent with those constraints.
.
Second, she should adopt some prior credence function c :
F ! ½0; 1 or other that minimizes
varðcÞ ¼ max iExpchiðIc 0Þ 2 min jExpchj ðIc 0Þ
over C. (Under a broad range of conditions, this “MaxSen prior” will turn
out to be unique.)64
64. Suppose that C is closed, bounded, and convex (see n. 48). And suppose that for
any f, g, and c ¼ lf þ ð1 2 lÞg in C (where 0 , l , 1), I satisﬁes two constraints,
which we might call worst-case expected convexity and best-case expected concavity:
ðWCE-ConvexityÞ max iExpchi ðIc 0Þ , l½ max jExpchj ðIðf 0ÞÞ
þ ð1 2 lÞ½ max kExpchkðIðg 0ÞÞ
ðBCE-ConcavityÞ min iExpchi ðIc 0Þ , l½ min jExpchj ðIðf 0ÞÞ
þ ð1 2 lÞ½ min kExpchkðIðg 0ÞÞ
If I satisﬁes BCE-Concavity, then whenever c strikes a compromise between f and g (that
is, c ¼ lf þ ð1 2 lÞg), the worst-case expected inaccuracy of c is better, on balance,
than the worst-case expected inaccuracies of f and g, respectively. So compromising has
an alethic beneﬁt: it safeguards against catastrophic worst-case failure. Similarly, if I
satisﬁes WCE-Convexity, then whenever c strikes a compromise between f and g, the
best case of the less compromising f and g is, on balance, better than that of the compro-
mise c. So compromising also has an alethic cost: it forgoes the possibility of maximal best-
case success.
When I satisﬁes these two constraints (and is continuous), var is a continuous, strictly
convex, real-valued function. And any such function takes a unique minimum on any
closed, bounded, convex set C. Specifying what to do when there is no unique MaxSen
prior—when var does not take a unique minimum on C, so that MaxSen does not yield a
Probabilistic Knowledge and Cognitive Ability
555
Downloaded from http://read.dukeupress.edu/the-philosophical-review/article-pdf/125/4/509/462964/509Konek.pdf by WIKIPEDIA LIBRARY user on 03 July 2023

MaxSen ﬁrst says that you ought to choose your prior credence
function from among the minimally rational credence functions that
satisfy the constraints imposed by your prior evidence. This bit is common
to all kinds of objective Bayesianism, including MaxEnt. Different Bayes-
ians disagree about the nature of one’s prior evidential constraints and
provide different justiﬁcations for complying with them. They also dis-
agree about which minimally rational credence functions consistent with
those prior evidential constraints are rationally permissible full stop. But
everyone agrees on this much: all minimally rational credence functions
obey some fairly uncontroversial epistemic norms. They obey Probabi-
lism, for example, and some version of Lewis’s Principal Principle.
These norms, in turn, guarantee that the candidate prior cre-
dence functions that you are choosing between—the minimally rational
credence function consistent with the constraints imposed by your evi-
dence—are not too badly off in terms of accuracy. For example, Joyce
(1998, 2009) shows that all and only the probabilistically coherent cre-
dence functions avoid the epistemic sin of accuracy domination. Every
probabilistically incoherent credence function b is accuracy dominated
by a coherent credence function c, in the sense that b is strictly less accu-
rate than c regardless of which world is actual. No coherent credence
function, in contrast, is accuracy dominated in this way.65 Similarly,
Pettigrew (2013b, 2016) shows that all and only the credence functions
that satisfy the Principal Principle avoid the epistemic sin of prior chance
domination. If a credence function b violates the Principal Principle, then
it is prior chance dominated by a credence function c that satisﬁes it in
the following sense: every chance function consistent with your prior
evidence expects b to attain a strictly lower degree of prior accuracy
than c. No credence function that satisﬁes the Principal Principle, in
contrast, is chance dominated in this way.
To recap, then, MaxSen comes in two bits. The ﬁrst directs you to
choose a prior credence function from among the minimally rational
credence functions that satisfy the constraints imposed by your prior
evidence. This guarantees that your prior credal state is not too badly
unique recommendation about which prior credence function to adopt—is beyond the
purview of my project. The aim, recall, is not to provide the deﬁnitive set of formal tools for
choosing prior credences, but rather, to illustrate one kind of tool for securing probabi-
listic knowledge in a range of inference problems.
65. See also Predd et al. 2009; Schervish, Seidenfeld, and Kadane 2009; and Pettigrew
2016.
J A S O N K O N E K
556
Downloaded from http://read.dukeupress.edu/the-philosophical-review/article-pdf/125/4/509/462964/509Konek.pdf by WIKIPEDIA LIBRARY user on 03 July 2023

off in terms of accuracy. At the very least, it avoids the epistemic defects of
accuracy domination and chance domination, respectively. MaxSen then
says that, from among the minimally rational credence functions that
satisfy the constraints imposed by your prior evidence, you ought to select
the prior that minimizes variance in objective expected posterior accu-
racy across the theoretical hypotheses under investigation. This bit is
unique to MaxSen. The motivation for minimizing variance is familiar:
If chance’s best estimate of how successful your credences will be—your
objective expected posterior accuracy—would be the same, or invariant,
regardless of how a certain nonevidential factor behaves—fortuitous
prior accuracy, in particular—then that factor plausibly plays no role in
explaining why your chance of being accurate to degree x, y, or z is what it
is. And chances are explanatory foci. So it also plays no role in explaining
why you actually attain the degree of posterior accuracy that you do.
Selecting prior credences by minimizing variance, then, will yield
posterior credences whose accuracy is explained, as much as possible, by
evidential rather than nonevidential factors; by the fact that your evi-
dence had a particular character (weighty, speciﬁc, nonmisleading, and
so forth), rather than the fact that from among all of the credal states
consistent with your prior evidence, you happened to settle on a particu-
larly accurate (or inaccurate) one. And this is just what is required for
accuracy to be the product of cognitive ability. So selecting prior cre-
dences in this way will yield posterior credences whose accuracy is, to
the greatest extent possible, the product of cognitive ability. The upshot:
MaxSen’s posterior credences will be particularly eligible candidates
for constituting probabilistic knowledge—more so than those delivered
by MaxEnt, or any other brand of objective Bayesianism. And it is no
mystery why. MaxSen simply directs us to choose the prior credences
that give rise to those eligible posteriors, whatever they are. So it beats
its competitors by construction.
MaxSen treats variance in objective expected posterior accuracy
across theoretical hypotheses, somewhat crudely, as the difference in its
maximum and minimum expected inaccuracies. Of course, this is only
one measure of variance. For example, if there were some good epistemic
reason to privilege a particular base measure on the space of theoretical
hypotheses, for the purposes of integration, then we could substitute
the standard statistical measure of variance for our somewhat crude
Probabilistic Knowledge and Cognitive Ability
557
Downloaded from http://read.dukeupress.edu/the-philosophical-review/article-pdf/125/4/509/462964/509Konek.pdf by WIKIPEDIA LIBRARY user on 03 July 2023

measure.66 (Though it is difﬁcult to see how we might privilege a particu-
lar base measure without simply begging the question in favor of some
prior credences or other.) But the crude measure will do, for our pur-
poses. It places an upper bound on statistical variance (whatever base
measure you choose), and plausibly any other measure of variance worth
its salt. So invariance according to our crude measure entails invariance
according to any reasonable measure.
More important, my aim is to illustrate one kind of formal tool
for securing probabilistic knowledge, not to provide the deﬁnitive set of
tools for that end. If successful, what we will end up with is a framework for
theorizing about how to secure probabilistic knowledge. That is what we
are really after. So we need not fuss too much about whether our measure
of variance is just right.
To illustrate the MaxSen method, imagine once more that a book-
ie hands you a coin and offers you a bet. You have no prior evidence about
the coin’s bias. (Or rather, you have almost no prior evidence. To make it
manageable to compute the MaxSen prior, assume that your prior evi-
dence rules out any credence function that does not take the form of a
beta distribution.)67 The generous bookie, however, allows you to ﬂip the
coin n times before deciding whether or not to take the bet.
MaxEnt recommends that you take your prior information (or
lack thereof) into account, in your inference problem, by adopting
uniform prior credences over hypotheses about the bias of the coin. It
recommends, always and everywhere, adopting credences that are as
close to uniform as possible, while satisfying the constraints imposed by
your prior evidence. And in this case, the constraints imposed by your
evidence are so loose that you can adopt perfectly uniform credences.
MaxSen, in contrast, recommends adopting nonuniform prior
credences. Which prior it recommends depends on additional details
regarding the setup of your experiment: in particular, how many times
you are going to ﬂip the coin. (See the discussion of the Likelihood
66. Suppose there is good epistemic reason to privilege a particular base measure m
on the space H of theoretical hypotheses, for the purposes of integration. Then substi-
tuting the standard statistical measure of variance in for our measure (the difference
between maximum and minimum expected inaccuracy) yields a version of MaxSen that
recommends minimizing VarðcÞ ¼
Ð
H f ðc; xÞm ðdxÞ, where f ðc; xÞ is the squared difference
between the mean objective expected posterior inaccuracy of c (relative to m), on the one
hand, and the expected inaccuracy determined by chx (the true chance function if H x
were true), on the other: f ðc; xÞ ¼ Ð
H ExpchyðIc 0Þm ðdyÞ 2 ExpchkðIc 0Þ
h
i2
.
67. See n. 53 for more information about beta distributions.
J A S O N K O N E K
558
Downloaded from http://read.dukeupress.edu/the-philosophical-review/article-pdf/125/4/509/462964/509Konek.pdf by WIKIPEDIA LIBRARY user on 03 July 2023

Principle in section 9.1 for more on MaxSen’s sensitivity to experimental
setup.) For example, if you are going to ﬂip the coin eight times (n ¼ 8),
then the MaxSen prior is the beta distribution with a ¼ b ¼ 1:45 (see
ﬁg. 7). If instead you are going to ﬂip the coin twenty times (n ¼ 20), then
the MaxSen prior is the beta distribution with a ¼ b < 2 (see ﬁg. 8). We
will now explore why.
Suppose that you plan to ﬂip the coin eight times. To construct
the MaxSen prior in this case, we need to ﬁnd the prior whose objective
Figure 7.
The MaxSen (beta) distribution when n ¼ 8 ða ¼ b ¼ 1:45Þ
Figure 8.
MaxSen beta prior with n ¼ 8 ða ¼ b ¼ 1:45Þ and ða ¼ b < 2Þ, respectively
Probabilistic Knowledge and Cognitive Ability
559
Downloaded from http://read.dukeupress.edu/the-philosophical-review/article-pdf/125/4/509/462964/509Konek.pdf by WIKIPEDIA LIBRARY user on 03 July 2023

expected posterior accuracy varies least across hypotheses about the
coin’s bias. We could use any number of optimization algorithms to do
this. But, since our aim is just to illustrate one kind of formal tool for
delivering skillfully produced credences, in the hopes of providing a
framework for theorizing about how to secure probabilistic knowledge,
we will use a rough but simple technique.
First, we run a simple regression to approximate the variance,
var ðcÞ ¼ max xExpchxðIc 0Þ 2 maxyExpchyðIc 0Þ;
of every prior credence function c consistent with the constraints
imposed by your prior evidence (see ﬁg. 9). Then we use standard
optimization techniques to ﬁnd the prior for which this approximated
variance is smallest. Remember, to make it manageable to compute the
MaxSen prior, we assumed that c takes the form of a beta distribution. This
means that c is fully determined by two shape parameters, a and b, which
determine how tightly c concentrates its probability density on hypoth-
eses about the coin’s bias, of the form B ¼ x (the larger a þ b is, the
more tightly it focuses its density), and where exactly c centers that den-
sity. So we can survey the full space of credence functions consistent with
your prior evidence by surveying all the values that a and b can take.
Inspection of ﬁgure 9 reveals that the prior we are searching for—the
prior whose objective expected posterior accuracy varies least across
hypotheses about the coin’s bias—is, approximately, the beta distri-
bution with a ¼ b ¼ 1:4. Standard optimization techniques show that
the prior that minimizes variance (in objective expected posterior accu-
racy) is, more precisely, the beta distribution with a ¼ b ¼ 1:45 (see
ﬁg. 7). This is the MaxSen prior.
In our toy example (when you plan to ﬂip the coin eight times),
MaxEnt delivers credences (uniform credences over hypotheses about
the coin’s bias) whose posterior accuracy is, to a large extent, the product
of cognitive ability. The objective expected posterior accuracy of those
credences varies fairly minimally across hypotheses about the coin’s bias
(see ﬁg. 5). This is good evidence that their posterior accuracy is ex-
plained primarily by facts about the evidence used to update them—
how weighty it is, how speciﬁc it is, how misleading it is, and so forth—
rather than fortuitous prior accuracy. And this is what is required for
posterior accuracy to be the product of cognitive ability.
All of this notwithstanding, MaxSen delivers posterior credences
whose accuracy is, to the greatest extent possible, the product of cognitive
J A S O N K O N E K
560
Downloaded from http://read.dukeupress.edu/the-philosophical-review/article-pdf/125/4/509/462964/509Konek.pdf by WIKIPEDIA LIBRARY user on 03 July 2023

ability. Their accuracy is explained, to the greatest possible degree, by the
character of the evidence on which they are based. The story of why this is
so, once again, is familiar.
Suppose chance’s best estimate of how successful your MaxSen-
recommended prior credences will be—your objective expected pos-
terior accuracy—would be exactly the same, regardless of how accurate
a hunch those credences happen to encode. Put differently, suppose it
would be the same, regardless of whether the true hypothesis about the
coin’s bias turns out to be B ¼ x, one that your prior credence function s
centers a great deal of probability density around (in which case s has a
high degree of prior accuracy), or whether it turns out to be B ¼ y, one
that s centers little probability density around (in which case s has a lower
degree of prior accuracy). More succinctly, suppose s satisﬁes
INVARIANCE. ExpchxðIs 0Þ ¼ ExpchyðIs 0Þ for all 0 # x; y # 1:
Like the MaxEnt prior u, the MaxSen prior s does not perfectly satisfy this
supposition. But it comes as close as any beta distribution can, simply by
construction: closer than the credences delivered by MaxEnt (see ﬁg. 10),
or any other brand of objective Bayesianism. And this really is pretty
close to invariant (see ﬁg. 11). The difference between s’s maximum
( ¼ 0.077) and minimum ( ¼ 0.065) objective expected inaccuracies is
only 0.012.
Figure 9.
Approximation of var with n ¼ 8
Probabilistic Knowledge and Cognitive Ability
561
Downloaded from http://read.dukeupress.edu/the-philosophical-review/article-pdf/125/4/509/462964/509Konek.pdf by WIKIPEDIA LIBRARY user on 03 July 2023

This tells us something important about what explains the accu-
racy of your posterior credences, and in turn, the amount of cognitive
ability that your credal state manifests. Counterfactual independence
provides good (if defeasible) evidence for explanatory irrelevance. And
the chance of your posterior credences attaining any particular degree of
accuracy, x, y, or z, is counterfactually independent of how accurate a
hunch your prior credences happen to encode. It would be the same,
Figure 11.
The objective expected posterior accuracy of the MaxSen prior s across
hypotheses B ¼ x
Figure 10.
Expected posterior accuracy of both u and s : Expchx ðIu 0Þ and Expchx ðIs 0Þ
(rescaled to emphasize difference in variation across hypotheses B ¼ x)
J A S O N K O N E K
562
Downloaded from http://read.dukeupress.edu/the-philosophical-review/article-pdf/125/4/509/462964/509Konek.pdf by WIKIPEDIA LIBRARY user on 03 July 2023

regardless of how accurate that hunch was. (This is what INVARIANCE tells
us.) So the accuracy of any such hunch is plausibly irrelevant (or nearly
so) for explaining why your chance of success (posterior accuracy) is what
it is. And chances are explanatory foci. So having a spot-on (or wildly off)
hunch (a nonevidential factor) also plays no role in explaining why you
actually attain the degree of posterior accuracy that you do. The accuracy
of your posterior credences is explained, rather, by the character of your
evidence: how weighty it is, how speciﬁc it is, how misleading it is, and so
forth. And this is just what is required for posterior accuracy to be the
product of cognitive ability.
Once more, the situation is not unlike the skilled Mars rover’s.
Whether it happens to emerge from its initial (blind) descent directly
above the landing site, or one-half mile to the north, or three-quarters of
a mile to the northeast, it has roughly the same chance of success (landing
close to the target). The result: initial proximity (a nonevidential factor)
plays virtually no role in explaining its success. Rather, facts about the
quality of its evidence (evidence about its trajectory prior to entering
Mars’s atmosphere, sensor readings after initial descent, and so forth)
and how it responds to that evidence are primarily responsible for
explaining its success.
You might worry that invariant chances of success come cheap. A
defective rover—no parachute, no sensors, no rocket thrusters, and so
forth—has the same chance of success, regardless of its initial proximity
to the landing site, namely, no chance! It is bound to be unsuccessful.
Similarly, a defective prior—for example, the incoherent prior c0 that
assigns credence 0 to all competing theoretical hypotheses H 1; . . . ; H n
and all possible experimental outcomes, or data items D1; . . . ; Dm —is
bound to be unsuccessful (inaccurate). In fact, it is certain to attain the
same (low) degree of posterior accuracy, on a wide range of plausible
inaccuracy measures, come what may.68 So its chance of success is invari-
ant too.
Does MaxSen recommend incoherent priors, then? No! It only
appears to do so if we ignore one of MaxSen’s two pieces of advice. First,
68. More carefully, c0 : F ! ½0; 1 will attain exactly the same degree of posterior
(and prior) accuracy, come what may, relative to any extensional inaccuracy measure I.
I is extensional iff Iðc; wÞ is a function exclusively of c (X) (c’s estimate of X ’s truth value),
and w ðXÞ (X’s truth-value at w), for all X [ F. Since c0 assigns exactly the same credence
(namely, 0) to every H i; Dj [ F, and exactly one of element of {H 1; . . . ; H n} and
{D1; . . . ; Dm} is true, come what may (both are partitions), c0’s inaccuracy is the same,
come what may, according to any extensional I.
Probabilistic Knowledge and Cognitive Ability
563
Downloaded from http://read.dukeupress.edu/the-philosophical-review/article-pdf/125/4/509/462964/509Konek.pdf by WIKIPEDIA LIBRARY user on 03 July 2023

MaxSen says that you ought to choose minimally rational prior credences
that satisfy the constraints imposed by your prior evidence. Then it says
that your credences should minimize variance (in objective expected
posterior accuracy across theoretical hypotheses) from among those
minimally rational, constraint-satisfying credences. So MaxSen always
recommends coherent prior credences, so long as minimally rational
credence functions are always probabilistically coherent. And all Bayes-
ians agree: they are. Accuracy-dominated credence functions are not
even minimally rational. And credence functions avoid the sin of accu-
racy domination just in case they are probabilistically coherent.
Still you might worry. Even if a rover is not fully defective—perhaps
it has a partially functioning parachute, bargain-bin sensors, tempera-
mental rocket thrusters, and so forth—it might yet have a very low chance
of success regardless of its initial proximity to the landing site. As a result,
its chance of success might be close to invariant. But such invariance is
cheap. There might well be another rover whose chance of success—
despite varying more signiﬁcantly across changes in initial proximity—is
always higher than that of the partially defective rover.
Similarly, even if a prior is minimally rational, you might worry—
even if it obeys Probabilism, the Principal Principle, and so forth—that it
could nevertheless have a uniformly low objective expected posterior
accuracy (low across all theoretical hypotheses). As a result, its objective
expected posterior accuracy might be close to invariant. But this sort of
invariance seems cheap. For all we have said, there might be another
prior whose objective expected posterior accuracy—despite varying
more signiﬁcantly across theoretical hypotheses—is always higher. That
is, there might be another prior that posterior chance dominates it.
Does MaxSen recommend such defective priors? No! As it turns
out, no prior credence function that satisﬁes the Principal Principle
is posterior chance dominated. We mentioned earlier that Pettigrew
(2013b, 2016) establishes the following result: if a credence function
satisﬁes the Principal Principle, then it is not prior chance dominated.
That is, no other credence function has uniformly higher objective
expected prior accuracy (higher across all theoretical hypotheses). In
fact, though, something stronger is true: if a credence function satisﬁes
the Principal Principle, then not only is it not prior chance dominated,
but it is not posterior chance dominated either. There is no other cre-
dence function that has uniformly higher objective expected posterior
J A S O N K O N E K
564
Downloaded from http://read.dukeupress.edu/the-philosophical-review/article-pdf/125/4/509/462964/509Konek.pdf by WIKIPEDIA LIBRARY user on 03 July 2023

accuracy.69 So MaxSen never recommends such defective priors, so
long as minimally rational credence functions always obey the Principal
Principle.
The take-home lesson is this: being accurate and being skillfully
produced are both epistemically laudable properties. Both are plausibly
necessary for probabilistic knowledge. And MaxSen is designed to secure
both. It directs you to choose minimally rational prior credences that
satisfy the constraints imposed by your prior evidence in order to ensure
that they are reasonably accurate. At the very least, such credences avoid
the epistemic sins of accuracy domination and chance domination (prior
and posterior). MaxSen then directs you to choose prior credences that
minimize variance in objective expected posterior accuracy (from among
the minimally rational credal states that satisfy those constraints) in order
to ensure that they are the product of cognitive ability.
To sum up, if what we have said is correct, then MaxSen delivers
credences that are not only reasonably likely to be accurate, in a wide
range of inference problems, but whose accuracy is, to the greatest extent
possible, explained by facts about the evidence (prior and experimental)
on which they are based. In turn, the accuracy of those credences is, as
much as possible, the product of cognitive ability.
So MaxSen takes us some way toward securing probabilistic knowl-
edge. It delivers credences that are maximally eligible candidates for
constituting such knowledge. It does as much as possible—more than
MaxEnt, or any other brand of objective Bayesianism—to help secure a
credal state in which you might plausibly be said to know, for example,
69. To see this, note that if a credence function c satisﬁes the version of the Principal
Principle I introduced earlier (n. 41)—the extended temporal principle—then
c ðX j T chÞ ¼ ch ðXÞ whenever c ðT chÞ . 0. This means that
Expc ðIb 0Þ ¼
X
1#i#m
X
w[Dj
c ðwÞIðbDj ; wÞ ¼
X
ch
c ðT chÞ
X
1#i#m
X
w[Dj
ch ðwÞIðbDj ; wÞ
2
4
3
5:
Greaves and Wallace (2006) show that Expc ðIb 0Þ takes a minimum at b ¼ c, so long as I is
a strictly proper inaccuracy score. But now suppose (for reductio) that some credence
function b – c has uniformly lower objective expected posterior inaccuracy, so that
Expch ðIb 0Þ ¼
X
1#i#m
X
w[Dj
ch ðwÞIðbDj ; wÞ ,
X
1#i#m
X
w[Dj
ch ðwÞIðcDj ; wÞ ¼ Expch ðIc 0Þ
for every chance function ch. Then whatever values the c ðT chÞ’s take, we have
Expc ðIb 0Þ , Expc ðIc 0Þ, which contradicts the claim that Expc ðIb 0Þ takes a minimum at
b ¼ c.
Probabilistic Knowledge and Cognitive Ability
565
Downloaded from http://read.dukeupress.edu/the-philosophical-review/article-pdf/125/4/509/462964/509Konek.pdf by WIKIPEDIA LIBRARY user on 03 July 2023

(22)
that the next trial of your experiment will almost certainly not pro-
duce data D,
(23)
that it’s more likely than not that the true theoretical hypothesis is
either Hi or Hj, or
(24)
that the true hypothesis is probably one of H 1; . . . ; H k.
Before concluding, it is worth examining these “maximally eligible can-
didates” a bit more closely. They obey a particular principle of equality, as we
will see in section 7. More speciﬁcally, MaxSen delivers credences that
give theoretical hypotheses equal consideration, in a certain sense, rather
than equal treatment. This tells us something important about the nature
of cognitive ability. The demands of cognitive ability manifest themselves
as demands of equal consideration.
7. Two Principles of Equality
Peter Singer (1975) argued that the most basic principle of equality is
a principle of equal consideration. The interests of all—human or other-
wise—deserve equal consideration in moral deliberation. Importantly,
though, “equal consideration for different beings may lead to different
treatment” (Singer 1975, 2). For example:
If I were to conﬁne a herd of cows within the boundaries of the county of,
say, Devon, I do not think I would be doing them any harm at all; if, on the
other hand, I were to take a group of people and restrict them to the same
county, I am sure many would protest that I had harmed them consider-
ably, even if they were allowed to bring their families and friends, and
notwithstanding the many undoubted attractions of that particular coun-
ty. (Singer 1985, 6)
Humans have interests “in mountain-climbing and skiing, in seeing the
world and in sampling foreign cultures,” whereas cows do not (Singer
1985, 6). So we are not required to treat humans and cows equally, simply
in virtue of giving equal consideration to their respective interests.
Indeed, we will often be required not to treat them equally.
We can and must, then, distinguish between two importantly
different principles of equality, according to Singer—principles of
equal treatment and consideration, respectively—which govern our practi-
cal and evaluative attitudes toward individuals. Similarly, we can (and, I
think, must) distinguish between importantly different principles of
equality governing our doxastic attitudes toward theoretical hypotheses. On
the one hand, we have a principle of equal doxastic treatment, which
J A S O N K O N E K
566
Downloaded from http://read.dukeupress.edu/the-philosophical-review/article-pdf/125/4/509/462964/509Konek.pdf by WIKIPEDIA LIBRARY user on 03 July 2023

demands that we treat theoretical hypotheses equally by giving them
equal credence. On the other hand, we have a principle of equal doxastic
consideration, which demands that we reason about theoretical hypoth-
eses in such a way as to give each one an equal chance of being discovered,
that is, of you learning of its truth (or having accurate posterior credences
about its truth), if indeed it is true.
If we had explored MaxEnt, and then called it a day before discov-
ering MaxSen, we might have suggested the following: While morality
requires us to give individuals equal consideration, the pursuit of prob-
abilistic knowledge requires us to give theoretical hypotheses equal treat-
ment. After all, MaxEnt recommends that you have credences that are
as close to uniform, or equal as possible, while satisfying the constraints
imposed by your prior evidence. And it seems to deliver credences that
are, to a large extent, the product of cognitive ability, and hence good
candidates for probabilistic knowledge. So we seem to have learned some-
thing surprising about the nature of cognitive ability, and in turn, about
the nature of probabilistic knowledge: both involve treating hypotheses
equally, by giving them equal credence (or as close to equal credence as
possible, while satisfying the constraints imposed by your prior evidence).
But this would have been a mistake. Just as morality requires us to
give individuals equal consideration, so too does the pursuit of probabilistic
knowledge require us to give theoretical hypotheses equal consideration.
Treating cows and humans equally is preferable, from the moral perspec-
tive, to the status quo (factory farms and the like). But it does not follow
that it is the morally best option. It does not follow that morality requires
equal treatment. Similarly, treating all theoretical hypotheses equally, by
adopting uniform or equal prior credences, is preferable, from the episte-
mic perspective, to adopting various more concentrated priors. It is a
better means to the end of securing skillfully produced credences, and
in turn, credences that are good candidates for probabilistic knowledge.
But it does not follow that it is the best option. It does not follow that
having such ability, or knowledge, requires equal treatment.
MaxSen, not MaxEnt, delivers the most skillfully produced cre-
dences possible, and in turn, maximally eligible candidates for probabi-
listic knowledge. But MaxSen does not recommend giving theoretical
hypotheses equal treatment by giving each one equal credence. Instead,
MaxSen recommends giving them equal consideration by giving each one
the same chance of being discovered, that is, of you learning of its
truth (or having accurate posterior credences about its truth), if indeed
Probabilistic Knowledge and Cognitive Ability
567
Downloaded from http://read.dukeupress.edu/the-philosophical-review/article-pdf/125/4/509/462964/509Konek.pdf by WIKIPEDIA LIBRARY user on 03 July 2023

it is true. This is just what minimizing variance in objective expected
posterior accuracy amounts to.
The primary difference between these two doxastic equality
principles—and, in turn, between MaxEnt and MaxSen—is that the
principle of equal consideration recognizes that certain theoretical
hypotheses speak up more clearly and forcefully than others do, to put
it metaphorically. Certain hypotheses have a high chance of producing
rather extreme, probative data: data that forces most priors (save for
fantastically pigheaded ones) to center most of their probability density
around them.70 They scream, if you will, “I’m true!” This sort of bois-
terous hypothesis does not need equal treatment (credence) to receive
equal consideration. It does not need equal treatment to have the same
chance of being discovered as other hypotheses, that is, of you learning of
its truth, if it is true.
For example, if you are going to ﬂip the coin of unknown bias
eight times, MaxSen recommends giving less credence to hypotheses
according to which the bias of the coin is closer to 0 or 1, and more cre-
dence to hypotheses according to which it is closer to 1/2, as ﬁgure 12
reminds us. The reason: hypotheses according to which the bias is
close to 0 or 1 speak up more clearly than hypotheses according to
which it is close to 1/2. They have a high chance of producing extreme,
probative data (almost all tails, or all heads, respectively): data that forces
most priors (the MaxSen prior included) to center most of their prob-
ability density around them. So they do not need as much credence
(equal treatment) to receive equal consideration.
This is a bit like a teacher in a classroom ﬁlled with children, some
of whom are boisterous and some of whom are reserved. To give equal
consideration to the opinions of all children, the teacher need not pay
equal attention to them all. Indeed, she should strive to listen more care-
fully to the reserved, quiet children: eliciting their thoughts, boosting
their conﬁdence, and so on. The boisterous children will make them-
selves known without such special attention.
If this is right, then we have learned something surprising
about the nature of cognitive ability, or skill, and in turn, about the nature
70. Strictly speaking, of course, hypotheses do not produce data. We ought to say: if
certain “boisterous” hypotheses were true (for example, if the bias of the coin were 0.99),
then there would be a high chance that conducting the experiment (for example, ﬂip-
ping the coin n times) would produce rather extreme, probative data (for example, a
string of nearly n heads).
J A S O N K O N E K
568
Downloaded from http://read.dukeupress.edu/the-philosophical-review/article-pdf/125/4/509/462964/509Konek.pdf by WIKIPEDIA LIBRARY user on 03 July 2023

of probabilistic knowledge. Having cognitive ability does not involve
treating hypotheses equally. Rather, it involves giving them equal consid-
eration. It involves reasoning about them in such a way that you have
an equal chance of discovering them (of learning that they are true, if
they are).
8. Conclusion
Moss (2013) provides compelling reasons to countenance probabilistic
knowledge. Credences can amount to knowledge in much the way that
full beliefs can. I suggested that whatever else it takes for an agent’s cre-
dences to amount to knowledge, they must satisfy both an antiluck con-
dition and an ability condition: their accuracy must not be lucky (it must
be safe); and it must be the product of cognitive ability or skill.
So I set out to ﬁnd formal tools for choosing prior credences, at
least in a range of inference problems, to help ensure those conditions
are met. The best tools for this end, I argued, would be designed speciﬁ-
cally to yield credences that manifest cognitive skill. Normally, cognitive
skill mitigates dependence on luck. When it does not, savvy prior con-
struction won’t help us much anyway.
I then provided an account of cognitive skill, or ability, that says:
having cognitive ability is a matter of reasoning in such a way that your
evidence explains the accuracy (or inaccuracy) of your credences to the
greatest degree possible. To help sift credal states that satisfy the ability
condition on probabilistic knowledge from ones that do not, I searched
Figure 12.
MaxSen prior (n ¼ 8)
Probabilistic Knowledge and Cognitive Ability
569
Downloaded from http://read.dukeupress.edu/the-philosophical-review/article-pdf/125/4/509/462964/509Konek.pdf by WIKIPEDIA LIBRARY user on 03 July 2023

for a summary statistic that tracks the amount of cognitive ability that
a credal state manifests. I pointed to one statistic that seems to ﬁt the
bill: variance in objective expected posterior accuracy across theoretical
hypotheses. The smaller this statistic, I argued, the greater the extent to
which a credal state’s posterior accuracy is a product of cognitive ability.
I then used this statistic to develop a novel kind of objective Bayes-
ianism, MaxSen. MaxSen advises adopting the prior credal state (from
among the credal states consistent with your prior evidence) that
minimizes this statistic. So the posterior accuracy of its credences are,
to the greatest extent possible, the product of cognitive ability. Those
credences, then, are particularly eligible candidates for constituting
probabilistic knowledge.
Finally, I argued that MaxSen teaches us something important
about the nature of cognitive ability and probabilistic knowledge. Having
credences that amount to probabilistic knowledge, and so inter alia are
the product of cognitive ability, requires giving theoretical hypotheses
equal consideration rather than equal treatment.
In appendixes A and B, I tie up some important loose ends. Ap-
pendix A addresses two pressing objections. For example, Venn (1866),
Keynes (1921), and Fisher (1922) provide examples that seem to show
that MaxEnt yields inconsistent results in a range of cases, depending
on how you describe them. The proponents of MaxSen must show that
these problems do not extend to their proposal. Appendix B discusses
MaxSen’s limitations.
9. Appendix A: Objections
9.1. Likelihood Principle
The MaxSen method seems to run afoul of the Likelihood Principle (LP)
(Edwards, Lindman, and Savage 1963, 237):
LIKELIHOOD PRINCIPLE. For any two experiments aimed at adjudicating
between theoretical hypotheses H 1; . . . ; H n, and any two items of
data D and D 0 produced by those experiments, if D and D 0 have the
same inverse probabilities (or ‘likelihoods’), up to an arbitrary posi-
tive constant k.0—which is just to say that all hypotheses Hi and Hj
(with 1 # i; j # n) render the ﬁrst datum, D, k times as probable
as the second datum, D 0 : p ðD j H iÞ=p ðD 0 j H iÞ ¼ k ¼ p ðD j H jÞ=
pðD 0 j H jÞ—then the evidential import of D and D 0 for H 1; . . . ; H n
is the same.
J A S O N K O N E K
570
Downloaded from http://read.dukeupress.edu/the-philosophical-review/article-pdf/125/4/509/462964/509Konek.pdf by WIKIPEDIA LIBRARY user on 03 July 2023

Many Bayesian statisticians, such as L. J. Savage, Bruno de Finetti, and
James Berger (as well as frequentist statisticians such as R. A. Fisher) take
the LP to be central to rational inductive inference. Birnbaum (1962)
summarizes the standard Bayesian rationale for the LP as follows: First, on
the Bayesian view, according to Birnbaum, the aim of rational inductive
inference is to use “experimental results along with other available
[prior] information” to determine a posterior that provides “an appro-
priate ﬁnal synthesis of available information” (Birnbaum 1962, 299).
Second, Bayes’s theorem tells us that posterior (postexperiment)
probabilities, p ð j DÞ, are fully determined by two components: a prior
(preexperiment) probability function, p ðÞ, which speciﬁes how prob-
able the various theoretical hypotheses H 1; . . . ; H n are in light of any
relevant prior information you might have, and an inverse probability
function, or likelihood function, p ðD j Þ, which speciﬁes how probable
the hypotheses H 1; . . . ; H n render D.
BAYES’S THEOREM.
p ðH i j DÞ ¼ ½p ðD j H iÞp ðH iÞ=p ðDÞ
¼ ½p ðD j H iÞp ðH iÞ=
X
jp ðD j H jÞp ðH jÞ
Finally, because the prior distribution captures the evidential import of
the prior data (no more, no less), the likelihood function (inverse prob-
abilities) must capture the evidential import of the experimental data,
on the Bayesian view (no more, no less). “In this sense,” Birnbaum (1962,
299) says, “we may say that [Bayes’s theorem] implies [the likelihood
principle].” “The contribution of experimental results to the determi-
nation of posterior probabilities is always characterized just by the likeli-
hood function and is otherwise independent of the structure of an
experiment” (ibid.).
MaxSen seems to violate the LP by making “extraneous” features
of the experimental setup—in particular, its stopping rule—relevant to
the evidential import of the experimental data. Stopping rules are rules
that specify when to stop gathering new data. For example, a ﬁxed stopping
rule might tell you to ﬂip a coin of unknown bias exactly ﬁve, or ten, or
ﬁfteen times. An optional stopping rule, on the other hand, might tell you to
ﬂip the coin until at least half of the tosses come up heads. Stopping rules
have no bearing on the import of the experimental data, according to the
LP, because they have no inﬂuence on likelihoods. Imagine, for example,
that you perform some experiment and it produces some outcome.
Whether you plan to perform ﬁve, or ten, or an indeﬁnite number of
Probabilistic Knowledge and Cognitive Ability
571
Downloaded from http://read.dukeupress.edu/the-philosophical-review/article-pdf/125/4/509/462964/509Konek.pdf by WIKIPEDIA LIBRARY user on 03 July 2023

trials after this ﬁrst trial, in accordance with various different stopping
rules, has no inﬂuence on likelihoods. It makes no difference to how
probable any of the various theoretical hypotheses under consideration
render the outcome of the ﬁrst trial. So it should have no bearing on the
evidential import of that outcome, according to the LP. All such features
of the experimental setup are extraneous.
The argument that MaxSen violates the LP, by making stopping
rules relevant to evidential import, or force, goes as follows: First, as any
proponent of the method would happily admit, stopping rules are rel-
evant to which prior you ought to adopt, according to MaxSen. Suppose
that you and your friend are going to ﬂip a coin, in order to adjudicate
between competing hypotheses about its bias. You adopt different ﬁxed
stopping rules: you plan to ﬂip the coin eight times, while your friend
plans to ﬂip it twenty times. Then MaxSen recommends that you adopt a
certain beta prior (a ¼ b ¼ 1:45) that gives more credence to hypotheses
according to which the bias is close to 1/2, and less credence to those
according to which it is closer to 0 or 1. It recommends that your friend
adopt a different, more concentrated prior (a ¼ b < 2) (see ﬁg. 8).
The argument continues as follows:
1.
Posterior probabilities reﬂect the evidential import of the
total data (prior and experimental) for the theoretical hy-
potheses under investigation (no more, no less).
2.
MaxSen renders posteriors sensitive to stopping rules (by
rendering priors sensitive to stopping rules).
3.
So, according to MaxSen, the evidential import of the total
available data is sensitive to stopping rules. (From 1 and 2)
4.
Stopping rules are obviously irrelevant to the evidential
import of the prior data. If they are relevant to the import
of the total data at all, it must be because they impact the
import of the experimental data.
5.
Hence, the evidential import of the experimental data is sen-
sitive to stopping rules, according to MaxSen. (From 3 and 4)
6.
The LPsays thatstopping rulesare irrelevantto theevidential
import of the experimental data.
7.
MaxSen violates the LP. (From 5 and 6)
In fact, though, MaxSen is perfectly consistent with the LP. The problem
with this argument is that premise 1 is false. A rational agent’s prior
credences reﬂect more than just the evidential import of his or her
prior data, and his or her posterior credences reﬂect more than just
J A S O N K O N E K
572
Downloaded from http://read.dukeupress.edu/the-philosophical-review/article-pdf/125/4/509/462964/509Konek.pdf by WIKIPEDIA LIBRARY user on 03 July 2023

the evidential import of his or her total data. A rational agent engages in
inquiry to secure probabilistic knowledge, according to the proponent of
MaxSen. His or her prior credences, then, must also reﬂect the right sort
of inductive strategy—the right sort of strategy for handling new data—so
that his or her posterior credences are eligible candidates for constituting
such knowledge. This means, inter alia, encoding an inductive strategy
that grounds cognitive skill or ability.
MaxSen does not render priors sensitive to stopping rules because
the evidential import of the experimental data is sensitive to stopping
rules. It is not. Rather, it renders priors sensitive to stopping rules because
facts about which inductive strategy is best suited to ground cognitive skill
or ability are sensitive to stopping rules.
Compare: you are part of the engineering team designing the new
Mars rover. You want to know: Which landing strategy makes the rover’s
chance of success (landing close to the target) more or less invariant,
regardless of whether it emerges from its initial (blind) descent directly
above the landing site, or one-half mile to the north, or three-quarters of a
mile to the northeast, and so forth? Which strategy, as a result, makes the
rover’s initial proximity to the landing site (within reasonable bounds)
more or less irrelevant for explaining its success? Clearly, the answer
depends on a host of variables, for example, how much fuel will be avail-
able for maneuvering once the rover is ﬁnally descending at a safe speed.
If quite a bit will be available, for example, one thousand pounds, then
perhaps a long burn in the direction of the landing site, followed by a
series of short burns would be best. If much less fuel will be available, for
example, six hundred pounds, a long burn might be out of the question.
Similarly, if you and your friend are going to ﬂip a coin, in order to
adjudicate between competing hypotheses about its bias, the right ques-
tion to ask (if you hope to secure probabilistic knowledge) is this: Which
inductive strategy (prior credences) makes your chance of success (pos-
terior accuracy) more or less invariant, regardless of how close the true
bias happens to fall to your prior estimate? Which strategy, as a result,
makes fortuitous prior success (accuracy) more or less irrelevant for
explaining posterior success (accuracy)? The answer depends on various
features of the experimental setup—in particular, the stopping rule in
play. The reason: the stopping rule determines how much data will be avail-
able to “maneuver” your credences toward the truth. And just as the best landing
strategies given six hundred and one thousand pounds of total available
fuel, respectively, might recommend using the ﬁrst ﬁve hundred pounds
differently, so too might the best inductive strategies in different exper-
Probabilistic Knowledge and Cognitive Ability
573
Downloaded from http://read.dukeupress.edu/the-philosophical-review/article-pdf/125/4/509/462964/509Konek.pdf by WIKIPEDIA LIBRARY user on 03 July 2023

imental contexts recommend using some initial bit of data differently, if
there are different total amounts of experimental data available in those
contexts. If you are going to ﬂip the coin eight times, and your friend is
going to ﬂip the coin twenty times—so that your friend has more (weight-
ier) data available to “maneuver” her credences toward the truth than you
do—your best inductive strategies, respectively, might recommend using
some initial bit of data (for example, the outcomes of the ﬁrst ﬁve ﬂips)
differently (that is, might recommend drawing different inferences on
the basis of this initial bit of data).
The moral: MaxSen is consistent with the LP, despite its sensitivity
to stopping rules. Stopping rules do determine certain features of the
MaxSen prior. But this is not because the evidential import of the exper-
imental data is sensitive to stopping rules, according to MaxSen. It is not.
The reason, instead, is that which inductive strategy is best suited to
ground cognitive skill, or ability, in a given context of inquiry is partially
determined by which stopping rule is in play in that context.
9.2. Language Dependence
As frequentist statisticians and dyed-in-the-wool subjective Bayesians
often note, MaxEnt seems to yield inconsistent results in a range of
cases, depending on how you describe them. The following example,
adapted from Fisher (1922, 324–25), illustrates the point. Imagine
once more that you are handed a coin. You plan to ﬂip it n times, in
order to adjudicate between competing hypotheses about its bias. You
have no relevant prior evidence. (Almost no prior evidence, anyway. Once
again, to make it manageable to compute the MaxSen prior, assume that
your prior evidence rules out any credence function that does not take
the form of a beta distribution.)
Given your dearth of evidence, MaxEnt recommends adopting
uniform prior credences u over hypotheses about the bias B of the
coin. It recommends spreading your credence evenly over all of those
hypotheses. But, Fisher (1922, 325) points out, you “might never have
happened to direct [your] attention to the particular quantity” B. You
might have focused, instead, on the quantity u ¼
ﬃﬃﬃﬃ
B
p
(or any number of
other quantities). “The quantity, u,” Fisher says, “measures the degree of
probability, just as well as [B ], and is even, for some purposes, the more
suitable variable” (ibid.). To put the point slightly differently, you might
have described your inference problem in different terms. Not as a prob-
lem that requires sorting out which of the various hypotheses about the
J A S O N K O N E K
574
Downloaded from http://read.dukeupress.edu/the-philosophical-review/article-pdf/125/4/509/462964/509Konek.pdf by WIKIPEDIA LIBRARY user on 03 July 2023

coin’s bias B is true, but rather, as one that requires sorting out which
hypothesis about the square root of its bias is true (what the true value of
u ¼
ﬃﬃﬃﬃ
B
p
is), or the cosine of its bias (d ¼ cos B), and so forth.
These redescriptions of your inference problem are equivalent, in
a sense. The hypotheses that they employ (about the bias of the coin,
about the square root of its bias, and so forth) divide up logical space in
exactly the same way (they partition the space W of possible worlds into
exactly the same sets).71 And there is no obvious epistemic reason to
prefer one way of describing your inference problem over any other.
Yet MaxEnt’s recommendation differs, depending on which description
you choose. For example, if you had focused on the square root of the
coin’s bias u ¼
ﬃﬃﬃﬃ
B
p
rather than its bias B—if you had framed your
inference problem as one that requires sorting out the true value of u
rather than B—then MaxEnt would have recommended spreading your
credence evenly over hypotheses about the value of u. It would have rec-
ommended adopting uniform prior credences u* over hypotheses about
the square root of the coin’s bias, which is equivalent to adopting a non-
uniform prior b* over hypotheses about its bias, as ﬁgure 13 shows.72
So MaxEnt recommends different prior credences, depending on
how you describe your inference problem. As a result, it recommends
making different (inconsistent) judgments, in those different cases. For
example, if you adopt the MaxEnt (uniform) prior u over hypotheses
about the coin’s bias B and then observe two heads in a row, your new
best estimate of that bias is 0.75. In contrast, if you adopt the MaxEnt
(uniform) prior u* over hypotheses about the square root of the coin’s bias
u and observe two heads, your best estimate is 0.71.
One might suspect that the MaxSen method is subject to a similar
sort of language, or description, dependence. And it is. But MaxSen also
furnishes a clear rationale for favoring one description of your infer-
ence problem over others. So it is not inconsistent, in the way MaxEnt
seems to be. It does not leave you in the precarious position of yielding
different prescriptions relative to different ways of describing your infer-
ence problem, with no good reason to choose between them.
71. For example, for any hypotheses B ¼ x about the bias of the coin, the set of
worlds that make the hypothesis B ¼ x true is exactly the set of worlds that makes
u ¼
ﬃﬃﬃx
p
true (the square root of the coin’s bias is
ﬃﬃﬃx
p ). So the partition {{w [ W j B ¼ x
is true at w} j x [ ½0; 1} of W is equal to {{w [ W j u ¼ y is true at w} j y [ ½0; 1}.
72. The uniform prior over hypotheses of the form u ¼ x is equivalent to the non-
uniform prior over hypotheses of the form B ¼ x, deﬁned by the probability density
f ðxÞ ¼ 1=ð2 ﬃﬃﬃx
p Þ (see ﬁg. 13).
Probabilistic Knowledge and Cognitive Ability
575
Downloaded from http://read.dukeupress.edu/the-philosophical-review/article-pdf/125/4/509/462964/509Konek.pdf by WIKIPEDIA LIBRARY user on 03 July 2023

Suppose that you plan to ﬂip the coin of unknown bias ﬁve times.
The MaxSen prior over hypotheses about the coin’s bias B is the beta
distribution s with a ¼ b < 1:2 (see ﬁg. 14). If, in contrast, you use Max-
Sen to determine prior credences over hypotheses about the square root of
the coin’s bias u, you will arrive at the distribution s* with a < 0:9 and b <
1:5 (see ﬁg. 15). And s* is not equivalent to s. Adopting the prior credence
function s* over hypotheses about the square root of the coin’s bias u is
equivalent to adopting a rather concentrated, resilient prior c* over
Figure 14.
MaxSen prior s for B
Figure 13.
Nonuniform prior b* over hypotheses about the value of B, which is equiv-
alent to the uniform prior u* over hypotheses about the value of u
J A S O N K O N E K
576
Downloaded from http://read.dukeupress.edu/the-philosophical-review/article-pdf/125/4/509/462964/509Konek.pdf by WIKIPEDIA LIBRARY user on 03 July 2023

hypotheses about its bias B, one that favors hypotheses according to
which the coin’s bias is close to 0 (see ﬁg. 16).73
So depending on how you describe your inference problem,
MaxSen recommends different prior credences. But this does not
amount to inconsistency, because MaxSen furnishes a clear rationale
for framing your inference problem as one about B rather than u
Figure 15.
MaxSen prior s* for u
Figure 16.
The biased prior c* over hypotheses about the value of B, which is equivalent
to the MaxSen prior s* over hypotheses about the value of u
73. The MaxSen prior over hypotheses of the form u ¼ x is equivalent to the non-
symmetric prior over hypotheses of the form B ¼ x, deﬁned by the probability density
g ðxÞ ¼ 0:65581
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1 2
ﬃﬃﬃx
p
p
=x 0:55 (see ﬁg. 16).
Probabilistic Knowledge and Cognitive Ability
577
Downloaded from http://read.dukeupress.edu/the-philosophical-review/article-pdf/125/4/509/462964/509Konek.pdf by WIKIPEDIA LIBRARY user on 03 July 2023

(or any other quantity). It furnishes a clear rationale for describing it as a
problem that requires sorting out the coin’s true bias rather than the
square root of its bias, or the cosine of its bias, and so forth.
Rational agents engage in inquiry, the thought goes, with the aim
of securing probabilistic knowledge not simply about theoretical hypotheses
(hypotheses about a virus’s infection mechanism, about the causal under-
pinnings of the climate system, and so forth) but also about nontheoretical
propositions (whether a patient’s cancer will remain in remission, whether
atmospheric CO2 levels will double by 2050, and so forth).74 In our
example, you aim to secure not only probabilistic knowledge about the
bias of the coin, or equivalently, the chance of heads, but also about wheth-
er the coin will in fact come up heads on the next toss.
Imagine that you adopt the MaxSen prior s over hypotheses about
the bias of the coin, B. Your friend adopts the MaxSen prior s* over
hypotheses about the square root of the coin’s bias, u. You observe
some data D and update. Then, if MaxSen lives up to its billing, the
accuracy of your posterior credences about B are, to the greatest extent
possible, the product of cognitive ability. Those credences, then, are good
candidates for probabilistic knowledge.
Not so, however, for your credences about u. Adopting the MaxSen
prior s over hypotheses about B is equivalent to adopting a rather con-
centrated, resilient prior c over hypotheses about u, one that favors
hypotheses according to which u is close to 1 (see ﬁg. 17).75 And the
objective expected posterior accuracy of c varies fairly signiﬁcantly across
hypotheses about the square root of the coin’s bias. So the accuracy (or
inaccuracy) of your actual posterior credences about u is explained to a
much greater extent by their fortuitous prior accuracy (or inaccuracy),
and hence is, to a much lesser extent, the product of cognitive ability.
Similar remarks apply to your friend’s credences about B and u (mutatis
mutandis).
Both of you seem on par, then. After all, probabilistic knowledge
about the chance of heads (which you plausibly have, and your friend
lacks) is no more intrinsically epistemically valuable than probabilistic
74. Nontheoretical propositions, for our purposes, are just the propositions to
which theoretical hypotheses (chance hypotheses, causal models, and so forth) assign
probabilities.
75. The MaxSen prior s over hypotheses of the form B ¼ x is equivalent to the non-
symmetric prior over hypotheses of the form u ¼ x, deﬁned by the probability density
h ðxÞ ¼ 2:9469x ðx 2 2 x 4Þ0:2(see ﬁg. 17).
J A S O N K O N E K
578
Downloaded from http://read.dukeupress.edu/the-philosophical-review/article-pdf/125/4/509/462964/509Konek.pdf by WIKIPEDIA LIBRARY user on 03 July 2023

knowledge about the square root of the chance of heads (which you lack,
and your friend has). But there is an important difference between you.
Given that you both satisfy the Principal Principle, your credence that the
coin will in fact come up heads on the next toss—call this proposition
“HEADS”—is also the product of cognitive ability, as we will see. Hers is less
so. If this is right, then there is good epistemic reason to describe your
inference problem in terms of B (that is, as a problem of adjudicating
between hypotheses about the coin’s bias, or chance of HEADS) rather
than u ¼
ﬃﬃﬃﬃ
B
2p
, or any other quantity. Describing your inference problem in
this way does the most to help you secure probabilistic knowledge about
whether the coin will in fact come up heads on the next toss.
It is not too hard to see why. Your credence for HEADS is tied to your
credences about the coin’s bias, or the chance of HEADS, if you are
rational, in a way that it is not tied to your credences about the square
root of its bias (or any other quantity). Minimally rational credence func-
tions satisfy the Principal Principle. And given that your credence func-
tion satisﬁes the Principal Principle, you estimate the truth-value of
HEADS by estimating its chance. Your credence (truth-value estimate)
for HEADS just is your best estimate (expectation) of the chance of
HEADS. Your credence for HEADS is not, in contrast, determined by
your best estimate (expectation) of u ¼
ﬃﬃﬃﬃ
B
2p
, d ¼ cos B, or any other func-
tion of the coin’s bias B. It is a well-known fact that one’s expected value
for B is not a function of one’s expected value for u, or d, and so forth (see,
Figure 17.
The biased prior c over hypotheses about the value of u, which is equivalent
to the MaxSen prior s over hypotheses about the value of B
Probabilistic Knowledge and Cognitive Ability
579
Downloaded from http://read.dukeupress.edu/the-philosophical-review/article-pdf/125/4/509/462964/509Konek.pdf by WIKIPEDIA LIBRARY user on 03 July 2023

for example, Pettigrew, n.d.).76 Best estimates (expectations) are simply
not cleanly related in this way. So neither is your credence for HEADS a
function of your best estimate (expectation) of u, d, and so forth.
This tells us something about how to explain the accuracy of
your posterior credence for HEADS. Given that you satisfy the Principal
Principle, and so estimate truth-values by estimating chances, two facts
explain its accuracy: (i) how close your posterior chance estimate was to
the true chance of heads (how accurate your posterior chance estimate
was), and (ii) whether events went as expected; whether the truth-value of
HEADS happened to fall close to (or far from) from its chance.
This in turn tells us something about how your evidence might
explain the accuracy of your posterior credence for HEADS. The character
of your evidence (about the ﬁve initial ﬂips) does not explain the latter
fact, namely, why the truth-value of HEADS happened, on this occasion, to
fall close to (far from) its chance. It has no bearing on the world’s falling
in line with chance or not. So it must explain the former fact if it is to be
relevant to the accuracy of your posterior credence at all. That is, it must
explain the accuracy of your posterior chance estimate if it is to explain the
accuracy of your posterior credence for HEADS. And it must do this last bit
if that credence is to count as skillfully produced, and hence be a candi-
date for constituting probabilistic knowledge.
At bottom, this is why describing your inference problem in terms
of B—framing it as a problem that requires sorting out the true bias, or
chance of HEADS—rather than u ¼
ﬃﬃﬃﬃ
B
2p
(or any other quantity) helps you
secure probabilistic knowledge about whether the coin will in fact come
up heads on the next toss. For your posterior credence in HEADS to con-
stitute probabilistic knowledge, your evidence must explain its accuracy to
the greatest degree possible. And it must do so by explaining the accuracy
of the underlying chance estimate that determines it. Now recall that our
summary statistic—variance in objective expected posterior accuracy—
tracks the extent to which one’s evidence explains one’s posterior accu-
racy (and, in turn, the extent to which one’s posterior accuracy is the
product of cognitive ability). The smaller that statistic, the greater the
extent to which the character of the evidence explains that accuracy.
So the objective expected posterior accuracy of the chance estimate that
76. To see this, consider a concrete example. Suppose you adopt the MaxSen prior
over hypotheses about the coin’s bias, B, namely, the beta distribution s with a ¼ b < 1:2.
Then your expected value for B is 0.5, while the square of your expected value for u is
approximately 0.453.
J A S O N K O N E K
580
Downloaded from http://read.dukeupress.edu/the-philosophical-review/article-pdf/125/4/509/462964/509Konek.pdf by WIKIPEDIA LIBRARY user on 03 July 2023

determines your credence in HEADS must vary minimally across chance
hypotheses. Finally—and this is the crucial point—this happens exactly
when you describe your inference problem in terms of the bias of the
coin, B.
To see this, measure the inaccuracy of your posterior chance esti-
mate—that is, the expected value of the coin’s bias B determined by
your posterior credence function c 0: Expc 0ðBÞ—by one particularly attrac-
tive scoring rule, namely, the Brier score (section 5.2). And measure
variance in objective expected posterior accuracy across chance hypoth-
eses, varðExpc 0ðBÞÞ, in the usual way (by the difference in its maximum
and minimum expected inaccuracies). Now note that varðExpc 0ðBÞÞ takes
a unique minimum when your prior credences over hypotheses about
the coin’s bias, B, are given by the beta distribution s with a ¼ b < 1:2
(see ﬁg. 18). And MaxSen delivers s as your prior exactly when you
describe your inference problem in terms of B rather than u ¼
ﬃﬃﬃﬃ
B
2p
, or
any other quantity.77
The moral: MaxSen is language, or description, dependent, like
MaxEnt. But it also furnishes a clear rationale for favoring one way of
describing your inference problem over others. The rationale: there is a
Figure 18.
var ðExpc 0ðBÞÞ for beta priors c with 0 # a; b # 2; given n ¼ 5
77. While var ðExpc 0ðBÞÞ takes a minimum at c ¼ s in this particular inference problem, it
is an open question whether this is true more generally.
Probabilistic Knowledge and Cognitive Ability
581
Downloaded from http://read.dukeupress.edu/the-philosophical-review/article-pdf/125/4/509/462964/509Konek.pdf by WIKIPEDIA LIBRARY user on 03 July 2023

single description that does the most to help secure probabilistic knowl-
edge not simply about theoretical hypotheses (for example, hypotheses
about the chances) but also about nontheoretical propositions (for
example, whether the coin will in fact come up heads). So MaxSen is
not inconsistent in the way that MaxEnt seems to be. It does not yield
different prescriptions relative to different ways of describing your infer-
ence problem while staying silent about how to choose between them.
10. Appendix B: Outstanding Issues
This essay outlines one kind of formal tool—the maximum sensitivity
principle, or MaxSen—for securing skillfully produced credences. But it
does not provide a full explication (or defense) of MaxSen. For example,
we restricted our attention to inference problems involving simple theo-
retical hypotheses about the bias of a coin and binomial data, that is, data
that comes in the form of a sequence of “successes” (heads) and “failures”
(tails) and is generated by a ﬁxed number of identical and independent
trials of an experiment (IID trials).
As it turns out, we can lift this restriction. MaxSen generalizes
straightforwardly to inference problems involving more complicated
theoretical hypotheses and data, for example, multinomial chance
hypotheses—hypotheses about the chance of not only two possible out-
comes (for example, heads or tails) but more generally, any ﬁnite number
of possible outcomes—and multinomial data, that is, sequences of such
outcomes generated by a ﬁxed number of IID trials. (Indeed, MaxSen
generalizes straightforwardly to inference problems involving any para-
metric family of chance hypotheses, that is, hypotheses about chance
distributions that can be described by a ﬁnite number of parameters.)
And quite a few inference problems in the sciences, engineering, and so
forth, are like this. They involve performing a ﬁxed number of identical
and independent trials of an experiment with only ﬁnitely many possible
outcomes, in order to adjudicate between competing theoretical hypoth-
eses, which specify the respective chances of those outcomes. Any prob-
lem of this sort can be thought of as sampling from a multinomial chance
distribution (see, by way of comparison, Walley 1996). Here, for example,
are two paradigmatic instances:
.
An aerospace engineering company tests a number of nearly
identical rocket prototypes and records which of the (ﬁnitely
many) components fail in any unsuccessful run, in order to
J A S O N K O N E K
582
Downloaded from http://read.dukeupress.edu/the-philosophical-review/article-pdf/125/4/509/462964/509Konek.pdf by WIKIPEDIA LIBRARY user on 03 July 2023

adjudicate between competing hypotheses about the chance of
such a rocket failing in one way or another.
.
A pharmacology lab runs a double-blind, randomized, con-
trolled study of a new drug and records any of the (ﬁnitely
many) possible side effects, in order to adjudicate between com-
peting hypotheses about the drug’s impact on one’s chances of
suffering such side effects.
Of course, many inference problems are not like this. Microbiologists,
for example, design and perform experiments aimed at adjudicating
between even more complex theoretical hypotheses than those discussed
above, for example, Hidden Markov models that describe the various
networks (protein-protein interaction networks, and so forth) that give
rise to cell behavior (see, by way of comparison, Barabasi and Oltvai
2004). In inference problems of this sort, prior credences will turn out
to be enormously complex (nonparametric priors).78 A fuller explication
of MaxSen would illustrate how inference techniques familiar from
nonparametric Bayesian statistics and machine learning (for example,
Markov Chain Monte Carlo) can be used to determine a MaxSen prior in
such problems, and to compute its posterior.79
In addition, in some contexts of inquiry, we simply do not know
enough about the data-generating process to know which data items our
experiment might produce (what statisticians call the sample space of the
experiment). For example, while you might know enough about drug
A to know what its possible side effects are (because it is relevantly similar
to other well-studied drugs), you might have no idea what the possible
side effects of drug B are. And in still other contexts of inquiry, new
scientiﬁc data is simply not generated by running IID trials of an exper-
iment, or anything of the sort, for example, in paleontology, geology, and
so forth. In such contexts, it is not obvious how to generalize MaxSen to
help secure probabilistic knowledge.
Examining the boundaries of the class of contexts in which
MaxSen is applicable and responding in full to language-dependence
concerns are tasks that require separate investigation. Our aim here
was simply to illustrate, in broad strokes, one kind of formal tool for
78. Nonparametric priors specify a joint distribution over an inﬁnite number of
parameters, for example, each of the uncountably many values of a probability density
function.
79. See Orbanz and Teh 2010 for an overview of nonparametric Bayesian statistics.
Probabilistic Knowledge and Cognitive Ability
583
Downloaded from http://read.dukeupress.edu/the-philosophical-review/article-pdf/125/4/509/462964/509Konek.pdf by WIKIPEDIA LIBRARY user on 03 July 2023

delivering skillfully produced credences, and in doing so, to provide a
framework for theorizing about how to secure probabilistic knowledge. I
conclude by raising a few additional questions to be addressed in future
research.
.
We speciﬁed the MaxSen prior using one particular proper
scoring rule, or inaccuracy measure, namely, the continuous
ranked probability score. How robust are our results across
other proper scoring rules, for example, the energy score
(Gneiting and Raftery 2007, 367)?
.
MaxSen minimizes the explanatory relevance of one particular
nonevidential factor, namely, prior accuracy. Are there any
other sorts of nonevidential factors whose explanatory rel-
evance we might mitigate by savvy prior construction? What
sorts of hurdles might we face when handling multiple factors
at once? What sorts of compromises might we be forced to
strike?
.
The MaxSen prior outperforms alternative precise priors,
including the MaxEnt prior, vis-a`-vis delivering skillfully pro-
duced posteriors. But we have not compared the MaxSen prior
to imprecise priors, or sets of probability functions. Is there good
epistemic reason to prefer the MaxSen prior to alternative
imprecise priors, at least in certain contexts of inquiry?
References
Barabasi, A.-L., and Z. N. Oltvai. 2004. “Network Biology: Understanding the
Cell’s Functional Organization.” Nature Reviews Genetics 5: 101–13.
Bennett, J. 1988. Events and Their Names. Indianapolis: Hackett Publishers.
Birnbaum, A. 1962. “On the Foundations of Statistical Inference.” Journal of the
American Statistical Association 57, no. 298: 269–306.
Easwaran, K. 2013. “Expected Accuracy Supports Conditionalization—and
Conglomerability and Reﬂection.” Philosophy of Science 80, no. 1: 119–42.
Edwards, W., H. Lindman, and L. J. Savage 1963. “Bayesian Statistical Inference
for Psychological Research.” Psychological Review 70, no. 3: 193–242.
Eriksson, L., and A. Hajek. 2007. “What Are Degrees of Belief ?” Studia Logica 86,
no. 2: 183–213.
Fetzer, J. 1982. “Probabilistic Explanations.” PSA: Proceedings of the Biennial Meet-
ing of Philosophy of Science Association 2: 194–207.
———. 1983. “Probability and Objectivity in Deterministic and Indeterministic
Situations.” Synthese 57: 367–86.
J A S O N K O N E K
584
Downloaded from http://read.dukeupress.edu/the-philosophical-review/article-pdf/125/4/509/462964/509Konek.pdf by WIKIPEDIA LIBRARY user on 03 July 2023

Fisher, R. 1922. “On the Mathematical Foundations of Theoretical Statistics.”
Philosophical Transactions of the Royal Society of London. Series A, Containing
Papers of a Mathematical or Physical Character 222: 309–68.
Foley, R. 1992. “The Epistemology of Belief and the Epistemology of Degrees of
Belief.” American Philosophical Quarterly 29, no. 2: 111–24.
Furrer, F., J. Aberg, and R. Renner. 2011. “Min- and Max-Entropy in Inﬁnite
Dimensions.” Communications in Mathematical Physics 306, no. 1: 165–86.
Gillies, D. 2000. “Varieties of Propensity.” British Journal for the Philosophy of Science
51: 807–35.
Gneiting, T., and A. Raftery. 2007. “Strictly Proper Scoring Rules, Prediction,
and Estimation.” Journal of the American Statistical Association 102, no. 477:
359–78.
Greaves, H., and D. Wallace. 2006. “Justifying Conditionalization: Conditiona-
lization Maximizes Expected Epistemic Utility.” Mind 115, no. 459: 607–32.
Greco, J., and J. Turri. 2013. “Virtue Epistemology.” In The Stanford Encyclopedia of
Philosophy. Winter 2013 Edition, ed. Edward N. Zalta. plato.stanford.edu/
archives/win2013/entries/epistemology-virtue/.
Hersbach, H. 2000. “Decomposition of the Continuous Ranked Probability
Score for Ensemble Prediction Systems.” Weather and Forecasting 15: 559–70.
Intergovernmental Panel on Climate Change (IPCC) 2013. “IPCC, 2013: Sum-
mary for Policymakers.” In Climate Change 2013: The Physical Science Basis;
Contribution of Working Group 1 to the Fifth Assessment Report of the Intergov-
ernmental Panel on Climate Change, ed. T. F. Stocker, D. Qin, G.-K. Plattner,
M. Tignor, S. K. Allen, J. Boschung, A. Nauels et al., 1–30. Cambridge:
Cambridge University Press.
Jaynes, E. T. 1957. “Information Theory and Statistical Mechanics.” Physical
Review 106, no. 4: 620–30.
Jeffrey, R. 1965. The Logic of Decision. Chicago: University of Chicago Press.
———. 1970. “Dracula Meets Wolfman: Acceptance vs. Partial Belief.” In Induc-
tion, Acceptance, and Rational Belief, ed. M. Swain, 157–85. Dordrecht: Reidel.
———. 2004. Subjective Probability: The Real Thing. Cambridge: Cambridge Uni-
versity Press.
Joyce, J. 1998. “A Nonpragmatic Vindication of Probabilism.” Philosophy of Science
65: 575–603.
———. 2005. “How Probabilities Reﬂect Evidence.” Philosophical Perspectives 19:
153–78.
———. 2007. “VIII—Epistemic Deference: The Case of Chance.” Proceedings of
the Aristotelian Society 107, part 2: 187–206.
———. 2009. “Accuracy and Coherence: Prospects for an Alethic Epistemology
of Partial Belief.” In Degrees of Belief, ed. F. Huber and C. Schmidt-Petri,
263–97. Dordrecht: Springer.
———. 2010. “A Defense of Imprecise Credences in Inference and Decision
Making.” Philosophical Perspectives 24: 281–323.
Probabilistic Knowledge and Cognitive Ability
585
Downloaded from http://read.dukeupress.edu/the-philosophical-review/article-pdf/125/4/509/462964/509Konek.pdf by WIKIPEDIA LIBRARY user on 03 July 2023

Kaplan, M. 2010. “In Defense of Modest Probabilism.” Synthese 176, no. 1: 41–55.
Keynes, J. M. 1921. A Treatise on Probability. London: Macmillan.
Kraft, C. H., J. W. Pratt, and A. Seidenberg. 1959. “Intuitive Probability on Finite
Sets.” Annals of Mathematical Statistics 30, no. 2: 408–19.
Krantz, D., D. Luce, P. Suppes, and A. Tversky. 1971. Foundations of Measurement.
Vol 1., Additive and Polynomial Representations. New York: Academic Press.
Kyburg, H. E. 1961. Probability and the Logic of Rational Belief. Middletown, CT:
Wesleyan University Press.
Leitgeb, H. 2013. “The Stability Theory of Belief.” Philosophical Review 123, no. 2:
131–71.
Leitgeb, H., and R. Pettigrew. 2010. “An Objective Justiﬁcation of Bayesianism
II: The Consequences of Minimizing Inaccuracy.” Philosophy of Science 77,
no. 2: 236–72.
Levi, I. 1980. The Enterprise of Knowledge. Cambridge, MA: MIT Press.
Lewis, D. 2000. “Causation as Inﬂuence.” Journal of Philosophy 97, no. 4: 182–97.
List, C., and M. Pivato. 2015. “Emergent Chance.” Philosophical Review 124, no. 1:
119–52.
Meacham, C. J. G. 2005. “Three Proposals regarding a Theory of Chance.” Nouˆs
39: 281–307.
———. 2010. “Two Mistakes regarding the Principal Principle.” British Journal
for the Philosophy of Science 61, no. 2: 407–31.
Mellor, D. H. 1995. The Facts of Causation. London: Routledge.
Moss, S. 2013. “Epistemology Formalized.” Philosophical Review 122, no. 1: 1–43.
Nozick, R. 1981. Philosophical Explanations. Oxford: Oxford University Press.
Oddie, G. 1997. “Conditionalization, Cogency, and Cognitive Value.” British
Journal for the Philosophy of Science 48, no. 4: 533–41.
Orbanz, P., and Y. W. Teh. 2010. “Bayesian Nonparametric Models.” In Encyclo-
pedia of Machine Learning, ed. Claude Sammut and Geoffrey I. Webb, 81–89.
New York: Springer.
Pettigrew, R. 2013a. “Epistemic Utility and Norms for Credences.” Philosophy
Compass 8, no. 10: 897–908.
———. 2013b. “A New Epistemic Utility Argument for the Principal Principle.”
Episteme 10, no. 1: 19–35.
———. 2016. Accuracy and the Laws of Credence. Oxford: Oxford University Press.
———. n.d. “Accuracy Domination Arguments and Credences as Estimates of
Truth-Values.” Unpublished manuscript.
Predd, J. B., R. Seiringer, E. H. Lieb, D. N. Osherson, H. V. Poor, and S. R.
Kulkarni. 2009. “Probabilistic Coherence and Proper Scoring Rules.” IEEE
Transactions on Information Theory 55, no. 10: 4786–92.
Pritchard, D. 2005. Epistemic Luck. Oxford: Clarendon.
———. 2008. “Sensitivity, Safety, and Anti-luck Epistemology.” In The Oxford
Handbook of Scepticism, ed. J. Greco, 437–55. Oxford: Oxford University
Press.
J A S O N K O N E K
586
Downloaded from http://read.dukeupress.edu/the-philosophical-review/article-pdf/125/4/509/462964/509Konek.pdf by WIKIPEDIA LIBRARY user on 03 July 2023

———. 2010. “Anti-luck Virtue Epistemology.” In The Nature and Value of Knowl-
edge: Three Investigations, ed. D. P. A. Haddock and A. Millar, 48–66. Oxford:
Oxford University Press.
———. 2012. “Anti-luck Virtue Epistemology.” Journal of Philosophy 109, no. 3:
247–79.
Savage, L. 1954. The Foundations of Statistics. New York: Dover.
Schervish, M., T. Seidenfeld, and J. Kadane 2009. “Proper Scoring Rules, Domi-
nated Forecasts, and Coherence.” Decision Analysis 6, no. 4: 202–21.
Scott, D. 1964. “Measurement Structures and Linear Inequalities.” Journal of
Mathematical Psychology 1, no. 2: 233–47.
Seidenfeld, T. 1986. “Entropy and Uncertainty.” Philosophy of Science 53, no. 4:
467–91.
Singer, P. 1975. Animal Liberation. New York: New York Review.
———. 1985. “Ethics and the New Animal Liberation Movement.” In In Defense
of Animals, 1–10. New York: Basil Blackwell.
Sosa, E. 1999. “How Must Knowledge Be Modally Related to What Is Known?”
Philosophical Topics 26: 373–84.
———. 2007. Apt Belief and Reﬂective Knowledge. Vol. 1, A Virtue Epistemology.
Oxford: Oxford University Press.
Swanson, E. 2006. “Interactions with Context.” PhD diss., Massachusetts Insti-
tute of Technology.
Titelbaum, M. 2015. Fundamentals of Bayesian Epistemology. Oxford: Oxford Uni-
versity Press.
Venn, J. 1866. The Logic of Chance. London: Macmillan.
Walley, P. 1996. “Inferences from Multinomial Data: Learning about a Bag of
Marbles.” Journal of the Royal Statistical Society, ser. B (Methodological) 58, no.
1: 3–57.
Williamson, J. 2010. In Defence of Objective Bayesianism. New York: Oxford Univer-
sity Press.
Zagzebski, L. 1996. Virtues of the Mind. Cambridge: Cambridge University Press.
Probabilistic Knowledge and Cognitive Ability
587
Downloaded from http://read.dukeupress.edu/the-philosophical-review/article-pdf/125/4/509/462964/509Konek.pdf by WIKIPEDIA LIBRARY user on 03 July 2023

