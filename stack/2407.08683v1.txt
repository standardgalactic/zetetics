SEED-Story: Multimodal Long Story Generation
with Large Language Model
Shuai Yang 1âˆ—
Yuying Ge 2â€ 
Yang Li 1
Yukang Chen 4
Yixiao Ge 2,3
Ying Shan 2,3
Yingcong Chen 1,5â€ 
1HKUST(GZ)
2ARC Lab, Tencent PCG
3Tencent AI Lab
4CUHK
5HKUST
Suddenly, George heard a 
noise. â€¦
George and the dog then played a 
game of hide and seek. George 
hid behind a couch â€¦
The next day, George and the dog 
decided to explore the city â€¦
George and the dog stood in 
front of the building, looking up 
at the lit windows â€¦
George then noticed a building 
with a reflective glass â€¦
They were in a room with a door, 
waiting for their friend to join 
them â€¦
Suddenly, the door opened, and a 
man in a yellow suit walked in â€¦
George stopped on the city 
sidewalk, looking up at the sky â€¦
He seemed deep in thought, 
unaware of George and the dog 
watching him from below â€¦.
To his surprise, the noise was 
Georgeâ€˜s friend, a small brown 
dog â€¦
He looked around with a curious 
expression, wondering what 
adventures awaited him â€¦
USER: Here is the beginning of 
a story about George, Please 
continue it â€¦
4
5
6
7
8
9
10
11
1
2
3
Figure 1: The introduced SEED-Story, powered by MLLM, is capable of generating multimodal
long stories from user-provided images and texts as the beginning of the story. The generated story
consists of rich and coherent narrative texts, along with images that are consistent in characters and
style. The story can span up to 25 multimodal sequences (see Appendix), even though we only use a
maximum of 10 sequences during training.
âˆ—Worked done at ARC Lab, Tencent PCG
â€ Corresponding Authors
Preprint. Under review.
arXiv:2407.08683v1  [cs.CV]  11 Jul 2024

Abstract
With the remarkable advancements in image generation and open-form text gener-
ation, the creation of interleaved image-text content has become an increasingly
intriguing field. Multimodal story generation, characterized by producing narra-
tive texts and vivid images in an interleaved manner, has emerged as a valuable
and practical task with broad applications. However, this task poses significant
challenges, as it necessitates the comprehension of the complex interplay be-
tween texts and images, and the ability to generate long sequences of coherent,
contextually relevant texts and visuals. In this work, we propose SEED-Story,
a novel method that leverages a Multimodal Large Language Model (MLLM)
to generate extended multimodal stories. Our model, built upon the powerful
comprehension capability of MLLM, predicts text tokens as well as visual to-
kens, which are subsequently processed with an adapted visual de-tokenizer to
produce images with consistent characters and styles. We further propose mul-
timodal attention sink mechanism to enable the generation of stories with up to
25 sequences (only 10 for training) in a highly efficient autoregressive manner.
Additionally, we present a large-scale and high-resolution dataset named StoryS-
tream for training our model and quantitatively evaluating the task of multimodal
story generation in various aspects. The models, codes and datasets are released in
https://github.com/TencentARC/SEED-Story.
1
Introduction
Interleaved image-text data is ubiquitous on the internet, characterized by multiple images interspersed
with text. In recent years, there has been a surge of interest in generating interleaved image-text
content [52, 16, 1, 11, 49], driven by the remarkable advancements in image generation [43, 26, 9, 36,
54] and open-form text generation [53, 48, 59]. This has given rise to Multimodal Story Generation,
an intriguing and valuable task that involves the generation of narrative texts and vivid images in
an interleaved manner. This task goes beyond traditional text-based storytelling by fusing text and
visuals to enrich narratives, creating a more immersive storytelling experience where visuals and text
dynamically interact and complement each other.
However, multimodal story generation poses significant challenges due to the complexity of the
inputs and the high demands on the quality of the outputs. Firstly, this task necessitates a thorough
comprehension of interleaved data, where text is not only abstract and narrative in nature, but also
deeply intertwined with complex images. The model must be adept at deciphering the intricate
relationships between images and texts to maintain a coherent narrative flow. Secondly, this task
requires the generation of not only a plausible text plot, but also visually captivating images that
are consistent in characters and styles. The model should be capable of achieving coherence in the
generation of both text and visuals, ensuring an engaging storytelling output.
Recently, Multimodal Large Language Models (MLLMs) [23, 60, 39, 5, 28, 58, 26, 22] have show-
cased powerful comprehension abilities in understanding multimodal data, which makes them ideally
suited for interleaved image-text content in multimodal stories. Consequently, we introduce SEED-
Story, a novel approach that builds upon the MLLM to harness its comprehension strength, while
further equipping it with the capability to generate coherent images align with the narrative texts.
Specifically, following previous work [45, 16], we utilize a pre-trained image tokenizer and de-
tokenizer, which can decode realistic images with SD-XL [41] by taking the features of a pre-trained
ViT as input. During training, given the interleaved visual and textual data, we adopt the next-word
prediction and image feature regression training objectives to regularize multimodal generation. A
fixed number of learnable queries are fed into the MLLM, where the output hidden states are trained
to reconstruct the ViT features of the target images. To further enhance the consistency of characters
and styles in generated images, we propose de-tokenizer adaptation, where the regressed image
features from the MLLM are fed into the de-tokenizer for tuning SD-XL. This adaptation allows for
better maintenance of coherence in low-level image details from the de-tokenizer, ensuring a more
visually consistent storytelling output.
2

Furthermore, to enable the efficient generation of coherent long stories, we propose a multimodal
attention sink mechanism based on window attention [7], which maintains a fixed-size sliding window
on the Key-Value (KV) states of the most recent tokens, as well as the beginning of text tokens,
images tokens, and the end of image tokens. We empirically find that retaining these tokens will
largely mitigate the modelâ€™s failure with window attention when the token length surpasses the cache
size, allowing our model to generalize to longer sequences than the training sequence length in an
efficient manner. Our model with the proposed multimodal attention sink mechanism can generate
long stories with up to 30 multimodal sequences, featuring rich text plots and diverse visual scenarios.
Additionally, we introduce a dataset named StoryStream for training and evaluating multimodal
story generation. We design an automatic pipeline that leverages MLLMs to obtain a large-scale and
high-resolution dataset featuring a sequence of narrative-rich texts and intriguing images, derived
from animated videos. StoryStream is four times larger in terms of data volume compared to the
existing largest story dataset [27], and it boasts higher image resolution, longer sequence lengths,
and more detailed story narratives. We further meticulously design evaluation metrics to assess
multimodal story generation, taking into account image style consistency, story engagement, and
image-text coherence. The evaluation results demonstrate that our model, SEED-Story, achieves
superior performance in these aspects.
In summary, Our contributions are three-fold. (1) We propose SEED-Story, a novel method that
leverages an MLLM to generate multimodal stories with rich narrative text and contextually relevant
images. (2) We propose multimodal attention sink to enable the efficient generation of long stories
with sequence lengths larger than those used during training. (3) We introduce StoryStream, a
large-scale dataset specifically designed for training and benchmarking multimodal story generation.
2
Related Work
Story Visualization vs. Multimodal Story Generation
The domain of story visualization, initially
pioneered by StoryGAN [25], has witnessed substantial advancements across various models focus-
ing on improving image quality, narrative flow, and character consistency. StoryGAN introduced
a framework based on sequential conditional GANs with dual discriminators that enhanced both
narrative coherence and visual fidelity. Subsequent developments such as DuCoStoryGAN [32]
and VLCStoryGAN [31] have integrated video captioning to better align text descriptions with
corresponding visuals. The field has further expanded with the adoption of diffusion models aimed
at refining the quality of generated story images. StoryDALL-E [33] utilizes pre-trained models
augmented with cross-attention layers to support story progression from an initial image. Innovations
like AR-LDM [35] and Story-LDM [42] have introduced auto-regressive diffusion models to create
coherent sequences of images, while TaleCrafter [17] has employed LoRA and optimization tech-
niques to ensure consistent characters throughout complex visual narratives. Intelligent Grimm [27]
has proposed open-ended story visualization and has developed the auto-regressive model StoryGen,
which includes a vision-language context module to further enhance character consistency. In con-
trast to all above, our work advances beyond story visualization by focusing on multimodal story
generation. This approach continues the story with additional text and creates corresponding images,
offering a richer, more engaging narrative experience into the story generation process.
MLLM for Multimodal Story Generation
In the rapidly evolving domain of large language
models (LLMs) [53, 8, 10] and multimodal large language models (MLLMs) [23, 60, 30, 40, 6,
29, 58, 26, 46, 57, 15, 14, 55, 11, 61, 47, 24], recent research, such as StoryGPTV [44], explores
using MLLMs for story generation by converting visual features into token embeddings. StoryGPTV
requires character and object masks for training its LDM. In contrast, our model, with the visual
tokenization and de-tokenization, have a stronger visual comprehension ability, making it trains
directly from interleaved text and image data without the need for masks. MM-interleaved [52] design
an MMFS module to process interleaved text-image data and achieve multimodal story generation
through image features and LLM output. While our approach directly generates images through LLM
output tokens. We take MM-interleaved as an important baseline and compare with it.
Visual Story Dataset
In the landscape of datasets for visual storytelling, various collections have
been developed. The VIST [20] dataset is noteworthy for its use of realistic images, though it
struggles with maintaining character consistency across stories. The Pororo [25] and Flintstones [18]
3

Table 1: Comparison of multimodal story generation datasets. The table provides details on the
number of images, their resolution, the total length of visual stories, and the average text length per
sentence, which indicates the narrative detail of the text. Note that StorySalon has various size of
images and we choose one of the typical sizes presented here.
Datasets
Number of Images
Resolution
Story Length
Avg Text Length
Flintstones [18]
122,560
128 Ã— 128
5
86
Pororo [25]
73,665
128 Ã— 128
5
74
StorySalon [27]
159,778
432 Ã— 803
14
106
StoryStream
257,850
480 Ã— 854
30
146
datasets, while popular for animation-based story datasets, are hindered by their low resolution and
the simplicity of their accompanying texts. Another significant dataset is StorySalon [27], which
offers high-resolution images and is large in scale, but it lacks global consistency across a series
of images. To address these gaps, we introduce StoryStream, a globally consistent, large-scale,
high-resolution animated style dataset with engaging, narrative-rich text for complex storytelling,
overcoming the limitations of existing datasets. The analysis is shown in Table 1.
3
Method
3.1
Story Generation with Multimodal Large Language Model
Visual Tokenization and De-tokenization
The overview of our method is presented in Figure 2.
To effectively extend visual stories, our model must comprehend and generate both images and text.
Drawing inspiration from recent advancements in generative MLLMs that unify image comprehension
and generation [41], we develop a multimodal story generation model. Our model employs a pre-
trained Vision Transformer [12] (ViT) as the visual tokenizer and a pre-trained diffusion model
as the visual de-tokenizer to decode images by using ViTâ€™s features as inputs. Specifically, visual
embeddings from the ViT tokenizer are fed into a learnable module, which then serves as inputs for
the U-Net of the pre-trained SD-XL [41]. This process replaces the original text features with visual
embeddings. During this stage, the parameters are optimized using open-world text-image pair data
as well as story data to enhance the modelâ€™s encoding-decoding capability. After this training phase,
we expect the visual tokenizer and de-tokenizer modules to preserve as much image information as
possible in the feature space.
Story Instruction Tuning
In our instruction tuning process for story generation, we sample a
random-length subset of a story data point for each iteration. The model is tasked with predicting
the next image and the next sentence of the story text. Within MLLM, all images are converted into
image features using a pre-trained ViT tokenizer. For the target text tokens, we perform next-token
prediction and use Cross Entropy loss to train for this discrete target. For the target image features,
the model uses a series of learnable queries as inputs and continuously outputs a series of latent
embeddings. We then compute the cosine similarity loss between the MLLMâ€™s output and the target
image features. During this stage, we fine-tune the SEED-Story model using a LoRA [19] module.
De-tokenizer Adaptation
After instruction tuning, the SEED-Story MLLM effectively produces
story images with correct semantics but lacks style consistency and details. We attribute this issue to
the misalignment between the latent space of the MLLM output and the image features. To address
this, we perform de-tokenizer adaptation for style and texture alignment. In this stage, only the
SD-XL image de-tokenizer is trained. Conditioned on the MLLM output embeddings, SD-XL is
expected to generate images that are pixel-level aligned with the ground truth. The separate training
of the de-tokenizer offers two key advantages. First, it avoids optimization conflicts between the
LLM and the de-tokenizer. Second, it conserves memory, making the process executable on GPUs
with limited memory.
3.2
Long Story Generation with Multimodal Attention Sink
Generating long visual stories has substantial potential in various applications, including education
and entertainment. However, creating these stories with MLLMs presents significant challenges.
Datasets for extended, interleaved stories are not only rare but also impede the training process due
4

SDXL Image
De-tokenizer
ViT 
Tokenizer
George, the small 
monkey, was on an 
icy adventureâ€¦
â€¦The man 
comforted Georgeâ€¦
They stood together against 
the snowy backdropâ€¦
Training sample
Target
Training pipeline
Image features
Stage1: Visual Tokeniza3on & De-tokeniza3on
SEED-Story: Mul&modal Large Language Model (LoRA     )
298
16
89
â€¦
â€¦
BOI
EOI
16
218
82
â€¦
â€¦
BOI
EOI
BOS
344 218
82
898
â€¦
BOI
EOI
344 218
82
â€¦
â€¦
BOI
EOI
Stage2: Instruc3on Tuning
Regression
Loss
344 218
82
898
â€¦
BOI
Next-token
prediction
Label
Target Image features
Learnable queries
MLLM outputs
The  man  comfort
They stood together
Stage3: De-tokenizer Adapta3on
Diffusion
Loss
MLLM outputs
SDXL Image
De-tokenizer
George the small
ðŸ”¥
ðŸ”¥
â„ 
ðŸ”¥
Figure 2: Overview of the SEED-Story Training Pipeline: In Stage 1, we pre-trains an SD-XL-based
de-tokenizer to reconstruct images by taking the features of a pre-trained ViT as inputs. In Stage 2,
we sample an interleaved image-text sequence of a random length and train the MLLM by performing
next-word prediction and image feature regression between the output hidden states of the learnable
queries and ViT features of the target image. In Stage 3, the regressed image features from the
MLLM are fed into the de-tokenizer for tuning SD-XL, enhancing the consistency of the characters
and styles in the generated images.
to their complexity. To address this, we have to employ a train-short-test-long approach, training
models on shorter narratives and extending to longer generations during inference.
Moreover, during inference, generating significantly longer stories than the training data often leads
to model degradation, producing lower-quality images, as illustrated in the first row of Figure 10.
This process also requires extensive token usage to ensure continuity and coherence, which in turn
increases memory and computational demands.
A simplistic solution for this is to use a sliding window technique, depicted in Figure 3 right (b).
However, this method disrupts the token relationships in the Key-Value (KV) cache, resulting in subpar
generative outcomes, as demonstrated by StreamingLLM [56]. To overcome this, StreamingLLM
introduces an attention sink mechanism that preserves the initial tokens, thus allowing for efficient
processing of lengthy generations without quality compromise. While effective in language models,
its efficacy diminishes in multimodal contexts, as shown in Figure 3 right (c).
To enhance long multimodal generation, we revisit the attention maps of MLLMs. After conducting
numerous experiments across various models and cases, we analyze the attention maps across different
5

(a) Dense a)en*on
(d) Mul*modal a)en*on sink
(c) Attention sink
â€¦
(b) Window a)en*on
â€¦
IMG
BOI
EOI
Current token
Sink tokens
Evicted tokens
â€¦
â€¦
â€¦
â€¦
â€¦
â€¦
Attention
Normal tokens
â€œ0â€ token
Punctua.on
Beginning of 
Image (BoI)
End of Image
(EoI)
Figure 3: Left: Visualization of the attention map when predicting the next token for multimodal
story generation. We observe that important attentions are aggregated into the first token of the whole
sequence (â€œ0â€ token), punctuation tokens, tokens adjacent to BoI, and tokens adjacent to EoI. Right:
The diagram of (a) Dense attention, which preserves all tokens in KV cache. (b) Window attention,
which evicts preceding tokens by a sliding window. (c) Attention sink, which preserves the beginning
tokens based on window attention. (d) Multimodal attention sink, which preserves the beginning of
text tokens, images tokens, and the end of image token based on window attention. It can efficiently
enable our model to generalize to generating longer sequences than the training sequence length.
layers and heads. Our analysis reveals that most queries predominantly focus on four types of tokens:
(1) starting tokens, (2) punctuation tokens, (3) beginning-of-image (BoI) tokens, and (4) end-of-image
(EoI) tokens. Unlike language-only models, MLLMs place considerable attention on specific image
tokens, particularly those near the BoI and EoI, as illustrated in Figure 3 left.
Building on these insights, we propose a new mechanism for extended generation in MLLMs, termed
the multimodal attention sink. During generation, we consistently retain the starting tokens and the
image tokens adjacent to the BoI and EoI. Although punctuation tokens receive high attention values,
their latent value norms are minimal, contributing insignificantly to the final output, so we do not
keep them, as noted by [13]. Our proposed mechanism enables our model to generate high-quality
images while maintaining a low computational footprint.
4
StoryStream Dataset
4.1
Dataset Construction
An ideal source for creating a multimodal story generation dataset is cartoon series, which inherently
contain rich plots and consistent character portrayals. We selected three cartoon series to construct
our dataset and we present the Curious George in the main body of our paper. The process begins
with collecting various series, from which we extract keyframes and their associated subtitles [21].
Each keyframe is then processed by GPT-4V [34] or Qwen-VL [6] to generate a detailed image
description. These elementsâ€”keyframe, subtitle, and descriptionâ€”are compiled into a single group.
We aggregate 30 such groups and input them into GPT-4, supplemented with background information
about the cartoon series. Following our instructions, GPT-4 generates high-quality narrative texts
suitable for training story generation models.
During dataset construction, we discovered that employing the above chain of thought approach
not only produces more accurate narrative text but also speeds up the construction process. Unlike
directly feeding all images directly to GPT-4, which is limited to 10 images due to API constrains, our
approach produces longer stories. We also significantly improve the modelâ€™s understanding of each
image by incorporating detailed descriptions. This enhancement in image comprehension enriches
the narrative details, providing a richer story generation reference.
6

Flintstone
Pororo
George, the small monkey, was
on an icy adventure with his
friend, the man in the yellow
snowsuit.
The snowflakes continued to fall
around them, creating a beautiful,
albeit cold, scene. Despite the
cold, the man comforted George,
who
was
showing
signs
of
concern.
They
stood
together
against
the
snowy backdrop, trying to figure out
their next move. George and his
friend found themselves standing on
an icy platform surrounded by water,
with large icebergs in the background.
George found comfort in the arms of his
friend. They stood together, ready to
face whatever came next. The man took
a moment to rest, seated against the icy
backdrop, still wearing his hat and
carrying a backpack, ready for their
next move.
Suddenly, the man had to run
through a snowy canyon. It was a
treacherous path with ice floes
floating
on
the
water
and
snowflakes falling from the sky.
Inside an icy cave, the man held
George
close.
They
peered
curiously
into
the
snowy
landscape,
cautious
of
what
might be out there.
854
480
StoryStream
Fred is holding his 
things in the room.
Wilma is walking 
across the room.
Fred is standing in a room 
talking while holding a pile of 
clothes and a hat over one arm.
Wilma is 
speaking in 
the room
Wilma is 
standing in a 
room, talking.
â€¦
128
A sleeping pirate pig 
wakes up.
Eddy smiles in front 
of a jail where his 
friends are.
A pirate pig gets back 
to sleep and wakes up 
again being surprised.
Eddy runs away 
and a pirate pig 
gets up.
A pirate pig runs to 
the open door. 
Pororo is hiding.
128
128
128
Length=30
Length=5
Length=5
â€¦
Figure 4: Data sample of our StoryStream dataset and existing multimodal story generation datasets.
Our multimodal story sequences consist of high-resolution images that are visually engaging, and
detailed narrative texts as underlined, closely resembling the real-world storybooks. Additionally, our
stories are more extended in length.
4.2
Key Features
Large-scale.
Our StoryStream dataset comprises three subsets totaling 257,850 released images.
This represents a significant improvement over existing datasets in terms of scale, specific numbers
are presented in column 2 of Table 1. To the best of my knowledge, it is the largest visual story
generation dataset featuring consistent main characters.
High Resolution.
Unlike existing story generation datasets, such as Flintstones and Pororo, which
offer images at a resolution of 128x128, our story stream dataset provides high-resolution images of
480x768.
Narrative Text.
Our dataset diverges from existing ones that utilize simple and descriptive lan-
guage. We offer abstract, narrative, detailed, and story-toned texts that are more akin to real-world
applications, such as visualizing narratives from a storybook, examples are shown in Figure 4. Story
7

text of existing datasets obey the form of â€œnameâ€ + â€œactionâ€, like â€œPoby is playing the violin.â€.
Contrarily, our story text involves more intrinsic elements. This effectively enhances the engagement
of audiences. An analysis of the average text length per sentence is shown in column 5 of Table 1.
Long Sequence.
Moreover, our dataset enhances long story comprehension by offering up to 30
images per story point. Within these 30 images, our corresponding texts present a cohesive narrative,
effectively conveying the progression and intricacies of extended stories.
a)In the art studio, George peeked 
from the edge, observing the 
creative chaos with an easel and 
scattered supplies.
b)George stood before an easel in 
a room, with paint cans and 
colorful splatters scattered on 
the ï¬‚oor.
c) A bespectacled man with a 
mustache stood in front of a 
half-painted wall, holding a 
notepad and pencil.
d)The man listened intently with a 
hand to his ear, a paint bucket 
and notepad beside him.
SEED-Story
LDM
StoryGen
a)
b)
c)
d)
Figure 5: Story visualization comparison between SEED-Story and baseline models. SEED-Story
generates images with higher quality and better consistency.
5
Experiment
5.1
Story Visualization
Table 2: Quantitative evaluation for story
visualization.
Model
FID â†“
CLIP Scoreâ†‘
LDM
67.29
0.7585
StoryGen
73.74
0.7573
SEED-Story
67.01
0.7793
Previous story generation approaches primarily utilize
diffusion models, focusing on visualizing story images.
These models take the previous image and text as input,
and then generate only the next image based on the current
text prompt. For a fair comparison, we adapt our model
to a visualization-only format. For StoryGen [27], we also
train it to produce images with previous images and texts. For LDM [43], we only give it text-image
pairs. The visual results are shown in Figure 5. SEED-Story model shows better style and character
consistency and higher quality compared to baselines. We also conduct a quantitative evaluation in
Table 2 to demonstrate our effectiveness.
(b) Image Style Consistency
(d) Image-Text Coherence
(c) Story Engagement
(a)
SEED wins
Figure 6: Quantitative evaluation of multimodal story generation between MM-interleaved versus
SEED-Story. (a): Histograms displaying FID scores. (b, c, d): GPT-4V is employed to choose a
preferred result generated by MM-interleaved and SEED-Story respectively. Pie charts show the win
rate, where â€œTiesâ€ indicates GPT-4V assesses their outcomes with equal scores.
8

5.2
Multimodal Story Generation
For comparative analysis, there are relatively few established methods for generating multimodal
stories. To establish a baseline for comparison, we fine-tune the recently developed MM-interleaved
model on our dataset. We detail the comparative results in Figure 6. The FID is employed to assess
the visual quality of the generated images. Additionally, we ask GPT-4V to compare and choose a
preferred option between each of the generation results of MM-interleaved and SEED-Story across
several dimensions: Style Consistency, which evaluates the stylistic uniformity across different
images; Story Engagement, which measures the ability of narratives to captivate and maintain
audience interest; and Image-Text Coherence, which assesses the alignment and relevance between
images and their accompanying texts. The appendix presents our visual results and details about
prompt engineering methods.
For qualitative presentation, we demonstrate our SEED-story effectively helps users control the
story plot. Figure 7 shows that using the same starting image but different initial texts, the narrative
branches into two distinct storylines.
We provide more cases in Figure 8 and Figure 9 to prove our multimodal long story generation
capabilities. SEED-story can generate long sequences with engaging plots and vivid images.
1
2
3
4
6
7
8
9
5
George was so excited that he 
jumped high in the air, his arms 
stretched out in front of him.
As George landed, he noticed a bee 
hovering nearby and reached out to 
touch it, curious about the small 
creature with stripes
George was right beside a bee, both of 
them in a grassy field with a building in 
the background where a small flower 
grew between the bricks
George was having a great time, smiling 
and extending his hand towards a bee that 
was flying near him, with the building and 
greenery behind him under a clear sky
He signaled to George, who was standing 
next to a house, that they had to go, with 
a lush green landscape and a wooden 
fence suggesting a peaceful, rural setting
George, wearing his yellow hat, looked 
puzzled but ready for adventure, standing 
against a simple grassy background with 
a stone wall partially visible
They hopped into a yellow car, George in 
the back seat, looking curious, with a 
building on the right and greenery around 
as they drove off into the distance
They drove through a picturesque 
rural landscape, with a large red barn, 
a small cottle, lush greenery, and a 
winding dirt road, all under a clear sky
Meanwhile, the man in the yellow hat was in 
a bit of a pickle, looking worried as he stood 
in front of a stone wall and a wooden 
structure, with a small window above him
USER: Please continue the story:
â€œAfter a day brimming with fun, they 
went back to their cheerful countryside 
residence, preparing for new adventures.â€
1
2
3
4
6
7
9
5
With a magnifying glass and a 
camera, George and his friend 
began their adventure, excited to 
explore the world around them.
George was thrilled to capture the 
beauty of the hills, the trees, and 
the wooden fence under the calm 
sky.
He took a picture of a quaint house 
surrounded by lush greenery, 
noticing small details through his 
magnifying lens.
Next, George captured his friend 
holding a book, with a vibrant 
landscape behind them, making 
the moment last forever.
He continued capturing the vibrant 
farm with his camera, capting the 
beauty of the farm, the farmhouse, 
and the lush greenery.
George then took a picture of a 
child holding a chicken, with a 
chicken coop in the background, 
capturing a fun moment on earth.
Next, George took a picture of four 
chickens in front of a small house, 
noticing a hand holding a blue 
object in the foreground.
With a smile, George held his camera, 
ready to capture another moment, 
excited to capture more of the world's 
beauty.
George then took a picture of his 
friend smiling, standing beside a 
bushy tree under the clear sky.
8
USER: Please continue the story:
â€œOn sunny day, George and his friend, 
the man in the yellow hat, decided to 
explore their farm.â€
Figure 7: Examples of multimodal story generation from SEED-Story. It shows two narrative
branches generated from the same initial image. The top branch starts with text referencing â€œthe man
in the yellow hat,â€ leading to images that include the character. The bottom branch starts without
mentioning the man, resulting in stories that diverge from the first by excluding him.
9

USER: Here is the beginning of 
a story about George, Please 
continue it â€¦
1
2
3
4
6
7
8
9
5
11
12
13
14
10
16
17
18
19
George stood next to a duck in 
front of a barn, surrounded by a 
grassy field and trees. The sky was 
blue with a few clouds.
15
21
22
23
24
20
A duck stood in the foreground, 
with a barn and trees in the 
background. The sky was blue with 
a small cloud.
George, wearing a puzzled 
expression, stood in front of a rural 
landscape with a large barn and a 
small house.
The background showed green 
fields, trees, and a blue sky with 
a single cloud.
George, with a curious expression, 
stood in front of a house with a 
barn in the background. The sky 
was blue with a small cloud.
A man in overalls and a hat was 
holding a bucket, standing next to a 
pile of dirt. George and a duck were 
there also, with a small house in the 
background.
George stood in a farm setting, 
looking at a small puddle. Behind 
him, a barn and a house were visible, 
along with greenery and a fence
A man in overalls was kneeling beside 
a pile of dirt, holding a shovel. George 
and a duck were there also, looking at 
the house in the background.
A duck stood in a grassy area, with 
a barn and trees in the background. 
The sky was blue with a small cloud.
A figure in overalls was holding a 
stick, standing next to a small 
house in a grassy area. There was a 
large bush on the side.
A man in overalls was digging a hole with 
a shovel in a grassy area. George and a 
duck were there also, looking on with 
curiosity. A small house sat in the 
background.
George stood in a grassy area, looking 
at a small puddle of water. Behind him, 
a barn and a house were visible, along 
with a wooden fence and trees.
A man in a hat and overalls was holding 
a stick, standing next to George. Both 
were looking at a muddy puddle in a 
grassy area, near a house and a tree.
George stood in a farm setting, looking 
towards a barn and a house. A puddle 
of water was on the ground in front of 
him, with greenery and a fence behind.
A character dressed in a suit and 
hat was standing in front of a rural 
landscape with a barn and a tree.
A man in a hat and shirt stood 
outside, looking surprised or 
confused. A tree and a building 
were visible in the background.
A man in a shirt and hat stood outside, 
looking surprised. George, the monkey, held 
onto the man's leg, looking up at him. A 
barn, a house, and a tree were in the 
background.
George, the cartoon monkey, looked 
surprised or worried. The background 
showed a grassy field with a small puddle 
of water and a tree trunk.
A man in a hat and overalls was 
kneeling in the grass, reaching out 
to George. A house and a tree were 
in the background.
George, the cartoon monkey, stood in a 
grassy area, looking curious. Behind him, a 
small puddle of water and a tree were visible. 
George appeared surprised or intrigued, 
with wide eyes and raised eyebrows.
A man in a hat and shirt looked 
surprised, standing in front of a barn 
and a tree. His mouth was open, and 
one hand was raised to his head.
George, the monkey, stood on a grassy 
field, looking curiously at a muddy puddle. 
Near the puddle, a pair of boots and a hat 
were visible. Behind, a fence and lush trees 
suggested a rural setting.
A man in a hat and shirt stood in front 
of a barn, looking surprised with one 
hand on the chest. A tree and grassy hill 
were visible in the background.
A man in a hat and shirt looked 
surprised, standing in front of a 
barn and a tree.
Figure 8: Multimodal long story generation results of SEED-Story.
10

USER: Here is the beginning of 
a story about George, Please 
continue it â€¦
1
2
3
4
6
7
8
9
5
11
12
13
14
10
16
17
18
19
Soon, George and the man in the 
yellow hat were back in their car, 
driving through a green tunnel. The 
car's interior was a comforting sight.
15
21
22
23
24
20
They drove along a curved road 
surrounded by grass and trees. The 
sky was clear, and George's heart 
was full of excitement.
Suddenly, George noticed a cat 
lying on the road. He crouched 
near the car's front wheel, 
watching the cat.
George stood near the parked car, 
looking up at the man in the 
yellow hat. They were surrounded 
by trees and a blue sky.
George, with his big eyes and 
mischievous smile, stood ready 
for the next adventure.
They reached a colorful park with a 
slide, a ladder, and a playground. 
The green bushes and trees added to 
the playful atmosphere.
George and the man in the yellow hat 
stood beside the parked car in the 
green landscape. They were ready for 
their next adventure.
Their journey led them to a vibrant 
landscape with a large slide in the 
foreground. A slide extended into a 
pool, inviting them to play.
They reached a vibrant roller coaster 
track set against a blue sky with fluffy 
white clouds. The ride featured red and 
yellow accents, adding to its charm.
The amusement park was colorful and 
bustling with visitors. A large Ferris 
wheel stood in the foreground, with 
buildings and greenery in the distance.
The man in the yellow hat and George 
stood in front of a roller coaster track, 
looking concerned. They were unsure of 
what to do next.
George, with his big, curious eyes, 
looked up at the man in the yellow hat. 
He was eager to have some fun.
Their journey took them to a colorful 
amusement park with a towering roller 
coaster. The park was surrounded by 
greenery and a cityscape.
The next day, George and the man in 
the yellow hat found themselves on a 
roller coaster. George appeared excited 
while the man looked concerned. Their 
adventure had just begun.
On their way home, they took a boat 
ride. The man held onto the steering 
wheel, while George and a child peeked 
out from the boat's window, smiling. 
They were having a great time.
Finally, they returned to their colorful house. 
The walls and the roof under the clear sky 
made the house look inviting and peaceful. 
It was the perfect end to a perfect day.
The man in the yellow hat and 
George waved goodbye from the 
doorway of their house. They were 
ready for another day of adventures.
At the end of the day, they returned to 
their vibrant yellow house. The walls 
and roof glowed in the twilight, and the 
trees and hills in the background added 
to the peacefulness of their home.
Finally, George and the man in the yellow hat sat 
on a bench inside a colorful cockpit with a steering 
wheel and control levers. They were ready to go 
home, but their adventurous spirit was ready for 
the next day.
The man in the yellow hat and George waved 
goodbye from the doorway of their house. 
They were ready for another day of 
adventures in the park, near the river.
Their last ride before heading home was a 
boat. The man held the steering wheel, 
while George and a child peeked out from 
a window. They were all smiling, looking 
forward to their next adventure.
Finally, George and the man in the yellow hat 
sat on a bench inside a colorful cage with a 
bright wheel. The man looked concerned while 
George appeared curious. They were ready to 
go home, but their adventurous spirit was 
ready for the next day.
Their last ride before heading home was a 
boat. The man held the steering wheel, while 
George and a child peeked out from the 
window. They were all smiling, looking 
forward to their next adventure.
The man in the yellow hat smiled as 
he looked at George, who was 
standing beside the car. The grassy 
hills and trees in the background 
added to the beauty of the scene.
Figure 9: Multimodal long story generation results of SEED-Story.
11

Dense
a&en'on
Window
a&en'on
Attention
sink
Mul'modal
A&n. sink
â€¦
â€¦
â€¦
â€¦
â€¦
â€¦
â€¦
â€¦
â€¦
â€¦
â€¦
â€¦
Inference
Length
No.1
No.2
No.18
No.19
No.35
No.39
No.44
â€¦
â€¦
â€¦
â€¦
Training
Length
â‰¤
Inference
Length
Training
Length
>
Figure 10: The visualization of generating long stories with different attention mechanisms. The
models adopting other mechanisms collapse when producing long sequence, while our model
employing multimodal attention sink consistently generates high-quality images.
Table 3: Quantitative evaluation of long story generation with various attention mechanisms. FID and
CLIP scores are calculated by comparing ground truth images with generated images. Inference time
and memory usage are calculated by generating 50 sequences multiple times for average.
Metrics
FID â†“
CLIP Score â†‘
Inference Time (s) â†“
Memory (GB) â†“
Dense Attn
119.72
0.705
569.67
37.99
Window Attn
334.90
0.598
450.64
30.81
Attn Sink
221.53
0.676
451.94
30.81
Multimodal Attn Sink
79.67
0.728
473.98
31.82
5.3
Long Story Generation
To verify our effectiveness in long story generation, we conduct an experiment visualizing a long
story using the SEED-Story model, but with varying attention mechanisms. We chunk our data into
stories of length of 10 considering the training efficiency. We set the window size as the same as the
training length. Qualitative results depict in Figure 10 demonstrate that window attention quickly
collapses when the inference length exceeds the training length. Both dense attention and attention
sink approaches fare better, yet still fail to produce meaningful images as the inference sequence
lengthens. In contrast, the multimodal attention sink consistently produces high-quality images.
In terms of efficiency, the multimodal attention sink exhibits significant improvement over dense
attention, with only a modest increase in time and memory costs compared to window attention and
vanilla attention sink. These additional costs stem from retaining extra image tokens in the KV cache.
Quantitative results presented in Table 3 substantiate the above conclusion.
6
Conclusion
This work introduces SEED-Story, a pioneering approach that leverages a Multimodal Large Language
Model to generate multimodal long stories with rich narrative text and contextually relevant images.
We propose a multimodal attention sink mechanism to enable our model to generalize to generating
long sequences in an efficient manner. We further present a high-quality dataset named StoryStream
for training and benchmarking the task of multimodal story generation effectively.
12

References
[1] Aiello, E., Yu, L., Nie, Y., Aghajanyan, A., Oguz, B.: Jointly training large autoregressive
multimodal models. arXiv preprint arXiv:2309.15564 (2023)
[2] An, C., Gong, S., Zhong, M., Li, M., Zhang, J., Kong, L., Qiu, X.: L-eval: Instituting
standardized evaluation for long context language models (2023)
[3] Animaj: Animaj official website (2024), https://www.animaj.com/#pocoyo/, accessed:
2024-05-22
[4] Animaj: Rabbids invasion official youtube channel (2024), https://www.youtube.com/
@RabbidsInvasion, accessed: 2024-05-22
[5] Bai, J., Bai, S., Yang, S., Wang, S., Tan, S., Wang, P., Lin, J., Zhou, C., Zhou, J.: Qwen-vl: A
versatile vision-language model for understanding, localization, text reading, and beyond (2023)
[6] Bai, J., Bai, S., Yang, S., Wang, S., Tan, S., Wang, P., Lin, J., Zhou, C., Zhou, J.: Qwen-vl: A
versatile vision-language model for understanding, localization, text reading, and beyond (2023)
[7] Beltagy, I., Peters, M.E., Cohan, A.: Longformer: The long-document transformer. arXiv
preprint arXiv:2004.05150 (2020)
[8] Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.D., Dhariwal, P., Neelakantan, A.,
Shyam, P., Sastry, G., Askell, A., et al.: Language models are few-shot learners. In: Advances
in Neural Information Processing Systems. vol. 33, pp. 1877â€“1901 (2020)
[9] Chen, J., Yu, J., Ge, C., Yao, L., Xie, E., Wu, Y., Wang, Z., Kwok, J., Luo, P., Lu, H., Li, Z.:
Pixart-Î±: Fast training of diffusion transformer for photorealistic text-to-image synthesis (2023)
[10] Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung,
H.W., Sutton, C., Gehrmann, S., et al.: Palm: Scaling language modeling with pathways. arXiv
preprint arXiv:2204.02311 (2022)
[11] Dong, R., Han, C., Peng, Y., Qi, Z., Ge, Z., Yang, J., Zhao, L., Sun, J., Zhou, H., Wei,
H., et al.: Dreamllm: Synergistic multimodal comprehension and creation. arXiv preprint
arXiv:2309.11499 (2023)
[12] Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani,
M., Minderer, M., Heigold, G., Gelly, S., et al.: An image is worth 16x16 words: Transformers
for image recognition at scale. arXiv preprint arXiv:2010.11929 (2020)
[13] Ge, S., Zhang, Y., Liu, L., Zhang, M., Han, J., Gao, J.: Model tells you what to discard: Adaptive
kv cache compression for llms. arXiv preprint arXiv:2310.01801 (2023)
[14] Ge, Y., Ge, Y., Zeng, Z., Wang, X., Shan, Y.: Planting a seed of vision in large language model.
arXiv preprint arXiv:2307.08041 (2023)
[15] Ge, Y., Zhao, S., Zeng, Z., Ge, Y., Li, C., Wang, X., Shan, Y.: Making llama see and draw with
seed tokenizer (2023)
[16] Ge, Y., Zhao, S., Zhu, J., Ge, Y., Yi, K., Song, L., Li, C., Ding, X., Shan, Y.: Seed-x: Multimodal
models with unified multi-granularity comprehension and generation (2024)
[17] Gong, Y., Pang, Y., Cun, X., Xia, M., Chen, H., Wang, L., Zhang, Y., Wang, X., Shan, Y.,
Yang, Y.: Talecrafter: Interactive story visualization with multiple characters. arXiv preprint
arXiv:2305.18247 (2023)
[18] Gupta, T., Schwenk, D., Farhadi, A., Hoiem, D., Kembhavi, A.: Imagine this! scripts to
compositions to videos. In: Proceedings of the European conference on computer vision
(ECCV). pp. 598â€“613 (2018)
[19] Hu, E.J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., Chen, W.: Lora:
Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685 (2021)
13

[20] Huang, T.H., Ferraro, F., Mostafazadeh, N., Misra, I., Agrawal, A., Devlin, J., Girshick, R., He,
X., Kohli, P., Batra, D., et al.: Visual storytelling. In: Proceedings of the 2016 conference of
the North American chapter of the association for computational linguistics: Human language
technologies. pp. 1233â€“1239 (2016)
[21] Kilian, M., Beaumont, R., Mendelevitch, D., Kulal, S., Blattmann, A.: video2dataset: Easily turn
large sets of video urls to a video dataset. https://github.com/iejMac/video2dataset
(2023)
[22] LaurenÃ§on, H., Saulnier, L., Tronchon, L., Bekman, S., Singh, A., Lozhkov, A., Wang, T.,
Karamcheti, S., Rush, A., Kiela, D., et al.: Obelics: An open web-scale filtered dataset of
interleaved image-text documents. Advances in Neural Information Processing Systems 36
(2024)
[23] Li, J., Li, D., Savarese, S., Hoi, S.: Blip-2: Bootstrapping language-image pre-training with
frozen image encoders and large language models. In: Proceedings of the International Confer-
ence on Machine Learning (ICML) (2023)
[24] Li, Y., Zhang, Y., Wang, C., Zhong, Z., Chen, Y., Chu, R., Liu, S., Jia, J.: Mini-gemini: Mining
the potential of multi-modality vision language models. arXiv preprint arXiv:2403.18814 (2024)
[25] Li, Y., Gan, Z., Shen, Y., Liu, J., Cheng, Y., Wu, Y., Carin, L., Carlson, D., Gao, J.: Storygan: A
sequential conditional gan for story visualization. In: Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition. pp. 6329â€“6338 (2019)
[26] Lin, Z., Liu, C., Zhang, R., Gao, P., Qiu, L., Xiao, H., Qiu, H., Lin, C., Shao, W., Chen, K.,
et al.: Sphinx: The joint mixing of weights, tasks, and visual embeddings for multi-modal large
language models. arXiv preprint arXiv:2311.07575 (2023)
[27] Liu, C., Wu, H., Zhong, Y., Zhang, X., Xie, W.: Intelligent grimmâ€“open-ended visual storytelling
via latent diffusion models. arXiv preprint arXiv:2306.00973 (2023)
[28] Liu, H., Li, C., Li, Y., Lee, Y.J.: Improved baselines with visual instruction tuning. arXiv
preprint arXiv:2310.03744 (2023)
[29] Liu, H., Li, C., Li, Y., Lee, Y.J.: Improved baselines with visual instruction tuning. arXiv
preprint arXiv:2310.03744 (2023)
[30] Liu, H., Li, C., Wu, Q., Lee, Y.J.: Visual instruction tuning. arXiv preprint arXiv:2304.08485
(2023)
[31] Maharana, A., Bansal, M.: Integrating visuospatial, linguistic and commonsense structure into
story visualization. arXiv preprint arXiv:2110.10834 (2021)
[32] Maharana, A., Hannan, D., Bansal, M.: Improving generation and evaluation of visual stories
via semantic consistency. arXiv preprint arXiv:2105.10026 (2021)
[33] Maharana, A., Hannan, D., Bansal, M.: Storydall-e: Adapting pretrained text-to-image trans-
formers for story continuation. In: European Conference on Computer Vision. pp. 70â€“87.
Springer (2022)
[34] OpenAI: Gpt-4v: Optimizing language models for dialogue. https://www.openai.com/
chatgpt (2023)
[35] Pan, X., Qin, P., Li, Y., Xue, H., Chen, W.: Synthesizing coherent story with auto-regressive
latent diffusion models. In: Proceedings of the IEEE/CVF Winter Conference on Applications
of Computer Vision. pp. 2920â€“2930 (2024)
[36] Patashnik, O., Wu, Z., Shechtman, E., Cohen-Or, D., Lischinski, D.: Styleclip: Text-driven
manipulation of stylegan imagery. In: Proceedings of the IEEE/CVF International Conference
on Computer Vision (ICCV). pp. 2085â€“2094 (October 2021)
[37] PBS
Kids:
Curious
george
official
website
(2024),
https://pbskids.org/
curiousgeorge/, accessed: 2024-05-22
14

[38] PBS Kids: Curious george official youtube channel (2024), https://www.youtube.com/
@CuriousGeorge, accessed: 2024-05-22
[39] Peng, Z., Wang, W., Dong, L., Hao, Y., Huang, S., Ma, S., Wei, F.: Kosmos-2: Grounding
multimodal large language models to the world. arXiv preprint arXiv:2306.14824 (2023)
[40] Peng, Z., Wang, W., Dong, L., Hao, Y., Huang, S., Ma, S., Wei, F.: Kosmos-2: Grounding
multimodal large language models to the world. arXiv preprint arXiv:2306.14824 (2023)
[41] Podell, D., English, Z., Lacey, K., Blattmann, A., Dockhorn, T., MÃ¼ller, J., Penna, J., Rombach,
R.: Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint
arXiv:2307.01952 (2023)
[42] Rahman, T., Lee, H.Y., Ren, J., Tulyakov, S., Mahajan, S., Sigal, L.: Make-a-story: Visual
memory conditioned consistent story generation. In: Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition. pp. 2493â€“2502 (2023)
[43] Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution image synthesis
with latent diffusion models. In: Proceedings of the IEEE/CVF conference on computer vision
and pattern recognition. pp. 10684â€“10695 (2022)
[44] Shen, X., Elhoseiny, M.: Storygpt-v: Large language models as consistent story visualizers
(2023)
[45] Sun, Q., Cui, Y., Zhang, X., Zhang, F., Yu, Q., Luo, Z., Wang, Y., Rao, Y., Liu, J., Huang, T.,
Wang, X.: Generative multimodal models are in-context learners (2023)
[46] Sun, Q., Yu, Q., Cui, Y., Zhang, F., Zhang, X., Wang, Y., Gao, H., Liu, J., Huang, T., Wang, X.:
Generative pretraining in multimodality. arXiv preprint arXiv:2307.05222 (2023)
[47] Sun, Q., Yu, Q., Cui, Y., Zhang, F., Zhang, X., Wang, Y., Gao, H., Liu, J., Huang, T., Wang, X.:
Generative pretraining in multimodality (2023)
[48] Taori, R., Gulrajani, I., Zhang, T., Dubois, Y., Li, X., Guestrin, C., Liang, P., Hashimoto, T.B.:
Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/
stanford_alpaca (2023)
[49] Team, C.: Chameleon: Mixed-modal early-fusion foundation models. arXiv e-prints pp. arXivâ€“
2405 (2024)
[50] TheLandBeforeTime:
The
land
before
time
official
website
(2024),
https://
thelandbeforetime.org, accessed: 2024-05-22
[51] TheLandBeforeTime: The land before time official youtube channel (2024), https://www.
youtube.com/@TheLandBeforeTime, accessed: 2024-05-22
[52] Tian, C., Zhu, X., Xiong, Y., Wang, W., Chen, Z., Wang, W., Chen, Y., Lu, L., Lu, T., Zhou,
J., et al.: Mm-interleaved: Interleaved image-text generative modeling via multi-modal feature
synchronizer. arXiv preprint arXiv:2401.10208 (2024)
[53] Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.A., Lacroix, T., RoziÃ¨re, B.,
Goyal, N., Hambro, E., Azhar, F., et al.: Llama: Open and efficient foundation language models.
arXiv preprint arXiv:2302.13971 (2023)
[54] Wang, L., Yang, S., Liu, S., cong Chen, Y.: Not all steps are created equal: Selective diffusion
distillation for image manipulation (2023)
[55] Wu, S., Fei, H., Qu, L., Ji, W., Chua, T.S.: Next-gpt: Any-to-any multimodal llm. arXiv preprint
arXiv:2309.05519 (2023)
[56] Xiao, G., Tian, Y., Chen, B., Han, S., Lewis, M.: Efficient streaming language models with
attention sinks. arXiv preprint arXiv:2309.17453 (2023)
15

[57] Yu, L., Shi, B., Pasunuru, R., Muller, B., Golovneva, O., Wang, T., Babu, A., Tang, B., Karrer,
B., Sheynin, S., et al.: Scaling autoregressive multi-modal models: Pretraining and instruction
tuning. arXiv preprint arXiv:2309.02591 (2023)
[58] Zhang, P., Dong, X., Wang, B., Cao, Y., Xu, C., Ouyang, L., Zhao, Z., Ding, S., Zhang, S.,
Duan, H., Yan, H., et al.: Internlm-xcomposer: A vision-language large model for advanced
text-image comprehension and composition. arXiv preprint arXiv:2309.15112 (2023)
[59] Zheng, L., Chiang, W.L., Sheng, Y., Zhuang, S., Wu, Z., Zhuang, Y., Lin, Z., Li, Z., Li, D.,
Xing, E.P., Zhang, H., Gonzalez, J.E., Stoica, I.: Judging llm-as-a-judge with mt-bench and
chatbot arena (2023)
[60] Zhu, D., Chen, J., Shen, X., Li, X., Elhoseiny, M.: Minigpt-4: Enhancing vision-language
understanding with advanced large language models. arXiv preprint arXiv:2304.10592 (2023)
[61] Zhu, J., Ding, X., Ge, Y., Ge, Y., Zhao, S., Zhao, H., Wang, X., Shan, Y.: Vl-gpt: A generative
pre-trained transformer for vision and language understanding and generation. arXiv preprint
arXiv:2312.09251 (2023)
16

Appendix
A
Implementation Details
A.1
Visual Tokenization and De-tokenization
For visual tokenization, we use Qwen-VL pre-trained ViT-G. We first resize the image to 448x448
images and then use ViT to produce its feature of length256 with 4096 dimension. (shape: [256,
4096]). Inside the MLLM, we use a Q-Former architecture to process the image embedding. It takse
the ViT image feature as key and Value, and conduct attention with its learnable queries. The length
of learnable queries are 64. For de-tokenization, we also use a Q-Former architecture to transform
the MLLM output to the shape of SD-XL condition embedding.
A.2
Instruction Tuning
Instruction tuning data is formatted as follows: for each story, we sample a random length and
compute losses on the last sequence (highlighted in red text). The sequence format is structured as:
<bos>[start of the story.][User prompt:
][following sequence
1][following sequence 2][following sequence 3][following
sequence 4] ...
[target sequence]<eos>
For our language model (LLM), we utilize the LLAMA2-7B pre-trained model and finetune it
using LoRA, supported by the peft library. The hyperparameter r is set to 6, and lora_alpha
is set to 32.
The modules optimized include the q_projection_layer, v_projection_layer,
k_projection_layer, o_projection_layer, gate_projection_layer, down_projection_layer,
and up_projection_layer. We employ a learning rate of 1 Ã— 10âˆ’4 to finetune this model on
our dataset across approximately 6 epochs, utilizing 8 NVIDIA-A800 GPUs.
A.3
De-tokenizer Adaptation
In this stage we fully finetune the SD-XL model. The data format is as the same as instruction
tuning, but we fix all MLLM params and optimize only the SD-XL. It takes the MLLM output and
is asked to produce image correspond to the ground truth. The SD-XL model was trained using 4
NVIDIA-A800 GPUs. A learning rate of 1 Ã— 10âˆ’4 was chosen to facilitate gradual weight updates,
ensuring stable convergence, while a weight decay of 0.03 was applied for regularization to prevent
overfitting. Training was performed using mixed precision (bf16), which significantly reduced
memory usage and accelerated the training process without compromising the modelâ€™s accuracy.
The model underwent three training epochs, balancing the learning of complex patterns against
computational resource use, optimized for large-scale datasets and sophisticated model architectures.
B
Attention Map Visualization
In this section, we present additional visualizations of attention maps. These maps are derived from
various model runs, including varying data lengths, attention heads, and layers. The visualizations
consistently reveal a pattern of attention focused on â€™0â€™ tokens, punctuation, tokens adjacent to
Begin-of-Image (BoI), and tokens adjacent to End-of-Image (EoI).
C
Data Usage and License
C.1
Curious George
Curious George is an animated series featuring George, a curious monkey whose adventures teach
preschoolers about math, science, and engineering. Guided by The Man with the Yellow Hat,
George explores the world through problem-solving and experimentation, making it a delightful and
educational experience for young viewers.
Curious George is released on PBS KIDS [37, 38], a not-for-profit institution. It is a production of
Imagine, WGBH and Universal. Curious George and related characters, created by Margret and H.A.
17

Figure 11: Visualization of attention maps from various model runs, showcasing attention patterns
across different data lengths, attention heads, and model layers. Notably, the maps highlight consistent
focus on â€™0â€™ tokens, punctuation, tokens adjacent to Begin-of-Image (BoI), and tokens adjacent to
End-of-Image (EoI).
Rey, are copyrighted and trademarked by Houghton Mifflin Harcourt and used under license. Licensed
by Universal Studios Licensing LLC. Television Series: Â©2024 Universal Studios. The terms of use
of them are provided in https://www.pbs.org/about/about-pbs/terms-of-use/.
Our usage fully comply with the terms of use. 1) Personal Uses Permitted: My project is non-
commercial and educational, which aligns with personal uses as outlined by PBS. we are not using
the information for commercial purposes or exploiting it in a manner inconsistent with PBS rules.
The use is strictly for educational and research purposes within an academic setting. 2) Userâ€™s
Obligation to Abide By Applicable Law: We will ensure all research activities comply with local
laws, particularly those relating to copyright and intellectual property rights. Our use will not involve
unauthorized reproduction, distribution, or exhibition that violates Intellectual Property Laws. All
data are for research only. 3) Content of Information: We will responsibly use the "Curious George"
materials, ensuring that all content used in our research is accurately cited and acknowledged. Any
PBS content incorporated into your project will be clearly attributed to PBS.
C.2
Rabbids Invasion
â€œRabbids Invasionâ€ is a French-American computer-animated TV series that breathes life into the
zany antics of Ubisoftâ€™s popular Rabbids video game characters. Created by Jean-Louis Momus and
featuring the voice of Damien Laquet, the show is a dynamic blend of humor and adventure tailored
for a family audience. Since its debut on August 3, 2013, on France 3, the series has enjoyed multiple
seasons and a global reach. The Rabbids are mischievous rabbit-like creatures whose escapades lead
them into all sorts of unpredictable and hilarious situations, making â€œRabbids Invasionâ€ a delight for
18

both kids and adults alike. Thanks to their release, we derive some subsets from the cartoon series
Rabbids Invasion [4, 3].
C.3
The Land Before Time
The Land Before Time, an iconic animated film series created by Judy Freudberg and Tony Geiss and
distributed by Universal Pictures, debuted in 1988 with significant contributions from Don Bluth,
George Lucas, and Steven Spielberg. This franchise, consisting of an initial film followed by 13
sequels, a TV series, video games, and extensive merchandising, explores the adventures of five
young dinosaurs who learn key life lessons about friendship and teamwork through their prehistoric
trials. Despite the absence of the original creators in the sequels, the series has continued to captivate
audiences, emphasizing themes of community and perseverance across its extensive narrative arc.
Thanks to their release, we derive some subsets from their websites [51, 50].
C.4
Appreciation
Leveraging the data derived from "Curious George," "Rabbids Invasion," and "The Land Before
Time," we have significantly advanced the capabilities of our story generation models. This progress
has direct and impactful implications for childrenâ€™s education by enhancing their imaginative faculties
and fostering a keen interest in learning. By integrating elements from these animated series into our
models, we not only engage young minds but also deepen their affection for animated storytelling.
Consequently, this not only meets but also amplifies educational objectives, such as improving literacy
and cognitive skills through enjoyable and interactive content. The successful application of data
from these beloved animations in our research exemplifies how academic pursuits can harmoniously
blend with educational entertainment, ultimately delivering multifaceted benefits that extend well
beyond conventional learning environments.
Lastly, we extend our profound appreciation to the creators and maintainers of "Curious George,"
"Rabbids Invasion," and "The Land Before Time," each a rich and vibrant resource that has signifi-
cantly contributed to the scope and success of our research. The engaging narratives and characters
from these series, especially the ever-curious George, the mischievous Rabbids, and the adventurous
dinosaurs from The Land Before Time, have provided invaluable data that enhanced our narrative
generation models. This project benefited immensely from the educational and entertaining content
crafted with meticulous attention to detail, fostering imagination and learning in young audiences.
We acknowledge the pivotal role that these animated series have played in advancing academic
research aimed at educational technology. The commitment of the teams behind these beloved series
to fostering curiosity and learning is both inspiring and exemplary. We are immensely grateful for the
opportunity to incorporate such cherished resources into our scholarly work.
D
Story Visualization Comparison
In this section, we present additional visualization comparison of our SEED-Story and other story
visualization methods. SEED-Story shows better image consistency and higher qualit, as shown in
Figure 12.
E
Multimodal Story Generation Results
In this section, we present our multimodal story generation results of our SEED-Story. It keeps
produce story image and text with high quality. Please see Figure 13.
We also showcase the visualization result of our model on Rabbids Invasion and The Land Before
Time. Please see Figure 14 and 15.
19

F
Details about GPT-4V Evaluation
F.1
Comparative Evaluation
To evaluate the effectiveness of MM-interleaved and SEED-Story in multimodal story generation,
we initiate an experiment where each model produces a story of five segments, based on a common
starting image and text. The segment limit is set to five to accommodate the constraints of GPT-4V,
which can handle a maximum of ten images per input session. In total, we generate 180 stories for
assessment. For evaluation, we employ GPT-4 or GPT-4V to determine which model produces the
better story in each case, based on the framework established in L-Eval [2]. We calculate the win rate
for each model to determine its performance relative to its counterpart. The prompt we used is shown
below.
â€œPlease act as an impartial judge and evaluate the quality of the generation story
contents provided by two AI assistants. Your job is to evaluate which assistantâ€™s
generation is better. Your evaluation should consider {the style consistency of the
story images / the engagement of the story / the coherence of the generated
text and images}. Avoid any position biases and ensure that the order in which
the responses were presented does not influence your decision. Do not allow the
length of the responses to influence your evaluation. Do not favor certain names of
the assistants.Be as objective as possible. After providing your explanation, output
your final verdict by strictly following this format: â€œ[[A]]â€ if assistant A is better,
â€œ[[B]]â€ if assistant B is better, and â€œ[[C]]â€ for a tie.â€
F.2
Score Evaluation
We also provide a prompt for directly estimating the performance of the generated results without
comparing to others. The prompt we used is shown below. We present the direct estimation score is
shown in Table 4
â€œPlease act as an impartial judge and evaluate the quality of the generation story
contents provided by an AI assistant. Your job is to give a score out of 10. Your
evaluation should consider {the style consistency of the story images / the en-
gagement of the story / the coherence of the generated text and images}. Do
not allow the length of the responses to influence your evaluation. Be as objective
as possible. After providing your explanation, output your final score by strictly
following this format: â€œ[[score]]â€, such as â€œ[[7]]â€.â€
Table 4: GPT4 score evaluation results in 3 different aspects-style consistency, story engaging level,
and text-image coherence.
Style â†‘
Engagingâ†‘
Coherenceâ†‘
SEED-Story
8.61
6.27
8.24
G
Story Video
To showcase the capabilities of our multimodal generation model, we employ a video generation
technique to animate the images. We then synchronize these moving images with audio to create a
narrative video, which is available in our supplementary materials.
H
Broader Impacts
This project may potentially produce copyrighted content, particularly when used inappropriately or
without adherence to existing intellectual property laws. To mitigate this risk, we will implement a
rigorous compliance framework that respects the copyrights of third parties. This involves setting
strict usage licenses that align with the legal standards dictated by our data sources. Our aim is to
protect intellectual property rights while fostering innovation and ethical use of our technology. We
20

also commit to educating users on the importance of respecting intellectual property rights when
using our technology. This will be achieved through detailed user guidelines, training sessions, and
readily available support to help users understand and navigate the complexities of copyright laws.
By taking these measures, we aim not only to comply with legal standards but also to promote a
culture of respect for intellectual property within our user community, thereby contributing positively
to the broader digital ecosystem.
I
Limitations
Lack of Realistic Data Experimentation:
This limitation points to a potential gap in the validation
of the SEED-Story model under practical, real-world conditions. Without experiments using realistic
data, itâ€™s difficult to ascertain how the model would perform in scenarios that are not perfectly
controlled or that deviate from the training conditions. This can be crucial, especially in applications
like storytelling where the context and variability of real-world data play significant roles. A possible
solution would be to incorporate a broader range of test conditions, including noisy data or data from
"in-the-wild" storytelling scenarios, to evaluate the robustness and adaptability of the model.
Training on a Non-Diverse Dataset:
The second limitation is the restriction of the modelâ€™s training
to animation datasets which does not cover a large scale or diverse styles. This can severely limit the
modelâ€™s ability to generalize and produce outputs in styles that are not represented in the training
data. This is particularly limiting in creative tasks such as storytelling where the ability to adapt to
various artistic and narrative styles is crucial. To mitigate this, expanding the dataset to include a
wider array of styles, genres, and visual aesthetics could be beneficial.
21

a) The man looked to the side with a 
surprised expression. The background 
appeared to be the interior of a rounded 
structure with a window.
b) A small monkey looked up at a person 
wearing an ou<it. In the background, 
another person stood next to a large 
dinosaur skeleton inside what appeared to 
be a museum.
c) A dinosaur skeleton stood inside a 
museum, with a woman in a coat 
gesturing towards it. In the background, a 
small ï¬gure waved enthusiasAcally from 
behind a glass window.
d) An woman with short hair and a lab coat 
stood with hands on hips, smirking. The 
background showed a wall with two light 
switches.
SEED-Story
LDM
StoryGen
a) And indeed, it had! A small, brown 
monkey named George was sitting on a 
nearby tree branch, hugging his legs with 
a curious expression. 
b) To his surprise, two raccoons, with 
mouths open and startled expressions, 
were looking at George. He was only 
partially visible, with his red hat peeking 
out from the bush.
c) Suddenly, a character wearing a red and 
white shirt, blue pants, and a yellow cap 
appeared. He stood a red bike with a 
basket, in the lush green park with trees 
and a wooden fence.
d) The character stood next to his red and 
yellow bicycle in the green park. He raised 
a hand to wave at George.
a) Suddenly, the man in the yellow hat 
stopped to look at something oï¬€-screen. 
George, the curious monkey, also turned 
to look in the same direcAon.
b) The bellhop conAnued his conversaAon on 
the phone while another ï¬gure held 
wooden planks. The room was ï¬lled with 
a classic armchair, console table with a 
ï¬‚ower vase, and a framed painAng.
c) Suddenly, there was a spill! An eggs had 
fallen, and cracked eggs, yolk, and 
eggshells were scaIered.
d) George sat on the ï¬‚oor with the man in 
the yellow suit and the bellhop in the red 
coat, amidst scaIered pieces of paper and 
a broken object.
SEED-Story
LDM
StoryGen
SEED-Story
LDM
StoryGen
Figure 12: More story visualization comparison of SEED-Story and other story visualization methods.
22

USER: Here is the beginning of 
a story about George, Please 
continue it â€¦
1
2
3
4
6
7
8
9
5
11
12
13
14
10
The man led George into a nearby 
building, where they found a cozy room 
with a wooden bed adorned with flowers. 
A lamp beside the bed cast a warm glow, 
making George feel right at home.
Feeling tired from their adventure, 
George climbed onto the bed, 
snuggling under the blanket. The 
room was peaceful, and George was 
quickly drifting off to sleep.
Suddenly, George woke up in the 
middle of the night, looking around 
curiously. The room was quiet, and 
George felt a sense of mystery.
George lay back down, resting his 
head on the pillow. His eyes were wide 
with curiosity, wondering what new 
adventures the next day would bring.
George followed the man into a colorful 
hallway. The man was gesturing 
towards a closed door, and George was 
curious about what was behind it.
George stood in front of a door, 
ready to push it open and see what 
was behind it. He was excited about 
the new discoveries he might find.
Behind the door, George found a 
sleeping cat. He gently pulled the door 
closed, leaving the cat to sleep in peace.
The next morning, George woke up 
feeling cheerful. He sat up in bed, 
looking around the room with a 
smile, ready for another day of 
adventures.
George decided to take the cat for 
a walk. He held the cat by the 
collar, looking quite pleased with 
himself.
George held the cat in his arms, 
looking at the curious cat. He was 
enjoying this new experience of being 
a cat walkkeeper.
George decided to take the cat for a walk 
around the building. He held the cat in his 
arms, looking at the curious cat. They 
passed by a man in a coat, who smiled at the 
sight of them.
George decided to take the cat for 
another walk. He held the cat in 
his arms, looking at the wall, ready 
for another adventure.
George found a cream-colored cat 
with a collar. He gently stroked
the cat's head, glad to have found a 
new friend.
George sat on the bed, watching as 
the man in the yellow suit walked 
towards a door. He wondered where 
the man was going, and if their 
adventure would lead them there.
USER: Here is the beginning of 
a story about George, Please 
continue it â€¦
1
2
3
4
6
7
8
9
5
11
12
13
14
10
In a nearby park, a pigeon had found a 
new friend, George, the curious monkey. 
They were standing on a wooden ledge, 
peering over it. Nearby, a taller bird 
observed them with interest.
Meanwhile, a gray pigeon had found a 
new perch on a building's edge. It 
overlooked a small balcony, where a man 
was enjoying a morning cup of coffee, 
oblivious to the pigeon's presence.
Elsewhere, a blue-gray pigeon was 
pecking at seeds on the ground. A 
person nearby watched intently, their 
eyes following the bird's every move.
George, the small brown monkey, was standing 
next to a large bird on a ledge. The bird looked 
content, while George seemed curious. The 
background showed a building with windows, a 
perfect playground for a curious monkey.
Meanwhile, a cartoon bird was 
pecking at some food on the ground. 
A human hand reached towards the 
bird, offering a treat.
George was back on the ledge, this 
time looking puzzled. The 
background showed a building, 
perfect for his little puzzle.
In a playful mood, George mimicked a 
pigeon, pecking at a crumb-covered ledge. 
The pigeon watched him, amused. In the 
background, a building stood tall, perfect 
for their little playground.
George was having a great time, leaning 
on the bird's perch with a smile. The 
background showed a building, perfect 
for a little monkey adventure.
George sat on a ledge, looking 
puzzled at a crumby area. The 
background showed a building, 
perfect for his little thought bubble.
Feeling adventurous, George stood on a 
city sidewalk, waving with a happy 
expression. The background showed 
colorful buildings and a clear sky, perfect 
for his little adventure.
George stood on a city sidewalk, 
waving with one hand. The background 
showed colorful buildings and a clear 
sky, perfect for his little adventure.
George stood on a city sidewalk, looking 
puzzled. The background showed colorful 
buildings and a street with a parked car, 
perfect for his little thought bubble.
Suddenly, George noticed a small object on 
the floor. He stood on the ledge, looking at 
it with curiosity. The background showed a 
building, perfect for his little investigations.
Suddenly, George noticed a small 
object on the ground. He stood on his 
two legs, reaching out to pick it up. 
Nearby, a pigeon rested on a ledge.
Figure 13: Multimodal story generation results of SEED-Story.
23

1
2
3
4
1
2
3
4
USER: Here is the beginning of the story. 
Please help me visualize the followingâ€¦ 
USER: Here is the beginning of the story. 
Please help me visualize the followingâ€¦ 
Figure 14: Story visualization result on Rabbids Invasion data.
1
2
3
4
1
2
3
4
USER: Here is the beginning of the story. 
Please help me visualize the followingâ€¦ 
USER: Here is the beginning of the story. 
Please help me visualize the followingâ€¦ 
Figure 15: Story visualization result on The Land Before Time data.
24

