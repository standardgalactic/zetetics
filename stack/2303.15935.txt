When Brain-inspired AI Meets AGI
Lin Zhao ∗1, Lu Zhang ∗2, Zihao Wu1, Yuzhong Chen3, Haixing
Dai1, Xiaowei Yu2, Zhengliang Liu1, Tuo Zhang4, Xintao Hu4, Xi
Jiang3, Xiang Li5, Dajiang Zhu2, Dinggang Shen6,7,8, and
Tianming Liu †1
1School of Computing, The University of Georgia, Athens 30602, USA
2Department of Computer Science and Engineering, The University of Texas at
Arlington, Arlington 76019, USA
3MOE Key Laboratory for Neuroinformation, School of Life Science and Technology,
University of Electronic Science and Technology of China, Chengdu 611731, China
4School of Automation, Northwestern Polytechnical University, Xi’an 710072, China
5Department of Radiology, Massachusetts General Hospital and Harvard Medical
School, Boston 02115, USA
6School of Biomedical Engineering, ShanghaiTech University, Shanghai 201210,
China
7Shanghai United Imaging Intelligence Co., Ltd., Shanghai 200230, China
8Shanghai Clinical Research and Trial Center, Shanghai, 201210, China
Abstract
Artiﬁcial General Intelligence (AGI) has been a long-standing goal of
humanity, with the aim of creating machines capable of performing any
intellectual task that humans can do. To achieve this, AGI researchers
draw inspiration from the human brain and seek to replicate its principles
in intelligent machines. Brain-inspired artiﬁcial intelligence is a ﬁeld that
has emerged from this endeavor, combining insights from neuroscience,
psychology, and computer science to develop more eﬃcient and power-
ful AI systems. In this article, we provide a comprehensive overview of
brain-inspired AI from the perspective of AGI. We begin with the cur-
rent progress in brain-inspired AI and its extensive connection with AGI.
We then cover the important characteristics for both human intelligence
and AGI (e.g., scaling, multimodality, and reasoning). We discuss impor-
tant technologies toward achieving AGI in current AI systems, such as
in-context learning and prompt tuning. We also investigate the evolution
of AGI systems from both algorithmic and infrastructural perspectives.
Finally, we explore the limitations and future of AGI.
∗Co-ﬁrst authors
†Corresponding author: tianming.liu@gmail.com
1
arXiv:2303.15935v1  [cs.AI]  28 Mar 2023

1
Brain-inspired AI and AGI
The human brain is widely considered one of the most intricate and advanced
information-processing systems in the world. It comprises over 86 billion neu-
rons, each capable of forming up to 10,000 synapses with other neurons, resulting
in an exceptionally complex network of connections that allows for the prolifer-
ation of intelligence. Along with the physiological complexity, the human brain
exhibits a wide range of characteristics that contribute to its remarkable func-
tional capabilities. For example, it can integrate data from multiple sensory
modalities, such as vision, hearing, and touch, allowing it to form a coherent
perception of the world. The brain’s ability to perform parallel processing is
also essential for eﬃciently handling multiple information streams simultane-
ously. This is fulﬁlled via the connections and real-time communications among
diﬀerent brain regions, though the mechanism is not fully understood. Besides,
the brain is highly adaptable, capable of reorganizing its structure and function
in response to changing environments and experiences. This property, known as
neuroplasticity, enables the brain to learn and develop new skills throughout life.
The human brain is also notable for its high-level cognitive functions, such as
problem-solving, decision-making, creativity, and abstract reasoning, supported
by the prefrontal cortex, a brain region that is particularly well-developed in
humans.
Creating artiﬁcial general intelligence (AGI) system that has human-level
or even higher intelligence and is capable of performing a wide range of intel-
lectual tasks, such as reasoning, problem-solving, and creativity, is the pursuit
of humanity for centuries, which can date back to the mid-20th century. In
the 1940s, pioneers such as Alan Turing developed early ideas about computing
machines and the potential for them to simulate human thinking [1]. From then
on, seeking to replicate the principles of human intelligence in artiﬁcial sys-
tems has signiﬁcantly promoted the development of AGI and the corresponding
applications. These principles include the structure and function of neural net-
works, the plasticity of synaptic connections, the dynamics of neural activity,
and more. In 1943, McCulloch and Pitts proposed the very ﬁrst mathematical
model of an artiﬁcial neuron [2], also known as McCulloch-Pitts (MCP) Neuron.
Inspired by the Hebbian theory of synaptic plasticity, Frank Rosenblatt came
up with the perceptron, a major improvement over the MCP neuron model [3],
and showed that by relaxing some of the MCP’s rules artiﬁcial neurons could
actually learn from data. However, the research of artiﬁcial neural network had
stagnated until the backprogation was proposed by Werbos in 1975 [4]. Back-
propagation was inspired by the way the brain modiﬁes the strengths of connec-
tions between neurons to learn and improve its performance through synaptic
plasticity. Backpropagation attempts to mimic this process by adjusting the
weights (synaptic strengths) between neurons in an artiﬁcial neural network.
Despite this early proposal, backpropagation did not gain widespread attention
until the 1980s, when researchers such as David Rumelhart, Geoﬀrey Hinton,
and Ronald Williams published papers that demonstrated the eﬀectiveness of
backpropagation for training neural networks [5].
2

Convolutional neural networks (CNNs) is one of the most widely used and
eﬀective types of neural networks for processing visual information [6]. CNNs
are also inspired by the hierarchical organization of the visual cortex in the
brain, which can be traced back to the work of David Hubel and Torsten Wiesel
in 1960s[7]. In the visual cortex, neurons are arranged in layers, with each layer
processing visual information in a hierarchical manner.
The input from the
retina is ﬁrst processed by a layer of simple cells that detect edges and orienta-
tions, and then passed on to more complex cells that recognize more complex
features such as shapes and textures. Their work provided insights into how the
visual system processes information and inspired the development of CNNs that
could mimic this hierarchical processing process. Attention mechanisms in arti-
ﬁcial neural network are also inspired by the way human brain selectively attend
to certain aspects of sensory input or cognitive processes, allowing us to focus
on important information while ﬁltering out irrelevant details [8]. Attention has
been studied in the ﬁelds of psychology and neuroscience for many years, and
its application to artiﬁcial intelligence signiﬁcantly advances our steps towards
AGI. The “Transformer” model, based on self-attention mechanism, has become
the basis for many state-of-the-art artiﬁcial neural networks such as BERT [9]
and GPT [10]. By adapting self-attention mechanisms into image processing,
Vision Transformer (ViT) [11] model demonstrated state-of-the-art performance
in various computer vision (CV) tasks by representing the image as a sequence
of patches.
Recently, more and more evidence suggests that artiﬁcial neural networks
(ANNs) and biological neural networks (BNNs) may share common principles
in optimizing network architecture. For example, the property of small-world
in brain structural and functional networks has been extensively studied in the
literature [12, 13, 14].
In a recent study, neural networks based on Watts-
Strogatz (WS) random graphs with small-world properties have demonstrated
competitive performances compared to hand-designed and NAS-optimized mod-
els [15]. Additionally, post-hoc analysis has shown that the graph structure of
top-performing ANNs, such as CNNs and MLP, is similar to that of real BNNs,
such as the network in the macaque cortex [16]. Chen et al. proposed a uni-
ﬁed and biologically-plausible relational graph representation of ViT models,
ﬁnding that model performance was closely related to graph measures and the
ViT has high similarity with real BNNs [17].
Zhao et al.
synchronized the
activation of ANNs and CNNs and found that CNNs with higher performance
are similar to BNNs in terms of visual representation activation [18]. Liu et al.
coupling the artiﬁcial neurons in BERT model with the biological neurons in the
human brain, and found that artiﬁcial neurons can carry mearningful linguis-
tic/semantic information and anchor to their biological neurons signatures with
interpretablility in a neurolinguistic context. Zhou et al. treated each hidden
dimension in Wav2Vec2.0 as a artiﬁcial neuron and connected them with bio-
logical counterparts in the human brain, suggesting a close relationship between
two domain in terms of neurolinguistic information.
Following this trend, there has been growing interest in developing brain-
inspired artiﬁcial intelligence by drawing inspiration from some human brain
3

prior knowledge, such as the organization of brain structure and function. For
example, Huang et al.
[19] proposed a brain inspired adversarial visual at-
tention network (BI-AVAN) which imitates the biased competition process in
human visual system to decode the human visual attention. Inspired by the
core-periphery organization of human brain, Yu et al. proposed a core-periphery
principle guided vision transformer model (CP-ViT) for image recognition with
improved performances and interpretability. Similarly, Zhao et al. implemented
the core-periphery principle in the design of network wiring patterns and the
sparsiﬁcation of the convolution operation. The proposed core-periphery prin-
ciple guided CNNs (CP-CNNs) demonstrate the eﬀectiveness and superiority
compared to CNNs and ViT-based methods. Another groups of studies opted
for spiking neural networks (SNNs) [20] which closely emulate the behavior of
biological neurons in the brain. For example, SNNs was employed to map and
understand the spatio-temploral brain data [21], decode and understand muscle
activity from electroencephalography signals [22], and brain-machine interfaces
[23, 24].
Brain-inspired AI has also contributed to the development of hardware ar-
chitectures that mimic the structure and function of the brain. Neuromorphic
computing, a ﬁeld of study that aims to design computer hardware that emu-
lates the biological neurons and synapses, has also gained increasing attention
in recent years [25, 26, 27, 28, 29, 30]. Neuromorphic chips are designed to
process information in a parallel and distributed way, similar to the way the
brain works, which can lead to signiﬁcant improvements in eﬃciency and speed
compared to traditional computing architectures. Some of the neuromorphic
chips, such as IBM’s TrueNorth chip [31] and Intel’s Loihi chip [28], use spik-
ing neural networks to process information in a way that is closer to how the
brain processes information. These chips have been used for a wide range of
applications, including image and speech recognition [32], robotics [33], and
autonomous vehicles [34].
The advancement of brain-inspired hardware also
provides a potential for signiﬁcant breakthroughs in the ﬁeld of AGI by paving
the road for generalized hardware platforms [30].
Overall, brain-inspired AI plays a crucial role in the development of AGI
(Figure 1). By drawing inspiration from the human brain, researchers can cre-
ate algorithms and architectures that are better suited to handle complex, real-
world problems that require a high degree of ﬂexibility and adaptability. This is
especially important for AGI, which aims to develop machines that can perform
a wide range of tasks, learn from experience, and generalize their knowledge
to new situations. The human brain is one of the most complex information-
processing system known to us, and it has evolved over millions of years to
be highly eﬃcient and eﬀective in handling complex tasks. By studying the
brain and developing AI systems that mimic its architecture and function, re-
searchers can create AGI that is more sophisticated and adaptable, bringing
us closer to the ultimate goal of creating machines that can match or surpass
human intelligence. In turn, AGI also has the potential to beneﬁt human intel-
ligence and deepen our understanding of intelligence. As we continue to study
and understand both human intelligence and AGI, these two systems will be-
4

Figure 1: The development of artiﬁcial general intelligence (AGI) has been
greatly inspired by the study of human intelligence (HI). In turn, AGI has the
potential to beneﬁt human intelligence. Current language models such as Chat-
GPT and GPT-4 use reinforcement learning with human feedback (RLHF) to
align their behavior with human values. As we continue to study and understand
both human intelligence and AGI, these two systems will become increasingly
intertwined, enhancing and supporting each other in new and exciting ways.
come increasingly intertwined, enhancing and supporting each other in new and
exciting ways.
2
Characteristics of AGI
2.1
Scale
The scale of brains varies greatly across diﬀerent animal species, ranging from
a few thousand neurons in the case of simple invertebrates such as nematode
worms, to over 86 billion neurons in the case of humans.
For example, the
brain of a fruit ﬂy contains around 100,000 neurons, and the brain of a mouse
contains around 70 million neurons. For primates, macaque monkey’s brain has
around 1.3 billion neurons [35] while the chimpanzee’s brain have around 6.2
billion neurons [36]. Compared to other animals, the human brain is the most
complex and sophisticated biological structure known to science, containing over
86 billion neurons. The scale of the brain, i.e., the number of neurons, is often
correlated with the cognitive abilities of the animal and considered as a factor
of intelligence [36, 37]. The size and complexity of the brain regions associated
with speciﬁc cognitive functions, such as language or memory, are often directly
related to the number of neurons they contain [38, 39, 40].
The relationship between the number of neurons and cognitive abilities is
also relevant for large language models (LLMs) such as GPT-2 and GPT-3.
While GPT-2 has 1.5 billion parameters and was trained on 40 gigabytes of text
data, GPT-3 has 175 billion parameters and was trained on 570 gigabytes of
text data. This signiﬁcant increase in the number of parameters has enabled
GPT-3 to outperform GPT-2 on a range of language tasks, demonstrating an
5

increase in its ability to perform complex language tasks. In fact, GPT-3 has
been shown to achieve human-like performance on several natural language pro-
cessing benchmarks, such as question-answering, language translation, and text
completion tasks.
Its size and capacity for natural language processing has
made it a powerful tool for various applications, including chatbots, content
generation, and language translation.
This trend is similar to the way larger brains are associated with more com-
plex cognitive functions in animals. As LLMs continue to scale up, it is expected
that they will become even more capable of few-shot learning of new skills from
a small number of training examples, similar to how animals with larger brains
have more sophisticated cognitive abilities. This correlation suggests that scale
may be a crucial factor in achieving artiﬁcial general intelligence. However, it’s
worth noting that the number of parameters alone does not determine the intel-
ligence of an LLM. The quality of the training data, the training process, and
the architecture of the model also play important roles in its performance. As
researchers continue to improve LLMs and explore new ways to advance AGI, it
will be interesting to see how the relationship between the number of parameters
and cognitive abilities evolves.
Figure 2: The concept “dog” represented in diﬀerent modalities.
2.2
Multimodality
The human brain’s ability to process and integrate information from multiple
sensory modalities simultaneously is a remarkable feat.
This feature allows
individuals to understand the world around them through various sources of
information, such as sight, sound, touch, taste, and smell. Moreover, processing
multimodal information enables people to make more accurate and comprehen-
sive assessments of their environment and communicate eﬀectively with others.
Consequently, successful learning from multiple modalities can enhance human
cognitive abilities. As we strive to create advanced artiﬁcial general intelligence
(AGI) systems that surpass human intelligence, it is crucial that they are capa-
ble of acquiring and ingesting knowledge from various sources and modalities to
solve tasks that involve any modality. For instance, an AGI should be able to
utilize knowledge learned from images and the knowledge base to answer natural
language questions, as well as use knowledge learned from text to perform visual
tasks. Ultimately, all modalities intersect through universal concepts, such as
the concept that a dog is a dog, regardless of how it is represented in diﬀerent
modalities (Figure 2).
To build multimodal AI systems, a promising approach is to incorporate
6

training signals from multiple modalities into LLMs. This requires aligning the
internal representations across diﬀerent modalities, enabling the AI system to
integrate knowledge seamlessly. For instance, when an AI system receives an
image and related text, it must associate the same object or concept between
the modalities. Suppose the AI sees a picture of a car with text referring to its
wheels. In that case, the AI needs to attend to the part of the image with the car
wheels when processing the text mentioning them. The AI must “understand”
that the image of the car wheels and the text referring to them describe the
same object across diﬀerent modalities.
In recent years, multimodal AI systems have been experimenting with align-
ing text/NLP and vision into an embedding space to facilitate multimodal
decision-making. Cross-modal alignment is essential for various tasks, includ-
ing text-to-image and image-to-text generation, visual question answering, and
video-language modeling. In the following section, we provide a brief overview
of these prevalent workloads and the corresponding state-of-the-art models.
2.2.1
Text-to-image and image-to-text generation
CLIP [41], DALL-E [42], and their successor GLIDE [43], VisualGPT[44] and
Diﬀusion[45] are some of the most well-known models that tackle image descrip-
tions (image-to-text generation) and text-to-image generation tasks. CLIP is
a pre-training method that trains separate image and text encoders and learns
to predict which images in a dataset are associated with various descriptions.
Notably, similar to the “Halle Berry” neuron in humans [46], CLIP has been
found to have multimodal neurons that activate when exposed to both the clas-
siﬁer label text and the corresponding image [47], indicating a fused multimodal
representation. DALL-E, on the other hand, is a variant of GPT-3 with 13 bil-
lion parameters that takes text as input and generates a sequence of images
to match the input text. The generated images are then ranked using CLIP.
GLIDE, an evolution of DALL-E, still uses CLIP to rank the generated images,
but the image generation is accomplished using a diﬀusion model [45]. Sta-
ble Diﬀusion is also based on diﬀusion models while it operates on the latent
space of powerful pre-trained autoencoders and thus in limited computational
resources while maintaining their quality and ﬂexibility. The VisualGPT is the
evolution of GPT-2 from a single language model to a multimodal model with
a self-resurrecting activation unit to produce sparse activations that prevent
accidental overwriting of linguistic knowledge.
2.2.2
Visual question answering
Visual question answering is a critical application of multimodal learning that
requires a model to correctly respond to a text-based question based on an im-
age. The VQA dataset [48] presents this task, and teams at Microsoft Research
have developed some of the leading approaches for it. One of these approaches
is METER [49], a general framework for training performant end-to-end vision-
language transformers using a variety of sub-architectures for the vision en-
7

coder, text encoder, multimodal fusion, and decoder modules. This ﬂexibility
allows METER to achieve state-of-the-art performance across a range of tasks.
Another promising approach is the Uniﬁed Vision-Language pretrained Model
(VLMo) [50], which uses a modular transformer network to jointly learn a dual
encoder and a fusion encoder. Each block in the network contains a pool of
modality-speciﬁc experts and a shared self-attention layer, oﬀering signiﬁcant
ﬂexibility for ﬁne-tuning.
This architecture has shown impressive results on
several benchmark datasets.
2.2.3
Video-language modeling
Traditionally, AI systems have struggled with video-based tasks due to the high
computational resources required. However, this is beginning to change, thanks
to eﬀorts in the domain of video-language modeling and other video-related mul-
timodal tasks, such as Microsoft’s Project Florence-VL. In mid-2021, Project
Florence-VL introduced ClipBERT [51], a combination of a CNN and a trans-
former model that operates on sparsely sampled frames. It is optimized in an
end-to-end fashion to solve popular video-language tasks. Subsequent evolu-
tions of ClipBERT, such as VIOLET [52] and SwinBERT[53], have introduced
Masked Visual-token Modeling and Sparse Attention to improve the state-of-
the-art in video question answering, video retrieval, and video captioning. While
each of these models has unique features, they all utilize a transformer-based ar-
chitecture. Typically, this architecture is coupled with parallel learning modules
to extract data from various modalities and unify them into a single multimodal
representation.
Recently, the emergence of GPT-4 has taken multimodal research to a new
level. According to the latest oﬃcial research paper [54], GPT-4 not only ex-
hibits high proﬁciency in various domains, including literature, medicine, law,
mathematics, physical sciences, and programming but also ﬂuently combines
skills and concepts from multiple domains, demonstrating impressive compre-
hension of complex ideas. Furthermore, GPT-4’s performance in all of these
tasks is remarkably close to human-level performance and often surpasses prior
models such as ChatGPT. Given the breadth and depth of GPT-4’s capabilities,
it could be viewed as an early version (albeit incomplete) of an artiﬁcial general
intelligence (AGI) system.
It is important to note that, in contrast to single-modality LLMs, multimodal
LLMs exhibit superior performance not only in cross-modal tasks but also in
single-modality tasks. For instance, the integration of multimodality in GPT-
4 results in better performance in textual tasks compared to ChatGPT [54].
This aligns with the way humans perceive the world through multiple sensory
modalities.
2.3
Alignment
While some LLMs like BERT [9], GPT [10], GPT-2[55], GPT-3 [56], and Text-
to-Text Transfer Transformer (T5) [57] have achieved remarkable success in
8

speciﬁc tasks, they still fall short of true Artiﬁcial General Intelligence (AGI)
due to their tendency to exhibit unintended behaviors. For example, they might
generate biased or toxic text, make up facts, or fail to follow user instructions.
The main reason behind these issues is the misalignment between the language
modeling objective used for many recent LLMs and the objective of safely and
helpfully following user instructions. Therefore, while these models have made
signiﬁcant advancements, they are not yet capable of emulating human-like
reasoning, decision-making, and understanding.
To achieve AGI, it’s crucial
to align language models with the user’s intention. This alignment will enable
LLMs to function safely and helpfully, making them more reliable for complex
tasks that require nuanced decision-making and understanding. To achieve this,
there is a need for better algorithms that steer agents towards human values
while fostering cross-disciplinary collaborations to clarify what “human values”
mean.
Recent developments in large language models (LLMs), such as Sparrow [58],
InstructGPT [59], ChatGPT, and GPT-4, have addressed the issue of alignment
with human instructions using reinforcement learning from human feedback
(RLHF). Reinforcement learning is a type of machine learning where the model
learns to make decisions based on feedback it receives in the form of rewards.
The goal of the model is to maximize its total reward over time. RLHF uses
human preferences as a reward signal to ﬁne-tune the LLMs and enable LLMs
to learn and improve from human feedback, which tries to predict what answers
the humans will react positively to and helps in reducing unintended behaviors
and increasing their reliability for complex tasks. Since the model learns from
humans in real-time, it becomes better and better at predicting. At the end
of the training process, AI systems start to imitate humans. RLHF has shown
promising results and is a signiﬁcant step towards developing LLMs that can
function safely and helpfully, aligning with human values and intentions.
2.4
Reasoning
Reasoning plays a crucial role in human intelligence and is essential for decision-
making, problem-solving, and critical thinking. Previous study [60] have ex-
plored factors that inﬂuence intelligence levels by comparing diﬀerent attributes
of brains across various mammalian species. The ﬁndings suggest that cogni-
tive abilities are primarily centered on the absolute number of neurons. Among
mammals, the human brain has the highest number of neurons, which gives it
superior reasoning and intelligence abilities compared to other species. Recently,
a similar phenomenon has also emerged in the LLMs. It has been observed that
the LLMs exhibit emergent behaviors, such as the ability to reason, when they
reach a certain size [61]. To enhance LLM’s reasoning abilities, two major types
of approaches have been developed. The ﬁrst type, known as Prompt-based
methods [62, 63, 64, 65], is more widely researched and involves leveraging ap-
propriate prompts to better stimulate the reasoning abilities that LLMs already
possess. The second type of approaches involves introducing program code into
the pre-training process [66, 67, 65], where it is trained alongside text to further
9

enhance the LLM’s reasoning ability. The two approaches have fundamentally
diﬀerent directions: using code to enhance LLM reasoning abilities represents
a strategy of directly enhancing LLM reasoning abilities by increasing the di-
versity of training data; while the Prompt-based approach does not promote
LLM’s own reasoning abilities, but rather provides a technical method for LLM
to better demonstrate this ability during problem-solving.
Currently, most existing works in the ﬁeld of large language models (LLM)
reasoning adopt prompt-based methods, which can roughly be divided into three
technical routes. The ﬁrst approach is the zero-shot Chain of Thought (CoT),
proposed by [62]. This method is simple and eﬀective, involving two stages. In
the ﬁrst stage, a prompt phrase, “Let’s think step by step”, is added to the ques-
tion, and the LLM outputs a speciﬁc reasoning process. In the second stage, the
reasoning process output by the LLM in the ﬁrst stage is concatenated with the
question, and the prompt phrase, “Therefore, the answer (arabic numerals) is”,
is added to obtain the answer. Such a simple operation can signiﬁcantly increase
the eﬀectiveness of the LLM in various reasoning tasks. For example, Zero-
shot-CoT achieves score gains from 10.4% to 40.7% on arithmetic benchmark
GSM8K [62]. The second approach is the Few-Shot CoT [63], which is currently
the main direction of LLM reasoning research. The main idea of Few-Shot CoT
is straightforward: to teach the LLM model to learn reasoning, provide some
manually written reasoning examples, and clearly explain the speciﬁc reasoning
steps one by one before obtaining the ﬁnal answer in the examples. These man-
ually written detailed reasoning processes are referred to as Chain of Thought
Prompting. The concept of CoT was ﬁrst explicitly proposed by [63]. Although
the method is simple, the reasoning ability of the LLM model has been greatly
improved after applying CoT. The accuracy of the GSM8K mathematical rea-
soning dataset increased to around 60.1% [63]. Based on CoT, subsequent works
[64, 65] have expanded from a single Prompt question to multiple Prompt ques-
tions, checked the correctness of intermediate reasoning steps, and improved the
accuracy of multiple outputs using weighted voting. These improvements have
continuously raised the accuracy of the GSM8K test set to around 83%. The
third approach is “Least-to-most prompting” [68]. The core idea is to decom-
pose a complex reasoning problem into several easier-to-solve subproblems that
can be solved sequentially, whereby solving a given subproblem is facilitated by
the answers to previously solved subproblems. After solving each subproblem,
we can derive the answer to the original problem from the answers to the sub-
problems. This idea is highly consistent with the divide-and-conquer algorithm
that humans use to solve complex problems. As our understanding of the brain
and LLMs continues to deepen, it will be interesting to investigate whether these
two network systems share an optimal structure.
3
Important Technology
Language models, such as LLMs, rely on several crucial techniques include zero-
shot prompting, few-shot prompting, in-context learning, and instruct. The un-
10

derlying expectation of these techniques is that AI systems can learn new tasks
rapidly by leveraging what they have learned in the past, much like humans do.
Through the use of these techniques, language models can be trained to per-
form a wide range of tasks, from generating coherent text to answering complex
questions, with greater accuracy and eﬃciency. Ultimately, these advancements
bring us closer to realizing the potential of AI to assist and augment human
intelligence in new and exciting ways. Of these techniques, instruct serves as
the interface utilized by ChatGPT, where users provide task descriptions in
natural language, such as “Translate this sentence from Chinese to English.”
Interestingly, zero-shot prompting was initially the term used for Instruct. Dur-
ing the early stages of zero-shot prompting, users faced diﬃculty expressing
tasks clearly, leading them to try various wordings and sentences repeatedly
to achieve optimal phrasing. Presently, Instruct involves providing a command
statement to facilitate LLM understanding. In Context Learning and few-shot
prompting share similar purposes, which involve presenting LLMs with a few
examples as templates to solve new problems. Accordingly, this article places
emphasis on introducing In Context Learning and Instruct.
3.1
In-Context Learning
The foremost capability of the human brain resides in its robust learning capac-
ity, enabling the execution of cognitive, computational, expressive, and motor
functions predicated on linguistic or visual prompts, often with minimal or no
examples. This attribute is central to the attainment of human-level Artiﬁcial
General Intelligence (AGI). Recent advancements in large-scale AGI models,
speciﬁcally GPT-4, have demonstrated such a promising capability. They are
pretrained on massive multimodal datasets, capturing a wide range of tasks
and knowledge while understanding diverse prompts from both linguistic and
visual domains.
This enables in context learning akin to the human brain’s
working mode, and driving AGI into real-world applications. In fact, follow-
ing the emergence of large-scale models like GPT-4 and Midjourney V5, many
industries, such as text processing and illustration, have witnessed disruptive
scenarios where AGI liberates human labor. These models leverage prior knowl-
edge acquired from pretraining across various tasks and context, allowing rapid
adaptation to novel tasks without the need for extensive labeled data for ﬁne-
tuning, which is a critical challenge in ﬁelds like medical[69] and robotics[70]
where labeled data is often limited or even unavailable.
In the context of AGI, in-context learning denotes the model’s capacity to
comprehend and execute new tasks by providing a limited number of input-
output pair examples[71] within prompts or merely a task description. Prompts
facilitate the model’s apprehension of the task’s structure and patterns, while
in context learning exhibit similarities to explicit ﬁne-tuning at the prediction,
representation, and attention behavior levels. This allows them to generalize to
and perform new tasks even better without further training or ﬁne-tuning[72]
and reduces the likelihood of overﬁtting downstream labeled training data.
Despite the absence of ﬁne-tuning requirements in these large-scale AGI
11

models, the trade-oﬀs include increased computational costs due to their mas-
sive parameter scale and the potential need for expert knowledge in formulat-
ing eﬀective prompts with examples during inference. Potential solutions en-
tail hardware advancements and the integration of more reﬁned domain-speciﬁc
knowledge during the pretraining phase.
3.2
Prompt and Instruction Tuning
Like human infants generally acquire various concepts about the world mostly by
observation, with very little direct intervention[73], the large-scale AGI models
also gain wide-ranging knowledge after initial large-scale unsupervised training
and have achieved remarkable generalization performance.
The prompt and
instruction tuning-based methods allow the pretrained models to achieve zero-
shot learning in numerous downstream applications[74].
The human brain is always an eﬃcient and orderly processor, providing tar-
geted feedback for the current task rather than speaking nonsense. In addition
to the brain’s innate pursuit of eﬃciency, moral and legal constraints ingrained
in human development also ensure that human interactions are orderly and ben-
eﬁcial. For AGI models to reach human-level performance, producing truthful
and harmless results based on instructions is an essential requirement.
Al-
though current large-scale AGI models have powerful generative capabilities, a
key question is whether these capabilities can be aligned with users’ intent. This
is important as it relates to whether the model can produce satisfactory results
for users, even in situations where tasks and prompts are unseen and unclear.
Additionally, as these models become more widely used, untruthful and toxic
outputs must be eﬀectively controlled.
InstructGPT[59] is at the forefront in this regard.
In order to improve
the quality of model outputs, supervised training is conducted using human-
provided prompts and demonstrations. The outputs generated by diﬀerent mod-
els are then collected and ranked by humans based on their quality. The models
are further ﬁne-tuned using a technique known as Reinforcement Learning from
Human Feedback (RLHF)[75], which utilizes human preferences as rewards to
guide the learning process. In addition, To avoid InstructGPT aligning exclu-
sively with human tasks at the expense of neglecting classical NLP tasks, a small
amount of the original data used to train GPT-3 (InstructGPT’s foundation) is
mixed in. Recent research[76, 77] has demonstrated that incorporating larger-
scale and more diverse task instruction datasets can further enhance model
performance.
4
Evolution of AGI
Artiﬁcial General Intelligence (AGI) refers to an advanced level of artiﬁcial in-
telligence (AI) that mirrors human-like abilities in understanding, learning, and
applying knowledge across a broad spectrum of tasks and subject areas. Unlike
narrow AI (e.g., a tailored convolutional neural network for face recognition),
12

which is designed to perform speciﬁc tasks, AGI is capable of adapting to new
situations, transferring domain knowledge, and exhibiting human-like cognitive
abilities beyond streamlined and formatted task-solving workﬂows in the cur-
rent literature [?, 78]. Overall, AGI could demonstrate remarkable versatility
and adaptability,
While the scientiﬁc community has not yet accomplished genuine AGI, the
advancement made in artiﬁcial intelligence and its subﬁelds (e.g., deep learning),
has laid the foundation for further exploration and the quest towards achieving
AGI.. Here’s a brief overview of the history of AGI:
4.1
Early Days of AI
The AGI concept can be traced back to the work of Alan Turing, who proposed
the idea that machines could think and learn like humans in a 1950 manuscript
“Computing Machinery and Intelligence”
[1]. Turing’s ideas laid the ground-
work for AI development and computer science in general.
In 1956, The Dartmouth workshop [79], organized by pioneers such as John
McCarthy, Marvin Minsky, Nathaniel Rochester, and Claude Shannon, marked
the inception of AI as an academic discipline. Their objective was to develop
machines that could imitate human intelligence. This collective endeavor played
a signiﬁcant role in shaping the future course of the AI community.
The initial optimism and enthusiasm in the ﬁeld led to the development of
early AI programs such as the General Problem Solver [80], , Logic Theorist
[81], and ELIZA
[82]. However, these AI systems were limited in scope and
unpractical for large scale real-world applications. A period known as the “AI
winter” occurred because of a decline in funding and interest in artiﬁcial intel-
ligence research. This was due to the lack of signiﬁcant progress made in the
ﬁeld and the unrealistic claims made by some researchers. Reduced funding
support, in turn, led to further decline in progress and a decrease in the number
of published research papers.
The renewed interest in AI was brought about by artiﬁcial neural networks
that were modeled after the structure and function of the human brain
[83,
84]. The backpropagation algorithm, introduced by Rumelhart, Hinton, and
Williams in 1986
[85], allowed neural networks to learn more eﬃciently and
laid down a solid foundation for modern neural networks.
In addition, the emergence of machine learning methods such as support
vector machines [86], decision trees [87], and ensemble methods [88] proved
to be powerful tools for pattern recognition and classiﬁcation. These methods
propelled AI research and empowered practical applications, further driving the
ﬁeld forward..
4.2
Deep Learning and Modern AGI
The development of deep learning, enabled by revolutionary advancements in
computing power and the availability of large datasets, has led to notable ad-
vancements in the ﬁeld of AI. Breakthroughs in computer vision, natural lan-
13

guage processing, and reinforcement learning are bringing the prospect of AGI
closer to becoming a tangible reality. In particular, the Transformer architec-
ture [89], introduced by Vaswani et al. in 2017, revolutionized language mod-
eling by leveraging self-attention mechanisms to capture global dependencies
and contextual relationships between words in a sequence. This breakthrough
laid the foundation for the rise of pre-trained language models, such as GPT-3
[56], and vision transformer (ViT) based models [11] in computer vision. This
shared architectural ancestry has also paved the way for the the development
of transformer-based multimodal models [90, 91] .
Since 2019, the introduction of large-scale language models like GPT-2 [55]
and GPT-3 [56], both based on the transformer architecture, have demonstrated
impressive natural language understanding and generation capabilities. While
these models are not yet AGI, they represent a signiﬁcant step of progress to-
wards achieving this goal. Both GPT-2 and GPT-3 are based on GPT
[10],
a decoder-only pre-trained language model that leverages self-attention mecha-
nisms to capture long-range dependencies between words in a sequence.
Recent advancements in AI give rise to groundbreaking extensions of the
GPT models, such as ChatGPT and GPT-4. ChatGPT builds upon the success
of GPT-3, incorporating reinforcement learning from human feedback (RLHF)
to generate outputs that properly align with human values and preferences. The
chatbot interface of CharGPT has enabled millions of users to engage with AI
in a more natural way, and it has been applied in diverse use cases such as essay
writing, question answering, search, translation [92], data augmentation [71],
computer-aided diagnosis
[93] and data de-identiﬁcation
[69]. On the other
hand, GPT-4 represents a signiﬁcant leap forward in the GPT series, with a
massive set of 10 trillion parameters.
It is capable of advanced math, logic
reasoning. In addition, the model excels in standard examinations such as the
USMLE, LSAT, and GRE [94]. GPT-4 has broad applicability and is expected
to solve an unprecedented range of problems. Its development is a testament to
the tremendous progress made in the pursuit of AGI.
4.3
The Infrastructure of AGI
One key aspect of AGI is the infrastructure required to support it.
Neural
networks have been a major component of this infrastructure, and their devel-
opment has evolved signiﬁcantly since their inception in the 1940s and 1950s.
Early artiﬁcial neural networks (ANNs) were limited in their capabilities due to
their simple linear models. However, the backpropagation algorithm [4], created
by Werbos in 1975, revolutionized the ﬁeld by making it possible to eﬃciently
train neural networks with multiple layers, including the perceptron. This algo-
rithm calculates gradients, which are used to update the weights of the neural
network during training, allowing it to learn and improve its performance over
time. Since the development of backpropagation, neural network research has
advanced rapidly, with the creation of more sophisticated architectures and op-
timization algorithms.
Today, neural networks are used for a wide range of
tasks, including image classiﬁcation, natural language processing, and predic-
14

tion, and continue to be an active area of research in machine learning and
artiﬁcial intelligence.
In addition to algorithm, the progress in hardware, particularly the develop-
ment of graphics processing units (GPUs) and tensor processing units (TPUs),
has made it possible to train deep neural networks eﬃciently, leading to the
widespread adoption of deep learning. This progress has enabled the develop-
ment of more powerful neural networks, which can tackle increasingly complex
problems and has accelerated the research and development of AGI. For exam-
ple, Microsoft’s investment of $1 billion in OpenAI in 2019 enabled the creation
of a dedicated Azure AI supercomputer, one of the world’s most powerful AI
systems. This supercomputer is equipped with over 285,000 CPU cores and
over 10,000 GPUs, and it is designed to support large-scale distributed training
of deep neural networks. Such investments in infrastructure are critical for the
development of AGI.
Recent advancements in AI models, particularly the GPT series [10, 55], have
provided valuable insights into the infrastructure requirements for AGI develop-
ment. To train AI models, three essential components of AGI infrastructure are
required: massive data requirements, computational resources, and distributed
computing systems. GPT models, including GPT-2 and GPT-3, were primar-
ily trained on large-scale web datasets, such as the WebText dataset, which
consisted of 45 terabytes of text data before preprocessing and deduplication,
reduced to around 40 gigabytes of text after preprocessing. Training a GPT
model requires powerful hardware and parallel processing techniques, as exem-
pliﬁed by GPT-3, which was trained using large-scale distributed training across
multiple GPUs, consuming a signiﬁcant amount of computational resources and
energy. Developing an AGI model, such as GPT-4, necessitates distributed com-
puting techniques. While the speciﬁc distributed computing systems used for
training GPT models may not be publicly disclosed, TensorFlow, PyTorch, and
Horovod are distributed computing frameworks that facilitate the implementa-
tion of these techniques. Researchers and developers can use these frameworks
to distribute the training process across multiple devices, manage device com-
munication and synchronization, and eﬃciently utilize available computational
resources.
5
Discussion
5.1
Limitations
While signiﬁcant progress has been made in the development of AGI and brain-
inspired AI, there are still several limitations that need to be overcome before we
can achieve true human-level intelligence in machines. Some of these limitations
include:
Limited understanding of the human brain: Despite signiﬁcant ad-
vancements in neuroscience and brain-inspired AI, we still have a limited un-
derstanding of how the human brain works. This makes it challenging to create
15

machines that can fully replicate human intelligence.
Data eﬃciency: Current AGI and brain-inspired AI systems require vast
amounts of training data to achieve comparable performance to humans. This is
in contrast to humans, who can learn from relatively few examples and generalize
to new situations with ease. How to eﬃciently learn from few samples is still
an opening question.
Ethics: There are also ethical considerations to consider with AGI. As
these systems become more intelligent, they may be able to make decisions that
have far-reaching consequences. Ensuring that these decisions are aligned with
human values and ethical principles is critical for preventing unintended harm.
Safety: Safety is also a signiﬁcant concern with AGI. Ensuring that these
systems do not cause unintended harm, either through malicious intent or unin-
tentional mistakes, is critical for their widespread adoption. Developing robust
safety mechanisms and ensuring that AGI systems are aligned with human val-
ues is essential.
Computational Cost: Current LLM models requires massive computa-
tional resources to train and operate, making it challenging to develop and
deploy in a wide range of scenarios. Meanwhile, the computational cost can
limit the number of researchers and organizations working in the ﬁeld, which
may slow the progress towards AGI. Additionally, the energy consumption of
AGI systems can be prohibitively high, making them unsustainable from an
environmental perspective.
5.2
Future of AGI
The future of AGI is an exciting and rapidly evolving ﬁeld. While the devel-
opment of AGI remains a challenge, it has the potential to revolutionize many
aspects of our lives, from healthcare to transportation to education. One po-
tential avenue for advancing AGI is through the creation of more powerful and
sophisticated AGI foundation models.
Recent breakthroughs in natural lan-
guage processing, computer vision, knowledge graph, and reinforcement learn-
ing have led to the development of increasingly advanced AGI models such as
ChatGPT and GPT-4. These models have shown impressive capabilities in var-
ious applications. Further advances in AGI foundation model research, as well
as improvements in hardware and computational algorithms, are very likely to
accelerate the development of AGI.
Another approach to developing AGI is through the integration of diﬀerent
AI systems and technologies across multiple domains, including adding human
in the loop through reinforcement learning from expert feedback. For example,
combining natural language processing with computer vision and robotics under
the guidance of human experts could lead to the creation of more versatile and
adaptable intelligent systems. This integration could also help overcome the
limitations of current AI systems, which are often specialized in speciﬁc domains
and lack the ﬂexibility to transfer knowledge across domains.
The development of AGI also requires the development of novel approaches
for machine learning, such as more eﬃcient instruct methods, in-context learning
16

algorithms, and reasoning paradigm, particularly by learning from the human
brain via brain-inspired AI. These approaches aim to enable machines to learn
from unstructured data without the need of labeling them and rapidly generalize
from a few examples, which is crucial for enabling machines to learn and adapt
to new tasks and environments.
Finally, ethical and societal implications of AGI development must be con-
sidered, including issues related to bias, privacy, and security. As AGI becomes
more powerful and pervasive, it is essential to ensure that it is developed and
used in a responsible and ethical manner that beneﬁts society as a whole and
aligns well with human value. Overall, while the development of AGI remains
a challenge, it has the potential to revolutionize many aspects of our lives and
bring signiﬁcant beneﬁts to society and humanity. Ongoing research and de-
velopment in AGI will continue to drive progress towards the ultimate goal of
creating truly intelligent machines.
6
Conclusion
In this article, we provided a comprehensive overview of brain-inspired AI from
the perspective of AGI, covering its current progress, important characteristics,
and technological advancements towards achieving AGI. We also discussed the
evolution, limitations and the future of AGI. In conclusion, brain-inspired AI is
a promising ﬁeld that has the potential to unlock the mysteries of human in-
telligence and pave the way for AGI. While signiﬁcant progress has been made
in recent years, there is still much work to be done to achieve AGI. It will re-
quire advances in technology, algorithms, and hardware, as well as continued
collaboration across multiple disciplines.
Nonetheless, the pursuit of AGI is
an important and worthwhile endeavor that has the potential to transform our
world in unprecedented ways. We hope this survey provides a valuable contribu-
tion to this exciting ﬁeld and inspires further research and development toward
the ultimate goal of AGI.
References
[1] Turing AM. Computing machinery and intelligence. Springer; 2009.
[2] McCulloch WS, Pitts W.
A logical calculus of the ideas immanent in
nervous activity. The bulletin of mathematical biophysics. 1943;5:115-33.
[3] Rosenblatt F. Principles of neurodynamics. perceptrons and the theory of
brain mechanisms. Cornell Aeronautical Lab Inc Buﬀalo NY; 1961.
[4] Werbos P. Beyond regression: New tools for prediction and analysis in
the behavioral sciences. PhD thesis, Committee on Applied Mathematics,
Harvard University, Cambridge, MA. 1974.
17

[5] Rumelhart DE, Hinton GE, Williams RJ. Learning internal representations
by error propagation. California Univ San Diego La Jolla Inst for Cognitive
Science; 1985.
[6] LeCun Y, Bengio Y, et al.
Convolutional networks for images, speech,
and time series.
The handbook of brain theory and neural networks.
1995;3361(10):1995.
[7] Hubel DH, Wiesel TN. Receptive ﬁelds, binocular interaction and func-
tional architecture in the cat’s visual cortex. The Journal of physiology.
1962;160(1):106.
[8] Posner MI, Petersen SE. The attention system of the human brain. Annual
review of neuroscience. 1990;13(1):25-42.
[9] Devlin J, Chang MW, Lee K, Toutanova K. Bert: Pre-training of deep
bidirectional transformers for language understanding.
arXiv preprint
arXiv:181004805. 2018.
[10] Radford A, Narasimhan K, Salimans T, Sutskever I, et al. Improving lan-
guage understanding by generative pre-training. OpenAI. 2018.
[11] Dosovitskiy A, Beyer L, Kolesnikov A, Weissenborn D, Zhai X, Unterthiner
T, et al. An image is worth 16x16 words: Transformers for image recogni-
tion at scale. arXiv preprint arXiv:201011929. 2020.
[12] Bassett DS, Bullmore E. Small-world brain networks. The neuroscientist.
2006;12(6):512-23.
[13] Bullmore E, Sporns O. Complex brain networks: graph theoretical anal-
ysis of structural and functional systems.
Nature reviews neuroscience.
2009;10(3):186-98.
[14] Bassett DS, Bullmore ET. Small-world brain networks revisited. The Neu-
roscientist. 2017;23(5):499-516.
[15] Xie S, Kirillov A, Girshick R, He K.
Exploring randomly wired neural
networks for image recognition. In: Proceedings of the IEEE/CVF Inter-
national Conference on Computer Vision; 2019. p. 1284-93.
[16] You J, Leskovec J, He K, Xie S. Graph structure of neural networks. In:
International Conference on Machine Learning. PMLR; 2020. p. 10881-91.
[17] Chen Y, Du Y, Xiao Z, Zhao L, Zhang L, Liu DW, et al. A Uniﬁed and
Biologically-Plausible Relational Graph Representation of Vision Trans-
formers. arXiv preprint arXiv:220611073. 2022.
[18] Zhao L, Dai H, Wu Z, Xiao Z, Zhang L, Liu DW, et al. Coupling Visual
Semantics of Artiﬁcial Neural Networks and Human Brain Function via
Synchronized Activations. arXiv preprint arXiv:220610821. 2022.
18

[19] Huang H, Zhao L, Hu X, Dai H, Zhang L, Zhu D, et al.
BI AVAN:
Brain inspired Adversarial Visual Attention Network.
arXiv preprint
arXiv:221015790. 2022.
[20] Ghosh-Dastidar S, Adeli H. Spiking neural networks. International journal
of neural systems. 2009;19(04):295-308.
[21] Kasabov NK. NeuCube: A spiking neural network architecture for map-
ping, learning and understanding of spatio-temporal brain data. Neural
Networks. 2014;52:62-76.
[22] Kumarasinghe K, Kasabov N, Taylor D.
Brain-inspired spiking neural
networks for decoding and understanding muscle activity and kinematics
from electroencephalography signals during hand movements.
Scientiﬁc
reports. 2021;11(1):2486.
[23] Dethier J, Nuyujukian P, Ryu SI, Shenoy KV, Boahen K.
Design and
validation of a real-time spiking-neural-network decoder for brain–machine
interfaces. Journal of neural engineering. 2013;10(3):036008.
[24] Kumarasinghe K, Kasabov N, Taylor D. Deep learning and deep knowledge
representation in Spiking Neural Networks for Brain-Computer Interfaces.
Neural Networks. 2020;121:169-85.
[25] Merolla PA, Arthur JV, Alvarez-Icaza R, Cassidy AS, Sawada J, Akopyan
F, et al. A million spiking-neuron integrated circuit with a scalable com-
munication network and interface. Science. 2014;345(6197):668-73.
[26] Benjamin BV, Gao P, McQuinn E, Choudhary S, Chandrasekaran AR,
Bussat JM, et al. Neurogrid: A mixed-analog-digital multichip system for
large-scale neural simulations. Proceedings of the IEEE. 2014;102(5):699-
716.
[27] Zhang B, Shi L, Song S. Creating more intelligent robots through brain-
inspired computing. Science Robotics. 2016;354(6318):1445.
[28] Davies M, Srinivasa N, Lin TH, Chinya G, Cao Y, Choday SH, et al. Loihi:
A neuromorphic manycore processor with on-chip learning.
Ieee Micro.
2018;38(1):82-99.
[29] Roy K, Jaiswal A, Panda P. Towards spike-based machine intelligence with
neuromorphic computing. Nature. 2019;575(7784):607-17.
[30] Pei J, Deng L, Song S, Zhao M, Zhang Y, Wu S, et al.
Towards arti-
ﬁcial general intelligence with hybrid Tianjic chip architecture. Nature.
2019;572(7767):106-11.
[31] Akopyan F, Sawada J, Cassidy A, Alvarez-Icaza R, Arthur J, Merolla P,
et al. Truenorth: Design and tool ﬂow of a 65 mw 1 million neuron pro-
grammable neurosynaptic chip. IEEE transactions on computer-aided de-
sign of integrated circuits and systems. 2015;34(10):1537-57.
19

[32] Indiveri
G,
Douglas
R.
Neuromorphic
vision
sensors.
Science.
2000;288(5469):1189-90.
[33] Sandamirskaya Y, Kaboli M, Conradt J, Celikel T. Neuromorphic com-
puting hardware and neural architectures for robotics. Science Robotics.
2022;7(67):eabl8419.
[34] Viale A, Marchisio A, Martina M, Masera G, Shaﬁque M.
LaneSNNs:
Spiking Neural Networks for Lane Detection on the Loihi Neuromorphic
Processor.
In: 2022 IEEE/RSJ International Conference on Intelligent
Robots and Systems (IROS). IEEE; 2022. p. 79-86.
[35] Christensen JR, Larsen KB, Lisanby SH, Scalia J, Arango V, Dwork AJ,
et al. Neocortical and hippocampal neuron and glial cell numbers in the
rhesus monkey. The Anatomical Record: Advances in Integrative Anatomy
and Evolutionary Biology: Advances in Integrative Anatomy and Evolu-
tionary Biology. 2007;290(3):330-40.
[36] Dicke U, Roth G.
Neuronal factors determining high intelligence.
Philosophical Transactions of the Royal Society B: Biological Sciences.
2016;371(1685):20150180.
[37] Herculano-Houzel S. The remarkable, yet not extraordinary, human brain
as a scaled-up primate brain and its associated cost. Proceedings of the
National Academy of Sciences. 2012;109(supplement 1):10661-8.
[38] Huttenlocher PR, et al.
Synaptic density in human frontal cortex-
developmental changes and eﬀects of aging. Brain Res. 1979;163(2):195-
205.
[39] Rakic P. A small step for the cell, a giant leap for mankind: a hypoth-
esis of neocortical expansion during evolution.
Trends in neurosciences.
1995;18(9):383-8.
[40] Sporns O. The human connectome: a complex network. Annals of the new
York Academy of Sciences. 2011;1224(1):109-25.
[41] Radford A, Kim JW, Hallacy C, Ramesh A, Goh G, Agarwal S, et al.
Learning transferable visual models from natural language supervision. In:
International conference on machine learning. PMLR; 2021. p. 8748-63.
[42] Ramesh A, Pavlov M, Goh G, Gray S, Voss C, Radford A, et al. Zero-
shot text-to-image generation. In: International Conference on Machine
Learning. PMLR; 2021. p. 8821-31.
[43] Nichol A, Dhariwal P, Ramesh A, Shyam P, Mishkin P, McGrew B, et al.
Glide:
Towards photorealistic image generation and editing with text-
guided diﬀusion models. arXiv preprint arXiv:211210741. 2021.
20

[44] Chen J, Guo H, Yi K, Li B, Elhoseiny M.
VisualGPT: Data-Eﬃcient
Adaptation of Pretrained Language Models for Image Captioning. In: Pro-
ceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR); 2022. p. 18030-40.
[45] Rombach R, Blattmann A, Lorenz D, Esser P, Ommer B. High-resolution
image synthesis with latent diﬀusion models.
In:
Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition;
2022. p. 10684-95.
[46] Quiroga RQ, Reddy L, Kreiman G, Koch C, Fried I.
Invariant vi-
sual representation by single neurons in the human brain.
Nature.
2005;435(7045):1102-7.
[47] Goh G, Cammarata N, Voss C, Carter S, Petrov M, Schubert L, et al.
Multimodal neurons in artiﬁcial neural networks. Distill. 2021;6(3):e30.
[48] Antol S, Agrawal A, Lu J, Mitchell M, Batra D, Zitnick CL, et al. Vqa:
Visual question answering. In: Proceedings of the IEEE international con-
ference on computer vision; 2015. p. 2425-33.
[49] Dou ZY, Xu Y, Gan Z, Wang J, Wang S, Wang L, et al. An empirical study
of training end-to-end vision-and-language transformers. In: Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition;
2022. p. 18166-76.
[50] Bao H, Wang W, Dong L, Liu Q, Mohammed OK, Aggarwal K, et al. Vlmo:
Uniﬁed vision-language pre-training with mixture-of-modality-experts. Ad-
vances in Neural Information Processing Systems. 2022;35:32897-912.
[51] Lei J, Li L, Zhou L, Gan Z, Berg TL, Bansal M, et al. Less is more: Clipbert
for video-and-language learning via sparse sampling. In: Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition;
2021. p. 7331-41.
[52] Fu TJ, Li L, Gan Z, Lin K, Wang WY, Wang L, et al. Violet: End-to-
end video-language transformers with masked visual-token modeling. arXiv
preprint arXiv:211112681. 2021.
[53] Lin K, Li L, Lin CC, Ahmed F, Gan Z, Liu Z, et al. Swinbert: End-to-end
transformers with sparse attention for video captioning. In: Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition;
2022. p. 17949-58.
[54] Bubeck S, Chandrasekaran V, Eldan R, Gehrke J, Horvitz E, Kamar E,
et al.
Sparks of Artiﬁcial General Intelligence: Early experiments with
GPT-4. arXiv preprint arXiv:230312712. 2023.
[55] Radford A, Wu J, Child R, Luan D, Amodei D, Sutskever I, et al. Language
models are unsupervised multitask learners. OpenAI blog. 2019;1(8):9.
21

[56] Brown T, Mann B, Ryder N, Subbiah M, Kaplan JD, Dhariwal P, et al.
Language models are few-shot learners. Advances in neural information
processing systems. 2020;33:1877-901.
[57] Raﬀel C, Shazeer N, Roberts A, Lee K, Narang S, Matena M, et al. Explor-
ing the limits of transfer learning with a uniﬁed text-to-text transformer.
The Journal of Machine Learning Research. 2020;21(1):5485-551.
[58] Glaese A, McAleese N, Trebacz M, Aslanides J, Firoiu V, Ewalds T, et al.
Improving alignment of dialogue agents via targeted human judgements.
arXiv preprint arXiv:220914375. 2022.
[59] Ouyang L, Wu J, Jiang X, Almeida D, Wainwright CL, Mishkin P, et al.
Training language models to follow instructions with human feedback.
arXiv preprint arXiv:220302155. 2022.
[60] Herculano-Houzel S. The human brain in numbers: a linearly scaled-up
primate brain. Frontiers in human neuroscience. 2009:31.
[61] Wei J, Tay Y, Bommasani R, Raﬀel C, Zoph B, Borgeaud S, et al. Emergent
abilities of large language models. arXiv preprint arXiv:220607682. 2022.
[62] Kojima T, Gu SS, Reid M, Matsuo Y, Iwasawa Y. Large language models
are zero-shot reasoners. arXiv preprint arXiv:220511916. 2022.
[63] Wei J, Wang X, Schuurmans D, Bosma M, Chi E, Le Q, et al.
Chain
of thought prompting elicits reasoning in large language models.
arXiv
preprint arXiv:220111903. 2022.
[64] Wang X, Wei J, Schuurmans D, Le Q, Chi E, Zhou D. Self-consistency
improves chain of thought reasoning in language models. arXiv preprint
arXiv:220311171. 2022.
[65] Li Y, Lin Z, Zhang S, Fu Q, Chen B, Lou JG, et al. On the advance of
making language models better reasoners. arXiv preprint arXiv:220602336.
2022.
[66] Zhang Z, Zhang A, Li M, Smola A. Automatic chain of thought prompting
in large language models. arXiv preprint arXiv:221003493. 2022.
[67] Suzgun M, Scales N, Sch¨arli N, Gehrmann S, Tay Y, Chung HW, et al.
Challenging BIG-Bench tasks and whether chain-of-thought can solve them.
arXiv preprint arXiv:221009261. 2022.
[68] Zhou D, Sch¨arli N, Hou L, Wei J, Scales N, Wang X, et al.
Least-to-
most prompting enables complex reasoning in large language models. arXiv
preprint arXiv:220510625. 2022.
[69] Liu Z, Yu X, Zhang L, Wu Z, Cao C, Dai H, et al. DeID-GPT: Zero-shot
Medical Text De-Identiﬁcation by GPT-4. arXiv preprint arXiv:230311032.
2023.
22

[70] Liu D, Chen Y, Wu Z.
Digital Twin (DT)-CycleGAN: Enabling Zero-
Shot Sim-to-Real Transfer of Visual Grasping Models. IEEE Robotics and
Automation Letters. 2023.
[71] Dai H, Liu Z, Liao W, Huang X, Wu Z, Zhao L, et al. ChatAug: Leveraging
ChatGPT for Text Data Augmentation. arXiv preprint arXiv:230213007.
2023.
[72] Dai D, Sun Y, Dong L, Hao Y, Sui Z, Wei F. Why Can GPT Learn In-
Context? Language Models Secretly Perform Gradient Descent as Meta
Optimizers. arXiv preprint arXiv:221210559. 2022.
[73] LeCun Y. A path towards autonomous machine intelligence version 0.9. 2,
2022-06-27. Open Review. 2022;62.
[74] Sanh V, Webson A, Raﬀel C, Bach SH, Sutawika L, Alyafeai Z, et al.
Multitask prompted training enables zero-shot task generalization. arXiv
preprint arXiv:211008207. 2021.
[75] Christiano PF, Leike J, Brown T, Martic M, Legg S, Amodei D. Deep
reinforcement learning from human preferences. Advances in neural infor-
mation processing systems. 2017;30.
[76] Chung HW, Hou L, Longpre S, Zoph B, Tay Y, Fedus W, et al. Scaling
instruction-ﬁnetuned language models.
arXiv preprint arXiv:221011416.
2022.
[77] Wang Y, Mishra S, Alipoormolabashi P, Kordi Y, Mirzaei A, Arunkumar
A, et al. Benchmarking generalization via in-context instructions on 1,600+
language tasks. arXiv preprint arXiv:220407705. 2022.
[78] Hodson H. DeepMind and Google: the battle to control artiﬁcial intelli-
gence. The Economist, ISSN. 2019:0013-613.
[79] Kline R. Cybernetics, automata studies, and the Dartmouth conference
on artiﬁcial intelligence.
IEEE Annals of the History of Computing.
2010;33(4):5-16.
[80] Nilsson NJ.
The quest for artiﬁcial intelligence.
Cambridge University
Press; 2009.
[81] Gugerty L. Newell and Simon’s logic theorist: Historical background and
impact on cognitive modeling. In: Proceedings of the human factors and
ergonomics society annual meeting. vol. 50. SAGE Publications Sage CA:
Los Angeles, CA; 2006. p. 880-4.
[82] Weizenbaum J.
ELIZA—a computer program for the study of natural
language communication between man and machine. Communications of
the ACM. 1966;9(1):36-45.
23

[83] Shanmuganathan S. Artiﬁcial neural network modelling: An introduction.
Springer; 2016.
[84] Lippmann R. An introduction to computing with neural nets. IEEE Assp
magazine. 1987;4(2):4-22.
[85] Rumelhart DE, Hinton GE, Williams RJ.
Learning representations by
back-propagating errors. nature. 1986;323(6088):533-6.
[86] Cortes C, Vapnik V.
Support-vector networks.
Machine learning.
1995;20:273-97.
[87] Quinlan JR. Induction of decision trees. Machine learning. 1986;1:81-106.
[88] Opitz D, Maclin R. Popular ensemble methods: An empirical study. Jour-
nal of artiﬁcial intelligence research. 1999;11:169-98.
[89] Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, et al.
Attention is all you need. Advances in neural information processing sys-
tems. 2017;30.
[90] Liu Z, He M, Jiang Z, Wu Z, Dai H, Zhang L, et al. Survey on natural
language processing in medical image analysis.
Zhong nan da xue xue
bao Yi xue ban= Journal of Central South University Medical Sciences.
2022;47(8):981-93.
[91] Hu R, Singh A. Unit: Multimodal multitask learning with a uniﬁed trans-
former. In: Proceedings of the IEEE/CVF International Conference on
Computer Vision; 2021. p. 1439-49.
[92] Qin C, Zhang A, Zhang Z, Chen J, Yasunaga M, Yang D. Is chatgpt a
general-purpose natural language processing task solver?
arXiv preprint
arXiv:230206476. 2023.
[93] Wang S, Zhao Z, Ouyang X, Wang Q, Shen D.
Chatcad: Interactive
computer-aided diagnosis on medical image using large language models.
arXiv preprint arXiv:230207257. 2023.
[94] OpenAI. GPT-4 Technical Report; 2023.
24

