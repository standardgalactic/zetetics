E(3)-Equivariant Graph Neural Networks for Data-Eﬃcient and Accurate Interatomic
Potentials
Simon Batzner∗,1 Albert Musaelian,1 Lixin Sun,1 Mario Geiger,2 Jonathan P. Mailoa,3
Mordechai Kornbluth,3 Nicola Molinari,1 Tess E. Smidt,4, 5 and Boris Kozinsky∗1, 3
1John A. Paulson School of Engineering and Applied Sciences,
Harvard University, Cambridge, MA 02138, USA
2 ´Ecole Polytechnique F´ed´erale de Lausanne, 1015 Lausanne, Switzerland
3Robert Bosch Research and Technology Center, Cambridge, MA 02139, USA
4Computational Research Division and Center for Advanced Mathematics for Energy Research Applications,
Lawrence Berkeley National Laboratory, Berkeley, CA 94720, USA
5Massachusetts Institute of Technology, Department of Electrical
Engineering and Computer Science, Cambridge, MA 02142, USA
This work presents Neural Equivariant Interatomic Potentials (NequIP), an E(3)-equivariant
neural network approach for learning interatomic potentials from ab-initio calculations for molecular
dynamics simulations. While most contemporary symmetry-aware models use invariant convolutions
and only act on scalars, NequIP employs E(3)-equivariant convolutions for interactions of geometric
tensors, resulting in a more information-rich and faithful representation of atomic environments.
The method achieves state-of-the-art accuracy on a challenging and diverse set of molecules and
materials while exhibiting remarkable data eﬃciency. NequIP outperforms existing models with
up to three orders of magnitude fewer training data, challenging the widely held belief that deep
neural networks require massive training sets. The high data eﬃciency of the method allows for the
construction of accurate potentials using high-order quantum chemical level of theory as reference
and enables high-ﬁdelity molecular dynamics simulations over long time scales.
INTRODUCTION
Molecular
dynamics
(MD)
simulations
are
an
indispensable tool for computational discovery in ﬁelds
as diverse as energy storage, catalysis, and biological
processes [1–3].
While the atomic forces required to
integrate Newton’s equations of motion can in principle
be obtained with high ﬁdelity from quantum-mechanical
calculations such as density functional theory (DFT),
in practice the unfavorable computational scaling of
ﬁrst-principles methods limits simulations to short time
scales and small numbers of atoms. This prohibits the
study of many interesting physical phenomena beyond
the time and length scales that are currently accessible,
even on the largest supercomputers.
Owing to their
simple functional form, classical models for the atomic
potential energy can typically be evaluated orders of
magnitude faster than ﬁrst-principles methods, thereby
enabling the study of large numbers of atoms over long
time scales. However, due to their limited mathematical
form, classical interatomic potentials, or force ﬁelds, are
inherently limited in their predictive accuracy which
has historically led to a fundamental trade-oﬀbetween
obtaining
high
computational
eﬃciency
while
also
predicting faithful dynamics of the system under study.
∗Corresponding authors
B.K., E-mail: bkoz@seas.harvard
S.B., E-mail: batzner@g.harvard.edu
The construction of ﬂexible models of the interatomic
potential energy based on Machine Learning (ML-IP),
and in particular Neural Networks (NN-IP), has shown
great promise in providing a way to move past this
dilemma,
promising to learn high-ﬁdelity potentials
from ab-initio
reference calculations while retaining
favorable computational eﬃciency [4–13].
One of the
limiting factors of NN-IPs is that they typically require
collection of large training sets of ab-initio calculations,
often including thousands or even millions of reference
structures [4, 9, 10, 14–16].
This computationally
expensive process of training data collection has severely
limited the adoption of NN-IPs as it quickly becomes
a bottleneck in the development of force-ﬁelds for new
systems.
In this work, we present the Neural Equivariant
Interatomic Potential (NequIP), a highly data-eﬃcient
deep
learning
approach
for
learning
interatomic
potentials from reference ﬁrst-principles calculations.
We show that the proposed method obtains high
accuracy compared to existing ML-IP methods across
a wide variety of systems, including small molecules,
water in diﬀerent phases, an amorphous solid, a reaction
at a solid/gas interface,
and a Lithium superionic
conductor. Furthermore, we ﬁnd that NequIP exhibits
exceptional data eﬃciency, enabling the construction of
accurate interatomic potentials from limited data sets
of fewer than 1,000 or even as little as 100 reference
ab-initio
calculations,
where other methods require
arXiv:2101.03164v3  [physics.comp-ph]  16 Dec 2021

2
orders of magnitude more.
It is worth noting that on
small molecular data sets, NequIP outperforms not only
other neural networks, but is also competitive with
kernel-based approaches, which typically obtain better
predictive accuracy than NN-IPs on small data sets
(although at signiﬁcant additional cost scaling in training
and prediction).
We further demonstrate high data
eﬃciency and accuracy with state-of-the-art results on a
training set of molecular data obtained at the quantum
chemical coupled-cluster level of theory.
Finally, we
validate the method through a series of simulations
and demonstrate that we can reproduce with high
ﬁdelity structural and kinetic properties computed from
NequIP simulations in comparison to ab-initio molecular
dynamics simulations (AIMD). We directly verify that
the performance gains are connected with the unique
E(3)-equivariant convolution architecture of the new
NequIP model.
Related Work
The ﬁrst applications of machine learning for the
development of interatomic potentials were built on
descriptor-based
approaches
combined
with
shallow
neural networks or Gaussian Processes [4, 5], designed
to
exhibit
invariance
with
respect
to
translation,
permutation of atoms of the same chemical species,
and rotation.
Recently, rotationally invariant graph
neural network interatomic potentials (GNN-IPs) have
emerged as a powerful architecture for deep learning
of interatomic potentials that eliminates the need for
hand-crafted descriptors and allows to instead learn
representations on graphs of atoms from invariant
features of geometric data (e.g.
radial distances or
angles) [9–11, 13].
In GNN-IPs, atomic structures are
represented by collections of nodes and edges, where
nodes in the graph correspond to individual atoms
and edges are typically deﬁned by simply connecting
every atom to all other atoms that are closer than some
cutoﬀdistance rc.
Every node/atom i is associated
with a feature hi ∈Rh, consisting of scalar values,
which is iteratively reﬁned via a series of convolutions
over neighboring atoms j based on both the distance
to neighboring atoms rij and their features hj.
This
iterative process allows information to be propagated
along the atomic graph through a series of convolutional
layers and can be viewed as a message-passing scheme
[17].
Operating only on interatomic distances allows
GNN-IPs to be rotation- and translation-invariant,
making both the output as well as features internal
to the network invariant to rotations. In contrast, the
method outlined in this work uses relative position
vectors rather than simply distances (scalars) together
with features comprised of not only scalars, but also
higher-order geometric tensors.
This makes internal
features instead equivariant
to rotation and allows
for angular information to be used by rotationally
equivariant ﬁlters.
Similar to other methods, we can
restrict convolutions to only a local subset of all other
atoms that lie closer to the central atom than a chosen
cutoﬀdistance rc, see Figure 1, left.
A series of related methods have recently been
proposed:
DimeNet [11] expands on using pairwise
interactions in a single convolution to include angular,
three-body terms,
but individual features are still
comprised of scalars (distances and three-body angles
are invariant to rotation), as opposed to vectors used in
this work.
Cormorant [18] uses an equivariant neural
network for property prediction on small molecules.
This method is demonstrated on potential energies of
small molecules but not on atomic forces or systems with
periodic boundary conditions. Townshend et al. [19] use
the framework of Tensor-Field Networks [20] to directly
predict atomic force vectors.
The predicted forces are
not guaranteed by construction to conserve energy since
they are not obtained as gradients of the total potential
energy.
This may lead to problems in simulations of
molecular dynamics over long times. None of these three
works [11, 18, 19] demonstrates capability to perform
molecular dynamics simulations.
After a ﬁrst version of this manuscript appeared online
[21], a series of other equivariant GNN-IPs have been
proposed, such as PaiNN [22] and NewtonNet [23]. Both
of these methods were proposed after NequIP and only
make use of l = 1 tensors. In addition, we also compare
a series of other works that have since been proposed,
including the GemNet [24], SpookyNet [25], and UNiTE
approaches [26].
The
contribution
of
the
present
work
is
the
introduction
of
a
deep
learning
energy-conserving
interatomic potential for both molecules and materials
built on E(3)-equivariant convolutions over geometric
tensors that yields state-of-the-art accuracy, outstanding
data-eﬃciency, and can with high ﬁdelity reproduce
structural
and
kinetic
properties
from
molecular
dynamics simulations.
RESULTS
Equivariance
The
concept
of
equivariance
arises
naturally
in
machine learning of atomistic systems (see e.g.
[27]):
physical properties have well-deﬁned transformation
properties under translation, reﬂection, and rotation of a
set of atoms. As a simple example, if a molecule is rotated

3
FIG. 1: Left: a set of atoms is interpreted as an atomic graph with local neighborhoods. Middle: every atom carries
a set of scalar, vector, and higher-order tensor features with it. Right: atoms exchange information via ﬁlters, that
are again geometric tensors of increasing order.
in space, the vectors of its atomic dipoles or forces also
rotate accordingly, via an equivariant transformation.
Equivariant neural networks are able to more generally
represent tensor properties and tensor operations of
physical systems (e.g.
vector addition, dot products,
and cross products).
Equivariant neural networks
are guaranteed to preserve the known transformation
properties
of
physical
systems
under
a
change
of
coordinates because they are explicitly constructed from
equivariant operations. Formally, a function f : X →Y
is equivariant with respect to a group G that acts on X
and Y if:
DY [g]f(x) = f(DX[g]x)
∀g ∈G, ∀x ∈X
(1)
where DX[g] and DY [g] are the representations of
the group element g in the vector spaces X and Y ,
respectively. Here, we focus on equivariance with respect
to E(3), i.e.
the group of rotations, reﬂections, and
translations in 3D space.
Neural Equivariant Interatomic Potentials
Given a set of atoms (a molecule or a material), we
aim to ﬁnd a mapping from atomic positions {⃗ri} and
chemical species {Zi} to the total potential energy Epot
and the forces acting on the atoms {⃗Fi}.
Following
previous work [4], this total potential energy is obtained
as a sum of atomic potential energies. Forces are then
obtained as the gradients of this predicted total potential
energy with respect to the atomic positions (thereby
guaranteeing energy conservation):
Epot =
X
i∈Natoms
Ei,atomic
(2)
⃗Fi = −∇iEpot
(3)
The atomic local energies Ei,atomic are the scalar node
attributes predicted by the graph neural network. Even
though the output of NequIP is the predicted potential
energy Epot,
which is invariant under translations,
reﬂection, and rotations, the network contains internal
features that are geometric tensors which are equivariant
to rotation and reﬂection.
This constitutes the core
diﬀerence between NequIP and existing scalar-valued
invariant GNN-IPs.
A series of methods has been introduced to realize
rotationally equivariant neural networks [13, 20, 28–
30].
Here,
we build on the layers introduced in
Tensor-Field
Networks
(TFN)
[20],
primitives
for
which are implemented in e3nn [31],
which enable
the
construction
of
neural
networks
that
exhibit
equivariance to translation, parity, and rotation. Every
atom in NequIP is associated with features comprised
of tensors of diﬀerent orders:
scalars, vectors, and
higher-order tensors.
Formally, the feature vectors
are geometric objects that comprise a direct sum of
irreducible representations of the O(3) symmetry group.
The feature vectors V (l,p)
acm are indexed by keys l, p, where
the “rotation order” l = 0, 1, 2, ... is a non-negative
integer and parity is one of p ∈(1, −1) which together
label the irreducible representations of O(3).
The
indices a, c, m, correspond to the atoms, the channels
(elements of the feature vector), and the representation
index which takes values m ∈[−l, l], respectively. The
convolutions that operate on these geometric objects
are equivariant functions instead of invariant ones,
i.e.
if a feature at layer k is transformed under a
rotation or parity transformation, then the output of
the convolution from layer k →k + 1 is transformed
accordingly.

4
Convolution
operations
are
naturally
translation
invariant, since their ﬁlters act on relative interatomic
distance vectors.
Moreover,
they are permutation
invariant since the sum over contributions from diﬀerent
atoms is invariant to permutations of those atoms. Note
that while atomic features are equivariant to permutation
of atom indices, globally, the total potential energy of
the system is invariant to permutation.
To achieve
rotation equivariance, the convolution ﬁlters F(⃗rij) are
constrained to be products of learnable radial functions
and spherical harmonics, which are equivariant under
SO(3) [20]:
F(⃗rij) = R(rij)Y (l)
m (ˆrij)
(4)
where if ⃗rij denotes the relative position from central
atom
i
to
neighboring
atom
j,
ˆrij
and
rij
are
the associated unit vector and interatomic distance,
respectively,
and
F(⃗rij)
denotes
the
corresponding
convolutional ﬁlter. It should be noted that all learnable
weights in the ﬁlter lie in the rotationally invariant radial
function R(rij). This radial function is implemented as a
small multi-layer perceptron that operates on interatomic
distances expressed in a basis of choice, R(rij) : R →Rh,
where h is the feature dimension:
R(rij) = Wnσ(...σ(W2σ(W1B(rij))))
(5)
where B(rij) is a basis embedding of the interatomic
distance, Wi are weight matrices and σ(x) denotes the
nonlinear activation function, for which we use the SiLU
activation function [32] in our experiments. Radial Bessel
functions and a polynomial envelope function fenv [11]
are used as the basis for the interatomic distances:
B(rij) = 2
rc
sin( nπ
rc rij)
rij
fenv(rij, rc)
(6)
where rc is a local cutoﬀradius, restricting interactions
to atoms closer than some cutoﬀdistance and fenv is the
polynomial deﬁned in [11] with p = 6 operating on the
interatomic distances normalized by the cutoﬀradius
rij
rc . The use of cutoﬀs/local atomic environments allows
the computational cost of evaluation to scale linearly
with the number of atoms. Similar to [11], we initialize
the Bessel functions with n = [1, 2, ..., Nb], where Nb is
the number of basis functions, and subsequently optimize
nπ via backpropagation rather than keeping it constant.
For systems with periodic boundary conditions, we use
neighbor lists as implemented in the ASE code [33] to
identify appropriate atomic neighbors.
Finally, in the convolution, the input atomic feature
tensor and the ﬁlter have to again be combined in an
equivariant manner, which is achieved via a geometric
tensor product that yields an output feature that again
is rotationally equivariant.
A tensor product of two
geometric tensors is computed via contraction with
the Clebsch-Gordan coeﬃcients, as outlined in [20]. A
tensor product between an input feature of order li
and a convolutional ﬁlter of order lf yields irreducible
representations of output orders |li −lf| ≤lo ≤|li + lf|.
In NequIP, we use a maximum rotation order lmax
and discard all tensor product operations that would
results in irreducibe representations with lo > lmax.
Omitting all higher-order interactions that go beyond
the 0 ⊗0 →0 interaction will result in a conventional
GNN-IP with invariant convolutions over scalar features,
similar to e.g. SchNet [9].
The ﬁnal symmetry the network needs to respect is
that of parity: how the tensor transformations when the
input is mirrored, i.e.
⃗x →−⃗x.
A tensor has even
parity (p = 1) if it is invariant to such a transformation;
it has odd parity (p = −1) if its sign ﬂips under that
transformation. Parity equivariance is achieved by only
allowing contributions from a ﬁlter and an incoming
tensor feature with parities pf and pi to contribute to an
output feature if the following selection rule is satisﬁed:
po = pipf
(7)
Finally, as outlined in [20], a full convolutional layer
L implementing an interaction with ﬁlter f acting on an
input i producing output o: li ⊗lf →lo is given by:
L(lo,po,lf ,li,pf ,pi)
acmo
(⃗ra, V (li,pi)
acmi ) =
X
mf ,mi
C(lo,mo)
(li,mi)(lf ,mf )
X
b∈S

R(lf ,li,pf ,pi)
c
(rab)

Y (lf )
mf (ˆrab)V (li,pi)
bcmi
(8)
where a and b index the central atom of the convolution
and the neighboring atom b ∈S, respectively, and C
indicates the Clebsch-Gordan coeﬃcients.
Note that
the
Clebsch-Gordan
coeﬃcients
do
not
depend
on
the parity of the arguments.
There can be multiple
L(lo,po)
acmo
tensors for a given output rotation order and

5
parity (lo, po) resulting from diﬀerent combinations of
(li, pi) and (lf, pf); we take all such possible output
tensors with lo ≤lmax and concatenate them. We also
divide the output of the sum over neighbors by
√
N,
where N denotes the average number of neighbors of
an atom.
To update the atomic features, the model
also uses dense layers that are applied in an atom-wise
fashion with weights shared across atoms, similar to
the self-interaction layers in SchNet [9]. While diﬀerent
weights are used for diﬀerent rotation orders, the same
set of weights is applied for all representation indices
m of a given tensor with rotation order l to maintain
equivariance.
The NequIP network architecture, shown in Figure 2,
is built on an atomic embedding, followed by a series of
interaction blocks, and ﬁnally an output block:
• Embedding: following SchNet, the initial feature
is generated using a trainable embedding that
operates on the atomic number Zi (represented
via a one-hot encoding) alone, implemented via a
trainable self-interaction layer.
• Interaction Block:
interaction blocks encode
interactions between neighboring atoms: the core
of this block is the convolution function, outlined in
equation 8. Features from diﬀerent tensor product
interactions that yield the same rotation and parity
pair (lo, po) are mixed by linear atom-wise self-
interaction layers.
We equip interaction blocks
with a ResNet-style update [34]: xk+1 = f(xk) +
Self-Interaction(xk), where f is the series of self-
interaction, convolution, concatenation, and self-
interaction.
The weights of the Self-Interaction
in the preceding formula are learned separately
for each species.
Finally, the mixed features
are processed by an equivariant SiLU-based gate
nonlinearity [28, 32] (even and odd scalars are not
gated, but instead are processed directly by SiLU
and tanh nonlinearities, respectively).
• Output Block: the l = 0 features of the ﬁnal
convolution are passed to an output block, which
consists of a set of two atom-wise self-interaction
layers.
For each atom the ﬁnal layer outputs a single scalar,
which is interpreted as the atomic potential energy.
These are then summed to give the total predicted
potential energy of the system (Equation 2). Forces are
subsequently obtained as the negative gradient of the
predicted total potential energy, thereby ensuring both
energy conservation and rotation-equivariant forces (see
equation 3).
Experiments
We validate the proposed method on a diverse series
of challenging data sets: ﬁrst we demonstrate that we
improve upon state-of-the-art accuracy on MD-17, a
data set of small, organic molecules that is widely used
for benchmarking ML-IPs [9, 11, 35–37].
Next, we
show that NequIP can accurately learn forces obtained
on small molecules at the quantum chemical CCSD(T)
level of theory [37].
To broaden the applicability of
the method beyond small isolated molecules, we ﬁnally
explore a series of extended systems with periodic
boundary conditions, consisting of both surfaces and bulk
materials: water in diﬀerent phases [15, 38], a chemical
reaction at a solid/gas interface, an amorphous Lithium
Phosphate [12], and a Lithium superionic conductor [13].
Details of the training procedure are provided in the
Methods section.
MD-17 small molecule dynamics
We ﬁrst evaluate NequIP on MD-17 [35–37], a data
set of small organic molecules in which reference values
of energy and forces are generated by ab-initio MD
simulations with DFT. Recently, a recomputed version
of the original MD-17 data with higher numerical
accuracy has been released, termed the revised MD-17
data set [39] (an example histogram of potential energies
and force components can be found in Appendix C). In
order to be able to compare results to a wide variety
of methods, we benchmark NequIP on both data sets.
For training and validation, we use a combined N=1,000
conﬁgurations. The mean absolute error in the energies
and force components is shown in Tables I and II. We
compare results using NequIP with those from published
leading MLIP models. We ﬁnd that NequIP signiﬁcantly
outperforms invariant GNN-IPs (such as SchNet [9]
and DimeNet [11]), shallow neural networks (such as
ANI [40]), and kernel-based approaches (such as GAP
[5], FCHL19/GPR [39, 41] and sGDML [37]). Finally,
we compare to a series of other methods including
ACE [42], SpookyNet [25], and GemNet [24] as well as
other equivariant neural networks such as PaiNN [22],
NewtonNet [23], and UNiTE [26]. Again, it should be
stressed that PaiNN and NewtonNet are lmax = 1-only
versions of equivariant networks. The results for ACE,
GAP, and ANI on the revised MD-17 data set are
those reported in [43].
Importantly, we train and test
separate NequIP models on both the original and the
revised MD-17 data set, and ﬁnd that NequIP obtains
signiﬁcantly lower energy errors on the revised data set,
while the force accuracy is similar on the two data sets.
In line with previous work [39], this suggests that the
noise ﬂoor on the original MD-17 data is higher on the
energies and that only the results on the revised MD-17

6
FIG. 2: The NequIP network architecture. Left: atomic numbers are embedded into l = 0 features, which are
reﬁned through a series of interaction blocks, creating scalar and higher-order tensor features. An output block then
generates atomic energies, which are pooled to give the total predicted energy. Middle: the interaction block,
containing the convolution. Right: the convolution combines the radial function R(r) which operates only on
interatomic distances with the spherical harmonic projection of the unit vector ˆrij via a tensor product.
data set should be used for comparing diﬀerent methods.
Remarkably, we ﬁnd that NequIP outperforms all
other methods. The consistent improvements in accuracy
compared to sGDML and FCHL19/GPR are particularly
surprising, as these are based on kernel methods, which
typically obtain better performance than deep neural
networks on small training sets. We run a convergence
scan on the rotation order l ∈{0, 1, 2, 3} and ﬁnd that
increasing the tensor rank beyond l = 1 gives a consistent
improvement. The signiﬁcant improvement from l = 0
to l = 1 highlights the crucial role of equivariance in
obtaining improved accuracy on this task.
Force training at quantum chemical accuracy
The
ability
to
achieve
high
accuracy
on
a
comparatively
small
data
set
facilitates
easier
development of Machine Learining Interatomic Potentials
on expensive high-order ab-initio quantum chemical
methods, such as e.g.
the coupled cluster method
CCSD(T). However, the high computational cost of
CCSD(T) has thus far hindered the use of reference
data structures at this level of theory, prohibited by the
need for large data sets that are required by available
NN-IPs. Leveraging the high data eﬃciency of NequIP,
we evaluate it on a set of molecules computed at
quantum chemical accuracy (aspirin at CCSD, all others
at CCSD(T)) [37] and compare the results to those
reported for sGDML [37] and GemNet [24] in table III.
Liquid Water and Ice Dynamics
To demonstrate the applicability of NequIP beyond
small molecules, we evaluate the method on a series of
extended systems with periodic boundary conditions. As
a ﬁrst example we use a joint data set consisting of liquid
water and three ice structures [15, 38] computed at the
PBE0-TS level of theory.
This data set [15] contains:
a) liquid water, P=1bar, T=300K, computed via path-
integral AIMD, b) ice Ih, P=1bar, T=273K, computed
via path-integral AIMD c) ice Ih, P=1bar, T=330K,
computed via classical AIMD d) ice Ih, P=2.13 kbar,
T=238K, computed via classical AIMD. A DeepMD NN-
IP model was previously trained [15] for water and ice
using a joint training set containing 133,500 reference
calculations of these four systems.
To assess data
eﬃciency of the NequIP architecture, we similarly train a
model jointly on all four parts of the data set, but using
only 133 structures for training, i.e. 1000x fewer data.
The 133 structures were sampled randomly following
a uniform distribution from the full data set available
online which consists of water and ice structures and is
made up of a total of 140,000 frames, coming from the
same MD trajectories that were used in the earlier work
[15]. Table IV compares the energy and force errors of

7
Molecule
SchNet
DimeNet
sGDML
PaiNN
SpookyNet
GemNet-(T/Q)
NewtonNet
UNiTE
NequIP (l=3)
Aspirin
Energy
16.0
8.8
8.2
6.9
6.5
-
7.3
-
5.7
Forces
58.5
21.6
29.5
14.7
11.2
9.4
15.1
6.8
8.2
Ethanol
Energy
3.5
2.8
3.0
2.7
2.3
-
2.6
-
2.2
Forces
16.9
10.0
14.3
9.7
4.1
3.7
9.1
4.0
3.8
Malonaldehyde
Energy
5.6
4.5
4.3
3.9
3.4
-
4.2
-
3.3
Forces
28.6
16.6
17.8
13.8
7.2
6.7
14.0
6.9
5.8
Naphthalene
Energy
6.9
5.3
5.2
5.0
5.0
-
5.1
-
4.9
Forces
25.2
9.3
4.8
3.3
3.9
2.2
3.6
2.8
1.6
Salicylic acid
Energy
8.7
5.8
5.2
4.9
4.9
-
5.0
-
4.6
Forces
36.9
16.2
12.1
8.5
7.8
5.4
8.5
4.2
3.9
Toluene
Energy
5.2
4.4
4.3
4.1
4.1
-
4.1
-
4.0
Forces
24.7
9.4
6.1
4.1
3.8
2.6
3.8
3.1
2.0
Uracil
Energy
6.1
5.0
4.8
4.5
4.6
-
4.6
-
4.5
Forces
24.3
13.1
10.4
6.0
5.2
4.2
6.5
4.2
3.3
TABLE I: Energy and Force MAE for molecules on the original MD-17 data set, reported in units of [meV] and
[meV/˚A], respectively, and a training budget of 1,000 reference conﬁgurations. For GemNet, the best result out of
the T/Q versions is presented and for PaiNN the best between force-only and joint force and energy training. For
UNiTE, we compare to the ”direct-learning” results reported in [26]. Best results are marked in bold.
Molecule
FCHL19
UNiTE
GAP
ANI
ACE
GemNet-(T/Q)
NequIP (l=0)
NequIP (l=1)
NequIP (l=2)
NequIP (l=3)
Aspirin
Energy
6.2
2.4
17.7
16.6
6.1
-
25.2
3.8
2.4
2.3
Forces
20.9
7.6
44.9
40.6
17.9
9.5
41.9
12.9
8.7
8.5
Azobenzene
Energy
2.8
1.1
8.5
15.9
3.6
-
20.3
1.1
0.8
0.7
Forces
10.8
4.2
24.5
35.4
10.9
-
42.3
5.6
4.2
3.6
Benzene
Energy
0.3
0.07
0.75
3.3
0.04
-
3.2
0.09
0.06
0.04
Forces
2.6
0.73
6.0
10.0
0.5
0.5
10.3
0.4
0.4
0.3
Ethanol
Energy
0.9
0.62
3.5
2.5
1.2
-
2.0
1.0
0.5
0.4
Forces
6.2
3.7
18.1
13.4
7.3
3.6
13.7
7.6
4.2
3.4
Malonaldehyde
Energy
1.5
1.1
4.8
4.6
1.7
-
4.4
1.6
0.9
0.8
Forces
10.2
6.6
26.4
24.5
11.1
6.6
23.4
10.4
6.0
5.2
Naphthalene
Energy
1.2
0.46
3.8
11.3
0.9
-
14.7
0.4
0.3
0.2
Forces
6.5
2.6
16.5
29.2
5.1
1.9
20.1
2.0
1.3
1.2
Paracetamol
Energy
2.9
1.9
8.5
11.5
4.0
-
17.5
2.1
1.4
1.4
Forces
12.2
7.1
28.9
30.4
12.7
-
37.6
10.8
6.9
6.9
Salicylic acid
Energy
1.8
0.73
5.6
9.2
1.8
-
11.4
1.0
0.8
0.7
Forces
9.5
3.8
24.7
29.7
9.3
5.3
28.7
5.7
4.2
4.0
Toluene
Energy
1.6
0.45
4.0
7.7
1.1
-
9.7
0.5
0.3
0.3
Forces
8.8
2.5
17.8
24.3
6.5
2.2
27.2
2.7
1.8
1.6
Uracil
Energy
0.6
0.58
3.0
5.1
1.1
-
10.0
0.6
0.4
0.4
Forces
4.2
3.8
17.6
21.4
6.6
3.8
25.8
4.1
3.0
3.2
TABLE II: Energy and Force MAE for molecules on the revised MD-17 data set, reported in units of [meV] and
[meV/˚A], respectively, and a training budget of 1,000 reference conﬁgurations. For GemNet, the best result out of
the T/Q versions is presented. For FCHL19, the best results between energy-only, force-only and joint force and
energy training are presented. For UNiTE, we compare to the ”direct-learning” results reported in [26]. Best results
are marked in bold.
Molecule
sGDML
GemNet-(T/Q)
NequIP (l=3)
Aspirin
Energy
6.9
-
2.0
Forces
33.0
10.3
8.3
Benzene
Energy
0.17
-
0.05
Forces
1.7
0.7
0.26
Ethanol
Energy
2.2
-
0.36
Forces
15.2
3.1
3.0
Malonaldehyde
Energy
2.6
-
0.72
Forces
16.0
5.9
4.5
Toluene
Energy
1.3
-
0.27
Forces
9.1
2.7
1.7
TABLE III: Energy and Force MAE for molecules at CCSD/CCSD(T) accuracy, reported in units of [meV] and
[meV/˚A], respectively, and a training budget of 1,000 reference conﬁgurations. For GemNet, the best result out of
the T/Q versions is presented.
NequIP trained on the 133 structures vs DeepMD trained
on 133,500 structures.
We ﬁnd that with 1000x fewer
training data NequIP signiﬁcantly outperforms DeepMD
on all four parts of the data set in the error on the
force components.
We note that there are 3N force
components for each training frame but only one energy
target. Consequently, one would except that on energies
the much larger training set used for DeepMD would
results in an even stronger diﬀerence. We ﬁnd that while
this is indeed the case, the NequIP results on the liquid
phase are surprisingly competitive.
Finally, we report
results using three diﬀerent weightings of energies and
forces in the loss function and see that increasing the
energy weighting results in signiﬁcantly improved energy

8
System
NequIP, a)
NequIP, b)
NequIP, c)
DeepMD
Liquid Water
Energy
-
1.6
1.7
1.0
Forces
12.5
51.4
12.2
40.4
Ice Ih (b)
Energy
-
2.5
4.3
0.7
Forces
10.8
57.8
10.4
43.3
Ice Ih (c)
Energy
-
3.9
10.2
0.7
Forces
12.5
29.1
12.2
26.8
Ice Ih (d)
Energy
-
2.6
12.7
0.8
Forces
10.3
24.1
10.1
25.4
TABLE IV: RMSE of energies and forces on liquid water and the three ices in units of [meV/molecule] and
[meV/A], with energy errors normalized by the number of molecules in the system. Note that the NequIP models
were trained on < 0.1% of the training data of DeepMD. NequIP model a) refers to loss function weighting
λF = 1, λE = 0, model b) to λF = 100, λE = 1, and model c) to λF = 100, 000, λE = 1.
Element
MAE
C
Forces
19.9
O
Forces
73.1
H
Forces
13.0
Cu
Forces
47.6
Total
Energy
0.50
Forces
38.4
TABLE V: Nequip MAE of energies and force
components for Formate on Cu system, per-element
basis. The training set consists of 2,500 structures,
units are [meV/atom] and [meV/A], respectively.
errors at the cost of a small increase in force error. We
note that the version of DeepMD published in [15] is not
smooth, and a smooth version has since been proposed
[44].
However, [44] does not report results on the
water/ice systems. It would be of interest to investigate
the performance of the smooth DeepMD version as a
function of training set size.
Heterogeneous catalysis of formate dehydrogenation
Next,
we
apply
NequIP
to
a
catalytic
surface
reaction.
In particular, we investigate the dynamics
of formate undergoing dehydrogenation decomposition
(HCOO∗→H∗+ CO2) on a Cu < 110 > surface
(see Figure 3).
This system is highly heterogeneous:
it has both metallic and covalent types of bonding
as well as charge transfer between the metal and the
molecule,
making it a particularly challenging test
system.
Diﬀerent states of the molecule also lead to
dissimilar C-O bond lengths [45, 46]. Training structures
consist of 48 Cu atoms and 4 atoms of the molecule
(HCOO* or CO2+H*).
The MAE of the predicted
forces using a NequIP model trained on 2,500 structures
is shown in Table V, demonstrating that NequIP is
able to accurately model the interatomic forces for this
complex reactive system. A more detailed analysis of the
resulting dynamics will be the subject of a separate study.
FIG. 3: Perspective view of atomic conﬁgurations of (a)
bidentate HCOO (b) monodentate HCOO and (c) CO2
and a hydrogen adatom on a Cu(110) surface. The
blue, red, black, and white spheres represent Cu, O, C,
and H atoms, respectively. The subset shown in each
subplot is the corresponding top view along the
< 110 > orientation.
Lithium Phosphate Amorphous Glass Formation
To examine the ability of the model to capture
dynamical properties,
we demonstrate that NequIP
can describe structural dynamics in amorphous lithium
phosphate with composition Li4P2O7. This material is
a member of the promising family of solid electrolytes
for Li-metal batteries [12, 47, 48], with non-trivial Li-
ion transport and phase transformation behaviors. The
data set consists of two 50ps-long AIMD simulations:
one of the molten structure at T=3000 K and another
of a quenched glass structure at T=600 K. We train
NequIP on a subset of 1,000 structures from the molten
trajectory.
Table VI shows the error in the force
components on both the test set from the AIMD molten
trajectory and the full AIMD quenched glass trajectory.
To then evaluate the physical ﬁdelity of the trained
model, we use it to run a set of ten MD simulations
of length 50 ps at T=600 K in the NVT ensemble and
compare the total radial distribution function (RDF)
without element distinction as well as the angular

9
FIG. 4: Quenched glass structure of Li4P2O7. The
insets show the P-O-O tetrahedral bond angle (bottom
left) as well as the O-P-P bridging angle between
corner-sharing phosphate tetrahedra (top right).
distribution functions (ADF) of the P-O-O (P central
atom) and O-P-P (O central atom) angles averaged
over ten runs to the ab-inito trajectory at the same
temperature.
The P-O-O angle corresponds to the
tetrahedral bond angle, while the O-P-P corresponds
to a bridging angle between corner-sharing phosphate
tetrahedra (Figure 4).
Figure 5 shows that NequIP
can accurately reproduce the RDF and the two ADFs,
in comparison with AIMD, after training on only 1,000
structures. This demonstrates that the model generates
the glass state and recovers its dynamics and structure
almost perfectly, despite having seen only the high-
temperature molten training data.
We also include
results from a longer NequIP-driven MD simulation of
500 ps, which can be found in Appendix A.
Lithium Thiophosphate Superionic Transport
To show that NequIP can model kinetic transport
properties from small training sets at high accuracy, we
study Li-ion diﬀusivity in LiPS (Li6.75P3S11) a crystalline
superionic Li conductor, consisting of a simulation cell of
83 atoms [13].
MD is widely used to study diﬀusion;
training a ML-IP to the accuracy required to predict
kinetic properties, however, has in the past required large
training set sizes ([49] e.g.
uses a data set of 30,874
structures to study Li diﬀusion in Li3PO4).
Here we
demonstrate that not only does NequIP obtain small
errors in the energies and force components, but it also
accurately predicts the diﬀusivity after training on a
data set obtained from an AIMD simulation.
Again,
we ﬁnd that very small training sets lead to highly
accurate models, as shown in Table VI for training set
sizes of 10, 100, 1,000 and 2,500 structures.
We run
a series of MD simulations with the NequIP potential
trained on 2,500 structures in the NVT ensemble at the
same temperature as the AIMD simulation for a total
simulation time of 50 ps and a time step of 0.25 fs, which
we found advantageous for the reliability and stability of
long simulations. We measure the Li diﬀusivity in these
NequIP-driven MD simulations (computed via the slope
of the mean square displacement) started from diﬀerent
initial velocities, randomly sampled from a Maxwell-
Boltzmann distribution.
We ﬁnd a mean diﬀusivity
of 1.25 x 10−5 cm2/s, in excellent agreement with the
diﬀusivity of 1.37 x 10−5 cm2/s computed from AIMD,
thus achieving a relative error of as little as 9%. Figure
6 shows the mean square displacements of Li for an
example run of NequIP in comparison to AIMD.
Data Eﬃciency
In
the
above
experiments,
NequIP
exhibits
exceptionally high data eﬃciency.
It is interesting
to consider the reasons for such high performance and
verify that it is connected to the equivariant nature
of the model.
First, it is important to note that
each training conﬁguration contains multiple labels:
in particular, for a training set of M ﬁrst-principles
calculations with structures consisting of N atoms, the
energies and force components together give a total of
M(3N + 1) labels.
In order to gain insight into the
reasons behind increased accuracy and data eﬃciency,
we perform a series of experiments with the goal of
isolating the eﬀect of using equivariant convolutions.
In particular, we run a set of experiments in which we
explicitly turn on or oﬀinteractions of higher order than
l = 0.
This deﬁnes two settings: ﬁrst, we train the
network with the full set of tensor features up to a given
order l and the corresponding equivariant interactions.
Second, we turn oﬀall interactions involving l > 0,
making the network a conventional invariant GNN-IP,
involving only invariant convolutions over scalar features
in a SchNet-style fashion.
As a ﬁrst test system we choose bulk water:
in
particular we use the data set introduced in [50].
We
train a series of networks with identical hyperparameters,
but vary the training set sizes between 10 and 1,000
structures.
As shown in Figure 7, we ﬁnd that the
equivariant
networks
with l
∈
1, 2, 3
signiﬁcantly
outperform the invariant networks with l
=
0 for
all data set sizes as measured by the MAE of force
components. This suggests that it is indeed the use of
tensor features and equivariant convolutions that enables
the high sample eﬃciency of NequIP. In addition, it

10
FIG. 5: Left: Radial Distribution Function, middle: Angular Distribution Function, tetrahedral bond angle, right:
Angular Distribution Function, bridging oxygen. All are deﬁned as probability density functions; NequIP results are
averaged over 10 runs with diﬀerent initial velocities.
System
Data set size
MAE
RMSE
LiPS
10
Energy
2.03
2.54
Forces
109.9
142.0
LiPS
100
Energy
0.44
0.56
Forces
28.4
36.8
LiPS
1,000
Energy
0.12
.15
Forces
8.3
11.2
LiPS
2,500
Energy
0.08
0.10
Forces
4.9
6.6
Li4P2O7, melt
1,000
Energy
0.4
0.8
Forces
38.6
62.7
Li4P2O7, quench
1,000
Energy
0.5
0.5
Forces
24.8
38.1
TABLE VI: NequIP E/F MAE/RMSE for LiPS and Li4P2O7 for diﬀerent data set sizes in units of [meV/A] and
[meV/atom]. The model for Li4P2O7 was trained exclusively on structures from the melted trajectory. The reported
test errors for the melt are computed on the remaining set of structures from the full melt trajectory; errors for the
quench are computed on the full quench trajectory.
FIG. 6: Comparison of the Lithium mean square
displacement of AIMD and an example NequIP
trajectory.
is apparent that the learning curves of equivariant
networks have a diﬀerent slope in log-log space. It has
been observed that learning curves typically follow a
power-law of the form [51]: ϵ ∝aN b where ϵ and N refer
to the generalization error and the number of training
points, respectively.
The exponent of this power-law
(or equivalently the slope in log-log space) determines
how fast a learning algorithm learns as new data
become available.
Empirical results have shown that
this exponent typically remains ﬁxed across diﬀerent
learning algorithms for a given data set, and diﬀerent
methods only shift the learning curve, leaving the log-log
slope unaﬀected [51].
The same trend can also be
observed for various methods on the aspirin molecule in
the MD-17 data set (see Figure 8) where across a series
of descriptors and regression models (sGDML, FCHL19,
and PhysNet [10, 37, 41]) the learning curves show an
approximately similar log-log slope (results obtained
from http://quantum-machine.org/gdml/#datasets).
To our surprise, we observe that the equivariant NequIP
networks break this pattern.
Instead they follow a
log-log slope with larger magnitude, meaning that they
learn faster as new data become available. An invariant
l = 0 NequIP network, however, displays a similar log-log
slope to other methods, suggesting that it is indeed the
equivariant nature of NequIP that allows for the change
in learning behavior.
Further increasing the rotation
order l beyond l = 1 again only shifts the learning curve
and does not results in an additional change in log-log
slope. To control for the diﬀerent number of weights and
features in orders of diﬀerent rotation order l, we report
weight- and feature-controlled data in Appendix B. Both

11
show qualitatively the same eﬀect. The Appendix also
contains results on the behavior of the energies, when
trained jointly with forces. For details on the training
setup and the control experiments, see the Methods
section.
We further note, that in [50], a Behler-Parrinello
Neural Network (BPNN) was trained on 1303 structures,
yielding a RMSE of ≈120 meV/˚A in forces when
evaluated on the remaining 290 structures.
We ﬁnd
that NequIP l = 2 models trained with as little as 100
and 250 data points obtain RMSEs of 129.8 meV/˚A
and 103.4 meV/˚A
respectively (note that Figure 7
shows the MAE). This provides further evidence that
NequIP exhibits signiﬁcantly improved data eﬃciency in
comparison with existing methods.
DISCUSSION
We
demonstrate
that
the
Neural
Equivariant
Interatomic Potential (NequIP), a new type of graph
neural network built on E(3)-equivariant convolutions,
exhibits state-of-the-art accuracy and exceptional data
eﬃciency on data sets of small molecules and periodic
materials.
We isolate that the improvements are due
to the introduction of equivariant representations in
place of more widely used invariant representations.
This raises questions about the optimal way to include
symmetry
in
Machine
Learning
for
molecules
and
materials. A better understanding of why equivariance
enables improved sample eﬃciency and accuracy is
likely to be a fruitful direction towards designing
better ML algorithms for the construction of Potential
Energy Surfaces. In addition to open questions around
the eﬀect of equivariance on accuracy and learning
dynamics, a clear theoretical understanding of how the
many-body character of interactions arises in Message
Passing Interatomic Potentials remains elusive.
We
expect the proposed method will enable researchers in
computational chemistry, physics, biology, and materials
science
to
conduct
molecular
dynamics
simulations
of complex reactions and phase transformations at
increased accuracy and eﬃciency.
METHODS
Software.
All
experiments
were
run
with
the
nequip
software
available
at
github.com/
mir-group/nequip
in
version
0.3.3,
git
commit
50ddbfc31bd44e267b7bb7d2d36d76417b0885ec.
In
addition, the e3nn library [31] was used under version
0.3.5,
PyTorch under version 1.9.0 [52],
PyTorch
Geometric under version 1.7.2 [53], and Python under
version 3.9.6.
Reference Data Sets.
original
MD-17:
MD-17
[35–37]
is
a
data
set
of
eight
small
organic
molecules,
obtained
from
MD simulations at T=500K and computed at the
PBE+vdW-TS
level
of
electronic
structure
theory,
resulting
in
data
set
sizes
between
133,770
and
993,237 structures.
The data set was obtained from
http://quantum-machine.org/gdml/#datasets.
For
each molecule, we use 950 conﬁgurations for training
and 50 for validation, sampled uniformly from the full
data set, and evaluate the test error on all remaining
conﬁgurations in the data set.
revised MD-17:
The revised MD-17 data set is
a
recomputed
version
of
MD-17
obtained
at
the
PBE/def2-SVP level of theory. Using a very tight SCF
convergence as well as a very dense DFT integration
grid, 100,000 structures [39] of the original MD-17 data
set were recomputed. The data set can be downloaded at
https://figshare.com/articles/dataset/Revised_
MD17_dataset_rMD17_/12672038.
For each molecule,
we use 950 conﬁgurations for training and 50 for
validation, sampled uniformly from the full data set, and
evaluate the test error on all remaining conﬁgurations in
the data set.
Molecules@CCSD/CCSD(T):
The
data
set
of
small
molecules
at
CCSD
and
CCSD(T)
accuracy
[37] contains positions, energies, and forces for ﬁve
diﬀerent small molecules:
Asprin (CCSD), Benzene,
Malondaldehyde,
Toluene,
Ethanol
(all
CCSD(T)).
Each data set consists of 1,500 structures with the
exception of Ethanol, for which 2,000 structures are
available.
For more detailed information, we direct
the reader to [37].
The data set was obtained from
http://quantum-machine.org/gdml/#datasets.
The
training/validation set consists of a total of 1,000
molecular structures which we split into 950 for training
and 50 for validation (sampled uniformly), and we test
the accuracy on all remaining structures (we use the
train/test split provided with the data set, but further
split the training set into training and validation sets).
Liquid Water and Ice: The data set of liquid waters
and ice structures [15, 38] was generated from classical
AIMD and path-integral AIMD simulations at diﬀerent
temperatures and pressures, computed with a PBE0-TS
functional [15]. The data set contains a total of 140,000
structures, of which 100,000 are liquid water and 20,000
are Ice Ih b),10,000 are Ice Ih c), and another 10,000 are
Ice Ih d). The liquid water system consists of 64 H2O
molecules (192 atoms), while the ice structures consist
of 96 H2O molecules (288 atoms). We use a validation

12
FIG. 7: Log-log plot of the predictive error on the water data set from [50] using NequIP with l ∈{0, 1, 2, 3} as a
function of training set size, measured via the force MAE. The equivariant networks display a diﬀerent scaling
behavior than the invariant network.
set of 50 frames and report the test accuracy on all
remaining structures in the data set.
Formate decomposition on Cu:
The decomposition
process
of
formate
on
Cu
involves
conﬁgurations
corresponding to the cleavage of the C-H bond, initial
and intermediate states (monodentate, bidentate formate
on Cu < 110 >) and ﬁnal states (H ad-atom with a
desorbed CO2 in the gas phase). Nudged elastic band
(NEB) method was ﬁrst used to generate an initial
reaction path of the C-H bond breaking.
12 short
ab initio molecular dynamics, starting from diﬀerent
NEB images, were run to collect a total of 6855 DFT
structures. The CP2K [54] code was employed for the
AIMD simulations. Each trajectory was generated with
a time step of 0.5 fs and 500 total steps.
We train
NequIP on 2,500 reference structures sampled uniformly
from the full data set of 6,855 structures, use a validation
set of 250 structures and evaluate the mean absolute
error on all remaining structures. Due to the unbalanced
nature of the data set (more atoms of Cu than in the
molecule), we use a per-element weighed loss function
in which atoms C, O1, O2, and H and the sum of all
Cu atoms all receive equal weights. We weight the force
term with N 2
atoms = 2, 704 and the energy term with 1.
Li4P2O7 glass:
The Li4P2O7 ab-initio data were
generated using an ab-initio melt-quench MD simulation,
starting with a stoichiometric crystal of 208 atoms (space
group P21/c) in a periodic box of 10.4 × 14.0 × 16.0
˚A. The dynamics used the Vienna Ab-Initio Simulation
Package (VASP) [55–57], with a generalized gradient
PBE functional [58], projector augmented wave (PAW)
pseudopotentials [59], a NVT ensemble and a Nos´e-
Hoover thermostat, a time step of 2 fs, a plane-wave
cutoﬀof 400 eV, and a Γ-point reciprocal-space mesh.
The crystal was melted at 3000 K for 50 ps, then
immediately quenched to 600 K and run for another
50 ps.
The resulting structure was conﬁrmed to be
amorphous by plotting the radial distribution function
of P-P distances.
The training was performed only
on the molten portion, and the MD simulations for
a quenched simulation.
We sample the training sets
uniformly from the full data set of 25,000 AIMD frames.
We use a validation set of 100 structures, and evaluate
the model on all remaining structures of the melt
trajectory as well as on the full quench trajectory.
LiPS:
Lithium
phosphorus
sulﬁde
(LiPS)
based
materials
are
known
to
exhibit
high
lithium
ion
conductivity,
making them attractive as solid-state
electrolytes for lithium-ion batteries. Other examples of
known materials in this family of superionic conductors
are LiGePS and LiCuPS-based compounds. The training

13
FIG. 8: Log-log plot of the predictive error on the aspirin molecule in MD-17 using NequIP with l ∈{0, 1, 2, 3} as a
function of data set size used for training and validation, measured via the force MAE. The plot also shows errors of
a series of other methods, including both kernel and deep-learning approaches.
data set is taken from a previous study on graph neural
network force ﬁeld [13], where the LiPS training data
were generated using ab-initio MD of an LiPS structure
with Li-vacancy (Li6.75P3S11) consisting of 27 Li, 12 P,
and 44 S atoms respectively.
The structure was ﬁrst
equilibrated and then run at 520 K using the NVT
ensemble for 50 ps with a 2.0 fs time step. The full data
set contains 25,001 MD frames. We choose training set
sizes of 10, 100, 1,000, and 2,500 frames with a ﬁxed
validation set size of 100.
Liquid Water, Cheng et al.:
The training set used
in the data eﬃciency experiments on water consists
of 1,593 reference calculations of bulk liquid water at
the revPBE0-D3 level of accuracy, with each structure
containing 192 atoms,
as given in [50].
Further
information can be found in [50].
The data set was
obtained from https://github.com/BingqingCheng/
ab-initio-thermodynamics-of-water.
We sample
the training set uniformly from the full data set and for
each experiment also use a validation set consisting of
100 structures. We then evaluate the error on a ﬁxed
hold-out test set of 190 structures.
Molecular Dynamics Simulations.
To run MD
simulations, NequIP force outputs were integrated with
the Atomic Simulation Environment (ASE) [33] in which
we implement a custom version of the Nos´e-Hoover
thermostat. We use this in-house implementation for the
both the Li4P2O7 as well as the LiPS MD simulations.
The thermostat parameter was chosen to match the
temperature ﬂuctuations observed in the AIMD run.
The RDF and ADFs for Li4P2O7 were computed with
a maximum distance of 6 ˚A (RDF) and 2.5 ˚A (both
ADFs). The Li4P2O7 MD simulations were started from
the ﬁrst frame of the AIMD quench simulation and the
LiPS simulation was started from the ﬁrst frame of the
reference AIMD simulation of the corresponding training
data.
Training. Networks are trained using a loss function
based on a weighted sum of energy and a force loss terms:

14
L = λE|| ˆE −E||2+λF
1
3N
N
X
i=1
3
X
α=1
||−∂ˆE
∂ri,α
−Fi,α||2 (9)
where N is the number of atoms in the system, ˆE is
the predicted potential energy, and λE and λF are the
energy- and force-weightings, respectively.
While it is
helpful to optimize the weightings as a hyperparameter,
we found a relative weighting of energies to forces
of 1 to N 2
atoms a suitable default choice.
Here the N
accounts for the fact that that potential energy is a global
quantity, while the atomic forces are local quantities and
the square accounts for the fact that we use a MSE loss.
This also makes the loss function size invariant. A full
set of the weightings used in this work can be found in
table VII.
We normalize the target energies by subtracting the
mean potential energy over the training set and scale
both the target energies and target force components by
the root mean square of the force components over the
training set. The predicted atomic energies ˆEi are scaled
and shifted by two learnable per-species parameters
before summing them for the total predicted potential
energy ˆE:
ˆE =
X
i
σsi ˆEi + λsi
(10)
where σsi and λsi are learnable per-species parameters
indexed by si, the species of atom i. They are initialized
to 1 and 0, respectively.
For the case of the joint training on water and ice,
since the liquid water and ice structures have diﬀerent
numbers of atoms, we do not scale or shift the potential
energy targets or force targets.
Instead, we initialize
the learnable per-species shift to the mean per-atom
energy and initialize the learnable per-species scale to
the average standard deviation over all force components
in the training set.
Learning Curve Experiments For learning curve
experiments on the aspirin molecule in MD-17, a series
of NequIP models with increasing order l ∈{0, 1, 2, 3}
were trained on varying data set sizes.
In particular,
experiments were performed with a budget for training
and validation of 200, 400, 600, 800, 1000 conﬁgurations,
of which 50 samples were used for validation while the
remaining ones were used for training.
The reported
test error was computed on the entire remaining MD-17
trajectory for each given budget. The weight-controlled
version of NequIP was set up by creating a l = 0
network with increased feature size that matches the
number of weights up to approx.
0.1% of the l = 1
network.
The feature-controlled version of NequIP
was set up by creating a l = 0 network with the same
number of features as the l = 1 network, i.e. 4x more
features than the original l = 0 network (1 scalar and
3 vector features), in particular the l = 1 network had
a feature conﬁguration of 64x0o + 64x0e + 64x1o +
64x1e while the original l = 0 network used 64x0e and
feature-controlled l = 0 network used 512x0e.
Hyperparameters.
All models were trained on a
NVIDIA Tesla V100 GPU in single-GPU training. For
the small molecule systems, we use 5 interaction blocks,
a learning rate of 0.01 and a batch size of 5. For the
periodic systems, we use 6 interaction blocks, a learning
rate of 0.005 and a batch size of 1.
We decrease the
initial learning rate by a decay factor of 0.8 whenever the
validation loss in the forces has not seen an improvement
for 50 epochs. We continuously save the model with the
best validation loss in the forces and use the model with
the overall best validation loss for evaluation on the test
set and MD simulations. For validation and test error
evaluation, we use an exponential moving average of the
training weights with weight 0.99. Training is stopped if
either of the following conditions is met: a) a maximum
training time of of 7 days is reached; b) a maximum
number 1,000,000 epochs is reached; c) the learning rate
drops below 10−6; d) the validation loss does not improve
for 1,000 epochs. We note that competitive results can
typically be obtained within a matter of hours or often
even minutes and most of the remaining training time
is spent on only small improvements in the errors. We
found the use of small batch sizes to be an important
hyperparameter. We also found it important to choose
the radial cutoﬀdistance rc appropriately for a given
system. In addition, we observed the number of layers to
not have a strong eﬀect as long as they were set within
a reasonable range. We use diﬀerent numbers of l and
feature dimensions for diﬀerent systems and similarly
also vary the cutoﬀradius for diﬀerent systems. A full
outline of the choices for l, feature size, cutoﬀradius as
well as the weights for energies and forces in the loss
function can be found in VII. All models were trained
with both even and odd features.
The weights were
initialized according to a standard normal distribution
(for details, see the e3nn software implementation [31]).
The invariant radial networks act on a trainable Bessel
basis of size 8 and were implemented with 3 hidden
layers of 64 neurons with SiLU nonlinearities between
them.
The even scalars of the ﬁnal interaction block
are passed to the output block, which ﬁrst reduces
the feature dimension to 16 even scalars through a
self-interaction layer.
Finally, through another self-
interaction layer, the feature dimension is reduced to
a single scalar output value associated with each atom
which is then summed over to give the total potential
energy.
Forces are obtained as the negative gradient
of this predicted total potential energy, computed via

15
automatic diﬀerentiation.
All models were optimized
with Adam with the AMSGrad variant in the PyTorch
implementation [60–62] with β1 = 0.9, β2 = 0.999,
and ϵ = 10−8 without weight decay.
The average
number of neighbors used for the
1
√
N normalization
of the convolution was computed over the full training
set.
For all molecular results, the average number of
neighbors was computed once on the N=1000 case for
revised MD-17 and used for all other experiments. For
the water sample eﬃciency and the LiPS experiments it
was computed once on the N=1000 and N=2500 cases,
respectively and then used for all other experiments for
that system. All input ﬁles for training of NequIP models
will be shared upon publication.
DATA AVAILABILITY
An open-source software implementation of NequIP is
available at https://github.com/mir-group/nequip.
All data sets will be made available upon publication.
[1] Richards,
W. D. et al.
Design and synthesis of
the superionic conductor na 10 snp 2 s 12.
Nature
communications 7, 1–8 (2016).
[2] Boero, M., Parrinello, M. & Terakura, K. First principles
molecular dynamics study of ziegler- natta heterogeneous
catalysis. Journal of the American Chemical Society 120,
2746–2752 (1998).
[3] Lindorﬀ-Larsen, K., Piana, S., Dror, R. O. & Shaw, D. E.
How fast-folding proteins fold.
Science 334, 517–520
(2011).
[4] Behler,
J.
&
Parrinello,
M.
Generalized
neural-
network representation of high-dimensional potential-
energy surfaces.
Physical review letters 98, 146401
(2007).
[5] Bart´ok, A. P., Payne, M. C., Kondor, R. & Cs´anyi,
G.
Gaussian approximation potentials: The accuracy
of quantum mechanics, without the electrons. Physical
review letters 104, 136403 (2010).
[6] Shapeev, A. V.
Moment tensor potentials:
A class
of
systematically
improvable
interatomic
potentials.
Multiscale Modeling & Simulation 14, 1153–1173 (2016).
[7] Thompson, A. P., Swiler, L. P., Trott, C. R., Foiles, S. M.
& Tucker, G. J. Spectral neighbor analysis method for
automated generation of quantum-accurate interatomic
potentials. Journal of Computational Physics 285, 316–
330 (2015).
[8] Vandermause, J. et al.
On-the-ﬂy active learning of
interpretable bayesian force ﬁelds for atomistic rare
events. npj Computational Materials 6, 1–11 (2020).
[9] Sch¨utt,
K.
et
al.
Schnet:
A
continuous-ﬁlter
convolutional neural network for modeling quantum
interactions.
In
Advances
in
neural
information
processing systems, 991–1001 (2017).
[10] Unke, O. T. & Meuwly, M. Physnet: A neural network for
predicting energies, forces, dipole moments, and partial
charges. Journal of chemical theory and computation 15,
3678–3693 (2019).
[11] Klicpera, J., Groß, J. & G¨unnemann, S.
Directional
message passing for molecular graphs.
arXiv preprint
arXiv:2003.03123 (2020).
[12] Mailoa, J. P. et al.
A fast neural network approach
for direct covariant forces prediction in complex multi-
element extended systems. Nature machine intelligence
1, 471–479 (2019).
[13] Park,
C. W. et al.
Accurate and scalable multi-
element graph neural network force ﬁeld and molecular
dynamics with direct force architecture. arXiv preprint
arXiv:2007.14444 (2020).
[14] Artrith,
N. & Kolpak,
A. M.
Understanding the
composition and activity of electrocatalytic nanoalloys
in aqueous solvents: A combination of dft and accurate
neural network potentials. Nano letters 14, 2670–2676
(2014).
[15] Zhang, L., Han, J., Wang, H., Car, R. & Weinan, E. Deep
potential molecular dynamics: a scalable model with the
accuracy of quantum mechanics. Physical review letters
120, 143001 (2018).
[16] Smith, J. S., Isayev, O. & Roitberg, A. E.
Ani-1: an
extensible neural network potential with dft accuracy at
force ﬁeld computational cost. Chemical science 8, 3192–
3203 (2017).
[17] Gilmer, J., Schoenholz, S. S., Riley, P. F., Vinyals, O.
& Dahl, G. E.
Neural message passing for quantum
chemistry. arXiv preprint arXiv:1704.01212 (2017).
[18] Anderson, B., Hy, T. S. & Kondor, R.
Cormorant:
Covariant molecular neural networks.
In Advances in
Neural Information Processing Systems, 14537–14546
(2019).
[19] Townshend, R. J., Townshend, B., Eismann, S. & Dror,
R. O.
Geometric prediction:
Moving beyond scalars.
arXiv preprint arXiv:2006.14163 (2020).
[20] Thomas, N. et al.
Tensor ﬁeld networks:
Rotation-
and translation-equivariant neural networks for 3d point
clouds. arXiv preprint arXiv:1802.08219 (2018).
[21] Batzner,
S. et al.
Se(3)-equivariant graph neural
networks for data-eﬃcient and accurate interatomic
potentials. arXiv preprint arXiv:2101.03164v1 (2021).
[22] Sch¨utt, K. T., Unke, O. T. & Gastegger, M. Equivariant
message passing for the prediction of tensorial properties
and molecular spectra. arXiv preprint arXiv:2102.03150
(2021).
[23] Haghighatlari, M. et al.
Newtonnet:
A newtonian
message passing network for deep learning of interatomic
potentials and forces. arXiv preprint arXiv:2108.02913
(2021).
[24] Klicpera, J., Becker, F. & G¨unnemann, S.
Gemnet:
Universal
directional
graph
neural
networks
for
molecules. arXiv preprint arXiv:2106.08903 (2021).
[25] Unke, O. T. et al. Spookynet: Learning force ﬁelds with
electronic degrees of freedom and nonlocal eﬀects. Nature
Communications 12, 7273 (2021). URL https://doi.
org/10.1038/s41467-021-27504-0.
[26] Qiao, Z. et al. Unite: Unitary n-body tensor equivariant
network with applications to quantum chemistry. arXiv
preprint arXiv:2105.14655 (2021).
[27] Grisaﬁ, A., Wilkins, D. M., Willatt, M. J. & Ceriotti,
M. Atomic-scale representation and statistical learning
of
tensorial
properties.
In
Machine
Learning
in
Chemistry: Data-Driven Algorithms, Learning Systems,

16
Data Set
Tensor rank l
# Features
rc
λE
λF
MD-17
3
64
4.0
1
1,000
revMD-17
{0, 1, 2, 3}
64
4.0
1
1,000
CCSD/CCSD(T)
3
64
4.0
1
1,000
Water+Ices, DeepMD
2
32
6.0
see IV
see IV
Formate on Cu
2
32
5.0
1
2,704
Li4P2O7
2
32
5.0.
1
43,264
LiPS
2
32
5.0
1
6,889
Water, Cheng et al.
{0, 1, 2, 3}
32
4.5
1
36,864
TABLE VII: Tensor rank l, feature size, radial cutoﬀin units of [˚A], as well as energy and force weights used in the
joint loss function. All models were trained with even and odd features, i.e. a tensor rank of l = 1 and 32 features
corresponds to 32x0o + 32x0e + 32x1o + 32x1e. The force weightings for formate on Cu, LiPO, LiPS, and the water
system for sample eﬃciency tests stem from N 2
atoms.
and Predictions, 1–21 (ACS Publications, 2019).
[28] Weiler, M., Geiger, M., Welling, M., Boomsma, W. &
Cohen, T. S. 3d steerable cnns: Learning rotationally
equivariant features in volumetric data.
In Advances
in Neural Information Processing Systems, 10381–10392
(2018).
[29] Kondor, R. N-body networks: a covariant hierarchical
neural
network
architecture
for
learning
atomic
potentials. arXiv preprint arXiv:1803.01588 (2018).
[30] Kondor, R., Lin, Z. & Trivedi, S.
Clebsch–gordan
nets: a fully fourier space spherical convolutional neural
network. In Advances in Neural Information Processing
Systems, 10117–10126 (2018).
[31] Geiger, M. et al. e3nn/e3nn: 2021-05-04 (2021). URL
https://doi.org/10.5281/zenodo.4735637.
[32] Hendrycks, D. & Gimpel, K. Gaussian error linear units
(gelus). arXiv preprint arXiv:1606.08415 (2016).
[33] Larsen,
A.
H.
et
al.
The
atomic
simulation
environment—a python library for working with atoms.
Journal of Physics:
Condensed Matter
29,
273002
(2017). URL https://doi.org/10.1088%2F1361-648x%
2Faa680e.
[34] He, K., Zhang, X., Ren, S. & Sun, J.
Deep residual
learning for image recognition.
In Proceedings of
the IEEE conference on computer vision and pattern
recognition, 770–778 (2016).
[35] Chmiela, S. et al. Machine learning of accurate energy-
conserving molecular force ﬁelds.
Science advances 3,
e1603015 (2017).
[36] Sch¨utt, K. T., Arbabzadah, F., Chmiela, S., M¨uller,
K. R. & Tkatchenko, A. Quantum-chemical insights from
deep tensor neural networks.
Nature Communications
8,
13890 (2017).
URL https://doi.org/10.1038/
ncomms13890.
[37] Chmiela,
S.,
Sauceda,
H.
E.,
M¨uller,
K.-R.
&
Tkatchenko, A.
Towards exact molecular dynamics
simulations with machine-learned force ﬁelds.
Nature
Communications 9, 3887 (2018).
[38] Ko, H.-Y. et al. Isotope eﬀects in liquid water via deep
potential molecular dynamics. Molecular Physics 117,
3269–3281 (2019).
[39] Christensen, A. S. & von Lilienfeld, O. A. On the role
of gradients for machine learning of molecular energies
and forces. Machine Learning: Science and Technology
1, 045018 (2020).
[40] Devereux, C. et al. Extending the applicability of the ani
deep learning molecular potential to sulfur and halogens.
Journal of Chemical Theory and Computation 16, 4192–
4202 (2020).
[41] Christensen, A. S., Bratholm, L. A., Faber, F. A. &
Anatole von Lilienfeld, O.
Fchl revisited: Faster and
more accurate quantum machine learning. The Journal
of Chemical Physics 152, 044107 (2020).
[42] Drautz, R. Atomic cluster expansion for accurate and
transferable interatomic potentials.
Physical Review B
99, 014104 (2019).
[43] Kov´acs, D. P. et al.
Linear atomic cluster expansion
force ﬁelds for organic molecules: beyond rmse. Journal
of Chemical Theory and Computation (2021).
[44] Zhang, L. et al. End-to-end symmetry preserving inter-
atomic potential energy model for ﬁnite and extended
systems.
Advances in Neural Information Processing
Systems 31 (2018).
[45] Sim, W. S., Gardner, P. & King, D. A.
Multiple
bonding conﬁgurations of adsorbed formate on ag111.
The Journal of Physical Chemistry 100, 12509–12516
(1996). 00000.
[46] Wang, G., Morikawa, Y., Matsumoto, T. & Nakamura,
J. Why is formate synthesis insensitive to copper surface
structures? The Journal of Physical Chemistry B 110,
9–11 (2006). 00050.
[47] Yu, X., Bates, J. B., Jellison, G. E. & Hart, F. X. A
stable thin-ﬁlm lithium electrolyte: Lithium phosphorus
oxynitride. Journal of The Electrochemical Society 144,
524–532 (1997).
URL https://doi.org/10.1149/1.
1837443.
[48] Westover, A. S. et al.
Plasma synthesis of spherical
crystalline and amorphous electrolyte nanopowders for
solid-state batteries. ACS Applied Materials & Interfaces
12, 11570–11578 (2020).
URL https://doi.org/10.
1021/acsami.9b20812.
[49] Li, W., Ando, Y., Minamitani, E. & Watanabe, S. Study
of li atom diﬀusion in amorphous li3po4 with neural
network potential. The Journal of chemical physics 147,
214106 (2017).
[50] Cheng, B., Engel, E. A., Behler, J., Dellago, C. &
Ceriotti, M. Ab initio thermodynamics of liquid and solid
water. Proceedings of the National Academy of Sciences
116, 1110–1115 (2019).
[51] Hestness, J. et al. Deep learning scaling is predictable,
empirically. arXiv preprint arXiv:1712.00409 (2017).

17
[52] Paszke, A. et al.
Pytorch: An imperative style, high-
performance deep learning library. In Advances in neural
information processing systems, 8026–8037 (2019).
[53] Fey, M. & Lenssen, J. E.
Fast graph representation
learning
with
pytorch
geometric.
arXiv
preprint
arXiv:1903.02428 (2019).
[54] Hutter,
J.,
Iannuzzi,
M.,
Schiﬀmann,
F.
&
VandeVondele,
J.
cp2k:
atomistic simulations of
condensed matter systems.
WIREs Computational
Molecular Science 4, 15–25 (2014). 00000.
[55] Kresse, G. & Hafner, J. Ab initiomolecular dynamics for
liquid metals.
Physical Review B 47, 558–561 (1993).
URL https://doi.org/10.1103/physrevb.47.558.
[56] Kresse, G. & Furthm¨uller, J.
Eﬃciency of ab-initio
total energy calculations for metals and semiconductors
using a plane-wave basis set. Computational Materials
Science 6, 15–50 (1996).
URL https://doi.org/10.
1016/0927-0256(96)00008-0.
[57] Kresse, G. & Furthm¨uller, J. Eﬃcient iterative schemes
forab initiototal-energy calculations using a plane-wave
basis set.
Physical Review B 54, 11169–11186 (1996).
URL https://doi.org/10.1103/physrevb.54.11169.
[58] Perdew, J. P., Burke, K. & Ernzerhof, M. Generalized
gradient approximation made simple.
Physical Review
Letters 77, 3865–3868 (1996). URL https://doi.org/
10.1103/physrevlett.77.3865.
[59] Kresse, G. & Joubert, D. From ultrasoft pseudopotentials
to the projector augmented-wave method.
Physical
Review B 59, 1758–1775 (1999). URL https://doi.org/
10.1103/physrevb.59.1758.
[60] Kingma, D. P. & Ba, J. Adam: A method for stochastic
optimization. arXiv preprint arXiv:1412.6980 (2014).
[61] Loshchilov, I. & Hutter, F.
Decoupled weight decay
regularization. arXiv preprint arXiv:1711.05101 (2017).
[62] Reddi, S. J., Kale, S. & Kumar, S. On the convergence
of adam and beyond. arXiv preprint arXiv:1904.09237
(2019).
ACKNOWLEDGEMENTS
We thank Jonathan Vandermause, Cheol Woo Park,
David Clark, Kostiantyn Lapchevskyi, Joshua Rackers,
and Benjamin Kurt Miller for helpful discussions.
Work at Harvard was supported by Bosch Research
and
the
Integrated
Mesoscale
Architectures
for
Sustainable Catalysis (IMASC), an Energy Frontier
Research Center funded by the US Department of
Energy (DOE), Oﬃce of Science, Oﬃce of Basic Energy
Sciences under Award No. DE-SC0012573 and by Award
No. DE-SC0022199. N.M. and B.K. are supported by
a
Multidisciplinary
University
Research
Initiative,
sponsored by the Oﬃce of Naval Research, under Grant
N00014-20-1-2418.
Work at Bosch Research was partially supported by
ARPA-E Award No. DE-AR0000775 and used resources
of the Oak Ridge Leadership Computing Facility at Oak
Ridge National Laboratory, which is supported by the
Oﬃce of Science of the Department of Energy under
Contract DE-AC05-00OR22725.
T.E.S. was supported by the Laboratory Directed
Research
and
Development
Program
of
Lawrence
Berkeley
National
Laboratory
and
the
Center
for
Advanced
Mathematics
for
Energy
Research
Applications, both under U.S. Department of Energy
Contract No. DE-AC02-05CH11231.
A.M is supported by U.S. Department of Energy, Oﬃce
of Science, Oﬃce of Advanced Scientiﬁc Computing
Research, Computational Science Graduate Fellowship
under Award Number(s) DE-SC0021110.
The
authors
acknowledge
computing
resources
provided by the Harvard University FAS Division of
Science Research Computing Group and by the Texas
Advanced Computing Center (TACC) at The University
of Texas at Austin under allocations DMR20009 and
DMR20013.
AUTHOR CONTRIBUTIONS
S.B. initiated the project,
conceived the NequIP
model, implemented the software and conducted all
software experiments under the guidance of B.K. A.M.
contributed to the development of the model and
the software implementation.
L.S. created the data
set and helped with MD simulations of formate/Cu,
and contributed to the development of the model
and its software implementation.
M.G. contributed
to the development of the model and the software
implementation.
J.P.M. contributed to analyzing the
LiPS conductor results and implemented the thermostat
for MD simulations together with S.B. M.K. generated
the AIMD data set of Li4P2O7, wrote software for
the analysis of MD results and contributed to the
benchmarking on this system. N.M. wrote software for
the estimation of diﬀusion coeﬃcients and contributed
to the interpretation of results.
T.E.S. contributed to
the conception of the model, guidance of computational
experiments
and
software
implementation.
B.K.
supervised the project from conception to design of
experiments, implementation, theory, as well as analysis
of data.
All authors contributed to writing the
manuscript.
COMPETING INTERESTS
The authors declare no competing interests.

18
APPENDIX A: LONG MOLECULAR DYNAMICS
SIMULATION OF LI4P2O7
Figure 9 shows the Radial Distribution Function
obtained from a MD simulation of simulation length
500ps compared to the AIMD simulation of 50ps. For
both simulations, there ﬁrst 10ps were not used in the
computation of the RDF.
APPENDIX B: LEARNING CURVES
Figure 10 shows energy errors as a function of training
set size on the data set from [50]. Figure 11 shows force
errors as a function of training set size on the apsirin
molecule in the MD-17 data set, together with weight-
and feature-controlled version of the l = 0 network.
APPENDIX C: REVISED MD-17 DATA SET
Figures 12 and 13 show histograms of the energy and
force labels on aspirin in the revised MD-17 data set,
respectively.

19
FIG. 9: Comparison of the Radial Distribution of a simulation of Li4P2O7 of length 500 ps driven by NequIP in
comparison to a 50 ps simulation driven by AIMD.
FIG. 10: Log-log plot of the predictive error on the water data set from [50] using NequIP with l ∈{0, 1, 2, 3} as a
function of training set size, measured via the energy MAE.

20
FIG. 11: Log-log plot of the predictive error on the aspirin molecule in MD-17 using NequIP with l ∈{0, 1, 2, 3} as a
function of data set size, measured via the force MAE. The plots also shows the weight- and feature-controlled
version of NequIP.

21
FIG. 12: Histogram of potential energies of all structures used for training and validation for the aspirin molecule in
the revised MD-17 data set. The mean energy was subtracted before plotting.

22
FIG. 13: Histogram of force components of all structures used for training and validation for the aspirin molecule in
the revised MD-17 data set.

