HISTORY 
SCHMIDHUBER 
THE SWISS AI LAB 
JÜRGEN 
IDSIA - USI & SUPSI  
NNAISENSE 

Jürgen Schmidhuber              
The Swiss AI Lab IDSIA         
Univ. Lugano & SUPSI  
http://www.idsia.ch/~juergen 
True Artificial 
Intelligence Will 
Change Everything 
 
NNAISENSE 

Jürgen Schmidhuber 
You_again Shmidhoobuh 

Learn & improve learning 
algorithm itself, and also the 
meta-learning algorithm, etc… 
My diploma thesis (1987): 
first concrete design of 
recursively self-improving AI 

With Hochreiter, Gers, Graves, Fernandez, Gomez, Bayer… 
1997-2009. Since 2015 on your phone! Google, Microsoft, IBM, others, all use LSTM now 

First: a beautiful simple 
pattern (discovered in 2014) 
of exponential acceleration of 
the most important events in 
the history of the universe 
from a human perspective 

Pattern starts at       
Ω - 13.8 B years: 
Big Bang 
http://www.idsia.ch/~juergen/history.html 
Ultimate Trend 
Will history converge 
around 2050 = Ω? 

Ω - 3.5 B years: 
Life 
Ω - ¼ of this time 

Ω - 0.9 B years: 
Animal-like life 
Ω - ¼ of this time 

Ω - 220 M years: 
Mammals 
Ω - ¼ of this time 

Ω - 55 M years: 
Primates 
Ω - ¼ of this time 

Ω - 13 M years: 
Hominids 
Ω - ¼ of this time 

Ω - 3.5 M years: 
Stone tools 
Ω - ¼ of this time 

Ω - 850,000 years: 
Controlled fire 
Ω - ¼ of this time 

Ω - 210,000 years: 
Anatomically   
modern man 
Ω - ¼ of this time 

Ω - 50,000 years: 
Behaviorallly   
modern man 
Ω - ¼ of this time 

Ω - 13,000 years: 
Neolithic revolution 
Ω - ¼ of this time 

Ω - 3,300 years: 
Iron age 
Ω - ¼ of this time 

Ω - 800 years: 
Guns & rockets 
Ω - ¼ of this time 

Ω - 200 years: 
Industrial revolution 
Ω - ¼ of this time 

Ω - 50 years (now): 
Information revolution 
Ω - ¼ of this time 

Ω - 12 years 
Small computers   
with 1 brain power 
Ω - 3 years 
? 
Ω - 9 months 
?? 
Ω - 10 weeks 
???? 
???????? 
http://www.idsia.ch/~juergen/history.html 

RNNAISSANCE 
SCHMIDHUBER 
THE SWISS AI LAB 
JÜRGEN 
IDSIA - USI & SUPSI  
NNAISENSE 

Deep Learning is a half century old although recent 
“tabloid science” stories claim it is a recent thing 
888 references, 88 pages:  
http://www.idsia.ch/~juergen/deep-learning-overview.html 

Critique (also at Google+) of paper by self-proclaimed “deep learning conspiracy” 
 (LeCun & Bengio & Hinton) who cite each other but not the pioneers of the field: 
http://www.idsia.ch/~juergen/deep-learning-conspiracy.html 
 

 Father of Deep Learning 
 Ivakhnenko et al, since 1965 
Deep multilayer perceptrons with 
polynomial activation functions 
Incremental layer-wise training by 
regression analysis - learn 
numbers of layers and units per 
layer - prune superfluous units 
8 layers already back in 1971 
still used in the 2000s 

http://www.idsia.ch/~juergen/firstdeeplearner.html 
Dechter, 1986 (ML) 
Aizenberg et al, 2000 (NNs) 
Gomez & Schmidhuber (2005): 
first NN paper with word 
combination “learn deep” in title 

http://www.idsia.ch/~juergen/who-invented-backpropagation.html 

Continuous BP in Euler-LaGrange Calculus + Dynamic 
Programming: Bryson 1961, Kelley 1960. BP through 
chain rule only: Dreyfus 1962. `Modern BP’ in sparse, 
discrete, NN-like nets: Linnainmaa 1970. Weight 
changes: Dreyfus 1973. Automatic differentiation: 
Speelpenning 1980. BP applied to NNs: Werbos 1982. 
Experiments & internal representations: Rumelhart et al 
86. RNNs: e.g., Williams, Werbos, Robinson, 1980s... 
Supervised Backpropagation (BP) 
….. 

The deepest NNs: 
RNNs are general computers 
Learn program = weight matrix  
http://www.idsia.ch/~juergen/rnn.html 

COMPARE:  HOCHREITER & BENGIO & FRASCONI & SCHMIDHUBER, 2001 
http://www.idsia.ch/~juergen/fundamentaldeeplearningproblem.html 

http://www.idsia.ch/~juergen/firstdeeplearner.html 
Schmidhuber 1991: Unsupervised 
pretraining for Hierarchical 
Temporal Memory: stack of RNN 
è history compression èspeed 
up supervised learning. 
Compare feedforward NN case: 
AutoEncoder stacks (Ballard 
1987) and Deep Belief NNs 
(Hinton et al 2006) 

With Hochreiter, Gers, Graves, Fernandez, Gomez, Bayer… 
1997-2007. Since 2015 on your phone! Google, Microsoft, IBM, others, all use LSTM now 

Long    
Short-Term 
Memory 
LSTM: no 
vanishing 
gradients 
Gradient-based LSTM variants (1997, 1999, 2001, 2005, 2007,…) learn many previously 
unlearnable Deep Learning tasks: context-sensitive grammars, music composition, R-
Learning robots, metalearning, speech recognition (vs HMMs/GMMs), protein prediction, 
connected handwriting, machine translation…. No bias towards recent or ancient events! 
Red: linear unit: 
self-weight 1.0: 
transports error 
across 1000s of 
time steps. 
Green: gates 
open / protect 
access. Blue: 
multiplications 
http://www.idsia.ch/~juergen/rnn.html 

Ex-PhD students (TUM & IDSIA): 
Sepp Hochreiter (PhD 1999) 
Felix Gers (PhD 2001) 
Alex Graves (PhD 2008) 
Daan Wierstra (PhD 2010) 
Justin Bayer (2009), others 
Postdocs at IDSIA (2000s): 
Fred Cummins 
Santiago Fernandez 
Faustino Gomez 
Others 
 
Today’s LSTM RNNs shaped by: 

Connectionist 
Temporal 
Classification (CTC): 
Graves, Fernandez, 
Gomez, Schmidhuber 
ICML 2006 
No pre-segmented 
data; RNN 
maximises 
probability of 
training set label 
sequences 
Alex 

Alex 
http://www.idsia.ch/~juergen/handwriting.html 
LSTM: First RNN     
to win contests:         
3 ICDAR 2009 
connected 
handwriting 
competitions 
E.g., Graves & 
Schmidhuber      
NIPS 2010 

LSTM for speech: 2003 as good 
as HMMs, 2007: LSTM stack gets 
best results on keyword spotting in 
a large corpus (vs HMMs). Today: 
best large vocabulary speech 
recognition … 
Fernandez 
Graves 
Schmidhuber 
ICANN 2007 
IJCAI 2007 

BAIDU’s DeepSpeech uses our  
CTC-based RNNs for end-to-end 
speech recognition without any 
HMMs / GMMs (Hannun et al., 
Baidu, 2014); broke Switchboard 
benchmark record 
Santiago 
Fernandez 
Since 2003/2007: Speech 
Recognition Revolution through 
RNNs, e.g., Graves et al, ICASSP 
2013: best results on TIMIT 
through LSTM. Google 2014: best 
large vocabulary speech rec. 

A dozen of the many 2014/2015 benchmark records 
with LSTM RNNs, often at major IT companies: 
1. Large vocabulary speech recognition (Sak et al., Google, Interspeech 2014) 
2. English to French translation (Sutskever et al., Google, NIPS 2014) 
3. Text-to-speech synthesis (Fan et al., Microsoft, Interspeech 2014) 
4. Prosody contour prediction (Fernandez et al., IBM, Interspeech 2014) 
5. Google Voice improved by 49% (Sak et al, 2015, now for >1 billion users) 
6. Syntactic parsing for NLP (Vinyals et al., Google, 2014) 
7. Photo-real talking heads (Soong and Wang, Microsoft, ICASSP 2015) 
8. Social signal classification (Brueckner & Schulter, ICASSP 2014) 
9. Arabic handwriting recognition (Bluche et al., DAS 2014) 
10. Image caption generation (Vinyals et al., Google, 2014) 
11. Keyword spotting (Chen et al., Google, ICASSP 2015) 
12. Video to textual description (Donahue et al., 2014; Li Yao et al., 2015) 
http://www.idsia.ch/~juergen/rnn.html 

1993: Gradient-
based meta-
RNNs that can 
learn to run their 
own weight 
change 
algorithm:          
J.  Schmidhuber. 
A self-referential 
weight matrix. 
ICANN 1993 
This was before LSTM. In 2001, however, Hochreiter taught a meta-LSTM to 
learn a learning algorithm for quadratic functions that was faster than backprop  

MNIST: 60,000 digits for 
training, 10,000 for testing, 7 
layer MLP; >12m weights; 
train 200 days on CPU = 5 on 
GPU; >1015 weight updates, 
5B/s, 2010: new world record 
0.35% (Ciresan et al.) Since 
then: decline of unsupervised 
pre-training for FNNs, like in 
the 1990s for RNNs 
Two old ideas:  backprop 
(3-5 decades old), training 
pattern deformations (Baird, 
1990, 2 decades old) 

1990s: Trend from 
unsupervised to 
supervised RNNs 
 
2000s: Trend from 
unsupervised to 
supervised FNNs 
 
Unsupervised è Supervised 
Both trends driven by our team 

Our Deep GPU-Based Max-Pooling CNNs (IJCAI 2011) 
e.g., http://www.idsia.ch/~juergen/deeplearning.html 
Alternating convolutional and subsampling layers (CNNs): Fukushima 1979. Backprop for 
CNNs: LeCun et al 1989. Max-pooling (MP): Weng 1992. Backprop for MPCNNs: Ranzato et 
al 2007, Scherer et al 2010, GPU-MPCNNs - Ciresan et al (Swiss AI Lab IDSIA, 2011) 

ICDAR 2011 offline 
Chinese handwriting 
recognition contest 
(4000 classes):             
1st & 2nd rank 
Oct 2013: again best 
results, first near-
human performance 
(Ciresan & 
Schmidhuber, 2013) 
http://www.idsia.ch/~juergen/
handwriting.html 

Ensembles of deep sparse CNNs 
+ Max-Pooling + MLP on top: 1 
year on CPU = 1 week on GPU. 
2011-2012: first human-
competitive MNIST result: 0.2% 
(after almost a decade of ~0.4%). 
Ciresan, Meier, Masci, 
Gambardella, Schmidhuber, IJCAI 
2011, IJCNN 2011, CVPR 2012 

http://www.idsia.ch/~juergen/superhumanpatternrecognition.html 
Traffic Sign Contest, Silicon Valley, 2011:  
Our GPU-MPCNN was twice better than humans 
3 times better than closest artificial competitor 
6 times better than best non-neural thing: FIRST 

IJCNN 2011 traffic sign  
recognition competition,        
Silicon Valley, 2011:  
1ST (0.56% ERROR)  
2ND HUMANS (1.16%)  
3RD (1.69%) 
4TH  (3.86%) 
Ciresan, Meier, Masci, 
Schmidhuber, IJCNN 2011,  
Neural Networks, 2012 
Very similar GPU-MPCNNs later 
used for ImageNet (Krizhevsky & 
Hinton 2012, Zeiler & Fergus 
2013, …) 

Ernst 
Dickmanns,    
the robot       
car pioneer,  
Munich, 80s 
1995: Munich to 
Denmark and 
back on public 
Autobahns, up to 
180 km/h, no 
GPS, passing 
other cars 
Robot Cars 
2014: 20 year anniversary of 
self-driving cars in highway traffic 
http://www.idsia.ch/~juergen/robotcars.html 

http://www.idsia.ch/~juergen/deeplearningwinsbraincontest.html 
Our Deep Learner 
Won ISBI 2012 Brain 
Image Segmentation 
Contest: 
First feedforward 
Deep Learner to win 
an image 
segmentation 
competition 
(but compare deep 
recurrent LSTM 
2009: segmentation 
& classification) 
Our Deep Learner 
Won ISBI 2012 Brain 
Image Segmentation 
Contest: 
First feedforward 
Deep Learner to win 
an image 
segmentation 
competition 
(but compare deep 
recurrent LSTM 
2009: segmentation 
& classification) 

Our Deep Learner Won ICPR 
2012 Contest on Mitosis 
Detection: First pure Deep 
Learner to win a contest on 
object detection (in large 
images). Very fast MPCNN 
scans: Masci, Giusti, Ciresan, 
Gambardella, Schmidhuber, 
ICIP 2013 
 

Thanks to Dan Ciresan & Alessandro Giusti 
http://www.idsia.ch/~juergen/deeplearningwinsMICCAIgrandchallenge.html 

• First recurrent NN to win contests (2009) 
• First NN to win connected handwriting contests (2009) 
• First outperformance of humans in a computer vision contest (2011) 
• First deep NN to win Chinese handwriting contest (2011) 
• European handwriting (MNIST): old error record almost halved (2011) 
• First deep NN to win image segmentation contest (2012) 
• First deep NN to win object detection contest (2012) 
• First deep NN to win medical imaging contest (2012) 
• First RNN controller that reinforcement learns from raw video (2013) 
• … 
Some of Our Deep Learning “Firsts” 

Image caption generation with LSTM RNNs translating internal representations of CNNs 
(Vinyals, Toshev, Bengio, Erhan, Google, 2014)  

Best Segmentation with PyramMiD-LSTM (NIPS 2015) 
Stollenga, Byeon, 
Liwicki, Schmidhuber 

LSTM learns knot-tying tasklets: 
Mayr Gomez Wierstra Nagy Knoll 
Schmidhuber, IROS’06 

Finds Complex Neural Controllers with a Million Weights – RAW VIDEO INPUT! 
Faustino Gomez, Jan Koutnik, Giuseppe Cuccu, J. Schmidhuber, GECCO 2013 
Reinforcement Learning in 
Partially Observable Worlds 

http://www.idsia.ch/~juergen/compressednetworksearch.html 

Octopus-arm control: 82 in, 32 out, 3'680 weights, only 
20 DCT coefficients, compression 1:184 
Octopus-arm with low-level vision, 32x32 in, 32 out, 
33'824 weights, 160 DCT, compression 1:211 
TORCS driving video game, low-level vision, 64x64 in, 3 
out, 1'115'139 weights, 200 DCT, compression 1:5575 
http://www.idsia.ch/~juergen/compressednetworksearch.html 

The first 4 members of DeepMind include 2 former PhD students of my lab. But I am not 
happy with their Nature paper, although 3 of its authors were trained here, because others at 
IDSIA published Reinforcement Learning with high-dimensional video input earlier 

No new NN winter, because physics dictates that future 
hardware will be 3D-RNN-like: many processors 
connected by many short and few long wires 
http://www.idsia.ch/~juergen/rnn.html 

IJCNN 1990, NIPS 1991: Reinforcement Learning      
with Recurrent Controller & Recurrent World Model 
A bit 
like 
AIXI, 
but with 
feasible 
local 
search 

IJNS 1991: R-Learning of Visual Attention 
on 1,000,000 times slower computers 

1991: current goal=extra fixed input 
2015: all of this is coming back! 

RoboCup World Champion 2004, Fastest League, 5m/s 
Alex @ IDSIA, led  
FU Berlin’s RoboCup 
World Champion 
Team 2004 
Lookahead expectation & planning with neural networks 
(Schmidhuber, IEEE INNS 1990):  successfully used for 
RoboCup by Alexander Gloye-Förster (went to IDSIA) 
http://www.idsia.ch/~juergen/learningrobots.html 

Formal theory of fun & novelty & 
surprise & attention & creativity & 
curiosity & art & science & humor 
Maximize Future Fun(Data X,O(t))~ 
∂CompResources(X,O(t))/∂t 
E.g., Connection Science 18(2):173-187, 2006 
IEEE Transactions AMD 2(3):230-247, 2010 
http://www.idsia.ch/~juergen/creativity.html 

PowerPlay not only solves but also continually 
invents problems at the borderline between what's 
known and unknown - training an increasingly 
general problem solver by continually searching for 
the simplest still unsolvable problem 

Jürgen Schmidhuber              
The Swiss AI Lab IDSIA         
Univ. Lugano & SUPSI  
http://www.idsia.ch/~juergen 
True Artificial 
Intelligence Will 
Change Everything 
 
NNAISENSE 

Next: build small 
animal-like AI that 
learns to think and 
plan hierarchically 
like a crow or a 
capuchin monkey 
 
Evolution 
needed billions 
of years for this, 
then only a few 
more millions 
for humans 
 

neural networks-based 
artificial intelligence 

Open Source Neural Networks Library by my PhD students K Greff and R Srivastava 
http://people.idsia.ch/~juergen/brainstorm.html 


