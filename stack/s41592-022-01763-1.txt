Nature Methods | Volume 20 | March 2023 | 400–402
400
nature methods
Brief Communication
https://doi.org/10.1038/s41592-022-01763-1
EnzymeML: seamless data flow and modeling 
of enzymatic data
Simone Lauterbach1, Hannah Dienhart1, Jan Range1, Stephan Malzacher2,3, 
Jan-Dirk Spöring 
  2,3, Dörte Rother2,3, Maria Filipa Pinto 
  4, Pedro Martins4, 
Colton E. Lagerman5, Andreas S. Bommarius 
  5, Amalie Vang Høst6, 
John M. Woodley 
  6, Sandile Ngubane 
  7, Tukayi Kudanga 
  7, 
Frank T. Bergmann 
  8, Johann M. Rohwer 
  9, Dorothea Iglezakis 
  10, 
Andreas Weidemann11, Ulrike Wittig 
  11, Carsten Kettner 
  12, Neil Swainston13, 
Santiago Schnell 
  14 & Jürgen Pleiss 
  1 
The design of biocatalytic reaction systems is highly complex owing to 
the dependency of the estimated kinetic parameters on the enzyme, 
the reaction conditions, and the modeling method. Consequently, 
reproducibility of enzymatic experiments and reusability of enzymatic 
data are challenging. We developed the XML-based markup language 
EnzymeML to enable storage and exchange of enzymatic data such as 
reaction conditions, the time course of the substrate and the product, 
kinetic parameters and the kinetic model, thus making enzymatic data 
findable, accessible, interoperable and reusable (FAIR). The feasibility and 
usefulness of the EnzymeML toolbox is demonstrated in six scenarios, for 
which data and metadata of different enzymatic reactions are collected 
and analyzed. EnzymeML serves as a seamless communication channel 
between experimental platforms, electronic lab notebooks, tools for 
modeling of enzyme kinetics, publication platforms and enzymatic reaction 
databases. EnzymeML is open and transparent, and invites the community 
to contribute. All documents and codes are freely available at  
https://enzymeml.org.
International efforts by the scientific community to improve rigor and 
reproducibility of experiments are essential to increase confidence in 
research results and advance scientific research. The reproducibility 
crisis in the biomedical sciences1 and catalytic sciences2 also exists 
in the fields of enzymology and biocatalysis; it mainly results from 
incomplete reporting of both the reaction conditions and experimental 
results3, as well as from the lack of accepted best practices, standards 
in experimentation4 and kinetic modeling5. In biocatalysis, one of the 
Received: 4 November 2020
Accepted: 21 December 2022
Published online: 9 February 2023
 Check for updates
1Institute of Biochemistry and Technical Biochemistry, University of Stuttgart, Stuttgart, Germany. 2Institute of Bio- and Geosciences 1, 
Forschungszentrum Jülich, Jülich, Germany. 3Aachen Biology and Biotechnology, RWTH Aachen University, Aachen, Germany. 4i3S, Instituto de 
Investigação e Inovação em Saúde da Universidade do Porto, University of Porto, Porto, Portugal. 5School of Chemical and Biomolecular Engineering, 
Georgia Institute of Technology, Atlanta, GA, USA. 6Department of Chemical and Biochemical Engineering, Technical University of Denmark, Kgs Lyngby, 
Denmark. 7Department of Biotechnology and Food Science, Durban University of Technology, Durban, South Africa. 8BioQUANT/COS, Heidelberg 
University, Heidelberg, Germany. 9Department of Biochemistry, Stellenbosch University, Stellenbosch, South Africa. 10Information and Communication 
Center, University of Stuttgart, Stuttgart, Germany. 11Heidelberg Institute for Theoretical Studies, Heidelberg, Germany. 12Beilstein-Institut, Frankfurt am 
Main, Germany. 13Institute of Systems, Molecular and Integrative Biology, University of Liverpool, Liverpool, UK. 14Department of Biological Sciences and 
Department of Applied and Computational Mathematics and Statistics, University of Notre Dame, Notre Dame, IN, USA.  
 e-mail: Juergen.Pleiss@itb.uni-stuttgart.de

Nature Methods | Volume 20 | March 2023 | 400–402
401
Brief Communication
https://doi.org/10.1038/s41592-022-01763-1
documents can be downloaded and directly used to compare results 
from different groups, to perform reanalysis using alternative kinetic 
models, or to design experiments based on the reaction conditions or 
the kinetic parameters, thus making enzymatic data reusable.
The feasibility and usefulness of the EnzymeML toolbox were 
demonstrated in six scenarios, in which the catalytic activity of single 
enzymes or enzyme cascades was investigated by measuring the time 
course of substrate or product concentration. The EnzymeML docu-
ment was created from the EnzymeML spreadsheet or from BioCatHub, 
and kinetic modeling was performed using different rate equations 
within Jupyter Notebook or using the modeling platforms COPASI, 
PySCeS and interferENZY. The results were uploaded to DaRUS (data 
repository of the University of Stuttgart) or to SABIO-RK. For a detailed 
description of the scenarios, see Supplementary Information.
Without a standardized data exchange format, the manual extrac-
tion of published enzymatic data, data acquisition upon measure-
ment, the analysis and modeling of experimental data, and the manual 
uploading of results into an enzyme reaction database are laborious 
and error-prone processes. The EnzymeML toolbox increases the effi-
ciency and quality of data management. Once the complete metadata 
and measured data are collected and documented in a standardized, 
reusable format during measurement and data acquisition, these meta-
data and data are available during the whole data life cycle until publica-
tion and reuse by other research groups without further reformatting, 
thus making data management more efficient and less error prone. The 
standardization by the EnzymeML format serves as a checklist for the 
comprehensive documentation of relevant metadata, facilitating the 
replicability of the experiment3. A standardized, machine-readable 
data exchange format also constitutes the core of laboratory automa-
tion and experimental design strategies, and enables the planning and 
subsequent analysis of high-throughput experimentation.
To report the details of the data management process, we recom-
mend using Jupyter Notebook as an easy-to-use, modular and versatile 
platform for the implementation and documentation of scalable, 
user-specific workflows, including the transfer, integration, visualiza-
tion, analysis and modeling of data. The comprehensive documenta-
tion of the measured data and the data analysis workflow contributes 
to establishing best practices in experimentation and kinetic modeling, 
because kinetic parameters depend on the kinetic model16,17 and the 
details of the modeling method4. Using a machine-readable format, dif-
ferent kinetic models and modeling methods can be easily compared, 
and the subsequent design of experiments is simplified.
Most data to be collected via EnzymeML will be used to curate 
mechanistic models and the relevant parameters of a given model under 
defined conditions. Interestingly, some aspects of biocatalysis remain 
more empirical. An interesting example is that in many cases the stabil-
ity of an enzyme under non-natural conditions is lacking a mechanistic 
understanding18. Nevertheless, curating large amounts of data about 
enzyme stabilities under various conditions might ultimately allow pat-
terns to be examined, which could also lead to hypotheses being pro-
posed for testing and validation. The EnzymeML toolbox will provide the 
necessary first step to allow reaction models to be developed and shared. 
This has enormous value for scientists but can also serve as a valuable 
first step for engineers interested in scaling reactions. By integrating the 
reaction models with mass balance models, well-documented enzymatic 
data can be used to predict scalability of processes and even provide the 
basis for a techno-economic assessment19.
We are aware that many critical experimental conditions and 
methods are not yet included in the EnzymeML document. Therefore, 
additional information will be included in the future, such as the prepa-
ration of the biocatalyst, details of the modeling process, or reactions 
catalyzed by enzyme cascades or whole-cell biocatalysts. EnzymeML 
can also be integrated with other standardized markup languages. 
Because EnzymeML is based on SBML, the Systems Biology Ontology20 
is used, which facilitates the integration with other existing ontologies.
primary causes for the incompleteness of data and metadata is the 
current practice of data management. Data acquisition, curation and 
documentation are generally performed manually using spreadsheets 
and laboratory notebooks, either paper-based or electronic. These 
tools do not guarantee completeness, as quality annotation takes time. 
As a consequence of poor annotation, experimental results typically 
lack essential information on data acquisition and analysis. In addition, 
passing experimental data to an analysis tool, exchanging with a project 
partner or publishing can be an error-prone process, because, owing 
to the lack of a common exchange format, ad hoc solutions involv-
ing legacy data formats with poor metadata structure, such as CSV,  
are used.
With the goal of supporting biocatalysis research in address-
ing rigor and reproducibility, we (a group of enzymologists, enzyme 
engineers and bioinformaticians) have developed a domain-specific 
data exchange format, EnzymeML, which streamlines data storage, 
interchange and analysis6,7. EnzymeML enables the FAIR (findable, 
accessible, interoperable and reusable) representation of experimen-
tal data8 and follows the Standards of Reporting Enzymology Data 
(STRENDA9) guidelines, which are recommended by more than 55 
international biochemistry journals (http://www.beilstein-institut.de/
en/projects/strenda/journals). To embrace extensibility and to exploit 
existing software packages, EnzymeML builds on the well-established 
Systems Biology Markup Language (SBML)10 but includes additional 
variables and metadata for enzyme catalysis. For example, it specifies 
the enzyme, substrate(s), product(s) and reaction conditions; reports 
the measured time course of substrate or product concentrations; and 
links experimental information with the selected kinetic model and 
estimated kinetic parameters.
The EnzymeML toolbox supports FAIR data management. It ena-
bles a researcher to collect data using the EnzymeML spreadsheet or the 
web-based electronic laboratory notebook BioCatHub11, and these data 
are converted to a machine-readable, structured document. EnzymeML 
takes advantage of the rich SBML-based ecosystem for the simulation 
and analysis of biochemical networks (for example, COPASI12 and 
PySCeS13) and of SBML-compatible databases of enzymatic data (for 
example, STRENDA DB14 and SABIO-RK15), and can be directly read 
by modeling tools or databases, without the need to copy and paste, 
or reformat the data. Therefore, the EnzymeML toolbox enables a 
seamless flow of enzymatic data from data acquisition to modeling to 
publication, making enzymatic data fully interoperable (Fig. 1).
The EnzymeML documents can be made publicly available by stor-
ing them as datasets in an EnzymeML Dataverse and can be referred 
to by a digital object identifier (DOI), thus making enzymatic data 
accessible. In addition, EnzymeML-specific metadata blocks in the 
EnzymeML Dataverse as well as the entries in specialized databases 
such as SABIO-RK make the EnzymeML documents findable. EnzymeML 
Data
publication
Data
acquisition
Modeling
visualization
Fig. 1 | Overview of the workflow and tools that were implemented in this 
work. First, data are acquired using the EnzymeML spreadsheet or the web-based 
electronic laboratory notebook BioCatHub, and are converted to EnzymeML. 
Second, the EnzymeML document is distributed to modeling platforms and 
visualization tools. Last, the EnzymeML document, including experimental and 
modeling data, is uploaded to DaRUS (Dataverse) and SABIO-RK.

Nature Methods | Volume 20 | March 2023 | 400–402
402
Brief Communication
https://doi.org/10.1038/s41592-022-01763-1
The creation of a new entry in existing databases (SABIO-RK and 
STRENDA DB) still requires manual, labor-intensive uploading of results 
by the authors or by the database curators. The possibility of an auto-
mated database upload of an EnzymeML document will encourage 
researchers to contribute their data actively to public repositories, 
thus making them findable and accessible. In addition, one or more 
EnzymeML documents can serve as supporting information for pub-
lications. However, the currently existing databases do not store the 
originally measured time-course data. Therefore, each EnzymeML 
document can also be stored as a dataset with a specific DOI on one 
of the more than 70 Dataverse installations worldwide and can be 
downloaded as a ready-to-use EnzymeML document. As all EnzymeML 
documents belonging to an experiment can be stored, not only data-
sets with positive results are made available. Therefore, EnzymeML 
contributes to the digitalization of catalytic sciences, thereby accel-
erating bioprocess development, reducing costs and improving the 
overall quality of research.
Online content
Any methods, additional references, Nature Portfolio reporting sum-
maries, source data, extended data, supplementary information, 
acknowledgements, peer review information; details of author con-
tributions and competing interests; and statements of data and code 
availability are available at https://doi.org/10.1038/s41592-022-01763-1.
References
1.	
Iqbal, S. A., Wallach, J. D., Khoury, M. J., Schully, S. D. & Ioannidis, 
J. P. A. Reproducible research practices and transparency across 
the biomedical literature. PLoS Biol. 14, e1002333 (2016).
2.	
Wulf, C. et al. A unified research data infrastructure for catalysis 
research—challenges and concepts. ChemCatChem 13,  
3223–3236 (2021).
3.	
Halling, P. et al. An empirical analysis of enzyme function 
reporting for experimental reproducibility: missing/incomplete 
information in published papers. Biophys. Chem. 242,  
22–27 (2018).
4.	
Stroberg, W. & Schnell, S. On the estimation errors of KM and 
V from time-course experiments using the Michaelis–Menten 
equation. Biophys. Chem. 219, 17–27 (2016).
5.	
Cvijovic, M. et al. Bridging the gaps in systems biology.  
Mol. Genet. Genomics 289, 727–734 (2014).
6.	
Pleiss, J. Standardized data, scalable documentation, sustainable 
storage—EnzymeML as a basis for fair data management in 
biocatalysis. ChemCatChem 13, 3909–3913 (2021).
7.	
Range, J. et al. EnzymeML—a data exchange format for 
biocatalysis and enzymology. FEBS J. 289, 5864–5874 (2022).
8.	
Wilkinson, M. D. et al. The FAIR Guiding Principles for scientific 
data management and stewardship. Sci. Data 3, 160018 (2016).
9.	
Tipton, K. F. et al. Standards for reporting enzyme data: the 
STRENDA Consortium: what it aims to do and why it should be 
helpful. Perspect. Sci. 1, 131–137 (2014).
10.	 Hucka, M. et al. The systems biology markup language (SBML): a 
medium for representation and exchange of biochemical network 
models. Bioinformatics 19, 524–531 (2003).
11.	
Malzacher, S., Range, J., Halupczok, C., Pleiss, J. & Rother, D. 
BioCatHub, a graphical user interface for standardized data 
acquisition in biocatalysis. Chem. Ing. Tech. 92, 1251–1251 (2020).
12.	 Hoops, S. et al. COPASI—a complex pathway simulator. 
Bioinformatics 22, 3067–3074 (2006).
13.	 Christensen, C. D., Hofmeyr, J. H. S. & Rohwer, J. M. 
PySCeSToolbox: a collection of metabolic pathway analysis tools. 
Bioinformatics 34, 124–125 (2018).
14.	 Swainston, N. et al. STRENDA DB: enabling the validation and 
sharing of enzyme kinetics data. FEBS J. 285, 2193–2204 (2018).
15.	 Wittig, U., Rey, M., Weidemann, A., Kania, R. & Müller, W. 
SABIO-RK: an updated resource for manually curated 
biochemical reaction kinetics. Nucleic Acids Res. 46,  
D656–D660 (2018).
16.	 Bezerra, R. M. F. & Dias, A. A. Discrimination among eight modified 
Michaelis–Menten kinetics models of cellulose hydrolysis with a 
large range of substrate/enzyme ratios: inhibition by cellobiose. 
Appl. Biochem. Biotechnol. 112, 173–184 (2004).
17.	 Buchholz, P. C. F., Ohs, R., Spiess, A. C. & Pleiss, J. Progress 
curve analysis within BioCatNet: comparing kinetic models for 
enzyme-catalyzed self-ligation. Biotechnol. J. 14, e1800183 (2019).
18.	 Dias Gomes, M., Moiseyenko, R. P., Baum, A., Jørgensen, T. M. 
& Woodley, J. M. Use of image analysis to understand enzyme 
stability in an aerated stirred reactor. Biotechnol. Prog. 35,  
e2878 (2019).
19.	 Woodley, J. M. Advances in biological conversion technologies: 
new opportunities for reaction engineering. React. Chem. Eng. 5, 
632–640 (2020).
20.	 Courtot, M. et al. Controlled vocabularies and semantics in 
systems biology. Mol. Syst. Biol. 7, 543 (2011).
Publisher’s note Springer Nature remains neutral with regard to 
jurisdictional claims in published maps and institutional affiliations.
Springer Nature or its licensor (e.g. a society or other partner) holds 
exclusive rights to this article under a publishing agreement with 
the author(s) or other rightsholder(s); author self-archiving of the 
accepted manuscript version of this article is solely governed by the 
terms of such publishing agreement and applicable law.
© The Author(s), under exclusive licence to Springer Nature America, 
Inc. 2023

Nature Methods
Brief Communication
https://doi.org/10.1038/s41592-022-01763-1
Methods
EnzymeML format
EnzymeML is a data exchange format that was developed to document 
enzymological experiments and enable seamless workflows from 
laboratory to analysis7. The data model is based on SBML and extends 
SBML toward enzymology. EnzymeML complies with the STRENDA 
guidelines and therefore enables the description of the most important 
aspects of enzymological experiments.
EnzymeML combines metadata that describe the reaction condi-
tions (protein sequence and concentration of the enzyme, with a stand-
ard identifier for chemical reagents (International Chemical Identifier 
(InChI)), and initial concentration of the substrate, temperature or pH) 
with time-course data (decreasing substrate or increasing product 
concentration) that were generated during an experiment. Hence, 
EnzymeML is capable of seamless workflows from data generation to 
modeling platforms such as COPASI or PySCeS. Furthermore, Enzy-
meML stores estimated parameters as well as the kinetic model, which 
facilitates a complete description that can be uploaded to databases 
such as SABIO-RK. Finally, metadata, time-course data and a descrip-
tion of the methods are stored within the OMEX archive as .xml, .csv 
and .txt files, respectively. The archive offers an efficient storage of 
both types of data and provides a manifest file to describe the content.
EnzymeML toolbox
In general, the user is not expected to read or write XML but to use soft-
ware solutions that offer multiple functionalities to handle EnzymeML. 
The software PyEnzyme (Version 1.1.4) has been developed to read, 
write and edit EnzymeML documents. The application programming 
interface (API) is written in the commonly used programming language 
Python (Version 3.8) and follows the general structure of EnzymeML. 
The API offers a simple syntax and requires only basic prior knowledge 
of Python. PyEnzyme also offers additional functionalities such as a 
compliance check for database submission.
Following the current standard in web development of unified 
communication protocols between applications, the EnzymeML 
toolbox was extended with a RESTful interface to facilitate program-
ming language-independent access to PyEnzyme functionalities. The 
RESTful API offers not only endpoints to read and write EnzymeML 
documents, but also a compliance check, the addition of kinetic mod-
els, the export of time-course data and, finally, the conversion of the 
EnzymeML spreadsheet, which will be described in the next section. 
The RESTful API is hosted publicly (at https://enzymeml.sloppy.zone) 
or can be locally served as a Docker container by using the ‘dockerfile’ 
in the PyEnzyme GitHub repository (https://github.com/EnzymeML/
PyEnzyme).
EnzymeML spreadsheet
The EnzymeML spreadsheet allows the simple creation of EnzymeML 
documents from experimental data. The .xlsx formatted file is struc-
tured into several subsheets, containing information about the used 
proteins and substrates, together with all necessary reaction condi-
tions and the time-course data on the substrate and product concen-
tration. To simplify the insertion of experimental data, the fields are 
color coded according to their necessity, and drop-down menus are 
available for many fields to guarantee consistency in the input. An 
EnzymeML document can be created from the spreadsheet either by 
a simple upload to a web interface (https://enzymeml.sloppy.zone/
template/convert) or by calling the convert function of the RESTful 
API. The spreadsheet can be found at the PyEnzyme GitHub repository 
(https://github.com/EnzymeML/PyEnzyme/blob/main/templates/
EnzymeML_Template.xlsm).
Jupyter Notebook-based workflow platform
Throughout the scenarios presented in this work, Jupyter Notebooks 
were used to develop and further demonstrate data processing and 
modeling. These computational notebooks, which were first intro-
duced in 2015 by the project Jupyter21, offer a convenient way of com-
bining code and documentation. Other than conventional integrated 
development environments, notebooks operate on a web basis and 
allow the integration of textual content apart from source code. Hence, 
any part of the code can be illustrated by text and images in the same 
document. This effectively renders a notebook as an electronic book 
that is shareable, editable and comprehensible. Finally, processing 
data within these notebooks at their fullest potential bridges the gap 
from theory to code, and vice versa.
All code for the scenarios was written in the programming lan-
guage Python, which makes it easy to use the PyEnzyme software to 
read and write the EnzymeML documents and to use well-established 
libraries such as matplotlib22 for visualization, and SciPy23 and lmfit24 
for modeling, curve fitting and parameter estimation.
All visualization and modeling workflows start with the reading 
of the EnzymeML document. In all scenarios, the EnzymeML docu-
ment was generated by converting the EnzymeML spreadsheet using 
the PyEnzyme software, except in scenario 2, in which the RESTful 
API was used, and scenario 3, in which the EnzymeML document was 
generated by BioCatHub. The EnyzmeML document is then read by a 
Jupyter Notebook for a first visual overview of the time-course data by 
plotting the concentration or absorbance over time, using the library 
matplotlib. In the modeling workflows, either the Jupyter Notebook is 
used as an interface to modeling tools such as interferENZY25, COPASI12 
and PySCeS13 (scenarios 1 and 5), or the modeling is performed by 
defining the kinetic equations directly in Python and fitting the param-
eters using the Python libraries SciPy and lmfit (scenarios 2 and 4). In 
scenario 6, initial rates have been computed with linear regression of 
the slopes using SciPy. Finally, the estimated parameters were added 
to the EnzymeML documents (scenarios 1, 4 and 5), and all EnzymeML 
documents were uploaded to data repositories such as DaRUS to  
receive a DOI.
EnzymeML metadata block for Dataverse as a publication 
platform
Dataverse is a platform for data repositories developed by the Institute 
of Quantitative Social Sciences at Harvard University together with an 
international community26. Over 70 institutional or discipline-specific 
data repositories use Dataverse. Datasets within a Dataverse installa-
tion are organized with the help of Dataverse containers. Each dataset 
within such a container contains one or more files, receives a DOI and 
is described with the help of structured metadata blocks. The used, 
optional and required fields of such metadata blocks can be config-
ured for each Dataverse container. Dataverse comes ‘out of the box’ 
with a set of preconfigured metadata blocks and offers the possibility 
to define new metadata blocks, which can then be shared between 
Dataverse instances.
We defined such a metadata block for EnzymeML that maps the 
data model of EnzymeML. This was done using a tab-separated file 
(.tsv), provided by the Dataverse development team, which incor-
porates all fields present in an EnzymeML document. In addition, 
compound fields allow the description of objects, effectively wrap-
ping multiple attributes into a single field (for instance, a protein can 
possess many different attributes). Therefore, many different objects 
can be defined in a package, without losing structure and relationship. 
With this block, EnzymeML data can be made publicly available in a 
findable, accessible and interoperable manner. Dataverse offers an 
input form and search options via its user interface and the possibility 
to automatically create and update datasets via its API. An extension of 
the EnzymeML software allows all metadata to be extracted automati-
cally from EnzymeML files and every field to be mapped to the metadata 
blocks present in the Dataverse API pyDaRUS (https://github.com/
JR-1991/pyDaRUS), which then creates a dataset and uploads files and 
metadata to the Dataverse installation.

Nature Methods
Brief Communication
https://doi.org/10.1038/s41592-022-01763-1
To upload an EnzymeML document to a Dataverse installation, 
PyEnzyme offers a method to prepare data for, and to transfer it to, any 
Dataverse instance. As the method is seamlessly integrated into the 
PyEnzyme library, such an upload can be automated within workflows, 
allowing an EnzymeML document to be distributed across as many 
Dataverses as desired. For all the scenarios presented in this work, the 
library has been used to upload datasets to DaRUS after modeling and 
visualization were executed.
Modeling by PySCeS
PySCeS27 (available from https://pysces.github.io) is an open-source 
Python-based software for the simulation and analysis of cellular sys-
tems. First released in 2005, it enables the user to perform a variety 
of analyses on kinetic models, including structural (stoichiometric) 
analysis, time-course simulation, steady state and metabolic control 
analysis, parameter scanning and stability analysis. PySCeSToolbox 
(Version 1.0.0) (ref. 13), which was released more recently (2018), incor-
porates higher-level methods such as generalized supply–demand 
analysis, symbolic control analysis and thermodynamic–kinetic analy-
sis of enzymatic reactions. In the context of this paper, the time-course 
simulation capabilities of PySCeS were used to fit experimental data 
to kinetic models.
As PySCeS is written in Python and can import and export SBML, it 
was relatively straightforward to implement a thin-layer interface to the 
PyEnzyme library. The kinetic model (including initial guess values for 
the kinetic parameters) and all of the measurements were automatically 
extracted from the EnzymeML document with PyEnzyme. In a one-line 
command, the thin layer then performed the optimization by combin-
ing the experimental measurements and setting up the simulation via 
the SBML model to fit the kinetic parameters with SciPy and lmfit. The 
fitted parameters, including error statistics, can subsequently be writ-
ten back into the EnzymeML document.
Modeling by COPASI
COPASI12 (available from https://copasi.org) is a mature software 
application for modeling, simulating and analyzing biochemical reac-
tion networks that has been in continued development over the past 
20 years. It is written in C++ and is open source, and there are regular 
releases, and this makes it easy to use COPASI on all operating systems. 
COPASI supports simulating models deterministically using ordinary 
differential equations, or stochastically, and implements a variety of 
hybrid simulation methods. In addition to these simulation methods, 
many analysis tasks have been built for steady state analysis, parameter 
estimation, optimization, sensitivity analysis and more. These tasks 
can be combined for flexible scanning and sampling tasks. COPASI can 
be used as a graphical user interface, from the command line or from a 
variety of programming languages.
As part of EnzymeML, a thin-layer interface has been implemented 
using the COPASI Python bindings. This allows an EnzymeML document 
to be opened, a kinetic model to be added to it and its parameters esti-
mated and written into the EnzymeML document again. For this article, 
we used a high-level Python API to COPASI together with the PyEnzyme 
library to read in EnzymeML documents and extract the measurement 
data. In scenario 5, we first estimate the parameters of a single reaction 
from the EnzymeML file, and we then start a modeling project that is 
based on ordinary differential equations or on adding a system of many 
coupled reactions to fit that measurement data.
Modeling by interferENZY
The webserver interferENZY25 (https://interferenzy.i3s.up.pt) is a freely 
available tool for enzymatic assay validation and kinetic analysis. The 
analysis can be performed for datasets obtained at different substrate 
concentrations and fixed enzyme concentration. The major outputs 
of the webserver are a final score between 0 and 20, and Michaelis–
Menten kinetic parameters. High final scores assure that the occurrence 
of hidden assay interferences is unlikely and that the estimated kinetic 
parameters are highly accurate. A Jupyter Notebook-based workflow 
was developed, which creates an input file for the interferENZY web-
server from an EnzymeML document. Two versions of the workflow 
exist, using either the PyEnzyme library or the RESTful API to read in 
the information from the EnzymeML file. All necessary information 
for the modeling, including the enzyme concentration, the initial 
substrate concentrations and the measured time-course data for the 
product or the substrate, is extracted. The data are formatted accord-
ing to the requirements set by interferENZY, using regular expressions, 
and exported as a new tab-delimited text file. The generated file can 
be directly uploaded to the modeling webtool. In a last step, the deter-
mined kinetic parameters can be added to the existing EnzymeML 
document, either by manual readout of the output file or by integration 
of the generated file into the Jupyter Notebook workflow.
Validation of EnzymeML documents
The landscape of biocatalytic and enzymology-focused databases 
offers a broad diversity of specializations. From purely protein 
sequence databases such as UniProt to reaction kinetic-focused 
instances such as SABIO-RK or STRENDA-DB, the minimal require-
ments differ significantly. For instance, although for SABIO-RK, kinetic 
models and estimated parameters are mandatory, for other databases 
such as UniProt, these are optional. In consequence, EnzymeML needs 
to adapt to any requirements of any major database given, but doing 
this from the developer side requires constant updates to react to 
structural changes of a database.
The EnzymeML toolbox provides a method to validate an Enzy-
meML document at the database side by using an EnzymeML validation 
object. This object specifies minimal requirements for all occurring 
tags in an EnzymeML document. To generate a validation object, 
database providers can use the EnzymeML validation spreadsheet. 
Ultimately, the resulting object will be hosted at the database’s server 
so that it can be used by PyEnzyme to validate an EnzymeML docu-
ment specific to a given database. For this work, an EnzymeML valida-
tion object was created for SABIO-RK. Any resulting document from 
the described scenarios was checked by our validator before it was 
uploaded.
Upload to SABIO-RK
SABIO-RK (http://sabiork.h-its.org/) is a manually curated database 
for biochemical reactions and their kinetic properties. It comprises 
characteristics of the reaction participants, enzyme details, kinetic 
parameters and rate laws as well as the corresponding experimental 
conditions. Data in SABIO-RK are not only mainly extracted from lit-
erature but also directly submitted by authors via SBML or EnzymeML. 
A manual curation process and consistency checks are intermediate 
steps before the database entries are finally published in the SABIO-RK 
database. The existing code for upload and extraction of files in SBML 
format was extended for extraction of EnzymeML annotations includ-
ing the UniProt identifier and the EC number of the enzyme. For extrac-
tion of SBML data, the Java library jSBML is used28. This library also 
allows the extraction of information from annotations. The data are 
mapped to the corresponding entities in SABIO-RK and are stored in 
a relational database.
Reporting summary
Further information on research design is available in the Nature Port-
folio Reporting Summary linked to this article.
Data availability
All data availability is listed in Supplementary Table 11.
Code availability
All code availability is listed in Supplementary Table 11.

Nature Methods
Brief Communication
https://doi.org/10.1038/s41592-022-01763-1
References
21.	 Kluyver, T. et al. Jupyter Notebooks—a publishing format for 
reproducible computational workflows. In Positioning and Power 
in Academic Publishing: Players, Agents and Agendas, Proc. 20th 
Int. Conf. on Electronic Publishing (eds. Loizides, F. & Schmidt, B.) 
87–90 (IOS Press, 2016).
22.	 Hunter, J. D. Matplotlib: a 2D graphics environment. Comput. Sci. 
Eng. 9, 90–95 (2007).
23.	 Virtanen, P. et al. SciPy 1.0: fundamental algorithms for scientific 
computing in Python. Nat. Methods 17, 261–272 (2020).
24.	 Newville, M. et al. lmfit/lmfit-py: 1.1.0; https://doi.org/10.5281/
zenodo.7370358 (2022).
25.	 Pinto, M. F. et al. interferENZY: a web-based tool for enzymatic 
assay validation and standardized kinetic analysis. J. Mol. Biol. 
433, 166613 (2021).
26.	 Crosas, M. The Dataverse Network®: an open-source application 
for sharing, discovering and preserving data. D-Lib Magazine 17,  
2 (2011).
27.	 Olivier, B. G., Rohwer, J. M. & Hofmeyr, J.-H. S. Modelling cellular 
systems with PySCeS. Bioinformatics 21, 560–561 (2005).
28.	 Dräger, A. et al. JSBML: A flexible java library for working with 
SBML. Bioinformatics 27, 2167–2168 (2011).
Acknowledgements
S.L., H.D., J.P.R. and J.P. were supported by the German Research 
Foundation under Germany’s Excellence Strategy (EXC 2075, grant 
390740016) and by the German Federal Ministry of Education and 
Research (grant 01DG17027). J.D.S. was supported by the German 
Research Foundation under Germany’s Excellence Strategy (EXC 
2186, grant 390919832). A.W. and U.W. were supported by the 
Klaus Tschira Foundation and the German Federal Ministry of 
Education and Research within de.NBI (031A540). T.K. and S.N. were 
supported by the National Research Foundation of South Africa 
(grants 105889 and 112099). J.M.R. was supported by the National 
Research Foundation of South Africa (grant 120859). A.H. and J.W. 
were partially funded by the Sino-Danish Center for Education and 
Research and the Technical University of Denmark. C.E.L. and A.S.B. 
were supported by the US Food and Drug Administration, Center for 
Drug Evaluation and Research, and Office of Pharmaceutical Quality, 
through grant U01FD006484. C.E.L. also acknowledges funding by 
the US National Science Foundation through the Graduate Research 
Fellowship Program (grant DGE-1650044). F.T.B. was supported 
by the German Federal Ministry of Education and Research within 
de.NBI (031L0104A). STRENDA and STRENDA DB are funded by 
the Beilstein-Institut. The funders had no role in study design, data 
collection and analysis, decision to publish or preparation of the 
manuscript.
Author contributions
S.M., J.D.S., M.F.P., C.E.L., A.V.H. and S.N. contributed data to the 
scenarios. D.R., P.M., A.S.B., J.M.W. and T.K. supervised the scenarios. 
F.T.B. and J.M.R. contributed their kinetic modeling platforms. D.I., 
A.W. and U.W. contributed database platforms. C.K., N.S. and S.S. 
contributed to the conceptualisation of EnzymeML and to the 
development of the protocols. S.L., H.D. and J.R. implemented 
the EnzymeML workflows and analyzed data. J.P. supervised the 
development and application of EnzymeML workflows and prepared 
the draft of the manuscript with input from all authors. All authors 
approved the final version of the manuscript.
Competing interests
The authors declare no competing interests.
Additional information
Supplementary information The online version contains 
supplementary material available at  
https://doi.org/10.1038/s41592-022-01763-1.
Correspondence and requests for materials should be addressed to 
Jürgen Pleiss.
Peer review information Nature Methods thanks Shelley Copley, 
Kenneth Johnson and the other, anonymous, reviewer(s) for their 
contribution to the peer review of this work. Primary Handling Editor: 
Arunima Singh, in collaboration with the Nature Methods team.
Reprints and permissions information is available at  
www.nature.com/reprints.

1
nature research  |  reporting summary
April 2020
Corresponding author(s):
Juergen Pleiss
Last updated by author(s): Apr 6, 2022
Reporting Summary
Nature Research wishes to improve the reproducibility of the work that we publish. This form provides structure for consistency and transparency 
in reporting. For further information on Nature Research policies, see our Editorial Policies and the Editorial Policy Checklist.
Statistics
For all statistical analyses, confirm that the following items are present in the figure legend, table legend, main text, or Methods section.
n/a Confirmed
The exact sample size (n) for each experimental group/condition, given as a discrete number and unit of measurement
A statement on whether measurements were taken from distinct samples or whether the same sample was measured repeatedly
The statistical test(s) used AND whether they are one- or two-sided 
Only common tests should be described solely by name; describe more complex techniques in the Methods section.
A description of all covariates tested
A description of any assumptions or corrections, such as tests of normality and adjustment for multiple comparisons
A full description of the statistical parameters including central tendency (e.g. means) or other basic estimates (e.g. regression coefficient) 
AND variation (e.g. standard deviation) or associated estimates of uncertainty (e.g. confidence intervals)
For null hypothesis testing, the test statistic (e.g. F, t, r) with confidence intervals, effect sizes, degrees of freedom and P value noted 
Give P values as exact values whenever suitable.
For Bayesian analysis, information on the choice of priors and Markov chain Monte Carlo settings
For hierarchical and complex designs, identification of the appropriate level for tests and full reporting of outcomes
Estimates of effect sizes (e.g. Cohen's d, Pearson's r), indicating how they were calculated
Our web collection on statistics for biologists contains articles on many of the points above.
Software and code
Policy information about availability of computer code
Data collection
N/A 
 
Data analysis
Scenario 1 https://github.com/EnzymeML/Lauterbach_2022/tree/main/Scenario1 
Scenario 2 https://github.com/EnzymeML/Lauterbach_2022/blob/main/Scenario2/AnalysingTimeCourseDataUsingMichaelisMenten.ipynb 
Scenario 3 https://github.com/EnzymeML/Lauterbach_2022/blob/main/Scenario3/CollectingDataByBioCatHubAndVisualisation.ipynb 
Scenario 4 https://github.com/EnzymeML/Lauterbach_2022/blob/main/Scenario4/InsightIntoMechanismByModelling.ipynb 
Scenario 5 https://github.com/EnzymeML/Lauterbach_2022/tree/main/Scenario5/ 
Scenario 6 https://github.com/EnzymeML/Lauterbach_2022/blob/main/Scenario6/MeasuringTheStabilityOfEnzymes.ipynb
For manuscripts utilizing custom algorithms or software that are central to the research but not yet described in published literature, software must be made available to editors and 
reviewers. We strongly encourage code deposition in a community repository (e.g. GitHub). See the Nature Research guidelines for submitting code & software for further information.
Data
Policy information about availability of data
All manuscripts must include a data availability statement. This statement should provide the following information, where applicable: 
- Accession codes, unique identifiers, or web links for publicly available datasets 
- A list of figures that have associated raw data 
- A description of any restrictions on data availability
All data is publicly available: 

2
nature research  |  reporting summary
April 2020
Scenario 1 https://doi.org/10.18419/darus-2483 
Scenario 2 https://doi.org/10.18419/darus-2470 
Scenario 3 https://doi.org/10.18419/darus-2466 
Scenario 4 https://doi.org/10.18419/darus-2467 
Scenario 5 https://doi.org/10.18419/darus-2468 
Scenario 6 https://doi.org/10.18419/darus-2469
Field-specific reporting
Please select the one below that is the best fit for your research. If you are not sure, read the appropriate sections before making your selection.
Life sciences
Behavioural & social sciences
 Ecological, evolutionary & environmental sciences
For a reference copy of the document with all sections, see nature.com/documents/nr-reporting-summary-flat.pdf
Life sciences study design
All studies must disclose on these points even when the disclosure is negative.
Sample size
N/A
Data exclusions
N/A
Replication
N/A
Randomization
N/A
Blinding
N/A
Reporting for specific materials, systems and methods
We require information from authors about some types of materials, experimental systems and methods used in many studies. Here, indicate whether each material, 
system or method listed is relevant to your study. If you are not sure if a list item applies to your research, read the appropriate section before selecting a response. 
Materials & experimental systems
n/a Involved in the study
Antibodies
Eukaryotic cell lines
Palaeontology and archaeology
Animals and other organisms
Human research participants
Clinical data
Dual use research of concern
Methods
n/a Involved in the study
ChIP-seq
Flow cytometry
MRI-based neuroimaging
No data has been excluded
Replication was successfull
Blinding was not applicable since all scenarios were already published
Randomization was not applicable since all scenarios were already published
Sample sizes were already determined by the published works replicated in this manuscript

