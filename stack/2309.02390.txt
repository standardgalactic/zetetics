5 September 2023
Explaining grokking through circuit efficiency
Vikrant Varma*, 1, Rohin Shah*, 1, Zachary Kenton1, JÃ¡nos KramÃ¡r1 and Ramana Kumar1
*Equal contributions, 1Google DeepMind
One of the most surprising puzzles in neural network generalisation is grokking: a network with perfect
training accuracy but poor generalisation will, upon further training, transition to perfect generalisation.
We propose that grokking occurs when the task admits a generalising solution and a memorising solution,
where the generalising solution is slower to learn but more efficient, producing larger logits with the
same parameter norm. We hypothesise that memorising circuits become more inefficient with larger
training datasets while generalising circuits do not, suggesting there is a critical dataset size at which
memorisation and generalisation are equally efficient. We make and confirm four novel predictions about
grokking, providing significant evidence in favour of our explanation. Most strikingly, we demonstrate
two novel and surprising behaviours: ungrokking, in which a network regresses from perfect to low test
accuracy, and semi-grokking, in which a network shows delayed generalisation to partial rather than
perfect test accuracy.
1. Introduction
When training a neural network, we expect that once training loss converges to a low value, the
network will no longer change much. Power et al. (2021) discovered a phenomenon dubbed grokking
that drastically violates this expectation. The network first â€œmemorisesâ€ the data, achieving low
and stable training loss with poor generalisation, but with further training transitions to perfect
generalisation. We are left with the question: why does the networkâ€™s test performance improve
dramatically upon continued training, having already achieved nearly perfect training performance?
Recent answers to this question vary widely, including the difficulty of representation learning (Liu
et al., 2022), the scale of parameters at initialisation (Liu et al., 2023), spikes in loss ("slingshots") (Thi-
lak et al., 2022), random walks among optimal solutions (Millidge, 2022), and the simplicity of
the generalising solution (Nanda et al., 2023, Appendix E). In this paper, we argue that the last
explanation is correct, by stating a specific theory in this genre, deriving novel predictions from the
theory, and confirming the predictions empirically.
We analyse the interplay between the internal mechanisms that the neural network uses to
calculate the outputs, which we loosely call â€œcircuitsâ€ (Olah et al., 2020). We hypothesise that there
are two families of circuits that both achieve good training performance: one which generalises well
(ğ¶gen ) and one which memorises the training dataset (ğ¶mem ). The key insight is that when there
are multiple circuits that achieve strong training performance, weight decay prefers circuits with high
â€œefficiencyâ€, that is, circuits that require less parameter norm to produce a given logit value.
Efficiency answers our question above: if ğ¶gen is more efficient than ğ¶mem , gradient descent can
reduce nearly perfect training loss even further by strengthening ğ¶gen while weakening ğ¶mem , which
then leads to a transition in test performance. With this understanding, we demonstrate in Section 3
that three key properties are sufficient for grokking: (1) ğ¶gen generalises well while ğ¶mem does not,
(2) ğ¶gen is more efficient than ğ¶mem , and (3) ğ¶gen is learned more slowly than ğ¶mem .
Since ğ¶gen generalises well, it automatically works for any new data points that are added to
the training dataset, and so its efficiency should be independent of the size of the training dataset.
In contrast, ğ¶mem must memorise any additional data points added to the training dataset, and so
Corresponding author(s): vikrantvarma@deepmind.com, rohinmshah@deepmind.com
arXiv:2309.02390v1  [cs.LG]  5 Sep 2023

Explaining grokking through circuit efficiency
Train loss
Test loss
Test accuracy
0
5
10
Epochs
1e3
101
10
2
10
5
Loss
0
1
(a) Grokking.
The original be-
haviour from Power et al. (2021).
We train on a dataset with ğ·â‰«
ğ·crit. As a result, ğ¶gen is strongly
preferred to ğ¶mem at convergence,
and we observe a transition to very
low test loss (100% test accuracy).
101
103
105
Epochs
101
10
2
10
5
0
1
(b) Ungrokking. If we take a net-
work that has already grokked and
train it on a new dataset with ğ·<
ğ·crit, the network reverts to signif-
icant memorisation, leading to a
transition back to poor test loss.
(Note the log scale for the x-axis.)
0
2
Epochs
1e7
101
10
2
10
5
0
1
Accuracy
(c) Semi-grokking.
When ğ·âˆ¼
ğ·crit, the memorising algorithm
and generalising algorithm com-
pete with each other at conver-
gence, so we observe a transition
to improved but not perfect test
loss (test accuracy of 83%).
Figure 1 | Novel grokking phenomena. When grokking occurs, we expect there are two algorithms
that perform well at training: â€œmemorisationâ€ (ğ¶mem , with poor test performance) and â€œgeneralisationâ€
(ğ¶gen , with strong test performance). Weight decay strengthens ğ¶gen over ğ¶mem as the training dataset
size increases. By analysing the point at which ğ¶gen and ğ¶mem are equally strong (the critical dataset
size ğ·crit), we predict and confirm two novel behaviours: ungrokking and semi-grokking.
its efficiency should decrease as training dataset size increases. We validate these predictions by
quantifying efficiencies for various dataset sizes for both ğ¶mem and ğ¶gen .
This suggests that there exists a crossover point at which ğ¶gen becomes more efficient than ğ¶mem ,
which we call the critical dataset size ğ·crit. By analysing dynamics at ğ·crit, we predict and demonstrate
two new behaviours (Figure 1). In ungrokking, a model that has successfully grokked returns to poor
test accuracy when further trained on a dataset much smaller than ğ·crit. In semi-grokking, we choose
a dataset size where ğ¶gen and ğ¶mem are similarly efficient, leading to a phase transition but only to
middling test accuracy.
We make the following contributions:
1. We demonstrate the sufficiency of three ingredients for grokking through a constructed simula-
tion (Section 3).
2. By analysing dynamics at the â€œcritical dataset sizeâ€ implied by our theory, we predict two novel
behaviours: semi-grokking and ungrokking (Section 4).
3. We confirm our predictions through careful experiments, including demonstrating semi-grokking
and ungrokking in practice (Section 5).
2. Notation
We consider classification using deep neural networks under the cross-entropy loss. In particular, we
are given a set of inputs ğ‘‹, a set of labels ğ‘Œ, and a training dataset, D = {(ğ‘¥1, ğ‘¦âˆ—
1), . . . (ğ‘¥ğ·, ğ‘¦âˆ—
ğ·)}.
For an arbitrary classifier â„: ğ‘‹Ã— ğ‘Œâ†’â„, the softmax cross entropy loss is given by:
Lx-ent(â„) = âˆ’1
ğ·
âˆ‘ï¸
(ğ‘¥,ğ‘¦âˆ—)âˆˆD
log
exp(â„(ğ‘¥, ğ‘¦âˆ—))
Ã
ğ‘¦â€²âˆˆğ‘Œ
exp(â„(ğ‘¥, ğ‘¦â€²)) .
(1)
The output of a classifier for a specific class is the class logit, denoted by ğ‘œğ‘¦
â„(ğ‘¥) B â„(ğ‘¥, ğ‘¦). When the
2

Explaining grokking through circuit efficiency
input ğ‘¥is clear from context, we will denote the logit as ğ‘œğ‘¦
â„. We denote the vector of the logits for all
classes for a given input as Â®ğ‘œâ„(ğ‘¥) or Â®ğ‘œâ„when ğ‘¥is clear from context.
Parametric classifiers (such as neural networks) are parameterised with a vector ğœƒthat induces
a classifier â„ğœƒ. The parameter norm of the classifier is ğ‘ƒâ„ğœƒB âˆ¥ğœƒâˆ¥. It is common to add weight decay
regularisation, which is an additional loss term Lwd(â„ğœƒ) = 1
2 (ğ‘ƒâ„ğœƒ)2. The overall loss is given by
L(â„ğœƒ) = Lx-ent(â„) + ğ›¼Lwd(â„ğœƒ),
(2)
where ğ›¼is a constant that trades off between softmax cross entropy and weight decay.
Circuits. Inspired by Olah et al. (2020), we use the term circuit to refer to an internal mechanism by
which a neural network works. We only consider circuits that map inputs to logits, so that a circuit ğ¶
induces a classifier â„ğ¶for the overall task. We elide this distinction and simply write ğ¶to refer to â„ğ¶,
so that the logits are ğ‘œğ‘¦
ğ¶, the loss is L(ğ¶), and the parameter norm is ğ‘ƒğ¶.
For any given algorithm, there exist multiple circuits that implement that algorithm. Abusing
notation, we use ğ¶gen (ğ¶mem ) to refer either to the family of circuits that implements the generalising
(memorising) algorithm, or a single circuit from the appropriate family.
3. Three ingredients for grokking
Given a circuit with perfect training accuracy (as with a pure memorisation approach like ğ¶mem or a
perfectly generalising solution like ğ¶gen ), the cross entropy loss Lx-ent incentivises gradient descent
to scale up the classifierâ€™s logits, as that makes its answers more confident, leading to lower loss (see
Theorem D.1). For typical neural networks, this would be achieved by making the parameters larger.
Meanwhile, weight decay Lwd pushes in the opposite direction, directly decreasing the parameters.
These two forces must be balanced at any local minimum of the overall loss.
When we have multiple circuits that achieve strong training accuracy, this constraint applies to
each individually. But how will they relate to each other? Intuitively, the answer depends on the
efficiency of each circuit, that is, the extent to which the circuit can convert relatively small parameters
into relatively large logits. For more efficient circuits, the Lx-ent force towards larger parameters is
stronger, and the Lwd force towards smaller parameters is weaker. So, we expect that more efficient
circuits will be stronger at any local minimum.
Given this notion of efficiency, we can explain grokking as follows. In the first phase, ğ¶mem is
learned quickly, leading to strong train performance and poor test performance. In the second phase,
ğ¶gen is now learned, and parameter norm is â€œreallocatedâ€ from ğ¶mem to ğ¶gen , eventually leading to a
mixture of strong ğ¶gen and weak ğ¶mem , causing an increase in test performance.
This overall explanation relies on the presence of three ingredients:
1. Generalising circuit: There are two families of circuits that achieve good training performance:
a memorising family ğ¶mem with poor test performance, and a generalising family ğ¶gen with good
test performance.
2. Efficiency: ğ¶gen is more â€œefficientâ€ than ğ¶mem , that is, it can produce equivalent cross-entropy
loss on the training set with a lower parameter norm.
3. Slow vs fast learning: ğ¶gen is learned more slowly than ğ¶mem , such that during early phases of
training ğ¶mem is stronger than ğ¶gen .
To illustrate the sufficiency of these ingredients, we construct a minimal example containing all
three ingredients, and demonstrate that it leads to grokking. We emphasise that this example is to be
3

Explaining grokking through circuit efficiency
treated as a validation of the three ingredients, rather than as a quantitative prediction of the dynamics
of existing examples of grokking. Many of the assumptions and design choices were made on the
basis of simplicity and analytical tractability, rather than a desire to reflect examples of grokking
in practice. The clearest difference is that ğ¶gen and ğ¶mem are modelled as hardcoded input-output
lookup tables whose outputs can be strengthened through learned scalar weights, whereas in existing
examples of grokking ğ¶gen and ğ¶mem are learned internal mechanisms in a neural network that can be
strengthened by scaling up the parameters implementing those mechanisms.
Generalisation. To model generalisation, we introduce a training dataset D and a test dataset Dtest.
ğ¶gen is a lookup table that produces logits that achieve perfect train and test accuracy. ğ¶mem is a lookup
table that achieves perfect train accuracy, but makes confident incorrect predictions on the test dataset.
We denote by Dmem the predictions made by ğ¶mem on the test inputs, with the property that there is
no overlap between Dtest and Dmem. Then we have:
ğ‘œğ‘¦
ğº(ğ‘¥) = ğŸ™[(ğ‘¥, ğ‘¦) âˆˆD or (ğ‘¥, ğ‘¦) âˆˆDtest]
ğ‘œğ‘¦
ğ‘€(ğ‘¥) = ğŸ™[(ğ‘¥, ğ‘¦) âˆˆD or (ğ‘¥, ğ‘¦) âˆˆDmem]
Slow vs fast learning. To model learning, we introduce weights for each of the circuits, and use
gradient descent to update the weights. Thus, the overall logits are given by:
ğ‘œğ‘¦(ğ‘¥) = ğ‘¤ğºğ‘œğ‘¦
ğº(ğ‘¥) + ğ‘¤ğ‘€ğ‘œğ‘¦
ğ‘€(ğ‘¥)
Unfortunately, if we learn ğ‘¤ğºand ğ‘¤ğ‘€directly with gradient descent, we have no control over the
speed at which the weights are learned. Inspired by Jermyn and Shlegeris (2022), we instead compute
weights as multiples of two â€œsubweightsâ€, and then learn the subweights with gradient descent.
More precisely, we let ğ‘¤ğº= ğ‘¤ğº1ğ‘¤ğº2 and ğ‘¤ğ‘€= ğ‘¤ğ‘€1ğ‘¤ğ‘€2, and update each subweight according to
ğ‘¤ğ‘–â†ğ‘¤ğ‘–âˆ’ğœ†Â· ğœ•L/ğœ•ğ‘¤ğ‘–. The speed at which the weights are strengthened by gradient descent can then
be controlled by the initial values of the weights. Intuitively, the gradient towards the first subweight
ğœ•L/ğœ•ğ‘¤1 depends on the strength of the second subweight ğ‘¤2 and vice-versa, and so low initial values
lead to slow learning. At initialisation, we set ğ‘¤ğº1 = ğ‘¤ğ‘€1 = 0 to ensure the logits are initially zero,
and then set ğ‘¤ğº2 << ğ‘¤ğ‘€2 to ensure ğ¶gen is learned more slowly than ğ¶mem .
Efficiency. Above, we operationalised circuit efficiency as the extent to which the circuit can convert
relatively small parameters into relatively large logits. When the weights are all one, each circuit
produces a one-hot vector as its logits, so their logit scales are the same, and efficiency is determined
solely by parameter norm. We define ğ‘ƒğºand ğ‘ƒğ‘€to be the parameter norms when weights are all one.
Since we want ğ¶gen to be more efficient than ğ¶mem , we set ğ‘ƒğº< ğ‘ƒğ‘€.
This still leaves the question of how to model parameter norm when the weights are not all one.
Intuitively, increasing the weights corresponds to increasing the parameters in a neural network to
scale up the resulting outputs. In a ğœ…-layer MLP with Relu activations and without biases, scaling
all parameters by a constant ğ‘scales the outputs by ğ‘ğœ…. Inspired by this observation, we model the
parameter norm of ğ‘¤ğºğ¶ğºas ğ‘¤1/ğœ…
ğº
ğ‘ƒğ‘–for some ğœ…> 0, and similarly for ğ‘¤ğ‘€ğ¶ğ‘€.
Theoretical analysis. We first analyse the optimal solutions to the setup above. We can ignore the
subweights, as they only affect the speed of learning: Lx-ent and Lwd depend only on the weights, not
subweights. Intuitively, to get minimal loss, we must assign higher weights to more efficient circuits â€“
but it is unclear whether we should assign no weight to less efficient circuits, or merely smaller but
still non-zero weights. Theorem D.4 shows that in our example, both of these cases can arise: which
one we get depends on the value of ğœ….
4

Explaining grokking through circuit efficiency
Train loss
Test loss
Gen logit
Mem logit
Parameter norm
0
5
10
Steps
1e3
101
100
10
1
10
2
Loss
0
5
10
(a)
All
three
ingredients.
When ğ¶gen is more efficient than
ğ¶mem but
learned
slower,
we
observe grokking. ğ¶gen only starts
to grow significantly by step 2500,
and then substitutes for ğ¶mem .
Total parameter norm falls due to
ğ¶gen â€™s higher efficiency.
0
5
10
Steps
1e3
101
100
10
1
10
2
Loss
0
5
10
(b) ğ¶gen less efficient than ğ¶mem .
We set ğ‘ƒğº> ğ‘ƒğ‘€. Since ğ¶gen is now
less efficient and learned slower,
it never grows, and test loss stays
high due to ğ¶mem throughout train-
ing.
0
5
10
Steps
1e3
101
100
10
1
10
2
Loss
0
5
10
(c)
ğ¶gen and
ğ¶mem learned
at
equal speeds. We set ğ‘¤ğº1 = ğ‘¤ğ‘€1
so they are learned equally quickly.
ğ¶gen is prioritised at least as much
as ğ¶mem throughout training, due
to its higher efficiency.
Thus,
test loss is very similar to train
loss throughout training, and no
grokking is observed.
Figure 2 | ğ¶gen must be learned slowly for grokking to arise. We learn weights ğ‘¤ğ‘€and ğ‘¤ğºthrough
gradient descent on the loss in Equation D.1. To model the fact that ğ¶gen is more efficient than ğ¶mem ,
we set ğ‘ƒğ‘€> ğ‘ƒğº. We see that we only get grokking when ğ¶gen is learned more slowly than ğ¶mem .
Experimental analysis. We run our example for various hyperparameters, and plot training and
test loss in Figure 2. We see that when all three ingredients are present (Figure 2a), we observe
the standard grokking curves, with a delayed decrease in test loss. By contrast, when we make the
generalising circuit less efficient (Figure 2b), the test loss never falls, and when we remove the slow
vs fast learning ingredient (Figure 2c), we see that test loss decreases immediately. See Appendix C
for details.
4. Why generalising circuits are more efficient
Section 3 demonstrated that grokking can arise when ğ¶gen is more efficient than ğ¶mem , but left open
the question of why ğ¶gen is more efficient. In this section, we develop a theory based on training
dataset size ğ·, and use it to predict two new behaviours: ungrokking and semi-grokking.
4.1. Relationship of efficiency with dataset size
Consider a classifier â„D obtained by training on a dataset D of size ğ·with weight decay, and a
classifier â„Dâ€² obtained by training on the same dataset with one additional point: Dâ€² = D âˆª{(ğ‘¥, ğ‘¦âˆ—)}.
Intuitively, â„Dâ€² cannot be more efficient than â„D: if it was, then â„Dâ€² would outperform â„D even on
just D, since it would get similar Lx-ent while doing better by weight decay. So we should expect that,
on average, classifier efficiency is monotonically non-increasing in dataset size.
How does generalisation affect this picture? Let us suppose that â„D successfully generalises to
predict ğ‘¦âˆ—for the new input ğ‘¥. Then, as we move from D to Dâ€², Lx-ent(â„D) likely does not worsen
with this new data point. Thus, we could expect to see the same classifier arise, with the same average
logit value, parameter norm, and efficiency.
Now suppose â„D instead fails to predict the new data point (ğ‘¥, ğ‘¦âˆ—). Then the classifier learned for
5

Explaining grokking through circuit efficiency
Dâ€² will likely be less efficient: Lx-ent(â„D) would be much higher due to this new data point, and so
the new classifier must incur some additional regularisation loss to reduce Lx-ent on the new point.
Applying this analysis to our circuits, we should expect ğ¶gen â€™s efficiency to remain unchanged
as ğ·increases arbitrarily high, since ğ¶gen does need not to change to accommodate new training
examples. In contrast, ğ¶mem must change with nearly every new data point, and so we should expect
its efficiency to decrease as ğ·increases. Thus, when ğ·is sufficiently large, we expect ğ¶gen to be more
efficient than ğ¶mem . (Note however that when the set of possible inputs is small, even the maximal ğ·
may not be â€œsufficiently largeâ€.)
Critical threshold for dataset size. Intuitively, we expect that for extremely small datasets (say,
ğ·< 5), it is extremely easy to memorise the training dataset. So, we hypothesise that for these
very small datasets, ğ¶mem is more efficient than ğ¶gen . However, as argued above, ğ¶mem will get less
efficient as ğ·increases, and so there will be a critical dataset size ğ·crit at which ğ¶mem and ğ¶gen are
approximately equally efficient. When ğ·â‰«ğ·crit, ğ¶gen is more efficient and we expect grokking, and
when ğ·â‰ªğ·crit, ğ¶mem is more efficient and so grokking should not happen.
Effect of weight decay on ğ·crit. Since ğ·crit is determined only by the relative efficiencies of ğ¶gen and
ğ¶mem , and none of these depends on the exact value of weight decay (just on weight decay being
present at all), our theory predicts that ğ·crit should not change as a function of weight decay. Of
course, the strength of weight decay may still affect other properties such as the number of epochs till
grokking.
4.2. Implications of crossover: ungrokking and semi-grokking.
By thinking through the behaviour around the critical threshold for dataset size, we predict the
existence of two phenomena that, to the best of our knowledge, have not previously been reported.
Ungrokking. Suppose we take a network that has been trained on a dataset with ğ·> ğ·crit and has
already exhibited grokking, and continue to train it on a smaller dataset with size ğ·â€² < ğ·crit. In this
new training setting, ğ¶mem is now more efficient than ğ¶gen , and so we predict that with enough further
training gradient descent will reallocate weight from ğ¶gen to ğ¶mem , leading to a transition from high
test performance to low test performance. Since this is exactly the opposite observation as in regular
grokking, we term this behaviour â€œungrokkingâ€.
Ungrokking can be seen as a special case of catastrophic forgetting (McCloskey and Cohen, 1989;
Ratcliff, 1990), where we can make much more precise predictions. First, since ungrokking should
only be expected once ğ·â€² < ğ·crit, if we vary ğ·â€² we predict that there will be a sharp transition from
very strong to near-random test accuracy (around ğ·crit). Second, we predict that ungrokking would
arise even if we only remove examples from the training dataset, whereas catastrophic forgetting
typically involves training on new examples as well. Third, since ğ·crit does not depend on weight
decay, we predict the amount of â€œforgettingâ€ (i.e. the test accuracy at convergence) also does not
depend on weight decay.
Semi-grokking. Suppose we train a network on a dataset with ğ·â‰ˆğ·crit. ğ¶gen and ğ¶mem would be
similarly efficient, and there are two possible cases for what we expect to observe (illustrated in
Theorem D.4).
In the first case, gradient descent would select either ğ¶mem or ğ¶gen , and then make it the maximal
circuit. This could happen in a consistent manner (for example, perhaps since ğ¶mem is learned faster
it always becomes the maximal circuit), or in a manner dependent on the random initialisation. In
either case we would simply observe the presence or absence of grokking.
6

Explaining grokking through circuit efficiency
In the second case, gradient descent would produce a mixture of both ğ¶mem and ğ¶gen . Since
neither ğ¶mem nor ğ¶gen would dominate the prediction on the test set, we would expect middling test
performance.
ğ¶mem would still be learned faster, and so this would look similar to grokking: an initial phase with
good train performance and bad test performance, followed by a transition to significantly improved
test performance. Since we only get to middling generalisation unlike in typical grokking, we call this
behaviour semi-grokking.
Our theory does not say which of the two cases will arise in practice, but in Section 5.3 we find
that semi-grokking does happen in our setting.
5. Experimental evidence
Our explanation of grokking has some support from from prior work:
1. Generalising circuit: Nanda et al. (2023, Figure 1) identify and characterise the generalising
circuit learned at the end of grokking in the case of modular addition.
2. Slow vs fast learning: Nanda et al. (2023, Figure 7) demonstrate â€œprogress measuresâ€ showing
that the generalising circuit develops and strengthens long after the network achieves perfect
training accuracy in modular addition.
To further validate our explanation, we empirically test our predictions from Section 4:
(P1) Efficiency: We confirm our prediction that ğ¶gen efficiency is independent of dataset size, while
ğ¶mem efficiency decreases as training dataset size increases.
(P2) Ungrokking (phase transition): We confirm our prediction that ungrokking shows a phase
transition around ğ·crit.
(P3) Ungrokking (weight decay): We confirm our prediction that the final test accuracy after
ungrokking is independent of the strength of weight decay.
(P4) Semi-grokking: We demonstrate that semi-grokking occurs in practice.
Training details. We train 1-layer Transformer models with the AdamW optimiser (Loshchilov and
Hutter, 2019) on cross-entropy loss (see Appendix A for more details). All results in this section are
on the modular addition task (ğ‘+ ğ‘mod ğ‘ƒfor ğ‘, ğ‘âˆˆ(0, . . . , ğ‘ƒâˆ’1) and ğ‘ƒ= 113) unless otherwise
stated; results on 9 additional tasks can be found in Appendix A.
5.1. Relationship of efficiency with dataset size
We first test our prediction about memorisation and generalisation efficiency:
(P1) Efficiency. We predict (Section 4.1) that memorisation efficiency decreases with increasing train
dataset size, while generalisation efficiency stays constant.
To test (P1), we look at training runs where only one circuit is present, and see how the logits ğ‘œğ‘¦
ğ‘–
vary with the parameter norm ğ‘ƒğ‘–(by varying the weight decay) and the dataset size ğ·.
Experiment setup. We produce ğ¶mem -only networks by using completely random labels for the
training data (Zhang et al., 2021), and assume that the entire parameter norm at convergence is
allocated to memorisation. We produce ğ¶gen -only networks by training on large dataset sizes and
checking that > 95% of the logit norm comes from just the trigonometric subspace (see Appendix B
for details).
7

Explaining grokking through circuit efficiency
25
30
40
60
90
Parameter norm
30
60
90
Correct logit
fixed logit values
0.5
1.0
2.0
4.0
8.0
Dataset size
1e3
(a) ğ¶mem scatter plot. At a fixed logit value (dot-
ted horizontal lines), parameter norm increases with
dataset size.
0.5
1.0
2.0
4.0
8.0
Dataset size
1e3
25
30
40
60
90
Parameter norm
50
60
70
Correct logit
(b) ğ¶mem isologit curves. Curves go up and right,
showing that parameter norm increases with dataset
size when holding logits fixed.
27
30
33
Parameter norm
30
60
90
120
Correct logit
fixed logit values
4
6
8
Dataset size
1e3
(c) ğ¶gen scatter plot. There is no obvious structure
to the colours, suggesting that the logit to parameter
norm relationship is independent of dataset size.
4
6
8
Dataset size
1e3
27
30
33
Parameter norm
50
70
90
Correct logit
(d) ğ¶gen isologit curves. The curves are flat, showing
that for fixed logit values the parameter norm does
not depend on dataset size.
Figure 3 | Efficiency of the ğ¶mem and ğ¶gen algorithms. We collect and visualise a dataset of triples
 ğ‘œğ‘¦, ğ‘ƒğ‘š, ğ· (correct logit, parameter norm, and dataset size), each corresponding to a training run
with varying random seed, weight decay, and dataset size, for both ğ¶mem and ğ¶gen . Besides a standard
scatter plot, we geometrically bucket logit values into six buckets, and plot â€œisologit curvesâ€ showing
the dependence of parameter norm on dataset size for each bucket. The results validate our theory
that (1) ğ¶mem requires larger parameter norm to produce the same logits as dataset size increases,
and (2) ğ¶gen uses the same parameter norm to produce fixed logits, irrespective of dataset size. In
addition, ğ¶mem has a much wider range of parameter norms than ğ¶gen , and at the extremes can be
more efficient than ğ¶gen .
Results. Figures 3a and 3b confirm our theoretical prediction for memorisation efficiency. Specifically,
to produce a fixed logit value, a higher parameter norm is required when dataset size is increased,
implying decreased efficiency. In addition, for a fixed dataset size, scaling up logits requires scaling up
parameter norm, as expected. Figures 3c and 3d confirm our theoretical prediction for generalisation
efficiency. To produce a fixed logit value, the same parameter norm is required irrespective of the
dataset size.
Note that the figures show significant variance across random seeds. We speculate that there are
many different circuits implementing the same overall algorithm, but they have different efficiencies,
and the random initialisation determines which one gradient descent finds. For example, in the case
of modular addition, the generalising algorithm depends on a set of â€œkey frequenciesâ€ (Nanda et al.,
2023); different choices of key frequencies could lead to different efficiencies.
It may appear from Figure 3c that increasing parameter norm does not increase logit value,
contradicting our theory. However, this is a statistical artefact caused by the variance from the random
seed. We do see â€œstripesâ€ of particular colours going up and right: these correspond to runs with the
same seed and dataset size, but different weight decay, and they show that when the noise from the
random seed is removed, increased parameter norm clearly leads to increased logits.
8

Explaining grokking through circuit efficiency
103
104
Reduced dataset size
0.0
0.5
1.0
Test accuracy
x + y mod P
0.1
0.5
1.0
1.5
2.0
Weight decay
Average accuracy
Per seed accuracy
Figure 4 | Ungrokking. We train on the full
dataset (achieving 100% test accuracy), and
then continue training on a smaller subset of
the full dataset. We plot test accuracy against
reduced dataset size for a range of weight de-
cays. We see a sharp transition from strong test
accuracy to near-zero test accuracy, that is in-
dependent of weight decay (different coloured
lines almost perfectly overlap). See Figure 8 for
more tasks.
0
1
2
3
Epochs
1e7
0.0
0.5
1.0
Test accuracy
0.25
0.50
0.75
1.00
Final accuracy
Figure 5 | Semi-grokking. We plot test accu-
racy against training epochs for a large sweep of
training runs with varying dataset sizes. Lines
are coloured by the final test accuracy at the
end of training. Out of 200 runs, at least 6 show
clear semi-grokking at the end of training. Many
other runs show transient semi-grokking, hover-
ing around middling test accuracy for millions
of epochs, or having multiple plateaus, before
fully generalising.
5.2. Ungrokking: overfitting after generalisation
We now turn to testing our predictions about ungrokking. Figure 1b demonstrates that ungrokking
happens in practice. In this section we focus on testing that it has the properties we expect.
(P2) Phase transition. We predict (Section 4.2) that if we plot test accuracy at convergence against
the size of the reduced training dataset ğ·â€², there will be a phase transition around ğ·crit.
(P3) Weight decay. We predict (Section 4.2) that test accuracy at convergence is independent of the
strength of weight decay.
Experiment setup. We train a network to convergence on the full dataset to enable perfect gener-
alisation, then continue training the model on a small subset of the full dataset, and measure the
test accuracy at convergence. We vary both the size of the small subset, as well as the strength of the
weight decay.
Results. Figure 4 shows the results, and clearly confirms both (P2) and (P3). Appendix A has
additional results, and in particular Figure 8 replicates the results for many additional tasks.
5.3. Semi-grokking: evenly matched circuits
Unlike the previous predictions, semi-grokking is not strictly implied by our theory. However, as we
will see, it turns out that it does occur in practice.
(P4) Semi-grokking. When training at around ğ·â‰ˆğ·crit, where ğ¶mem and ğ¶gen have roughly equal
efficiencies, the final network at convergence should either be entirely composed of the most efficient
circuit, or of roughly equal proportions of ğ¶mem and ğ¶gen . If the latter, we should observe a transition
to middling test accuracy well after near-perfect train accuracy.
There are a number of difficulties in demonstrating an example of semi-grokking in practice.
First, the time to grok increases super-exponentially as the dataset size ğ·decreases (Power et al.,
2021, Figure 1), and ğ·crit is significantly smaller than the smallest dataset size at which grokking
9

Explaining grokking through circuit efficiency
has been demonstrated. Second, the random seed causes significant variance in the efficiency of
ğ¶gen and ğ¶mem , which in turn affects ğ·crit for that run. Third, accuracy changes sharply with the ğ¶gen to
ğ¶mem ratio (Appendix A). To observe a transition to middling accuracy, we need to have balanced
ğ¶gen and ğ¶mem outputs, but this is difficult to arrange due to the variance with random seed. To
address these challenges, we run many different training runs, on dataset sizes slightly above our
best estimate of the typical ğ·crit, such that some of the runs will (through random noise) have an
unusually inefficient ğ¶gen or an unusually efficient ğ¶mem , such that the efficiencies match and there is
a chance to semi-grok.
Experiment setup. We train 10 seeds for each of 20 dataset sizes evenly spaced in the range
[1500, 2050] (somewhat above our estimate of ğ·crit).
Results. Figure 1c shows an example of a single run that demonstrates semi-grokking, and Figure 5
shows test accuracies over time for every run. These validate our initial hypothesis that semi-grokking
may be possible, but also raise new questions.
In Figure 1c, we see two phenomena peculiar to semi-grokking: (1) test accuracy â€œspikesâ€ several
times throughout training before finally converging, and (2) training loss fluctuates in a set range.
We leave investigation of these phenomena to future work.
In Figure 5, we observe that there is often transient semi-grokking, where a run hovers around
middling test accuracy for millions of epochs, or has multiple plateaus, before generalising perfectly.
We speculate that each transition corresponds to gradient descent strengthening a new generalising
circuit that is more efficient than any previously strengthened circuit, but took longer to learn. We
would guess that if we had trained for longer, many of the semi-grokking runs would exhibit full
grokking, and many of the runs that didnâ€™t generalise at all would generalise at least partially to show
semi-grokking.
Given the difficulty of demonstrating semi-grokking, we only run this experiment on modular
addition. However, our experience with modular addition shows that if we only care about values
at convergence, we can find them much faster by ungrokking from a grokked network (instead of
semi-grokking from a randomly initialised network). Thus the ungrokking results on other tasks
(Figure 8) provide some support that we would see semi-grokking on those tasks as well.
6. Related work
Grokking. Since Power et al. (2021) discovered grokking, many works have attempted to understand
why it happens. Thilak et al. (2022) suggest that â€œslingshotsâ€ could be responsible for grokking,
particularly in the absence of weight decay, and Notsawo Jr et al. (2023) discuss a similar phenomenon
of â€œoscillationsâ€. In contrast, our explanation applies even where there are no slingshots or oscillations
(as in most of our experiments). Liu et al. (2022) show that for a specific (non-modular) addition
task with an inexpressive architecture, perfect generalisation occurs when there is enough data
to determine the appropriate structured representation. However, such a theory does not explain
semi-grokking. We argue that for more typical tasks on which grokking occurs, the critical factor is
instead the relative efficiencies of ğ¶mem and ğ¶gen . Liu et al. (2023) show that grokking occurs even
in non-algorithmic tasks when parameters are initialised to be very large, because memorisation
happens quickly but it takes longer for regularisation to reduce parameter norm to the â€œGoldilocks
zoneâ€ where generalisation occurs. This observation is consistent with our theory: in non-algorithmic
tasks, we expect there exist efficient, generalising circuits, and the increased parameter norm creates
the final ingredient (slow learning), leading to grokking. However, we expect that in algorithmic tasks
such as the ones we study, the slow learning is caused by some factor other than large parameter
10

Explaining grokking through circuit efficiency
norm at initialisation.
Davies et al. (2023) is the most related work. The authors identify three ingredients for grokking
that mirror ours: a generalising circuit that is slowly learned but is favoured by inductive biases, a
perspective also mirrored in Nanda et al. (2023, Appendix E). We operationalise the â€œinductive biasâ€
ingredient as efficiency at producing large logits with small parameter norm, and to provide significant
empirical support by predicting and verifying the existence of the critical threshold ğ·crit and the novel
behaviours of semi-grokking and ungrokking.
Nanda et al. (2023) identify the trigonometric algorithm by which the networks solve modular
addition after grokking, and show that it grows smoothly over training, and Chughtai et al. (2023)
extend this result to arbitrary group compositions. We use these results to define metrics for the
strength of the different circuits (Appendix B) which we used in preliminary investigations and for
some results in Appendix A (Figures 9 and 10). Merrill et al. (2023) show similar results on sparse
parity: in particular, they show that a sparse subnetwork is responsible for the well-generalising logits,
and that it grows as grokking happens.
Weight decay. While it is widely known that weight decay can improve generalisation (Krogh and
Hertz, 1991), the mechanisms for this effect are multifaceted and poorly understood (Zhang et al.,
2018). We propose a mechanism that is, to the best of our knowledge, novel: generalising circuits tend
to be more efficient than memorising circuits at large dataset sizes, and so weight decay preferentially
strengthens the generalising circuits.
Understanding deep learning through circuit-based analysis. One goal of interpretability is to
understand the internal mechanisms by which neural networks exhibit specific behaviours, often
through the identification and characterisation of specific parts of the network, especially compu-
tational subgraphs (â€œcircuitsâ€) that implement human-interpretable algorithms (Cammarata et al.,
2021; Elhage et al., 2021; Erhan et al., 2009; Geva et al., 2020; Li et al., 2022; Meng et al., 2022;
Olah et al., 2020; Wang et al., 2022). Such work can also be used to understand deep learning.
Olsson et al. (2022) explain a phase change in the training of language models by reference to
induction heads, a family of circuits that produce in-context learning. In concurrent work, Singh et al.
show that the in-context learning from induction heads is later replaced by in-weights learning in the
absence of weight decay, but remains strong when weight decay is present. We hypothesise that this
effect is also explained through circuit efficiency: the in-context learning from induction heads is a
generalising algorithm and so is favoured by weight decay given a large enough dataset size.
Michaud et al. (2023) propose an explanation for power-law scaling (Barak et al., 2022; Hestness
et al., 2017; Kaplan et al., 2020) based on a model in which there are many discrete quanta (algorithms)
and larger models learn more of them. Our explanation involves a similar structure: we posit the
existence of two algorithms (quanta), and analyse the resulting training dynamics.
7. Discussion
Rethinking generalisation. Zhang et al. (2021) pose the question of why deep neural networks
achieve good generalisation even when they are easily capable of memorising a random labelling of
the training data. Our results gesture at a resolution: in the presence of weight decay, circuits that
generalise well are likely to be more efficient given a large enough dataset, and thus are preferred over
memorisation circuits, even when both achieve perfect training loss (Section 4.1). Similar arguments
may hold for other types of regularisation as well.
11

Explaining grokking through circuit efficiency
Necessity of weight decay. The biggest limitation of our explanation is that it relies crucially on
weight decay, but grokking has been observed even when weight decay is not present (Power et al.,
2021; Thilak et al., 2022) (though it is slower and often much harder to elicit (Nanda et al., 2023,
Appendix D.1)). This demonstrates that our explanation is incomplete.
Does it also imply that our explanation is incorrect? We do not think so. We speculate there is
at least one other effect that has a similar regularising effect favouring ğ¶gen over ğ¶mem , such as the
implicit regularisation of gradient descent (Lyu and Li, 2019; Smith and Le, 2017; Soudry et al., 2018;
Wang et al., 2021), and that the speed of the transition from ğ¶mem to ğ¶gen is based on the sum of these
effects and the effect from weight decay. This would neatly explain why grokking takes longer as
weight decay decreases (Power et al., 2021), and does not completely vanish in the absence of weight
decay. Given that there is a potential extension of our theory that explains grokking without weight
decay, and the significant confirming evidence that we have found for our theory in settings with
weight decay, we are overall confident that our explanation is at least one part of the true explanation
when weight decay is present.
Broader applicability: beyond parameter norm. Another limitation is that we only consider one
kind of constraint that gradient descent must navigate: parameter norm. Typically, there are many
other constraints: fitting the training data, capacity in â€œbottleneck activationsâ€ (Elhage et al., 2021),
interference between circuits (Elhage et al., 2022), and more. This may limit the broader applicability
of our theory, despite its success in explaining grokking.
Broader applicability: realistic settings. A natural question is what implications our theory has
for more realistic settings. We expect that the general concepts of circuits, efficiency, and speed of
learning continue to apply. However, in realistic settings, good training performance is typically
achieved when the model has many different circuit families that contribute different aspects (e.g.
language modelling requires spelling, grammar, arithmetic, etc). We expect that these will have a
wide variety of learning speeds and efficiencies (although note that â€œefficiencyâ€ is not as well defined
in this setting, because the circuits donâ€™t get perfect training accuracy).
In contrast, the key property for grokking in â€œalgorithmicâ€ tasks like modular arithmetic is that
there are two clusters of circuit families â€“ one slowly learned, efficient, generalising cluster, and one
quickly learned, inefficient, memorising cluster. In particular, our explanation relies on there being
no circuits in between the two clusters. Therefore we observe a sharp transition in test performance
when shifting from the memorising to the generalising cluster.
Future work. Within grokking, several interesting puzzles are still left unexplained. Why does
the time taken to grok rise super-exponentially as dataset size decreases? How does the random
initialisation interact with efficiency to determine which circuits are found by gradient descent? What
causes generalising circuits to develop slower? Investigating these puzzles is a promising avenue for
further work.
While the direct application of our work is to understand the puzzle of grokking, we are excited
about the potential for understanding deep learning more broadly through the lens of circuit efficiency.
We would be excited to see work looking at the role of circuit efficiency in more realistic settings, and
work that extends circuit efficiency to consider other constraints that gradient descent must navigate.
8. Conclusion
The central question of our paper is: in grokking, why does the networkâ€™s test performance improve
dramatically upon continued training, having already achieved nearly perfect training performance?
Our explanation is: the generalising solution is more â€œefficientâ€ but slower to learn than the memorising
12

Explaining grokking through circuit efficiency
solution. After quickly learning the memorising circuit, gradient descent can still decrease loss
even further by simultaneously strengthening the efficient, generalising circuit and weakening the
inefficient, memorising circuit.
Based on our theory we predict and demonstrate two novel behaviours: ungrokking, in which a
model that has perfect generalisation returns to memorisation when it is further trained on a dataset
with size smaller than the critical threshold, and semi-grokking, where we train a randomly initialised
network on the critical dataset size which results in a grokking-like transition to middling test accuracy.
Our explanation is the only one we are aware of that has made (and confirmed) such surprising
advance predictions, and we have significant confidence in the explanation as a result.
Acknowledgements
Thanks to Paul Christiano, Xander Davies, Seb Farquhar, Geoffrey Irving, Tom Lieberum, Eric Michaud,
Vlad Mikulik, Neel Nanda, Jonathan Uesato, and anonymous reviewers for valuable discussions and
feedback.
References
B. Barak, B. Edelman, S. Goel, S. Kakade, E. Malach, and C. Zhang. Hidden progress in deep learning:
SGD learns parities near the computational limit. Advances in Neural Information Processing Systems,
35:21750â€“21764, 2022.
N. Cammarata, G. Goh, S. Carter, C. Voss, L. Schubert, and C. Olah. Curve circuits. Distill, 2021. doi:
10.23915/distill.00024.006. https://distill.pub/2020/circuits/curve-circuits.
B. Chughtai, L. Chan, and N. Nanda. A toy model of universality: Reverse engineering how networks
learn group operations. arXiv preprint arXiv:2302.03025, 2023.
X. Davies, L. Langosco, and D. Krueger. Unifying grokking and double descent. arXiv preprint
arXiv:2303.06173, 2023.
N. Elhage, N. Nanda, C. Olsson, T. Henighan, N. Joseph, B. Mann, A. Askell, Y. Bai, A. Chen, T. Conerly,
N. DasSarma, D. Drain, D. Ganguli, Z. Hatfield-Dodds, D. Hernandez, A. Jones, J. Kernion, L. Lovitt,
K. Ndousse, D. Amodei, T. Brown, J. Clark, J. Kaplan, S. McCandlish, and C. Olah. A mathematical
framework for Transformer circuits. Transformer Circuits Thread, 2021. https://transformer-
circuits.pub/2021/framework/index.html.
N. Elhage, T. Hume, C. Olsson, N. Schiefer, T. Henighan, S. Kravec, Z. Hatfield-Dodds, R. Lasenby,
D. Drain, C. Chen, R. Grosse, S. McCandlish, J. Kaplan, D. Amodei, M. Wattenberg, and
C. Olah. Toy models of superposition. Transformer Circuits Thread, 2022. https://transformer-
circuits.pub/2022/toy_model/index.html.
D. Erhan, Y. Bengio, A. Courville, and P. Vincent. Visualizing higher-layer features of a deep network.
University of Montreal, 1341(3):1, 2009.
M. Geva, R. Schuster, J. Berant, and O. Levy. Transformer feed-forward layers are key-value memories.
arXiv preprint arXiv:2012.14913, 2020.
J. Hestness, S. Narang, N. Ardalani, G. Diamos, H. Jun, H. Kianinejad, M. Patwary, M. Ali, Y. Yang, and
Y. Zhou. Deep learning scaling is predictable, empirically. arXiv preprint arXiv:1712.00409, 2017.
13

Explaining grokking through circuit efficiency
A.
Jermyn
and
B.
Shlegeris.
Multi-component
learning
and
s-curves,
2022.
URL
https://www.alignmentforum.org/posts/RKDQCB6smLWgs2Mhr/
multi-component-learning-and-s-curves.
J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child, S. Gray, A. Radford, J. Wu,
and D. Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020.
A. Krogh and J. Hertz. A simple weight decay can improve generalization. Advances in neural
information processing systems, 4, 1991.
K. Li, A. K. Hopkins, D. Bau, F. ViÃ©gas, H. Pfister, and M. Wattenberg. Emergent world representations:
Exploring a sequence model trained on a synthetic task. arXiv preprint arXiv:2210.13382, 2022.
Z. Liu, O. Kitouni, N. Nolte, E. J. Michaud, M. Tegmark, and M. Williams. Towards understanding
grokking: An effective theory of representation learning. arXiv preprint arXiv:2205.10343, 2022.
Z. Liu, E. J. Michaud, and M. Tegmark. Omnigrok: Grokking beyond algorithmic data, 2023.
I. Loshchilov and F. Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101,
2019.
K. Lyu and J. Li. Gradient descent maximizes the margin of homogeneous neural networks. arXiv
preprint arXiv:1906.05890, 2019.
M. McCloskey and N. Cohen. Catastrophic interference in connectionist networks: The sequential
learning problem. Psychology of Learning and Motivation, 24:109â€“165, 1989.
K. Meng, D. Bau, A. Andonian, and Y. Belinkov. Locating and editing factual knowledge in GPT. arXiv
preprint arXiv:2202.05262, 2022.
W. Merrill, N. Tsilivis, and A. Shukla. A tale of two circuits: Grokking as competition of sparse and
dense subnetworks. arXiv preprint arXiv:2303.11873, 2023.
E. J. Michaud, Z. Liu, U. Girit, and M. Tegmark. The quantization model of neural scaling. arXiv
preprint arXiv:2303.13506, 2023.
B.
Millidge.
Grokking
â€™grokkingâ€™,
2022.
URL
https://www.beren.io/
2022-01-11-Grokking-Grokking/.
N. Nanda, L. Chan, T. Liberum, J. Smith, and J. Steinhardt. Progress measures for grokking via
mechanistic interpretability. arXiv preprint arXiv:2301.05217, 2023.
P. Notsawo Jr, H. Zhou, M. Pezeshki, I. Rish, G. Dumas, et al. Predicting grokking long before it
happens: A look into the loss landscape of models which grok. arXiv preprint arXiv:2306.13253,
2023.
C. Olah, N. Cammarata, L. Schubert, G. Goh, M. Petrov, and S. Carter. Zoom in: An introduction to
circuits. Distill, 2020. doi: 10.23915/distill.00024.001. https://distill.pub/2020/circuits/zoom-in.
C. Olsson, N. Elhage, N. Nanda, N. Joseph, N. DasSarma, T. Henighan, B. Mann, A. Askell, Y. Bai,
A. Chen, T. Conerly, D. Drain, D. Ganguli, Z. Hatfield-Dodds, D. Hernandez, S. Johnston, A. Jones,
J. Kernion, L. Lovitt, K. Ndousse, D. Amodei, T. Brown, J. Clark, J. Kaplan, S. McCandlish, and C. Olah.
In-context learning and induction heads. Transformer Circuits Thread, 2022. https://transformer-
circuits.pub/2022/in-context-learning-and-induction-heads/index.html.
14

Explaining grokking through circuit efficiency
A. Power, Y. Burda, H. Edwards, I. Babuschkin, and V. Misra. Grokking: Generalization beyond
overfitting on small algorithmic datasets. In Mathematical Reasoning in General Artificial Intelligence
Workshop, ICLR, 2021. URL https://mathai-iclr.github.io/papers/papers/MATHAI_
29_paper.pdf.
R. Ratcliff. Connectionist models of recognition memory: constraints imposed by learning and
forgetting functions. Psychology Review, 97(2):285â€“308, 1990.
A. Singh, S. Chan, T. Moskovitz, E. Grant, A. Saxe, and F. Hill. The transient nature of emergent
in-context learning in transformers. Forthcoming.
S. L. Smith and Q. V. Le. A bayesian perspective on generalization and stochastic gradient descent.
arXiv preprint arXiv:1710.06451, 2017.
D. Soudry, E. Hoffer, M. S. Nacson, S. Gunasekar, and N. Srebro. The implicit bias of gradient descent
on separable data. The Journal of Machine Learning Research, 19(1):2822â€“2878, 2018.
V. Thilak, E. Littwin, S. Zhai, O. Saremi, R. Paiss, and J. Susskind. The slingshot mechanism: An empir-
ical study of adaptive optimizers and the grokking phenomenon. arXiv preprint arXiv:2206.04817,
2022.
A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. u. Kaiser, and I. Polo-
sukhin.
Attention is all you need.
In Advances in Neural Information Processing Systems,
volume 30, 2017. URL https://proceedings.neurips.cc/paper_files/paper/2017/
file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf.
B. Wang, Q. Meng, W. Chen, and T.-Y. Liu. The implicit bias for adaptive optimization algorithms
on homogeneous neural networks. In International Conference on Machine Learning, pages 10849â€“
10858. PMLR, 2021.
K. Wang, A. Variengien, A. Conmy, B. Shlegeris, and J. Steinhardt. Interpretability in the wild: a
circuit for indirect object identification in gpt-2 small. arXiv preprint arXiv:2211.00593, 2022.
C. Zhang, S. Bengio, M. Hardt, B. Recht, and O. Vinyals. Understanding deep learning (still) requires
rethinking generalization. Communications of the ACM, 64(3):107â€“115, 2021.
G. Zhang, C. Wang, B. Xu, and R. Grosse. Three mechanisms of weight decay regularization. arXiv
preprint arXiv:1810.12281, 2018.
15

Explaining grokking through circuit efficiency
0.0
0.5
1.0
Accuracy
train
test
101
10
2
10
5
Loss
0.0
0.5
1.0
1.5
2.0
2.5
3.0
3.5
4.0
Epochs
1e7
30
40
50
Parameter norm
Figure 6 | Examining a single semi-grokking run in detail. We plot accuracy, loss, and parameter
norm over training for a single cherry-picked modular addition run at a dataset size of 1532 (12% of
the full dataset). This run shows transient semi-grokking. At epoch 0.8 Ã— 107, test accuracy rises to
around 0.55, and then stays there for 107 epochs, because ğ¶gen and ğ¶mem efficiencies are balanced.
At epoch 1.8 Ã— 107, we speculate that gradient descent finds an even more efficient ğ¶gen circuit,
as parameter norm drops suddenly and test accuracy rises to 1. At epoch 3.2 Ã— 107 we see test
loss rise, we do not know why. There seem to be multiple phases, perhaps corresponding to the
network transitioning between mixtures of multiple circuits with increasing efficiencies, but further
investigation is needed.
A. Experimental details and more evidence
For all our experiments, we use 1-layer decoder-only transformer networks (Vaswani et al., 2017)
with learned positional embeddings, untied embeddings/unembeddings, The hyperparameters are
as follows: ğ‘‘model = 128 is the residual stream width, ğ‘‘head = 32 is the size of the query, key, and
value vectors for each attention head, ğ‘‘mlp = 512 is the number of neurons in the hidden layer of
the MLP, and we have ğ‘‘model/ğ‘‘head = 4 heads per self-attention layer. We optimise the network with
full batch training (that is, using the entire training dataset for each update) using the AdamW
optimiser (Loshchilov and Hutter, 2019) with ğ›½1 = 0.9, ğ›½2 = 0.98, learning rate of 10âˆ’3, and weight
decay of 1.0. In some of our experiments we vary the weight decay in order to produce networks
with varying parameter norm.
Following Power et al. (2021), for a binary operation ğ‘¥â—¦ğ‘¦, we construct a dataset of the form
âŸ¨ğ‘¥âŸ©âŸ¨â—¦âŸ©âŸ¨ğ‘¦âŸ©âŸ¨=âŸ©âŸ¨ğ‘¥â—¦ğ‘¦âŸ©, where âŸ¨ğ‘âŸ©stands for the token corresponding to the element ğ‘. We choose a
fraction of this dataset at random as the train dataset, and the remainder as the test dataset. The
first 4 tokens âŸ¨ğ‘¥âŸ©âŸ¨â—¦âŸ©âŸ¨ğ‘¦âŸ©âŸ¨=âŸ©are the input to the network, and we train with cross-entropy loss over the
final token âŸ¨ğ‘¥â—¦ğ‘¦âŸ©. For all modular arithmetic tasks we use the modulus ğ‘= 113, so for example the
size of the full dataset for modular addition is ğ‘2 = 12769, and ğ‘‘vocab = 115, including the âŸ¨+âŸ©and
âŸ¨=âŸ©tokens.
16

Explaining grokking through circuit efficiency
0.0
0.2
0.4
0.6
0.8
1.0
Epochs
1e6
0.0
0.2
0.4
0.6
0.8
1.0
Test accuracy
400
600
800
1000
1200
1400
Reduced dataset size
Figure 7 | Many ungrokking runs. We show test accuracy over epochs for a range of ungrokking
runs for modular addition. Each line represents a single run, and we sweep over 7 geometrically
spaced dataset sizes in [390, 1494] with 10 seeds each. Each run is initialised with parameters from
a network trained on the full dataset (the initialisation runs are not shown), so test accuracy starts
at 1 for all runs. When the dataset size is small enough, the network ungroks to poor test accuracy,
while train accuracy remains at 1 (not shown). For an intermediate dataset size, we see ungrokking
to middling test accuracy as ğ¶gen and ğ¶mem efficiencies are balanced.
A.1. Semi-grokking
In Section 5.3 we looked at all semi-grokking training runs in Figure 5. Here, we investigate a single
example of transient semi-grokking in more detail (see Figure 6). We speculate that there are multiple
circuits with increasing efficiencies for ğ¶gen , and in these cases the more efficient circuits are slower to
learn. This would explain transient semi-grokking: gradient descent first finds a less efficient ğ¶gen and
we see middling generalisation, but since we are using the upper range of ğ·crit, eventually gradient
descent finds a more efficient ğ¶gen leading to full generalisation.
A.2. Ungrokking
In Figure 7, we show many ungrokking runs for modular addition, and in Figure 8 we show ungrokking
across many other tasks.
We have already seen that ğ·crit is affected by the random initialisation. It is interesting to compare
ğ·crit when starting with a given random initialisation, and when ungrokking from a network that was
trained to full generalisation with the same random initialisation. Figure 5 shows a semi-grokking run
that achieves a test accuracy of âˆ¼0.7 with a dataset size of âˆ¼2000, while Figure 7 shows ungrokking
runs that achieve a test accuracy of âˆ¼0.7 with a dataset size of around 800â€“1000, less than half of
what the semi-grokking run required.
In Figure 10b, the final test accuracy after ungrokking shows a smooth relationship with dataset
size, which we might expect if ğ¶gen is getting stronger on a smoothly increasing number of inputs
compared to ğ¶mem . However due to the difficulties discussed previously, we donâ€™t see a smooth
relationship between test accuracy and dataset size in semi-grokking.
These results suggest that ğ·crit is an oversimplified concept, because in reality the initialisation
and training dynamics affect which circuits are found, and therefore the dataset size at which we see
middling generalisation.
17

Explaining grokking through circuit efficiency
103
104
0.0
0.5
1.0
Test accuracy
x/y mod P
103
104
x
y mod P
103
104
xy mod P
103
104
0.0
0.5
1.0
Test accuracy
x/y mod P or x
y mod P
103
104
x2 + y2 mod P
103
104
x2 + xy + y2 mod P
103
104
Reduced dataset size
0.0
0.5
1.0
Test accuracy
xyx
1 for x, y
S5
103
104
Reduced dataset size
xy for x, y
S5
103
104
Reduced dataset size
xyx for x, y
S5
0.1
0.5
1.0
1.5
2.0
Weight decay
Average accuracy
Per seed accuracy
Figure 8 | Ungrokking on many other tasks. We plot test accuracy against reduced dataset size for
many other modular arithmetic and symmetric group tasks (Power et al., 2021). For each run, we
train on the full dataset (achieving 100% accuracy), and then further train on a reduced subset of the
dataset for 100k steps. The results show clear ungrokking, since in many cases test accuracy falls
below 100%, often to nearly 0%. For most datasets the transition point is independent of weight
decay (different coloured lines almost perfectly overlap).
18

Explaining grokking through circuit efficiency
0
2000
4000
6000
8000
10000
Epochs
10
9
10
7
10
5
10
3
10
1
101
Loss
Train loss
Test loss
0
15
30
45
60
Ctrig logit
Cmem logit
Parameter norm
Figure 9 | Grokking occurs because ğ¶gen is more efficient than ğ¶mem . We show loss, parameter
norm, and the value of the correct logit for ğ¶gen and ğ¶mem for a randomly-picked training run. By step
200, the train accuracy is already perfect (not shown), train loss is low while test loss has risen, and
parameter norm is at its maximum value, indicating strong ğ¶mem . Train loss continues to fall rapidly
until step 1500, as parameter norm falls and the ğ¶gen logit becomes higher than the ğ¶mem logit. At
step 3500, test loss starts to fall as the high ğ¶gen logit starts to dominate, and by step 6000 we get
good generalisation.
A.3. ğ¶gen and ğ¶mem development during grokking
In Figure 9 we show ğ¶gen and ğ¶mem development via the proxy measures defined in Appendix B for
a randomly-picked grokking run. Looking at these measures was very useful to form a working
theory for why grokking happens. However as we note in Appendix B, these proxy measures tend to
overestimate ğ¶gen and underestimate ğ¶mem .
We note some interesting phenomena in Figure 9:
1. Between epochs 200 to 1500, both the ğ¶gen and ğ¶mem logits are rising while parameter norm is
falling, indicating that gradient descent is improving efficiency (possibly by removing irrelevant
parameters).
2. After epoch 4000, the ğ¶gen logit falls while the ğ¶mem logit is already âˆ¼0. Since test loss continues
to fall, we expect that incorrect logits from ğ¶mem on the test dataset are getting cleaned up, as
described in Nanda et al. (2023).
A.4. Tradeoffs between ğ¶gen and ğ¶mem
In Section 4.1 we looked at the efficiency of ğ¶gen -only and ğ¶mem -only circuits. In this section we train
on varying dataset sizes so that the network develops a mixture of ğ¶gen and ğ¶mem circuits, and study
their relative strength using the correct logit as a proxy measure (described in Appendix B).
As we demonstrated previously, ğ¶mem â€™s efficiency drops with increasing dataset size, while ğ¶gen â€™s
stays constant. Theorem D.4 (case 2) suggests that parameter norm allocated to a circuit is proportional
to efficiency, and since logit values also increase with parameter norm, this implies that the ratio of
the ğ¶gen to ğ¶mem logit ğ‘œğ‘¦
ğ‘¡/ğ‘œğ‘¦
ğ‘šshould increase monotonically with dataset size.
In Figure 10a we see exactly this: the logit ratio changes monotonically (over 6 orders of magnitude)
with increasing dataset size.
Due to the difficulties in training to convergence at small dataset sizes, we initialised all parameters
19

Explaining grokking through circuit efficiency
103
104
Dataset size
10
2
100
102
104
oy
t / oy
m
(a) Logit ratio (ğ‘œğ‘¦
ğ‘¡/ğ‘œğ‘¦
ğ‘š) vs dataset size (ğ·).
Colours correspond to different bucketed val-
ues of parameter norm (ğ‘ƒ). Each line shows
that as dataset size increases, a fixed param-
eter norm (fixed colour) is being reallocated
smoothly towards increasing the trigonometric
logit compared to the memorisation logit.
103
104
Dataset size
0.0
0.2
0.4
0.6
0.8
1.0
Test accuracy
(b) Test accuracy vs dataset size (ğ·). We
see a smooth dependence on dataset size.
Each line shows that as dataset size in-
creases, the reallocation of a fixed parame-
ter norm (fixed colour) towards ğ¶gen from
ğ¶mem results in increasing accuracy.
2 Ã— 101
3 Ã— 101
4 Ã— 101
Parameter norm
Figure 10 | Relative strength at convergence. We report logit ratios and test accuracy at convergence
across a range of training runs, generated by sweeping over the weight decay and random seed to
obtain different parameter norms at the same dataset size. We use the ungrokking runs from Figure 7,
so every run is initialised with parameters obtained by training on the full dataset.
from a ğ¶gen -only network trained on the full dataset. We confirmed that in all the runs, at convergence,
the training loss from the ğ¶gen -initialised network was lower than the training loss from a randomly
initialised network, indicating that this initialisation allows our optimiser to find a better minimum
than from random initialisation.
B. ğ¶gen and ğ¶mem in modular addition
In modular addition, given two integers ğ‘, ğ‘and a modulus ğ‘as input, where 0 â‰¤ğ‘, ğ‘< ğ‘, the task
is to predict ğ‘+ ğ‘mod ğ‘. Nanda et al. (2023) identified the generalising algorithm implemented
by a 1-layer transformer after grokking (visualised in Figure 11), which we call the â€œtrigonometricâ€
algorithm. In this section we summarise the algorithm, and explain how we produce our proxy metrics
for the strength of ğ¶gen and ğ¶mem .
Trigonometric logits. We explain the structure of the logits produced by the trigonometric algorithm.
For each possible label ğ‘âˆˆ{0, 1, . . . ğ‘âˆ’1}, the trigonometric logit ğ‘œğ‘will be given by Ã
ğœ”ğ‘˜cos(ğœ”ğ‘˜(ğ‘+
ğ‘âˆ’ğ‘)), for a few key frequencies ğœ”ğ‘˜= 2ğœ‹ğ‘˜
ğ‘with integer ğ‘˜. For the true label ğ‘âˆ—= ğ‘+ ğ‘mod ğ‘, the
term ğœ”ğ‘˜(ğ‘+ ğ‘âˆ’ğ‘âˆ—) is an integer multiple of 2ğœ‹, and so cos(ğœ”ğ‘˜(ğ‘+ ğ‘âˆ’ğ‘âˆ—)) = 1. For any incorrect label
ğ‘â‰ ğ‘+ ğ‘mod ğ‘, it is very likely that at least some of the key frequencies satisfy cos(ğœ”ğ‘˜(ğ‘+ ğ‘âˆ’ğ‘)) â‰ª1,
creating a large difference between ğ‘œğ‘and ğ‘œğ‘âˆ—.
Trigonometric algorithm. There is a set of key frequencies ğœ”ğ‘˜. (These frequencies are typically
whichever frequencies were highest at the time of random initialisation.) For an arbitrary label ğ‘, the
logit ğ‘œğ‘is computed as follows:
1. Embed the one-hot encoded number ğ‘to sin(ğœ”ğ‘˜ğ‘) and cos(ğœ”ğ‘˜ğ‘) for the various frequencies ğœ”ğ‘˜.
20

Explaining grokking through circuit efficiency
Figure 11 | The trigonometric algorithm for modular arithmetic (reproduced from Nanda et al.
(2023)). Given two numbers ğ‘and ğ‘, the model projects each point onto a corresponding rotation
using its embedding matrix. Using its attention and MLP layers, it then composes the rotations to
get a representation of ğ‘+ ğ‘mod ğ‘. Finally, it â€œreads offâ€ the logits for each ğ‘âˆˆ{0, 1, . . . , ğ‘âˆ’1},
by rotating by âˆ’ğ‘to get ğ‘ğ‘œğ‘ (ğœ”(ğ‘+ ğ‘âˆ’ğ‘)), which is maximised when ğ‘+ ğ‘â‰¡ğ‘mod ğ‘ƒ(since ğœ”is a
multiple of 2ğœ‹).
Do the same for ğ‘.
2. Compute cos(ğœ”ğ‘˜(ğ‘+ ğ‘)) and sin(ğœ”ğ‘˜(ğ‘+ ğ‘)) using the intermediate attention and MLP layers via
the trigonometric identities:
cos(ğœ”ğ‘˜(ğ‘+ ğ‘)) = cos(ğœ”ğ‘˜ğ‘) cos(ğœ”ğ‘˜ğ‘) âˆ’sin(ğœ”ğ‘˜ğ‘) sin(ğœ”ğ‘˜ğ‘)
sin(ğœ”ğ‘˜(ğ‘+ ğ‘)) = sin(ğœ”ğ‘˜ğ‘) cos(ğœ”ğ‘˜ğ‘) + cos(ğœ”ğ‘˜ğ‘) sin(ğœ”ğ‘˜ğ‘)
3. Use the output and unembedding matrices to implement the trigonometric identity:
ğ‘œğ‘=
âˆ‘ï¸
ğœ”ğ‘˜
cos(ğœ”ğ‘˜(ğ‘+ ğ‘âˆ’ğ‘)) =
âˆ‘ï¸
ğœ”ğ‘˜
cos(ğœ”ğ‘˜(ğ‘+ ğ‘)) cos(ğœ”ğ‘˜ğ‘) + sin(ğœ”ğ‘˜(ğ‘+ ğ‘)) sin(ğœ”ğ‘˜ğ‘).
Isolating trigonometric logits. Given a classifier â„, we can aggregate its logits on every possible
input, resulting in a vector Â®ğ‘â„of length ğ‘3 where Â®ğ‘ğ‘,ğ‘,ğ‘
â„
= ğ‘œğ‘
â„(â€œğ‘+ ğ‘=â€) is the logit for label ğ‘on the
input (ğ‘, ğ‘). We are interested in identifying the contribution of the trigonometric algorithm to Â®ğ‘â„.
We use the same method as Chughtai et al. (2023) and restrict Â®ğ‘â„to a much smaller trigonometric
subspace.
For a frequency ğœ”ğ‘˜, let us define the ğ‘3-dimensional vector Â®ğ‘ğœ”ğ‘˜as Â®ğ‘ğ‘,ğ‘,ğ‘
ğœ”ğ‘˜
= cos(ğœ”ğ‘˜(ğ‘+ ğ‘âˆ’ğ‘)). Since
Â®ğ‘ğœ”ğ‘˜= Â®ğ‘ğœ”ğ‘âˆ’ğ‘˜, we set 1 â‰¤ğ‘˜â‰¤ğ¾, where ğ¾= âŒˆ(ğ‘âˆ’1)/2âŒ‰, to obtain ğ¾distinct vectors, ignoring the
constant bias vector. These vectors are orthogonal, as they are part of a Fourier basis.
Notice that any circuit that was exactly following the learned algorithm described above would only
produce logits in the directions Â®ğ‘ğœ”ğ‘˜for the key frequencies ğœ”ğ‘˜. So, we can define the trigonometric
contribution to Â®ğ‘â„as the projection of Â®ğ‘â„onto the directions Â®ğ‘ğœ”ğ‘˜. We may not know the key frequencies
in advance, but we can sum over all ğ¾of them, giving the following definition for trigonometric logits:
Â®ğ‘â„,ğ‘‡=
ğ¾
âˆ‘ï¸
ğ‘˜=1
( Â®ğ‘â„Â· Ë†ğ‘ğœ”ğ‘˜)Ë†ğ‘ğœ”ğ‘˜
21

Explaining grokking through circuit efficiency
where Ë†ğ‘ğœ”ğ‘˜is the normalised version of Â®ğ‘ğœ”ğ‘˜. This corresponds to projecting onto a ğ¾-dimensional
subspace of the ğ‘3-dimensional space in which Â®ğ‘â„lives.
Memorisation logits. Early in training, neural networks memorise the training dataset without
generalising, suggesting that there exists a memorisation algorithm, implemented by the circuit
ğ¶mem 1. Unfortunately, we do not understand the algorithm underlying memorisation, and so cannot
design a similar procedure to isolate ğ¶mem â€™s contribution to the logits. However, we hypothesise that
for modular addition, ğ¶gen and ğ¶mem are the only two circuit families of importance for the loss. This
allows us to define the ğ¶mem contribution to the logits as the residual:
Â®ğ‘â„,ğ‘€= Â®ğ‘â„âˆ’Â®ğ‘â„,ğ‘‡
ğ¶trig and ğ¶mem circuits. We say that a circuit is a ğ¶trig circuit if it implements the ğ¶trig algorithm, and
similarly for ğ¶mem circuits. Importantly, this is a many-to-one mapping: there are many possible
circuits that implement a given algorithm.
We isolate ğ¶trig (Â®ğ‘œğ‘¡) and ğ¶mem (Â®ğ‘œğ‘š) logits by projecting the output logits (Â®ğ‘œ) as described in
Appendix B. We cannot directly measure the circuit weights ğ‘¤ğ‘¡and ğ‘¤ğ‘š, but instead use an indirect
measure: the value of the logit for the correct class given by each circuit, i.e. ğ‘œğ‘¦
ğ‘¡and ğ‘œğ‘¦
ğ‘š.
Flaws These metrics should be viewed as an imperfect proxy measure for the true strength of the
trigonometric and memorisation circuits, as they have a number of flaws:
1. When both ğ¶trig and ğ¶mem are present in the network, they are both expected to produce high
values for the correct logits, and low values for incorrect logits, on the train dataset. Since the
ğ¶trig and ğ¶mem logits are correlated, it becomes more likely that Â®ğ‘â„,ğ‘‡captures ğ¶mem logits too.
2. In this case we would expect our proxy measure to overestimate the strength of ğ¶trig and
underestimate the strength of ğ¶mem . In fact, in our experiments we do see large negative correct
logit values for ğ¶mem on training for semi-grokking, which probably arises because of this effect.
3. Logits are not inherently meaningful; what matters for loss is the extent to which the correct
logit is larger than the incorrect logits. This is not captured by our proxy metric, which only
looks at the size of the correct logit. In a binary classification setting, we could instead use the
difference between the correct and incorrect logit, but it is not clear what a better metric would
be in the multiclass setting.
C. Details for the minimal example
In Figure 2 we show that two ingredients: multiple circuits with different efficiencies, and slow and
fast circuit development, are sufficient to reproduce learning curves that qualitatively demonstrate
grokking. In Table 1 we provide details about the simulation used to produce this figure.
As explained in Section 3, the logits produced by ğ¶gen and ğ¶mem are given by:
ğ‘œğ‘¦
ğº(ğ‘¥) = ğŸ™[(ğ‘¥, ğ‘¦) âˆˆD or (ğ‘¥, ğ‘¦) âˆˆDtest]
(3)
ğ‘œğ‘¦
ğ‘€(ğ‘¥) = ğŸ™[(ğ‘¥, ğ‘¦) âˆˆD or (ğ‘¥, ğ‘¦) âˆˆDmem]
(4)
1In reality, there are at least two different memorisation algorithms: commutative memorisation (which predicts the
same answer for (ğ‘, ğ‘) and (ğ‘, ğ‘)) and non-commutative memorisation (which does not). However, this difference does not
matter for our analyses, and we will call both of these â€œmemorisationâ€ in this paper.
22

Explaining grokking through circuit efficiency
Table 1 | Hyperparameters used for our simulations.
(a) ğ¶gen learned slower but more
efficient than ğ¶mem .
Parameter
Value
ğ‘ƒğ‘”
1
ğ‘ƒğ‘š
2
ğœ…
1.2
ğ›¼
0.005
ğ‘¤ğ‘”1(0)
0
ğ‘¤ğ‘”2(0)
0.005
ğ‘¤ğ‘š1(0)
0
ğ‘¤ğ‘š2(0)
1
ğ‘
113
ğœ†
0.01
(b) ğ¶gen less efficient than ğ¶mem .
Parameter
Value
ğ‘ƒğ‘”
4
ğ‘ƒğ‘š
2
ğœ…
1.2
ğ›¼
0.005
ğ‘¤ğ‘”1(0)
0
ğ‘¤ğ‘”2(0)
0.005
ğ‘¤ğ‘š1(0)
0
ğ‘¤ğ‘š2(0)
1
ğ‘
113
ğœ†
0.01
(c) ğ¶gen and ğ¶mem learned at equal
speeds.
Parameter
Value
ğ‘ƒğ‘”
1
ğ‘ƒğ‘š
2
ğœ…
1.2
ğ›¼
0.005
ğ‘¤ğ‘”1(0)
0
ğ‘¤ğ‘”2(0)
1
ğ‘¤ğ‘š1(0)
0
ğ‘¤ğ‘š2(0)
1
ğ‘
113
ğœ†
0.01
These are scaled by two independent weights for each circuit, giving the overall logits as:
ğ‘œğ‘¦(ğ‘¥) = ğ‘¤ğº1ğ‘¤ğº2ğ‘œğ‘¦
ğº(ğ‘¥) + ğ‘¤ğ‘€1ğ‘¤ğ‘€2ğ‘œğ‘¦
ğ‘€(ğ‘¥)
(5)
We model the parameter norms according to the scaling efficiency in Section D.1, inspired by a
ğœ…-layer MLP with Relu activations and without biases:
ğ‘ƒâ€²
ğ‘= (ğ‘¤ğ‘1ğ‘¤ğ‘2)1/ğœ…ğ‘ƒğ‘for ğ‘âˆˆ(ğ‘”, ğ‘š).
From Equations (3) to (5) we get the following equations for train and test loss respectively:
Ltrain = âˆ’log
exp(ğ‘¤ğ‘”1ğ‘¤ğ‘”2 + ğ‘¤ğ‘š1ğ‘¤ğ‘š2)
(ğ‘âˆ’1) + exp(ğ‘¤ğ‘”1ğ‘¤ğ‘”2 + ğ‘¤ğ‘š1ğ‘¤ğ‘š2) + Lwd,
Ltest = âˆ’log
exp(ğ‘¤ğ‘”1ğ‘¤ğ‘”2)
(ğ‘âˆ’2) + exp(ğ‘¤ğ‘”1ğ‘¤ğ‘”2) + exp(ğ‘¤ğ‘š1ğ‘¤ğ‘š2) + Lwd,
where ğ‘is the number of labels, and the weight decay loss is:
Lwd = ğ‘ƒâ€²2
ğ‘”+ ğ‘ƒâ€²2
ğ‘š.
The weights ğ‘¤ğ‘ğ‘–are updated based on gradient descent:
ğ‘¤ğ‘ğ‘–(ğœ) â†ğ‘¤ğ‘ğ‘–(ğœâˆ’1) âˆ’ğœ†ğœ•Ltrain
ğœ•ğ‘¤ğ‘ğ‘–
where ğœ†is a learning rate. The initial values of the parameters are ğ‘¤ğ‘ğ‘–(0). In Table 1 we list the values
of the simulation hyperparameters.
D. Proofs of theorems
We assume we have a set of inputs ğ‘‹, a set of labels ğ‘Œ, and a training dataset, D = {(ğ‘¥1, ğ‘¦1), . . . (ğ‘¥ğ·, ğ‘¦ğ·)}.
Let â„be a classifier that assigns a real-valued logit for each possible label given an input. We denote
23

Explaining grokking through circuit efficiency
an individual logit as ğ‘œğ‘¦
â„(ğ‘¥) B â„(ğ‘¥, ğ‘¦). When the input ğ‘¥is clear from context, we will denote the logit
as ğ‘œğ‘¦
â„. Excluding weight decay, the loss for the classifier is given by the softmax cross-entropy loss:
Lx-ent(â„) = âˆ’1
ğ·
âˆ‘ï¸
(ğ‘¥,ğ‘¦)âˆˆD
log
exp(ğ‘œğ‘¦
â„)
Ã
ğ‘¦â€²âˆˆğ‘Œ
exp(ğ‘œğ‘¦â€²
â„)
.
For any ğ‘âˆˆâ„, let ğ‘Â·â„be the classifier whose logits are multipled by ğ‘, that is, (ğ‘Â·â„)(ğ‘¥, ğ‘¦) = ğ‘Ã—â„(ğ‘¥, ğ‘¦).
Intuitively, once a classifier achieves perfect accuracy, then the true class logit ğ‘œğ‘¦âˆ—will be larger than
any incorrect class logit ğ‘œğ‘¦â€², and so loss can be further reduced by scaling up all of the logits further
(increasing the gap between ğ‘œğ‘¦âˆ—and ğ‘œğ‘¦â€²).
Theorem D.1. Suppose that the classifier â„has perfect accuracy, that is, for any (ğ‘¥, ğ‘¦âˆ—) âˆˆD and any
ğ‘¦â€² â‰ ğ‘¦âˆ—we have ğ‘œğ‘¦âˆ—
â„> ğ‘œğ‘¦â€²
â„. Then, for any ğ‘> 1, we have Lx-ent(ğ‘Â· â„) < Lx-ent(â„).
Proof. First, note that we can rewrite the loss function as:
Lx-ent(â„) = âˆ’1
ğ·
âˆ‘ï¸
(ğ‘¥,ğ‘¦âˆ—)
log
exp(ğ‘œğ‘¦âˆ—
â„)
Ã
ğ‘¦â€² exp(ğ‘œğ‘¦â€²
â„)
= 1
ğ·
âˆ‘ï¸
(ğ‘¥,ğ‘¦âˆ—)
log
Â©Â­Â­Â­
Â«
Ã
ğ‘¦â€² exp(ğ‘œğ‘¦â€²
â„)
exp(ğ‘œğ‘¦âˆ—
â„)
ÂªÂ®Â®Â®
Â¬
= 1
ğ·
âˆ‘ï¸
(ğ‘¥,ğ‘¦âˆ—)
log Â©Â­
Â«
1 +
âˆ‘ï¸
ğ‘¦â€²â‰ ğ‘¦âˆ—
exp(ğ‘œğ‘¦â€²
â„âˆ’ğ‘œğ‘¦âˆ—
â„)ÂªÂ®
Â¬
Since we are given that ğ‘œğ‘¦âˆ—
â„> ğ‘œğ‘¦â€²
â„, for any ğ‘> 1 we have ğ‘(ğ‘œğ‘¦â€²
â„âˆ’ğ‘œğ‘¦âˆ—
â„)) < ğ‘œğ‘¦â€²
â„âˆ’ğ‘œğ‘¦âˆ—
â„. Since exp, log,
and sums are all monotonic, this gives us our desired result:
Lx-ent(ğ‘Â·â„) = 1
ğ·
âˆ‘ï¸
(ğ‘¥,ğ‘¦âˆ—)
log Â©Â­
Â«
1 +
âˆ‘ï¸
ğ‘¦â€²â‰ ğ‘¦âˆ—
exp(ğ‘(ğ‘œğ‘¦â€²
â„âˆ’ğ‘œğ‘¦âˆ—
â„))ÂªÂ®
Â¬
< 1
ğ·
âˆ‘ï¸
(ğ‘¥,ğ‘¦âˆ—)
log Â©Â­
Â«
1 +
âˆ‘ï¸
ğ‘¦â€²â‰ ğ‘¦âˆ—
exp(ğ‘œğ‘¦â€²
â„âˆ’ğ‘œğ‘¦âˆ—
â„)ÂªÂ®
Â¬
= Lx-ent(â„).
â–¡
We now move on to Theorem D.4. First we establish some basic lemmas that will be used in the
proof:
Lemma D.2. Let ğ‘, ğ‘, ğ‘Ÿâˆˆâ„with ğ‘, ğ‘â‰¥0 and 0 < ğ‘Ÿâ‰¤1. Then (ğ‘+ ğ‘)ğ‘Ÿâ‰¤ğ‘ğ‘Ÿ+ ğ‘ğ‘Ÿ.
Proof. The case with ğ‘= 0 or ğ‘= 0 is clear, so let us consider ğ‘, ğ‘> 0. Let ğ‘¥=
ğ‘
ğ‘+ğ‘and ğ‘¦=
ğ‘
ğ‘+ğ‘. Since
0 â‰¤ğ‘¥â‰¤1, we have ğ‘¥(1âˆ’ğ‘Ÿ) â‰¤1, which implies ğ‘¥â‰¤ğ‘¥ğ‘Ÿ. Similarly ğ‘¦â‰¤ğ‘¦ğ‘Ÿ. Thus ğ‘¥ğ‘Ÿ+ ğ‘¦ğ‘Ÿâ‰¥ğ‘¥+ ğ‘¦= 1.
Substituting in the values of ğ‘¥and ğ‘¦we get
ğ‘ğ‘Ÿ+ğ‘ğ‘Ÿ
(ğ‘+ğ‘)ğ‘Ÿâ‰¥1, which when rearranged gives us the desired
result.
â–¡
Lemma D.3. For any ğ‘¥, ğ‘, ğ‘Ÿâˆˆâ„with ğ‘Ÿâ‰¥1, there exists some ğ›¿> 0 such that for any ğœ–< ğ›¿we have
ğ‘¥ğ‘Ÿâˆ’(ğ‘¥âˆ’ğœ–)ğ‘Ÿ> ğ›¿(ğ‘Ÿğ‘¥ğ‘Ÿâˆ’1 âˆ’ğ‘).
Proof. The function ğ‘“(ğ‘¥) = ğ‘¥ğ‘Ÿis everywhere-differentiable and has derivative ğ‘Ÿğ‘¥ğ‘Ÿâˆ’1. Thus we can
choose ğ›¿such that for any ğœ–< ğ›¿we have âˆ’ğ‘< ğ‘¥ğ‘Ÿâˆ’(ğ‘¥âˆ’ğœ–)ğ‘Ÿ
ğ›¿
âˆ’ğ‘Ÿğ‘¥ğ‘Ÿâˆ’1 < ğ‘. Rearranging, we get ğ‘¥ğ‘Ÿâˆ’(ğ‘¥âˆ’ğœ–)ğ‘Ÿ>
ğ›¿(ğ‘Ÿğ‘¥ğ‘Ÿâˆ’1 âˆ’ğ‘) as desired.
â–¡
24

Explaining grokking through circuit efficiency
D.1. Weight decay favours efficient circuits
To flesh out the argument in Section 3, we construct a minimal example of multiple circuits {ğ¶1, . . . ğ¶ğ¼}
of varying efficiencies that can be scaled up or down through a set of non-negative weights ğ‘¤ğ‘–. Our
classifier is given by â„= Ãğ¼
ğ‘–=1 ğ‘¤ğ‘–ğ¶ğ‘–, that is, the output â„(ğ‘¥, ğ‘¦) is given by Ãğ¼
ğ‘–=1 ğ‘¤ğ‘–ğ¶ğ‘–(ğ‘¥, ğ‘¦).
We take circuits ğ¶ğ‘–that are normalised, that is, they produce the same average logit value. ğ‘ƒğ‘–
denotes the parameter norm of the normalised circuit ğ¶ğ‘–. We decide to call a circuit with lower ğ‘ƒğ‘–
more efficient. However, it is hard to define efficiency precisely. Consider instead the parameter
norm ğ‘ƒâ€²
ğ‘–of the scaled circuit ğ‘¤ğ‘–ğ¶ğ‘–. If we define efficiency as either the ratio âˆ¥Â®ğ‘œğ¶ğ‘–âˆ¥/ğ‘ƒâ€²
ğ‘–or the derivative
ğ‘‘âˆ¥Â®ğ‘œğ¶ğ‘–âˆ¥/ğ‘‘ğ‘ƒâ€²
ğ‘–, then it would vary with ğ‘¤ğ‘–since Â®ğ‘œğ¶ğ‘–and ğ‘ƒâ€²
ğ‘–can in general have different relationships
with ğ‘¤ğ‘–. We prefer ğ‘ƒğ‘–as a measure of relative efficiency as it is intrinsic to ğ¶ğ‘–rather than depending
on its scaling ğ‘¤ğ‘–.
Gradient descent operates over the weights ğ‘¤ğ‘–(but not ğ¶ğ‘–or ğ‘ƒğ‘–) to minimise L = Lx-ent + ğ›¼Lwd.
Lx-ent can easily be rewritten in terms of ğ‘¤ğ‘–, but for Lwd we need to model the parameter norm of the
scaled circuits ğ‘¤ğ‘–ğ¶ğ‘–. Notice that, in a ğœ…-layer MLP with Relu activations and without biases, scaling
all parameters by a constant ğ‘scales the outputs by ğ‘ğœ…. Inspired by this observation, we model the
parameter norm of ğ‘¤ğ‘–ğ¶ğ‘–as ğ‘¤1/ğœ…
ğ‘–
ğ‘ƒğ‘–for some ğœ…> 0. This gives the following effective loss:
L( Â®ğ‘¤) = Lx-ent
 
ğ¼âˆ‘ï¸
ğ‘–=1
ğ‘¤ğ‘–ğ¶ğ‘–
!
+ ğ›¼
2
ğ¼âˆ‘ï¸
ğ‘–=1
(ğ‘¤
1
ğœ…
ğ‘–ğ‘ƒğ‘–)2
We will generalise this to any ğ¿ğ‘-norm (where ğ‘> 0). Standard weight decay corresponds to
ğ‘= 2. We will also generalise to arbitrary differentiable, bounded training loss functions, instead of
cross-entropy loss specifically. In particular, we assume that there is some differentiable Ltrain(â„) such
that there exists a finite bound ğµâˆˆâ„such that âˆ€â„: Ltrain(â„) â‰¥ğµ. (In the case of cross-entropy loss,
ğµ= 0.)
With these generalisations, the overall loss is now given by:
L( Â®ğ‘¤) = Ltrain
 
ğ¼âˆ‘ï¸
ğ‘–=1
ğ‘¤ğ‘–ğ¶ğ‘–
!
+ ğ›¼
ğ‘
ğ¼âˆ‘ï¸
ğ‘–=1
(ğ‘¤
1
ğœ…
ğ‘–ğ‘ƒğ‘–)ğ‘
(6)
The following theorem establishes that the optimal weight vector allocates more weight to more
efficient circuits, under the assumption that the circuits produce identical logits on the training
dataset.
Theorem D.4. Given ğ¼circuits ğ¶ğ‘–and associated ğ¿ğ‘parameter norms ğ‘ƒğ‘–, assume that every circuit
produces the same logits on the training dataset, i.e. âˆ€ğ‘–, ğ‘—, âˆ€(ğ‘¥, _) âˆˆD, âˆ€ğ‘¦â€² âˆˆğ‘Œwe have ğ‘œğ‘¦â€²
ğ¶ğ‘–(ğ‘¥) = ğ‘œğ‘¦â€²
ğ¶ğ‘—(ğ‘¥).
Then, any weight vector Â®ğ‘¤âˆ—âˆˆâ„ğ¼that minimizes the loss in Equation 6 subject to ğ‘¤ğ‘–â‰¥0 satisfies:
1. If ğœ…â‰¥ğ‘, then ğ‘¤âˆ—
ğ‘–= 0 for all ğ‘–such that ğ‘ƒğ‘–> minğ‘—ğ‘ƒğ‘—.
2. If 0 < ğœ…< ğ‘, then ğ‘¤âˆ—
ğ‘–âˆğ‘ƒ
âˆ’ğ‘ğœ…
ğ‘âˆ’ğœ…
ğ‘–
.
Intuition . Since every circuit produces identical logits, their weights are interchangeable with each
other from the perspective of Lx-ent, and so we must analyse how interchanging weights affects Lwd.
Lwd grows as ğ‘‚(ğ‘¤2/ğœ…
ğ‘–
). When ğœ…> 2, Lwd grows sublinearly, and so it is cheaper to add additional
weight to the largest weight, creating a â€œrich get richerâ€ effect that results in a single maximally
efficient circuit getting all of the weight. When ğœ…< 2, Lwd grows superlinearly, and so it is cheaper
to add additional weight to the smallest weight. As a result, every circuit is allocated at least some
weight, though more efficient circuits are still allocated higher weight than less efficient circuits.
25

Explaining grokking through circuit efficiency
Sketch . The assumption that every circuit produces the same logits on the training dataset implies
that Ltrain is purely a function of Ãğ¼
ğ‘–=1 ğ‘¤ğ‘–. So, for Ltrain, a small increase ğ›¿ğ‘¤to ğ‘¤ğ‘–can be balanced by
a corresponding decrease ğ›¿ğ‘¤to some other weight ğ‘¤ğ‘—.
For Lwd, an increase ğ›¿ğ‘¤to ğ‘¤ğ‘–produces a change of approximately ğ›¿Lwd
ğ›¿ğ‘¤ğ‘–Â· ğ›¿ğ‘¤= ğ›¼
ğœ…(ğ‘ƒğ‘–(ğ‘¤ğ‘–)ğ‘Ÿ)ğ‘Â· ğ›¿ğ‘¤,
where ğ‘Ÿ= 1
ğœ…âˆ’1
ğ‘= ğ‘âˆ’ğœ…
ğ‘ğœ…. So, an increase of ğ›¿ğ‘¤to ğ‘¤ğ‘–can be balanced by a decrease of

ğ‘ƒğ‘–(ğ‘¤ğ‘–)ğ‘Ÿ
ğ‘ƒğ‘—(ğ‘¤ğ‘—)ğ‘Ÿ
ğ‘
ğ›¿ğ‘¤to
some other weight ğ‘¤ğ‘—. The two cases correspond to ğ‘Ÿâ‰¤0 and ğ‘Ÿ> 0 respectively.
Case 1: ğ‘Ÿâ‰¤0. Consider ğ‘–, ğ‘—with ğ‘ƒğ‘—> ğ‘ƒğ‘–. The optimal weights must satisfy ğ‘¤âˆ—
ğ‘–â‰¥ğ‘¤âˆ—
ğ‘—(else you could
swap ğ‘¤âˆ—
ğ‘–and ğ‘¤âˆ—
ğ‘—to decrease loss). But then ğ‘¤âˆ—
ğ‘—must be zero: if not, we could increase ğ‘¤âˆ—
ğ‘–by ğ›¿ğ‘¤and
decrease ğ‘¤âˆ—
ğ‘—by ğ›¿ğ‘¤, which keeps Lx-ent constant and decreases Lwd (since ğ‘ƒğ‘–(ğ‘¤âˆ—
ğ‘–)ğ‘Ÿ< ğ‘ƒğ‘—(ğ‘¤âˆ—
ğ‘—)ğ‘Ÿ).
Case 2: ğ‘Ÿ> 0. Consider ğ‘–, ğ‘—with ğ‘ƒğ‘—> ğ‘ƒğ‘–. As before we must have ğ‘¤âˆ—
ğ‘–â‰¥ğ‘¤âˆ—
ğ‘—. But now ğ‘¤âˆ—
ğ‘—must not
be zero: otherwise we could increase ğ‘¤âˆ—
ğ‘—by ğ›¿ğ‘¤and decrease ğ‘¤âˆ—
ğ‘–by ğ›¿ğ‘¤to keep Lx-ent constant and
decrease Lwd, since ğ‘ƒğ‘—(ğ‘¤âˆ—
ğ‘—)ğ‘Ÿ= 0 < ğ‘ƒğ‘–(ğ‘¤âˆ—
ğ‘–)ğ‘Ÿ. The balance occurs when ğ‘ƒğ‘—(ğ‘¤âˆ—
ğ‘—)ğ‘Ÿ= ğ‘ƒğ‘–(ğ‘¤âˆ—
ğ‘–)ğ‘Ÿ, which means
ğ‘¤âˆ—
ğ‘–âˆğ‘ƒâˆ’1/ğ‘Ÿ
ğ‘–
.
Proof. First, notice that our conclusions trivially hold for Â®ğ‘¤âˆ—= Â®0 (which can be a minimum if e.g.
the circuits are worse than random). Thus for the rest of the proof we will assume that at least one
weight is non-zero.
In addition, L â†’âˆwhenever any ğ‘¤ğ‘–â†’âˆ(because Ltrain â‰¥ğµand Lwd â†’âˆas any one ğ‘¤ğ‘–â†’âˆ).
Thus, any global minimum must have finite Â®ğ‘¤.
Notice that, since the circuit logits are independent of ğ‘–, we have â„= (Ã
ğ‘–ğ‘¤ğ‘–) ğ‘“, and so Ltrain( Â®ğ‘¤) is
purely a function of the sum of weights Ãğ¼
ğ‘–=1 ğ‘¤ğ‘–, and the overall loss can be written as:
L( Â®ğ‘¤) = Ltrain
 
ğ¼âˆ‘ï¸
ğ‘–=1
ğ‘¤ğ‘–
!
+ ğ›¼
ğ‘
ğ¼âˆ‘ï¸
ğ‘–=1
((ğ‘¤ğ‘–)
1
ğœ…ğ‘ƒğ‘–)ğ‘
We will now consider each case in order.
Case 1: ğœ…â‰¥ğ‘. Assume towards contradiction that there is a global minimum Â®ğ‘¤âˆ—where ğ‘¤âˆ—
ğ‘—> 0 for
some circuit ğ¶ğ‘—with non-minimal ğ‘ƒğ‘—. Let ğ¶ğ‘–be a circuit with minimal ğ‘ƒğ‘–(so that ğ‘ƒğ‘–< ğ‘ƒğ‘—), and let its
weight be ğ‘¤âˆ—
ğ‘–.
Consider an alternate weight assignment Â®ğ‘¤â€² that is identical to Â®ğ‘¤âˆ—except that ğ‘¤â€²
ğ‘—= 0 and
ğ‘¤â€²
ğ‘–= ğ‘¤âˆ—
ğ‘–+ ğ‘¤âˆ—
ğ‘—. Clearly Ã
ğ‘–ğ‘¤âˆ—
ğ‘–= Ã
ğ‘–ğ‘¤â€²
ğ‘–, and so Ltrain( Â®ğ‘¤âˆ—) = Ltrain( Â®ğ‘¤â€²). Thus, we have:
L( Â®ğ‘¤âˆ—) âˆ’L( Â®ğ‘¤â€²)
=
 
ğ›¼
ğ‘
ğ¼âˆ‘ï¸
ğ‘š=1
((ğ‘¤âˆ—
ğ‘š)
1
ğœ…ğ‘ƒğ‘š)ğ‘
!
âˆ’
 
ğ›¼
ğ‘
ğ¼âˆ‘ï¸
ğ‘š=1
((ğ‘¤â€²
ğ‘š)
1
ğœ…ğ‘ƒğ‘š)ğ‘
!
= ğ›¼
ğ‘

(ğ‘¤âˆ—
ğ‘–)
ğ‘
ğœ…ğ‘ƒğ‘
ğ‘–+ (ğ‘¤âˆ—
ğ‘—)
ğ‘
ğœ…ğ‘ƒğ‘
ğ‘—âˆ’(ğ‘¤â€²
ğ‘–)
ğ‘
ğœ…ğ‘ƒğ‘
ğ‘–

> ğ›¼
ğ‘ğ‘ƒğ‘
ğ‘–

(ğ‘¤âˆ—
ğ‘–)
ğ‘
ğœ…+ (ğ‘¤âˆ—
ğ‘—)
ğ‘
ğœ…âˆ’(ğ‘¤â€²
ğ‘–)
ğ‘
ğœ…

since ğ‘ƒğ‘—> ğ‘ƒğ‘–
= ğ›¼
ğ‘ğ‘ƒğ‘
ğ‘–

(ğ‘¤âˆ—
ğ‘–)
ğ‘
ğœ…+ (ğ‘¤âˆ—
ğ‘—)
ğ‘
ğœ…âˆ’(ğ‘¤âˆ—
ğ‘–+ ğ‘¤âˆ—
ğ‘—)
ğ‘
ğœ…

definition of ğ‘¤â€²
ğ‘–
â‰¥ğ›¼
ğ‘ğ‘ƒğ‘
ğ‘–

(ğ‘¤âˆ—
ğ‘–)
ğ‘
ğœ…+ (ğ‘¤âˆ—
ğ‘—)
ğ‘
ğœ…âˆ’

(ğ‘¤âˆ—
ğ‘–)
ğ‘
ğœ…+ (ğ‘¤âˆ—
ğ‘—)
ğ‘
ğœ…

using Lemma D.2 since 0 < ğ‘
ğœ…â‰¤1
= 0
26

Explaining grokking through circuit efficiency
Thus we have L( Â®ğ‘¤âˆ—) > L( Â®ğ‘¤â€²), contradicting our assumption that Â®ğ‘¤âˆ—is a global minimum of L.
This completes the proof for the case that ğœ…â‰¥ğ‘.
Case 2: ğœ…< ğ‘. First, we will show that all weights are non-zero at a global minimum (excluding the
case where Â®ğ‘¤âˆ—= Â®0, discussed at the beginning of the proof). Assume towards contradiction that there
is a global minimum Â®ğ‘¤âˆ—with ğ‘¤âˆ—
ğ‘—= 0 for some ğ‘—. Choose some arbitrary circuit ğ¶ğ‘–with nonzero weight
ğ‘¤âˆ—
ğ‘–.
Choose some ğœ–1 > 0 satisfying ğœ–1 <
ğ‘
2ğœ…(ğ‘¤âˆ—
ğ‘–)
ğ‘
ğœ…âˆ’1. By applying Lemma D.3 with ğ‘¥= ğ‘¤âˆ—
ğ‘–, ğ‘= ğœ–1, ğ‘Ÿ= ğ‘
ğœ…,
we can get some ğ›¿> 0 such that for any ğœ–< ğ›¿we have (ğ‘¤âˆ—
ğ‘–)
ğ‘
ğœ…âˆ’(ğ‘¤âˆ—
ğ‘–âˆ’ğœ–)
ğ‘
ğœ…> ğ›¿( ğ‘
ğœ…(ğ‘¤âˆ—
ğ‘–)
ğ‘
ğœ…âˆ’1 âˆ’ğœ–1).
Choose some ğœ–2 > 0 satisfying ğœ–2 < min(ğ‘¤âˆ—
ğ‘–, ğ›¿,

ğ‘
2ğœ…(ğ‘¤âˆ—
ğ‘–)
ğ‘
ğœ…âˆ’1 ğ‘ƒğ‘
ğ‘–
ğ‘ƒğ‘
ğ‘—

1
ğ‘
ğœ…âˆ’1 ). Consider an alternate weight
assignment defined Â®ğ‘¤â€² that is identical to Â®ğ‘¤âˆ—except that ğ‘¤â€²
ğ‘—= ğœ–2 and ğ‘¤â€²
ğ‘–= ğ‘¤âˆ—
ğ‘–âˆ’ğœ–2. As in the previous
case, Ltrain( Â®ğ‘¤âˆ—) = Ltrain( Â®ğ‘¤â€²). Thus, we have:
L( Â®ğ‘¤âˆ—) âˆ’L( Â®ğ‘¤â€²)
= ğ›¼
ğ‘

(ğ‘¤âˆ—
ğ‘–)
ğ‘
ğœ…ğ‘ƒğ‘
ğ‘–âˆ’(ğ‘¤âˆ—
ğ‘–âˆ’ğœ–2)
ğ‘
ğœ…ğ‘ƒğ‘
ğ‘–âˆ’ğœ–
ğ‘
ğœ…
2 ğ‘ƒğ‘
ğ‘—

= ğ›¼
ğ‘

ğ‘ƒğ‘
ğ‘–((ğ‘¤âˆ—
ğ‘–)
ğ‘
ğœ…âˆ’(ğ‘¤âˆ—
ğ‘–âˆ’ğœ–2)
ğ‘
ğœ…) âˆ’ğœ–
ğ‘
ğœ…
2 ğ‘ƒğ‘
ğ‘—

> ğ›¼
ğ‘

ğ‘ƒğ‘
ğ‘–ğ›¿( ğ‘
ğœ…(ğ‘¤âˆ—
ğ‘–)
ğ‘
ğœ…âˆ’1 âˆ’ğœ–1) âˆ’ğœ–
ğ‘
ğœ…
2 ğ‘ƒğ‘
ğ‘—

application of Lemma D.3 discussed above
> ğ›¼
ğ‘

ğ‘ƒğ‘
ğ‘–ğ›¿( ğ‘
ğœ…(ğ‘¤âˆ—
ğ‘–)
ğ‘
ğœ…âˆ’1 âˆ’ğ‘
2ğœ…(ğ‘¤âˆ—
ğ‘–)
ğ‘
ğœ…âˆ’1) âˆ’ğœ–
ğ‘
ğœ…
2 ğ‘ƒğ‘
ğ‘—

we chose ğœ–1 < ğ‘
2ğœ…(ğ‘¤âˆ—
ğ‘–)
ğ‘
ğœ…âˆ’1
> ğ›¼
ğ‘

ğ‘ƒğ‘
ğ‘–ğœ–2
ğ‘
2ğœ…(ğ‘¤âˆ—
ğ‘–)
ğ‘
ğœ…âˆ’1 âˆ’ğœ–
ğ‘
ğœ…
2 ğ‘ƒğ‘
ğ‘—

we chose ğœ–2 < ğ›¿
= ğ›¼ğœ–2
ğ‘
 ğ‘
2ğœ…(ğ‘¤âˆ—
ğ‘–)
ğ‘
ğœ…âˆ’1ğ‘ƒğ‘
ğ‘–âˆ’ğœ–
ğ‘
ğœ…âˆ’1
2
ğ‘ƒğ‘
ğ‘—

> ğ›¼ğœ–2
ğ‘
 
ğ‘
2ğœ…(ğ‘¤âˆ—
ğ‘–)
ğ‘
ğœ…âˆ’1ğ‘ƒğ‘
ğ‘–âˆ’ğ‘
2ğœ…(ğ‘¤âˆ—
ğ‘–)
ğ‘
ğœ…âˆ’1 ğ‘ƒğ‘
ğ‘–
ğ‘ƒğ‘
ğ‘—
ğ‘ƒğ‘
ğ‘—
!
we chose ğœ–2 <
"
ğ‘
2ğœ…(ğ‘¤âˆ—
ğ‘–)
ğ‘
ğœ…âˆ’1 ğ‘ƒğ‘
ğ‘–
ğ‘ƒğ‘
ğ‘—
#
1
ğ‘
ğœ…âˆ’1
)
= 0
Note that in the last step, we rely on the fact that ğœ…< ğ‘: this lets us use an upper bound on ğœ–2 to
get an upper bound on ğœ–
ğ‘
ğœ…âˆ’1
2
, and so a lower bound on the overall expression.
Thus we have L( Â®ğ‘¤âˆ—) > L( Â®ğ‘¤â€²), contradicting our assumption that Â®ğ‘¤âˆ—is a global minimum of L.
So, for all ğ‘–we have ğ‘¤ğ‘–> 0.
In addition, as ğ‘¤ğ‘–â†’âˆwe have L( Â®ğ‘¤) â†’âˆ, so Â®ğ‘¤âˆ—cannot be at the boundaries, and instead lies in
the interior. Since ğ‘> ğœ…, L( Â®ğ‘¤) is differentiable everywhere. Thus, we can conclude that its gradient
27

Explaining grokking through circuit efficiency
at Â®ğ‘¤âˆ—is zero:
ğ›¿L
ğ›¿ğ‘¤ğ‘–
= 0
ğ›¿Ltrain
ğ›¿ğ‘¤ğ‘–
+
ğ›¼ğ‘ƒğ‘
ğ‘–
ğœ…(ğ‘¤âˆ—
ğ‘–)
ğ‘
ğœ…âˆ’1 = 0
ğ‘ƒğ‘
ğ‘–(ğ‘¤âˆ—
ğ‘–)
ğ‘âˆ’ğœ…
ğœ…= âˆ’ğœ…
ğ›¼
ğ›¿Ltrain
ğ›¿ğ‘¤ğ‘–
ğ‘¤âˆ—
ğ‘–ğ‘ƒ
ğ‘ğœ…
ğ‘âˆ’ğœ…
ğ‘–
=

âˆ’ğœ…
ğ›¼
ğ›¿Ltrain
ğ›¿ğ‘¤ğ‘–

ğœ…
ğ‘âˆ’ğœ…
Since Ltrain( Â®ğ‘¤) is a function of
ğ¼Ã
ğ‘—=1
ğ‘¤ğ‘—, we can conclude that ğ›¿Ltrain
ğ›¿ğ‘¤ğ‘–
= ğ›¿Ltrain
ğ›¿Ã
ğ‘—ğ‘¤ğ‘—Â·
ğ›¿Ã
ğ‘—ğ‘¤ğ‘—
ğ›¿ğ‘¤ğ‘–
= ğ›¿Ltrain
ğ›¿Ã
ğ‘—ğ‘¤ğ‘—, which
is independent of ğ‘–. So the right hand side of the equation is independent of ğ‘–, allowing us to conclude
that ğ‘¤âˆ—
ğ‘–âˆğ‘ƒ
âˆ’ğ‘ğœ…
ğ‘âˆ’ğœ…
ğ‘–
.
â–¡
28

