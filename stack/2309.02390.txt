5 September 2023
Explaining grokking through circuit efficiency
Vikrant Varma*, 1, Rohin Shah*, 1, Zachary Kenton1, János Kramár1 and Ramana Kumar1
*Equal contributions, 1Google DeepMind
One of the most surprising puzzles in neural network generalisation is grokking: a network with perfect
training accuracy but poor generalisation will, upon further training, transition to perfect generalisation.
We propose that grokking occurs when the task admits a generalising solution and a memorising solution,
where the generalising solution is slower to learn but more efficient, producing larger logits with the
same parameter norm. We hypothesise that memorising circuits become more inefficient with larger
training datasets while generalising circuits do not, suggesting there is a critical dataset size at which
memorisation and generalisation are equally efficient. We make and confirm four novel predictions about
grokking, providing significant evidence in favour of our explanation. Most strikingly, we demonstrate
two novel and surprising behaviours: ungrokking, in which a network regresses from perfect to low test
accuracy, and semi-grokking, in which a network shows delayed generalisation to partial rather than
perfect test accuracy.
1. Introduction
When training a neural network, we expect that once training loss converges to a low value, the
network will no longer change much. Power et al. (2021) discovered a phenomenon dubbed grokking
that drastically violates this expectation. The network first “memorises” the data, achieving low
and stable training loss with poor generalisation, but with further training transitions to perfect
generalisation. We are left with the question: why does the network’s test performance improve
dramatically upon continued training, having already achieved nearly perfect training performance?
Recent answers to this question vary widely, including the difficulty of representation learning (Liu
et al., 2022), the scale of parameters at initialisation (Liu et al., 2023), spikes in loss ("slingshots") (Thi-
lak et al., 2022), random walks among optimal solutions (Millidge, 2022), and the simplicity of
the generalising solution (Nanda et al., 2023, Appendix E). In this paper, we argue that the last
explanation is correct, by stating a specific theory in this genre, deriving novel predictions from the
theory, and confirming the predictions empirically.
We analyse the interplay between the internal mechanisms that the neural network uses to
calculate the outputs, which we loosely call “circuits” (Olah et al., 2020). We hypothesise that there
are two families of circuits that both achieve good training performance: one which generalises well
(𝐶gen ) and one which memorises the training dataset (𝐶mem ). The key insight is that when there
are multiple circuits that achieve strong training performance, weight decay prefers circuits with high
“efficiency”, that is, circuits that require less parameter norm to produce a given logit value.
Efficiency answers our question above: if 𝐶gen is more efficient than 𝐶mem , gradient descent can
reduce nearly perfect training loss even further by strengthening 𝐶gen while weakening 𝐶mem , which
then leads to a transition in test performance. With this understanding, we demonstrate in Section 3
that three key properties are sufficient for grokking: (1) 𝐶gen generalises well while 𝐶mem does not,
(2) 𝐶gen is more efficient than 𝐶mem , and (3) 𝐶gen is learned more slowly than 𝐶mem .
Since 𝐶gen generalises well, it automatically works for any new data points that are added to
the training dataset, and so its efficiency should be independent of the size of the training dataset.
In contrast, 𝐶mem must memorise any additional data points added to the training dataset, and so
Corresponding author(s): vikrantvarma@deepmind.com, rohinmshah@deepmind.com
arXiv:2309.02390v1  [cs.LG]  5 Sep 2023

Explaining grokking through circuit efficiency
Train loss
Test loss
Test accuracy
0
5
10
Epochs
1e3
101
10
2
10
5
Loss
0
1
(a) Grokking.
The original be-
haviour from Power et al. (2021).
We train on a dataset with 𝐷≫
𝐷crit. As a result, 𝐶gen is strongly
preferred to 𝐶mem at convergence,
and we observe a transition to very
low test loss (100% test accuracy).
101
103
105
Epochs
101
10
2
10
5
0
1
(b) Ungrokking. If we take a net-
work that has already grokked and
train it on a new dataset with 𝐷<
𝐷crit, the network reverts to signif-
icant memorisation, leading to a
transition back to poor test loss.
(Note the log scale for the x-axis.)
0
2
Epochs
1e7
101
10
2
10
5
0
1
Accuracy
(c) Semi-grokking.
When 𝐷∼
𝐷crit, the memorising algorithm
and generalising algorithm com-
pete with each other at conver-
gence, so we observe a transition
to improved but not perfect test
loss (test accuracy of 83%).
Figure 1 | Novel grokking phenomena. When grokking occurs, we expect there are two algorithms
that perform well at training: “memorisation” (𝐶mem , with poor test performance) and “generalisation”
(𝐶gen , with strong test performance). Weight decay strengthens 𝐶gen over 𝐶mem as the training dataset
size increases. By analysing the point at which 𝐶gen and 𝐶mem are equally strong (the critical dataset
size 𝐷crit), we predict and confirm two novel behaviours: ungrokking and semi-grokking.
its efficiency should decrease as training dataset size increases. We validate these predictions by
quantifying efficiencies for various dataset sizes for both 𝐶mem and 𝐶gen .
This suggests that there exists a crossover point at which 𝐶gen becomes more efficient than 𝐶mem ,
which we call the critical dataset size 𝐷crit. By analysing dynamics at 𝐷crit, we predict and demonstrate
two new behaviours (Figure 1). In ungrokking, a model that has successfully grokked returns to poor
test accuracy when further trained on a dataset much smaller than 𝐷crit. In semi-grokking, we choose
a dataset size where 𝐶gen and 𝐶mem are similarly efficient, leading to a phase transition but only to
middling test accuracy.
We make the following contributions:
1. We demonstrate the sufficiency of three ingredients for grokking through a constructed simula-
tion (Section 3).
2. By analysing dynamics at the “critical dataset size” implied by our theory, we predict two novel
behaviours: semi-grokking and ungrokking (Section 4).
3. We confirm our predictions through careful experiments, including demonstrating semi-grokking
and ungrokking in practice (Section 5).
2. Notation
We consider classification using deep neural networks under the cross-entropy loss. In particular, we
are given a set of inputs 𝑋, a set of labels 𝑌, and a training dataset, D = {(𝑥1, 𝑦∗
1), . . . (𝑥𝐷, 𝑦∗
𝐷)}.
For an arbitrary classifier ℎ: 𝑋× 𝑌→ℝ, the softmax cross entropy loss is given by:
Lx-ent(ℎ) = −1
𝐷
∑︁
(𝑥,𝑦∗)∈D
log
exp(ℎ(𝑥, 𝑦∗))
Í
𝑦′∈𝑌
exp(ℎ(𝑥, 𝑦′)) .
(1)
The output of a classifier for a specific class is the class logit, denoted by 𝑜𝑦
ℎ(𝑥) B ℎ(𝑥, 𝑦). When the
2

Explaining grokking through circuit efficiency
input 𝑥is clear from context, we will denote the logit as 𝑜𝑦
ℎ. We denote the vector of the logits for all
classes for a given input as ®𝑜ℎ(𝑥) or ®𝑜ℎwhen 𝑥is clear from context.
Parametric classifiers (such as neural networks) are parameterised with a vector 𝜃that induces
a classifier ℎ𝜃. The parameter norm of the classifier is 𝑃ℎ𝜃B ∥𝜃∥. It is common to add weight decay
regularisation, which is an additional loss term Lwd(ℎ𝜃) = 1
2 (𝑃ℎ𝜃)2. The overall loss is given by
L(ℎ𝜃) = Lx-ent(ℎ) + 𝛼Lwd(ℎ𝜃),
(2)
where 𝛼is a constant that trades off between softmax cross entropy and weight decay.
Circuits. Inspired by Olah et al. (2020), we use the term circuit to refer to an internal mechanism by
which a neural network works. We only consider circuits that map inputs to logits, so that a circuit 𝐶
induces a classifier ℎ𝐶for the overall task. We elide this distinction and simply write 𝐶to refer to ℎ𝐶,
so that the logits are 𝑜𝑦
𝐶, the loss is L(𝐶), and the parameter norm is 𝑃𝐶.
For any given algorithm, there exist multiple circuits that implement that algorithm. Abusing
notation, we use 𝐶gen (𝐶mem ) to refer either to the family of circuits that implements the generalising
(memorising) algorithm, or a single circuit from the appropriate family.
3. Three ingredients for grokking
Given a circuit with perfect training accuracy (as with a pure memorisation approach like 𝐶mem or a
perfectly generalising solution like 𝐶gen ), the cross entropy loss Lx-ent incentivises gradient descent
to scale up the classifier’s logits, as that makes its answers more confident, leading to lower loss (see
Theorem D.1). For typical neural networks, this would be achieved by making the parameters larger.
Meanwhile, weight decay Lwd pushes in the opposite direction, directly decreasing the parameters.
These two forces must be balanced at any local minimum of the overall loss.
When we have multiple circuits that achieve strong training accuracy, this constraint applies to
each individually. But how will they relate to each other? Intuitively, the answer depends on the
efficiency of each circuit, that is, the extent to which the circuit can convert relatively small parameters
into relatively large logits. For more efficient circuits, the Lx-ent force towards larger parameters is
stronger, and the Lwd force towards smaller parameters is weaker. So, we expect that more efficient
circuits will be stronger at any local minimum.
Given this notion of efficiency, we can explain grokking as follows. In the first phase, 𝐶mem is
learned quickly, leading to strong train performance and poor test performance. In the second phase,
𝐶gen is now learned, and parameter norm is “reallocated” from 𝐶mem to 𝐶gen , eventually leading to a
mixture of strong 𝐶gen and weak 𝐶mem , causing an increase in test performance.
This overall explanation relies on the presence of three ingredients:
1. Generalising circuit: There are two families of circuits that achieve good training performance:
a memorising family 𝐶mem with poor test performance, and a generalising family 𝐶gen with good
test performance.
2. Efficiency: 𝐶gen is more “efficient” than 𝐶mem , that is, it can produce equivalent cross-entropy
loss on the training set with a lower parameter norm.
3. Slow vs fast learning: 𝐶gen is learned more slowly than 𝐶mem , such that during early phases of
training 𝐶mem is stronger than 𝐶gen .
To illustrate the sufficiency of these ingredients, we construct a minimal example containing all
three ingredients, and demonstrate that it leads to grokking. We emphasise that this example is to be
3

Explaining grokking through circuit efficiency
treated as a validation of the three ingredients, rather than as a quantitative prediction of the dynamics
of existing examples of grokking. Many of the assumptions and design choices were made on the
basis of simplicity and analytical tractability, rather than a desire to reflect examples of grokking
in practice. The clearest difference is that 𝐶gen and 𝐶mem are modelled as hardcoded input-output
lookup tables whose outputs can be strengthened through learned scalar weights, whereas in existing
examples of grokking 𝐶gen and 𝐶mem are learned internal mechanisms in a neural network that can be
strengthened by scaling up the parameters implementing those mechanisms.
Generalisation. To model generalisation, we introduce a training dataset D and a test dataset Dtest.
𝐶gen is a lookup table that produces logits that achieve perfect train and test accuracy. 𝐶mem is a lookup
table that achieves perfect train accuracy, but makes confident incorrect predictions on the test dataset.
We denote by Dmem the predictions made by 𝐶mem on the test inputs, with the property that there is
no overlap between Dtest and Dmem. Then we have:
𝑜𝑦
𝐺(𝑥) = 𝟙[(𝑥, 𝑦) ∈D or (𝑥, 𝑦) ∈Dtest]
𝑜𝑦
𝑀(𝑥) = 𝟙[(𝑥, 𝑦) ∈D or (𝑥, 𝑦) ∈Dmem]
Slow vs fast learning. To model learning, we introduce weights for each of the circuits, and use
gradient descent to update the weights. Thus, the overall logits are given by:
𝑜𝑦(𝑥) = 𝑤𝐺𝑜𝑦
𝐺(𝑥) + 𝑤𝑀𝑜𝑦
𝑀(𝑥)
Unfortunately, if we learn 𝑤𝐺and 𝑤𝑀directly with gradient descent, we have no control over the
speed at which the weights are learned. Inspired by Jermyn and Shlegeris (2022), we instead compute
weights as multiples of two “subweights”, and then learn the subweights with gradient descent.
More precisely, we let 𝑤𝐺= 𝑤𝐺1𝑤𝐺2 and 𝑤𝑀= 𝑤𝑀1𝑤𝑀2, and update each subweight according to
𝑤𝑖←𝑤𝑖−𝜆· 𝜕L/𝜕𝑤𝑖. The speed at which the weights are strengthened by gradient descent can then
be controlled by the initial values of the weights. Intuitively, the gradient towards the first subweight
𝜕L/𝜕𝑤1 depends on the strength of the second subweight 𝑤2 and vice-versa, and so low initial values
lead to slow learning. At initialisation, we set 𝑤𝐺1 = 𝑤𝑀1 = 0 to ensure the logits are initially zero,
and then set 𝑤𝐺2 << 𝑤𝑀2 to ensure 𝐶gen is learned more slowly than 𝐶mem .
Efficiency. Above, we operationalised circuit efficiency as the extent to which the circuit can convert
relatively small parameters into relatively large logits. When the weights are all one, each circuit
produces a one-hot vector as its logits, so their logit scales are the same, and efficiency is determined
solely by parameter norm. We define 𝑃𝐺and 𝑃𝑀to be the parameter norms when weights are all one.
Since we want 𝐶gen to be more efficient than 𝐶mem , we set 𝑃𝐺< 𝑃𝑀.
This still leaves the question of how to model parameter norm when the weights are not all one.
Intuitively, increasing the weights corresponds to increasing the parameters in a neural network to
scale up the resulting outputs. In a 𝜅-layer MLP with Relu activations and without biases, scaling
all parameters by a constant 𝑐scales the outputs by 𝑐𝜅. Inspired by this observation, we model the
parameter norm of 𝑤𝐺𝐶𝐺as 𝑤1/𝜅
𝐺
𝑃𝑖for some 𝜅> 0, and similarly for 𝑤𝑀𝐶𝑀.
Theoretical analysis. We first analyse the optimal solutions to the setup above. We can ignore the
subweights, as they only affect the speed of learning: Lx-ent and Lwd depend only on the weights, not
subweights. Intuitively, to get minimal loss, we must assign higher weights to more efficient circuits –
but it is unclear whether we should assign no weight to less efficient circuits, or merely smaller but
still non-zero weights. Theorem D.4 shows that in our example, both of these cases can arise: which
one we get depends on the value of 𝜅.
4

Explaining grokking through circuit efficiency
Train loss
Test loss
Gen logit
Mem logit
Parameter norm
0
5
10
Steps
1e3
101
100
10
1
10
2
Loss
0
5
10
(a)
All
three
ingredients.
When 𝐶gen is more efficient than
𝐶mem but
learned
slower,
we
observe grokking. 𝐶gen only starts
to grow significantly by step 2500,
and then substitutes for 𝐶mem .
Total parameter norm falls due to
𝐶gen ’s higher efficiency.
0
5
10
Steps
1e3
101
100
10
1
10
2
Loss
0
5
10
(b) 𝐶gen less efficient than 𝐶mem .
We set 𝑃𝐺> 𝑃𝑀. Since 𝐶gen is now
less efficient and learned slower,
it never grows, and test loss stays
high due to 𝐶mem throughout train-
ing.
0
5
10
Steps
1e3
101
100
10
1
10
2
Loss
0
5
10
(c)
𝐶gen and
𝐶mem learned
at
equal speeds. We set 𝑤𝐺1 = 𝑤𝑀1
so they are learned equally quickly.
𝐶gen is prioritised at least as much
as 𝐶mem throughout training, due
to its higher efficiency.
Thus,
test loss is very similar to train
loss throughout training, and no
grokking is observed.
Figure 2 | 𝐶gen must be learned slowly for grokking to arise. We learn weights 𝑤𝑀and 𝑤𝐺through
gradient descent on the loss in Equation D.1. To model the fact that 𝐶gen is more efficient than 𝐶mem ,
we set 𝑃𝑀> 𝑃𝐺. We see that we only get grokking when 𝐶gen is learned more slowly than 𝐶mem .
Experimental analysis. We run our example for various hyperparameters, and plot training and
test loss in Figure 2. We see that when all three ingredients are present (Figure 2a), we observe
the standard grokking curves, with a delayed decrease in test loss. By contrast, when we make the
generalising circuit less efficient (Figure 2b), the test loss never falls, and when we remove the slow
vs fast learning ingredient (Figure 2c), we see that test loss decreases immediately. See Appendix C
for details.
4. Why generalising circuits are more efficient
Section 3 demonstrated that grokking can arise when 𝐶gen is more efficient than 𝐶mem , but left open
the question of why 𝐶gen is more efficient. In this section, we develop a theory based on training
dataset size 𝐷, and use it to predict two new behaviours: ungrokking and semi-grokking.
4.1. Relationship of efficiency with dataset size
Consider a classifier ℎD obtained by training on a dataset D of size 𝐷with weight decay, and a
classifier ℎD′ obtained by training on the same dataset with one additional point: D′ = D ∪{(𝑥, 𝑦∗)}.
Intuitively, ℎD′ cannot be more efficient than ℎD: if it was, then ℎD′ would outperform ℎD even on
just D, since it would get similar Lx-ent while doing better by weight decay. So we should expect that,
on average, classifier efficiency is monotonically non-increasing in dataset size.
How does generalisation affect this picture? Let us suppose that ℎD successfully generalises to
predict 𝑦∗for the new input 𝑥. Then, as we move from D to D′, Lx-ent(ℎD) likely does not worsen
with this new data point. Thus, we could expect to see the same classifier arise, with the same average
logit value, parameter norm, and efficiency.
Now suppose ℎD instead fails to predict the new data point (𝑥, 𝑦∗). Then the classifier learned for
5

Explaining grokking through circuit efficiency
D′ will likely be less efficient: Lx-ent(ℎD) would be much higher due to this new data point, and so
the new classifier must incur some additional regularisation loss to reduce Lx-ent on the new point.
Applying this analysis to our circuits, we should expect 𝐶gen ’s efficiency to remain unchanged
as 𝐷increases arbitrarily high, since 𝐶gen does need not to change to accommodate new training
examples. In contrast, 𝐶mem must change with nearly every new data point, and so we should expect
its efficiency to decrease as 𝐷increases. Thus, when 𝐷is sufficiently large, we expect 𝐶gen to be more
efficient than 𝐶mem . (Note however that when the set of possible inputs is small, even the maximal 𝐷
may not be “sufficiently large”.)
Critical threshold for dataset size. Intuitively, we expect that for extremely small datasets (say,
𝐷< 5), it is extremely easy to memorise the training dataset. So, we hypothesise that for these
very small datasets, 𝐶mem is more efficient than 𝐶gen . However, as argued above, 𝐶mem will get less
efficient as 𝐷increases, and so there will be a critical dataset size 𝐷crit at which 𝐶mem and 𝐶gen are
approximately equally efficient. When 𝐷≫𝐷crit, 𝐶gen is more efficient and we expect grokking, and
when 𝐷≪𝐷crit, 𝐶mem is more efficient and so grokking should not happen.
Effect of weight decay on 𝐷crit. Since 𝐷crit is determined only by the relative efficiencies of 𝐶gen and
𝐶mem , and none of these depends on the exact value of weight decay (just on weight decay being
present at all), our theory predicts that 𝐷crit should not change as a function of weight decay. Of
course, the strength of weight decay may still affect other properties such as the number of epochs till
grokking.
4.2. Implications of crossover: ungrokking and semi-grokking.
By thinking through the behaviour around the critical threshold for dataset size, we predict the
existence of two phenomena that, to the best of our knowledge, have not previously been reported.
Ungrokking. Suppose we take a network that has been trained on a dataset with 𝐷> 𝐷crit and has
already exhibited grokking, and continue to train it on a smaller dataset with size 𝐷′ < 𝐷crit. In this
new training setting, 𝐶mem is now more efficient than 𝐶gen , and so we predict that with enough further
training gradient descent will reallocate weight from 𝐶gen to 𝐶mem , leading to a transition from high
test performance to low test performance. Since this is exactly the opposite observation as in regular
grokking, we term this behaviour “ungrokking”.
Ungrokking can be seen as a special case of catastrophic forgetting (McCloskey and Cohen, 1989;
Ratcliff, 1990), where we can make much more precise predictions. First, since ungrokking should
only be expected once 𝐷′ < 𝐷crit, if we vary 𝐷′ we predict that there will be a sharp transition from
very strong to near-random test accuracy (around 𝐷crit). Second, we predict that ungrokking would
arise even if we only remove examples from the training dataset, whereas catastrophic forgetting
typically involves training on new examples as well. Third, since 𝐷crit does not depend on weight
decay, we predict the amount of “forgetting” (i.e. the test accuracy at convergence) also does not
depend on weight decay.
Semi-grokking. Suppose we train a network on a dataset with 𝐷≈𝐷crit. 𝐶gen and 𝐶mem would be
similarly efficient, and there are two possible cases for what we expect to observe (illustrated in
Theorem D.4).
In the first case, gradient descent would select either 𝐶mem or 𝐶gen , and then make it the maximal
circuit. This could happen in a consistent manner (for example, perhaps since 𝐶mem is learned faster
it always becomes the maximal circuit), or in a manner dependent on the random initialisation. In
either case we would simply observe the presence or absence of grokking.
6

Explaining grokking through circuit efficiency
In the second case, gradient descent would produce a mixture of both 𝐶mem and 𝐶gen . Since
neither 𝐶mem nor 𝐶gen would dominate the prediction on the test set, we would expect middling test
performance.
𝐶mem would still be learned faster, and so this would look similar to grokking: an initial phase with
good train performance and bad test performance, followed by a transition to significantly improved
test performance. Since we only get to middling generalisation unlike in typical grokking, we call this
behaviour semi-grokking.
Our theory does not say which of the two cases will arise in practice, but in Section 5.3 we find
that semi-grokking does happen in our setting.
5. Experimental evidence
Our explanation of grokking has some support from from prior work:
1. Generalising circuit: Nanda et al. (2023, Figure 1) identify and characterise the generalising
circuit learned at the end of grokking in the case of modular addition.
2. Slow vs fast learning: Nanda et al. (2023, Figure 7) demonstrate “progress measures” showing
that the generalising circuit develops and strengthens long after the network achieves perfect
training accuracy in modular addition.
To further validate our explanation, we empirically test our predictions from Section 4:
(P1) Efficiency: We confirm our prediction that 𝐶gen efficiency is independent of dataset size, while
𝐶mem efficiency decreases as training dataset size increases.
(P2) Ungrokking (phase transition): We confirm our prediction that ungrokking shows a phase
transition around 𝐷crit.
(P3) Ungrokking (weight decay): We confirm our prediction that the final test accuracy after
ungrokking is independent of the strength of weight decay.
(P4) Semi-grokking: We demonstrate that semi-grokking occurs in practice.
Training details. We train 1-layer Transformer models with the AdamW optimiser (Loshchilov and
Hutter, 2019) on cross-entropy loss (see Appendix A for more details). All results in this section are
on the modular addition task (𝑎+ 𝑏mod 𝑃for 𝑎, 𝑏∈(0, . . . , 𝑃−1) and 𝑃= 113) unless otherwise
stated; results on 9 additional tasks can be found in Appendix A.
5.1. Relationship of efficiency with dataset size
We first test our prediction about memorisation and generalisation efficiency:
(P1) Efficiency. We predict (Section 4.1) that memorisation efficiency decreases with increasing train
dataset size, while generalisation efficiency stays constant.
To test (P1), we look at training runs where only one circuit is present, and see how the logits 𝑜𝑦
𝑖
vary with the parameter norm 𝑃𝑖(by varying the weight decay) and the dataset size 𝐷.
Experiment setup. We produce 𝐶mem -only networks by using completely random labels for the
training data (Zhang et al., 2021), and assume that the entire parameter norm at convergence is
allocated to memorisation. We produce 𝐶gen -only networks by training on large dataset sizes and
checking that > 95% of the logit norm comes from just the trigonometric subspace (see Appendix B
for details).
7

Explaining grokking through circuit efficiency
25
30
40
60
90
Parameter norm
30
60
90
Correct logit
fixed logit values
0.5
1.0
2.0
4.0
8.0
Dataset size
1e3
(a) 𝐶mem scatter plot. At a fixed logit value (dot-
ted horizontal lines), parameter norm increases with
dataset size.
0.5
1.0
2.0
4.0
8.0
Dataset size
1e3
25
30
40
60
90
Parameter norm
50
60
70
Correct logit
(b) 𝐶mem isologit curves. Curves go up and right,
showing that parameter norm increases with dataset
size when holding logits fixed.
27
30
33
Parameter norm
30
60
90
120
Correct logit
fixed logit values
4
6
8
Dataset size
1e3
(c) 𝐶gen scatter plot. There is no obvious structure
to the colours, suggesting that the logit to parameter
norm relationship is independent of dataset size.
4
6
8
Dataset size
1e3
27
30
33
Parameter norm
50
70
90
Correct logit
(d) 𝐶gen isologit curves. The curves are flat, showing
that for fixed logit values the parameter norm does
not depend on dataset size.
Figure 3 | Efficiency of the 𝐶mem and 𝐶gen algorithms. We collect and visualise a dataset of triples
 𝑜𝑦, 𝑃𝑚, 𝐷 (correct logit, parameter norm, and dataset size), each corresponding to a training run
with varying random seed, weight decay, and dataset size, for both 𝐶mem and 𝐶gen . Besides a standard
scatter plot, we geometrically bucket logit values into six buckets, and plot “isologit curves” showing
the dependence of parameter norm on dataset size for each bucket. The results validate our theory
that (1) 𝐶mem requires larger parameter norm to produce the same logits as dataset size increases,
and (2) 𝐶gen uses the same parameter norm to produce fixed logits, irrespective of dataset size. In
addition, 𝐶mem has a much wider range of parameter norms than 𝐶gen , and at the extremes can be
more efficient than 𝐶gen .
Results. Figures 3a and 3b confirm our theoretical prediction for memorisation efficiency. Specifically,
to produce a fixed logit value, a higher parameter norm is required when dataset size is increased,
implying decreased efficiency. In addition, for a fixed dataset size, scaling up logits requires scaling up
parameter norm, as expected. Figures 3c and 3d confirm our theoretical prediction for generalisation
efficiency. To produce a fixed logit value, the same parameter norm is required irrespective of the
dataset size.
Note that the figures show significant variance across random seeds. We speculate that there are
many different circuits implementing the same overall algorithm, but they have different efficiencies,
and the random initialisation determines which one gradient descent finds. For example, in the case
of modular addition, the generalising algorithm depends on a set of “key frequencies” (Nanda et al.,
2023); different choices of key frequencies could lead to different efficiencies.
It may appear from Figure 3c that increasing parameter norm does not increase logit value,
contradicting our theory. However, this is a statistical artefact caused by the variance from the random
seed. We do see “stripes” of particular colours going up and right: these correspond to runs with the
same seed and dataset size, but different weight decay, and they show that when the noise from the
random seed is removed, increased parameter norm clearly leads to increased logits.
8

Explaining grokking through circuit efficiency
103
104
Reduced dataset size
0.0
0.5
1.0
Test accuracy
x + y mod P
0.1
0.5
1.0
1.5
2.0
Weight decay
Average accuracy
Per seed accuracy
Figure 4 | Ungrokking. We train on the full
dataset (achieving 100% test accuracy), and
then continue training on a smaller subset of
the full dataset. We plot test accuracy against
reduced dataset size for a range of weight de-
cays. We see a sharp transition from strong test
accuracy to near-zero test accuracy, that is in-
dependent of weight decay (different coloured
lines almost perfectly overlap). See Figure 8 for
more tasks.
0
1
2
3
Epochs
1e7
0.0
0.5
1.0
Test accuracy
0.25
0.50
0.75
1.00
Final accuracy
Figure 5 | Semi-grokking. We plot test accu-
racy against training epochs for a large sweep of
training runs with varying dataset sizes. Lines
are coloured by the final test accuracy at the
end of training. Out of 200 runs, at least 6 show
clear semi-grokking at the end of training. Many
other runs show transient semi-grokking, hover-
ing around middling test accuracy for millions
of epochs, or having multiple plateaus, before
fully generalising.
5.2. Ungrokking: overfitting after generalisation
We now turn to testing our predictions about ungrokking. Figure 1b demonstrates that ungrokking
happens in practice. In this section we focus on testing that it has the properties we expect.
(P2) Phase transition. We predict (Section 4.2) that if we plot test accuracy at convergence against
the size of the reduced training dataset 𝐷′, there will be a phase transition around 𝐷crit.
(P3) Weight decay. We predict (Section 4.2) that test accuracy at convergence is independent of the
strength of weight decay.
Experiment setup. We train a network to convergence on the full dataset to enable perfect gener-
alisation, then continue training the model on a small subset of the full dataset, and measure the
test accuracy at convergence. We vary both the size of the small subset, as well as the strength of the
weight decay.
Results. Figure 4 shows the results, and clearly confirms both (P2) and (P3). Appendix A has
additional results, and in particular Figure 8 replicates the results for many additional tasks.
5.3. Semi-grokking: evenly matched circuits
Unlike the previous predictions, semi-grokking is not strictly implied by our theory. However, as we
will see, it turns out that it does occur in practice.
(P4) Semi-grokking. When training at around 𝐷≈𝐷crit, where 𝐶mem and 𝐶gen have roughly equal
efficiencies, the final network at convergence should either be entirely composed of the most efficient
circuit, or of roughly equal proportions of 𝐶mem and 𝐶gen . If the latter, we should observe a transition
to middling test accuracy well after near-perfect train accuracy.
There are a number of difficulties in demonstrating an example of semi-grokking in practice.
First, the time to grok increases super-exponentially as the dataset size 𝐷decreases (Power et al.,
2021, Figure 1), and 𝐷crit is significantly smaller than the smallest dataset size at which grokking
9

Explaining grokking through circuit efficiency
has been demonstrated. Second, the random seed causes significant variance in the efficiency of
𝐶gen and 𝐶mem , which in turn affects 𝐷crit for that run. Third, accuracy changes sharply with the 𝐶gen to
𝐶mem ratio (Appendix A). To observe a transition to middling accuracy, we need to have balanced
𝐶gen and 𝐶mem outputs, but this is difficult to arrange due to the variance with random seed. To
address these challenges, we run many different training runs, on dataset sizes slightly above our
best estimate of the typical 𝐷crit, such that some of the runs will (through random noise) have an
unusually inefficient 𝐶gen or an unusually efficient 𝐶mem , such that the efficiencies match and there is
a chance to semi-grok.
Experiment setup. We train 10 seeds for each of 20 dataset sizes evenly spaced in the range
[1500, 2050] (somewhat above our estimate of 𝐷crit).
Results. Figure 1c shows an example of a single run that demonstrates semi-grokking, and Figure 5
shows test accuracies over time for every run. These validate our initial hypothesis that semi-grokking
may be possible, but also raise new questions.
In Figure 1c, we see two phenomena peculiar to semi-grokking: (1) test accuracy “spikes” several
times throughout training before finally converging, and (2) training loss fluctuates in a set range.
We leave investigation of these phenomena to future work.
In Figure 5, we observe that there is often transient semi-grokking, where a run hovers around
middling test accuracy for millions of epochs, or has multiple plateaus, before generalising perfectly.
We speculate that each transition corresponds to gradient descent strengthening a new generalising
circuit that is more efficient than any previously strengthened circuit, but took longer to learn. We
would guess that if we had trained for longer, many of the semi-grokking runs would exhibit full
grokking, and many of the runs that didn’t generalise at all would generalise at least partially to show
semi-grokking.
Given the difficulty of demonstrating semi-grokking, we only run this experiment on modular
addition. However, our experience with modular addition shows that if we only care about values
at convergence, we can find them much faster by ungrokking from a grokked network (instead of
semi-grokking from a randomly initialised network). Thus the ungrokking results on other tasks
(Figure 8) provide some support that we would see semi-grokking on those tasks as well.
6. Related work
Grokking. Since Power et al. (2021) discovered grokking, many works have attempted to understand
why it happens. Thilak et al. (2022) suggest that “slingshots” could be responsible for grokking,
particularly in the absence of weight decay, and Notsawo Jr et al. (2023) discuss a similar phenomenon
of “oscillations”. In contrast, our explanation applies even where there are no slingshots or oscillations
(as in most of our experiments). Liu et al. (2022) show that for a specific (non-modular) addition
task with an inexpressive architecture, perfect generalisation occurs when there is enough data
to determine the appropriate structured representation. However, such a theory does not explain
semi-grokking. We argue that for more typical tasks on which grokking occurs, the critical factor is
instead the relative efficiencies of 𝐶mem and 𝐶gen . Liu et al. (2023) show that grokking occurs even
in non-algorithmic tasks when parameters are initialised to be very large, because memorisation
happens quickly but it takes longer for regularisation to reduce parameter norm to the “Goldilocks
zone” where generalisation occurs. This observation is consistent with our theory: in non-algorithmic
tasks, we expect there exist efficient, generalising circuits, and the increased parameter norm creates
the final ingredient (slow learning), leading to grokking. However, we expect that in algorithmic tasks
such as the ones we study, the slow learning is caused by some factor other than large parameter
10

Explaining grokking through circuit efficiency
norm at initialisation.
Davies et al. (2023) is the most related work. The authors identify three ingredients for grokking
that mirror ours: a generalising circuit that is slowly learned but is favoured by inductive biases, a
perspective also mirrored in Nanda et al. (2023, Appendix E). We operationalise the “inductive bias”
ingredient as efficiency at producing large logits with small parameter norm, and to provide significant
empirical support by predicting and verifying the existence of the critical threshold 𝐷crit and the novel
behaviours of semi-grokking and ungrokking.
Nanda et al. (2023) identify the trigonometric algorithm by which the networks solve modular
addition after grokking, and show that it grows smoothly over training, and Chughtai et al. (2023)
extend this result to arbitrary group compositions. We use these results to define metrics for the
strength of the different circuits (Appendix B) which we used in preliminary investigations and for
some results in Appendix A (Figures 9 and 10). Merrill et al. (2023) show similar results on sparse
parity: in particular, they show that a sparse subnetwork is responsible for the well-generalising logits,
and that it grows as grokking happens.
Weight decay. While it is widely known that weight decay can improve generalisation (Krogh and
Hertz, 1991), the mechanisms for this effect are multifaceted and poorly understood (Zhang et al.,
2018). We propose a mechanism that is, to the best of our knowledge, novel: generalising circuits tend
to be more efficient than memorising circuits at large dataset sizes, and so weight decay preferentially
strengthens the generalising circuits.
Understanding deep learning through circuit-based analysis. One goal of interpretability is to
understand the internal mechanisms by which neural networks exhibit specific behaviours, often
through the identification and characterisation of specific parts of the network, especially compu-
tational subgraphs (“circuits”) that implement human-interpretable algorithms (Cammarata et al.,
2021; Elhage et al., 2021; Erhan et al., 2009; Geva et al., 2020; Li et al., 2022; Meng et al., 2022;
Olah et al., 2020; Wang et al., 2022). Such work can also be used to understand deep learning.
Olsson et al. (2022) explain a phase change in the training of language models by reference to
induction heads, a family of circuits that produce in-context learning. In concurrent work, Singh et al.
show that the in-context learning from induction heads is later replaced by in-weights learning in the
absence of weight decay, but remains strong when weight decay is present. We hypothesise that this
effect is also explained through circuit efficiency: the in-context learning from induction heads is a
generalising algorithm and so is favoured by weight decay given a large enough dataset size.
Michaud et al. (2023) propose an explanation for power-law scaling (Barak et al., 2022; Hestness
et al., 2017; Kaplan et al., 2020) based on a model in which there are many discrete quanta (algorithms)
and larger models learn more of them. Our explanation involves a similar structure: we posit the
existence of two algorithms (quanta), and analyse the resulting training dynamics.
7. Discussion
Rethinking generalisation. Zhang et al. (2021) pose the question of why deep neural networks
achieve good generalisation even when they are easily capable of memorising a random labelling of
the training data. Our results gesture at a resolution: in the presence of weight decay, circuits that
generalise well are likely to be more efficient given a large enough dataset, and thus are preferred over
memorisation circuits, even when both achieve perfect training loss (Section 4.1). Similar arguments
may hold for other types of regularisation as well.
11

Explaining grokking through circuit efficiency
Necessity of weight decay. The biggest limitation of our explanation is that it relies crucially on
weight decay, but grokking has been observed even when weight decay is not present (Power et al.,
2021; Thilak et al., 2022) (though it is slower and often much harder to elicit (Nanda et al., 2023,
Appendix D.1)). This demonstrates that our explanation is incomplete.
Does it also imply that our explanation is incorrect? We do not think so. We speculate there is
at least one other effect that has a similar regularising effect favouring 𝐶gen over 𝐶mem , such as the
implicit regularisation of gradient descent (Lyu and Li, 2019; Smith and Le, 2017; Soudry et al., 2018;
Wang et al., 2021), and that the speed of the transition from 𝐶mem to 𝐶gen is based on the sum of these
effects and the effect from weight decay. This would neatly explain why grokking takes longer as
weight decay decreases (Power et al., 2021), and does not completely vanish in the absence of weight
decay. Given that there is a potential extension of our theory that explains grokking without weight
decay, and the significant confirming evidence that we have found for our theory in settings with
weight decay, we are overall confident that our explanation is at least one part of the true explanation
when weight decay is present.
Broader applicability: beyond parameter norm. Another limitation is that we only consider one
kind of constraint that gradient descent must navigate: parameter norm. Typically, there are many
other constraints: fitting the training data, capacity in “bottleneck activations” (Elhage et al., 2021),
interference between circuits (Elhage et al., 2022), and more. This may limit the broader applicability
of our theory, despite its success in explaining grokking.
Broader applicability: realistic settings. A natural question is what implications our theory has
for more realistic settings. We expect that the general concepts of circuits, efficiency, and speed of
learning continue to apply. However, in realistic settings, good training performance is typically
achieved when the model has many different circuit families that contribute different aspects (e.g.
language modelling requires spelling, grammar, arithmetic, etc). We expect that these will have a
wide variety of learning speeds and efficiencies (although note that “efficiency” is not as well defined
in this setting, because the circuits don’t get perfect training accuracy).
In contrast, the key property for grokking in “algorithmic” tasks like modular arithmetic is that
there are two clusters of circuit families – one slowly learned, efficient, generalising cluster, and one
quickly learned, inefficient, memorising cluster. In particular, our explanation relies on there being
no circuits in between the two clusters. Therefore we observe a sharp transition in test performance
when shifting from the memorising to the generalising cluster.
Future work. Within grokking, several interesting puzzles are still left unexplained. Why does
the time taken to grok rise super-exponentially as dataset size decreases? How does the random
initialisation interact with efficiency to determine which circuits are found by gradient descent? What
causes generalising circuits to develop slower? Investigating these puzzles is a promising avenue for
further work.
While the direct application of our work is to understand the puzzle of grokking, we are excited
about the potential for understanding deep learning more broadly through the lens of circuit efficiency.
We would be excited to see work looking at the role of circuit efficiency in more realistic settings, and
work that extends circuit efficiency to consider other constraints that gradient descent must navigate.
8. Conclusion
The central question of our paper is: in grokking, why does the network’s test performance improve
dramatically upon continued training, having already achieved nearly perfect training performance?
Our explanation is: the generalising solution is more “efficient” but slower to learn than the memorising
12

Explaining grokking through circuit efficiency
solution. After quickly learning the memorising circuit, gradient descent can still decrease loss
even further by simultaneously strengthening the efficient, generalising circuit and weakening the
inefficient, memorising circuit.
Based on our theory we predict and demonstrate two novel behaviours: ungrokking, in which a
model that has perfect generalisation returns to memorisation when it is further trained on a dataset
with size smaller than the critical threshold, and semi-grokking, where we train a randomly initialised
network on the critical dataset size which results in a grokking-like transition to middling test accuracy.
Our explanation is the only one we are aware of that has made (and confirmed) such surprising
advance predictions, and we have significant confidence in the explanation as a result.
Acknowledgements
Thanks to Paul Christiano, Xander Davies, Seb Farquhar, Geoffrey Irving, Tom Lieberum, Eric Michaud,
Vlad Mikulik, Neel Nanda, Jonathan Uesato, and anonymous reviewers for valuable discussions and
feedback.
References
B. Barak, B. Edelman, S. Goel, S. Kakade, E. Malach, and C. Zhang. Hidden progress in deep learning:
SGD learns parities near the computational limit. Advances in Neural Information Processing Systems,
35:21750–21764, 2022.
N. Cammarata, G. Goh, S. Carter, C. Voss, L. Schubert, and C. Olah. Curve circuits. Distill, 2021. doi:
10.23915/distill.00024.006. https://distill.pub/2020/circuits/curve-circuits.
B. Chughtai, L. Chan, and N. Nanda. A toy model of universality: Reverse engineering how networks
learn group operations. arXiv preprint arXiv:2302.03025, 2023.
X. Davies, L. Langosco, and D. Krueger. Unifying grokking and double descent. arXiv preprint
arXiv:2303.06173, 2023.
N. Elhage, N. Nanda, C. Olsson, T. Henighan, N. Joseph, B. Mann, A. Askell, Y. Bai, A. Chen, T. Conerly,
N. DasSarma, D. Drain, D. Ganguli, Z. Hatfield-Dodds, D. Hernandez, A. Jones, J. Kernion, L. Lovitt,
K. Ndousse, D. Amodei, T. Brown, J. Clark, J. Kaplan, S. McCandlish, and C. Olah. A mathematical
framework for Transformer circuits. Transformer Circuits Thread, 2021. https://transformer-
circuits.pub/2021/framework/index.html.
N. Elhage, T. Hume, C. Olsson, N. Schiefer, T. Henighan, S. Kravec, Z. Hatfield-Dodds, R. Lasenby,
D. Drain, C. Chen, R. Grosse, S. McCandlish, J. Kaplan, D. Amodei, M. Wattenberg, and
C. Olah. Toy models of superposition. Transformer Circuits Thread, 2022. https://transformer-
circuits.pub/2022/toy_model/index.html.
D. Erhan, Y. Bengio, A. Courville, and P. Vincent. Visualizing higher-layer features of a deep network.
University of Montreal, 1341(3):1, 2009.
M. Geva, R. Schuster, J. Berant, and O. Levy. Transformer feed-forward layers are key-value memories.
arXiv preprint arXiv:2012.14913, 2020.
J. Hestness, S. Narang, N. Ardalani, G. Diamos, H. Jun, H. Kianinejad, M. Patwary, M. Ali, Y. Yang, and
Y. Zhou. Deep learning scaling is predictable, empirically. arXiv preprint arXiv:1712.00409, 2017.
13

Explaining grokking through circuit efficiency
A.
Jermyn
and
B.
Shlegeris.
Multi-component
learning
and
s-curves,
2022.
URL
https://www.alignmentforum.org/posts/RKDQCB6smLWgs2Mhr/
multi-component-learning-and-s-curves.
J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child, S. Gray, A. Radford, J. Wu,
and D. Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020.
A. Krogh and J. Hertz. A simple weight decay can improve generalization. Advances in neural
information processing systems, 4, 1991.
K. Li, A. K. Hopkins, D. Bau, F. Viégas, H. Pfister, and M. Wattenberg. Emergent world representations:
Exploring a sequence model trained on a synthetic task. arXiv preprint arXiv:2210.13382, 2022.
Z. Liu, O. Kitouni, N. Nolte, E. J. Michaud, M. Tegmark, and M. Williams. Towards understanding
grokking: An effective theory of representation learning. arXiv preprint arXiv:2205.10343, 2022.
Z. Liu, E. J. Michaud, and M. Tegmark. Omnigrok: Grokking beyond algorithmic data, 2023.
I. Loshchilov and F. Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101,
2019.
K. Lyu and J. Li. Gradient descent maximizes the margin of homogeneous neural networks. arXiv
preprint arXiv:1906.05890, 2019.
M. McCloskey and N. Cohen. Catastrophic interference in connectionist networks: The sequential
learning problem. Psychology of Learning and Motivation, 24:109–165, 1989.
K. Meng, D. Bau, A. Andonian, and Y. Belinkov. Locating and editing factual knowledge in GPT. arXiv
preprint arXiv:2202.05262, 2022.
W. Merrill, N. Tsilivis, and A. Shukla. A tale of two circuits: Grokking as competition of sparse and
dense subnetworks. arXiv preprint arXiv:2303.11873, 2023.
E. J. Michaud, Z. Liu, U. Girit, and M. Tegmark. The quantization model of neural scaling. arXiv
preprint arXiv:2303.13506, 2023.
B.
Millidge.
Grokking
’grokking’,
2022.
URL
https://www.beren.io/
2022-01-11-Grokking-Grokking/.
N. Nanda, L. Chan, T. Liberum, J. Smith, and J. Steinhardt. Progress measures for grokking via
mechanistic interpretability. arXiv preprint arXiv:2301.05217, 2023.
P. Notsawo Jr, H. Zhou, M. Pezeshki, I. Rish, G. Dumas, et al. Predicting grokking long before it
happens: A look into the loss landscape of models which grok. arXiv preprint arXiv:2306.13253,
2023.
C. Olah, N. Cammarata, L. Schubert, G. Goh, M. Petrov, and S. Carter. Zoom in: An introduction to
circuits. Distill, 2020. doi: 10.23915/distill.00024.001. https://distill.pub/2020/circuits/zoom-in.
C. Olsson, N. Elhage, N. Nanda, N. Joseph, N. DasSarma, T. Henighan, B. Mann, A. Askell, Y. Bai,
A. Chen, T. Conerly, D. Drain, D. Ganguli, Z. Hatfield-Dodds, D. Hernandez, S. Johnston, A. Jones,
J. Kernion, L. Lovitt, K. Ndousse, D. Amodei, T. Brown, J. Clark, J. Kaplan, S. McCandlish, and C. Olah.
In-context learning and induction heads. Transformer Circuits Thread, 2022. https://transformer-
circuits.pub/2022/in-context-learning-and-induction-heads/index.html.
14

Explaining grokking through circuit efficiency
A. Power, Y. Burda, H. Edwards, I. Babuschkin, and V. Misra. Grokking: Generalization beyond
overfitting on small algorithmic datasets. In Mathematical Reasoning in General Artificial Intelligence
Workshop, ICLR, 2021. URL https://mathai-iclr.github.io/papers/papers/MATHAI_
29_paper.pdf.
R. Ratcliff. Connectionist models of recognition memory: constraints imposed by learning and
forgetting functions. Psychology Review, 97(2):285–308, 1990.
A. Singh, S. Chan, T. Moskovitz, E. Grant, A. Saxe, and F. Hill. The transient nature of emergent
in-context learning in transformers. Forthcoming.
S. L. Smith and Q. V. Le. A bayesian perspective on generalization and stochastic gradient descent.
arXiv preprint arXiv:1710.06451, 2017.
D. Soudry, E. Hoffer, M. S. Nacson, S. Gunasekar, and N. Srebro. The implicit bias of gradient descent
on separable data. The Journal of Machine Learning Research, 19(1):2822–2878, 2018.
V. Thilak, E. Littwin, S. Zhai, O. Saremi, R. Paiss, and J. Susskind. The slingshot mechanism: An empir-
ical study of adaptive optimizers and the grokking phenomenon. arXiv preprint arXiv:2206.04817,
2022.
A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. u. Kaiser, and I. Polo-
sukhin.
Attention is all you need.
In Advances in Neural Information Processing Systems,
volume 30, 2017. URL https://proceedings.neurips.cc/paper_files/paper/2017/
file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf.
B. Wang, Q. Meng, W. Chen, and T.-Y. Liu. The implicit bias for adaptive optimization algorithms
on homogeneous neural networks. In International Conference on Machine Learning, pages 10849–
10858. PMLR, 2021.
K. Wang, A. Variengien, A. Conmy, B. Shlegeris, and J. Steinhardt. Interpretability in the wild: a
circuit for indirect object identification in gpt-2 small. arXiv preprint arXiv:2211.00593, 2022.
C. Zhang, S. Bengio, M. Hardt, B. Recht, and O. Vinyals. Understanding deep learning (still) requires
rethinking generalization. Communications of the ACM, 64(3):107–115, 2021.
G. Zhang, C. Wang, B. Xu, and R. Grosse. Three mechanisms of weight decay regularization. arXiv
preprint arXiv:1810.12281, 2018.
15

Explaining grokking through circuit efficiency
0.0
0.5
1.0
Accuracy
train
test
101
10
2
10
5
Loss
0.0
0.5
1.0
1.5
2.0
2.5
3.0
3.5
4.0
Epochs
1e7
30
40
50
Parameter norm
Figure 6 | Examining a single semi-grokking run in detail. We plot accuracy, loss, and parameter
norm over training for a single cherry-picked modular addition run at a dataset size of 1532 (12% of
the full dataset). This run shows transient semi-grokking. At epoch 0.8 × 107, test accuracy rises to
around 0.55, and then stays there for 107 epochs, because 𝐶gen and 𝐶mem efficiencies are balanced.
At epoch 1.8 × 107, we speculate that gradient descent finds an even more efficient 𝐶gen circuit,
as parameter norm drops suddenly and test accuracy rises to 1. At epoch 3.2 × 107 we see test
loss rise, we do not know why. There seem to be multiple phases, perhaps corresponding to the
network transitioning between mixtures of multiple circuits with increasing efficiencies, but further
investigation is needed.
A. Experimental details and more evidence
For all our experiments, we use 1-layer decoder-only transformer networks (Vaswani et al., 2017)
with learned positional embeddings, untied embeddings/unembeddings, The hyperparameters are
as follows: 𝑑model = 128 is the residual stream width, 𝑑head = 32 is the size of the query, key, and
value vectors for each attention head, 𝑑mlp = 512 is the number of neurons in the hidden layer of
the MLP, and we have 𝑑model/𝑑head = 4 heads per self-attention layer. We optimise the network with
full batch training (that is, using the entire training dataset for each update) using the AdamW
optimiser (Loshchilov and Hutter, 2019) with 𝛽1 = 0.9, 𝛽2 = 0.98, learning rate of 10−3, and weight
decay of 1.0. In some of our experiments we vary the weight decay in order to produce networks
with varying parameter norm.
Following Power et al. (2021), for a binary operation 𝑥◦𝑦, we construct a dataset of the form
⟨𝑥⟩⟨◦⟩⟨𝑦⟩⟨=⟩⟨𝑥◦𝑦⟩, where ⟨𝑎⟩stands for the token corresponding to the element 𝑎. We choose a
fraction of this dataset at random as the train dataset, and the remainder as the test dataset. The
first 4 tokens ⟨𝑥⟩⟨◦⟩⟨𝑦⟩⟨=⟩are the input to the network, and we train with cross-entropy loss over the
final token ⟨𝑥◦𝑦⟩. For all modular arithmetic tasks we use the modulus 𝑝= 113, so for example the
size of the full dataset for modular addition is 𝑝2 = 12769, and 𝑑vocab = 115, including the ⟨+⟩and
⟨=⟩tokens.
16

Explaining grokking through circuit efficiency
0.0
0.2
0.4
0.6
0.8
1.0
Epochs
1e6
0.0
0.2
0.4
0.6
0.8
1.0
Test accuracy
400
600
800
1000
1200
1400
Reduced dataset size
Figure 7 | Many ungrokking runs. We show test accuracy over epochs for a range of ungrokking
runs for modular addition. Each line represents a single run, and we sweep over 7 geometrically
spaced dataset sizes in [390, 1494] with 10 seeds each. Each run is initialised with parameters from
a network trained on the full dataset (the initialisation runs are not shown), so test accuracy starts
at 1 for all runs. When the dataset size is small enough, the network ungroks to poor test accuracy,
while train accuracy remains at 1 (not shown). For an intermediate dataset size, we see ungrokking
to middling test accuracy as 𝐶gen and 𝐶mem efficiencies are balanced.
A.1. Semi-grokking
In Section 5.3 we looked at all semi-grokking training runs in Figure 5. Here, we investigate a single
example of transient semi-grokking in more detail (see Figure 6). We speculate that there are multiple
circuits with increasing efficiencies for 𝐶gen , and in these cases the more efficient circuits are slower to
learn. This would explain transient semi-grokking: gradient descent first finds a less efficient 𝐶gen and
we see middling generalisation, but since we are using the upper range of 𝐷crit, eventually gradient
descent finds a more efficient 𝐶gen leading to full generalisation.
A.2. Ungrokking
In Figure 7, we show many ungrokking runs for modular addition, and in Figure 8 we show ungrokking
across many other tasks.
We have already seen that 𝐷crit is affected by the random initialisation. It is interesting to compare
𝐷crit when starting with a given random initialisation, and when ungrokking from a network that was
trained to full generalisation with the same random initialisation. Figure 5 shows a semi-grokking run
that achieves a test accuracy of ∼0.7 with a dataset size of ∼2000, while Figure 7 shows ungrokking
runs that achieve a test accuracy of ∼0.7 with a dataset size of around 800–1000, less than half of
what the semi-grokking run required.
In Figure 10b, the final test accuracy after ungrokking shows a smooth relationship with dataset
size, which we might expect if 𝐶gen is getting stronger on a smoothly increasing number of inputs
compared to 𝐶mem . However due to the difficulties discussed previously, we don’t see a smooth
relationship between test accuracy and dataset size in semi-grokking.
These results suggest that 𝐷crit is an oversimplified concept, because in reality the initialisation
and training dynamics affect which circuits are found, and therefore the dataset size at which we see
middling generalisation.
17

Explaining grokking through circuit efficiency
103
104
0.0
0.5
1.0
Test accuracy
x/y mod P
103
104
x
y mod P
103
104
xy mod P
103
104
0.0
0.5
1.0
Test accuracy
x/y mod P or x
y mod P
103
104
x2 + y2 mod P
103
104
x2 + xy + y2 mod P
103
104
Reduced dataset size
0.0
0.5
1.0
Test accuracy
xyx
1 for x, y
S5
103
104
Reduced dataset size
xy for x, y
S5
103
104
Reduced dataset size
xyx for x, y
S5
0.1
0.5
1.0
1.5
2.0
Weight decay
Average accuracy
Per seed accuracy
Figure 8 | Ungrokking on many other tasks. We plot test accuracy against reduced dataset size for
many other modular arithmetic and symmetric group tasks (Power et al., 2021). For each run, we
train on the full dataset (achieving 100% accuracy), and then further train on a reduced subset of the
dataset for 100k steps. The results show clear ungrokking, since in many cases test accuracy falls
below 100%, often to nearly 0%. For most datasets the transition point is independent of weight
decay (different coloured lines almost perfectly overlap).
18

Explaining grokking through circuit efficiency
0
2000
4000
6000
8000
10000
Epochs
10
9
10
7
10
5
10
3
10
1
101
Loss
Train loss
Test loss
0
15
30
45
60
Ctrig logit
Cmem logit
Parameter norm
Figure 9 | Grokking occurs because 𝐶gen is more efficient than 𝐶mem . We show loss, parameter
norm, and the value of the correct logit for 𝐶gen and 𝐶mem for a randomly-picked training run. By step
200, the train accuracy is already perfect (not shown), train loss is low while test loss has risen, and
parameter norm is at its maximum value, indicating strong 𝐶mem . Train loss continues to fall rapidly
until step 1500, as parameter norm falls and the 𝐶gen logit becomes higher than the 𝐶mem logit. At
step 3500, test loss starts to fall as the high 𝐶gen logit starts to dominate, and by step 6000 we get
good generalisation.
A.3. 𝐶gen and 𝐶mem development during grokking
In Figure 9 we show 𝐶gen and 𝐶mem development via the proxy measures defined in Appendix B for
a randomly-picked grokking run. Looking at these measures was very useful to form a working
theory for why grokking happens. However as we note in Appendix B, these proxy measures tend to
overestimate 𝐶gen and underestimate 𝐶mem .
We note some interesting phenomena in Figure 9:
1. Between epochs 200 to 1500, both the 𝐶gen and 𝐶mem logits are rising while parameter norm is
falling, indicating that gradient descent is improving efficiency (possibly by removing irrelevant
parameters).
2. After epoch 4000, the 𝐶gen logit falls while the 𝐶mem logit is already ∼0. Since test loss continues
to fall, we expect that incorrect logits from 𝐶mem on the test dataset are getting cleaned up, as
described in Nanda et al. (2023).
A.4. Tradeoffs between 𝐶gen and 𝐶mem
In Section 4.1 we looked at the efficiency of 𝐶gen -only and 𝐶mem -only circuits. In this section we train
on varying dataset sizes so that the network develops a mixture of 𝐶gen and 𝐶mem circuits, and study
their relative strength using the correct logit as a proxy measure (described in Appendix B).
As we demonstrated previously, 𝐶mem ’s efficiency drops with increasing dataset size, while 𝐶gen ’s
stays constant. Theorem D.4 (case 2) suggests that parameter norm allocated to a circuit is proportional
to efficiency, and since logit values also increase with parameter norm, this implies that the ratio of
the 𝐶gen to 𝐶mem logit 𝑜𝑦
𝑡/𝑜𝑦
𝑚should increase monotonically with dataset size.
In Figure 10a we see exactly this: the logit ratio changes monotonically (over 6 orders of magnitude)
with increasing dataset size.
Due to the difficulties in training to convergence at small dataset sizes, we initialised all parameters
19

Explaining grokking through circuit efficiency
103
104
Dataset size
10
2
100
102
104
oy
t / oy
m
(a) Logit ratio (𝑜𝑦
𝑡/𝑜𝑦
𝑚) vs dataset size (𝐷).
Colours correspond to different bucketed val-
ues of parameter norm (𝑃). Each line shows
that as dataset size increases, a fixed param-
eter norm (fixed colour) is being reallocated
smoothly towards increasing the trigonometric
logit compared to the memorisation logit.
103
104
Dataset size
0.0
0.2
0.4
0.6
0.8
1.0
Test accuracy
(b) Test accuracy vs dataset size (𝐷). We
see a smooth dependence on dataset size.
Each line shows that as dataset size in-
creases, the reallocation of a fixed parame-
ter norm (fixed colour) towards 𝐶gen from
𝐶mem results in increasing accuracy.
2 × 101
3 × 101
4 × 101
Parameter norm
Figure 10 | Relative strength at convergence. We report logit ratios and test accuracy at convergence
across a range of training runs, generated by sweeping over the weight decay and random seed to
obtain different parameter norms at the same dataset size. We use the ungrokking runs from Figure 7,
so every run is initialised with parameters obtained by training on the full dataset.
from a 𝐶gen -only network trained on the full dataset. We confirmed that in all the runs, at convergence,
the training loss from the 𝐶gen -initialised network was lower than the training loss from a randomly
initialised network, indicating that this initialisation allows our optimiser to find a better minimum
than from random initialisation.
B. 𝐶gen and 𝐶mem in modular addition
In modular addition, given two integers 𝑎, 𝑏and a modulus 𝑝as input, where 0 ≤𝑎, 𝑏< 𝑝, the task
is to predict 𝑎+ 𝑏mod 𝑝. Nanda et al. (2023) identified the generalising algorithm implemented
by a 1-layer transformer after grokking (visualised in Figure 11), which we call the “trigonometric”
algorithm. In this section we summarise the algorithm, and explain how we produce our proxy metrics
for the strength of 𝐶gen and 𝐶mem .
Trigonometric logits. We explain the structure of the logits produced by the trigonometric algorithm.
For each possible label 𝑐∈{0, 1, . . . 𝑝−1}, the trigonometric logit 𝑜𝑐will be given by Í
𝜔𝑘cos(𝜔𝑘(𝑎+
𝑏−𝑐)), for a few key frequencies 𝜔𝑘= 2𝜋𝑘
𝑝with integer 𝑘. For the true label 𝑐∗= 𝑎+ 𝑏mod 𝑝, the
term 𝜔𝑘(𝑎+ 𝑏−𝑐∗) is an integer multiple of 2𝜋, and so cos(𝜔𝑘(𝑎+ 𝑏−𝑐∗)) = 1. For any incorrect label
𝑐≠𝑎+ 𝑏mod 𝑝, it is very likely that at least some of the key frequencies satisfy cos(𝜔𝑘(𝑎+ 𝑏−𝑐)) ≪1,
creating a large difference between 𝑜𝑐and 𝑜𝑐∗.
Trigonometric algorithm. There is a set of key frequencies 𝜔𝑘. (These frequencies are typically
whichever frequencies were highest at the time of random initialisation.) For an arbitrary label 𝑐, the
logit 𝑜𝑐is computed as follows:
1. Embed the one-hot encoded number 𝑎to sin(𝜔𝑘𝑎) and cos(𝜔𝑘𝑎) for the various frequencies 𝜔𝑘.
20

Explaining grokking through circuit efficiency
Figure 11 | The trigonometric algorithm for modular arithmetic (reproduced from Nanda et al.
(2023)). Given two numbers 𝑎and 𝑏, the model projects each point onto a corresponding rotation
using its embedding matrix. Using its attention and MLP layers, it then composes the rotations to
get a representation of 𝑎+ 𝑏mod 𝑝. Finally, it “reads off” the logits for each 𝑐∈{0, 1, . . . , 𝑝−1},
by rotating by −𝑐to get 𝑐𝑜𝑠(𝜔(𝑎+ 𝑏−𝑐)), which is maximised when 𝑎+ 𝑏≡𝑐mod 𝑃(since 𝜔is a
multiple of 2𝜋).
Do the same for 𝑏.
2. Compute cos(𝜔𝑘(𝑎+ 𝑏)) and sin(𝜔𝑘(𝑎+ 𝑏)) using the intermediate attention and MLP layers via
the trigonometric identities:
cos(𝜔𝑘(𝑎+ 𝑏)) = cos(𝜔𝑘𝑎) cos(𝜔𝑘𝑎) −sin(𝜔𝑘𝑎) sin(𝜔𝑘𝑏)
sin(𝜔𝑘(𝑎+ 𝑏)) = sin(𝜔𝑘𝑎) cos(𝜔𝑘𝑏) + cos(𝜔𝑘𝑎) sin(𝜔𝑘𝑏)
3. Use the output and unembedding matrices to implement the trigonometric identity:
𝑜𝑐=
∑︁
𝜔𝑘
cos(𝜔𝑘(𝑎+ 𝑏−𝑐)) =
∑︁
𝜔𝑘
cos(𝜔𝑘(𝑎+ 𝑏)) cos(𝜔𝑘𝑐) + sin(𝜔𝑘(𝑎+ 𝑏)) sin(𝜔𝑘𝑐).
Isolating trigonometric logits. Given a classifier ℎ, we can aggregate its logits on every possible
input, resulting in a vector ®𝑍ℎof length 𝑝3 where ®𝑍𝑎,𝑏,𝑐
ℎ
= 𝑜𝑐
ℎ(“𝑎+ 𝑏=”) is the logit for label 𝑐on the
input (𝑎, 𝑏). We are interested in identifying the contribution of the trigonometric algorithm to ®𝑍ℎ.
We use the same method as Chughtai et al. (2023) and restrict ®𝑍ℎto a much smaller trigonometric
subspace.
For a frequency 𝜔𝑘, let us define the 𝑝3-dimensional vector ®𝑍𝜔𝑘as ®𝑍𝑎,𝑏,𝑐
𝜔𝑘
= cos(𝜔𝑘(𝑎+ 𝑏−𝑐)). Since
®𝑍𝜔𝑘= ®𝑍𝜔𝑝−𝑘, we set 1 ≤𝑘≤𝐾, where 𝐾= ⌈(𝑝−1)/2⌉, to obtain 𝐾distinct vectors, ignoring the
constant bias vector. These vectors are orthogonal, as they are part of a Fourier basis.
Notice that any circuit that was exactly following the learned algorithm described above would only
produce logits in the directions ®𝑍𝜔𝑘for the key frequencies 𝜔𝑘. So, we can define the trigonometric
contribution to ®𝑍ℎas the projection of ®𝑍ℎonto the directions ®𝑍𝜔𝑘. We may not know the key frequencies
in advance, but we can sum over all 𝐾of them, giving the following definition for trigonometric logits:
®𝑍ℎ,𝑇=
𝐾
∑︁
𝑘=1
( ®𝑍ℎ· ˆ𝑍𝜔𝑘)ˆ𝑍𝜔𝑘
21

Explaining grokking through circuit efficiency
where ˆ𝑍𝜔𝑘is the normalised version of ®𝑍𝜔𝑘. This corresponds to projecting onto a 𝐾-dimensional
subspace of the 𝑝3-dimensional space in which ®𝑍ℎlives.
Memorisation logits. Early in training, neural networks memorise the training dataset without
generalising, suggesting that there exists a memorisation algorithm, implemented by the circuit
𝐶mem 1. Unfortunately, we do not understand the algorithm underlying memorisation, and so cannot
design a similar procedure to isolate 𝐶mem ’s contribution to the logits. However, we hypothesise that
for modular addition, 𝐶gen and 𝐶mem are the only two circuit families of importance for the loss. This
allows us to define the 𝐶mem contribution to the logits as the residual:
®𝑍ℎ,𝑀= ®𝑍ℎ−®𝑍ℎ,𝑇
𝐶trig and 𝐶mem circuits. We say that a circuit is a 𝐶trig circuit if it implements the 𝐶trig algorithm, and
similarly for 𝐶mem circuits. Importantly, this is a many-to-one mapping: there are many possible
circuits that implement a given algorithm.
We isolate 𝐶trig (®𝑜𝑡) and 𝐶mem (®𝑜𝑚) logits by projecting the output logits (®𝑜) as described in
Appendix B. We cannot directly measure the circuit weights 𝑤𝑡and 𝑤𝑚, but instead use an indirect
measure: the value of the logit for the correct class given by each circuit, i.e. 𝑜𝑦
𝑡and 𝑜𝑦
𝑚.
Flaws These metrics should be viewed as an imperfect proxy measure for the true strength of the
trigonometric and memorisation circuits, as they have a number of flaws:
1. When both 𝐶trig and 𝐶mem are present in the network, they are both expected to produce high
values for the correct logits, and low values for incorrect logits, on the train dataset. Since the
𝐶trig and 𝐶mem logits are correlated, it becomes more likely that ®𝑍ℎ,𝑇captures 𝐶mem logits too.
2. In this case we would expect our proxy measure to overestimate the strength of 𝐶trig and
underestimate the strength of 𝐶mem . In fact, in our experiments we do see large negative correct
logit values for 𝐶mem on training for semi-grokking, which probably arises because of this effect.
3. Logits are not inherently meaningful; what matters for loss is the extent to which the correct
logit is larger than the incorrect logits. This is not captured by our proxy metric, which only
looks at the size of the correct logit. In a binary classification setting, we could instead use the
difference between the correct and incorrect logit, but it is not clear what a better metric would
be in the multiclass setting.
C. Details for the minimal example
In Figure 2 we show that two ingredients: multiple circuits with different efficiencies, and slow and
fast circuit development, are sufficient to reproduce learning curves that qualitatively demonstrate
grokking. In Table 1 we provide details about the simulation used to produce this figure.
As explained in Section 3, the logits produced by 𝐶gen and 𝐶mem are given by:
𝑜𝑦
𝐺(𝑥) = 𝟙[(𝑥, 𝑦) ∈D or (𝑥, 𝑦) ∈Dtest]
(3)
𝑜𝑦
𝑀(𝑥) = 𝟙[(𝑥, 𝑦) ∈D or (𝑥, 𝑦) ∈Dmem]
(4)
1In reality, there are at least two different memorisation algorithms: commutative memorisation (which predicts the
same answer for (𝑎, 𝑏) and (𝑏, 𝑎)) and non-commutative memorisation (which does not). However, this difference does not
matter for our analyses, and we will call both of these “memorisation” in this paper.
22

Explaining grokking through circuit efficiency
Table 1 | Hyperparameters used for our simulations.
(a) 𝐶gen learned slower but more
efficient than 𝐶mem .
Parameter
Value
𝑃𝑔
1
𝑃𝑚
2
𝜅
1.2
𝛼
0.005
𝑤𝑔1(0)
0
𝑤𝑔2(0)
0.005
𝑤𝑚1(0)
0
𝑤𝑚2(0)
1
𝑞
113
𝜆
0.01
(b) 𝐶gen less efficient than 𝐶mem .
Parameter
Value
𝑃𝑔
4
𝑃𝑚
2
𝜅
1.2
𝛼
0.005
𝑤𝑔1(0)
0
𝑤𝑔2(0)
0.005
𝑤𝑚1(0)
0
𝑤𝑚2(0)
1
𝑞
113
𝜆
0.01
(c) 𝐶gen and 𝐶mem learned at equal
speeds.
Parameter
Value
𝑃𝑔
1
𝑃𝑚
2
𝜅
1.2
𝛼
0.005
𝑤𝑔1(0)
0
𝑤𝑔2(0)
1
𝑤𝑚1(0)
0
𝑤𝑚2(0)
1
𝑞
113
𝜆
0.01
These are scaled by two independent weights for each circuit, giving the overall logits as:
𝑜𝑦(𝑥) = 𝑤𝐺1𝑤𝐺2𝑜𝑦
𝐺(𝑥) + 𝑤𝑀1𝑤𝑀2𝑜𝑦
𝑀(𝑥)
(5)
We model the parameter norms according to the scaling efficiency in Section D.1, inspired by a
𝜅-layer MLP with Relu activations and without biases:
𝑃′
𝑐= (𝑤𝑐1𝑤𝑐2)1/𝜅𝑃𝑐for 𝑐∈(𝑔, 𝑚).
From Equations (3) to (5) we get the following equations for train and test loss respectively:
Ltrain = −log
exp(𝑤𝑔1𝑤𝑔2 + 𝑤𝑚1𝑤𝑚2)
(𝑞−1) + exp(𝑤𝑔1𝑤𝑔2 + 𝑤𝑚1𝑤𝑚2) + Lwd,
Ltest = −log
exp(𝑤𝑔1𝑤𝑔2)
(𝑞−2) + exp(𝑤𝑔1𝑤𝑔2) + exp(𝑤𝑚1𝑤𝑚2) + Lwd,
where 𝑞is the number of labels, and the weight decay loss is:
Lwd = 𝑃′2
𝑔+ 𝑃′2
𝑚.
The weights 𝑤𝑐𝑖are updated based on gradient descent:
𝑤𝑐𝑖(𝜏) ←𝑤𝑐𝑖(𝜏−1) −𝜆𝜕Ltrain
𝜕𝑤𝑐𝑖
where 𝜆is a learning rate. The initial values of the parameters are 𝑤𝑐𝑖(0). In Table 1 we list the values
of the simulation hyperparameters.
D. Proofs of theorems
We assume we have a set of inputs 𝑋, a set of labels 𝑌, and a training dataset, D = {(𝑥1, 𝑦1), . . . (𝑥𝐷, 𝑦𝐷)}.
Let ℎbe a classifier that assigns a real-valued logit for each possible label given an input. We denote
23

Explaining grokking through circuit efficiency
an individual logit as 𝑜𝑦
ℎ(𝑥) B ℎ(𝑥, 𝑦). When the input 𝑥is clear from context, we will denote the logit
as 𝑜𝑦
ℎ. Excluding weight decay, the loss for the classifier is given by the softmax cross-entropy loss:
Lx-ent(ℎ) = −1
𝐷
∑︁
(𝑥,𝑦)∈D
log
exp(𝑜𝑦
ℎ)
Í
𝑦′∈𝑌
exp(𝑜𝑦′
ℎ)
.
For any 𝑐∈ℝ, let 𝑐·ℎbe the classifier whose logits are multipled by 𝑐, that is, (𝑐·ℎ)(𝑥, 𝑦) = 𝑐×ℎ(𝑥, 𝑦).
Intuitively, once a classifier achieves perfect accuracy, then the true class logit 𝑜𝑦∗will be larger than
any incorrect class logit 𝑜𝑦′, and so loss can be further reduced by scaling up all of the logits further
(increasing the gap between 𝑜𝑦∗and 𝑜𝑦′).
Theorem D.1. Suppose that the classifier ℎhas perfect accuracy, that is, for any (𝑥, 𝑦∗) ∈D and any
𝑦′ ≠𝑦∗we have 𝑜𝑦∗
ℎ> 𝑜𝑦′
ℎ. Then, for any 𝑐> 1, we have Lx-ent(𝑐· ℎ) < Lx-ent(ℎ).
Proof. First, note that we can rewrite the loss function as:
Lx-ent(ℎ) = −1
𝐷
∑︁
(𝑥,𝑦∗)
log
exp(𝑜𝑦∗
ℎ)
Í
𝑦′ exp(𝑜𝑦′
ℎ)
= 1
𝐷
∑︁
(𝑥,𝑦∗)
log
©­­­
«
Í
𝑦′ exp(𝑜𝑦′
ℎ)
exp(𝑜𝑦∗
ℎ)
ª®®®
¬
= 1
𝐷
∑︁
(𝑥,𝑦∗)
log ©­
«
1 +
∑︁
𝑦′≠𝑦∗
exp(𝑜𝑦′
ℎ−𝑜𝑦∗
ℎ)ª®
¬
Since we are given that 𝑜𝑦∗
ℎ> 𝑜𝑦′
ℎ, for any 𝑐> 1 we have 𝑐(𝑜𝑦′
ℎ−𝑜𝑦∗
ℎ)) < 𝑜𝑦′
ℎ−𝑜𝑦∗
ℎ. Since exp, log,
and sums are all monotonic, this gives us our desired result:
Lx-ent(𝑐·ℎ) = 1
𝐷
∑︁
(𝑥,𝑦∗)
log ©­
«
1 +
∑︁
𝑦′≠𝑦∗
exp(𝑐(𝑜𝑦′
ℎ−𝑜𝑦∗
ℎ))ª®
¬
< 1
𝐷
∑︁
(𝑥,𝑦∗)
log ©­
«
1 +
∑︁
𝑦′≠𝑦∗
exp(𝑜𝑦′
ℎ−𝑜𝑦∗
ℎ)ª®
¬
= Lx-ent(ℎ).
□
We now move on to Theorem D.4. First we establish some basic lemmas that will be used in the
proof:
Lemma D.2. Let 𝑎, 𝑏, 𝑟∈ℝwith 𝑎, 𝑏≥0 and 0 < 𝑟≤1. Then (𝑎+ 𝑏)𝑟≤𝑎𝑟+ 𝑏𝑟.
Proof. The case with 𝑎= 0 or 𝑏= 0 is clear, so let us consider 𝑎, 𝑏> 0. Let 𝑥=
𝑎
𝑎+𝑏and 𝑦=
𝑏
𝑎+𝑏. Since
0 ≤𝑥≤1, we have 𝑥(1−𝑟) ≤1, which implies 𝑥≤𝑥𝑟. Similarly 𝑦≤𝑦𝑟. Thus 𝑥𝑟+ 𝑦𝑟≥𝑥+ 𝑦= 1.
Substituting in the values of 𝑥and 𝑦we get
𝑎𝑟+𝑏𝑟
(𝑎+𝑏)𝑟≥1, which when rearranged gives us the desired
result.
□
Lemma D.3. For any 𝑥, 𝑐, 𝑟∈ℝwith 𝑟≥1, there exists some 𝛿> 0 such that for any 𝜖< 𝛿we have
𝑥𝑟−(𝑥−𝜖)𝑟> 𝛿(𝑟𝑥𝑟−1 −𝑐).
Proof. The function 𝑓(𝑥) = 𝑥𝑟is everywhere-differentiable and has derivative 𝑟𝑥𝑟−1. Thus we can
choose 𝛿such that for any 𝜖< 𝛿we have −𝑐< 𝑥𝑟−(𝑥−𝜖)𝑟
𝛿
−𝑟𝑥𝑟−1 < 𝑐. Rearranging, we get 𝑥𝑟−(𝑥−𝜖)𝑟>
𝛿(𝑟𝑥𝑟−1 −𝑐) as desired.
□
24

Explaining grokking through circuit efficiency
D.1. Weight decay favours efficient circuits
To flesh out the argument in Section 3, we construct a minimal example of multiple circuits {𝐶1, . . . 𝐶𝐼}
of varying efficiencies that can be scaled up or down through a set of non-negative weights 𝑤𝑖. Our
classifier is given by ℎ= Í𝐼
𝑖=1 𝑤𝑖𝐶𝑖, that is, the output ℎ(𝑥, 𝑦) is given by Í𝐼
𝑖=1 𝑤𝑖𝐶𝑖(𝑥, 𝑦).
We take circuits 𝐶𝑖that are normalised, that is, they produce the same average logit value. 𝑃𝑖
denotes the parameter norm of the normalised circuit 𝐶𝑖. We decide to call a circuit with lower 𝑃𝑖
more efficient. However, it is hard to define efficiency precisely. Consider instead the parameter
norm 𝑃′
𝑖of the scaled circuit 𝑤𝑖𝐶𝑖. If we define efficiency as either the ratio ∥®𝑜𝐶𝑖∥/𝑃′
𝑖or the derivative
𝑑∥®𝑜𝐶𝑖∥/𝑑𝑃′
𝑖, then it would vary with 𝑤𝑖since ®𝑜𝐶𝑖and 𝑃′
𝑖can in general have different relationships
with 𝑤𝑖. We prefer 𝑃𝑖as a measure of relative efficiency as it is intrinsic to 𝐶𝑖rather than depending
on its scaling 𝑤𝑖.
Gradient descent operates over the weights 𝑤𝑖(but not 𝐶𝑖or 𝑃𝑖) to minimise L = Lx-ent + 𝛼Lwd.
Lx-ent can easily be rewritten in terms of 𝑤𝑖, but for Lwd we need to model the parameter norm of the
scaled circuits 𝑤𝑖𝐶𝑖. Notice that, in a 𝜅-layer MLP with Relu activations and without biases, scaling
all parameters by a constant 𝑐scales the outputs by 𝑐𝜅. Inspired by this observation, we model the
parameter norm of 𝑤𝑖𝐶𝑖as 𝑤1/𝜅
𝑖
𝑃𝑖for some 𝜅> 0. This gives the following effective loss:
L( ®𝑤) = Lx-ent
 
𝐼∑︁
𝑖=1
𝑤𝑖𝐶𝑖
!
+ 𝛼
2
𝐼∑︁
𝑖=1
(𝑤
1
𝜅
𝑖𝑃𝑖)2
We will generalise this to any 𝐿𝑞-norm (where 𝑞> 0). Standard weight decay corresponds to
𝑞= 2. We will also generalise to arbitrary differentiable, bounded training loss functions, instead of
cross-entropy loss specifically. In particular, we assume that there is some differentiable Ltrain(ℎ) such
that there exists a finite bound 𝐵∈ℝsuch that ∀ℎ: Ltrain(ℎ) ≥𝐵. (In the case of cross-entropy loss,
𝐵= 0.)
With these generalisations, the overall loss is now given by:
L( ®𝑤) = Ltrain
 
𝐼∑︁
𝑖=1
𝑤𝑖𝐶𝑖
!
+ 𝛼
𝑞
𝐼∑︁
𝑖=1
(𝑤
1
𝜅
𝑖𝑃𝑖)𝑞
(6)
The following theorem establishes that the optimal weight vector allocates more weight to more
efficient circuits, under the assumption that the circuits produce identical logits on the training
dataset.
Theorem D.4. Given 𝐼circuits 𝐶𝑖and associated 𝐿𝑞parameter norms 𝑃𝑖, assume that every circuit
produces the same logits on the training dataset, i.e. ∀𝑖, 𝑗, ∀(𝑥, _) ∈D, ∀𝑦′ ∈𝑌we have 𝑜𝑦′
𝐶𝑖(𝑥) = 𝑜𝑦′
𝐶𝑗(𝑥).
Then, any weight vector ®𝑤∗∈ℝ𝐼that minimizes the loss in Equation 6 subject to 𝑤𝑖≥0 satisfies:
1. If 𝜅≥𝑞, then 𝑤∗
𝑖= 0 for all 𝑖such that 𝑃𝑖> min𝑗𝑃𝑗.
2. If 0 < 𝜅< 𝑞, then 𝑤∗
𝑖∝𝑃
−𝑞𝜅
𝑞−𝜅
𝑖
.
Intuition . Since every circuit produces identical logits, their weights are interchangeable with each
other from the perspective of Lx-ent, and so we must analyse how interchanging weights affects Lwd.
Lwd grows as 𝑂(𝑤2/𝜅
𝑖
). When 𝜅> 2, Lwd grows sublinearly, and so it is cheaper to add additional
weight to the largest weight, creating a “rich get richer” effect that results in a single maximally
efficient circuit getting all of the weight. When 𝜅< 2, Lwd grows superlinearly, and so it is cheaper
to add additional weight to the smallest weight. As a result, every circuit is allocated at least some
weight, though more efficient circuits are still allocated higher weight than less efficient circuits.
25

Explaining grokking through circuit efficiency
Sketch . The assumption that every circuit produces the same logits on the training dataset implies
that Ltrain is purely a function of Í𝐼
𝑖=1 𝑤𝑖. So, for Ltrain, a small increase 𝛿𝑤to 𝑤𝑖can be balanced by
a corresponding decrease 𝛿𝑤to some other weight 𝑤𝑗.
For Lwd, an increase 𝛿𝑤to 𝑤𝑖produces a change of approximately 𝛿Lwd
𝛿𝑤𝑖· 𝛿𝑤= 𝛼
𝜅(𝑃𝑖(𝑤𝑖)𝑟)𝑞· 𝛿𝑤,
where 𝑟= 1
𝜅−1
𝑞= 𝑞−𝜅
𝑞𝜅. So, an increase of 𝛿𝑤to 𝑤𝑖can be balanced by a decrease of

𝑃𝑖(𝑤𝑖)𝑟
𝑃𝑗(𝑤𝑗)𝑟
𝑞
𝛿𝑤to
some other weight 𝑤𝑗. The two cases correspond to 𝑟≤0 and 𝑟> 0 respectively.
Case 1: 𝑟≤0. Consider 𝑖, 𝑗with 𝑃𝑗> 𝑃𝑖. The optimal weights must satisfy 𝑤∗
𝑖≥𝑤∗
𝑗(else you could
swap 𝑤∗
𝑖and 𝑤∗
𝑗to decrease loss). But then 𝑤∗
𝑗must be zero: if not, we could increase 𝑤∗
𝑖by 𝛿𝑤and
decrease 𝑤∗
𝑗by 𝛿𝑤, which keeps Lx-ent constant and decreases Lwd (since 𝑃𝑖(𝑤∗
𝑖)𝑟< 𝑃𝑗(𝑤∗
𝑗)𝑟).
Case 2: 𝑟> 0. Consider 𝑖, 𝑗with 𝑃𝑗> 𝑃𝑖. As before we must have 𝑤∗
𝑖≥𝑤∗
𝑗. But now 𝑤∗
𝑗must not
be zero: otherwise we could increase 𝑤∗
𝑗by 𝛿𝑤and decrease 𝑤∗
𝑖by 𝛿𝑤to keep Lx-ent constant and
decrease Lwd, since 𝑃𝑗(𝑤∗
𝑗)𝑟= 0 < 𝑃𝑖(𝑤∗
𝑖)𝑟. The balance occurs when 𝑃𝑗(𝑤∗
𝑗)𝑟= 𝑃𝑖(𝑤∗
𝑖)𝑟, which means
𝑤∗
𝑖∝𝑃−1/𝑟
𝑖
.
Proof. First, notice that our conclusions trivially hold for ®𝑤∗= ®0 (which can be a minimum if e.g.
the circuits are worse than random). Thus for the rest of the proof we will assume that at least one
weight is non-zero.
In addition, L →∞whenever any 𝑤𝑖→∞(because Ltrain ≥𝐵and Lwd →∞as any one 𝑤𝑖→∞).
Thus, any global minimum must have finite ®𝑤.
Notice that, since the circuit logits are independent of 𝑖, we have ℎ= (Í
𝑖𝑤𝑖) 𝑓, and so Ltrain( ®𝑤) is
purely a function of the sum of weights Í𝐼
𝑖=1 𝑤𝑖, and the overall loss can be written as:
L( ®𝑤) = Ltrain
 
𝐼∑︁
𝑖=1
𝑤𝑖
!
+ 𝛼
𝑞
𝐼∑︁
𝑖=1
((𝑤𝑖)
1
𝜅𝑃𝑖)𝑞
We will now consider each case in order.
Case 1: 𝜅≥𝑞. Assume towards contradiction that there is a global minimum ®𝑤∗where 𝑤∗
𝑗> 0 for
some circuit 𝐶𝑗with non-minimal 𝑃𝑗. Let 𝐶𝑖be a circuit with minimal 𝑃𝑖(so that 𝑃𝑖< 𝑃𝑗), and let its
weight be 𝑤∗
𝑖.
Consider an alternate weight assignment ®𝑤′ that is identical to ®𝑤∗except that 𝑤′
𝑗= 0 and
𝑤′
𝑖= 𝑤∗
𝑖+ 𝑤∗
𝑗. Clearly Í
𝑖𝑤∗
𝑖= Í
𝑖𝑤′
𝑖, and so Ltrain( ®𝑤∗) = Ltrain( ®𝑤′). Thus, we have:
L( ®𝑤∗) −L( ®𝑤′)
=
 
𝛼
𝑞
𝐼∑︁
𝑚=1
((𝑤∗
𝑚)
1
𝜅𝑃𝑚)𝑞
!
−
 
𝛼
𝑞
𝐼∑︁
𝑚=1
((𝑤′
𝑚)
1
𝜅𝑃𝑚)𝑞
!
= 𝛼
𝑞

(𝑤∗
𝑖)
𝑞
𝜅𝑃𝑞
𝑖+ (𝑤∗
𝑗)
𝑞
𝜅𝑃𝑞
𝑗−(𝑤′
𝑖)
𝑞
𝜅𝑃𝑞
𝑖

> 𝛼
𝑞𝑃𝑞
𝑖

(𝑤∗
𝑖)
𝑞
𝜅+ (𝑤∗
𝑗)
𝑞
𝜅−(𝑤′
𝑖)
𝑞
𝜅

since 𝑃𝑗> 𝑃𝑖
= 𝛼
𝑞𝑃𝑞
𝑖

(𝑤∗
𝑖)
𝑞
𝜅+ (𝑤∗
𝑗)
𝑞
𝜅−(𝑤∗
𝑖+ 𝑤∗
𝑗)
𝑞
𝜅

definition of 𝑤′
𝑖
≥𝛼
𝑞𝑃𝑞
𝑖

(𝑤∗
𝑖)
𝑞
𝜅+ (𝑤∗
𝑗)
𝑞
𝜅−

(𝑤∗
𝑖)
𝑞
𝜅+ (𝑤∗
𝑗)
𝑞
𝜅

using Lemma D.2 since 0 < 𝑞
𝜅≤1
= 0
26

Explaining grokking through circuit efficiency
Thus we have L( ®𝑤∗) > L( ®𝑤′), contradicting our assumption that ®𝑤∗is a global minimum of L.
This completes the proof for the case that 𝜅≥𝑞.
Case 2: 𝜅< 𝑞. First, we will show that all weights are non-zero at a global minimum (excluding the
case where ®𝑤∗= ®0, discussed at the beginning of the proof). Assume towards contradiction that there
is a global minimum ®𝑤∗with 𝑤∗
𝑗= 0 for some 𝑗. Choose some arbitrary circuit 𝐶𝑖with nonzero weight
𝑤∗
𝑖.
Choose some 𝜖1 > 0 satisfying 𝜖1 <
𝑞
2𝜅(𝑤∗
𝑖)
𝑞
𝜅−1. By applying Lemma D.3 with 𝑥= 𝑤∗
𝑖, 𝑐= 𝜖1, 𝑟= 𝑞
𝜅,
we can get some 𝛿> 0 such that for any 𝜖< 𝛿we have (𝑤∗
𝑖)
𝑞
𝜅−(𝑤∗
𝑖−𝜖)
𝑞
𝜅> 𝛿( 𝑞
𝜅(𝑤∗
𝑖)
𝑞
𝜅−1 −𝜖1).
Choose some 𝜖2 > 0 satisfying 𝜖2 < min(𝑤∗
𝑖, 𝛿,

𝑞
2𝜅(𝑤∗
𝑖)
𝑞
𝜅−1 𝑃𝑞
𝑖
𝑃𝑞
𝑗

1
𝑞
𝜅−1 ). Consider an alternate weight
assignment defined ®𝑤′ that is identical to ®𝑤∗except that 𝑤′
𝑗= 𝜖2 and 𝑤′
𝑖= 𝑤∗
𝑖−𝜖2. As in the previous
case, Ltrain( ®𝑤∗) = Ltrain( ®𝑤′). Thus, we have:
L( ®𝑤∗) −L( ®𝑤′)
= 𝛼
𝑞

(𝑤∗
𝑖)
𝑞
𝜅𝑃𝑞
𝑖−(𝑤∗
𝑖−𝜖2)
𝑞
𝜅𝑃𝑞
𝑖−𝜖
𝑞
𝜅
2 𝑃𝑞
𝑗

= 𝛼
𝑞

𝑃𝑞
𝑖((𝑤∗
𝑖)
𝑞
𝜅−(𝑤∗
𝑖−𝜖2)
𝑞
𝜅) −𝜖
𝑞
𝜅
2 𝑃𝑞
𝑗

> 𝛼
𝑞

𝑃𝑞
𝑖𝛿( 𝑞
𝜅(𝑤∗
𝑖)
𝑞
𝜅−1 −𝜖1) −𝜖
𝑞
𝜅
2 𝑃𝑞
𝑗

application of Lemma D.3 discussed above
> 𝛼
𝑞

𝑃𝑞
𝑖𝛿( 𝑞
𝜅(𝑤∗
𝑖)
𝑞
𝜅−1 −𝑞
2𝜅(𝑤∗
𝑖)
𝑞
𝜅−1) −𝜖
𝑞
𝜅
2 𝑃𝑞
𝑗

we chose 𝜖1 < 𝑞
2𝜅(𝑤∗
𝑖)
𝑞
𝜅−1
> 𝛼
𝑞

𝑃𝑞
𝑖𝜖2
𝑞
2𝜅(𝑤∗
𝑖)
𝑞
𝜅−1 −𝜖
𝑞
𝜅
2 𝑃𝑞
𝑗

we chose 𝜖2 < 𝛿
= 𝛼𝜖2
𝑞
 𝑞
2𝜅(𝑤∗
𝑖)
𝑞
𝜅−1𝑃𝑞
𝑖−𝜖
𝑞
𝜅−1
2
𝑃𝑞
𝑗

> 𝛼𝜖2
𝑞
 
𝑞
2𝜅(𝑤∗
𝑖)
𝑞
𝜅−1𝑃𝑞
𝑖−𝑞
2𝜅(𝑤∗
𝑖)
𝑞
𝜅−1 𝑃𝑞
𝑖
𝑃𝑞
𝑗
𝑃𝑞
𝑗
!
we chose 𝜖2 <
"
𝑞
2𝜅(𝑤∗
𝑖)
𝑞
𝜅−1 𝑃𝑞
𝑖
𝑃𝑞
𝑗
#
1
𝑞
𝜅−1
)
= 0
Note that in the last step, we rely on the fact that 𝜅< 𝑞: this lets us use an upper bound on 𝜖2 to
get an upper bound on 𝜖
𝑞
𝜅−1
2
, and so a lower bound on the overall expression.
Thus we have L( ®𝑤∗) > L( ®𝑤′), contradicting our assumption that ®𝑤∗is a global minimum of L.
So, for all 𝑖we have 𝑤𝑖> 0.
In addition, as 𝑤𝑖→∞we have L( ®𝑤) →∞, so ®𝑤∗cannot be at the boundaries, and instead lies in
the interior. Since 𝑞> 𝜅, L( ®𝑤) is differentiable everywhere. Thus, we can conclude that its gradient
27

Explaining grokking through circuit efficiency
at ®𝑤∗is zero:
𝛿L
𝛿𝑤𝑖
= 0
𝛿Ltrain
𝛿𝑤𝑖
+
𝛼𝑃𝑞
𝑖
𝜅(𝑤∗
𝑖)
𝑞
𝜅−1 = 0
𝑃𝑞
𝑖(𝑤∗
𝑖)
𝑞−𝜅
𝜅= −𝜅
𝛼
𝛿Ltrain
𝛿𝑤𝑖
𝑤∗
𝑖𝑃
𝑞𝜅
𝑞−𝜅
𝑖
=

−𝜅
𝛼
𝛿Ltrain
𝛿𝑤𝑖

𝜅
𝑞−𝜅
Since Ltrain( ®𝑤) is a function of
𝐼Í
𝑗=1
𝑤𝑗, we can conclude that 𝛿Ltrain
𝛿𝑤𝑖
= 𝛿Ltrain
𝛿Í
𝑗𝑤𝑗·
𝛿Í
𝑗𝑤𝑗
𝛿𝑤𝑖
= 𝛿Ltrain
𝛿Í
𝑗𝑤𝑗, which
is independent of 𝑖. So the right hand side of the equation is independent of 𝑖, allowing us to conclude
that 𝑤∗
𝑖∝𝑃
−𝑞𝜅
𝑞−𝜅
𝑖
.
□
28

