J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
BAYESIAN DECISION THEORY 

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
2 Credits 
  Some of these slides were sourced and/or modified 
from: 
 Christopher Bishop, Microsoft UK 
 Simon Prince, University College London 
 Sergios Theodoridis, University of Athens & Konstantinos 
Koutroumbas, National Observatory of Athens 

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
3 Bayesian Decision Theory:  Topics 
1. 
Probability 
2. 
The Univariate Normal Distribution 
3. 
Bayesian Classifiers 
4. 
Minimizing Risk 
5. 
The Multivariate Normal Distribution 
6. 
Decision Boundaries in Higher Dimensions 
7. 
Parameter Estimation 
8. 
Mixture Models and EM 
9. 
Nonparametric Density Estimation 
10.  What are Bayes Nets? 

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
4 Problems for this Meeting 
  Problems 2.1-2.4 
  Assigned Problem: 2.2 

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
5 Bayesian Decision Theory:  Topics 
1. 
Probability 
2. 
The Univariate Normal Distribution 
3. 
Bayesian Classifiers 
4. 
Minimizing Risk 
5. 
The Multivariate Normal Distribution 
6. 
Decision Boundaries in Higher Dimensions 
7. 
Parameter Estimation 
8. 
Mixture Models and EM 
9. 
Nonparametric Density Estimation 
10.  Training and Evaluation Methods 
11.  What are Bayes Nets? 

Topic 1.  Probability 

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
7 Random Variables 
  A random variable is a variable whose value is uncertain. 
  For example, the height of a randomly selected person in this 
class is a random variable – I won’t know its value until the 
person is selected. 
  Note that we are not completely uncertain about most random 
variables.   
  For example, we know that height will probably be in the 5’-6’ range.   
  In addition, 5’6” is more likely than 5’0” or 6’0”.  
  The function that describes the probability of each possible 
value of the random variable is called a probability 
distribution. 

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
8 Probability Distributions 
  For a discrete distribution, the probabilities over all 
possible values of the random variable must sum to 1. 

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
9 Probability Distributions 
  For a discrete distribution, we can talk about the probability of a particular score 
occurring, e.g., p(Province = Ontario) = 0.36. 
  We can also talk about the probability of any one of a subset of scores occurring, 
e.g., p(Province = Ontario or Quebec) = 0.50. 
  In general, we refer to these occurrences as events. 

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
10 Probability Distributions 
  For a continuous distribution, the probabilities over all possible values of 
the random variable must integrate to 1 (i.e., the area under the curve must 
be 1). 
  Note that the height of a continuous distribution can exceed 1! 
S h a d e d   a r e a   =   0 . 6 8 3 
S h a d e d   a r e a   =   0 . 9 5 4 
S h a d e d   a r e a   =   0 . 9 9 7 

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
11 Continuous Distributions 
  For continuous distributions, it does not make sense to talk about the 
probability of an exact score. 
  e.g., what is the probability that your height is exactly 65.485948467… inches? 
55 
60 
65 
70 
75 
0 
0.02 
0.04 
0.06 
0.08 
0.1 
0.12 
0.14 
0.16 
Height (in) 
Probability p 
Normal Approximation to probability distribution for height of Canadian females 
(parameters from General Social Survey, 1991) 
5'3.8"
2.6"
s
µ =
=
? 

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
12 Continuous Distributions 
 
It does make sense to talk about the probability of observing a score that falls within a certain 
range 
 
e.g., what is the probability that you are between 5’3” and 5’7”? 
 
e.g., what is the probability that you are less than 5’10”? 
55 
60 
65 
70 
75 
0 
0.02 
0.04 
0.06 
0.08 
0.1 
0.12 
0.14 
0.16 
Height (in) 
Probability p 
Normal Approximation to probability distribution for height of Canadian females 
(parameters from General Social Survey, 1991) 
5'3.8"
2.6"
s
µ =
=
Valid events 

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
13 Probability Densities 
Probability density (PDF) 
Cumulative distribution (CDF) 

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
14 Transformed Densities 
Observations falling within x + !x
(
)  tranform to the range y + !y
(
)
! px (x) "x = py (y) "y
! py (y) ! px (x) "x
"y
Note that in general, !y " !x.
Rather, !y
!x
" dy
dx
 as !x " 0.
Thus py (y) = px (x) dx
dy

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
15 Joint Distributions 
Marginal Probability 
Conditional Probability 
Joint Probability 
 

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
16 Joint Distributions 
 Sum Rule 
Product Rule 
 

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
17 Joint Distributions:  The Rules of Probability 
  Sum Rule 
  Product Rule 

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
18 Marginalization 
We can recover probability distribution of any variable in a joint distribution 
by integrating (or summing) over the other variables 

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
19 Conditional Probability 
  Conditional probability of X given that Y=y* is relative 
propensity of variable X to take different outcomes given that 
Y is fixed to be equal to y* 
  Written as Pr(X|Y=y*) 

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
20 Conditional Probability 
  Conditional probability can be extracted from joint probability 
  Extract appropriate slice and normalize 

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
21 Conditional Probability 
  More usually written in compact form 
•  Can be re-arranged to give 
 

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
22 Independence 
  If two variables X and Y are independent then variable X tells 
us nothing about variable Y (and vice-versa) 

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
23 Independence 
  When variables are independent, the joint factorizes into a 
product of the marginals: 

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
24 Bayes’ Rule 
From before: 
Combining: 
Re-arranging: 

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
25 Bayes’ Rule Terminology 
Posterior – what we know 
about y after seeing x 
Prior – what we know 
about y before seeing x 
Likelihood – propensity for 
observing a certain value of 
X given a certain value of Y 
Evidence –a constant to 
ensure that the left hand 
side is a valid distribution 

End of Lecture 2 

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
27 Bayesian Decision Theory:  Topics 
1. 
Probability 
2. 
The Univariate Normal Distribution 
3. 
Bayesian Classifiers 
4. 
Minimizing Risk 
5. 
The Multivariate Normal Distribution 
6. 
Decision Boundaries in Higher Dimensions 
7. 
Parameter Estimation 
8. 
Mixture Models and EM 
9. 
Nonparametric Density Estimation 
10.  Training and Evaluation Methods 
11.  What are Bayes Nets? 

Topic 2.  The Univariate Normal Distribution 

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
29 The Gaussian Distribution 
MATLAB Statistics Toolbox Function:   
normpdf(x,mu,sigma) 

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
30 Central Limit Theorem  
 The distribution of the sum of N i.i.d. random 
variables becomes increasingly Gaussian as N grows. 
 Example: N uniform [0,1] random variables. 

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
31 Expectations 
Condi3onal Expecta3on 
(discrete) 
Approximate Expecta3on 
(discrete and con3nuous) 

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
32 Variances and Covariances 

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
33 Gaussian Mean and Variance 

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
34 Bayesian Decision Theory:  Topics 
1. 
Probability 
2. 
The Univariate Normal Distribution 
3. 
Bayesian Classifiers 
4. 
Minimizing Risk 
5. 
The Multivariate Normal Distribution 
6. 
Decision Boundaries in Higher Dimensions 
7. 
Parameter Estimation 
8. 
Mixture Models and EM 
9. 
Nonparametric Density Estimation 
10.  Training and Evaluation Methods 
11.  What are Bayes Nets? 

Topic 3.  Bayesian Classifiers 

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
36 
Bayesian Classification 
  Input feature vectors 
  Assign the pattern represented by feature vector x 
to the most probable of the available classes 
 
 
 
 
That is, 
x = x1,x2,...,xl
!"
#$
T
!1,! 2,...,! M
x ! " i :P(" i | x) is maximum.
Posterior 

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
37 
  Computation of posterior probabilities 
  Assume known 
  Prior probabilities 
   Likelihoods 
  
P(!1),P(! 2)...,P(!M)
p x |! i
(
),
i = 1,2,…,M
Bayesian Classification 

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
38 Bayes’ Rule for Classification 
p ! i | x
(
) =
p x |! i
(
)p ! i
(
)
p x( )
,
where 
p x( ) =
p x |! i
(
)p ! i
(
)
i=1
M
"

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
39 M=2 Classes 
39 
  Given x classify it according to the rule 
 
  Equivalently:  classify x according to the rule  
 
  For equiprobable classes the test becomes 
If P(!1 x) > P(! 2 x)  " !1
If P(! 2 x) > P(!1 x)  " ! 2
If p x |!1
(
)P !1
(
) > p x |! 2
(
)P ! 2
(
) " !1
If p x |! 2
(
)P ! 2
(
) > p x |!1
(
)P !1
(
) " ! 2
If p x |!1
(
) > p x |! 2
(
) " !1
If p x |! 2
(
) > p x |!1
(
) " ! 2

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
40 Example: Equiprobable Classes 
 
)
(
)
(
2
2
1
1
ω
ω
→
→
R
R
 
and
 

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
41 Example:  Equiprobable Classes 
41 
  Probability of error 
  The black and red shaded areas represent 
  Thus 
 
  Bayesian classifier is OPTIMAL:  it 
minimizes the classification error 
probability 
Pe ! P(error)
= P !2
(
)P error|!2
(
) + P !1
(
)P error|!1
(
)
= 1
2
p(x !2)dx +
"#
x0
$
1
2
p(x !1)dx
x0
+#
$
P error | !2
(
) =
p(x !2)dx
"#
x0
$
P error | !1
(
) =
p(x !1)dx
x0
"
#
and 

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
42 Example:  Equiprobable Classes 
  To see this, observe that 
shifting the threshold 
increases the error rate 
for one class of patterns 
more than it decreases 
the error rate for the 
other class. 

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
43 
  In general, for M classes and unequal priors, the decision rule 
 
 
minimizes the expected error rate. 
The General Case 
43 
P(! i | x) > P(! j | x)  "j # i
$ ! i

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
44 Types of Error 
  Minimizing the expected error rate is a pretty 
reasonable goal. 
  However, it is not always the best thing to do. 
  Example:   
  You are designing a pedestrian detection algorithm for an 
autonomous navigation system. 
  Your algorithm must decide whether there is a pedestrian 
crossing the street. 
  There are two possible types of error: 
  False positive:  there is no pedestrian, but the system thinks there 
is. 
  Miss:  there is a pedestrian, but the system thinks there is not. 
  Should you give equal weight to these 2 types of error? 

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
45 Bayesian Decision Theory:  Topics 
1. 
Probability 
2. 
The Univariate Normal Distribution 
3. 
Bayesian Classifiers 
4. 
Minimizing Risk 
5. 
The Multivariate Normal Distribution 
6. 
Decision Boundaries in Higher Dimensions 
7. 
Parameter Estimation 
8. 
Mixture Models and EM 
9. 
Nonparametric Density Estimation 
10.  Training and Evaluation Methods 
11.  What are Bayes Nets? 

Topic 4.  Minimizing Risk 

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
47 The Loss Matrix 
  To deal with this problem, instead of minimizing error 
rate, we minimize something called the risk. 
  First, we define the loss matrix L, which quantifies the 
cost of making each type of error. 
  Element λij of the loss matrix specifies the cost of 
deciding class j when in fact the input is of class i. 
  Typically, we set λii=0 for all i. 
  Thus a typical loss matrix for the M = 2 case would have 
the form 
L =
0
!12
!21
0
"
#
$
$
%
&
'
'

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
48 Risk 
  Given a loss function, we can now define the risk 
associated with each class k as: 
  where Ri is the region of the input space where we 
will decide ωi. 
rk =
!ki
p x |"k
(
)dx
Ri#
i=1
M
$
Probability we will decide Class ! i  given pattern from Class !k

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
49 Minimizing Risk 
  Now the goal is to minimize the expected risk r, 
given by 
r =
rkP !k
(
)
k=1
M
"

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
50 Minimizing Risk 
  We need to select the decision regions Ri to minimize the risk r. 
  Note that the set of Ri are disjoint and exhaustive. 
  Thus we can minimize the risk by ensuring that each input x 
falls in the region Ri that minimizes the expected loss for that 
particular input, i.e., 
r =
rkP !k
(
)
k=1
M
"
rk =
!ki
p x |"k
(
)dx
Ri#
i=1
M
$
Letting li =
!kip x | "k
(
)P "k
(
)
k=1
M
#
,
we select the partioning regions such that
x $Ri if  li < lj   %j & i
where 

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
51 Example:  M=2 
  For the 2-class case: 
   and 
 
  Thus we assign x to ω1 if 
 
  i.e., if  
l1 = !11p x | "1
(
)P "1
(
) + !21p x | "2
(
)P "2
(
)
l2 = !12p x | "1
(
)P "1
(
) + !22p x | "2
(
)P "2
(
)
!21 " !22
(
) p x | #2
(
)P #2
(
) < !12 " !11
(
) p x | #1
(
)P #1
(
)
p x | !1
(
)
p x | !2
(
)
>
P !2
(
) "21 # "22
(
)
P !1
(
) "12 # "11
(
)
.
Likelihood Ratio Test 

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
52 Likelihood Ratio Test 
  Typically, the loss for a correct decision is 0.  Thus the likelihood 
ratio test becomes 
  In the case of equal priors and equal loss functions, the test 
reduces to 
p x | !1
(
)
p x | !2
(
)
>
P !2
(
) "21 # "22
(
)
P !1
(
) "12 # "11
(
)
.
p x | !1
(
)
p x | !2
(
)
>
P !2
(
) "21
P !1
(
) "12
.
p x | !1
(
)
p x | !2
(
)
> 1.
? 
? 
? 

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
53 Example 
  Consider a one-dimensional input space, with features 
generated by normal distributions with identical variance: 
 
 
    where 
  Let’s assume equiprobable classes, and higher loss for errors on 
Class 2, specifically: 
p(x !1) ! N µ1," 2
(
)
p(x !2) ! N µ2," 2
(
)
µ1 = 0, µ2 = 1, and ! 2 = 1
2
!21 = 1,  !12 = 1
2
.

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
54 Results 
  The threshold has shifted to the left – why? 

End of Lecture 3 

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
56 Bayesian Decision Theory:  Topics 
1. 
Probability 
2. 
The Univariate Normal Distribution 
3. 
Bayesian Classifiers 
4. 
Minimizing Risk 
5. 
The Multivariate Normal Distribution 
6. 
Decision Boundaries in Higher Dimensions 
7. 
Parameter Estimation 
8. 
Mixture Models and EM 
9. 
Nonparametric Density Estimation 
10.  Training and Evaluation Methods 
11.  What are Bayes Nets? 

Topic 5  The Multivariate Normal Distribution 

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
58 The Multivariate Gaussian 
MATLAB Statistics Toolbox Function:   
mvnpdf(x,mu,sigma) 

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
59 Geometry of the Multivariate Gaussian 
where ! " Mahalanobis distance from µ to x
See Appendix B for a review of 
matrices and eigenvectors. 
where (u i,!i) are the ith eigenvector and eigenvalue of ".
Eigenvector equation:  !ui = "iui
 Note that ! real and symmetric "  #i real.
MATLAB Statistics Toolbox Function:   
mahal(x,y) 

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
60 Geometry of the Multivariate Gaussian 
! = Mahalanobis distance from µ to x
where (u i,!i) are the ith eigenvector and eigenvalue of ".
 or y = U(x - µ)

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
61 Moments of the Multivariate Gaussian  
thanks to anti-symmetry of z  

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
62 Moments of the Multivariate Gaussian  

5.1 Application:  Face Detection 

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
64 
Model # 1: Gaussian, uniform covariance 
Pixel 1 
Pixel 2 
m face 
m non-face 
Fit model using maximum likelihood criterion 
s  Face 
59.1 
s  non-face 
69.1 
Face ‘template’ 

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
65 Model 1 Results 
0
0.2
0.4
0.6
0.8
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Pr(Hit) 
Pr(False Alarm) 
Results based on 200 cropped faces and 200 non-faces from 
the same database.  
How does this work with a 
real image? 

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
66 
Model # 2: Gaussian, diagonal covariance 
Pixel 1 
Pixel 2 
m face 
m non-face 
Fit model using maximum likelihood criterion 
s  Face 
 
s  non-face 

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
67 
0
0.2
0.4
0.6
0.8
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Model 2 Results 
Pr(Hit) 
Pr(False Alarm) 
Results based on 200 cropped faces and 200 non-faces from 
the same database.  
Diagonal 
Uniform 
More sophisticated 
model unsurprisingly 
classifies new faces 
and non-faces better. 

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
68 
Model # 3: Gaussian, full covariance 
Pixel 1 
Pixel 2 
Fit model using maximum 
likelihood criterion 
PROBLEM:  we cannot fit this model.  We 
don’t have enough data to estimate the full 
covariance matrix. 
 
N=800 training images 
D=10800 dimensions 
 
Total number of measured numbers =  
ND = 800x10,800 = 8,640,000   
 
Total number of parameters in cov matrix = 
(D+1)D/2  = (10,800+1)x10,800/2 = 
58,325,400  

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
69 Transformed Densities Revisited 
Observations falling within x + !x
(
)  tranform to the range y + !y
(
)
! px (x) "x = py (y) "y
! py (y) ! px (x) "x
"y
Note that in general, !y " !x.
Rather, !y
!x
" dy
dx
 as !x " 0.
Thus py (y) = px (x) dx
dy

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
70 Problems for this week 
  Problems 2.7 – 2.17, 2.19 – 2.21, 2.23 – 2.27 are 
all good! 
 At least do problem 2.14.  We will talk about this 
Monday. (Hopefully one of you will present a solution!) 
  Also, MATLAB exercises up to 1.4.4 are good. 
 At least do Exercise 1.4.2.  We will talk about this next 
week. 

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
71 Bayesian Decision Theory:  Topics 
1. 
Probability 
2. 
The Univariate Normal Distribution 
3. 
Bayesian Classifiers 
4. 
Minimizing Risk 
5. 
The Multivariate Normal Distribution 
6. 
Decision Boundaries in Higher Dimensions 
7. 
Parameter Estimation 
8. 
Mixture Models and EM 
9. 
Nonparametric Density Estimation 
10.  Training and Evaluation Methods 
11.  What are Bayes Nets? 

Topic 6.   
Decision Boundaries in Higher Dimensions 

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
73 
73 
Decision Surfaces 
  If decision regions Ri and Rj 
are contiguous, deﬁne!
  Then the decision surface !
separates the two decision 
regions.  g(x) is positive on 
one side and negative on the 
other.!
!
Ri:  P !i | x
(
) > P ! j | x
(
)
Rj :   P ! j | x
(
) > P !i | x
(
)
g(x) ! P("i | x) # P(" j | x)
+ 
 - 
g(x) = 0
g(x) = 0

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
74 Discriminant Functions 
74 
  If f (.) monotonic, the rule remains the same if we use: 
 
   
 
            is a discriminant function 
  In general, discriminant functions can be defined in other ways, 
independent of Bayes.   
  In theory this will lead to a suboptimal solution 
  However, non-Bayesian classifiers can have significant advantages: 
  Often a full Bayesian treatment is intractable or computationally prohibitive. 
  Approximations made in a Bayesian treatment may lead to errors avoided 
by non-Bayesian methods. 
 
x ! "i  if:  f (P("i x )) > f (P(" j x))  # i $ j
gi (x) ! f (P("i | x))

End of Lecture 4 

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
76 
Multivariate Normal Likelihoods 
76 
  Multivariate Gaussian pdf 
called the covariance matrix 
p(x !i ) =
1
(2")
!
2 #i
1
2
exp $ 1
2
(x $ µi )% #i
$1(x $ µi )
&
'(
)
*+
µi = E x
,- ./
#i = E (x $ µi )(x $ µi )%
,-
./  

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
77 Logarithmic Discriminant Function 
77 
        is monotonic.  Define: 
ln(!)
gi (x) = ln p x | !i
(
)P !i
(
)
(
) = ln p x | !i
(
) + lnP(!i )
= ! 1
2
(x ! µi )T "i
!1(x ! µi ) + lnP(#i ) + Ci
where
Ci = ! !
2
ln2$ ! 1
2
ln "i
  
p(x !i ) =
1
(2")
!
2 #i
1
2
exp $ 1
2
(x $ µi )% #i
$1(x $ µi )
&
'(
)
*+

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
78 
−10
−5
0
5
10
−10
−5
0
5
10
0
0.1
0.2
0.3
0.4
0.5
Quadratic Classifiers 
  Thus the decision surface has a quadratic form. 
  For a 2D input space, the decision curves are 
quadrics (ellipses, parabolas, hyperbolas or, in 
degenerate cases, lines). 
gi (x) = ! 1
2
(x ! µi )T "i
!1(x ! µi ) + lnP(#i ) + Ci

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
79 Example:  Isotropic Likelihoods 
79 
 
Suppose that the two likelihoods are both isotropic, but with different means and 
variances.  Then 
 
 
And  
 
 will be a quadratic equation in 2 variables. 
gi (x) = !
1
2" i
2 (x1
2 + x2
2) + 1
" i
2 (µi 1x1 + µi2x2) !
1
2" i
2 (µi 1
2 + µi2
2 ) + ln P #i
(
)
(
) + Ci
gi (x) ! gj (x) = 0
gi (x) = ! 1
2
(x ! µi )T "i
!1(x ! µi ) + lnP(#i ) + Ci

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
80 Equal Covariances 
  The quadratic term of the decision boundary is 
given by 
  Thus if the covariance matrices of the two 
likelihoods are identical, the decision boundary is 
linear. 
gi (x) = ! 1
2
(x ! µi )T "i
!1(x ! µi ) + lnP(#i ) + Ci
1
2
xT !j
"1 " !i
"1
(
)x

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
81 Linear Classifier 
  In this case, we can drop the quadratic terms and express the 
discriminant function in linear form: 
gi (x) = ! 1
2
(x ! µi )T "!1(x ! µi ) + lnP(#i ) + Ci
gi (x) = w i
T x + wio
w i = !"1µi
wi 0 = lnP(#i ) " 1
2
µ
T
i!"1µi

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
82 Example 1: Isotropic, Identical Variance 
! = " 2I.  Then
w
T (x # x o) = 0,  where
w = µi # µj ,  and
x o = 1
2
(µi + µj ) # " 2 ln P($i )
P($ j )
µi # µj
µi # µj
2
 
gi (x) = w i
T x + wio
w i = !"1µi
wi 0 = lnP(#i ) " 1
2
µ
T
i!"1µi
Decision  
Boundary 

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
83 Example 2: Equal Covariance 
gi (x) = w i
T x + wio
w i = !"1µi
wi 0 = lnP(#i ) " 1
2
µ
T
i!"1µi
 gij (x) = w
T (x ! x 0) = 0
 w = !"1(µi " µj ),
 
x 0 = 1
2
(µi + µj ) ! ln P("i )
P(" j )
#
$
%
&
'
(
µi ! µj
µi ! µj
2
)!1
,
and
x
)!1 * (x
T )!1x)
1
2
where 

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
84 Minimum Distance Classifiers 
  If the two likelihoods have identical covariance AND 
the two classes are equiprobable, the discrimination 
function simplifies: 
gi (x) = ! 1
2
(x ! µi )T "i
!1(x ! µi ) + lnP(#i ) + Ci
gi (x) = ! 1
2
(x ! µi )T "!1(x ! µi )

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
85 Isotropic Case 
  In the isotropic case, 
  Thus the Bayesian classifier simply assigns the class 
that minimizes the Euclidean distance de between the 
observed feature vector and the class mean. 
gi (x) = ! 1
2
(x ! µi )T "!1(x ! µi ) = !
1
2# 2 x ! µi
2
de = x ! µi

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
86 General Case:  Mahalanobis Distance 
  To deal with anisotropic distributions, we simply classify according 
to the Mahalanobis distance, deﬁned as!
  Since the covariance matrix is symmetric, it can be represented as!
!
    where!
    and where!
!
!
  Then we have   !
dm = gi (x) = (x ! µi )T "!1(x ! µi )
(
)
1/2
! = "#"T
the columns vi  of ! are the eigenvectors of "
! is a diagonal matrix whose diagonal elements "i  
are the corresponding eigenvalues.
d 2
m = (x ! µi )T "T #!1"(x ! µi )

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
87 General Case:  Mahalanobis Distance 
d 2
m = (x ! µi )T "T #!1"(x ! µi )
Let !
x = "Tx.  Then the coordinates of !
x  are the projections of x
onto the eigenvectors of #, and we have:
!
x1 "
!
µi 1
(
)
2
#1
+ ! +
!
xl "
!
µil
(
)
2
#l
= dm
2
Thus the curves of constant 
Mahalanobis distance c have ellipsoidal form.

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
88 
Given !1, !2 :   P(!1) = P(!2) and p(x !1) = N(µ1,  "), p(x !2) = N(µ2,  "),
µ1 =
0
0
#
$
%
&
'
( ,   µ2 =
3
3
#
$%
&
'( ,   " =
1.1
0.3
0.3
1.9
#
$
%
&
'
(
classify the vector
x =
1.0
2.2
#
$%
&
'(
using Bayesian classification: 
•  !-1 =
0.95
"0.15
"0.15
0.55
#
$
%
&
'
(
•  Compute Mahalanobis dm  from µ1, µ2 :
d 2
m,1 =
1.0,
2.2
!"
#$ %&1
1.0
2.2
!
"
'
#
$
( = 2.952, d 2
m,2 =
&2.0,
&0.8
!"
#$ %&1
&2.0
&0.8
!
"
'
#
$
( = 3.672
•   Classify  x ! "1.  Observe that dE ,2 < dE ,1
Example: 

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
89 Bayesian Decision Theory:  Topics 
1. 
Probability 
2. 
The Univariate Normal Distribution 
3. 
Bayesian Classifiers 
4. 
Minimizing Risk 
5. 
The Multivariate Normal Distribution 
6. 
Decision Boundaries in Higher Dimensions 
7. 
Parameter Estimation 
8. 
Mixture Models and EM 
9. 
Nonparametric Density Estimation 
10.  Training and Evaluation Methods 
11.  What are Bayes Nets? 

Topic 7. Parameter Estimation 

Topic 7.1 Maximum Likelihood Estimation 

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
92 Maximum Likelihood Parameter Estimation 
92 
 
Suppose we believe input vectors x are distributed as
p(x) ! p(x;"),  where " is an unknown parameter.
Given independent training input vectors X = x 1,x2,...x N
{
}
we want to compute the maximum likelihood estimate " ML for ".
Since the input vectors are independent, we have
p(X;") ! p(x 1,x2,...x N;") = #
k=1
N
p(x k;")

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
93 Maximum Likelihood Parameter Estimation 
93 
p(X;!) = "
k=1
N
p(x k;!)
Let L(!) " ln p(X;!) = #
k=1
N
ln p(x k;!)
The general method is to take the derivative of L
with respect to !,  set it to 0 and solve for ! :
ˆ! ML :   $L(!)
$(!)
=
$ln p(x k;!)
$(!)
k=1
N
%
= 0

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
94 
Properties of the Maximum Likelihood Estimator 
94 
Let ! 0 be the true value of the unknown parameter vector.
Then
! ML is asymptotically unbiased: lim
  N"#E[! ML] = ! 0
! ML is asymptotically consistent: lim
N"$E ˆ! ML % ! 0
2
= 0

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
95 Example: Univariate Normal 
Likelihood func3on 

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
96 Example:  Univariate Normal 

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
97 
Example:  Univariate Normal 
Thus ! ML is biased (although asymptotically unbiased).

End of Lecture 5 

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
99 Example:  Multivariate Normal 
  Given i.i.d. data                             , the log likeli-
hood function is given by 

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
100 Maximum Likelihood for the Gaussian  
  Set the derivative of  the log likelihood function to zero, 
  and solve to obtain 
  One can also show that 
Recall:  If x and a are vectors, then !
!x xta
(
) = !
!x a tx
(
) = a
"
#$
%
&'

Topic 7.1 Bayesian Parameter Estimation 

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
102 
  Assume     is known. Given i.i.d. data 
                           , the likelihood function for 
   is given by 
  This has a Gaussian shape as a function of   (but it 
is not a distribution over  ). 
! 2
µ
µ
µ
Bayesian Inference for the Gaussian (Univariate Case) 

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
103 
Bayesian Inference for the Gaussian (Univariate Case) 
  Combined with a Gaussian prior over   , 
  this gives the posterior 
  Completing the square over   , we see that 
µ
µ

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
104 Bayesian Inference for the Gaussian 
  … where 
  Note: 
Shortcut:  p µ | X
(
) has the form C exp !"2
(
).
Get "2 in form aµ2 ! 2bµ +c = a(µ !b / a)2 + const and identify
µN = b / a
1
# N
2 = a

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
105 Bayesian Inference for the Gaussian 
  Example: 
µ0 = 0
µ = 0.8
! 2 = 0.1

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
106 Maximum a Posteriori (MAP) Estimation 
 
In MAP estimation, we use the value of µ that maximizes
the posterior p µ | X
(
) :
µMAP = µN.

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
107 Full Bayesian Parameter Estimation 
  In both ML and MAP, we use the training data X to 
estimate a specific value for the unknown parameter 
vector θ, and then use that value for subsequent 
inference on new observations x:   
  These methods are suboptimal, because in fact we 
are always uncertain about the exact value of θ, 
and to be optimal we should take into account the 
possibility that θ assumes other values. 
p x | !
(
)

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
108 Full Bayesian Parameter Estimation 
  In full Bayesian parameter estimation, we do not 
estimate a specific value for θ. 
  Instead, we compute the posterior over θ, and then 
integrate it out when computing
 
 : 
p x | X
(
)
p(x X ) =
p(x !)p(! X )d!
"
p(! X ) =
p(X !)p(!)
p(X )
=
p(X !)p(!)
p(X !)p(!)d!
"
p(X !) = #
k=1
N
p(x k !)

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
109 
Example:  Univariate Normal with Unknown Mean 
Consider again the case p(x µ) !N µ,!
(
)where !  is known and µ ! N µ0,! 0
(
)
We showed that p µ|X
(
) ! N µN,! N
2
(
), where 
In the MAP approach, we approximate p(x X ) ! N µN ,! 2
(
)
In the full Bayesian approach, we calculate p(x X ) =
p(x | µ)p(µ X ) d µ
!
which can be shown to yield p(x X ) ! N µN ,! 2 + ! N
2
(
)

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
110 Hints for Exercise 1.4.2 
  Here are some MATLAB functions you may find useful in solving 
Exercise 1.4.2 
  mnrnd 
  mvnrnd 
  mvnpdf 
  mean 
  cov 
  squeeze 
  sum 
  repmat 
  inv 
  min 
  max 
  zeros 
  ones 

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
111 Problem 1.4.2 
function pe=pr142 
%Exercise 1.4.2 from PR Matlab Manual 
  
m(:,1)=[0 0 0]'; 
m(:,2)=[1 2 2]'; 
m(:,3)=[3 3 4]'; 
S=[0.8 0.2 0.1;0.2 0.8 0.2; 0.1 0.2 0.8]; 
N=1000; 
  
%Part 1 
ntrain=mnrnd(N,ones(3,1)/3); %Number of training pts generated by each 
class 
ntest=mnrnd(N,ones(3,1)/3); %Number of test pts generated by each class 
test=[]; 
mml=zeros(3,3); 
Smli=zeros(3,3,3); 
for i=1:3 
    train=mvnrnd(m(:,i),S,ntrain(i)); %training vectors from class i 
    test=[test;mvnrnd(m(:,i),S,ntest(i))]; %test vectors from class i 
     
    mml(i,:)=mean(train); %ML estimate of mean for class i 
    Smli(i,:,:)=ntest(i)*cov(train,1);%weighted ML estimate of 
covariance for class i 
end 
  
Sml=squeeze(mean(Smli)/N); %ML estimate of common covariance 
  
%Part 2:  Euclidean distance 
for i=1:3 
    dsq(:,i)=sum((test-repmat(mml(i,:),N,1)).^2,2); 
end 
[m,idx(:,1)]=min(dsq'); 
  
%Part 3: Mahalanobis distance 
for i=1:3 
    y=test-repmat(mml(i,:),N,1); 
    dsq(:,i)=sum((y*inv(Sml)).*y,2); 
end 
[m,idx(:,2)]=min(dsq'); 
  
%Part 4:  Maximum likelihood classifier 
for i=1:3 
    p(:,i)=mvnpdf(test,mml(i,:),Sml); 
end 
[m,idx(:,3)]=max(p'); 
  
%Ground truth classes 
idxgt=[ones(ntest(1),1);2*ones(ntest(2),1);3*ones(ntest(3),1)]; 
  
for i=1:3 
    pe(i)=mean(idx(:,i)~=idxgt); %Error rate for class i 
end 
!

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
112 Bayesian Decision Theory:  Topics 
1. 
Probability 
2. 
The Univariate Normal Distribution 
3. 
Bayesian Classifiers 
4. 
Minimizing Risk 
5. 
The Multivariate Normal Distribution 
6. 
Decision Boundaries in Higher Dimensions 
7. 
Parameter Estimation 
8. 
Mixture Models and EM 
9. 
Nonparametric Density Estimation 
10.  Training and Evaluation Methods 
11.  What are Bayes Nets? 

Topic 8 Mixture Models and EM 

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
114 Motivation 
  What do we do if a distribution is not well-
approximated by a standard parametric model? 

8.1 Intuition 

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
116 Mixtures of Gaussians 
  Combine simple models  
into a complex model: 
Component 
Mixing coefficient 
K=3 

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
117 Mixtures of Gaussians 

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
118 Mixtures of Gaussians 
  Determining parameters 
 
 using maximum 
log likelihood 
  Solution: use standard, iterative, numeric 
optimization methods or the expectation 
maximization algorithm.  
Log of a sum; no closed form maximum. 
µ, !  and "

End of Lecture 6 

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
120 Hidden Variable Interpretation 
x
j = 1
j = 2
j = 3
x
j = 1
j = 2
j = 3
j
j
p x | P1,…PJ , µ1,…µJ ,! 1,…! J
(
) =
PjN x;µj ,! j
2
(
)
j =1
J
"
=
p(j)p x | j
(
)
j =1
J
"

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
121 Hidden Variable Interpretation 
ASSUMPTIONS!
!
•  for each training datum xi there is a hidden variable ji.!
•  ji represents which Gaussian xi came from!
•  hence ji takes discrete values!
OUR GOAL: 
THING TO NOTICE:!
!
If we knew the hidden variables ji for the training data it would be easy to 
estimate parameters   – just estimate individual Gaussians separately.!
To estimate the parameters !:
The means µ j
The covariances " j
The weights (mixing coefficients) Pj
for all J components of the model.
!
x
j = 1
j = 2
j = 3
j
p x | P1,…PJ , µ1,…µJ ,! 1,…! J
(
) =
PjN x;µj ,! j
2
(
)
j =1
J
"
=
p(j)p x | j
(
)
j =1
J
"

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
122 
x
j = 1
j = 2
j = 3
j
Hidden Variable Interpretation 
THING TO NOTICE #2:!
!
If we knew the parameters     it would very easy to estimate the posterior 
distribution over each hidden variable ji using Bayes’ rule:!
j=1!
j=2!
j=3!
Pr(j|x)!
!
p j | x,!
(
) =
p x | j,!
(
)Pj
p x | j,!
(
)Pj
j =1
J
"

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
123 Expectation Maximization 
Chicken and egg problem:   
 
• 
could ﬁnd j1...N  if we knew !
• 
could ﬁnd   if we knew j1...N!
Solution:  Expectation Maximization (EM) algorithm 
 
 
 
 
 (Dempster, Laird and Rubin 1977) 
EM for Gaussian mixtures can be viewed as alternation between 2 steps: 
  
1. Expectation Step (E-Step) 
  
•  For ﬁxed    ﬁnd posterior distribution over responsibilities j1...N!
 
2. Maximization Step (M-Step) 
 
•  Now use these posterior probabilities to re-estimate!
!
!
!
!

8.2 Math 

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
125 Mixture Model 
Let 
xk,k = 1,…N  denote the training input observations, assumed to be independent
jk ![1,…J ] indicate the component of the mixture from which the observation was drawn
(Note that xk  is observable but jk  is hidden.)
Let 
!t = "t , Pt
(
)  represent the unknown parameters we are trying to estimate, where
" represents the vector of coefficients for the component distributions and
P represents the mixing coefficients.
Our mixture model is p xk | !
(
) =
Pjkp xk | jk;!
(
)
j =1
J
"

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
126 Q Function 
We will iteratively estimate !,  starting with an initial guess !(0) 
and monotonically improving our estimate !(t) at successive time steps t.
 
For this purpose, we define a Q function
Q !;!(t)
(
) = E
Pjk log p xk | jk;"
(
)
k=1
N
#
$
%
&
'
(
)
 
The Q function represents the expected log likelihood of the training data,
given our most recent estimate of the parameters !(t), where the expectation
is taken over the possible values of the hidden labels jk.

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
127 Expectation Step 
  In the E-Step, we calculate the (expected) log 
probability over the possible parameter values: 
Q !;!(t)
(
) = E
Pjk log p xk | jk;"
(
)
k=1
N
#
$
%
&
'
(
)
=
E Pjk log p xk | jk;"
(
)
$
%&
'
()
k=1
N
#
=
P jk | xk;!
(
)Pjk log p xk | jk;"
(
)
jk =1
J
#
k=1
N
#

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
128 Maximization Step 
  In the M-Step, we select for our new parameter 
estimate the value that maximizes this expected log 
probability: 
!(t + 1) = argmax
!
Q !;!(t)
(
)

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
129 Example:  Mixture of Isotropic Gaussians 
  E-Step: 
p xk | j;!
(
) =
1
2"# 2
j
(
)
!
2
exp $
xk $ µj
2
2# j
2
%
&
'
''
(
)
*
**
Q !;!(t)
(
) =
p j | xk;!
(
) " l
2
log# j
2 "
1
2# j
2 xk " µj
2
+ logPj
$
%
&&
'
(
))
j =1
J
*
k=1
N
*

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
130 Example:  Mixture of Isotropic Gaussians 
  Parameter Update Equations: 
µj (t + 1) =
P j | xk;!(t)
(
)xk
k=1
N
"
P j | xk;!(t)
(
)
k=1
N
"
! j
2(t + 1) =
P j | xk;"(t)
(
) xk # µj (t + 1)
2
k=1
N
$
l
P j | xk;"(t)
(
)
k=1
N
$
Pj (t + 1) = 1
N
P j | xk;!(t)
(
)
k=1
N
"
P j | xk;!(t)
(
) =
p xk | j;"(t)
(
)Pj (t)
p xk | j;"(t)
(
)Pj (t)
j =1
J
#
  Responsibilities Update Equation: 
(e)
L = 5
!2
0
2
!2
0
2

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
131 Univariate Gaussian Mixture Example 

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
132 2-Component Bivariate MATLAB Example 
0
20
40
60
80
100
0
20
40
60
80
100
Assignment grade (%)
Exam grade (%)
CSE 2011Z 2010W

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
133 2-Component Bivariate MATLAB Example 
%update responsibilities!
    for i=1:k!
        p(:,i)=alphas(i).*mvnpdf(x,mu(i,:),squeeze(S(i,:,:)));!
    end!
    p=p./repmat((sum(p,2)),1,k);!
!
%update parameters!
    for i=1:k!
        Nk=sum(p(:,i));!
        mu(i,:)=p(:,i)'*x/Nk;!
        dev=x-repmat(mu(i,:),N,1);!
        S(i,:,:)=(repmat(p(:,i),1,D).*dev)'*dev/Nk;!
        alphas(i)=Nk/N;!
    end!
!
0
20
40
60
80
100
0
20
40
60
80
100
Assignment grade (%)
Exam grade (%)
CSE 2011Z 2010W

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
134 Bivariate Gaussian Mixture Example 
(a)
0
0.5
1
0
0.5
1
(b)
0
0.5
1
0
0.5
1
(c)
0
0.5
1
0
0.5
1
Samples from p xk, jk | !
(
)
Samples from p xk | !
(
)
Responsibilities P jk | xk;!(t)
(
)

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
135 Expectation Maximization 
  EM is guaranteed to monotonically increase the 
likelihood. 
  However, since in general the likelihood is non-
convex, we are not guaranteed to find the globally 
optimal parameters. 

8.3 Applications 

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
137 Old Faithful Example 
(a)
!2
0
2
!2
0
2
(b)
!2
0
2
!2
0
2
(c)
L = 1
!2
0
2
!2
0
2
(d)
L = 2
!2
0
2
!2
0
2
(e)
L = 5
!2
0
2
!2
0
2
(f)
L = 20
!2
0
2
!2
0
2
Time to next eruption (min) 
Duration of eruption (min) 

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
138 Face Detection Example: 2 Components 
0.4999 
0.4675 
0.5001 
0.5325 
Face Model 
Parameters 
Non-Face 
Model 
Parameters 
Mean 
Standard 
deviation 
Prior 
Mean 
Standard 
deviation 
Prior 
Each component is still assumed to 
have diagonal covariance. 
 
The face model and non-face 
model have divided the data into 
two clusters.  In each case, these 
clusters have roughly equal 
weights.   
 
The primary thing that these seem 
to have captured is the 
photometric  (luminance) variation.   
 
Note that the standard deviations 
have become smaller than for the 
single Gaussian model as any 
given data point  is likely to be 
close to one mean or the other. 

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
139 Results for MOG 2 Model 
0
0.2
0.4
0.6
0.8
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Pr(Hit) 
Pr(False Alarm) 
MOG 2 
Diagonal 
Uniform 
Performance improves 
relative to a single 
Gaussian model, 
although it is not 
dramatic. 
 
We have a better 
description of the data 
likelihood. 

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
140 MOG 5 Components 
0.0988 
0.1925 
0.2062 
0.2275 
0.1575 
0.1737 
0.2250 
0.1950 
0.2200 
0.1863 
Face Model 
Parameters 
Non-Face 
Model 
Parameters 
Mean 
Standard 
deviation 
Prior 
Mean 
Standard 
deviation 
Prior 

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
141 MOG 10 Components 
0.0075 
0.1425 
0.1437 
0.0988 
0.1038 
0.1187 
0.1638 
0.1175 
0.1038 
0.0000 
0.1137 
0.0688 
0.0763 
0.0800 
0.1338 
0.1063 
0.1263  
0.0900 
0.1063 
0.0988 

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
142 
0
0.2
0.4
0.6
0.8
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Pr(Hit) 
Pr(False Alarm) 
MOG 10 
MOG 2 
Diagonal 
Uniform 
Performance improves 
slightly more, 
particularly at low false 
alarm rates. 
Results for Mog 10 Model 

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
143 Background Subtraction 
GOAL : (i) Learn background model  (ii) use this to segment regions 
where the background has been occluded  
Test Image 
Desired Segmentation 

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
144 What if the scene isn’t static? 
Gaussian is no longer a good 
fit to the data. 
 
Not obvious exactly what 
probability model would fit 
better. 

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
145 Background Mixture Model 

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
146 Background Subtraction Example 



End of Lecture 7 

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
148 Bayesian Decision Theory:  Topics 
1. 
Probability 
2. 
The Univariate Normal Distribution 
3. 
Bayesian Classifiers 
4. 
Minimizing Risk 
5. 
The Multivariate Normal Distribution 
6. 
Decision Boundaries in Higher Dimensions 
7. 
Parameter Estimation 
8. 
Mixture Models and EM 
9. 
Nonparametric Density Estimation 
10.  Training and Evaluation Methods 
11.  What are Bayes Nets? 

9.  Nonparametric Methods 

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
150 Nonparametric Methods 
  Parametric distribution models are restricted to specific 
forms, which may not always be suitable; for example, 
consider modelling a multimodal distribution with a 
single, unimodal model. 
  You can use a mixture model, but then you have to 
decide on the number of components, and hope that 
your parameter estimation algorithm (e.g., EM) 
converges to a global optimum! 
  Nonparametric approaches make few assumptions 
about the overall shape of the distribution being 
modelled, and in some cases may be simpler than using 
a mixture model. 

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
151 Histogramming 
  Histogram methods partition 
the data space into distinct 
bins with widths Δi and count 
the number of observations, ni, 
in each bin. 
• 
Often, the same width is used 
for all bins, Δi = Δ. 
• 
Δ acts as a smoothing 
parameter. 
  In a D-dimensional space, using 
M bins in each dimension will 
require MD bins! 
The curse of dimensionality 

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
152 Kernel Density Estimation 
  Assume observations drawn 
from a density p(x) and 
consider a small region R 
containing x such that 
  The expected number K out 
of N observations that will 
lie inside R is given by 
  If the volume V of R is 
sufficiently small, p(x) is 
approximately constant 
over R and 
  Thus 

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
153 Kernel Density Estimation 
Kernel Density Estimation: fix V, estimate K 
from the data. Let R be a hypercube centred 
on x and define the kernel function (Parzen 
window) 
 
It follows  that  
      and hence 

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
154 Kernel Density Estimation 
To avoid discontinuities in p(x), use a smooth kernel, e.g. a Gaussian 
(Any kernel k(u) such that 
will work.) 
h acts as a smoother. 

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
155 KDE Example 

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
156 Kernel Density Estimation 
  Problem:  if V is fixed, there may be too few points 
in some regions to get an accurate estimate. 

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
157 Nearest Neighbour Density Estimation 
Nearest Neighbour 
Density Estimation: fix K, 
estimate V from the data. 
Consider a hypersphere 
centred on x and let it 
grow to a volume V* that 
includes K of the given N 
data points. Then 
for j=1:np!
     d=sort(abs(x(j)-xi));!
     V=2*d(K(i));!
     phat(j)=K(i)/(N*V);!
end!
 

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
158 Nearest Neighbour Density Estimation 
Nearest Neighbour 
Density Estimation: fix K, 
estimate V from the data. 
Consider a hypersphere 
centred on x and let it 
grow to a volume V* that 
includes K of the given N 
data points. Then 
K=5
 
True distribution
Training data
KNN Estimate
K=10
K=100

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
159 Nearest Neighbour Density Estimation 
  Problem:  does not generate a proper density (for 
example, integral is unbounded on    ) 
  In practice, on finite domains, can normalize. 
  But makes strong assumption on tails  
! D
! 1
x
"
#$
%
&'

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
160 Nonparametric Methods 
  Nonparametric models (not histograms) require 
storing and computing with the entire data set.  
  Parametric models, once fitted, are much more 
efficient in terms of storage and computation. 

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
161 K-Nearest-Neighbours for Classification 
  Given a data set with Nk data points from class Ck 
and                      ,  we have 
  and correspondingly 
  Since                    , Bayes’ theorem gives 

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
162 K-Nearest-Neighbours for Classification 
K = 1 
K = 3 

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
163 
  K acts as a smother 
   As              , the error rate of the 1-nearest-
neighbour classifier is never more than twice the 
optimal error (obtained from the true conditional class 
distributions). 
K-Nearest-Neighbours for Classification  

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
164 KNN Example 

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
165 Naïve Bayes Classifiers 
  All of these nonparametric methods require lots of data 
to work.  If        training points are required for 
accurate estimation in 1 dimension, then 
   points are 
required for D-dimensional input vectors. 
  It may sometimes be possible to assume that the 
individual dimensions of the feature vector are 
conditionally independent.  Then we have 
  This reduces the data requirements to  
p x | !i
(
) =
p x j | !i
(
)
j =1
D
"
O N D
(
)
O DN
(
).
O N( )

End of Lecture 8 

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
167 Bayesian Decision Theory:  Topics 
1. 
Probability 
2. 
The Univariate Normal Distribution 
3. 
Bayesian Classifiers 
4. 
Minimizing Risk 
5. 
The Multivariate Normal Distribution 
6. 
Decision Boundaries in Higher Dimensions 
7. 
Parameter Estimation 
8. 
Mixture Models and EM 
9. 
Nonparametric Density Estimation 
10.  Training and Evaluation Methods 
11.  What are Bayes Nets? 

10.  Training and Evaluation Methods 

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
169 Machine Learning System Design 
  The process of solving a particular classification or 
regression problem typically involves the following 
sequence of steps: 
1. 
Design and code promising candidate systems 
2. 
Train each of the candidate systems (i.e., learn the 
parameters) 
3. 
Evaluate each of the candidate systems 
4. 
Select and deploy the best of these candidate systems 

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
170 Using Your Training Data 
  You will always have a finite amount of data on 
which to train and evaluate your systems. 
  The performance of a classification system is often 
data-limited:  if we only had more data, we could 
make the system better. 
  Thus it is important to use your finite data set wisely. 

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
171 Overfitting 
  Given that learning is often data-limited, it is 
tempting to use all of your data to estimate the 
parameters of your models, and then select the 
model with the lowest error on your training data. 
  Unfortunately, this leads to a notorious problem 
called over-fitting. 

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
172 Example: Polynomial Curve Fitting   

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
173 Sum-of-Squares Error Function 

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
174 
How do we choose M, the order of the model? 

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
175 1st Order Polynomial 

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
176 3rd Order Polynomial 

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
177 9th Order Polynomial 

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
178 Over-fitting 
Root‐Mean‐Square (RMS) Error: 

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
179 Overfitting and Sample Size 
9th Order Polynomial 

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
180 Over-fitting and Sample Size 
9th Order Polynomial 

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
181 Methods for Preventing Over-Fitting 
  Bayesian parameter estimation 
  Application of prior knowledge regarding the probable 
values of unknown parameters can often limit over-fitting of 
a model 
  Model selection criteria 
  Methods exist for comparing models of differing complexity 
(i.e., with different types and numbers of parameters) 
  Bayesian Information Criterion (BIC) 
  Akaike Information Criterion (AIC) 
  Cross-validation 
  This is a very simple method that is universally applicable. 

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
182 Cross-Validation 
  The available data are partitioned into disjoint 
training and test subsets. 
  Parameters are learned on the training sets.   
  Performance of the model is then evaluated on the 
test set. 
  Since the test set is independent of the training set, 
the evaluation is fair:  models that overlearn the 
noise in the training set will perform poorly on the 
test set. 

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
183 Cross-Validation:  Choosing the Partition 
  What is the best way to partition the 
data? 
  A larger training set will lead to more accurate 
parameter estimation. 
  However a small test set will lead to a noisy 
performance score. 
  If you can afford the computation time, repeat 
the training/test cycle on complementary 
partitions and then average the results.  This 
gives you the best of all worlds:  accurate 
parameter estimation and accurate evaluation. 
  In the limit:  the leave-one-out method   

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
184 Bayesian Decision Theory:  Topics 
1. 
Probability 
2. 
The Univariate Normal Distribution 
3. 
Bayesian Classifiers 
4. 
Minimizing Risk 
5. 
The Multivariate Normal Distribution 
6. 
Decision Boundaries in Higher Dimensions 
7. 
Parameter Estimation 
8. 
Mixture Models and EM 
9. 
Nonparametric Density Estimation 
10.  Training and Evaluation Methods 
11.  What are Bayes Nets? 

10.  What are Bayes Nets? 

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
186 
Directed Graphical Models and the Role of Causality 
  Bayes nets are directed acyclic graphs in which each node represents a 
random variable. 
  Arcs signify the existence of direct causal influences between linked 
variables. 
  Strengths of influences are quantified by conditional probabilities 
  NB:  For this to hold it is critical that the graph be acyclic. 
where pak  is the set of 'parent' nodes of node k.

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
187 Bayesian Networks 
  Directed Acyclic Graph (DAG) 
From the definition of conditional probabilities (product rule): 
In general: 
This corresponds to a complete graph. 

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
188 Bayesian Networks 
  However, many systems have sparser causal 
relationships between their variables. 
General Factorization 

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
189 Generative Models of Perception 

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
190 Discrete Variables 
  General joint distribution: K2 -1 parameters 
  Independent joint distribution: 2(K -1) parameters 

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
191 Discrete Variables 
  General distributions require many parameters. 
  General joint distribution over M variables:  
KM -1 parameters 
  It is thus extremely important to identify structure in 
the system that corresponds to a sparser graphical 
model and hence fewer parameters. 

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
192 Binary Variable Example 
19
2 
  S:  Smoker? 
  C:  Cancer? 
  H:  Heart Disease? 
  (H1, H2):  Results of medical 
tests for heart disease 
  (C1, C2):  Results of medical 
tests for cancer 

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
193 Discrete Variables 
  Example:  M -node Markov chain  
 K -1 + (M -1) K(K -1) parameters 

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
194 Using Bayes Nets 
19
4 
  Once a DAG has been constructed, the joint probability can be 
obtained by multiplying the marginal (root nodes) and the 
conditional (non-root nodes) probabilities. 
  Training: Once a topology is given, probabilities are estimated 
via the training data set. There are also methods that learn the 
topology. 
  Probability Inference: This is the most common task that Bayesian 
networks help us to solve efficiently. Given the values of some of 
the variables in the graph, known as evidence, the goal is to 
compute the conditional probabilities for some of the other 
variables, given the evidence. 

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
195 Inference in Bayes Nets 
  In inference, we clamp some of the variables to 
observed values, and then compute the posterior 
over other, unobserved variables. 
  Simple example: 

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
196 Example 
 a) Suppose x has been measured and its value is 1.  What is 
the probability that w is 0? 
 b) Suppose w is measured and its value is 1.  What is the 
probability that x is 0? 
 

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
197 Message Passing 
19
7 
  For a), computation propagates from node x to node w, resulting 
in P(w0|x1) = 0.63. 
  For b), computation propagates in the opposite direction, resulting 
in  P(x0|w1) = 0.4. 
  In general, the required inference information is computed via a 
combined process of “message passing” among the nodes of the 
DAG. 
  Complexity: 
  For singly connected graphs, message passing algorithms amount 
to a complexity  linear in the number of nodes. 

Probability & Bayesian Inference 
J. Elder 
CSE 4404/5327 Introduction to Machine Learning and Pattern Recognition 
198 Bayesian Decision Theory:  Topics 
1. 
Probability 
2. 
The Univariate Normal Distribution 
3. 
Bayesian Classifiers 
4. 
Minimizing Risk 
5. 
The Multivariate Normal Distribution 
6. 
Decision Boundaries in Higher Dimensions 
7. 
Parameter Estimation 
8. 
Mixture Models and EM 
9. 
Nonparametric Density Estimation 
10.  What are Bayes Nets? 

