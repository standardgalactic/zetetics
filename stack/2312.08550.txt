Harmonics of Learning:
Universal Fourier Features
Emerge in Invariant Networks
Giovanni Luca Marchetti
KTH
glma@kth.se
Christopher Hillar
Redwood Center for Theoretical Neuroscience
hillarmath@gmail.com
Danica Kragic
KTH
dani@kth.se
Sophia Sanborn
UC Santa Barbara
sanborn@ucsb.edu
Abstract
In this work, we formally prove that, under certain conditions, if a neural network
is invariant to a finite group then its weights recover the Fourier transform on that
group. This provides a mathematical explanation for the emergence of Fourier
features – a ubiquitous phenomenon in both biological and artificial learning
systems. The results hold even for non-commutative groups, in which case the
Fourier transform encodes all the irreducible unitary group representations. Our
findings have consequences for the problem of symmetry discovery. Specifically,
we demonstrate that the algebraic structure of an unknown group can be recovered
from the weights of a network that is at least approximately invariant within certain
bounds. Overall, this work contributes to a foundation for an algebraic learning
theory of invariant neural network representations.
Figure 1: Emergent Circular Harmonics. Weights learned by a neural network trained for
invariance to planar rotation resemble circular haromonics. Data from [44].
1
Introduction and Related Work
Artificial neural networks trained on natural data exhibit a striking phenomenon: regardless of exact
initialization, dataset, or training objective, models trained on the same data domain frequently
converge to similar learned representations [29]. For example, the early layer weights of diverse
image models tend to converge to Gabor filters and color-contrast detectors [35]. Remarkably,
many of these same features are observed in the visual cortex [20, 16, 49], suggesting a form of
representational universality that transcends biological and artificial substrates. While such findings
arXiv:2312.08550v1  [cs.LG]  13 Dec 2023

are empirically well-established in the mechanistic interpretability literature [41], the field lacks
theoretical explanations.
Spatially localized versions of canonical 2D Fourier basis functions, such as Gabor filters or wavelets,
are perhaps the most frequently observed universal features in image models. They commonly arise in
the early layers of vision models – trained with efficient coding [36, 4], classification [35], temporal
coherence [23], and next-step prediction [11] objectives – as well as in the primary visual cortices of
diverse mammals – including cats [21], monkeys [22], and mice [10]. Non-localized Fourier features
have been observed in networks trained to solve tasks that permit cyclic wraparound – for example,
modular arithmetic [34], more general group compositions [7], or invariance to the group of cyclic
translations [44]. In the domain of spatial navigation, the so-called grid cells of the entorhinal cortex
[33] display periodic firing patterns at different spatial frequencies as they build a map of space.
Their response properties are naturally modeled with the harmonics of the twisted torus [15, 37, 13].
Similar features also emerge in artificial neural networks trained to solve spatial navigation tasks
[2, 8]. The ubiquity of these features across diverse learning systems is both striking and unexplained.
In this work, we provide a mathematical explanation for the emergence of Fourier features in learning
systems such as neural networks. We argue that the mechanism responsible for this emergence is the
downstream invariance of the learner to the action of a group of symmetries (e.g. planar translation
or rotation). Since natural data typically possess symmetries, invariance is a fundamental bias that is
injected both implicitly and sometimes explicitly into learning systems [6]. Motivated by this, we
derive theoretical guarantees for the presence of Fourier features in invariant learners that apply to a
broad class of machine learning models.
Our results rely on the inextricable link between harmonic analysis and group theory [12]. The
standard discrete Fourier transform is a special case of more general Fourier transforms on groups,
which can be defined by replacing the standard basis of harmonics by irreducible unitary group
representations. The latter are equivalent to the familiar definition for cyclic or, more generally,
commutative groups, but are more involved for non-commutative ones. In order to accommodate
both the scenarios, we develop a general theory that applies, in principle, to arbitrary finite groups.
This work represents an attempt to provide mathematical grounding for a general algebraic theory of
representation learning, while addressing the universality hypothesis for neural networks [35, 32]. A
suite of earlier theoretical works [24, 17, 14] established such universality for sparse coding models
[36], deriving the conditions under which a network will recover the original bases that generate data
through sparse linear combinations. In this case, the statistics of the data determine the uniqueness of
the representation. Our findings, on the other hand, are purely algebraic, since they rely exclusively
on the invariance properties of the learner. Given the centrality of invariance to many machine
learning tasks, our theory encompasses a broad class of scenarios and neural network architectures,
while providing a new perspective on classical neuroscience [39, 21]. As such, it sets a foundation
for a learning theory of representations in artificial and biological neural systems, grounded in the
mathematics of symmetry.
1.1
Overview of Results
In this section, we provide a non-technical overview of the theoretical results presented in this work.
Our main result can be summarized as follows.
Informal Theorem 1.1 (Theorem 3.1 and Corollary 3.2). If φ(W, x) is a parametric function of
a certain kind that is invariant in the input variable x to the action of a finite group G, then each
component of its weights W coincides with a harmonic of G up to a linear transformation. In
particular, when the weights are orthonormal, W coincides with the Fourier transform of G up to
linear transformations.
In the above, the term ‘harmonic’ refers to an irreducible unitary representation of G. Indeed, one-
dimensional unitary representations correspond to homomorphisms with the unit circle U(1) ⊆C,
which is reminiscent of the classical definition via the imaginary exponential. However, non-
commutative groups can have higher-dimensional irreducible representations, intuitively meaning
that harmonics are valued in unitary matrices. In this case, the components of W can be interpreted
as capsules in the sense of [43] i.e., neural units processing matrix-valued signals.
We show that the hypothesis on φ in Theorem 1.1 is satisfied by several machine learning models from
the literature. In particular, the theorem applies to the recently-introduced (Bi)Spectral Networks [44],
2

to single fully-connected layers of McCulloch-Pitts neurons, and, to an extent, to traditional deep
networks. Since harmonic analysis is naturally formalized over the complex numbers, we consider
models with complex weights W, which fits into the larger program of complex-valued machine
learning [3, 48, 30].
The group-theoretical Fourier transform encodes the entire group structure of G. Therefore, as a
consequence of Theorem 1.1 the multiplication table of G can be recovered from the weights W
of an invariant parametric function φ – a fact empirically demonstrated in [44]. This addresses the
question of symmetry discovery – an established machine learning problem aiming to recover the
unknown group of symmetries of data with minimal supervision and prior knowledge [40, 46, 9].
Since the multiplication table is a discrete object, it is expected that the invariance constraint on φ
can be loosened while still recovering the group correctly. To this end, we prove the following.
Informal Theorem 1.2 (Theorem 3.6). If φ(W, x) is ‘almost invariant’ to G according to certain
functional bounds and the weights are ‘almost orthonormal’, then the multiplicative table of G can
be recovered from W.
Lastly, we implement a model satisfying the requirements of our theory and demonstrate its symmetry
discovery capabilities. To this end, we train it via contrastive learning on an objective encouraging
invariance and extract the multiplicative table of G from its weights. Our Python implementation is
available at a public repository 1.
2
Mathematical Background
We begin by introducing the fundamental concepts from harmonic analysis and group theory used in
this paper. For a complete treatment, we refer the reader to [12].
2.1
Groups and Actions
A group is an algebraic object whose elements represent abstract symmetries, which can be composed
and inverted.
Definition 1. A group is a set G equipped with a composition map G × G →G denoted by
(g, h) 7→gh, an inversion map G →G denoted by g 7→g−1, and a distinguished identity element
1 ∈G such that for all g, h, k ∈G:
Associativity
Inversion
Identity
g(hk) = (gh)k
g−1g = gg−1 = 1
g1 = 1g = g
A map ρ : G →G′ between groups is called a homomorphism if ρ(gh) = ρ(g)ρ(h) for all g, h ∈G.
Examples of groups include the permutations of a set and the general linear group GL(V ) of invertible
operators over a vector space V , both equipped with the usual composition and inversion of functions.
A further example that will be relevant in this work is the unitary group U(V ) ⊆GL(V ) associated
to a Hilbert space V , consisting of operators U satisfying UU † = I, where † denotes the conjugate
transpose and I is the identity matrix. Groups satisfying gh = hg for all g, h ∈G are deemed
commutative.
The idea of a space X having G as a group of symmetries is abstracted by the notion of group action.
Definition 2. An action by a group G on a set X is a map G × X →X denoted by (g, x) 7→g · x,
satisfying for all g, h ∈G, x ∈X:
Associativity
Identity
g · (h · x) = (gh) · x
1 · x = x
A map φ: X →Z between sets acted upon by G is called equivariant if φ(g · x) = g · φ(x) for all
g ∈G, x ∈X. It is called invariant if moreover G acts trivially on Z or, explicitly, if φ(g ·x) = φ(x).
In general, the following actions can be defined for arbitrary groups: G acts on any set trivially by
g · x = x, and G acts on itself seen as a set via (left) multiplication by g · h = gh. Further examples
are GL(V ) and U(V ) acting on V by evaluating operators.
1https://github.com/sophiaas/spectral-universality
3

2.2
Harmonic Analysis on Groups
Harmonic analysis on groups [42] generalizes standard harmonic analysis. We focus here on finite
groups for simplicity, which are sufficient for practical applications. This avoids technicalities such
as integrability conditions and continuity issues arising for infinite groups. We start by considering
commutative groups and cover non-commutative ones in Section 2.3.
Let G be a finite commutative group of order |G|. Denote by ⟨G⟩= CG the free complex vector
space generated by G. Intuitively, an element x = (xg)g∈G ∈⟨G⟩represents a complex-valued
signal over G. The space ⟨G⟩is endowed with the convolution product,
(x ⋆y)g =
X
h∈G
xhyh−1g,
(1)
and is acted upon by G via g · x = δg ⋆x = (xg−1h)h∈G, where δg is the canonical basis vector.
Definition 3. The dual G∨of G is the set of homomorphisms ρ : G →U(1), where U(1) ⊆C
is the group of unitary complex numbers equipped with multiplication. It is itself a group when
equipped with pointwise composition (ρµ)(g) = ρ(g)µ(g).
A homomorphism ρ ∈G∨intuitively represents a harmonic over G, generalizing the familiar notion
from signal processing. If we endow ⟨G⟩with the canonical scalar product ⟨x, y⟩= P
g∈G xgyg,
then G∨⊆⟨G⟩forms an orthogonal basis with all the norms equal to |G|. The linear base-change is,
by definition, the Fourier transform over ⟨G⟩:
Definition 4. The Fourier transform is the map ⟨G⟩→⟨G∨⟩, x 7→ˆx, defined for ρ ∈G∨as:
ˆxρ = ⟨ρ, x⟩.
(2)
The Fourier transform is a linear isometry or, equivalently, a unitary operator, up to a multiplicative
constant of |G|. Moreover, it exchanges the convolution product ⋆over ⟨G⟩with the Hadamard
product ⊙over ⟨G∨⟩. Definition 4 generalizes the usual discrete Fourier transform in the following
sense. For an integer d > 0, consider the cyclc group Cd with d elements. Concretely G = Z/dZ ≃
Cd is the group of integers modulo d equipped with addition as composition. The dual G∨consists
of homomorphisms of the form:
Z/dZ ∋g 7→e2π√−1gk/d,
(3)
for k ∈{0, · · · , d−1}. Equation 2 specializes then to the familiar definition of the Fourier transform.
2.3
Non-Commutative Harmonic Analysis
So far, we have assumed that G is commutative. In this section we briefly discuss the extension of
Fourier theory to non-commutative groups. This however requires more elaborate theoretical tools,
which we now introduce. To begin with, in order to perform harmonic analysis on general groups it is
necessary to discuss unitary representations. The latter will play the role of matrix-valued harmonics.
Definition 5. A unitary representation of G is an action by G on a finite-dimensional Hilbert space V
via unitary operators or, in other words, a homomorphism ρV : G →U(V ). A unitary representation
is said to be irreducible if V does not contain any non-trivial2 sub-representations.
We denote by Irr(G) the set of all irreducible representations of G up to isomorphism. Moreover, for
a vector space V we denote by End(V ) the space of its linear operators.
Definition 6. The Fourier transform is the map ⟨G⟩→L
ρV ∈Irr(G) End(V ), x 7→ˆx, defined for
ρV ∈Irr(G) as:
ˆxρV =
X
g∈G
ρV (g)†xg ∈End(V ).
(4)
This generalizes Definition 4 since for a commutative group, ρV is irreducible if, and only if,
dim(V ) = 1. Analogously to the commutative setting, the Fourier transform exchanges the convo-
lution product ⋆with the point-wise operator composition, which we still denote by ⊙. Moreover,
2The trivial sub-representations of V are 0 and V .
4

the Fourier transform is a unitary operator up to a multiplicative constant of |G| with respect to the
normalized Hilbert-Schmidt scalar product on End(V ), given by:
⟨A, B⟩= dim(V ) tr(A†B).
(5)
The norm associated to the Hilbert-Schmidt scalar product is the Frobenius norm. The relations
between irreducible unitary representations coming from the unitarity of the Fourier transform are
known as Schur orthogonality relations.
3
Theoretical Results
We now present the primary theoretical contributions of this work. Concretely, we demonstrate that if
certain parametric functions are invariant to a group then their weights must almost coincide with
harmonics, i.e. irreducible unitary group representations. We start by introducing general algebraic
notions and principles, and then proceed to specialize them to machine learning scenarios.
Let G be a finite group, H be a set, and V1, · · · , Vk be complex finite-dimensional Hilbert spaces. In
what follows, we will consider the space,
W = ⟨G⟩⊗
M
i
End(Vi) ≃
M
i
End(Vi)⊕G.
(6)
W is a Hilbert space when endowed with the scalar product given by the product of the canonical
scalar product over ⟨G⟩and the normalized Hilbert-Schmidt scalar products over End(Vi) (see
Equation 5). For W ∈W, we will denote each of its components as Wi = (Wi(g))g ∈End(Vi)⊕G.
Moreover, we will often interpret elements W ∈W as linear maps ⟨G⟩→L
i End(Vi) via W(x) =
P
g∈G W(g)xg for x ∈⟨G⟩, where W(g) = (Wi(g))i. Note that G acts on the left tensor factor
of W while for each i, U(Vi) acts on the right tensor factor of ⟨G⟩⊗End(Vi) by composition of
operators.
Definition 7. We say that a map φ: W →H has unitary symmetries if for W, W ′ ∈W of the same
norm, φ(W) = φ(W ′) implies that for each i there exists a unitary operator Ui ∈U(Vi) such that
Wi = Ui · W ′
i.
In the context of machine learning, H will represent the hypothesis space, consisting of functions
the model can learn. On the other hand, φ will represent the parametrization of such hypotheses,
with its domain W being the space of weights. The component ⟨G⟩of W will be responsible for
parametrizing the input space, while each component End(Vi) will represent a computational unit, i.e.
a complex-valued neuron in the language of neural networks. For commutative groups, we simply
have Vi = C ≃End(Vi). In general, End(Vi) can be thought of as parametrizing matrix-valued
signals, which, as mentioned in Section 1.1, are computed by neural units sometimes referred to as
capsules [43].
The following is an abstract algebraic principle at the core of this work.
Theorem 3.1. Suppose that φ: W →H has unitary symmetries and that for some W ∈W the
following holds:
• φ(g · W) = φ(W) for all g ∈G.
• W, seen as a linear map ⟨G⟩→L
i End(Vi), is surjective.
Then for every i there exist W ′
i ∈End(Vi) and an irreducible unitary representation ρi : G →U(Vi)
such that for all g ∈G,
Wi(g) = W ′
iρi(g)†.
(7)
Proof. Since φ has unitary symmetries and ∥g · W∥= ∥W∥, it follows that for every g ∈G and
every i there exists ρi(g) ∈U(Vi) such that
g · Wi = ρi(g) · Wi.
(8)
In particular, by considering the component with index 1 ∈G on both sides of Equation 8, we see
that Wi(g−1) = Wi(1) ρi(g). We wish to show that ρi is a homomorphism. To this end, for all
g, h ∈G it holds that
ρi(gh) · Wi = (gh) · Wi = g · (ρi(h) · Wi) = ρi(h) · (g · Wi) = (ρ(g)ρ(h)) · Wi.
(9)
5

By the surjectivity hypothesis, the set {Wi(g)}g∈G generates End(Vi) as a vector space. Therefore,
Equation 9 implies that ρi(gh) = ρi(g)ρi(h), as desired. Lastly, note that ρi is irreducible by the
sujectivity assumption. Indeed, a nontrivial linear subspace of Vi fixed by ρi(g) for all g would be
sent by Wi(g) into a fixed proper subspace due to Equation 8. This contradicts the surjectivity of
Wi.
Note that the surjectivity assumption implies the constraint P
i dim(Vi)2 ≤|G|. As a consequence
of the result above, the full Fourier transform arises with an additional orthogonality assumption.
Corollary 3.2. Suppose that φ: W →H has unitary symmetries and that for some W ∈W the
following holds:
• φ(g · W) = φ(W) for all g ∈G.
• W is unitary up to a multiplicative constant, i.e. W †W = |G|I.
Then W is the Fourier transform up to composing each of the components Wi by an operator with
Frobenius norm equal to 1.
Proof. Note that the unitarity assumption above implies the surjectivity assumption from Proposition
3.1. Therefore, it follows that for every i, there exists an irreducible unitary representation ρi :
G →U(Vi) such that Wi(g) = Wi(1) ρi(g)†. We wish to show that if i ̸= j then ρi and ρj are
non-isomorphic representations. If not, the orthogonality assumption implies that
0 =
X
g∈G
Wi(g) ⊗Wj(g) =
X
g∈G

Wi(1) ⊗Wj(1)
  ρi(g)⊤⊗ρj(g)†
=
(10)
= |G| Wi(1) ⊗Wj(1),
(11)
where ⊤denotes the transpose and where the last identity follows from the Schur orthogonality
relations. But then Wi(1) or Wj(1) is vanishing, which contraddicts the unitarity assumption.
Lastly, in order to compute the Frobenius norm of Wi(1), note that
|G| ∥Wi(1)∥2 =
X
g∈G
∥Wi(g) ρi(g)∥2 =
X
g∈G
∥Wi(g)∥2 = |G|,
(12)
from which ∥Wi(1)∥= 1 follows.
Again, the orthogonality assumption implies that V1, · · · , Vk are the ambient Hilbert spaces of all the
irreducible unitary representations of G up to isomorphism, and in particular P
i dim(Vi)2 = |G|.
We now wish to discuss the other crucial assumption of Proposition 3.1 requiring that φ(g · W) =
φ(W) for all g ∈G, which is reminiscent of invariance. However, when H is a space of functions,
we are typically interested in models that are invariant in the input variable rather than the weight
variable. Therefore, we introduce the following condition, aimed at reconciling inputs and weights.
To this end, suppose that H is a set of functions X →Y, where X is a set acted upon by G and Y is a
set. We adhere to the notation φ(W, x) = φ(W)(x), x ∈⟨G⟩, W ∈W, for simplicity.
Definition 8. We say that φ satisfies the adjunction property if
φ(W, g · x) = φ(g−1 · W, x)
(13)
for all x ∈X, g ∈G.
The adjunction property implies that if φ(W, x) is invariant in x, then φ(g · W, x) = φ(W, x) for all
x, g, recovering the assumption of Proposition 3.1.
Remark 3.1. As explained above, the tensor component ⟨G⟩of W typically represents the input space
of a given machine learning model. One can consider the more general scenario when data consist of
complex signals over a finite set S acted upon by G, therefore replacing ⟨G⟩= CG by CS. This is
the case, for example, for data consisting of images acted by the cyclic group via rotations, since the
input space cannot be identified with signals over G. Assuming the action over S is free, meaning
that g · s = s implies g = 1, S can be decomposed into copies of G deemed orbits. Specifically,
there is an equivariant isomorphism S ≃G ⊔· · · ⊔G, which in turn induces a linear isomorphism
6

CS ≃⟨G⟩⊕p, where p is the number of orbits. The results from this section can be extended to this
scenario by applying all the arguments to the copies of ⟨G⟩separately, each of which will serve as a
domain for its set of irreducible unitary representations. When the action is not free, it is necessary to
take into account stabilizers, i.e. g ∈G such that g · s = s for some s ∈S. Roughly speaking, we
expect that the results from this section can be adapted to an extent, obtaining unitary representations
‘up to stabilizers’. However, the precise meaning of the latter has yet to be clarified, and goes beyond
the scope of this work.
3.1
Examples
In this section, we provide examples of machine learning models with unitary symmetries. As
anticipated, in the context of machine learning H and W represent the hypothesis space and the
parameter space, respectively. Indeed, in what follows H will consist of functions of the form
⟨G⟩→Y for some codomain Y, and we will adhere to the notation from Definition 8 accordingly.
All the models considered in this section satisfy the adjunction property.
3.1.1
Spectral Networks
We start by considering Spectral Networks – a class of polynomial machine learning models that
inspired this work—and to which our theory applies naturally. These models were introduced by [44]
in cubic form based on the invariant theory of ⟨G⟩(see Section B in the Appendix for an overview
of the latter). The overall idea behind Spectral Networks is to approximate the n-order polynomial
invariants (deemed spectral invariants) of ⟨G⟩for an unknown group G. Specifically, suppose that
V1, · · · , Vk are the ambient Hilbert spaces of the irreducible unitary representations of G. Given
a multi-index i = (i1, · · · , in) ∈{1, · · · , k}n, The Spectral Network of order n is defined as the
collection of parametric maps φi(W, ·) : ⟨G⟩→End(Vi1 ⊗· · · ⊗Vin):
φi(W, x) = Wi1(x) ⊗· · · ⊗Win(x)

W †
i1 ⊙· · · ⊙W †
in

(x),
(14)
where W = ⊕iWi ∈W = ⟨G⟩⊕i ⊗End(Vi), ⊙denotes the G-wise tensor product of operators,
and x denotes the component-wise conjugate of x. For a commutative G, since Vi = C for all i, the
above expression reduces to:
φi(W, x) = Wi1x · · · Winx Wi1 ⊙· · · ⊙Win x.
(15)
For n = 1 Spectral Networks are deemed Power-Spectral Networks, and for a commutative G they
take the form φi(W, x) = |Wix|2. The latter can be simply interpreted as a linear model followed
by an activation function. Even though the squared absolute value is uncommon as an activation
function in machine learning, it has appeared in models of biological neural networks [1].
For simplicity, we will consider only the Spectral Networks involving a single unitary representation;
that is, we will focus on constant multi-indices i = (i, · · · , i) in Equation 14. To this end, let V be a
finite-dimensional Hilbert space and H be the set of functions ⟨G⟩→End(V ⊗n) for some n ∈N.
We set W = ⟨G⟩⊗End(V ).
Proposition 3.3. Consider the Spectral Network, given by:
φ(W, x) = W(x)⊗n W †⊙n(x).
(16)
Then φ has unitary symmetries.
We refer to the Appendix for a proof.
3.1.2
McCulloch-Pitts Neurons and Deep Networks
While Spectral Networks provide the most direct application of our theory, in this section we discuss
the most common and fundamental neural network primitives in deep learning: the fully-connected
McCulloch-Pitts neuron [31] and the deep neural network. We consider models with complex
coefficients and focus on commutative groups, i.e. all the Hilbert spaces Vi from Definition 7 will be
equal to C.
A McCulloch-Pitts neuron has the form φ(W, x) = σ(Wx), where σ: C →Y is a map playing
the role of an activation function and W ∈W = ⟨G⟩is the weight vector. For σ(z) = |z|2, the
McCulloch-Pitts neuron reduces to a commutative Power-Spectral Network i.e., a Spectral Network
with n = 1. The hypothesis space H consists of functions ⟨G⟩→Y.
7

Proposition 3.4. Consider a map σ: C →Y and let φ(W, x) = σ(Wx). Suppose that 0 ∈C is
isolated in its fiber of σ, i.e. there exists an open subspace O ⊆C such that σ−1(σ(0)) ∩O = {0}.
Then φ has unitary symmetries.
We refer to the Appendix for a proof. The above assumption on σ is satisfied by popular activations
functions from neuroscience and machine learning, such as the sigmoid and the leaky Rectified Linear
Unit (ReLU), applied after taking complex absolute value.
Next, we discuss the case of classical deep networks. We model the latter as φ(W, x) = χ(|W(x)|2),
where W ∈W = ⟨G⟩⊗Ck and | · | denotes the component-wise absolute value. Here, k is the
number of neurons in the first hidden layer of the network, while χ: Rk
≥0 →R is the head of the
network, encompassing all the layers after the first one. Note that since typical neural networks are
real-valued, we combine real and complex models. Namely, the first layer of φ is complex, and its
output is fed into the real head χ by taking squared absolute values.
Differently from Section 3, for the next result we will restrict W to the subspace of ⟨G⟩⊗Ck
consisting of W such that the components Wi are orthonormal. This implies, in particular, the
constrain k ≤|G|. Note that W is closed by the actions of G and U(C). The orthonormality
condition is anyway necessary in order to recover the full Fourier transform, as stated in Corollary
3.2.
Proposition 3.5. Suppose that there is an open subspace O ⊆Rk
≥0 containing 0 where χ is affine
with distinct non-vanishing coefficients i.e., χ(z) = P
i aizi + b for z ∈O with 0 ̸= ai ̸= aj for
i ̸= j. Then φ(W, x) = χ(|W(x)|2) has unitary symmetries.
We refer to the Appendix for a proof. Since typical (real-valued) deep neural networks have piece-wise
linear activations functions such as (leaky) ReLU, they define piece-wise affine maps and therefore
are affine when restricted to appropriate open subspaces. Moreover, the hypothesis on the coefficients
ai in Proposition 3.5 is generic, meaning that it defines an open dense subset of (a1, · · · , ak) ∈Rk.
3.2
Group Recovery
Proposition 3.2 allows us to recover the group structure of G up to isomorphism from the weights
of a map with unitary symmetries. In other words, this enables the recovery of an unknown group
in a data-driven manner from the weights of an invariant machine learning model, addressing the
problem of symmetry discovery discussed in Section 1.1. The procedure was originally suggested
and validated empirically in [44].
To this end, assume that φi satisfies the requirements of Proposition 3.2. Moreover, we introduce the
additional assumption that Wi(1) = I ∈End(Vi) for all i. If that is the case, W coincides exactly
with the Fourier transform by Proposition 3.2. This implies that the multiplication table of G can be
recovered by:
gh = argmin
l∈G
∥W(g) ⊙W(h) −W(l)∥,
(17)
where ⊙denotes the Hadamard product i.e., component-wise operator composition. Note that this
notation has a different meaning here than in Section 3.1.1. Since the W(g)’s are orthogonal, the
only possible values for the norms over which the minimum is performed are 0 and
p
2|G|.
The multiplication table of G is a discrete object, while the weights W ∈W can vary continuously.
Therefore, it is natural to expect that the invariance condition (g · W = W for all g ∈G) can
be relaxed, while still recovering the multiplication table correctly. In what follows, we analyze
relaxations of invariance and give bounds in which the group recovery algorithm holds. We start by
introducing a quantity measuring how close a map is to having unitary symmetries. To this end, we
assume that H is a metric space with distance function ∆: H × H →R≥0.
Definition 9. Given a map φ: W →H, its unitarity defect is defined for δ ∈R>0 as:
ωφ(δ) =
sup
W,W ′∈W
∥W ∥=∥W ′∥
∆(φ(W ),φ(W ′))≤δ
max
i
inf
U∈U(Vi)
∥Wi −U · W ′
i∥,
(18)
Note that φ has unitary symmetry if, and only if, ωφ(0) = 0. The following is our main relaxation
result.
8

Theorem 3.6. Suppose that φ: W →H is a map and fix W ∈W. Denote
L = ∥W †W −|G|I∥∞,
(19)
where ∥· ∥∞is the uniform Frobenius norm for G × G matrices. Suppose that the following holds:
• For all g ∈G:
ωφi (∆(φ(g · W), φ(W))) <
q
1
2 −
L
|G|
p
|G| + L + 1
(20)
• L ≤|G|
2 .
• Wi(1) = I ∈End(Vi) for all i.
Then Equation 17 holds, i.e. the group recovery algorithm is correct.
Proof. Firstly, the definition of L implies the inequalities |G| −L ≤∥W(g)∥2 ≤|G| + L and
|⟨W(g), W(h)⟩| ≤L for all g, h ∈G. In particular,
∥W(g) −W(h)∥2 = ∥W(g)∥2 + ∥W(h)∥2 −2Re(⟨W(g), W(g)⟩) ≥2|G| −4L.
(21)
By hypothesis, for each i and g ∈G there exists ρi(g) ∈U(Vi) such that
∥g−1 · Wi −ρi(g) · Wi∥<
q
1
2 −
L
|G|
p
|G| + L + 1
.
(22)
In particular, ∥Wi(gh) −ρi(g)Wi(h)∥is bounded by the same quantity for all g, h ∈G. Therefore,
via the triangle inequality we see that:
∥Wi(g)Wi(h) −Wi(gh)∥≤
(23)
≤∥Wi(h)Wi(g) −ρi(g)Wi(h)∥+ ∥Wi(gh) −ρi(g)Wi(h)∥=
(24)
= ∥Wi(h)∥
|
{z
}
≤√
|G|+L
∥Wi(g) −ρi(g)Wi(1)∥+ ∥Wi(gh) −ρi(g)Wi(h)∥<
(25)
<
s
1
2 −L
|G|.
(26)
Equation 23 implies that
∥W(g) ⊙W(h) −W(gh)∥<
p
|G|
s
1
2 −L
|G| =
p
2|G| −4L
2
.
(27)
Since ∥W(p) −W(q)∥≥
p
2|G| −4L for all p ̸= q ∈G, W(g) ⊙W(h) is closer to W(gh) than
to any other W(q) for q ∈G, which immediately implies the desired result.
The assumption on L in the above result is a relaxation of the unitarity assumption in Corollary 3.2
since L = 0 if, and only if, W is unitary up to a multiplicative constant.
We provide an explicit bound for the unitarity defect of the McCulloch-Pitts neurons discussed in
Section 3.1. To this end, let H be the space of continuous functions defined on the unit sphere in ⟨G⟩
equipped with the uniform metric (i.e., the L∞distance) as ∆.
Proposition 3.7. Let W ∈⟨G⟩and consider φ(W, x) = σ(Wx). Suppose that the activation
function σ: C →C is continuous and satisfies the following coercivity condition: there exist
constants C ∈R>0, n ∈N such that for every x ∈C:
|σ(0) −σ(x)| ≥C |x|n.
(28)
Then the unitarity defect of φ satisfies for δ < C:
ωφ(δ) ≤
v
u
u
u
t2

1 −
s
1 −
 δ
C
 2
n

.
(29)
We refer to the Appendix for a proof. Note that the coercivity condition from above plays the role of
the assumption on σ from Proposition 3.4.
9

C6 ≃C2 × C3
C2 × C2 × C2
D3 ≃S3
Figure 2: Learned Group Multiplication Tables. Tables inferred by Power-Spectral Networks for
the groups C6 (commutative), C2 × C2 × C2 (commutative), and D3 (non-commutative). Rows and
columns are labeled with integers that index group elements. Each cell of the table contains the index
of the group element obtained by composing the group elements indexed in the row and column.
Note that the table is a symmetric matrix if, and only if, the group is commutative.
3.2.1
Implementation
We now empirically explore the theory developed in this paper and demonstrate that Spectral Networks
are able to recover the group structure in practice. To this end, we implement a non-commutative
Power-Spectral Network φi(W, x) = Wix W †
i x with weights W ∈Cdi×di×d, where d = |G|
is the cardinality of the given group and d1, · · · , dk are the dimensions of its irreducible unitary
representations. As discussed at the beginning of Section 3.2, we force Wi(1) = I ∈Cdi×di, where
the index 1 is arbitrarily chosen.
Following [44], we train the model via contrastive learning [25]. Namely, given a finite dataset D of
pairs (x, y), where x, y ∈⟨G⟩≃Cd and x = g · y for an unknown g ∈G, the objective optimized
by the model is:
L(W) =
X
(x,y)∈D
X
i
∥φi(W, x) −φi(W, y)∥2 + η ∥dI −WW †∥2,
(30)
where η > 0 is a hyper-parameter and ∥· ∥is the Frobenius norm. The first term in Equation 30
encourages invariance with respect to G while the second one encourages W to be unitary.
The model is trained via the Adam optimizer [28], which interprets the complex weights as real
tensors of doubled dimensionality. The dataset D ⊆⟨G⟩⊕⟨G⟩is generated procedurally by first
sampling x from a standard Gaussian distribution over ⟨G⟩≃R2d, then sampling g ∈G uniformly,
and finally producing the datapoint (x, y = g · x) ∈D. We provide a Python implementation of both
the model and the experiments at a public repository (see Section 1.1). The code is available in both
the PyTorch [38] and the JAX [5] frameworks.
Once trained, we evaluate the model by checking whether the multiplication table M
∈
{1, · · · , d}d×d obtained via the group recovery algorithm described by Equation 17 coincides with
the one of G. Since there is no canonical ordering on G, the table is recovered up to a permutation
π of {1, · · · , d} acting as (π · M)i,j = π(Mπ−1(i),π−1(j)). Therefore, we check whether π · M
coincides with the table of G for all the permutations π. Figure 2 reports the (correct) multiplication
tables obtained at convergence for both commutative and non-commutative groups. Specifically, we
consider the cyclic group C6, the product of cyclic groups C2 × C2 × C2 and the dihedral group D3,
which is isomorphic to the symmetric group S3.
In order to validate empirically the robustness of the group recovery procedure, we additionally
train the model on data corrupted by white noise, i.e. D consists of pairs (x, y = g · x + ϵ), where
ϵ is sampled from an isotropic Gaussian distribution. We vary the standard deviation of the latter
and report in Figure 3 the number of times the multiplication table is recovered correctly across 20
training runs – a metric referred to as ‘table accuracy’.
As can be seen, the group structure is recovered most of the times even with large amounts of noise
– up to ∼0.5 standard deviation. The performance quickly degrades as the noise reaches a critical
10

0.550
0.555
0.560
0.565
0.570
0.575
Noise std
0.0
0.2
0.4
0.6
0.8
1.0
Table accuracy
C5
0.510
0.515
0.520
0.525
0.530
0.535
0.540
Noise std
0.0
0.2
0.4
0.6
0.8
1.0
Table accuracy
C6
Figure 3: Group Recovery Accuracy. Accuracy for the recovery of the group multiplication tables
across 20 training runs as the amount of noise injected into data increases. The transition from 1.0 to
0.0 accuracy is sharp, and here we visualize only the noise regions where the values are non-trivial.
threshold. This demonstrates empirically that the group recovery procedure is robust to noise, which
is in line with the theoretical bounds from Theorem 3.6.
4
Conclusions, Limitations, and Future Work
In this work, we proved that if a machine learning model of a certain kind is invariant to a finite
group, then its weights are closely related to the Fourier transform on that group. We discussed how,
as a consequence, the algebraic structure of an unknown group can be recovered from a model that
is invariant. We established these results for both commutative and non-commutative groups, and
discussed relaxed conditions under which the group recovery procedure holds. Our results represent
a first step towards a mathematical explanation of universal features inferred by both biological and
artificial neural networks.
Due to its open-ended nature, this work is subject to a number of limitations and leaves directions
open for future investigation. First, our theory encompasses models with complex-valued weights,
which are non-canonical in machine learning. Thus, exploring analogues of the theory over real
numbers is an interesting direction that would fit more directly with current practices in the field.
In addition, our theoretical framework encompasses learning models with unitary symmetries. The
latter is a technical property satisfied by Spectral Networks and, to an extent, by traditional deep
networks. However, it is not clear what other models fit into our framework, and whether the notion
is general enough to accommodate other computational primitives fundamental to machine learning,
such as the attention mechanisms. This is an open question that is worthy of investigation.
Lastly, in this work we focused on groups and their associated harmonics. However, the representa-
tions within neural networks or biological systems often resemble imperfect, or localized, versions of
harmonics, i.e. wavelets, such as Gabors. Since wavelets do not describe group homomorphisms,
our theory would need to be extended to accommodate this kind of locality. We suspect that this
may be achieved by generalizing the framework to groupoids – an algebraic group-like structure that
formalizes a locally-defined composition. This, however, goes beyond the scope of our work, and we
leave it for future research.
Acknowledgements
This work was supported by the Swedish Research Council, the Knut and Alice Wallenberg Founda-
tion and the European Research Council (ERC-BIRD-884807).
11

References
[1] Edward H Adelson and James R Bergen. Spatiotemporal energy models for the perception of
motion. Josa a, 2(2):284–299, 1985.
[2] Andrea Banino, Caswell Barry, Benigno Uria, Charles Blundell, Timothy Lillicrap, Piotr
Mirowski, Alexander Pritzel, Martin J Chadwick, Thomas Degris, Joseph Modayil, et al. Vector-
based navigation using grid-like representations in artificial agents. Nature, 557(7705):429–433,
2018.
[3] Joshua Bassey, Lijun Qian, and Xianfang Li. A survey of complex-valued neural networks.
arXiv preprint arXiv:2101.12249, 2021.
[4] Anthony J Bell and Terrence J Sejnowski. The “independent components” of natural scenes are
edge filters. Vision research, 37(23):3327–3338, 1997.
[5] James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal
Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao
Zhang. JAX: composable transformations of Python+NumPy programs. In GitHub, 2018.
[6] Michael M Bronstein, Joan Bruna, Taco Cohen, and Petar Veliˇckovi´c. Geometric deep learning:
Grids, groups, graphs, geodesics, and gauges. arXiv preprint arXiv:2104.13478, 2021.
[7] Bilal Chughtai, Lawrence Chan, and Neel Nanda. A toy model of universality: Reverse
engineering how networks learn group operations. arXiv preprint arXiv:2302.03025, 2023.
[8] Christopher J Cueva and Xue-Xin Wei. Emergence of grid-like representations by training
recurrent neural networks to perform spatial localization. arXiv preprint arXiv:1803.07770,
2018.
[9] Krish Desai, Benjamin Nachman, and Jesse Thaler. Symmetry discovery with deep learning.
Physical Review D, 105(9):096031, 2022.
[10] Ursula C Dräger. Receptive fields of single cells and topography in mouse visual cortex. Journal
of Comparative Neurology, 160(3):269–289, 1975.
[11] Pierre-Étienne H Fiquet and Eero P Simoncelli. Polar prediction of natural videos. arXiv
preprint arXiv:2303.03432, 2023.
[12] Gerald B Folland. A course in abstract harmonic analysis, volume 29. CRC press, 2016.
[13] Richard J Gardner, Erik Hermansen, Marius Pachitariu, Yoram Burak, Nils A Baas, Benjamin A
Dunn, May-Britt Moser, and Edvard I Moser. Toroidal topology of population activity in grid
cells. Nature, 602(7895):123–128, 2022.
[14] Charles J Garfinkle and Christopher J Hillar. On the uniqueness and stability of dictionaries for
sparse representation of noisy signals. IEEE Transactions on Signal Processing, 67(23):5884–
5892, 2019.
[15] Alexis Guanella, Daniel Kiper, and Paul Verschure. A model of grid cells based on a twisted
torus topology. International journal of neural systems, 17(04):231–240, 2007.
[16] Charles A Hass and Gregory D Horwitz. V1 mechanisms underlying chromatic contrast
detection. Journal of Neurophysiology, 109(10):2483–2494, 2013.
[17] Christopher J Hillar and Friedrich T Sommer. When can dictionary learning uniquely recover
sparse data from subsamples? IEEE Transactions on Information Theory, 61(11):6290–6297,
2015.
[18] Roger A Horn, Roger A Horn, and Charles R Johnson. Topics in matrix analysis. Cambridge
university press, 1994.
[19] Roger A Horn and Charles R Johnson. Matrix analysis. Cambridge university press, 2012.
[20] David H Hubel and Torsten N Wiesel. Receptive fields of single neurones in the cat’s striate
cortex. The Journal of physiology, 148(3):574, 1959.
[21] David H Hubel and Torsten N Wiesel. Receptive fields, binocular interaction and functional
architecture in the cat’s visual cortex. The Journal of physiology, 160(1):106, 1962.
[22] David H Hubel and Torsten N Wiesel. Receptive fields and functional architecture of monkey
striate cortex. The Journal of physiology, 195(1):215–243, 1968.
12

[23] Jarmo Hurri and Aapo Hyvärinen. Simple-cell-like receptive fields maximize temporal coher-
ence in natural video. Neural Computation, 15(3):663–691, 2003.
[24] Guy Isely, Christopher Hillar, and Fritz Sommer. Deciphering subsampled data: adaptive
compressive sampling as a principle of brain communication. Advances in neural information
processing systems, 23, 2010.
[25] Ashish Jaiswal, Ashwin Ramesh Babu, Mohammad Zaki Zadeh, Debapriya Banerjee, and Fillia
Makedon. A survey on contrastive self-supervised learning. Technologies, 9(1):2, 2020.
[26] Ramakrishna Kakarala. Completeness of bispectrum on compact groups. arXiv preprint
arXiv:0902.0196, 1, 2009.
[27] Ramakrishna Kakarala. The bispectrum as a source of phase-sensitive invariants for fourier
descriptors: a group-theoretic approach. Journal of Mathematical Imaging and Vision, 44:341–
353, 2012.
[28] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
[29] Yixuan Li, Jason Yosinski, Jeff Clune, Hod Lipson, and John Hopcroft. Convergent learning:
Do different neural networks learn the same representations? arXiv preprint arXiv:1511.07543,
2015.
[30] Sindy Löwe, Phillip Lippe, Maja Rudolph, and Max Welling. Complex-valued autoencoders
for object discovery. arXiv preprint arXiv:2204.02075, 2022.
[31] Warren S McCulloch and Walter Pitts. A logical calculus of the ideas immanent in nervous
activity. The bulletin of mathematical biophysics, 5:115–133, 1943.
[32] Luca Moschella, Valentino Maiorca, Marco Fumero, Antonio Norelli, Francesco Locatello, and
Emanuele Rodola. Relative representations enable zero-shot latent space communication. arXiv
preprint arXiv:2209.15430, 2022.
[33] Edvard I Moser, Emilio Kropff, and May-Britt Moser. Place cells, grid cells, and the brain’s
spatial representation system. Annu. Rev. Neurosci., 31:69–89, 2008.
[34] Neel Nanda, Lawrence Chan, Tom Liberum, Jess Smith, and Jacob Steinhardt. Progress
measures for grokking via mechanistic interpretability. arXiv preprint arXiv:2301.05217, 2023.
[35] Chris Olah, Nick Cammarata, Ludwig Schubert, Gabriel Goh, Michael Petrov, and Shan Carter.
An overview of early vision in inceptionv1. Distill, 5(4):e00024–002, 2020.
[36] Bruno A Olshausen and David J Field. Sparse coding with an overcomplete basis set: A strategy
employed by v1? Vision research, 37(23):3311–3325, 1997.
[37] Jeff Orchard, Hao Yang, and Xiang Ji. Does the entorhinal cortex use the fourier transform?
Frontiers in computational neuroscience, 7:179, 2013.
[38] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,
Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas
Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy,
Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style,
high-performance deep learning library. In Advances in Neural Information Processing Systems
32, pages 8024–8035. Curran Associates, Inc., 2019.
[39] Walter Pitts and Warren S McCulloch. How we know universals: The perception of auditory
and visual forms. The Bulletin of mathematical biophysics, 9:127–147, 1947.
[40] Rajesh Rao and Daniel Ruderman. Learning lie groups for invariant visual perception. Advances
in neural information processing systems, 11, 1998.
[41] Tilman Räuker, Anson Ho, Stephen Casper, and Dylan Hadfield-Menell. Toward transparent ai:
A survey on interpreting the inner structures of deep neural networks. In 2023 IEEE Conference
on Secure and Trustworthy Machine Learning (SaTML), pages 464–483. IEEE, 2023.
[42] Walter Rudin. Fourier analysis on groups. Bull. Amer. Math. Soc, 70:230–232, 1964.
[43] Sara Sabour, Nicholas Frosst, and Geoffrey E Hinton. Dynamic routing between capsules.
Advances in neural information processing systems, 30, 2017.
[44] Sophia Sanborn, Christian Shewmake, Bruno Olshausen, and Christopher Hillar. Bispectral
neural networks. International Conference on Learning Representations (ICLR), 2023.
13

[45] Fethi Smach, Cedric Lemaître, Jean-Paul Gauthier, Johel Miteran, and Mohamed Atri. Gener-
alized fourier descriptors with applications to objects recognition in svm context. Journal of
mathematical imaging and Vision, 30:43–71, 2008.
[46] Jascha Sohl-Dickstein, Ching Ming Wang, and Bruno A Olshausen. An unsupervised algorithm
for learning lie group transformations. arXiv preprint arXiv:1001.1027, 2010.
[47] Bernd Sturmfels. Algorithms in invariant theory. Springer Science & Business Media, 2008.
[48] Chiheb Trabelsi, Olexa Bilaniuk, Ying Zhang, Dmitriy Serdyuk, Sandeep Subramanian, João Fe-
lipe Santos, Soroush Mehri, Negar Rostamzadeh, Yoshua Bengio, and Christopher J Pal. Deep
complex networks (2017). arXiv preprint arXiv:1705.09792, 2017.
[49] Konstantin F Willeke, Kelli Restivo, Katrin Franke, Arne F Nix, Santiago A Cadena, Tori Shinn,
Cate Nealley, Gabby Rodriguez, Saumil Patel, Alexander S Ecker, et al. Deep learning-driven
characterization of single cell tuning in primate visual area v4 unveils topological organization.
bioRxiv, pages 2023–05, 2023.
14

Appendix
A
Proofs of Theoretical Results
A.1
Proof of Proposition 3.3
In order to prove this proposition, we will need some technical facts from matrix algebra. We start by
showing the following uniqueness result.
Lemma A.1. Let d′ ≥d and A, B ∈Cd′×d. If AA† = BB†, then there exists a unitary matrix
U ∈Cd×d such that A = BU.
Proof. From the polar decomposition for matrices (see [18, Theorem 3.1.9]), we know that:
A = PV, B = QW,
(31)
where P, Q ∈Cd′×d′ are Hermitian positive semidefinite and V, W ∈Cd′×d have orthonormal rows.
Also, P 2 = AA† = BB† = Q2 from which it follows that P = Q by uniqueness of square roots of
positive semidefinite Hermitian matrices (see [19, Theorem 7.2.6]). In particular, we have:
A = PV = QV = QWW †V = BU,
(32)
with U = W †V unitary.
Remark A.1. We note that if A and B are real matrices, then the conclusion holds with U being a
real orthogonal matrix.
Next, we show that positive semidefinite Hermitian matrices possess unique tensor roots.
Lemma A.2. Let A, B ∈Cd×d be Hermitian and positive semidefinite. If A⊗n = B⊗n for some
n > 0, then A = B.
Proof. From the Spectral Theorem we know that:
A = UDU †,
(33)
where U is unitary and D is diagonal. From A⊗n = B⊗n it follows that D⊗n = C⊗n, where
C = U †BU. Note that the (point-wise) Hadamard product of matrices is a submatrix of the tensor
(Kronecker) product. In particular, the off-diagonal entries of C must vanish. On the diagonal we
have (Di,i)n = (Ci,i)n for every i, and therefore Di,i = Ci,i since they are non-negative. This
shows that C = D, implying that B = UDU † = A.
By putting together the above lemmas, we obtain the following.
Lemma A.3. Let A1, · · · , Ak, B1, · · · , Bk ∈Cd×d. Suppose that for some n > 0, for all x ∈Ck:
 X
i
xiAi
!⊗n  X
i
xiA†⊗n
i
!
=
 X
i
xiBi
!⊗n  X
i
xiB†⊗n
i
!
.
(34)
Then there exists a unitary matrix U ∈Cd×d such that Ai = BiU for every i.
Proof. By multilinearity of the tensor product we see that for all x ∈Ck:
X
i1,··· ,in+1
xi1 · · · xinxin+1(Ai1 ⊗· · · ⊗Ain)A†⊗n
in+1 =
(35)
=
X
i1,··· ,in+1
xi1 · · · xinxin+1(Ai1A†
in+1) ⊗· · · ⊗(AinA†
in+1) =
(36)
=
X
i1,··· ,in+1
xi1 · · · x1nxin+1(Bi1B†
in+1) ⊗· · · ⊗(BinB†
in+1).
(37)
Since a polynomial vanishes as function if and only if it is the zero polynomial, it follows that
(Ai1A†
in+1) ⊗· · · ⊗(AinA†
in+1) = (Bi1B†
in+1) ⊗· · · ⊗(BinB†
in+1) for all i1, · · · , in+1, and in
15

particular (AiA†
j)⊗n = (BiB†
j)⊗n for all i, j. From Lemma A.2 we conclude that AiAj = BiBj
for all i, j, which can be rephrased as AA† = BB†, where A, B are the (dk) × d matrices obtained
by concatenating the Ai’s and Bi’s respectively. From Lemma A.1 we conclude that A = BU for a
unitary d × d matrix U, as desired.
Proposition 3.3 now follows immediately from Lemma A.3 by setting Ai = W(gi) and Bi = W ′(gi)
for gi ∈G and W, W ′ ∈⟨G⟩⊗End(V ) of the same norm.
A.2
Proof of Proposition 3.4
Proof. Pick W, W ′ ∈⟨G⟩of the same norm such that φ(W, x) = φ(W ′, x) for all x ∈⟨G⟩. Given
the open set O ⊆C from the hypothesis on σ, consider O′ = {x ∈⟨G⟩| Wx, W ′x ∈O}, which is
open and non-empty since 0 ∈O′. For x ∈O′, Wx = 0 implies φ(W, x) = φ(W ′, x) = 0, from
which it follows that W ′x = 0 by definition of O. Therefore, W and W ′ share the same orthogonal
complement, implying that W ′ = ρW for some ρ ∈C. Since W and W ′ have the same norm, we
conclude that ρ ∈U(C).
A.3
Proof of Proposition 3.5
Proof. Consider W, W ′ ∈W such that φ(W, x) = φ(W ′, x) for all x ∈⟨G⟩. Given the open set
O ⊆Rk
≥0 from the hypothesis on χ, consider O′ = {x ∈⟨G⟩| |Wx|2, |W ′x|2 ∈O}, which is open
and non-empty since 0 ∈O′. For x ∈O′, φ(W, x) can be written as:
φ(W, x) =
X
i
ai|Wi x|2 + b,
(38)
with the ai’s being distinct, and similarly for φ(W ′, x). Since Hermitian forms are determined by
their restriction on an open set, we deduce the following identity of operators:
X
i
aiWi ⊗Wi =
X
i
aiW ′
i ⊗W ′
i.
(39)
Since both the sets {Wi}i and {W ′
i}i are orthonormal by hypothesis on W, both sides of Equation 39
define a spectral decomposition, i.e. a decomposition into projections over orthonormal vectors. Since
the eigenvalues ai are distinct and non-vanishing, it follows that Wi = ρiW ′
i for some ρi ∈U(C), as
desired.
A.4
Proof of Proposition 3.7
In order to prove this proposition, we will need the following technical fact from linear algebra.
Lemma A.4. Let H be a finite-dimensional complex Hilbert space, v, w ∈H normal and ε ∈R
such that 0 < ε < 1. Suppose that for every normal x orthogonal to w, it holds that |⟨x, v⟩| ≤ε.
Then there exists ρ ∈U(C) such that
∥v −ρw∥≤
r
2

1 −
p
1 −ε2

.
(40)
Proof. Consider an orthogonal decomposition v = ⟨w1, v⟩w1 + ⟨w2, v⟩w2, where w1 ∈w⊥is
normal and w2 = ρw for some ρ ∈U(C) such that ⟨w2, v⟩∈R≥0. It follows that:
1 = ∥v∥2 = |⟨w1, v⟩|2 + ⟨w2, v⟩2.
(41)
The hypothesis implies then that ⟨w2, v⟩≥
√
1 −ε2. Therefore,
∥v −w2∥2 = 2 −2⟨w2, v⟩≤2

1 −
p
1 −ε2

,
(42)
as desired.
Remark A.2. Note that the right-hand side of Equation 40 is bounded by the concise quantity
√
2ε.
We are now ready to prove Proposition 3.7.
16

Proof. Consider δ ∈R>0 and W, W ′ ∈⟨G⟩of the same norm such that ∆(φ(W), φ(W ′)) ≤δ.
The latter and the hypotheses together imply that if x ∈⟨G⟩is normal such that W ′x = 0, then:
C |Wx|n ≤|σ(0) −σ(Wx)| ≤δ.
(43)
By Lemma A.4 there exists ρ ∈U(C) such that:
∥W −ρW ′∥≤
v
u
u
u
t2

1 −
s
1 −
 δ
C
 2
n

,
(44)
from which the claim follows.
B
Spectral Invariants
In this section, we overview the theory of invariants over ⟨G⟩, i.e. (polynomial) maps ⟨G⟩→C that
are invariant with respect to the action by G. To this end, we recall the following notion.
Definition 10. Fix n > 0 and ρ = (ρ1, . . . , ρn) ∈(G∨)n. The spectrum of order n associated to ρ
is defined for x ∈⟨G⟩as:
βρ(x) = ˆxρ1 · · · ˆxρn ˆxρ1···ρn
(45)
The spectra of order n are invariant polynomials of degree n + 1 containing one conjugate variable.
The presence of the latter is necessary for invariance. For n = 1, 2 they are alternatively referred to as
power spectra and bispectra respectively. Note that the power spectra reduce simply to βρ(x) = |ˆxρ|2,
ρ ∈G∨, and constitute a standard tool in signal processing. Bispectra, together with higher-order
spectra, were first introduced in [27]. It is immediate to see that spectra generate all the polynomial
invariants of ⟨G⟩(see also [47, Theorem 2.1.4]).
Proposition B.1. The space of polynomial invariants of degree n + 1 over ⟨G⟩(with one conjugate
variable) is generated as a complex vector space by the spectra of order n.
Proof. This follows from interpreting the invariance condition via the Fourier transform. Namely,
given ρ ∈(G∨)n+1 consider the monomial over ⟨G∨⟩defined by ˆxρ1 · · · ˆxρn ˆxρn+1. Since g · ˆx =
(ρ(g) ˆxλ)λ∈G∨, the monomial is invariant if and only if ρ1(g) · · · ρn(g)ρn+1(g) = 1 for all g ∈G,
i.e. ρn+1 = ρ1 · · · ρn. Since monomials linearly generate polynomials, the claim follows.
A remarkable aspect of spectra of even order is the fact that they jointly determine real (generic)
elements of ⟨G⟩up to the action by G – a property known as completeness. This was first shown in
[45]. For convenience, we report below a simple proof for finite commutative groups.
Proposition B.2. Fix n even. Suppose that x, y ∈RG ⊆⟨G⟩are such that ˆxρ, ˆyρ ̸= 0 for all
ρ ∈G∨. If βρ(x) = βρ(y) for all ρ ∈(G∨)n then x = g · y for some g ∈G.
Proof. By setting ρ = (1, · · · , 1) we see that ⟨1, x⟩n+1 = ⟨1, y⟩n+1 ∈R \ {0} and therefore
⟨1, x⟩= ⟨1, y⟩since n is even.
For ρ ∈G∨, by setting ρ = (ρ, ρ, 1, · · · , 1), we see that
⟨ρ, x⟩⟨ρ, x⟩⟨1, x⟩n−1 = |⟨ρ, x⟩|2⟨1, x⟩n−1 = |⟨ρ, y⟩|2⟨1, y⟩n−1 and therefore |⟨ρ, x⟩| = |⟨ρ, y⟩|.
Note that here we relied on the fact that x and y are real. This implies that the following map
η : G∨→C takes values in U(C):
η(ρ) = ⟨ρ, x⟩
⟨ρ, y⟩.
(46)
Now, η(1) = 1 since ⟨1, y⟩= ⟨1, y⟩. By setting ρ = (ρ, µ, ρµ, 1, · · · , 1) we see that η(ρ)η(µ) =
η(ρµ) for all ρ, µ ∈G∨and therefore η ∈(G∨)∨. Since the Fourier transform sends G∨⊆⟨G⟩
to (G∨)∨⊆⟨G∨⟩, there exists g ∈G such that η(ρ) = ρ(g). This means that ⟨ρ, x⟩= ρ(g)⟨ρ, y⟩,
which implies x = g · y by the equivariance properties of the Fourier transform.
Spectral invariants can be defined in the non-commutative case, but arise subtleties. First, we replace
the group structure of G∨with the tensor product ⊗of unitary representations. However, Irr(G) is
not closed with respect to ⊗. This is circumvented by considering Clebsch-Gordan coefficients, i.e.
irreducible unitary sub-representations of tensor products. This leads to the following definition of
operator-valued spectra.
17

Definition 11. Fix n > 0 and ρ = (ρV1, · · · , ρVn) ∈Irr(G)n. The spectrum of order n associated to
ρ is defined for x ∈⟨G⟩as:
βρ(x) = ˆxρV1 ⊗· · · ⊗ˆxρVn

ˆx†
ρT1 ⊕· · · ⊕ˆx†
ρTk

∈End(V1 ⊗· · · ⊗Vn)
(47)
where the direct sum runs over the k irreducible unitary representations appearing in an orthogonal
decomposition V1 ⊗· · · ⊗Vn = T1 ⊕· · · ⊕Tk.
The completeness of spectra of order n ≥2 (Proposition B.2) extends to the non-commutative case
[26].
18

