Emergence in large language models
Jason Wei
Google Brain

Outline
●
Emergent abilities of large language models (Wei et al., 2022).
●
Chain-of-thought prompting elicits reasoning in large language models (Wei et al., 2022).
○
Self-consistency improves chain-of-thought reasoning in language models (Wang et al., 2022).
○
Least-to-most prompting enables complex reasoning in large language models (Zhou et al., 2022).
○
Language models are multilingual chain-of-thought reasoners (Shi et al., 2022).
○
Challenging BIG-Bench tasks and whether chain-of-thought can solve them.
●
Scaling instruction-finetuned language models (Chung et al., 2022).
●
Feel free to interupt with questions :)


Predictable gains as a result of scaling
●
Emergent abilities of large language model (TMLR ‘22). 
J. Wei, Y. Tay, R. Bommasani, C. Raffel, B. Zoph, S. Borgeaud, D. Yogatama, M. Bosma, D. Zhou, D. Metzler, E. 
Chi, T. Hashimoto, O. Vinyals, P. Liang, J. Dean, & W. Fedus.

Emergence in science
●
Emergence:
“a qualitative change that arises 
from quantitative changes”
https://bounded-regret.ghost.io/future-ml-systems-will-be-qualitatively-different/
Emergent abilities of large language model (TMLR ‘22). 
J. Wei, Y. Tay, R. Bommasani, C. Raffel, B. Zoph, S. Borgeaud, D. Yogatama, M. Bosma, D. Zhou, D. Metzler, E. 
Chi, T. Hashimoto, O. Vinyals, P. Liang, J. Dean, & W. Fedus.

Emergent abilities in large language models definition
An ability is emergent if it is not present in 
smaller models but is present in larger models.
●
How to measure the size of the model?
○
Training FLOPs
○
Number of model parameters
○
Training dataset size
Emergent abilities of large language model (TMLR ‘22). 
J. Wei, Y. Tay, R. Bommasani, C. Raffel, B. Zoph, S. Borgeaud, D. Yogatama, M. Bosma, D. Zhou, D. Metzler, E. 
Chi, T. Hashimoto, O. Vinyals, P. Liang, J. Dean, & W. Fedus.

Emergence in few-shot prompting
Emergent abilities of large language model (TMLR ‘22). 
J. Wei, Y. Tay, R. Bommasani, C. Raffel, B. Zoph, S. Borgeaud, D. Yogatama, M. Bosma, D. Zhou, D. Metzler, E. 
Chi, T. Hashimoto, O. Vinyals, P. Liang, J. Dean, & W. Fedus.
Input (English): The 1931 
Malay census was an alarm 
bell.
Target (IPA): ðə 1931 
ˈmeɪleɪ ˈsɛnsəs wɑz ən 
əˈlɑrm bɛl.

Emergent prompting strategies
●
Prompting techniques beyond few-shot 
prompting that improve the ability of language 
models 
○
Only work at sufficient model scale
Emergent abilities of large language model (TMLR ‘22). 
J. Wei, Y. Tay, R. Bommasani, C. Raffel, B. Zoph, S. Borgeaud, D. Yogatama, M. Bosma, D. Zhou, D. Metzler, E. 
Chi, T. Hashimoto, O. Vinyals, P. Liang, J. Dean, & W. Fedus.

Any questions?
Next: chain-of-thought prompting
Emergent abilities of large language model (TMLR ‘22). 
J. Wei, Y. Tay, R. Bommasani, C. Raffel, B. Zoph, S. Borgeaud, D. Yogatama, M. Bosma, D. Zhou, D. Metzler, E. 
Chi, T. Hashimoto, O. Vinyals, P. Liang, J. Dean, & W. Fedus.


Ed Chi
Quoc Le
Olivier 
Bousquet
Dale 
Schuurmans
Fei Xia
Aakanksha 
Chowdhery
Sharan 
Narang
Maarten 
Bosma
Nathan 
Scales
Brian Ichter
Le Hou
Nathaneal 
Schärli 
Denny Zhou
Xuezhi Wang
Jason Wei
Google I/O 2022
Video: 
https://twitter.com/Google/status/152518
8695875366912

Motivation
12
Chain of thought prompting elicits reasoning in large language models (NeurIPS ‘22). 
J. Wei, X. Wang, D. Schuurmans, M. Bosma, B. Ichter, F. Xia, E. Chi, Q. Le, & D. Zhou.
This movie sucks!
Thank you!
Novak Djokovic wins 
the 2021 French Open.
Big LM
Sentiment = negative
Topic = Tennis
Translation: 谢谢!
“type 1”
Take the last letters of 
the words in "Elon Musk" 
and concatenate them.
“uk”
The cafeteria had 23 apples. If 
they used 20 to make lunch 
and bought 6 more, how many 
apples do they have?
27
“type 2”

Example of chain of thought
Question: Roger has 5 tennis balls. He buys 2 
more cans of tennis balls. Each can has 3 tennis 
balls. How many tennis balls does he have now?
13
Chain of thought:  Roger started with 5 balls. 2 
cans of 3 tennis balls each is 6 tennis balls. 5 + 6 
= 11. The answer is 11.
Chain of thought prompting elicits reasoning in large language models (NeurIPS ‘22). 
J. Wei, X. Wang, D. Schuurmans, M. Bosma, B. Ichter, F. Xia, E. Chi, Q. Le, & D. Zhou.

14
Chain of thought prompting elicits reasoning in large language models (NeurIPS ‘22). 
J. Wei, X. Wang, D. Schuurmans, M. Bosma, B. Ichter, F. Xia, E. Chi, Q. Le, & D. Zhou.

GPT-3 demo
15
Chain of thought prompting elicits reasoning in large language models (NeurIPS ‘22). 
J. Wei, X. Wang, D. Schuurmans, M. Bosma, B. Ichter, F. Xia, E. Chi, Q. Le, & D. Zhou.

16
Results on grade-school math problems 
(GSM8K) 
LaMDA
PaLM
Finetuned 
GPT-3 175B
Finetuned 
SOTA
Chain of thought prompting elicits reasoning in large language models (NeurIPS ‘22). 
J. Wei, X. Wang, D. Schuurmans, M. Bosma, B. Ichter, F. Xia, E. Chi, Q. Le, & D. Zhou.

17
Last Letter Concatentation
Q: Take the last letters of the 
words in "Elon Musk" and 
concatenate them. 
A: The last letter of “Elon” is 
“n”. The last letter of “Musk” is 
“k”. Concatenating them is 
“nk”. So the answer is nk.
“In domain” 
(2 letters)
OOD length 
generalization
(4 letters)
Chain of thought prompting elicits reasoning in large language models (NeurIPS ‘22). 
J. Wei, X. Wang, D. Schuurmans, M. Bosma, B. Ichter, F. Xia, E. Chi, Q. Le, & D. Zhou.

18
StrategyQA
Finetuned 
SOTA
Real model output #1
Q:  Can you hide a basketball 
in a sand cat's ear?
A: A basketball is about 30 
cm in diameter. A sand cat's 
ear is about 2 cm in 
diameter. Thus, a basketball 
would not fit in a sand cat's 
ear. So the answer is no.
Real model output #2
Q:  Can oysters be used in 
guitar manufacturing?
A: Oysters can be used to 
make pearls. Pearls can be 
used to make guitar inlays. 
Thus, oysters can be used in 
guitar manufacturing. So the 
answer is yes.
Human
Chain of thought prompting elicits reasoning in large language models (NeurIPS ‘22). 
J. Wei, X. Wang, D. Schuurmans, M. Bosma, B. Ichter, F. Xia, E. Chi, Q. Le, & D. Zhou.

Real model output
Question: I spilled my coke on the table, 
could you throw it away and then bring 
me something to help clean?
Model answer:
Explanation: The user has spilled their 
coke on the table. I will throw awaythe 
coke and then bring the user a sponge. 
Plan: find(coke), pick(coke), find(trash), 
put(coke), find(sponge),pick(sponge), 
find(table), put(sponge).
SayCan (Robotics)
Chain of thought prompting elicits reasoning in large language models (NeurIPS ‘22). 
J. Wei, X. Wang, D. Schuurmans, M. Bosma, B. Ichter, F. Xia, E. Chi, Q. Le, & D. Zhou.

Why does scaling up improve chain of thought reasoning?
Chain of thought prompting elicits reasoning in large language models (NeurIPS ‘22). 
J. Wei, X. Wang, D. Schuurmans, M. Bosma, B. Ichter, F. Xia, E. Chi, Q. Le, & D. Zhou.

Why chain of thought?
Variable computation.
21
Human-readable reasoning.
Natural language is broadly applicable.
Works with just prompting!
Before (finetuning)
Now (prompting)
●
Large dataset with 
intermediate step 
annotations
●
Many task-specific 
models
●
Chain of thought 
annotations, but 
few-shot only
●
Single model for all 
tasks
●
Program induction by rationale 
generation: learning to solve and 
explain algebraic word problems 
(ACL ‘17).
●
Leap-of-thought: Teaching 
pre-trained models to systematically 
reason over implicit knowledge 
(NeurIPS ‘20).
●
Training verifiers to solve math word 
problems (OpenAI ‘21).
●
More…
Chain of thought prompting elicits reasoning in large language models (NeurIPS ‘22). 
J. Wei, X. Wang, D. Schuurmans, M. Bosma, B. Ichter, F. Xia, E. Chi, Q. Le, & D. Zhou.

Limitations
●
Incremental or zero-gain when task 
performance is already good
●
Some prompt engineering required
SingleOp
SingleEq
AddSub
ASDiv
Model scale (# params in billions)
Solve rate (%)
The baseline is already strong on easy 
single or two-step math problems.
Chain of thought prompting elicits reasoning in large language models (NeurIPS ‘22). 
J. Wei, X. Wang, D. Schuurmans, M. Bosma, B. Ichter, F. Xia, E. Chi, Q. Le, & D. Zhou.

23
Chain of thought prompting elicits reasoning in large language models (NeurIPS ‘22). 
J. Wei, X. Wang, D. Schuurmans, M. Bosma, B. Ichter, F. Xia, E. Chi, Q. Le, & D. Zhou.

A decade of deep learning…
features
features
features
features
[features]
Early days
0/1
0/1
0/1
0/1
0/1
AlexNet
(2012)
0/1
0/1
0/1
0/1
ship
( x
y )
,
pos/neg
pos/neg
pos/neg
pos/neg
BERT
(2018)
pos/neg
“i love this 
movie”
“i love this 
movie”
“i love this 
movie”
“i love this 
movie”
“i love this 
movie”
Chain of thought prompting elicits reasoning in large language models (NeurIPS ‘22). 
J. Wei, X. Wang, D. Schuurmans, M. Bosma, B. Ichter, F. Xia, E. Chi, Q. Le, & D. Zhou.

Beyond (x, y) with language models (GPT-3)...
“Can a sturgeon ﬁt 
in a laptop bag?”
“no”
 x
,
y )
 x
, chain of thought  +  y)
 (instruction    +
“no”
“A sturgeon is a type of ﬁsh that 
is 7-12 feet long. That is much 
larger than a laptop bag.”
“Can a sturgeon ﬁt 
in a laptop bag?”
“Give a yes/no 
answer to this 
question:”
 (instruction    +
“Give a yes/no 
answer to this 
question:”
(Cool, but not this talk)
Chain of thought prompting elicits reasoning in large language models (NeurIPS ‘22). 
J. Wei, X. Wang, D. Schuurmans, M. Bosma, B. Ichter, F. Xia, E. Chi, Q. Le, & D. Zhou.

Any questions?
Next: extensions of chain of thought prompting


Motivation
Key idea: 
●
Ask the LM to generate a diverse set of possible 
solutions, and if some of them lead to the same 
answer, then it’s more likely to be the correct answer
Given a question, do people always solve it in the same way?
Self-consistency improves chain-of-thought reasoning in language models. 
X. Wang, J. Wei, D. Schuurmans, Q. Le, E. Chi, S. Narang, A. Chowdhery, & D. Zhou.

Chain of thought with greedy decode
Q: Roger has 5 tennis balls. He buys 2 
more cans of tennis balls. Each can has 3 
tennis balls. How many tennis balls does 
he have now? 
A: Roger started with 5 balls. 2 cans of 3 
tennis balls each is 6 tennis balls. 5 + 6 = 
11. The answer is 11.
…
Q: Janet’s ducks lay 16 eggs per day. She 
eats three for breakfast every morning 
and bakes muffins for her friends every 
day with four. She sells the remainder for 
$2 per egg. How much does she make 
every day?
A:
Language 
model
This means she uses 3 + 4 = 7 eggs every day.  
She sells the remainder for $2 per egg, so in 
total she sells 7 * $2 = $14 per day. 
The answer is $14.
Greedy decode
●
Wrong answer due to incorrect reasoning path
●
Is there a way to self-check this solution?
Prompt example with 
chain of thought
Actual question to ask
Self-consistency improves chain-of-thought reasoning in language models. 
X. Wang, J. Wei, D. Schuurmans, Q. Le, E. Chi, S. Narang, A. Chowdhery, & D. Zhou.

Self-consistency: sample decode
Q: Roger has 5 tennis balls. He buys 2 
more cans of tennis balls. Each can has 3 
tennis balls. How many tennis balls does 
he have now? 
A: Roger started with 5 balls. 2 cans of 3 
tennis balls each is 6 tennis balls. 5 + 6 = 
11. The answer is 11.
Q: Janet’s ducks lay 16 eggs per day. She 
eats three for breakfast every morning 
and bakes muffins for her friends every 
day with four. She sells the remainder for 
$2 per egg. How much does she make 
every day?
A:
Prompt with example chain of thought
Language 
model
She has 16 - 3 - 4 = 9 eggs 
left. So she makes $2 * 9 = 
$18 per day. 
Sample decode with diverse reasoning paths
She eats 3 for breakfast, so 
she has 16 - 3 = 13 left. Then 
she bakes muffins, so she 
has 13 - 4 = 9 eggs left. So 
she has 9 eggs * $2 = $18. 
This means she uses 3 + 4 = 7 
eggs every day. So in total 
she sells 7 * $2 = $14 per day. 
The answer is $18.
The answer is $14.
The answer is $18.
Note the reasoning paths are optional, so you don’t 
have to look at them after getting the ﬁnal answer
Self-consistency improves chain-of-thought reasoning in language models. 
X. Wang, J. Wei, D. Schuurmans, Q. Le, E. Chi, S. Narang, A. Chowdhery, & D. Zhou.

Self-consistency: majority vote
Q: Roger has 5 tennis balls. He buys 2 
more cans of tennis balls. Each can has 3 
tennis balls. How many tennis balls does 
he have now? 
A: Roger started with 5 balls. 2 cans of 3 
tennis balls each is 6 tennis balls. 5 + 6 = 
11. The answer is 11.
Q: Janet’s ducks lay 16 eggs per day. She 
eats three for breakfast every morning 
and bakes muffins for her friends every 
day with four. She sells the remainder for 
$2 per egg. How much does she make 
every day?
A:
Prompt with example chain of thought
Language 
model
She has 16 - 3 - 4 = 9 eggs 
left. So she makes $2 * 9 = 
$18 per day. 
Sample decode with diverse reasoning paths
She eats 3 for breakfast, so 
she has 16 - 3 = 13 left. Then 
she bakes muffins, so she 
has 13 - 4 = 9 eggs left. So 
she has 9 eggs * $2 = $18. 
This means she uses 3 + 4 = 7 
eggs every day. So in total 
she sells 7 * $2 = $14 per day. 
The answer is $18.
The answer is $14.
The answer is $18.
Majority vote 
on the answers
The answer is $18.
Self-consistency improves chain-of-thought reasoning in language models. 
X. Wang, J. Wei, D. Schuurmans, Q. Le, E. Chi, S. Narang, A. Chowdhery, & D. Zhou.

Related work
●
The OpenAI paper trained a verifier using an additional language model
○
What we propose here is much simpler: No need to train a separate model
●
Compared to many other existing works
○
No fine-tuning
○
No need for extra human annotation
○
No additional modules
Self-consistency improves chain-of-thought reasoning in language models. 
X. Wang, J. Wei, D. Schuurmans, Q. Le, E. Chi, S. Narang, A. Chowdhery, & D. Zhou.

Results
●
When combined with PaLM-540B: beats SoTA on 8 out of 10 reasoning tasks
●
Previous SoTA are from various papers with task-specific training, while 
self-consistency is a generic approach over the same frozen LM
Self-consistency improves chain-of-thought reasoning in language models. 
X. Wang, J. Wei, D. Schuurmans, Q. Le, E. Chi, S. Narang, A. Chowdhery, & D. Zhou.

Works for other common NLP tasks too
E.g., natural language inference, closed-book QA, etc.
Self-consistency improves chain-of-thought reasoning in language models. 
X. Wang, J. Wei, D. Schuurmans, Q. Le, E. Chi, S. Narang, A. Chowdhery, & D. Zhou.
●
Even helps when sometimes adding CoT might hurt performance, e.g., e-SNLI, RTE

Consistency <-> Uncertainty
●
Consistency: %decodes agreeing with the 
majority vote
●
The consistency is nicely correlated with the 
accuracy
●
May use to provide uncertainty estimates:
○
low “consistency” -> “uncertain”
Self-consistency improves chain-of-thought reasoning in language models. 
X. Wang, J. Wei, D. Schuurmans, Q. Le, E. Chi, S. Narang, A. Chowdhery, & D. Zhou.

Least-to-most prompting
Hierarchical prompting method
●
Even more degrees of freedom
Least-to-most prompting enables complex reasoning in large language models.
D. Zhou, N. Schärli, L. Hou, J. Wei, N. Scales, X. Wang, D. Schuurmans, O. Bousquet, Q. Le, & E. Chi.

Least-to-most prompting
Least-to-most prompting enables complex reasoning in large language models.
D. Zhou, N. Schärli, L. Hou, J. Wei, N. Scales, X. Wang, D. Schuurmans, O. Bousquet, Q. Le, & E. Chi.

Least-to-most prompting
Least-to-most prompting enables complex reasoning in large language models.
D. Zhou, N. Schärli, L. Hou, J. Wei, N. Scales, X. Wang, D. Schuurmans, O. Bousquet, Q. Le, & E. Chi.

Least-to-most prompting
Least-to-most prompting enables complex reasoning in large language models.
D. Zhou, N. Schärli, L. Hou, J. Wei, N. Scales, X. Wang, D. Schuurmans, O. Bousquet, Q. Le, & E. Chi.

Multilingual chain-of-thought prompting
Language models are multilingual chain-of-thought reasoners.
F. Shi, M. Suzgun, M. Freitag, X. Wang, S. Srivats, S. Vosoughi, H. Chung, Y. Tay, S. Ruder, D. Zhou, D. Das, & J. Wei.

Multilingual chain-of-thought prompting
Language models are multilingual chain-of-thought reasoners.
F. Shi, M. Suzgun, M. Freitag, X. Wang, S. Srivats, S. Vosoughi, H. Chung, Y. Tay, S. Ruder, D. Zhou, D. Das, & J. Wei.
Pretty high accuracy on 
math problems given and 
solved in underrepresented 
languages

Chain-of-thought on BIG-Bench tasks
Challenging BIG-Bench tasks and whether chain-of-thought can solve them.
M. Suzgun, N. Scales, N. Schärli, S. Gehrmann, Y. Tay, H. W. Chung, A. Chowdhery, Q. V. Le, E. Chi, D. Zhou, and J. Wei.

Chain-of-thought on BIG-Bench tasks
Challenging BIG-Bench tasks and whether chain-of-thought can solve them.
M. Suzgun, N. Scales, N. Schärli, S. Gehrmann, Y. Tay, H. W. Chung, A. Chowdhery, Q. V. Le, E. Chi, D. Zhou, and J. Wei.

Instruction-finetuned language models are zero-shot learners.
J. Wei, M. Bosma, V. Zhao, K. Guu, A. Yu, B. Lester, N. Du, A. Dai, and Q. Le. ICLR 2022.

Motivation
45
Pretraining objective
(Language modeling)
Downstream inference
(NLP task)
Few-shot prompting
Prompt tuning
...
“This movie sucks.” This movie 
review is {negative,positive}.

46
Can we use “a little bit” of supervision 
to teach the model to perform many 
NLP tasks?
i.e., zero-shot!
Instruction tuning

“Instruction tuning”—finetuning a language model on a 
collection of tasks described via instructions—improves the 
zero-shot performance of language models on unseen tasks.
47


NLP tasks and datasets
●
62 NLP datasets
●
12 “task clusters”
49

Templates
50
We generate many natural instruction 
templates for each task

Evaluation splits
51
We evaluate on “unseen” / 
“zero-shot” tasks where no 
datasets from that task were seen 
during instruction tuning.

FLAN Training details
●
137B parameter LaMDA-PT checkpoint
●
Instruction tune for 30k steps on 62 
datasets spanning 12 task clusters
52

Summary of results
●
25 datasets spanning NLI, reading comprehension, closed-book QA, 
commonsense reasoning, coreference resolution, and translation
●
Baselines: LaMDA-PT (base LM), GPT-3 175B
53
FLAN almost always outperforms LaMDA-PT
On 20 of 25 tasks, zero-shot FLAN outperforms zero-shot GPT-3
On 10 tasks, zero-shot FLAN even outperforms few-shot GPT-3

Results: NLI, reading comprehension, closed-book QA
54

Flan-U-PaLM-540B
Flan-T5-XXL (11B)
Flan-PaLM-540B
https://arxiv.org/abs/2210.11416
Scaling instruction-finetuned language models.
{H. Chung, L. Hou, S. Longpre}, …, and J. Wei. arXiv 2022.

Instruction finetuning
# Tasks
Largest 
model size 
(# params)
T0 (Sanh et al., 2021)
170
11B
Tk-Instruct (Wang et al., 2022)
1.5K
11B
FLAN-1 (Wei et al., 2021)
62
137B
Multi-task prompted training enables zero-shot task generalization. Sanh et al., 2021.
Benchmarking generalization via in-context instructions on 1,600+ language tasks. Wang et al., 2022.
Finetuned language models are zero-shot learners. Wei et al., 2021.
Flan (this paper)
1.8K
540B
Scaling instruction-finetuned language models.
{H. Chung, L. Hou, S. Longpre}, …, and J. Wei. arXiv 2022.

Scaling finetuning tasks
Task collections from prior 
work are combined. 
Held-out tasks are 
challenging and not 
included in the finetuning 
datasets.
Dataset with 
chain-of-thought 
annotations are 
included to enable 
reasoning.
Scaling instruction-finetuned language models.
{H. Chung, L. Hou, S. Longpre}, …, and J. Wei. arXiv 2022.

Instructional templates
We train on examples 
without and without 
exemplars, and with and 
without chain-of-thought.
Scaling instruction-finetuned language models.
{H. Chung, L. Hou, S. Longpre}, …, and J. Wei. arXiv 2022.

Flan models
Instruction 
finetuning is 
compute efficient!
They have different 
architecture and 
pre-training objectives.
The models we train span 
a variety of sizes (80M to 
540B parameters).
Scaling instruction-finetuned language models.
{H. Chung, L. Hou, S. Longpre}, …, and J. Wei. arXiv 2022.

Scaling model size and number of finetuning tasks
Scaling improves performance 
by a lot. Instruction finetuning 
further improves performance 
by a large margin.
Scaling finetuning tasks also 
improves performance, though 
with a decreasing effect
Scaling instruction-finetuned language models.
{H. Chung, L. Hou, S. Longpre}, …, and J. Wei. arXiv 2022.

Flan-PaLM achieves new SoTA on several benchmarks
Combining Flan-PaLM with CoT 
prompting and self-consistency 
achieves new state-of-the-art 
on three benchmark suites.
Self-consistency improves chain-of-thought reasoning in large language models. Wang et al., 2022.
Scaling instruction-finetuned language models.
{H. Chung, L. Hou, S. Longpre}, …, and J. Wei. arXiv 2022.

Instruction finetuning unlocks zero-shot reasoning
Instruction-finetuning with CoT data 
unlocks zero-shot reasoning via “let’s 
think step-by-step”.
Scaling instruction-finetuned language models.
{H. Chung, L. Hou, S. Longpre}, …, and J. Wei. arXiv 2022.

Usability evaluation
Flan-PaLM is preferred by human 
raters compared to PaLM.
Scaling instruction-finetuned language models.
{H. Chung, L. Hou, S. Longpre}, …, and J. Wei. arXiv 2022.

Public Flan-T5 checkpoints
We release public Flan-T5 
checkpoints, which have very 
strong performance even 
compared to larger models.
Models: https://huggingface.co/docs/transformers/model_doc/flan-t5
Scaling instruction-finetuned language models.
{H. Chung, L. Hou, S. Longpre}, …, and J. Wei. arXiv 2022.

Flan takeaways
Scaling instruction-finetuning 
(# tasks, # model parameters) 
improves performance.
Instruction finetuning also improves 
zero-shot reasoning and model usability 
by a large margin.
More? See the paper:
●
Flan-T5 results and public checkpoints
●
Ablations on CoT
●
Qualitative examples
●
Exact experimental results
●
Etc.
https://arxiv.org/abs/2210.11416
Scaling instruction-finetuned language models.
{H. Chung, L. Hou, S. Longpre}, …, and J. Wei. arXiv 2022.

Conclusions of talk
●
Language models acquire emergent abilities as they get scaled up (emergent abilities 
survey).
●
One is the ability to do multi-step reasoning (chain of thought and follow-up work).
●
Language models can be greatly enhanced via instruction finetuning, which improves 
few-shot performance, reasoning, and usability (Flan).
●
There are reasons to believe that language models will continue to get bigger and better.
○
Even more new abilities may emerge :)

Looking forward (just my personal interests)
●
Scaling
●
Better prompting and characterization of language model abilities
●
Applied work (therapy, creative writing, science)
●
Benchmarks
●
Compute-efficient methods for better language models

Thanks.
jasonwei@google.com

