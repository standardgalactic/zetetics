Simple and Principled Uncertainty Estimation with
Deterministic Deep Learning via Distance Awareness
Jeremiah Zhe Liu∗
Google Research & Harvard University
jereliu@google.com
Zi Lin†
Google Research
lzi@google.com
Shreyas Padhy†
Google Research
shreyaspadhy@google.com
Dustin Tran
Google Research
trandustin@google.com
Tania Bedrax-Weiss
Google Research
tbedrax@google.com
Balaji Lakshminarayanan
Google Research
balajiln@google.com
Abstract
Bayesian neural networks and deep ensembles are principled approaches to esti-
mate the predictive uncertainty of a deep learning model. However their practicality
in real-time, industrial-scale applications are limited due to their heavy memory
and inference cost. This motivates us to study principled approaches to high-quality
uncertainty estimation that require only a single deep neural network (DNN). By
formalizing the uncertainty quantiﬁcation as a minimax learning problem, we ﬁrst
identify distance awareness, i.e., the model’s ability to properly quantify the dis-
tance of a testing example from the training data manifold, as a necessary condition
for a DNN to achieve high-quality (i.e., minimax optimal) uncertainty estimation.
We then propose Spectral-normalized Neural Gaussian Process (SNGP), a simple
method that improves the distance-awareness ability of modern DNNs, by adding
a weight normalization step during training and replacing the output layer with a
Gaussian Process. On a suite of vision and language understanding tasks and on
modern architectures (Wide-ResNet and BERT), SNGP is competitive with deep
ensembles in prediction, calibration and out-of-domain detection, and outperforms
the other single-model approaches.3
1
Introduction
Efﬁcient methods that reliably quantify a deep neural network (DNN)’s predictive uncertainty
are important for industrial-scale, real-world applications, which include examples such as object
recognition in autonomous driving [22], ad click prediction in online advertising [76], and intent
understanding in a conversational system [84]. For example, for a natural language understanding
(NLU) model built for a domain-speciﬁc chatbot service (e.g, weather inquiry), the user’s input
utterance to the model can be of any topic, and the model needs to understand reliably and in real-time
whether to abstain or to trigger one of its known APIs.
When deep classiﬁers make predictions on input examples that are far from the support of the training
set, their performance can be arbitrarily bad [4, 14]. This motivates the need for methods that are
aware of the distance between an input test example and previously seen training examples, so they
can return a uniform (i.e., maximum entropy) distribution over output labels if the input is too far
from the training set (i.e., the input is out-of-domain) [30]. Gaussian processes (GPs) with suitable
kernels enjoy such a property. However, to apply Gaussian processes to a high-dimensional machine
∗Work done at Google Research.
†Work done as an Google AI Resident.
3Code available at https://github.com/google/uncertainty-baselines/tree/master/baselines.
34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
arXiv:2006.10108v2  [cs.LG]  26 Oct 2020

learning problem, it is usually necessary to perform some form of feature extraction or dimensionality
reduction using a DNN. Ideally, the hidden representation of a DNN should reﬂect a meaningful
distance in the data manifold (e.g., the semantic textual similarity between two sentences), such that
this “distance aware” property is preserved. However, as we will show in the experiments, this is
often not guaranteed for common deep learning models (cf. Figure 1).
(a) Gaussian Process
(b) Deep Ensemble
(c) MC Dropout
(d) DNN-GP
(e) SNGP (Ours)
(f) Gaussian Process
(g) Deep Ensemble
(h) MC Dropout
(i) DNN-GP
(j) SNGP (Ours)
Figure 1: The uncertainty surface of a GP and different DNN approaches on the two ovals (Top Row) and
two moons (Bottom Row) 2D classiﬁcation benchmarks. SNGP is the only DNN-based approach achieving a
distance-aware uncertainty similar to the gold-standard GP. Training data for positive (Orange) and negative
classes (Blue). OOD data (Red) not observed during training. Background color represents the estimated model
uncertainty (See 1e and 1j for color map). See Section 5.1 for details.
We propose a simple solution to this problem, namely adding spectral normalization to the weights
in each (residual) layer [54]. We refer to our method as ”Spectral-normalized Neural Gaussian
Processes” (SNGP). We show that this provides bounds on ||h(x)−h(x′)||H relative to ||x−x′||X,
where x and x′ are two inputs, h(x) is a deep feature extractor, and ||.||X a semantically meaningful
distance for the data manifold. We can then safely pass h(x) into a distance-aware GP output layer.
To ensure computational scalability, we approximate the GP posterior using a Laplace approximation
to the random feature expansion of the GP, which gives rise to a model posterior that can be learned
scalably and in closed-form with minimal modiﬁcation to the training pipeline of a deterministic
DNN, and allows us to efﬁciently compute the predictive uncertainty on a per-input basis without
Monte Carlo sampling.
In the rest of this paper, we ﬁrst theoretically motivate the importance of distance awareness for
a model’s ability uncertainty estimation by studying it as a minimax learning problem (Section
2). We then introduce our SNGP method in detail in Section 3, and experimentally evaluate its
performance against other single-model approaches as well as deep ensembles in Section 5 [42].
On two challenging real world problems, namely image classiﬁcation (using a Wide Resnet model
on CIFAR-10 and CIFAR-100) and conversational intent understanding (using a BERT model on
CLINC out-of-scope (OOS) intent dataset), we show that the SNGP method attains an uncertainty
performance (e.g., calibration and out-of-domain (OOD) detection) that is competitive with that of a
deep ensemble, while maintaining the accuracy and latency of a single deterministic DNN.
2
Distance Awareness: An Important Condition for High-Quality
Uncertainty Estimation
Notation and Problem Setup
Consider a data-generation distribution p∗(y|x), where y ∈
{1,...,K} is the space of K-class labels, and x ∈X ⊂Rd is the input data manifold equipped
with a suitable metric ||.||X. In practice, the training data D = {yi,xi}N
i=1 is often collected from a
subset of the full input space XIND ⊂X . As a result, the full data-generating distribution p∗(y|x) is
in fact a mixture of an in-domain (IND) distribution pIND(y|x) = p∗(y|x,x ∈XIND) and also an OOD
distribution pOOD(y|x) = p∗(y|x,x ̸∈XIND) [52, 66]:
p∗(y|x) =
p∗(y,x ∈XIND|x)
+
p∗(y,x ̸∈XIND|x)
= p∗(y|x,x ∈XIND)∗p∗(x ∈XIND)+ p∗(y|x,x ̸∈XIND)∗p∗(x ̸∈XIND).
(1)
2

During training, the model learns the in-domain distribution p∗(y|x,x ∈XIND) from the data D, but
does not have knowledge about p∗(y|x,x ̸∈XIND). In the weather-service chatbot example, the out-
of-domain space XOOD = X /XIND is the space of all natural utterances not related to weather queries,
whose elements usually do not have a meaningful correspondence with the in-domain intent labels
yk ∈{1,...,K}. Therefore, the out-of-domain distribution p∗(y|x,x ̸∈XIND) can be very different
from the in-domain distribution p∗(y|x,x ∈XIND), and we only expect the model to generalize well
within XIND. However, during testing, the model needs to construct a predictive distribution p(y|x)
for the entire input space X = XIND ∪XOOD, since the users’ utterances can be of any topic.
2.1
Uncertainty Estimation as a Minimax Learning Problem
To formulate the uncertainty estimation as a learning problem under (1), we need to deﬁne a loss
function to measure a model p(y|x)’s quality of predictive uncertainty. A popular uncertainty
metric is Expected Calibration Error (ECE), deﬁned as C(p, p∗) = E

|E(y∗= ˆy| ˆp = p)−p|

, which
measures the difference in expectation between the model’s predictive conﬁdence (e.g., the maximum
probability score) and its actual accuracy [29, 56]. However, ECE is not suitable as a loss function,
since it is not uniquely minimized at p = p∗. Speciﬁcally, there can exist a trivial predictor that
ignores the input example and achieves perfect calibration by predicting randomly according to the
marginal distribution of the labels [24].
To this end, a theoretically more well-founded uncertainty metric is to examine strictly proper scoring
rules [25] s(., p∗), which are uniquely minimized by the true distribution p = p∗. Examples include
log-loss and Brier score. Proper scoring rules are related to ECE in that each is an upper bound of the
calibration error by the classic calibration-reﬁnement decomposition [10]. Therefore, minimizing
a proper scoring rule implies minimizing the calibration error of the model. Consequently, we
can formalize the problem of uncertainty quantiﬁcation as the problem of constructing an optimal
predictive distribution p(y|x) to minimize the expected risk over the entire x ∈X , i.e., an Uncertainty
Risk Minimization problem:
inf
p∈P S(p, p∗) = inf
p∈P E
x∈X

s(p, p∗|x)

.
(2)
Unfortunately, directly minimizing (2) over the entire input space X is not possible even with
inﬁnite amounts of data. This is because since the data is collected only from XIND, the true OOD
distribution p∗(y|x,x ̸∈XIND) is never learned by the model, and generalization is not guaranteed
since p∗(y|x,x ∈XIND) and p∗(y|x,x ̸∈XIND) are not assumed to be similar. As a result, the naive
practice of using a model trained only with in-domain data to generate OOD predictions can lead to
arbitrarily bad results, since nature can happen to produce an OOD distribution p∗(y|x,x ̸∈XIND)
that is at odds with the model prediction. This is clearly undesirable for safety-critical applications.
To this end, a more prudent strategy is to instead minimize the worst-case risk with respect to all
possible p∗∈P∗, i.e., construct p(y|x) to minimize the Minimax Uncertainty Risk:
inf
p∈P
h
sup
p∗∈P∗S(p, p∗)
i
.
(3)
In game-theoretic nomenclature, the uncertainty estimation problem acts as a two-player game of
model v.s. nature, where the goal of the model is to produce a minimax strategy p that minimizes the
risk S(p, p∗) against all possible (even adversarial) moves p∗of nature. Under the classiﬁcation task
and for Brier score, the solution to the minimax problem (3) adopts a simple and elegant form:
p(y|x) = p(y|x,x ∈XIND)∗p∗(x ∈XIND)+ puniform(y|x,x ̸∈XIND)∗p∗(x ̸∈XIND).
(4)
This is very intuitive: if an input point is in the training data domain, trust the model, otherwise use a
uniform (maximum entropy) prediction. For the practice of uncertainty estimation, (4) is conceptually
important in that it veriﬁes that there exists a unique optimal solution to the uncertainty estimation
problem (3). Furthermore, this optimal solution can be constructed conveniently. Speciﬁcally, it can
be constructed as a mixture of a discrete uniform distribution puniform and the in-domain predictive
distribution p(y|x,x ∈XIND) that the model has already learned from data, assuming one can quantify
p∗(x ∈XIND) well. In fact, the expression (4) can be shown to be optimal for a broad family of
scoring rules known as the Bregman score, which includes the Brier score and the widely used log
score as the special cases. We derive (4) in Appendix B.
2.2
Input Distance Awareness as a Necessary Condition
In light of Equation (4), a key capacity for a deep learning model to reliably estimate predictive
uncertainty is its ability to quantify, either explicitly or implicitly, the domain probability p(x ∈XIND).
3

This requires the model to have a good notion of the distance (or dissimilarity) between a testing
example x and the training data XIND with respect to a meaningful distance ||.||X for the data manifold
(e.g., semantic textual similarity [12] for language data). Deﬁnition 1 makes this notion more precise:
Deﬁnition 1 (Input Distance Awareness). Consider a predictive distribution p(y|x) trained on a
domain XIND ⊂X , where (X ,||.||X) is the input data manifold equipped with a suitable metric
||.||X. We say p(y|x) is input distance aware if there exists u(x) a summary statistic of p(y|x) that
quantiﬁes model uncertainty (e.g., entropy, predictive variance, etc) that reﬂects the distance between
x and the training data with respect to ||.||X, i.e.,
u(x) = v
 d(x,XIND)

where v is a monotonic function and d(x,XIND) = Ex′∼XIND||x−x′||2
X. is the distance between x and
the training data domain.
A classic model that satisﬁes the distance-awareness property is a Gaussian process (GP) with a
radial basis function (RBF) kernel. Its predictive distribution p(y|x) = softmax(g(x)) is a soft-
max transformation of the GP posterior g ∼GP under the cross-entropy likelihood, and its pre-
dictive uncertainty can be expressed by the posterior variance u(x∗) = var(g(x∗)) = 1 −k∗⊤Vk∗
for k∗
i = exp(−1
2l ||x∗−xi||2
X) and VN×N a ﬁxed matrix determined by data. Then u(x∗) increases
monotonically toward 1 as x∗moves further away from XIND [61]. In view of the expression (4), the
input distance awareness property is important for both calibration and OOD detection. However,
this property is not guaranteed for a typical deep learning model [33]. Consider a discriminative
deep classiﬁer with dense output layer logitk(x) = h(x)⊤β k, whose model conﬁdence (i.e., maximum
predictive probability) is characterized by the magnitude of the class logits, which is deﬁned by the
inner product distances between the hidden representation h(x) and the decision boundaries {β k}K
k=1
(see, e.g., Figure 1b-1c and 1g-1h). As a result, the model computes conﬁdence for a x∗based not on
its distance from the training data XIND, but based on its distance from the decision boundaries, i.e.,
the model uncertainty is not input distance aware.
Two Conditions for Input Distance Awareness in Deep Learning Notice that a deep learning
model logit(x) = g◦h(x) is commonly composed of a hidden mapping h : X →H that maps the
input x into a hidden representation space h(x) ∈H , and an output layer g that maps h(x) to the label
space. To this end, a DNN logit(x) = g◦h(x) can be made input distance aware via a combination
of two conditions: (1) make the output layer g distance aware, so it outputs an uncertainty metric
reﬂecting distance in the hidden space ||h(x)−h(x′)||H (in practice, this can be achieved by using
a GP with a shift-invariant kernel as the output layer), and (2) make the hidden mapping distance
preserving (deﬁned below), so that the distance in the hidden space ||h(x)−h(x′)||H has a meaningful
correspondence to the distance ||x−x′||X in the data manifold. From the mathematical point of view,
this is equivalent to requiring h to satisfy the bi-Lipschitz condition [67]:
L1 ∗||x1 −x2||X ≤||h(x1)−h(x2)||H ≤L2 ∗||x1 −x2||X,
(5)
for positive and bounded constants 0 < L1 < 1 < L2. It is worth noticing that for a deep learning
model, the bi-Lipschitz condition (5) usually leads the model’s hidden space to preserve a semantically
meaningful distance in the input data manifold X , rather than a naive metric such as the square
distance in the pixel space. This is because that the upper Lipschitz bound ||h(x1) −h(x2)||H ≤
L2 ∗||x1 −x2||X is an important condition for the adversarial robustness of a deep network, which
prevents the hidden representations h(x) from being overly sensitive to the semantically meaningless
perturbations in the pixel space [65, 80, 75, 37, 71]. On the other hand, the lower Lipschitz bound
||h(x1)−h(x2)||H ≥L1 ∗||x1 −x2||X prevents the hidden representation from being unnecessarily
invariant to the semantically meaningful changes in the input manifold [38, 77]. Combined together,
the bi-Lipschitz condition essentially encourages h to be an approximately isometric mapping, thereby
ensuring that the learned representation h(x) has a robust and meaningful correspondence with the
semantic properties of the input data x. Although not stated explicitly, learning an approximately
isometric and geometry-preserving mapping is a common goal in machine learning. For example,
image classiﬁers strive to learn a mapping from image manifold to a hidden space that can be well-
separated by a set of linear decision boundaries, and sentences encoders aim to project sentences into
a vector space where the cosine distance reﬂects the semantic similarity in natural language. Finally,
it is worth noting that preserving such approximate isometry in a neural network is possible even
after signiﬁcant dimensionality reduction [8, 32, 59, 64].
4

3
SNGP: A Simple Approach to Distance-aware Deep Learning
In this section we propose Spectral-normalized Neural Gaussian Process (SNGP), a simple method
to improve the input distance awareness ability of a modern residual-based DNN (e.g., ResNet,
Transformer) by (1) making the output layer distance aware and (2) making the hidden layers
distance preserving, as discussed in Section 2.2. Full method is summarized in Algorithms 1-2.
3.1
Distance-aware Output Layer via Laplace-approximated Neural Gaussian Process
To make the output layer g : H →Y distance aware, SNGP replaces the typical dense output layer
with a Gaussian process (GP) with an RBF kernel, whose posterior variance at x∗is characterized by
its L2 distance from the training data in the hidden space. Speciﬁcally, given N training samples D =
{yi,xi}N
i=1 and denoting hi = h(xi), the Gaussian-process output layer gN×1 = [g(h1),...,g(hN)]⊤
follows a multivariate normal distribution a priori:
gN×1 ∼MVN(0N×1,KN×N),where Ki,j = exp(−||hi −h j||2
2/2),
(6)
and the posterior distribution is computed as p(g|D) ∝p(D|g)p(g) where p(g) is the GP prior in
(6) and p(D|g) is the data likelihood for classiﬁcation (i.e., the exponentiated cross-entropy loss).
However, computing the exact Gaussian process posterior for a large-scale classiﬁcation task is
both analytically intractable and computationally expensive, In this work, we propose a simple
approximation strategy for GP that is based on a Laplace approximation to the random Fourier feature
(RFF) expansion of the GP posterior [61]. Our approach gives rise to a closed-form posterior that
is end-to-end trainable with the rest of the neural network, and empirically leads to an improved
quality in estimating the posterior uncertainty. Speciﬁcally, we ﬁrst approximate the GP prior in (6)
by deploying a low-rank approximation to the kernel matrix K = ΦΦ⊤using random features [60]:
gN×1 ∼MVN(0N×1,ΦΦ⊤
N×N),
where
Φi,DL×1 =
p
2/DL ∗cos(−WLhi +bL),
(7)
where hi = h(xi) is the hidden representation in the penultimate layer with dimension DL−1. Φi is
the ﬁnal layer with dimension DL, it contains WL,DL×DL−1 a ﬁxed weight matrix whose entries are
sampled i.i.d. from N(0,1), and bL,DL×1 a ﬁxed bias term whose entries are sampled i.i.d. from
Uni form(0,2π). As a result, for the kth logit, the RFF approximation to the GP prior in (6) can be
written as a neural network layer with ﬁxed hidden weights W and learnable output weights β k:
gk(hi) =
p
2/DL ∗cos(−WLhi +bL)⊤β k,
with prior
β k,DL×1 ∼N(0,IDL×DL).
(8)
Notice that conditional on h, β = {β k}K
k=1 is the only learnable parameter in the model. As a
result, the RFF approximation in (8) reduces an inﬁnite-dimensional GP to a standard Bayesian
linear model, for which many posterior approximation methods (e.g., expectation propagation (EP))
can be applied [53]. In this work, we choose the Laplace method due to its simplicity and the
fact that its posterior variance has a convenient closed form [61]. Brieﬂy, the Laplace method
approximates the RFF posterior p(β|D) using a Gaussian likelihood centered around the maximum
a posterior (MAP) estimate ˆβ = argmaxβ p(β|D), such that p(βk|D) ≈MVN( ˆβk, ˆΣk = ˆH−1
k ), where
ˆHk,(i,j) =
∂2
∂βi∂βj log p(βk|D)|βk= ˆβk is the DL × DL Hessian matrix of the log posterior likelihood
evaluated at the MAP estimates. Under the linear-model formulation of the RFF posterior, the
posterior precision matrix (i.e., the inverse covariance matrix) adopts a simple expression ˆΣ
−1
k
=
I+∑N
i=1 ˆpi,k(1−ˆpi,k)ΦiΦ⊤
i , where pi,k is the model prediction softmax( ˆgi) under the MAP estimates
ˆβ = {βk}K
k=1 [61]. To summarize, the Laplace posterior for GP under the RFF approximation is:
βk|D ∼MVN( ˆβk, ˆΣk),
where
ˆΣ
−1
k
= I+
N
∑
i=1
ˆpi,k(1−ˆpi,k)ΦiΦ⊤
i .
(9)
During minibatch training, the posterior mean ˆβ is updated via regular stochastic gradient descent
(SGD) with respect to the (unnormalized) log posterior −log p(β|D) = −log p(D|β) + 1
2||β||2
where −log p(D|β) is the cross-entropy loss. The posterior precision matrix is updated cheaply as
ˆΣ
−1
k,t = (1−m)∗ˆΣ
−1
k,t−1 +m∗∑M
i=1 ˆpi,k(1−ˆpi,k)ΦiΦ⊤
i for a minibatch of size M and m a small scaling
coefﬁcient. This computation only needs to be performed by passing through training data once at
the ﬁnal epoch. As a result, the GP posterior (9) can be learned scalably and in closed-form with
minimal modiﬁcation to the training pipeline of a deterministic DNN. It is worth noting that the
Laplace approximation to the RFF posterior is asymptotically exact by the virtue of the Bernstein-von
Mises (BvM) theorem and the fact that (8) is a ﬁnite-rank model [16, 23, 46, 57].
5

3.2
Distance-preserving Hidden Mapping via Spectral Normalization
Replacing the output layer g with a Gaussian process only allows the model logit(x) = g◦h(x) to
be aware of the distance in the hidden space ||h(x1) −h(x2)||H. It is also important to ensure the
hidden mapping h is distance preserving so that the distance in the hidden space ||h(x)−h(x′)||H
has a meaningful correspondence to the distance in the input space ||x−x′||X. To this end, we notice
that modern deep learning models (e.g., ResNets, Transformers) are commonly composed of residual
blocks, i.e., h(x) = hL−1 ◦···◦h2 ◦h1(x) where hl(x) = x+gl(x). For such models, there exists a
simple method to ensure h is distance preserving: by bounding the Lipschitz constants of all nonlinear
residual mappings {gl}L−1
l=1 to be less than 1. We state this result formally below:
Proposition 1 (Lipschitz-bounded residual block is distance preserving [3]). Consider a hidden
mapping h : X →H with residual architecture h = hL−1 ◦...h2 ◦h1 where hl(x) = x+gl(x). If for
0 < α ≤1, all gl’s are α-Lipschitz, i.e., ||gl(x)−gl(x′)||H ≤α||x−x′||X
∀(x,x′) ∈X . Then:
L1 ∗||x−x′||X ≤||h(x)−h(x′)||H ≤L2 ∗||x−x′||X,
where L1 = (1−α)L−1 and L2 = (1+α)L−1, i.e., h is distance preserving.
Proof is in Appendix E.1. The ability of a residual network to construct a geometry-preserving metric
transform between the input space X and the hidden space H is well-established in learning theory
and generative modeling literature, but the application of these results in the context of uncertainty
estimation for DNN appears to be new [3, 5, 32, 64].
Consequently, to ensure the hidden mapping h is distance preserving, it is sufﬁcient to ensure that
the weight matrices for the nonlinear residual block gl(x) = σ(Wlx+bl) to have spectral norm (i.e.,
the largest singular value) less than 1, since ||gl||Lip ≤||Wlx+bl||Lip ≤||Wl||2 ≤1. In this work,
we enforce the aforementioned Lipschitz constraint on gl’s by applying the spectral normalization
(SN) on the weight matrices {Wl}L−1
l=1 as recommended in [5]. Brieﬂy, at every training step, the SN
method ﬁrst estimate the spectral norm ˆλ ≈||Wl||2 using the power iteration method [26, 54], and
then normalizes the weights as:
Wl =
(
c∗Wl/ˆλ
if c < ˆλ
Wl
otherwise
(10)
where c > 0 is a hyperparameter used to adjust the exact spectral norm upper bound on ||Wl||2 (so
that ||Wl||2 ≤c). This hyperparameter is useful in practice since the other regularization mechanisms
(e.g., Dropout, Batch Normalization) in the hidden layers can rescale the Lipschitz constant of the
original residual mapping [26]. Therefore, (10) allows us more ﬂexibility in controlling the spectral
norm of the neural network weights so it is the most compatible with the architecture at hand.
Method Summary We summarize the method in Algorithms 1-2. As shown, for every minibatch
step, the model ﬁrst updates the hidden-layer weights {Wl,bl}L−1
l=1 and the trainable output weights
β = {βk}K
k=1 via SGD, then performs spectral normalization, and ﬁnally (if in the ﬁnal epoch)
performs precision matrix update (Equation (9). We discuss further details (e.g. computational
complexity) in Appendix A.
Algorithm 1 SNGP Training
1: Input:
Minibatches {Di}N
i=1 for Di = {ym,xm}M
m=1.
2: Initialize:
ˆΣ = I,WL
iid∼N(0,1),bL
iid∼U(0,2π)
.
3: for train step = 1 to max step do
4:
SGD update
n
β,{Wl}L−1
l=1 ,{bl}L−1
l=1
o
5:
Spectral Normalization {Wl}L−1
l=1 (10).
6:
if ﬁnal epoch then
7:
Update precision matrix {ˆΣ
−1
k }K
k=1 (9).
8:
end if
9: end for
10: Compute posterior covariance ˆΣk = inv(ˆΣ
−1
k ).
Algorithm 2 SNGP Prediction
1: Input: Testing example x.
2: Compute Feature:
ΦDL×1 =
p
2/DL ∗cos(WLh(x)+bL),
3: Compute Posterior Mean:
logitk(x) = Φ⊤β k
4: Compute Posterior Variance:
vark(x) = Φ⊤ˆΣkΦ.
5: Compute Predictive Distribution:
p(y|x) =
Z
m∼N(logit(x),var(x)) softmax(m)
6

4
Related Work
Single-model approaches to deep classiﬁer uncertainty Recent work examines uncertainty meth-
ods that add few additional parameters or runtime cost to the base model. The state-of-the-art on
large-scale tasks are efﬁcient ensemble methods [79, 21], which cast a set of models under a single
one, encouraging independent member predictions using low-rank perturbations. These methods are
parameter-efﬁcient but still require multiple forward passes from the model. SNGP investigates an
orthogonal approach that improves the uncertainty quantiﬁcation by imposing suitable regularization
on a single model, and therefore requires only a single forward pass during inference. There exists
other runtime-efﬁcient, single-model approaches to estimate predictive uncertainty, achieved by
either replacing the loss function [33, 50, 51, 68, 69], the output layer [6, 72, 11, 48], or computing a
closed-form posterior for the output layer [62, 70, 41]. SNGP builds on these approaches by also
considering the intermediate representations which are necessary for good uncertainty estimation,
and proposes a simple method (spectral normalization) to achieve it. A recent method named De-
terministic Uncertainty Quantiﬁcation (DUQ) also regulates the neural network mapping but uses a
two-sided gradient penalty [77]. The two-sided gradient penalty can be undesirable for a residual
network, since imposing ||∇f|| = 1 onto a residual connection f(x) = x+g(x) can force g(x) toward
0, leading to an identity mapping. We compare with DUQ in our experiments.
Laplace approximation and GP inference with DNN Laplace approximation has a long history
in GP and NN literature [73, 17, 61, 49, 63], and the theoretical connection between a Laplace-
approximated DNN and GP has being explored recently [40]. Differing from these works, SNGP
applies the Laplace approximation to the posterior of a neural GP, rather than to a shallow GP or
a dense-output-layer DNN. Earlier works that combine a GP with a DNN usually perform MAP
estimation [11] or structured Variational Inference (VI) [9, 81]. These approaches were shown to
lead to poor calibration by recent work [74], which proposed a simple ﬁx by combing Monte Carlo
Dropout (MC Dropout) with random Fourier features, which we term Calibrated Deep Gaussian
Process (MCD-GP). SNGP differs from MCD-GP in that it considered a different regularization
approach (spectral normalization) and can compute its posterior uncertainty more efﬁciently in a
single forward pass. We compare with MCD-GP in our experiments. Appendix D contains further
related work on distance-preserving neural networks and open-set classiﬁcation.
5
Experiments
5.1
2D Synthetic Benchmark
We ﬁrst study the behavior of the uncertainty surface of a SNGP model under a suite of 2D clas-
siﬁcation benchmarks. Speciﬁcally, we consider the two ovals benchmark (Figure 1, row 1) and
the two moons benchmark (Figure 1, row 2). The two ovals benchmark consists of two near-ﬂat
Gaussian distributions, which represent the two in-domain classes (orange and blue) that are separable
by a linear decision boundary. There also exists an OOD distribution (red) that the model doesn’t
observe during training. Similarly, the two moons dataset consists of two banana-shaped distributions
separable by a nonlinear decision boundary. We consider a 12-layer, 128-unit deep architecture
ResFFN-12-128. The full experimental details are in Appendix C.
Figure 1 shows the results, where the background color visualizes the uncertainty surface output by
each model. We ﬁrst notice that the shallow Gaussian process models (Figures 1a and 1f) exhibit
an expected behavior for high-quality predictive uncertainty: it generates low uncertainty in XIND
that is supported by the training data (purple color), and generates high uncertainty when x is far
from XIND (yellow color), i.e., input distance awareness. As a result, the shallow GP model is able to
assign low conﬁdence to the OOD data (colored in red), indicating reliable uncertainty quantiﬁcation.
On the other hand, deep ensembles (Figures 1b, 1g) and MC Dropout (Figures 1c, 1h) are based on
dense output layers that are not distance aware. As a result, both methods quantify their predictive
uncertainty based on the distance from the decision boundaries, assigning low uncertainty to OOD
examples even if they are far from the data. Finally, the DNN-GP (Figures 1d and1i) and SNGP
(Figures 1e and1j) both use GP as their output layers, but with SNGP additionally imposing the
spectral normalization on its hidden mapping h(.). As a result, the DNN-GP’s uncertainty surfaces
are still strongly impacted by the distance from decision boundary, likely caused by the fact that the
un-regularized hidden mapping h(x) is free to discard information that is not relevant for prediction.
On the other hand, the SNGP is able to maintain the input distance awareness property via its
7

bi-Lipschitz constraint, and exhibits a uncertainty surface that is analogous to the gold-standard
model (shallow GP) despite the fact that SNGP is based on a 12-layer network.
5.2
Vision and Language Understanding
Baseline Methods
All methods included in the vision and language understanding experiments are
summarized in Table 1. Speciﬁcally, we evaluate SNGP on a Wide ResNet 28-10 [83] for image
classiﬁcation, and BERTbase [18] for language understanding. We compare against a deterministic
baseline and two ensemble approaches: MC Dropout (with 10 dropout samples) and deep ensem-
bles (with 10 models), all trained with a dense output layer and no spectral regularization. We
consider three single-model approaches: MCD-GP (with 10 samples), Deterministic Uncertainty
Quantiﬁcation (DUQ) (see Section 4). For all models that use GP layer, we keep DL = 1024 and
compute predictive distribution by performing Monte Carlo averaging with 10 samples. We also
include two ablated version of SNGP: DNN-SN which uses spectral normalization on its hidden
weights and a dense output layer (i.e. distance preserving hidden mapping without distance-aware
output layer), and DNN-GP which uses the GP as output layer but without spectral normalization
on its hidden layers (i.e., distance-aware output layer without distance-preserving hidden mapping).
Further experiment details and recommendations for practical implementation are in Appendix C. All
baselines are built on the uncertainty baselines framework.
Additional
Output
Ensemble
Multi-pass
Methods
Regularization
Layer
Training
Inference
Deterministic
-
Dense
-
-
MC Dropout
Dropout
Dense
-
Yes
Deep Ensemble
-
Dense
Yes
Yes
MCD-GP
Dropout
GP
-
Yes
DUQ
Gradient Penalty
RBF
-
-
DNN-SN
Spec Norm
Dense
-
-
DNN-GP
-
GP
-
-
SNGP
Spec Norm
GP
-
-
Table 1: Summary of methods used in experiments. Multi-pass Inference refers to whether the method needs to
perform multiple forward passes to generate the predictive distribution.
CIFAR-10 and CIFAR-100 We evaluate the model’s predictive accuracy and calibration error under
both clean CIFAR testing data and its corrupted versions termed CIFAR-*-C [34]. To evaluate the
model’s OOD detection performance, we consider two tasks: a standard OOD task using SVHN as
the OOD dataset for a model trained on CIFAR-10/-100, and a difﬁcult OOD task using CIFAR-100
as the OOD dataset for a model trained on CIFAR-10, and vice versa. We compute the uncertainty
score for OOD using the Dempster-Shafer metric as introduced in [68], which empirically leads
to better performance for distance-aware models (see Appendix C). Table 2-3 reports the results.
As shown, for predictive accuracy, SNGP is competitive with that of a deterministic network, and
outperforms the other single-model approaches. For calibration error, SNGP clearly outperforms the
other single-model approaches and is competitive with the deep ensemble. Finally, for OOD detection,
SNGP outperforms not only the deep ensembles and MC Dropout approaches that are based on a
dense output layer, but also the MCD-GP and DUQ that are based on the GP layer, illustrating the
importance of the input distance awareness property for high-quality performance in uncertainty
quantiﬁcation.
Accuracy (↑)
ECE (↓)
NLL (↓)
OOD AUPR (↑)
Latency (↓)
Method
Clean
Corrupted
Clean
Corrupted
Clean
Corrupted
SVHN
CIFAR-100
(ms / example)
Deterministic
96.0 ± 0.01
72.9 ± 0.01
0.023 ± 0.002
0.153 ± 0.011
0.158 ± 0.01
1.059 ± 0.02
0.781 ± 0.01
0.835 ± 0.01
3.91
MC Dropout
96.0 ± 0.01
70.0 ± 0.02
0.021 ± 0.002
0.116 ± 0.009
0.173 ± 0.01
1.152 ± 0.01
0.971 ± 0.01
0.832 ± 0.01
27.10
Deep Ensembles
96.6 ± 0.01
77.9 ± 0.01
0.010 ± 0.001
0.087 ± 0.004
0.114 ± 0.01
0.815 ± 0.01
0.964 ± 0.01
0.888 ± 0.01
38.10
MCD-GP
95.5 ± 0.02
70.0 ± 0.01
0.024 ± 0.004
0.100 ± 0.007
0.172 ± 0.01
1.157 ± 0.01
0.960 ± 0.01
0.863 ± 0.01
29.53
DUQ
94.7 ± 0.02
71.6 ± 0.02
0.034 ± 0.002
0.183 ± 0.011
0.239 ± 0.02
1.348 ± 0.01
0.973 ± 0.01
0.854 ± 0.01
8.68
DNN-SN
96.0 ± 0.01
72.5 ± 0.01
0.025 ± 0.004
0.178 ± 0.013
0.171 ± 0.01
1.306 ± 0.01
0.974 ± 0.01
0.859 ± 0.01
5.20
DNN-GP
95.9 ± 0.01
71.7 ± 0.01
0.029 ± 0.002
0.175 ± 0.008
0.221 ± 0.02
1.380 ± 0.01
0.976 ± 0.01
0.887 ± 0.01
5.58
SNGP (Ours)
95.9 ± 0.01
74.6 ± 0.01
0.018 ± 0.001
0.090± 0.012
0.138 ± 0.01
0.935 ± 0.01
0.990 ± 0.01
0.905 ± 0.01
6.25
Table 2: Results for Wide ResNet-28-10 on CIFAR-10, averaged over 10 seeds.
Detecting Out-of-Scope Intent in Conversational Language Understanding To validate the
method beyond image modalities, we also evaluate SNGP on a practical language understanding task
where uncertainty quantiﬁcation is of natural importance: dialog intent detection [44, 78, 82, 84]. In a
goal-oriented dialog system (e.g. chatbot) built for a collection of in-domain services, it is important
for the model to understand if an input natural utterance from an user is in-scope (so it can activate
8

Accuracy (↑)
ECE (↓)
NLL (↓)
OOD AUPR (↑)
Latency (↓)
Method
Clean
Corrupted
Clean
Corrupted
Clean
Corrupted
SVHN
CIFAR-10
(ms / example)
Deterministic
79.8 ± 0.02
50.5 ± 0.04
0.085 ± 0.004
0.239 ± 0.020
0.872 ± 0.01
2.756 ± 0.03
0.882 ± 0.01
0.745 ± 0.01
5.20
MC Dropout
79.6 ± 0.02
42.6 ± 0.08
0.050 ± 0.003
0.202 ± 0.010
0.825 ± 0.01
2.881 ± 0.01
0.832 ± 0.01
0.757 ± 0.01
46.79
Deep Ensemble
80.2 ± 0.01
54.1 ± 0.04
0.021 ± 0.004
0.138± 0.013
0.666 ± 0.02
2.281 ± 0.03
0.888 ± 0.01
0.780 ± 0.01
42.06
MCD-GP
79.5± 0.04
45.0 ± 0.05
0.085 ± 0.005
0.159 ± 0.009
0.937 ± 0.01
2.584 ± 0.02
0.873 ± 0.01
0.754 ± 0.01
44.20
DUQ
78.5 ± 0.02
50.4 ± 0.02
0.119 ± 0.001
0.281 ± 0.012
0.980 ± 0.02
2.841 ± 0.01
0.878 ± 0.01
0.732 ± 0.01
6.51
DNN-SN
79.9 ± 0.02
48.6 ± 0.02
0.098± 0.004
0.272± 0.011
0.918 ± 0.01
3.013± 0.01
0.879± 0.03
0.745± 0.01
6.20
DNN-GP
79.2 ± 0.03
47.7 ± 0.03
0.064± 0.005
0.166± 0.003
0.885± 0.009
2.629± 0.01
0.876± 0.01
0.746± 0.02
6.82
SNGP (Ours)
79.9 ± 0.03
49.0 ± 0.02
0.025 ± 0.012
0.117 ± 0.014
0.847 ± 0.01
2.626 ± 0.01
0.923 ± 0.01
0.801 ± 0.01
6.94
Table 3: Results for Wide ResNet-28-10 on CIFAR-100, averaged over 10 seeds.
one of the in-domain services) or out-of-scope (where the model should abstain). To this end, we
consider training an intent understanding model using the CLINC OOS intent detection benchmark
dataset [44]. Brieﬂy, the OOS dataset contains data for 150 in-domain services with 150 training
sentences in each domain, and also 1500 natural out-of-domain utterances. We train the models only
on in-domain data, and evaluate their predictive accuracy on the in-domain test data, their calibration
and OOD detection performance on the combined in-domain and out-of-domain data. The results
are in Table 4. As shown, consistent with the previous vision experiments, SNGP is competitive in
predictive accuracy when compared to a deterministic baseline, and outperforms other approaches in
calibration and OOD detection.
Accuracy (↑)
ECE (↓)
NLL (↓)
OOD
Latency (↓)
Method
AUROC (↑)
AUPR (↑)
(ms / example)
Deterministic
96.5 ± 0.11
0.024 ± 0.002
3.559 ± 0.11
0.897 ± 0.01
0.757 ± 0.02
10.42
MC Dropout
96.1 ± 0.10
0.021 ± 0.001
1.658 ± 0.05
0.938 ± 0.01
0.799 ± 0.01
85.62
Deep Ensemble
97.5 ± 0.03
0.013 ± 0.002
1.062 ± 0.02
0.964 ± 0.01
0.862 ± 0.01
84.46
MCD-GP
95.9 ± 0.05
0.015 ± 0.003
1.664 ± 0.04
0.906 ± 0.02
0.803 ± 0.01
88.38
DUQ
96.0 ± 0.04
0.059 ± 0.002
4.015 ± 0.08
0.917 ± 0.01
0.806 ± 0.01
15.60
DNN-SN
95.4 ± 0.10
0.037 ± 0.004
3.565 ± 0.03
0.922 ± 0.02
0.733 ± 0.01
17.36
DNN-GP
95.9 ± 0.07
0.075 ± 0.003
3.594 ± 0.02
0.941 ± 0.01
0.831 ± 0.01
18.93
SNGP
96.6 ± 0.05
0.014 ± 0.005
1.218 ± 0.03
0.969 ± 0.01
0.880 ± 0.01
17.36
Table 4: Results for BERTBase on CLINC OOS, averaged over 10 seeds.
6
Conclusion
We propose SNGP, a simple approach to improve a single deterministic DNN’s ability in predictive
uncertainty estimation. It makes minimal changes to the architecture and training/prediction pipeline
of a deterministic DNN, only adding spectral normalization to the hidden mapping, and replacing the
dense output layer with a random feature layer that approximates a GP. We theoretically motivate
input distance awareness, the key design principle behind SNGP, via a learning-theoretic analysis
of the uncertainty estimation problem. We also propose a closed-form approximation method to
make the GP posterior end-to-end trainable in linear time with the rest of the neural network. On a
suite of vision and language understanding tasks and on modern architectures (ResNet and BERT),
SNGP is competitive with a deep ensemble in prediction, calibration and out-of-domain detection,
and outperforms other single-model approaches.
A central observation we made in this work is that good representational learning is important
for good uncertainty quantiﬁcation. In particular, we highlighted bi-Lipschitz (Equation (5)) as
an important condition for the learned representation of a DNN to attain high-quality uncertainty
performance, and proposed spectral normalization as a simple approach to ensure such property in
practice. However, it is worth noting that there exists other representation learning techniques, e.g.,
data augmentation or unsupervised pretraining, that are known to also improve a network’s uncertainty
performance [35, 36]. Analyzing whether and how these approaches contribute to improve a DNN bi-
Lipschitz condition, and whether the bi-Lipschitz condition is sufﬁcient in explaining these methods’
success, are interesting avenues of future work. Furthermore, we note that the spectral norm bound
α < 1 in Proposition 1 forms only a sufﬁcient condition for ensuring bi-Lipschitz [5]. In practice, we
observed that for convolutional layers, a looser norm bound is needed for state-of-the-art performance
(see Section C), raising questions of whether the current regularization approach is precise enough
in controlling the spectral norm of a convolutional kernel, or if there is an alternative mechanism at
play in ensuring the bi-Lipschitz criterion. Finally, from a probabilistic learning perspective, SNGP
focuses on learning a single high-quality model pθ(y|x) for a deterministic representation. Therefore
we expect it to provide complementary beneﬁts to approaches such as (efﬁcient) ensembles and
Bayesian neural networks [21, 42, 79] which marginalize over the representation parameters as well.
9

Acknowledgements We would like to thank Kevin Murphy, Deepak Ramachandran, Jasper Snoek,
and Timothy Nguyen at Google Research for the insightful comments and fruitful discussion.
Broader Impact
This work proposed a simple and practical methodology to improve the uncertainty estimation
performance of a deterministic deep learning model. Experiment results showcased the method’s
ability in improving model performance in calibration and OOD detection while maintaining similar
level of accuracy and latency, therefore illustrating its feasibility for industrial-scale applications.
We hope the proposed approach can be used to bring concrete improvements to AI-driven, socially-
relevant services where uncertainty is of natural importance. Examples include medical and policy
decision making, online toxic comment management, fairness-aware recommendation systems, etc.
Nonetheless, we do not claim that the improvement illustrated in this paper solve the problem of
model uncertainty entirely. This is because the analysis and experiments in this study may not capture
the full complexity of the real-world use cases, and there will always be room for improvement.
Designers of machine learning systems are encouraged to proactively confront the shortcomings of
model uncertainty and the underlying models that generate these conﬁdences. Even with a proper
user interface, there is always room to misinterpret model outputs and probabilities, such as with
nuanced applications such as election predictions, and users of these models should to be properly
trained to take these factors into account.
References
[1] S. An, F. Boussaid, and M. Bennamoun. How Can Deep Rectiﬁer Networks Achieve Linear
Separability and Preserve Distances? In International Conference on Machine Learning, pages
514–523, June 2015. ISSN: 1938-7228 Section: Machine Learning.
[2] C. Anil, J. Lucas, and R. Grosse. Sorting Out Lipschitz Function Approximation. In Interna-
tional Conference on Machine Learning, pages 291–301, May 2019. ISSN: 1938-7228 Section:
Machine Learning.
[3] P. Bartlett, S. Evans, and P. Long. Representing smooth functions as compositions of near-
identity functions with implications for deep network optimization. arXiv, 2018.
[4] P. L. Bartlett and M. H. Wegkamp. Classiﬁcation with a Reject Option using a Hinge Loss.
Journal of Machine Learning Research, 9(Aug):1823–1840, 2008.
[5] J. Behrmann, W. Grathwohl, R. T. Q. Chen, D. Duvenaud, and J.-H. Jacobsen. Invertible
Residual Networks. In International Conference on Machine Learning, pages 573–582, May
2019. ISSN: 1938-7228 Section: Machine Learning.
[6] A. Bendale and T. E. Boult. Towards Open Set Deep Networks. 2016 IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), 2016.
[7] J. O. Berger. Statistical Decision Theory and Bayesian Analysis. Springer Series in Statistics.
Springer-Verlag, New York, 2 edition, 1985.
[8] A. Blum. Random Projection, Margins, Kernels, and Feature-Selection. In C. Saunders,
M. Grobelnik, S. Gunn, and J. Shawe-Taylor, editors, Subspace, Latent Structure and Feature
Selection, Lecture Notes in Computer Science, pages 52–68, Berlin, Heidelberg, 2006. Springer.
[9] J. Bradshaw, A. G. d. G. Matthews, and Z. Ghahramani. Adversarial Examples, Uncertainty, and
Transfer Testing Robustness in Gaussian Process Hybrid Deep Networks. arXiv:1707.02476
[stat], July 2017. arXiv: 1707.02476.
[10] J. Br¨ocker. Reliability, sufﬁciency, and the decomposition of proper scores. Quarterly Journal of
the Royal Meteorological Society: A journal of the atmospheric sciences, applied meteorology
and physical oceanography, 135(643):1512–1519, 2009.
[11] R. Calandra, J. Peters, C. E. Rasmussen, and M. P. Deisenroth. Manifold Gaussian Processes
for regression. 2016 International Joint Conference on Neural Networks (IJCNN), 2016.
10

[12] D. Cer, M. Diab, E. Agirre, I. Lopez-Gazpio, and L. Specia. SemEval-2017 Task 1: Semantic
Textual Similarity Multilingual and Crosslingual Focused Evaluation. In Proceedings of the
11th International Workshop on Semantic Evaluation (SemEval-2017), pages 1–14, Vancouver,
Canada, Aug. 2017. Association for Computational Linguistics.
[13] A. Chernodub and D. Nowicki. Norm-preserving Orthogonal Permutation Linear Unit Activa-
tion Functions (OPLU). arXiv:1604.02313 [cs], Jan. 2017. arXiv: 1604.02313.
[14] C. Cortes, M. Mohri, and A. Rostamizadeh. Learning Non-Linear Combinations of Kernels. In
Y. Bengio, D. Schuurmans, J. D. Lafferty, C. K. I. Williams, and A. Culotta, editors, Advances
in Neural Information Processing Systems 22, pages 396–404. Curran Associates, Inc., 2009.
[15] J. Daunizeau. Semi-analytical approximations to statistical moments of sigmoid and softmax
mappings of normal variables. Feb. 2017.
[16] G. P. Dehaene. A deterministic and computable Bernstein-von Mises theorem. ArXiv, 2019.
[17] J. S. Denker and Y. LeCun. Transforming Neural-Net Output Levels to Probability Distributions.
In R. P. Lippmann, J. E. Moody, and D. S. Touretzky, editors, Advances in Neural Information
Processing Systems 3, pages 853–859. Morgan-Kaufmann, 1991.
[18] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. BERT: Pre-training of Deep Bidirec-
tional Transformers for Language Understanding. arXiv:1810.04805 [cs], Oct. 2018. arXiv:
1810.04805.
[19] L. Dinh, D. Krueger, and Y. Bengio. NICE: Non-linear Independent Components Estimation.
arXiv:1410.8516 [cs], Oct. 2014. arXiv: 1410.8516.
[20] L. Dinh, J. Sohl-Dickstein, and S. Bengio.
Density estimation using Real NVP.
arXiv:1605.08803 [cs, stat], May 2016. arXiv: 1605.08803.
[21] M. Dusenberry, G. Jerfel, Y. Wen, Y. Ma, J. Snoek, K. Heller, B. Lakshminarayanan, and
D. Tran. Efﬁcient and Scalable Bayesian Neural Nets with Rank-1 Factors. Proceedings of the
International Conference on Machine Learning, 1, 2020.
[22] D. Feng, L. Rosenbaum, and K. Dietmayer. Towards Safe Autonomous Driving: Capture
Uncertainty in the Deep Neural Network For Lidar 3D Vehicle Detection. Apr. 2018.
[23] D. Freedman. Wald Lecture: On the Bernstein-von Mises theorem with inﬁnite-dimensional
parameters. The Annals of Statistics, 27(4):1119–1141, Aug. 1999.
[24] T. Gneiting, F. Balabdaoui, and A. E. Raftery. Probabilistic forecasts, calibration and sharpness.
Journal of the Royal Statistical Society: Series B (Statistical Methodology), 69(2):243–268, Apr.
2007.
[25] T. Gneiting and A. E. Raftery. Strictly Proper Scoring Rules, Prediction, and Estimation. Journal
of the American Statistical Association, 102(477):359–378, Mar. 2007.
[26] H. Gouk, E. Frank, B. Pfahringer, and M. Cree. Regularisation of Neural Networks by Enforcing
Lipschitz Continuity. Apr. 2018.
[27] P. D. Gr ˜AŒnwald and A. P. Dawid. Game theory, maximum entropy, minimum discrepancy and
robust Bayesian decision theory. Annals of Statistics, 32(4):1367–1433, Aug. 2004. Publisher:
Institute of Mathematical Statistics.
[28] I. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin, and A. Courville. Improved training of
wasserstein GANs. In Proceedings of the 31st International Conference on Neural Information
Processing Systems, NIPS’17, pages 5769–5779, Long Beach, California, USA, Dec. 2017.
Curran Associates Inc.
[29] C. Guo, G. Pleiss, Y. Sun, and K. Q. Weinberger. On Calibration of Modern Neural Networks. In
International Conference on Machine Learning, pages 1321–1330, July 2017. ISSN: 1938-7228
Section: Machine Learning.
11

[30] D. Hafner, D. Tran, T. Lillicrap, A. Irpan, and J. Davidson. Reliable Uncertainty Estimates in
Deep Neural Networks using Noise Contrastive Priors. July 2018.
[31] R. E. Harang and E. M. Rudd. Principled Uncertainty Estimation for Deep Neural Networks,
2018. Library Catalog: www.semanticscholar.org.
[32] M. Hauser and A. Ray. Principles of Riemannian Geometry in Neural Networks. In I. Guyon,
U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors,
Advances in Neural Information Processing Systems 30, pages 2807–2816. Curran Associates,
Inc., 2017.
[33] M. Hein, M. Andriushchenko, and J. Bitterwolf. Why ReLU Networks Yield High-Conﬁdence
Predictions Far Away From the Training Data and How to Mitigate the Problem. pages 41–50,
2019.
[34] D. Hendrycks and T. Dietterich. Benchmarking Neural Network Robustness to Common
Corruptions and Perturbations. Sept. 2018.
[35] D. Hendrycks, K. Lee, and M. Mazeika. Using Pre-Training Can Improve Model Robustness
and Uncertainty. In International Conference on Machine Learning, pages 2712–2721, May
2019. ISSN: 1938-7228 Section: Machine Learning.
[36] D. Hendrycks*, N. Mu*, E. D. Cubuk, B. Zoph, J. Gilmer, and B. Lakshminarayanan. AugMix:
A Simple Method to Improve Robustness and Uncertainty under Data Shift. In International
Conference on Learning Representations, 2020.
[37] J.-H. Jacobsen, J. Behrmann, R. Zemel, and M. Bethge. Excessive Invariance Causes Adversarial
Vulnerability. Sept. 2018.
[38] J.-H. Jacobsen, J. Behrmannn, N. Carlini, F. Tram ˜Aˇsr, and N. Papernot. Exploiting Excessive
Invariance caused by Norm-Bounded Adversarial Robustness. Mar. 2019.
[39] r.-H. Jacobsen, A. W. M. Smeulders, and E. Oyallon. i-RevNet: Deep Invertible Networks. Feb.
2018.
[40] M. E. E. Khan, A. Immer, E. Abedi, and M. Korzepa. Approximate Inference Turns Deep
Networks into Gaussian Processes. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d. Alch ˜A©-
Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems 32,
pages 3094–3104. Curran Associates, Inc., 2019.
[41] A. Kristiadi, M. Hein, and P. Hennig. Being Bayesian, Even Just a Bit, Fixes Overconﬁdence in
ReLU Networks. arXiv:2002.10118 [cs, stat], Feb. 2020. arXiv: 2002.10118.
[42] B. Lakshminarayanan, A. Pritzel, and C. Blundell. Simple and Scalable Predictive Uncertainty
Estimation using Deep Ensembles. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus,
S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems
30, pages 6402–6413. Curran Associates, Inc., 2017.
[43] J. Landes. Probabilism, entropies and strictly proper scoring rules. International Journal of
Approximate Reasoning, 63:1–21, Aug. 2015.
[44] S. Larson, A. Mahendran, J. J. Peper, C. Clarke, A. Lee, P. Hill, J. K. Kummerfeld, K. Leach,
M. A. Laurenzano, L. Tang, and J. Mars. An Evaluation Dataset for Intent Classiﬁcation and
Out-of-Scope Prediction. arXiv:1909.02027 [cs], Sept. 2019. arXiv: 1909.02027.
[45] N. D. Lawrence and J. Q. Candela. Local Distance Preservation in the GP-LVM Through Back
Constraints. Jan. 2006.
[46] L. LeCam. Convergence of Estimates Under Dimensionality Restrictions. The Annals of
Statistics, 1(1):38–53, Jan. 1973.
[47] K. Lee, H. Lee, K. Lee, and J. Shin. Training Conﬁdence-calibrated Classiﬁers for Detecting
Out-of-Distribution Samples. In International Conference on Learning Representations, 2018.
12

[48] D. Macedo, T. I. Ren, C. Zanchettin, A. L. I. Oliveira, A. Tapp, and T. Ludermir. Isotropic
Maximization Loss and Entropic Score: Fast, Accurate, Scalable, Unexposed, Turnkey, and
Native Neural Networks Out-of-Distribution Detection. arXiv:1908.05569 [cs, stat], Feb. 2020.
arXiv: 1908.05569.
[49] D. J. C. MacKay. A practical Bayesian framework for backpropagation networks. Neural
Computation, 4(3):448–472, May 1992. Number: 3 Publisher: MIT Press.
[50] A. Malinin and M. Gales. Predictive Uncertainty Estimation via Prior Networks. In S. Bengio,
H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in
Neural Information Processing Systems 31, pages 7047–7058. Curran Associates, Inc., 2018.
[51] A. Malinin and M. Gales.
Prior Networks for Detection of Adversarial Attacks.
arXiv:1812.02575 [cs, stat], Dec. 2018. arXiv: 1812.02575.
[52] A. Meinke and M. Hein. Towards neural networks that provably know when they don’t know.
In International Conference on Learning Representations, 2020.
[53] T. P. Minka. A family of algorithms for approximate bayesian inference. phd, Massachusetts
Institute of Technology, USA, 2001. AAI0803033.
[54] T. Miyato, T. Kataoka, M. Koyama, and Y. Yoshida. Spectral Normalization for Generative
Adversarial Networks. In International Conference on Learning Representations, 2018.
[55] Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y. Ng. Reading Digits in Natural
Images with Unsupervised Feature Learning.
In NIPS Workshop on Deep Learning and
Unsupervised Feature Learning 2011, 2011.
[56] J. Nixon, M. W. Dusenberry, L. Zhang, G. Jerfel, and D. Tran. Measuring calibration in deep
learning. In CVPR Workshop, 2019.
[57] M. Panov and V. Spokoiny. Finite Sample Bernstein von Mises Theorem for Semiparametric
Problems. Bayesian Analysis, 10(3):665–710, Sept. 2015.
[58] M. Parry, A. P. Dawid, and S. Lauritzen. Proper local scoring rules. Annals of Statistics,
40(1):561–592, Feb. 2012. Publisher: Institute of Mathematical Statistics.
[59] D. C. Perrault-Joncas. Metric Learning and Manifolds: Preserving the Intrinsic Geometry.
2017.
[60] A. Rahimi and B. Recht. Random Features for Large-Scale Kernel Machines. In J. C. Platt,
D. Koller, Y. Singer, and S. T. Roweis, editors, Advances in Neural Information Processing
Systems 20, pages 1177–1184. Curran Associates, Inc., 2008.
[61] C. E. Rasmussen and C. K. I. Williams. Gaussian Processes for Machine Learning. University
Press Group Limited, Jan. 2006. Google-Books-ID: vWtwQgAACAAJ.
[62] C. Riquelme, G. Tucker, and J. Snoek. Deep Bayesian Bandits Showdown: An Empirical
Comparison of Bayesian Deep Networks for Thompson Sampling. In International Conference
on Learning Representations, 2018.
[63] H. Ritter, A. Botev, and D. Barber. A Scalable Laplace Approximation for Neural Networks. In
International Conference on Learning Representations, 2018.
[64] F. Rousseau, L. Drumetz, and R. Fablet. Residual Networks as Flows of Diffeomorphisms.
Journal of Mathematical Imaging and Vision, 62(3):365–375, Apr. 2020.
[65] W. Ruan, X. Huang, and M. Kwiatkowska. Reachability analysis of deep neural networks with
provable guarantees. In Proceedings of the 27th International Joint Conference on Artiﬁcial
Intelligence, IJCAI’18, pages 2651–2659, Stockholm, Sweden, July 2018. AAAI Press.
[66] W. J. Scheirer, L. P. Jain, and T. E. Boult. Probability Models for Open Set Recognition. IEEE
Transactions on Pattern Analysis and Machine Intelligence, 36(11):2317–2324, Nov. 2014.
Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence.
13

[67] M. O. Searcod. Metric Spaces. Springer London, London, 2007 edition edition, Aug. 2006.
[68] M. Sensoy, L. Kaplan, and M. Kandemir. Evidential Deep Learning to Quantify Classiﬁcation
Uncertainty. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and
R. Garnett, editors, Advances in Neural Information Processing Systems 31, pages 3179–3189.
Curran Associates, Inc., 2018.
[69] L. Shu, H. Xu, and B. Liu.
DOC: Deep Open Classiﬁcation of Text Documents.
arXiv:1709.08716 [cs], Sept. 2017. arXiv: 1709.08716.
[70] J. Snoek, O. Rippel, K. Swersky, R. Kiros, N. Satish, N. Sundaram, M. M. A. Patwary,
Prabhat, and R. P. Adams. Scalable Bayesian Optimization Using Deep Neural Networks.
arXiv:1502.05700 [stat], Feb. 2015. arXiv: 1502.05700.
[71] J. Sokolic, R. Giryes, G. Sapiro, and M. R. D. Rodrigues. Robust Large Margin Deep Neural
Networks. IEEE Transactions on Signal Processing, 2017.
[72] N. Tagasovska and D. Lopez-Paz. Single-Model Uncertainties for Deep Learning. In H. Wallach,
H. Larochelle, A. Beygelzimer, F. d. Alch ˜A©-Buc, E. Fox, and R. Garnett, editors, Advances in
Neural Information Processing Systems 32, pages 6417–6428. Curran Associates, Inc., 2019.
[73] L. Tierney, R. E. Kass, and J. B. Kadane. Approximate Marginal Densities of Nonlinear
Functions. Biometrika, 76(3):425–433, 1989. Publisher: [Oxford University Press, Biometrika
Trust].
[74] G.-L. Tran, E. V. Bonilla, J. Cunningham, P. Michiardi, and M. Filippone. Calibrating Deep
Convolutional Gaussian Processes. In The 22nd International Conference on Artiﬁcial In-
telligence and Statistics, pages 1554–1563, Apr. 2019. ISSN: 1938-7228 Section: Machine
Learning.
[75] Y. Tsuzuku, I. Sato, and M. Sugiyama. Lipschitz-Margin Training: Scalable Certiﬁcation of
Perturbation Invariance for Deep Neural Networks. In S. Bengio, H. Wallach, H. Larochelle,
K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Pro-
cessing Systems 31, pages 6541–6550. Curran Associates, Inc., 2018.
[76] B. van Aken, J. Risch, R. Krestel, and A. Loser. Challenges for Toxic Comment Classiﬁcation:
An In-Depth Error Analysis. In Proceedings of the 2nd Workshop on Abusive Language Online
(ALW2), pages 33–42, Brussels, Belgium, Oct. 2018. Association for Computational Linguistics.
[77] J. van Amersfoort, L. Smith, Y. W. Teh, and Y. Gal. Simple and Scalable Epistemic Uncertainty
Estimation Using a Single Deep Deterministic Neural Network. arXiv:2003.02037 [cs, stat],
Mar. 2020. arXiv: 2003.02037.
[78] N. Vedula, N. Lipka, P. Maneriker, and S. Parthasarathy. Towards Open Intent Discovery for
Conversational Text. arXiv:1904.08524 [cs], Apr. 2019. arXiv: 1904.08524.
[79] Y. Wen, D. Tran, and J. Ba. BatchEnsemble: an Alternative Approach to Efﬁcient Ensemble
and Lifelong Learning. In International Conference on Learning Representations, 2020.
[80] T.-W. Weng, H. Zhang, P.-Y. Chen, J. Yi, D. Su, Y. Gao, C.-J. Hsieh, and L. Daniel. Evaluating
the Robustness of Neural Networks: An Extreme Value Theory Approach. In International
Conference on Learning Representations, 2018.
[81] A. G. Wilson, Z. Hu, R. Salakhutdinov, and E. P. Xing. Stochastic Variational Deep Ker-
nel Learning. In Proceedings of the 30th International Conference on Neural Information
Processing Systems, NIPS’16, pages 2594–2602, USA, 2016. Curran Associates Inc.
[82] M.-A. Yaghoub-Zadeh-Fard, B. Benatallah, F. Casati, M. Chai Barukh, and S. Zamanirad. User
Utterance Acquisition for Training Task-Oriented Bots: A Review of Challenges, Techniques
and Opportunities. IEEE Internet Computing, pages 1–1, 2020. Conference Name: IEEE
Internet Computing.
[83] S. Zagoruyko and N. Komodakis. Wide Residual Networks. arXiv:1605.07146 [cs], June 2017.
arXiv: 1605.07146.
14

[84] Y. Zheng, G. Chen, and M. Huang. Out-of-Domain Detection for Natural Language Understand-
ing in Dialog Systems. IEEE/ACM Transactions on Audio, Speech, and Language Processing,
28:1198–1209, 2020. Conference Name: IEEE/ACM Transactions on Audio, Speech, and
Language Processing.
15

Supplementary Material:
Simple and Principled Uncertainty Estimation with
Deterministic Deep Learning via Distance Awareness
Contents
A Method Summary
17
B
Formal Statements
18
C Experiment Details and Further Results
19
C.1
2D Synthetic Benchmark . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
19
C.2
Vision and Language Understanding . . . . . . . . . . . . . . . . . . . . . . . . .
21
D Additional Related Work
23
E
Proof
24
E.1
Proof of Proposition 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
24
E.2
Proof of Lemma 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
24
16

A
Method Summary
Architecture
Given a deep learning model logit(x) = g ◦h(x) with L −1 hidden layers of size
{Dl}L
l=1, SNGP makes two changes to the model:
1. adding spectral normalization to the hidden weights {Wl}L
l=1, and
2. replacing the dense output layer g(h) = h⊤β with a GP layer. Under the RFF approximation,
the GP layer is simply a one-layer network with DL hidden units g(h) ∝cos(−WLhi +
bL)⊤β, where {WL,bL} are frozen weights that are initialized from a Gaussian and a
uniform distribution, respectively (as described in Equation (8)).
Training
Algorithm 1 summarizes the training step. As shown, for every minibatch step, the model
ﬁrst updates the hidden-layer weights {Wl,bl}L−1
l=1 and the trainable output weights β = {βk}K
k=1
via SGD, then performs spectral normalization using power iteration method ([26] which has time
complexity O(∑L−1
l=1 Dl)), and ﬁnally performs precision matrix update (Equation (9), time complexity
O(D2
L)). Since {Dl}L−1
l=1 are ﬁxed for a given architecture and usually DL ≤1024, the computation
scales linearly with respect to the sample size. We use DL = 1024 in the experiments.
Prediction
Algorithm 2 summarizes the prediction step. The model ﬁrst performs the conventional
forward pass, which involves (i) computing the ﬁnal hidden feature Φ(x)DL×1, and (ii) computing the
posterior mean ˆmk(x) = Φ⊤β k (time complexity O(DL)). Then, using the posterior variance matri-
ces {ˆΣk}K
k=1, the model computes the predictive variance ˆσk(x)2 = Φ(x)⊤ˆΣΦ(x) (time complexity
O(D2
L)).
To estimate the predictive distribution pk = exp(mk)/∑k exp(mk) where mk ∼N
 ˆmk(x), ˆσ2
k (x)

, we
calculate its posterior mean using Monte Carlo averaging. Notice that this Monte Carlo averaging is
computationally very cheap since it only involves sampling from a closed-form distribution whose
parameters ( ˆm, ˆσ2) are already computed by the single feed-forward pass (i.e., a single call to
tf.random.normal). This is different from the full Monte Carlo sampling used by MC Dropout or
deep ensembles which require multiple forward passes and are computationally expensive. As shown
in the latency results in experiments (Section 5.2), the extra variance-related computation only adds a
small overhead to the inference time of a deterministic DNN. In the experiments, we use 10 samples
to compute the mean predictive distribution.
In applications where the inference latency is of high priority (e.g., real-time pCTR prediction for
online advertising), we can reduce the computational overhead further by replacing the Monte Carlo
averaging with the mean-ﬁeld approximation [15]. We leave this for future work.
17

B
Formal Statements
Minimax Solution to Uncertainty Risk Minimization
The expression in (4) seeks to answer the
following question: assuming we know the true domain probability p∗(x ∈XIND), and given a model
p(y|x,x ∈XIND) that we have already learned from data, what is the best solution we can construct
to minimize the minimax objective (3)? The interest of this conclusion is not to construct a practical
algorithm, but to highlight the theoretical necessity of taking into account the domain probability in
constructing a good solution for uncertainty quantiﬁcation. If the domain probability is not necessary,
then the expression of the unique and optimal solution to the minimax probability should not contain
p∗(x ∈XIND) even if it is available. However the expression of (4) shows this is not the case.
To make the presentation clear, we formalize the statement about (4) into the below proposition:
Proposition 2 (Minimax Solution to Uncertainty Risk Minimization).
Given:
(a) p(y|x,x ∈XIND) the model’s predictive distribution learned from data D = {yi,xi}N
i=1
(b) p∗(x ∈XIND) the true domain probability,
then there exists an unique optimal solution to the minimax problem (3), and it can be constructed
using (a) and (b) as:
p(y|x) = p(y|x,x ∈XIND)∗p∗(x ∈XIND)+ puniform(y|x,x ̸∈XIND)∗p∗(x ̸∈XIND)
(11)
where puniform(y|x,x ̸∈XIND) = 1
K is a discrete uniform distribution for K classes.
As discussed in Section 2.1, the solution (11) is not only optimal for the minimax Brier risk, but is
in fact optimal for a wide family of strictly proper scoring rules known as the (separable) Bregman
score [58]:
s(p, p∗|x) =
K
∑
k=1
n
[p∗(yk|x)−p(yk|x)]ψ′(p∗(yk|x))−ψ(p∗(yk|x))
o
(12)
where ψ is a strictly concave and differentiable function. Bregman score reduces to the log score
when ψ(p) = p∗log(p), and reduces to the Brier score when ψ(p) = p2 −1
K .
Therefore we will show (11) for the Bregman score. The proof relies on the following key lemma:
Lemma 1 (puniform is Optimal for Minimax Bregman Score in x ̸∈XIND).
Consider the Bregman score in (12). At a location x ̸∈XIND where the model has no information
about p∗other than ∑K
k=1 p(yk|x) = 1, the solution to the minimax problem
inf
p∈P sup
p∗∈P∗s(p, p∗|x)
is the discrete uniform distribution, i.e., puniform(yk|x) = 1
K ∀k ∈{1,...,K}.
The proof for Lemma 1 is in Section E.2. It is worth noting that Lemma 1 only holds for a strictly
proper scoring rule [24]. For a non-strict proper scoring rule (e.g., the ECE), there can exist inﬁnitely
many optimal solutions, making the minimax problem ill-posed.
We are now ready to prove Proposition 2:
Proof. Denote XOOD = X /XIND. Decompose the overall Bregman risk by domain:
S(p, p∗) = Ex∈X
 s(p, p∗|x)

=
Z
X s
 p, p∗|x

p∗(x)dx
=
Z
X s
 p, p∗|x

∗

p∗(x|x ∈XIND)p∗(x ∈XIND)+ p∗(x|x ∈XOOD)p∗(x ∈XOOD)

dx
= Ex∈XIND
 s(p, p∗|x)

p∗(x ∈XIND)+Ex∈XOOD
 s(p, p∗|x)

p∗(x ∈XOOD)
= SIND(p, p∗)∗p∗(x ∈XIND)+SOOD(p, p∗)∗p∗(x ∈XOOD).
where we have denoted SIND(p, p∗) = Ex∈XIND
 s(p, p∗|x)

and SOOD(p, p∗) = Ex∈XOOD
 s(p, p∗|x)

.
18

Now consider decomposing the sup risk supp∗S(p, p∗) for a given p.
Notice that sup risk
supp∗S(p, p∗) is separable by domain for any p ∈P. This is because SIND(p, p∗) and SOOD(p, p∗) has
disjoint support, and we do not impose assumption on p∗:
sup
p∗S(p, p∗) = sup
p∗

SIND(p, p∗)

∗p∗(x ∈XIND)+sup
p∗

SOOD(p, p∗)

∗p∗(x ∈XOOD)
We are now ready to decompose the minimax risk infp supp∗S(p, p∗). Notice that the minimax risk is
also separable by domain due to the disjoint in support:
inf
p sup
p∗S(p, p∗) = inf
p
h
sup
p∗

SIND(p, p∗)

∗p∗(x ∈XIND)+sup
p∗

SOOD(p, p∗)

∗p∗(x ∈XOOD)
i
= inf
p sup
p∗

SIND(p, p∗)

∗p∗(x ∈XIND)+inf
p sup
p∗

SOOD(p, p∗)

∗p∗(x ∈XOOD), (13)
also notice that the in-domain minimax risk infp supp∗

SIND(p, p∗)

is ﬁxed due to condition (a).
Therefore, to show that (11) is the optimal and unique solution to (13), we only need to show puniform
is the optimal and unique solution to infp supp∗

SOOD(p, p∗)

. To this end, notice that for a given p:
sup
p∗∈P∗

SOOD(p, p∗)

=
Z
XOOD
sup
p∗[s(p, p∗|x)]p(x|x ∈XOOD)dx,
(14)
due to the fact that we don’t impose assumption on p∗(therefore p∗is free to attain the global
supreme by maximizing s(p, p∗|x) at every single location x ∈XOOD). Furthermore, there exists p
that minimize supp∗s(p, p∗|x) at every location of x ∈XOOD, then it minimizes the integral [7]. By
Lemma 1, such p exists and is unique, i.e.:
puni form = arginf
p∈P
sup
p∗∈P∗SOOD(p, p∗).
In conclusion, we have shown that puniform is the unique solution to infp supp∗SOOD(p, p∗). Combining
with condition (a)-(b), we have shown that the unique solution to (13) is (11).
C
Experiment Details and Further Results
C.1
2D Synthetic Benchmark
For both benchmarks, we sample 500 observations xi = (x1i,x2i) from each of the two in-domain
classes (orange and blue), and consider a deep architecture ResFFN-12-128, which contains 12
residual feedforward layers with 128 hidden units and dropout rate 0.01. The input dimension is
projected from 2 dimensions to the 128 dimensions using a dense layer.
In addition to SNGP, we also visualize the uncertainty surface of the below approaches: Gaussian
process (GP) is a standard Gaussian process directly taking xi as in input. In low-dimensional
datasets, GP is often considered the gold standard for uncertainty quantiﬁcation. Deep Ensemble is an
ensemble of 10 ResFFN-12-128 models with dense output layers, MC Dropout uses single ResFFN-
12-128 model with dense output layer and 10 dropout samples. DNN-GP uses a single ResFFN-
12-128 model with the GP Layer (described in Section 3.1) without spectral normalization. Finally,
SNGP uses a single ResFFN-12-128 model with the GP layer and with the spectral normalization.
For these two binary classiﬁcation tasks, in Figure 1 we plot the predictive uncertainty for GP,
DNN-GP and SNGP as the posterior predictive variance of the logits, i.e., u(x) = var(logit(x))
which ranges between [0,1] under the RBF kernel. For MC Dropout and Deep Ensemble, since
these two methods don’t provide an convenient expression of predictive variance, we plot their
predictive uncertainty as the distance of the maximum predictive probability from 0.5, i.e., u(x) =
1−2∗|p(x)−0.5|, so that u(x) ∈[0,1].
Figure 2-3 compares the aforementioned methods in terms of the same metric based on the predictive
probability introduced in the last paragraph: u(x) = 1−2∗|p−0.5|. We also included DNN-SN (a
ResFFN-12-128 model with spectral normalization but no GP layer) into comparison. As shown,
compared to the uncertainty surface based on predictive variance (Figure 1), the uncertainty surface
based on predictive probability shows stronger inﬂuence from the model’s decision boundary. This
empirical observation seems to suggest that the predictive uncertainty from the GP logits can be
a better metric for calibration and OOD detection. We will explore the performance difference of
different uncertainty metrics in calibration and OOD performance in the future work.
19

(a) Gaussian Process
(b) Deep Ensemble
(c) MC Dropout
(d) DNN-SN
(e) DNN-GP
(f) SNGP (Ours)
Figure 2: The uncertainty surface of a GP and different DNN approaches on the two ovals 2D classiﬁcation
benchmarks. The uncertainty is computed in terms of the distance of the maximum predictive probability from
0.5, i.e. u(x) = 1−2∗|p(x)−0.5|. Background color represents the estimated model uncertainty (See 1e for
color map).
(a) Gaussian Process
(b) Deep Ensemble
(c) MC Dropout
(d) DNN-SN
(e) DNN-GP
(f) SNGP (Ours)
Figure 3: The uncertainty surface of a GP and different DNN approaches on the two moons 2D classiﬁcation
benchmarks. The uncertainty is computed in terms of the distance of the maximum predictive probability from
0.5, i.e. u(x) = 1−2∗|p(x)−0.5|. Background color represents the estimated model uncertainty (See Figures
1j and 1e for color map).
20

C.2
Vision and Language Understanding
Hyperparameter Conﬁguration
SNGP is composed of two components: Spectral Normalization
(SN) and Gaussian Process (GP) layer, both are available at the open-source Edward2 probabilistic
programming library 4.
Spectral normalization contains two hyperparameters: the number of power iterations and the upper
bound for spectral norm (i.e., c in Equation (10)). In our experiments, we ﬁnd it is sufﬁcient to ﬁx
power iteration to 1. The value for the spectral norm bound c controls the trade-off between the
expressiveness and the distance-awareness of the residual block, where a small value of c may shrink
the residual block back to identity mapping hence harming the expressiveness, while a large value of
c may lead to the loss of bi-Lipschitz property (Proposition 1). Furthermore, the proper range of c
depends on the layer type: for dense layers (e.g., the intermediate and the output dense layers of a
Transformer), it is sufﬁcient to set c to a value between (0.95,1). For the convolutional layers, the
norm bound needs to be set to a larger value to not impact the model’s predictive performance. This
is likely caused by the fact that the current spectral normalization technique does not have a precise
control of the true spectral norm of the convolutional kernel, in conjuction with the fact that the other
regularization mechanisms (e.g., BatchNorm and Dropout) may rescale a layer’s spectral norm in
unexpected ways [26, 54]. In general, we recommend performing a grid search for c ∈{0.9,1,2,...}
to identify the smallest possible values of c that still retains the predictive performance of the original
model. In the experiments, we set the norm bound to c = 6 for a WideResNet model.
The GP layer contains 3 hyperparameters for the main layer (Equation (8)), and 2 hyperparameters
for its covariance module (Equation (9). The three hyperparameter for the main layers are the hidden
dimension (DL, i.e., the number of random features), the length-scale parameter l for the RBF kernel,
and the strength of L2 regularization on output weights βk. In the experiments, we ﬁnd the model’s
performance to be not very sensitive to these parameters. Setting DL = 1024 or 2048, l = 2.0 and L2
regularization to 0 are sufﬁcient in most cases. The two hyperparameters for the covariance module is
the ridge factor s and the discount factor m, they come into the update rule of the precision matrix as:
Σ−1
k,0 = s∗I,
Σ−1
k,t = m∗Σ−1
k,t−1 +(1−m)∗
M
∑
i=1
ˆpik(1−ˆpik)ΦiΦ⊤
i ,
i.e., the ridge factor s serves to control the stability of matrix inverse (if the number of sample
size n is small), and m controls how fast the moving average update converges to the population
value Σk = sI+∑n
i=1 ˆpik(1−ˆpik)ΦiΦ⊤
i . Similar to other moving-average update method, these two
parameters can impact the quality of learned covariance matrix in non-trivial ways. In general, we
recommend conducting some small scale experiments on the data to validate the learning quality of
the moving average update in approximating the population covariance. Alternatively, the covariance
update can be computed exactly at the ﬁnal epoch by initialize Σ−1
k,0 = 0 and simply using the update
formula Σ−1
k,t = Σ−1
k,t−1 +∑M
i=1 ˆpik(1−ˆpik)ΦiΦ⊤
i . In the experiments, we set s = 0.001 and m = 0.999,
which is sufﬁcient for our setting where the number of minibatch steps per epoch is large.
We also implemented two additional functionalities for GP layers: input dimension projection and
input layer normalization. The input dimension project serves to project the hidden dimension of the
penultimate layer DL−1 to a lower value D′
L−1 (using a random Gaussian matrix WDL−1×D′
L−1), it can
be projected down to a smaller dimension. Input layer normalization applies Layer Normalization to
the input hidden features, which is akin to performing automatic relevance determination (ARD)-
style variable selection to the input features. In the experiments, we always turn on the input layer
normalization and set input layer normalization to 128. Although later ablation studies revealed that
the model performance is not sensitive to these values.
Data Preparation and Computing Infrastructure
For CIFAR-10 and CIFAR-100, we followed
the original Wide ResNet work to apply the standard data augmentation (horizontal ﬂips and random
crop-ping with 4x4 padding) and used the same hyperparameter and training setup [83]. The only
exception is the learning rate and training epochs, where we ﬁnd a smaller learning rate (0.04 for
CIFAR-10 and 0.08 for CIFAR100, v.s. 0.1 for the original WRN model) and longer epochs (250 for
SNGP v.s. 200 for the original WRN model) leads to better performance.
4https://github.com/google/edward2
21

Spectral Normalization
Gaussian Process Layer
Power Iteration
1
Hidden Dimension
1024 (WRN), 2048 (BERT)
Spectral Norm Bound
6.0 (WRN), 0.95 (BERT)
Length-scale Parameter
2.0
L2 Regularization
0.0
Covariance Ridge Factor
0.001
Covariance Discount Factor
0.999
Projected Input Dimension
128 (WRN) None (BERT)
Input Layer Normalization
True
Table 5: Hyperparameters of SNGP used in the experiments, where important hyperparameters are highlighted
in bold.
For CLINC OOS intent understanding data, we pre-tokenized the sentences using the standard BERT
tokenizer5 with maximum sequence length 32, and created standard binary input mask for the BERT
model that returns 1 for valid tokens and 0 otherwise. Following the original BERT work, we used
the Adam optimizer with weight decay rate 0.01 and warmup proportion 0.1. We initialize the model
from the ofﬁcial BERTBase checkpoint6. For this ﬁne-tuning task, we using a smaller step size (5e−5
for SNGP .v.s. 1e−4 for the original BERT model) but shorter epochs (40 for SNGP v.s. 150 for
the original BERT model) leads to better performance. When using spectral normalization, we set
the hyperparameter c = 0.95 and apply it to the pooler dense layer of the classiﬁcation token. We
do not spectral normalization to the hidden transformer layers, as we ﬁnd the pre-trained BERT
representation is already competent in preserving input distance due to the masked language modeling
training, and further regularization may in fact harm its predictive and calibration performance.
All models are implemented in TensorFlow and are trained on 8-core Cloud TPU v2 with 8 GiB of
high-bandwidth memory (HBM) for each TPU core. We use batch size 32 per core.
Evaluation
For CIFAR-10 and CIFAR-100, we evaluate the model’s predictive accuracy and
calibration error under both clean and corrupted versions of the CIFAR testing data. The corrupted
data, termed CIFAR10-C, includes 15 types of corruptions, e.g., noise, blurring, pixelation, etc, over
5 levels of corruption intensity [34]. We also evaluate the model performance in OOD detection
by using the CIFAR-10/CIFAR-100 model’s uncertainty estimate as a predictive score for OOD
classiﬁcation, where we consider a standard OOD task by testing CIFAR-10/CIFAR-100 model’s
ability in detecting samples from the Street View House Numbers (SVHN) dataset [55], and a more
difﬁcult OOD task by testing CIFAR-10’s ability in detecting samples from the CIFAR-100 dataset,
and vice versa. Speciﬁcally, for all models, we compute the OOD uncertainty score using the so-called
Dempster-Shafer metric [68], which empirically leads to better performance for a distance-aware
model. Given logits for K classes {hk(xi)}K
k=1, this metric computes its uncertainty for a test example
xi as:
u(xi) =
K
K +∑K
k=1 exp
 hk(xi)
.
(15)
As shown, for a distance-aware model where the magnitude of the logits reﬂects the distance from the
observed data manifold, u(xi) can be a more effective metric since it is monotonic to the magnitude of
the logits. On the other hand, the maximum probability pmax = argmaxk exp(hk)/∑K
k=1 exp
 hk(xi)

does not take advantage of this information since it normalizes over the exponentiated logits.
In terms of evaluation metrics, we assess the model’s calibration performance using the empirical
estimate of ECE:
ˆ
ECE = ∑M
m=1
|Bm|
n |acc(Bm)−conf(Bm)| which estimates the difference in model’s
accuracy and conﬁdence by partitioning model prediction into M bins {Bm}M
m=1 [29]. In this work,
we choose M = 15. We assess the model’s OOD performance using Area Under Precision-Recall
(AUPR). Finally, we measure each method’s inference latency by millisecond per image.
For CLINC OOS intent detection data, we evaluate the predictive accuracy on the in-domain test data,
evaluate the ECE and OOD detection performance on the combined in-domain and out-of-domain
testing data, and we measure inference latency by millisecond per sentence.
5https://github.com/google-research/bert
6https://storage.googleapis.com/bert models/2020 02 20/uncased L-12 H-768 A-12.zip
22

D
Additional Related Work
Distance-preserving neural networks and bi-Lipschitz condition The theoretic connection be-
tween distance preservation and the bi-Lipschitz condition is well-established [67], and learning an
approximately isometric, distance-preserving transform has been an important goal in the ﬁelds of
dimensionality reduction [8, 59], generative modeling [45, 19, 20, 39], and adversarial robustness
[37, 65, 71, 75, 80]. This work is a novel application of the distance preservation property for
uncertainty quantiﬁcation. There existing several methods for controlling the Lipschitz constant of
a DNN (e.g., gradient penalty or norm-preserving activation [1, 2, 13, 28]), and we chose spectral
normalization in this work due to its simplicity and its minimal impact on a DNN’s architecture and
the optimization dynamics [3, 5, 64].
Open Set Classiﬁcation The uncertainty risk minimization problem in Section 2 assumes a data-
generation mechanism similar to the open set recognition problem [66], where the whole input space
is partitioned into known and unknown domains. However, our analysis is unique in that it focuses on
measuring a model’s behavior in uncertainty quantiﬁcation and takes a rigorous, decision-theoretic
approach to the problem. As a result, our analysis works with a special family of risk functions
(i.e., the strictly proper scoring rule) that measure a model’s performance in uncertainty calibration.
Furthermore, it handles the existence of unknown domain via a minimax formulation, and derives the
solution by using a generalized version of maximum entropy theorem for the Bregman scores [27, 43].
The form of the optimal solution we derived in (4) takes an intuitive form, and has been used by many
empirical work as a training objective to leverage adversarial training and generative modeling to
detect OOD examples [30, 31, 47, 51, 52]. Our analysis provide strong theoretical support for these
practices in verifying rigorously the uniqueness and optimality of this solution, and also provides a
conceptual uniﬁcation of the notion of calibration and the notion of OOD generalization. Furthermore,
it is used in this work to motivate a design principle (input distance awareness) that enables strong
OOD performance in discriminative classiﬁers without the need of explicit generative modeling.
23

E
Proof
E.1
Proof of Proposition 1
The proof for Proposition 1 is an adaptation of the classic result of [3] to our current context:
Proof. First establish some notations. We denote I(x) = x the identity function such that for h(x) =
x+g(x), we can write g = h−I. For h : X →H , denote ||h|| = sup
n
||f(x)||H
||x||X
for x ∈X ,||x|| > 0
o
.
Also denote the Lipschitz seminorm for a function h as:
||h||L = sup
||h(x)−h(x′)||H
||x−x′||X
for
x,x′ ∈X ,x ̸= x′

(16)
It is worth noting that by the above deﬁnitions, for two functions (x′ −x) : X × X →X and
(h(x)−h(x′)) : X ×X →H who shares the same input space, the Lipschitz inequality can be ex-
pressed using the ||.|| norm, i.e., ||h(x)−h(x′)||H ≤α||x−x′||X implies ||h(x′)−h(x)|| ≤α||x−x′||,
and vice versa.
Now assume ∀l, ||gl||L = ||hl −I||L ≤α < 1. We will show Proposition 1 by ﬁrst showing:
(1−α)||x−x′|| ≤||hl(x)−hl(x′)|| ≤(1+α)||x−x′||,
(17)
which is the bi-Lipschitz condition for a single residual block.
First show the left hand side:
||x−x′|| ≤||x−x′ −(hl(x)−hl(x′))+(hl(x)−hl(x′))||
≤||(hl(x′)−x′)−(hl(x)−x)||+||hl(x)−hl(x′)||
≤||gl(x′)−gl(x)||+||hl(x)−hl(x′)||
≤α||x′ −x||+||hl(x)−hl(x′)||,
where the last line follows by the assumption ||gl||L ≤α. Rearranging, we get:
(1−α)||x−x′|| ≤||hl(x)−hl(x′)||.
(18)
Now show the right hand side:
||hl(x)−hl(x′)|| = ||x+gl(x)−(x′ +gl(x′))|| ≤||x−x′||+||gl(x)−gl(x′))|| ≤(1+α)||x−x′||.
Combining (18)-(19), we have shown (17), which also implies:
(1−α)||x−x′||X ≤||hl(x)−hl(x′)||H ≤(1+α)||x−x′||X
(19)
Now show the bi-Lipschitz condition for a L-layer residual network h = hL ◦hL−1 ◦···◦h1. It is easy
to see that by induction:
(1−α)L||x−x′||X ≤||h(x)−h(x′)||H ≤(1+α)L||x−x′||X
(20)
Denoting L1 = (1−α)L and L2 = (1+α)L, we have arrived at expression in Proposition 1.
E.2
Proof of Lemma 1
Proof. This proof is an application of the generalized maximum entropy theorem to the case of
Bregman score. We shall ﬁrst state the generalized maximum entropy theorem to make sure the
proof is self-contained. Brieﬂy, the generalized maximum entropy theorem veriﬁes that for a
general scoring function s(p, p∗|x) with entropy function H(p|x), the maximum-entropy distribution
p′ = argsup
p
H(p|x) attains the minimax optimality :
Theorem 1 (Maximum Entropy Theorem for General Loss [27]). Let P be a convex, weakly closed
and tight set of distributions. Consider a general score function s(p, p∗|x) with an associated entropy
function deﬁned as H(p|x) = infp∗∈P∗s(p, p∗|x). Assume below conditions on H(p|x) hold:
• (Well-deﬁned) For any p ∈P, H(p|x) exists and is ﬁnite.
24

• (Lower-semicontinous) For a weakly converging sequence pn →p0 ∈P where H(pn|x) is
bounded below, we have s(p, p0|x) ≤liminfn→∞s(p, pn|x) for all p ∈P.
Then there exists an maximum-entropy distribution p′ such that
p′ = sup
p∈P
H(p) = sup
p∈P
inf
p∗∈P∗s(p, p∗|x) = inf
p∈P sup
p∗∈P∗s(p, p∗|x).
Above theorem states that the maximum-entropy distribution attains the minimax optimality for a
scoring function s(p, p∗|x), assuming its entropy function satisfying certain regularity conditions.
Authors of [27] showed that the entropy function of a Bregman score satisﬁes conditions in Theorem
1. Consequently, to show that the discrete uniform distribution is minimax optimal for Bregman score
at x ̸∈XIND, we only need to show discrete uniform distribution is the maximum-entropy distribution.
Recall the deﬁnition of the strictly proper Bregman score [58]:
s(p, p∗|x) =
K
∑
k=1
n
[p∗(yk|x)−p(yk|x)]ψ′(p∗(yk|x))−ψ(p∗(yk|x))
o
(21)
where ψ is differentiable and strictly concave. Moreover, its entropy function is:
H(p|x) = −
K
∑
k=1
ψ(p(yk|x))
(22)
Our interest is to show that for x ∈XOOD, the maximum-entropy distribution for the Bregman score
is the discrete uniform distribution p(yk|x) = 1
K . To this end, we notice that in the absence of any
information, the only constraint on the predictive distribution is that ∑k p(yk|x) = 1. Therefore,
denoting p(yk|x) = pk, we can set up the optimization problem with respect to Bregman entropy (22)
using the Langrangian form below:
L(p|x) = H(p|x)+λ ∗(∑
k
pk −1) = −
K
∑
k=1
ψ(pk)+λ ∗(∑
k
pk −1)
(23)
Taking derivative with respect to pk and λ:
∂
∂pk
L = −ψ′(pk)+λ = 0
(24)
∂
∂λ L =
K
∑
k=1
pk −1 = 0
(25)
Notice that since ψ(p) is strictly concave, the function ψ′(p) is monotonically decreasing and
therefore invertible. As a result, to solve the maximum entropy problem, we can solve the above
systems of equation by ﬁnding a inverse function ψ
′−1(p), which lead to the simpliﬁcation:
pk = ψ
′−1(λ);
K
∑
k=1
pk = 1.
(26)
Above expression essentially states that all pk’s should be equal and sum to 1. The only distribution
satisfying the above is the discrete uniform distribution, i.e., pk = 1
K ∀k.
25

