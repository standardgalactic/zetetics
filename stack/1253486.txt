Technische Universit¨at M¨unchen
Department of Mathematics
Master’s Thesis
Bayesian Analysis of the One-Factor Copula
Model with Applications to Finance
Benedikt Schamberger
Supervisor: Prof. Claudia Czado, Ph.D.
Advisors: Lutz Gruber, Prof. Claudia Czado, Ph.D.
Submission Date: 09.04.2015

I hereby declare that this thesis is my own work and that no other sources have been used
except those clearly indicated and referenced.
Munich, 09.04.2015

Zusammenfassung
Vine Copulas erm¨oglichen die Modellierung von hochdimensionalen Abh¨angigkeitsstrukt-
uren anhand von zwei-dimensionalen Bausteinen. Die verwandten Faktorcopula Modelle
verwenden das gleiche Prinzip, mit dem Unterschied, dass vorrausgesetzt wird, dass den
Variablen Faktoren, die nicht beobachtet werden k¨onnen, zugrunde liegen. Diese beiden
Modelle und weitere Grundlagen, die f¨ur ihre Anwendung n¨otig sind, werden vorgestellt
und anschließend anhand von europ¨aischen Aktienrenditen verglichen.
Zus¨atzlich wird f¨ur den Spezialfall von einem latenten Faktor mit einem Markov chain
Monte Carlo Verfahren eine Bayesianische Analyse des Faktorcopula Modells entwickelt.
Verschiedene Varianten des Verfahrens werden mit Hilfe einer Simulationsstudie mitein-
ander verglichen, und neben Vine Copulas dazu verwendet um in einer Anwendungsstudie
die Risikomaße Value-at-Risk und Expected Shortfall f¨ur ein Portfolio vorherzusagen.
i

Contents
1
Introduction
1
2
Probability theory and copulas
3
2.1
Basics and distributions
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
3
2.2
Copulas
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
6
2.2.1
Basic properties . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
6
2.2.2
Elliptical copulas . . . . . . . . . . . . . . . . . . . . . . . . . . . .
9
2.2.3
Archimedean copulas . . . . . . . . . . . . . . . . . . . . . . . . . .
11
2.2.4
Extreme value copulas . . . . . . . . . . . . . . . . . . . . . . . . .
14
2.3
Measures of association . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
18
2.4
Tail dependence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
21
2.4.1
Bivariate tail dependence . . . . . . . . . . . . . . . . . . . . . . . .
21
2.4.2
Multivariate tail dependence . . . . . . . . . . . . . . . . . . . . . .
22
2.4.3
Conditional tail dependence . . . . . . . . . . . . . . . . . . . . . .
23
2.5
Vines and vine copulas . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
24
2.5.1
Graph theory notation . . . . . . . . . . . . . . . . . . . . . . . . .
24
2.5.2
Regular vines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
25
2.5.3
Regular vine distributions and copulas . . . . . . . . . . . . . . . .
26
3
Statistical methods and backtesting
31
3.1
Assessment of model ﬁt . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
31
3.2
Model backtests . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
35
3.2.1
Value-at-Risk . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
35
3.2.2
Expected shortfall
. . . . . . . . . . . . . . . . . . . . . . . . . . .
37
4
Sampling Methods
39
4.1
Markov chain Monte Carlo methods . . . . . . . . . . . . . . . . . . . . . .
39
4.1.1
Gibbs sampler . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
39
4.1.2
Metropolis-Hastings algorithm . . . . . . . . . . . . . . . . . . . . .
40
4.2
Metropolis-Hastings within Gibbs . . . . . . . . . . . . . . . . . . . . . . .
41
4.2.1
Mode and curvature matching . . . . . . . . . . . . . . . . . . . . .
42
4.2.2
Expectation and variance matching . . . . . . . . . . . . . . . . . .
43
4.2.3
Independence and random walk samplers . . . . . . . . . . . . . . .
43
4.3
Adaptive Rejection Metropolis Sampling . . . . . . . . . . . . . . . . . . .
44
4.3.1
Acceptance-Rejection Sampling . . . . . . . . . . . . . . . . . . . .
44
4.3.2
Adaptive Rejection Sampling
. . . . . . . . . . . . . . . . . . . . .
45
4.3.3
Adaptive Rejection Metropolis Sampling . . . . . . . . . . . . . . .
47
5
Marginal models
49
5.1
ARMA-GARCH processes . . . . . . . . . . . . . . . . . . . . . . . . . . .
49
5.1.1
Deﬁnition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
49
5.1.2
Quasi-maximum likelihood estimation . . . . . . . . . . . . . . . . .
51
5.1.3
Implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
52
5.2
Dynamic linear models . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
54
ii

6
Factor copula models
59
6.1
Model formulation and examples
. . . . . . . . . . . . . . . . . . . . . . .
59
6.1.1
One-factor copula model . . . . . . . . . . . . . . . . . . . . . . . .
59
6.1.2
Two-factor copula model . . . . . . . . . . . . . . . . . . . . . . . .
62
6.1.3
Factor copula models with more than two factors
. . . . . . . . . .
67
6.2
Properties of the one- and two-factor copula model
. . . . . . . . . . . . .
67
6.2.1
Dependence properties . . . . . . . . . . . . . . . . . . . . . . . . .
67
6.2.2
Tail properties . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
69
7
Bayesian analysis of the one-factor copula model
72
7.1
Preliminaries
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
72
7.2
Prior for copula parameters
. . . . . . . . . . . . . . . . . . . . . . . . . .
73
7.3
Prior for latent variables . . . . . . . . . . . . . . . . . . . . . . . . . . . .
74
7.4
Likelihood for observations . . . . . . . . . . . . . . . . . . . . . . . . . . .
74
7.5
Posteriors and full conditional densities . . . . . . . . . . . . . . . . . . . .
75
8
Simulation study
78
8.1
Simulated data
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
78
8.1.1
Scenarios
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
78
8.1.2
Simulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
78
8.2
Marginal maximum likelihood estimation . . . . . . . . . . . . . . . . . . .
79
8.3
Gibbs sampler . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
80
8.3.1
Starting values
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
81
8.3.2
ARMS within Gibbs
. . . . . . . . . . . . . . . . . . . . . . . . . .
82
8.3.3
Metropolis-Hastings within Gibbs . . . . . . . . . . . . . . . . . . .
82
8.3.4
Code validation . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
83
8.4
Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
85
8.4.1
Marginal maximum likelihood estimation . . . . . . . . . . . . . . .
85
8.4.2
Gibbs sampling with Gumbel linking copulas . . . . . . . . . . . . .
86
8.4.3
Thinned individual ARMS . . . . . . . . . . . . . . . . . . . . . . .
94
8.4.4
Posterior mode estimation . . . . . . . . . . . . . . . . . . . . . . .
96
8.4.5
Credible intervals for the latent variable
. . . . . . . . . . . . . . .
96
8.4.6
Gibbs sampling with Gaussian and survival Gumbel linking copulas
99
8.5
Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101
9
Empirical study
102
9.1
Historical data
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 102
9.2
Marginal models
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105
9.2.1
In-sample ARMA-GARCH . . . . . . . . . . . . . . . . . . . . . . . 105
9.2.2
Out-of-sample ARMA-GARCH . . . . . . . . . . . . . . . . . . . . 108
9.2.3
Time-varying ARMA DLM
. . . . . . . . . . . . . . . . . . . . . . 114
9.3
Dependence models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 119
9.3.1
Setup of vine copula models . . . . . . . . . . . . . . . . . . . . . . 119
9.3.2
Vine copula models with index data . . . . . . . . . . . . . . . . . . 120
9.3.3
Vine copula models without index data . . . . . . . . . . . . . . . . 124
9.3.4
Setup of factor copula models . . . . . . . . . . . . . . . . . . . . . 126
iii

9.3.5
One-factor copula model . . . . . . . . . . . . . . . . . . . . . . . . 126
9.3.6
Two-factor copula model . . . . . . . . . . . . . . . . . . . . . . . . 127
9.3.7
Comparison of vine and factor copula models
. . . . . . . . . . . . 128
9.4
Bayesian inference for the one-factor copula model . . . . . . . . . . . . . . 129
9.4.1
Behavior of the latent variable over time . . . . . . . . . . . . . . . 132
9.4.2
Joint distribution of the latent variable and bank stocks . . . . . . . 133
9.5
Value-at-Risk and expected shortfall forecasts
. . . . . . . . . . . . . . . . 139
9.5.1
Portfolio composition . . . . . . . . . . . . . . . . . . . . . . . . . . 139
9.5.2
Value-at-Risk and expected shortfall backtests . . . . . . . . . . . . 140
10 Conclusions and outlook
143
A Simulation study: Figures and tables
144
A.1 Traceplots and histograms . . . . . . . . . . . . . . . . . . . . . . . . . . . 144
A.2 Boxplots of latent variable . . . . . . . . . . . . . . . . . . . . . . . . . . . 185
A.3 Acceptance rate and eﬀective sample size . . . . . . . . . . . . . . . . . . . 204
A.4 Posterior mode estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . 207
B Empirical study: Figures and tables
209
B.1 In-sample ARMA–GARCH . . . . . . . . . . . . . . . . . . . . . . . . . . . 209
B.2 Time-varying ARMA DLM . . . . . . . . . . . . . . . . . . . . . . . . . . . 211
B.3 Vine copula models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 214
B.3.1
ARMA-GARCH vine copula ﬁt tables
. . . . . . . . . . . . . . . . 214
B.3.2
DLM vine copula ﬁt tables . . . . . . . . . . . . . . . . . . . . . . . 221
B.4 Estimated parameters of the one- and two-factor copula models
. . . . . . 228
B.5 Bayesian inference for the one-factor copula model . . . . . . . . . . . . . . 231
B.5.1
Trace and density plots of years 2005 to 2013
. . . . . . . . . . . . 231
B.5.2
Trace and density plots of individual years . . . . . . . . . . . . . . 235
B.5.3
Posterior mode estimates . . . . . . . . . . . . . . . . . . . . . . . . 263
B.5.4
Behavior of the latent variable over time . . . . . . . . . . . . . . . 264
B.6 Value-at-Risk and expected shortfall forecasts
. . . . . . . . . . . . . . . . 274
B.6.1
Vine copula ﬁts of individual years
. . . . . . . . . . . . . . . . . . 274
List of Figures
279
List of Tables
286
List of Algorithms
289
iv

1
Introduction
After the initial work done by Sklar (1959), it took several decades for copula based
dependence models to rise to prominence. Although the copula approach to multivariate
dependence oﬀers a plethora of possibilities, copulas also require a high degree of skill
in implementation.
In a majority of ﬁnancial applications, Gaussian and Student’s t
copulas are applied to model the dependence in portfolios, even though these models
often fail to account for asymmetries or tail dependence in the data (see Embrechts,
2009). Consequently, some of the blame of the ﬁnancial crisis was attributed to copula
models, since they failed to accurately depict the risk of joint failures (see Salmon, 2012).
More general high dimensional copulas suﬀer from increased theoretical and computa-
tional complexity, motivating the more recent introduction of vine copulas by a series of
works by Joe (1997), Bedford and Cooke (2002), Kurowicka and Cooke (2006) and Aas
et al. (2009). Vine copulas are a ﬂexible class of copula models that create higher dimen-
sional dependence structures out of well-studied bivariate building blocks in a pair-copula
construction, circumventing the use of a single high-dimensional copula.
In many applications, the dependence of observed events can also be explained by un-
observed latent variables, such as economic factors. Generalizing a result of Joe (2011),
Krupskii and Joe (2013) suggest the use of factor copula models that are related to vine
copulas, when the assumption of an underlying factor structure is justiﬁed. Factor copula
models generalize multivariate normal models that explain the dependence in random
variables by a linear relation of a few normally distributed latent factors. Similarly to
vine copulas, bivariate copula building blocks are used to achieve an adaptable overarching
structure.
The aim of this Master’s thesis is to compare the performance of vine and factor copula
models based on a data set consisting of a time series of ﬁnancial stock returns. As part
of modeling multivariate data with copulas, marginal time series models are examined
in a ﬁrst step, before the multivariate dependence structure can be analyzed. In order
to facilitate a full Bayesian inference of the stock returns, a Markov chain Monte Carlo
method is derived for the factor copula model, in the case when only one underlying
latent factor is assumed. In conjunction with marginal dynamic linear models (DLM),
this enables forecasting of key risk measures such as Value-at-Risk and expected short-
fall. Furthermore, Bayesian analysis of the factor copula model allows the study of the
unobserved variable, providing additional information about historical random events.
The remainder of this thesis is structured in the following way. Section 2 states some fun-
damental results from probability theory, provides deﬁnitions of distributions and copula
families, and gives an introduction into copula and vine copula theory.
Additionally,
measures to quantify association and tail behavior are discussed. In Section 3 statistical
methods, tests and backtests are presented, which are used to assess the goodness-of-ﬁt
of models and to evaluate forecasting performance. General sampling methods with fo-
cus on Metropolis-Hastings sampling in the context of a Gibbs sampler are introduced
in Section 4. Section 5 provides the deﬁnition of classical autoregressive moving average
(ARMA) generalized auto-regressive conditional heteroskedastic (GARCH) and Bayesian
DLMs, which are used as marginal time series models for the ﬁnancial data set. Factor
copula models and some of their most important properties constitute the content of Sec-
tion 6. As a special case of the general factor copula models, a Bayesian analysis of the
1

1
Introduction
one-factor copula model is derived in Section 7 and validated in a simulation study in
Section 8. Finally, in Section 9, all of the previous considerations are combined in a study
of an empirical data set of daily log returns. The performance of vine copula and factor
copula models for multivariate dependence is compared and the Bayesian forecast of risk
measures analyzed.
2

2
Probability theory and copulas
This section provides a brief overview over results from probability theory that are used
in proofs in the later parts of the thesis, as well a short introduction to copulas and
important measures of dependence. Finally, some basics of the more involved vine copula
theory are provided.
Concerning notation, a probability space (Ω, F, P) is assumed for all random variables,
and will not be explicitly mentioned in the following. E and Var denote the expectation
and variance, if they exist, of a random variable X ∼F, respectively, where F is the
(cumulative) distribution function or density of X. ran X denotes the range of random
variable X, i.e., the set of values that X can obtain.
Section 2.1 contains some results from probability theory and the deﬁnitions of named
distributions that occur in this thesis.
In Section 2.2 fundamentals of copula theory,
including the seminal theorem by Sklar, and parametric families that are common in
practice are presented. Sections 2.3 and 2.4 focus on measuring the diﬀerences in the
dependence structure of vectors of random variables and their relation to copulas. Finally,
Section 2.5 demonstrates how bivariate copulas can be used in pair-copula constructions
to build dependence structures in high dimensions.
2.1
Basics and distributions
Some results from probability theory, and distributions that are used in the later parts of
the thesis are given in the following. Thorough introductions to probability theory can
be found in, e.g., Gut (2009) and Durrett (2010).
Deﬁnition 2.1 (Mode and curvature). Let f be a twice continuously diﬀerentiable density
with support S. Then, the mode xm of f is deﬁned as
xm ··= arg max
x∈S
f(x),
and the curvature c at point x ∈S as
c(x) ··= f ′′(x).
Note that xm must not be unique.
Theorem 2.2 (Continuous law of total probability). Let X ∼FX, E[X] < ∞and
Y ∼FY be continuous random variables. Then, for all x ∈ran X and y ∈ran Y
P(X ≤x) =
Z ∞
−∞
E[1{X≤x} | Y = y] dFY (y) =
Z ∞
−∞
FX| Y (x | y) dFY (y).
Proof. See (Gut, 2009, Theorem 2.1).
Important distributions and a few of their properties that are used later in the thesis are
given in the following.
Deﬁnition 2.3 (Bernoulli distribution). A random variable X ∼Ber(p) with probability
mass function
P(X = i) = pi(1 −p)1−i,
i = 0, 1,
p ∈[0, 1],
has a Bernoulli distribution with success probability p.
3

2
Probability theory and copulas
Deﬁnition 2.4 (Uniform distribution). A random variable U ∼U(a, b) with density
f(x; a, b) = 1{a≤x≤b},
x ∈R,
a < b, a, b ∈R,
where 1 is the indicator function, has a (continuous) uniform distribution on the interval
(a, b).
If a = 0 and b = 1, U(0, 1) is called standard uniform distribution. U(0, 1) is frequently
used for sampling random variates and it is the marginal distribution of copulas.
Deﬁnition 2.5 (Beta distribution). A random variable X ∼Beta(α, β) with density
f(x; α, β) =
1
B(α, β)xα−1(1 −x)β−1,
x ∈[0, 1],
α, β > 0,
has a Beta distribution with shape parameters α and β. B is the beta function deﬁned
as
B(x, y) =
Z 1
0
tx−1(1 −t)y−1 dt,
x, y > 0.
Expectation and variance of X ∼Beta(α, β) are given by
E[x] =
α
α + β ,
Var(X) =
αβ
(α + β)2(α + β + 1)
,
respectively (see Gut, 2009, p. 283), and mode and curvature by
xm(α, β) =
α −1
α + β −2,
α, β > 1,
and
c(x, α, β) =
1
B(α, β) xα−3(1 −x)β−3
α2 + x2(α + β −3)(α + β −2)
−2(α −1)x(α + β −3) −3α + 2

,
respectively. For α = β = 1, the Beta distribution coincides with the uniform distribution.
Deﬁnition 2.6 (Gamma distribution). A random variable X ∼G(s, r) with density
f(x; s, r) =
rs
Γ(s)xs−1e−rx,
x > 0,
r, s > 0
has a Gamma distribution with shape s and rate r. Γ is the gamma function deﬁned as
Γ(t) =
Z ∞
0
xt−1e−x dx,
t > 0.
Expectation and variance of X ∼Gamma(r, s) are given by
E[X] = r
s,
Var(X) = r
s2,
respectively (see Gut, 2009, p. 283).
4

2.1
Basics and distributions
Deﬁnition 2.7 (Normal distribution). A random variable X ∼N(µ, σ2) with density
f(x; µ, σ2) =
1
√
2πσ2 e−(x−µ)2
2σ2 ,
x ∈R,
µ ∈R, σ2 > 0,
has a normal distribution with expectation µ and variance σ2.
If µ = 0 and σ2 = 1, N(0, 1) is called standard normal distribution. The density of
the standard normal distribution is denoted by ϕ, its distribution function by Φ and its
quantile function by Φ−1.
In the multivariate case, a d-dimensional random vector X ∼N(µ, Σ) with density
f(x; µ, Σ) = (2π)−d
2|Σ|−1
2e−1
2 (x−µ)⊤Σ−1(x−µ),
x ∈Rd,
µ ∈Rd, Σ ∈[−1, 1]d×d,
has a multivariate normal distribution with mean vector µ and symmetric positive deﬁnite
covariance matrix Σ.
Deﬁnition 2.8 (Truncated normal distribution). A random variable X ∼N(a,b)(µ, σ2)
with density
f(x; a, b, µ, σ2) =
1
√
2πσ2e−(x−µ)2
2σ2
Φ
  b−µ
σ

−Φ
  a−µ
σ
,
x ∈(a, b),
a, b ∈[−∞, ∞],
µ ∈R, σ2 > 0,
where ϕ and Φ are the density and distribution function of the standard normal distribu-
tion, respectively, has a truncated normal distribution.
Mode and curvature of X ∼N(a,b)(µ, σ2) are given by
xm(a, b, µ, σ2) =





a,
µ < a,
µ,
a ≤µ ≤b,
b,
µ > b,
and
c(x, a, b, µ, σ2) = e−(x−µ)2
2σ2 (µ2 −σ2 + x2 −2µx)
√
2π σ5 Φ
  b−x
σ

−Φ
  a−x
σ
 ,
respectively. For a = −∞and b = ∞the truncated normal and normal distributions
coincide.
Deﬁnition 2.9 (Student’s t distribution). A random variable X ∼tν with density
f(x; ν) =
Γ
  ν+1
2

√νπ Γ
  ν
2


1 + x2
ν
−ν+1
2
,
x ∈R,
ν > 0,
has a (Student’s) t distribution with ν degrees of freedom. For ν →∞the Student’s t
distribution converges to a standard normal distribution.
In the multivariate case, a d-dimensional random vector X ∼tν(Σ) with density
f(x; ν, Σ) = (νπ)−d
2|Σ|−1
2 Γ
  ν+2
2

Γ
  ν
2


1 + x⊤Σ−1x
ν
−ν+d
2
,
x ∈R,
ν > 0, Σ ∈[−1, 1]d×d,
5

2
Probability theory and copulas
has a multivariate (Student’s) t distribution with ν degrees of freedom and symmetric
positive deﬁnite dispersion matrix Σ. Note that Σ is not the variance of X ∼tν(Σ),
instead the variance is given by
Var(X) =
ν
ν −2 Σ,
ν > 2,
and not deﬁned for ν ≤2.
In some cases, the non-central multivariate Student’s t distribution is denoted by Y ∼
tν(µ, Σ) with µ ∈Rd. Y can be derived from X ∼tν(Σ) by Y
d= µ + X.
Deﬁnition 2.10 (Chi-square distribution). A random variable X ∼χ2
ν with density
f(x; ν) =
1
Γ
  n
2
 x
1
2 n−1
1
2
 n
2
e−x
2 ,
x > 0,
ν ∈N,
has a chi-square distribution with ν degrees of freedom.
The α quantile of the chi-square distribution with ν degrees of freedom is denoted by χ2
α,ν.
2.2
Copulas
While the fundamental idea of copulas, the decomposition of multivariate distribution
functions into their univariate margins and a multivariate copula, goes back to Sklar
(1959), a broad adoption in practice has been more recent.
The ability to treat well
understood marginal distributions and the multivariate dependence structure of a random
vector separately, is a powerful tool if applied correctly. However, copulas have many
pitfalls of their own, and the fast adoption of copula theory in ﬁnance and risk management
is criticized by, e.g., Mikosch (2006).
Some of the blame for the ﬁnancial crisis of the years 2008 and 2009 is often attributed to
the use of the Gaussian copula for the pricing of collateralized debt obligations (CDO),
pioneered by Li (2000).
Therefore, it is important to keep both the possibilities and
limitations of copulas in mind.
The following sections provide some of the most important basics of copulas and introduce
parametric copula families that are used in the later parts of the thesis.
For the presented copula families, in some cases, Kendall’s tau and lower (upper) tail
dependence λL (λU) are provided. The deﬁnition of these measures and some of their
properties are given in Sections 2.3 and 2.4, respectively.
Discussion is limited to absolutely continuous copulas with suitably nice distribution
functions. A thorough introduction to copulas is provided by Nelsen (2006) and Mai and
Scherer (2012) with applications to ﬁnance and risk management described in Mai and
Scherer (2014), McNeil et al. (2005) and Embrechts et al. (2003).
2.2.1
Basic properties
This section provides the deﬁnition and a few of the fundamental properties of copulas.
Deﬁnition 2.11 (Copula). A d-dimensional distribution function C with standard uni-
form U(0, 1) margins is called a copula.
6

2.2
Copulas
Theorem 2.12 (Sklar’s theorem).
1. Let F be a d-dimensional distribution function with margins F1, . . . , Fd. Then, there
exists a copula C such that
F(x1, . . . , xd) = C(F1(x1), . . . , Fd(xd)),
x1, . . . , xd ∈R.
C is unique on Qd
j=1 ran Fj and given by
C(u1, . . . , ud) = F(F −1
1 (u1), . . . , F −1
d (ud)),
uj ∈ran Fj, j = 1, . . . , d.
2. If C is a copula and Fj, j = 1, . . . , d, are univariate distribution functions, then
F as deﬁned above is a d-dimensional distribution function with univariate margins
Fj.
Proof. See Sklar (1996) and R¨uschendorf (2009, Theorem 2.2).
Analogously to Sklar’s theorem, d-dimensional survival functions F can also be decom-
posed into marginal survival functions and a copula.
Theorem 2.13 (Survival version of Sklar’s theorem). Let X1, . . . , Xd ∼F, where F is
a d-dimensional distribution function and F its survival functions with marginal survival
functions F 1, . . . , F d, i.e., for all x1, . . . , xd ∈R,
F(x1, . . . , xd) = P(X1 > x1, . . . , Xd > xd)
and
F j(xj) = P(Xj > xj),
for j = 1, . . . , d. Then, there exists a d-dimensional copula bC, such that
F(x1, . . . , xd) = bC(F 1(x1), . . . , F d(xd)),
x1, . . . , xd ∈R.
If F1, . . . , F d are continuous, bC is unique. In this case, bC is called the survival copula of
X1, . . . , Xd.
Note that bC is a proper distribution function, while F is not in general.
Proof. See Mai and Scherer (2012, Theorem 1.3).
It is possible to state sharp bounds for the distribution functions of copulas.
These
become particularly important in the context of measures of association (see Section 2.3)
by providing theoretical maximal and minimal values.
Theorem 2.14 (Fr´echet-Hoeﬀding bounds). Let C be a d-dimensional copula, then
W(u) ≤C(u) ≤M(u),
u = (u1, . . . , ud) ∈[0, 1]d,
where
W(u) = max{u1 + . . . + ud −d + 1, 0}
and
M(u) = min{u1, . . . , ud}
are the lower and upper Fr´echet-Hoeﬀding bounds, respectively. For d = 2 W is often
called the countermonotonic and M the comonotonic copula.
7

2
Probability theory and copulas
Proof. See Joe (1997, Theorem 3.1).
While M is a copula for all d ∈N, W is only a copula for d = 2. It can still be shown
that the lower bound is sharp by the following theorem.
Theorem 2.15. For all d ≥3 and u ∈[0, 1]d there exists a copula C such that
C(u) = W(u).
Proof. See Nelsen (2006, Theorem 2.10.13).
For the simulation of copulas, the notion of conditional copula is important. Its inverse is
used for sampling from arbitrary copulas. In two dimensions, the algorithm is given by,
e.g., Mai and Scherer (2012, Algorithm 1.2).
Deﬁnition 2.16 (Conditional copula). Let C be a d-dimensional copula and U ∼C.
The conditional probability of Uj ≤uj given U1 = u1, . . . , Uj−1 = uj−1, j = 1, . . . , d, is
called the conditional copula (function) of Uj given U1 = u1, . . . , Uj−1 = uj−1, denoted by
C(uj | u1, . . . , uj−1) ··= P(Uj ≤uj | U1 = u1, . . . , Uj−1 = uj−1).
The generalization of the conditional copulas to higher dimensions requires the following
notation.
Let C be a d-dimensional copula. Then, for j = 1, . . . , d, and for all x = (x1, . . . , xd) ∈Rd
and u = (u1, . . . , ud) ∈(0, 1)d
D1,...,jC(x) ··=
∂j
∂u1 · · · ∂uj
C(u)

u=x.
The density c of copula C can therefore be written as the special case j = d,
c(x1, . . . , xd) ··= D1,...,dC(x1, . . . , xd).
In general, let I ⊂{1, . . . , d}, such that I = {i1, . . . , ij}, ik ̸= iℓfor k ̸= ℓ. Then,
DIC(x) ··=
∂j
∂ui1∂ui2 · · · ∂uij
C(u)

u=x.
Theorem 2.17. Let C be a d-dimensional copula which admits for d ≥3 continuous
partial derivatives with respect to the ﬁrst d −1 arguments. Then,
C(uj | u1, . . . , uj−1) =
Dj−1,...,1C(1,...,j)(u1, . . . , uj)
Dj−1,...,1C(1,...,j−1)(u1, . . . , uj−1)
for all j = 2, . . . , d and almost every u1, . . . , uj−1 ∈(0, 1), where C(j1,...,jk)(uj1, . . . , ujk)
denotes C(u1, . . . , ud) with uj = 1 for all j ̸∈{j1, . . . , jk}.
In particular, for a bivariate copula C and j = 2, it holds
C(u2 | u1) = D1C(1,2)(u1, u2)
D1C(1)(u1)
=
∂C(u1,u2)
∂u1
∂C(u1,1)
∂u1
=
∂
∂u1
C(u1, u2),
since C(u1, 1) = u1, for all u1 ∈(0, 1).
8

2.2
Copulas
Proof. See Schmitz (2003, Theorem 2.27).
Remark 2.18 (Rotated copulas). It is possible to modify existing bivariate copula famil-
ies to allow for more general forms of dependence. For a copula C that admits only positive
dependence, e.g., the 90◦rotated copula exhibits only negative dependence. Rotation by
180◦leads to the reﬂected or survival copula bC.
A list of the most commonly used rotations of an absolutely continuous bivariate copula
density c, and u1, u2 ∈(0, 1), is given below:
1. 90◦: c90(u1, u2) ··= c(1 −u1, u2),
2. 180◦: c180(u1, u2) ··= c(1 −u1, 1 −u2),
3. 270◦: c270(u1, u2) ··= c(u1, 1 −u2).
2.2.2
Elliptical copulas
Elliptical copulas can be derived from elliptical distributions by the application of Sklar’s
theorem. For a d-dimensional elliptical distribution with distribution function F, margins
F1, . . . , Fd and corresponding quantile functions F −1
1 , . . . , F −1
d , the elliptical copula is given
by C(u1, . . . , ud) = F(F −1
1 (u1), . . . , F −1
d (ud)), u1, . . . , ud ∈[0, 1].
Therefore, since the
quantile functions F −1
j , j = 1, . . . , d can often not be stated explicitly, elliptical copulas
are most of the time implicitly given by a multidimensional integral. Prominent examples
of elliptical distributions are, e.g., the multivariate normal or Student’s t distribution.
In general, elliptical distributions are aﬃne transformations of spherical distributions. Let
X be a random variable from a d-dimensional elliptical distribution. Then, it follows
X
d= µ + AY ,
where µ ∈Rd, d ∈N, A ∈Rd×k, k ∈N, and Y is uniformly distributed on the unit
sphere Sk.
A deeper analysis of spherical and elliptical distributions, and their properties is not
covered in this thesis. Thorough discussions are given by, e.g, McNeil et al. (2005, Chapter
3.3) and Mai and Scherer (2012, Chapter 4).
In the following some basic properties and important examples of elliptical copulas are
given.
Deﬁnition 2.19. Let F be the distribution function and F −1 the quantile function of an
elliptical distribution, respectively. Then, a copula C with
C(u1, . . . , ud) = F(F −1
1 (u1), . . . , F −1
d (ud)),
u1, . . . , ud ∈[0, 1],
is called an elliptical copula.
Lemma 2.20. Let C be an elliptical copula. Then,
1. C is radially symmetric, i.e., C = bC,
2. the upper and lower tail dependence coeﬃcients are the same, i.e., λL = λU (see
Deﬁnition 2.47).
9

2
Probability theory and copulas
Proof. See Mai and Scherer (2012, Lemma 4.6).
A quick way to identify radial symmetry in a bivariate data set is to check whether it is
possible to mirror the scatter plot at the u 7→1 −u line. If this leads to (roughly) the
same plot, an elliptical copula might prove an adequate choice of dependence structure.
The two most prominent elliptical copulas are the Gaussian and Student’s t copula. Since
the Gaussian copula does not exhibit any tail dependence, it is not suitable for modeling
extreme events.
In this case, the Student’s t copula oﬀers an alternative, when tail
dependence is desired.
Deﬁnition 2.21 (Gaussian copula). Let ΦP be the distribution function of d-dimensional
multivariate normal distribution Nd(0, P ), with symmetric positive deﬁnite correlation
matrix P ∈[−1, 1]d×d. Then, the copula C given by
C(u1, . . . , ud; P ) = ΦP (Φ−1(u1), . . . , Φ−1(ud))
=
Z Φ−1(u1)
−∞
· · ·
Z Φ−1(ud)
−∞
1
q
(2π)d|P |
e−1
2 x⊤P −1x dx,
u1, . . . , ud ∈[0, 1],
where x = (x1, . . . , xd)⊤, is called Gaussian copula.
In the case d = 2, with correlation ρ ∈(−1, 1), the expression reduces to
C(u1, u2; ρ) =
Z Φ−1(u1)
−∞
Z Φ−1(u2)
−∞
1
2π
p
1 −ρ2 e
−
x2
1−2ρx1x2+x2
2
2(1−ρ2)
dx2 dx1.
Kendall’s tau, as a function of the copula parameter ρ of the Gaussian copula, is given
by
τ(ρ) = 2
π arcsin(ρ) ∈(−1, 1),
ρ ∈(−1, 1).
(2.1)
Since the conditional distribution function of the bivariate Gaussian copula will be used
in later parts of the thesis, it is explicitly stated here.
Lemma 2.22 (Conditional bivariate Gaussian copula). Let C be a bivariate Gaussian
copula and (U1, U2) ∼C. The conditional bivariate Gaussian copula of U1 = u1 | U2 = u2
with correlation ρ ∈(−1, 1) is given by
CU1| U2(u1 | u2) = Φ
Φ−1(u1) −ρ Φ−1(u2)
p
1 −ρ2

,
u1, u2 ∈[0, 1].
Proof. See Mai and Scherer (2012, Example 5.3).
Deﬁnition 2.23 (Student’s t copula). Let t−1
ν
be the quantile function of the Student’s
t distribution with ν degrees of freedom. Let tν,P be the distribution function of the
d-dimensional Student’s t distribution with symmetric positive deﬁnite dispersion matrix
P ∈[−1, 1]d×d. Then, for u1, . . . , ud ∈[0, 1] the copula C given by
C(u1, . . . , ud; P , ν) = tν,P (t−1
ν (u1), . . . , t−1
ν (ud))
=
Z t−1
ν (u1)
−∞
· · ·
Z t−1
ν (ud)
−∞
Γ
  ν+d
2

Γ
  ν
2
q
(πν)d|P |

1 + x⊤P −1x
ν
−ν+d
2
dx,
10

2.2
Copulas
where x = (x1, . . . , xd)⊤, is called Student’s t copula.
In the case d = 2, with correlation ρ ∈(−1, 1) and degrees of freedom ν > 2, the
expression reduces to
C(u1, u2; ρ, ν) =
Z t−1
ν (u1)
−∞
Z t−1
ν (u2)
−∞
1
2π
p
1 −ρ2

1 + x2
1 −2ρx1x2 + x2
2
ν(1 −ρ2)
−ν+2
2
dx2 dx1.
The relationship between ρ and Kendall’s tau remains the same as for the Gaussian copula,
given by (2.1). Thus, it is unaﬀected by the degrees of freedom parameter ν.
2.2.3
Archimedean copulas
Archimedean copulas oﬀer more ﬂexibility than elliptical copulas for many applications by,
e.g, allowing for asymmetric tails. The name Archimedean copula stems from an analytical
property and was ﬁrst used in Ling (1965). Archimedean copulas oﬀer a wide array of
possible dependencies, while retaining analytical tractability due their construction. More
details about Archimedean copulas can be found in, e.g., Mai and Scherer (2012, Chapter
2) and Joe (1997, Chapter 5).
Deﬁnition 2.24 (Archimedean copula). A d-dimensional Copula C that has a functional
representation
Cϕ(u1, . . . , ud) = ϕ

d
X
j=1
ϕ−1(uj)

,
u1, . . . , ud ∈[0, 1],
(2.2)
for a non-increasing function ϕ : [0, 1]d →[0, 1] with ϕ(0) = 1, limx→∞ϕ(x) = 0 and
inverse ϕ−1, is called an Archimedean copula. Function ϕ is called an (Archimedean)
generator.
Remark 2.25 (Exchangeable copulas). By deﬁnition, the distribution of an Archimedean
copula is independent of permutations of (u1, . . . , ud). Copulas with this property are
called exchangeable. In the bivariate case, scatter plots of exchangeable copulas can be
detected if the plots stays the same when mirrored at the u 7→u line.
Example 2.26 (Independence copula). Choosing generator ϕ(x) = e−x in (2.2), it follows
ϕ−1(y) = −ln(y), and it can be seen that the independence copula Π given by
Π(u1, . . . , ud) = e−
 −Pd
j=1 ln(uj)

=
d
Y
j=1
uj,
u1, . . . , ud ∈(0, 1),
is an Archimedean copula.
In the following, some commonly used families of Archimedean copulas and a few of their
properties are given.
Deﬁnition 2.27 (Clayton copula). A d-dimensional Archimedean copula with generator
function
ϕθ(x) = (1 + x)−1
θ ,
11

2
Probability theory and copulas
inverse generator function
ϕ−1
θ (y) = y−θ −1,
x, y ∈(0, 1), and distribution function
C(u1, . . . , ud; θ) =

d
X
j=1
u−θ
j
−d + 1
−1
θ
,
u1, . . . , ud ∈[0, 1],
for θ > 0, is called Clayton copula. Its name is derived from Clayton (1978).
In the case d = 2, the density of the Clayton copula is given by
c(u1, u2; θ) = (1 + θ)(u1u2)−θ−1(u−θ
1
+ u−θ
2
−1)
−1
θ −2,
u1, u2 ∈(0, 1).
Kendall’s tau, as a function of the copula parameter θ of the Clayton copula, is given by
τ(θ) =
θ
θ + 2 ∈(0, 1),
θ > 0.
Deﬁnition 2.28 (Gumbel copula). A d-dimensional Archimedean copula with generator
function
ϕθ(x) = e−x1/θ,
inverse generator function
ϕ−1
θ (y) = (−ln(y))θ
x, y ∈(0, 1), and distribution function
C(u1, . . . , ud; θ) = e
 −Pd
j=1(−ln(uj))θ1/θ
,
u1, . . . , ud ∈[0, 1],
for θ ≥1, is called Gumbel copula.
In the case d = 2, the density of the Gumbel copula is given by
c(u1, u2; θ) = 1
uv
 (−ln(u1)(−ln(u2)
θ−1e
 −(−ln(u1))θ−(−ln(u2))θ1/θ

(1 −θ)
 −(−ln(u1))θ −(−ln(u2))θ 1
θ −2 +
 −(−ln(u1))θ −(−ln(u2)θ 2
θ −2
for u1, u2 ∈(0, 1).
Kendall’s tau, as a function of the copula parameter θ of the Gumbel copula, is given by
τ(θ) = θ −1
θ
∈[0, 1),
θ ≥1.
Deﬁnition 2.29 (Frank copula). A d-dimensional Archimedean copula with generator
function
ϕθ(x) = −ln
e−θx −1
e−θ −1

,
inverse generator function
ϕ−1
θ (y) = −ln
(e−θ −1)e−y
θ

12

2.2
Copulas
x, y ∈(0, 1), and distribution function
C(u1, . . . , ud; θ) = −1
θ ln

1 +
Qd
j=1(e−θuj −1)
e−θ −1

,
u1, . . . , ud ∈[0, 1],
for θ > 0, is called a Frank copula.
In the case d = 2, the density of the Frank copula is given by
c(u1, u2; θ) = −
θ(eθ −1)eθ(u1+u2+1)
(eθ −eθ(u1+1) −eθ(u2+1) + eθ(u1+u2))2,
u1, u2 ∈(0, 1).
Kendall’s tau, as a function of the copula parameter θ of the Frank copula, is given by
τ(θ) = 1 + 4D1(θ) −1
θ
∈(0, 1),
θ > 0,
where D1 is the ﬁrst order Debye function given by (see Abramowitz and Stegun, 1972,
p. 998)
D1(θ) = 1
θ
Z θ
0
t
et −1 dt.
While the previous one-parametric Archimedean copulas are the ones that are most com-
monly used, there are also Archimedean copulas with two parameters that are often
helpful in the context of ﬁnance. Since these families have a more complex form, densities
are omitted.
Deﬁnition 2.30 (BB1 (Clayton-Gumbel) copula). A d-dimensional Archimedean copula
with generator function
ϕθ,δ(x) = (1 + x
1
δ )
−1
θ ,
inverse generator function
ϕ−1
θ,δ(y) = (y−θ −1)
δ
x, y ∈(0, 1), and distribution function
C(u1, . . . , ud; θ, δ) =
 
1 +

d
X
j=1
(u−θ
j
−1)
δ 1
δ !−1
θ
.
for θ > 0, δ ≥1, is called BB1 (Clayton-Gumbel) copula.
Kendall’s tau, as a function of the copula parameters θ and δ of the BB1 copula, is given
by
τ(θ, δ) = 1 −
2
δ(θ + 2) ∈
1
2, 1

,
θ > 0, δ ≥1.
Deﬁnition 2.31 (BB8 (Frank-Joe) copula). A d-dimensional Archimedean copula with
generator function
ϕθ,δ(x) = 1
δ

1 −

1 −
 1 −(1 −δ)θ
e−x 1
θ 
,
13

2
Probability theory and copulas
inverse generator function
ϕ−1
θ,δ(y) = ln
 (1 −δ)θ −1
(1 −δy)θ −1

x, y ∈(0, 1), and distribution function
C(u1, . . . , ud; θ, δ) = 1
δ
 
1 −

1 −
1 −Qd
j=1(1 −(1 −δuj)θ)
1 −(1 −δ)θ
 1
θ !
for θ ≥1, δ ∈(0, 1], is called BB8 (Frank-Joe) copula.
Kendall’s tau, as a function of the copula parameter θ and δ of the BB8 copula, is given
by
τ(θ, δ) = 1 −4
Z 1
0
ln
(1 −δt)θ −1
(1 −δ)θ −1
1 −δt −(1 −δt)−θ + (1 + δt)−θδt
θδ
dt ∈(0, 1),
for θ ≥1, δ ∈(0, 1].
2.2.4
Extreme value copulas
Extreme value copulas arise in multivariate extreme value theory and oﬀer convenient
properties to make them more analytically tractable.
More formally, let F be a d-
dimensional distribution function with margins F1, . . . , Fd. Let independent identically
distributed (i.i.d.) random vectors
 (X(n)
1 , . . . , X(n)
d )

n∈N, (X(n)
1 , . . . , X(n)
d ) ∼F, be given,
and deﬁne
M n
k ··= max

X(1)
k , . . . , X(n)
k
	
,
k = 1, . . . , d,
as the component-wise maxima. Then, if there exist appropriate sequences (an
1)n∈N,. . .,
(an
d)n∈N, (bn
1)n∈N, . . . , (bn
d)n∈N and a random vector (X1, . . . , Xd) such that
M n
1 −an
1
bn
1
, . . . , M n
d −an
d
bn
d

d
−→(X1, . . . , Xd),
n →∞,
it can be shown that the copula of (X1, . . . , Xd) must be an extreme value copula.
A thorough discussion of extreme value copulas can be found in, e.g., Mai and Scherer
(2012, Chapter 1.2.5), Nelsen (2006, Chapter 3.3.4) and Joe (1997, Chapter 7.2).
Deﬁnition 2.32 (Extreme value copula). A d-dimensional copula C with
C(ut
1, . . . , ut
d) = C(u1, . . . , ud)t,
for all t > 0,
u1, . . . , ud ∈[0, 1],
is called an extreme value copula.
Some examples of extreme value copulas are the independence copula Π, the upper
Fr´echet-Hoeﬀding bound M and the Gumbel copula. Besides Π, the Gumbel copula is
the only Archimedean extreme value copula as shown in Mai and Scherer (2012, Theorem
4.5.2).
Furthermore, it can be shown that all extreme value copulas admit a speciﬁc representa-
tion.
14

2.2
Copulas
Theorem 2.33. A d-dimensional copula C is an extreme value copula if and only if there
exists a ﬁnite (positive) measure δ on
Sd ··= {(u1, . . . , ud) ∈[0, 1]d : u1 + . . . + ud = 1},
with
R
Sd udδ(du1, . . . , dud)=1, for j = 1, . . . , d, such that
C(u1, . . . , ud) = e
 Pd
j=1 ln(uj)

P

ln(u1)
Pd
j=1 ln(uj),...,
ln(ud)
Pd
j=1 ln(uj)

,
u1, . . . , ud ∈[0, 1].
P is called the Pickands dependence function and given by
P(w1, . . . , wd) =
Z
Sd
max{u1w1, . . . , udwd} δ(du1, . . . , dud),
for all (w1, . . . , wd) ∈Sd.
Proof. See Mai and Scherer (2012, Theorem 1.5).
A more general case of the bivariate Gumbel copula that allows for additional asymmetries
is given by the Tawn copula. Its A function was originally used as an asymmetric extension
of logistic regression (see Tawn, 1988).
Deﬁnition 2.34 (Tawn copula). A bivariate copula with distribution function
C(u1, u2; θ, α, β) = e
 ln(u1)+ln(u2)

A
 ln(u2)
ln(u1u2)

,
u1, u2 ∈[0, 1],
where
A(x) = (1 −α)x + (1 −β)(1 −x) +

αθ(1 −x)θ + βθxθ 1
θ ,
x ∈[0, 1],
for θ ≥1 and α, β ∈[0, 1], is called Tawn copula.
If α = β = 0, A(x) ≡1 and it follows C = Π. If α = β = 1, C is the Gumbel copula.
If α ̸= β, the Tawn copula is asymmetric. It is also possible to reduce the number of
parameters to two by setting either β = 1 or α = 1, these copulas are called Tawn type 1
and Tawn type 2, respectively.
15

2
Probability theory and copulas
Gaussian copula
U1
U2
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
ρ = 0.4  
ρ = 0.9  
Student's t copula
U1
U2
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
ρ = 0.4 , ν = 3  
ρ = 0.9 , ν = 6  
Clayton copula
U1
U2
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
θ = 0.7  
θ = 6  
Gumbel copula
U1
U2
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
θ = 1.3  
θ = 4  
Frank copula
U1
U2
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
θ = 2.4  
θ = 2.4  
BB1 copula
U1
U2
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
θ = 1 , σ = 1.5  
θ = 1.5 , σ = 2.5  
BB8 copula
U1
U2
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
θ = 1.5 , σ = 0.3  
θ = 0.3 , σ = 0.7  
Tawn copula
U1
U2
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
θ = 2 , α = 0.3 , β = 1  
θ = 5 , α = 1 , β = 0.7  
Figure 2.1: Contour plots of the density of the bivariate case of copula families intro-
duced in Section 2.2 for two parameter choices each. Margins are transformed to the
standard normal distribution.
16

2.2
Copulas
Summary of bivariate copula characteristics
Name
C
Parameter(s)
Kendall’s tau
λL
λU
Gaussian
R Φ−1(u1)
−∞
R Φ−1(u2)
−∞
1
2π√
1−ρ2 e
−
x2
1−2ρx1x2+x2
2
2(1−ρ2)
dx1 dx2
ρ ∈(0, 1)
2
π arcsin(ρ)
0
0
Student’s t
R t−1
ν (u1)
−∞
R t−1
ν (u2)
−∞
1
2π√
1−ρ2

1 + x2
1−2ρx1x2+x2
2
ν(1−ρ2)
−ν+2
2
dx1 dx2
ρ ∈(0, 1), ν > 2
2
π arcsin(ρ)
2 tν+1

−
q
(ν+1)(1−ρ)
1+ρ
!
λU = λL
Gumbel
e
 (−ln(u1))θ+(−ln(u2))θ1/θ
θ ≥1
1 −1
θ
0
2 −2
1
θ
Clayton
(u−θ
1
+ u−θ
2
−1)
−1
θ
θ > 0
θ
θ+2
2−1
θ
0
Frank
−1
θ ln

1 + (e−θu1+e−θu2−1)
e−θ−1

θ > 0
1 + 4D1(θ)−1
θ
0
0
D1(θ) = 1
θ
R θ
0
t
et−1 dt
BB1

1 +
 (u−θ
1
−1)
δ(u−θ
2
−1)
δ 1
δ −1
θ
θ > 0, δ ≥1
1 −
2
δ(θ+2)
2−1
θδ
2 −2
1
δ
BB8
1
δ

1 −(1 −1−(1−(1−δu1)θ)(1−(1−δu2)θ)
1−(1−δ)θ
)
1
θ

θ ≥1, δ ∈(0, 1]
1 −4
R 1
0 ln

(1−δt)θ−1
(1−δ)θ−1

1−δt−(1−δt)−θ+(1+δt)−θδt
θδ
dt
0
2 −2
1
θ , if δ = 1
Tawn
e
 ln(u1)+ln(u2)

A

ln(u2)
ln(u1u2)

θ ≥1, α, β ∈[0, 1]
R 1
0
t(1−t)A′′(t)
A(t)
dt
0
0
A(x) = (1 −α)x + (1 −β)(1 −x) +
 αθ(1 −x)θ + βθxθ 1
θ
Table 2.1: Summary of bivariate distribution functions, parameter ranges, Kendall’s tau values, and upper and lower tail depend-
ence for the copula families introduced in Section 2.2.
17

2
Probability theory and copulas
2.3
Measures of association
For applications it is often desirable to summarize the strength of dependence between
random variables with a single number. Although this obviously leads to a dramatic loss
of information compared to the full copula function, measures of association oﬀer a quick
assessment of the degree of association.
For simplicity, many of the following properties will only be stated for the bivariate case
d = 2.
As a basis for statements about measures of association with respect to copulas, the notion
of concordance ordering is important.
Deﬁnition 2.35 (Concordance ordering). Let C1 and C2 be two arbitrary d-dimensional
copulas. The concordance ordering is a partial ordering on the set of all copulas, given by
C1 ≼C2 ⇔C1(u) ≤C2(u)
for all u ∈[0, 1]d.
If C1 ≼C2, then C1 (or U1 ∼C1) is called less concordant than C2 (or U2 ∼C2).
Conversely, C2 (or U2 ∼C2) is called more concordant.
The most widely used measure of association is Pearson’s (linear/product moment) cor-
relation coeﬃcient.
Deﬁnition 2.36 (Pearson’s correlation). Let X1, X2 be random variables with E[X2
j ] <
∞, j = 1, 2. (Pearson’s) correlation coeﬃcient is deﬁned by
ρ = ρX1X2 = ρ(X1, X2) =
Cov(X1, X2)
p
Var(X1) Var(X2)
=
E

(X1 −E[X1])(X2 −E[X2])

q
E

(X1 −E[X1])2
E

(X2 −E[X2])2.
For n ∈N realizations (xi1, xi2), i = 1, . . . , n, of a random sample from (X1, X2), the
empirical (sample) version of Pearson’s correlation coeﬃcient is given by
ρn =
Pn
i=1(xi1 −x·1)(xi2 −x·2)
qPn
i=1 (xi1 −x·1)2 Pn
i=1 (xi2 −x·2)2,
where x·j = 1
n
Pn
i=1 xij, j = 1, 2.
Some of the most important characteristics of Pearson’s correlation coeﬃcient are given
in the following.
Remark 2.37. Let X1, X2 be random variables with E[X2
j ] < ∞, j = 1, 2. Then,
1. |ρ| ≤1 and |ρ| = 1 ⇔∃a, b ∈R, a ̸= 0 : X2 = aX1 + b a.s. with a > 0 (a < 0) ⇔
ρ = 1 (ρ = −1),
2. if X1, X2 are independent, ρ = 0,
3. ρ is invariant under strictly increasing linear transformations on ran X1 and ran X2,
4. if (Xi1, Xi2) has copula Ci and E[X2
ij] < ∞, i, j ∈{1, 2},
C1 ≼C2 ⇒ρ(X11, X12) ≤ρ(X21, X22).
18

2.3
Measures of association
For dimension d = 3, the notion of partial correlation is often useful.
Deﬁnition 2.38 (Partial correlation). Let X1, X2 and X3 be random variables with
E[X2
i ] < ∞, i = 1, 2. Then, the partial correlation (coeﬃcient) ρ12;3 of X1 and X2 given
X3 is deﬁned as
ρ12;3 =
Cov(X1, X2 | X3)
p
Var(X1 | X3) Var(X2 | X3)
.
ρ12;3 can also be expressed in terms of correlations ρ13 and ρ23 as
ρ12;3 =
ρ12 −ρ13ρ23
p
(1 −ρ2
13)(1 −ρ2
23)
.
Proof. See Fujikoshi et al. (2010, Deﬁnition 4.3.1).
Another useful application of Pearson’s correlation is autocorrelation. Autocorrelation
measures to what extent values of a time series depend on previously realized values.
In practice, one often assumes that realizations of a time series are independent if the
autocorrelation is close to zero. This line of reasoning will be used in the later parts
of this thesis to assess the independence of ﬁltered empirical data and dependence in a
Markov chain.
Deﬁnition 2.39 (Autocorrelation). Let (Xt)t∈N0 be a stochastic process. The autocor-
relation at lag k is deﬁned as
ρk = ρ(Xt, Xt−k),
k ≤t,
for all t ∈N0.
For n ∈N realizations xi, i = 1, . . . , n, of a random sample from (Xt)t∈N0, the sample
autocorrelation at lag k is given by
bρk = ck
c0
,
k = 0, . . . , n −1
where
ck = 1
n
n−k
X
i=1
(xi −x)(xi−k −x),
with x = 1
n
Pn
i=1 xi.
Since Pearson’s correlation has many drawbacks as a measure of association, Scarsini
(1984) introduced an axiomatic deﬁnition of (rank-correlation) measures of concordance.
Deﬁnition 2.40 (Measure of concordance). Let X1, X2 be random variables. A measure
of concordance is a function κ = κ(X1, X2) which satisﬁes the following conditions:
1. Domain: κ is deﬁned for every pair (X1, X2) of continuously distributed random
variables.
2. Range: κ ∈[−1, 1].
3. Symmetry: κ(X1, X2) = κ(X2, X1).
19

2
Probability theory and copulas
4. Change of sign: κ(−X1, X2) = −κ(X1, X2).
5. Independence: If X1 and X2 are independent, then κ(X1, X2) = 0.
6. Coherence: Let (Xi1, Xi2) be pairs of continuous random variables with copulas Ci,
i = 1, 2, and C1 ≼C2, then
κ(X11, X12) ≤κ(X21, X22).
7. Continuity: Let (Xn1, Xn2) ∼Fn, n ∈N, (X1, X2) ∼F with (Fn)n∈N, F continuous
and Fn →F for n →∞pointwise, then
κ(Xn1, Xn2)
n→∞
−−−→κ(X1, X2).
Some important properties that make measures of concordance preferable over Pearson’s
correlation are given below.
Lemma 2.41. Let κ be a measure of concordance and (X1, X2) a pair of continuously
distributed random variables with copula C. Then,
1. κ(X1, X2) = κ(C),
2. if Tj is strictly monotone increasing on ran Xj, j = 1, 2,
κ(T1(X1), T2(X2)) = κ(X1, X2),
3. if T1 is strictly monotone increasing on ran X1 and T2 strictly monotone decreasing
on ran X2,
κ(T1(X1), T2(X2)) = −κ(X1, X2),
4. κ(W) = −1 and κ(M) = 1.
Proof. See Nelsen (2006, Theorem 5.1.8.).
Part 1 of Lemma 2.41 shows that measuring concordance for arbitrary bivariate random
vectors can be reduced to measuring concordance of its copula. This allows the use of
relationships between measures of concordance and the parameters of a parametric copula
family for an assessment of association. It also enables a variety of estimation techniques.
The most prominent and widely used measures of concordance are Kendall’s tau and
Spearman’s rho. In this thesis, heavy use of existing one-to-one functional relationships
between Kendall’s tau and the parameter of one-parametric copula families is made,
therefore they have already been stated for the most important copulas in the previous
section. Formal deﬁnitions for Kendall’s tau and Spearman’s rho, and their corresponding
empirical versions are given in the following.
Deﬁnition 2.42 (Kendall’s tau). Let Xj ∼Fj, j = 1, 2, and (X′
1, X′
2) be an i.i.d. copy
of (X1, X2). Then Kendall’s tau is deﬁned as
τ = τX1X2 = τ(X1, X2) = E

sign((X1 −X′
1)(X2 −X′
2))

= P
 (X1 −X′
1)(X2 −X′
2) > 0

−P
 (X1 −X′
1)(X2 −X′
2) < 0

,
20

2.4
Tail dependence
where sign(x) = 1(0,∞)(x) −1(−∞,0)(x).
For n ∈N realizations (xi1, xi2), i = 1, . . . , n, of a random sample from (X1, X2), the
empirical (sample) version of Kendall’s tau is given by
τn =
1
 n
2

X
1≤i<j≤n
sign
 (xi1 −xj1)(xi2 −xj2)

,
i.e., when choosing two pairs (xi1, xi2) and (xj1, xj2), i ̸= j, i, j = 1, . . . , n, from n ob-
servations, the empirical Kendall’s tau is the fraction of the number concordant minus
discordant pairs over the total number of possible pairs.
Deﬁnition 2.43 (Spearman’s rho). Let Xj ∼Fj, j = 1, 2. Then Spearman’s rho is
deﬁned as
ρS = ρS(X1, X2) = ρ
 F1(X1), F2(X2)

.
(2.3)
For n realizations (xi1, xi2), i = 1, . . . , n, of a random sample from (X1, X2), the sample
(empirical) version of Spearman’s rho is given by
ρS,n =
Pn
i=1(Ri1 −R·1)(Ri2 −R·2)
qPn
i=1 (Ri1 −R·1)
2 Pn
i=1 (Ri2 −R·2)
2 =
12
n(n2 −1)
 n
X
i=1
Ri1Ri2

−n
n + 1
2
2
,
where R·j = 1
n
Pn
i=1 Rij = n+1
2
and Rij denotes the rank of xij within x1j, . . . , xnj, j = 1, 2.
In contrast to Pearson’s correlation, Kendall’s tau and Spearman’s rho are both measures
of accordance, as shown in the next theorem.
Theorem 2.44. If X1 and X2 are continuous random variables with copula C, then
Kendall’s tau from Deﬁnition 2.42 and Spearman’s rho from Deﬁnition 2.43 are measures
of concordance.
Proof. See Nelsen (2006, Theorem 5.1.9)
2.4
Tail dependence
In the context of copulas, tail dependence is used to measure joint extremal dependence
of two or more random variables, e.g., the lower-left and upper-right tails of bivariate
distributions. In conjunction with an appropriate choice of association, tail dependence
is an important property for the right choice of copula. Since the focus is on extremal
events, tail dependence is of great importance in risk management and modeling rare
events such as natural catastrophes.
In the following tail dependence will be deﬁned for dimensions d = 2 and d > 2 separately.
Since it is often useful in the context of vine copulas (see Section 2.5), the deﬁnition of
conditional tail dependence is also given.
2.4.1
Bivariate tail dependence
Similarly to general copula theory, the bivariate case of tail dependence is the one stud-
ied the most in literature. Unlike multivariate tail dependence, where several possible
extensions of the case with two random variables exist, for dimension d = 2 the lower and
upper tail dependence have well established deﬁnitions and convenient formulas.
21

2
Probability theory and copulas
Deﬁnition 2.45 (Upper and lower tail dependence). Let Xj ∼Fj with quantile functions
F −1
j , j = 1, 2. If these limits exist, then λL (λU) deﬁned as
λL = lim
u↓0 P(X2 ≤F −1
2 (u) | X1 ≤F −1
1 (u)),
λU = lim
u↑1 P(X2 > F −1
2
| X1 > F −1
1 (u)),
is called the lower (upper) tail dependence coeﬃcient.
In almost all practical applications the lower and upper tail dependence coeﬃcients exist,
but this does not necessarily have to be the case, as shown by Kortschak and Albrecher
(2009).
The following theorem establishes a connection between a copula and the tail dependence
coeﬃcients, and provides a useful formula to calculate their values for some copula families.
Theorem 2.46. Let random variables (X1, X2) have copula C. If λL and λU exist, then
they are given by
λL = lim
u↓0
C(u, u)
u
,
λU = 2 −lim
u↑1
1 −C(u, u)
1 −u
= lim
u↓0
bC(u, u)
u
.
Proof. See Nelsen (2006, Theorem 5.4.2)
2.4.2
Multivariate tail dependence
As an extension to the thoroughly studied bivariate tail dependence, multivariate tail
dependence functions can be deﬁned. Those functions are a useful tool when analyzing
tail dependence behavior in vine copulas and factor copula models.
This section directly follows Joe (2011, Chapter 8.3), which contains additional properties
and discussions.
Deﬁnition 2.47 (Multivariate tail dependence). Let C be a d-dimensional copula. Then,
if these limits exist, deﬁne
b(w) = lim
u↓0
C(uw)
u
,
b∗(w) = lim
u↓0
bC(uw)
u
,
for all w ∈Rd
+. b (b∗) is called the lower (upper) tail dependence function and C is said to
have multivariate lower (upper) tail dependence if b (b∗) is non-zero, and no multivariate
lower (upper) tail dependence if b(w) ≡0 (b∗(w) ≡0).
Remark 2.48. Under some regularity conditions on the tails of copula C, assuming that
taking the limit and diﬀerentiation are commutative, and that C is d-times diﬀerentiable,
the following relation holds for u →0 with ui = uwi, w1, . . . , wd ∈R+, i = 1, . . . , d,
∂dC(u1, . . . , ud)
∂u1 · · · ∂ud
∼u−(d−1)∂db(u1, . . . , ud)
∂w1 · · · ∂wd
.
(2.4)
A similar expression can be derived for b∗.
22

2.4
Tail dependence
Some of the most important properties of the multivariate tail dependence function b are
summarized below. Note, that they also hold for b∗, since b∗can be viewed as the lower
tail dependence of the copula if all variables are reﬂected, i.e., the lower tail dependence
of copula C′ if C′ is the copula of U ′
j ··= 1 −Uj, j = 1, . . . , d and (U1, . . . , Ud) ∼C.
Lemma 2.49 (Properties of multivariate tail dependence functions). Let b be deﬁned as
in Deﬁnition 2.47. Then,
1. b is increasing,
2. b(w) is homogeneous of order 1, i.e., b(tw) = tb(w) for all t ≥0, w ∈Rd
+,
3. if b(1, . . . , 1) > 0, b(w) > 0 for all w ∈Rd
+.
Proof. See Joe (2011, Proposition 2.2).
2.4.3
Conditional tail dependence
For some results for the tail dependence of vine copulas, the notion of conditional tail
dependence is required.
Deﬁnition 2.50 (Conditional tail dependence). Let I = {i1, . . . , im} ⊂{1, . . . , d} with
m ≥2 and CI be the corresponding margin of the d-dimensional copula C.
Deﬁne
i−j ··= (ij−1, ij+1, . . . , im) for j = 1, . . . , d. Then, deﬁne
tij| i−j(wij | wi−j) = lim
u↓0 Cij| i−j(wij | uwi−j),
t∗
ij| i−j(wij | wi−j) = lim
u↓0 Cij| i−j(1 −uwij | 1 −uwi−i),
for all (wi1, . . . , wim) ∈Rd
+. t (t∗) is called the lower (upper) conditional tail dependence
function and interpreted the same way as the multivariate tail dependence in Deﬁni-
tion 2.47.
The conditional tail dependence functions t and t∗can be derived from the derivatives of
the margins of the multivariate dependence functions b and b∗as shown below.
Lemma 2.51. Let I = {i1, . . . , im} ⊂{1, . . . , d}, m ≥2, d ∈N, and b{i1,...,im} and b{−ij}
be lower tail dependence functions of CI and CI\{ij} for j = 1, . . . , d. Then,
tij| i−j(wij | wi−j) =
∂m−1b{i1,...,im}
∂wi1 · · · ∂wij−1∂wij+1 · · · ∂wim
,
∂m−1b{i−j}
∂wi1 · · · ∂wij−1∂wij+1 · · · ∂wim
Proof. See Joe (2011, Proposition 2.3).
23

2
Probability theory and copulas
2.5
Vines and vine copulas
One major drawback of copulas is that while most parts of copula theory can be exten-
ded to an arbitrary dimension d in a straightforward way, calculations quickly become
more and more complex as d increases. Further, most of the commonly used measures
of association, e.g., Pearson’s or Spearman’s rho and Kendall’s tau, or tail dependence
coeﬃcients are only well understood for bivariate distributions.
Vine copulas use these thoroughly studied bivariate building blocks to sequentially as-
semble distributions in higher dimensions, oﬀering advantages in both visualization and
computation. Originally introduced in Cooke (1997), and further studied in Bedford and
Cooke (2001) and Bedford and Cooke (2002), vines are graphic in nature, generalizing
Markov trees and, therefore, oﬀering more freedom in the underlying structure. Similar
reasoning is employed in hierarchical Archimedean copulas, which allow building blocks
of dimensions higher than two, but are restricted to Archimedean linking copulas. The
deﬁnition and properties of hierarchical Archimedean copulas are given, e.g., in Mai and
Scherer (2012, Section 2.4) and applications can be found in Hering et al. (2010).
In the following, some of the most important deﬁnitions, results and an example of vine
copulas are given. A thorough introduction is given by Kurowicka and Cooke (2006), Aas
et al. (2009) and Czado (2010).
2.5.1
Graph theory notation
Some graph theory and accompanying notation is necessary to be able to state properties
of vine copulas. A full compact course of discrete mathematics and graph theory can be
found in, e.g., Taraz (2012).
Deﬁnition 2.52 (Graph, subgraph, node, edge, neighbor, degree).
1. A (undirected) graph G = (N, E) consists of two non-empty sets N and E with
E ⊆

{x, y} : x, y ∈N
	
.
2. A graph G′ = (N ′, E′) with graph G = (N, E), and E′ ⊆E, N ′ ⊆N is called a
subgraph of G.
3. e ∈E is called an edge of graph G = (N, E) and v ∈N a node.
4. u, v ∈N are called neighbors in graph G = (N, E), if {u, v} ∈E.
5. d(v) is called the degree of v ∈N in graph G = (E, N) and given by the number of
neighbors of v in G.
To deﬁne vine tree sequences, the deﬁnition of trees within the context of graph theory
has to be given. First, some important subgraphs are deﬁned.
Deﬁnition 2.53 (Path, circle, connected, tree).
1. A graph P = (N, E) with N = {v0, v1, . . . , vk}, k ∈N, and E = {{v0, v1}, {v1, v2},
. . ., {vk−1, vk}} is called a path.
2. A path P = (N, E) with v0 = vk is called a cycle.
24

2.5
Vines and vine copulas
3. A graph G = (N, E) where for all u, v ∈N there exists a path P = (N ′, E′) with
v′
0 = u and v′
k = v is called connected.
4. An acyclic, connected graph T = (N, E) is called a tree.
This allows the deﬁnition of the following characterization of trees. Note that for nota-
tional convenience, G + e (G −e) for graph G = (N, E) will denote G′ = (N, E′) with
E′ = E ∪e (E′ = E \ e).
Theorem 2.54 (Characterization of trees). For a graph T = (N, E) the following is
equivalent
1. T is a tree.
2. For all u, v ∈N , u ̸= v, there exists a unique path P = (N ′, E′), N ′ ⊆N, E′ ⊆E,
with v′
0 = u and v′
k = v in T.
3. T is minimally connected, i.e., for every e ∈E, T is connected but T −e is not.
4. T is maximally acyclic, i.e., for every u, v ∈N, u ̸= v and {u, v} ̸∈E, T contains
no cycle but T + {u, v} does.
Proof. See Taraz (2012, Theorem 9.3).
Finally, some important tree structures, commonly used in conjunction with vine copulas,
are given in the next deﬁnition.
Deﬁnition 2.55 (Spanning tree, star).
1. A subgraph T = (N, E′) of G = (N, E) where T is a tree, is called a spanning tree.
2. A graph G = (N, E) where there exists a v ∈N such that d(v) = |N| −1 (|N|
denotes the number of nodes in N) is called a star and v the root node.
2.5.2
Regular vines
Using the graph theory notation of Section 2.5.1, regular vine tree sequences can be
deﬁned. The starting point are vines, sequences of nested trees where the edges of tree
Tj are the nodes of tree Tj+1, j ∈N, and each tree has the maximum number of edges.
Formal deﬁnitions are given in the following.
Deﬁnition 2.56 (Vine, regular vine tree sequence). V = (T1, . . . , Td−1) is a vine (tree
sequence) on d ≥2, elements if
1. Tj, j = 1, . . . , d −1 is connected,
2. T1 = (N1, E1) is a tree with nodes N1 = {1, . . . , d} and edges E1,
3. for j = 2, . . . , d −1, Tj is a tree with nodes Nj = Ej−1 and edges Ej.
V is called a regular vine (or R-vine) (tree sequence) if it is a vine and, additionally, the
following proximity condition holds:
25

2
Probability theory and copulas
4. (Proximity) For j = 2, . . . , d −1 and every {u, v} ∈Ej it holds that |u ∩v| = 1,
i.e., the edges {u1, u2}, {v1, v2} in tree Tj−1 corresponding to nodes u and v in tree
Tj share exactly one node.
In general, there is a large amount of possible regular vines trees, when only the ﬁrst tree
is given. Morales-N´apoles (2011) show that there are d!/2 · 2cd with cd =
 d−2
2

regular
vines on d elements. Special cases for which the representation is unique, given the ﬁrst
tree, are D(rawable)-vines and C(anonical)-vines. D- and C-vines are ﬁxed by the ﬁrst
element of the sequence and can be characterized by the degrees of nodes in the trees.
Deﬁnition 2.57 (D-vine, C-vine). A regular vine V = (T1, . . . , Td−1) on d elements is
called a
1. D-vine if the maximal degree of each node in T1 is 2,
2. C-vine if for each tree Tj, j = 1, . . . , d −1, there is a unique node with degree d −j,
i.e., each tree is a star. The root node of tree T1 is called root.
One way to characterize vine structures is by constraint, conditioned and conditioning
sets. The nodes that can be reached by a given edge are called the constraint set of
that edge. If two edges are joined by an edge in the next tree, the intersection of their
respective constraint sets is called the conditioning set, and the symmetric diﬀerence of
those constraint sets the conditioned set. This deﬁnition is formalized below
Deﬁnition 2.58 (Complete union, constraint, conditioned and conditioning set).
1. For e ∈Ej, j = 1, . . . , d, Ae with
Ae ··=
n
k ∈N1 | ∃e1 ∈E1, . . . , ej−1 ∈Ej−1 : k ∈e1 ∈. . . ∈ej−1 ∈e
o
is called the complete union of e.
2. Ae with e ∈Ej, j = 1, . . . , d, is called the constraint set associated with e.
3. For edge e = {u, v} and j = 1, . . . , d
De = Au ∩Av
is called the conditioning set associated with e and
{Ce,u, Ce,v} = {Au \ De, Av \ De}
the conditioned set associated with e.
2.5.3
Regular vine distributions and copulas
So far, vine tree structures are only of interest in graph theory. In order to construct
multivariate distribution, vine trees can be used in conjunction with random variables.
Since the ultimate goal is to use well understood bivariate copulas for joint and conditional
distributions, only regular vines are considered in the remainder of this thesis.
More
general dependence structures are detailed in Bedford and Cooke (2002).
To be able to incorporate random variables into graph theory, some deﬁnitions are re-
quired.
26

2.5
Vines and vine copulas
Deﬁnition 2.59 (Bivariate associated and conditional copulas). Let (X1, . . . , Xd) and
(U1, . . . , Ud), d ∈N, be vectors of random variables. Let D = {1, . . . , d} \ {i, j}, i ̸= j,
i, j ≤d, be a set of indices.
1. The bivariate copula associated with (Xi, Xj), given XD = xD, is denoted by
Ci,j;D(·, ·; xD) with density ci,j;D(·, ·; xD).
2. The conditional distribution of (Ui, Uj), given UD = uD, is denoted by Ci,j| D(·, ·; uD)
with density ci,j| D(·, ·; uD).
3. For distinct indices i, j, i1, . . . , ik, k < d −2, with i < j and i1 < . . . < id, the
following notation is used:
ci,j;i1,...,ik ··= ci,j;i1,...,ik(F(xi | xi1, . . . , xik), F(xj | xi1,...,ik); xi1, . . . , xik).
The notation of Deﬁnition 2.59 greatly simpliﬁes writing down densities of regular vine
distributions given below.
Deﬁnition 2.60 (Regular vine distribution). Let F be a d-dimensional distribution func-
tion and (X1, . . . , Xd) ∼F. F is called a regular vine distribution if there exists a tuple
(F, V, B) such that
1. F = (F1, . . . , Fd) is a vector of continuous invertible marginal distribution functions
Fj of Xj, j = 1, . . . , d.
2. V is a regular vine on d elements.
3. B = {Be | j = 1, . . . , d −1, e ∈Ej} where for all e Be is a symmetric bivariate
copula that admits a density, and Ej is the set of edges of Tj in the regular vine.
While regular vine distributions are deﬁned for symmetric copulas in Deﬁnition 2.60,
this is only a technical assumptions. It is required since regular vines were deﬁned on
undirected graphs. In practice, also non-symmetric bivariate copulas may be used.
Since regular vine distributions consist solely of bivariate copulas, it is convenient to
specify association with Spearman’s rho or Kendall’s tau. In many cases, these values can
be used to calculate the corresponding copula parameters.
As another notational aid, the copulas Be can be deﬁned in the following way:
Deﬁnition 2.61. Let Be be the copula corresponding to edge e = {u, v}. Then Be can
be denoted by CCe,u,Ce,v;De and the respective density by cCe,u,Ce,v;De.
Using all of the previous notation, the density of a regular copula vine can now be stated.
Theorem 2.62. Let (F, V, B) be as in Deﬁnition 2.60. Then there exists a unique dis-
tribution with density (arguments are omitted for notational simplicity)
f1...d = f1 · · · fd
d−1
Y
j=1
Y
e∈Ej
cCe,uCe,v;De(FCe,u| De, FCe,v| De),
27

2
Probability theory and copulas
where for all e ∈Ej, j = 1, . . . , d −1, with e = {u, v} the distribution function of XCe,u
and XCe,v, given XDe, can be written as
F(xCe,u, xCe,v | xDe) = Be
 F(xCe,u | xDe), F(xCe,v | xDe)

,
with margins F(xj) = Fj(xj), j = 1, . . . , d.
Proof. See Kurowicka and Cooke (2006, existence: Theorem 4.3, density decomposition:
Theorem 4.2).
Example 2.63 (Density decomposition). An example of Theorem 2.62 in dimension
d = 5, for density f1,...,5, is given below. Decomposition is done in accordance to the regular
vine distribution visualized in Figure 2.2. Random variables X1, . . . , X5 are denoted by
the numbers 1, . . . , 5, respectively. Previously introduced simpliﬁed notation is used, and
arguments of densities are omitted.
f1...5 =f1 f2 f3 f4 f5
· c1,2 c2,3 c3,4 c3,5
· c1,3| 2 c2,4| 3 c4,5| 3
· c1,4| 2,3 c2,5| 3,4
· c1,5| 2,3,4.
In the context of comparing d-dimensional copulas to corresponding regular vine distri-
butions, it will be helpful to look at random vectors with uniform margins.
Deﬁnition 2.64 (Regular vine copula). A regular vine distribution F on d elements with
tuple (F, V, B), where F = (F1, . . . , Fd) and Fj ∼U(0, 1), j = 1, . . . , d, is called a regular
vine copula.
In the context of the factor copula models in Section 6, the special case of the canonical
vine copula will be particularly interesting. Therefore, the decomposition of Theorem 2.62
will be restated for C-vine copulas.
Lemma 2.65 (Density of C-vine copulas). Let C be a C-vine copula on d elements with
density c. Then,
c(u1, . . . , ud) =
d−1
Y
j=1
d−j
Y
i=1
cj,j+1;1,...,j−1,
u1, . . . , ud ∈(0, 1).
(2.5)
This decomposition is also called pair copula decomposition (PCC).
Proof. Application of Theorem 2.62 to a C-vine copula.
In order to specify a full regular vine copula, a bivariate copula is required for each edge.
Popular copula classes such as elliptical, Archimedean or extreme value copulas are often
chosen in this context. Regular vine matrices oﬀer a convenient way to encode the vine
structure and the choice of bivariate linking copulas.
Deﬁnition 2.66 (Regular vine matrix). A matrix M = (mi,j)1≤i,j≤d, d ≥3, with
28

2.5
Vines and vine copulas
1. {m1,i, . . . , mi,i} ⊂{m1,j, . . . , mj,j} for all 1 ≤i < j ≤d, i.e., all entries of a chosen
column are contained in all columns to the right of that column,
2. mi,i /∈{m1,i−1, . . . , mi−1,i−1}, for all 1 ≤i ≤d, i.e., the diagonal entry of a chosen
column is not contained in any column left of that column,
3. for all i = 3, . . . , d, k = 1, . . . , i −1 there exists a j and ℓwith j < i and ℓ< j such
that

mk,i, {m1,i, . . . , mk−1,i}
	
=

mj,j, {mi,j, . . . , mℓ,j}
	
or

mk,i, {m1,i, . . . , mk−1,i}
	
=

mℓ,j, {m1,j, . . . , mℓ−1,j, mj,j}
	
,
is called a regular vine matrix (RVM).
In some applications, e.g., the R (see R Core Team, 2015) package VineCopula (see
Schepsmeier et al., 2014), the transpose M ⊤of a regular vine matrix M oﬀers useful
properties. Therefore, the transposed version will be used in the remainder of this thesis.
Furthermore, instead of containing variables to indicate bivariate connections in the vine
copula, entries of the RVM may also contain copula families or parameters of the corres-
ponding pair-copula to provide details of the vine copula.
Remark 2.67. Property 3 of Deﬁnition 2.66 is the counterpart of the proximity condition
of Deﬁnition 2.56.
It can be proven that there exists a one-to-one relationship between regular vine matrices
and regular vine trees. An algorithm to construct R-vine matrices is given by Mai and
Scherer (2012, Algorithm 5.1).
An example for a regular vine matrix can be seen in Figure 2.2.
Sometimes only the dependence in the lower levels (trees Tj up to some j < d) of the
vine structure is of interest, while the higher levels might be neglected. In these cases
it is possible to reduce the complexity of the resulting regular vine copula by choosing
the independence copula for all bivariate copulas after level j. These vine copulas will be
called truncated. They are more formally deﬁned in Brechmann et al. (2012).
Deﬁnition 2.68 (Truncated regular vine copula). A regular vine copula on d elements,
where all bivariate copulas conditioning on K ≤d or more variables are independence
copulas, is called truncated at level K.
Due to the relation of C-vine copulas and factor copula models, the density of Lemma 2.65
will also be stated for a C-vine copula truncated at level K.
Lemma 2.69 (Density of a C-vine copula truncated at level K). Let C be a C-vine copula
on d elements truncated at level K ≤d with density c. Then
c(u1, . . . , ud) =
K
Y
j=1
d−j
Y
i=1
cj,j+i;1,...,j−1,
u1, . . . , ud ∈(0, 1).
(2.6)
Proof. See Brechmann et al. (2012, Section 4).
29

2
Probability theory and copulas
T1 :
1
2
3
4
5
3,5
1,2
2,3
3,4






1
5
2
4
5
4
3
4
5
3
2
3
3
5
5






T2 :
1,2
2,3
3,4
3,5
1,3|2
2,4|3
4,5|3






1
5
2
4
5
4
3
4
5
3
2
3
3
5
5






T3 :
1,3|2
2,4|3
4,5|3
1,4|2,3
2,5|3,4






1
5
2
4
5
4
3
4
5
3
2
3
3
5
5






T4 :
1,4|2,3
2,5|3,4
1,5|2,3,4






1
5
2
4
5
4
3
4
5
3
2
3
3
5
5






Figure 2.2: Example of an R-Vine on 5 elements with the corresponding RVM. Variables
in the conditioned set are written in blue, variables in the conditioning set in green.
30

3
Statistical methods and backtesting
In this thesis, ﬁts and forecasts of several statistical models are compared. This section
introduces the methods used to assess the ﬁt of models and to backtest their forecasting
capabilities.
Section 3.1 provides deﬁnitions for some of the most important statistics and tests for
model ﬁts. The risk measures used to evaluate the models’ backtesting performance are
given in Section 3.2.
3.1
Assessment of model ﬁt
The tools necessary to assess model ﬁt in the later parts of this thesis are given in this
section. A thorough introduction to mathematical statistics and modeling can be found
in, e.g., Czado and Schmidt (2011, Chapters 3–5).
For this section it will be assumed that Θ ⊂Rd, and that there is a statistical model with
corresponding densities {f(·, θ) : θ ∈Θ}.
Deﬁnition 3.1 (Likelihood function). The function L given by
L(θ, x) ··= f(x, θ),
θ ∈Θ,
x ∈Rn
is called likelihood function of parameter θ for observations x.
The likelihood function can be interpreted as the likelihood of parameter θ under obser-
vations {X = x} in the discrete case, or {X is in a ε neighborhood of x} for ε →0 in
the case of a continuous density. Therefore, the estimator bθ = bθ(x), which maximizes the
likelihood for given observations x, is a clear choice.
Deﬁnition 3.2 (Maximum likelihood estimator). If there exists a measurable function
bθ, such that
L
 bθ(x), x

= max
θ∈Θ L(θ, x),
for all x ∈Rn,
θ ∈Θ,
then bθ is called the maximum likelihood estimator (MLE) of θ.
If bθ exists, then q(θ), for function q with support Θ, can be estimated by q(bθ(x)). In this
case q(bθ(x)) is called the maximum likelihood estimate of q(θ).
Remark 3.3. Under some technical assumptions, the MLE bθ is both consistent and
asymptotically normally distributed (see, e.g., Czado and Schmidt, 2011, Theorems 4.20
and 4.26).
In many cases, likelihood function L is a product of densities of independent random
variables. Since maximization of a product is generally more complex than maximization
of a sum, the logarithm can be used to transform L into a sum. The resulting maximum
likelihood estimator remains unchanged, but is often easier to derive.
Deﬁnition 3.4 (Log-likelihood function). The function ℓgiven by
ℓ(θ, x) ··= ln
 L(θ, x)

,
θ ∈Θ,
x ∈Rn
where L is the likelihood-function, is called the log-likelihood function. For an estimator
bθ of θ, ℓ(bθ, x) is called the log-likelihood.
31

3
Statistical methods and backtesting
Introduced by Wald (1943), the Wald test utilizes the MLE in conjunction with the
log-likelihood to investigate whether the parameters θ of a model take a speciﬁc value
θ0 ∈Rd. Often the value of interest θ0 is zero, in order to verify if certain parameters
have an inﬂuence on the model. In this thesis, the Wald test will be stated only for
one-dimensional parameters θ ∈R and values θ0 ∈R.
As an auxiliary result, the observed information is stated. The observed information is an
approximation of the Fisher information that is used to assess the amount of information a
random variable contains about an unknown parameter (see Lehmann and Casella, 1998,
Chapter 5).
Deﬁnition 3.5 (Observed information). Let x = (x1, . . . , xn) ∈Rn be observations and
θ0 ∈R. The value In given by
In(θ0) ··= −
n
X
i=1
∂2 ln f(xi, θ)
∂θ2

θ=θ0
is called observed (Fisher) information.
Using the above deﬁnition of observed information, the Wald test can be deﬁned.
Deﬁnition 3.6 (Wald test). Let θ0 ∈R. The test
H0 : θ = θ0
vs.
H1 : θ ̸= θ0
is called Wald test with test statistic
Wn ··= (bθ −θ0)2 In(bθ),
where bθ is the MLE of θ, and In the observed information. Wn is asymptotically χ2
1
distributed, therefore H0 is rejected at level α if Wn > χ2
1−α,1.
When comparing two parametrizations of a statistical model with a varying number (di-
mension) of parameters and the same log-likelihood function ℓ, the parametrization with
more parameters usually nets the higher log-likelihood. Since sparser models are com-
monly easier to interpret and require fewer observations to estimate adequately, they are
often preferable in practice. The Akaike information criterion (AIC), proposed by Akaike
(1974) as a new way to measure model ﬁt, promotes sparser models by penalizing the
number of parameters in the model.
Deﬁnition 3.7 (Akaike information criterion). The function
AIC(θ) ··= −2ℓ(θ, x) + 2k,
θ ∈Θ,
x ∈Rn,
where ℓis the log-likelihood function, θ a parameter vector of dimension k, and x a vector
of observations, is called Akaike information criterion (AIC).
In many applications, the model that minimizes the AIC is chosen as the most appropriate
model. Those models achieve a balance between a high log-likelihood and a relatively low
number of parameters k.
As an alternative to asses the appropriateness of a model, the Bayesian, or Schwarz,
information criterion (BIC) was introduced by Schwarz et al. (1978). It was formulated
as a Bayesian view of model ﬁt for data from exponential families. BIC is closely related
to the AIC, though it penalizes the number k of parameters even more heavily.
32

3.1
Assessment of model ﬁt
Deﬁnition 3.8 (Bayesian information criterion). The function
BIC(θ) ··= −2ℓ(θ, x) + k ln(n),
θ ∈Θ, x ∈Rn,
where ℓis the log-likelihood function, θ a parameter vector of dimension k, and x a vector
of observations, is called Bayesian information criterion (BIC).
One downside of log-likelihood, AIC and BIC is the reliance on the same likelihood func-
tion to contrast diﬀerent choices of parameters for a given set of observations. When
examining two or more models that yield diverse likelihood functions, this approach is
often not feasible.
In order to compare models with diﬀering likelihood functions, the Vuong test can be
employed. Vuong’s (closeness) test, ﬁrst introduced in Vuong (1989), is a likelihood ratio
test based on the Kullback-Leibler divergence of Kullback and Leibler (1951). It tests
two non-nested and possibly misspeciﬁed models for whether either is closer to the true
model than the other, or whether both models are equally close.
As auxiliary results, the likelihood ratio test statistic and Kullback-Leibler divergence are
deﬁned.
Deﬁnition 3.9 (Likelihood ratio test statistic). Let x = (x1, . . . , xn) ∈Rn be a observa-
tions. Let Fθ and Gγ be statistical models with densities f and g, and parameter vectors
θ ∈Θ and γ ∈Γ, respectively. The function
LRn
 bθn, bγn
 ··= Lf
n
 bθn, x

Lg
n
 bγn, x
 =
n
Y
i=1
f
 xi, bθn

g
 xi, bγn) ,
where bθn, bγn are MLEs of θ and γ, respectively, is called likelihood ratio (LR) test statistic
for model Fθ versus Gγ.
Note that similarly to AIC and BIC, a correction term for the number of parameters may
be added.
Deﬁnition 3.10 (Kullback-Leibler divergence). Let the conditions of Deﬁnition 3.9 hold
and let X be a random variable. The function
KL(f, q) ··= Ef

ln
f(X, θ)
g(X, θ)

=
Z ∞
−∞
f(x) ln
f(x, θ)
g(x, θ)

dx
is called Kullback-Leibler divergence.
Note that the Kullback-Leibler divergence is in general not symmetric. It can also be
shown that is non-negative and zero if f = g (see Izenman, 2008, Chapter 15.3.10).
Using the likelihood ratio test statistic, the Vuong test can now be deﬁned.
Deﬁnition 3.11 (Vuong test). Let the conditions of Deﬁnition 3.9 hold. The test
H0 : Fθ and Gγ are equally close to the true model vs.
HF : F is closer to the true model than G or HG : G is closer to the true model than F,
33

3
Statistical methods and backtesting
is called Vuong test with test statistic
V (F, G, X) ··= ln
 LRn
 bθn, bγn

√n bωn
,
where
bω2
n = 1
n
n
X
i=1

ln
f
 Xi, bθn

g
 Xi, bγn

2
−
1
n
n
X
i=1
ln
f
 Xi, bθn

g
 Xi, bγn

2
.
V is asymptotically standard normally distributed, therefore, Fθ (Gγ) is preferred if V is
greater (smaller) than the (negative) 1 −α quantile of the standard normal distribution
Φ−1(1 −α).
Since time series data often exhibits autocorrelation, although independence is desirable
for the analysis of copula data, the Ljung-Box test can be used to test for independence
in a data set. It is commonly used after ﬁtting an ARMA model to test whether the
residuals are independent of each other.
Deﬁnition 3.12 (Ljung-Box test). Let x ∈Rn be observations. The test
H0 : x is independently distributed up to lag h
vs.
H1 : x is not independently distributed up to lag h
is called Ljung-Box test with test statistic
Q = n(n + 2)
h
X
k=1
bρk
n −k,
where bρk is the sample autocorrelation of x at lag k and h ∈N the maximum number of
lags being tested.
Q is asymptotically χ2
h distributed, therefore H0 is rejected at level α if Q > χ2
1−α,h.
Remark 3.13. When testing the residuals of an ARMA(q, p) process (see Section 5.1),
Davidson and MacKinnon (2004) suggests setting the degrees of freedom of the χ2 distri-
bution to h + q + p to achieve more precise results.
If a time series exhibits autocorrelation, it is possible to equate the number of autocorrel-
ated samples to a corresponding number of independent samples. Using the relationship
between the variance of an independent sample and the variance of a sample from a weakly
stationary time series to calculate the autocorrelation time factor, eﬀective sample size
(ESS) indicates the number of independent samples that would show the same amount of
variance as a given sample from a time series (see Thompson, 2010). ESS is often utilized
in the context of generating random variates to check for desired independence between
variates in a sample.
Deﬁnition 3.14 (Eﬀective sample size). Let xi, i = 1, . . . , n, be n ∈N realizations of a
random sample from (Xt)t∈N0. Then, the eﬀective sample size (ESS) is given by
ESS = n
bτac
,
34

3.2
Model backtests
where bτac is the empirical autocorrelation time given by
bτac = 1 + 2
n
X
k=1
bρk,
where bρk is the empirical autocorrelation at lag k.
Note that bτac may also be deﬁned with the absolute values of empirical autocorrelations,
leading to ESS≤n.
3.2
Model backtests
After the successful speciﬁcation of a model, it is possible to use historical data sets
to forecast values of interest over time to measure a model’s performance compared to
observed realizations. This can serve both as a validity check of a proposed model, as
well as a criterion to choose the best of several competing models. In a ﬁnancial context,
the values of interest of this backtesting procedure are often risk measures to assess the
risk associated with a ﬁnancial position.
In the following, an overview over the backtesting methodology used in the later parts of
this thesis is given. A thorough discussion of risk measures in the context of quantitative
risk management is given by (McNeil et al., 2005, Chapter 2), with additional backtests
discussed in (Christoﬀersen, 2011, Chapter 13).
3.2.1
Value-at-Risk
The most commonly used risk measure in practice is the Value-at-Risk (VaR). Replacing
variance as a tool assess the riskiness of a ﬁnancial position, VaR has become part of
ﬁnancial regulations such as Basel II/III (see Basel Committee on Banking Supervision,
2006, 2011) and Solvency II (see European Parliament and Council, 2009) to regulate the
exposure of banks and insurance companies to sources of losses, respectively. Berkowitz
and O’Brien (2002) showed that many commercial banks used to, and possibly still do,
employ inadequate methods to calculate their exposure to negative market events. There-
fore, as a ﬁgure necessary for daily operations of many institutions, VaR is an important
tool for backtesting.
One of the major ﬂaws of VaR is that it is not subadditive as shown by, e.g., McNeil et al.
(2005, Example 6.7). Diversiﬁcation in a a portfolio may not be correctly rewarded, since
a low chance of high losses is favored over a higher chance of low losses.
Furthermore, with respect to copula theory, it is often assumed that the comonotonicity
copula represents the worst-case VaR of a portfolio. In reality, the highest VaR might be
much larger as shown by Embrechts et al. (2013).
Deﬁnition 3.15 (Value-at-Risk). Let α ∈(0, 1), and X ∼F continuous with quantile
function F −1. Then,
VaRα(X) = sup

x ∈R : P(X < x) ≤α
	
= −F −1(α)
is called Value-at-Risk (VaR) at conﬁdence level 1 −α.
35

3
Statistical methods and backtesting
For n ∈N realizations xi, i = 1, . . . , n, of a random sample from (X)t∈N0 with order
statistic x[1] < . . . < x[n], the empirical VaR at level α is given by
d
VaRα,n = −x⌊αn⌋.
The empirical VaR may also calculated by, possibly interpolated, empirical quantile func-
tions.
One important part of VaR backtests is the number of times that a loss is above a given
VaR value. These events are called VaR violations and give rise to the hit sequence.
Deﬁnition 3.16 (Hit sequence). Let (Xt)t=1,...,T, T ∈N, be a sequence of random vari-
ables and γ ∈(0, 1). The sequence (It)t=1,...,T given by
It = 1{Xt<−VaRγ(Xt)},
t = 1, . . . , T,
is called hit sequence.
If the model is speciﬁed correctly, VaRγ violations would happen randomly over time and
with probability α for each forecasting interval. Therefore, the hit sequence should be
independent Bernoulli random variables with probability of success γ. This motivates the
following test.
Deﬁnition 3.17 (Unconditional coverage test). Let (It)t=1,...,T be as in Deﬁnition 3.16.
Deﬁne
ni =
T
X
t=1
1{It=i},
i = 0, 1.
Assuming It+1 ∼Ber(p), t = 1, . . . , T, the likelihood ratio test
H0 : p = γ
vs.
H1 : p ̸= α
is called unconditional coverage test with test statistic
LRuc = −2 ln
 
(1 −p)n0pn1
 1 −n1
T
n0  n1
T
n1
!
.
LRuc is asymptotically χ2
1 distributed, therefore H0 is rejected at level α if LRuc > χ2
1−α,1.
In practice, several subsequent severe losses might cause a bank to go bankrupt, while
longer periods between each loss provide a chance for recovery. Therefore, even if VaRα
violations happen with probability α, it is still important to check whether those violations
are clustered in certain intervals or appear independently over time.
This leads to an independence test based on ﬁrst-order Markov chain properties given
below. Additional details provided by Christoﬀersen (2011, Chapter 13.2.3).
Deﬁnition 3.18 (Independence test). Let (It)t=1,...,T be as in Deﬁnition 3.16. Deﬁne
nij =
T−1
X
t=1
1{It=i,It+1=j},
i, j = 0, 1,
36

3.2
Model backtests
i.e., the number of observations of i that are followed by an observation of j. The likelihood
ratio test
H0 : (It)t=1,...,T are independent
vs.
H1 : (It)t=1,...,T are not independent
is called independence test. Let
bπ01 ··=
n01
n00 + n01
,
bπ11 =
n11
n10 + n11
,
bπ = n01 + n11
T
then, the test statistic is given by
LRind = −2 ln

(1 −bπ)n00+n10bπn01+n11
(1 −bπ01)n00bπn01
01 (1 −bπ11)n10bπn11
11

.
LRind is asymptotically χ2
1 distributed, therefore H0 is rejected at level α if LRuc > χ2
1−α,1.
As a ﬁnal step, the joint hypothesis that VaR violations happen both with the expected
frequency and independently can be tested.
Deﬁnition 3.19 (Conditional coverage test). Let (It+1)t=1,...,T be as in Deﬁnition 3.16.
Assuming It+1 ∼Ber(p), t = 1, . . . , T, the test
H1 : (It)t=1,...,T ∼Ber(γ) i.i.d.
vs. H1 : not H0,
is called conditional coverage test with test statistic
LRcc = LRuc + LRind .
LRcc is asymptotically χ2
2 distributed, therefore H0 is rejected at level α if LRuc > χ2
1−α,2.
3.2.2
Expected shortfall
The expected shortfall (ES) is a risk measure closely tied to VaR and set to replace it
as the risk measure of choice in some regulatory frameworks (see Basel Committee on
Banking Supervision, 2013). In contrast to VaR, ES is subadditive and its worst-case
value can be derived with the comonotonicity copula (see McNeil et al., 2005, Remark
6.17.). However, ES is only deﬁned for random variables with ﬁnite mean, which might
be problematic when modeling operational losses or natural catastrophes. Furthermore,
backtesting methodologies for the ES are not well-established yet and still topic of aca-
demic and regulatory discussion (see Embrechts et al., 2014). Part of the problem is that
ES is not elicitable, whereas VaR is (see Gneiting, 2011), i.e., there is no easy way to
compare several competing ES forecasts. Therefore, in this thesis, ES will not be formally
backtested, and instead ES forecasts compared to their empirical values. Note, that more
recently, Acerbi and Szekely (2014) proposed bootstrap based backtesting procedures,
which might prove fruitful in the future.
Deﬁnition 3.20 (Expected shortfall). Let α ∈(0, 1), and X ∼F continuous with
E[|X|] < ∞and quantile function F −1. Then,
ESα(X) ··= −1
α
Z α
0
F −1(u) du = 1
α
Z a
0
VaRα(u) du
37

3
Statistical methods and backtesting
is called expected shortfall (ES) at conﬁdence level α.
For n ∈N realizations xi, i = 1, . . . , n, of a random sample from (X)t∈N0, with order
statistic x[1] < . . . < x[n], the empirical ES at level α is given by
c
ESα,n = −
1
⌊αn⌋
⌊αn⌋
X
i=1
x[i],
i.e., the negative of the mean of all observations below −d
VaRα,n.
38

4
Sampling Methods
The main part of this thesis consists of the development of a Markov chain Monte Carlo
(MCMC) scheme to sample from posterior distributions in factor copula models (see
Sections 6 and 7). This section provides details of the Markov chain sampling techniques
that are employed.
Section 4.1 brieﬂy introduces the Gibbs and Metropolis-Hastings (MH) samplers. In Sec-
tion 4.2 the Metropolis-Hastings within Gibbs sampling schemes that are used in the
simulation study (see Section 8) are detailed with their corresponding R implementations.
Finally, Section 4.3.3 provides the theoretical foundation for Adaptive Rejection Metro-
polis Sampling (ARMS), which is used as an alternative to pure MH techniques.
4.1
Markov chain Monte Carlo methods
MCMC sampling oﬀers a ﬂexible way to sample from arbitrary target densities, where it
is suﬃcient to know the density up to a constant. The procedure uses specially designed
Markov chains with stationary distributions that corresponds to the desired target density.
This sections provides the deﬁnitions of common MCMC sampling techniques. Details
about conditions for the existence of and convergence to stationary distributions are
omitted, these can be found in, e.g., Gilks et al. (1995a, Chapter 4). A general introduction
to MCMC, is given by Gamerman and Lopes (2006, Chapter 4).
4.1.1
Gibbs sampler
Gibbs sampling originated in the context of image processing. Its name is derived from the
fact that it was proposed by Geman and Geman (1984) for sampling from the Gibbs dis-
tribution. The algorithm was later generalized to allow for sampling of other multivariate
distributions.
The starting point is density f(X) that samples are to be drawn from, where X =
(X1, . . . , Xd)⊤and Xj, j = 1, . . . , d, can be a scalar, vector or matrix.
For Gibbs
sampling it is important that the full conditional densities p(Xj | X−j), where X−j =
(X1, . . . , Xj−1, Xj+1, . . . , Xd)⊤, are known and can be sampled from. Furthermore, start-
ing values X0 inside the support of f are required to start the algorithm. In subsequent
iterations t ≥2, the previous values Xt−1 are used as conditioning values.
One iteration of the Gibbs sampler is given by Algorithm 4.1.
When convergence is
reached, a sample generated by the Gibbs sampling algorithm is from target distribution
f.
The Gibbs sampler implemented for this thesis uses sequential updates of every component
in each iteration. It is possible to use diﬀering updating schemes, including randomly
choosing the components to update as discussed in Roberts and Sahu (1997).
Since
convergence to the stationary distribution might be slow, samples from MCMC schemes
are commonly discarded up to some point b ∈N, where the chain appears suﬃciently
stationary. In this context, the number b is called burn-in.
(Posterior) samples from an MCMC sampler are often heavily autocorrelated. This fact
is undesirable when the goal is to generate an i.i.d. sample from the target distribution
39

4
Sampling Methods
Algorithm 4.1: General algorithm for Gibbs sampling from target density f, known
up to a constant, given full conditional distributions p(Xj | X−j), j = 1, . . . , d and
current values Xt.
Input: Target density f
Full conditional distributions p(Xj, | X−j), j = 1, . . . , d
Current values Xt = (Xt
1, . . . , Xt
d)⊤
Output: Next value Xt+1
1 for j = 1 to d do
2
Sample Xt+1
j
from p(Xj | Xt
−j)
3 end
and might be remedied by, e.g., using only every k-th sample and discarding the others.
Then, the resulting sample is called thinned with factor k.
4.1.2
Metropolis-Hastings algorithm
Proposed by Metropolis et al. (1953) and Hastings (1970), the MH algorithm was ﬁrst
used in the context of physics, but achieved much wider adoption later. In this thesis, the
MH algorithm will only be discussed for univariate random variables, although the same
procedure is also valid for multivariate ones.
Similarly to the Gibbs sampler, the starting point of the MH algorithm is a density f that
samples should be drawn from. Generally, the density f should be complex enough such
that sampling with other available algorithms is unfeasible. For the MH algorithm it is
suﬃcient to know the target density up to a constant term and to substitute π given by
f(x) ∝π(x),
x ∈R,
for the target density, since constants are canceled out by the acceptance step of the
algorithm. π and f are used interchangeably and π will also be referred to as target
density, although it might not be a proper density.
Furthermore, for generating random variates, an auxiliary density q is used. The only
requirement for proposal density q is that it has the same support as f and that samples
can be drawn from it, possibly also by MH. In practice, choosing q is a critical part of the
success of the algorithm, since the resulting Markov chain has to both suﬃciently explore
the support of f, and to converge quickly.
Finally, a starting value X0 inside the support of f is required to start the Markov chain.
In subsequent iterations t ≥2, the previous iteration Xt−1 is used in the acceptance step
of the algorithm.
One iteration of the MH algorithm in its most general form is given in Algorithm 4.2 with
acceptance step 3. The acceptance probability of a proposal Y ∼q, given previous value
Xt is given by
α(Xt, Y ) = min

1, π(Y )q(Xt | Y )
π(Xt)q(Y | Xt)

.
The empirical acceptance probability of a sample x1, . . . , xT, T ∈N, from a Markov chain
is called acceptance rate.
40

4.2
Metropolis-Hastings within Gibbs
Algorithm 4.2: General algorithm for Metropolis-Hastings sampling of target dens-
ity f = π/K, known up to constant K ̸= 0, given proposal distribution q and current
Markov chain value Xt.
Input: Target density f = π/K
Proposal distribution q with same support as π
Current value of Markov chain Xt
Output: Next value of Markov chain Xt+1
1 Generate Y ∼q(· | Xt)
2 Generate independent U ∼U(0, 1)
3 if U ≤π(Y )q(Xt| Y )
π(Xt)q(Y | Xt) then
4
Set Xt+1 = Y
5 else
6
Set Xt+1 = Xt
7 end
Example 4.1 (Independence sampler). It is possible to choose the proposal distribution
in such a way that in each iteration of the MH algorithm, density q(Xt, Y ) = q(Y ) is
independent of previous iteration Xt. This special case of the MH algorithm is called
independence sampler. The acceptance probability reduces to
α(Xt, Y ) = min

1, π(Y )
π(Xt)

= min

1, f(Y )
f(Xt)

.
A popular choice for q is the density of the prior.
Example 4.2 (Random walk sampler). The proposals of the MH algorithm can have the
form of a random walk, i.e., for each iteration t ∈N0,
Xt+1 = Xt + Wt,
where Wt is from a distribution that is independent of the Markov chain.
Popular choices for the distribution of Wt include the normal and Student’s t distribution
centered around zero.
4.2
Metropolis-Hastings within Gibbs
In this section, the Metropolis-Hastings within Gibbs sampling schemes that are used
in the later parts of the thesis are discussed. As a combination of both MH and Gibbs
sampling, Metropolis-Hastings within Gibbs uses MH for sampling from the full con-
ditional distributions within the Gibbs sampling routine.
These methods are used to
sample from latent variable V1, bounded in (0, 1), and non-negative Fisher z-transforms
H of Kendall’s tau of the parameters of the bivariate Gumbel copulas (see Section 8)
in this thesis. Therefore, the R implementation in the context of the simulation study is
discussed in detail.
41

4
Sampling Methods
4.2.1
Mode and curvature matching
This method matches the mode and curvature at the mode of the proposal distribution
to those of the target density. In general, this requires the proposal distribution to have
two parameters.
Since it is assumed that latent variable V1 is bounded in (0, 1), a Beta(α, β) distribution
is utilized as proposal distribution for V1,k, k = 1, . . . , n, n ∈N. The mode of the Beta
distribution is given by
xm(α, β) =
α −1
α + β −2,
α, β > 1,
and curvature, for 0 < x < 1 and α, β > 0, by
c(x, α, β) =
1
B(α, β)
 xα−3(1 −x)β−3(α2 + x2(α + β −3)(α + β −2)
−2(α −1)x(α + β −3) −3α + 2)

.
For the non-negative transforms of the copula parameters Hj, j = 1, . . . , d, d ∈N,
truncated normal N[0,U](µ, σ2) proposals are used, where U ≈2.65 = z(0.99) and z is
the Fisher z-transformation (7.1). Here, the upper bound U is chosen close to maximal
association of Kendall’s tau equal to one, to avoid numerical problems when evaluating
the bivariate Gumbel density.
The mode of N[0,U](µ, σ2) is given by
xm(µ) = µ,
0 ≤µ ≤U,
and curvature by
c(x, µ, σ) = e−(x−µ)2
2σ2 (µ2 −σ2 + x2 −2µx)
√
2π σ5 Φ
  U−x
σ

−Φ
  x
σ

.
In the simulation, numerical approximations for the mode and curvature of the target full
conditional distributions (7.7) and (7.9) have to be used, due to their complexity.
At each iteration of the Gibbs sampler and for every variable V1,k, k = 1, . . . , n and Hj,
j = 1, . . . , d, in turn, the following three steps are performed for the corresponding target
density, given the simulated data and current values of all other variables:
1. Calculate the approximate mode bxm by numerical optimization with the R function
optimize.
2. Calculate the numerical second derivative at the point bxm with the hessian function
of the numDeriv package (see Gilbert and Varadhan, 2012) to arrive at approximate
curvature bc at the mode.
3. Solve the system of equations
bxm = xp
m(θ1, θ2),
|bc| = |cp(bxm, θ1, θ2)|,
where xp
m and cp are the mode and curvature of the proposal distribution, respect-
ively, numerically for parameters θ1 and θ2 of the proposal distribution with the R
function uniroot or optimize.
42

4.2
Metropolis-Hastings within Gibbs
4.2.2
Expectation and variance matching
Similarly to mode and curvature matching, a proposal distribution with two parameters
is used to match the expectation and variance of the proposal distribution to those of the
target full conditional density.
As previously, the Beta(α, β) distribution is used as proposal distribution for latent
variables V1,k, k = 1, . . . , n, n ∈N.
Expectation and variance of a random variable
X ∼Beta(α, β) are given by
E[X] =
α
α + β ,
Var(X) =
αβ
(α + β)2(α + β + 1)
,
with α, β > 0, respectively.
Given expectation 0 < µ < 1 and variance σ2 > 0, this simple functional form enables an
analytic solution for the parameters of the Beta(α, β) distribution. They are given by
α = −µ(µ2 −µ + σ2)
σ2
,
β =
α
µ −1.
For the Fisher z-transforms Hj, j = 1, . . . , d of the copula parameters, a G(s, r) dis-
tributions is utilized for the proposals. Expectation and variance of a random variable
X ∼G(s, r) are given by
E[X] = s
r,
Var(X) = s
r2,
with s, r > 0, respectively.
Similarly to the Beta distribution, given expectation 0 < µ < ∞and variance σ2 > 0,
these equations can be solved for the parameters of the G(s, r) distribution. They are
given by
s = µ2
σ2,
r = s
µ.
(4.1)
Again, in the simulation, numerical approximations for the expectation and variance of
the target full conditional distributions (7.7) and (7.9) have to be used.
At each iteration of the Gibbs sampler and for every variable V1,k, k = 1, . . . , n and Hj,
j = 1, . . . , d, in turn, the following two steps are performed for the corresponding target
density, given the simulated data and current values of all other variables:
1. Calculate numerical approximations bµ and bσ2 of the expectation and variance, re-
spectively, of the target full conditional distribution with the R function integrate.
2. Calculate the parameters of the proposal distribution with the analytical formulas
of (4.1).
4.2.3
Independence and random walk samplers
As a numerically less complex alternative to the previously introduced methods, an inde-
pendence sampler for Vk,1, k = 1, . . . , n, and a random walk sampler for Hj, j = 1, . . . , d,
are used.
43

4
Sampling Methods
Parameters for the proposal distributions are chosen in such a way, that the acceptance
rate is roughly 25%. This is close to the optimal acceptance rate of 23% for multivariate
normal random walks, derived by Roberts et al. (1997).
This leads to the following two proposals for every iteration t ∈N of the Gibbs sampler:
1. For all V t
k,1, k = 1, . . . , n, U(0, 1) proposals are used.
2. For all Ht
j, j = 1, . . . , n, a random walk sampler with truncated normal N[0,U](Ht−1
j
,
σ2) steps is used, where U ≈2.65 = z(0.99), as previously, and σ = 0.2.
4.3
Adaptive Rejection Metropolis Sampling
In this section, the ARMS algorithm, introduced by Gilks et al. (1995b), and underlying
sampling methods are presented.
A major advantage of ARMS is in its automatic nature. While it is in many cases possible
to create very eﬃcient sampling algorithms, by, e.g., tuning the proposal distribution of
a MH sampling scheme, ARMS enables the algorithmic generation of either an envelope
for rejection sampling in the case of univariate log-concave densities, or proposal distri-
butions for general univariate densities. Despite this generality, ARMS still demonstrates
good performance in most situations and is particularly appealing within the context of
Gibbs sampling, where a large number of univariate samples coming from a variety of
distributions is often required.
4.3.1
Acceptance-Rejection Sampling
Acceptance-Rejection Sampling, also called Rejection Sampling, enables drawing from
target densities f = π/K, known up to constant K ̸= 0, that are analytically intractable.
Instead of sampling from p directly, it is suﬃcient to be able to sample from a density q
that fulﬁlls π(x) ≤Aq(x) for A < ∞and all x in the support of π. As Aq has to cover π,
q is also often called a blanketing density or blanket.
Given blanket q, random variates with density p can be generated by Algorithm 4.3.
Algorithm 4.3: Algorithm for Acceptance-Rejection Sampling from density f =
π/K, known up to constant K ̸= 0, with blanketing density q that fulﬁlls π(x) ≤
Aq(x), A < ∞, for all x in the support of π.
Input: Target density f = π/K
Blanketing density q
Output: Sample Y ∼p
1 Generate Y ∼q
2 Generate U ∼U(0, 1) independently of Y
3 if U ≤
π(Y )
Aq(Y ) then
4
Accept Y
5 else
6
Go back to step 1
7 end
44

4.3
Adaptive Rejection Metropolis Sampling
The proof that Acceptance-Rejection Sampling produces variates with the desired density
is given in Gamerman and Lopes (2006, Section 1.5.1).
It is also important to note, that the acceptance probability in step 3 of Algorithm 4.3 is
1/A. Therefore, it is desirable to ﬁnd a density q with a constant A that is close to one,
in order to maximize the chance of acceptance.
4.3.2
Adaptive Rejection Sampling
One way to achieve closely ﬁtting blanketing densities in the case of log-concave target
densities f = π/K, K ̸= 0, is Adaptive Rejection Sampling (ARS), introduced by Gilks
(1992).
A function π is called log-concave if
log(π(x1)) −log(π(x2)) ≤log(π(x2)) −log(π(x3)),
for all x1 < x2 < x3 in the support of π, or, alternatively, if (log(π))′ is non-increasing.
Many popular distributions such as the normal, Gamma with s > 1, or densities from
generalized linear models with canonical link functions (see Dellaportas and Smith, 1993)
are log-concave (more examples are given by Gilks, 1992, Table 2).
As the name implies, ARS follows the same idea as Rejection Sampling, but instead of
choosing an arbitrary density q to draw from, a blanket is created adaptively. Starting
with a set of initial points S = {x1, . . . , xn}, n ∈N, in the support of π, the intersections
of either the tangent or secant lines of ln(π(x1)), . . . , ln(π(xn)) are calculated. In the case
of bounded supports, the values of the outermost tangents at the bounds are also added
as intersection points. Then, the line segments between those intersections are used to
construct a piecewise blanket qS of π.
The choice between tangent or secant lines is one of computational eﬀort. While tangent
lines provide a better approximation, they also require the calculation of the derivative
of ln(π) (or π), while secant lines only require the evaluation of π on S.
In the tangent case, the blanket is given by piecewise exponential distributions with
parameters chosen by the derivative of ln(π) at the points x1, . . . , xn, and support given
by the intersection points of adjacent lines. A full description of the algorithm with proofs
is given by Wild and Gilks (1993).
The adaptiveness of the algorithm follows from the fact that points x∗, drawn by the
algorithm, can be incorporated into the grid S = {x1, . . . , xn} that is used to create the
blanketing density if they are rejected. Assume that x∗is drawn from qS, the piecewise
exponential density given by the initial set of grid points S. If x∗is accepted, then it is
a sample from p. If it is rejected, x∗is added to S and a new tangent line in the interval
(xj, xj+1], j = 1, . . . , n −1, that x∗belongs to is added to the envelope qS. This method
gradually improves the approximation qS of π, and therefore increases acceptance rates.
Figure 4.1 shows an example of the ARS algorithm for a target Beta(2, 2.5) density.
Commonly 2 to 4 starting points oﬀer a good approximation to start the algorithm and
the same number of points is added before the ﬁrst successful draw.
ARS requires log-concavity to ensure that qS is a valid blanketing density of π, a require-
ment that is dropped for ARMS. Although ARS works well in the univariate case, since
the exponential distribution can eﬃciently be sampled from, multivariate extensions using
hyperplanes exist, but lead to more complex sampling from qS.
45

4
Sampling Methods
0.0
0.2
0.4
0.6
0.8
1.0
−2.5
−1.5
−0.5
0.5
1.0
x
Log density
G
G
G
G
G
G
G
G
G
G
G
G
0.0
0.2
0.4
0.6
0.8
1.0
−2.5
−1.5
−0.5
0.5
1.0
x
Log density
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
Figure 4.1:
Visualization of the ARS algorithm with density π (solid black line)
of the Beta(2, 2.5) distribution.
Top: Starting with initial grid points (x1, x2, x3) =
(0.1, 0.4, 0.85) (dashed blue lines), tangents of ln(π(xj)), j = 1, . . . , 3, are drawn (dashed
grey lines), and points of intersection of tangents, and intersections with the bounds of
the support (red dots), are calculated. The line segments between intersections (solid
green lines) are then used as blanketing density q. Bottom: Point x∗= 0.7 was rejected
and is added to the grid to improve q.
46

4.3
Adaptive Rejection Metropolis Sampling
Algorithm 4.4: Algorithm for Adaptive Rejection Sampling from target density
f = π/K, known up to constant K ̸= 0, with initial set of grid points S.
Input: Target density f = π/K
Initial set of grid points S
Output: Sample Y ∼p
1 Calculate piecewise exponential density qS
2 Generate Y ∼qS
3 Generate U ∼U(0, 1) independently of Y
4 if U ≤
π(Y )
qS(Y ) then
5
Accept Y
6 else
7
Add Y to S
8
Go back to step 1
9 end
4.3.3
Adaptive Rejection Metropolis Sampling
ARS can be extended to non-log-concave univariate densities f = π/K, K ̸= 0, by adding
a MH acceptance step to Algorithm 4.4. Samples will no longer be independent, but part
of a dependent Markov chain. Therefore, the advantage of ARMS over ordinary MH is in
the adaptive way to create a proposal density that is close to the target density, increasing
acceptance rates.
As for ARS, the starting point is a set of grid points S and a piecewise exponential blanket
qS, created as described in Section 4.3.2. Since π is not necessarily log-concave, it might
not be fully covered by qS, requiring the use of min{π, qS} in the MH acceptance step.
Furthermore, a previous value of the Markov chain, Xt, t ∈N0 is required.
Algorithm 4.5 describes how samples from π are generated by ARMS. If π is log-concave,
ARMS reduces to ARS, as the MH step will always lead to acceptance. If π is nearly
log-concave, then ARMS will work well, becoming less eﬃcient the less log-concave π is,
as the number of MH rejections increases.
When used in conjunction with Gibbs sampling, it is also possible to use the previous
iteration to calculate a starting grid S that allows qS to closely match π. As long as S is
chosen independently of the current value Xt of the Markov chain, the resulting chain’s
stationary distribution will be π, as shown by Gilks et al. (1995b).
It is important to note that Besag et al. (1995) show that the number of iterations in
the ARS part of Algorithm 4.5 might be unbounded. Hence, it could be necessary to
introduce a constant c ∈N and advance to the MH step using qS −min{π(Y ), qS(Y )}
instead of min{π(Y ), qS(Y )}, where Y is the c-th draw of ARS (see Roberts et al., 1995).
47

4
Sampling Methods
Algorithm 4.5: Algorithm for Adaptive Rejection Metropolis Sampling from target
density f = π/K, known up to constant K ̸= 0, with initial set of grid points S and
current Markov chain value Xt.
Input: Target density f = π/K
Initial set of grid points S
Current value of the Markov chain Xt
Output: Next value of the Markov chain Xt+1
1 Calculate piecewise exponential density qS
2 Generate Y ∼qS
3 Generate independent U ∼U(0, 1)
4 if U ≤p(Y )/qS(Y ) then
5
Set XA = Y
6 else
7
Add Y to S
8
Go back to Step 1
9 end
10 Generate independent U ∼U(0, 1)
11 if U ≤π(XA) min{π(Xt),qS(Xt)}
π(Xt) min{π(XA),qS(XA)} then
12
Set Xt+1 = XA
13 else
14
Set Xt+1 = Xt
15 end
48

5
Marginal models
One of the major advantages of analyzing data with copulas is that the process of specify-
ing a model can be separated into a univariate marginal and multivariate part. In many
applications there are thoroughly studied univariate models that accurately describe data,
but multivariate extensions are diﬃcult or wholly unavailable.
This thesis considers two diﬀerent marginal models. ARMA-GARCH models are popular
for modeling returns of stocks and often provide good ﬁts and forecasting performance.
DLMs are a highly ﬂexible class of Bayesian state space models that demonstrate good
modeling performance in a wide range of applications, and are particularly appealing if a
full Bayesian analysis of data is desired.
In Section 5.1 ARMA-GARCH processes are deﬁned and the implementation used to ﬁt
empirical data given. Section 5.2 provides important deﬁnitions and properties of DLMs
with a focus on the variance discounting DLM.
5.1
ARMA-GARCH processes
ARMA-GARCH processes are widely used in practice. They are frequently employed
both in quantitative risk management, e.g., McNeil et al. (2005, Chapter 4), and ﬁnance,
e.g., Hull (2012, Chapter 22), but are also utilized for ﬁltering time series data.
In this thesis, as a point of reference to a fully Bayesian analysis, an ARMA(1, 1)-
GARCH(1, 1) marginal model is used to transform empirical stock returns to copula data
in Section 9.
5.1.1
Deﬁnition
This section provides a brief overview over the stochastic processes necessary to deﬁne
ARMA-GARCH processes. Thorough details about properties, parameter estimation and
forecasting are omitted for brevity. A deeper discussion of the theory of ARMA-GARCH
models can be found in, e.g., Box et al. (2008).
Deﬁnition 5.1 (White noise process). A sequence (εt)t∈N0 of i.i.d. random variables with
E[εt] = 0
and
Var(εt) = σ2
εt,
for all t ∈N0, is called a white noise process.
In practice, white noise processes are often viewed as a series of independent normally
distributed ,,shocks” with zero mean. They are a random component added on top a
deterministic signal, making it harder to discern.
Some of the most popular choices for time series generated by a white noise process will
be discussed in the following.
Deﬁnition 5.2 (Autoregressive process). A stochastic process (Xt)t∈N0 of the form
Xt =
p
X
i=1
ϕiXt−i + εt,
49

5
Marginal models
where ϕ1, . . . , ϕp is a ﬁnite number of non-zero weights, Xt−i = 0 for t < i, and (εt)t∈N0
is a white noise process, is called an autoregressive process of order p (AR(p)).
Deﬁnition 5.3 (Moving average process). A stochastic process (Xt)n∈N0 of the form
Xt =
q
X
j=1
ψjεt−j + εt,
where ψ1, . . . , ψq is a ﬁnite number of non-zero weights, εt−j = 0 for t < j, and (ε)n∈N0 is
a white noise process, is called a moving average process of order q (MA(q)).
In practice, data often suggests an underlying process that exhibits both the characterist-
ics of an autoregressive and a moving average process. Since there only exist undesirable
representations of a ﬁnite moving average processes as an inﬁnite autoregressive process
(see Box et al., 2008, Chapter 3.1.1) and no representation of an autoregressive process
as a moving average process, it is often helpful to use a mixed model of both an AR(p)
and a MA(q) process.
Deﬁnition 5.4 (Mixed autoregressive-moving average process). A stochastic process
(Xt)n∈N0 of the form
Xt = µ +
p
X
i=1
ϕiXt−i +
q
X
j=1
ψjεt−j + εt,
where ϕ1, . . . , ϕp, ψ1, . . . , ψq is a ﬁnite number of non-zero weights, µ ∈R, Xt−i = 0, for
t < i, εt−j = 0, for t < j, and (ε)n∈N0 a white noise process, is called a mixed autoregressive
moving average process of orders p and q (ARMA(p, q)).
Sometimes it is not possible to capture all characteristics of a time series by relying solely
on linear models. In these cases (conditional) heteroskedastic models, which exhibit phases
of diﬀering degrees of volatility depending on past values of the process, can be of use.
To this end, autoregressive conditional heteroskedastic (ARCH) models were introduced
by Engle (1982) and later generalized by Bollerslev (1986).
Deﬁnition 5.5 (Autoregressive conditional heteroskedastic model). A stochastic process
(Xt)t∈N0 with white noise process (εt)t∈N0 that fulﬁlls
E[εt | εt−1, . . . , ε0] = 0
and
σ2
t ··= Var(ε2
t | εt−1, . . . , ε0) = ω +
r
X
i=1
αiε2
t−i
where ω > 0, α1, . . . , αr ≥0, and εt−i = 0 for t < i, is called an autoregressive conditional
heteroskedastic process of order r (ARCH(r)). In this context, the white noise process is
often called innovations.
It is natural to generalize the deﬁnition of the ARCH process by including the history of
its conditional variance.
50

5.1
ARMA-GARCH processes
Deﬁnition 5.6 (Generalized ARCH). A stochastic process (Xt)t∈N0 with white noise
process (εt)t∈N0 as in Deﬁnition 5.5, and
σ2
t = ω +
r
X
i=1
αiε2
t−i +
s
X
j=1
βjσt−j,
where β1, . . . , βs ≥0, and σ2
t−j = 0 for t < j, is called a generalized ARCH model of orders
r and p (GARCH(r, s)).
All of the previously deﬁned processes can be combined to create the ARMA-GARCH
process.
Deﬁnition 5.7 (ARMA-GARCH process). An ARMA(p, q) process (Xt)t∈N0 as in Deﬁn-
ition 5.4, with a GARCH(r, s) white noise process (εt)t∈N0 as in Deﬁnition 5.6, is called
an ARMA(p, q)-GARCH(r, s) process.
Example 5.8 (ARMA(1,1)-GARCH(1,1) process). One important special case of the
ARMA(p, q)-GARCH(r, s) process is the ARMA(1, 1)-GARCH(1, 1) model. Since it is
used to ﬁlter empirical data in Section 9.2, it is explicitly stated here.
The ARMA(1, 1)-GARCH(1, 1) process (Xt)t∈N0 with white noise process (εt)t∈N0 is given
by
Xt = µ + ϕ1Xt−1 + ψ1εt−1 + εt,
and
σ2
t = ω + α1ε2
t−1 + β1σ2
t−1,
where ϕ1, ψ1, α1, β1 ̸= 0, µ ∈R and ω > 0.
5.1.2
Quasi-maximum likelihood estimation
There are several possibilities for estimating parameters of ARMA-GARCH models, each
with its own set of conditions and convergence behavior. When estimating GARCH pro-
cesses, one of the most popular methods is quasi-maximum likelihood estimation (QMLE)
since it only places mild assumptions on the time series. Alternatives such as least-squares
and maximum likelihood are available, but more restrictive by, e.g., requiring higher mo-
ments to exist which is often not the case for ﬁnancial data.
In the following, QMLE of ARMA-GARCH processes, as an extension of GARCH pro-
cesses, is described. A thorough discussion about conditions and convergence is given by
Francq and Zakoian (2011, Chapter 7.2). Additional estimation techniques for GARCH
models are outlined in Francq and Zakoian (2011, Chapters 6 and 9), and for ARMA
models in Box et al. (2008, Chapter 7.1).
Let (Xt)t=0,...,T, T ∈N, originate from an ARMA(p, q)-GARCH(r, s) process as in Deﬁni-
tion 5.7, with known orders r, s, p and q, and parameters ϑ = (µ, ϕ1, . . . , ϕp, ψ1, . . . , ψq, ω,
α1, . . . , αr, β1, . . . , βs). Let initial value bσ2
0 > 0 be given. Then, setting eε0 = X0 −µ, eεt
and eσt can be calculated iteratively for all t = 1, . . . , T by
eεt ··= eεt(ϑ) = Xt −µ −
p
X
i=1
ϕ1Xt−i −
q
X
j=1
ψjeεt−j,
eσ2
t ··= eσ2
t (ϑ) = ω +
r
X
i=1
αieε2
t−i +
s
X
j=1
βjeσ2
t−j,
51

5
Marginal models
where Xk, eεk, eσk = 0 for k < 0. When eεt is estimated as bε using an empirical time series
it is called residual, and if eσ2
t is also estimated as bσ2
t , bεt/bσ2
t is called standardized residual.
By Deﬁnition 5.7, eεt are assumed to be independent with zero mean, conditional variance
eσ2
t , and conditional density f(·; eσ2
t ) for all t = 0, . . . , n. Therefore, the likelihood function
is given by
L(ϑ) =
n
Y
t=0
f(eεt; eσ2
t ),
and the QMLE by
bϑ = arg max
ϑ∈Θ
L(ϑ).
Popular choices for conditional densities f are the normal and Student’s t distribution.
Additional choices of conditional distributions are discussed in Section 5.1.3.
Remark 5.9. Initial value eσ2
0 can either be set by expert judgment, use of relations of
the time series and its ﬁrst conditional variance or simply be added as an additional
parameter for maximum likelihood estimation. Adequateness of the choice of eσ2
0 can be
checked by model backtesting.
5.1.3
Implementation
Fitting of ARMA(1, 1)-GARCH(1, 1) models is done with the R package rugarch (see
Ghalanos, 2014). rugarch enables the estimation of parameters, ﬁltering, forecasting,
backtesting and simulation from ARMA-GARCH models with a high ﬂexibility in the
choice of both the GARCH model and conditional distribution of the innovations.
Choices for the conditional distribution of the ARMA-GARCH innovations include the
normal, Student’s t, negative-inverse Gaussian (NIG) and generalized hyperbolic (GH)
distributions. The NIG and GH distributions have four and ﬁve parameters, respect-
ively, and allow to model the leptokurtic behavior often encountered in stock returns, as
discussed in, e.g., Chen et al. (2008). Their deﬁnitions are given in the following. A com-
parison of the densities of all of the mentioned four distributions is shown in Figure 5.1.
Deﬁnition 5.10 (Generalized hyperbolic distribution). The generalized hyperbolic (GH)
distribution is a popular choice for modeling heavy-tailed risk factors, introduced by
Barndorﬀ-Nielsen (1977). In its parametrization with ﬁve parameters, the density of the
GH distribution is given by
f(x; λ, α, β, δ, µ) =
√
α2−β2
δ
λ
√
2πKλ

δ
p
α2 −β2

Kλ−1
2

α
q
δ2 + (x −µ)2
√
δ2+(x−µ)2
α
 1
2 −λ
eβ(x−µ),
x ∈R,
where λ, α, β, δ, µ ∈R with
δ ≥0, |β| < α
if λ > 0,
δ > 0, |β| < α
if λ = 0,
δ > 0, |β| ≤α
if λ < 0,
52

5.1
ARMA-GARCH processes
−3
−2
−1
0
1
2
3
0.0
0.2
0.4
0.6
0.8
x
Density
Normal
Student's t
Normal−inverse Gaussian
Generalized hyperbolic
Figure 5.1: Densities of the normal, Student’s t, normal-inverse Gaussian and general-
ized hyperbolic distribution. All distributions are centered around zero with unit variance
and no skewness.
and Kλ is the modiﬁed Bessel function of the third kind given by,
Kλ(x) = 1
2
Z ∞
0
yλ−1e−x
2 (y+y−1) dy.
(5.1)
Deﬁnition 5.11 (Normal-inverse Gaussian distribution). The normal-inverse Gaussian
(NIG) distribution is an important subclass of the GH distribution, where λ = −1
2. The
density of the NIG distribution is given by
f(x; α, β, δ, µ) = αδ
π
K1

α
q
δ2 + (x −µ)2
q
δ2 + (x −µ)2
eδ√
α2−β2+β(x−µ),
x ∈R,
where µ ∈R, δ > 0, |β| ≤α and K1 is deﬁned as in (5.1).
53

5
Marginal models
5.2
Dynamic linear models
In this thesis, univariate DLMs with variance discounting are used as a Bayesian altern-
ative to ARMA-GARCH models as marginal models for stock data. This section provides
the deﬁnitions and properties of the underlying DLMs, and motivates the use of variance
discounting.
While DLMs can easily be deﬁned for multivariate observations Xt ∈Rp, (see Harrison
and West, 1999, Chapter 16), only the univariate case p = 1 will be covered in this thesis.
A thorough discussion of DLMs with variance discounting is given by Prado and West
(2010, Chapter 4.3.6) and Harrison and West (1999, Chapter 10.8.2).
Deﬁnition 5.12 (Univariate DLM). Let (Xt)t∈N with Xt ∈R for all discrete time points
t ∈N. Then, the general (normal) DLM is deﬁned by a set of quadruples
{F , G, V, W }t = {Ft, Gt, Vt, Wt},
where Ft ∈Rn, Gt ∈Rn×n, Vt ∈R and Wt ∈Rn×n are known for all t. These quadruples
govern the distributions of observations Xt and parameter vector θt ∈Rn sequentially for
each time t by
(Xt | θt) ∼N(F ⊤
t θt, Vt)
(5.2)
and
(θt | θt−1) ∼N(Gtθt−1, Wt).
(5.3)
Implicitly, equations (5.2) and (5.3) depend on the set of information Dt−1 available before
time t, with initial information D0, but for notational simplicity this will not be explicitly
stated. Dt includes, in particular, past observations Xt−1, . . . , X1, and variances Vt and
Wt.
Alternatively, (5.2) and (5.3) can be written as
Xt = F ⊤
t θt + νt,
νt ∼N(0, Vt)
(5.4)
and
θt = Gtθt−1 + ωt,
ωt ∼N(0, Wt),
(5.5)
where νt and ωt are independent for all t. (5.4) is called observation equation and (5.5)
evolution, state or system equation.
This can be used to derive posterior and forecasting distributions.
Theorem 5.13. The model deﬁned by Deﬁnition 5.12 with initial prior information
(θ0 | D0) ∼N(m0, C0)
for given mean vector m0 ∈Rn and variance matrix C0 ∈Rn×n has the following one-step
forecasts and posteriors for all t:
1. Posterior at time t −1 for some mt−1 ∈Rn and Ct−1 ∈Rn×n:
(θt−1 | Dt−1) ∼N(mt−1, Ct−1).
54

5.2
Dynamic linear models
2. Prior at time t:
(θt | Dt−1) ∼N(at, Rt),
with at = Gtmt−1 and Rt = GtCt−1G⊤
t + Wt.
3. One-step ahead forecast:
(Xt | Dt−1) ∼N(ft, Qt),
where ft = F ⊤
t at and Qt = F ⊤
t RtFt + Vt.
4. Posterior at time t:
(θt | Dt) ∼N(mt, Ct),
with mt = at+Atet and Ct = Rt = AtQtA⊤
t , where At = RtFt/Qt and et = Xt−ft.
Proof. See Harrison and West (1999, Theorem 4.1).
It is reasonable to assume that the regression vectors Ft and evolution matrices Gt are
known, since they are part of model design, which is in the hands of the modeler. Fur-
thermore, evolution variance matrix Wt is commonly speciﬁed by the discount principle.
This leaves only the observational variance Vt as an unknown quantity to arrive at a fully
speciﬁed DLM quadruple.
Assuming that Vt = V for all t, unknown and constant, and setting φ = V −1 as the
unknown precision parameter, enables a fully conjugate Bayesian analysis as described in
the following.
Deﬁnition 5.14 (DLM with unknown constant variance). For all t ∈N, let the DLM
with unknown constant observational variance V and precision φ = V −1 be given by the
observation equation
Xt = F ⊤
t θt + νt,
νt ∼N(0, V ),
system equation
θt = Gtθt−1 + ωt,
ωt ∼N(0, V W ∗
t ),
and initial information
(θ0 | D0, φ) ∼N(m0, V C∗
0),
(φ | D0) ∼G
n0
2 , n0S0
2

.
Given V , the same (conditional) independence assumptions hold as in Deﬁnition 5.12.
The initial priors m0, C∗
0, n0, S0 and matrices {Ft, Gt, W ∗
t } have to be supplied, where,
due to E[φ | D0] = 1/S0, S0 is a ﬁrst point estimate of V .
Assuming unknown constant variance V for the DLM leads to new distributional results
for observations Xt and precision φ = V −1.
Theorem 5.15. Let the DLM of Deﬁnition 5.14 be given.
Then, for all t ∈N, the
following holds:
55

5
Marginal models
1. For φ = V −1:
(φ | Dt−1) ∼G
nt−1
2 , nt−1St−1
2

,
(φ | Dt) ∼G
nt
2 , ntSt
2

,
with nt = nt−1 + 1 and ntSt = nt−1St−1 + e2
t/Q∗
t.
2. One-step ahead forecasts, unconditional on V :
(θt | Dt−1) ∼tnt−1(at, Rt),
(Xt | Dt−1) ∼tnt−1(ft, Qt),
with at = Gtmt−1, R∗
t = GtC∗
t−1G⊤
t + W ∗
t , ft = F ⊤
t at, Q∗
t = 1 + F ⊤
t R∗
tFt, and
Rt = St−1R∗
t and Qt = St−1Q∗
t.
Proof. See Harrison and West (1999, Theorem 4.3).
The ﬁnal step is to allow for time-varying, random variance or precision Vt or φt = 1/Vt,
respectively, for all t. With this notation, the time t −1 posterior of φt−1 can be written
as
(φt−1 | Dt−1) ∼G
nt−1
2 , dt−1
2

,
(5.6)
where dt−1 = nt−1St−1 and nt = nt−1 + 1.
Since it is desirable to retain a Gamma distribution, which is conjugate to the updating
function of Xt, variance discounting enables the modeling of the decay of information of
the precision. This method preserves Gamma prior and posterior distributions for φt. Let
β ∈(0, 1) be the variance discount factor, and the new time t prior be given by
(φt | Dt−1) ∼G
βnt−1
2
, βdt−1
2

.
(5.7)
Then, the location remains unchanged compared to (5.6), as
E[φt | Dt−1] = E[φt−1 | Dt−1] =
1
St−1
,
but the dispersion is increased by the discounting of the degrees of freedom parameter,
βnt−1 < nt−1.
Formally, this can be introduced by using γt, independent of φt−1 for all t, with
(γt | Dt−1) ∼Beta
βnt−1
2
, (1 −β)nt−1
2

.
Hence, E[γt | Dt−1] = β, and, given φt−1, φt is set to
φt = γt
β φt−1.
56

5.2
Dynamic linear models
In conjunction with prior (5.6), this formula leads to the desired Gamma distribution of
φt in (5.7) for all nt−1, dt−1 and β.
γt/β can be viewed as independent, random shocks to φt, resulting in increased uncer-
tainty, while leaving the general closed-form updating formulas intact. In practice, sensible
discount factors β are commonly between 0.95 and 0.99, where β = 1 leads to the special
case Vt = V for all t.
This motivation leads to the DLM with variance discounting deﬁned below.
Deﬁnition 5.16 (DLM with constant variance discounting). For all t ∈N, let the DLM
with unknown variance Vt, respectively precision φt = V −1
t
, be given by observation
equation
Xt = F ⊤
t θt + νt,
νt ∼N

0, kt
φt

,
system equation
θt = Gtθt−1 + ωt,
ωt ∼tnt−1(0, Wt),
precision
φt = γt
β φt−1,
γt ∼Beta
βnt−1
2
, (1 −β)nt−1
2

,
and initial information
(θ0 | D0, φ0) ∼N(m0, C0),
(φ0 | D0) ∼G
n0
2 , d0
2

.
ν1, . . . , νt, ω1, . . . , ωt are independent given φt, and γt is independent of φt−1 for all t.
Furthermore, β ∈(0, 1) is a constant and known discount factor, kt ∈[0, 1] are known
weights, and the vectors Ft ∈Rn and matrices G, Wt ∈Rn×n are known.
Remark 5.17. The model of Deﬁnition 5.16 has the closed-form updating scheme shown
in Table 5.1 (see Harrison and West, 1999, Table 10.4).
57

5
Marginal models
Univariate DLM with constant variance discounting
Observation:
Xt = F ⊤
t θt + νt,
νt ∼N
 0, kt
φt

System:
θt = Gtθt−1 + ωt,
ωt ∼tnt−1(0, Wt)
Precision:
φt =
γt
βφt−1,
γt ∼Beta
 β nt−1
2 , (1 −β)nt−1
2

Information:
(θt−1 | Dt−1) ∼tnt−1(mt−1, Ct−1)
(θt | Dt−1) ∼tnt−1(at, Rt)
with at = Gtmt−1, Rt = GtCt−1G⊤
t + Wt
(φt−1 | Dt−1) ∼G
  nt−1
2 , dt−1
2

(φt | Dt−1) ∼G
 β nt−1
2 , β dt−1
2

with St−1 = dt−1
nt−1
Forecast:
(Xt | Dt−1) ∼tβnt−1(ft, Qt)
with ft = F ⊤
t at and Qt = F ⊤
t RtFt + St−1
Update:
(θt | Dt) ∼tnt(mt, Ct)
(φt | Dt) ∼G
  nt
2 , dt
2

with mt = at + Atet, Ct =
St
St−1(Rt −AtA⊤
t Qt),
nt = βnt−1 + 1, dt = βdt−1 +
1
QtSt−1e2
t, St = dt
nt,
where et = Xt −ft and At =
1
QtRtFt
Table 5.1: Model formulation, forecasting and updating formulas for univariate DLM
with constant variance discounting parameter β ∈(0, 1).
58

6
Factor copula models
Factor copula models are conditional independence random eﬀects models. They extend
multivariate normal conditional independence models as shown in Joe (2014, Section
3.10). The case with one latent factor was originally developed in Joe (2011) and later
generalized to arbitrary numbers of factors in Krupskii and Joe (2013).
It is possible to deﬁne factor copula models for discrete random variables, though, in
this thesis, only the continuous case will be considered. Discrete response variables are
examined in Nikoloulopoulos and Joe (2012) and Joe (2014, Section 3.10.2).
In the following, the model will be introduced with special focus on the one- and two-factor
cases. Furthermore, some important properties will be presented.
The p-factor copula model is deﬁned in Section 6.1, and examples for the special cases
p = 1 and p = 2 are given. Section 6.2 provides theoretical results for the dependence
and tail properties of the one- and two-factor copula models.
6.1
Model formulation and examples
The p-factor copula model is used to describe the distribution of a d-dimensional random
vector U = (U1, . . . , Ud) with standard uniform margins Uj ∼U(0, 1) for j = 1, . . . , d.
Since its margins are standardized, the distribution of U is given by the d-dimensional
copula C. An integral part of the model is the assumption that U1, . . . , Ud are independent
given p latent variables V1, . . . , Vp.
Without loss of generality, transforming marginal
distributions if necessary, it can be assumed that V1, . . . , Vp are i.i.d. U(0, 1) distributed
random variables. Denoting the conditional distribution function of Uj given V1, . . . , Vp
by Fj|V1,...,Vp, the distribution function of copula C can then be written as
C(u1, . . . , ud) = F(u1, . . . , ud)
=
Z
[0,1]p F1,...,d| V1,...,Vp(u1, . . . , ud | v1, . . . , vp)fV1,...,Vp(v1, . . . , vp) dv1 · · · dvp,
(6.1)
for u1, . . . , ud ∈[0, 1], where in the second step the law of total probability (see The-
orem 2.2) was used. Note that V1, . . . , Vp are i.i.d. U(0, 1), hence
fV1,...,Vp(v1, . . . , vp) =
p
Y
i=1
fVi(vi) =
p
Y
i=1
1{0≤vi≤1} = 1,
for all v1, . . . , vp ∈[0, 1].
(6.2)
Inserting (6.2) into (6.1), and using the conditional independence of U1, . . . , Ud given
V1, . . . , Vp, it follows
C(u1, . . . , ud) =
Z
[0,1]p
d
Y
j=1
Fj|V1,...,Vp(uj|v1, . . . , vp) dv1 · · · dvp.
(6.3)
6.1.1
One-factor copula model
The simplest case of the p-factor copula is given by p = 1, resulting in the one-factor
copula model. One-factor copula models oﬀer a high degree of analytical tractability and
are therefore used for Bayesian analysis in Section 7.
59

6
Factor copula models
Let Cj,V1 and cj,V1 denote the joint distribution function and density of (Uj, V1) for j =
1, . . . , d, respectively. Since both Uj and V1 are U(0, 1) random variables it follows that
Fj|V1(uj|v1) = Cj|V1(uj|v1) and (6.3) can be rewritten as
C(u1, . . . , ud) =
Z 1
0
d
Y
j=1
Fj|V1(uj|v1) dv1 =
Z 1
0
d
Y
j=1
Cj|V1(uj|v1) dv1.
(6.4)
If it exists, the corresponding density can be derived via diﬀerentiation. Note that due to
Theorem 2.17, Cj|V1(uj | v1) = ∂Cj,V1(uj, v1)/∂v1 and thus
∂2
∂u∂v1
Cj,V1(uj, v1) = ∂
∂uCj|V1(u|v1) = cj,V1(uj, v1),
uj, v1 ∈(0, 1).
(6.5)
Hence, the density can be derived from (6.4) by
c(u1, . . . , ud) = ∂dC(u1, . . . , ud)
∂u1 · · · ∂ud
=
Z 1
0
d
Y
j=1
∂Cj|V1(uj|v1)
∂ud
dv1 =
Z 1
0
d
Y
j=1
cj,V1(uj, v1) dv1,
(6.6)
for v1, u1, . . . , ud ∈(0, 1).
(6.6) shows that the dependence in the model is given by d bivariate copulas C1,V1, . . .,
Cd,V1 with no restrictions on copula classes or parameters. Consequently, any conditional
independence model for absolutely continuous random variables, conditioned on one latent
variable, can be written in this form.
A major advantage of this model is that varying types of tail dependence can be incorpor-
ated. If, e.g., all chosen copulas Cj,V1, j = 1, . . . , d exhibit lower (upper) tail dependence,
the resulting factor copula model also shows lower (upper) tail dependence (see Joe et al.,
2010; Joe, 2011).
Remark 6.1 (Equivalence of one-factor copula model and C-vine copula truncated at level
1). It can be shown that the one-factor copula model is equivalent to a C-vine truncated
at level 1 with root node V1, and integration over the latent variable. A visualization of
this equivalence for dimension d = 5 is shown in Figure 6.1, and a formal proof is given
in the following.
Let X1 ··= V1, (X2, . . . , Xd+1) ··= (U1, . . . , Ud), and note for margins Fj of Xj that Fj(u) =
u, for u ∈[0, 1], j = 1, . . . , d. Thus, by Lemma 2.69 it follows
c(u1, . . . , ud+1) =
d
Y
j=1
c1,j+1 =
d
Y
j=1
c1,j+1(Fj(uj), Fj+1(uj+1)) =
d
Y
j=1
c1,j+1(uj, uj+1),
(6.7)
for u1, . . . , ud+1 ∈(0, 1). Rewriting (6.7) in the notation of the one-factor copula model
leads to
c(v1, u1, . . . , ud) =
d
Y
j=1
cj,V1(uj, v1),
v1, u1, . . . , ud ∈(0, 1).
Finally, integration over V1 yields (6.6).
60

6.1
Model formulation and examples
V1
U1
C1,V1
U2
C2,V1
U3
C3,V1
U4
C4,V1
U5
C5,V1
X1
X2
C2,1
X3
C3,1
X4
C4,1
X5
C5,1
X6
C6,1
Figure 6.1: Visualisation of the one-factor copula model (left) and corresponding C-vine
truncated at level 1 (right) for dimension d = 5.
Example 6.2 (One-factor copula model with Gaussian linking copulas). An important
special case of the one-factor copula model is given by the choice of only bivariate Gaussian
linking copulas for all Cj,V1, j = 1, . . . , d. Then, the model corresponds to a one-factor
normal conditional independence model with correlations ρ11, . . . , ρd1 ∈(0, 1), as shown
in the following.
Let uj = Φ(zj) for zj ∈R, j = 1, . . . , d. It follows from (6.4)
F(z1, . . . , zd) ··= C(Φ(z1), . . . , Φ(zd)) =
Z 1
0
d
Y
j=1
Cj|V1(Φ(zj)|v1) dv1.
(6.8)
Since Cj,V1 is the Gaussian copula with correlation ρj1 for all j = 1, . . . , d, Lemma 2.22
yields
Cj| V1(Φ(zj) | v1) = Φ
 
zj −ρj1Φ−1(v1)
q
1 −ρ2
j1
!
.
(6.9)
Hence, (6.9) can be inserted into (6.8) to arrive at
Z 1
0
d
Y
j=1
Cj|V1(Φ(zj)|v1) dv1 =
Z 1
0
d
Y
j=1
Φ
 
zj −ρj1Φ−1(v1)
q
1 −ρ2
j1
!
dv1.
(6.10)
Substituting w = Φ−1(v1) with dv1 = ϕ(w) dw, (6.10) can ﬁnally be simpliﬁed to
Z 1
0
d
Y
j=1
Φ
 
zj −ρj1Φ−1(v1)
q
1 −ρ2
j1
!
dv1 =
Z ∞
−∞
d
Y
j=1
Φ
 
zj −ρj1w
q
1 −ρ2
j1
!
ϕ(w) dw.
(6.11)
This model agrees with a multivariate normal model with a one-factor correlation struc-
ture and representation
Zj = ρj1W +
q
1 −ρ2
j1 εj,
j = 1, . . . , d,
(6.12)
61

6
Factor copula models
where W, ε1, . . . , εd are i.i.d. N(0, 1) random variables. This can be shown by
F(z1, . . . , zd) = P(Z1 ≤z1, . . . , Zd ≤zd)
= P

ρ11W +
q
1 −ρ2
11 ε1 ≤z1, . . . , ρd1W +
q
1 −ρ2
d1 εd ≤zd

,
(6.13)
where
P

ρj1W +
q
1 −ρ2
j1 εj ≤zj

= P
 
εj ≤zj −ρj1W
q
1 −ρ2
j1
!
=
Z ∞
−∞
P
 
εj ≤zj −ρj1w
q
1 −ρ2
j1
!
ϕ(w) dw
=
Z ∞
−∞
Φ
 
zj −ρj1w
q
1 −ρ2
j1
!
ϕ(w) dw,
(6.14)
for j = 1, . . . , d. In the second step the law of total probability and the fact that W is a
N(0, 1) random variable were used.
Hence, noting that W, ε1, . . . , εd are independent and inserting (6.14), (6.13) can be ex-
pressed as
P

ρ11W +
q
1 −ρ2
11 ε1 ≤z1, . . . , ρd1W +
q
1 −ρ2
d1 εd ≤zd

=
d
Y
j=1
P

ρj1W +
q
1 −ρ2
j1 εj ≤zj

=
Z ∞
−∞
d
Y
j=1
Φ
 
zj −ρj1w
q
1 −ρ2
j1
!
ϕ(w) dw,
which agrees with the one-factor copula distribution function given in (6.11).
Application of Theorem 2.2 to (6.11) proves that the one-factor copula model with only
bivariate Gaussian linking copulas leads to the multivariate Gaussian copula. The same
does not hold for the Student’s t copula.
Similarly to the Gaussian copula, the d-
dimensional Student’s t copula with ν degrees of freedom can be represented as an R-vine
copula, where all edges in tree Tj, j = 1, . . . , d, are bivariate Student’s t copulas with
νj = ν + j −1 degrees of freedom (see Mai and Scherer, 2012, Theorems 5.2 and 5.3).
Though, in this case, integration over the latent variable does not lead to the retention
of Student’s t copula, but instead to the representation of the one-factor copula model as
a one-dimensional integral. In general this one-dimensional integral is the easiest repres-
entation for all choices of linking copulas, except for the Gaussian case, necessitating the
use of numerical integration in most applications.
6.1.2
Two-factor copula model
Adding one additional latent variable to the one-factor copula model, the resulting model
for p = 2 is the two-factor copula model. The two-factor copula model shares most of
the properties of the one-factor case and can be analyzed similarly. Its representation as
a two-dimensional integral is still numerically feasible in applications, and it is therefore
62

6.1
Model formulation and examples
compared with vine copula models in Section 9 of this thesis. In the following some of
the properties of the two-factor copula model are discussed.
As previously, let Cj,V1 denote the copula of (Uj, V1) for j = 1, . . . , d. In addition to the
general framework of the p-factor copula model, a simplifying assumption similar to vine
copula theory, that Fj| V1(· | v1) ··= FUj| V1(· | v1) and FV2| V1(· | v1) do not depend on the
value of v1, is necessary. Under this assumption, the distribution function of the copula
of the joint distribution Fj,V2| V1(·, · | v1) of Fj| V1(· | v1) and FV2| V1(·, v1) and its density can
be written as Cj,V2;V1(·, ·) and cj,V2;V1(·, ·), respectively. Since V1 and V2 are assumed to be
independent U(0, 1) random variables, it follows
FV2| V1(v2 | v1) = FV2(v2) = v2,
for all v1, v2 ∈[0, 1].
(6.15)
By Theorem 2.2,
Fj,V2| V1(uj, v2 | v1) =
Z v2
−∞
Fj| V1,V2(uj | v1, v′
2) dFV2(v′
2) =
Z v2
0
Fj| V1,V2(uj | v1, v′
2) dv′
2,
(6.16)
for all uj, v1, v2 ∈[0, 1], j = 1, . . . , d. Hence, partial diﬀerentiation of (6.16) w.r.t. v2
leads to
Fj| V1,V2(uj | v1, v2) =
∂
∂v2
Fj,V2| V1(uj, v2 | v1).
(6.17)
Using Sklar’s theorem (Theorem 2.12) for Fj,V2| V1 in (6.17), it follows
∂
∂v2
Fj,V2| V1(uj, v2 | v1) =
∂
∂v2
CjV2;V1
 Fj| V1(uj | v1), FV2| V1(v2 | v1)

.
(6.18)
Note that since U1, . . . , Ud and V1 are U(0, 1) random variables, Fj| V1 = Cj| V1. This fact
and inserting (6.15) into (6.18) yield
Fj| V1,V2(uj | v1, v2) =
∂
∂v2
CjV2;V1
 Cj| V1(uj | v1), v2

= Cj| V2;V1
 Cj| V1(uj | v1) | v2

,
(6.19)
where Cj| V2;V1(u1 | u2) = ∂Cj,V2;V1(u1, u2)/∂v. Applying (6.19) to the original factor copula
model (6.3) leads to the ﬁnal representation
C(u1, . . . , ud) =
Z 1
0
Z 1
0
d
Y
j=1
Fj| V1,V2(uj | v1, v2) dv1 dv2
=
Z 1
0
Z 1
0
d
Y
j=1
Cj| V2;V1
 Cj| V1(uj | v1) | v2

dv1 dv2.
(6.20)
Similarly to the one-factor copula model, the density c(u1, . . . , ud) of the two-factor copula
model can be derived by diﬀerentiating (6.20) with respect to u1, . . . , ud. Note that by
the chain rule and (6.5)
∂Cj| V2;V1
 Cj| V1(uj | v1) | v2

∂uj
= cj| V2;V1
 Cj| v1(uj | v1) | v2

cj,V1(uj | v1),
63

6
Factor copula models
and hence, it follows
c(u1, . . . , ud) = ∂dC(u1, . . . , ud)
∂u1 · · · ∂ud
=
Z 1
0
Z 1
0
d
Y
j=1
∂Cj| V2;V1
 Cj| V1(uj | v1) | v2

∂uj
dv1 dv2
=
Z 1
0
Z 1
0
d
Y
j=1

cj,V2;V1
 Cj| V1(uj | v1), v2

cj,V1(uj, v1)

dv1 dv2,
u1, . . . , ud ∈(0, 1).
For the full speciﬁcation of the dependence structure of this model 2d bivariate linking
copulas C1,V1, . . . , Cd,V1, C1,V2;V1, . . . , Cd,V2;V1 are required. Since there are as previously
no restrictions for the choice of copulas, a plethora of dependencies can be modeled,
extending the possibilities of the one-factor copula model.
Remark 6.3 (Equivalence of two-factor copula model and C-vine copula truncated at
level 2). Similarly to the one-factor case, the two-factor copula model is equivalent to a
C-vine copula truncated at level 2, rooted in latent variables V1 and V2, with integration
over both latent variables. A visualization of this fact for dimension d = 5 is shown in
Figure 6.2, and a formal proof is given in the following.
Let (X1, X2) ··= (V1, V2) and Xj+2 ··= Uj for j = 1, . . . , d. Then, by Lemma 2.69 it follows
c(u1, . . . , ud+2) =
2
Y
i=1
d+2−i
Y
j=1
ci+j,i;1,...,i−1 =
 d
Y
j=1
c2+j,2;1
! d+1
Y
j=1
c1+j,1
!
,
(6.21)
for u1, . . . , ud+2 ∈(0, 1). As in (6.7), the second product of (6.21) is given by
d+1
Y
j=1
c1+j,1 =
d+1
Y
j=1
c1+j,1(u1+j, u1).
(6.22)
By Deﬁnition 2.59, the ﬁrst product of (6.21) can be written as
d
Y
j=1
c2+j,2;1 =
d
Y
j=1
c2+j,2;1
 F2+j| 1(u2+j | u1), F2| 1(u2 | u1)

=
d
Y
j=1
c2+j,2;1
 C2+j| 1(u2+j | u1), u2

,
(6.23)
since X1 and X2 (U1 and U2) are i.i.d. U(0, 1), and X2+j, X1 ∼U(0, 1), j = 1, . . . , d.
Inserting (6.22) and (6.23) into (6.21) yields
c(u1, . . . , ud+2) =
 d
Y
j=1
c2+j,2;1
 C2+j| 1(u2+j | u1), u2

! d+1
Y
j=1
c1+j,1
 u1+j, u1

!
.
(6.24)
In the notation of the factor copula model, (6.24) is equivalent to
c(v1, v2, u1, . . . , ud) =
 d
Y
j=1
cj,V2;V1
 Cj| V1(uj | v1), v2

! d
Y
j=1
cj,V1(uj, v1)
!
cV2,V1(v2, v1)
=
d
Y
j=1
cj,V2;V1
 Cj| V1(uj | v1), v2

cj,V1(uj, v1),
(6.25)
64

6.1
Model formulation and examples
V1, V2
U1, V2
C1,V2;V1
U2, V2
C2,V2;V1
U3, V2
C3,V2;V1
U4, V2
C4,V2;V1
U5, V2
C5,V2;V1
X1, X2
X3, X1
C3,2;1
X4, X1
C4,2;1
X5, X1
C5,2;1
X6, X1
C6,2;1
X7, X1
C7,2;1
Figure 6.2: Visualization of the second tree of the C-vine truncated at level 2 corres-
ponding to the two-factor copula model, in the notation of the factor copula model (left)
and C-vine copula (right) for dimension d = 5.
where it was used that cV1,V2 ≡1 since V1 and V2 are independent. Integration over V1
and V2 in (6.25) leads to the density of the factor copula model.
Similarly to the one-factor copula model, an analytically tractable example for the two-
factor copula model is given by choosing only bivariate Gaussian linking copulas.
Example 6.4 (Two-factor copula model with Gaussian linking copulas). Similarly to the
case with one latent factor, the two-factor multivariate normal model is a special case of
the two-factor copula model. A proof of this fact is given in the following.
Let Cj,V1 and Cj,V2;V1 denote bivariate Gaussian copulas with correlations ρj1 ∈(0, 1)
and γj = ρj2/(1 −ρ2
j1)1/2 for j = 1, . . . , d, respectively. Here, ρj2 ∈(0, 1) denotes the
correlation of Zj = Φ(Uj) and W2 = Φ(V2), and hence by the independence of V1 and V2,
γj is the partial correlation of Zj and W2 given W1 = Φ(V1). This can be shown by using
Lemma 2.38
ρZjW2;W1 =
ρZjW2 −ρZjW1ρW2W1
q
(1 −ρ2
ZjW1)(1 −ρ2
W2W1)
=
ρj2 −ρj1 · 0
q
(1 −ρ2
j1)(1 −02)
=
ρj2
q
1 −ρ2
j1
= γj.
Since Cj,V1 and Cj,V2;V1 are bivariate Gaussian copulas, by Lemma 2.22, the conditional
copulas Cj| V1 and Cj| V2;V1 can be written as
Cj| V1(uj | v1) = Φ
 
Φ−1(uj) −ρj1Φ−1(v1)
q
1 −ρ2
j1
!
and
Cj| V2;V1(uj | v2, v1) = Φ
 
Φ−1(uj) −γjΦ−1(v2)
q
1 −γ2
j
!
,
65

6
Factor copula models
for v1, v2, u1, . . . , ud ∈(0, 1), j = 1, . . . , d, respectively. Therefore,
Cj| V2,V1
 Cj|V1(uj | v1) | v2

= Cj| V2;V1
 
Φ
 
Φ−1(uj) −ρj1Φ−1(v1)
q
1 −ρ2
j1
!
| v2
!
= Φ
  
Φ−1(uj) −ρj1Φ−1(v1)
q
1 −ρ2
j1
−γjΦ−1(v2)
!q
1 −γ2
j
!
= Φ
 Φ−1(uj) −ρj1Φ−1(v1) −γj
q
1 −ρ2
j1 Φ−1(v2)
q
(1 −ρ2
j1)(1 −γ2
j )
!
.
(6.26)
Let zj = Φ(uj), j = 1, . . . , d, then the distribution function of the two-factor copula model
is given by inserting (6.26) into (6.20)
F(z1, . . . , zd) = C(Φ(z1), . . . , Φ(zd))
=
Z 1
0
Z 1
0
d
Y
j=1
Φ
 zj −ρj1Φ−1(v1) −γj
q
1 −ρ2
j1 Φ−1(v2)
q
(1 −ρ2
j1)(1 −γ2
j )
!
dv1 dv2.
(6.27)
Substituting wi = Φ−1(vi) with dvi = ϕ(wi) dwi, i = 1, 2, (6.27) can be simpliﬁed to
F(z1, . . . , zd) =
Z ∞
−∞
Z ∞
−∞
d
Y
j=1
Φ
 zj −ρj1w1 −γj
q
1 −ρ2
j1 w2
q
(1 −ρ2
j1)(1 −γ2
j )
!
ϕ(w1)ϕ(w2) dw1 dw2.
(6.28)
This models agrees with the multivariate normal model with two-factor correlation struc-
ture and representation
Zj = ρj1W1 + ρj2W2 +
q
(1 −ρ2
j1)(1 −γ2
j ) εj,
j = 1, . . . , d,
(6.29)
where W1, W2, ε1, . . . , εd are i.i.d. N(0, 1) random variables. As previously, using the law
of total probability,
P(Zj ≤zj) = P
 ρj1W1 + ρj2W2 +
q
(1 −ρ2
j1)(1 −γ2
j ) εj ≤zj

= P
 
εj ≤zj −ρj1W1 −ρj2W2
q
(1 −ρ2
j1)(1 −γ2
j )
!
= P
 
εj ≤
zj −ρj1W1 −γj
q
1 −ρ2
j1 W2
q
(1 −ρ2
j1)(1 −γ2
j )
!
=
Z ∞
−∞
Z ∞
−∞
Φ
 zj −ρj1w1 −γj
q
1 −ρ2
j1 w2
q
(1 −ρ2
j1)(1 −γ2
j )
!
ϕ(w1)ϕ(w2) dw1 dw2,
(6.30)
66

6.2
Properties of the one- and two-factor copula model
for j = 1, . . . , d. Thus, due to conditional independence of Z1, . . . , Zd given W1 and W2,
and using (6.30), the distribution function F of Z1, . . . , Zd is given by
F(z1, . . . , zd) = P(Z1 ≤z1, . . . , Zd ≤zd)
=
Z ∞
−∞
Z ∞
−∞
d
Y
j=1
Φ
 zj −ρj1w1 −γj
q
1 −ρ2
j1 w2
q
(1 −ρ2
j1)(1 −γ2
j )
!
ϕ(w1)ϕ(w2) dw1 dw2,
which agrees with the results of the copula factor model in (6.28).
6.1.3
Factor copula models with more than two factors
In the previous two sections it was shown that factor copulas models are an extension
of conditional independence multivariate normal models with one or two factors. This
property straightforwardly extends to arbitrary p > 2.
Let the p-factor multivariate normal model be speciﬁed by correlation matrix Σ and factor
structure Σ = AA⊤+ Ψ2, where A ∈Rd×p is a matrix of loadings and Ψ2 ∈Rd×d a
diagonal matrix of residual variances.
In contrast to the parametrization of the mul-
tivariate normal model, the factor copula model uses partial correlations ρZjWk;W1,...,Wk−1,
j = 1, . . . , d, k = 1, . . . , p, for factors two and higher, since, as seen in Section 6.1.2,
after the ﬁrst factor the bivariate linking copulas are copulas of conditional distributions.
A more detailed analysis of the relationship of multivariate normal p-factor models and
p-factor copula models is given in Krupskii and Joe (2013, Section 2.2) and Joe (2014,
Section 3.10.1).
The equivalence of factor copula models and truncated C-vine copulas rooted in the
latent variables V1, . . . , Vp can also be extended to higher numbers of latent factors. As
was the case for the one and two-factor copula models, the p-factor copula model for
d-dimensional random variables can be viewed as a C-vine copula of (X1, . . . , Xd+p) ··=
(U1, . . . , Ud, V1, . . . , Vp), truncated at level p with latent variables V1, . . . , Vp integrated out
of the density (see Joe, 2014, Section 3.10.1).
6.2
Properties of the one- and two-factor copula model
Varying choices of bivariate linking copulas alter the behavior of the resulting factor copula
model. Therefore, some of the dependence and tail properties of the one- and two-factor
copula models are presented in this section. Properties are stated for the bivariate margin
C1,2 of the factor copula model for simplicity, but can be applied to any arbitrary bivariate
margin.
A thorough discussion of the following multivariate dependence and tail properties is given
in Joe (1997, Chapter 2.1). Additional applications of these properties to factor copula
models can be found in Krupskii and Joe (2013, Chapter 3).
6.2.1
Dependence properties
Some dependence properties of the bivariate linking copulas of the factor copula model
extend to its bivariate margins under mild assumptions, including positive quadrant de-
pendence, increasing in concordance ordering and stochastic increasing.
67

6
Factor copula models
Positive quadrant dependence indicates that dependence is stronger in the ﬁrst and third
quadrant, i.e., for pairs of variables distributed with a positive quadrant dependent copula,
large or small values occur more often than with independence. For negative quadrant
dependence the same holds true for the second and fourth quadrants.
Increasing in
concordance ordering emerges if dependence becomes stronger when a copula’s dependence
parameter increases, while for copula families with more than one dependence parameter,
all other parameters remain ﬁxed.
Stochastically increasing describes the property of
bivariate conditional distributions that the probability of high values of the ﬁrst variable
increases when the second variable becomes larger. Formal deﬁnitions of these properties
are given below.
Deﬁnition 6.5 (Positive quadrant dependence). Let X = (X1, X2) ∼F be a bivariate
random variable. Then, if
P(X1 > x1, X2 > x2) ≥P(X1 > x1)P(X2 > x2),
(6.31)
or equivalently,
P(X1 ≤x1, X2 ≤x2) ≥P(X1 ≤x1)P(X2 ≤x2),
(6.32)
for all x1, x2 ∈R, X, or F, is called positive quadrant dependent (PQD). X is called
negative quadrant dependent (NQD) if the inequalities in (6.31) and (6.32) are reversed.
Deﬁnition 6.6 (Increasing in concordance ordering). Let C be a bivariate copula with
parameter vector θ ∈Rp, and let θ−i = (θ1, . . . , θi−1, θi+1, . . . , θp). Then, if
C(u1, u2; θi, θ−i) ≤C(u1, u2; θ′
i, θ−i),
u1, u2 ∈[0, 1],
θ−i ∈Rp−1,
(6.33)
for all θi ≤θ′
i, i = 1, . . . , p, C is called increasing in concordance ordering. If the inequality
in (6.33) is reversed, C is called decreasing in concordance ordering.
Deﬁnition 6.7 (Stochastically increasing). Let X = (X1, X2) ∼F be a bivariate random
variable and Fj| i be the conditional distribution function of Xj given Xi, i ̸= j, i, j = 1, 2.
Then, if
P(Xj > xj | Xi = xi) = 1 −Fj| i(xj | xi),
(6.34)
is increasing in xi ∈R for all xj ∈R, Xj is called stochastically increasing (SI) in Xi. Xj
is called stochastically decreasing (SD) in Xi if (6.34) is decreasing in xi for all xj.
A prominent example fulﬁlling all of the above properties in the region of positive de-
pendence is the Gaussian copula, but there are many commonly used copulas that also
satisfy some or all of these characteristics.
In the following, it will be assumed that all bivariate linking copulas of the corresponding
factor copula model are twice continuously diﬀerentiable on (0, 1)2. The results are ﬁrst
stated, without loss of generality, for the bivariate margin C1,2 of the one-factor copula
model.
Lemma 6.8 (SI in the one-factor copula model). Let Cj| V1 = CUi| V1 for j = 1, 2 be SI,
i.e., P(Uj > u | V1 = v) = 1 −Cj| V1(u | v) is non-decreasing in v ∈(0, 1) for all u ∈(0, 1).
Let (U1, U2) ∼C1,2 be a bivariate margin of the one-factor copula model (6.4). Then,
Cov(U1, U2) ≥0,
and C1,2 is PQD or C1,2 ≥u1u2 for all u1, u2 ∈(0, 1).
68

6.2
Properties of the one- and two-factor copula model
Proof. See Krupskii and Joe (2013, Proposition 1).
Lemma 6.9 (PQD and increasing in concordance ordering in the one-factor copula
model). Let C1,2 be a bivariate margin of the one-factor copula model (6.4). Let Cj,V1
be ﬁxed and Cj| V1 stochastically increasing (decreasing) for j = 1, 2.
1. If C1,V1 increases in concordance ordering, then C1,2 is increasing (decreasing) in
concordance.
2. If C1| V1 is SI, then C2| 1 is stochastically increasing (decreasing).
Proof. See Krupskii and Joe (2013, Proposition 2).
The results for the one-factor copula model can easily be extended to two factors in the
case of PQD and increasing in concordance ordering. Extensions for the SI property are
not readily available.
Lemma 6.10 (PQD in the two-factor copula model). Let Cj| V1(· | v1) and Cj| V2;V1(· | v2)
of the two-factor copula model (6.20) be SI for j = 1, 2. Then, the bivariate margin C1,2
is PQD.
Proof. See Krupskii and Joe (2013, Proposition 3).
Lemma 6.11 (Increasing in concordance ordering in the two-factor model). Let C1,V2;V1 of
the two-factor copula model (6.20) be increasing in concordance ordering, and let C2| V2;V1
be stochastically increasing (decreasing). Then, the bivariate margin C1,2 is increasing
(decreasing) in concordance ordering.
Proof. See Krupskii and Joe (2013, Proposition 4).
6.2.2
Tail properties
In this section some tail properties of the one- and two-factor copula model are presented.
Only the lower tail is considered, since properties of the upper tail can be obtained by
the reﬂection Uj 7→1 −Uj for all j = 1, . . . , d.
Deﬁnition 6.12 (Tail order). The lower tail order of a bivariate copula C1,2 is κL ≥1 if
C1,2(u, u) ∼ℓL(u)uκL,
u →0,
where ℓL is a slowly varying function, e.g., a constant or power of −log. If C1,2(u, u) = 0
for all 0 < u < u0 and some u0 ∈(0, 1], then κL = ∞.
Similarly, the upper tail order κU ≥1 is given by
bC1,2(u, u) ∼ℓU(u)uκU,
u →0,
where bC1,2(u1, u2) = u1 + u2 −C1,2(1 −u1, 1 −u2) for u1, u2 ∈[0, 1] is the survival copula.
Remark 6.13. Lower values of κU or κL correspond to higher dependence, i.e., the
highest tail dependence is achieved for κL = 1 or κU = 1. The comonotonic copula, M,
exhibits maximal positive tail dependence with κL = κU = 1 and ℓL(u) = ℓU(u) = 1. In
contrast, the countermonotonic copula, W, has maximal negative tail dependence with
κL = κU = ∞, since there is no probability in the ﬁrst and third quadrant.
69

6
Factor copula models
Tail orders can be used to characterize tail asymmetry: If κL > κU (κL < κU) then
the tail dependence of C1,2 is skewed towards the the upper (lower) tail. Furthermore,
if C1,2(u, u) ∼λLuκ and bC1,2(u, u) ∼λUuκ for u →0 and κ ≥1 with λU > λL > 0
(λL > λU > 0), then the tail dependence of C1,2 is skewed towards the upper (lower) tail.
Values of κ = κL, κU can be split into three categories:
1. Tail dependence if κ = 1.
2. Intermediate tail dependence if 1 < κ < 2.
3. Tail quadrant independence if κ = 2.
In the two-factor copula model, the bivariate margin C1,2 inherits lower (upper) tail
dependence from the bivariate linking copulas under some mild assumptions.
Lemma 6.14 (Tail dependence in the two-factor copula model). For some h > 0, let
limu→0 Cj| V1(u | hu) = tj(h), and let Cj| V2;V1(u | v) and Cj| V1(u | v) be continuous func-
tions of u, v ∈[0, 1] for j = 1, 2 in the two-factor copula model (6.20).
Further, let
limh→0 tj(h) = tj0 > 0, and Cj| V2;V1(tj0 | 0) ≥k0 > 0 for j = 1, 2. Then, the bivariate
margin C1,2 is lower tail dependent. A similar results holds for upper tail dependence.
Proof. See Krupskii and Joe (2013, Proposition 5).
The condition for Cj| V1 of Lemma 6.14, j = 1, . . . , d is implied by the lower tail dependence
of Cj,V1. Therefore, lower (upper) tail dependence can be modeled by lower (upper) tail
dependence of the bivariate linking copulas Cj,V1 of the observed variables with the ﬁrst
latent factor.
Similarly to tail dependence, in many cases also intermediate tail dependence can be
inherited by bivariate margins C1,2 of the factor copula model as, e.g., in Example 6.15.
Example 6.15 (Intermediate tail dependence in the two-factor copula model with Gaus-
sian linking copulas). Let the bivariate Gaussian copula with correlation matrix
P =
1
ρ
ρ
1

,
P −1 =
1
ρ2 −1
−1
ρ
ρ
−1

,
for 0 < ρ < 1 be given. In Hua and Joe (2011, Chapter 2.2) it is shown that the tail
order of a d-dimensional Gaussian copula is κ = 1⊤
d P −11d, where 1d = (1, . . . , 1)⊤with
dimension d. Thus, the tail order of the bivariate Gaussian copula is given by
κ = 1⊤
2 P −112 =
1
ρ2 −1
 1
1
 −1
ρ
ρ
−1
 1
1

= 2(ρ −1)
ρ2 −1
=
2(ρ −1)
(ρ + 1)(ρ −1) =
2
ρ + 1.
If the bivariate linking copulas of the two-factor copula model are Gaussian with correl-
ations ρj1 for Cj,V1 and γj = ρj2/(1 −ρ2
j1)1/2 for Cj,V2;V1, j = 1, 2, respectively, then the
bivariate margin C1,2 is again a Gaussian copula as shown in Example 6.4.
70

6.2
Properties of the one- and two-factor copula model
Using the representation of 6.29, the parameter of C12 is given by
ρ12 = ρ(Z1, Z2) = ρ

ρ11W1 + ρ12W2 +
q
(1 −ρ2
11)(1 −γ2
1) ε1,
ρ21W1 + ρ22W2 +
q
(1 −ρ2
21)(1 −γ2
2) ε2

= ρ11ρ21ρW1W1 + ρ12ρ22ρW2W2
+
q
(1 −ρ2
21)(1 −γ2
2)
q
(1 −ρ2
11)(1 −γ2
1)ρε1ε2
= ρ11ρ21 + ρ12ρ22 + 0,
since W1, W2, ε1, ε2 are i.i.d. N(0, 1) random variables. Hence, C1,2 has positive interme-
diate tail dependence if all ρj1 and ρj2 are positive for j = 1, 2.
Intermediate tail dependence can also be obtained by bivariate extreme value linking
copulas that have upper tail dependence and intermediate lower tail dependence.
Consider the one-factor copula model. Let Cj,V1 be a bivariate extreme value copula with
representation
Cj,V1(u1, u2) = e−(w1+w2)Aj
 w2
w1+w2

= (u1u2)A
 ln(u2)
ln(u1u2)

,
u1, u2 ∈(0, 1),
(6.35)
where wj = −ln(uj) and Aj : [0, 1] →[1
2, 1] is a convex function with Aj(t) ≥max{t, 1−t}
for j = 1, 2.
Lemma 6.16. Let Aj of (6.35) be a continuously diﬀerentiable function with A′
j(t) > −1
for 0 < t < 1
2, j = 1, 2. Then, the lower tail order of the bivariate marginal C1,2 of the
one-factor copula model (6.4) is given by
ξ∗= min
0<s<∞

ξ(s)
	
∈[1, 2],
where
ξ(s) = (s + 1)

A1

s
s + 1

+ A2

s
s + 1

−s.
Proof. See Krupskii and Joe (2013, Proposition 6).
Remark 6.17. Under the assumptions of Lemma 6.16, the bivariate copula margin C1,2
exhibits intermediate lower tail dependence for 1 < ξ∗< 2.
In the proof of Lemma 6.16 it is shown that ξ(s) ≥(s+1)(
s
s+1 +
1
s+1)−s = 1 and ξ(s) = 1
only if Aj
 s∗
s∗+1

=
s∗
s∗+1 and A3−j
 s∗
s∗+1

=
1
s∗+1 for some s∗> 0 and j = 1, 2. With
Aj(t) ≥max{t, 1 −t}, t ∈[0, 1], j = 1, 2, it follows s∗= 1 and Ai
  1
2

= 1
2, which implies
that C1,V1 and C2,V1 are the comonotonic copula. In all other cases the lower tail order is
larger than 1.
71

7
Bayesian analysis of the one-factor copula model
7
Bayesian analysis of the one-factor copula model
For the majority of choices of bivariate linking copulas, the density of the p-factor copula
model deﬁned in Section 7 requires the numerical calculation of a p-dimensional integ-
ral which quickly becomes unfeasible for higher dimensions. Bayesian analysis remedies
this problem by explicitly sampling V1, . . . , Vp, avoiding high-dimensional integration and,
therefore, increasing the number of models that can be examined. Although this advant-
age is more pronounced for larger numbers of p, as a ﬁrst step, a Bayesian approach for
the one-factor copula model is developed in this thesis.
Preliminary considerations about assumptions for the model and transformation of the
copula parameters are presented in Section 7.1. Priors for the variables of the Bayesian
analysis are given in Sections 7.2 and 7.3. Finally, the likelihood function for observations
is derived in Section 7.4 and used to calculate the full conditional densities for Gibbs
sampling in Section 7.5. A summary of the model assumptions, derived priors, posteriors
and likelihood function is given in Table 7.1
7.1
Preliminaries
As a reminder, the one-factor copula model of Section 6.1.1 assumes a d-dimensional ran-
dom vector U = (U1, . . . , Ud), where Uj ∼U(0, 1) for all j = 1, . . . , d, with distribution C,
i.e., U ∼C. Furthermore, it is assumed that there is a latent variable V1 ∼U(0, 1), and
that U1, . . . , Ud are independent given V1. Then, distribution function Fj| V1 can be ex-
pressed by an appropriate choice of (parametric) bivariate linking copula with parameters
θ.
In contrast to this approach, Bayesian analysis views V1 and θ as random parameters
with corresponding prior and posterior distributions.
The distribution function and density of the one-factor copula model derived in Sec-
tion 6.1.1 are given by
C(u1, . . . , ud) =
Z 1
0
d
Y
j=1
Fj| V1(uj | v1) dv1 =
Z 1
0
d
Y
j=1
Cj| V1(uj | v1) dv1,
u1, . . . , ud ∈[0, 1],
and
c(u1, . . . , ud) =
Z 1
0
d
Y
j=1
cj,V1(uj, v1) dv1,
u1, . . . , ud ∈(0, 1),
respectively.
For Bayesian inference, the dependence on the copula parameter vector, assuming para-
metric copula families, θ = (θ1, . . . , θd), where θj = (θj1, . . . , θjkj)⊤∈Rkj, kj ∈N,
j = 1, . . . , d, may again be a vector for copula families with more than one parameter,
can be written explicitly as
c(u1, . . . , ud | θ) =
Z 1
0
d
Y
j=1
cj,v1(uj, v1 | θj) dv1.
Since copula parameters θ1, . . . , θd are assumed to be random variables, they will be
transformed, when possible, from arbitrary ranges to real numbers. This transformation
72

7.2
Prior for copula parameters
requires each copula Cj,V1, j = 1, . . . , d to be known and Cj,V1 to have a one-to-one
relationship between Kendall’s tau and its parameters θj, as is the case for most one
parameter copula families and some two parameters ones, such as the Student’s t copula.
The following discussion will be restricted to cases where these properties are fulﬁlled.
Given Cj,V1, Kendall’s tau can be calculated as a function of θj with τj ··= τ(θj) ∈[−1, 1]
for corresponding j = 1, . . . , d. τj, in turn, will then be transformed with the Fisher
z-transform(ation), given by
z(τ) = 1
2 ln
1 + τ
1 −τ

,
τ ∈(−1, 1)
(7.1)
and inverse transformation
z−1(x) = 1 −
2
e2x + 1,
x ∈R.
Introduced by Fisher (1921), the Fisher z-transform was developed as a transformation
of the sample correlation. Applied to sample correlation ρ, the transformed variable z(ρ)
is asymptotically normal with a variance that is almost constant, independent of the
random variables underlying ρ. The combination of the transformation from the scale of
the copula parameter θj to Kendall’s tau τj, and, ﬁnally, to Fisher z-transform Hj, allows
the use of prior and proposal distributions over the real line.
Therefore, the copula parameters will be analyzed in the form H = (H1, . . . , Hd) ∈Rd,
where
Hj ··= z(τj) = z(τ(θj)),
j = 1, . . . , d.
It is important to note that, although values of Kendall’s tau are in the interval [−1, 1],
the Fisher z-transformation is only well deﬁned for τ ∈(−1, 1). For this thesis, it is
assumed that θj, and therefore τj, is a continuous random variable. Therefore, the set
{−1, 1} has probability zero, and, hence this fact should not pose a problem in practice.
Finally, sampling from the (joint) posterior distributions of the latent variable and copula
parameters relies on the corresponding densities, which are derived in the following. For
notational simplicity, subscripts of densities f of the data and p of posterior distributions
will be omitted. Instead, the corresponding random variables will be made explicit by
their arguments. Subscripts of f, when it is given by bivariate copula densities c, and of
prior densities will be stated.
Bayesian analysis of the one-factor copula model is fully discussed in the following sections.
7.2
Prior for copula parameters
Let Hj be the transform of the parameters of the copula of variable Uj and latent factor V1,
j = 1, . . . , d. While there is naturally dependence between the variables Uj, a fundamental
part of factor copula models is that Uj are assumed to be independent given latent factor
V1. Therefore, dependence between the parameters θj of Uj and V1, if it exists, should be
minimal. This justiﬁes the assumption that H1, . . . , Hd are independent.
Thus, the prior πH corresponding to H = (H1, . . . , Hd) is given by
πH(η1, . . . , ηd) =
d
Y
j=1
πHj(ηj),
η1, . . . , ηd ∈R.
73

7
Bayesian analysis of the one-factor copula model
Determining speciﬁc individual priors πHj is more diﬃcult, and depends on the choice of
bivariate linking copula Cj,V1. For copula families with Kendall’s tau values that span
the whole interval (−1, 1), a normal or Student’s t distributions might be appropriate,
while for, e.g., Gumbel linking copulas, which only admit τ ∈[0, 1), a normal distribution
truncated to [0, ∞) or a Gamma prior may yield good results.
7.3
Prior for latent variables
Let U = (U1, . . . , Un)⊤with Ui = (Ui1, . . . , Uid), be a n × d matrix of n ∈N i.i.d. obser-
vations of a d-dimensional one-factor model. For each observation Ui, i = 1, . . . , n, there
is a distinct, independent realization V1,i of latent variable V1. Therefore, for Bayesian
inference, V1 = (V1,1, . . . , V1,n)⊤yields an additional n independent parameters. Note
that V1 refers to variables V1,1, . . . , V1,n in the context of Bayesian analysis, and V1 to the
original latent variable of the one-factor copula model.
In contrast to the prior of H, the assumption that observations U stem from a one-factor
copula model directly leads to a standard uniform prior πV1,i(·) = 1{0≤·≤1} for i = 1, . . . , n,
since V1 ∼U(0, 1).
Thus, the prior πV1 corresponding to V1 is given by
πV1(v1,1, . . . , v1,n) =
n
Y
i=1
πv1,i(v1,i) =
n
Y
i=1
1{0≤v1,i≤1} ≡1,
v1,1, . . . , v1,n ∈(0, 1).
Thus far, independence within the transformed copula parameters H and latent variables
V1 is assumed. Considering that V1 describes the state of a latent variable and Hj the
copula parameters of Uj and V1, j = 1, . . . , d, it is reasonable to expect the inﬂuence
between V1 and H to be minor. This leads to the assumption of independence between
all n + d parameters V1,1, . . . , V1,n, H1, . . . , Hd. Therefore, the joint prior of V1 and H is
given by
πV1,H(v1, η) = πV1(v1)πH(η) =
 n
Y
i=1
πV1,i(v1,i)
 d
Y
j=1
πHj(ηj)

,
for v1 = (v1,1, . . . , v1,n)⊤∈(0, 1)n and η = (η1, . . . , ηd) ∈Rd.
7.4
Likelihood for observations
The likelihood of V1 and H, given observations U = u = (ui,j)i=1,...,n,j=1,...,d ∈(0, 1), can
be written as
L(v1, η | u) = f(u | v1, η) =
n
Y
i=1
f(ui | v1,i, η) =
n
Y
i=1
d
Y
j=1
f(uij | v1,i, ηj),
(7.2)
for v1,1, . . . , v1,n ∈(0, 1) and η1, . . . , ηd ∈R. Independence of U1, . . . , Un was used in the
second step, and conditional independence of Ui1, . . . , Uid given V1,i, i = 1, . . . , n, in the
third.
74

7.5
Posteriors and full conditional densities
For all i = 1, . . . , n and j = 1, . . . , d, the density f in (7.2) can be written as
f(uij | v1,i, ηj) = f(uij, v1,i, ηj)
πV1,i,Hj(v1,i, ηj) = f(uij, v1,i | ηj)πHj(ηj)
πV1,i(v1,i)πHj(ηj)
= f(uij, v1,i | ηj)
1
= cj,V1(uij, v1,i | ηj),
(7.3)
since V1,i and Hj are independent, πV1,i ≡1, Uij ∼U(0, 1), and V1,1, . . . , V1,n ∼U(0, 1)
i.i.d., allowing the direct use of the bivariate copula density cj,V1 in the last step.
Inserting (7.3) into (7.2) leads to the likelihood function
L(v1, η | u) =
n
Y
i=1
d
Y
j=1
cj,V1(uij, v1,i | ηj).
(7.4)
7.5
Posteriors and full conditional densities
The preceding priors and likelihood function are used in the derivation of the posterior
and the full conditional distributions required for Gibbs sampling.
The joint posterior of V1 and H, given observations U = u = (uij)i=1,...,n,j=1,...,d ∈(0, 1),
can be written as
p(v1, η | u) = f(u, v1, η)
f(u)
= f(u | v1, η)πV1H(v1, η)
f(u)
= f(u | v1, η)πV1(v1)πH(η)
f(u)
, (7.5)
for v1 = (v1,1, . . . , v1,n)⊤∈(0, 1)n and η = (η1, . . . , ηd) ∈Rd.
Note that f(u | v1, η) = L(v1, η | u), and πV1(v1) ≡1, hence, inserting (7.4) into (7.5)
yields
p(v1, η | u) =
Qn
i=1
Qd
j=1 cj,V1(uij, v1,i | ηj)

πH(η)
f(u)
=
Qn
i=1
Qd
j=1 cj,V1(uij, v1,i | ηj)
 Qd
j=1 πHj(ηj)
f(u)
.
(7.6)
This result may now be used to calculate the full conditional densities of V1,i, i = 1, . . . , n
and Hj, j = 1, . . . , d.
Let V1,−i = (V1,1, . . . , V1,i−1, V1,i+1, . . . , V1,n)⊤and H−j = (H1, . . . , Hj−1, Hj+1, . . . , Hd)
for i = 1, . . . , n and j = 1, . . . , d, respectively. Then, for the full conditional density of
V1,k given V1,−k = v1,−k ∈(0, 1)k−1, U = u ∈(0, 1)n×d and H = η ∈Rd, it holds for all
k = 1, . . . , n
p(v1,k | v1,−k, u, η) =
f(v1, u, η)
f(v1,−k, u, η) = p(v1, η | u)f(u)
f(v1,−k, u, η)
=
Qn
i=1
Qd
j=1 cj,V1(uij, v1,i | ηj)
Qd
j=1 πHj(ηj)

f(u)
f(u)f(v1,−k, u, η)
,
(7.7)
where in the third step (7.6) was inserted.
75

7
Bayesian analysis of the one-factor copula model
For the sampling techniques that are used in this thesis it is suﬃcient to know the target
densities up to a constant factor. Therefore, all terms independent of V1,k can summarized
in a proportionality sign. Hence, (7.7) can be written as
p(v1,k | v1,−k, u, η) ∝
d
Y
j=1
cj,V1(ukj, v1,k | ηj).
(7.8)
Similarly, to (7.8), the full conditional density of Hℓ, given H−ℓ= η−ℓ∈Rd−1, U = u ∈
(0, 1)n×d and V = v ∈(0, 1)n, can be derived for all ℓ= 1, . . . , d as
p(ηℓ| η−ℓ, u, v1) = f(η, u, v1)
f(η−ℓ, u, v1) = p(η, v1 | u)f(u)
f(η−ℓ, u, v1)
=
Qn
i=1
Qd
j=1 cj,V1(uij, v1,i | ηj)
Qd
j=1 πηj(ηj)

f(u)
f(u)f(η−ℓ, u, v1)
∝
 n
Y
i=1
cℓ,V1(uiℓ, v1,i | ηℓ)

πHℓ(ηℓ),
(7.9)
where as previously terms independent of Hℓare omitted.
76

7.5
Posteriors and full conditional densities
Observations:
U = (U1, . . . , Un)⊤= (Uij)i=1,...,n,j=1...,d ∈(0, 1)n×d
Parameters:
V1 = (V1,1, . . . , V1,n)⊤∈(0, 1)n
H = (H1, . . . , Hd) ∈Rd,
where Hj = z(τ(θj)), j = 1, . . . , d
Priors:
πV1,H = πV1πH =
 Qn
i=1 πV1,i
 Qd
j=1 πHj

,
with Qn
i=1 πV1,i(v1,i) = Qn
i=1 1{0≤v1,i≤1} ≡1
Likelihood:
L(v1, η | u) = Qn
i=1
Qd
j=1 cj,V1(uij, v1,i | ηj),
where U = u, V1 = v1, H = η
Posteriors:
p(v1,k | v1,−k, u, η) ∝Qd
j=1 cj,V1(ukj, v1,k | ηj), k = 1, . . . , n
p(ηℓ| η−ℓ, u, v1) ∝
 Qn
i=1 cℓ,V1(uiℓ, v1,i | ηℓ)

πηℓ(ηℓ), ℓ= 1, . . . , d
Assumptions:
U1, . . . , Un i.i.d. with copula C,
Ui1, . . . , Uid independent given V1 for all i = 1, . . . , n,
V1,1, . . . , V1,n, H1, . . . , Hd independent
Table 7.1: Summary of the Bayesian setup for the one-factor copula model.
77

8
Simulation study
8
Simulation study
The aims of this simulation study is to verify the correctness of the Gibbs sampling routine
for the one-factor copula model derived in Section 7, and to compare its performance to
the marginal MLE of the unaltered model.
In order to cover a wide range of dependencies, a total of three scenarios of Kendall’s tau
values for the parameters of the bivariate linking copulas are considered. Furthermore,
all linking copulas are chosen to be same, although it would be easily possible to have a
separate choice of linking copula for each variable.
Section 8.1 introduces the simulated data sets of this study. As a point of comparison to
the posterior modes of the Gibbs sampler, marginal MLE is discussed in Section 8.2. The
Gibbs sampler and the ﬁve diﬀerent sampling methods for the full conditionals are given
in Section 8.3. Results of the simulation study are presented and discussed in Section 8.4,
and a ﬁnal summary given in Section 8.5.
8.1
Simulated data
In this simulation study, N = 100 data sets from the one-factor copula (see Section 6.1.1)
for three scenarios are investigated.
Each data set contains n = 200 observations of
dimension d = 5. For all bivariate linking copulas C1,V1, . . . , C5,V1, the bivariate Gumbel
copula is chosen. As an extreme value copula with upper tail dependence and lower tail
independence, the bivariate Gumbel copula is frequently a good ﬁt for tail asymmetric
ﬁnancial data such as negative stock returns, which have a relatively high probability of
being jointly high but are rarely jointly low. Additionally, the density of the bivariate
Gumbel copula quickly tends to inﬁnity at (u, u), with u ↓0 or u ↑1, allowing to test the
robustness of the Gibbs sampling routine for a more extreme choice of linking copulas.
8.1.1
Scenarios
In order to explore the behavior of the Gibbs sampler for varying marginal dependence
between the variables Uj and latent factor V1, j = 1, . . . , 5, three diﬀerent scenarios of
Kendall’s tau values corresponding to the copula parameters are considered. For the ﬁrst
scenario, all copula parameters are set to values corresponding to low association, with
Kendall’s tau values of 0.1 ≤τ ≤0.2. In contrast, for the second scenario high Kendall’s
tau values of 0.5 ≤τ ≤0.8 are used. Finally, the third scenario represents a mix of the
low and high scenarios, with Kendall’s tau values of 0.1 ≤τ ≤0.8.
Detailed values of Kendall’s tau, Gumbel parameter θ, and Fisher z-transform (see Sec-
tion 7.1) of τ are given in Table 8.1.
8.1.2
Simulation
The one-factor copula model assumes conditional independence of U1, . . . , Ud given lat-
ent standard uniform variable V1. Therefore, after generation of latent variable V1, one
sample can be generated by the conditional sampling method (see Mai and Scherer, 2012,
Algorithm 1.2) for each dimension.
For this study, simulation of the data sets was done with the R package CopulaModel
(see Krupskii, 2014), accompanying Joe (2014). Function sim1fact was slightly modiﬁed
78

8.2
Marginal maximum likelihood estimation
Low τ
C1,V1
C2,V1
C3,V1
C4,V1
C5,V1
τ
0.10
0.12
0.15
0.18
0.20
θ
1.11
1.14
1.18
1.21
1.25
η
0.10
0.13
0.15
0.18
0.20
High τ
C1,V1
C2,V1
C3,V1
C4,V1
C5,V1
τ
0.50
0.57
0.65
0.73
0.80
θ
2.00
2.35
2.86
3.64
5.00
η
0.55
0.65
0.78
0.92
1.10
Mixed τ
C1,V1
C2,V1
C3,V1
C4,V1
C5,V1
τ
0.10
0.28
0.45
0.62
0.80
θ
1.11
1.38
1.82
2.67
5.00
η
0.10
0.28
0.48
0.73
1.10
Table 8.1: Parameters for the simulation of one-factor copula data with bivariate Gumbel
linking copulas for three scenarios of Kendall’s tau values with n = 200 observations of
dimension d = 5. Parameters are given in three scales: τ: Kendall’s tau, θ: bivariate
Gumbel copula parameter and η: Fisher z-transformation of τ.
to return the sample of latent variable V1 in addition to the sample of U1, . . . , Ud. The
algorithm underlying sim1fact is given by Algorithm 8.1.
8.2
Marginal maximum likelihood estimation
As a benchmark, results of marginal maximum likelihood estimation as described in
Krupskii and Joe (2013, Section 4) are compared to the posterior estimates of the Gibbs
sampler. Therefore, a brief overview over MLE in factor copula models is given in the
following. Note that in general, due to the complexity of the expression, both numerical
integration and optimization are required.
Let an i.i.d. sample u = (ui)i=1,...,n ∈(0, 1)n×d, where ui = (ui1, . . . , uid) has U(0, 1)
margins and joint distribution C(·; θ), with copula parameter vector θ = (θ1, . . . , θd)⊤,
be given.
Then, according to the derivation of Section 6.1.1, the marginal likelihood
function can be written as
L(u1, . . . , ud; θ) =
n
Y
i=1
c(ui1, . . . , uid; θ),
(8.1)
where in the one-factor case
c(ui1, . . . , uid; θ) =
Z 1
0
d
Y
j=1
cj,V1(uij, v1; θj) dv1,
i = 1, . . . , n.
(8.2)
In order to evaluate the marginal likelihood function, the integral in (8.2) is calculated
numerically with a Gauss-Legendre quadrature of the form
c(ui1, . . . , uid; θ) ≈
nq
X
k=1
wk
d
Y
i=1
cj,V1(uij, xk; θj),
79

8
Simulation study
Algorithm 8.1: Algorithm for simulation of data from a one-factor copula
model, given parameter vector θ and inverses of conditional distribution functions
C−1
1| V1, . . . , C−1
d| V1.
Input: Sample size n ∈N
Parameter vector of bivariate linking copulas θ = (θ1, . . . , θd)⊤,
θj = (θj1, . . . , θjk)⊤, ji ∈R, i = 1, . . . , k, j = 1, . . . , d
Inverse functions of the conditional distribution functions of bivariate
linking copulas C1,V1, . . . , Cd,V1 given by C−1
1| V1, . . . , C−1
d| V1
Output: Sample u ∈[0, 1]n×d of size n and dimension d from the one-factor copula
model with bivariate linking copulas C1,V1, . . . , Cd,V1 and parameters θ
1 Generate v = (v1, . . . , vn)⊤with v1, . . . , vn i.i.d. U(0, 1);
2 for j = 1 to d do
3
for i = 1 to n do
4
Generate u∗∼U(0, 1)
5
Set uij = C−1
j| V1(u∗| vi; θj)
6
end
7 end
where nq ∈R is the number of quadrature points, (wk)k=1,...,nq > 0 are the quadrature
weights, and (xk)k=1,...,nq ∈(0, 1) are the nodes. Optimization of the marginal likelihood
(8.1) is then done via a modiﬁed Newton-Raphson algorithm.
In this thesis, the marginal MLE of θ was calculated with function f90ml1fact of the
CopulaModel R package. For the number of quadrature points, nq = 21 was chosen, as
recommended in Krupskii and Joe (2013, Section 4).
8.3
Gibbs sampler
As a Bayesian method to analyze the parameter vector θ in the context of a one-factor
copula model, a Gibbs sampling routine (see Section 4.1.1) is implemented. This en-
ables direct sampling of latent variable V1, and therefore, does not require the numerical
integration in (8.2) which may become troublesome in higher dimensions.
Gibbs sampling is done via iteratively sampling from the full conditional densities of V1,i,
i = 1, . . . , n and Hj, j = 1, . . . , d, where Hj is the Fisher z-transform of the Kendall’s tau
value corresponding to copula parameter θj (see Section 7.1).
Given i.i.d. data u = (uij)i=1,...,n,j=1,...,d ∈(0, 1)n×d from a one-factor copula model, and
a choice of bivariate linking copulas cj,V1, j = 1, . . . , d, the following full conditionals (see
Section 7.5) are sampled from
p(v1,k | v1,−k, u, η) ∝
d
Y
j=1
cj,V1(ukj, v1,k | ηj),
v1,k ∈(0, 1),
k = 1, . . . , n,
p(ηℓ| η−ℓ, u, v1) ∝
 n
Y
i=1
cℓ,V1(uiℓ, v1,i | ηℓ)
!
πHℓ(ηℓ),
nℓ∈R,
ℓ= 1, . . . , d,
80

8.3
Gibbs sampler
where πHℓis the density of the prior for Hℓ. Hence, in this study, in total n+d = 200+5 =
205 parameters have to be sampled in each iteration of the Gibbs sampler.
As prior πHℓ, a truncated normal N[0,U](0, 1002) distribution, where U ≈2.65 = z(0.99),
is chosen. For the calculation of its density, the R implementation in the truncnorm (see
Trautmann et al., 2014) package is used.
Furthermore, in order to avoid numerical problems while evaluating the full conditional
densities, values ui < 0.001 are set to 0.001 and values ui > 0.999 to 0.999, i = 1, 2 for
the arguments of the density of the bivariate Gumbel copula c(u1, u2; θ). Additionally,
bivariate parameters θ > 100 are set to 100.
Since the full conditional densities admit no convenient known form, ﬁve diﬀerent methods
to sample from the full conditional distributions, discussed in Sections 8.3.2 and 8.3.3,
are compared. The strategy to ﬁnd a starting point for the Gibbs sampling routine is
described in below.
8.3.1
Starting values
As a Markov chain method, Gibbs sampling requires starting values for the simulation of
both V1 and H. Appropriately chosen starting values allow for faster convergence and,
in the case of H, can also be used as a starting point for the optimization of the marginal
maximum likelihood.
The core of the proposed strategy, is to ﬁrst estimate the transform of the bivariate copula
parameters H, and then use this estimate to calculate the maximum of the full conditional
density for each V1,i, i = 1, . . . , n.
In order to estimate H, one variable Uj, j = 1, . . . , d is chosen as a proxy for latent factor
V1. This is done by ﬁnding the star S with the maximal sum of absolute Kendall’s tau
values between each pair of variables. The root node of S is then used as the proxy,
and starting values for all other variables are set the Kendall’s tau of the corresponding
variable with the root node. Since a starting value is also required for the root node itself,
the highest value of absolute Kendall’s tau within S is used.
After transformation, this should oﬀer a rough estimate of the bivariate copula parameters.
These can then be used for estimation of V1,i, i = 1, . . . , n, via maximization of the full
conditional density. This procedure is explained more formally in the following.
Given n observations of data of dimension d with uniform marginals u = (ui)i=1,...,n ∈
(0, 1)n×d, the strategy to ﬁnd starting values consists of the following steps:
1. Calculate T = (τ1, . . . , τd) = (τij)i=1,...,d,j=1,...,d ∈[−1, 1]d×d, the matrix of pairwise
empirical Kendall’s tau values, where entry τij is the empirical Kendall’s tau of
column ui and uj.
2. Find column j⋆of T , with the highest sum of absolute Kendall’s tau, i.e.,
j⋆= arg max
j=1,...,d
(
d
X
i=1
|τij|
)
and set
τ 0
j⋆=
max
i=1,...,j⋆−1,j⋆+1,...,d τij⋆.
81

8
Simulation study
3. Set the starting value for the Fisher z-transforms of Kendall’s tau as
η0 = z(τ1j⋆, . . . , τj⋆−1j⋆, τ 0
j⋆, τj⋆+1j⋆, . . . , τdj⋆),
where z is the Fisher z-transformation. Note that for copula families that admit
either only positive or negative dependence, the sign of the corresponding Kendall’s
tau value may have to be adjusted.
4. Given η0, calculate the starting values v0
1,i, i = 1, . . . , n by
v0
1,i = arg max
v∈(0,1)

p(v | ui, η0)
	
,
where p is the full conditional density of V1,i given by (7.8).
8.3.2
ARMS within Gibbs
One method to sample from the full conditional distributions is ARMS of Section 4.3.
For this thesis, the implementation of the R package HI (see Petris and original C code by
Wally Gilks, 2013) derived from C code originally created for Gilks et al. (1995b) was
employed.
ARMS works for arbitrary univariate densities, and in its R implementation also for mul-
tivariate ones.
Since one requirement of this implementation of ARMS is a convex,
bounded support, for unbounded supports, bounds are chosen such that the probabil-
ity for a random variable to lie within them is close to one. For this study, as bounds for
H, the lower bound 0 and upper bound U ≈2.65 = z(0.99) are selected.
ARMS is used both in its univariate and multivariate form.
For the individual application, in each iteration of the Gibbs sampler, every variable V1,i,
i = 1, . . . , n, and Hj, j = 1, . . . , d, is sampled in turn and separately.
For the block application, all components of V1 and all components H are used to create
an n-dimensional and a d-dimensional block, respectively. Therefore, in each iteration,
one sample is drawn from an n-dimensional and then one sample is drawn from a d-
dimension distribution. In this case, the density to draw from is given by the product of
densities for V1,i, i = 1, . . . , n, and Hj, j = 1, . . . , d, respectively.
8.3.3
Metropolis-Hastings within Gibbs
In addition to individual and block ARMS, the three MH methods discussed in Sec-
tions 4.2.1–4.2.3 are utilized to sample from V1,i, i = 1, . . . , n and Hj, j = 1, . . . , d,
individually. A short summary of each method is given in the following.
1. Mode and curvature matching (see Section 4.2.1): Parameters of the proposal dis-
tribution are chosen such that the mode and curvature of the proposal and target
densities are matched.
Proposal distributions: V1,1, . . . , V1,n ∼Beta(α, β), H1,. . . , Hd ∼N[0,2.65](µ, σ2) .
2. Expectation and variance matching (see Section 4.2.2): Parameters of the proposal
distribution are chosen such that the expectation and variance of the proposal and
target densities are matched.
Proposal distributions: V1,1, . . . , V1,n ∼Beta(α, β), H1, . . . , Hd ∼Gamma(r, s).
82

8.3
Gibbs sampler
3. Independence and random walk samplers (see Section 4.2.3): An independence
sampler is used for latent variables V1, and a random walk sampler for the trans-
forms of the copula parameters H.
Proposal distributions: At iteration t ≥1 of the Gibbs sampler V1,1, . . . , V1,n ∼
U(0, 1), Ht
j ∼N[0,2.65](Ht−1
j
, 0.22), j = 1, . . . , d.
8.3.4
Code validation
As a ﬁnal step of preparation for the Gibbs sampler, the correctness of the implementation
of the sampling methods, described in Sections 8.3.2 and 8.3.3, for the full conditional
distributions is tested. In order to verify that all methods generate random variates from
the desired target distribution, for each method, m = 1 000 samples for the scenario of
mixed Kendall’s tau values are drawn.
Due to the high number of parameters, only the subset {V1,5, V1,19, V1,32, V1,99} of {V1,1, . . . ,
V1,200} is shown for the latent variable. This subset was chosen such that the true values
are roughly equally spaced in the interval [0, 1]. For the transformed copula parameters
H, variates from all Hj, j = 1, . . . , 5 are generated.
As target densities, the full conditional densities of V1 and H given by (7.8) and (7.9),
respectively, are used.
Evaluation of these densities is done with the ﬁrst one of the
previously generated 100 data sets u, and corresponding latent states v1 and transformed
Kendall’s tau values η, as conditional values. For V1,i, i = 5, 10, 32, 99, η = (η1, . . . , η5)
was set to the true values of the mixed Kendall’s tau scenario, in conjunction with the
corresponding row of data ui1, . . . , ui5. For Hj, j = 1, . . . , 5, v1 = (v1,1, . . . , v1,200)⊤was
set to the true values returned by the simulation, in conjunction with the corresponding
column of data u1j, . . . , u200j.
The results shown in Figure 8.1 indicate that all sampling methods expect for block
ARMS yield the desired random variates. Block ARMS appears to struggle with correctly
sampling V1 due to the high dimensional density involved when sampling V1,1, . . . , V1,200
simultaneously, while the lower dimensional H is unaﬀected.
83

8
Simulation study
0.0
0.2
0.4
0.6
0.8
1.0
0
2
4
6
8
12
V1
Density
Individual ARMS for V1
V1,5  
V1,19  
V1,32  
V1,99  
0.0
0.2
0.4
0.6
0.8
1.0
1.2
0
2
4
6
8
12
H
Density
Individual ARMS for H
H1  
H2  
H3  
H4  
H5  
0.0
0.2
0.4
0.6
0.8
1.0
0
2
4
6
8
12
V1
Density
Block ARMS for V1
V1,5  
V1,19  
V1,32  
V1,99  
0.0
0.2
0.4
0.6
0.8
1.0
1.2
0
2
4
6
8
12
H
Density
Block ARMS for H
H1  
H2  
H3  
H4  
H5  
0.0
0.2
0.4
0.6
0.8
1.0
0
2
4
6
8
12
V1
Density
Mode and curvature matching for V1
V1,5  
V1,19  
V1,32  
V1,99  
0.0
0.2
0.4
0.6
0.8
1.0
1.2
0
2
4
6
8
12
H
Density
Mode and curvature matching for H
H1  
H2  
H3  
H4  
H5  
0.0
0.2
0.4
0.6
0.8
1.0
0
2
4
6
8
12
V1
Density
Expectation and variance matching for V1
V1,5  
V1,19  
V1,32  
V1,99  
0.0
0.2
0.4
0.6
0.8
1.0
1.2
0
2
4
6
8
12
H
Density
Expectation and variance matching for H
H1  
H2  
H3  
H4  
H5  
0.0
0.2
0.4
0.6
0.8
1.0
0
2
4
6
8
12
V1
Density
Independence and random walk samplers for V1
V1,5  
V1,19  
V1,32  
V1,99  
0.0
0.2
0.4
0.6
0.8
1.0
1.2
0
2
4
6
8
12
H
Density
Independence and random walk samplers for H
H1  
H2  
H3  
H4  
H5  
Figure 8.1: Validation of the ﬁve sampling methods of Sections 8.3.2 and 8.3.3. For each
method, 1 000 samples from the full conditional density of the subset of latent variables
V1,5, V1,19, V1,32, V1,99 (left side), and all transformed copula parameters, H1, . . . , H5 (right
side), are drawn. For the evaluation of the conditional densities all conditional values
are set to the true values used in the simulation of the corresponding data set. Target
densities are represented by solid lines, empirical sample density estimates in dotted lines.
84

8.4
Results
8.4
Results
In this section, the results of marginal MLE and the Gibbs sampler are given. Both the
relative performance of each of the ﬁve sampling methods within Gibbs sampling, as well
as the ability of the Gibbs sampler to ﬁnd the correct underlying copula parameters are
examined.
As a reminder, the three intervals of Kendall’s tau values (detailed in Table 8.1) investig-
ated are given by:
1. Low τ: 0.1 ≤τ ≤0.2.
2. High τ: 0.5 ≤τ ≤0.8.
3. Mixed τ: 0.1 ≤τ ≤0.8.
8.4.1
Marginal maximum likelihood estimation
Marginal ML estimation was done for N = 100 data sets from the one-factor model with
Gumbel linking copulas. Each data set has n = 200 observations of dimension d = 5 for
three scenarios of Kendall’s tau values.
For each scenario and each bivariate linking copula Cj,V1, j = 1, . . . , 5, three diﬀerent
statistics are calculated to measure deviation of the MLE from the true value used in the
simulation of the data sets. Both estimated and true copula parameters are transformed
to their corresponding Kendall’s tau values bτ and τ, respectively. The statistics utilized
to measure the diﬀerence between true and estimated values are given in the following.
Let N be the number of data sets that are used for ML estimation.
Let bτij and τij
be the MLE and true value of the parameter of copula Cj,V1, j = 1, . . . , d, of data set
k, k = 1, . . . , N, in Kendall’s tau scale, respectively. Then, the following statistics are
calculated:
1. Mean absolute diﬀerence as the average deviation from the true value
|bτ −τ|mean,j ··= 1
N
N
X
k=1
|bτkj −τkj|.
2. Maximum absolute diﬀerence as the worst-case deviation
|bτ −τ|max,j ··=
max
k=1,...,N |bτkj −τkj|.
3. Mean squared diﬀerence as the average deviation with less weight on low deviations
and more weight on high ones
(bτ −τ)2
mean,j ··= 1
N
N
X
k=1
(bτkj −τkj)2.
85

8
Simulation study
Marginal MLE
C1,V1
C2,V1
C3,V1
C4,V1
C5,V1
Low τ
True τ
0.10
0.12
0.15
0.18
0.20
|bτ −τ|mean
0.07
0.07
0.08
0.08
0.08
|bτ −τ|max
0.66
0.41
0.72
0.46
0.27
(bτ −τ)2
mean
9.9e-03
7.6e-03
1.4e-02
1.2e-02
9.4e-03
High τ
True τ
0.50
0.57
0.65
0.73
0.80
|bτ −τ|mean
0.03
0.03
0.02
0.02
0.02
|bτ −τ|max
0.09
0.08
0.07
0.06
0.05
(bτ −τ)2
mean
1.1e-03
9.7e-04
6.0e-04
4.8e-04
4.3e-04
Mixed τ
True τ
0.10
0.28
0.45
0.62
0.80
|bτ −τ|mean
0.03
0.04
0.03
0.03
0.04
|bτ −τ|max
0.13
0.12
0.12
0.10
0.11
(bτ −τ)2
mean
1.6e-03
2.2e-03
1.3e-03
1.3e-03
2.3e-03
Table 8.2: Results of N = 100 repetitions of marginal MLE for data from a one-factor
copula model with Gumbel linking copulas. Each data set consists of n = 200 observations
of dimension d = 5 for three ranges of Kendall’s tau values.
The results shown in Table 8.2 suggest that the scenario with low Kendall’s tau values
is the most diﬃcult one to estimate. For this scenario, a high estimate for one copula
is paired with estimates that are lower than the true value for all other copulas. This
may indicate that it is hard for the model to distinguish the source of dependence, when
overall dependence in the data is low. While inaccuracies can also be attributed to the
relatively low number of observations, the scenarios of high and mixed values produce
improved results.
Particularly the case of only high values of Kendall’s tau produces
estimates close to the true values most of the time, with the worst-case deviation being
close to the average deviations of the case of low Kendall’s tau values.
8.4.2
Gibbs sampling with Gumbel linking copulas
For each of the ﬁve diﬀerent sampling methods introduced in Section 8.3, a single run
of 3 000 iterations of the Gibbs sampler was done on the ﬁrst data set of Section 8.1 for
all three scenarios of Kendall’s tau values. Starting values were chosen as described in
Section 8.3.1. Although convergence to stationary behavior appeared within less than 100
iterations for the considered data sets, as a precaution, the initial 1 000 iterations were
discarded as burn-in, leading to a ﬁnal posterior sample size of 2 000.
As exemplary trace and density plots of V1, variables V1,3 and V1,189 are shown in Fig-
ures 8.2 and 8.3, respectively. In the scenario of high Kendall’s tau values, good conver-
gence behavior of the posterior samples of V1,3 can be observed. All sampling methods
but block ARMS produce independent variates with stationary distribution close to the
underlying value. Samples from MH with mode and curvature matching are more concen-
trated than those of the other methods, causing the true value to be outside most of the
probability mass of the empirical density, although the absolute diﬀerence of true value
and posterior mode is small. Block ARMS introduces a high amount of autocorrelation
between each iteration, leading to a posterior mode that oﬀers a very inaccurate estimate
86

8.4
Results
and, therefore, unusable sample. In contrast, the posterior sample of V1,189, in the case
of low Kendall’s tau values, shows inadequate convergence behavior for all methods, as
it stuck near the upper bound of the latent variable. This appears to be caused by the
true value being very high with V1,189 ≈0.94. Furthermore, individual ARMS, expect-
ation and variance matching and independence and random walk sampling are heavily
dispersed over the whole interval (0, 1), when not close to the upper bound.
Similarly to the positive example of V1,3 and negative example of V1,189, H1 and H5,
in Kendall’s tau scale τ1 and τ5, are shown for the case of mixed Kendall’s tau values in
Figures 8.4 and 8.5, respectively. As previously, all methods but block ARMS successfully
produce independent variates with posterior modes close to the true values and stationary
distributions for τ1. Block ARMS shows higher autocorrelation than the other methods
with a posterior mode that is far from the value underlying the simulated data.
In
this case, mode and curvature matching is less concentrated than was the case for V1,3,
performing similarly to the other methods. For τ5, all methods except for block ARMS
and mode and curvature matching produce highly autocorrelated variates that are still
close to the true value. Block ARMS is not successful in ﬁnding the underlying copula
parameter, and mode and curvature matching is stuck near the upper bound as was the
case for V1,189.
Additional plots for the subset V1,1, . . . , V1,5 of V1,1, . . . , V1,200 and H1, . . . , H5 for all ﬁve
sampling methods and three scenarios of Kendall’s tau values are shown in Appendix A.1,
Figures A.1–A.36. Additionally, boxplots for all latent variables V1,1, . . . , V1,200 are given
in Appendix A.2, Figures A.41–A.58.
Another important factor is the speed with which each method can generate random
variates. Since the autocorrelation of the posterior sample may vary, eﬀective sample size
is used to gauge the number of independent samples. For this purpose, often one eﬀective
sample is viewed as one independent one. Therefore, as a statistic for the sampling speed
of each method, eﬀective sample sizes per minute are calculated based on the runtime
of the Gibbs sampler. Note that eﬀective sample size is not invariant under the copula
parameter, Kendall’s tau and Fisher z-transformation scales and minor variations may
occur. For the following, the transformed H scale is used for calculations.
The results shown Table 8.3 demonstrate that MH with independence and random walk
samplers and individual ARMS are the most eﬃcient sampling methods. As can be seen
in Table 8.5, individual ARMS is slower in absolute terms due to the higher number of
function evaluations needed, but eﬀective sample sizes are higher, causing it to be only
slightly less eﬃcient overall. Mode and curvature matching oﬀers middling performance
due to the numerical optimization, root ﬁnding and integration that are done in each
iteration of every variable. Expectation and variance matching is sensitive to the explicit
numerical integration routine used and care has to be taken to strike the right balance
between speed and accuracy. In this simulation, the three necessary numerical integrations
slow down the method considerably. Block ARMS is the least eﬃcient method due to
the complexity of the likelihood function that is evaluated and the high dimensional
exponential distribution samples are drawn from, further discouraging its use.
All of
these calculations were run on a i5-3570K CPU at 3.4 GHz, parallelized on 3 cores with
the doParallel (see Analytics and Weston, 2014) R package.
In general, it can be observed in Table 8.4 that there is a huge diﬀerence between eﬀective
sample sizes of variables within the same method and scenario. While on average the
87

8
Simulation study
ESS/min.
Ind. ARMS
Block ARMS
Mode/Curv.
Exp./Var.
Indep./Walk
Low τ
H1
42.8
6.8
65.1
3.4
80.7
H2
37.3
9.5
61.8
2.4
155.3
H3
21.9
8.9
97.9
3.5
251.3
H4
7.1
8.9
2.9
0.9
8.7
H5
18.4
10.6
83
2.8
152.9
High τ
H1
482.4
3.8
70.7
43.8
484.1
H2
437
0.6
87.8
41.2
487.6
H3
301.3
2.4
95.7
35.6
251.7
H4
294.4
1.4
88.7
27.6
239.9
H5
89.1
0.9
2.2
11.6
91.7
Mixed τ
H1
621.4
11
89.9
39.5
1007.8
H2
446.9
2.6
94.4
35.2
540
H3
105.9
0.5
86.6
11.8
448.2
H4
36.9
1
90.6
1.9
55.2
H5
11
0.4
2.8
0.3
11.3
Table 8.3:
Eﬀective sample sizes per minute (ESS/min.)
of the output of a single
run of the Gibbs sampler based on simulated data from a one-factor copula model with
Gumbel linking copulas with n = 200 observations of dimension d = 5.
The Gibbs
sampler was run for 3 000 iterations and the ﬁrst 1 000 iterations discarded as burn-in.
From left to right the methods are: Individual ARMS (ind. ARMS), block ARMS, mode
and curvature matching (Mode/Curv.), expectation and variance matching (Exp./Var.),
and independence and random walk samplers (Indep./Walk). The highest value for each
scenario and parameter are emphasized in bold face.
posterior samples show an acceptable amount of autocorrelation, there exists a small
amount of variables that are heavily autocorrelated.
A complete table of calculated acceptance rates and eﬀective sample sizes for all methods
and scenarios is given in Appendix A.3, Table A.1.
88

8.4
Results
0
500
1000
1500
2000
0.3
0.5
0.7
0.9
Iteration
V1,3
Individual ARMS for V1,3
True value
Posterior mode
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
0
5
15
25
V1,3
Density
Individual ARMS for V1,3
True value
Posterior mode
0
500
1000
1500
2000
0.3
0.5
0.7
0.9
Iteration
V1,3
Block ARMS for V1,3
True value
Posterior mode
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
0
5
15
25
V1,3
Density
Block ARMS for V1,3
True value
Posterior mode
0
500
1000
1500
2000
0.3
0.5
0.7
0.9
Iteration
V1,3
Mode and curvature matching for V1,3
True value
Posterior mode
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
0
5
15
25
V1,3
Density
Mode and curvature matching for V1,3
True value
Posterior mode
0
500
1000
1500
2000
0.3
0.5
0.7
0.9
Iteration
V1,3
Expectation and variance matching for V1,3
True value
Posterior mode
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
0
5
15
25
V1,3
Density
Expectation and variance matching for V1,3
True value
Posterior mode
0
500
1000
1500
2000
0.3
0.5
0.7
0.9
Iteration
V1,3
Independence and random walk samplers for V1,3
True value
Posterior mode
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
0
5
15
25
V1,3
Density
Independence and random walk samplers for V1,3
True value
Posterior mode
Figure 8.2: Trace and density plots of a single run of the Gibbs sampler with ﬁve diﬀerent
sampling methods, based on simulated data from a one-factor copula model with Gumbel
linking copulas with n = 200 observations of dimension d = 5. As a positive example for
V1 = (V1,1, . . . , V1,200)⊤, the posterior sample of V1,3 for high Kendall’s tau values is shown.
The Gibbs sampler was run for 3 000 iterations and the ﬁrst 1 000 iterations discarded as
burn-in.
89

8
Simulation study
0
500
1000
1500
2000
0.0
0.4
0.8
Iteration
V1,189
Individual ARMS for V1,189
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
1.0
0
5
15
25
V1,189
Density
Individual ARMS for V1,189
True value
Posterior mode
0
500
1000
1500
2000
0.0
0.4
0.8
Iteration
V1,189
Block ARMS for V1,189
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
1.0
0
5
15
25
V1,189
Density
Block ARMS for V1,189
True value
Posterior mode
0
500
1000
1500
2000
0.0
0.4
0.8
Iteration
V1,189
Mode and curvature matching for V1,189
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
1.0
0
5
15
25
V1,189
Density
Mode and curvature matching for V1,189
True value
Posterior mode
0
500
1000
1500
2000
0.0
0.4
0.8
Iteration
V1,189
Expectation and variance matching for V1,189
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
1.0
0
5
15
25
V1,189
Density
Expectation and variance matching for V1,189
True value
Posterior mode
0
500
1000
1500
2000
0.0
0.4
0.8
Iteration
V1,189
Independence and random walk samplers for V1,189
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
1.0
0
5
15
25
V1,189
Density
Independence and random walk samplers for V1,189
True value
Posterior mode
Figure 8.3: Trace and density plots of a single run of the Gibbs sampler with ﬁve diﬀerent
sampling methods based on simulated data from a one-factor copula model with Gumbel
linking copulas with n = 200 observations of dimension d = 5. As a negative example
for V1 = (V1,1, . . . , V1,200)⊤, the posterior sample of V1,189 for low Kendall’s tau values
is shown. The Gibbs sampler was run for 3 000 iterations and the ﬁrst 1 000 iterations
discarded as burn-in.
90

8.4
Results
0
500
1000
1500
2000
0.00
0.10
0.20
0.30
Iteration
τ1
Individual ARMS for τ1
True value
Posterior mode
0.00
0.05
0.10
0.15
0.20
0.25
0.30
0
2
4
6
8
12
τ1
Density
Individual ARMS for τ1
True value
Posterior mode
0
500
1000
1500
2000
0.00
0.10
0.20
0.30
Iteration
τ1
Block ARMS for τ1
True value
Posterior mode
0.00
0.05
0.10
0.15
0.20
0.25
0.30
0
2
4
6
8
12
τ1
Density
Block ARMS for τ1
True value
Posterior mode
0
500
1000
1500
2000
0.00
0.10
0.20
0.30
Iteration
τ1
Mode and curvature matching for τ1
True value
Posterior mode
0.00
0.05
0.10
0.15
0.20
0.25
0.30
0
2
4
6
8
12
τ1
Density
Mode and curvature matching for τ1
True value
Posterior mode
0
500
1000
1500
2000
0.00
0.10
0.20
0.30
Iteration
τ1
Expectation and variance matching for τ1
True value
Posterior mode
0.00
0.05
0.10
0.15
0.20
0.25
0.30
0
2
4
6
8
12
τ1
Density
Expectation and variance matching for τ1
True value
Posterior mode
0
500
1000
1500
2000
0.00
0.10
0.20
0.30
Iteration
τ1
Independence and random walk samplers for τ1
True value
Posterior mode
0.00
0.05
0.10
0.15
0.20
0.25
0.30
0
2
4
6
8
12
τ1
Density
Independence and random walk samplers for τ1
True value
Posterior mode
Figure 8.4: Trace and density plots of a single run with ﬁve diﬀerent sampling methods of
the Gibbs sampler based on simulated data from a one-factor copula model with Gumbel
linking copulas with n = 200 observations of dimension d = 5. As a positive example
for τ = (τ1, . . . , τ5), the posterior sample of τ1 for mixed Kendall’s tau values is shown.
The Gibbs sampler was run for 3 000 iterations and the ﬁrst 1 000 iterations discarded as
burn-in.
91

8
Simulation study
0
500
1000
1500
2000
0.2
0.4
0.6
0.8
1.0
Iteration
τ5
Individual ARMS for τ5
True value
Posterior mode
0.2
0.4
0.6
0.8
1.0
0
5
15
25
τ5
Density
Individual ARMS for τ5
True value
Posterior mode
0
500
1000
1500
2000
0.2
0.4
0.6
0.8
1.0
Iteration
τ5
Block ARMS for τ5
True value
Posterior mode
0.2
0.4
0.6
0.8
1.0
0
5
15
25
τ5
Density
Block ARMS for τ5
True value
Posterior mode
0
500
1000
1500
2000
0.2
0.4
0.6
0.8
1.0
Iteration
τ5
Mode and curvature matching for τ5
True value
Posterior mode
0.2
0.4
0.6
0.8
1.0
0
5
15
25
τ5
Density
Mode and curvature matching for τ5
True value
Posterior mode
0
500
1000
1500
2000
0.2
0.4
0.6
0.8
1.0
Iteration
τ5
Expectation and variance matching for τ5
True value
Posterior mode
0.2
0.4
0.6
0.8
1.0
0
5
15
25
τ5
Density
Expectation and variance matching for τ5
True value
Posterior mode
0
500
1000
1500
2000
0.2
0.4
0.6
0.8
1.0
Iteration
τ5
Independence and random walk samplers for τ5
True value
Posterior mode
0.2
0.4
0.6
0.8
1.0
0
5
15
25
τ5
Density
Independence and random walk samplers for τ5
True value
Posterior mode
Figure 8.5: Trace and density plots of a single run with ﬁve diﬀerent sampling methods of
the Gibbs sampler based on simulated data from a one-factor copula model with Gumbel
linking copulas with n = 200 observations of dimension d = 5. As a negative example
for τ = (τ1, . . . , τ5), the posterior sample of τ5 for mixed Kendall’s tau values is shown.
The Gibbs sampler was run for 3 000 iterations and the ﬁrst 1 000 iterations discarded as
burn-in.
92

8.4
Results
ESS
Ind. ARMS
Block ARMS
Mode/Curv.
Exp./Var.
Indep./Walk
Low τ
H1
122
170
1330
120
43
H2
106
240
1263
83
83
H3
62
225
2000
121
135
H4
20
224
60
32
5
H5
52
266
1696
98
82
High τ
H1
1460
97
1375
1018
258
H2
1323
15
1709
956
260
H3
912
60
1861
828
134
H4
891
36
1726
641
128
H5
270
22
43
269
49
Mixed τ
H1
1850
278
1701
978
546
H2
1330
67
1788
872
292
H3
315
11
1640
293
243
H4
110
25
1716
48
30
H5
33
9
53
7
6
Table 8.4:
Eﬀective sample sizes (ESS) of the output of a single run of the Gibbs
sampler based on simulated data from a one-factor copula model with Gumbel linking
copulas with n = 200 observations of dimension d = 5. The Gibbs sampler was run
for 3 000 iterations and the ﬁrst 1 000 iterations discarded as burn-in. From left to right
the methods are: Individual ARMS (ind. ARMS), block ARMS, mode and curvature
matching (Mode/Curv.), expectation and variance matching (Exp./Var.), and independ-
ence and random walk samplers (Indep./Walk). The highest value for each scenario and
parameter are emphasized in bold face.
Ind. ARMS
Block ARMS
Mode/Curv.
Exp./Var.
Indep./Walk
Relative runtime
5.49
47.01
36.49
51.42
1.00
Acceptance rate
100%
99.75%
91.98%
92.85%
26.84%
Table 8.5: Average relative runtime and acceptance rates over the three scenarios of
Kendall’s tau values of the output of a single run of the Gibbs sampler based on sim-
ulated data from a one-factor copula model with Gumbel linking copulas with n = 200
observations of dimension d = 5. The Gibbs sampler was run for 3 000 iterations and the
ﬁrst 1 000 iterations discarded as burn-in. From left to right the methods are: Individual
ARMS (ind. ARMS), block ARMS, mode and curvature matching (Mode/Curv.), expect-
ation and variance matching (Exp./Var.), and independence and random walk samplers
(Indep./Walk).
93

8
Simulation study
Acceptance rate
Eﬀ. sample size
V1
H
V1
H
Low
Min
17%
100%
310
349
Median
100%
100%
797
2000
Max
100%
100%
11370
2127
High
Min
100%
100%
1490
1825
Median
100%
100%
2000
2000
Max
100%
100%
2969
2141
Mixed
Min
100%
100%
1335
1129
Median
100%
100%
2000
2000
Max
100%
100%
3636
2349
H1
H2
H3
H4
H5
Low
AR
100%
100%
100%
100%
100%
ESS
2000
2000
2127
349
1638
ESS/min
4
4
4
1
3
High
AR
100%
100%
100%
100%
100%
ESS
2000
2000
1825
2141
1914
ESS/min
3
3
3
4
3
Mixed
AR
100%
100%
100%
100%
100%
ESS
2349
2095
2000
2000
1129
ESS/min
4
4
3
3
2
Table 8.6: Summary of 2 000 iterations of one run of the thinned (thin=300) individual
ARMS Gibbs sampler output for three scenarios for Kendall’s tau values.
Left side:
Summary of minimum, median and maximum acceptance rates and eﬀective sample sizes
of V1 and H. Right side: Acceptance rate (AR), eﬀective sample size (ESS) and eﬀective
sample size per minute (ESS/min) of H1, . . . , H5 individually. Since the output is thinned,
a rejection indicates at least 300 consecutively rejected samples.
8.4.3
Thinned individual ARMS
Analysis of the posterior samples in Section 8.4.2 indicates a high amount of autocor-
relation for some variables. Therefore, in order to assess the eﬀect of thinning, Gibbs
sampling was repeated on the previous data set with the individual ARMS method.
A total of 601 000 samples were generated, where the ﬁrst 1 000 were discarded as burn-in,
as previously. The remaining 600 000 iterations are thinned such that only every 300-th
observation was used and all others discarded, resulting in a ﬁnal sample of size 2 000.
This high thinning factor was chosen because the worst-case eﬀective sample size was very
low and corresponding autocorrelation plot revealed noticeable amounts of autocorrelation
for lags > 100.
Exemplary trace and density plots of V1,3, V1,189, and H1 and H5 in Kendall’s tau scale
of τ1 and τ5, respectively, with corresponding scenario of Section 8.4.2, are shown in
Figure 8.6. It can be seen that thinning successfully improves the issue of autocorrelation
of τ5, though V1,189 still remains stuck to its upper bound. This implies at least a partial
beneﬁt of thinning which is reinforced by the full data of eﬀective sample sizes given by
Table 8.6. After thinning, even the worst-case eﬀective sample size is at an acceptable
level. The obvious downside is the vastly increased runtime of the Gibbs sampler, reﬂected
by the low eﬀective sample sizes per minute. Note that eﬀective sample sizes above 2 000
are caused by high amounts of negative correlation, resulting in the sample size being
divided by a factor less than 1 (see Deﬁnition 3.14).
94

8.4
Results
0
500
1000
1500
2000
0.0
0.4
0.8
Iteration
V1,3
Thinned individual ARMS for V1,3
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
1.0
0
5
15
25
V1,3
Density
Thinned individual ARMS for V1,3
True value
Posterior mode
0
500
1000
1500
2000
0.0
0.4
0.8
Iteration
V1,189
Thinned individual ARMS for V1,189
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
1.0
0
5
15
25
V1,189
Density
Thinned individual ARMS for V1,189
True value
Posterior mode
0
500
1000
1500
2000
0.0
0.4
0.8
Iteration
τ1
Thinned individual ARMS for τ1
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
1.0
0
5
15
25
τ1
Density
Thinned individual ARMS for τ1
True value
Posterior mode
0
500
1000
1500
2000
0.0
0.4
0.8
Iteration
τ5
Thinned individual ARMS for τ5
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
1.0
0
5
15
25
τ5
Density
Thinned individual ARMS for τ5
True value
Posterior mode
Figure 8.6: Trace and density plots of thinned individual ARMS of the Gibbs sampler
based on simulated data from a one-factor copula model with Gumbel linking copulas
with n = 200 observations of dimension d = 5. V1,3 for high Kendall’s tau values, V1,189
for low Kendall’s tau values, and τ1 and τ5 for mixed Kendall’s tau values are shown.
The Gibbs sampler was run for 600 000 iterations, the ﬁrst 1 000 iterations discarded as
burn-in and the remaining iterations thinned with factor 300 for a ﬁnal sample size of
2 000.
95

8
Simulation study
Mean squared error
C1,V1
C2,V1
C3,V1
C4,V1
C5,V1
Low τ
Marginal MLE
9.9e-03
7.6e-03
1.4e-02
1.2e-02
9.4e-03
Ind. ARMS
4.7e-03
5.2e-03
1.1e-02
7.9e-03
1.8e-02
Mode/Curv.
7.3e-02
9.3e-02
1.4e-01
1.5e-01
2.4e-01
Ind./Walk
4.6e-03
1.8e-02
6.2e-03
1.1e-02
1.6e-02
High τ
Marginal MLE
1.1e-03
9.7e-04
6.0e-04
4.8e-04
4.3e-04
Ind. ARMS
1.1e-03
9.5e-04
5.9e-04
5.3e-04
3.6e-04
Mode/Curv.
1.6e-03
2.3e-03
2.8e-03
4.9e-03
3.2e-02
Ind./Walk
1.1e-03
1.1e-03
7.1e-04
5.4e-04
4.4e-04
Mixed τ
Marginal MLE
1.6e-03
2.2e-03
1.3e-03
1.3e-03
2.3e-03
Ind. ARMS
1.6e-03
2.0e-03
1.4e-03
1.6e-03
9.5e-03
Mode/Curv.
1.4e-03
2.2e-03
2.2e-03
1.6e-02
3.4e-02
Ind./Walk
1.7e-03
2.4e-03
1.4e-03
1.4e-03
5.2e-03
Table 8.7: Mean squared error ((bτ −τ)2
mean) of N = 100 repetitions of marginal MLE
and posterior modes of the Gibbs sampler with individual ARMS (Ind. ARMS), mode
and curvature matching (Mode/Curv.), and independence and random walk sampler
(Ind./Walk). Each data set consists of n = 200 observations of dimension d = 5 from a
one-factor copula model with Gumbel linking copulas. The lowest values of each copula
and scenario are emphasized in bold face.
8.4.4
Posterior mode estimation
Since the observations of Section 8.4.2 revealed individual ARMS, MH with mode and
curvature matching, and MH with independence and random walk sampler to be the most
promising methods, the Gibbs sampling routine was repeated for all N = 100 data sets.
Analogously to the marginal maximum likelihood estimation of Section 8.4.1, statistics
for the quality of the posterior mode estimates of the copula parameters are calculated.
Table 8.7 shows that the mean squared errors of the marginal MLE and all three Gibbs
sampling methods are nearly identical, with individual ARMS achieving the lowest errors
by a slim margin. This validates the ability of the Gibbs sampling routine to correctly
ﬁnd the underlying copula parameters.
All results of the posterior mode estimation can be found in Table A.3.
8.4.5
Credible intervals for the latent variable
In addition to the samples of the copula parameters, Bayesian analysis of the one-factor
copula model also provides variates for latent variable V1. Although the states of the latent
variable are generally of secondary importance, they enable the analysis of the assumed
underlying factor over time. Therefore, the ability of the Gibbs sampling routine to ﬁnd
the correct latent variables V1 of the simulated data sets is assessed. This is facilitated
through construction of empirical 95% credible intervals, symmetric around the posterior
median, of the samples of V1 for all N = 100 data sets, three scenarios of Kendall’s tau
values and three sampling methods of Section 8.4.4. The formal deﬁnitions and results
are given in the following.
96

8.4
Results
Let vk
1 = (vk
1,1, . . . , vk
1,n)⊤be a posterior sample of V1 = (V1,1, . . . , V1,n)⊤based on data set
k, k = 1, . . . , N, and let V k
1,i, i = 1, . . . , n be the true value of V1,i used in the simulation of
the data set. Finally, let bq−1
α
be the empirical quantile function at level α ∈(0, 1). Deﬁne
bV CIα
1
(vk
1) ··= 1
n
n
X
i=1
1
bq−1
1−α
2
(vk
1,i)≤V k
1,i≤bq−1
1+α
2
(vk
1,i)
	,
i.e., bV CIα
1
(vk
1) is the percentage of true values V k
1,1, . . . , V k
1,n within the corresponding em-
pirical α quantiles of samples vk
1,1, . . . , vk
1,n of data set k.
Then, the following statistics are calculated:
1. Minimal percentage of true values within an α credible interval centered around the
empirical median
bV CIα
1,min ··=
min
k=1,...,N
bV CIα
1
(vk
1)
	
.
2. Median percentage of true values within an α credible interval centered around the
empirical median
bV CIα
1,median ··= median
k=1,...,N
bV CIα
1
(vk
1)
	
.
3. Maximal percentage of true values within an α credible interval centered around the
empirical median
bV CIα
1,max ··=
max
k=1,...,N
bV CIα
1
(vk
1)
	
.
The results in Table 8.8 show that individual ARMS and MH with independence and
random walk samplers oﬀer similar performance, with the median close to the theoretical
value of 95% in all three scenarios. When it comes to worst-case performance, the inde-
pendence and random walk sampling method has a moderate lead. In contrast, mode and
curvature matching frequently fails to accurately identify the correct latent variable due
to concentration of the posterior sample to smaller intervals that do not contain the true
value of the corresponding latent variable.
97

8
Simulation study
Credible intervals
Ind. ARMS
Mode/Curv.
Ind./Walk
Low τ
bV CI0.95
1,min
10.00%
1.50%
22.00%
bV CI0.95
1,median
92.75%
5.00%
93.00%
bV CI0.95
1,max
97.50%
8.50%
97.00%
High τ
bV CI0.95
1,min
89.00%
12.50%
88.00%
bV CI0.95
1,median
94.50%
20.50%
93.50%
bV CI0.95
1,max
98.00%
86.50%
97.00%
Mixed τ
bV CI0.95
1,min
20.00%
8.00%
38.00%
bV CI0.95
1,median
91.00%
19.00%
92.25%
bV CI0.95
1,max
99.00%
27.50%
98.50%
Table 8.8: Percentages of true values of latent variable V1 within an empirical 95%
credible interval, symmetric around the median, of the posterior sample of the Gibbs
sampler. Shown are the minimum (bV CI0.95
1,min ), median (bV CI0.95
1,median) and maximum (bV CI0.95
1,max )
percentage for N = 100 data sets, three ranges of Kendall’s tau values and three methods
of Gibbs sampling.
Percentages that are closest to the theoretical value of 95% are
emphasized in bold face. Deﬁnitions of the statistics are given Section 8.4.5.
98

8.4
Results
Low τ
C1,V1
C2,V1
C3,V1
C4,V1
C5,V1
τ
0.10
0.12
0.15
0.18
0.20
θGaussian
0.16
0.20
0.23
0.27
0.31
θs.Gumbel
1.11
1.14
1.18
1.21
1.25
η
0.10
0.13
0.15
0.18
0.20
High τ
C1,V1
C2,V1
C3,V1
C4,V1
C5,V1
τ
0.50
0.57
0.65
0.73
0.80
θGaussian
0.71
0.79
0.85
0.91
0.95
θs.Gumbel
2.00
2.35
2.86
3.64
5.00
η
0.55
0.65
0.78
0.92
1.10
Mixed τ
C1,V1
C2,V1
C3,V1
C4,V1
C5,V1
τ
0.10
0.28
0.45
0.62
0.80
θGaussian
0.16
0.42
0.65
0.83
0.95
θs.Gumbel
1.11
1.38
1.82
2.67
5.00
η
0.10
0.28
0.48
0.73
1.10
Table 8.9: Parameters for the simulation of one-factor copula data with bivariate Gaus-
sian and survival Gumbel linking copulas for three scenarios of Kendall’s tau values with
n = 200 observations of dimension d = 5 Parameters are given in three scales: τ: Kendall’s
tau, θs.Gumbel: bivariate survival Gumbel copula parameter, θGaussian: bivariate Gaussian
copula parameter, and η: Fisher z transform of τ.
8.4.6
Gibbs sampling with Gaussian and survival Gumbel linking copulas
As a veriﬁcation of the Gibbs sampling routine for additional choices of bivariate linking
copulas, one data set with n = 200 observations of dimension d = 5 was generated from
each of the one-factor copula model with Gaussian and survival Gumbel linking copulas
for all three scenarios of Kendall’s tau values. The corresponding copula parameters in
Kendall’s tau, parameter and Fisher z-transformation scales are given in Table 8.9. Note
that the copula parameters for the bivariate Gumbel and survival Gumbel are the same.
Afterwards, these data sets were used to run the individual ARMS Gibbs sampler for
3 000 iterations where, as previously, the ﬁrst 1 000 iterations are discarded as burn-in.
Exemplary trace and density plots in Figure 8.7 show that the Gibbs sampler works cor-
rectly with bivariate Gumbel and survival Gumbel copulas. This suggests that arbitrary
choices of linking copulas should be feasible.
Additional trace and density plots can be found in Appendix A.1, Figures A.37–A.40.
Acceptance rate and eﬀective sample sizes are given in Table A.2.
99

8
Simulation study
0
500
1000
1500
2000
0.0
0.4
0.8
Iteration
V1,1
Individual ARMS with Gaussian copulas for V1,1
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
0
2
4
6
8
V1,1
Density
Individual ARMS with Gaussian copulas for V1,1
True value
Posterior mode
0
500
1000
1500
2000
0.0
0.4
0.8
Iteration
τ1
Individual ARMS with Gaussian copulas for τ1
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
0
2
4
6
8
τ1
Density
Individual ARMS with Gaussian copulas for τ1
True value
Posterior mode
0
500
1000
1500
2000
0.0
0.4
0.8
Iteration
V1,1
Individual ARMS with survival Gumbel copulas for V1,1
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
0
2
4
6
8
V1,1
Density
Individual ARMS with survival Gumbel copulas for V1,1
True value
Posterior mode
0
500
1000
1500
2000
0.0
0.4
0.8
Iteration
τ1
Individual ARMS with survival Gumbel copulas for τ1
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
0
2
4
6
8
τ1
Density
Individual ARMS with survival Gumbel copulas for τ1
True value
Posterior mode
Figure 8.7: Trace and density plots a single run of the individual ARMS Gibbs sampler
based on simulated data from a one-factor copula model with Gaussian and survival
Gumbel linking copulas with n = 200 observations of dimension d = 5. As examples for
V1 and H, V1,1 and τ1 are shown for mixed Kendall’s tau values. The Gibbs sampler was
run for 3 000 iterations and the ﬁrst 1 000 iterations discarded as burn-in.
100

8.5
Conclusions
8.5
Conclusions
The results of the previous sections demonstrate that Bayesian analysis of the one-factor
copula model with the proposed Gibbs sampler methods is fruitful.
Based on the comparison of the posterior mode estimates of the copula parameters to their
marginal MLE, both perform similarly well in this study, with a narrow lead achieved by
the Gibbs sampler with individual ARMS. This slight gain is oﬀset by the vastly increased
computational eﬀort of Bayesian inference. Nevertheless, the Gibbs sampler also enables
the investigation of latent variable V1, which is the underlying factor assumed by the
one-factor copula model.
Some parameters exhibit a high amount of autocorrelation in their corresponding posterior
samples.
In the case of the copula parameters, this phenomenon can successfully be
addressed by the use of thinning. In the case of latent factors that are very close to either
0 or 1, all Gibbs sampling models are largely unable to produce appropriate results.
However, since these situations are exceedingly rare, the capability of the Gibbs sampler
to correctly ﬁnd the underlying latent states is mostly undiminished, demonstrating little
practical importance.
Examining the diﬀerences of the ﬁve proposed sampling methods, MH with independence
and random walk samplers performed best in this simulation study. This method oﬀers
the highest sampling eﬃciency in most cases due to its simple structure, coupled with
the most accurate detection of the underlying latent variables. Its only downside is the
relatively low acceptance rate compared to the other methods, suggesting that it is best
used in conjunction with thinning.
Individual ARMS performed comparably well, surpassing independence and random walk
sampling in some scenarios. Its computational complexity strikes a balance between the
numerical integration and optimizations required by mode and curvature and expectation
and variance matching, and the straightforward structure of the independence sampler.
The main advantage of ARMS is in its generality. In contrast to all other methods, no
manual tuning of proposal distributions is required, assisting swift and general imple-
mentations.
Each of the remaining three methods has a corresponding noticeable weakness. Mode
and curvature matching suﬀers from highly concentrated posterior samples that fail to
explore the support suﬃciently, and therefore often suggests unreliable parameter choices.
Expectation and variance matching has the longest runtime due to the use of several nu-
merical integrations, but does not generally improve the generated sample compared to,
e.g., individual ARMS. Block ARMS struggles to correctly sample from the high dimen-
sional density of the latent variable, producing unusable output. A potential remedy for
this deﬁciency would be dividing V1 into several smaller blocks, which is not investigated
in the scope of this thesis.
Finally, due to its generality and good performance in all areas that were investigated,
individual ARMS is used for the empirical study of Section 9.
101

9
Empirical study
9
Empirical study
In this section, all of the previously introduced models and concepts are applied to an
empirical data set. The data consists of daily log returns of eight European bank stocks
and two indices from years 2004 to 2013. These returns are used to both validate the
ﬁndings for the Gibbs sampling routine for the one-factor copula model of Section 8, and
to compare the performance of the vine copula and one- and two-factor copula dependence
models. Furthermore, by utilizing marginal DLMs, fully Bayesian forecasts for VaR and
ES are explored.
Section 9.1 introduces the historical data set in full detail. Performance of the ARMA-
GARCH and DLM marginal models is discussed in Section 9.2. In Section 9.3 ﬁts of the
vine copula and factor copula dependence models are examined and compared. Bayesian
inference of the one-factor copula model is done in Section 9.4, and the behavior of the
latent variable explored. Finally, Bayesian forecasts of VaR and ES are investigated in
Section 9.5.
9.1
Historical data
Historical stock returns are used as the underlying data set in the following sections.
The main part of the data set comprises daily log returns in EUR of eight banks within
the European Union. Furthermore, data from two nested indices that contain all of the
bank stocks are added, enabling the comparison of factor copula models that assume an
unobserved underlying variable to vine copula models, where the index returns take the
role of observed common variables. Table 9.1 gives an overview over the included stocks
and indices. A detailed discussion of the data set is given in the following.
Raw stock data was downloaded from http://finance.yahoo.com with the help of the
get.hist.quote function of R package tseries (see Trapletti and Hornik, 2015). Daily
adjusted close prices (,,AdjClose”) that are adjusted for dividends, splits, etc., are used
as quotes to be able to calculate meaningful daily close-to-close log returns. The eight
bank stocks are chosen such that both economically strong countries are more heavily
represented, and at the same time a single-country bias is avoided. The corresponding
tickers are: ACA.PA, BBVA.MC, BNP.PA, CBK.DE, DBK.DE, GLE.PA, ISP.MI and
SAN.MC.
As overarching indices, STOXX Europe 600 and its sub-index STOXX Europe 600 Banks
in EUR gross return are incorporated. Quotes for these indices are freely available at the
STOXX homepage (http://www.stoxx.com).
The ten-year horizon of the time series was set to start at 2004-01-01 and end at 2013-
12-31. Since the data set includes stocks from several European countries, trading days
may diﬀer due to varying bank holidays for each exchange. Therefore, the union of all
available dates is calculated, where for stocks that are not traded at a given day, the
asset value of the previous day is reused. This method introduces some additional zero
returns into the data, but avoids altering actual historical returns which could happen by
choosing, e.g., the intersection of all available dates. Applying this principle, a total of
2 607 daily observations of dimension 10 comprising the price of each stock or index were
downloaded.
Finally, for each series of stock or index data (St)t=1,...,2607, log returns Rt are calculated
102

9.1
Historical data
Indices
Symbol
Index name
Index type
No. of components
SXXP
STOXX Europe 600
EUR gross return
600
SX7P
STOXX Europe 600 Banks
EUR gross return
47
Bank stocks
Ticker
Company name
Exchange
ACA.PA
Credit Agricole S.A.
Euronext - Paris
BBVA.MC
Banco Bilbao Vizcaya Argentaria
Madrid Stock Exchange
BNP.PA
BNP Paribas SA
Euronext - Paris
CBK.DE
Commerzbank AG
XETRA
DBK.DE
Deutsche Bank AG
XETRA
GLE.PA
Societe Generale Group
Euronext - Paris
ISP.MI
Intesa Sanpaolo S.p.A.
Borsa Italiana
SAN.MC
Banco Santander
Madrid Stock Exchange
Table 9.1: Overview over included stocks and indices. Top: Symbol, index name, index
type and number of components of nested indices. Bottom: Ticker, company name and
corresponding exchange of bank stocks.
2004: 261
2005: 260
2006: 260
2007: 261
2008: 262
2009: 261
2010: 261
2011: 258
2012: 261
2013: 261
Table 9.2: Number of daily log return observations per year.
by
Rt = ln
St+1
St

,
t = 1, . . . , 2 606.
The number of daily log return observations of each individual year are given in Table 9.2.
In the remainder, the data set will be ordered in the following way: First indices SXXP
and SX7P are shown, then the eight bank stocks follow in alphabetical order, abbreviated
by the ticker names, where the exchange identiﬁer is omitted. In the context of the factor
copula models of Sections 9.3.5 and 9.3.6, stocks are identiﬁed with variables in the same
alphabetical order, i.e., U1 corresponds to ACA.PA, U2 to BBVA.MC, U3 to BNP.PA, U4
to CBK.DE, U5 to DBK.DE, U6 to GLE.PA, U7 to ISP.MI and U8 to SAN.MC.
Plots of the daily log returns time series are shown in Figure 9.1.
103

9
Empirical study
2004
2006
2008
2010
2012
2014
−0.2
−0.1
0.0
0.1
0.2
Time
Daily log return
Daily log returns of SXXP
2004
2006
2008
2010
2012
2014
−0.2
−0.1
0.0
0.1
0.2
Time
Daily log return
Daily log returns of SX7P
2004
2006
2008
2010
2012
2014
−0.2
−0.1
0.0
0.1
0.2
Time
Daily log return
Daily log returns of ACA
2004
2006
2008
2010
2012
2014
−0.2
−0.1
0.0
0.1
0.2
Time
Daily log return
Daily log returns of BBVA
2004
2006
2008
2010
2012
2014
−0.2
−0.1
0.0
0.1
0.2
Time
Daily log return
Daily log returns of BNP
2004
2006
2008
2010
2012
2014
−0.2
−0.1
0.0
0.1
0.2
Time
Daily log return
Daily log returns of CBK
2004
2006
2008
2010
2012
2014
−0.2
−0.1
0.0
0.1
0.2
Time
Daily log return
Daily log returns of DBK
2004
2006
2008
2010
2012
2014
−0.2
−0.1
0.0
0.1
0.2
Time
Daily log return
Daily log returns of GLE
2004
2006
2008
2010
2012
2014
−0.2
−0.1
0.0
0.1
0.2
Time
Daily log return
Daily log returns of ISP
2004
2006
2008
2010
2012
2014
−0.2
−0.1
0.0
0.1
0.2
Time
Daily log return
Daily log returns of SAN
Figure 9.1: Daily log returns of two indices and eight bank stocks in the ten-year period
of 2004-01-01 to 2013-12-31. For each time series there are 2 606 observations.
104

9.2
Marginal models
9.2
Marginal models
Analysis of dependence with copulas requires i.i.d data with U(0, 1) marginals. Since the
time series data of daily log returns of Section 9.1 consists of real values that may be
autocorrelated, the models of Section 5 are applied to each margin of the corresponding
index or stock to transform the returns to copula data.
9.2.1
In-sample ARMA-GARCH
In order to be able to compare ARMA-GARCH to DLM models, out-of-sample copula
data is required for both choices of marginal model. In this context, out-of-sample indic-
ates the transformation of a new data point with the one-day ahead forecast distribution
based on the previous data points. In contrast, in-sample includes the whole data set to
estimate parameters, and afterwards, using these parameters, ﬁltered residuals are trans-
formed. In practice, out-of-sample forecasts are required to perform meaningful backtests,
since new observations are gathered over time and cannot be incorporated into the model
before they occur.
For this ﬁnancial application, daily log returns have to be transformed with the forecast
distribution of the next trading day. While out-of-sample forecasts arise naturally from the
deﬁnition of the variance discounting DLM, they are more involved for ARMA-GARCH
models. Each one-day ahead forecast of the ARMA-GARCH model that utilities the same
underlying data as the DLM, requires an individual estimation of all parameters. Due
to the huge amount of computational eﬀort of this procedure, a preliminary in-sample
analysis of ARMA(1, 1)-GARCH(1, 1) ﬁts of the model is done in this section, and the
results later applied to generate out-of-sample forecasts and, ﬁnally, copula data.
The distributions of Section 5.1.3 are considered for the ARMA(1, 1)-GARCH(1, 1) in-
novations. As a reminder, in the ARMA(1, 1)-GARCH(1, 1) model the daily log returns
Rt and conditional variance σ2
t of innovations εt are given by
Rt = µ + ϕ1Rt−1 + ψ1εt−1 + εt,
and
σ2
t = ω + α1ε2
t−1 + β1σ2
t−1,
for t = 1, . . . , 2 606, respectively, where ϕ1, ψ1, α1, β1 ̸= 0, µ ∈R, ω > 0, R0 = ε0 =
0 and σ2
0 > 0.
Candidate distributions for εt are the normal, Student’s t, NIG and
GH distributions, as exemplary distributions with zero, one, two and three parameters,
respectively. The number of parameters is reduced compared to the most commonly used
parametrizations due to normalization to zero mean and unit variance, i.e., instead of
examining X ∼N(µ, σ2) the distribution of (X −µ)/σ ∼N(0, 1) is utilized. Generally
speaking, these distributions become more and more leptokurtic, and furthermore, the
parameters of the NIG and GH distribution can be chosen such that the densities are
skewed.
Median log-likelihood, AIC and BIC of the ARMA(1, 1)-GARCH(1, 1) ﬁts of the daily
log return series of all indices and stocks, for every choice of distribution are shown in
Table 9.3. Here, the median is taken over the individual marginal ARMA-GARCH models.
The AIC and BIC criteria suggest the use of Student’s distributed innovations, since it
appears that the additional parameters of the NIG and GH distributions do not improve
105

9
Empirical study
Normal
Student’s t
NIG
GH
LLH
6592
6632
6632
6633
AIC
-13172
-13249
-13249
-13248
BIC
-13136
-13208
-13202
-13195
Table 9.3: Median log-likelihood (LLH), AIC and BIC of ARMA(1, 1)-GARCH(1, 1)
model ﬁts to the daily log return data of two indices and eight stocks, with normal,
Student’s t, negative-inverse Gaussian (NIG) and generalized hyperbolic (GH) distributed
innovations. The highest median log-likelihood, and lowest AIC and BIC are emphasized
in bold face.
information suﬃciently, although only the normal distribution performs noticeably worse
than the other choices.
Table 9.4 shows the full details of the ARMA(1, 1)-GARCH(1, 1) ﬁt with Student’s t
distributed innovations for the daily log return data. The AR(1) and MA(1) coeﬃcients
ϕ1 and ψ1, respectively, are not signiﬁcant at level α = 0.05 for index SX7P and stocks
ACA, BBVA, CBK, DBK and GLE, but highly signiﬁcant for the remaining data sets.
The ARCH(1) component α1 shows high p-values for stocks ACA and BBVA, and is just
above α = 0.05 for CBK. The mean µ and GARCH(1) coeﬃcient β1 ﬁt well for all stocks,
while the mean of the conditional variance ω is not signiﬁcant for any. Finally, the degrees
of freedom ν of the Student’s t distribution are all well below 10, indicating noticeably
heavier tails than the normal distribution. Exemplary plots of several diagnostics for the
ﬁltered daily log returns of the index SX7P are given in Figure 9.2.
Copula data is generated by a probability integral transform of the residuals with the
Student’s t distribution and corresponding estimated degrees of freedom. Given estimated
ARMA(1, 1)-GARCH(1, 1) parameters bµ, bϕ1, bψ1, bν, estimated conditional variance bσ2
t ,
and previous return Rt−1 and residual bεt−1, the residual of current return Rt is given by
bεt = Rt −bµ −bϕ1Rt−1 −bψ1bεt−1,
t = 1, . . . , 2 606.
By the assumption of Student’s t distributed innovations εt, t = 1, . . . , 2606, with condi-
tional variance σ2
t and degrees of freedom ν, the corresponding Student’s t distribution
function F is used to generate standard uniform data by setting
but = F
 bεt
bσ2
t
; bν

,
t = 1, . . . , 2 606.
Similarly, forecasts are produced by quantile function F −1 and desired quantile q ∈(0, 1).
In summary, although the ARMA(1, 1) components may be omitted in some cases, the
ARMA(1, 1)-GARCH(1, 1) model oﬀers an adequate ﬁt.
Therefore, higher ARMA or
GARCH orders will not be considered to minimize complexity in the marginal models,
and the ARMA(1, 1)-GARCH(1, 1) model with Student’s t innovations will be used as a
common model for all stocks and indices in the following section.
106

9.2
Marginal models
Empirical Density of Standardized Residuals
zseries
Probability
−4
−2
0
2
4
0.0
0.1
0.2
0.3
0.4
0.5
0.6
Median:  −0.04 | Mean:  −0.0466
GARCH model :  sGARCH
G
G
normal Density
std (0,1) Fitted Density
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
GG
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
GGG
G
G
G
G
G
GG
G
GG
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
GG
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GGGGG
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GGG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
GG
G
GG
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GGG
G
G
GG
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GGG
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
GG
G
G
GGGG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
GG
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
GGG
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
GG
G G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
GGG
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
GG
GG
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
GG
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
GG
G
G
G
G
G
G
G
GG
G
G
G
G
G
GG
G
G
G
GG
G
G
G
G
G
G
GG
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GGG
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
GG
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
GG
GG
G
GG
G
G
G
G
G
G
G
G
G
G
G
GG
G
GG
G
GG
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
GG
G
GG
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
GG
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
GG
G
GG
G
G
G
G
G
GGG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
GG
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
GG
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
GG
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GGG
G
G
G
G
G
G
GGG
G
G
GG
GGG
G
G
G
G
GGGG
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GGG
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
GG
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
−4
−2
0
2
4
−4
−2
0
2
4
std − QQ Plot
Theoretical Quantiles
Sample Quantiles
GARCH model :  sGARCH
Copula histogram
U
Density
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
Figure 9.2: Diagnostics of an ARMA(1, 1)-GARCH(1, 1) ﬁt with Student’s t distrib-
uted innovations to the daily log returns of index SX7P. From left to right: Histogram
with overlaid theoretical density of the standardized residuals, QQ-plot of standardized
residuals, and histogram of residuals transformed to standard uniform margins.
SXXP
SX7P
ACA
BBVA
BNP
Estimate
P-value
Estimate
P-value
Estimate
P-value
Estimate
P-value
Estimate
P-value
µ
8.70e-04
0.00
6.37e-04
0.00
4.11e-04
0.22
4.44e-04
0.12
7.05e-04
0.00
ϕ1
0.88
0.00
-0.49
0.31
-0.58
0.53
-0.24
0.33
0.84
0.00
ψ1
-0.92
0.00
0.51
0.29
0.59
0.51
0.31
0.20
-0.87
0.00
ω
1.58e-06
0.50
1.12e-06
0.68
2.61e-06
0.76
2.19e-06
0.85
2.63e-06
0.54
α1
0.11
0.00
0.09
0.01
0.07
0.19
0.08
0.41
0.08
0.01
β1
0.88
0.00
0.91
0.00
0.93
0.00
0.92
0.00
0.92
0.00
ν
6.93
0.00
7.38
0.00
6.60
0.00
6.47
0.00
7.61
0.00
LLH
8442
7541
6293
6950
6597
CBK
DBK
GLE
ISP
SAN
Estimate
P-value
Estimate
P-value
Estimate
P-value
Estimate
P-value
Estimate
P-value
µ
2.61e-04
0.47
4.09e-04
0.18
6.47e-04
0.04
4.14e-04
0.10
7.18e-04
0.01
ϕ1
0.24
0.34
-0.09
0.77
-0.34
0.21
0.76
0.00
-0.91
0.00
ψ1
-0.20
0.45
0.13
0.67
0.38
0.15
-0.80
0.00
0.91
0.00
ω
3.81e-06
0.65
3.33e-06
0.35
2.70e-06
0.36
2.75e-06
0.54
2.56e-06
0.47
α1
0.08
0.07
0.09
0.00
0.09
0.00
0.07
0.02
0.10
0.00
β1
0.92
0.00
0.91
0.00
0.91
0.00
0.92
0.00
0.90
0.00
ν
4.94
0.00
7.29
0.00
7.05
0.00
6.37
0.00
6.40
0.00
LLH
6104
6666
6328
6522
6944
Table 9.4: Estimated parameters, p-values of a Wald test for hypothesis H0 : θ ̸= 0
and log-likelihoods (LLH) of marginal ARMA(1, 1)-GARCH(1, 1) model ﬁts to daily log
return data for all indices and stocks. µ, ϕ1 and ψ1 denote the mean, AR(1) and MA(1)
components, and ω, α1 and β1 denote the mean of the conditional variance, ARCH(1)
and GARCH(1) coeﬃcients, respectively.
The degrees of freedom of the Student’s t
distribution is given by ν.
107

9
Empirical study
9.2.2
Out-of-sample ARMA-GARCH
In accordance with the ﬁndings of Section 9.2.1, an ARMA(1, 1)-GARCH(1, 1) model
with Student’s t distributed innovations is used to transform the daily log returns of each
variable individually and out-of-sample to standard uniform margins.
Since the ARMA-GARCH model requires a basis of roughly one year of daily observations
for an adequate ﬁt, the 261 log returns of the ﬁrst year are used for an initial estimation
of the parameters of the model. Afterwards, one-day ahead forecasts are calculated for
each of the remaining 2 345 trading days. The forecast for day t, t = 262, . . . , 2 606, is
produced by ﬁtting the ARMA-GARCH model to returns R1, . . . , Rt−1, i.e., a total of
2 345 ﬁts are done. Although forecasting with a rolling window of, e.g., Rt−252, . . . , Rt−1 is
also possible, the maximal amount of data is chosen for this thesis to facilitate numerically
stable parameter estimation.
Plots of the daily log returns with corresponding ARMA(1, 1)-GARCH(1, 1) forecast and
90% conﬁdence intervals are shown in Figure 9.4. The conﬁdence bands are chosen sym-
metric around the forecast value, i.e., the upper band represents the 95% quantile and
the lower band the 5% quantile of the Student’s t distribution with estimated mean and
variance, respectively. Hence, the negative value of the lower quantile could also be used
in risk management as the VaR at conﬁdence level 95%.
Finally, the residuals of the ARMA-GARCH model are transformed to standard uniform
copula data with a probability integral transform for each index and stock, and every
daily observation. Histograms of the resulting marginal transformed data are shown in
Figure 9.5. The ﬁgure shows that uniformity of the transformed data is mostly fulﬁlled.
The histograms of indices SXXP and SX7P suggest that too many residuals are in the
lower tail, i.e., that the lower tail of the assumed distribution of the innovations is too
light. In contrast, stocks BNP, CBK, DBK, ISP and SAN display a relatively large amount
of residuals close to zero. Overall, the use of the forecast distribution for the generation
of copula data appears justiﬁed, and the use of non-parametric approaches such as the
empirical distribution function is not necessary.
For comparison, Figure 9.3 visualizes the diﬀerence between out-of-sample and in-sample
ARMA-GARCH forecasts for index SX7P. For generation of the in-sample data, an
ARMA(1, 1)-GARCH(1, 1) model with Student’s t distributed innovations is ﬁtted on
all 2606 daily log returns observations, the residuals transformed to the standard uni-
form scale, and then the 261 observations of the ﬁrst year discarded to ensure the same
amount of data for both the in-sample and out-of-sample forecasts. An improvement in
the lower tail for the in-sample forecasts is noticeable, suggesting a, at least partially,
more appropriate forecast distribution. However, shifting more residuals to the center of
the distribution also cause an excess of density close to zero for stocks such as DBK, ISP
and SAN. Histograms of in-sample ARMA-GARCH copula data of the remaining indices
and stocks are shown in Appendix B.1, Figure B.1.
In addition to uniformity, analysis of copula data in the following sections of this thesis
requires independence between all observations from the same index or stock. In order to
validate independence, p-values of Ljung-Box tests at three diﬀerent lags are presented in
Table 9.5. The results show that the null hypothesis of independence can not be rejected
at level α = 0.05 for any of the stocks or indices. Thus, independence of observations
appears to be a reasonable assumption. Additionally, percentages of conﬁdence interval
108

9.2
Marginal models
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.4
0.8
1.2
Out−of−sample histogram of SX7P
U
Density
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.4
0.8
1.2
In−sample histogram of SX7P
U
Density
Figure 9.3:
Comparison of out-of-sample (left) and in-sample (right) ARMA(1, 1)-
GARCH(1, 1) residuals of daily log returns of index SX7P transformed to copula data. A
total of 2345 observations from years 2005 to 2013 are transformed. The theoretical value
of the standard uniform density is indicated by the dotted grey line.
90% CI
SXXP
SX7P
ACA
BBVA
BNP
CBK
DBK
GLE
ISP
SAN
Above CI
4.39%
5.16%
5.80%
5.71%
5.29%
5.25%
5.50%
5.46%
5.71%
5.46%
Below CI
7.59%
6.87%
6.06%
6.61%
6.14%
6.18%
6.31%
6.18%
6.95%
7.04%
Ljung-Box test p-values
Lag 7
0.27
0.27
0.22
0.73
0.16
0.12
0.61
0.07
0.56
0.57
Lag 10
0.51
0.42
0.48
0.86
0.23
0.31
0.58
0.12
0.59
0.79
Lag 15
0.77
0.47
0.63
0.77
0.27
0.62
0.68
0.13
0.40
0.76
Table 9.5: Top: Percentages of daily log returns above and below a symmetric 90%
conﬁdence interval (CI) around the forecast values based on out-of-sample ARMA(1, 1)-
GARCH(1, 1) with Student’s t distributed innovations. Bottom: P-values of Ljung-Box
tests of copula data generated by the ARMA-GARCH model at lags 7, 10 and 15.
violations are shown. Conﬁrming the results of the histograms, these values suggest a
lower tail that is too light, while the upper tail is close to being theoretically optimal.
Since ultimately the dependence structure of the indices and stocks is of interest, pairwise
scatter plots in standard uniform U scale and standard normal Z scale are shown in
Figure 9.6.
Transformation from the U scale to Z scale is achieved by application of
the quantile function Φ−1 of the standard normal distribution to each margin. These
plots suggest that a bivariate Student’s t copula could oﬀer a good ﬁt for the pairwise
dependence.
The empirical contour plots of Figure 9.7 corroborate these preliminary
ﬁndings. Pairwise Kendall’s tau and Spearman’s rho are highest between index SXXP
and its sub-index SX7P, and are generally higher between the bank stocks and the sub-
index than within stocks or between stocks and index SXXP. Overall, pairwise dependence
is medium-to-high with most Kendall’s tau values being in the range of 0.4 to 0.7.
109

9
Empirical study
−0.2
−0.1
0.0
0.1
0.2
2005
2007
2009
2011
2013
Daily log returns of SXXP
Time
Daily log return
Daily log return
Forecast
90% confidence bands
−0.2
−0.1
0.0
0.1
0.2
2005
2007
2009
2011
2013
Daily log returns of SX7P
Time
Daily log return
Daily log return
Forecast
90% confidence bands
−0.2
−0.1
0.0
0.1
0.2
2005
2007
2009
2011
2013
Daily log returns of ACA
Time
Daily log return
Daily log return
Forecast
90% confidence bands
−0.2
−0.1
0.0
0.1
0.2
2005
2007
2009
2011
2013
Daily log returns of BBVA
Time
Daily log return
Daily log return
Forecast
90% confidence bands
−0.2
−0.1
0.0
0.1
0.2
2005
2007
2009
2011
2013
Daily log returns of BNP
Time
Daily log return
Daily log return
Forecast
90% confidence bands
−0.2
−0.1
0.0
0.1
0.2
2005
2007
2009
2011
2013
Daily log returns of CBK
Time
Daily log return
Daily log return
Forecast
90% confidence bands
−0.2
−0.1
0.0
0.1
0.2
2005
2007
2009
2011
2013
Daily log returns of DBK
Time
Daily log return
Daily log return
Forecast
90% confidence bands
−0.2
−0.1
0.0
0.1
0.2
2005
2007
2009
2011
2013
Daily log returns of GLE
Time
Daily log return
Daily log return
Forecast
90% confidence bands
−0.2
−0.1
0.0
0.1
0.2
2005
2007
2009
2011
2013
Daily log returns of ISP
Time
Daily log return
Daily log return
Forecast
90% confidence bands
−0.2
−0.1
0.0
0.1
0.2
2005
2007
2009
2011
2013
Daily log returns of SAN
Time
Daily log return
Daily log return
Forecast
90% confidence bands
Figure 9.4: Daily log returns (grey) with ARMA(1, 1)-GARCH(1, 1) forecast value (dark
blue) and 90% conﬁdence bands (blue). The observations of year 2004 are used for the
initial parameter estimates, and then a total of 2 345 daily forecasts for years 2005 to 2013
produced.
110

9.2
Marginal models
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.4
0.8
1.2
Histogram of SXXP copula data
U
Density
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.4
0.8
1.2
Histogram of SX7P copula data
U
Density
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.4
0.8
1.2
Histogram of ACA copula data
U
Density
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.4
0.8
1.2
Histogram of BBVA copula data
U
Density
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.4
0.8
1.2
Histogram of BNP copula data
U
Density
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.4
0.8
1.2
Histogram of CBK copula data
U
Density
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.4
0.8
1.2
Histogram of DBK copula data
U
Density
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.4
0.8
1.2
Histogram of GLE copula data
U
Density
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.4
0.8
1.2
Histogram of ISP copula data
U
Density
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.4
0.8
1.2
Histogram of SAN copula data
U
Density
Figure 9.5: Histograms of the out-of-sample ARMA(1, 1)-GARCH(1, 1) residuals trans-
formed to standard uniform margins by probability integral transform. Residuals were
generated out-of-sample by marginal ARMA-GARCH ﬁts for 2 345 daily log return ob-
servations of years 2005 to 2013. The theoretical value of the standard uniform density is
indicated by the dotted grey line.
111

9
Empirical study
SXXP
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GGG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GGG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GGG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
GG
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
GG
G
G
G
G
GG
G
G
G
G
G
GG
G
G
G
G
G
G
G G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G G
G
G
G
GG
G
G
G
G
GG
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
GG
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
SX7P
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
GG
G
G
G
G
G
G
G G G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GGG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GGG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
GG
G
G
G
G
G
G
G G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GGG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GGG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
GG
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
ACA
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GGG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
GG
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
GG
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G GG
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G GG
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G GG G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
GG
G
G G
G
G
G
G
G
G
G
G
G
G G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
GG
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G G
G
G G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
BBVA
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GGG
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GGG
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
GG
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G G
G G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
GG
G
G
GG
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
GGG
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
GG G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
GG
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
GG
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
GG
G
G
G
G
G
G
GG
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
GG
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
GG
G
G
G G
G
G
G G
G
G
GG
G
G
G
G
G
G
G
G
GG
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG G
G
G
G
G
G
G
GG
G
G G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
BNP
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
GG
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G GG
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
GG
G G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G G
G
G
G
G G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G G
G
G
G
G
G
GG
G G
G
G
G
G
GG
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
GG
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G G
G
G
G
G G
G
G
G
G
GG
G
GG
G
G
G
G
G
G G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
GG G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
GG
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
GGG
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G GG
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
GG
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
CBK
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
GGG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GGG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
GG
G
G
G
G
GG
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
GG
G
G
G
G
G
G
G
G
GG
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
GG
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
GG
G
G
G
G
GG
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G G
GG
G
G
G
G
G
G
G
GG
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
GG
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
GGG
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GGG
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
GG
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
GG
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
GGG
G
GG
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G GG
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GGG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
GG
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
GG
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG G
G
G
G
G
G
G
G
G
G
GGG
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
DBK
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
GG
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
GG
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G G
G G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
GG
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
GG
G
G
GG
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
GG
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GGG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
GG
G
G
G
G
G
GG G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
GG
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
GGG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
GG
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
GG
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GGG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
GGG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
GG
GG
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
GG
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
GG
G
G
G
G
GG
G
G
G
G
G G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
GG
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
GG
G
G
GG G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G G
G
G
G
G
G
G
G
GG
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
GG
G
G
G
G
G
GG
G
G
G G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
GG
G
G
G
G
G
G
G
GG
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
GGG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GLE
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G GG
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
GGG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GGG
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GGG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GGG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GGG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
GG
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GGG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
GG G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
ISP
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G G
GG
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
GG
G
G
G
G
G
GG
G
G
G
G
G
G
GG
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
GG
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G G
G
GG
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
GG
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GGG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
GG
G
G
G
G
G
GG
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GGG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G GG
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
GGG
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
GG
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G GG
G G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
GG
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GGG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GGG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G GG
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
SAN
Figure 9.6: Scatter plots of pairwise copula data in U (upper right triangle) and Z
(lower left triangle) scale. Copula data was generated out-of-sample by marginal ARMA-
GARCH ﬁts for 2 345 daily log return observations of years 2005 to 2013. For visibility
only a random sample of observations of size 700 is shown.
112

9.2
Marginal models
SXXP
 0.01 
 0.05 
 0.1 
 0.01 
 0.05 
 0.1 
 0.01 
 0.05 
 0.1 
 0.01 
 0.05 
 0.1 
 0.01 
 0.05 
 0.01 
 0.05 
 0.1 
 0.01 
 0.05 
 0.1 
 0.01 
 0.05 
 0.1 
 0.01 
 0.05 
 0.1 
G
τ = 0.73
ρS = 0.90
SX7P
 0.01 
 0.05 
 0.1 
 0.01 
 0.05 
 0.1 
 0.01 
 0.05 
 0.1 
 0.01 
 0.05 
 0.1 
 0.01 
 0.05 
 0.1 
 0.01 
 0.05 
 0.1 
 0.01 
 0.05 
 0.1 
 0.01 
 0.05 
 0.1 
G
τ = 0.52
ρS = 0.70
G
τ = 0.62
ρS = 0.80
ACA
 0.01 
 0.05 
 0.1 
 0.01 
 0.05 
 0.1 
 0.01 
 0.05 
 0.01 
 0.05 
 0.1 
 0.01 
 0.05 
 0.1 
 0.01 
 0.05 
 0.1 
 0.01 
 0.05 
 0.1 
G
τ = 0.57
ρS = 0.76
G
τ = 0.65
ρS = 0.83
G
τ = 0.51
ρS = 0.69
BBVA
 0.01 
 0.05 
 0.1 
 0.01 
 0.05 
 0.1 
 0.01 
 0.05 
 0.1 
 0.01 
 0.05 
 0.1 
 0.01 
 0.05 
 0.1 
 0.01 
 0.05 
 0.1 
G
τ = 0.57
ρS = 0.75
G
τ = 0.67
ρS = 0.85
G
τ = 0.57
ρS = 0.76
G
τ = 0.55
ρS = 0.73
BNP
 0.01 
 0.05 
 0.1 
 0.01 
 0.05 
 0.1 
 0.01 
 0.05 
 0.1 
 0.01 
 0.05 
 0.01 
 0.05 
 0.1 
G
τ = 0.45
ρS = 0.62
G
τ = 0.52
ρS = 0.71
G
τ = 0.45
ρS = 0.62
G
τ = 0.44
ρS = 0.61
G
τ = 0.44
ρS = 0.61
CBK
 0.01 
 0.05 
 0.1 
 0.01 
 0.05 
 0.1 
 0.01 
 0.05 
 0.1 
 0.01 
 0.05 
 0.1 
G
τ = 0.58
ρS = 0.76
G
τ = 0.67
ρS = 0.85
G
τ = 0.53
ρS = 0.71
G
τ = 0.54
ρS = 0.72
G
τ = 0.58
ρS = 0.76
G
τ = 0.51
ρS = 0.69
DBK
 0.01 
 0.05 
 0.1 
 0.01 
 0.05 
 0.01 
 0.05 
G
τ = 0.54
ρS = 0.73
G
τ = 0.66
ρS = 0.84
G
τ = 0.58
ρS = 0.77
G
τ = 0.52
ρS = 0.71
G
τ = 0.63
ρS = 0.81
G
τ = 0.45
ρS = 0.62
G
τ = 0.55
ρS = 0.74
GLE
 0.01 
 0.05 
 0.01 
 0.05 
 0.1 
G
τ = 0.46
ρS = 0.64
G
τ = 0.55
ρS = 0.74
G
τ = 0.46
ρS = 0.64
G
τ = 0.49
ρS = 0.66
G
τ = 0.48
ρS = 0.65
G
τ = 0.39
ρS = 0.55
G
τ = 0.48
ρS = 0.65
G
τ = 0.48
ρS = 0.65
ISP
 0.01 
 0.05 
 0.1 
G
τ = 0.57
ρS = 0.76
G
τ = 0.65
ρS = 0.83
G
τ = 0.51
ρS = 0.69
G
τ = 0.69
ρS = 0.86
G
τ = 0.55
ρS = 0.74
G
τ = 0.42
ρS = 0.59
G
τ = 0.54
ρS = 0.73
G
τ = 0.52
ρS = 0.71
G
τ = 0.48
ρS = 0.65
SAN
Figure 9.7: Contour plot of bivariate copula density in Z space (upper right triangle), and
empirical Kendall’s tau (τ) and Spearman’s ρ (ρS) values (lower left triangle). Copula
data was generated out-of-sample by marginal ARMA-GARCH ﬁts for 2 345 daily log
return observations of years 2005 to 2013.
113

9
Empirical study
9.2.3
Time-varying ARMA DLM
A Bayesian option for the marginal models is the variance discounting DLM (see Sec-
tion 5.2).
In its most general form, there are virtually unlimited possibilities for the
choice of parameters, such as the simple case of only including a time-varying mean. In
this section, a particular parameter choice, which will be called time-varying ARMA(1, 1)-
GARCH(1, 1) DLM, is presented, and its forecasting performance analyzed. This marginal
model will be used in conjunction with Bayesian copula dependence models to produce
VaR forecasts in the last part of the empirical study. Note that in contrast to the ARMA-
GARCH models, the variance discounting DLM always produces out-of-sample forecasts,
which more accurately portray practical applications.
Similarly to the ARMA(1, 1)-GARCH(1, 1) model of the previous sections, the time-
varying ARMA(1, 1) DLM includes a mean, an autoregressive component, and a moving
average component of order one. Contrary to the ARMA-GARCH model though, these
parameters are all time-varying instead of constant, and the variance of the innovations
is governed by variance discounting. A formal discussion of the DLM is given in the
following.
Let (Rt)t=1,...,T, T ∈N, be a series of daily log returns. Then, the observation equation
of the time-varying ARMA(1, 1)-GARCH(1, 1) DLM is given by
Rt = µt + ϕtRt−1 + ψtεt−1 + εt,
εt ∼N

0, 1
φt

,
t = 1, . . . , T,
with R0 = ε0 = 0, µt, ϕt, ψt ∈R, and φt > 0 for all t = 1, . . . , T.
In terms of the
general model of Deﬁnition 5.16, this corresponds to the choices of θt = (µt, ϕt, ψt)⊤and
Ft = (1, Rt−1, εt−1)⊤.
Parameter vector θt follows a random walk with multivariate Student’s t distributed
innovations, i.e., the system equation is given by


µt
ϕt
ψt

=


µt−1
ϕt−1
ψt−1

+


ω1,t
ω2,t
ω3,t

,
ωt =


ω1,t
ω2,t
ω3,t

∼Tnt−1(0, Wt),
t = 1, . . . , T,
with µ0 = ϕ0 = ψ0 = 0.
As previously in terms of the general model, this choice
corresponds to Gt = I3, where Id is the identity matrix in dimension d.
Additionally, it is assumed that precision φt, given discount factor β ∈(0, 1), follows
φt = γt
β φt−1,
γt ∼Beta
βnt−1
2
, (1 −β)nt−1
2

,
t = 1, . . . , T,
with φ0 = 0, and n0 ∈N.
The only remaining part of the model that still needs to be determined is the variance
Wt of innovations of θt.
Here, discounting with constant factor δ ∈(0, 1), in addi-
tion to discount factor β of the general model, is utilized. According to Theorem 5.15,
(θt | Dt−1) ∼Tnt−1(at, Rt), where Rt = GtCt−1G⊤
t +Wt for all t = 1, . . . , T, with positive
deﬁnite matrix C0. This implies, with Gt = I3,
Wt = 1 −δ
δ
Ct−1,
t = 1, . . . , T,
114

9.2
Marginal models
such that
Rt = GtCt−1G⊤
t + Wt = Ct−1 + 1 −δ
δ
Ct−1 = 1
δCt−1,
t = 1, . . . , T.
Since δ ∈(0, 1), variance Cs increases when stepping from time point t −1 to t. In turn,
forecast (θt+1 | Dt) is less certain, describing the loss of predictability in an environment
that changes over time.
A summary of the time-varying ARMA(1, 1) model is given by Table 9.6. For the ﬁt
of the marginal daily log return data, discount factors β = 0.97 and δ = 0.99 are used.
Initial priors are set to m0 = (0, 0, 0)⊤, C0 = 10−3 · I3, n0 = 5 and d0 = 10−3. Similarly
to the ARMA-GARCH model, the forecast distribution is Student’s t distributed.
Plots of the daily log returns with corresponding time-varying ARMA(1, 1) DLM forecast
and 90% conﬁdence intervals are given in Figure 9.8. The ﬁgure shows that both the
forecast value and conﬁdence bands vary more wildly for the DLM compared to ARMA-
GARCH. Furthermore, the variance of the forecast of the returns decreases more slowly
in the case of the DLMs, due to their long memory owing to the relatively high variance
discount factor for the variance of innovations of δ = 0.99.
As previously, the residuals of each stock are transformed to standard uniform scale with
the help of the forecast distribution of the time-varying ARMA(1, 1) DLM. The resulting
histograms are shown in Figure 9.9. For all stocks and indices both tails are slightly too
light compared to the standard uniform distribution. This indicates that too few empirical
residuals are in the tails of the forecast distribution, i.e., that the forecast distribution is
too heavy-tailed. Similarly to the copula data generated by the ARMA(1, 1)-GARCH(1, 1)
model, stocks BNP, CBK, DBK, ISP and SAN display a surplus of residuals close to zero,
causing an excessive amount of marginal copula data near 0.5.
Percentages of 90% conﬁdence interval violations and Ljung-Box tests at three lags are
given by Table 9.7. The conﬁdence intervals of the daily log returns are more accurate
for the DLM than the ARMA-GARCH model, demonstrating improved VaR forecasting
capabilities. Both the upper and lower band are close to their theoretically optimal values.
P-values of the Ljung-Box test are generally lower than for the ARMA-GARCH copula,
but independence of the observations can not be rejected at conﬁdence level α = 0.05 for
any of the indices or stocks at lag 15. Therefore, although less satisfactory for the DLM
than for the ARMA-GARCH model, independence of the observations is a reasonable
assumption.
Overall, despite signs of a too heavy-tailed forecast distribution, adequate copula data is
generated by the time-varying ARMA(1, 1) DLM. Forecasting performance is improved
compared to the previously discussed ARMA-GARCH model, and independence of ob-
servations may also be assumed. Therefore, the copula data of the DLM will be used
alongside the data of the ARMA-GARCH model in the following sections.
Since only the margins are altered, pairwise scatter and empirical bivariate contour plots
of the DLM copula data are largely unchanged in comparison to their ARMA-GARCH
counterparts. They can be viewed in Appendix B.2, Figures B.2 and B.3, respectively.
115

9
Empirical study
Summary of the time-varying ARMA(1, 1) DLM
Observation:
Rt = µt + ϕtRt−1 + ψtεt−1 + εt,
εt ∼N
 0, 1
φt

System:


µt
ϕt
ψt

=


µt−1
ϕt−1
ψt−1

+ ωt,
ωt ∼tnt−1

0, (1−δ)
δ
Ct−1

Precision:
φt = γt/βφt−1,
γt ∼Beta
 β nt−1
2 , (1 −β)nt−1
2

Initial priors:
m0 = (0, 0, 0)⊤, C0 = 10−3 · I3, n0 = 5, d0 = 10−3
Discount factors:
β = 0.97, δ = 0.99
DLM notation:
Ft =


1
Rt−1
εt−1

, θt =


µt
ϕt
ψt

, Gt = I3, Wt = (1−δ)
δ
Ct−1, kt ≡1
Information:
(θt−1 | Dt−1) ∼tnt−1(mt−1, Ct−1)
(θt | Dt−1) ∼tnt−1(at, Rt)
with at = Gtmt−1, Rt = GtCt−1G⊤
t + Wt
St−1 = dt−1
nt−1
Update:
(φt | Dt) ∼G
  nt
2 , dt
2

(φt | Dt−1) ∼G
 β nt−1
2 , β dt−1
2

with mt = at + Atet, Ct =
St
St−1(Rt −AtA⊤
t Qt),
nt = βnt−1 + 1, dt = βdt−1 +
1
QtSt−1e2
t, St = dt
nt,
where et = Rt −ft and At =
1
QtRtFt
Forecast:
(Rt | Dt−1) ∼tβnt−1(ft, Qt)
with ft = F ⊤
t at and Qt = F ⊤
t RtFt + St−1
Table 9.6: Equations, supplied values and updating rules for time-varying ARMA-DLM
ﬁt to log return data. DLM notation corresponds to the general case of Deﬁnition 5.16.
90% CI
SXXP
SX7P
ACA
BBVA
BNP
CBK
DBK
GLE
ISP
SAN
Above CI
4.14%
4.56%
4.90%
4.90%
4.43%
4.86%
4.22%
4.73%
4.73%
4.61%
Below CI
5.33%
4.86%
4.35%
5.07%
4.65%
4.43%
5.29%
4.78%
5.33%
5.16%
Ljung-Box test p-values
Lag 7
0.05
0.19
0.02
0.51
0.04
0.13
0.56
0.16
0.16
0.60
Lag 10
0.15
0.37
0.07
0.66
0.04
0.27
0.59
0.17
0.26
0.80
Lag 15
0.31
0.36
0.12
0.55
0.07
0.53
0.63
0.20
0.20
0.55
Table 9.7: Top: Percentages of daily log returns above and below a symmetric 90%
conﬁdence interval around the forecast values based on the time-varying ARMA(1, 1)
DLM. Bottom: P-values of Ljung-Box tests of copula data generated by the DLM at lags
7, 10 and 15.
116

9.2
Marginal models
−0.2
−0.1
0.0
0.1
0.2
2004
2006
2008
2010
2012
Daily log returns of SXXP
Time
Daily log return
Daily log return
Forecast
90% confidence bands
−0.2
−0.1
0.0
0.1
0.2
2004
2006
2008
2010
2012
Daily log returns of SX7P
Time
Daily log return
Daily log return
Forecast
90% confidence bands
−0.2
−0.1
0.0
0.1
0.2
2004
2006
2008
2010
2012
Daily log returns of ACA
Time
Daily log return
Daily log return
Forecast
90% confidence bands
−0.2
−0.1
0.0
0.1
0.2
2004
2006
2008
2010
2012
Daily log returns of BBVA
Time
Daily log return
Daily log return
Forecast
90% confidence bands
−0.2
−0.1
0.0
0.1
0.2
2004
2006
2008
2010
2012
Daily log returns of BNP
Time
Daily log return
Daily log return
Forecast
90% confidence bands
−0.2
−0.1
0.0
0.1
0.2
2004
2006
2008
2010
2012
Daily log returns of CBK
Time
Daily log return
Daily log return
Forecast
90% confidence bands
−0.2
−0.1
0.0
0.1
0.2
2004
2006
2008
2010
2012
Daily log returns of DBK
Time
Daily log return
Daily log return
Forecast
90% confidence bands
−0.2
−0.1
0.0
0.1
0.2
2004
2006
2008
2010
2012
Daily log returns of GLE
Time
Daily log return
Daily log return
Forecast
90% confidence bands
−0.2
−0.1
0.0
0.1
0.2
2004
2006
2008
2010
2012
Daily log returns of ISP
Time
Daily log return
Daily log return
Forecast
90% confidence bands
−0.2
−0.1
0.0
0.1
0.2
2004
2006
2008
2010
2012
Daily log returns of SAN
Time
Daily log return
Daily log return
Forecast
90% confidence bands
Figure 9.8: Daily log returns (grey) with time-varying ARMA(1, 1) forecast value (dark
blue) and 90% conﬁdence bands (blue). The observations of year 2004 are used for the
initial parameter estimates, and then a total of 2 345 daily forecasts for years 2005 to 2013
produced.
117

9
Empirical study
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.4
0.8
1.2
Histogram of SXXP copula data
U
Density
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.4
0.8
1.2
Histogram of SX7P copula data
U
Density
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.4
0.8
1.2
Histogram of ACA copula data
U
Density
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.4
0.8
1.2
Histogram of BBVA copula data
U
Density
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.4
0.8
1.2
Histogram of BNP copula data
U
Density
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.4
0.8
1.2
Histogram of CBK copula data
U
Density
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.4
0.8
1.2
Histogram of DBK copula data
U
Density
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.4
0.8
1.2
Histogram of GLE copula data
U
Density
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.4
0.8
1.2
Histogram of ISP copula data
U
Density
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.4
0.8
1.2
Histogram of SAN copula data
U
Density
Figure 9.9: Histograms of the time-varying ARMA(1, 1) DLM residuals transformed to
standard uniform margins by probability integral transform. Residuals were generated
by marginal DLM ﬁts to 2 345 daily log return observations of years 2005 to 2013. The
theoretical value of the standard uniform density is indicated by the dotted grey line.
118

9.3
Dependence models
9.3
Dependence models
After transformation of each time series of daily log return data to standard uniform scale
by marginal ARMA-GARCH models and DLMs, multivariate copula dependence models
can be analyzed.
For this thesis, vine copula models (see Section 2.5), and the one-
and two-factor copula models (see Sections 6.1.1 and 6.1.2, respectively) are considered.
Details of the ﬁts of vine and factor copula models to the empirical copula data sets are
given in the following.
9.3.1
Setup of vine copula models
The ﬁrst considered multivariate dependence structure for the previously discussed copula
data sets is vine copulas. The p-factor copula model is equivalent to a C-vine copula
truncated at level p, rooted at latent variables V1, . . . , Vp (see Section 6). Thus, in this
section, the performance of a variety of vine copula structures is investigated to be able to
compare them to the one- and two-factor copula models in the later parts of this thesis.
In total, 16 vine copulas are ﬁtted to both the ARMA(1, 1)-GARCH(1, 1) and time-varying
ARMA(1, 1) DLM copula data. The structures of these vine copulas are ordered in the
following way:
1. For each type of marginal model, data is reorganized to generate two copula data
sets. Since factor copula models assume that latent variables are not observed, the
reduced set consists solely of the eight bank stocks, removing data of indices SXXP
and SX7P. The second, full data set, includes all data from the two indices and eight
stocks for a combined dimension of 10. Viewed as proxies for latent variables V1 and
V2, data from the two indices is used both to explore the vine copula’s ability to ﬁnd
underlying data structures, as well as to roughly estimate the loss of information
incurred when potential choices for the latent factors are no longer observed.
2. Two sets of choices for the bivariate copulas are investigated. One unrestricted set
consists of all copula families of R package VineCopula. One restricted set is similar
to the linking copulas used in Krupskii and Joe (2013, Chapter 5.4), augmented by
the Student’s t copula. Available choices include the bivariate Gaussian, Student’s
t, Gumbel, survival Gumbel, Frank and BB1 copula.
3. Truncated and non-truncated vine structures are examined. Truncation at level 2 is
used to ensure the same number of dependence levels between the vine copula model
and two-factor copula model, and to assess the additional gain of higher levels of
dependencies. In contrast, non-truncated vine copula models are able to use the
maximal amount of dependence structures available to them.
4. R- and C-vine structures are ﬁt. C-vine copulas are chosen due to their similarity
to factor copula models. They are utilized to determine the eﬀect of the restriction
of structure similar to the one induced by factor copula models, compared to less
restrictive R-vines.
A summary of all the vine copulas with corresponding abbreviations is given in Table 9.8.
Copula data is ﬁt with the RVineStructureSelect function of R package VineCopula.
119

9
Empirical study
Abbreviation
Description
RV I/CV I
R-/C-vine, non-truncated, non-restricted, with index data
RV /CV
R-/C-vine, non-truncated, non-restricted, without index data
RV I
R/CV I
R
R-/C-vine, non-truncated, restricted, with index data
RVR/CVR
R-/C-vine, non-truncated, restricted, without index data
rv2I/cv2I
R-/C-vine, truncated at level 2, non-restricted, with index data
rv2/cv2
R-/C-vine, truncated at level 2, non-restricted, without index data
rv2I
R/cv2I
R
R-/C-vine, truncated at level 2, restricted, with index data
rv2R/cv2R
R-/C-vine, truncated at level 2, restricted, without index data
Table 9.8: Abbreviations and descriptions of the vine copula models ﬁt to the data sets.
Tree structures are selected sequentially according to the heuristic algorithm of Dißmann
et al. (2013, Algorithm 3.1), and bivariate copula families are selected by minimal AIC
value. For all vine copulas, each set of bivariate copula data is tested for independence
at level α = 0.05, and the corresponding bivariate copula chosen to be the independence
copula if the hypothesis of independence cannot be rejected.
Log-likelihoods of vine copulas ﬁt to the DLM data are lower than those ﬁt on the ARMA-
GARCH ﬁltered data, but structures and families remain largely unchanged. Therefore,
for brevity, in the following sections only vine copulas ﬁt on the ARMA-GARCH ﬁltered
data are analyzed in-depth.
9.3.2
Vine copula models with index data
In this section, ﬁts of the vine copula models to the copula data with indices, generated
out-of-sample by the ARMA(1, 1)-GARCH(1, 1) model with Student’s t innovations, are
analyzed. Adding index SXXP and sub-index SX7P increases the total dimension of the
data set to 10.
Table 9.9 shows that the non-truncated R-vine copula generally provides the best ﬁt for
the data with indices, according to AIC and BIC. Restricting the set of available choices
for the bivariate linking copulas slightly improves the BIC for the R-vine copula, i.e.,
a copula choice that produces a lower likelihood in one of the lower trees increases the
overall BIC of the model. Truncation at level 2 always leads to inferior models, even
when accounting for the highly decreased number of parameters, indicating a noticeable
amount of dependence in the higher trees. The highest loss of likelihood due to truncation
is incurred by the C-vine copulas. In general, all non-truncated R-vine and C-vine copulas
perform similarly well, and noticeably better than their truncated counterparts.
The preliminary results of the log-likelihood comparison are reﬂected by a series of Vuong
tests at level α = 0.05 with AIC and BIC correction shown in Table 9.10. The restricted
and non-restricted R- and C-vine copulas, RV I
R and RV I, respectively, and CV I
R and
CV I, respectively, are tied, and are preferable to all other ﬁtted vine copula models. This
conﬁrms the previous ﬁndings that there is a non-negligible amount of dependence in
the higher trees. Truncated R-vines are chosen over the truncated C-vines, indicating
that simpliﬁcation of the vine structure is more severe in the truncated than in the non-
truncated case.
120

9.3
Dependence models
Vine copulas
RV I
RV I
R
rv2I
rv2I
R
CV I
CV I
R
cv2I
cv2I
R
No. of par.
60
59
34
34
63
62
33
31
LLH
14331
14335
14185
14183
14319
14336
13437
13447
AIC
-28543
-28551
-28302
-28298
-28513
-28548
-26808
-26832
BIC
-28197
-28211
-28106
-28102
-28150
-28191
-26617
-26654
Table 9.9: Number of parameters (No. of par.), log-likelihood (LLH), AIC, and BIC
of vine structures ﬁt to ARMA-GARCH copula data with indices of total dimension 10.
The highest log-likelihood, and lowest AIC and BIC are emphasized in bold face.
AIC
RV I
R
rv2I
rv2I
R
CV I
CV I
R
cv2I
cv2I
R
RV I
tie
RV I
RV I
tie
tie
RV I
RV I
RV I
R
RV I
R
RV I
R
tie
tie
RV I
R
RV I
R
rv2I
tie
CV I
CV I
R
rv2I
rv2I
rv2I
R
CV I
CV I
R
rv2I
R
rv2I
R
CV I
tie
CV I
CV I
CV I
R
CV I
R
CV I
R
cv2I
tie
BIC
RV I
R
rv2I
rv2I
R
CV I
CV I
R
cv2I
cv2I
R
RV I
tie
RV I
RV I
tie
tie
RV I
RV I
RV I
R
RV I
R
RV I
R
tie
tie
RV I
R
RV I
R
rv2I
tie
tie
tie
rv2I
rv2I
rv2I
R
tie
CV I
R
rv2I
R
rv2I
R
CV I
tie
CV I
CV I
CV I
R
CV I
R
CV I
R
cv2I
tie
Table 9.10: Decisions of Vuong tests at level α = 0.05 with AIC correction (left) and
BIC correction (right). Vine copula models are ﬁt to copula data with indices of total
dimension 10, generated by the ARMA-GARCH model.
Figure 9.10 shows trees 1 to 4 of the best ﬁtting R- and C-vine copula models according
to BIC, the family-restricted R-vine and family-restricted C-vine, RV I
R and CV I
R, respect-
ively. The R-vine structure is close to a C-vine rooted at sub-index SX7P in the ﬁrst tree,
only stock SAN represents a minor exception. Greater deviations from this structure
occur in the later trees. In both cases, all bivariate copulas in the ﬁrst tree are Student’s
t copulas, and diﬀerent choices only appear when bivariate dependence is relatively low.
Although high values of Kendall’s tau are most prominent in the ﬁrst two trees, values
above 0.1 still appear in the later trees to a noticeable extent, explaining the relatively
worse ﬁt of vine copulas truncated at level 2.
The number of bivariate vine families occurring in each of the diﬀerent vine copula models
is given by Table 9.11. The vast majority of ﬁtted families is given by symmetric tail
dependent Student’s t copulas. These are supplemented by smaller amounts of asymmetric
tail dependent Gumbel, BB8 and Tawn copulas.
In general, lifting the restriction of
the family set leads to Student’s t copulas to be replaced by (rotated) BB8 copulas.
Tail independent families such as Gaussian and Frank are rarely chosen, indicating tail
dependence in the data set.
Tables with all details of the copula ﬁts are given in Appendix B.3.1, Tables B.1–B.4.
121

9
Empirical study
Tree 1
t,0.65
t,0.5
t,0.65
t,0.64
t,0.59
t,0.53
t,0.63
t,0.68
t,0.71
SX7P
DBK
CBK
BNP
GLE
ACA
ISP
BBVA
SAN
SXXP
Tree 1
t,0.63
t,0.59
t,0.53
t,0.64
t,0.65
t,0.5
t,0.65
t,0.71
t,0.63
SX7P
SAN
ACA
ISP
GLE
DBK
CBK
BNP
SXXP
BBVA
Tree 2
t,0.15
t,0.09
t,0.24
t,0.2
t,0.06
t,0.09
t,0.26
t,−0.09
SX7P,CBK
SX7P,DBK
SX7P,BNP
SX7P,GLE
SX7P,ACA
SX7P,ISP
SX7P,BBVA
BBVA,SAN
SXXP,SX7P
Tree 2
t,0.04
t,0.06
t,0.2
F,0.07
F,0.1
t,0.17
N,−0.05
t,0.04
SX7P,ACA
SX7P,SAN
SX7P,ISP
SX7P,GLE
SX7P,DBK
SX7P,CBK
SX7P,BNP
SXXP,SX7P
SX7P,BBVA
Tree 3
I,0
t,0.03
t,0.11
t,0.02
t,0.04
t,0.02
I,0
BNP,DBK|SX7P
CBK,DBK|SX7P
BNP,GLE|SX7P
ACA,GLE|SX7P
ACA,ISP|SX7P
BBVA,ISP|SX7P
SX7P,SAN|BBVA
SXXP,GLE|SX7P
Tree 3
t,0.07
I,0
t,0.03
I,0
t,0.06
t,0.03
t,0.4
ACA,ISP|SX7P
ACA,SAN|SX7P
ACA,GLE|SX7P
ACA,DBK|SX7P
ACA,CBK|SX7P
ACA,BNP|SX7P
SXXP,ACA|SX7P
ACA,BBVA|SX7P
Tree 4
G,0.03
SG,0.03
t,0.02
I,0
I,0
t,−0.05
DBK,GLE|SX7P,BNP
BNP,CBK|SX7P,DBK
ACA,BNP|SX7P,GLE
GLE,ISP|SX7P,ACA
ACA,BBVA|SX7P,ISP
ISP,SAN|SX7P,BBVA
SXXP,ACA|SX7P,GLE
Tree 4
t,0.03
t,0.05
G,0.02
t,0.2
t,−0.07
I,0
GLE,SAN|SX7P,ACA
ISP,SAN|SX7P,ACA
DBK,SAN|SX7P,ACA
CBK,SAN|SX7P,ACA
BNP,SAN|SX7P,ACA
SXXP,SAN|SX7P,ACA
BBVA,SAN|SX7P,ACA
Figure 9.10: Vine tree plots of trees 1 to 4 of family-restricted R-vine copula (left) and
family-restricted C-vine copula (right), RV I
R and CV I
R, respectively. Copula data with
indices of total dimension 10 was generated by the ARMA-GARCH model.
122

9.3
Dependence models
Families
I
N
t
G
F
BB8
SC
SG
SBB8
BB8 270
Tawn2 270
RV I
14
0
24
0
1
2
1
0
3
0
0
RV I
R
13
0
27
1
3
0
0
1
0
0
0
rv2I
28
0
15
0
0
0
0
0
2
0
0
rv2I
R
28
0
17
0
0
0
0
0
0
0
0
CV I
12
1
24
1
1
1
0
0
3
1
1
CV I
R
11
2
28
1
3
0
0
0
0
0
0
cv2I
28
0
12
0
1
0
0
0
3
0
1
cv2I
R
28
1
14
0
2
0
0
0
0
0
0
Table 9.11: Number of occurrences of bivariate copula families in each vine copula ﬁt.
Vine copula models are ﬁt to copula data with indices of total dimension 10, generated
by the ARMA-GARCH model. The following abbreviations are used: independence (I),
Gaussian (N), Student’s t (t), Gumbel (G), Frank (F) and Clayton (C) copulas, where
preﬁx ,,S” indicates the survival version and suﬃx ,, 270” rotation by 270◦.
123

9
Empirical study
Vine copulas
RV
RVR
rv2
rv2R
CV
CVR
cv2
cv2R
No. of par.
53
50
25
25
52
47
26
25
LLH
9361
9355
8910
8910
9353
9341
8665
8662
AIC
-18617
-18610
-17770
-17770
-18603
-18588
-17278
-17275
BIC
-18312
-18322
-17626
-17626
-18303
-18317
-17128
-17131
Table 9.12: Number of parameters (No. of par.), log-likelihood (LLH), AIC and BIC of
vine structures ﬁt to ARMA-GARCH copula data without indices of total dimension 8.
The highest log likelihood, and lowest AIC and BIC are emphasized in bold face.
AIC
RVR
rv2
rv2R
CV
CVR
cv2
cv2R
RV
tie
RV
RV
tie
tie
RV
RV
RVR
RVR
RVR
tie
tie
RVR
RVR
rv2
tie
CV
CVR
rv2
rv2
rv2R
CV
CVR
rv2R
rv2R
CV
tie
CV
CV
CVR
CVR
CVR
cv2
tie
BIC
RVR
rv2
rv2R
CV
CVR
cv2
cv2R
RV
tie
RV
RV
tie
tie
RV
RV
RVR
RVR
RVR
tie
tie
RVR
RVR
rv2
tie
CV
CVR
rv2
rv2
rv2R
CV
CVR
rv2R
rv2R
CV
tie
CV
CV
CVR
CVR
CVR
cv2
tie
Table 9.13: Decisions of Vuong tests at level α = 0.05 with AIC correction (left) and
BIC correction (right). Vine copula models are ﬁt to copula data without indices of total
dimension 8, generated by the ARMA-GARCH model.
9.3.3
Vine copula models without index data
In this section, ﬁts of the vine copula models to the copula data without indices, generated
out-of-sample by the ARMA(1, 1)-GARCH(1, 1) model with Student’s t innovations, are
analyzed.
As previously, Table 9.12 shows that the non-truncated R-vine copula provides the best to
the data with respect to log-likelihood, AIC and BIC. Restriction of the family set leads
to fewer parameters and a reduced BIC. While the non-truncated R- and C-vine copulas
are very similar, the corresponding truncated vine perform noticeably worse. Compared
to the copula data with indices, the relative loss of log-likelihood to truncation is even
more severe in the case of the R-vine. Note that for the R-vine copula truncated at level
2 family restriction has no eﬀect on vine copula ﬁt.
Vuong tests at level α = 0.05, given by Table 9.13, ﬁnd the non-truncated R- and C-vines
tied and, superior to all other models. Similarly to the case with index data, the truncated
R-vines are preferred over their C-vine counterparts.
Figure 9.11 shows trees 1 to 4 of the best ﬁtting R- and C-vine copula models accord-
ing to BIC, the family-restricted R-vine and family-restricted C-vine, RVR and CVR,
respectively. Although both models produce very similar log-likelihoods, their structure
is markedly diﬀerent. The only exception is BNP, it is the most central variable in the
ﬁrst tree of both structures. Further, a lower sum of Kendall’s tau values in the ﬁrst tree
of the C-vine is compensated for by higher association in the higher trees.
The number of occurrences of copula families in each vine copula is given by Table 9.14.
As was the case with index data, the vast majority of the bivariate copulas is given by
the symmetric tail dependent Student’s t copula, indicating tail dependence in the data
124

9.3
Dependence models
Tree 1
t,0.53
t,0.68
t,0.47
t,0.56
t,0.61
t,0.56
t,0.48
BNP
SAN
BBVA
ISP
DBK
GLE
ACA
CBK
Tree 1
t,0.53
t,0.53
t,0.56
t,0.55
t,0.46
t,0.42
t,0.61
BNP
SAN
BBVA
DBK
ACA
ISP
CBK
GLE
Tree 2
t,0.18
t,0.13
t,0.24
t,0.21
t,0.23
F,0.14
BBVA,SAN
BNP,SAN
BBVA,ISP
BNP,DBK
BNP,GLE
ACA,GLE
CBK,DBK
Tree 2
t,0.52
t,0.23
t,0.2
t,0.24
F,0.2
t,0.18
BBVA,BNP
BNP,SAN
BNP,DBK
ACA,BNP
BNP,ISP
BNP,CBK
BNP,GLE
Tree 3
t,0.18
F,0.1
t,0.1
F,0.15
t,0.09
ISP,SAN|BBVA
BBVA,BNP|SAN
DBK,SAN|BNP
DBK,GLE|BNP
ACA,BNP|GLE
BNP,CBK|DBK
Tree 3
t,0.11
F,0.17
t,0.13
F,0.26
t,0.16
BBVA,DBK|BNP
BBVA,SAN|BNP
ACA,BBVA|BNP
BBVA,ISP|BNP
BBVA,CBK|BNP
BBVA,GLE|BNP
Tree 4
t,0.11
t,0.07
t,0.09
F,0.11
BBVA,DBK|BNP,SAN
BNP,ISP|BBVA,SAN
GLE,SAN|BNP,DBK
ACA,DBK|BNP,GLE
CBK,GLE|BNP,DBK
Tree 4
t,0.04
t,0.09
F,0.12
t,0.2
ACA,DBK|BBVA,BNP
DBK,SAN|BBVA,BNP
DBK,ISP|BBVA,BNP
CBK,DBK|BBVA,BNP
DBK,GLE|BBVA,BNP
Figure 9.11: Vine tree plots of trees 1 to 4 of family-restricted R-vine copula (left) and
family-restricted C-vine copula (right), RVR and CVR, respectively. Copula data without
indices was generated by the ARMA-GARCH model.
125

9
Empirical study
Families
I
N
t
F
BB8
SBB8
RV I
0
1
20
2
3
2
RV I
R
0
1
22
5
0
0
rv2I
15
0
12
1
0
0
rv2I
R
15
0
12
1
0
0
CV I
2
0
21
0
2
3
CV I
R
2
0
21
5
0
0
cv2I
15
0
12
0
0
1
cv2I
R
15
0
12
1
0
0
Table 9.14: Number of occurrences of bivariate copula families in each vine copula ﬁt.
Vine copula models are ﬁt to copula data with without indices of total dimension 8, gener-
ated by the ARMA-GARCH model. The following abbreviations are used: independence
(I), Gaussian (N), Student’s t (t) and Frank (F) copulas, where preﬁx ,,S” indicates the
survival version.
set. Furthermore, restricting the set of possible choices for the bivariate families mostly
leads to BB8 copulas to be replaced by Frank and Student’s t copulas.
Tables with all details of the vine copula ﬁts are given in Appendix B.3.1, Tables B.5 and
B.6.
9.3.4
Setup of factor copula models
In addition to vine copula models, one- and two-factor copula models are ﬁt to the ARMA-
GARCH and DLM copula data. Data of the indices is omitted, since factor copula models
assume the common variables to be latent. Thus, copula data sets are reduced to the eight
bank stocks for both the one- and two-factor copula model ﬁts.
Similarly to Section 8.2, factor copula model parameters are estimated by the marginal
maximum likelihood method implemented in R package CopulaModel. As recommended
by Krupskii and Joe (2013, p. 93) 21 quadrature points are chosen for each latent variable
in the factor copula model, i.e., nq = 21 for the one- and nq = 42 for the two-factor copula
model.
9.3.5
One-factor copula model
One-factor copula models with Frank, Gumbel, Student’s t and BB1 bivariate linking
copulas for all variables are estimated with function f90ml1fact. Simplifying the model
through the choice of a single copula family leads to easier interpretation, although bivari-
ate copulas of each stock and the latent variable may be chosen individually. Separate
choices for each copula may achieve higher likelihoods, however it can be diﬃcult to de-
termine the most appropriate family, when the latent variable is not observed. Krupskii
and Joe (2013, p. 96) also note that the performance of factor copula models with Stu-
dent’s t linking copula is similar to multivariate Student’s t factor models (see Kl¨uppelberg
and Kuhn, 2009), but the latter require fewer parameters. Furthermore, the degrees of
freedom parameters of the Student’s t copulas are not estimated, and have to be ﬁxed
126

9.3
Dependence models
ARMA-GARCH one-factor copula model
Frank
Gumbel
t(5)
BB1
No. of par.
8
8
8
16
LLH
7825
7843
8613
8445
AIC
-15634
-15670
-17211
-16857
BIC
-15588
-15623
-17165
-16765
DLM one-factor copula model
Frank
Gumbel
t(5)
BB1
No. of par.
8
8
8
16
LLH
8145
7434
8297
7859
AIC
-16274
-14853
-16577
-15685
BIC
-16228
-14807
-16531
-15593
Table 9.15: Number of parameters (No. of par), log-likelihood (LLH), AIC and BIC
of the ﬁts of the one-factor copula models to ARMA-GARCH (left) and DLM (right)
copula data. All bivariate linking copulas are chosen to be Frank, Gumbel, Student’s t or
BB1 copulas. For the Student’s t copula, the degrees of freedom parameter ν is given in
parenthesis. The highest log-likelihood, and lowest AIC and BIC, are emphasized in bold
face.
and equal for all variables. Since the vine copula analysis of the previous section suggests
values around ν = 5 as a reasonable estimate for the bivariate Student’s t copula between
stocks and the bank sub-index SX7P, this value is chosen for the one-factor copula models.
Results of the one-factor copula estimation for the ARMA-GARCH and DLM copula data
sets with eight bank stocks are given in Table 9.15. Student’s t copulas provide the best ﬁt
according to log-likelihood, as suspected by the high amount of linking Student’s t copulas
encountered during analysis of the vine copula models of the previous section. Similarly
to vine copulas, excluding the case of bivariate Frank copulas, models ﬁt on the DLM
copula data produce lower likelihoods than the respective models for the ARMA-GARCH
data. Copula data generated by marginal DLM models favors Frank copulas in particular,
almost closing the gap to the choice of Student’s t copulas. Overall, log-likelihoods of the
one-factor copula models are lower than those of the vine factor copula models without
index data, justifying the exploration of the two-factor copula model.
9.3.6
Two-factor copula model
Similarly to the one-factor copula models, Frank, Gumbel and Student’s t linking copulas
are chosen for all variables and both factors. Additionally, models where Gumbel linking
copulas are used for the ﬁrst factor and Frank copulas for the second factor, and models
where BB1 copulas are used for the ﬁrst factor and Frank copulas for the second factor
are ﬁt. The degrees of freedom parameter of the bivariate Student’s t linking copulas are
set to ν1 = 5 for the ﬁrst factor and ν2 = 10 for the second factor. As previously, this is
motivated by the estimated degrees of freedom parameters of Student’s t copulas in the
vine copula models in the ﬁrst and second tree, respectively. Two-factor copula models
are estimated by the f90ml2fact function of the CopulaModel package.
The results of the estimation of the two-factor copula model for the ARMA-GARCH
and DLM copula data sets without indices are shown in Table 9.16. Compared to the
one-factor copula model, log-likelihoods, AICs and BICs are improved. For the ARMA-
GARCH data, choosing Student’s t copula for all linking copulas yields the lowest AIC
and BIC, as was the case in the one-factor copula model. However, the best ﬁt of the DLM
data is provided by the model with BB1 copulas for the ﬁrst factor and Frank copulas
for the second factor. In both cases Student’s t and BB1-Frank linking copulas produce
127

9
Empirical study
ARMA-GARCH two-factor copula model
Frank
Gumbel
t(5,10)
Gumbel-Frank
BB1-Frank
No. of par.
16
16
16
16
24
LLH
8821
8244
9171
8873
9171
AIC
-17609
-16455
-18309
-17715
-18294
BIC
-17517
-16363
-18217
-17623
-18156
DLM two-factor copula model
Frank
Gumbel
t(5,10)
Gumbel-Frank
BB1-Frank
No. of par.
16
16
16
16
24
LLH
9009
7930
8838
8923
9140
AIC
-17986
-15828
-17644
-17814
-18232
BIC
-17894
-15736
-17552
-17722
-18093
Table 9.16: Number of parameters (No. of par), log-likelihood (LLH), AIC and BIC
of the ﬁts of the two-factor copula models for ARMA-GARCH (top) and DLM (bottom)
copula data. All bivariate linking copulas are chosen to be Frank, Gumbel, Student’s t, all
linking copulas with the ﬁrst factor Gumbel and with the second factor Frank (Gumbel-
Frank), or all linking copulas with the ﬁrst factor BB1 and with the second factor Frank
(BB1-Frank). For the Student’s t copula, the degrees of freedom parameters with the
ﬁrst factor ν1 and with the second factor ν2 are given in parenthesis (ν1,ν2). The highest
log-likelihood, and lowest AIC and BIC, respectively, are emphasized in bold face.
similar ﬁts, with the other choices of copulas lagging behind noticeably. In general, the
vastly increased numerical complexity of the two- compared to the one-factor copula model
is justiﬁed by the greatly improved likelihoods.
9.3.7
Comparison of vine and factor copula models
Analysis of the vine and factor copula models in the previous sections enables a direct
comparison of log-likelihoods. Since the one- and two-factor copula models generally use
fewer parameters than vine copulas, models are chosen according to lowest BIC, in order
to adequately penalize higher amounts of parameters. Further, due to the equivalence of
the two-factor copula model and a C-vine copula truncated at level 2 with roots at the
latent variables, truncated C-vines are also added to the comparison. Note though, that
due the absence of observations for the latent variables, structures of ﬁtted C-vines will
not coincide with those implied by the two-factor copula model.
The summary presented in Table 9.17 shows that the non-truncated R-vine copula with
restricted family set RVR provides the best ﬁt among the competing models with respect to
BIC for both the ARMA-GARCH and DLM copula data. However, the two-factor copula
model lags only slightly behind at a vastly decreased number of parameters, making it
particularly appealing when sparsity is valued highly. The one-factor copula and truncated
C-vine copula models oﬀer comparable performance, though they are noticeably inferior
to the other two models. This indicates that these reduced models are inadequate to
model dependence in the data set accurately, due to their simpliﬁed structures. Overall,
both vine copulas and two-factor copula models prove sensible choices for this data set of
daily log returns of bank stocks.
128

9.4
Bayesian inference for the one-factor copula model
Vine copula models
Factor copula models
ARMA-GARCH
RVR
cv2R
t(5)
t(5,10)
No. of par.
50
25
8
16
LLH
9355
8662
8613
9171
AIC
-18610
-17275
-17221
-18309
BIC
-18322
-17131
-17165
-18217
Vine copula models
Factor copula models
DLM
RVR
cv2R
t(5)
BB1-Frank
No. of par.
45
24
8
24
LLH
9246
8580
8297
9140
AIC
-18403
-17113
-16577
-18232
BIC
-18144
-16975
-16531
-18093
Table 9.17: Comparison of vine copula (RVR and cv2R) and one- and two-factor copula
(t(5), t(5,10) and BB1-Frank) models with ARMA-GARCH (top) and DLM (bottom)
copula data. Number of parameters (No. of par), log-likelihood (LLH), AIC and BIC are
shown. The highest log-likelihood, and lowest AIC and BIC, respectively, are emphasized
in bold face.
9.4
Bayesian inference for the one-factor copula model
Bayesian inference for the one-factor copula model is done with the individual ARMS
Gibbs sampler introduced in Section 8.3.2. In order to enable a wholly Bayesian analysis,
the copula data produced by the time-varying ARMA(1, 1) DLM is employed. Similarly
to the analysis of the factor copula models in Section 9.3.4, indices SXXP and SX7P are
omitted. Thus, the copula data set comprises a total of 2345 daily observations of eight
bank stocks from years 2005 to 2013.
Two distinct analyses are done with the Gibbs sampler. First, all 2 345 observations of
dimension 8 of the data set are used at once, leading to 2 345 + 8 = 2 353 parameters
to be sampled from, since each observation presents a distinct state of latent variable
V1. Then, each year of the data set, consisting of about 260 daily observations, is looked
at separately, producing nine smaller yearly models. For both of these scenarios, three
diﬀerent choices of copulas are considered. All bivariate linking copulas of the model are
set to be either Gumbel, survival Gumbel or Gaussian copulas.
The Gibbs sampler is run for 2 000 iterations for each data set and choice of bivariate
linking copulas, and the ﬁrst 1 000 iterations discarded as burn-in for a total posterior
sample size of 1 000. Starting values are chosen according to the strategy discussed in
Section 8.3.1.
Exemplary trace and density plots of posterior samples of the copula parameter for ACA
and the latent variable V1 in Kendall’s tau scale are shown in Figure 9.12. For this ex-
ample, bivariate linking copulas in the one-factor copula model are chosen to be Gaussian
copulas. The ﬁgure indicates that the output of the Gibbs sampler is noticeable more
diﬀuse when run on the whole data set, than when year 2005 is analyzed separately. This
may be caused by the numerical complexity introduced by the high amount of parameters
of the simultaneous analysis of nine years of daily data, although it may also stem from
a change in the underlying dependence over time.
129

9
Empirical study
0
200
400
600
800
1000
0.2
0.4
0.6
0.8
Iteration
τACA
Traceplot of τACA for years 2005 to 2013
Posterior mode
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0
5
15
25
35
τACA
Density
Histogram of τACA for years 2005 to 2013
Posterior mode
0
200
400
600
800
1000
0.2
0.4
0.6
0.8
Iteration
τACA
Traceplot of τACA for year 2005
Posterior mode
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0
5
15
25
35
τACA
Density
Histogram of τACA for year 2005
Posterior mode
Figure 9.12: Trace and density plots of the individual ARMS Gibbs sampler based on
2345 DLM ﬁltered daily observations of years 2005 to 2013 (top), and 260 observations of
year 2005 (bottom) with Gaussian linking copulas. The posterior sample in Kendall’s tau
scale τACA of the copula parameter of latent variable V1 with bank stock ACA is shown.
The diﬀerence in posterior estimates of the copula parameter between using data of indi-
vidual years and all years at once can be seen in Table 9.18. The posterior mode estimates
of the copula parameters of the stocks with the latent factor change moderately over time,
possibly causing more uncertainty when all years are viewed at once.
Additional trace and density plots, and posterior mode estimates are given in Appen-
dices B.5.1 and B.5.2, Figures B.4–B.33 and Appendix B.5.3, Tables B.15 and B.16,
respectively.
130

9.4
Bayesian inference for the one-factor copula model
Posterior modes
ACA
BBVA
BNP
CBK
DBK
GLE
ISP
SAN
2005
0.48
0.61
0.62
0.38
0.57
0.68
0.34
0.57
2006
0.55
0.63
0.65
0.52
0.70
0.64
0.43
0.68
2007
0.68
0.64
0.74
0.55
0.70
0.64
0.49
0.56
2008
0.61
0.73
0.62
0.54
0.67
0.54
0.57
0.71
2009
0.66
0.77
0.71
0.58
0.69
0.64
0.65
0.78
2010
0.71
0.69
0.75
0.57
0.65
0.70
0.68
0.70
2011
0.69
0.73
0.74
0.45
0.67
0.72
0.62
0.74
2012
0.73
0.68
0.79
0.63
0.70
0.79
0.71
0.68
2013
0.65
0.73
0.71
0.34
0.62
0.71
0.60
0.71
2005-2013
0.57
0.48
0.47
0.46
0.39
0.51
0.64
0.48
Table 9.18: Posterior mode estimates of bivariate copula parameters of 8 bank stocks
with latent variable V1, based on Gibbs sampler output of size 1 000 in Kendall’s tau scale.
The Gibbs sampler with survival Gumbel linking copulas was run on DLM ﬁltered daily
observations of individual years 2005 to 2013 (top), and on all 2 345 daily observations at
once (bottom).
131

9
Empirical study
9.4.1
Behavior of the latent variable over time
One advantage of the Bayesian analysis of the one-factor copula model over marginal
maximum likelihood estimation is the additional posterior sample of latent variable V1.
This sample enables the investigation of the assumed underlying factor over time.
As previously, there are 1 000 samples of V1,t for each day t = 1, . . . , 2 345. Therefore,
the state of V1 is extracted as the mean of 100 equidistant iterations, i.e., iterations
i = 1, 11, . . . , 991, after burn-in, of the individual ARMS Gibbs sampler for the one-factor
copula model with Gaussian linking copulas.
All choices of linking copulas produced
extremely similar results, causing an individual analysis of each copula to be unnecessary.
Figure 9.13 shows the traceplots of V1 for the simultaneous analysis of all 2 345 daily
observations, and of the single analyses of pre-crisis year 2005 and crisis year 2008 with
Gaussian linking copulas. In all cases the latent variable shows a high amount of auto-
correlation, and distinctly diﬀerent traces in the cases of individual years 2005 and 2008.
Traceplots of the one-factor copula model with Gumbel and survival Gumbel copulas are
given in Appendix B.5.4, Figures B.34 and B.35.
2006
2008
2010
2012
2014
0.0
0.4
0.8
Time
V1
Traceplot of V1 for years 2005 to 2013
Jan
Mar
May
Jul
Sep
Nov
Jan
0.0
0.4
0.8
Time
V1
Traceplot of V1 for year 2005
Jan
Mar
May
Jul
Sep
Nov
Jan
0.0
0.4
0.8
Time
V1
Traceplot of V1 for year 2008
Figure 9.13: Traceplots of the latent variable V1 over time. The individual ARMS Gibbs
sampler for the one-factor copula model with Gaussian linking copulas was run on all 2 345
daily observations (top), and on 260 and 262 daily observations of years 2005 (bottom
left) and 2008 (bottom right), respectively. Values for V1 for each day are derived from
the mean of 100 equidistant iterations out of a total sample size of 1 000. Copula data of
total dimension 8 was generated by marginal DLMs.
132

9.4
Bayesian inference for the one-factor copula model
ACA with Gumbel copulas
 0.02 
 0.05 
 0.1 
 0.2 
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
U
V1
 0.02 
 0.05 
 0.1 
 0.2 
Empirical density
Theoretical density
ACA with Gaussian copulas
 0.02 
 0.05 
 0.1 
 0.2 
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
U
V1
 0.02 
 0.05 
 0.1 
 0.2 
Empirical density
Theoretical density
ACA with survival Gumbel copulas
 0.02 
 0.05 
 0.1 
 0.2 
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
U
V1
 0.02 
 0.05 
 0.1 
 0.2 
Empirical density
Theoretical density
Figure 9.14: Contour plots with standard normal margins of the DLM ﬁltered copula
data set of ACA and posterior samples of latent variable V1 for daily log returns from
years 2005 to 2013. 1 000 samples of latent variable V1 were generated by an individual
ARMS Gibbs sampler for a one-factor copula model with Gumbel (left), Gaussian (middle)
and survival Gumbel (right) linking copulas. The mean of all iterations of the Gibbs
sampler is used for latent variable V1 and the copula parameters. Empirical densities are
indicated by solid black lines, theoretical bivariate copula densities with corresponding
copula parameters by dotted blue lines.
9.4.2
Joint distribution of the latent variable and bank stocks
In addition to the analysis of the behavior of latent variable V1 over time, the joint
distribution of V1 and each of the bank stocks is investigated. As previously, the underlying
copula data is given by the DLM ﬁltered daily log returns of eight bank stocks. Posterior
samples of the bivariate copula parameters and the latent factor are produced by the
individual ARMS Gibbs sampler described in Section 9.4.
Exemplary, the one-factor
copula model with Gaussian linking copulas is shown for the majority of this section.
Bivariate behavior is studied by empirical contour plots of the posterior sample of V1 and
marginal copula data. As a point of reference, theoretical contours of the corresponding
bivariate copula densities and copula parameter are overlaid.
Figures 9.15 and 9.16 show contour plots with standard normal margins of three stocks
for Gibbs sampler iterations 250, 500, 750 and 1 000 for years 2005 to 2013 simultaneously
and individual year 2005, respectively. The empirical contour plots of the joint analysis
of all nine years resemble their theoretically implied shapes more closely than those of the
individual analysis of year 2005, though in general both are similar.
Furthermore, contour plots, where the mean over all 1 000 samples of latent variable
V1,t of day t = 1, . . . , 2 345 and of the copula parameters of the joint and individual
analyses are taken, are shown in Figures 9.17 and 9.18, respectively. Again, empirical and
theoretical contours match closely when the whole data set of years 2005 to 2013 is used,
but some stocks show a noticeable deviation when only the 260 observations of year 2005
are considered. With the data of the individual year, theoretical densities of, e.g., stocks
BBVA and GLE, exhibit higher association than their empirical counterparts.
An example for ACA with Gumbel, Gaussian and survival Gumbel copulas for years 2005
to 2013 is shown in Figure 9.14. The Gaussian copula is closest to its theoretical values,
since the asymmetry of the Gumbel and survival Gumbel copulas does not appear to be
reﬂected by the data.
133

9
Empirical study
ACA, iteration 250
 0.01 
 0.05 
 0.1 
 0.2 
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
U
V1
 0.01 
 0.05 
 0.1 
 0.2 
Empirical density
Theoretical density
BBVA, iteration 250
 0.01 
 0.05 
 0.1 
 0.2 
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
U
V1
 0.01 
 0.05 
 0.1 
Empirical density
Theoretical density
BNP, iteration 250
 0.01 
 0.05 
 0.1 
 0.2 
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
U
V1
 0.01 
 0.05 
 0.1 
 0.2 
Empirical density
Theoretical density
ACA, iteration 500
 0.01 
 0.05 
 0.1 
 0.2 
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
U
V1
 0.01 
 0.05 
 0.1 
 0.2 
Empirical density
Theoretical density
BBVA, iteration 500
 0.01 
 0.05 
 0.1 
 0.2 
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
U
V1
 0.01 
 0.05 
 0.1 
 0.2 
Empirical density
Theoretical density
BNP, iteration 500
 0.01 
 0.05 
 0.1 
 0.2 
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
U
V1
 0.01 
 0.05 
 0.1 
 0.2 
Empirical density
Theoretical density
ACA, iteration 750
 0.01 
 0.05 
 0.1 
 0.2 
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
U
V1
 0.01 
 0.05 
 0.1 
 0.2 
Empirical density
Theoretical density
BBVA, iteration 750
 0.01 
 0.05 
 0.1 
 0.2 
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
U
V1
 0.01 
 0.05 
 0.1 
 0.2 
Empirical density
Theoretical density
BNP, iteration 750
 0.01 
 0.05 
 0.1 
 0.2 
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
U
V1
 0.01 
 0.05 
 0.1 
 0.2 
Empirical density
Theoretical density
ACA, iteration 1000
 0.01 
 0.05 
 0.1 
 0.2 
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
U
V1
 0.01 
 0.05 
 0.1 
Empirical density
Theoretical density
BBVA, iteration 1000
 0.01 
 0.05 
 0.1 
 0.2 
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
U
V1
 0.01 
 0.05 
 0.1 
 0.2 
Empirical density
Theoretical density
BNP, iteration 1000
 0.01 
 0.05 
 0.1 
 0.2 
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
U
V1
 0.01 
 0.05 
 0.1 
Empirical density
Theoretical density
Figure 9.15: Contour plots with standard normal margins of the DLM ﬁltered copula data set of 8 bank stocks and posterior
samples of latent variable V1 for daily log returns of years 2005 to 2013. 1 000 samples of latent variable V1 were generated by an
individual ARMS Gibbs sampler for a one-factor copula model with Gaussian linking copulas. Iterations 250, 500, 750 and 1 000
of the Gibbs sampler for stocks ACA (top), BBVA (middle) and BNP (bottom) are shown. Empirical densities are indicated by
solid black lines, theoretical bivariate Gaussian copula densities with copula parameter of the corresponding iteration by dotted
blue lines.
134

9.4
Bayesian inference for the one-factor copula model
ACA, iteration 250
 0.01 
 0.05 
 0.1 
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
U
V1
 0.01 
 0.05 
 0.1 
 0.2 
Empirical density
Theoretical density
BBVA, iteration 250
 0.01 
 0.05 
 0.1 
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
U
V1
 0.01 
 0.05 
 0.1 
 0.2 
Empirical density
Theoretical density
BNP, iteration 250
 0.01 
 0.05 
 0.1 
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
U
V1
 0.01 
 0.05 
 0.1 
 0.2 
Empirical density
Theoretical density
ACA, iteration 500
 0.01 
 0.05 
 0.1 
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
U
V1
 0.01 
 0.05 
 0.1 
 0.2 
Empirical density
Theoretical density
BBVA, iteration 500
 0.01 
 0.05 
 0.1 
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
U
V1
 0.01 
 0.05 
 0.1 
 0.2 
Empirical density
Theoretical density
BNP, iteration 500
 0.01 
 0.05 
 0.1 
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
U
V1
 0.01 
 0.05 
 0.1 
 0.2 
Empirical density
Theoretical density
ACA, iteration 750
 0.01 
 0.05 
 0.1 
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
U
V1
 0.01 
 0.05 
 0.1 
 0.2 
Empirical density
Theoretical density
BBVA, iteration 750
 0.01 
 0.05 
 0.1 
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
U
V1
 0.01 
 0.05 
 0.1 
 0.2 
Empirical density
Theoretical density
BNP, iteration 750
 0.01 
 0.05 
 0.1 
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
U
V1
 0.01 
 0.05 
 0.1 
 0.2 
Empirical density
Theoretical density
ACA, iteration 1000
 0.01 
 0.05 
 0.1 
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
U
V1
 0.01 
 0.05 
 0.1 
 0.2 
Empirical density
Theoretical density
BBVA, iteration 1000
 0.01 
 0.05 
 0.1 
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
U
V1
 0.01 
 0.05 
 0.1 
 0.2 
Empirical density
Theoretical density
BNP, iteration 1000
 0.01 
 0.05 
 0.1 
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
U
V1
 0.01 
 0.05 
 0.1 
 0.2 
Empirical density
Theoretical density
Figure 9.16: Contour plots with standard normal margins of the DLM ﬁltered copula data set of 8 bank stocks and posterior
samples of latent variable V1 for daily log returns of year 2005. 1 000 samples of latent variable V1 were generated by an individual
ARMS Gibbs sampler for a one-factor copula model with Gaussian linking copulas. Iterations 250, 500, 750 and 1 000 of the Gibbs
sampler for stocks ACA (top), BBVA (middle) and BNP (bottom) are shown. Empirical densities are indicated by solid black lines,
theoretical bivariate Gaussian copula densities with copula parameter of the corresponding iteration by dotted blue lines.
135

9
Empirical study
ACA
 0.02 
 0.05 
 0.1 
 0.2 
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
U
V1
 0.02 
 0.05 
 0.1 
 0.2 
Empirical density
Theoretical density
BBVA
 0.02 
 0.05 
 0.1 
 0.2 
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
U
V1
 0.02 
 0.05 
 0.1 
 0.2 
Empirical density
Theoretical density
BNP
 0.02 
 0.05 
 0.1 
 0.2 
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
U
V1
 0.02 
 0.05 
 0.1 
 0.2 
Empirical density
Theoretical density
CBK
 0.02 
 0.05 
 0.1 
 0.2 
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
U
V1
 0.02 
 0.05 
 0.1 
 0.2 
Empirical density
Theoretical density
DBK
 0.02 
 0.05 
 0.1 
 0.2 
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
U
V1
 0.02 
 0.05 
 0.1 
 0.2 
Empirical density
Theoretical density
GLE
 0.02 
 0.05 
 0.1 
 0.2 
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
U
V1
 0.02 
 0.05 
 0.1 
 0.2 
Empirical density
Theoretical density
ISP
 0.02 
 0.05 
 0.1 
 0.2 
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
U
V1
 0.02 
 0.05 
 0.1 
 0.2 
Empirical density
Theoretical density
SAN
 0.02 
 0.05 
 0.1 
 0.2 
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
U
V1
 0.02 
 0.05 
 0.1 
 0.2 
Empirical density
Theoretical density
Figure 9.17: Contour plots with standard normal margins of the DLM ﬁltered copula
data set of 8 bank stocks and posterior samples of latent variable V1 for daily log re-
turns from years 2005 to 2013. 1 000 samples of latent variable V1 were generated by an
individual ARMS Gibbs sampler for a one-factor copula model with Gaussian linking cop-
ulas. The mean over all iterations of the Gibbs sampler is used for latent variable V1 and
the copula parameters. Empirical densities are indicated by solid black lines, theoretical
bivariate Gaussian copula densities with corresponding copula parameters by dotted blue
lines.
136

9.4
Bayesian inference for the one-factor copula model
ACA
 0.02 
 0.05 
 0.1 
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
U
V1
 0.02 
 0.05 
 0.1 
 0.2 
Empirical density
Theoretical density
BBVA
 0.02 
 0.05 
 0.1 
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
U
V1
 0.02 
 0.05 
 0.1 
 0.2 
Empirical density
Theoretical density
BNP
 0.02 
 0.05 
 0.1 
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
U
V1
 0.02 
 0.05 
 0.1 
 0.2 
Empirical density
Theoretical density
CBK
 0.02 
 0.05 
 0.1 
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
U
V1
 0.02 
 0.05 
 0.1 
Empirical density
Theoretical density
DBK
 0.02 
 0.05 
 0.1 
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
U
V1
 0.02 
 0.05 
 0.1 
 0.2 
Empirical density
Theoretical density
GLE
 0.02 
 0.05 
 0.1 
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
U
V1
 0.02 
 0.05 
 0.1 
 0.2 
Empirical density
Theoretical density
ISP
 0.02 
 0.05 
 0.1 
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
U
V1
 0.02 
 0.05 
 0.1 
Empirical density
Theoretical density
SAN
 0.02 
 0.05 
 0.1 
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
U
V1
 0.02 
 0.05 
 0.1 
 0.2 
Empirical density
Theoretical density
Figure 9.18: Contour plots with standard normal margins of the DLM ﬁltered copula
data set of 8 bank stocks and posterior samples of latent variable V1 for daily log returns
from year 2005.
1 000 samples of latent variable V1 were generated by an individual
ARMS Gibbs sampler for a one-factor copula model with Gaussian linking copulas. The
mean over all iterations of the Gibbs sampler is used for latent variable V1 and the copula
parameters. Empirical densities are indicated by solid black lines, theoretical bivariate
Gaussian copula densities with corresponding copula parameters by dotted blue lines.
137

9
Empirical study
Additional contour plots for Gumbel and survival Gumbel linking copulas are shown in
Appendix B.5.4, Figures B.36–B.43.
138

9.5
Value-at-Risk and expected shortfall forecasts
9.5
Value-at-Risk and expected shortfall forecasts
In this section, backtesting performance of Bayesian variants of the one-factor copula
model and vine copula models is compared. Since VaR and ES are the most prominent
risk measures in practice, they are utilized to examine the diﬀerences in the multivariate
dependence models. Considering the aim is a wholly Bayesian forecast that incorporates
parameter uncertainty, daily log returns without indices, ﬁltered by marginal time-varying
ARMA(1, 1) DLMs, are chosen as underlying copula data, and the same DLMs are also
used for marginal forecasts of each stock.
For the factor copula models, the individual ARMS Gibbs sampler for the one-factor
copula model with bivariate Gaussian, Gumbel and survival Gumbel linking copulas (see
Section 9.4) is used. In order to generate copula data, H1, . . . , H8 of each iteration of
the Gibbs sampler are transformed to copula parameters θ1, . . . , θ8 of the corresponding
family and inserted, in conjunction with the inverse of the conditional distribution, into
Algorithm 8.1.
For the vine copula models, the family-restricted R-vine and C-vine truncated at level
2, RVR and cv2R, respectively, are investigated (see Section 9.3.1). RVR emerged as the
best-ﬁtting vine copula with respect to BIC, and cv2 is structurally close to the two-factor
copula model, and is therefore utilized as a point for comparison. In contrast to the pre-
vious maximum likelihood estimation of parameters of the vine copulas, for the following
forecasts, parameters are drawn by MCMC, after the vine structure and copula families
have been chosen. As previously, the structure and families are determined by ﬁtting vine
copula models to the copula data of individual years with the RVineStructureSelect
function. Bayesian samples of the parameters of the vine copulas are drawn by the C++
implementation of the within-model method of Gruber and Czado (2015), and then used
to simulate from the corresponding vine. For this thesis, both the posterior samples and
copula data of the vine copulas were kindly provided by Lutz Gruber. Details of the under-
lying vine copula models of the yearly data are given in Appendix B.6.1, Tables B.17–B.20.
For both vine copula and factor copula models, the ﬁt of the model to the copula data
of the previous year is used to generate daily forecasts for the current year, i.e., models
based on data from years 2005 to 2012 produce forecasts for years 2006 to 2013. For each
day, a total of 1 000 samples, after burn-in, are produced, oﬀering a reasonable amount
of samples for VaR and ES at conﬁdence level 90%. Although regulation often requires
higher conﬁdence levels such as 99%, 99.5% or even 99.9%, producing reliable estimates
at these levels is only statistically feasible for very high amounts of samples (see, e.g.,
McNeil et al., 2005, Example 7.15).
9.5.1
Portfolio composition
Empirical and forecast VaR and ES are calculated on the basis of a portfolio consisting
of the eight bank stocks of Section 9.1.
For simplicity, the daily readjusted constant
mix strategy with equal weights in all stocks is analyzed, although arbitrary investment
strategies are possible. Daily portfolio log returns are viewed as relative returns, i.e.,
changes in the total value of the portfolio are not explicitly incorporated. The strategy is
explained in full detail below.
Before the ﬁrst trading day of year 2005, the initial monetary amount is split into eighths,
and one eighth invested into each of the eight bank stocks. After each of the following
139

9
Empirical study
2006
2008
2010
2012
2014
0.5
1.0
1.5
Portfolio value
Time
Relative portfolio value
2006
2008
2010
2012
2014
−0.15
−0.05
0.05
0.15
Portfolio daily log returns
Time
Daily log return
Empirical 90% VaR
Empirical 90% ES
Figure 9.19: Historical relative portfolio value of a constant mix strategy with equal
weights in the 8 bank stocks ACA, BBVA, BNP, CBK, DBK, GLE, ISP and SAN for
years 2005 to 2013. Portfolio weights are readjusted daily.
2 345 trading days, the portfolio is readjusted such that the relative portfolio weight is
again one eighth in each stock. Hence, the amount invested in each stock is equal and
constant at the start of each trading day.
The historical portfolio value and daily log returns are shown in Figure 9.19. After an
initial gain, the ﬁnancial crisis roughly halved the portfolio value, leading to a ﬁnal loss
for the investment. The majority of VaR and ES violations occur in the years 2008 to
2010 and later half of 2011, due to clusters of high losses. The empirical 90% VaR and
ES of the portfolio in this time frame were 2.43% and 4.36%, respectively.
9.5.2
Value-at-Risk and expected shortfall backtests
For each trading day t, t = 1, . . . , 2 085, in the years 2006 to 2013, based on 1 000 MCMC
samples of the copula parameters of the corresponding model, 1 000 samples of copula
data ui = (ui
j)j=1,...,8 = (utj)i
t=1,...,2085,j=1,...,8, i = 1, . . . , 1 000 are generated by simulation
from the R-vine and truncated C-vine copulas, and the individual ARMS Gibbs sampler
for the one-factor copula model with Gumbel, Gaussian and survival Gumbel linking
copulas. Margins ui
j, of the resulting copula data, are in turn input into the inverse of
the forecast distribution of the corresponding marginal time-varying ARMA(1, 1) DLMs,
given by the inverse Student’s t distribution function (see Table 9.6) to generate forecast
returns bri
j. This enables the calculation of a forecast for the portfolio return bpi
t for each
iteration i, i = 1, . . . , 1 000 and day t, t = 1, . . . , 2 085, given by
bpi
t = 1
8
8
X
j=1
ri
tj.
Finally, 90% VaR and ES forecasts are derived by applying their corresponding empirical
formulas (see Section 3.2) to bpt = (bpi
t)i=1,...,1000 for each day t.
As part of the backtesting procedure, the formal tests of Section 3.2.1 are used to assess
the performance of VaR forecasts. Backtests for ES are reduced to comparing forecast to
empirical values, due to the diﬃculties of backtesting ES brieﬂy discussed in Section 3.2.2.
140

9.5
Value-at-Risk and expected shortfall forecasts
Vine copulas
One-factor copula models
RVR
cv2R
Gumbel
Gaussian
s. Gumbel
Empirical
90% VaR violations
9.64%
9.88%
10.17%
9.59%
9.35%
10.02%
90% ES
4.07%
4.03%
4.00%
4.10%
4.13%
4.36%
Uncond. coverage test
0.58
0.85
0.80
0.53
0.32
-
Indep. test
0.11
0.04
0.21
0.16
0.06
-
Cond. coverage test
0.24
0.13
0.44
0.30
0.10
-
Table 9.19: Percentages of 90% VaR violations, and p-values of VaR backtests, and 90%
ES of daily log return forecasts of an equally weighted constant mix portfolio. From left to
right: Family-restricted R-vine copula (RVR), family-restricted C-vine copula truncated at
level 2 (cv2R), the one-factor copula model with Gumbel, Gaussian and survival Gumbel
(s. Gumbel) linking copulas, and the empirical values are shown. Forecasts are based on
1 000 MCMC samples for each day generated by the corresponding model ﬁt on the DLM
ﬁltered daily log return data of the previous year. A total of 2 085 daily forecasts are
calculated for years 2006 to 2013. Model-based forecasts that are closest to the empirical
values are emphasized in bold face.
Plots of the daily log returns of the portfolio with forecast 90% VaR and ES are shown
in Figure 9.20. All three methods produce visually nearly indistinguishable forecasts that
quickly adapt to changing volatility in the underlying returns. Table 9.19 conﬁrms that
all three methods perform similarly well. The percentages of VaR violations are close to
the theoretically optimal value of 10%, with the truncated C-vine exhibiting a slight lead
over the other two models. The p-values of the conditional coverage tests indicate that the
null hypothesis of independent VaR violations with probability 10% cannot be rejected
for any of the models at conﬁdence level α = 0.05. With respect to ES at conﬁdence level
90%, the Gibbs sampler for the one-factor copula model with survival Gumbel linking
copulas forecasts an ES that most closely matches the empirical value, though the values
for the vine copula models are only marginally lower. Overall, based on these results,
all three methods would be viable fully Bayesian models to forecast VaR and ES for this
portfolio, although ES forecasts are slightly too conservative.
141

9
Empirical study
2006
2008
2010
2012
2014
−0.14
−0.08
−0.02
R−vine copula
Time
Daily log return
90 % VaR forecast
90 % ES forecast
2006
2008
2010
2012
2014
−0.14
−0.08
−0.02
Truncated C−vine copula
Time
Daily log return
90 % VaR forecast
90 % ES forecast
2006
2008
2010
2012
2014
−0.14
−0.08
−0.02
One−factor model with Gumbel copulas
Time
Daily log return
90 % VaR forecast
90 % ES forecast
2006
2008
2010
2012
2014
−0.14
−0.08
−0.02
One−factor model with Gaussian copulas
Time
Daily log return
90 % VaR forecast
90 % ES forecast
2006
2008
2010
2012
2014
−0.14
−0.08
−0.02
One−factor model with survival Gumbel copulas
Time
Daily log return
90 % VaR forecast
90 % ES forecast
Figure 9.20:
Daily log returns of the equally weighted constant mix portfolio with
(negative) 90% VaR (blue lines) and ES (green lines) forecasts. Forecasts are based on
1 000 MCMC samples for each day generated by the corresponding model ﬁt to the DLM
ﬁltered daily log return data of the previous year. A total of 2 085 daily forecasts are
calculated for years 2006 to 2013.
142

10
Conclusions and outlook
The results of this thesis can be split into two distinct parts, the derivation of a Bayesian
analysis of the one-factor copula model and the analysis of a data set of European daily
stock log returns.
A Gibbs sampler for the Bayesian analysis of the one-factor copula model was successfully
implemented and a variety of sampling methods for the full conditional distributions
compared.
In terms of eﬃciency, measured by eﬀective sample size per minute, the
independence and random walk samplers performed best, followed by individual ARMS.
ARMS oﬀered greater ﬂexibility due to the generality of the sampling scheme, and higher
absolute eﬀective sample sizes at the cost of increased computational complexity.
Multivariate dependence analysis of the empirical data comprised two components. As
the ﬁrst component, marginal ARMA-GARCH and DLM time series models were used
to transform the data set of European daily log stock returns to standard uniform copula
data.
It was found that both the ARMA(1, 1)-GARCH(1, 1) model with Student’s t
innovations and the time-varying ARMA(1, 1) DLM oﬀered adequate performance for the
considered data set, although each model showed some deﬁciencies. While the ARMA-
GARCH model is too light in the lower tail speciﬁcally, the DLM exhibits an overly
heavy-tailed forecast distribution causing slight deviations from uniformity in the marginal
data. Even though the DLM oﬀered acceptable performance in the version presented in
this thesis, diﬀerent choices of parameters might further improve its performance, oﬀering
a compelling alternative to models that rely on maximum likelihood estimation.
In the multivariate dependence analysis, several vine copula structures, and one- and two-
factor copulas were examined. Within the vine copulas, non-truncated vine accomplished
superior ﬁts with respect to log-likelihood, AIC and BIC, indicating that truncation at
level 2 is not suitable for the data set of the empirical study. Changes from R-vine to
C-vine structures and restriction of the available families had only a minor inﬂuence.
Similarly, the more expansive two-factor copula model achieved better results than the
sparser one-factor copula model. In both cases Student’s t linking copulas proved a good
choice for the daily log returns. In a direct comparison of vine copulas and the two-factor
copula model, the models performed similarly well, with a slight advantage awarded to
the R-vine in BIC, although the two-factor copula model used far fewer parameters.
Backtests of Bayesian forecasts of VaR and ES at conﬁdence level 90% by vine and one-
factor copula models in conjunction with marginal DLM models revealed that the R-vine,
truncated C-vine and one-factor copula model all produced similarly good results. For the
90% VaR, the results of the C-vine truncated at level 2 matched the theoretical value most
closely, and the one-factor copula model with survival Gumbel linking copulas realized
the most accurate 90% ES forecast.
Going forward, an extension of the Bayesian analysis of factor copula models to the higher
dimensional cases of two or more latent factor appears fruitful. The two-factor copula
model ﬁt the empirical data set noticeable better than its counterpart with one factor,
suggesting room for straightforward improvements. Furthermore, additional choices of
bivariate linking copulas can be explored. In particular, varying choices for each pair
of latent factor and variable may more accurately represent data at the possible cost of
interpretability, and thus may be considered in the future.
143

A
Simulation study: Figures and tables
A
Simulation study: Figures and tables
In this part of the appendix, supplementary ﬁgures and tables of the Gibbs sampler
output of the simulation study of Section 8 are given. For all methods except thinned
individual ARMS, the Gibbs sampler was run for 3 000 iterations and the ﬁrst 1 000
iterations discarded as burn-in.
In the case of thinned individual ARMS, the Gibbs
sampler was run for 601 000 iterations, the ﬁrst 1 000 iterations discarded as burn-in, and
then the remaining samples thinned with factor 300, leading to a ﬁnal sample size of
2 000. Copula data was created by simulation from the one-factor copula model for three
diﬀerent scenarios of Kendall’s tau values and three diﬀerent choices of bivariate linking
copulas.
A.1
Traceplots and histograms
In this section, additional trace and density plots for all methods and scenarios of Kendall’s
tau values are shown to assess the convergence of the sampling methods within the Gibbs
sampler. The following plots show trace plots (left) and density plots (right), where the
posterior mode is indicated by a dotted red line and the true value of the corresponding
variable by a solid blue line. Due to the large number of posterior variables V1,1, . . . , V1,200
for V1, in case of the latent variable, only the subset V1,1, . . . , V1,5 is shown.
144

A.1
Traceplots and histograms
Individiual ARMS for V1
0
500
1000
1500
2000
0.0
0.4
0.8
Iteration
V1,1
Traceplot of V1,1
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
1.0
0
1
2
3
4
V1,1
Density
Histogram of V1,1
True value
Posterior mode
0
500
1000
1500
2000
0.0
0.4
0.8
Iteration
V1,2
Traceplot of V1,2
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
1.0
0
1
2
3
4
V1,2
Density
Histogram of V1,2
True value
Posterior mode
0
500
1000
1500
2000
0.0
0.4
0.8
Iteration
V1,3
Traceplot of V1,3
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
1.0
0
1
2
3
4
V1,3
Density
Histogram of V1,3
True value
Posterior mode
0
500
1000
1500
2000
0.0
0.4
0.8
Iteration
V1,4
Traceplot of V1,4
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
1.0
0
1
2
3
4
V1,4
Density
Histogram of V1,4
True value
Posterior mode
0
500
1000
1500
2000
0.0
0.4
0.8
Iteration
V1,5
Traceplot of V1,5
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
1.0
0
1
2
3
4
V1,5
Density
Histogram of V1,5
True value
Posterior mode
Figure A.1: Trace and density plots of the output from a single run of the individual
ARMS Gibbs sampler based on simulated data from a one-factor copula model with
Gumbel linking copulas (n = 200 observations, dimension d = 5) for the low Kendall’s
tau values scenario. The subset V1,1, . . . , V1,5 of V1,1, . . . , V1,200 is shown.
145

A
Simulation study: Figures and tables
Block ARMS for V1
0
500
1000
1500
2000
0.0
0.4
0.8
Iteration
V1,1
Traceplot of V1,1
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
1.0
0
5
15
25
V1,1
Density
Histogram of V1,1
True value
Posterior mode
0
500
1000
1500
2000
0.0
0.4
0.8
Iteration
V1,2
Traceplot of V1,2
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
1.0
0
5
15
25
V1,2
Density
Histogram of V1,2
True value
Posterior mode
0
500
1000
1500
2000
0.0
0.4
0.8
Iteration
V1,3
Traceplot of V1,3
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
1.0
0
5
15
25
V1,3
Density
Histogram of V1,3
True value
Posterior mode
0
500
1000
1500
2000
0.0
0.4
0.8
Iteration
V1,4
Traceplot of V1,4
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
1.0
0
5
15
25
V1,4
Density
Histogram of V1,4
True value
Posterior mode
0
500
1000
1500
2000
0.0
0.4
0.8
Iteration
V1,5
Traceplot of V1,5
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
1.0
0
5
15
25
V1,5
Density
Histogram of V1,5
True value
Posterior mode
Figure A.2: Trace and density plots of a single run of the block ARMS Gibbs sampler
based on simulated data from a one-factor copula model with Gumbel linking copulas
(n = 200 observations, dimension d = 5) for the low Kendall’s tau values scenario. The
subset V1,1, . . . , V1,5 of V1,1, . . . , V1,200 is shown.
146

A.1
Traceplots and histograms
MH with mode and curvature matching for V1
0
500
1000
1500
2000
0.0
0.4
0.8
Iteration
V1,1
Traceplot of V1,1
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
1.0
0
5
15
25
V1,1
Density
Histogram of V1,1
True value
Posterior mode
0
500
1000
1500
2000
0.0
0.4
0.8
Iteration
V1,2
Traceplot of V1,2
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
1.0
0
5
15
25
V1,2
Density
Histogram of V1,2
True value
Posterior mode
0
500
1000
1500
2000
0.0
0.4
0.8
Iteration
V1,3
Traceplot of V1,3
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
1.0
0
5
15
25
V1,3
Density
Histogram of V1,3
True value
Posterior mode
0
500
1000
1500
2000
0.0
0.4
0.8
Iteration
V1,4
Traceplot of V1,4
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
1.0
0
5
15
25
V1,4
Density
Histogram of V1,4
True value
Posterior mode
0
500
1000
1500
2000
0.0
0.4
0.8
Iteration
V1,5
Traceplot of V1,5
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
1.0
0
5
15
25
V1,5
Density
Histogram of V1,5
True value
Posterior mode
Figure A.3: Trace and density plots of a single run of the mode and curvature matching
Gibbs sampler based on simulated data from a one-factor copula model with Gumbel
linking copulas (n = 200 observations, dimension d = 5) for the low Kendall’s tau values
scenario. The subset V1,1, . . . , V1,5 of V1,1, . . . , V1,200 is shown.
147

A
Simulation study: Figures and tables
MH with expectation and variance matching for V1
0
500
1000
1500
2000
0.0
0.4
0.8
Iteration
V1,1
Traceplot of V1,1
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
1.0
0
1
2
3
4
V1,1
Density
Histogram of V1,1
True value
Posterior mode
0
500
1000
1500
2000
0.0
0.4
0.8
Iteration
V1,2
Traceplot of V1,2
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
1.0
0
1
2
3
4
V1,2
Density
Histogram of V1,2
True value
Posterior mode
0
500
1000
1500
2000
0.0
0.4
0.8
Iteration
V1,3
Traceplot of V1,3
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
1.0
0
1
2
3
4
V1,3
Density
Histogram of V1,3
True value
Posterior mode
0
500
1000
1500
2000
0.0
0.4
0.8
Iteration
V1,4
Traceplot of V1,4
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
1.0
0
1
2
3
4
V1,4
Density
Histogram of V1,4
True value
Posterior mode
0
500
1000
1500
2000
0.0
0.4
0.8
Iteration
V1,5
Traceplot of V1,5
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
1.0
0
1
2
3
4
V1,5
Density
Histogram of V1,5
True value
Posterior mode
Figure A.4: Trace and density plots of a single run of the expectation and variance
matching Gibbs sampler based on simulated data from a one-factor copula model with
Gumbel linking copulas (n = 200 observations, dimension d = 5) for the low Kendall’s
tau values scenario. The subset V1,1, . . . , V1,5 of V1,1, . . . , V1,200 is shown.
148

A.1
Traceplots and histograms
Independence and random walk sampler for V1
0
500
1000
1500
2000
0.0
0.4
0.8
Iteration
V1,1
Traceplot of V1,1
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
1.0
0.0
1.0
2.0
3.0
V1,1
Density
Histogram of V1,1
True value
Posterior mode
0
500
1000
1500
2000
0.0
0.4
0.8
Iteration
V1,2
Traceplot of V1,2
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
1.0
0.0
1.0
2.0
3.0
V1,2
Density
Histogram of V1,2
True value
Posterior mode
0
500
1000
1500
2000
0.0
0.4
0.8
Iteration
V1,3
Traceplot of V1,3
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
1.0
0.0
1.0
2.0
3.0
V1,3
Density
Histogram of V1,3
True value
Posterior mode
0
500
1000
1500
2000
0.0
0.4
0.8
Iteration
V1,4
Traceplot of V1,4
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
1.0
0.0
1.0
2.0
3.0
V1,4
Density
Histogram of V1,4
True value
Posterior mode
0
500
1000
1500
2000
0.0
0.4
0.8
Iteration
V1,5
Traceplot of V1,5
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
1.0
0.0
1.0
2.0
3.0
V1,5
Density
Histogram of V1,5
True value
Posterior mode
Figure A.5: Trace and density plots of a single run of the independence and random
walk Gibbs sampler based on simulated data from a one-factor copula model with Gumbel
linking copulas (n = 200 observations, dimension d = 5) for the low Kendall’s tau values
scenario. The subset V1,1, . . . , V1,5 of V1,1, . . . , V1,200 is shown.
149

A
Simulation study: Figures and tables
Thinned ARMS sample of V1
0
500
1000
1500
2000
0.0
0.4
0.8
Iteration
V1,1
Traceplot of V1,1
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
1.0
0
5
15
25
V1,1
Density
Histogram of V1,1
True value
Posterior mode
0
500
1000
1500
2000
0.0
0.4
0.8
Iteration
V1,2
Traceplot of V1,2
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
1.0
0
5
15
25
V1,2
Density
Histogram of V1,2
True value
Posterior mode
0
500
1000
1500
2000
0.0
0.4
0.8
Iteration
V1,3
Traceplot of V1,3
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
1.0
0
5
15
25
V1,3
Density
Histogram of V1,3
True value
Posterior mode
0
500
1000
1500
2000
0.0
0.4
0.8
Iteration
V1,4
Traceplot of V1,4
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
1.0
0
5
15
25
V1,4
Density
Histogram of V1,4
True value
Posterior mode
0
500
1000
1500
2000
0.0
0.4
0.8
Iteration
V1,5
Traceplot of V1,5
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
1.0
0
5
15
25
V1,5
Density
Histogram of V1,5
True value
Posterior mode
Figure A.6: Trace and density plots of the thinned (thin=300) output from the indi-
vidual ARMS Gibbs sampler based on simulated data from a one-factor copula model with
Gumbel linking copulas (n = 200 observations, dimension d = 5) for the low Kendall’s
tau values scenario. The subset V1,1, . . . , V1,5 of V1,1, . . . , V1,200 is shown.
150

A.1
Traceplots and histograms
Individiual ARMS for H
0
500
1000
1500
2000
0.0
0.4
0.8
Iteration
H1
Traceplot of H1
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
1.0
0
1
2
3
4
5
6
H1
Density
Histogram of H1
True value
Posterior mode
0
500
1000
1500
2000
0.0
0.4
0.8
Iteration
H2
Traceplot of H2
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
1.0
0
1
2
3
4
5
6
H2
Density
Histogram of H2
True value
Posterior mode
0
500
1000
1500
2000
0.0
0.4
0.8
Iteration
H3
Traceplot of H3
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
1.0
0
1
2
3
4
5
6
H3
Density
Histogram of H3
True value
Posterior mode
0
500
1000
1500
2000
0.0
0.4
0.8
Iteration
H4
Traceplot of H4
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
1.0
0
1
2
3
4
5
6
H4
Density
Histogram of H4
True value
Posterior mode
0
500
1000
1500
2000
0.0
0.4
0.8
Iteration
H5
Traceplot of H5
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
1.0
0
1
2
3
4
5
6
H5
Density
Histogram of H5
True value
Posterior mode
Figure A.7: Trace and density plots of a single run of the individual ARMS Gibbs
sampler based on simulated data from a one-factor copula model with Gumbel linking
copulas (n = 200 observations, dimension d = 5) for the low Kendall’s tau values scenario.
The full set H1, . . . , H5 is shown.
151

A
Simulation study: Figures and tables
Block ARMS for H
0
500
1000
1500
2000
−0.1
0.1
0.3
Iteration
H1
Traceplot of H1
True value
Posterior mode
−0.1
0.0
0.1
0.2
0.3
0.4
0
5
10
15
H1
Density
Histogram of H1
True value
Posterior mode
0
500
1000
1500
2000
−0.1
0.1
0.3
Iteration
H2
Traceplot of H2
True value
Posterior mode
−0.1
0.0
0.1
0.2
0.3
0.4
0
5
10
15
H2
Density
Histogram of H2
True value
Posterior mode
0
500
1000
1500
2000
−0.1
0.1
0.3
Iteration
H3
Traceplot of H3
True value
Posterior mode
−0.1
0.0
0.1
0.2
0.3
0.4
0
5
10
15
H3
Density
Histogram of H3
True value
Posterior mode
0
500
1000
1500
2000
−0.1
0.1
0.3
Iteration
H4
Traceplot of H4
True value
Posterior mode
−0.1
0.0
0.1
0.2
0.3
0.4
0
5
10
15
H4
Density
Histogram of H4
True value
Posterior mode
0
500
1000
1500
2000
−0.1
0.1
0.3
Iteration
H5
Traceplot of H5
True value
Posterior mode
−0.1
0.0
0.1
0.2
0.3
0.4
0
5
10
15
H5
Density
Histogram of H5
True value
Posterior mode
Figure A.8: Trace and density plots of a single run of the block ARMS Gibbs sampler
based on simulated data from a one-factor copula model with Gumbel linking copulas
(n = 200 observations, dimension d = 5) for the low Kendall’s tau values scenario. The
full set H1, . . . , H5 is shown.
152

A.1
Traceplots and histograms
MH with mode and curvature matching for H
0
500
1000
1500
2000
0.0
1.0
2.0
Iteration
H1
Traceplot of H1
True value
Posterior mode
0.0
0.5
1.0
1.5
2.0
2.5
0
2
4
6
8
10
H1
Density
Histogram of H1
True value
Posterior mode
0
500
1000
1500
2000
0.0
1.0
2.0
Iteration
H2
Traceplot of H2
True value
Posterior mode
0.0
0.5
1.0
1.5
2.0
2.5
0
2
4
6
8
10
H2
Density
Histogram of H2
True value
Posterior mode
0
500
1000
1500
2000
0.0
1.0
2.0
Iteration
H3
Traceplot of H3
True value
Posterior mode
0.0
0.5
1.0
1.5
2.0
2.5
0
2
4
6
8
10
H3
Density
Histogram of H3
True value
Posterior mode
0
500
1000
1500
2000
0.0
1.0
2.0
Iteration
H4
Traceplot of H4
True value
Posterior mode
0.0
0.5
1.0
1.5
2.0
2.5
0
2
4
6
8
10
H4
Density
Histogram of H4
True value
Posterior mode
0
500
1000
1500
2000
0.0
1.0
2.0
Iteration
H5
Traceplot of H5
True value
Posterior mode
0.0
0.5
1.0
1.5
2.0
2.5
0
2
4
6
8
10
H5
Density
Histogram of H5
True value
Posterior mode
Figure A.9: Trace and density plots of a single run of the mode and curvature matching
Gibbs sampler based on simulated data from a one-factor copula model with Gumbel
linking copulas (n = 200 observations, dimension d = 5) for the low Kendall’s tau values
scenario. The full set H1, . . . , H5 is shown.
153

A
Simulation study: Figures and tables
MH with expectation and variance matching for H
0
500
1000
1500
2000
0.0
0.4
0.8
Iteration
H1
Traceplot of H1
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
0
1
2
3
4
5
H1
Density
Histogram of H1
True value
Posterior mode
0
500
1000
1500
2000
0.0
0.4
0.8
Iteration
H2
Traceplot of H2
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
0
1
2
3
4
5
H2
Density
Histogram of H2
True value
Posterior mode
0
500
1000
1500
2000
0.0
0.4
0.8
Iteration
H3
Traceplot of H3
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
0
1
2
3
4
5
H3
Density
Histogram of H3
True value
Posterior mode
0
500
1000
1500
2000
0.0
0.4
0.8
Iteration
H4
Traceplot of H4
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
0
1
2
3
4
5
H4
Density
Histogram of H4
True value
Posterior mode
0
500
1000
1500
2000
0.0
0.4
0.8
Iteration
H5
Traceplot of H5
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
0
1
2
3
4
5
H5
Density
Histogram of H5
True value
Posterior mode
Figure A.10: Trace and density plots of a single run of the expectation and variance
matching Gibbs sampler based on simulated data from a one-factor copula model with
Gumbel linking copulas (n = 200 observations, dimension d = 5) for the low Kendall’s
tau values scenario. The full set H1, . . . , H5 is shown.
154

A.1
Traceplots and histograms
Independence and random walk sampler for H
0
500
1000
1500
2000
0.0
0.4
0.8
Iteration
H1
Traceplot of H1
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
1.0
0
2
4
6
H1
Density
Histogram of H1
True value
Posterior mode
0
500
1000
1500
2000
0.0
0.4
0.8
Iteration
H2
Traceplot of H2
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
1.0
0
2
4
6
H2
Density
Histogram of H2
True value
Posterior mode
0
500
1000
1500
2000
0.0
0.4
0.8
Iteration
H3
Traceplot of H3
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
1.0
0
2
4
6
H3
Density
Histogram of H3
True value
Posterior mode
0
500
1000
1500
2000
0.0
0.4
0.8
Iteration
H4
Traceplot of H4
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
1.0
0
2
4
6
H4
Density
Histogram of H4
True value
Posterior mode
0
500
1000
1500
2000
0.0
0.4
0.8
Iteration
H5
Traceplot of H5
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
1.0
0
2
4
6
H5
Density
Histogram of H5
True value
Posterior mode
Figure A.11: Trace and density plots of a single run of the independence and random
walk Gibbs sampler based on simulated data from a one-factor copula model with Gumbel
linking copulas (n = 200 observations, dimension d = 5) for the low Kendall’s tau values
scenario. The full set H1, . . . , H5 is shown.
155

A
Simulation study: Figures and tables
Thinned ARMS sample of H
0
500
1000
1500
2000
0.0
1.0
2.0
Iteration
H1
Traceplot of H1
True value
Posterior mode
0.0
0.5
1.0
1.5
2.0
2.5
0
2
4
6
8
10
H1
Density
Histogram of H1
True value
Posterior mode
0
500
1000
1500
2000
0.0
1.0
2.0
Iteration
H2
Traceplot of H2
True value
Posterior mode
0.0
0.5
1.0
1.5
2.0
2.5
0
2
4
6
8
10
H2
Density
Histogram of H2
True value
Posterior mode
0
500
1000
1500
2000
0.0
1.0
2.0
Iteration
H3
Traceplot of H3
True value
Posterior mode
0.0
0.5
1.0
1.5
2.0
2.5
0
2
4
6
8
10
H3
Density
Histogram of H3
True value
Posterior mode
0
500
1000
1500
2000
0.0
1.0
2.0
Iteration
H4
Traceplot of H4
True value
Posterior mode
0.0
0.5
1.0
1.5
2.0
2.5
0
2
4
6
8
10
H4
Density
Histogram of H4
True value
Posterior mode
0
500
1000
1500
2000
0.0
1.0
2.0
Iteration
H5
Traceplot of H5
True value
Posterior mode
0.0
0.5
1.0
1.5
2.0
2.5
0
2
4
6
8
10
H5
Density
Histogram of H5
True value
Posterior mode
Figure A.12: Trace and density plots of the thinned (thin=300) output from the indi-
vidual ARMS Gibbs sampler based on simulated data from a one-factor copula model with
Gumbel linking copulas (n = 200 observations, dimension d = 5) for the low Kendall’s
tau values scenario. The full set H1, . . . , H5 is shown.
156

A.1
Traceplots and histograms
Individiual ARMS for V1
0
500
1000
1500
2000
0.0
0.4
0.8
Iteration
V1,1
Traceplot of V1,1
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
1.0
0
5
10
15
20
25
V1,1
Density
Histogram of V1,1
True value
Posterior mode
0
500
1000
1500
2000
0.0
0.4
0.8
Iteration
V1,2
Traceplot of V1,2
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
1.0
0
5
10
15
20
25
V1,2
Density
Histogram of V1,2
True value
Posterior mode
0
500
1000
1500
2000
0.0
0.4
0.8
Iteration
V1,3
Traceplot of V1,3
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
1.0
0
5
10
15
20
25
V1,3
Density
Histogram of V1,3
True value
Posterior mode
0
500
1000
1500
2000
0.0
0.4
0.8
Iteration
V1,4
Traceplot of V1,4
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
1.0
0
5
10
15
20
25
V1,4
Density
Histogram of V1,4
True value
Posterior mode
0
500
1000
1500
2000
0.0
0.4
0.8
Iteration
V1,5
Traceplot of V1,5
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
1.0
0
5
10
15
20
25
V1,5
Density
Histogram of V1,5
True value
Posterior mode
Figure A.13: Trace and density plots of a single run of the individual ARMS Gibbs
sampler based on simulated data from a one-factor copula model with Gumbel linking
copulas (n = 200 observations, dimension d = 5) for the high Kendall’s tau values scenario.
The subset V1,1, . . . , V1,5 of V1,1, . . . , V1,200 is shown.
157

A
Simulation study: Figures and tables
Block ARMS for V1
0
500
1000
1500
2000
0.0
0.4
0.8
Iteration
V1,1
Traceplot of V1,1
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
1.0
0
5
10
15
V1,1
Density
Histogram of V1,1
True value
Posterior mode
0
500
1000
1500
2000
0.0
0.4
0.8
Iteration
V1,2
Traceplot of V1,2
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
1.0
0
5
10
15
V1,2
Density
Histogram of V1,2
True value
Posterior mode
0
500
1000
1500
2000
0.0
0.4
0.8
Iteration
V1,3
Traceplot of V1,3
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
1.0
0
5
10
15
V1,3
Density
Histogram of V1,3
True value
Posterior mode
0
500
1000
1500
2000
0.0
0.4
0.8
Iteration
V1,4
Traceplot of V1,4
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
1.0
0
5
10
15
V1,4
Density
Histogram of V1,4
True value
Posterior mode
0
500
1000
1500
2000
0.0
0.4
0.8
Iteration
V1,5
Traceplot of V1,5
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
1.0
0
5
10
15
V1,5
Density
Histogram of V1,5
True value
Posterior mode
Figure A.14: Trace and density plots of a single run of the block ARMS Gibbs sampler
based on simulated data from a one-factor copula model with Gumbel linking copulas
(n = 200 observations, dimension d = 5) for the high Kendall’s tau values scenario. The
subset V1,1, . . . , V1,5 of V1,1, . . . , V1,200 is shown.
158

A.1
Traceplots and histograms
MH with mode and curvature matching for V1
0
500
1000
1500
2000
0.0
0.4
0.8
Iteration
V1,1
Traceplot of V1,1
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
1.0
0
5
15
25
V1,1
Density
Histogram of V1,1
True value
Posterior mode
0
500
1000
1500
2000
0.0
0.4
0.8
Iteration
V1,2
Traceplot of V1,2
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
1.0
0
5
15
25
V1,2
Density
Histogram of V1,2
True value
Posterior mode
0
500
1000
1500
2000
0.0
0.4
0.8
Iteration
V1,3
Traceplot of V1,3
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
1.0
0
5
15
25
V1,3
Density
Histogram of V1,3
True value
Posterior mode
0
500
1000
1500
2000
0.0
0.4
0.8
Iteration
V1,4
Traceplot of V1,4
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
1.0
0
5
15
25
V1,4
Density
Histogram of V1,4
True value
Posterior mode
0
500
1000
1500
2000
0.0
0.4
0.8
Iteration
V1,5
Traceplot of V1,5
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
1.0
0
5
15
25
V1,5
Density
Histogram of V1,5
True value
Posterior mode
Figure A.15: Trace and density plots of a single run of the mode and curvature matching
Gibbs sampler based on simulated data from a one-factor copula model with Gumbel
linking copulas (n = 200 observations, dimension d = 5) for the high Kendall’s tau values
scenario. The subset V1,1, . . . , V1,5 of V1,1, . . . , V1,200 is shown.
159

A
Simulation study: Figures and tables
MH with expectation and variance matching for V1
0
500
1000
1500
2000
0.0
0.4
0.8
Iteration
V1,1
Traceplot of V1,1
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
1.0
0
5
10
20
V1,1
Density
Histogram of V1,1
True value
Posterior mode
0
500
1000
1500
2000
0.0
0.4
0.8
Iteration
V1,2
Traceplot of V1,2
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
1.0
0
5
10
20
V1,2
Density
Histogram of V1,2
True value
Posterior mode
0
500
1000
1500
2000
0.0
0.4
0.8
Iteration
V1,3
Traceplot of V1,3
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
1.0
0
5
10
20
V1,3
Density
Histogram of V1,3
True value
Posterior mode
0
500
1000
1500
2000
0.0
0.4
0.8
Iteration
V1,4
Traceplot of V1,4
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
1.0
0
5
10
20
V1,4
Density
Histogram of V1,4
True value
Posterior mode
0
500
1000
1500
2000
0.0
0.4
0.8
Iteration
V1,5
Traceplot of V1,5
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
1.0
0
5
10
20
V1,5
Density
Histogram of V1,5
True value
Posterior mode
Figure A.16: Trace and density plots of a single run of the expectation and variance
matching Gibbs sampler based on simulated data from a one-factor copula model with
Gumbel linking copulas (n = 200 observations, dimension d = 5) for the high Kendall’s
tau values scenario. The subset V1,1, . . . , V1,5 of V1,1, . . . , V1,200 is shown.
160

A.1
Traceplots and histograms
Independence and random walk sampler for V1
0
500
1000
1500
2000
0.0
0.4
0.8
Iteration
V1,1
Traceplot of V1,1
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
1.0
0
5
15
25
V1,1
Density
Histogram of V1,1
True value
Posterior mode
0
500
1000
1500
2000
0.0
0.4
0.8
Iteration
V1,2
Traceplot of V1,2
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
1.0
0
5
15
25
V1,2
Density
Histogram of V1,2
True value
Posterior mode
0
500
1000
1500
2000
0.0
0.4
0.8
Iteration
V1,3
Traceplot of V1,3
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
1.0
0
5
15
25
V1,3
Density
Histogram of V1,3
True value
Posterior mode
0
500
1000
1500
2000
0.0
0.4
0.8
Iteration
V1,4
Traceplot of V1,4
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
1.0
0
5
15
25
V1,4
Density
Histogram of V1,4
True value
Posterior mode
0
500
1000
1500
2000
0.0
0.4
0.8
Iteration
V1,5
Traceplot of V1,5
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
1.0
0
5
15
25
V1,5
Density
Histogram of V1,5
True value
Posterior mode
Figure A.17: Trace and density plots of a single run of the independence and random
walk Gibbs sampler based on simulated data from a one-factor copula model with Gumbel
linking copulas (n = 200 observations, dimension d = 5) for the high Kendall’s tau values
scenario. The subset V1,1, . . . , V1,5 of V1,1, . . . , V1,200 is shown.
161

A
Simulation study: Figures and tables
Thinned ARMS sample of V1
0
500
1000
1500
2000
0.0
0.4
0.8
Iteration
V1,1
Traceplot of V1,1
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
1.0
0
5
10
20
V1,1
Density
Histogram of V1,1
True value
Posterior mode
0
500
1000
1500
2000
0.0
0.4
0.8
Iteration
V1,2
Traceplot of V1,2
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
1.0
0
5
10
20
V1,2
Density
Histogram of V1,2
True value
Posterior mode
0
500
1000
1500
2000
0.0
0.4
0.8
Iteration
V1,3
Traceplot of V1,3
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
1.0
0
5
10
20
V1,3
Density
Histogram of V1,3
True value
Posterior mode
0
500
1000
1500
2000
0.0
0.4
0.8
Iteration
V1,4
Traceplot of V1,4
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
1.0
0
5
10
20
V1,4
Density
Histogram of V1,4
True value
Posterior mode
0
500
1000
1500
2000
0.0
0.4
0.8
Iteration
V1,5
Traceplot of V1,5
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
1.0
0
5
10
20
V1,5
Density
Histogram of V1,5
True value
Posterior mode
Figure A.18: Trace and density plots of the thinned (thin=300) output from the indi-
vidual ARMS Gibbs sampler based on simulated data from a one-factor copula model with
Gumbel linking copulas (n = 200 observations, dimension d = 5) for the high Kendall’s
tau values scenario. The subset V1,1, . . . , V1,5 of V1,1, . . . , V1,200 is shown.
162

A.1
Traceplots and histograms
Individiual ARMS for H
0
500
1000
1500
2000
0.4
0.8
1.2
Iteration
H1
Traceplot of H1
True value
Posterior mode
0.4
0.6
0.8
1.0
1.2
1.4
0
2
4
6
8
H1
Density
Histogram of H1
True value
Posterior mode
0
500
1000
1500
2000
0.4
0.8
1.2
Iteration
H2
Traceplot of H2
True value
Posterior mode
0.4
0.6
0.8
1.0
1.2
1.4
0
2
4
6
8
H2
Density
Histogram of H2
True value
Posterior mode
0
500
1000
1500
2000
0.4
0.8
1.2
Iteration
H3
Traceplot of H3
True value
Posterior mode
0.4
0.6
0.8
1.0
1.2
1.4
0
2
4
6
8
H3
Density
Histogram of H3
True value
Posterior mode
0
500
1000
1500
2000
0.4
0.8
1.2
Iteration
H4
Traceplot of H4
True value
Posterior mode
0.4
0.6
0.8
1.0
1.2
1.4
0
2
4
6
8
H4
Density
Histogram of H4
True value
Posterior mode
0
500
1000
1500
2000
0.4
0.8
1.2
Iteration
H5
Traceplot of H5
True value
Posterior mode
0.4
0.6
0.8
1.0
1.2
1.4
0
2
4
6
8
H5
Density
Histogram of H5
True value
Posterior mode
Figure A.19: Trace and density plots of a single run of the individual ARMS Gibbs
sampler based on simulated data from a one-factor copula model with Gumbel linking
copulas (n = 200 observations, dimension d = 5) for the high Kendall’s tau values scenario.
The full set H1, . . . , H5 is shown.
163

A
Simulation study: Figures and tables
Block ARMS for H
0
500
1000
1500
2000
0.2
0.6
1.0
Iteration
H1
Traceplot of H1
True value
Posterior mode
0.2
0.4
0.6
0.8
1.0
1.2
0
2
4
6
H1
Density
Histogram of H1
True value
Posterior mode
0
500
1000
1500
2000
0.2
0.6
1.0
Iteration
H2
Traceplot of H2
True value
Posterior mode
0.2
0.4
0.6
0.8
1.0
1.2
0
2
4
6
H2
Density
Histogram of H2
True value
Posterior mode
0
500
1000
1500
2000
0.2
0.6
1.0
Iteration
H3
Traceplot of H3
True value
Posterior mode
0.2
0.4
0.6
0.8
1.0
1.2
0
2
4
6
H3
Density
Histogram of H3
True value
Posterior mode
0
500
1000
1500
2000
0.2
0.6
1.0
Iteration
H4
Traceplot of H4
True value
Posterior mode
0.2
0.4
0.6
0.8
1.0
1.2
0
2
4
6
H4
Density
Histogram of H4
True value
Posterior mode
0
500
1000
1500
2000
0.2
0.6
1.0
Iteration
H5
Traceplot of H5
True value
Posterior mode
0.2
0.4
0.6
0.8
1.0
1.2
0
2
4
6
H5
Density
Histogram of H5
True value
Posterior mode
Figure A.20: Trace and density plots of a single run of the block ARMS Gibbs sampler
based on simulated data from a one-factor copula model with Gumbel linking copulas
(n = 200 observations, dimension d = 5) for the high Kendall’s tau values scenario. The
full set H1, . . . , H5 is shown.
164

A.1
Traceplots and histograms
MH with mode and curvature matching for H
0
500
1000
1500
2000
0.5
1.5
2.5
Iteration
H1
Traceplot of H1
True value
Posterior mode
0.5
1.0
1.5
2.0
2.5
0
2
4
6
8
10
H1
Density
Histogram of H1
True value
Posterior mode
0
500
1000
1500
2000
0.5
1.5
2.5
Iteration
H2
Traceplot of H2
True value
Posterior mode
0.5
1.0
1.5
2.0
2.5
0
2
4
6
8
10
H2
Density
Histogram of H2
True value
Posterior mode
0
500
1000
1500
2000
0.5
1.5
2.5
Iteration
H3
Traceplot of H3
True value
Posterior mode
0.5
1.0
1.5
2.0
2.5
0
2
4
6
8
10
H3
Density
Histogram of H3
True value
Posterior mode
0
500
1000
1500
2000
0.5
1.5
2.5
Iteration
H4
Traceplot of H4
True value
Posterior mode
0.5
1.0
1.5
2.0
2.5
0
2
4
6
8
10
H4
Density
Histogram of H4
True value
Posterior mode
0
500
1000
1500
2000
0.5
1.5
2.5
Iteration
H5
Traceplot of H5
True value
Posterior mode
0.5
1.0
1.5
2.0
2.5
0
2
4
6
8
10
H5
Density
Histogram of H5
True value
Posterior mode
Figure A.21: Trace and density plots of a single run of the mode and curvature matching
Gibbs sampler based on simulated data from a one-factor copula model with Gumbel
linking copulas (n = 200 observations, dimension d = 5) for the high Kendall’s tau values
scenario. The full set H1, . . . , H5 is shown.
165

A
Simulation study: Figures and tables
MH with expectation and variance matching for H
0
500
1000
1500
2000
0.4
0.8
1.2
Iteration
H1
Traceplot of H1
True value
Posterior mode
0.4
0.6
0.8
1.0
1.2
1.4
0
2
4
6
8
H1
Density
Histogram of H1
True value
Posterior mode
0
500
1000
1500
2000
0.4
0.8
1.2
Iteration
H2
Traceplot of H2
True value
Posterior mode
0.4
0.6
0.8
1.0
1.2
1.4
0
2
4
6
8
H2
Density
Histogram of H2
True value
Posterior mode
0
500
1000
1500
2000
0.4
0.8
1.2
Iteration
H3
Traceplot of H3
True value
Posterior mode
0.4
0.6
0.8
1.0
1.2
1.4
0
2
4
6
8
H3
Density
Histogram of H3
True value
Posterior mode
0
500
1000
1500
2000
0.4
0.8
1.2
Iteration
H4
Traceplot of H4
True value
Posterior mode
0.4
0.6
0.8
1.0
1.2
1.4
0
2
4
6
8
H4
Density
Histogram of H4
True value
Posterior mode
0
500
1000
1500
2000
0.4
0.8
1.2
Iteration
H5
Traceplot of H5
True value
Posterior mode
0.4
0.6
0.8
1.0
1.2
1.4
0
2
4
6
8
H5
Density
Histogram of H5
True value
Posterior mode
Figure A.22: Trace and density plots of a single run of the expectation and variance
matching Gibbs sampler based on simulated data from a one-factor copula model with
Gumbel linking copulas (n = 200 observations, dimension d = 5) for the high Kendall’s
tau values scenario. The full set H1, . . . , H5 is shown.
166

A.1
Traceplots and histograms
Independence and random walk sampler for H
0
500
1000
1500
2000
0.4
0.8
1.2
Iteration
H1
Traceplot of H1
True value
Posterior mode
0.4
0.6
0.8
1.0
1.2
1.4
0
2
4
6
8
10
H1
Density
Histogram of H1
True value
Posterior mode
0
500
1000
1500
2000
0.4
0.8
1.2
Iteration
H2
Traceplot of H2
True value
Posterior mode
0.4
0.6
0.8
1.0
1.2
1.4
0
2
4
6
8
10
H2
Density
Histogram of H2
True value
Posterior mode
0
500
1000
1500
2000
0.4
0.8
1.2
Iteration
H3
Traceplot of H3
True value
Posterior mode
0.4
0.6
0.8
1.0
1.2
1.4
0
2
4
6
8
10
H3
Density
Histogram of H3
True value
Posterior mode
0
500
1000
1500
2000
0.4
0.8
1.2
Iteration
H4
Traceplot of H4
True value
Posterior mode
0.4
0.6
0.8
1.0
1.2
1.4
0
2
4
6
8
10
H4
Density
Histogram of H4
True value
Posterior mode
0
500
1000
1500
2000
0.4
0.8
1.2
Iteration
H5
Traceplot of H5
True value
Posterior mode
0.4
0.6
0.8
1.0
1.2
1.4
0
2
4
6
8
10
H5
Density
Histogram of H5
True value
Posterior mode
Figure A.23: Trace and density plots of a single run of the independence and random
walk Gibbs sampler based on simulated data from a one-factor copula model with Gumbel
linking copulas (n = 200 observations, dimension d = 5) for the high Kendall’s tau values
scenario. The full set H1, . . . , H5 is shown.
167

A
Simulation study: Figures and tables
Thinned ARMS sample of H
0
500
1000
1500
2000
0.4
0.8
1.2
Iteration
H1
Traceplot of H1
True value
Posterior mode
0.4
0.6
0.8
1.0
1.2
1.4
0
2
4
6
8
H1
Density
Histogram of H1
True value
Posterior mode
0
500
1000
1500
2000
0.4
0.8
1.2
Iteration
H2
Traceplot of H2
True value
Posterior mode
0.4
0.6
0.8
1.0
1.2
1.4
0
2
4
6
8
H2
Density
Histogram of H2
True value
Posterior mode
0
500
1000
1500
2000
0.4
0.8
1.2
Iteration
H3
Traceplot of H3
True value
Posterior mode
0.4
0.6
0.8
1.0
1.2
1.4
0
2
4
6
8
H3
Density
Histogram of H3
True value
Posterior mode
0
500
1000
1500
2000
0.4
0.8
1.2
Iteration
H4
Traceplot of H4
True value
Posterior mode
0.4
0.6
0.8
1.0
1.2
1.4
0
2
4
6
8
H4
Density
Histogram of H4
True value
Posterior mode
0
500
1000
1500
2000
0.4
0.8
1.2
Iteration
H5
Traceplot of H5
True value
Posterior mode
0.4
0.6
0.8
1.0
1.2
1.4
0
2
4
6
8
H5
Density
Histogram of H5
True value
Posterior mode
Figure A.24: Trace and density plots of the thinned (thin=300) output from the indi-
vidual ARMS Gibbs sampler based on simulated data from a one-factor copula model with
Gumbel linking copulas (n = 200 observations, dimension d = 5) for the high Kendall’s
tau values scenario. The full set H1, . . . , H5 is shown.
168

A.1
Traceplots and histograms
Univariate ARMS for V1
0
500
1000
1500
2000
0.0
0.4
0.8
Iteration
V1,1
Traceplot of V1,1
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
1.0
0
5
10
20
V1,1
Density
Histogram of V1,1
True value
Posterior mode
0
500
1000
1500
2000
0.0
0.4
0.8
Iteration
V1,2
Traceplot of V1,2
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
1.0
0
5
10
20
V1,2
Density
Histogram of V1,2
True value
Posterior mode
0
500
1000
1500
2000
0.0
0.4
0.8
Iteration
V1,3
Traceplot of V1,3
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
1.0
0
5
10
20
V1,3
Density
Histogram of V1,3
True value
Posterior mode
0
500
1000
1500
2000
0.0
0.4
0.8
Iteration
V1,4
Traceplot of V1,4
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
1.0
0
5
10
20
V1,4
Density
Histogram of V1,4
True value
Posterior mode
0
500
1000
1500
2000
0.0
0.4
0.8
Iteration
V1,5
Traceplot of V1,5
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
1.0
0
5
10
20
V1,5
Density
Histogram of V1,5
True value
Posterior mode
Figure A.25: Trace and density plots of a single run of the individual ARMS Gibbs
sampler based on simulated data from a one-factor copula model with Gumbel linking
copulas (n = 200 observations, dimension d = 5) for the mixed Kendall’s tau values
scenario. The subset V1,1, . . . , V1,5 of V1,1, . . . , V1,200 is shown.
169

A
Simulation study: Figures and tables
Block ARMS for V1
0
500
1000
1500
2000
0.0
0.4
0.8
Iteration
V1,1
Traceplot of V1,1
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
1.0
0
2
4
6
8
10
V1,1
Density
Histogram of V1,1
True value
Posterior mode
0
500
1000
1500
2000
0.0
0.4
0.8
Iteration
V1,2
Traceplot of V1,2
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
1.0
0
2
4
6
8
10
V1,2
Density
Histogram of V1,2
True value
Posterior mode
0
500
1000
1500
2000
0.0
0.4
0.8
Iteration
V1,3
Traceplot of V1,3
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
1.0
0
2
4
6
8
10
V1,3
Density
Histogram of V1,3
True value
Posterior mode
0
500
1000
1500
2000
0.0
0.4
0.8
Iteration
V1,4
Traceplot of V1,4
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
1.0
0
2
4
6
8
10
V1,4
Density
Histogram of V1,4
True value
Posterior mode
0
500
1000
1500
2000
0.0
0.4
0.8
Iteration
V1,5
Traceplot of V1,5
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
1.0
0
2
4
6
8
10
V1,5
Density
Histogram of V1,5
True value
Posterior mode
Figure A.26: Trace and density plots of a single run of the block ARMS Gibbs sampler
based on simulated data from a one-factor copula model with Gumbel linking copulas
(n = 200 observations, dimension d = 5) for the mixed Kendall’s tau values scenario. The
subset V1,1, . . . , V1,5 of V1,1, . . . , V1,200 is shown.
170

A.1
Traceplots and histograms
MH with mode and curvature matching for V1
0
500
1000
1500
2000
0.0
0.4
0.8
Iteration
V1,1
Traceplot of V1,1
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
1.0
0
5
15
25
V1,1
Density
Histogram of V1,1
True value
Posterior mode
0
500
1000
1500
2000
0.0
0.4
0.8
Iteration
V1,2
Traceplot of V1,2
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
1.0
0
5
15
25
V1,2
Density
Histogram of V1,2
True value
Posterior mode
0
500
1000
1500
2000
0.0
0.4
0.8
Iteration
V1,3
Traceplot of V1,3
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
1.0
0
5
15
25
V1,3
Density
Histogram of V1,3
True value
Posterior mode
0
500
1000
1500
2000
0.0
0.4
0.8
Iteration
V1,4
Traceplot of V1,4
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
1.0
0
5
15
25
V1,4
Density
Histogram of V1,4
True value
Posterior mode
0
500
1000
1500
2000
0.0
0.4
0.8
Iteration
V1,5
Traceplot of V1,5
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
1.0
0
5
15
25
V1,5
Density
Histogram of V1,5
True value
Posterior mode
Figure A.27: Trace and density plots of a single run of the mode and curvature matching
Gibbs sampler based on simulated data from a one-factor copula model with Gumbel
linking copulas (n = 200 observations, dimension d = 5) for the mixed Kendall’s tau
values scenario. The subset V1,1, . . . , V1,5 of V1,1, . . . , V1,200 is shown.
171

A
Simulation study: Figures and tables
MH with expectation and variance matching for V1
0
500
1000
1500
2000
0.0
0.4
0.8
Iteration
V1,1
Traceplot of V1,1
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
1.0
0
5
15
25
V1,1
Density
Histogram of V1,1
True value
Posterior mode
0
500
1000
1500
2000
0.0
0.4
0.8
Iteration
V1,2
Traceplot of V1,2
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
1.0
0
5
15
25
V1,2
Density
Histogram of V1,2
True value
Posterior mode
0
500
1000
1500
2000
0.0
0.4
0.8
Iteration
V1,3
Traceplot of V1,3
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
1.0
0
5
15
25
V1,3
Density
Histogram of V1,3
True value
Posterior mode
0
500
1000
1500
2000
0.0
0.4
0.8
Iteration
V1,4
Traceplot of V1,4
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
1.0
0
5
15
25
V1,4
Density
Histogram of V1,4
True value
Posterior mode
0
500
1000
1500
2000
0.0
0.4
0.8
Iteration
V1,5
Traceplot of V1,5
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
1.0
0
5
15
25
V1,5
Density
Histogram of V1,5
True value
Posterior mode
Figure A.28: Trace and density plots of a single run of the expectation and variance
matching Gibbs sampler based on simulated data from a one-factor copula model with
Gumbel linking copulas (n = 200 observations, dimension d = 5) for the mixed Kendall’s
tau values scenario. The subset V1,1, . . . , V1,5 of V1,1, . . . , V1,200 is shown.
172

A.1
Traceplots and histograms
Independence and random walk sampler for V1
0
500
1000
1500
2000
0.0
0.4
0.8
Iteration
V1,1
Traceplot of V1,1
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
1.0
0
5
10
20
V1,1
Density
Histogram of V1,1
True value
Posterior mode
0
500
1000
1500
2000
0.0
0.4
0.8
Iteration
V1,2
Traceplot of V1,2
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
1.0
0
5
10
20
V1,2
Density
Histogram of V1,2
True value
Posterior mode
0
500
1000
1500
2000
0.0
0.4
0.8
Iteration
V1,3
Traceplot of V1,3
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
1.0
0
5
10
20
V1,3
Density
Histogram of V1,3
True value
Posterior mode
0
500
1000
1500
2000
0.0
0.4
0.8
Iteration
V1,4
Traceplot of V1,4
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
1.0
0
5
10
20
V1,4
Density
Histogram of V1,4
True value
Posterior mode
0
500
1000
1500
2000
0.0
0.4
0.8
Iteration
V1,5
Traceplot of V1,5
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
1.0
0
5
10
20
V1,5
Density
Histogram of V1,5
True value
Posterior mode
Figure A.29: Trace and density plots of a single run of the independence and random
walk Gibbs sampler based on simulated data from a one-factor copula model with Gumbel
linking copulas (n = 200 observations, dimension d = 5) for the mixed Kendall’s tau values
scenario. The subset V1,1, . . . , V1,5 of V1,1, . . . , V1,200 is shown.
173

A
Simulation study: Figures and tables
Thinned individual ARMS sample of V1
0
500
1000
1500
2000
0.0
0.4
0.8
Iteration
V1,1
Traceplot of V1,1
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
1.0
0
5
10
20
30
V1,1
Density
Histogram of V1,1
True value
Posterior mode
0
500
1000
1500
2000
0.0
0.4
0.8
Iteration
V1,2
Traceplot of V1,2
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
1.0
0
5
10
20
30
V1,2
Density
Histogram of V1,2
True value
Posterior mode
0
500
1000
1500
2000
0.0
0.4
0.8
Iteration
V1,3
Traceplot of V1,3
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
1.0
0
5
10
20
30
V1,3
Density
Histogram of V1,3
True value
Posterior mode
0
500
1000
1500
2000
0.0
0.4
0.8
Iteration
V1,4
Traceplot of V1,4
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
1.0
0
5
10
20
30
V1,4
Density
Histogram of V1,4
True value
Posterior mode
0
500
1000
1500
2000
0.0
0.4
0.8
Iteration
V1,5
Traceplot of V1,5
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
1.0
0
5
10
20
30
V1,5
Density
Histogram of V1,5
True value
Posterior mode
Figure A.30: Trace and density plots of the thinned (thin=300) output from a single
run of the individual ARMS Gibbs sampler based on simulated data from a one-factor
copula model with Gumbel linking copulas (n = 200 observations, dimension d = 5)
for the mixed Kendall’s tau values scenario. The subset V1,1, . . . , V1,5 of V1,1, . . . , V1,200 is
shown.
174

A.1
Traceplots and histograms
Individiual ARMS for H
0
500
1000
1500
2000
0.0
0.5
1.0
1.5
Iteration
H1
Traceplot of H1
True value
Posterior mode
0.0
0.5
1.0
1.5
0
2
4
6
8
H1
Density
Histogram of H1
True value
Posterior mode
0
500
1000
1500
2000
0.0
0.5
1.0
1.5
Iteration
H2
Traceplot of H2
True value
Posterior mode
0.0
0.5
1.0
1.5
0
2
4
6
8
H2
Density
Histogram of H2
True value
Posterior mode
0
500
1000
1500
2000
0.0
0.5
1.0
1.5
Iteration
H3
Traceplot of H3
True value
Posterior mode
0.0
0.5
1.0
1.5
0
2
4
6
8
H3
Density
Histogram of H3
True value
Posterior mode
0
500
1000
1500
2000
0.0
0.5
1.0
1.5
Iteration
H4
Traceplot of H4
True value
Posterior mode
0.0
0.5
1.0
1.5
0
2
4
6
8
H4
Density
Histogram of H4
True value
Posterior mode
0
500
1000
1500
2000
0.0
0.5
1.0
1.5
Iteration
H5
Traceplot of H5
True value
Posterior mode
0.0
0.5
1.0
1.5
0
2
4
6
8
H5
Density
Histogram of H5
True value
Posterior mode
Figure A.31: Trace and density plots of a single run of the individual ARMS Gibbs
sampler based on simulated data from a one-factor copula model with Gumbel linking
copulas (n = 200 observations, dimension d = 5) for the mixed Kendall’s tau values
scenario. The full set H1, . . . , H5 is shown.
175

A
Simulation study: Figures and tables
Block ARMS for H
0
500
1000
1500
2000
0.0
0.4
0.8
1.2
Iteration
H1
Traceplot of H1
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
1.0
1.2
0
2
4
6
8
12
H1
Density
Histogram of H1
True value
Posterior mode
0
500
1000
1500
2000
0.0
0.4
0.8
1.2
Iteration
H2
Traceplot of H2
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
1.0
1.2
0
2
4
6
8
12
H2
Density
Histogram of H2
True value
Posterior mode
0
500
1000
1500
2000
0.0
0.4
0.8
1.2
Iteration
H3
Traceplot of H3
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
1.0
1.2
0
2
4
6
8
12
H3
Density
Histogram of H3
True value
Posterior mode
0
500
1000
1500
2000
0.0
0.4
0.8
1.2
Iteration
H4
Traceplot of H4
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
1.0
1.2
0
2
4
6
8
12
H4
Density
Histogram of H4
True value
Posterior mode
0
500
1000
1500
2000
0.0
0.4
0.8
1.2
Iteration
H5
Traceplot of H5
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
1.0
1.2
0
2
4
6
8
12
H5
Density
Histogram of H5
True value
Posterior mode
Figure A.32: Trace and density plots of a single run of the block ARMS Gibbs sampler
based on simulated data from a one-factor copula model with Gumbel linking copulas
(n = 200 observations, dimension d = 5) for the mixed Kendall’s tau values scenario. The
full set H1, . . . , H5 is shown.
176

A.1
Traceplots and histograms
MH with mode and curvature matching for H
0
500
1000
1500
2000
0.0
1.0
2.0
Iteration
H1
Traceplot of H1
True value
Posterior mode
0.0
0.5
1.0
1.5
2.0
2.5
0
2
4
6
8
10
H1
Density
Histogram of H1
True value
Posterior mode
0
500
1000
1500
2000
0.0
1.0
2.0
Iteration
H2
Traceplot of H2
True value
Posterior mode
0.0
0.5
1.0
1.5
2.0
2.5
0
2
4
6
8
10
H2
Density
Histogram of H2
True value
Posterior mode
0
500
1000
1500
2000
0.0
1.0
2.0
Iteration
H3
Traceplot of H3
True value
Posterior mode
0.0
0.5
1.0
1.5
2.0
2.5
0
2
4
6
8
10
H3
Density
Histogram of H3
True value
Posterior mode
0
500
1000
1500
2000
0.0
1.0
2.0
Iteration
H4
Traceplot of H4
True value
Posterior mode
0.0
0.5
1.0
1.5
2.0
2.5
0
2
4
6
8
10
H4
Density
Histogram of H4
True value
Posterior mode
0
500
1000
1500
2000
0.0
1.0
2.0
Iteration
H5
Traceplot of H5
True value
Posterior mode
0.0
0.5
1.0
1.5
2.0
2.5
0
2
4
6
8
10
H5
Density
Histogram of H5
True value
Posterior mode
Figure A.33: Trace and density plots of a single run of the mode and curvature matching
Gibbs sampler based on simulated data from a one-factor copula model with Gumbel
linking copulas (n = 200 observations, dimension d = 5) for the mixed Kendall’s tau
values scenario. The full set H1, . . . , H5 is shown.
177

A
Simulation study: Figures and tables
MH with expectation and variance matching for H
0
500
1000
1500
2000
0.0
1.0
2.0
Iteration
H1
Traceplot of H1
True value
Posterior mode
0.0
0.5
1.0
1.5
2.0
2.5
0
2
4
6
8
H1
Density
Histogram of H1
True value
Posterior mode
0
500
1000
1500
2000
0.0
1.0
2.0
Iteration
H2
Traceplot of H2
True value
Posterior mode
0.0
0.5
1.0
1.5
2.0
2.5
0
2
4
6
8
H2
Density
Histogram of H2
True value
Posterior mode
0
500
1000
1500
2000
0.0
1.0
2.0
Iteration
H3
Traceplot of H3
True value
Posterior mode
0.0
0.5
1.0
1.5
2.0
2.5
0
2
4
6
8
H3
Density
Histogram of H3
True value
Posterior mode
0
500
1000
1500
2000
0.0
1.0
2.0
Iteration
H4
Traceplot of H4
True value
Posterior mode
0.0
0.5
1.0
1.5
2.0
2.5
0
2
4
6
8
H4
Density
Histogram of H4
True value
Posterior mode
0
500
1000
1500
2000
0.0
1.0
2.0
Iteration
H5
Traceplot of H5
True value
Posterior mode
0.0
0.5
1.0
1.5
2.0
2.5
0
2
4
6
8
H5
Density
Histogram of H5
True value
Posterior mode
Figure A.34: Trace and density plots of a single run of the expectation and variance
matching Gibbs sampler based on simulated data from a one-factor copula model with
Gumbel linking copulas (n = 200 observations, dimension d = 5) for the mixed Kendall’s
tau values scenario. The full set H1, . . . , H5 is shown.
178

A.1
Traceplots and histograms
Independence and random walk sampler for H
0
500
1000
1500
2000
0.0
0.5
1.0
1.5
Iteration
H1
Traceplot of H1
True value
Posterior mode
0.0
0.5
1.0
1.5
0
2
4
6
8
H1
Density
Histogram of H1
True value
Posterior mode
0
500
1000
1500
2000
0.0
0.5
1.0
1.5
Iteration
H2
Traceplot of H2
True value
Posterior mode
0.0
0.5
1.0
1.5
0
2
4
6
8
H2
Density
Histogram of H2
True value
Posterior mode
0
500
1000
1500
2000
0.0
0.5
1.0
1.5
Iteration
H3
Traceplot of H3
True value
Posterior mode
0.0
0.5
1.0
1.5
0
2
4
6
8
H3
Density
Histogram of H3
True value
Posterior mode
0
500
1000
1500
2000
0.0
0.5
1.0
1.5
Iteration
H4
Traceplot of H4
True value
Posterior mode
0.0
0.5
1.0
1.5
0
2
4
6
8
H4
Density
Histogram of H4
True value
Posterior mode
0
500
1000
1500
2000
0.0
0.5
1.0
1.5
Iteration
H5
Traceplot of H5
True value
Posterior mode
0.0
0.5
1.0
1.5
0
2
4
6
8
H5
Density
Histogram of H5
True value
Posterior mode
Figure A.35: Trace and density plots of a single run of the independence and random
walk Gibbs sampler based on simulated data from a one-factor copula model with Gumbel
linking copulas (n = 200 observations, dimension d = 5) for the mixed Kendall’s tau values
scenario. The full set H1, . . . , H5 is shown.
179

A
Simulation study: Figures and tables
Thinned ARMS sample of H
0
500
1000
1500
2000
0.0
1.0
2.0
Iteration
H1
Traceplot of H1
True value
Posterior mode
0.0
0.5
1.0
1.5
2.0
2.5
0
2
4
6
8
H1
Density
Histogram of H1
True value
Posterior mode
0
500
1000
1500
2000
0.0
1.0
2.0
Iteration
H2
Traceplot of H2
True value
Posterior mode
0.0
0.5
1.0
1.5
2.0
2.5
0
2
4
6
8
H2
Density
Histogram of H2
True value
Posterior mode
0
500
1000
1500
2000
0.0
1.0
2.0
Iteration
H3
Traceplot of H3
True value
Posterior mode
0.0
0.5
1.0
1.5
2.0
2.5
0
2
4
6
8
H3
Density
Histogram of H3
True value
Posterior mode
0
500
1000
1500
2000
0.0
1.0
2.0
Iteration
H4
Traceplot of H4
True value
Posterior mode
0.0
0.5
1.0
1.5
2.0
2.5
0
2
4
6
8
H4
Density
Histogram of H4
True value
Posterior mode
0
500
1000
1500
2000
0.0
1.0
2.0
Iteration
H5
Traceplot of H5
True value
Posterior mode
0.0
0.5
1.0
1.5
2.0
2.5
0
2
4
6
8
H5
Density
Histogram of H5
True value
Posterior mode
Figure A.36: Trace and density plots of the thinned (thin=300) output from a single
run of the individual ARMS Gibbs sampler based on simulated data from a one-factor
copula model with Gumbel linking copulas (n = 200 observations, dimension d = 5) for
the mixed Kendall’s tau values scenario. The full set H1, . . . , H5 is shown.
180

A.1
Traceplots and histograms
Individual ARMS with Gaussian copulas for V1
0
500
1000
1500
2000
0.0
0.4
0.8
Iteration
V1,1
Traceplot of V1,1
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
1.0
0
2
4
6
8
10
V1,1
Density
Histogram of V1,1
True value
Posterior mode
0
500
1000
1500
2000
0.0
0.4
0.8
Iteration
V1,2
Traceplot of V1,2
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
1.0
0
2
4
6
8
10
V1,2
Density
Histogram of V1,2
True value
Posterior mode
0
500
1000
1500
2000
0.0
0.4
0.8
Iteration
V1,3
Traceplot of V1,3
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
1.0
0
2
4
6
8
10
V1,3
Density
Histogram of V1,3
True value
Posterior mode
0
500
1000
1500
2000
0.0
0.4
0.8
Iteration
V1,4
Traceplot of V1,4
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
1.0
0
2
4
6
8
10
V1,4
Density
Histogram of V1,4
True value
Posterior mode
0
500
1000
1500
2000
0.0
0.4
0.8
Iteration
V1,5
Traceplot of V1,5
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
1.0
0
2
4
6
8
10
V1,5
Density
Histogram of V1,5
True value
Posterior mode
Figure A.37: Trace and density plots of a single run of the individual ARMS Gibbs
sampler based on simulated data from a one-factor copula model with Gaussian linking
copulas (n = 200 observations, dimension d = 5) for the mixed Kendall’s tau values
scenario. The subset V1,1, . . . , V1,5 of V1,1, . . . , V1,200 is shown.
181

A
Simulation study: Figures and tables
Individual ARMS with survival Gumbel copulas for V1
0
500
1000
1500
2000
0.0
0.4
0.8
Iteration
V1,1
Traceplot of V1,1
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
1.0
0
5
10
15
V1,1
Density
Histogram of V1,1
True value
Posterior mode
0
500
1000
1500
2000
0.0
0.4
0.8
Iteration
V1,2
Traceplot of V1,2
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
1.0
0
5
10
15
V1,2
Density
Histogram of V1,2
True value
Posterior mode
0
500
1000
1500
2000
0.0
0.4
0.8
Iteration
V1,3
Traceplot of V1,3
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
1.0
0
5
10
15
V1,3
Density
Histogram of V1,3
True value
Posterior mode
0
500
1000
1500
2000
0.0
0.4
0.8
Iteration
V1,4
Traceplot of V1,4
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
1.0
0
5
10
15
V1,4
Density
Histogram of V1,4
True value
Posterior mode
0
500
1000
1500
2000
0.0
0.4
0.8
Iteration
V1,5
Traceplot of V1,5
True value
Posterior mode
0.0
0.2
0.4
0.6
0.8
1.0
0
5
10
15
V1,5
Density
Histogram of V1,5
True value
Posterior mode
Figure A.38: Trace and density plots of a single run of the individual ARMS Gibbs
sampler based on simulated data from a one-factor copula model with survival Gumbel
linking copulas (n = 200 observations, dimension d = 5) for the mixed Kendall’s tau
values scenario. The subset V1,1, . . . , V1,5 of V1,1, . . . , V1,200 is shown.
182

A.1
Traceplots and histograms
Individual ARMS with Gaussian copulas for H
0
500
1000
1500
2000
0.0
1.0
2.0
Iteration
H1
Traceplot of H1
True value
Posterior mode
0.0
0.5
1.0
1.5
2.0
0
2
4
6
8
H1
Density
Histogram of H1
True value
Posterior mode
0
500
1000
1500
2000
0.0
1.0
2.0
Iteration
H2
Traceplot of H2
True value
Posterior mode
0.0
0.5
1.0
1.5
2.0
0
2
4
6
8
H2
Density
Histogram of H2
True value
Posterior mode
0
500
1000
1500
2000
0.0
1.0
2.0
Iteration
H3
Traceplot of H3
True value
Posterior mode
0.0
0.5
1.0
1.5
2.0
0
2
4
6
8
H3
Density
Histogram of H3
True value
Posterior mode
0
500
1000
1500
2000
0.0
1.0
2.0
Iteration
H4
Traceplot of H4
True value
Posterior mode
0.0
0.5
1.0
1.5
2.0
0
2
4
6
8
H4
Density
Histogram of H4
True value
Posterior mode
0
500
1000
1500
2000
0.0
1.0
2.0
Iteration
H5
Traceplot of H5
True value
Posterior mode
0.0
0.5
1.0
1.5
2.0
0
2
4
6
8
H5
Density
Histogram of H5
True value
Posterior mode
Figure A.39: Trace and density plots of a single run of the individual ARMS Gibbs
sampler based on simulated data from a one-factor copula model with Gaussian linking
copulas (n = 200 observations, dimension d = 5) for the mixed Kendall’s tau values
scenario. The full set H1, . . . , H5 is shown.
183

A
Simulation study: Figures and tables
Individual ARMS with survival Gumbel copulas for H
0
500
1000
1500
2000
0.0
0.5
1.0
1.5
Iteration
H1
Traceplot of H1
True value
Posterior mode
0.0
0.5
1.0
1.5
0
2
4
6
8
H1
Density
Histogram of H1
True value
Posterior mode
0
500
1000
1500
2000
0.0
0.5
1.0
1.5
Iteration
H2
Traceplot of H2
True value
Posterior mode
0.0
0.5
1.0
1.5
0
2
4
6
8
H2
Density
Histogram of H2
True value
Posterior mode
0
500
1000
1500
2000
0.0
0.5
1.0
1.5
Iteration
H3
Traceplot of H3
True value
Posterior mode
0.0
0.5
1.0
1.5
0
2
4
6
8
H3
Density
Histogram of H3
True value
Posterior mode
0
500
1000
1500
2000
0.0
0.5
1.0
1.5
Iteration
H4
Traceplot of H4
True value
Posterior mode
0.0
0.5
1.0
1.5
0
2
4
6
8
H4
Density
Histogram of H4
True value
Posterior mode
0
500
1000
1500
2000
0.0
0.5
1.0
1.5
Iteration
H5
Traceplot of H5
True value
Posterior mode
0.0
0.5
1.0
1.5
0
2
4
6
8
H5
Density
Histogram of H5
True value
Posterior mode
Figure A.40: Trace and density plots of a single run of the individual ARMS Gibbs
sampler based on simulated data from a one-factor copula model with survival Gumbel
linking copulas (n = 200 observations, dimension d = 5) for the mixed Kendall’s tau
values scenario. The full set H1, . . . , H5 is shown.
184

A.2
Boxplots of latent variable
A.2
Boxplots of latent variable
In this section, boxplots of latent variables V1,1, . . . , V1,200 are shown to be able to assess
the ability of the sampling methods within Gibbs sampling to correctly identify the states
of V1. Gibbs sampler output for all sampling methods and scenarios of Kendall’s tau
values is examined. In these plots the posterior mode is indicated by the red dotted line
and the true value of the latent variable by a solid blue line. The inner box shows the
50% and the outer whisker show the 95% credible intervals centered around the median
of the posterior samples. Posterior means are given by the solid black line inside the box.
185

A
Simulation study: Figures and tables
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
Figure A.41: Boxplots of the individual ARMS Gibbs sampler output of V1,1, . . . , V1,200,
for simulated data of the one-factor copula model with bivariate Gumbel linking copulas
and the scenario of low Kendall’s tau values (n = 200, d = 5). The Gibbs sampler was
run for 2 000 iterations and the ﬁrst 1 000 were discarded as burn-in.
186

A.2
Boxplots of latent variable
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
Figure A.42: Boxplots of the block ARMS Gibbs sampler output of V1,1, . . . , V1,200, for
simulated data of the one-factor copula model with bivariate Gumbel linking copulas and
the scenario of low Kendall’s tau values (n = 200, d = 5). The Gibbs sampler was run
for 2 000 iterations and the ﬁrst 1 000 were discarded as burn-in.
187

A
Simulation study: Figures and tables
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
Figure A.43: Boxplots of the mode and curvature matching Gibbs sampler output of
V1,1, . . . , V1,200, for simulated data of the one-factor copula model with bivariate Gumbel
linking copulas and the scenario of low Kendall’s tau values (n = 200, d = 5). The Gibbs
sampler was run for 2 000 iterations and the ﬁrst 1 000 were discarded as burn-in.
188

A.2
Boxplots of latent variable
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
Figure A.44: Boxplots of the expectation and variance matching Gibbs sampler output
of V1,1, . . . , V1,200, for simulated data of the one-factor copula model with bivariate Gumbel
linking copulas and the scenario of low Kendall’s tau values (n = 200, d = 5). The Gibbs
sampler was run for 2 000 iterations and the ﬁrst 1 000 were discarded as burn-in.
189

A
Simulation study: Figures and tables
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
Figure A.45: Boxplots of the independence and random walk Gibbs sampler output of
V1,1, . . . , V1,200, for simulated data of the one-factor copula model with bivariate Gumbel
linking copulas and the scenario of low Kendall’s tau values (n = 200, d = 5). The Gibbs
sampler was run for 2 000 iterations and the ﬁrst 1 000 were discarded as burn-in.
190

A.2
Boxplots of latent variable
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
Figure A.46:
Boxplots of the thinned (thin=300) individual ARMS Gibbs sampler
output of V1,1, . . . , V1,200, for simulated data of the one-factor copula model with bivariate
Gumbel linking copulas and the scenario of low Kendall’s tau values (n = 200, d = 5).
The Gibbs sampler was run for 2 000 iterations and the ﬁrst 1 000 were discarded as
burn-in.
191

A
Simulation study: Figures and tables
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
Figure A.47: Boxplots of the individual ARMS Gibbs sampler output of V1,1, . . . , V1,200,
for simulated data of the one-factor copula model with bivariate Gumbel linking copulas
and the scenario of high Kendall’s tau values (n = 200, d = 5). The Gibbs sampler was
run for 2 000 iterations and the ﬁrst 1 000 were discarded as burn-in.
192

A.2
Boxplots of latent variable
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
Figure A.48: Boxplots of the block ARMS Gibbs sampler output of V1,1, . . . , V1,200, for
simulated data of the one-factor copula model with bivariate Gumbel linking copulas and
the scenario of high Kendall’s tau values (n = 200, d = 5). The Gibbs sampler was run
for 2 000 iterations and the ﬁrst 1 000 were discarded as burn-in.
193

A
Simulation study: Figures and tables
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
Figure A.49: Boxplots of the mode and curvature matching Gibbs sampler output of
V1,1, . . . , V1,200, for simulated data of the one-factor copula model with bivariate Gumbel
linking copulas and the scenario of high Kendall’s tau values (n = 200, d = 5). The Gibbs
sampler was run for 2 000 iterations and the ﬁrst 1 000 were discarded as burn-in.
194

A.2
Boxplots of latent variable
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
Figure A.50: Boxplots of the expectation and variance matching Gibbs sampler output
of V1,1, . . . , V1,200, for simulated data of the one-factor copula model with bivariate Gumbel
linking copulas and the scenario of high Kendall’s tau values (n = 200, d = 5). The Gibbs
sampler was run for 2 000 iterations and the ﬁrst 1 000 were discarded as burn-in.
195

A
Simulation study: Figures and tables
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
Figure A.51: Boxplots of the independence and random walk Gibbs sampler output of
V1,1, . . . , V1,200, for simulated data of the one-factor copula model with bivariate Gumbel
linking copulas and the scenario of high Kendall’s tau values (n = 200, d = 5). The Gibbs
sampler was run for 2 000 iterations and the ﬁrst 1 000 were discarded as burn-in.
196

A.2
Boxplots of latent variable
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
Figure A.52:
Boxplots of the thinned (thin=300) individual ARMS Gibbs sampler
output of V1,1, . . . , V1,200, for simulated data of the one-factor copula model with bivariate
Gumbel linking copulas and the scenario of high Kendall’s tau values (n = 200, d = 5).
The Gibbs sampler was run for 2 000 iterations and the ﬁrst 1 000 were discarded as
burn-in.
197

A
Simulation study: Figures and tables
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
Figure A.53: Boxplots of the individual ARMS Gibbs sampler output of V1,1, . . . , V1,200,
for simulated data of the one-factor copula model with bivariate Gumbel linking copulas
and the scenario of mixed Kendall’s tau values (n = 200, d = 5). The Gibbs sampler was
run for 2 000 iterations and the ﬁrst 1 000 were discarded as burn-in.
198

A.2
Boxplots of latent variable
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
Figure A.54: Boxplots of the block ARMS Gibbs sampler output of V1,1, . . . , V1,200, for
simulated data of the one-factor copula model with bivariate Gumbel linking copulas and
the scenario of mixed Kendall’s tau values (n = 200, d = 5). The Gibbs sampler was run
for 2 000 iterations and the ﬁrst 1 000 were discarded as burn-in.
199

A
Simulation study: Figures and tables
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
Figure A.55: Boxplots of the mode and curvature matching Gibbs sampler output of
V1,1, . . . , V1,200, for simulated data of the one-factor copula model with bivariate Gumbel
linking copulas and the scenario of mixed Kendall’s tau values (n = 200, d = 5). The
Gibbs sampler was run for 2 000 iterations and the ﬁrst 1 000 were discarded as burn-in.
200

A.2
Boxplots of latent variable
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
Figure A.56: Boxplots of the expectation and variance matching Gibbs sampler output
of V1,1, . . . , V1,200, for simulated data of the one-factor copula model with bivariate Gumbel
linking copulas and the scenario of mixed Kendall’s tau values (n = 200, d = 5). The
Gibbs sampler was run for 2 000 iterations and the ﬁrst 1 000 were discarded as burn-in.
201

A
Simulation study: Figures and tables
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
Figure A.57: Boxplots of the independence and random walk Gibbs sampler output of
V1,1, . . . , V1,200, for simulated data of the one-factor copula model with bivariate Gumbel
linking copulas and the scenario of mixed Kendall’s tau values (n = 200, d = 5). The
Gibbs sampler was run for 2 000 iterations and the ﬁrst 1 000 were discarded as burn-in.
202

A.2
Boxplots of latent variable
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
0.0
0.6
Figure A.58:
Boxplots of the thinned (thin=300) individual ARMS Gibbs sampler
output of V1,1, . . . , V1,200, for simulated data of the one-factor copula model with bivariate
Gumbel linking copulas and the scenario of mixed Kendall’s tau values (n = 200, d = 5).
The Gibbs sampler was run for 2 000 iterations and the ﬁrst 1 000 were discarded as
burn-in.
203

A
Simulation study: Figures and tables
A.3
Acceptance rate and eﬀective sample size
In this section, tables with statistics of the Gibbs sampler output for all methods and
scenarios of Kendall’s tau values are shown. In the case of individual ARMS, the Gibbs
sampler for the one-factor copula model with Gaussian and survival Gumbel linking cop-
ulas is also examined. The following tables show the summarized acceptance rates and
eﬀective sample sizes for V1 and H, and full details of acceptance rate, eﬀective sample
size and eﬀective sample size per minute for Fisher z-transformed copula parameters
H1, . . . , H5.
204

A.3
Acceptance rate and eﬀective sample size
Individual ARMS
Acceptance rate
Eﬀ. sample size
V1
H
V1
H
Low
Min
43%
100%
42
20
Median
100%
100%
349
62
Max
100%
100%
2000
122
High
Min
26%
100%
339
270
Median
100%
100%
2000
912
Max
100%
100%
2365
1460
Mixed
Min
25%
100%
110
33
Median
100%
100%
1080
315
Max
100%
100%
2384
1850
H1
H2
H3
H4
H5
Low
AR
100%
100%
100%
100%
100%
ESS
122
106
62
20
52
ESS/min
29
25
15
5
12
High
AR
100%
100%
100%
100%
100%
ESS
1460
1323
912
891
270
ESS/min
322
291
201
196
59
Mixed
AR
100%
100%
100%
100%
100%
ESS
1850
1330
315
110
33
ESS/min
414
298
71
25
7
Block ARMS
Acceptance rate
Eﬀ. sample size
V1
H
V1
H
Low
Min
100%
99%
1
170
Median
100%
99%
3
225
Max
100%
99%
21
266
High
Min
100%
100%
1
15
Median
100%
100%
4
36
Max
100%
100%
19
97
Mixed
Min
100%
100%
1
9
Median
100%
100%
3
25
Max
100%
100%
26
278
H1
H2
H3
H4
H5
Low
AR
99%
99%
99%
99%
99%
ESS
170
240
225
224
266
ESS/min
5
6
6
6
7
High
AR
100%
100%
100%
100%
100%
ESS
97
15
60
36
22
ESS/min
3
0
2
1
1
Mixed
AR
100%
100%
100%
100%
100%
ESS
278
67
11
25
9
ESS/min
7
2
0
1
0
MH with mode and curvature matching
Acceptance rate
Eﬀ. sample size
V1
H
V1
H
Low
Min
2%
40%
25
60
Median
53%
95%
961
1330
Max
85%
100%
2100
2000
High
Min
7%
87%
29
43
Median
59%
97%
1146
1709
Max
88%
98%
1793
1861
Mixed
Min
7%
87%
54
53
Median
61%
97%
1137
1701
Max
87%
99%
1869
1788
H1
H2
H3
H4
H5
Low
AR
94%
95%
100%
40%
98%
ESS
1330
1263
2000
60
1696
ESS/min
43
41
65
2
55
High
AR
97%
97%
97%
98%
87%
ESS
1375
1709
1861
1726
43
ESS/min
47
59
64
59
1
Mixed
AR
99%
97%
97%
97%
87%
ESS
1701
1788
1640
1716
53
ESS/min
60
63
58
60
2
MH with expectation and variance matching
Acceptance rate
Eﬀ. sample size
V1
H
V1
H
Low
Min
58%
87%
118
32
Median
95%
88%
617
98
Max
99%
92%
2064
121
High
Min
87%
95%
87
269
Median
96%
96%
1697
828
Max
99%
98%
2368
1018
Mixed
Min
83%
86%
28
7
Median
94%
96%
572
293
Max
97%
98%
2030
978
H1
H2
H3
H4
H5
Low
AR
88%
89%
87%
92%
87%
ESS
120
83
121
32
98
ESS/min
2
2
2
1
2
High
AR
95%
96%
96%
97%
98%
ESS
1018
956
828
641
269
ESS/min
29
27
24
18
8
Mixed
AR
86%
92%
96%
96%
98%
ESS
978
872
293
48
7
ESS/min
26
23
8
1
0
Independence and random walk samplers
Acceptance rate
Eﬀ. sample size
V1
H
V1
H
Low
Min
9%
25%
27
5
Median
69%
32%
310
82
Max
79%
34%
1528
135
High
Min
1%
20%
11
49
Median
23%
23%
323
134
Max
36%
24%
546
260
Mixed
Min
1%
21%
9
6
Median
25%
24%
297
243
Max
41%
38%
675
546
H1
H2
H3
H4
H5
Low
AR
32%
32%
34%
25%
33%
ESS
43
83
135
5
82
ESS/min
54
104
168
6
102
High
AR
24%
24%
23%
20%
20%
ESS
258
260
134
128
49
ESS/min
323
325
168
160
61
Mixed
AR
38%
28%
24%
22%
21%
ESS
546
292
243
30
6
ESS/min
672
360
299
37
8
Table A.1: Summary of 2 000 iterations of a single run of the Gibbs sampler for the
one-factor copula model with Gumbel linking copulas for ﬁve diﬀerent methods and three
scenarios for Kendall’s tau values. Left side: Summary of minimum, median and max-
imum acceptance rates and eﬀective sample sizes of V1 and H. Right side: Acceptance
rate (AR), eﬀective sample size (ESS) and eﬀective sample size per minute (ESS/min) of
H1, . . . , H5 individually.
205

A
Simulation study: Figures and tables
Individual ARMS with Gaussian copulas
V1
H
V1
H
Acceptance rate
Eﬀ. sample size
Low
Min
72%
100%
33
7
Median
100%
100%
204
50
Max
100%
100%
2200
126
High
Min
23%
99%
178
186
Median
100%
100%
2000
1224
Max
100%
100%
2964
2007
Mixed
Min
25%
100%
23
9
Median
100%
100%
825
1617
Max
100%
100%
2685
1806
H1
H2
H3
H4
H5
Low
AR
100%
100%
100%
100%
100%
ESS
7
100
36
126
50
ESS/min
1
16
6
20
8
High
AR
100%
100%
100%
100%
99%
ESS
2007
1317
1224
620
186
ESS/min
298
196
182
92
28
Mixed
AR
100%
100%
100%
100%
100%
ESS
1617
1806
1617
32
9
ESS/min
236
264
236
5
1
Individual ARMS with survival Gumbel copulas
V1
H
V1
H
Acceptance rate
Eﬀ. sample size
Low
Min
48%
100%
265
74
Median
100%
100%
1128
172
Max
100%
100%
2001
263
High
Min
27%
100%
382
308
Median
100%
100%
2000
977
Max
100%
100%
2490
1625
Mixed
Min
29%
100%
148
47
Median
100%
100%
1659
878
Max
100%
100%
2736
1457
H1
H2
H3
H4
H5
Low
AR
100%
100%
100%
100%
100%
ESS
74
263
92
172
213
ESS/min
10
34
12
22
28
High
AR
100%
100%
100%
100%
100%
ESS
1625
995
977
734
308
ESS/min
202
123
121
91
38
Mixed
AR
100%
100%
100%
100%
100%
ESS
1336
1457
878
166
47
ESS/min
168
183
110
21
6
Table A.2: Summary of 2 000 iterations of a single run of the Gibbs sampler for the
individual ARMS method for the one-factor copula model with Gaussian and survival
Gumbel linking copulas and three scenarios for Kendall’s tau values. Left side: Summary
of minimum, median and maximum acceptance rates and eﬀective sample sizes of V1 and
H. Right side: Acceptance rate (AR), eﬀective sample size (ESS) and eﬀective sample
size per minute (ESS/min) of H1, . . . , H5 individually.
206

A.4
Posterior mode estimation
A.4
Posterior mode estimation
In this section, tables with statistics of the posterior mode estimates of the Gibbs sampler
are shown. Mean absolute, max absolute and mean squared error of N = 100 repetitions
of posterior mode estimation of the copula parameters of the one-factor copula model in
Kendall’s tau scale are shown for the individual ARMS, mode and curvature matching
and independence and random walk sampler methods for all scenarios of Kendall’s tau
values.
207

A
Simulation study: Figures and tables
Individual ARMS
C1,V1
C2,V1
C3,V1
C4,V1
C5,V1
Low τ
True τ
0.10
0.12
0.15
0.18
0.20
|bτ −τ|mean
0.06
0.06
0.07
0.07
0.08
|bτ −τ|max
0.21
0.20
0.77
0.31
0.77
(bτ −τ)2
mean
4.7e-03
5.2e-03
1.1e-02
7.9e-03
1.8e-02
High τ
True τ
0.50
0.57
0.65
0.73
0.80
|bτ −τ|mean
0.03
0.03
0.02
0.02
0.02
|bτ −τ|max
0.09
0.08
0.06
0.06
0.04
(bτ −τ)2
mean
1.1e-03
9.5e-04
5.9e-04
5.3e-04
3.6e-04
Mixed τ
True τ
0.10
0.28
0.45
0.62
0.80
|bτ −τ|mean
0.03
0.04
0.03
0.03
0.08
|bτ −τ|max
0.13
0.11
0.10
0.10
0.18
(bτ −τ)2
mean
1.6e-03
2.0e-03
1.4e-03
1.6e-03
9.5e-03
MH with mode and curvature matching
C1,V1
C2,V1
C3,V1
C4,V1
C5,V1
Low τ
True τ
0.10
0.12
0.15
0.18
0.20
|bτ −τ|mean
0.13
0.17
0.24
0.25
0.37
|bτ −τ|max
0.89
0.86
0.84
0.81
0.79
(bτ −τ)2
mean
7.3e-02
9.3e-02
1.4e-01
1.5e-01
2.4e-01
High τ
True τ
0.50
0.57
0.65
0.73
0.80
|bτ −τ|mean
0.03
0.04
0.05
0.06
0.18
|bτ −τ|max
0.11
0.13
0.10
0.25
0.19
(bτ −τ)2
mean
1.6e-03
2.3e-03
2.8e-03
4.9e-03
3.2e-02
Mixed τ
True τ
0.10
0.28
0.45
0.62
0.80
|bτ −τ|mean
0.03
0.04
0.04
0.08
0.18
|bτ −τ|max
0.12
0.11
0.17
0.36
0.25
(bτ −τ)2
mean
1.4e-03
2.2e-03
2.2e-03
1.6e-02
3.4e-02
MH with independence and random walk samplers
C1,V1
C2,V1
C3,V1
C4,V1
C5,V1
Low τ
True τ
0.10
0.12
0.15
0.18
0.20
|bτ −τ|mean
0.05
0.08
0.06
0.08
0.09
|bτ −τ|max
0.22
0.81
0.22
0.38
0.65
(bτ −τ)2
mean
4.6e-03
1.8e-02
6.2e-03
1.1e-02
1.6e-02
High τ
True τ
0.50
0.57
0.65
0.73
0.80
|bτ −τ|mean
0.03
0.03
0.02
0.02
0.02
|bτ −τ|max
0.09
0.10
0.07
0.06
0.05
(bτ −τ)2
mean
1.1e-03
1.1e-03
7.1e-04
5.4e-04
4.4e-04
Mixed τ
True τ
0.10
0.28
0.45
0.62
0.80
|bτ −τ|mean
0.03
0.04
0.03
0.03
0.06
|bτ −τ|max
0.14
0.12
0.12
0.11
0.17
(bτ −τ)2
mean
1.7e-03
2.4e-03
1.4e-03
1.4e-03
5.2e-03
Table A.3: Statistics of diﬀerences of posterior modes and true Kendall’s tau value of
N = 100 repetitions of the Gibbs sampler routine for three sampling methods and data
from a one-factor copula model with Gumbel linking copulas, sample size n = 200 and
dimension d = 5, for three ranges of Kendall’s tau values.
208

B
Empirical study: Figures and tables
In this part of the appendix, supplementary ﬁgures and tables of the empirical study of
Section 9 are provided.
B.1
In-sample ARMA–GARCH
In this section, histograms with overlaid theoretical density of the standard uniform distri-
bution of the copula data generated by in-sample ARMA(1, 1)-GARCH(1, 1) models with
Student’s t innovations applied to each margin are given. ARMA-GARCH models are
ﬁt to 2 606 daily log returns of eight bank stocks and two indices. The 261 observations
of year 2004 are then discarded to create a common basis for comparison, leaving 2 345
observations. Further details are given in Section 9.2.1.
209

B
Empirical study: Figures and tables
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.4
0.8
1.2
Histogram of SXXP copula data
U
Density
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.4
0.8
1.2
Histogram of SX7P copula data
U
Density
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.4
0.8
1.2
Histogram of ACA copula data
U
Density
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.4
0.8
1.2
Histogram of BBVA copula data
U
Density
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.4
0.8
1.2
Histogram of BNP copula data
U
Density
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.4
0.8
1.2
Histogram of CBK copula data
U
Density
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.4
0.8
1.2
Histogram of DBK copula data
U
Density
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.4
0.8
1.2
Histogram of GLE copula data
U
Density
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.4
0.8
1.2
Histogram of ISP copula data
U
Density
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.4
0.8
1.2
Histogram of SAN copula data
U
Density
Figure B.1:
Histograms of the in-sample ARMA(1, 1)-GARCH(1, 1) residuals trans-
formed to standard uniform data with a probability integral transform. Residuals are
generated in-sample by marginal ARMA–GARCH ﬁts for 2 345 daily log return obser-
vations of years 2005 to 2013. The theoretical value of the standard uniform density is
indicated by the dotted grey line.
210

B.2
Time-varying ARMA DLM
B.2
Time-varying ARMA DLM
In this section, pairwise scatter and contour plots of the copula data generated by marginal
time-varying ARMA(1, 1) DLMs are given. Scatter plots are given in Z scale (lower left
triangle) and U scale (upper right triangle). Contour plots of the empirical density with
standard normal margins (upper right triangle) are shown with empirical Kendall’s tau
and Spearman’s rho values (lower left triangle). DLM models are ﬁt on 2 606 daily log
returns of eight bank stocks and two indices. The 261 observations of year 2004 are then
discarded to create a common basis for comparison, leaving 2 345 observations. Further
details are given in Section 9.2.3.
211

B
Empirical study: Figures and tables
SXXP
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GGG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
GGG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
GG
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G G
G G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G G
G
G
G G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
GG
G
G
G
G
GG
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G G
GG
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
GG
G
G
G
G
G
GG
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GGG
G G
G
GG
G
G
G
G
GG
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
GG
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GGG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
GG
G
G
G G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
SX7P
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
GG
G
GGG
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G G
G
GG
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G G
G
GG
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
GG
G
G
G
G
G
G
G
G
G
G
G G G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
GG
G
G
G
G G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G G
G
G
G G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
GG
GG
GG
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
GG
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
GG
G
G G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
GGG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
ACA
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G G
G
GG
G
G
G
G
G
G
G
G G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
GG
G
G
G
G
G
G
GG
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GGG
G G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
GGG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
GG
G
GG
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GGG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
GG
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G G
G
G
G
G
G
G
G
G G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
GG
G
GG
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GGG
GG
G
G
G
G
G
G
G
GGG G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G G
G
G G
G
G
G
G
G
G
G
G
G
GG
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
GG
G
G
G
G
G
G
G
GG
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G GG
G
GG
G
G
G
G
G
G
G
G G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
GG
G
G
GG
G
G
G
G
G
GG
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G G
G
G
G
G
G
G
G
G
GG
G
G GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
BBVA
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
GG
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
GG
G
G
G G
G
G
G G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G G
GG
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G GG
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
GGG
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
GG
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GGG
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
GG
G
G
G G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G G
G
G
GG
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G G
G
G
GG
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
GG
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G G
GG
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
GGG
GG
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
GG
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GGG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
GG
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
GG
GG
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G G
G
G
GG
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
GG
G
G
G
G
G
G
GG
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
GG
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
GG
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
GGG
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
GGG
G
G
G
G
G
G
GG
G
G
G
G
G
G
G G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
GG
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
GGG
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G G
G
G
G
GG
G
G
G
G
G
G G G
G
G
G G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GGG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G G
G
G
G
G
G
G
GG
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
GGG
G
G
GG
G
G
G
G
G
G G
G
GG
G
G
GG
G
G
G G
G
G
GG
G
G
G
G
G
G
G
G
G G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
GG
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GGG
G
G
G
G
G
G
GG
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GGG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
BNP
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G GG
G
G
G
G
G
GG
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G G
G
G
G G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
GG
G
G
G
GG
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G G G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
GG
G
GG G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G G
G
G
GG
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
GG
G
G
G
G
G
G
G
GG
G G
G
G
G
G
G
GGG
G
G
G
G
G G
G
G
G
G
G
GG
G
G
G
GG
GG
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
GG
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
GG
G
G
G
G G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
GG
G
G G
GG
G
G
G
G
G
G
G
G
G
GG G
G
G
G G
G
G
G
G
GG
G
G
G
G
G
G
G
GG
G
G
G
G
GGG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
GG
GG
G
GG
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
GGG
G
G
G
G
G
G
G
G
G
G
G
GG
G
G GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
GG
G
G
G
G
G
G
G
G
GG
G G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
GG
G
G
G
GG
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
GG
G
G
G
G
GG
G
G
GG
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
GG
G
G
G
G
G
G
GG
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
GGG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GGGGG
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GGG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
GG
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
GG
G
G
G
G
G
G
G
G
G
G
G
G
GG G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
GG
GG
G
G
G
G
G G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
GG
G
G
G
G
G
G
G
GGG
G
G
G
G
G
G
G
GG
G
G
G
G GGG
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G G
G G
G
G
G
G
G
GGG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
GG
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
GG
GG
G
GG
G
G
G
G
G
G G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
CBK
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
GG
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
GG
G
G G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
GG
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
GG
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G G G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G G
GG
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
GGG
G G
G
GG
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
GG
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
GG
G
G
G
G
G
G
G G
G
G
GG
G
G
G
G G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
GG
G G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
GG G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G G
G G
GG
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
GGG
GG
G
GG
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
GG
G
G G
G GG
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
GG
G
G
G
G
G
G
G
G
G
G G
G
GG
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
GG
GG
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G G
GG
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
GG
G
G
G
G
GG
G
G
G
GG G
G
G
G
GG
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
GG
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
GG
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
GG
G
G GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
GG
GG G
G
G G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
GG
G
G
G
G
G
G G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
GG
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GGG
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
GG
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GGG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GGGG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
GG
GG
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
GG
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
GG
G
G
G
G
G
G
G G
G
G
G
G
GG G
G
G
G
G G
G
G
G
G G
G
G
G
G
G
GG
G
G
G
GG
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
GG
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GGGG
GG
G
G
G
G
G
G
G
G G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
GG
G
G
G
G
G
G
GG
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G G
G
G
G
GG
G
G
G
G
GG
G
GG
G
G
G
G
G
G
G
G
G
G
G G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
GG
G
GG
G
G
G
G
G
G
G
G
G
G
GG
G
GG
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
GG
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
GG G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G G
G
G
G G
G
G
G
G
G
G G
G
G G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
GG
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
GGG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
DBK
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
GG
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
GG
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG G
G
G
G
GG
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G G
G
G
G
G
G
G G
G
G
G
G
G
G
G G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G G
G
G
G
G
G
G
G
GG
G
G
G
G
GG
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
GGG
G G
G
GG
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
GG
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G GG
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
GG
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
GG G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GGG
GG
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
GGG
GG
G
GG
G
G
G
G
GGG G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
GG
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
GG
G
G
G
G
G
G
G
G
G
G G
G
GG
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GGG
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
GG
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G G
G
G
G
G
GG
G
G
G
GG
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
GG
GG
G
G
G
G
GG
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
GGG
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
GG
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
GGGG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
GG
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G G G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
GG
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
GG
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
GGGG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GGG
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G G
G
G
G G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GGG
G
G
G
G
G
GG
G
G
GG
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
GG
G G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
GG
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
GG
G
G
G
G
G
G
G
GG
GG
G
G
G
G
GG
GG
G
G
G
G
G G
G
G
GG
GG
G
G
G
G
G G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
GG
G
G
GG
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
GG
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GGG
GG
G
G
G
G
G
G
G
G GG
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
GGG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
GG
G G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
GG
G
GG
G
G
G
G G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
GG
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GGG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
GG
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G GG
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G G
GGG
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G G
G
G
G
GG
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
GG
GG
G
GG
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
GG
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G GG
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G G
G
G
G
G G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
GG
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
GG
G
G
G
G
G
G
G
G
G
G
G
GG
G
GG
G
G
G
G
G
G
G G
G
G
G
G
GG
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
GGG
GG
G
G
G
G
G
G
G
G
G G GG
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
GG
GG
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GLE
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
GGG
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
GG
G
GG
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G G
G
G
G
G
G
G
G
GG
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
GG
G
G G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
GG
G
G
G
G G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
GG
G
GG
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G G
G
G
G
GG
G
GG
G
G
G
G
G
G
G
GGG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G G
G G
G G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G G
G
G
GG
G
G
G
G
G
G
G
GG
G
G
GG
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
GG G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
GG
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
GG
G
G
G
G
G
GG
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GGG
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GGG
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
GG
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
GG
G
G
G
G
G
G
G
G
G G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
GG
G
G
G
G
G
G
G
G
GG
G
G G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
GG
G
G
G
G
G
G
G
G GG
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
GG G
G
G
G
G
G
GG
G
G
G G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
GG
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
GGG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
GG
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
GG
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
GG
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
GG
G
G
G
GG
G
GG
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G GG
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
GG
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G G
G
G
GG
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
GG
G
G
G
GG
G
GG
G
G
G
G
G
G
G
GGG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
ISP
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
GG
GG
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
GG
G
G G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
GG
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
GG
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G G
G
GG
G
G
G
G
G
G
G G G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
GG
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
GG
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G GG
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GGG
G G
G
GG
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
GG
G
G
G
G
G
G
G
GGGG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
GG
G
G
G
G
G
GG
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G G
G G
G
G
G
G
G
G
G
G
G
G
G
G
GGG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G G
G
G G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
GG
G
G G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
GG
G
G
G
GG
G
GG
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
GG
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
GG
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
GGG
GG
G
GG
G
G
GG
GGG G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
GG
GG
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
GG
G
G
G
G
G
G
GG G
G
GG G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
GG
G
G
GG
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
GG
GG
G
G
G
G
G
G
G
GG
G
G
G G
G
G
G
G
G
G
G
G
GG
G
G
GG
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G G
GG G
G
G
G
G
G
G
G
G G G
G
G
G G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
GG
G G
G
G
G
G
G
GG
G
G
G
G
GG
GG
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
GG
G
G
G
G
G
GG
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
GGG
G
GG
G
G
G
G
G
G G
G
GG
G
G
G
G
G
G
G G
G
G
GG
G
G
G
G
G
G
G
G
G G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
GG
G
G
G
G
GGGG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
GG
G
G
G
GG
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
GG
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
GG
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GGGG
G
G
G
G
G
G G
G
G
G
G
G
G
G
G GG
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GGG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
GG
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GGG
G
GG
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GGG
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG GGG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G G
G
G
G
G
G
G
G
G
G
G
G
GG
GG
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
GG
G
G
G
G
G
G
G
GG G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
GG
G
GG
G
G
G
G
G
G
GG
G
G
GG
G
G
G
G
G
G
G
G
GG G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GGG
GG
G
G
G
G
G
G G
G GG
G G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
GGG
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
GG
G G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
GG
G G
G G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G G
G
G
G
G G
G
G
G
G
G G
G
G G
G
G
G
G
G
GG
G
G
G
G
G
G
G G
G
G
G
G
G
G
G G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G G
GG
G
G G
G
G
G
G
GG
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
GG
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
GG
GGG G
GGG G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
GG
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
GG
G
G
G
G
G
GG
G
G G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
GG
G
GG
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
GG
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GGG
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
GG
G
G
G G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G G
G
G
GG
G
G
G
G
G
G
GG
G
G
G
G
GG
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G GG
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
GG
G
G
G
G
G
G G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
GG
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G GG
G
GG
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
GG
G
G
GG
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
GG
G
G
G G
G
G
G
G
G
G
G
G
G
G G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GGG
GG
G
G
G
G
G
G
G
G
G G GG
G
G
G
G
G
GG
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
GG
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
GGG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GGG
G
G
GG
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
GG
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
GG G
G
G
G
G
GG
G GG
G
G
GG
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
GG
G
GG
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
GG
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GGG
GG
G
G
G
G
G
G
G
GGGGG
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
GG
G
G G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G GG
G G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
GG
G
G GG G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
GG
G
G
G
G
GG
G
G
GG
G
GG
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
GG
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
GG G
G
G
G
G
G
G
G
G
G
GGG
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G G
G
GG
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
GG G
G
G
G
G
G
G
GG
GG
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
SAN
Figure B.2: Scatter plots of pairwise copula data in U (upper right triangle) and Z
(lower left triangle) scale. Copula data was generated by marginal DLM ﬁts for 2 345
daily log return observations of years 2005 to 2013. For visibility only a random sample
of observations of size 700 is shown.
212

B.2
Time-varying ARMA DLM
SXXP
 0.01 
 0.05 
 0.1 
 0.01 
 0.05 
 0.01 
 0.05 
 0.01 
 0.05 
 0.1 
 0.01 
 0.05 
 0.01 
 0.05 
 0.01 
 0.05 
 0.01 
 0.05 
 0.01 
 0.05 
G
τ = 0.73
ρS = 0.89
SX7P
 0.01 
 0.05 
 0.01 
 0.05 
 0.01 
 0.05 
 0.1 
 0.01 
 0.05 
 0.01 
 0.05 
 0.1 
 0.01 
 0.05 
 0.1 
 0.01 
 0.05 
 0.01 
 0.05 
 0.1 
G
τ = 0.52
ρS = 0.70
G
τ = 0.61
ρS = 0.80
ACA
 0.01 
 0.05 
 0.01 
 0.05 
 0.1 
 0.01 
 0.05 
 0.01 
 0.05 
 0.1 
 0.01 
 0.05 
 0.01 
 0.05 
 0.1 
 0.01 
 0.05 
G
τ = 0.57
ρS = 0.76
G
τ = 0.65
ρS = 0.83
G
τ = 0.51
ρS = 0.69
BBVA
 0.01 
 0.05 
 0.1 
 0.01 
 0.05 
 0.01 
 0.05 
 0.01 
 0.05 
 0.1 
 0.01 
 0.05 
 0.01 
 0.05 
 0.1 
G
τ = 0.57
ρS = 0.75
G
τ = 0.67
ρS = 0.85
G
τ = 0.57
ρS = 0.76
G
τ = 0.55
ρS = 0.73
BNP
 0.01 
 0.05 
 0.01 
 0.05 
 0.01 
 0.05 
 0.1 
 0.01 
 0.05 
 0.01 
 0.05 
G
τ = 0.44
ρS = 0.61
G
τ = 0.52
ρS = 0.70
G
τ = 0.45
ρS = 0.62
G
τ = 0.44
ρS = 0.61
G
τ = 0.44
ρS = 0.61
CBK
 0.01 
 0.05 
 0.01 
 0.05 
 0.1 
 0.01 
 0.05 
 0.01 
 0.05 
G
τ = 0.58
ρS = 0.76
G
τ = 0.67
ρS = 0.85
G
τ = 0.52
ρS = 0.71
G
τ = 0.53
ρS = 0.72
G
τ = 0.57
ρS = 0.76
G
τ = 0.50
ρS = 0.68
DBK
 0.01 
 0.05 
 0.01 
 0.05 
 0.01 
 0.05 
G
τ = 0.54
ρS = 0.72
G
τ = 0.65
ρS = 0.83
G
τ = 0.58
ρS = 0.76
G
τ = 0.52
ρS = 0.70
G
τ = 0.63
ρS = 0.81
G
τ = 0.45
ρS = 0.62
G
τ = 0.55
ρS = 0.73
GLE
 0.01 
 0.05 
 0.01 
 0.05 
G
τ = 0.47
ρS = 0.64
G
τ = 0.54
ρS = 0.73
G
τ = 0.45
ρS = 0.62
G
τ = 0.48
ρS = 0.66
G
τ = 0.47
ρS = 0.64
G
τ = 0.39
ρS = 0.54
G
τ = 0.47
ρS = 0.65
G
τ = 0.47
ρS = 0.64
ISP
 0.01 
 0.05 
G
τ = 0.57
ρS = 0.76
G
τ = 0.64
ρS = 0.82
G
τ = 0.50
ρS = 0.69
G
τ = 0.69
ρS = 0.85
G
τ = 0.55
ρS = 0.74
G
τ = 0.42
ρS = 0.58
G
τ = 0.54
ρS = 0.72
G
τ = 0.52
ρS = 0.70
G
τ = 0.47
ρS = 0.64
SAN
Figure B.3: Contour plot of bivariate copula density in Z space (upper right triangle),
and empirical Kendall’s tau (τ) and Spearman’s ρ (ρS) values (lower left triangle). Copula
data was generated by marginal DLM ﬁts for 2 345 daily log return observations of years
2005 to 2013.
213

B
Empirical study: Figures and tables
B.3
Vine copula models
In this section, tables with the details of the ﬁtted vine copula models are given. The tables
show copula families with theoretical Kendall’s tau value of the parameter estimate in
parentheses, and for the Student’s t copula the degrees of freedom parameter ν in subscript
one the left. The corresponding RVM is given on the right. The following abbreviations
are used in the RVM for the bank stocks: S1: ACA, S2: BBVA, S3: BNP, S4: CBK, S5:
DBK, S6: GLE, S7: ISP and S8: SAN. More details are given in Section 9.3.1.
B.3.1
ARMA-GARCH vine copula ﬁt tables
This subsection contains tables with the details of the vine copula models ﬁt to the
ARMA-GARCH copula data.
214

B.3
Vine copula models
Families
RV I, ARMA-GARCH
I
I
F(-.04)
I
I
I
I
t22.1(.05)
I
SC(.02)
I
I
t15.6(.04)
t19.8(.02)
BB8(.08)
t16.9(-.05)
I
I
t13.6(.02)
SBB8(.04)
BB8(.04)
I
t13.7(.02)
t30(.04)
I
t17.5(.1)
t23.9(.03)
I
t14.2(-.09)
t10.6(.26)
t9.8(.09)
t12.1(.06)
SBB8(.23)
t7.7(.24)
t11.8(.09)
SBB8(.17)
t9.3(.71)
t3.4(.68)
t5.4(.64)
t7.2(.53)
t6.4(.59)
t6(.64)
t5.7(.65)
t5.6(.5)
t5.7(.65)
RVM
I1
S4
S8
S5
S4
S2
S3
S5
S4
S7
S8
S3
S5
S4
S1
S2
S6
S3
S5
S4
S6
S7
S1
S6
S3
S5
S4
S3
S1
S7
S1
S6
S3
S5
S4
S4
S6
I2
S7
S1
S6
S3
S5
S5
S5
I2
S2
I2
I2
I2
I2
I2
I2
I2
I2
Families
RV I
R, ARMA-GARCH
I
I
F(-.04)
I
I
I
I
t22.1(.05)
I
F(.03)
I
I
t15.8(.04)
t20.2(.02)
F(.08)
t18.2(-.05)
I
I
t13.6(.02)
SG(.03)
G(.03)
I
t13.7(.02)
t30(.04)
t14.3(.02)
t18.8(.11)
t23.9(.03)
I
t14.2(-.09)
t10.6(.26)
t9.8(.09)
t12.1(.06)
t8.4(.2)
t7.7(.24)
t11.8(.09)
t17.6(.15)
t9.3(.71)
t3.4(.68)
t5.4(.64)
t7.2(.53)
t6.4(.59)
t6(.64)
t5.7(.65)
t5.6(.5)
t5.7(.65)
RVM
I1
S4
S8
S5
S4
S2
S3
S5
S4
S7
S8
S3
S5
S4
S1
S2
S6
S3
S5
S4
S6
S7
S1
S6
S3
S5
S4
S3
S1
S7
S1
S6
S3
S5
S4
S4
S6
I2
S7
S1
S6
S3
S5
S5
S5
I2
S2
I2
I2
I2
I2
I2
I2
I2
I2
Table B.1: Bivariate copula families and RVMs of the R-vine copulas with index data with (top) and without (bottom) restriction
of choices for bivariate linking copula families RVI and RV I
R, respectively. Theoretical Kendall’s tau values are given in parentheses
behind the family name, and values above 0.2 emphasized in bold face. For the Student’s t copula, the degrees of freedom parameter
ν is stated as subscript tν. The following abbreviations are used for stock and indices names in the RVM: I1: SXXP, I2: SX7P, S1:
ACA, S2: BBVA, S3: BNP, S4: CBK, S5: DBK, S6: GLE, S7: ISP and S8: SAN.
215

B
Empirical study: Figures and tables
Families
CV I, ARMA-GARCH
I
I
I
N(.03)
I
I
t14.4(.05)
BB8 270(-.05)
I
BB8(.03)
I
I
t12.8(.06)
t17.3(.14)
t24.6(.02)
I
t17.1(-.07)
t8.1(.2)
G(.03)
t18.6(.05)
I
t4.7(.4)
t8.2(.03)
t13(.06)
I
t10.5(.03)
I
t11.2(.07)
SBB8(.05)
Tawn2 270(-.03)
t13.8(.17)
F(.1)
SBB8(.07)
SBB8(.23)
t12.1(.06)
t20.3(.04)
t5.4(.64)
t9.3(.71)
t5.7(.65)
t5.6(.5)
t5.7(.65)
t6(.64)
t7.2(.53)
t6.4(.59)
t5.8(.63)
RVM
S2
S3
I1
I1
S3
S3
S4
S4
S4
S4
S7
S7
S7
S7
S5
S5
S5
S5
S5
S7
S6
S6
S6
S6
S6
S6
S7
S7
S8
S8
S8
S8
S8
S8
S8
S1
S1
S1
S1
S1
S1
S1
S1
S8
S8
I2
I2
I2
I2
I2
I2
I2
I2
I2
I2
Families
CV I
R, ARMA-GARCH
I
I
I
N(.03)
I
I
t14.9(.06)
t25.3(-.05)
I
F(.03)
I
I
t12.9(.06)
t17.8(.14)
t24.7(.02)
I
t20.2(-.07)
t8.3(.2)
G(.02)
t19.2(.05)
t15.6(.03)
t4.7(.4)
t8.1(.03)
t13(.06)
I
t10.5(.03)
I
t11.2(.07)
t27.2(.04)
N(-.05)
t13.8(.17)
F(.1)
F(.07)
t8.4(.2)
t12.1(.06)
t20.3(.04)
t5.4(.64)
t9.3(.71)
t5.7(.65)
t5.6(.5)
t5.7(.65)
t6(.64)
t7.2(.53)
t6.4(.59)
t5.8(.63)
RVM
S2
S3
I1
I1
S3
S3
S4
S4
S4
S4
S7
S7
S7
S7
S5
S5
S5
S5
S5
S7
S6
S6
S6
S6
S6
S6
S7
S7
S8
S8
S8
S8
S8
S8
S8
S1
S1
S1
S1
S1
S1
S1
S1
S8
S8
I2
I2
I2
I2
I2
I2
I2
I2
I2
I2
Table B.2: Bivariate copula families and RVMs of the R-vine copulas with index data with (top) and without (bottom) restriction
of choices for bivariate linking copula families CVI and CV I
R, respectively. Theoretical Kendall’s tau values are given in parentheses
behind the family name, and values above 0.2 emphasized in bold face. For the Student’s t copula, the degrees of freedom parameter
ν is stated as subscript tν. The following abbreviations are used for stock and indices names in the RVM: I1: SXXP, I2: SX7P, S1:
ACA, S2: BBVA, S3: BNP, S4: CBK, S5: DBK, S6: GLE, S7: ISP and S8: SAN.
216

B.3
Vine copula models
Families
rv2I, ARMA-GARCH
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
t14.2(-.09)
t10.6(.26)
t9.8(.09)
t12.1(.06)
SBB8(.23)
t7.7(.24)
t11.8(.09)
SBB8(.17)
t9.3(.71)
t3.4(.68)
t5.4(.64)
t7.2(.53)
t6.4(.59)
t6(.64)
t5.7(.65)
t5.6(.5)
t5.7(.65)
RVM
I1
S4
S8
S5
S4
S2
S3
S5
S4
S7
S8
S3
S5
S4
S1
S2
S6
S3
S5
S4
S6
S7
S1
S6
S3
S5
S4
S3
S1
S7
S1
S6
S3
S5
S4
S4
S6
I2
S7
S1
S6
S3
S5
S5
S5
I2
S2
I2
I2
I2
I2
I2
I2
I2
I2
Families
rv2I
R, ARMA-GARCH
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
t14.2(-.09)
t10.6(.26)
t9.8(.09)
t12.1(.06)
t8.4(.2)
t7.7(.24)
t11.8(.09)
t17.6(.15)
t9.3(.71)
t3.4(.68)
t5.4(.64)
t7.2(.53)
t6.4(.59)
t6(.64)
t5.7(.65)
t5.6(.5)
t5.7(.65)
RVM
I1
S4
S8
S5
S4
S2
S3
S5
S4
S7
S8
S3
S5
S4
S1
S2
S6
S3
S5
S4
S6
S7
S1
S6
S3
S5
S4
S3
S1
S7
S1
S6
S3
S5
S4
S4
S6
I2
S7
S1
S6
S3
S5
S5
S5
I2
S2
I2
I2
I2
I2
I2
I2
I2
I2
Table B.3: Bivariate copula families and RVMs of the R-vine copulas with index data with (top) and without (bottom) restriction
of choices for bivariate linking copula families rv2I and rv2I
R, respectively. Theoretical Kendall’s tau values are given in parentheses
behind the family name, and values above 0.2 emphasized in bold face. For the Student’s t copula, the degrees of freedom parameter
ν is stated as subscript tν. The following abbreviations are used for stock and indices names in the RVM: I1: SXXP, I2: SX7P, S1:
ACA, S2: BBVA, S3: BNP, S4: CBK, S5: DBK, S6: GLE, S7: ISP and S8: SAN.
217

B
Empirical study: Figures and tables
Families
cv2I, ARMA-GARCH
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
Tawn2 270(-.03)
SBB8(.05)
SBB8(.07)
SBB8(.23)
F(.1)
t13.8(.17)
t12.1(.06)
t20.3(.04)
t9.3(.71)
t5.4(.64)
t5.7(.65)
t6(.64)
t5.6(.5)
t5.7(.65)
t7.2(.53)
t6.4(.59)
t5.8(.63)
RVM
I1
S5
S2
S2
S5
S5
S6
S6
S6
S6
S7
S7
S7
S7
S4
S4
S4
S4
S4
S7
S3
S3
S3
S3
S3
S3
S7
S7
S8
S8
S8
S8
S8
S8
S8
S1
S1
S1
S1
S1
S1
S1
S1
S8
S8
I2
I2
I2
I2
I2
I2
I2
I2
I2
I2
Families
cv2I
R, ARMA-GARCH
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
t13.8(.17)
N(-.05)
t27.2(.04)
F(.1)
F(.07)
t8.4(.2)
t12.1(.06)
t20.3(.04)
t5.7(.65)
t9.3(.71)
t5.4(.64)
t5.6(.5)
t5.7(.65)
t6(.64)
t7.2(.53)
t6.4(.59)
t5.8(.63)
RVM
S3
S4
I1
I1
S4
S2
S2
S2
S4
S4
S7
S7
S7
S7
S5
S5
S5
S5
S5
S7
S6
S6
S6
S6
S6
S6
S7
S7
S8
S8
S8
S8
S8
S8
S8
S1
S1
S1
S1
S1
S1
S1
S1
S8
S8
I2
I2
I2
I2
I2
I2
I2
I2
I2
I2
Table B.4: Bivariate copula families and RVMs of the R-vine copulas with index data with (top) and without (bottom) restriction
of choices for bivariate linking copula families cv2I and cv2I
R, respectively. Theoretical Kendall’s tau values are given in parentheses
behind the family name, and values above 0.2 emphasized in bold face. For the Student’s t copula, the degrees of freedom parameter
ν is stated as subscript tν. The following abbreviations are used for stock and indices names in the RVM: I1: SXXP, I2: SX7P, S1:
ACA, S2: BBVA, S3: BNP, S4: CBK, S5: DBK, S6: GLE, S7: ISP and S8: SAN.
218

B.3
Vine copula models
Families
RV , ARMA-GARCH
t27.5(.04)
N(.06)
SBB8(.08)
t24(.04)
t18.2(.05)
t14.3(.08)
BB8(.1)
t19.8(.09)
BB8(.07)
t16.1(.11)
BB8(.1)
SBB8(.15)
t19.7(.1)
F(.1)
t13(.18)
F(.14)
t9.7(.23)
t11.1(.21)
t7.5(.24)
t9.8(.13)
t7.8(.18)
t5.6(.48)
t4.3(.56)
t4.2(.61)
t5.4(.56)
t5.6(.47)
t3.4(.68)
t5.4(.53)
RVM
S4
S7
S1
S2
S7
S6
S8
S2
S7
S5
S1
S8
S2
S7
S7
S6
S5
S8
S2
S3
S2
S3
S3
S5
S8
S8
S3
S8
S5
S6
S3
S3
S2
S8
S3
S3
Families
RVR, ARMA-GARCH
t26.9(.04)
N(.06)
F(.08)
t24(.04)
t18.3(.05)
t13.9(.08)
F(.11)
t19.6(.09)
t22.3(.07)
t16.1(.11)
t21.1(.09)
F(.15)
t19.7(.1)
F(.1)
t13(.18)
F(.14)
t9.7(.23)
t11.1(.21)
t7.5(.24)
t9.8(.13)
t7.8(.18)
t5.6(.48)
t4.3(.56)
t4.2(.61)
t5.4(.56)
t5.6(.47)
t3.4(.68)
t5.4(.53)
RVM
S4
S7
S1
S2
S7
S6
S8
S2
S7
S5
S1
S8
S2
S7
S7
S6
S5
S8
S2
S3
S2
S3
S3
S5
S8
S8
S3
S8
S5
S6
S3
S3
S2
S8
S3
S3
Families
CV , ARMA-GARCH
I
BB8(.05)
I
t15.6(.06)
t24.6(.04)
t15.9(.05)
t9.4(.2)
BB8(.12)
t13.1(.09)
t18.5(.04)
t17.9(.16)
SBB8(.26)
t14.1(.13)
SBB8(.16)
t19.8(.11)
t15.1(.18)
SBB8(.2)
t8.1(.24)
t14.4(.2)
t8.4(.23)
t4.4(.52)
t4.2(.61)
t6.6(.42)
t6.6(.46)
t5.4(.55)
t5.4(.56)
t5.1(.53)
t5.4(.53)
RVM
S6
S8
S4
S4
S8
S7
S7
S7
S8
S1
S1
S1
S1
S8
S5
S5
S5
S5
S5
S8
S2
S2
S2
S2
S2
S2
S8
S8
S3
S3
S3
S3
S3
S3
S3
S3
Families
CVR, ARMA-GARCH
I
F(.06)
I
t15.6(.06)
t24.7(.04)
t15.8(.05)
t9.5(.2)
F(.12)
t13.1(.09)
t18.5(.04)
t17.9(.16)
F(.26)
t14.1(.13)
F(.17)
t19.8(.11)
t15.1(.18)
F(.2)
t8.1(.24)
t14.4(.2)
t8.4(.23)
t4.4(.52)
t4.2(.61)
t6.6(.42)
t6.6(.46)
t5.4(.55)
t5.4(.56)
t5.1(.53)
t5.4(.53)
RVM
S6
S8
S4
S4
S8
S7
S7
S7
S8
S1
S1
S1
S1
S8
S5
S5
S5
S5
S5
S8
S2
S2
S2
S2
S2
S2
S8
S8
S3
S3
S3
S3
S3
S3
S3
S3
Table B.5: Bivariate copula families and RVMs of the R- and C-vine copulas without
index data with and without restriction of choices for bivariate linking copula families.
Theoretical Kendall’s tau values are given in parentheses behind the family name, and
values above 0.2 emphasized in bold face. For the Student’s t copula, the degrees of
freedom parameter ν is stated as subscript tν. The following abbreviations are used for
stock and indices names in the RVM: S1: ACA, S2: BBVA, S3: BNP, S4: CBK, S5:
DBK, S6: GLE, S7: ISP and S8: SAN.
219

B
Empirical study: Figures and tables
Families
rv2, ARMA-GARCH
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
F(.14)
t9.7(.23)
t11.1(.21)
t7.5(.24)
t9.8(.13)
t7.8(.18)
t5.6(.48)
t4.3(.56)
t4.2(.61)
t5.4(.56)
t5.6(.47)
t3.4(.68)
t5.4(.53)
RVM
S4
S7
S1
S2
S7
S6
S8
S2
S7
S5
S1
S8
S2
S7
S7
S6
S5
S8
S2
S3
S2
S3
S3
S5
S8
S8
S3
S8
S5
S6
S3
S3
S2
S8
S3
S3
Families
rv2R, ARMA-GARCH
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
F(.14)
t9.7(.23)
t11.1(.21)
t7.5(.24)
t9.8(.13)
t7.8(.18)
t5.6(.48)
t4.3(.56)
t4.2(.61)
t5.4(.56)
t5.6(.47)
t3.4(.68)
t5.4(.53)
RVM
S4
S7
S1
S2
S7
S6
S8
S2
S7
S5
S1
S8
S2
S7
S7
S6
S5
S8
S2
S3
S2
S3
S3
S5
S8
S8
S3
S8
S5
S6
S3
S3
S2
S8
S3
S3
Families
cv2, ARMA-GARCH
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
SBB8(.2)
t8.1(.24)
t15.1(.18)
t14.4(.2)
t8.4(.23)
t4.4(.52)
t6.6(.42)
t6.6(.46)
t4.2(.61)
t5.4(.55)
t5.4(.56)
t5.1(.53)
t5.4(.53)
RVM
S4
S8
S7
S7
S8
S6
S6
S6
S8
S1
S1
S1
S1
S8
S5
S5
S5
S5
S5
S8
S2
S2
S2
S2
S2
S2
S8
S8
S3
S3
S3
S3
S3
S3
S3
S3
Families
cv2R, ARMA-GARCH
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
F(.2)
t8.1(.24)
t15.1(.18)
t14.4(.2)
t8.4(.23)
t4.4(.52)
t6.6(.42)
t6.6(.46)
t4.2(.61)
t5.4(.55)
t5.4(.56)
t5.1(.53)
t5.4(.53)
RVM
S4
S8
S7
S7
S8
S6
S6
S6
S8
S1
S1
S1
S1
S8
S5
S5
S5
S5
S5
S8
S2
S2
S2
S2
S2
S2
S8
S8
S3
S3
S3
S3
S3
S3
S3
S3
Table B.6: Bivariate copula families and RVMs of the R- and C-vine copulas without
index data with and without restriction of choices for bivariate linking copula families.
Theoretical Kendall’s tau values are given in parentheses behind the family name, and
values above 0.2 emphasized in bold face. For the Student’s t copula, the degrees of
freedom parameter ν is stated as subscript tν. The following abbreviations are used for
stock and indices names in the RVM: S1: ACA, S2: BBVA, S3: BNP, S4: CBK, S5:
DBK, S6: GLE, S7: ISP and S8: SAN.
220

B.3
Vine copula models
B.3.2
DLM vine copula ﬁt tables
This subsections contains tables with the details of the vine copula models ﬁt to the DLM
copula data.
221

B
Empirical study: Figures and tables
Families
RV I
R, DLM
t22.4(.05)
F(.05)
I
t14.1(.02)
I
I
t18.6(.06)
BB8 270(-.05)
I
t27.5(.03)
I
I
I
I
N(-.04)
I
I
t17.5(.04)
t22.4(.03)
I
I
t15.8(.12)
I
BB8(.04)
SBB8(.05)
t26.6(.03)
SBB8(.03)
t14.6(.03)
SBB8(.28)
t17.5(-.07)
SBB8(.25)
SBB8(.18)
F(.11)
SBB8(.08)
t11.2(.09)
t10.9(.26)
t6.8(.67)
t8.3(.73)
t7(.65)
t6.8(.67)
t6.7(.52)
t7.7(.61)
t7.5(.55)
t8.9(.66)
t4.2(.7)
RVM
S3
S8
I1
S2
S8
S6
S7
S2
S8
S5
S5
S7
S2
S8
S4
S4
S5
S7
S2
S8
S1
I1
S4
S5
S7
S2
S8
S7
S1
S1
S4
S1
S7
S2
S8
I2
S6
S6
S1
S4
S1
S7
S2
S8
S8
I2
I2
I2
I2
I2
I2
I2
S2
S2
S2
Families
RV I
R, DLM
t22.6(.05)
F(.05)
I
t15.9(.02)
I
I
t18.9(.06)
t18.2(-.03)
I
t27.1(.03)
I
I
SG(.02)
I
N(-.04)
I
I
t18.7(.04)
t25.7(.03)
I
I
t21.1(.11)
I
G(.02)
t25.8(.05)
t26.3(.03)
t30(.03)
t14.6(.03)
F(.29)
t17.5(-.07)
t8.5(.22)
F(.18)
F(.11)
t15.5(.07)
t11.2(.09)
t10.9(.26)
t6.8(.67)
t8.3(.73)
t7(.65)
t6.8(.67)
t6.7(.52)
t7.7(.61)
t7.5(.55)
t8.9(.66)
t4.2(.7)
RVM
S3
S8
I1
S2
S8
S6
S7
S2
S8
S5
S5
S7
S2
S8
S4
S4
S5
S7
S2
S8
S1
I1
S4
S5
S7
S2
S8
S7
S1
S1
S4
S1
S7
S2
S8
I2
S6
S6
S1
S4
S1
S7
S2
S8
S8
I2
I2
I2
I2
I2
I2
I2
S2
S2
S2
Table B.7: Bivariate copula families and RVMs of the R-vine copulas with index data with (top) and without (bottom) restriction
of choices for bivariate linking copula families RVI and RV I
R, respectively. Theoretical Kendall’s tau values are given in parentheses
behind the family name, and values above 0.2 emphasized in bold face. For the Student’s t copula, the degrees of freedom parameter
ν is stated as subscript tν. The following abbreviations are used for stock and indices names in the RVM: I1: SXXP, I2: SX7P, S1:
ACA, S2: BBVA, S3: BNP, S4: CBK, S5: DBK, S6: GLE, S7: ISP and S8: SAN.
222

B.3
Vine copula models
Families
CV I, DLM
I
I
I
I
I
t21.4(.06)
t16.2(.05)
BB8 270(-.05)
I
F(.03)
I
I
t16.4(.11)
F(.03)
SBB8(.06)
F(.05)
BB8 270(-.04)
I
SBB8(.18)
F(.09)
BB8(.05)
t5.5(.41)
t8.9(.02)
BB8(.08)
t14.8(.03)
t24.6(.03)
I
t13.1(.09)
I
t17.5(-.07)
SBB8(.28)
t15.8(.07)
SBB8(.25)
BB8(.08)
t14.9(.05)
I
t8.9(.66)
t8.3(.73)
t6.8(.67)
t6.8(.67)
t7.7(.61)
t6.7(.52)
t7.5(.55)
t7(.65)
t9.2(.65)
RVM
S2
S3
I1
I1
S3
S3
S5
S5
S5
S5
S7
S7
S7
S7
S1
S1
S1
S1
S1
S7
S4
S4
S4
S4
S4
S4
S7
S7
S8
S8
S8
S8
S8
S8
S8
S6
S6
S6
S6
S6
S6
S6
S6
S8
S8
I2
I2
I2
I2
I2
I2
I2
I2
I2
I2
Families
CV I
R, DLM
I
I
I
I
I
t21.5(.06)
t16.1(.05)
F(-.04)
I
F(.03)
I
I
t22.3(.11)
F(.03)
F(.06)
F(.05)
t23.9(-.02)
I
F(.18)
F(.09)
F(.05)
t5.5(.41)
t8.9(.02)
t17.7(.07)
t14.8(.03)
t26.6(.03)
I
t13.1(.09)
I
t17.5(-.07)
F(.29)
t15.8(.07)
t8.5(.22)
t24.5(.06)
t14.9(.05)
I
t8.9(.66)
t8.3(.73)
t6.8(.67)
t6.8(.67)
t7.7(.61)
t6.7(.52)
t7.5(.55)
t7(.65)
t9.2(.65)
RVM
S2
S3
I1
I1
S3
S3
S5
S5
S5
S5
S7
S7
S7
S7
S1
S1
S1
S1
S1
S7
S4
S4
S4
S4
S4
S4
S7
S7
S8
S8
S8
S8
S8
S8
S8
S6
S6
S6
S6
S6
S6
S6
S6
S8
S8
I2
I2
I2
I2
I2
I2
I2
I2
I2
I2
Table B.8: Bivariate copula families and RVMs of the R-vine copulas with index data with (top) and without (bottom) restriction
of choices for bivariate linking copula families CVI and CV I
R, respectively. Theoretical Kendall’s tau values are given in parentheses
behind the family name, and values above 0.2 emphasized in bold face. For the Student’s t copula, the degrees of freedom parameter
ν is stated as subscript tν. The following abbreviations are used for stock and indices names in the RVM: I1: SXXP, I2: SX7P, S1:
ACA, S2: BBVA, S3: BNP, S4: CBK, S5: DBK, S6: GLE, S7: ISP and S8: SAN.
223

B
Empirical study: Figures and tables
Families
rv2I, DLM
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
SBB8(.28)
t17.5(-.07)
SBB8(.25)
SBB8(.18)
F(.11)
SBB8(.08)
t11.2(.09)
t10.9(.26)
t6.8(.67)
t8.3(.73)
t7(.65)
t6.8(.67)
t6.7(.52)
t7.7(.61)
t7.5(.55)
t8.9(.66)
t4.2(.7)
RVM
S3
S8
I1
S2
S8
S6
S7
S2
S8
S5
S5
S7
S2
S8
S4
S4
S5
S7
S2
S8
S1
I1
S4
S5
S7
S2
S8
S7
S1
S1
S4
S1
S7
S2
S8
I2
S6
S6
S1
S4
S1
S7
S2
S8
S8
I2
I2
I2
I2
I2
I2
I2
S2
S2
S2
Families
rv2I
R, DLM
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
t17.5(-.07)
F(.29)
t8.5(.22)
F(.18)
F(.11)
t15.5(.07)
t11.2(.09)
t10.9(.26)
t8.3(.73)
t6.8(.67)
t7(.65)
t6.8(.67)
t6.7(.52)
t7.7(.61)
t7.5(.55)
t8.9(.66)
t4.2(.7)
RVM
I1
S3
S3
S8
S8
S6
S2
S2
S8
S5
S7
S7
S2
S8
S4
S5
S5
S7
S2
S8
S1
S4
S4
S5
S7
S2
S8
S7
S1
S1
S4
S1
S7
S2
S8
I2
S6
S6
S1
S4
S1
S7
S2
S8
S8
I2
I2
I2
I2
I2
I2
I2
S2
S2
S2
Table B.9: Bivariate copula families and RVMs of the R-vine copulas with index data with (top) and without (bottom) restriction
of choices for bivariate linking copula families rv2I and rv2I
R, respectively. Theoretical Kendall’s tau values are given in parentheses
behind the family name, and values above 0.2 emphasized in bold face. For the Student’s t copula, the degrees of freedom parameter
ν is stated as subscript tν. The following abbreviations are used for stock and indices names in the RVM: I1: SXXP, I2: SX7P, S1:
ACA, S2: BBVA, S3: BNP, S4: CBK, S5: DBK, S6: GLE, S7: ISP and S8: SAN.
224

B.3
Vine copula models
Families
cv2I, DLM
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
SBB8(.28)
t17.5(-.07)
I
BB8(.08)
t15.8(.07)
SBB8(.25)
t14.9(.05)
I
t6.8(.67)
t8.3(.73)
t8.9(.66)
t6.7(.52)
t6.8(.67)
t7.7(.61)
t7.5(.55)
t7(.65)
t9.2(.65)
RVM
S3
S4
I1
I1
S4
S2
S2
S2
S4
S4
S7
S7
S7
S7
S5
S5
S5
S5
S5
S7
S1
S1
S1
S1
S1
S1
S7
S7
S8
S8
S8
S8
S8
S8
S8
S6
S6
S6
S6
S6
S6
S6
S6
S8
S8
I2
I2
I2
I2
I2
I2
I2
I2
I2
I2
Families
cv2I
R, DLM
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
F(.29)
t17.5(-.07)
I
t24.5(.06)
t15.8(.07)
t8.5(.22)
t14.9(.05)
I
t6.8(.67)
t8.3(.73)
t8.9(.66)
t6.7(.52)
t6.8(.67)
t7.7(.61)
t7.5(.55)
t7(.65)
t9.2(.65)
RVM
S3
S4
I1
I1
S4
S2
S2
S2
S4
S4
S7
S7
S7
S7
S5
S5
S5
S5
S5
S7
S1
S1
S1
S1
S1
S1
S7
S7
S8
S8
S8
S8
S8
S8
S8
S6
S6
S6
S6
S6
S6
S6
S6
S8
S8
I2
I2
I2
I2
I2
I2
I2
I2
I2
I2
Table B.10: Bivariate copula families and RVMs of the R-vine copulas with index data with (top) and without (bottom) restriction
of choices for bivariate linking copula families cv2I and cv2I
R, respectively. Theoretical Kendall’s tau values are given in parentheses
behind the family name, and values above 0.2 emphasized in bold face. For the Student’s t copula, the degrees of freedom parameter
ν is stated as subscript tν. The following abbreviations are used for stock and indices names in the RVM: I1: SXXP, I2: SX7P, S1:
ACA, S2: BBVA, S3: BNP, S4: CBK, S5: DBK, S6: GLE, S7: ISP and S8: SAN.
225

B
Empirical study: Figures and tables
Families
RV , DLM
SG(.03)
F(.06)
F(.08)
t20.3(.03)
SBB8(.06)
t18.4(.07)
BB8(.09)
F(.09)
BB8(.08)
t18.4(.12)
BB8(.09)
SBB8(.15)
F(.12)
F(.12)
SBB8(.2)
F(.13)
t10.5(.24)
t11.9(.21)
t8.5(.24)
t11(.14)
t9.5(.18)
F(.54)
t4.7(.59)
t4.5(.63)
t6.8(.58)
t6.4(.49)
t4.2(.7)
t6.9(.56)
RVM
S4
S7
S1
S2
S7
S6
S8
S2
S7
S5
S1
S8
S2
S7
S7
S6
S5
S8
S2
S3
S2
S3
S3
S5
S8
S8
S3
S8
S5
S6
S3
S3
S2
S8
S3
S3
Families
RVR, DLM
SG(.03)
F(.07)
F(.08)
t21.6(.03)
F(.07)
t17.3(.07)
t24(.08)
F(.09)
t24(.07)
t18(.12)
t19.3(.09)
F(.15)
F(.12)
F(.12)
F(.2)
F(.13)
t10.5(.24)
t11.9(.21)
t8.5(.24)
t11(.14)
t9.5(.18)
F(.54)
t4.7(.59)
t4.5(.63)
t6.8(.58)
t6.4(.49)
t4.2(.7)
t6.9(.56)
RVM
S4
S7
S1
S2
S7
S6
S8
S2
S7
S5
S1
S8
S2
S7
S7
S6
S5
S8
S2
S3
S2
S3
S3
S5
S8
S8
S3
S8
S5
S6
S3
S3
S2
S8
S3
S3
Families
CV , DLM
I
BB8(.04)
I
t19.2(.05)
t24.7(.04)
t17.4(.06)
t9.5(.2)
BB8(.11)
SBB8(.1)
t24.8(.03)
t20.1(.16)
F(.25)
t16.1(.14)
SBB8(.17)
t19.2(.1)
SBB8(.21)
t21.7(.18)
t8.6(.25)
F(.22)
t9.7(.24)
t4.7(.52)
t4.5(.63)
F(.48)
t7.3(.48)
t6.5(.58)
t6.8(.58)
t6.8(.56)
t6.9(.56)
RVM
S6
S8
S4
S4
S8
S7
S7
S7
S8
S1
S1
S1
S1
S8
S5
S5
S5
S5
S5
S8
S2
S2
S2
S2
S2
S2
S8
S8
S3
S3
S3
S3
S3
S3
S3
S3
Families
CVR, DLM
I
t27.3(.04)
I
t19.2(.05)
t24(.04)
t17.2(.06)
t9.6(.2)
F(.12)
F(.1)
t24.5(.03)
t19.6(.16)
F(.25)
t16.1(.14)
F(.17)
t19.2(.1)
F(.21)
t21.7(.18)
t8.6(.25)
F(.22)
t9.7(.24)
t4.7(.52)
t4.5(.63)
F(.48)
t7.3(.48)
t6.5(.58)
t6.8(.58)
t6.8(.56)
t6.9(.56)
RVM
S6
S8
S4
S4
S8
S7
S7
S7
S8
S1
S1
S1
S1
S8
S5
S5
S5
S5
S5
S8
S2
S2
S2
S2
S2
S2
S8
S8
S3
S3
S3
S3
S3
S3
S3
S3
Table B.11: Bivariate copula families and RVMs of the R- and C-vine copulas without
index data with and without restriction of choices for bivariate linking copula families.
Theoretical Kendall’s tau values are given in parentheses behind the family name, and
values above 0.2 emphasized in bold face. For the Student’s t copula, the degrees of
freedom parameter ν is stated as subscript tν. The following abbreviations are used for
stock and indices names in the RVM: S1: ACA, S2: BBVA, S3: BNP, S4: CBK, S5:
DBK, S6: GLE, S7: ISP and S8: SAN.
226

B.3
Vine copula models
Families
rv2, DLM
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
F(.13)
t10.5(.24)
t11.9(.21)
t8.5(.24)
t11(.14)
t9.5(.18)
F(.54)
t4.7(.59)
t4.5(.63)
t6.8(.58)
t6.4(.49)
t4.2(.7)
t6.9(.56)
RVM
S4
S7
S1
S2
S7
S6
S8
S2
S7
S5
S1
S8
S2
S7
S7
S6
S5
S8
S2
S3
S2
S3
S3
S5
S8
S8
S3
S8
S5
S6
S3
S3
S2
S8
S3
S3
Families
rv2R, DLM
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
F(.13)
t10.5(.24)
t11.9(.21)
t8.5(.24)
t11(.14)
t9.5(.18)
F(.54)
t4.7(.59)
t4.5(.63)
t6.8(.58)
t6.4(.49)
t4.2(.7)
t6.9(.56)
RVM
S4
S7
S1
S2
S7
S6
S8
S2
S7
S5
S1
S8
S2
S7
S7
S6
S5
S8
S2
S3
S2
S3
S3
S5
S8
S8
S3
S8
S5
S6
S3
S3
S2
S8
S3
S3
Families
cv2, DLM
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
t21.7(.18)
SBB8(.21)
t8.6(.25)
F(.22)
t9.7(.24)
t4.7(.52)
F(.48)
t4.5(.63)
t7.3(.48)
t6.5(.58)
t6.8(.58)
t6.8(.56)
t6.9(.56)
RVM
S4
S8
S6
S6
S8
S7
S7
S7
S8
S1
S1
S1
S1
S8
S5
S5
S5
S5
S5
S8
S2
S2
S2
S2
S2
S2
S8
S8
S3
S3
S3
S3
S3
S3
S3
S3
Families
cv2R, DLM
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
t21.7(.18)
F(.21)
t8.6(.25)
F(.22)
t9.7(.24)
t4.7(.52)
F(.48)
t4.5(.63)
t7.3(.48)
t6.5(.58)
t6.8(.58)
t6.8(.56)
t6.9(.56)
RVM
S4
S8
S6
S6
S8
S7
S7
S7
S8
S1
S1
S1
S1
S8
S5
S5
S5
S5
S5
S8
S2
S2
S2
S2
S2
S2
S8
S8
S3
S3
S3
S3
S3
S3
S3
S3
Table B.12: Bivariate copula families and RVMs of the R- and C-vine copulas without
index data with and without restriction of choices for bivariate linking copula families.
Theoretical Kendall’s tau values are given in parentheses behind the family name, and
values above 0.2 emphasized in bold face. For the Student’s t copula, the degrees of
freedom parameter ν is stated as subscript tν. The following abbreviations are used for
stock and indices names in the RVM: S1: ACA, S2: BBVA, S3: BNP, S4: CBK, S5:
DBK, S6: GLE, S7: ISP and S8: SAN.
227

B
Empirical study: Figures and tables
B.4
Estimated parameters of the one- and two-factor copula
models
In this section, tables with parameter estimates with corresponding theoretical Kendall’s
tau values in parentheses for the one- and two-factor copula models are given. In the case
of the one-factor copula all bivariate linking copulas are chosen to be from the same copula
family. In the case of the two-factor copula model, two models with diﬀerent choices of
bivariate linking copulas for latent factor V1 and V2 are also considered. Underlying copula
data is either generated by ARMA-GARCH or the DLMs. For compactness, stocks are
abbreviated inside the tables in the following way: 1: ACA, 2: BBVA, 3: BNP, 4: CBK,
5: DBK, 6: GLE, 7: ISP and 8: SAN. Further details are given in Section 9.3.4.
228

B.4
Estimated parameters of the one- and two-factor copula models
ARMA-GARCH one-factor copula model
C1,V1
C2,V1
C3,V1
C4,V1
C5,V1
C6,V1
C7,V1
C8,V1
Frank
9.29 (0.65)
10.35 (0.67)
11.9 (0.71)
6.43 (0.54)
10.22 (0.67)
10.77 (0.69)
7.01 (0.56)
10.29 (0.67)
Gumbel
2.55 (0.61)
2.83 (0.65)
3.04 (0.67)
1.93 (0.48)
2.76 (0.64)
2.85 (0.65)
2.09 (0.52)
2.83 (0.65)
t(5)
0.84 (0.63)
0.87 (0.67)
0.89 (0.7)
0.72 (0.51)
0.86 (0.66)
0.88 (0.68)
0.76 (0.55)
0.87 (0.67)
BB1
0.56, 2.07 (0.62)
0.64, 2.22 (0.66)
0.6, 2.42 (0.68)
0.55, 1.55 (0.5)
0.54, 2.23 (0.65)
0.66, 2.23 (0.66)
0.61, 1.64 (0.53)
0.68, 2.18 (0.66)
ARMA-GARCH two-factor copula model
C1,v1
C2,V1
C3,V1
C4,V1
C5,V1
C6,V1
C7,V1
C8,V1
Frank
4.13 (0.4)
11.79 (0.71)
4.64 (0.43)
3.6 (0.36)
4.9 (0.45)
4.07 (0.39)
4.45 (0.42)
10.45 (0.68)
Gumbel
1.33 (0.25)
1.06 (0.05)
1.31 (0.24)
1.24 (0.19)
1.24 (0.2)
1.39 (0.28)
1.15 (0.13)
1.04 (0.04)
Gumbel
1.41 (0.29)
2.45 (0.59)
1.45 (0.31)
1.34 (0.25)
1.5 (0.34)
1.39 (0.28)
1.49 (0.33)
2.37 (0.58)
BB1
0.34, 1.31 (0.35)
0.75, 2.04 (0.64)
0.39, 1.34 (0.37)
0.36, 1.22 (0.3)
0.37, 1.39 (0.39)
0.37, 1.28 (0.34)
0.45, 1.31 (0.38)
1.09, 1.83 (0.65)
t(5)
0.68 (0.47)
0.92 (0.75)
0.73 (0.52)
0.6 (0.41)
0.73 (0.52)
0.69 (0.48)
0.67 (0.47)
0.93 (0.76)
C1,V2;V1
C2,V2;V1
C3,V2;V1
C4,V2;V1
C5,V2;V1
C6,V2;V1
C7,V2;V1
C8,V2;V1
Frank
7.41 (0.58)
5.71 (0.5)
9.52 (0.65)
4.53 (0.43)
6.95 (0.56)
10.1 (0.67)
4.38 (0.42)
5.49 (0.49)
Gumbel
2.42 (0.59)
3.61 (0.72)
2.9 (0.66)
1.8 (0.44)
2.57 (0.61)
2.8 (0.64)
2.02 (0.51)
3.77 (0.73)
Frank
8.47 (0.62)
8.52 (0.62)
11.13 (0.69)
5.32 (0.48)
8.24 (0.61)
11.06 (0.69)
5.24 (0.47)
8.06 (0.6)
Frank
7.96 (0.6)
6.71 (0.55)
10.38 (0.68)
4.87 (0.45)
7.59 (0.59)
10.63 (0.68)
4.65 (0.43)
6.65 (0.55)
t(5)
0.71 (0.5)
0.39 (0.25)
0.78 (0.57)
0.5 (0.33)
0.66 (0.46)
0.81 (0.6)
0.46 (0.31)
0.38 (0.24)
Table B.13: Estimated parameters of the one- (top) and two-factor (bottom) copula models of copula data from ARMA-GARCH
ﬁltered daily log returns of eight bank stocks. Corresponding theoretical Kendall’s taus are given in parenthesis.
229

B
Empirical study: Figures and tables
DLM one-factor copula model
C1,v1
C2,V1
C3,V1
C4,V1
C5,V1
C6,V1
C7,V1
C8,V1
Frank
10.51 (0.68)
11.59 (0.7)
13.11 (0.73)
7.36 (0.58)
11.33 (0.7)
11.96 (0.71)
7.92 (0.6)
11.52 (0.7)
Gumbel
2.66 (0.62)
2.95 (0.66)
3.21 (0.69)
1.99 (0.5)
2.89 (0.65)
2.97 (0.66)
2.16 (0.54)
2.93 (0.66)
t(5)
0.86 (0.66)
0.88 (0.69)
0.9 (0.72)
0.75 (0.54)
0.88 (0.68)
0.89 (0.7)
0.78 (0.57)
0.88 (0.68)
BB1
0.46, 2.24 (0.64)
0.55, 2.38 (0.67)
0.52, 2.63 (0.7)
0.38, 1.71 (0.51)
0.44, 2.42 (0.66)
0.56, 2.41 (0.67)
0.58, 1.72 (0.55)
0.45, 2.44 (0.67)
DLM two-factor copula model
C1,V1
C2,V1
C3,V1
C4,V1
C5,V1
C6,V1
C7,V1
C8,V1
Frank
4.88 (0.45)
12.85 (0.73)
5.56 (0.49)
4.31 (0.41)
5.75 (0.5)
4.89 (0.45)
5.31 (0.48)
11.84 (0.71)
Gumbel
1.34 (0.25)
1.08 (0.07)
1.3 (0.23)
1.24 (0.19)
1.25 (0.2)
1.35 (0.26)
1.15 (0.13)
1.07 (0.06)
Gumbel
1.25 (0.2)
1.91 (0.48)
1.28 (0.22)
1.23 (0.19)
1.33 (0.25)
1.23 (0.19)
1.36 (0.26)
1.87 (0.47)
BB1
0.19, 1.16 (0.21)
0.56, 1.52 (0.48)
0.23, 1.17 (0.23)
0.16, 1.16 (0.2)
0.22, 1.22 (0.26)
0.19, 1.14 (0.2)
0.31, 1.2 (0.27)
0.61, 1.45 (0.47)
t(5)
0.7 (0.5)
0.93 (0.76)
0.76 (0.55)
0.63 (0.43)
0.75 (0.54)
0.72 (0.51)
0.71 (0.5)
0.93 (0.76)
C1,V2;V1
C2,V2;V1
C3,V2;V1
C4,V2;V1
C5,V2;V1
C6,V2;V1
C7,V2;V1
C8,V2;V1
Frank
7.81 (0.6)
5.58 (0.49)
9.83 (0.66)
4.76 (0.44)
7.1 (0.57)
10.39 (0.68)
4.4 (0.42)
5.3 (0.48)
Gumbel
2.57 (0.61)
3.69 (0.73)
3.07 (0.67)
1.88 (0.47)
2.74 (0.63)
2.91 (0.66)
2.15 (0.53)
3.83 (0.74)
Frank
10.43 (0.68)
12.74 (0.73)
13.27 (0.74)
6.64 (0.55)
10.04 (0.67)
12.95 (0.73)
6.64 (0.55)
11.68 (0.71)
Frank
10.41 (0.68)
13.03 (0.73)
13.39 (0.74)
6.59 (0.54)
10.01 (0.67)
13.3 (0.74)
6.6 (0.54)
11.99 (0.71)
t(5)
0.73 (0.52)
0.43 (0.28)
0.78 (0.57)
0.51 (0.34)
0.67 (0.47)
0.81 (0.6)
0.47 (0.31)
0.4 (0.26)
Table B.14: Estimated parameters of the one- (top) and two-factor (bottom) copula models of copula data from DLM ﬁltered
daily log returns of eight bank stocks. Corresponding theoretical Kendall’s taus are given in parenthesis.
230

B.5
Bayesian inference for the one-factor copula model
B.5
Bayesian inference for the one-factor copula model
In this section, additional tables of the Gibbs sampler with individual ARMS for the
empirical copula data set generated by marginal DLMs are given. The Gibbs sampler is
run on both the combined data set of years 2005 to 2013 and of each individual year. For
each case, 2 000 samples are generated, and the ﬁrst 1 000 samples discarded as burn-in,
for a ﬁnal sample size of 1 000. More details are given in Section 9.4.
B.5.1
Trace and density plots of years 2005 to 2013
This subsection contains supplementary trace and density plots for the combined copula
data set of years 2005 to 2013.
231

B
Empirical study: Figures and tables
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ1
Traceplot of τ1 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ1
Density
Histogram of τ1 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ2
Traceplot of τ2 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ2
Density
Histogram of τ2 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ3
Traceplot of τ3 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ3
Density
Histogram of τ3 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ4
Traceplot of τ4 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ4
Density
Histogram of τ4 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ5
Traceplot of τ5 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ5
Density
Histogram of τ5 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ6
Traceplot of τ6 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ6
Density
Histogram of τ6 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ7
Traceplot of τ7 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ7
Density
Histogram of τ7 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ8
Traceplot of τ8 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ8
Density
Histogram of τ8 
Posterior mode
Figure B.4: Trace (left) and density plots (right) of a single run of the Gibbs sampler
with individual ARMS for transforms τ1, . . . , τ8 of the parameters of Gumbel linking
copula. The Gibbs sampler was run on 2 345 DLM ﬁltered daily log returns of years 2005
to 2013 for 2 000 iterations, and the ﬁrst 1 000 iterations discarded as burn-in.
232

B.5
Bayesian inference for the one-factor copula model
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ1
Traceplot of τ1 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ1
Density
Histogram of τ1 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ2
Traceplot of τ2 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ2
Density
Histogram of τ2 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ3
Traceplot of τ3 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ3
Density
Histogram of τ3 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ4
Traceplot of τ4 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ4
Density
Histogram of τ4 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ5
Traceplot of τ5 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ5
Density
Histogram of τ5 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ6
Traceplot of τ6 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ6
Density
Histogram of τ6 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ7
Traceplot of τ7 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ7
Density
Histogram of τ7 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ8
Traceplot of τ8 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ8
Density
Histogram of τ8 
Posterior mode
Figure B.5: Trace (left) and density plots (right) of a single run of the Gibbs sampler
with individual ARMS for transforms τ1, . . . , τ8 of the parameters of Gaussian linking
copulas. The Gibbs sampler was run on 2 345 DLM ﬁltered daily log returns of years 2005
to 2013 for 2 000 iterations, and the ﬁrst 1 000 iterations discarded as burn-in.
233

B
Empirical study: Figures and tables
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ1
Traceplot of τ1 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ1
Density
Histogram of τ1 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ2
Traceplot of τ2 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ2
Density
Histogram of τ2 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ3
Traceplot of τ3 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ3
Density
Histogram of τ3 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ4
Traceplot of τ4 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ4
Density
Histogram of τ4 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ5
Traceplot of τ5 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ5
Density
Histogram of τ5 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ6
Traceplot of τ6 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ6
Density
Histogram of τ6 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ7
Traceplot of τ7 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ7
Density
Histogram of τ7 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ8
Traceplot of τ8 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ8
Density
Histogram of τ8 
Posterior mode
Figure B.6: Trace (left) and density plots (right) of a single run of the Gibbs sampler
with individual ARMS for transforms τ1, . . . , τ8 of the parameters of survival Gumbel
linking copulas. The Gibbs sampler was run on 2 345 DLM ﬁltered daily log returns of
years 2005 to 2013 for 2 000 iterations, and the ﬁrst 1 000 iterations discarded as burn-in.
234

B.5
Bayesian inference for the one-factor copula model
B.5.2
Trace and density plots of individual years
In this section, supplementary trace and density plots and tables of the posterior mode
estimates in Kendall’s tau scale for the copula data sets of individual years are shown.
235

B
Empirical study: Figures and tables
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ1
Traceplot of τ1 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ1
Density
Histogram of τ1 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ2
Traceplot of τ2 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ2
Density
Histogram of τ2 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ3
Traceplot of τ3 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ3
Density
Histogram of τ3 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ4
Traceplot of τ4 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ4
Density
Histogram of τ4 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ5
Traceplot of τ5 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ5
Density
Histogram of τ5 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ6
Traceplot of τ6 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ6
Density
Histogram of τ6 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ7
Traceplot of τ7 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ7
Density
Histogram of τ7 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ8
Traceplot of τ8 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ8
Density
Histogram of τ8 
Posterior mode
Figure B.7: Trace (left) and density plots (right) of a single run of the Gibbs sampler
with individual ARMS for transforms τ1, . . . , τ8 of the parameters of Gumbel linking
copulas. The Gibbs sampler was run on 260 DLM ﬁltered daily log returns of year 2005
for 2 000 iterations, and the ﬁrst 1 000 iterations discarded as burn-in.
236

B.5
Bayesian inference for the one-factor copula model
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ1
Traceplot of τ1 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ1
Density
Histogram of τ1 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ2
Traceplot of τ2 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ2
Density
Histogram of τ2 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ3
Traceplot of τ3 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ3
Density
Histogram of τ3 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ4
Traceplot of τ4 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ4
Density
Histogram of τ4 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ5
Traceplot of τ5 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ5
Density
Histogram of τ5 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ6
Traceplot of τ6 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ6
Density
Histogram of τ6 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ7
Traceplot of τ7 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ7
Density
Histogram of τ7 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ8
Traceplot of τ8 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ8
Density
Histogram of τ8 
Posterior mode
Figure B.8: Trace (left) and density plots (right) of a single run of the Gibbs sampler
with individual ARMS for transforms τ1, . . . , τ8 of the parameters of Gaussian linking
copulas. The Gibbs sampler was run on 260 DLM ﬁltered daily log returns of year 2005
for 2 000 iterations, and the ﬁrst 1 000 iterations discarded as burn-in.
237

B
Empirical study: Figures and tables
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ1
Traceplot of τ1 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ1
Density
Histogram of τ1 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ2
Traceplot of τ2 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ2
Density
Histogram of τ2 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ3
Traceplot of τ3 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ3
Density
Histogram of τ3 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ4
Traceplot of τ4 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ4
Density
Histogram of τ4 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ5
Traceplot of τ5 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ5
Density
Histogram of τ5 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ6
Traceplot of τ6 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ6
Density
Histogram of τ6 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ7
Traceplot of τ7 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ7
Density
Histogram of τ7 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ8
Traceplot of τ8 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ8
Density
Histogram of τ8 
Posterior mode
Figure B.9: Trace (left) and density plots (right) of a single run of the Gibbs sampler
with individual ARMS for transforms τ1, . . . , τ8 of the parameters of survival Gumbel
linking copulas. The Gibbs sampler was run on 260 DLM ﬁltered daily log returns of year
2005 for 2 000 iterations, and the ﬁrst 1 000 iterations discarded as burn-in.
238

B.5
Bayesian inference for the one-factor copula model
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ1
Traceplot of τ1 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ1
Density
Histogram of τ1 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ2
Traceplot of τ2 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ2
Density
Histogram of τ2 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ3
Traceplot of τ3 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ3
Density
Histogram of τ3 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ4
Traceplot of τ4 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ4
Density
Histogram of τ4 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ5
Traceplot of τ5 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ5
Density
Histogram of τ5 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ6
Traceplot of τ6 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ6
Density
Histogram of τ6 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ7
Traceplot of τ7 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ7
Density
Histogram of τ7 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ8
Traceplot of τ8 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ8
Density
Histogram of τ8 
Posterior mode
Figure B.10: Trace (left) and density plots (right) of a single run of the Gibbs sampler
with individual ARMS for transforms τ1, . . . , τ8 of the parameters of Gumbel linking
copulas. The Gibbs sampler was run on 260 DLM ﬁltered daily log returns of year 2006
for 2 000 iterations, and the ﬁrst 1 000 iterations discarded as burn-in.
239

B
Empirical study: Figures and tables
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ1
Traceplot of τ1 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ1
Density
Histogram of τ1 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ2
Traceplot of τ2 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ2
Density
Histogram of τ2 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ3
Traceplot of τ3 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ3
Density
Histogram of τ3 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ4
Traceplot of τ4 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ4
Density
Histogram of τ4 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ5
Traceplot of τ5 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ5
Density
Histogram of τ5 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ6
Traceplot of τ6 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ6
Density
Histogram of τ6 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ7
Traceplot of τ7 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ7
Density
Histogram of τ7 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ8
Traceplot of τ8 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ8
Density
Histogram of τ8 
Posterior mode
Figure B.11: Trace (left) and density plots (right) of a single run of the Gibbs sampler
with individual ARMS for transforms τ1, . . . , τ8 of the parameters of Gaussian linking
copulas. The Gibbs sampler was run on 260 DLM ﬁltered daily log returns of year 2006
for 2 000 iterations, and the ﬁrst 1 000 iterations discarded as burn-in.
240

B.5
Bayesian inference for the one-factor copula model
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ1
Traceplot of τ1 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ1
Density
Histogram of τ1 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ2
Traceplot of τ2 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ2
Density
Histogram of τ2 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ3
Traceplot of τ3 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ3
Density
Histogram of τ3 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ4
Traceplot of τ4 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ4
Density
Histogram of τ4 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ5
Traceplot of τ5 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ5
Density
Histogram of τ5 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ6
Traceplot of τ6 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ6
Density
Histogram of τ6 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ7
Traceplot of τ7 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ7
Density
Histogram of τ7 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ8
Traceplot of τ8 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ8
Density
Histogram of τ8 
Posterior mode
Figure B.12: Trace (left) and density plots (right) of a single run of the Gibbs sampler
with individual ARMS for transforms τ1, . . . , τ8 of the parameters of survival Gumbel
linking copulas. The Gibbs sampler was run on 260 DLM ﬁltered daily log returns of year
2006 for 2 000 iterations, and the ﬁrst 1 000 iterations discarded as burn-in.
241

B
Empirical study: Figures and tables
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ1
Traceplot of τ1 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ1
Density
Histogram of τ1 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ2
Traceplot of τ2 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ2
Density
Histogram of τ2 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ3
Traceplot of τ3 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ3
Density
Histogram of τ3 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ4
Traceplot of τ4 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ4
Density
Histogram of τ4 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ5
Traceplot of τ5 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ5
Density
Histogram of τ5 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ6
Traceplot of τ6 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ6
Density
Histogram of τ6 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ7
Traceplot of τ7 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ7
Density
Histogram of τ7 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ8
Traceplot of τ8 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ8
Density
Histogram of τ8 
Posterior mode
Figure B.13: Trace (left) and density plots (right) of a single run of the Gibbs sampler
with individual ARMS for transforms τ1, . . . , τ8 of the parameters of Gumbel linking
copulas. The Gibbs sampler was run on 260 DLM ﬁltered daily log returns of year 2007
for 2 000 iterations, and the ﬁrst 1 000 iterations discarded as burn-in.
242

B.5
Bayesian inference for the one-factor copula model
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ1
Traceplot of τ1 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ1
Density
Histogram of τ1 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ2
Traceplot of τ2 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ2
Density
Histogram of τ2 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ3
Traceplot of τ3 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ3
Density
Histogram of τ3 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ4
Traceplot of τ4 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ4
Density
Histogram of τ4 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ5
Traceplot of τ5 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ5
Density
Histogram of τ5 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ6
Traceplot of τ6 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ6
Density
Histogram of τ6 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ7
Traceplot of τ7 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ7
Density
Histogram of τ7 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ8
Traceplot of τ8 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ8
Density
Histogram of τ8 
Posterior mode
Figure B.14: Trace (left) and density plots (right) of a single run of the Gibbs sampler
with individual ARMS for transforms τ1, . . . , τ8 of the parameters of Gaussian linking
copulas. The Gibbs sampler was run on 260 DLM ﬁltered daily log returns of year 2007
for 2 000 iterations, and the ﬁrst 1 000 iterations discarded as burn-in.
243

B
Empirical study: Figures and tables
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ1
Traceplot of τ1 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ1
Density
Histogram of τ1 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ2
Traceplot of τ2 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ2
Density
Histogram of τ2 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ3
Traceplot of τ3 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ3
Density
Histogram of τ3 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ4
Traceplot of τ4 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ4
Density
Histogram of τ4 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ5
Traceplot of τ5 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ5
Density
Histogram of τ5 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ6
Traceplot of τ6 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ6
Density
Histogram of τ6 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ7
Traceplot of τ7 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ7
Density
Histogram of τ7 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ8
Traceplot of τ8 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ8
Density
Histogram of τ8 
Posterior mode
Figure B.15: Trace (left) and density plots (right) of a single run of the Gibbs sampler
with individual ARMS for transforms τ1, . . . , τ8 of the parameters of survival Gumbel
linking copulas. The Gibbs sampler was run on 260 DLM ﬁltered daily log returns of year
2007 for 2 000 iterations, and the ﬁrst 1 000 iterations discarded as burn-in.
244

B.5
Bayesian inference for the one-factor copula model
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ1
Traceplot of τ1 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ1
Density
Histogram of τ1 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ2
Traceplot of τ2 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ2
Density
Histogram of τ2 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ3
Traceplot of τ3 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ3
Density
Histogram of τ3 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ4
Traceplot of τ4 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ4
Density
Histogram of τ4 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ5
Traceplot of τ5 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ5
Density
Histogram of τ5 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ6
Traceplot of τ6 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ6
Density
Histogram of τ6 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ7
Traceplot of τ7 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ7
Density
Histogram of τ7 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ8
Traceplot of τ8 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ8
Density
Histogram of τ8 
Posterior mode
Figure B.16: Trace (left) and density plots (right) of a single run of the Gibbs sampler
with individual ARMS for transforms τ1, . . . , τ8 of the parameters of Gumbel linking
copulas. The Gibbs sampler was run on 260 DLM ﬁltered daily log returns of year 2008
for 2 000 iterations, and the ﬁrst 1 000 iterations discarded as burn-in.
245

B
Empirical study: Figures and tables
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ1
Traceplot of τ1 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ1
Density
Histogram of τ1 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ2
Traceplot of τ2 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ2
Density
Histogram of τ2 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ3
Traceplot of τ3 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ3
Density
Histogram of τ3 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ4
Traceplot of τ4 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ4
Density
Histogram of τ4 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ5
Traceplot of τ5 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ5
Density
Histogram of τ5 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ6
Traceplot of τ6 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ6
Density
Histogram of τ6 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ7
Traceplot of τ7 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ7
Density
Histogram of τ7 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ8
Traceplot of τ8 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ8
Density
Histogram of τ8 
Posterior mode
Figure B.17: Trace (left) and density plots (right) of a single run of the Gibbs sampler
with individual ARMS for transforms τ1, . . . , τ8 of the parameters of Gaussian linking
copulas. The Gibbs sampler was run on 260 DLM ﬁltered daily log returns of year 2008
for 2 000 iterations, and the ﬁrst 1 000 iterations discarded as burn-in.
246

B.5
Bayesian inference for the one-factor copula model
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ1
Traceplot of τ1 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ1
Density
Histogram of τ1 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ2
Traceplot of τ2 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ2
Density
Histogram of τ2 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ3
Traceplot of τ3 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ3
Density
Histogram of τ3 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ4
Traceplot of τ4 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ4
Density
Histogram of τ4 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ5
Traceplot of τ5 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ5
Density
Histogram of τ5 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ6
Traceplot of τ6 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ6
Density
Histogram of τ6 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ7
Traceplot of τ7 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ7
Density
Histogram of τ7 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ8
Traceplot of τ8 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ8
Density
Histogram of τ8 
Posterior mode
Figure B.18: Trace (left) and density plots (right) of a single run of the Gibbs sampler
with individual ARMS for transforms τ1, . . . , τ8 of the parameters of survival Gumbel
linking copulas. The Gibbs sampler was run on 260 DLM ﬁltered daily log returns of year
2008 for 2 000 iterations, and the ﬁrst 1 000 iterations discarded as burn-in.
247

B
Empirical study: Figures and tables
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ1
Traceplot of τ1 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ1
Density
Histogram of τ1 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ2
Traceplot of τ2 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ2
Density
Histogram of τ2 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ3
Traceplot of τ3 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ3
Density
Histogram of τ3 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ4
Traceplot of τ4 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ4
Density
Histogram of τ4 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ5
Traceplot of τ5 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ5
Density
Histogram of τ5 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ6
Traceplot of τ6 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ6
Density
Histogram of τ6 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ7
Traceplot of τ7 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ7
Density
Histogram of τ7 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ8
Traceplot of τ8 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ8
Density
Histogram of τ8 
Posterior mode
Figure B.19: Trace (left) and density plots (right) of a single run of the Gibbs sampler
with individual ARMS for transforms τ1, . . . , τ8 of the parameters of Gumbel linking
copulas. The Gibbs sampler was run on 260 DLM ﬁltered daily log returns of year 2009
for 2 000 iterations, and the ﬁrst 1 000 iterations discarded as burn-in.
248

B.5
Bayesian inference for the one-factor copula model
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ1
Traceplot of τ1 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ1
Density
Histogram of τ1 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ2
Traceplot of τ2 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ2
Density
Histogram of τ2 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ3
Traceplot of τ3 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ3
Density
Histogram of τ3 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ4
Traceplot of τ4 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ4
Density
Histogram of τ4 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ5
Traceplot of τ5 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ5
Density
Histogram of τ5 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ6
Traceplot of τ6 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ6
Density
Histogram of τ6 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ7
Traceplot of τ7 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ7
Density
Histogram of τ7 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ8
Traceplot of τ8 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ8
Density
Histogram of τ8 
Posterior mode
Figure B.20: Trace (left) and density plots (right) of a single run of the Gibbs sampler
with individual ARMS for transforms τ1, . . . , τ8 of the parameters of Gaussian linking
copulas. The Gibbs sampler was run on 260 DLM ﬁltered daily log returns of year 2009
for 2 000 iterations, and the ﬁrst 1 000 iterations discarded as burn-in.
249

B
Empirical study: Figures and tables
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ1
Traceplot of τ1 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ1
Density
Histogram of τ1 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ2
Traceplot of τ2 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ2
Density
Histogram of τ2 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ3
Traceplot of τ3 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ3
Density
Histogram of τ3 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ4
Traceplot of τ4 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ4
Density
Histogram of τ4 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ5
Traceplot of τ5 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ5
Density
Histogram of τ5 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ6
Traceplot of τ6 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ6
Density
Histogram of τ6 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ7
Traceplot of τ7 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ7
Density
Histogram of τ7 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ8
Traceplot of τ8 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ8
Density
Histogram of τ8 
Posterior mode
Figure B.21: Trace (left) and density plots (right) of a single run of the Gibbs sampler
with individual ARMS for transforms τ1, . . . , τ8 of the parameters of survival Gumbel
linking copulas. The Gibbs sampler was run on 260 DLM ﬁltered daily log returns of year
2009 for 2 000 iterations, and the ﬁrst 1 000 iterations discarded as burn-in.
250

B.5
Bayesian inference for the one-factor copula model
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ1
Traceplot of τ1 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ1
Density
Histogram of τ1 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ2
Traceplot of τ2 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ2
Density
Histogram of τ2 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ3
Traceplot of τ3 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ3
Density
Histogram of τ3 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ4
Traceplot of τ4 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ4
Density
Histogram of τ4 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ5
Traceplot of τ5 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ5
Density
Histogram of τ5 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ6
Traceplot of τ6 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ6
Density
Histogram of τ6 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ7
Traceplot of τ7 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ7
Density
Histogram of τ7 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ8
Traceplot of τ8 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ8
Density
Histogram of τ8 
Posterior mode
Figure B.22: Trace (left) and density plots (right) of a single run of the Gibbs sampler
with individual ARMS for transforms τ1, . . . , τ8 of the parameters of Gumbel linking
copulas. The Gibbs sampler was run on 260 DLM ﬁltered daily log returns of year 2010
for 2 000 iterations, and the ﬁrst 1 000 iterations discarded as burn-in.
251

B
Empirical study: Figures and tables
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ1
Traceplot of τ1 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ1
Density
Histogram of τ1 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ2
Traceplot of τ2 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ2
Density
Histogram of τ2 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ3
Traceplot of τ3 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ3
Density
Histogram of τ3 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ4
Traceplot of τ4 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ4
Density
Histogram of τ4 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ5
Traceplot of τ5 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ5
Density
Histogram of τ5 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ6
Traceplot of τ6 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ6
Density
Histogram of τ6 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ7
Traceplot of τ7 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ7
Density
Histogram of τ7 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ8
Traceplot of τ8 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ8
Density
Histogram of τ8 
Posterior mode
Figure B.23: Trace (left) and density plots (right) of a single run of the Gibbs sampler
with individual ARMS for transforms τ1, . . . , τ8 of the parameters of Gaussian linking
copulas. The Gibbs sampler was run on 260 DLM ﬁltered daily log returns of year 2010
for 2 000 iterations, and the ﬁrst 1 000 iterations discarded as burn-in.
252

B.5
Bayesian inference for the one-factor copula model
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ1
Traceplot of τ1 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ1
Density
Histogram of τ1 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ2
Traceplot of τ2 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ2
Density
Histogram of τ2 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ3
Traceplot of τ3 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ3
Density
Histogram of τ3 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ4
Traceplot of τ4 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ4
Density
Histogram of τ4 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ5
Traceplot of τ5 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ5
Density
Histogram of τ5 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ6
Traceplot of τ6 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ6
Density
Histogram of τ6 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ7
Traceplot of τ7 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ7
Density
Histogram of τ7 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ8
Traceplot of τ8 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ8
Density
Histogram of τ8 
Posterior mode
Figure B.24: Trace (left) and density plots (right) of a single run of the Gibbs sampler
with individual ARMS for transforms τ1, . . . , τ8 of the parameters of survival Gumbel
linking copulas. The Gibbs sampler was run on 260 DLM ﬁltered daily log returns of year
2010 for 2 000 iterations, and the ﬁrst 1 000 iterations discarded as burn-in.
253

B
Empirical study: Figures and tables
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ1
Traceplot of τ1 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ1
Density
Histogram of τ1 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ2
Traceplot of τ2 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ2
Density
Histogram of τ2 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ3
Traceplot of τ3 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ3
Density
Histogram of τ3 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ4
Traceplot of τ4 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ4
Density
Histogram of τ4 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ5
Traceplot of τ5 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ5
Density
Histogram of τ5 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ6
Traceplot of τ6 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ6
Density
Histogram of τ6 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ7
Traceplot of τ7 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ7
Density
Histogram of τ7 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ8
Traceplot of τ8 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ8
Density
Histogram of τ8 
Posterior mode
Figure B.25: Trace (left) and density plots (right) of a single run of the Gibbs sampler
with individual ARMS for transforms τ1, . . . , τ8 of the parameters of Gumbel linking
copulas. The Gibbs sampler was run on 260 DLM ﬁltered daily log returns of year 2011
for 2 000 iterations, and the ﬁrst 1 000 iterations discarded as burn-in.
254

B.5
Bayesian inference for the one-factor copula model
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ1
Traceplot of τ1 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ1
Density
Histogram of τ1 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ2
Traceplot of τ2 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ2
Density
Histogram of τ2 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ3
Traceplot of τ3 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ3
Density
Histogram of τ3 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ4
Traceplot of τ4 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ4
Density
Histogram of τ4 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ5
Traceplot of τ5 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ5
Density
Histogram of τ5 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ6
Traceplot of τ6 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ6
Density
Histogram of τ6 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ7
Traceplot of τ7 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ7
Density
Histogram of τ7 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ8
Traceplot of τ8 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ8
Density
Histogram of τ8 
Posterior mode
Figure B.26: Trace (left) and density plots (right) of a single run of the Gibbs sampler
with individual ARMS for transforms τ1, . . . , τ8 of the parameters of Gaussian linking
copulas. The Gibbs sampler was run on 260 DLM ﬁltered daily log returns of year 2011
for 2 000 iterations, and the ﬁrst 1 000 iterations discarded as burn-in.
255

B
Empirical study: Figures and tables
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ1
Traceplot of τ1 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ1
Density
Histogram of τ1 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ2
Traceplot of τ2 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ2
Density
Histogram of τ2 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ3
Traceplot of τ3 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ3
Density
Histogram of τ3 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ4
Traceplot of τ4 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ4
Density
Histogram of τ4 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ5
Traceplot of τ5 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ5
Density
Histogram of τ5 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ6
Traceplot of τ6 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ6
Density
Histogram of τ6 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ7
Traceplot of τ7 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ7
Density
Histogram of τ7 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ8
Traceplot of τ8 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ8
Density
Histogram of τ8 
Posterior mode
Figure B.27: Trace (left) and density plots (right) of a single run of the Gibbs sampler
with individual ARMS for transforms τ1, . . . , τ8 of the parameters of survival Gumbel
linking copulas. The Gibbs sampler was run on 260 DLM ﬁltered daily log returns of year
2011 for 2 000 iterations, and the ﬁrst 1 000 iterations discarded as burn-in.
256

B.5
Bayesian inference for the one-factor copula model
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ1
Traceplot of τ1 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ1
Density
Histogram of τ1 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ2
Traceplot of τ2 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ2
Density
Histogram of τ2 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ3
Traceplot of τ3 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ3
Density
Histogram of τ3 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ4
Traceplot of τ4 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ4
Density
Histogram of τ4 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ5
Traceplot of τ5 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ5
Density
Histogram of τ5 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ6
Traceplot of τ6 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ6
Density
Histogram of τ6 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ7
Traceplot of τ7 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ7
Density
Histogram of τ7 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ8
Traceplot of τ8 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ8
Density
Histogram of τ8 
Posterior mode
Figure B.28: Trace (left) and density plots (right) of a single run of the Gibbs sampler
with individual ARMS for transforms τ1, . . . , τ8 of the parameters of Gumbel linking
copulas. The Gibbs sampler was run on 260 DLM ﬁltered daily log returns of year 2012
for 2 000 iterations, and the ﬁrst 1 000 iterations discarded as burn-in.
257

B
Empirical study: Figures and tables
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ1
Traceplot of τ1 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ1
Density
Histogram of τ1 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ2
Traceplot of τ2 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ2
Density
Histogram of τ2 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ3
Traceplot of τ3 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ3
Density
Histogram of τ3 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ4
Traceplot of τ4 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ4
Density
Histogram of τ4 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ5
Traceplot of τ5 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ5
Density
Histogram of τ5 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ6
Traceplot of τ6 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ6
Density
Histogram of τ6 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ7
Traceplot of τ7 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ7
Density
Histogram of τ7 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ8
Traceplot of τ8 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ8
Density
Histogram of τ8 
Posterior mode
Figure B.29: Trace (left) and density plots (right) of a single run of the Gibbs sampler
with individual ARMS for transforms τ1, . . . , τ8 of the parameters of Gaussian linking
copulas. The Gibbs sampler was run on 260 DLM ﬁltered daily log returns of year 2012
for 2 000 iterations, and the ﬁrst 1 000 iterations discarded as burn-in.
258

B.5
Bayesian inference for the one-factor copula model
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ1
Traceplot of τ1 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ1
Density
Histogram of τ1 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ2
Traceplot of τ2 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ2
Density
Histogram of τ2 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ3
Traceplot of τ3 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ3
Density
Histogram of τ3 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ4
Traceplot of τ4 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ4
Density
Histogram of τ4 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ5
Traceplot of τ5 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ5
Density
Histogram of τ5 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ6
Traceplot of τ6 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ6
Density
Histogram of τ6 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ7
Traceplot of τ7 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ7
Density
Histogram of τ7 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ8
Traceplot of τ8 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ8
Density
Histogram of τ8 
Posterior mode
Figure B.30: Trace (left) and density plots (right) of a single run of the Gibbs sampler
with individual ARMS for transforms τ1, . . . , τ8 of the parameters of survival Gumbel
linking copulas. The Gibbs sampler was run on 260 DLM ﬁltered daily log returns of year
2012 for 2 000 iterations, and the ﬁrst 1 000 iterations discarded as burn-in.
259

B
Empirical study: Figures and tables
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ1
Traceplot of τ1 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ1
Density
Histogram of τ1 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ2
Traceplot of τ2 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ2
Density
Histogram of τ2 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ3
Traceplot of τ3 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ3
Density
Histogram of τ3 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ4
Traceplot of τ4 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ4
Density
Histogram of τ4 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ5
Traceplot of τ5 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ5
Density
Histogram of τ5 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ6
Traceplot of τ6 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ6
Density
Histogram of τ6 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ7
Traceplot of τ7 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ7
Density
Histogram of τ7 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ8
Traceplot of τ8 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ8
Density
Histogram of τ8 
Posterior mode
Figure B.31: Trace (left) and density plots (right) of a single run of the Gibbs sampler
with individual ARMS for transforms τ1, . . . , τ8 of the parameters of Gumbel linking
copulas. The Gibbs sampler was run on 260 DLM ﬁltered daily log returns of year 2013
for 2 000 iterations, and the ﬁrst 1 000 iterations discarded as burn-in.
260

B.5
Bayesian inference for the one-factor copula model
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ1
Traceplot of τ1 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ1
Density
Histogram of τ1 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ2
Traceplot of τ2 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ2
Density
Histogram of τ2 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ3
Traceplot of τ3 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ3
Density
Histogram of τ3 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ4
Traceplot of τ4 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ4
Density
Histogram of τ4 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ5
Traceplot of τ5 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ5
Density
Histogram of τ5 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ6
Traceplot of τ6 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ6
Density
Histogram of τ6 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ7
Traceplot of τ7 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ7
Density
Histogram of τ7 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ8
Traceplot of τ8 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ8
Density
Histogram of τ8 
Posterior mode
Figure B.32: Trace (left) and density plots (right) of a single run of the Gibbs sampler
with individual ARMS for transforms τ1, . . . , τ8 of the parameters of Gaussian linking
copulas. The Gibbs sampler was run on 260 DLM ﬁltered daily log returns of year 2013
for 2 000 iterations, and the ﬁrst 1 000 iterations discarded as burn-in.
261

B
Empirical study: Figures and tables
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ1
Traceplot of τ1 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ1
Density
Histogram of τ1 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ2
Traceplot of τ2 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ2
Density
Histogram of τ2 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ3
Traceplot of τ3 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ3
Density
Histogram of τ3 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ4
Traceplot of τ4 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ4
Density
Histogram of τ4 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ5
Traceplot of τ5 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ5
Density
Histogram of τ5 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ6
Traceplot of τ6 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ6
Density
Histogram of τ6 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ7
Traceplot of τ7 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ7
Density
Histogram of τ7 
Posterior mode
0
200
400
600
800
1000
0.4
0.6
0.8
1.0
Iteration
τ8
Traceplot of τ8 
Posterior mode
0.4
0.6
0.8
1.0
0
5
10
20
30
τ8
Density
Histogram of τ8 
Posterior mode
Figure B.33: Trace (left) and density plots (right) of a single run of the Gibbs sampler
with individual ARMS for transforms τ1, . . . , τ8 of the parameters of survival Gumbel
linking copulas. The Gibbs sampler was run on 260 DLM ﬁltered daily log returns of year
2013 for 2 000 iterations, and the ﬁrst 1 000 iterations discarded as burn-in.
262

B.5
Bayesian inference for the one-factor copula model
B.5.3
Posterior mode estimates
This subsection contains posterior mode estimates of the Gibbs sampler for the one-factor
copula model for individual years and years 2005 to 2013 simultaneously. The bivariate
linking copulas are either all Gumbel or survival Gumbel.
ACA
BBVA
BNP
CBK
DBK
GLE
ISP
SAN
2005
0.49
0.60
0.62
0.38
0.54
0.65
0.30
0.57
2006
0.57
0.59
0.65
0.51
0.69
0.62
0.41
0.64
2007
0.67
0.63
0.74
0.54
0.69
0.66
0.49
0.56
2008
0.59
0.75
0.61
0.57
0.66
0.57
0.57
0.72
2009
0.63
0.77
0.71
0.57
0.71
0.63
0.63
0.76
2010
0.69
0.70
0.76
0.58
0.68
0.70
0.67
0.71
2011
0.68
0.70
0.72
0.46
0.66
0.73
0.61
0.71
2012
0.72
0.66
0.78
0.60
0.68
0.77
0.71
0.67
2013
0.63
0.72
0.71
0.40
0.63
0.72
0.58
0.71
2005-2013
0.49
0.69
0.73
0.47
0.69
0.73
0.63
0.75
Table B.15:
Posterior mode estimates of bivariate copula parameters of eight bank
stocks with latent variable V1, based on Gibbs sampler output of size 1 000 in Kendall’s
tau scale. The Gibbs sampler with Gumbel linking copulas was run on DLM ﬁltered daily
observations of individual years 2005 to 2013 (top), and on all 2 345 daily observations at
once (bottom).
ACA
BBVA
BNP
CBK
DBK
GLE
ISP
SAN
2005
0.50
0.59
0.64
0.46
0.59
0.68
0.37
0.57
2006
0.55
0.62
0.65
0.55
0.70
0.66
0.45
0.66
2007
0.66
0.63
0.75
0.53
0.69
0.66
0.47
0.56
2008
0.59
0.70
0.61
0.55
0.67
0.57
0.56
0.67
2009
0.64
0.78
0.70
0.60
0.68
0.62
0.65
0.78
2010
0.70
0.68
0.73
0.55
0.62
0.67
0.66
0.68
2011
0.68
0.71
0.73
0.43
0.64
0.72
0.59
0.72
2012
0.74
0.67
0.78
0.62
0.69
0.79
0.69
0.66
2013
0.69
0.70
0.73
0.37
0.63
0.72
0.58
0.70
2005-2013
0.44
0.71
0.71
0.50
0.70
0.68
0.62
0.71
Table B.16:
Posterior mode estimates of bivariate copula parameters of eight bank
stocks with latent variable V1, based on Gibbs sampler output of size 1 000 in Kendall’s
tau scale. The Gibbs sampler with survival Gumbel linking copulas was run on DLM
ﬁltered daily observations of individual years 2005 to 2013 (top), and on all 2 345 daily
observations at once (bottom).
263

B
Empirical study: Figures and tables
B.5.4
Behavior of the latent variable over time
This subsection contains trace plots of the latent variable over time for the one-factor
copula model with Gumbel and survival Gumbel linking copulas. The trace plots show
the distribution of the latent variable V1 for the simultaneous analysis of years 2005 to
2013 (top) and the analyses of individual years 2005 and 2008 (bottom). Furthermore,
contour plots of the joint distribution of the copula data set and posterior samples of the
latent variable V1 are shown.
2006
2008
2010
2012
2014
0.0
0.4
0.8
Time
V1
Traceplot of V1 for years 2005 to 2013
Jan
Mar
May
Jul
Sep
Nov
Jan
0.0
0.4
0.8
Time
V1
Traceplot of V1 for year 2005
Jan
Mar
May
Jul
Sep
Nov
Jan
0.0
0.4
0.8
Time
V1
Traceplot of V1 for year 2008
Figure B.34: Traceplots of the latent variable V1 over time. The Gibbs sampling routine
of the one-factor copula model with Gumbel linking copulas was run on all 2 345 daily
observations (top), and on 260 and 262 daily observations of years 2005 (bottom left) and
2008 (bottom right), respectively. Values for V1 for each day are derived from the mean
of 100 equidistant iterations out of a total sample size of 1 000. Copula data is given by
DLM ﬁltered daily log returns of 8 bank stocks.
264

B.5
Bayesian inference for the one-factor copula model
2006
2008
2010
2012
2014
0.0
0.4
0.8
Time
V1
Traceplot of V1 for years 2005 to 2013
Jan
Mar
May
Jul
Sep
Nov
Jan
0.0
0.4
0.8
Time
V1
Traceplot of V1 for year 2005
Jan
Mar
May
Jul
Sep
Nov
Jan
0.0
0.4
0.8
Time
V1
Traceplot of V1 for year 2008
Figure B.35: Traceplots of the latent variable V1 over time. The Gibbs sampling routine
of the one-factor copula model with survival Gumbel linking copulas was run on all 2 345
daily observations (top), and on 260 and 262 daily observations of years 2005 (bottom
left) and 2008 (bottom right), respectively. Values for V1 for each day are derived from
the mean of 100 equidistant iterations out of a total sample size of 1 000. Copula data is
given by DLM ﬁltered daily log returns of 8 bank stocks.
265

B
Empirical study: Figures and tables
ACA, iteration 250
 0.01 
 0.05 
 0.1 
 0.2 
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
U
V1
 0.01 
 0.05 
 0.1 
 0.2 
Empirical density
Theoretical density
BBVA, iteration 250
 0.01 
 0.05 
 0.1 
 0.2 
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
U
V1
 0.01 
 0.05 
 0.1 
 0.2 
Empirical density
Theoretical density
BNP, iteration 250
 0.01 
 0.05 
 0.1 
 0.2 
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
U
V1
 0.01 
 0.05 
 0.1 
 0.2 
Empirical density
Theoretical density
ACA, iteration 500
 0.01 
 0.05 
 0.1 
 0.2 
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
U
V1
 0.01 
 0.05 
 0.1 
 0.2 
Empirical density
Theoretical density
BBVA, iteration 500
 0.01 
 0.05 
 0.1 
 0.2 
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
U
V1
 0.01 
 0.05 
 0.1 
 0.2 
Empirical density
Theoretical density
BNP, iteration 500
 0.01 
 0.05 
 0.1 
 0.2 
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
U
V1
 0.01 
 0.05 
 0.1 
 0.2 
Empirical density
Theoretical density
ACA, iteration 750
 0.01 
 0.05 
 0.1 
 0.2 
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
U
V1
 0.01 
 0.05 
 0.1 
 0.2 
Empirical density
Theoretical density
BBVA, iteration 750
 0.01 
 0.05 
 0.1 
 0.2 
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
U
V1
 0.01 
 0.05 
 0.1 
 0.2 
Empirical density
Theoretical density
BNP, iteration 750
 0.01 
 0.05 
 0.1 
 0.2 
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
U
V1
 0.01 
 0.05 
 0.1 
 0.2 
Empirical density
Theoretical density
ACA, iteration 1000
 0.01 
 0.05 
 0.1 
 0.2 
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
U
V1
 0.01 
 0.05 
 0.1 
 0.2 
Empirical density
Theoretical density
BBVA, iteration 1000
 0.01 
 0.05 
 0.1 
 0.2 
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
U
V1
 0.01 
 0.05 
 0.1 
 0.2 
Empirical density
Theoretical density
BNP, iteration 1000
 0.01 
 0.05 
 0.1 
 0.2 
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
U
V1
 0.01 
 0.05 
 0.1 
Empirical density
Theoretical density
Figure B.36: Contour plots with standard normal margins of the DLM ﬁltered copula data set of 8 bank stocks and posterior
samples of latent variable V1 for daily log returns of years 2005 to 2013. 1 000 samples of latent variable V1 were generated by an
individual ARMS Gibbs sampler for a one-factor copula model with Gumbel linking copulas. Iterations 250, 500, 750 and 1000 of
the Gibbs sampler for stocks ACA (top), BBVA (middle) and BNP (bottom) are shown. Empirical densities are indicated by solid
black lines, theoretical bivariate Gaussian copula densities with copula parameter of the corresponding iteration by dotted blue
lines.
266

B.5
Bayesian inference for the one-factor copula model
ACA, iteration 250
 0.01 
 0.05 
 0.1 
 0.2 
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
U
V1
 0.01 
 0.05 
 0.1 
 0.2 
Empirical density
Theoretical density
BBVA, iteration 250
 0.01 
 0.05 
 0.1 
 0.2 
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
U
V1
 0.01 
 0.05 
 0.1 
 0.2 
Empirical density
Theoretical density
BNP, iteration 250
 0.01 
 0.05 
 0.1 
 0.2 
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
U
V1
 0.01 
 0.05 
 0.1 
 0.2 
Empirical density
Theoretical density
ACA, iteration 500
 0.01 
 0.05 
 0.1 
 0.2 
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
U
V1
 0.01 
 0.05 
 0.1 
 0.2 
Empirical density
Theoretical density
BBVA, iteration 500
 0.01 
 0.05 
 0.1 
 0.2 
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
U
V1
 0.01 
 0.05 
 0.1 
 0.2 
Empirical density
Theoretical density
BNP, iteration 500
 0.01 
 0.05 
 0.1 
 0.2 
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
U
V1
 0.01 
 0.05 
 0.1 
 0.2 
Empirical density
Theoretical density
ACA, iteration 750
 0.01 
 0.05 
 0.1 
 0.2 
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
U
V1
 0.01 
 0.05 
 0.1 
 0.2 
Empirical density
Theoretical density
BBVA, iteration 750
 0.01 
 0.05 
 0.1 
 0.2 
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
U
V1
 0.01 
 0.05 
 0.1 
 0.2 
Empirical density
Theoretical density
BNP, iteration 750
 0.01 
 0.05 
 0.1 
 0.2 
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
U
V1
 0.01 
 0.05 
 0.1 
 0.2 
Empirical density
Theoretical density
ACA, iteration 1000
 0.01 
 0.05 
 0.1 
 0.2 
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
U
V1
 0.01 
 0.05 
 0.1 
 0.2 
Empirical density
Theoretical density
BBVA, iteration 1000
 0.01 
 0.05 
 0.1 
 0.2 
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
U
V1
 0.01 
 0.05 
 0.1 
 0.2 
Empirical density
Theoretical density
BNP, iteration 1000
 0.01 
 0.05 
 0.1 
 0.2 
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
U
V1
 0.01 
 0.05 
 0.1 
Empirical density
Theoretical density
Figure B.37: Contour plots with standard normal margins of the DLM ﬁltered copula data set of 8 bank stocks and posterior
samples of latent variable V1 for daily log returns of years 2005 to 2013. 1 000 samples of latent variable V1 were generated by an
individual ARMS Gibbs sampler for a one-factor copula model with survival Gumbel linking copulas. Iterations 250, 500, 750 and
1 000 of the Gibbs sampler for stocks ACA (top), BBVA (middle) and BNP (bottom) are shown. Empirical densities are indicated
by solid black lines, theoretical bivariate Gaussian copula densities with copula parameter of the corresponding iteration by dotted
blue lines.
267

B
Empirical study: Figures and tables
ACA, iteration 250
 0.01 
 0.05 
 0.1 
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
U
V1
 0.01 
 0.05 
 0.1 
 0.2 
Empirical density
Theoretical density
BBVA, iteration 250
 0.01 
 0.05 
 0.1 
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
U
V1
 0.01 
 0.05 
 0.1 
 0.2 
Empirical density
Theoretical density
BNP, iteration 250
 0.01 
 0.05 
 0.1 
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
U
V1
 0.01 
 0.05 
 0.1 
 0.2 
Empirical density
Theoretical density
ACA, iteration 500
 0.01 
 0.05 
 0.1 
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
U
V1
 0.01 
 0.05 
 0.1 
 0.2 
Empirical density
Theoretical density
BBVA, iteration 500
 0.01 
 0.05 
 0.1 
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
U
V1
 0.01 
 0.05 
 0.1 
 0.2 
Empirical density
Theoretical density
BNP, iteration 500
 0.01 
 0.05 
 0.1 
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
U
V1
 0.01 
 0.05 
 0.1 
 0.2 
Empirical density
Theoretical density
ACA, iteration 750
 0.01 
 0.05 
 0.1 
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
U
V1
 0.01 
 0.05 
 0.1 
 0.2 
Empirical density
Theoretical density
BBVA, iteration 750
 0.01 
 0.05 
 0.1 
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
U
V1
 0.01 
 0.05 
 0.1 
 0.2 
Empirical density
Theoretical density
BNP, iteration 750
 0.01 
 0.05 
 0.1 
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
U
V1
 0.01 
 0.05 
 0.1 
 0.2 
Empirical density
Theoretical density
ACA, iteration 1000
 0.01 
 0.05 
 0.1 
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
U
V1
 0.01 
 0.05 
 0.1 
 0.2 
Empirical density
Theoretical density
BBVA, iteration 1000
 0.01 
 0.05 
 0.1 
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
U
V1
 0.01 
 0.05 
 0.1 
 0.2 
Empirical density
Theoretical density
BNP, iteration 1000
 0.01 
 0.05 
 0.1 
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
U
V1
 0.01 
 0.05 
 0.1 
 0.2 
Empirical density
Theoretical density
Figure B.38: Contour plots with standard normal margins of the DLM ﬁltered copula data set of 8 bank stocks and posterior
samples of latent variable V1 for daily log returns of year 2005. 1 000 samples of latent variable V1 were generated by an individual
ARMS Gibbs sampler for a one-factor copula model with Gumbel linking copulas. Iterations 250, 500, 750 and 1 000 of the Gibbs
sampler for stocks ACA (top), BBVA (middle) and BNP (bottom) are shown. Empirical densities are indicated by solid black lines,
theoretical bivariate Gaussian copula densities with copula parameter of the corresponding iteration by dotted blue lines.
268

B.5
Bayesian inference for the one-factor copula model
ACA, iteration 250
 0.01 
 0.05 
 0.1 
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
U
V1
 0.01 
 0.05 
 0.1 
 0.2 
Empirical density
Theoretical density
BBVA, iteration 250
 0.01 
 0.05 
 0.1 
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
U
V1
 0.01 
 0.05 
 0.1 
 0.2 
Empirical density
Theoretical density
BNP, iteration 250
 0.01 
 0.05 
 0.1 
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
U
V1
 0.01 
 0.05 
 0.1 
 0.2 
Empirical density
Theoretical density
ACA, iteration 500
 0.01 
 0.05 
 0.1 
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
U
V1
 0.01 
 0.05 
 0.1 
 0.2 
Empirical density
Theoretical density
BBVA, iteration 500
 0.01 
 0.05 
 0.1 
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
U
V1
 0.01 
 0.05 
 0.1 
 0.2 
Empirical density
Theoretical density
BNP, iteration 500
 0.01 
 0.05 
 0.1 
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
U
V1
 0.01 
 0.05 
 0.1 
 0.2 
Empirical density
Theoretical density
ACA, iteration 750
 0.01 
 0.05 
 0.1 
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
U
V1
 0.01 
 0.05 
 0.1 
 0.2 
Empirical density
Theoretical density
BBVA, iteration 750
 0.01 
 0.05 
 0.1 
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
U
V1
 0.01 
 0.05 
 0.1 
 0.2 
Empirical density
Theoretical density
BNP, iteration 750
 0.01 
 0.05 
 0.1 
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
U
V1
 0.01 
 0.05 
 0.1 
 0.2 
Empirical density
Theoretical density
ACA, iteration 1000
 0.01 
 0.05 
 0.1 
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
U
V1
 0.01 
 0.05 
 0.1 
 0.2 
Empirical density
Theoretical density
BBVA, iteration 1000
 0.01 
 0.05 
 0.1 
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
U
V1
 0.01 
 0.05 
 0.1 
 0.2 
Empirical density
Theoretical density
BNP, iteration 1000
 0.01 
 0.05 
 0.1 
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
U
V1
 0.01 
 0.05 
 0.1 
 0.2 
Empirical density
Theoretical density
Figure B.39: Contour plots with standard normal margins of the DLM ﬁltered copula data set of eight bank stocks and posterior
samples of latent variable V1 for daily log returns of year 2005. 1 000 samples of latent variable V1 were generated by an individual
ARMS Gibbs sampler for a one-factor copula model with survival Gumbel linking copulas. Iterations 250, 500, 750 and 1 000 of
the Gibbs sampler for stocks ACA (top), BBVA (middle) and BNP (bottom) are shown. Empirical densities are indicated by solid
black lines, theoretical bivariate Gaussian copula densities with copula parameter of the corresponding iteration by dotted blue
lines.
269

B
Empirical study: Figures and tables
ACA
 0.02 
 0.05 
 0.1 
 0.2 
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
U
V1
 0.02 
 0.05 
 0.1 
 0.2 
Empirical density
Theoretical density
BBVA
 0.02 
 0.05 
 0.1 
 0.2 
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
U
V1
 0.02 
 0.05 
 0.1 
 0.2 
Empirical density
Theoretical density
BNP
 0.02 
 0.05 
 0.1 
 0.2 
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
U
V1
 0.02 
 0.05 
 0.1 
 0.2 
Empirical density
Theoretical density
CBK
 0.02 
 0.05 
 0.1 
 0.2 
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
U
V1
 0.02 
 0.05 
 0.1 
 0.2 
Empirical density
Theoretical density
DBK
 0.02 
 0.05 
 0.1 
 0.2 
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
U
V1
 0.02 
 0.05 
 0.1 
 0.2 
Empirical density
Theoretical density
GLE
 0.02 
 0.05 
 0.1 
 0.2 
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
U
V1
 0.02 
 0.05 
 0.1 
 0.2 
Empirical density
Theoretical density
ISP
 0.02 
 0.05 
 0.1 
 0.2 
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
U
V1
 0.02 
 0.05 
 0.1 
 0.2 
Empirical density
Theoretical density
SAN
 0.02 
 0.05 
 0.1 
 0.2 
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
U
V1
 0.02 
 0.05 
 0.1 
 0.2 
Empirical density
Theoretical density
Figure B.40: Contour plots with standard normal margins of the DLM ﬁltered copula
data set of eight bank stocks and posterior samples of latent variable V1 for daily log
returns from years 2005 to 2013. 1 000 samples of latent variable V1 were generated by
an individual ARMS Gibbs sampler for a one-factor copula model with Gumbel linking
copulas. The mean of all iterations of the Gibbs sampler is used for latent variable V1 and
the copula parameters. Empirical densities are indicated by solid black lines, theoretical
bivariate Gaussian copula densities with corresponding copula parameters by dotted blue
lines.
270

B.5
Bayesian inference for the one-factor copula model
ACA
 0.02 
 0.05 
 0.1 
 0.2 
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
U
V1
 0.02 
 0.05 
 0.1 
 0.2 
Empirical density
Theoretical density
BBVA
 0.02 
 0.05 
 0.1 
 0.2 
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
U
V1
 0.02 
 0.05 
 0.1 
 0.2 
Empirical density
Theoretical density
BNP
 0.02 
 0.05 
 0.1 
 0.2 
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
U
V1
 0.02 
 0.05 
 0.1 
 0.2 
Empirical density
Theoretical density
CBK
 0.02 
 0.05 
 0.1 
 0.2 
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
U
V1
 0.02 
 0.05 
 0.1 
 0.2 
Empirical density
Theoretical density
DBK
 0.02 
 0.05 
 0.1 
 0.2 
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
U
V1
 0.02 
 0.05 
 0.1 
 0.2 
Empirical density
Theoretical density
GLE
 0.02 
 0.05 
 0.1 
 0.2 
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
U
V1
 0.02 
 0.05 
 0.1 
 0.2 
Empirical density
Theoretical density
ISP
 0.02 
 0.05 
 0.1 
 0.2 
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
U
V1
 0.02 
 0.05 
 0.1 
 0.2 
Empirical density
Theoretical density
SAN
 0.02 
 0.05 
 0.1 
 0.2 
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
U
V1
 0.02 
 0.05 
 0.1 
 0.2 
Empirical density
Theoretical density
Figure B.41: Contour plots with standard normal margins of the DLM ﬁltered copula
data set of eight bank stocks and posterior samples of latent variable V1 for daily log
returns from years 2005 to 2013. 1 000 samples of latent variable V1 were generated by
an individual ARMS Gibbs sampler for a one-factor copula model with survival Gumbel
linking copulas. The mean of all iterations of the Gibbs sampler is used for latent variable
V1 and the copula parameters.
Empirical densities are indicated by solid black lines,
theoretical bivariate Gaussian copula densities with corresponding copula parameters by
dotted blue lines.
271

B
Empirical study: Figures and tables
ACA
 0.02 
 0.05 
 0.1 
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
U
V1
 0.02 
 0.05 
 0.1 
 0.2 
Empirical density
Theoretical density
BBVA
 0.02 
 0.05 
 0.1 
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
U
V1
 0.02 
 0.05 
 0.1 
 0.2 
Empirical density
Theoretical density
BNP
 0.02 
 0.05 
 0.1 
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
U
V1
 0.02 
 0.05 
 0.1 
 0.2 
Empirical density
Theoretical density
CBK
 0.02 
 0.05 
 0.1 
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
U
V1
 0.02 
 0.05 
 0.1 
Empirical density
Theoretical density
DBK
 0.02 
 0.05 
 0.1 
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
U
V1
 0.02 
 0.05 
 0.1 
 0.2 
Empirical density
Theoretical density
GLE
 0.02 
 0.05 
 0.1 
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
U
V1
 0.02 
 0.05 
 0.1 
 0.2 
Empirical density
Theoretical density
ISP
 0.02 
 0.05 
 0.1 
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
U
V1
 0.02 
 0.05 
 0.1 
Empirical density
Theoretical density
SAN
 0.02 
 0.05 
 0.1 
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
U
V1
 0.02 
 0.05 
 0.1 
 0.2 
Empirical density
Theoretical density
Figure B.42: Contour plots with standard normal margins of the DLM ﬁltered copula
data set of eight bank stocks and posterior samples of latent variable V1 for daily log
returns from year 2005. 1 000 samples of latent variable V1 were generated by an individual
ARMS Gibbs sampler for a one-factor copula model with Gumbel linking copulas. The
mean of all iterations of the Gibbs sampler is used for latent variable V1 and the copula
parameters. Empirical densities are indicated by solid black lines, theoretical bivariate
Gaussian copula densities with corresponding copula parameters by dotted blue lines.
272

B.5
Bayesian inference for the one-factor copula model
ACA
 0.02 
 0.05 
 0.1 
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
U
V1
 0.02 
 0.05 
 0.1 
 0.2 
Empirical density
Theoretical density
BBVA
 0.02 
 0.05 
 0.1 
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
U
V1
 0.02 
 0.05 
 0.1 
 0.2 
Empirical density
Theoretical density
BNP
 0.02 
 0.05 
 0.1 
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
U
V1
 0.02 
 0.05 
 0.1 
 0.2 
Empirical density
Theoretical density
CBK
 0.02 
 0.05 
 0.1 
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
U
V1
 0.02 
 0.05 
 0.1 
 0.2 
Empirical density
Theoretical density
DBK
 0.02 
 0.05 
 0.1 
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
U
V1
 0.02 
 0.05 
 0.1 
 0.2 
Empirical density
Theoretical density
GLE
 0.02 
 0.05 
 0.1 
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
U
V1
 0.02 
 0.05 
 0.1 
 0.2 
Empirical density
Theoretical density
ISP
 0.02 
 0.05 
 0.1 
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
U
V1
 0.02 
 0.05 
 0.1 
Empirical density
Theoretical density
SAN
 0.02 
 0.05 
 0.1 
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
U
V1
 0.02 
 0.05 
 0.1 
 0.2 
Empirical density
Theoretical density
Figure B.43: Contour plots with standard normal margins of the DLM ﬁltered copula
data set of eight bank stocks and posterior samples of latent variable V1 for daily log
returns from year 2005. 1 000 samples of latent variable V1 were generated by an individual
ARMS Gibbs sampler for a one-factor copula model with survival Gumbel linking copulas.
The mean of all iterations of the Gibbs sampler is used for latent variable V1 and the copula
parameters. Empirical densities are indicated by solid black lines, theoretical bivariate
Gaussian copula densities with corresponding copula parameters by dotted blue lines.
273

B
Empirical study: Figures and tables
B.6
Value-at-Risk and expected shortfall forecasts
In this section, supplementary tables for the 90% VaR and ES portfolio forecasts of Sec-
tion 9.5 are given. The underlying data is generated by marginal time-varying ARMA(1, 1)
DLMs and observations are split into individual years.
B.6.1
Vine copula ﬁts of individual years
This subsection contains tables with the details of the ﬁtted RVR and cv2R copula models.
The tables show copula families with theoretical Kendall’s tau value of the parameter
estimate in parentheses, and for the Student’s t copula the degrees of freedom parameter
ν in subscript (left), and the corresponding RVM (left). The following abbreviations are
used in the RVM for the bank stocks: S1: ACA, S2: BBVA, S3: BNP, S4: CBK, S5:
DBK, S6: GLE, S7: ISP and S8: SAN. Further details are given in Section 9.3.1.
274

B.6
Value-at-Risk and expected shortfall forecasts
Families
RVR, year 2005
I
I
I
I
SG(.1)
I
I
I
I
I
F(.11)
F(.11)
F(.14)
N(.09)
F(.15)
SG(.18)
F(.19)
F(.19)
F(.13)
F(.25)
F(.27)
F(.44)
F(.62)
F(.49)
SG(.32)
F(.51)
t6.6(.48)
t7.1(.53)
RVM
S4
S3
S3
S1
S7
S1
S8
S5
S7
S7
S7
S8
S5
S8
S5
S2
S2
S8
S5
S8
S6
S6
S1
S2
S2
S2
S8
S8
S5
S6
S6
S6
S6
S2
S2
S2
Families
RVR, year 2006
I
I
I
I
F(.09)
I
t4.2(.09)
I
F(.17)
N(.11)
F(.17)
G(.1)
F(.13)
I
t7.6(.17)
t8.4(.24)
F(.24)
I
F(.11)
N(.2)
G(.26)
F(.52)
SG(.55)
SG(.48)
SG(.4)
F(.62)
F(.58)
t11(.58)
RVM
S1
S7
S2
S2
S7
S4
S4
S6
S7
S7
S6
S3
S6
S5
S6
S5
S4
S3
S8
S5
S3
S8
S5
S8
S3
S8
S5
S8
S3
S8
S5
S6
S3
S8
S5
S5
Families
RVR, year 2007
I
I
I
I
I
I
I
I
F(.1)
SG(.12)
I
N(.17)
N(.14)
F(.14)
F(.17)
F(.2)
F(.15)
F(.21)
SG(.13)
F(.21)
C(.09)
t4.1(.6)
t14(.53)
t4.1(.63)
t7.6(.56)
t11.3(.63)
t5.5(.63)
F(.51)
RVM
S8
S7
S4
S6
S7
S6
S4
S6
S7
S2
S5
S2
S2
S7
S1
S1
S1
S5
S5
S7
S3
S3
S3
S1
S1
S5
S7
S7
S2
S5
S3
S3
S3
S5
S5
S5
Families
RVR, year 2008
I
I
G(.09)
F(.12)
SG(.05)
I
G(.06)
I
F(.18)
t6.8(.07)
G(.07)
t10.6(.11)
I
F(.28)
F(.1)
SG(.04)
F(.35)
F(.26)
t8.3(.12)
F(.27)
F(.25)
F(.62)
t4.3(.54)
t7.2(.57)
t7.5(.53)
t14.8(.71)
t17.4(.56)
t12.3(.51)
RVM
S4
S7
S6
S6
S7
S1
S1
S5
S7
S3
S3
S8
S5
S7
S2
S2
S2
S8
S5
S7
S5
S8
S1
S2
S8
S5
S7
S8
S5
S3
S3
S2
S8
S8
S7
S7
Table B.17: Bivariate copula families and RVMs of the R- vine copulas without index
data of years 2005 to 2008 and with restriction of choices for bivariate linking copula
families.
Theoretical Kendall’s tau values are given in parentheses behind the family
name, and values above 0.2 emphasized in bold face. For the Student’s t copula, the
degrees of freedom parameter ν is stated as subscript tν. The following abbreviations are
used for stock and indices names in the RVM: S1: ACA, S2: BBVA, S3: BNP, S4: CBK,
S5: DBK, S6: GLE, S7: ISP and S8: SAN.
275

B
Empirical study: Figures and tables
Families
cv2R , year 2005
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
F(.17)
F(.19)
F(.13)
F(.19)
F(.25)
F(.39)
F(.62)
SG(.38)
SG(.32)
F(.49)
F(.51)
t6.6(.48)
t8.8(.47)
RVM
S3
S7
S4
S4
S7
S7
S8
S8
S8
S1
S1
S1
S1
S8
S5
S5
S5
S5
S5
S8
S2
S2
S2
S2
S2
S2
S8
S8
S6
S6
S6
S6
S6
S6
S6
S6
Families
cv2R , year 2006
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
N(.19)
F(.24)
F(.37)
t4.2(.17)
N(.15)
t8.8(.17)
t7.2(.53)
SG(.48)
t5.4(.56)
G(.5)
SG(.39)
t7.1(.55)
t11(.58)
RVM
S2
S7
S4
S4
S7
S3
S3
S3
S7
S1
S1
S1
S1
S7
S7
S8
S8
S8
S8
S8
S6
S6
S6
S6
S6
S6
S8
S8
S5
S5
S5
S5
S5
S5
S5
S5
Families
cv2R , year 2007
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
SG(.13)
SG(.19)
C(.14)
SG(.13)
BB1(.15)
SG(.36)
t4.1(.63)
t6.7(.44)
t6.4(.48)
t11.3(.63)
t5.5(.63)
t7.6(.56)
F(.56)
RVM
S6
S7
S7
S8
S8
S4
S4
S4
S8
S1
S1
S1
S1
S8
S5
S5
S5
S5
S5
S8
S2
S2
S2
S2
S2
S2
S8
S8
S3
S3
S3
S3
S3
S3
S3
S3
Families
cv2R , year 2008
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
G(.11)
G(.19)
t4.4(.21)
t4.9(.25)
G(.17)
t6.6(.52)
F(.62)
t15.7(.52)
F(.57)
t12.5(.53)
t4.7(.5)
N(.58)
t17.4(.56)
RVM
S4
S8
S1
S1
S8
S7
S7
S7
S8
S3
S3
S3
S3
S8
S6
S6
S6
S6
S6
S8
S2
S2
S2
S2
S2
S2
S8
S8
S5
S5
S5
S5
S5
S5
S5
S5
Table B.18: Bivariate copula families and RVMs of the C- vine copulas truncated at level
2 without index data of years 2005 to 2008 and with restriction of choices for bivariate
linking copula families. Theoretical Kendall’s tau values are given in parentheses behind
the family name, and values above 0.2 emphasized in bold face. For the Student’s t copula,
the degrees of freedom parameter ν is stated as subscript tν. The following abbreviations
are used for stock and indices names in the RVM: S1: ACA, S2: BBVA, S3: BNP, S4:
CBK, S5: DBK, S6: GLE, S7: ISP and S8: SAN.
276

B.6
Value-at-Risk and expected shortfall forecasts
Families
RVR, year 2009
I
I
I
I
I
G(.09)
N(.11)
SG(.11)
I
I
N(.14)
N(.15)
N(.1)
F(.15)
SG(.19)
F(.16)
N(.28)
N(.2)
F(.31)
t7.6(.1)
t5.4(.14)
F(.58)
F(.61)
F(.62)
t12.2(.63)
t11.6(.61)
SG(.77)
t5.2(.62)
RVM
S4
S1
S1
S6
S7
S6
S7
S2
S7
S5
S2
S5
S2
S7
S7
S8
S8
S5
S2
S3
S2
S3
S3
S8
S8
S8
S3
S8
S5
S6
S3
S3
S2
S8
S3
S3
Families
RVR, year 2010
I
I
I
I
I
I
F(.19)
I
I
I
G(.09)
F(.12)
SG(.13)
I
F(.1)
F(.19)
F(.19)
G(.21)
SG(.12)
N(.29)
t4.6(.1)
G(.55)
G(.61)
t13.1(.66)
t6.3(.67)
t16.9(.59)
t9(.78)
t13.1(.6)
RVM
S4
S2
S5
S8
S2
S1
S7
S8
S2
S6
S1
S7
S8
S2
S7
S6
S1
S7
S8
S2
S2
S3
S6
S3
S7
S8
S3
S8
S5
S3
S6
S3
S3
S8
S3
S3
Families
RVR, year 2011
SG(.07)
SG(.06)
I
I
I
SG(.08)
I
I
I
N(.15)
G(.1)
N(.14)
C(.18)
I
SG(.08)
N(.09)
N(.25)
F(.23)
F(.23)
C(.09)
t8.3(.1)
t7.7(.44)
t5.9(.67)
BB1(.59)
t4.7(.68)
t11.1(.58)
t9.8(.8)
F(.65)
RVM
S4
S7
S1
S2
S7
S5
S8
S2
S7
S3
S1
S8
S2
S7
S7
S6
S5
S8
S2
S6
S2
S3
S3
S6
S8
S8
S6
S8
S5
S6
S3
S6
S2
S8
S6
S6
Families
RVR, year 2012
I
I
I
C(.09)
I
I
I
G(.08)
I
I
t6.7(.16)
BB1(.16)
F(.09)
I
I
N(.2)
SG(.2)
G(.17)
F(.24)
F(.29)
C(.1)
t5.9(.7)
BB1(.74)
F(.62)
t5.7(.64)
G(.62)
t13.3(.62)
N(.82)
RVM
S1
S8
S6
S2
S8
S4
S4
S2
S8
S5
S5
S4
S2
S8
S3
S7
S5
S7
S2
S8
S7
S3
S7
S3
S7
S2
S8
S8
S6
S3
S5
S3
S7
S2
S2
S2
Table B.19: Bivariate copula families and RVMs of the R- vine copulas without index
data of years 2009 to 2012 and with restriction of choices for bivariate linking copula
families.
Theoretical Kendall’s tau values are given in parentheses behind the family
name, and values above 0.2 emphasized in bold face. For the Student’s t copula, the
degrees of freedom parameter ν is stated as subscript tν. The following abbreviations are
used for stock and indices names in the RVM: S1: ACA, S2: BBVA, S3: BNP, S4: CBK,
S5: DBK, S6: GLE, S7: ISP and S8: SAN.
277

B
Empirical study: Figures and tables
Families
cv2R , year 2009
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
SG(.19)
N(.37)
t9.3(.21)
SG(.26)
N(.32)
t6.6(.16)
F(.56)
F(.57)
t11.6(.61)
t3.9(.62)
t5.1(.61)
F(.56)
SG(.77)
RVM
S4
S8
S6
S6
S8
S7
S7
S7
S8
S5
S5
S5
S5
S8
S3
S3
S3
S3
S3
S8
S1
S1
S1
S1
S1
S1
S8
S8
S2
S2
S2
S2
S2
S2
S2
S2
Families
cv2R , year 2010
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
F(.21)
N(.1)
F(.34)
F(.16)
F(.22)
N(.64)
G(.5)
t6.3(.67)
t16.9(.59)
G(.61)
t12(.63)
t5.9(.59)
t13.1(.6)
RVM
S4
S8
S6
S6
S8
S7
S7
S7
S8
S5
S5
S5
S5
S8
S1
S1
S1
S1
S1
S8
S2
S2
S2
S2
S2
S2
S8
S8
S3
S3
S3
S3
S3
S3
S3
S3
Families
cv2R , year 2011
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
t8(.41)
F(.11)
F(.24)
F(.46)
F(.34)
SG(.16)
F(.65)
t9.8(.8)
N(.39)
N(.61)
t15.5(.58)
N(.56)
N(.58)
RVM
S6
S7
S2
S2
S7
S4
S4
S4
S7
S3
S3
S3
S3
S7
S5
S5
S5
S5
S5
S7
S1
S1
S1
S1
S1
S1
S7
S8
S8
S8
S8
S8
S8
S8
S7
S7
Families
cv2R , year 2012
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
C(.09)
C(.13)
N(.3)
t11.2(.14)
N(.15)
t9.7(.68)
F(.59)
BB1(.74)
G(.62)
t7.4(.68)
t5.7(.64)
t5.4(.61)
t6.7(.6)
RVM
S4
S8
S6
S6
S8
S7
S7
S7
S8
S1
S1
S1
S1
S8
S5
S5
S5
S5
S5
S8
S2
S2
S2
S2
S2
S2
S8
S8
S3
S3
S3
S3
S3
S3
S3
S3
Table B.20: Bivariate copula families and RVMs of the C- vine copulas truncated at level
2 without index data of years 2009 to 2012 and with restriction of choices for bivariate
linking copula families. Theoretical Kendall’s tau values are given in parentheses behind
the family name, and values above 0.2 emphasized in bold face. For the Student’s t copula,
the degrees of freedom parameter ν is stated as subscript tν. The following abbreviations
are used for stock and indices names in the RVM: S1: ACA, S2: BBVA, S3: BNP, S4:
CBK, S5: DBK, S6: GLE, S7: ISP and S8: SAN.
278

List of Figures
List of Figures
2.1
Contour plots of bivariate copulas for several families. . . . . . . . . . . . .
16
2.2
Example of an R-Vine on 5 elements with the corresponding RVM.
. . . .
30
4.1
Visualization of the ARS algorithm for a Beta(2, 2.5) density.
. . . . . . .
46
5.1
Densities of the normal, Student’s t, normal-inverse Gaussian and general-
ized hyperbolic distribution. . . . . . . . . . . . . . . . . . . . . . . . . . .
53
6.1
Visualisation of the one-factor copula model and corresponding C-vine
truncated at level 1 for dimension d = 5. . . . . . . . . . . . . . . . . . . .
61
6.2
Visualization of the second tree of the C-vine truncated at level 2 corres-
ponding to the two-factor copula model for dimension d = 5. . . . . . . . .
65
8.1
Validation of the ﬁve sampling methods of Sections 8.3.2 and 8.3.3.
. . . .
84
8.2
Trace and density plots of V1,3 for high Kendall’s tau values for a single
run of the Gibbs sampler for Gumbel linking copulas with ﬁve diﬀerent
sampling methods (n = 200, d = 5). . . . . . . . . . . . . . . . . . . . . . .
89
8.3
Trace and density plots of V1,189 for low Kendall’s tau values for a single
run of the Gibbs sampler for Gumbel linking copulas with ﬁve diﬀerent
sampling methods (n = 200, d = 5). . . . . . . . . . . . . . . . . . . . . . .
90
8.4
Trace and density plots of H1 for mixed Kendall’s tau values for a single
run of the Gibbs sampler for Gumbel linking copulas with ﬁve diﬀerent
sampling methods (n = 200, d = 5). . . . . . . . . . . . . . . . . . . . . . .
91
8.5
Trace and density plots of H5 for mixed Kendall’s tau values for a single
run of the Gibbs sampler for Gumbel linking copulas with ﬁve diﬀerent
sampling methods (n = 200, d = 5). . . . . . . . . . . . . . . . . . . . . . .
92
8.6
Trace and density plots of thinned individual ARMS of the Gibbs sampler
based on simulated data from a one-factor copula model with Gumbel
linking copulas (n = 200, d = 5).
. . . . . . . . . . . . . . . . . . . . . . .
95
8.7
Trace and density plots of the individual ARMS Gibbs sampler for V1,1
and τ1 for mixed Kendall’s tau values with Gaussian and survival Gumbel
linking copulas (n = 200, d = 5).
. . . . . . . . . . . . . . . . . . . . . . . 100
9.1
Daily log returns of two indices and eight bank stocks in the ten-year period
of 2004-01-01 to 2013-12-31. . . . . . . . . . . . . . . . . . . . . . . . . . . 104
9.2
Diagnostics of an ARMA(1, 1)-GARCH(1, 1) ﬁt with Student’s t distrib-
uted innovations to the daily log returns of index SX7P. . . . . . . . . . . . 107
9.3
Comparison of out-of-sample and in-sample ARMA(1, 1)-GARCH(1, 1) re-
siduals of daily log returns of index SX7P transformed to copula data. . . . 109
9.4
Daily log returns (grey) with ARMA(1, 1)-GARCH(1, 1) forecast value and
90% conﬁdence bands. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 110
9.5
Histograms of the out-of-sample ARMA(1, 1)-GARCH(1, 1) residuals trans-
formed to standard uniform margins by probability integral transform.
. . 111
9.6
Scatter plots of pairwise ARMA-GARCH copula data in U and Z scale. . . 112
9.7
Contour plot of bivariate copula density in Z space, and empirical Kendall’s
tau and Spearman’s ρ values for ARMA-GARCH copula data. . . . . . . . 113
9.8
Daily log returns with time-varying ARMA(1, 1) forecast value and 90%
conﬁdence bands. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117
279

List of Figures
9.9
Histograms of the time-varying ARMA(1, 1) DLM residuals transformed to
standard uniform margins by probability integral transform.
. . . . . . . . 118
9.10 Vine tree plots of trees 1 to 4 of RV I
R and CV I
R for ARMA-GARCH copula
data. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 122
9.11 Vine tree plots of trees 1 to 4 of RVR and CVR for ARMA-GARCH copula
data. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 125
9.12 Trace and density plots of the individual ARMS Gibbs sampler for τACA
based on 2345 DLM ﬁltered daily observations of years 2005 to 2013, and
260 observations of year 2005 with Gaussian linking copulas. . . . . . . . . 130
9.13 Traceplots of the latent variable over time, based on the individual ARMS
Gibbs sampler for the one-factor copula model with Gaussian copulas. . . . 132
9.14 Contour plots of ACA and the mean of the posterior samples with Gumbel,
Gaussian and survival Gumbel linking copulas of latent variable V1 for daily
log returns of year 2005 to 2013. . . . . . . . . . . . . . . . . . . . . . . . . 133
9.15 Contour plots of ACA, BBVA and BNP, and posterior samples with Gaus-
sian linking copulas of latent variable V1 for iterations 250, 500, 750 and
1000 for daily log returns of years 2005 to 2013. . . . . . . . . . . . . . . . 134
9.16 Contour plots of ACA, BBVA and BNP, and posterior samples with Gaus-
sian linking copulas of latent variable V1 for iterations 250, 500, 750 and
1000 for daily log returns of year 2005. . . . . . . . . . . . . . . . . . . . . 135
9.17 Contour plots of 8 bank stocks and the mean of the posterior samples with
Gaussian linking copulas of latent variable V1 for daily log returns of years
2005 to 2013.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 136
9.18 Contour plots of 8 bank stocks and the mean of the posterior samples with
Gaussian linking copulas of latent variable V1 for daily log returns of year
2005. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 137
9.19 Historical relative portfolio value of a constant mix strategy with equal
weights.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 140
9.20 Daily log returns of the equally weighted constant mix portfolio with 90%
VaR and ES forecasts based on vine and one-factor copula model MCMC
samples and DLM marginal models. . . . . . . . . . . . . . . . . . . . . . . 142
A.1 Trace and density plots of V1 for the individual ARMS Gibbs sampler with
Gumbel linking copulas for low Kendall’s tau values.
. . . . . . . . . . . . 145
A.2 Trace and density plots of V1 for the block ARMS Gibbs sampler with
Gumbel linking copulas for low Kendall’s tau values.
. . . . . . . . . . . . 146
A.3 Trace and density plots of V1 for the mode and curvature matching Gibbs
sampler with Gumbel linking copulas for low Kendall’s tau values. . . . . . 147
A.4 Trace and density plots of V1 for the expectation and variance matching
Gibbs sampler with Gumbel linking copulas for low Kendall’s tau values. . 148
A.5 Trace and density plots of V1 for the independence and random walk Gibbs
sampler with Gumbel linking copulas for low Kendall’s tau values. . . . . . 149
A.6 Trace and density plots of V1 the thinned individual ARMS Gibbs sampler
with Gumbel linking copulas for low Kendall’s tau values. . . . . . . . . . . 150
A.7 Trace and density plots of H for the individual ARMS Gibbs sampler with
Gumbel linking copulas for low Kendall’s tau values.
. . . . . . . . . . . . 151
280

List of Figures
A.8 Trace and density plots of H for the block ARMS Gibbs sampler with
Gumbel linking copulas for low Kendall’s tau values.
. . . . . . . . . . . . 152
A.9 Trace and density plots of H for the mode and curvature matching Gibbs
sampler with Gumbel linking copulas for low Kendall’s tau values. . . . . . 153
A.10 Trace and density plots of H for the expectation and variance matching
Gibbs sampler with Gumbel linking copulas for low Kendall’s tau values. . 154
A.11 Trace and density plots of H for the independence and random walk Gibbs
sampler with Gumbel linking copulas for low Kendall’s tau values. . . . . . 155
A.12 Trace and density plots of H the thinned individual ARMS Gibbs sampler
with Gumbel linking copulas for low Kendall’s tau values. . . . . . . . . . . 156
A.13 Trace and density plots of V1 the individual ARMS Gibbs sampler with
Gumbel linking copulas for high Kendall’s tau values. . . . . . . . . . . . . 157
A.14 Trace and density plots of V1 for the block ARMS Gibbs sampler with
Gumbel linking copulas for high Kendall’s tau values. . . . . . . . . . . . . 158
A.15 Trace and density plots of V1 for the mode and curvature matching Gibbs
sampler with Gumbel linking copulas for high Kendall’s tau values.
. . . . 159
A.16 Trace and density plots of V1 for the expectation and variance matching
Gibbs sampler with Gumbel linking copulas for high Kendall’s tau values. . 160
A.17 Trace and density plots of V1 for the independence and random walk Gibbs
sampler with Gumbel linking copulas for high Kendall’s tau values.
. . . . 161
A.18 Trace and density plots of V1 for the thinned individual ARMS Gibbs
sampler with Gumbel linking copulas for high Kendall’s tau values.
. . . . 162
A.19 Trace and density plots of H for the individual ARMS Gibbs sampler with
Gumbel linking copulas for high Kendall’s tau values. . . . . . . . . . . . . 163
A.20 Trace and density plots of H for the block ARMS Gibbs sampler with
Gumbel linking copulas for high Kendall’s tau values. . . . . . . . . . . . . 164
A.21 Trace and density plots of H for the mode and curvature matching Gibbs
sampler with Gumbel linking copulas for high Kendall’s tau values.
. . . . 165
A.22 Trace and density plots of H for the expectation and variance matching
Gibbs sampler with Gumbel linking copulas for high Kendall’s tau values. . 166
A.23 Trace and density plots of H for the independence and random walk Gibbs
sampler with Gumbel linking copulas for high Kendall’s tau values.
. . . . 167
A.24 Trace and density plots of H for the thinned individual ARMS Gibbs
sampler with Gumbel linking copulas for high Kendall’s tau values.
. . . . 168
A.25 Trace and density plots of V1 for the individual ARMS Gibbs sampler with
Gumbel linking copulas for mixed Kendall’s tau values. . . . . . . . . . . . 169
A.26 Trace and density plots of V1 for the block ARMS Gibbs sampler with
Gumbel linking copulas for mixed Kendall’s tau values. . . . . . . . . . . . 170
A.27 Trace and density plots of V1 for the mode and curvature matching Gibbs
sampler with Gumbel linking copulas for mixed Kendall’s tau values.
. . . 171
A.28 Trace and density plots of V1 for expectation and variance matching Gibbs
sampler with Gumbel linking copulas for mixed Kendall’s tau values.
. . . 172
A.29 Trace and density plots of V1 for the independence and random walk Gibbs
sampler with Gumbel linking copulas for mixed Kendall’s tau values.
. . . 173
A.30 Trace and density plots of V1 for the thinned individual ARMS Gibbs
sampler with Gumbel linking copulas for mixed Kendall’s tau values.
. . . 174
281

List of Figures
A.31 Trace and density plots of H for the individual ARMS Gibbs sampler with
Gumbel linking copulas for mixed Kendall’s tau values. . . . . . . . . . . . 175
A.32 Trace and density plots of H for the block ARMS Gibbs sampler with
Gumbel linking copulas for mixed Kendall’s tau values. . . . . . . . . . . . 176
A.33 Trace and density plots of H for the mode and curvature matching Gibbs
sampler with Gumbel linking copulas for mixed Kendall’s tau values.
. . . 177
A.34 Trace and density plots of H for expectation and variance matching Gibbs
sampler with Gumbel linking copulas for mixed Kendall’s tau values.
. . . 178
A.35 Trace and density plots of H for the independence and random walk Gibbs
sampler with Gumbel linking copulas for mixed Kendall’s tau values.
. . . 179
A.36 Trace and density plots of H for the thinned individual ARMS Gibbs
sampler with Gumbel linking copulas for mixed Kendall’s tau values.
. . . 180
A.37 Trace and density plots of V1 for the individual ARMS Gibbs sampler with
Gaussian linking copulas for mixed Kendall’s tau values.
. . . . . . . . . . 181
A.38 Trace and density plots of V1 for the individual ARMS Gibbs sampler with
survival Gumbel linking copulas for mixed Kendall’s tau values. . . . . . . 182
A.39 Trace and density plots of H for the individual ARMS Gibbs sampler with
Gaussian linking copulas for mixed Kendall’s tau values.
. . . . . . . . . . 183
A.40 Trace and density plots of H for the individual ARMS Gibbs sampler with
survival Gumbel linking copulas for mixed Kendall’s tau values. . . . . . . 184
A.41 Boxplots of the individual ARMS Gibbs sampler of V1 for the one-factor
copula model with Gumbel copulas and low Kendall’s tau values.
. . . . . 186
A.42 Boxplots of the block ARMS Gibbs sampler of V1 for the one-factor copula
model with Gumbel copulas and low Kendall’s tau values.
. . . . . . . . . 187
A.43 Boxplots of mode and curvature matching Gibbs sampler of V1 for one-
factor copula model with Gumbel copulas and low Kendall’s tau values. . . 188
A.44 Boxplots of the expectation and variance matching Gibbs sampler of V1 for
the one-factor copula model with Gumbel copulas and low Kendall’s tau
values. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 189
A.45 Boxplots of the independence and random walk Gibbs sampler of V1 for
the one-factor copula model with Gumbel copulas and low Kendall’s tau
values. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 190
A.46 Boxplots of the thinned individual ARMS Gibbs sampler of V1 for the one-
factor copula model with Gumbel copulas and low Kendall’s tau values. . . 191
A.47 Boxplots of the individual ARMS Gibbs sampler of V1 for the one-factor
copula model with Gumbel copulas and high Kendall’s tau values. . . . . . 192
A.48 Boxplots of the block ARMS Gibbs sampler of V1 for the one-factor copula
model with Gumbel copulas and high Kendall’s tau values. . . . . . . . . . 193
A.49 Boxplots of mode and curvature matching Gibbs sampler of V1 for one-
factor copula model with Gumbel copulas and high Kendall’s tau values.
. 194
A.50 Boxplots of the expectation and variance matching Gibbs sampler of V1 for
the one-factor copula model with Gumbel copulas and high Kendall’s tau
values. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 195
A.51 Boxplots of the independence and random walk Gibbs sampler of V1 for
the one-factor copula model with Gumbel copulas and high Kendall’s tau
values. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 196
282

List of Figures
A.52 Boxplots of the thinned individual ARMS Gibbs sampler of V1 for the one-
factor copula model with Gumbel copulas and high Kendall’s tau values.
. 197
A.53 Boxplots of the individual ARMS Gibbs sampler of V1 for the one-factor
copula model with Gumbel copulas and mixed Kendall’s tau values. . . . . 198
A.54 Boxplots of the block ARMS Gibbs sampler of V1 for the one-factor copula
model with Gumbel copulas and mixed Kendall’s tau values. . . . . . . . . 199
A.55 Boxplots of the mode and curvature matching Gibbs sampler of V1 for the
one-factor copula model with Gumbel copulas and mixed Kendall’s tau
values. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 200
A.56 Boxplots of the expectation and variance matching Gibbs sampler of V1 for
the one-factor copula model with Gumbel copulas and mixed Kendall’s tau
values. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 201
A.57 Boxplots of the independence and random walk Gibbs sampler of V1 for
the one-factor copula model with Gumbel copulas and mixed Kendall’s tau
values. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 202
A.58 Boxplots of the thinned individual ARMS Gibbs sampler of V1 for the one-
factor copula model with Gumbel copulas and mixed Kendall’s tau values. 203
B.1 Histograms of the in-sample ARMA(1, 1)-GARCH(1, 1) residuals trans-
formed to standard uniform data with a probability integral transform. . . 210
B.2 Scatter plots of pairwise DLM copula data in U and Z scale. . . . . . . . . 212
B.3 Contour plot of bivariate copula density in Z space, and empirical Kendall’s
tau and Spearman’s ρ values for ARMA-GARCH copula data. . . . . . . . 213
B.4 Trace and density plots of the Gibbs sampler with individual ARMS for
τ1, . . . , τ8 with Gumbel copulas for years 2005 to 2013.
. . . . . . . . . . . 232
B.5 Trace and density plots of the Gibbs sampler with individual ARMS for
τ1, . . . , τ8 with Gaussian copulas for years 2005 to 2013. . . . . . . . . . . . 233
B.6 Trace and density plots of the Gibbs sampler with individual ARMS for
τ1, . . . , τ8 with survival Gumbel copulas for years 2005 to 2013. . . . . . . . 234
B.7 Trace and density plots of the Gibbs sampler with individual ARMS for
τ1, . . . , τ8 with Gumbel copulas for year 2005. . . . . . . . . . . . . . . . . . 236
B.8 Trace and density plots of the Gibbs sampler with individual ARMS for
τ1, . . . , τ8 with Gaussian copulas for year 2005. . . . . . . . . . . . . . . . . 237
B.9 Trace and density plots of the Gibbs sampler with individual ARMS for
τ1, . . . , τ8 with survival Gumbel copulas for year 2005. . . . . . . . . . . . . 238
B.10 Trace and density plots of the Gibbs sampler with individual ARMS for
τ1, . . . , τ8 with Gumbel copulas for year 2006. . . . . . . . . . . . . . . . . . 239
B.11 Trace and density plots of the Gibbs sampler with individual ARMS for
τ1, . . . , τ8 with Gaussian copulas for year 2006. . . . . . . . . . . . . . . . . 240
B.12 Trace and density plots of the Gibbs sampler with individual ARMS for
τ1, . . . , τ8 with survival Gumbel copulas for year 2006. . . . . . . . . . . . . 241
B.13 Trace and density plots of the Gibbs sampler with individual ARMS for
τ1, . . . , τ8 with Gumbel copulas for year 2007. . . . . . . . . . . . . . . . . . 242
B.14 Trace and density plots of the Gibbs sampler with individual ARMS for
τ1, . . . , τ8 with Gaussian copulas for year 2007. . . . . . . . . . . . . . . . . 243
B.15 Trace and density plots of the Gibbs sampler with individual ARMS for
τ1, . . . , τ8 with survival Gumbel copulas for year 2007. . . . . . . . . . . . . 244
283

List of Figures
B.16 Trace and density plots of the Gibbs sampler with individual ARMS for
τ1, . . . , τ8 with Gumbel copulas for year 2008. . . . . . . . . . . . . . . . . . 245
B.17 Trace and density plots of the Gibbs sampler with individual ARMS for
τ1, . . . , τ8 with Gaussian copulas for year 2008. . . . . . . . . . . . . . . . . 246
B.18 Trace and density plots of the Gibbs sampler with individual ARMS for
τ1, . . . , τ8 with survival Gumbel copulas for year 2008. . . . . . . . . . . . . 247
B.19 Trace and density plots of the Gibbs sampler with individual ARMS for
τ1, . . . , τ8 with Gumbel copulas for year 2009. . . . . . . . . . . . . . . . . . 248
B.20 Trace and density plots of the Gibbs sampler with individual ARMS for
τ1, . . . , τ8 with Gaussian copulas for year 2009. . . . . . . . . . . . . . . . . 249
B.21 Trace and density plots of the Gibbs sampler with individual ARMS for
τ1, . . . , τ8 with survival Gumbel copulas for year 2009. . . . . . . . . . . . . 250
B.22 Trace and density plots of the Gibbs sampler with individual ARMS for
τ1, . . . , τ8 with Gumbel copulas for year 2010. . . . . . . . . . . . . . . . . . 251
B.23 Trace and density plots of the Gibbs sampler with individual ARMS for
τ1, . . . , τ8 with Gaussian copulas for year 2010. . . . . . . . . . . . . . . . . 252
B.24 Trace and density plots of the Gibbs sampler with individual ARMS for
τ1, . . . , τ8 with survival Gumbel copulas for year 2010. . . . . . . . . . . . . 253
B.25 Trace and density plots of the Gibbs sampler with individual ARMS for
τ1, . . . , τ8 with Gumbel copulas for year 2011. . . . . . . . . . . . . . . . . . 254
B.26 Trace and density plots of the Gibbs sampler with individual ARMS for
τ1, . . . , τ8 with Gaussian copulas for year 2011. . . . . . . . . . . . . . . . . 255
B.27 Trace and density plots of the Gibbs sampler with individual ARMS for
τ1, . . . , τ8 with survival Gumbel copulas for year 2011. . . . . . . . . . . . . 256
B.28 Trace and density plots of the Gibbs sampler with individual ARMS for
τ1, . . . , τ8 with Gumbel copulas for year 2012. . . . . . . . . . . . . . . . . . 257
B.29 Trace and density plots of the Gibbs sampler with individual ARMS for
τ1, . . . , τ8 with Gaussian copulas for year 2012. . . . . . . . . . . . . . . . . 258
B.30 Trace and density plots of the Gibbs sampler with individual ARMS for
τ1, . . . , τ8 with survival Gumbel copulas for year 2012. . . . . . . . . . . . . 259
B.31 Trace and density plots of the Gibbs sampler with individual ARMS for
τ1, . . . , τ8 with Gumbel copulas for year 2013. . . . . . . . . . . . . . . . . . 260
B.32 Trace and density plots of the Gibbs sampler with individual ARMS for
τ1, . . . , τ8 with Gaussian copulas for year 2013. . . . . . . . . . . . . . . . . 261
B.33 Trace and density plots of the Gibbs sampler with individual ARMS for
τ1, . . . , τ8 with survival Gumbel copulas for year 2013. . . . . . . . . . . . . 262
B.34 Traceplots of the latent variable V1 over time based on the Gibbs sampler
for the one-factor copula model with Gumbel linking copulas.
. . . . . . . 264
B.35 Traceplots of the latent variable V1 over time based on the Gibbs sampler
for the one-factor copula model with survival Gumbel linking copulas. . . . 265
B.36 Contour plots of ACA, BBVA and BNP, and posterior samples with Gum-
bel linking copulas of latent variable V1 for iterations 250, 500, 750 and
1 000 for daily log returns of years 2005 to 2013. . . . . . . . . . . . . . . . 266
B.37 Contour plots of ACA, BBVA and BNP, and posterior samples with survival
Gumbel linking copulas of latent variable V1 for iterations 250, 500, 750 and
1 000 for daily log returns of years 2005 to 2013. . . . . . . . . . . . . . . . 267
284

List of Figures
B.38 Contour plots of ACA, BBVA and BNP, and posterior samples with Gum-
bel linking copulas of latent variable V1 for iterations 250, 500, 750 and
1 000 for daily log returns of year 2005. . . . . . . . . . . . . . . . . . . . . 268
B.39 Contour plots of ACA, BBVA and BNP, and posterior samples with survival
Gumbel linking copulas of latent variable V1 for iterations 250, 500, 750 and
1 000 for daily log returns of year 2005. . . . . . . . . . . . . . . . . . . . . 269
B.40 Contour plots of eight bank stocks and the mean of the posterior samples
with Gumbel linking copulas of latent variable V1 for daily log returns of
years 2005 to 2013. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 270
B.41 Contour plots of eight bank stocks and the mean of the posterior samples
with survival Gumbel linking copulas of latent variable V1 for daily log
returns of years 2005 to 2013. . . . . . . . . . . . . . . . . . . . . . . . . . 271
B.42 Contour plots of eight bank stocks and the mean of the posterior samples
with Gumbel linking copulas of latent variable V1 for daily log returns of
year 2005. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 272
B.43 Contour plots of eight bank stocks and the mean of the posterior samples
with survival Gumbel linking copulas of latent variable V1 for daily log
returns of year 2005.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 273
285

List of Tables
List of Tables
2.1
Summary of bivariate distribution function, parameter ranges, Kendall’s
tau values, and upper and lower tail dependence for the copulas. . . . . . .
17
5.1
Model formulation, forecasting and updating formulas for univariate DLM
with constant variance discounting parameter β ∈(0, 1).
. . . . . . . . . .
58
7.1
Summary of the Bayesian setup for the one-factor copula model. . . . . . .
77
8.1
Parameters for the simulation of one-factor copula data with Gumbel link-
ing copulas for three scenarios of Kendall’s tau values (n = 200, d = 5). . .
79
8.2
Results of N = 100 repetitions of marginal MLE for data from a one-factor
copula model with Gumbel linking copulas (n = 200, d = 5). . . . . . . . .
86
8.3
Eﬀective sample sizes per minute of the output of a single run of the Gibbs
sampler based on simulated data from a one-factor copula model with Gum-
bel linking copulas (n = 200, d = 5).
. . . . . . . . . . . . . . . . . . . . .
88
8.4
Eﬀective sample sizes of the output of a single run of the Gibbs sampler
based on simulated data from a one-factor copula model with Gumbel
linking copulas with (n = 200, d = 5). . . . . . . . . . . . . . . . . . . . . .
93
8.5
Average relative runtime and acceptance rates of the output of a single run
of the Gibbs sampler based on simulated data from a one-factor copula
model with Gumbel linking copulas (n = 200, d = 5). . . . . . . . . . . . .
93
8.6
Summary of 2 000 iterations of one run of the thinned (thin=300) individual
ARMS Gibbs sampler output. . . . . . . . . . . . . . . . . . . . . . . . . .
94
8.7
Mean squared error of N = 100 repetitions of marginal MLE and posterior
modes of the Gibbs sampler. . . . . . . . . . . . . . . . . . . . . . . . . . .
96
8.8
Percentages of true values of latent variable V1 within an empirical 95%
credible interval of N = 100 repetitions of the Gibbs sampler of the one-
factor copula model with Gumbel linking copulas for three methods. . . . .
98
8.9
Parameters for the simulation of one-factor copula data with Gaussian and
survival Gumbel linking copulas for three scenarios of Kendall’s tau values
(n = 200, d = 5). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
99
9.1
Overview over included stocks and indices. . . . . . . . . . . . . . . . . . . 103
9.2
Number of daily log return observations per year. . . . . . . . . . . . . . . 103
9.3
Median log-likelihood, AIC and BIC of ARMA(1, 1)-GARCH(1, 1) model
ﬁts to the daily log return data, with normal, Student’s t, NIG and GH
distributed innovations. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 106
9.4
Estimated parameters, p-values and log-likelihoods of the ARMA(1, 1)-
GARCH(1, 1) model ﬁts to daily log return data for all indices and stocks. 107
9.5
Conﬁdence interval violations and Ljung-Box tests for the out-of-sample
ARMA(1, 1)-GARCH(1, 1) model with Student’s t distributed innovations. 109
9.6
Equations, supplied values and updating rules for time-varying ARMA-
DLM ﬁt to log return data.
. . . . . . . . . . . . . . . . . . . . . . . . . . 116
9.7
Conﬁdence interval violations and Ljung-Box tests for the time-varying
ARMA(1, 1) DLM. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 116
9.8
Abbreviations and descriptions of the vine copula models.
. . . . . . . . . 120
9.9
Number of parameters, log-likelihood, AIC, and BIC of vine structures ﬁt
on ARMA-GARCH copula data with indices.
. . . . . . . . . . . . . . . . 121
286

List of Tables
9.10 Decisions of Vuong tests at level α = 0.05 with AIC correction and BIC
correction for ARMA-GARCH copula data with indices.
. . . . . . . . . . 121
9.11 Number of occurrences of bivariate copula families in each vine copula ﬁt
for ARMA-GARCH copula data with indices.
. . . . . . . . . . . . . . . . 123
9.12 Number of parameters, log-likelihood, AIC, and BIC of vine structures ﬁt
on ARMA-GARCH copula data without indices. . . . . . . . . . . . . . . . 124
9.13 Decisions of Vuong tests at level α = 0.05 with AIC correction and BIC
correction for ARMA-GARCH copula data without indices. . . . . . . . . . 124
9.14 Number of occurrences of bivariate copula families in each vine copula ﬁt
for DLM copula data without indices. . . . . . . . . . . . . . . . . . . . . . 126
9.15 Number of parameters, log-likelihood, AIC and BIC of the ﬁts of the one-
factor copula models to ARMA-GARCH and DLM copula data. . . . . . . 127
9.16 Number of parameters, log-likelihood, AIC and BIC of the ﬁts of the two-
factor copula models for ARMA-GARCH and DLM copula data. . . . . . . 128
9.17 Comparison of RVR and cv2R and one- and two-factor copula models (t(5),
t(5,10) and BB1-Frank) with ARMA-GARCH and DLM copula data. . . . 129
9.18 Posterior mode estimates of bivariate copula parameters of 8 bank stocks
with latent variable V1, based on Gibbs sampler output of size 1 000 in
Kendall’s tau scale. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 131
9.19 Percentages of 90% VaR violations, VaR backtests, and 90% ES of daily
log return forecasts of an equally weighted constant mix portfolio based
on vine and one-factor copula model MCMC samples and DLM marginal
models.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 141
A.1 Acceptance rate, eﬀective sample size and eﬀective sample size per minute
of ﬁve methods for the Gibbs sampler with Gumbel linking copulas. . . . . 205
A.2 Acceptance rate, eﬀective sample size and eﬀective sample size per minute
of ﬁve methods for the Gibbs sampler with Gaussian and survival Gumbel
linking copulas. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 206
A.3 Statistics of posterior mode estimates of N = 100 repetitions of the Gibbs
sampler for three sampling methods for the one-factor copula model with
Gumbel linking copulas.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 208
B.1 Copula families and RVMs of RVI and RV I
R with ARMA-GARCH copula
data. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 215
B.2 Copula families and RVMs of CVI and CV I
R with ARMA-GARCH copula
data. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 216
B.3 Copula families and RVMs of rv2I and rv2I
R with ARMA-GARCH copula
data. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 217
B.4 Copula families and RVMs of cv2I and cv2I
R with ARMA-GARCH copula
data. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 218
B.5 Copula families and RVMs of RV , RV I
R, CV and CVR with ARMA-GARCH
copula data. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 219
B.6 Copula families and RVMs of rv2, rv2I
R, cv2 and cv2R with ARMA-GARCH
copula data. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 220
B.7 Copula families and RVMs of RVI and RV I
R with DLM copula data. . . . . 222
B.8 Copula families and RVMs of CVI and CV I
R with DLM copula data. . . . . 223
B.9 Copula families and RVMs of rv2I and rv2I
R with DLM copula data.
. . . 224
287

List of Tables
B.10 Copula families and RVMs of cv2I and cv2I
R with DLM copula data. . . . . 225
B.11 Copula families and RVMs of RV , RVR, CV and CVR with DLM copula
data. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 226
B.12 Copula families and RVMs of rv2, rv2R, cv2 and cv2R with DLM copula
data. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 227
B.13 Estimated parameters of the one- and two-factor copula models of ARMA-
GARCH copula data. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 229
B.14 Estimated parameters of the one- and two-factor copula models of DLM
copula data. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 230
B.15 Posterior mode estimates of based on the individual ARMS Gibbs sampler
with Gumbel linking copulas for simultaneous and individual years 2005 to
2013. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 263
B.16 Posterior mode estimates of based on the individual ARMS Gibbs sampler
with survival Gumbel linking copulas for simultaneous and individual years
2005 to 2013.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 263
B.17 Copula families and RVMs of RVR with DLM copula data for years 2005,
2006, 2007 and 2008. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 275
B.18 Copula families and RVMs of cv2R with DLM copula data for years 2005,
2006, 2007 and 2008. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 276
B.19 Copula families and RVMs of RVR with DLM copula data for years 2009,
2010, 2011 and 2012. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 277
B.20 Copula families and RVMs of cv2R with DLM copula data for years 2009,
2010, 2011 and 2012. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 278
288

List of Algorithms
List of Algorithms
4.1
General algorithm for Gibbs sampling from target density f. . . . . . . . . .
40
4.2
General algorithm for Metropolis-Hastings sampling from target density f =
π/K.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
41
4.3
Algorithm for Acceptance-Rejection Sampling from density f = π/K. . . . .
44
4.4
Algorithm for Adaptive Rejection Sampling from target density f = π/K
.
47
4.5
Algorithm for Adaptive Rejection Metropolis Sampling from target density
f = π/K. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
48
8.1
Algorithm for simulation of data from a one-factor copula model. . . . . . .
80
289

References
References
Aas, K. Modelling the dependence structure of ﬁnancial assets: A survey of four copulas,
2004.
Aas, K., Czado, C., Frigessi, A., and Bakken, H. Pair-copula constructions of multiple
dependence. Insurance: Mathematics and economics, 44(2):182–198, 2009.
Abramowitz, M. and Stegun, I. A. Handbook of Mathematical Functions: With Formulas,
Graphs, and Mathematical Tables. Courier Dover Publications, 1972.
Acerbi, C. and Szekely, B. Back-testing expected shortfall. Risk, 2014.
Akaike, H. A new look at the statistical model identiﬁcation. Automatic Control, IEEE
Transactions on, 19(6):716–723, 1974.
Analytics, R. and Weston, S. doParallel: Foreach parallel adaptor for the parallel package,
2014. URL http://CRAN.R-project.org/package=doParallel. R package version
1.0.8.
Barndorﬀ-Nielsen, O. Exponentially decreasing distributions for the logarithm of particle
size. Proceedings of the Royal Society of London. A. Mathematical and Physical Sci-
ences, 353(1674):401–419, 1977.
Basel Committee on Banking Supervision. International Convergence of Capital Meas-
urement and Capital Standards: A Revised Framework Comprehensive Version. Bank
for International Settlements, 2006. URL http://www.bis.org/publ/bcbs128.pdf.
Basel Committee on Banking Supervision. Basel III: A global regulatory framework for
more resilient banks and banking systems. Bank for International Settlements, 2011.
URL http://www.bis.org/publ/bcbs189.pdf.
Basel Committee on Banking Supervision. Fundamental review of the trading book: A
revised market risk framework. Bank for International Settlements, 2013. URL http:
//www.bis.org/publ/bcbs265.pdf.
Bedford, T. and Cooke, R. M. Probability density decomposition for conditionally de-
pendent random variables modeled by vines.
Annals of Mathematics and Artiﬁcial
intelligence, 32(1-4):245–268, 2001.
Bedford, T. and Cooke, R. M. Vines: A new graphical model for dependent random
variables. Annals of Statistics, pages 1031–1068, 2002.
Berkowitz, J. and O’Brien, J. How Accurate are Value-at-Risk Models at Commercial
Banks? The journal of ﬁnance, 57(3):1093–1111, 2002.
Besag, J., Green, P., Higdon, D., Mengersen, K., et al.
Bayesian Computation and
Stochastic Systems. Statistical Science, 10(1):3–41, 1995.
Bollerslev, T. Generalized autoregressive conditional heteroskedasticity. Journal of Eco-
nometrics, 31(3):307–327, 1986.
290

References
Box, G. E. P., Jenkins, G. M., and Reinsel, G. C. Time series analysis : forecasting and
control. Wiley series in probability and statistics. Wiley, Hoboken, NJ, 4. ed. edition,
2008.
Brechmann, E. C., Czado, C., and Aas, K. Truncated regular vines in high dimensions
with application to ﬁnancial data. Canadian Journal of Statistics, 40(1):68–85, 2012.
Chen, Y., H¨ardle, W., and Jeong, S.-O. Nonparametric risk management with generalized
hyperbolic distributions. Journal of the American Statistical Association, 103(483):910–
923, 2008.
Christoﬀersen, P. F. Elements of Financial Risk Management. Academic Press, 2011.
Clayton, D. G. A model for association in bivariate life tables and its application in
epidemiological studies of familial tendency in chronic disease incidence. Biometrika,
65(1):141–151, 1978.
Cooke, R. M. Markov and Entropy Properties of Tree- and Vine-Dependent Variables. In
Proceedings of the ASA Section of Bayesian Statistical Science, 1997.
Czado, C. Pair-copula constructions of multivariate copulas. In Copula theory and its
applications, pages 93–109. Springer, 2010.
Czado, C. and Schmidt, T. Mathematische Statistik. Springer, 2011.
Dahl, D. B.
xtable: Export tables to LaTeX or HTML, 2014.
URL http://CRAN.
R-project.org/package=xtable. R package version 1.7-4.
Davidson, R. and MacKinnon, J. G. Econometric theory and methods, volume 5. Oxford
University Press New York, 2004.
Dellaportas, P. and Smith, A. F. Bayesian inference for generalized linear and proportional
hazards models via gibbs sampling. Applied Statistics, pages 443–459, 1993.
Demarta, S. and McNeil, A. J. The t copula and related copulas. International statistical
review, 73(1):111–129, 2005.
Dißmann, J., Brechmann, E. C., Czado, C., and Kurowicka, D. Selecting and Estimating
Regular Vine Copulae and Application to Financial Returns. Computational Statistics
& Data Analysis, 59:52–69, 2013.
Durrett, R. Probability: Theory and examples, volume 3. Cambridge university press,
2010.
Eberlein, E. and Prause, K. The generalized hyperbolic model: Financial derivatives and
risk measures.
In Mathematical Finance—Bachelier Congress 2000, pages 245–267.
Springer, 2002.
Embrechts, P. Copulas: A personal view. Journal of Risk and Insurance, 76(3):639–650,
2009.
291

References
Embrechts, P., Lindskog, F., and McNeil, A. Modelling dependence with copulas and
applications to risk management. Handbook of heavy tailed distributions in ﬁnance, 8
(1):329–384, 2003.
Embrechts, P., Puccetti, G., and R¨uschendorf, L. Model uncertainty and VaR aggregation.
Journal of Banking & Finance, 37(8):2750–2764, 2013.
Embrechts, P., Puccetti, G., R¨uschendorf, L., Wang, R., and Beleraj, A. An academic
response to Basel 3.5. Risks, 2(1):25–48, 2014.
Engle, R. F. Autoregressive conditional heteroscedasticity with estimates of the variance
of United Kingdom inﬂation. Econometrica: Journal of the Econometric Society, pages
987–1007, 1982.
European Parliament and Council.
On the taking-up and pursuit of the business of
Insurance and Reinsurance (Solvency II).
Oﬃcial Journal of the European Union,
2009. URL http://eur-lex.europa.eu/legal-content/EN/TXT/PDF/?uri=CELEX:
32009L0138&from=EN.
Fisher, R. A. On the ,,Probable Error” of a Coeﬃcient of Correlation Deduced from a
Small Sample. Metron, 1:3–32, 1921.
Francq, C. and Zakoian, J.-M. GARCH models: Structure, statistical inference and ﬁn-
ancial applications. John Wiley & Sons, 2011.
Fujikoshi, Y., Ulyanov, V., and Shimizu, R. Multivariate Statistics: High-Dimensional
and Large-Sample Approximations. Wiley series in probability and statistics, 2010.
Gamerman, D. and Lopes, H. F. Markov chain Monte Carlo: stochastic simulation for
Bayesian inference. CRC Press, 2006.
Geman, S. and Geman, D. Stochastic relaxation, Gibbs distributions, and the Bayesian
restoration of images. Pattern Analysis and Machine Intelligence, IEEE Transactions
on, 6(6):721–741, 1984.
Ghalanos, A. rugarch: Univariate GARCH models, 2014. R package version 1.3-4.
Gilbert, P. and Varadhan, R. numDeriv: Accurate Numerical Derivatives, 2012. URL
http://CRAN.R-project.org/package=numDeriv. R package version 2012.9-1.
Gilks, W. R. Adaptive rejection sampling for Gibbs sampling. Applied Statistics, 41:
337–348, 1992.
Gilks, W. R., Richardson, S., and Spiegelhalter, D. J. Markov Chain Monte Carlo in
Practice. Chapman & Hall, 1995a.
Gilks, W. R., Best, N., and Tan, K. Adaptive rejection Metropolis sampling within Gibbs
sampling. Applied Statistics, 44:455–473, 1995b.
Gneiting, T. Making and evaluating point forecasts. Journal of the American Statistical
Association, 106(494):746–762, 2011.
292

References
Gruber, L. and Czado, C.
Sequential Bayesian Model Selection of Regular Vine
Copulas.
Bayesian Analysis, Advance Publication, pages 1–27, 2015.
URL http:
//projecteuclid.org/euclid.ba/1423083634.
Gut, A. An intermediate course in probability. Springer, 2009.
Harrison, J. and West, M. Bayesian forecasting and dynamic models. Springer, 1999.
Hastings, W. K. Monte Carlo sampling methods using Markov chains and their applica-
tions. Biometrika, 57(1):97–109, 1970.
Hering, C., Hofert, M., Mai, J.-F., and Scherer, M. Constructing hierarchical Archimedean
copulas with L´evy subordinators. Journal of Multivariate Analysis, 101(6):1428–1433,
2010.
Hofert, M. and Schepsmeier, U. Guidelines for Statistical Projects: Coding and Typo-
graphy, 2014.
Hua, L. and Joe, H. Tail order and intermediate tail dependence of multivariate copulas.
Journal of Multivariate Analysis, 102(10):1454–1471, 2011.
Hull, J. Options, futures, and other derivatives. Pearson, 8. ed., global ed. edition, 2012.
Izenman, A. Modern Multivariate Statistical Techniques : Regression, Classiﬁcation, and
Manifold Learning. Springer-Verlag New York, 2008.
Joe, H. Multivariate models and multivariate dependence concepts, volume 73 of Mono-
graphs on statistics and applied probability. CRC Press, 1997.
Joe, H. Tail dependence in vine copulae. Dependence Modeling: Vine Handbook, pages
165–189, 2011.
Joe, H. Dependence Modeling with Copulas. CRC Press, 2014. URL http://copula.
stat.ubc.ca/.
Joe, H., Li, H., and Nikoloulopoulos, A. K. Tail dependence functions and vine copulas.
Journal of Multivariate Analysis, 101(1):252–270, 2010.
Kl¨uppelberg, C. and Kuhn, G. Copula structure analysis. Journal of the Royal Statistical
Society: Series B (Statistical Methodology), 71(3):737–753, 2009.
Kortschak, D. and Albrecher, H.
Asymptotic results for the sum of dependent non-
identically distributed random variables. Methodology and Computing in Applied Prob-
ability, 11(3):279–306, 2009.
Krupskii, H. J. P. CopulaModel: Dependence Modeling with Copulas, 2014. R package
version 0.6.
Krupskii, P. and Joe, H. Factor Copula Models for Multivariate Data. Journal of Mul-
tivariate Analysis, 120:85–101, 2013.
293

References
Kullback, S. and Leibler, R. A. On information and suﬃciency. The Annals of Mathem-
atical Statistics, pages 79–86, 1951.
Kurowicka, D. and Cooke, R. M. Uncertainty Analysis with High Dimensional Dependence
Modelling. John Wiley & Sons, 2006.
Lee, P. M. Bayesian statistics: An introduction. John Wiley & Sons, 2012.
Lehmann, E. L. and Casella, G. Theory of point estimation, volume 31. Springer Science
& Business Media, 1998.
Li, D. X. On default correlation: A copula function approach. The Journal of Fixed
Income, 9(4):43–54, 2000.
Ling, C. Representation of associative functions. Publication Mathematicae Debrecen, 12:
189–212, 1965.
Ljung, G. M. and Box, G. E. On a measure of lack of ﬁt in time series models. Biometrika,
65(2):297–303, 1978.
Mai, J.-F. and Scherer, M. Simulating Copulas: Stochastic Models, Sampling Algorithms,
and Applications, volume 4. World Scientiﬁc, 2012.
Mai, J.-F. and Scherer, M.
Financial Engineering with Copulas Explained.
Palgrave
Macmillan, 2014.
McNeil, A. J., Frey, R., and Embrechts, P. Quantitative risk management: Concepts,
techniques, and tools. Princeton University Press, 2005.
Metropolis, N., Rosenbluth, A. W., Rosenbluth, M. N., Teller, A. H., and Teller, E.
Equation of state calculations by fast computing machines. The journal of chemical
physics, 21(6):1087–1092, 1953.
Mikosch, T. Copulas: Tales and facts. Extremes, 9(1):3–20, 2006.
Morales-N´apoles, O.
Counting vines.
Dependence Modeling: Vine Copula Handbook,
pages 189–218, 2011.
Nelsen, R. B. An Introduction to Copulas. Springer, 2006.
Neyman, J. and Pearson, E. On the Problem of the Most Eﬃcient Tests of Statistical
Hypotheses. Philosophical Transactions of the Royal Society, 231:289–337, 1933.
Nikoloulopoulos, A. K. and Joe, H. Factor copula models for item response data. Psy-
chometrika, pages 1–25, 2012.
Petris, G. and original C code by Wally Gilks, L. T. HI: Simulation from distributions
supported by nested hyperplanes, 2013. URL http://CRAN.R-project.org/package=
HI. R package version 0.4.
294

References
Plummer, M., Best, N., Cowles, K., and Vines, K. CODA: Convergence Diagnosis and
Output Analysis for MCMC. R News, 6(1):7–11, 2006. URL http://CRAN.R-project.
org/doc/Rnews/.
Prado, R. and West, M. Time series : Modeling, Computation and Inference. Texts in
statistical science. A Chapman & Hall book. CRC Press, 2010.
R Core Team. R: A Language and Environment for Statistical Computing. R Foundation
for Statistical Computing, Vienna, Austria, 2015. URL http://www.R-project.org/.
Roberts, G. O. and Sahu, S. K. Updating schemes, correlation structure, blocking and
parameterization for the Gibbs sampler. Journal of the Royal Statistical Society: Series
B (Statistical Methodology), 59(2):291–317, 1997.
Roberts, G. O., Gelman, A., Gilks, W. R., et al. Weak convergence and optimal scaling
of random walk Metropolis algorithms. The annals of applied probability, 7(1):110–120,
1997.
Roberts, G., Sahu, S., and Gilks, W. [Bayesian Computation and Stochastic Systems]:
Comment. Statistical science, pages 49–51, 1995.
R¨uschendorf, L.
On the distributional transform, Sklar’s theorem, and the empirical
copula process. Journal of Statistical Planning and Inference, 139(11):3921–3927, 2009.
Salmon, F. The formula that killed Wall Street. Signiﬁcance, 9(1):16–20, 2012.
Scarsini, M. On measures of concordance. Stochastica, 8, 1984.
Schepsmeier, U., Stoeber, J., Brechmann, E. C., and Graeler, B.
VineCopula: Stat-
istical inference of vine copulas, 2014. URL http://CRAN.R-project.org/package=
VineCopula. R package version 1.3.
Schmitz, V. Copulas and Stochastic Processes. PhD thesis, Bibliothek der RWTH Aachen,
2003.
Schwarz, G. et al. Estimating the dimension of a model. The annals of statistics, 6(2):
461–464, 1978.
Sklar, A. Fonctions de R´epartition `A N Dimensions Et Leurs Marges. Universit´e Paris
8, 1959.
Sklar, A. Random variables, distribution functions, and copulas: A personal look back-
ward and forward. Lecture notes-monograph series, pages 1–14, 1996.
Taraz, A. Diskrete Mathematik: Grundlagen und Methoden. Birkh¨auser, 2012.
Tawn, J. A. Bivariate extreme value theory: Models and estimation. Biometrika, 75(3):
397–415, 1988.
Thompson, M. B. A Comparison of Methods for Computing Autocorrelation Time. arXiv
preprint arXiv:1011.0175, 2010.
295

References
Trapletti, A. and Hornik, K. tseries: Time Series Analysis and Computational Finance,
2015. URL http://CRAN.R-project.org/package=tseries. R package version 0.10-
34.
Trautmann, H., Steuer, D., Mersmann, O., and Bornkamp, B. truncnorm: Truncated
normal distribution, 2014. URL http://CRAN.R-project.org/package=truncnorm.
R package version 1.0-7.
Vuong, Q. H. Likelihood ratio tests for model selection and non-nested hypotheses. Eco-
nometrica: Journal of the Econometric Society, pages 307–333, 1989.
Wald, A. Tests of statistical hypotheses concerning several parameters when the number
of observations is large. Transactions of the American Mathematical society, 54(3):
426–482, 1943.
Wild, P. and Gilks, W.
Algorithm AS 287: Adaptive Rejection Sampling from Log-
Concave Density Functions. Applied Statistics, 42(4):701–709, 1993.
Zeileis, A. and Grothendieck, G. zoo: S3 Infrastructure for Regular and Irregular Time
Series. Journal of Statistical Software, 14(6):1–27, 2005. URL http://www.jstatsoft.
org/v14/i06/.
296

