BenchCLAMP: A Benchmark for
Evaluating Language Models on Semantic Parsing
Subhro Roy
Sam Thomson
Tongfei Chen
Richard Shin
Adam Pauls
Jason Eisner
Benjamin Van Durme
Microsoft Semantic Machines
sminfo@microsoft.com
Abstract
We introduce BenchCLAMP, a Benchmark
to evaluate Constrained LAnguage Model
Parsing, which produces semantic outputs
based on the analysis of input text through con-
strained decoding of a prompted or ﬁne-tuned
language model. Developers of pretrained lan-
guage models currently benchmark on classi-
ﬁcation, span extraction and free-text genera-
tion tasks. Semantic parsing is neglected in
language model evaluation because of the com-
plexity of handling task-speciﬁc architectures
and representations. Recent work has shown
that generation from a prompted or ﬁne-tuned
language model can perform well at semantic
parsing when the output is constrained to be a
valid semantic representation. BenchCLAMP
includes context-free grammars for six seman-
tic parsing datasets with varied output mean-
ing representations, as well as a constrained
decoding interface to generate outputs covered
by these grammars. We provide low, medium,
and high resource splits for each dataset, allow-
ing accurate comparison of various language
models under different data regimes.
Our
benchmark supports both prompt-based learn-
ing as well as ﬁne-tuning, and provides an
easy-to-use toolkit for language model devel-
opers to evaluate on semantic parsing.
1
Introduction
Large pretrained language models when ﬁne-tuned
on target data can achieve state-of-the-art perfor-
mance on a host of NLP tasks (Liu et al., 2019;
Raffel et al., 2020; Wang et al., 2021; He et al.,
2021). Models like GPT-3 (Brown et al., 2020),
Codex (Chen et al., 2021) and T0 (Sanh et al., 2021)
have also shown impressive zero- and few-shot
performance, when prompted with task descrip-
tions and examples. Research on large language
models is commonly validated by performance on
downstream NLP tasks. Past work has evaluated
new pretrained language models on classiﬁcation,
extraction, and generation (Liu et al., 2019; He
et al., 2021). However, semantic parsing is gener-
ally not considered a testbed for such evaluation
since most state-of-the-art systems involve dataset-
speciﬁc model architectures and meaning represen-
tation constraints.
Recently, Shin et al. (2021) and Scholak et al.
(2021) have shown that standard generation from
a ﬁne-tuned or few-shot prompted language
model can perform competitively in semantic
parsing tasks, when the output of the language
model is constrained to produce valid meaning
representations. However, it is still challenging to
set up constrained generation for a new dataset
and language model due to the variation in
meaning
representations
and
model-speciﬁc
tokenization. In this paper, we introduce a new
benchmark called BenchCLAMP (Benchmark
for Constrained Language Model Parsing) that
covers six semantic parsing datasets with four
different meaning representations.
We release
context-free grammars for each dataset and provide
a toolkit to perform efﬁcient constrained decoding
to generate valid meaning representations. Our
benchmark reduces the barrier for language
model developers to evaluate on semantic parsing.
The benchmark is made available at https:
//github.com/microsoft/semantic_
parsing_with_constrained_lm.
2
Related Work
Language Models for Semantic Parsing
Re-
cent work has shown that one can generate an
analysis of a natural language sentence, such as
a semantic parse, by asking a large language model
to continue a prompt that includes the sentence
(Chen et al., 2021; Li et al., 2021; Schucher et al.,
2021). We refer to this as “language model parsing.”
To avoid ill-formed analyses, it is possible to con-
strain the generation so that the generated output
satisﬁes hard task-speciﬁc constraints. Shin et al.
(2021) showed that constrained generation from
arXiv:2206.10668v1  [cs.CL]  21 Jun 2022

Dataset
Metric
Example Representation
SMCalFlow (Andreas et al., 2020)
Lispress Match
(Yield (Event.start (FindNumNextEvent
TreeDST (Cheng et al., 2020)
(Event.subject_?
(?~= “meeting”)) 1L)))
MTOP (Li et al., 2021)
Exact Match
[IN:Get_Message [SL:Type_Content video]
[SL:Sender Atlas]]
Overnight (Wang et al., 2015)
Denotation Match
(call listValue (call getProperty
en.block.block1 (string color)))
Spider (Yu et al., 2018)
Test suite
SELECT born_state FROM head GROUP BY
born_state HAVING count(*) >= 3
CoSQL (Yu et al., 2019)
Execution
Table 1: List of datasets covered by BenchCLAMP, along with evaluation metric and an example representation.
For SMCalﬂow and TreeDST, we use the Lispress format (lisp-like serialization format for programs) of the data
released by (Platanios et al., 2021).
few-shot–prompted GPT-3 and ﬁne-tuned BART
models outperformed task-speciﬁc semantic pars-
ing architectures in low-resource settings. Scholak
et al. (2021) were able to achieve state-of-the-art
performance in SQL prediction by ﬁne-tuning a
T5-3B model (Raffel et al., 2020) and using con-
strained decoding. As the above works used dif-
ferent evaluation settings, it is hard to see which
techniques work best under different data regimes.
NLP Benchmarks
Multiple benchmarks have
been introduced to track progress on speciﬁc NLP
tasks, and to encourage multi-task learning using
diverse datasets. The GLUE (Wang et al., 2018)
and SuperGLUE (Wang et al., 2019) benchmarks
are widely used by language model developers to
evaluate model efﬁcacy. However, these bench-
marks focus on classiﬁcation and span extraction,
and do not include structured prediction tasks like
semantic parsing. Our benchmark ﬁlls this gap.
3
Datasets
Data Splits
BenchCLAMP includes six popular
semantic parsing datasets with a varied set of mean-
ing representation formalisms (details in Table 1).
For each dataset, we create the following splits:
1. We create three low-resource train splits of
500 examples, each sampled from the training
portion of the dataset. We create a single low-
resource development set of 50 examples sam-
pled from the development portion of the dataset.
We always report mean of these splits.
2. We similarly create a medium-resource train
split of 5000 examples paired with a dev set of
500 examples.
3. We consider a high-resource split with the en-
tire training set of the dataset, paired with the
medium-resource development set.
To make it feasible for researchers to evaluate
large pretrained models on BenchCLAMP, we ran-
domly sample 2000 examples from the test set of
each dataset, and evaluate test performance on this
smaller set. We use the full test set in cases where
there are less than 2000 examples. We also re-
lease a smaller randomly-sampled 100-example
test set for each dataset to evaluate models accessed
through costly API calls like GPT-3 and Codex.
Results on a 100-example test set will have wide
error bars and should be used with caution. See
Appendix D for a discussion on result variance.
We allow for the evaluation on full test sets of the
datasets to compare with state of the art results.
For datasets that do not release public test sets, like
SMCalFlow, Spider, and CoSQL, we treat the de-
velopment set as the test set, and sample 10% of the
training set and treat it as the development set for
splits creation. For datasets which include dialogue
interactions, we ensure that all turns of a dialogue
belong to the same split. The Overnight train set
was already small (< 5k examples), so we do not
have a separate medium split for it.
Grammars
We release context-free grammars
for all datasets to constrain generation to valid
meaning representations.
For SMCalFlow and
TreeDST, we use the Lispress-format datasets re-
leased by Platanios et al. (2021). We create a non-
terminal corresponding to each type present in the
training data. For each (sub-)expression with type
t, we add a production rule with the non-terminal
for t generating the non-terminals of its component
(sub-)expression types, or component terminal plan
fragments. For MTOP, we induce a simple type
system on the meaning representation, and then
follow a similar procedure to extract the grammar.

For all data splits, we use the full training data
to derive the grammar. We envision that in realis-
tic scenarios, the grammar will be provided by a
domain developer, and hence will have complete
coverage of the domain (even when some plan frag-
ments might not have appeared in the low-resource
dataset). We also add results with grammars in-
duced from low-resource splits in Appendix C.
We use a publicly available SQL grammar (antlr,
2022) for Spider and CoSQL. For each example,
we add schema-speciﬁc constraints to the grammar
to generate consistent table and column names.
4
Experimental Setup and Results
We use BenchCLAMP to ﬁne-tune and evaluate
ﬁve language models with varying number of pa-
rameters: T5-base (220M), T5-large (770M), T5-
3B (3B), BART-large (406M) and CodeT5-base
(220M). The input to our model is the utterance
concatenated with the string representation of the
context (conversation context, database schema,
etc.), and the output is the semantic parse. We
evaluate two large GPT-based language models:
GPT-3 and Codex, using few-shot prompting on
the 100 example test sets. For each input (utter-
ance concatenated with context) we select a set of
20 relevant examples from the training set using
BM25 (Rubin et al., 2021). We create a prompt
using these examples, following the template in
Shin et al. (2021) and limiting the total length of
the prompt to be 1500 tokens. This leaves room in
GPT-3’s buffer to generate an output of up to 548
tokens. Details of the input format and training are
provided in Appendices A and B.
We use the code released by Shin et al. (2021) to
support constrained generation of semantic repre-
sentations. At each step, the preﬁx generated until
that point is incrementally parsed by Earley’s algo-
rithm (Earley, 1970) to determine the set of legal
next tokens, which is used for subsequent token
generation. We extend their method to support all
autoregressive language models and sequence-to-
sequence models. Unless otherwise mentioned, we
always use constrained decoding to report metrics.
Impact of Context
The datasets in Bench-
CLAMP require a model to use a variety of con-
texts. SMCalﬂow, TreeDST and CoSQL datasets
all have conversational context. Spider and CoSQL
have database schema context which informs the
target SQL prediction. BenchCLAMP allows us
to perform a controlled investigation of the effect
Conv.
Context
SMCalFlow
TreeDST
Low
Med
High
Low
Med
High
No context
37.0
63.8
72.9
42.4
68.4
76.0
Last agent
utt.
42.6
70.7
80.6
59.6
82.7
87.9
Last user &
agent utt.
40.0
71.5
81.3
58.8
86.2
90.0
Table 2: Effect of conversational context on the perfor-
mance of ﬁne-tuned T5-large (without constraints)
Conversational
Context
DB values?
Low
Med
High
No context
no
21.3
35.9
34.4
yes
25.3
38.9
40.3
Last interaction
no
24.1
40.4
39.1
yes
28.2
44.2
44.4
All interactions
no
24.3
36.8
43.0
yes
24.9
44.9
48.8
Table 3: Performance of BART-large (without con-
straints) on the CoSQL dataset with varying context.
of context. Table 2 shows that while using the last
agent and user utterance is helpful for all settings,
the low-data regime does even better when using
only the last agent utterance; without more data,
training struggles to learn how to utilize (or ignore)
the additional context. We ﬁnd similar results for
CoSQL in Table 3. Also, SQL prediction always
beneﬁts from including database values in the con-
text along with the database schema information.
Input formats are detailed in Appendix A. The best
settings for context for each data regime is used for
all subsequent experiments.
Few-Shot Prompt Structure
In the few-shot
prompting scenario, we manipulate the context
choice and ordering of examples in our prompt
to Codex. The results in Table 4 show that order-
ing the most relevant example at the end closest
to the generation heads is helpful in the low-data
regime, indicating that GPT-3 and Codex pay more
Prompt
Order
Conv. Context
Low
Med
Random
No context
35.7
52.0
Best First
No context
34.3
53.0
Best Last
No context
36.7
52.0
Best Last
Last agent utt.
34.0
41.0
Best Last
Last user & agent utt.
26.0
31.0
Table 4: Lispress match accuracy of the Codex lan-
guage model on the SMCalﬂow dataset with different
prompt order and conversational context.

LM
SMCalﬂow
TreeDST
MTOP (en)
Overnight (blocks)
Low
Med
High
Low
Med
High
Low
Med
High
Low
High
GPT-3†
28.7
42.0
48.0
37.7
59.0
65.0
49.0
61.0
62.0
49.3
50.0
Codex†
36.7
52.0
58.0
46.3
60.0
64.0
53.0
64.0
69.0
58.0
59.0
T5-base
41.6
69.7
78.6
62.0
85.8
89.4
55.8
80.6
84.3
64.4
63.9
CodeT5-base
37.3
67.5
81.1
56.8
84.4
90.0
46.0
75.0
81.7
60.4
65.2
BART-large
42.5
71.4
83.0
61.1
86.4
89.8
59.3
81.8
84.0
61.2
63.7
T5-large
46.3
73.1
82.1
64.2
87.2
90.1
57.2
81.7
85.2
62.8
68.9
T5-3B
48.7
75.9
83.0
64.1
87.2
90.3
61.2
82.4
86.0
63.2
66.2
T5-large
42.6
71.5
81.3
59.6
86.2
90.0
53.4
81.4
85.6
62.1
68.7
unconstrained
Table 5: Performance of language models on 4 non-SQL datasets. † indicates few-shot prompted and evaluated
on the 100 example test set. Remaining LMs are ﬁnetuned and evaluated on 2k example test sets. We also show
the performance of T5-large with unconstrained decoding to illustrate the contribution of constraints. Metrics are
dataset-speciﬁc (see Table 1). The best score in each column is boldfaced.
LM
Spider
CoSQL
Low
Med
High
Low
Med
High
T5-base
33.8
56.8
58.9
26.8
42.7
43.4
CodeT5-
base
37.6
58.0
62.2
27.3
46.2
48.2
BART-
large
42.5
62.7
63.9
29.1
48.8
51.5
T5-large
44.1
65.5
66.5
32.1
52.4
55.5
T5-3B
48.6
70.3
72.3
34.7
56.4
56.2
T5-large
42.4
64.6
65.7
30.7
50.0
53.8
unconstr.
Table 6: Test suite execution accuracy of ﬁne-tuned lan-
guage models on two SQL datasets.
Dataset
Current State of the Art
Our T5-3B
SMCalﬂow
80.4 (Platanios et al., 2021)
83.7
TreeDST
88.1 (Platanios et al., 2021)
91.5
MTOP (en)
86.4 (Pasupat et al., 2021)
86.0
Overnight
65.2 (Cao et al., 2019)
66.2
(blocks)
Spider
75.5 (Scholak et al., 2021)
72.2
CoSQL
56.9 (Scholak et al., 2021)
52.3
Table 7: Comparison of our ﬁnetuned T5-3B model
with current state of the art models on full test sets. We
report exact match accuracy for Spider and CoSQL to
match the settings of previous work. The best score in
each row is boldfaced.
attention to the recent past. In higher data regime,
all prompt examples are almost equally relevant,
hence the order does not matter as much. We ﬁnd
that context does not help; one of the reasons being
that we can ﬁt fewer examples in the prompt if we
need to include context for each example.
Effect of Constraints
Tables 5 and 6 show the
effect of constraints while generating from ﬁne-
tuned T5-large model. We ﬁnd that constrained
decoding is most beneﬁcial in low-data regimes,
giving on average 2.7% gain over unconstrained
decoding. In the high-resource setting, the average
gain is less than 1%, suggesting that the full data is
nearly sufﬁcient to learn the constraint system.
Benchmarking Language Models
We show the
performance of all language models on all Bench-
CLAMP datasets in tables 5 and 6. We ﬁnd that
performance increases with model size for most
ﬁne-tuned language models. Few-shot prompting
of GPT-3 and Codex are still not on par with ﬁne-
tuned models. For non-SQL datasets, even smaller
language models reach close to the best perfor-
mance in the high resource setting. However, for
Spider and CoSQL, model size seems important in
all data settings. This is likely because the model
has to reason about the database schema to generate
SQL queries, making it a harder learning problem.
Comparison with State of the Art
Table 7 com-
pares our T5-3B model with the best-performing
models in the literature. We outperform state-of-
the-art models on the SMCalﬂow, TreeDST and
Overnight-blocks datasets. For Spider and CoSQL,
our scores are lower than the state of the art, al-
though they follow a CLAMP approach. This is
likely because we use a general SQL grammar
to constrain decoding, whereas the SQL in these
datasets covers only a small fraction of the SQL
grammar. We believe using a more constrained
grammar will improve performance.
5
Conclusion
We introduce a benchmark comprising six semantic
parsing datasets with varying meaning representa-
tions. We support few-shot prompting, ﬁne-tuning

and constraint decoding for all autoregressive lan-
guage models and sequence-to-sequence models
on these datasets. We hope that this work will
encourage language model developers to consider
semantic parsing as a test-bed in future work.
References
Jacob Andreas, John Bufe, David Burkett, Charles
Chen, Josh Clausman, Jean Crawford, Kate Crim,
Jordan DeLoach, Leah Dorner, Jason Eisner, Hao
Fang, Alan Guo, David Hall, Kristin Hayes, Kellie
Hill, Diana Ho, Wendy Iwaszuk, Smriti Jha, Dan
Klein, Jayant Krishnamurthy, Theo Lanman, Percy
Liang, Christopher H. Lin, Ilya Lintsbakh, Andy Mc-
Govern, Aleksandr Nisnevich, Adam Pauls, Dmitrij
Petters, Brent Read, Dan Roth, Subhro Roy, Jesse
Rusak,
Beth Short,
Div Slomin,
Ben Snyder,
Stephon Striplin, Yu Su, Zachary Tellman, Sam
Thomson, Andrei Vorobev, Izabela Witoszko, Jason
Wolfe, Abby Wray, Yuchen Zhang, and Alexander
Zotov. 2020.
Task-oriented dialogue as dataﬂow
synthesis. Transactions of the Association for Com-
putational Linguistics, 8:556–571.
antlr. 2022.
grammars-v4.
https://github.
com/antlr/grammars-v4.
Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell,
Sandhini Agarwal,
Ariel Herbert-Voss,
Gretchen Krueger, Tom Henighan, Rewon Child,
Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,
Clemens Winter, Christopher Hesse, Mark Chen,
Eric Sigler, Mateusz Litwin, Scott Gray, Ben-
jamin Chess, Jack Clark, Christopher Berner, Sam
McCandlish, Alec Radford, Ilya Sutskever, and
Dario Amodei. 2020.
Language models are few-
shot learners.
Computing Research Repository,
arXiv:2005.14165.
Ruisheng Cao, Su Zhu, Chen Liu, Jieyu Li, and Kai
Yu. 2019. Semantic parsing with dual learning. In
Proceedings of the 57th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 51–64,
Florence, Italy. Association for Computational Lin-
guistics.
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming
Yuan, Henrique Ponde de Oliveira Pinto, Jared
Kaplan, Harrison Edwards, Yuri Burda, Nicholas
Joseph, Greg Brockman, Alex Ray, Raul Puri,
Gretchen Krueger, Michael Petrov, Heidy Khlaaf,
Girish Sastry, Pamela Mishkin, Brooke Chan, Scott
Gray, Nick Ryder, Mikhail Pavlov, Alethea Power,
Lukasz Kaiser, Mohammad Bavarian, Clemens Win-
ter, Philippe Tillet, Felipe Petroski Such, Dave Cum-
mings, Matthias Plappert, Fotios Chantzis, Eliza-
beth Barnes, Ariel Herbert-Voss, William Hebgen
Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie
Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain,
William Saunders, Christopher Hesse, Andrew N.
Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan
Morikawa, Alec Radford, Matthew Knight, Miles
Brundage, Mira Murati, Katie Mayer, Peter Welin-
der, Bob McGrew, Dario Amodei, Sam McCandlish,
Ilya Sutskever, and Wojciech Zaremba. 2021. Evalu-
ating large language models trained on code. CoRR,
abs/2107.03374.
Jianpeng
Cheng,
Devang
Agrawal,
Héctor
Martínez Alonso, Shruti Bhargava, Joris Driesen,
Federico Flego, Dain Kaplan, Dimitri Kartsaklis,
Lin Li, Dhivya Piraviperumal, Jason D. Williams,
Hong Yu, Diarmuid Ó Séaghdha, and Anders
Johannsen. 2020. Conversational semantic parsing
for dialog state tracking.
In Proceedings of the
2020 Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 8107–8117,
Online. Association for Computational Linguistics.
Jay Earley. 1970. An efﬁcient context-free parsing al-
gorithm.
Communications of the ACM, 13(2):94–
102.
Pengcheng He, Jianfeng Gao, and Weizhu Chen. 2021.
Debertav3: Improving deberta using electra-style
pre-training with gradient-disentangled embedding
sharing. CoRR, abs/2111.09543.
Haoran Li, Abhinav Arora, Shuohui Chen, Anchit
Gupta, Sonal Gupta, and Yashar Mehdad. 2021.
MTOP: A comprehensive multilingual task-oriented
semantic parsing benchmark. In Proceedings of the
16th Conference of the European Chapter of the
Association for Computational Linguistics: Main
Volume, pages 2950–2962, Online. Association for
Computational Linguistics.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-
dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
Luke Zettlemoyer, and Veselin Stoyanov. 2019.
Roberta: A robustly optimized bert pretraining ap-
proach. arXiv preprint arXiv:1907.11692.
Panupong Pasupat, Yuan Zhang, and Kelvin Guu. 2021.
Controllable semantic parsing via retrieval augmen-
tation. In Proceedings of the 2021 Conference on
Empirical Methods in Natural Language Processing,
pages 7683–7698, Online and Punta Cana, Domini-
can Republic. Association for Computational Lin-
guistics.
Emmanouil Antonios Platanios, Adam Pauls, Subhro
Roy, Yuchen Zhang, Alexander Kyte, Alan Guo,
Sam Thomson, Jayant Krishnamurthy, Jason Wolfe,
Jacob Andreas, and Dan Klein. 2021.
Value-
agnostic conversational semantic parsing.
In Pro-
ceedings of the 59th Annual Meeting of the Associa-
tion for Computational Linguistics and the 11th In-
ternational Joint Conference on Natural Language
Processing (Volume 1: Long Papers), pages 3666–
3681, Online. Association for Computational Lin-
guistics.
Colin Raffel, Noam Shazeer, Adam Roberts, Kather-
ine Lee, Sharan Narang, Michael Matena, Yanqi

Zhou, Wei Li, and Peter J. Liu. 2020.
Exploring
the limits of transfer learning with a uniﬁed text-to-
text transformer. Journal of Machine Learning Re-
search, 21(140):1–67.
Ohad Rubin, Jonathan Herzig, and Jonathan Berant.
2021. Learning to retrieve prompts for in-context
learning. CoRR, abs/2112.08633.
Victor Sanh, Albert Webson, Colin Raffel, Stephen H.
Bach, Lintang Sutawika, Zaid Alyafeai, Antoine
Chafﬁn, Arnaud Stiegler, Teven Le Scao, Arun Raja,
Manan Dey, M Saiful Bari, Canwen Xu, Urmish
Thakker, Shanya Sharma Sharma, Eliza Szczechla,
Taewoon Kim, Gunjan Chhablani, Nihal Nayak,
Debajyoti Datta, Jonathan Chang, Mike Tian-Jian
Jiang, Han Wang, Matteo Manica, Sheng Shen,
Zheng Xin Yong, Harshit Pandey, Rachel Bawden,
Thomas Wang, Trishala Neeraj, Jos Rozen, Ab-
heesht Sharma, Andrea Santilli, Thibault Fevry, Ja-
son Alan Fries, Ryan Teehan, Stella Biderman, Leo
Gao, Tali Bers, Thomas Wolf, and Alexander M.
Rush. 2021.
Multitask prompted training enables
zero-shot task generalization.
Torsten Scholak, Nathan Schucher, and Dzmitry Bah-
danau. 2021. PICARD: Parsing incrementally for
constrained auto-regressive decoding from language
models. In Proceedings of the 2021 Conference on
Empirical Methods in Natural Language Processing,
pages 9895–9901, Online and Punta Cana, Domini-
can Republic. Association for Computational Lin-
guistics.
Nathan Schucher, Siva Reddy, and Harm de Vries.
2021. The power of prompt tuning for low-resource
semantic parsing. CoRR, abs/2110.08525.
Noam Shazeer and Mitchell Stern. 2018. Adafactor:
Adaptive learning rates with sublinear memory cost.
In Proceedings of the 35th International Conference
on Machine Learning, volume 80 of Proceedings
of Machine Learning Research, pages 4596–4604.
PMLR.
Richard Shin, Christopher Lin, Sam Thomson, Charles
Chen, Subhro Roy, Emmanouil Antonios Platanios,
Adam Pauls, Dan Klein, Jason Eisner, and Benjamin
Van Durme. 2021.
Constrained language models
yield few-shot semantic parsers. In Proceedings of
the 2021 Conference on Empirical Methods in Natu-
ral Language Processing, pages 7699–7715, Online
and Punta Cana, Dominican Republic. Association
for Computational Linguistics.
Alex Wang,
Yada Pruksachatkun,
Nikita Nangia,
Amanpreet Singh, Julian Michael, Felix Hill, Omer
Levy, and Samuel Bowman. 2019.
Superglue: A
stickier benchmark for general-purpose language un-
derstanding systems. In Advances in Neural Infor-
mation Processing Systems, volume 32. Curran As-
sociates, Inc.
Alex Wang, Amanpreet Singh, Julian Michael, Fe-
lix Hill, Omer Levy, and Samuel Bowman. 2018.
GLUE: A multi-task benchmark and analysis plat-
form for natural language understanding.
In Pro-
ceedings of the 2018 EMNLP Workshop Black-
boxNLP: Analyzing and Interpreting Neural Net-
works for NLP, pages 353–355, Brussels, Belgium.
Association for Computational Linguistics.
Yushi Wang, Jonathan Berant, and Percy Liang. 2015.
Building a semantic parser overnight. In Proceed-
ings of the 53rd Annual Meeting of the Association
for Computational Linguistics and the 7th Interna-
tional Joint Conference on Natural Language Pro-
cessing (Volume 1: Long Papers), pages 1332–1342,
Beijing, China.
Zirui Wang, Adams Wei Yu, Orhan Firat, and Yuan Cao.
2021. Towards zero-label language learning. CoRR,
abs/2109.09193.
Tao Yu, Rui Zhang, Heyang Er, Suyi Li, Eric Xue,
Bo Pang, Xi Victoria Lin, Yi Chern Tan, Tianze
Shi, Zihan Li, Youxuan Jiang, Michihiro Yasunaga,
Sungrok Shim, Tao Chen, Alexander Fabbri, Zifan
Li, Luyao Chen, Yuwen Zhang, Shreya Dixit, Vin-
cent Zhang, Caiming Xiong, Richard Socher, Wal-
ter Lasecki, and Dragomir Radev. 2019. CoSQL: A
conversational text-to-SQL challenge towards cross-
domain natural language interfaces to databases. In
Proceedings of the 2019 Conference on Empirical
Methods in Natural Language Processing and the
9th International Joint Conference on Natural Lan-
guage Processing (EMNLP-IJCNLP), pages 1962–
1979, Hong Kong, China. Association for Computa-
tional Linguistics.
Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga,
Dongxu Wang, Zifan Li, James Ma, Irene Li,
Qingning Yao,
Shanelle Roman,
Zilin Zhang,
and Dragomir Radev. 2018.
Spider:
A large-
scale human-labeled dataset for complex and cross-
domain semantic parsing and text-to-SQL task. In
Proceedings of the 2018 Conference on Empirical
Methods in Natural Language Processing, pages
3911–3921, Brussels, Belgium.

A
Format for Model Inputs
For experiments related to ﬁne-tuned language
models with SMCalﬂow and TreeDST with last
user and agent utterance as context, the input to
the model has the format l | a | u, where u is the
input natural language utterance, l is the last user
utterance, a is the last agent utterance and | is a
separator symbol. When using only last agent ut-
terance as context, the input is a | u, and for using
no context, the input to the model is simply u.
We use the the following format for Spider and
CoSQL: c , d , u, where c is any conversational con-
text if applicable, d is a rendering of the database
schema with or without values and u is the user ut-
terance. We use the database schema representation
used in (Scholak et al., 2021) for d. For c, we con-
catenate the past utterances in the conversational
context with the separator symbol |.
Our few shot prompting experiments use the
prompt template of (Shin et al., 2021). The input
to the model has the following format:
Let’s translate what a human user says into what a
computer might say.
Human:
uc1
Computer:
p1
Human:
uc2
Computer:
p2
. . .
Human:
uc
Computer:
where uc is the rendering of context and utterance
into text according to the strategy described for
ﬁnetuning experiments. For the ith prompt exam-
ple, we refer to its context and utterance rendering
as uci, and its semantic parse as pi.
B
Training Details
For ﬁne-tuning experiments, we train the language
models with batch size 32 for 10 000 steps using
AdaFactor (Shazeer and Stern, 2018), saving a
checkpoint every 5000 steps. We use 1000 linear
warmup steps and then linear decay the learning
rate to 0. We tune all models with learning rates
10−4 and 10−5, except for T5-3B for which we
only used 10−4 to save compute. The best per-
forming checkpoint on the dev set is used to report
scores on the test set.
Constraint Grammar
SMCalFlow
TreeDST
Low
Med
Low
Med
Unconstrained
42.6
71.5
59.6
86.2
Induced from train split
45.6
73.1
62.3
87.2
Induced from full train
46.3
73.1
64.2
87.2
Table 8: Effect of different grammar induction data on
the Lispress match constrained decoding accuracy of
ﬁne-tuned T5-large.
C
Grammars induced from Low
Resource Splits
The grammars released for SMCalﬂow, TreeDST
and MTOP were induced using the full train dataset.
This grammar is then used even with low and
medium resource train splits. We expect the gram-
mar will be provided by a developer of the domain
and hence will cover all valid representations. How-
ever, for the sake of completeness, we report here
the impact of using grammar induced from the cor-
responding train sets. Table 8 shows the results
with constrained decoding with train split induced
grammar, and compares the performance with un-
constrained decoding and decoding with grammar
induced from full train set. The gains from con-
straints drop by 1−2% for low resource splits when
using train split induced grammar instead of full
train induced grammar. It does not affect results
for the medium resource splits.
D
Variance of Results
All low-resource results in the main paper are a
mean of the three training data splits. Table 9 re-
ports the average standard deviation for each model
over the three low resource splits, averaged over
four datasets. We ﬁnd a high standard deviation
of GPT-3 and Codex; one of the factors being the
small size of the test set being used for evalua-
tion of prompted language models (100 examples).
Finetuned models show relatively low variance,
consistently having standard deviation lower than
2%.
E
More dataset details
We release data splits for all domains of Overnight
and all languages in MTOP. But for brevity,
we benchmark on a single domain of Overnight
(blocks) and a single language from MTOP (En-
glish). All other datasets used in the benchmark
are in English.

Model
Avg. Standard Deviation
GPT-3
4.7
Codex
3.2
T5-base
1.2
CodeT5-base
1.1
BART-large
1.4
T5-large
2.0
T5-3B
1.5
Table 9: Standard deviation of the scores for each lan-
guage model over the three low resource splits, aver-
aged over non-SQL datasets.
We did not evaluate few shot prompted GPT-3
/ Codex on SQL datasets. The input sequences
for these datasets are very long, since it has to
include an encoding of the database schema with
values. As a result, we can accommodate only a
couple examples in the prompt for GPT-3 / Codex,
which is not sufﬁcient for good few shot prompted
performance.

