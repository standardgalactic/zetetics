ReduNet: A White-box Deep Network from the Principle of
Maximizing Rate Reduction∗
Kwan Ho Ryan Chan †, ⋄
ryanchankh@berkeley.edu
Yaodong Yu †, ⋄
yyu@eecs.berkeley.edu
Chong You †, ⋄
cyou@berkeley.edu
Haozhi Qi †
hqi@berkeley.edu
John Wright ‡
johnwright@ee.columbia.edu
Yi Ma †
yima@eecs.berkeley.edu
† Department of Electrical Engineering and Computer Sciences
University of California, Berkeley, CA 94720-1776, USA
‡ Department of Electrical Engineering
Department of Applied Physics and Applied Mathematics
Columbia University, New York, NY, 10027, USA
Abstract
This work attempts to provide a plausible theoretical framework that aims to interpret
modern deep (convolutional) networks from the principles of data compression and discrimi-
native representation. We argue that for high-dimensional multi-class data, the optimal
linear discriminative representation maximizes the coding rate diﬀerence between the whole
dataset and the average of all the subsets. We show that the basic iterative gradient ascent
scheme for optimizing the rate reduction objective naturally leads to a multi-layer deep
network, named ReduNet, which shares common characteristics of modern deep networks.
The deep layered architectures, linear and nonlinear operators, and even parameters of
the network are all explicitly constructed layer-by-layer via forward propagation, although
they are amenable to ﬁne-tuning via back propagation. All components of so-obtained
“white-box” network have precise optimization, statistical, and geometric interpretation.
Moreover, all linear operators of the so-derived network naturally become multi-channel
convolutions when we enforce classiﬁcation to be rigorously shift-invariant. The derivation in
the invariant setting suggests a trade-oﬀbetween sparsity and invariance, and also indicates
that such a deep convolution network is signiﬁcantly more eﬃcient to construct and learn
in the spectral domain. Our preliminary simulations and experiments clearly verify the
eﬀectiveness of both the rate reduction objective and the associated ReduNet. All code and
data are available at https://github.com/Ma-Lab-Berkeley.
Keywords:
rate reduction, white-box deep network, linear discriminative representation,
multi-channel convolution, sparsity and invariance trade-oﬀ
“What I cannot create, I do not understand.”
— Richard Feynman
.
∗This paper integrates previous two manuscripts: https://arxiv.org/abs/2006.08558 and https://
arxiv.org/abs/2010.14765, with signiﬁcantly improved organization, presentation, and new results.
The ﬁrst manuscript appeared as a conference paper in NeurIPS 2020 (Yu et al., 2020). The second
manuscript is the new addition.
.
⋄The ﬁrst three authors contributed equally to this work.
©2021 Kwan Ho Ryan Chan, Yaodong Yu, Chong You, Haozhi Qi, John Wright, and Yi Ma.
License: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/.
arXiv:2105.10446v3  [cs.LG]  29 Nov 2021

Chan, Yu, You, Qi, Wright, and Ma
Contents
1
Introduction and Overview
4
1.1
Background and Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . .
4
1.2
Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
6
1.2.1
Objectives for Deep Learning . . . . . . . . . . . . . . . . . . . . . .
6
1.2.2
Architectures for Deep Networks . . . . . . . . . . . . . . . . . . . .
9
1.3
A Principled Objective for Discrimiative Representation via Compression
.
10
1.4
A Constructive Approach to Deep Networks via Optimization . . . . . . . .
12
2
The Principle of Maximal Coding Rate Reduction
13
2.1
Measure of Compactness for Linear Representations
. . . . . . . . . . . . .
13
2.2
Principle of Maximal Coding Rate Reduction . . . . . . . . . . . . . . . . .
15
2.3
Properties of the Rate Reduction Function . . . . . . . . . . . . . . . . . . .
17
3
Deep Networks from Maximizing Rate Reduction
19
3.1
Gradient Ascent for Rate Reduction on the Training . . . . . . . . . . . . .
20
3.2
Gradient-Guided Feature Map Increment
. . . . . . . . . . . . . . . . . . .
22
3.3
Deep Network for Optimizing Rate Reduction . . . . . . . . . . . . . . . . .
24
3.4
Comparison with Other Approaches and Architectures . . . . . . . . . . . .
26
4
Deep Convolution Networks from Invariant Rate Reduction
27
4.1
1D Serial Data and Shift Invariance
. . . . . . . . . . . . . . . . . . . . . .
28
4.2
A Fundamental Trade-oﬀbetween Invariance and Sparsity . . . . . . . . . .
29
4.3
Fast Computation in the Spectral Domain . . . . . . . . . . . . . . . . . . .
33
4.4
2D Translation Invariance . . . . . . . . . . . . . . . . . . . . . . . . . . . .
35
4.5
Overall Network Architecture and Comparison
. . . . . . . . . . . . . . . .
35
5
Experimental Veriﬁcation
38
5.1
Experimental Veriﬁcation of the MCR2 Objective . . . . . . . . . . . . . . .
38
5.2
Experimental Veriﬁcation of the ReduNet . . . . . . . . . . . . . . . . . . .
41
6
Conclusions and Discussions
44
A Properties of the Rate Reduction Function
48
A.1 Preliminaries
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
48
A.2 Lower and Upper Bounds for Coding Rate . . . . . . . . . . . . . . . . . . .
49
A.3 An Upper Bound on Coding Rate Reduction
. . . . . . . . . . . . . . . . .
51
A.4 Main Results: Properties of Maximal Coding Rate Reduction . . . . . . . .
51
A.5 Proof of Main Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
52
B ReduNet for 1D Circular Shift Invariance
59
B.1
Properties of Circulant Matrix and Circular Convolution . . . . . . . . . . .
59
B.2
Circulant Matrix and Circulant Convolution for Multi-channel Signals . . .
60
B.3
Fast Computation in Spectral Domain . . . . . . . . . . . . . . . . . . . . .
62
2

ReduNet: A White-box Deep Network from Rate Reduction
C ReduNet for 2D Circular Translation Invariance
67
C.1 Doubly Block Circulant Matrix . . . . . . . . . . . . . . . . . . . . . . . . .
68
C.2 Fast Computation in Spectral Domain . . . . . . . . . . . . . . . . . . . . .
69
D Additional Simulations and Experiments for MCR2
72
D.1 Simulations - Verifying Diversity Promoting Properties of MCR2 . . . . . .
72
D.2 Implementation Details
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
72
D.3 Additional Experimental Results . . . . . . . . . . . . . . . . . . . . . . . .
74
D.3.1
PCA Results of MCR2 Training versus Cross-Entropy Training . . .
74
D.3.2
Experimental Results of MCR2 in the Supervised Learning Setting. .
76
D.4 Comparison with Related Work on Label Noise . . . . . . . . . . . . . . . .
78
D.5 Learning from Gaussian noise corrupted data. . . . . . . . . . . . . . . . . .
78
D.6 Experimental Results of MCR2 in the Self-supervised Learning Setting . . .
79
D.6.1
Self-supervised Learning of Invariant Features . . . . . . . . . . . . .
79
D.6.2
Clustering Metrics and More Results . . . . . . . . . . . . . . . . . .
81
E Implementation Details and Additional Experiments for ReduNets
84
E.1
Visualization of Rotation and Translation on MNIST . . . . . . . . . . . . .
84
E.2
Additional Experiments on Learning Rotational Invariance on MNIST . . .
85
E.3
Additional Experiments on Learning 2D Translation Invariance on MNIST
86
E.4
Additional Experiments on Learning Mixture of Gaussians in S1 and S2
. .
87
E.5
Additional Experimental Results of ReduNet and Scattering Transform
. .
88
E.6
Equivariance of learned features using Translation-Invariant ReduNet
. . .
89
References
90
3

Chan, Yu, You, Qi, Wright, and Ma
1. Introduction and Overview
1.1 Background and Motivation
In the past decade or so, the practice of deep networks has captured people’s imagination
by its empirical successes in learning useful representations from large-scale real-world data
such as images, speech, and natural languages (LeCun et al., 2015). To a large extent, the
dramatic revival of deep networks is attributed to remarkable technological advancements in
scalable computation platforms (hardware and software) and equally importantly, ample
data, real or simulated, for training and evaluation of the networks (Krizhevsky et al., 2012).
The practice of deep networks as a “black box.”
Nevertheless, the practice of deep
networks has been shrouded with mystery from the very beginning. This is largely caused
by the fact that deep network architectures and their components are often designed based
on years of trial and error, then trained from random initialization via back propagation
(BP) (Rumelhart et al., 1986), and then deployed as a “black box”. Many of the popular
techniques and recipes for designing and training deep networks (to be surveyed in the related
work below) were developed through heuristic and empirical means, as opposed to rigorous
mathematical principles, modeling and analysis. Due to lack of clear mathematical principles
that can guide the design and optimization, numerous techniques, tricks, and even hacks
need to be added upon the above designing and training process to make deep learning “work
at all” or “work better” on real world data and tasks. Practitioners constantly face a series
of challenges for any new data and tasks: What architecture or particular components they
should use for the network? How wide or deep the network should be? Which parts of the
networks need to be trained and which can be determined in advance? Last but not the least,
after the network has been trained, learned and tested: how to interpret functions of the
operators; what are the roles and relationships among the multiple (convolution) channels;
how can they be further improved or adapted to new tasks? Besides empirical evaluation, it
is challenging to provide rigorous guarantees for certain properties of so obtained networks,
such as invariance and robustness. Recent studies have shown that popular networks in fact
are not invariant to common deformations (Azulay and Weiss, 2018; Engstrom et al., 2017);
they are prone to overﬁt noisy or even arbitrary labels (Zhang et al., 2017a); and they remain
vulnerable to adversarial attacks (Szegedy et al., 2013); or they suﬀer catastrophic forgetting
when incrementally trained to learn new classes/tasks (McCloskey and Cohen, 1989; Delange
et al., 2021; Wu et al., 2021). The lack of understanding about the roles of learned operators
in the networks often provoke debates among practitioners which architecture is better than
others, for example MLPs versus CNNs versus Transformers (Tay et al., 2021) etc. This
situation seems in desperate need of improvements if one wishes deep learning to be a science
rather than an “alchemy.” Therefore, it naturally raises a fundamental question that we
aim to address in this work: how to develop a principled mathematical framework for better
understanding and design of deep networks?
Existing attempts to interpret or understand deep networks.
To uncover the
mystery of deep networks, a plethora of recent studies on the mathematics of deep learning
has, to a large extent, improved our understanding on key aspects of deep models, including
over-ﬁtting and over-parameterization (Arora et al., 2019; Belkin et al., 2018), optimization
(Jin et al., 2017a,b; Gidel et al., 2019; Gunasekar et al., 2018); expressive power (Rolnick
4

ReduNet: A White-box Deep Network from Rate Reduction
and Tegmark, 2017; Hanin, 2017), generalization bounds (Bartlett et al., 2017; Golowich
et al., 2017) etc. Nevertheless, these eﬀorts are typically based on simpliﬁed models (e.g.
networks with only a handful layers (Zhong et al., 2017; Soltanolkotabi et al., 2018; Zhang
et al., 2019c; Mei and Montanari, 2019)); or results are conservative (e.g. generalization
bounds (Bartlett et al., 2017; Golowich et al., 2017)); or rely on simpliﬁed assumptions
(e.g. ultra-wide deep networks (Jacot et al., 2018; Du et al., 2019; Allen-Zhu et al., 2019;
Buchanan et al., 2020)); or only explain or justify one of the components or characteristics
of the network (e.g. dropout (Cavazza et al., 2018; Mianjy et al., 2018; Wei et al., 2020)).
On the other hand, the predominant methodology in practice still remains trial and error.
In fact, this is often pushed to the extreme by searching for eﬀective network structures and
training strategies through extensive random search techniques, such as Neural Architecture
Search (Zoph and Le, 2016; Baker et al., 2017), AutoML (Hutter et al., 2019), and Learning
to Learn (Andrychowicz et al., 2016).
A new theoretical framework based on data compression and representation.
Our approach deviates from the above eﬀorts in a very signiﬁcant way. Most of the existing
theoretical work view the deep networks themselves as the object for study. They try
to understand why deep networks work by examining their capabilities in ﬁtting certain
input-output relationships (for given class labels or function values). We, in this work,
however, advocate to shift the attention of study back to the data and try to understand
what deep networks should do. We start our investigation with a fundamental question:
What exactly do we try to learn from and about the data? With the objective clariﬁed,
maybe deep networks, with all their characteristics, are simply necessary means to achieve
such an objective. More speciﬁcally, in this paper, we develop a new theoretical framework
for understanding deep networks around the following two questions:
1. Objective of Representation Learning: What intrinsic structures of the data should we
learn, and how should we represent such structures? What is a principled objective
function for learning a good representation of such structures, instead of choosing
heuristically or arbitrarily?
2. Architecture of Deep Networks:
Can we justify the structures of modern deep
networks from such a principle? In particular, can the networks’ layered architecture
and operators (linear or nonlinear) all be derived from this objective, rather than
designed heuristically and evaluated empirically?
This paper will provide largely positive and constructive answers to the above questions.
We will argue that, at least in the classiﬁcation setting, a principled objective for a deep
network is to learn a low-dimensional linear discriminative representation of the data (Section
1.3). The optimality of such a representation can be evaluated by a principled measure from
(lossy) data compression, known as rate reduction (Section 2). Appropriately structured deep
networks can be naturally interpreted as optimization schemes for maximizing this measure
(Section 3 and 4). Not only does this framework oﬀer new perspectives to understand and
interpret modern deep networks, they also provide new insights that can potentially change
and improve the practice of deep networks. For instance, the resulting networks will be
entirely a “white box” and back propagation from random initialization is no longer the
only choice for training the networks (as we will verify through extensive experiments in
Section 5).
5

Chan, Yu, You, Qi, Wright, and Ma
For the rest of this introduction, we will ﬁrst provide a brief survey on existing work
that are related to the above two questions. Then we will give an overview of our approach
and contributions before we delve into the technical details in following sections.
1.2 Related Work
Given a random vector x ∈RD which is drawn from a mixture of k distributions D = {Dj}k
j=1,
one of the most fundamental problems in machine learning is how to eﬀectively and eﬃciently
learn the distribution from a ﬁnite set of i.i.d samples, say X = [x1, x2, . . . , xm] ∈RD×m.
To this end, we seek a good representation through a continuous mapping, f(x, θ) : RD →
Rn, that captures intrinsic structures of x and best facilitates subsequent tasks such as
classiﬁcation or clustering.1
1.2.1 Objectives for Deep Learning
Supervised learning via cross entropy.
To ease the task of learning D, in the popular
supervised setting, a true class label, represented as a one-hot vector yi ∈Rk, is given for
each sample xi. Extensive studies have shown that for many practical datasets (images,
audio, and natural language, etc.), the mapping from the data x to its class label y can
be eﬀectively modeled by training a deep network (Goodfellow et al., 2016), here denoted
as f(x, θ) : x 7→y with network parameters θ ∈Θ. This is typically done by minimizing
the cross-entropy loss over a training set {(xi, yi)}m
i=1, through backpropagation over the
network parameters θ:
min
θ∈Θ CE(θ, x, y) .= −E[⟨y, log[f(x, θ)]⟩] ≈−1
m
m
X
i=1
⟨yi, log[f(xi, θ)]⟩.
(1)
Despite its eﬀectiveness and enormous popularity, there are two serious limitations with this
approach: 1) It aims only to predict the labels y even if they might be mislabeled. Empirical
studies show that deep networks, used as a “black box,” can even ﬁt random labels (Zhang
et al., 2017b). 2) With such an end-to-end data ﬁtting, despite plenty of empirical eﬀorts in
trying to interpret the so-learned features (Zeiler and Fergus, 2014), it is not clear to what
extent the intermediate features learned by the network capture the intrinsic structures of the
data that make meaningful classiﬁcation possible in the ﬁrst place. Recent work of Papyan
et al. (2020); Fang et al. (2021); Zhu et al. (2021) shows that the representations learned
via the cross-entropy loss (1) exhibit a neural collapsing phenomenon,2 where within-class
variability and structural information are getting suppressed and ignored, as we will also see
in the experiments. The precise geometric and statistical properties of the learned features are
also often obscured, which leads to the lack of interpretability and subsequent performance
1. Classiﬁcation is where deep learning demonstrated the initial success that has catalyzed the explosive
interest in deep networks (Krizhevsky et al., 2012).
Although our study in this paper focuses on
classiﬁcation, we believe the ideas and principles can be naturally generalized to other settings such as
regression.
2. Essentially, once an over-parameterized network ﬁts the training data, regularization (such as weight
decay) would collapse weight components or features that are not the most relevant for ﬁtting the class
labels. Besides the most salient feature, informative and discriminative features that also help deﬁne a
class can be suppressed.
6

ReduNet: A White-box Deep Network from Rate Reduction
guarantees (e.g., generalizability, transferability, and robustness, etc.) in deep learning.
Therefore, one of the goals of this work is to address such limitations by reformulating the
objective towards learning explicitly meaningful and useful representations for the data x, in
terms of feature linearity, discriminativeness, and richness.
Minimal versus low-dimensional representations from deep learning.
Based on
strong empirical evidence that intrinsic structures of high-dimensional (imagery) data are
rather low-dimensional3, it has been long believed that the role of deep networks is to learn
certain (nonlinear) low-dimensional representations of the data, which has an advantage over
classical linear dimension reduction methods such as PCA (Hinton and Salakhutdinov, 2006).
Following this line of thought, one possible approach to interpret the role of deep networks
is to view outputs of intermediate layers of the network as selecting certain low-dimensional
latent features z = f(x, θ) ∈Rn of the data that are discriminative among multiple classes.
Learned representations z then facilitate the subsequent classiﬁcation task for predicting
the class label y by optimizing a classiﬁer g(z):
x
f(x,θ)
−−−−−−→z(θ)
g(z)
−−−−→y.
(2)
The information bottleneck (IB) formulation (Tishby and Zaslavsky, 2015) further hypoth-
esizes that the role of the network is to learn z as the minimal suﬃcient statistics for
predicting y. Formally, it seeks to maximize the mutual information I(z, y) (Cover and
Thomas, 2006) between z and y while minimizing I(x, z) between x and z:
max
θ∈Θ IB(x, y, z(θ)) .= I(z(θ), y) −βI(x, z(θ)),
β > 0.
(3)
Given one can overcome some caveats associated with this framework (Kolchinsky et al.,
2018), such as how to accurately evaluate mutual information with ﬁnitely samples of
degenerate distributions, this framework can be helpful in explaining certain behaviors
of deep networks. For example, recent work (Papyan et al., 2020) indeed shows that the
representations learned via the cross-entropy loss expose a neural collapse phenomenon.
That is, features of each class are mapped to a one-dimensional vector whereas all other
information of the class is suppressed. More discussion on neural collapse will be given
in Section 2.3. But by being task-dependent (depending on the label y) and seeking a
minimal set of most informative features for the task at hand (for predicting the label y
only), the so learned network may sacriﬁce robustness in case the labels can be corrupted or
transferrability when we want to use the features for diﬀerent tasks. To address this, our
framework uses the label y as only side information to assist learning discriminative yet
diverse (not minimal) representations; these representations optimize a diﬀerent intrinsic
objective based on the principle of rate reduction.4
Reconciling contractive and contrastive learning.
Complementary to the above
supervised discriminative approach, auto-encoding (Baldi and Hornik, 1989; Kramer, 1991;
3. For example, the digits in MNIST approximately live on a manifold with intrinsic dimension no larger
than 15 (Hein and Audibert, 2005), the images in CIFAR-10 live closely on a 35-dimensional manifold
(Spigler et al., 2019), and the images in ImageNet have intrinsic dimension of ∼40 (Pope et al., 2021).
4. As we will see in the experiments in Section 5, indeed this makes learned features much more robust to
mislabeled data.
7

Chan, Yu, You, Qi, Wright, and Ma
Hinton and Salakhutdinov, 2006) is another popular unsupervised (label-free) framework
used to learn good latent representations, which can be viewed as a nonlinear extension
to the classical PCA (Jolliﬀe, 2002). The idea is to learn a compact latent representation
z ∈Rn that adequately regenerates the original data x to certain extent, through optimizing
decoder or generator g(z, η):
x
f(x,θ)
−−−−−−→z(θ)
g(z,η)
−−−−−−→bx(θ, η).
(4)
Typically, such representations are learned in an end-to-end fashion by imposing certain
heuristics on geometric or statistical “compactness” of z, such as its dimension, energy, or
volume. For example, the contractive autoencoder (Rifai et al., 2011) penalizes local volume
expansion of learned features approximated by the Jacobian
min
θ

∂z
∂θ
 .
(5)
When the data contain complicated multi-modal low-dimensional structures, naive heuristics
or inaccurate metrics may fail to capture all internal subclass structures or to explicitly
discriminate among them for classiﬁcation or clustering purposes. For example, one con-
sequence of this is the phenomenon of mode collapsing in learning generative models for
data that have mixed multi-modal structures (Li et al., 2020). To address this, we propose a
principled rate reduction measure (on z) that promotes both the within-class compactness
and between-class discrimination of the features for data with mixed structures.
If the above contractive learning seeks to reduce the dimension of the learned representa-
tion, contrastive learning (Hadsell et al., 2006; Oord et al., 2018; He et al., 2019) seems to do
just the opposite. For data that belong to k diﬀerent classes, a randomly chosen pair (xi, xj)
is of high probability belonging to diﬀerence classes if k is large.5 Hence it is desirable that
the representation zi = f(xi, θ) of a sample xi should be highly incoherent to those zj
of other samples xj whereas coherent to feature of its transformed version τ(xi), denoted
as z(τ(xi)) for τ in certain augmentation set T in consideration. Hence it was proposed
heuristically that, to promote discriminativeness of the learned representation, one may seek
to minimize the so-called contrastive loss:
min
θ −log exp(⟨zi, z(τ(xi))⟩)
P
j̸=i exp(⟨zi, zj⟩) ,
(6)
which is small whenever the inner product ⟨zi, z(τ(xi))⟩is large and ⟨zi, zj⟩is small for
i ̸= j.
As we may see from the practice of both contractive learning and contrastive learning,
for a good representation of the given data, people have striven to achieve certain trade-oﬀ
between the compactness and discriminativeness of the representation. Contractive learning
aims to compress the features of the entire ensemble, whereas contrastive learning aims to
expand features of any pair of samples. Hence it is not entirely clear why either of these
two seemingly opposite heuristics seems to help learn good features. Could it be the case
that both mechanisms are needed but each acts on diﬀerent part of the data? As we will
5. For example, when k ≥100, a random pair is of probability 99% belonging to diﬀerent classes.
8

ReduNet: A White-box Deep Network from Rate Reduction
see, the rate reduction principle precisely reconciles the tension between these two seemingly
contradictory objectives by explicitly specifying to compress (or contract) similar features in
each class whereas to expand (or contrast) the set of all features in multiple classes.
1.2.2 Architectures for Deep Networks
The ultimate goal of any good theory for deep learning is to facilitate a better understanding
of deep networks and to design better network architectures and algorithms with performance
guarantees. So far we have surveyed many popular objective functions that promote certain
desired properties of the learned representation z = f(x, θ) of the data x. The remaining
question is how the mapping f(x, θ) should be modeled and learned.
Empirical designs of deep (convolution) neural networks.
The current popular
practice is to model the mapping with an empirically designed artiﬁcial deep neural network
and learn the parameters θ from random initialization via backpropagation (BP) (Rumelhart
et al., 1986). Starting with the AlexNet (Krizhevsky et al., 2012), the architectures of modern
deep networks continue to be empirically revised and improved. Network architectures
such as VGG (Simonyan and Zisserman, 2015), ResNet (He et al., 2016), DenseNet (Huang
et al., 2017), Recurrent CNN or LSTM (Hochreiter and Schmidhuber, 1997), and mixture of
experts (MoE) (Shazeer et al., 2017) etc. have continued to push the performance envelope.
As part of the eﬀort to improve deep networks’ performance, almost every component
of the networks has been scrutinized empirically and various revisions and improvements
have been proposed. They are not limited to the nonlinear activation functions (Maas et al.,
2013; Klambauer et al., 2017; Xu et al., 2015; Nwankpa et al., 2018; Hendrycks and Gimpel,
2016), skip connections (Ronneberger et al., 2015; He et al., 2016), normalizations (Ioﬀe
and Szegedy, 2015; Ba et al., 2016; Ulyanov et al., 2016; Wu and He, 2018; Miyato et al.,
2018), up/down sampling or pooling (Scherer et al., 2010), convolutions (LeCun et al., 1998;
Krizhevsky et al., 2012), etc. Nevertheless, almost all such modiﬁcations are developed
through years of empirical trial and error or ablation study. Some recent practices even take
to the extreme by searching for eﬀective network structures and training strategies through
extensive random search techniques, such as Neural Architecture Search (Zoph and Le, 2016;
Baker et al., 2017), AutoML (Hutter et al., 2019), and Learning to Learn (Andrychowicz
et al., 2016).
However, there has been apparent lack of direct justiﬁcation of the resulting network
architectures from the desired learning objectives, e.g. cross entropy or contrastive learning.
As a result, it is challenging if not impossible to rigorously justify why the resulting network is
the best suited for the objective, let alone to interpret the learned operators and parameters
inside. In this work, we will attempt to derive network architectures and components as
entirely a “white box” from the desired objective (say, rate reduction).
Constructive approaches to deep (convolution) networks.
For long, people have
noticed structural similarities between deep networks and iterative optimization algorithms,
especially those for solving sparse coding. Even before the revival of deep networks, Gregor
and LeCun (2010) has argued that algorithms for sparse coding, such as the FISTA algorithm
(Beck and Teboulle, 2009), can be viewed as a deep network and be trained using BP for better
coding performance, known as LISTA (learned ISTA). Later Papyan et al. (2017); Giryes
et al. (2018); Monga et al. (2019); Sun et al. (2020) have proposed similar interpretations
9

Chan, Yu, You, Qi, Wright, and Ma
of deep networks as unrolling algorithms for sparse coding in convolutional or recurrent
settings. However, it remains unclear about the role of the convolutions (dictionary) in each
layer and exactly why such low-level sparse coding is needed for the high-level classiﬁcation
task. To a large extent, this work will provide a new perspective to elucidate the role of the
sparsifying convolutions in a deep network: not only will we reveal why sparsity is needed for
ensuring invariant classiﬁcation but also the (multi-channel) convolution operators can be
explicitly derived and constructed.
Almost all of the above networks inherit architectures and initial parameters from sparse
coding algorithms, but still rely on back propagation (Rumelhart et al., 1986) to tune these
parameters. There have been eﬀorts that try to construct the network in a purely forward
fashion, without any back propagation. For example, to ensure translational invariance
for a wide range of signals, Bruna and Mallat (2013); Wiatowski and B¨olcskei (2018) have
proposed to use wavelets to construct convolution networks, known as ScatteringNets. As
a ScatteringNet is oblivious to the given data and feature selection for classiﬁcation, the
required number of convolution kernels grow exponentially with the depth. Zarka et al. (2020,
2021) have also later proposed hybrid deep networks based on scattering transform and
dictionary learning to alleviate scalability. Alternatively, Chan et al. (2015) has proposed to
construct much more compact (arguably the simplest) networks using principal components
of the input data as the convolution kernels, known as PCANets. However, in both cases of
ScatteringNets and PCANets the forward-constructed networks seek a representation of the
data that is not directly related to a speciﬁc (classiﬁcation) task. To resolve limitations of
both the ScatteringNet and the PCANet, this work shows how to construct a data-dependent
deep convolution network in a forward fashion that leads to a discriminative representation
directly beneﬁcial to the classiﬁcation task. More discussion about the relationships between
our construction and these networks will be given in Section 4.5 and Appendix E.5.
1.3 A Principled Objective for Discrimiative Representation via Compression
Whether the given data X of a mixed distribution D = {Dj}k
j=1 can be eﬀectively classiﬁed
depends on how separable (or discriminative) the component distributions Dj are (or can be
made). One popular working assumption is that the distribution of each class has relatively
low-dimensional intrinsic structures. There are several reasons why this assumption is
plausible: 1). High dimensional data are highly redundant; 2). Data that belong to the
same class should be similar and correlated to each other; 3). Typically we only care
about equivalent structures of x that are invariant to certain classes of deformation and
augmentations. Hence we may assume the distribution Dj of each class has a support on a
low-dimensional submanifold, say Mj with dimension dj ≪D, and the distribution D of x
is supported on the mixture of those submanifolds, M = ∪k
j=1Mj, in the high-dimensional
ambient space RD, as illustrated in Figure 1 left.
With the manifold assumption in mind, we want to learn a mapping z = f(x, θ) that
maps each of the submanifolds Mj ⊂RD to a linear subspace Sj ⊂Rn (see Figure 1 middle).
To do so, we require our learned representation to have the following properties, called a
linear discriminative representation (LDR):
10

ReduNet: A White-box Deep Network from Rate Reduction
f(x, θ)
RD
Rn
M
M1
M2
Mj
xi
S1
S2
Sj
zi
Figure 1: Left and Middle: The distribution D of high-dim data x ∈RD is supported on a
manifold M and its classes on low-dim submanifolds Mj, we learn a map f(x, θ) such
that zi = f(xi, θ) are on a union of maximally uncorrelated subspaces {Sj}. Right:
Cosine similarity between learned features by our method for the CIFAR10 training
dataset. Each class has 5,000 samples and their features span a subspace of over 10
dimensions (see Figure 12(c)).
1. Within-Class Compressible: Features of samples from the same class/cluster should be
relatively correlated in a sense that they belong to a low-dimensional linear subspace.6
2. Between-Class Discriminative: Features of samples from diﬀerent classes/clusters
should be highly uncorrelated and belong to diﬀerent low-dimensional linear subspaces.
3. Diverse Representation: Dimension (or variance) of features for each class/cluster
should be as large as possible as long as they stay uncorrelated from the other classes.
The third item is desired because we want the learned features to reveal all possible causes
why this class is diﬀerent from all other classes7 (see Section 2 for more detailed justiﬁcation).
Notice that the ﬁrst two items align well with the spirit of the classic linear discriminant
analysis (LDA) (Hastie et al., 2009). Here, however, although the intrinsic structures of each
class/cluster may be low-dimensional, they are by no means simply linear (or Gaussian)
in their original representation x and they need be to made linear through a nonlinear
transform z = f(x). Unlike LDA (or similarly SVM8), here we do not directly seek a
discriminant (linear) classiﬁer. Instead, we use the nonlinear transform to seek a linear
discriminative representation9 (LDR) for the data such that the subspaces that represent all
the classes are maximally incoherent. To some extent, the resulting multiple subspaces {Sj}
6. Linearity is a desirable property for many reasons. For example, in engineering, it makes interpolat-
ing/extrapolating data easy via superposition which is very useful for generative purposes. There is also
scientiﬁc evidence that the brain represents objects such as faces as a linear subspace (Chang and Tsao,
2017).
7. For instance, to tell “Apple” from “Orange”, not only do we care about the color, but also the shape and
the leaves. Interestingly, some images of computers are labeled as “Apple” too.
8. For instance, Vinyals et al. (2012) has proposed to use SVMs in a recursive fashion to build (deep)
nonlinear classiﬁers for complex data.
9. In this work, to avoid confusion, we always use the word “discriminative” to describe a representation,
and “discriminant” a classiﬁer. To some extent, one may view classiﬁcation and representation are dual
to each other. Discriminant methods (LDA or SVM) are typically more natural for two-class settings
(despite many extensions to multiple classes), whereas discriminative representations are natural for
multi-class data and help reveal the data intrinsic structures more directly.
11

Chan, Yu, You, Qi, Wright, and Ma
can be viewed as discriminative generalized principal components (Vidal et al., 2016) or, if
orthogonal, independent components (Hyv¨arinen and Oja, 2000) of the resulting features z
for the original data x. As we will see in Section 3, deep networks precisely play the role of
modeling this nonlinear transform from the data to an LDR!
For many clustering or classiﬁcation tasks (such as object detection in images), we
consider two samples as equivalent if they diﬀer by certain classes of domain deformations
or augmentations T = {τ}. Hence, we are only interested in low-dimensional structures
that are invariant to such deformations (i.e., x ∈M iﬀτ(x) ∈M for all τ ∈T ), which are
known to have sophisticated geometric and topological structures (Wakin et al., 2005) and
can be diﬃcult to learn precisely in practice even with rigorously designed CNNs (Cohen
and Welling, 2016a; Cohen et al., 2019). In our framework, this is formulated in a very
natural way: all equivariant instances are to be embedded into the same subspace, so that
the subspace itself is invariant to the transformations under consideration (see Section 4).
There are previous attempts to directly enforce subspace structures on features learned
by a deep network for supervised (Lezama et al., 2018) or unsupervised learning (Ji et al.,
2017; Zhang et al., 2018; Peng et al., 2017; Zhou et al., 2018; Zhang et al., 2019b,a; Lezama
et al., 2018). However, the self-expressive property of subspaces exploited by these work
does not enforce all the desired properties listed above as shown by Haeﬀele et al. (2021).
Recently Lezama et al. (2018) has explored a nuclear norm based geometric loss to enforce
orthogonality between classes, but does not promote diversity in the learned representations,
as we will soon see. Figure 1 right illustrates a representation learned by our method on the
CIFAR10 dataset. More details can be found in the experimental Section 5.
In this work, to learn a discriminative linear representation for intrinsic low-dimensional
structures from high-dimensional data, we propose an information-theoretic measure that
maximizes the coding rate diﬀerence between the whole dataset and the sum of each
individual class, known as rate reduction. This new objective provides a more unifying view
of above objectives such as cross-entropy, information bottleneck, contractive and contrastive
learning. We can rigorously show that when the intrinsic dimensions the submanifolds are
known and this objective is optimized, the resulting representation indeed has the desired
properties listed above.
1.4 A Constructive Approach to Deep Networks via Optimization
Despite tremendous advances made by numerous empirically designed deep networks, there
is still a lack of rigorous theoretical justiﬁcation of the need or reason for “deep layered”
architectures10 and a lack of fundamental understanding of the roles of the associated
operators, e.g. linear (multi-channel convolution) and nonlinear activation in each layer.
Although many works mentioned in Section 1.2.2 suggest the layered architectures might
be interpreted as unrolled optimization algorithms (say for sparse coding), there is lack of
explicit and direct connection between such algorithms and the objective (say, minimizing
the cross entropy for classiﬁcation). As a result, there is lack of principles for network
design: How wide or deep should the network be? What has improved about features
learned between adjacent layers? Why are multi-channel convolutions necessary for image
10. After all, at least in theory, Barron (1991) already proved back in early 90’s that a single-layer neural
network can eﬃciently approximate a very general class of functions or mappings.
12

ReduNet: A White-box Deep Network from Rate Reduction
classiﬁcation instead of separable convolution kernels11? Can the network parameters be
better initialized than purely randomly?
In this paper, we attempt to provide some answers to the above questions and oﬀer a
plausible interpretation of deep neural networks by deriving a class of deep (convolution)
networks from the perspective of learning an LDR for the data. We contend that all key
features and structures of modern deep (convolution) neural networks can be naturally
derived from optimizing the rate reduction objective, which seeks an optimal (invariant) linear
discriminative representation of the data. More speciﬁcally, the basic iterative projected
gradient ascent scheme for optimizing this objective naturally takes the form of a deep neural
network, one layer per iteration. In this framework, the width of the network assumes a
precise role as the statistical resource needed to preserve the low-dimensional (separable)
structures of the data whereas the network depth as the computational resource needed to
map the (possibly nonlinear) structures to a linear discriminative representation.
This principled approach brings a couple of nice surprises: First, architectures, operators,
and parameters of the network can be constructed explicitly layer-by-layer in a forward
propagation fashion, and all inherit precise optimization, statistical and geometric interpre-
tation. As a result, the so constructed “white-box” deep network already gives a rather
discriminative representation for the given data even without any back propagation training
(see Section 3). Nevertheless, the so-obtained network is actually amenable to be further
ﬁne-tuned by back propagation for better performance, as our experiments will show. Second,
in the case of seeking a representation rigorously invariant to shift or translation, the network
naturally lends itself to a multi-channel convolutional network (see Section 4). Moreover,
the derivation indicates such a convolutional network is computationally more eﬃcient to
construct in the spectral (Fourier) domain, analogous to how neurons in the visual cortex
encode and transit information with their spikes (Eliasmith and Anderson, 2003; Belitski
et al., 2008).
2. The Principle of Maximal Coding Rate Reduction
2.1 Measure of Compactness for Linear Representations
Although the three properties listed in Section 1.3 for linear discriminative representations
(LDRs) are all highly desirable for the latent representation z, they are by no means easy to
obtain: Are these properties compatible so that we can expect to achieve them all at once?
If so, is there a simple but principled objective that can measure the goodness of the resulting
representations in terms of all these properties? The key to these questions is to ﬁnd a
principled “measure of compactness” for the distribution of a random variable z or from
its ﬁnite samples Z. Such a measure should directly and accurately characterize intrinsic
geometric or statistical properties of the distribution, in terms of its intrinsic dimension
or volume. Unlike cross-entropy (1) or information bottleneck (3), such a measure should
11. Convolution kernels/dictionaries such as wavelets have been widely practiced to model and process 2D
signals such as images. Convolution kernels used in modern CNNs are nevertheless multi-channel, often
involving hundreds of channels altogether at each layer. Most theoretical modeling and analysis for
images have been for the 2D kernel case. The precise reason and role for multi-channel convolutions have
been elusive and equivocal. This work aims to provide a rigorous explanation.
13

Chan, Yu, You, Qi, Wright, and Ma
not depend exclusively on class labels so that it can work in all supervised, self-supervised,
semi-supervised, and unsupervised settings.
Measure of compactness from information theory.
In information theory (Cover
and Thomas, 2006), the notion of entropy H(z) is designed to be such a measure. However,
entropy is not well-deﬁned for continuous random variables with degenerate distributions.
This is unfortunately the case for data with relatively low intrinsic dimension. The same
diﬃculty resides with evaluating mutual information I(x, z) for degenerate distributions.
To alleviate this diﬃculty, another related concept in information theory, more speciﬁcally
in lossy data compression, that measures the “compactness” of a random distribution is
the so-called rate distortion (Cover and Thomas, 2006): Given a random variable z and a
prescribed precision ϵ > 0, the rate distortion R(z, ϵ) is the minimal number of binary bits
needed to encode z such that the expected decoding error is less than ϵ, i.e., the decoded bz
satisﬁes E[∥z −bz∥2] ≤ϵ. This quantity has been shown to be useful in explaining feature
selection in deep networks (MacDonald et al., 2019). However, the rate distortion of an
arbitrary high-dimensional distribution is intractable, if not impossible, to compute, except
for simple distributions such as discrete and Gaussian.12 Nevertheless, as we have discussed
in Section 1.3, our goal here is to learn a ﬁnal representation of the data as linear subspaces.
Hence we only need a measure of compactness/goodness for this class of distributions.
Fortunately, as we will explain below, the rate distortions for this class of distributions can
be accurately and easily computed, actually in closed form!
Rate distortion for ﬁnite samples on a subspace.
Another practical diﬃculty in
evaluating the rate distortion is that we normally do not know the distribution of z.
Instead, we have a ﬁnite number of samples as learned representations {zi = f(xi, θ) ∈
Rn, i = 1, . . . , m}, for the given data samples X = [x1, . . . , xm]. Fortunately, Ma et al.
(2007) provides a precise estimate on the number of binary bits needed to encode ﬁnite
samples from a subspace-like distribution. In order to encode the learned representation
Z = [z1, . . . , zm] up to a precision, say ϵ, the total number of bits needed is given by the
following expression: L(Z, ϵ) .=
  m+n
2

log det
 I +
n
mϵ2 ZZ∗
.13 This formula can be derived
either by packing ϵ-balls into the space spanned by Z as a Gaussian source or by computing
the number of bits needed to quantize the SVD of Z subject to the precision, see Ma et al.
(2007) for proofs. Therefore, the compactness of learned features as a whole can be measured
in terms of the average coding length per sample (as the sample size m is large), a.k.a. the
coding rate subject to the distortion ϵ:
R(Z, ϵ) .= 1
2 log det

I +
n
mϵ2 ZZ∗
.
(7)
See Figure 2 for an illustration.
Rate distortion of samples on a mixture of subspaces.
In general, the features Z
of multi-class data may belong to multiple low-dimensional subspaces. To evaluate the rate
distortion of such mixed data more accurately, we may partition the data Z into multiple
12. The same diﬃculties lie with the information bottleneck framework (Tishby and Zaslavsky, 2015) where
one needs to evaluate (diﬀerence of) mutual information for degenerate distributions in a high-dimensional
space (3).
13. We use superscript ∗to indicate (conjugate) transpose of a vector or a matrix
14

ReduNet: A White-box Deep Network from Rate Reduction
Figure 2: Lossy coding scheme: Given a precision ϵ, we pack the space/volume spanned by the data
Z with small balls of diameter 2ϵ. The number of balls needed to pack the space gives
the number of bits needed to record the location of each data point zi, up to the given
precision E[∥z −bz∥2] ≤ϵ.
subsets: Z = Z1 ∪Z2 ∪. . . ∪Zk, with each Zj containing samples in one low-dimensional
subspace.14 So the above coding rate (7) is accurate for each subset. For convenience, let
Π = {Πj ∈Rm×m}k
j=1 be a set of diagonal matrices whose diagonal entries encode the
membership of the m samples in the k classes. More speciﬁcally, the diagonal entry Πj(i, i)
of Πj indicates the probability of sample i belonging to subset j. Therefore Π lies in a
simplex: Ω.= {Π | Πj ≥0, Π1 + · · · + Πk = I}. Then, according to Ma et al. (2007), with
respect to this partition, the average number of bits per sample (the coding rate) is
Rc(Z, ϵ | Π) .=
k
X
j=1
tr(Πj)
2m
log det

I +
n
tr(Πj)ϵ2 ZΠjZ∗

.
(8)
When Z is given, Rc(Z, ϵ | Π) is a concave function of Π. The function log det(·) in the
above expressions has been long known as an eﬀective heuristic for rank minimization
problems, with guaranteed convergence to local minimum (Fazel et al., 2003). As it nicely
characterizes the rate distortion of Gaussian or subspace-like distributions, log det(·) can be
very eﬀective in clustering or classiﬁcation of mixed data (Ma et al., 2007; Wright et al.,
2008; Kang et al., 2015).
2.2 Principle of Maximal Coding Rate Reduction
On one hand, for learned features to be discriminative, features of diﬀerent classes/clusters
are preferred to be maximally incoherent to each other. Hence they together should span
a space of the largest possible volume (or dimension) and the coding rate of the whole
set Z should be as large as possible. On the other hand, learned features of the same
class/cluster should be highly correlated and coherent. Hence each class/cluster should only
14. By a little abuse of notation, we here use Z to denote the set of all samples from all the k classes. For
convenience, we often represent Z as a matrix whose columns are the samples.
15

Chan, Yu, You, Qi, Wright, and Ma
span a space (or subspace) of a very small volume and the coding rate should be as small as
possible. Shortly put, learned features should follow the basic rule that similarity contracts
and dissimilarity contrasts.
To be more precise, a good (linear) discriminative representation Z of X is one such
that, given a partition Π of Z, achieves a large diﬀerence between the coding rate for the
whole and that for all the subsets:
∆R(Z, Π, ϵ) .= R(Z, ϵ) −Rc(Z, ϵ | Π).
(9)
If we choose our feature mapping to be z = f(x, θ) (say modeled by a deep neural network),
the overall process of the feature representation and the resulting rate reduction w.r.t. certain
partition Π can be illustrated by the following diagram:
X
f(x, θ)
−−−−−−→Z(θ)
Π,ϵ
−−−−→∆R(Z(θ), Π, ϵ).
(10)
The role of normalization.
Note that ∆R is monotonic in the scale of the features Z.
So to make the amount of reduction comparable between diﬀerent representations, we need
to normalize the scale of the learned features, either by imposing the Frobenius norm of each
class Zj to scale with the number of features in Zj ∈Rn×mj: ∥Zj∥2
F = mj or by normalizing
each feature to be on the unit sphere: zi ∈Sn−1. This can be compared to the use of “batch
normalization” in the practice of training deep neural networks (Ioﬀe and Szegedy, 2015).15
Besides normalizing the scale, normalization could also act as a precondition mechanism that
helps accelerate gradient descent (Liu et al., 2021).16 This interpretation of normalization
becomes even more pertinent when we realize deep networks as an iterative scheme to
optimize the rate reduction objective in the next two sections. In this work, to simplify the
analysis and derivation, we adopt the simplest possible normalization schemes, by simply
enforcing each sample on a sphere or the Frobenius norm of each subset being a constant.17
Once the representations can be compared fairly, our goal becomes to learn a set of
features Z(θ) = f(X, θ) and their partition Π (if not given in advance) such that they
maximize the reduction between the coding rate of all features and that of the sum of
features w.r.t. their classes:
max
θ, Π ∆R
 Z(θ), Π, ϵ

= R(Z(θ), ϵ)−Rc(Z(θ), ϵ | Π),
s.t. ∥Zj(θ)∥2
F = mj, Π ∈Ω. (11)
We refer to this as the principle of maximal coding rate reduction (MCR2), an embodiment
of Aristotle’s famous quote: “the whole is greater than the sum of the parts.” Note that for
the clustering purpose alone, one may only care about the sign of ∆R for deciding whether
to partition the data or not, which leads to the greedy algorithm in (Ma et al., 2007). More
15. Notice that normalizing the scale of the learned representations helps ensure that the mapping of each
layer of the network is approximately isometric. As it has been shown in the work of Qi et al. (2020),
ensuring the isometric property alone is adequate to ensure good performance of deep networks, even
without the batch normalization.
16. Similar to the role that preconditioning plays in the classic conjugate gradient descent method (Shewchuk,
1994; Nocedal and Wright, 2006a).
17. In practice, to strive for better performance on speciﬁc data and tasks, many other normalization schemes
can be considered such as layer normalization (Ba et al., 2016), instance normalization (Ulyanov et al.,
2016), group normalization (Wu and He, 2018), spectral normalization (Miyato et al., 2018).
16

ReduNet: A White-box Deep Network from Rate Reduction
Figure 3: Comparison of two learned representations Z and Z′ via reduced rates: R is the number
of ϵ-balls packed in the joint distribution and Rc is the sum of the numbers for all the
subspaces (the green balls). ∆R is their diﬀerence (the number of blue balls). The MCR2
principle prefers Z (the left one).
speciﬁcally, in the context of clustering ﬁnite samples, one needs to use the more precise
measure of the coding length mentioned earlier, see (Ma et al., 2007) for more details. Here
to seek or learn the most discriminative representation, we further desire that the whole
is maximally greater than the sum of the parts. This principle is illustrated with a simple
example in Figure 3.
Relationship to information gain.
The maximal coding rate reduction can be viewed
as a generalization to information gain (IG), which aims to maximize the reduction of entropy
of a random variable, say z, with respect to an observed attribute, say π: maxπ IG(z, π) .=
H(z) −H(z | π), i.e., the mutual information between z and π (Cover and Thomas,
2006). Maximal information gain has been widely used in areas such as decision trees
(Quinlan, 1986). However, MCR2 is used diﬀerently in several ways: 1) One typical setting
of MCR2 is when the data class labels are given, i.e. Π is known, MCR2 focuses on learning
representations z(θ) rather than ﬁtting labels. 2) In traditional settings of IG, the number
of attributes in z cannot be so large and their values are discrete (typically binary). Here the
“attributes” Π represent the probability of a multi-class partition for all samples and their
values can even be continuous. 3) As mentioned before, entropy H(z) or mutual information
I(z, π) (Hjelm et al., 2018) is not well-deﬁned for degenerate continuous distributions or
hard to compute for high-dimensional distributions, whereas here the rate distortion R(z, ϵ)
is and can be accurately and eﬃciently computed for (mixed) subspaces, at least.
2.3 Properties of the Rate Reduction Function
In theory, the MCR2 principle (11) is very general and can be applied to representations Z
of any distributions with any attributes Π as long as the rates R and Rc for the distributions
can be accurately and eﬃciently evaluated. The optimal representation Z⋆and partition
Π⋆should have some interesting geometric and statistical properties. We here reveal nice
properties of the optimal representation with the special case of linear subspaces, which
have many important use cases in machine learning. When the desired representation for
17

Chan, Yu, You, Qi, Wright, and Ma
Z is multiple subspaces (or Gaussians), the rates R and Rc in (11) are given by (7) and
(8), respectively. At any optimal representation, denoted as Z⋆= Z1
⋆∪· · · ∪Zk
⋆⊂Rn, it
should achieve the maximal rate reduction. One can show that Z⋆has the following desired
properties (see Appendix A for a formal statement and detailed proofs).
Theorem 1 (Informal Statement) Suppose Z⋆= Z1
⋆∪· · · ∪Zk
⋆is the optimal solution
that maximizes the rate reduction (11) with the rates R and Rc given by (7) and (8). Assume
that the optimal solution satisﬁes rank(Zj
⋆) ≤dj.18 We have:
• Between-class Discriminative: As long as the ambient space is adequately large (n ≥
Pk
j=1 dj), the subspaces are all orthogonal to each other, i.e. (Zi
⋆)∗Zj
⋆= 0 for i ̸= j.
• Maximally Diverse Representation: As long as the coding precision is adequately
high, i.e., ϵ4 < minj
nmj
m
n2
d2
j
o
, each subspace achieves its maximal dimension, i.e.
rank(Zj
⋆) = dj. In addition, the largest dj −1 singular values of Zj
⋆are equal.
In other words, the MCR2 principle promotes embedding of data into multiple independent
subspaces,19 with features distributed isotropically in each subspace (except for possibly
one dimension). In addition, among all such discriminative representations, it prefers the
one with the highest dimensions in the ambient space (see Section 5.1 and Appendix D for
experimental veriﬁcation). This is substantially diﬀerent from objectives such as the cross
entropy (1) and information bottleneck (3). The optimal representation associated with
MCR2 is indeed an LDR according to deﬁnition given in Section 1.3.
Relation to neural collapse.
A line of recent work Papyan et al. (2020); Han et al.
(2021); Mixon et al. (2020); Zhu et al. (2021) discovered, both empirically and theoretically,
that deep networks trained via cross-entropy or mean squared losses produce neural collapse
features.
That is, features from each class become identical, and diﬀerent classes are
maximally separated from each other. In terms of the coding rate function, the neural
collapse solution is preferred by the objective of minimizing the coding rate of the “parts”,
namely Rc(Z(θ), ϵ | Π) (see Table 4 of the Appendix D). However, the neural collapse
solution leads to a small coding rate for the “whole”, namely R(Z(θ)), hence is not an
optimal solution for maximizing the rate reduction. Therefore, the beneﬁt of MCR2 in
preventing the collapsing of the features from each class and producing maximally diverse
representations can be attributed to introducing and maximizing the term R(Z(θ)).
Comparison to the geometric OLE loss.
To encourage the learned features to be
uncorrelated between classes, the work of Lezama et al. (2018) has proposed to maximize the
diﬀerence between the nuclear norm of the whole Z and its subsets Zj, called the orthogonal
18. Notice that here we assume we know a good upper bound for the dimension dj of each class. This requires
us to know the intrinsic dimension of the submanifold Mj. In general, this can be a very challenging
problem itself even when the submanifold is linear but noisy, which is still an active research topic
(Hong et al., 2020). Nevertheless, in practice, we can decide the dimension empirically through ablation
experiments, see for example Table 5 for experiments on the CIFAR10 dataset.
19. In this case, the subspaces can be viewed as the independent components (Hyv¨arinen and Oja, 2000) of
the so learned features. However, when the condition n ≥Pk
j=1 dj is violated, the optimal subspaces may
not be orthogonal. But experiments show that they tend to be maximally incoherent, see Appendix E.4.
18

ReduNet: A White-box Deep Network from Rate Reduction
low-rank embedding (OLE) loss: maxθ OLE(Z(θ), Π) .= ∥Z(θ)∥∗−Pk
j=1 ∥Zj(θ)∥∗, added
as a regularizer to the cross-entropy loss (1). The nuclear norm ∥· ∥∗is a nonsmooth convex
surrogate for low-rankness and the nonsmoothness potentially poses additional diﬃculties in
using this loss to learn features via gradient descent, whereas log det(·) is smooth concave
instead. Unlike the rate reduction ∆R, OLE is always negative and achieves the maximal
value 0 when the subspaces are orthogonal, regardless of their dimensions. So in contrast to
∆R, this loss serves as a geometric heuristic and does not promote diverse representations.
In fact, OLE typically promotes learning one-dimensional representations per class, whereas
MCR2 encourages learning subspaces with maximal dimensions (Figure 7 of Lezama et al.
(2018) versus our Figure 17). More importantly, as we will see in the next section, the precise
form of the rate distortion plays a crucial role in deriving the deep network operators, with
precise statistical and geometrical meaning.
Relation to contractive or contrastive learning.
If samples are evenly drawn from
k classes, a randomly chosen pair (xi, xj) is of high probability belonging to diﬀerent
classes if k is large. For example, when k ≥100, a random pair is of probability 99%
belonging to diﬀerent classes. We may view the learned features of two samples together
with their augmentations Zi and Zj as two classes. Then the rate reduction ∆Rij =
R(Zi ∪Zj, ϵ)−1
2(R(Zi, ϵ)+R(Zj, ϵ)) gives a “distance” measure for how far the two sample
sets are. We may try to further “expand” pairs that likely belong to diﬀerent classes. From
Theorem 1, the (averaged) rate reduction ∆Rij is maximized when features from diﬀerent
samples are uncorrelated (Zi)∗Zj = 0 (see Figure 3) and features Zi from augmentations
of the same sample are compressed into the same subspace. Hence, when applied to sample
pairs, MCR2 naturally conducts the so-called contrastive learning (Hadsell et al., 2006; Oord
et al., 2018; He et al., 2019) and contractive learning (Rifai et al., 2011) together that we
have discussed in the introduction Section 1.2.1. But MCR2 is not limited to expand or
compress pairs of samples and can uniformly conduct “contrastive/contractive learning” for
a subset with any number of samples as long as we know they likely belong to diﬀerent (or
the same) classes, say by randomly sampling subsets from a large number of classes or with
membership derived from a good clustering method.
3. Deep Networks from Maximizing Rate Reduction
In the above section, we have presented rate reduction (11) as a principled objective for
learning a linear discriminative representation (LDR) for the data. We have, however,
not speciﬁed the architecture of the feature mapping z(θ) = f(x, θ) for extracting such a
representation from input data x. A straightforward choice is to use a conventional deep
network, such as ResNet, for implementing f(x, θ). As we show in the experiments (see
Section 5.1), we can eﬀectively optimize the MCR2 objective with a ResNet architecture
and obtain discriminative and diverse representations for real image data sets.
There remain several unanswered problems with using a ResNet. Although the learned
feature representation is now more interpretable, the network itself is still not. It is unclear
why any chosen “black-box” network is able to optimize the desired MCR2 objective at all.
The good empirical results (say with a ResNet) do not necessarily justify the particular choice
19

Chan, Yu, You, Qi, Wright, and Ma
in architectures and operators of the network: Why is a deep layered model necessary;20
what do additional layers try to improve or simplify; how wide and deep is adequate; or
is there any rigorous justiﬁcation for the convolutions (in the popular multi-channel form)
and nonlinear operators (e.g. ReLu or softmax) used? In this section, we show that using
gradient ascent to maximize the rate reduction ∆R(Z) naturally leads to a “white-box” deep
network that represents such a mapping. All network layered architecture, linear/nonlinear
operators, and parameters are explicitly constructed in a purely forward propagation fashion.
3.1 Gradient Ascent for Rate Reduction on the Training
From the previous section, we see that mathematically, we are essentially seeking a continuous
mapping f(·) : x 7→z from the data X = [x1, . . . , xm] ∈RD×m (or initial features extracted
from the data21) to an optimal representation Z = [z1, . . . , zm] ⊂Rn×m that maximizes the
following coding rate reduction objective:
∆R(Z, Π, ϵ) = R(Z, ϵ) −Rc(Z, ϵ | Π)
.= 1
2 log det

I + αZZ∗
|
{z
}
R(Z,ϵ)
−
k
X
j=1
γj
2 log det

I + αjZΠjZ∗
|
{z
}
Rc(Z,ϵ|Π)
,
(12)
where for simplicity we denote α =
n
mϵ2 , αj =
n
tr(Πj)ϵ2 , γj = tr(Πj)
m
for j = 1, . . . , k.
The question really boils down to whether there is a constructive way of ﬁnding such a
continuous mapping f(·) from x to z? To this end, let us consider incrementally maximizing
the objective ∆R(Z) as a function of Z ⊂Sn−1. Although there might be many optimization
schemes to choose from, for simplicity we ﬁrst consider the arguably simplest projected
gradient ascent (PGA) scheme:22
Zℓ+1 ∝Zℓ+ η · ∂∆R
∂Z

Zℓ
s.t.
Zℓ+1 ⊂Sn−1, ℓ= 1, 2, . . . ,
(13)
for some step size η > 0 and the iterate starts with the given data Z1 = X23. This scheme
can be interpreted as how one should incrementally adjust locations of the current features
Zℓ, initialized as the input data X, in order for the resulting Zℓ+1 to improve the rate
reduction ∆R(Z), as illustrated in Figure 4.
20. Especially it is already long known that even a single layer neural network is already a universal functional
approximator with tractable model complexity (Barron, 1991).
21. As we will see the necessity of such a feature extraction in the next section.
22. Notice that we use superscript j on Zj to indicate features in the jth class and subscript ℓon Zℓto
indicate all features at ℓ-th iteration or layer.
23. Again, for simplicity, we here ﬁrst assume the initial features Z1 are the data themselves. Hence the data
and the features have the same dimension n. This needs not to be the case though. As we will see in
the next section, the initial features can be some (lifted) features of the data to begin with and could in
principle have a diﬀerent (much higher) dimension. All subsequent iterates have the same dimension.
20

ReduNet: A White-box Deep Network from Rate Reduction
Figure 4: Incremental deformation via gradient ﬂow to both ﬂatten data of each class
into a subspace and push diﬀerent classes apart. Notice that for points whose
memberships are unknown, those marked as “⋄”, their gradient cannot be directly
calculated.
Simple calculation shows that the gradient ∂∆R
∂Z entails evaluating the following derivatives
of the two terms in ∆R(Z):
1
2
∂log det(I+αZZ∗)
∂Z

Zℓ
= α(I+αZℓZ∗
ℓ)−1
|
{z
}
Eℓ∈Rn×n
Zℓ,
(14)
1
2
∂
 γj log det(I + αjZΠjZ∗)

∂Z

Zℓ
= γj αj(I + αjZℓΠjZ∗
ℓ)−1
|
{z
}
Cj
ℓ∈Rn×n
ZℓΠj.
(15)
Notice that in the above, the matrix Eℓonly depends on Zℓand it aims to expand all the
features to increase the overall coding rate; the matrix Cj
ℓdepends on features from each
class and aims to compress them to reduce the coding rate of each class. Then the complete
gradient ∂∆R
∂Z

Zℓ∈Rn×m is of the form:
∂∆R
∂Z

Zℓ
=
Eℓ
|{z}
Expansion
Zℓ−
k
X
j=1
γj
Cj
ℓ
|{z}
Compression
ZℓΠj.
(16)
Remark 2 (Interpretation of Eℓand Cj
ℓas Linear Operators) For any zℓ∈Rn,
Eℓzℓ= α(zℓ−Zℓ[qℓ]⋆)
where
[qℓ]⋆.= argmin
qℓ
α∥zℓ−Zℓqℓ∥2
2 + ∥qℓ∥2
2.
(17)
Notice that [qℓ]⋆is exactly the solution to the ridge regression by all the data points Zℓ
concerned. Therefore, Eℓ(similarly for Cj
ℓ) is approximately (i.e. when m is large enough)
the projection onto the orthogonal complement of the subspace spanned by columns of Zℓ.
21

Chan, Yu, You, Qi, Wright, and Ma
Figure 5: Interpretation of Cj
ℓand Eℓ: Cj
ℓcompresses each class by contracting the features to a
low-dimensional subspace; Eℓexpands all features by contrasting and repelling features
across diﬀerent classes.
Another way to interpret the matrix Eℓis through eigenvalue decomposition of the covariance
matrix ZℓZ∗
ℓ. Assuming that ZℓZ∗
ℓ
.= UℓΛℓU ∗
ℓwhere Λℓ.= diag{λ1
ℓ, . . . , λn
ℓ}, we have
Eℓ= α Uℓdiag

1
1 + αλ1
ℓ
, . . . ,
1
1 + αλn
ℓ

U ∗
ℓ.
(18)
Therefore, the matrix Eℓoperates on a vector zℓby stretching in a way that directions of
large variance are shrunk while directions of vanishing variance are kept. These are exactly
the directions (14) in which we move the features so that the overall volume expands and
the coding rate will increase, hence the positive sign. To the opposite eﬀect, the directions
associated with (15) are “residuals” of features of each class deviate from the subspace to
which they are supposed to belong. These are exactly the directions in which the features need
to be compressed back onto their respective subspace, hence the negative sign (see Figure 5).
Essentially, the linear operations Eℓand Cj
ℓin gradient ascend for rate reduction are
determined by training data conducting “auto-regressions”. The recent renewed understanding
about ridge regression in an over-parameterized setting (Yang et al., 2020; Wu and Xu, 2020)
indicates that using seemingly redundantly sampled data (from each subspaces) as regressors
do not lead to overﬁtting.
3.2 Gradient-Guided Feature Map Increment
Notice that in the above, the gradient ascent considers all the features Zℓ= [z1
ℓ, . . . , zm
ℓ] as
free variables. The increment Zℓ+1 −Zℓ= η ∂∆R
∂Z

Zℓdoes not yet give a transform on the
entire feature domain zℓ∈Rn. According to equation (16), the gradient cannot be evaluated
at a point whose membership is not known, as illustrated in Figure 4. Hence, in order to ﬁnd
the optimal f(x, θ) explicitly, we may consider constructing a small increment transform
g(·, θℓ) on the ℓ-th layer feature zℓto emulate the above (projected) gradient scheme:
zℓ+1 ∝zℓ+ η · g(zℓ, θℓ)
subject to
zℓ+1 ∈Sn−1
(19)
such that:

g(z1
ℓ, θℓ), . . . , g(zm
ℓ, θℓ)

≈∂∆R
∂Z

Zℓ. That is, we need to approximate the gradient
ﬂow ∂∆R
∂Z that locally deforms all (training) features {zi
ℓ}m
i=1 with a continuous mapping g(z)
22

ReduNet: A White-box Deep Network from Rate Reduction
deﬁned on the entire feature space zℓ∈Rn. Notice that one may interpret the increment
(19) as a discretized version of a continuous diﬀerential equation:
˙z = g(z, θ).
(20)
Hence the (deep) network so constructed can be interpreted as certain neural ODE (Chen
et al., 2018). Nevertheless, unlike neural ODE where the ﬂow g is chosen to be some generic
structures, here our g(z, θ) is to emulate the gradient ﬂow of the rate reduction on the
feature set (as shown in Figure 4):
˙Z = ∂∆R
∂Z ,
(21)
and its structure is entirely derived and fully determined from this objective, without any
other priors or heuristics.
By inspecting the structure of the gradient (16), it suggests that a natural candidate for
the increment transform g(zℓ, θℓ) is of the form:
g(zℓ, θℓ)
.= Eℓzℓ−
k
X
j=1
γjCj
ℓzℓπj(zℓ)
∈Rn,
(22)
where πj(zℓ) ∈[0, 1] indicates the probability of zℓbelonging to the j-th class. The increment
depends on: First, a set of linear maps represented by Eℓand {Cj
ℓ}k
j=1 that depend only on
statistics of features of the training Zℓ; Second, membership {πj(zℓ)}k
j=1 of any feature zℓ.
Notice that on the training samples Zℓ, for which the memberships Πj are known, the so
deﬁned g(zℓ, θ) gives exactly the values for the gradient ∂∆R
∂Z

Zℓ.
Since we only have the membership πj for the training samples, the function g(·) deﬁned
in (22) can only be evaluated on the training. To extrapolate g(·) to the entire feature space,
we need to estimate πj(zℓ) in its second term. In the conventional deep learning, this map
is typically modeled as a deep network and learned from the training data, say via back
propagation. Nevertheless, our goal here is not to learn a precise classiﬁer πj(zℓ) already.
Instead, we only need a good enough estimate of the class information in order for g(·) to
approximate the gradient ∂∆R
∂Z well.
From the geometric interpretation of the linear maps Eℓand Cj
ℓgiven by Remark 2, the
term pj
ℓ
.= Cj
ℓzℓcan be viewed as (approximately) the projection of zℓonto the orthogonal
complement of each class j. Therefore, ∥pj
ℓ∥2 is small if zℓis in class j and large otherwise.
This motivates us to estimate its membership based on the following softmax function:
bπj(zℓ) .=
exp (−λ∥Cj
ℓzℓ∥)
Pk
j=1 exp (−λ∥Cj
ℓzℓ∥)
∈[0, 1].
(23)
Hence the second term of (22) can be approximated by this estimated membership:
k
X
j=1
γjCj
ℓzℓπj(zℓ) ≈
k
X
j=1
γjCj
ℓzℓ· bπj(zℓ)
.= σ

[C1
ℓzℓ, . . . , Ck
ℓzℓ]

,
(24)
23

Chan, Yu, You, Qi, Wright, and Ma
which is denoted as a nonlinear operator σ(·) on outputs of the feature zℓthrough k groups
of ﬁlters: [C1
ℓ, . . . , Ck
ℓ]. Notice that the nonlinearality arises due to a “soft” assignment of
class membership based on the feature responses from those ﬁlters.
Remark 3 (Approximate Membership with a ReLU Network) The choice of the
softmax is mostly for its simplicity as it is widely used in other (forward components of) deep
networks for purposes such as clustering, gating (Shazeer et al., 2017) and routing (Sabour
et al., 2017). In practice, there are many other simpler nonlinear activation functions that
one can use to approximate the membership bπ(·) and subsequently the nonlinear operation
σ in (24). Notice that the geometric meaning of σ in (24) is to compute the “residual” of
each feature against the subspace to which it belongs. There are many diﬀerent ways one
may approximate this quantity. For example, when we restrict all our features to be in the
ﬁrst (positive) quadrant of the feature space,24 one may approximate this residual using the
rectiﬁed linear units operation, ReLUs, on pj = Cj
ℓzℓor its orthogonal complement:
σ(zℓ) ∝zℓ−
k
X
j=1
ReLU
 P j
ℓzℓ

,
(25)
where P j
ℓ= (Cj
ℓ)⊥is (approximately) the projection onto the j-th class and ReLU(x) =
max(0, x). The above approximation is good under the more restrictive assumption that
projection of zℓon the correct class via P j
ℓis mostly large and positive and yet small or
negative for other classes.
Overall, combining (19), (22), and (24), the increment feature transform from zℓto zℓ+1
now becomes:
zℓ+1 ∝zℓ+ η · Eℓzℓ−η · σ
 [C1
ℓzℓ, . . . , Ck
ℓzℓ]

= zℓ+ η · g(zℓ, θℓ)
s.t.
zℓ+1 ∈Sn−1,
(26)
with the nonlinear function σ(·) deﬁned above and θℓcollecting all the layer-wise parameters.
That is θℓ=

Eℓ, C1
ℓ, . . . , Ck
ℓ, γj, λ
	
. Note features at each layer are always “normalized”
by projecting onto the unit sphere Sn−1, denoted as PSn−1. The form of increment in (26)
can be illustrated by a diagram in Figure 6 left.
3.3 Deep Network for Optimizing Rate Reduction
Notice that the increment is constructed to emulate the gradient ascent for the rate reduction
∆R. Hence by transforming the features iteratively via the above process, we expect the
rate reduction to increase, as we will see in the experimental section. This iterative process,
once converged say after L iterations, gives the desired feature map f(x, θ) on the input
z1 = x, precisely in the form of a deep network, in which each layer has the structure shown
in Figure 6 left:
f(x, θ) = φL ◦φL−1 ◦· · · ◦φ2 ◦φ1(z1),
φℓ(zℓ, θℓ)
.= zℓ+1 = PSn−1[zℓ+ η · g(zℓ, θℓ)],
g(zℓ, θℓ) = Eℓzℓ−σ
 [C1
ℓzℓ, . . . , Ck
ℓzℓ]

.
(27)
24. Most current neural networks seem to adopt this regime.
24

ReduNet: A White-box Deep Network from Rate Reduction
(a) ReduNet
256, 1x1, 64
256-d in
64, 3x3, 64
64, 1x1, 256
256-d out
256, 1x1, 4
256-d in
4, 3x3, 4
4, 1x1, 256
256-d out
256, 1x1, 4
4, 3x3, 4
4, 1x1, 256
256, 1x1, 4
4, 3x3, 4
4, 1x1, 256
32 path in total
. . .
(b) ResNet and ResNeXt.
Figure 6: Network Architectures of the ReduNet and comparison with others. (a): Layer structure
of the ReduNet derived from one iteration of gradient ascent for optimizing rate reduction.
(b) (left): A layer of ResNet (He et al., 2016); and (b) (right): A layer of ResNeXt (Xie
et al., 2017a). As we will see in Section 4, the linear operators Eℓand Cj
ℓof the ReduNet
naturally become (multi-channel) convolutions when shift-invariance is imposed.
Algorithm 1 Training Algorithm for ReduNet
Input: X = [x1, . . . , xm] ∈RD×m, Π, ϵ > 0, feature dimension n, λ, and a learning rate η.
1: Set α = n/(mϵ2), {αj = n/(tr
 Πj
ϵ2)}k
j=1, {γj = tr
 Πj
/m}k
j=1.
2: Set Z1 .= [z1
1, . . . , zm
1 ] = X ∈Rn×m (assuming n = D for simplicity).
3: for ℓ= 1, 2, . . . , L do
4:
# Step 1:Compute network parameters Eℓand {Cj
ℓ}k
j=1.
5:
Eℓ.= α(I+αZℓZ∗
ℓ)−1 ∈Rn×n,
{Cj
ℓ
.= αj(I + αjZℓΠjZ∗
ℓ)−1 ∈Rn×n}k
j=1;
6:
# Step 2:Update feature Zℓ.
7:
for i = 1, . . . , m do
8:
# Compute soft assignment {bπj(zi
ℓ)}k
j=1.
9:

bπj(zi
ℓ) .=
exp (−λ∥Cj
ℓzi
ℓ∥)
Pk
j=1 exp (−λ∥Cj
ℓzi
ℓ∥) ∈[0, 1]
k
j=1
;
10:
# Update feature zi
ℓ.
11:
zi
ℓ+1 = PSn−1

zi
ℓ+ η

Eℓzi
ℓ−Pk
j=1 γjCj
ℓzi
ℓ· bπj(zi
ℓ)

∈Rn;
12:
end for
13: end for
Output: features ZL+1, the learned parameters {Eℓ}L
ℓ=1 and {Cj
ℓ}k,L
j=1,ℓ=1, {γj}k
j=1.
As this deep network is derived from maximizing the rate reduced, we call it the ReduNet.
We summarize the training and evaluation of ReduNet in Algorithm 1 and Algorithm 2,
respectively. Notice that all parameters of the network are explicitly constructed layer by
layer in a forward propagation fashion. The construction does not need any back propagation!
The so learned features can be directly used for classiﬁcation, say via a nearest subspace
classiﬁer.
25

Chan, Yu, You, Qi, Wright, and Ma
Algorithm 2 Evaluation Algorithm for ReduNet
Input: x ∈RD, network parameters {Eℓ}L
ℓ=1 and {Cj
ℓ}k,L
j=1,ℓ=1, {γj}k
j=1, feature dimension
n, λ, and a learning rate η.
1: Set z1 = x ∈Rn (assuming n = D for simplicity).
2: for ℓ= 1, 2, . . . , L do
3:
# Compute soft assignment {bπj(zℓ)}k
j=1.
4:

bπj(zℓ) .=
exp (−λ∥Cj
ℓzℓ∥)
Pk
j=1 exp (−λ∥Cj
ℓzℓ∥) ∈[0, 1]
k
j=1
;
5:
# Update feature zℓ.
6:
zℓ+1 = PSn−1

zℓ+ η

Eℓzℓ−Pk
j=1 γjCj
ℓzℓ· bπj(zℓ)

∈Rn;
7: end for
Output: feature zL+1
3.4 Comparison with Other Approaches and Architectures
Like all networks that are inspired by unfolding certain iterative optimization schemes, the
structure of the ReduNet naturally contains a skip connection between adjacent layers as
in the ResNet (He et al., 2016) (see Figure 6 middle). Empirically, people have found that
additional skip connections across multiple layers may improve the network performance, e.g.
the Highway networks (Srivastava et al., 2015) and DenseNet (Huang et al., 2017). In our
framework, the role of each layer is precisely interpreted as one iterative gradient ascent step
for the objective function ∆R. In our experiments (see Section 5), we have observed that the
basic gradient scheme sometimes converges slowly, resulting in deep networks with hundreds
of layers (iterations)! To improve the eﬃciency of the basic ReduNet, one may consider in
the future accelerated gradient methods such as the Nesterov acceleration (Nesterov, 1983)
or perturbed accelerated gradient descent (Jin et al., 2018). Say to minimize or maximize a
function h(z), such accelerated methods usually take the form:
 qℓ+1
=
zℓ+ βℓ· (zℓ−zℓ−1),
zℓ+1
=
qℓ+1 + η · ∇h(qℓ+1).
(28)
They require introducing additional skip connections among three layers ℓ−1, ℓand ℓ+ 1.
For typical convex or nonconvex programs, the above accelerated schemes can often reduce
the number of iterations by a magnitude (Wright and Ma, 2021).
Notice that, structure wise, the k + 1 parallel groups of channels E, Cj of the ReduNet
correspond to the “residual” channel of the ResNet (Figure 6 middle). Remarkably, here
in ReduNet, we know they precisely correspond to the residual of data auto-regression (see
Remark 2). Moreover, the multiple parallel groups actually draw resemblance to the parallel
structures that people later empirically found to further improve the ResNet, e.g. ResNeXt
(Xie et al., 2017a) (shown in Figure 6 right) or the mixture of experts (MoE) module adopted
in Shazeer et al. (2017).25 Now in ReduNet, each of those groups Cj can be precisely
interpreted as an expert classiﬁer for each class of objects. But a major diﬀerence here is
25. The latest large language model, the switched transformer (Fedus et al., 2021), adopts the MoE architecture,
in which the number of parallel banks (or experts) k are in the thousands and the total number of
parameters of the network is about 1.7 trillion.
26

ReduNet: A White-box Deep Network from Rate Reduction
that all above networks need to be initialized randomly and trained via back propagation
whereas all components (layers, operators, and parameters) of the ReduNet are by explicit
construction in a forward propagation. They all have precise optimization, statistical and
geometric interpretation.
Of course, like any other deep networks, the so-constructed ReduNet is amenable to
ﬁne-tuning via back-propagation if needed. A recent study from Giryes et al. (2018) has
shown that such ﬁne-tuning may achieve a better trade oﬀbetween accuracy and eﬃciency
of the unrolled network (say when only a limited number of layers, or iterations are allowed
in practice). Nevertheless, for the ReduNet, one can start with the nominal values obtained
from the forward construction, instead of random initialization. Beneﬁts of ﬁne-tuning and
initialization will be veriﬁed in the experimental section (see Table 2).
4. Deep Convolution Networks from Invariant Rate Reduction
So far, we have considered the data x and their features z as vectors. In many applications,
such as serial data or imagery data, the semantic meaning (labels) of the data are invariant
to certain transformations g ∈G (for some group G) (Cohen and Welling, 2016b; Zaheer
et al., 2017). For example, the meaning of an audio signal is invariant to shift in time; and
the identity of an object in an image is invariant to translation in the image plane. Hence,
we prefer the feature mapping f(x, θ) is rigorously invariant to such transformations:
Group Invariance: f(x ◦g, θ) ∼f(x, θ),
∀g ∈G,
(29)
where “∼” indicates two features belonging to the same equivalent class. Although to
ensure invariance or equivarience, convolutional operators has been common practice in
deep networks (Cohen and Welling, 2016b), it remains challenging in practice to train
an (empirically designed) convolution network from scratch that can guarantee invariance
even to simple transformations such as translation and rotation (Azulay and Weiss, 2018;
Engstrom et al., 2017). An alternative approach is to carefully design convolution ﬁlters
of each layer so as to ensure translational invariance for a wide range of signals, say using
wavelets as in ScatteringNet (Bruna and Mallat, 2013) and followup works (Wiatowski and
B¨olcskei, 2018). However, in order to ensure invariance to generic signals, the number of
convolutions needed usually grows exponentially with network depth. That is the reason
why this type of network cannot be constructed so deep, usually only several layers.
In this section, we show that the MCR2 principle is compatible with invariance in a very
natural and precise way: we only need to assign all transformed versions {x ◦g | g ∈G}
into the same class as the data x and map their features z all to the same subspace S.
Hence, all group equivariant information is encoded only inside the subspace, and any
classiﬁer deﬁned on the resulting set of subspaces will be automatically invariant to such
group transformations. See Figure 7 for an illustration of the examples of 1D rotation and
2D translation. We will rigorously show in the next two sections (as well as Appendix B
and Appendix C) that, when the group G is circular 1D shifting or 2D translation, the
resulting deep network naturally becomes a multi-channel convolution network! Because
the so-constructed network only needs to ensure invariance for the given data X or their
features Z, the number of convolutions needed actually remain constant through a very
deep network, as oppose to the ScatteringNet.
27

Chan, Yu, You, Qi, Wright, and Ma
Figure 7: Illustration of the sought representation that is equivariant/invariant to image rotation
(left) or translation (right): all transformed images of each class are mapped into the
same subspace that are incoherent to other subspaces. The features embedded in each
subspace are equivariant to transformation group whereas each subspace is invariant to
such transformations.
4.1 1D Serial Data and Shift Invariance
To classify one-dimensional data x = [x(0), x(1), . . . , x(D −1)] ∈RD invariant under
shifting, we take G to be the group of all circular shifts. Each observation xi generates
a family {xi ◦g | g ∈G} of shifted copies, which are the columns of the circulant matrix
circ(xi) ∈RD×D given by
circ(x) .=


x(0)
x(D −1)
. . .
x(2)
x(1)
x(1)
x(0)
x(D −1)
· · ·
x(2)
...
x(1)
x(0)
...
...
x(D −2)
...
...
...
x(D −1)
x(D −1)
x(D −2)
. . .
x(1)
x(0)


∈RD×D.
(30)
We refer the reader to Appendix B.1 or Kra and Simanca (2012) for properties of circulant
matrices. For simplicity, let Z1 .= [z1
1, . . . , zm
1 ] = X ∈Rn×m26. Then what happens if we
construct the ReduNet from their circulant families circ(Z1) =

circ(z1
1), . . . , circ(zm
1 )

∈
Rn×nm? That is, we want to compress and map all these into the same subspace by the
ReduNet.
Notice that now the data covariance matrix:
circ(Z1)circ(Z1)∗
=

circ(z1), . . . , circ(zm)
 
circ(z1), . . . , circ(zm)
∗
(31)
=
m
X
i=1
circ(zi
1)circ(zi
1)∗∈Rn×n
(32)
26. Again, to simplify discussion, we assume for now that the initial features Z1 are X themselves hence
have the same dimension n. But that does not need to be the case as we will soon see that we need to
lift X to a higher dimension.
28

ReduNet: A White-box Deep Network from Rate Reduction
associated with this family of samples is automatically a (symmetric) circulant matrix.
Moreover, because the circulant property is preserved under sums, inverses, and products,
the matrices E1 and Cj
1 are also automatically circulant matrices, whose application to a
feature vector z ∈Rn can be implemented using circular convolution “⊛”. Speciﬁcally, we
have the following proposition.
Proposition 4 (Convolution structures of E1 and Cj
1) The matrix
E1 = α
 I + αcirc(Z1)circ(Z1)∗−1
(33)
is a circulant matrix and represents a circular convolution:
E1z = e1 ⊛z,
where e1 ∈Rn is the ﬁrst column vector of E1 and “⊛” is circular convolution deﬁned as
(e1 ⊛z)i .=
n−1
X
j=0
e1(j)x(i + n −j mod n).
Similarly, the matrices Cj
1 associated with any subsets of Z1 are also circular convolutions.
Not only the ﬁrst-layer parameters E1 and Cj
1 of the ReduNet become circulant convo-
lutions but also the next layer features remain circulant matrices. That is, the incremental
feature transform in (26) applied to all shifted versions of a z1 ∈Rn, given by
circ(z1) + η · E1circ(z1) −η · σ

[C1
1circ(z1), . . . , Ck
1 circ(z1)]

,
(34)
is a circulant matrix. This implies that there is no need to construct circulant families from
the second layer features as we did for the ﬁrst layer. By denoting
z2 ∝z1 + η · g(z1, θ1) = z1 + η · e1 ⊛z1 −η · σ

[c1
1 ⊛z1, . . . , ck
1 ⊛z1]

,
(35)
the features at the next level can be written as
circ(Z2) =

circ(z1
2), . . . , circ(zm
2 )

=

circ(z1
1 + ηg(z1
1, θ1)), . . . , circ(zm
1 + ηg(zm
1 , θ1))

.
Continuing inductively, we see that all matrices Eℓand Cj
ℓbased on such circ(Zℓ) are
circulant, and so are all features. By virtue of the properties of the data, ReduNet has taken
the form of a convolutional network, with no need to explicitly choose this structure!
4.2 A Fundamental Trade-oﬀbetween Invariance and Sparsity
There is one problem though: In general, the set of all circular permutations of a vector z
gives a full-rank matrix. That is, the n “augmented” features associated with each sample
(hence each class) typically already span the entire space Rn. For instance, all shifted
versions of a delta function δ(n) can generate any other signal as their (dense) weighted
superposition. The MCR2 objective (11) will not be able to distinguish classes as diﬀerent
subspaces.
29

Chan, Yu, You, Qi, Wright, and Ma
One natural remedy is to improve the separability of the data by “lifting” the original
signal to a higher dimensional space, e.g., by taking their responses to multiple, ﬁlters
k1, . . . , kC ∈Rn:
z[c] = kc ⊛x = circ(kc)x
∈Rn,
c = 1, . . . , C.
(36)
The ﬁlters can be pre-designed invariance-promoting ﬁlters,27 or adaptively learned from
the data,28 or randomly selected as we do in our experiments. This operation lifts each
original signal x ∈Rn to a C-channel feature, denoted as ¯z .= [z[1], . . . , z[C]]∗∈RC×n.
Then, we may construct the ReduNet on vector representations of ¯z, denoted as vec(¯z) .=
[z[1]∗, . . . , z[C]∗] ∈RnC. The associated circulant version circ(¯z) and its data covariance
matrix, denoted as ¯Σ(¯z), for all its shifted versions are given as:
circ(¯z) .=
" circ(z[1])
...
circ(z[C])
#
∈RnC×n,
¯Σ(¯z) .=
" circ(z[1])
...
circ(z[C])
#
[ circ(z[1])∗,...,circ(z[C])∗] ∈RnC×nC, (37)
where circ(z[c]) ∈Rn×n with c ∈[C] is the circulant version of the c-th channel of the feature
¯z. Then the columns of circ(¯z) will only span at most an n-dimensional proper subspace in
RnC.
However, this simple lifting operation (if linear) is not suﬃcient to render the classes
separable yet—features associated with other classes will span the same n-dimensional
subspace. This reﬂects a fundamental conﬂict between invariance and linear (subspace)
modeling: one cannot hope for arbitrarily shifted and superposed signals to belong to the
same class.
One way of resolving this conﬂict is to leverage additional structure within each class,
in the form of sparsity: Signals within each class are not generated as arbitrary linear
superposition of some base atoms (or motifs), but only sparse combinations of them and
their shifted versions, as shown in Figure 8. More precisely, let Dj = [dj
1, . . . , dj
c] denote a
matrix with a collection of atoms associated for class j, also known as a dictionary, then
each signal x in this class is sparsely generated as:
x = dj
1 ⊛z1 + . . . + dj
c ⊛zc = circ(Dj)z,
(38)
for some sparse vector z.
Signals in diﬀerent classes are then generated by diﬀerent
dictionaries whose atoms (or motifs) are incoherent from one another. Due to incoherence,
signals in one class are unlikely to be sparsely represented by atoms in any other class.
Hence all signals in the k class can be represented as
x =

circ(D1), circ(D2), . . . , circ(Dk)
¯z,
(39)
where ¯z is sparse.29 There is a vast literature on how to learn the most compact and optimal
sparsifying dictionaries from sample data, e.g. (Li and Bresler, 2019; Qu et al., 2019) and
27. For 1D signals like audio, one may consider the conventional short time Fourier transform (STFT); for
2D images, one may consider 2D wavelets as in the ScatteringNet (Bruna and Mallat, 2013).
28. For learned ﬁlters, one can learn ﬁlters as the principal components of samples as in the PCANet (Chan
et al., 2015) or from convolution dictionary learning (Li and Bresler, 2019; Qu et al., 2019).
29. Notice that similar sparse representation models have long been proposed and used for classiﬁcation
purposes in applications such a face recognition, demonstrating excellent eﬀectiveness (Wright et al.,
2009; Wagner et al., 2012). Recently, the convolution sparse coding model has been proposed by Papyan
et al. (2017) as a framework for interpreting the structures of deep convolution networks.
30

ReduNet: A White-box Deep Network from Rate Reduction
Fish
C channels
sparse
Duck
C channels
sparse
=
=
C convolution kernels
C convolution kernels
Figure 8: Each input signal x (an image here) can be represented as a superposition of
sparse convolutions with multiple kernels dc in a dictionary D.
subsequently solve the inverse problem and compute the associated sparse code z or ¯z.
Recent studies of Qu et al. (2020a,b) even show under broad conditions the convolution
dictionary learning problem can be solved eﬀectively and eﬃciently.
Nevertheless, for tasks such as classiﬁcation, we are not necessarily interested in the
precise optimal dictionary nor the precise sparse code for each individual signal. We are
mainly interested if collectively the set of sparse codes for each class are adequately separable
from those of other classes. Under the assumption of the sparse generative model, if the
convolution kernels {kc}C
c=1 match well with the “transpose” or “inverse” of the above
sparsifying dictionaries D = [D1, . . . , Dk], also known as the analysis ﬁlters (Nam et al.,
2013; Rubinstein and Elad, 2014), signals in one class will only have high responses to a
small subset of those ﬁlters and low responses to others (due to the incoherence assumption).
Nevertheless, in practice, often a suﬃcient number of, say C, random ﬁlters {kc}C
c=1 suﬃce
the purpose of ensuring so extracted C-channel features:

k1 ⊛x, k2 ⊛x, . . . , kC ⊛x
∗=

circ(k1)x, . . . , circ(kC)x
∗∈RC×n
(40)
for diﬀerent classes have diﬀerent response patterns to diﬀerent ﬁlters hence make diﬀerent
classes separable (Chan et al., 2015).
Therefore, in our framework, to a large extent the number of channels (or the width of
the network) truly plays the role as the statistical resource whereas the number of layers
(the depth of the network) plays the role as the computational resource. The theory of
compressive sensing precisely characterizes how many measurements are needed in order to
preserve the intrinsic low-dimensional structures (including separability) of the data (Wright
31

Chan, Yu, You, Qi, Wright, and Ma
Lifting & Thresholding
Fish
C channels
sparse
Duck
Lifting & Thresholding
C channels
sparse
C convolution kernels
C convolution kernels
Figure 9: Estimate the sparse code ¯z of an input signal x (an image here) by taking
convolutions with multiple kernels kc and then sparsifying.
and Ma, 2021). As optimal sparse coding is not the focus of this paper, we will use the
simple random ﬁlter design in our experiments, which is adequate to verify the concept.30
The multi-channel responses ¯z should be sparse. So to approximate the sparse code ¯z,
we may take an entry-wise sparsity-promoting nonlinear thresholding, say τ(·), on the above
ﬁlter outputs by setting low (say absolute value below ϵ) or negative responses to be zero:
¯z .= τ
 
circ(k1)x, . . . , circ(kC)x
∗
∈RC×n.
(41)
Figure 9 illustrates the basic ideas. One may refer to Rubinstein and Elad (2014) for a more
systematical study on the design of the sparsifying thresholding operator. Nevertheless,
here we are not so interested in obtaining the best sparse codes as long as the codes are
suﬃciently separable. Hence the nonlinear operator τ can be simply chosen to be a soft
thresholding or a ReLU. These presumably sparse features ¯z can be assumed to lie on a
lower-dimensional (nonlinear) submanifold of RnC, which can be linearized and separated
from the other classes by subsequent ReduNet layers, as illustrated later in Figure 11.
The ReduNet constructed from circulant version of these multi-channel features ¯Z .=
[¯z1, . . . , ¯zm] ∈RC×n×m, i.e., circ( ¯Z) .= [circ(¯z1), . . . , circ(¯zm)] ∈RnC×nm, retains the good
invariance properties described above: the linear operators, now denoted as ¯E and ¯Cj,
remain block circulant, and represent multi-channel 1D circular convolutions.
Speciﬁcally,
we have the following result (see Appendix B.2 for a proof).
Proposition 5 (Multi-channel convolution structures of ¯E and ¯Cj) The matrix
¯E .= α
 I + αcirc( ¯Z)circ( ¯Z)∗−1
(42)
30. Although better learned or designed sparsifying dictionaries and sparse coding schemes may surely lead
to better classiﬁcation performance, at a higher computational cost.
32

ReduNet: A White-box Deep Network from Rate Reduction
is block circulant, i.e.,
¯E =


¯
E1,1
···
¯
E1,C
...
...
...
¯
EC,1 ···
¯
EC,C

∈RnC×nC,
where each ¯Ec,c′ ∈Rn×n is a circulant matrix. Moreover, ¯E represents a multi-channel
circular convolution, i.e., for any multi-channel signal ¯z ∈RC×n we have
¯E · vec(¯z) = vec(¯e ⊛¯z).
In above, ¯e ∈RC×C×n is a multi-channel convolutional kernel with ¯e[c, c′] ∈Rn being the
ﬁrst column vector of ¯Ec,c′, and ¯e ⊛¯z ∈RC×n is the multi-channel circular convolution
deﬁned as
(¯e ⊛¯z)[c] .=
C
X
c′=1
¯e[c, c′] ⊛¯z[c′],
∀c = 1, . . . , C.
Similarly, the matrices ¯Cj associated with any subsets of ¯Z are also multi-channel circular
convolutions.
From Proposition 5, shift invariant ReduNet is a deep convolutional network for multi-
channel 1D signals by construction. Notice that even if the initial lifting kernels are separated
(41), the matrix inverse in (42) for computing ¯E (similarly for ¯
Cj) introduces “cross talk”
among all C channels. This multi-channel mingling eﬀect will become more clear when we
show below how to compute ¯E and ¯Cj eﬃciently in the frequency domain. Hence, unlike
Xception nets (Chollet, 2017), these multi-channel convolutions in general are not depth-wise
separable.31
4.3 Fast Computation in the Spectral Domain
The calculation of ¯E in (42) requires inverting a matrix of size nC×nC, which has complexity
O(n3C3). By using the relationship between circulant matrix and Discrete Fourier Transform
(DFT) of a 1D signal, this complexity can be signiﬁcantly reduced.
Speciﬁcally, let F ∈Cn×n be the DFT matrix,32 and DFT(z) .= F z ∈Cn×n be the DFT
of z ∈Rn, where C denotes the set of complex numbers. We know all circulant matrices
can be simultaneously diagonalized by the discrete Fourier transform matrix F :
circ(z) = F ∗diag(DFT(z))F .
(43)
We refer the reader to Fact 5 of the Appendix B.3 for more detailed properties of circulant
matrices and DFT. Hence the covariance matrix ¯Σ(¯z) of the form (37) can be converted to
a standard “blocks of diagonals” form:
¯Σ(¯z) =


F ∗
0
0
0
...
0
0
0
F ∗




D11(¯z)
· · ·
D1C(¯z)
...
...
...
DC1(¯z)
· · ·
DCC(¯z)




F
0
0
0
...
0
0
0
F

∈RnC×nC,
(44)
31. It remains open what additional structures on the data would lead to depth-wise separable convolutions.
32. Here we scaled the matrix F to be unitary, hence it diﬀers from the conventional DFT matrix by a 1/√n.
33

Chan, Yu, You, Qi, Wright, and Ma
where Dcc′(¯z) .= diag(DFT(z[c])) · diag(DFT(z[c′]))∗∈Cn×n is a diagonal matrix. The
middle of RHS of (44) is a block diagonal matrix after a permutation of rows and columns.
Given a collection of multi-channel features {¯zi ∈RC×n}m
i=1, we can use the relation in
(44) to compute ¯E (and similarly for ¯Cj) as
¯E =


F ∗
0
0
0
...
0
0
0
F ∗

·α


I + α
m
X
i=1


D11(¯zi)
· · ·
D1C(¯zi)
...
...
...
DC1(¯zi)
· · ·
DCC(¯zi)





−1
·


F
0
0
0
...
0
0
0
F

∈RnC×nC.
(45)
The matrix in the inverse operator is a block diagonal matrix with n blocks of size C × C
after a permutation of rows and columns. Hence, to compute ¯E and ¯Cj ∈RnC×nC, we only
need to compute in the frequency domain the inverse of C × C blocks for n times and the
overall complexity is O(nC3). 33
The beneﬁt of computation with DFT motivates us to construct the ReduNet in the
spectral domain. Let us consider the shift invariant coding rate reduction objective for shift
invariant features {¯zi ∈RC×n}m
i=1:
∆Rcirc( ¯Z, Π) .= 1
n∆R(circ( ¯Z), ¯Π)
= 1
2n log det
 
I + αcirc( ¯Z)circ( ¯Z)∗
!
−
k
X
j=1
γj
2n log det
 
I + αjcirc( ¯Z) ¯Πjcirc( ¯Z)∗
!
,
(46)
where α =
Cn
mnϵ2 =
C
mϵ2 , αj =
Cn
tr(Πj)nϵ2 =
C
tr(Πj)ϵ2 , γj =
tr(Πj)
m
, and ¯Πj is augmented
membership matrix in an obvious way. The normalization factor n is introduce because
the circulant matrix circ( ¯Z) contains n (shifted) copies of each signal. Next, we derive the
ReduNet for maximizing ∆Rcirc( ¯Z, Π).
Let DFT( ¯Z) ∈CC×n×m be data in spectral domain obtained by taking DFT on the
second dimension and denote DFT( ¯Z)(p) ∈CC×m the p-th slice of DFT( ¯Z) on the second
dimension. Then, the gradient of ∆Rcirc( ¯Z, Π) w.r.t. ¯Z can be computed from the expansion
¯E ∈CC×C×n and compression ¯Cj ∈CC×C×n operators in the spectral domain, deﬁned as
¯E(p) .= α ·

I + α · DFT( ¯Z)(p) · DFT( ¯Z)(p)∗−1
∈CC×C,
¯Cj(p) .= αj ·

I + αj · DFT( ¯Z)(p) · Πj · DFT( ¯Z)(p)∗−1
∈CC×C.
(47)
In above, ¯E(p) (resp., ¯Cj(p)) is the p-th slice of ¯E (resp., ¯Cj) on the last dimension. Speciﬁcally,
we have the following result (see Appendix B.3 for a complete proof).
Theorem 6 (Computing multi-channel convolutions ¯E and ¯Cj) Let ¯U ∈CC×n×m
and ¯
W j ∈CC×n×m, j = 1, . . . , k be given by
¯U(p)
.=
¯E(p) · DFT( ¯Z)(p),
(48)
¯
W j(p)
.=
¯Cj(p) · DFT( ¯Z)(p),
(49)
33. There is strong scientiﬁc evidence that neurons in the visual cortex encode and transmit information in
the rate of spiking, hence the so-called spiking neurons (Softky and Koch, 1993; Eliasmith and Anderson,
2003). Nature might be exploiting the computational eﬃciency in the frequency domain for achieving
shift invariance.
34

ReduNet: A White-box Deep Network from Rate Reduction
input
data
cube
output
data
cube
Figure 10: For invariance to 2D translation, ¯E and ¯Cj are automatically multi-channel 2D convolu-
tions.
for each p ∈{0, . . . , n −1}. Then, we have
1
2n
∂log det(I + α · circ( ¯Z)circ( ¯Z)∗)
∂¯Z
=
IDFT( ¯U),
(50)
γj
2n
∂log det(I + αj · circ( ¯Z) ¯Πjcirc( ¯Z)∗)
∂¯Z
=
γj · IDFT( ¯
W jΠj).
(51)
In above, IDFT( ¯U) is the time domain signal obtained by taking inverse DFT on each
channel of each signal in ¯U.
By this result, the gradient ascent update in (13) (when applied to ∆Rcirc( ¯Z, Π)) can be
equivalently expressed as an update in spectral domain on ¯Vℓ.= DFT( ¯Zℓ) as
¯Vℓ+1(p) ∝¯Vℓ(p) + η

¯Eℓ(p) · ¯Vℓ(p) −
k
X
j=1
γj ¯Cj
ℓ(p) · ¯Vℓ(p)Πj
,
p = 0, . . . , n −1,
(52)
and a ReduNet can be constructed in a similar fashion as before. For implementation details,
we refer the reader to Algorithm 3 of Appendix B.3.
4.4 2D Translation Invariance
In the case of classifying images invariant to arbitrary 2D translation, we may view the
image (feature) z ∈R(W×H)×C as a function deﬁned on a torus T 2 (discretized as a W × H
grid) and consider G to be the (Abelian) group of all 2D (circular) translations on the torus.
As we will show in the Appendix C, the associated linear operators ¯E and ¯Cj’s act on the
image feature z as multi-channel 2D circular convolutions, as shown in Figure 10. The
resulting network will be a deep convolutional network that shares the same multi-channel
convolution structures as empirically designed CNNs for 2D images (LeCun et al., 1995;
Krizhevsky et al., 2012) or ones suggested for promoting sparsity (Papyan et al., 2017)! The
diﬀerence is that, again, the convolution architectures and parameters of our network are
derived from the rate reduction objective, and so are all the nonlinear activations. Like the
1D signal case, the derivation in Appendix C shows that this convolutional network can be
constructed much more eﬃciently in the spectral domain. See Theorem 17 of Appendix C
for a rigorous statement and justiﬁcation.
4.5 Overall Network Architecture and Comparison
Following the above derivation, we see that in order to ﬁnd a linear discriminative rep-
resentation (LDR) for multiple classes of signals/images that is invariant to translation,
35

Chan, Yu, You, Qi, Wright, and Ma
C channels
sparse
Multi-Class
Signals
Lifting &
Sparse Coding
Invariant
Rate Reduction
Incoherent
Subspaces
)
Rn
S1
S2
Sj
zi
Figure 11: The overall process for classifying multi-class signals with shift invariance: Multi-
channel lifting, sparse coding, followed by a multi-channel convolution ReduNet
for invariant rate reduction. These components are necessary in order to map
shift-invariant multi-class signals to incoherent (linear) subspaces as an LDR.
Note that the architectures of most modern deep neural networks resemble this
process. The so-learned LDR facilitates subsequent tasks such as classiﬁcation.
sparse coding, a multi-layer architecture with multi-channel convolutions, diﬀerent nonlinear
activation, and spectrum computing all become necessary components for achieving the
objective eﬀectively and eﬃciently. Figure 11 illustrates the overall process of learning such
a representation via invariant rate reduction on the input sparse codes.
Connections to convolutional and recurrent sparse coding.
As we have discussed
in the introduction, it has been long noticed that there are connections between sparse
coding and deep networks, including the work of Learned ISTA (Gregor and LeCun, 2010)
and many of its convolutional and recurrent variants (Wisdom et al., 2016; Papyan et al.,
2017; Sulam et al., 2018; Monga et al., 2019). Although both sparsity and convolution are
advocated as desired characteristics for such networks, their precise roles for the classiﬁcation
task have never been fully revealed. For instance, Papyan et al. (2017) has suggested a
convolutional sparse coding framework for the CNNs. It actually aligns well with the early
lifting and sparse coding stage of the overall process. Nevertheless, our new framework
suggests, once such sparse codes are obtained, one needs subsequent ReduNet to transform
them to the desired LDRs. Through our derivations, we see how lifting and sparse coding
(via random ﬁltering or sparse deconvolution), multi-channel convolutions ( ¯E, ¯Cj), and
diﬀerent nonlinear operations: grouping bπj(·), normalization PSn−1(·), and soft thresholding
τ(·) are all derived as necessary processes from the objective of maximizing rate reduction
of the learned features while enforcing shift invariance.
Comparison with ScatteringNet and PCANet.
Using convolutional operators/with
pooling to ensure equivarience/invariance have been common practice in deep networks
(LeCun and Bengio, 1995; Cohen and Welling, 2016a), but the number of convolutions needed
has never been clear and their parameters need to be learned via back propagation from
randomly initialized ones. Of course, one may also choose a complete basis for the convolution
ﬁlters in each layer to ensure translational equivariance/invariance for a wide range of signals.
36

ReduNet: A White-box Deep Network from Rate Reduction
ScatteringNet (Bruna and Mallat, 2013) and many followup works (Wiatowski and B¨olcskei,
2018) have shown to use modulus of 2D wavelet transform to construct invariant features.
However, the number of convolutions needed usually grow exponentially in the number of
layers. That is the reason why ScatteringNet type networks cannot be so deep and typically
limited to only 2-3 layers. In practice, it is often used in a hybrid setting Zarka et al.
(2020, 2021) for better performance and scalability. On the other hand, PCANet (Chan
et al., 2015) argues that one can signiﬁcantly reduce the number of convolution channels
by learning features directly from the data. In contrast to ScatteringNet and PCANet,
the proposed invariant ReduNet by construction learns equivariant features from the data
that are discriminative between classes. As experiments on MNIST in Appendix E.5 show,
the representations learned by ReduNet indeed preserve translation information. In fact,
scattering transform can be used with ReduNet in a complementary fashion. One may replace
the aforementioned random (lifting) ﬁlters with the scattering transform. As experiments in
the Appendix E.5 show this can yield signiﬁcantly better classiﬁcation performance.
Notice that, none of the previous “convolution-by-design” approaches explain why we
need multi-channel (3D) convolutions instead of separable (2D) ones, let alone how to design
them. In contrast, in the new rate reduction framework, we see that both the forms and
roles of the multi-channel convolutions ( ¯E, ¯Cj) are explicitly derived and justiﬁed, the
number of ﬁlters (channels) remains constant through all layers, and even values of their
parameters are determined by the data of interest. Of course, as mentioned before the values
of the parameters can be further ﬁne-tuned to improve performance, as we will see in the
experimental section (Table 2).
Sparse coding, spectral computing, and subspace embedding in nature.
Notice
that sparse coding has long been hypothesized as the guiding organization principle for the
visual cortex of primates (Olshausen and Field, 1996). Through years of evolution, the
visual cortex has learned to sparsely encode the visual input with all types of localized
and oriented ﬁlters. Interstingly, there have been strong scientiﬁc evidences that neurons
in the visual cortex transmit and process information in terms of rates of spiking, i.e. in
the spectrum rather than the magnitude of signals, hence the so-called “spiking neurons”
(Softky and Koch, 1993; Eliasmith and Anderson, 2003; Belitski et al., 2008). Even more
interestingly, recent studies in neuroscience have started to reveal how these mechanisms
might be integrated in the inferotemporal (IT) cortex, where neurons encode and process
information about high-level object identity (e.g. face recognition), invariant to various
transformations (Majaj et al., 2015; Chang and Tsao, 2017). In particular, Chang and
Tsao (2017) went even further to hypothesize that high-level neurons encode the face space
as a linear subspace with each cell likely encoding one axis of the subspace (rather than
previously thought “an exemplar”). The framework laid out in this paper suggests that
such a “high-level” compact (linear and discriminative) representation can be eﬃciently and
eﬀectively learned in the spectrum domain via an arguably much simpler and more natural
“forward propagation” mechanism. Maybe, just maybe, nature has already learned to exploit
what mathematics reveals as the most parsimonious and economic.
37

Chan, Yu, You, Qi, Wright, and Ma
5. Experimental Veriﬁcation
In this section, we conduct experiments to (1). Validate the eﬀectiveness of the proposed
maximal coding rate reduction (MCR2) principle analyzed in Section 2; and (2). Verify
whether the constructed ReduNet, including the basic vector case ReduNet in Section 3
and the invariance ReduNet in Section 4, achieves its design objectives through experiments.
Our goal in this work is not to push the state of the art performance on any real datasets
with additional engineering ideas and heuristics, although the experimental results clearly
suggest this potential in the future. All code is implemented in Python mainly using NumPy
and PyTorch. All of our experiments are conducted in a computing node with 2.1 GHz Intel
Xeon Silver CPU, 256GB of memory and 2 Nvidia RTX2080. Implementation details and
many more experiments and can be found in Appendix D and E.
5.1 Experimental Veriﬁcation of the MCR2 Objective
In this subsection, we present experimental results on investigating the MCR2 objective
function for training neural networks. Our theoretical analysis in Section 2 shows how the
maximal coding rate reduction (MCR2) is a principled measure for learning discriminative and
diverse representations for mixed data. In this section, we demonstrate experimentally how
this principle alone, without any other heuristics, is adequate to learning good representations.
More speciﬁcally, we apply the widely adopted neural network architectures (such as
ResNet (He et al., 2016)) as the feature mapping z = f(x, θ) and optimize the neural
network parameters θ to achieve maximal coding rate reduction. Our goal here is to validate
eﬀectiveness of this principle through its most basic usage and fair comparison with existing
frameworks. More implementation details and experiments are given in Appendix D. The
code for reproducing the results on the eﬀectiveness of MCR2 objective in this section can
be found in https://github.com/Ma-Lab-Berkeley/MCR2.
Supervised learning via rate reduction.
When class labels are provided during train-
ing, we assign the membership (diagonal) matrix Π = {Πj}k
j=1 as follows: for each sample
xi with label j, set Πj(i, i) = 1 and Πl(i, i) = 0, ∀l ̸= j. Then the mapping f(·, θ) can
be learned by optimizing (11), where Π remains constant. We apply stochastic gradient
descent to optimize MCR2, and for each iteration we use mini-batch data {(xi, yi)}m
i=1 to
approximate the MCR2 loss.
Evaluation via classiﬁcation.
As we will see, in the supervised setting, the learned
representation has very clear subspace structures. So to evaluate the learned representations,
we consider a natural nearest subspace classiﬁer. For each class of learned features Zj, let
µj ∈Rn be the mean of the representation vectors of j-th class and U j ∈Rn×rj be the ﬁrst
rj principal components for Zj, where rj is the estimated dimension of class j. The predicted
label of a test data x′ is given by j′ = argminj∈{1,...,k} ∥(I −U j(U j)∗)(f(x′, θ) −µj)∥2
2.
Experiments on real data.
We consider CIFAR10 dataset (Krizhevsky, 2009) and
ResNet-18 (He et al., 2016) for f(·, θ). We replace the last linear layer of ResNet-18 by
a two-layer fully connected network with ReLU activation function such that the output
dimension is 128. We set the mini-batch size as m = 1, 000 and the precision parameter
ϵ2 = 0.5. More results can be found in Appendix D.3.2.
38

ReduNet: A White-box Deep Network from Rate Reduction
100
101
102
103
104
Number of iterations
0
10
20
30
40
50
60
70
Loss
∆R
R
Rcc
(a) Evolution of R, Rc, ∆R during
the training process.
0
100
200
300
400
500
Epoch
0
10
20
30
40
50
60
70
Loss
∆R (train)
∆R (test)
R (train)
R (test)
Rc (train)
Rc (test)
c
c
(b) Training loss versus testing
loss.
0
5
10
15
20
25
30
Components
0
5
10
15
20
25
30
Sigular Values
(c) PCA: (red) overall data;
(blue) individual classes.
Figure 12: Evolution of the rates of MCR2 in the training process and principal components of
learned features.
Figure 12(a) illustrates how the two rates and their diﬀerence (for both training and test
data) evolves over epochs of training: After an initial phase, R gradually increases while Rc
decreases, indicating that features Z are expanding as a whole while each class Zj is being
compressed. Figure 12(c) shows the distribution of singular values per Zj and Figure 1
(right) shows the angles of features sorted by class. Compared to the geometric loss (Lezama
et al., 2018), our features are not only orthogonal but also of much higher dimension. We
compare the singular values of representations, both overall data and individual classes,
learned by using cross-entropy and MCR2 in Figure 17 and Figure 18 in Appendix D.3.1.
We ﬁnd that the representations learned by using MCR2 loss are much more diverse than
the ones learned by using cross-entropy loss. In addition, we ﬁnd that we are able to select
diverse images from the same class according to the “principal” components of the learned
features (see Figure 13 and Figure 19).
One potential caveat of MCR2 training is how to optimally select the output dimension
n and training batch size m. For a given output dimension n, a suﬃciently large batch
size m is needed in order to achieve good classiﬁcation performance. More detail study on
varying output dimension n and batch size m is listed in Table 5 of Appendix D.3.2.
Ratio=0.0
Ratio=0.1
Ratio=0.2
Ratio=0.3
Ratio=0.4
Ratio=0.5
CE Training
0.939
0.909
0.861
0.791
0.724
0.603
MCR2 Training
0.940
0.911
0.897
0.881
0.866
0.843
Table 1: Classiﬁcation results with features learned with labels corrupted at diﬀerent levels.
Robustness to corrupted labels. Because MCR2 by design encourages richer represen-
tations that preserves intrinsic structures from the data X, training relies less on class
labels than traditional loss such as cross-entropy (CE). To verify this, we train the same
network using both CE and MCR2 with certain ratios of randomly corrupted training labels.
Figure 14 illustrates the learning process: for diﬀerent levels of corruption, while the rate
for the whole set always converges to the same value, the rates for the classes are inversely
proportional to the ratio of corruption, indicating our method only compresses samples
with valid labels. The classiﬁcation results are summarized in Table 1. By applying exact
39

Chan, Yu, You, Qi, Wright, and Ma
C1
C2
C3
C4
C5
C6
C7
C8
C9
C10
(a) Bird
C1
C2
C3
C4
C5
C6
C7
C8
C9
C10
(b) Ship
Figure 13: Visualization of principal components learned for class 2-‘Bird’ and class 8-‘Ship’. For
each class j, we ﬁrst compute the top-10 singular vectors of the SVD of the learned
features Zj. Then for the l-th singular vector of class j (denoted by ul
j), and for the
feature of the i-th image of class j (denoted by zi
j), we calculate the absolute value of
inner product, |⟨zi
j, ul
j⟩|, then we select the top-10 images according to |⟨zi
j, ul
j⟩| for
each singular vector. In the above two ﬁgures, each row corresponds to one singular
vector (component Cl). The rows are sorted based on the magnitude of the associated
singular values, from large to small.
0
5000
10000
15000
20000
25000
Number of iterations
10
15
20
25
30
35
40
45
50
Loss
noise=0.0
noise=0.1
noise=0.3
noise=0.5
(a) ∆R
 Z(θ), Π, ϵ

.
0
5000
10000
15000
20000
25000
Number of iterations
52.5
55.0
57.5
60.0
62.5
65.0
67.5
70.0
Loss
noise=0.0
noise=0.1
noise=0.3
noise=0.5
(b) R(Z(θ), ϵ).
0
5000
10000
15000
20000
25000
Number of iterations
0
10
20
30
40
50
Loss
noise=0.0
noise=0.1
noise=0.3
noise=0.5
(c) Rc(Z(θ), ϵ | Π).
Figure 14: Evolution of rates R, Rc, ∆R of MCR2 during training with corrupted labels.
the same training parameters, MCR2 is signiﬁcantly more robust than CE, especially with
higher ratio of corrupted labels. This can be an advantage in the settings of self-supervised
learning or constrastive learning when the grouping information can be very noisy. More
detailed comparison between MCR2 and OLE (Lezama et al., 2018), Large Margin Deep
Networks (Elsayed et al., 2018), and ITLM (Shen and Sanghavi, 2019) on learning from
noisy labels can be found in Appendix D.4 (Table 8).
Beside the supervised learning setting, we explore the MCR2 objective in the self-
supervised learning setting. We ﬁnd that the MCR2 objective can learn good representations
40

ReduNet: A White-box Deep Network from Rate Reduction
without using any label and achieve better performance over other highly engineered methods
on clustering tasks. More details on self-supervised learning can be found in Section D.6.
5.2 Experimental Veriﬁcation of the ReduNet
In this section, we verify whether the so constructed ReduNet (developed in Section 3 and 4)
achieves its design objectives through experiments on synthetic data and real images. The
datasets and experiments are chosen to clearly demonstrate the properties and behaviors
of the proposed ReduNet, in terms of learning the correct truly invariant discriminative
(orthogonal) representation for the given data. Implementation details and more experiments
and can be found in Appendix E. The code for reproducing the ReduNet results can be
found in https://github.com/Ma-Lab-Berkeley/ReduNet.
Learning Mixture of Gaussians in S2.
Consider a mixture of three Gaussian distribu-
tions in R3 that is projected onto S2. We ﬁrst generate data points from these two distribu-
tions, X1 = [x1
1, . . . , xm
1 ] ∈R3×m, xi
1 ∼N(µ1, σ2
1I), and π(xi
1) = 1; X2 = [x1
2, . . . , xm
2 ] ∈
R2×m, xi
2 ∼N(µ2, σ2
2I), and π(xi
2) = 2; X3 = [x1
3, . . . , xm
3 ] ∈R3×m, xi
3 ∼N(µ3, σ2
3I), and
π(xi
2) = 3. We set m = 500, σ1 = σ2 = σ3 = 0.1 and µ1, µ2, µ3 ∈S2. Then we project
all the data points onto S2, i.e., xi
j/∥xi
j∥2. To construct the network (computing Eℓ, Cj
ℓ
for ℓ-the layer), we set the number of iterations/layers L = 2, 00034, step size η = 0.5, and
precision ϵ = 0.1. As shown in Figure 15(a)-15(b), we can observe that after the mapping
f(·, θ), samples from the same class converge to a single cluster and the angle between two
diﬀerent clusters is approximately π/4, which is well aligned with the optimal solution Z⋆of
the MCR2 loss in S2. MCR2 loss of features on diﬀerent layers can be found in Figure 15(c).
Empirically, we ﬁnd that our constructed network is able to maximize MCR2 loss and
converges stably and samples from the same class converge to one cluster and diﬀerent
clusters are orthogonal to each other. Moreover, we sample new data points from the same
distributions for both cases and ﬁnd that new samples form the same class consistently
converge to the same cluster center as the training samples. More simulation examples and
details can be found in Appendix E.4.
Rotational Invariance on MNIST Digits.
We now study the ReduNet on learning
rotation invariant features on the real 10-class MNIST dataset (LeCun, 1998). We impose a
polar grid on the image x ∈RH×W , with its geometric center being the center of the 2D
polar grid (as illustrated in Figure 21 in the Appendix). For each radius ri, i ∈[C], we can
sample Γ pixels with respect to each angle γl = l · (2π/Γ) with l ∈[Γ]. Then given a sample
image x from the dataset, we represent the image in the (sampled) polar coordinate as a
multi-channel signal xp ∈RΓ×C. Our goal is to learn a rotation invariant representation,
i.e., we expect to learn f(·, θ) such that {f(xp ◦g, θ)}g∈G lie in the same subspace, where
g is the cyclic-shift in polar angle. We use m = 100 training samples (10 from each class)
and set Γ = 200, C = 15 for polar sampling. By performing the above sampling in polar
coordinate, we can obtain the data matrix Xp ∈R(Γ·C)×m. For the ReduNet, we set the
34. We do this only to demonstrate our framework leads to stable deep networks even with thousands of
layers! In practice this is not necessary and one can stop whenever adding new layers gives diminishing
returns. For this example, a couple of hundred is suﬃcient. Hence the clear optimization objective gives
a natural criterion for the depth of the network needed. Remarks in Section 3.4 provide possible ideas to
further reduce the number of layers.
41

Chan, Yu, You, Qi, Wright, and Ma
−1.00
−0.75
−0.50
−0.25
0.00
0.25
0.50
0.75
1.00
−1.00−0.75−0.50−0.250.00 0.25 0.50 0.75 1.00
−1.00
−0.75
−0.50
−0.25
0.00
0.25
0.50
0.75
1.00
(a) Xtrain (3D)
−1.00
−0.75
−0.50
−0.25
0.00
0.25
0.50
0.75
1.00
−1.00−0.75−0.50−0.250.00 0.25 0.50 0.75 1.00
−1.00
−0.75
−0.50
−0.25
0.00
0.25
0.50
0.75
1.00
(b) Ztrain (3D)
0
250 500 750 1000 1250 1500 1750 2000
Layers
0.0
0.5
1.0
1.5
2.0
2.5
3.0
3.5
4.0
Loss
∆R (train)
∆R (test)
R (train)
R (test)
Rc (train)
Rc (test)
(c) Loss (3D)
Figure 15: Original samples and learned representations for 3D Mixture of Gaussians. We visualize
data points X (before mapping f(·, θ)) and learned features Z (after mapping f(·, θ))
by scatter plot. In each scatter plot, each color represents one class of samples. We also
show the plots for the progression of values of the objective functions.
number of layers/iterations L = 40, precision ϵ = 0.1, step size η = 0.5. Before the ﬁrst
layer, we perform lifting of the input by 1D circulant-convolution with 20 random Gaussian
kernels of size 5.
0
400
800
1200
1600
2000
0
400
800
1200
1600
2000
0.5
1.0
(a) Xshift (RI-MNIST)
0
400
800
1200
1600
2000
0
400
800
1200
1600
2000
0.0
0.5
1.0
(b) ¯Zshift (RI-MNIST)
0.0
0.2
0.4
0.6
0.8
1.0
Similarity
0.0
0.5
1.0
1.5
2.0
2.5
3.0
3.5
Count
×106
(c) Similarity (RI-MNIST)
0
5
10
15
20
25
30
35
40
Layers
0.00
0.05
0.10
0.15
0.20
0.25
0.30
Loss
∆R (train)
∆R (test)
R (train)
R (test)
Rc (train)
Rc (test)
(d) Loss (RI-MNIST)
0
400
800
1200
1600
0
400
800
1200
1600
0.0
0.5
1.0
(e) Xshift (TI-MNIST)
0
400
800
1200
1600
0
400
800
1200
1600
0.0
0.5
1.0
(f) ¯Zshift (TI-MNIST)
0.0
0.2
0.4
0.6
0.8
1.0
Similarity
0.0
0.5
1.0
1.5
2.0
Count
×106
(g) Similarity (TI-MNIST)
0
5
10
15
20
25
Layers
0.00
0.05
0.10
0.15
0.20
0.25
0.30
0.35
Loss
∆R (train)
∆R (test)
R (train)
R (test)
Rc (train)
Rc (test)
(h) Loss (TI-MNIST)
Figure 16: (a)(b) and (e)(f) are heatmaps of cosine similarity among shifted training data Xshift and
learned features ¯Zshift, for rotation and translation invariance respectively. (c)(g) are
histograms of the cosine similarity (in absolute value) between all pairs of features across
diﬀerent classes: for each pair, one sample is from the training dataset (including all shifts)
and one sample is from another class in the test dataset (including all possible shifts).
There are 4 × 106 pairs for the rotation (c) and 2.56 × 106 pairs for the translation (g).
To evaluate the learned representation, each training sample is augmented by 20 of its
rotated version, each shifted with stride=10. We compute the cosine similarities among the
m × 20 augmented training inputs Xshift and the results are shown in Figure 16(a). We
compare the cosine similarities among the learned features of all the augmented versions,
42

ReduNet: A White-box Deep Network from Rate Reduction
Initialization
Backpropagation
Test Accuracy


0.898


0.932


0.978
Table 2: Test accuracy of 2D translation-invariant ReduNet, ReduNet-bp (without initialization),
and ReduNet-bp (with initialization) on the MNIST dataset.
i.e., ¯Zshift and summarize the results in Figure 16(b). As we see, the so constructed rotation-
invariant ReduNet is able to map the training data (as well as all its rotated versions) from
the 10 diﬀerent classes into 10 nearly orthogonal subspaces. That is, the learnt subspaces
are truly invariant to shift transformation in polar angle. Next, we randomly draw another
100 test samples followed by the same augmentation procedure. We compute the cosine
similarity histogram between features of all shifted training and those of the new test samples
in Figure 16(c). In Figure 16(d), we visualize the MCR2 loss on the ℓ-th layer representation
of the ReduNet on the training and test dataset. From Figure 16(c) and Figure 16(d), we
can ﬁnd that the constructed ReduNet is indeed able to maximize the MCR2 loss as well as
generalize to the test data.
2D Translation Invariance on MNIST Digits.
In this part, we provide experimental
results to use the invariant ReduNet to learn representations for images that are invariant
to the 2D cyclic translation. Essentially we view the image as painted on a torus and
can be translated arbitrarily, as illustrated in Figure 22 in the Appendix. Again we use
the 10-class MNIST dataset. We use m = 100 training samples (10 samples from each
class) for constructing the ReduNet and use another 100 samples (10 samples from each
class) as the test dataset. We apply 2D circulant convolution to the (1-channel) inputs
with 75 random Gaussian kernels of size 9 × 9 before the ﬁrst layer of ReduNet. For the
translation-invariant ReduNet, we set L = 25, step size η = 0.5, precision ϵ = 0.1. Similar
to the rotational invariance task, to evaluate the performance of ReduNet with regard
to translation, we augment each training/testing sample by 16 of its translational shifted
version (with stride=7). The heatmap of the similarities among the m × 16 augmented
training inputs Xshift and the learned features circ( ¯Z)shift are compared in Figure 16(e) and
Figure 16(f). Similar to the rotation case, Figure 16(g) shows the histogram of similarity
between features of all shifted training samples and test samples. Clearly, the ReduNet can
map training samples from the 10 classes to 10 nearly orthogonal subspaces and invariant to
all possible 2D translations on the training dataset. Also, the MCR2 loss in Figure 16(h)
is increasing with the increased layer. This veriﬁes that the proposed ReduNet can indeed
maximize the objective and be invariant to transformations as it is designed to.
Back Propagation on ReduNet.
Once the {Eℓ}L
ℓ=1 and {C1
ℓ, . . . , Ck
ℓ}L
ℓ=1 of the Re-
duNet are constructed, we here further test whether the architecture is amenable to ﬁne-
tuning via back propagation. For the 2D translation-invariant ReduNet, we add a fully-
connected layer with weight having dimensions CHW ×k after the last layer of the ReduNet,
where k is the number of classes. We denote this modiﬁed architecture as ReduNet-bp.
We consider the 10-class classiﬁcation problem on the MNIST dataset, and compare three
43

Chan, Yu, You, Qi, Wright, and Ma
networks: (1). ReduNet by the forward construction, (2). ReduNet-bp initialized by the con-
struction, (3). ReduNet-bp with random initialization using the same backbone architecture.
To initialize the networks, we use m = 500 samples (50 from each class) and set number
of layers/iterations L = 30, step size η = 0.5, precision ϵ = 0.1. Before the ﬁrst layer, we
perform 2D circulant-convolution to the inputs with 16 channels with 7×7 random Gaussian
kernels. For ReduNet-bp, we further train the constructed ReduNet via back propagation
by adding an extra fully connected layer and using cross-entropy loss. We update the model
parameters by using SGD to minimize the loss on the entire MNIST training data. To
evaluate the three networks, we compare the standard test accuracy on 10, 000 MNIST
test samples. For ReduNet, we apply the nearest subspace classiﬁer for prediction. For
ReduNet-bp with and without initialization, we take the argmax of the ﬁnal fully connected
layer as the prediction. The results are summarized in Table 2. We ﬁnd that (1). The
ReduNet architecture can be optimized by SGD and achieve better standard accuracy after
back propagation; (2). Using constructed ReduNet for initialization can achieve better
performance compared with the same architecture with random initialization.
6. Conclusions and Discussions
In this paper, we have laid out a theoretical and computational framework based on data
compression which allows us to understand and interpret not only the characteristics of
modern deep networks but also reveal their purposes and functions as a white box. From this
new perspective, we see that at a high level, the objective of seeking a linear discriminative
representation (via deep learning) aligns well with the objectives of the classic linear
discriminant analysis (Hastie et al., 2009), independent component analysis (Hyv¨arinen and
Oja, 2000), and generalized principal component analysis (Vidal et al., 2016). The main
diﬀerence is that now we are able to conduct such analyses through the lens of a constructive
nonlinear mapping and an intrinsic measure. This renders all these analyses so much more
general hence practical for real-world data.
Despite the long history of practicing artiﬁcial (deep) neural networks since their inception
in 1940-1950’s (McCulloch and Pitts, 1943; Rosenblatt, 1958), the architectures and operators
in deep networks have been mainly proposed or designed empirically and trained via back
propagation as a black box (Rumelhart et al., 1986). Table 3 left column summarizes the
main characteristics of the conventional practice of deep networks,35 whereas the right
column highlights comparison of our new compression based framework to the current
practice of deep neural networks, on which we will elaborate a little more below.
White box versus black box.
This new framework oﬀers a constructive approach to
derive deep (convolution) networks entirely as a white box from the objective of learning
a low-dimensional linear discriminative representation for the given (mixed) data. The
goodness of the representation is measured by the intrinsic rate reduction. The resulting
network, called the ReduNet, emulates a gradient-based iterative scheme to optimize the
rate reduction objective. It shares almost all the main structural characteristics of modern
deep networks. Nevertheless, its architectures, linear and nonlinear operators and even
35. There are exceptions such as ScatteringNets (Bruna and Mallat, 2013; Wiatowski and B¨olcskei, 2018),
whose operators are pre-designed and ﬁxed, as also indicated in the table.
44

ReduNet: A White-box Deep Network from Rate Reduction
Conventional DNNs
Compression (ReduNets)
Objectives
input/output ﬁtting
rate reduction
Deep architectures
trial & error
iterative optimization
Layer operators
empirical
projected gradient
Shift invariance
CNNs + augmentation
invariant ReduNets
Initializations
random/pre-designed
forward computed
Training/ﬁne-tuning
back prop/ﬁxed
forward/back prop
Interpretability
black box
white box
Representations
unknown or unclear
incoherent subspaces (LDR)
Table 3: Comparison between conventional deep networks and compression based ReduNets.
their values are all derived from the data and all have precise geometric and statistical
interpretation. In particular, we ﬁnd it is rather intriguing that the linear operator of
each layer has a non-parameteric “data auto-regression” interpretation. Together with the
“forward construction,” they give rather basic but universal computing mechanisms that even
simple organisms/systems can use to learn good representations from the data that help
future classiﬁcation tasks (e.g. object detection or recognition).
Forward versus backward optimization and reﬁnement.
This constructive and
white-box approach has demonstrated potential in liberating the practice of deep networks
from relying (almost entirely) on random initialization and back propagation of all network
parameters. It oﬀers eﬀective mechanisms to construct (hence initialize) deep networks in a
forward fashion. The forward-constructed ReduNet already exhibits descent classiﬁcation
performance.
Preliminary experiments given in this paper indicate that the ReduNet
architecture is amenable to ﬁne-tuning via back propagation too (say with new training
data), and initialization with the forward-constructed network has advantages over random
initialization. Furthermore, the constructive nature of ReduNet makes it amenable to other
ﬁne-tuning schemes such as forward propagation or incremental learning. Since one no longer
has to update all network parameters simultaneously as a black box, one can potentially
avoid the so-called “catastrophic forgetting” (McCloskey and Cohen, 1989) in the sequential
or incremental learning setting, as the recent work of Wu et al. (2021) suggests.
Modeling invariance and equivariance.
In the case of seeking classiﬁcation invariant
to certain transformation groups (say translation), the new framework models both eqvuivari-
ance and invariance in a very natural way: all equivariant instances are mapped into the same
subspace and the resulting subspaces are hence invariant to the transformation. As we have
shown, in this case, the ReduNet naturally becomes a deep convolution network. Arguably,
this work gives a new constructive and explicit justiﬁcation for the role of multi-channel
convolutions in each layer (widely adopted in modern CNNs) as incremental operators to
compress or expand all equivariant instances for learning an LDR for the data. Moreover, our
derivation reveals the fundamental computational advantage in constructing and learning
45

Chan, Yu, You, Qi, Wright, and Ma
such multi-channel convolutions in the spectral domain. Simulations and experiments on
synthetic and real data sets clearly verify such forward-constructed ReduNet can be invariant
for all transformed instances. The computation scales gracefully with the number of classes,
channels and sample dimension/size.
Invariance and sparsity.
The new framework also reveals a fundamental trade-oﬀbe-
tween sparsity and invariance: essentially one cannot expect to separate diﬀerent classes
of signals/data if signals in each class can be both arbitrarily shifted and arbitrarily super-
imposed. To achieve invariance to all translation, one must impose that signals in each
class are sparsely generated so that all shifted samples span a proper submanifold (in the
high-dimensional space) and features can be mapped to a proper subspace. Although sparse
representation for individual signals have been extensively studied and well understood in
the literature (Wright and Ma, 2021), very little is yet known about how to characterize the
distribution of sparse codes of a class of (equivariant or locally equivariant) signals and its
separability from other classes. This fundamental trade-oﬀbetween sparsity and invariance
certainly merits further theoretical study, as it will lead to more precise characterization of
the statistical resource (network width) and computational resource (network depth) needed
to provide performance guarantees, say for (multi-manifold) classiﬁcation (Buchanan et al.,
2020).
Further improvements and extensions.
We believe the proposed rate reduction pro-
vides a principled framework for designing new networks with interpretable architectures
and operators that can provide performance guarantees (say invariance) when applied to
real-world datasets and problems. Nevertheless, the purposes of this paper are to introduce
the basic principles and concepts of this new framework. We have chosen arguably the
simplest and most basic gradient-based scheme to construct the ReduNet for optimizing
the rate reduction. As we have touched upon brieﬂy in the paper, many powerful ideas
from optimization can be further applied to improve the eﬃciency and performance of the
network, such as acceleration, precondition/normalization, and regularization. Also there
are additional relationships and structures among the channel operators E and Cj that
have not been exploited in this work for computational eﬃciency.
The reader may have realized that the basic ReduNet is expected to work when the data
have relatively benign nonlinear structures when each class is close to be a linear subspace or
Gaussian distribution. Real data (say image classes) have much more complicated structures:
each class can have highly nonlinear geometry and topology or even be multi-modal itself.
Hence in practice, to learn a better LDR, one may have to resort to more sophisticated
strategies to control the compression (linearization) and expansion process. Real data also
have additional priors and data structures that can be exploited. For example, one can reduce
the dimension of the ambient feature space whenever the intrinsic dimension of the features
are low (or sparse) enough. Such dimension-reduction operations (e.g. pooling or striding)
are widely practiced in modern deep (convolution) networks for both computational eﬃciency
and even accuracy. According to the theory of compressive sensing, even a random projection
would be rather eﬃcient and eﬀective in preserving the (discriminative) low-dimensional
structures (Wright and Ma, 2021).
In this work, we have mainly considered learning a good representation Z for the data
X when the class label Π is given and ﬁxed. Nevertheless, notice that in its most general
46

ReduNet: A White-box Deep Network from Rate Reduction
form, the maximal rate reduction objective can be optimized against both the representation
Z and the membership Π. In fact, the original work of Ma et al. (2007) precisely studies
the complementary problem of learning Π by maximizing ∆R(Z, Π, ϵ) with Z ﬁxed, hence
equivalent to minimizing only the compression term: minΠ Rc(Z, ϵ | Π). Therefore, it
is obvious that this framework can be naturally extended to unsupervised settings if the
membership Π is partially known or entirely unknown and it is to be optimized together
with the representation Z. In a similar vein to the construction of the ReduNet, this may
entail us to examine the joint dynamics of the gradient of the representation Z and the
membership Π:
˙Z = η · ∂∆R
∂Z ,
˙Π = γ · ∂∆R
∂Π .
(53)
Last but not the least, in this work, we only considered data that are naturally embedded
(as submanifolds) in a vector space (real or complex). There have been many work that study
and apply deep networks to data with additional structures or in a non-Euclidean space. For
example, in reinforcement learning and optimal control, people often use deep networks to
process data with additional dynamical structures, say linearizing the dynamics (Lusch et al.,
2018). In computer graphics or many other ﬁelds, people deal with data on a non-Euclidean
domain such as a mesh or a graph (Bronstein et al., 2017). It remains interesting to see how
the principles of data compression and linear discriminative representation can be extended
to help study or design principled white-box deep networks associated with dynamical or
graphical data and problems.
Acknowledgments
Yi would like to thank professor Yann LeCun of New York University for a stimulating
discussion in his oﬃce back in November 2019 when they contemplated a fundamental
question: what does or should a deep network try to optimize? At the time, they both
believed the low-dimensionality of the data (say sparsity) and discriminativeness of the
representation (e.g. contrastive learning) have something to do with the answer. The
conversation had inspired Yi to delve into this problem more deeply while self-isolated at
home during the pandemic.
Yi would also like to thank Dr. Harry Shum who has had many hours of conversations
with Yi about how to understand and interpret deep networks during the past couple of
years. In particular, Harry suggested how to better visualize the representations learned by
the rate reduction, including the results shown in Figure 13.
Yi acknowledges support from ONR grant N00014-20-1-2002 and the joint Simons
Foundation-NSF DMS grant #2031899, as well as support from Berkeley FHL Vive Center
for Enhanced Reality and Berkeley Center for Augmented Cognition.
Chong and Yi
acknowledge support from Tsinghua-Berkeley Shenzhen Institute (TBSI) Research Fund.
Yaodong, Haozhi, and Yi acknowledge support from Berkeley AI Research (BAIR). John
acknowledges support from NSF grants 1838061, 1740833, and 1733857.
47

Chan, Yu, You, Qi, Wright, and Ma
Appendix A. Properties of the Rate Reduction Function
This section is organized as follows. We present background and preliminary results for the
log det(·) function and the coding rate function in Section A.1. Then, Section A.2 and A.3
provide technical lemmas for bounding the coding rate and coding rate reduction functions,
respectively. Finally, these lemmas are used to prove our main theoretical results about the
properties of the rate reduction function. The main results (given informally as Theorem 1
in the main body) are stated formally in Section A.4 and a proof is given in Section A.5.
Notations
Throughout this section, we use Sn
++, R+ and Z++ to denote the set of
symmetric positive deﬁnite matrices of size n × n, nonnegative real numbers and positive
integers, respectively.
A.1 Preliminaries
Properties of the log det(·) function.
Lemma 7 The function log det(·) : Sn
++ →R is strictly concave. That is,
log det((1 −β)Z1 + βZ2)) ≥(1 −β) log det(Z1) + β log det(Z2)
for any β ∈(0, 1) and {Z1, Z2} ⊆Sn
++, with equality holds if and only if Z1 = Z2.
Proof
Consider an arbitrary line given by Z = Z0 + t∆Z where Z0 and ∆Z ̸= 0 are
symmetric matrices of size n × n. Let f(t) .= log det(Z0 + t∆Z) be a function deﬁned on an
interval of values of t for which Z0 + t∆Z ∈Sn
++. Following the same argument as in Boyd
and Vandenberghe (2004), we may assume Z0 ∈Sn
++ and get
f(t) = log det Z0 +
n
X
i=1
log(1 + tλi),
where {λi}n
i=1 are eigenvalues of Z
−1
2
0
∆ZZ
−1
2
0
. The second order derivative of f(t) is given
by
f′′(t) = −
n
X
i=1
λ2
i
(1 + tλi)2 < 0.
Therefore, f(t) is strictly concave along the line Z = Z0 + t∆Z. By deﬁnition, we conclude
that log det(·) is strictly concave.
Properties of the coding rate function.
The following properties, also known as the
Sylvester’s determinant theorem, for the coding rate function are known in the paper of Ma
et al. (2007).
Lemma 8 (Commutative property) For any Z ∈Rn×m we have
R(Z, ϵ) .= 1
2 log det

I +
n
mϵ2 ZZ∗
= 1
2 log det

I +
n
mϵ2 Z∗Z

.
Lemma 9 (Invariant property) For any Z ∈Rn×m and any orthogonal matrices U ∈
Rn×n and V ∈Rm×m we have
R(Z, ϵ) = R(UZV ∗, ϵ).
48

ReduNet: A White-box Deep Network from Rate Reduction
A.2 Lower and Upper Bounds for Coding Rate
The following result provides an upper and a lower bound on the coding rate of Z as a
function of the coding rate for its components {Zj}k
j=1. The lower bound is tight when all
the components {Zj}k
j=1 have the same covariance (assuming that they have zero mean).
The upper bound is tight when the components {Zj}k
j=1 are pair-wise orthogonal.
Lemma 10 For any {Zj ∈Rn×mj}k
j=1 and any ϵ > 0, let Z = [Z1, · · · , Zk] ∈Rn×m with
m = Pk
j=1 mj. We have
k
X
j=1
mj
2 log det

I +
n
mjϵ2 Zj(Zj)∗

≤m
2 log det

I +
n
mϵ2 ZZ∗
≤
k
X
j=1
m
2 log det

I +
n
mϵ2 Zj(Zj)∗
,
(54)
where the ﬁrst equality holds if and only if
Z1(Z1)∗
m1
= Z2(Z2)∗
m2
= · · · = Zk(Zk)∗
mk
,
and the second equality holds if and only if (Zj1)∗Zj2 = 0 for all 1 ≤j1 < j2 ≤k.
Proof By Lemma 7, log det(·) is strictly concave. Therefore,
log det

k
X
j=1
βjSj
≥
k
X
j=1
βj log det(Sj), for all {βj > 0}k
j=1,
k
X
j=1
βj = 1 and {Sj ∈Sn
++}k
j=1,
where equality holds if and only if S1 = S2 = · · · = Sk.
Take βj =
mj
m and Sj =
I +
n
mjϵ2 Zj(Zj)∗, we get
m
2 log det

I +
n
mϵ2 ZZ∗
≥
k
X
j=1
mj
2 log det

I +
n
mjϵ2 Zj(Zj)∗

,
with equality holds if and only if Z1(Z1)∗
m1
= · · · = Zk(Zk)∗
mk
. This proves the lower bound in
(54).
We now prove the upper bound. By the strict concavity of log det(·), we have
log det(Q) ≤log det(S) + ⟨∇log det(S), Q −S⟩, for all {Q, S} ⊆Sm
++,
where equality holds if and only if Q = S. Plugging in ∇log det(S) = S−1 (see e.g., Boyd
and Vandenberghe (2004)) and S−1 = (S−1)∗gives
log det(Q) ≤log det(S) + tr(S−1Q) −m.
(55)
49

Chan, Yu, You, Qi, Wright, and Ma
We now take
Q = I +
n
mϵ2 Z∗Z = I +
n
mϵ2


(Z1)∗Z1
(Z1)∗Z2
· · ·
(Z1)∗Zk
(Z2)∗Z1
(Z2)∗Z2
· · ·
(Z2)∗Z2
...
...
...
...
(Zk)∗Z1
(Zk)∗Z2
· · ·
(Zk)∗Zk

, and
(56)
S = I +
n
mϵ2


(Z1)∗Z1
0
· · ·
0
0
(Z2)∗Z2
· · ·
0
...
...
...
...
0
0
· · ·
(Zk)∗Zk

.
From the property of determinant for block diagonal matrix, we have
log det(S) =
k
X
j=1
log det

I +
n
mϵ2 (Zj)∗Zj
.
(57)
Also, note that
tr(S−1Q)
= tr


(I +
n
mϵ2 (Z1)∗Z1)−1(I +
n
mϵ2 (Z1)∗Z1)
· · ·
(I +
n
mϵ2 (Z1)∗Z1)−1(I +
n
mϵ2 (Z1)∗Zk)
...
...
...
(I +
n
mϵ2 (Zk)∗Zk)−1(I +
n
mϵ2 (Zk)∗Z1)
· · ·
(I +
n
mϵ2 (Zk)∗Zk)−1(I +
n
mϵ2 (Zk)∗Zk)


= tr


I
· · ·
⃝
...
...
...
⃝
· · ·
I

= m,
(58)
where “⃝” denotes nonzero quantities that are irrelevant for the purpose of computing the
trace. Plugging (57) and (58) back in (55) gives
m
2 log det

I +
n
mϵ2 Z∗Z

≤
k
X
j=1
m
2 log det

I +
n
mϵ2 (Zj)∗Zj
,
where the equality holds if and only if Q = S, which by the formulation in (56), holds if and
only if (Zj1)∗Zj2 = 0 for all 1 ≤j1 < j2 ≤k. Further using the result in Lemma 8 gives
m
2 log det

I +
n
mϵ2 ZZ∗
≤
k
X
j=1
m
2 log det

I +
n
mϵ2 Zj(Zj)∗
,
which produces the upper bound in (54).
50

ReduNet: A White-box Deep Network from Rate Reduction
A.3 An Upper Bound on Coding Rate Reduction
We may now provide an upper bound on the coding rate reduction ∆R(Z, Π, ϵ) (deﬁned in
(11)) in terms of its individual components {Zj}k
j=1.
Lemma 11 For any Z ∈Rn×m, Π ∈Ωand ϵ > 0, let Zj ∈Rn×mj be ZΠj with zero
columns removed. We have
∆R(Z, Π, ϵ) ≤
k
X
j=1
1
2m log

detm  I +
n
mϵ2 Zj(Zj)∗
detmj

I +
n
mjϵ2 Zj(Zj)∗


,
(59)
with equality holds if and only if (Zj1)∗Zj2 = 0 for all 1 ≤j1 < j2 ≤k.
Proof From the deﬁnition of ∆R(Z, Π, ϵ) in Eq. (11), we have
∆R(Z, Π, ϵ)
= R(Z, ϵ) −Rc(Z, ϵ | Π)
= 1
2 log

det

I +
n
mϵ2 ZZ∗
−
k
X
j=1
tr(Πj)
2m
log

det

I + n ZΠjZ∗
tr(Πj)ϵ2

= 1
2 log

det

I +
n
mϵ2 ZZ∗
−
k
X
j=1
 mj
2m log

det

I + nZj(Zj)∗
mjϵ2

≤
k
X
j=1
1
2 log

det

I +
n
mϵ2 Zj(Zj)∗
−
k
X
j=1
 mj
2m log

det

I + nZj(Zj)∗
mjϵ2

=
k
X
j=1
1
2m log

detm 
I +
n
mϵ2 Zj(Zj)∗
−
k
X
j=1
 1
2m log

detmj

I + nZj(Zj)∗
mjϵ2

=
k
X
j=1
1
2m log

detm  I +
n
mϵ2 Zj(Zj)∗
detmj

I +
n
mjϵ2 Zj(Zj)∗


,
where the inequality follows from the upper bound in Lemma 10, and that the equality holds
if and only if (Zj1)∗Zj2 = 0 for all 1 ≤j1 < j2 ≤k.
A.4 Main Results: Properties of Maximal Coding Rate Reduction
We now present our main theoretical results. The following theorem states that for any
ﬁxed encoding of the partition Π, the coding rate reduction is maximized by data Z that is
maximally discriminative between diﬀerent classes and is diverse within each of the classes.
This result holds provided that the sum of rank for diﬀerent classes is small relative to the
ambient dimension, and that ϵ is small.
Theorem 12 Let Π = {Πj ∈Rm×m}k
j=1 with {Πj ≥0}k
j=1 and
Π1 + · · · + Πk = I be
a given set of diagonal matrices whose diagonal entries encode the membership of the m
51

Chan, Yu, You, Qi, Wright, and Ma
samples in the k classes. Given any ϵ > 0, n > 0 and {n ≥dj > 0}k
j=1, consider the
optimization problem
Z⋆∈argmax
Z∈Rn×m ∆R(Z, Π, ϵ)
s.t. ∥ZΠj∥2
F = tr(Πj), rank(ZΠj) ≤dj, ∀j ∈{1, . . . , k}.
(60)
Under the conditions
• (Large ambient dimension) n ≥Pk
j=1 dj, and
• (High coding precision) ϵ4 < minj∈{1,...,k}

tr(Πj)
m
n2
d2
j

,
the optimal solution Z⋆satisﬁes
• (Between-class discriminative) (Zj1
⋆)∗Zj2
⋆= 0 for all 1 ≤j1 < j2 ≤k, i.e., Zj1
⋆and
Zj2
⋆lie in orthogonal subspaces, and
• (Within-class diverse) For each j ∈{1, . . . , k}, the rank of Zj
⋆is equal to dj and either
all singular values of Zj
⋆are equal to tr(Πj)
dj
, or the dj −1 largest singular values of Zj
⋆
are equal and have value larger than tr(Πj)
dj
,
where Zj
⋆∈Rn×tr(Πj) denotes Z⋆Πj with zero columns removed.
A.5 Proof of Main Results
We start with presenting a lemma that will be used in the proof to Theorem 12.
Lemma 13 Given any twice diﬀerentiable f : R+ →R, integer r ∈Z++ and c ∈R+,
consider the optimization problem
max
x
r
X
p=1
f(xp)
s.t. x = [x1, . . . , xr] ∈Rr
+, x1 ≥x2 ≥· · · ≥xr, and
r
X
p=1
xp = c.
(61)
Let x⋆be an arbitrary global solution to (61). If the conditions
• f′(0) < f′(x) for all x > 0,
• There exists xT > 0 such that f′(x) is strictly increasing in [0, xT ] and strictly decreasing
in [xT , ∞),
• f′′( c
r) < 0 (equivalently, c
r > xT ),
are satisﬁed, then we have either
• x⋆= [ c
r, . . . , c
r], or
52

ReduNet: A White-box Deep Network from Rate Reduction
• x⋆= [xH, . . . , xH, xL] for some xH ∈( c
r,
c
r−1) and xL > 0.
Proof The result holds trivially if r = 1. Throughout the proof we consider the case where
r > 1.
We consider the optimization problem with the inequality constraint x1 ≥· · · ≥xr in
(61) removed:
max
x=[x1,...,xr]∈Rr
+
r
X
p=1
f(xp)
s.t.
r
X
p=1
xp = c.
(62)
We need to show that any global solution x⋆= [(x1)⋆, . . . , (xr)⋆] to (62) is either x⋆=
[ c
r, . . . , c
r] or x⋆= [xH, . . . , xH, xL] · P for some xH > c
r, xL > 0 and permutation matrix
P ∈Rr×r. Let
L(x, λ) =
r
X
p=1
f(xp) −λ0 ·


r
X
p=1
xp −c

−
r
X
p=1
λpxp
be the Lagragian function for (62) where λ = [λ0, λ1, . . . , λr] is the Lagragian multiplier. By
the ﬁrst order optimality conditions (i.e., the Karush–Kuhn–Tucker (KKT) conditions, see,
e.g., (Nocedal and Wright, 2006b, Theorem 12.1)), there exists λ⋆= [(λ0)⋆, (λ1)⋆, . . . , (λr)⋆]
such that
r
X
p=1
(xq)⋆= c,
(63)
(xq)⋆≥0, ∀q ∈{1, . . . , r},
(64)
(λq)⋆≥0, ∀q ∈{1, . . . , r},
(65)
(λq)⋆· (xq)⋆= 0, ∀q ∈{1, . . . , r},
and
(66)
[f′((x1)⋆), . . . , f′((xr)⋆)] = [(λ0)⋆, . . . , (λ0)⋆] + [(λ1)⋆, . . . , (λr)⋆].
(67)
By using the KKT conditions, we ﬁrst show that all entries of x⋆are strictly positive. To
prove by contradiction, suppose that x⋆has r0 nonzero entries and r −r0 zero entries for
some 1 ≤r0 < r. Note that r0 ≥1 since an all zero vector x⋆does not satisfy the equality
constraint (63).
Without loss of generality, we may assume that (xp)⋆> 0 for p ≤r0 and (xp)⋆= 0
otherwise. By (66), we have
(λ1)⋆= · · · = (λr0)⋆= 0.
Plugging it into (67), we get
f′((x1)⋆) = · · · = f′((xr0)⋆) = (λ0)⋆.
From (67) and noting that xr0+1 = 0 we get
f′(0) = f′(xr0+1) = (λ0)⋆+ (λr0+1)⋆.
Finally, from (65), we have
(λr0+1)⋆≥0.
53

Chan, Yu, You, Qi, Wright, and Ma
Combining the last three equations above gives f′(0) −f′((x1)⋆) ≥0, contradicting the
assumption that f′(0) < f′(x) for all x > 0. This shows that r0 = r, i.e., all entries of x⋆
are strictly positive. Using this fact and (66) gives
(λp)⋆= 0 for all p ∈{1, . . . , r}.
Combining this with (67) gives
f′((x1)⋆) = · · · = f′((xr)⋆) = (λ0)⋆.
(68)
It follows from the fact that f′(x) is strictly unimodal that
∃xH ≥xL > 0 s.t. {(xp)⋆}r
p=1 ⊆{xL, xH}.
(69)
That is, the set {(xp)⋆}r
p=1 may contain no more than two values. To see why this is true,
suppose that there exists three distinct values for {(xp)⋆}r
p=1. Without loss of generality we
may assume that 0 < (x1)⋆< (x2)⋆< (x3)⋆. If (x2)⋆≤xT (recall xT := arg maxx≥0 f′(x)),
then by using the fact that f′(x) is strictly increasing in [0, xT ], we must have f′((x1)⋆) <
f′((x2)⋆) which contradicts (68). A similar contradiction is arrived by considering f′((x2)⋆)
and f′((x3)⋆) for the case where (x2)⋆> xT .
There are two possible cases as a consequence of (69). First, if xL = xH, then we have
(x1)⋆= · · · = (xr)⋆. By further using (63) we get
(x1)⋆= · · · = (xr)⋆= c
r.
It remains to consider the case where xL < xH. First, by the unimodality of f′(x), we
must have xL < xT < xH, therefore
f′′(xL) > 0 and f′′(xH) < 0.
(70)
Let ℓ:= |{p : xp = xL}| be the number of entries of x⋆that are equal to xL and h := r −ℓ.
We show that it is necessary to have ℓ= 1 and h = r −1. To prove by contradiction, assume
that ℓ> 1 and h < r −1. Without loss of generality we may assume {(xp)⋆= xH}h
p=1 and
{(xp)⋆= xL}r
p=h+1. By (70), we have
f′′((xp)⋆) > 0 for all p > h.
In particular, by using h < r −1 we have
f′′((xr−1)⋆) > 0 and f′′((xr)⋆) > 0.
(71)
On the other hand, by using the second order necessary conditions for constraint optimization
(see, e.g., (Nocedal and Wright, 2006b, Theorem 12.5)), the following result holds
v⋆∇xxL(x⋆, λ⋆)v ≤0,
for all


v :
*
∇x


r
X
p=1
(xp)⋆−c

, v
+
= 0



⇐⇒
r
X
p=1
f′′((xp)⋆) · v2
p ≤0,
for all


v = [v1, . . . , vr] :
r
X
p=1
vp = 0


.
(72)
54

ReduNet: A White-box Deep Network from Rate Reduction
Take v to be such that v1 = · · · = vr−2 = 0 and vr−1 = −vr ̸= 0. Plugging it into (72) gives
f′′((xr−1)⋆) + f′′((xr)⋆) ≤0,
which contradicts (71). Therefore, we may conclude that ℓ= 1. That is, x⋆is given by
x⋆= [xH, . . . , xH, xL], where xH > xL > 0.
By using the condition in (63), we may further show that
(r −1)xH + xL = c =⇒xH =
c
r −1 −c
xL
<
xL
r −1,
(r −1)xH + xL = c =⇒(r −1)xH + xH > c =⇒xH > c
r,
which completes our proof.
Proof [Proof of Theorem 12] Without loss of generality, let Z⋆= [Z1
⋆, . . . , Zk
⋆] be the
optimal solution of problem (60).
To show that Zj
⋆, j ∈{1, . . . , k} are pairwise orthogonal, suppose for the purpose of
arriving at a contradiction that (Zj1
⋆)∗Zj2
⋆
̸= 0 for some 1 ≤j1 < j2 ≤k.
By using
Lemma 11, the strict inequality in (59) holds for the optimal solution Z⋆. That is,
∆R(Z⋆, Π, ϵ) <
k
X
j=1
1
2m log


detm 
I +
n
mϵ2 Zj
⋆(Zj
⋆)∗
detmj

I +
n
mjϵ2 Zj
⋆(Zj
⋆)∗


.
(73)
On the other hand, since Pk
j=1 dj ≤n, there exists {U j
⋄∈Rn×dj}k
j=1 such that the columns
of the matrix [U 1
⋄, . . . , U k
⋄] are orthonormal. Denote Zj
⋆= U j
⋆Σj
⋆(V j
⋆)∗the compact SVD of
Zj
⋆, and let
Z⋄= [Z1
⋄, . . . , Zk
⋄],
where Zj
⋄= U j
⋄Σj
⋆(V j
⋆)∗.
It follows that
(Zj1
⋄)∗Zj2
⋄= V j1
⋆Σj1
⋆(U j1
⋄)∗U j2
⋄Σj2
⋆(V j2
⋆)∗= V j1
⋆Σj1
⋆0Σj2
⋆(V j2
⋆)∗= 0
for all 1 ≤j1 < j2 ≤k.
(74)
That is, the matrices Z1
⋄, . . . , Zk
⋄are pairwise orthogonal. Applying Lemma 11 for Z⋄gives
∆R(Z⋄, Π, ϵ) =
k
X
j=1
1
2m log


detm 
I +
n
mϵ2 Zj
⋄(Zj
⋄)∗
detmj

I +
n
mjϵ2 Zj
⋄(Zj
⋄)∗



=
k
X
j=1
1
2m log


detm 
I +
n
mϵ2 Zj
⋆(Zj
⋆)∗
detmj

I +
n
mjϵ2 Zj
⋆(Zj
⋆)∗


,
(75)
where the second equality follows from Lemma 9. Comparing (73) and (75) gives ∆R(Z⋄, Π, ϵ) >
∆R(Z⋆, Π, ϵ), which contradicts the optimality of Z⋆. Therefore, we must have
(Zj1
⋆)∗Zj2
⋆= 0 for all 1 ≤j1 < j2 ≤k.
55

Chan, Yu, You, Qi, Wright, and Ma
Moreover, from Lemma 9 we have
∆R(Z⋆, Π, ϵ) =
k
X
j=1
1
2m log


detm 
I +
n
mϵ2 Zj
⋆(Zj
⋆)∗
detmj

I +
n
mjϵ2 Zj
⋆(Zj
⋆)∗


.
(76)
We now prove the result concerning the singular values of Zj
⋆. To start with, we claim that
the following result holds:
Zj
⋆∈arg max
Zj
log

detm  I +
n
mϵ2 Zj(Zj)∗
detmj

I +
n
mjϵ2 Zj(Zj)∗



s.t. ∥Zj∥2
F = mj, rank(Zj) ≤dj.
(77)
To see why (77) holds, suppose that there exists eZj such that ∥eZj∥2
F = mj, rank( eZj) ≤dj
and
log


detm 
I +
n
mϵ2 eZj( eZj)∗
detmj

I +
n
mjϵ2 eZj( eZj)∗


> log


detm 
I +
n
mϵ2 Zj
⋆(Zj
⋆)∗
detmj

I +
n
mjϵ2 Zj
⋆(Zj
⋆)∗


.
(78)
Denote eZj = eU j eΣj( eV j)∗the compact SVD of eZj and let
Z⋄= [Z1
⋆, . . . , Zj−1
⋆
, Zj
⋄, Zj+1
⋆
, . . . , Zk
⋆],
where Zj
⋄:= U j
⋆eΣj( eV j)∗.
Note that ∥Zj
⋄∥2
F = mj, rank(Zj
⋄) ≤dj and (Zj
⋄)∗Zj′
⋆= 0 for all j′ ̸= j. It follows that Z⋄is
a feasible solution to (60) and that the components of Z⋄are pairwise orthogonal. By using
Lemma 11, Lemma 9 and (78) we have
∆R(Z⋄, Π, ϵ)
=
1
2m log


detm 
I +
n
mϵ2 Zj
⋄(Zj
⋄)∗
detmj

I +
n
mjϵ2 Zj
⋄(Zj
⋄)∗


+
X
j′̸=j
1
2m log


detm 
I +
n
mϵ2 Zj′
⋆(Zj′
⋆)∗
detmj′ 
I +
n
mj′ϵ2 Zj′
⋆(Zj′
⋆)∗



=
1
2m log


detm 
I +
n
mϵ2 eZj( eZj)∗
detmj

I +
n
mjϵ2 eZj( eZj)∗


+
X
j′̸=j
1
2m log


detm 
I +
n
mϵ2 Zj′
⋆(Zj′
⋆)∗
detmj′ 
I +
n
mj′ϵ2 Zj′
⋆(Zj′
⋆)∗



>
1
2m log


detm 
I +
n
mϵ2 Zj
⋆(Zj
⋆)∗
detmj

I +
n
mjϵ2 Zj
⋆(Zj
⋆)∗


+
X
j′̸=j
1
2m log


detm 
I +
n
mϵ2 Zj′
⋆(Zj′
⋆)∗
detmj′ 
I +
n
mj′ϵ2 Zj′
⋆(Zj′
⋆)∗



=
k
X
j=1
1
2m log


detm 
I +
n
mϵ2 Zj
⋆(Zj
⋆)∗
detmj

I +
n
mjϵ2 Zj
⋆(Zj
⋆)∗


.
Combining it with (76) shows ∆R(Z⋄, Π, ϵ) > ∆R(Z⋆, Π, ϵ), contradicting the optimality
of Z⋆. Therefore, the result in (77) holds.
Observe that the optimization problem in (77) depends on Zj only through its singular
values. That is, by letting σj := [σ1,j, . . . , σmin(mj,n),j] be the singular values of Zj, we have
log

detm  I +
n
mϵ2 Zj(Zj)∗
detmj

I +
n
mjϵ2 Zj(Zj)∗


=
min{mj,n}
X
p=1
log
 
(1 +
n
mϵ2 σ2
p,j)m
(1 +
n
mjϵ2 σ2
p,j)mj
!
,
56

ReduNet: A White-box Deep Network from Rate Reduction
also, we have
∥Zj∥2
F =
min{mj,n}
X
p=1
σ2
p,j and rank(Zj) = ∥σj∥0.
Using these relations, (77) is equivalent to
max
σj∈R
min{mj,n}
+
min{mj,n}
X
p=1
log
 
(1 +
n
mϵ2 σ2
p,j)m
(1 +
n
mjϵ2 σ2
p,j)mj
!
s.t.
min{mj,n}
X
p=1
σ2
p,j = mj, and rank(Zj) = ∥σj∥0
(79)
Let (σj)⋆= [(σ1,j)⋆, . . . , (σmin{mj,n},j)⋆] be an optimal solution to (79). Without loss of
generality we assume that the entries of (σj)⋆are sorted in descending order. It follows that
(σp,j)⋆= 0 for all p > dj,
and
[(σ1,j)⋆, . . . , (σdj,j)⋆] =
argmax
[σ1,j,...,σdj,j]∈R
dj
+
σ1,j≥···≥σdj,j
dj
X
p=1
log
 
(1 +
n
mϵ2 σ2
p,j)m
(1 +
n
mjϵ2 σ2
p,j)mj
!
s.t.
dj
X
p=1
σ2
p,j = mj.
(80)
Then we deﬁne
f(x; n, ϵ, mj, m) = log
 
(1 +
n
mϵ2 x)m
(1 +
n
mjϵ2 x)mj
!
,
and rewrite (80) as
max
[x1,...,xdj ]∈R
dj
+
x1≥···≥xdj
dj
X
p=1
f(xp; n, ϵ, mj, m)
s.t.
dj
X
p=1
xp = mj.
(81)
We compute the ﬁrst and second derivative for f with respect to x, which are given by
f′(x; n, ϵ, mj, m) =
n2x(m −mj)
(nx + mϵ2)(nx + mjϵ2),
f′′(x; n, ϵ, mj, m) = n2(m −mj)(mmjϵ4 −n2x2)
(nx + mϵ2)2(nx + mjϵ2)2 .
Note that
• 0 = f′(0) < f′(x) for all x > 0,
• f′(x) is strictly increasing in [0, xT ] and strictly decreasing in [xT , ∞), where xT =
ϵ2q
m
n
mj
n , and
57

Chan, Yu, You, Qi, Wright, and Ma
• by using the condition ϵ4 < mj
m
n2
d2
j , we have f′′(mj
dj ) < 0.
Therefore, we may apply Lemma 13 and conclude that the unique optimal solution to (81)
is either
• x⋆= [mj
dj , . . . , mj
dj ], or
• x⋆= [xH, . . . , xH, xL] for some xH ∈(mj
dj ,
mj
dj−1) and xL > 0.
Equivalently, we have either
• [(σ1,j)⋆, . . . , (σdj,j)⋆] =
hq mj
dj , . . . ,
q mj
dj
i
, or
• [(σ1,j)⋆, . . . , (σdj,j)⋆] = [σH, . . . , σH, σL] for some σH ∈
q mj
dj ,
q mj
dj−1

and σL > 0,
as claimed.
58

ReduNet: A White-box Deep Network from Rate Reduction
Appendix B. ReduNet for 1D Circular Shift Invariance
It has been long known that to implement a convolutional neural network, one can achieve
higher computational eﬃciency by implementing the network in the spectral domain via the
fast Fourier transform (Mathieu et al., 2013; Lavin and Gray, 2016; Vasilache et al., 2015).
However, our purpose here is diﬀerent: We want to show that the linear operators E and
Cj (or ¯E and ¯Cj) derived from the gradient ﬂow of MCR2 are naturally convolutions when
we enforce shift-invariance rigorously. Their convolution structure is derived from the rate
reduction objective, rather than imposed upon the network. Furthermore, the computation
involved in constructing these linear operators has a naturally eﬃcient implementation in
the spectral domain via fast Fourier transform. Arguably this work is the ﬁrst to show
multi-channel convolutions, together with other convolution-preserving nonlinear operations
in the ReduNet, are both necessary and suﬃcient to ensure shift invariance.
To be somewhat self-contained and self-consistent, in this section, we ﬁrst introduce our
notation and review some of the key properties of circulant matrices which will be used to
characterize the properties of the linear operators ¯E and ¯Cj and to compute them eﬃciently.
The reader may refer to Kra and Simanca (2012) for a more rigorous exposition on circulant
matrices.
B.1 Properties of Circulant Matrix and Circular Convolution
Given a vector z = [z(0), z(1), . . . , z(n −1)]∗∈Rn, we may arrange all its circular shifted
versions in a circulant matrix form as
circ(z)
.=


z(0)
z(n −1)
. . .
z(2)
z(1)
z(1)
z(0)
z(n −1)
· · ·
z(2)
...
z(1)
z(0)
...
...
z(n −2)
...
...
...
z(n −1)
z(n −1)
z(n −2)
. . .
z(1)
z(0)


∈Rn×n.
(82)
Fact 1 (Convolution as matrix multiplication via circulant matrix) The multipli-
cation of a circulant matrix circ(z) with a vector x ∈Rn gives a circular (or cyclic)
convolution, i.e.,
circ(z) · x = z ⊛x,
(83)
where
(z ⊛x)i =
n−1
X
j=0
x(j)z(i + n −j mod n).
(84)
Fact 2 (Properties of circulant matrices) Circulant matrices have the following prop-
erties:
• Transpose of a circulant matrix, say circ(z)∗, is circulant;
• Multiplication of two circulant matrices is circulant, for example circ(z)circ(z)∗;
59

Chan, Yu, You, Qi, Wright, and Ma
• For a non-singular circulant matrix, its inverse is also circulant (hence representing a
circular convolution).
These properties of circulant matrices are extensively used in this work as for char-
acterizing the convolution structures of the operators E and Cj. Given a set of vectors
[z1, . . . , zm] ∈Rn×m, let circ(zi) ∈Rn×n be the circulant matrix for zi. Then we have the
following (Proposition 4 in the main body and here restated for convenience):
Proposition 14 (Convolution structures of E and Cj) Given a set of vectors Z =
[z1, . . . , zm], the matrix:
E = α
 I + α
m
X
i=1
circ(zi)circ(zi)∗−1
is a circulant matrix and represents a circular convolution:
Ez = e ⊛z,
where e is the ﬁrst column vector of E. Similarly, the matrices Cj associated with any
subsets of Z are also circular convolutions.
B.2 Circulant Matrix and Circulant Convolution for Multi-channel Signals
In the remainder of this section, we view z as a 1D signal such as an audio signal. Since
we will deal with the more general case of multi-channel signals, we will use the traditional
notation T to denote the temporal length of the signal and C for the number of channels.
Conceptually, the “dimension” n of such a multi-channel signal, if viewed as a vector, should
be n = CT.36 As we will also reveal additional interesting structures of the operators E
and Cj in the spectral domain, we use t as the index for time, p for the index of frequency,
and c for the index of channel.
Given a multi-channel 1D signal ¯z ∈RC×T , we denote
¯z =


¯z[1]∗
...
¯z[C]∗

= [¯z(0), ¯z(1), . . . , ¯z(T −1)] = {¯z[c](t)}c=C,t=T−1
c=1,t=0
.
(85)
To compute the coding rate reduction for a collection of such multi-channel 1D signals, we
may ﬂatten the matrix representation into a vector representation by stacking the multiple
channels of ¯z as a column vector. In particular, we let
vec(¯z) = [¯z[1](0), ¯z[1](1), . . . , ¯z[1](T −1), ¯z[2](0), . . .]
∈R(C×T).
(86)
Furthermore, to obtain shift invariance for the coding rate reduction, we may generate
a collection of shifted copies of ¯z (along the temporal dimension). Stacking the vector
36. Notice that in the main paper, for simplicity, we have used n to indicate both the 1D “temporal” or 2D
“spatial” dimension of a signal, just to be consistent with the vector case, which corresponds to T here.
All notation should be clear within the context.
60

ReduNet: A White-box Deep Network from Rate Reduction
representations for such shifted copies as column vectors, we obtain
circ(¯z) .=


circ(¯z[1])
...
circ(¯z[C])


∈R(C×T)×T .
(87)
In above, we overload the notation “circ(·)” deﬁned in (82).
We now consider a collection of m multi-channel 1D signals {¯zi ∈RC×T }m
i=1. Compactly
representing the data by ¯Z ∈RC×T×m in which the i-th slice on the last dimension is ¯zi,
we denote
¯Z[c] = [¯z1[c], . . . , ¯zm[c]] ∈RT×m,
¯Z(t) = [¯z1(t), . . . , ¯zm(t)] ∈RC×m.
(88)
In addition, we denote
vec( ¯Z) = [vec(¯z1), . . . , vec(¯zm)] ∈R(C×T)×m,
circ( ¯Z) = [circ(¯z1), . . . , circ(¯zm)] ∈R(C×T)×(T×m).
(89)
Then, we deﬁne the shift invariant coding rate reduction for ¯Z ∈RC×T×m as
∆Rcirc( ¯Z, Π) .= 1
T ∆R(circ( ¯Z), ¯Π)
= 1
2T log det
 
I +α·circ( ¯Z)·circ( ¯Z)∗
!
−
k
X
j=1
γj
2T log det
 
I +αj ·circ( ¯Z)· ¯Πj ·circ( ¯Z)∗
!
,
(90)
where α =
CT
mTϵ2 =
C
mϵ2 , αj =
CT
tr(Πj)Tϵ2 =
C
tr(Πj)ϵ2 , γj =
tr(Πj)
m
, and ¯Πj is augmented
membership matrix in an obvious way. Note that we introduce the normalization factor T
in (90) because the circulant matrix circ( ¯Z) contains T (shifted) copies of each signal.
By applying (14) and (15), we obtain the derivative of ∆Rcirc( ¯Z, Π) as
1
2T
∂log det

I + αcirc( ¯Z)circ( ¯Z)∗
∂vec( ¯Z)
= 1
2T
∂log det

I + αcirc( ¯Z)circ( ¯Z)∗
∂circ( ¯Z)
∂circ( ¯Z)
∂vec( ¯Z)
= α

I + αcirc( ¯Z)circ( ¯Z)∗−1
|
{z
}
¯
E ∈R(C×T )×(C×T )
vec( ¯Z),
(91)
γj
2T
∂log det

I + αjcirc( ¯Z)Πjcirc( ¯Z)∗
∂vec( ¯Z)
= γj αj

I + αjcirc( ¯Z)Πjcirc( ¯Z)∗−1
|
{z
}
¯
Cj ∈R(C×T )×(C×T )
vec( ¯Z)Πj.
(92)
In the following, we show that ¯E · vec(¯z) represents a multi-channel circular convolution.
Note that
¯E = α


I+α Pm
i=1 circ(zi[1])circ(zi[1])∗···
Pm
i=1 circ(zi[1])circ(zi[C])∗
...
...
...
Pm
i=1 circ(zi[C])circ(zi[1])∗
··· I+Pm
i=1 αcirc(zi[C])circ(zi[C])∗


−1
.
(93)
61

Chan, Yu, You, Qi, Wright, and Ma
By using Fact 2, the matrix in the inverse above is a block circulant matrix, i.e., a block
matrix where each block is a circulant matrix. A useful fact about the inverse of such a
matrix is the following.
Fact 3 (Inverse of block circulant matrices) The inverse of a block circulant matrix
is a block circulant matrix (with respect to the same block partition).
The main result of this subsection is the following (Proposition 5 in the main body and
here restated for convenience):
Proposition 15 (Convolution structures of ¯E and ¯Cj) Given a collection of multi-
channel 1D signals {¯zi ∈RC×T }m
i=1, the matrix ¯E is a block circulant matrix, i.e.,
¯E .=


¯E1,1
· · ·
¯E1,C
...
...
...
¯EC,1
· · ·
¯EC,C

,
(94)
where each ¯Ec,c′ ∈RT×T is a circulant matrix. Moreover, ¯E represents a multi-channel
circular convolution, i.e., for any multi-channel signal ¯z ∈RC×T we have
¯E · vec(¯z) = vec(¯e ⊛¯z).
In above, ¯e ∈RC×C×T is a multi-channel convolutional kernel with ¯e[c, c′] ∈RT being the
ﬁrst column vector of ¯Ec,c′, and ¯e ⊛¯z ∈RC×T is the multi-channel circular convolution
(with “⊛” overloading the notation from Eq. (84)) deﬁned as
(¯e ⊛¯z)[c] =
C
X
c′=1
¯e[c, c′] ⊛¯z[c′],
∀c = 1, . . . , C.
(95)
Similarly, the matrices ¯Cj associated with any subsets of ¯Z are also multi-channel circular
convolutions.
Note that the calculation of ¯E in (93) requires inverting a matrix of size (C ×T)×(C ×T).
In the following, we show that this computation can be accelerated by working in the frequency
domain.
B.3 Fast Computation in Spectral Domain
Circulant matrix and Discrete Fourier Transform.
A remarkable property of circu-
lant matrices is that they all share the same set of eigenvectors that form a unitary matrix.
We deﬁne the matrix:
FT .=
1
√
T


ω0
T
ω0
T
· · ·
ω0
T
ω0
T
ω0
T
ω1
T
· · ·
ωT−2
T
ωT−1
T
...
...
...
...
...
ω0
T
ωT−2
T
· · ·
ω(T−2)2
T
ω(T−2)(T−1)
T
ω0
T
ωT−1
T
· · ·
ω(T−2)(T−1)
T
ω(T−1)2
T


∈CT×T ,
(96)
62

ReduNet: A White-box Deep Network from Rate Reduction
where ωT .= exp(−2π√−1
T
) is the roots of unit (as ωT
T = 1). The matrix FT is a unitary
matrix: FT F ∗
T = I and is the well known Vandermonde matrix. Multiplying a vector with
FT is known as the discrete Fourier transform (DFT). Be aware that the conventional DFT
matrix diﬀers from our deﬁnition of FT here by a scale: it does not have the
1
√
T in front.
Here for simplicity, we scale it so that FT is a unitary matrix and its inverse is simply its
conjugate transpose F ∗
T , columns of which represent the eigenvectors of a circulant matrix
(Abidi et al., 2016).
Fact 4 (DFT as matrix-vector multiplication) The DFT of a vector z ∈RT can be
computed as
DFT(z) .= FT · z
∈CT ,
(97)
where
DFT(z)(p) =
1
√
T
T−1
X
t=0
z(t) · ωp·t
T ,
∀p = 0, 1, . . . , T −1.
(98)
The Inverse Discrete Fourier Transform (IDFT) of a signal v ∈CT can be computed as
IDFT(v) .= F ∗
T · v
∈CT
(99)
where
IDFT(v)(t) =
1
√
T
T−1
X
p=0
v(p) · ω−p·t
T
,
∀t = 0, 1, . . . , T −1.
(100)
Regarding the relationship between a circulant matrix (convolution) and discrete Fourier
transform, we have:
Fact 5 An n × n matrix M ∈Cn×n is a circulant matrix if and only if it is diagonalizable
by the unitary matrix Fn:
FnMF ∗
n = D
or
M = F ∗
nDFn,
(101)
where D is a diagonal matrix of eigenvalues.
Fact 6 (DFT are eigenvalues of the circulant matrix) Given a vector z ∈CT , we
have
FT · circ(z) · F ∗
T = diag(DFT(z))
or
circ(z) = F ∗
T · diag(DFT(z)) · FT .
(102)
That is, the eigenvalues of the circulant matrix associated with a vector are given by its DFT.
Fact 7 (Parseval’s theorem) Given any z ∈CT , we have ∥z∥2 = ∥DFT(z)∥2. More
precisely,
T−1
X
t=0
|z[t]|2 =
T−1
X
p=0
|DFT(z)[p]|2.
(103)
This property allows us to easily “normalize” features after each layer onto the sphere Sn−1
directly in the spectral domain (see Eq. (26) and (128)).
63

Chan, Yu, You, Qi, Wright, and Ma
Circulant matrix and Discrete Fourier Transform for multi-channel signals.
We
now consider multi-channel 1D signals ¯z ∈RC×T . Let DFT(¯z) ∈CC×T be a matrix where
the c-th row is the DFT of the corresponding signal z[c], i.e.,
DFT(¯z) .=


DFT(z[1])∗
...
DFT(z[C])∗


∈CC×T .
(104)
Similar to the notation in (85), we denote
DFT(¯z) =


DFT(¯z)[1]∗
...
DFT(¯z)[C]∗


= [DFT(¯z)(0), DFT(¯z)(1), . . . , DFT(¯z)(T −1)]
= {DFT(¯z)[c](t)}c=C,t=T−1
c=1,t=0
.
(105)
As such, we have DFT(z[c]) = DFT(¯z)[c].
By using Fact 6, circ(¯z) and DFT(¯z) are related as follows:
circ(¯z) =


F ∗
T · diag(DFT(z[1])) · FT
...
F ∗
T · diag(DFT(z[C])) · FT

=


F ∗
T
· · ·
0
...
...
...
0
· · ·
F ∗
T

·


diag(DFT(z[1]))
...
diag(DFT(z[C]))

·FT . (106)
We now explain how this relationship can be leveraged to produce a fast computation of ¯E
deﬁned in (91). First, there exists a permutation matrix P such that


diag(DFT(z[1]))
diag(DFT(z[2]))
...
diag(DFT(z[C]))

= P ·


DFT(¯z)(0)
0
· · ·
0
0
DFT(¯z)(1)
· · ·
0
...
...
...
...
0
0
· · ·
DFT(¯z)(T −1)

.
(107)
Combining (106) and (107), we have
circ(¯z) · circ(¯z)∗=


F ∗
T
· · ·
0
...
...
...
0
· · ·
F ∗
T

· P · D(¯z) · P∗·


FT
· · ·
0
...
...
...
0
· · ·
FT

,
(108)
where
D(¯z) .=


DFT(¯z)(0) · DFT(¯z)(0)∗
· · ·
0
...
...
...
0
· · ·
DFT(¯z)(T −1) · DFT(¯z)(T −1)∗

.
(109)
Now, consider a collection of m multi-channel 1D signals ¯Z ∈RC×T×m. Similar to the
notation in (88), we denote
DFT( ¯Z)[c] = [DFT(¯z1)[c], . . . , DFT(¯zm)[c]] ∈RT×m,
DFT( ¯Z)(p) = [DFT(¯z1)(p), . . . , DFT(¯zm)(p)] ∈RC×m.
(110)
64

ReduNet: A White-box Deep Network from Rate Reduction
By using (108), we have
¯E =


F ∗
T
· · ·
0
...
...
...
0
· · ·
F ∗
T

· P · α ·
"
I + α ·
m
X
i=1
D(¯zi)
#−1
· P∗·


FT
· · ·
0
...
...
...
0
· · ·
FT

.
(111)
Note that α ·

I + α · Pm
i=1 D(¯zi)
−1 is equal to
α
" I+αDFT( ¯
Z)(0)·DFT( ¯
Zi)(0)∗···
0
...
...
...
0
··· I+αDFT( ¯
Z)(T−1)·DFT( ¯
Z)(T−1)∗
#−1
=


α(I+αDFT( ¯
Z)(0)·DFT( ¯
Z)(0)∗)
−1 ···
0
...
...
...
0
··· α(I+αDFT( ¯
Z)(T−1)·DFT( ¯
Z)(T−1)∗)
−1

.
(112)
Therefore, the calculation of ¯E only requires inverting T matrices of size C × C. This
motivates us to construct the ReduNet in the spectral domain for the purpose of accelerating
the computation, as we explain next.
Shift-invariant ReduNet in the Spectral Domain.
Motivated by the result in (112),
we introduce the notations ¯E(p) ∈RC×C×T and ¯Cj(p) ∈RC×C×T given by
¯E(p)
.=
α ·

I + α · DFT( ¯Z)(p) · DFT( ¯Z)(p)∗−1
∈CC×C,
(113)
¯Cj(p)
.=
αj ·

I + αj · DFT( ¯Z)(p) · Πj · DFT( ¯Z)(p)∗−1
∈CC×C.
(114)
In above, ¯E(p) (resp., ¯Cj(p)) is the p-th slice of ¯E (resp., ¯Cj) on the last dimension. Then,
the gradient of ∆Rcirc( ¯Z, Π) with respect to ¯Z can be calculated by the following result
(Theorem 6 in the main body and here restated for convenience).
Theorem 16 (Computing multi-channel convolutions ¯E and ¯Cj) Let ¯U ∈CC×T×m
and ¯
W j ∈CC×T×m, j = 1, . . . , k be given by
¯U(p)
.=
¯E(p) · DFT( ¯Z)(p),
(115)
¯
W j(p)
.=
¯Cj(p) · DFT( ¯Z)(p),
j = 1, . . . , k,
(116)
for each p ∈{0, . . . , T −1}. Then, we have
1
2T
∂log det(I + α · circ( ¯Z)circ( ¯Z)∗)
∂¯Z
=
IDFT( ¯U),
(117)
γj
2T
∂log det(I + αj · circ( ¯Z) ¯Πjcirc( ¯Z)∗)
∂¯Z
=
γj · IDFT( ¯
W jΠj).
(118)
Proof [Also proof to Theorem 6 in the main body]
65

Chan, Yu, You, Qi, Wright, and Ma
From (14), (106) and (111), we have
1
2
∂log det

I + αcirc( ¯Z)circ( ¯Z)∗
∂circ(¯zi)
= ¯Ecirc(¯zi) = ¯E
" F ∗
T ···
0
... ... ...
0
··· F ∗
T
# " diag(DFT(zi[1]))
...
diag(DFT(zi[C]))
#
FT
(119)
=
" F ∗
T ···
0
... ... ...
0
··· F ∗
T
#
· P · α ·
"
I + α ·
X
i
D(¯zi)
#−1
·
" DFT(¯zi)(0) ···
0
...
...
...
0
··· DFT(¯zi)(T−1)
#
· FT
(120)
=
" F ∗
T ···
0
... ... ...
0
··· F ∗
T
#
· P ·
" ¯E(0)·DFT(¯zi)(0) ···
0
...
...
...
0
···
¯E(T−1)·DFT(¯zi)(T−1)
#
· FT
(121)
=
" F ∗
T ···
0
... ... ...
0
··· F ∗
T
#
· P ·
" ¯ui(0) ···
0
...
...
...
0
···
¯ui(T−1)
#
· FT =
" F ∗
T ···
0
... ... ...
0
··· F ∗
T
#
·
" diag(¯ui[1])
...
diag(¯ui[C])
#
· FT
(122)
= circ(IDFT(¯ui)).
(123)
Therefore, we have
1
2
∂log det

I + α · circ( ¯Z) · circ( ¯Z)∗
∂¯zi
= 1
2
∂log det

I + α · circ( ¯Z) · circ( ¯Z)∗
∂circ(¯zi)
· ∂circ(¯zi)
∂¯zi
= T · IDFT(¯ui).
(124)
By collecting the results for all i, we have
∂1
2T log det

I + α · circ( ¯Z) · circ( ¯Z)∗
∂¯Z
= IDFT( ¯U).
(125)
In a similar fashion, we get
∂γj
2T log det

I + αj · circ( ¯Z) · ¯Πj · circ( ¯Z)∗
∂¯Z
= γj · IDFT( ¯
W j · Πj).
(126)
By the above theorem, the gradient ascent update in (13) (when applied to ∆Rcirc( ¯Z, Π))
can be equivalently expressed as an update in frequency domain on ¯Vℓ.= DFT( ¯Zℓ) as
¯Vℓ+1(p) ∝¯Vℓ(p) + η

¯Eℓ(p) · ¯Vℓ(p) −
k
X
j=1
γj ¯Cj
ℓ(p) · ¯Vℓ(p)Πj
,
p = 0, . . . , T −1.
(127)
Similarly, the gradient-guided feature map increment in (26) can be equivalently expressed
as an update in frequency domain on ¯vℓ.= DFT(¯zℓ) as
¯vℓ+1(p) ∝¯vℓ(p) + η · ¯Eℓ(p)¯vℓ(p) −η · σ

[ ¯C1
ℓ(p)¯vℓ(p), . . . , ¯Ck
ℓ(p)¯vℓ(p)]

,
p = 0, . . . , T −1,
(128)
66

ReduNet: A White-box Deep Network from Rate Reduction
Algorithm 3 Training Algorithm (1D Signal, Shift Invariance, Spectral Domain)
Input: ¯Z ∈RC×T×m, Π, ϵ > 0, λ, and a learning rate η.
1: Set α =
C
mϵ2 , {αj =
C
tr(Πj)ϵ2 }k
j=1, {γj =
tr(Πj)
m
}k
j=1.
2: Set ¯V1 = {¯vi
1(p) ∈CC}T−1,m
p=0,i=1
.= DFT( ¯Z) ∈CC×T×m.
3: for ℓ= 1, 2, . . . , L do
4:
# Step 1:Compute network parameters Eℓand {Cj
ℓ}k
j=1.
5:
for p = 0, 1, . . . , T −1 do
6:
¯Eℓ(p) .= α ·

I + α · ¯Vℓ(p) · ¯Vℓ(p)∗−1,
¯Cj
ℓ(p) .= αj ·

I + αj · ¯Vℓ(p) · Πj · ¯Vℓ(p)∗−1;
7:
end for
8:
# Step 2:Update feature ¯V .
9:
for i = 1, . . . , m do
10:
# Compute (approximately) projection at each frequency p.
11:
for p = 0, 1, . . . , T −1 do
12:
Compute {¯pij
ℓ(p) .= ¯Cj
ℓ(p) · ¯vi
ℓ(p) ∈CC×1}k
j=1;
13:
end for
14:
# Compute overall (approximately) projection by aggregating over
frequency p.
15:
Let {¯Pij
ℓ= [¯pij
ℓ(0), . . . , ¯pij
ℓ(T −1)] ∈CC×T }k
j=1;
16:
# Compute soft assignment from (approximately) projection.
17:
Compute
n
bπij
ℓ=
exp(−λ∥¯Pij
ℓ∥F )
Pk
j=1 exp(−λ∥¯Pij
ℓ∥F )
ok
j=1;
18:
# Compute update at each frequency p.
19:
for p = 0, 1, . . . , T −1 do
20:
¯vi
ℓ+1(p) = PSn−1

¯vi
ℓ(p) + η

¯Eℓ(p)¯vi
ℓ(p) −Pk
j=1 γj · bπij
ℓ· ¯Cj
ℓ(p) · ¯vi
ℓ(p)

;
21:
end for
22:
end for
23:
Set ¯Zℓ+1 = IDFT( ¯Vℓ+1) as the feature at the ℓ-th layer;
24: end for
Output: features ¯ZL+1, the learned ﬁlters { ¯Eℓ(p)}ℓ,p and { ¯Cj
ℓ(p)}j,ℓ,p.
subject to the constraint that ∥¯vℓ+1∥F = ∥¯zℓ+1∥F = 1 (the ﬁrst equality follows from Fact 7).
We summarize the training, or construction to be more precise, of ReduNet in the
spectral domain in Algorithm 3.
Appendix C. ReduNet for 2D Circular Translation Invariance
To a large degree, both conceptually and technically, the 2D case is very similar to the 1D
case that we have studied carefully in the previous Appendix B. For the sake of consistency
and completeness, we here give a brief account.
67

Chan, Yu, You, Qi, Wright, and Ma
C.1 Doubly Block Circulant Matrix
In this section, we consider z as a 2D signal such as an image, and use H and W to denote
its “height” and “width”, respectively. It will be convenient to work with both a matrix
representation
z =


z(0, 0)
z(0, 1)
· · ·
z(0, W −1)
z(1, 0)
z(1, 1)
· · ·
z(1, W −1)
...
...
...
...
z(H −1, 0)
z(H −1, 1)
· · ·
z(H −1, W −1)


∈RH×W ,
(129)
as well as a vector representation
vec(z) .=
h
z(0, 0), . . . , z(0, W −1), z(1, 0), . . . ,
z(1, W −1), . . . , z(H −1, 0), . . . , z(H −1, W −1)
i∗
∈R(H×W).
(130)
We represent the circular translated version of z as transp,q(z) ∈RH×W by an amount of p
and q on the vertical and horizontal directions, respectively. That is, we let
transp,q(z)(h, w) .= z(h −p mod H, w −q mod W),
(131)
where ∀(h, w) ∈{0, . . . , H −1} × {0, . . . , W −1}.
It is obvious that trans0,0(z) = z.
Moreover, there is a total number of H×W distinct translations given by {transp,q(z), (p, q) ∈
{0, . . . , H −1} × {0, . . . , W −1}}. We may arrange the vector representations of them into
a matrix and obtain
circ(z) .=
h
vec(trans0,0(z)), . . . , vec(trans0,W−1(z)), vec(trans1,0(z)), . . . , vec(trans1,W−1(z)),
. . . ,
vec(transH−1,0(z)), . . . , vec(transH−1,W−1(z))
i
∈R(H×W)×(H×W).
(132)
The matrix circ(z) is known as the doubly block circulant matrix associated with z (see, e.g.,
Abidi et al. (2016); Sedghi et al. (2019)).
We now consider a multi-channel 2D signal represented as a tensor ¯z ∈RC×H×W , where
C is the number of channels. The c-th channel of ¯z is represented as ¯z[c] ∈RH×W , and the
(h, w)-th pixel is represented as ¯z(h, w) ∈RC. To compute the coding rate reduction for a
collection of such multi-channel 2D signals, we may ﬂatten the tenor representation into a
vector representation by concatenating the vector representation of each channel, i.e., we let
vec(¯z) = [vec(¯z[1])∗, . . . , vec(¯z[C])∗]∗
∈R(C×H×W)
(133)
Furthermore, to obtain shift invariance for coding rate reduction, we may generate a collection
of translated versions of ¯z (along two spatial dimensions). Stacking the vector representation
for such translated copies as column vectors, we obtain
circ(¯z) .=


circ(¯z[1])
...
circ(¯z[C])


∈R(C×H×W)×(H×W).
(134)
68

ReduNet: A White-box Deep Network from Rate Reduction
We can now deﬁne a translation invariant coding rate reduction for multi-channel 2D
signals. Consider a collection of m multi-channel 2D signals {¯zi ∈RC×H×W }m
i=1. Compactly
representing the data by ¯Z ∈RC×H×W×m where the i-th slice on the last dimension is ¯zi,
we denote
circ( ¯Z) = [circ(¯z1), . . . , circ(¯zm)]
∈R(C×H×W)×(H×W×m).
(135)
Then, we deﬁne
∆Rcirc( ¯Z, Π) .=
1
HW ∆R(circ( ¯Z), ¯Π) =
1
2HW log det
 
I + α · circ( ¯Z) · circ( ¯Z)∗
!
−
k
X
j=1
γj
2HW log det
 
I + αj · circ( ¯Z) · ¯Πj · circ( ¯Z)∗
!
,
(136)
where α =
CHW
mHWϵ2 =
C
mϵ2 , αj =
CHW
tr(Πj)HWϵ2 =
C
tr(Πj)ϵ2 , γj =
tr(Πj)
m
, and ¯Πj is augmented
membership matrix in an obvious way.
By following an analogous argument as in the 1D case, one can show that ReduNet for
multi-channel 2D signals naturally gives rise to the multi-channel 2D circulant convolution
operations. We omit the details, and focus on the construction of ReduNet in the frequency
domain.
C.2 Fast Computation in Spectral Domain
Doubly block circulant matrix and 2D-DFT.
Similar to the case of circulant matrices
for 1D signals, all doubly block circulant matrices share the same set of eigenvectors, and
these eigenvectors form a unitary matrix given by
F .= FH ⊗FW
∈C(H×W)×(H×W),
(137)
where ⊗denotes the Kronecker product and FH, FW are deﬁned as in (96).
Analogous to Fact 4, F deﬁnes 2D-DFT as follows.
Fact 8 (2D-DFT as matrix-vector multiplication) The 2D-DFT of a signal z ∈RH×W
can be computed as
vec(DFT(z)) .= F · vec(z)
∈C(H×W),
(138)
where ∀(p, q) ∈{0, . . . , H −1} × {0, . . . , W −1},
DFT(z)(p, q) =
1
√
H · W
H−1
X
h=0
W−1
X
w=0
z(h, w) · ωp·h
H ωq·w
W .
(139)
The 2D-IDFT of a signal v ∈CH×W can be computed as
vec(IDFT(v)) .= F ∗
T · vec(v)
∈C(H×W),
(140)
where ∀(h, w) ∈{0, . . . , H −1} × {0, . . . , W −1},
IDFT(v)(h, w) =
1
√
H · W
H−1
X
p=0
W−1
X
q=0
v(p, q) · ω−p·h
H
ω−q·w
W
.
(141)
69

Chan, Yu, You, Qi, Wright, and Ma
Analogous to Fact 9, F relates DFT(z) and circ(z) as follows.
Fact 9 (2D-DFT are eigenvalues of the doubly block circulant matrix) Given a sig-
nal z ∈CH×W , we have
F · circ(z) · F ∗= diag(vec(DFT(z)))
or
circ(z) = F ∗· diag(vec(DFT(z))) · F .
(142)
Doubly block circulant matrix and 2D-DFT for multi-channel signals.
We now
consider multi-channel 2D signals ¯z ∈RC×H×W . Let DFT(¯z) ∈CC×H×W be a matrix
where the c-th slice on the ﬁrst dimension is the DFT of the corresponding signal z[c]. That
is, DFT(¯z)[c] = DFT(z[c]) ∈CH×W . We use DFT(¯z)(p, q) ∈CC to denote slicing of ¯z on
the frequency dimensions.
By using Fact 9, circ(¯z) and DFT(¯z) are related as follows:
circ(¯z) =


F ∗· diag(vec(DFT(z[1]))) · F
...
F ∗· diag(vec(DFT(z[C]))) · F


=


F ∗
· · ·
0
0
· · ·
0
...
...
...
0
· · ·
F ∗

·


diag(vec(DFT(z[1])))
diag(vec(DFT(z[2])))
...
diag(vec(DFT(z[C])))

· F .
(143)
Similar to the 1D case, this relation can be leveraged to produce a fast implementation of
ReduNet in the spectral domain.
Translation-invariant ReduNet in the Spectral Domain.
Given a collection of
multi-channel 2D signals ¯Z ∈RC×H×W×m, we denote
DFT( ¯Z)(p, q) .= [DFT(¯z1)(p, q), . . . , DFT(¯zm)(p, q)]
∈RC×m.
(144)
We introduce the notations ¯E(p, q) ∈RC×C×H×W and ¯Cj(p, q) ∈RC×C×H×W given by
¯E(p, q)
.=
α ·

I + α · DFT( ¯Z)(p, q) · DFT( ¯Z)(p, q)∗−1
∈CC×C,
(145)
¯Cj(p, q)
.=
αj ·

I + αj · DFT( ¯Z)(p, q) · Πj · DFT( ¯Z)(p, q)∗−1
∈CC×C.
(146)
In above, ¯E(p, q) (resp., ¯Cj(p, q)) is the (p, q)-th slice of ¯E (resp., ¯Cj) on the last two
dimensions. Then, the gradient of ∆Rcirc( ¯Z, Π) with respect to ¯Z can be calculated by the
following result.
Theorem 17 (Computing multi-channel 2D convolutions ¯E and ¯Cj) Suppose ¯U ∈
CC×H×W×m and ¯
W j ∈CC×H×W×m, j = 1, . . . , k are given by
¯U(p, q)
.=
¯E(p, q) · DFT( ¯Z)(p, q),
(147)
¯
W j(p, q)
.=
¯Cj(p, q) · DFT( ¯Z)(p, q),
j = 1, . . . , k,
(148)
70

ReduNet: A White-box Deep Network from Rate Reduction
for each (p, q) ∈{0, . . . , H −1} × {0, . . . , W −1}. Then, we have
1
2HW
∂log det(I + α · circ( ¯Z)circ( ¯Z)∗)
∂¯Z
=
IDFT( ¯U),
(149)
1
2HW
∂
 γj log det(I + αj · circ( ¯Z) ¯Πjcirc( ¯Z)∗)

∂¯Z
=
γj · IDFT( ¯
W jΠj).
(150)
This result shows that the calculation of the derivatives for the 2D case is analogous to that
of the 1D case. Therefore, the construction of the ReduNet for 2D translation invariance
can be performed using Algorithm 3 with straightforward extensions.
71

Chan, Yu, You, Qi, Wright, and Ma
Appendix D. Additional Simulations and Experiments for MCR2
D.1 Simulations - Verifying Diversity Promoting Properties of MCR2
As proved in Theorem 12, the proposed MCR2 objective promotes within-class diversity.
In this section, we use simulated data to verify the diversity promoting property of MCR2.
As shown in Table 4, we calculate our proposed MCR2 objective on simulated data. We
observe that orthogonal subspaces with higher dimension achieve higher MCR2 value, which
is consistent with our theoretical analysis in Theorem 12.
D.2 Implementation Details
Training Setting.
We mainly use ResNet-18 (He et al., 2016) in our experiments, where
we use 4 residual blocks with layer widths {64, 128, 256, 512}. The implementation of network
architectures used in this paper are mainly based on this github repo.37 For data augmenta-
tion in the supervised setting, we apply the RandomCrop and RandomHorizontalFlip. For
the supervised setting, we train the models for 500 epochs and use stage-wise learning rate
decay every 200 epochs (decay by a factor of 10). For the supervised setting, we train the
models for 100 epochs and use stage-wise learning rate decay at 20-th epoch and 40-th epoch
(decay by a factor of 10).
Evaluation Details.
For the supervised setting, we set the number of principal compo-
nents for nearest subspace classiﬁer rj = 30. We also study the eﬀect of rj in Section D.3.2.
For the CIFAR100 dataset, we consider 20 superclasses and set the cluster number as 20,
which is the same setting as in Chang et al. (2017); Wu et al. (2018).
Datasets.
We apply the default datasets in PyTorch, including CIFAR10, CIFAR100, and
STL10.
Augmentations T used for the self-supervised setting.
We apply the same data
augmentation for CIFAR10 dataset and CIFAR100 dataset and the pseudo-code is as follows.
import torchvision.transforms as transforms
TRANSFORM = transforms.Compose([
transforms.RandomResizedCrop(32),
transforms.RandomHorizontalFlip(),
transforms.RandomApply([transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)], p=0.8),
transforms.RandomGrayscale(p=0.2),
transforms.ToTensor()])
The augmentations we use for STL10 dataset and the pseudo-code is as follows.
37. https://github.com/kuangliu/pytorch-cifar
72

ReduNet: A White-box Deep Network from Rate Reduction
R
Rc
∆R
Orthogonal?
Output Dimension
Random Gaussian
552.70
193.29
360.41

512
Subspace (dj = 50)
545.63
108.46
437.17

512
Subspace (dj = 40)
487.07
92.71
394.36

512
Subspace (dj = 30)
413.08
74.84
338.24

512
Subspace (dj = 20)
318.52
54.48
264.04

512
Subspace (dj = 10)
195.46
30.97
164.49

512
Subspace (dj = 1)
31.18
4.27
26.91

512
Random Gaussian
292.71
154.13
138.57

256
Subspace (dj = 25)
288.65
56.34
232.31

256
Subspace (dj = 20)
253.51
47.58
205.92

256
Subspace (dj = 15)
211.97
38.04
173.93

256
Subspace (dj = 10)
161.87
27.52
134.35

256
Subspace (dj = 5)
98.35
15.55
82.79

256
Subspace (dj = 1)
27.73
3.92
23.80

256
Random Gaussian
150.05
110.85
39.19

128
Subspace (dj = 12)
144.36
27.72
116.63

128
Subspace (dj = 10)
129.12
24.06
105.05

128
Subspace (dj = 8)
112.01
20.18
91.83

128
Subspace (dj = 6)
92.55
16.04
76.51

128
Subspace (dj = 4)
69.57
11.51
58.06

128
Subspace (dj = 2)
41.68
6.45
35.23

128
Subspace (dj = 1)
24.28
3.57
20.70

128
Subspace (dj = 50)
145.60
75.31
70.29

128
Subspace (dj = 40)
142.69
65.68
77.01

128
Subspace (dj = 30)
135.42
54.27
81.15

128
Subspace (dj = 20)
120.98
40.71
80.27

128
Subspace (dj = 15)
111.10
32.89
78.21

128
Subspace (dj = 12)
101.94
27.73
74.21

128
Table 4: MCR2 objective on simulated data.
We evaluate the proposed MCR2 objective
deﬁned in (11), including R, Rc, and ∆R, on simulated data. The output dimension d is
set as 512, 256, and 128. We set the batch size as m = 1000 and random assign the label
of each sample from 0 to 9, i.e., 10 classes. We generate two types of data: 1) (Random
Gaussian) For comparison with data without structures, for each class we generate random
vectors sampled from Gaussian distribution (the dimension is set as the output dimension
d) and normalize each vector to be on the unit sphere. 2) (Subspace) For each class, we
generate vectors sampled from its corresponding subspace with dimension dj and normalize
each vector to be on the unit sphere. We consider the subspaces from diﬀerent classes are
orthogonal/nonorthogonal to each other.
import torchvision.transforms as transforms
TRANSFORM = transforms.Compose([
transforms.RandomResizedCrop(96),
transforms.RandomHorizontalFlip(),
transforms.RandomApply([transforms.ColorJitter(0.8, 0.8, 0.8, 0.2)], p=0.8),
transforms.RandomGrayscale(p=0.2),
GaussianBlur(kernel_size=9),
transforms.ToTensor()])
73

Chan, Yu, You, Qi, Wright, and Ma
Cross-entropy training details.
For CE models presented in Table 1, Figure 17(d)-
17(f), and Figure 18, we use the same network architecture, ResNet-18 (He et al., 2016), for
cross-entropy training on CIFAR10, and set the output dimension as 10 for the last layer.
We apply SGD, and set learning rate lr=0.1, momentum momentum=0.9, and weight decay
wd=5e-4. We set the total number of training epoch as 400, and use stage-wise learning rate
decay every 150 epochs (decay by a factor of 10).
D.3 Additional Experimental Results
D.3.1 PCA Results of MCR2 Training versus Cross-Entropy Training
0
5
10
15
20
25
30
Components
0
5
10
15
20
25
30
Sigular Values
(a) PCA: MCR2 training learned
features for overall data (ﬁrst 30
components).
0
20
40
60
80
100
120
Components
0
5
10
15
20
25
30
Sigular Values
(b) PCA: MCR2 training learned
features for overall data.
0
5
10
15
20
25
30
Components
0
5
10
15
20
25
Sigular Values
(c) PCA: MCR2 training learned
features for every class.
0
5
10
15
20
25
30
Components
0
10
20
30
40
50
60
70
Sigular Values
(d) PCA: cross-entropy training
learned features for overall data
(ﬁrst 30 components).
0
20
40
60
80
100
120
Components
0
10
20
30
40
50
60
70
Sigular Values
(e) PCA: cross-entropy training
learned features for overall data.
0
5
10
15
20
25
30
Components
0
2
4
6
8
10
12
Sigular Values
(f) PCA: cross-entropy training
learned features for every class.
Figure 17: Principal component analysis (PCA) of learned representations for the MCR2 trained
model (ﬁrst row) and the cross-entropy trained model (second row).
For comparison, similar to Figure 12(c), we calculate the principle components of
representations learned by MCR2 training and cross-entropy training. For cross-entropy
training, we take the output of the second last layer as the learned representation. The
results are summarized in Figure 17. We also compare the cosine similarity between learned
representations for both MCR2 training and cross-entropy training, and the results are
presented in Figure 18.
As shown in Figure 17, we observe that representations learned by MCR2 are much
more diverse, the dimension of learned features (each class) is around a dozen, and the
74

ReduNet: A White-box Deep Network from Rate Reduction
Figure 18: Cosine similarity between learned features by using the MCR2 objective (left) and CE
loss (right).
airplane 
automobile 
bird 
cat 
deer 
dog 
frog 
horse 
ship 
truck
(a) 10 representative images from each class based
on top-10 principal components of the SVD of
learned representations by MCR2.
airplane 
automobile 
bird 
cat 
deer 
dog 
frog 
horse 
ship 
truck
(b) Randomly selected 10 images from each class.
Figure 19: Visualization of top-10 “principal” images for each class in the CIFAR10 dataset. (a)
For each class-j, we ﬁrst compute the top-10 singular vectors of the SVD of the learned
features Zj. Then for the l-th singular vector of class j, ul
j, and for the feature of the
i-th image of class j, zi
j, we calculate the absolute value of inner product, |⟨zi
j, ul
j⟩|, then
we select the largest one for each singular vector within class j. Each row corresponds
to one class, and each image corresponds to one singular vector, ordered by the value of
the associated singular value. (b) For each class, 10 images are randomly selected in the
dataset. These images are the ones displayed in the CIFAR dataset website (Krizhevsky,
2009).
75

Chan, Yu, You, Qi, Wright, and Ma
dimension of the overall features is nearly 120, and the output dimension is 128. In contrast,
the dimension of the overall features learned using entropy is slightly greater than 10, which
is much smaller than that learned by MCR2. From Figure 18, for MCR2 training, we ﬁnd
that the features of diﬀerent class are almost orthogonal.
Visualize representative images selected from CIFAR10 dataset by using MCR2.
As mentioned in Section 1.3, obtaining the properties of desired representation in the proposed
MCR2 principle is equivalent to performing nonlinear generalized principle components on
the given dataset. As shown in Figure 17(a)-17(c), MCR2 can indeed learn such diverse and
discriminative representations. In order to better interpret the representations learned by
MCR2, we select images according to their “principal” components (singular vectors using
SVD) of the learned features. In Figure 13, we visualize images selected from class-‘Bird’
and class-‘Ship’. For each class, we ﬁrst compute top-10 singular vectors of the SVD of the
learned features and then for each of the top singular vectors, we display in each row the
top-10 images whose corresponding features are closest to the singular vector. As shown in
Figure 13, we observe that images in the same row share many common characteristics such
as shapes, textures, patterns, and styles, whereas images in diﬀerent rows are signiﬁcantly
diﬀerent from each other—suggesting our method captures all the diﬀerent “modes” of the
data even within the same class. Notice that top rows are associated with components with
larger singular values, hence they are images that show up more frequently in the dataset.
In Figure 19(a), we visualize the 10 “principal” images selected from CIFAR10 for each
of the 10 classes. That is, for each class, we display the 10 images whose corresponding
features are most coherent with the top-10 singular vectors. We observe that the selected
images are much more diverse and representative than those selected randomly from the
dataset (displayed on the CIFAR oﬃcial website), indicating such principal images can be
used as a good “summary” of the dataset.
D.3.2 Experimental Results of MCR2 in the Supervised Learning Setting.
Training details for mainline experiment.
For the model presented in Figure 1
(Right) and Figure 12, we use ResNet-18 to parameterize f(·, θ), and we set the out-
put dimension d = 128, precision ϵ2 = 0.5, mini-batch size m = 1, 000. We use SGD in
Pytorch (Paszke et al., 2019) as the optimizer, and set the learning rate lr=0.01, weight
decay wd=5e-4, and momentum=0.9.
Experiments for studying the eﬀect of hyperparameters and architectures.
We
present the experimental results of MCR2 training in the supervised setting by using various
training hyperparameters and diﬀerent network architectures. The results are summarized
in Table 5. Besides the ResNet architecture, we also consider VGG architecture (Simonyan
and Zisserman, 2015) and ResNext achitecture (Xie et al., 2017b). From Table 5, we ﬁnd
that larger batch size m can lead to better performance. Also, models with higher output
dimension d require larger training batch size m.
Eﬀect of rj on classiﬁcation.
Unless otherwise stated, we set the number of components
rj = 30 for nearest subspace classiﬁcation.
We study the eﬀect of rj when used for
classiﬁcation, and the results are summarized in Table 6. We observe that the nearest
subspace classiﬁcation works for a wide range of rj.
76

ReduNet: A White-box Deep Network from Rate Reduction
Arch
Dim n
Precision ϵ2
BatchSize m
lr
ACC
Comment
ResNet-18
128
0.5
1,000
0.01
0.922
Mainline, Fig 12
ResNext-29
128
0.5
1,000
0.01
0.925
Different
Architecture
VGG-11
128
0.5
1,000
0.01
0.907
ResNet-18
512
0.5
1,000
0.01
0.886
Effect of
Output
Dimension
ResNet-18
256
0.5
1,000
0.01
0.921
ResNet-18
64
0.5
1,000
0.01
0.922
ResNet-18
128
1.0
1,000
0.01
0.930
Effect of
precision
ResNet-18
128
0.4
1,000
0.01
0.919
ResNet-18
128
0.2
1,000
0.01
0.900
ResNet-18
128
0.5
500
0.01
0.823
Effect of
Batch Size
ResNet-18
128
0.5
2,000
0.01
0.930
ResNet-18
128
0.5
4,000
0.01
0.925
ResNet-18
512
0.5
2,000
0.01
0.924
ResNet-18
512
0.5
4,000
0.01
0.921
ResNet-18
128
0.5
1,000
0.05
0.860
Effect of lr
ResNet-18
128
0.5
1,000
0.005
0.923
ResNet-18
128
0.5
1,000
0.001
0.922
Table 5: Experiments of MCR2 in the supervised setting on the CIFAR10 dataset.
Number of components
rj = 10
rj = 20
rj = 30
rj = 40
rj = 50
Mainline (Label Noise Ratio=0.0)
0.926
0.925
0.922
0.923
0.921
Label Noise Ratio=0.1
0.917
0.917
0.911
0.918
0.917
Label Noise Ratio=0.2
0.906
0.906
0.897
0.906
0.905
Label Noise Ratio=0.3
0.882
0.879
0.881
0.881
0.881
Label Noise Ratio=0.4
0.864
0.866
0.866
0.867
0.864
Label Noise Ratio=0.5
0.839
0.841
0.843
0.841
0.837
Table 6: Eﬀect of number of components rj for nearest subspace classiﬁcation in the supervised
setting.
Eﬀect of ϵ2 on learning from corrupted labels.
To further study the proposed MCR2
on learning from corrupted labels, we use diﬀerent precision parameters, ϵ2 = 0.75, 1.0, in
addition to the one shown in Table 1. Except for the precision parameter ϵ2, all the other
parameters are the same as the mainline experiment (the ﬁrst row in Table 5). The ﬁrst
row (ϵ2 = 0.5) in Table 7 is identical to the MCR2 training in Table 10. Notice that with
slightly diﬀerent choices in ϵ2, one might even see slightly improved performance over the
ones reported in the main body.
77

Chan, Yu, You, Qi, Wright, and Ma
Precision
Ratio=0.1
Ratio=0.2
Ratio=0.3
Ratio=0.4
Ratio=0.5
ϵ2 = 0.5
0.911
0.897
0.881
0.866
0.843
ϵ2 = 0.75
0.923
0.908
0.899
0.876
0.836
ϵ2 = 1.0
0.919
0.911
0.896
0.870
0.845
Table 7: Eﬀect of Precision ϵ2 on classiﬁcation results with features learned with labels corrupted
at diﬀerent levels by using MCR2 training.
D.4 Comparison with Related Work on Label Noise
We compare the proposed MCR2 with OLE (Lezama et al., 2018), Large Margin Deep Net-
works (Elsayed et al., 2018), and ITLM (Shen and Sanghavi, 2019) in label noise robustness
experiments on CIFAR10 dataset. In Table 8, we compare MCR2 with OLE (Lezama et al.,
2018) and Large Margin Deep Networks (Elsayed et al., 2018) on the corrupted label task
using the same network, MCR2 achieves signiﬁcant better performance. We compare MCR2
with ITLM (Shen and Sanghavi, 2019) using the same network. MCR2 achieves better
performance without any noise ratio dependent hyperparameters as required by Shen and
Sanghavi (2019).
ResNet18
Ratio=0.1
Ratio=0.2
Ratio=0.3
Ratio=0.4
Ratio=0.5
OLE
0.910
0.860
0.806
0.717
0.610
LargeMargin
0.901
0.874
0.837
0.785
0.724
MCR2
0.911
0.897
0.881
0.866
0.843
WRN16
Ratio=0.1
Ratio=0.3
Ratio=0.5
Ratio=0.7
ITLM
0.903
0.882
0.825
0.647
MCR2
0.915
0.888
0.842
0.670
Table 8: Comparison with related work (OLE (Lezama et al., 2018), LargeMargin (Elsayed et al.,
2018), ITLM (Shen and Sanghavi, 2019)) on learning from noisy labels.
D.5 Learning from Gaussian noise corrupted data.
We investigate the performance of MCR2 training with corrupted data by adding varying
levels of Gaussian noise. For each corruption level, we add N(0, σ2) to the input images with
diﬀerent standard deviations σ ∈{0.04, 0.06, 0.08, 0.09, 0.1} as in Hendrycks and Dietterich
(2018). We train the same architecture ResNet-18 as the previous experiments for 500
epochs, set mini-batch size to m = 1000 and optimize using SGD with learning rate lr=0.01,
momentum momentum=0.9 and weight decay wd=5e-4. We also decrease the learning rate
to 0.001 at epoch 200 and to 0.0001 at epoch 400. In our objective, we set precision
ϵ2 = 0.5. To compare the performance of MCR2 versus cross-entropy (CE), we train the
same architecture using the cross-entropy loss for 200 epochs and optimize using SGD with
78

ReduNet: A White-box Deep Network from Rate Reduction
learning rate lr=0.1, momentum momentum=0.9 and weight decay wd=5e-4. We also use
Cosine Annealing learning rate scheduler during training. We show the respective testing
accuracy in Table 9. Although the classiﬁcation result of using MCR2 slightly lags behind
that of using CE, when the noise level is small, their performances are comparable with
each other. Similar sensitivity to the noise indicates that the reason might be because of the
choice of the same network architecture. We reserve the study on how to improve robustness
to input noise for future work.
Noise level
σ = 0.04
σ = 0.06
σ = 0.08
σ = 0.09
σ = 0.1
CE Training
0.912
0.897
0.876
0.867
0.857
MCR2 Training
0.909
0.882
0.869
0.855
0.829
Table 9: Classiﬁcation results of features learned with inputs corrupted by Gaussian noise at diﬀerent
levels.
D.6 Experimental Results of MCR2 in the Self-supervised Learning Setting
D.6.1 Self-supervised Learning of Invariant Features
Learning invariant features via rate reduction. Motivated by self-supervised learning
algorithms (LeCun et al., 2004; Kavukcuoglu et al., 2009; Oord et al., 2018; He et al., 2019;
Wu et al., 2018), we use the MCR2 principle to learn representations that are invariant
to certain class of transformations/augmentations, say T with a distribution PT . Given a
mini-batch of data {xj}k
j=1 with mini-batch size equals to k, we augment each sample xj
with n transformations/augmentations {τ i(·)}n
i=1 randomly drawn from PT . We simply label
all the augmented samples Xj = [τ1(xj), . . . , τn(xj)] of xj as the j-th class, and Zj the
corresponding learned features. Using this self-labeled data, we train our feature mapping
f(·, θ) the same way as the supervised setting above. For every mini-batch, the total number
of samples for training is m = kn.
Evaluation via clustering. To learn invariant features, our formulation itself does not
require the original samples xj come from a ﬁxed number of classes. For evaluation, we
may train on a few classes and observe how the learned features facilitate classiﬁcation
or clustering of the data. A common method to evaluate learned features is to train an
additional linear classiﬁer (Oord et al., 2018; He et al., 2019), with ground truth labels. But
for our purpose, because we explicitly verify whether the so-learned invariant features have
good subspace structures when the samples come from k classes, we use an oﬀ-the-shelf
subspace clustering algorithm EnSC (You et al., 2016), which is computationally eﬃcient
and is provably correct for data with well-structured subspaces. We also use K-Means on
the original data X as our baseline for comparison. We use normalized mutual information
(NMI), clustering accuracy (ACC), and adjusted rand index (ARI) for our evaluation metrics,
see Appendix D.6.2 for their detailed deﬁnitions.
Controlling dynamics of expansion and compression. By directly optimizing the
rate reduction ∆R = R −Rc, we achieve 0.570 clustering accuracy on CIFAR10 dataset,
which is the second best result compared with previous methods. Empirically, we observe
79

Chan, Yu, You, Qi, Wright, and Ma
0
10000
20000
30000
40000
50000
Number of iterations
0
1
2
3
4
5
6
Loss
∆R
R
Rcc
(a) MCR2
0
10000
20000
30000
40000
50000
Number of iterations
0
2
4
6
8
10
Loss
∆R
R
Rcc
(b) MCR2-CTRL.
Figure 20: Evolution of the rates of (left) MCR2 and (right) MCR2-CTRL in the training process
in the self-supervised setting on CIFAR10 dataset.
that, without class labels, the overall coding rate R expands quickly and the MCR2 loss
saturates (at a local maximum), see Fig 20(a). Our experience suggests that learning a
good representation from unlabeled data might be too ambitious when directly optimizing
the original ∆R. Nonetheless, from the geometric meaning of R and Rc, one can design
a diﬀerent learning strategy by controlling the dynamics of expansion and compression
diﬀerently during training. For instance, we may re-scale the rate by replacing R(Z, ϵ) with
eR(Z, ϵ) .=
1
2γ1
log det(I + γ2d
mϵ2 ZZ∗).
(151)
With γ1 = γ2 = k, the learning dynamics change from Figure 20(a) to Figure 20(b): All
features are ﬁrst compressed then gradually expand. We denote the controlled MCR2
training by MCR2-CTRL.
Experiments on real data. Similar to the supervised learning setting, we train exactly
the same ResNet-18 network on the CIFAR10, CIFAR100, and STL10 (Coates et al., 2011)
datasets. We set the mini-batch size as k = 20, number of augmentations for each sample as
n = 50 and the precision parameter as ϵ2 = 0.5. Table 10 shows the results of the proposed
MCR2-CTRL in comparison with methods JULE (Yang et al., 2016), RTM (Nina et al.,
2019), DEC (Xie et al., 2016), DAC (Chang et al., 2017), and DCCM (Wu et al., 2019)
that have achieved the best results on these datasets. Surprisingly, without utilizing any
inter-class or inter-sample information and heuristics on the data, the invariant features
learned by our method with augmentations alone achieves a better performance over other
highly engineered clustering methods. More comparisons and ablation studies can be found
in Appendix D.6.2.
Nevertheless, compared to the representations learned in the supervised setting where
the optimal partition Π in (11) is initialized by correct class information, the representations
here learned with self-supervised classes are far from being optimal. It remains wide open
how to design better optimization strategies and dynamics to learn from unlabelled or
80

ReduNet: A White-box Deep Network from Rate Reduction
Dataset
Metric
K-Means
JULE
RTM
DEC
DAC
DCCM
MCR2-Ctrl
CIFAR10
NMI
0.087
0.192
0.197
0.257
0.395
0.496
0.630
ACC
0.229
0.272
0.309
0.301
0.521
0.623
0.684
ARI
0.049
0.138
0.115
0.161
0.305
0.408
0.508
CIFAR100
NMI
0.084
0.103
-
0.136
0.185
0.285
0.387
ACC
0.130
0.137
-
0.185
0.237
0.327
0.375
ARI
0.028
0.033
-
0.050
0.087
0.173
0.178
STL10
NMI
0.124
0.182
-
0.276
0.365
0.376
0.446
ACC
0.192
0.182
-
0.359
0.470
0.482
0.491
ARI
0.061
0.164
-
0.186
0.256
0.262
0.290
Table 10: Clustering results on CIFAR10, CIFAR100, and STL10 datasets.
partially-labelled data better representations (and the associated partitions) close to the
global maxima of the MCR2 objective (11).
Training details of MCR2-CTRL.
For three datasets (CIFAR10, CIFAR100, and STL10),
we use ResNet-18 as in the supervised setting, and we set the output dimension d = 128,
precision ϵ2 = 0.5, mini-batch size k = 20, number of augmentations n = 50, γ1 = γ2 = 20.
We observe that MCR2-CTRL can achieve better clustering performance by using smaller γ2,
i.e., γ2 = 15, on CIFAR10 and CIFAR100 datasets. We use SGD as the optimizer, and set
the learning rate lr=0.1, weight decay wd=5e-4, and momentum=0.9.
Training dynamic comparison between MCR2 and MCR2-CTRL.
In the self-supervised
setting, we compare the training process for MCR2 and MCR2-CTRL in terms of R, eR, Rc,
and ∆R. For MCR2 training shown in Figure 20(a), the features ﬁrst expand (for both R
and Rc) then compress (for Rc). For MCR2-CTRL, both eR and Rc ﬁrst compress then eR
expands quickly and Rc remains small, as we have seen in Figure 20(b).
Clustering results comparison.
We compare the clustering performance between MCR2
and MCR2-CTRL in terms of NMI, ACC, and ARI. The clustering results are summarized in
Table 11. We ﬁnd that MCR2-CTRL can achieve better performance for clustering.
NMI
ACC
ARI
MCR2
0.544
0.570
0.399
MCR2-Ctrl
0.630
0.684
0.508
Table 11: Clustering comparison between MCR2 and MCR2-CTRL on CIFAR10 dataset.
D.6.2 Clustering Metrics and More Results
We ﬁrst introduce the deﬁnitions of normalized mutual information (NMI) (Strehl and
Ghosh, 2002), clustering accuracy (ACC), and adjusted rand index (ARI) (Hubert and
Arabie, 1985).
81

Chan, Yu, You, Qi, Wright, and Ma
Normalized mutual information (NMI).
Suppose Y is the ground truth partition
and C is the prediction partition. The NMI metric is deﬁned as
NMI(Y, C) =
Pk
i=1
Ps
j=1 |Yi ∩Cj| log
 m|Yi∩Cj|
|Yi||Cj|

rPk
i=1 |Yi| log

|Yi|
m
 Ps
j=1 |Cj| log
 |Cj|
m
,
where Yi is the i-th cluster in Y and Cj is the j-th cluster in C, and m is the total number
of samples.
Clustering accuracy (ACC).
Given m samples, {(xi, yi)}m
i=1. For the i-th sample xi,
let yi be its ground truth label, and let ci be its cluster label. The ACC metric is deﬁned as
ACC(Y , C) = max
σ∈S
Pm
i=1 1{yi = σ(ci)}
m
,
where S is the set includes all the one-to-one mappings from cluster to label, and Y =
[y1, . . . , ym], C = [c1, . . . , cm].
Adjusted rand index (ARI).
Suppose there are m samples, and let Y and C be two
clustering of these samples, where Y = {Y1, . . . , Yr} and C = {C1, . . . , Cs}. Let mij denote
the number of the intersection between Yi and Cj, i.e., mij = |Yi ∩Cj|. The ARI metric is
deﬁned as
ARI =
P
ij
 mij
2

−
P
i
 ai
2
 P
j
 bj
2
  m
2

1
2
P
i
 ai
2

+ P
j
 bj
2

−
P
i
 ai
2
 P
j
 bj
2
  m
2
,
where ai = P
j mij and bj = P
i mij.
Comparison with Ji et al. (2019); Hu et al. (2017).
We compare MCR2 with IIC (Ji
et al., 2019) and IMSAT (Hu et al., 2017) in Table 12. We ﬁnd that MCR2 outperforms
IIC (Ji et al., 2019) and IMSAT (Hu et al., 2017) on both CIFAR10 and CIFAR100 by a
large margin. For STL10, Hu et al. (2017) applied pretrained ImageNet models and Ji et al.
(2019) used more data for training.
Dataset
Metric
IIC
IMSAT
MCR2-Ctrl
CIFAR10
NMI
-
-
0.630
ACC
0.617
0.456
0.684
ARI
-
-
0.508
CIFAR100
NMI
-
-
0.387
ACC
0.257
0.275
0.375
ARI
-
-
0.178
Table 12: Compare with Ji et al. (2019); Hu et al. (2017) on clustering.
82

ReduNet: A White-box Deep Network from Rate Reduction
More experiments on the eﬀect of hyperparameters of MCR2-CTRL.
We provide
more experimental results of MCR2-CTRL training in the self-supervised setting by varying
training hyperparameters on the STL10 dataset. The results are summarized in Table 13.
Notice that the choice of hyperparameters only has small eﬀect on the performance with
the MCR2-CTRL objective. We may hypothesize that, in order to further improve the
performance, one has to seek other, potentially better, control of optimization dynamics or
strategies. We leave those for future investigation.
Arch
Precision ϵ2
Learning Rate lr
NMI
ACC
ARI
ResNet-18
0.5
0.1
0.446
0.491
0.290
ResNet-18
0.75
0.1
0.450
0.484
0.288
ResNet-18
0.25
0.1
0.447
0.489
0.293
ResNet-18
0.5
0.2
0.477
0.473
0.295
ResNet-18
0.5
0.05
0.444
0.496
0.293
ResNet-18
0.25
0.05
0.454
0.489
0.294
Table 13: Experiments of MCR2-CTRL in the self-supervised setting on STL10 dataset.
83

Chan, Yu, You, Qi, Wright, and Ma
Appendix E. Implementation Details and Additional Experiments for
ReduNets
In this section, we provide additional experimental results related to ReduNet in Section 5.
Obviously, in this work we have chosen a simplest design of the ReduNet and do not
particularly optimize any of the hyper parameters, such as the number of initial channels,
kernel sizes, normalization, and learning rate etc., for the best performance or scalability.
The choices are mostly for convenience and just minimally adequate to verify the concept.
We leave all such practical matters for us and others to investigate in the future.
We ﬁrst present the visualization of rotated and translated images of the MNIST dataset
in Section E.1. We provide additional experimental results on rotational invariance in
Section E.2 and translational invariance in Section E.3. In Section E.4, we provide more
results on learning mixture of Gaussians.
E.1 Visualization of Rotation and Translation on MNIST
In this subsection, we present the visualization of rotation and translation images on the
MNIST dataset. The rotation examples are shown in Figure 21 and the translation examples
are shown in Figure 22.
Figure 21: Examples of rotated images of MNIST digits, each by 18◦. (Left) Diagram for
polar coordinate representation; (Right) Rotated images of digit ‘0’ and ‘1’.
Figure 22: (Left) A torus on which 2D cyclic translation is deﬁned; (Right) Cyclic translated
images of MNIST digits ‘0’ and ‘1’ (with stride=7).
84

ReduNet: A White-box Deep Network from Rate Reduction
E.2 Additional Experiments on Learning Rotational Invariance on MNIST
Eﬀect of channel size.
We study the eﬀect of the number of channels on the rotation
invariant task on the MNIST dataset. We apply the same parameters as in Figure 16(b)
except that we vary the channel size from 5 to 20, where the number of channels used
in Figure 16(b) is 20. We summarize the results in Table 14, Figure 23 and Figure 24.
From Table 14, we ﬁnd that the invariance training accuracy increases when we increase
the channel size. As we can see from Figure 23, the shifted learned features become more
orthogonal across diﬀerent classes when the channel size is large. Also, we observe that with
more channel size, the training loss converges faster (in Figure 24).
Channel
5
10
15
20
Training Acc
1.000
1.000
1.000
1.000
Test Acc
0.560
0.610
0.610
0.610
Invariance Training Acc
0.838
0.972
0.990
1.000
Invariance Test Acc
0.559
0.609
0.610
0.610
Table 14: Training accuracy of 1D rotation-invariant ReduNet with diﬀerent number of
channels on the MNIST dataset.
0
400
800
1200
1600
2000
0
400
800
1200
1600
2000
0.5
1.0
(a) 5 Channels
0
400
800
1200
1600
2000
0
400
800
1200
1600
2000
0.5
1.0
(b) 10 Channels
0
400
800
1200
1600
2000
0
400
800
1200
1600
2000
0.5
1.0
(c) 15 Channels
0
400
800
1200
1600
2000
0
400
800
1200
1600
2000
0.0
0.5
1.0
(d) 20 Channels
Figure 23: Heatmaps of cosine similarity among shifted learned features (with diﬀerent
channel sizes) ¯Zshift for rotation invariance on the MNIST dataset (RI-MNIST).
0
5
10
15
20
25
30
35
40
Layers
0.00
0.02
0.04
0.06
0.08
Loss
R  (train)
R  (test)
R (train)
R (test)
Rc (train)
Rc (test)
(a) 5 Channels
0
5
10
15
20
25
30
35
40
Layers
0.000
0.025
0.050
0.075
0.100
0.125
0.150
0.175
Loss
R  (train)
R  (test)
R (train)
R (test)
Rc (train)
Rc (test)
(b) 10 Channels
0
5
10
15
20
25
30
35
40
Layers
0.05
0.10
0.15
0.20
0.25
Loss
R  (train)
R  (test)
R (train)
R (test)
Rc (train)
Rc (test)
(c) 15 Channels
0
5
10
15
20
25
30
35
40
Layers
0.05
0.10
0.15
0.20
0.25
0.30
0.35
Loss
R  (train)
R  (test)
R (train)
R (test)
Rc (train)
Rc (test)
(d) 20 Channels
Figure 24: Training and test losses of rotational invariant ReduNet for rotation invariance
on the MNIST dataset (RI-MNIST).
85

Chan, Yu, You, Qi, Wright, and Ma
E.3 Additional Experiments on Learning 2D Translation Invariance on
MNIST
Eﬀect of channel size.
We study the eﬀect of the number of channels on the translation
invariant task on the MNIST dataset. We use the same parameters as in Figure 16(f) except
that we vary the channel size from 5 to 75, where the number of channels used in Figure 16(f)
is 75. Similar to the observations in Section E.2, in Table 15, we observe that both the
invariance training accuracy and invariance test accuracy increase when we increase the
channel size. From Figure 25 and Figure 26, we ﬁnd that when we increase the number of
channels, the shifted learned features become more orthogonal across diﬀerent classes and
the training loss converges faster.
Channel
5
15
35
55
75
Training Acc
1.000
1.000
1.000
1.000
1.000
Test Acc
0.680
0.610
0.670
0.770
0.840
Invariance Training Acc
0.879
0.901
0.933
0.933
0.976
Invariance Test Acc
0.619
0.599
0.648
0.767
0.838
Table 15: Training/test accuracy of 2D translation-invariant ReduNet with diﬀerent number of
channels on the MNIST dataset.
0
400
800
1200
1600
2000
0
400
800
1200
1600
2000
0.5
1.0
(a) 5 Channels
0
400
800
1200
1600
0
400
800
1200
1600
0.5
1.0
(b) 15 Channels
0
400
800
1200
1600
0
400
800
1200
1600
0.0
0.5
1.0
(c) 35 Channels
0
400
800
1200
1600
0
400
800
1200
1600
0.0
0.5
1.0
(d) 55 Channels
0
400
800
1200
1600
0
400
800
1200
1600
0.0
0.5
1.0
(e) 75 Channels
Figure 25: Heatmaps of cosine similarity among shifted learned features (with diﬀerent
channel sizes) ¯Zshift for translation invariance on the MNIST dataset (TI-MNIST).
0
5
10
15
20
25
Layers
0.000
0.005
0.010
0.015
0.020
0.025
Loss
R  (train)
R  (test)
R (train)
R (test)
Rc (train)
Rc (test)
(a) 5 Channels
0
5
10
15
20
25
Layers
0.00
0.01
0.02
0.03
0.04
0.05
0.06
0.07
Loss
R  (train)
R  (test)
R (train)
R (test)
Rc (train)
Rc (test)
(b) 15 Channels
0
5
10
15
20
25
Layers
0.00
0.02
0.04
0.06
0.08
0.10
0.12
0.14
0.16
Loss
R  (train)
R  (test)
R (train)
R (test)
Rc (train)
Rc (test)
(c) 35 Channels
0
5
10
15
20
25
Layers
0.00
0.05
0.10
0.15
0.20
0.25
Loss
R  (train)
R  (test)
R (train)
R (test)
Rc (train)
Rc (test)
(d) 55 Channels
0
5
10
15
20
25
Layers
0.00
0.05
0.10
0.15
0.20
0.25
0.30
0.35
Loss
∆R (train)
∆R (test)
R (train)
R (test)
Rc (train)
Rc (test)
(e) 75 Channels
Figure 26: Training and test losses of translation invariant ReduNet for translation invariance
on the MNIST dataset (TI-MNIST).
86

ReduNet: A White-box Deep Network from Rate Reduction
E.4 Additional Experiments on Learning Mixture of Gaussians in S1 and S2
Additional experiments on S1 and S2.
We also provide additional experiments on
learning mixture of Gaussians in S1 and S2 in Figure 27. We can observe similar behavior of
the proposed ReduNet: the network can map data points from diﬀerent classes to orthogonal
subspaces.
−1.0
−0.5
0.0
0.5
1.0
−1.0
−0.5
0.0
0.5
1.0
0
250
500
0
250
500
0.5
1.0
(a) X(2D) (left:
scatter plot; right:
cosine similarity visualization)
−1.0
−0.5
0.0
0.5
1.0
−1.0
−0.5
0.0
0.5
1.0
0
250
500
0
250
500
0.5
1.0
(b) Z(2D) (left:
scatter plot; right:
cosine similarity visualization)
0
250 500 750 1000 1250 1500 1750 2000
Layers
0.0
0.5
1.0
1.5
2.0
2.5
Loss
∆R (train)
R (train)
Rc (train)
∆R (test)
R (test)
Rc (test)
(c) Loss
−1.00
−0.75
−0.50
−0.25
0.00
0.25
0.50
0.75
1.00
−1.00−0.75−0.50−0.250.00 0.25 0.50 0.75 1.00
−1.00
−0.75
−0.50
−0.25
0.00
0.25
0.50
0.75
1.00
0
250
500
750
0
250
500
750
0.5
1.0
(d) X(3D) (left:
scatter plot; right:
cosine similarity visualization)
−1.00
−0.75
−0.50
−0.25
0.00
0.25
0.50
0.75
1.00
−1.00−0.75−0.50−0.250.00 0.25 0.50 0.75 1.00
−1.00
−0.75
−0.50
−0.25
0.00
0.25
0.50
0.75
1.00
0
250
500
750
0
250
500
750
0.5
1.0
(e) Z(3D) (left: scatter plot; right: co-
sine similarity visualization)
0
250 500 750 1000 1250 1500 1750 2000
Layers
0.0
0.5
1.0
1.5
2.0
2.5
3.0
3.5
4.0
Loss
∆R (train)
∆R (test)
R (train)
R (test)
Rc (train)
Rc (test)
(f) Loss
Figure 27: Learning mixture of Gaussians in S1 and S2. (Top) For S1, we set σ1 = σ2 = 0.1;
(Bottom) For S2, we set σ1 = σ2 = σ3 = 0.1.
Additional experiments on S1 with more than 2 classes.
We try to apply ReduNet
to learn mixture of Gaussian distributions on S1 with the number of class is larger than 2.
Notice that these are the cases to which Theorem 1 no longer applies. These experiments
suggest that the MCR2 still promotes between-class discriminativeness with so constructed
ReduNet. In particular, the case on the left of Figure 28 indicates that the ReduNet has
“merged” two linearly correlated clusters into one on the same line. This is consistent with
the objective of rate reduction to group data as linear subspaces.
−1.0
−0.5
0.0
0.5
1.0
−1.0
−0.5
0.0
0.5
1.0
−1.0
−0.5
0.0
0.5
1.0
−1.0
−0.5
0.0
0.5
1.0
(a) 3 classes. (Left) X; (Right) Z
−1.0
−0.5
0.0
0.5
1.0
−1.0
−0.5
0.0
0.5
1.0
−1.0
−0.5
0.0
0.5
1.0
−1.0
−0.5
0.0
0.5
1.0
(b) 6 classes. (Left) X; (Right) Z
Figure 28: Learning mixture of Gaussian distributions with more than 2 classes. For both
cases, we use step size η = 0.5 and precision ϵ = 0.1. For (a), we set iteration
L = 2, 500; for (b), we set iteration L = 4, 000.
87

Chan, Yu, You, Qi, Wright, and Ma
E.5 Additional Experimental Results of ReduNet and Scattering Transform
Comparing ResNet and ReduNet on maximizing ∆R using CIFAR10.
We com-
pare the objective value reached by training a ResNet-18 versus constructing a ReduNet
that maximizes the MCR2 objective in an iterative fashion. For training ResNet-18, we use
the same setting as Section 5.1, with mini-batch size m = 1000 and precision ϵ2 = 0.5. For
training ReduNet, we ﬁrst apply scattering transform to the training samples, then ﬂatten
the scatter-transformed features and project them to a 128-dimensional feature vector using
a random linear projection. Then we construct a 4000-layer ReduNet with precision ϵ2 = 0.1,
step size η = 0.5 and λ = 500. We vary the scales {23, 24} and angles {4, 8, 16} in scattering
transforms, but set the phase to 4 for all sets of experiments. In both cases, we use all
50000 training samples. Our results are listed in Table 16. Empirically, features learned
by training a ResNet-18 with MCR2 achieves a higher ∆R of 48.65 than those learned by
constructing a ReduNet, achieving a ∆R of 46.14.
Scales
Angles
∆R
R
Rc
23
4
46.1418
66.0627
19.9208
23
8
46.3207
66.1358
19.8151
23
16
46.4162
66.1474
19.7312
24
4
40.5377
58.8128
18.2751
24
8
41.3374
60.1895
18.8521
24
16
42.5311
61.5164
18.9854
ResNet-18
48.6497
69.2653
20.6155
Table 16: Objective values under scattering transform with varying scales and angles.
Comparing ReduNet with random ﬁlters and scattering transform.
Here we
provide comparisons of using ReduNet on MNIST digits that are lifted by scattering transform
versus by random ﬁlters. We select m = 5000 training samples (500 from each class), and set
precision ϵ2 = 0.1 and step size η = 0.1. We vary the number of layers for each comparison,
as the numbers of layers needed for the rate reduction objective to converge also varies. For
each comparison, we construct two networks: 1). We apply scattering transform to each
training sample, then ﬂatten the feature vector and construct a ReduNet, and 2). We apply
a number random ﬁlters, such that the resulting dimensions of the feature are the same. We
ablate across diﬀerent scales {23, 24} and diﬀerent angles {4, 8, 16}, but keep the number of
phases to 4 the same. To compute the testing accuracy of each architecture, we use all 10000
testing samples in the MNIST dataset. We list details about the feature dimensions and test
accuracies in Table 17. From these empirical results, we have shown that using scattering
transform outperforms using random ﬁlters in all scenarios. This implies that scattering
transform is better at making the original data samples more separable than the random
ﬁlters. With a more suitable lifting operator, ReduNet is able to linearize low-dimensional
structure into incoherent subspaces. This also showcases that scattering transform and
ReduNet complement each other when learning diverse and discrminative features.
88

ReduNet: A White-box Deep Network from Rate Reduction
Lifting
n
L
Acc
Scattering (23 scales, 4 angles)
441
150
0.9734
Random ﬁlters
441
150
0.9225
Scattering (23 scales, 8 angles)
873
100
0.9781
Random ﬁlters
873
100
0.9153
Scattering (23 scales, 16 angles)
1737
50
0.9795
Random ﬁlters
1737
50
0.9061
Scattering (24 scales, 4 angles)
65
400
0.8952
Random ﬁlters
65
400
0.8794
Scattering (24 scales, 8 angles)
129
300
0.9392
Random ﬁlters
129
300
0.9172
Scattering (24 scales, 16 angles)
257
200
0.9569
Random ﬁlters
257
200
0.8794
Table 17: Lifting by Scattering transform versus by random ﬁlters. The dimension of the
feature vector n and the number of layers used in the ReduNet L are also stated.
E.6 Equivariance of learned features using Translation-Invariant ReduNet
We investigate how features learned using Translational-Invariant ReduNet possess equiv-
ariant properties. More speciﬁcally, we construct a ReduNet using 500 training samples of
MNIST digits (50 from each class) with number of layers L = 30, precision ϵ2 = 0.1, step
size η = 0.5, and λ = 500. We also apply 2D circulant convolution to the (1-channel) inputs
with 16 random Gaussian kernels of size 7 × 7.
To evaluate its equivariant properties, we augment each training and test sample by
shifting 7 pixels in each canonical direction, resulting in 9 augmented images for each original
image. As shown in Figure 29, we compute the distance between the representation of
a (shifted) test sample and representations of training samples (including all translation
augmentations) using cosine similarity. By computing the top-9 largest inner products, we
observe that each augmented test sample is closest to a training sample with the similar
translation.
89

Chan, Yu, You, Qi, Wright, and Ma
(a) Class 3
(b) Class 7
Figure 29: Veriﬁcation of Equivariance in ReduNet. For each class of samples, and for
all training samples (and their augmentations) zi and a test sample (and its
augmentations) ˆz, we compute their inner product |⟨ˆz, zi⟩|, ∀i. We select the top-
9 largest inner products (sorted from left to right), and visualize their respective
image. The test sample and its augmented version are highlighted in red.
References
Mongi A Abidi, Andrei V Gribok, and Joonki Paik. Optimization Techniques in Computer
Vision. Springer, 2016.
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via
over-parameterization. In International Conference on Machine Learning, pages 242–252.
PMLR, 2019.
Marcin Andrychowicz, Misha Denil, Sergio Gomez, Matthew W Hoﬀman, David Pfau,
Tom Schaul, Brendan Shillingford, and Nando De Freitas. Learning to learn by gradient
descent by gradient descent. In Advances in neural information processing systems, pages
3981–3989, 2016.
Sanjeev Arora, Nadav Cohen, Wei Hu, and Yuping Luo. Implicit regularization in deep
matrix factorization.
In Advances in Neural Information Processing Systems, pages
7411–7422, 2019.
Aharon Azulay and Yair Weiss. Why do deep convolutional networks generalize so poorly
to small image transformations? arXiv preprint arXiv:1805.12177, 2018.
Jimmy Lei Ba, Jamie Ryan Kiros, and Geoﬀrey E Hinton. Layer normalization. arXiv
preprint arXiv:1607.06450, 2016.
90

ReduNet: A White-box Deep Network from Rate Reduction
Bowen Baker, Otkrist Gupta, N. Naik, and R. Raskar. Designing neural network architectures
using reinforcement learning. ArXiv, abs/1611.02167, 2017.
Pierre Baldi and Kurt Hornik. Neural networks and principal component analysis: Learning
from examples without local minima. Neural networks, 2(1):53–58, 1989.
A. Barron. Approximation and estimation bounds for artiﬁcial neural networks. Machine
Learning, 14:115–133, 1991.
Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin
bounds for neural networks. In Advances in Neural Information Processing Systems, pages
6241–6250, 2017.
Amir Beck and Marc Teboulle. A fast iterative shrinkage-thresholding algorithm for linear
inverse problems. SIAM Journal on Imaging Sciences, 2(1):183–202, 2009.
Andrei Belitski, Arthur Gretton, Cesare Magri, Yusuke Murayama, Marcelo A. Montemurro,
Nikos K. Logothetis, and Stefano Panzeri. Low-frequency local ﬁeld potentials and spikes
in primary visual cortex convey independent visual information. Journal of Neuroscience,
28(22):5696–5709, 2008. ISSN 0270-6474. doi: 10.1523/JNEUROSCI.0009-08.2008. URL
https://www.jneurosci.org/content/28/22/5696.
Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mand al. Reconciling modern machine
learning and the bias-variance trade-oﬀ. arXiv e-prints, art. arXiv:1812.11118, Dec 2018.
Stephen P Boyd and Lieven Vandenberghe. Convex optimization. Cambridge university
press, 2004.
Michael M. Bronstein, Joan Bruna, Yann LeCun, Arthur Szlam, and Pierre Vandergheynst.
Geometric deep learning: Going beyond euclidean data. IEEE Signal Processing Magazine,
34(4):18–42, 2017. doi: 10.1109/MSP.2017.2693418.
Joan Bruna and St´ephane Mallat. Invariant scattering convolution networks. IEEE Trans-
actions on Pattern Analysis and Machine Intelligence, 35(8):1872–1886, 2013.
Sam Buchanan, Dar Gilboa, and John Wright. Deep networks and the multiple manifold
problem. arXiv preprint arXiv:2008.11245, 2020.
Jacopo Cavazza, Pietro Morerio, Benjamin Haeﬀele, Connor Lane, Vittorio Murino, and
Rene Vidal. Dropout as a low-rank regularizer for matrix factorization. In International
Conference on Artiﬁcial Intelligence and Statistics, pages 435–444, 2018.
Tsung-Han Chan, Kui Jia, Shenghua Gao, Jiwen Lu, Zinan Zeng, and Yi Ma. PCANet: A
simple deep learning baseline for image classiﬁcation? TIP, 2015.
Jianlong Chang, Lingfeng Wang, Gaofeng Meng, Shiming Xiang, and Chunhong Pan. Deep
adaptive image clustering.
In Proceedings of the IEEE International Conference on
Computer Vision, pages 5879–5887, 2017.
Le Chang and Doris Tsao. The code for facial identity in the primate brain. Cell, 169:
1013–1028.e14, 06 2017. doi: 10.1016/j.cell.2017.05.011.
91

Chan, Yu, You, Qi, Wright, and Ma
Tian Qi Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary
diﬀerential equations. In NeurIPS, 2018.
Francois Chollet. Xception: Deep learning with depthwise separable convolutions. In 2017
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 1800–1807,
2017.
Adam Coates, Andrew Ng, and Honglak Lee.
An analysis of single-layer networks in
unsupervised feature learning. In International Conference on Artiﬁcial Intelligence and
Statistics, pages 215–223, 2011.
Taco Cohen and Max Welling. Group equivariant convolutional networks. In International
Conference on Machine Learning, pages 2990–2999, 2016a.
Taco S. Cohen and Max Welling.
Group equivariant convolutional networks.
CoRR,
abs/1602.07576, 2016b. URL http://arxiv.org/abs/1602.07576.
Taco S Cohen, Mario Geiger, and Maurice Weiler. A general theory of equivariant CNNs
on homogeneous spaces. In Advances in Neural Information Processing Systems, pages
9142–9153, 2019.
Thomas M. Cover and Joy A. Thomas. Elements of Information Theory (Wiley Series
in Telecommunications and Signal Processing). Wiley-Interscience, USA, 2006. ISBN
0471241954.
Matthias Delange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, Ales Leonardis,
Greg Slabaugh, and Tinne Tuytelaars. A continual learning survey: Defying forgetting in
classiﬁcation tasks. IEEE Transactions on Pattern Analysis and Machine Intelligence,
2021.
Simon S. Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably
optimizes over-parameterized neural networks. In International Conference on Learning
Representations, 2019. URL https://openreview.net/forum?id=S1eK3i09YQ.
Chris Eliasmith and Charles Anderson. Neural Engineering: Computation, Representation
and Dynamics in Neurobiological Systems. Cambridge, MA, 01 2003.
Gamaleldin Elsayed, Dilip Krishnan, Hossein Mobahi, Kevin Regan, and Samy Bengio. Large
margin deep networks for classiﬁcation. In Advances in neural information processing
systems, pages 842–852, 2018.
Logan Engstrom, Brandon Tran, Dimitris Tsipras, Ludwig Schmidt, and Aleksander Madry.
A rotation and a translation suﬃce: Fooling CNNs with simple transformations. arXiv,
2017.
Cong Fang, Hangfeng He, Qi Long, and Weijie J Su. Layer-peeled model: Toward under-
standing well-trained deep neural networks. arXiv preprint arXiv:2101.12699, 2021.
92

ReduNet: A White-box Deep Network from Rate Reduction
M. Fazel, H. Hindi, and S. P. Boyd. Log-det heuristic for matrix rank minimization with
applications to Hankel and Euclidean distance matrices. In Proceedings of the 2003
American Control Conference, 2003., volume 3, pages 2156–2162 vol.3, June 2003. doi:
10.1109/ACC.2003.1243393.
William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion
parameter models with simple and eﬃcient sparsity. arXiv:2101.03961, 01 2021.
Gauthier Gidel, Francis Bach, and Simon Lacoste-Julien. Implicit regularization of discrete
gradient dynamics in linear neural networks. In Advances in Neural Information Processing
Systems, pages 3196–3206, 2019.
R. Giryes, Y. C. Eldar, A. M. Bronstein, and G. Sapiro. Tradeoﬀs between convergence
speed and reconstruction accuracy in inverse problems. IEEE Transactions on Signal
Processing, 66(7):1676–1690, 2018. doi: 10.1109/TSP.2018.2791945.
Raja Giryes, Yonina C Eldar, Alex M Bronstein, and Guillermo Sapiro. Tradeoﬀs between
convergence speed and reconstruction accuracy in inverse problems. IEEE Transactions
on Signal Processing, 66(7):1676–1690, 2018.
Noah Golowich, Alexander Rakhlin, and Ohad Shamir. Size-independent sample complexity
of neural networks. CoRR, abs/1712.06541, 2017. URL http://arxiv.org/abs/1712.
06541.
Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio.
Deep learning,
volume 1. MIT Press, 2016.
Karol Gregor and Yann LeCun. Learning fast approximations of sparse coding. In Proceedings
of the 27th International Conference on International Conference on Machine Learning,
pages 399–406, 2010.
Suriya Gunasekar, Jason D Lee, Daniel Soudry, and Nati Srebro. Implicit bias of gradient
descent on linear convolutional networks. In Advances in Neural Information Processing
Systems, pages 9461–9471, 2018.
Raia Hadsell, Sumit Chopra, and Yann LeCun. Dimensionality reduction by learning an
invariant mapping. In 2006 IEEE Computer Society Conference on Computer Vision and
Pattern Recognition, CVPR 2006, pages 1735–1742, 2006.
Benjamin David Haeﬀele, Chong You, and Rene Vidal. A critique of self-expressive deep
subspace clustering. In International Conference on Learning Representations, 2021.
XY Han, Vardan Papyan, and David L Donoho. Neural collapse under mse loss: Proximity
to and dynamics on the central path. arXiv preprint arXiv:2106.02073, 2021.
Boris Hanin. Universal function approximation by deep neural nets with bounded width
and ReLU activations. arXiv preprint arXiv:1708.02691, 2017.
Trevor Hastie, Robert Tibshirani, and Jerome Friedman. The Elements of Statistical Learning.
Springer, second edition, 2009.
93

Chan, Yu, You, Qi, Wright, and Ma
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In CVPR, 2016.
Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast
for unsupervised visual representation learning. arXiv preprint arXiv:1911.05722, 2019.
Matthias Hein and Jean-Yves Audibert. Intrinsic dimensionality estimation of submanifolds
in rd. In Proceedings of the 22nd international conference on Machine learning, pages
289–296, 2005.
Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common
corruptions and perturbations. In International Conference on Learning Representations,
2018.
Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint
arXiv:1606.08415, 2016.
G. E. Hinton and R. R. Salakhutdinov. Reducing the dimensionality of data with neural
networks. Science, 313(5786):504–507, 2006. ISSN 0036-8075. doi: 10.1126/science.1127647.
URL https://science.sciencemag.org/content/313/5786/504.
R Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Phil Bachman,
Adam Trischler, and Yoshua Bengio. Learning deep representations by mutual information
estimation and maximization. arXiv preprint arXiv:1808.06670, 2018.
Sepp Hochreiter and J¨urgen Schmidhuber. Long short-term memory. Neural computation, 9:
1735–80, 12 1997. doi: 10.1162/neco.1997.9.8.1735.
David Hong, Yue Sheng, and Edgar Dobriban. Selecting the number of components in PCA
via random signﬂips, 2020.
Weihua Hu, Takeru Miyato, Seiya Tokui, Eiichi Matsumoto, and Masashi Sugiyama. Learn-
ing discrete representations via information maximizing self-augmented training.
In
International Conference on Machine Learning, pages 1558–1567, 2017.
Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely
connected convolutional networks. In 2017 IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), pages 2261–2269, 2017.
Lawrence Hubert and Phipps Arabie. Comparing partitions. Journal of Classiﬁcation, 2(1):
193–218, 1985.
Frank Hutter, Lars Kotthoﬀ, and Joaquin Vanschoren, editors. Automatic Machine Learning:
Methods, Systems, Challenges. Springer, 2019.
A. Hyv¨arinen and E. Oja.
Independent component analysis: algorithms and applica-
tions.
Neural Networks, 13(4):411–430, 2000.
ISSN 0893-6080.
doi:
https://doi.
org/10.1016/S0893-6080(00)00026-5. URL https://www.sciencedirect.com/science/
article/pii/S0893608000000265.
94

ReduNet: A White-box Deep Network from Rate Reduction
Sergey Ioﬀe and Christian Szegedy. Batch normalization: Accelerating deep network training
by reducing internal covariate shift. In International Conference on Machine Learning,
pages 448–456, 2015.
Arthur Jacot, Franck Gabriel, and Cl´ement Hongler. Neural tangent kernel: Convergence and
generalization in neural networks. June 2018. URL http://arxiv.org/abs/1806.07572.
Pan Ji, Tong Zhang, Hongdong Li, Mathieu Salzmann, and Ian Reid. Deep subspace
clustering networks. In Advances in Neural Information Processing Systems, pages 24–33,
2017.
Xu Ji, Jo˜ao F Henriques, and Andrea Vedaldi. Invariant information clustering for unsuper-
vised image classiﬁcation and segmentation. In Proceedings of the IEEE International
Conference on Computer Vision, pages 9865–9874, 2019.
Chi Jin, Rong Ge, Praneeth Netrapalli, Sham M. Kakade, and Michael I. Jordan. How to
escape saddle points eﬃciently. In International conference on machine learning, 2017a.
Chi Jin, Praneeth Netrapalli, and Michael I. Jordan. Accelerated gradient descent escapes
saddle points faster than gradient descent. arXiv:1711.10456 [cs.LG], 2017b.
Chi Jin, Praneeth Netrapalli, and Michael I. Jordan. Accelerated gradient descent escapes
saddle points faster than gradient descent. In S´ebastien Bubeck, Vianney Perchet, and
Philippe Rigollet, editors, Conference On Learning Theory, COLT 2018, Stockholm,
Sweden, 6-9 July 2018, volume 75 of Proceedings of Machine Learning Research, pages
1042–1085. PMLR, 2018. URL http://proceedings.mlr.press/v75/jin18a.html.
Ian T Jolliﬀe. Principal Component Analysis. Springer-Verlag, 2nd edition, 2002.
Zhao Kang, Chong Peng, Jie Cheng, and Qiang Cheng. Logdet rank minimization with
application to subspace clustering. CoRR, abs/1507.00908, 2015. URL http://arxiv.
org/abs/1507.00908.
Koray Kavukcuoglu, Marc’Aurelio Ranzato, Rob Fergus, and Yann LeCun. Learning invariant
features through topographic ﬁlter maps. In 2009 IEEE Conference on Computer Vision
and Pattern Recognition, pages 1605–1612. IEEE, 2009.
G¨unter Klambauer, Thomas Unterthiner, Andreas Mayr, and Sepp Hochreiter.
Self-
normalizing neural networks. In Proceedings of the 31st International Conference on
Neural Information Processing Systems, pages 972–981, 2017.
Artemy Kolchinsky, Brendan D Tracey, and Steven Van Kuyk. Caveats for information
bottleneck in deterministic scenarios. arXiv preprint arXiv:1808.07593, 2018.
Irwin Kra and Santiago R Simanca.
On circulant matrices.
Notices of the American
Mathematical Society, 59:368–377, 2012.
Mark A Kramer. Nonlinear principal component analysis using autoassociative neural
networks. AIChE Journal, 37(2):233–243, 1991.
95

Chan, Yu, You, Qi, Wright, and Ma
Alex
Krizhevsky.
Learning
multiple
layers
of
features
from
tiny
images.
2009.
URL http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.222.
9220&rep=rep1&type=pdf.
Alex Krizhevsky, Ilya Sutskever, and Geoﬀrey E Hinton. Imagenet classiﬁcation with deep
convolutional neural networks. In Advances in neural information processing systems,
pages 1097–1105, 2012.
Andrew Lavin and Scott Gray. Fast algorithms for convolutional neural networks. In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages
4013–4021, 2016.
Yann LeCun. The MNIST database of handwritten digits, 1998. URL http://yann.lecun.
com/exdb/mnist/.
Yann LeCun and Yoshua Bengio. Convolutional networks for images, speech, and time series.
In The handbook of brain theory and neural networks. MIT Press, 1995.
Yann LeCun, Lawrence D Jackel, L´eon Bottou, Corinna Cortes, John S Denker, Harris
Drucker, Isabelle Guyon, Urs A Muller, Eduard Sackinger, Patrice Simard, et al. Learning
algorithms for classiﬁcation: A comparison on handwritten digit recognition. Neural
networks: the statistical mechanics perspective, 261(276):2, 1995.
Yann LeCun, L´eon Bottou, Yoshua Bengio, and Patrick Haﬀner. Gradient-based learning
applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.
Yann LeCun, Fu Jie Huang, and Leon Bottou. Learning methods for generic object recognition
with invariance to pose and lighting. In Proceedings of the 2004 IEEE Computer Society
Conference on Computer Vision and Pattern Recognition, 2004. CVPR 2004., volume 2,
pages II–104. IEEE, 2004.
Yann LeCun, Yoshua Bengio, and Geoﬀrey Hinton. Deep learning. Nature, 521(7553):
436–444, 2015.
Jos´e Lezama, Qiang Qiu, Pablo Mus´e, and Guillermo Sapiro. OLE: Orthogonal low-rank
embedding-a plug and play geometric loss for deep learning. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition, pages 8109–8118, 2018.
Ke Li, Shichong Peng, Tianhao Zhang, and Jitendra Malik. Multimodal image synthesis with
conditional implicit maximum likelihood estimation. International Journal of Computer
Vision, 2020.
Yanjun Li and Yoram Bresler. Multichannel sparse blind deconvolution on the sphere. IEEE
Transactions on Information Theory, 65(11):7415–7436, 2019.
Sheng Liu, Xiao Li, Yuexiang Zhai, Chong You, Zhihui Zhu, Carlos Fernandez-Granda, and
Qing Qu. Convolutional normalization: Improving deep convolutional network robustness
and training. arXiv preprint arXiv:2103.00673, 2021.
96

ReduNet: A White-box Deep Network from Rate Reduction
Bethany Lusch, J. Kutz, and Steven Brunton. Deep learning for universal linear embeddings of
nonlinear dynamics. Nature Communications, 9, 11 2018. doi: 10.1038/s41467-018-07210-0.
Yi Ma, Harm Derksen, Wei Hong, and John Wright. Segmentation of multivariate mixed
data via lossy data coding and compression. PAMI, 2007.
Andrew L Maas, Awni Y Hannun, and Andrew Y Ng. Rectiﬁer nonlinearities improve neural
network acoustic models. In Proc. ICML, volume 30, page 3. Citeseer, 2013.
Jan MacDonald, Stephan W¨aldchen, Sascha Hauch, and Gitta Kutyniok. A rate-distortion
framework for explaining neural network decisions. CoRR, abs/1905.11092, 2019. URL
http://arxiv.org/abs/1905.11092.
Najib J. Majaj, Ha Hong, Ethan A. Solomon, and James J. DiCarlo. Simple learned
weighted sums of inferior temporal neuronal ﬁring rates accurately predict human core
object recognition performance. Journal of Neuroscience, 35(39):13402–13418, 2015. ISSN
0270-6474. doi: 10.1523/JNEUROSCI.5181-14.2015. URL https://www.jneurosci.org/
content/35/39/13402.
Michael Mathieu, Mikael Henaﬀ, and Yann LeCun. Fast training of convolutional networks
through FFTs, 2013.
Michael McCloskey and Neal J Cohen. Catastrophic interference in connectionist networks:
The sequential learning problem. In Psychology of learning and motivation, volume 24,
pages 109–165. Elsevier, 1989.
W. McCulloch and W. Pitts. A logical calculus of the ideas immanent in nervous activity.
the Bulletin of Mathematical Biology, 5:115–133, 1943.
Song Mei and Andrea Montanari. The generalization error of random features regression: Pre-
cise asymptotics and double descent curve. ArXiv Statistics e-prints, arXiv:1908.05355v2,
08 2019.
Poorya Mianjy, Raman Arora, and Rene Vidal. On the implicit bias of dropout. In Jennifer Dy
and Andreas Krause, editors, Proceedings of the 35th International Conference on Machine
Learning, volume 80 of Proceedings of Machine Learning Research, pages 3540–3548. PMLR,
10–15 Jul 2018. URL http://proceedings.mlr.press/v80/mianjy18b.html.
Dustin G Mixon, Hans Parshall, and Jianzong Pi. Neural collapse with unconstrained
features. arXiv preprint arXiv:2011.11619, 2020.
Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normal-
ization for generative adversarial networks. arXiv preprint arXiv:1802.05957, 2018.
Vishal Monga, Yuelong Li, and Yonina C Eldar. Algorithm unrolling: Interpretable, eﬃcient
deep learning for signal and image processing. arXiv preprint arXiv:1912.10557, 2019.
S. Nam, M.E. Davies, M. Elad, and R. Gribonval. The cosparse analysis model and algorithms.
Applied and Computational Harmonic Analysis, 34(1):30 – 56, 2013. ISSN 1063-5203.
doi: https://doi.org/10.1016/j.acha.2012.03.006. URL http://www.sciencedirect.com/
science/article/pii/S1063520312000450.
97

Chan, Yu, You, Qi, Wright, and Ma
Yurii Nesterov. A method for unconstrained convex minimization problem with the rate of
convergence o (1/kˆ 2). In Doklady AN USSR, volume 269, pages 543–547, 1983.
Oliver Nina, Jamison Moody, and Clarissa Milligan. A decoder-free approach for unsupervised
clustering and manifold learning with random triplet mining. In Proceedings of the IEEE
International Conference on Computer Vision Workshops, pages 0–0, 2019.
J. Nocedal and S. Wright. Numerical Optimization. Springer, New York, 2rd edition, 2006a.
Jorge Nocedal and Stephen J. Wright. Numerical Optimization. Springer, New York, NY,
USA, second edition, 2006b.
Chigozie Nwankpa, Winifred Ijomah, Anthony Gachagan, and Stephen Marshall. Activation
functions: Comparison of trends in practice and research for deep learning. arXiv preprint
arXiv:1811.03378, 2018.
Bruno A Olshausen and David J Field. Emergence of simple-cell receptive ﬁeld properties
by learning a sparse code for natural images. Nature, 381(6583):607, 1996.
Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive
predictive coding. arXiv preprint arXiv:1807.03748, 2018.
Vardan Papyan, Yaniv Romano, and Michael Elad. Convolutional neural networks analyzed
via convolutional sparse coding.
The Journal of Machine Learning Research, 18(1):
2887–2938, 2017.
Vardan Papyan, XY Han, and David L Donoho. Prevalence of neural collapse during the
terminal phase of deep learning training. arXiv preprint arXiv:2008.08186, 2020.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,
Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imper-
ative style, high-performance deep learning library. In Advances in Neural Information
Processing Systems, pages 8024–8035, 2019.
Xi Peng, Jiashi Feng, Shijie Xiao, Jiwen Lu, Zhang Yi, and Shuicheng Yan. Deep sparse
subspace clustering. arXiv preprint arXiv:1709.08374, 2017.
Phil Pope, Chen Zhu, Ahmed Abdelkader, Micah Goldblum, and Tom Goldstein. The intrinsic
dimension of images and its impact on learning. In International Conference on Learning
Representations, 2021. URL https://openreview.net/forum?id=XJk19XzGq2J.
Haozhi Qi, Chong You, Xiaolong Wang, Yi Ma, and Jitendra Malik. Deep isometric learning
for visual recognition. In Proceedings of the International Conference on International
Conference on Machine Learning, 2020.
Qing Qu, Xiao Li, and Zhihui Zhu. A nonconvex approach for exact and eﬃcient multichannel
sparse blind deconvolution. In Advances in Neural Information Processing Systems, pages
4017–4028, 2019.
Qing Qu, Xiao Li, and Zhihui Zhu. Exact recovery of multichannel sparse blind deconvolution
via gradient descent. SIAM Journal on Imaging Sciences, 13(3):1630–1652, 2020a.
98

ReduNet: A White-box Deep Network from Rate Reduction
Qing Qu, Yuexiang Zhai, Xiao Li, Yuqian Zhang, and Zhihui Zhu. Geometric analysis of
nonconvex optimization landscapes for overcomplete learning. In International Confer-
ence on Learning Representations, 2020b. URL https://openreview.net/forum?id=
rygixkHKDH.
J. R. Quinlan.
Induction of decision trees.
Mach. Learn., 1(1):81–106, March 1986.
ISSN 0885-6125. doi: 10.1023/A:1022643204877. URL https://doi.org/10.1023/A:
1022643204877.
Salah Rifai, Pascal Vincent, Xavier Muller, Xavier Glorot, and Yoshua Bengio. Contrac-
tive auto-encoders: Explicit invariance during feature extraction. In In International
Conference on Machine Learning, page 833–840, 2011.
David Rolnick and Max Tegmark. The power of deeper networks for expressing natural
functions. CoRR, abs/1705.05502, 2017. URL http://arxiv.org/abs/1705.05502.
Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for
biomedical image segmentation. In International Conference on Medical image computing
and computer-assisted intervention, pages 234–241. Springer, 2015.
F. Rosenblatt. The perceptron: a probabilistic model for information storage and organization
in the brain. Psychological review, 65 6:386–408, 1958.
R. Rubinstein and M. Elad. Dictionary learning for analysis-synthesis thresholding. IEEE
Transactions on Signal Processing, 62(22):5962–5972, 2014.
David E. Rumelhart, Geoﬀrey E. Hinton, and Ronald J. Williams. Learning representations by
back-propagating errors. Nature, 323(6088):533–536, October 1986. doi: 10.1038/323533a0.
Sara Sabour, Nicholas Frosst, and Geoﬀrey E. Hinton. Dynamic routing between capsules.
CoRR, abs/1710.09829, 2017. URL http://arxiv.org/abs/1710.09829.
Dominik Scherer, Andreas M¨uller, and Sven Behnke. Evaluation of pooling operations in
convolutional architectures for object recognition. In International conference on artiﬁcial
neural networks, pages 92–101. Springer, 2010.
Hanie Sedghi, Vineet Gupta, and Philip M Long. The singular values of convolutional layers.
In ICLR, 2019.
Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoﬀrey Hinton,
and JeﬀDean. Outrageously large neural networks: The sparsely-gated mixture-of-experts
layer. In ICLR, 2017. URL https://openreview.net/pdf?id=B1ckMDqlg.
Yanyao Shen and Sujay Sanghavi. Learning with bad training data via iterative trimmed
loss minimization. In International Conference on Machine Learning, pages 5739–5748.
PMLR, 2019.
Jonathan R Shewchuk. An introduction to the conjugate gradient method without the
agonizing pain. Technical report, Carnegie Mellon University, USA, 1994.
99

Chan, Yu, You, Qi, Wright, and Ma
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale
image recognition. In ICLR, 2015.
William R Softky and Christof Koch. The highly irregular ﬁring of cortical cells is inconsistent
with temporal integration of random EPSPs. Journal of Neuroscience, 13(1):334–350,
1993.
Mahdi Soltanolkotabi, Adel Javanmard, and Jason D Lee. Theoretical insights into the
optimization landscape of over-parameterized shallow neural networks. IEEE Transactions
on Information Theory, 65(2):742–769, 2018.
Stefano Spigler, Mario Geiger, and Matthieu Wyart. Asymptotic learning curves of kernel
methods: empirical data vs teacher-student paradigm. arXiv preprint arXiv:1905.10843,
2019.
Rupesh Kumar Srivastava, Klaus Greﬀ, and J¨urgen Schmidhuber. Highway networks. arXiv
preprint arXiv:1505.00387, 2015.
Alexander Strehl and Joydeep Ghosh. Cluster ensembles—a knowledge reuse framework for
combining multiple partitions. Journal of Machine Learning Research, 3(Dec):583–617,
2002.
Jeremias Sulam, Vardan Papyan, Yaniv Romano, and Michael Elad. Multilayer convolutional
sparse modeling: Pursuit and dictionary learning. IEEE Transactions on Signal Processing,
2018.
Xiaoxia Sun, Nasser M Nasrabadi, and Trac D Tran. Supervised deep sparse coding networks
for image classiﬁcation. IEEE Transactions on Image Processing, 29:405–418, 2020.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian
Goodfellow, and Rob Fergus. Intriguing properties of neural networks. arXiv preprint
arXiv:1312.6199, 2013.
Yi Tay, Mostafa Dehghani, Jai Gupta, Dara Bahri, Vamsi Aribandi, Zhen Qin, and Donald
Metzler. Are pre-trained convolutions better than pre-trained transformers?, 2021.
Naftali Tishby and Noga Zaslavsky. Deep learning and the information bottleneck principle.
In 2015 IEEE Information Theory Workshop (ITW), pages 1–5. IEEE, 2015.
Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Instance normalization: The
missing ingredient for fast stylization. arXiv preprint arXiv:1607.08022, 2016.
Nicolas Vasilache, J. Johnson, Micha¨el Mathieu, Soumith Chintala, Serkan Piantino, and
Y. LeCun. Fast convolutional nets with fbﬀt: A GPU performance evaluation. CoRR,
abs/1412.7580, 2015.
Rene Vidal, Yi Ma, and S. S. Sastry. Generalized Principal Component Analysis. Springer
Publishing Company, Incorporated, 1st edition, 2016. ISBN 0387878106.
100

ReduNet: A White-box Deep Network from Rate Reduction
Oriol Vinyals, Yangqing Jia, Li Deng, and Trevor Darrell. Learning with recursive perceptual
representations. In Proceedings of the 25th International Conference on Neural Information
Processing Systems - Volume 2, NIPS’12, page 2825–2833, Red Hook, NY, USA, 2012.
Curran Associates Inc.
Andrew Wagner, John Wright, Arvind Ganesh, Zihan Zhou, Hossein Mobahi, and Yi Ma.
Toward a practical face recognition system: Robust alignment and illumination by sparse
representation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 34(2):
372–386, 2012.
Michael B Wakin, David L Donoho, Hyeokho Choi, and Richard G Baraniuk. The multiscale
structure of non-diﬀerentiable image manifolds. In Proceedings of SPIE, the International
Society for Optical Engineering, pages 59141B–1, 2005.
Colin Wei, Sham Kakade, and Tengyu Ma. The implicit and explicit regularization eﬀects of
dropout. In Hal Daum´e III and Aarti Singh, editors, Proceedings of the 37th International
Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research,
pages 10181–10192. PMLR, 13–18 Jul 2020. URL http://proceedings.mlr.press/v119/
wei20d.html.
T. Wiatowski and H. B¨olcskei. A mathematical theory of deep convolutional neural networks
for feature extraction. IEEE Transactions on Information Theory, 2018.
Scott Wisdom, Thomas Powers, James Pitton, and Les Atlas. Interpretable recurrent neural
networks using sequential sparse recovery. ArXiv, abs/1611.07252, 2016.
John Wright and Yi Ma. High-Dimensional Data Analysis with Low-Dimensional Models:
Principles, Computation, and Applications. Cambridge University Press, 2021.
John Wright, Yangyu Tao, Zhouchen Lin, Yi Ma, and Heung-Yeung Shum. Classiﬁcation
via minimum incremental coding length (MICL). In Advances in Neural Information
Processing Systems, pages 1633–1640, 2008.
John Wright, Allen Y. Yang, Arvind Ganesh, S. Shankar Sastry, and Yi Ma. Robust
face recognition via sparse representation. IEEE Trans. Pattern Anal. Mach. Intell.,
31(2):210–227, February 2009. ISSN 0162-8828. doi: 10.1109/TPAMI.2008.79. URL
http://dx.doi.org/10.1109/TPAMI.2008.79.
Denny Wu and J. Xu. On the optimal weighted ℓ2 regularization in overparameterized linear
regression. ArXiv, abs/2006.05800, 2020.
Jianlong Wu, Keyu Long, Fei Wang, Chen Qian, Cheng Li, Zhouchen Lin, and Hongbin Zha.
Deep comprehensive correlation mining for image clustering. In Proceedings of the IEEE
International Conference on Computer Vision, pages 8150–8159, 2019.
Yuxin Wu and Kaiming He. Group normalization. In Proceedings of the European conference
on computer vision (ECCV), pages 3–19, 2018.
101

Chan, Yu, You, Qi, Wright, and Ma
Zhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin. Unsupervised feature learning
via non-parametric instance discrimination. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pages 3733–3742, 2018.
Ziyang Wu, Christina Baek, Chong You, and Yi Ma. Incremental learning via rate reduction.
In IEEE Computer Society Conference on Computer Vision and Pattern Recognition,
2021.
Junyuan Xie, Ross Girshick, and Ali Farhadi. Unsupervised deep embedding for clustering
analysis. In International Conference on Machine Learning, pages 478–487, 2016.
Saining Xie, Ross Girshick, Piotr Doll´ar, Zhuowen Tu, and Kaiming He. Aggregated residual
transformations for deep neural networks. In CVPR, 2017a.
Saining Xie, Ross Girshick, Piotr Doll´ar, Zhuowen Tu, and Kaiming He. Aggregated residual
transformations for deep neural networks. In Proceedings of the IEEE conference on
computer vision and pattern recognition, pages 1492–1500, 2017b.
Bing Xu, Naiyan Wang, Tianqi Chen, and Mu Li. Empirical evaluation of rectiﬁed activations
in convolutional network. arXiv preprint arXiv:1505.00853, 2015.
Jianwei Yang, Devi Parikh, and Dhruv Batra. Joint unsupervised learning of deep represen-
tations and image clusters. In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pages 5147–5156, 2016.
Zitong Yang, Yaodong Yu, Chong You, Jacob Steinhardt, and Yi Ma. Rethinking bias-
variance trade-oﬀfor generalization of neural networks. In International Conference on
Machine Learning (ICML), 2020.
Chong You, Chun-Guang Li, Daniel P Robinson, and Ren´e Vidal. Oracle based active
set algorithm for scalable elastic net subspace clustering. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition, pages 3928–3937, 2016.
Yaodong Yu, Kwan Ho Ryan Chan, Chong You, Chaobing Song, and Yi Ma. Learning diverse
and discriminative representations via the principle of maximal coding rate reduction.
Advances in Neural Information Processing Systems, 33, 2020.
Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Russ R Salakhut-
dinov, and Alexander J Smola. Deep sets. In I. Guyon, U. V. Luxburg, S. Bengio,
H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural
Information Processing Systems 30, pages 3391–3401. Curran Associates, Inc., 2017. URL
http://papers.nips.cc/paper/6931-deep-sets.pdf.
John Zarka, Louis Thiry, Tom´as Angles, and St´ephane Mallat. Deep network classiﬁcation
by scattering and homotopy dictionary learning. In International Conference on Learning
Representations, 2020. URL https://openreview.net/forum?id=SJxWS64FwH.
John Zarka, Florentin Guth, and St´ephane Mallat. Separation and concentration in deep
networks. In International Conference on Learning Representations, 2021. URL https:
//openreview.net/forum?id=8HhkbjrWLdE.
102

ReduNet: A White-box Deep Network from Rate Reduction
Matthew D Zeiler and Rob Fergus. Visualizing and understanding convolutional networks.
In European Conference on Computer Vision, pages 818–833. Springer, 2014.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Under-
standing deep learning requires rethinking generalization. In ICLR, 2017a.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Under-
standing deep learning requires rethinking generalization. In International Conference on
Learning Representations, 2017b.
Junjian Zhang, Chun-Guang Li, Chong You, Xianbiao Qi, Honggang Zhang, Jun Guo, and
Zhouchen Lin. Self-supervised convolutional subspace clustering network. In Proceedings
of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5473–5482,
2019a.
Tong Zhang, Pan Ji, Mehrtash Harandi, Richard Hartley, and Ian Reid. Scalable deep
k-subspace clustering. In Asian Conference on Computer Vision, pages 466–481. Springer,
2018.
Tong Zhang, Pan Ji, Mehrtash Harandi, Wenbing Huang, and Hongdong Li.
Neural
collaborative subspace clustering. arXiv preprint arXiv:1904.10596, 2019b.
Xiao Zhang, Yaodong Yu, Lingxiao Wang, and Quanquan Gu. Learning one-hidden-layer
relu networks via gradient descent. In The 22nd International Conference on Artiﬁcial
Intelligence and Statistics, pages 1524–1534. PMLR, 2019c.
Kai Zhong, Zhao Song, Prateek Jain, Peter L Bartlett, and Inderjit S Dhillon. Recovery
guarantees for one-hidden-layer neural networks. In International conference on machine
learning, pages 4140–4149. PMLR, 2017.
Pan Zhou, Yunqing Hou, and Jiashi Feng.
Deep adversarial subspace clustering.
In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages
1596–1604, 2018.
Zhihui Zhu, Tianyu Ding, Jinxin Zhou, Xiao Li, Chong You, Jeremias Sulam, and Qing Qu.
A geometric analysis of neural collapse with unconstrained features, 2021.
Barret Zoph and Quoc V Le. Neural architecture search with reinforcement learning. arXiv
preprint arXiv:1611.01578, 2016.
103

