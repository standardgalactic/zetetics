Stable Diffusion
Binxu Wang, John Vastola
Machine Learning from Scratch
Nov.1st 2022

Whatâ€™s the deal with all these pictures?

These pictures were generated by Stable Diffusion, 
a recent diffusion generative model. 
You may have also heard of DALLÂ·E 2, which works in a similar way.
It can turn text prompts (e.g. â€œan astronaut riding a horseâ€) into images.
It can also do a variety of other things!

Could be a model of imagination.
Why should we care?
Similar techniques could be used to generate
any number of things (e.g. neural data).
Itâ€™s cool!
"a lovely cat running
in the desert in Van
Gogh style, trending
art."

Itâ€™s complicatedâ€¦
but hereâ€™s the high-level idea.
How does it work?
â€œBatman eating pizza 
in a diner"

What do we need?
1. Method of learning to generate new stuff given many examples
Example pictures of people
â€œbad stick figure 
drawing"

What do we need?
â€œcool professor personâ€
3. Way to compress images 
(for speed in training and generation)
2. Way to link text and images
ğ‘§[0: 3, : , : ]

What do we need?
â€¦since when youâ€™re generating something new, you need a 
way to safely go beyond the images youâ€™ve seen before.
4. Way to add in good image-related inductive biasesâ€¦

What do we need?
4. Way to add in good inductive biases
1. Method of learning to generate new stuff
3. Way to compress images
2. Way to link text and images
Forward/reverse dffusion
Text-image representation model
Autoencoder
U-net 
architecture
Making a â€˜goodâ€™ generative model is about making all these parts work together well!
+     â€˜attentionâ€™


Stable Diffusion in Action

Cartoon with StableDiffusion + Cartoon
https://www.reddit.com/r/Sta
bleDiffusion/comments/xcjj7u
/sd_img2img_after_effects_i
_generated_2_images_and/

Some Resources
â€¢ Diffusion model in general 
â€¢ What are Diffusion Models? | Lil'Log
â€¢ Generative Modeling by Estimating Gradients of the Data Distribution | 
Yang Song 
â€¢ Stable diffusion
â€¢ Annotated & simplified code: U-Net for Stable Diffusion (labml.ai)
â€¢ Illustrations: The Illustrated Stable Diffusion â€“ Jay Alammar
â€¢ Attention & Transformers
â€¢ The Illustrated Transformer

Outline
â€¢ Stable Diffusion is cool! 
â€¢ Build Stable Diffusion â€œfrom Scratchâ€
â€¢ Principle of Diffusion models (sampling, learning)
â€¢ Diffusion for Images â€“ UNet architecture
â€¢ Understanding prompts â€“ Word as vectors, CLIP
â€¢ Let words modulate diffusion â€“ Conditional Diffusion, Cross Attention
â€¢ Diffusion in latent space â€“ AutoEncoderKL
â€¢ Training on Massive Dataset. â€“ LAION 5Billion
â€¢ Letâ€™s try ourselves. 

Principle of Diffusion Models 
Learning to generate by iterative denoising. 

â€œCreating noise from data is easy; 
Creating data from noise is generative modeling.â€
-- Song, Yang

Diffusion models
â€¢ Forward diffusion (noising)
â€¢ ğ‘¥0 â†’ğ‘¥1 â†’â‹¯ğ‘¥ğ‘‡
â€¢ Take a data distribution ğ‘¥0~ğ‘(ğ‘¥), turn it into noise by 
diffusion ğ‘¥ğ‘‡~ğ’©0, ğœ2ğ¼
â€¢ Reverse diffusion (denoising)
â€¢ ğ‘¥ğ‘‡â†’ğ‘¥ğ‘‡âˆ’1 â†’â‹¯ğ‘¥0
â€¢ Sample from the noise distribution ğ‘¥ğ‘‡~ğ’©(0, ğœ2ğ¼),  
reverse the diffusion process to generate data ğ‘¥0~ğ‘(ğ‘¥)
ğ’™ğŸ
ğ’™ğŸ
ğ’™ğ‘»
ğ’™ğ‘»âˆ’ğŸ

Math Formalism
â€¢ For a forward diffusion process 
ğ‘‘ğ’™= ğ‘“ğ’™, ğ‘¡ğ‘‘ğ‘¡+ ğ‘”ğ‘¡ğ‘‘ğ’˜
â€¢ There is a backward diffusion process that reverse the time 
ğ‘‘ğ’™= ğ‘“ğ‘¥, ğ‘¡âˆ’ğ‘”ğ‘¡2âˆ‡ğ‘¥log ğ‘(ğ’™, ğ‘¡) ğ‘‘ğ‘¡+ ğ‘”ğ‘¡ğ‘‘ğ’˜
â€¢ If we know the time-dependent score function âˆ‡ğ‘¥log ğ‘(ğ’™, ğ‘¡)
â€¢ Then we can reverse the diffusion process.

Animation for the Reverse Diffusion
Score Vector Field
Reverse Diffusion guided by the score vector field
https://yang-song.net/blog/2021/score/

Training diffusion model =
Learning to denoise
â€¢ If we can learn a score model 
ğ‘“ğœƒğ‘¥, ğ‘¡â‰ˆâˆ‡log ğ‘(ğ‘¥, ğ‘¡)
â€¢ Then we can denoise samples, by running the reverse diffusion equation. ğ‘¥ğ‘¡â†’ğ‘¥ğ‘¡âˆ’1
â€¢ Score model ğ‘“ğœƒ: ğ’³Ã— 0,1 â†’ğ’³
â€¢ A time dependent vector field over ğ‘¥space.
â€¢ Training objective: Infer noise from a noised sample 
ğ‘¥âˆ¼ğ‘ğ‘¥, ğœ–âˆ¼ğ’©0, ğ¼, ğ‘¡âˆˆ[0,1]
min ğœ–+ ğ‘“ğœƒğ‘¥+ ğœğ‘¡ğœ–, ğ‘¡
2
2
â€¢ Add Gaussian noise ğœ–to an image ğ‘¥with scale ğœğ‘¡, learn to infer the noise ğœ. 

Conditional denoising
â€¢ Infer noise from a noised sample, based on a condition ğ‘¦
â€¢ ğ‘¥, ğ‘¦âˆ¼ğ‘ğ‘¥, ğ‘¦, ğœ–âˆ¼ğ’©0, ğ¼, ğ‘¡âˆˆ[0,1]
â€¢ min ğœ–âˆ’ğ‘“ğœƒğ‘¥+ ğœğ‘¡ğœ–, ğ‘¦, ğ‘¡
2
2
â€¢ Conditional score model ğ‘“ğœƒ: ğ’³Ã— ğ’´Ã— 0,1 â†’ğ’³
â€¢ Use Unet as to model image to image mapping 
â€¢ Modulate the Unet with condition (text prompt).  

Comparing 
Generative 
Models
https://lilianweng.github.io/posts/2021-07-11-diffusion-models/

GAN
â€¢ One shot generation. Fast. 
â€¢ Harder to control in one pass. 
â€¢ Adversarial min-max objective. Can 
collapse.
Diffusion
â€¢ Multi-iteration generation. Slow.
â€¢ Easier to control during generation. 
â€¢ Simple objective, no adversary in 
training. 
Diffusion vs GAN / VAE

Activation maximization ~ 
Reverse Diffusion
â€¢ For a neuron, activation maximization 
can be realized by gradient ascent 
ğ‘§ğ‘¡+1 â†ğ‘§ğ‘¡+ âˆ‡ğ‘“ğºğ‘§ğ‘¡
+ ğœ–
â€¢ Homologous to the reverse diffusion 
equation.
â€¢ Idea: Neuron activation defines a 
Generative model on image space.

Modelling Score function 
over Image Domain
Introducing UNet

Convolutional Neural Network
â€¢ CNN parametrizes function 
over images
â€¢ Motivation
â€¢ Features are translational 
invariant
â€¢ Extract feature at different 
scale / abstraction level
â€¢ Key modules
â€¢ Convolution
â€¢ Downsamping (Max-pool)
VGG
Features of larger scale (larger RF)
Higher abstraction level. 

CNN + inverted CNN â‡’UNet
â€¢ Inverted CNN 
(generator) can 
generate images. 
â€¢ CNN + inverted CNN 
could model Image â†’
Image function. 
Down Sampling
Up Sampling
Convolution
TransposedConvolution

UNet: a natural architecture for image-to-
image function
Skip connection
Transporting information 
at the same resolution.
Down (sampling) 
side
Encoder
Up (sampling) 
side
Decoder

Key Ingredients of UNet
â€¢ Convolution operation 
â€¢ Save parameter, spatial 
invariant
â€¢ Down/Up sampling
â€¢ Multiscale / Hierarchy 
â€¢ Learn modulation at multi scale 
and multi-abstraction levels.
â€¢ Skip connection 
â€¢ No bottleneck
â€¢ Route feature of the same 
scaledirectly. 
â€¢ Cf. AutoEncoder has bottleneck

Note: Add Time Dependency
â€¢ The score function is time-dependent. 
â€¢ Target:  ğ‘ ğ‘¥, ğ‘¡= âˆ‡ğ‘¥log ğ‘(ğ‘¥, ğ‘¡)
â€¢ Add time dependency
â€¢ Assume time dependency is spatially 
homogeneous. 
â€¢ Add one scalar value per channel ğ‘“(ğ‘¡)
â€¢ Parametrize ğ‘“(ğ‘¡) by MLP / linear of Fourier basis. 
Conv
tensor
ğ’•
âŠ•
Linear/
MLP
ğ‘¡embedding
[ğ¬ğ¢ğ§ğğ’Šğ’•,
ğœğ¨ğ¬ğğ’Šğ’•,
â€¦ ]

Unet in Stable 
Diffusion
(conv_in): Conv2d(4, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
(time_proj): Timesteps()
(time_embedding): TimestepEmbedding
(linear_1): Linear(in_features=320, out_features=1280, bias=True)
(act): SiLU()
(linear_2): Linear(in_features=1280, out_features=1280, bias=True)
(down_blocks):
(0): CrossAttnDownBlock2D
(1): CrossAttnDownBlock2D
(2): CrossAttnDownBlock2D
(3): DownBlock2D
(up_blocks):
(0): UpBlock2D
(1): CrossAttnUpBlock2D
(2): CrossAttnUpBlock2D
(3): CrossAttnUpBlock2D
(mid_block): UNetMidBlock2DCrossAttn
(attentions):
(resnets):
(conv_norm_out): GroupNorm(32, 320, eps=1e-05, affine=True)
(conv_act): SiLU()
(conv_out): Conv2d(320, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))

How to understand prompts?
Language / Multimodal Transformer, CLIP!

Word as Vectors: Language Model 101
â€¢ Unlike pixel, meaning of word are 
not explicitly in the characters. 
â€¢ Word can be represented as index 
in dictionary
â€¢ But index is also meaning less. 
â€¢ Represent words in a vector space
â€¢ Vector geometry => semantic relation.
I   love   cats and   dogs .
Words in a 
sentence
328,      793,   3989,     537,    3255,   269
Token Index
Word 
Vectors

Word Vector in Context: 
RNN / Transformers
â€¢ Meaning of word depends on context, 
not always the same. 
â€¢ â€œI book a ticket to buy that book.â€
â€¢ Word vectors should depend on context. 
â€¢ Transformers let each word â€œabsorbâ€ 
influence from other words to be 
â€œcontextualizedâ€
I   love   cats   and   dogs  .
Transformer
Block
Transformer
Block
N layers â€¦â€¦ 
More on attention laterâ€¦

Learning Word Vectors: 
GPT & BERT & CLIP
â€¢ Self-supervised learning of word 
representation
â€¢ Predicting missing / next words in 
a sentence. (BERT, GPT)
â€¢ Contrastive Learning, matching 
image and text. (CLIP)
MLM â€” Sentence-Transformers documentation (sbert.net)
Downstream Classifier can decode: 
Part of speech, Sentiment, â€¦

Joint Representation for Vision and Language : 
CLIP
â€¢ Learn a joint encoding 
space for text caption 
and image 
â€¢ Maximize 
representation similarity 
between an image and 
its caption. 
â€¢ Minimize other pairs
Vision 
Transformer
Transformer
CLIP paper 2021

Choice of text encoding
â€¢ Encoder in Stable Diffusion: pre-trained  CLIP ViT-L/14 text encoder
â€¢ Word vector can be randomly initialized and learned online. 
â€¢ Representing other conditional signals
â€¢ Object categories (e.g. Shark, Trout, etc.): 
â€¢ 1 vector per class
â€¢ Face attributes (e.g. {female, blonde hair, with glasses, â€¦}, {male, short hair, dark skin}): 
â€¢ set of vectors, 1 vector per attributes
â€¢ Time to be creative!! 

How does text affect diffusion?
Incoming Cross Attention

I   love   cats   and   dogs  .
Original 
sentence
Origin of Attention: 
Machine Translation (Seq2Seq)
â€¢ Use Attention to retrieve useful info from a batch of vectors. 
ğ‘’1
ğ‘’2
ğ‘’3
ğ‘’4
ğ‘’5
ğ‘’6
Encoder 
hidden state
(Word 
Vectors)
â„1
â„2
â„3
Decoder 
hidden state
(Word 
Vectors)
J'adore
les chats et les chiens.
French 
Translation

From Dictionary to Attention
Dictionary: Hard-indexing
â€¢ `dic = {1 : ğ‘£1, 2 : ğ‘£2, 3 : ğ‘£3}`
â€¢ Keys  1,2,3
â€¢ Values  ğ‘£1, ğ‘£2, ğ‘£3
â€¢ `dic[2]` 
â€¢ Query 2
â€¢ Find 2 in keys 
â€¢ Get corresponding value. 
â€¢ Retrieving values as matrix vector product
â€¢ One hot vector over the keys
â€¢ Matrix vector product
ğ‘£1 ğ‘£2 ğ‘£3
1
0
0
Ã—
=
ğ‘£2
ğŸ
ğŸ
ğŸ‘

From Dictionary to Attention
Attention: Soft-indexing 
â€¢ Soft indexing
â€¢ Define an attention distribution 
ğ‘over the keys
â€¢ Matrix vector product.
â€¢ Distribution based on similarity 
of query and key. 
ğ‘£1 ğ‘£2 ğ‘£3
0.8
0.1
0.1
Ã—
=
ğŸ
ğŸ
ğŸ‘
ğ‘£2
ğ‘£1
ğ‘£3
0.8
+0.1
+0.1

QKV attention
â€¢ Query : what I need (Jâ€™adore : â€œI want subject pronoun & verbâ€)
â€¢ Key : what the target provide (I : â€œHere is the subjectâ€)
â€¢ Value : the information to be retrieved (latent related to Je or Jâ€™ )
â€¢ Linear projection of â€œword vectorâ€
â€¢ Query   ğ‘ğ‘–= ğ‘Šğ‘â„ğ‘–
â€¢ Key   ğ‘˜ğ‘—= ğ‘Šğ‘˜ğ‘’ğ‘—
â€¢ Value  ğ‘£ğ‘—= ğ‘Šğ‘£ğ‘’ğ‘—
â€¢ ğ‘’ğ‘—hidden state of encoder (English, source)
â€¢ â„ğ‘–hidden state of decoder (French, target)

Attention mechanism 
â€¢ Compute the inner product (similarity) of key ğ‘˜and query ğ‘
â€¢ SoftMax the normalized score as attention distribution.
ğ‘ğ‘–ğ‘—= SoftMax
ğ‘˜ğ‘—
ğ‘‡ğ‘ğ‘–
ğ‘™ğ‘’ğ‘›(ğ‘)
, à·
ğ‘—
ğ‘ğ‘–ğ‘—= 1
â€¢ Use attention distribution to weighted average values ğ‘£. 
ğ‘ğ‘–= à·
ğ‘—
ğ‘ğ‘–ğ‘—ğ‘£ğ‘—

Visualizing Attention 
matrix ğ’‚ğ’Šğ’‹
â€¢ French 2 English
â€¢ â€œLearnt to pay Attentionâ€
â€¢ â€œla zone economique
europeenneâ€ -> â€œthe 
European Economic Areaâ€
â€¢ â€œa ete signeâ€ -> â€œwas 
signedâ€
https://jalammar.github.io/visualizing-neural-machine-
translation-mechanics-of-seq2seq-models-with-attention/
Attention + RNN

Cross & Self Attention 
â€¢ Cross Attention
â€¢ Tokens in one language pay attention 
to tokens in another. 
â€¢ Self Attention (ğ‘’ğ‘–= â„ğ‘–)
â€¢ Tokens in a language pay attention to 
each other. 
â„1
â„2
â„3
Decoder 
hidden state
(Word 
Vectors)
J'adore
les chats
French 
Translation

https://jalammar.github.io/illustrated-gpt2/
â€œA robot must obey the order given it.â€

https://jalammar.github.io/illustrated-gpt2/
â€œA robot must obey the order given it.â€

https://jalammar.github.io/illustrated-gpt2/

Note: Feed Forward network
â€¢ Attention is usually followed by a 
2-layer MLP and Normalization
â€¢ Learn nonlinear transform.

Text2Image as translation 
â€œ A ballerina chasing her cat running 
on the grass in the style of Monet "
Encoded 
Word Vectors
Latent State 
of Image
Spatial Dimensions
Channel 
Dimensions
Sequence Dimensions
Patch 
Vectors!
Target language: Images
Source language: Words

Text2Image as translation 
â€œ A ballerina chasing her cat running
on the grass in the style of Monet "
Encoded 
Word Vectors
Latent State 
of Image
Spatial Dimensions
Channel 
Dimensions
Sequence Dimensions
Cross Attention:
Image to Words
Self Attention:
Image to Image

Spatial Transformer
â€¢ Rearrange spatial tensor to 
sequence.
â€¢ Cross Attention 
â€¢ Self Attention 
â€¢ FFN 
â€¢ Rearrange back to spatial tensor 
(same shape)

Tips: Implementing attention `einops` lib
â€¢ `einops.rearrange` function 
â€¢ Shift order of axes 
â€¢ Split / combine dimension.
â€¢ `torch.einsum` function
â€¢ Multiply & sum tensors along 
axes. 

â€¢ Down blocks
ResBlock
SpatialTransformer
ResBlock
SpatialTransformer
DownSample
ResBlock
SpatialTransformer
ResBlock
SpatialTransformer
DownSample
ResBlock
SpatialTransformer
ResBlock
SpatialTransformer
DownSample
ResBlock
ResBlock
â€¢ Up blocks
ResBlock
ResBlock
ResBlock
UpSample
ResBlock
SpatialTransformer
ResBlock
SpatialTransformer
ResBlock
SpatialTransformer
UpSample
ResBlock
SpatialTransformer
ResBlock
SpatialTransformer
ResBlock
SpatialTransformer
UpSample
ResBlock
SpatialTransformer
ResBlock
SpatialTransformer
ResBlock
SpatialTransformer
UNet = Giant Sandwich of 
Spatial transformer + 
ResBlock (Conv layer)

Spatial transformer + ResBlock (Conv layer)
â€¢ Alternating Time and Word Modulation
â€¢ Alternating Local and Nonlocal operation
Resblock
Spatial 
Transformer
Resblock
Spatial 
Transformer
Latent 
tensor
ğŸ’, ğŸ”ğŸ’, ğŸ”ğŸ’
Time 
embedding
ğŸğŸğŸ–ğŸ
Word 
Vectors
ğ‘³ğ’”ğ’†ğ’’, ğŸ•ğŸ–ğŸ’

Diffusion in Latent Space
Adding in AutoEncoder
Rombach, R., Blattmann, A., Lorenz, D., Esser, P., & Ommer, B. (2022). High-
resolution image synthesis with latent diffusion models, CVPR

Diffusion in latent space 
â€¢ Motivation:
â€¢ Natural images are high dimensional
â€¢ but have many redundant details that could be 
compressed / statistically filled out
â€¢ Division of labor
â€¢ Diffusion model -> Generate low resolution sketch
â€¢ AutoEncoder -> Fill out high resolution details
â€¢ Train a VAE model to compress images into latent 
space. 
â€¢ ğ‘¥â†’ğ‘§â†’ğ‘¥
â€¢ Train diffusion models in latent space of ğ‘§. 
32 pix 
180 pix 
ğ‘‘= 97200
ğ‘‘= 2352
DownSampling
ğ‘¥
[3,512,512]
ğ‘§
[4,512/ğ‘“, 512/ğ‘“]
à·œğ‘¥
[3,512,512]

Spatial Compression Tradeoff
â€¢ LDM-{ğ‘“}. ğ‘“= Spatial downsampling factor
â€¢ Higher ğ‘“leads to faster sampling, with degraded image quality (FID â†‘)
â€¢ Fewer sampling steps leads to faster sampling, with lower quality (FID â†‘)
Face
CelebA-HQ
ImageNet

Spatial Compression Tradeoff
â€¢ LDM-{ğ‘“}. ğ‘“= Spatial downsampling factor
â€¢ Too little compression ğ‘“= 1,2 or too much compression ğ‘“= 32, makes 
diffusion hard to train. 

Details in Stable Diffusion
â€¢ In stable diffusion, spatial downsampling ğ‘“= 8
â€¢ ğ‘¥is (3, 512, 512) image tensor
â€¢ ğ‘§is (4, 64, 64) latent tensor

Regularizing the Latent Space
â€¢ KL regularizer
â€¢ Similar to VAE, make latent distribution like Gaussian distribution. 
â€¢ VQ regularizer
â€¢ Make the latent representation quantized to be a set of discrete tokens.

Let the GPUs roar! 
Training data & details.

Large Data Training
â€¢ SD is trained on ~ 2 Billion image â€“ caption (English) pairs. 
â€¢ Scraped from web, filtered by CLIP.
â€¢ https://laion.ai/blog/laion-5b/


Diffusion Process Visualized

Meaning of latent space
â€¢ Latent state contains a â€œsketch versionâ€ of the image. 
ğ‘§[0: 3, : , : ]

