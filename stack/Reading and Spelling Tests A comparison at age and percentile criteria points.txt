Full Terms & Conditions of access and use can be found at
https://www.tandfonline.com/action/journalInformation?journalCode=cepp20
Educational Psychology in Practice
theory, research and practice in educational psychology
ISSN: 0266-7363 (Print) 1469-5839 (Online) Journal homepage: www.tandfonline.com/journals/cepp20
Reading and Spelling Tests A comparison at age
and percentile criteria points
John Cook
To cite this article: John Cook (1999) Reading and Spelling Tests A comparison at age
and percentile criteria points, Educational Psychology in Practice, 15:1, 9-19, DOI:
10.1080/0266736990150103
To link to this article:  https://doi.org/10.1080/0266736990150103
Published online: 19 Oct 2007.
Submit your article to this journal 
Article views: 110
View related articles 

Reading and Spelling Tests
A comparison at age and percentile criteria points
John Cook
Summary
LEAs have developed SEN Audits to assist with
equitable allocation to those schools most in need.
Criteria, which parallel the Code of Practice stages,
show the number of children at each stage. Reading
achievement is central to the criteria.
Twelve reading and six spelling tests were compared
at two achievement criteria points: (1) two years below
chronological age, and (2) at or below the 5th percentile.
Close agreement was found between RA and CA at
the 5th percentile. The sole use of an achievement age
measure of years below chronological age is unreliable,
seriously underestimates children's progress and leads
to inaccurate conclusions about the effectiveness of
intervention programmes.
Introduction
The Code of Practice (DfE, 1994) defined a stage
structure of increasing involvement with children
showing difficulties in schools. If a youngster showed
needs beyond the expertise of the class teacher (Stage
1), further advice, support and resources could be
sought from within school (Stage 2). Should this prove
insufficient, schools could call on external support
from LEA or health services (Stage 3). If necessary,
following intervention by school, a statutory assessment
(Stage 4) could be completed which might lead to a
statement of special educational needs (Stage 5).
Movement both ways across stages was envisaged.
Sets of criteria have subsequently been devised by LEAs
to define movement from one stage to the next
according to children's changing educational needs.
Faced with increasing demands on resources, some
LEAs have sought to clarify and quantify a school's
resource needs by making use of the number of children
placed at each Code of Practice stage. This SEN Audit,
properly moderated, assists LEAs in the distribution
of the resources to schools according to the degree of
need. This might include, for example, the amount of
Stage 3 learning support money for a school or the
schools allocated to an EP.
The stage criteria for the placement of children who
present general or specific learning difficulties invariably
include a reading score. This paper explores the validity
of reading achievement criteria by comparing two
procedures which specify cut-off points between stages.
National Curriculum tests
The National Curriculum (NC) has specific Levels and
Attainment Targets for aspects of literacy; Speaking
and Listening, Reading and Writing. At the end of
each Key Stage, children are placed at a Level according
to their current performance that is dependent on their
scores for the NC tests. Until quite recently, the reading
level has been assessed by tests not standardised using
a systematic norming procedure. The previous lack of
traditional standardisation techniques led to significant
criticism of the validity of the stage level placement
and the accuracy of the estimates made. For example,
researchers at Manchester (Pumfrey, 1992) reported
a poor correlation between the NC Level and
performance on a nationally standardised reading test,
the British Ability Scales - Word Reading (Elliott et
al, 1983).
Responding to these criticisms, the Qualifications
and Curriculum Authority (QCA, 1998) now publish
standardised scores for their tests but still do not give
validity data. NC Level criteria are based on a complete
year group. For Key Stage 1, individual children with
birthdays late in the school year are at a significant
disadvantage because they are directly compared, using
the same criteria, to others who could be up to 11
months older. This effect may also penalise smaller
schools where chance variation in an age group could
Educational Psychology in Practice Vol 15, No 1, April 1999

lead to proportionally more younger children from
one year to the next. A decline in the percentage at a
Level may just be due to younger children in the age
group.
Reading and spelling tests
Graded Word
Standardised reading tests offer a valid alternative to
the NC assessment procedures. Schonell originally
developed his Graded Word Reading Test to assess
how well children progressed through his scheme of
graded reading books. The Schonell test went though
several norm revisions (eg Schonell, 1965) and is still
occasionally used. Graded word reading lists continue
to be a method of measuring reading achievement.
The Word Reading scale of the revised British Ability
Scales (BAS II) retained this form from the original
version. Part of the Wechsler Objective Reading Dimen-
sions (WORD) and the previously Macmillan, now
NFER-Nelson Graded Word Test also continue this
tradition. (NB Any reader unfamiliar with the test
abbreviations should refer to page 14. Full details of
all tests are contained in the reference list.)
Cloze Type
An alternative procedure makes use of the cloze format.
Children are presented with a sentence, one of the
words of which is missing. From a given list of words
children are required to underline or circle the word
which best fits into the sentence. The revised Edinburgh
tests, the Suffolk Reading Scale and the group reading
tests of Young are of this form, as are the NFER-Nelson
GRT6-12 and part of GRT 8-14.
Prose Reading
Some tests place words in text and assess reading
achievement by the accuracy with which children read
them correctly. The Salford Sentence Reading Test
and the Accuracy component of the New Neale
Analysis of Reading Ability make use of this method.
Comprehension
Other tests attempt to measure the extent to which a
child draws meaning from the words in a passage. The
Comprehension scales of the New Neale and the
Wechsler Objective Reading Dimensions give passages
for a child to read and then assess reading by the
accuracy with which questions about the passage are
answered. The Individual Reading Scale is similar and,
although it gives achievement scores, it is essentially
diagnostic in function. Many of the NFER-Nelson
group tests, including part of the GRT8-14, are
comprehension tests with a multiple-choice design.
The QCA extend this format to include more free
writing in response to a passage. A new dimension has
recently been added to comprehension tests by the
Reading Progress Tests that are designed for regular
group administration to assess the progress made by
children as they move through school.
Spelling
Many might suggest that a spelling test should be part
of any criteria because it presents substantial diagnostic
information in addition to a score. All spelling tests
are of a similar form but three are restricted to psycholo-
gists; the original BAS, the BASH and the spelling
component of the WORD. The SPAR and the now
ageing Vernon are acceptable alternatives for teacher
administration. A new set of tests, the British Spelling
Test Series, spans the school years and beyond. It
extends the usual word-spelling format with additional
cloze-style responses. The QCA spelling tests
incorporate both individual words and cloze items.
Qualitative information for diagnostic purposes can
be gained from most tests. It is the prime function of
the Margaret Peters Diagnostic Spelling which does
not attempt to go beyond the approximate age range
of each of the passages.
Reading age or Standardised Score
There are considerable difficulties in both professional
and survey terms if sole use is made of a reading age
score to describe an achievement level. The rejection
of the parallel concept of mental age and the adoption
of deviation score measures has long since ceased to
be controversial. Achievement assessed by months
away from chronological age is open to considerable
question because the relative severity expressed in this
way may change as the child grows older. For those
tests giving a reading age score only and not a
standardised score there are substantial practical and
theoretical problems. The NC Level needs also the
qualification of a standardised score for the same
reasons.
10
Educational Psychology in Practice Vol 15, No 1, April 1999

Associated with a standardised score is the confidence
band. This gives a statistical estimate of how sure we
can be that the 'true score' lies within a range either
side of the given score. An error of measurement is
associated with all scores given in this paper which
varies according to test, distance of the score from the
mean, and age.
Comparing test scores
Traditional methods
Evidence of inter-test, or concurrent, validity is always
addressed in the manual of the test concerned. How
closely a new test compares with existing tests is a
critical factor which the designer addresses by quoting
inter-test correlation. However, some recent studies
have suggested differences between tests. For example,
the Neale Analysis of Reading Ability - Revised was
reported to differ from the BAS (Gregory and Gregory,
1994) and show gender differences between parallel
forms (Stothard and Hulme, 1991). Whetton (1990)
rebuffed the criticisms by suggesting that that the results
obtained might well be due to changes in reading
standards. However, Sacre and Masterson (1997)
continued to find, albeit with a relatively small local
sample of children from one school, differences between
the revised Neale, the original BAS Word Reading, the
NFER-Nelson GRT6-12 and a local Hertfordshire
reading test. Most studies of test comparison adopt
designs similar to Gregory and Gregory (1994) or
Sacre and Masterson (1997). However, any such study
is subject to the criticism of Halliwell and Feltham
(1995) that the samples are small, local and cannot
compare to the size and specificity of the original
standardisation. Only a national survey similar to that
carried out in the 1950s would be appropriate.
Nevertheless, the New Neale, NARAII, was published
as a second edition with revised norms.
An alternative method for comparing
test scores
When developing a method of illustrating relative
progress for reading, Cook and Cook (1996) reported
the variety of reading tests available for use. This was
of concern for them because doubt was placed on the
accuracy with which comparisons could be made
between successive scores on different reading tests.
Reading tests are disparate in design, are standardised
on different populations and do not measure the same
aspect of reading. How confident could the authors
be that a general chart illustrating a child's reading
progress could be applied to all tests? What was the
relevance of, for example, a BAS Word Reading Age
at 8 years of age to a New Neale Accuracy score at a
review 12 months later?
A procedure, which parallels the traditional validity
measures, could be devised that assesses the accuracy
of tests at specific age discrepancies. If two tests were
comparable, a similar proportion of children would
be expected to perform, for example, 18 months above
average for their age for each test. The proportion
should change in a similar manner as the age group
changes but a reading age at a specified chronological
age should have a common percentile rank independent
of the test used. Equally, the question arises when an
LEA defines, in part at least, an Audit or Code of
Practice stage involving a reading measure. How
confident could administrators be that the test chosen
reliably reflected an accurate picture of a child's
achievement level? Educational psychologists (EPs),
when perusing a child's school records, are frequently
presented with reading scores from different tests. If
there were differences between test scores over time,
how valid is it to compare scores?
Comparison criteria
In a study, reading and spelling tests can be compared
by making use of criteria points in a parallel way to
an Audit of SEN. The following assumptions are made:
• 
if each reading test is developed from an adequate
population sample and is reliable and valid
according to test standards, the reading scores
obtained should be comparable
• 
each test should show a similar distribution of
scores with similar proportions of children predicted
to fall within specified values. If Test A showed,
for example, a standardised score of 80 (the 9th
percentile) for children scoring a reading age of 8
years at a chronological age of 10 years, the same
should apply for Test B
• 
the children forming the proportion may vary from
one test to another because different tests measure
different aspects of reading and children's skills
differ. For example, one child might make better
use of contextual cues than another.
Reading age criterion
A commonly set criterion for reading failure adopted
for movement from Stage 2 to Stage 3 is for reading
age to be two years below chronological age.
Educational Psychology in Practice Vol 15, No 1, April 1999
11

Percentile rank criterion
A percentile criterion threshold point for a similar stage
transfer is the 5th percentile rank: the score below
which 5 per cent of the standardised population would
fall in a specified age band.
With these levels of difficulty, an intervention by
school staff is likely to be needed and specialist advice
from an outside agency sought.
Method
The manuals for each reading and spelling test were
searched for the following information according to
the two comparison criteria:
• 
the proportions of children falling two years below
their chronological age were compared and
tabulated
• 
the reading ages of children at the 5th percentile
rank were compared and tabulated.
Comparisons were made: (i) for each test, and (ii) at
six-monthly intervals of chronological ages.
For each comparison the tables in the test manual
had to include both standardised score-raw score
conversions and reading age-raw score conversions.
At each age, a raw score obtained on a test could be
converted to both a standardised score and a reading
age.
Age criterion calculations
Two years were subtracted from each chronological
age to give the reading age. From the reading age-raw
score table, the raw score for this reading age was
found. This was the raw score of a child reading two
years below this chronological age for this test. The
raw score was then used in the standardised score-raw
score table to find the standardised score at the
chronological age and thence the percentile rank. The
percentile obtained for the test concerned was entered
in Table 1 at each chronological age point.
Percentile rank calculations
For each chronological age point and test, the table for
standardised score-raw score was entered at the 5th
percentile (SS75). The raw score at this value was
recorded. This raw score was taken to the reading
age-raw score table. The reading age equivalent to the
raw score was found and entered in Table 2 according
to chronological age and test. This was the reading age
at the 5th percentile. (Worked examples for each criterion
point are included in the Appendix.) An entry in the
tables refers to the chronological age in half yearly values.
The first edition of the Revised Neale has percentile
points for 12 month ranges, the same age being used
for conversion from, for example, CA 8;0 to 8;11
years. The value was taken to refer to the mid point
of the range, ie 8;6 years, and the corresponding yearly
points interpolated. The Reading Progress Test and
Table 1. Comparison for each test of the percentage of age groups with a reading age two or more
years below chronological age
Chronol. age
7;0
7;6
8;0
8;6
9;0
9;6
10;0
10;6
11;0
11;6
12;0
BAS-
WR
2
2
3
5
7
11
16
20
23
21
22
Graded
BASII-
WR
7
7
5
5
6
10
14
19
21
21
19
Word
NFER-
GWR
10
12
12
12
12
9
9
6
7
WORD-
BR
3
6
8
10
10
13
18
27
32
GRT
6-12
5
7
9
12
10
12
13
16
18
GRT
8-14
18
18
19
19
19
SUFF
7
8
9
12
13
16
18
20
Cloze
SPAR-R
3
5
8
13
16
19
21
23
21
YGR
3
4
6
10
13
16
18
19
19
N-ACC
2ed
8/3*
10/7*
14/10*
16
19
21
23
25
27
Prose
N-ACC
12
14
15
16
16
18
20
20
21
22
SALF
3
6
10
13
15
18
19
21
25
Example: The WORD Basic Reading (WORD-BR) predicts 8 per cent of 9 year olds will be reading two years below their
chronological age. ie 8 per cent atCA 9,0 years will have a reading age of 7;0 years or less.
*Difference between scores for Form 1 and Form 2 at these ages.
12
Educational Psychology in Practice Vol 15, No 1, April 1999

Table 2. Comparison for each test of the reading age at the 5th percentile point at chronological
age
Chronol. age
7;0
7;6
8;0
8;6
9;0
9;6
10;0
10;6
11;0
11;6
12;0
BAS-
WR
5;8
6,0
6;3
6;6
6;8
6;9
7;0
7;2
7;4
7;6
7;8
Graded Word
BASII-
WR
5;0-
5;4
6;1
6;7
6; 10
7;1
7;1
7;4
7;7
7;10
8;3
NFER-
GWR
6.0-
6.0-
6.0-
6;0
6;3
6;9
7;3
7;9
8; 6
9;0
9;9
WORD-
BR
6.0-
6.0-
6;3
6;3
6;6
6;9
7,0
7;3
7;6
7;9
8;0
GRT
5-T2
6,0
6;3
6;6
7,0
7;6
7;9
8;3
8;6
8;9
GRT
8-14
8,0*
8;6*
Cloze
SUFF
6; 6
6; 11
7;4
7;8
8,0
8; 5
8;8
SPAR-R
6;1
6;4
6;5
6;8
6; 10
7,0
7;1
7;4
7;5
7;8
YGR
5; 10
6;1
6; 5
6;7
6; 10
7;1
7;4
7;5
7;9
8;0
8;3
N-ACC
2ed
6,0
6,1
6;3
6;5
6;7
6;10
7;0
7;3
7;4
7;7
Prose
N-ACC
5;1
5;3
5;6
6;0
6;7
7;0
7;5
7;9
8;0
SALF
6;0
6;2
6; 5
6;8
6;11
7;2
7;5
7;8
7; 11
* Because there are only two values for GRT 8-14 they have been omitted from Figure 1.
the British Spelling Test series each recommend a
different conversion table for specified age ranges. The
recommendations have been adhered to and the results
entered chronologically. Some tests do not include
standardised scores or percentiles and were therefore
not included; for example, the Individual Reading Test.
Others, like the Revised Edinburgh Tests, because of
a restricted age range, were not open to the calcula-
tions. Non-inclusion does not represent any criticism
of the test on the part of the author.
Test groupings
Following the classification above, the tests were
grouped as below, together with abbreviations used
in tables. The parallel form used is also indicated.
Word Recognition
British Ability Scales Word Reading
British Ability Scales II Word Reading
NFER-Nelson (Macmillan) GRT
Wechsler Objective Reading
Dimensions - Basic Reading
Revised, Accuracy Form 1
Neale Analysis Of Reading Ability,
N-ACC
Cloze Type
NFER-Nelson GRT 6-12
NFER-Nelson GRT 8-14, Form X
Suffolk Reading Scale, Level 2
Form A
SPAR Reading, 2nd Ed
Young GRT, 2nd Ed
Prose Type
Neale Analysis of Reading Ability,
BAS-WR
BASII-WR
NFER-GR
WORD-BR
GRT6-12
GRT8-14
SUFF
SPARR
YGRT
Revised (2nd Ed), Accuracy Form 1
Salford Sentence Reading
Comprehension
Neale Analysis of Reading Ability,
Revised, Comprehension Form 1
Neale Analysis Of Reading Ability,
Revised (2nd Ed), Comprehension
Form 1
NFER-Nelson GRT 8-14, Form X
Reading Progress Tests, Stage 2
(RPT 3-6)
Wechsler Objective Reading Scale
Spelling
British Ability Scales
British Ability Scales II
British Spelling Test Series, Forms 2X
SPAR, 2nd Ed
Vernon Graded Spelling
Wechsler Objective Reading
Dimensions
N-ACCII
SALF
N-COMP
N-COMPII
GRT8-14
RPROG
WORD-
COMP
BAS-SP
BASII-SP
BSTS
SPARS
VERN
WORDSP
Results
The results are presented in parallel tables corres-
ponding to the alternative reading age and percentile
criteria. Comprehension and spelling tests are tabulated
separately because they test different skills to those
Educational Psychology in Practice Vol 15, No I.April 1999
13

being assessed in general reading achievement. Each
value is entered in the respective table at six-monthly
intervals of chronological age. In some instances, there
is no table value, for example, when the raw score at
the 5th percentile is below the normed reading
minimum.
The data in the tables do not easily fit into a form
for statistical analysis. Comments therefore relate to
observations about the scores obtained and do not
have the rigour that is usual in studies of this nature.
However, the results are of practical importance for
the allocation and placement of children at SEN stages.
Differences between and within tests are shown which
impose restraints on the generality with which tests
may be used to compare children's performance.
The age criterion
The general reading results (Table 1) show the majority
of tests indicating an increasing proportion of children
meeting the age criterion, reading two years or more
below chronological age, as the children grow older.
At 8 years of age, most tests show about 5 per cent
meeting the criterion. The proportion increases to
between 10 and 15 per cent at 10 years and 20 per
cent or more at 12 years of age.
There are marked differences between the NFER-
GWR and most of the other test scores. This test shows
little variation in the proportions of children meeting
the criterion over the age range. Just six percentile
points variation over four years is the smallest, with
a slight numerical fall as age increases. Only half the
number of older children meet the criterion for the
NFER-GWR when it is compared to the other tests.
This is significant if the NFER-GWR is used by schools
to show need for allowances at GCSE examinations
which have an age-based criterion. The GRT 8-14
also shows little difference between scores but they are
similar in value to other test scores for the age range
under investigation.
The age criterion also indicates differences between
the first edition of the new Neale Accuracy and other
tests for young children under 9;0 years. Between the
ages of 7;0 and 9;0 years, a critical time for the detection
of reading failure, this test shows about 12-15 per
cent. Most other tests have a value of 3-7 per cent.
Using the criterion with the Neale is likely to produce
two to four times as many children. Some change has
been achieved in the second edition but the proportion
predicted remains the highest for young, poor readers
under the age of 9 years. More worrying is a difference
between the two parallel forms of the second edition
for this younger group.
It is also possible to compare directly the original
BAS Word Reading standardised in 1976 and the
Revised Version of 1995. Thirty-eight out of the first
40 words are common to both, although not in the
same order of difficulty. For those children below 8;0
years, there are two to three times more children
meeting this criterion today than 20 years ago. At the
age of 8 years children's reading extends beyond the
common 40 words and no further direct comparisons
may be made.
There are also interesting results for the older age
range. The WORD-BR, which is the preferred reading
test of many EPs, shows nearly one third of the
population of 12 year olds meeting the criterion. To
extend the age range of this investigation, if, as was
the case a few years ago, the GCSE examination
dispensation was based on a two years below average
criterion, approaching 50 per cent of the 16+ age group
would be eligible.
The percentile criterion
The data for the 5th percentile criterion are shown in
Table 2. There is good agreement between the reading
age and chronological age, with the majority of tests
deviating from the mean value by less than six months
of reading age through the range investigated. Between
8 and 10 years, the spread for most is no more than
three months.
In Table 2, the lower reading age values for the first
edition of the revised Neale and younger children are
clearly illustrated. Equivalent reading age scores at the
5th percentile show the Neale to be nine to 12 months
below most other tests for children of 9;0 years and
below. At 8;0 years of age a reading accuracy score
for the Neale is under 5lA years and represents the
same proportion of the population as a reading score
of 6-6V2 years for most other tests. The second edition
shows reading age scores closer to the other test scores.
The NFER GWR seriously underestimates at the
upper end of this age range. At CA 11;6 years it
suggests a reading age some 12-18 months above the
mean of other tests for the same proportion of the
population.
Figure 1 has been included despite the rather confused
image. It is difficult to separate each of the lines in the
figure because they are so similar in form. To link this
criterion with the previous (age) criterion, the line
representing two or more years reading age below
chronological age could be added to Figure 1. It is a
diagonal from the lower left, CA 7;0 and RA 5;0, to
the upper right, CA 12;0 and RA 10;0.
Comprehension tests
The five comprehension tests do not show the same
gradual progression of percentile values at a reading
14
Educational Psychology in Practice Vol 15, No I.April 1999

-BAS-WR
-BASI1-WR
-NFER-GWR
-W-WORD-BR
- G R T 
6-12
-SUFF
-SPAR-R
- Y G R
-N-ACC
-SALF
7;6
8;0
8.6
10;6
12;0
9;0 
9;6 
10;0
Chronological Age
Figure 1. Comparison of reading tests at the 5th percentile
Table 3. Comparison for each test of the percentage of age groups with a reading age two or more
years below chronological age
Chronol. age
7;0
7;6
8;0
8;6
9;0
9;6
10;0
10;6
11;0
11;6
12;0
N-COMP
4
10
15
18
20
19
18
20
21
22
Comprehension Tests
N-COMP 2ed 
GRT 8-14
9
12
14
18
19
21
23
23
21
21
18
16
18
RPROG
10
10
21
21
23
23
21
23
23
23
23
WORD-COMP
3
12
12
14
13
13
14
18
21
age two years below that was apparent for the three
other test types. Children appear to reach a percentile
of between 12 per cent and 20 per cent at about 8V2
years and maintain this level until the 12-year-old
upper value is reached. The second edition of the new
Neale shows lower proportions than the original
version for younger children but similar values as age
increases (see Table 3).
At the 5th percentile criterion (Table 4), there is
good numerical agreement between the first edition of
the new Neale and WORD above the 9-year level in
much the same way as seen in the other test grouping.
The second edition is a good match with WORD across
the range. The Reading Progress Test shows a pattern
of development unlike either, with reading age scores
some 18-24 months below. In the age range under
consideration, the reading age scores for the GRT 8-14
were below the test minimum of 8;0 years for the 5th
percentile. The GRT 8-14 column is therefore omitted
from Table 4.
Educational Psychology in Practice Vol 15, No 1, April 1999
15

Table 4. Comparison for each test of the reading ages at the 5th percentile for chronological ages
Chronol. age
7;0
7;6
8;0
8;6
9;0
9;6
10;0
10;6
11;0
11;6
12;0
N-COMP
5;1
5;5
5;9
6;2
6;6
6;10
7;2
7;5
7;8
7;8
Comprehension Tests
N-COMP 2ed 
RPROG
6;1
6;4
6;6*
6; 10
7;0
7;1
7;4
7;7
6;7
5,0
5;6
5;8
5;10
8;6
WORD-COMP
6;3
6;3
6;6
6;9
7,0
7;3
7;6
7;9
* interpolated 
score.
Spelling tests
Most of the spelling tests mirror the development
trend suggested by the main group of reading tests.
An increasing number of children meet the age
criterion, as they grow older (see Table 5). Very
similar proportions, too, are indicated for the spelling
tests as had been shown for the reading tests. The
new British Spelling Series presents a totally different
picture. A standard proportion of about 20 per cent
of children meets the two-year age criterion for this
new test series.
The 5th percentile criterion, illustrated in Table 6,
shows the close relationship between spelling age and
chronological age for all tests except the BSTS. Spelling
improves by about 2Vi years in the 5-year range for
most of the tests. The BSTS shows gains of one year
of spelling for each chronological year for the range.
Conclusions
When establishing criteria for a SEN audit, a research
study or in general professional practice, it is essential
to use percentiles or standardised scores in addition to
achievement and chronological ages. The proportions
of children meeting a reading or spelling age criterion
Table 5. Comparison for each test of the percentage of age groups with a spelling age two or more
years below chronological age
Chronol. age
7;0
7;6
8;0
8;6
9;0
9;6
10;0
10;6
11;0
11;6
12;0
BAS
2
2
3
5
7
11
16
20
23
21
22
BASH
2
4
3
3
5
8
14
18
19
18
16
BSTS
18
18
18
18
18
18
19*
Spelling
SPAR-S
2
6
9
14
16
19
19
21
21
VERN
4
7
10
12
13
14
16
18
18
WORDS
3
6
7
10
18
19
18
19
21
*value atCA =
years.
16
Educational Psychology in Practice Vol 15, No 1, April 1999

Table 6. Comparison for each test of the spelling age at the 5th percentile point at chronological
age
Chronol. age
7;0
7;6
8;0
8;6
9;0
9;6
10;0
10;6
11;0
11;6
12;0
BAS
5;8
6;0
6;3
6;6
6;8
6;9
7;0
7;2
7;4
7;6
7;8
BASH
5;4
5; 10
6;4
6;10
7;1
7;4
7;4
7;7
8,0
8;3
8;9
BSTS
5;0
5;7
5; 10
6;8
7;0
7;6
7;9*
Spelling
SPAR-S
6;1
6;2
6;4
6;7
6;9
6;11
7;0
7;1
7;3
7;7
VERN
5;9
5; 11
6;1
6;4
6;5
6;8
6; 10
7;1
7;5
7;8
8;0
1/l/ORD-S
6;0
6;3
6;9
7;0
7;0
7;3
7;9
8;0
8;3
*value atCA = 11;11 years.
increase, as children grow older. Any conclusions based
only on an achievement age are seriously compromised.
Between 3 and 5 per cent of 8 year olds meet a two
years below chronological age criterion but about 25
per cent of 12 year olds and nearly 50 per cent of
school leavers.
National Curriculum Levels are open to a similar
criticism, because they are achievement age related
and should not be seen as independent of the
standardised score.
These conclusions have important implications,
particularly for younger children. At the 5th percentile
at 8 years of age, the first edition of the new Neale
scores 5;6 years and the BAS Word Reading scores 6;6
years. To quote, therefore, only a reading score on the
new Neale for an 8-year-old poor reader is to exaggerate
the difficulty. This may well be of some interest to EPs
whose results differ in a SEN Tribunal.
Evaluation of intervention programmes
Concern, therefore, must be expressed when only
reading or spelling age scores are used to evaluate the
progress made by children following intervention. This
paper shows that although reading age scores fall
further behind relative to chronological age, children
may in fact be improving relative to their age group.
For example, consider a child who has maintained a
disparity of two years below chronological age from
8 to 12 years. Rather than be disappointed that the
'gap' between reading age and chronological age has
not closed, it is a real gain; the child has moved from
the third percentile to within the average range (22nd
percentile). Reliance on validating interventions using
only achievement ages underestimates the value of the
teaching given.
Similarly, doubt must also be attached to NC 'Added
Value' calculations if the Level differences between
children at the end of KS1 are different to those at the
end of KS2, KS3 or GCSE grades. Level differences at
KS1 are affected significantly by within year ages. For
example, using the KS1 1998 Reading Comprehension
test, to obtain Level 3 requires a raw score of 14. This
becomes SS115 (84th percentile) for a child of 6;9
years, but SS109 (73rd percentile) for one of age 7;8
years who will be in the same year group. The same
raw score predicts a greater proportion of older children
meeting the Level criterion. The proportions are much
closer for the Reading Comprehension Level 2 test,
but for the Spelling Test the differences between are
more pronounced for all of the Level estimates.
An alternative validation method and diagnostic
implications
Inter-test validity is usually established by evaluating
the correlation between two tests administered to the
same group of children. In this study the similarity of
reading/spelling age scores and chronological age at
the 5th percentile parallels the good validity which
exists between tests at the 5th percentile over the age
range investigated. Above the age of 8 years, there is
good comparability of scores obtained. Differences
between reading test scores, therefore, have diagnostic
significance and are not related to statistical differ-
ences between tests. That comprehension tests differ
Educational Psychology in Practice Vol 15, No 1, April 1999
17

in developmental form from the general reading tests
is not unexpected. Drawing conclusions from text is
different conceptually to recognising what is written.
It is important when validating tests to go beyond
a simple correlation. Similar values should be shown
for different tests across the achievement range and
age range of the tests concerned. Making statistical
comparisons between tests at each of several percentile
points may be an alternative method of validation and
is a further research development to be considered.
Rate of progress
At the 5th percentile point, most of the tests showed
an average gain of about six months over a 12-month
period. This contrasts to the average reader at the 50th
percentile gaining one year of reading in a year. Poor
readers, therefore, start to read later, progress more
slowly than does an average reader and probably
continue to find reading difficult throughout life. This
can be illustrated using the Wide Range Achievement
Tests (Jastak and Wilkinson, 1984) which extend across
the human age span from 5 years to old age. Some
people learn and remember the words from a standard
list quicker than others do. As is shown in Figure 2,
we reach different plateaux of words recognised when
in our 20s or 30s, at which point the decline begins.
Should we be spending more time developing an
individual's strengths rather than devoting so much
time and effort, as some express, to try to bring
everyone to an average level in reading? If reading is
a multiply-faceted skill, should we be surprised if people
differ in reading skills throughout their lives?
There was a suggestion that two of the tests, the
NFER-GRT and the new British Spelling test, showed
a different rate of progress. Both showed rates of
progress similar to that expected at the 50th percentile.
Poorer achievers made the same rate of progress as
good readers for these tests but were behind because
they started later. The tests are both products of the
Figure 2. Wide Range Achievement Tests - Revised.
One and two standard deviations from the mean
reading research group based at the University of East
London. Are different sets of statistical constraints
added which would alter the developmental character-
istics of the tests? Further investigation is necessary.
Evidence of a decline in standards?
It is possible to make a comparison between the original
and revised versions on the BAS Word Reading which
may add to the evidence about changing reading
standards. The first 40 or so of the words on the reading
card are common to both tests, although with a changed
order of difficulty. The responses of younger poor readers
are mostly restricted to these words. It is possible,
therefore, directly to compare the performance of young
poor readers on the same set of words between the
original standardisation in 1975 and the revision
published in 1996. Twice as many younger children were
reading two years below their chronological age in 1996
as there were in 1975. This is direct evidence of a decline
in reading standards over the past 20 years. The stimulus
words change after the first 40. No further direct
comparisons can be made.
Acknowledgements
I should like to thank my friends and colleagues Sandie Smith
and Marie Cherrington for their assistance in preparing this
paper. The comments of the referees also were helpful and
constructive.
References
Bookbinder, G.E. (1976) The Salford Sentence Reading Test.
London: Hodder and Stoughton.
Cook, J. and Cook, R. (1996) The Reading and Spelling Progress
Charts - Revised. Littlehampton: Lyminster Publications.
Department for Education (1994) Code of Practice on the
Identification and Assessment of Special Educational Needs.
London: Central Office of Information.
Elliott, C.D., Murray, D.J. and Pearson, L. S. (1983) British
Ability Scales - Word Reading. Windsor: NFER-Nelson.
Elliott, C.D. with Smith, P. and McCulloch, K. (1997) British
Ability Scales - Word Reading. Second Edition. Windsor:
NFER-Nelson.
Gledhill, M., Hall, B. and McDevitt, B. (1992) 'Reading Standards
in Cheshire Schools'. In Pumfrey, P. (Ed) Reading Standards:
Issues and Evidence. Leicester: British Psychological Society.
Gregory, H. M. and Gregory, A. H. (1994) 'A comparison of
the Neale and the BAS Reading Tests', Educational Psychology
in Practice, 10 (1), 15-18.
Hagley, F. (1987) The Suffolk Reading Scale. Windsor:
NFER-Nelson.
Halliwell, M. and Feltham, R. (1995) 'Comparing the Neale and
the BAS Reading Tests: a reply to Gregory and Gregory',
Educational Psychology in Practice, 10 (4), 228-230.
Jastak, S. and Wilkinson, G. S. (1984) The Wide Range
Achievement Test - Revised. Wilmington, Delaware: Jastak
Associates.
18
Educational Psychology in Practice Vol 15, No 1, April 1999

Macmillan Test Unit (1985) NFER-Nelson Group Reading Test
6-12. Windsor: NFER-Nelson.
Macmillan Test Unit (1985) NFER-Nelson Group Reading Test
9-14. Windsor: NFER-Nelson.
Neale, M. D., Christophers, U. and Whetton, C. (1989) The
Neale Analysis of Reading Ability — Revised British Edition.
Windsor: NFER-Nelson.
Neale, M. D., Whetton, C., Caspall, L. and McMulloch, K.
(1997) The Neale Analysis of Reading Ability - Second
Revised British Edition. Windsor: NFER-Nelson.
Pumfrey, P. (1992) Reading Standards: Issues and Evidence.
Leicester: British Psychological Society.
Qualifications and Curriculum Authority (1998) Key Stage 1
English Tests. Teachers Guide Levels 1-3. London: HMSO.
Rust, J., Golombok, S. and Trickey, G. (1993) Wechsler Objective
Reading Dimensions - British Edition. Sidcup: The Psycho-
logical Corporation.
Sacre, L. and Masterson, J. (1997) 'A comparison of four reading
tests', Educational Psychology in Practice, 13 (3), 188-196).
Schonell, F. J. (1965) Reading and Spelling Tests. London: Oliver
and Boyd.
Stothard, S. and Hulme, C. (1991) 'A note of caution concerning
the Neale Analysis of Reading Ability (Revised)', British
Journal of Educational Psychology, 61, 226—229.
Turner, N. (1990) Sponsored Reading Failure. Warlingham:
IPSET.
Vincent, D. and Crumpler, M. (1997) British Spelling Test Series.
Windsor: NFER-Nelson.
Vincent, D., Crumpler, M. and de la Mare, M. (1996) Reading
Progress Tests. London: Hodder and Stoughton.
Whetton, C. (1990) The Neale Analysis of Reading Ability
(Revised British Edition) - A technical paper. Windsor:
NFER-Nelson.
Young, D. (1980) Young Group Reading Test (2nd Ed).
Sevenoaks: Hodder and Stoughton Educational.
Young, D. (1987) SPAR Spelling and Reading Tests (2nd Ed).
London: Hodder and Stoughton Educational.
John Cook is Specialist Senior Educational Psych-
ologist with West Sussex LEA. His Lyminster Publica-
tions produces The Reading and Spelling Progress
Charts. The address for correspondence is Dragon
House, Lyminster, Littlehampton, West Sussex,
BN17 7QJ. E-mail: jandrcook@mcmail.com and
website www.readingcharts.cwcom.net
This article was accepted for publication in October
1998.
Appendix
Example 1: Age criterion calculation
For the NFER-Nelson GRT6-12, 30 correctly answered
questions give the following score:
At 10 years of age:
From Table 4, page 14 of the Manual,
Reading Age = 8.0 years gives a Raw score = 30
From Table 3, page 13 of the Manual
At CA=10,
Raw Score = 30 SS = 80
This test therefore gives a standard score of 80 for CA 10
years and RA 8 years.
However, SS =80 represents the 9th percentile rank.
The GRT 6-12 predicts that some 9 per cent of children will
have a reading age of 8 years or less at a chronological age
of 10 years.
In Table 1, at CA 10 years the NFER-Nelson GRT shows a
value of 9.
Example 2: Percentile criterion calculation
For the BAS Word Reading Test A, 23 words correctly read
gives the following scores:
At 8;6 years of age,
From Centile table, page 116 of Manual 4
T = 33, Centile point = 5th,
Gives Ability = 70
From Table for Test A, pagell4 of Manual 4
Ability = 70 gives raw score = 23
From Word Reading table, page 118 of Manual 4
Ability score = 70 gives reading age = 6;6 years
The BAS Word Reading Test predicts that at die 5th percentile
at a chronological age of 8;6 years, children will have a
reading age of 6;6 years.
In Table 2, at CA 8;6 years the BAS shows the value 6;6.
(Note: exceptionally for this example, this reading age also
represents the first reading age criterion. In Table 1 therefore,
at CA 8;6 years the BAS shows the value 5.)
Educational Psychology in Practice Vol 15, No 1, April 1999
19

