Control Transformer: Robot Navigation in Unknown Environments
through PRM-Guided Return-Conditioned Sequence Modeling
Daniel Lawson and Ahmed H. Qureshi
Abstract— Learning long-horizon tasks such as navigation
has presented difficult challenges for successfully applying rein-
forcement learning to robotics. From another perspective, under
known environments, sampling-based planning can robustly
find collision-free paths in environments without learning.
In this work, we propose Control Transformer that models
return-conditioned sequences from low-level policies guided
by a sampling-based Probabilistic Roadmap (PRM) planner.
We demonstrate that our framework can solve long-horizon
navigation tasks using only local information. We evaluate our
approach on partially-observed maze navigation with MuJoCo
robots, including Ant, Point, and Humanoid. We show that
Control Transformer can successfully navigate through mazes
and transfer to unknown environments. Additionally, we apply
our method to a differential drive robot (Turtlebot3) and show
zero-shot sim2real transfer under noisy observations.
I. INTRODUCTION
Long-horizon robot control (LRC) from raw local observa-
tions is a challenging task requiring a robot to make decisions
in a reactive manner, avoiding collisions from partially
observed obstacles, while navigating toward the desired goal
state. Once solved, it will have widespread applications rang-
ing from search and rescue [24] to autonomous driving [3],
where local observations are usually available to the robot
system for decision-making. Deep Reinforcement Learning
(DRL) [14] has demonstrated promise in learning robotic
control policies from local observations. However, standard
DRL methods struggle with long-horizon problems and are
often exhibited in toy tasks with no collision avoidance
constraints.
Recent advancements have led to offline DRL methods
[4], [16] that leverage a robot motion datasets from an expert
for training sequence models solving control problems. How-
ever, to date, these methods are demonstrated in solving sim-
ulated locomotion tasks with either no collision-avoidance
constraints or noisy real-world local sensory observations.
Furthermore, the assumption of having a large robot skill
dataset a priori limits their applicability to scenarios where
obtaining robot skills is challenging. The unsupervised robot
skill discovery methods [2], [36] could be used to provide
a way to obtain the dataset for offline DRL. However, these
methods do not generalize to high-dimensional robot systems
such as 23 Degree-Of-Freedom (DOF) Mujoco Humanoid
Robot or struggle in exploring complex, cluttered, large
environments.
The authors are with the Department of Computer Science, Purdue
University, West Lafayette, IN 47907 USA (email: lawson95@purdue.edu;
ahqureshi@purdue.edu)
Fig. 1: Control Transformer navigating in cluttered envi-
ronments with a differential drive robot, showing sim2real
transfer. In each trajectory, the robot is conditioned to reach
a target goal position.
To overcome the limitations mentioned above and scale
offline DRL approaches to complex, long-horizon, and real-
word robot motion tasks, this paper presents a novel frame-
work called Control Transformer (CT). The CT method
solves long-horizon, high-dimensional, robot navigation and
locomotion tasks from local raw observations. The main
contributions of the paper are summarized as follows:
• A scalable Probabilistic Road Map (PRM)-guided method
for robot skill discovery that learns robot local policy en-
semble over PRM Graph in large, cluttered environments.
• A return-conditioned Transformer framework for sequence
modeling of robot behavior using associated local observa-
tions. The return conditioning is based on a learnable value
function that determines the initial cost-to-go to the target
position and guides the Transformer’s sequence modeling
and generation process.
• A fine-tuning strategy that leverages the failures of CT
during execution and refines its behavior without catas-
trophic forgetting, leading to overall better performance in
challenging cluttered scenarios.
• Evaluation of the proposed CT method in complex, multi-
task simulated and real-world settings with 2 DOF point-
mass, 3 DOF differential drive, 12 DOF ant, and 23 DOF
humanoid robots. The real-world experiments are with
Turtlebot3 demonstrating direct sim2real transfer of our
framework.
• The results demonstrate that our method outperforms ex-
isting approaches, and to the best of our knowledge, this
paper presents the first in-depth analysis and evaluation
of the transformer-based offline RL method in solving
complex problems and their sim2real transfer with noisy
robot states and sensor information (Fig. 1).
arXiv:2211.06407v3  [cs.RO]  13 Jul 2023

II. RELATED WORK
A. Hierarchical Reinforcement Learning and Skill Learning
An approach for extending RL to long-horizon tasks are
Hierarchical Reinforcement Learning (HRL) methods [13]
which use several policies acting at different levels of ab-
straction. Many approaches exist, such as sub-goal or feudal
HRL methods where higher-level policies provide goals for
lower-level policies to execute [26], [39]. Other methods
have a higher level policy select lower-level policies, or
options, to execute; this can be learned in an end-to-end
manner [1] but it can be difficult to transfer options to
different tasks. Other HRL methods independently first focus
on learning low-level skills in an unsupervised manner, such
as empowerment methods, which aim to learn diverse skills
that can be used for different downstream tasks [7], [11],
[36]. Deep Skill Graphs (DSG) [2] introduces graph-based
skill learning using the options framework, and learns a
discrete graph representation of the environment. During un-
supervised training, a skill graph is incrementally expanded
through learning options. For obtaining a low-level policy,
we also use graph-based learning but show how we can
obtain a single goal-conditioned policy to complete many
skills provided by sampling-based planning, rather than many
unconditional policies (or options). Rather than having a
hierarchy of policies as in HRL, trained in an end-to-end
manner, our low-level policy serves to generate data for a
more capable transformer policy.
B. Sampling-based Planning
Sampling-based planning methods such as Probabilistic
Roadmaps (PRM) [19] and Rapidly Exploring Random Trees
[22] are algorithms traditionally used in motion planning
to find constraint-free paths from one point to another in
a robot’s configuration space. This is accomplished through
sampling many constraint-free points and connecting neigh-
boring points if they are reachable using local planning.
Sampling-based planning has been extended through neural
motion planning (NMP) [15], [17], [29] approaches which
train neural networks that efficiently plan by learning sam-
pling distributions from expert demonstrations. In contrast,
our approach learns by imitating executed actions follow-
ing sampling-based plans rather than learning the plans
themselves as in NMP. Sampling-based planning has been
combined with RL in PRM-RL [8], providing a simple
method for long-horizon navigation tasks. Sampling-based
planning provides robots with high-level plans, where low-
level navigation between waypoints in the plan is executed
using a learned goal-directed policy. While our method uses
PRMs during training, we obtain a policy that can operate
without sampling-based planning, allowing it to operate and
generalize to environments that are unknown and partially
observed.
C. Conditional Supervised Learning
Recent work has investigated posing online RL as a
supervised learning problem [6] via goal [10] or reward
conditioning [20], [35], [37]. Instead of learning a normal
policy, which given a state, directly outputs optimal actions,
a return-conditioned approach learns to conditionally output
actions given both state and desired reward. While trans-
formers [38] are an appealing model for sequential decision
problems, it has been difficult to adapt transformers to
typical online RL [28]. However, Decision Transformer (DT)
[4], has found success using transformers by training them
as return-conditioned policies. DT operates in the offline
setting, requiring high-quality data. In this paper, we provide
a method for training Decision Transformer style policies
without prior expert demonstrations and scaling them to
complex problems with collision constraints and noisy real-
world observations.
III. METHOD
We now begin to discuss our proposed methods, which
are summarized by Algorithm 1 and shown in Fig. 2.
Algorithm 1: Overall Control Transformer (CT)
Input: recovery iterations I , trajectory per iteration T
Output: CT πθ, undiscounted value function Vϕ
πc ←with Algorithm 2 or set to known low-level
controller
T ←Collect T long-horizon trajectories by guiding πc
with sampling-based planning via Algorithm 3.
Vϕ ←argminϕ E(st, ˆ
Rt)∼T [( ˆ
Rt −Vϕ(st|gt))2] via SGD
Lθ = Eτ∼T [∥at −πθ(at|·)∥2]
πθ ←argminθLθ via SGD
/* Optional fine-tuning
*/
for i = 1 . . . I do
Collect T rec, T fail with Algorithm 4
T ←T S {T rec, T fail}
πθ ←continued training with Lθ
end
A. PRM-based Task Graph
We first generally discuss how sampling-based planning
can decompose long-horizon control problems. We use
Probalistic Roadmaps (PRM) [19], obtaining a graph G =
(E, V ) where vertices are constraint-free points and edges
signify that points are reachable using a local planner. We
can obtain this graph by sampling n constraint-free points
in our environment, and then connect the sampled point
to neighbors within connect-distance d, forming an edge in
the graph, if a constraint-free path exists from one point to
another.
Given a graph G obtained via PRM, and a local controller
which we describe as a policy πc(a|s, g), we can reach any
goal point xg from any start point x0. This is accomplished
by finding the closest points in the graph to the start and the
goal, w0 and wg, and then using a shortest path algorithm
to obtain a sequence of waypoints (w0, w1, . . . , wg). A robot
can then traverse from start to goal by taking actions sampled
from πc(a|s, g = wt −xt), where wt is the closest waypoint
not yet reached at the current timestep, and xt is the robot
position at the current timestep. The sequence of waypoints
or plan that guides πc can be fixed, or updated as the robot
acts.

B. Local Skill Learning
In this section, we will describe a method for learning πc.
Optionally, in cases where we have a robot with a simple
model and known reliable controller, we could skip this step.
We demonstrate the former in our MuJoCo experiments and
the latter with a differential drive controller in our Turtlebot3
experiments. We use goal-conditioned RL [18], modeling
our problem through a goal-conditioned Markov decision
process (MDP) described by tuple (S, G, A, R, p, T, ρ0, ρg),
where S, A are the state and action spaces, G is the goal
space, p is the (unknown) transition function p(s′|s, a), ρ0
is a distribution over start states and ρg is a distribution
over goal states, and T is the maximum length of an
episode. In the goal-conditioned setting, the reward, R, is
also conditioned on the goal, giving R(s, a|g). We aim to
find a policy π∗(a|s, g) that maximizes expected return,
Es0∼ρ0,g∼ρg,at∼π,st∼p[Pt=T
t=0 R(st, at|g)] [27]. We propose
that we can use sampling-based planning to provide goals
to train policies. Given a training environment, we first
use PRM to obtain a graph G = (V, E). During each
episode of training, we sample an edge from the graph
which serves as the start and goal for that episode. This
process is described in Algorithm 2, which is compatible
with any goal-directed RL algorithm. Specifically, we use
Soft Actor-Critic (SAC) [12] in our experiments with dense
rewards that are proportional to progress toward the goal.
Low-level policies can be efficiently trained, as the state
space of policies only contains proprioceptive information,
like robot joint angles and velocities, as they do not need to
learn to avoid constraints such as obstacles.
Algorithm 2: Training Low-level Controller
Input: Graph G = (V, E), episodes T
Output: goal-directed policy πc
ω(a|s, g)
for t = 1 . . . T do
/* Reset environment with new goal
edge and robot configuration
*/
x0, xg ←edge e sampled uniformly from E
gt ←xg −xt
πc
ω ←perform an episode of policy learning
end
C. PRM-guided Skill Discovery
Given πc we can use the process defined in Section
III-A as a data generating process as described in Al-
gorithm 3. This process operates by guiding πc, return-
ing a set of trajectories T
=
{τi}T
i=1 where τi
=
(sp
0, g0, at, rt, . . . sp
T , gT , aT , rT ), which we call planning
trajectories. Goals and rewards g and r are set to be with
respect to the final goal rather than the waypoints that πc
followed. While the original states, sp
t may only contain low-
dimensional proprioceptive information such as robot joint
configurations, we augment the state with high-dimensional
exteroceptive observations, ot, such as a local map or camera
mounted on the robot, providing st
= (sp
t
: ot). By
adding local observations, a policy trained on this data can
learn to act without the need for planning. Additionally, we
can use any reward function Rτ(s|g) for labeling planning
trajectories, which can be different than the reward function
used to train a low-level policy.
Algorithm 3: PRM-guided data collection
Input: number of trajectories to collect T, reset interval L,
Graph G, controller πc
Output: planning trajectories T
T ←∅
successes ←0
i ←0
while successes < T do
if i % L = 0 then
/* Randomize environment
*/
G ←(V, E) Sample new PRM over environment,
providing robot configurations V and edges E.
end
x0, xg ←sampled vertices from V
xt ←x0
τ c = (sp
0, gc
0, a0, rc
t, . . . sp
T , gc
T , aT , rc
T ) ←perform
rollout of πc(· | st, gt = xtarget −xt), updating
xtarget with plan(xt, xg, G) as waypoints xtarget
reached.
gt ←xg −xt
rt ←Rτ(sp
t+1|g = xg −xt)
st ←(sp
t : ot)
τ = (s0, g0, at, rt, . . . sT , gT , aT , rT )
if ∥xt −xg∥< ϵ. then
T ←T S{τ}
successes ←successes + 1
end
i ←i + 1
end
D. Control Transformer
This section presents the architecture and training process
for Control Transformer.
1) Decision Transformer: We first begin by explaining
the architecture for Decision Transformer [4]. DT con-
siders RL as a sequence modeling problem, where a se-
quence is a trajectory τ
= ( ˆR0, s0, a0, . . . , ˆRT , sT , aT )
consisting of states, actions, and return-to-go (RTG), ˆ
Rt =
PT
i=t rt. To predict an action, DT uses a deterministic policy
πθ(at| ˆRh:t, sh:t, ah:t−1) where h = min(1, t −k) and k
is the context length, which specifies the number of past
transitions which are used to predict the next action. After
embedding each state, action, and return-to-go, DT uses a
GPT-style [31] autoregressive transformer decoder to predict
actions. During evaluation, we can condition on a target
return-to-go for the first timestep which can be equal to
the maximum return-to-go achieved in the dataset, or this
value times some constant. In some cases, this leads DT to
extrapolate, exhibiting behavior that exceeds the best in the
dataset. For conditioning during the following timesteps, we
can adjust the target return-to-go by the empirical reward,
giving ˆRt = ˆRt−1 −rt−1, so one only needs to estimate
a good initial value. We refer to an estimate of RTG at
time t as ˜
Rt. We can train a DT by sampling trajectories

Robot Skill Discovery
Long-horizon Control
PRM-based Task Graph
Control Transformer
Fig. 2: Overview of our learning process. We use PRMs to decompose navigation tasks into discrete graphs, where each
edge can be considered a skill. We can use a known low-level controller, or edges can be used as goals for training a
a low-level policy with model-free RL. We then guide a controller to complete long-horizon tasks, collecting trajectories.
On these trajectories, we train Control Transformer to perform return-conditioned sequence modeling. Afterward, we can
optionally fine-tune Control Transformer with planning-guided fine-tuning on failure cases without catastrophic forgetting.
and minimizing mean-squared error on action predictions,
as used in Algorithm 1.
2) Control Transformer: To learn from sampling-guided
data collection, we consider approaching our problem
as a similar sequence modeling problem, but we con-
sider a goal-directed problem, with trajectories of the
form τ = ( ˆR0, s0, g0, a0, . . . , ˆRT , sT , gT , aT ), and policy
π(at| ˆRh:t, sh:t, ah:t−1, gh:t). We also consider the partially-
observed multi-environment setting, where a policy may
operate in multiple environments with the same underlying
task (navigation) but with a different structure for each
environment. While we can learn to autoregressively predict
actions on this sequence, we would face problems when
conditioning our policy, as in DT, the optimal return-to-go is
assumed to be constant. This is because we do not know an
optimal ˆR0 which is dependent on the unknown environment
structure, which may change across episodes, and starting
state and goal positions. Thus, we must explore changes that
will allow Decision Transformers to be able to generalize to
unknown environments, operating from any starting position
to any goal. We propose a modification that is compatible
with any goal-directed sequential modeling problem.
One approach would be to learn the full distribution of
return from offline data, Pθ( ˆRt| ˆRh:t−1, sh:t, ah:t−1) [23],
followed by sampling from this distribution for conditioning
during inference time. However, it is difficult to learn the
full distribution of return in the goal-directed setting in such
a way that we can generalize, predicting return in unseen
environments. Instead, we propose learning the mean of this
distribution, the undiscounted goal-directed value function
V (st|gt) = ET [PT
i=t rt|st, gt]. This is approximated by
Vϕ(st|gt), which estimates the expected return-to-go at state
s given goal g under the data distribution T . This function
is also not conditioned on past history, as at inference
time, we predict the first return, ˜R0 = Vϕ(s0|g0), and use
˜Rt = ˜Rt−1 −rt−1. We parameterize Vϕ as a separate, neural
network and minimize:
L(ϕ) = Est,at, ˆ
Rt∼T [( ˆ
Rt −Vϕ(st|gt))2]
To query for more optimal behavior, one could condition
on kVϕ(st|gt), where k is some constant. Alternatively, one
can train Vϕ on only the top X% trajectories, or those
that satisfy some condition, such as trajectories where no
collision occurred.
E. Planning-guided Fine-tuning
A common problem with offline learning relates to distri-
bution shift, where when a trained policy is deployed, the
distribution of trajectories encountered while rolling out the
policy does not match the training distribution [34]. This
can cause compounding errors, leading to situations where
the policy cannot recover. For example, in navigation, the
policy could make slight errors, leading to an unrecoverable
obstacle collision. In our framework, the low-level controller
guided by planning, may have made few collisions, thus
there may be few or no demonstrations of recovering from
significant failure in the training distribution. To handle
this problem, we propose planning-guided fine-tuning as
described in Algorithm 4. After offline training, we roll
out πθ until some function f(s) identifies a failure, after
which, we take over with πc guided by sampling-based
planning. If πc successfully reaches the goal by the end of
the episode, then we add two trajectories to our dataset,
where the first is an unsuccessful trajectory for reaching
the final goal that ends when failure is detected, and the
second trajectory demonstrates successfully reaching the goal
starting from failure. After augmenting our dataset with
recovery and failure trajectories, we can perform additional
offline learning.
F. Architecture details
In this section, we provide additional architectural details
for CT. Obstacle information is represented using local oc-
cupancy maps, which are processed by a small convolutional
neural network (CNN). Occupancy map representations are

Algorithm 4: Recovery-based data collection
Input: Control Transformer πθ, low-level controller πc,
value function Vϕ, failure identifier f(s), number of
recoveries to collect T
Output: trajectories T rec, T fail
recoveries ←0
while recoveries < T do
τ fail ←rollout πθ until f(s) returns true, recording
start of episode until failure.
τ rec, success ←take over with πc guided by
sampling-based planner as in Algorithm 3.
if success then
T fail ←T fail S τ fail
T rec ←T rec S τ rec
recoveries ←recoveries + 1
end
end
then concatenated with both the proprioceptive state and the
goal embeddings, resulting in a single state embedding. For a
single timestep, embeddings of states, actions, and returns are
separately passed as inputs to CT. A shared learned positional
embedding is also added to each modality’s embedding.
When predicting an action, we pass the last K timesteps,
feeding 3K embeddings to the transformer. With K > 1
the transformer has memory over its past interaction. After
passing input embeddings through the transformer, we get
a transformed embedding for st which is passed to a linear
layer to predict the next action. The value network similarly
processes local occupancy maps with a CNN, as well as
additional state and goal information, which is then passed
through an MLP which outputs ˜Rt.
G. Hyperparameters and Training Details
We list hyperparameters for Control Transformer in Table
I. For sampling PRMs, we use a connect-distance of 10 and
200 sampled points for MuJoCo experiments, and a connect-
distance of 2 and 150 sampled points for Turtlebot3. For
low-level policies trained with SAC, we use networks with
2 hidden layers of size 256 for ant and point as well as 512
for humanoid. During training, we use a batch size of 256
and use a learning rate of 3 × 10−4. For rewards in offline
data, we use r(s|g) = −∥gt∥2, with added collision penalties
for Turtlebot3. For training low-level policies in MuJoCo,
we use distance-based rewards for ant, but find that velocity
rewards work better for point and humanoid. We also add
an alive bonus for humanoid, which helps prevent the robot
from falling over. We implement SAC with Stable Baselines3
[32], and CQL, a baseline, using https://github.com/
young-geng/cql.
IV. EXPERIMENTS
We evaluate our method on simulated MuJoCo robots
involving navigation in large mazes and Turtlebot3 naviga-
tion in cluttered environments, where we show transfer to
a physical robot. We also show that the transformer model
allows quick transfer learning to new dynamical systems.
TABLE I: Control Transformer hyperparameters used for
MuJoCo and Turtlebot experiments.
Parameter
MuJoCo
Turtle
Number of layers
4
4
Number of attention heads
4
4
Embedding dimension
512
128
Batch size
64
128
Nonlinearity function
ReLU
ReLU
Training Context length K
50
5
Evaluation Context length K
200
5
Dropout
0.1
0.1
Learning rate
4 × 10−4
10−4
Weight decay
10−3
10−4
Linear Learning rate warmup
104 updates
104 updates
Gradient updates
7.5 × 104
1.5 × 104
A. MuJoCo experiments
We first describe our MuJoCo experiments which are
based on the D4RL [9] maze environments, where we
modify the observation space to contain local occupancy
map observations. The original observation space solely
contains proprioceptive information, where methods must
memorize which space is obstacle-free, rather than learn to
avoid obstacles. This modification allows us to train and test
policies that generalize to unseen maze environments. For
our experiments, we test in two maze environments, which
include a single training maze (Fig. 3a), that has the structure
of “AntMaze Large” from D4RL, and an evaluation maze
(Fig. 3b), which is unseen during training, testing general-
ization. This is a difficult task, as we might expect a policy
to overfit the structure of the single training environment. In
these environments, we test 2 DOF point, 12 DOF ant, and
23 DOF humanoid robots.
(a) Training maze
(b) Evaluation maze
Fig. 3: Environments for Section IV-A. In (b), k × k local
map observation where k = 25 highlighted in red
For obstacle information, we use a 2 × 25 × 25 local
occupancy map as shown in Fig. 3, which has one channel
for obstacle information and a second channel that encodes
the location of the goal if it is within the local region. For
this experiment, we train a low-level controller to obtain
skills guided by planning as in Section III-B with Soft Actor-
Critic [12], and then collect trajectories as in Section III-C,
generating 1000 trajectories in the training environment to
serve as data for Control Transformer.
We report our method trained with several variations of
our framework trained on the same data. This includes just
using planning trajectories, Control Transformer (CT), and
training without rewards on planning trajectories, Behavior
Cloning Control Transformer (BC-CT). We also compare

Fig. 4: Control Transformer execution in unseen environ-
ments for point, and ant robots.
our approach utilizing return-conditioned sequence modeling
to Q-learning on our same collected data with Conservative
Q-Learning (CQL) [21], which is a strong, state-of-the-art
method for offline model-free RL using temporal difference
(TD) learning. We use CQL, as prior work [4] finds CQL
to be better than other offline TD methods on MuJoCo
tasks. We refer to this baseline of CQL trained on our
PRM-guided data as planning trajectory CQL (PT-CQL).
We also evaluated the state-of-the-art model-free RL method
SAC on our test scenarios. However, SAC fails to learn,
staying close to 0% success rate; therefore, we exclude it
in our comparative analysis. Model-free HRL methods are
also excluded as they struggle to solve long-horizon, multi-
goal locomotion tasks with collision-avoidance constraints
[30], validating the need for offline RL methods.
We evaluate each offline-RL method on 100 randomly
sampled start and goal pairs in both the training and eval-
uation environment, as shown in Fig. 3. We show our
results in Table II, reporting success and average return for
randomly sampled start and goal positions, and find that
our transformer-based methods, CT and BC-CT, perform the
best. We also show trajectories for ant and point in two
additional unseen maze environments in Fig. 4. A trajectory
from humanoid in an unseen hall environment Fig. 5 in which
TABLE II: Results of Control Transformer (CT), Behavioral
Cloning Control Transformer (BC-CT), planning guided fine-
tuning Control Transformer (F-CT), and Conservative Q-
Learning with planning trajectories obtained with our frame-
work (PT-CQL) on partially-observed maze environments.
We report average scaled return per episode followed by
average success rate (%). We average results over 3 trained
models for our transformer models.
Env
Robot
CT
BC-CT
PT-CQL
Seen
Point
-45.67(84.33)
-48.76(82.67)
-64.46(76)
Ant
-59.28(90)
-60.90(88)
-83.03(81)
Humanoid
-222.56(19)
-243.1(19.67)
-281.77(0.01)
Unseen
Point
-120.47(60.33)
-115.76(57.67)
-132.72(53)
Ant
-89.5(77.33)
-86.66(77)
-112.38(66)
Humanoid
-243.09(18)
-240.04(19.67)
-308(0.02)
our method demonstrates up to a 26% success rate. Note that,
in these environments, the success rates of our methods are
significantly higher than the baseline approach.
1) Transfer Learning across Different Dynamical Sys-
tems: A large amount of work has shown the successes of
transformers for transfer learning, where large pre-trained
transformers can be adapted for a specific task by fine-tuning
on a small amount of data. However, transfer learning has
been difficult in RL. With the adoption of transformers in RL,
it may be possible to make more progress. It has been found
that initializing Decision Transformers with weights from
a language model leads to significantly faster convergence
[33]. We hypothesize that Control Transformers can be fine-
tuned to quickly learn a policy for a different robot.
To implement this idea, we retain transformer and convo-
lution weights from a trained Control Transformer on a dif-
ferent robot and only re-initialize a few (thousand) weights,
particularly the linear embeddings in the input for states and
actions, as well as the final layer for action prediction. We
report our results in Fig. 6, where we show transfer from
ant-to-point, and point-to-ant. We find that transfer learning
with Control Transformer significantly speeds up training
new robots, which may be useful under limited data or
computational budgets.
B.
Real World Mobile Navigation
We also evaluate our approach on a Turtlebot3 differential-
drive robot. We perform training in randomized cluttered
environments simulated with PyBullet [5], and then transfer
Control Transformer to a physical robot, without additional
fine-tuning. We use goals gt = (∆x, ∆y), specifying the
translation from the robot’s current position to the goal posi-
tion. We use a proprioceptive state space consisting solely of
the difference in yaw between the robot’s current orientation
and the orientation in the direction of the goal, which is
represented by a unit vector sp = [cos(∆ψ), sin(∆ψ)].
Information about obstacles is provided by a Lidar sensor
with a 1-meter radius mounted on the Turtlebot. At each
timestep, we project raw Lidar data into a 2 × 25 × 25
occupancy map, allowing us to use the same architecture

Fig. 5: Humanoid trajectory in an unseen hall environment, showing our method’s generalization from maze environments.
Fig. 6: Control Transformer transfer results.
as in MuJoCo experiments. On the physical robot, we use
wheel encoders and IMU to track the robot position, which
we must use to calculate gt (and ∆ψ).
For this experiment, we demonstrate that our framework
can leverage known kinematics models to accelerate training
Control Transformer. Instead of learning a low-level con-
troller for data collection, we use a simple controller which
outputs linear velocity commands (V ) proportional to the
distance to the goal, and angular velocity (ω) commands
proportional to the angle between the robot orientation and
the goal. With differential drive robots, the angular velocity
for the right and left wheel is defined by ωR = V +ω(b/2)
r
,
ωL
=
V −ω(b/2)
r
, where b is the distance between the
two wheels and r is the wheel radius. We train Control
Transformer in the same manner as in Section IV-A, with
an action space of target linear and angular velocities. We
collect 1000 planning trajectories, and we reset the structure
of the environment every 25 episodes, randomly setting new
obstacle locations and widths, and sampling a new PRM.
In addition to CT, BC-CT as used in MuJoCo experiments,
we also perform further training on recovery and failure
trajectories with Algorithm 4 with T = 500 recoveries and
I = 1 iterations, which we refer to as planning-guided
fine-tuning Control Transformer (F-CT). We compare each
method, and report results averaged over 20 randomized
environments and 25 start-goal pairs per environment. We
find that BC-CT, CT, and F-CT attain a mean success rate
of 87.6±1.33%, 92.47±.94, and 95.87 ± .9% respectively.
We find benefits for both return conditioning (CT), as well as
additional fine-tuning (F-CT). In Fig. 7, we show generaliza-
tion with environment variation, increasing obstacle density,
compared to environments seen during training.
Next, we evaluate zero-shot sim2real transfer, evaluating
Control Transformer with planning-guided fine-tuning (F-
CT) in the real world. We fix a specific obstacle configu-
ration, as well as 7 start-goal positions. As discussed earlier,
Control Transformer is not provided with the structure of
the environment, and only uses local LiDAR data projected
onto a 2D occupancy map, without any other processing,
with a context length of k = 5 timesteps. We categorize
trajectories into three categories, consisting of successful
trajectories, where the goal is reached without any obstacle
collisions, partially successful trajectories, where the goal is
reached, but the robot grazes obstacles, which may occur
if the robot makes a turn, but the side of the robot hits
an obstacle. A trajectory is a failure if the robot directly
runs into the obstacle from the front of the robot, knocks
over an obstacle, or continues to push an obstacle as it
moves. We report a 5/7 ≈71.4% partial success rate, and
a 3/7 ≈42.9% full success rate, showing three trajectories
in Fig. 1. Green and red trajectories show two successes,
while blue is a partial success. While we find good results in
simulation, we believe some shortcomings in the real world
are due to inaccurate robot odometry, which is needed for
goal-conditioning throughout the duration of an episode, and
from noisy LiDAR data.
Fig. 7: Trajectories executed on unseen, cluttered environ-
ments, with Turtlebot3
V. CONCLUSION AND FUTURE WORK
In this work, we presented a framework that uses planning
with return-conditioned sequence modeling for learning poli-
cies capable of long-horizon control tasks such as navigation.
We demonstrated that our method learns to navigate on
difficult partially-observed mazes and cluttered navigation
environments. We also showed that our method generalizes
to new environment configurations, and is capable of zero-
shot sim2real transfer, without additional real-world data. We

also highlight that in one-shot sim2real transfer, the partial
failures are primarily due to the robot’s noisy odometry and
sensor data.
Our future work will introduce state estimation techniques
in real robot transfer to overcome noisy observations. In
addition, we also aim to map real-world environments using
NeRF [25], performing planning-guided training in envi-
ronments that more closely resemble reality, using visual
observations. Finally, we also plan to expand our work to
tasks such as autonomous driving, where it would be possible
to also collect human demonstrations, jointly training on a
fixed amount of human data, and planning-guided data.
REFERENCES
[1] Pierre-Luc Bacon, Jean Harb, and Doina Precup. The option-critic
architecture.
In Proceedings of the AAAI Conference on Artificial
Intelligence, volume 31, 2017.
[2] Akhil Bagaria, Jason K. Senthil, and George Dimitri Konidaris. Skill
discovery for exploration and planning using deep skill graphs. In
ICML, 2021.
[3] Mark Campbell, Magnus Egerstedt, Jonathan P How, and Richard M
Murray.
Autonomous driving in urban environments: approaches,
lessons and challenges.
Philosophical Transactions of the Royal
Society
A:
Mathematical,
Physical
and
Engineering
Sciences,
368(1928):4649–4672, 2010.
[4] Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover,
Misha Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch.
Decision transformer: Reinforcement learning via sequence modeling.
Advances in neural information processing systems, 34, 2021.
[5] Erwin Coumans and Yunfei Bai.
Pybullet, a python module for
physics simulation for games, robotics and machine learning. http:
//pybullet.org, 2016–2021.
[6] Scott Emmons, Benjamin Eysenbach, Ilya Kostrikov, and Sergey
Levine. Rvs: What is essential for offline rl via supervised learning?
In International Conference on Learning Representations, 2021.
[7] Benjamin Eysenbach, Abhishek Gupta, Julian Ibarz, and Sergey
Levine. Diversity is all you need: Learning skills without a reward
function. In International Conference on Learning Representations,
2018.
[8] Aleksandra Faust, Kenneth Oslund, Oscar Ramirez, Anthony Francis,
Lydia Tapia, Marek Fiser, and James Davidson. Prm-rl: Long-range
robotic navigation tasks by combining reinforcement learning and
sampling-based planning. In 2018 IEEE International Conference on
Robotics and Automation (ICRA), pages 5113–5120. IEEE, 2018.
[9] Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey
Levine. D4rl: Datasets for deep data-driven reinforcement learning,
2020.
[10] Dibya Ghosh, Abhishek Gupta, Ashwin Reddy, Justin Fu, Col-
ine Manon Devin, Benjamin Eysenbach, and Sergey Levine. Learning
to reach goals via iterated supervised learning.
In International
Conference on Learning Representations, 2020.
[11] Karol Gregor, Danilo Jimenez Rezende, and Daan Wierstra. Varia-
tional intrinsic control. ArXiv, abs/1611.07507, 2017.
[12] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine.
Soft actor-critic: Off-policy maximum entropy deep reinforcement
learning with a stochastic actor.
In International conference on
machine learning, pages 1861–1870. PMLR, 2018.
[13] Matthias Hutsebaut-Buysse, Kevin Mets, and Steven Latré. Hierar-
chical reinforcement learning: A survey and open research challenges.
Mach. Learn. Knowl. Extr., 4:172–221, 2022.
[14] Julian Ibarz, Jie Tan, Chelsea Finn, Mrinal Kalakrishnan, Peter Pastor,
and Sergey Levine. How to train your robot with deep reinforcement
learning: lessons we have learned.
The International Journal of
Robotics Research, 40(4-5):698–721, jan 2021.
[15] Brian Ichter, James Harrison, and Marco Pavone. Learning sampling
distributions for robot motion planning.
2018 IEEE International
Conference on Robotics and Automation (ICRA), pages 7087–7094,
2018.
[16] Michael Janner, Qiyang Li, and Sergey Levine. Offline reinforcement
learning as one big sequence modeling problem. In Advances in Neural
Information Processing Systems, 2021.
[17] Jacob J. Johnson, Linjun Li, Ahmed H. Qureshi, and Michael C. Yip.
Motion planning transformers: One model to plan them all. ArXiv,
abs/2106.02791, 2021.
[18] Leslie Pack Kaelbling. Learning to achieve goals. In IJCAI, 1993.
[19] L.E. Kavraki, P. Svestka, J.-C. Latombe, and M.H. Overmars. Proba-
bilistic roadmaps for path planning in high-dimensional configuration
spaces. IEEE Transactions on Robotics and Automation, 12(4):566–
580, 1996.
[20] Aviral Kumar, Xue Bin Peng, and Sergey Levine. Reward-conditioned
policies. ArXiv, abs/1912.13465, 2019.
[21] Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine.
Conservative q-learning for offline reinforcement learning. Advances
in Neural Information Processing Systems, 33:1179–1191, 2020.
[22] Steven M. LaValle. Rapidly-exploring random trees : a new tool for
path planning. The annual research report, 1998.
[23] Kuang-Huei Lee, Ofir Nachum, Sherry Yang, Lisa Lee, C. Daniel Free-
man, Sergio Guadarrama, Ian Fischer, Winnie Xu, Eric Jang, Henryk
Michalewski, and Igor Mordatch. Multi-game decision transformers.
In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun
Cho, editors, Advances in Neural Information Processing Systems,
2022.
[24] Yugang Liu and Goldie Nejat. Robotic urban search and rescue: A
survey from the control perspective. Journal of Intelligent & Robotic
Systems, 72:147–165, 2013.
[25] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T
Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes
as neural radiance fields for view synthesis. Communications of the
ACM, 65(1):99–106, 2021.
[26] Ofir Nachum, Shixiang Shane Gu, Honglak Lee, and Sergey Levine.
Data-efficient hierarchical reinforcement learning. In NeurIPS, 2018.
[27] Soroush Nasiriany, Vitchyr Pong, Steven Lin, and Sergey Levine. Plan-
ning with goal-conditioned policies. Advances in Neural Information
Processing Systems, 32, 2019.
[28] Emilio Parisotto, Francis Song, Jack Rae, Razvan Pascanu, Caglar
Gulcehre, Siddhant Jayakumar, Max Jaderberg, Raphael Lopez Kauf-
man, Aidan Clark, Seb Noury, et al.
Stabilizing transformers for
reinforcement learning.
In International conference on machine
learning, pages 7487–7498. PMLR, 2020.
[29] A. H. Qureshi, Mayur Joseph Bency, and Michael C. Yip. Motion
planning networks. 2019 International Conference on Robotics and
Automation (ICRA), pages 2118–2124, 2019.
[30] Ahmed H Qureshi, Jacob J Johnson, Yuzhe Qin, Taylor Henderson,
Byron Boots, and Michael C Yip. Composing task-agnostic policies
with deep reinforcement learning.
In International Conference on
Learning Representations.
[31] Alec Radford and Karthik Narasimhan. Improving language under-
standing by generative pre-training. 2018.
[32] Antonin Raffin, Ashley Hill, Adam Gleave, Anssi Kanervisto, Max-
imilian Ernestus, and Noah Dormann.
Stable-baselines3: Reliable
reinforcement learning implementations. Journal of Machine Learning
Research, 22(268):1–8, 2021.
[33] Machel Reid, Yutaro Yamada, and Shixiang Shane Gu. Can wikipedia
help offline reinforcement learning? ArXiv, abs/2201.12122, 2022.
[34] Stéphane Ross, Geoffrey J. Gordon, and J. Andrew Bagnell.
A
reduction of imitation learning and structured prediction to no-regret
online learning. In International Conference on Artificial Intelligence
and Statistics, 2010.
[35] Juergen Schmidhuber.
Reinforcement learning upside down: Don’t
predict rewards - just map them to actions. ArXiv, abs/1912.02875,
2019.
[36] Archit Sharma, Shixiang Gu, Sergey Levine, Vikash Kumar, and Karol
Hausman.
Dynamics-aware unsupervised discovery of skills.
In
International Conference on Learning Representations, 2019.
[37] Rupesh Kumar Srivastava, Pranav Shyam, Filipe Mutz, Wojciech
Jaskowski, and Jürgen Schmidhuber. Training agents using upside-
down reinforcement learning. CoRR, abs/1912.02877, 2019.
[38] Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit,
Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin.
Attention is all you need. In NIPS, 2017.
[39] Alexander Sasha Vezhnevets, Simon Osindero, Tom Schaul, Nicolas
Heess, Max Jaderberg, David Silver, and Koray Kavukcuoglu. Feudal
networks for hierarchical reinforcement learning.
In International
Conference on Machine Learning, pages 3540–3549. PMLR, 2017.

