The History of 
the GPU - Steps 
to Invention
Jon Peddie

The History of the GPU - Steps to Invention

Jon Peddie 
The History of the GPU -
Steps to Invention

Jon Peddie 
Jon Peddie Research 
Tiburon, CA, USA 
ISBN 978-3-031-10967-6
ISBN 978-3-031-10968-3 (eBook) 
https://doi.org/10.1007/978-3-031-10968-3 
© The Editor(s) (if applicable) and The Author(s), under exclusive license to Springer Nature 
Switzerland AG 2022 
This work is subject to copyright. All rights are solely and exclusively licensed by the Publisher, whether 
the whole or part of the material is concerned, speciﬁcally the rights of translation, reprinting, reuse 
of illustrations, recitation, broadcasting, reproduction on microﬁlms or in any other physical way, and 
transmission or information storage and retrieval, electronic adaptation, computer software, or by similar 
or dissimilar methodology now known or hereafter developed. 
The use of general descriptive names, registered names, trademarks, service marks, etc. in this publication 
does not imply, even in the absence of a speciﬁc statement, that such names are exempt from the relevant 
protective laws and regulations and therefore free for general use. 
The publisher, the authors, and the editors are safe to assume that the advice and information in this book 
are believed to be true and accurate at the date of publication. Neither the publisher nor the authors or 
the editors give a warranty, expressed or implied, with respect to the material contained herein or for any 
errors or omissions that may have been made. The publisher remains neutral with regard to jurisdictional 
claims in published maps and institutional afﬁliations. 
This Springer imprint is published by the registered company Springer Nature Switzerland AG 
The registered company address is: Gewerbestrasse 11, 6330 Cham, Switzerland

Foreword 
History often elicits strong responses whether it is studied in school, the subject of 
documentary ﬁlms and books, or passed orally from generation to generation. No 
matter the source, no history can cover every event for any one person. My own 
memory demonstrates that daily. 
I believe that history is an essential subject. Understanding what happened in 
the past gives insight into what worked and (perhaps more importantly) what didn’t 
work and why. In addition, history provides context for current events. We learn from 
history in important ways. 
Computing itself is a relatively new ﬁeld. Many science and engineering ﬁelds 
are signiﬁcantly older and their history has been documented extensively. There are 
substantive debates about what counts as the ﬁrst digital computer. Sufﬁce it to say 
that digital computers are not much more than 100 years old. 
Computer graphics is an even newer ﬁeld. It integrates disparate display technolo-
gies, digital and analog computers, and a human’s innate capability to see pictures 
on a ﬂat screen. Verne Hudson from Boeing-Wichita coined the term circa 1960. His 
collaborator, Bill Fetter, popularized it. 
Jon’s book complements a spate of recent publications devoted to the history 
of different aspects of computer graphics. Books by Peddie, Masson, and Carlson 
describe the ﬁeld in general. Smith traces the evolution of the pixel. Llach looks at 
graphics in building and architecture, Weisberg the history of CAD, and Gaboury 
the inﬂuence of the University of Utah. This is just a sampling. 
What I ﬁnd interesting about the authors is that many are intimately involved with 
the ﬁeld rather than historians per se. A number of them are pioneers or students of 
pioneers who have ﬁrst-hand knowledge of the history they are documenting. These 
authors write with both authority and immediacy. 
This book provides a broad view of the graphics processing unit. Jon has been 
involved with special purpose graphics processing technology since day one. He does 
an excellent job documenting processors dedicated to generating better images faster. 
Like any history, it’s not complete. The book does provide a coherent, well-organized 
view of the evolution of a valuable technology. Jon emphasizes how GPUs evolved 
from custom processors devoted to picture generation to general-purpose parallel
v

vi
Foreword
processors. It provides context that helps the reader better understand how GPUs ﬁt 
into the computer graphics world. 
I was totally unaware of Hudson and Fetter and the existence of computers 
and computer graphics until the late 1960s. I didn’t enter high school until 1962. 
My curriculum included Latin, Greek, and little science. Therefore, I could barely 
spell “computer.” Ironically, I retired from Boeing as a Senior Technical Fellow in 
visualization and interactive techniques after a 35-year career. 
The computer graphics bug bit me as a Johns Hopkins undergrad in 1969. Bill 
Huggins, who had spent his sabbatical learning computer animation with Bell Labs 
pioneers, recruited me to make computer-animated educational ﬁlms. The process 
was arduous. It involved punched cards, line printer keyframes, a microﬁlm recorder 
(located in Brooklyn NY), an assembly language animation “language,” and an IBM 
7094 mainframe. There were no interactive devices for animators/programmers, no 
color output, no shaded images, and no sound. Just white lines on a black background. 
And I loved it! 
My early career let me create more animated ﬁlms and learn about interactive 
graphics at Battelle-Columbus Labs. I became aware that a digital computer can 
display one frame at a time whether the frame is part of a projected ﬁlm or displayed 
on a graphics screen. The human visual system does the rest and gives a person the 
illusion of continuous motion as long as each image is shown quickly enough. 
For the ﬁlm, a projector shows frames fast enough (24–30 Hz) to make the motion 
seem continuous. Images on interactive device screens must be redrawn at the same 
rate or faster. Current interactive devices established a redraw rate at 60+ Hz. The 
requirement to draw new frames interactively ultimately led to the work with GPUs. A 
ﬁlm may take compute-centuries to produce enough frames for a full-length animated 
ﬁlm. Projectors are responsible for showing the frames fast enough. 
GPUs help reduce compute-centuries for a ﬁlm to something more reasonable 
by improving overall throughput. Interactivity pushes compute performance even 
harder. In today’s interactive graphics world, GPUs must compute a completely new 
frame fast enough to create the illusion of continuous motion. Put another way, the 
image generation compute task, the task GPUs perform, must determine the color 
of each pixel on each frame fast enough to convince the human visual system that 
image transformations (either 2D or 3D) are continuous. 
My work at Boeing emphasized acceptable interactive performance. I was able 
to work at a Boeing scale (interactively working with the complete digital design 
of a commercial airplane like a 787, ~2 billion polygons) on a GPU-equipped PC 
to make end-users think the task was easy. I often measure success by making the 
difﬁculty of complicated behind-the-scenes tasks seem simple when in actual use. 
I think Jon’s discussion about GPU evolution to become a generalized parallel 
processor adds real value. It conﬁrms my belief that the most successful and powerful 
technologies are those that can be generalized and applied to problems the original 
developers never foresaw. GPUs ﬁt that proﬁle.

Foreword
vii
Pay careful attention to the lessons learned from GPU evolution and generaliza-
tion. Those lessons can be applied to the reader’s own work. And understand how 
forthcoming generations of GPUs can be extended to provide even more value in the 
future. 
Sammamish, WA, USA 
June 2022 
D. J. Kasik

Preface 
This is the ﬁrst book in the three-book series on the History of the GPU. 
History books are challenging to write. Technical history books are incredibly 
challenging. Why? Because things don’t happen in an orderly sequence. Although 
one might think that event A leads to event B, often A leads to D, and B leads to C, 
but C leads to G. 
Because the integrated graphics processing unit (GPU) has been employed in so 
many systems (platforms) and evolved since 1996, how do you tell a 2D story in a 
linear presentation such as the book? 
One possibility is to list everything chronologically. Another approach is to list 
things by platform. And yet another choice is to list items by company, or by 
applications. 
I have chosen a combination of all three. 
This ﬁrst book in the series covers the developments that lead up to the integrated 
GPU, from the early 1960s to the late 1990s 
The book has two main sections, the PC platform and other platforms. Other 
platforms include workstations and game machines. 
Each chapter is designed to be read independently, hence there may be some 
redundancy. Hopefully, each one tells an interesting story. 
In general, a company is discussed and introduced in the year of its formation. 
However, a company may be discussed in multiple time periods in multiple chapters 
depending on how signiﬁcant their developments were and what impact they had on 
the industry.
ix

x
Preface
History of the GPU
 Steps to Invention 
Book 1 
Eras and Environment 
Book 2 
New Developments 
Book 3 
1. Preface
1. Preface
1. Preface 
2. History of 
the GPU 
2. Race to build 
the first GPU 
2. Second Era of 
GPUs (2001-2006) 
3. 1980-1990 
Graphics Controllers 
on Other Platforms 
3. GPU Functions
3. Third to Fifth Era 
of GPUs 
4. 1980-1989 
Graphics Controllers 
on PCs 
4. Major Era of GPUs
4. Mobile GPUs 
5. 1990-1995 
Graphics Controllers 
on PCs 
5. First Era of GPUs
5. Game Console GPUs 
6. 1990-1999 
Graphics Controllers 
on Other Platforms 
6. GPU 
Environment-Hardware
6. Compute GPUs 
7. 1996-1999 
Graphics Controller 
on PCs 
7. Application Program 
Interface (API)
7. Open GPUs 
8. What is a GPU 
8. GPU 
Environment-Software 
Extensions 
8. Sixth Era of GPUs 
The History of the GPU - Steps to Invention 
I mark the GPU’s introduction as the ﬁrst fully integrated single chip with hardware 
geometry processing capabilities—transform and lighting. Nvidia gets that honor on 
the PC by introducing their GeForce 256 based on the NV10 chip in October 1999. 
However, Silicon Graphics Inc. (SGI) introduced an integrated GPU in the Nintendo 
64 in 1996, and ArtX developed an integrated GPU for the PC a month after Nvidia. 
As you will learn, Nvidia did not introduce the concept of a GPU, nor did they

Preface
xi
develop the ﬁrst hardware implementation of transform and lighting. But Nvidia was 
the ﬁrst to bring all that together in a mass-produced single-chip device. 
The evolution of the GPU did not stop with the inclusion of the transformation and 
lighting (T&L) engine because the ﬁrst era of such GPUs had ﬁxed-function T&L 
processors—that was all they could do and when they were not doing that they sat 
idle using power. The GPU kept evolving and has gone through six eras of evolution 
ending up today as a universal computing machine capable of almost anything. 
However, to fully appreciate and hopefully understand what wonderful devel-
opment the GPU has been, it is necessary to know where, why, and how it was 
developed. To do that I start the story with the early computers from the late 1950s 
and 1960s. 
Now GPUs are ubiquitous. 
What Is In and Not In These Books 
As a public speaker and former engineer, you can tell from the above diagram; I like 
block diagrams. I have attempted to illustrate all the innovative GPUs and some of 
their predecessors with block diagrams. In some cases, I could not ﬁnd sufﬁcient 
data to construct a diagram; in some cases, the best I could do was a system-level 
diagram where the GPU is just a block. 
In these books, you won’t ﬁnd any formulas (no math), code examples, operating 
application examples, user interface illustrations, and hopefully no commercials or 
propaganda. 
Notable quotes and long quotations are presented indented to identify them as 
important and separate from the text. 
At the end is the glossary. Not every term used in the book is in the glossary as 
many of the explanations are in the body text. 
There is also a list of acronyms. The tech industry loves acronyms, and they can 
save time in communicating; they can also be very confusing. The acronym lists the 
acronym and a brief description. 
Signiﬁcant Things 
One of my goals for these books was to identify those developments that I (and 
hopefully others) thought were inﬂection points and disruptive results—things that 
moved the industry and or changed its direction. I marked those milestones in bold 
italics. 
The introduction of the GPU was just such a thing. It has profoundly and forever 
changed how computers work and are used. 
I hope you ﬁnd this and the following books interesting and informative. I 
have personally lived through almost all of it and have known most of the people

xii
Preface
mentioned. Many are acquaintances, and many are friends. Many of the people 
mentioned have generously contributed to this book with fact-checking, storytelling, 
and encouragement. However, it’s necessary to say that any mistakes or inaccuracies 
are all my own. 
The Author 
A Lifetime of Chasing Pixels 
I have been working in computer graphics since the early 1960s, ﬁrst as an engineer, 
then as an entrepreneur (I found four companies and ran three others), ending up 
in a failed attempt at retiring in 1982 as an industry consultant and advisor. Over 
the years, I watched, advised, counseled, and reported on developing companies 
and their technology. I saw the number of companies designing or building graphics 
controllers swell from a few to over forty-ﬁve. In addition, there have been over thirty 
companies designing or making graphics controllers for mobile devices. 
I’ve written and contributed to several other books on computer graphics (seven 
under my name and six co-authored). I’ve lectured at several universities around the 
world, written uncountable articles, and acquired a few patents, all with a single, 
passionate thread—computer graphics and the creation of beautiful pictures that tell 
a story. This book is liberally sprinkled with images—block diagrams of the chips, 
photos of the chips, the boards they were put on, and the systems they were put 
in—and pictures of some of the people who invented and created these marvelous 
devices that impact and enhance our daily lives—many of them I am proud to say 
are good friends of mine. 
I laid out the book in such a way (I hope) that you can open it up to any page and 
start to get the story. You can read it linearly; if you do, you’ll probably ﬁnd a new 
information and probably more than you ever wanted to know. My email address is 
in various parts of this book, and I try to answer every one, hopefully within 48 h. 
I’d love to hear comments, your stories, and your suggestions. 
The following is an alphabetical list of all the people (at least I hope it’s all of 
them) who helped me with this project. A couple of them have passed away, sorry to 
say. Hopefully, this book will help keep the memory of them and their contributions 
alive. 
Thanks for reading 
Jon Peddie—Chasing pixels, and ﬁnding gems

Preface
xiii
Acknowledgments and Contributors 
The following people helped me with editing, interviews, data, photos, and most of 
all encouragement. I literally and ﬁguratively could not have done this without them. 
Anand Patel—Arm 
Andrew Wolfe—S3 
Ashraf Eassa—Nvidia 
Atif Zafar—Pixilica 
Borger Ljosland—Falanx 
Brian Kelleher—DEC, and ﬁnally Nvidia 
Bryan Del Rizzo—3dfx & Nvidia 
Carrell Killebrew—TI/ATI/AMD 
Chris Malachowsky—Nvidia 
Curtis Priem—Nvidia 
Dado Banatao—S3 
Dan Vivoli—Nvidia 
Dan Wood—Matrox, Intel 
Daniel Taranovsky—ATI 
Dave Erskine—ATI & AMD 
Dave Kasik—Boeing 
Dave Orton—SGI, ArtX, ATI & AMD 
David Harold—Imagination Technologies 
Edvaed Sergard—Falanx 
Emily Drake—Siggraph 
Eric Demers—AMD/Qualcomm 
Frank Paniagua—Video Logic 
Gary Tarolli—3dfx 
George Sidiropoulos—Think Silicon 
Gerry Stanley—Real3D 
Henry C. Lin—Nvidia 
Henry Chow—Yamaha & Giga Pixel

xiv
Preface
Henry Fuchs—UNC 
Henry Quan—ATI 
Hossain Yassaie—Imagination Technologies 
Iakovos Istamoulis—Think Silicon 
Ian Hutchinson—Arm 
Jay Eisenlohr—Rendition 
Jay Torberg—Microsoft 
Jeff Bush—Nyuzi 
Jeff Fischer—Weitek & Nvidia 
Jem Davis—Arm 
Jensen Huang—Nvidia 
Jim Pappas—Intel 
Joe Curley—Tseng/Intel 
John Poulton—UNC & Nvidia 
Jonah Alben—Nvidia 
Karl Guttag—TI 
Karthikeyan (Karu) Sankaralingam—University of Wisconsin-Madison 
Kathleen Maher—JPA & JPR 
Ken Potashner—S3 & SonicBlue 
Kristen Ray—Arm 
Lee Hirsch—Nvidia 
Luke Kenneth Casson Leighton—Libre-GPU 
Mark Kilgard—Nvidia (Iris GL) 
Mary Whitton—Iknoas 
Megan Zea—PCI SIG 
Melissa Scuse—Arm 
Mike Diehl—HP 
Mike Mantor—AND 
Mikko Alho—Siru 
Mikko Nurmi—Bitboys

Preface
xv
Neal Leavitt—Editing 
Neil Trevett—3Dlabs & Khronos 
Nick England—Iknoas 
Pedro Duarte—Universities of Coimbra 
Peter McGuinness—SGS Thompson 
Peter L. Segal—AT & T 
Petri Norlund—Bitboys 
Phil Roges—ATI 
Richard Huddy—ATI 
Richard Selvaggi—Tseng Labs 
Rick Bergman—ATI/AMD 
Robert Dow—JPR 
Ross Smith—3dfx 
Ruchika Saini—editing 
Sasa Marinkovic—ATI & AMD 
Simon Fenny—Video Logic & Imagination Technologies 
Stefan Demetrescu—Stanford 
Stephen Morein—Stellar 
Steve Brightﬁeld—SiliconArts 
Steve Edelson—Edson Labs 
Tatsuo Yamamoto—Sega/DMP 
Tim Leland—Qualcomm 
Timothy Miller—Traversal Technology 
Tom Forsyth—3Dlabs 
Tony Tamasi—3dfx & Nvidia 
Trevor Wing—Video Logic

Contents 
1 
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1 
1.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1 
1.2
First Computer Graphics System (1949) . . . . . . . . . . . . . . . . . . . . . .
4 
1.3
The Graphics Processor Unit (1999) . . . . . . . . . . . . . . . . . . . . . . . . . .
12 
1.3.1
The Evolution of Graphics Controllers to GPUs . . . . . . . .
14 
1.4
Performance (2000–2026) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
16 
1.5
The GPU’s Changing Role . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
17 
1.6
The GPU’s Application . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
19 
1.6.1
AI and Machine Learning . . . . . . . . . . . . . . . . . . . . . . . . . . .
19 
1.6.2
Accelerated Computing and Supercomputers
. . . . . . . . . .
20 
1.6.3
Content Creation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
20 
1.6.4
Gaming . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
20 
1.6.5
Molecular Modeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
21 
1.6.6
Video and Photo Editing . . . . . . . . . . . . . . . . . . . . . . . . . . . .
21 
1.6.7
Vehicle Navigation and Robots . . . . . . . . . . . . . . . . . . . . . .
22 
1.6.8
Crypto Mining . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
22 
1.6.9
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
22 
1.7
The Many Roles of the GPU Require Additional Names . . . . . . . .
22 
1.8
Types of GPUs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
26 
1.9
Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
28 
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
28 
2 
1980–1989, Graphics Controllers on Other Platforms . . . . . . . . . . . . . .
31 
2.1
Ikonas Graphics Systems (1978–1982) . . . . . . . . . . . . . . . . . . . . . . .
34 
2.2
Pixel Planes—The Foundation of the GPU (1980–2000) . . . . . . . .
39 
2.2.1
HP Acquires Division (1996) . . . . . . . . . . . . . . . . . . . . . . . .
48 
2.3
Processor per Polygon—The Demetrescu Caltech 
Architecture (1980) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
60 
2.4
The Geometry Engine (1981) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
63 
2.5
Matrox SM640 with Geometry Engine (1987) . . . . . . . . . . . . . . . . .
67
xvii

xviii
Contents
2.6
SGI’s Personal Integrated Raster Imaging System (IRIS) 
Workstation (1988) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
67 
2.7
SGI’s IrisVision AIB (1988) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
69 
2.8
NEC’s µPD7220 Pioneering Graphics Display Controller 
(1982) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
69 
2.9
Hitachi ACRTC HD63484 (1984) . . . . . . . . . . . . . . . . . . . . . . . . . . . .
73 
2.10 
Truevision (1984–1987) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
77 
2.11 
Tl 34010 (1986) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
78 
2.11.1 
TI Epilogue . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
85 
2.12 
MAGIC—Multiple Application Graphics Integrated Circuit 
(1987) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
85 
2.13 
Raster Technologies Vertex Processor (1987) . . . . . . . . . . . . . . . . . .
87 
2.14 
Amiga (1988) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
88 
2.15 
Sun’s GX Graphics Accelerator Board (1989) . . . . . . . . . . . . . . . . .
91 
2.15.1 
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
94 
2.16 
Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
94 
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
95 
3 
1980–1989, Graphics Controllers on PCs . . . . . . . . . . . . . . . . . . . . . . . . .
99 
3.1
1980–1989, Graphics Controllers on the PC Platform . . . . . . . . . . .
99 
3.2
CRT Control (1975–1987) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
102 
3.2.1
The Video Output—LUT-DAC (~1981–1987) . . . . . . . . . .
102 
3.2.2
Brooktree (1983–1996) . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
103 
3.2.3
Edsun Labs (1989–1991)
. . . . . . . . . . . . . . . . . . . . . . . . . . .
104 
3.2.4
Summary of Video Output . . . . . . . . . . . . . . . . . . . . . . . . . .
108 
3.3
IBM Graphics History (1981–1990) . . . . . . . . . . . . . . . . . . . . . . . . . .
108 
3.3.1
IBM CGA (1981) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
109 
3.3.2
IBM EGA (1984) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
109 
3.3.3
EGA Begets VGA to XGA . . . . . . . . . . . . . . . . . . . . . . . . . .
111 
3.3.4
The IBM Professional Graphics Controller—PGC 
(1984) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
112 
3.3.5
The IBM 8514/A (1987) . . . . . . . . . . . . . . . . . . . . . . . . . . . .
114 
3.3.6
IBM VGA (1987–1991) . . . . . . . . . . . . . . . . . . . . . . . . . . . .
116 
3.3.7
Those Clones . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
120 
3.3.8
IBM Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
121 
3.4
The Market Expands (1986–1987) . . . . . . . . . . . . . . . . . . . . . . . . . . .
121 
3.5
Intel’s Pre-GPU History (1983–2003) . . . . . . . . . . . . . . . . . . . . . . . .
122 
3.5.1
82720 (1983) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
122 
3.5.2
82786 (1986) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
122 
3.5.3
i860 (1989) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
124 
3.5.4
i740 (1998) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
125 
3.5.5
i810 (1999) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
126 
3.5.6
Extreme Graphics (2001) . . . . . . . . . . . . . . . . . . . . . . . . . . .
128 
3.5.7
Intel Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
129 
3.6
Tseng Labs (1983–1997) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
129

Contents
xix
3.6.1
Winning Awards Was not Enough . . . . . . . . . . . . . . . . . . . .
135 
3.6.2
It Could Have Been the First GPU . . . . . . . . . . . . . . . . . . . .
136 
3.6.3
The End . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
137 
3.7
SGI’s IrisVision (1988–1994) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
138 
3.7.1
The Legacy of IrisVision—Pellucid, Media Vision, 
and 3dfx (1991–1994) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
141 
3.7.2
Media Vision (1990–1994) . . . . . . . . . . . . . . . . . . . . . . . . . .
142 
3.7.3
Benchmarking . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
143 
3.8
Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
144 
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
144 
4 
1980–1995 the Progenitors: Graphics Controller on PCs . . . . . . . . . . .
147 
4.1
1990–1995, Graphics Controllers on PCs . . . . . . . . . . . . . . . . . . . . .
147 
4.1.1
IBM XGA (1990) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
147 
4.1.2
Summary 1990 to 1995 . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
152 
4.2
The IGC to IGP (1991) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
152 
4.2.1
The First Workstation IGC . . . . . . . . . . . . . . . . . . . . . . . . . .
153 
4.2.2
The First PC IGC . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
153 
4.3
Bitboys (1991–1999) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
154 
4.3.1
Pyramid3D 25202 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
157 
4.3.2
Pyramid3D 25201 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
158 
4.3.3
The Eight P’s . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
160 
4.3.4
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
162 
4.4
Artist Graphics (1979–2098) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
162 
4.4.1
Artist Graphics Shows 3GA Graphics Accelerator . . . . . .
163 
4.4.2
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
165 
4.5
Number Nine Imagine 128 (1992–1999) . . . . . . . . . . . . . . . . . . . . . .
166 
4.5.1
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
169 
4.6
Rendition (1992–1998) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
169 
4.6.1
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
176 
4.7
Stellar—RSSI (1993–2000) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
177 
4.7.1
Reality Simulations Systems PixelSquirt . . . . . . . . . . . . . .
179 
4.7.2
Stellar is Born (1997) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
181 
4.7.3
VelaTX (1998) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
182 
4.7.4
Broadcom Acquires Stellar (2000)
. . . . . . . . . . . . . . . . . . .
183 
4.7.5
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
184 
4.8
Matrox Millennium (1994–2014) . . . . . . . . . . . . . . . . . . . . . . . . . . . .
184 
4.8.1
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
188 
4.9
VideoLogic/Imagination Technologies Tiling (1994–) . . . . . . . . . .
188 
4.9.1
NEC-Imagination Technologies PCX (1994–1999) . . . . .
193 
4.9.2
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
198 
4.10 
Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
200 
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
200

xx
Contents
5 
1990 to 1999 Graphics Controllers on Other Platform . . . . . . . . . . . . .
203 
5.1
Workstations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
203 
5.1.1
Workstation Graphics
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
204 
5.1.2
HP Artist (1993) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
205 
5.1.3
Silicon Reality (1994–1998) . . . . . . . . . . . . . . . . . . . . . . . . .
209 
5.1.4
The Saga of Evans & Sutherland’s Pre-GPU Effort 
(1995–2001) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
214 
5.1.5
3Dlabs Permedia (1997) . . . . . . . . . . . . . . . . . . . . . . . . . . . .
222 
5.1.6
Intergraph Wildcat (1998–2000) . . . . . . . . . . . . . . . . . . . . .
230 
5.2
Game Consoles . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
234 
5.2.1
Sega . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
235 
5.2.2
Sega Genesis (1988)
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
235 
5.2.3
Sony PlayStation (1994) . . . . . . . . . . . . . . . . . . . . . . . . . . . .
235 
5.2.4
Atari Jaguar (1993) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
240 
5.2.5
Nintendo 64 (1996)—The First T&L in a Console . . . . . .
243 
5.2.6
ArtX and the Nintendo GameCube (1998) . . . . . . . . . . . . .
250 
5.2.7
NEC Electronics’ PowerVR (1996) . . . . . . . . . . . . . . . . . . .
253 
5.3
Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
261 
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
262 
6 
1996–1999, Graphics Controllers on PCs . . . . . . . . . . . . . . . . . . . . . . . . .
265 
6.1
The ATI 3D Rage (1995) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
265 
6.1.1
Approaching the GPU . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
269 
6.1.2
The Saga of ATI (1985–2006) . . . . . . . . . . . . . . . . . . . . . . .
273 
6.2
Nvidia’s Quadratic Processor, the NV1 (1993–) . . . . . . . . . . . . . . . .
275 
6.2.1
Nvidia Epilogue . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
282 
6.3
3dfx Voodoo (1994–2000) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
283 
6.3.1
SLI Was Not a New Concept . . . . . . . . . . . . . . . . . . . . . . . .
290 
6.3.2
The SST-1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
290 
6.4
Yamaha YGV612 RPA (1995–1996) . . . . . . . . . . . . . . . . . . . . . . . . .
300 
6.5
Real3D (1995–1999) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
303 
6.5.1
A Stand-Alone Company . . . . . . . . . . . . . . . . . . . . . . . . . . .
306 
6.5.2
Real3D and Silicon Graphics Settle Out of Court . . . . . . .
307 
6.5.3
Intel Acquires Real3D (October 25, 1999) . . . . . . . . . . . . .
309 
6.5.4
3dfx and Intel Patent Cross-License Agreement . . . . . . . .
311 
6.6
Microsoft Talisman—The Chip That Never Was (1996) . . . . . . . . .
311 
6.7
Nvidia Riva 128 (1996) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
319 
6.8
S3 Virge 86C385 (1996) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
322 
6.8.1
S3 Epilogue . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
328 
6.9
Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
329 
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
330

Contents
xxi
7 
What is a GPU? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
333 
7.1
What is a GPU? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
333 
7.2
The GPU . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
335 
7.2.1
Vendors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
336 
7.2.2
Shaders, Processors, Units, and Cores . . . . . . . . . . . . . . . . .
337 
7.2.3
Getting to a Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
338 
7.3
The Six Eras of GPUs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
339 
7.3.1
Pre-GPU Graphics Controllers (1960–1998) . . . . . . . . . . .
340 
7.3.2
First-Era GPUs (1999–2000) Fixed Function . . . . . . . . . . .
341 
7.3.3
Second-Era GPUs (2000–2006) Programmable 
Shaders . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
341 
7.3.4
Third-Era GPUs (2006–2009) Uniﬁed Shaders . . . . . . . . .
342 
7.3.5
Fourth-Era GPUs (2009–2015) Compute Shaders . . . . . . .
342 
7.3.6
Fifth-Era GPUs (2015–2020) Ray Tracing and AI . . . . . .
342 
7.3.7
Sixth-Era GPUs (2020–) Mesh Shaders . . . . . . . . . . . . . . .
343 
7.3.8
The Range of the GPU and This Book . . . . . . . . . . . . . . . .
343 
7.4
Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
344 
7.5
Epilog . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
344 
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
345 
Appendix A: Acronyms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
347 
Appendix B: Deﬁnitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
353 
Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
393

List of Figures 
Fig. 1.1
A raster graphics display consists of quantized elements 
known as pixels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3 
Fig. 1.2
The small-scale experimental machine (SSEM), called 
baby, was built at the University of Manchester in June 1948 . . .
4 
Fig. 1.3
Baby’s dot-matrix display . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5 
Fig. 1.4
Whirlwind—the ﬁrst interactive digital computer. Stephen 
Dodd (sitting), Jay Forrester, Robert Everett, and Ramona 
Ferenz at the Whirlwind I, test control display in the Barta 
Building, 1950 (Courtesy of The MITRE Corporation) . . . . . . . .
6 
Fig. 1.5
Using a light gun on a SAGE air defense screen to pick 
a target aircraft (Courtesy of IBM)
. . . . . . . . . . . . . . . . . . . . . . . .
7 
Fig. 1.6.
3D perspective drawing created by William Fetter at Boing 
(Courtesy of McGraw-Hill) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7 
Fig. 1.7
Ivan Sutherland demonstrating Sketchpad (Courtesy 
of Wikipedia) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
8 
Fig. 1.8
The ﬁrst stand-alone workstation, IDI’s IDDIOM 
with Calligraphic screen and light-pen (Courtesy of IEEE) 
[16] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
9 
Fig. 1.9
An engineer using a light pen on a Control Data 274 
Digigraphics vector display terminal, circa 1965 (Courtesy 
of the Charles Babbage Institute Archives, University 
of Minnesota Libraries)
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
9 
Fig. 1.10
A Tektronix graphics terminal system board (Courtesy 
of Legalizeadulthood) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
11 
Fig. 1.11
Wolfenstein 3D was the ﬁrst PC-based 3D ﬁrst-person 
shooter (Courtesy of Software & Apogee Software 1992 id) . . . .
12 
Fig. 1.12
The evolutionary path of the GPU . . . . . . . . . . . . . . . . . . . . . . . . .
14 
Fig. 1.13
The entire image above was calculated; it is 
not a photograph or texture map (Courtesy of Epic Games, 
Nanite demo) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
15 
Fig. 1.14
Performance of popular platforms over time . . . . . . . . . . . . . . . . .
16
xxiii

xxiv
List of Figures
Fig. 1.15
Nvidia’s GeForce 256, the ﬁrst single-chip GPU (Courtesy 
of Konstantin Lanzet, Wikipedia)
. . . . . . . . . . . . . . . . . . . . . . . . .
17 
Fig. 1.16
GPUs have become ubiquitous and accelerated science, 
resulting in new products, enhanced vehicle safety, 
and many other applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
18 
Fig. 1.17
Taxonomy of names . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
25 
Fig. 1.18
GPUs are found in many types of systems and have 
different preﬁxes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
26 
Fig. 1.19
The problems with segmentations and names . . . . . . . . . . . . . . . .
27 
Fig. 2.1
Monochrome 2D line drawing done with a NEC 7220, 
on a PC ruining AutoCAD circa in the early 1980s 
(Historic American Buildings Survey: Library of Congress) . . . .
32 
Fig. 2.2
Imaginary worlds in color 3D games run on graphics chips 
circa 1990 (Courtesy of Wikipedia) . . . . . . . . . . . . . . . . . . . . . . . .
32 
Fig. 2.3
Historical view of the generic organization of 2D/3D raster 
systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
34 
Fig. 2.4
Ikonas graphics system block diagram . . . . . . . . . . . . . . . . . . . . .
36 
Fig. 2.5
Nick England and Mary Whitton recreating a 1980 Ikonas 
booth at Siggraph’s 25th anniversary in 1998 (Courtesy 
of England) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
37 
Fig. 2.6
The Ikonas system (Courtesy of England) . . . . . . . . . . . . . . . . . . .
37 
Fig. 2.7
Tim Van Hook’s ray tracing code rendering bi-cubic 
B-spline and polygonal surfaces (Courtesy of Nick 
England) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
38 
Fig. 2.8
Tim Van Hook, Fellow, ATI Technologies 2001 (Courtesy 
of Nick England) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
39 
Fig. 2.9
Fuchs’s background involved modeling chromosomes 
on graphics systems. He constructed 3D models from laser 
scans of people and objects before coming to UNC in 1978 
(Courtesy of UNC Endeavors) . . . . . . . . . . . . . . . . . . . . . . . . . . . .
40 
Fig. 2.10
Pixel planes overview block diagram—the template 
for the GPU . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
42 
Fig. 2.11
Pixel Planes’ functional organization, the ring was the key 
to communications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
44 
Fig. 2.12
Pixel planes memory chip block diagram . . . . . . . . . . . . . . . . . . .
45 
Fig. 2.13
Professor Henry Fuchs manipulates joysticks on the pixel 
planes system while an associate holds a memory board 
in front (Courtesy of Department of Computer Science, 
University of North Carolina)
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
46 
Fig. 2.14
PixelFlow prototype system block diagram
. . . . . . . . . . . . . . . . .
47 
Fig. 2.15
PixelFlow system organization . . . . . . . . . . . . . . . . . . . . . . . . . . . .
49 
Fig. 2.16
PixelFlow board
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
49 
Fig. 2.17
EMC memory chip in PixelFlow system was segmented . . . . . . .
51 
Fig. 2.18
The long 30-year trail (and tale) of pixel planes . . . . . . . . . . . . . .
59

List of Figures
xxv
Fig. 2.19
Dr. Professor Henry Fuchs, the father of the GPU 
(Courtesy of Department of Computer Science, University 
of North Carolina) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
59 
Fig. 2.20
Stefan Demetrescu (Courtesy of Lasergraphics) . . . . . . . . . . . . . .
60 
Fig. 2.21
Cohen and Demetrescu’s pipelined polygon architecture . . . . . . .
61 
Fig. 2.22
Competing surface processors acted much like a dataﬂow 
machine . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
62 
Fig. 2.23
James Clark (Courtesy of IEEE Computer)
. . . . . . . . . . . . . . . . .
64 
Fig. 2.24
Dot product . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
64 
Fig. 2.25
Three basic operations performed by a graphics system: 
transformation, clipping, and scaling . . . . . . . . . . . . . . . . . . . . . . .
65 
Fig. 2.26
A block diagram of the Geometry Engine corresponding 
to the photo in Fig. 2.27 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
66 
Fig. 2.27
Photograph of the Geometry Engine (Courtesy of ACM 
0-89791-076-1/82/007/0127) . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
66 
Fig. 2.28
Matrox SM 640 was the ﬁrst 3D PC AIB and used SGI’s 
Geometry Engine (Courtesy of Matrox) . . . . . . . . . . . . . . . . . . . .
67 
Fig. 2.29
SGI’s IRIS 2000 graphics workstation, circa 1985 
(Courtesy of wiki.preterhuman.net) . . . . . . . . . . . . . . . . . . . . . . . .
68 
Fig. 2.30
NEC’s µPD7220 was the ﬁrst integrated graphics 
controller chip . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
70 
Fig. 2.31
Layout of the µPD7220—notice the (dark) RAM area 
(Courtesy of Nikkei) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
71 
Fig. 2.32
Block diagram of NEC’s µPD7220 GDC . . . . . . . . . . . . . . . . . . .
72 
Fig. 2.33
Hitachi HD63484 ACRTC, more functionality and larger 
than the 7220 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
74 
Fig. 2.34
An ELSA workstation add-in board using the Hitachi 
HD63484, the top row of chips are memory (Courtesy 
of VGA Museum) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
74 
Fig. 2.35
A Force Computer VME SYS68k/AGC-1A add-in board 
based on the Hitachi HD63484 chip (Courtesy of Force 
Computers) [52] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
76 
Fig. 2.36
Block diagram of the Hitachi HD63484 graphics controller . . . .
77 
Fig. 2.37
Karl Guttag (Courtesy of Guttag)
. . . . . . . . . . . . . . . . . . . . . . . . .
79 
Fig. 2.38
The TMS34010’s system block diagram . . . . . . . . . . . . . . . . . . . .
80 
Fig. 2.39
The TMS34010’s internal architecture block diagram . . . . . . . . .
81 
Fig. 2.40
A Spea TI TMS34010-based AIB with a memory 
at the top, a VGA (clone) chip onboard and a TI LUT-DAC 
(Courtesy of Konstantin Lanzet Wikipedia) . . . . . . . . . . . . . . . . .
82 
Fig. 2.41
A photograph of the Texas Instruments’ TMS3020 
Graphics System Processor die (Courtesy of Pauli 
Rautakorpi, Wikipedia) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
83 
Fig. 2.42
The University of Sussex’s MAGIC used in a system . . . . . . . . .
86 
Fig. 2.43
Jay Torborg (Courtesy of Velotech) . . . . . . . . . . . . . . . . . . . . . . . .
87 
Fig. 2.44
A typical graphics command processing data ﬂow . . . . . . . . . . . .
88

xxvi
List of Figures
Fig. 2.45
The Commodore Amiga block diagram
. . . . . . . . . . . . . . . . . . . .
89 
Fig. 2.46
A picture in the HAM mode, showing all 4,096 colors 
at once on-screen. Such an image was displayed 
on an Amiga 1000 in 1985! (Courtesy of The Amiga 
Museum) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
90 
Fig. 2.47
Sun Microsystem’s GX graphics accelerator AIB functions . . . .
92 
Fig. 2.48
Sun Microsystem’s GX AIB (Courtesy of Curtis Priem) . . . . . . .
93 
Fig. 3.1
The IBM PC circa 1981 (Courtesy of Wikipedia, Rama & 
Musée Bolo) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
100 
Fig. 3.2
Brooktree LUT-DAC . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
103 
Fig. 3.3
Brooktree LUT-DAC chip (Courtesy of Thomas Schanz 
Wikipedia) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
104 
Fig. 3.4
Steve Edelson, Edsun Laboratories (Courtesy of Edelson) . . . . .
105 
Fig. 3.5
A breadboard with wire-wrap pins (Courtesy of Russ 
Shumaker) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
106 
Fig. 3.6
Edsun’s triangle demo: The bitmap was animated 
and rainbow-rotated around the edges (Courtesy of Steve 
Edelson) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
106 
Fig. 3.7
Analog devices/Edsun Labs CEG/DAC . . . . . . . . . . . . . . . . . . . . .
107 
Fig. 3.8
The CEG/DAC accepted processor and pixel data . . . . . . . . . . . .
107 
Fig. 3.9
IBM’s CGA add-in board (Courtesy of Wikipedia) . . . . . . . . . . .
109 
Fig. 3.10
IBM EGA add-in board. Notice the similarity to the CGA 
in form factor and layout (Courtesy of Vlask)
. . . . . . . . . . . . . . .
110 
Fig. 3.11
The integrated EGA controller reduced the size of the AIBs 
of the era (Courtesy of VGA Museum) . . . . . . . . . . . . . . . . . . . . .
110 
Fig. 3.12
The 4-bit RGBI palette added an intensity bit . . . . . . . . . . . . . . . .
111 
Fig. 3.13
Three-board-set of IBM’s Professional Graphics Controller 
(PGC) (Courtesy of John Elliot Vintage PCs) . . . . . . . . . . . . . . . .
113 
Fig. 3.14
Block diagram of IBM PGC with microprocessor 
and graphics emulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
113 
Fig. 3.15
IBM’s 8514/A AIB (lower) and memory board (above) 
formed a sandwich (Courtesy of os2museum.com) . . . . . . . . . . .
114 
Fig. 3.16
Block diagram of IBM’s 8514/A in system . . . . . . . . . . . . . . . . . .
115 
Fig. 3.17
The organization of a VGA/8514/A system . . . . . . . . . . . . . . . . .
116 
Fig. 3.18
IBM’s highly integrated motherboard-based VGA chip 
(Courtesy of Wikipedia) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
117 
Fig. 3.19
A VGA board with EISA tab (top) and ISA tab (bottom); 
note VGA connector on each end of the board (Courtesy 
of ELSA/Wikipedia) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
118 
Fig. 3.20
IBM VGA block diagram . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
119 
Fig. 3.21
The ubiquitous 15-pin VGA connector . . . . . . . . . . . . . . . . . . . . .
120 
Fig. 3.22
History of VLSI graphics chips . . . . . . . . . . . . . . . . . . . . . . . . . . .
121 
Fig. 3.23
The Intel SBX275 video graphics controller with 82,720 
chip (Courtesy of Multibus International) . . . . . . . . . . . . . . . . . . .
122

List of Figures
xxvii
Fig. 3.24
Intel 82,786 die shot (Courtesy of https://Commons.wik 
imedia.org) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
123 
Fig. 3.25
Intel i860 microprocessor (Courtesy of Wikipedia) . . . . . . . . . . .
124 
Fig. 3.26
Intel i740 prototype AIB with an AGP connector (Courtesy 
of www.SSSTjy.com) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
126 
Fig. 3.27
Intel i810 IGC . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
127 
Fig. 3.28
Intel 810 chipset (Courtesy of Wikipedia) . . . . . . . . . . . . . . . . . . .
128 
Fig. 3.29
Intel’s i845 Northbridge chipset (left) was surprisingly 
small (Courtesy of Wikipedia) . . . . . . . . . . . . . . . . . . . . . . . . . . . .
128 
Fig. 3.30
Tseng Labs’ ET4000AX. (Courtesy of Eep386: Wikipedia) . . . .
130 
Fig. 3.31
Tseng Labs’ ET4000 block diagram . . . . . . . . . . . . . . . . . . . . . . .
131 
Fig. 3.32
STB ET6000 AIB with 2 MB frame buffer and pad 
for additional MDRAM (Courtesy of Joe Curley) . . . . . . . . . . . .
132 
Fig. 3.33
STB ET6000 block diagram . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
135 
Fig. 3.34
SGI’s IrisVision block diagram . . . . . . . . . . . . . . . . . . . . . . . . . . .
138 
Fig. 3.35
SGI’s MCA-based IrisVision board-set interconnections 
(Courtesy of SGI) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
139 
Fig. 3.36
SGI’s IrisVision AIB circa 1999 (Courtesy of eBay) . . . . . . . . . .
140 
Fig. 3.37
Don Strimbu’s Nozzle was a 2D drawing benchmark used 
for many years (Courtesy of CAD Nauseam) . . . . . . . . . . . . . . . .
144 
Fig. 4.1
IBM XGA AIB (Courtesy of CC BY-SA 3.0, commons. 
Wikimedia) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
148 
Fig. 4.2
XGA block diagram, the coprocessor was the graphics 
engine . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
149 
Fig. 4.3
IBM XGA-2 AIB (Courtesy of OS2 Museum) . . . . . . . . . . . . . . .
151 
Fig. 4.4
Integrated graphics controller circa 1992 (IGC) . . . . . . . . . . . . . .
153 
Fig. 4.5
Bitboys’ Pyramid3D AIB (Courtesy of Petri Nordlund) . . . . . . .
155 
Fig. 4.6
Bitboys 3D Graphics pipeline processing hierarchy . . . . . . . . . . .
156 
Fig. 4.7
Pyramid3D system architecture . . . . . . . . . . . . . . . . . . . . . . . . . . .
156 
Fig. 4.8
Bitboy’s Pyramid3D architecture . . . . . . . . . . . . . . . . . . . . . . . . . .
159 
Fig. 4.9
Symmetric multiprocessing with multiple Pryamid3D chips . . . .
161 
Fig. 4.10
Artist Graphics’s 3GA controller . . . . . . . . . . . . . . . . . . . . . . . . . .
164 
Fig. 4.11
Number Nine’s Imagine 128 PCI AIB circa 1995 (Courtesy 
of Wikipedia) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
167 
Fig. 4.12
Rendition team pose in front of their ﬁrst ofﬁcial ofﬁce 
in Mountain View California, 1994 (Courtesy of Jay 
Eisenlohr)
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
170 
Fig. 4.13
Mike Boich (Courtesy of Mike Boice Wikipedia) . . . . . . . . . . . .
171 
Fig. 4.14
Verité 2200 block diagram (outlined items new integrated 
components)
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
174 
Fig. 4.15
Jay Eisenlohr (Courtesy of Engineering Oregon State) . . . . . . . .
177 
Fig. 4.16
Pixel Squirt 3D core . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
180 
Fig. 4.17
Matrox’s ﬁrst graphics AIB, the ALT-256, was designed 
in 1978 for early microcomputers (Courtesy of Matrox) . . . . . . .
185

xxviii
List of Figures
Fig. 4.18
Matrox Millennium ISA, circa 1997 (Courtesy of Gona.eu 
BY-SA 3.0 Wikipedia) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
186 
Fig. 4.19
Sir Hossein Yassaie (Courtesy of The Times) . . . . . . . . . . . . . . . .
189 
Fig. 4.20
Tile-based deferred rendering (TBDR) pipeline . . . . . . . . . . . . . .
190 
Fig. 4.21
Martin Ashton (Courtesy of Ashton) . . . . . . . . . . . . . . . . . . . . . . .
191 
Fig. 4.22
Simon Fenney (Courtesy of Fenney) . . . . . . . . . . . . . . . . . . . . . . .
192 
Fig. 4.23
Prototype TBDR FPGA AIB (Courtesy of Simon Fenney) . . . . .
193 
Fig. 4.24
VideoLogic’s Apocalypse 5D. (Courtesy of Fabian 
Günther-Borstel) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
194 
Fig. 4.25
Planet of death (Courtesy of Ubisoft) . . . . . . . . . . . . . . . . . . . . . .
196 
Fig. 4.26
The Series2 PMX1, a prototype of what later became 
the Neon 250 (Courtesy of Imagination Technologies) . . . . . . . .
199 
Fig. 5.1
HP’s balanced compression/decompression with CPU 
and HP Artist chip . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
206 
Fig. 5.2
HP’s Artist chip block diagram . . . . . . . . . . . . . . . . . . . . . . . . . . .
207 
Fig. 5.3
Signature in an integrated circuit chip (Courtesy of Florida 
State University’s Silicon Zoo project) . . . . . . . . . . . . . . . . . . . . .
209 
Fig. 5.4
Silicon Reality’s TAZ Core function block diagram . . . . . . . . . . .
211 
Fig. 5.5
Silicon Reality’s Tantrum block diagram
. . . . . . . . . . . . . . . . . . .
212 
Fig. 5.6
Silicon Reality’s Tantrum product functional block diagram . . . .
213 
Fig. 5.7
E&S Realimage DirectBurst graphics block diagram . . . . . . . . . .
216 
Fig. 5.8
Evans & Sutherland’s RealImage 2000 block diagram
. . . . . . . .
218 
Fig. 5.9
Interleaved relationships in the late 1990s workstation 
market . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
219 
Fig. 5.10
3Dlabs’ Glint block diagram . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
223 
Fig. 5.11
3Dlabs’ Glint 300SX core architecture had many stages . . . . . . .
224 
Fig. 5.12
The 3Dlabs’ Glint MX processor relied on an external 
geometry processor . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
224 
Fig. 5.13
3Dlabs’ road map in the mid-90s (Courtesy of 3Dlabs) . . . . . . . .
226 
Fig. 5.14
3Dlabs’ Permedia consumer-level 3D controller . . . . . . . . . . . . .
228 
Fig. 5.15
The history of 3Dlabs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
230 
Fig. 5.16
Elsa’s GLoria-XL 40-MB 3Dlabs-based workstation AIB 
(Courtesy of VGA Museum) . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
231 
Fig. 5.17
Intergraph’s Intense Wildcat (single pipeline) block 
diagram . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
231 
Fig. 5.18
The graphics from the Sega Genesis were not impressive, 
but in 1988 and for a standard deﬁnition TV, they rivaled 
arcade game machines (Courtesy of Wikipedia) . . . . . . . . . . . . . .
236 
Fig. 5.19
Ken Kutaragi (Courtesy of Wikipedia) . . . . . . . . . . . . . . . . . . . . .
236 
Fig. 5.20
Sony’s 1991 PlayStation gamer developer demo 
with quasi-3D textures (Courtesy of https://www.youtube. 
com/watch?v=rwNt_9GvpFI)
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
238 
Fig. 5.21
Comparison of afﬁne and correct perspective texture 
mapping (Courtesy of Darkness3560 for Wikipedia) . . . . . . . . . .
238 
Fig. 5.22
PlayStation block diagram . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
239

List of Figures
xxix
Fig. 5.23
Atari Jaguar block diagram
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
241 
Fig. 5.24
Nintendo 64, the ﬁrst console with a 3D accelerator 
(Courtesy of Wikipedia) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
244 
Fig. 5.25
Nintendo 64 motherboard, CPU, and reality coprocessor, 
with RDRAM below the processors (Courtesy of Nintendo) . . . .
244 
Fig. 5.26
Nintendo 64 block diagram . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
245 
Fig. 5.27
Nintendo 64 reality block diagram . . . . . . . . . . . . . . . . . . . . . . . . .
246 
Fig. 5.28
Nintendo 64 RCP block diagram . . . . . . . . . . . . . . . . . . . . . . . . . .
247 
Fig. 5.29
Nintendo GameCube system board (Courtesy of Nintendo) . . . .
252 
Fig. 5.30
Arcade implementation of NEC’s VideoLogic ISP and TSP 
chips . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
254 
Fig. 5.31
NEC/VL ISP block diagram . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
254 
Fig. 5.32
NEC/VL TSP block diagram . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
255 
Fig. 5.33
Trevor Wing (Courtesy of Register of Chinese Herbal 
Medicine) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
256 
Fig. 5.34
NEC/VL PCX1 block diagram . . . . . . . . . . . . . . . . . . . . . . . . . . . .
257 
Fig. 5.35
Tatsuo Yamamoto (Courtesy of Yamamoto) . . . . . . . . . . . . . . . . .
258 
Fig. 5.36
Hideki Sato (Courtesy of Sega Retro) . . . . . . . . . . . . . . . . . . . . . .
259 
Fig. 6.1
Kwok Yeun Ho, ATI founder and CEO (Courtesy of ATI)
. . . . .
266 
Fig. 6.2
ATI’s ﬁrst graphics chip and board, 
the CW16800-A (Courtesy of TechPowerUp)
. . . . . . . . . . . . . . .
267 
Fig. 6.3
ATI Vantage (left) and Ultra—notice the difference 
in the memory to the left of the graphics controller chips 
(Courtesy of VGA Museum) . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
269 
Fig. 6.4
ATI’s 3D Rage AIB (Courtesy of TechPowerUp) . . . . . . . . . . . . .
270 
Fig. 6.5
ATI 3D Rage II internal block diagram . . . . . . . . . . . . . . . . . . . . .
272 
Fig. 6.6
ATI’s ImpacTV chip function block diagram . . . . . . . . . . . . . . . .
272 
Fig. 6.7
History of ATI’s acquisitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
275 
Fig. 6.8
ATI’s David Orton and AMD’s Hector Ruiz ofﬁcially 
announce the historic merger (Courtesy of AMD) . . . . . . . . . . . .
276 
Fig. 6.9
Chris Malachowsky (Courtesy of Nvidia) . . . . . . . . . . . . . . . . . . .
277 
Fig. 6.10
Curtis Priem (Courtesy of Rensselaer)
. . . . . . . . . . . . . . . . . . . . .
277 
Fig. 6.11
Diamond Multimedia’s Edge 3D with SGS (Nvidia) chip 
(Courtesy of Wikipedia) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
279 
Fig. 6.12
Nvidia’s NV1 block diagram . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
279 
Fig. 6.13
Jensen Huang right after Sega delivered three arcade 
machines to Nvidia for testing and integration in 1995 . . . . . . . .
281 
Fig. 6.14
The founders of 3dfx: Scott Sellers, Gary Tarolli, and Ross 
Smith (Courtesy of Smith) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
284 
Fig. 6.15
The basic 3dfx graphics engine . . . . . . . . . . . . . . . . . . . . . . . . . . .
288 
Fig. 6.16
3dfx developed scan-line interleaving in 1995 (Courtesy 
of Martín Gamero Prieto) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
289 
Fig. 6.17
Block diagram of 3dfx’s frame buffer interface chip . . . . . . . . . .
291 
Fig. 6.18
The Voodoo1 from 3dfx, released in 1996 (Courtesy 
of Wikipedia) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
292

xxx
List of Figures
Fig. 6.19
Interstate ’76, released in 1997 by Activision, running 
on a Voodoo 1 (Courtesy of Wikipedia)
. . . . . . . . . . . . . . . . . . . .
293 
Fig. 6.20
Comparison of 3dfx performance to VGA (Courtesy 
of 3dfx) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
296 
Fig. 6.21
The last AIB from 3dfx, the Voodoo5 5500 (Courtesy 
of Wikipedia, Konstantin Lanzet)
. . . . . . . . . . . . . . . . . . . . . . . . .
298 
Fig. 6.22
The history of 3dfx . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
298 
Fig. 6.23
Yamaha YGY611 block diagram . . . . . . . . . . . . . . . . . . . . . . . . . .
301 
Fig. 6.24
Paradise’s YGY612-based Tasmania AIB (Courtesy 
of Vogonwiki) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
303 
Fig. 6.25
Gerald Stanley, Real3D’s founder, and CEO (Courtesy 
of Stanley) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
304 
Fig. 6.26
LMC’s Real3D multichip Starﬁghter AIB . . . . . . . . . . . . . . . . . . .
306 
Fig. 6.27
Memory bandwidth requirement (Mbytes/second) . . . . . . . . . . . .
313 
Fig. 6.28
Talisman system hardware partitioning . . . . . . . . . . . . . . . . . . . . .
313 
Fig. 6.29
Talisman polygon object processor . . . . . . . . . . . . . . . . . . . . . . . .
315 
Fig. 6.30
Talisman image layer compositor
. . . . . . . . . . . . . . . . . . . . . . . . .
316 
Fig. 6.31
Nvidia’s NV3 RIVA 128 media accelerator
. . . . . . . . . . . . . . . . .
320 
Fig. 6.32
Diamond Multimedia’s RIVA 128, NV3-based Viper V330 
4 MB gaming AIB circa 1995 (Courtesy of Mathías Tabó, 
Wikipedia) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
321 
Fig. 6.33
Nvidia’s RIVA 128 3D graphics engine . . . . . . . . . . . . . . . . . . . . .
321 
Fig. 6.34
Dado Banatao, founder of S3 (Courtesy of Positively 
Filipino) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
323 
Fig. 6.35
Terry Holdt would run S3 for ten years (Courtesy of Team 
6502) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
324 
Fig. 6.36
S3’s ViRGE 86C385 3D graphics board (Courtesy of VGA 
Legacy)
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
324 
Fig. 6.37
The S3 ViRGE 86C385 block diagram . . . . . . . . . . . . . . . . . . . . .
325 
Fig. 6.38
S3 Savage3D AIB (Courtesy of VGA Museum)
. . . . . . . . . . . . .
327 
Fig. 6.39
Ken Potashner, S3’s ﬁnal CEO (Courtesy of SONICblue) . . . . . .
327 
Fig. 6.40
S3’s timeline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
329 
Fig. 7.1
The basic GPU pipeline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
334 
Fig. 7.2
The many elements of a modern GPU . . . . . . . . . . . . . . . . . . . . . .
336 
Fig. 7.3
A shaded triangle
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
337 
Fig. 7.4
Shaders, processors, clusters, and cores make up a GPU . . . . . . .
338 
Fig. 7.5
The various SIMD clusters of the GPU suppliers . . . . . . . . . . . . .
338 
Fig. 7.6
Raster or direct rendering uses CG tricks to approach 
realism and is not physically correct. . . . . . . . . . . . . . . . . . . . . . . .
339 
Fig. 7.7
The eras of GPU development . . . . . . . . . . . . . . . . . . . . . . . . . . . .
341

List of Tables 
Table 1.1
Nvidia architectural names . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
23 
Table 2.1
PixelFlow’s custom chips characteristics . . . . . . . . . . . . . . . . . . .
51 
Table 3.1
Some of the VGA clone suppliers . . . . . . . . . . . . . . . . . . . . . . . . .
120 
Table 4.1
Speciﬁcations of the 3GA controller . . . . . . . . . . . . . . . . . . . . . . .
165 
Table 4.2
Matrox Mystique resolutions and refresh rates . . . . . . . . . . . . . . .
187 
Table 4.3
Imagination technologies PowerVR features . . . . . . . . . . . . . . . .
198 
Table 5.1
Silicon Reality’s TAZ core speciﬁcations . . . . . . . . . . . . . . . . . . .
210 
Table 5.2
Silicon Reality’s TAZ 3D mode speciﬁcations . . . . . . . . . . . . . . .
211 
Table 5.3
Silicon Reality’s Tantrum features . . . . . . . . . . . . . . . . . . . . . . . . .
213 
Table 5.4
Silicon Reality’s 2D features . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
213 
Table 5.5
Silicon Reality’s Tantrum, range of display formats . . . . . . . . . .
214 
Table 5.6
Game consoles before GPUs . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
235 
Table 6.1
The family of versions of the ATI 3D Rage graphics 
controller . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
271 
Table 6.2
Talisman characteristics and features . . . . . . . . . . . . . . . . . . . . . .
312
xxxi

Chapter 1 
Introduction 
1.1 
Introduction 
Over the years, the computer’s central processing unit (CPU) eventually incorporated 
every coprocessor developed to augment and add to its function except for one major 
processor, the GPU. A GPU is a specialized processor developed initially to accelerate 
graphics rendering and geometry transformations. 
The CPU has even incorporated graphics processing. However, the CPU has not 
terminated the GPU’s stand-alone value as it did with ﬂoating-point processors, 
digital signal processors (DSPs), video Codecs, and other accelerators. The GPU 
survives as a stand-alone coprocessor because the GPU scales almost inﬁnitely— 
adding transistors to create thousands of processor cores. The only asymptote a 
GPU might face is inter-processor communications. Clustering groups of processors 
(shaders) overcomes that barrier. Coherent caches have also scaled well and address 
the GPU’s inter-processor communications bottleneck. GPUs have been a signiﬁcant 
beneﬁciary of Moore’s law [3] (which postulates that the number of transistors on a 
chip doubles and cuts the price in half approximately every 1.5 to 2 years). 
The GPU is a wonderful device and has made tremendous contributions to the 
computer’s capabilities. 
GPUs can process data simultaneously, which is known as parallel processing. 
As a result, GPUs are used in applications beyond gaming and simulation. Applica-
tions as far-ranging as artiﬁcial intelligence (AI), machine learning (ML), CAD, and 
compute-intensive tasks use GPUs. GPUs have been used as accelerators for photo 
and video editing and high-performance computers (HPC) and supercomputers. 
GPUs were initially stand-alone discrete hardware units (dGPU). Later, in 2010, 
they were added to the CPU (iGPU) but still held their place as a stand-alone device. 
GPUs can contain specialized AI elements and multimedia accelerators for video and 
audio, and ray tracing accelerators. As the GPU advanced beyond its original role as 
a graphics processor and found use in pure computing applications not requiring a 
display, its additional capabilities have been called General-Purpose GPU (GPGPU), 
or GPU compute (cGPU). It is a false term because there is nothing general-purpose
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2022 
J. Peddie, The History of the GPU - Steps to Invention, 
https://doi.org/10.1007/978-3-031-10968-3_1 
1

2
1
Introduction
about a GPU. It cannot run an operating system, manage disk drives and peripherals, 
or boot up a system, nor is there anything general-purpose about a parallel processor. 
Those are the jobs of the CPU. GPUs are specialized devices with speciﬁc and 
specialized parallel processing capabilities. 
The terms semiconductor, integrated circuit (IC), and chip will be used inter-
changeably in this book and should be considered synonyms. 
What does a GPU do? It is almost everything needed to calculate in parallel from 
geometry processing to image processing to AI training and accelerated computing. 
Computer graphics is about geometry; as Pixar cofounder Alvy Ray Smith says 
in his book, A Biography of the Pixel, “Computer graphics is geometry in, pixels out 
[4].” There’s much more to the GPU, however. Image processing is pixels in, pixels 
out, whereas AI training and compute-acceleration is data in, data out—and a GPU 
does all of that and more. 
And David Kasik of Boeing says: 
I consider computer graphics to be about creating, displaying, and modifying visual content to 
communicate to others. The sources are much broader than geometry: cameras, simulations, 
brain waves, sounds, etc. All forms of data can have a visual manifestation. 
The integration of the rendering engine and the transformation and lighting (T&L) 
engine into a graphics controller converting it into a GPU was done to solve a geom-
etry problem. Therefore, you will ﬁnd a lot of discussion about geometry but no math 
in this book. 
Transformation and lighting are critical components of computer graphics and the 
GPU. Transform means to convert the coordinates of the 3D model to the coordinates 
of the viewing or display device. The coordinates are described by the vertices of 
objects in a scene. Lighting refers to the simulation of light in a scene—on the objects 
in a scene—and the effect of light from one object to another and the scene. It is a 
complicated process. 
As will be explained in later chapters, the GPU will ﬁnd its way into general 
compute applications as a parallel processor, totally devoid of any graphics functions. 
Graphics processing units were developed for three-dimensional (3D) computer 
graphics applications, ﬁrst for CAD and then for games. The historical develop-
ment of computer graphics gives an overview of the inﬂuences that heralded the 
development and invention of the GPU. The following is an overview of those devel-
opments, which will provide a foundation for appreciating the GPU’s development 
and evolution. 
Displays and Pixels 
In computer graphics and digital imaging, a pixel is the smallest addressable element 
in a raster image, or the smallest addressable element in a digital display such as 
an LCD. A pixel is not addressable in geometry. Geometry is at the front end of a 
processing pipeline and the output is a raster scan, sized to a speciﬁc physical display, 
and measured in pixels. 
A GPU’s primary function is to drive a display (although GPUs used for compute 
acceleration do not drive a display). GPUs and graphics controllers before them

1.1 Introduction
3
deliver pixel information to a display in a serial manner, a few bits at a time. Displays 
used for graphics have evolved from stoke or vector writers, also known as calli-
graphic displays to TV-like raster-scan displays, to digital displays in ﬂat panels. The 
vector displays drew straight lines from one point to another on a screen. Figure 1.9 
best illustrates a vector display. With the introduction of low-cost computers, scan-
line cathode ray tube (CRT) TV displays were adopted. They sweep a beam across 
the screen (front) of the display and turn on or off the bean to make the screen light 
up. When the beam got from the left side to the right, it would snap back to the left, 
and slightly lower and then start the next scan line. Thus, a CRT’s resolution was 
measured by how many scan lines it could display. When digital computers began 
using CRTs, the beam was turned on or off at speciﬁc intervals across the scan line, 
corresponding to data in a frame buffer in the computer. That became known as raster 
graphics, and the operation known as a raster scan. 
CRTs were replaced by liquid crystal displays (LCDs). LCDs were intrinsically 
digital with each pixel on the screen consisting of a discrete crystal element. LCDs 
were also ﬂat and manufactured like a semiconductor. The data delivered to the LCD 
came from the computer’s frame buffer and was delivered serially as well. The LCD 
didn’t use a ﬂy-back or snap-back process but used line and row drivers to move the 
data from the frame buffer to the appropriate pixel location. However, the process 
was still known as a raster scan. 
The term raster is used to describe the display process, and it is also used to 
describe the construction of an image and is referred to as raster graphics. It is also 
called bitmapped graphics. A bitmapped image is a digital image, and its pixels 
(usually square) form a quantized display as illustrated in Fig. 1.1. 
A raster display resolution is measured by the number of pixels in row times the 
number of rows (lines). So, a high-deﬁnition display has dimensions of 1920 pixels 
per row times 1080 rows, or 2.07 million pixels (commonly called two megapixels, 
written as 2 mpix). 
The output stage of a GPU is referred to as a rasterizer.
Fig. 1.1 A raster graphics  
display consists of quantized 
elements known as pixels 

4
1
Introduction
1.2 
First Computer Graphics System (1949) 
Most people attribute the start of interactive 3D graphics in computers to the 
Sketchpad project at MIT in 1963 by Ivan Sutherland. However, General Motors 
started DAC-1 in 1959–1960 as an interactive surface design system. Sketchpad itself 
was a 2D system that introduced concepts still in use today: e.g., constraint-based 
sketching and masters/instances. Tim Johnson developed a 1964 version (Sketchpad 
III) that was 3D [5]. 
The ﬁrst electronic stored-program computer built was the Small-Scale Experi-
mental Machine (SSEM), called Baby. Frederic C. Williams, Tom Kilburn, and Geoff 
Tootill constructed it at the University of Manchester. The ﬁrst program ran on it on 
June 21, 1948 [6] (Fig. 1.2). 
Baby was designed to use and demonstrate data stored on a cathode ray tube 
(CRT), which became known as a Williams tube. The tube could remember 2048 
bits. The system proved it was practical to read and write data reliably at a speed 
suitable for use in a computer. 
The First Digital Display was in 1948 
Baby had a small oscilloscope-like display tube that could display alpha-numeric 
data. It didn’t do lines at any angle, but clever graphics images were created with the 
dot-matrix screen (Fig. 1.3).
In the U.S., the Ofﬁce of Naval Research and the U.S. Air Force funded the 
Whirlwind project to develop a large-scale digital machine to take on the arduous 
task of calculation for real-time problems like aircraft simulation and air trafﬁc 
control.
Fig. 1.2 The small-scale experimental machine (SSEM), called baby, was built at the University 
of Manchester in June 1948 

1.2 First Computer Graphics System (1949)
5
Fig. 1.3 Baby’s dot-matrix 
display
The ﬁrst animated computer graphic 1949 
Begun at the Servomechanisms Lab at MIT in 1944 under the direction of Jay 
Forrester, the Whirlwind and Forrester were moved to the Digital Computer Lab and 
started focusing on using the computer for graphics displays of air trafﬁc control and 
gunﬁre control. Forrester’s project became part of the government’s Semi-Automatic 
Ground Environment (SAGE) program. SAGE, in turn, was part of the Navy’s 
Airplane Stability and Control Analyzer (ASCA) project. The system was planned 
for a programmable ﬂight simulation environment and demonstrated in 1951 [7]. 
Programmable ﬂight simulation 1951 
Whirlwind computer construction began in 1948 and employed 175 people, including 
seventy engineers and technicians. By the third quarter of 1949, the computer was 
advanced enough to solve an equation and display its solution on an oscilloscope; it 
even got used for the ﬁrst animated and interactive computer graphics game called 
Blackjack [8]. 
The Whirlwind was the First Digital Computer Used for Computer Graphics 
Whirlwind was the ﬁrst to use video displays for output and operate in real time. It 
was not just an electronic replacement of older mechanical systems [9] (Fig. 1.4).
The Whirlwind used a new core memory for Random Access Memory (RAM), 
the ﬁrst computer to do so. The SAGE air defense system became operational in 
1958 with more advanced display capabilities.

6
1
Introduction
Fig. 1.4 Whirlwind—the ﬁrst interactive digital computer. Stephen Dodd (sitting), Jay Forrester, 
Robert Everett, and Ramona Ferenz at the Whirlwind I, test control display in the Barta Building, 
1950 (Courtesy of The MITRE Corporation)
The SAGE air defense system, which drew heavily on experience with Whirlwind, 
also incorporated interactive graphics consoles and light “guns” in the 1950s to 
simulate and track the position of enemy bombers [10] (Fig. 1.5).
Robert Everett designed an input device, which he called a light gun or light pen. 
It gave the operators a means of getting identiﬁcation information about an aircraft. 
When the light gun was pointed at a screen icon representing a plane, the pen saw the 
light (from the screen) and sent an interrupt to Whirlwind. Whirlwind then displayed 
text about the plane’s identiﬁcation, speed, and direction [11]. 
First 3D Perspective Computer Generated Images in 1960 
William Fetter at Boeing in 1960 produced 3D perspective drawing and made an 
animation of an airplane’s cockpit—he also coined the term Computer Graphics 
along with Verne Hudson[12] (Fig. 1.6).
In Fetter’s book, Computer Graphics in Communication (page 13), he describes 
how one draws a series of x, y positions and the computer then connects as straight-
line segments [13]. 
2D line drawings were made on large 30-inch screens that were derivatives of 
oscilloscopes and radar screens.

1.2 First Computer Graphics System (1949)
7
Fig. 1.5 Using a light gun on a SAGE air defense screen to pick a target aircraft (Courtesy of IBM)
Fig. 1.6. 3D perspective drawing created by William Fetter at Boing (Courtesy of McGraw-Hill)
Straight Lines 1962 
In early 1962, a senior technical staff member at IBM, Jack Elton Bresenham (PhD., 
Stanford University, 1964) developed the line drawing algorithm named after him 
[14]. The original application for the algorithm was with x, y pen plotters (e.g., 
Calcomp). Unlike interactive calligraphic displays, pen plotters could move or draw 
to one of eight positions using stepper motors that moved a small, ﬁxed distance. 
The problem was computing the direction to move the pen for each step. 
Early versions were slow and expensive because they required to multiply and 
divide. Bresenham’s algorithm worked in integer coordinates and did not require

8
1
Introduction
multiplication and divisions, making it faster than other approaches. Drawing lines 
on a plotter using Bresenham’s algorithm was a signiﬁcant accomplishment. The 
same technique applies to today’s raster displays. 
In 1963, while a graduate student at MIT, Ivan Sutherland developed and demon-
strated a 2D drawing program named Sketchpad, shown in Fig. 1.7 Sketchpad was 
a 2D drawing system for constraint-based engineering drawings. It was not 3D and 
did show how effective interactive computer graphics could be [15]. 
Fig. 1.7 Ivan Sutherland demonstrating Sketchpad (Courtesy of Wikipedia)

1.2 First Computer Graphics System (1949)
9
Computers had had calligraphic screens since the 1950s when MIT developed the 
ﬁrst real-time Whirlwind computer. 
In the early 1960s, RMS, a small test equipment company, ventured into the 
development of designing and manufacturing a completely transistorized character 
generator. That led the company to change its name to Information Displays, Inc. 
(IDI). At IDI, Carl Machover, Kenneth King, and Alfred Pestone developed the 
IDIIOM system for Britain’s National Engineering Laboratories (NAL) in 1963. 
The IDIIOM was the ﬁrst stand-alone CAD platform. According to the company, 
IDIIOM was the ﬁrst general-purpose design workstation, the ﬁrst commercial 
CADD platform, and the ﬁrst software drafting package to operate on its own 
computer without communication links to a mainframe computer (Fig. 1.8). 
Unlike today’s GPU-based systems that rapidly generate bitmaps, IDI display 
systems employed vector controls. Machover commented: 
In the early 1960s, we operated in a vector world. Raster was not a common computer graphics 
display: The cost of memory to store the bit map was enormous. Most early computer graphic 
systems were all stroke writers, or vector writers. Not raster. Raster did not become a broadly 
used technology until the late 1960s and early 1970s when the cost of storing bitmaps went 
down. Until that point, the deﬂection ampliﬁers, which you used with stroke writers, were 
very powerful and needed kilowatts to operate. They also needed high speeds. And the 
resulting cost of monitors was consistently too high [17]. 
Calligraphic screens dominated until the mid-1980s when raster devices became 
the primary interactive display technique. By 1968, systems (e.g., Adage, Vector 
General) with general-purpose 4× 4 matrix transformation capabilities, clipping, and 
perspective divide became available. Shortly after that, IBM, Evans and Sutherland,
Fig. 1.8 The ﬁrst stand-alone workstation, IDI’s IDDIOM with Calligraphic screen and light-pen 
(Courtesy of IEEE) [16]

10
1
Introduction
Fig. 1.9 An engineer using 
a light pen on a Control Data 
274 Digigraphics vector 
display terminal, circa 1965 
(Courtesy of the Charles 
Babbage Institute Archives, 
University of Minnesota 
Libraries)
and other companies announced commercial systems with real-time manipulation of 
3D wireframe models with perspective (Fig. 1.9). 
The displays evolved too, and lower-cost raster displays, based on TV tech-
nology, replaced the big expensive vectorscopes. Raster, also known as scan-line 
screens, created a foundation for graphics controllers and established a standard 
image method. Raster displays came into prominence in the late 1970s and early 
1980s and got their most signiﬁcant boost with the introduction of the IBM PC in 
1981. 
The Term Pixel Introduced (1965) 
Frederic Crockett Billingsley [18], an American engineer, developed digital image-
processing techniques to support the U.S. space probes to the moon, Mars, and other 
planets. 
In 1965, Billingsley published two papers using the word pixel and may have 
been the ﬁrst to publish that neologism for a picture (pix) element (el) [19, 20]. 
First Tablet and Workstation (1972) 
The Alto handheld computer project began in late 1972. Alan Kay led the project 
at Xerox’s Palo Alto Research Center (PARC). The Alto was a testbed for Kay’s 
ideas for the (now) famous Dynabook [21] tablet design. However, Kay realized that 
the technology did not exist to develop a tablet computer completely. He correctly 
forecast that the technology would not be available until the end of the century. Kay 
saw the Alto as a vision or a rallying call for others who might later evolve into a 
fully ﬂedged Dynabook. 
Most technology people agree that the ﬁrst workstation was developed at Xerox 
PARC in 1972 when Project Alto launched a personal computer for research.

1.2 First Computer Graphics System (1949)
11
Game Console Introduced (1972) 
Magnavox’s Odyssey was the ﬁrst commercially available home video game console. 
It set the stage for Atari and others. 
A small team at Sanders Associates led by Ralph H. Baer [22] designed the 
Odyssey for Magnavox. It was completed and released in the United States in 
September 1972. The Odyssey consisted of the central console, known as the Brown 
Box, which connected to a television and had two rectangular controllers. 
The Personal Computer (1975) 
Lamont Wood traces the introduction of the ﬁrst PC to 1970 with the Datapoint 2200 
[23]. It was a small, stand-alone ofﬁce computer and not intended for consumers. The 
Mark-8 microcomputer, designed by Jonathan Titus in 1974, was the ﬁrst computer 
consumers could play with. It used the Intel 8008 processor. Shortly thereafter, MIT 
completed their prototype, the Altair 8800 microcomputer. Titus’s original name for 
the computer was PE-8 in honor of Popular Electronics magazine. 
The Genesis of the GPU—Pixel Planes (1980–2000) 
The Pixel Planes project in 1980 was the foundation project that led to the GPU. 
The need to generate 3D images for protein research led to the development of a 
system called Pixel Planes at the University of North Carolina (UNC) in 1980. It was a 
new concept for designing graphics hardware that allocated one processor per pixel— 
meaning that many parts of the images on the screen got generated simultaneously, 
vastly improving the speed at which programs could produce graphics. Work on 
Pixel Planes, under the direction of Henry Fuchs, continued at UNC through 1997, 
when the ﬁnal iteration of the project, Pixel Flow, was developed. 
Calligraphic graphics terminals were the primary devices for displaying CAD 
drawings and other line-art images through the late 1970s and early 1980s. 
Microcomputer-based raster displays from companies like Commodore, Radio 
Shack, and others lacked compute power. 
The story of the GPU begins with the introduction of the ﬁrst very large-scale 
integrated circuit (VLSI), a graphics controller developed by the Nippon Electric 
Company (NEC) in 1982. That VLSI semiconductor chip lit up the industry and 
was used in CAD terminals, PCs, and client–server display systems. Before its 
arrival, graphics controllers used in terminals were large circuit boards with dozens 
of discrete logic and memory chips (Fig. 1.10).
One of the most popular graphics terminals of the time was the Tektronix 4014. 
But it was designed and built before the venerable NEC 7220 chip. As a result, it, like 
its peers, had circuit boards with dozens of logic chips and memory, as illustrated in 
Fig. 1.10. The 4014 did not perform any on-terminal computation. All projections 
and transformations were done on an attached minicomputer or mainframe. 
3D Graphics in Games (1983) 
The 3D polygonal game, I, Robot [24, 25], released in 1983 by Atari, was the ﬁrst 
to be produced and sold commercially.

12
1
Introduction
Fig. 1.10 A Tektronix 
graphics terminal system 
board (Courtesy of 
Legalizeadulthood)
The shaded 3D game genre emerged in 1992 with Wolfenstein 3D. It established 
the genre and the model on which designers would base subsequent titles. Inspired 
by Muse Software’s 2D video games Castle Wolfenstein and Beyond Castle Wolfen-
stein (introduced in the 1980s), the PC DOS version was released on May 5, 1992 
(Fig. 1.11). 
Critics and game journalists widely regard the game as popularizing the genre on 
the PC and establishing the basic run-and-shoot archetype for subsequent ﬁrst-person 
shooter (FPS) games.
Fig. 1.11 Wolfenstein 3D 
was the ﬁrst PC-based 3D 
ﬁrst-person shooter 
(Courtesy of Software & 
Apogee Software 1992 id) 

1.3 The Graphics Processor Unit (1999)
13
Shaders (1988) 
Shading software (aka shaders) calculates rendering effects such as lighting, colors, 
reﬂections and shadows, position, and other image enhancement functions. Pixar 
introduced the term shader in May 1988 in their RenderMan Interface Speciﬁcation. 
Early shaders ran on CPUs. Subsequent graphics processors provided specialized 
hardware that had just enough capability to run shaders directly in the graphics 
pipeline. The ﬁrst graphics hardware device with built-in shader capabilities was the 
Nvidia GeForce 3 (NV20), released on February 27, 2001. It could only process 
pixel shaders. Soon after that, GPUs supported vertex shaders. A vertex shader (i.e., 
processor) manages the raw geometry of the 3D model and converts it into the 
coordinate space of the display. The pixel shader manages the pixel’s visibility and 
coloring. 
1.3 
The Graphics Processor Unit (1999) 
Today’s GPU is quite different from the ﬁrst graphics controllers of the 1970s and 
1980s. Modern GPUs combine multiple stages and are the beneﬁciary of VLSI. 
By 1985, graphics controllers, the GPU’s predecessor, became heterogeneous in 
their functions adding audio and video (AV) capabilities. Then in the early 1990s, 
integrated graphics processors (IGPs) in CPU chipsets appeared. 
SGI Introduced the First GPU for Consoles in 1996 
Arcade game system boards have had hardware T&L since 1993 (Sega Model 2 
by Real3D), and video game consoles have had it since the Nintendo 64’s Reality 
Coprocessor GPU created by SGI in 1996. PCs implemented T&L in software until 
1999. 
The Term GPU was Introduced in 1997 by 3Dlabs 
Jim Clark introduced the ﬁrst integrated geometry engine in 1981. In 1997, UK-based 
3Dlabs developed the ﬁrst dedicated programmable T&L engine—they called it the 
Glint Gamma processor. Transform and Lighting was part of the Glint workstation 
graphics chips. 3Dlabs ﬁrst introduced the term GPU—geometry processor unit with 
the Glint chip. 
While Glint was developed for the workstation market as a coprocessor, Nvidia 
introduced the GeForce 256, a single-chip graphics processor with a T&L engine 
for the consumer market in 1999. The company popularized the term GPU to mean 
graphics processor unit. The company has been associated with it ever since and 
gets credited with inventing the GPU. 
Nvidia Built the First Single-Chip PC GPU in 1999 
GPUs have a wide range of applications and provide computing power for ﬁelds as 
diverse as industrial design, scientiﬁc computing, ﬁnance, aerospace, biomedicine,

14
1
Introduction
autonomous driving, and data centers. The GPU’s computing power has grown 
dramatically and given the GPU a dominant role in games and movies. Due to 
the high technical threshold, and signiﬁcant investment needed, companies such as 
Advanced Micro Devices (AMD), Intel, and Nvidia have long dominated the GPU 
market. 
The speciﬁc deﬁning moment of the existence of the ﬁrst GPU is difﬁcult to 
determine—not impossible with enough time and effort, but very difﬁcult because 
you must ﬁnd the people who were there and hope they still have their notebooks or 
a good memory. 
And multiple dates could constitute ﬁrst: the date the project started, the date 
the prototype worked, the announced date of the part, the date the ﬁrst part was 
shipped, or the date the ﬁrst end-user got one. Because the only one of those dates 
that every Company always uses is the announcement—announcement date is used 
in this book. 
1.3.1 
The Evolution of Graphics Controllers to GPUs 
The evolutionary path from a primary 2D display controller to today’s version of a 
GPU had several steps or phases that were not necessarily linear. However, except 
for some phase shifts, it generally looked like the following diagram (Fig. 1.12).
At the 1994 COMDEX conference, Intel’s founder and Chairman of the Board, 
Andy Grove, said, “We need to achieve built-in multimedia and communications 
capability,” Grove said, “And we need to achieve it ahead of this information conduit 
coming our way. Otherwise, it will turn into a debilitating situation for us.” 
And yet, Intel did not integrate video or graphics until 2010, and serious audio 
processing did not appear until 2016. GPUs integrated those functions in the late 
1990s and early 2000s. 
In 2006, Nvidia introduced its proprietary parallel programming software derived 
from C++ , called CUDA—compute uniﬁed device architecture. CUDA is a soft-
ware layer that gives direct access to the GPU’s virtual instruction set and parallel 
computational elements, for the execution of compute kernels. 
Tessellation, AI, and Ray Tracing (2000–2020) 
Software developers moved specialized computer graphics operations in their appli-
cations that used to run on the CPU to dedicated hardware accelerators within the 
GPU. Because of that change, those functions ran incredibly faster. It was a delightful 
blend of Moore’s law and putting processors where performance called for it. 
AMD introduced hardware tessellation in 2001. In 2018, Nvidia introduced hard-
ware AI and accelerated ray tracing. Then, in 2020, Nvidia amazed the world and 
introduced Ampere, the largest GPU ever made—an astonishing 38 billion transistors 
with 8192 processors and hundreds of other specialized processors.

1.3 The Graphics Processor Unit (1999)
15
Fig. 1.12 The evolutionary path of the GPU
Mesh Shading (2016–2020) 
GPU development never slowed, and in 2016 AMD introduced primitive shaders 
in their Graphics Core Next (GCN) Vega GPU. Then, in 2018, Nvidia introduced 
the mesh shader capability in its Turing GPU. In response, Microsoft introduced 
the DirectX 12 Ultimate in May 2020, allowing software developers to exploit the 
hardware developments. 
Shortly after that, Epic released a demo of mesh shading on the new PlayStation 
5. It showed images of billions of subpixels in real time [26] (Fig. 1.13).
The visuals were stunning and made with what Epic called nanite virtualized 
micropolygon geometry. That new level of detail (LOD) geometry would free artists 
to create polygon detail beyond what the eye could see.

16
1
Introduction
Fig. 1.13 The entire image above was calculated; it is not a photograph or texture map (Courtesy 
of Epic Games, Nanite demo)
1.4 
Performance (2000–2026) 
Over most of the history of GPUs, vendors have battled over performance ﬁgures. 
Graphics quality is often a subjective interpretation, so how do you convince 
customers that your often closely matched product is better than the competition? 
Performance. 
Computer and computer graphics performance are measured in several ways. No 
one way is the best or more correct, and it is a matter of what is important to the 
user and the user’s level of understanding. GigaFLOPS (billions of ﬂoating-point 
operations per second) is a good option because it is an empirical measurement and 
one used with different platforms for comparison (Fig. 1.14).
The arrows in Fig. 1.14 indicate the time span between platforms obtaining similar 
levels of performance. 
For this book, Giga Floating-point Operations Per Second (GFLOPS) also helps 
tell the story. Performance improvements over time are shown in Fig. 1.14, and there 
is an apparent leveling-off in gains over time. Several observers have commented that 
Moore’s law is slowing down. Remembering that Moore’s law is an economics obser-
vation, the slowdown is true—it is more expensive to obtain performance improve-
ments as processor counts rise. The photolithography and other manufacturing equip-
ment have their own kind of inverse Moore’s law—as process size shrinks, equipment 
costs double. That, in turn, delays the introduction of new process nodes and parts. 
On the other hand, practical limitations also inspire innovative ways to squeeze more 
performance out of nanometer (nm) and angstrom-sized transistors—architectural 
tricks with caches, multichips, 3D memory stacking, new materials, and most of all, 
software.

1.5 The GPU’s Changing Role
17
Fig. 1.14 Performance of popular platforms over time
1.5 
The GPU’s Changing Role 
With the introduction of VLSI graphics controllers, costs came down, demand 
increased, and production went up. Twenty years later, VLSI chips with millions of 
transistors and parallel processors entered the market labeled as graphics processor 
units—GPUs (Fig. 1.15).
GPUs replaced all the logic on the Tektronix circuit board apart from memory 
and a few input/output chips. But more importantly, they added new, more robust 
functionality at a fraction of the cost of system boards. 
In book number two of this series, What is a GPU, one of the precepts of the series 
is introduced—the eras of the GPU. Since its introduction, the GPU has evolved, 
becoming more programable and more of a compute platform than a specialized 
graphics engine. And, as a result, it is now central to the progress of digital technology. 
While it may always be used as a graphics engine, it has also taken on other roles in 
computing and added specialized compute engines for things like AI and ray tracing. 
Merchant Services Evolve 
In the last two decades, hardware design tools have matched those in software design. 
Designers have introduced tools that are easy to install and use, including some that 
even run in a web browser and are free. Prototyping boards have also become more 
available with off-the-shelf boards from Raspberry Pi, micro:bit, and others such as 
NET Gad. Services such as online storefronts, accessible and powerful design tools,

18
1
Introduction
Fig. 1.15 Nvidia’s GeForce 
256, the ﬁrst single-chip 
GPU (Courtesy of 
Konstantin Lanzet, 
Wikipedia)
consultancies and collaborators, crowdfunding, incubators, and next-day delivery 
networks have reduced time and cost. They have also reduced the risk for new projects 
in established companies as well as start-up companies [27]. 
However, those tools, services, and support are not guarantees to success, and 
there are many instances where new hardware products, especially consumer prod-
ucts, have failed. A working prototype is just the start. Design and prototyping are 
proof only of concept. The other challenge in hardware development is the need for 
capital. In-house projects and start-ups need to purchase materials and manufacturing 
equipment (tooling). Crowdfunding has offset some of the ﬁnancial obstacles. 
Open-source intellectual property (IP) has also helped organizations develop new 
products, reusing sections or functions that do not represent the value-added portion 
of a new design but are needed to have a functional unit. 
Most, or all, of those infrastructure and tool improvements are because of the 
GPU. The GPU is even used to create more advanced GPUs (Fig. 1.16).
Designers created powerful GPUs in the late 1990s for two applications: CAD 
and 3D gaming on a PC. By then, the CAD market was smaller than the gaming 
market, but it still accounted for millions of users whose processing demands were 
growing. The larger gaming market provided economy of scale for the consumer 
GPU suppliers, and they could offer the devices for economical prices. At the begin-
ning of the twenty-ﬁrst century, computer scientists recognized the cost-effectiveness 
of GPUs as parallel processors. Consumer GPUs became accessible with their 
own software development environments and displayed the potential to solve some 
challenging computing problems.

1.6 The GPU’s Application
19
Fig. 1.16 GPUs have become ubiquitous and accelerated science, resulting in new products, 
enhanced vehicle safety, and many other applications
1.6 
The GPU’s Application 
Because of the redundancy of the processors in a GPU (also referred to as shaders), 
computer architects and engineers saw that the GPU would also be a highly scal-
able device, making it suitable for other applications and price points in addition 
to gaming. Along with the broadening application base of GPUs (of diverse sizes), 
several new programming languages emerged to make employing GPUs easier. As 
a result, today’s GPUs are more efﬁcient than ever, accelerate a broader range of 
applications, and have become as ubiquitous as the CPU and the display. GPUs are 
not just for gaming anymore. 
A few of the more popular applications are brieﬂy listed in the following section. 
1.6.1 
AI and Machine Learning 
AI and ML offer some of the most exciting applications for a GPU. AI applies 
machine learning, deep learning, and other techniques to solve actual problems.

20
1
Introduction
Computer scientist and ML pioneer Tom M. Mitchell deﬁned ML as a branch of 
(AI). Speciﬁcally, Mitchell said, “Machine learning is the study of computer algo-
rithms that allow computer programs to automatically improve through experience 
[28].” 
Because of the GPU’s single instruction, multiple data (SIMD) construction, it 
offers a fantastic computational capability to process massive amounts of data. That 
makes the GPU useful for applications that inherently generate large data sets such 
as image recognition, recommendation systems, and database indexing. However, 
processing massive amounts of data is possible with GPUs as long as the CPU-GPU 
data path is sufﬁciently fast and the algorithms can be implemented in parallel. 
AI and ML use deep learning (DL) techniques and deconvolutional neural 
networks (DNNs) to create extensive data sets that could take years for conven-
tional CPUs to process. A GPU can work through that massive amount of data in 
minutes. Such speed gives researchers answers sooner—enabling them to ask the 
next question—and speeds up scientiﬁc and medical research. 
1.6.2 
Accelerated Computing and Supercomputers 
There is another class of challenges that involve copious quantities of data that share 
some common attributes. Credit card transactions are one example. Hundreds of 
millions of people use credit cards every day. The data associated with those trans-
actions is almost unimaginably large. Remote computers complete the transactions 
instantly—no one wants to wait at a checkout counter or ATM for their card to be 
processed. The computers must also check the transaction for fraud and sufﬁcient 
funds—in seconds. 
Scientists and engineers create simulations of viruses and vehicles with millions 
to trillions of data points. Then those data points are stressed and tested. The effect 
on adjacent points is equally critical, and those analyses must happen as quickly as 
possible. A typical comment among engineers and scientists doing such work is, “I 
need an answer before I retire or die.” 
GPUs are used as accelerator coprocessors in supercomputers. They are used 
to accelerate speciﬁc and specialized parts of a problem. They are not used for 
general-purpose work or system operations and overhead functions. 
1.6.3 
Content Creation 
The ﬁrst graphics processors drew lines for engineering drawings using CAD. CAD 
was the ﬁrst digital content creation (DCC) application and is still an essential require-
ment. Drawing lines is relatively simple. It is complicated to shade the areas within 
those lines so that they look and behave realistically and accurately. Today, GPUs

1.6 The GPU’s Application
21
have risen to prominence for video editing, rendering, and extraordinary special 
effects. 
Subsequent applications involve animations and simulations of actors, imagined 
space, ancient sailing ships, tigers, and ants coming to life because of GPUs. Content 
creation for ﬁlm, TV, and today’s games are taxing GPU in every possible way. 
1.6.4 
Gaming 
Because so many people play video games (estimated to be 30 to 100 million players), 
gaming has always been a signiﬁcant computer application. Gaming appeared on 
computers as early as 1949 [29]. Video games, especially adventure and ﬁrst-person 
world games, are massive simulations. Simulating and rendering such complex envi-
ronments has evolved beyond the capabilities of a single CPU. Developed as copro-
cessors, graphics processors ofﬂoaded the main CPU and took on the arduous task of 
rendering the elaborate scenes and actions. Such games require thirty to sixty (now 
120?) frames per second. 
Gaming propelled the development of the GPU to new heights as the demand for 
realism and speed increased. Games have become computationally intensive, with 
amazing realistic images and massive game worlds. GPUs developed simultaneously 
as the more complex games and high-resolution displays reached 5 k and 8 k (11 
and 33 million pixels, respectively) with the promise of 16 k (75 million pixels). The 
burden such displays place on the GPU goes up exponentially. 
1.6.5 
Molecular Modeling 
Molecular modeling is the method and means of using a computer to study 
molecules and their properties. In the 1960s, scientists and chemists made wireframe 
visualizations of molecules on big CRT screens and pen plotters. 
MIT developed the ﬁrst interactive display of molecular structures in the mid-
1960s. It ran on Project MAC (Multi-Access Computer), an early time-sharing 
mainframe computer, Cyrus Levinthal [30] and his colleagues designed as a 
model-building program to work with protein structures. 
The structures of molecules have been an appealing choice for developing new 
computer graphics tools because the data are relatively easy to create, and the results 
are usually visually pleasing. Molecular graphics are especially useful in representing 
global and local properties of molecules, such as their electrostatic potential. The 
graphics models can also be animated to represent molecular processes and chemical 
reactions.

22
1
Introduction
1.6.6 
Video and Photo Editing 
Imaging, including photo editing, was also an early application of GPUs. Digitized 
photos and computer-generated images have always pushed the limits of processing 
as artists and photographers strive to manipulate every pixel to get effects, correct 
blemishes, remove elements, etc. 
In video, the ﬁrst genuinely nonlinear editor was the analog CMX 600, intro-
duced in 1971 by CMX Systems, a joint venture (JV) between The U.S. Columbia 
Broadcasting System (CBS) and Memorex. 
The concept moved to computers and enabled the digital nonlinear (video) editing 
(DNLE) era in the early 1990s. The ﬁrst DNLE was the Quantel system named Harry, 
released in 1985. Harry was the ﬁrst all-digital video editing and effects compositing 
system. 
Modern GPUs have dedicated video compression and decompression (CODEC) 
engines, which provide r video creation and playback capabilities and are more 
power-efﬁcient. GPUs have also been used in transcoding, i.e., converting from one 
video format (e.g., MP4) to another (e.g., VP1). Note that AMD and Intel have built 
and integrated dedicated transcoding engines into their CPUs that are even more 
efﬁcient. 
1.6.7 
Vehicle Navigation and Robots 
Because GPUs can consume and compute results for large quantities of data, they are 
ideal for processing all the input needed for autonomous vehicles (AVs). Autonomous 
robots are a similar case. GPUs are useful in all phases: from concept through 
simulation and testing to deployment as a device controller. 
1.6.8 
Crypto Mining 
In late 2015, the monitoring of transactions based on crypto coins like Ethereum used 
graphic add-in boards (AIBs) to brute-force search the web to ﬁnd such transactions. 
This practice is called mining or crypto mining. As the value of crypto coins went up 
in price (relative to the U.S. dollar), the demand for AIBs went up, which drove up 
prices, grossly distorted the market, and caused shortages. In 2021, Nvidia introduced 
a new class of GPUs they called the Cryptocurrency Mining Processor (CMP) and 
led to the term “mining GPU” (mGPU). In this way, the company tried to ensure 
GeForce AIBs went to gamers and that would smooth out supply and demand.

1.7 The Many Roles of the GPU Require Additional Names
23
1.6.9 
Summary 
The preceding is a brief recap of some of the applications where GPUs are used. 
The list will continue to expand as adjacent markets see the advantage of massive 
parallel processing. The complete list of all the applications that use GPUs would be 
enormous. There must be 50 or more just in AI. The above summarizes some of the 
most prominent. 
1.7 
The Many Roles of the GPU Require Additional Names 
GPU use has spread to multiple platforms, and several naming schemas have become 
standardized (Table 1.1): 
. Category name—GPU, CPU, and APU.
. Architecture (also called microarchitecture)—for example, AMD’s Radeon 
DNA (RDNA), Intel’s Xe, or Nvidia’s Ampere.
. Code name—for example, AMD’s Navi, Intel’s Arc, or Nvidia’s A40.
. Model name or number—for example, AMD’s Navi21 and Nvidia’s GA100.
. AIB brand and model—for example, AMD’s Radeon RX 6000, Intel’s 
Alchemist, or Nvidia’s Ray Tracing Texel eXtreme (RTX) 3080 Ti.
. Generation—for example, AMD’s Radon 6000 or Nvidia’s 3000. Generation 
names are not reliable for reasons known to the manufacturers. In 2008, Nvidia
Table 1.1 Nvidia architectural names 
Model name Product name
Architecture Nvidia 
NV04
Riva TNT, TNT2
Fahrenheit 
NV10
GeForce 256, GeForce 2, GeForce 4 MX
Celsius 
NV20
GeForce 3, GeForce 4 Ti
Kelvin 
NV30
GeForce 5/GeForce FX
Rankine 
NV40
GeForce 6, GeForce 7
Curie 
NV50
GeForce 8, GeForce 9, GeForce 100, GeForce 200, GeForce 
300 
Tesla 
NVC0
GeForce 400, GeForce 500
Fermi 
NVE0
GeForce 600, GeForce 700, GeForce GTX Titan
Kepler 
NV110
GeForce 750, GeForce 900
Maxwell 
NV130
GeForce 1060, GeForce 1070
Pascal 
NV140
Nvidia Titan V
Volta 
NV160
GeForce RTX 2060, GeForce GTX 1660
Turing 
NV170
GeForce RTX 3060, GeForce RTX 3070
Ampere

24
1
Introduction
offered the GeForce 9000 series and then, presumably to avoid going to a ﬁve-
digit number, brought out the GTX 200 series, and stayed with that until 2018 
when it introduced the RTX 2000 series. ATI used Rage 128 in 1998, then went 
to Radeon 7000 in 200, then Radeon 1000 in 2005, climbing up to Radeon HD 
(high-deﬁnition) 7000 in 2012 and then dropping to RX 200 in 2015, and back to 
RX 5000 in 2019.
. AIB Marketing names have been Radeon for AMD, Arc for Intel, and GeForce 
for Nvidia. 
It has become common, although incorrect, to refer to an AIB as a GPU. People 
might say “the new Nvidia GeForce GPU” using GPU instead of AIB. So, the 
terms have become interchangeable and indistinguishable for many consumers and 
reporters. It is like referring to a car as the motor (and automobiles used to be called 
motor cars). For example, if someone had a Wizbang twelve-thirty-four add-in board 
in their desktop PC, they might say, “My PC has a Wizbang 1234 GPU.” Or “The 
GPU in my system is a Wizbang 1234.” 
The term card is frequently used to describe or denote an AIB—a semantic pref-
erence. AIB is a more speciﬁc and descriptive term and is the convention used in this 
book; however, it may seem pedantic. 
Because of the ambiguity in identifying a GPU, people refer to a graphics card 
in a notebook or laptop. There is no provision for an AIB in notebooks, especially 
thin and lightweight models. The GPU and its memory are soldered directly to a 
notebook’s system board. 
The original purpose of the GPU was to accelerate the graphics pipeline for 
games and CAD visualization. Displaying 3D models requires geometry processing 
of the models (to improve mesh quality) and matrix math (to transform and project) 
followed by rendering to determine each pixel’s color. Rendering shows visual effects 
(e.g., transparency, reﬂection, and smoothness). Those are two separate and noncom-
plementary tasks, but both are served admirably by high-speed parallel processing. 
The algorithms that determine the correct rendering colors using GPU processors 
became known as Shaders. 
Shaders and Processors 
The term shaders has become synonymous with processor. People will refer to the 
number of shaders in a GPU, or a ﬁxed-function processor as a tessellation shader. 
A shader is a program that is run on a processor, and sometimes it is a ﬁxed-function 
processor. But the distinction isn’t respected, and so the parallel processor in a GPU 
are known as shaders. 
SIMD is the type of architecture used in a GPU. CISC (complex instruction set 
computer) is the type of architecture used in an ×86 CPU. RISC (reduced instruction 
set computer) is the type of architecture used CPU architecture used in mobile devices 
[31]. 
The GPU and CPU architecture designs are given names. Examples would be the 
AMD RDNA, the Intel Golden Cove, or the Nvidia Hopper.

1.7 The Many Roles of the GPU Require Additional Names
25
The architectural design may be used in several versions or generations and given 
a code name. Examples would be AMD’s Navi, Intel’s Adler Lake, or Nvidia’s 
Ampere. 
Models or versions can be made from an architectural design forming a family. 
Examples include AMD’s Radeon, Intel’s Core in, and Nvidia’s RTX. 
The GPUs are mounted on graphics AIBs or the PC’s system board. The system 
board is also known as a motherboard and abbreviated as a mobo. CPUs are mounted 
on the device’s system board. 
Each AIB is given a brand name and a model name such as AMD’s Radeon (brand) 
RX6000 (model), Intel’s Core (brand?) nth-Gen (model?), or Nvidia’s RTX (brand) 
3080 (model) (Fig. 1.17). 
As mentioned above, all those names get used interchangeably. It is frequently due 
to a misunderstanding of the hierarchy of the naming structure of the manufacturer. At 
other times, the interchange is due to laziness or attempting to sound knowledgeable 
or cool. It only matters if the usage is misleading, e.g., referring to a “graphics card” 
in a notebook. 
The mass-produced GPU quickly rose to a level where it enjoyed the economy of 
scale the ×86 processor had. It became recognized as a cost-effective processor with 
massive computing density. Soon, the GPU was used as a compute accelerator. Other 
than an awkward programming interface that only a serious coder could appreciate, 
it exceeded the expectations of users and suppliers. Over time, GPU-based systems 
moved into the top ten of the 500 fastest supercomputers. Once there, they never left.
Fig. 1.17 Taxonomy of 
names 

26
1
Introduction
GPUs were then integrated into the ×86 CPU and ARM-based system-on-a-chip 
(SoC) as shared-memory GPUs. 
The First EGPU was by ATI in 2008 
As laptops became notebooks, they became thinner and lighter. The power and space 
needed for a powerful GPU became problematic. Designers developed experimental 
systems that introduced high-speed connections used by GPUs, known as the periph-
eral component interconnect express (PCIe). However, the complexities of cabling, 
connectors, and line drivers proved too expensive and cumbersome to be effective. 
Fujitsu offered a PCIe product they called the Amilo GraphicsBooster [32]. The 
introduction of the high-speed serial input–output (I/O) communications technology, 
Thunderbolt (AKA Mini DisplayPort · IEEE 1394 (2011)) and then USB-C (2017) 
made it practical to have an external GPU. However, the additional case and power 
supply kept the price up. 
When USB-C/Thunderbolt was introduced, the situation changed. With USB-
C, it was possible to send PCIe signals over a low-cost, high-bandwidth cable and 
connector. That could make an external AIB/GPU a practical docking option. 
1.8 
Types of GPUs 
The GPU has evolved and been used on several dissimilar platforms and applica-
tions but sharing similar instruction set architecture (ISA) and applications program 
interface (API) suites. 
Lower-case preﬁxes have been applied to GPUs to distinguish them and have 
been generally accepted and used by the industry, e.g., mGPU for crypto mining. 
The GPU was used in so many conﬁgurations that it was necessary to add a preﬁx 
to identify the type of GPU and what application used it. That created the following 
designations:
. dGPU—a discrete (stand-alone) processor with its own private high-speed 
memory—dGPUs are on AIBs and system boards in notebooks.
. iGPU—a scaled-down version with fewer processors than a discrete GPU and 
using shared local RAM with the CPU.
. vGPU—a virtual GPU on an AIB with a powerful dGPU located remotely in the 
cloud or a campus server.
. mGPU—a GPU used by crypto miners—later named CMP—a cryptocurrency 
mining GPU .
. eGPU—an external AIB with a dGPU in a stand-alone cabinet (typically called 
a breadbox) used as a graphics booster and docking station for a notebook. 
The term eGPU is used for embedded GPUs and ExpressCard GPUs [33]. The 
ﬁrst company to demonstrate the concept was ATI in 2008 when they showed their 
External Graphics Port (XGP), an external GPU case attached to a laptop. 
The diagram in Fig. 1.18 shows the relationship between the types of GPUs.

1.8 Types of GPUs
27
Fig. 1.18 GPUs are found 
in many types of systems and 
have different preﬁxes
. GPUs are in PCs as dGPUs and iGPUs, and often both are present in a PC 
simultaneously.
. GPUs are in smartphone and tablet SoCs.
. GPUs are in game consoles.
. GPUs are in vehicles for entertainment systems, customizable dashboards, and 
autonomous driving systems.
. GPUs are used as computer accelerators in supercomputers and servers.
. GPUs are in airplane and ship cockpits, augmented reality (AR) and virtual reality 
(VR) systems, cameras, digital cinema projectors, robots, scientiﬁc instruments, 
toys, home security devices, TVs, and visualization and simulation systems. 
The GPU grew out of a need and demand for faster, more realistic games. But the 
GPU market is far from a game; it is a mission-critical market with high demands, 
high-stakes, extraordinary development, and advancement exceeding Moore’s law 
by orders of magnitude. 
Look around; how many GPUs do you think are in your life? Probably more than 
you can imagine (Fig. 1.19).
Getting the terminology right is a challenge in any book, discussion, or presen-
tation. As the above illustration shows, the same name can apply to multiple things, 
and the same thing can have multiple names. It is The Tyranny of Terminology [34].

28
1
Introduction
Fig. 1.19 The problems with segmentations and names
1.9 
Conclusion 
This chapter was offered as an introduction to the history of the GPU and some of the 
many terms used in describing a GPU and the environment in which it lives. GPUs 
have been with us since the turn of the century. To some, that may seem forever. 
But they didn’t just suddenly appear like dandelions. The need for a GPU began in 
the 1960s. We did not even have a term for it then, just a desire. Computer graphics 
has been, and always will be, limited by memory. In the 1960s, memory was more 
precious than the CPU. Today, memory is less expensive on a unit basis than the 
processors it serves. Even so, 32 GB of RAM will represent almost a third of the 
price of an AIB, so memory is still not cheap—just more available and reliable. 
GPUs are remarkable devices, but it was VLSI and Moore’s law that made them 
possible. 
References 
1. Redmond, K. C. and Smith, T. M. Project Whirlwind: The History of a Pioneer Computer. 
Bedford, MA, Digital Press. ISBN 0-932376-09-6. (1980) 
2. Canning, C. Predicting the Past, The Lark, (April 5, 2017), https://www.larktheatre.org/blog/ 
predicting-past/ 
3. Moore’s law, Moore’s law – Wikipedia, https://en.wikipedia.org/wiki/Moore%27s_law 
4. Smith, A. R.  A Biography of the Pixel, MIT Press, https://mitpress.mit.edu/books/biography-
pixel (August 3, 2021) 
5. Yares, E. 50 years of CAD, Design, (February 13, 2013), https://www.designworldonline.com/ 
50-years-of-cad/

References
29
6. The Manchester Small Scale Experimental Machine - “The Baby”, http://curation.cs.manche 
ster.ac.uk/computer50/www.computer50.org/mark1/new.baby.html 
7. Committee on Innovations in Computing and Communications: Lessons from History, National 
Research Council, Funding a Revolution: Government Support for Computing Research, The  
National Academies Press, ISBN-10: 0-309-06278-0 (1999) 
8. Fedorkow, G. Gambling On Whirlwind: How The Us Navy Spent $3 Million+ And Got A 
Computer Game, Computer History Museum, (October 22, 2019), https://computerhistory.org/ 
blog/gambling-on-whirlwind-how-the-us-navy-spent-3-million-and-got-a-computer-game/ 
9. Peddie, J Developing the Computer, in The History of Visual Magic in Computers, Springer 
Nature Switzerland AG., pages 148–158, (2013) 
10. Jacobs, J.F. The SAGE Air Defense System: A Personal History, MITRE Corporation, (1986) 
11. Carlson, W.E. Computer Graphics and Computer Animation: A Retrospective Overview, Ohio  
State University, (2017), https://ohiostate.pressbooks.pub/graphicshistory/ 
12. Oppenheimer, R. William Fetter, E.A.T., and 1960s Computer Graphics Collaborations 
in Seattle, (2005), https://www.academia.edu/7801224/William_Fetter_E_A_T_and_1960s_ 
Computer_Graphics_Collaborations_in_Seattle 
13. Fetter, W.A. Computer Graphics in Communication Hardcover, McGraw-Hill; First Edition, 
(January 1, 1965), https://openlibrary.org/works/OL7390788W/Computer_graphics_in_com 
munication 
14. Bresenham, J. E., Algorithm for computer control of a digital plotter, IBM Systems Journal, 
Volume: 4, Issue: 1, (1965), https://ieeexplore.ieee.org/document/5388473 
15. Kasik, D and Senesac, C.J. Visualization: Past, Present, and Future at The Boeing Company, 
(2014) 
http://gpdisonline.com/wp-content/uploads/past-presentations/DX28_Boeing-Kasik-
Senesac-Visualization-DX-Open.pdf 
16. Bissell, Don. Was the IDIIOM the First Stand-Alone CAD Platform? IEEE Annals of the 
History of Computing (Volume: 20, Issue: 2, Apr-Jun 1998), https://ieeexplore.ieee.org/cart/ 
download.jsp?partnum=667292&searchProductType=IEEE%20Journals%20Magazines 
17. Don Bissell’s 14 Mar. 1997 interview with Carl Machover. 
18. Billingsley, C.B. https://en.wikipedia.org/wiki/Frederic_C._Billingsley 
19. Billingsley C.B. Digital Video Processing At JPL, SPIE, Volume Number: 0003 (September 
26, 1965) 
20. Richard, L. Pixels and Me, lecture, https://www.youtube.com/watch?v=D6n2Esh4jDY 
21. Dynabook,
https://history-computer.com/products/dynabook-complete-history-of-the-dyn 
abook-computer/ 
22. Martin, D Ralph H. Baer, Inventor of First System for Home Video Games, Is Dead at 92, 
New York Times, (December 7, 2014), https://www.nytimes.com/2014/12/08/business/ralph-
h-baer-dies-inventor-of-odyssey-ﬁrst-system-for-home-video-games.html 
23. Wood, L Datapoint: The Lost Story of the Texans Who Invented the Personal Computer 
Revolution, Hugo House Publishing, Ltd., Engelwood, CO. (2010) 
24. “I, Robot – Videogame by Atari”. Killer List of Videogames. (1983) Retrieved 2009-08-19. 
25. I, Robot (arcade game) http://en.wikipedia.org/wiki/I,_Robot_(arcade_game). 
26. A ﬁrst look at Unreal Engine 5, (June 15, 2020), https://www.unrealengine.com/en-US/blog/ 
a-ﬁrst-look-at-unreal-engine-5 
27. Hodges, S and Chen, N. Long Tail Hardware:Turning Device Concepts Into Viable Low Volume 
Products, IEEE Pervasive Computing, Volume: 18, Issue: (4, Oct.-Dec. 1, 2019) https://doi. 
org/10.1109/MPRV.2019.2947966 
28. Iriondo, Roberto, Machine Learning (ML) vs. Artiﬁcial Intelligence (AI) — Crucial 
Differences, 
https://medium.com/towards-artiﬁcial-intelligence/differences-between-ai-and-
machine-learning-and-why-it-matters-1255b182fc6 
29. Peddie, J Developing the Applications, History of Visual Magic in Computers; How Beautiful 
Images are Made in CAD, 3D, VR, and AR,” (p 81) Springer, London. (2013). 
30. Francoeur, E Cyrus Levinthal, the Kluge and the origins of interactive molecular graphics, 
Endeavour Vol. 26(4) (2002) https://tinyurl.com/995uczze

30
1
Introduction
31. Peddie, J The Many Roles and Names of the GPU - Its versatility has led it to multiple and 
various platforms, IEEE Computer Society, (2018), https://www.computer.org/publications/ 
tech-news/chasing-pixels/the-many-roles-and-names-of-the-gpu 
32. Peddie, J. ATI’s XGP—external hybrid graphics for laptops, TechWatch, Volume 8, Number 1 
page 17. (June 16, 2008) 
33. eGPU candidate system list, https://www.techinferno.com/index.php?/forums/topic/3108-
egpu-candidate-system-list/#post57511 (2013) 
34. The Tyranny of Terminology, Jon Peddie Research TechWatch (May 28, 2010), https://www. 
jonpeddie.com/editorials/the-tyranny-of-terminology/

Chapter 2 
1980–1989, Graphics Controllers 
on Other Platforms 
Computer graphics started with the 2D drawing which is commonly referred to 
today as CAD drawings. Douglas Ross a researcher at the Massachusetts Institute of 
Technology (MIT), is credited with being the ﬁrst person to use the term computer-
aided design (CAD) in the 1950s [1]. 
CAD software was developed at General Motors by Patrick Hanratty in the late 
1950s, but it was proprietary and something of a company secret. In 1962 Ivan 
Sutherland famously showed his version at MIT. 
In January 1982, one year after the PC was introduced, John Walker introduced 
his company Autodesk, and its new program, AutoCAD. AutoCAD would go on to 
become the most popular and well know CAD program ever. 
In 1981 NEC introduced the ﬁrst VLSI 2D graphics controller, the famous 
µPD7220 and shortly thereafter Autodesk showed the iconic 2D drawing of the 
famous Carson mansion in Eureka California (Fig. 2.1).
The clock of technological developments in computer graphics hardware and 
software ran fast and by the early 1990s we had higher resolution color displays and 
graphics controllers that could drive them. The age of 3D games emerged in arcade 
machines, on PCs and in game consoles (Fig. 2.2).
At the same time work was being done on more challenging problems using large 
computers, screens, and refrigerator sized graphics processors. 
In the early to mid-1970s, researchers at universities, pharmaceuticals, various 
government agencies, and vanguard ﬁlm studios wanted to exploit the power of the 
digital computer for creating realistically believable, and accurate images, quickly. 
It was the new frontier, and the rule book hadn’t been written yet, no one really knew 
what was possible, or impossible, and so they just did it. 
There were several centers of excellence in the emerging area of computer graphics 
then, and none stood out more than the University of North Carolina along with New 
York Institute of Technology, in NY, The University of Utah, Rensselaer Polytechnic, 
and Cornell in NY, and Ohio State. 
The story begins with two parallel developments—a dynamic duo also at UNC, 
Nick England and Mary Whitton who developed the ﬁrst raster graphics processor in
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2022 
J. Peddie, The History of the GPU - Steps to Invention, 
https://doi.org/10.1007/978-3-031-10968-3_2 
31

32
2
1980–1989, Graphics Controllers on Other Platforms
Fig. 2.1 Monochrome 2D line drawing done with a NEC 7220, on a PC ruining AutoCAD circa 
in the early 1980s (Historic American Buildings Survey: Library of Congress)
Fig. 2.2 Imaginary worlds in color 3D games run on graphics chips circa 1990 (Courtesy of 
Wikipedia)

2
1980–1989, Graphics Controllers on Other Platforms
33
the mid-1970s at the University of North Carolina (UNC) and NEC’s VLSI semicon-
ductor work in Japan in the late 1970s. In those days, graphics terminals were large, 
expensive, complicated to program, and often challenging to use. Raster scan line 
displays were becoming popular, and costs were dropping. A demand was building 
for lower cost graphics terminals. The timing was right for a graphics chip. But a 
drawing engine would not be enough, the researchers at UNC reasoned. 
NEC and the companies who followed it were able to take advantage of Moore’s 
law, a manufacturing trend that enabled chips to be made at half the size of the 
previous generation. The importance of Moore’s law cannot be underestimated in 
this era of computer graphics development. 
Douglas C. Engelbart and Gordon Moore observed this in the early 1960s. 
However, Engelbart [2], who invented the mouse, was a Stanford Research Insti-
tute researcher and not marketing oriented. Engelbart wrote an article, “Microelec-
tronics, and the Art of Similitude,” in 1959 (published in 1960 [3]) that projected 
the downscaling of integrated circuits (ICs). Engelbart presented those ideas at the 
1960 International Solid-State Circuits Conference with Gordon Moore in the audi-
ence [4, 5]. In 1965, Moore, an engineer and cofounder of Fairchild Semiconductor, 
published an article in Electronics magazine that plotted the number of transistors 
on a chip [6]. Caltech physicist Carver Mead would be credited with calling Moore’s 
observation [7] “Moore’s law” around 1970. However, there is no direct reference to 
that event [8]. 
Regardless of the originality or the naming, the concept of Moore’s observation 
has proven to be the foundation of the world’s most remarkable technology and 
has spawned dozens of industries, hundreds of companies, and thousands of careers 
(Fig. 2.3).
The other driving force was the development of very large-scale integration 
(VLSI). VLSI moved the industry from a sea of logic chips to just one chip, and 
only slightly larger than four logic chips. The evolution from the ﬁrst VLSI 2D 
monochrome graphics controller, the NEC µPD7220, to the VLSI-integrated T&L 
3D GPU chip, the Nvidia NV10, took 20 years. Almost every year from 1982 to 
2012, the industry produced a new graphics controller chip, and each one, built on 
those of the past, added new features while costing the same and performing more 
and faster operations due to Moore’s law. In this chapter, we chronicle the narrative 
of the developments that led to the GPU. 
Computer graphics started on the big machines, what we today call mainframes 
and evolved to high performance terminals and workstations. As the workstations and 
big machines were winding down and being replaced by mid- and minicomputers, 
the PC emerged and with it opened computing up to the masses, which included 
multimedia and entertainment. It also opened up mass production and lower prices. 
In this chapter, we track the big, pioneering machines and with them the development 
of the computer graphics principles we still use. In the next chapter, we look at the 
development and evolution of the PC, and in subsequent chapters in Book three, we 
examine other platforms such as game consoles, and mobile devices.

34
2
1980–1989, Graphics Controllers on Other Platforms
Fig. 2.3 Historical view of the generic organization of 2D/3D raster systems
2.1 Ikonas Graphics Systems (1978–1982) 
The foundation of the GPU. 
Interesting computer graphic research was going on at North Carolina State Univer-
sity (NCSU) in the early 1970s. As a typical research university there was an odd 
assortment of computers, displays, printers, and storage units. But in the computer 
graphics research lab run by Dr. John Staudhammer, there was a standout system, 
the Adage AGT-30. The Adage AGT-30 had an interactive 3D graphics display 
embedded within a highly capable 30-bit computer. It had a hybrid digital–analog

2.1 Ikonas Graphics Systems (1978–1982)
35
section for providing 3D transformations of vector lists on the way from core memory 
to a calligraphic display. 
In 1972 a young engineer named Nick England visited Staudhammer, saw the 
AGT-30 creating interactive 3D graphics, and suddenly knew what he wanted to do 
when he grew up (although he secretly planned to never grow up). In addition to 
the conventional light pen, the lab had one of Tom Ellis’ Rand tablets [9]. England 
was assigned to designing and building an interface for the Rand tablet and writing a 
simple sketch program. That set the stage for what was to come next. NASA wanted 
to experiment with raster displays for the glass-cockpit of the future—up until then 
everything had been calligraphic display based. England built a prototype proof-of-
concept hardware raster display vector generator that ran about 100 ns/pixel. That 
led him to research trade magazines where he learned about AMD’s new 2901-bit-
slice family—4 bits of the arithmetic unit. It was his eureka moment and he realized 
he could use it to build a programmable graphics processor plus frame buffer that 
was fast enough to create the kind of 3D displays NASA wanted. He designed a 
circuit board that used eight of them to make up a 32-bit processor with a 64-bit wide 
instruction RAM, possibly the ﬁrst operational very-large instruction-word (VLIW) 
processor [9]. Running off the Adage unit (as a host), this processor drove a raster 
display and satisﬁed NASA’s objective. 
At the same time, fellow grad student Mary Whitton had designed and was building 
a programmable matrix multiplier coprocessor, which today is referred to as an 
AI tensor processor. The matrix multiplier and the VLIW bit-slice processor were 
combined in what would become the Ikonas system and created the proto GPU. 
In 1978, England and Whitton (who had married in 1974) thought they had the 
basis for a company and formed Ikonas in the back room of their rented house. As 
England describes it: 
“Our sophisticated business plan was to build four or ﬁve systems, take whatever money we 
made, and go on a vacation to Europe. When the money ran out, we would come home and 
get real jobs [10].” 
They presented the system, by handing out ﬂyers, at Siggraph ’78, and showed a 
block diagram of the system. 
The system was designed around a synchronous multi-master bus with 32 bits of 
data plus 24 bits of the address. Via that bus, both the host computer and the graphics 
processor had access to memory, control registers, etc. (see Fig. 2.4). Some customers 
bought display systems with no processor, while a few others added their own custom 
processors. Ikonas added a few more functional units to the original processor design. 
The result was a machine that was general purpose yet optimized for graphics and 
imaging functions. In a single instruction, one could add two numbers, multiply two 
numbers, increment a loop counter, write a pixel, and branch to a subroutine based 
on a condition code—all simultaneously in a single clock cycle.
The frame buffer memory could be accessed in multiple ways—one pixel at a 
time, multiple pixels at once, or as 32-bit general-purpose RAM. The frame buffer 
memory could be used to store a Z buffer as well as multiple frames of color info. The

36
2
1980–1989, Graphics Controllers on Other Platforms
Fig. 2.4 Ikonas graphics system block diagram
video display stream carried 32 bits per-pixel as RGBA or, via a bit-level crossbar 
switch, in most any other format. 
The bus and processor clock was independent from the video pixel and frame 
rates which were completely programmable—almost everything was programmable, 
including the display resolution. However, when the system was ﬁrst turned on 
nothing worked until it was programmed [11]. “Display programmability,” said 
England, “became a favorite of visual perception researchers and of graphics terminal 
developers like DEC, Tektronix, and HP.” 
The system was a success and word spread among researchers in the ﬁeld. Turner 
Whitted (developer of recursive ray tracing) and John Jarvis at Bell Labs got system 
number two, and Henry Fuchs (developer of Pixel Planes) and Fred Brooks at UNC-
CH got number three. 
In 1979 England and Whitton demonstrated a 512 × 512 × 24 display at 
SIGGRAPH ’79. A full-color frame buffer was a pretty big deal in that time frame. A 
few years earlier, Dr. Alexander Schure, a wealthy entrepreneur started the New York 
Institute of Computer Graphics, and hired Ed Catmull (who later founded Pixar) to 
run the lab. Catmull ordered three 8-bit frame buffers from Evan and Sutherland (at 
$60,000 each plus the $80,000 for the ﬁrst) and built the ﬁrst 24-bit frame buffer [12]. 
England and Whitton had now accomplished that in one system for considerably less 
money (Fig. 2.5).
At the Siggraph ’79 conference, Turner Whitted presented a paper with his ﬁrst 
famous ray tracing images. England then loaded Turner’s images (Using eight-inch 
ﬂoppy disks: one disk for each primary color: red, green, and blue) and showed these 
ray tracing results to a very interested, and in some cases astonished crowd on the 
trade show ﬂoor. 
The Ikonas system with its user programmability and display ﬂexibility went on 
to become a standard for graphics research labs in industry and academia. 
They added more software tools and a graphics language (IDL) for real-time 
display applications. At the time the Graphical Kernel System (GKS) was introduced 
by ISO, quickly followed by the PHIGS graphics display list library but Ikonas 
couldn’t wait for them and did IDL on their own (Fig. 2.6).

2.1 Ikonas Graphics Systems (1978–1982)
37
Fig. 2.5 Nick England and Mary Whitton recreating a 1980 Ikonas booth at Siggraph’s 25th 
anniversary in 1998 (Courtesy of England)
Fig. 2.6 The Ikonas system 
(Courtesy of England)

38
2
1980–1989, Graphics Controllers on Other Platforms
Fig. 2.7 Tim Van Hook’s 
ray tracing code rendering 
bi-cubic B-spline and 
polygonal surfaces (Courtesy 
of Nick England) 
NASA, Lockheed, Boeing, and many other aerospace groups used IDL for cockpit 
display prototyping; the last Ikonas systems were in daily use generating displays 
in a cockpit training simulator over 25 years later. Ikonas sold systems to a number 
of ﬁrms in the entertainment industry—Atari, Lucasﬁlm, Marks & Marks, Robert 
Abel, and others. 
In 1982 Adage, Inc. acquired Ikonas and moved manufacturing operations to 
Boston. That was the same Adage that made the AGT-30 that lit up England’s 
imagination and drew him to computer graphics—a full circle. 
At Ohio State University as a grad student, Tim Van Hook developed animation 
in Chuck Csuri’s lab. His academic training was in Fine Arts, and he taught himself 
software and hardware development. Van Hook joined Ikonas and in 1985, he wrote 
microcode for solid modeling, so solid regions could be deﬁned within a buffer 
instead of just the front Z buffer version of it. Van Hook then wrote a ray tracer— 
and it was done entirely within this graphics processor designed in the late 1970s. 
Although it did not run in real-time, it ran about 100 times faster than any computer 
at the time (Fig. 2.7). 
In the late 1980s, Van Hook joined SGI and became the architect for the Nintendo 
64 graphics processor—the ﬁrst integrated GPU for consoles in 1996 (see Book 
Two—The Race to Build the First GPU). Van Hook left SGI and cofounded ArtX 
where he was the architect for the Nintendo GameCube graphics processor and 
developed the ﬁrst integrated graphics GPU in 1998 (Fig. 2.8).
Ikonas proved to be not only the fountain head for GPU development, VLIW 
processors, matrix math engines, and 24-bit frame buffers, but also a place for fertile 
minds to reach their potential and go on to greater things. 
Between 1978 and 1988 Ikonas and Adage sold over 400 systems. England, 
Whitton, and Van Hook went on to form Trancept Systems and develop more amazing 
graphics systems.

2.2 Pixel Planes—The Foundation of the GPU (1980–2000)
39
Fig. 2.8 Tim Van Hook, 
Fellow, ATI Technologies 
2001 (Courtesy of 
Nick England)
2.2 Pixel Planes—The Foundation of the GPU (1980–2000) 
How the actual ﬁrst multiprocessor GPU never became one. 
One of the most signiﬁcant projects in the genesis of the GPU was the Pixel Planes 
project created at the University of North Carolina in the early 1980s. One of the most 
inﬂuential people in computer graphics was the creator of that project, Dr. Henry H. 
Fuchs. 
Pixel Planes was the precursor and foundation for today’s modern GPU. 
In the early 1970s, the metal-oxide-semiconductor (MOS) enabled integrated circuit 
technology to leap forward. With MOS, it was possible to integrate more than 10,000 
transistors into a single chip [13]. That led to very large-scale integration (VLSI) in the 
1970s and 1980s. Devices were being created with tens of thousands of MOS transis-
tors, and that would grow to hundreds of thousands, then millions, and now billions. 
Thanks to VLSI, several exciting new processor and system designs appeared, not 
the least of which were massive parallel processors. 
In 1975 Hungarian-born Henry Fuchs took a position as adjunct associate 
professor of mathematical sciences and medical computer science at the University 
of Texas at Dallas after completing his Ph.D. in Computer Science at the University 
of Utah. While there, he had an epiphany about parallel processors’ applicability to 
computer graphics. VLSI was advancing rapidly, and with it came promising beneﬁts 
but also challenges. Fuchs thought computer graphics could be an essential beneﬁt

40
2
1980–1989, Graphics Controllers on Other Platforms
Fig. 2.9 Fuchs’s background involved modeling chromosomes on graphics systems. He 
constructed 3D models from laser scans of people and objects before coming to UNC in 1978 
(Courtesy of UNC Endeavors) 
to molecular modeling and protein research—views shared by Fred Brooks at UNC 
(Fig. 2.9). 
In 1978, Fuchs became a professor of Computer Science at UNC-Chapel Hill; 
in 1988, he became a Federico Gil Distinguished Professor, a position he held until 
retirement. 
“I found kindred spirits here immediately,” Fuchs said of UNC-Chapel Hill. 
Fuchs, Brooks, and Steve Pizer wanted a minicomputer that could run the 
programs they were developing. Pizer was building a medical imaging program. 
It was still common in those days to ﬁnd researchers writing programs using a stack 
of IBM punch cards and then submitting them to a central computer center. 
This desire for faster rendering in the late 1970s led to the initiation of a project 
at UNC called Pixel Planes. The project was launched in 1980 with the goal of 
providing one processor per-pixel. Such a design would allow for the simultaneous 
generation of many parts of the screen image. Graphics rendering was slow at the 
time, and this approach would vastly improve the speed. The ﬁnal version of the 
Pixel Flow project was developed in 1997. 
Henry Fuchs was one of the ﬁrst true believers in Moore’s law but there were 
others. Fuchs appeared on a panel at the annual computer graphics conference 
SIGGRAPH 1980 with luminaries such as SGI founder Jim Clark, virtual reality 
(VR) pioneer and Ivan Sutherland collaborator Bob Sproull, and ArpaNet developer 
Danny Cohen. There, Fuchs shared some of his ideas about large arrays of proces-
sors dedicated to computer graphics. Clark would develop the Geometry Engine,

2.2 Pixel Planes—The Foundation of the GPU (1980–2000)
41
which successfully exploited the beneﬁts of VLSI and helped revolutionize computer 
graphics [14]. 
Fuchs published his ﬁrst paper with his long-term collaborator John W. Poulton on 
the concept of using massively parallel blocks of processors for computer graphics, 
which he named Pixel Planes in an article in 1981 [15]. Poulton would later join 
Nvidia in 2009 as a senior distinguished research scientist, joining another fellow 
collaborator from UNC, William J. Dally, Nvidia’s CTO, and a former Stanford 
professor. 
The Pixel Planes technology created a design in which many of today’s concepts 
in computer graphics and 3D have evolved. 
Many people in the industry now say if you look at the insides of graphics processing units, 
many of the ideas started out with the Pixel Planes series of machines, said Fuchs in 2016 
[16]. 
Fuchs and his team reasoned that such systems should, if reasonably priced, 
have a signiﬁcant impact in various areas such as architectural design, molecular 
modeling, mechanical design, vehicle simulation, and operator training, as well as 
medical diagnosis and therapy. However, the computational work required to render 
convincing pictures of 3D objects was (and still is) very difﬁcult. The work load 
exceeded the capacities of the most powerful general-purpose computers of the day. 
The researchers at UNC thought the user interface would likely characterize worksta-
tions with real-time 3D capability. As personal computers improved, more resources 
were committed to the user interface. The idea that all users could have a dedicated 
computer with a high-resolution display was no longer controversial and became 
generally acknowledged. Such machines formed the basis for modern desktop work-
stations. Alan Kay in 1977 and Charles P. Thacker in 1979 postulated such ideas in 
the Alto Personal Computer project at Xerox Parc [17]. 
Fuchs and the researchers at UNC started Pixel Planes in 1980 as a research 
project. The Defense Advanced Research Projects Agency partially funded it. The 
project’s goal was to design and build an interactive 3D display that was affordable 
and highly functional. 
The researchers at UNC believed when users could easily manipulate and modify 
in real-time, 3D objects that looked and behaved realistically, we would have reached 
the next plateau in workstation performance. 
During the 1980s, the researchers built the early prototypes (Pixel Planes 1, 2, 
and 3) based on Fuchs’s idea of incorporating processing into the frame buffer— 
a processor per-pixel. Such designs were also called UNC designs and enhanced 
memories. Pixel Planes 1 had four processors, the second model had four by 64, and 
the third had 64 by 64. 
The Design. 
Many papers have been written about the Pixel Planes systems, and many are 
referenced in this summary. The design was a system that could quickly produce

42
2
1980–1989, Graphics Controllers on Other Platforms
Fig. 2.10 Pixel planes overview block diagram—the template for the GPU 
realistic CG images of 3D continuous color tone objects from a database of poly-
gons. It pioneered massive use of single instruction-multiple-data and tile-rendering 
techniques that have become commonplace and critical in today’s GPUs. 
The design used an array of identical LSI chips, which formed a pixel-mapped 
image. It had a buffer with processing capabilities known as enhanced memory 
chips (EMCs), illustrated in Fig. 2.10. The system performed visibility calculations 
concurrently for all pixels using a depth buffer algorithm. 
The SIMD array did polygon edge deﬁnition and smooth shading and performed 
visibility calculations. Those operations are highly efﬁcient using just a small amount 
of each pixel processor’s memory. 
The signiﬁcant features of this design were as follows [18]: 
1. The polygon processing time during image generation was as fast as the real-time 
line drawing systems. 
2. Processing time for a polygon was independent of the size and orientation of 
that polygon and increased only linearly with the number of vertices. Convex 
polygons with any number of vertices could be processed. 
3. The design was implemented in many identical chips; it was modular and easily 
expandable to accommodate increased screen resolution. 
4. The design consisted almost entirely of a rectangular grid of cells, each corre-
sponding to a single pixel; a cell consisted of a set of storage registers augmented 
with minimal processing hardware. 
The high-speed rendering system featured a frame buffer composed of custom 
logic-EMCs. Those devices were programmable and could perform the most time-
consuming and repetitive pixel-oriented tasks in parallel for each pixel. That approach 
offered an efﬁcient tree-structured computation unit to calculate the proper values 
for every pixel in parallel—a uniﬁed mathematical formulation for the work. The 
system contained 512 × 512 pixels by 72 bits per-pixel implemented with 2,048 
custom 300-nm N-type metal-oxide-semiconductor (nMOS) [19] chips. Each chip 
had 63,000 transistors and operated at 10 million instructions per second (MIPS). 
The team developed new algorithms for rendering spheres (for molecular 
modeling), adding shadows, and enhancing medical images. The system was 
used in UNC’s graphics laboratory for molecular modeling, medical imaging, and 
architectural modeling for several years.

2.2 Pixel Planes—The Foundation of the GPU (1980–2000)
43
The UNC team explored the utility of a radical methodology to raster graphics. 
The front-end speciﬁed objects on the screen, and the memory processors used that 
description to generate the ﬁnal image. In linear screen space, image primitives such 
as lines, polygons, and spheres got described by expressions (and operations). For 
example, the coefﬁcients of A, B, and C values desired at each pixel were Ax + 
By + C, where x,y is the pixel’s location on the screen. Therefore, the information 
going to the frame buffer did not consist of address-data pairs (x,y addresses, RGB 
data) but ABCs and operation codes. General-purpose processors or special hardware 
dedicated to a particular set of graphics functions did the time-consuming calculations 
in other raster systems. The Pixel Planes system was a general-purpose raster engine, 
and it was especially powerful when the pixel operations were described in linear or 
planar expressions [20]. 
Fuchs and Poulton’s 1981 paper, “Pixel Planes: A VLSI-Oriented Design for 3D 
Raster Graphics,” introduced the system, and Henry Fuchs, John Poulton, Alan Paeth, 
and Alan Bell further developed it in their 1982 paper, “Developing PixelPlanes, 
a Smart Memory-Based Raster Graphics System [21].” Fuchs et al.’s 1985 paper, 
“Fast Spheres, Shadows, Textures, Transparencies, and Image Enhancements in Pixel 
Planes [22],” provides the basic algorithm for performing polygon rendering Z buffer 
tests, Gouraud shading, and more elaborate algorithms such as spherical display and 
shading and shadow casting. And a detailed description of a small working prototype 
can be found in Poulton et al.’s report, “Pixel Planes 4 Graphics Engine, Technical 
Report 1985 [23].” 
How it works. The overall Pixel Planes system contained a conventional graphics 
pipeline that traversed a hierarchical display list, computed viewing transformations, 
performed lighting calculations, clipped polygons (or other primitives) that were 
not visible, and did perspective division. The resulting colored-polygon vertices 
in screen coordinates were then “translated” into the form of data (A, B, C) for 
linear expressions together with instructions for the smart frame buffer. An Image 
Generation Controller converted word-parallel A, B, and C plus instructions to the 
bit-serial form required by the EMCs. A video controller scanned out video data 
and refreshed a standard raster display (see ﬁgure). The graphics pipeline and the 
translator were implemented in the prototype system with a single-board general-
purpose processor and off-the-shelf components [24]. 
One of the keystones of the Pixel Planes architecture was the concept of disabling 
or ignoring pixel processors that would be outside of a triangle. That would save 
wasted (non-visible) operations. 
The ring network, shown in Fig. 2.11, was revolutionary and would be mimicked 
years later in the Intel Larrabee design and ATI. A ring network has better perfor-
mance than bus topology, even when the number of nodes is increased. Ring network 
can handle a high volume of nodes and heavy trafﬁc in a network as compared 
to bus topology due to the Token passing principle. Ring topology provides good 
communication over a long distance [25].
The system’s heart was the intelligent frame buffer: an array of custom, VLSI 
processor-EMCs. The chips had three main parts:

44
2
1980–1989, Graphics Controllers on Other Platforms
Fig. 2.11 Pixel Planes’ 
functional organization, the 
ring was the key to 
communications
• A conventional dynamic memory array that stored all pixel data for two 64-pixel 
columns on the screen
• 128 tiny one-bit arithmetic logic units (ALUs)
• A multiplier/accumulator (M/A) that simultaneously generated linear expressions 
for all pixels. 
All the ALUs in the system executed the same micro-instruction simultaneously 
through what is known as single instruction, multiple data (SIMD) processing. All 
memories received the same address (each pixel ALU operated on its corresponding 
bit of data) simultaneously. At each pixel, a multiplier provided the power of two 
10-bit multiply-add operations (M/As). The common part to the pixels in a single 
column was factored out of the M/A (10 stages of X-multiplier and the ﬁrst four 
stages of the Y-multiplier), which reduced the area of the chip used by the M/A 
function. Six stages of the Y-multiplier could be built as a binary tree because Y-
products for a column were closely related. That reduced the cost in the silicon area 
to about 1.2 bit-serial M/A stages per-pixel for the entire linear expression evaluator. 
The prototype chips had 70% of their area devoted to memory and 30% to 
processing circuits; each chip contained two identical 64-pixel modules like those 
shown in Fig. 2.12. In 1987, UNC received a U.S. patent (No. 4,590,465) for the 
system’s basic design; others were awarded later.
Work continued on implementing solid modeling with near real-time rendering 
directly from a constructive solid geometry (CSG) tree. Jack Goldfeather, Jeff 
Hultquist, and Henry Fuchs extended the work in solid geometry and described 
it in their paper, “Fast Constructive Solid Geometry Display in the Pixel-Powers 
Graphics [26].” 
Transparent polygons by pseudorandom screens were also implemented, and 
textured surfaces as grids of tiny polygons were demonstrated on the system in June 
1987. The researchers hoped to include in the hardware the capability to directly 
evaluate quadratic expressions and speed up rendering to render multiple primitives 
at different parts of the screen simultaneously.

2.2 Pixel Planes—The Foundation of the GPU (1980–2000)
45
Fig. 2.12 Pixel planes memory chip block diagram
The ﬁrst full-scale prototype was regularly used in UNC’s computer graphics 
laboratory from 1981 to 1987. The Pixel Planes graphics system is shown in Fig. 2.13. 
The cabinet containing custom hardware is on the left. A small upper rack contains 
the graphics processor, Image Generation Controller, and Video Controller; a large 
lower rack contains the 2048 logic-EMCs. The Host and Pixel Planes monitor are 
on the right.
Under Grants, the (U.S.) Defense Advanced Research Projects Agency, the 
National Institutes of Health, and the National Science Foundation (under Grants) 
supported the Pixel Planes research. 
Virtual Reality: Part of the Plan. 
In the late 1980s, VR was a hot topic. The National Aeronautics and Space Admin-
istration (NASA) and a couple of universities were building head-mounted displays 
that embraced one’s entire visual system. They incorporated small displays, one 
for each eye. Resolution from the 640 × 480 resolution of the IBM video graphics 
adaptor (VGA’s) to the 1024 × 768 resolution of the extended graphics array (XGA). 
It was quickly discovered that the displays needed to be refreshed much faster than 
30 times a second—and that called for the most powerful systems available. 
SGI was the leading CG company at that time. The Pixel Planes system was as 
fast and powerful as the systems SGI offered, but Pixel Planes was considered a 
university project, not a commercial system like those SGI was building.

46
2
1980–1989, Graphics Controllers on Other Platforms
Fig. 2.13 Professor Henry Fuchs manipulates joysticks on the pixel planes system while an asso-
ciate holds a memory board in front (Courtesy of Department of Computer Science, University of 
North Carolina)
The UNC team developed a new design for a commercial version of Pixel Planes 
they called PixelFlow (Fig. 2.14). Charles Grimsdale was the managing director at 
Division Group Ltd. in the UK. Division was one of the most prominent commercial 
VR companies. Grimsdale heard about the project and contacted UNC. At the same 
time, Hewlett Packard (HP) was looking to use its workstations to compete with SGI 
and another leading CG company, Evens & Sutherland.
HP followed what Fuchs and the UNC team were doing and established a relation-
ship with the university to help develop the PixelFlow design. With HP’s help, the 
team made some signiﬁcant improvements, especially in high-speed inter-processor 
communications. 
Division, founded in 1989, had become a publicly held company and bought an 
exclusive license from UNC in 1994 for the PixelFlow design. 
Division would use the latest PixelFlow design based on the Pixel Planes 6. The 
company said at the time that it could render over 5 million Gouraud-shaded trian-
gles per second. Pixel Planes 6 featured Division’s design for photo-texturing that 
produced a system capable of 300,000 photo-textured triangles per second.

2.2 Pixel Planes—The Foundation of the GPU (1980–2000)
47
Fig. 2.14 PixelFlow prototype system block diagram
Gouraud shading, is an interpolation 
method to produce a continuous shading 
of polygon surfaces. It is used to achieve 
a continuous lighting on triangle meshes 
by calculating the lighting at the corners 
of each triangle and linearly interpolating 
the resulting colors for each pixel in 
between. Henri Gouraud published the 
technique in 1971 
Systems were expected to cost $320,000. The company said it would focus its 
business on the mid-range and high-end sectors that offered more signiﬁcant proﬁt 
margins. “There is more money in professional systems,” said Pierre duPont, director 
for marketing at Division. “The margins are better, although we may get some trickle-
down.” 
He foresaw the possibility of Division becoming a supplier to original equipment 
manufacturers (OEMs) looking for technologies such as PixelFlow but said the ﬁrm’s 
ideal price point was the workstation, which cost around $48,000 at the time. duPont 
complained that media coverage tended to stress VR applications for home and games 
because professional applications did not have sex appeal. Division was working to 
get the message across that VR was a serious business tool and that the non-games 
portion was over half of the market and increasing. Unfortunately, the VR market 
was not growing fast enough to sustain the company [27].

48
2
1980–1989, Graphics Controllers on Other Platforms
2.2.1 
HP Acquires Division (1996) 
In June 1996, HP bought a license to the PixelFlow design from UNC and acquired 
Division Group PLC (Bristol, UK), including the PFX graphics system [28]. 
During the previous 2 years, Division developed in collaboration with UNC-
Chapel Hill, the PFX system based on the PixelFlow graphics architecture. The 
design had very impressive speciﬁcations. HP believed it could set new standards 
in scalability, programmability, and performance, which led HP to acquire Division 
after having taken a UNC license. HP expected its ﬁrst PixelFlow-based products 
would be available in 1997. 
HP acquired the assets of the PFX graphics system for $6 million and hired a 
25-person development team. They then became HP’s Chapel Hill Graphics Lab, 
and part of HP’s Workstation Systems Division. 
HP said its Chapel Hill Graphics Lab would create groundbreaking graphics 
products. In July 1997, HP preannounced its plans to introduce a high-end computer 
for the virtual prototyping market, the Visualize PxF1. 
This new system was aimed at SGI’s Onyx2 InﬁniteReality users. According to 
HP, the PxF1 would be capable of rendering graphics at up to 100 million polygons/s, 
running the HP-UX operating system, and be used in rapid prototyping. 
SGI’s ﬁrst reaction to the announcement came from Director of Marketing for 
Advanced Graphics Drew Henry, who said, “It is a 1991 architecture.” He was 
quoted in the San Francisco Chronicle as saying that PixelFlow technology “is not 
adequate to service the needs of the high-end graphics market. Being able to create 
very realistic scenes requires a set of features that this particular system does not 
have.” 
But HP did not plan to stop with the PxF1. The company’s goal was to scale its 
high-end technology to the desktop, and it had plans to develop high-end graphics 
boards for workstations and personal computers. 
HP Visualize PixelFlow (August 1997). 
In August 1997, at SIGGRAPH, HP (Palo Alto, CA) introduced their HP Visualize 
PixelFlow, claiming it was the world’s fastest and most scalable 3D visualization 
system. Built with technology from Division, the system was the result of 18 years 
of government funding and UNC research. At his point, HP’s big bet seemed to be 
paying off. 
The massive system was composed of one to four joined cabinets with larger 
systems possible (if not necessarily affordable). A cabinet was about 0.5 m wide, 
1.5 m deep, and 1.5 m tall. Each cabinet contained up to nine boards. The overall 
organizational structure of the system is shown in Fig. 2.15.
The boards were identical and at least one board had a host interface and one had 
a video-out interface. Figure 2.16 shows the general layout of the board.
Power consumption was estimated at 1,000 W per board (worst case), but the 
actual silicon consumed a peak of only about 700 W.

2.2 Pixel Planes—The Foundation of the GPU (1980–2000)
49
Fig. 2.15 PixelFlow system organization
Fig. 2.16 PixelFlow board

50
2
1980–1989, Graphics Controllers on Other Platforms
The boards performed different functions:
• One or more boards were renderer boards,
• One or more performed shading and texture mapping as shader boards,
• And one or more boards were used as frame buffers. 
The tiled-architecture set of rendering boards rendered one 128 × 64 block in 
parallel, with each board rendering some of the tiles’ polygons. The rendered tiles 
from each board were Z composited together by the backplane. The renderer boards 
did not do texture mapping or per-pixel lighting; instead, a shader board captured the 
composited tile and did the lighting and texture mapping. The resulting tile was then 
sent to a frame buffer board, where it was placed into the frame buffer with other 
tiles to make up the entire image. 
Each board had a generous geometry front-end, an array of SIMD processors, and 
texture mapping hardware. Two 180 Mega Hertz (MHz) HP PA8000 processors did 
the geometry. They had two Mbytes of 256-bit wide 180 MHz cache static random-
access memory and shared a 128 Mbyte DRAM memory system for display lists. 
There were 8,192 100-MHz, 8-bit processors operating in a SIMD array for pixel 
processing, arranged as a 128 × 64 grid. Each processor had an 8-bit ALU, 384 bytes 
of DRAM memory, and the output from a linear evaluation tree distributed across 
the array of processors. The linear evaluation tree generated results of an equation of 
Ax + By + C for each processor in which x and y are the position of the individual 
processor in the array. Each board was essentially a very high-end multiprocessor 
server and a supercomputer-class SIMD computer capable of 800 billion operations 
per second. 
The SIMD array and linear evaluation tree were the key elements of several 
generations of UNC Pixel Planes graphics hardware. They were initially developed 
to investigate compositing architectures for rendering. Compositing architectures, 
sometimes called sort last, transform and render in parallel and merge the parallel 
rendering streams at the video output stage. Sort middle, which SGI and most other 
vendors developed, had a parallel transformation system—a crossbar or bus in the 
middle and a parallel back end. System scalability was limited by the need to grow 
the crossbar as the number of polygons increased continually. The compositing back-
plane used by PixelFlow grew linearly. Each rendering board had additional hardware 
and merged its 50 Gbps output with the 50 Gbps output from the previous board. A 
nine-board system could output a total of 450 Gbps into the compositing network. 
The system was the outgrowth of Professor Henry Fuchs’s efforts at UNC. The 
original Pixel Planes project had been funded since 1980 by ARPA, DARPA, NSF, 
NASA, DOE, NSA, and NIST.1 In later years, Division contributed to the develop-
ment funding until they sold their hardware group to HP, which took over the UNC 
PixelFlow development effort and introduced it as a product. The project had been
1 ARPA— Advanced Research Projects Agency, DARPA— Defense Advanced Research Projects 
Agency, NSF— National Science Foundation, NASA—National Aeronautics and Space Admin-
istration, DOE—Department of Energy, NSA— National Security Agency, and NIST— National 
Institute of Standards and Technology. 

2.2 Pixel Planes—The Foundation of the GPU (1980–2000)
51
a signiﬁcant training ground for some of the best and brightest CG scientists for the 
last two decades. Many lessons of what to do and not to do in hardware design had 
been learned there—sometimes the hard way. 
There were several custom chips in the HP PixelFlow system that used ball grid 
array (BGA), ceramic pin grid array (CPGA), and metric quad ﬂat pack, as listed in 
Table 2.1. 
One of the interesting chips was the EMC memory, which is shown in Fig. 2.17. 
According to HP, the PixelFlow system would deliver unparalleled 3D graphics 
performance and unprecedented photorealism. Users could interactively visualize 
their most signiﬁcant and complex data sets and replace physical prototypes with 
highly realistic virtual models. HP expected the performance to allow manufacturers 
to slash product development time and expense. 
The HP Visualize PixelFlow was a large-scale parallel architecture. It had tens of 
thousands of pixel processors and an internal network with a bandwidth of 12.8 Gbps. 
HP said it was the ﬁrst system to apply rendering power where it was most needed: 
to the visible, on-screen pixels. That, said the company, would result in the real-time
Table 2.1 PixelFlow’s custom chips characteristics 
Chip
Transistors
Die size (mm)
Package
Boards 
RHInO
1.1
15.5 × 14.2
504 CPGA
1 
GeNIe
1.36
11.3 × 11.3
352 BGA
1 
EMC
3.10
14.6 × 11.0
208 MQUAD
32 
IGC
1.36
11.0 × 11.0
352 BGA
1 
TASIC
0.63
11.0 × 11.0
352 BGA
8 
Fig. 2.17 EMC memory 
chip in PixelFlow system 
was segmented 

52
2
1980–1989, Graphics Controllers on Other Platforms
generation of animation-quality images. Previously it would take several minutes or 
more to generate realistic animated images with accurate highlights, shadows, and 
reﬂections. 
Taking SGI head-on, HP’s Vice President (VP) and General Manager (GM) of 
the Personal Systems Group Duane E. Zitzner, said, 
HP today shatters any previous notion of supercomputing-level performance in complex visu-
alization with the world’s fastest 3D graphics engine. This brings our technical computing 
customers one step closer to realizing the vision of virtual engineering and will be a catalyst 
in HP’s strategy to attain leadership across the 3D graphics spectrum. This investment in our 
ﬂagship Unix system products ultimately will beneﬁt users of our entire technical computing 
portfolio — from scalable Convex supercomputers to high-volume Microsoft Windows NT 
workstations. 
The HP Visualize PixelFlow J-Class symmetric multiprocessing workstations 
targeted the high performance mechanical and electrical CAD (MCAD/MCAE) 
engineers in the automotive and aerospace ﬁelds. HP initially developed it for HP’s 
Unix system and the architecture, engineering, and construction (AEC) markets. 
Those users required the maximum available graphics performance for interac-
tively manipulating large data sets. And for viewing they needed greater realism 
for simulation-based design, mechanical design, virtual engineering, styling, and 
digital prototyping. 
Unlike competitive visualization systems, such as Silicon Graphics’ Onyx2 
InﬁniteReality, HP said its Visualize PixelFlow’s performance scaled linearly 
according to system size. A chassis could accommodate up to nine Flow Units. 
Each unit had two PA-8000 processors and 8,192-pixel processors. The system was 
capable of generating 16 million triangles per second. Two chassis, tightly coupled, 
could, HP said, provide twice the performance of a single chassis—more than twice 
the performance of the Onyx2 Inﬁnite Reality system. HP claimed the performance 
could scale linearly to six chassis or more and exceed 100 million triangles/s. 
HP Visualize PixelFlow supported OpenGL and extended it to support the frame-
based architecture. It provided access to new rasterizers, interpolators, and shaders. 
They included the following:
• Phong-shaded textures for realistic lighting of textured surfaces.
• Bump mapping for more realistic 3D surfaces, such as tire treads, pebbled leather, 
or bumpy terrain.
• Dynamic shadow and reﬂection mapping usually associated with computationally 
expensive, non-real-time software rendering.
• Special objects such as accurate spheres and cylinders. 
HP’s DirectModel toolkit for large model rendering could be run on the system. 
With it, an aerospace designer could render, manipulate, and ﬂy through a fuselage 
design. The fast rendering would aid in the search for problems with the designs. 
At the time, several independent software vendors supported the Visualize 
PixelFlow in engineering design and virtual prototyping, including Dassault 
Systèmes, Division, and Engineering Animation, giving the machine a good potential 
market.

2.2 Pixel Planes—The Foundation of the GPU (1980–2000)
53
The HP Visualize PixelFlow was expected to be available during the ﬁrst quarter 
of 1998 and was thought to cost $500,000 for a single-cabinet, nine-board system. 
Several people involved with the Pixel Planes and PixelFlow development were 
surprised when HP introduced it as a commercial product. They pointed out that it 
was designed as a research machine with many design decisions biased in favor of 
ﬂexibility and familiarity instead of cost. Meanwhile, UNC had received funding 
to develop Imageﬂow. It was a system to extend 3D hardware into the hot topic of 
general-purpose image-based rendering. Talisman (Sect. 6.6) was one small, limited 
example. 
HP had planned to challenge SGI’s position as the dominant supplier in the 
high-end graphics workstation market. However, this did not pan out. In January 
1998, 2 years after HP’s acquisition of Division, HP canceled its high-end Visualize 
PixelFlow graphics subsystem. HP had planned to launch the system in January to 
attract customers from SGI. Instead, the promising Chapel Hill, UNC R&D unit shut 
down. HP invested over $6 million in license fees and several million more devel-
oping it. But HP could not anticipate the arrangement between Microsoft and SGI 
to create a new OpenGL-based graphics architecture called Fahrenheit. Microsoft 
had been in discussions with HP to develop a new PixelFlow-based DirectModel 
graphics system. Fahrenheit also superseded Microsoft’s Talisman rendering tech-
nology that had roots to the Pixel Planes technology HP was using. Trying to save 
face, HP said at the time its existing C240 HP-UX Unix workstations provided just 
as much graphics performance as the Visualize PixelFlow engine it demonstrated 
at SIGGRAPH 1997 in August. HP said it would use the PixelFlow technology in 
other, more mainstream graphics products. 
The innovative 3D graphics system was based around having each pixel on a 
screen managed by a dedicated processor. The chip-level PixelFlow had up to 8,192 
or 16,384 processing elements, with the potential for far more, in a single chip. 
As previously mentioned, the development was a collaboration between HP, UNC, 
and the UK-based Division Group. HP’s decision to withdraw PixelFlow from the 
visualization market propelled Division Group’s plans to get out of the hardware 
market and into CAD/CAM (computer automated manufacturing) visualization. 
That decision caused a split between the four company founders, with Ray 
McConnell and Phil Atkin leaving the board and creating PixelFusion through a 
management buy-out (MBO) and taking a license for the 3D graphics hardware that 
was said to be 100 times faster than currently available technologies. 
Division and Ivex Corp began shipping Pixel Planes 5 custom chips in 1994, and 
in September 1997, the Pixel Planes 5 system was retired from service at UNC. 
PTC Acquires Division. 
Division originally developed and marketed toolkits that facilitated the implemen-
tation of VR visualization software and Waltham, Massachusetts-based Parametric 
Technology Corp. (PTC) had been an investor in the company. On January 21, 1999, 
PTC said it would acquire the Division Group. The deal would be worth $48 million

54
2
1980–1989, Graphics Controllers on Other Platforms
in cash and stock, a large sum for the day and over 4.5 times Division’s revenues. 
The company changed its focus 2 years before the acquisition and started selling a 
virtual prototype, and a visualization package called dV/MockUp, and that attracted 
PTC. 
In September 1998, Division had announced the planned acquisition of Object-
Logic, a small software ﬁrm in San Diego. This deal closed in March 1999. PTC 
subsequently offered that company’s primary application as dV/ProductView. This 
latter package enabled users to view, markup, and circulate product data generated 
on a wide range of CAD systems with conventional web browsers. 
2.2.1.1 
PixelFusion (1998) 
In January 1998, Division Group sold its PixelFlow and Pixel Planes parallel 
processing chip technology to PixelFusion. PixelFusion was a new company in 
Bristol formed by a managed buy-out by Division engineers and managers with 
engineers from SGS Thomson and Inmos who had worked on Inmos Transputer 
projects. 
The transputer was an innovative and 
pioneering microprocessor from the 1980s, 
with integrated memory and serial 
communication links, intended for parallel 
computing. It was designed and produced by 
Inmos, a semiconductor company based in 
Bristol, United Kingdom. The design 
influenced several later processors including 
some GPU designs. Ex Inmos employees 
The founders of PixelFusion intended to develop a single chip using concepts 
from the Pixel Planes and PixelFlow architecture. The developers said they had 
been waiting for the technology to do it. Although Pixel Planes could have been 
implemented in 350 nm technology, the part would have been too large to be prac-
tical. Therefore, they waited for 250 nm technology before starting the integrated 
subsystem on a chip. The developers thought they would achieve outstanding polygon 
performance with SIMD processing and that the performance would have been 
squeezed with any technology larger than 250 nm [29]. 
The PixelFusion team made architectural modiﬁcations from what they learned 
from Pixel Planes and PixelFlow. The designers also took some HP chips and ran 
them on an add-in board; however, all the 3D complexity was in software. They did 
not use any of the ﬂow ideas for composition. 
They hoped to have a single chip ﬁnished by the following year. Ian Lazenby, the 
managing director, said using software and a SIMD array with a highly tuned ALU 
was a more efﬁcient way to get a product to market than hard-wired algorithms.

2.2 Pixel Planes—The Foundation of the GPU (1980–2000)
55
The SIMD processors were 8-bit ALUs, and the designers said there would be 
many thousands of them on one chip. The basic design called for integrating thou-
sands of ALUs on one chip paired with its memory. Each processor would have 
at least 512 bytes of memory. The processors would be interconnected on a point-
to-point basis in a daisy-chain fashion using the Fuzion bus developed for Pixel 
Planes. The frame store would also be used for texture images in a uniﬁed memory 
architecture (UMA) conﬁguration (like SGI’s O2). 
The PixelFusion design had a close coupling between the ALU and memory, 
a requirement with a SIMD architecture. The PixelFusion SIMD design was so 
demanding level of DRAM cell access not ordinarily available in embedded DRAM 
designs at the time it was required. The United Manufacturing Company (UMC) 
Group in Taiwan won the fabrication contract (beating out dominate TSMC—Taiwan 
Semiconductor Manufacturing Company). That put UMC in a leading position in 
advanced embedded DRAM. 
The PixelFusion chip had an accelerated graphics port (AGP) bus interface and 
multiple high-speed (600 MHz) Rambus interfaces between the DRAM, processors, 
and controllers; one would be not enough to get the data in and out of the chip. The 
chip also had an internal Fuzion bus to control the peripheral functions on the chip 
and the chip itself. The programmable microcode on the chip deﬁned the personality 
of the chip. The initial design had 50 million transistors on one chip. 
The PixelFusion design promised a level of performance not yet seen in other 3D 
chips:
• Processing performance at multiple TFLOPS (1 × 1012) Ops/s (8-bit Add or 
Multiply) Levels
• Internal memory bandwidth at multiple TFLOPS (1 × 1012) Bytes/s Levels
• Software-based architecture, which included arbitrary algorithms and precision 
in units of eight bits
• Performance limited by die size, not architecture. 
The result from a performance standpoint was signiﬁcant. In departure from 
a typical marketing presentation, the company expressed minimum performance 
instead of maximum to underscore their potential. The performance includes the 
following:
• Minimum peak polygon rates 
– Twenty-ﬁve pixels, four sample anti-aliased, z-buffered 
– Eleven million polygons/s
• Minimum sustained polygon rates 
– 1 million polygons/s 640 × 480 at 60 Hz 
– Multiple light-sourced, Phong shaded, bump-mapped 
– Twenty-ﬁve pixels, four sample anti-aliased, trilinear. 
The feature set was equally as impressive:

56
2
1980–1989, Graphics Controllers on Other Platforms
Anisotropic texture ﬁltering
Bilinear resampling of data in UMA 
Classic texture MIP mapping
Massive per-subpixel blend/lerp performance 
Mirrors, environment mapping
Multi-pass rendering 
Procedural textures, multi-sample
Scaling and warping 
Transparency
Trilinear and bilinear ﬁltering 
3D textures
Anti-aliasing as standard 
Arbitrary numbers of subpixel stencil buffers
Math performed at the subpixel level 
Real Bump mapping
Real Phong 
Shadow and light volumes
Soft selection of pixel subsamples 
Volumetric rendering 
The designers said the systems would execute two terra-ops in one chip with an 
internal clock rate of 200 MHz. It had an internal lookup table and digital-to-analog 
converter (LUT-DAC) display output. The chip would be in a 300-pin BGA package, 
and the designers hoped to offer a core that would sell for less than $35. Atkin said 
that products that used the F100 (Pixel Flow’s name for their proposed GPU) would 
be competitive with 3Dlabs’ Glint board pricing—i.e., ~$3,000. 
The company was initially self-funded, and then founder and Chief Executive 
Ofﬁcer (CEO) Ian Lazenby brought an additional £5 million from outside investors 
(angels) into the 17-person company. PixelFusion also opened ofﬁces in Silicon 
Valley. By the end of the year, there would be 40 people working on this innovative 
design, said Lazenby. 
“The concepts, however,” commented acting Chief Marketing Ofﬁcer Ilene Sterns, “are not 
restricted to just graphics display [30].” 
The designers and their enthusiastic investors saw applications for the SIMD archi-
tecture in MPEG-encoding and decoding applications, HDTV, and other processor-
intensive applications. Sterns said PixelFusion would offer information on the new 
design at the upcoming Microsoft Windows Hardware Conference (WinHEC) in 
Orlando, FL. The company’s business model was like many new start-ups in those 
days—intellectual property (IP) licensing—like Imagination, Rambus, and others. 
The founders initially said they did not want to build chips. 
Meanwhile, back in the U.S., in the summer of 1998, HP and UNC had two 
prototype machines up and running in support of application development at HP’s 
Fort Collins. Fuchs, Steve Molnar, and John Eyles, the hardware designer team 
members at UNC, produced custom chips and boards. 
Other commercial versions of PixelFlow were under development by patent 
licensees Integrated Device Technology (IDT) and, of course, PixelFusion. 
In late 1999, PixelFusion launched a private placement of 1.3 million shares at 
£7.5 per share to gain £10 million of ﬁnancing. Their ﬁrst chip, called Fuzion 150, was 
due to tape out shortly. It was supposed to be available in systems by the mid-2000 
[31].

2.2 Pixel Planes—The Foundation of the GPU (1980–2000)
57
The Precursor of the Modern GPU. 
The company’s business strategy would be fabless semiconductor development and 
IP licensing. PixelFusion was distinguished by its use of a SIMD parallel architecture 
in a single chip. The pixel processor and memory were integrated and replicated many 
times in a single chip just as modern GPUs are today. That was the precursor of the 
modern GPU. It provided the foundation for a unique chip from other suppliers that 
were used in 3D at the time. 
The architecture was adaptable and could be used in a variety of applications. Such 
applications (as suggested in the company’s investment offering) included digital 
signal processing, graphics and imaging as well as video, and high-speed printers. 
The company said the chip could be used as a coprocessor with various processors. 
PixelFusion announced Number Nine would develop an AIB with the Fuzion 150 
for the PC workstation market. PTC was a shareholder in PixelFusion (a result of 
the acquisition of the Division software), which suggested the chip would enter the 
CAD market. The offering indicated that the company intended to go public as soon 
as reasonably practicable. 
Industry analyst John Latta observed at the time, 
“When the Fuzion chip enters the market, it will be the ﬁrst large market silicon expression of 
the UNC PixelPlanes PixelFlow architecture. Yet, PixelFusion has signiﬁcantly extended this 
architecture with its own IP. Although the ﬁrst implementation will be focused on the high-
end 3D market, which is overpopulated with players, it is the application of the architecture 
in other markets that holds the long-term potential [31].” 
This statement implies the company had to develop new market segments. As with 
previous designs employing parallel architectures, the critical factor in their use was 
the developers’ tools. Executing a design-manufacturing plan well and getting the 
ﬁrst chip out was vital. Still, the company also had to demonstrate how its architecture 
could be utilized in markets well beyond 3D. 
Completion dates were missed, software was late, things slipped, as they always 
do, and the money started to run out. 
In February 2000, PixelFusion’s board appointed Ilene Sterns as its CEO and to 
its board of directors. Sterns replaced Ian Lazenby, who had joined the company at its 
founding. Lazenby was instrumental in raising initial funding for the company and 
establishing critical relationships in the industry. The Number Nine relationship was 
especially key. The company was supposed to be PixelFusions’ ﬁrst add-in board 
customer. However, Number Nine fell on hard times, and S3 bought the company. 
The ofﬁcial word from PixelFusion upon the appointment of Sterns was that the 
company’s business plan encompassed many market sectors, including the develop-
ment of an IP Licensing division for which there were comparable UK models such 
as ARM Holdings plc. 
As for the chip, Sterns said the company was very close to tape out: 
We are now developing strong strategic partnerships and producing market-driven products. 
In the past couple of months, we have expanded our market focus to include video/broadcast 
and have attracted a number of prestigious customers.

58
2
1980–1989, Graphics Controllers on Other Platforms
When the company announced the Fuzion 150 in November 1998, it said it would 
ﬁnish tape out in Q2 1999 and have the ﬁrst parts by Q3. The company was 2 years 
late, and the writing was on the wall. Nvidia had introduced its GPU in late 1999, 
and ATI quickly followed, and both companies were on their second generation by 
the time PixelFusion released their ﬁrst generation. 
Nonetheless, Sterns raised another £10 million in funding from private investors 
for the company. In total, PixelFusion had raised £25 million since April 1997. 
2.2.1.2 
The End? 
For the second time in a year, PixelFusion replaced its chief executive, with Ilene 
Sterns stepping down as managing director after 9 months. The company said it was 
looking for a new CEO. 
“Ilene had a graphics background, and because of our change in direction, it 
was felt it would be appropriate to have someone in that area,” said a company 
spokesperson. 
One of the ﬁrm’s founders, Phil Atkin, also resigned. As a principal engineer, 
he engineered the Pixel Planes and PixelFlow technology from UNC to be mass-
produced. Akin and Sterns would later marry. 
The ﬁrm’s ﬁrst graphics chip, the Fuzion 150, was being used for development 
purposes only. 
“We had no ﬁrm contracts placed for the F150. It is possible we might not commer-
cialize it,” Tom Beese, PixelFusion’s former VP of marketing and acting CEO, told 
Electronics Weekly [32]. 
The company hoped to use its massively parallel processor technology to power 
40-Gbit/s communications systems. However, the change in focus and the complexity 
of the new ﬁeld would mean a delay in reaching production. 
“That is one of the things we had to consider. Looking at 40Gbit solutions, we are 
looking at a horizon that is not immediate,” said Beese. “We believe the real need 
for true 40Gbit will occur in late 2002 and early 2003.” 
Although PixelFusion still had substantial funds, having raised over £34 million 
in the past 2 years, Beese said more might be required. 
In May 2001, PixelFusion decided to move into the networking sector and return 
to being an IP supplier after abandoning the graphics chip market (Fig. 2.18).
The company renamed itself ClearSpeed and said the processor array that was 
the heart of its Fuzion graphics controller had been redesigned to handle network-
processing tasks such as packet routing and classiﬁcation. 
And so, the saga of the Pixel-Plains project ended. ClearSpeed sold some network 
chips until 2009 and then faded away. 
Had PixelFusion met its original schedule, it would have been the producer of the 
ﬁrst GPU, which would have been a ﬁtting and proper conclusion for the far-sighted 
design of Pixel Planes some 20 years earlier (Fig 2.19).
The 1992 SIGGRAPH Computer Graphics Achievement Award was presented 
to Dr. Henry Fuchs for his contributions to high performance, parallel display

2.2 Pixel Planes—The Foundation of the GPU (1980–2000)
59
Fig. 2.18 The long 30-year trail (and tale) of pixel planes
Fig. 2.19 Dr. Professor 
Henry Fuchs, the father of 
the GPU (Courtesy of 
Department of Computer 
Science, University of North 
Carolina)
architecture. He was a pioneer who recognized the importance of parallelism for 
graphics processors and provided leadership to achieve a practical implementation 
of massively parallel high-speed display processors—Pixel Planes.

60
2
1980–1989, Graphics Controllers on Other Platforms
2.3 Processor per Polygon—The Demetrescu Caltech 
Architecture (1980) 
In 1979, Caltech University graduate student Stefan Demetrescu began work on his 
master’s thesis. His idea was to create a processor for each polygon. Some years 
later, he was asked if he knew about Fuchs’s work at UNC and the Pixel Planes 
process because they seemed to have a similar goal. He smiled broadly and said he 
was fully aware of that architecture. “The two architectures are quite different,” he 
said (Fig. 2.20). 
The Demetrescu Caltech architecture is a processor per-object system consisting 
of a long pipeline of high-speed processors implemented in VLSI, each responsible 
for only one 3D object. The pixels ﬂow through these processors in real-time, and 
the ﬁnal image emerges complete at the far end, ready for transmission to a display. 
The Fuchs UNC architecture differs in that it is a processor per-pixel-group design. 
It consists of a smart frame buffer that allocates computing for each small group of 
pixels rather than per object. The 3D objects get presented to the processors, and 
they modify the pixels for which they are responsible in parallel. 
The two researchers’ solutions were independent but driven by the same goal— 
complex polygon-based image systems needed more real-time computing power, 
and the quest has never ended. 
The PixelFusion concept [33], which has been cited many times, described an 
innovative architecture for rapidly rendering many polygons using a distributed 
Z buffer algorithm. The system employed a pipeline of custom processors, each 
responsible for a single polygon. 
A good summary and comparison of the Demetrescu and Fuchs architectures can 
be found in Chap. 19 of Foley and van Dam’s Computer Graphics, 2nd edition [34]. 
It is co-authored by none other than Henry Fuchs himself. The Fuchs Pixel Planes
Fig. 2.20 Stefan 
Demetrescu (Courtesy of 
Lasergraphics) 

2.3 Processor per Polygon—The Demetrescu Caltech Architecture (1980)
61
(page 894), Demetrescu Scan Line Access memory architecture (page 897), and his 
Caltech processor per-object architecture are described on page 900. 
Demetrescu’s design had the desirable feature of generating pixels serially as 
needed for the raster scan, eliminating a frame buffer. Memory was a precious 
commodity in those days and one of the drawbacks to robust computer graphics 
solutions. Each pixel in all objects potentially visible competed among themselves, 
favoring the object closest to the observer. It would win and be displayed. 
The surface of each object got assigned to a special purpose processor for each 
frame. The processor consisted of a surface processor and a comparator processor 
(see ﬁgure). Each surface processor generated a data packet for each pixel—the 
color/intensity (i) and the distance from the observer (Z) independently of all other 
processors. The comparator accepted two sets of (Z, i) pairs and selected the (Z, i) 
pair with the z closest to the observer. Thus, for each pixel packet, the (Z, i) of the  
front surface was available at the output of the last comparator. Demetrescu described 
the process as processors competing. 
Demetrescu and his advisor, Dr. Danny Cohen, presented the design at the 1980 
SIGGRAPH Conference Panel on Trends on High Performance Graphics Systems 
and showed the block diagram in Fig. 2.21. 
The packets moved through the pipeline in raster scan order so that each chip 
knew the X, Y address of the current pixel.
Fig. 2.21 Cohen and Demetrescu’s pipelined polygon architecture 

62
2
1980–1989, Graphics Controllers on Other Platforms
Fig. 2.22 Competing surface processors acted much like a dataﬂow machine 
If the set-up values got loaded into all the polygons fast enough, and if each 
processor could complete its task relative to a pixel in one pixel’s video refresh time, 
the system could run in real-time. Years later, in 1986, Demetrescu told Fuchs that 
the chip was designed and built but not debugged [35]. Shown in Fig. 2.22 is the data 
ﬂow of the processor. 
Demetrescu’s concept was so intriguing and inspiring that others built upon it. 
Richard Weinberg [36] proposed an extension to handle anti-aliasing. In that scheme, 
the pipeline would pass color and Z values for a pixel and a sequence of polygon parts 
visible within the pixel area. If a polygon processor found its polygon to be partially 
visible in the current pixel, it would add the portion of its polygon lying within the 
pixel to the sequence of inputs it received for the current pixel. Also, it would cull 
from the list any polygon parts that became obscured by its polygon. The processor 
would then output all polygon parts still visible at that pixel. That enhancement would 
signiﬁcantly increase the amount of data passing through the pipeline and the work 
required from each polygon processor. At the end of the pipeline, a ﬁlter processor 
would appropriately combine all visible portions of polygons for each pixel and, 
therefore, calculate a reasonable anti-aliased image. 
In addition to being brilliant and energetic, Demetrescu was in the right place at 
the right time. 
While in high school, he was fortunate enough to work for a physics professor 
at UC Irvine. He wrote graphical computer programs to teach undergraduates about 
physics. Visual programs back in the mid-1970s were quite the novelty. At that time, 
there were no PCs, so the graphical programs ran from a time-shared mainframe. 
Demetrescu oversaw the program that taught undergraduates about Einstein’s 
theory of special relativity. As a result, he got to do a lot of computer graphics in 
high school before attending Caltech. “Caltech was a fantastic place for computer 
graphics,” said Demetrescu. 
His thesis advisors were the heavyweights in early computer graphics: Jim Blinn, 
developer of the Jet Propulsion Laboratories (JPL) Voyager ﬂy-by animations who 
later developed many computer graphics algorithms; Ivan Sutherland, the cofounder 
of the legendary computer graphics company Evans and Sutherland, which did 
pioneering work in real-time hardware and accelerated 3D computer graphics; and

2.4 The Geometry Engine (1981)
63
Danny Cohen, who developed the ﬁrst real-time visual ﬂight simulator on a general 
purpose computer. “It was great working with these guys!” he added. 
Demetrescu subsequently went on to Stanford for his Ph.D. His thesis was the 
invention of an intelligent DRAM with built-in processing. It allowed for the very 
fast rasterization of polygons within the memory itself, thus sidestepping the problem 
of the von Neumann bottleneck of limited bandwidth between the memory and the 
processor, which still plagues us to this day in GPU and CPU architectures. 
Demetrescu’s Scan Line Access Memory was subsequently patented and licensed 
by Stanford to AMD’s DRAM division. Sadly, AMD closed their entire DRAM 
division before commercializing the concept [37]. 
Demetrescu went on to found Lasergraphics, which makes Motion Picture Film 
Scanning Systems. 
2.4 The Geometry Engine (1981) 
The chip that launched an industry. 
Although not really a graphics chip because it did not directly manipulate any pixels, 
the Geometry Engine, introduced in 1981, was a VLSI breakthrough [38]. 
The Geometry Engine was a special purpose processor with a four-component 
vector processor, a ﬂoating-point processor for accomplishing three basic opera-
tions in computer graphics: matrix transformations, clipping, and mapping to output 
device coordinates. Developed by Jim Clark and Marc Hannah at Stanford University 
around 1981, it was the ﬁrst dedicated vertex processor for what has since become 
a commoditized element in contemporary GPUs of all types. Clark took the device 
and formed SGI that same year. The company became one of the most famous and 
inﬂuential graphics companies ever. SGI was acquired from bankruptcy by Rackable 
solutions in 2009, and in 2016, HP acquired Rackable/SGI, thus ending the era of 
SGI, but not the legend (Fig. 2.23).
Clark wrote an expository essay in early 1980 on the nMOS semiconductor capa-
bility and its use in VLSI designs. In Clark’s words, “The system is designed to 
perform three of computer graphics’ very common geometric functions. A single chip 
type is used in twelve slightly different conﬁgurations to accomplish 4 × 4 matrix  
multiplications; line, character, and polygon clipping; and scaling of the clipped 
results to display device coordinates. This chip is referred to as the Geometry Engine 
[39].” 
Geometry transformation is based on dot products as part of matrix multiply 
(Fig. 2.24).
The Geometry Engine (shown in Fig. 2.25) was a four-component vector function 
unit that allowed simple operations on ﬂoating-point numbers. It was composed of 
four identical function units, each of which had an 8-bit characteristic and a 20-
bit mantissa. It accomplished the operations with a straightforward structure that 
consisted of an ALU, three registers, and a stack. The basic unit could do parallel

64
2
1980–1989, Graphics Controllers on Other Platforms
Fig. 2.23 James Clark 
(Courtesy of IEEE 
Computer)
Fig. 2.24 Dot product
adds, subtracts, and similar two-variable operations on either the mantissa or the 
characteristic. Because one of the registers could shift down and one could shift up, 
it could also do multiplies and divides at the rate of one multiply or divide step per 
micro-cycle. The 12-chip system consisted of 1,344 copies of a single bit-slice layout 
that composed the ﬁve elements. Four pins on the chip told the microcode which of 
the twelve functions to do according to its position in the subsystem organization.
The key to the design was using design techniques advocated by Introduction to 
VLSI Systems (Mead and Conway 1980) [40]. Because that book was the author’s 
ﬁrst exposure to nMOS circuit design, it signiﬁcantly inﬂuenced the methodology 
used in arriving at the architecture. The periodic structures advocated in the book are 
evident at several levels of the organization of the chip(s), and the timing methodology 
advocated by Charles Seitz [41]. It was fundamental to the system.

2.4 The Geometry Engine (1981)
65
Fig. 2.25 Three basic operations performed by a graphics system: transformation, clipping, and 
scaling
In Clark’s paper, he describes it this way: “There are three geometric operations 
that almost every graphics system must do. Figure 2.25 illustrates these three func-
tions. The ﬁrst one is transformations. Typically, objects or picture subroutines are 
deﬁned using such primitives as lines, characters, or polygons.” 
The Geometry Engine was a four-component vector function unit whose archi-
tecture is best illustrated by the chip photograph shown in Figs. 2.26 and 2.27. Each 
of the four function units along the bottom two-thirds of the photo consists of two 
copies of a computing unit, a mantissa, and characteristic. The chip also had an 
internal clock generator at the top left corner and a microprogram counter with a 
push-down subroutine stack, shown at the top right. The upper third of the chip 
was the control store, which held the equivalent of 40k bits of the control store.

66
2
1980–1989, Graphics Controllers on Other Platforms
That control store contained all the microcode to implement the instructions and 
ﬂoating-point computations [42]. 
Clark began designing his Geometry Engine in June 1979 and had a prototype in 
early December. As illustrated in Fig. 4.23, the chip had 40,000 transistors and was 
fabricated in a 3,000 nm VLSI process [43]. 
After releasing his landmark paper in Lambda, Clark and Charles Kuta, Kurt 
Akeley, David J. Brown, and Abbey Silverstone founded SGI in 1981.
Fig. 2.26 A block diagram 
of the Geometry Engine 
corresponding to the photo in 
Fig. 2.27 
Fig. 2.27 Photograph of the 
Geometry Engine (Courtesy 
of ACM 
0-89791-076-1/82/007/0127) 

2.6 SGI’s Personal Integrated Raster Imaging System (IRIS) Workstation …
67
Fig. 2.28 Matrox SM 640 
was the ﬁrst 3D PC AIB and 
used SGI’s Geometry Engine 
(Courtesy of Matrox) 
2.5 Matrox SM640 with Geometry Engine (1987) 
The Geometry Engine was a breakthrough in size and performance. Matrox, the 
pioneer graphics AIB company in Montreal, would put it to work for PCs. Having 
introduced their ﬁrst PC graphics third party board in 1978 [44], Matrox was the 
ﬁrst company to offer a single 3D AIB, the SM 640, in 1987. Matrox built 2D 
graphics boards for the PC and adapted SGI’s Geometry Engine chip on a second-
layer mezzanine board to handle the 3D work, as shown in Fig. 2.28. 
The product was not a commercial success because of the limited number of 3D 
applications available for the PC. The expected migration of minicomputer and work-
station applications to the PC took far longer than Matrox or anyone else expected. 
But it lit up the imagination of all the AIB suppliers and future ones to come. 
2.6 SGI’s Personal Integrated Raster Imaging System 
(IRIS) Workstation (1988) 
A few years later, SGI employed the Geometry System board in a workstation called 
the IRIS, which consisted of the following:
• A processor/memory board with the Motorola 68000 and 256k bytes of RAM, 
expandable to 2M bytes. The 68000 microprocessor executed instructions in the 
onboard memory at 8 MHz. The memory was mapped and segmented for sixteen 
processors. Additional memory got accessed over the Multibus at normal Multibus 
rates.

68
2
1980–1989, Graphics Controllers on Other Platforms
• A Geometry Subsystem, with Multibus interface FIFOs at the input and output of 
the Geometry System and from 10 to 12 copies of the Geometry Engine
• A custom 1024 × 1024 Color Raster Subsystem with high performance hardware 
for polygon ﬁll, vector drawing, and arbitrary variable-pitch characters. The hard-
ware and ﬁrmware provided color and textured lines and polygons, character clip-
ping, color mapping of up to 256 colors, and selectable double- or single-buffered 
image planes.
• A 10-megabit Ethernet interface board 
Figure 2.29 is a photo [44] of the SGI IRIS system, which was remarkably compact 
for the time and astonishingly powerful. 
The ﬁrst systems had an IRIS graphics option featuring the 6 MHz Geometry 
Engine. From 1985, IRIS 2000 systems featured the enhanced IRIS graphics option, 
including the 8 MHz Geometry Engine. The geometry engine chip was just a compo-
nent on the Iris AIB, and the AIB was just a subsystem within the workstation. A 
geometry engine was not a GPU or a graphics controller.
Fig. 2.29 SGI’s IRIS 2000 graphics workstation, circa 1985 (Courtesy of https://wiki.preterhum 
an.net) 

2.8 NEC’s µPD7220 Pioneering Graphics Display Controller (1982)
69
2.7 SGI’s IrisVision AIB (1988) 
In 1988, SGI introduced the MIPS-based workstation computer, the Personal IRIS 
series. It used IBM’s MicroChannel bus. That was a big breakthrough for IBM to get 
SGI to agree to use it. And SGI saw it as a pathway into the PC market. 
The Geometry System was a powerful computing system for graphics applica-
tions. It combined several useful geometric computing primitives in a custom VLSI 
system. It was a system with a future because of its scalable nature. The system got 
implemented on a single, half-million transistor, integrated circuit chip within less 
than 5 years with a correspondingly reduced cost and increased speed. 
Today Clark’s Geometry Engine’s principles can be found in every new GPU, and 
hundreds of millions are in use. 
You will read about the Iris vision again in the 3dfx story. 
2.8 NEC’s µPD7220 Pioneering Graphics Display 
Controller (1982) 
The ﬁrst graphics controller chip. 
Integrated graphics chips, controllers, and processors changed the course of the 
CG industry. As they gradually assimilated important components and functions 
they took advantage of denser and less expensive memory and gained performance 
and price efﬁciencies. They were disruptive devices, and in addition to changing 
how things got accomplished, the application of those chips made many companies 
successful. Ironically, almost all of those companies are gone now [45]. 
In 1982, NEC changed the emerging computer graphics market landscape just as 
the PC was introduced, signiﬁcantly changing the heretofore specialized and expen-
sive CG industry. NEC Information Systems, the U.S. arm of the Nippon Electric 
Company (now NEC), introduced the µPD7220 Graphics Display Controller (GDC). 
The project began in 1979, and NEC published a paper on it in 1981 at the IEEE 
International Solid-State Circuit Conference in February 1981 [46]. 
Before the µPD7220, graphics display systems came in two classes: high-end 
CAD systems connected to big IBM and Digital Equipment Corporation mainframes 
and low-end microcomputer systems based on the ﬂedgling Intel 4004 CPU, which 
was the precursor of the PC. 
VLSI technology was just getting rolling, and devices with astounding numbers 
of transistors—30,000 to 40,000—were being built with the precursor of comple-
mentary metal-oxide-semiconductor (CMOS), N-type metal-oxide-semiconductor 
(NMOS). CMOS was expensive and had larger feature sizes. An NMOS chip could 
have gates as small as ﬁve microns (5,000 nm), but they paid for that with power 
dissipation challenges. The µPD7220, running on 5v, drew 1.5 W and used a 40-pin 
ceramic package. The single chip, shown in Fig. 2.30, replaced a whole board of ICs.

70
2
1980–1989, Graphics Controllers on Other Platforms
Fig. 2.30 NEC’s µPD7220 was the ﬁrst integrated graphics controller chip 
The chip incorporated all the cathode-ray tube (CRT) control functions (known 
as the CRTC) and graphics primitives for arcs, lines, circles, and special characters. 
The graphics display controller’s (GDC’s) sophisticated instruction set minimized 
processor software overhead, graphics ﬁgure drawing, and direct memory access 
(DMA) transfer capabilities. It supported a light pen and could drive up to four 
megabits of bitmapped graphics memory, which was a lot for the time. Shown in 
Fig. 2.31 is an illustration of the chip.
Before the µPD7220, every graphics device had its own primitives drawing 
library, with IBM’s 2250 (1974) and Tektronix’s 4010 (1972) being the most popular. 
The µPD7220 established an easy-to-use, low-level set of instructions application 
developers could easily embed in their programs and thereby speed up drawing time. 
GDC Components. 
The system microprocessor used an 8-bit bidirectional interface to control the 
GDC. It also provided access to the FIFO buffer. The command processor then 
interpreted the contents of the FIFO. The command bytes were decoded, and the 
succeeding parameters were distributed to their proper destinations within the GDC. 
The command processor would yield to the bus interface when both accessed the 
FIFO simultaneously.

2.8 NEC’s µPD7220 Pioneering Graphics Display Controller (1982)
71
Fig. 2.31 Layout of the 
µPD7220—notice the (dark) 
RAM area (Courtesy of 
Nikkei)
In addition, the GDC had DMA control circuitry that could coordinate data trans-
fers over the microprocessor interface when using an external DMA controller, as 
shown at the top of the block diagram in Fig. 2.32.
Sixteen-byte RAM storing parameters got used repetitively during the display and 
drawing processes. In character mode, the RAM held four sets of partitioned display 
area parameters; in graphics mode, the drawing pattern and graphics character took 
the place of two of the sets of parameters. 
Based on the clock input, built-in sync logic generated raster timing signals for 
almost any interlaced, noninterlaced, or repeat ﬁeld interlaced video format. The 
generator got programmed during the idle period following a reset. In video sync 
slave mode, it coordinated timing between multiple GDCs. 
The GDC was a great demonstration of VLSI, incorporating dozens of functions 
in one chip. The block diagram of the chip is shown in Fig. 2.32 [47]. 
The chip’s memory timing circuitry provided two memory cycle types: a two-
clock period refresh cycle and the read-modify-write (RMW) cycle, which took four 
clock periods. 
The chip also had programmable zoom capability. A horizontal zoom was cleverly 
produced by slowing down the display refresh rate while maintaining the video sync 
rates. Vertical zoom was achieved by repetitively reading each line the same number 
as the horizontal zoom number. 
The graphics heart of the device was its drawing processor, which contained the 
logic for calculating the position and address of the pixels in the objects. Given 
a starting point and appropriate drawing parameters, the drawing controller, did 
not need any assistance to complete the ﬁgure drawing, which was a signiﬁcant 
breakthrough for an integrated controller of the time. 
The display memory controller’s tasks, however, were numerous. Its primary 
purpose was to multiplex the address and data information in and out of the display 
memory. It also contained the 16-bit logic unit used to modify the display memory

72
2
1980–1989, Graphics Controllers on Other Platforms
Fig. 2.32 Block diagram of NEC’s µPD7220 GDC
contents during RMW cycles, the character mode line counter, and the refresh counter 
for dynamic RAMs. The memory controller apportioned the video ﬁeld time between 
the various types of cycles. 
Raster graphics was coming into its own, and the displays were vector writers 
in the high-end graphics display market. They were very accurate, and one could 
take measurements from the screen. To manipulate lines and other graphics objects, 
a photosensitive light pen was used. It would detect the light from the screen 
and, through x-y timing signals, know where the pen was looking. The µPD7220 
incorporated those features too. 
If two rising edges on the light pen input coincided during successive video ﬁelds, 
the pulses would get accepted as an accurate light pen detection. A status bit indicated 
to the system microprocessor that the light pen register contained a valid address.

2.9 Hitachi ACRTC HD63484 (1984)
73
The GDC did not have a BitBlt function, however. (Bit Block Transfer, a function 
for moving a rectangular block of color data for a rectangle of pixels from the source 
to the display.) 
The NEC PD7220 GDC was the ﬁrst commercially available chip to handle low-
level image generation with help from a processor. It made possible the construc-
tion of frame buffers with much lower parts count and thus much lower cost. For 
example, an early system to use this chip, the Vectrix Corp. (Greensboro, NC) VX 
128, contained a 480 × 672 pixel by three bits/pixel frame buffer, an internal 16-bit 
processor (Intel 8088), serial and parallel ports, separate package, and power supply. 
The VX 128 cost $2,000 in early 1983. 
The PD7220 launched a ﬂurry of under-$10,000 frame buffers and stimulated 
numerous improved versions from other chip vendors, some of which are described 
later. 
The chip quickly became popular and was the basis for several dumb terminals (a 
dumb terminal could not be programmed and just displayed images and text), and a 
few graphics terminals. The controller could support 1024 × 1024 resolutions with 
four planes of color, so some systems employed multiple 7220s to get more color 
depth. In June 1983, Intel released the 82720, a clone of the µPD7220. 
The µPD7220 was produced until 1987, when it got replaced by a newer, faster 
version, the µPD72120. Seeing its success and the emerging market for computer 
graphics, Hitachi and TI also introduced graphics processors a few years later. 
The successor to the µPD7220 was the extremely popular Hitachi ACTRC 
HD63484, which eclipsed the pioneering 7220 in unit shipments, only to be super-
seded too by the powerful TI TMS34010, and graphics controllers from S3 and 
others. Moore’s law and the CG industry are unrelenting. 
2.9 Hitachi ACRTC HD63484 (1984) 
The second graphics processor. 
With the advent of large-scale integrated circuits (LSI) coming into their own in the 
late 1970s and early 1980s, fueling the PC revolution and several other developments, 
came a succession of remarkably powerful graphics controllers. As mentioned at the 
beginning of this chapter, NEC introduced the ﬁrst LSI fully integrated graphics chip 
in 1982 with the NEC µ7220. It was wildly successful in ﬁnding its way into graphics 
terminals and workstations but not PCs built by IBM. However, it did get used quite 
extensively by aftermarket suppliers [48]. 
Hitachi did NEC one better and introduced their HD63484 ACRTC Advanced 
CRT Controller chip in 1984. As shown in Fig. 2.33, it was slightly larger than the 
NEC 7220. It could generate images up to a resolution of 4096 × 4096 in a 1-bit 
mode within a two Mbyte display (frame buffer) memory. The ACRTC also became 
very popular and found a home in dozens of products, from terminals to PC graphics

74
2
1980–1989, Graphics Controllers on Other Platforms
boards like the ELSA workstation AIB shown in Fig. 2.34. However, these chips, 
pioneers of commodity graphics controllers, were just 2D drawing engines with 
built-in font generation. That same year, IBM introduced their enhanced graphics 
adapter (EGA), which became the standard for mainstream PCs with its many clones. 
But companies that wanted high-resolution, bitmapped graphics chose the Hitachi 
HD63484. 
Fig. 2.33 Hitachi HD63484 ACRTC, more functionality and larger than the 7220 
Fig. 2.34 An ELSA workstation add-in board using the Hitachi HD63484, the top row of chips 
are memory (Courtesy of VGA Museum)

2.9 Hitachi ACRTC HD63484 (1984)
75
The LSI HD63484 was built with 2,000 nm CMOS technology and had around 
60,000 transistors (a Motorola 68020 of the time had about 190,000). The ACRTC 
could run at 8 MHz. 
The ACRTC enabled (4096 × 4096 pixels) screen resolutions, eight times bigger 
than HD (1920 × 1080). Although it was only 1-bit deep, it offered a unique (at the 
time) interleaved access mode for ﬂashless displays. If one wanted 16-bit color (which 
was supported), then one would have to drop down to 1024 × 1024 resolution, which 
was still astounding at the time, and only a few monitors could support it. However, the 
super high-resolution monochrome was targeted at the emerging bitmapped desktop 
publishing market. The chip had full programmability of the CRT’s timing signal 
capability for whatever monitor you hung on it. 
The ACRTC could support up to two Mbyte of video RAM and had an asyn-
chronous DMA bus interface that could be mapped to the PC ISA-16, Versa Module 
Eurocard (VME) bus, or the P1014 16-bit busses. According to the company, it was 
optimized for the 68000 MPU family and the 68459 DMAC. With the DMA capa-
bility, it was possible to provide master or slave synchronization to multiple ACRTCs 
or other devices. 
BitBlt. 
The HD63484 was the ﬁrst graphics chip to have a BitBlt engine [49]. BitBlt is an 
abbreviation for bit block (blocks of bits) transfers used to combine several bitmaps 
into one using a Boolean function. 
The Xerox Alto computer was The ﬁrst to use BitBlt. In November 1975, Dan 
Ingalls, Larry Tesler, Bob Sproull, and Diana Merry programmed the BitBlt routine 
in the Smalltalk-72 system [50]. Later, Ingalls developed a redesigned version in 
microcode. 
At least two bitmaps are involved in the operation, source, and destination. A 
third could also be employed called the mask, and sometimes a fourth was used to 
create a stencil. The pixels in each bitmap were combined (bitwise) to a speciﬁc 
raster operation pipeline (ROP), and the result got written to the destination. The 
ROP is a Boolean formula [51]. The Commodore Amiga’s graphics chipset could 
combine three source bitmaps using 256 Boolean functions with three inputs. It was 
quite advanced for the day, especially so in such a low-cost device. 
High-Level Commands. 
The chip offered a high-level command, which reduced software development costs. 
In this way, the ACRTC converted logical x, y coordinates to physical frame buffer 
addresses. It supported 38 commands, including line, rectangle, polyline, polygon, 
circle, ellipse, arc, ellipse arc, ﬁlled rectangle, paint, pattern, and copy. An on-chip 32-
byte pattern RAM got used for powerful graphic environments. Conditional drawing 
functions were available for drawing patterns, color mixing, and software windowing, 
supporting clipping, and hitting.

76
2
1980–1989, Graphics Controllers on Other Platforms
Fig. 2.35 A Force Computer VME SYS68k/AGC-1A add-in board based on the Hitachi HD63484 
chip (Courtesy of Force Computers) [52] 
The ACRTC found its way into platforms beyond the PC, as illustrated by the 
VMEbus-based AIB in Fig. 2.35. The Versa Module Eurocard, as it was also known, 
was used in large systems, workstations, and test equipment that used the Motorola 
68000 processor. The adaptability of the ACRTC to different busses, OS, APIs, and 
central processors was a tribute to its versatility and robustness. 
You could control four hardware windows with the ACRTC, zooming and smooth 
scrolling in vertical and horizontal directions. It also offered the ability to display 
up to 256 colors and a maximum drawing speed of 2 million pixels per second in 
monochrome and color applications, which proved to be helpful in high performance 
CAD terminals and workstations of the time. 
Workstation users could set eight user-deﬁnable video attributes, and it also had 
light pen detection. 
The chip was very popular and got incorporated into several long-lifetime prod-
ucts. To provide a continued supply of ACRTCs, clones of the chip were developed 
using innovASIC’s MILES—Managed IC Lifetime Extension System—cloning 
technology. The block diagram shown in Fig. 2.36 is adapted from the Hitachi 
HD3484 user’s manual [53].
That was in the early days of the PC, and IBM had designed an expansion bus 
architecture that was only 8-bits wide in the original 1981 version of the PC. By 
1984, the bus got extended to 16-bits with the introduction of the PC AT. With that 
came a ﬂurry of AIBs, the ﬁrst generation of which used the NEC µ7220. By 1986

2.10 Truevision (1984–1987)
77
Fig. 2.36 Block diagram of the Hitachi HD63484 graphics controller
there were 88 AIBs, and the Hitachi was displacing the µ7220, appearing in 22% 
of the AIBs. The ACRTC was a breakthrough chip, and by 1988, 194 AIBs were 
offered, and 24% of them had adopted the HD63484. The ACRTC got eclipsed by a 
new, more powerful, and programmable graphics processor, the Texas Instruments 
TMS34010. 
Hitachi made a valiant effort in extending the ACRTC design to a 3D chip, but the 
bandwidth needed, and other issues were too complicated, so they ultimately failed. 
For reasons only known to the management of Hitachi, the company abandoned the 
graphics market, just as it was about to take off. 
2.10 Truevision (1984–1987) 
AT&T used to be into advanced graphics and image processing and many of the 
leading concepts that survive and underpin today’s products were created there. 
Electronic Photography and Imaging Center (EPICenter), an internal spin-off of 
AT&T and cofounded by Carl Calabria.

78
2
1980–1989, Graphics Controllers on Other Platforms
They developed color frame buffers and advanced image processing and editing 
systems. The TARGA (Truevision Advanced Raster Graphics Adapter) was their 
AIB product. Later they developed the VISTA AIB. 
TARGA and VISTA boards were the ﬁrst AIBs for PCs to support what was known 
as High Color or TrueColor displays. Intended for professional computer image 
synthesis and video editing with PCs the resolutions of TGA image ﬁles matched 
the National Television Standards Committee (NTSC) and the Phase Alternation by 
Line (PAL) video formats. 
Not a graphics controller per se’ Truevision established a 24 and 32-bit color 
format which became a de facto standard. Subsequent graphics controllers, AIB, 
and GPUs would advertise they were TARGA compatible or supported the TRAGA 
color format. 
2.11 Tl 34010 (1986) 
The ﬁrst programmable graphics processor chip could have led to the GPU. 
The ﬁrst programmable graphics processor was the TMS34010, introduced in 
1986 by Texas Instruments. It is one of the processors that changed the course of the 
CG industry. 
A couple of years earlier, in 1984, Texas Instruments (TI) launched the TMS4161, 
its VRAM. The TMS34010 and VRAMs are related but not how one might think. 
Karl Guttag, a senior engineer at TI, worked on the TMS9918 Sprite Chip and two 
16-bit CPUs. As a result, he and others at TI could see memory bandwidth was 
a problem in keeping the processors fed with data. The concept of putting a shift 
register in a DRAM and turning it into a VRAM to speed things up was discussed at 
TI, but it was impractical to use in a system due to how it worked. So Guttag’s team 
got together with TI’s MOS Memory group and worked out a deal. Guttag’s team 
would help deﬁne a system to take advantage of the VRAM if the memory division 
would build it. With the memory division’s support, Guttag started the deﬁnition of 
what became the TMS34010Fig. 2.37.
In between the VRAM design and the release of the 34010, Guttag’s team also 
developed the TMS34061, a simple VRAM controller that they could get into 
production much faster than the more complicated 34010 [54]. 
TI offered the TMS340 Graphics Design Kit for designing bitmapped graphics 
systems. The $99 kit included components likely to be used in a graphics system, 
such as TI’s TMS4161 64K by 4 VRAM, the TMS34061 video system controller, 
and the TMS34070 LUT-DAC TI called a color palette chip at the time. 
But the TMS34010 introduced in 1986, the ﬁrst programmable integrated circuit 
graphics processor, was the real prize and would change the trajectory of the PC and 
CG industry [55]. 
The 34010 was a 32-bit processor that included graphics-oriented instructions 
[56]. It could operate as a CPU and graphics controller. It was not a fully-ﬂedged 
GPU as deﬁned in this book because it did not incorporate ﬂoating-point processing

2.11 Tl 34010 (1986)
79
Fig. 2.37 Karl Guttag 
(Courtesy of Guttag)
or support using a ﬂoating-point coprocessor (nor did any other graphics chip then); 
that was later provided by the TMS34020 (a ﬁrst in graphics processing chips). In 
the era of 2D graphics, ﬂoating-point was not a signiﬁcant consideration. It was 3D 
games that later drove ﬂoating-point in consumer PCs. 
TI did much of the architectural design of the 34010 in Houston, Texas, and most 
of the ﬁnal logic design at its Bedford, UK lab. Guttag and Carrell Killebrew, the 
applications and systems engineering manager at TI, got one of the ﬁrst test samples 
from the fab working in Houston at 100 kHz in November 1985. 
When Killebrew got the initial samples, they discovered the chip layout had an 
error in one of the microcode address lines that inverted it. There was no way to ﬁx 
it, so they had to wait for a new silicon version. That happened in the early evening 
of Friday, December 20, 1985. Killebrew received a wafer from the fab and carried it 
to the packaging ﬂoor, where only a few technicians worked. TI was on a mandatory 
two-week-long Christmas shutdown, so everyone had left, but Killebrew had stayed 
hoping to get the ﬁxed silicon samples. The technician running the prototype pack-
aging line refused to package the 34010s because his crew was working overtime 
under contract for a third party and were under orders to do no work for any other 
party. Killebrew talked the lead technician into scribing the wafer. The technician 
then showed Killebrew how to break the die, heat the package and the die, mount the 
die into a package, and do the wire bonding between the die pads and the package 
pads. The technician left, and Killebrew blind packaged all the usable die from the 
wafer. Blind packaging meant none of the dies would get tested; they would get 
tested later by placing them in a development board and seeing if they worked. 
Why did he do all that during the holidays? Because Killebrew had committed 
to IBM’s Workstation Group in Kingston that TI would deliver an AIB by the end 
of the ﬁrst week in January 1986. Somebody had to get chips packaged and start 
testing them in development boards. Killebrew spent those 2 weeks of vacation in a 
lab testing chips. He keeps one of them in his drawer.

80
2
1980–1989, Graphics Controllers on Other Platforms
A week later, in January 1986, Guttag went to California and personally showed 
a working 34010 to Steve Jobs at NeXT. Jobs praised the design, but unbeknownst 
to Guttag, Jobs had already decided on the Motorola 68030. 
In May 1986, shortly after the release of the TI TMS34010, Intel announced the 
82786, which became available in September. It was a graphics controller that could 
use either DRAM or VRAM, but it was not programmable like the 34010. (The 
82786 is discussed in the next chapter.) 
Speedy. 
TI claimed the 40 MHz 34010 was a faster general-purpose processor than the 
popular 25 MHz Intel 80286 in typical graphics applications; the 80286 also used an 
external ﬂoating-point coprocessor. The 34010 had to wait on the host 90–95% of 
the time because Microsoft Windows was structured to pass mostly very low-level 
commands. Texas Instruments Graphics Architecture (TIGA) was TI’s API, created 
before DirectX, Glide, or any other PC graphics APIs. It deﬁned the software inter-
face to the 34010. Using the API, any software written for TIGA would run correctly 
on any TIGA-compliant AIB. Figure 2.38 shows the AIB system organization of the 
revolutionary chip. 
TI initially sold all 34010s at 40 MHz, the baseline design speed, even though 
virtually all early parts passed at 50 MHz. Why only 50 MHz? The product engineers 
had not checked out the test program at a higher rate, and there was a concern that 
the process might not provide a stable yield above 40 MHz. As the product engineers 
gained experience, the test program speed increased to 60 MHz, and the process 
engineers at the fab guaranteed that the process was stable. Virtually all TMS34010s 
ever produced would run at 60 MHz with ample margin, but TI wanted to charge a
Fig. 2.38 The TMS34010’s system block diagram 

2.11 Tl 34010 (1986)
81
premium for anything above 50 MHz (and did). A few customers ﬁgured that out and 
bought 50 MHz parts and then ran them at 60 MHz or higher. Some 34010s could 
run above 70 MHz, and some could go even to 90 MHz. 
The processor had various dedicated graphics instructions implemented in its 
hardware. They consisted of primary graphics functions, such as line drawing, ﬁlling 
a pixel array, and pixel block transfers (BitBlt). 
The 34010 offered BitBlt, plane masking, pixel transfers, transparency, pixel 
processing, multiple-bit pixel operations, Boolean processing examples, and Window 
checking. 
There was also an indirect graphics-oriented x, y addressing mode register. In 
that mode, a register held a pixel’s screen address in x, y form. The mode off-loaded 
the CPU from the time-consuming job of mapping each pixel’s memory address to 
its screen location. The diagram in Fig. 2.39 is adapted from the TMS34010 user’s 
guide [57]. 
Guttag’s team designed the processor from the ground up; it was not a modiﬁcation 
of an earlier design. The 34010 had thirty 32-bit registers split into an A bank and 
a B bank. The A bank registers were general purpose. Applications could use them 
for temporary storage during computation. The B bank registers were specialized 
for graphics and held information such as the location and dimensions of a clipping 
window or the foreground and background colors. 
TI made signiﬁcant advancements in the design by going to thirty 32-bit registers 
rather than the sixteen or fewer found in most machines of the time. That choice
Fig. 2.39 The TMS34010’s internal architecture block diagram 

82
2
1980–1989, Graphics Controllers on Other Platforms
was motivated by the desire to ease assembly-level programming and make time-
critical functions run faster. A large register ﬁle meant all the parameters for the most 
time-critical functions could be kept in the processor, eliminating the thrashing of 
parameters between the register ﬁle and memory. 
The processor’s register-to-register operations could be done in a single cycle and 
occurred in parallel with the completion of previously started write cycles by the 
memory controller. 
The 34010 had a 256B instruction cache (i-cache), which was critical to perfor-
mance. It could hold the inner loop of many basic 2D graphics functions such that 
during pixel drawing, the memory data bus was free for data-only use. Intel has 
claimed that they offered the world’s ﬁrst integrated i-cache on a chip, but the fact is 
that the 34010 was the ﬁrst. 
The 34010 ran at 40 MHz and later 50 and 60 MHz, which was high for the time 
and a credit to TI’s fab. Even so, many OEMs over-clocked the processor to gain 
a little performance differentiation, which revealed the design’s headroom, another 
tribute to it. 
Like other graphics controllers of the time, the 34010 required an external LUT-
DAC. The most popular LUT-DACs at the time were from Brooktree. TI AIB partners 
used the Brooktree LUT-DAC and the TMS34010 with a true color 24-bit frame 
buffer. 
Even though TI designed the 34010 to run an OS like DOS directly, Guttag’s 
team built-in the capability for working as a coprocessor, giving OEMs numerous 
opportunities for applications and product differentiation. An example of it used for 
a Spea workstation AIB is shown in Fig. 2.40. 
During those years, there were three major market forces: high-end PCs, where 
the TMS34010 found many homes; consumer and commercial PCs, where VGA was 
the dominant standard; and a variety of game consoles and arcade machines. TI did 
well in the high-end and arcade game machines markets and several special purpose 
systems in scientiﬁc instruments, avionics, and process control systems. The chip is 
illustrated in Fig. 2.41.
Fig. 2.40 A Spea TI TMS34010-based AIB with a memory at the top, a VGA (clone) chip onboard 
and a TI LUT-DAC (Courtesy of Konstantin Lanzet Wikipedia) 

2.11 Tl 34010 (1986)
83
Fig. 2.41 A photograph of 
the Texas Instruments’ 
TMS3020 Graphics System 
Processor die (Courtesy of 
Pauli Rautakorpi, Wikipedia) 
In 1998 TI released its second generation processor, the TMS 34020, between 
six and 50 times faster in critical graphics operations than its predecessor [58]. The 
34020 ran at 10 MIPS using its 512-byte instruction cache. It could connect to a 
second 34020 or the TMS34082 graphics ﬂoating-point coprocessor, reaching 40 
million ﬂoating-point operations per second. 
TI’s API. 
A few years after introducing the TMS34010, in 1989, TI introduced the TIGA API. 
TIGA was independent of resolution and color depth, which provided a certain degree 
of future-prooﬁng. The API was for high-end graphics and CAD applications. 
TI initially believed that supporting various graphics standards would be sufﬁcient. 
More applications would come to market if they used standard interfaces (such as 
CGI/CGM, GKS, NAPLPS, PHIGS, and Microsoft Windows). However, CGI was 
not widely adopted at that time, and neither was Windows, nor would it be for at 
least another half-decade. It was a classic case of the TI management thinking the 
34010 was the better mousetrap; therefore, the world would come a-running. It did 
not. This bit of myopia (along with VGA compatibility) held the 34010 back from 
broader adoption in the only PC that mattered: IBM and its clones. 
The Video Electronics Standards Association’s (VESA) NEC-inspired Super 
VGA (SVGA) became the de facto standard for PC graphics resolution (and moni-
tors), following the VGA. As a result, AIB suppliers added a VGA chip to their board 
to be compatible with DOS and Windows applications.

84
2
1980–1989, Graphics Controllers on Other Platforms
Despite the ﬂexibility of the TMS34010 and the efﬁciency of TIGA, OEMs 
rejected it as a product for consumer and commercial PCs. Instead, it was adopted 
for high-end AIBs. 
Initially, Microsoft did not support the 34010 in Windows. However, they came 
around because the processor did such an excellent job in display list processing, 
and, via TIGA, it was easier to manage in Windows. At the time, Windows was just 
passing low-level commands. 
Windows was structured to have the host do all the drawing, and that technique 
worked alright with EGA and VGA. IBM’s 8514/A driver did a good job BLTing 
but was not as good as the 34010 for line draws, which was critical to CAD users. 
Nonetheless, Microsoft had an allegiance to IBM and said the 34010 did not perform 
as well as the 8514/A on twenty essential graphics functions. Microsoft had to eat 
their words when it discovered that Microsoft’s Windows memory management of 
fonts caused the problems on the host side. 
Microsoft was opposed to having any intermediary software between an appli-
cation and the hardware other than something Microsoft wrote itself. That was 
essentially the “not-invented-here” syndrome. Only after TI hired Microsoft’s Fred 
Einstein as a consultant on a direct-to-the-hardware driver did Microsoft acknowl-
edge TI’s performance. It was TIGA with very little of Einstein’s input that made 
Windows blazingly fast. That caused Microsoft to realize that Windows would be 
crippled and held back if it did not have device-independent interfaces for graphics 
accelerators. Einstein had Bill Gates’ ear because Gates had been very critical of 
IBM’s 8514 performance on Windows until Einstein bypassed the Windows inter-
face to the 8514 and wrote directly to the register’s 8514 driver. For Microsoft’s 
marketing purposes, TI referred to TIGA as a Windows accelerator. TI also pointed 
out that it needed a game-friendly interface for accelerators. Otherwise, game devel-
opment on Windows would be crippled so severely that game developers would 
revert to DOS (which they did). That was the real genesis of the DirectX interfaces. 
Modern gaming owes a great deal to TI in this regard because IBM 
(CGA/EGA/VGA/8514) never lobbied Microsoft for a device-independent inter-
face, and Microsoft mostly ignored game developers. The only party that pushed 
Microsoft in this regard was TI’s Graphics Department. 
It took a while, but Microsoft came to recognize that AIBs needed two or three 
screens of memory to run Windows applications adequately. Presentation Manager 
(PM), the graphical user interface that IBM and Microsoft introduced in the OS/2 in 
late 1988, required even more. Also, Windows did not handle bitmaps very well at 
the time. Microsoft improved that with Windows 386, which allowed applications 
to run in or out of Windows (allowing various applications to run multiple bitmaps 
simultaneously). 
Finally, Microsoft became a supporter of TIGA and announced that TI-based AIBs 
could get Windows and PM drivers via TIGA as part of the TIGA package. Windows, 
however, could not beneﬁt from a hardware accelerated line drawing engine. At the 
time, Microsoft advised customers not to use AutoCAD with Windows, but users 
did anyway.

2.12 MAGIC—Multiple Application Graphics Integrated Circuit (1987)
85
2.11.1 
TI Epilogue 
If TI had stayed in the market, it would have been obvious to integrate a 2D VGA, 
the LUT-DAC, and the FFP. The company would have evolved into developing the 
ﬁrst GPU as they almost already had it. 
In 1991, Guttag was made a TI fellow. He was also given the National Computer 
Graphics Association (NCGA) Award for Technical Excellence for his pioneering 
work on VRAM. 
By mid-1988, TI sold off its memory business to Micron, focused its attention 
on digital signal processors, and started developing its Open Multimedia Applica-
tions Platform (OMAP) line, which would lead TI to the smartphone market. TI 
discontinued the 34010 and 334020 in the early 1990s. 
Guttag, who joined TI in 1977 right after attending graduate school at the Univer-
sity of Michigan, left TI in 1998 and started several other companies, becoming a 
world-renowned consultant in optics and augmented reality markets. 
2.12 MAGIC—Multiple Application Graphics Integrated 
Circuit (1987) 
MAGIC was an architecture for a Multiple Application Graphics Integrated Circuit, 
developed in the UK by Helen R. Finch, Mark Agate, A. A. Garel, Paul Lister, and 
Richard Grimsdale of the University of Sussex in 1987. The goal was to produce a 
VLSI geometry processor for various graphics applications, from personal computers 
through workstations to real-time ﬂight simulator visual systems. Therefore, essential 
requirements for this processor were ﬂexibility and performance [59]. 
MAGIC was developed as part of the PRISM (Processors for Real-time Image 
Synthesis and Manipulation) project. It was run collaboratively with GEC Research 
Ltd. at Wembley and Singer Link Miles at Lancing [60]. 
The MAGIC geometry processor formed a part of a graphics system based on 
parallel processing being developed at Sussex. The number of MAGIC processors 
used was determined by the desired system performance. Each unit could control the 
transformation of points and edges from the coordinate system of the world model 
into screen coordinates and perform the associated clipping operations. The block 
diagram of the MAGIC chip in a PRISM system is illustrated in Fig. 2.42.
The project’s goal was to produce a modular, expandable chipset capable of 
performing all the operations necessary to transform 3D databases into realistic 
visual images. The suggested applications for the chipset ranged from small personal 
computer systems, through high performance CAD systems, to full state-of-the-art 
ﬂight simulator visual systems. 
The overall system was divided into three traditional subsystems: the application 
and database processor (CPU), which selected portions of the database for processing; 
the geometry processor (T&L), which transformed and clipped 3D world model

86
2
1980–1989, Graphics Controllers on Other Platforms
Fig. 2.42 The University of Sussex’s MAGIC used in a system
objects into displayable 2D objects; and the display or drawing processor, whose 
function was to perform various sorting and visual enhancement algorithms on the 
data before displaying it. 
The operations performed in the geometry subsystem were well deﬁned and based 
on existing algorithms. Many of the geometry algorithms typically required in a 
polygon-based graphics system could be implemented. Various system conﬁgura-
tions were possible, each using multiple instances of MAGIC. 
The 
arithmetic 
elements 
were 
an 
array 
multiplier/divider 
and 
several 
adder/subtractors; the register ﬁles were specially designed units suitable for general 
vector manipulation. 
The system transformed the polygons from the 3D representation to the 2D 
perspective projection to the viewing screen coordinates and provided a clipping 
operation option in 3D or 2D. They developed two scan conversion systems: a zone 
management processor that used the coherence inherent in the polygon and a second 
system based on a line processor that used coherence with spans of a polygon on 
successive scan lines. 
MAGIC had ﬂexibility through a writeable microprogram store. That meant the 
speciﬁc functionality of MAGIC could be deﬁned by the user rather than the chip

2.13 Raster Technologies Vertex Processor (1987)
87
designer. For ﬂexibility, it used ﬂoating-point numbers in the IEEE format. However, 
VLSI implementations of fast ﬂoating-point processors at the time required large 
areas of silicon, while the speed of smaller serial ALUs was a limitation for high 
performance applications. Those problems were reconciled by designing MAGIC as 
a controller for geometric operations and providing a fast ﬂoating-point coprocessor 
(AMD 1985, Analog Devices 1985) to perform numerical computation. 
2.13 Raster Technologies Vertex Processor (1987) 
In the early 1980s, Raster technologies was founded in the Boston area to design 
and build high-resolution graphics terminals. It used Yamaha chips for the graphics 
processor and developed a programmable vertex processor. 
Raster technologies recognized that high performance could be attained, as 
demonstrated by Clark’s Geometry Engine. However, more complex operations such 
as surface tessellation and lighting calculations could not easily be handled by a 
highly pipelined architecture because of the difﬁculty in reconﬁguring the pipeline 
to execute a wide variety of different graphics operations efﬁciently (Fig. 2.43). 
Parallel processors have been explored by many researchers for drawing opera-
tions. Fuchs’s Pixel Planes is a prime example. However, relatively little work was 
done using parallel processing for front-end geometric and arithmetic operations, 
observed Raster technologies founder John (Jay) G. Torborg in a Siggraph paper in 
1987 [61]. 
In Torborg’s design, graphics arithmetic processors handled complex compu-
tational functions such as transformations, clipping, tessellation, lighting models, 
picking, and general command processing. After processing, the graphics arithmetic 
processors transferred low-level drawing primitives (points, vectors, triangles, etc.)
Fig. 2.43 Jay Torborg 
(Courtesy of Velotech) 

88
2
1980–1989, Graphics Controllers on Other Platforms
Fig. 2.44 A typical graphics command processing data ﬂow 
to the image memory units (shown in Fig. 2.44). The uniqueness of this architecture 
was in how it determined the order in which the low-level drawing primitives were 
transferred. 
It is one of the ﬁrst examples of a vertex shader design. The arithmetic units 
were not called shaders in those days. Torborg used Yamaha VLSI ﬂoating-point 
processors to construct his front-end. 
Torborg would go on to develop Microsoft’s Talisman graphics processor 
discussed in Sect. 6.6. 
2.14 Amiga (1988) 
Commodore, developer of the popular PET computer (1977), one of the ﬁrst 
microcomputers with bitmapped graphics, acquired Amiga Corporation in 1984. 
The Commodore Amiga was a low-cost landmark machine when it launched in 
1985. It had high color graphics and displayed 4,096 colors simultaneously (using the 
Amiga Hold-And-Modify (HAM) display mode). In comparison, the Apple Macin-
tosh, introduced a year, before was only black and white, and IBM’s EGA, also 
released in 1984, only had sixteen colors. An Amiga sold for $1,295 at the time, 
whereas an IBM PC and Apple Mac sold for $2,145 and $2,495, respectively.

2.14 Amiga (1988)
89
The Amiga had stereo sound, whereas most other machines had no sound or mono 
sound output. It had a multitasking operating system (which Sun Microsystems tried 
to buy), whereas other machines could only do one thing at a time. Engineering marvel 
that it was, Commodore had difﬁculty marketing the machine’s many advancements 
to consumers. There were few applications for the machines and the consumer market 
was not quite ready. 
The Amiga had four main chips: the main processor, a Motorola 68000, a general-
purpose control chip named Angus that managed the system’s RAM and contained 
a BitBlt engine (a blitter that could do fast transfers of data in memory without 
involving the main processor), and the Copper video-synchronizing coprocessor. The 
audio chip was called Paula, which had four independent 8-bit pulse-code modulation 
(PCM) sound channels. 
Blitter OBjects (known as BOBs) are like sprites but were drawn by the blitter into 
the screen bitmap. Unlike sprites, they were not independent of the screen colors and 
resolution but could use as many colors as the chosen screen mode allowed. Shown 
in Fig. 2.45 is a generalized block diagram of an Amiga system. 
The graphics controller was named Denise, and it would be the ﬁrst of three 
generations. Denise was a contraction of “Display Enabler.” 
The Denise could be programmed to get video or image data from 1 to 5-bit planes 
and translate that into a color LUT entry. The number of bit planes was arbitrary, 
so if all the colors were not needed, 2, 4, 8, or sixteen could be used instead. The 
number of bit planes (and the resolution) could be changed on the ﬂy, usually by 
the Copper video coprocessor. That made very economical use of RAM and helped 
balance CPU processing speed and graphic quality. There can also be a sixth-bit 
plane, which could be used in one of three special graphics modes: 
Dual-playﬁeld. In this mode, each (of the three) bit planes were drawn on top of 
each other. Each plane could be independently scrolled while the background color
Fig. 2.45 The Commodore 
Amiga block diagram 

90
2
1980–1989, Graphics Controllers on Other Platforms
Fig. 2.46 A picture in the 
HAM mode, showing all 
4,096 colors at once 
on-screen. Such an image 
was displayed on an Amiga 
1000 in 1985! (Courtesy of 
The Amiga Museum) 
of the top playﬁeld came through. That proved helpful for video overlay of things 
like scrolling text on images. 
Extra-HalfBrite. This mode checked if a pixel was set on the sixth-bit plane and 
then cut the pixel’s brightness in half. 
HAM. That was one of the most popular modes. Each 6-bit pixel was interpreted as 
two control bits and four data bits. The control bits could be set to modify red, green, 
or blue. The four data bits could be used as a 16-color display lookup. That allowed 
for all 4,096 colors to be displayed on the screen at once. It also provided a form of 
lossy image compression in hardware. Figure 2.46 shows an example of the image 
quality of the Amiga. 
The chip also supported two horizontal graphics resolutions: low-res (using 140 
ns pixels) and hi-res (using 70 ns pixels at 320 or 640 horizontal pixels wide without 
using overscan). 
The pixel output was regulated by the main system clock and based on the NTSC 
colorburst timing. The Denise chip also supported overscan and could provide data 
for up to 800 lines. However, it was only helpful for scrolling and special effects that 
involved a partial display of large graphics. Nonetheless, it fully supported 736 × 
482 (NTSC) and 736 × 580 (PAL) TV resolutions. 
The Amiga was ideal for presenting graphics in a TV mode, given its timing 
structure to scanlines and its DMA resource allocations. However, the Denise chip 
did not support a dedicated text mode. 
The Denise chip could also composite up to eight 16-pixel-wide sprites per scan 
line. The sprites had three visible colors and one transparent color. Using the CPUs 
and the Copper registers, each sprite could be reused multiple times in a single frame 
and increase the total sprites per frame. One of the ﬁrst Amiga games to utilize sprite 
repositioning was Hybris, released in 1988.

2.15 Sun’s GX Graphics Accelerator Board (1989)
91
The Amiga and its Denise chip was the ﬁrst multimedia device to blend video, 
graphics, and audio. It got used in some amazing video production projects, given 
open access to all the chipset’s registers. 
Lorain Begat Denise. 
The Amiga’s original chipset codename was Lorraine. The development work on 
the Lorraine project in 1983 used an 8 MHz Sage IV, Motorola 68000-based 
microcomputer nicknamed Agony. 
Amiga funded the Lorraine development by selling game controllers it had built 
and securing a loan from Atari. The chipset was designed for video game machines. 
But the company had terrible timing. Amiga Corporation created it just as the ﬁrst 
home video game boom was ending. In 1983, there was a recession in the console 
gaming market in the U.S. It was deemed the video game crash of 1983–1985. The 
crash was due to market saturation of game consoles and available games and fading 
interest in console games as gamers moved to PCs. 
In July 1984, Atari Inc. was bought by Jack Tramie, Commodore’s recently 
resigned CEO and founder. In 1985, he launched the Amiga based on Atari chips. 
One of the most signiﬁcant features of the Amiga was how its unique chips 
could be programmed to produce exceptional graphics for the time. The Amiga was 
well known for that capability and became hobbyists’ and experimenters’ favorite 
machine, even after the company had gone out of business. 
However, despite all its advanced technical features and capabilities, the poorly 
marketed Amiga 1000 was not a success. 
Amiga Epilogue. 
The Enhanced Chip Set (ECS) was the second generation of the Amiga Computers 
chipset and offered minor improvements over the original design. ECS was also in the 
Amiga 3000 introduced in 1990. Another version was developed, but by April 1994, 
Commodore International ﬁled for bankruptcy. The Amiga was a highly advanced 
system, but Commodore was already facing ﬁnancial difﬁculties, and the Amiga 
could not save it [62]. 
The Atari chipset pushed multimedia development into the design of graphics 
controllers. It inﬂuenced Nvidia and Yamaha’s decisions to make their ﬁrst chip multi-
media devices. The Atari chipset also inspired engineers at AT&T who developed 
the Targa board. 
2.15 Sun’s GX Graphics Accelerator Board (1989) 
Tightening the graphics pipeline by shifting the work from the CPU to the graphics 
AIB was a continuing theme (and still is). All the major CG hardware suppliers

92
2
1980–1989, Graphics Controllers on Other Platforms
experimented with, developed, and offered improved systems and subsystems toward 
that goal. Sun Microsystem was no exception and had a long history of graphics 
innovations, some created internally, some from acquisitions, and some from 3rd 
party suppliers like 3Dlabs. 
Previous graphics accelerators and smart terminals had some intelligence, either 
a CPU, a bit-slice processor, or a DSP which controlled the graphics hardware and 
processed much of the rendering calculations. Other offerings were just dumb frame 
buffers, often in a separate box. 
But from the 1980s to the beginning of the 1990s, powerful 3D graphics systems 
were expensive, extensive, and difﬁcult to program. Sun, and others, wanted to 
democratize computer graphics and put its power in everyone’s hands. Sun had been 
a pioneer in driving down the costs of workstations by using open operating systems 
and associated graphics libraries. It followed Sun would take that philosophy and 
apply it to a CG AIB. 
The GX AIB was a family of boards consisting of a Sun GX frame buffer controller 
(FBC), several Mbytes of VRAM, a Brooktree LUT-DAC, a custom transform engine, 
cursor ASIC (TEC) and bus interface, and memory controller chips, as illustrated in 
Fig. 2.47 [63]. 
Sun used the 80/20 rule in the design. In the case of the GX, 20% of the graphics 
algorithms were identiﬁed that would impact at least 80% of the dollars within each 
targeted market sector (Fig. 2.48).
The AIB also used a different approach to the graphics pipeline and worked 
on quadrangles. An arbitrary quadrilateral was the GX’s only geometric primitive. 
The GX worked with drawing primitives (points, lines, rectangles, triangles, etc.) as 
degenerate cases of a quadrilateral.
Fig. 2.47 Sun 
Microsystem’s GX graphics 
accelerator AIB functions 

2.15 Sun’s GX Graphics Accelerator Board (1989)
93
Fig. 2.48 Sun 
Microsystem’s GX AIB 
(Courtesy of Curtis Priem)
The operations and primitives that the GX did not support directly were available 
by multiple applications of one (or more) of the supported basic operations. That 
approach is how CISC instructions can be made up by combining simpler RISC 
instructions. 
The GX did not support circle and arc primitives. But those primitives could be 
approximated with short line segments. Also, the GX only provided ﬂat shading of 
images. However, by breaking the object into smaller objects (tessellation), each 
could have a different color, providing an approximation to Gouraud shading. Sun 
claimed the high performance of its GX’s rendering hardware made such an approach 
attractive. Taking it even further, the company suggested if a tessellated polygon 
got close to one pixel in size, it would get results equivalent to the Phong-shading 
technique. Such tessellation, however, took several cycles. 
High-level integration was one of the main ideas of the GX design. The GX’s 
code name was LEGO, which stood for Low-End Graphics Option. Low-end did not 
mean performance; it was the price range of the targeted workstations. 
The FBC chip performed all rendering and drawing operations and placed the 
results in the frame buffer. It processes only three instructions:
• DRAWs, which render arbitrary, ﬁlled quadrilaterals,
• FONTs, which display precomputed pixel images, and
• BLITs, which transfer block images. 
The ﬁrst rendering algorithm selected drew arbitrary ﬁlled quadrilaterals that 
could be self-intersecting and degenerated. Sun felt it was essential to simplify 
geometric rendering into one algorithm in the FBC due to the limited silicon resources 
of the day.

94
2
1980–1989, Graphics Controllers on Other Platforms
The transformation engine and cursor chip (TEC) modiﬁed all graphics. It 
supported the computational requirements of graphics programs, provided a hard-
ware cursor, and contained the timing generation logic for the frame buffer. The TEC 
had a 64-bit register ﬁle and a 32-bit signed multiply/accumulator, which combined 
to provide arithmetic calculations up to 50 MFLOPS. 
The TEC included explicit support for the typical 2D and 3D graphics-
transformation pipeline. The modeling and viewing (MV) matrix could consist of 
anything from a simple 3 × 2 matrix used by Sun class 20 graphics workstations to 
a 4  × 4 matrix used by class 30 graphics workstations. 
A Sun SPARCstation IGX 4/60 GX equipped system could generate 244k 
vectors/s, which signiﬁcantly exceeded the 80 k vectors/s SGI could accomplish 
with the Personal Iris 4D/20 
The GX board (including frame buffer) used less than 20 watts from the 
workstation. 
2.15.1 
Summary 
Curtis Priem designed the FBC and TEC chips for Sun. He and his colleague Chris 
Malachowsky would later join Jensen Huang and form Nvidia. Their ﬁrst chip, the 
NV1 (1993) would employ the quadrilinear processing technique. How that turned 
out is discussed in Sect. 6.2. 
2.16 Conclusion 
The 1980s were the fountainhead of graphics systems. CAD, molecular modeling, 
and simulation prompted the demand for graphics, and nothing was ever fast enough 
or had high enough resolution—in computer graphics too much is not enough. 
Decoupling from the CPU and using stand-alone geometry processing to translate 
the computer’s coordinates to the screens became apparent early on, prompting such 
developments as the Geometry Engine and several clever designs with dedicated 
geometry processors. 
Recognizing the need for processing picture elements, which were increasing 
exponentially from 320 × 200 (64,000 pixels) to 1024 × 768 (786,432 pixels) and 
beyond, led to the creation of the Pixel Planes systems and development of SIMD 
architectures and the foundation of the GPU. 
And moving in synchronization with these developments and the unrelenting 
demand for more performance and lower costs was Moore’s law. All the elements 
were set in place. What would follow would be even more developments pushing 
the industry ever closer to a single chip with all the elements in it to do all graphics 
processing and at consumer prices.

References
95
References 
1. Geddes, D. The history of computer-aided design and computer-aided manufacturing 
(CAD/CAM), Vita Technical Foam Services (May 5, 2020), tinyurl.com/45fesaba 
2. Engelbart, D. C. https://en.wikipedia.org/wiki/Douglas_Engelbart 
3. Engelbart, D. C., Microelectronics, and the Art of Similitude, 1960 IEEE International Solid-
State Circuits Conference. Digest of Technical Papers, (February 12. 1960), https://ieeexplore. 
ieee.org/document/1157297 
4. Moore’s law, https://en.wikipedia.org/wiki/Moore%27s_law 
5. Markoff, J. It’s Moore’s Law But Another Had The Idea First, The New York Times. (April 
18, 2005), https://web.archive.org/web/20120304111901/http://www.nytimes.com/2005/04/ 
18/technology/18moore.html 
6. Moore, G. E. Cramming more components onto integrated circuits, Electronics Maga-
zine.(April 19, 1965), https://en.wikipedia.org/wiki/Moore%27s_law 
7. Courtland, R. Q&A: Carver Mead, IEEE Spectrum, (15 Apr 2015), https://spectrum.ieee.org/ 
semiconductors/devices/qa-carver-mead 
8. Courtland, R. The Murky Origins of Moore’s Law, IEEE Spectrum (28 Apr 2015), https://spe 
ctrum.ieee.org/tech-talk/semiconductors/devices/the-murky-origins-of-moores-law 
9. https://en.wikipedia.org/wiki/Very_long_instruction_word 
10. England, N. The Graphics System for the 80’s, IEEE Computing Edge, October 2021, Volume 
7, Number 10, Page 25, https://ieeexplore.ieee.org/document/9082270 
11. England, N. A Graphics System Architecture For Interactive Application-Speciﬁc Display 
Functions, IEEE Computer Graphics Application, vol. 6, no. 1, pp. 60–70, Jan. 1986 
12. Smith, A. A Biography of the Pixel, MIT Press (August 202), https://mitpress.mit.edu/books/ 
biography-pixel 
13. Hittinger, W. C. Metal-Oxide-Semiconductor Technology, Scientiﬁc American, Vol. 229, No. 
2 (August 1973), pp. 48-59, https://www.jstor.org/stable/24923169?seq=1 
14. Henry F., Cohen, D., Sproull, R., Clark, J. H., and Parke, F. I. Trends in high performance 
graphic systems (Panel Session), SIGGRAPH 1980, https://dl.acm.org/doi/10.1145/965105. 
807468 
15. Fuchs, H. and Poulton, J. Pixel-Planes: A VLSI-Oriented Design for a Raster Graphics Engine, 
VLSI Design, 3rd Quarter, 1981. 2(3), pp 20-28 
16. Seelinger, P. Through the Looking Glass, Endeavors magazine, UNC, August 5, 2016, https:// 
endeavors.unc.edu/through_the_looking_glass/ 
17. Thacker, C. P., McCreight, E. M., Lampson, B. W., Sproull, R. F. and Boggs. D. R. 11 1979. 
ALTO: A Personal Computer, Xerox Corp.; also in Siewiorek, Daniel P., C. Gordon Bell, and 
Allen Newell, Computer Structures: Principles and Examples, McGraw-Hill, 1982, pp 549-572 
18. Fuchs, H., Poulton, J. (June 12 1981), PIXEL-PLANES: a VLSI-oriented Design for 3-D 
Raster Graphics, Proceedings of the 7th Canadian Man-Computer Communications Confer-
ence: Waterloo, Ontario, Canada, 10 -, 343-347, https://graphicsinterface.org/proceedings/cmc 
cc1981/cmccc1981-50/ 
19. Difference between CMOS and NMOS Technology, https://www.elprocus.com/difference-bet 
ween-nmos-cmos-technology/ 
20. Young, F. W. and Rheingans, P. Visualizing Structure in High-Dimensional Multivariate Data 
, The IBM Journal of Research and Development, Volume 35, Number 1/2, (January/March 
1991), https://ieeexplore.ieee.org/document/5389809 
21. Fuchs, H., Poulton, J., Paeth, A., Bell A. Developing Pixel Planes, A Smart Memory-Based 
Raster Graphics System, Proceedings, Conference on Advanced Research in VLSI, Cambridge, 
Mass. 19B2, (January 25-27), https://books.google.com/books/about/Proceedings_Confer 
ence_on_Advanced_Resea.html?id=_f8YAQAAIAAJ 
22. Fuchs et al, Fast Spheres, Shadows, Textures, Transparencies, And Image Enhancements In 
Pixel-Planes, SIGGRAPH, Volume 19, Number 3, p111, (1985), https://dl.acm.org/toc/sig 
graph/1985/19/3

96
2
1980–1989, Graphics Controllers on Other Platforms
23. Poulton, J., Austin, J. D., Eyles, J. G.,. Heinecke, J., Hsieh,C. H., and Fuchs, H. Pixel-Planes 
4 Graphics Engine, Technical Report, Department of Computer Science, UNC Chapel Hill, 
(1985), http://www.cs.unc.edu/~fuchs/publications/PixelPlanes4.pdf 
24. Fuchs, H., Poulton, J., Eyles, J., and Greer, T. Coarse-Grain and Fine-Grain Parallelism in the 
Next Generation Pixel-Planes Graphics System, Proceeding of the International Conference 
and Exhibition on Parallel Processing for Computer Vision and Display, University of Leeds, 
United Kingdom, January 12-l5, 1988, http://www.cs.unc.edu/techreports/88-014.pdf 
25. Jahejo, A. Ring Topology | Advantages & Disadvantages of Ring Topology, Computer Network 
Topology, https://computernetworktopology.com/ring-topology-advantages-disadvantages/ 
26. Goldfeather, J., Hultquist, J. P. M., and Fuchs, H.. Fast Constructive Solid Geometry Display 
in the Pixel-Powers Graphics System, Computer Graphics, 20(4), (Proceedings of SIGGRAPH 
’86), pp 107-116., https://dl.acm.org/doi/10.1145/15922.15898 
27. Division, reporting this week, bets its future on cunning pixel ﬂow, professional applications, 
CBR Staff Writer, Tech Monitor, (10 Aug 1994), https://techmonitor.ai/?s=10+aug+1994 
28. Peddie, J. PixelFlow graphics architecture uses massively parallel processor-per-pixel 
approach, The PC Graphics Report (June 1996) 
29. Maher, K. PixelFusion, The Peddie Report, Volume XI, Number 10 (March 16, 1998) 
30. Fischer, A. Semiconductors: PixelFusion, The Peddie Report,Volume XI, Number 10 –, pp 
315, (March 16, 1998) 
31. Latta, J. PixelFusion Launches Next Round of Financing, The WAVE Report on Digital Media, 
4th Wave, Inc., Issue #9108 (November 15, 1999) 
32. Ball, R. PixelFusion draws up plans to drop graphics emphasis, Electronics Weekly, (October 
4, 2000), https://tinyurl.com/2wr9pkvz 
33. Demetrescu, S., A VLSI-Based Real-Time Hidden-Surface Elimination Display System, 
Master’s Thesis, Department of Computer Science, California Institute of Technology, (1980), 
https://thesis.library.caltech.edu/6896/2/Demetrescu_sg_1980.pdf 
34. Foley, J. D., van Dam, A., Feiner, S. and Hughes, J. Computer Graphics: Principles and Practice 
in C (2nd ed.)., Addison-Wesley Professional, (14 August 1995) 
35. Fuchs, H., An Introduction to Pixel-planes and other VLSI-intensive Graphics Systems, NATO 
International Advanced Study Institute, Theoretical Foundations of Computer Graphics and 
CAD, (July 17, 1987), https://link.springer.com/chapter/10.1007/978-3-642-83539-1_25 
36. Weinberg, R., Parallel Processing Image Synthesis and Anti-Aliasing, Computer Graphics, Vol. 
15, No. 3 (Proceedings of 1981 SIGGRAPH Conference), pp. 55-61, (August 1981), https:// 
link.springer.com/chapter/10.1007/978-3-642-46514-7_8 
37. Demetrescu, S. High Speed Memory and Processor System for Raster Display, U.S. patent 
number 4,648,045, https://tinyurl.com/n3ps8xj9 
38. Peddie, J. Famous Graphics Chips: Geometry Engine, IEEE Computer Society, https://www. 
computer.org/publications/tech-news/chasing-pixels/geometry-engine 
39. Clark, J. H., Stanford University, Lambda Second Quarter (1980), http://ai.eecs.umich.edu/peo 
ple/conway/VLSI/ClassicDesigns/GeomEng/GeomEng.L2Q80.pdf 
40. Mead, C., and Conway, L. Introduction to VLSI Systems, Addison-Wesley, 1978. 
41. Seitz, C. L. Department of Computer Science, California Institute of Technology 
42. Clark H. J. Computer Systems Laboratory, Stanford University and Silicon Graphics, Inc., Palo 
Alto, California, Computer Graphics Volume 16, Number (July 03, 1982) 
43. Markoff, J. The design process of VLSI circuits accelerates, Infoworld, (March 15, 1892), 
page 17, https://tinyurl.com/e63jfwc 
44. Peddie, J. The History of Visual Magic in Computers, (2013), Springer Link 
45. Peddie, J. Famous Graphics Chips: NEC µPD7220 Graphics Display Controller, IEEE 
Computer Society, https://www.computer.org/publications/tech-news/chasing-pixels/famous-
graphics-chips 
46. Tetsuji O., Higuchi, M., Uno, T., Kamaya, M., and Suzuki, M. (February 1981). A Single-chip 
Graphic Display Controller. International Solid State Circuit Conference. IEEE: 170–171 
47. µPS7220/7220A Graphics Display Controller User’s manual, NEC Electronics Inc. 
(December 1985), http://www.bitsavers.org/components/nec/uPD7220-uPD7220A_User_M 
anual_Dec85.pdf

References
97
48. Peddie, J. GPU History: Hitachi ARTC HD63484. The second graphics processor, IEEE 
Computer Society, https://www.computer.org/publications/tech-news/chasing-pixels/gpu-his 
tory-hitachi-artc-hd63484 
49. Peddie, J. Multimedia and Graphics Controllers, McGraw-Hill Professional Book Group (April 
1994) 
50. Ingalls, D. BitBlt, Xerox Inter-Ofﬁce Memorandum 19 November 1975, http://bitsavers.org/ 
pdf/xerox/alto/BitBlt_Nov1975.pdf 
51. Sanchez, J.; Canton, M. P. Displaying Bit-Mapped images, Software solutions for engineers 
and scientists, CRC Press. p. 690. (2007) 
52. Force Computers AGC-1 user’s manual, (1986), https://doc.lagout.org/science/0_Comp 
uter%20Science/0_Computer%20History/old-hardware/forceComputers/800106_AGC-1_U 
sers_Manual_Aug86.pdf 
53. Hitachi HD63484 ACRTC Advanced CRT Controller User’s Manual, (November 1984), http:// 
kazojc.com/elementy_czynne/IC/HD63484-ARTIC.pdf 
54. Peddie, J. Famous Graphics Chips: TI TMS34010 and VRAM. The ﬁrst programmable graphics 
processor chip, IEEE Computer Society, https://tinyurl.com/pkedarab 
55. Killbrew, C. The TMS34010 Graphics System Processor, Byte, page 195, (December 1986), 
https://archive.org/details/byte-magazine-1986-12/page/n199/mode/2up 
56. TMS34010, Wikipedia, https://en.wikipedia.org/wiki/TMS34010 
57. TMS34010 User’s Guide, Texas Instruments, (1986), https://fabiensanglard.net/nbajamte/t34 
010_user_guide.pdf 
58. Peterson, R., Killebrew Jr., C. R., Albers, T., and Guttag, K. Taking the Wraps off the 
34020, Byte, p. 257, (September 1988), https://ia600106.us.archive.org/32/items/byte-mag 
azine-1988-09/BYTE-1988-09.pdf 
59. Grimsdale, R L., Lister, P. F. (others), A multiple application graphics integrated circuit MAGIC 
II. EGGH’87: Proceedings of the Second Eurographics conference on Advances in Computer 
Graphics Hardware, Pages 81–92, (August 1987) 
60. Agate, M., Finch, H.R., Garel, A.A., Grimsdale, R.L., Lister, P. F., A Multiple Application 
Graphics Integrated Circuit – MAGIC II, Eurographics (1987) http://diglib.eg.org/bitstream/ 
handle/10.2312/EGGH.EGGH87.081-092/081-092.pdf?sequence=1&isAllowed=y 
61. Torborg, J. G. A Parallel Processor Architecture for Graphics Arithmetic Operations, ACM  
Computer Graphics, Volume 21, Number 4, (July 1987) 
62. Reimer J. A history of the Amiga, part 10: The downfall of Commodore, (January 22, 
2017), https://arstechnica.com/gaming/2017/01/a-history-of-the-amiga-part-10-the-downfall-
of-commodore/ 
63. Priem, C. R. Developing the GX Graphics Accelerator Architecture, IEEE Micro, (February 
1990), http://www.pix.net/staff/lidl/papers/gx_graphics_architecture.pdf

Chapter 3 
1980–1989, Graphics Controllers on PCs 
3.1 
1980–1989, Graphics Controllers on the PC Platform 
Graphics systems had been in existence since the early 1970s. Other standards or de 
facto standards had been established long before the PC, and they were the attractive 
platforms and markets that motivated companies to innovate. Those platforms were 
primarily in what today would be called the professional class or segment—big 
machines with big screens, big mission-critical projects, and big prices. 
Also, before the PC, there was a thriving and vibrant microcomputer segment. This 
segment appealed to hobbyists and small businesses with fewer ﬁnancial resources. 
But the machines were amazingly capable and powerful and, as is well chronicled, 
inspired Bill Gates to start Microsoft. 
Computer graphics started in the 1950s for military programs. In the 1960s, it 
evolved to industrial use, primarily for the computer-aided design of automobiles 
and airplanes. In the 1970s, building design and molecular modeling applications 
embraced it, and the computer graphics (CG) ﬁeld was ﬁrmly established. While 
the big companies were learning how to make use of this powerful new tool, the 
hobbits were learning about it too. And they led to IBM’s interest and investment in 
developing the PC. 
IBM solidiﬁed the concept of small computers for individuals, and the IBM brand 
inspired conﬁdence. IBM created the industry we enjoy today, but they did not do it 
all independently or alone. 
Therefore, the evolution of the GPU is chronicled in this chapter with a view to the 
PC’s inﬂuence. IBM was not alone. Semiconductor suppliers such as NEC, Hitachi, 
and Texas Instruments were developing parts for the broadest possible market. They 
also wanted a market with high average selling prices (ASPs) so they could get the 
biggest and fastest return possible on the investment made in developing their semi-
conductors, their chips. Initially, they targeted graphics terminals and workstations, 
and soon learned the PC would be a much bigger market. 
The target market for the ﬁrst very large-scale integrated semiconductors designed 
for graphics applications was the users of larger professional graphics systems, not
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2022 
J. Peddie, The History of the GPU - Steps to Invention, 
https://doi.org/10.1007/978-3-031-10968-3_3 
99

100
3
1980–1989, Graphics Controllers on PCs
Fig. 3.1 The IBM PC circa 
1981 (Courtesy of 
Wikipedia, Rama & Musée 
Bolo) 
microcomputers or the PC. Later, the parts would not only ﬁnd their way to the PC, 
but also ultimately found a much larger customer base. 
In this section, we look at the developments in graphics controllers and processors 
that led to the GPU used in the PC. Until the smartphone became popular in the late 
2000s, the PC, shown in Fig. 3.1, was the biggest platform for computer graphics. 
The PC, like everything in the computer industry, was an evolutionary product. But 
it was also, like a few things in the computer industry, a disruptive and revolutionary 
product. 
The world changed on August 12, 1981. On that day, IBM held a press conference 
in a ballroom of the Waldorf Astoria hotel and announced its ﬁrst Personal Computer 
(model 5150). That machine was developed by a team led by IBM VP Don Estridge in 
Boca Raton, Florida. It upended the computer business and the world and spawned a 
trillion-dollar industry. Today, millions of people work in the PC industry, and almost 
everyone on earth uses one or more. 
The PC has proved to be one of the signiﬁcant transformational technologies 
in human history, but there were microcomputers that came before it. IBM built a 
development machine called SCAMP (Special Computer, APL Machine Portable) 
in 1972 [1]. Datapoint, based in San Antonio Texas, introduced their Datapoint 2200 
in 1970 [2], and one of the ﬁrst commercial microcomputers was the French Micral, 
released in 1973 by Réalisation d’Études Électroniques (R2E) [3]. The Datapoint 
2200 and the Mircal were based on the Intel 8008 and were the ﬁrst non-kit computers. 
Micro Instrumentation and Telemetry Systems—MITS announced the Altair 8800, 
the ﬁrst computer kit, in January 1975; it inspired Bill Gates and Paul Allen and laid 
the foundation for the IBM PC [4]. Also in 1975, IBM introduced the 5100 Portable 
computer. 
IBM’s version of the story is that the emergence of microcomputers inspired a 
meeting between William C. Lowe, who was systems manager for IBM’s entry-level 
division, and CEO Frank Cary [5]. Although the company had built small computers 
before Lowe and Cary both believed that IBM could no longer ignore the challenge

3.1 1980–1989, Graphics Controllers on the PC Platform
101
of a microcomputer, Lowe said he could come up with a true personal computer— 
small, easy to use, and low cost. They set a goal price of $1,500. Cary challenged 
Lowe to develop a prototype in one month, and Project Chess was born [6]. 
In July 1980, before IBM’s Project Chess was formally approved, the company 
sent a team led by Jack Sams to meet with several software companies including 
Digital Research and Microsoft to discuss the PC market. IBM wanted to build an 
open architecture system and the OS was a crucial element of that plan. 
In just about every account of the meeting, IBM asked Microsoft about operating 
systems, and Bill Gates referred IBM to Digital Research (DR), even getting DR’s 
founder Gary Kildall on the phone to arrange a meeting for the following day. Kildall 
had the CP/M operating system that Microsoft wanted to use and that almost all other 
microcomputers of the day were using, including the Osborne 1 and Kaypro II. 
There are various tellings of how and why Kildall did not get the IBM deal in Fire 
in the Valley: The Making of the Personal Computer by Paul Freiberger and Michael 
Swain [7], and in Wallace and Erickson’s Hard Drive: Bill Gates and the Making of 
the Microsoft Empire [8], Kildall says he was on a business trip and could not make 
it in time. Stories also have it that IBM met with Kildall’s wife, Dorothy McEwan, 
and presented her with a one-sided nondisclosure agreement (NDA). 
IBM laid the problem at Microsoft’s door, and Paul Allen pointed to Seattle 
Computer Products, which was using what they considered a temporary operating 
system as they waited for DRI to ﬁnish a system suitable for their system. It was 
QDOS, and Allen licensed it. That is a very abbreviated version of what has become an 
origin story of the computer, and it has fueled simmering feuds and the mythology of 
several companies. Kildall went on to release DR DOS as a challenger in the market. 
The true genius of the ﬁrst IBM PC was that it had a quasi-open architecture. 
Other computers could be built on the same model, and many were. The explosion 
of IBM clones propelled the nascent PC market, and IBM nurtured this growing 
landscape by introducing standards that continue to deﬁne the machine even today: 
the ISA bus that grew to AGP, and the CGA display standard that evolved to VGA 
and XGA (and still lurks in the heart of today’s GPUs). The original BIOS and its 
concepts, which are still found in today’s version, and the physical layout of desktop 
machines and the motherboard are all 40-year-old designs with a few modiﬁcations 
and updates. 
IBM’s brand and backing legitimized and stabilized the small computer. The clone 
makers put it in millions of ofﬁces and homes. 
You could say the IBM PC was the Model T or the Kitty Hawk of small computers. 
However, you choose to describe and honor it, be grateful IBM did it. IBM legitimized 
the small computer.

102
3
1980–1989, Graphics Controllers on PCs
3.2 
CRT Control (1975–1987) 
The microcomputers that emerged in the 1970s used raster-scan CRTs as their 
display. The CRTs were inexpensive due to TV production and, compared to stroke 
writers which were used in much smaller quantities for workstations and military 
and research computers. 
The early microcomputers had a TV out plug that connected to the TV’s antenna 
input (and was usually set at channel 3), or a RGB analog outputs for more 
professional monitors. 
To get an image to the CRT, you needed a timing generator and shift register 
to send the bits that became pixels. The Hitachi HD46505 was an N-channel MOS 
device CRT controller (CRTC) introduced in the late 1970s. A variety of Japanese 
computers from Sony, Sharp, Panasonic, and Casio used it. The CRTC has a bus 
interface compatible with the popular 6502’s bus system. The 6502 was introduced 
in 1975 by MOS Technology. The Motorola 6545 CRTC was a U.S.-made, second-
source part, and very popular, used in dozens of systems (the IBM MDA, Hercules 
Graphics AIB, IBM CGA, Apple, Pet, TRS80, BBC computers, and more). The 
CRTC had a bus interface compatible with the 6502-bus system. 
The U.S. put an embargo on 6845 during the Cold War, and the device was cloned 
in Bulgaria (U.S.S.R.) under the designation CM607. 
CRTC devices were used up till the VGA was introduced in 1987. Non-IBM AIBs 
such as the Hitachi 63,484 ACRTC and later had built-in CRTCs. 
3.2.1 
The Video Output—LUT-DAC (~1981–1987) 
The graphics controller was not the only part that was being integrated. The output 
section of a graphics system had to convert the digital data into analog voltages 
to drive color CRT monitors. That required a digital-to-analog converter (D/A, or 
DAC). 
The Digital-to-Analog Converter (DAC) is used to convert digital signals to analog 
signals that can be output to speakers, headphones, and in the case of video to analog 
monitors. 
In front of the DAC is a small memory known as lookup table (LUT). The LUT 
accepts the output of the frame buffer and can convert the bits into RGB color 
combinations. 
The monitors used three primary colors: red, blue, and green (and some models 
had a fourth input for synchronization). The primary colors could be blended to 
produce tens of millions of shades. Building three DACs (one for each primary 
color) could require up to 25 separate chips depending upon the color range, which 
was a function of the voltage level.

3.2 CRT Control (1975–1987)
103
3.2.2 
Brooktree (1983–1996) 
Henry Sour Katzenstein (1927–2003), a native of Shreveport, Louisiana, was the 
ﬁrst recipient of a doctorate in physics from the University of Connecticut in 1954 
and did postdoctoral research at MIT. 
In 1981, Dr. Henry Sour Katzenstein of Los Angeles started work on a very 
accurate integrated three-DAC circuit design. By 1983, he was satisﬁed enough with 
his work that he obtained some money from venture capitalists (VCs) and founded 
the Brooktree Corporation. He named the company after the street he lived on in San 
Diego. 
The ﬁrst commercial product was called a videoDAC (Fig. 3.2) and was introduced 
in 1985. It accepted 8 bits in and put out three analog signals. The D/As could convert 
digital data to analog at the rate of 75 MHz, which meant the product could drive 
CRT monitors with very high resolution—2048 × 2048, 60 fps. That was almost 
four times faster than a collection of discrete parts could achieve. So, the Brooktree 
videoDAC was faster, only one part, and less expensive than the 25 parts it replaced. 
Electronic Design called the device trailblazing [9]. The design was based on what 
Katzenstein called the Brooktree Matrix. However, volume shipments of videoDAC 
did not take off until 1988. 
The DACs did not have voltage outputs. They drove current, not voltage. The 
monitors (or graphics boards) had resistors that converted the current to voltage. 
The current output was the critical invention that allowed Brooktree to integrate the 
RAM LUT, logic, and DAC into traditional silicon (instead of discrete components 
or emitter-coupled high-speed logic (ECL)) is a high-speed integrated circuit). As 
the DAC outputs changed the value of the current, they were pulling or pushing from 
either the positive or negative supply. In other words, they were drawing full constant 
current no matter what analog levels were on the output. That reduced the noise on 
the power supply to almost zero. However, it meant that the chip ran at full power all 
the time, and that is why the LUT-DACs were always hot (like burning your ﬁngers 
hot). You never touched one a second time. 
Other companies such as AMD, Analog Device, and TI were also in the market. 
But Brooktree’s part was superior, and it quickly began to take market share away 
from the others. Customers included big names in the computer graphics industry 
such as Sun Microsystems, Apple Computer, Toshiba, and IBM. Brooktree was
Fig. 3.2 Brooktree 
LUT-DAC 

104
3
1980–1989, Graphics Controllers on PCs
Fig. 3.3 Brooktree 
LUT-DAC chip (Courtesy of 
Thomas Schanz Wikipedia) 
especially successful against AMD, and it captured the niche market that had been 
dominated by AMD with video add-in boards up to this time. 
AMD reacted with its own version and Brooktree sued, alleging AMD was selling 
cloned chips at lower prices to recapture lost market share. The cloned chips were 
allegedly copies of two of Brooktree’s chips. Brooktree argued that between 1981 
and 1986, it invested approximately $3.8 million in developing its videoDAC. On 
October 1, 1990, a jury found AMD guilty and awarded Brooktree $30 million [10]. 
In 1990, Brooktree made another revolutionary device by adding the color lookup 
table (LUT) to the videoDAC and created the RAMDAC (aka LUT-DAC) or palette 
DAC (Fig. 3.3). 
The DAC suppliers like Brooktree made 24-bit (triple 8-bit) DACs for worksta-
tions. Then when it introduced LUT-DACs, they were 8-bit LUTs. The LUT-DAC 
used in the PC at the time had lower performance, 6-bit LUT for 18 bits of RGB 
(262,144 colors). 
But semiconductor integration was unrelenting, and graphics controller chip 
suppliers began to integrate the LUT-DAC into the controller to reduce parts and 
cost. IBM integrated the LUT-DAC into the VGA controller in 1987. 
In 1990, Acumos, a small start-up company that was making VGA-compatible 
(VGA clone) chips introduced a device with an integrated LUT-DAC. In 1991, Cirrus 
Logic, one of the largest VGA clone builders, acquired Acumos. 
Things were not looking good for stand-alone LUT-DAC chips, and in 1996, 
Rockwell Semiconductor bought Brooktree, which became Conexant in 1998. 
3.2.3 
Edsun Labs (1989–1991) 
In 1989, the IBM VGA controller and its many clones were still the number-one-
selling devices, having been introduced in 1987. The resolution of the controller was 
the standard setting, 640 × 480. Steve Edelson, founder of Edsun Laboratories in 
Waltham, Massachusetts, came up with a clever way to enhance the color depth of the 
VGA’s 18 bits (262,144 colors) to 24 bits (16.7 million). He called it the Continuous 
Edge Graphics Digital-to-Analog Converter (CEG/DAC) (Fig. 3.4).

3.2 CRT Control (1975–1987)
105
Fig. 3.4 Steve Edelson, 
Edsun Laboratories 
(Courtesy of Edelson) 
Edelson got the idea of an edge-based graphic system in 1981. At that time, there 
were vector screens (great sharp, smooth lines) and bitmaps (good solid areas but 
jagged edges). Edelson’s observation was that the interior of a graphic solid was 
boring; all the important stuff happened when the CRT beam crossed an edge. He 
patented a system where the memory stored edge descriptions and the output stage 
handled the CRT perfectly for anti-aliased edges and sharper images and text. 
Edelson licensed it multiple times for professional markets, e.g., broadcast tele-
vision real-time animation for sports overlays and Japanese chyrons (for higher 
apparent resolution of their fonts). 
He then put graphics aside and started experimenting with semicustom ICs (called 
gate arrays in those days). Edelson made a series of PC interface chips for peripherals 
that sold well into the Taiwan/California PC board business. 
With knowledge of chips and the PC market, it was natural for Edelson to try to 
ﬁgure out how to get the CEG edge system into a PC. It was not obvious how or 
where to insert it, but over a period of weeks, Edelson and his team ﬁgured out how 
to put a subset CEG edge machine into a compatible LUT-DAC. 
In those days, designs were fabricated with discrete logic chips mounted on a 
board (breadboard) in sockets with pins that stuck through the board, as shown in 
Fig. 3.5. Wires were tightly wrapped around the pins and connected to other pins.
When the circuit was debugged, the ﬁnal logic diagram would then be inputted 
to the electronic design automation (EDA) CAD to create an application-speciﬁc 
integrated circuit (ASIC). 
It took Edelson and his team over a year to develop in wire-wrap, plus the software, 
and another year to get fully working silicon (a lot of custom work on DACs back 
then). 
Early results were impressive, so Edsun decided to enhance imagery with a second 
trick—a dynamic LUT. That allowed LUT edits that reloaded one LUT entry to be 
right in the pixel stream. The screen continued showing the current color to cover 
the three-pixel gap.

106
3
1980–1989, Graphics Controllers on PCs
Fig. 3.5 A breadboard with 
wire-wrap pins (Courtesy of 
Russ Shumaker)
Although the edits could be buried anywhere, Edsun decided to just blank some 
pixels on the left edge of each line. With a handful of changes per line, one could 
greatly expand the number of LUT values used over the different regions of the 
image—and there was an engine to blend them too. 
At that point, Edsun was able to show the amazing ray-traced samples with no 
visible defects (Fig. 3.6). That was a surprise even to them. 
Pixel weighting also worked well for eliminating jaggedness in lines in static 
images but did not work well for performing temporal aliasing for sequences of 
animated images. 
Although the CEG/DAC could display graphics at near 24-bit-per-pixel (bpp) 
quality, it was better suited to static than dynamic images because the CEG/DAC 
was not a true 24-bpp device but achieved its performance by embedding information 
to reprogram the palette (lookup table—LUT) in the bitmap and pixel weighting, a 
process of specifying pixel colors as weighted mixes of adjacent pixels (see Fig. 3.7). 
That reprogramming added overhead.
The CEG/DAC was a sophisticated high-speed, mixed-signal chip that incor-
porated digital signal processing (DSP) into the already eclectic combination of 
analog (three DACs), memory (LUT), and miscellaneous logic found on graphics
Fig. 3.6 Edsun’s triangle 
demo: The bitmap was 
animated and 
rainbow-rotated around the 
edges (Courtesy of Steve 
Edelson) 

3.2 CRT Control (1975–1987)
107
Fig. 3.7 Analog 
devices/Edsun labs 
CEG/DAC
chips. To obtain the needed performance on a chip of manageable size, advanced 
computer-aided design (CAD) technologies were employed. 
Because the CEG/DACs had to be interchangeable with devices having a limited 
number of observable digital outputs, testing was tricky. To handle this, the part 
contained four serial scan chains: three were in the mixing logic to allow external 
loading and sampling of the interpolator inputs and outputs and the read-only memory 
(ROM) outputs; the fourth chain passed through the control logic (Fig. 3.8). 
Analog Devices licensed the design from Edsun and produced a chip, the AD7148 
[11]. It provided 1280 × 1024 apparent resolution with a nonbinary 792,000 colors, 
transparently solving the longstanding jaggies aliasing problem. 
The stars aligned for Edsun as one of the VCs who invested in the company was 
the former VP of Analog’s DSP division. He arranged everything; a friendly win–win 
negotiation one could only dream of was signed. Analog Devices was a natural, a 
few miles away in Massachusetts; they had lots of DAC experience and high-end 
DSPs; and they were coming off success with PC parts for disk drives and audio. 
The deal was that Edsun would do the digital design and software, and Analog 
Devices would make the custom chip with no charge for the nonrecurring engineering 
(NRE). Edsun bought chips from Analog at cost, and both Analog and Edsun would 
sell the chips. 
In 1991, Analog decided to buy Edsun to bring the part in-house. Analog got some 
great engineers and a bunch of product ideas that made tens of millions for Analog 
in the years that followed.
Fig. 3.8 The CEG/DAC 
accepted processor and pixel 
data 

108
3
1980–1989, Graphics Controllers on PCs
However, when the industry moved to 1024 × 768 24-bit color and memory was 
less expensive, the CEG/DAC usefulness petered out and Analog Devices withdrew 
the product. The advance of cheaper memory allowed full 24-bit bitmaps, removing 
the need for memory-saving tricks such as lookup tables and CEG. 
3.2.4 
Summary of Video Output 
The video output section of the graphics controllers and then the GPUs was and is 
a critical section. As demanding in speed requirements, and during the analog days 
equally challenging in voltage accuracy. As the LUT-DAC got integrated into the 
graphics controller, the mixed-signal (analog and digital) aspects of semiconductor 
manufacturing were tested. Analog sections didn’t scale like digital; they had to be 
a certain physical size to work, so a special design effort was needed. As the LCD 
monitor slowly replaced the analog CRTs, the ﬁrst couple of generations of inherently 
digital displays (the LCDs) had an analog front end so they would be compatible 
with older graphics AIBs. Finally in early 2000, all digital systems began to take 
over making life easier for everyone. There is more on the evolution and transition 
of display output in the second book in this series, What is a GPU? 
3.3 
IBM Graphics History (1981–1990) 
IBM did not invent the beloved PC, the personal computer. It did create the name 
and validate the concept of a small personal computer that could be used in ofﬁces 
and at home. It also stabilized the burgeoning microcomputer industry, which had 
various processor types and operating systems, and few standards, making them 
incompatible with one another—and subsequently limiting their growth. IBM made 
small computers safe for businesses of all sizes whether they had an onsite computer 
expert (IT) or not. 
But IBM did more than just brand a small computer; it established de facto stan-
dards, especially in the display and graphics controller area. From 1981 to 1990, 
IBM set the standards, led the industry, and was the system and device to be compat-
ible with, emulate or copy. And copy it the industry did, to the point where IBM 
could no longer compete and so-called PC clones dominated the market. With the 
rise of the clones, IBM struggled to differentiate itself and oversaw the development 
of an Ill-advised alternative operating system (OS/2) and bus designs (EISA), but 
the company could not establish a defensible position in the industry it had created. 
After continuous market share loss, IBM sold its PC division to Lenovo in 2004. By 
2013, Lenovo was the number-one PC supplier—one man’s rubbish may be another’s 
treasure [12].

3.3 IBM Graphics History (1981–1990)
109
Fig. 3.9 IBM’s CGA add-in board (Courtesy of Wikipedia) 
3.3.1 
IBM CGA (1981) 
When IBM introduced the Intel 8080–based Personal Computer (PC) in 1981, it was 
equipped with an AIB called the Color Graphics Adapter (CGA). The CGA AIB 
had 16 kilobytes of video memory and could drive either an NTSC-TV monitor or 
a dedicated 4-bit RGB CRT monitor, such as the IBM 5153 color display (Fig. 3.9). 
It did not have a dedicated controller and was assembled using a half-dozen LSI 
chips. The large chip in the center was a CRT timing controller (CRTC), typically 
a Motorola MC6845. The 6845 was not a complete graphics controller. Its function 
was to generate the proper timing signals needed to drive the display memory and 
calculate the memory address of the next pixel to be drawn [13]. 
There were 69 chips on the CGA board. Eight of them were for memory. The 
AIB was designed in 1980. NEC had not yet introduced its 7220 chip and when it 
did later in 1980, it was easy to see the beneﬁt of replacing dozens of chips with 
just one. These old graphics boards are works of art and, like the piping in a steam 
engine, are fascinating to look at and wonder about. 
3.3.2 
IBM EGA (1984) 
The initiation of bitmapped graphics and the chip clone wars. 
The CGA and EGA AIBs were over 33 cm (13 in.) long and 10.7 cm tall (4.2 in). 
IBM introduced the second-generation Enhanced Graphics Adapter (EGA) in 1984, 
which superseded and exceeded the capabilities of the CGA (Fig. 3.10). The EGA 
was then superseded by the VGA standard in 1987.
The EGA established a new industry. It was not an integrated chip; however, its 
I/O was well documented, and it became one of the most copied AIBs in history 
(Fig. 3.11). A year after IBM introduced the EGA AIB, Chips and Technologies 
came out with a chipset that duplicated what the IBM AIB could do. Within a year,

110
3
1980–1989, Graphics Controllers on PCs
Fig. 3.10 IBM EGA add-in board. Notice the similarity to the CGA in form factor and layout 
(Courtesy of Vlask)
the low-cost EGA chips had captured over 40% of the market. Other chip companies 
such as ATI, NSI, Paradise, and Tseng Labs also produced EGA clone chips and 
fueled the explosion of clone-based boards. By 1986, there were over two dozen 
such suppliers, and the list was growing. Even the clones got cloned, and Everex 
took a license from C&T so it could manufacture an EGA chip for its PCs. 
The EGA controller was not anything special, really. It offered 640 × 350 reso-
lution with 16 colors (from a 6-bit palette of 64 colors) and a pixel aspect ratio of 
1:1.37. It had the ability to adjust the frame buffer’s output aspect ratio by changing 
the resolution, giving it three additional hard-wired display modes: 640 × 350 w/2 
colors, with an aspect ratio of 1:1.37; 640 × 200 w/16 colors and a 1:2.4 aspect ratio; 
and 320 × 200 w/16 colors and a 1:1.2 aspect ratio. Some EGA clones extended the 
EGA features to include 640 × 400, 640 × 480, and even 720 × 540, along with
Fig. 3.11 The integrated EGA controller reduced the size of the AIBs of the era (Courtesy of VGA 
Museum) 

3.3 IBM Graphics History (1981–1990)
111
Fig. 3.12 The 4-bit RGBI 
palette added an intensity bit 
hardware detection of the attached monitor and a special 400-line interlace mode to 
use with older CGA monitors. 
The big breakthrough for the EGA, and why it attracted so many imitators, was 
that its graphics modes were bitmapped planar instead of the previous-generation 
interlaced CGA and Hercules AIBs. The video memory was divided into four pages 
(except 640 × 350 × 2, which had two pages), one for each component of the RGBI 
color space (Fig. 3.12) [14]. 
Each bit represented one pixel. If a bit in the red page was enabled and none of 
the equivalent bits in the other pages were, a red pixel appeared in that location on 
screen. If all the other bits for that pixel were also enabled, it would become white, 
and so forth. 
The EGA moved us out of character-based graphics and into true bitmapped 
graphics, based on a standard. Similar things had been accomplished with mods to 
micros such as Commodore PET and Radio Shack TRS80, and direct from micro-
computer manufacturers likef IMSI and Color Graphics, but they did not use an 
integrated VLSI chip. The EGA would be the last AIB to have a digital output. Its 
successor, the VGA, would have analog signaling and a larger color palette. 
When Autodesk’s AutoCAD and other graphics applications began to appear in the 
mid-eighties, dual-monitor arrangements became attractive. In the DOS era, users 
of graphics applications used Hercules for a second display because of its higher 
resolution (720 × 350 compared to EGA’s 640 × 350). Several software packages 
such as AutoCAD included software drivers to support a Hercules AIB to display 
user interface (UI) elements and dialog boxes [15]. The Hercules AIB was the ﬁrst 
thirdparty device spawned by the PC, and it used the same connector as a CGA or 
EGA AIB and the same IBM monitor. 
3.3.3 
EGA Begets VGA to XGA 
The IBM PC, the personal/micro, created a new segment or category— 
consumer/commercial. The users in the commercial segment were not too concerned 
with high resolution, and certainly not graphics performance. Applications were 
appearing for the IBM PC that demanded certain graphics qualities. Lotus 1-2-3 
launched in 1983 and took advantage of the IBM PCs expanded memory and multi-
color display. At the same time, desktop publishing was coming into its own with 
PageMaker and Ventura. These apps attracted a large market and created a demand 
for very high resolutions. But the volume market was commercial and consumer.

112
3
1980–1989, Graphics Controllers on PCs
Even the latter segment was subdivided. A new class of consumers made them-
selves known. They were buying machines speciﬁcally for games. They wanted 
high resolution and performance, but they famously did not want to pay the price the 
professional graphics (i.e., workstation) users were being charged. Wing Commander 
introduced in 1990 by Origin Systems is an early example of a new class of games 
emerging for the PC that taxed the system and required better graphics. These games 
were interactive, thrilling, and pushed the system to produce vibrant graphics on the 
display. 
3.3.4 
The IBM Professional Graphics Controller—PGC 
(1984) 
When the NEC 7220 and Hitachi 63,484 ACRTC graphics controllers (discussed in 
the previous chapter) went to the professional market in 1984, IBM, the industry 
leader and standard-setter, recognized an opportunity, and in the same year it intro-
duced the commercial/consumer-class EGA. In addition, it also introduced a profes-
sional graphics AIB: the PGC often called the Professional Graphics Adapter and 
sometimes Professional Graphics Array [16].  The PGC  offered a high resolution of  
640 × 480 pixels with 256 colors out of a palette of 4,096 colors. The refresh rate 
was 60 Hz. Like the EGA, the PGC was not an integrated chip. 
The PGC consisted of three interconnected PCBs and contained a graphics 
processor and memory (see Fig. 3.13). Targeted for programs such as CAD and 
page layout, the PGC was at the time of its release the most advanced graphics board 
for the IBM XT (Fig. 3.13).
The PGC had a simple graphics controller chip and used an external DAC and 
discrete logic chip for many other functions and lookup tables, as illustrated in 
Fig. 3.14.
The PCG supported 640 × 480 graphics and produced 256 colors from a palette 
of 4096. The PGC had two modes of operation: CGA (320 × 200) and native. The 
PGC’s matching display was the IBM 5175, an analog RGB monitor that was unique 
to it and not compatible with any other graphics board. 
Not widely used in commercial and consumer-class PCs, the PCG’s price tag of 
$4,290 compared favorably to a $50,000 dedicated CAD workstation of the time, 
even including the cost of a PC XT model 87 ($4,995). At today’s prices, the price 
of that AIB would be about $10,000. 
In the late 1980s, the original IBM peripheral bus known as Industry Standard 
Architecture (ISA) had evolved from a 4.7 MHz 8-bit bus to an 8 MHz 16-bit and 
by virtue of its standardization, opened a gateway for third-party AIB manufac-
turers. The clone PCs and accessories were marginalizing IBM’s core PC business, 
and the company wanted to halt that. In 1987, IBM diverted from its commitment 
to openness and reasonable licensing policy and introduced a new PC, the PS/2, 
with a proprietary OS (OS/2) and system bus—the MicroChannel—which was not

3.3 IBM Graphics History (1981–1990)
113
Fig. 3.13 Three-board-set of IBM’s Professional Graphics Controller (PGC) (Courtesy of John 
Elliot Vintage PCs)
Fig. 3.14 Block diagram of IBM PGC with microprocessor and graphics emulation
backward-compatible with ISA boards. The 8514/A high-resolution graphics adapter 
was the ﬁrst AIB for the 10 MHz Micro Channel.

114
3
1980–1989, Graphics Controllers on PCs
3.3.5 
The IBM 8514/A (1987) 
IBM discontinued the PGC in 1987, replacing it with the much-higher-resolution 
8514 and breaking with the acronym description of AIBs [of something] GA. The 
8514 could generate 1024 × 768 pixels at 256 colors and 43.5 Hz interlaced. The 
8514 was a signiﬁcant development and IBM’s ﬁrst integrated high-resolution VLSI 
graphics chip. 
The 8514/A was IBM’s ﬁrst discrete graphics coprocessor. 
IBM for a long time offered two levels of display capabilities, one for general-
purpose business users doing word processing, database entry, and Lotus spread-
sheets, and one for engineering users—the latter always having higher resolution 
and more expensive monitors and controllers. 
Introduced with the IBM Personal System/2 computers in April 1987. 
Rumors of the 8514/A project began circulating as early as 1985. The chip was 
developed in Hursley, UK, not too far from the Texas Instruments Bedford develop-
ment center, the birthplace of the popular TSM34010. The famous Video Graphics 
Array (VGA) and 8514/A chips and add-in boards (AIBs) were all designed at IBM 
Hursley, as were the monitors. 
The 8514/A was an optional upgrade to the MicroChannel architecture–based 
PS/2’s VGA and was introduced within 3 months of the PS/2’s introduction 
(Fig. 3.15). 
The 8514/A (see Fig. 3.16) was the ﬁrst ﬁxed-function graphics accelerator for 
PCs from IBM with the support of 1024 × 768 resolution and up to 256 colors. The 
basic 8514/A with 512 KB VRAM supported only 16 colors; the 512 KB memory 
expansion brought the total to 1 MB VRAM and supported 256 colors [17].
Fig. 3.15 IBM’s 8514/A AIB (lower) and memory board (above) formed a sandwich (Courtesy of 
os2museum.com) 

3.3 IBM Graphics History (1981–1990)
115
Fig. 3.16 Block diagram of 
IBM’s 8514/A in system 
Along with the introduction of the 8514/A was a 16 in. 1024 × 768 CRT monitor 
for the PS2. The 8514 was the monitor; 8514/A was the AIB with the “/A” desig-
nating adapter. The 8514/A was a PS/2 accessory, and IBM did not produce any ISA 
versions. however, several clone suppliers did build 8514/A equivalent AIBs for the 
ISA bus. In 1990, IBM replaced the 8514/A replaced with the XGA, which was like 
VGA, and 8514/A rolled into one. 
What made the 8514/A interesting was the accelerator, which was known 
as the draw(ing) engine. The 8514/A was the ﬁrst widespread ﬁxed-function 
accelerator and therefore relatively inexpensive, and it was a fast accelerator for 
the time. Other graphics accelerators of the period used the Texas Instruments 
TMS34010/TMS34020 chips—a RISC processor running at around 50 MHz. They 
were more ﬂexible but more complex to program, more expensive, and generally 
about as fast as the simpler 8514/A. 
The 8514/A introduced a 2D accelerator design which was a de facto standard for 
several years. It included drawing commands for rectangle and area ﬁlls, BitBlts, and 
line drawing with X/Y coordinates. All the drawing operations could be trimmed to 
a clipped (scissor) rectangle. 
The 8514/A was the beginning of large-scale integration graphics chips and 
compared to the graphics controller of the PGC, the 8514/A-controller chip was 
huge for the time. 
The 8514/A was capable of four new graphics modes not available in VGA. IBM 
named them the advanced function modes and there were options in addition to 640 
× 480. The three other modes got up to 1024 × 768 pixels; however, somewhat 
ironically, it did not support the conventional alphanumeric or graphics modes of the 
other video standards since it executed only in the advanced function modes. In a 
typical system, VGA automatically took over when a standard mode was called for 
by an application or the OS [18]. 
As a result, an interesting feature of the 8514/A was that a system containing it 
could operate with two monitors. In this case, the usual setup was to connect the 
8514 monitor to the 8514/A, and a standard monitor to the VGA. Figure 3.17 shows 
the connection of a VGA and an 8514/A in a system, illustrated in it.

116
3
1980–1989, Graphics Controllers on PCs
Fig. 3.17 The organization 
of a VGA/8514/A system 
Although often cited as the ﬁrst PC mass-market ﬁxed-function accelerator, IBM’s 
8514/A was not the ﬁrst PC AIB to support hardware acceleration. That distinction 
goes to NEC µ7220-based AIBs. 
Up until the 8514/A’s introduction, PC graphics acceleration was relegated to 
expensive workstation-class graphics coprocessor boards. One could get copro-
cessor boards using special CPU or digital signal processor chips, which were 
programmable. However, ﬁxed-function accelerators such as the 8514/A sacriﬁced 
programmability for a better cost/performance ratio. 
At the time, the IBM Display Adapter 8514/A sold for $1,290. 
3.3.6 
IBM VGA (1987–1991) 
IBM’s Video Graphics Array (VGA) was the most signiﬁcant graphics chip ever to 
be produced in terms of volume and longevity. The VGA was introduced with the 
IBM PS/2 line of computers in 1987, along with the 8514. The two AIBs shared an 
output connector that became the industry standard for decades, the VGA connector, 
which as mentioned earlier in this chapter began the era of analog displays. An analog 
signal gave a much greater range capability of color gradations. The popularity of the 
VGA connector was the catalyst for the formation of the Video Electronics Standards 
Association (VESA) in 1989 to ensure compatibility among the different products. 
This too is a signiﬁcant device and will be covered separately in a later section. It 
is listed here to show the complexity of the market at the time and how things were 
rapidly changing. 
The VGA was the most popular graphics chip ever. 
It is said about airplanes that the DC3 and 737 are the most popular planes ever 
built. The 737, in particular, is the best-selling airplane ever. The same could be said 
for the ubiquitous VGA and its big brother the XGA. XGA will be discussed in 
depth in 4.1. IBM XGA (1990). The VGA, which can still be found buried in today’s 
modern GPUs and CPUs, set the foundation for a video standard and an application

3.3 IBM Graphics History (1981–1990)
117
Fig. 3.18 IBM’s highly integrated motherboard-based VGA chip (Courtesy of Wikipedia) 
programming standard. XGA expanded the video standard to a higher resolution and 
with more performance [19]. 
On April 2, 1987, when IBM rolled out the PS/2 line of personal computers, 
one of the hardware announcements was the VGA display chip, a standard that has 
lasted for over 25 years. While the VGA was an incremental improvement over its 
predecessor EGA (1984) and remained backward compatible with the EGA as well 
as the earlier (1981) CGA and MDA, its forward compatibility is what gives it such 
historical recognition (Fig. 3.18). 
The IBM PS/2 Model 80 was the ﬁrst computer from IBM built around Intel’s 
386 CPU and was used to introduce several new standards. Most notable were the 
onboard VGA graphics with 256 kB RAM, the 32-bit bus Microchannel Architecture 
(MCA), card identiﬁcation and conﬁguration by BIOS, and RGB video signal route 
through. The MCA could accommodate the previous-generation 8514/A graphics 
board and route its output through the VGA chip on the motherboard. 
One of the signiﬁcant features of the VGA was the integration of the color lookup 
table (cLUT) and digital-to-analog converter (DAC). Before the VGA, LUT-DACs, 
as they were called, were separate chips supplied by Brooktree, TI, and others as 
mentioned at the beginning of this chapter. Those products would become obsolete, 
but it did not happen overnight. The integrated logic of the VGA also contained the 
CRT controller and replaced ﬁve or more other chips; only external memory was 
needed. The VGA showed the path to future fully integrated devices. 
The VGA also sparked a new wave of cloning and made the fortunes of several 
companies such as Cirrus Logic, S3, Chips and Technologies, and three dozen others. 
The IBM 5162, more commonly known as the IBM PC XT/286, was an extremely 
popular PC and used a 16-bit expansion bus, which allowed upgraded graphics boards

118
3
1980–1989, Graphics Controllers on PCs
to be plugged in, replacing the IBM EGA board. Because the PS/2 used the MCA, 
some board manufacturers offered a board with two tabs, one for ISA and one for 
MCA. And shortly later in 1988, the Extended Industry Standard Architecture (EISA) 
bus for IBM PC-compatible computers was introduced, with MCA and ISA signaling. 
It was developed by a consortium of PC clone vendors (the “Gang of Nine”) as a 
counter to IBM’s use of its proprietary MicroChannel architecture (MCA) in its PS/2 
series, and boards appeared with tabs for both ISA and EISA sockets (Fig. 3.19). 
The basic system video was generated by what IBM referred to as a Type 1 
or Type 2 video subsystem, that is, VGA or XGA. The circuitry that provided the 
VGA function included a video buffer, a DAC, and test circuitry. Video memory was 
mapped as four planes of 64 Kb by 8 bits (maps 0 through 3). The video DAC drove 
the analog output to the display connector. A test circuit was used to check for the 
type of display attached, color or monochrome. 
The auxiliary video connector allowed video data to be passed between the video 
subsystem and an adapter plugged into the channel connector. That was a common 
technique up until the late 1990s. Companies offering higher-resolution and/or 3D-
capable graphics chips did not include a VGA controller to save costs and assumed 
a VGA controller would already be in a system as a default. IBM did not provide 
any high-resolution graphics drivers for the VGA so higher-resolution and higher-
performance AIBs were added by the users. 
Figure 3.20 is a block diagram of the VGA adapted from the IBM VGA XGA 
Technical Reference Manual [20].
The original VGA speciﬁcations deviated from previous controllers by not 
offering hardware support for sprites. Sprites are 2D images or animations overlaid 
into a scene. They are the non-static elements within a 2D game, moving indepen-
dently of the background, often used to represent player-controlled characters, props, 
enemy units, etc.
Fig. 3.19 A VGA board with EISA tab (top) and ISA tab (bottom); note VGA connector on each 
end of the board (Courtesy of ELSA/Wikipedia) 

3.3 IBM Graphics History (1981–1990)
119
Fig. 3.20 IBM VGA block 
diagram
The clock was selectable at 25.175 MHz or 28.322 MHz for the master pixel clock, 
but the usual line rate was ﬁxed at 31.46875 kHz. The VGA speciﬁed a maximum of 
800 horizontal pixels and 600 lines, which was greater than the 640 × 480 monitors 
that were being offered at the time. 
Refresh rates could be as high as 70 Hz, with a vertical blank interrupt (not all 
the clone boards supported that to cut costs). 
The chip could support a planar mode (up to 16 colors; four-bit planes) and a 
packed-pixel mode (256 colors; mode 13 h as it was commonly referred to). The 
chip did not have the BitBlt capability (i.e., a blitter) for transferring bit blocks from 
main memory to display memory instead it supported very fast data transfers via 
VGA latch registers. There was some primitive Raster Ops support, a barrel shifter, 
and something IBM called hardware smooth scrolling support, which was just a bit 
of buffering. 
A barrel shifter is a digital circuit that can shift a data word by a speciﬁed number 
of bits without a CPU. A common usage of a barrel shifter is in the hardware imple-
mentation of ﬂoating-point arithmetic. In today’s modern GPUs, there are thousands 
of 32-bit ﬂoating processors (Fig. 3.21).
The VGA speciﬁcation included a resolution, a physical connector speciﬁcation, 
and video signaling. It is still supported today, and one can ﬁnd projectors with 
VGA connectors, which require an adapter cable when used with newer computers 
or graphics boards. 
In addition to the clone chip suppliers, several other companies incorporated the 
VGA structure into their chips (Table 3.1).

120
3
1980–1989, Graphics Controllers on PCs
Fig. 3.21 The ubiquitous 
15-pin VGA connector
Table 3.1 Some of the VGA 
clone suppliers 
ATI AMD
IIT
S3 graphics 
Chips and 
technologies 
Intergraph
SiS 
Cirrus logic
LSI
Texas instruments 
Cornerstone 
imaging 
Matrox
Trident 
microsystems 
Genoa
NEC
Tseng labs 
Headland 
technologies 
Oak technology
Video 7 
Hercules
Paradise 
systems/western 
digital 
Winbond 
Hualon
Plantronics 
ATI AMD
ReaItek 
No other chip has had a profound impact on the computer business that VGA has 
had, and the industry owes a great debt to IBM for developing it. Sadly, IBM did not 
proﬁt as much from their invention as other suppliers did. 
While VGA was capturing the mainstream, SGI was trying to enter the PC market 
from the high-end. 
3.3.7 
Those Clones 
In the late 1980s and early 1990s, several companies reverse-engineered the 8514/A 
and offered clone chips (i.e., software compatible) with ISA support. Prominent 
among those were Paradise Systems’ (which was acquired by Western Digital) 
PWGA-1 (also known as the WD9500), Chips and Technologies’ 82C480, and ATI’s 
Mach 8 and later Mach 32 chips. In the early 1990s, compatible 8514 boards were 
also based on TI’s TMS34010 chip. All the clones were faster, due in part to new 
higher-density VRAM chips, and as a result pushed the display resolution up to 1280 
× 1024 with 24-bit, 16 million colors—truly a workstation in a PC. 
IBM had one more graphics controller effort, the XGA, which would be the 
superset of the VGA and 8414/A.

3.4 The Market Expands (1986–1987)
121
3.3.8 
IBM Summary 
The EGA can be seen as the foundation controller that birthed an industry of third-
party graphics controllers (Fig. 3.22). It became the graphics chip of the commercial 
and consumer PC graphics market ending the segmentation of users. 
By 1984, the computer market had been consolidated into two main platforms, 
PCs (which included the Apple computer) and workstations. Microcomputers had 
died off in the early 1980s due to the introduction of the PC. Gaming consoles stayed 
as living-room-TV-based devices, and big machines called servers were replacing 
what had been mainframes. Supercomputers were still being produced at the rate 
of three or four a year. All those machines used some type of graphics, and a few 
graphics terminals were still produced to serve the small but consistent high-end 
markets. However, by 1988 they all used standard graphics chips, sometimes several 
of them. 
The EGA speciﬁcation was the catalyst for a new breed of companies. They were 
nimble and fabless and they pushed the industry forward. One such company, AMD, 
is still with us, having acquired the pioneer graphics company ATI (and a EGA 
clone-maker). 
3.4 
The Market Expands (1986–1987) 
The success of the IBM PC with its open bus structure, and dozens of new applications 
attracted many new companies to the market to develop graphics controllers and 
AIBs. New companies (start-ups) emerged, and products showed up a year or two 
later. Among them were Acer Labs (Ali), Headland Technology, IIT, Macronix, Oak 
Technology, Silicon Engines, SiS, Trident, ULSI, and VIA. 
Established companies like ATI, Intel, Matrox, NEC, and Texas Instruments 
continued to bring out new products as well. By 1996, there would be over 70 
graphics controller chip makers in the PC market and specialized professional and 
other platform markets.
Fig. 3.22 History of VLSI 
graphics chips 

122
3
1980–1989, Graphics Controllers on PCs
Fig. 3.23 The Intel SBX275 video graphics controller with 82,720 chip (Courtesy of Multibus 
International) 
3.5 
Intel’s Pre-GPU History (1983–2003) 
Intel tried ﬁve times since 1983 and once more in 2020. 
As the largest semiconductor company, Intel’s efforts and contributions to computer 
graphics run throughout the history of the industry. Until 2018, the company never 
had a dedicated graphics processor group. As a result, its efforts were inconsistent. 
This section ties them together up to the introduction of the GPU rather than list them 
in chronological order, as is done for the other devices because of the inconsistency of 
design, and the company’s apparent opportunistic efforts to offer a graphics solution. 
The following is a brief review of some of those efforts. 
3.5.1 
82720 (1983) 
In 1982, there were a number of companies chasing the computer graphics market. 
NEC would signiﬁcantly change the heretofore specialized and expensive computer 
graphics industry. NEC Information Systems, the U.S. division of Nippon Electric 
Company (now NEC), introduced the µPD7220 Graphics Display Controller. NEC 
started the project in 1979 and delivered a paper on it in 1981 at the IEEE International 
Solid-State Circuit Conference in February 1981 [21]. 
Intel licensed NEC’s graphics, and in June 1983, the company brought out the 
82,720, a clone of the µPD7220; it rolled out its iSBX 275 multibus-based add-in 
graphics board (AIB) with the chip later that year (Fig. 3.23). Intel continued to offer 
the product up to 1986. You can read more about the venerable µPD7220 [22]. 
3.5.2 
82786 (1986) 
Intel saw the rise in discrete graphics controllers such as NEC’s µPD7220, Hitachi’s 
HD63484, and the several clones of IBM’s EGA and concluded they ought to be the

3.5 Intel’s Pre-GPU History (1983–2003)
123
ones ﬁlling that socket. Intel’s intention always was and still is to provide every bit 
of silicon in a PC, and a graphics controller would be no exception. 
In 1986, the company introduced the 82,786 as an intelligent graphics coprocessor 
that would replace subsystems and boards that traditionally used discrete components 
and/or software for graphics functions. It was designed for any microprocessor but 
targeted at Intel’s l6-bit 80,186 and 80,286 and 32-bit 80,386 (Fig. 3.24). 
The 82,786 was a VLSI graphics coprocessor. “One of the key hardware extensions 
that support the speed needed to do graphics and text is a graphics coprocessor,” said 
Bill Gates at the time. The 82,786 used VRAM, and Intel said it could provide 
virtually unlimited color support and resolution [23]. 
Intel’s 82,786 was available in a single 88-pin grid array or leaded carrier. It 
contained a display processor with a CRT controller and a bus interface unit with a 
DRAM/VRAM controller supporting 4 MB of memory. Intel was in the game [24]. 
Intel sold the chip as a merchant part, and independent AIB suppliers built boards 
with it. In 1987, two companies were offering three AIBs using the 82,786, and 
by 1988, 10 companies were offering 15 AIBs using the chip. The chip was not as 
powerful compared to others on the market, most noteworthy being Texas Instru-
ments’ TSM34010 (discussed in 2.11 T1 34010 (1986)), nor as popular as the IBM 
VGA and its many clones (discussed later in this chapter). Intel withdrew the chip 
with the introduction of the 86,486 microprocessor in 1989.
Fig. 3.24 Intel 82,786 die shot (Courtesy of https://Commons.wikimedia.org) 

124
3
1980–1989, Graphics Controllers on PCs
3.5.3 
i860 (1989) 
The i860 (code named N10) had several unique elements for the time. It employed 
a very-long-instruction-word (VLIW) architecture with high-speed ﬂoating-point 
operations. It had a 32-bit ALU core along with a 64-bit FPU that consisted of three 
elements: a multiplier, an adder (ADD), and a graphics processor. 
The graphics unit was unusual for the era. It had a 64-bit integer unit that used the 
FPU’s registers as eight 128-bit registers. Also, it had some SIMD-like commands 
and a fundamental 64-bit integer math capability. The i860 inspired the Multi-Media 
Extensions—MMX in Intel’s Pentium and subsequent processors (Fig. 3.25). 
Intel found customers beyond the PC world. Steve Jobs’ company NeXT built 
the NeXTdimension, an accelerated 32-bit color AIB, sold by NeXT from 1991 that 
gave the NeXTcube color capabilities with PostScript planned. It used the i860 to 
run a complete PostScript stack. However, the company did not ﬁnish the PostScript 
part of the project, so it ended up just moving color pixels around. 
Truevision, an add-in board maker founded in 1987 when AT&T spun off their 
Electronic Photography and Imaging Center (EPICenter), produced an i860-based 
accelerator board intended for use with their Targa and Vista frame buffer AIBs. Pixar 
made a custom version of RenderMan to run on the AIB that ran approximately four 
times faster than the 386 host. Another example was SGI’s RealityEngine, which 
used several i860XP processors in its geometry engine. 
That type of use slowly disappeared as general-purpose CPUs started to match 
i860’s performance and as Intel turned its focus to Pentium processors for general-
purpose computing. Intel terminated the i860 project in the mid-1990s and followed
Fig. 3.25 Intel i860 
microprocessor (Courtesy of 
Wikipedia) 

3.5 Intel’s Pre-GPU History (1983–2003)
125
with the i960. The company merged it with the FPU to become the i960. Several 
graphics terminals used the chip. 
The University of North Carolina, where the Pixel Planes project began in 1980, 
developed a commercial version of the system they called PixelFlow, which used 
i860s for geometry calculations to feed the Pixel Planes’ enhanced SIMD memory 
(see page 2–70 for more detail on PixelFlow). 
3.5.4 
i740 (1998) 
One of the more Byzantine product developments, however, was Intel’s i740 (code 
named Auburn). The graphics engine came out of simulator development in 1995 
when Martin Marietta and Lockheed merged to form Lockheed Martin Corporation. 
Lockheed Martin decided to market the graphics technology in January 1995. 
They set up the Real3D division to produce the R3D/100 chip. One of their 
ﬁrst customers was Sega. This led to the company’s most successful product run, 
designing the 3D hardware used in over 200,000 Sega Model2 and Model3 arcade 
game machines—two of the most popular systems in history. 
In 1997, Intel purchased notebook graphics chipmaker Chips and Technologies 
for $430 million. However, no products taking advantage of technology acquired in 
the merger ever emerged. 
In May 1996, Real3D formed a partnership with Intel and Chips and Technologies 
to introduce technology for an AIB for PCs for project Auburn. In August 1997, Intel 
formally introduced the new AGP bus for graphics AIBs, designed to replace the 
PCI bus and be faster. The Auburn project resulted in the AGP-bus-based Intel i740 
graphics processor, which Intel released in 1998, shown in Fig. 3.26. At the time, 
Intel purchased a 20% minority interest in Real3D.
The i740 combined the 2D of C&T with the 3D rendering engine from Real3D. The Pentium 
processor did the T&L work. Taking advantage of the AGP 2x texturing bandwidth, the i740 
was able to achieve 1.3 Giga bytes per second (GB/s) peak bandwidth. The i740 used AGP 
texturing. Other 3D graphic controllers of the time used AGP texturing to upload textures 
through a DMA mode if the local video memory was not enough. Intel’s i740, however, didn’t 
do texture fetching from local memory and used AGP memory. As a result, its performance 
suffered and the bus would get saturated. Intel put in deep buffered the pipeline to hold back 
host latencies. They also used sideband addressing which allowed the transfer of texture 
data and addresses at once. In 1997 and 98 AGP was just coming into the market. Therefore, 
some companies tried making a PCI bridge to use the i740 chip. That had limited success 
and the whole project was failing. 
By late 1999, Intel did two things: it shut down the i740 project and acquired the assets 
of Real3D from Lockheed Martin. As Real3D crumbled, ATI hired many of the remaining 
employees and opened an ofﬁce in Orlando, Florida [25]. 
On another front, before the sale of its assets to Nvidia, 3dfx had sued Real3D over 
patent infringements. Intel settled the issue by selling all the intellectual property back 
to 3dfx, which ultimately ended up in Nvidia’s hands. Also in 1999, as part of a legal 
settlement, Nvidia acquired SGI’s graphics development resources, which included a 10%

126
3
1980–1989, Graphics Controllers on PCs
Fig. 3.26 Intel i740 prototype AIB with an AGP connector (Courtesy of www.SSSTjy.com)
share in Real3D. That triggered a series of lawsuits, joined by ATI. The two companies were 
involved in lawsuits over Real3D’s patents until a 2001 cross-licensing settlement. 
Intel exited the discrete graphics chip market for PCs it had entered less than 
18 months earlier to fanfare, but dismal sales. Intel continued on to produce inte-
grated graphics chipsets, which combine a standard PC core chipset which included 
a memory controller, instruction cache, bus interfaces, and more with a graphics 
processor (see i810, next section). Intel’s integration of these parts ultimately led 
to Intel’s CPUs with integrated graphics processors in 2010. Along the way, these 
developments led to consumer-priced computers for under $1,000. 
The i740 experience caused hard feelings at Intel, and many in the company said 
Intel would never again venture into discrete graphics again. Then in 2007, Intel tried 
once more with the Larrabee multi-multiprocessor project. That too ended in failure, 
and management said, “Never again” (again). Most of those people are gone, and in 
2019, the company launched a new generation of a discrete graphics chip family, the 
Xe. 
3.5.5 
i810 (1999) 
The industry expected Intel’s integrated graphics controller (IGC) the i810 IGC (the 
82,810, code named Whitney) to be the integrated version of the i740 (see block 
diagram in Fig. 3.27). That belief came from Intel’s hints that i810 would have the 
core of the company’s upcoming low-to-mid-range graphics chip i752. The i752 was

3.5 Intel’s Pre-GPU History (1983–2003)
127
the successor of the i740 launched in April 1999. Intel built it on the 150-nm process 
and the Portola graphics processor [26]. 
Instead of other system components having to communicate with the 
memory/CPU via the PCI bus, Intel’s Accelerated Hub Architecture allowed direct 
communication between the memory, CPU and disks, peripherals, plus a graphics 
adapter. 
The i810 was one of Intel’s most successful chipsets with graphics and was consid-
ered by many as a breakout product. It demonstrated Intel could design and produce 
useful graphics and at a competitive price. It forced competitors to lower their prices, 
and therefore their margins, and therefore their R&D budgets for the next-generation 
products. And it established Intel as the market leader in IGCs (Fig. 3.28).
Intel manufactured it and tweaked versions of it for 3 years. It was a shared memory 
architecture with direct access to the system’s main memory via the memory bus.
Fig. 3.27 Intel i810 IGC 

128
3
1980–1989, Graphics Controllers on PCs
Fig. 3.28 Intel 810 chipset 
(Courtesy of Wikipedia)
3.5.6 
Extreme Graphics (2001) 
Intel began its Extreme Graphics family with the i830 (code named Amador) chipset. 
Designed for Pentium III-M, the systems used old SDRAM memory, limiting them 
to 1066 MB/s bandwidth, like earlier GPUs. The clock rate dropped from the i815’s 
230–166 MHz on the Amador chipsets to conserve power and reduce heat output. 
In 2002, Intel introduced the i845 chipset (code named Brookdale). It was the start 
of Intel’s push to establish its iGPUs as serious contenders in the gaming market. The 
i845 (shown in Fig. 3.29) had a new 32bpp graphics hardware engine. It employed 
Intel’s Dynamic Video Memory Technology (DVMT) and Intel Zone Rendering. 
Zone Rendering reduced memory bandwidth requirements by segmenting the frame 
buffer into zones. It sorted the triangles into memory by zone and processed each 
zone’s memory. 
Fig. 3.29 Intel’s i845 
Northbridge chipset (left) 
was surprisingly small 
(Courtesy of Wikipedia)

3.6 Tseng Labs (1983–1997)
129
The i845 had two texture units and could do four textures in a single pass. Its ﬁll 
rate was 200 to 266 Mpixels/s and was DirectX 8.1 compatible. It did not have any 
vertex shaders (leaving that to the CPU) but did support bump mapping, environment 
mapping, and anisotropic ﬁltering. The chipsets were the fastest members of Intel’s 
family of IGCs, thanks in a large part to support for DDR333 memory and a 533 MHz 
FSB speed. 
3.5.7 
Intel Summary 
Intel has been described as (accused of?) having a split personality around graphics. 
One side says × 86 is king and there shall be no pretenders to the throne. This is 
proven, say the accusers, in Intel’s repeated canceling of non-×86 products and its 
expensive and quixotic pursuit of Larrabee which essentially relied on multiple × 
86-based chips optimized for graphics processing (Larrabee is discussed in detail 
in Book three, New GPUs). The other side says Intel wants to ﬁll all sockets with 
home-built products and has all the skills and technology necessary to build a GPU 
or graphics controller. 
Intel’s ﬁrst effort was with licensing the NEC µPD7220 and introducing it as the 
Intel 82,720 Graphics Display Controller [27]. The company also licensed graphics 
technology from Imagination Technologies and AMD. The company also developed 
a couple of discrete graphics controllers (the i740 with technology that evolved from 
the company’s Real3D) described in the preceding sections. 
In 2009, Intel integrated a GPU in the same chip with the ×86 CPU in the Clarks-
dale (80,616) processor. It was marketed as the Core i3, i5, and Pentium, and Intel 
was the ﬁrst to offer such an integrated device. 
3.6 
Tseng Labs (1983–1997) 
Could have been the ﬁrst GPU supplier. 
Tseng Labs could have been the ﬁrst company to introduce a GPU. In 1983, Jack 
Hsiao Nan Tseng and John J. Gibbons started Tseng Laboratories in Newtown, Penn-
sylvania, a small township northeast of Philadelphia. Tseng Labs would design and 
build graphics controllers for AIBs for the IBM PC and compatibles. The company 
operated from 1983 to December 1997, when ATI eventually acquired it. Tseng 
Labs was best known for its ET3000, ET4000, and ET6000 chips, which were VGA-
compatible. The company came up with a clever way to use a conventional DRAM 
frame buffer, saving memory costs. The ET4000 also had a novel high-speed host 
interface that improved throughput. When Microsoft Windows 3.0 came out in 1990, 
the Tseng Lab’s controllers grew in popularity, and the company enjoyed fast growth 
with its GUI accelerators. The industry was moving from the old IBM PC ISA bus

130
3
1980–1989, Graphics Controllers on PCs
to the local bus, and the ET4000 was one of the last AIBs based on ISA. It provided 
the core IP for three generations of ET4000/W32 2D accelerators [28]. 
The ET3000 series came out in September 1987, less than 6 months after the IBM 
VGA announcement. It shipped in high volume for over two years in an era with 50 
companies producing VGA-compatibles. For its day, the ET3000 was feature-rich, 
supporting 1024 × 768 displays with a frame buffer size of 2 MB. 
Those were rough and tumble times. Joe Curley, who has had many jobs in 
the industry and was at the time of this writing Intel’s Vice President and General 
Manager—Intel Software Products and Ecosystem, was Tseng’s vice president of 
marketing (among other roles) at that time. “The ET3000 had a neat feature that 
allowed the controller to tristate all the memory outputs,” said Curley, “the idea 
was we could marry a TI 34,010 or i860 and do shared memory. And folks were 
interested. All you had to do to enable the feature was write a sequence to some 
extended registers in CRTC. Only one issue: there was no enable bit. 
“I thanked God nightly none of our competitors wrote a VGA compatibility 
program that set that bit. It would not affect anyone else—they did not map the 
same registers. But we would get a lobotomy. Yeah, we pulled that from ET4000.” 
In those fast-changing early days, being lucky could be a lot better than being good, 
but Tseng was also very good.” 
Tseng Labs developed several advanced graphics controller features that are still 
in contemporary GPUs. It had the ﬁrst integrated local bus controller and supported 
8- to 24-bit packed-pixel color modes. 
In 1992, IBM began using Tseng’s ET4000/AX in some new PS/1 80486SX 2133 s 
and its Consultant 2155 series PCs for the Canadian and UK markets (Fig. 3.30). 
The W32 followed the ET4000AX, and the W32i would follow the W32. 
With the ET4000/W32, the company introduced a novel image memory access 
(IMA). It offered a high-speed asynchronous input for video or graphics into the 
display buffer. The chip had extended register sets and the ﬁrst local bus graphics
Fig. 3.30 Tseng Labs’ 
ET4000AX (Courtesy of 
Eep386: Wikipedia)

3.6 Tseng Labs (1983–1997)
131
Fig. 3.31 Tseng Labs’ ET4000 block diagram
designs. Furthermore, the W32 could expand a 1-bit-per-pixel (monochrome) pixel 
map into an 8-bit-perpixel map. That was a helpful operation when painting fonts. 
The ET400/W32 block diagram is shown in Fig. 3.31. 
In November 1992, Tseng said they were working on a new version of the chip, 
the W32i. It would have more bandwidth and could take the IMA capability to 1024 
× 768. The company was also working on adding an audio capability [29]. 
However, the new W32i was not coming fast enough, and in 1993, Tseng Labs 
was sued by investors because the company made misleading statements about its 
sales prospects. In February 1993, ET4000 sales slowed down. The plaintiffs alleged 
Tseng Labs had anticipated a steep drop in ET4000 sales due to the arrival of the 
next generation of graphics chips. The plaintiffs argued that despite that knowledge, 
Tseng Labs made misleading statements to the market beginning in February 1993, 
which led investors to believe that sales for the ET4000 would continue to be strong 
and help drive earnings. 
The W32i had planar or linear packed-pixel graphics modes. Tseng Labs claimed 
it provided the desktop computer market with the highest quality 2D images at the 
time because it had true color 24-bit modes (16.7 million colors). 
Using the IMA bus, Tseng Labs created the motion video accelerator category for 
mainstream PCs with a series of video image-processing circuits, branded VIPeR. 
VIPeR chips provided relatively high-quality live and computer-generated video. 
At the 1993 COMputer Dealers’ Exhibition (COMDEX), Tseng Labs was one 
of the ﬁrst companies to show its VIPeR video accelerator with image-processing 
circuits. (VideoLogic in the UK was developing similar technology.)

132
3
1980–1989, Graphics Controllers on PCs
Fig. 3.32 STB ET6000 AIB with 2 MB frame buffer and pad for additional MDRAM (Courtesy 
of Joe Curley) 
When Andy Fischer from Jon Peddie Associates (JPA) mentioned to Joe Curley 
that the name Viper was being used by others, Curley said, “Yeah, but ours stands 
for Video Image ProcEssoR; theirs is named after a Dodge.” 
The ET4000/W32i was a graphics chip compatible with the 8-/16-/32-bit buses 
ISA, EISA, MCA, and CPU local buses. It included a graphical user interface 
(GUI) accelerator and had advanced features for developing imaging and multimedia 
markets. The host interface was the second generation of Tseng Labs’ cache/memory 
management architecture and had nearly ﬁve times greater performance than its 
predecessor, the ET4000/AX. A unique feature of ET4000 architecture was that the 
design maximized the capabilities of DRAM and eliminated the need for VRAM, 
which reduced costs. Several AIB builders like STB used the chip. The ET600 AIB 
is shown in Fig. 3.32. Toshiba fabricated the chip in their Iwate 650 nm facility. 
The ET4000 was a large chip for its time. The magic of an ET4000 was a clever 
Least Recently Used (LRU) cache. (The cache design got used by a software company 
that licensed it to many PC companies for industry-standard benchmark performance 
testing.) Tseng Labs later migrated the cache to multiple ports, allowing follow-on 
products to handle various independent dynamic bandwidths to access DRAM with 
almost no latency. 
Among the chip’s features were a 256-raster operation capability, a high 
throughput graphics engine for faster hardware-accelerated BitBlt, support for a 
hardware-driven cursor or a second simultaneous display window, an imaging port, 
and memory-to-memory BLTs with masking and pixel ampliﬁcation. Tseng Labs 
claimed at the time that its pixel ampliﬁcation could speed up text printing, color 
expansion, and area ﬁll operations by up to 10 times [30]. 
The ﬁrst VIPeR (Video Image ProcessoR) chip came out in 1994. The VIPeR 
offered high-quality real-time as well as computer-generated video. Key features 
included a professional-quality image scaler and interfaces to popular NTSC/PAL

3.6 Tseng Labs (1983–1997)
133
video decoders. The ﬁrst-generation VIPeR converted YUV1 4:2:2 or 4:1:1 to 24-
bit RGB, while future versions output YUV and allowed color space conversion in 
the SuperVGA. The chips got employed in video products from Jazz Multimedia 
and Matrox. Tsang Labs was successful because it had more sophisticated video-
processing algorithms than its competitors. 
By May 1994, the company had achieved two consecutive quarters of sales 
increases and net income. However, when compared to prior years’ periods, the 
results were not as good. Due to competitive market conditions, the company got 
lower margins on the sales of the W32 family. The company also suffered delays in 
achieving expected cost reductions. 
According to Jack Tseng, “The Company has started to achieve signiﬁcant cost 
reductions on the W32i and the W32p which should begin to have an impact on our 
costs during the second quarter. Further cost reduction efforts are also in progress, 
and additional reductions are possible by the fourth quarter [31].” 
On November 22, 1994, Tseng Labs announced two new products promising 
full-screen, accelerated playback and motion video capture in a lower-cost version 
of VIPeR. 
“We now expect production ramp-up in the second half of 1994,” said Tseng. He 
promised the integrated ET6000 for Q2’95. 
Joe Curley, Tseng’s director of marketing at the time, said, “The VIPeR f/x will 
enable multi-media providers to develop products for sizing and scaling of video 
images to arbitrary window sizes from 16 × 16 pixels to 1024 × 768 in true color 
without dropping frames [31].” 
It was a lower-cost version with 4:1:1 and 1024 × 768 × 24 capability. 
At the time, Tseng and Curley did not want to say much about the new semisecret 
ET6000 device. Tseng said his view was that a virtual port cache was the essence of 
the display controller. He claimed to have been building it since the ET4000. 
The new chip would display multiple windows from multiple sources: telecom, 
graphics, audio, etc. The ET6000 would be a big, fast switcher. One of its unique 
features was that the chip could, according to Tseng, predict the direction and 
frequency of upcoming data. It then could route that data to the appropriate memory. 
Tseng expected a two-giga-bytes-per-second (GB/s) internal data rate switching 
speed. Latency would be within one to two clock cycles at 60 MHz and synchronize 
within one clock cycle. 
Tseng Labs had started 3D development in early 1995. In 1995, the Tseng engi-
neers had spent some time on a homegrown, scalable, programmable architecture 
using very large instruction word (VLIW) parallelism that could execute 2D and 3D 
graphics operations as well as video operations (focusing on MPEG decoding at the 
time). Three-D T&L was not on their feature list at the time, but others might have
1 The Y in YUV stands for “luma,” which is brightness, or lightness, and black and white TVs 
decode only the Y part of the signal. U and V provide color information and are “color difference” 
signals of blue minus luma (B-Y) and red minus luma (R-Y). YUV color space was invented as a 
broadcast solution to send color information through channels built for monochrome signals. 

134
3
1980–1989, Graphics Controllers on PCs
been thinking about it. Much of Richard Selvaggi’s focus was on the pixel-processing 
level and up the pipeline to the triangle setup level. 
At some point, reality took hold that a programmable graphics and video processor 
was not going to be able to compete with hardwired systems, so Tseng engineers 
scaled back their efforts, and that resulted in the ET6300 architecture, but clearly 
Tseng Labs had the idea for some sort of GPU a few years ahead of its time. At that 
time, they were doing triangle setup on a homegrown RISC-like processor and added 
some specialized instructions to accelerate 3D setup—but it was a coprocessor, not 
integrated. 
Tseng said the 6000 would come in two versions, a 3.3- and 5-V class chip. 
Manufactured in 600 nm technology, it could reach 4 GB/s. The 6000 would be a 
fully integrated controller with 135 MHz output LUT-DAC. 
One of the unique features of the ET6000 was the choice to use MDRAM 
for graphics memory. MDRAM provided a large array of small memory banks. 
Combined with a novel fast-paging memory controller in ET6000, Tseng Labs 
demonstrated efﬁcient utilization of over 90% of MDRAM’s theoretical peak band-
width. There would be two independent 16-bit paths to MDRAM, treated as two 
different buses (Fig. 3.33). The new device would have a 16-bit IMA and direct 
system bus support for ISA, PCI, and VLB.
However, Tseng knew big problems lay in front of them. He knew; he just did not 
know what to do about them. 
Tseng commented: 
The primary distinction of the ET6000 series is that it will allow original equipment and 
add-in board manufacturers to offer products with substantially higher performance than is 
currently available with DRAM-based products, including 700 Mbyte of sustained video 
throughput at a cost point not currently available in the market. We believe that the oppor-
tunity to become the leader with this cost-effective, high-performance technology is key 
to the company’s long-term future success as a leader in the graphics and rapidly evolving 
multimedia markets. While we are optimistic about the future prospects for the ET6000, 
we recognize that because of the short product design cycles in the computer industry as a 
whole, the company’s change in product focus could adversely affect revenues in the ﬁrst 
half of 1995 as sales of the current-generation products will likely decline before the ET6000 
is ready for commercial delivery [31]. 
According to Tseng, “It will be very competitive—it is almost like giving all that 
for nothing. Like W32 pricing.” 
When asked about the future, he said, “We will have 3D in the future. There will 
be three separate parts in each family, a media channel and a ﬂat panel version, in 
addition to the ﬁrst unit.”

3.6 Tseng Labs (1983–1997)
135
Fig. 3.33 STB ET6000 block diagram
3.6.1 
Winning Awards Was not Enough 
In November 1995, Tseng Labs unveiled the ET6000 at COMDEX, which won a 
“Best of Show” product award. The AIB could show full-motion, full-screen video 
on PCs and up to four different video windows on a screen. The company claimed the 
chip also had the core technology for 3D graphics, teleconferencing, and advanced 
multimedia products. 
Tseng described the ET6000 device as a 128-bit multimedia graphics and video 
processor with one GB/s bandwidth using MDRAM. It had an integrated 128-bit 
graphics accelerator, a video image processor, a 24-bit LUT-DAC, and a clock gener-
ator in, and would support resolutions and color depths up to 1280 × 1024 at 16.8 
million colors.

136
3
1980–1989, Graphics Controllers on PCs
Tseng Labs once dominated the world of computer graphics; however, competitors 
outpaced the company by introducing more advanced chips and more versions. Add-
in board and computer makers included those chips in their products rather than 
Tseng’s. The result was a dramatic decline in the company’s earnings and its stock 
price. The company had to win some old customers back and get some new ones to 
make its future as bright as its history. Sadly, it did not. 
In the mid-1990s, too many companies ﬂooded the market. Many of them were 
building or said they were making 3D graphics chips. The added competition drove 
down margins and, with them, the R&D budget. The company also misused some of 
its resources, further starving development. 
By 1996, Tseng Labs had lost signiﬁcant market share to S3 Graphics and ATI 
Technologies. And surprisingly, given its novel design leadership, the company was 
late integrating a LUT-DAC into its controllers. It was not until the ET6000 was 
introduced in late 1996 that it had an integrated LUT-DAC. Its competitors had been 
shipping such devices for over a year. Integrating the LUT-DAC lowered the cost 
of an AIB, giving competitors a market advantage over Tseng’s AIB builders. That 
severely hurt Tseng’s competitiveness. 
3.6.2 
It Could Have Been the First GPU 
On November 18, 1996, Tseng Labs announced its plans for a family of 3D graphics 
accelerators. Based on Tseng’s ET6000 design, the ﬁrst of the new 3D processors 
would be named the ET6300 and targeted at Windows 95. It would include an inte-
grated triangle setup engine and ﬂoating-point to ﬁxed-point conversion unit—it 
could have been the ﬁrst integrated single-chip GPU if it had been built. 
By integrating those critical features, Tseng estimated the ET6300 could eliminate 
up to 50% of the CPU workload, freeing valuable cycles for improved software audio 
and video decoding, Eliminating the transfer setup for each triangle would increase 
usable PCI bandwidth and boost triangle rates. Removing this transfer overhead 
allowed next-generation games to model scenes using many smaller triangles for 
greater geometric ﬁdelity and depth of ﬁeld. Balancing the 3D tasks more efﬁciently 
between CPU and graphics accelerator would allow the ET6300 to offer higher 
performance and improved 3D realism to high-performance game and entertainment 
titles of the times. 
Tseng Labs expected to be one of the ﬁrst mainstream graphics suppliers to deliver 
advanced rendering capabilities with an integrated triangle setup engine and high-
speed synchronous memory to the market. The company said limited engineering 
samples for alpha customers would be in the fourth quarter. Production units would 
use a state-of-the-art 350 nm, 3.3 V process. Tseng Labs was planning to introduce 
the ET6300 in early 1997. Additional 3D plans for 1997 also included support for 
Intel’s Advanced Graphics Port (AGP) standard. 
The competition in the 2D and GUI accelerator market slowed Tseng Labs down. 
Lower margins not only crippled R&D, but also stretched the company’s cash ﬂow,

3.6 Tseng Labs (1983–1997)
137
making it difﬁcult for the company to buy parts. Lacking the funds to complete 
the development of its 3D engine (the ET6300), the company’s board of directors 
decided to abandon plans for a next-generation part and chose instead to preserve 
cash and ﬁnd someone to buy the company. That strategy resulted in the sale of the 
company to ATI. 
In December 1996, the reduced sales of Tseng’s 2D products and margin pressures 
created increased operating losses. The company expected the losses to continue in 
subsequent quarters until the ET6300 3D graphics accelerator achieved meaningful 
sales—and that would not happen before the third quarter of 1997. 
Tseng Labs was innovative: crazy, chaotic, immature, maybe—but innovative. 
They did the ﬁrst 132 column alpha/numeric, SuperEGA, SuperVGA, cheap true 
color local bus; the ﬁrst bidirectional over-the-top bus (SLI owes a lot to that); 
and more. Stories abound around Jack Tseng’s seat-of-the-pants management style. 
Ultimately, the company was not agile enough to pivot when necessary, so driven 
were they by the pursuit of 3D. They weren’t alone, other companies would fall on 
similar swords. 
3.6.3 
The End 
In November 1997, the company announced a new strategic plan. Given the lead 
time and research and development costs required to produce new graphics prod-
ucts, the company decided to cease development efforts on all future products. 
However, Tseng said it intended to continue to work with 3D and multimedia graphics 
technologies to position the company for strategic partners and merger candidates. 
In addition to a reported 20–30% staff reduction, Tseng announced that it would 
further reduce staff to just 50 essential employees. 
A week later, Jack Tseng resigned. Another company founder, John J. Gibbons, 
replaced him as president, CEO, and chairman of the board. 
On December 22, 1997, ATI announced it would acquire the graphics design 
assets of Tseng Labs for approximately $3 million. Adrian Hartog, ATI’s VP of 
engineering, said, “This purchase will enable us to augment an already considerable 
3D team and hastens our move to operate concurrent development groups in order 
to maintain our momentum ahead of the pack [32].” 
Under the terms of the purchase agreement, ATI acquired all the graphic design 
assets of Tseng Labs, including speciﬁc hardware and software licenses. It leased 
Tseng’s Pennsylvania facilities. Approximately 40 key employees, including engi-
neering and marketing personnel, accepted offers of employment with ATI’s U.S. 
subsidiary, ATI Research, Inc. 
The programmable approach to 3D setup being worked on by Tseng was made 
obsolete very quickly by dedicated pipelined hardware when the team from Tseng 
joined ATI in December 1997. In fact, the Tseng team owned that portion of the 
pipeline for the ﬁrst Radeon chip. And when the team from Real3D joined ATI in 
1999, they had the T&L technology and took over that section of the pipeline.

138
3
1980–1989, Graphics Controllers on PCs
After the asset purchase by ATI, what Tseng Labs had left was sold to Cell 
Pathways, Inc., in Horsham, Pennsylvania. Cell Pathways acquired Tseng Labs for 
5.5 million shares, valued at about $41 million. 
Jack Tseng was a justiﬁably proud engineer who made mistakes like those of 
other justiﬁably proud engineers who had founded their own companies. A string of 
successes breeds an understandable arrogance that only a series of failures seems to 
cure. Tseng rose to prominence in the EGA era and continued to preside over the 
company’s dominance into the days of the VGA. 
3.7 
SGI’s IrisVision (1988–1994) 
Capitalizing on the success of the Geometry Engine, SGI built a series of great 
graphics workstations. Then, anticipating the ultimate threat from PCs, SGI in 1987 
launched a project to create the Personal Iris workstation and introduced it in 1988. 
Within the workstation was a powerful VME-based graphics AIB board subsystem, 
the TG-V Graphics System. VME was a popular bus and package system used in 
some workstations and many industrial and test equipment systems. 
After seeing SGI’s system, IBM contacted SGI about developing a MicroChannel 
architecture (MCA) bus version of the board-set for IBM’s new RS-6000 Unix work-
station. SGI decided to make an IBM PS/2 MCA version of the AIB and developed a 
three-to-four board sandwiched product in 1990. IBM licensed the TG-V board-set 
and Iris library. However, the TV-G lacked hardware support for high-end IrisGL 
features, such as texture mapping, alpha blending, accumulation, and stencil buffers. 
Bharat Sastri, the director of engineering at SGI, was responsible for the 3D 
graphics adapter SGI developed for the IBM RS/6000, the IrisVision board, Indigo, 
and several other interesting designs. 
The TG-V board-set consisted of two main boards, each with a daughter card. 
The two main boards were tightly connected to ﬁt into two adjacent card slots [33]. 
The general organization of IrisVision shown in Fig. 3.34 is the TG-V MCA version 
of the IrisVision graphics system (adapted from Roger Brown’s former web page) 
[34]. 
The MGE was the base board and contained the host adapter with DMA and the 
T&L engine. Ironically, the board used a Weitek 3132 ﬂoating-point processor with 
20 millions of ﬂoating-point operations per second (MFLOPS) and not the fabled
Fig. 3.34 SGI’s IrisVision block diagram 

3.7 SGI’s IrisVision (1988–1994)
139
SGI geometry engine. There was also an integer processor from SGI (rated at 10 
MIPS). 
It had a memory daughterboard, the MZB. The MGE and MZB used an on-card 
slot connected to the MRV and its optional daughter card via over-the-top ribbon 
cables (Fig. 3.35). 
The MRV raster video board contained the basic 8-bit planes of frame buffer 
memory, two-bit planes of overlay frame buffer, and two bits of window ID bit 
planes for 12 bpp. 
It had an optional 24-bit 3.75 MB DRAM z-buffer daughter card, the MEV2. 
For non-3D applications, the z-buffer saved the contents of the frame buffer as an 
off-screen memory. The MEZ was attached to the main MRV2 board. 
A fully conﬁgured frame buffer had a total of 1280 × 1024 × (32/8) or 5 MB of 
VRAM implemented in 256 K by eight memory chips. It used 256 K VRAM instead 
of 64 K VRAM in the workstation VME version to reduce the board’s size. 
The AIB was a 3D and video-only board and offered a VGA pass-through for 2D. 
In August 1990, SGI added support for stereo vision to the IrisVision board, 
generating an alternate-view 60 frames a second for an effective 120 Hz fps rate. 
In 1987, seeing the PC market slipping away from them due to clones, IBM 
introduced a proprietary operating system, OS2, and a new high bandwidth AIB bus, 
the MicroChannel architecture (MCA). In 1991, SGI introduced the MicroChannel– 
based IrisVision, one of the ﬁrst 3D accelerator AIBs available for the high-end PC 
market. The IrisVision AIB is shown in Fig. 3.36.
A few years later, IBM licensed both the graphics subsystem and the (then-new) 
IRIS Graphics Library (IRIS GL) API for their RS/6000 POWERstation line of 
POWER1–based workstations. 
On September 24, 1991, SGI announced at the A/E/C Conference that it was ship-
ping a DOS Software Developers’ Kit, which provided the IRIS Graphics Library 
(GL) application programming interface to DOS applications via the IrisVision 
board. The Integrated Raster Imaging System Graphics Library was a proprietary
Fig. 3.35 SGI’s MCA-based 
IrisVision board-set 
interconnections (Courtesy 
of SGI) 

140
3
1980–1989, Graphics Controllers on PCs
Fig. 3.36 SGI’s IrisVision AIB circa 1999 (Courtesy of eBay)
graphics API created by Silicon Graphics in the early 1980s. It used IRIS graphical 
workstations to generate 2D and 3D computer graphics. It was evolved into OpenGL. 
In addition, the SDK had a multimedia front end, allowing the creation of walk-
throughs and animations. SGI branded it IrisView, and it used imported Autodesk 
3D.DXF ﬁles. The latest version offered limited texture mapping. When coupled 
with an optional onboard Genlock, frame sequences could output to NTSC or PAL 
videotape and run as an accurate motion video. SGI also included the ability to 
link digitized sound with frames, providing tools for developers to build scripting 
languages and multimedia applications. 
The IrisVision board-set, however, was quite impressive in and of itself. Dynamic 
movement, rotations, walk-throughs, and the like could be viewed virtually in real 
time in wireframe and with surprising speed using Gouraud shaded images. A user 
could stop the wireframe walk-through at any point, and its shaded image generated 
very quickly. It was one of the ﬁrst useful applications seen for the boards that were 
capable of onboard rendering. While other vendors at the conference had similar 
products, the SGI board-set had two main advantages: it was a GL-based product 
from SGI, and it was the only product to provide for sound sync and high-resolution 
rendering. 
There was speculation about what applications might appear using the IrisVision 
product and its impact on Mac-based and other products aimed at rendering, anima-
tion, and multimedia presentations. SGI said it would provide the source code for 
IrisView to any developer. Some GL-based applications ran faster under DOS than 
UNIX. 
IrisVision History (from Roger Brown’s web page):
• 1988: Personal Iris workstation introduced by SGI.
• 1989: OEM MicroChannel version designed and produced for IBM RS-6000 
workstations.
• 1990: Product announced at the California Palace of the Legion of Honor.
• 1991: Product introduced at Fall COMDEX in Las Vegas.
• 1991: Shipments begin; version 2.0 software and SDK released.

3.7 SGI’s IrisVision (1988–1994)
141
• 1992: Product line acquired by Pellucid, Inc., in turn acquired by Media Vision 
Technology. 
The SGI IrisVision was one of the ﬁrst 3D accelerator AIBs available for the high-
end PC market. It was an adaptation of the graphics pipeline found in the Personal 
IRIS workstation to the MicroChannel architecture and consumer ISA buses found 
on most modern PCs of the day. It is also noteworthy that it was the ﬁrst time any 
variant of IRIS GL was ported to the PC, which led to the creation of the OpenGL 
API. 
3.7.1 
The Legacy of IrisVision—Pellucid, Media Vision, 
and 3dfx (1991–1994) 
Bharat Sastri, the former director of engineering at SGI responsible for the 3D 
graphics adapter SGI developed for the IBM RS/6000 (the IrisVision board), left 
SGI in September 1991. In November 1991, Sastri and a few of his colleagues from 
SGI, such as Gary Tarolli and Scott Sellers, incorporated Pellucid (which means 
“extremely clear”). By the end of the month, they had eight people and $2 million 
in new VC capital. 
A couple of weeks later, SGI contacted Sastri about the possibility of taking the 
IrisVision product from SGI. He liked the idea but was a little hesitant at ﬁrst because 
he thought SGI had sold only a few hundred. When he checked, he found that SGI had 
shipped almost one thousand. He believed a high-end PC graphics segment market 
existed and saw IrisVision as an appropriate vehicle for that market. 
Pellucid took the remaining inventory of the product and committed to selling 
it and supporting the existing customers. To do that, the company ran some ads 
announcing their takeover of the product and began contacting all the known users 
via the SGI sales force. Most of the users were developers, and some were bundling 
the board with their software, primarily animation applications. 
Pellucid offered the 24-bit ISA-based SGI IrisVision two-board-set for $6,243.75, 
while the 8-bit ISA version was $4,368.75. 
IBM had announced they were dropping the 8-bit version of the board. The 
company had concluded 3D needed 16 or 24 bits, with 16 or 24 bits of z-buffer. 
Pellucid agreed with that and promoted the board-set with 8 and 24 bits but expected 
the 24-bit module to be the most popular. They also had a license to build and sell a 
graphics board for the SGI Indigo starter system with 1024 × 768 by 8-bit resolution 
but no z-buffer. 
Along with the IrisVision boards, Pellucid had full rights to the REX chip, the 
foundation of the SGI Indigo system. They intended to incorporate Indigo low-end 
graphics into new products but did not give a time frame. Pellucid had full mask rights 
and the chip design, allowing them to modify the chip for use in the PC environment. 
The company began working on a new chipset for high-end PC graphics for 
applications such as desktop publishing (DTP), CAD, animation, and procedures

142
3
1980–1989, Graphics Controllers on PCs
such as architectural walk-throughs. The chipset targeted the DOS environment based 
on Intel platforms. The Pellucid people believed the advent of the local bus and faster 
CPUs; the DOS environment represented the most robust and complete platform in 
the market. They planned on Windows 3.1 ﬁrst, then NT, and then either OS/2 or 
maybe Solaris. They also envisioned a Macintosh product in the future. 
The chips would consist of a graphics engine (dubbed R80) and a mode manager. 
Together, they allowed multimode operation at high resolution in a windowing envi-
ronment. It was possible, for example, to have one window open running a double-
buffered animation, another doing color indexing, and another organized with a 
z-buffer. The overall resolution was programmable from 640 × 480 to 1280 × 1024. 
At the maximum resolution (1280), the system could refresh at 80 Hz. The R80 had 
a deep FIFO, but the company would not reveal the size. Pellucid had not decided 
how to handle VGA yet. The two choices they were looking at were an analog switch 
and a VGA window. 
The chips used an 800 nm CMOS technology. At the time, Pellucid hoped the set 
would sell for less than $100. They planned to have a system on display at the’93 
CeBIT conference. 
Sastri and his team operated Pellucid for a little over two years—then, in June 
1993, they got an offer they could not refuse. 
3.7.2 
Media Vision (1990–1994) 
In May 1990, Paul Jain and Tim Bratton founded Media Vision. Media Vision built 
sound AIBs and CD-ROM kits in Fremont, California. It was widely known for its Pro 
AudioSpectrum PC sound card, which it often bundled with CD-ROM drives—its 
spectacular growth and ending. 
Effectively and aggressively using its capital and stock from a successful public 
offering, Media Vision gave Pellucid a letter of intent to acquire the company in 
March 1993. Then in June, Media Vision acquired all outstanding shares of Pellucid 
for $15 million of Media Vision common stock. Media Vision planned to form a new 
subsidiary and merge Pellucid into it [35]. 
Pellucid would become a wholly owned subsidiary and the foundation of Media 
Vision’s newly formed Visual Technology Group. Its technology would be offered 
as graphics accelerator products and be integrated with Media Vision’s existing tech-
nology to provide new digital video products for personal computers. The ﬁrst prod-
ucts, add-in boards, would be sold through traditional computer retail channels and 
were expected out that summer, with OEM products to follow [36]. 
Bharat Sastri, founder, and president of Pellucid, became vice president for 
visualization technology of the Visual Technology Group.

3.7 SGI’s IrisVision (1988–1994)
143
Greg Reznick, who had recently joined Media Vision, would become vice pres-
ident of the Visual Technology Group. He had formerly been vice president of 
marketing at graphics chip and AIB maker Video Seven, along with several others 
of Media Vision’s executive team members. 
In 1994, Jain stepped down as CEO and entered a legal battle with the U.S. 
Department of Justice. Jain was tried and convicted of two counts of wire fraud; 25 
other charges against him were dropped. 
Media Vision sold all its assets to a newly formed ﬁrm, Aureal Semiconductor, 
in 1995 and thus ended the saga. The downfall of Media Vision cost investors and 
bondholders U.S.$200 million [37]. 
Smith, Sellers, and Tarolli went on to form 3dfx and spun out Quantum3D from 
it. Sastri went to Alliance Semiconductor and then took over Quantum3D. 
The 3dfx story and how the founders got from SGI to Media Vision to 3dfx 
is told in Sect. 1.7.1. The legacy of IrisVision—Pellucid, Media Vision, and 3dfx 
(1991–1994). 
3.7.3 
Benchmarking 
The original IBM PC came with an ISA-based AIB called the monochrome display 
adapter (MDA), and it established a set of instructions on how to drive a display. 
Therefore, to replace the MDA, one had to build an MDA-compatible board (the 
terms card and board were, and still are, used interchangeably). The MDA could 
only generate monochrome 7 × 9 dot characters. 
Right after the PC came out, the ﬁrst independent graphics AIB supplier, Hercules, 
appeared. Hercules offered the ﬁrst bitmapped AIB with a higher resolution of 720 × 
350. Also, during this period, entry-level graphics boards were being introduced. In 
1984, IBM, the standards setter, introduced the EGA, which provided low resolution 
(640 × 350) 16-color bitmapped graphics. A half dozen suppliers cloned the EGA 
chip. 
AutoCAD, a new low-cost, PC-based CAD program, was introduced right after 
the PC. In 1983, Don Strimbu created a single detailed view of a ﬁrehose nozzle, 
shown in Fig. 3.37, which became known as The Nozzle and was used to benchmark 
a graphics AIB.
The Nozzle was used as a benchmark to see how fast a graphics AIB could render 
it. It was not a totally fair test as the PC’s processor and memory were also in the 
loop and could dramatically inﬂuence the result. But it was all the industry had at the 
time and was an appreciated and well-used benchmark for several years. The iconic 
image has since been reimagined in 3D.

144
3
1980–1989, Graphics Controllers on PCs
Fig. 3.37 Don Strimbu’s 
Nozzle was a 2D drawing 
benchmark used for many 
years (Courtesy of CAD 
Nauseam)
3.8 
Conclusion 
From 1980 to 1999, the foundations for the GPU were laid. The story of the devel-
opments is told in Chap. 5 and this chapter. Chapter 5 traces the development on 
other platforms, that is, other than the PC. This chapter traces the developments on 
the PC. The other platforms (workstations and compute) were where the founda-
tional computer graphics work was done and the principles for the basic pipeline and 
architecture of a GPU were established. The PC introduced volume manufacturing 
and, with it, lower costs. The chasm between the big systems on other platforms 
and the mass-produced lower-cost solutions was a constant challenge, and the larger 
system builders, with a few exceptions, could not evolve and died. Likewise, many 
of the smaller volume producers couldn’t develop the complex circuitry needed 
for higher-quality and higher-performance graphics, and they too died off. It was a 
perfect analog of Darwinism, of evolution, adaption, and survival of the ﬁttest. And 
it didn’t happen in a nice tidy linear way. Some companies leap-frogged others, some 
successfully transitioned from one platform to another, and some branched out into 
a totally different path. It was never dull, always exciting, and sometimes downright 
scary. Fortunes were made and lost, and through it all, the computer graphics hard-
ware business has become a multi-billion-dollar market with 100 s of thousands of 
participants and billions of users. 
References 
1. IBM 5100, https://en.wikipedia.org/wiki/IBM_5100#cite_note-Timeline_of_Computer_His 
tory-2 
2. Datapoint 2200, https://en.wikipedia.org/wiki/Datapoint_2200 
3. Micral, https://en.wikipedia.org/wiki/Micral 
4. Altair 8800, https://en.wikipedia.org/wiki/Altair_8800 
5. The birth of the IBM PC, https://www.ibm.com/ibm/history/exhibits/pc25/pc25_birth.html 
6. Miller, M. J.  Project Chess: The Story Behind the Original IBM PC, (August 12, 2021), https:// 
www.pcmag.com/news/project-chess-the-story-behind-the-original-ibm-pc 
7. Freiberger, P. and Swain, M. Fire in the Valley: The Making of the Personal Computer, McGraw-
Hill, (1984), https://www.google.com/books/edition/Fire_in_the_Valley/guwnAQAAIAAJ? 
hl=en

References
145
8. Wallace, J., and Erickson, J. Hard Drive: Bill Gates and the Making of the Microsoft Empire 
(Harper Collins, 1992), https://www.goodreads.com/book/show/41611.Hard_Drive 
9. “Katzenstein, H. S.” Hartford Courant 
10. Brooktree v. Advanced Micro Devices, San.Diego.CA, (1990), https://www.casemine.com/jud 
gement/us/59148920add7b049344fdc17 
11. Sweber, B. RAM-DAC Upgrade Dramatically Enhances VGA Graphics, Analog Dialogue 
23, no. 3 (1990), https://www.analog.com/media/en/analog-dialogue/volume-24/number-3/art 
icles/volume24-number3.pdf#page=3 
12. Urquhart, H. Introduction to Popular Tales of the West Highlands (Edinburgh: Edmonston and 
Douglas 1860), https://tinyurl.com/u77cwe6e 
13. Peddie, J. Famous Graphics Chips: EGA to VGA, IEEE Computer Society, https://www.com 
puter.org/publications/tech-news/chasing-pixels/famous-graphics-chips-ega-to-vga 
14. Wikipedia, s.v. List of Monochrome and RGB Color Formats, https://en.wikipedia.org/wiki/ 
List_of_monochrome_and_RGB_color_formats 
15. Benchoff, B. VGA In Memoriam, Hackaday, https://hackaday.com/2016/01/29/vga-in-mem 
oriam/ 
16. Peddie, J. Famous Graphics Chips: IBM’s professional graphics, the PGC and 8514/A, IEEE 
Computer Society, https://www.computer.org/publications/tech-news/chasing-pixels/Famous-
Graphics-Chips-IBMs-professional-graphics-the-PGC-and-8514A 
17. Necasek, M. The 8514/A Graphics Accelerator, OS/2 Museum, (May 9, 2013), http://www. 
os2museum.com/wp/the-8514a-graphics-accelerator/ 
18. Richter, J., and Bud Smith, B. Graphics Programming for the 8514/A, M&T Books, (1990), 
https://books.google.com/books/about/Graphics_Programming_for_the_8514_A.html?id= 
0rxUkgEACAAJ 
19. Peddie, J. Famous Graphics Chips: IBM’s VGA. The VGA was the most popular graphics chip 
ever, IEEE Computer Society, https://www.computer.org/publications/tech-news/chasing-pix 
els/Famous-Graphics-Chips-IBMs-VGA 
20. BM VGA XGA Technical Reference Manual (May 1992), http://bitsavers.trailing-edge.com/ 
pdf/ibm/pc/cards/IBM_VGA_XGA_Technical_Reference_Manual_May92.pdf 
21. Peddie, J. Famous Graphics Chips: Intel’s GPU History, IEEE Computer Society, https://www. 
computer.org/publications/tech-news/chasing-pixels/intels-gpu-history 
22. Peddie, J. Famous Graphics Chips: NEC µPD7220 Graphics Display Controller, IEEE 
Computer Society https://www.computer.org/publications/tech-news/chasing-pixels/famous-
graphics-chips 
23. Shires, G. A New VLSI Graphics Coprocessor: The Intel 82786, IEEE Computer Graphics 
and Applications 6, no. 10 (October 1986), https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=& 
arnumber=4056737 
24. Peddie, J. Famous Graphics Chips: Intel’s 82786 Intel’s First Discrete Graphics 
Coprocessor, https://www.computer.org/publications/tech-news/chasing-pixels/Famous-Gra 
phics-Chips-Intels-82786-Intels-First-Discrete-Graphics-Coprocessor 
25. Peddie, J. Famous Graphics Chips: Intel740, https://www.computer.org/publications/tech-
news/chasing-pixels/famous-graphics-chips-Intel740 
26. Peddie, J. Famous Graphics Chips: The Integrated Graphics Controller, https://www.computer. 
org/publications/tech-news/chasing-pixels/the-integrated-graphics-controller 
27. Antognetti, P, Anceau,F., and Vuillemin, J. Microarchitecture of VLSI Computers, Springer 
Science & Business Media, (December 6, 2012) 
28. Peddie, J. 1990 Tseng ET4000, IEEE Computer Society, https://www.computer.org/publicati 
ons/tech-news/chasing-pixels/1990-tseng-et4000 
29. Fisher, A. Semiconductor Review, The PC Graphics Report V 5, no. 48, (November 24, 1992) 
30. W32i ET4000/W32i Graphics Accelerator Data Book, Tseng Labs, (1993), https://datasheet 
spdf.com/pdf/502285/Tsenglabs/ET4000-W32I/1 
31. Peddie, J. Tseng Labs announces two new products, The PC Graphics Report Volume VII, 
Number 48, pp 1021, (November 22, 1994)

146
3
1980–1989, Graphics Controllers on PCs
32. Ramat, O. ATI acquires graphics design assets of Tseng, The PC Graphics report, Volume X, 
Number 50, pp1587, (December 22, 1997) 
33. SGI Micro-Channel IRISVISION Adapter, http://ohlandl.ipv7.net/video/Iris.html. 
34. SGI IrisVision, Ardent Tool of Capitalism, https://ardent-tool.com/video/Iris.html#Product_D 
escription 
35. CBR Staff Writer, Media Vision Buys Super-Graphics Firm Pellucid, Tech Monitor, (June 1, 
1993), https://techmonitor.ai/techonology/media_vision_buys_super_graphics_firm_pellucid 
36. Media Vision Completes Acquisition of Pellucid, The PC Graphics Report 6, no. 23 (June 8, 
1993) 
37. Wikipedia, s.v. “Media Vision,” https://en.wikipedia.org/wiki/Media_Vision

Chapter 4 
1980–1995 the Progenitors: Graphics 
Controller on PCs 
Each one of the companies mentioned in this chapter signiﬁcantly added to the 
development of the GPU and would inform its eventual realization and future. 
4.1 
1990–1995, Graphics Controllers on PCs 
The second decade was kicked off with the introduction of the long-rumored and 
awaited IBM XGA. It was quickly copied (cloned), and the copying led IBM to the 
undeniable conclusion that it just couldn’t compete in such a fast-moving, overpop-
ulated, and cut-throat market. XGA was IBM’s last AIB. Several cloners discovered 
the same hard facts and exited the market in the send half of the decade. 
4.1.1 
IBM XGA (1990) 
The end of an era. 
IBM introduced the eXtended Graphics Array XGA graphics chip and add-in board 
(AIB) in late October 1990. It was the last graphics chip AIB IBM would produce 
after setting all the standards for the industry it had created.1 
Developed with the VGA for the PS/2, the XGA was a Type 2 video subsystem 
(the VGA being a Type 1). XGA was a high-resolution graphics chip capable of 
displaying 1024 × 768 pixels, which IBM called PELs—a contraction of Picture
1 Peddie, J. Famous Graphics Chips: IBM’s XGA, IEEE Computer Society, https://www.computer. 
org/publications/tech-news/chasing-pixels/famous-graphics-chips-ibms-xga. 
“Many people in the industry now say if you look at the insides of graphics processing units, many 
of the ideas started out with the Pixel Planes series of machines.”—Henry Fuchs. 
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2022 
J. Peddie, The History of the GPU - Steps to Invention, 
https://doi.org/10.1007/978-3-031-10968-3_4 
147

148
4
1980–1995 the Progenitors: Graphics Controller on PCs
Fig. 4.1 IBM XGA AIB (Courtesy of CC BY-SA 3.0, commons. Wikimedia) 
Element. It could display colors from a palette of 256 k—6 bits per primary. XGA 
added support (beyond the 8514/A) for high color (65,536 colors, 16 bpp/primary) 
at 640 × 480. 
The XGA was a graphics controller chip, and the AIB it was used on was also 
called XGA (see Fig. 4.1). 
IBM’s XGA combined an upgraded VGA version and features from the 8514/A. 
The XGA AIB had an onboard chip. The AIB was in the new PS/2 Model 90 and 90 
XP. A stand-alone upgrade AIB (the IBM PS/2 XGA Display Adapter/A) was also 
available for existing PS/2 s. The price was $1,095 for an XGA with 512 KB VRAM 
and an additional $350 for a memory upgrade to 1 MB VRAM. That was a lot of 
money then. 
The signiﬁcant philosophical and architectural change in the XGA was the inte-
gration of the VGA subsystem. In a way, this was an admission of defeat, said 
Michal Necasek of the OS/2 Museum, as “IBM’s strategy of providing an onboard 
VGA chip with an additional high-resolution accelerator such as the 8514/A clearly 
hadn’t worked out.”2 It was also the way most subsequent non-IBM graphics would 
be constructed. 
Because of the VGA integration, the XGA was not backward compatible with 
the 8514/A. Also, unlike the 8514/A, the host CPU could directly access the entire 
XGA frame buffer CPU. Furthermore, up to eight XGAs could be used in one system 
through a bus mastering capability in the XGA. However, because the VGA used 
ﬁxed I/O and memory-mapped address spaces, only one VGA could be active at a 
time in a system.  
The XGA had several such unique features, including a new 64 × 64 hardware 
sprite used as a mouse cursor. Earlier chips such as the EGA, VGA, and 8514/A used 
software to manage the mouse overlay, which at the time was not a trivial challenge. 
The XGA video subsystem components included the following: 
• System bus interface. 
• Memory and CRT controller.
2 Necasek, M. The XGA Graphics Chip, OS/2 Museum, (March 19, 2013), http://www.os2mus 
eum.com/wp/the-xga-graphics-chip/. 

4.1 1990–1995, Graphics Controllers on PCs
149
• Coprocessor. 
• Video memory. 
• Attribute controller. 
• Sprite controller. 
• Alphanumeric (A/N) font and sprite buffer. 
• Serializer. 
• Palette. 
• Video digital-to-analog convertor (DAC). 
Figure 4.2 shows a block diagram of the IBM XGA. 
The XGA’s coprocessor provided hardware drawing-assist functions throughout 
real or virtual memory. The XGA had the following essential functions: 
• PEL-block and bit block transfers (PxBlt). 
• Line drawing. 
• Area ﬁlling. 
• Logical and arithmetic mixing. 
• Map masking. 
• Scissoring. 
• X- and Y-axis addressing.
Fig. 4.2 XGA block diagram, the coprocessor was the graphics engine 

150
4
1980–1995 the Progenitors: Graphics Controller on PCs
The XGA used a 16 or 32-bit data bus for all system memory and I/O addresses, 
whereas the VGA subsystem used either an 8-bit or 16-bit data bus. The XGA 
Display Adapter automatically adjusted its use of the MicroChannel appropriately 
to the system’s capacity. With a 16-bit data bus, XGA used a 512 KB video display 
buffer; a 32-bit data bus used a 1 MB video display buffer. Access to the XGA was via 
two sets of registers. The ﬁrst set was mapped into the system’s I/O space, whereas 
the other registers were mapped into memory. 
The original XGA supported 1-, 2-, 4-, and 8-bits-per-pixel colors at resolutions 
up 2014 × 768 interlaced. In a noninterlaced mode, it could support 16-bits-per-pixel 
colors. The XGA output went directly to a VGA connector, either on the AIB or the 
system board. 
The serializer (shown in Fig. 4.2) and DAC converted the data in the video display 
buffer to the screen image. The video display buffer stored video data in 1-, 2-, 4-, 8-, 
or 16-bit pixels. The video mode in which the computer was operating determined 
the number of bits per pixel. Each memory location in the buffer held one pixel and 
corresponded to a speciﬁc location on the screen. The binary value of each 1-, 2-, 4-, 
or 8-bit pixel was used as an index into the palette (LUT). It determined the color 
displayed at that location. If the computer was in the direct color mode, each pixel 
was 16 bits, and it did not use the palette to determine the colors. The XGA offered 
many options to the application developer. 
The serializer took the data from the video display buffer and converted it into 
a serial bit stream. If the pixels were 1, 2, 4, or 8 bits, the binary value of each 
pixel corresponded to one of the 256 memory locations in the palette. Each memory 
location contained 18 bits, divided into three 6-bit values that represented speciﬁc 
intensities of red, blue, and green. In the direct color mode (palette bypass mode), 
each 16-bit pixel had a 5-bit red intensity value, a 5-bit blue intensity value, and a 
6-bit green intensity value for 65,536 possible colors. The DAC then converted the 
digital color-intensity values to analog values for the monitor. 
Although targeted for OS/2-based PS/2s, IBM, recognizing that not everyone was 
signing up for OS/2, provided drivers for Windows 2.1 and 3.0, OS/2 1.2, and popular 
software packages such as AutoCAD. 
XGA-2. IBM followed up the original XGA with XGA-2 in 1992, which had built-
in support for noninterlaced 1024 × 768 resolution and a 1 MB VRAM standard 
(Fig. 4.3). The XGA-2 included a programmable PLL circuit and could support pixel 
clocks up to 90 MHz; this enabled up to 75 Hz refresh at 1024 × 768 resolution. 
The 800 × 600 resolution was also supported, at up to 16 bpp. The XGA-2 also had 
an improved DAC with 8 bits per channel rather than 6 bits like the original XGA, 
which increased the displayable colors to 16 million.
The PS/2E, introduced in 1993, featured a full-sized internal PC speaker, two 
single inline memory module (SIMM) sockets, and an extended bank of memory 
soldered directly to the motherboard. It featured 1 MB of video memory for the 
onboard XGA-2 graphics adapter, which was attached to the ISA bus instead of the 
usual MCA bus. 
The XGA had been the subject of speculation in the late 1980s as word of its 
development leaked out of IBM’s Hursley Labs in the UK. Its architecture was quite

4.1 1990–1995, Graphics Controllers on PCs
151
Fig. 4.3 IBM XGA-2 AIB (Courtesy of OS2 Museum)
advanced for the time with a linear frame buffer aperture, a ﬂexible bus mastering, 
a draw engine, and a hardware sprite cursor. When the XGA was released, most PC 
graphics AIBs were dumb frame buffer, upgraded VGA AIBs (with higher resolution 
to i800 × 600) known as SuperVGAs. 
It took the rest of the PC graphics hardware industry several years to catch up with 
the XGA’s capabilities. In many ways, the XGA was a classic 1990s design. Even if 
it never reached its full potential, it could have easily supported up to 4 MB VRAM 
and 24/32 bpp True Color pixel formats, although it did not support the Truevision 
Targa format. 
When the XGA came out, IBM ﬂoated the idea that it would sell the chip to other 
companies and allow them to build AIBs with it as a second source. However, in the 
Spring of 1991, the company changed its position and instead offered to license the 
design [1]. Speculation at the time was that IBM did not want to reveal how much it 
had cost them to build the chip and be accused of dumping. 
Instead of selling the chips, IBM thoroughly documented the register interface’s 
hardware speciﬁcations. Then IBM licensed the design to SGS-Thomson (Inmos), 
Intel, and a few small companies such as Integrated Information Technology (IIT). 
But the market had moved from the character-based user interface to a bitmapped 
graphical user interface (GUI), and the new chips and AIBs were known as GUI 
accelerators [2]. (pronounced as gooey accelerators.) 
Because of the software support IBM had developed for the XGA, it only worked 
in a 386 or 386-based PS/2 (including 386SX-based PS/2s), and by 1992 the 486 
was the new CPU of choice.

152
4
1980–1995 the Progenitors: Graphics Controller on PCs
4.1.2 
Summary 1990 to 1995 
The XGA was the end of IBM’s leadership and dominance in the PC market, just 
ten years after the company had invented and introduced the PC. Twelve years later, 
IBM sold the entire PC product line to Lenovo. Today, that company is the second or 
third-largest PC supplier—an example of one company’s castoff is another’s gold. 
The next signiﬁcant development in PC graphics came with the introduction of 
3D graphics accelerators such as 3dfx’s Voodoo, ATI’s 3D Rage, and Nvidia’s Riva 
128. 
The PC graphics industry owes a debt of gratitude to IBM for establishing the 
standards and foundation for all graphics chips. 
4.2 
The IGC to IGP (1991) 
Following Moore’s law, integrated graphics became quite powerful and popular. 
In the early 1990s, the graphics controller was integrated into the CPU’s Northbridge 
and became known as an integrated graphics controller (IGC). By 2020 the GPU was 
invented, and it got integrated into the CPU, becoming the IGP, or an iGPU. In that 
conﬁguration, the graphics controller and the CPU shared the system’s DRAM, and 
the conﬁguration was known as a shared-memory architecture or a uniﬁed memory 
architecture (UMA). It is still used today in PCs, consoles, and smartphones. 
Integrated graphics appeared in the workstation space in 1991. IGCs found their 
way into smartphones, tablets, automobiles, and game consoles [3]. A typical block 
diagram of an IGC in a system is shown in Fig. 4.4.
Integrated graphics have evolved from being part of the chipset to being integrated 
within the CPU, which Intel ﬁrst did in 2010. AMD followed Intel with the Llano in 
2011 but with a much bigger and more powerful GPU. In between, the industry saw 
a half-dozen or more clever innovative designs appearing from various suppliers, 
many of them no longer with us. 
Color Key also called "chroma key 
A technique for superimposing one video 
image onto another. Used to place a 
complimentary scene behind people. It is also 
used for creating special effects such as 
having an object like a car float on 
something, like a swimming pool. 

4.2 The IGC to IGP (1991)
153
Fig. 4.4 Integrated graphics controller circa 1992 (IGC)
4.2.1 
The First Workstation IGC 
1991, May—One of the ﬁrst examples of integrating a graphics controller with 
other components was the Scalable Processor Architecture (SPARC) enhancement 
chipset from Weitek. This chipset consisted of the W8701 SPARC microprocessor 
and the W8720 IGC. The W8701 integrated a ﬂoating-point unit into a SPARC RISC 
microprocessor. It ran at 40 MHz and was socket- and binary-compatible with the 
SPARC integer unit (IU) standard. 
4.2.2 
The First PC IGC 
1995, June—Taiwan-based Silicon Integrated Systems introduced the SiS6204, 
the ﬁrst PC-based IGC chipset for Intel processors. It combined the Northbridge 
functions with a graphics controller and set the stage for a new category: the IGC. 
SiS developed two IGCs, the 6204 for the 16-bit ISA bus and the 6205 for the 
newer peripheral component interconnect (PCI) interface. The graphics controller 
offered an integrated VGA with display resolution up to 1280 × 1024 × 16.8 million 
colors (but interlaced). It had a 64-bit BitBlt engine with an integrated Philips SAA 
7110 video decoder interface that provided YUV 4:2:2 support, color key video 
overlay support, color space converter, integer video scaling in 1/64 unit increments, 
and VESA DDC1 and DDC2B signaling support. It offered a UMA capability in

154
4
1980–1995 the Progenitors: Graphics Controller on PCs
conjunction with SiS’s 551 × UMA chipsets. However, most importantly, it proved 
what one could integrate into a small, low-cost chip. SiS and ALi were the only two 
companies initially awarded licenses to produce third-party chipsets for the Pentium 
4. 
1999, January—In the late 1990s, workstation giant SGI tried to meet the 
oncoming threat of the popular and ever-improving X86 processors from Intel. SGI 
developed the Visual Workstation 320 and 540 using an Intel Pentium processor and 
designed the Cobalt IGC. It was a massive chip for the time with over 1,000 pins and 
cost more than the CPU. It also showed what performance one could obtain with a 
UMA in which the graphics processor shared the system memory with the CPU. It 
allowed up to 80 percent of the system RAM available for graphics. However, the 
allocation was static and adjusted only via a proﬁle. 
1999, April—Intel led the industry by integrating more functions and capabilities 
into the CPU. In 1989, when it introduced the venerable 486, Intel incorporated an 
FPU, the ﬁrst chip to do so. Ten years later, the company introduced the 82810 IGC 
(code named Whitney). 
4.3 
Bitboys (1991–1999) 
The company that was almost the ﬁrst with a powerful GPU does 2D in mobile 
phones. 
Bitboys Oy was founded in Noormarkku, Finland, in 1991, two years before Nvidia. 
Sami Tammilehto and brothers Mika and Kaj Tuomi were the founding members, 
with Petri Nordlund joining later. The company’s brain trust nurtured a longtime 
love for 3D computer graphics and was part of Finland’s rich community of demo 
artists. In conjunction with their friends around the Helsinki area, Bitboys created 
music, video, and computer graphics for the Demo scene that brought the community 
together. 
Bitboys had started research in 1993 on a PC-based graphics processor that could 
accelerate 3D CAD graphics. They built an ISA-bus-based prototype using ready-
made components, including a TI TMS34010 chip. And Bitboys then began readying 
their Pyramid3D graphics chip design for market. 
By 1995, Bitboys had begun working with the integrated circuit designer company 
VLSI Solutions (Founded in 1992 in Tampere, Finland) to develop a graphics 
processor. At VLSI, they met integrated circuit designer and marketing company 
TriTech (founded in 1990 in Singapore), part of Charter, a semiconductor fabrica-
tion company (founded in 1987 in Singapore). That led to a business arrangement and 
the development of the TR25201 for TriTech. And that led to the TR25202, TR25203, 
and TR25204, all great 3D designs, but that was all. Tritech made a sample chip or 
two and showed them to a few people at Cebit in 1998. 
A year before Bitboys formalized its company in 1991, the Singapore govern-
ment’s Singapore Technology (ST) group launched the TriTech Microelectronics

4.3 Bitboys (1991–1999)
155
company, a Singapore Technology (ST) afﬁliate, and a fabless designer of semicon-
ductor products. One of TriTech’s objectives was to acquire computer technology for 
a portfolio of semiconductor products. Those chips would then be manufactured by 
Charter Semiconductor, also an ST afﬁliate. ST started in 1987; one of Singapore’s 
largest industrial conglomerates is indirectly but wholly owned by the government 
of Singapore. 
Bitboys met with the management of TriTech and established an intellectual prop-
erty (IP) licensing arrangement like that of Arm and Imagination Technologies. 
Bitboys would design products, TriTech would productize and market them, and 
Charter would build them. 
In the Fall of 1996, TriTech announced its new 3D chip, the Pyramid3D (Fig. 4.5). 
Kok Chin Chang, section manager of graphics products, and Winston Chen, director 
of multimedia marketing, said TriTech’s goal was to deliver four out of the ﬁve stages 
of a full 3D processor, as shown in Fig. 4.6 [4]. 
The announcement of the 3D chip caused a tremendous stir in the industry. It had 
a 3D pipeline back to triangle setup, built-in VGA, and LUT-DAC. It would be the 
most integrated, most powerful, and biggest chip ever built. It had a new pipeline 
construct and promised a new wave of application acceleration. It had many of the 
features found only on big SGI workstations. Because of the chip’s notoriety, it is 
worth going into some of its details. 
As Fig. 4.6 shows, most 3D chips available in the early 1990s (and many of the 
planned units) satisﬁed level 5. A few with various forms of setup engines (and a 
coprocessor at that) satisﬁed levels 4 and 5. Using Bitboy’s design, Tritech planned 
to satisfy levels 2 to 5 shown in the block diagram with its Pyramid 25201 as a single
Fig. 4.5 Bitboys’ Pyramid3D AIB (Courtesy of Petri Nordlund)

156
4
1980–1995 the Progenitors: Graphics Controller on PCs
Fig. 4.6 Bitboys 3D Graphics pipeline processing hierarchy
chip. However, to meet the less-demanding needs of the market, they also planned 
to offer a smaller unit, the 25202, which would satisfy levels 4 and 5. 
The overall organization of the chip is shown in Fig. 4.7. 
The highlights of the chip were as follows:
• Full 10-component RGB model support (three-color diffuse RGB, three-color 
specular RGB, transparency, fog, and u/v texture).
Fig. 4.7 Pyramid3D system architecture 

4.3 Bitboys (1991–1999)
157
• Radiosity lighting support. 
• Multiple simultaneous lighted textures with ﬁltering. 
• Complex shading effects, including bump mapping and specular lighting. 
• Perspective correction for both texture and true color shading. 
• Uniﬁed memory buffer architecture. 
• Programmable pixel pipeline. 
• High integration, including video refresh, clock synthesizer, and true color DAC. 
4.3.1 
Pyramid3D 25202 
The TR25202 was a signiﬁcant step to the GPU, but it did not include a geom-
etry processor. Primitives were initialized in the geometry stage by the host CPU, 
then rasterized in TR25202’s primitive processor. TriTech planned to offer a single-
chip implementation of the rendering stage of a traditional three-stage 3D graphics 
pipeline comprising tessellation, geometry, and rendering. The device contained a 
PCI bus master interface, a clock synthesizer, and a LUT-DAC. The stages in the 
chip are discussed in the following sections. 
Pixel processor—The pixel processor handled visibility checking using the z-
buffer, performed texture data fetching, and blended colors for transparency and 
other effects. It received a list of pixels and properties from the primitive processor 
and wrote the resulting colors to the local frame buffer memory. The process used 
a shading program provided by the user (arguably this was the ﬁrst programmable 
pixel shader unit, not the one in GeForce 3, which was introduced ﬁve years later in 
2001). All operations used 32-bit color. In calculating the ﬁnal color, it was possible to 
use multiple textures (including performing a dependent texture lookup) and special 
effects such as fog, environment mapping, and bump mapping (more on that later). 
Primitive processor—The primitive processor generated the individual pixels 
that formed each primitive and forwarded them to the pixel processor. Primitives 
could be triangles, lines, or 2D regions, deﬁned by their edges. The primitive 
processor ﬁrst determined which pixels were inside the primitive and then calculated 
eight associated pixel properties. They controlled the pixels’ color, transparency, 
fog intensity, specular intensity, primary texture, and secondary texture. All prop-
erties, including colors, were interpolated with perspective correction and claimed 
the company without performance penalties. “This,” said Chang, “guarantees that 
lighting and texture will match perfectly, and any undesired warping is avoided” [4]. 
Texturing—Textures could range from 32 × 32 pixels with 4-bit indexed color to 
1024 × 1024 pixels with 32-bit true color. The texture unit did not have a 256-color 
palette built-in because it would have consumed too much die space. Instead, the 
pixel processor is read in the color index and performed a memory lookup into a 
palette texture to fetch the 32-bit color value. Textures could contain transparency 
information. It was possible to use mipmapping and bilinear and trilinear ﬁltering 
for all texture formats. “That,” Chen said, “will maximize image quality without 
maximizing memory usage.”

158
4
1980–1995 the Progenitors: Graphics Controller on PCs
Perspective correction—It applied perspective correction for textures and 
shading without any performance penalty, said the company. That, TriTech claimed, 
ensured that textures and lighting on all surfaces matched perfectly. And full perspec-
tive correction also eliminated incorrect surface warping, which could otherwise be 
visible on nearby surfaces. 
Shading—It performed complex shading in a single pass and could do multiple 
simultaneous textures. It combined textures with interpolated colors and settings 
using programmable shading operations. Complex shading effects such as textured 
surfaces with environment mapping could be efﬁciently generated. 
Memory management—TriTech bragged about its uniﬁed buffer architecture, 
which was used to store all the data, including frame buffer, textures, and geometry. 
The uniﬁed buffer architecture added ﬂexibility and made memory usage more efﬁ-
cient in that no memory was ever left unused because of having been assigned the 
wrong function. As a result, less memory was needed to support a wide range of 3D 
applications. 
For maximum performance, the memory interface supported SDRAM memory, 
which at the time could achieve an 800 MB/second transfer rate (using a 64-bit-wide 
bus). In addition, a reduced bus width (32 bits) was possible if less memory was 
desired. 
4.3.2 
Pyramid3D 25201 
The 25201 contained all the functions of the 25202 plus geometry operations in stages 
2 and 3, which the host normally performed. It could have been the ﬁrst integrated 
single-chip GPU if it had been built (assuming Tseng didn’t get there ﬁrst). 
Geometry processor—The geometry processor was based on a three-issue VLIW 
architecture with a packed instruction word. It had three parallel ALUs and additional 
units for division and other tasks. The ALUs were all three-cycle pipelines with 
multiplication as the ﬁrst stage, addition as the second, and shifting as the ﬁnal 
stage. The addition was also used as the second stage of the multiply operation; 
therefore, both could not be started on the same cycle. The processor also had three 
data memories, so there was no need to use the external graphics memory during the 
calculations. Additional details of the system’s architecture are shown in Fig. 4.8.
The geometry processor interfaced with the other blocks through full access to 
the chip’s registers. Normally, the task of the geometry processor was to process a 
data stream and calculate values to the primitive processor registers. The stream was 
read from the external graphics memory into the stream buffer, from which it could 
be loaded to the ALU registers one word at a time. The stream interface could also 
write sequential data to the external graphics memory. 
An additional unit connected to the ALU2 performed logic operations and bit-ﬁeld 
extraction and testing. 
Another unit performed a normalization operation that calculated how many bits 
an integer could be shifted left without overﬂowing.

4.3 Bitboys (1991–1999)
159
Fig. 4.8 Bitboy’s Pyramid3D architecture
Finally, a third unit connected to the ALU2 was a radix-four divider providing 
two bits of result per cycle. It could be used to calculate 24-bit or 32-bit divisions in 
the background. 
To implement a fully functional processor, conditional and unconditional branches 
were included. The branching was based on condition codes generated by the three 
ALUs and the logic unit. 
There was also a special sort operation, which compared three integers while 
storing their order in a special bus indexing register. That made it possible to sort 
three triangle vertices into the top, middle, and bottom and then access the related 
data through bus indexing. 
The geometry processor had three internal data memories consisting of 128 32-bit 
locations. The memories were dual-ported, so it was possible to read X and write Y 
simultaneously or load both the X and Y registers simultaneously. 
The user provided the program that controlled the geometry processor and stored it 
in the external graphics memory. The program was cached into a 512-word four-way 
set-associative on-chip instruction cache with a 128-word block size. The program 
could also initiate prefetching of the cache blocks. 
As usual for a VLIW processor, the architecture and pipelining in the geometry 
processor were visible to the programmer, and one had to consider all the pipeline 
effects. That enabled the production of maximally efﬁcient code but required more 
care in programming. It was also essential to organize code correctly and use the 
instruction cache efﬁciently. When a sequence of similar triangles was drawn, the 
whole geometry program should have ﬁt into the instruction cache at the same time. 
VGA core—Because backward compatibility with existing software was a neces-
sity, the Pyramid3D included a VGA-compatible core (designed in-house) with 
SVGA support through a VESA-compliant BIOS. VGA was only used for running

160
4
1980–1995 the Progenitors: Graphics Controller on PCs
old VGA software and not used in 2D GUI acceleration, which used the 3D hardware. 
For example, when Chang was asked how the Pyramid 3D handled X–Y interpo-
lation for video scaling, he smiled and said, “It is just another texture map to us” 
[4]. 
4.3.3 
The Eight P’s 
TriTech put forth the concept of the eight Ps as follows: 
PCI—The chip’s PCI interface allowed the host linear access to the frame buffer 
and memory-mapped registers. The frame buffer could be accessed in both RGB and 
YUV formats. In addition, bus mastering was supported so that textures, geometry, 
and individual triangles could be read from the main memory (minimizing host 
processor overhead). The company intended to replace the PCI block with an AGP 
interface in Q3 ’97, reducing eight Ps down to seven. That never happened. 
Programmability—Programmability was central to the Pyramid3D architecture. 
The full programmability of the pixel processor made it possible to realize many 
shading models and create new ones to suit the application. 
Performance (predicted, but sadly not realized): 
Rendering performance with 64-bit bus using SDRAM 
• 1,300,000 Gouraud shaded 25-pixel triangles/second with a point light. 
• 1,000,000 randomly rotated z-buffered. 
• 800,000 randomly rotated textured Gouraud shaded 25-pixel triangles/second. 
• 650,000 z-buffered dual-textured Gouraud shaded triangles/second. 
• Pixel ﬁll rate 50,000,000 pixels/second. 
Rendering performance with 32-bit bus using SDRAM 
• 900,000 z-buffered Gouraud shaded triangles/second. 
• 650,000 z-buffered textured Gouraud shaded triangles/second. 
• 550,000 z-buffered dual-textured Gouraud shaded triangles/second. 
The company said the primitive processor could process up to 50 million 
pixels/second. 
Parallelism—To hit real workstation performance (i.e., 10 million poly-
gons/second at the time), the chip had a deep FIFO method that could be used 
for merging the input of another 2501, as shown in Fig. 4.9.
The company believed they could string ten or more 2501s together in such a 
fashion and hit the 10 billion polygon mark. 
Package—The 25201 and the 25202 were going to be supplied in a 304-pin BGA. 
Price—In 1996, The target price for the 25202 was $45, and the 25201 would 
come in under $70. 
Production—The company planned to show samples of the 25202 at COMDEX, 
and full production of both parts was forecasted for Q1 ’97, but that never happened.

4.3 Bitboys (1991–1999)
161
Fig. 4.9 Symmetric multiprocessing with multiple Pryamid3D chips
Process—The controllers were to be built by TriTech’s sister company, Chartered 
Semiconductor Manufacturing, using 350 nm technology, which was the state of the 
art at the time. 
At the 1997 IEEE HotChips conference on August 26 in Palo Alto, California, 
Ville Eerola from TriTech gave a presentation on the Bitboys Pyramid 3D processor 
[5]. At the time, the company had DirectDraw and D3D drivers running on a simu-
lator. They planned to offer a Windows NT OpenGL driver in Q2 ,97 and a Heidi 
driver in Q3 ,97. The company said there would be a 3D software development kit 
(SDK), and, as one might expect, a 3D graphics library would be available. Unfortu-
nately, D3D at the time did not support bump mapping or radiosity. TriTech said they 
were “working with Microsoft” to get those functions added to D3D as extensions. 
Two years later, in March 1998, at the Microsoft WinHEC 98 conference and show 
in Orlando, Florida, Microsoft announced it would license TriTech’s proprietary 
bump mapping technology for use in future versions of the Microsoft DirectX set of 
APIs, beginning with DirectX 6.0. 
At the announcement, Bettina Briz, Vice President of marketing for TriTech, said, 
“Microsoft’s selection of the Pyramid3D bump mapping technology is a testimonial to our 
expertise in real-time 3-D graphics. This agreement highlights TriTech’s ability to develop 
enabling technologies, as well as highly integrated products with value-added features. 
TriTech ﬁrst developed its bump mapping technology as part of its Pyramid3D family 
of 3-D graphics accelerators introduced in 1997. For the ﬁrst time in almost two decades, 
TriTech’s bump mapping approach overcomes computational limitations, making real-time 
bump mapping possible in low-cost 3-D graphics hardware” [6]. 
Briz went on to say the devices were targeted at high-end business workstations 
as well as consumer PCs. And then she surprised everyone by announcing a massive 
price reduction: “Available now in high volumes; the Pyramid3D TR25202 device 
is priced below $17 each in quantities of 10,000 devices. The Pyramid3D TR25204 
device is priced below $20 each in quantities of 10,000 devices.” Given that the

162
4
1980–1995 the Progenitors: Graphics Controller on PCs
company had not shipped any parts, so it could not be enjoying economy of scale, 
the massive price cut was seen for what it was—panicked selling. 
Although the technology of TriTech’s Pyramid3D architecture was interesting 
and exciting, there was a concern about the viability of its approach. By poking into 
stages 2 and 3, TriTech was encroaching into Intel’s territory. Intel wanted to provide 
the cycles for objects and some of the vertex work. Also, getting 3D software engines 
to split up the tasks between the host and coprocessor looked as if it would take some 
political maneuvering to convince developers. Needless to say, the 3D engines would 
run a lot faster, and the host would be available for more challenging things such 
as the AI aspects of 3D games; however, corporate religion and NIH (“not invented 
here”) would create some obstacles for TriTech. 
In addition to Intel, the company had to convince Microsoft to add Real3D features 
such as radiosity and bump mapping to D3D. That put TriTech in contention with 
Microsoft’s Talisman Group, which was also trying to dictate a new architectural 
approach to enable more efﬁcient memory use. Microsoft wanted to get its extensions 
added to D3D. Given Microsoft’s limited resources for D3D in those days, it seemed 
obvious which group was going to win even though Talisman would ultimately fail. 
Less than two months after the March Microsoft announcement at WinHec, 
TriTech Microelectronics, Inc., announced on May 17, 1998, that it was getting 
out of the market altogether [7]. Having gotten into the 3D graphics market only a 
year earlier with the Pyramid3D family of VLIW graphics chips, TriTech decided 
instead to concentrate on analog audio and mixed-signal ASICs, its core business. 
There were manufacturing problems in the Charter fab in general, for which 
Charter blamed TriTech, citing poor design rules. It has also been speculated that 
TriTech was a money sink, and Singapore Technologies’ didn’t want to continue to 
provide more funding. 
But the real problem could have been Crystal Semiconductor’s ﬁling suit against 
TriTech for patent violations ﬁled on January 10, 1997 [8]. Tritech was placed under 
judicial management on July 2, 1999, and on October 15, 1999, the company ceased 
operations [9]. In 2001, Crystal Semiconductor was awarded $48 million in damages 
from TriTech. 
4.3.4 
Summary 
Bitboys retreated to Noormarkku and started the next phase of its colorful life, 
discussed in Book three. 
4.4 
Artist Graphics (1979–2098) 
The Beale brothers, Horace and Robert, would be unusual in any industry; they 
were both religious and could be uncompromising. Robert Beale developed some

4.4 Artist Graphics (1979–2098)
163
eccentric ideas about the IRS and the necessity of paying taxes, but their advances 
in the computer graphics industry were undeniable. On their way from CAD to 
commodity PCs, the company developed a great chip. 
Artist Graphics, founded in 1979 by Horace and Robert Beale in Minneapolis, 
released its ﬁrst AIB, the Artist 1, in November of 1982, based on an NEC 7220 and 
sporting a gigantic 1024 × 768 resolution [10]. 
Traditional semiconductor suppliers such as Hitachi, NEC, and Texas Instruments 
were not keeping up with the demands of the graphics industry, and the graphics 
board companies wanted higher margins for more proﬁt from graphics boards. As 
the computer graphics board industry shifted from commodity to proprietary graphics 
chips, Artist Graphics developed its own graphics chip, the GPX, in late 1992 for 
its Xj1000 AIB. The chip accelerated wireframe rotations and object shading thanks 
to the Xj1000’s z-buffer support and onboard rendering engine, which is explicitly 
designed for accelerating CAD software and display manipulations. 
Artist Graphics was the ﬁrst private company to develop a proprietary graphics 
chip in 1992 
The GPX processor could produce between 1.1 million and 625,000 2D vectors per 
second (10 pixels long), 240,000 3D vectors per second, and 16,000 shaded polygons 
per second (100 pixel 3D shaded triangles). It offered 1280 × 1024 resolution with 
8-bit color, and at 800 × 600, it offered 24-bit true color. The AIB had 2 MB of 
VRAM and a 72-pin SIMM socket for DRAM expansion, which could hold up to 
8 MB of 80S DRAM used for z-buffering and display list storage. There was also an 
onboard Chips & Technologies (C&T) CT450 VGA controller with its own 512 KB 
of DRAM. The Xj1000 sold for $1,495 in 2019, equivalent to $2,738 in 2019. 
The chip was a technical success, but the company could not generate enough 
volume to offset the costs of development. However, in 1995, Artist Graphics tried 
again with their 3GA chip. 
4.4.1 
Artist Graphics Shows 3GA Graphics Accelerator 
Artist Graphics revealed their new 3D accelerator chip, the 3GA, at the Design 
Engineering Conference in Chicago in March 1995. It was a VRAM-based 64-bit 
controller with an advanced video graphic array (AVGA) core and management for 
a 16-bit z-buffer (Fig. 4.10). It supported dithering and Gouraud shading as well as 
texture mapping and six arithmetic operations. The AVGA could support 8-, 16-, 
and 32-bit pixels, and it had a fast BLT engine, eight stencil modes, an 8 × 8 color 
expansion capability, and rectangular and arbitrary region clipping.
The chip supported 4 MB of VRAM for resolutions up to 2048 × 2048 and refresh 
rates up to 90 Hz at 1600 × 1200, as well as up to 8 MB of DRAM. There was a 
128-byte host FIFO, a direct 32-bit VESA local bus (VLB), a PCI, and a 16-bit ISA 
interface. The 50 MHz part used a 240-pin plastic quad ﬂat pack (PQFP) package.

164
4
1980–1995 the Progenitors: Graphics Controller on PCs
Fig. 4.10 Artist Graphics’s 3GA controller
The chip had a FIFO write buffer for drawing-engine commands that decoupled 
the CPU-host interface from the 3GA drawing engine. It contained 32 longword (32 
bits) entries. The FIFO managed all pipeline issues with the drawing engine and 
would not overwrite a register if in use.

4.4 Artist Graphics (1979–2098)
165
Table 4.1 Speciﬁcations of 
the 3GA controller 
Conﬁguration
Speed 
2D lines
1,600,000 lines/second 
3D lines (shaded, z-buffered)
280,000 lines/second 
2D texture-mapped triangles
75,00 triangles/second 
3D texture-mapped triangles 
(z-buffered) 
65,000 triangles/second 
3D Gouraud shaded triangles 
(z-buffered) 
110,000 triangles/second 
Screen-to-screen bitBLTs
80,000,000 pixels/second 
Memory-to-screen bitBLTs 
(color expanding) 
245,000,000 pixels/second 
Fill area
1,100,000,000 pixels/second 
Graphics and display registers controlled most of the 3GA enhanced-mode opera-
tions. They consisted of display control, frame buffer base addresses and pitches (the 
number of bytes in each row on-screen), coordinates, primitive, and interpolation 
registers. 
The memory controller serviced requests from the display controller, host inter-
face, and pixel engine. It generated all the VRAM/DRAM control signals as well as 
the local device cycles needed to access the BIOS, ROM, DAC, and clock generator. 
Table 4.1 lists some of the key speciﬁcations of the chip. 
The AIB exhibited excellent performance based on 8 bits/pixel, 10-pixel lines, 
and 50-pixel triangles using a 90 MHz Pentium with PCI. In its ﬁrst trials, the chip 
exceeded its forecasts. The AIB came with 2 MB of VRAM for $395 (almost $800 
today). 
4.4.2 
Summary 
Too much too late. As good as the design was—and it was very good—the develop-
ment cost was so high and the sales volume so low, combined with new commodity 
suppliers such as ATI and Nvidia (who were moving faster with superior products), 
that the company just could not compete, and in 1996, 16 years after its founding, 
Artist Graphics folded. 
The largest part of the CAD market at that time was the AutoCAD market, but 
only a small portion of those users could justify the expense of a specialized graphics 
card for CAD. A commodity CAD market did not yet exist.

166
4
1980–1995 the Progenitors: Graphics Controller on PCs
4.5 
Number Nine Imagine 128 (1992–1999) 
The age of proprietary graphics chips. 
By the early 1990s, the PC industry was still expanding and offering plenty of 
opportunities for all. IBM had lost its position of leadership, and for a few years, the 
market existed on commercial off-the-shelf (COTS) graphics chips from TI and a 
bunch of XGA and VGA clone builders [11]. 
Number Nine was founded in 1982 by Andrew Najda and Stan Bialek as Number 
Nine Computer Corporation. The company was initially founded to supply accel-
erator boards for the Apple II. With a wry sense of humor, the company named its 
products for Beatles songs and buried snippets from the Fab Four’s songs in the 
bios splash screens. The company was renamed Number Nine Visual Technology 
Corporation in the early 1990s. 
In 1994, Number Nine was a small company in Boston that had been selling 
me-too clone AIBs. It surprised the industry with what would be the ﬁrst of a series 
of proprietary custom graphics chips called the Imagine 128. Subsequent products 
were named Revolution, Imagine, Sgt. Pepper, Ticket to Ride, One After 901, Number 
Nine, Cloud Number Nine, etc. [12]. 
Number Nine got its start in Cambridge, Massachusetts, in 1982 by offering a MOS 
65c02 microprocessor-based AIB for the Apple II. The company went on to develop 
an NEC µPD7220 AIB and a number of TI TMS 34010-based AIBs. Later the 
company was the ﬁrst to offer 256-color and true color AIBs. Such accomplishments 
raised the price of its AIB, which limited the number of home computer consumers 
as customers. 
In the mid-to-late 1980s, the biggest opportunity and most challenging applica-
tion for PC graphics boards were to accelerate AutoCAD and its competitor’s CAD 
programs. To accelerate CAD, the AIB suppliers developed clever ways of handling a 
display list, a series of instructions that told the AIB what to draw and where. Number 
Nine was selling a TI 34010-based AIB called the Pepper Pro 1280. It offered one 
resolution (1280 × 1024) and had limited drivers running under NNios (Number 
Nine Intelligent OS). It did have a fast and rich AutoCAD display list processor; 
however, it was relatively expensive. 
Number Nine offered the ﬁrst 128-bit memory bus in the industry 
In the early 1990s, PCs were getting the ﬁrst 32-bit and 64-bit graphics, and the 
market was shifting as interest in 3D gaming and content creation began to develop. 
By 1993, there were 16 companies planning or offering a 3D graphics AIB, and in 
1994, the number jumped to 30 companies. 
The early 1990s also saw the introduction of the PCI interface for accessory 
boards. 
Number Nine knew it had to do something to break out of the pack and differentiate 
itself. The company had prided and positioned itself as a performance leader in 
graphics, and so it took the bold move of committing to an ASIC and the ﬁrst 128-bit 
memory bus in the industry. Graphics performance is heavily dependent on memory

4.5 Number Nine Imagine 128 (1992–1999)
167
read/writes, and so the wider and faster the memory bus, the higher the performance. 
Other than a little overclocking of COTS VRAM, there was not much one could 
do to squeeze more performance out of memory, but if one made a wider bus, then 
more data could be moved faster. Graphics is an insatiable data consumption beast; 
Number Nine stepped up to feed it. 
But it was a race. In those days, a conference in Las Vegas called COMDEX 
was the premier place to announce and show off new PC products. All the computer 
dealers and OEMs went there to sell stuff to each other. Billed as a dealers’ showcase, 
it became a magnet for the press and the geeky civilians who could get in. 
Number Nine was already sourcing its mainstream graphics chips from S3 for use 
in AIBs for its retail consumer line and primary OEM AIB market. Knowing well in 
advance that S3 was going to make a big announcement about 64-bit graphics chips 
(Vision 964 VRAM-based and Vision 864 DRAM-based) at the 1994 COMDEX 
show, Number Nine jointly announced 64-bit AIBs at the same show. But Number 
Nine caught everyone by surprise, stole the show, and created a lot of buzz with the 
announcement of the world’s ﬁrst 128-bit graphics chips when everyone else was 
announcing their 64-bit offerings. It was a giant leap, the company’s ﬁrst chip—and 
it worked right out of the gate on day one (Fig. 4.11). 
The Imagine 128 GPU introduced a full 128-bit graphics processor—GPU, 
internal processor bus, and memory bus were all 128 bits. However, there was no, 
or very little, hardware support for 3D graphics operations. The second version had 
Gouraud shading added with 32-bit z-buffering, dual display buffering, and a 256-bit 
video rendering engine. 
The company brought out its second generation of the Imagine 128 (Imagine 128-
II) in 1995 with an intelligent command processor, a VLIW processor that enabled 
the CPU to send multiple drawing commands directly into the graphics memory in a
Fig. 4.11 Number Nine’s Imagine 128 PCI AIB circa 1995 (Courtesy of Wikipedia) 

168
4
1980–1995 the Progenitors: Graphics Controller on PCs
FIFO execution manner. A hardware device-independent bitmap (DIB) conversion 
(Microsoft Windows device-independent bitmap) ofﬂoaded the CPU. 
The video engine was separated from the drawing engine and increased to 256 bits 
wide and 8 pixels per clock with bilinear interpolations. It also contained Number 
Nine’s ﬁrst Real3D engine. The 3D engine could draw lines and do Gouraud shaded 
triangles and 16- or 32-bit z-buffering, volume clipping, and spatial blending. The 
ﬁrst implementation of the Imagine 128 II came with 4 or 8 MB of VRAM, and later 
the company offered a less expensive version with a 4 MB EDO-DRAM option. The 
graphics accelerator chip’s 100 MHz memory controller could handle 800 megabytes 
per second single-ported performance with EDO-DRAM and, with VRAM, even 
more. It came with a Direct3D driver, and the company claimed it could do 610,000 
Gouraud shaded 50-pixel triangles per second in 16-bit color. 
Like so many of its competitors, Number Nine suffered in the transition from DOS 
to a GUI-based OS. And, like its competitors, it used a low-cost Cirrus Logic chip 
with 512 kB to manage DOS calls. That ultimately proved inadequate for advanced 
DOS graphics applications, including games. The company developed a proprietary 
BIOS that supported Imagine II under DOS to address the issue, and the company 
sent an upgrade to customers upon request. 
Number Nine and its competitors got caught in the bus wars of the time. The PCI 
interface was introduced in 1993, replacing the VL bus introduced the year before, 
and then PCI was replaced by AGP in 1996. Thus, Number Nine had to come out 
with an AGP version as well. 
Also, at the time the buses were changing, APIs were developed independently, 
and four types of memory chips were offered: conventional DRAM, EDO-DRAM, 
VRAM from TI, and a newer, less expensive WRAM (Windows RAM—unrelated 
to Microsoft’s Windows) that Samsung developed for AIBs. WRAM was a high-
performance dual-ported video RAM that had about 25 percent more bandwidth 
than VRAM. It had features that made it more efﬁcient to read data used in block 
ﬁlls and text drawing and handled high resolution (such as 1600 × 1200) using true 
color. But Samsung never got a second source, and in 2000, GDDR was introduced, 
obsoleting WRAM and VRAM. 
Nonetheless, the company found itself supporting three memory types and two 
bus types while competing with three dozen other AIB suppliers—not an easy task 
when combined with new versions of Windows, APIs, and driver requirements. 
In 1995, the company went public and developed new graphics accelerators and 
AIBs. It produced the popular Revolution 3D powered by the T2R (Ticket to Ride) 
chip and the Revolution—derivatives of the 128-bit Imagine, each with faster 3D 
and video scaling features. 
On April 20, 1999, BankBoston Business Credit established a $15 million loan to 
Number Nine. Then, on August 9, 1999, Number Nine announced that it had entered 
into a relationship with PixelFusion to use the Fuzion 150 chip, but that chip never 
went into production. 
And then, in December 1999, the company announced it had a letter of intent 
from S3 (later S3 Graphics Co.), one of its major competitors, to buy Number Nine’s

4.6 Rendition (1992–1998)
169
assets and IP. By mid-2000, S3 had completed the acquisition, and Number Nine 
ceased operations—17 years after it started. 
In the end, as with most graphics AIB makers, it became impossible to survive 
being a commodity graphics device within a commodity computer. Lady Madonna. 
4.5.1 
Summary 
In 2002, Francis Bruno and James Macleod, two engineers from Number Nine, 
started Silicon Spectrum. They licensed Number Nine’s graphics technology from 
S3 and did an implementation in FPGAs. 
Then, in 2013, after Silicon Spectrum had closed, Bruno tried to crowdfund an 
open-source GPU based on a Number Nine Ticket to Ride IV-derived design [13]. 
Started on the kickstarter.com platform, the campaign did not succeed, and only 
$13,000 of the $200,000 goal was received. Nonetheless, Bruno did release the 
source code under a GPL3 license in August 2014. 
4.6 
Rendition (1992–1998)
Rendition made a big splash with their Vérité 3D controller and was on the path to 
building a GPU, but they ran out of money and time and ended up being bought by 
Micro (Fig. 4.12). 
Synema started in 1992 in Oregon as an idea Jim Peterson and Jay Eisenlohr had 
about accelerating rendering on a PC. Eisenlohr came from Mentor, an electronic 
design automation (EDA) software company. Peterson came from Intel. Their ﬁrst 
venture round was with Interwest Partners and Matrix Partners. However, the team 
knew they had to be in Silicon Valley, so they moved in late 1993 and changed the 
name to Rendition in 1994. 
There were no standardized programming interfaces or APIs for 3D graphics such 
as Direct3D or OpenGL when Rendition started. Therefore, Rendition, like 3dfx and 
Nvidia, had to develop their own API and hope they could convince game developers 
to write to it. Rendition’s ﬁrst API was Speedy3D for DOS. They then developed the 
Redline API for Windows. 
The company announced the 3D rendering chip Vérité V1000 in 1995 and released 
the chip in 1996. 
The rendition was one of the ﬁrst companies to produce good-looking anti-aliased 
vectors and use point-sampling and bilinear ﬁltering. The company also did an 
excellent job on the way they treated and ﬁltered textures. 
The company offered the AIB as a replacement for whatever the customer had 
already installed. The chip supported both VGA 2D and 3D drawing, with an impres-
sive 3D-in-a-window mode via context switches. As a result, the AIB had a single 
output VGA port.

170
4
1980–1995 the Progenitors: Graphics Controller on PCs
Fig. 4.12 Rendition team pose in front of their ﬁrst ofﬁcial ofﬁce in Mountain View California, 
1994 (Courtesy of Jay Eisenlohr)
The V1000 used a MIPS, RISC, and CPU core for triangle setup and pre-rendering, 
making it the ﬁrst VGA-compatible graphics chip to offer programmability for 
rendering. The TI TMS34010 was the ﬁrst programmable graphics chip (1986) but 
did not support VGA. 
Almost as soon as the newly named Rendition got to Silicon Valley, the team went 
knocking on VCs and potential partners’ doors looking for investment and help. And 
although Jay Eisenlohr was acting as CEO and president, the team wanted someone 
with more star power and experience. 
The company had a good story to tell and some impressive simulation results. 
Everyone was impressed, some scared. The company lined up commitments from 
Mitsubishi Electronics America, Interwest Partners, Matrix Partners, Enterprise Part-
ners, and Unterberg-Harris Interactive Media Limited Partners. In August 1995, 
Rendition announced its second round of investment, $7 million, a large sum for 
those days and a nascent market. 
As part of the deal, the investors insisted on installing a more seasoned executive 
and brought in Mike Boich, former CEO, and founder of Radius (AIBs and monitors) 
[14] (Fig. 4.13).
Radius was founded in 1986 by Burrell Smith, who had the initial idea, Mike 
Boich, Alain Rossman, Andy Hertzfeld, William Carter, and others, who were the 
developers and evangelists for the Macintosh at Apple. Radius developed and sold 
graphics AIBs for the Mac for $995 ($2,500 in 2022).

4.6 Rendition (1992–1998)
171
Fig. 4.13 Mike Boich (Courtesy of Mike Boice Wikipedia)
“The thing we try to do is turn the Macintosh into a graphics workstation,” said 
Boich, Radius’ CEO and Chairman of the Board, said in the Business Journal-San 
Jose. “Our concentration is on high-performance graphics systems.” 
Radius used commercial-off-the-shelf (COTS) chips from 3Dlabs and SGS 
Thompson and never developed its own chip. 
Radius went public in August of 1990. The company soared and then fell, had a 
slight resurgence, and dropped some more [15]. Boich was asked to resign. Then 
Boich was reappointed as president and chief executive ofﬁcer. “I learned a lot 
watching another person do the job,” Boich said in Business Journal-San Jose [16]. 
Then Boich left Radius again and was replaced in March of 1993 by Charles W. 
Berger. 
So, when Boich took over Rendition in 1995, he came with hard-won experience 
in the graphics board market and dealing with investors. 
He would need them because Rendition was going to break barriers and upset 
the game market. Rendition’s Vérité graphics controller had a 64-bit bus and could 
address up to 64 MB. The AIB came with 4 MB of EDO-DRAM with a theoretical 
400 MB/s bandwidth. The Vérité was not the fastest, the ATI Mach64 had a 528 MB/s 
memory bandwidth, and the 3dfx Voodoo, also with 4 MB, could reach 800 MB/s. 
But then luck struck. In 1996, two games had users excited—Tomb Raider, 
developed by Core Design, and Quake, created by id Software. 
Through its proprietary API, the V1000 could use its hardware acceleration to 
run Quake with high frame rates; in addition to its 25 megapixels/sec ﬁll rate, the 
V1000 generated very high-quality images. 
John Carmack of Id, the developers of Quake, commented, “The image quality 
is signiﬁcantly better than software-dithered, bilinear interpolated textures, and 
subpixel, subtexel polygon models. It is faster than software even at 320 by 200, 
and at 512 by 384, it is almost twice as fast” [17]. 
That recommendation carried a lot of weight, and four AIB suppliers launched 
products: Creative Labs with the 3D Blaster PCI, Sierra with the Screamin’ 3D,

172
4
1980–1995 the Progenitors: Graphics Controller on PCs
Canopus with its Total 3D, and the Intergraph Reactor were the ﬁrst. MiRO joined 
them shortly after that. 
However, as good as the V1000 was, it slowed down when called (by the game) 
to do z-buffering. If a game made a depth test, the V100’s ﬁll rate dropped to 12.5 
megapixels and the frame rate dropped by 50%. Therefore, game developers who 
ported to the V1000 tried to minimize the z-buffer reads [18]. 
Then in January 1997, id Software introduced a new version of Quake based on 
a stripped-down version of OpenGL, which they named GLQuake. That rattled a lot 
of organizations, AIB suppliers, chip suppliers, and Microsoft. 
Id Software’s thin and light version of the powerful OpenGL API opened the door 
for any AIB. And those companies that had suitable hardware and could write tight 
drivers did exceptionally well. 3dfx’s Voodoo and Voodoo 2 were two of them. 
The 3dfx Voodoo performed well in GLQuake, especially at 512 × 384 in 16-color 
mode, and got a reliable 40 + fps. The Rendition V1000 had to use the Pentium CPU, 
and local memory for z-buffer tests could not keep up. Also, Carmack of id, who had 
once been Rendition’s champion, said he had difﬁculties programming the Vérité. 
So, id decided to shift from proprietary APIs to an industry-standard OpenGL. 
Rendition introduced several novel features that sometimes were, unfortunately, 
ahead of the times. One was a direct-memory-access (DMA) controller for the PCI 
bus. It could transfer data much faster than the conventual ﬁrst-in, ﬁrst-out (FIFO) bus 
mode. However, the PCI introduced a few years earlier (1992) only had limited bus 
mastering capabilities required for DMA operations. As a result, problems occurred 
with the Vérité AIBs. If a non-Intel chipset was not capable of DMA, the Vérité 
AIB had to fall back to a FIFO operation which severely reduced performance. In 
other cases, the non-Intel chipset DMA capability was improperly implemented or 
incomplete, so the transfers were slower than they should have been. Those issues 
caused problems for users and may have cost Rendition and OEM (Number Nine). 
Rob Mullis, VP of software development, designed a utility to test the PC’s ability 
to handle DMA transfers to counter the problem. 
The Vérité had a triangle setup process that ran on an integrated MIPS processor, 
which was quite advanced for a consumer device. 3Dlabs has such a setup but 
is targeted at the professional market. Rendition promoted its setup engine as an 
advantage over 3dfx’s Voodoo. The irony was hardware setup reduced the host 
CPU’s processing requirements for drawing complex 3D scenes. But then the Vérité 
burdened the CPU with z-buffer checks. 
Also, the Vérité did not have the ﬁll rate to take advantage of on performance 
advantage of the setup engines; its peak rate was 25Mpixels/second while the Voodoo 
was 40. However, 3dfx became the preferred AIB for gamers who could afford it. 
The V1000, with its integrated 2D/VGA core and triangle setup, attracted gamers 
looking for a general-purpose graphics board that could also provide a good gaming 
experience for less cost. 
However, the V1000’s 2D performance in some games was not as good as the 
competitor’s, especially in MCGA/VGA resolution or Mode X. 
In 1998, Jazz Multimedia built an AIB that had an AGP connector on one edge 
and a PCI connector on the opposite—a user could plug it into whatever type of PC

4.6 Rendition (1992–1998)
173
they had. The V1000, designed in 1995, had PCI. To adapt it to AGP (introduced in 
1997) required adding a bridge circuit. 
Even though the V1000 had a degree of programmability, it could not adapt to 
emerging APIs and offer much performance. But the second generation was already 
in ﬁnal testing Rendition would evolve. 
In early 1998, Rendition attracted attention with their Verité 2000 series (the 2100 
and 2200), which differed just in clock and RLUT-DAC speed. 
The V2100 and subsequent V2200 were reﬁned versions of the V1000 technology. 
The company updated the bus interface to AGP, and the V2 × 00 had a single-cycle 
pixel computation (the V1000 took two or more to calculate a pixel). That boosted the 
chips’ ﬁll rate nearly twofold and, combined with faster memory, and a slightly faster 
core clock rate, offered performance ahead of 3dfx Voodoo Graphics (the benchmark 
of the time). 
Things were moving fast in the late 1990s. Nvidia made a remarkable comeback 
with the Riva 128, 3dfx was at the pinnacle of its success (and heading for an equally 
spectacular fall). Tseng labs were peaking and following 3dfx down, and ATI was 
ascending. The rendition was not in sync with all the changes in the industry, and 
the V2100 reﬂected that. A bug in the company’s simulation and synthesis software 
delayed the V2100 for several months, but the industry did not wait for them, and 
neither did the gamers. 
In July 1997, the company demonstrated the new Verité V2200. Rendition revealed 
more information about the chip to end speculation about the new part. Key 3D perfor-
mance improvements over the V1000 accelerator included support for 100 MHz 
SGRAM and the addition of single-cycle ﬂoating-point-to-integer conversion in the 
RISC processor. An integrated RAMDAC and PLL reduce system cost compared 
to the external components required in the V1000 and 2D and video performance 
to a level competitive with current 2D parts. Additional new features in the V2200 
included advanced video capabilities: 
• MPEG-2 motion estimation hardware assistance with 8-bit digital video output. 
• Software ﬂicker ﬁlter and scan compensation for TV output (external NTSC/PAL 
encoder required). 
• VMI-compliant 8-bit video input bus, Point-to-point synchronous, no decimation, 
and glueless support for Bt829-style digitizer. 
• Digital in/digital out (with no RGB conversion artifacts). 
• Video scaling (ﬁltering used 8 bits of fractional precision) and had downscaling 
without decimation. 
Rendition described the 2D performance improvements as four to eight times the 
V1000 and two to four times the 3D speed improvements. 3D applications written 
to the AIB through Speedy 3D or RRedline were expected to have the performance 
improvement close to doubled. Direct3D performance was good, and some games 
(e.g., Forsaken) ran better on a V2200 AIB than on a 3dfx Voodoo AIB (Fig. 4.14).
New to the Verité architecture since the V1000 was a dedicated triangle engine 
that ofﬂoaded edge walking and scan-line setup calculations from the RISC engine. 
Those tasks could occur in parallel with the RISC operations, allowing parallel setup

174
4
1980–1995 the Progenitors: Graphics Controller on PCs
Fig. 4.14 Verité 2200 block diagram (outlined items new integrated components)
and rendering operations. The V2200’s triangle engine rendered a triangle with all 
12 Direct3D attributes at the rate of one cycle per span plus one cycle per pixel. The 
chip could perform trilinear ﬁltering in two passes. By contrast, it took eight cycles 
for the V1000 chip to render a single pixel. 
The company’s 3D performance targets for the chip were, with Direct3D, to 
exceed 200 on 3D WinBench 97 (on a 266 MHz Pentium II) and, with OpenGL, to 
exceed 20 ViewPerf CDRS 03. Rendition quoted 50 million pixels/sec. (bilinear 
ﬁltered, texture-mapped, Gouraud shaded, perspective correct, Z-buffered). The 
triangle engine could achieve a throughput of 500,000 triangles/sec. The chip made 
a ﬂoating-point to integer conversion in one cycle. Jay Eisenlohr said that when the 
CPU adds asynchronous calls, the controller would speed up even more [19]. 
The V2200 had a separate drawing engine to perform per-pixel drawing oper-
ations. They included texel ﬁltering, pixel blending, Z-buffering, fog blending, 
dithering, and specular highlighting. It had a dedicated hardware engine for edge, 
line, and point anti-aliasing (in a single pass). The RISC processor or the triangle 
engine could control the pixel engine and render a bilinear ﬁltered, Z-buffered, 
fogged, and blended pixel with diffuse and specular shading in a single cycle. The 
pixel engine also handled the conversion from YUV to RGB color space and video 
scaling with precision selectable (3, 5, or more tap) ﬁltering. It supported Multiple 
video windows to aid multi-party video conferencing applications. A new 8-bit VMI 
compatible video input port provided glueless support for popular video digitizers, 
and the V2200 handled ﬂicker ﬁltering and removal of temporal aliasing artifacts 
during de-interlacing. The chip delivered digital video (as required for Macrovision

4.6 Rendition (1992–1998)
175
compliance) at NTSC, PAL, and SECAM rates for output to TV via a low-cost 
external encoder. Stereo glasses were also supported. 
The V2200 integrated motion compensation for the acceleration of software 
DVD/MPEG-2 decoding. The motion compensation algorithm was Rendition’s own, 
but they could hook to CompCore or Mediamatics’ software decoding architecture. 
The V2200 had an integrated 203 MHz LUT-DAC with a 64 × 64 by two-bit 
hardware cursor. DAC speed was 230 MHz for high refresh display modes at 1600 
× 1200 resolution. 
The 64-bit memory interface supported up to 16 Mbytes of (up to) 100 MHz 
SDRAM and SGRAM. Rendition said the V2200 could achieve a peak data rate of 
3.2 Gbps using SGRAM’s block write feature. Host connections were PCI, 66 MHz 
PCI, and AGP (1x). In both the PCI and AGP cases, the chip had bus master chained 
DMA. 
2D performance was quoted at >100 with WinBench 97 (1024 × 768 × 16 @ 
75 Hz), whereas the original V1000 delivered less than 70 million Winmarks. Target 
performance with DirectDraw was >150 fps with the Fox & Bear benchmark at 640 × 
480 × 16 bpp. Eisenlohr said the Super VGA performance had improved by a factor 
of three over the original Verité chip, thanks to a memory-mapped I/O architecture 
that was new with the V2000 family. 
3D API support included Direct3D, OpenGL, and Rendition’s own RRedline 
(Windows) and Speedy3D (DOS). 
The Verité 2200 was tightly packed, using only 1.8 transistors (350 nm), and 
packaged in a 256-pin PBGA. They scheduled production for September 1997, and 
the price would be $30 in 10,000-unit quantities. 
The company planned a higher performance part in late 1998, but it was not 
realized. 
The rendition was the darling of the industry 18 months earlier, in January 1996. 
Then they were eclipsed by 3dfx and never quite regained their prestigious position, 
even though theirs was the ﬁrst 3D accelerator to improve Quake. The company 
attempted a comeback with the V2200 and promised more in the pipeline. The 
performance segment was heating up in those days. With 3dfx, 3Dlabs, NEC, Number 
Nine, Nvidia, Oak, and Rendition in that space and more coming (Fujitsu, Intel, 
Philips, and…), the game developers were put on notice to wake up and start taking 
advantage of the available horsepower. Rendition had a few lined up using its API 
RRedline. However, the Rendition V1000 and indirectly its successor the V2200 
were handicapped because of their lack of a hardware-accelerated z-buffer. When a 
game developer enabled depth test, the ﬁll rate dropped. 
The company was also working on a new AIB design that would incorporate a 
Fujitsu FXG-1 Pinolite FFP as a geometry and T&L front end, together with the 
planned V3300. Hercules announced they would bring out an AIB called Thriller 
Conspiracy with it in late 1998. The V3300 was announced initially scheduled for 
delivery in 1999 but it was never released. 
But its design wins that count, and Rendition had work to do to capture the OEMs. 
Nvidia had swept up a group of them, NEC had some surprising announcements

176
4
1980–1995 the Progenitors: Graphics Controller on PCs
coming up, and Rendition was going to have to work extra hard in winning the hearts 
and wallets of the OEMs. 
But the clock and the money ran out, and in June 1998, Micron Technology bought 
Rendition for $93 M. At ﬁrst, the team at Rendition was excited about integrating 
Rendition’s graphics capabilities with Micron’s memory technology. But it never 
came to be. 
Rendition and Micron worked on Socket X, and Micron envisioned embedded 
memory graphics technology as one of its paths out of the commodity memory 
business. The acquisition let Micron add Rendition’s graphics architecture and design 
expertise to its portfolio and gave Rendition access to Micron’s fabs, engineering 
resources, sales force, and working capital. 
“Graphics accelerators represent an ideal application that takes advantage of 
Micron’s newly developed embedded DRAM technology. The joining of forces 
between Micron Technology and Rendition positions us to service the graphics 
accelerator market in a unique way by having ownership of the architecture, design, 
and manufacturing of these devices,” said Steve Appleton, Micron Technology’s 
Chairman, President, and CEO [20]. 
As for the Fujitsu FPP, Rendition investigated using some parts of the FPP to add 
to their 3D accelerator. But when they accepted Micron’s offer, the relationship that 
was developing with Fujitsu vanished (Fig. 4.15).
Micron was interested in developing the processor for the integration of DRAM 
with CMOS. 
“What better way to show the integration of DRAM and CMOS performance on 
the same substrate than with a fast graphics rendering chip,” said Eisenlohr at the 
time. 
As it turned out, the ambitious plan was out of touch with reality—the internal 
CMOS process at Micron was behind the industry by a few generations. Rendition 
did make a successful V3000, but it was not ready for prime time. The project got 
transferred to Micron San Jose, and the Portland people were laid off or given the 
ability to negotiate offers from San Jose. Eisenlohr was the last man standing from 
Rendition in Portland. He literally turned off the lights. He then moved in with 
the Micron salespeople and worked on special projects in Boise and San Jose for 
six months. Eisenlohr was asked to move to Boise and turned the job down. Steve 
Appleton then suggested to the Board to let Eisenlohr go. 
4.6.1 
Summary 
Eisenlohr and Anthony Mark Jones went on to found Ambric, a parallel processor 
chip company, in 2003 in Beaverton, Oregon. And in 2008, Eisenlohr was elected to 
the Academy of Distinguished Engineers. Micron still makes commodity RAM.

4.7 Stellar—RSSI (1993–2000)
177
Fig. 4.15 Jay Eisenlohr 
(Courtesy of Engineering 
Oregon State)
4.7 
Stellar—RSSI (1993–2000) 
Reality Simulation Systems (RSSI) was founded in 1993 at Rensselaer Polytechnic 
Institute’s Venture Creations, Rensselaer Polytechnic Institute’s (RPI) incubator 
in Troy, New York, by Mike Lewis. Lewis was a recent graduate of RPI. His 
pal and fellow graduate Stephen (Steve) Morein graduated with him and was the 
lead designer of the chip. Their goal was to develop very high-performance, cost-
effective 3D graphics processors and related technology for the interactive electronic 
entertainment market [21]. 
RSSI was an outgrowth of the once-mighty Rensselaer incubator, which opened 
in 1980 and nurtured start-up companies from the graphics lab. Sadly, that haven 
of pixel manglers atrophied and scattered its denizens across the computer industry. 
RPI launched one of the ﬁrst business incubators in the country. 
The team developed a tiling design with the loveable name Pixel Squirt. 
Microsoft’s Talisman project and VideoLogic’s PowerVR had inspired Morein; he 
admired the simplicity and logic of the concept. 
Lewis and Morein moved their operations from Troy, New York, to San Jose, 
California, in 1994 to be at the center of the action in Silicon Valley. Lewis attracted

178
4
1980–1995 the Progenitors: Graphics Controller on PCs
local angel investors and in 1995 was able to raise additional capital. Lewis used 
the money well and enhanced the design enough to attract investors. In 1997, Sky 
Capital invested $4 million and later would help Lewis form Stellar Semiconductor. 
Lewis and Morein developed an innovative 3D architecture. Lewis named it Pixel-
Squirt (nicknamed PIX), which he said offered several improvements over the tradi-
tional methods of 3D rendering. The architecture, said Lewis, addressed the band-
width and memory requirements necessary to achieve visual realism for 3D at reso-
lutions of 1024 × 768 and higher, the standard for high resolution established by 
IBM in 1987 with the 8514/A. 
In 1996, S-MOS Systems and RSSI announced a long-term joint development and 
marketing agreement to design 3D technology and products for personal computers. 
S-MOS worked with Steve Morein to develop the SPC1515 PIX. Morein said at 
the time, “Even with the drop in memory prices, it does not pay to re-invent 2D. 
Instead, make it [the PIX] work with any card that supports DirectDraw surfaces” 
[22]. When asked if they might take advantage of Tseng’s Image Memory Access 
(IMA) port, a high-speed asynchronous input for video or graphics directly to the 
display buffer, Morein said they investigated that method. When asked who the 
competition was, Sandeep Gupta of S-MOS said, “Rendition, maybe, but the real 
competition is a pair of skates for a Christmas present” [22]. 
The competition is a pair of skates for Christmas 
RSSI did not do lookup writebacks to the CPU and instead used a large 250 K on-
chip cache. They also developed unique code that used Pentium’s dual pipeline. The 
pipeline’s organization was more like a traditional image generator. Morein said the 
next version of the chip would take advantage of the higher-speed AGP bus. 
The SCP1515 performed point-sampled texel-address calculations for texture 
mapping and supported 32 × 32 to 1024 × 1024 resolution maps. Textures got stored 
in the host system memory, and the chip could manage 65,536 separate texture maps 
and up to 128 MB of addressable texture map data. PIX also supported PCI burst 
transfers. 
When asked how he felt their design related to the streaming processor concept 
of Talisman, Morein said, “We started with tile and rejected it. They do a nice job, 
but it is better if you do not have to use them. We do texture lookup after visibility. 
Render into texture maps; then software texture lookup” [22]. 
Morein said they could sustain data transfer rates greater than 100 MB/second 
over PCI but that it would not work with VGA chips that inserted any “not-
ready” commands (causing the controller to have to wait). The chip could run at 
66 Mpixels/second (z-buffered, 640 × 480). At 800 × 600, the chip reached 45 
Mpixels/second. 
Under the agreement terms, S-MOS would provide the manufacturing (through 
its corporate afﬁliate Seiko Epson in Japan), worldwide sales, marketing, and co-
development resources. RSSI would provide key technology and design expertise in 
all phases of development. The establishment of that partnership was a breakthrough 
and validation for RSSI. Seiko was (and still is) a very well-respected precision tech-
nology company, and the Japanese are highly diligent in their partnerships. The last

4.7 Stellar—RSSI (1993–2000)
179
thing the 115-year-old company wanted was to be embarrassed or have its reputation 
damaged. 
Tom Endicott, then Vice President of S-MOS marketing and sales, said, “We 
chose to work with RSSI because of their unique and innovative approach to three-
dimensional design for personal computers and their speciﬁc knowledge of the 
computer games market. While others were approaching the problem of 3D graphics 
from a workstation point of view, RSSI approached the problem from the PC user’s 
point of view” [23]. It was ironic that consumers would pay for game acceleration, 
and corporations using AutoCAD wouldn’t. 
The ﬁrst chip of a planned three to come out of the agreement was the SPC1515, or 
PIX (i.e., PixelSquirt). S-MOS said the SPC1515 would target games and VRML 3D. 
Lewis said S-MOS would use existing 2D graphics subsystem buffer memory and 
main memory to reduce cost and improve performance. When S-MOS forecasted 
the bill of materials, the company said it would be $60 for a 3D upgrade board; 
that would have been an aggressive price for the market segment the company was 
targeting. As it turned out, the chip was not realizable. S-MOS introduced the chip 
concept at the 1996 Game Developers Conference (GDC), but it never went into 
production. 
In May 1996, S-MOS showed simulations of its RSSI-based SPC1515 or PIX, 
a 3D rendering engine for PCs. The PIX used a system’s existing 2D graphics 
subsystem and buffer memory. S-MOS said at the time that it expected the part the 
following week from its corporate manufacturing afﬁliate Seiko Epson. The simula-
tions were smooth with good-looking images. S-MOS was optimistic that it would 
have a motherboard design win to announce soon. 
4.7.1 
Reality Simulations Systems PixelSquirt 
RSSI’s frame-buffer-less PixelSquirt, made by S-MOS, however, was shown at the 
1995 CGDC (Fig. 4.16). At the conference, RSSI’s president David Bernstein said, 
“Our relationship with S-MOS has created an excellent working partnership to enable 
the complete development and introduction of our ﬂexible 3D graphics technology. 
The simplicity of RSSI designs combined with S-MOS’s ﬁrst-class manufacturing 
facilities will allow very quick product cycles, as demanded by the PC marketplace” 
[23].
RSSI developed a scalable image generator, which was interesting. High-
performance visualization and simulation systems such as ﬂight simulators used 
image generators. For increased performance, it was possible to daisy-chain several 
PixelSquirts together. RSSI built an AIB with four PixelSquirts and a master 
controller. The AIB accepted a video stream from a VGA board via the feature 
connector and then shaded polygons. It got the polygon edge information via the 
PCI bus. It then rendered at 100,000 ﬂat-shaded 400-pixel triangles/second using a

180
4
1980–1995 the Progenitors: Graphics Controller on PCs
Fig. 4.16 Pixel Squirt 3D core
greater-than-three-billion pixel/second ﬁll rate. The AIB could deliver 2,000 trian-
gles/frame independent of the frame rate. The AIB offered 24-bit color with a 1-bit 
alpha plane and a 24-bit z-buffer. The chip supported resolutions up to 1024 × 768. 
RSSI offered the four PixelSquirts with a DAC and a master controller chip on an 
AIB called LittleSquirt, with an estimated price of under $500. Although the AIB was 
not appropriate for gamers’ budgets at that time, the design showed the company’s 
capabilities. The price was also lower than that offered by any other image generator. 
But even with S-MOS’s support, the military and commercial aircraft companies did 
not feel comfortable using such a small company, so RSSI did not have much success 
in that market segment. As has been proven too many times, big companies prefer 
to deal with big companies. 
In mid-1997, the company began work on a new architecture, Aquila PX. Aquila 
PX offered high-performance 2D, 3D, and video and simultaneous NTSC/PAL TV 
output. Lewis said the design would deliver 100 Mpixels/sec. It had a ﬂoating-
point setup engine, a 4 K texture cache, a 230 MHz LUT-DAC, and a nonlinear 
three-line ﬂicker ﬁlter for TV output. Lewis said Aquila PX could support 1024 
× 768 × 16 resolution with a 1 MB texture buffer in a 4 MB conﬁguration. A 
follow-on device, they named VelaTX, was a 3D-only chip that Lewis said could 
achieve 250 Mpixels/sec and incorporated many advanced 3D features such as 
anisotropic texturing. VelaTX would work in conjunction with any existing 2D 
graphics accelerator, Lewis claimed. The 3D cores from both devices were available 
for licensing.

4.7 Stellar—RSSI (1993–2000)
181
In late 1997, Morein left RSSI and went to ATI. Aquila PX never made it out of 
the lab, but in 1998, the company announced its VelaTX. While at ATI, Morein who 
had long been an admirer of Tseng’s IMA, participated in the acquisition of Tseng 
Labs’ assets. 
4.7.2 
Stellar is Born (1997) 
In late 1997, RSSI was restructured and renamed Stellar Semiconductor. With the 
help of Sky Capital, Stellar bought the assets of RSSI, which included a design for 
a new 3D chip code named Aquila PX. Some executives left S-MOS Systems to 
help establish the company, including Sandeep Gupta. Gupta had been the senior 
product manager for graphics products at S-MOS and became Stellar’s CEO. Joseph 
C. Del Rio, VP of engineering and cofounder of Stellar, was the Executive Director 
of engineering at S-MOS. And Michael Lewis, the company’s CTO, was with RSSI 
before moving to Stellar [24]. 
The company announced itself at the Second Intellectual Property in Electronics 
Seminar (IP98) at the Westin Hotel in Santa Clara, California, on March 24, 1998. 
The company had over 25 employees then and had completed two rounds of ﬁnancing 
from venture capitalists. At that time, S-MOS was still pursuing its own course. 
Gupta’s assessment was that the 30-some existing 3D companies had merely 
implemented an existing age-old 3D architecture with its roots in military, CAD, 
and simulation systems. He was correct. “While most companies struggle to squeeze 
performance out of this architecture, they have not even begun to grapple with the 
bandwidth and memory issues required in modern-day, cost-sensitive PC and set-
top 3D systems,” the ever-quotable Gupta said at the time [25]. Gupta predicted that 
high resolutions and large texture maps would place unprecedented memory size and 
bandwidth demands. Stellar rightly claimed that it was one of the few companies to 
recognize the problem and break from traditional architectures. 
Because of pending patent applications, few details were available at the time. 
Gupta said the design could achieve high-performance, high-resolution, and high-
quality realism. Furthermore, the architecture such as Talisman and PowerVR did 
not use a z-buffer, had a real-time data ﬂow, and used half the gates of alternative 
solutions. The architecture was developed in 1993 and was implemented in an AIB 
a year later. 
Stellar had planned to develop a 3D IP. “We also plan to move into the fabless 
semiconductor business by creating, marketing, and selling graphics engines for the 
add-in card and motherboard desktop PC arena,” said Gupta. However, according to 
Gupta, the Stellar graphics accelerator would target a niche market in the 3D space, 
not well served by the other graphics companies. “3D is eye-candy for consumers, 
whether you are talking about graphics in the car, at home, on a PC, or otherwise. It 
is very compelling in all aspects of life,” said Gupta. “Millions of devices are being 
shipped to consumers with or without 3D graphics. Those without can now utilize 
our 3D core and add a very compelling feature.”

182
4
1980–1995 the Progenitors: Graphics Controller on PCs
“We do not plan on doing dozens of licensings per year,” Gupta added, smiling. 
“We plan on having a few licensees, who we call partners, and work with them on 
their high-performance graphics-enabled devices.” 
Stellar had two licensees signed up for its 3D core. Although the company would 
not comment on who they were, Broadcom was most likely one. 
The company planned to introduce a proprietary 3D graphics engine in 2Q’98 as 
a synthesized hardware description language (HDL) netlist and claimed to have two 
offshore foundries qualiﬁed to build it. 
The ﬁrst 3D core was DirectX 5.0 compliant, and the company said it would use 
less than 250,000 gates and be synthesizable up to 100 MHz. Stellar claimed to have 
proven the core twice in silicon with software drivers using Direct3D and OpenGL. 
Gupta said it would take Stellar less than a week to hook the existing 3D core design 
into a company’s device. “Because the architecture is pipelined, a company can 
balance the performance loading and host interface effectively,” he added. 
A man with a mission, the exuberant CEO said, 
Our twofold business model evolved because the same technological advantages that make 
the PixelSquirt architecture extremely well suited to ICs for the PC market also make the 
core suitable for licensing to the digital entertainment markets. 
“The architecture is very elegant, providing excellent performance in a small form factor. 
What is more, it is highly scalable, so the product road map will create multiple market 
opportunities for Stellar Semiconductor and its customers. [26] 
That was an insightful and long-range vision—bridging disparate markets (content 
creations and engineering, which wouldn’t happen until much later. 
The 3D IP core and the graphics chips used RSSI’s original PixelSquirt archi-
tecture, which in turn used a parallel processor and a multiple pipelined design. 
PixelSquirt’s tiling engine eliminated the need for z-buffering because it removed 
hidden surfaces before ﬁltering, texture mapping, and atmospheric conditioning. 
Stellar said a key advantage of its core was the ability to interface easily to the 
existing host interface and memory controller blocks. In those cases, the host IF block 
was required to provide only a bus master read connection to the host CPU, and the 
memory controller needed only to provide a read/write interface to the memory for 
texture map storage. The company said the 3D core was small and highly scalable and 
could offer licensees a range of price/performance options. Stellar had ﬁve patents 
in process at the time. 
4.7.3 
VelaTX (1998) 
Stellar described VelaTX as the ﬁrst of a family of coprocessors and 3D rendering 
engines based on the PixelSquirt architecture. The company claimed it could deliver 
200-million-pixel/s rendering without z-buffering; z-buffer elimination, Stellar 
reminded everyone, reduces fast memory requirements. Previously PixelSquirt had 
been offered as a synthesizable core.

4.7 Stellar—RSSI (1993–2000)
183
Instead of rendering one polygon at a time, the PixelSquirt rendered a pixel at a 
time in raster order, starting with 24-bit ﬂoating-point hidden surface removal. The 
remaining operations acted only on data that would go to the screen. 
The chip had 2.5 MB of DRAM integrated with the renderer via a 512-bit bus 
to speed up texture mapping. Further texture storage used external SDRAM of up 
to 8 MB. Stellar said VelaTX would support numerous OpenGL and DirectX 6 
features in hardware. They would include perspective correction, specular high-
lighting, alpha-blend and texture blend modes, multiple fog modes, and DirectX 6 
texture compression. 
The design had an AGP-to-PCI bridge, P-Pipe, VIP/VMI ports, and a memory 
expansion bus, enabling the chip to form a hub for multimedia expansion AIBs. The 
VelaTX was packaged in a 388-pin BGA and would sell for $35 each in quantities 
of 10,000. Stellar said they would be shipping them in the fourth quarter of 1998 or 
the ﬁrst quarter of 1999. 
4.7.3.1
Stellar and Sican (1999) 
At the 1999 Intellectual Property in Electronics seminar (ip99) in Edinburgh, Scot-
land, Stellar and Sican GmbH (Hanover, Germany) announced a marketing and sales 
agreement. Sican, which provided cores and design services, would market and sell 
Stellar’s IP cores alongside Sican’s existing library of core products. Sican would 
also offer design services to Stellar’s customer base [27]. 
According to Valentin von Tils, Vice President of design for Sican, the IP core 
offerings from the two companies would complement each other. Sican was offering 
audio and video decoding, broadband media access, and bus interface cores. Adding 
graphics to the mix gave Sican a bigger footprint in the multimedia, communications, 
and networking applications segments. 
Von Tils added that he thought the combined strength would greatly enhance 
Sican’s ability to provide a robust set of cores for customers who are designing 
system-on-a-chip multimedia solutions in Europe. The synergy with their design 
services looked like a great ﬁt, but it would be short-lived. 
4.7.4 
Broadcom Acquires Stellar (2000) 
After several months of negotiations, Broadcom, a maker of high-speed communi-
cations chips, said it would acquire Stellar Semiconductor to help Broadcom move 
into set-top box and handheld Internet appliance markets. “This acquisition provides 
Broadcom with an important piece of technology required to deliver high-end 3D 
games to digital set-top boxes,” said Broadcom’s CEO, Henry Nicholas [28]. 
Broadcom would use 785,223 shares of common stock for the acquisition. The 
stock-swap deal was valued at about $162 million [29]. Stellar had 30 employees 
and a good patent portfolio.

184
4
1980–1995 the Progenitors: Graphics Controller on PCs
“After working with Broadcom for nearly a year, we are excited about combining 
forces to address the burgeoning consumer digital entertainment market,” said Gupta 
[30]. 
Broadcom said it would account for the acquisition as a pooling of interest. A 
one-time charge would be taken in the ﬁrst quarter to cover the expense related to 
the transaction. 
4.7.5 
Summary 
Broadcom tried to use the Stellar technology in a set-top box (STB) chip but could 
not ﬁnd many OEMs willing to pay the price for the added performance. The cable 
companies also did not have the content or bandwidth to make good use of it back 
then. 
In 2012 Broadcom in association with Raspberry Pi Foundation in the UK created 
the Raspberry Pi and used the Stellar graphics controller. 
Lewis left Broadcom a few years later and, in 2015, started Mycroft AI, an open-
source equivalent to Amazon Echo and Google Home. 
4.8 
Matrox Millennium (1994–2014) 
Longest-produced graphics chip ever. 
Dorval, Canada-based Matrox is the oldest continuously operating graphics AIB 
company in the world, founded in 1976 by Lorne Trottier and Branko Mati´c. The 
name is derived from “Ma” in Mati´c and “Tro” in Trottier. In 2019, Trottier took 
over the entire company [31]. The two entrepreneurs met at Canadian Marconi (now 
CMC Electronics) in the 1970s, where Trottier went after getting his MS in electrical 
engineering at McGill. 
Matrox’s ﬁrst AIB was the ALT-256 for S-100 bus computers (Fig. 4.17), released 
in 1978 before IBM introduced the PC. ATI started seven years later (also in Canada), 
and eight years after that, Nvidia was founded. One of the lauded pioneer PC 
suppliers, Hercules developed its AIB in 1982, three years after Matrox [32].
Companies in those days built AIBs with discrete logic and an integrated chip 
known as a CRTC (CRT controller). It handled the timing for the display and displays 
based on TV standards. There are still vestigial references to TV in modern products, 
such as the 60 Hz frame sync used in games and supported by all modern AIBs and 
GPUs. 
Matrox is the oldest continuously operating graphics AIB company in the world 
Artist Graphics was the ﬁrst private company to develop a proprietary graphics chip 
in 1992. Matrox developed its own graphics chip in 1994. Artist Graphics shut down 
in 1995; Matrox is still shipping AIBs. NEC developed a commercial graphics LSI

4.8 Matrox Millennium (1994–2014)
185
Fig. 4.17 Matrox’s ﬁrst graphics AIB, the ALT-256, was designed in 1978 for early microcomputers 
(Courtesy of Matrox)
chip in 1982, and many AIB suppliers used it (including Artist). Committing to an 
ASIC for just internal use was a big step in those days for a company with a small 
albeit growing market. Nvidia introduced their NV1 in 1995, but Matrox beat them 
with the Matrox Impression that came out in 1994. 
What made the Matrox MGA noteworthy was its longevity. The Matrox Impres-
sion Plus 220 ISA (MGA-IMP+/A/220) AIB was based on the IS-ATHENA R1 
graphics chip and came with 2 MB of VRAM. Matrox was able to extend the life of 
the MGA and, during the 1990s, produced the Matrox Millennium (Fig. 4.18) and 
Mystique series AIBs.
In 1994, Matrox introduced the Matrox Impression, an AIB that worked with a 
Millennium card to provide 3D acceleration. 
In 1996, Matrox introduced the Mystique. 
In 1997, Matrox introduced the MGA-based Millennium II AIB. 
In 1998, the company introduced the Millennium G200 AGP with 8 MB. 
Based on the MGA-1024SG, a new SGRAM 3D video graphics controller, Matrox 
unveiled the Mystique in May 1996. The tweaked design included hardware texture 
mapping with texture compression. 
Matrox was known as a signiﬁcant player in the high-end 2D graphics accelerator 
market. The AIBs it produced were excellent Windows accelerators, and some of 
the later boards excelled at MS-DOS. Matrox introduced its Impression Plus in 
1994 to innovate with one of the ﬁrst 3D accelerator boards. Still, that board could 
accelerate only a limited feature set (no texture mapping) and aim primarily at CAD 
applications—the acceleration improved performance over the CPU executing the 
graphics functions (Fig. 4.19).

186
4
1980–1995 the Progenitors: Graphics Controller on PCs
Fig. 4.18 Matrox Millennium ISA, circa 1997 (Courtesy of Gona.eu BY-SA 3.0 Wikipedia)
Seeing the slow but steady growth and interest in 3D graphics on PCs with Nvidia, 
Rendition, and ATI’s new AIBs, Matrox began experimenting with 3D acceleration 
more aggressively and produced the Mystique. In 1997, Mystique was the company’s 
most feature-rich 3D accelerator, but it still lacked key features, including bilinear 
ﬁltering. Then, in early 1998, Matrox teamed up with PowerVR to produce an add-in 
3D board called Matrox M3D using the PowerVR PCX2 chipset. This board was 
one of the few cases in which Matrox would outsource its graphics processor, and it 
was undoubtedly a stopgap measure to hold out until the G200 project was ready to 
go. 
It featured a unique rendering method called inﬁnite planes instead of polygons 
and had no z-buffer. The inﬁnite plane setup caused problems for developers to use 
to working with traditional polygons, a similar problem to what Nvidia faced with 
their ﬁrst chip. 
Designated as the successor to Matrox’s successful MGA Millennium and based 
on the company’s ﬁfth generation of its homegrown graphics/video accelerator, the 
MGA-1064SG, the Matrox Mystique was the ﬁrst product Matrox explicitly targeted 
at the consumer market. 
The MGA-1064SG was a 64-bit SGRAM controller with a 32-bit VGA core 
that integrated a 135 MHz triple 8-bit LUT-DAC. The PCI interface mastering 
controller also incorporated hardware 3D texture mapping, including a hardware 
divide engine for efﬁcient perspective correction of texture maps. The chip had a

4.8 Matrox Millennium (1994–2014)
187
Table 4.2 Matrox Mystique 
resolutions and refresh rates 
Resolution
Colors
Refresh rate 
2 MB 4 MB  
1280 × 1024
256
16.7 million
75 Hz 
1152 × 864
65,536
16.7 million
100 Hz 
1024 × 768
65,536
16.7 million
120 Hz 
800 × 600
16.7 million
16.7 million
170 Hz 
640 × 480
16.7 million
16.7 million
200 Hz 
Gouraud shading engine and supported CLUT 4 (4-bit color lookup table) and CLUT 
8 texture compression, allowing developers to reduce the size of source texture maps 
by 2:1 or 4:1, respectively. Each source texture map could have a unique 256-color 
palette associated with it. 
According to the company at the time, the MGA-1064SG could process 25 million 
texels/second where the texels were perspective correct, Gouraud shaded, transparent, 
color lookup table (CLUT) 8-expanded to 16-bit RGB, and z-buffered. A 16-bit 
hardware z-buffer is a way for developers to enable shared SGRAM, and texture 
transparency is a 1-bit control. 
The MGA-1064SG’s video engine implemented both X and Y interpolation, 
which Matrox said was an improvement over the earlier generation MGA-2064 W 
(Millennium) and provided hardware color space conversion (see Table 4.2). 
Game and 3D API support included DirectDraw and Direct3D as well as support 
Criterion’s RenderWare. (RenderWare was the foundation for Microsoft’s DirectX 
and is discussed in the following chapters.) As was typical for the time, Matrox had 
an in-house 3D API called MSI (Matrox Simple Interface). APIs are discussed in 
Book two, What is a GPU? 
The MGA Millennium was both a critical and a popular favorite. The board’s 
success as a game platform was a surprise to Matrox, particularly since gamers 
scorned the Millennium’s predecessors due to lackluster VGA performance. As the 
ﬁrst mainstream graphics accelerator with hardware 3D, the Millennium rode the 
wave of 3D hype and cast favorable light on Matrox in the market even though the 
company was not positioning its product for interactive entertainment applications. 
The Mystique stood to beneﬁt from its association with gaming. It sold for $279 
for a 2 MB board, $149 for a 2 MB SGRAM upgrade module, and $399 for a 
factory-conﬁgured 4 MB board. 
The G200 was Matrox’s ﬁrst fully AGP-compliant graphics processor, powering 
the Millennium G200 and Mystique G200. Intel introduced AGP (Accelerated 
Graphics Port) in 1996. With the G200, Matrox sought to combine its past prod-
ucts’ 2D and video acceleration with a full-featured 3D accelerator. Although the 
earlier Millennium II featured AGP, it did not support the complete AGP feature set. 
The G200 used DIME (Direct Memory Execute) to speed texture transfers between 
the AIB and main system RAM via AGP. That allowed the G200 to use system RAM 
as texture storage if the AIB’s local RAM was of insufﬁcient size for the task at hand. 
G200 was one of the ﬁrst AIBs to support this feature.

188
4
1980–1995 the Progenitors: Graphics Controller on PCs
The chip was a 128-bit core with dual 64-bit buses in Matrox’s dual bus organiza-
tion. Each bus was unidirectional and designed to speed data transfer to and from the 
functional units within the chip. By doubling the internal data path with two separate 
buses instead of a more extensive single bus, Matrox reduced data transfer latencies 
by improving overall bus efﬁciency. The memory interface was 64 bits. 
The G200 supported 32-bit color depth rendering, which substantially pushed the 
image quality upward by ending dithering artifacts caused by the then-more-typical 
16-bit color depth. Matrox called this technology “Vibrant Color Quality” (VCQ). 
The chip also supported features such as trilinear mipmap ﬁltering and anti-aliasing 
(though both are rarely used). The G200 could render 3D at all resolutions supported 
in 2D. Architecturally, the 3D pipeline looked like a single pixel pipeline with a 
single texture management unit. The core had a RISC processor Matrox called the 
WARP. 
The Millennium G200 used the new SGRAM memory and a faster LUT-DAC, 
whereas the Mystique G200 was cheaper and equipped with slower SDRAM memory 
but gained a TV-out port. Most G200 boards shipped with 8 MB RAM, expandable 
to 16 MB with an add-on module. The AIBs also had ports for individual add-on 
boards, which could add various functionality like video accelerators. The G200 was 
also Matrox’s ﬁrst graphics processor that needed additional cooling with a heatsink. 
The company made iteration after iteration of the basic MGA from 1994 to 2001, 
when it introduced its ﬁrst new architecture in almost a decade, the Parhelia. That 
Matrox was able to extend and successfully use the MGA for so long is a tribute to 
the design and the designers. 
4.8.1 
Summary 
Matrox peaked in the late 1990s and 2000s with its AIBs and could not sustain the 
level of leadership over time and “kind of ﬂamed out,” said Trottier [33]. In 2014, 
Matrox announced that it would give up making chips and build graphics AIBs with 
AMD GPUs. 
4.9 
VideoLogic/Imagination Technologies Tiling (1994–) 
Tony Maclaren founded VideoLogic in 1985. The original ambition for the company 
was to focus on video acceleration, and the company had some success in the video 
training market. By the mid-90 s, the scope had expanded to graphics, sound accel-
eration, home audio systems, video capture, and videoconferencing systems. The 
company started trading on the London Stock Exchange in 1994. 
In 1992, the company hired Hossein Yassaie as its technical director. After getting 
his Ph.D. at the University of Birmingham, Yassaie had worked at STMicroelec-
tronics and then Inmos. He set up digital signal processing (DSP) and digital video

4.9 VideoLogic/Imagination Technologies Tiling (1994–)
189
Fig. 4.19 Sir Hossein 
Yassaie (Courtesy of The 
Times) 
development at Inmos and managed the system divisions. In 1998, the board of 
directors elected him CEO. 
In the early days of the PC, there was no mechanism to bring video in and display 
it. VideoLogic designed the ﬁrst video application program interface (API) for MS-
DOS in 1989 to make it easier for customers to use their products. A few years later, 
in 1992, Microsoft introduced the digital video media control interface (DV MCI) 
with Intel’s support. Microsoft based the interface primarily on VideoLogic’s design. 
In addition to video, Yassaie advocated having the company develop a graphics 
capability. “You cannot have video with also doing graphics,” he commented more 
than once. 
Imagination Technologies established the PowerVR division in 1994 and intro-
duced the PowerVR tiling architecture and tile-based deferred rendering (TBDR). 
PowerVR used display list rendering, batching polygons before rendering them. 
Typically, a graphics controller, accelerator, or GPU would draw polygons one by one. 
PowerVR’s TBDR architecture captures the whole scene before starting to render. It 
then sorts through the image, identifying occluded pixels and rejecting them before 
processing them [34]. A block diagram of the TBDR pipeline is shown in Fig. 4.20.
In the case of other graphics controllers or accelerators, triangles get sent in any 
order to the controller, and the z-buffer decides on a pixel level which polygon will 
appear in front of others. This standard method requires a random-access z-buffer 
with the same X–Y dimensions (or greater) as the screen [35]. 
If you could watch a scene generated (a single frame in less than 33 ms), you 
would see the image created polygon by polygon. If you could see the same scene

190
4
1980–1995 the Progenitors: Graphics Controller on PCs
Fig. 4.20 Tile-based deferred rendering (TBDR) pipeline
drawn by the PowerVR, the picture would start to ﬁll in by tiles, left to right and 
top to bottom. Thanks to that method, VideoLogic could get rid of the external z-
buffer, thereby saving memory and memory bandwidth. Imagination pre-sorted the 
polygons from back to front. It then ignored (deferred) the polygons that were behind 
other polygons. 
The technique of deferred rendering is also known as chunking. Chunking refers 
to strategies for improving performance by using special knowledge of a situation to 
aggregate related memory-allocation requests. For example, if it was known a certain 
type of object was required in groups of eight, instead of allocating and freeing each 
object individually, making sixteen calls to the heap manager, one could allocate and 
free an array of eight of the objects, reducing the number of calls to two. 
VideoLogic’s deferred rendering process works as follows: 
1 The triangle data gets called from the host memory. Then, in the controller or 
accelerator, the triangles for the scene are gathered and chunked (by doing an 
intersect check). 
2 The ﬁrst chunk gets rendered as usual, except it takes place in the chip’s rendering 
cache.

4.9 VideoLogic/Imagination Technologies Tiling (1994–)
191
3 The ﬁnal rendering in the cache is then copied to the back buffer so each pixel is 
written once at most. 
4 Repeat for each chunk. 
5 Flip the back buffer to the front buffer. 
And the process is complete. 
Z-buffer memory and memory bandwidth do not get used. All the z comparisons 
occur on-chip. Only visible pixels drawn in the display memory are textured, shaded, 
and lit, saving unnecessary graphics processing and memory bandwidth for texture 
fetches. 
Henry Fuchs ﬁrst tried Tile-based designs at the University of North Carolina in 
Raleigh in 1989. John Warnock’s algorithm in 1969 was a forerunner to deferred 
rendering [36] (Fig. 4.21). (Warnock is the inventor of PostScript and founder of 
Adobe.) 
Pixel planes and other foundational tiling concepts were more deferred texture 
application than deferred rendering. Each parallel pixel-processing unit (there were 
over 16 k processors) had 208 bits of storage [37]. Each triangle/surface (or higher-
order primitives) received all the math for the visibility and shading, plus u v  calcu-
lations for all the pixels (retaining the data corresponding to the per-pixel nearest). 
All the hidden surface removal (HSR) and shading operations used the same unit 
with the same evaluation units. After the visible surfaces pixel got determined, the 
pixel-processing units picked off just the texel(s) they needed. 
PowerVR VideoLogic separated the HSR and the shading/texturing units. They 
did not do the uv/shading calculations until the visible parts were determined and 
then actively accessed just the relevant texel data rather than broadcasting everything 
(Fig. 4.22).
Fig. 4.21 Martin Ashton 
(Courtesy of Ashton) 

192
4
1980–1995 the Progenitors: Graphics Controller on PCs
Fig. 4.22 Simon Fenney 
(Courtesy of Fenney) 
The inspiration for tiling architecture came from Martin Ashton and Simon Fenney 
in 1992. Martin’s initial PowerVR hardware system had a single object list for the 
entire frame (though it rendered in bursts of 32 × 1 pixels), and the performance was 
not what they wanted. Fenney thought dividing the screen into tiles with a list per 
tile would be more efﬁcient. 
The prototype board was a modiﬁed ﬁeld-programmable gate array (see Fig. 4.23) 
to have 16 tiles, which gave more than a ten times speed-up. Those were much larger 
than today’s tiles—Ashton and Fenney would have liked to have had more but were 
at the limit of the changes possible with the prototype.
The Inmos Transputer inspired Fenney with its four serial ports that could form 
a hypercube.3 Fenney had exploited that ability in 1991 and had created a system 
of ~20 interconnected T800s Transputers. He wrote a ray tracer and a 2D rasterizer 
(in the Occum language). It divided the scene into small tiles, making it easy to 
parallelize over the transputer array.
3 Hypercube, https://en.wikipedia.org/wiki/Hypercube. 

4.9 VideoLogic/Imagination Technologies Tiling (1994–)
193
Fig. 4.23 Prototype TBDR FPGA AIB (Courtesy of Simon Fenney)
4.9.1 
NEC-Imagination Technologies PCX (1994–1999) 
Fenney was one of the lead architects of the Trident Project that VideoLogic started 
in 1992 to develop the TBDR design. In 1994, the company signed up NEC to build 
it. The ﬁrst version of the device shipped in 1996. 
The company established PowerVR as a division of VideoLogic. In 1994, it intro-
duced the PowerVR graphics controller. It announced that NEC and VideoLogic 
had launched a framework agreement for the collaborative development of 3D tech-
nology. VideoLogic would license its new technology, called PowerVR, to NEC 
Electronics. NEC would manufacture (fab) the device and sell the PowerVR product 
under an exclusive worldwide license as a sole distributor. PowerVR was IMG’s 
real-time 3D image technology developed at VideoLogic over three years. NEC also 
agreed to buy 2.29 percent of the VideoLogic Group as part of the deal. 
The 3D accelerator chips were designated Series 1, a.k.a. PowerVR (the 1 
implied), or PVR. Trying to be all things to the market, VideoLogic had a smor-
gasbord of names, series, code names, brands, and part numbers and would refer to 
them interchangeably, requiring outsiders to carry a decoder chart. The NEC partner-
ship would start with the Midas3, which became the PCX1 of the Series 1 PowerVR 
brand. 
By 1996, NEC had produced chips and introduced a new family of 3D graphics 
processors. The company targeted the arcade/console markets with a multichip solu-
tion and the PC market with a single integrated chip. The arcade chipset consisted 
of an image synthesis processor (ISP) and a texture shading processor. 
NEC marketed the PC family under the name PowerVR Technology. NEC said 
it would provide a high-level API developed by VideoLogic called PowerVR SGL. 
The API would support arcade/console and PC implementations, and the company 
claimed it would signiﬁcantly reduce cross-platform development costs [38]. 
A little while later, NEC Electronics announced a key design win with Compaq 
for its new lineup of Presario home PCs due out later that year. Compaq wanted it 
known it was planning a product based on the PowerVR architecture in advance of 
the E3 gamer conference to encourage developer support of this technology [39].

194
4
1980–1995 the Progenitors: Graphics Controller on PCs
4.9.1.1
Series1 PCX2—1997 
On March 25, 1997, NEC announced the second version of its PowerVR 3D 
controller, the PCX2 (and although it was the second device, it was still part of Series 
1 of PowerVR architectural design). The PowerVR PCX2 (code named Midas 5 or 
CLX2) was launched in April 1997. NEC and VideoLogic claimed it would offer 40 
percent higher performance than the market leader, 3dfx’s Voodoo Graphics. It was 
a process shrink, from 500 to 350 nm. That got the chip a 10 percent boost in pixels 
and texels per second—far from the 40 percent promised. 
According to Dr. Johannes Baston, NEC’s senior marketing engineer in Europe, 
“The new chip delivers 50 percent more performance than the PCX1 it replaces.” 
[40] Trevor Wing, VP of marketing at VideoLogic, said, “The new chip’s increased 
performance will enable higher 3D frame rate, higher resolutions, and better color. 
With it, we are encouraging game developers to break out of the 640 × 480 limita-
tions of today’s games.”[40] Unfortunately, the facts did not support management’s 
enthusiasm for typical game titles. Counterintuitively, the products’ performance 
appeared better for more complex workloads because of its TBDR. 
VideoLogic’s Apocalypse 5D AIB (Fig. 4.24) took a multichip approach. The 5D 
combined Tseng Labs’ MDRAM-based ET6000 2D VGA accelerator with NEC’s 
PowerVR-based PCX2 chip. The Apocalypse 5D was similar to an AIB made by 
Hercules (Stingray 128/3D) that used a 3dfx Voodoo Rush chip Alliance made, but 
with one crucial difference: VideoLogic did not rely on z-buffer to render images 
[41]. 
The PCX2 added features over the PCX1, such as bilinear and adaptive ﬁltering, 
dithering from 24 to 16 bit, and compositing. The feature set consisted of the 
following:
Fig. 4.24 VideoLogic’s Apocalypse 5D. (Courtesy of Fabian Günther-Borstel) 

4.9 VideoLogic/Imagination Technologies Tiling (1994–)
195
• On-chip HSR and deferred texturing. 
• Tile-based architecture*. 
• Up to 1.5 million textured, shaded polygons/second. 
• Up to 70 million pixels/second ﬁll rate. 
• Bilinear and adaptive bilinear texture ﬁltering (Talisman-like). 
• Enhanced triangle setup. 
• Multipass rendering. 
• Mip-mapped, anti-aliased texturing, Gouraud shading. 
• Multimode translucency. 
The chip’s adaptive ﬁltering offered bilinear ﬁltering for close-to-front objects 
and perspective-correct mipmapping when in the background. Like its predecessor, 
the PCX2 used VideoLogic’s Inﬁnite Surface-based algorithm, which provided low-
level polygon mesh-based modeling. It deﬁned convex polygons by edge surfaces, 
which allowed the controller to use existing mesh models. Because bounded surfaces 
deﬁned convex objects, high-level object modeling got used as well. That, in turn, 
supported functions such as shadows and spotlights (of which some game makers 
were taking advantage, such as turning on a car’s lights at night). Other beneﬁts of a 
surface-based algorithm were that shadows were cast accurately on different objects 
in the scene and could be proportional to the light’s intensity, very much like ray 
tracing. 
The PCX2 was pin and software compatible with the PCX1 and packaged in a 
208-pin PQFP. 
The company was trying to show how it could enhance games that used Intel’s 
SIMD processor MMX. The PCX2 could run Planet of Death (Fig. 4.25), a popular 
game from Ubisoft, as a demonstration. Imagination would switch the PCX2 on and 
off to show how the game looked with just MMX and MMX accelerated by PCX2. 
The results were impressive—but could they be trusted?
4.9.1.2
Not Playing Fair 
Relationships between the vendors and their suppliers were about to change, causing 
a substantial alteration in the market landscape. 
In April 1997, Jon Peddie Associates (JPA), a computer graphics consulting ﬁrm 
in Northern California, was contracted by NEC Electronics to conduct a round of 
Direct3D benchmark testing to support NEC’s claim of its next-generation PowerVR 
accelerator, the PCX2. The test results were indeed favorable, and NEC included the 
results in a press release for the PCX2. 
After the results were made public, JPA discovered NEC’s driver optimization and 
defeated it by simply renaming the binary executable test. JPA reran the tests and got 
lower results. After ﬁnding the optimization, JPA felt the data reported in NEC’s press 
release was, at best, an incomplete picture of the PowerVR’s actual performance and 
reported the change in its assessment. The ﬁrm felt no end-user beneﬁt would come 
from making a speciﬁc brute-force optimization for a benchmark test.

196
4
1980–1995 the Progenitors: Graphics Controller on PCs
Fig. 4.25 Planet of death (Courtesy of Ubisoft)
NEC had a different opinion and issued the following statement: “NEC Elec-
tronics, Inc. and VideoLogic Ltd. stand behind the Microsoft Direct3D benchmark 
Figs for PowerVR completed on March 28, 1997, by JPA, and subsequently published 
by NEC Electronics in a press release issued Monday, April 7, 1997.” 
VideoLogic promised a new driver release for the shipping product that would 
duplicate or surpass the performance JPA saw in its tests of the optimized drivers. 
Soon after, NEC and VideoLogic introduced the PowerVR Ready Games 
Enhancement Program with unique branding to identify games optimized for the 
PowerVR boards. Imagination explicitly optimized PowerVR drivers for each game 
in the program. That at least informed gamers of what they were getting. 
4.9.1.3
Series2 CLX2, PMX1—1998–2000 
In 1999, the company entered into a licensing agreement with NEC (now Renesas) 
for VideoLogic’s PowerVR technology. NEC also invested in VideoLogic (gaining 
a 2.29 percent stake in the company) and acquired the rights to manufacture and sell 
the chip. That deal changed the course for the company. In 1999, Yassaie made a 
strategic corporate decision to reorient the company, become an IP licensing ﬁrm, 
and change the name to Imagination Technologies (Imagination). 
The PowerVR Series2 controllers, or accelerators (code named CLX2), repre-
sented another process shrink. Imagination developed the chip for the Sega Dream-
cast console. Winning the Dreamcast business was good news for VideoLogic and

4.9 VideoLogic/Imagination Technologies Tiling (1994–)
197
NEC. But NEC had limited fab capacity, and expansion would take time and a lot of 
money. Therefore, the PC version, the Neon 250, was a year late to the market, not 
showing up until late 1999. 
NEC said in February 1999 that it was providing sample quantities of the PowerVR 
250 graphics chip. The company said it would integrate 2D and 3D and accommodate 
up to 32 MB of SD/SGRAM. 
“The launch of PowerVR 250 completes the ﬁrst phase of our cross-platform 
strategy for the second-generation technology,” said Trevor Wing, VideoLogic’s VP 
of marketing [42]. 
NEC said multi-texturing would be accomplished at the back end of the 3D 
pipeline, emphasizing the deferred rendering tile approach [43]. 
Engineering evaluation samples of early PowerVR 250 controllers were made 
available by NEC in November 1998. The company said the optimized version of 
the device would be released to manufacturing that month, with volume availability 
planned for Q1 1999 [44]. 
The speciﬁcations looked promising (Table 4.3):
The performance of the Neon 250 was on average with the previous year’s AIBs. 
As a result, it was not very popular and did not sell well. The VideoLogic PowerVR 
series2 Neon was compatible with DirectX 6.0. Its method of full ﬂoating-point 
geometry and texture setup engine put a load on the CPU. The chip was aligned with 
DirectX and had a capable 2D VGA engine. 
The Neon 250 could have kept VideoLogic in the headlines if it arrived on 
time. Inﬁnite planes were dropped to reduce the number of transistors (because 
few developers employed them). However, Neon 250 had other features, such as 
a programmable processor in the front end for rendering and texture compression 
(Fig. 4.26).
The reviews of the Neon 250 were merciless, approaching cruel. One reviewer 
wrote, “The Neon 250 is a big fat blunder in the shape of a video card” [45]. 
At ﬁrst, there were delays in developing the Series 2 technology itself. Then, 
because of limited fab capacity, the PC version of the chip suffered from neglect as 
NEC committed resources to Sega’s Dreamcast with Series 2 chips. 
When the chips did go into production, manufacturing errors required VideoLogic 
to ditch it all and redesign it all over again. All that might have been forgotten about 
if the Neon 250 had been worth the wait—but it was not. 
Visual defects and artifacts appeared in the games’ run on the AIB. Even though 
VideoLogic put many Direct3D and OpenGL rendering tweaks in its drivers, textures 
still popped in and out of place. Even turning on the advanced mipmapping and 
ﬁltering features did not solve the problems. Commented a reviewer, “The Neon 250 
just does not have the muscle on advanced feature set to compete with the big boys” 
[45]. 
In April 1999, VideoLogic would move away from NEC. VideoLogic struggled 
to get a new AIB out, to be known as the Neon, and was embarrassed in the market-
place by NEC’s difﬁculties. Then, on August 31, 1999, VideoLogic reorganized 
and changed its name to Imagination Technologies and continued working on the 
third-generation designs.

198
4
1980–1995 the Progenitors: Graphics Controller on PCs
Table 4.3 Imagination technologies PowerVR features 
2D engine
3D engine 
Full ROP, text, and line primitives
Tile-based reduced bandwidth rendering 
engine 
Full VGA compatibility
32-bit ﬂoating-point z-buffering calculation 
function with no performance penalty 
YUV to RGB color space conversion
Up to 5 M polygons/sec (forward-facing 
delivered to the screen) 
MPEG-2 decode assist (motion compensation 
acceleration) 
Fill rate 200–500Mpixels/sec (depending on 
scene complexity) 
Integrated 230 MHz DAC (1600 × 1200 at 
85 Hz) 
Triangle and Texture Setup 
Color key overlay
Polygon setup engine 
Multiple video windows
Bus mastered parameter fetch 
3D Engine
Advanced texturing (bilinear, trilinear, 
anisotropic, bump mapping) 
Tile-based reduced bandwidth rendering 
engine 
True-color 32 bpp pipeline 
32-bit ﬂoating-point z-buffering calculation 
function with no performance penalty 
Translucency sorting 
Up to 5 M polygons/sec (forward-facing 
delivered to the screen) 
Image supersampling/scene anti-aliasing 
acceleration)
Per-pixel loadable table fog 
Integrated 230 MHz DAC (1600 × 1200 at 
85 Hz) 
Specular highlights with offset colors 
Color key overlay
Alpha + Multipass Blending 
Multiple video windows
Multi-texturing support 
Color key and alpha-blended textures 
D3D and OpenGL blend modes 
Environment mapping
Nonetheless, NEC and Imagination stayed partners and did well on other platforms 
like consoles and arcade systems. 
4.9.2 
Summary 
Introduced in November 1998, the Dreamcast was Sega’s seventh and ﬁnal home 
video game console. Sega discontinued it in early 2001. The Dreamcast used a 
Hitachi SH-4 CPU and VideoLogic PowerVR graphic controller. 
The company achieved a signiﬁcant design win with its PowerVR IP cores in 
October 2006 when Intel decided to use Imagination’s IP GPU. Intel would use the

4.9 VideoLogic/Imagination Technologies Tiling (1994–)
199
Fig. 4.26 The Series2 PMX1, a prototype of what later became the Neon 250 (Courtesy of 
Imagination Technologies)
GPU with its PC, mobile computing, and consumer processors in speciﬁc market 
segments. Intel also bought six million shares in Imagination (representing 2.9 
percent of the company). 
In late November 2008, Imagination announced a license agreement for a high-
performance version of its PowerVR SGX GPU with a new undisclosed partner, 
later revealed as Apple. In December 2008, Apple purchased a 3.6 percent stake in 
the company for £3.2 million. It subsequently became clear that Imagination had 
already worked with Apple since 2006. Its tile-based deferred rendering (TBDR) 
GPU architecture has been the basis for every iPhone and iPad from inception to 
now. 
Yassaie continued to expand the company through acquisitions and, in December 
2010, acquired Caustic Graphics for $27 million. Caustic, founded by a group of 
former Apple engineers, was a hardware/software real-time ray tracing graphics 
technology developer. In December 2011, Imagination signed a licensing agreement 
with Qualcomm for its PowerVR portfolio. 
Imagination was at the top of its game. On December 29, 2012, the royal family 
of the UK awarded Yassaie a knighthood. 
And then things started to slide. Just before being knighted, Yassaie bought MIPS 
Technologies for $100 million. He made several other acquisitions of questionable 
strategic or ﬁnancial value, perhaps in anticipation of what was to come. 
On 8 February 2016, after slumps in proﬁts and spiraling costs, Yassaie stepped 
down as CEO because shareholders called for his resignation. 
In March 2016, rumors circulated Apple had considered buying Imagination 
Technologies but never made an offer. More sinisterly, from 2015 to 2017, Apple 
engaged in a “brain drain” of Imagination Technologies’ engineers and executives. 
It even opened an ofﬁce for chip development in St Albans, close to Imagination 
Technologies’ headquarters. 
In April 2017, Imagination’s stock price fell by 70 percent after Apple said it 
would stop using Imagination’s IP within the next two years. At the time, Apple 
accounted for over half of the company’s revenue.

200
4
1980–1995 the Progenitors: Graphics Controller on PCs
Also, in November 2017, Imagination sold various assets and divisions, including 
MIPS, to the venture capitalist (VC) ﬁrm Tallywood. 
The end? On June 22, 2017, Imagination Technologies announced it was putting 
the company up for sale. Then, on September 25, 2017, the company announced its 
acquisition by Canyon Bridge, a private equity fund backed by Chinese government 
funds. Imagination was delisted from the LSE and returned to private hands. 
Not quite. Apple did not ultimately stop using Imagination’s GPU IP. In January 
2020, Imagination Technologies announced a new multiyear license agreement with 
Apple, including access to a broader range of Imagination’s IP in exchange for 
license fees. The company also announced several design wins in China, notably in 
the desktop and data center segment, and began hiring. It developed AI processor 
technology, got a certiﬁcation for automated automobiles, and announced several new 
families of GPUs. In April 2021, the company announced an intention to return to the 
CPU market with new IP cores built around the RISC-V instruction set architecture 
(ISA). 
4.10 
Conclusion 
The ﬁrst half of the 1990s decade could almost be called the highlight of the graphics 
controller industry. So many companies were being formed, so many new ideas were 
put forth, and so many disappointed and lost investment. It was a period of churn, 
discovery, new alliances, and colossal failures. 
And it created a cadre of graphics chip and AIB developers who would go on to 
other companies, having learned from their mistakes but losing their dream or quest. 
They would be paced by Moore’s law and it could not move fast enough for them. 
References 
1. Nico, K. Big Blue Reneges on Plan to Sell XGA Chips, Infoworld, pp5, (April 1, 1991), https:// 
tinyurl.com/twec7m7s. 
2. Corcoran, C. IBM XGA Standard Flounders as Users Buy GUI Accelerators, Infoworld, 8, 
(December 28, 1992). 
3. Peddie, J. Famous Graphics Chips: The Integrated Graphics Controller, IEEE Computer 
Society, https://www.computer.org/publications/tech-news/chasing-pixels/the-integrated-gra 
phics-controller. 
4. Peddie, J. TriTech Pyramid3D 250x 3D controllers, PC Graphics Report, Volume IX, Number 
44, pp 1434, (October 29,1996). 
5. Eerola, V. Pyramid3DTM Real-time Graphics Processor, IEEE Hotchips, (August 26, 
1997), 
https://old.hotchips.org/wp-content/uploads/hc_archives/hc09/3_Tue/HC9.S10/HC9. 
10.3.pdf. 
6. Briz, B. Microsoft to License TriTech Microelectronics 3-D Graphics Technology, (March 25, 
1998), https://news.microsoft.com/1998/03/25/microsoft-to-license-tritech-microelectronics-
3-d-graphics-technology/.

References
201
7. CBR Staff Writer, Tritech Withdraws from 3D Graphics Chip Race, Tech Monitor, (May 17, 
1998), https://techmonitor.ai/technology/tritech_withdraws_from_3d_graphics_chip_race. 
8. Crystal Semicond. v. Tritech Microelec, https://casetext.com/case/crystal-semicond-v-tritech-
microelec. 
9. Chartered Semiconductor Manufacturing 1999 Annual Report, http://media.corporate-ir.net/ 
media_ﬁles/irol/93/93866/reports/CHRT_1999_AR.pdf. 
10. Peddie, J. Famous Graphics Chips: Artist Graphics GPX, IEEE Computer Society, https:// 
www.computer.org/publications/tech-news/chasing-pixels/artist-graphics-gpx. 
11. Peddie, J. Famous Graphics Chips: Number Nine’s Imagine 128, IEEE Computer 
Society, 
https://www.computer.org/publications/tech-news/chasing-pixels/famous-graphics-
chips-number-nines-imagine-128. 
12. Number Nine Visual Technology Company History, Number Nine Visual Technology, (1999), 
https://web.archive.org/web/19990117083724/http://www.nine.com/about.html. 
13. Bruno, F. Open Source Graphics Processor (GPU), Kickstarter,  https://www.kickstarter.com/ 
projects/725991125/open-Source:-graphics-processor-gpu. 
14. Radius Inc. Company-Histories.com, https://www.company-histories.com/Radius-Inc-Com 
pany-History.html. 
15. Borrel, J. Verbatum: An interview with Mike Boich, president Radius Corporation, Macworld, 
Vol. 5 no. 3. p. 81–03, (March, 1988), https://archive.org/details/MacWorld_8803_March_1 
988/page/n83/mode/2up?q=boich. 
16. Gengo, L. Rapidly Rising Radius Moving into Its Second Mansion, Business Journal-San Jose, 
p. 9, (September 5, 1988). 
17. Carmack, J. The rendition 3d accelerated version of Quake looks very good. (Aug 22, 1996), 
John Carmack Archive - plan (1996), (March 18, 2007), https://fabiensanglard.net/fd_proxy/ 
doom3/pdfs/johnc-plan_1996.pdf. 
18. Podell, S. VQuake, BSS post (October 30, 1997), https://www.vogonswiki.com/index.php/VQu 
ake#Details. 
19. Peddie, J. Rendition introduces Verité 2000 family, The Peddie Report, (August 16, 1997). 
20. Foremski, T. (June 25, 1990) Micron Technology to acquire Rendition, https://tinyurl.com/ca3 
5na7s. 
21. Peddie, J. Famous Graphics Chips: Stellar—RSSI (1993 – 2000), IEEE Computer Society, 
https://www.computer.org/publications/tech-news/chasing-pixels/stellar-rssi-1993-2000. 
22. Peddie, J. (August 13, 1996) S-MOS dates Tseng PC Graphics Report, Volume IX, Number 33 
pp 1125. 
23. Fischer, A. S-MOS and Reality Simulation Systems partner for 3D, PC Graphics Report, Volume 
IX, Number 15. pp 500, (April 9, 1996).. 
24. Brown, P. A Stellar Market Entrance—Stellar Semiconductor, Electronic News, (1998) https:// 
indexarticles.com/business/electronic-news/a-stellar-market-entrance-stellar-semiconductor/. 
25. StellarSemiconductors-3DGraphicsProcessors, Pinestream Consulting Group, (1997), http:// 
www.pinestream.com/demodetail/1184/Stellar-Semiconductors. 
26. Fisher, A. Stellar Semiconductor now open for business, The Peddie Report, Volume XI, 
Number 11, pp 339, (March 23, 1998). 
27. Peter, C. Stellar and Sican Team Up on Cores, EETimes, (November 3, 1999) http://www.eet 
imes.com/story/OEG19991103S0011. 
28. Thomas, M. Broadcom acquires 3D graphics technology, Electronics Weekly, (March 
3, 2000) 
https://www.electronicsweekly.com/news/archived/resources-archived/broadcom-
acquires-3d-graphics-technology-2000-03/. 
29. SBN News Staff, Broadcom Buys Stellar Semiconductor to Expand into 3D, EETimes, 
(March 3, 2000), https://www.eetimes.com/broadcom-buys-stellar-semiconductor-to-expand-
into-3D/. 
30. Broadcom acquires Stellar Semiconductor to expand 3-D offerings EETimes, (March 2, 
2000), https://www.eetimes.com/broadcom-acquires-stellar-semiconductor-to-expand-3-d-off 
erings/.

202
4
1980–1995 the Progenitors: Graphics Controller on PCs
31. Lorne Trottier Acquires Full Ownership of Matrox, to Lead New Era of Tech Innovation, 
Matrox, (September 6, 2019), https://www.matrox.com/en/video/media/press-releases/lorne-
trottier-acquires-full-ownership-matrox-to-lead-new-era-tech-innovation. 
32. Peddie, J. Famous Graphics Chips: Matrox MGA, IEEE Computer Society, https://www.com 
puter.org/publications/tech-news/chasing-pixels/matrox-mga. 
33. Arial, T. Lorne Trottier’s Ever-Expanding Universe, The Montrealer, (March 3, 2017), https:// 
themontrealeronline.com/2017/03/lorne-trottier-ever-expanding-universe/. 
34. Tile-Based Deferred Rendering (TBDR), https://docs.imgtec.com/PowerVR_Architecture/top 
ics/powervr_architecture_tile_based_deferred_rendering__tbdr.html. 
35. Introduction to PowerVR for Developers, (December 5, 2021), http://cdn.imgtec.com/sdk-doc 
umentation/Introduction_to_PowerVR_for_Developers.pdf. 
36. Warnock, J. A Hidden Surface Algorithm for Computer Generated Halftone Pictures (doctoral 
thesis, University of Utah, 1969). 
37. Cohen, J. Virtual Worlds, Computer Science 600.460, Johns Hopkins University, (Spring 2000), 
https://www.cs.jhu.edu/~cohen/VW2000/syllabus.html. 
38. Peddie, J. NEC Introduces 3D Graphics Processor Family, The PC Graphics Report 9, no. 9. 
(February 27, 1996). 
39. Peddie, J. NEC and Compaq Choose Sides for the 3D Tournament, The PC Graphics Report, 
9, no. 20, (May 14, 1996). 
40. Peddie, J. NEC shows PCX2, The PC Graphics Rport, Volume X, Number 12, pp 351, (March 
25 1997). 
41. Hill, J. and Jan Ozer, J. 3-D Showdown, PC Mag, (June 24, 1997), https://tinyurl.com/y85 
35krm. 
42. Mullen, M. NEC Begins Sampling PowerVR 250, Gamspot, (April 28, 2000), https://www.gam 
espot.com/articles/nec-begins-sampling-powervr-250/1100-2465730/. 
43. Maximum PC, (February 1999), https://tinyurl.com/2uzdzpfh. 
44. Taken, F. PowerVR 250 Details!, Tweakers, (December 2, 1998), https://tweakers.net/nieuws/ 
164/powervr-250-details.html. 
45. Downey, S. Neon Dead on Arrival, Maximum PC, 86, (January 2000), https://tinyurl.com/b7z 
59d4w.

Chapter 5 
1990 to 1999 Graphics Controllers 
on Other Platform 
In the early 1990s, large proprietary processor-based workstation companies like 
Evans & Sutherland, HP, Intergraph, SGI, Sun, and others were transitioning to 
commercial off-the-shelf semiconductors like Intel’s × 86 CPUs and graphics AIBs 
with 3Dlabs, TI, and Hitachi processors. Compaq, Dell, and specialty companies 
like Boxx demonstrated the cost advantages. Intel had introduced its Pentium Pro 
processor aimed at OEMs in the visualization and CAD workstation market, and 
the handwriting was on the wall. A COTS supplier like Intel could offer economies 
of scale that were double to 10 times what an individual supplier like Sun HP or 
SGI could realize. It was a gut-wrenching decision to give up the differentiating 
proprietary processors in order to be price competitive with start-ups with little to no 
R&D or differentiation. 
The transition did not just disrupt the high-end workstation market; the game 
console market suffered the same intrusion. And those machines, although much 
higher in volume, had a very much smaller part cost budget. 
All these platforms would beneﬁt from consumer-priced integrated GPU, as will 
be demonstrated. Some however fought it and could not evolve and are no longer 
with us. 
5.1 
Workstations 
A workstation is described as a high-performance computer system that is basi-
cally designed for a single user and has advanced graphics capabilities, large storage 
capacity, and a powerful central processing unit. However, the deﬁnition and differ-
entiation of a workstation has become ever more difﬁcult as PCs get more powerful 
processors, memory, networking, storage, and graphics. A workstation is typically 
more capable than a PC and less than a server (which can manage a large network of 
peripheral PCs or workstations and handle immense data-processing and reporting 
tasks).
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2022 
J. Peddie, The History of the GPU - Steps to Invention, 
https://doi.org/10.1007/978-3-031-10968-3_5 
203

204
5
1990 to 1999 Graphics Controllers on Other Platform
It used to be that high-end workstations could accommodate high-resolution or 
three-dimensional graphic interfaces, sophisticated multitask software, and advanced 
abilities to communicate with other computers, but now PCs can do that too. Work-
stations are used primarily to perform computationally intensive scientiﬁc and engi-
neering tasks. They have also been used in some complex ﬁnancial and business 
applications. 
Workstation were differentiated by error-correcting RAM (ECC), larger amounts 
of RAM (a PC would top out at 32 GB, while a workstation could have 128 GB, 
enterprise level hardware security, and high-resolution graphics. Workstations used 
to be differentiated by using Intel Xeon processors, sometimes two of them. When 
intel introduced the 12th Gen CPUs in 2022 it incorporated ECC capability, very 
high RAM capacity, and added PCIe lines for graphics and NVMe SSDs for PCs 
and for workstation, leaving the vPro security chipset for workstations, and usually a 
higher-capacity power supply (PSU) AMD continued to offer its Ryzen Threadripper 
PRO processors for workstations. 
Workstations typically run the CPU and graphics at a slightly slower speed than a 
PC. This may seem counter-intuitive when a workstation is (or was) considered high-
performance. However, workstations were and are sold based on super reliability. 
Companies buy them for mission critical projects, and they often are run 24 h a day. 
Turning down the clocks, having a higher capacity PSU, better cooling and tool-less 
servicing to replace parts are now what differentiates workstations. 
5.1.1 
Workstation Graphics 
High-end graphics terminals were one of the ﬁrst computer graphics products to 
emerge. In the early days the computer display was an integral and integrated part 
of the computer. IBM introduced a stand-alone graphics terminal, the 2250 display 
terminal in 1965, and showed the way. Xerox PARC was developing smart terminals, 
terminals with their own dedicated computer which would become known as work-
stations. In 1985 IBM introduced the famous IBM 5080 color 3D high-resolution 
(1024 × 1024) powered by an IBM 3250 graphic display system that did 2D/3D 
transformations. A half dozen other companies were producing workstations. Work-
station AIBs emerged, and the category expanded. Some examples that lead to the 
single chip GPU are discussed in the following sections. These examples show how 
the industry was moving toward the GPU perhaps not in unison or with an interna-
tional declaration, but as goal of getting better, faster, and yet lower cost graphics 
processing capabilities. 
In 1999 Dell entered the workstation market and used commodity off the shelf 
components offering comparable performance and signiﬁcantly lower prices. The 
big-iron workstation makers as they were called scoffed at a Windows-based machine 
with Intel CPU and who knows what graphics controller being able to match the 
performance or accuracy of their classic system with Unix. By 2021 there would be 
no custom-made proprietary workstations.

5.1 Workstations
205
5.1.2 
HP Artist (1993) 
HP established its workstation division in Fort Collins, Colorado in 1980. For the 
ﬁrst 18 years, HP designed and built proprietary workstations creating the PA-RISC 
processors, the HP-UX operating system, HP proprietary graphics accelerators and 
even memory, hard drives, displays, and input devices. The HP-developed graphics 
accelerators began as separate chassis including creation of a standalone graphics 
accelerator based on the PixelFlow architecture in collaboration with the University 
of North Carolina’s Chapel Hill Graphics Lab. With HP’s 1989 acquisition of Apollo 
Computer a new graphics architecture was born: HP’s Next Generation Low End (HP 
NGLE), incorporating the best aspects of HP and Apollo’s graphics architectures. 
Through this 18 year time period, the HP Graphics Lab created over 150 graphics-
speciﬁc patents. 
In 1993, HP competed head-to-head in the workstation market with IBM, Sun, 
DEC, NEC, and others. Those companies built machines and many of the components 
in them. Facing heavy competition from Sun, HP set the following design goal 
for its development team: its new 32-bit HP 9000/712 workstation would reach 
the performance levels of 1992 era workstations and servers at a fraction of their 
fabrication costs. Their target was the earlier generation HP 9000/735. To accomplish 
this goal, HP employed VLSI technologies for the processor components, which were 
state of the art at the time [1]. 
What they came up with became the heart of HP’s workstation line: the famous 
PA-RISC CPU. There was an equally impressive coprocessor for graphics—the HP 
Artist chip (no relationship to the Artist graphics 3GA chip described in chapter 
four), which replaced the CRX window accelerator AIB. That board marked the 
beginning of standardized graphics-hardware (HP-NGLE) architecture for window 
system acceleration. 
HP [2] chose that architecture for its simplicity of implementation and the clean 
model it presented to the software driver developers. One of HP’s fundamental design 
decisions was to accelerate key primitives only—a RISC approach. Many earlier 
controllers chose to run the gamut of graphical operations, including ellipses and 
arithmetic pixel operations. Graphics subsystems designed with those controllers 
were typically expensive and exhibited only moderate window system performance. 
In the CRX and subsequent accelerators, including the Model 712’s graphics chip, 
HP decided to accelerate a carefully chosen smaller set of primitives, described in 
the following sections. 
When the engineers at HP approached the problem of reducing costs, there were 
three major areas to address (in order of priority): 
• Fast 2D GUI 
• Digital video decompression support—both locally and over LAN/WAN 
• Efﬁcient 3D graphics. 
Additionally, the designers included:

206
5
1990 to 1999 Graphics Controllers on Other Platform
• Vector, rectangle, frame buffer bitBLT, text, and cursor hardware 
• Bit/pixel frame buffer access mode, VRAM block write 
• Boolean raster operations 
• Two lookup tables to reduce palette conﬂict. 
The Artist chip was the industry’s ﬁrst single chip which combined a GUI accel-
erator, a frame buffer controller (32 bits wide), two lookup tables (LUTs), video 
timing, cursor control, and an integrated LUT-DAC. The chip could support 1 or 2 
Mbytes of VRAM and provided 8 bits up to its highest resolution of 1280 × 1024 at 
72 Hz non-interlaced refresh. A ninth bit controlled the selection of one of the two 
LUT-DACs. The chip also included a built-in programmable PLL that eliminated the 
need for a timing crystal [3]. 
The design balanced the CPU’s strengths with those of the graphics controller; it 
performed the video compression and decompression on the CPU while performing 
color space conversion and compression/decompression on the HP Artist chip, 
illustrated in Fig. 5.1. 
HP also developed a proprietary color compression algorithm (HP Color 
Recovery) that could squeeze 24-bit color to 8 bits while retaining the look of 24-bit 
color. 
The performance of the Artist chip was impressive at the time: 
• Large rectangle ﬁll—850 Mega Pixels/s (Mpixels) 
• Vectors/s (10-pixel random)—21 M/s 
• 10 × 10 rectangles—1.7 M/s 
• Text (6 × 13 characters/s)—1 M 
• 3D transformed vectors/s > 1 M 
• Frame buffer bitBLT (unaligned pixels/s)—47 M 
The CPU handled transformations, clipping, and lighting, z-buffering, and pixel 
color interpolation for polygons. The Artist chip took care of vector rasterizing and 
color compression into the frame buffer. The chip had 70 ns VRAMs and reached a 
page mode speed of 37.5 ns using the VRAM for the plane mask, extended data out, 
and block copying. As a result, the chip could deliver 850 Mpixels/s for constant-color 
objects. Refer to the block diagram in Fig. 5.2.
Fig. 5.1 HP’s balanced compression/decompression with CPU and HP Artist chip 

5.1 Workstations
207
Fig. 5.2 HP’s Artist chip block diagram 
The chip was built using the 800 nm 3-layer (aluminum) HP CMOS26B process 
and had 525,000 transistors. The die size was 9.7 × 12.1 mm, and HP packaged it in 
208-pin metal quad ﬂat-package (QFP) or 240 metric QFP (MQFP) with ﬂat-panel 
driver output. The chip had a 40–80 MHz GUI/RAM clock and generated a 25–135 
MHz video output, and it only used 3.5 W under the heaviest workloads.

208
5
1990 to 1999 Graphics Controllers on Other Platform
As mentioned, low cost was the primary objective of the graphics chip’s design. 
As a measure of HP’s success, the manufacturing cost for the Model 712 graphics 
subsystem was one-third of the cost of the original CRX graphics subsystem. In 
addition, the entry-level 1024 × 768-pixel version of the graphics chip was ﬁve 
times less than the CRX subsystem while being 50% faster. 
HP achieved those cost reductions through an aggressive amount of integration. 
The graphics chip represents the culmination of a series of optimizations of the CRX 
family, combining almost the entire GUI accelerator onto a single chip. The only 
signiﬁcant function not integrated was the frame buffer. 
The HP Artist chip was one of the ﬁrst to employ software programmable resolu-
tions. One of the problems with previous workstation graphics subsystems was that 
they operated at a ﬁxed video resolution and refresh rate. This posed problems in 
conﬁguring systems at the factory and during customer upgrades. 
The Artist graphics chip incorporated an advanced digital frequency synthesizer 
that generated the clocks necessary for the video subsystem. The synthesizer, which 
used HP’s proprietary digital phase-locked loop technology, allowed software conﬁg-
uration of the resolution and frequency of the video signal. Thus, it was possible 
to connect different monitors without changing any video hardware. The initially 
supported formats included 
• 640 × 480 pixels with 60 Hz, standard VESA timing 
• 800 × 600 pixels at 60 Hz refresh 
• 1024 × 1024 pixels at 75 Hz and ﬂat panel 
• 1280 × 1024 pixels at 72 Hz. 
As new monitor timings appeared, one could program the graphics chip with the 
parameters associated with the new monitor. 
5.1.2.1
Summary 
HP created the graphics chip from a system-level-optimized design approach, trying 
to optimize the use of technology. This strategy enabled them to meet their goals of 
low manufacturing cost, while still getting good performance at their cost point. They 
also had to achieve architectural compatibility, while introducing some important 
new functionalities. The Artist chip was a breakthrough product for HP and served 
them well for many years. But it would not be enough, and the company would 
have to evolve or die—it evolved, but it was not painlessly. As HP moved from 
proprietary to COTS parts, design centers would be closed, and design engineers 
would be transferred or let go. But the company did it and became one of the leading 
workstation suppliers. The Artist AIB was HP’s last proprietary workstation graphics 
product.

5.1 Workstations
209
5.1.2.2
HP Epilogue 
In the 1980s and early 1990s, semiconductors were laid out on large backlit plotter 
tables. The layout engineers would put a signature (a symbol, cartoon, or picture) 
in the pattern somewhere. They did that for three reasons: to sign a work of art, for 
copyright protection, and because they could (it was not easy). Later, when chip layout 
used electronic design automation (EDA). Many designers continued the tradition 
(Fig. 5.3). 
The above micrograph is a logo concealed on an HP chip from the early 1990s. 
The same chip also featured 20 designers’ initials and the following message: “If 
you can read this you are too damn close!”. 
5.1.3 
Silicon Reality (1994–1998) 
SRI was formed in Seattle, WA in 1994 when Steve Tibbits from Pico and George 
Saul from Xtar who specialized in ﬂight simulation for the aviation industry. They 
developed ideas for a 3D graphics engine. Pico had been focused on VLSI circuit 
design and was founded by Tibbits in 1994 as a design engineering ﬁrm and was 
doing quite well. Saul became SRI’s president in January 1997. He was a veteran of 
the original Fairchild Semiconductor and, later, National Semiconductor and Hitachi 
(at Hitachi America he was VP and Deputy General Manager). Silicon Reality. 
In October 1996, SRI announced the TAZ Core (for Texture, Anti-alias, Z-buffer) 
and claimed it could perform polygon setup at 1.4 million polygons/s and ﬁll 75 
million textured pixels/s. The company said full featured graphics controllers using
Fig. 5.3 Signature in an integrated circuit chip (Courtesy of Florida State University’s Silicon Zoo 
project) 

210
5
1990 to 1999 Graphics Controllers on Other Platform
Table 5.1 Silicon Reality’s 
TAZ core speciﬁcations 
Trilinear mip-mapped 
perspective correct textures 
4 × 4 subpixel anti-aliasing 
Built-in polygon setup
Over 30 fps scene update rate 
Renders over 1,400,000 
Gouraud shaded polygons/s 
Automatic level of detail 
Renders over 1,000,000 
mip-mapped and ﬁltered 
polygons/s 
Transparency, smoke, fog 
Pixel ﬁll rate of 75 million 
pixels/s 
Fully double buffered frame 
store 
24- or 16-bit Z-buffer with 
8-bit stencil buffer 
Adjustable resolution and color 
depth 
Easy-to-use high level 
polygon command interface 
Direct access to Frame and 
Z-buffers 
the TAZ Core would be available in the ﬁrst quarter of 1997. According to CEO 
Tibbits, the command stream and texture modes have been optimized for D3D. 
“The core has all of the high-end features like polygon setup, trilinear MIP 
mapping, perspective correction, per-pixel fog and transparency, anti-aliasing and 
a selection of texture modes that are typically only found in very expensive ﬂight 
simulators. One of the items that differentiates our product is that all of these features 
can be turned on with little or no impact on performance,” said, Terry Coleman, VP 
of Engineering. 
The TAZ Core features list included (Table 5.1). 
According to SRI, the TAZ Core logic 3D pipeline performed over 26 billion 
operations/s (26 Giga-operations per second—GOPS). The company claimed no 
other dedicated rasterization processor, DSP, or general purpose processor could 
match the rendering performance of TAZ. Double or single frame buffers were 
supported and could be implemented with RDRAM, MDRAM, SDRAM, or other 
advanced memory technologies, refer to Table 5.2. The TAZ-Core was available in 
register-transfer level (RTL) Verilog form.
The TAZ Core accepted commands through a PCI or AGP interface. For 3D oper-
ations those commands consisted of register instructions for mode set-up, polygon 
commands that specify the attributes of a polygon, vertex commands for triangle 
construction, texture load commands, and triangle commands to invoke rasteriza-
tion. The register commands included video timing control, OpenGL mode selection, 
memory conﬁguration, texture and fog control, and various other parameters. The 
TAZ Core was compatible with OpenGL, D3D, RenderWare, BRender, and Heidi 
APIs, among others. Figure 5.4 shows the organization of the controller.
TAZ’s 3D pipeline decoded incoming commands, sorted edges, interpolated, anti-
aliased, applied textures, mixed fog, did depth compare and wrote resulting pixels 
into the frame buffer. While that was happening the video display was refreshed 
from the alternate frame buffer. The high pixel ﬁll rates in the texture mode were 
accomplished using an on-chip texture cache which could be loaded through a bus

5.1 Workstations
211
Table 5.2 Silicon Reality’s 
TAZ 3D mode speciﬁcations 
Resolution
640 × 480 
800 × 600 
1024 × 768 
Bits per-pixel (RGBA)
32
32
16 
Double buffered
Y
Y
Y 
Screen refresh (Hz)
75
75
75 
Pixel ﬁll rate 
(millions/s) 
75
75
75 
Texture maps (32 × 32 
× 32)* 
380
380
380 
Textured polygons/s
1.06 M
1.06 M
1.06 M 
Z-Buffer
Y
N
N 
*Assumes a single (optional) 2 Mbyte texture memory (up to 32 
Mbytes allowed)
Fig. 5.4 Silicon Reality’s TAZ Core function block diagram
interface or from dedicated texture memory. The TAZ Core would support 32 bits 
of Z-buffer per-pixel or 24 bits of Z with 8 bits of stencil buffer. The core logic was 
designed to provide true color (24-bit) fully textured animation at 30 fps with screen 
resolutions up to 1280 × 1024. TAZ’s frame buffer would interface with a variety of 
memory types, including SDRAM and RAMBU.S. 
SRI’s TAZ had the feature set and performance criteria to match up to the better 
3D controllers on the market. The feature set was a combination of D3D and OpenGL 
acceleration functions, and right on the money for 1997. It is funny how we never 
referred to consumer 2D and commercial or professional 2D in the past. It looked 
like 3D was going the same way. Get the feature sets to satisfy the PC market and

212
5
1990 to 1999 Graphics Controllers on Other Platform
let pricing and performance position you in the market. However, there were a lot 
of problems for semiconductor vendors as they grappled with the issue of satisfying 
the two wildly contrasting worlds of OpenGL and D3D. 
By May 1997 the company had expanded its staff and made George Saul President 
and Lawrence Leske as VP to take a little pressure from Steve Tibbitts. 
Silicon Reality developed the TAZ 3D graphics architecture about the same time 
as E&S did REALimage. 
The ﬁrst product of the TAZ family was the Tantrum (previously code named 
Tiger), an integrated 64-bit 3D/2D VGC with triangle setup, rasterization, texturing, 
and Gouraud shading. The chip was expected to reach 1,000,000 trilinear mip-
mapped, perspective corrected, textured polygons/s while maintaining a peak ﬁll 
rate of 75 M pixels/s with all options on. The organization of the chip is shown in 
Fig. 5.5. 
To accomplish that task (all options on), the TAZ engine generated one 64-bit 
pixel per clock (8888-RGBA + 24 bits of Z + 8 bits of stencil) for real-time texel 
ﬁll capabilities. In addition, the chip had a dedicated texture memory interface. The 
company believed dedicated texture memory optimized high-performance 3D. The 
implementation of the dedicated memory is shown in Fig. 5.6.
The chip’s 3D features included (Table 5.3).
However, the company wanted to point out the Tantrum had an integrated 2D 
VGA as well and listed its 2D features as (Table 5.4).
Additional features included bus mastering and high-level command input stream, 
integrated true-color 220 MHz DACs, and EDO or SGRAM for texture and frame
Fig. 5.5 Silicon Reality’s Tantrum block diagram 

5.1 Workstations
213
Fig. 5.6 Silicon Reality’s 
Tantrum product functional 
block diagram
Table 5.3 Silicon Reality’s 
Tantrum features 
Polygon setup of over 1 
million polygons/s 
Perspective correction 
Pixel ﬁll rate over 75 million 
pixels/s 
As a VGC, the chip’s video 
features contain: 
Trilinear texture mapping
YUV to RGB color space 
conversion 
Gouraud and specular 
shading 
Continuous scaling of any size 
or shape 
Z-buffering
True-color video in a window 
Anti-aliasing
Mixed color depths for 
video/graphics 
Automatic level of detail
Color keying 
Per-pixel transparency and 
fog 
VMI 1.4
Table 5.4 Silicon Reality’s 
2D features 
64-bit high-speed windows acceleration Line and pattern draw 
High-performance bit block transfer
Hardware cursor 
Color expansion and color keying
Clipping 
Polygon ﬁll
Overlay 
Raster operations
buffer (2, 4, 8 Mbytes). The range of modes and resolutions supported is as shown 
in Table 5.5.
The company claimed it would offer full featured device driver support with 3D 
standards such as Direct3D and OpenGL for Windows 95 and NT. 
The Tantrum was packaged in a 272-pin BGA and fabricated in 350 nm tech-
nology. Engineering samples were expected in September and production quantities 
by December.

214
5
1990 to 1999 Graphics Controllers on Other Platform
Table 5.5 Silicon Reality’s 
Tantrum, range of display 
formats 
Resolution
Bits per-pixel in 
2-D modes 
Bits per-pixel in 
3-D modes 
320 × 240 through 
1600 × 1200 
8, 16, 32
8, 16, 32, 64
Tantrum looked good on paper, and the company had been moving steadily 
forward in their development. Based on the speciﬁcations, it looked like it would 
come out a little on the high side of mainstream and be targeted at the professional 
workstation market. This seemed like a smart strategy—allowing Silicon Reality to 
prove the product and then scale it down for the high-volume game market. 
But then in July 1998 Evans & Sutherland announced it has acquired the assets 
of Silicon Reality. The asset purchase was concluded on June 26, 1998. Financial 
terms were not disclosed. 
SRI was late with their chip and Saul spent the last year and a half raising capital 
and looking for customers. The one big customer they had in Japan backed out of 
the deal and that left the company hurting. In his travels Saul called on Jeff Dunn 
CEO at AccelGraphics and Dunn liked TAZ. But Dunn was busy. Busy, but not 
forgetful. After the dust settled from the E&S acquisition of AccelGraphics, Dunn 
told Evans & Sutherland’s vice president and general manager, Rick Maule about 
TAZ. Maule already knew about it, but Dunn’s reminder got Maule’s juices ﬂowing 
and, as they say in cheap novels, the rest is history. 
5.1.4 
The Saga of Evans & Sutherland’s Pre-GPU Effort 
(1995–2001) 
The Evans and Sutherland (E&S) company in Salt Lake city, Utah, was to computer 
graphics what the Beatles were to rock and roll: they created a new business, are 
adored by their customers, and respected by their competitors, and the company set 
the standard for what every graphics company would like to be. 
E&S, like a few other ﬁrms developed advanced state-of-the-art graphics tech-
niques and technologies that many other companies beneﬁted from. Small companies 
that did not have the R&D budgets or talent pool keyed off of what E&S did, and 
E&S seldom slapped their hands for patent infringement. The company established 
the subsidy business model for graphics R&D without knowing it, but could not 
sustain it, the same problem SGI, Sun, and HP ran into. Low-end graphics that got 
better and better while enjoying huge volumes undermined the once leading R&D 
teams of the great graphics giants. 
E&S was founded in 1968, just a few years after then grad student Ivan Sutherland 
showed his Sketchpad at MIT. E&S quickly shot to the top in the ﬂedgling computer 
graphics market and had some notoriety for building the ﬁrst commercially available 
8-bit frame buffer named the LDS-1 (LDS stood for Light Drawing system, but

5.1 Workstations
215
because the company was in Utah, it was said to be a take-off on Latter Day Saints, 
the dominate religion in Salt lake City). The famous New York Institute of Technology 
computer lab got not just one of the $60,000 units, but three of them, and made a 
24-bit frame buffer which Ed Catmull (who studied under Sutherland at Utah Univ) 
and Alvy ray Smith used to make the ﬁrst digital movies [4]. 
Back in the early 1990s E&S was king of the hill in the simulation market, with 
SGI a comfortable second, followed by a cadre of smaller ﬁrms, most gone now. 
E&S always had a great vision, and the company saw the impact and potential of 
PC-based products, so set out to exploit its hard-won knowledge in the Sim market 
with lower cost CPUs and similar architectures. 
E&S developed ﬂight simulators (through the purchase of GE’s ﬂight simulator 
division) and would move into high end workstations (Picture System 1, 2 and PS300 
series) with stroke-writer screens (calligraphic vector-scope), and then into large CRT 
screen, pioneering anti-aliasing so a raster scan display would look as sharp and be 
as accurate as a stroke-writer. During the 1990s E&S tried to expand into several 
other commercial markets. The Freedom Series graphics engine was developed to 
work with Sun Microsystems, IBM, Hewlett Packard, and DEC workstations. 3D 
Pro technology was developed for the ﬁrst wave of 3D graphics cards for PCs. It is 
there we pick up the  E&S story.  
But the PC of the time was in no condition to support the kind of graphics E&S 
and its competitors in Colorado and Mountain View were building, so it looked for 
another approach and found it in Japan. 
In late October 1995 E&S formed a partnership with Mitsubishi Electronics to 
develop the yet unnamed 3DPro chip set. It would be a small system of controller 
and very smart RAM, based on an advance internal logic RAM design Mitsubishi 
had developed in 1993 called CDRAM. Sun Microsystems was equally impressed 
and interested in that RAM design and formed an R&D effort with Mitsubishi for the 
development of 3DRAM, but not for commercial sales, for Sun systems only. Sun 
was at the time also an E&S customer buying high-end Freedom systems as attached 
high-performance graphics (so was HP, IBM, and others). 
It did not happen overnight and with a brief abortive showing at Siggraph’95 the 
cat was out of the bag that E&S would soon have a PC graphics AIB. One year later, 
E&S’s RealImage, the desktop end of the company’s products was introduced in 
August at Siggraph ‘96 (Fig. 5.7).
According to James R. Oyler, E&S president and CEO at the time, “We have 
anticipated for some time that RealImage needed to be independent as it became 
more specialized and less related to our core simulation business. E&S will continue 
to use technology from the unit, and will help Real Vision, Inc. during the transition 
period, but will also use other available technologies in its ongoing businesses.” 
Also, in 1996 Mitsubishi (like many other Japanese and Korean companies at 
the time) set up a U.S. subsidiary for advanced technology to integrate hardware 
and software developments [5]. They did that to overcome what some viewed as 
cultural and corporate obstacles and to provide a faster response to developments 
in the multimedia market—a market they clearly wanted to dominate. Mitsubishi’s 
North American afﬁliate, Mitsubishi Electronic America’s (MEA), a wholly owned

216
5
1990 to 1999 Graphics Controllers on Other Platform
Fig. 5.7 E&S Realimage DirectBurst graphics block diagram
subsidiary was called VSIS, an acronym for VLSI Systems Solutions. VSIS became 
the marketing arm of the RealImage AIBs. VSIS is another story for another time, 
but it was not a happy relationship. 
While E&S was building what was known at the time as, the big iron, in Utah, 
a small start-up with experience in mid-range simulation systems was started in 
the Paciﬁc Northwest. Silicon Reality (SRI) was formed in 1994 when members 
of two companies (Xtar and Pico) developed ideas for a 3D graphics engine. Xtar 
specialized in ﬂight simulation for the aviation industry while Pico focused on VLSI 
circuit design. Pico was founded by Steve Tibbits in 1994 as a design engineering 
ﬁrm and did quite well. 
In December 1996 RealImage technology was declared available in sample quan-
tities, with volume production promised for Q1'97. And by May 1997 AccelGraphics 
had signed up and announced the AccelECLIPSE based on the 3DPro chipset to be 
delivered in Q2'97 with the amazing by today’s standards price of $3,495 with a 15 
Mbyte frame buffer and 4 Mbytes texture memory. Shipments began in June 1997. 
While looking for new and adjacent markets, in late 1995, San Jose based 
AIB builder Diamond Multimedia acquired a small Starnberg (near Munich)-based 
workstation AIB builder, Spea, and the FireGL brand. In March 1997, Diamond 
announced it would build a new workstation AIB using E&S’ 3Dpro/2mp chipset. 
E&S/Mitsubishi now had two respected OEM customers; things were looking good. 
In one clean sweep Mitsubishi and E&S got to publicly redeﬁne and reposition 
their companies and products, and Diamond got to thrust itself into the ultra-high-
end—which they knew little about. This class of board was sold to a few companies,

5.1 Workstations
217
which is a customer-intimate business model. Customer intimacy was, culturally 
and philosophically, as far removed from Diamond’s process/distribution model as 
Uranus is from the Sun. Market watchers wished them well in their new adventure 
but did not hold much hope. 
The names of the E&S product kept slipping around, and somewhere along the 
way the AIB became the RealImage 1000, and it could deliver 60 million pixels/s. 
In May 1997 Evans and Sutherland hired Rick Maule as VP and General Manager 
of Desktop Graphics at E&S. Maule had been at Lockheed (Real3D) before. At E&S 
Maule thought the RealImage technology helped to close the gap between Unix and 
PC solutions. 
Moore’s law was starting to click in for graphics and by August 1997 E&S showed 
a prototype of their RealImage 2000 with 90 megapixels/s. 
Maule who understood that to succeed, E&S and its partner should stick to their 
knitting and leverage their strong points—graphics technology from E&S and semi-
conductor manufacturing from Mitsubishi. VSIS decided it would be better off in 
the chipset business and now had to ﬁnd an OEM customer. Chipsets of the time 
connected the main CPU with memory and peripheral devices. 
VSIS seemed like a good idea, but the company did not know how to sell high 
technology products and E&S ended up doing all the work for a percentage of 
the sales. So in February 1998 E&S assumed full responsibility for the sales and 
marketing of the RealImage technology. VSIS was no longer an E&S partner in this 
effort. With advanced products like theirs, customers needed to interface with the 
architects, and they had to become involved with each sale. 
In March 1998 Mitsubishi started volume manufacturing of the RealImage 2000 
(previously marketed as the 3DPro/2mp), the ﬁrst generation The RealImage 1000 
offered OpenGL acceleration at sustained ﬁll rates of up to 60 million pixels/s 
(textured, lit, bilinear ﬁltered, Z-buffered, alpha-blended with transparency, window 
clipped), or half that many pixels/s with trilinear ﬁltering. E&S also claimed up to 
two million vectors or triangle vertices/s with the original technology. 
The new RealImage 2000 technology provided a 50% increase in ﬁll rates and a 2 
× increase in the geometry ingest rate—up to 90 megapixels/s (pixel deﬁned above) 
with bilinear ﬁltering, or half that speed with trilinear, and four million vectors or 
triangle vertices/s. Additional enhancements to the original architecture included an 
integrated VGA and much faster 2D performance (200 Mpixels/s aligned BLT rate 
as opposed to a 60 Mpixels/s aligned BLT rate with the ﬁrst generation technology). 
Figure 5.8 is a block diagram of the chip and its place in the system.
RealImage technology was implemented in a chipset that relied on the unique 
characteristics of Mitsubishi’s 3DRAM and CDRAM memory technology. Frame 
buffer conﬁgurations of 8, 10, 15, 20, and 30 Mbytes of 3DRAM were supported, 
and CDRAM texture buffer sizes could be conﬁgured at 4, 8, 16, or 32 Mbytes. 
Boards based on the RealImage 2000 could support resolutions up to 1900 × 1200, 
double buffered true color pixels with 24-bit z. HDTV resolution of 1920 × 1080 
were also available. 
In March 1998 E&S was hot. If you were the best graphics company in the world 
and partnered with one of the biggest semiconductor manufacturers in the world,

218
5
1990 to 1999 Graphics Controllers on Other Platform
Fig. 5.8 Evans & Sutherland’s RealImage 2000 block diagram
what would you do? You would look for the baddest graphics board company in the 
world, so Maule went looking. 
AccelGraphics was a board builder founded in 1994. It was a management buyout 
from Kuboto Graphics. Kuboto Graphics was started in 1991 as the outgrowth of two 
workstation ﬁrms (Aedent and Stellar, which is discussed in The History of Visual 
Magic: How Beautiful Images are Made in CAD, 3D, VR and AR9). AccelGraphics 
was supposed to design a new graphics chipset, but instead used 3Dlabs’ chips. The 
company went public in 1997. It had difﬁculties but returned to proﬁtability in the 
ﬁrst quarter of 1998. It simultaneously announced the sale of the company to E&S 
that same year. AccelGraphics said E&S would acquire them for approximately 
$52 million in cash and stock, and in June 1998, E&S did acquire AccelGraphics 
for approximately $23.7 million in cash and 1,109,303 shares. Now E&S had an 
AIB company that was buying chips from its competitor 3Dlabs. By July 1998 
AccelGraphics announced the RealImage 2100-based Galaxy product. 
The pixels were barely dry when in July 1998 Intel bought 8.2% of E&S for $24 
million, another step in the processor giant’s push to expand its presence in graphics 
chip technology. It was the largest investment Intel had made. 
In December 1998 the RealImage 3000 was announced with a spec of 100 Mpix/s. 
The 3000 would also have a geometry engine coprocessor. The vision of an integrated 
device was on the drawing board and in the minds of the E&S people. 
And then something funny happened. In March 1999 the RealImage 2000 was 
degraded to 70 Mpix and renamed RealImage 1200. 
Then another surprise announcement, in July 1998 E&S acquired the assets of 
Silicon Reality (discussed in Sect. 5.1.3 above). This was one more piece in Rick 
Maule’s plan to become vertically integrated and offer a total solution to the OEMs.

5.1 Workstations
219
Now he controlled three levels of silicon from the high-end workstation class to the 
entry-level PC. He got the board business with AccelGraphics (albeit high-end) and 
hoped to open the channel to mainline PC markets. This was easier done on paper 
than in reality, but Maule and Jeff Dunn (AccelGraphics’ President and Founder) 
had been around and were conﬁdent they could solve the problems. 
Incestuous 
The situation in the workstation market was crazy (Fig. 5.9). 3Dlabs was selling to 
Diamond and most other workstation builders. E&S was doing the same, and Real3D 
was trying to. Intel had invested in all of them. 
Meanwhile, Intel was preparing for the roll out of the Pentium III (code named 
Katmai). Maule pointed out, “Customers [of Intel] are going to have to buy Katmai-
based processors whether they like it or not and pay what Intel asks for them because 
there is no real competition out there now.” 
Getting current high-end graphics engines to take advantage of the SIMD-like 
Katmai multi-ﬂoating-point capability required a new data structuring and thinking, 
Maule added. “This includes the application and failing of the graphics controller’s 
driver. SIMD requires an order of arrays data structure. Currently there are no 
compilers that will optimize for SIMD, even Intel’s Proton, and Microsoft’s C&C++ 
does not support it all yet.” 
E&S began studying this issue about 18 months earlier and had been running 
simulations with Intel as well as independently. This work revealed the need for a 
change in data structure to get the maximum beneﬁt from the Katmai instructions.
Fig. 5.9 Interleaved relationships in the late 1990s workstation market 

220
5
1990 to 1999 Graphics Controllers on Other Platform
E&S thought they had it ﬁgured out. This was Maule’s big message to the OEMs— 
you are going to have to buy Katmai and if you want some differentiation, you had 
better use a graphics controller that has the SIMD issue ﬁgured out. 
The second point was the lack of application support for multiprocessing. At that 
time, the application developers were becoming more aware of the issue and were 
adding multithreading to their programs, but it would become a process that took 
much more time than the parties realized. Intel was, of course, working with the 
ISVs to get that corrected (because they obviously wanted to sell more processors 
per system). E&S had been doing multithreading for years in the UNIX world and 
put it in their drivers in Q1 1999. E&S had been stung a little by 3Dlabs/DPS’s 
PowerThreads and was unleashing some of E&S’ big guns. 
There were also emotional aspects about geometry engines (GEs). E&S was 
getting ready to announce their geometry engine solution in Q1 1999. And then the 
story becomes as Byzantine as anything in the computer industry. Compaq owned 
Digital Equipment Corp (DEC) and had rights to the Alpha processor. The Alpha 
processor was a ﬂoating-point processor. Compaq was talking about a system, some-
time in the future, which would have an Alpha as a geometry engine for some unde-
ﬁned graphics. E&S was a supplier to Compaq. But Intel owned some part of Alpha 
too (and the DEC fab). And as was well known at the time, Intel did not think adding 
non-Intel processors (RISC, CISC, and DSP) was a very good idea. But Compaq 
was Intel’s biggest customer (or at least its second biggest). Add to this mélange the 
competitive advantage HP and Intergraph/IBM had with their proprietary GE-based 
graphics and one starts to get a feel for the emotional elements of the situation. 
Commenting on it, Maule said, “OEMs can make much more money selling a 
system with two or more CPUs in them than any number of GEs.” He pointed out 
that the price comparison of geometry engines was well known, and the OEMs could 
not mark them up enough to make them worthwhile. Furthermore, the little added 
beneﬁt a geometry engine brought was not cost-effective. “BUT” he added, “there 
is an emotional issue and so OEMs will react to the need for a geometry engine 
if for no other reason than because HP and Intergraph have made it a “must-have” 
selling item for the power hungry.” So, E&S being a competitive, customer-oriented 
supplier said they would meet the market demand and introduce a geometry engine. 
E&S was on a roll and by May 1999 the RealImage 3000 was introduced at the 
promised 100 Mpix/s. Then in August 1999 E&S announced the RealImage 4000 
with 125 Mpix/s ﬁll rate. Moore’s law was certainly at work for E&S. 
And then things got quiet, real quiet. E&S was depending on a geometry engine 
for the RealImage 3000 and 4000 that simply did not get delivered. The employment 
contracts started timing out and the AccelGraphics folks packed up their vans and 
headed back to California. 
In mid-1999 Mitsubishi decided to pull the plug on VSIS, and quietly shut the 
operation down in February 2000 [6]. 
By December 1999 E&S’s workstation division was in the process of realigning 
itself in the wake of disappointing performance and considerable turnover. Rick 
Maule left during the summer, and George Saul (from the SRI acquisition) was 
handling the reorganization. He impressed E&S’s President and CEO Jim Oyler,

5.1 Workstations
221
who praised Saul’s management skills and expressed high hopes for the Workstation 
group because of the new management. He said, “George offers the exact skill set 
and experience we need for continued progress in our video and graphics subsystem 
business.” And George became the new general manager. 
5.1.4.1
Summary 
Eight months later at Siggraph 2000, the word was out that E&S was looking for 
a buyer for the RealImage division, and in November 2000 the company publicly 
announced it was in the process of spinning-off the group. It was also working on a 
new super video chip, the REALimage 5000 [7]. 
After struggling more than seven years trying to make a go of it in the PC AIB 
business, E&S ﬁnally threw in the towel and sold off their RealImage group. It was 
not that E&S did not try, did not develop some great graphics controllers, or did not 
invest almost a $100 million, and did not have a hang-in-there will to win, they just 
could not carry the load any more with such poor returns. 
Unable or unwilling to market the RealImage 4000 as a merchant workstation 
add-in-board, E&S used it for their own PC-based Sim systems and in December 
2000 announced the E&S simFusion systems would be based on the RealImage 4000 
technology. 
On September 5, 2001, Evans & Sutherland Computer Corporation announced 
it had reached an agreement to sell its RealImage business unit. The sale is for a 
maximum value of $12 million, consisting of cash plus future royalties up to $6 
million for RealImage technology, other assets, and support during a seven-month 
transition period leading to closing the transaction on April 1, 2002 [8]. 
The buyer of the RealImage unit was Real Vision, Inc., a Japanese company, 
which had been a partner with E&S in the development of technology for professional 
video applications. Real Vision indicated it would continue the development of the 
technology. E&S said they would maintain a technical staff in Salt Lake City to 
support Real Vision during the transition period. 
E&S reported RealImage transition costs of $2.9 billion. Some of the team in Salt 
Lake and most of the team in Seattle would stay with the RealImage group under 
the direction of the Japanese management. George Saul moved on, and the world 
awaited the announcement of the ﬁrst product from the new RealImage company. 
As a result of the sale, E&S reduced its RealImage stafﬁng at its Salt Lake City 
headquarters and closed its ofﬁces in San Jose and Seattle. In addition, E&S also 
announced reductions in its other business units, including the simulation business 
and a group that produced digital theater and planetarium systems, and other soft-
ware. The total reduction in stafﬁng at all locations was approximately 10% of the 
company’s 840 employees, with over half coming from the Salt Lake City site, its 
largest facility. 
E&S selected a silicon solution from ATI and said it would be announcing a new 
range of AIBs for their PC-IG Sim system.

222
5
1990 to 1999 Graphics Controllers on Other Platform
It saddened almost everyone in the industry, even E&S’ competitors to see the 
pioneer and technological leader shrink away. E&S went back to designing and 
building simulators and then developed the worlds most sophisticated planetarium 
system, thousands of them are in use today. 
In 2020, Elevate Entertainment Inc., an afﬁliate of Mirasol Capital, LLC, acquired 
E&S and took the company private [9]. 
Almost ﬁrst 
E&S could have built the ﬁrst single chip GPU if they had recognized sooner that 
they had the wrong team working on the job. The sad part is E&S had all the talent 
needed and more to design and build a GPU, but they could not see beyond the 
workstation market where they originated—they never saw Nvidia coming. 
5.1.5 
3Dlabs Permedia (1997) 
The ﬁrst commercial workstation-class chip for the PC 
The origin of 3Dlabs was in 1983 when Osman Kent and Yavuz Ahiska started 
benchMark Technologies in London. They sold benchMark to DuPont in 1988 for 
$12 million, and it became DuPont Pixel. 
Later, the managers of DuPont Pixel made, a buyout offer, and DuPont corporate 
accepted and in 1994, they established 3Dlabs. The company immediately released 
details on the Glint family of 3D graphics processors—the 300SX and 300TX—in 
April 1994. 
Glint was the ﬁrst fully integrated, OpenGL-compatible workstation graphics 
chip [10]. Other companies like ATI, Dynamic Pictures, and Nvidia would also 
develop OpenGL workstation-class chips. The larger, more established companies, 
such as Evans & Sutherland, HP, IBM, Intergraph, SGI, and SUN—often called 
big iron companies—built large, complex graphics boards with multiple proprietary 
processors. All these big iron companies would succumb to the economic advantages 
of the single chip designs and the use of the mass-market, high-volume production 
of the core design behind them. The big iron suppliers would either adopt the single 
chip parts or retreat from the market, as DEC, E&S, IBM, Intergraph, Lockheed, 
SGI, and SUN did. By 2000, these big companies had begun exiting the graphics 
market and began the long, painful process of shrinking; some, like DEC, did not 
make it. 
The ﬁrst Glint chips offered the equivalent of high-end SGI Indigo graphics in a 
single chip—for less than the VRAM frame buffer memory cost [11]. The Glint chips 
provided 24-bit 2D and 3D acceleration, an on-chip PCI interface, and LUT-DAC 
control, making a complete graphics subsystem possible with a minimal chip count. 
The smaller, more agile companies kept pushing the technology. In 1997, 3Dlabs 
developed its Glint Gamma processor, the ﬁrst programmable transform and lighting

5.1 Workstations
223
engine for its Glint workstation graphics chips; it even coined the term GPU—geom-
etry processor unit—before Nvidia announced its graphics processor unit (GPU). 
3Dlabs’ GPU was a separate chip named Delta. Before introducing the Delta chip, 
the ﬂoating-point processor in the CPU did the T&L. And, in some cases, with DSPs. 
Whereas 3Dlabs’ GMX was a coprocessor to the Glint SX and MX rasterizer, 
Nvidia developed an integrated T&L engine for its consumer graphics chips, branded 
as GeForce. ATI quickly followed with its Radeon graphics chip. By the end of 1999, 
the number of graphics chip suppliers had dropped to 12 (from a high of 45), and the 
consolidation was not over yet; even though the market was rapidly expanding, the 
commodity suppliers were dying—they had to innovate or die. 
The Glint 300 SX was manufactured in 1994 using IBM’s 3.3 v, 500 nm process, 
and the chip operated at 2.5 billion operations/s and sold for $150 in volume. 
Right on the heels of the Glint SX, 3Dlabs introduced the Glint TX. Both the 
300SX and 300TX were highly advanced in design. They used one million transistors 
and ran at the equivalent of 2.5 billion operations per second (GOPS). In comparison, 
a typical VGA chip used 50,000 transistors at that time. 
Glint processed primitives (e.g., triangles) generated by an application and passed 
to Glint via the OpenGL API. Glint was a rendering pipeline processor. 
Triangles were passed through the pipeline, and messages were generated 
concerning what should happen in each stage. The primitives were Gouraud-shaded 
Z-buffered triangle(s), with dithering. 
The following block diagrams (Figs. 5.10, 5.11, and 5.12) are based on information 
supplied by Neil Trevett, former vice president of 3Dlabs. 
Fig. 5.10 3Dlabs’ Glint block diagram

224
5
1990 to 1999 Graphics Controllers on Other Platform
Fig. 5.11 3Dlabs’ Glint 300SX core architecture had many stages 
Fig. 5.12 The 3Dlabs’ Glint MX processor relied on an external geometry processor 
The application generated the triangle vertex information and made the necessary 
OpenGL calls to draw it. 
The OpenGL server/library would take the vertex information (from the model) 
and transform, clip, and light it. It would then calculate the initial values and deriva-
tives to interpolate the position and color depth. All these values were in ﬁxed-point

5.1 Workstations
225
integers and had unique message tags. Some of the values (the depth derivatives) had 
more than 32 bits to cope with (for dynamic range and resolution) and, because of 
the limitations in memory size or bandwidth, had to be sent in two halves. 
A digital differential analyzer (DDA) in computer graphics is a hardware or soft-
ware system used to interpolate variables over an interval between the start and end 
point. DDAs get used for the rasterization of lines, triangles, and polygons. 
In Glint, the derivative start and end parameter messages were received and ﬂowed 
down the message stream to the appropriate function blocks. The depth unit received 
the depth parameters, and derivatives/The color DDA received the RGB parameters 
and derivatives. The rasterizer got the unit edge values and derivatives. 
The rasterizer unit received the render triangle message. And until the triangle had 
been rasterized (but not necessarily written to the frame store), it blocked subsequent 
messages (from the host). It was a tricky and complicated mechanism and surprisingly 
efﬁcient. 
3Dlabs continued to expand the product line and in 1997 introduced a smaller, 
faster model, the Glint MX. The MX was a high-performance graphics processor 
that combined workstation-class 3D graphics acceleration and state-of-the-art 2D 
performance in a single chip. The MX accelerated all the 3D rendering opera-
tions, including Gouraud shading, texture mapping, depth buffering, anti-aliasing, 
and alpha blending. Implemented around a scalable memory architecture, the MX 
reduced the cost and complexity of 3D graphics within a windowing environment, 
and the MX was pin-compatible with the 300SX and 500TX processors. 
The Glint rasterizers and OpenGL accelerators delivered what the company 
promised but were held back by the processing speed and I/O of the CPU’s ﬂoating-
point capability. 3Dlabs knew that this would be a problem and had hoped that 
Moore’s law would solve the issue, but it did not; as a result, the company devel-
oped its FPU, but as a dedicated geometry processor rather than an inherently slower 
general purpose FPU. They called it the Delta processor, and it was to be a coprocessor 
to the Glint. 3Dlabs’ road map is shown in Fig. 5.13.
While initially targeting traditional 3D applications, including CAD, 3Dlabs told 
JPA that it envisioned the long-term market to include games, multimedia, VR, and 
interactive television. Although the company did enter a relationship with Creative 
Labs (that was only revealed later, and after the fact), 3Dlabs’ entry into the consumer 
marketplace was still some time off with its Permedia chip. 
The market was still wide open. Windows NT 4.0 was a year away, and the 
applications base that 3Dlabs needed was two years away. Between 1994 and 1996, 
anything to do with professional 3D graphics was considered fair game. Intel bought 
into Lockheed Martin’s 3D intellectual property, knowing that even back in 1995, 
that Intel was weighing its options in a Windows-dominated, post-Cold War world. 
What 3Dlabs did, which no other manufacturer had managed to imitate or come 
close to doing, was to focus entirely on its PC OEM customers. People in the industry 
saw the Intel and Microsoft road maps, and companies like Compaq ﬁgured out 
UNIX was their next target. This was when the PC industry stood on the brink of 
world domination and was looking for foes to vanquish. This was also when Apple 
was losing ground in terms of performance with the PowerPC processor, and then

226
5
1990 to 1999 Graphics Controllers on Other Platform
Fig. 5.13 3Dlabs’ road map in the mid-90s (Courtesy of 3Dlabs)
a recession hit. The industry was looking for ways to save money and still make 
progress. 
The other thing that 3Dlabs did well was to sell professional graphics to an audi-
ence that knew nothing of it. Much of the interest in PCs in the middle of the nineties 
was related to multimedia. 3Dlabs evangelized the sexiest component, which was 
out of the reach of most users and developers: real-time 3D. It helped to have SGI 
showing off simultaneously, splashing Hollywood special effects at the traditionally 
staid computer shows. It also helped to have a host of APIs promising to enable the 
next generation of mass-market 3D applications. 
The initial problem that 3Dlabs had in evangelizing the market was one that 
companies still face today: the PC versus workstation debate. Ultimately, 3Dlabs and 
other like-minded souls saw that the workstation market was where the applications 
were for professional graphics. Until there was a compelling reason to do so, they 
would not move to the PC. It is worth bearing in mind that during the pre-Windows 
95 and NT 4.0 eras, there was still some question whether the PC would not just end 
up being a 3D game machine. There was some level of confusion over how much 
software rendering would get done through APIs such as RenderWare and Reality 
Lab (discussed in Book two, What is a GPU?). The professional graphics market 
caught the PC graphics market unprepared. 
3Dlabs was not. Neither were the companies that invested in their technology, such 
as Elsa, SPEA, Omnicomp, and AccelGraphics. Those companies, big or small, still 
carried with them the experience and expertise of AutoCAD graphics sales and were 
bolstered by workstation-savvy developers. SIGGRAPH 94 was the beginning of the

5.1 Workstations
227
momentum behind OpenGL. Microsoft showed its support for the API by including 
it in the beta CD for Daytona, which would eventually become the NT platform of 
version 3.0. 
5.1.5.1
Permedia 
The advantage that competitors like ATI and Nvidia had was adapting the chips they 
were developing for the mass consumer market to the smaller-volume professional 
graphics market. To counter that, 3Dlabs announced a lower-cost version of its high-
end chip, the Permedia—3Dlabs would from pro to consumer. 
During this time, one of the leading companies in the hardware gaming market 
was Creative Labs. They entered the market with award winning and fantastically 
popular Sound Blaster AIB, and then expanded to graphics AIBs. 
In 1995, the company introduced its Permedia graphics processor (see block 
diagram, Fig. 5.14). With Permedia, 3Dlabs answered all its competition’s complaints 
about the Glint (too expensive, too high-end, no VGA, no PCI, used only VRAM, 
requires z-buffer, etc.). Its technology partnership with IBM had shown up in several 
ways: 3.3 V 350 nm fabrication, a special version of its 562-process, and PCI capa-
bility. With its Creative Labs’ 3D Blaster game library compatibility and 3Dlabs’ 
long list of OEMs, that part ushered in a new wave of game boards for the PC. The 
chip sold for $50 [12].
3Dlabs continued to pick up the established workstation companies and several 
others. In 1996, TI would renew its interest in graphics by licensing 3Dlabs’ Permedia 
and become a second source for the chip. 
Creative Labs was one of the ﬁrst to adopt the new chip. However, as good as the 
chip was, it could never reach the price points and design cycles that ATI and Nvidia 
were hitting, and 3Dlabs found itself losing market share, sales, and money. 
The company had gone public in April 1994 and armed with a tangible ﬁnan-
cial vehicle—its stock—the company went looking for acquisitions to broaden its 
capabilities. 
In July 1998, 3Dlabs acquired Dynamic Pictures, a competitor. Dynamic Pictures 
was a spin-out of Digital Equipment Corporation (DEC) in 1993. DEC developed 
3D chipsets at its Palo Alto design center in the early nineties, and the last one, 
called PixelVision was built for the DEC ZLX-M/L graphics line of workstations. 
It was a second-generation architecture; the ﬁrst generation was called PixelStamp. 
When DEC shut down the Palo Alto MIPS-based workstation business and team, 
they retained a small group of engineers working on graphics so they would continue 
to build the hardware for the Alpha-based workstations being designed on the East 
Coast. DEC began retrenching and selling off assets and in 1993 the Palo Alto 
engineers left and incorporated as Dynamic Pictures. DEC gave the group the rights 
to the PixelVision design. The architecture was inspired by the PixelPlanes paper.

228
5
1990 to 1999 Graphics Controllers on Other Platform
Fig. 5.14 3Dlabs’ Permedia consumer-level 3D controller
5.1.5.2
3Dlabs—The First Dedicated CG T&L Processor (1997) 
In November 1997, at the Comdex conference, 3Dlabs announced two new high-
performance chipset families within its Glint family: the Glint DMX and the Glint 
GMX (code named Gamma or G1). These chipsets provided geometry and rasteriza-
tion performance and accelerated the same 3Dlabs 2D/3D driver set. The company 
claimed it would accelerate graphics tasks such as transforms, lighting, clipping, and

5.1 Workstations
229
texture mapping to 66 million pixels per second, accelerated by its three-GFLOPS 
ﬂoating point capability. 
The Glint DMX and GMX families supported high-resolution displays up to 2K 
× 2K at 24-bit (true) color, large texture memories up to 96 Mbytes, and deep 32-bit 
color and z-buffers. Both chipsets also accelerated OpenGL 1.1 rendering opera-
tions, including Gouraud shading, advanced texture mapping, per-pixel mipmapping, 
trilinear texture ﬁltering, anti-aliasing, multi-plane stenciling, and alpha blending 
with destination alpha support. By February 1998, the company was providing sample 
quantities of the GMX to selected OEM customers. The three GFLOP G1 could 
process 4.75 million triangles per second. 
3Dlabs developed the ﬁrst multichip implementation of a geometry processing 
unit—a GPU was. However, it called the chipset a video processing unit or VPU. 
Later, when Nvidia introduced the ﬁrst fully integrated graphics processor unit 
(GPU), 3Dlabs reconsidered its naming. 
In 1999, workstations were using a new high-speed serial memory called RDRAM 
[13], developed by Rambus. In 1999, 3Dlabs introduced the Glint Rx, which used 
Rambus for its memory—the ﬁrst graphics chip to do so. 
Then, in April 2000, 3Dlabs announced that it would acquire the Intense3D divi-
sion of Intergraph. Intergraph’s Intense3D division’s (Huntsville, AL) 3D-T Pro 
1000 was an AIB competitive with 3Dlabs AIBs. 3Dlabs agreed it would continue to 
supply Intergraph with graphics products for its workstations and media processing 
products as part of the deal. This was an important point as Intergraph software (for 
CAD, GIS, image processing and other applications) was a big and serious business 
for the company. If they looked like they were deserting their customers, it would 
have been very damaging for them. 
Paradise Lost 
3Dlabs was founded in April 1994 and announced the Glint 3D rasterizing engine 
for high-end 3D CAD applications in November of that year [14]. 
The company developed a series of high-performance graphics processors and 
entered the consumer segment with its popular Permedia processor in 1995. 
In July 1995, the consumer version, Gigi, was developed with Creative Labs and 
gave 3Dlabs the distinction of being one of the ﬁrst companies to ship a 3D games 
chip. 
3Dlabs then acquired two competitors, strengthening its position in the worksta-
tion segment. However, larger companies like ATI and Nvidia overwhelmed 3Dlabs 
with faster product introductions and lower-cost parts due to the economy of scale 
of supplying the emerging PC gaming and commercial segments (Fig. 5.15).
When the dot-com bubble [15] popped in the late 1990s, 3Dlabs was losing money 
and market share, and could no longer compete. 
In June 2002, Creative Labs, 3Dlabs’ long-time partner, bought the company [16]. 
Then, in early 2005, Creative Labs shut down the workstation business of 3Dlabs 
and announced that it would stop developing professional 3D graphic chips and 
focus on embedded and mobile media processors. The company making the new

230
5
1990 to 1999 Graphics Controllers on Other Platform
Fig. 5.15 The history of 
3Dlabs
media processor products, designed in the original 3Dlabs U.K. R&D Center by the 
workstation graphics chip designers, was named ZiiLabs. 
The Elsa add-in board pictured in Fig. 5.16 was equipped with a 3Dlabs setup and 
rendering engines and 40 MB of onboard RAM. For an expensive board, the Elsa 
GLoria-XL ($1,650) was a lackluster performer on most of the benchmarks in 1998. 
The GLoria-XL used 3Dlabs’ Delta geometry processor and Glint MX rendering 
chip, with a 16-MB VRAM frame buffer and up to 40 MB of EDO DRAM texture 
memory, which produced 24-bit double-buffered resolutions up to 1920 × 1080.
3Dlabs was a pioneer in many respects, the ﬁrst to implement OpenGL in 
silicon, the ﬁrst to build a commercial geometry and lighting engine, and the ﬁrst 
to use Rambus to drive massive texture maps. The company’s IP portfolio still runs 
through the industry and is a source of comfort for some companies because of the 
foundational protections it offers. 
5.1.6 
Intergraph Wildcat (1998–2000) 
Through its deal with Intergraph, 3Dlabs was partnering with one of the oldest 
high-performance graphics companies in the industry. In August 1998, Intergraph 
(Huntsville, AL), announced its latest hardware graphics design—the ParaScale

5.1 Workstations
231
Fig. 5.16 Elsa’s GLoria-XL 40-MB 3Dlabs-based workstation AIB (Courtesy of VGA Museum)
architecture. It was a composition of parts and functions that could be used in parallel, 
scalable and shrinkable as silicon technology progresses. 
From the ParaScale architecture Intergraph intended to develop several classes or 
families of products. The ﬁrst introduced was the Intense 3D Wildcat family. The 
ﬁrst product within the Wildcat family was the Intense 3D 4100 series. 
The Wildcat chipset consisted of three parts: a bus interface controller, a geometry 
accelerator, and a rasterization engine as shown in Fig. 5.17. 
The Wildcat pipeline could be used in various conﬁguration using two to four 
of them to drive a single display or up to four of them driving four displays. When 
driving a single monitor the screen was interleaved by scan line per controller. There 
was a fragment bus for the setup engine to help keep things synchronized. 
The bus interface controller supported AGP 2X and PCI (64-bit) buses. The 
controller (in cooperation with the driver) used local memory to store the display list. 
The driver intercepted calls from the application and put the data in local memory
Fig. 5.17 Intergraph’s Intense Wildcat (single pipeline) block diagram 

232
5
1990 to 1999 Graphics Controllers on Other Platform
(reducing calls to/from main memory). This reduced bus transactions, which could 
have a major impact on performance. 
The geometry accelerator was used to off-load 3D lighting, transformation, and 
clipping calculations from the host. This too improved performance and allowed the 
CPU to spend more time on the application. 
The part was a major piece of Intergraph’s secret sauce. In earlier designs Inter-
graph was using Analog Devices’ DSPs (SHARK) for geometry. The new geometry 
accelerator was completely home grown and had two 1.5 GFLOPS engines in it 
that worked in tandem. They were programmable via a VLIW, which meant the 
chipset could be adapted to future APIs such as DirectX6 and Fahrenheit. Up to four 
geometry accelerators could be put in one system and they scaled linearly, giving up 
to 12 GFLOPS peak performance. This was good news for users because it meant 
they should be able to scale up without having to do the continual application modi-
ﬁcation required in the past. Intergraph promised that virtually any 3D graphics 
software application could immediately be used with the new technology without 
any modiﬁcation. 
The Wildcat chipset had a state-of-the-art rasterizer. It performed triangle setup, 
texture processing and basically all the functions in rasterization. The company 
claimed a single rasterizer could produce 6 M triangles/s (25-pixel with Z-buffer on), 
and 110 Mpixels/s full texture performance with trilinear ﬁltering. There were two 
buses, 128-bit wide frame buffer, and 64-bit wide texture memory. The controller did 
texture object caching, and those cached textures could be used as replicated texture. 
The controller supported bi- and trilinear ﬁltering and had 16- and 32-bit texture 
support. There was also hardware accelerated 3D volumetric texture and (OpenGL) 
4D-texture extension support. 
The Z-buffer supported ﬂoating and ﬁxed point and could be conﬁgured up to 
32-bits per-pixel on a window-by-window basis. 
The controller offered full OpenGL alpha blending (including double-buffer alpha 
for video apps). It had hardware accelerated per-pixel fog with linear, exponential, 
and user-deﬁned fog models. Complex fog and atmospheric effects were supported. 
The rasterizer had full-screen anti-aliasing using supersampling (16 × 16). Intergraph 
had a proprietary anti-aliasing technique they called SuperScene which only stored 
super samples when needed. This technique used a lot less memory. 
Frame-lock across multiple displays was provided (within and outside a system 
so they all traverse from top to bottom at the same time).It has gen- and rate-lock. 
Frame sequential and interlace stereo support was provided for HMDs and shutter 
glasses. 
The chipset would support up to 64 Mbytes of frame buffer per controller plus 256 
Mbytes of texture memory (with a 4 × 4 conﬁguration) that would provide a system 
with 256 Mbytes of frame buffer and 1 Gbyte of texture memory. The OpenGL 1.2 
imaging pipeline was also supported in hardware. 
All available APIs were supported (2D GDI, DirectDraw, D3D, OpenGL, and 
RenderGL).

5.1 Workstations
233
Intergraph was stacking the chipset up against E&S’s Realimage 200, 3Dlabs’ 
GMX 200, HP’s Visualize fx6, and SGI’s InﬁniteReality. The company thought they 
could outperform and outprice all those contenders. 
Steve Pesto, Intergraph’s Product Division Executive Director was excited about 
it, and said, “The power of the new architecture will deliver radically improved 
real-time performance today for even the largest 3D models. This performance will 
spur sales for VARs [Value-added reseller] in the graphics and technical computing 
markets. Graphics professionals are clamoring for this technology today, but they 
didn’t expect it to be delivered until into the year 2000.” Grinning, he added, “It 
won’t be a difﬁcult job selling it [17].” 
The chips were packaged in BGAs (ball grid arrays) using 250 nm technology from 
IBM’s fab. The bus interface controller was a 474-pin part; the geometry accelerator 
is a 360-pin part; and the rasterization engine was a 624-pin part—not small parts 
by any stretch of the imagination. 
Intergraph had been building graphics systems for almost 30 years in 1998, starting 
out as M&S Systems (founded in 1969) [18]. The company knew about as much about 
real graphics as any company could. And because they were in Huntsville they tended 
to keep their people, so the knowledge stayed with the company. This knowledge 
showed up in the new chipset. It also showed that even though the company had some 
stunning losses for an extended period, it had not given up. It had been investing in 
R&D the whole time. Intergraph meant to do some serious damage to the bottom line 
of the other companies in this high-end segment. With design wins from Dell and 
IBM to its credit, the company was demonstrating it could play in multiple arenas. 
The conﬁgurable Wildcat chipset should have done well in the workstation OEM 
market. 
The market for the new Intergraph technology would have been for the high- to 
extreme high-end NT graphics workstation user. This was the market segment the big 
boys like E&S, HP, Intergraph, SGI were bringing down from UNIX land. There had 
been some attempts in the past to offer scalable architecture (MXX/Glint, Dynamic 
Pictures) on the PC; UNIX systems had that capability for over a decade. What that 
meant to end users was they had real alternatives to UNIX-based hardware, but there 
was still a lag in the software. 
There was also the overall size of the market to be taken into consideration. The 
high-end segment was robust, but tiny compared to NT and the PC. Furthermore, 
the segment for super high-powered controllers like Wildcat were at the top of the 
triangle in the segment (which had been dominated by 3Dlabs). Intergraph had a 
realistic sales goal and expectations for the new chipset. 
3Dlabs take over 
And then in April 2000, 3Dlabs announced it had signed a deﬁnitive agreement to 
acquire the assets of Intense3D from Intergraph for $25 million in a transaction that 
was primarily stock and some cash [19]. 
Neal Trevett of 3Dlabs said, “it looks like a very good deal. There’s very little 
overlap so it should be accretive almost right away.”

234
5
1990 to 1999 Graphics Controllers on Other Platform
In addition, 3Dlabs would continue to supply Intergraph with graphics products 
for its workstations and media processing products. The deal swelled the ranks of 
3Dlabs with 95 people (75 in engineering) giving 3Dlabs a headcount of 250 people 
worldwide. Intense3D would maintain its operations in Huntsville, 3Dlabs said they 
would be introducing Wildcat products through its worldwide distribution channel 
in the third quarter. 3Dlabs said it also planned to expand its product development 
into the growing video and digital media content creation markets. 
5.2 
Game Consoles 
In 1972, two former Ampex engineers named Steve Mayer and Larry Emmons started 
Cyan Engineering to research next-generation video game systems. Atari helped fund 
Cyan in 1973 and received a prototype known as Stella. Stella deviated from the past 
where game machines used custom logic, which only ran a small number of games, 
and instead used a general purpose CPU, the famous MOS Technology 6502. It had 
a MOS Technology 6532 RAM I/O chip, and Stella’s display and the sound chip was 
known as the TIA, for Television Interface Adaptor. It was the ﬁrst computer-based 
game machine, and it could play all four of Atari’s then-current games. Atari made 
it into the Video Computer System or VCS (later renamed the Atari 2600 when the 
Atari 5200 was released) [20]. 
Atari kept Cyan Engineering as a separate entity, but it was an exclusive engi-
neering arm for Atari, which would later be called Atari Grass Valley. Cyan stayed 
separate for business and ﬁnance reasons; Cyan could research competitors without 
them knowing that it was for Atari. Also, Cyan had its own credit line and was able 
to prop up Atari and purchase equipment and parts when Atari used up its credit. It 
was a way for Atari to have a lifeboat (ﬁnancially) during tough times. 
Atari became the leading company for arcade games and video game machines 
for the home (however, the Magnavox Odyssey, introduced in 1972, is generally 
considered the ﬁrst commercially available home video game console). Nolan Bush-
nell, founder of Atari and developer of Pong, needed to bring the console to market, 
so he sold Atari to Warner Communications in 1976 to get capital. Atari became 
a well-known name, and in 1984, the market crashed, some say because of Atari. 
Many great people came through Atari in their careers, including Steve Jobs and Dave 
Theurer. Jack Tramiel (founder of Commodore Computer) bought Atari in 1984. In 
1993, Atari produced its last gaming system, the 64-bit Jaguar, but it failed to sell in 
a sad case of too little too late when the console market was at a critical juncture. 
In 2010, Atari shifted its business model from retail game titles to digital games 
for iOS and Android. The ﬁnal chapter of the Atari story may have been written in 
January 2013, when Atari ﬁled for Chapter 11 bankruptcy. It wanted to separate from 
the structural ﬁnancial encumbrances of its French parent holding company, Atari 
S.A. (formerly Infogrames S.A.), and secure independent capital for future growth. 
For the complete story on Atari, see Business Is Fun by Marty Goldberg [21].

5.2 Game Consoles
235
Table 5.6 Game consoles before GPUs 
Generation
Company
Model
Graphics controller
Date 
Fourth
Sega
Sega Genesis
PSG Texas Instruments
1988 
Fifth
Sony
PlayStation
Sony Toshiba
1993 
Fifth
Atari
Jaguar
Tom Flare Technology
1993 
Fifth
Panasonic
FZ (EDO)
Madam and Clio
1994 
Fifth
Sega
Saturn
VD1 and VD2
1995 
Fifth
3DO
Interactive Multiplayer
2 custom video coprocessors
1995 
Fifth
Nintendo
64
Reality Coprocessor SGI
1996 
Sixth
Sega
Dreamcast
Imagination Technologies
1997 
The ﬁfth generation of game consoles (shown in Table 5.6) brought 3D accelerated 
graphics hardware to the home market. The main combatants in the console war of 
the mid-1990s were the Sega Saturn, Nintendo 64, and Sony PlayStation. 
Home TV game consoles were relatively simple devices; they had to be to keep 
costs down. The expansion into realistic-looking games with three-dimensional 
objects and interactivity did not begin until the introduction of the Sony PlayStation. 
5.2.1 
Sega 
5.2.2 
Sega Genesis (1988) 
The Sega Genesis did not offer any unique graphics capabilities and was basically 
just a CPU-based drawing engine with a TV output (Fig. 5.18). A Texas Instruments 
SN76489 PSG chip was incorporated into the system’s ‘video display processor 
(VDP) and fed an RF transmitter connected to the TV’s antenna input.
The most noteworthy thing about the Genesis was its CPU—a 16/32-bit Motorola 
68,000 CPU running at 7.6 MHz. Sega pushed the console market into the 32-bit era. 
5.2.3 
Sony PlayStation (1994) 
The PlayStation was a true inﬂection point in the gaming industry and would 
change it forever 
Ken Kutaragi proposed the concept of the PlayStation after Nintendo decided to use 
Philips’s optical drive instead of Sony’s. Kutaragi, an engineering manager at the 
time, was bold and forceful in his proposal in a nontraditional manner, especially for 
corporate Japan. The PlayStation was a great success, putting Sony in the console

236
5
1990 to 1999 Graphics Controllers on Other Platform
Fig. 5.18 The graphics from the Sega Genesis were not impressive, but in 1988 and for a standard 
deﬁnition TV, they rivaled arcade game machines (Courtesy of Wikipedia)
market, and Kutaragi was called “The Father of the PlayStation.” Kutaragi had a 
string of successes with the PlayStation and rose to chairman and CEO of American 
operations in 1997, executive president in 1999, and CEO and president of Sony 
Corporation in 2003 [22] (Fig. 5.19). 
The PlayStation’s roots go back to two events. The ﬁrst was Kutaragi’s fascination 
with the 3D TV capabilities of Sony’s System G(azo), which he saw in 1984 at Sony’s 
Information Processing Research Center in Atsugi, Japan. They had a demo using 
3D texture mapping of a computer generated face changing shape. This left a lasting 
impression on the young engineer. (Gazo is Japanese for picture or image.) 
“What a powerful game machine we could make with System G,” said Kutaragi, 
who had focused his college thesis on computer graphics. Kutaragi had been 
interested in the game industry for some time [23].
Fig. 5.19 Ken Kutaragi 
(Courtesy of Wikipedia) 

5.2 Game Consoles
237
The second event was a press conference that Nintendo and Philips held at the 1991 
Consumer Electronics Show (CES) after Sony announced a Nintendo–Sony console 
project called Famicon. Sony had developed an advanced optical disc reader, which 
became known as a CD-ROM, and Sony had convinced Nintendo to use it. They 
signed an agreement in 1988. However, Nintendo’s management decided in 1991 
that they did not like the contract terms and thought it would give Sony too much 
control over Nintendo’s content, so they went to Philips and struck a new deal for a 
CD player [24]. 
Kutaragi argued Sony should build the system anyway. 
“There was a fair amount of resistance within Sony for devoting precious resources 
toward the creation of ‘children’s playthings.’ However, we were convinced that the 
technological and business prowess required to position ourselves at the apex of a new 
entertainment age existed within our walls,” recalled Kutaragi in his interview with 
Jeff Cork, a reporter from Game Informer in 2019 [25]. “Hence, I went straight to Mr. 
Ohga, the president of Sony at that time, who possessed the necessary knowledge and 
experience in both the software and hardware ﬁelds. I believe his dream at the time 
was to build Sony’s next big business domain from the ground up. At the executive 
meeting slated to decide the future of the PlayStation project, I voiced my passion 
to Mr. Ohga directly, to which he responded, ‘If you believe you can do it, then do 
it!’ Those decisive words still echo in my mind like it was yesterday.” 
The breaking of the partnership between Nintendo and Sony infuriated Sony’s 
President Norio Ohga, who responded by giving Kutaragi the responsibility of devel-
oping the PlayStation project to compete with Nintendo. Ohga told Sony (the PlaySta-
tion group) and Sony Music to form a joint venture because the PlayStation project 
did not ﬁt either of their cultures. 
Unbeknownst to several top managers, Kutaragi worked in secret on a new game 
console, using the reduced-cost System G technology. 
One of the pioneers in 3D graphics for TV was a Japanese company named 
Nichimen and was the inspiration for Sony’s System G. Sony contracted Nichimen 
to help with a demo of a 3D tyrannosaurus rex (Fig. 5.20). It was one of the most 
successful game demos ever made.
Sony showed the dinosaur demo: it had an extremely high-poly count, and it was 
animated in real-time on a black background. 
The PlayStation (PS) used a MIPS CPU and a custom, Sony-designed graphics 
controller. The graphics controller connected to the system bus, along with the CPU 
and the memory. There was 1 MB of VRAM for graphics and 2 MB for all other 
system functions. (Therefore, some people said that the PS had 3 MB of RAM.) 
The graphics controller could generate a 1024 × 513 × 16 image or a 960 × 512 
× 24 image, but it could only display 640 × 480 (TV resolution). Sony stored the 
color lookup table in the non-displayable area of the VRAM [26]. The CPU’s second 
coprocessor (COP2) was a transform engine that did 3D math. It used ﬁxed-point 
arithmetic, which was a close enough approximation. The graphics controller was 
just a 2D drawing engine. Therefore, the graphics controller had no way of changing 
the perspective of the images; it could only do afﬁne texture mapping, which made 
textures look skewed when a game rotated maps on the z-axis.

238
5
1990 to 1999 Graphics Controllers on Other Platform
Fig. 5.20 Sony’s 1991 PlayStation gamer developer demo with quasi-3D textures (Courtesy of 
https://www.youtube.com/watch?v=rwNt_9GvpFI)
Afﬁne texture mapping performs a linear interpolation across a surface (Fig. 5.21). 
Since this involves only 2D math, it is the fastest form of texture mapping. In the 
PlayStation, vertices were projected in 3D space and onto the TV screen during 
rendering. 
Afﬁne texture mapping does not use a polygon’s vertices’ depth (z) information. 
When a polygon is not perpendicular to the viewer, it produces a noticeable defect. 
However, it is inexpensive to implement. 
Filling a triangle involves checking every pixel in it to determine what color is 
needed to paint it or add a texture; perspective and math operations must run—1/z
Fig. 5.21 Comparison of afﬁne and correct perspective texture mapping (Courtesy of Dark-
ness3560 for Wikipedia) 

5.2 Game Consoles
239
Fig. 5.22 PlayStation block 
diagram 
for every pixel. Divide operations are time-consuming, so the PlayStation did not 
do them. Not doing divides only works if 1/z is constant over the entire triangle. 
Therefore, geometry distortions show up when a triangle has a greater angle, as it 
can when it gets closer to the camera (i.e., the viewer or the screen). 
To get around the distortion problem, Sony advised game developers to use small 
triangles because the 1/z values do not vary too much across the face of a triangle 
[27]. 
The overall organization of the PlayStation is in Fig. 5.22. 
The graphics controller, also referred to as the graphics engine, managed the 
VRAM frame buffer and the drawing of primitives, polygons, and textures. It 
supported the following features: 
• Alpha blending (4 per-texel alpha blending modes) 
• An adjustable frame buffer (1024 × 512) 
• Clipping 
• Colored light sourcingDithering 
• Emulation of simultaneous backgrounds (to simulate parallax scrolling) 
• Flat or Gouraud shading and texture mapping 
• Fog 
• Framebuffer effects 
• Mask bit 
• Multi-pass rendering 
• No line restriction 
• Offscreen rendering 
• Render to texture 
• Texture window 
• Transparency effects

240
5
1990 to 1999 Graphics Controllers on Other Platform
It also had special sprite effects: 
• Fading 
• Priority 
• Rotation 
• Scaling up/down 
• Transparency 
• Vertical and horizontal line scroll 
• Warping 
Kutaragi designed the SPC700, the DSP sound processor used in the SNES, and 
one of the ﬁrst sound synthesizer chips. 
People thought of games differently after the release of the PlayStation. “They 
became something cool,” said Makoto Iwai, former GM of Sony Computer 
Electronics Korea. 
“A lot of companies wanted to do 3D—even construction companies wanted to 
make 3D simulations,” said Kazuyuki Hashimoto in an interview for a Polygon 
documentary celebrating PlayStation’s 25th anniversary [23]. 
The PlayStation was launched and was almost immediately a success, especially 
in Japan. Kutaragi became a folk hero, and he was the best-known person in the 
console industry [28]. 
PlayStation eventually became one of Sony’s mainstay businesses, a triumph that 
drew a happy quote from Mr. Ohga. 
“One day, while he was gazing skyward from the conference room window, Ohga-
san said, ‘Wonderful, Kutaragi. Simply wonderful.’ I’ll never forget it,” said Kutaragi. 
Kutaragi repeated the success with the following generation of the PlayStation, 
which turned out to be the best-selling gaming console ever. Sony’s PlayStation 
division became a billion-dollar business. 
Today, most games are made in pretty much the same way. However, in those days, 
there were no game engines one could license, so game developers had to ﬁgure out 
everything for themselves. 
PlayStation laid the path for how consoles, OS, API, game engines, and games 
themselves get made today. Before the PlayStation, chip manufacturers we are not 
especially interested in consoles: today, they are. 
The PlayStation established a ﬁgure of merit and a level of performance. Smart-
phone suppliers describe their devices as having PlayStation capabilities. IGP 
suppliers do the same thing. 
The PlayStation changed the gaming industry forever. 
5.2.4 
Atari Jaguar (1993) 
The Sony PlayStation had set the bar: all consoles would have some 3D capability 
from then on. The console community was small, and secrets were hard to keep. All 
the console developers talked to the game developers, and information got passed

5.2 Game Consoles
241
around even when nondisclosure agreements (NDAs) were signed. The people at 
Atari learned about the Sega–Sony deal to build a console with 3D capabilities. It was 
a trend everyone knew was coming; they were just not sure when. The management 
of Atari decided that it was happening now. 
In 1990, a year before the fateful dueling press announcements at CES, Atari hired 
Flare Technologies, based in Cambridge, U.K. Cambridge had a history with 3D soft-
ware and was also home to Arm. Sega had thrust the console industry into the 32-bit 
world, so Atari asked Flare to develop two system designs—32-bit and 64-bit designs 
named Panther and Jaguar, respectively. Although it might seem impossible today, 
Flare produced the 64-bit design ahead of schedule. Leapfrogging the competition 
with 64 bits was too good to pass up, so Atari went with Jaguar and abandoned the 
Panther design (see block diagram, Fig. 5.23). Whispers about development work on 
the Panther had been heard since 1988; some people mistakenly believed that those 
rumors were about the Jaguar [29]. Atari had been doing research and development 
on next-generation video game consoles since the mid-1980s. 
The 64-bit Atari Jaguar had a 32-bit Motorola 68,000 RISC CPU, a powerful chip 
used in the original Macintosh and Lisa PCs. It also had two custom coprocessors, 
Tom (not an acronym) for graphics and Jerry for audio. 
The Tom chip ran at 26.59 MHz. Atari referred to the graphics controller as a 
graphics processing unit (GPU) with a 32-bit RISC architecture and 4 KB of internal 
RAM. All graphical effects were software-based. The core had some additional 
instructions intended for 3D operations. 
The object processor was a 64-bit nonprogrammable-state machine; it contained 
the CLUT and provided all video output from the system. It delivered 16-bit color. 
The blitter used 64-bit high-speed logic operations and did z-buffering and 
Gouraud shading using 64-bit internal registers.
Fig. 5.23 Atari Jaguar block diagram 

242
5
1990 to 1999 Graphics Controllers on Other Platform
The DRAM controller offered 8-, 16-, 32-, and 64-bit memory management for 
game developers who knew how to take advantage of such capabilities—not many 
did. 
The Jaguar only had ﬁve processors, but the object processor and the blitter were 
64-bit capable. However, since the blitter and the object processor were in the Tom 
chip, it was by extension a 64-bit chip. Also, the Jaguar used a 64-bit memory 
architecture, according to Jez San, the founder of Argonaut Software. The debates 
about whether the Jaguar was or was not a 64-bit machine repulsed many potential 
customers [30]. 
According to Jaguar designer John Mathieson, “Jaguar had a 64-bit memory 
interface to get a high bandwidth out of cheap DRAM [31]. Where the system needs 
to be 64 bit then it is 64 bit, so the object processor, which takes data from DRAM 
and builds the display, is 64 bit, and the blitter, which does all the 3D rendering, 
screen clearing, and pixel shufﬂing, is 64 bit. Where the system does not need to 
be 64-bit, it is not. There is no point in a 64-bit address space in a games console! 
3D calculations and audio processing do not generally use 64-bit numbers, so there 
would be no advantage to 64-bit processors for this [29].” 
The Jaguar was capable of the following visual effects [32]: 
• High-speed scrolling (via the object processor) 
• Texture mapping on two- and three-dimensional objects (GPU and blitter) 
• Morphing one object into another object 
• The object processor preformed scaling, rotation, distortion, and skewing of 
sprites and images 
• The GPU and blitter took care of lighting and shading from single and multiple 
light sources 
• Transparency (object processor). 
The object processor could also perform rendering and was capable of four modes 
of operation: 
850 million one-bit pixels/s, 
35 million 24-bit pixels/s, 
26 million 32-bit pixels/s), 
or 50 million Gouraud-shaded pixels/s. 
It could theoretically deliver an unlimited number of sprites of any size—in prac-
tice sprites were limited by processor cycles rather than a ﬁxed value in hardware. 
Designed for North American NTSC and European and Japanese PAL TVs, the 
Jaguar offered programmable screen resolutions, from 160 to 800 pixels per line. 
Theoretically, the resolution could be up to 1350 pixels per line. 
The Tom chip had 750,000 transistors. Atari commissioned three versions of 
the Tom chip: V1.0, which Toshiba made in December 1993; V1.1, which Toshiba 
produced in March 1994; and a special V1.0 made by Motorola in September 1994.

5.2 Game Consoles
243
5.2.4.1
The End of Atari Consoles 
The Atari Jaguar could not compete with the Sega Saturn and Sony PlayStation. 
The Atari Jaguar could not compete with the 3D generation of consoles ushered in 
by the Sony PlayStation. Atari’s original plan was to compete with the 3DO variants, 
the Super Nintendo, and the Sega Mega Drive. It may have been able to compete 
with the Panasonic 3DO on a technical level, and it easily out-performed the Sega 
Mega Drive and Super Nintendo. 
Atari never realized the sales levels forecast for the product, even after price 
reductions. 
Atari abandoned the Jaguar in 1995, having sold only 125,000 and still holding 
100,000 in its inventory. 
On April 8, 1996, Atari Corporation agreed to merge with JTS, Inc., in a reverse 
takeover thus forming JTS Corporation [33]. The merger was ﬁnalized on July 30. 
On March 13, 1998, JTS sold the Atari name and all the Atari properties to Hasbro 
Interactive. 
See Book three for the story about the resurrection of the Atari console brand with 
the Atari VCS console. 
5.2.5 
Nintendo 64 (1996)—The First T&L in a Console 
A breakthrough design, genuinely disruptive 
Silicon Graphics was a leader and highly respected workstation developer that rose 
to fame and fortune based on its VLSI geometry processor. In the ensuing years, 
SGI developed leading high-end graphics technologies. A high-end super-high-
performance workstation could cost over $100,000. Therefore, the idea of adapting 
such state-of-the-art technology to a consumer product like a game console that sold 
for a few hundred dollars was considered bold, challenging, and crazy (Fig. 5.24).
Nonetheless, in 1992 and early 1993, Silicon Graphics (SGI) founder and CEO 
Jim Clark met with Nintendo CEO Hiroshi Yamauchi to discuss just that—squeezing 
an SGI graphics system into a console [34]. Thus the idea of the Nintendo 64 was 
born. 
Even the number, 64, was outrageous. Then, most consoles were struggling to get 
from 8 to 32 bits; 64 bits was considered science ﬁction. 
Even so, they did it, and on November 24, 1995, at Nintendo’s 7th Annual 
Shoshinkai trade show, the company unveiled the Nintendo 64 console. Later in 
May 1996, at the E3 conference in Los Angeles, Nintendo showed the Nintendo 64 
and announced that it would be available in the U.S. in September. 
It was a fantastic amount of technology crammed into a ridiculously small package 
at a crazy low price of $250 (Fig. 5.25).
The little supercomputer would be considered feature-rich today; and, other than 
the clock speeds would be a competitive device.

244
5
1990 to 1999 Graphics Controllers on Other Platform
Fig. 5.24 Nintendo 64, the ﬁrst console with a 3D accelerator (Courtesy of Wikipedia)
Fig. 5.25 Nintendo 64 motherboard, CPU, and reality coprocessor, with RDRAM below the 
processors (Courtesy of Nintendo)

5.2 Game Consoles
245
The Nintendo 64 features included: 
• 64-bit custom MIPS R4300 CPU, with a clock speed of 93.75 MHz 
• 4 Mbytes Rambus DRAM (RDRAM) with a maximum bandwidth of 4,500 
Mbits/s 
• Sound, graphics, and pixel drawing coprocessors, with a clock speed of 62.5 MHz 
• Resolutions ranging from 256 × 224 to 640 × 480 (normal resolution is 320 
× 240, 24 bpp) 
• 32-bit RGBA frame buffer, with 21-bit color video output 
The graphics processor includes 
• z-buffer 
• Anti-aliasing 
• Texture mapping: trilinear interpolated with mipmaps, environmental mapping, 
and perspective correction 
• Size: 10.23 × 7.48 × 2.87 inches; weight: 2.42 pounds 
The system came with a multifunction 2D and 3D game controller, including 
digital and analog joysticks and multiple buttons. The MIPS and RPC processors 
were 350 nm manufactured by NEC for Nintendo. 
The system’s organization consisted of two main chips: the main CPU, and the 
reality coprocessor (RCP), both designed by SGI. Figure 5.26 shows the overall 
arrangement. 
The VR4300 main CPU was a 64-bit microprocessor that ran at 93.75 MHz with 
64-bit registers, data paths, and buffers to ensure high-speed data movement within 
the chip. The wide data paths were particularly important for operations such as 
bit-stream decoding, matrix manipulation, and core video and graphics processing
Fig. 5.26 Nintendo 64 block diagram 

246
5
1990 to 1999 Graphics Controllers on Other Platform
Fig. 5.27 Nintendo 64 reality block diagram 
features. The VR4300 device also supported double-precision ﬂoating-point opera-
tions for high-performance graphics. Large on-chip caches (16 Kbytes instruction and 
8 Kbytes data) provided higher performance for interactive applications by reducing 
the need for frequent memory accesses. They built the chip using NEC’s 350 nm 
drawn (270 nm effective) process technology. 
There were two subsystems within the RCP: the reality signal processor (RSP) 
and the reality display processor (RDP) shown in Fig. 5.27. 
As pointed out in Rodrigo Copetti’s excellent review of the Nintendo 64 [35], the 
RSP contained the following: 
• The Scalar Unit: A MIPS R400-based CPU that implemented a subset of the R400 
instruction set. 
• The Vector Unit: A coprocessor that performed vector operations with thirty-two 
128-bit registers, with each register sliced into eight parts to operate eight 16-bit 
vectors at once (just like SIMD instructions on conventional GPUs). 
• The System Control: Another coprocessor that provided DMA functionality and 
controlled the neighbor display processor module. 
The RSP used a 128-bit MIPS R4000 as a vector processor (integer only). 
Programmable through microcode, the chip was reconﬁgurable by an applica-
tion (game) for different workloads and functions. Nintendo also included several 
program kernels in microcode, which added acceleration. The RSP did geometry 
transformations, triangle setup, clipping, and lighting calculations. 
The RSP fed the RDP, illustrated in Fig. 5.28.

5.2 Game Consoles
247
Fig. 5.28 Nintendo 64 RCP block diagram 
After the RSP ﬁnished processing polygon data, it sent rasterization commands 
to the RDP. Those commands got sent through a dedicated bus called XBU.S. or 
through the main RAM. 
The RDP was just another processor with ﬁxed functionality. It had multiple 
engines used to apply textures over polygons and project them on a 2D bitmap. It 
could process triangles or rectangles as primitives; the latter was for drawing sprites. 
The RDP’s rasterization pipeline contained the following blocks: 
• A rasterizer that allocated the initial bitmap that served as a frame buffer:

248
5
1990 to 1999 Graphics Controllers on Other Platform
• A texture unit that applied textures to polygons using 4 KB of dedicated memory 
(TMEM) allowed up to eight tiles for texturing. It could also perform the following 
operations on them 
• 4-to-1 bilinear ﬁltering for smoothing out textures 
• Perspective correction to improve the coordinate precision of the textures 
• A color combiner, which mixed and interpolated multiple layers of colors (for 
instance, to apply shaders) 
• A blender that mixed pixels against the current frame buffer to apply translucency, 
anti-aliasing, fog, dithering, and z-buffering, the latter being critical for efﬁciently 
culling unseen polygons from the camera viewpoint (replacing software-based 
polygon sorting methods, which could drain a lot of CPU resources) 
• A memory interface used by multiple blocks to read and write the current frame 
buffer in RAM and ﬁll the TMEM. 
The RDP could produce 16.8 million colors and display 320 × 240 to 640 × 
480 pixels. Most games that used the system’s higher resolution 640 × 480 mode 
required the Expansion Pak RAM upgrade. 
The system had several advanced, high-end graphics capabilities, such as: 
• Real-time anti-aliasing. 
• Advanced texture-mapping techniques—mipmapping. 
• Real-time depth buffering—removed hidden surfaces during rendering. 
• Automatic load management—enabled the objects in the scene to move smoothly 
and realistically by automatically tuning the graphics processing. 
The console came with a new three-grip controller that allowed 360-degree preci-
sion movement. A 3D stick enabled players to rotate to any angle and control the 
speed of a character’s movement. Other new additions included C Buttons, which 
changed a player’s perspective, and a Z Trigger, for shooting games. In addition, 
the controller featured a memory pack accessory, which allowed players to use a 
memory card to save game play information on their controller. This enabled players 
to take their game-play data with them and play on other Nintendo 64 systems. 
Over 350,000 Nintendo 64s sold within days of its release 
Console Wars. The console market was highly contested then (like now), and new 
companies were entering the market as older ones were driven out. As a result, the 
suppliers started a price war that almost ruined them all. 
In August 1996, Nintendo announced plans to drop the price of the Nintendo 64 to 
$199.50 just before it launched in the U.S. They did that to match the 32-bit systems 
from Sony and Sega. Sony, in turn, reacted by reducing prices on many of its titles 
to $39.99. Sega said that it did not plan to reduce its console price. 
Then, six months later, in March 1997, Nintendo announced an even lower price 
of $149.95 for the Nintendo 64. At the time, the company cited the favorable foreign 
exchange rates and the production efﬁciencies resulting from the planned global sales 
increase to 12 million units for the 12 months starting on April 1, 1997. With Sony,

5.2 Game Consoles
249
Sega, and Nintendo all at below $150, it was not unreasonable to expect sub-$100 
sales bonanzas for Christmas 1997. 
By May 1998, there were only two players in the console market, the Sony 
PlayStation, and the Nintendo 64. The Saturn system could still be found, but not for 
long. 
Then, in August 1999, Nintendo dropped the Nintendo 64 to under $100, as fall 
and the holiday market heated up. Price cutting meant the console suppliers thought 
the market was saturated and no new gamers were entering. Therefore, the only tool 
they thought they had to try and gain market share was to outbid the competitors. It 
was clear they did not know how to market their products to attract new users. And 
in the end, it did not help any of them, but the consumers liked it. 
5.2.5.1
Summary 
The Nintendo 64 had many advanced techniques and solutions that have become the 
basis for modern 3D gaming. 
Here are some features developers used on the software that paved the way for 
modern game engines. 
• Trilinear mipmapping, often the most touted feature 
• Edge-based anti-aliasing (which we have today as FXAA and MLAA) 
• Basic real-time lighting. 
The one thing that stands out about the reality coprocessor (RCP) is it was one 
of the ﬁrst GPU-like systems and fully programmable [36]. The processor ran on 
microcode, which developers could tweak to suit their needs. The problem was that 
Nintendo did not release the tools for tweaking those codes until late in the N64’s life 
span. However, once the tools were available, a few developers pushed the system 
to its limits. 
Other features that game studios exploited included the following: 
• Clever use of clipping allowed sections of the game world not visible only to be 
rendered when the player got close to them. 
• Banjo Kazooie had a novel way to push out large textures for detailed environ-
ments. One of the issues was that it caused memory fragmentation, which meant 
even though enough memory was available, it was not contiguous memory. So, 
they had a real-time memory defragger run during the game. 
• Texture streaming. Game developer Factor5 used it for Indiana Jones and the 
Infernal Machine. It allowed them to stream textures as they got rendered, 
overcoming the 4 KB texture-memory limit. 
• Frame buffer effects. Effects like motion blur, shadow mapping, cloaking, and 
render-to-texture (textures that were created and updated at run time). 
• Level of detail. A trick in which, if a model is sufﬁciently far away, it gets swapped 
for a low-poly model.

250
5
1990 to 1999 Graphics Controllers on Other Platform
The Nintendo 64 was truly ahead of its time, but unlike other pioneers, it did not 
suffer for being so advanced. The Nintendo 64 did very well and drove the industry 
forward toward more realistic and high-performance computer graphics. 
5.2.5.2
Nintendo Epilogue 
Nintendo may have spurred the creation of the PlayStation. In the early 1990s, 
Nintendo partnered with Sony to develop a new CD-ROM console and attachment 
for the Super Nintendo system, resulting in a prototype that fans called the Nintendo 
PlayStation. 
However, Sony’s deal with Nintendo fell through. Sony ultimately decided to ditch 
Nintendo and launch the PlayStation on its own—a decision that would completely 
change the course of the video game industry. It led to the birth of Sony’s massive 
PlayStation brand and a signiﬁcant and long-term competitor for Nintendo. 
In May 1999, Nintendo decided to use IBM’s 400-MHz, 128-bit PowerPC chip, 
called Gekko, in Nintendo’s new Dolphin game console. 
Nintendo also announced that the new system would be using a new graphics chip 
designed by ArtX, Inc. ArtX was formed in 1998 by ex-SGI/MIPS employees who 
had developed the Nintendo 64 graphics processor. 
Then, in May 2001, Nintendo announced the GameCube, scheduled for launch 
in Japan on September 14, in North America on November 5, and in Europe in early 
2002. The GameCube featured the Flipper Chip from ATI (through ATI’s acquisition 
of ArtX). The integrated processor included a 2D and 3D graphics engine, a DSP 
for audio processing from Macronix, and all system I/O functions, including a CPU, 
system memory, joystick, optical disk, ﬂash card, modem, and video interfaces, 
and an on-chip high bandwidth frame buffer. IBM supplied the 485-MHz Gekko 
microprocessor. 
5.2.6 
ArtX and the Nintendo GameCube (1998) 
Announcement vs. shipment 
Nvidia gets credited for introducing the ﬁrst graphics chip with an integrated T&L 
engine for the PC in 1999 and popularizing the acronym GPU. 
However, as discussed above, in 1993, SGI began designing the Reality Copro-
cessor for the Nintendo 64 game console. The Nintendo 64 shipped in 1995 in Japan 
with the fully integrated, but not named, GPU. 
The Nintendo 64 was a great machine, but SGI was in a downward spiral [37]. The 
company’s management bickered internally, and SGI went through three presidents 
from 1994 to 1997. 
In 1994, Jim Clark, inventor of the Geometry Engine and founder of SGI, resigned 
from the company a year after signing the deal with Nintendo. His resignation caused

5.2 Game Consoles
251
concern at Nintendo of America, and it was the ﬁrst of many setbacks in SGI’s and 
Nintendo’s relationship [38]. Nintendo and SGI had arguments over the price of 
the chips. Worried about the instability of the management at SGI, Nintendo began 
looking for a new CPU and graphics processor supplier. 
Nintendo almost found a new supplier through a series of acquisitions involving 
3DO, Panasonic Matsushita, and CagEnt. Next Gen Magazine reported that Howard 
Lincoln, Chairman of Nintendo of America, and Genyo Takeda, the general manager 
of Nintendo’s Integrated Research & Development division, visited CagEnt to 
explore working together. CagEnt was the renamed (by Samsung) development group 
of 3DO that had a new console chipset design. In late 1997, Nintendo offered to 
acquire CagEnt from Samsung, but Nintendo could not agree on the ownership split, 
and the deal fell through [39]. 
In 1997 Nintendo went to ArtX right after, ArtX started. ArtX had 20 engineers 
who had previously worked at SGI on the design of the Nintendo 64. ArtX received 
its initial funding from Taiwanese PC maker Acer, Inc. 
“We said we really did not want to divert ourselves, but Nintendo can make a 
pretty compelling argument, and it was a pretty huge opportunity, so we decided to 
go ahead in mid-’98,” said Tim Van Hook, chief designer for the Nintendo 64 and a 
founder of ArtX, as reported by Rick Merritt of EE Times [40]. The opportunity was 
huge for a startup. 
ArtX’s Greg Buchner told EE Times, “They [Nintendo] had given up on SGI. The 
last of the people they trusted were gone, and they went looking for the people. It is 
not a company-to-company thing for Nintendo; it is a person-to-person thing.” 
Word leaked out in February 1998 that ArtX would design the graphics chip for the 
new Nintendo GameCube, code named Dolphin. In May 1999, at the E3 conference 
in Los Angeles, Nintendo ﬁnally made it ofﬁcial. Howard Lincoln, Nintendo of 
America’s chairman, ofﬁcially announced the company’s next-generation console. 
The company announced that it was also dropping the MIPS chips used in their 
popular N64 machines and replacing them with custom PowerPC CPUs developed 
by IBM. The new 400-MHz Processor CPU, dubbed Gekko, would be manufactured 
at 180 nm and use copper interconnects, an advanced technique at the time. Nintendo 
planned the release of the console for release in mid-2000. 
“…But, you will remember the company had a rough time getting the Nintendo 
64 out the door on time,” commented industry analyst Kathleen Maher. “ArtX is sexy, 
but Sony is, well, Sony. If we had to handicap this race, we would say that being ﬁrst 
is not necessary to win, but being on time is going to be crucial [41].” 
The 2000 date came and went, but Nintendo did not ship; Nintendo missed the 
Christmas 2000 shopping season. This gave Sony the advantage of releasing its 
console one year before its competitors. 
In February 2000, ATI announced the acquisition of ArtX, and the Nintendo 
project moved to ATI. ATI now had a Silicon Valley base in Santa Clara, a mile 
down the road from Nvidia’s headquarters. Engineers at the ATI site ﬁnished the 
GameCube graphics chip, and Nintendo shipped the GameCube in Japan and North 
America in 2001 and PAL territories in 2002 (Fig. 5.29).

252
5
1990 to 1999 Graphics Controllers on Other Platform
Fig. 5.29 Nintendo GameCube system board (Courtesy of Nintendo) 
An IBM manufacturing facility in Burlington, Vermont, built the 200-MHz ArtX 
Flipper chip in a 180 nm process, like the Gekko CPU. The only thing that did not 
connect directly to the Flipper chip was a set of digital-analog converters (DACs) 
for the TV and the audio output. Therefore, the communication between Flipper and 
Gekko was critical. 
The Flipper-Gekko was a 128-bit system with a 400-MHz port dual-chip design, 
much like the Nintendo 64. However, as impressive as the speciﬁcations were, they 
ruled out backward compatibility, a feature Sony bragged about with the PlayStation 
2. 
ArtX used embedded DRAM and high-speed DRAM with a memory bus band-
width of 3.2 Gbps. The DRAM, built by NEC, was described by the company as an 
application-speciﬁc memory. 
The Flipper chip also made use of the popular and efﬁcient texture-compression 
scheme developed by GPU maker S3. Howard Lincoln, co-chairman of Nintendo 
of America, Inc., said, “With their S3’s unique graphics compression technology, 
developers will be able to provide players with more complex and colorful graphics. 
Coupled with our previously announced strategic agreements with companies like 
IBM, Matsushita, ArtX, and MoSys, incorporation of S3 technology will make 
Dolphin a console without equal [42].” 
The Flipper chip was almost (some said exactly) the same as the IGP ArtX (and 
subsequently ATI) built for Acer for the Aladdin 7 chipset.

5.2 Game Consoles
253
5.2.6.1
ArtX Epilogue—The End of an Era 
In February 2007, Nintendo announced it had terminated support for the GameCube 
and discontinued the console. The company was shifting its development and manu-
facturing endeavors to the Wii and Nintendo DS. AMD, which had acquired ATI in 
2005, built the Hollywood chip for the Wii. However, in late 2015, Nintendo termi-
nated its 20-year relationship with the SGI/ArtX/ATI/AMD console chip designers 
and chose Nvidia’s Tegra for its Switch Console. It was a tremendous success, selling 
over 30 million units in the ﬁrst year. 
5.2.7 
NEC Electronics’ PowerVR (1996) 
Imagination Technologies introduced its PowerVR graphics controller in 1994. At 
the same time, it announced that it had established an agreement with NEC for the 
collaborative development of 3D technology. VideoLogic would license its tech-
nology to NEC. As an exclusive distributor, NEC would sell the PowerVR product 
worldwide. 
PowerVR was Imagination’s real-time 3D image technology, developed at Vide-
oLogic over the previous three years. As part of the deal, NEC also agreed to buy 
2.29% of the VideoLogic Group [43]. 
Then, in 1996, NEC Electronics introduced a new family of 3D graphics proces-
sors, targeting the arcade/console markets with a multichip solution and the PC 
market with an integrated single chip implementation. Marketed as PowerVR Tech-
nology, NEC said it would provide a high-level API developed by VideoLogic called 
PowerVR SGL. The API would support arcade/console and PC implementations, 
and the company claimed that it would signiﬁcantly reduce the development costs 
of cross-platform efforts. 
The arcade chipset consisted of an image synthesis processor (ISP) and a texture 
shading processor (TSP). 
The arcade implementation was scalable using multiple ISP chips or multiple 
higher-performance CPUs, as shown in Fig. 5.30. An ISP was added for each 
processor. The company said that up to eight processors/ISPs could be added without 
hitting diminishing returns.
Having more ISPs would be beneﬁcial in cases where there were a lot of overdraws 
and/or shadows because shadows could be visibility ﬁlled (but not texture/shading 
ﬁlled). The ISPs also shared their caches. 
NEC cited performance characteristics of 257,000 sustained (one million peak) 
100-pixel triangles at 640 × 480 resolution at 30 fps and 16.7 million colors. Every 
pixel was mipmapped, textured, fogged, lit, and shaded. The company claimed it 
was able to obtain superior performance from a reduced memory architecture. 
The ISP performed image synthesis, which included hidden surface removal, 
shadow generation, and depth cueing (see block diagram, Fig. 5.31). Since the ISP

254
5
1990 to 1999 Graphics Controllers on Other Platform
Fig. 5.30 Arcade implementation of NEC’s VideoLogic ISP and TSP chips
preformed hidden surface removal in hardware, it did not need a z-buffer. This elim-
inated the cost, as well as the memory bandwidth requirement, of a z-buffer. In addi-
tion, since hidden surface removal was at the device’s clock speed, it beneﬁted from 
improvements in semiconductor manufacturing improvements—it was Moore’s law 
friendly. 
The ISP was composed of 32 processing elements (PEs)—each processing a pixel 
in parallel, so 32 pixels got processed at a time. Each additional ISP added to the 
system added another 32 processing elements (shaders).
Fig. 5.31 NEC/VL ISP block diagram 

5.2 Game Consoles
255
The TSP performed texturing, shading, an display management. Texturing and 
shading were performed only on visible pixels, making the performance nearly inde-
pendent of the scene complexity. This deferred processing also reduced the band-
width requirements of the texture and frame buffer memories. Unlike a conventional 
rendering system where every polygon is textured and written to the frame buffer 
and then potentially overwritten, PowerVR textured and wrote only visible pixels. 
This was possible because the ISP’s hidden surface removal approach created pixel 
groups (tiles) that were fully resolved in z-depth and constituted visible pixels only. 
For every surface in a tile, the ISP performed a ray tracing operation that deter-
mined, among all the surfaces associated with that tile, the surface closest to the 
viewer. Several parameters represented each surface by A, B, and C, which provided 
the depth of that surface at that point for a particular pixel position. 
The ISP processing unit had a pre-calculation unit that computed the initial depth. 
The processing array contained 32 processing elements that performed hidden surface 
removal for 32 adjacent pixels. The ISP plane/polygon parameters were cached in 
the on-chip memory and used by the processing engine in the hidden surface removal 
operation. 
Associated with each surface was an 18-bit identiﬁer that got passed to the TSP 
to fetch the appropriate shading and texture parameters. Finally, there was a 4-bit 
instruction that the PEs used. The instruction identiﬁed the type of surface to be 
processed; typical surface attributes were forward/reverse and visible/invisible. The 
TSP can be seen in Fig. 5.32. 
Fig. 5.32 NEC/VL TSP block diagram

256
5
1990 to 1999 Graphics Controllers on Other Platform
Fig. 5.33 Trevor Wing 
(Courtesy of Register of 
Chinese Herbal Medicine) 
“Because PowerVR technology minimizes the memory bandwidth requirement, it will not 
suffer from the technology ‘brick wall’ facing other 3D solutions. Memory bandwidth is not 
critical to the PowerVR architecture. Without that performance bottleneck, PowerVR tech-
nology becomes a solution that can grow as silicon technology improves,” said VideoLogic 
Vice President of Marketing Trevor Wing. 
Another key feature of the PowerVR technology was its 3D rendering algo-
rithm, described by Wing as an inﬁnite plane (surface)-based. Wing stated that this 
approach enabled advanced lighting capabilities such as efﬁcient full shadows and 
real-time searchlights typically not found on other 3D systems. The rendering algo-
rithm also allowed the system to efﬁciently accommodate polygons, polygon meshes, 
inﬁnite planes, and convex objects constructed from such inﬁnite planes. The inﬁ-
nite surface-based algorithm used low-level polygon mesh-based modeling. Edge 
surfaces got deﬁned as convex polygons, and that allowed using existing mesh-based 
models. There were also high-level object modeling capabilities. Convex objects 
also got deﬁned by bounded surfaces, which allowed high-performance shadows 
and spotlights (Fig. 5.33). 
In high-performance z-buffered systems, the bandwidth requirement meant 
multiple memory banks and costs. For example, if a system could deliver 1 M trian-
gles/s (assume 200 pixels/triangle), it would need 200 M read-modify-write cycles, 
which resulted in over 400 M accesses/s to memory and required an effective memory 
speed of <2.5 ns. This would require multiple expensive memory banks. NEC said 
that PowerVR avoided that by doing on-chip hidden surface removal. Although 
there were no explicit z-buffers, NEC claimed that the chip had high accuracy, with 
Z-information equivalent to a 32-bit z-buffer. 
A high-performance system’s texture memory speed also needed to be high, which 
resulted in redundancy. Potentially, (multiples of) 200 M accesses to texture memory 
were needed. And that could lead to multiple copies of the texture memory. The 
PowerVR technology avoided that by using deferred texturing, which only requires 
one bank of slow memory.

5.2 Game Consoles
257
On-chip and easy access to z-information enabled the further processing of 
shadows, translucency, and visibility effects (fog, haze, and gloom per-pixel). This 
got used for generating realistic ﬁre, trees, lens ﬂare, dirty glass, and water effects. 
Shadows got cast accurately on other objects in a scene, proportional to the light 
intensity and dynamically generated in every frame. The shine of spotlights was 
reﬂected accurately on different objects. All these features were in production before 
Microsoft introduced its Talisman design. 
PowerVR offered perspective-correct texture mapping and solid images with no 
pixel shimmer (linear mipmapping). NEC stated no shifting/jumping even on large 
polygons (because they divide per-pixel) occurred. NEC claimed the chip performed 
an exact match to polygon edges (fractional u, v). 
The single chip implementation, called the PCX1, incorporated the features and 
functions of both the ISP and TSP devices, plus a PCI interface. However, the PCX1 
did not include 2D or VGC capabilities, but interfaces to a VGA device could be 
made via the PCI bus. The integrated unit with TSP and ISP components is shown 
in Fig. 5.34. 
Like the component parts, the PCX1 had a high-speed path on-chip and parallel 
processing elements in the ISP sections. 
The TSP and ISP chips sold for $210 per chipset in 1996. NEC said the expected 
volume price of the 208-pin PQFP chip was under $50 in 10,000-unit quantities by 
mid-1997. NEC also offered development kits, including reference boards, refer-
ence designs, Direct3D and PowerVR SGL libraries, online documentation, sample 
applications, and model converters.
Fig. 5.34 NEC/VL PCX1 block diagram 

258
5
1990 to 1999 Graphics Controllers on Other Platform
5.2.7.1
Dreamcast (1997) 
Although it helped put VideoLogic on the map, the Sega Dreamcast got off to a rocky 
start with plenty of intrigues. 
In July 1983, Hayao Nakayama became president of Sega Enterprises. One of 
his ambitions was to have Sega enter the developing Japanese home console market. 
In 1993, Nakayama brought Shoichiro Irimajiri into the company to help realize 
the dream of producing a next-generation console—the Saturn. Nakayama desper-
ately wanted to beat the competition. Atari had just released its Jaguar console, and 
Nakayama was worried about its performance and release date. Nakayama autho-
rized Irimajiri to ﬁnd some new talent. He found Tatsuo Yamamoto in March 1996 
and invited him to participate in developing the new generation of consoles. Irima-
jiri wanted to draw on Yamamoto’s experience as a director of engineering at IBM 
Austin in developing PC and workstation hardware and software tools. Yamamoto 
was appointed vice president of Sega of America and director of Sega of Japan, 
reporting to Irimajiri (Fig. 5.35). 
In Japan, Nakayama was pushing the development of the Saturn over the best-
selling Genesis, and as a result, Sega’s sales fell off. Things got so bad that in 
1996, Nakayama resigned as co-chairman of Sega. Due to the company’s deterio-
rating ﬁnancial situation, Nakayama resigned as president of Sega in July 1998, and 
Irimajiri took over. 
Meanwhile, in Redwood City, California, in 1996, Yamamoto was launching a 
new console project called Black Belt (also known as Dural). Yamamoto based his 
design on the 3dfx Voodoo [44]. For the ﬁrst time, creating a Sega console began not 
in Japan but in the United States. 
Hideki Sato joined Sega in April 1971. He was the project leader on the popular 
Genesis console and ran the next-generation console project, reporting to Irimajiri. 
When Sato heard about the decision to launch the next console project in the U.S., 
he considered it heresy. Sato struggled to accept the decision but never really could 
[45] (Fig. 5.36).
Now Sega had two (or maybe three) console programs running. On the American 
side, Tatsuo Yamamoto and his engineers were working on the Black Belt project 
and the White Belt project—an unofﬁcial project that was a more open (white) 
architecture, unlike Black Belt, which was a proprietary (black) architecture.
Fig. 5.35 Tatsuo Yamamoto 
(Courtesy of Yamamoto) 

5.2 Game Consoles
259
Fig. 5.36 Hideki Sato 
(Courtesy of Sega Retro)
The White Belt was to be an open architecture, capitalizing on industrial innovation and not 
being limited to Sega alone. I had a machine in mind that would allow developers to create 
a game on a PC or a console and switch between them easily through compiler options, like 
what is now on PlayStation and Xbox. However, at the time, in the video game industry, 
and especially at Sega, following such a direction was a double-edged sword, so it had to be 
done with the greatest care.—Tatsuo Yamamoto [46]. 
According to Yamamoto, he had planned to use the PowerPC 603e processor for 
White Belt, but its development ended when Black Belt became an ofﬁcial Sega 
project. 
Meanwhile, in Japan, Hideki Sato and his team launched the Guppy project. 
Sato’s team preferred the PowerVR2 designed by Imagination Technologies and 
manufactured by NEC. 
This arrangement was difﬁcult for outsiders, but Japanese companies are well 
known for creating ongoing competition among their employees. The famous Digital 
Equipment Corporation had a similar policy, which many say led to its demise [47]. 
3dfx’s chip was conventional in design. Its ﬁrst-generation Voodoo was over-
whelmingly popular in the PC gaming industry; it was followed by the Voodoo 
II, which had the same architecture as the Black Belt and steadied its popularity. 
Although PowerVR was not comparable to Voodoo in either image quality or perfor-
mance, its eventual adoption by Sega paved the way for the company’s future. After 
a long period of internal turmoil in management, Irimajiri chose the PowerVR2. 
PowerVR was struggling to grow in the PC industry at the time and would not have 
been able to achieve its subsequent success in smartphones without Sega’s decision. 
“Its’ high-quality graphics are 100 times more powerful than the Sega Saturn and 
other previous consoles,” said Sato at the time. Furthermore, NEC also planned to 
support Dreamcast development by incorporating the same graphics chip in its line 
of personal computers. The collaboration with Microsoft and Windows CE was done 
for the same purpose: to make game development easier. However, the cooperation 
with Microsoft, which the U.S. team proposed and realized for Black Belt, was not 
fully utilized by Sega of Japan, which insisted on developing its proprietary software 
library.

260
5
1990 to 1999 Graphics Controllers on Other Platform
As for the CPU, the next generation of consoles would use SH4, Hitachi’s next-
generation processor, because Sega had already used an SH processor in the previous 
generation of game consoles. 
When Yamamoto joined Sega, the Sega game development division was already 
working with Hitachi to deﬁne and incorporate secret 3D graphics-speciﬁc instruc-
tions into SH4. Seeing the SH4’s dual-issue pipeline with its added instructions for 
vector and geometry acceleration, Yamamoto accepted the decision to use SH4 for 
Black Belt. In late 1996, Sega of Japan ofﬁcially told Hitachi that a U.S. team would 
develop the next-generation console. 
In January 1997, Hitachi hand-carried the ﬁrst SH4 silicon to the Sega of America 
ofﬁce for the ﬁrst Black Belt prototype. 
In 2019, Sato spoke about those days [48, 49]. The Saturn also used a Hitachi 
CPU, the SH-2, so it was an easy transition. 
Motorola had the MC68020, the successor to the MC68000. It was a strong-selling 32-bit 
CISC microprocessor. Sega of America, who were developing their own 16-bit Genesis 
games, wanted to use the MC68020 in the Saturn. That would have allowed for essentially 
updated versions of the current types of game software, and the development libraries could 
easily be done. They wanted to go for forward compatibility [50]. 
However, from my viewpoint, this lacked the necessary jump in technology. I thought 
that it might be okay to move forward with such a continuation of the current technology, 
but all the same, I felt we needed to move in a new direction, to change things up. Compared 
with the 16-bit generation, we needed to move away from mask ROMs, from solid-state 
memory, which was too expensive. CD-ROMs had become cheap, but the technology was 
no longer new. The PC Engine had already been using it for years. We needed something 
more. 
At the time, Hitachi happened to be developing the SH processor. After seeing the specs, 
I was impressed by its high performance. I decided to go with it, even though it was still in 
development (this was a very rash move for me). The SH is a RISC (Reduced Instruction) 
CPU, and at that time, NEC was also developing one, the V Series. I felt that Hitachi’s SH 
was good, so I went with that.—Hideki Sato. 
The decision to use the NEC-backed PowerVR Series II GPU was a blow to 3dfx, 
and it sued Sega for $155 million, claiming that Sega misled them. Sega settled the 
lawsuit for $10.5 million. 
There were a lot of rumors and speculations about this project, none of which are completely 
accurate. The 3dfx Voodoo II chipset that took the PC gaming by storm in the late 90s was 
at least partially the outcome of our Black Belt project after it was abandoned by Sega. I 
visited the 3dfx garage early 1996 to meet the founders and witnessed the bring-up of the 
early generation Voodoo. It was the moment I’ll never forget. We were all set on 3dfx, and 
I negotiated the OS deal with Microsoft. I wanted to capitalize on the Voodoo success and 
design a game console which could run popular PC games with a compiler switch options. 
U.S. game companies were extremely excited about the idea of using Voodoo, Glide API, 
and MS tools. When I met Bill Gates, he told me in person he would lose sleep if he did not 
get into game software, which made up 50 percent of the software market. I thought that it 
was a Win-Win for Sega and Microsoft and three days later, Microsoft and Sega signed the 
licensing agreement. 
The Japanese design (GUPPY) was a quick and simple copy of our Black Belt system 
design. GUPPY never existed until our 3dfx based prototype was delivered to Japan in mid-
1997. The Japanese team swapped the GPU from 3dfx to PowerVR, which was a mistake

5.3 Conclusion
261
in my opinion. Eventually, they we are not able to build enough units at launch and to make 
it worse, the game titles were all delayed due to the late hardware changes to the immature 
GPU and the software tools. Imagination emphasized the advantage of the tile architecture 
but never wanted to discuss the upfront tile sorting overhead. 
Microsoft, Silicon Graphics, and Hitachi offered to hire my entire team and we made a 
group decision to join Hitachi, who had extended the best offer.—Yamamoto [51]. 
Sega sold 9.13 million Dreamcast units before its discontinuation. However, the 
original expectations were they would sell 20 million units. The reasons commen-
tators gave on why Dreamcast was not successful was the aggressive marketing by 
Sony for the PS2 and a lack of support from EA and Squaresoft, considered the most 
popular third-party game developers. Others said the lack of support from EA was a 
symptom rather than the cause of Sega’s decline. 
Hideki Sato designed many Sega consoles and served as president from 2001 to 
2003. 
Later, after leaving Hitachi in 2004, Yamamoto took over Digital Media Profes-
sionals Inc., an emerging GPU company in Tokyo, transformed the company from a 
chip builder to an IP supplier, and took the company public in 2011. 
Charles Bellﬁeld, NEC Electronics’ project manager for PowerVR, was credited 
with helping NEC Electronics get the contract with Sega for the Dreamcast. 
I was at this meeting on July 4, 1997, in Haneda, in the SEGA ofﬁces in Tokyo. We were 
sent there to introduce PowerVR technology. We have shown a series of games running this 
technology on PC. There was Tomb Raider, a game called Ultimate Race by Kalisto, and 
Flight Unlimited by Looking Glass. During the presentation, we emphasized the fact that 
PowerVR technology offered exceptional value for money.—Charlie Belﬁeld [52]. 
Later, in July 1999, Belﬁeld joined Sega of America’s marketing team. Belﬁeld 
rose to the position of Sega of America’s vice president. Perhaps his most important 
work was done when he served as vice president of strategic planning and corporate 
affairs. Along with then Sega of America president Peter Moore, Belﬁeld helped 
guide the company away from hardware production after the demise of the Dreamcast 
toward its current role as a software supplier. Belﬁeld passed away in 2013 [53]. 
Sega, one of the pioneers of the arcade business, having started in 1966, carried 
on in the arcades-location-based entertainment business in Japan after quitting the 
console market. In 2022 Sega sold off its arcade business (to the Japanese amusement 
rental business Genda), while at the same time opening a new studio in Sapporo, to 
focus on a range of content for consoles and arcade machines. 
5.3 
Conclusion 
The evolution of 2D monochromatic graphics controllers to powerhouse 3D SIMD 
processors was like any other species’ evolution, fraught with expansion, collapse, 
and natural selection. 
The 2D controllers evolved to 3D, as dramatically as a ﬁsh moving onto land. 
Color evolved from 2-bit monochrome to 36-bit HDR, like the development of

262
5
1990 to 1999 Graphics Controllers on Other Platform
sentient development. Manufacturing and marketing challenges revealed the differ-
ences between the comfortable and small professional-graphics segment and the 
take-no-prisoners world of consumer electronics. 
Dozens of companies tried to transition from 2 to 3D and discovered that 3D was 
orders of magnitude more complicated and difﬁcult. Companies that had mastered 
simple pixel on–off for the intensity were crushed by the complexities of color 
spaces and the 3D nature of color. Suppliers who had mastered line-drawing were 
overwhelmed by x, y, z being transformed into u, v, and then often into u1, v1. 
Fluctuations in memory prices, not unlike any other commodity pricing environment, 
completely confounded and confused organizations structured with a cost-plus, 12-
month-contract-price supply chain. 
Companies that knew about computer graphics were naïve about scaling. Dozens 
of companies in the professional space could not understand consumer suppliers’ 
organizational structures and learned that companies could not scale down. In 
contrast, consumer suppliers like ATI and Nvidia showed them how easy it was 
to scale up and satisfy the professional segment. 
The consumer space was not just about x, y, z, graphics. It also included audio 
and video, and to succeed, one had to be a multimedia company. One also had to 
be a software company and understand everything about APIs, operating systems, 
drivers, applications, and security. The technical complexities that go into a modern 
GPU make up a very long list. It is incredible that any single company can master 
it,—there are, by evidence, very few who can. 
However, they could not do it without the semiconductor manufacturing fabrica-
tors—the fabs. Merchant fabs, like TSMC, UMC, and Charter, and private fabs that 
also offered merchant services, like IBM and STMicro, can be credited with enabling 
VLSI, which was used to create the GPU. 
References 
1. Peddie, J. Famous Graphics Chips: HP’s Artist Graphics, IEEE Computer Society, https:// 
www.computer.org/publications/tech-news/chasing-pixels/famous-graphics-chips-hps-artist-
graphics 
2. Martin, P. An Integrated Graphics Accelerator for a Low-Cost Multimedia Workstation, 
apr95a5.pdf (hp.com) 
3. Thayer, L. J., A Highly-Integrated Workstation Graphics System, Hot Chips Symposium, 
(August 
9, 
1993), 
https://old.hotchips.org/wp-content/uploads/hc_archives/hc05/2_Mon/ 
HC05.S4/HC05.4.1-Thayer-HP-Workstation.pdf 
4. Masson, T. Brief History of the New York Institute of Technology Computer Graphics Lab, 
https://www.cs.cmu.edu/~ph/nyit/masson/nyit.html 
5. Mitsubishi Funds Value-Added System Design Start-up HPC Wire, (July 12, 1996), https:// 
www.hpcwire.com/1996/07/12/mitsubishi-funds-value-added-system-design-start-up/ 
6. Cataldo, A. Mitsubishi quietly closes U.S. SoC venture, (February 22, 2000), https://www.eet 
imes.com/mitsubishi-quietly-closes-u-s-soc-venture/ 
7. Form 10-K. (2000, December 31) Retrieved from https://tinyurl.com/mr3vks52 
8. Maher, K. Evans & Sutherland spin out RealImage division, PCGR Volume XIV, Number 39, 
pp, (September 24, 2001)

References
263
9. Kleiman, J. Elevate Entertainment acquiring Evans & Sutherland and Spitz, (February 11, 
2020), https://www.inparkmagazine.com/elevate-es-spitz/ 
10. Peddie, J. The history of Visual Magic in Computers, Springer, London, (2013) 
11. Peddie, J. 3Dlabs announces the details of GLiNT OpenGL Chip, The PC Graphics Report, 
Vol. 2, No. 17, pp. 345, (April 26, 1994) 
12. Peddie, J. 3Dlabs’ Permedia, The PC Graphics Report, Vol. 3, No. 44, p. 1084, (October 24, 
1995) 
13. https://en.wikipedia.org/wiki/RDRAM 
14. Peddie, J. Famous Graphics Chips: 3Dlabs’ Glint to Permedia, IEEE Computer Society, https:// 
www.computer.org/publications/tech-news/chasing-pixels/3dlabs-glint-to-permedia 
15. Dot-com bubble, https://en.wikipedia.org/wiki/Dot-com_bubble 
16. Peddie, J. 3Dlabs was acquired by Creative Labs, TechWatch, Vol. 2, No. 24, p. 24, (November 
25, 2002) 
17. Peddie, J. (August 3, 1998) Intergraph’s Wildcat, The Peddie Report, Volume XI, Number 30, 
p. 937 
18. Weisberg, D. The Engineering Design Revolution: The People, Companies and Computer 
Systems That Changed Forever the Practice of Engineering, (2008), http://www.cadhistory. 
net/14%20Intergraph.pdf 
19. Maher, M. 3Dlabs signs deﬁnitive agreement to acquire Intense3d, The Peddie report, Volume 
XIII, Number 15, p. 533, (April 10, 2000) 
20. Peddie, J. The History of Visual Magic in Computers: How Beautiful Images are Made in 
CAD, 3D, VR and AR, Springer, (2013), ISBN 978-1-4471-4932-3, https://www.springer.com/ 
gp/book/9781447149316 
21. Goldberg, M. Business is fun. Carmel: Syzygy Co. Ataribook.com. (2012). ISBN 0985597402. 
22. Ken Kutaragi, 1950, https://www.referenceforbusiness.com/biography/F-L/Kutaragi-Ken-
1950.html#ixzz70FnKaPJl 
23. Hester, B. How PlayStation democratized 3D video games: A quick history lesson on how 
PlayStation came to be, (December 5, 2019), https://www.polygon.com/features/2019/12/5/ 
20997745/how-playstation-democratized-3d-video-games 
24. Sheff, D. Game Over: How Nintendo Conquered the World, First Gamepress Edition, (January 
1999), https://archive.org/stream/0966961706/0966961706_djvu.txt 
25. Cork, J. PlayStation, The First 25 Years: An oral history of Sony’s big gaming play, and how it 
changed the world, (December 3, 2019), https://www.gameinformer.com/feature/2019/12/03/ 
the-ﬁrst-25-years 
26. Nicholas, H. (halkun), Gears Episode 2 - PlayStation Architecture, (October 17, 2013), https:// 
www.youtube.com/watch?v=MPXpH2hxuNc 
27. Harte, T. Why do 3D models on the PlayStation 1 wobble so much? (November 
2017), https://retrocomputing.stackexchange.com/questions/5019/why-do-3d-models-on-the-
playstation-1-wobble-so-much 
28. Leone, M. The legacy of PlayStation creator Ken Kutaragi, in 24 stories: Look back at the 
early days of PlayStation and its ambitious, innovative, and complicated father, (November 
26, 2018), https://www.polygon.com/2018/11/26/18080492/playstation-history-ken-kutaragi-
sony 
29. Jung, R. Atari museum.com Jaguar FAQ, (October 1, 2000), http://www.atarimuseum.com/ 
faqs/jaguar_faq.html (Wayback: https://tinyurl.com/226rhw6c) 
30. Sanglard, F. The Polygons of Another World: Atari Jaguar, (March 13, 2020), https://fabien 
sanglard.net/another_world_polygons_Jaguar/ 
31. Brennan, M; Dunn, T; and Mathieson, J. Technical, Reference, Manual, Tom & Jerry, (February 
28, 2001), Revision 8, by, https://fabiensanglard.net/another_world_polygons_Jaguar/jag_v8. 
pdf 
32. Atari Corp. Jaguar Software Reference Manual - Version 2.4 (PDF). (1995) Atari Corp. 
33. Fisher, A. Atari and JTS to merge, The PC Graphics Report, Volume IX, Number 8, p. 222, 
(February 20, 1996)

264
5
1990 to 1999 Graphics Controllers on Other Platform
34. Peddie, J. Famous Graphics Chips: Nintendo 64, IEEE Computer Society, (February 18, 2000) 
https://www.jonpeddie.com/blog/famous-graphics-chips-nintendo-64 
35. Copetti, R. Nintendo 64 Architecture: A Practical Analysis, (October 7, 2019) https://www.cop 
etti.org/writings/consoles/nintendo-64/ 
36. Peddie, J. Famous graphics chips: Nintendo 64, Breakthrough design, genuinely disruptive, 
(February 18, 2020, https://www.jonpeddie.com/blog/famous-graphics-chips-nintendo-64 
37. Hof R. D., Sager, I, and Himelstein, L. The Sad Saga of Silicon Graphics, BusinessWeek, 
(August 4, 1997), https://tinyurl.com/4smv5h83 
38. Lewis, M., The New Thing: A Silicon Valley Story, W. W. Norton & Company (October 17, 
1999) 
39. Rogers, E. A Dolphin’s Tale: The Story of GameCube, Gwern.net, (January 7, 2014), https:// 
www.gwern.net/docs/technology/2014-rogers-adolphinstalethestoryofgamecube.html 
40. Merritt R. The startup that saved ATI, EE Times, (April 21, 2003), https://tinyurl.com/3x6 
4nexs 
41. Maher, K. Nintendo to use IBM’s PowerPC chips in new Dolphin game console, The Peddie 
Report, Volume XII, number 22, (May 31, 1999) 
42. McMillian, B. S3’s S3TC texture compression to be used in Nintendo’s upcoming Dolphin 
game console, The Peddie Report Volume XII, number 40, (October 4, 1999) 
43. Peddie, J. NEC introduces 3D graphics processor family, The PC Graphics Report IX, no. 9., 
(February 1996) 
44. Dreamcast, Sega Database Project, https://sega.fandom.com/wiki/Dreamcast. 
45. The History of Sega Console Hardware Hideki Sato Interview, Shmuplations, (1998), http:// 
shmuplations.com/segahistory/ 
46. Monterrin, R. Génération SEGA - volume 1 1934–1991: De StandardGames à la Mega Drive 
Omake Books, (2021) 
47. Lewis, M.S. Technology Change or Resistance to Changing Institutional Logics: The Rise and 
Fall of Digital Equipment Corporation, The Journal of Applied Behavioral Science, (January 
7, 2019), https://doi.org/10.1177/0021886318822305 
48. Gryson, Hideki Sato on the Sega Saturn, https://www.sega-16.com/forum/showthread.php? 
33506-Hideki-Sato-on-the-Sega-Saturn-(incredible-new-interview) 
49. Sega R., History of the Sega Dreamcast/Development, Sega Retro, (accessed April 11, 2020), 
https://segaretro.org/History_of_the_Sega_Dreamcast/Development#.22Saturn_2.22 
50. Adams, K.J. Sega Saturn – How Sega won over Japan and lost the world, (April 8, 2019), 
https://karajaneadams.medium.com/sega-saturn-72b0b3398c1b 
51. Interview with Tatsuo Yamamoto, June 8, 2021 
52. Perry, D.D. The Rise And Fall of The Dreamcast, (September 9, 2009), https://www.gamasu 
tra.com/view/feature/4128/the_rise_and_fall_of_the_dreamcast.php?page=2 
53. Obituary: Sega’s Dreamcast spokesman Charles Bellﬁeld, MCV Staff, (November 
19, 2013), https://www.mcvuk.com/business-news/publishing/obituary-segas-dreamcast-spo 
kesman-charles-bellﬁeld/

Chapter 6 
1996–1999, Graphics Controllers on PCs 
What does it take to be a survivor, a leader, an innovator in a nascent industry 
with everything changing? Moore’s law was running full speed. Standards were 
being introduced faster than most companies had time to understand the previous 
ones. Competitors were popping up everywhere. The PC market was exploding, and 
gamers seemed insatiable. 
Companies that seemed invincible fell. Companies that seemed doomed for failure 
or sold for scrap not only survived, but excelled also. Companies that led the industry 
with novel and exciting technology could not come up with a second act. And compa-
nies that should have known better made disastrous business decisions and squan-
dered critical resources on foolish adventures. There was no rule book, no single big 
company to emulate. It was the wild west, ironically started by button-downed IBM, 
who was no longer a participant. 
6.1 The ATI 3D Rage (1995) 
One of the ﬁrst free-D chips and add-in boards. 
In the 1980s, the People’s Republic of China (PRC) was asserting its rights to take 
back the islands of Macau and Hong Kong. Many people worried that Taiwan would 
be next [1, 2] (Fig. 6.1).
KY, as he was known, was born in China in 1950. He was accepted into the 
National Cheng Kung University in Taiwan and graduated in 1974, earning a degree 
in electrical engineering. Kwok Yuen Ho (KY) got a job in Hong Kong. But he could 
see change was coming and with a new set of rules and currency controls. In October 
1983, KY decided to take a vacation and chose Canada, having been curious about 
it for some time. While there, he fell in love with Toronto as well as his future wife, 
Betty. KY decided to stay, and a year later, he applied for immigrant status. 
At the time, Canada was very welcoming to Chinese entrepreneurs and engineers 
[3]. However, even though he had experience in the technology sector, KY had
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2022 
J. Peddie, The History of the GPU - Steps to Invention, 
https://doi.org/10.1007/978-3-031-10968-3_6 
265

266
6
1996–1999, Graphics Controllers on PCs
Fig. 6.1 Kwok Yeun Ho, 
ATI founder and CEO 
(Courtesy of ATI)
difﬁculty ﬁnding a job. He and two other recent Hong Kong immigrants, Lee Lau 
and Benny Lau, decided to go out on their own. By pooling their assets, they were able 
to raise a C$300,000 bank loan. In August 1985, the three entrepreneurs launched 
their own company to produce graphics AIBs and the graphics chips that powered 
them for PCs [4]. 
KY and his team founded Array Technologies Inc. (ATI) in Thornhill, Ontario, 
Canada, just outside of Toronto, taking advantage of various tax subsidies. The 
company was a pioneer in the graphics chip and add-in board market. ATI produced 
its ﬁrst AIBs for IBM and Commodore. By 1987, the company had become an 
independent graphics board retailer, introducing its EGA Wonder and VGA Wonder 
product lines that year. ATI Technologies Inc. went public in 1993. And in 2006, it 
was acquired by AMD and formed the Radeon Technology Group. Along the way, 
it developed some leading-edge products and acquired several companies. 
In October 1985, Ho and his partners designed and built application-speciﬁc 
integrated circuits (ASICs) and developed a graphic controller. They unveiled the 
company’s ﬁrst graphics board product in November (see Fig. 6.2). The company 
produced AIBs for IBM and Commodore. In its ﬁrst year of operation, the company 
sold about C$10 million of those AIBs.
The ﬁrst chip was a 1,000 nm ASIC, code named 16,800, and fabbed at LSI 
Logic. Nicknamed the “Swiss army knife,” it was a popular graphics solution and 
color emulation chip for AIBs. ATI AIBs emulated many graphics standards (MDA, 
CGA, Plantronics, and Hercules) onto a single board irrespective of the monitor 
used. ATI’s key advantage was reducing the complexity of PC graphics and software 
compatibility and reducing the need for multiple (expensive) monitors. The AIBs, 
called Small Wonder, were instant hit and the start of a rocket-propelled journey for 
ATI. 
The little board had 64 KB of DRAM running at 5 MHz, while the controller ran at 
10 MHz, and with a 32-bit memory bus, it achieved 20 MB/s. Although the convention 
of having code names for architecture was not established yet, ATI referred to the

6.1 The ATI 3D Rage (1995)
267
Fig. 6.2 ATI’s ﬁrst graphics chip and board, the CW16800-A (Courtesy of TechPowerUp)
16,800 as the Wonder design or architecture. Some joked it was a wonder it worked 
with all the stuff that was in it. 
By 1987, the company had become a contender for the top ﬁve in share worldwide 
graphics board. They would introduce the EGA Wonder card that year. Due to the 
complexity of the EGA design, ATI decided to build a companion chip that would 
emulate different modes in addition to those provided by the Chips and Technologies 
82C435 ASIC. 
However, ATI’s growth was temporarily handicapped by the worldwide shortages 
of memory chips and working capital. As a sign of his cunning and strategic thinking, 
KY sold graphics boards with Dual in-line package (DIP) sockets without memory. 
PC OEM vendors such as Hynix and Samsung leveraged their internal memory supply 
advantage, bundling ATI boards with their systems. ATI would follow the VIP card 
(VGA Improved Performance), using a C&T ASIC with an ATI companion chip. 
The EGA Wonder and VGA Wonder AIBs that year were compatible with all 
computer monitors, graphics interfaces, and software on the market at the time. The 
company was soon bringing in about C$60 million in annual sales and billing itself 
as the largest graphics board maker in the world. 
In the fall of 1995, as Toronto got ready for winter, ATI announced its ﬁrst combi-
nation 2D, 3D, and MPEG-1 accelerator chip under the name 3D Rage.1 The 3D 
Xpression add-in board (AIB) was based on the 3D Rage graphics chip and featured 
elemental 3D acceleration, one year behind the pioneer Matrox Millennium PC 3D 
chip and at the same time as the S3 Virge.
1 In those days, most companies used coded numbers to designate a chip because it was just a 
component, and branding was not considered necessary—Matrox was the exception. 

268
6
1996–1999, Graphics Controllers on PCs
ATI was using all of its chips on ATI boards, and ATI was the second-largest 
graphics company in Canada behind Matrox. 
Along the way, ATI was gaining notoriety in the tech and OEM communities. The 
company joined VESA (Video Electronics Standards Association). It was elected to 
the steering committee, and it won editorial acclaim with awards such as PC Mag’s 
Editor’s Choice in a roundup of top SVGA boards. 
KY preached the importance of becoming number one in business and an unre-
lenting drive for success with an ideology from warfare and his military training 
in the Taiwanese army. Although being number two or three in market share was a 
signiﬁcant accomplishment for a small Canadian start-up, KY often countered with 
a challenge that as losers in battle, number two is buried next to number three. 
By the end of 1989, Jon Peddie Associates estimated that ATI had grown to 
become the third-largest AIB vendor in market share worldwide, behind Paradise 
and Video Seven. Along the way, ATI surpassed major brands such as Hercules, 
Tecmar, Plantronics, and others that failed to innovate. That was no small feat for a 
privately funded company ﬁnanced just by internal operations, a bank credit line on 
receivables, and Canadian government research tax credits. 
In a quest to move upmarket, ATI took on its most ambitious project to date: to 
reverse engineer the 8514/A graphics accelerator despite IBM’s warnings to write to 
their API. At the time engineers often dismissed the API approach, opting instead to 
code directly to the chip’s registers, referred to as coding or writing to the metal for the 
fastest performance. Halfway into the development, IBM announced the eXtended 
Graphics Array XGA graphics chip and AIB in late October 1990, rendering the 
8514/A standard obsolete. 
What happened next was a breakthrough for ATI as the company stepped out of 
the shadows of following IBM and adding incremental improvements to establish 
its independent innovation. And more importantly, it set the groundwork for ATI to 
capture the ﬁrst position in market share in addition to the high-end performance 
crown. 
Despite concerns over poor market acceptance and sinking precious capital into 
an obsolete graphics standard, the management team at ATI charged ahead into the 
abyss, not knowing what was on the other side. Marketing had convinced executive 
management that ATI could reposition the 8514/A as a Windows accelerator board 
and tackle the high-end market. 
ATI launched the 8514/Ultra and 8514/Vantage at Las Vegas Comdex in November 
of 1989. In an uncharacteristic move that put conventionally private KY into the 
limelight, the Comdex campaign was “Ho Knows” with KY in various sports poses 
as a spoof of Nike’s two-year “Bo Knows” campaign for Bo Jackson Nikes. ATI 
populated Comdex with “Ho Knows” billboards and large bags with KY’s pictures 
in baseball and football poses. 
Although ATI’s 8514 family was delayed due to the ATI 38800 or mach8 ASIC 
revisions and did not ship until the end of 1990, the products received instant 
acceptance and were heavily back-ordered. The 8514/Ultra outperformed SVGA 
on synthetic benchmarks up to 100 times, while the 8514/Vantage did so by up to 
50 times. ATI bracketed Paradise and S3 for ﬁxed function coprocessor boards by

6.1 The ATI 3D Rage (1995)
269
Fig. 6.3 ATI Vantage (left) and Ultra—notice the difference in the memory to the left of the graphics 
controller chips (Courtesy of VGA Museum) 
outperforming them and underpricing them. The Ultra used expensive dual-ported 
VRAM, and the Vantage used less expensive DRAM (refer to Fig. 6.3). 
Much to the chagrin of competitors using the Paradise 5514 chip, TI3020 copro-
cessor, and S3’s 911/928 chips, ATI’s advantage in cost and performance led to 
editorial acclaim with all the top PC publications and as the top seller in Windows 
accelerator boards. 
In early 1991, ATI followed the success of the 8514 by adding its 28,800 VGA 
chip onto a single chip with the Graphic Ultra onto a single board. The success 
and performance halo created by ATI’s 8514 family caused leading PC brands such 
as Gateway 2000, Dell, and PC Unlimited to add conﬁgurations with ATI’s mach8 
boards, often at a premium of $500 or more. Gateway had beneﬁted from their close 
working relationship with ATI and used graphics performance to differentiate their 
system. 
6.1.1 
Approaching the GPU 
Before the 3D Rage, introduced in April 1996, ATI had a chip called the Mach64 
it released in 1994. It was a 2D GUI, or Windows accelerator, and became popular 
on the Graphics Pro Turbo board. The chip numbers evolved from there: the 264CT 
second-generation Mach64 2D accelerator, 264VT (the ﬁrst ATI chip with video 
acceleration), and 264GT (the ﬁrst ATI chip with 3D acceleration,—which was built 
by the team from Kubota Graphics in Boston that ATI had acquired in 1994). 
As the company approached the launch of COMDEX 1995, it learned that S3 was 
planning to launch the ViRGE 3D accelerator. Phil Eisler, the manager at ATI of the 
new chip, was not thrilled about launching a product called the ATI 264GT at the 
same time, so he started searching for a name with some energy behind it to compete 
with S3’s ViRGE. 
Back then, ATI had a small run of a 2D board product called the ATI Arcade 
Rage. Eisler appropriated the Rage brand and changed the name of the 264GT and 
introduced the ATI 3D Rage at COMDEX 95 as the ATI 3D Rage.

270
6
1996–1999, Graphics Controllers on PCs
Fig. 6.4 ATI’s 3D Rage AIB (Courtesy of TechPowerUp) 
The 3D Rage was a versatile and scalable controller, and ATI made seven or eight 
versions. It was one of the ﬁrst graphics controllers to integrate the LUT-DAC and 
clock synthesizer. The CT version had an integrated LUT-DAC, and all versions had 
a VGA  core (see Fig.  6.4). 
The controller had support for all the popular buses of the time: ISA, VLB, PCI, 
and limited VESA BIOS Extensions (VBEs) support. It had a one-pixel shader, no 
vertex shaders, one texture mapping unit (TMU), and one raster-operations pipeline 
(ROP). The controller’s clock ran at 44 MHz. 
The memory clock could run at 57 MHz and be overclocked up to 30% until the 
memory became unreliable. It had a 32/64-bit memory bus and could provide up to 
456 MB/s bandwidth. It could support up to 8 MB of memory (16 for 3D Rage Pro). 
The LUT-DAC could generate up to 16.7 million colors (at 1280 × 1024) and 
65,000 colors (at 1600 × 1200 resolution). 
The chip was made at ST Micro/SGS-Thomson (SGS) in a 500 nm process, had 
ﬁve million transistors, and was in a 90 mm package. 
The 3D Rage represented a departure from the mach32/64 in that it was no longer 
register compatible with previous ATI graphics accelerators or the 8514/A (but it did 
retain VGA register compatibility). That departure was necessary to resolve some 
design limitations that were the legacy of the older-generation chips. Fortunately, it 
preserved almost all the functionality of the mach32/64 in the 3D Rage and some 
valuable enhancements. 
The many variations of the 3D Rage family, the GX (ﬁrst four columns), and the 
CT family are shown in Table 6.1.
From a very rough architectural perspective, the 3D Rage family resembled the 
mach32 more than it did the mach64CT family. However, from a functionality- and 
register-level perspective, the 3D Rage GX was almost identical to the 3D Rage CT.

6.1 The ATI 3D Rage (1995)
271
Table 6.1 The family of versions of the ATI 3D Rage graphics controller 
Feature
GX-C/D
GX-E*
GX-F
CX
CT
VT
GT (Rage I, 
Rage II, II+, 
IIC, Rage 
PRO) LB/GM 
(Rage LTPRO, 
Rage XL) LM 
(Rage 
MOBILITY 
M/P/M1) 
Maximum 
memory 
8 MB
8 MB
8 MB
4 MB
4 MB
4 MB
8 MB.  3D  
charger (2 MB 
EDO DRAM), 
3D xpression+ 
(2 or 4 MB 
SDRAM) 
Minimum 
memory 
512 KB
1 MB
1 MB
512 KB
1 MB
1 MB
1 MB  
Supported 
bus types 
ISA, VLB, 
PCI 
PCI
ISA, 
VLB, 
PCI 
ISA, VLB, 
PCI 
PCI
PCI
PCI, AGP 
*Revision E was a short-lived version used only in Apple Power Macintosh-based boards
ATI made several versions of the AIB with TV video out, and some AIBs had TV 
tuners on them. 
Free-D. The price difference between 2D AIBs and 3D AIBs was so slight that 
the term free-D became popular when describing them. 
In late 1996, the company began shipping its second-generation 3D Xpression PC 
2 TV. Its board featured its new 3D Rage II chip and a new homegrown NTSC/PAL 
encoder chip called ImpacTV. Like its predecessor, the product targeted consumer 
multimedia applications. With the new TV output support, the board was suited for 
deployment in the family room. It could drive a big-screen TV for games and record 
digital video or animation to videotape, or in business settings, where it could drive 
large-screen TV monitors for presentation display. 
The company said then that the new controller provided about 20% better 2D 
performance and twice the 3D performance of the original chip. They accomplished 
that by increasing the memory clock from 63 to 73 MHz, boosting the size of the 
on-chip texture cache, and making pipeline improvements to increase concurrency. 
New features of the 3D Rage II were a 16-bit z-buffer and support for 4- and 8-bit 
palletized textures. 
ATI supplied the chip with an integrated 170 MHz LUT-DAC standard; however, 
the company could screen for qualiﬁcation at 200 MHz, which they offered to OEM 
chip customers as an option. SGS fabricated the 3D Rage II in a 500 nm, 5 V CMOS 
process and packaged it in a 208-pin PQFP. Figure 6.5 shows the internal organization 
of the chip.

272
6
1996–1999, Graphics Controllers on PCs
Fig. 6.5 ATI 3D Rage II internal block diagram 
The new ATI board also incorporated ATI’s ImpacTV TV encoder with a direct 
interface to ATI’s 3D Rage II chip. The 28-pin PLCC device communicated with the 
Rage II through the ATI Multimedia Channel (AMC) interface. The chip included a 
ﬂicker ﬁlter and special circuitry to eliminate dot crawl. It had composite and S-Video 
output as well as SCART output for European component video systems. 
The chip was compatible with all NTSC and PAL formats. It featured 
programmable timing to generate correct NTSC or PAL signals from various 
computer display modes, including legacy VGA modes and newer low-resolution 
DirectX game modes. Illustrated in the block diagram in Fig. 6.6 is the video output 
section of the chip. 
In a 2 Mbyte conﬁguration, the 3D Xpression PC 2 TV AIB had a street price 
of $219. The ImpacTV chip was available for about $10 in OEM quantities. The 
3D Rage II remained at about $30 in OEM volumes. ATI used a variety of silicon 
suppliers back then: SGS, NEC, UMC, and eventually TSMC.
Fig. 6.6 ATI’s ImpacTV chip function block diagram 

6.1 The ATI 3D Rage (1995)
273
ATI had great success with the 3D Rage, and IBM chose it for implementation on 
the motherboard of IBM’s Aptiva multimedia home PCs. It was the ﬁrst announced 
3D chip motherboard design win for the home entertainment market. Other compa-
nies such as Sony and NEC followed suit. At the time, the company was shipping an 
average of a million Rage chips a quarter. 
6.1.2 
The Saga of ATI (1985–2006) 
ATI developed plenty of innovative and exciting technology on its own and rose 
to become the top supplier for a while. However, the market for graphics solutions 
was expanding rapidly into the low-end and high-end gaming sectors, the console 
gaming segment, the professional graphics workstation segment, and the commercial 
markets. Several companies offered solutions for each one of those segments, but by 
themselves, the segments were not enough to sustain the R&D and manufacturing 
costs. ATI recognized that the situation called for an economy-of-scale approach, a 
strategy like Nvidia had accomplished. 
As a result, ATI began to acquire several companies and, in so doing, helped write 
the history of graphics semiconductors. 
The ﬁrst signiﬁcant acquisition, which surprised many people, was in 1994 when 
ATI picked up the design team from high-end and high-ﬂying Kubota graphics. This 
leading-edge workstation supplier had a long history of its own acquisitions. And 
yes, it was the same Kubota that made the heavy construction equipment in Japan. 
They ventured into computer graphics as part of a diversiﬁcation program but could 
not rationalize the continued investment costs. 
Then, in 1997, ATI picked up the assets of a once-leading PC graphics chip and 
board supplier, Tseng Labs. Like many other graphics chip suppliers of the time, 
Tseng could not make the transition to 3D. Saying that does not reveal the true 
nature of the situation. It was not so much that Tseng or other companies did not 
have engineers who knew what and how to do it. They did indeed, and the acquiring 
companies quickly hired those engineers. It was usually the investment cost in dollars 
and time that prevented the transition. ATI, Matrox, and S3 successfully made the 
transition; the rest of the 3D chip companies were start-ups like 3dfx and Nvidia. 
In Mountain View, California, Chromatic Research was founded in 1993 and 
announced its ﬁrst product—the Mpact media engine. Mpact, designed as a software-
upgradable multimedia processor, combined video, 2D graphics acceleration, 3D 
graphics acceleration, audio, FAX/modem, telephony, and videophone. 
In October 1998, ATI acquired Chromatics Research. ATI had long wanted a 
Silicon Valley design center, and Chromatics had been looking for a big brother. 
Chromatics was one of the ﬁrst highly integrated single chip SoC suppliers. 
In 1999, ATI again surprised the industry by acquiring part of Lockheed Martin’s 
Real3D team; Intel acquired the other part. Real3D made very high-end graphics 
for simulators and tried to parlay that into the consumer market, but this time, 
management’s patience and checkbook were insufﬁcient.

274
6
1996–1999, Graphics Controllers on PCs
The big haul for ATI was when it acquired ArtX in February 2000, the ex–SGI 
developers of the Nintendo 64 and subsequent suppliers of Nintendo graphics chips. 
That deal propelled ATI into the console business. They almost had all segments 
covered by now. 
And if all that was not enough in March 2001, ATI acquired Fire GL (FGL), 
formerly a division of S3, which became Sonicblue, not to be confused with S3 
Graphics, a division of VIA. FGL had been competing in the workstation market 
against 3Dlabs, Intense 3D (later acquired by 3Dlabs), HP, and E&S. S3 had gotten 
into the workstation business by acquiring a string of graphics companies. 
By 1996, multi-monitors became practical and affordable, and they were consid-
ered essential by CAD engineers and Wall Street traders. But they were complicated 
to set up. A former graphics board company, Appian Graphics, had distinguished 
itself by developing a robust and easy-to-use software driver that would make multi-
monitors less complicated. That was a feature ATI thought it should have, so in 
August 2001, ATI acquired Appian’s HydraVision desktop management software. 
ATI and subsequently AMD went on to be the leaders in multi-display. 
Next, ATI went after the integrated graphics market. The company already had 
a pretty good integrated graphics controller (IGC), but novel and interesting things 
were being done in Taiwan. ATI chose to acquire it, rather than take the time it would 
need to duplicate that work in a rapidly moving market. 
TV was yet another platform, and ATI had been developing TV capability for 
some time. To speed up the company’s capabilities and presence in what was seen 
at the time as a new multimedia system, ATI in February 2002 acquired NxtWave, 
which was producing TV demodulators (Fig. 6.7).
In November 2005, the founder and CEO of ATI, KY Ho, retired at the peak of 
his success with accolades for his philanthropy. 
The headline in March 2006 read, “ATI acquires Macrosynergy.” Macrosynergy 
was a division of the Shanghai-based chipset ﬁrm XGI Technology, and ATI wanted 
to expand its presence in China. ATI picked up 100 Shanghai-based Macrosynergy 
employees and an undisclosed number of design engineers based in XGI’s Silicon 
Valley ofﬁce as part of the deal. However, ATI did not buy XGI outright, as had been 
rumored. In 2003, XGI formed from the graphics division of SIS, the inventor of the 
IGC. ATI saw Macrosynergy’s presence in China as a gateway into the burgeoning 
Chinese market. Trident was another successful 2D graphics chip company that could 
not make the transition to 3D. XGI acquired Trident, and UMC (a semiconductor 
fab in Taiwan) reacquired SIS, which it had initially helped start. 
Around 2002, mobile phones were starting to take off, and several start-ups 
emerged with graphics coprocessors for them, Personal digital assistants (PDA), 
and other battery-powered personal devices. 
Bitboys Oy was a hardware development graphics company based in Finland, 
founded in 1991. The company began with a revolutionary high-end graphics chip 
that became TriTech’s Pyramid3D. It was challenging to fabricate, and Pyramid3D 
quickly disappeared. However, Bitboys was clever enough to adopt some of their 
concepts into a portable device and sold the concept to NEC, a big player in the 
emerging phone market.

6.2 Nvidia’s Quadratic Processor, the NV1 (1993–)
275
Fig. 6.7 History of ATI’s 
acquisitions
Portable devices were one more platform ATI felt it had to conquer, and in May 
2006, ATI acquired Bitboys. 
And then, in August 2006, AMD acquired ATI for $5.4 billion (Fig. 6.8).
AMD picked up a company with a graphics solution for every platform imaginable, 
from handhelds to TVs to workstations to consoles to everything in between. On 
August 30, 2010, AMD retired the ATI brand [5] for its graphics chipsets in favor of 
the AMD name. 
6.2 Nvidia’s Quadratic Processor, the NV1 (1993–) 
An advanced design the industry was not ready for.

276
6
1996–1999, Graphics Controllers on PCs
Fig. 6.8 ATI’s David Orton and AMD’s Hector Ruiz ofﬁcially announce the historic merger 
(Courtesy of AMD)
In the late 1980s, Jen-Hsun (Jensen) Huang joined LSI as an application engineer 
after doing ECL microprocessor design at AMD. Due to his understanding of the 
aspects of large-scale chip design, they assigned him to the Sun Microsystems project 
that was re-designing the Sparc CPU for the Sun Microsystems’ Sparc Station 1 
workstation. 
The project was run at Sun by Chris Malachowsky, an engineer, and Curtis Priem, 
a graphics chip designer. Priem designed Sun’s GX (Lego) graphics accelerator. The 
GX graphics accelerator was larger and more random than LSI’s Sea of Gates gate 
array ASIC architecture could accommodate. One of the two Lego chips had 50 K 
gates, considerably beyond the ~10 k gate norm for large chips in LSI’s technology 
portfolio (Fig. 6.9).
Huang worked out a way to mitigate the myriad of challenges encountered using 
LSI’s ASICs. In many areas where problems or opportunities existed, one of the 
three companies would write or provide their tools and automation. Getting the Lego 
chipset designed, fabbed, and into production ultimately required a lot of direct 
interaction among Huang, Priem, and Malachowsky. That led to a strong friendship 
and a lot of mutual respect for each other’s skills and talents. They became great 
friends and still are. 
Along the way, Huang developed LSI’s Coreware Design library business, used it 
to design a Sparc CPU for Sun, and ultimately took over running the very LSI design 
center he had joined as an application engineer. As the design center manager, he 
oversaw the design and production of two additional generations of Sun’s Lego 
chipset by LSI.

6.2 Nvidia’s Quadratic Processor, the NV1 (1993–)
277
Fig. 6.9 Chris 
Malachowsky (Courtesy of 
Nvidia)
Huang, Malachowsky, and Priem then planned to start a company to develop a 
graphics chip. They called their ﬂedging start-up Primal Computer at the time and 
worked out of Curtis Priem’s back bedroom in Fremont, California (Fig. 6.10). 
In April 1993, they formalized the company, changed the name, and founded 
Nvidia with the ambitious goal of revolutionizing the PC and console gaming market 
with 3D. They succeeded beyond even their wildest dreams but not without a few 
bumps and bruises, which made them smarter and stronger. At the time, there were 
more than two dozen graphics chips companies, a number that would soar to 47 in 
just the PC market three years later. By 2006, Nvidia was the only independent still 
operating. 
Nvidia announced itself in the summer of 1994. It planned to develop single-chip 
media accelerators for high-quality interactive multimedia. Jensen Huang, as presi-
dent and co-founder called them “industry-deﬁning Media Accelerators.” According 
to Haung, Nvidia’s ﬁrst-generation media accelerator would enable PC platforms to 
deliver high-performance 2D and 3D graphics, video, a digital joystick interface,
Fig. 6.10 Curtis Priem 
(Courtesy of Rensselaer) 

278
6
1996–1999, Graphics Controllers on PCs
and advanced audio capability to accelerate the rapidly growing installed base of 
multimedia applications. 
The original premise was to build a 3D graphics controller based on quadratic 
texture mapping. Quadratic rendering was a dramatically different approach to the 
popular polygon rendering of the day. Priem developed the quadrilinar concepts on 
the Sun GX (Lego) project (see Chap. 4). 
Nvidia also sought to differentiate itself by incorporating sound into the chip. If 
successful, that would mean a gamer had to buy only one add-in board (AIB). 
Nvidia followed the fabless model. However, it was not easy for a start-up to get 
a fab’s attention and, most of all, commitment. Setting up a product line was (and 
still is) a risky and tricky business. Nvidia got lucky and convinced SGS-Thomson 
Microelectronics (SGS) to build the chip, and in exchange, SGS could also market 
it [6]. 
SGS and Nvidia announced two versions of the NV-1 multimedia accelerator. They 
claimed they were the ﬁrst multimedia accelerators to offer real-time, photorealistic 
3D graphics in a single chip. SGS and Nvidia then took the bold step of going to 
Comdex in 1994 to show off the chip. 
Nvidia drove a truck from Sunnyvale to Las Vegas and set it up in a hotel room. 
They had three working boards, two of which were in computers in a meeting room, 
and one was back at the hotel because Curtis and another fellow were still writing 
the drivers. 
After they set up in the room, a security man came around and advised them 
that they should pay for a dedicated security guard to look after their equipment. 
They turned down the offer, and when the Nvidia team returned the next day, they 
discovered everything was gone, stolen. Ah, Vegas! They did the rest of the show 
with the backup computer. 
It was at Comdex that the Nvidia people met Sega and showed them the NV1. 
Nvidia marketed the NV1 VRAM version and SGS offered the STG2000, a 
DRAM version. Introduced in May 1995, the STG2000 was a PCI-based multimedia 
PCI AIB. Nvidia also got a design win with Diamond, the largest AIB supplier of 
the time. Diamond then introduced the Diamond Edge 3D into the retail channel 
(Fig. 6.11).
But Nvidia, Diamond, and SGS could not get game developers to invest in the 
NV1 quadratic texture mapping and Nvidia’s SDK for it. 
The ﬁrst company to show interest in Nvidia’s chip was Sega. They saw it as a way 
to bring the games available on the Sega Saturn to the PC platform. Saga’s interest 
prompted Nvidia to include a joypad port in the chip. 
The NVI chip was the PC industry’s ﬁrst single-chip accelerator supporting the 
multimedia features of Windows 3.11. Those features included 2D/GUI accelera-
tion, real-time texture-mapped 3D acceleration, wavetable audio acceleration, full-
motion video acceleration, and digital input acceleration (refer to Fig. 6.12). The 
NVI also supported legacy applications with multimedia drivers for Windows 3.11 
and integrated VGA for DOS.

6.2 Nvidia’s Quadratic Processor, the NV1 (1993–)
279
Fig. 6.11 Diamond Multimedia’s Edge 3D with SGS (Nvidia) chip (Courtesy of Wikipedia)
Fig. 6.12 Nvidia’s NV1 block diagram 
The differentiator was it used Microsoft’s DirectDraw 2D API and had VGA 
support, so only a single graphics chip was needed. Everyone else at the time rendered 
over the video port. 
The key features of the chip included the following:
• Single-chip multimedia accelerator
• Support for industry standards

280
6
1996–1999, Graphics Controllers on PCs
• GUI acceleration
• Software video acceleration (Indeo, MPEG, Cinepak)
• Real-time texture-mapped 3D graphics
• Acceleration of all 3D standards: quadrilaterals and curves
• Hardware audio wavetable synthesis
• Enhanced digital game port
• System-level performance and cost optimization. 
The pure graphics (2D and 3D) portions of the chip had about 200 thousand of 
the approximately 250 thousand transistors that made up NV1. 
The chip was the earliest example of accelerated computing, making the periph-
erals the accelerators. A virtualized programming interface allowed hardware to 
change while software continued to work, i.e., programming architecture. Nvidia’s 
strategy today goes back to that very beginning concept. 
Cofounder Curtis Priem and Chief Scientist David Rosenthal were the principal 
architects of the chip and its programming model; cofounder of Nvidia Chris Mala-
chowsky was the chief engineer. Their goal went beyond NV1; it was to create a 
virtualized architecture for all things IO (including graphics), audio, etc., that Nvidia 
could build on well into the future. 
As a result, Nvidia’s SDK was very object-oriented. A developer would open a 
class type (graphics, streaming, audio, etc.) and then write directly to the class type’s 
registers in the targeted chip. If the chip supported the class and those registers, 
the operation would be executed in the chip’s hardware. If not, a privileged software 
kernel would be called by the chip that would execute the operation on the host, often 
with support from the chip’s hardware. The process was invisible to the developer; 
as far as the developer was aware, the targeted chip supported everything exposed in 
the SDK. 
Before Microsoft introduced DirectX, Nvidia envisioned game developers would 
write to the SDK for their application’s acceleration(s). 
Much of that architecture is at the heart of Nvidia’s design today and is how the 
company has used so much code between chip generations. Today, the drivers enable 
the evolution of Nvidia’s architecture, which, in turn, inspires new platforms created 
with it such as CUDA, Deepsteam, etc. 
Nvidia said their goal was a multimedia architecture to address the well-known 
challenges for multimedia PCs of the time. The PC of that era was not so much 
a platform as a collection of single-function add-ons that made life difﬁcult for 
developers and consumers. Nvidia claimed its architecture provided a “coherent 
multimedia platform” with concurrent media streaming, interactive 3D graphics, 
high-ﬁdelity audio synthesis, full-motion video texturing, and a digital joystick. 
The big win for Nvidia, PR-wise at least, was Sega. Sega of America established 
an exclusive licensing agreement with Nvidia and said it would convert Sega’s Saturn 
and arcade software to CD-ROMs for PCs equipped with Nvidia’s multimedia accel-
erators. Nvidia set up an exclusive license for PC 3D accelerator AIBs. That meant 
Sega could not port Saturn-based games to any other hardware 3D PC AIB. That 
software pushed Sega Saturn’s sales to more than one million units in Japan since

6.2 Nvidia’s Quadratic Processor, the NV1 (1993–)
281
its introduction in November 1994. Launched in the U.S., in May 1995, Sega Saturn 
quickly sold out of its limited U.S. debut distribution. The company said it would 
be in full distribution by early September. Saturn games were to be released for 
the Nvidia-based products three to six months after ﬁrst appearing on the Saturn 
platform. 
Tom Kalinske, president and CEO of Sega of America, said at the time that he 
believed the markets for gaming on home PCs and dedicated video game machines 
such as Sega Saturn would thrive in the future. The PC games market could reach 
approximately 20–25% of the video game market by 1999. Sega said it would spend 
$30 to $50 million on advertising for Saturn over the next year. 
According to Nvidia, the Sega software would take advantage of every facet of 
their NV1 technology and drive the chip to its limit. Huang, said he thought “PC 
consumers are going to be stunned with the results of our combined efforts [7] 
(Fig. 6.13).” 
Intel also got into the act and planned a presentation at the big computer graphics 
conference, SIGGRAPH 1995 with Nvidia. Intel said it would also make Sonic the 
Hedgehog available to OEMs. (Intel ported it using its native signal processing soft-
ware, NSP. That later resulted in a ﬁght with Microsoft that got to the Department of 
Justice with Intel testifying that Microsoft leveraged its monopoly power in Windows 
to restrict Intel’s ability to compete with MS [8]—but that is a story for another time.)
Fig. 6.13 Jensen Huang right after Sega delivered three arcade machines to Nvidia for testing and 
integration in 1995 

282
6
1996–1999, Graphics Controllers on PCs
The Sega deal was a signiﬁcant coup for Nvidia. The company’s quad patch 
approach had some observers wondering if it was too radical a departure from 
conventional tri-meshes to get any developers interested. 
6.2.1 
Nvidia Epilogue 
The NV1 used quadrilateral geometry rather than triangles like traditional graphics 
pipelines used. The chip not only dealt with four vertices per polygon, but also ﬁve 
additional points to deﬁne surface curves. Quadrilateral meshes are a useful choice 
for modeling many classes of 3D surfaces. They can approximate Bézier patches and 
describe rotational surfaces and rectangular height ﬁelds. 
Quadrilateral meshes (also called patches) were easier for the CPU since the 
setup was accomplished with less polygons compared to triangle-based graphics. A 
sphere, for example, can be formed with just four quadrilaterals. But, game devel-
opers seldom used such shapes. Quadrilateral surfaces evolved somewhat naturally 
from blitters of earlier eras and functions for sprite scaling, rotating, and curving were 
added. But classic texture formats cannot be mapped easily onto complex quadrilat-
eral shapes and each surface must have its own texture with coordinates tied with 
patch orientation. And even though 3DO, and Sega in the Saturn the console world 
used them PC game developers wouldn’t and the approach was dropped. Market 
interest in the NV1 ended when Microsoft announced the DirectX speciﬁcations 
based upon triangle polygon rendering. 
Nvidia was too far ahead of the curve and the industry chose to stay with polygonal 
graphics. There were criticisms of Nvidia’s approach that 2D graphics performance 
could not compete with other options at that time, especially in DOS. And its audio 
quality was not as good as was expected. 
Sega backed out of the deal to use the NV1 in the next-generation console, referred 
to as Saturn II. Sega had ﬁnancial problems caused by being late to market with the 
original Saturn and, Sega engineers were ﬁnding it difﬁcult to program the NV1. The 
blow was devastating to Nvidia. 
After SEGA became disillusioned with NV1, Huang visited Sega Japan and tried 
to win back the console project. Instead, he came back with what some critics said was 
the booby prize—a joint project with Hitachi to build NV2 for a supposed cartridge 
accelerator code named Mutara. That project turned into a nightmare and never got 
made. It was the ﬁnal attempt to create the quad-based engine. Curtis retired as a 
hardware architect after that but contributed to other projects within Nvidia. 
Quads were not the only reason the NV1 failed; it had too many non-graphics-
related items. The audio processor did not perform well. It was almost impossible to 
get the graphics and audio engineers to agree because the processor was not a DSP, 
and the audio guys just did not like it. A security feature also required a separate 
serial programmable ROM (PROM) to store a unique ID, and people were suspicious 
of that. And being an expensive AIB, it failed to win the PC gamer market.

6.3 3dfx Voodoo (1994–2000)
283
The loss of the forward business and Nvidia’s investment in designing the NV2, 
which never came to market, almost killed the company. In one of the industry’s most 
dramatic and daring turnarounds, Nvidia dropped its spherical approach, repositioned 
the company to a polygon design, and introduced the Riva 128 two years later—one 
of the most successful graphics chips ever [9]. 
Developing chips with little to no revenue was ﬁnancially challenging. When 
Nvidia was doing the roadshow for their ﬁrst funding, they talked to venture capitalist 
Don Valentine (Sequoia Capital). He was perplexed about whether Nvidia was a 
graphics, audio, or gaming company. Huang said they were all of them. Valentine 
told him it would not work, and they must pick one, just one. 
It was during that time, in a hallway conversation Priem had with Huang, that 
solidiﬁed their destiny. They were going to build GPUs. They would make them 
forever no matter how many transistors they put in a chip. Only in the ten years or 
so did the silicon catch up, and the GPU ﬁnally ended up in other markets besides 
graphics. 
6.2.1.1 
Nvidia Console? 
During 1995, Sega was porting games (including Panzer Dragoon and Virtua Fighter) 
to the Diamond Edge AIB in exchange for Nvidia building a game box for them. 
The project’s code name was Mutara (taken from Star Trek II and III). It was based 
on the NV2 and was supposed to be the follow-on game box to the Genesis console. 
Nvidia had a prototype working in the lab when Sega canceled the project. Sega had 
come to realize the Saturn was too expensive and too hard to program. Instead, the 
company wanted an entry-level game box to replace Genesis. It was an exciting time, 
and Curtis Priem made seven trips to Japan over seven months. 
The Edge 3D was initially going to be a multimedia platform Nvidia would 
develop for Sega. It would include wavetable audio synthesis, dual game ports, and 
a unique ID per board with 64-bit data encryption standard (DES) decryption to lock 
games to the hardware, which caused gamers to revolt. 
When the market rejected the NV1, Nvidia had to lay off half of their employees. 
They kept just the graphics team for NV5 (Riva 128). Nvidia’s ambition served as 
a wake-up call to Microsoft which was shocked by how close Nvidia had come to 
building a platform. As a result, Microsoft accelerated its efforts to create a master 
API and completed DirectX. 
A few years later, Microsoft would come calling, and all that multimedia stuff 
was put back into a chip created for the Xbox, but the Xbox was not Nvidia’s ﬁrst 
game box. 
6.3 3dfx Voodoo (1994–2000) 
Voodoo changed the gaming landscape with 3D.

284
6
1996–1999, Graphics Controllers on PCs
Fig. 6.14 The founders of 3dfx: Scott Sellers, Gary Tarolli, and Ross Smith (Courtesy of Smith) 
In 1994, in San Jose, California, three men started a revolutionary 3D graphics 
company called 3dfx Interactive. At inception, it was spelled “3Dfx.” It changed to 
“3dfx” (lowercase “d”) in the 1998–1999-time frame (Fig. 6.14). 
Scott Sellers was the chief engineer, Gary Tarolli was head of development, and 
Ross Smith was head of marketing. They had met while working at Silicon Graphics 
(SGI). They left SGI, and two of them joined Pellucid, while the other went to Media 
Vision. Then Media Vision acquired Pellucid, and they were together again. 
Gary Tarolli joined SGI in 1983 from Digital Equipment Corporation (DEC). 
Fresh out of Princeton, Scott Sellers joined SGI in 1991, while Ross Smith worked 
in business development at MIPS Computer Systems and SGI after SGI acquired 
MIPS in 1992. 
While at SGI, Tarolli architected the graphics chip and software for the IRIS 
Indigo, including the system’s Starter Graphics, which used the Indigo’s MIPS CPU 
for geometry processing as a cost reduction. While at SGI, Sellers and Tarolli also 
helped develop SGI’s IrisVision PC AIB, a high-end 3D graphics board based on 
SGI graphics technology. 
Bharat Sastri was the director of engineering for IrisVision. SGI decided not to 
enter the PC market, and Sastri left SGI in September 1991. 
In November 1991, Gary Tarolli, Scott Sellers, and Herb Kuta (an SGI cofounder, 
and was involved in the design of the pipelined Geometry Engine) also left SGI to 
join Sastri in starting Pellucid, and Ross Smith joined them soon afterward. 
A short time later, not knowing what to do with IrisVision, SGI licensed it to 
Pellucid. IrisVision was a Gouraud shading, 3D-only, 10 k polygon/sec AIB for CAD 
applications (AutoCAD and Intergraph) that supported DOS and Windows PCs with 
ISA buses and IBM’s Micro Channel architecture. The AIB sold for $4,000, more 
than the price of a PC. 
In 1993, Media Vision bought Pellucid, and Sellers, Tarolli, and Smith went with 
the acquisition, as did Sastri for a period before he went on to Alliance Semiconductor. 
Sellers had previously been the principal engineer at Media Vision, a company that 
experienced ﬁnancial trouble in 1994 when Paul Jain, the founder, became embroiled 
in a massive ﬁnancial scandal (see the section on “Media Vision (1990–1994)” in 
this chapter).

6.3 3dfx Voodoo (1994–2000)
285
When Media Vision shut down in August 1994, some of the former Pellucid 
staff left to found 3dfx Interactive. Interestingly, other Pellucid employees, including 
David Schmenk, Dwight Diercks, Todd Miller, and others, joined Nvidia and became 
intimately involved in the rivalry with 3dfx that would deﬁne the 3D graphics market 
for some time. 
Also, while at MediaVision, the Pellucid team developed a true color 2D/VGA 
graphics ASIC and board for the VESA local bus called the ProGraphics-1024. At 
the time, it was the highest-performance 2D graphics board in the market. (refer to 
Fig. 6.22 to see the many paths that led to 3dfx.) 
In November 1995, 3dfx announced a technical cooperation agreement with 
Alliance Semiconductor to ensure compatibility with Alliance’s ProMotion video 
processor. However, the ultimate goal was to leverage Alliance’s high-performance 
DRAM and 3dfx’s new 3D technology to enable a new generation of video games 
compatible with PCs, high-end game enthusiast graphics boards, and arcade-level 
systems. The two companies had also planned future development efforts. 
“We believe 3dfx Interactive will set a new standard for high-performance 3D 
entertainment,” said Bharat Sastri, vice president of systems engineering at Alliance. 
“This is exactly the type of relationship we look for—one that integrates the strengths 
of the partners to enable a dramatic advancement in features and performance [10].” 
In late 1995, Fujitsu announced it would use 3dfx in a new series of gaming PCs 
it was planning to bring out. Several AIB suppliers such as Orchid and Diamond also 
announced plans to use 3dfx’s chipset during the same period. 
The team started 3dfx with backing from Gordon (Gordie) A. Campbell’s Tech-
Farm. In an interview, Campell would remember the encounter as one of the worst 
meetings he ever had. Campbell asked Smith, who was fresh from a bad experience 
at Exponential Technology, which attempted to build microprocessors for Apple 
clones, “It is clear you do not want to work at another microprocessor company, so 
what do you really want to do [11]?” Smith told him that he and two others wanted 
to start a PC 3D graphics company. Campbell liked the idea and agreed to meet 
the other members at the Tide House, a popular bar in Silicon Valley. After what 
the founders described as an animated conversation and many beers, they struck a 
deal. Later, with Campbell’s backing, the company raised $5.5 million in 1995 from 
venture capitalists. 
The original 3dfx business plan was to develop a graphics hardware and software 
architecture and platform that would connect the coin-op/arcade world with home 
PCs. The thinking was that many hit games, such as Sega’s Daytona and Virtua 
Fighter, Capcom’s Street Fighter, Midway’s Mortal Kombat, and Namco’s Tekken, 
originated in the coin-op world and were only available in the home only on consoles 
such as Nintendo 64. With a common architecture, coin-op games could readily come 
home on 3dfx-enabled PCs, and the home market would push coin-op companies to 
employ 3dfx in the arcade, resulting in a virtuous circle that would drive games and 
3dfx graphics sales.

286
6
1996–1999, Graphics Controllers on PCs
In March 1996, the company announced the launch of System3D, a customizable, 
low-cost, scalable system that was designed to power the next generation of texture-
mapped 3D arcade games. System3D was based on 3dfx’s new Obsidian 3D graphics 
board for coin-op/location-based entertainment (LBE) games. 
In the same month, the company secured an even more signiﬁcant investment of 
$11.6 million and shortly afterward signed a contract with Micronics Computers, 
which owned the Orchid brand. 3dfx was to develop an arcade video game plat-
form based on their add-in boards (AIBs). The company had launched a few minor 
offerings, but the bombshell that would rock the industry was yet to come [12]. 
In the fourth quarter of 1996, two signiﬁcant events occurred in the PC market. 
First, Intel revealed that it would be introducing its SIMD instruction set MMX. The 
name caused multiple suits with AMD (had many court battles with Intel, produced 
marketing material from Intel indicating that MMX stood for Matrix Math Exten-
sions [13]) and Cyrix (who co-opted the name and used it on their 6 × 86). Intel 
added MMX to its multimedia applications, including games and initial tests, which 
improved using conventional VGA controllers. Intel ofﬁcially announced the MMX 
Pentium in March of 1997 [14]. 
Also, in November 1996, 3dfx ofﬁcially released its Voodoo Graphics chipset. 
Voodoo was a 3D-only AIB that required an external VGA chip. VGA data for 
PC bootup and 2D graphics were input to the Voodoo AIB using a pass-through 
cable to feed the VGA information to the display. That was the same approach used 
with IrisVision. The reason for eliminating the VGA from the Voodoo Graphics 
architecture was twofold. First, the 3dfx team did not have VGA expertise, so they 
would have to license a VGA core, which added IP cost, and would take up silicon. 
Second, and more importantly, with a VGA, Voodoo Graphics would be measured 
on 2D performance and compatibility with productivity apps, which was dilutive to 
the mission of being a graphics board for 3D games. The 3dfx team felt that if they 
had suitable games, its core audience would not care about 2D and VGA capabilities 
since every PC came with it intrinsically. The company y would later change that 
attitude and put 2D on their AIBs. 
The new 3dfx Voodoo AIB increased the frame rate of 3D games from 10 fps to 
30 or 60 fps with just a CPU (with MMX) and a VGA AIB. At higher resolutions, 
the performance gain was even more signiﬁcant, reaching up to 300%. All of that 
came with higher-quality graphics such as perspective-correct rendering, a precision 
w-buffer, and “trilinear” ﬁltered textures that employed dithering as a means of 
minimizing mipmap artifacts. That was unimagined at the time, changed the market 
forever, and overshadowed the notion that the CPU represented the future of polygon-
based applications such as games. The era of 3D accelerators had begun. 
3dfx changed the landscape of the PC gaming market. 
Voodoo was the ﬁrst PC-based 3D accelerator to provide proper z-buffering, texture 
ﬁltering, and accelerated rendering. At the time, other PC-based 3D graphics attempts 
became decelerators because they ran so slowly. Voodoo delivered a textured ﬁll 
rate of 40+ Mpixel/s, which was workstation-class 3D graphics performance (for

6.3 3dfx Voodoo (1994–2000)
287
comparison, an SGI O2 workstation, introduced in 1996, produced 30–40 Mpixel/s 
textured ﬁll). 
The company had been relatively quiet for two years before it released Voodoo, and 
dozens of speculative stories had sprung up around it, adding to the mystery, myth, 
and anticipation. Although the company ended up primarily being a PC graphics 
chip supplier to AIB suppliers, it started as a board supplier for arcade machines. 
The New York City-based Innovative Concepts in Entertainment (ICE) was one of 
the ﬁrst (if not the ﬁrst) to use 3dfx’s parts. 
Before announcing the Voodoo chipset, Smith, Tarolli, and Sellers showed their 
SST-1 graphics engine plans for 3D acceleration to various arcade, PC game devel-
opers, and AIB companies. SST was an acronym for Sellers, Smith, and Tarolli. But 
they also liked the connotation of fast, i.e., supersonic transport (SST). 
The founders of 3dfx had been at SGI during the period when the company 
developed the ﬁrst 3D gaming chip for a console, the Nintendo 64, released in 1996. 
However, SGI was going through turmoil and chose not to enter the gaming industry. 
That decision caused other chip designers to leave, and they started ArtX (which 
was later acquired by ATI). In contrast, others went to Nvidia following a patent suit 
settlement (see the section on Nvidia and the earlier chapter, Graphics controllers on 
PCs 1980–1989). 
An interesting event, formative in 3dfx’s early history, was a meeting Smith and 
Herb Kuta’s had while at Media Vision with John Pasierb from American Laser 
Games. Pasierb indicated that the coin-op game market was moving to 3D because of 
Sega’s new 3D System (developed by Lockheed Martin), coupled with the Nintendo 
64 and Sony PlayStation in the home. According to Pasierb, the rest of the coin-op 
market needed a platform. At his invitation, Smith and Kuta attended the Amuse-
ment & Music Operators Association (AMOA) coin-op trade show in Long Beach. 
They saw the buzz around Sega’s Daytona and Virtua Fighter, along with the rela-
tively primitive 3D graphics available from the platform. Knowing what a PC and 
an innovative 3D architecture could do, the genesis for 3dfx was begun. 
3dfx had the SST-1 GPU manufactured using TSMC’s 600 nm production process, 
with a die size of 135 mm2 and a transistor count of one million. It supported DirectX 
5.0 and featured a one-pixel shader, one texture mapping unit, and one ROP. There 
were no vertex shaders, and the design relied on the CPU for T&L and triangle setup. 
However, the frame-buffer interface (FBI) and the texture mapping engine (TREX) 
did a partial triangle setup. The CPU still did the transformations, but the FBI/TREX 
did a signiﬁcant amount of work. The organization of the chips is shown in Fig. 6.15.
As a result, Voodoo could achieve triangle throughput rates that were much higher 
than others at the time. The company showed a demo of Voodoo1 rendering over 1 m 
triangles/sec at the Game Developers Conference, a very cool demo of a skeleton 
with beautiful rendering in great detail sustaining a million triangles a second on a 
Pentium processor.

288
6
1996–1999, Graphics Controllers on PCs
Fig. 6.15 The basic 3dfx graphics engine
In its entry-level conﬁguration, an SST-1 graphics solution consisted of two 
rendering ASICs: TREX (texture-mapping engine) and FBI (frame buffer inter-
face), refer to Fig. 6.15. Both supported various memory types, including stan-
dard, extended data out (EDO), and synchronous DRAM to provide a wide range of 
price/performance options. 
FBI served as a PCI slave device, and all communication from the host CPU 
to the SST-1 graphics subsystem came via the FBI. It implemented basic 3D primi-
tives, including Gouraud shading, alpha blending, depth buffering, and dithering. The 
following diagram (based on the SST-1 Graphics Engine for 3D Game Acceleration 
Manual [15]) shows the organization of FBI and TREX. 
The FBI had a PCI interface and a private bus to the TREX, which performed 
texture mapping, perspective correction, and bilinear ﬁltering, and had four memory 
channels used to examine the four nearest neighbors and apply a weighting value. 
An AIB could use up to three TREX chips for trilinear ﬁltering and multiple textures 
at no performance cost, while the Diamond board featured two TEX chips. 
First graphics chips to deliver full-speed trilinear ﬁltering and dual-texturing. 
The texture chip was also known as a texture mapping unit (TMU). Voodoo1 
supported three TMUs, although, for many arcade systems, it shipped with two. 
Voodoo (and Voodoo2) were the ﬁrst graphics chips to deliver full-speed trilinear 
ﬁltering and dual-texturing when equipped with two TMUs, games such as Quake 
and Unreal heavily leveraged those features.

6.3 3dfx Voodoo (1994–2000)
289
Memory bandwidth was a critical issue that affected the creation of a 3D engine. 
Sellers had been responsible for several innovative memory designs at SGI and 
Pellucid and invented a six-memory interleaved system for the engine. Two inter-
leaves were used in the frame buffer controller (one for the z-buffer and one for the 
color), while the remaining four allowed the TREX chip to perform bilinear ﬁltering. 
The FBI had two-way interleaving in TREX 4. 
“One of the cool things about this interleave was that there was no penalty for z-
buffering or alpha-blending. Other 3D accelerators at the time took big performance 
hits when turning on z-buffering, and furthermore, performance hits when alpha 
blending,” said Scott Sellers. “With the two-way interleave that we created, we could 
peak at 50 megapixels (Mpix) a second, and sustain 40 megapixels per second fully 
z-buffered and alpha-blended [11].” 
First multi-AIB rendering system for PC. 
Another one of the innovative features of that design was its ability to gang entire 
boards together, which became known as scan-line interleaving (SLI). That also 
made it possible to increase the maximum screen resolution from 800 × 600 to 1024 
× 768 (Fig. 6.16). 
Both graphics boards had a full copy of the texture set, and each had half the 
frame buffer (color and z). The scan lines were simply alternated to double the ﬁll 
rate. 
“Our design approach was similar to SGI’s Reality Engine,” added Sellers. “We 
did not want to force developers to make tradeoffs. And at the time, many developers 
still did not want to use z-buffering because it was so expensive, requiring extra 
memory. We changed that perception with Voodoo since z-buffering came for free 
(no perf loss), making life much easier on the developers [11].”
Fig. 6.16 3dfx developed scan-line interleaving in 1995 (Courtesy of Martín Gamero Prieto) 

290
6
1996–1999, Graphics Controllers on PCs
Although the marketing department had said that SLI was unnecessary, the engi-
neers put it in anyway, and it turned out to be one of the board’s most important 
and popular features. The ribbon cable (seen in the photo) at the top of the AIBs 
connected them, and the black VGA cable from the smaller AIB was how the 3dfx 
AIB(s) incorporated 2D video for DOS and Windows apps other than games. There 
was an electro-mechanical relay on the Voodoo AIB, and it could be heard clicking 
when switching from DOS/Windows operations to 3D gaming. Smith was asked if 
that was a technological embarrassment (i.e., using relays in the age of VLSI semi-
conductors). “No,” he said, smirking. “That was a feature. That signaled the fun was 
about to begin—gaming [11].” 
6.3.1 
SLI Was Not a New Concept 
The addition of processors and a frame buffer to share workloads to speed up 
rendering dates to the late 1970s and early 1980s. The frame buffer was divided 
in an interleaved fashion. Every nth pixel in a row or every eighth pixel in a column 
got assigned to the same memory. A geometric primitive such as a polygon or line 
segment fell into many frame buffer sections and processed relevant elements [16]. 
In 1979, Henry Fuchs and Brian Johnson developed a similar scheme using 8-bit 
microprocessors [17]. 
Jim Clark and Mark Hannah suggested a variation of that idea in which they 
added eight processors that were responsible for all the processors in every eighth 
row. Those higher-level processors could perform the setup calculations, meaning 
that the rendering primitives had lower start-up costs [18]. 
In 1981, Satish Gupta, Bob Sproull, and Ivan Sutherland developed a workstation 
display system in which an interleaved 64-piece frame buffer with optimized proces-
sors moved the display data. Each processor was connected to its eight neighbors 
and three others further away [19]. 
Although there were a variety of multichip graphics systems (and SGI deployed 
many of them), Tony Tamasi, who was at 3dfx and became a VP at Nvidia, said 
during our interview [20], “As far as I’m aware, Voodoo was the ﬁrst to productize 
that for a personal computer. (Although it was only productized for the mainstream 
consumer with Voodoo2 and for coin-op/LBE and ultimately professional simulation 
and training applications on the Obsidian Graphics Boards sold by the company and 
its spin-off, Quantum3D).” 
6.3.2 
The SST-1 
The diagram in Fig. 6.17 illustrates the abstract rendering engine of the SST-1 
graphics subsystem. The rendering engine was a pipeline through which passed

6.3 3dfx Voodoo (1994–2000)
291
each screen pixel drawn. The individual stages of the pipeline modiﬁed the pixels or 
made decisions about them. 
The SST-1 used an asynchronous FIFO 64 entries deep, which allowed for sufﬁ-
cient write posting capabilities for high performance. The FIFO was asynchronous 
with respect to the graphics engine, thus allowing the memory interface to operate at 
maximum frequency regardless of the frequency of the PCI interface. For maximum 
bus bandwidth, it incorporated zero wait-state writes. The FIFO: SST-1 memory was 
also able to use off-screen frame buffer memory, which increased the effective depth 
of the PCI interface FIFO. 
Even though it did not sound that interesting, it was a novel concept at the time. 
Other 3D accelerators had some concept of putting display lists in memory, but those 
were expensive to set up and not particularly friendly toward how developers wanted 
to develop games. What 3dfx called the memory FIFO allowed the CPU to continue 
to dump commands into FBI and keep working, which allowed the CPU to be much 
more efﬁcient. Most other 3D accelerators had shallow PCI FIFOs. As a result, any 
reasonably large triangle that would take some time to render would back up the 
rendering engine and ultimately back up the PCI bus, causing the CPU to halt and 
wait for the PCI FIFO to drain. 3dfx got around those problems by automatically
Fig. 6.17 Block diagram of 3dfx’s frame buffer interface chip 

292
6
1996–1999, Graphics Controllers on PCs
having the FBI spill the PCI FIFO into an unused portion of the EDO frame buffer 
memory. All that happened automatically without the CPU or the game developers 
even knowing it was happening. 
The SST-1 graphics engine from 3dfx Interactive was the ﬁrst video subsystem 
that enabled PCs and low-cost video game platforms to host 3D entertainment appli-
cations. Optimized for real-time texture-mapped 3D games and educational titles, 
SST-1 provided acceleration for advanced 3D features, including true-perspective 
texture mapping with trilinear mipmapping and lighting, detail and projected texture 
mapping, texture anti-aliasing, and high-precision subpixel correction. However, the 
SST-1 was not for everybody; some found that bilinear ﬁltering made the images 
blurry, while others were annoyed by the gamma correction. Voodoo2 used a color 
lookup table for gamma correction, which relied on 16-bit dithered color data from 
the frame buffer as an index. The 24-bit output of the gamma-correction color table 
got sent to the monitor. (You can see an example of the bilinear ﬁltering creating a 
blurriness in Book three, What is a GPU? The GPU’s Functions, Bilinear upscaling 
ﬁlters out most of the raster jaggies ﬁgure.) 
In an interview with Scott Seller [21], he said, “Most PC 3D accelerators at the time 
did not do a proper perspective correction. Rendition is the only one I recall that did 
at that time. That is why texture mapping looked so crappy on most 3D accelerators 
at the time. Gary and I ﬁgured out a very efﬁcient way to do the division required for 
perspective correction using table lookups and Newton–Raphson calculations. That 
was a critical enabler that allowed for perspective correction done without using a 
lot of transistors. And the division per-pixel could be pipelined and sustained full 
50 m pixels/sec.” 
The ﬁrst PC implementation of the chipset was on the Voodoo1 AIBs offered by 
several board manufacturers (see Fig. 6.18). The Voodoo AIB sold for $300 in 1996 
from most suppliers. 
Fig. 6.18 The Voodoo1 from 3dfx, released in 1996 (Courtesy of Wikipedia)

6.3 3dfx Voodoo (1994–2000)
293
Voodoo Graphics was close to instant success. Gaming using 3D was just getting 
started, and the patched 2D AIBs on the market left a lot to be desired. The Voodoo 
also offered state-of-the-art triangle-shading and graphics features, the type previ-
ously found only in a workstation, showing the founders’ backgrounds (in this sense, 
shading refers to the coloring of the triangle rather than processing, as in a shader). 
The 3dfx founders saw themselves as an entertainment company and did a lot 
more marketing than any other chip company or start-up before them. That included 
evangelizing game developers and giving them C-Simulators and even SGI RE2 
emulators they could use to develop applications before the availability of hardware. 
Smith had been at pre-IPO MIPS, where he had seen ﬁrsthand the impact a good 
developer relations program could produce. As a result, 3dfx had compelling content 
when the ﬁrst boards came out. They also invested in impressive demos to show off 
their features. 
There is a widely repeated story that may be true that during a show-and-tell 
conference held by the former investment banking ﬁrm Hambrecht and Quist, SGI 
was in the ballroom showing off their latest $100,000 workstation. Meanwhile, in 
the basement, 3dfx presented their boards with a demo of the same quality. (see 
Fig. 6.19.) Word spread throughout the conference, and 3dfx soon had more people 
crammed into their tiny space than SGI had (according to Campbell). 
Voodoo Graphics came out about a year before Microsoft Direct3D, and Direct3D 
was limited to Windows. 3dfx knew that DOS support was essential for PC games
Fig. 6.19 Interstate ’76, released in 1997 by Activision, running on a Voodoo 1 (Courtesy of 
Wikipedia) 

294
6
1996–1999, Graphics Controllers on PCs
and that embedded OS support was necessary for arcade systems. So 3dfx devel-
oped a simple low-level application program interface (API) rasterization library 
called Glide, which included the complete feature set of the Voodoo graphics hard-
ware. With that API, programmers could reach the hardware’s full potential under 
DOS, Windows, or embedded operating systems. And because of its easy-to-use and 
straightforward nature, Glide appealed to both PC and coin-op game developers. 
Glide also ran on Windows, albeit in full-screen-only mode. 
Glide was not originally in the 3dfx plan. Initially, 3dfx approached Dev Bose at 
Intel about licensing Intel’s 3D API, 3DR, but Intel insisted that 3DR be licensed 
only for ×86/Windows platforms, and the 3dfx team felt that DOS was still an 
essential requirement to succeed in the PC game market. Also, the coin-op market 
used PowerPC and MIPS CPUs, so 3dfx needed an API that could port to other 
architectures and OS. 
OpenGL was not viable as an API since a critical provision of the OpenGL license 
was that licensees’ platforms must pass the OpenGL Compliance Test. Since the 
Voodoo architecture focused on games, not CAD, 3dfx did not include features 
required for OpenGL compliance. That saved silicon and reduced testing and Q/A for 
features that would not beneﬁt games. At 3dfx, new features proposed got challenged, 
“will this feature help video games and reduce cost?” If the feature did not beneﬁt 
games (or reduce costs), it got scratched from the list of initially supported features. 
So, CAD features such as line engines and stippled lines got dropped—in some cases 
with a certain amount of glee (because of the consternation those features caused 
for implementation). At one point early on, Tarolli suggested the company be called 
“NoCAD.” 
Tarolli had developed the concepts for Glide before founding 3dfx since SGI 
had a similar pipeline and since 3dfx needed a software renderer to determine which 
features it needed for optimal video game rendering. 3dfx developed Glide to support 
SGI IRIX, DOS, and ultimately Windows. Glide got ported to several embedded 
operating systems for the coin-op market. Brian Hook (employee number ﬁve at 
3dfx) contributed to Glide, although a year later, he would leave 3dfx for id Software, 
joining John Carmack and Michael Abrash, the developers of Doom, one of the most 
popular games of all time. Later, when Carmack was on the technical advisory board 
of 3dfx, he saw that 3dfx hardware could achieve everything he was implementing in 
software. Other advisory board members were Tim Sweeney from Epic, Tom Porter 
from Pixar, and Ken Perlin from NYU. 
“Brian was amazingly talented. He wrote the entire original Glide spec over a 
weekend,” said Sellers [11]. 
The Obsidian board proved to be good enough for use as a coin-op/LBE AIB and 
as the graphics engine for an image generator for professional ﬂight simulators. In 
late March 1997, with Gordon Campbell’s blessing and backing, 3dfx spun off the 
advanced group and formed Quantum3D. 
Smith accompanied the spin-off, whose mission was to develop and market 
the 3dfx Obsidian boards to the coin-op/LBE and simulation markets as a dedi-
cated customer of 3dfx. Quantum3D built extreme graphics products enabled by the 
3dfx architecture. Quantum3D also developed turnkey PC-based coin-op platforms

6.3 3dfx Voodoo (1994–2000)
295
(Quicksilver and Graphite) and Image Generators (Mercury, AAlchemy, and Inde-
pendence). Quantum3D products were widely employed by coin-op and gambling 
companies such as Midway Atari (Hydrothunder), Konami (Dance Revolution), 
Incredible Technologies (Golden Tee Golf ), and several military simulators and 
training systems, including the F-15E, UK-CATT, and others [22]. The UK-CATT 
program was the most extensive combined arms training system globally. It reﬂected 
a watershed moment in the training and simulation market when PC-based image 
generators, fostered by Quantum3D and 3dfx, essentially took over the market. 
Ross Smith left 3dfx to run Quantum3D in 1997 and continued to do so until 2008 
when the company changed its business model and moved into simulation software 
and systems integration. 
In April 1997, 3dfx introduced Voodoo Rush, designed for integration with 
existing 2D/VGA ASICS, and essentially as a precursor to Banshee. Designed to 
address PC consumers who needed both a 2D and a 3D gaming solution, Voodoo 
Rush did a marginal job of both, which resulted in the product being a failure in 
the market. With Voodoo Rush, the frame buffer was halved and shared between the 
3D and 2D chips. That limited the 3D display resolution to around 512 × 384. The 
AIB also had lower refresh rates since the Alliance and Macronix chips were limited 
to 175 and 160 MHz LUT-DAC. Essentially Voodoo Rush reinforced that 3dfx’s 
original notions on integrating 2D/VGA into Voodoo Graphics were accurate. 
But 3dfx suffered another setback. In March 1997, 3dfx was awarded a contract 
from Sega for developing a chipset (based on Voodoo Graphics) for the new Dream-
cast game console. Sega invested nearly $2 million in the development of that chipset. 
However, in July 1997, Sega abruptly dropped the project, and 3dfx became aware for 
the ﬁrst time that its chipset had been deselected in favor of an NEC device (based on 
Imagination Technologies’ PowerVR IP). 3dfx ﬁled a $155 million lawsuit against 
Sega Enterprises Ltd., Sega of America, and NEC, claiming that Sega and NEC had 
interfered with the 3dfx contract and that Sega and NEC had misappropriated 3dfx 
IP. A year later, they reached an out-of-court settlement for $10.5 million, although 
3dfx claimed it had lost $55 million in revenue due to the lost contract [23]. 
During the summer of 1997, 3dfx announced its initial public offering, raised 
$33 million, and formally launched Voodoo Rush, which represented an attempt to 
produce a single AIB with 2D and 3D capability. The AIB’s SST-1 chip handled 
games based on 3dfx’s Glide API, while a companion Alliance or Macronix chip 
handled 2D applications and other older 3D games. However, the 3dfx chip/memory 
ran at 50 MHz, while the Alliance AT25 chip ran at 72 MHz, resulting in artifacts 
appearing in the display. 
In February 1998, 3dfx launched Voodoo2, the company’s follow-on to the orig-
inal Voodoo Graphics. Voodoo2, like Voodoo Graphics, was a 3D-only graphics 
subsystem that provided a substantial step up from Voodoo Graphics in rendering 
quality and performance. Voodoo2 was a huge success. That was the pinnacle of 
3dfx, when the most popular 3D games, including id Software’s Quake, required 
Voodoo2-based graphics boards for optimum performance (Fig. 6.20).
Also, in early 1998, 3dfx began a new graphics chip project called Rampage, 
a 2D/3D graphics processor. However, although they were several years ahead of

296
6
1996–1999, Graphics Controllers on PCs
Fig. 6.20 Comparison of 3dfx performance to VGA (Courtesy of 3dfx)
the competition, the design to be used in a new AIB would take over two years to 
develop. To accelerate the development, in the summer of 1998, 3dfx set up an ofﬁce 
in Austin, Texas, and tasked software and hardware teams with developing 2D and 
3D Windows device drivers for Rampage. After hearing about the work that was 
taking place at Nvidia and ATI, the 3dfx hardware team in Austin began developing 
transform and lighting (T&L) engines and MPEG CODEC designs; unfortunately, 
they would not be ready in time to meet the competition in the market. 
In 1998, Bharat Sastri left Alliance to join Quantum3D as its president and CEO. 
In February 1998, 3dfx and Quantum3D issued a joint announcement of their plans 
to further develop the arcade market and visualization. As a result of that agreement, 
the two companies would collaborate on development and marketing and would 
cross-license products where applicable. Quantum3D’s President and CEO, Bharat 
Sastri, said the deal would enable Quantum3D to do what it did best, “adding unique 
value to 3Dfx technologies, and integrating them into advanced graphics subsystems 
and PC-based real-time 3D systems, that are unmatched in performance and value 
[24].” 
3dfx ﬁled suit against Nvidia for patent infringement in September 1998, claiming 
that Nvidia was using 3dfx Interactive’s patented multi-texturing technology in their 
Riva TNT. 
Despite that, in November 1998, Creative Labs and its subsidiary CTI Ltd., which 
had been using 3dfx’s chips, bought 6.5% of 3dfx Interactive, thus giving the company 
a war chest of cash. Then, when Creative announced its uniﬁed driver technology, 
which enabled gamers to take advantage of Glide technology on Creative’s Nvidia-
based Riva TNT boards, 3dfx sued its shareholder Creative. 
In December 1998, 3dfx surprised the industry and announced it would acquire 
Richardson, Texas-based AIB builder STB Systems—a move that would mark the

6.3 3dfx Voodoo (1994–2000)
297
beginning of the end for the company. 3dfx announced that it intended to expand its 
role as a graphic chip maker in a stock deal valued at $141 million. 
3dfx’s desire to get into the consumer board business was why Ross Smith left 
3dfx and started Quantum3D. Smith said, 
“I was adamantly against doing this getting into consumer boards,” said Smith 
during our interview. “But the volume and revenues of consumer graphics board 
space was too alluring. The management team overlooked the fact that the market 
required superb execution and deep pockets for inventory, etc. I argued we were a 
boutique gaming company with a great following, and this move would dilute our 
brand and our mission. I presented a biz plan where 3dfx would sell high-end boards 
(Obsidians) and Gaming PCs (think of Falcon NW, Alienware, etc.) through our 
own channel, and we would sell silicon to graphics AIB makers for the volume biz, 
all while working with game developers to publish games for our platforms. Upon 
reﬂection, this is the exact model that Nvidia adopted with Quadro and GeForce 
products. With two years of missed product introductions, the rest was history”. 
STB Systems was a leading AIB supplier in the late 1990s and had begun using the 
Voodoo chipset for its boards. The company also had a world-class board fabrication 
plant in Mexico. In mid-1998, 3dfx introduced its Voodoo Banshee, a single chip 
solution, claiming it was the equal of Voodoo2 except for multi-textures but with 
superior 2D performance. The dynamic environment effects, bump mapping, or other 
techniques requiring multi-texturing accomplished in a single pass on the Voodoo2 
required a dual-pass approach on the Voodoo Banshee. The AIB sold for $380. 
In March 1999, Quantum 3D closed a mezzanine funding round with TechFarm, 
West Coast Ventures, Charter Capital Ventures, Chase Ventures, and Interactive 
investments. 
3dfx heard Nvidia was working on T&L and launched a project to build a T&L 
engine as a separate chip. An AGP bridge chip did T&L. Two AGP ports (connected 
to the primary AGP bus, to the Rampage rendering chip). The idea was that 3dfx could 
sell lower-cost Rampage AIBs without the T&L chip and had higher-end offerings 
with full hardware T&L using the T&L engine being designed. 
3dfx was now routinely slipping product schedules, a process that had started 
with the Voodoo2. The company had innovated with the original Voodoo and then 
simply made faster versions of the same thing repeatedly, while its competitors were 
investing in R&D and moving forward. Although 3dfx was also investing in R&D, the 
company missed two annual product cycles, which were pivotal in the PC graphics 
market, and was too late with Rampage. 
The company struggled to build a single chip solution (2D, 3D, and VGA), which 
Banshee was. They were late to start the Banshee project (too distracted by other 
markets such as arcade instead of focusing on PC OEM requirements), and the 
project was late to market. That began the tailspin of 3dfx. There are many stories 
of “if only.” However, if Banshee had been delivered on time, 3dfx would have been 
in a much stronger position. Banshee was the one that hurt them and caused the 
company (arguably in desperation) to acquire STB, which was the death knell for 
the company (Fig. 6.21).

298
6
1996–1999, Graphics Controllers on PCs
Fig. 6.21 The last AIB from 3dfx, the Voodoo5 5500 (Courtesy of Wikipedia, Konstantin Lanzet) 
Fig. 6.22 The history of 3dfx

6.3 3dfx Voodoo (1994–2000)
299
On March 28, 2000, in an attempt to catch up, 3dfx bought GigaPixel, a small IP 
company started in 1997 by SGI engineers. 
GigaPixel had almost won the contract to supply chips to Microsoft for the original 
Xbox (Nvidia eventually won the graphics controller design). GigaPixel had a signed 
contract with Microsoft, and GigaPixel engineers had moved into the Microsoft 
development ofﬁces to co-develop the Xbox chip. It was speculated Microsoft used 
the contract with GigaPixel to get better terms with Nvidia. With the termination of 
the contract, Microsoft paid a breakup fee (millions of dollars) to GigaPixel. 
Regardless of the loss of the Microsoft deal, 3dfx went ahead and bought the 
company with 15.6 million common shares, which were worth about $186 million. 
Ben Zhu, who was an engineer at SGI came up with the idea of GigaPixel. Ben 
Garlick and Alex Minkin, also SGI alumni, were founders. George Haber joined 
with his venture capital contacts. The design was not based on the initial ideas of 
tiling found in Talisman (Talisman is discussed in Sect. 6.6). Zhu ﬁgured out how 
to solve the Z-buffer sorting problem. However, the conceptual similarity between 
GigaPixel’s idea and Microsoft’s Talisman may have inﬂuenced Microsoft’s offering 
to Gigapixel. 
It was a very novel technology. It used on-chip tile rendering, like Imagination 
Technologies, but without all Imagination’s compatibility problems. 3dfx was devel-
oping a chip called Fusion that integrated the GigaPixel technology, planned for 
introduction after Rampage. 3dfx thought it would have been a true game-changer. 
It was very far along in development but was never taped out. 
Then, in August 2000, Nvidia sued 3dfx, claiming that it had infringed on ﬁve of 
their patents, and requested an injunction barring 3dfx from selling and distributing 
its Voodoo3, Voodoo4, and Voodoo5 products (Fig. 6.22). 
Things were not going well for 3dfx. They had trouble getting new chips out, and 
due to the acquisition of STB, they had lost almost all their AIB partners, meaning 
that revenue was falling. The company was out of cash and could not defend itself 
adequately against Nvidia’s suit. The PC industry started to implode due to the 
bursting of the internet bubble, and by the end of 2000, 3dfx sold its company’s 
assets to Nvidia for $120 million. 3dfx shut down its operations, laid off all its 
employees, and used the money from the sale to pay off its creditors (several of 
whom were Nvidia’s key suppliers). The companies struck a deal in which Nvidia 
paid 3dfx $70 million in cash and one million shares of its common stock and agreed 
to loan 3dfx $15 million to its creditors. Nvidia hired a signiﬁcant number (more than 
a hundred) of 3dfx’s laid-off employees. Since the world of 3D graphics employment 
was extremely small, many ex-SGI engineers were reunited at Nvidia. 
Rampage had taped out—the design was sent to the fab, and 3dfx was waiting to 
get the ﬁrst samples back from TSMC as the implosion happened. They got a few 
pieces from TSMC and were just getting them tested and brought up in the lab when 
the Nvidia sale was announced. 3dfx was very close to bringing Rampage to market. 
The Nvidia suit and associated expenses of defending against it were the problems. 
3dfx had too many people and a low-margin business they had acquired from STB. 
That sapped their cash, and they could not afford to invest as much as needed in 
future chip design.

300
6
1996–1999, Graphics Controllers on PCs
3dfx rose like a ﬁrecracker and burnt out like one in less than ﬁve years; however, 
its name and technology persist. The Computer History Museum held an oral history 
panel with Ross Smith, Scott Sellers, Gary Tarolli, and Gordon Campbell in 2014. 
Many of the people Nvidia hired from 3dfx were still working there over 20 years 
later, a fantastic retention rate that is very rare in the industry. 
There are numerous websites devoted to 3dfx, and many people still have working 
boards [25]. Several books have been written about the company, and it has become 
something of a cult celebrity [26]. The romantic passion is unique. 
6.4 Yamaha YGV612 RPA (1995–1996) 
A great design that lived less than two years. 
Yamaha was a developer of video display controllers or processors (VDP) in the 
late 1980s. The company made IBM-compatible CGA display controllers such as 
the YGY603. The company also made the VDP for the Sega Genesis video game 
console and the VDP for TI’s popular TI-99 calculator. Yamaha was best known for 
its audio signal processors. Still, the understanding of signal processing carries over 
to graphics, and in 1984, the company announced its 3D processor, the YGV611 
Rendering Polygon Accelerator (RPA). 
Yamaha Systems Technology’s (YST) YGV611 RPA was designed to offer low-
cost, high-speed rendering in 2D/3D for PCs or low-cost workstations. It featured 
high-speed short vector drawing, Gouraud shading, texture mapping, video capture, 
hidden surface removal, and BitBlt. The chip had a 16/32-bit host bus interface that 
could operate up to 33 MHz and a 128-bit (64 interleaved) memory interface, just 
the things needed for high-performance 2D and 3D graphics. The RPR supported up 
to 1280 × 1024 16-bit color. 
Yamaha ofﬁcially introduced the chip at Comdex in 1984. The company said the 
chip culminated ten years of research into 3D and IC technology [27]. 
The speciﬁcations included the following:
• 33 MHz clock frequency
• 550 K polygons/second with Gouraud shading
• 270 K polygons/second with Gouraud shading and texture map
• 32-bit color support
• 16-bit (DRAM or VRAM) z-buffer support with no performance penalty
• Dual-buffering at 32 bits
• Support for up to 32 Mbytes of VRAM
• Frame buffer and texture map both stored in VRAM memory
• 240-pin QFP. 
Although the chip was a 3D coprocessor, it did not have any VGA capabilities 
(see Fig. 6.23). While it did not have the perspective capability, the chip created the

6.4 Yamaha YGV612 RPA (1995–1996)
301
Fig. 6.23 Yamaha YGY611 block diagram 
illusion of perspective by manipulating the texture map. The host PC did the lighting 
and 3D transforms. 
Additionally, the chip offered the following:
• Hidden surface removal (z-buffer of 16 bits)
• Video capture function supported
• Short vector drawing up to 1600 Wsec
• BitBlt-T supported
• 1.6 million short vectors per second drawing
• 1.8 million character block transfers/sec (9 × 11)
• Low-power CMOS, 240 SQFP. 
The demos conducted by Henry Choy, Yamaha’s product manager, were impres-
sive. They included 3D lines, ﬂat-shaded polygons, Gouraud-shaded polygons, and 
Gouraud-shaded polygons with texture mapping. While the company did not provide 
actual performance numbers, the performance displayed was very high, considering 
it was an ISA-bus board. 
Pricing for the chip was under $100 ($250 in 2020 dollars) with a projected street 
price of $600 ($1,500 today) for a board with 2 Mbytes of VRAM. It makes today’s 
supercomputer AIBs seem cheap; plus, you needed a VGA AIB to go with it. 
At the time, Yamaha said they would bring out a second-generation chip that used 
DRAM to lower the board price to $300. 
Shortly after, Comdex ‘94, STB, Diamond, and Paradise announced they would 
build add-in boards (AIBs) using the chip. That drove the price down to $80, with 
production scheduled for the second quarter of 1995. 
The chip was an immediate hit. A few weeks later, at the GamesPC Consor-
tium meeting, Criterion, developer of the popular RenderWare 3D API and graphics 
rendering engine used in video games, took advantage of the chip to run its Cyber-
Street demo game. The company showed real-time video mapping on a rotating cube 
and wowed the audience [28].

302
6
1996–1999, Graphics Controllers on PCs
Yamaha said at the time that they were working on the chip’s next-generation 
version. It would support DRAM, have no video support, have an integrated LUT-
DAC, and display 640 × 480 maximum resolution. Choy said it would be available 
in the ﬁrst quarter of 1995 with an SRP of less than $60. 
The Yamaha chip simulated perspective by warping the texture map while the 
GLiNT chip calculated true perspective. For example, the demo was a street scene 
with building walls made of stone, much like a Doom scene. The Yamaha demo 
displayed straight lines as wavy, while the GLiNT displayed them as genuinely 
straight. 
Many people did not realize it at the time, but Yamaha had been in the 3D and 
graphics chip business for over ten years. Yamaha designed and manufactured Raster 
Technology’s graphics terminal systems in the 1980s. 
In February 1995, Microsoft announced it was working with Yamaha and 3Dlabs 
to develop 3D DDI drivers for Windows 95. Microsoft was emphatic about 3D AIB 
offering hardware texture mapping for games [29]. 
Yamaha kept its promises, and in May 1995, it announced the YGV612 DRAM 
version that was pin and software compatible with VRAM-based YGV611. It would 
sell for $40, and production would start in August [30]. 
“As the ﬁrst 3D silicon available to them, the YGV611 immediately attracted 
the attention of independent software developers at Fall Comdex,” stated Robert 
Starr, YST assistant general manager and director of sales and marketing. “Only two 
quarters after that, the YGV612 can satisfy the requirements of DOS and Windows 
95 game developers,” he concluded [30]. 
Yahama claimed the YGV612 could render 300,000 Gouraud shading poly-
gons/second and 150,000 texture-mapped 50-pixel polygons/second with its z-buffer 
capabilities, said Starr, “The chip can easily perform hidden surface removal. Like 
the 611.” The YGC612 supported resolutions up to 1280 × 1024 with 64 K colors. 
However, unlike the 611, the YGV612 used the PCI interface and the VL-bus 
(with glue logic). Yahama predicted AIBs would be in the $250 range. Once again, 
Western Digital adopted and used it in their Paradise Tasmania 3D AIB (Fig. 6.24).
The lack of a standard API was the weak link at the time. Western Digital and 
others did not expect a standard interface layer for 3D APIs or applications until the 
second half of 1996, with Direct3D from Microsoft fully deﬁned and a Direct3D 
hardware abstraction layer (HAL) in place. That meant that the AIB suppliers had to 
partner with software vendors and pick one that had the most supported games. All 
the AIB suppliers struggled to get some DOS games running by Christmas. After that, 
developers expected support from the DOS incarnations of RenderWare, BRender, 
and Reality Lab to bring additional titles. 
Western Digital decided to use the popular polygon rendering approach Yamaha 
was offering. An alternative rendering technique, quadratic texture mapping, was 
being promoted by Nvidia with their NV1 chip. 
Yamaha was a victim of the times too. By 1996, the PC market was at its peak. 
Venture capital shifted from hardware to software. The internet was the future, and 
if a company did not have an internet story to tell, it was just not interesting.

6.5 Real3D (1995–1999)
303
Fig. 6.24 Paradise’s YGY612-based Tasmania AIB (Courtesy of Vogonwiki)
Peaking in 1994, the U.S.’s GDP started sliding. Even though unemployment 
was going down, dozens of promising start-ups failed because they could not ﬁnd 
supporting partners, customers, or investors. In 1994 and 1995, the Fed raised short-
term interest rates by three percentage points. As long-term interest rates rose faster 
than short-term, employment growth fell from three percent to below two percent. 
Yamaha shifted its interest and investment and put its R&D money into new audio 
processors and Arm-based multimedia chips. There was no YGY613 or any other 
3D parts coming out of Yamaha after 1996. 
Yamaha was a pioneer in graphics chips beginning in the 1980s and a leader in 
3D. It was surprising and sad to see them leave the category. But taking the long 
view, their presence in the 3D chip market could be seen as a ﬂash in the pan, having 
been in the 3D market for less than two years. 
6.5 Real3D (1995–1999) 
Real3D came into the PC graphics market in 1995 with a strong history in simulation 
and game machines dating back to the late 1960s. Created from a division within 
Lockheed Martin that came from a research and engineering division originally part 
of GE Aerospace, it was best known for its Project Apollo Visual Docking Simulator, 
the ﬁrst full-color 3D computer-generated image system. 
In 1991, GE began looking for commercial adjacent market applications of its 
Compu-Scene real-time 3D graphics technology. Market surveys of the time fore-
casted an increasing demand for more realistic graphics in entertainment systems. 
That led GE to contact Sega of Japan because they had heard Sega was about to go on 
a two-year development internally to improve their polygon-based arcade graphics

304
6
1996–1999, Graphics Controllers on PCs
hardware (Model 1) and add texture mapping. GE suggested Sega could use GE’s 
proven real-time 3D graphics technology. Sega liked what they saw and heard, signed 
a deal with GE, and got to market 14 months early. 
Then in 1993, the GE group was sold to Martin Marietta for over 3 billion U.S. 
dollars. And in 1995, Martin Marietta and Lockheed merged and formed Lockheed 
Martin Corporation (LMC). 
GE Aerospace was producing components for the Sega Model 2 and Sega Model 3 
arcade system boards. The company was also working on Saturn 2 for Sega. However, 
Sega canceled that contract in favor of 3dfx in 1996. Sega then dropped 3dfx in favor 
of NEC/PowerVR in 1997, which got used in the Sega Dreamcast. 
With the Sega arcade deal locked in, the management of LMC decided in 1994 to 
leverage their technology and commercialize the developments the graphics group 
was pursuing. In January 1995, LMC established the Real3D group and formalized 
its relationship with Sega. That led to a successful product run with Real3D 3D 
hardware design shipped in the Sega Model2 and Model3 arcade systems. Sega 
broke records, and over 250,000 models 2 and 3 were sold, making them the most 
popular coin-op systems ever. 
LMC also had some discussions with Sega about the console market. Although the 
volumes looked promising, the margins were thin. The two companies did not think 
it would be a win–win situation like LMC had in the arcade market. LMC’s Real3D 
was hoping the corporate PC market would start using 3D graphics hardware. But 
they were designed for the gaming market (Fig. 6.25). 
The new unit planned to produce graphics chips for commercial training and 
engineering workstations and PCs. Gerald W. Stanley, a former Lockheed Martin 
executive and cofounder of Apollo Computer returned to the company to become 
president of Real3D. The company had 110 people. 
Starﬁghter introduced 
In March 1995, at Microsoft’s Windows Hardware Engineering Conference 
(WinHEC ’95), Real3D went public and demonstrated an AIB they named Starﬁghter 
[31]. The board had impressive speciﬁcations. It had a geometry processor and offered
Fig. 6.25 Gerald Stanley, 
Real3D’s founder, and CEO 
(Courtesy of Stanley) 

6.5 Real3D (1995–1999)
305
pixel write rates of up to 33 million pixels/second, up to 750,000 clipped and sten-
ciled polygons/second (25-pixel triangles), 24-bit depth buffer, up to 192 color texture 
maps (128 × 128 mipmapped) in real-time alpha blended, and line processing up to 
1.5 million/second [32]. 
The geometry processor consisted of a 32-bit ﬂoating-point processor rated at 100 
MFLOPS. The graphics processor provided hardware acceleration for points, lines, 
and polygons. It supported depth buffering, Gouraud shading, directed light source 
illumination, exponential fog, alpha testing, scissor and stipple masking, multi-pass 
anti-aliasing, and block read/write. The texture processor executed true perspective 
corrected texture mapping and mipmapped level-of-detail selection in hardware. 
The chipset supported up to 5 Mbytes of z-buffer memory and 512 Kbytes to 10 
Mbytes of frame buffer memory. Up to 8 Mbytes of memory could be for texture 
maps from 32 × 32 to 512 × 512 and 8 to 32 bits deep. The chipset used MoSys 
RAM and had a Bt485 LUT-DAC. It also could phase-lock to external 2D hardware 
accelerators and internally multiplexed incoming RGB video data before outputting 
to the DAC. 
GE/LMC/Real3D had invested over $200 million in research and development 
and had more than 200 related patents in the ﬁeld. 
Lockheed Martin’s R3D/100 chipset provided faster processing through a patented 
hardware design that incorporated geometry processing, rasterization, and texture 
mapping (see block diagram, Fig. 6.28). The texture processor applied mipmapped 
color texture to polygons in a true 3D corrected perspective. And the R3D/100’s 
ﬂoating-point geometry processor removed the processing burden from the host 
CPU. The AIB was a GPU design, but not a single integrated chip like Nvidia would 
produce (Fig. 6.26).
The R3D/100 chipset supported all 3D/DDI APIs such as OpenGL. Packaged in 
a 208-pin and 304-pin QFP, The two chips dissipated 4 W and ran at 3.3 V and 
33 MHz. Pricing was $142 for the 1024 × 768 16-bit version and $174 for the 1280 
× 1024 24-bit version. 
Project Auburn and the i740 
In May 1996, the company formed a partnership with Intel and Chips and Technolo-
gies to create an AIB for the Auburn project [33]. The project became a showcase for 
the AGP bus developed by Intel. Auburn came out in 1998 as the Intel740. However, 
it was not a successful product and was sold off a year later under the Starﬁghter and 
Lightspeed brand names. 
A month later, the company showed the Sega Model 3 engine running animated 
characters from the future version of Virtua Fighter 3, and the results were extraordi-
nary. Character animations were unusually ﬂuid, and unique subtleties such as facial 
expressions and eye movements made watching those ﬁghting characters almost a 
meditative experience. 
Sega Model 3 
Model 3 used a PowerPC 603C host CPU and two R3D/PRO-1000 chips. The 
design could achieve over one million perspective-correct, trilinear interpolated

306
6
1996–1999, Graphics Controllers on PCs
Fig. 6.26 LMC’s Real3D multichip Starﬁghter AIB
micro-textured polygons/second. The graphics/geometry engine supported specular 
Gouraud shading, texture edge anti-aliasing, fog, 32 levels of translucency, clipping, 
and level of detail, and Z-buffer on a non-interlaced, 640 × 480 (or 496 × 384) 
display. The audio subsystem used the same chip as the Sega Saturn. Sega indicated 
at the time that the Model 3 engine was for coin-op LBE installations, then after cost 
reductions in-game console boxes and PCs. 
6.5.1 
A Stand-Alone Company 
On January 19, 1998, Lockheed Martin Corporation announced it had spun off its 
Real3D division in Orlando, Florida, and had formed an independent corporation 
(Real3D, Inc.), with Lockheed Martin retaining majority ownership. The company 
also announced that Intel had purchased a 20% minority interest in Real3D [34]. 
Real3D was known for its co-development of the Intel740. It also designed the 
image processor used in the Sega 2 and 3 arcade games. The company had built 
simulation systems and stand-alone high-end graphics controllers (the Pro-1000) 
and began marketing the 3D laser image scanner RealScan 3D.

6.5 Real3D (1995–1999)
307
As part of its investment, Lockheed Martin transferred its intellectual property and 
patents related to 3D technology to Real3D. Real3D also maintained the agreement 
with Sega Enterprises for the development of arcade graphics chips and boards. 
The investment in Real3D, combined with Intel’s investment in 3Dfx, 3Dlabs, 
and the soon-to-be-acquired Chips and Technologies, demonstrated Intel’s massive 
interest in high-performance graphics. 
6.5.2 
Real3D and Silicon Graphics Settle Out of Court 
GE Aerospace had been deﬁning and inventing the computer graphics metaphor. 
They thought it was valuable and asked the U.S. government to give them patents 
on what they developed. The U.S. government, and other governments, agreed and 
awarded the patents. 
Years later (about 20 of them), GE Aerospace kept adding to their IP, and the 
portfolio ended up with Lockheed Martin. 
Meanwhile, back in the late ’70s, when Ivan Sutherland was teaching computer 
graphics at the University of Utah, one of his brighter students, a tall fellow named 
Jim Clark, came up with the concept that earned him a Ph.D. He thought it would 
be a good idea to build a dedicated ﬂoating-point engine as an ASIC. He called the 
thing a geometry engine. Clark then started Silicon Graphics in the early ’80s and 
continued patenting things. 
Then in the mid-’90s, Real3D’s president, Gerry Stanley, decided it was time to 
collect the rent and notiﬁed a few companies about Real3D’s IP portfolio (some of 
Real3D’s active IP went back to April 1986—“Incremental Terrain Image Genera-
tion”). In the autumn of 1995, Lockheed Martin ﬁled suit against SGI based on its 
patents on texture mapping. There was some dancing around, and later the ante was 
upped to include anti-aliasing, micro-textures, and terrain mapping. SGI countered 
by suing Real3D, claiming Real3D’s R3D/100 violated SGI’s IP. 
At the end of December 1997, Lockheed Martin spun Real3D out and formed 
Real3D, Inc. At that time, Intel also purchased a minority interest in Real3D. Lock-
heed Martin transferred most of its 3D graphics patent portfolio to Real3D. The 
company also inherited the ongoing litigation between Lockheed Martin and SGI. 
But neither company wanted to duke it out. Gerry Stanley gets the credit for taking 
the ﬁrst step toward ﬁnding an alternate solution and helping both companies crawl 
back from the brink. The suit was settled in principle in July 1998 (if it had not been, 
it would have gone to court in September). Three things came out of the settlement. 
Real3D and Silicon Graphics announced a broad strategic relationship that 
included a minority equity investment in Real3D by SGI. Lockheed Martin remained 
the majority owner of Real3D, while Intel and Silicon Graphics held minority inter-
ests. Silicon Graphics’ interest in Real3D represented slightly less than 10%. SGI’s 
investment in Real3D underwent a federal review under the Hart-Scott-Rodino Act 
and received approval.

308
6
1996–1999, Graphics Controllers on PCs
The two companies terminated all litigation and executed a patent cross-license 
covering 3D graphics in a separate agreement. Furthermore, the two companies said 
they would collaborate on future software technologies and initiatives. That gave 
both companies early access to each other’s technology initiatives. However, the two 
companies had no plans to codevelop any products. 
Real3D now had technology licensing and development agreements with Intel, 
Sega, Lockheed Martin, and Silicon Graphics and, according to Gerry Stanley, 
Real3D’s President, “We will aggressively pursue other licensing deals as well. 
The company had a large and valuable patent portfolio and extensive experience 
developing graphics products that ranged from PCs and workstations to arcades and 
simulators. We believe these types of licensing and co-development agreements will 
prove mutually beneﬁcial to both other companies and ourselves [35].” 
The agreement with SGI had no impact on Real3D’s relationship with Intel. Intel 
maintained a minority interest in Real3D, and the two companies continued to collab-
orate on PC graphics initiatives. Intel also had a technology licensing agreement with 
Real3D, giving them access to Real3D’s intellectual property and patent portfolio. 
But Intel did not get access or license to any of SGI’s IP (however, Intel and SGI had 
a different deal). 
One of the outcomes was that Real3D and SGI established cooperative marketing, 
technology, and business development arrangements. SGI would also assist in 
marketing Real3D’s content-creation hardware, such as RealScan 3D, and both 
companies said they would participate in several other strategic initiatives. Also, 
Real3D would have the opportunity to be a preferred supplier of graphics technology 
for SGI’s visual computing products. 
Like any announcement of that magnitude, it left as many questions as it answered. 
Would Real3D play a role in the Fahrenheit initiative of SGI and Microsoft because 
of that agreement? 
Protecting intellectual property and technology developments is critical to all 
companies, particularly those in the technology industry. It was counterproductive 
for companies to spend tremendous resources on litigation. Real3D was not inter-
ested in prolonged legal battles. The message they were trying to deliver was that a 
comprehensive graphics patent portfolio would be made available, but the company 
would also aggressively defend its innovations and inventions. 
Real3D was not shy about letting the world know it was rolling out an ofﬁcial 
licensing program. The good news was Stanley was a deal-man. He believed strategic 
partnerships with industry leaders deliver signiﬁcant advantages. Stanley said, “It 
was increasingly difﬁcult for companies to be successful without strong industry 
partnerships, and Real3D had demonstrated the value that strategic relationships 
bring to the marketplace and will continue to pursue relationships which advance the 
market for 3D graphics technology [35].”

6.5 Real3D (1995–1999)
309
6.5.3 
Intel Acquires Real3D (October 25, 1999) 
An epitaph. 
For a company that seemed to have all the horsepower in the world, Real3D could 
not get any traction. 
Real3D ofﬁcially started in January 1996, but the company’s history went back 
to the early 1960s. The experience and technology portfolio that was the basis 
for Real3D’s entry into the graphics market came from some of the world’s most 
respected and innovative companies. 
Real3D was in the ﬁnal stages of developing their integrated 3D-only chip, 
the R3D/100, initially designed for the Pro1000 simulator (and possibly the next-
generation Sega machine). The project started in 1996, and by late 1997, it was a 
stillborn ﬂop with no 2D and no integrated LUT-DAC. 
Now the company was in a bind—no graphics product and an antsy customer in 
Japan. In a brave move, the company launched itself into the AIB business using 
Intel’s i740 and some superior drivers. The AIB won countless awards, and almost 
no design wins. Still no traction. 
But Real3D was moving on several fronts at once, and one of them was the IP 
play. They had taken on SGI, the plan being to knock off the big guys ﬁrst. 
By then, the company’s next-generation 3D controller should have been in tape-
out, and it was not. And to add insult to injury, ATI, a signiﬁcant competitor wooed 
away most of the design team. The company was just not getting any footing; it was 
slipping backward, and it fell back to its main line of defense: the IP. They took on 
Sega and reached a very amicable settlement. 
But the writing was on the wall. And in addition, the parent company, Lockheed 
Martin, had its troubles and wanted to shed some of its businesses. But Gerry Stanley 
was not ﬁnished—not yet. He had seen a 3D camera a few years back and could also 
see dozens of essential applications for it, things no one had thought of that the 
camera could do. Fifteen years later, Apple would think of them. And Stanley could 
see how he could leverage Real3D’s staff, IP, and other resources to make it happen. 
And he did make it happen. Sales for the high-performance 3D camera began to grow 
into a few, then a few dozen, and then the promise of hundreds. It looked like the IP 
giant was ﬁnally going to prove itself. But Lockheed Martin was not looking and did 
not care. The word came down: sell it. 
Real3D invited the leading companies in the industry to look the camera over. 
3Dlabs, Nvidia, S3, a couple of others, and of course Intel and SGI. Hambrecht and 
Quist, a longtime helper in Real3D’s deals, also made a few calls to see if they could 
ﬁnd a buyer. There were also discussions of an MBO—all good plans and ideas. 
And in the meantime, the company had positive cash ﬂow and increasing sales on a 
high-margin, unique product. But Lockheed Martin had problems with its military 
projects and could not wait; word from on high came: sell it or close by year-end. It 
was not negotiable. 
In June, Lockheed Martin reported in its 8-K about the strategic and organizational 
review it had begun. As a result of that review, the corporation announced plans

310
6
1996–1999, Graphics Controllers on PCs
to realign its businesses, ﬂatten its management structure, reduce corporate staff, 
evaluate the divestiture of non-core operations, and enhance ﬁnancial management 
processes. The new organizational structure took effect October 1, 1999, D-day. 
The company assessed the possible divestiture of eight non-core business units. 
They employed about nine thousand people and had a combined estimated sales of 
approximately $1.4 billion (excluding $400 million in intercompany sales). Real3D 
certainly did not ﬁt in with what Lockheed Martin considered its core business. 
Lockheed Martin had been losing money all year ($128 million on sales of $12 
billion for the ﬁrst half of 1999) and was carrying a ton of debt ($9 billion). It was 
looking for cash and relief from operating costs. The company wanted to build F-22 
(Raptor) Stealth ﬁghter-bombers. 
The total cost of developing and building the F-22 was roughly $60 billion over the 
25-year life of the development and production program. The program was already 
in mid-development; by 2012, 341 operational ﬁghters would cost an estimated $41 
billion. The ﬂyaway cost of each F-22 would be approximately $167 million in 2022 
dollars. 
Opponents of the F-22 program suggested canceling it altogether and relying 
on existing F-15 ﬁghters to sustain America’s air superiority. Lockheed Martin had 
bigger ﬁsh to fry and more signiﬁcant problems to solve than a few measly megapixels 
per second. The divestiture of Real3D was put into high gear. 
Intel, to their credit, negotiated a right of ﬁrst refusal on any buyout deals for 
Real3D. It was prudent to protect itself from any marauding lawyers to buy IP and 
soak the violators (Intel made a similar arrangement when it invested in Imagi-
nation Technologies in October 2006). Intel was also committed to advancing the 
capabilities of the PC platform and had focused on bringing 3D technology to main-
stream PCs, something Lockheed Martin could never do or would ever want to do. 
Intel recognized Real3D’s experience and intellectual property value and made its 
original equity investment, knowing it was well-positioned to take advantage of the 
growing 3D trend. So, when the M&A guy from Lockheed Martin called Intel, Intel 
moved and moved fast and made an offer. It was quickly reviewed and accepted over 
the protests of some Lockheed Martin BOD members. 
Intel purchased the remaining Lockheed Martin stockholdings for an undisclosed 
sum (rumored to be in the $30 to $50 million range). Intel let go of most of the 
employees but rehired a couple of dozen as Intel contractors. Lockheed Martin 
provided good to generous severance packages for all the employees, amounting 
to full pay to the end of the year and as much as a full year’s pay for some key 
people. Unfortunately for the former Real3D employees, Intel did not convert their 
Real3D options to Intel stock or options. 
Intel already owed Real3D about $20 million in royalties, so the company bought 
its own debt. But brighter still, it got the royalty ﬂow from the other Real3D licensees 
and the ongoing 3D camera business. And even more brilliant than that, it now had 
the most extensive IP graphics portfolio in the universe. 
The irony was that Intel, with all the resources in the world, all the IP, and more 
smart engineers per square foot than any 10 companies combined, still could not beat 
little companies such as ATI, Matrox, Nvidia, or S3. But now it had a club, and only

6.6 Microsoft Talisman—The Chip That Never Was (1996)
311
S3 might have been immune from it. If Intel decided it needed to use its IP to thwart 
those smart-aleck little graphics companies that still controlled 90% of the market, 
it would not hesitate. 
6.5.4 
3dfx and Intel Patent Cross-License Agreement 
When Intel acquired Real3D and its patent portfolios, it also received the lawsuits 
that went with them. One of those lawsuits was against 3dfx. In April 2000, the 
companies announced a settlement in the form of a mutual patent cross-licensing 
agreement. As part of the agreement, 3dfx and Intel amicably agreed to dismiss all 
pending patent infringement lawsuits between 3dfx and Real3D. The three Real3D 
patents were at issue [36]. 
That was good news for 3dfx, which had no quarrel with Intel but had an 
outstanding one with Nvidia. 
Then in late 2000, Nvidia bought 3dfx’s technological assets. 3dfx’s intellectual 
property included Real3D’s graphics patents, licensed from Intel. 
By January 2001, Nvidia had acquired all of SGI’s graphics development 
resources, including a 10% share in Real3D. That led to a series of lawsuits, joined 
by ATI. The two companies were involved in lawsuits over Real3D’s patents until a 
2001 cross-licensing settlement. 
6.6 Microsoft Talisman—The Chip That Never Was (1996) 
In 1996, as the 3D graphics chip market was in its ascendancy and with new compa-
nies declaring devices every month, Microsoft shocked the industry by introducing 
a radically different approach—tiling. The conventional architecture for a graphics 
chip had been (and still is) what was known as an immediate-mode pipeline. The  
tiling approach was compositing 2D sub-images to the screen. 
Microsoft presented its new 3D graphics and multimedia hardware architecture, 
code named Talisman (Table 6.2), at SIGGRAPH in 1996 [37]. Microsoft said at 
the time, “it exploits both spatial and temporal coherence to reduce the cost of high-
quality animation.” The goals of that new technology approach were ambitious and 
startling.
Talisman dealt with graphics much differently from existing approaches. Instead 
of dumping data into a frame buffer, Talisman composed a scene that incorporated 
objects. It then stitched the separate objects (i.e., images) together in a compositing 
process. Talisman rendered individually animated objects into independent image 
layers. It combined the layers at video refresh rates to create the ﬁnal display. During 
the compositing, it applied an afﬁne transformation (i.e., morphing) process for 
translation, rotation, scaling, and skew to simulate the 3D motion of objects. That

312
6
1996–1999, Graphics Controllers on PCs
Table 6.2 Talisman 
characteristics and features 
Feature
Characteristics 
Desktop resolution
1344 × 1024 
Display resolution
1024 × 768 
Color depth
32 bits 
Update rate (fps)
72 
3D scene complexity
20 K+ 
z-buffer resolution
26 bits 
Texture ﬁltering
Anisotropic 
Shadows
Filtered 
Anti-aliasing
4 × 4 subpixel 
Translucency
256 levels 
Audio/video acceleration
1394, 3D sound integrated 
Input ports
USB integrated
provided a multiplier on the 3D rendering performance and exploited temporal image 
coherence. Image and texture compression reduced Bandwidth demand. 
Talisman did not need to regenerate the frame that changed in animation or video. 
Instead, it reused the frames, bending and warping the image—as long as it did not 
change too much—retendering it in imperceptible tenths of a second. 
Microsoft Research started the project in 1991 and kept it low proﬁle with just 
a handful of people working in basic and applied research. By 1996, the group had 
grown to over 100 as Microsoft sought to be among the top computer research labs in 
the country among such prestigious names as Bell Labs and Xerox PARC. Ten of the 
52 papers presented at SIGGRAPH 1996 were from Microsoft—a record number of 
papers accepted from any company. 
“Microsoft is not trying to make money on Talisman. We want to make money 
on the software once the entire industry leaps forward,” said Jim Kajiya, senior 
researcher of the group at the time [38]. 
Microsoft hoped Talisman would show up in late 1997 on motherboards and 
add-in boards. Such systems would support MPEG-2 decoding, advanced 3D audio, 
videoconferencing, advanced video editing, and true-color 2D and 3D graphics at 
resolutions of 1024 × 768. That dream was never realized, although a few companies 
tried. The company said with that design, performance rivaling high-end 3D graphics 
workstations could be achieved for two to three hundred dollars. 
The design had four key concepts:
• Object-based temporal rendering
• Priority rendering to speciﬁc objects
• Incremental approximations for intermediate frames
• Image compression used for textures and rendered images
• Advanced rendering techniques

6.6 Microsoft Talisman—The Chip That Never Was (1996)
313
• Polygon 
anti-aliasing, 
anisotropic 
texture 
ﬁltering, 
multi-pass 
rendering 
(shadows, reﬂection maps)
• Integrated graphics, video, and audio. 
PC graphics systems designers struggle with three major problems: memory 
bandwidth, latency, and cost. However, Talisman needed greater TV resolution to 
deliver the compelling image quality and high entertainment satisfaction. Consider 
the impact it had on memory bandwidth at the time, as shown in Fig. 6.27. 
Talisman used 3D objects. Those objects would be preprocessed into polygons 
and sorted based on their distance from the viewport. A compositor chip would then 
process the polygons on the ﬂy, add texture maps, and stream the pixels to a special 
LUT-DAC in real-time.
Fig. 6.27 Memory bandwidth requirement (Mbytes/second) 
Fig. 6.28 Talisman system hardware partitioning 

314
6
1996–1999, Graphics Controllers on PCs
Talisman used four major concepts:
• Composited image layers with afﬁne transformation
• Image compression
• Chunking
• Multi-pass rendering. 
Talisman changed the ﬂow of data through a 3D pipeline. In a conventional 
approach, geometry calculations would create a polygon list in the main memory. 
The polygons were broken into spans of pixels. A 3D chip would read the pixel 
spans, calculate which pixels from each span would be visible, calculate lighting, 
and save those intermediate results. The chip then used the intermediate data to read 
texture maps from dedicated texture memory and write modiﬁed texture data into a 
screen buffer. 
Microsoft divided the processing tasks differently, as shown in Fig. 6.28. 
In the Talisman design, there was no screen buffer. The rendering operations had 
to keep up with the LUT-DAC. It assigned tasks to different pieces of hardware, 
many of which were new to the PC world. 
The goals of composited image layers were as follows:
• independent objects rendered into separate image layers (sprites)
• object images were updated only when they changed
• object image resolutions could vary
• sprites were alpha-composited at display rates
• afﬁne transformation of each sprite at display rates
• 4X reduction in processing requirements. 
Overall control remained with 3D driver software running on the host CPU. A 
new multimedia DSP chip under development at Samsung handled the geometry 
calculation, lighting, and Z-sorting—operations typically done on the host in most 
graphics systems. The DSP would process objects to produce a sorted polygon list 
in the main memory. 
Intel had traditionally and vigorously opposed the transfer of tasks from the CPU 
to another processor. However, at the time, Intel did not seem too concerned about 
that and was busy introducing its SIMD MMX integrated coprocessor and deni-
grating RISC processors; in Intel’s mind, X86 would rule. However, speculation was 
that Microsoft worked with Samsung because Intel’s CPU Road map did not offer 
sufﬁcient computing power for compelling interactive 3D. 
Talisman’s approach to 3D—like Nvidia’s NV1 based on quadratic surfaces, 
RSSI/S-MOS, and VideoLogic’s PowerVR—was quite different from traditional 
solutions. Also, except for Nvidia, there was no clear compatibility path for 2D 
graphics. Another critical point was that the application had to be ported to the 
graphics controller to take advantage of the hardware. However, Microsoft said at 
the time that Talisman would be D3D compatible. 
In a conventional frame buffer approach, the 3D hardware had one entire frame, 
typically 1/30 of a second, to render all the visible polygons. If the chip falls behind 
in a particularly dense area of the screen, it could always catch up by going faster

6.6 Microsoft Talisman—The Chip That Never Was (1996)
315
Fig. 6.29 Talisman polygon object processor 
through a sparse area. Only the total number of rendered pixel spans matters. The 
Talisman polygon object processor, also known as the tiler chip, is shown in Fig. 6.29. 
Talisman’s rendering engine had to keep up with the LUT-DAC. The row had to 
be rendered before the LUT-DAC needed the pixel data no matter how dense the 
image. When computing on the ﬂy like that, there was no opportunity for graceful 
degradation. 
The compositor (to be built by Cirrus Logic) would fetch polygons from the main 
memory, break them down into spans, and apply texture-mapping. The compositor 
worked on one macroblock (i.e., tile) of 32 × 32 pixels at a time, identifying which 
polygons would be visible in that macroblock and computing the textures for those 
polygons. The macroblock information was forwarded to the intelligent Fujitsu LUT-
DAC, which drove the pixels onto the screen. 
Macroblocks could conserve graphics memory by splitting a picture into chunks 
[39], compressing and computing each one separately. Storage requirements for 
images in chunks could be smaller by a factor of 60. That was the secret. Every 
graphics architecture was constrained and struggled against bandwidth limitations. 
Overcoming that bandwidth limitation was the key to getting high-performance 
3D. Talisman was able to perform multi-pass rendering to generate reﬂections and 
shadows in real time. 
There were several objectives for the chunking:
• each sprite rendered in 32 × 32 chunks
• all polygons for a chunk rendered before proceeding to the next chunk

316
6
1996–1999, Graphics Controllers on PCs
• allowed 32 × 32 depth buffer to be on-chip
• anti-aliasing was supported with depth buffering and transparency using an on-
chip fragment buffer
• 32× reduction in frame buffer memory capacity
• 64× reduction in frame buffer bandwidth. 
The compositor is connected to the main memory via the AGP bus. However, 
the bandwidth demands of the compositor would be too high for the 66 MHz AGP. 
Therefore, Microsoft chose to use a texture and rendering compression JPEG-like 
scheme, which they called TREC [40], to reduce the size of the texture map data 
stored in the main memory. That meant the compositor had to do a TREC expansion 
on the ﬂy along with its other tasks. 
The image layer compositor was the other custom VLSI chip developed for the 
reference hardware implementation. That part was responsible for generating the 
graphics output from a collection of depth-sorted image layers. Figure 6.30 shows 
the image layer compositor. 
The image or sprite engine carried out several objectives:
• Performed addressing, ﬁltering, and transformation of sprites for compositing.
• Processed 32 scan lines together.
• The data structure maintained a z-sorted list of sprites (sets of chunks) visible in 
every 32 scan-line regions.
• The sprite engine performed afﬁne transformations (scaling, rotation, translation 
subpixel, and shear).
• Passed pixel and alpha data to compositing buffers at 4 pixels/clock cycle.
Fig. 6.30 Talisman image layer compositor 

6.6 Microsoft Talisman—The Chip That Never Was (1996)
317
Several beneﬁts (and some cautions) came out of that new design. 
Color and alpha:
• The default was 24 bits of color and 8 bits of alpha.
• There was no performance advantage to using lower color or alpha resolutions 
(however, using lots of alpha within a model, especially overlapping translucent 
polygons, would slow the system; overlapping models or sprites with translucency 
were OK).
• It compressed textures more than sprites (careful texture authoring gave the best 
results). 
Texture ﬁltering:
• All textures had to be rectangular powers of two (256 × 256, 1024 × 32, 64 × 
128, etc.).
• Trilinear mipmapping was available at full speed (40 Mpixels/second in a 
reference implementation).
• Anisotropic ﬁltering had a variable performance (1:1 2:1 at full speed, 2:1 4:1 at 
half speed, 6:1 8:1 at fourth speed, maximum anisotropy was 16:1).
• There was no speed advantage for bilinear ﬁltering or point sampling. 
Shadows:
• It used a multi-pass rendering mode.
• Shadows were created individually per object and light source (they could be 
used only sparingly, only for shadows such as essential objects, and only from a 
primary light source). 
Reﬂection mapping:
• It used a multi-pass rendering mode.
• It required rendering a low-resolution version of the scene from another viewpoint 
(used processing time).
• It depended on a reﬂective object, possibly requiring the creation of mipmaps 
(used processing time). 
Scene complexity:
• Object sprites could be used in about four frames before rendering.
• It assumed 500 K to 1 M polygons/second rendered into sprites.
• It allowed for audio and video.
• Developers were advised to design for scalability and to use LOD. 
The fundamental technique used by Microsoft was to replace image synthesis 
with image processing. Image processing and 3D graphics have always had an inti-
mate theoretical relationship, which was evident to anyone who looked at a typical 
SIGGRAPH proceeding. 
Talisman was a challenging piece of technology, and it stressed the traditional 
infrastructure of participating semiconductor and software companies. That fed the 
skeptics and rumormongers. The people in the labs and ofﬁces working on it had a

318
6
1996–1999, Graphics Controllers on PCs
different view. Contrary to suggestions by nonbelievers and nonparticipants, there 
was no depression or fear. Instead, the hundred or so engineers on Talisman projects 
worldwide and the hundred or so developing Talisman were highly enthusiastic, 
proud, and just a little impatient. 
Talisman, however, was not on schedule, at least not the original schedule. But 
why should it have been? It embodied new techniques, new design, new people in 
some cases, and new rules. 
The ﬁrst ofﬁcial implementation of Talisman was to be the Escalate project. Esca-
late was a multimedia subsystem implemented on an add-in board. Three compa-
nies—Cirrus Logic (Fremont, CA), Fujitsu Electronics (San Jose, CA), and Philips 
Semiconductor (Mountain View, CA)—jointly developed, built, and offered refer-
ence design boards for Escalate. The original (optimistic) schedule allowed the boards 
to be available in late Q2 or early Q3 ’97. That schedule slipped again and again. 
Microsoft’s objective was to enhance the capability of the PC to deliver genuinely 
compelling entertainment experiences with display and sound quality beyond what 
was currently available from TVs. Some companies indicated they wanted to license 
it, and others were afraid they would suffer severe competitive pressure because of 
it. 
For example, Scott Sellers, a GPU architect and founder of 3dfx, said at the time, 
The fundamental problem we have with Talisman is the whole paradigm it forces software 
developers into. This is very much like the old image generators of the ‘80s that absolutely 
require software developers to keep up with the frame rate of the monitor. With the old image 
generators, when you found yourself in a scene in a ﬂight sim that had too many polygons on 
it, and the CPU could not keep up, the runway started to fall off. That is exactly the problem 
Talisman is going to have. You have to be fully optimized on Talisman before you really see 
anything running on the screen. It is a really, really tough situation [41]. 
Epilogue 
The tiling approach was later adopted by all but one of the mobile GPU suppliers 
and IP providers, and the one that did not adopt it abandoned the market. Today 
every smartphone and tablet had a GPU with a tiling engine, and every PC still uses 
immediate-mode GPU processing. 
Microsoft did not invent tiling. The early work on tiled rendering was part of the 
Pixel Planes 5 architecture (1981–1988) [42, 43]. The Pixel Planes 5 project validated 
the tiled approach and invented many techniques now viewed as standard for tiled 
renderers. It was the work most widely cited by other papers in the ﬁeld. 
The tiled approach was also known early in the history of software rendering. 
Implementations of Reyes rendering often divide the image into tile buckets. 
Why was it not adopted? 
Several factors came to fruition at the time, not the least of which Microsoft devel-
oped. Remember, the project started in 1991. At that time, the impact of Moore’s law 
was not fully appreciated despite evidence of it everywhere. As a result, CPUs got 
faster, RAM increased in capacity and speed (while dropping in price), and overall

6.7 Nvidia Riva 128 (1996)
319
bandwidth increased. Added to that, Microsoft introduced DirectX, Windows stabi-
lized, and Windows 95 was introduced, which supported higher resolutions. Games 
became more efﬁcient, and ﬁrst-person shooters became popular. So tiling was not 
a good choice for PCs and workstations but, as mentioned, ideal for power-starved 
mobile devices. 
6.7 Nvidia Riva 128 (1996) 
A four-year tortuous journey that turned the company around. 
Nvidia was and still is resilient, if nothing else, and seeing the early difﬁculties using 
quadratic surfaces, the company began investigating polygonal graphics processing. 
Just 18 months after Microsoft released DirectX (September 30, 1995) with 
basically no customers and signiﬁcantly reduced staff, Nvidia brought out the 
polygonal-based DirectX compatible NV3—RIVA 128. 
It was not an April Fool’s joke in 1997 when Nvidia announced the RIVA 128; 
however, it was almost the company’s last gasp. 
RIVA 128. Short for Real-time Interactive Video and Animation, the RIVA 128 
was one of the ﬁrst AGP AIBs, and it introduced many consumers to the Nvidia name. 
Nvidia went through three iterations of its name over the next few years. Starting 
with nVidia, then NVidia, then Nvidia, and now it prefers using all capital letters. 
Regardless of the company’s calligraphy, the RIVA 128 was interesting (see block 
diagram, Fig. 6.31). It supported higher resolutions than its Voodoo 1 competition 
but had poor initial driver support, which meant it could not perform to the same 
standard. That would change.
Packed with 3.5 million transistors and manufactured by SGS-Thomson at 350 nm, 
the chip was in a 90 mm2 package and supported AGP 1× and PCI. A 100 MHz 
clock offered a 100 MHz texture and pixel ﬁll rate and came equipped with 4 MB 
SDR RAM on a 128-bit memory bus, also running at 100 MHz. 
The chip offered advanced features in a small package:
• Massive array of ﬂoating-point geometry processing units.
• Fast 32-bit VGA/SVGA.
• 128-bit 2D/GUI/DirectDraw acceleration.
• Video acceleration (including acceleration for MPEG 1, MPEG 2, and Indeo).
• Multiple color space conversion engines.
• X and Y up and down video scaling.
• CCIR-656 video port.
• 206 MHz LUT-DAC.
• NTSC and PAL output with ﬂicker ﬁlter and over-scan/under-scan support.
• Bus mastering DMA 66 MHz AGP 1.0 interface.
• Bus mastering DMA PCI 2.1 interface.
• 128-bit 100 MHz SGRAM frame buffer interface with bus bandwidth of 1.6 G 
per second.

320
6
1996–1999, Graphics Controllers on PCs
Fig. 6.31 Nvidia’s NV3 RIVA 128 media accelerator
Diamond Multimedia stuck with Nvidia, and when the NV3 was ready, they 
introduced an AIB based on it (Fig. 6.32). STB was the AIB launch partner for RIVA 
128. Diamond came out with their AIB afterward after PC benchmark scores were 
published.
There were many challenges to overcome in delivering high-quality 3D accel-
eration on a PC at an affordable cost. One of the goals was to ofﬂoad the CPU to 
concentrate on transforms, lighting, and gameplay. The graphics processor had to 
provide highly detailed content with hundreds of thousands of triangles, which at 
100 bytes per triangle could rapidly saturate the system bus. 
High frame rates with detailed content were extremely compute-intensive and 
yet were demanded by the user. Therefore, a 3D accelerator had to support billions 
of ﬂoating-point and integer computations per second. And that required detailed 
content with rich textures (10+ MBytes). That required storing textures in system 
memory and overcoming latency and bandwidth issues. 
High frame rates also required sustaining peak performance in the frame buffer 
interface. Meanwhile, z-buffering and alpha blending needed read modify write oper-
ations that could quickly saturate the available bandwidth—lots of challenges. (The 
RIVA 128 block diagram is shown in Fig. 6.33.)
The RIVA 128 met those challenges and offered display list processing, which 
avoided CPU stalls by allowing the graphics pipeline to execute independently of 
the CPU. It had a 5 GFLOP setup and rendering engine, which freed the CPU from 
any setup mathematics.

6.7 Nvidia Riva 128 (1996)
321
Fig. 6.32 Diamond Multimedia’s RIVA 128, NV3-based Viper V330 4 MB gaming AIB circa 
1995 (Courtesy of Mathías Tabó, Wikipedia)
Fig. 6.33 Nvidia’s RIVA 128 3D graphics engine
The NV3 was one of the ﬁrst chips to offer vertex caching, which avoided satu-
rating the host interface with repeated vertices that lowered bus occupancy and 
reduced system memory collisions.

322
6
1996–1999, Graphics Controllers on PCs
And it had a 15 group of pictures (GOP) integer pixel processing engine, one of 
the highest of the time. It offered texture caching, which allowed texture storage in 
system memory on both PCI and AGP. 
Riva hit all of the vital feature requirements of the time. It included D3D accelera-
tion, high-performance VGA, and video performance. Nvidia also got OEMs behind 
the product. It was, after all, the ﬁrst Windows accelerator designed with DirectX 
in mind. As the OEM season got underway toward the end of August 1997, Dell, 
Gateway, and Micron had each signed up for the Riva 128. Dell and Gateway were 
to be serviced by STB Systems, while Micron went to Diamond Multimedia. Nvidia 
also had Canopus, ASUSComputer International, Elsa, and E4 on board. Not bad 
for a company that was rumored to have been looking for a purchaser in early 1997. 
S3 was believed to have come closest to buying Nvidia, but Huang put a high price 
tag on the company, from $25 million to $50 million. It might have been a cheap 
investment at the time, but the market had too many would-be mainstream 2D/3D 
vendors, and no one seemed safe. 
The RIVA 128 was the last AIB the company would build without a fan. Chips 
got bigger and started drawing more power, and needed external cooling. 
The RIVA128 put Nvidia back on the map. It generated revenue that the company 
plowed back into R&D and helped establish Nvidia as a leader in computer graphics. 
The 128 was the turning point and catalyst to the company’s multi-decades of success. 
It was also the forerunner to the GPU, as is explained in the next chapter. 
6.8 S3 Virge 86C385 (1996) 
A 2D chip supplier that tried for VR 16 years too early. 
After developing a-chipset, Diosdado (Dado) Banatao cofounded Chips and Tech-
nologies (C&T) in 1985 with Gordon (Gordie) A. Campbell. The ﬁrm was an early 
fabless semiconductor company that developed system logic chipsets for IBM’s PC-
XT and the PC-AT. Its ﬁrst product was a four-chip EGA package that handled 
the functions of 19 of IBM’s proprietary chips on the Enhanced Graphics Adapter 
(EGA). After 22 months, the company went public. Then In 1997, Intel acquired 
C&T, primarily for its graphics chip business (Fig. 6.34).
In 1984, Banatao and his business partner Francis Siu founded the high-tech 
company Moston, starting with a capital of $500,000. Mostron was a manufacturer of 
motherboards. They also hired Ron Yara from Intel. The trio would make a signiﬁcant 
and lasting mark on the computer industry. 
S3 was founded and incorporated in January in Fremont, California, in January 
1989 by Banatao and Ronald Yara. They named the company S3 because it was 
Banatao’s third start-up. 
S3 began the development of a 2D graphics controller. By the mid-1990s, two 
signiﬁcant developments were taking place. One was the explosion of 3D graphics

6.8 S3 Virge 86C385 (1996)
323
Fig. 6.34 Dado Banatao, 
founder of S3 (Courtesy of 
Positively Filipino)
chip companies, and the other was the emergence of virtual reality from the labs to 
industry. 
The company did very well and introduced a string of VGA and GUI accelerators. 
The company sold the Trio, ViRGE, Savage 3D, and Chrome series of graphics 
processors. 
In 1990, 20 companies were making or had declared they would make a 3D 
graphics chip. By 1996, the number of suppliers exploded to 70. And by 2000, 
the number of suppliers dropped to 12 due to the complexity of developing a 3D 
chip—and the collapse of the internet bubble. 
Around 1990, the notion of virtual reality began to become popular. Before that, 
the virtual reality industry mainly provided VR devices for medical, ﬂight simulation, 
automobile industry design, and military training purposes from 1970 to 1990. By 
the end of the century, things had gotten quiet. The dream of VR failed to be realized 
to the level of its promise. That story would repeat in 2015. 
In 1991, the company introduced its famous S3 911 chip (also known as the S3 
Carrera) as a Windows (or GUI) accelerator. 
With the introduction of the Sony PlayStation and then the Nintendo 64 with 3D 
capability, consumers and OEMs by the mid-1990s began to demand 3D function-
ality from PC graphics AIBs. S3 responded to the demand and, in 1995, introduced 
the S3 Virtual Reality Graphics Engine (ViRGE) graphics chipset, one of the ﬁrst 
2D/3D accelerators designed for the mass market. S3 sought to capture two market 
movements at once: VR and 3D (Fig. 6.35).
In January 1992, Terry Holdt, an executive with 23 years of experience in the 
electronics industry, was appointed president and CEO. Holdt had been one of the 
developers of the famous and ubiquitous MOS 6502 processor. 
In 1993, S3 went public and was considered the third most proﬁtable technology 
company in the world [44]. 
The ViRGE, or 86C385, was a mediocre 3D controller but did well in the market 
because of its low price and excellent 2D capabilities, which was still the primary 
market in terms of applications—i.e., games and ofﬁce apps. Figure 6.36 shows a 
ViRGE AIB.

324
6
1996–1999, Graphics Controllers on PCs
Fig. 6.35 Terry Holdt would 
run S3 for  ten years  
(Courtesy of Team 6502)
Fig. 6.36 S3’s ViRGE 86C385 3D graphics board (Courtesy of VGA Legacy) 
The S3 Engine provided 2D acceleration for Windows application performance 
and a high-performance 3D rendering engine for games and other interactive 3D 
applications. It had BitBlt, line drawing, and polygon ﬁll Windows acceleration. The 
3D features included ﬂat shading, Gouraud shading, and texture mapping support. 
Advanced texture mapping features included perspective correction, bilinear and 
trilinear ﬁltering, mipmapping, and z-buffering. The S3d Engine also had direct 
support for utilizing video as a texture map. Those features provided the most realistic 
user experience for interactive 3D applications at the time. 
However, game developers had to employ S3’s proprietary API to use the 3D 
acceleration; S3 did not initially offer any OpenGL support in their drivers, and not 
enough game developers used the S3 features to make it a successful product. 
Other advanced features of the S3d Engine included S3’s proprietary compressed 
texture formats, which the company claimed improved performance and reduced 
memory requirements. It also used S3’s MUX buffering feature, which allowed

6.8 S3 Virge 86C385 (1996)
325
for z-buffering with no additional memory requirement. S3’s texture compression 
algorithm, known as S3TC, would become an industry standard and survive long 
after the company. 
S3 was one of the ﬁrst to offer a streams processor, which processor provided the 
stretching and YUV color space conversion features required for full-screen video 
playback with both software CODECs and hardware MPEG-1 sources. The stream 
processor allowed the simultaneous display of graphics and video of different color 
depths. For example, it was possible to display a 24-bpp-equivalent video on top of 
an 8-bit graphics background. That saved memory bandwidth and storage capacity 
while permitting higher frame rates. 
The chip offered what S3 called its Scenic Highway. It allowed direct connection 
to S3’s Scenic/MX2 MPEG-1 audio and video decoder as well as video digitizers 
such as Philips’ SAA7110/SAA7111. (The S3 ViRGE block diagram is in Fig. 6.37.) 
The S3 ViRGE provided linear addressing of up to 4 MBytes of display memory. 
Linear addressing of more than 64 Kbytes required the CPU to run in protected 
mode. Linear addressing was useful when software needed direct access to display 
memory. ViRGE offered two linear addressing schemes. The old method could be 
used when memory-mapped I/O (MMIO) was disabled. The second scheme was 
in conjunction with the new MMIO method. The new MMIO method for ViRGE 
offered a 64-MByte addressing window. 
In 1996, S3, United Microelectronics Corporation, a fab, started in Taiwan in 
1980, Alliance Semiconductor, a memory company founded in 1985 in Santa Clara,
Fig. 6.37 The S3 ViRGE 86C385 block diagram 

326
6
1996–1999, Graphics Controllers on PCs
formed a partnership and created the United Semiconductor Corporation. In 1997, 
USC was manufacturing most of the Alliance’s memory and graphics products, and 
Alliance had a 19% ownership in the USC joint venture [45]. At the time, ATI was also 
having its chips built at USC (and SGS Thompson). Alliance reduced its ownership 
to 15.5% in 1998 to raise cash, and in 1998, S3 sold one-third of its holdings to UMC, 
also to raise cash, leaving S3 with 16% ownership. USC would become a cash cow 
for S3. 
Also, in 1996, S3 announced the resignation of its president and CEO, Terry Holdt. 
The company described the decision as being dictated by the quality of life issues. 
He had spent the last several years commuting weekly to Silicon Valley from his 
Southern California home. Although Holdt’s retirement was effective immediately, 
he remained active as vice chairman of the board. 
Gary Johnson, vice president of S3’s graphics business units, assumed the position 
of president and CEO. Johnson joined S3 in 1994. 
In early 1997, the company built three chrome towers just off 101. The company 
was struggling, and such an expense didn’t make sense. 
Then in 1998, Terry Holdt came back from retirement and assumed the position 
of CEO, and Gary Johnson stepped back. Banatao was still on the board. But Banato 
would soon leave to join the Mayﬁeld Fund venture capital ﬁrm and two years later 
found his own venture capital ﬁrm Tallwood Venture Capital which became the 
leading VC ﬁrm for semiconductor startups. 
“I look forward to returning to full-time operational responsibility at the helm,” 
said Terry Holdt. “Based on products currently under development, I expect S3 to 
re-enter the high end of the PC graphics market in 1998. Gary will remain to support 
me by continuing to focus on a number of key strategic initiatives he launched during 
the last year, and Dado will remain on the Board and assist on strategic direction. 
In 1998, S3 introduced a new 3D graphics processor, the Savage3D (Fig. 6.38). 
The chip had better texture ﬁltering than the ViRGE by replacing bilinear with a 
single cycle process. However, multi-texturing still needed multi-pass rendering. If 
two textures were applied to the same pixel, the whole scene would be processed 
twice.
The new chip also had improved video capabilities and a built-in TV encoder with 
scaling and MPEG interpolation for DVDs. 
Before S3 introduced the Savage3D in 1998, Banato left S3 and joined the 
Mayﬁeld Fund venture capital ﬁrm. After two years, the company invited him to 
be a general partner. But Dado had bigger ideas and decided instead to start his own 
venture capital ﬁrm he named Tallwood Venture Capital with a capital of 300 million 
U.S. dollars, all of which came from his pocket (Fig. 6.39).
In 1998, Ken Potashner replaced Terry Holdt. The S3 team began developing an 
Internet Appliance. It did that and in 2000 became SONICblue. 
S3 tried to maintain growth by developing additional chips, such as an audio chip. 
By 1999, the company was in ﬁnancial trouble. Partially due to the collapse of the 
internet bubble and mainly due to the ascendancy of Nvidia and ATI. The number of 
3D chip companies was dropping rapidly. Looking for other markets and ﬁnancial 
support, S3 formed a partnership with Taiwan-based Via Technologies. S3 would

6.8 S3 Virge 86C385 (1996)
327
Fig. 6.38 S3 Savage3D AIB (Courtesy of VGA Museum)
Fig. 6.39 Ken Potashner, 
S3’s ﬁnal CEO (Courtesy of 
SONICblue)
adopt its 3D chip technology to an integrated graphics chip (IGC) made by Via. The 
IGC, made in a 180 nm process, would work with AMD and Intel ×86 processors 
and provide the northbridge functions. The original business plan of S3 in 1989 was 
to build core logic parts (that was partly due to the experience the founders had from 
their C&T days). And a 10-year Intel IP swap in 1998 gave S3 access to Intel buses 
and processor I/O. 
At the time, S3 held a 16% ownership in USC, a semiconductor fabrication (fab) 
company. USC made the IGC, which gave S3/Via a price advantage. 
Then in 1999, the PC market was struggling, and S3 sold all its shares in USC to 
UMC, raising $500 million. S3 also licensed UMC rights to use 29 of S3’s patents 
for $42 million.

328
6
1996–1999, Graphics Controllers on PCs
Diamond Multimedia had been a long-term OEM customer of S3 and marketed 
several AIBs with S3 chips—the companies and management were well known to 
each other. Buckling under competitive pressure, Diamond sought to ﬁnd an alter-
native market and developed an MP3 player called Diamond Rio. The Recording 
Industry Association of America tried to block Diamond from selling it and sued 
Diamond. On June 16, 1999, Diamond Multimedia won in court the right to continue 
making the units. Seven days later, S3 swooped in and bought Diamond for $173 
million. (Diamond’s win against The Recording Industry Association of America 
paved the way for Apple’s iPod, introduced in October 2001.) 
Buying Diamond just about ended all the company’s other AIB partnerships, 
damaging its cash-ﬂow needs and causing it other losses with PC OEMs. 
S3 launched the Savage 2000 in late 1999, shortly after GeForce 256. However, 
S3 could not get its Direct3D 7.0 drivers working, enabling hardware T&L support. 
The company had no path forward in PC graphics. 
A month later and almost out of cash, S3 announced the formation of S3-Via, a 
joint partnership. At the same time, the company was developing a music player it 
would show at the Comdex conference. 
Wanting to buy some business in early 2000, S3 considered acquiring what was 
left of Number Nine, hoping it would pick up the IBM deal Number Nine had. At the 
last minute, S3 pulled out of the discussions, and Number Nine ﬁled for bankruptcy. 
Rumors circulated that Via would acquire cash-strapped S3. In the spring of 2000, 
Via did acquire S3 (for $320 million) in a convoluted deal. 
What was left without S3 continued on in November 200 renamed as SonicBlue. 
Two months after the pioneer of digital video recording, ReplayTV abandoned the 
consumer hardware market, in February 2001, Sonicblue acquired ReplayTV for 
about $120 million. 
Then in August 2002, Sonicblue’s board of directors ﬁred chief executive, presi-
dent, and chairman, Ken Potashner. The board then appointed Gregory Ballard, the 
company’s executive vice president for marketing and product management, to serve 
as the interim chief executive. Ballard had been the CEO of 3dfx. He left 3dfx in 
2002 and went to Sonicblue. As CEO of 3dfx, Ballard had made the decision to buy 
STB, which many believe hastened the failure of 3dfx. 
Finally, in 2003, unable to ﬁnd enough customers for its multimedia products, the 
company ﬁled for bankruptcy. 
6.8.1 
S3 Epilogue 
In July 2011, HTC Corporation announced it would buy the VIA Technologies stake 
in S3 Graphics, thus becoming the majority owner of S3 Graphics and signifying the 
last chapter in its history (Fig. 6.40).
And although the S3 ViRGE, which was named for VR, was never used in any 
VR devices or applications, HTC became one of the leading VR headset suppliers in 
2019, 30 years after S3 was formed with the ambition of becoming a VR company.

6.9 Conclusion
329
Fig. 6.40 S3’s timeline
Banatao’s innovations are still used in today’s desktop and laptop computers. 
Besides the ﬁrst 10-Mbit Ethernet CMOS MAC, his other contributions to the modern 
computer industry include the physical layer (PHY) chip. Bonatao is referred to as 
the Bill Gates of the Philippines, sponsors scholarships, and is a philanthropist. 
The legacy of S3 may have carried on further. Via had an ×86 license it obtained 
in 1999 through its acquisitions of Cyrix and Centaur Technology, and it co-owns 
Zhaoxin, a Chinese chipmaker. Originally, Zhaoxin designed CPUs based on the 
Via Isaiah microarchitecture. Eventually, it introduced its microarchitectures (which 
were likely evolutions of the Isaiah). In 2020, Via transferred particular CPU and 
chipsets-related IP to Zhaoxin. And the chipsets allegedly contain S3 VGA cores 
[46]. 
6.9 Conclusion 
This was it. The second half of the decade of 1990. The GPU was in sight, and over 
a half dozen companies had a design. They were all tracking Moore’s law to see who 
could squeeze in enough transistors to handle the geometry processor and lighting 
engines or a T&L processor. They all could, they thought, if they had the money, or 
had the fab, the luck. But a race only has one winner, and history forgets the rest.

330
6
1996–1999, Graphics Controllers on PCs
The stage was set. The true GPU waits in the wings. Hold on to your hats because 
once we get there all hell is going to break loose, and we’ll go for a ride that is 
unbelievable and unimaginable at the time. 
By 2001, the industry would consolidate to less than 10 suppliers and continue 
to shrink. It was a little sad but every engineer that was laid off from one company 
was almost immediately hired by one of the survivors. And we all owe a lot to all 
those crazy companies that took a chance and failed but in the process pushed chip 
technology ahead to get to the GPU—a processor that can do anything. 
References 
1. Stephen, Dr. S. Hong Kong, and the Transfer to China: Issues and Prospects, Foreign Affairs, 
Defence and Trade Group Current Issues Brief 33 1996–97, Parliament of Australia (23 June 
1997), https://tinyurl.com/8mws6jpe 
2. Hook, B., and Neves, M. Santos. B. The Role of Hong Kong and Macau in China’s Relations 
with Europe, The China Quarterly, Cambridge University Press, (2002), https://library.fes.de/ 
libalt/journals/swetsfulltext/13230835.pdf 
3. Li, Peter S., The Rise, and Fall of Chinese Immigration to Canada: Newcomers from Hong 
Kong Special Administrative Region of China and Mainland China, 1980–20002, International 
Migration, Volume 43, Issue 3, Wiley Online Library, (November 8, 2005), https://onlinelib 
rary.wiley.com/toc/14682435/2005/43/3 
4. ATI Technologies Inc.—Company Proﬁle, Information, Business Description, History, Back-
ground Information on ATI Technologies Inc., https://www.referenceforbusiness.com/history2/ 
19/ATI-Technologies-Inc.html#ixzz6uaNbjOif 
5. Pop, S. AMD Decides to Drop the ATI Brand—Softpedia. News.softpedia.com, (August 
30, 2010), https://news.softpedia.com/news/AMD-Really-Dropping-the-ATI-Brand-154168. 
shtml 
6. Peddie, J. Nvidia and SGS-Thompson announce multimedia accelerators, The PC Graphics 
Report, Volume VII, Number 22, pp 572. (May 23, 1995) 
7. Peddie, J. SEGA titles to run on PCs with Nvidia, PC Graphics Report, Volume VIII, Number 
32, pp 809 
8. United States v. Microsoft: Trial Summaries (page 2), https://cyber.harvard.edu/msdoj/transc 
ript/summaries2.html 
9. Peddie, J, Famous Graphics Chips: Nvidia’s RIVA 128, IEEE Computer Society, https://www. 
computer.org/publications/tech-news/chasing-pixels/famous-graphics-chips-nvidias-riva128 
10. Fisher, A. 3Dfx Interactive announces technical cooperation agreement with Alliance Semi-
conductor, PC Graphics Report, Volume VIII, Number 49, pp 1348, (November 28, 1995) 
11. 3dfx Oral History Panel with Ross Smith, Scott Sellers, Gary Tarolli, and Gordon Camp-
bell, Computer History Museum (May 27, 2014). https://www.youtube.com/watch?v=3Mg 
hYhf-GhU 
12. 3dfx Interactive Announces Microsoft 3D Game Enhanced, https://www.bloomberg.com/press-
releases/1996-08-19/3dfx-interactive-announces-microsoft-3d-game-enhanced-by 
13. Intel and Advance Micro agree on chip trademark. The New York Times. April 22, 1997. 
Archived from the original on January 13, 2019. (Retrieved January 13, 2019) 
14. Intel Announces First In A Series Of Pentium Overdrive Processor Upgrades Featuring 
MMX Media Enhancement Technology, https://newsroom.intel.com/news-releases/intel-ann 
ounces-ﬁrst-in-a-series-of-pentium-overdrive-processor-upgrades-featuring-mmx-media-enh 
ancement-technology/#gs.v7tkzm 
15. 3dfx SST-1 High Performance e Graphics Engine for 3D Game Acceleration, (December 1, 
1999), http://www.o3one.org/hwdocs/video/voodoo_graphics.pdf

References
331
16. Fuchs, H. Distributing a Visible Surface Algorithm Over Multiple Processors, Proceedings of 
1977 ACM Annual Conference, pp 449–451, (October 1977), http://www.cs.unc.edu/~fuchs/ 
publications/IntroPixel-Planes87.pdf 
17. Fuchs, H. and Johnson B., An Expandable Multiprocessor Architecture for Video Graphics, 
Proceedings of the 6th Annual Symposium on Computer Architecture, ACM-IEEE, New York, 
pp 58–67, (April 1979), https://dl.acm.org/doi/10.1145/800090.802893 
18. Clark, J. and Hannah M. Distributed Processing in a High-Performance Smart Image Memory, 
Lambda (since 1981, called VLSI Design), Vol. 1, No. 4, 4th Quarter, 1980, pp 40–45. https:// 
ai.eecs.umich.edu/people/conway/VLSI/VLSIDesMag/Lambda4Q80.pdf 
19. Gupta, S., and Sproull R., and Sutherland I. E. A VLSI Architecture for Updating Raster-Scan 
Displays, Computer Graphics, Vol. 15, No. 3 (Proceedings of 1981 SIGGRAPH Conference), 
pp 71–78, (August 1981), https://dl.acm.org/doi/10.1145/965161.806791 
20. Interview with Tony Tamasi, August 18, 2021 
21. Interview with Scot Sellers, August 18, 2021 
22. Quantum3D Wins Image Generator Deal, HPCwire, (December 1, 2000), https://www.hpc 
wire.com/2000/12/01/quantum3d-wins-image-generator-deal/ 
23. History of the Sega Dreamcast/Development, https://segaretro.org/History_of_the_Sega_Drea 
mcast/Development 
24. Ramat, O. Quantum3D completes mezzanine funding round with investment from 3Dfx, The  
PC Graphics Report, Volume XII, Number 9, pp 357, (March 1, 1999) 
25. The Dodge Garage 3dfx Collection, http://www.thedodgegarage.com/3dfx/ 
26. Prieto, M.G. The Legacy of 3dfx—ebook, https://www.jonpeddie.com/store/the-legacy-of-
3dfx/ 
27. Peddie, J. Yamaha demonstrates 3-D chip, The PC Graphics Report, Volume VII, Number 45, 
pp 900, (November 1, 1994) 
28. Peddie, J. Yamaha demonstrates 3-D chip, The PC Graphics Report, Volume VII, Number 48, 
pp 1025, (November 22, 1994) 
29. Peddie, J. GamePC Consortium, The PC Graphics Report, Volume VII, Number 7, pp 180, 
(February 7, 1995) 
30. Peddie, J. Yamaha announces DRAM version of its 3-D controller, The PC Graphics Report, 
Volume VII, Number 22, pp 580, (May 23, 1995) 
31. Windycityguy, Hardcore Gaming Exclusive Interview with Real 3D, Rage3D, (November 28, 
2008), http://www.rage3d.com/board/showthread.php?t=33936554 
32. Peddie, J. Lockheed Martin’s Real3D update, The PC Graphics Report, Volume VIII, Number 
48 pp 1289, (November 21, 1995) 
33. Fisher, A. Intel and Lockheed Martin 3D partnership, The PC Graphics Report, Volume IX, 
Number 20, pp 688, (May 14, 1996) 
34. Peddie, J. Real3D now an independent corporation, The PC Graphics Report, Volume XI, 
Number 2, pp 44, (January 19, 1998) 
35. Peddie, J. Real 3D and Silicon Graphics settle out of court, The Peddie Report, Volume XI, 
Number 40, pp 1318, (October 12, 1998) 
36. Maher, K. 3dfx enters into patent cross-license agreement with Intel to end the Real3D suit, 
The PC Graphics Report, Volume XIII, Number 15, pp 534, (April 10, 2000) 
37. Torborg, J. and Kajiya, J. T. Talisman: Commodity realtime 3D graphics for the PC, 
SIGGRAPH96: 23rd International Conference on Computer Graphics and Interactive Tech-
niques, Pp 353–363, Association for Computing Machinery, New York, NY, (August 1996), 
https://dl.acm.org/doi/10.1145/237170.237274 
38. Peddie, J. Microsoft describes Talisman, The PC Graphics Report, Volume IX, Number 33, pp 
1118, (August 13, 1996) 
39. Chunking, https://en.wikipedia.org/wiki/Chunking_(computing) 
40. Structured Video Architectures, http://alumni.media.mit.edu/~wad/vsp/node3.html 
41. Ramat, O. The 3Dfx Interview by Omid, The PC Graphics Report, Volume IX, Number 28, pp 
963, (July 9, 1996)

332
6
1996–1999, Graphics Controllers on PCs
42. Mahaney, J. History. Pixel-Planes, University of North Carolina at Chapel Hill, Archived from 
the original on 2008-09-29. (June 22, 1998) 
43. Fuchs, H. Pixel-planes 5: a heterogeneous multiprocessor graphics system 5using processor-
enhanced memories, Pixel-Planes. ACM (1989-07-01) 
44. Solee, T. Dado Banatao Success Story, Millionaire Acts, Dado Banatao Success Story— 
Millionaire Acts, (June 11, 2009) 
45. Alliance Semiconductor reports results for its second ﬁscal quarter, The Peddie Report, Volume 
X, Number 41, pp 1309, (October 20, 1997) 
46. Tyson, M. Chinese CPU maker Zhaoxin to launch a dGPU this year, Hexus, (10 July, 2020), 
https://hexus.net/tech/news/graphics/144073-chinese-cpu-maker-zhaoxin-launch-dgpu-year/

Chapter 7 
What is a GPU? 
7.1 
What is a GPU? 
Graphical processor units—GPUs have used the transistor bounty of Moore’s law 
to improve their richness and quality. Scene rendering employing ﬁxed shaders, 
programmable vertex shaders, pixels shaders, and raytracing coupled with the abun-
dance of workload parallelism has made discrete GPUs command a market and 
use/case that has been impervious to CPU integration. The GPU has proven to be the 
most cost-effective device available, from high-end games, visualization, and HPC 
to smartphones, laptops, and other mobile environments in vehicles. 
The Pipeline. This book has many examples of the GPU pipeline as it evolves 
and goes from one era to the next. You will also learn that there are several names for 
the same thing. That is an evolutionary throwback. For example, a triangle might be 
called a polygon or a primitive. Rasterization might be called scan conversion, etc. 
Figure 7.1 shows the basic concept of a graphics pipeline and the development of an 
image from a 3D model.
Each stage of the pipeline will expand and then ultimately condense into one single 
type of shader as the shaders making up the stages become more programmable and 
powerful. The deﬁnition of a shader is discussed in Sect. 2.2 Shaders, Processors, 
Units, and Cores. 
In his book, A Biography of the Pixel, Alvy ray Smith establishes what he calls 
the Central Dogma [1]. 
Pictures shall be based on models built from Euclidean geometry in three-
dimensions and viewed in two dimensions with Renaissance perspective. 
Ray’s dogma describes exactly what a GPU does (and must do). 
The eras. Computer graphics processors have evolved considerably, primarily 
driven by semiconductor developments enabled by Moore’s law. Establishing the 
thresholds of one generation or era to another is somewhat arbitrary but necessary 
and desired. The eras are: 
First era GPUs (1999–2000) ﬁxed function 
Second era GPUs (2001–2005) programmable shaders
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2022 
J. Peddie, The History of the GPU - Steps to Invention, 
https://doi.org/10.1007/978-3-031-10968-3_7 
333

334
7
What is a GPU?
Fig. 7.1 The basic GPU pipeline
Third era GPUs (2006–2010) uniﬁed shaders 
Fourth era GPUs (2010–2015) compute shaders 
Fifth era GPUs (2015–2020) ray tracing and AI 
Sixth era GPUs (2020– ) mesh shaders and compute. 
The GPU is an extensible and extendable device that has demonstrated its 
scalability for decades, and no doubt we will see new developments and eras 
The graphics processor unit—GPU, is a component in a computer. That computer 
can be a PC, a game console, a smartphone, a vehicle, a point-of-sale (POS) machine, 
a supercomputer, or a robot. 
The GPU itself can be a stand-alone device or integrated within another 
semiconductor such as a CPU or a system on a chip (SoC). 
The GPU magniﬁcently evolved from its ﬁrst incarnation with a ﬁxed transform 
and lighting engine (T&L) to an elaborate programmable parallel processor with 
thousands of ﬂoating-point processors—and there seems no end in sight. 
Engines versus shaders and processors. The convention used in this book refers 
to programmable processors as shaders or processors. An engine is a ﬁxed function, 
non-programmable type of processor that does one thing and only one thing. Engines 
are usually faster and more efﬁcient because they do not carry the overhead of 
programmability—but they are less ﬂexible. 
The GPU is the only coprocessor that has not been assimilated and integrated 
into the CPU. GPUs have been integrated into the CPU but that did not termi-
nate their stand-alone value as it did with ﬂoating-point processors, digital signal 
processors, compressors and decompressors, and other accelerators. The reason 
the GPU survived as a stand-alone coprocessor is because the GPU has scaled 
so well, economically, and technically. The only asymptotes a GPU might face is 
inter-processor communications, which triggers Amdahl’s Law, which posits that in 
parallel processing there are inevitable processes that must be processed in sequence 
and those processes will mediate the ability to increase performance with the addi-
tion of more processors [3]. However, even that can be overcome by putting groups 
of shaders in clusters. Increased coherent cache sizes have scaled over time and they 
aid the GPU’s scalers’ inter-processor communications.

7.2 The GPU
335
Having said that another asymptote may be physical limitations such as silicon 
area and available parallelism. Heat is also an important factor in processor 
performance. 
One of the main attractions of the GPU is its ability to handle workloads that can 
be done in parallel and as soon as data is available. GPUs are a bit like data-ﬂow 
machines—a lot of data-ﬂow machines. 
7.2 
The GPU 
Creating and producing a GPU is a big undertaking. The project concept, develop-
ment, and approval can take years, at least one and as many as ﬁve. The R&D cycle 
of a GPU chip takes at least three to ﬁve years from the project approval and funding. 
It begins with IP research and maybe licensing. Then the project must go through 
multiple stages such as design, chip design, tape out,1 testing, supporting software 
development, application scenarios, and ecological considerations. The cycle is long, 
the investment is signiﬁcant, the technical barriers are formidable, and there are a 
limited number of individuals who have the required skills. From concept to the 
market launch can take as much as ten years, yet some companies have managed it in 
just a few years. Start-ups need several funding rounds and several budget approvals 
for projects within an existing company. The return on investment (ROI) is always 
the big question. Depending on the vision and ambition of the funders, it can make 
or break a project and company … and has. 
The graphics processor unit of today is quite different from the ﬁrst graphics 
controllers discussed in the following chapters. The GPU is the culmination of 
increasing function and large-scale semiconductor integration. The job of the orig-
inal graphics controllers was simply to get the pixels to the display and draw lines, 
but by 1985, we saw the graphics controllers, which were the GPU’s predecessor, 
become heterogeneous in their functions and incorporate features that were executed 
in separate discrete logic. As early as 1991, we began to see VLSI semiconductors 
integrated with graphics processors. 
GPUs can process data simultaneously, making them useful for gaming, machine 
learning, video editing, and artiﬁcial intelligence. GPUs are integrated into the 
computer’s CPU and offered as a discrete, stand-alone hardware unit. 
The diagram in Fig. 7.2 shows some of the many elements of a modern GPU. The 
GPU processes audio and can do specialized matrix math for applications like AI 
training. It can do video encoding, decoding, and transcoding employing multiple 
standards. It can drive multiple displays at different resolutions and through various 
standards. The GPU relies on a localized, tightly coupled high-speed private memory, 
and has high-speed communications with the CPU, other GPUs, and other specialized
1 Tapeout is the ﬁnal result of the design process for integrated circuits or printed circuit boards 
before they are sent for manufacturing. Tapeout is when the photomask of the circuit is sent to the 
fabrication facility. 

336
7
What is a GPU?
Fig. 7.2 The many elements of a modern GPU 
processors in the system. The modern GPU has grown to incorporate a multitude of 
tasks. 
The single instruction, multiple data—SIMD processor is a critical component of 
the modern GPU. It is primarily a collection of ﬂoating-point units—FPUs. Those 
FPUs do geometry translations from the computer model of a 3D element to the 
real-world and 2D display coordinates. 
7.2.1 
Vendors 
The GPU, like all processors and components in a computer—any computer, is 
provided by an independent hardware vendor (IHV). The software that surrounds 
a GPU such as the operating system (OS), applications (apps), and tools (such as 
compilers, languages, and libraries) are provided by independent software vendors 
(ISVs).

7.2 The GPU
337
7.2.2 
Shaders, Processors, Units, and Cores 
The term shader is almost synonymous with GPU. In computer graphics and GPUs 
in particular, a shader is both software and hardware. 
Originally, a shader was a small program used for coloring, or shading polygons 
used in 3D scenes. You can think of coloring the interior of a triangle drawn on a 
sheet of paper—shading it (Fig. 7.3). The term was introduced by Pixar in 1988. The 
shader has since evolved to handle a multitude of tasks in the GPU. 
A shader program runs on a processor. When the processor was programmed with 
the shading algorithm it was (and is) referred to as a shader. You will hear, and read 
in this book about types of shaders, for example, a vertex shader (refer to Fig. 7.1. 
The basic GPU pipeline). A vertex shader is a processor running a vertex program. 
A geometry shader is a processor running a geometry program. Physically, the two 
processors are identical. 
As the number of shaders in a GPU increased, it was necessary to organize them 
in blocks of processors or a cluster. A cluster of shaders (processors) could work 
on a thread simultaneously. Over the years, the GPU suppliers developed individual 
deﬁnitions and terminology to describe their elements and organization. 
The primary processing unit in a GPU is an arithmetic-logic unit (ALU)—a 
ﬂoating-point unit, and typically 32-bits. It is also known as a shader, processing 
element (PE), and sometimes a core. But the term core gets used in multiple cases. 
The FPUs, or shaders, are grouped in a cluster. The GPU suppliers have various 
names for the cluster such as a Compute Unit (CU—AMD), Uniﬁed shading cluster 
array (USC—Imagination Technologies) [3], an Execution Unit (EU—Intel), a 
Processing Unit, a Streaming multiprocessor (SM—Nvidia) [4], and a core. Clusters 
can have 8, 16, 32, or 64 FPUs in them. 
The cluster then makes up the GPU, as illustrated in Fig. 7.4.
A compute unit is a SIMD engine in an AMD GPU and a streaming multiprocessor 
in an Nvidia GPU. Each compute unit has several processing elements (ALU/stream 
processor). For example, A compute unit of an Intel HD 5000 GPU has 80 processing 
elements (16 processing cores with 5 ALUs per processing core). 
The GPU’s cluster organization is very similar from one supplier to another as 
illustrated in Fig. 7.5.
Fig. 7.3 A shaded triangle 

338
7
What is a GPU?
Fig. 7.4 Shaders, processors, clusters, and cores make up a GPU
Fig. 7.5 The various SIMD clusters of the GPU suppliers 
7.2.3 
Getting to a Model 
A 3D model is a construct of triangles. Twelve triangles make up a cube, two triangles 
per side. The more triangles a model has, the higher its resolution and realism. But 
as the number of triangles goes up, the translation or transforms of vertices increase 
by three. And the values of those coordinates (the vertices of the triangles) must be

7.3 The Six Eras of GPUs
339
Fig. 7.6 Raster or direct rendering uses CG tricks to approach realism and is not physically correct. 
highly accurate and, therefore, a large number. In a computer, FFP with a ﬂoating 
(relocatable decimal point) handles large numbers. 
Triangles are used to make models, images, and pictures. Image generation, also 
known as Computer Graphics (CG), or CGI—Computer Generated Imagery, is a 
collection of techniques or tricks designed to make the most realistic image possible 
within the available computer hardware, as well as the budgets of time, funding, and 
skillset. In addition, the ﬁdelity of the image generation inﬂuences the techniques 
used. For example, designers creating a photorealistic image of an automobile want 
the result to be physically accurate and reﬂect light from every surface and facet of 
the automobile model. Animators, movie makers, and game developers do not need 
to create a perfect and complete reproduction of the world, They just need to content 
that looks perfect (Fig.7.6). To accomplish those broad and seemingly contradictory 
objectives requires that software tools providers and hardware processor suppliers 
offer several styles and versions of their products. 
The bunny is such a rendered animated character (Courtesy of Blender). On the 
right is a physically correct rendered automobile (Courtesy of Autogaleria). Similar, 
but different CG processes were used to create both images. 
Dozens of books have been written on these subjects and this chapter will offer 
an overview of the concepts and provide references for those who want to dig deeper 
into the topics. 
7.3 
The Six Eras of GPUs 
Computer graphics processors have evolved considerably over the years, largely 
driven by semiconductor developments enabled by Dr. Gordon Moore’s observation 
affectionately known as Moore’s law, which states that the number of transistors in 
a dense integrated circuit (IC) doubles about every two years. 
Ten years later in 1975, Moore modiﬁed his law, stating that the increasing tech-
nical difﬁculties associated with the production of enhanced microchips would cause

340
7
What is a GPU?
the number of devices located on a chip to double every two years, another prediction 
born out by industry trends [5]. 
Establishing the thresholds of one generation or era to another is necessarily 
arbitrary but helpful in understanding the GPUs increasing inﬂuence in computing. 
The foundation for this book picks the introduction of the fully integrated graphics 
processor now known as a GPU and popularized by Nvidia. 
7.3.1 
Pre-GPU Graphics Controllers (1960–1998) 
Before the GPU, graphics controllers were responsible for getting dots and lines to 
a screen. They were developed for two major segments of computing: large systems 
(now called mainframes) and micro-computers (the forerunners of the PC). Graphics 
controllers emerged from mainframes into stand-alone (and large) systems, and at one 
time, there were over a dozen suppliers. Companies like Evans and Sutherland, IBM, 
Intergraph, Silicon Graphics (SGI), and Tektronix, to name a few, came into being 
from the mid-1960s to the early 1980s. Those companies and various universities 
developed and established the concepts of the graphics pipeline and vertex transfor-
mation of triangles, texture mapping, and the z-buffer that we take for granted today. 
The large systems also evolved from calligraphic stroke-writing displays developed 
for Radar and oscilloscopes with bit-mapped raster-scan displays developed for the 
television industry. Micro-computers appeared in the early 1970s based on Intel’s 
8008 processor. Graphics controllers shrank in size as VLSI semiconductors were 
developed. 
Micro-computers evolved into the PC in the early 1980s, and graphics controllers 
appeared from ATI, Matrox, NEC, Hitachi, and Texas Instruments (TI). IBM VGA 
clones appeared from companies like Chips and Technologies, Cirrus Logic, and S3, 
while dedicated 3D graphics chips were introduced by companies like 3dfx, ATI, 
and Nvidia. They all relied on the CPU or a dedicated ﬂoating-point processor to do 
the transformations and triangle setup. 
The GPU would turn out to be a major inﬂection point. Other companies such 
as 3Dlabs, Bitboys, PixelFusion, S3, and Tseng labs had ambitious ideas and were 
on the path toward a GPU. It was a race, but none of the companies in the race 
except ATI and Nvidia had the ﬁnancial resources, due to the volume of shipments 
in the consumer market for their pre-GPU products, to accelerate the development. 
Functional integrated GPUs were built by SGI and ArtX for the console market, but 
they could not be scaled to the PC before the stand-alone GPU appeared. 
The foundation for the concept of the GPU came from the Pixel Planes project at 
the University of North Carolina in the 1980s and culminated in the formation of the 
PixelFusion company in 1998. 
Once the GPU was introduced it evolved and we mark those evolutionary steps 
as the eras of the GPU mentioned at the beginning of this chapter. Refer to Fig. 7.7
We brieﬂy trace the GPU’s eras in the following section., and then in greater detail 
in separate chapters later in the book.

7.3 The Six Eras of GPUs
341
Fig. 7.7 The eras of GPU development
Each era has delivered better performance, higher integration, more features, 
and increased programmability. In addition to the hardware aspects of a graphics 
processor, there is the software interface known as the application programming 
interface (API) and the OS. In addition, there are associated interfaces and standards 
for the display and the communications to/from the overall system and CPU. 
7.3.2 
First-Era GPUs (1999–2000) Fixed Function 
Although several companies were on track to produce a fully integrated graphics 
processor, what we would end up calling a GPU, was developed by Nvidia and 
they were the ﬁrst to integrate the T&L functions and call their processor a GPU. 
ATI quickly followed and called their processor a visual processing unit, VPU. A 
few years later, other companies introduced GPUs. Microsoft enabled and encour-
aged the developments with the introduction of the Direct3D API, which offered a 
common development platform for developers. DirectX 7 was the ﬁrst PC API to 
support transformation and lighting, the same year Nvidia introduced their GPU. The 
counterpart to DirectX is OpenGL, an open standard that came from the workstation 
sector—it already supported T&L. 
The ﬁrst-era GPUs include ATI’s Radeon 7500, Nvidia’s GeForce 256 and 
GeForce2, S3’s Savage3D, and SIS’s Xabree 600. The math operations included 
cube map textures and signed math operations, Powerful as they were, the proces-
sors developed in the ﬁrst era were more conﬁgurable than their predecessors, but 
they are not truly programmable. 
7.3.3 
Second-Era GPUs (2000–2006) Programmable Shaders 
Entirely programmable T&L shaders distinguish the second era of GPUs. Microsoft’s 
DirectX 8.0, released in November 2000, introduced programmability in the vertex 
and pixel shaders. That freed developers from having to write code for hardware

342
7
What is a GPU?
states—the application speciﬁed a sequence of instructions for processing vertices 
The ﬁrst AIB with a programmable pixel shader was the Nvidia GeForce 3 (NV20), 
released in February 2001. The Microsoft’s Xbox (2001), and the GeForce4 Ti 
(February 2002) also offered programmable shaders. 
The ﬁrst GPU to support Microsoft’s DirectX 9.0 was the ATI R300 Radeon 8500 
released in 2001. It had fully programmable pixel and vertex shaders. However, 
Microsoft did not introduce DirectX 9.0 until August 2002, which meant there were 
few programs that could make use of ATI’s programmability. 
7.3.4 
Third-Era GPUs (2006–2009) Uniﬁed Shaders 
Nvidia launched the ﬁrst DirectX 10 model 4.0-compatible uniﬁed shader GPU archi-
tecture, the G80 on November 8, 2006, with the introduction of its Tesla architecture. 
A uniﬁed GPU processor was released on the GeForce 8800 GTX and GTS AIBs. In 
2007, AMD introduced its Radeon HD 2900 AIB with its R600 (TeraScale) uniﬁed 
shader GPU based on the Graphics Next Architecture. 
The uniﬁed shader capability was based on a very long instruction word (VLIW) 
design in which the GPU executes operations in parallel. The GPU cores were 
organized into multithreaded multiprocessors known as streaming processors. 
Direct3D 10 and OpenGL 3.2 also introduced geometry shaders. That ended the 
era of one vertex in and one vertex out and allowed geometry to be generated from 
within a shader. Complex geometry could now be generated entirely on the graphics 
hardware. 
7.3.5 
Fourth-Era GPUs (2009–2015) Compute Shaders 
The industry entered the fourth era with specialized programmable processors 
within the GPU. The GPU already had specialized processors within it such as 
DSPs (for audio), Arm or RISC-V (for security and memory management), and 
compressor/decompressor (CODECs) for video (MPEG, MP4, etc.); however, they 
were ﬁxed function processors. 
Advanced functions such as programmable geometry shaders, a Hull and Domain 
shader for Tessellation was part of the upgrade which signiﬁcantly changed the power 
and usefulness of the GPU. 
7.3.6 
Fifth-Era GPUs (2015–2020) Ray Tracing and AI 
With AI capabilities came improved image processing that resulted in speeding up 
the rendering time for ray traced images. AI also helped with ﬁltering and sharpening

7.3 The Six Eras of GPUs
343
the images, again to improve performance as measured in frames-per-second (fps). 
Nvidia introduced it’s Deep Learning Super Sampling (DLSS). to speed up real-time 
ray tracing. 
7.3.7 
Sixth-Era GPUs (2020–) Mesh Shaders 
To accommodate developers’ increasing appetite for migrating geometry work to 
compute shaders, in 2017, AMD introduced a new type of shader called a primitive 
shader. It was a programmable geometry stage in their Vega GPU pipeline. Primitive 
shaders had the same access as a compute shader and gave developers access to all 
the data, they needed to process geometry. Primitive shaders led to task shaders, and 
that led to mesh shaders (MS). 
Mesh shaders were introduced with DirectX 12 Ultimate, Vulkan extension in 
2018. This was really the future of the geometry pipeline, by reducing the linear 
pipeline concept. 
AMD introduced specialized scaling processors with the Virtual Super Resolution 
(VSR). Nvidia introduced variable resolution shaders (VRS) 
Nvidia presented a paper on mesh and task shaders in 2019, at the Associa-
tion of Computing Machinery (ACM) special interest group-graphics (SIGGRAPH) 
conference [6]. Khronos, the independent API consortium, announced it would use 
Nvidia’s mesh shader code as an extension to Vulkan and OpenGL. And in March 
2020 Microsoft introduced DirectX 12 Ultimate which included Mesh Shaders, and 
the world shifted. 
Mesh shaders expand the capabilities and performance of the geometry pipeline. 
Mesh shaders incorporate the features of vertex and geometry shaders into a single 
shader stage through batch processing of primitives and vertices data before the 
rasterizer. The shaders are also capable of amplifying and culling geometry resulting 
in very small triangles. 
The ﬁrst three eras or generations were major steps forward. Mesh shaders are 
more incremental, and evolutionary. Bigger steps forward were the beginning of 
compute (introduced with Nvidia’s Tesla G80, AMD’s Radeon Instinct and Intel 
Xeon Phi), and support for AI with the Tensor cores (introduced in the Nvidia Volta), 
as well as support for path tracing. 
7.3.8 
The Range of the GPU and This Book 
GPUs can be used for just about any compute intensive task, especially those that 
can be solved with parallel processing. GPUs were initially developed for computer 
graphics. As software tools for GPUs improved, they became supercomputer-like

344
7
What is a GPU?
accelerators and applied to artiﬁcial intelligence, simulation, and autonomous vehi-
cles. It is beyond the scope of this book to delve deeply into those compute appli-
cations. We will look at them and discuss them brieﬂy, but the main scope and 
majority of information in this book will be about the GPU’s application as a graphics 
accelerator. 
7.4 
Conclusion 
The GPU is the epitome of Moore’s law, scaling with it and often ahead of it. The 
GPU is gathering of processors and conﬁgured to run as a parallel processor, and as 
such, it multiples on itself multiplying the multiplication. 
It didn’t happen at once. Researchers and engineers had to learn the power and 
capabilities and programmers had to learn how to exploit the power of parallel proces-
sors. That resulted in the GPU expanding in capabilities and purpose and created eras 
of GPU development. 
The eras will be discussed in more detail in the subsequent sections. 
The next book, The History of the GPU: The Eras and Environment, discusses 
the basic functions of A GPU and the concepts of the graphics pipeline. The pipeline 
evolves from one era to the next as features and functions are added while others are 
eliminated or merged. 
During the 1990s, the computer graphics market was expanding in all segments. 
Moore’s law was making it possible to build impressive and ever cheaper parts, 
demand was high, and almost all the companies thought they could build a better, 
proprietary solution and own the market. Enthusiasm, expectations, and Excitement 
were high, and CG was being discovered and the industry was being created. 
From console machines to big arcade boxes, simulation systems and stand-alone 
workstations to the introduction of micro-computers and then the PC, the industry 
was being driven forward by competition and enabling technology as the survival of 
the ﬁttest played out. 
The smartphone and autonomous vehicles were the latest platforms to incorporate 
GPUs, and they are discussed in Book Three—New Developments. 
In the following book, Eras and Environment, you will discover how ﬁfteen 
companies were on the path to building the ﬁrst fully integrated GPU. Some 
succeeded in the console, and Northbridge segments, and Nvidia got the prize of 
being the ﬁrst to offer a fully integrated GPU for the PC. You will also learn about 
the GPU. 
7.5 
Epilog 
In April 2022, I conducted a panel discussion, Chasing Pixels: The Pioneering 
Graphics Processors, with some of the pioneers of computer graphics featured in this

References
345
book: Nick England and Mary Whitton of Ikonas, John Poulton and Henry Fuchs of 
the Pixel Planes project, Peter Segal of the AT&T Pixel Processor, and Curtis Priem 
of Sun and Nvidia. The video of that discussion can be found here: https://www.you 
tube.com/watch?v=gvGq87XNXwU. 
References 
1. Smith, A.R. A Biography of the pixel, MIT Press p 457 (2021) 
2. Amdahl’s law, https://en.wikipedia.org/wiki/Amdahl%27s_law 
3. Beets, K. Graphics cores: trying to compare apples to apples, (February 24, 2014), https://www. 
imaginationtech.com/blog/graphics-cores-trying-compare-apples-apples/ 
4. Sanglard, F. A History of Nvidia Stream Multiprocessor, (May 2, 2020),https://fabiensanglard. 
net/cuda/ 
5. Moore’s 
Law, 
https://www.encyclopedia.com/science-and-technology/computers-and-electr 
ical-engineering/electrical-engineering/moores-law 
6. Sathe, R, and Kraemer, M. Applications of Mesh Shading with Dx12, Siggraph 2019, https://dev 
eloper.nvidia.com/siggraph/2019/video/sig918-vid

Appendix A 
Acronyms 
Does anyone really read a glossary? Hopefully yes. They take a lot of time and 
research to write, and can inform, clear up ambiguities, and ever cause some people 
to change their perspective. The trick is to know what to put in and leave out. 
Throughout this book, speciﬁc terms will be used that assume the reader 
understands and is familiar with the industry. 
Terminology and conventions change over time. 
Common acronyms used in this book and the computer graphics industry (product 
names are not included). 
Acronym
Meaning 
ACM
Association of Computing Machinery 
ACRTC
Advanced CRT Controller 
ADD
Adder 
AEC
Architecture, engineering, and construction 
AGP
Accelerated Graphics Port 
AI
Artiﬁcial intelligence 
AIB
Add-in board AKA a card 
ALU
Arithmetic unit 
AMD
Advanced Micro Devices 
API
Application programming interface 
APU
Accelerated processor unit 
AR
Augmented reality 
ASP
Average selling price 
AV
Autonomous vehicles or Audio-visual 
AVGA
Advanced Video Graphic Array 
BGA
Ball grid array 
BitBlt
Bit-block transfer (a biltter performs bitBlt operations)
(continued)
© The Editor(s) (if applicable) and The Author(s), under exclusive license 
to Springer Nature Switzerland AG 2022 
J. Peddie, The History of the GPU - Steps to Invention, 
https://doi.org/10.1007/978-3-031-10968-3 
347

348
Appendix A: Acronyms
(continued)
Acronym
Meaning
BPP
Bit-per-pixel 
CAD
Computer-aided design 
CAM
Computer-aided manufacturing 
CEO
Chief executive ofﬁcer 
CG
Computer graphics 
CGA
Color graphics adapter 
CGI
Computer graphics interactive 
cGPU
Compute GPU (GP GPU) 
CISC
Architecture (complex instruction set computer) 
CLUT
Color lookup table 
CMOS
Complementary metal–oxide–semiconductor 
CMP
Cryptocurrency mining GPU (AKA mGPU) 
CODEC
Compression or decompression 
COMDEX
COMputer Dealers’ Exhibition 
COTS
Commercial-off-the-shelf 
CPGA
Ceramic Pin Grid Array 
CPU
Central processing unit 
CRT
Cathode ray-tube 
CRTC
CRT controller 
CSG
Constructive solid geometry 
CU
Compute unit 
CUDA
Compute Uniﬁed Device Architecture, Nvidia 
DAC
Digital-to-analog converter 
DCC
Digital content creation applications 
DDA
Digital differential analyzer 
DDR
Double data rate 
DEC
Digital Equipment Corporation 
dGPU
Discrete GPU 
DIB
Device-independent bitmap 
DIME
Direct Memory Execute 
DIP
Dual in-line package 
DL
Deep-learning 
DLSS
Deep learning super sampling 
DMA
Direct-memory-access 
DNLE
Digital non-linear video editing 
DNN
Deconvolutional neural network, also Deep-learning neural network 
DSP
Digital signal processor
(continued)

Appendix A: Acronyms
349
(continued)
Acronym
Meaning
ECL
Emitter-coupled logic 
EDA
Electronic design automation 
EGA
Enhanced graphics adapter 
eGPU
External GPU Also embedded GPU and for ExpressCard GPU 
EISA
Extended Industry Standard Architecture 
EMC
Enhanced memory chips 
EU
Execution unit 
FBI
Frame-buffer interface 
FPGA
Field programmable gate array 
FPS
First-person shooter game 
Fps
Frames-per-second 
FPU
Floating-point unit 
GB/s
Giga bytes per second 
GCN
Graphics core next 
GDC
Graphics Display Controller (NEC µPD7220) 
GDC
Game developers conference 
GDDR
Graphics DDR 
GFLOPS
Giga (billion) FLOPS 
GM
General manager 
GOP
Group of pictures 
GOPS
Giga-operations per second 
GPGPU
General-Purpose GPU 
GPU
Graphics processing unit 
GPU
Geometry processor unit 
GUI
Graphical user interface 
HAL
Hardware abstraction layer 
HD
High-deﬁnition 
HDL
Hardware description language 
HMD
Head-mounted display 
HPC
High-performance computers or computing 
I/O
Input–output 
IBM
International Business Machines 
IC
Integrated circuit 
IGC
Integrated graphics controller 
IGP
Integrated graphics processor 
iGPU
Integrated GPU 
IHV
Independent hardware vendor
(continued)

350
Appendix A: Acronyms
(continued)
Acronym
Meaning
IMA
Image memory access 
IP
Intellectual property 
ISA
Instruction set architecture 
ISA
industry-standard architecture 
ISP
Image synthesis processor 
ISV
Independent software vendors 
JPA
Jon Peddie Associates 
JV
Joint venture 
LBE
Location-based entertainment 
LEGO
Low End Graphics Option 
LOD
Level of detail 
LRU
Least recently used cache (also last used cache) 
LSI
Large-scale integration 
LUT-DAC
Look-up-table, digital to analog converter 
M/A
Multiplier/accumulator 
MBO
Management buy-out 
MCA
Micro-channel architecture 
MCAD
Mechanical CAD 
MCAE
Mechanical computer-aided engineering 
MCI
Media control interface 
MDA
Monochrome display adapter 
MFLOPS
Million ﬂoating-point operations per second 
MHz
Mega Hertz 
MIPS
Millions of instructions per second 
ML
Machine learning 
MMX
Multi-Media Extensions—Intel’s SIMD engine 
Mpix
Megapixels 
MQFP
Metal quad ﬂat package 
MS
Mesh shader 
MV
Modeling and viewing 
NASA
National Aeronautics and Space Administration 
NCGA
National Computer Graphics Association 
NEC
Nippon Electric Company 
NIH
Not-invented here 
NLE
Non-linear video editing 
nm
Nanometer 
nMOS
N-type metal–oxide–semiconductor
(continued)

Appendix A: Acronyms
351
(continued)
Acronym
Meaning
NTSC
National Television Standards Committee 
OEM
Original equipment manufacturer 
OMAP
Open Multimedia Applications Platform 
PAL
Phase Alternation by Line (U.K. and Europe) 
PCI
Peripheral component interface 
PCIe
Peripheral component interconnect express 
PCM
Pulse-code modulation 
PDA
Personal digital assistant 
PE
Processing elements 
PEL
Picture Element 
PGA
Professional graphics adapter 
PGC
Professional graphics controller 
POS
Point of sale (machine or terminal) 
PQFP
Plastic quad ﬂat pack 
PROM
Programmable read-only memory 
PTC
Parametric Technology Company 
QFP
Quad ﬂat package 
RAM
Random access memory 
RCP
Reality Co-Processor (Sony) 
RDP
Reality display processor (Sony) 
RISC
Reduced instruction set computer 
ROI
Return on investment 
ROM
Read-only memory 
ROP
Raster Operation Pipeline (also expressed as ROPS and Render Output Units) 
RPA
Rendering Polygon Accelerator 
RSP
Reality signal processor (Sony) 
RTL
Register-transfer level 
RTX
Ray Tracing Texel eXtreme 
S3TC
S3 texture compression 
SDK
Software development kit 
SGI
Silicon Graphics Incorporated 
SIGGRAPH
Special interest group graphics (of the ACM) 
SIMD
Single instruction, multiple data 
SIMM
Single inline memory module 
SM
Streaming multiprocessor 
SoC
System on a chip 
SPARC
Scalable Processor Architecture
(continued)

352
Appendix A: Acronyms
(continued)
Acronym
Meaning
STB
Set-top box 
SVGA
Super VGA 
T&L
Transform and lighting 
TI
Texas Instruments 
TIGA
Texas Instruments Graphics Architecture 
TREX
Texture mapping engine 
TSMC
Taiwan Semiconductor Manufacturing Company 
TSP
Texture shading processor (NEC/VL) 
UMA
Uniﬁed memory architecture 
UMC
United Manufacturing Company 
UNC
University of North Carolina 
USC
Uniﬁed shading cluster array 
VBE
VESA BIOS Extension 
VC
Venture capitalist 
VDP
Video display processor 
VESA
Video Electronics Standards Association’ 
VGA
Video graphics adaptor 
vGPU
Virtual GPU 
VLB
VESA local bus 
VLIW
Very large instruction word 
VLSI
Very-large-scale integration 
VME
Versa Module Eurocard 
VP
Vice president 
VPGR
Vector general purpose register 
VPU
Visual processing unit 
VRS
Variable rate shading or variable resolution shader 
WinHEC
Microsoft Windows Hardware Conference 
XGA
External Graphics Port 
XGA
Extended graphics adaptor 
YUV
A color encoding system used by PAL

Appendix B 
Deﬁnitions 
2D—Two dimensional, used to refer to “ﬂat” graphics which only have two axes 
(plural of axis), X & Y, along which drawing occurs, such as those used in normal 
Windows applications. Includes drawing functions like line drawing, BitBLTs, text 
display, polygons, etc. Most common form of computer graphics, since displays are 
2D as well. 
3D—Three dimensional, used to refer to the rendering/display of graphics which 
are 3D in nature (i.e., exist along three axes, X, Y, and Z). In existing PC graphics 
systems, this 3D data needs to be rendered into a 2D surface, namely the display. 
This is something that graphics chips that offer 3D acceleration specialize in, offering 
features such as 3D lines, texture mapping, perspective correction, alpha blending, 
and color interpolation for smooth shading (used in simulating lighted scenes). 
3D scene—A 3D scene is composed of interlocking groups of triangles that make 
up all visible surfaces. By performing mathematical operations on the vertices at the 
corners of each triangle, the geometry-processing engine can place, orient, animate, 
color, and light every object and surface that needs to be drawn. Small programs called 
vertex shaders, uploaded to the graphics chip and executed by the vertex-processing 
engine, control the process. 
ASIC—An “Application Speciﬁc Integrated Circuit” is similar to an FPGA, but ﬁxed 
at the factory, and much cheaper to produce in quantity. 
Adaptive sync—Technology for LCD displays that support a dynamic refresh rate 
aimed at reducing screen tearing. In 2015, VESA announced Adaptive-Sync as an 
ingredient component of the DisplayPort 1.2a speciﬁcation. See FreeSync. 
Adder—A device with two or more inputs which performs the operation of adding 
the inputs and outputting the result. Traditional use in computing is a binary adder, 
in which the inputs and output are binary numbers. Inputs can range in width from 
one bit to many bits. The output of an adder is typically one bit wider than the largest 
input to account for a possible carry situation.
© The Editor(s) (if applicable) and The Author(s), under exclusive license 
to Springer Nature Switzerland AG 2022 
J. Peddie, The History of the GPU - Steps to Invention, 
https://doi.org/10.1007/978-3-031-10968-3 
353

354
Appendix B: Deﬁnitions
Adobe RGB—Adobe RGB (1998) is a color space, developed by Adobe Systems in 
1998. It has a wider gamut than the sRGB (mainly in the cyan-green range of colors) 
and is widely used in professional printing. 
AGP—Acronym for Accelerated Graphics Port. This is a new bus technology that 
Intel introduced in the mid-1990s to provide faster access to graphics boards, and 
ultimately allow these graphics boards to utilize system memory for storage of addi-
tional off-screen graphics elements. However, while some PCs with AGP support 
(usually in the form of a single slot for a graphics board) have been shipping since late 
in 1997, there is no commercially available popular software that currently takes any 
real advantage of AGP’s system memory sharing ability. This is probably because so 
few installed systems currently offer AGP support, and because memory prices have 
dropped enough so that graphics board makers can offer huge amounts of memory 
(4, 8, and even 12 MB) on graphics boards at very low prices, eliminating the need 
to use system memory for additional graphics storage. 
AIB (Add-in board)—An add-in board, also known as a card is a board that gets 
plugged into the PC. When an AIB contains a GPU and memory it is known as a 
graphics AIB or graphics card. It plugs into either PCIexpress or the older buss AGP. 
ALU, Arithmetic Logic Unit—The circuits in a microprocessor where all arithmetic 
and logical instructions are carried out. Distinguished from an Arithmetic Unit by 
the inclusion of logical functions (shift, compare, etc.) as well as arithmetic (add, 
subtract, multiply, etc.) in its repertoire of functions. 
Ambient Occlusion—To create realistic shadowing around objects, developers use 
an effect called Ambient Occlusion (AO); sometimes called “poor man’s ray tracing.” 
AO can account for the occlusion of light, creating non-uniform shadows that add 
depth to the scene. Most commonly, games use Screen Space Ambient Occlusion 
(SSAO) for the rendering of AO effects. There are many variants, though all are 
based on early AO tech, and as such suffer from a lack of shadow deﬁnition and 
quality, resulting in a minimal increase in image quality (IQ) compared to the same 
scene without AO. 
Anaglyph 3D—Unrelated to 3D. This is a method of simulating a depth image on 
a ﬂat 2D display by overlaying colored images representing the view from left and 
right eyes, then ﬁltering the image presented to each eye through an appropriately 
colored lens. 
Anisotropic ﬁltering (AF)—a method of enhancing the image quality of textures 
on surfaces of computer graphics that are at oblique viewing angles with respect to 
the camera where the projection of the texture (not the polygon or other primitive on 
which it is rendered) appears to be non-orthogonal (thus the origin of the word: “an” 
for not, “iso” for same, and “tropic” from tropism, relating to direction; anisotropic 
ﬁltering does not ﬁlter the same in every direction). 
API—Acronym for Application Programming Interface. A series of functions 
(located in a specialized programming library), which allow an application to perform

Appendix B: Deﬁnitions
355
certain specialized tasks. In computer graphics, APIs are used to expose or access 
graphics hardware functionality in a uniform way (i.e., for a variety of graphics 
hardware devices) so that applications can be written to take advantage of that func-
tionality without needing to completely understand the underlying graphics hard-
ware, while maintaining some level of portability across diverse graphics hardware. 
Examples of these types of APIs include OpenGL, and Microsoft’s Direct3D. An 
API is a software program that interfaces an application (Word, Excel, a game, etc.) 
to the GPU as well as the CPU and operating system of the PC. The API informs the 
application of the resources available to it, which is called exposing the functionality. 
If a GPU or CPU has certain capabilities and the API doesn’t expose them then the 
application will not be able to take advantage of them. The leading graphics APIs 
are DirectX and OpenGL. 
API—support Rendering and computing APIs supported by the GPU and the driver. 
APU—The AMD Accelerated Processing Unit (APU), formerly known as Fusion, 
is the marketing term for a series of 64-bit microprocessors from Advanced Micro 
Devices (AMD), designed to act as a central processing unit (CPU) and graphics 
accelerator unit (GPU) on a single chip. 
ARIB STD-B67—Hybrid Log-Gamma (HLG) is a high dynamic range (HDR) stan-
dard that was jointly developed by the BBC and NHK. HLG deﬁnes a nonlinear 
transfer function in which the lower half of the signal values use a gamma curve and 
the upper half of the signal values use a logarithmic curve. 
ASP—Average selling price. 
Architecture—The name of the design, the microarchitecture used for the GPU. It, 
too, will be a proper noun such as AMD’s Radeon DNA or Nvidia’s Hopper. 
Aspect ratio—The ratio of length to height of computer and TV screens, video, ﬁlm, 
or still images. Nearly all TV screens are 4:3 aspect ratio. Digital TVs are moving 
to widescreen which is 16:9 aspect ratio. 
Attach rate—An attach rate (also called an attach ratio) measures how many add-on 
products are sold with each of the basic product or platform and is expressed as a 
percentage. 
AU, Arithmetic Unit—The circuits in a microprocessor where all arithmetic instruc-
tions are carried out. Often found in combination with separate logic and other units, 
controlled by a long, or very long, instruction word. 
Augmented Reality—Augmented Reality (AR) overlays digitally-created content 
into the user’s real-world environment. AR experiences can range from informa-
tional text overlaid on objects or locations to interactive photorealistic virtual objects. 
AR differs from Mixed Reality in that AR objects (e.g., graphics, sounds) are 
superimposed on, and not integrated into, the user’s environment. 
Backlight—The backlight is the source of light of the LCD display panels. The type 
of backlight determines the image quality and the color space of the display. There 
are various backlights like CCFL, LED, WLED, RGB-LED, etc.

356
Appendix B: Deﬁnitions
BGA—Ball-grid array, a type of surface-mount packaging (a chip carrier) used for 
integrated circuits. 
Bidirectional reﬂectance distribution function (BRDF)—a function of four real 
variables that deﬁnes how light is reﬂected at an opaque surface. It is employed in the 
optics of real-world light, in computer graphics algorithms, and in computer vision 
algorithms. The function takes an incoming light direction, and outgoing direction, 
(taken in a coordinate system where the surface normal lies along the z-axis) and 
returns the ratio of reﬂected radiance exiting to the irradiance incident on the surface 
from direction the light source. 
Bidirectional scattering distribution function (BSDF)—Introduced in 1980 by 
Bartell, Dereniak, and Wolfe, it is often used to name the general mathematical func-
tion which describes the way in which the light is scattered by a surface. However, in 
practice this phenomenon is usually split into the reﬂected and transmitted compo-
nents, which are then treated separately as BRDF (bidirectional reﬂectance distribu-
tion function) and BTDF (bidirectional transmittance distribution function). BSDF 
is a superset and the generalization of the BRDF and BTDF. 
Bidirectional scattering-surface reﬂectance distribution function (BSSRDF)— 
or B surface scattering RDF describes the relation between outgoing radiance and 
the incident ﬂux, including the phenomena like subsurface scattering (SSS). The 
BSSRDF describes how light is transported between any two rays that hit a surface. 
Bidirectional texture functions (BTF)—Bidirectional texture function is a 6-
dimensional function depending on planar texture coordinates as well as on view 
and illumination spherical angles. In practice this function is obtained as a set of 
several thousand color images of material sample taken during different camera and 
light positions. 
Bilinear Filtering—When a small texture is used as a texture map on a large surface, 
a stretching will occur and large block pixels will appear. Bilinear ﬁltering smoothens 
out this blocky appearance by applying a blur. 
Binary—A counting system in which only two digits exist, ‘0’ and ‘1.’ Also known 
as the base-2 counting system. Each digit represents an additive magnitude of a power

Appendix B: Deﬁnitions
357
of 2, based on its position, with the right-most digit representing 2 to the 0th power 
(20), the next digit representing 2 to the 1st power (21), etc. For example, the binary 
number 1001B converts to a decimal or base-10 number as follows: 1 * 23 + 0 * 22 
+ 0 * 21 + 1 * 20 = 8 + 0 + 0 + 1 = 9. The binary system is the basis for all digital 
computing. 
Binary Digits—The numbers ‘0’ and ‘1’ in the binary counting system. Also called 
a bit. 
Binary Notation—In various graphics hardware reference documents, as well as in 
some programming languages, it’s common to see binary numbers (a combination 
of binary digits) listed as the binary digits followed by the letter ‘B’ or ‘b,’ as in the 
example listed under the term “Binary.” 
Binary Units—One or more bits. 
Binning—Binning is a sorting process in which superior-performing chips are sorted 
from speciﬁed and lower-performing chips. It can be used for CPUs, GPUs (graphics 
cards), and RAM. The manufacturing process is never perfect, especially given the 
incredible precision necessary and number of transistors to produce GPUs and other 
semiconductors. Manufacturing high-performance and expensive GPUs results in 
getting some that cannot run at the speciﬁed frequencies. Those parts however may 
be able to run at slower speeds and can be sold as less expensive GPUs. 
Bit—Acronym derived from the term “Binary digIT” (see deﬁnition above). 
Bit-depth-bpp—see Bits per-pixel and BPP. 
Bitmap—A bitmap image is a dot matrix data structure that represents a generally 
rectangular grid of pixels (points of color), viewable via a monitor, paper, or other 
display medium. A bitmap is a way of describing a surface, such as a computer screen 
(display) as having several bits or points that can be individually illuminated, and at 
various levels of intensity. A bit-mapped 4k monitor would have over 8-million bits 
or pixels. 
Bits per channel—see Bits per-pixel. 
Bits per-pixel—Bits per channel are the number of bits used to represent one of 
the color channels (Red, Green, Blue). The ‘bit depth’ setting when editing images, 
speciﬁes the number of bits used for each color channel—bits per channel (BPC). 
The human eye can only discern about 10 million different colors. An 8-bit neutral 
(single color) gradient can only have 256 different values which is why similar tones 
in an image can cause artifacts. Those artifacts are called posterization. A 16-bit 
setting (BPC) would result in 48-bits per-pixel (BPP). The available number of pixel 
values of that is (2^48). 
Bilter—A BitBlt process or engine. BitBit is a data operation commonly used in 
computer graphics in which several bitmaps are combined into one using a Boolean 
function. The operation involves at least two bitmaps, one source and destination,

358
Appendix B: Deﬁnitions
possibly a third that is often called the “mask” and sometimes a fourth used to create 
a stencil. 
BPP—BPP is an acronym for Bits Per Pixel. The number of bits per-pixel deﬁnes 
the depth of the color space usable by a graphics device. The following table shows 
the relationship between BPP and colors: 
BPP
Number of available colors 
1
2 
2
4 
4
16 
8
256 
15
32,768 
16
65,536 
24
16,777,216 
Also see Bits per-pixel. 
Brightness—An attribute of visual perception in which a source appears to be radi-
ating or reﬂecting light. In other words, brightness is the perception elicited by the 
luminance of a visual target. It is not necessarily proportional to luminance. This 
is a subjective attribute/property of an object being observed and one of the color 
appearance parameters of color appearance models. Brightness refers to an absolute 
term and should not be confused with Lightness. 
Bump-mapped—Bump mapping is a technique for creating the appearance of depth 
from a 2D image or texture map. Bump mapping gives the illusion of depth by adding 
surface detail by responding to light direction—it assumes brighter parts are closer 
to the viewer. It was developed by Jim Blinn and is based on Lambertian reﬂectance 
which postulates the apparent brightness of a Lambertian surface to an observer is 
the same regardless of the observer’s angle of view. 
Bus interface—The connection that attaches the graphics processor to the system 
(typically an expansion slot, such as PCI, AGP, or PCIe). 
Byte (kbyte, Mbyte, Gbyte, Tbyte)—1 Byte = 8bits (1 byte = 256 discrete values 
(brightness, color, etc.) A collection of 8-bits, accessible as a single unit. As such, a 
byte may represent one of 256 (28) numbers. 
• 1 kilobyte = ~1000 bytes (1024 bytes) 
• 1 Megabyte = ~1000 kilobytes (1,048,576 bytes) 
• 1 Gigabyte = ~1000 Megabytes 
• 1 Terabyte = ~1000 Gigabytes 
CAD—Computer-aided design. 
CAE—Computer-aided engineering.

Appendix B: Deﬁnitions
359
CAGR—Compound average growth rate. 
Cache, Cache Memory—Many processor chips depend on external memory to 
store the bulk of their data. Since access to external memory is slow compared to 
processor speeds, a smaller, faster on-chip memory called a cache is used to improve 
performance. Since the cache holds only a small part of the required data, the cache 
controller runs one of a set of algorithms that attempt to ensure that the processor 
has the fastest possible access to the data it needs at any one time. Many processors 
have a hierarchy of progressively larger and slower on-chip caches in an attempt to 
match the speed and data locality requirements of the processor with the external 
DRAM array. These are referred to as level one (L1), level two (L2) etc. 
Calligraphic display—See Vector scope. 
CFD—Computational ﬂuid dynamics. 
CGI—Computer-generated imagery. 
Chipset—Typically, a pair of chips that manage the data ﬂows and trafﬁc between 
the system memory, CPU, disk drives, keyboard and mouse, and various I/O ports 
(e.g., USB, Ethernet, etc.)—see southbridge and northbridge. 
Chrominance—Chrominance (chroma or C for short) is the signal used in video 
systems to convey the color information of the picture, separately from the accom-
panying luma signal (or Y for short). Chrominance is usually represented as two 
color-difference components: U = B, − Y, (blue − luma) and V = R, − Y, (red 
− luma). Each of these difference components may have scale factors and offsets 
applied to it, as speciﬁed by the applicable video standard. 
Complementary metal–oxide–semiconductor (CMOS) sensor—A CMOS sensor 
is an array of active pixel sensors in complementary metal–oxide–semiconductor 
(CMOS) or N-type metal-oxide-semiconductor (NMOS, Live MOS) technologies. 
Clamp—A clamp is a device which takes an input and produces an output which is 
bounded. A traditional clamp will have two or three different inputs: the signal or 
number to be clamped; the upper bound to clamp to; and possibly a lower bound to 
clamp to. When the signal/numeric input to be clamped is received, it is compared 
against the upper bound, and if it exceeds it, is replaced by the upper bounding value. 
Similarly, if there is a lower bound, the signal/numeric input is compared and if found 
lower than the lower bound, it’s replaced with the lower bound. The result of all the 
bounding is then passed on to the output of the device. 
Clone mode—Duplicates the computer’s screen on the other monitor(s), it’s referred 
to as “Duplicate (in multiple displays’ pull-down menu window). It can be useful 
for presentations, and sometimes to provide a different representation of the same 
output. 
Codename—The GPU manufacturer’s engineering codename for the device. 
Color—In current computer graphics systems, color display information is generated 
as a blend of three colored light components: red, green, and blue (RGB). The

360
Appendix B: Deﬁnitions
combination of all three of these color components at full intensity produces a white 
output, while the absence of all three produces black output. Blending these three-
color components at different intensities can produce a near inﬁnite number of distinct 
colors. While a display monitor tends to require each color component to have a 
voltage from 0 (off) to 0.7 V (full intensity), a computer graphics subsystem tends to 
deal with color in digital terms, on a pixel per-pixel basis. Each pixel has a speciﬁc 
depth, also known as BPP. Each pixel, in the process of being displayed from video 
memory, passes through a component called a RAMDAC. For 8 BPP or less, the 
pixel value read from video memory is usually passed through the LUT portion of 
a RAMDAC in order to produce the requisite RGB information. For greater than 8 
BPP modes, pixels generally bypass the LUTs and go directly to the DACs. In order 
to do this such pixels must be deﬁned with ﬁxed possible ranges of RGB. Therefore, 
it is standard that 15-bit pixels have 5-bits each of R, G, and B, with one bit left 
unused; 16-bit pixels have 5-bits each of R and B, and 6-bits of G; and 24 and 32-bit 
pixels have 8-bits each of R, G, and B (with 8-bits unused in 32-bit pixels). 5-bits 
gives 32 distinct intensity levels of a color component, 6-bits gives 64 levels, and 
8-bits gives 256 intensity levels. It should be noted that pixel modes that go through 
the LUT are called “indexed” color modes, while those that don’t are referred to as 
“direct color” or “true color” modes. 
Color gamut—The entire range of colors available on a particular device such as a 
monitor or printer. A monitor, which displays RGB signals, typically has a greater 
color gamut than a printer, which uses CMYK inks. Also see Gamut, and wide color 
gamut. 
Color space—See color gamut and gamut. 
Combine—The verb used to describe an operation in which two or more values or 
signals are added or concatenated with each other in order to produce a combined 
output. 
Comparator—A comparator is a device which generally takes two inputs, compares 
them, and based on the result of the comparison, produces a binary output or signal to 
indicate the result of the comparison. For example, for a “greater-than” comparator, 
the ﬁrst input would be compared against the second input, and if the ﬁrst is larger, 
a TRUE (usually a binary 1) would be output. 
Computational Photography—Processing of still or moving images with the 
objective of modifying, enhancing or manipulating the images themselves. 
Conformal rendering—Foveation that offers a smoothly varying transition from 
the high acuity region and the low acuity region. Considered more efﬁcient than 
traditional foveated rendering because it requires fewer rendered pixels than other 
techniques. 
Conservative Raster—When standard rasterization does not compute the desired 
result is shown, where one green and one blue triangle have been rasterized. These 
triangles overlap geometrically, but the standard rasterization process does not detect 
this fact.

Appendix B: Deﬁnitions
361
Comparing standard and conservative rasterization 
With conservative rasterization, the overlap is always properly detected, no matter 
what resolution is used. This property can enable collision detection. 
Constant Dither—A constant dither is the application of a dither value which doesn’t 
change over the course of a set of dithering operations. 
Contrast ratio—The contrast ratio is a property of a display system, deﬁned as the 
ratio of the luminance of the brightest color (white) to that of the darkest color (black) 
that the system is capable of producing. A high contrast ratio is a desired aspect of 
any display. It has similarities with dynamic range. 
Convolution—Convolution is a mathematical operation on two functions (f and 
g); it produces a third function, that is typically viewed as a modiﬁed version of 
one of the original functions, giving the integral of the pointwise multiplication of 
the two functions as a function of the amount that one of the original functions is 
translated. Convolution is similar to cross-correlation. It has applications that include 
probability, statistics, computer vision, natural language processing, image and signal 
processing, engineering, and differential equations.

362
Appendix B: Deﬁnitions
By Cmglee—Own work, CC BY-SA 3.0, https://commons.wikimedia.org/w/index. 
php?curid=20206883 
Core clock—The GPU’s reference or base frequency (and boost if available) is 
expressed in MHz or GHz. 
CNN (Convolutional Neural Network)—A Deep Neural Network (DNN) that has 
the connectivity in one or more of its layers arranged so that each node in Layer 
N is a convolution between a rectangular subset of the nodes in layer N − 1 and a 
convolution kernel whose weights are found by training. The arrangement is designed 
to mimic the human visual system and has proven to be very successful at image 
classiﬁcation as long as very large training data sets are available. 
CPU—Acronym for Central Processing Unit. In PC terms, this refers to the 
microprocessor that runs the PC, such as an Intel Pentium chip. 
Crossbar—A crossbar switch, or matrix switch is an assembly of individual switches 
between multiple inputs and multiple outputs that connects the inputs to the outputs 
in a matrix manner. Crossbar switches were developed for information processing 
applications such as telephony and circuit switching. 
CRT—Cathode Ray Tube. Technical name for a display, screen, and/or monitor. 
Most commonly associated with computer displays. 
DAC—Digital to Analog Converter. A DAC is used to translate a digital (integer) 
input, such as a pixel value, into an analog (non-integer) voltage signal. DACs are 
used in CD players to convert CD data into sounds. DACs are also a key component of 
any graphics subsystem, since they convert the pixel values into colors on the screen. 
Graphics boards typically use a device known as a RAMDAC, which combines DACs 
with Look-Up Tables (LUTs). RAMDACs typically contain three LUTs and three 
DACs, one each for the red, green, and blue color components of a pixel. See “Color” 
and “LUT” for more information. 
DCI P3—DCI P3 is a color space, introduced in 2007 by the SMP T E. It is used in 
digital cinema and has a much wider gamut than the sRGB. 
dGPU—The basic, discrete (stand-alone) processor that always had its own private 
high-speed (GDDR) memory. dGPUs are applied to AIBs and system boards in 
notebooks. 
Desktop GPU segments—The desktop is segmented into ﬁve categories, and the 
desktop discrete GPUs follow the same designations 
• Workstation 
• Enthusiast 
• Performance 
• Mainstream 
• Value.

Appendix B: Deﬁnitions
363
Device Driver—A device driver is a low-level (i.e., close to the hardware) piece 
of software which allows operating systems and/or applications to access hardware 
functionality without actually having to understand exactly how the hardware oper-
ates. Without the appropriate device drivers, one would not be able to install a new 
graphics board, for example, to use with Windows, because Windows wouldn’t know 
how to communicate with the graphics board to make it work. 
De-warp—In vision systems, this refers to the process of correcting the spherical 
distortion introduced by the optical components of the system. Especially where a 
single camera is capturing a very wide ﬁeld of view, signiﬁcant distortion can be 
present. This is usually, but not always, removed in the ISP before any signiﬁcant 
vision processing or further computational photography is done. 
Die Size—The square area of the chip, typically measured in square millimeters 
(mm2). 
Direct3D—Also known as D3D, Direct3D is the 3D graphics API that’s part of 
Microsoft’s DirectX foundation library for hardware support. Direct3D actually has 
two APIs, one which calls the other (called Direct3D Retained Mode or D3D RM) 
and hides the complexity of the lower level API (called Direct3D Immediate Mode 
or D3D IM). Direct3D is becoming increasingly popular as a method used by games 
and application developers to create 3D graphics, because it provides a reasonable 
level of hardware independence, while still supporting a large variety of 3D graphics 
functionality (see “3D”). 
Display Port—DisplayPort is a VESA digital display interface standard for a digital 
audio/video interconnect, between a computer and its display monitor, or a computer 
and a home-theater system. DisplayPort is designed to replace digital (DVI) and 
analog component video (VGA) connectors in the computer monitors and video 
cards. 
Dithering—Used to hide the banding of colors when rendering with a low number 
of colors (for example 16-bits). Banding is what happens when there are not enough 
shades of colors, resulting in the eye being able to see a distinct change of colors 
between two shades. Dithering is also a way to visually simulate a larger number 
of colors on a display monitor by interleaving pixels of more limited colors in a 
small grid or matrix pattern, much in the way a magazine’s color pictures are actu-
ally composed of small colored dots. Dithering takes advantage of the human eye’s 
capability to blend regions of color. For example, if you could only display red and 
blue pixels, but wanted to give the visual impression of purple, you would create a 
matrix of interleaved red and blue pixels, as depicted using letters below (B = Blue, 
R = Red): 
BRBRBRBR 
RBRBRBRB 
BRBRBRBR 
RBRBRBRB 
When viewed from a distance, the human eye would blend the red and blue pixels 
in this pattern, making the area appear to be a shade of purple. This technique allows

364
Appendix B: Deﬁnitions
one to simulate thousands of color in exchange for a small loss in detail, even when 
there are only 16 or 256 colors available for display as might be the case when a 
graphics subsystem is conﬁgured to display in an indexed color mode (see “Color”). 
DMCVT—Dynamic Metadata for Color Volume Transforms, SMPTE ST 2094. 
Dolby Vision—12-bit HDR, BT.2020, PQ, Dolby Vision dynamic metadata. 
DVI (Digital Visual Interface)—DVI is a VESA (Video Electronics Standards Asso-
ciation) standard interface for a digital display system. DVI sockets are found on the 
back panel of AIBs and some PCs and also on ﬂat panel monitors and TVs, DVD 
players, data projectors and cable TV set-top boxes. DVI was introduced in and uses 
TMDS signaling. DVI supports High bandwidth Digital Content Protection, which 
enforces digital rights management (see HDCP). 
Dynamic contrast—The dynamic contrast shows the ratio between the brightest 
and the darkest color, which the display can reproduce over time, for example, in the 
course of playing a video. 
EDF—Emissive Distribution Functions. 
eGPU—An AIB with a dGPU located in a stand-alone cabinet (typically called a 
breadbox) and used as an external booster and docking station for a notebook. 
Electronic Imaging—Electronic Imaging is a broad term that deﬁnes a system of 
image capture using a focusing lens sensor with a sensor behind it to translate the 
image into electronic signals. Those signals are then ﬁltered, processed, and made 
available for storage and/or display. A technique for inputting, recording, processing, 
storing, transferring, and using images. (ISO 12651-1). Using computers and/or 
specialized hardware/software to capture (copy), store, process, manipulate, and 
distribute ‘ﬂat information’ such as documents, photographs, paintings, drawings, 
and plans, through digitization. 
End-to-end latency—see Motion-to-photon latency. 
Energy Conservation—The concept of energy conservation states that an object 
cannot reﬂect more light than it receives. 
Energy conservation scales 
For practical purpose, more diffuse and rough materials will reﬂect dimmer and 
wider highlights, while smoother and more reﬂective materials will reﬂect brighter 
and tighter highlights. 
Error correction model (ECM)—belongs to a category of multiple time series 
models most commonly used for data where the underlying variables have a long-
run stochastic trend, also known as cointegration. ECMs are a theoretically-driven

Appendix B: Deﬁnitions
365
approach useful for estimating both short-term and long-term effects of one-time 
series on another. The term error-correction relates to the fact that last-periods devia-
tion from a long-run equilibrium, the error, inﬂuences its short-run dynamics. Thus, 
ECMs directly estimate the speed at which a dependent variable returns to equilibrium 
after a change in other variables. 
Extended mode—Extended mode creates one virtual display with the resolution of 
all participating monitors. Depending on the hardware and software employed, the 
monitors may have to have the same resolution. (there’s more on this in the next 
sections). Both of these modes present the display space to the user as a contiguous 
area, allowing objects to be moved between, or even straddled across displays as if 
they are one. 
Fab—The fabrication process. The average feature size of the transistors in the GPU 
expressed in nanometers (nm). 
FEA—Finite element analysis. 
Field of view—The ﬁeld of view (also ﬁeld of vision, abbreviated FOV) is the 
extent of the observable world that is seen at any given moment. In case of optical 
instruments or sensors it is a solid angle through which a sensor detects the presence 
of light. 
Fill rate: 
• Pixel The rate at which the raster operators can render pixels to a display, measured 
in Pixels/s. 
• Texture The rate at which the texture mapping units can map surfaces onto a 
polygon mesh, measured in Texels/s. 
Fixed function—Fixed-function accelerator AIBs take some of the load off the CPU 
by executing speciﬁc graphics functions, such as BitBlt operations and line draws. 
That makes them better than frame buffers for environments that heavily load the 
system CPU, such as Windows. Those types of AIBs have also been called Windows 
and graphical user interface (GUI) accelerators. 
A ﬁxed function can also apply to the graphics pipeline, such as a T&L stage or 
a tessellation stage. 
Flat shading—A rendering method to determine brightness by the normal vector 
on a polygon and the position of the light source and to shade the entire surface of 
a polygon with the color of the brightness. This rendering method produces a clear 
difference in the colors of adjacent polygons, making their boundary lines visible, 
so it is unsuitable for rendering smooth surfaces. 
Floating-point Unit—An Arithmetic Unit which operates on ﬂoating-point data. 
Most general purpose ﬂoating-point units observe the IEEE 754 standard which 
governs formats, precision, rounding, handling of exceptions, etc. Special purpose 
AUs found in GPUs and other DSPs optimized for speciﬁc tasks do not always do

366
Appendix B: Deﬁnitions
so and hence different results can be obtained for the same instructions executed on 
different AUs. This is one of the challenges of heterogeneous computing. 
FLOP—An acronym for Floating point Operations Per Second used as a measure 
of the computational throughput of a ﬂoating-point arithmetic unit. 
FOV, Field of view—The ﬁeld of view (also ﬁeld of vision, abbreviated FOV) is the 
extent of the observable world that is seen at any given moment. In case of optical 
instruments or sensors it is a solid angle through which a sensor detects the presence 
of light. 
Foveated imaging—a digital image processing technique in which the image resolu-
tion, or amount of detail, varies across the image according to one or more “ﬁxation 
points.” A ﬁxation point indicates the highest resolution region of the image and 
corresponds to the center of the eye’s retina, the fovea. 
Foveated rendering—A graphics rendering technique which uses an eye tracker 
integrated with a virtual reality headset to reduce the rendering workload by limiting 
the image quality in the peripheral vision (outside of the zone gazed by the fovea). 
FPGA—A Field Programmable Gate Array is a reprogrammable logic gate chip 
whose internal gate connections can be altered by downloading a bitstream to the 
card with a special program written for that purpose. 
FPU—A ﬂoating-point unit (FPU) is a part of a computer system specially designed 
to carry out operations on ﬂoating-point numbers. Typical operations are addition, 
subtraction, multiplication, division, and square root. FPUs can be found within a 
CPU, in GPU shaders, and in DSPs and stand-alone coprocessors. 
Fragment shader—Pixel shaders, also known as fragment shaders, compute color 
and other attributes of each fragment. The simplest kinds of pixel shaders output one 
screen pixel as a color value; more complex shaders with multiple inputs/outputs are 
also possible. Pixel shaders range from always outputting the same color, to applying 
a lighting value, to doing bump mapping, shadows, specular highlights, translucency 
and other phenomena. They can alter the depth of the fragment for z-buffering. 
Frame buffer—The separate and private local memory for a GPU on a graphics 
AIB. The term frame buffer is a bit out of date since the GPU’s local memory holds 
much more than just a frame or an image for the display as they did when originally 
developed. Today the GPU’s local memory holds programs (known as shaders) and 
various textures, as well as partial results from various calculations, and two to three 
sets of images for the display as well as depth information known as a z-buffer. 
Frame Rate Control (FRC)—a method, which allows the pixels to show more color 
tones. With quick cyclic switching between different color tones, an illusion for a 
new intermediate color tone is created. For example, by using FRC, a 6-bit display 
panel can show 16.7 million colors, which are typical for 8-bit display panels, and 
not the standard 262,200 colors, instead. There are different FRC algorithms. 
Frame-rate converter (FRC)—Frame rate, also known as frame frequency and 
frames per second (FPS), is the frequency (rate) at which an imaging device produces

Appendix B: Deﬁnitions
367
unique consecutive images called frames. FRC (Frame Rate Conversion) algorithms 
are used in compression, video format conversion, quality enhancement, stereo 
vision, etc. FRC algorithm increases the total number of frames in the video sequence. 
This is performed by inserting new frames (interpolated frames) between each pair 
of neighbor frames of original video sequence. 
FreeSync—The brand name for an adaptive synchronization technology for LCD 
displays that support a dynamic refresh rate aimed at reducing screen tearing. 
FreeSync was initially developed by AMD. FreeSync is a hardware/software solution 
that utilizes DisplayPort Adaptive-Sync protocols to enable smooth, tearing-free and 
low-latency gameplay. 
Frustrum, viewing—A viewing frustum is the 3D volume in a scene relative to 
the viewer. The shape of the volume affects how models are projected from camera 
space onto the screen. The most common type of projection, a perspective projection, 
is responsible for making objects near the camera appear bigger than objects in 
the distance. For perspective viewing, the viewing frustum can be visualized as a 
pyramid, with the camera positioned at the tip. This pyramid is intersected by a front 
and back clipping plane. The volume within the pyramid between the front and back 
clipping planes is the viewing frustum. Objects are visible only when they are in this 
volume. 
G-buffer—Tile Based Deferred Rendering (TBDR). 
Gamma correction—Gamma correction, gamma nonlinearity, gamma encoding, or 
often simply gamma, is the name of a nonlinear operation used to code and decode 
luminance or tristimulus values in video or still image systems. Gamma correction 
is, in the simplest cases, deﬁned by the following power-law expression: 
Plot of the sRGB standard gamma-expansion nonlinearity (red), and its local gamma value, slope 
in log–log space (blue) 

368
Appendix B: Deﬁnitions
In most computer systems, images are encoded with a gamma of about 0.45 
and decoded with a gamma of 2.2. The sRGB color space standard used with most 
cameras, PCs, and printers does not use a simple power-law nonlinearity as above, but 
has a decoding gamma value near 2.2 over much of its range. Gamma is sometimes 
confused and/or improperly used as “Gamut.” 
Gamut—In color reproduction, including computer graphics and photography, the 
gamut, or color gamut is a certain complete subset of colors. 
Typical gamut map. The grayed-out horseshoe shape is the entire range of possible chromaticities, 
displayed in the CIE 1931 chromaticity diagram format 
The most common usage refers to the subset of colors which can be accurately 
represented in a given circumstance, such as within a given color space or by a 
certain output device. 
Also see Color gamut, and wide color gamut. 
GDDR—An abbreviation for double data rate type six synchronous graphics 
random-access memory, is a modern type of synchronous graphics random-access 
memory (SGRAM) with a high bandwidth (“double data rate”) interface designed 
for use in graphics cards, game consoles, and high-performance computation. 
Geometry engine—Geometric manipulation of modelling primitives, transforma-
tions, are applied to the vertices of polygons, or other geometric objects used as 
modelling primitives, as part of the ﬁrst stage in a classical geometry-based graphic 
image rendering pipeline, which is referred to as the geometry engine. Geometry 
transformations were originally implemented in software on the CPU or a dedicated

Appendix B: Deﬁnitions
369
ﬂoating-point unit, or a DSP. In the early 1980s a device called the Geometry Engine 
was developed by Jim Clark and Marc Hannah at Stanford University. 
Geometry shaders—Geometry shaders, introduced in Direct3D 10 and OpenGL 
3.2, generate graphics primitives, such as points, lines, and triangles, from primi-
tives sent to the beginning of the graphics pipeline. Executed after vertex shaders 
geometry shader programs take as input a whole primitive, possibly with adjacency 
information. For example, when operating on triangles, the three vertices are the 
geometry shader’s input. The shader can then emit zero or more primitives, which 
are rasterized and their fragments ultimately passed to a pixel shader. 
Global illumination—“Global illumination” (GI) is a term for lighting systems that 
model this effect. Without indirect lighting, scenes can look harsh and artiﬁcial. 
However, while light received directly is fairly simple to compute, indirect lighting 
computations are highly complex and computationally heavy. 
Gouraud shading—A rendering method to produce color gradual shading over the 
entire surface of a polygon is performed by determining brightness with the normal 
vector at each vertex of a polygon and the position of the light source, and performing 
linear interpolation between vertices. 
The normal vector at each vertex can be determined by taking an average of 
the normal vectors of all the polygons having the common vertex. For a triangular 
polygon, the brightness at each vertex is determined by the normal vector obtained 
for each vertex and the position of the light source. Therefore, the brightness of pixels 
inside a triangle is determined by interpolation. This rendering method represents 
color gradual variations between adjacent polygons, so it is suitable for rendering 
smooth surfaces. 
GPC—A graphics-processing cluster (GPC) is group, or collection, of specialized 
processors known as shaders, or simultaneous multiprocessors, or stream processors. 
Organized as a SIMID processor they can execute (process) a similar instruction 
(program, or kernel) simultaneously, or in parallel. Hence, they are known as a 
parallel processor. (A shader is a computer program that is used to do shading: the 
production of appropriate levels of color within an image.) 
GPU (Graphics processing unit)—The GPU is the chip that drives the display 
(monitor) and generates the images on the screen (and has also been called a Visual 
Processing Unit or VPU). The GPU processes the geometry and lighting effects and 
transforms objects every time a 3D scene is redrawn—these are mathematically-
intensive tasks and hence the GPU has upwards to hundreds of ﬂoating-point 
processor (also called Shaders or Stream Processors.) Because the GPU has so many 
powerful 32-bit ﬂoating-point processors, it has been employed as a special purpose 
processor for various scientiﬁc calculations other than display ad is referred to as a 
GPGPU in that case. The GPU has its own private memory on a graphics AIB which 
is called a frame buffer. When a small (less than ﬁve processors) GPU is put inside a 
northbridge (making it an IGP) the frame buffer is dropped and the GPU uses system 
memory. The GPU has to be compatible with several interface standards including

370
Appendix B: Deﬁnitions
software APIs such as OpenGL and Microsoft’s DirectX, physical I/O standards 
within the PC such as Intel’s Accelerated Graphics Port (AGP) technology and PCI 
Express, and output standards known as VGA, DVI, HDMI, and Display Port. 
GPU-Compute (GPGPU—General Purpose Graphics Processor Unit)—The 
term “GPGPU” is a bit misleading in that general purpose computing such as the 
type an ×86 CPU might perform cannot be done on a GPU. However, because GPUs 
have so many (hundreds in some cases) powerful (32-bit) ﬂoating-point processors, 
they have been employed in certain applications requiring massive vector operations 
and mathematical intensive problems in science, ﬁnance, and aerospace applications. 
The application of a GPU can yield several orders of magnitude higher performance 
than a conventional CPU. 
GPU Preemption—the ability to interrupt or halt an active task (context switch) on 
a processor and replace it with another task, and then later resume the previous task 
this is a concept In the era of single core CPUs preemption was how multitasking was 
accomplished. Interruption in a GPU, which is designed for streaming processing, is 
problematic in that it could necessitate a restart of a process and thereby delay a job. 
Modern GPUs can save state and resume a process as soon as the interruptive job is 
ﬁnished. 
Graphics Adapters—A graphics adapter is the device, subsystem, add-in board, 
chip, or adapter used to generate a synthetic image and drive a display. It has been 
called many things over the decades. Here are the names used in this book. The 
differences may seem subtle, but they are used to differentiate one device from 
another. For example, it is common to see the acronym GPU used when speaking or 
writing about an add-in board. They are not synonyms, and a GPU is a component 
of an AIB. That is not a pedantic diatribe. It would be like referring to an engine 
or transmission to denote an entire automobile or truck. Part of the reason for the 
misuse of terms is misunderstanding, another reason is the ease of speech (like calling 
someone Tom instead of Thomas), and the third is that it is more fun and exciting to 
use. People like to say GPGPU, an initialism for general purpose GPU, as a shorthand 
notation for GPU-computer. So, we are not be the terminology police, but we can try 
to clarify the differences. Generally, an acronym should be a pronounceable word. 
Graphics controller—A graphics controller or graphics chip is a nonprogrammable 
device designed primarily to drive a screen. More advanced versions have some prim-
itive drawing or shading graphic capabilities. The primary differentiation between a 
controller and coprocessor or GPU is the programmable capability. 
Graphics Coprocessors—Co-processors (also written as coprocessors) can serve 
as programmable processors, such as the Texas Instruments’ TI TMS34010 and Tl 
TMS34020 series. Co-processors can run all the graphics functions of an API and 
display lists for applications such as CAD. 
Graphics driver—A device driver is a software stack that controls computer 
graphics hardware and supports graphics rendering APIs and is released under a free 
and open-source software license. Graphics device drivers are written for speciﬁc

Appendix B: Deﬁnitions
371
hardware to work within the context of a speciﬁc operating system kernel and to 
support a range of APIs used by applications to access the graphics hardware. They 
may also control output to the display, if the display driver is part of the graphics 
hardware. 
G-Sync—A proprietary adaptive sync technology developed by Nvidia aimed 
primarily to eliminate screen tearing and the need for software deterrents such as 
V-sync. G-Sync eliminates screen tearing by forcing a video display to adapt to the 
framerate of the outputting device rather than the other way around, which could 
traditionally be refreshed halfway through the process of a frame being output by 
the device, resulting in two or more frames being shown at once. 
HBAO+—Developed by Nvidia, HBAO+ claims the company, improves upon 
existing Ambient Occlusion (AO) techniques and adds richer, more detailed, more 
realistic shadows around objects that occlude rays of light. Compared to previous 
techniques, Nvidia claims HBAO+ is faster, more efﬁcient, and signiﬁcantly better. 
HBM (High Bandwidth Memory)—HMB is a high-performance RAM interface 
for 3D-stacked DRAM from AMD and Hynix. It is to be used in conjunction with 
high-performance graphics accelerators and network devices. The ﬁrst devices to use 
HBM are the AMD Fiji GPUs. 
HDCP (High bandwidth Digital Content Protection)—HDCP is an encryption 
system for enforcing digital rights management (DRM) over DVI and HDMI inter-
faces. The copy protection system (DRM) resides in the computer and prevents the 
user of the PC from copying the video content. 
HDMI (High-Deﬁnition Multimedia Interface)—HDMI is a digital, point-to-point 
interface for audio and video signals designed as a single-cable solution for home 
theater and consumer electronics equipment and also supported in graphics AIBs 
and some PC motherboards. Introduced in 2002 by the HDMI consortium, HDMI is 
electrically identical to video-only DVI. 
Heterogeneous processors—Heterogeneous computing refers to systems that use 
more than one kind of processor or cores. These systems gain performance or energy 
efﬁciency not just by adding the same type of processors, but by adding dissim-
ilar coprocessors, usually incorporating specialized processing capabilities to handle 
particular tasks. 
Hexadecimal—Hexadecimal is the base-16 number system, which has the following 
digits: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, A, B, C, D, E, and F. Each hexadecimal digit therefore 
can also be represented by 4-bits (also called a “Nibble”), with two hexadecimal digits 
fully occupying a byte (8-bits). Also referred to as “Hex.” Hexadecimal notation 
is frequently used in low-level programming, such as accessing a graphics chip 
or writing device drivers. In the C and C++ programming languages, hexadecimal 
numbers are designated by preﬁxing them with a “0x” (zero “x”), while in Intel 
assembly language, hexadecimal numbers have a sufﬁx of “H,” and may have a 
preﬁx of “0” (zero) if the ﬁrst digit is greater than “9.” For example, the hex number

372
Appendix B: Deﬁnitions
notation for “E988” would appear as 0xE988 in C or C++ and as 0E988H in assembly 
language. 
HDR10—10-bit HDR using BT.2020, PQ and static metadata. 
High dynamic range (HDR)—a dynamic range higher than what is considered to be 
standard dynamic range. The term is often used in discussing displays, photography, 
3D rendering. 
High Dynamic Range TV (ITU-R BT.2100). 
Also see Wide color gamut. 
High-dynamic-range imaging (HDRI)—The compositing and tone-mapping of 
images to extend the dynamic range beyond the native capability of the capturing 
device. 
HEVC—High-Efﬁciency Video Codec (ITU-T h.265)—2× more efﬁcient than 
AVC. 
HFR—High Frame Rate (100 and 120 fps). 
HLG—Hybrid Log Gamma Transfer Function for HDR signals (ITU-R BT.2100). 
HLG deﬁnes a nonlinear transfer function in which the lower half of the signal 
values use a gamma curve (SD & HD) and the upper half of the signal values use a 
logarithmic curve. HLG is backwards compatible with SDR. 
HPU—(Heterogeneous Processor Unit)—An integrated multi-core processor with 
two or more ×86 cores, and four or more programmable GPU cores. 
Hull Shaders—See Tessellation shaders. 
IGP (Integrated Graphics Processor)—An IGP is a chip that is the result of inte-
grating a graphics processor with the northbridge chip (see northbridge and chipset) 
An IGP may refer to enhanced video capabilities, such as 3D acceleration, in contrast 
to an IGC (integrated graphics controller) that is a basic VGA controller. When a 
small (less than ﬁve processors) GPU is put inside a northbridge (making it an IGP) 
the frame buffer is dropped and the GPU uses system memory, this is also known as 
a UMA—uniﬁed memory architecture. 
iGPU—A scaled down version, with fewer shaders (processors) than a discrete GPU 
which uses shared local RAM (DDR) with the CPU. 
Image generation—The image generation stage in a GPU where the ﬁnal, displayed 
pictures are created before being sent to the screen. It is where the user engages with 
the results of the entire system. In the case of movies or TV, it is passive. In the 
case of computers, it is interactive, such as playing a game. In the case of interactive 
images, they can be for content creation work or content consumption. There is a 
relentless demand and need for high quality, fast response, and image generation in 
all cases. 
Image sensor—An image sensor, photo-sensor, or imaging sensor is a device, which 
detects the presence of visible light, infrared transmission (IR), and/or ultraviolet

Appendix B: Deﬁnitions
373
(UV) energy. That information constitutes an image. It does so by converting the 
variable attenuation of waves of light (as they pass through or reﬂect off objects) 
into electrical signals. Image sensors are used in electronic imaging devices of both 
analog and digital types, which include digital cameras, camera modules, medical 
imaging equipment, night vision equipment such as thermal imaging devices, radar, 
sonar, and others. The Digital Image Sensor is an Integrated Circuit Chip which has 
an array of light sensitive components on the surface. The array is formed by the 
individual photosensitive points. Each photosensitive sensor point inside the image 
circle acts to convert the light to an electrical signal. The full set of electrical signals 
are converted into an image by the on-board computer. 
ISP, Image Synthesis Processor—An ISP refers to a processing unit which accepts 
as input the raw samples from an imaging sensor and converts them into a human-
viewable image. The samples may have undergone some pre-processing by the sensor 
circuitry to abstract certain details of the sensor operation but in general, they are 
presented in the form of a ‘mosaic’ of color samples without correction for things 
like lens distortion, defective pixels and temporal sampling artefacts. These things, 
as well as extracting the image from the color sample mosaic and encoding the output 
into a standard format are the responsibility of the ISP. 
ITU-R BT.2020—AKA Rec2020 deﬁnes various aspects of ultra-high-deﬁnition 
television (UHDTV) with standard dynamic range (SDR) and wide color gamut 
(WCG), including picture resolutions, frame rates with progressive scan, bit depths, 
color primaries 
ITU-R BT.2100—deﬁnes various aspects of high dynamic range (HDR) video such 
as display resolution (HDTV and UHDTV), bit depth, Bit Values (Files), frame rate, 
chroma subsampling, color space 
ITU-R BT.709—AKA Rec709 standardizes the format of high-deﬁnition television, 
having 16:9 (widescreen) aspect ratio. 
Jitter—In computer graphics, to “jitter a pixel” means to place it off side of its normal 
placement by some random amount in order to achieve a more natural appearance. 
It is also described as shaking. The term is used in several ways, but it always refers 
to some offset of time and space from the norm. 
JPR—Jon Peddie Research. 
Judder—Vertical synchronization can also cause artifacts in video and movie presen-
tations, as they are generally recorded at frame rates signiﬁcantly lower than the 
typical monitor frame rates (24–30 frame/s). When such a movie is played on a 
monitor set for a typical 60 Hz refresh rate, the video player misses the monitor’s 
deadline fairly frequently, in addition to the interceding frames being displayed at 
a slightly higher rate than intended for, resulting in an effect similar to judder. (See 
Telecine: Frame rate differences.) 
KB—Kilobyte. 1024 bytes, where each byte consists of 8-bits of data.

374
Appendix B: Deﬁnitions
Launch—There is no standard. It can be the date the GPU ﬁrst shipped, or the date 
of the announcement. 
LCD (Liquid crystal display)—The technology used for displays in notebook and 
other smaller computers. Like light-emitting diode (LED) and gas-plasma tech-
nologies, LCDs allow displays to be much thinner than cathode ray tube (CRT) 
technology. 
Least Signiﬁcant Bit—In a number in binary representation, the least signiﬁcant bit 
is the one with the lowest value, i.e., the rightmost bit when a number is shown in 
traditional binary form. When this term is in plural form, it refers to multiple bits of 
least signiﬁcance. 
Level of Detail—A term used to describe one or more different sets of detail deﬁning 
a particular geometry or raster image. A low Level of Detail geometry or image is 
one that would be used for rendering an image at a great viewing distance, while a 
high Level of Detail would be used when at a short viewing distance. In raster images 
where texture mapping is used for rendering, Level of Detail is used interchangeably 
with the term “mip-map.” In normal 3D graphics usage, a low numbered Level of 
Detail refers to higher detail, with a Level of Detail 0 (zero) being the highest level 
of detail. See “mip-map.” 
LOD—See Level of Detail. 
LOD Value—A value, used mostly in raster rendering, to deﬁne a calculated viewing 
distance in terms of LODs. For example, an LOD value of 1.585 would indicate a 
view distance located between LOD 1 and LOD 2. 
Long Short-Term Memory (LSTM)—A component of a recurrent neural network 
that includes a memory cell. The component can be used to ‘remember’ events over 
arbitrary periods of time. LSTM can also refer to any network that makes use of 
LSTM components. 
LSB—See Least Signiﬁcant Bit. 
Luminance—A photometric measure of the luminous intensity per unit area of light 
travelling in a given direction. It describes the amount of light that passes through, 
is emitted or reﬂected from a particular area, and falls within a given solid angle. 
The SI unit for luminance is candela per square meter (cd/m2). A non-SI term for 
the same unit is the “nit”. The CGS unit of luminance is the stilb, which is equal to 
one candela per square centimeter or 10 kcd/m2. 
LUT—Acronym for Look-Up Table. LUTs are part of the RAMDAC of a graphics 
subsystem, and in modern graphics chips are usually located within the chip itself. 
The LUT is the part of the output section of a graphics board which translates a pixel 
value (primarily in 4 or 8 BPP indexed color modes) into its red, green, and blue 
components. Once the components have been determined, they are passed through 
the three DACs (red, green, and blue) to generate displayable signals. A diagram of 
this operation, showing an 8-bit pixel, with a value of 250, going through the LUTs, 
is below:

Appendix B: Deﬁnitions
375
LUX—A Lux is one lumen per square meter. 
M&A—Mergers and acquisitions. 
M&E—Media and entertainment. 
Mapped—a term that is used often in computer graphics, which loosely means to 
be ﬁtted to something. One maps to a spatial distribution of (something). A texture 
map is a 2D image of something, bricks, or wood paneling for example. 
MB—Megabyte. 1 megabyte is equal to 1 KB * 1 KB = 1024 × 1024 = 1,048,576 
bytes. 
MDL—Material deﬁnition library. 
Memory: 
• Bus width The bit width of the memory bus. 
• Size of the graphics memory expressed in Gigabytes (GB). 
• Clock The reference or base frequency of the memory clock, expressed in MHz 
or GHz. 
• Bandwidth The maximum rate of data transfer across the memory expressed in 
mega- or gigabytes per second (GB/s, or MB/s). 
MIP-map—A mip-map is one of a series of different versions of the same texture, 
each at a different resolution. Each version is generally one-quarter the size of 
the version preceding it. See also “Mip mapping,” “Bilinear Filtering,” “Trilinear 
Filtering,” “Texel,” and “Texture Mapping.” Mip-mapping is the use of mip-maps

376
Appendix B: Deﬁnitions
during the rendering process. For example, when an image is rendered using nearest 
mip-map selection, the version of the texture that most closely matches the size of the 
image is the one chosen for rendering. In a linearly interpolated mipmapping oper-
ation (also known as “Trilinear Filtering”), a weighted average of the two nearest 
mip-maps based on the LOD value. The term MIP is an acronym for the latin expres-
sion “Multum in parva”—(many in small) implying the presence of many images in 
a small package. 
Mixed Reality—Mixed Reality (MR) seamlessly blends a user’s real-world envi-
ronment with digitally-created content, where both environments coexist to create 
a hybrid experience. In MR, the virtual objects behave in all aspects as if they are 
present in the real world, e.g., they are occluded by physical objects, their lighting 
is consistent with the actual light sources in the environment, they sound as though 
they are in the same space as the user. As the user interacts with the real and virtual 
objects, the virtual objects will reﬂect the changes in the environment as would any 
real object in the same space. 
Model—The marketing name for a GPU assigned by the manufacture, for example, 
AMD’s Radeon, Intel’s Xe, and Nvidia’s GeForce. A model can also be a 3D object. 
For example, the design of a car is a 3D model. 
Motherboard—The main circuit board in a PC, also known as a system boar or a 
planar (by IBM.) Graphics AIBs and other cards (i.e., audio, gigabyte Ethernet, etc.), 
as well as memory, the CPU, and disk drive cables plug into the motherboard. 
Motion-To-Photon Latency (MTPL)—also known as the End-to-end latency is the 
delay between the movement of the user’s head and the change of the VR device’s 
display reﬂecting the user’s movement. As soon as the user’s head moves, the VR 
scenery should match the movement. The more delay (latency) between the two 
actions, the more unrealistic the VR world seems. To make the VR world realistic, 
VR systems want low latency of <20 ms. 
Multi Frame Noise Reduction (MFNR)—Automatically take multiple images 
continuously, combine them, reduce the noise, and record them as one image. With 
multi frame noise reduction, one can select larger ISO numbers than the maximum 
ISO sensitivity. The image recorded is one combined image. 
Multiplexer—A multiplexer, also known as a MUX, is an electronic device that acts 
as a switching circuit. A mux has two or more data inputs, along with a switch or 
select input which determines which of the data inputs is passed to the output portion 
of the device. 
Multi-Projection—Multi-projection can refer to an image created using multiple 
projectors mapped on to a screen, or set of screens (as in a CAVE) for 3D projection 
mapping using multiple projectors. It can also refer to multiple projections within a 
screen in computer graphics. 
Multiplayer game—Multiplayer games have traditionally meant that humans are 
playing with other humans cooperatively, competing against each other, or both.

Appendix B: Deﬁnitions
377
Artiﬁcial Intelligence controlled players have historically been excluded from the 
traditional deﬁnition of multiplayer game. However as AI technology progresses 
this is likely to change. In the future human controlled player’s skill and behavior 
tracked over time could program the skill and behavior of a unique AI that can be 
substituted for the human’s participation in the game. 
Battle Royale is a game mode that creates a translucent dome (or other demarcation) 
over/around the entire playing area. As the match progresses the dome starts to shrink 
toward a random point on the map. Players must stay within the bounds of the dome 
or take damage leading to death. The shrinking dome “herds” players into smaller and 
smaller areas, eventually ensuring that they will be in “close combat.” In summary 
Battle Royale mode allows large scale combat using long range weapons and vehicles 
over large distances; but eventually forces the remaining players into CQC (close 
quarter combat); ensuring that the round time does not extend too long; and a new 
round can begin. 
Permadeath is a video game and simulation feature where the player’s death elim-
inates them from the ability to continue participation in the game or continue as the 
speciﬁc entity they were playing. Permadeath can come in a number of forms. In 
multiplayer combat games this usually means having to wait until the next round 
starts if killed. In most multiplayer combat games rounds last 5–30 min; however, it 
is theoretically possible that death in a game would permanently exclude the player 
from further participation. 
In other multiplayer combat games, permadeath can mean losing all your equip-
ment and your position on the map, forcing the player to respawn with no equipment 
as a new “entity.” Even though there is no waiting period, the ramiﬁcations of dying 
are signiﬁcant, as the player has often spent signiﬁcant time equipping themselves 
and moving to strategic areas of the map. 
Persistent world games track (or attempt to track) the entire game universe as 
individual objects and the state of each object in one single instance. For example, 
in a massive multiplayer persistent world games if a tree is cut down the tree will 
forever be cut down for all players, and for all of time. In single player games the 
user sometimes has the ability to “restart” the universe or run multiple iterations of 
the universe. Running multiple iterations is known as “sharding” the universe. In this 
former case the tree would reappear or in the latter case have various states of being 
dependent on the shard being played. 
Sharded world games can have multiple simultaneous existences of the same base 
game universe in varying degrees of state. Sharded world games that employ procedu-
rally generated universes can have multiple simultaneous versions of non-matching 
universes. In either case for multiplayer, this is usually done to reduce the server 
load of players and reduce latency by grouping players from geographical regions 
into the most optimal “shard.” There can be hundreds of servers running the same 
universe but with unique player participation and parametric states of being. 
MUX—See Multiplexer.

378
Appendix B: Deﬁnitions
Nit—A nit is candela per square meter (cd/m). 
Normal map—A normal maps can be referred to as a newer, better type of bump 
map. A normal map creates the illusion of depth detail on the surface of a model but 
it does it differently than a bump map that uses grayscale values to provide either 
up or down information. It is a technique used for faking the lighting of bumps and 
dents—an implementation of bump mapping. It is used to add details without using 
more polygons. 
Northbridge—The Northbridge is the controller that interconnects the CPU to 
memory via the frontside bus (FSB). It also connects peripherals via high-speed 
channels such as PCI Express, and the AGP bus. 
NTSC (National Television Systems Committee)—Analog color television system 
standard used in U.S.A, Canada, Mexico and Japan. Other standards include 
NURBS—Non-uniform rational basis spline (NURBS) is a mathematical model 
commonly used in computer graphics for generating and representing curves and 
surfaces. It offers great ﬂexibility and precision for handling both analytic (surfaces 
deﬁned by common mathematical formulae) and modeled shapes. 
Oscilloscope—Early oscilloscopes used cathode ray tubes (CRTs) as their display 
element. Storage oscilloscopes used special storage CRTs to maintain a steady 
display of a signal brieﬂy presented. Storage scopes (e.g., Tektronix 4010 series) 
were often used in computer graphics as a vector scope. 
ODM—Original device manufacturer. 
OLED (Organic light-emitting diode)—A light-emitting diode (LED) in which 
the emissive electroluminescent layer is a ﬁlm of organic compound that emits light 
in response to an electric current. This layer of organic semiconductor is situated 
between two electrodes; typically, at least one of these electrodes is transparent. 
OLEDs are used to create digital displays in devices such as television screens, 
computer monitors, portable systems such as mobile phones. 
Open Graphics Library (OpenGL)—a cross-language, cross-platform application 
programming interface (API) for rendering 2D and 3D vector graphics. The API is 
typically used to interact with a graphics processing unit (GPU), to achieve hardware-
accelerated rendering. 
OpenVDB—OpenVDB is an Academy Award-winning open-source C++ library 
comprising a novel hierarchical data structure and a suite of tools for the efﬁcient 
storage and manipulation of sparse volumetric data discretized on three-dimensional 
grids. It was developed by DreamWorks Animation for use in volumetric applica-
tions typically encountered in feature ﬁlm production and is now maintained by the 
Academy Software Foundation (ASWF). https://github.com/AcademySoftwareFou 
ndation/openvdb. 
Ordered Dither—An ordered dither is the application of a series of dither values 
which change in according to a particular pattern, most often in the form of a matrix

Appendix B: Deﬁnitions
379
(also referred to as a “dither matrix”), over the course of a set of dithering operations. 
An ordered dither is traditionally applied positionally, using the modulus of the 
destination pixel X and Y position as an index into the dither matrix. 
Outside-In-Tracking—Outside-In-Tracking is a form of positional tracking where 
ﬁxed external sensors placed around the viewer are used to determine the position 
of the headset and any associated tracked peripherals. Various methods of tracking 
can be used, including, but not limited to, optical and IR. 
PAL—Analog TV system used in Europe, and elsewhere. 
Palette—The computer graphics term “palette” is derived from the concept of an 
artist’s palette, the ﬂat piece of material upon which the artist would select and blend 
his colors to create the desired shades. The palette on a graphics board speciﬁes the 
range of colors available in any one pixel. For example, standard VGAs tend to have 
a palette of 262,144 colors, stemming from the fact that each color in the palette 
is composed of 6-bits each of red, green, and blue (total of 18-bits, and 2^18 = 
262,144). However, since the VGA can only display 16 or 256 colors on-screen at 
any one time, it means that each one of these 16 or 256 colors must be chosen from 
the larger palette via a set of LUTs. See “LUT” for details. 
PAM—Potential available market. 
PCI—Acronym for Peripheral Component Interface. PCI is a bus standard which 
Intel developed to overcome the performance bottlenecks inherent in the ISA bus 
design, and most modern graphics boards are PCI-based (i.e., they need to be inserted 
into the PCI bus in order to work). 
Performance: 
• Shader operations How many operations the pixel shaders (or uniﬁed shaders) 
can perform, measured in Operations/s. 
• Vertex operations The number of operations processed on the vertex shaders in 
Direct3D 9.0c and older GPUs, expressed in Vertices/second. 
Phong shading—refers to an interpolation technique for surface shading in 3D 
computer graphics. It is also called Phong interpolation or normal-vector interpola-
tion shading. Speciﬁcally, it interpolates surface normals across rasterized polygons 
and computes pixel colors based on the interpolated normals and a reﬂection model. 
Phong shading may also refer to the speciﬁc combination of Phong interpolation and 
the Phong reﬂection model. 
Ping-Pong buffering—A technique for managing the sharing of real time streaming 
data between software threads or hardware units. Two or more buffers are allocated 
and the thread or hardware unit responsible for acquiring the data is given control 
of the ﬁrst buffer. As soon as that buffer is ﬁlled, control is handed to the thread or 
unit responsible for processing the data. While processing is happening, control of 
the second buffer is given to the input thread or unit, and is ﬁlled with the streaming

380
Appendix B: Deﬁnitions
data. Filling and processing continue to alternate, or ping-pong between buffers in 
this fashion indeﬁnitely. 
Pixel—Acronym for PIX ELement (“Pix” is a shortened version of “Picture”). The 
name given to one sample of picture information. Can refer to an individual sample of 
RGB luminance or chrominance information. A pixel is the smallest unit of display 
that a computer can access to display information but may consist of one or more 
bits (see “BPP”). 
Pixel density—Information of the number of pixels in a unit of length. With the 
decrease of the display size and the increase of its resolution, the pixel density 
increases. 
Pixel pitch—The pixel pitch shows the distance from the centers of two neighboring 
pixels. In displays, which have a native resolution (the TFT ones, for example), the 
pixel pitch depends on the resolution and the size of the screen. 
Player versus Player (PvP)—Player versus player (PvP) refers to a game that is 
designed for gamers to compete against other gamers, rather than against the game’s 
artiﬁcial intelligence (AI). PvP games generally feature an AI that acts as a second 
player if the gamer plays solo. PvP games are the opposite of player versus envi-
ronment (PvE) games, where the player contends largely with computer-controlled 
characters or situations. 
Polygonal modeling—In 3D computer graphics, polygonal modeling is an approach 
for modeling objects by representing or approximating their surfaces using polygon 
meshes. Polygonal modeling is well suited to scanline rendering and is therefore the 
method of choice for real-time computer graphics. 
Power Island—In chip design, it is common practice to isolate unrelated parts of 
the circuitry from each other and supply the power to each isolate region separately 
so that they can be individually powered down when not required. This saves power 
because CMOS transistors in particular ‘leak’ charge into the substrate even when 
inactive, as long as they are powered. It is necessary to take special steps to isolate 
circuits in MOS devices because, unless modiﬁed, all transistors are (somewhat 
weakly) connected together via the substrate. 
PPI (Pixels per inch)—The pixel density (resolution) of an electronic image device, 
such as a computer monitor or television display, or image digitizing device such as 
a camera or image scanner also referred to as pixels per centimeter (PPCM).

Appendix B: Deﬁnitions
381
The term “dots per inch” (dpi), extended from the print medium, is sometimes used 
instead of pixels per inch. The dot pitch determines the absolute limit of the possible 
pixels per inch. However, the displayed resolution of pixel s (picture elements) that 
is set up for the display is usually not as ﬁne as the dot pitch. 
Projection mapping—Projection mapping, also known as video mapping and spatial 
augmented reality, is a projection technology used to turn objects, often irregularly 
shaped, into a display surface for video projection. The technique dates back to the 
late 1960s, where it was referred to as video mapping, spatial augmented reality, or 
shader lamps. This technique is used by artists and advertisers alike who can add 
extra dimensions, optical illusions, and notions of movement onto previously static 
objects. 
PQ—Perceptual Quantizer Transfer Function for HDR signals (SMPTE ST 2084, 
ITU-R BT.2100). 
PvP—See Player versus Player. 
RAMDAC—Acronym for Random Access Memory Digital to Analog Converter. 
The “RAM” portion of a RAMDAC refers to the LUTs, which by necessity are 
RAMs, while the “DAC” refers to the Digital to Analog Converters. See “DAC” and 
“LUT” for more details. 
Raster graphics—also called scan-line, and bitmap graphics, a type of digital 
display that uses tiny 4-sided but not necessarily square pixels, or picture elements, 
arranged in a grid formation to represent an image. Raster scan graphics has origins in 
television technology, with images constructed much like the pictures on a television 
screen. 
Raster-scan display—A CRT uses a raster scan. Developed for television tech-
nology, an electron beam sweeps across the screen, from top to bottom covering one 
row at a time. The beams intensity is turned on and off as it moves across each row 
to create images. The screen points are referred to as pixels. 
Recurrent Neural Network (RNN)—A class of Neural Networks whose connec-
tions form a directed cyclic graph. In other words, unlike a Feedforward network

382
Appendix B: Deﬁnitions
such as a CNN, the connections include feedback so that outputs can affect subse-
quent inputs, giving rise to temporal behaviors. An example of an RNN is the Long 
Short-Term Memory (LSTM) network popular in speech recognition. 
Reﬂective shadow maps—Reﬂective shadow maps (RSMs) are an extension to a 
standard shadow map, where every pixel is considered as an indirect light source. 
The illumination due to these indirect lights is evaluated on-the-ﬂy using adaptive 
sampling in a fragment shader. By using screen-space interpolation of the indirect 
lighting, it is possible to achieve interactive rates, even for complex scenes. Since 
visualizations and games mainly work in screen space, the additional effort is largely 
independent of scene complexity. The resulting indirect light is approximate, but 
leads to plausible results and is suited for dynamic scenes. 
Register ﬁle—Microprocessors hold data for immediate processing in a small 
amount of fast, local memory referred to as a register ﬁle. This memory is closely 
managed by the compiler and the amount and characteristics of it are crucial to the 
performance of the processor. There is typically, but not always, one register ﬁle for 
each ALU or execution unit in the processor and the organization of the register ﬁle 
closely matches the organization of the execution unit. For example, a register ﬁle 
associated with the scalar unit of a 32-bit processor would be 32-bits wide, whereas 
one associated with a SIMD vector unit with four 32-bit paths would be 128-bits 
wide to hold the required four 32-bit operands. 
Relative luminance—Relative luminance is formed as a weighted sum of linear 
RGB components, not gamma-compressed ones. Even so, luma is often erroneously 
called luminance. SMPTE EG 28 recommends the symbol Y, to denote luma and 
the symbol Y to denote relative luminance. 
Render farm—A render farm is high performance computer system, e.g., a computer 
cluster, built to render computer generated imagery (CGI), typically for ﬁlm and 
television visual effects. 
Resolution, screen resolution—The number of horizontal and vertical pixels on a 
display screen. The more pixels, the more information is visible without scrolling. 
Screen resolutions have a pixel count such as 1600 × 1200, which means 1,600 
horizontal pixels and 1,200 vertical pixels. 
RGB—Red, Green, and Blue. Color components of a pixel blended to create a 
speciﬁc color on a display monitor. See “Color” for additional details. 
Room-Scale VR—Room-Scale VR is an implementation of 6 DoF including the 
required use of Spherical Video tracking, where the user is able to move around a room 
sized environment using real-world motion as reﬂected in the virtual environment. 
ROP—ROP stands for Raster Operator; Raster Operators (ROPs) handle several 
chores near the end of the of the pixel pipeline. ROPs handle anti-aliasing, Z and 
color compression, and the actual writing of the pixel to the output buffer. 
Rounding—An arithmetic operation which adjusts a number up or down relative to 
its magnitude in relation to some deﬁned magnitude. Normally, rounding is used to

Appendix B: Deﬁnitions
383
adjust a number in integer-fractional format to integer, with fractional values of 0 
to less than 0.5 (or 1/2) being adjust downward, and fractional values of 0.5 to less 
than 1 being adjusted upward. Programmatically, rounding is usually accomplished 
by adding 0.5 to a number, and then truncating the resulting fractional amount. See 
“Truncation.” For example, 1.585 rounded is 2, while 1.499 rounded is 1. 
RSMs—See reﬂective shadow maps. 
RT—Ray tracer or ray tracing. 
SaaS—Software as a service. 
SAM—Served available market. 
Scan-line display—see Raster graphics display. 
Scanline rendering—an algorithm for visible surface determination, in 3D computer 
graphics, that works on a row-by-row basis rather than a polygon-by-polygon or 
pixel-by-pixel basis. 
Screen size—On 2D displays, such as computer monitors and TVs, the display size 
(or viewable image size or VIS) is the physical size of the area where pictures and 
videos are displayed. The size of a screen is usually described by the length of its 
diagonal, which is the distance between opposite corners, usually in inches. 
Screen tearing—A visual artifact in video display where a display device shows 
information from multiple frames in a single screen draw. The artifact occurs when 
the video feed to the device is not in sync with the display’s refresh rate. This can be 
due to non-matching refresh rates—in which case the tear line moves as the phase 
difference changes (with speed proportional to difference of frame rates). 
SDK—Software development kit. 
SECAM—Analog TV system used in France and parts of Russia and the Mideast. 
SDR—Standard Dynamic Range TV (Rec.601, Rec.709, Rec.2020). 
Shaders—Shaders is a broadly used term in graphics and can pertain to the 
processing of specialized programs for geometry (known as vertex shading or 
transform and lighting), or pixels shading. 
Shifter—A device which shifts numbers 1 or more bit positions. For example, the 
decimal number 14 (1110b), when passed through a shift which shifts one bit to the 
left, would produce decimal 7 (0111b). Each bit shift to the right is equivalent to an 
integer divide by 2, while each bit shift to the left is equivalent to an integer multiply 
by 2. Shifters are normally used to scale values up or down. 
SIMD—Same Instruction Multiple Data describes computers with multiple 
processing elements that perform the same operation on multiple data points simul-
taneously. Such machines exploit data level parallelism, but not concurrency: there 
are simultaneous (parallel) computations, but only a single process (instruction) at a

384
Appendix B: Deﬁnitions
given moment. SIMD is particularly applicable to common tasks like adjusting the 
contrast and colors in a digital image. 
SOM—Share of market. 
Southbridge—The Southbridge controller handles the remaining I/O, including the 
PCI bus, parallel and Serial ATA drives (IDE), USB, FireWire, serial and parallel 
ports and audio ports. Earlier chipsets supported the ISA bus in the Southbridge. 
Starting with Intel’s 8xx chipsets, Northbridge and Southbridge were changed to 
Memory Controller and I/O Controller (see Intel Hub Architecture). 
Span mode—Some applications, such as games, have an explicit screen resolution 
setting. They will typically default to monitor’s registered resolution. In span mode, 
which is a feature of the driver provided by the GPU supplier, it is possible to make 
one contagious display that spans across all the monitors you choose. Then, when 
the application is opened, it will ﬁll the screens. 
sRGB—sRGB is a color space, developed jointly by Hewlett- Packard and Microsoft 
in 1996. It is used in different devices such as printers, displays, TV sets, cameras, 
etc. The sRGB color space covers about 72% of the NTSC color space. 
Static contrast—The static contrast shows the ratio between the brightest and the 
darkest color, which the display can reproduce simultaneously, for example, within 
one and the same frame/scene. 
Stream Processors—A stream processor is a ﬂoating-point processor found in a 
GPU and is also known as a shader processor. 
Stuttering—A term used to describe a quality defect that manifests as irregular 
delays between frames rendered by the GPU(s), causing the instantaneous frame 
rate of the longest delay to be signiﬁcantly lower than the frame rate reported (by 
benchmarking application). In lower frame rates when this effect may be apparent 
the moving video appears to stutter, resulting in a degraded gameplay experience in 
the case of a video game, even though the frame rate seems high enough to provide 
a smooth experience. Single-GPU conﬁgurations do not suffer from this defect in 
most cases and can in some cases output a subjectively smoother video compared 
to a multi-GPU setup using the same video card model. Micro stuttering is inherent 
to multi-GPU conﬁgurations using alternate frame rendering (AFR), such as Nvidia 
SLi and AMD CrossFireX but can also exist in certain cases in single-GPU systems. 
Variations in the rate of data input and processing speed can result in overﬂow, 
which can sometimes be prevented by allocating more than two buffers, in which 
case the system is referred to as a circular buffer. 
Subdivision surface—Subdivision smooths and adds extra resolution to curves and 
surfaces at display and/or render time. The renderer subdivides the surface until 
it’s smooth down to the pixel level. The smooth surface can be calculated from the 
coarse mesh as the limit of recursive subdivision of each polygonal face into smaller 
faces that better approximate the smooth surface. This lets one work with efﬁcient 
low-polygon models and only add the smoothing “on demand” on the graphics card

Appendix B: Deﬁnitions
385
(for display) or in the renderer. The tradeoff is that subdivision curves/surfaces take 
slightly longer to render. However, smoothing low-resolution polylines using curve 
subdivision is still much faster than working with inherently smooth primitives such 
as NURBS curves. 
Subsurface scattering (SSS)—Also known as subsurface light transport (SSLT), is 
a mechanism of light transport in which light penetrates the surface of a translucent 
object, is scattered by interacting with the material, and exits the surface at a different 
point. 
Sub-pixel Morphological Anti-aliasing (SMAA)—This ﬁlter detects edges in a 
rendered image and classiﬁes edge crossings into various shapes and shades, in an 
attempt to make the edges or lines look smoother. Almost every GPU developer has 
their own version of anti-aliasing. 
Super-ray—a grouping of rays within and across views, as a key component of a 
light ﬁeld processing pipeline. 
TAM—total available market. 
Taring and frame dropping—Vsync, where the monitor is synchronized to the 
powerline frequency, can cause the screen to be refreshed halfway through the process 
of a frame being output by the GPU, resulting in two or more frames being shown 
at once. 
TBP—The typical AIB power consumption, measured in watts. 
TDP (Thermal design power)—The maximum heat generated by the GPU, expressed 
in watts. 
Telecine—Telecine is the process of transferring motion picture ﬁlm into video. The 
most complex part of telecine is the synchronization of the mechanical ﬁlm motion 
and the electronic video signal. Normally, best results are then achieved by using a 
smoothing (interpolating algorithm) rather than a frame duplication algorithm (such 
as 3:2 pulldown, etc.). 
Tessellation shaders—A Tessellation Shader adds two new shader stages to the 
traditional model. Tessellation Control Shaders (also known as Hull Shaders) and 
Tessellation Evaluation Shaders (also known as Domain Shaders), which together 
allow simpler meshes to be subdivided into ﬁner meshes at run-time according to a 
mathematical function. The function can be related to a variety of variables, most 
notably the distance from the viewing camera to allow active level-of-detail scaling. 
This allows objects close to the camera to have ﬁne detail, while further away ones can 
have coarser meshes, yet seem comparable in quality. It also can drastically reduce 
mesh bandwidth by allowing meshes to be reﬁned once inside the shader units instead 
of down-sampling very complex ones from memory. Some algorithms can up-sample 
any arbitrary mesh, while others allow for “hinting” in meshes to dictate the most 
characteristic vertices and edges. Tessellation shaders were introduced in OpenGL 
4.0 and Direct3D 11.

386
Appendix B: Deﬁnitions
One cannot use tessellation to implement subdivision schemes that requires the 
previous vertex position to compute the next vertex positions. 
Texel—Acronym for TEXture ELement or TEXture pixEL—the unit of data which 
makes up each individually addressable part of a texture. A texel is the texture 
equivalent of a pixel. 
Texture Mapping—The act of applying a texture to a surface during the rendering 
process. In simple texture mapping, a single texture is used for the entire surface, 
no matter how visually close or distant the surface is from the viewer. A somewhat 
more visually appealing form of texture mapping involves using a single texture 
with bilinear ﬁltering, while an even more advanced form of texture mapping uses 
multiple textures of the same image but with different levels of detail, also known 
as mip mapping. See also “Bilinear Filtering,” “Level of Detail,” “Mip-map,” “Mip 
mapping,” and “Trilinear Filtering.” 
Texture Map—Same thing as “Texture.” 
Texture—A texture is a special bitmap image, much like a pattern, but which is 
intended to be applied to a 3D surface in order to quickly and efﬁciently create a 
realistic rendering of a 3D image without having to simulate the contents of the image 
in 3D space. That sounds complicated, but in fact it’s very simple. For example, if 
you have a sphere (a 3D circle) and want to make it look like the planet Earth, you 
have two options. The ﬁrst is that you meticulously plot each nuance in the land 
and sea onto the surface of the sphere. The second option is that you take a picture 
of the Earth as seen from space, use it as a texture, and apply it to the surface of 
the sphere. While the ﬁrst option could take days or months to get right, the second 
option can be nearly instantaneous. In fact, texture mapping is used broadly in all 
sorts of real-time 3D programs and their subsequent renderings, because of its speed 
and efﬁciency. 3D games are certainly among the biggest beneﬁciaries of textures, 
but other 3D applications, such as simulators, virtual reality, and even design tools 
take advantage of textures too. 
Tile Based Deferred Rendering (TBDR)—defers the lighting calculations until all 
objects have been rendered, and then it shades the whole visible scene in one pass. 
This is done by rendering information about each object to a set of render targets 
that contain data about the surface of the object this set of render targets is normally 
called the G-buffer. 
Tiled rendering—The process of subdividing a computer graphics image by a 
regular grid in optical space and rendering each section of the grid, or tile, sepa-
rately. The advantage to this design is that the amount of memory and bandwidth is 
reduced compared to immediate mode rendering systems that draw the entire frame 
at once. This has made tile rendering systems particularly common for low-power 
handheld device use. Tiled rendering is sometimes known as a “sort middle” archi-
tecture, because it performs the sorting of the geometry in the middle of the graphics 
pipeline instead of near the end.

Appendix B: Deﬁnitions
387
ToF—An acronym for Time of Flight. Used to refer to active sensors which measure 
distance to objects in a scene by emitting infra-red pulses and measuring the time 
taken to detect the reﬂection. These sensors simplify the computational task of 
producing a point cloud from image data but are more expensive and lower resolution 
than regular CMOS sensors. 
Tone mapping—A technique used in image processing and computer graphics to 
map one set of colors to another to approximate the appearance of high-dynamic-
range images in a medium that has a more limited dynamic range. 
Transcoding—Transcoding is the process of converting a media ﬁle or object from 
one format to another. Transcoding is often used to convert video formats (i.e., Beta to 
VHS, VHS to QuickTime, QuickTime to MPEG). But it is also used to ﬁt HTML ﬁles 
and graphics ﬁles to the unique constraints of mobile devices and other Web-enabled 
products. 
Trilinear Filtering—A combination of bilinear ﬁltering and mipmapping, which 
enhances the quality of texture mapped surfaces. For each surface that is rendered, 
the two mip-maps closest to the desired level of detail will be used to compute pixel 
colors that are the most realistic by bilinearly sampling each mip-map and then using 
a weighted average between the two results to produce the rendered pixel. 
Trilinear mipmapping—see above, trilinear ﬁltering. 
Transistors—The number of transistors in the GPU or chip. 
Truncation—An arithmetic operation which simply removes the fractional portion 
of a number in integer-fraction format to produce an integer, without regard for the 
magnitude of the fractional portion. Therefore, 2.99 and 2.01 truncated are both 2. 
See also “Rounding.” 
UDIM—an enhancement to the UV mapping and texturing workﬂow that makes 
UV map generation easier and assigning textures simpler. The term UDIM comes 
from U-Dimension and design UV ranges. UDIM is an automatic UV offset system 
that assigns an image onto a speciﬁc UV tile, which allows one to use multiple lower 
resolution texture maps for neighboring surfaces, producing a higher resolution result 
without having to resort to using a single ultra-high-resolution image. UDIM was 
invented by Richard Addison-Wood and came from Weta Digital (circa 2002). 
UI—User interface. 
UHD Alliance Premium Logo—High-end HDR TV requirements Rec.709, P3 or 
Rec.2020. 
Ultra HD Blu-ray—HDR disc format using HEVC, HDR10, and optionally Dolby 
Vision. 
UMA (Uniﬁed Memory Architecture)—When an IGP is employed in a PC, it 
needs memory (sometimes called a frame buffer). One of the beneﬁts of an IGP is 
the reduced cost realized by eliminating a separate frame buffer, and to replace that

388
Appendix B: Deﬁnitions
extra memory a portion of the PC’s main or system memory is used for the frame 
buffer. When that it done the organization is known as a uniﬁed memory architecture. 
USB, Universal Serial Bus—The Universal Serial Bus (USB) is a common inter-
face that enables communication between devices and a host controller such as a 
personal computer (PC). It connects peripheral devices such as digital cameras, mice, 
keyboards, printers, scanners, media devices, external hard drives and ﬂash drives. 
VAR—Value-added reseller. 
Vblank—In a raster graphics display, the vertical blanking interval (VBI), also 
known as the vertical interval or VBLANK, is the time between the end of the ﬁnal 
line of a frame or ﬁeld and the beginning of the ﬁrst line of the next frame. It is 
present in analog television, VGA, DVI and other signals. 
Vector—In computer programming, a vector quantity refers to any group of similar 
values which are grouped together and processed as a unit, either serially or in 
parallel. A vector can contain any number of elements. An example from computer 
graphics is the vector which describes the location of a point in four-dimensional 
space. P = x, y, z, w. Commonly referred to as Vec4 as it has four elements, it can 
be efﬁciently processed by a four-wide SIMD Vector Unit. 
Vector error correction model (VECM)—The basic ECM approach as described 
above suffers from a number of weaknesses. Namely it is restricted to only a single 
equation with one variable designated as the dependent variable, explained by another 
variable that is assumed to be weakly exogeneous for the parameters of interest. It 
also relies on pretesting the time series to ﬁnd out whether variables are I(0) or I(1). 
These weaknesses can be addressed through the use of Johansen’s procedure. Its 
advantages include that pretesting is not necessary, there can be numerous cointe-
grating relationships, all variables are treated as endogenous and tests relating to 
the long-run parameters are possible. The resulting model is known as a vector error 
correction model (VECM), as it adds error correction features to a multi-factor model 
known as vector auto-regression (VAR). 
Vector display/scope—a display used for computer graphics up through the 1970s. 
A type of CRT, like an oscilloscope. In a vector display, the image is composed of 
drawn lines rather than an array of pixels as in raster graphics. The CRT’s electron 
beam draws lines along an arbitrary path between two points, rather than following 
the same horizontal raster path for all images. Vector displays had no aliasing and 
were so accurate physical measurements could be taken from the screen. For that 
reason they were also called calligraphic displays. 
Vector graphics—refers to a method of generating electronic images using math-
ematical formulae to calculate the start, end, and path of a line. Images of varying 
complexity can be produced by combining lines into curved and polygonal shapes, 
resulting in inﬁnitely scalable objects with no loss of deﬁnition. 
Vector Unit (SIMD Vector Unit)—An Arithmetic Unit or Arithmetic Logic Unit 
which operates on one or more vectors at a time, using the same instruction for all 
values in the vector.

Appendix B: Deﬁnitions
389
Verilog/HDL—A “Hardware Description Language” is a textual representation of 
logic gates and registers. It differs from a programming language mainly in that 
it describes a parallel structure in space rather than a sequence of actions in time. 
Verilog is one of the most popular HDLs and resembles C or C++ in its syntax. 
VESA—Video Electronics Standards Association, a technical standards organization 
for computer display, PC, workstation and computing environments standards. The 
organization incorporated in California July 1989. 
VFX—Visual effects. 
VGA (Video Graphics Array)—VGA is a resolution and electrical interface stan-
dard original developed by IBM It was the defector display standard for the PC. VGA 
has three analog signals, red blue, and green (RGB) and uses an analog monitor. 
Graphics AIBs output analog signals. All CRTs and most ﬂat panel monitors accept 
VGA signals, although ﬂat panels may also have a DVI interface for display adapters 
that output digital signals. 
vGPU—An AIB with a powerful dGPU located remotely in the cloud or a campus 
server. 
Vignetting—A reduction of an image’s brightness or saturation at the periphery 
compared to the image center. 
VPNA—See Visual Processing Unit. 
Virtual Reality—Virtual Reality (VR) is a fully immersive user environment 
affecting or altering the sensory input(s) (e.g., sight, sound, touch, and smell) and 
allowing interaction with those sensory inputs by the user’s engagement with the 
virtual world. Typically, but not exclusively, the interaction is via a head-mounted 
display, use of spatial or other audio, and/or hand controllers (with or without tactile 
input or feedback). 
VR Video and VR Images—VR Video and VR Images are still or moving imagery 
specially formatted as separate left and right eye images usually intended for display 
in a VR headset. VR Video capture and subsequent display are not exclusive to 360° 
formats and may also include content formatted to 180° or 270°; content does not 
need to visually surround a user to deliver a sense of depth and presence. 
Vision Processing—Processing of still or moving images with the objective of 
extracting semantic or other information. 
VLIW (Very Long Instruction Word)—Often abbreviated to VLIW. A micropro-
cessor instruction which combines multiples of the lowest level of instruction words 
and presents them simultaneously to control multiple execution units in parallel. 
Voxel—A voxel is a value in three-dimensional space. Voxel is a combination of 
“volume” and “pixel” where pixel is a combination of “picture” and “element.” This 
is analogous to a texel, which represents 2D image data in a bitmap (also referred 
to as a pixmap). Voxels are used in the visualization and analysis of medical and

390
Appendix B: Deﬁnitions
scientiﬁc data. (Some volumetric displays use voxels to describe their resolution. 
For example, a display might be able to show 512 × 512 × 512 voxels.) Both ray 
tracing and ray-casting, as well as rasterization, can be applied to voxel data to obtain 
2D raster graphics to depict on a monitor. 
VGPR (Vector general purpose registers)— 
VPU (Vector Processing Unit)—a vector processor or array processor implements 
an instruction set containing instructions that operate on one-dimensional arrays of 
data called vectors. 
Today’s CPUs architectures have instructions for a form of vector processing 
on multiple (vectorized) data sets, typically known as SIMD (Single Instruction, 
Multiple Data). Common examples include Intel ×86’s MMX, SSE and AVX instruc-
tions, AMD’s 3DNow! Extensions, as well as Arm’s Neon and its scalable vector 
extension (SVE). 
VPU (Visual Processing Unit)—a Visual Processing Unit (VPU) is a silicon chip 
or IP block dedicated to Computational Photography and/or Vision Processing. 
A vision processing unit (VPU) is an emerging class of microprocessor; it is a 
speciﬁc type of AI accelerator, designed to accelerate machine vision tasks. Vision 
processing units are distinct from video processing units (which are specialized 
for video encoding and decoding) in their suitability for running machine vision 
algorithms such as CNN (convolutional neural networks). The name belies the real 
importance of the function and should include neural network accelerator, which 
results in the acronym VPNA. 
VR—Virtual reality. 
V-sync—Vertical synchronization of the monitor’s refresh rate based on the power 
line frequency, 60 or 50 Hz. 
VXGI is a new approach to computing a fast, approximate form of global illumination 
(GI) dynamically in real-time on the GPU. This new GI technology uses a voxel grid 
to store scene and lighting information, and a novel voxel cone tracing process to 
gather indirect lighting from the voxel grid. The purpose for VXGI is to run in 
real-time and doing full ray tracing of the scene is too computationally intense, so 
approximations are required. 
VXGI—Voxel Global Illumination (VXGI), developed by Nvidia, features one-
bounce indirect diffuse, specular light, reﬂections, and area lights. It is an advance-
ment in realistic lighting, shading and reﬂections. VGXI is a three-step process: 
Voxelization, Light injection, and ﬁnal gathering., and is employed in next-generation 
games and game engines. 
WCG—Wide Color Gamut—anything wider than Rec.709, DCI P3, Rec.2020 
—See wide color gamut. 
Wave—

Appendix B: Deﬁnitions
391
Wide color gamut—High Dynamic Range (HDR) displays a greater difference in 
light intensity from white to black, Wide Color Gamut (WGC) provides a greater 
range of colors. The wide-gamut RGB color space (or Adobe Wide Gamut RGB) is 
an RGB color space developed by Adobe Systems that offers a large gamut by using 
pure spectral primary colors. It is able to store a wider range of color values than 
sRGB or Adobe RGB color spaces. 
Also see HDR, and Color gamut. 
X Reality—X Reality (XR) is a general term to cover the multiple types of expe-
riences and technologies across VR, AR, MR and any future similar areas. All of 
these systems have in common some level of display technology (e.g., video, audio) 
mixed with a method to track where the user is looking or moving (e.g., up/down, 
side-to-side, turning around). How those systems work individually, and together, 
determines which of the more deﬁned experiences the product would be named – VR, 
AR, MR, or some future XR. 
Z-buffer—A memory buffer used by the GPU that holds the depth of each pixel 
(Z axis.) When an image is drawn, each (X-Y) pixel is matched against the z buffer 
location. If the next pixel in line to be drawn is below the one that is already there, 
it is ignored.

Index 
A 
A.A. Garel, 85 
Abrash, Michael, 294 
Accelerated Graphics Port (AGP), 55 
AccelGraphics, 218 
Acer, 251 
Acumos, 104 
Add-In Boards (AIBs), 22, 24 
Advanced CRT Controller (ACRTC), 73 
Advanced Research Projects Agency 
(ARPA), 50 
Afﬁne texture mapping, 238 
Agate, Mark, 85 
Ahisk, Yavuz, 222 
AIB model, 23 
Akeley, Kurt, 66 
Alchemist, 23 
Alliance Semiconductor, 285 
Amdahl’s law, 1 
AMD, Hollywood, 253 
Amiga Denise chip, 90 
Amiga Lorraine, 91 
Animation, 20 
Announcement date, 13 
Appian Graphics, 274 
Appleton, Steve, 176 
Arc, 22 
Architecture, Engineering, and 
Construction (AEC), 52 
Artiﬁcial Intelligence (AI), 1, 19 
ArtX, 274 
Ashton, Martin, 192 
Association of Computing Machinery 
(ACM), 343 
ATI 3D Rage, 267, 269 
Atkin, Phil, 53 
Autodesk, 31 
Autonomous Vehicles (AVs), 22 
B 
Baby, 4 
Baer, Ralph H., 10 
Banatao, Dado, 323 
Baston Dr. Johannes, 194 
Bedford, UK lab, UK, TI, 79 
Beese, Tom, 58 
Bell, Alan, 43 
Bellﬁeld, Charles, 261 
BenchMark, 222 
Bernstein, David, 179 
Bialek, Stan, 166 
Billingsley, Frederic Crockett, 10 
BitBlt, 73, 75 
Bitboys, 275 
Black Belt, Sega, 258 
Boich, Mike, 170 
Bose, Dev, 293 
Breadbox, 26 
Bresenham, Jack Elton, 7 
Briz, Bettina, 161 
Brooks, Fred, 40 
Brooktree, 103 
Brown, David J., 66 
Brown, Roger, 140 
Buchner, Greg, 251 
Bushnell, Nolan, 234 
C 
CagEnt, 251 
Calabria, Carl, 77
© The Editor(s) (if applicable) and The Author(s), under exclusive license 
to Springer Nature Switzerland AG 2022 
J. Peddie, The History of the GPU - Steps to Invention, 
https://doi.org/10.1007/978-3-031-10968-3 
393

394
Index
Calligraphic display, 3 
Campbell, Gordon (Gordie) A., 285, 322 
Carmack, John, 171, 294 
Cary, Frank, 100 
Category name, 22 
Cathode-Ray Tube (CRT), 3 
Catmull, Ed, 215 
CEG/DAC, 104 
Chen, Winston, 155 
Chip, 2 
Choy, Henry, 301 
Chroma key, 154 
Chromatics Research, 273 
Chunking, 190, 314, 316 
Cirrus Logic, 104 
Clark, Jim, 40, 63, 243, 250, 290 
Cobalt IGC, 154 
Code name, 23 
Cohen, Danny, 40, 60 
Coleman, Terry, 210 
Color key, 153 
Commercial-Off-The-Shelf (COTS), 167 
Commodore Amiga, 88 
Commodore Pet, 88 
Complementary 
Metal-Oxide-Semiconductor 
(CMOS), 69 
Complex Instruction Set Computer (CISC), 
24 
Compression and Decompression 
(CODEC), 21 
Computer-Aided Design (CAD), 1, 2 
Computer Automated Manufacturing 
(CAM), 53 
Computer Graphics (CG), 339 
Computer Graphics Interactive (CGI), 339 
Compute shaders, 342 
Compute Uniﬁed Device Architecture 
(CUDA), 14 
Compute Unit (CU), 337 
Conexant, 104 
Constructive Solid Geometry (CSG), 44 
Copetti, Rodrigo, 246 
Creative Labs, 227 
Criterion, 301 
CRT Controller (CRTC), 70, 102, 109 
Cryptocurrency Mining Processor (CMP), 
26 
Crypto mining, 22 
Curley, Joe, 130 
D 
Dally, William J., 41 
Datapoint, 100 
Datapoint 2200, 10 
Deconvolutional Neural Networks (DNN), 
19 
Deep Learning (DL), 19 
Deep Learning Super Sampling (DLSS), 
343 
Defense Advanced Research Projects 
Agency (DARPA), 50 
Del Rio, Joseph C., 181 
Delta processor, 225 
Delta T&L processor, 223 
Demetrescu Caltech architecture, 60 
Demetrescu, Stefan, 60 
Denise, Commodore Amiga, 91 
Department of Energy (DOE), 50 
Diamond Multimedia, 216 
Digital Content Creation (DCC), 1, 20 
Digital differential analyzer, 225 
Digital Equipment Corporation (DEC), 
222, 227, 259 
Digital-to-Analog Converter (DAC), 102 
Direct-Memory-Access (DMA), 172 
Direct Memory Execute (DIME), 188 
Discrete GPU (dGPU), 1, 26 
Dodd, Stephen, 6 
Dolphin, Nintendo, 251 
Domain shader, 342 
Dot-com bubble, 229 
Dot product, 63 
DRAM, 55 
Dreamcast, 198 
Dual In-line Package (DIP), 267 
Dunn, Jeff, 214, 219 
DuPont, Pierre, 47 
DuPont Pixe, 222 
Dynabook, 10 
Dynamic Pictures, 222, 227 
E 
Edelson, Steve, 104 
Eerola, Ville, 161 
EETimes, 251 
8514, 114 
Einstein, Fred, 84 
Eisenlohr, Jay, 169 
Eisler, Phil, 269 
Elsa GLoria-XL, 230 
Endicott, Tom, 179 
Engelbart, Douglas C., 33 
Engines vs. shaders, 334 
England, Nick, 35

Index
395
Enhanced Memory Chips (EMCs), 42 
Eras of GPUs, 341 
Erickson, Jim, 101 
Estridge, Don, 100 
Evans & Sutherland (E&S), 46, 214, 222 
Everett, Robert, 6 
Execution Unit (EU), 337 
Extended Graphics Array (XGA), 25, 147 
External GPU (eGPU), 25, 26 
Eyles, John, 56 
F 
Fahrenheit, 53, 308 
Fenney, Simon, 192 
Ferenz, Ramona, 6 
Fetter, William, 6 
Finch, Helen, 85 
First GPU, the, 13 
First-In, First-Out (FIFO), 172 
First PC-based 3D FPS game, 12 
First programmable pixel shader, 157 
First real-time computer, 5 
First 3D-polygonal game, 12 
First use of video displays, 5 
First workstation, 10 
Fisher, Andy, 132 
Flipper chip, 252 
Floating-point operations per second, 16 
Floating-point processor, 336 
Forrester, Jay, 5 
Frame buffer, 3 
Frame-Buffer Interface (FBI), 287 
Frames-per-second (fps), 343 
Free-D, 271 
Freiberger, Paul, 101 
Fuchs, Henry, 11, 39, 43, 44, 290 
Fujitsu FXG-1 Pinolite (FFP), 175, 336 
G 
GameCube, 251 
Gaming, 20 
GeForce 256, 13 
Gekko microprocessor, 250 
General Motors, 31 
General-Purpose GPU (GPGPU), 1 
Generation, 23 
Generations of GPUs, 341 
Geometry engine, 63 
Geometry processor unit, 13 
Geometry shader, 342 
Geometry translations, 336 
Gibbons‘, John J., 129 
Giga-Bytes per-second (GB/s), 133 
Giga-Operations Per Second (GOPS), 210 
GiGi, 229 
Glide, 294 
Glint, 13 
Glint Gamma processor, 222 
Glint SX, 223 
Glint 3D, 229 
Glint TX, 223 
Goldfeather, Jack, 44 
Gouraud shading, 46, 140 
Graphical Kernel System (GKS), 36 
Graphical Processor Unit (GPU), 13, 335 
Grimsdale, Charles, 46 
Grimsdale, Richard, 85 
Group of Pictures (GOP), 322 
Grove, Andy, 14 
GUI acceleration, 205 
Gupta, Sandeep, 178 
Gupta, Satish, 290 
Guttag, Karl, 78 
H 
Hannah, Marc, 63 
Hannah, Mark, 290 
Hanratty, Patrick, 31 
Harber, George, 298 
Hardware Abstraction Layer (HAL), 302 
Harry video editing system, 21 
Hashimoto, Kazuyuki, 240 
Henry, Drew, 48 
Hewlett Packard (HP), 46, 222 
High Color, 78 
High-Deﬁnition (HD), 23 
High-Performance Computer (HPC), 1 
Hitachi HD63484, 76 
Ho, Kwok Yuen (KY), 265 
Holdt, Terry, 323 
Hook, Brian, 294 
HP Artist, 205 
Huang, Jen-Hsun, 276 
Hudson, Verne, 6 
Hull shader, 342 
Hultquist. Jeff, 44 
Hursley Labs, IBM, 114 
I 
IBM, 222 
IBM CGA, 109 
IBM EGA, 109

396
Index
IBM 8514, 114 
IBM PGA, 112 
IDIIOM, 8 
Id Software, 172 
Ikonas, 35 
Image generation, 339 
Imagination Technologies, 193, 253 
Independent Hardware Vendor (IHV), 336 
Independent Software Vendor (ISV), 336 
Inﬁnite planes, 187 
Inﬁnite Surface-based algorithm, 
VideoLogic., 195 
Ingalls, Dan, 75 
Inmos, 54 
Inmos Transputer, 192 
Integrated GPU (iGPU), 1, 26 
Integrated Graphics Controller (IGC), 152 
Integrated Graphics Processor (IGP), 152 
Intel, 218 
Intel Clarksdale, 129 
Intel 82786, 123 
Intel i740, 125 
Intel i810, 126 
Intel iSBX 275, 122 
Intense 3D Wildcat, 231 
Intergraph, 222 
Intense3D, Intergraph, 231 
Irimajiri, Shoichiro, 258 
IRIS Indigo, 284 
I, Robot, 12 
Iwai, Makoto, 240 
J 
Jain, Paul, 142, 284 
Johnson, Brian, 290 
Joint Venture (JV), 21 
Jon Peddie Associates, 195 
K 
Kajiya, James T, 312 
Katzenstein Henry Sour, 103 
Kay, Alan, 10, 41 
Kent, Osman, 222 
Khronos, 343 
Kilburn, Tom, 4 
Kildall, Gary, 101 
Killebrew, Carrell, 79 
King, Kenneth, 8 
Kubota, 273 
Kuboto Graphics, 218 
Kuta, Charles, 66 
Kuta, Herb, 284 
Kutaragi, Ken, 235 
L 
Large-Scale Integrated (LSI) chips, 42 
Latta, John, 57 
Lazenbury, Ian, 56 
Lego, Sun Microsystems, 276 
Levinthal, Cyrus, 21 
Lewis, Mike, 177 
Light gun, 6 
Lincoln, Howard, 251 
Liquid Crystal Display (LCD), 3 
Lister, Paul, 85 
Lockheed, 222 
Lookup Table (LUT), 102 
Low-End Graphics Option (LEGO), 93 
Lowe, William C., 100 
LSI, Sea of Gates gates, 276 
LUT-DAC, 56, 104 
M 
Machine Learning (ML), 1, 19 
Machover, Carl, 8 
Macrosynergy, 274 
Magnavox Odyssey, 10 
Malachowsky, Chris, 276 
Marketing name, 23 
Mask, 75 
Massachusetts Institute of Technology 
(MIT), 8, 31 
Mathieson, John, 242 
Matrox SM 640, 67 
Maule, Rick, 214, 217 
McConnell, Ray, 53 
Mead, Carver, 33 
Mechanical and electrical CAD 
(MCAD/MCAE), 52 
Media Vision, 284 
Merritt, Rick, 251 
Merry, Diana, 75 
Mesh Shaders (MS), 343 
Micral, 100 
MicroChannel, 113 
Microsoft Windows Hardware Conference 
(WinHEC), 56 
Millions of Floating-Point Operation Per 
Second (MFLOPS), 138 
Mini DisplayPort · IEEE 1394, 25 
Mining GPU (mGPU), 22, 26 
Mitchell, Tom M., 19 
Mitsubishi, 215

Index
397
Mitsubishi Electronic America’s (MEA), 
215 
Molnar, Steve, 56 
Moore, Gordon, 33 
Moore, Peter, 261 
Moore’s law, 1 
Morein, Stephen, 177 
Motorola 6545 CRTC, 102 
MPD7220, 72 
Multi-Media Extension (MMX), 124, 195, 
286 
Multi-pass rendering, 314 
Multiple Application Graphics Integrated 
Circuit (MAGIC), 85 
Mutara, 283 
N 
Najda, Andrew, 166 
Nakayama, Hayao, 258 
Nanite, 15 
National Aeronautics and Space 
Administration (NASA), 45, 50 
National Institute of Standards and 
Technology (NIST), 50 
National Science Foundation (NSF), 50 
National Security Agency (NSA), 50 
National Television Standards Committee 
(NTSC), 78 
Necasek, Michal, 148 
NEC µPD7220, 122 
Next Gen Magazine, 251 
NextWave, 274 
Nicholas, Henry, 183 
Nintendo 64, 245 
Nintendo Switch, 253 
Nippon Electric Company (NEC), 69, 193, 
253 
Nonlinear video Editing (NLE), 21 
Nordlund, Petri, 154 
Nvidia, GPU, 250 
Nvidia NV1, 278 
Nvidia, Tegra, 253 
O 
Ohga, Norio, 237 
Open Multimedia Applications Platform 
(OMAP), 85 
Oyler, James R., 215 
P 
Paeth, Alan, 43 
Palette DAC, 104 
ParaScale, Intergraph, 230 
PCX1 (NEC/VL), 257 
PE-8 microcomputer, 11 
Pellucid, 284 
Perlin, Ken, 294 
Permedia, 227 
Personal Computer, 100 
Pestone, Alfred, 8 
Pesto, Steve, 233 
Peterson, Jim, 169 
Phase Alternation by Line (PAL), 78 
Picture Element (PEL), 147 
Pinolite, 175 
Pixel, 2 
Pixel, ﬁrst use, 10 
PixelFlow, 46, 125 
PixelFusion, 53 
Pixel Planes, 11, 39, 318 
PixelVision, 227 
Pizer, Steve, 40 
Plastic Quad Flat Pack (PQFP), 163 
Porter, Tom, 294 
Potashner, Ken, 327 
Poulton, John, 41, 43 
PowerVR, 193, 253 
PowerVR tiling architecture, 189 
Priem, Curtis, 94, 276 
Primal Computer, 277 
Processing Elements (PEs), 53, 254 
Professional Graphics Controller (PGC), 
112 
Programmable shaders, 342 
Project Chess, 101 
Proto GPU, 35 
Pulse-Code Modulation (PCM), 89 
Q 
Quadratic rendering, 278 
Quake, 172 
Quantel, 21 
R 
Radeon, 23 
Rambus, 55 
Rambus DRAM (RDRAM), 243 
Random Access Memory (RAM), 6 
Raspberry Pi Foundation, 184 
Rasterizer, 3 
Raster Operation Pipeline (ROP), 75 
Raster-scan, 3 
Ray, Alvy, 2

398
Index
Ray tracing, 255 
RealImage, 221 
Reality Coprocessor (RCP), 245 
Reality display processor, 246 
Reality Signal Processor (RSP), 246 
Real3D, 273 
Real-time Interactive Video and Animation 
accelerator, 319 
Real Vision, 221 
Redline, Rendition, 169 
Reduced Instruction Set Computer (RISC), 
24 
Register-Transfer Level (RTL), 210 
Rendering Polygon Accelerator (RPA), 300 
RenderMan, 12 
RenderWare, 301 
Rensselaer Polytechnic Institute’s (RPI) 
incubator, 177 
Return on Investment (ROI), 335 
Reyes, 318 
Reznick, Greg, 143 
RGBI, 111 
RISC-V, 343 
Riva 128, 319 
Robots, 22 
Rockwell Semiconductor, 104 
Ross, Douglas, 31 
S 
Sams, Jack, 101 
Samsung, 251 
San, Jez, 242 
Sastri, Bharat, 138, 284 
Sato, Hideki, 258 
Saul, George, 209 
Scan-Line Interleaving (SLI), 289 
Scan-lines, 3 
Sega arcade, 261 
Sega Genesis, 283 
Sega Saturn, 278 
Sellers, Scott, 284 
Semi-Automatic Ground Environment 
(SAGE) program, 5 
Set-Top Box (STB), 184 
SGI IRIS, 68 
Shader, 12, 24, 337 
SIGGRAPH, 343 
Silicon Graphics Inc (SGI), 45, 63, 66, 222 
Silicon Reality, 209 
Silverstone, Abbey, 66 
Single Instruction, Multiple Data (SIMD), 
19, 24, 42, 336 
SiS6204, the ﬁrst PC IGC, 153 
Siu, Francis, 322 
Sketchpad, 8 
Small-scale experimental machine, 4 
Smith, Alvy Ray, 215 
Smith, Ross, 284 
Software programmable resolution, 208 
Sound Blaster, 227 
Spea, 216 
Special Computer, APL Machine Portable 
(SCAMP), 100 
Sprites, 118 
Sproull, Bob, 40, 75, 290 
Stanley, Gerald W., 304 
Starr, Robert, 302 
Staudhammer, John, 34 
Stencil, 75 
Sterns, Ilene, 56 
S3 Texture Compression (S3TC), 325 
S3 ViRGE, 269 
ST Micro/SGS-Thomson (SGS), 270 
Streaming Multiprocessor (SM), 337 
Strimbu, Don, 143 
Sun Microsystem’s GX graphics 
accelerator, 92 
Supercomputer, 20 
Super VGA (SVGA), 83 
Sutherland, Ivan, 8, 31, 40, 214, 290 
Swain, Michael, 101 
Sweeney, Tim, 294 
System-on-a-Chip (SoC), 25, 27 
T 
Takeda, Genyo, 251 
Talisman, 311 
Talisman TREC, 316 
Tamasi, Tony, 290 
Tantrum, 212 
Tarolli, Gary, 284 
TAZ Core, 209 
3dfx Interactive, 285 
3Dlabs, 222 
3DO, 251 
3DR, 293 
3DRAM, 215 
TechFarm, 285 
Tesler, Larry, 75 
Tessellation, 342 
Texas Instruments Graphics Architecture 
(TIGA), 80 
Texture Mapping Engine (TREX), 287 
Texture Mapping Unit (TMU), 288

Index
399
Thacker, Charles P., 41 
Thunderbolt, 25 
Tibbits, Steve, 209, 216 
Tile-Based Deferred Rendering (TBDR), 
189 
Tiler chip, Talisman, 315 
Tiling, 311 
Tiling architecture, 189 
TI TMS34010, 78 
Titus, Jonathan, 10 
TMEM, 248 
Tootill, Geoff, 4 
Torborg, Jay, 87, 311 
Transcoding, 22 
Transform and Lighting (T&L), 2, 228 
Transform and Lighting(T&L) engine, 334, 
341 
Transputer, 54 
Trevett, Neil, 223, 233 
Trottier, Lorne, 184 
TrueColor, 78 
Tseng, Jack Hsiao Nan, 129 
Tseng Labs, 273 
Tuomi, Kai, 154 
Tuomi, Mika, 154 
Type 2 video subsystem, 147 
U 
Uniﬁed Memory Architecture (UMA), 152 
Uniﬁed shaders, 342 
Uniﬁed Shading Cluster array (USC), 337 
University of North Carolina (UNC), 11 
USB-C, 25 
User-Interface (UI), 111 
V 
Valentine, Don, 283 
Value-Added Reseller (VAR), 233 
Van Hook, Tim, 38, 251 
Variable Resolution Shaders (VRS), 343 
Vector display, 3 
Vectorscope, 10 
Vector writer, 3 
Vérité V1000, 169 
Versa Module Eurocard (VME), 76 
Vertex and pixel shaders, 341 
Very Large-Scale Integrated circuit (VLSI), 
11 
Very Long Instruction Word (VLIW), 342 
VESA BIOS Extensions (VBEs), 270 
VGA core, 270 
VideoDAC, 103 
Video Display Processor (VDP), 235, 300 
Video Electronics Standards Association 
(VESA), 83, 116 
Video Graphics Array (VGA), 115, 116 
VideoLogic, 189, 193, 253 
Virtual GPU (vGPU), 26 
Virtual Reality Graphics Engine (ViRGE), 
323 
Virtual Super Resolution (VSR), 343 
Von Tils, Valentin, 183 
Voodoo, 286 
VSIS, 216, 217 
W 
Walker, John, 31 
Wallace, James, 101 
Weinberg, Richard, 62 
Weitek 3132, 138 
Whirlwind computer, 5 
Whitton, Mary, 35 
Wii, 253 
Wildcat, Intergraph, 231 
Williams, Frederic C., 4 
Williams tube, 4 
Wing, Trevor, 194, 256 
Wolfenstein 3D, 12 
Wood, Lamont, 10 
Workstation, 203 
Y 
Yamamoto, Tatsuo, 258 
Yamauchi, Hiroshi, 243 
Yara, Ronald, 323 
Yassaie, Hossein, 189 
Z 
Zhu, Ben, 298 
ZiiLabs, 230 
Zitzne, Duane E., 52

