Nature Computational Science | Volume 3 | February 2023 | 149–163
149
nature computational science
https://doi.org/10.1038/s43588-022-00394-y
Article
Persistent spectral theory-guided protein 
engineering
Yuchi Qiu 
  1 & Guo-Wei Wei 
  1,2,3 
Although protein engineering, which iteratively optimizes protein fitness 
by screening the gigantic mutational space, is constrained by experimental 
capacity, various machine learning models have substantially expedited 
protein engineering. Three-dimensional protein structures promise 
further advantages, but their intricate geometric complexity hinders 
their applications in deep mutational screening. Persistent homology, 
an established algebraic topology tool for protein structural complexity 
reduction, fails to capture the homotopic shape evolution during filtration 
of given data. Here we introduce a Topology-offered Protein Fitness (TopFit) 
framework to complement protein sequence and structure embeddings. 
Equipped with an ensemble regression strategy, TopFit integrates the 
persistent spectral theory, which is a new topological Laplacian, and two 
auxiliary sequence embeddings to capture mutation-induced topological 
invariant, shape evolution and sequence disparity in the protein fitness 
landscape. The performance of TopFit is assessed by 34 benchmark 
datasets with 128,634 variants, involving a vast variety of protein structure 
acquisition modalities and training set size variations.
Protein engineering aims to design or discover proteins with desir-
able functions, such as improving the phenotype of living organ-
isms, enhancing enzyme catalysis and boosting antibody efficacy1. 
Traditional protein engineering approaches, including directed evo-
lution2 and rational design3, resort to experimentally screening an 
astronomically large mutational space, which is both expensive and 
time-consuming. As a result, only a small fraction of the mutational 
space can be explored even with high-throughput screenings. Recently, 
data-driven machine learning models have been developed to reduce 
the cost and expedite the process of protein engineering by establish-
ing the protein-to-fitness map (that is, fitness landscape) from sparsely 
sampled experimental data4,5.
The most successful machine learning models for the protein fit-
ness landscape are based on protein sequences using self-supervised 
models. There are two major types of self-supervised deep protein 
language model: local evolutionary and global evolutionary. Local 
evolutionary models infer mutational effects by utilizing homologs 
from the multiple sequence alignment (MSA)6 to model the mutational 
distribution of a target protein, such as Potts models7, variational 
autoencoders (VAEs)8,9 and MSA transformer10. Global evolutionary 
models learn from large sequence databases such as UniProt11 to infer 
natural selection paths. Natural language processing models have 
been adopted for extracting protein sequence information. For exam-
ple, Tasks Assessing Protein Embeddings (TAPE)12 constructed three 
models, including transformer, dilated residual network (ResNet) and 
long short-term memory (LSTM) architectures. Bepler13 and UniRep14 
are two LSTM-based models. Recently, large-scale protein transformer 
models trained on hundreds of millions of sequences have shown 
advanced performance in modeling protein properties, such as evolu-
tionary scale modeling (ESM)15 and ProtTrans16 models. In addition, the 
hybrid fine-tuned approaches combining both local and global evolu-
tionary data can further improve the models. For example, eUniRep is 
a fine-tuned LSTM-based UniRep model by learning from local MSAs17. 
The ESM model can be fine-tuned using either training data from down-
stream tasks15 or local MSAs18. The transformer-based Tranception 
scores mutations via not only a global autoregressive inference but also 
Received: 9 August 2022
Accepted: 22 December 2022
Published online: 20 February 2023
 Check for updates
1Department of Mathematics, Michigan State University, East Lansing, MI, USA. 2Department of Biochemistry and Molecular Biology, Michigan  
State University, East Lansing, MI, USA. 3Department of Electrical and Computer Engineering, Michigan State University, East Lansing, MI, USA.  
 e-mail: weig@msu.edu

Nature Computational Science | Volume 3 | February 2023 | 149–163
150
Article
https://doi.org/10.1038/s43588-022-00394-y
Topology concerns the invariant properties of a geometric object 
under continuous deformations (for example, filtration). Topological 
data analysis (TDA) offers an effective abstraction of geometric and 
structural complexity25,26. Persistent homology (PH), a main workhorse 
of TDA, was integrated with machine learning models to reduce the 
structural complexity in drug designs27 and protein–protein interac-
tions28. Filtration of a given point cloud may induce both homotopic 
shape evolution and changes in topological invariants, which allow 
multiscale analysis from TDA. However, PH captures only changes of 
topological invariants and is insensitive to the homotopic shape evo-
lution. The persistent spectral theory (PST), a multiscale topological 
Laplacian, was designed to overcome this drawback29. PST fully recov-
ers the topological invariants in its harmonic spectra and captures the 
homotopic shape evolution of data during the multiscale analysis29,30. 
Its advantages for drug design have been confirmed31.
In this Article, we introduce a PST-based topological machine 
learning model, called Topology-offered Protein Fitness (TopFit), 
to navigate the protein fitness landscape. We demonstrate that the 
machine learning models with the PST embedding show substantial 
improvement over those with sequence-based approaches. How-
ever, the PST-based models require high-quality structures, whereas 
sequence-based models can deal with more general cases without 
resorting to 3D structures. These two approaches naturally comple-
ment each other in machine learning-assisted protein engineering. 
To further extend the potential of the topology-based methods, we 
combine 18 regression models into an ensemble regressor to ensure 
robust and accurate predictions over a variety of protein engineering 
scenarios, ranging from small training sets, imperfect structures, vary-
ing experimental modalities, noisy data and so on. TopFit integrates 
complementary structure–sequence embeddings and an ensemble 
regressor to achieve better performance over existing methods on 34 
deep mutational scanning (DMS) datasets.
Results
Overview of TopFit
TopFit infers fitness landscape, a function that maps proteins to their 
fitness, from a small number of labeled data. Our framework consists 
of two embedding modules, that is, topological structure-based and 
residue sequence-based embeddings, and an ensemble regressor 
weighting many supervised learning models (Fig. 1).
In topological structure-based embedding, PST29 is employed to 
generate protein structure-based embedding. A structure optimization 
protocol generates and optimizes both wild-type and mutant structures 
(Methods). PST assesses the mutation-induced local structure changes 
near the mutational sites (Fig. 1a). Mathematically, PST constructs a fam-
ily of topological Laplacians via filtration, which delineates interatomic 
interacting patterns at various characteristic scales. The topological 
persistence30 and homotopic shape evolution of the local structure dur-
ing filtration can be assessed from harmonic and non-harmonic spectra 
of the Laplacians (that is, zero and non-zero eigenvalues), respectively 
(Fig. 1a). The site- and element-specific strategies are further introduced 
in assisting PST to capture critical relevant biological information, such 
as hydrogen bonds, electrostatics, hydrophobicity and so on (Methods). 
PST embeddings are generated on each pair of atoms for both local wild-
type and mutant structures, and they are concatenated for downstream 
supervised models (Methods).
In sequence-based embedding, both protein sequences and evo-
lutionary scores can be utilized (Fig. 1b). We tested nine sequence 
embedding methods and five evolutionary scores. Among them, the 
top-performing sequence embedding and evolutionary score are 
adopted in TopFit. Features from two embedding modules are concat-
enated and fed into the downstream ensemble regression to predict 
the protein fitness landscape. In particular, we combined PST embed-
ding, one sequence-based embedding and one evolutionary score 
to build TopFit. In protein engineering, the number of experimental 
a local retrieval inference utilizing MSAs19. One usage of evolutionary 
models is to generate the latent space representation for building a 
downstream fitness regression model4. Another is to generate a prob-
ability density to evaluate the likelihood that a given mutation occurs20. 
Such a probability likelihood, referred to as evolutionary score or 
zero-shot prediction, predicts the relative fitness of protein variants 
of interest in an unsupervised approach without the requirement of 
supervised label acquisition. The recent integration of local and global 
evolutionary models21 and the combination of evolutionary score and 
sequence embedding20 discovered the complementary roles of various 
sequence modeling approaches in building accurate fitness predictors.
In addition, three-dimensional (3D) structure- or geometry-based 
models play an important role in predicting the protein fitness land-
scape22. They offer more comprehensive and explicit descriptions of 
the biophysical properties of a protein and its fitness. With in-depth 
physical chemical knowledge, biophysical models such as FoldX23 and 
Rosetta24 are widely used to analyze protein fitness from its structure. 
However, the intricate structural complexity of biomolecules hin-
ders the application of first principles, such as quantum mechanics, 
to excessively deep mutational screening and, thus, the most suc-
cessful methods rely on their effective reduction of biomolecular  
structural complexity.
Sequences
b
Wild type
Mutant
a Three-dimensional structure
Tree
models
c
Ensemble regression
Harmonic spectra
Filtration
Non-harmonic spectra
Persistent spectral
theory (PST)
Deep protein
language models
Transformer
VAE
...
mLSTM
ANNs
Kernel
models
Fitness
landscape
+
+
Fitness
x2
x1
Sequence-based features
PST-based features
Top
models
Fig. 1 | Conceptual diagram of the TopFit method. a, A topological structure-
based module generates PST embedding. A wild-type 3D structure is obtained 
from the PDB or AF database. Both wild-type and mutant structures are 
optimized for local mutational-site structure extraction. PST embedding is 
generated from the spectra of persistent Laplacians where the harmonic and non-
harmonic spectra delineate the mutational atlas. b, Sequence-based embedding 
and evolutionary scores are generated by various deep protein language models. 
c, Ensemble regression recruits multiple regressors in predicting fitness. 
Features from two featurization modules are concatenated and fed into the 
downstream ensemble regression. The ensemble regression ranks and selects 
top-performing models from 18 regressors in three major classes: ANNs, kernel 
models and tree models. The hyperparameters of each regressor are optimized. 
The ensemble regression averages the predictions from top regressors to 
navigate the fitness landscape. Colors of frames and arrows indicate workflows in 
different modules: a structure-based module (green), b sequence-based module 
(orange), c regression module (blue).

Nature Computational Science | Volume 3 | February 2023 | 149–163
151
Article
https://doi.org/10.1038/s43588-022-00394-y
labeled data is typically small (that is, about 10–102) and accumulated 
at each round of screening. We utilize an ensemble regression model to 
provide a unified model with high accuracy and strong generalization 
for various sizes of training data. From a pool of multiple regressors, 
hyperparameters for individual regressors were first optimized by 
Bayesian optimization, and predictions from the top N models were 
averaged (that is, consensus) for fitness predictions32. Specifically, 
we employed 18 regressors of three major types: artificial neural net-
works (ANNs), kernel models and tree models (Fig. 1c, Methods and 
Supplementary Table 1). As the evolutionary scores contain relatively 
higher-level features than embeddings do, we treated them differently 
in ANNs and ridge regression to enhance their weights over other 
features (Methods). When multiple structures (for example, nuclear 
magnetic resonance (NMR) structure) are available for the same target 
protein, the ensemble regression can further average the predictions 
from individual structures to improve accuracy and robustness.
PST for delineating protein mutational atlas
PH25,26 has been widely applied to analyze the topology of point cloud 
data. However, it is not sensitive to homotopic shape evolution of data 
given by filtration. Recently, PST was proposed to reveal the homotopic 
shape evolution of data (Fig. 2)29. Here we briefly introduce and compare 
these two topological methods.
Atoms in a protein structure are described as a set of point clouds 
(Fig. 2a). A simplicial complex glues a set of interacting nodes, called 
simplexes (Fig. 2b), to construct a shape realization of the point cloud. 
A simplicial complex can be built from the intersection pattern defined 
by fixed-radius balls centered at the sample points. Čech complex, Rips 
complex and alpha complex are commonly used to construct simpli-
cial complexes25. The filtration process creates a family of simplicial 
complexes by continuously growing the radii of balls to establish a 
multiscale analysis (Fig. 2c).
PH is a popular approach for analyzing the family of simplicial 
complexes obtained from filtration. For an individual simplicial com-
plex, topological invariants, that is, Betti numbers, are calculated as the 
rank of the homology group. In particular, Betti-0, Betti-1 and Betti-2 
describe numbers of independent components, rings and cavities, 
respectively. PH analyzes the family of simplicial complexes and tracks 
the persistence of topological invariants, visualized by persistent 
barcodes25. The number of kth Betti bars at a fixed filtration param-
eter is the Betti-k for the corresponding simplicial complex (Fig. 2d). 
PH captures only topological changes and fails to detect homotopic 
geometric changes. For example, the two middle patterns in Fig. 2c 
are topologically equivalent (that is, no change in Fig. 2d) but have 
different geometric shapes.
Alternatively, the topology of point cloud data can be described by 
combinatorial Laplacians33, one kind of topological Laplacian. Loosely 
speaking, the 0-combinatorial Laplacian, L0, is a graph Laplacian that 
describes the pairwise interactions (that is, edges, 1-simplexes) among 
vertices (0-simplexes)34,35. The high-dimensional k-combinatorial 
Laplacian, Lk, describes interactions among high-dimensional com-
ponents (that is, k-simplexes) upon their adjacency associated with 
lower- and higher-dimensional simplexes (Methods). According to 
the combinatorial Hodge theorem36, the topological connectivity (or 
invariant) described by the combinatorial Laplacian is revealed by 
its kernel space or the harmonic spectra (that is, zero eigenvalues), 
whereas the non-harmonic spectra, such as the Fiedler value (that 
is, smallest non-zero eigenvalue), describes the so-called algebraic 
connectivity and homotopic shape information. Nonetheless, like 
homology, combinatorial Laplacian has limited descriptive power 
for complex data. We introduced PST to construct multiscale topo-
logical Laplacians29. Specifically, we used filtration to create a family 
of simplicial complexes at various scales and study their topological 
and geometric properties from a series of spectra of combinatorial 
Laplacians. PST not only recovers the full topological persistence of 
the PH30 but also deciphers the homotopic shape evolution of data. As 
shown in Fig. 2e, the number of harmonic zero eigenvalues are always 
the same as the number of persistent barcodes in Fig. 2d. In addition, 
the non-harmonic spectra reveal both topological changes and the 
homotopic shape evolution when there were no topological changes 
(Fig. 2e). PST comprehensively characterizes the geometry of an object 
from a family of frequencies manifesting evolving shapes induced by 
the filtration of data, which provides another illustration of the famous 
question “Can one hear the shape of a drum?”, raised by Mark Kac37.
Protein fitness benchmark overview
We compared the performance of machine learning models built on 
various featurization strategies. We benchmarked two topological 
structure-based embeddings, nine sequence-based embeddings and 
five evolutionary density scores on 34 benchmark deep DMS datasets 
across 27 proteins. In addition, we benchmarked TopFit, which inte-
grates one PST embedding, one sequence-based embedding and one 
evolutionary score in its concatenated feature library. 3D structures 
of the target protein at the domain of interest were obtained from the 
Protein Data Bank (PDB)22 or AlphaFold (AF)38. X-ray structures were 
Simplicial complexes and filtration
c
Homotopy
H0
H1
Topology
d
PH
Filtration parameter
b
0-simplex
1-simplex
2-simplex
3-simplex
a
Point cloud
Harmonic
L0
L1
Non-harmonic
Spetra
e
PST
Counts
Counts
0
16
0
0
1
2
0
16
0
0
2
0
30
0
7
0
1
2
10
0
0
16
0
0
16
0
1
Topology
+ homotopy
Fig. 2 | PST for topological persistence and homotopic shape evolution. a, 3D 
protein structure data are given as a point cloud. b, A simplex provides the basic 
building block of the simplicial complex. c, Balls with a fixed radius are placed at 
each point to generate a covering of the point cloud. The simplicial complex is 
constructed from the covering patterns of the point cloud. Filtration generates 
a family of simplicial complexes with a series of increasing radii. d, PH provides a 
topological representation from persistent barcodes at homology group Hi at the 
ith dimension. e, PST analyzes the spectra of persistent Laplacians. The harmonic 
spectra reveal topological persistence as PH does, and the non-harmonic spectra 
capture the homotopic shape evolution of data. The abscissa shows the range of 
spectra and the ordinate shows the number of spectra in the range.

Nature Computational Science | Volume 3 | February 2023 | 149–163
152
Article
https://doi.org/10.1038/s43588-022-00394-y
selected with the highest priority. Otherwise, NMR, cryo-electron 
microscopy (EM) or AF structures were employed depending on their 
availability and quality (Methods). For each dataset, 20% of the avail-
able labeled data were randomly selected as the test set in each run. 
The training set was randomly sampled within the remaining 80% of 
the data. We are particularly interested in small training data to mimic 
the circumstances in protein engineering. The fixed number of training 
data is picked as multiples of 24 ranging from 24 to 240 to mimic the 
smallest 24-well experimental assay. Low-N refers to the extremely small 
24 training data. In addition, we also tested 80/20 train/test split and 
fivefold cross-validation to have direct comparisons with other meth-
ods. To ensure reproducibility, 20 independent repeats are performed 
a
Counts
Regressor comparisons
0.3
0.4
0.5
Spearman ρ
Top models in ensemble
Kernels
Trees
ANNs
0
20
40
60
80
Percentage (%)
Ntrain
24
168
96
240
Ntrain = 240
0
5
10
15
20
25
19/34
8/34
9/34
3/34
6/34
3/34
1/34
1/34
Ntrain = 24
21/34
8/34
18/34
20/34
14/34
7/34
2/34
3/34
1/34
1/34
3/34
PH
ESM
eUniRep
PST
LSTM
UniRep
Transformer
(TAPE)
Bepler
ResNet
One-hot
Georgiev
b
d
e
c
PST is best?
Yes
No
20
30
40
50
Ntrain = 240
Coil (%)
P = 1 × 10–3
Counts
0
5
10
15
20
25
0.6
Spearman ρ
Ntrain
24
168
96
240
0.1
0.2
0.3
0.4
0.5
0.6
Average performance (34 datasets)
PH
ESM
eUniRep
PST
LSTM
UniRep
Transformer
(TAPE)
Bepler
ResNet
One-hot
Georgiev
Embedding
VAE
ESM
eUniRep
EVE
Evolutionary score
Tranception
Ntrain = 24
P = 0.01
20
30
40
50
Coil (%)
X-ray
NMR
AF
EM
PST is best?
Yes
No
Ensemble (all)
Ensemble (kernel)
Ensemble (tree)
Ensemble (ANN)
Ridge
PH
ESM
eUniRep
PST
LSTM
UniRep
Transformer
(TAPE)
Bepler
ResNet
One-hot
Georgiev
Ntrain
24
168
96
240
X-ray
NMR
AF
EM
Fig. 3 | Prediction from single embedding on fitness landscape measured by 
Spearman correlations. a, Line plots showing the average ρ over 34 datasets. 
Ensemble regression is used, except ridge regression for Georgiev and one-hot 
embeddings. Absolute values of ρ are shown for evolutionary scores. The width 
of the shading shows the 95% confidence interval from n = 20 independent 
repeats. b, Histograms showing the frequency that an embedding is ranked as  
the best across 34 datasets for training data size 24 (top) and 240 (bottom).  
For each dataset, the best embedding has average ρ over n = 20 independent 
repeats within the 95% confidence interval of the embedding with the highest 
average ρ. In a and b, structure-based embeddings include PH and PST.  
Sequence-based embeddings include ESM15, eUniRep17, Georgiev49, UniRep14, 
Bepler13 and three TAPE models (LSTM, transformer and ResNet)12. Evolutionary 
scores include DeepSequence VAE8, EVE9, Tranception19, ESM15 and eUniRep17.  
c, Boxplots showing the distribution of percentages of coils for protein structure 
for each dataset, where boxplots are presented on the 34 datasets classified 
into two classes depending on whether PST embedding is the best embedding. 
Scatter plots show same data as boxplots but for individual datasets. A one-sided 
Mann–Whitney U-test examines the statistical significance that two classes have 
different percentages of coils. Boxplots show five-number summary where the 
center line shows the median, the upper and lower limits of the box show the 
upper and lower quartiles, and the upper and lower whiskers show the maximum 
and the minimum by excluding ‘outliers’ outside the interquartile range. The 
training data size is 24 (top) and 240 (bottom). The classes that PST is ranked 
as the best or not the best have sample sizes n = 21 versus n = 13 (top) and n = 19 
versus n = 15 (bottom). P values are P = 0.01 (top) and P = 1 × 10−3 (bottom). d, Line 
plots showing comparisons between different regression for average ρ over 34 
datasets using PST embedding. Error bars show the 95% confidence interval from 
n = 20 independent repeats. e, Histograms showing the model occurrence in 
ensemble regression over 34 datasets using the PST embedding.

Nature Computational Science | Volume 3 | February 2023 | 149–163
153
Article
https://doi.org/10.1038/s43588-022-00394-y
for the fixed size of the training data, and 10 independent repeats are 
performed for fivefold cross-validation or 80/20 train/test split.
The computational predictions of mutational effects are typically 
employed to rank and prioritize candidate proteins for experimental 
screening in the next round. The primary goal is to assess the ranking 
quality of protein fitness. The Spearman rank correlation (ρ)7,8,17,18,20,21,39 
and the normalized discounted cumulative gain (NDCG)20,32,40 are two 
commonly used evaluation metrics in protein engineering (Methods).  
The Spearman correlation evaluates the overall ranking on all data 
where the high- and low-fitness proteins contribute equally. In con-
trast, NDCG quantifies the ranking quality with a higher weight on 
the high-fitness proteins. This may be more useful for the greedy 
search in selecting top samples. As the specific fitness is not avail-
able for the unsupervised evolutionary models, high-quality evo-
lutionary scores can either positively or negatively rank the fitness. 
Indeed, absolute values of these quantities are used to evaluate  
evolutionary scores.
Comparing PST embedding with sequence-based embedding
Here we compared various embeddings in building up machine learn-
ing models over 34 datasets. The results evaluated by Spearman cor-
relation are mainly reported in this section (Fig. 3 and Extended Data  
Figs. 1–3). Similar results evaluated by NDCG are also provided 
(Extended Data Figs. 1, 4 and 5).
For topological structure-based embeddings, the PST embed-
ding outperforms PH embedding on nearly all datasets for any size 
of training data (Extended Data Fig. 2). PST achieves a higher average 
Spearman correlation than PH (Fig. 3a). This indicates that the inclusion 
of non-harmonic spectra from PST provides critical homotopic shape 
information in learning the fitness landscape.
Comparisons of sequence-based embeddings are discussed in 
Supplementary Note 1. Overall, ESM achieves the best average perfor-
mance for large training data with over 168 data, and eUniRep and TAPE 
LSTM achieve the best average performance for small training data. 
Comparisons of evolutionary scores are discussed in Supplementary  
Note 2. Overall, DeepSequence VAE8 achieves the best average perfor-
mance on 34 datasets (Fig. 3a) and it ranks as the best model for the 
majority of datasets (21/34).
Comparing all embedding methods, the topological structure-
based PST embedding achieves the best average Spearman corre-
lation over 34 datasets for any size of training data. It outperforms 
the best sequence-based embedding for training data size Ntrain = 24, 
96, 168 and 240 with average differences Δρ = 0.02, 0.02, 0.03 and 
0.03, and P values from a one-sided Mann–Whitney U-test of 5 × 10−3, 
2 × 10−5, 3 × 10−8 and 4 × 10−8, respectively. Regardless of the size of 
training data, PST always has the highest frequency of being ranked 
as the best embedding across 34 datasets (Fig. 3b and Extended Data  
Fig. 3b). Categorizing datasets by their taxonomy, PST shows improve-
ment over sequence-based embeddings on eukaryote and prokaryote 
datasets, but no clear improvement is observed on human datasets 
(Supplementary Fig. 1).
Impact of structural data quality on PST-based model
PST extracts explicit biochemical information from 3D structures that 
is correlated to protein fitness, whereas sequence-based embeddings 
may provide less precise information by encoding them indirectly by 
learning from a large sequence database. It is important to understand 
the potential limitations of PST embedding. It is intuitive that the per-
formance of the PST-based model may depend on the quality of the 
structural data. The percentage of random coils in a target protein is a 
quantity for structural data quality, where the appearance of random 
coils, an unstable form of secondary structures, reduces the overall 
quality of the structure data. Whether PST embedding achieves the 
most accurate performance is correlated with the percentages of ran-
dom coils (Fig. 3c). Another quantity for structure data quality is the B 
factor, which quantifies the atomic displacement in X-ray structures, 
and it similarly affects PST performance (Extended Data Fig. 6).
Impact of ensemble regression on model generalization
In protein engineering, labeled data are iteratively collected from exper-
iments, resulting in tens or hundreds of training data. The ensemble 
regression by weighting top-performing models can be adapted to vari-
ous sizes of training data in a single model. We discuss the advantages 
of the ensemble strategy to enhance the model accuracy, robustness, 
adaptivity and scalability.
The ensemble strategy improves model accuracy (Fig. 3d). It out-
performs ridge regression on 33/34 datasets with Ntrain = 240 (Sup-
plementary Fig. 4); the only underperforming dataset, Calmodulin-1, 
exhibits poor performance ρ < 0.15 for both methods (Extended Data 
Fig. 2). The ensemble of a single class of models shows slightly lower 
Spearman correlation than the full ensemble model, except the low-N 
case (Fig. 3d and Supplementary Fig. 3).
We further performed analysis on the frequency of the top-per-
forming model occurrence in the ensemble to demonstrate the adap-
tivity and scalability of the ensemble regression. Kernel models with 
a relatively weak fitting ability may be more suitable for small train-
ing data to prevent overfitting. ANN models with powerful universal 
approximation ability may be more accurate with sufficient training 
data. The tree-based models avoid overfitting by selecting useful fea-
tures from the tree hierarchy independent of training data size. As a 
result, as the training data increase, kernel models have an initial large 
and decreasing occurrence, ANN models have opposite observations 
to kernel models and the occurrence of tree-based models remains 
stationary (Fig. 3e and Extended Data Fig. 7).
The ensemble strategy can also be used by averaging the predic-
tions from multiple structures to enhance the robustness against the 
uncertainty from structure data. The ensemble treatment on multiple 
NMR models improves the predictions from a single NMR model (Sup-
plementary Fig. 5). However, the ensemble has much higher compu-
tational costs, which are proportional to the number of structures.
TopFit for navigating fitness landscape
Among PST embedding, sequence-based embedding and evolutionary 
scores, we show that at least one strategy can provide accurate predic-
tions regardless of the quality of structural data and the number of 
training data. TopFit combines these three complementary strategies 
in navigating the fitness landscape.
We first built TopFit using the PST embedding, aided by DeepSe-
quence VAE evolutionary score and ESM embedding. With 240 training 
data, TopFit outperforms DeepSequence VAE score, ESM embedding 
Fig. 4 | Comparisons between TopFit embeddings and other methods for 
fitness prediction. a–c, All supervised models use 240 labeled training data. 
The results were evaluated by Spearman correlation ρ. DeepSequence VAE 
takes the absolute value of ρ. In a and b, TopFit combines PST embedding, ESM 
embedding and DeepSequence VAE score. The average ρ from 20 independent 
repeats is shown. The 34 datasets are categorized by the structure modality 
used: X-ray, NMR, AF and EM. a, Dot plots showing the results across the 34 
datasets. See Supplementary Data 1 for the full list of datasets and references. 
b, Dot plots showing pairwise comparison between TopFit and one method at 
each plot. Medians of difference for average Spearman correlation Δρ across 
all datasets are shown. One-sided rank-sum test determines the statistical 
significance that TopFit has better performance than VAE score, ESM embedding 
and PST embedding for n = 34 datasets with P values of 0.087, 0.083 and 2 × 10−7, 
respectively. c, Comparisons for extrapolation that predicts datasets with 
multiple mutations using the single-mutation data. Bars show average values 
and error bars show 95% confidence interval from n = 20 independent repeats. 
Scatter plots show ρ from each repeat.

Nature Computational Science | Volume 3 | February 2023 | 149–163
154
Article
https://doi.org/10.1038/s43588-022-00394-y
and PST embedding on the majority of datasets with median differ-
ences in Spearman correlation of Δρ = 0.087, 0.083 and 0.062, respec-
tively (Fig. 4a,b). Similar observations were found for TopFit using PST 
embedding, aided by VAE score and eUniRep embedding (Extended 
Data Fig. 8). The concatenated featurization endows TopFit with advan-
tages over other embedding strategies on most datasets regardless of 
training data sizes (Extended Data Figs. 9 and 10). An exception is that 
VAE is more accurate for the low-N case. Similarly, the concatenated 
ESM
PST
VAE
TopFit (VAE + PST + ESM)
X-ray
NMR
AF
EM
Spearman ρ
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
0
1
0
ESM
TopFit (VAE + PST + ESM)
β-lactamase, Stiﬀler 2015
β-lactamase, Firnberg 2014
GB1, Olson 2014
TIM Barrell (T. thermophilus), Chan 2017
β-lactamase, Jacquier 2013
YAP1 (WW domain 1), Araya 2012
Ubiquitin, Roscoe 2013
avGFP, Sarkisyan 2016
DNA methylase Haelll, Rockah-Shmuel 2015
PSD95 (PDZ domain), McLaughlin 2012
TIM Barrell (S. solfataricus), Chan 2017
Kanamycin kinase APH(3’)-II, Melnikov 2014
Ubiquitin, Roscoe 2014
Aliphatic amidase, Wrenbeck 2017
GTPase HRas, Matreyek 2017
Ubiquitin, Mavor 2016
TIM Barrell (T. maritima), Chan 2017
SUMO-conjugating enzyme UBC9, Roth 2017
β-lactamase, Deng 2012
Thiopurine S-methyltransferase, Matreyek 2019
Levoglusosan kinase (stabilized), Klesmith 2015
Small ubiquitin-related modifier 1, Roth 2017
Mitogen-activated protein kinase 1, Brenan 2016
Levoglucosan kinase, Klesmith 2016
Thiamin pyrophosphokinase 1, Roth 2017
Calmodulin-1, Roth 2017
Translation initiation factor IF-1, Kelsic 2016
BRCA 1 (RING domain) (e3), Starita 2015
UBE4B (U-box domain), Starita 2013
BRCA 1 (RING domain) (y2h), Starita 2015
GAL4 (DNA-binding domain), Kitzman 2015
PTEN, Matreyek 2019
HSP90 (ATPase domain), Mishra 2016
PABA (RRM domain), Melamed 2013
1
0
1
0
PST
a
b
1.6 Å
2.4 Å
3.2 Å
4.0 Å
4.8 Å
Resolution
(X-ray and EM only)
Method
X-ray
NMR
AF
EM
1
0
VAE
Median ∆ρ = 0.087
P = 3 × 10–7
Median ∆ρ = 0.062
P = 2 × 10–7
Median ∆ρ = 0.083
P = 2 × 10–7
TopFit
(VAE + PST + ESM)
c
0.4
0.5
0.6
0.7
0.8
Spearman ρ
avGFP, Sarkisyan 2016
GB1, Olson 2014
Extrapolation (trained on single mutation)
TopFit
(VAE + PST + eUniRep)
PST
ESM
eUniRep
LSTM
VAE

Nature Computational Science | Volume 3 | February 2023 | 149–163
155
Article
https://doi.org/10.1038/s43588-022-00394-y
features augmented with other evolutionary scores also achieve sub-
stantial improvement (Supplementary Figs. 6 and 7).
Furthermore, we carried out an extrapolation task where the 
model trained on single-mutation variants is used to navigate datasets 
including multiple mutational variants, which are avGFP41 and GB142 
datasets. Interestingly, we employed ridge regression due to its better 
performance than ensemble regression for extrapolation (Supplemen-
tary Fig. 8). PST embedding achieves the best performance over other 
sequence-based embeddings in both datasets. TopFit further improves 
the performance (Fig. 4c). An exception was found for the GB1 dataset, 
where inclusion of poorly performing eUniRep embedding and VAE 
score underperforms PST embedding alone (Fig. 4c).
Next, we compared TopFit with two existing regression models 
for fitness predictions (Fig. 5). One is the augmented VAE model, which 
is the best reported model combining sequence embedding and evo-
lutionary score20. The other is ECNet, which combines both local and 
global evolutionary embeddings21. TopFit for comparisons uses PST 
embedding aided by VAE score and ESM embedding. Due to the high 
computational costs for the large training data size (that is, 80/20 train/
test split and fivefold cross-validation), we carry out TopFit on only  
27 single-mutation datasets using X-ray or AF structures. For low- 
N case, the augmented VAE model outperforms TopFit on 33/34 data-
sets by largely retaining accuracy from VAE. When more training data 
are available, TopFit becomes more accurate, where it outperforms 
the augmented VAE model in at least 19/34 datasets. For the 80/20 
train/test split, TopFit completely outperforms the augmented VAE 
model on 25/27 datasets, and the average difference on Spearman for 
the other two underperforming sets is relatively low (Supplementary 
Tables 4 and 5). Compared with ECNet, TopFit again shows the better 
performance on 25/27 datasets. Underperforming TopFit on the thia-
min pyrophosphokinase dataset shows a relatively low average margin 
with ECNet Δρ = −0.012. The other underperforming dataset is levoglu-
cosan kinase with a relatively large average margin Δρ = −0.048 (Sup-
plementary Tables 4 and 5). Interestingly, TopFit outperforms ECNet 
on a dataset generated from identical experiments using a similar but 
stabilized protein (that is, levoglucosan kinase (stabilized)). A trade-
off between thermal stability and catalytic efficiency was discovered 
for the levoglucosan kinase protein43, and the unstablized dataset was 
more difficult to predict for all methods (Extended Data Fig. 2). The 
quality of the structure data is more sensitive to thermal stability than 
sequence data, which leads to the outperformance of TopFit over ECNet 
on the stablized set and underperformance on the unstablized set.
Discussion
Protein structures are intricate, diverse, variable and adaptive to the 
interactive environment and thus are elusive to an appropriate theo-
retical description in deep mutational screening. PST improves PH in 
offering ultimate abstraction of geometric and structural complexity 
PABP (RRM domain), Melamed 2013
****
***
NS
*
*
NS
NS
NS
*
****
**
NS
****
NS
**
*
NS
NS
*
****
*
NS
NS
**
**
**
*
***
****
****
*
NS
NS
**
****
NS
NS
NS
**
**
**
NS
NS
NS
NS
NS
*
NS
NS
***
NS
****
NS
*
**
*
NS
NS
NS
NS
NS
*
****
*
NS
NS
****
NS
****
NS
NS
NS
****
***
****
***
NS
***
NS
**
NS
**
NS
****
***
****
NS
**
***
*
*
***
**
NS
**
*
****
*
NS
NS
****
*
****
NS
NS
NS
****
****
***
***
NS
***
NS
**
NS
**
*
****
****
****
NS
**
*
**
****
****
***
NS
****
****
****
NS
NS
*
****
**
****
*
****
NS
****
NS
**
***
****
****
****
****
**
**
*
****
****
****
***
****
****
NS
NS
**
**
***
****
UBE4B (U-box domain), Starita 2013
BRCA 1 (RING domain) (y2h), Starita 2015
BRCA 1 (RING domain) (e3), Starita 2015
TraNSlation initiation factor IF-1, Kelsic 2016
GB1, Olson 2014
avGFP, Sarkisyan 2016
Mitogen-activated protein kinase 1, Brenan 2016
β-lactamase, Jacquier 2013
Thiopurine S-methyltraNSferase, Matreyek 2019
DNA methylase HaeIII, Rockah-Shmuel 2015
PSD95 (PDZ domain), McLaughlin 2012
Calmodulin-1, Roth 2017
Ubiquitin, Mavor 2016
TIM barrell (T. thermophilus), Chan 2017
Kanamycin kinase APH(3')-II, Melnikov 2014
β-lactamase, Firnberg 2014
β-lactamase, Stiﬀler 2015
TIM barrell (S. solfataricus), Chan 2017
SUMO-conjugating enzyme UBC9, Roth 2017
TIM barrell (T. maritima), Chan 2017
YAP1 (WW domain 1), Araya 2012
Ubiquitin, Roscoe 2014
GTPase HRas, Matreyek 2017
Ubiquitin, Roscoe 2013
Small ubiquitin-related modifier 1, Roth 2017
Levoglucosan kinase (stabilized), Klesmith 2015
Aliphatic amidase, Wrenbeck 2017
β-lactamase, Deng 2012
Thiamin pyrophosphokinase 1, Roth 2017
Levoglucosan kinase, Klesmith 2016
GAL4 (DNA-binding domain), Kitzman 2015
PTEN, Matreyek 2019
HSP90 (ATPase domain), Mishra 2016
X-ray
NMR
AF
EM
≤–0.10
–0.05
0
0.05
≥0.10
Relative improvement
of Spearman (ρ) 
NS
*
**
***
****
P > 0.05
P ≤ 0.05
P ≤ 0.01
P ≤ 0.001
P ≤ 0.0001
24
96
168
240
80/20
split
Fivefold
CV
Augmented VAE
(ref. 20)
ECNet
(ref. 21)
Median ∆ρ
–0.059
0.002
0.021
0.025
1/34
19/34
24/34
25/34
Frequency of TopFit outperformance
0.044
25/27
0.035
25/27
Fig. 5 | Comparisons between TopFit and other regression models for fitness 
predictions using Spearman correlation. Heat map showing the relative 
improvement in ρ achieved by TopFit. TopFit is compared with the augmented 
VAE model20 for training data sizes of 24, 96, 168 and 240, and an 80/20 train/test 
split. TopFit is compared with ECNet21 using five-fold cross-validation (FiveFold 
CV). A one-side Mann–Whitney U-test examines the statistical significance that 
two methods being compared have different ρ. The P values are annotated in 
the heat map as follows: NS, not significant with P > 0.05; *P ≤ 0.05; **P ≤ 0.01; 
***P ≤ 0.001; ****P ≤ 0.0001. TopFit has n = 20 independent repeats for 24, 96, 
168 and 240 training data, and n = 10 for 80/20 train/test split and FiveFold CV. 
Augmented VAE was reproduced by us with n = 20 independent repeats. Results 
for ECNet were obtained from their work with average ρ, and indeed P values 
were not reported. See Supplementary Data 1 for the full list of datasets and 
references.

Nature Computational Science | Volume 3 | February 2023 | 149–163
156
Article
https://doi.org/10.1038/s43588-022-00394-y
of proteins. Our PST embedding for dimension 0 had a relatively high 
dimension to provide the most critical information with basic connec-
tivity between vertices. For dimensions 1 and 2, our PST embedding is 
relatively crude to retain essential information and keep the feature 
dimension low under the element- and site-specific strategies. The low-
dimensional features can better accommodate with machine learning 
models in avoiding overfitting issues for small training data size, as well 
as reducing computational costs. Although the stable representations, 
such as persistent landscape44 and the persistent image45 for PH and 
a potential informative representation of non-harmonic persistent 
spectra may provide more enriched information, they typically gener-
ate a large number of features (Supplementary Note 6). How to han-
dle potential overfitting from these representations in biomolecular 
systems remains an interesting issue. The extraordinary performance 
of PST in predicting protein fitness opens a door for its further appli-
cations in a vast variety of interactive biomolecular systems, such as 
drug discovery, antibody design, molecular recognition and so on.
The quality of protein structures largely affects the performance 
of the PST-based model (Fig. 3c). The biotechnology used for structure 
determination is another important factor that affects structural data 
quality and subsequently impacts the PST-based model performance. 
For example, X-ray structures are generally more reliable than struc-
tures obtained by other modalities, such as NMR and cryo-EM. Recently, 
AF structures have become a viable alternative to experimental struc-
tural data. The AF structure achieves similar accuracy to the single NMR 
structure, while ensemble techniques using multiple NMR structures 
improves the performance (Supplementary Fig. 5). The combination 
of structure- and sequenced-based embeddings gives rise to robust 
predictions, even for low-quality structures. PST delineates the specific 
geometry and topology of a mutation, while sequence-based featuriza-
tion captures the rules of evolution from a huge library of sequences. 
This complementarity is valuable and generalizable to a wide variety 
of other biomolecular problems.
The ensemble of regressors has better generalizability by inherit-
ing advantages from various classes of models. With larger training 
data, the deeper and wider neural networks or more sophisticated 
deep learning architectures such as attention layers21 and convolution 
layers32 may further improve the performance. The random split of 
training and testing sets mainly discussed in this work may be more 
important for the late stage of protein engineering with sufficient train-
ing data. However, the initial stage may require model generalization 
for extrapolations. One extrapolation task is to predict multiple muta-
tions from single-mutation data. A more effective supervised model 
is under discussion (Supplementary Fig. 8). Another extrapolation 
task is to predict mutations at unseen sites. TopFit promises limited 
improvement over evolutionary scores (Supplementary Figs. 9–13 
and Supplementary Note 5). In general, predicting out-of-distribution 
data may violate the nature of supervised models, and how to build a 
more accurate supervised model for this task is interesting. Nonethe-
less, the unsupervised evolutionary scores may be more effective for 
extrapolation at the early protein engineering stage with insufficient 
training data (that is, low-N case). In practice, the calibration of the 
balance between exploitation and exploration is essential for protein 
engineering. The commonly used supervised models including TopFit 
are designed only for exploitation. Models such as Gaussian process46 
and hierarchical subspace searching strategy40,47 may couple with 
TopFit mixture features to balance this trade-off.
TopFit provides a general framework to build supervised protein 
fitness models by combining PST structural features with sequence-
based features. Arbitrary sequence-based embedding and evolution-
ary scores can be used. TopFit can be continuously improved by the 
quickly evolving state-of-art sequence-based models. In this work, we 
mainly tested TopFit with single type of score, especially, DeepSequence 
VAE. Tranception score may be more powerful than VAE for datasets 
with low MSA depths or viral protein datasets19. The equipment of any 
evolutionary score can largely enhance model generalization and accu-
racy (Supplementary Fig. 6). Interestingly, inclusion of multiple scores 
can promise further improvement (Supplementary Note 3 and Sup-
plementary Fig. 7). Furthermore, one may combine multiple structure 
data for ensemble predictions for improvement (Supplementary Fig. 5). 
However, all these approaches have additional computational costs. The 
combined features tested in this study provide the minimal models that 
combine models built on distinct data resources such as local homolo-
gous sequences, large-scale sequence data and 3D structure data.
Methods
Datasets
For all DMS datasets, we followed the convention in EVmutation7 and 
DeepSequence8 to exclude data with mutational positions not covered 
by MSA.
The complete GB1 dataset includes almost all single (1,045 
mutants) and all double (535,917 mutants) mutations at 55 positions42. 
Here we used all single mutations and double mutations with high 
epistasis selected in a previous study (its Supplementary Table 2)48. The 
MSA of GB1 covers the last 54 residues, and mutations that include the 
positions outside MSA are excluded. As a result, the GB1 dataset used 
here includes 2,457 mutations.
The avGFP dataset41 includes 54,025 mutations. We followed pre-
vious studies by picking up mutations at positions that have more 
than 30% gaps in MSA to focus on regions with sufficient evolutionary 
data17,20. Positions 15–150 were selected out of 237 positions, resulting 
in 7,775 mutants with numbers of mutations between one and nine.
The other 32 datasets were selected from DeepSequence8. We 
searched available 3D structural data from UniProt11 and the PDB 
database22 for each dataset. If a PDB entry largely covered the muta-
tional domain, the PDB structure and the corresponding dataset were 
selected. A full list of the datasets can be found in Supplementary Data 1.
Multiple sequence alignments
MSAs search the homologous sequences of the wild-type sequence in 
each DMS dataset. For the 32 DeepSequence datasets, we adopted the 
MSAs used in the original study8. MSAs of avGFP were obtained from 
ref. 20. The MSAs for GB1 were generated by EvCoupling server6 by 
searching against the UniRep100 database with bitscore 0.4, resulting 
in 387 sequences that cover 54/55 positions. We also explored different 
bitscores for GB1 in the MSA search, but a larger bitscore leads to fewer 
sequences and a smaller bitscore fails to cover 70% of positions of the 
wild-type sequence.
Sequence-based models
Multiple sequence-based evolutionary models were used in this study 
to generate sequence embeddings or evolutionary scores.
Constant embedding. Two constant embeddings, one-hot and Geor-
giev, were used in this study. The one-hot embedding is an uninforma-
tive categorical strategy without biochemical information. We consider 
20 canonical amino acids and a sequence of length L is encoded as a 
20L vector. Georgiev embedding49 provides a 19-dimensional repre-
sentation for over 500 physicochemical quantities of amino acid in the 
AAIndex database50. A sequence is encoded as a 19L vector.
ESM transformer. The ESM-1b transformer15 is a transformer model51,52 
that learns 250 million sequences using a masked filling procedure. Its 
architecture contains 34 layers with 650 million parameters.
The ESM transformer was used to generate sequence embedding. 
At each layer, the ESM model encodes a sequence with length L into a 
matrix with dimensions 1,280 × L by excluding the start and terminal 
tokens. In this study, we took the sequence representation from the 
final (34th) layer and averaged over the axis for sequence length, result-
ing in a 1,280-component vector.

Nature Computational Science | Volume 3 | February 2023 | 149–163
157
Article
https://doi.org/10.1038/s43588-022-00394-y
The ESM transformer was also used to generate evolutionary 
score to predict fitness. Specifically, the difference in conditional log-
likelihoods between the mutated amino acids and the wild-type amino 
acids is taken as the evolutionary score. Given a sequence with length 
L, s = s1s2 ... sL, the masked filling model generates probability distribu-
tions for amino acids at masked positions. For a masked ith position 
residue si, the distribution is given by a conditional probability P(si|s(−i)), 
where s(−i) is the remaining sequence excluding the masked ith position. 
To reduce the computational cost, the pseudo-log-likelihoods (PLLs) 
estimate the exact log-likelihood of a given sequence:
PLL(s) =
L
∑
i=1
log P(si|s(−i)).
(1)
The evolutionary score is approximated by the difference in PLLs 
between wild-type (w) and mutant (m) sequences:
PLL(m) −PLL(w) =
L
∑
i=1
log P(mi|m(−i)) −log P(wi|w(−i))
≈
∑
i ∈V,
V = {i|mi ≠wi}
log P(mi|m(−i)) −log P(wi|w(−i))
(2)
The evolutionary score can be approximated by the second line in the 
above equation by summing up the difference between mutant and 
wild-type PLLs at mutated positions as the PLLs at non-mutated posi-
tions have similar values. We conducted this approximation following 
previous work20,32 to save computational cost.
UniRep and eUniRep. UniRep14 is an LSTM model53 that learns from  
27 million sequences from UniRef50 data. To adapt the UniRep to 
specific protein families, the ‘evo-tuning’ procedure is introduced to 
fine-tune the models using evolutionary sequences (that is, homolo-
gous sequences from MSA). The resulting eUniRep17 model includes 
more specific information for the target sequence.
These LSTM models can generate a sequence embedding by 
encoding a sequence into a 1,900 × L matrix using the final hidden 
layer. The sequence embedding with 1,900 dimensions is obtained by 
averaging over the sequence length axis.
In addition, the evolutionary score can be generated. The LSTM 
models can also be viewed as sequence distribution. The evolutionary 
score is also given by the log-likelihood. Given the first i amino acids 
of a sequence, the conditional probabilities of the next amino acid 
P(si+1∣s1 ... si) is parameterize by the LSTM model. The sequence prob-
ability is given by
P(s) =
L
∏
i=1
P(si+1|s1 ⋯si).
(3)
The log-likelihood is then taken as the evolutionary score.
In the fine-tuning procedure, we followed the protocols and hyper-
parameters used in previous work17,20. The evolutionary sequences 
from MSA were split into 80% training set and 20% validation set during 
the fine-tuning. The learning rate is taken as 1 × 10−5 and the number 
of epochs is 10,000. We performed the fine-tuning procedure if the 
fine-tuned model was not performed by previous work20.
TAPE embedding. The TAPE models contain multiple pre-trained 
protein language models to generate sequence embeddings12. Three 
models were created by TAPE: ResNet54, transformer51,52 and LSTM53. 
Two existing LSTM-based models, UniRep14 and Bepler13, were also 
implemented. The sequence embeddings generated from the latent 
space representation have dimensions of 256, 512, 2,048, 100 and 
1,900 for ResNet, transformer, LSTM, Bepler and UniRep, respectively.  
Both TAPE and the original UniRep implemented the UniRep model. 
In this study, we used only the one from the original implementation.
DeepSequence VAE evolutionary score. DeepSequence VAE8 is a 
variational autoencoder model that learns sequence distribution from 
MSA. As the log-likelihood is intractable, the evidence lower bound 
(ELBO) is used to estimate the sequence log-likelihood to predict the 
sequence mutational effect. Followed by the original DeepSequence, 
we trained five VAE models with different random seeds and generated 
400 ELBO samples for each model, and the average of all 2,000 ELBO 
samples was used. In this study, we only performed DeepSequence on 
avGFP, GB1 and two BRCA1 datasets. Other VAE scores were obtained 
from data provided in DeepSequence8.
Tranception evolutionary score. Tranception19 is an advanced self-
supervised transformer-based protein language model with 700 million 
parameters that promotes specialization across attention heads for 
enhanced protein modeling. The pre-trained model rank mutations 
using PLLs followed by the protocol in ESM score (equation (5)). Tran-
ception employs a special bidirectional scoring to have improvement 
over the unidirectional scoring used in the traditional transformer-
based model. It uses training data augmented by all sequences and 
their reverse. The evolutionary score from the autoregressive inference 
mode, log PA(s), is obtained by averaging PLL scores for sequences in 
both forward and reverse order. In addition to the global autoregres-
sive score, Tranception further proposed a local evolutionary score, 
retrieval inference, that is based on the empirical distribution of amino 
acids in MSAs. It calculates a local log-likelihood, log PR(s), from retrieval 
inference for sequence s. The Tranception score includes both global 
and local scores via a weighted geometric average in probability space:
log P(s) = (1 −α) log PA(s) + α log PR(s),
(4)
where α is the retrieval inference weight, and we used α = 0.6, which was 
reported as the optimal value for the validation data. The Tranception 
work has calculated scores for 32 datasets which are used in our work. 
We calculated the scores for the two BRCA1 datasets for RING domain 
by using the MSAs obtained from the description above.
EVE evolutionary score. Evolutionary model of variant effect (EVE) is a 
Bayesian variational autoencoder model that is self-trained on MSAs to 
learn the sequence distribution. The ELBO is used to estimate sequence 
log-likelihood to predict sequence mutational effect. We adopted the 
ensemble EVE scores from Tranception work19 for 32 datasets. The 
scores for the remaining two BRCA1 datasets for RING domain were 
adopted from EVE work9.
Structural data preparation
The raw wild-type structure data are obtained from the PDB data-
base22 or AF38. The Visual Molecular Dynamics (VMD) package is used 
to remove the water and select the target chain in the corresponding 
DMS dataset55. The Jackal package56, a protein structure modeling tool, 
is used to optimize wild-type structures and generate mutant struc-
tures. The force parameter CHARMM22 is used in this study. Missing 
hydrogen atoms in the raw structure data are added using the ‘profix’ 
function in Jackal56:
./profix -fix 0 $pdbfile
where $pdbfile is the file name of the structure data. The raw structure 
data may have a few mutations away from the wild-type sequence. The 
wild-type structure is generated from the raw structure via mutations 
using ‘scap’ with fixed backbone:
./scap -ini 20 -min 4 $mutant_list $pdbfile

Nature Computational Science | Volume 3 | February 2023 | 149–163
158
Article
https://doi.org/10.1038/s43588-022-00394-y
where $mutant_list is the document for mutation information. Then 
the structure is optimized with 20 rounds of side-chain conformations 
minimization using ‘scap’ in Jackal56:
./scap -ini 20 -min 4 $pdbfile
From the wild-type structure, each mutant structure in the dataset is 
obtained from using ‘scap’ with fixed backbone, and further optimized 
by conformation minimization.
For both wild-type and mutant structures, the local structure is 
extracted by VMD55 for the preparation of site-specific analysis. Specifi-
cally, atoms belong to mutational sites and atoms near the mutational 
sites within a distance of 13 Å were selected.
Simplicial complex and chain complex
Graph is a representation for a point cloud consisting of vertices and 
edges for modeling pairwise interactions, such as atoms and bonds 
in molecules. Simplicial complex, the generalization of graph, con-
structs more enriched shapes to include high-dimensional objects.  
A simplicial complex is composed of simplexes up to certain dimen-
sions. A k-simplex, σk, is a convex hull of k + 1 affinely independent 
points v0, v1, v2, ..., vk:
σk ∶= [v0, v1, v2, ⋯, vk] = {
k
∑
i=0
λivi
||||
k
∑
i=0
λi = 1; λi ∈[0, 1], ∀i } .
(5)
In Euclidean space, 0-simplex is a point, 1-simplex is an edge, 
2-simplex is a triangle and 3-simplex is a tetrahedron. The k-simplex 
can describe abstract simplex for k > 3.
A subset of the k + 1 vertices of a k-simplex, σk, with m + 1 vertices 
forming a convex hull in a lower dimension and is called an m-face of 
the k-simplex σm, denoted as σm ⊂ σk. A simplicial complex K is a finite 
collection of simplexes satisfying two conditions: (1) any face of a 
simplex in K is also in K; (2) the intersection of any two simplexes in K 
is either empty or a shared face.
The interactions between two simplexes can be described by 
adjacency. For example, in graph theory, two vertices (0-simplexes) are 
adjacent if they share a common edge (1-simplex). Adjacency for k-sim-
plexes with k > 0 includes both upper and lower adjacency. Two distinct 
k-simplexes, σ1 and σ2, in K are upper adjacent, denoted σ1 ≈ Uσ2, if both 
are faces of a (k + 1)-simplex in K, called a common upper simplex. Two 
distinct k-simplexes, σ1 and σ2, in K are lower adjacent, denoted σ1 ≈ Lσ2, 
if they share a common (k − 1)-simplex as their face, called a common 
lower simplex. Either common upper simplex or common lower sim-
plex is unique for two upper or lower adjacent simplexes. The upper 
degree of a k-simplex, degU(σk), is the number of (k + 1)-simplexes in 
K of which σk is a face; the lower degree of a k-simplex, degL(σk), is the 
number of non-empty (k − 1)-simplexes in K that are faces of σk, which 
is always k + 1. The degree of k-simplex (k > 0) is defined as the sum of 
its upper and lower degrees
deg(σk) = degU(σk) + degL(σk) = degU(σk) + k + 1.
(6)
For k = 0, the degree of a vertex is:
deg(σ0) = degU(σ0).
(7)
A simplex has orientation determined by the ordering of its ver-
tices, except 0-simplex. For example, clockwise and anticlockwise 
orderings of three vertices determine the two orientations of a triangle. 
Two simplexes, σ1 and σ2, defined on the same vertices are similarly 
oriented if their orderings of vertices differ from an even number of 
permutations, otherwise, they are dissimilarly oriented.
Algebraic topology provides a tool to calculate simplicial complex. 
A k-chain is a formal sum of oriented k-simplexes in K with coefficients 
on ℤ. The set of all k-chains of simplicial complex K together with the 
addition operation on ℤ constructs a free Abelian group Ck(K), called 
a chain group. To link chain groups from different dimensions, the 
k-boundary operator, ∂k: Ck(K) → Ck−1(K), maps a k-chain in the form of 
a linear combination of k-simplexes to the same linear combination of 
the boundaries of the k-simplexes. For a simple example where the 
k-chain has one oriented k-simplex spanned by k + 1 vertices as defined 
in equation (5), its boundary operator is defined as the formal sum of 
its all (k − 1) faces:
∂kσk =
k
∑
i=0
(−1)
iσk−1
i
=
k
∑
i=0
(−1)
i [v0, ⋯,
̂vi, ⋯, vk] ,
(8)
where σk−1
i
= [v0, ⋯, ̂vi, ⋯, vk] is the (k − 1)-simplex with its vertex vi  
being removed. The most important topological property is that a 
boundary has no boundary: ∂k−1∂k = ∅, where ∅ is the empty set.
A sequence of chain groups connected by boundary operators 
defines the chain complex:
⋯
∂n+1
→Cn(K)
∂n→Cn−1(K)
∂n−1
→⋯
∂1→C0(K)
∂0
→∅.
(9)
When n exceeds the dimension of K, Cn(K) is an empty vector space and 
the corresponding boundary operator is a zero map.
Combinatorial Laplacian
For the k-boundary operator ∂k: Ck → Ck−1 in K, let ℬk be the matrix rep-
resentation of this operator relative to the standard bases for Ck and 
Ck−1 in K. ℬk ∈ℤM×N is the matrix representation of boundary operator 
under the standard bases {σk
i }
N
i=1 and {σk−1
j
}
M
j=1 of Ck and Ck−1. Associated 
with the boundary operator ∂k, the adjoint boundary operator is 
∂∗
k ∶Ck−1 →Ck, where its matrix representation is the transpose of the 
matrix, ℬT, with respect to the same ordered bases to the boundary 
operator.
The k-combinatorial Laplacian, a topological Laplacian, is a linear 
operator Δk: Ck(K) → Ck(K)
∆k ∶= ∂k+1∂∗
k+1 + ∂∗
k∂k,
(10)
and its matrix representation, Lk, is given by
Lk = ℬk+1ℬT
k+1 + ℬT
k ℬk.
(11)
In particular, the 0-combinatorial Laplacian (that is, graph Lapla-
cian) is given as follows since ∂0 is an zero map:
L0 = ℬ1ℬT
1 .
(12)
The elements of k-combinatorial Laplacian matrices are
(Lk)i,j =
⎧
⎪⎪⎪
⎨
⎪⎪⎪
⎩
deg (σk
i ) , if i = j
1, if i ≠j, σk
i
U≁σk
j and σk
i
L∼σk
j with similar orientation
−1, if i ≠j, σk
i
U≁σk
j and σk
i
L∼σk
j with dissimilar orientation
0, if i ≠j, either σk
i
U∼σk
j or σk
i
L≁σk
j .
(13)
For k = 0, the graph Laplacian matrix L0 is
(L0)i, j =
⎧⎪
⎨⎪
⎩
deg (σ0
i ) , if i = j
−1, if i ≠j, σ0
i
U∼σ0
j
0, otherwise .
(14)

Nature Computational Science | Volume 3 | February 2023 | 149–163
159
Article
https://doi.org/10.1038/s43588-022-00394-y
The multiplicity of zero spectra of Lk gives the Betti-k number, 
according to combinatorial Hodge theorem36:
βk = dim(Lk) −rank(Lk) = null(Lk).
(15)
The Betti numbers describe topological invariants. Specifically, β0, β1 
and β2 may be regarded as the numbers of independent components, 
rings and cavities, respectively.
Persistent spectral theory
PST is facilitated by filtration, which introduces a multiscale analysis 
of the point cloud. The filtration is a family of simplicial complexes, 
{Kt}t∈ℝ+, parameterized by a single parameter t (that is, the radius of ball 
in Fig. 2c) and ordered by inclusion. There are several properties of the 
family of simplicial complexes:
	(1)	 For two values of parameters t′ < t′′, we have Kt′ ⊆Kt′′.
	(2)	 There are only a finite number of shape changes in the filtration, 
and we can find at most n filtration parameters such that
∅⫋Kt0 ⫋Kt1 ⫋⋯⫋Ktn = K,
(16)
where K is the largest simplicial complex can be obtained from the filtra-
tion. Suppose ti is the smallest filtration parameter where we observe 
the ith shape changes. Then for any filtration parameter t, the simplicial 
complex is corresponding to one simplicial complex in equation (16):
Kt = {
Kti,
if t ∈[ti−1, ti) , i ≤n,
Ktn,
if t ∈[tn, ∞) .
(17)
There are various simplicial complexes that can be used to con-
struct the filtration, such as Rips complex, Čech complex and Alpha 
complex. For example, the Rips complex of K with radius t consists of 
all simplexes with diameter at most 2t:
V(t) = {σ ⊆K|diam (σ) ≤2t} .
(18)
In the filtration, a family of chain complexes can be constructed:
{⋯
∂t
k+2
⇌
∂t∗
k+2
Ct
k+1
∂t
k+1
⇌
∂t∗
k+1
Ct
k
∂t
k⇌
∂t∗
k
⋯
∂t
1⇌
∂t∗
1
Ct
0⇌
∂t
0
∂t∗
0 ∅}
t∈ℝ+
(19)
where Ct
k = Ck(Kt) is the chain group for subcomplex Kt, and its k-bound-
ary operator is ∂t
k ∶Ck(Kt) →Ck−1(Kt). The family of any k-dimensional 
chain complexes {Ct
k}t∈ℝ+ has similar two properties for the simplicial 
complexes {Kt}t∈ℝ+ in filtration.
The homotopic shape changes with a small increment of filtration 
parameter may be subject to noise from the data. The persistence may 
be considered to enhance the robustness when calculating the Lapla-
cian. First, we define the p-persistent chain group ℂt,p
k
⊆Ct+p
k  whose 
boundary is in Ct
k−1:
ℂt,p
k
= {α ∈Ct+p
k
|∂t+p
k
(α) ∈Ct
k−1} ,
(20)
where ∂t+p
k
∶Ct+p
k
→Ct+p
k−1 is the k-boundary operator for chain group Ct+p
k . 
Then we can define a p-persistent boundary operator, ðt,p
k , as the restric-
tion of ∂t+p
k  on the p-persistent chain group ℂt,p
k :
ðt,p
k
= ∂t+p
k
|ℂt,p
k ∶ℂt,p
k
→Ct
k−1.
(21)
Then PST defines a family of p-persistent k-combinatorial Lapla-
cian operators ∆t,p
k
∶Ck(Kt) →Ck(Kt) (ref. 29), which is defined as
∆t,p
k
= ðt,p
k+1(ðt,p
k+1)
∗
+ (∂t
k)
∗
∂t
k.
(22)
We denote ℬt,p
k+1 and ℬt
k as the matrix representations for boundary 
operators ðt,p
k+1 and ∂t
k, respectively. Then the Laplacian matrix for  
∆t,p
k  is
ℒt,p
k
= ℬt,p
k+1(ℬt,p
k+1)
T
+ (ℬt
k)
Tℬt
k.
(23)
Since the Laplacian matrix, ℒt,p
k , is positive-semidefinite, its spectra are 
all real and non-negative
St,p
k
= Spectra(ℒt,p
k ) = {(λ1)
t,p
k , (λ2)
t,p
k , ⋯, (λN)
t,p
k },
(24)
where N is the dimension of a standard basis for Ct
k, and ℒt,p
k  has dimen-
sion N × N. The k-persistent Betti number βt,p
k  can be obtained from the 
multiplicity of the harmonic spectra of ℒt,p
k :
βt,p
k
= dim(ℒt,p
k ) −rank(ℒt,p
k ) = null(ℒt,p
k ) = #{i|(λi)
t,p
k
∈St,p
k , and(λi)
t,p
k
= 0}.
(25)
In addition, the rest of the spectra, that is, the non-harmonic part, 
capture additional geometric information. The family of spectra of the 
persistent Laplacians reveals the homotopic shape evolution (Fig. 2e).
Site- and element-specific PST for protein mutation 
embedding
The site- and element-specific strategies are used to generate PST 
embedding to study mutation-induced structural changes. The site-
specific strategy focuses on mutational sites to reduce computational 
complexity. The element-specific strategy considers the pairwise inter-
actions between two atomic groups to encode appropriate physical and 
chemical interactions in embedding. For example, hydrogen bonds can 
be inferred from the spectra of oxygen–nitrogen persistent Laplacians 
whereas hydrophobicity can be extracted from carbon–carbon pairs.
Specifically, the atoms in a protein are classified into various 
subsets:
	(1)	
𝒜𝒜m: atoms in the mutation site.
	(2)	 𝒜𝒜mn(r): atoms in the neighborhood of the mutation site within a 
cut-off distance, r.
	(3)	 𝒜𝒜ele(E): atoms in the system with element type, E.
An atomic group combines the atoms within (𝒜𝒜1) and near the 
mutational sites (𝒜𝒜2):
𝒜𝒜1 = 𝒜𝒜m ∩𝒜𝒜ele(E1),
𝒜𝒜2 = 𝒜𝒜mn ∩𝒜𝒜ele(E2),
(26)
where element types E1 and E2 can be C, N, O and all heavy atoms. Nine 
single atomic pairs are constructed where 𝒜𝒜1 and 𝒜𝒜2 pick one of the 
heavy elements in {C, N, O} each time, and a heavy atomic pair is con-
structed where both 𝒜𝒜1 and 𝒜𝒜2 represent all heavy elements {C, N, O}.
PST analyzes the union of atoms 𝒜𝒜1 ∪𝒜𝒜2 to embed critical biophysi-
cal atomic interactions. The Euclidean distance De or a bipartite dis-
tance Dmod is used. The bipartite distance excludes interactions between 
the atoms from the same set, where the distance between two arbitrary 
atoms ai, aj ∈𝒜𝒜1 ∪𝒜𝒜2 is defined as the following:
Dmod(ai, aj) = {
∞, if ai, aj ∈𝒜𝒜1, or ai, aj ∈𝒜𝒜2,
De(ai, aj), otherwise.
(27)
For zero dimensions, filtration using Rips complex with Dmod dis-
tance is used. The 0-dimensional PST features are generated at fixed 
scales. Specifically, 0-persistent Laplacian features are generated at 
10 fixed filtration parameters: 2 Å, 3 Å, ..., 11 Å. The short scales below 
2 Å are excluded as bond-breaking events are irrelevant to the protein 
fitness of our interest. For each Laplacian, we count the number of 
harmonic spectra and calculate five statistical values of non-harmonic 

Nature Computational Science | Volume 3 | February 2023 | 149–163
160
Article
https://doi.org/10.1038/s43588-022-00394-y
spectra: minimum, maximum, mean, standard deviation and sum. A 
60-dimensional vector is generated. This type of feature is calculated 
on nine atomic pairs. For multiple mutational sites, the features are 
summed up over mutational sites. Finally, the dimension of 0-dimen-
sional PST features for a protein is 540.
For one or two dimensions, filtration using Alpha complex with 
De distance is used. The local protein structure has a small number of 
atoms that can generate a limited number of high-dimensional sim-
plexes, leading to negligible shape changes. As a result, we extracted 
features from only harmonic spectra of persistent Laplacians coding 
topological invariants for the high-dimensional interactions. The 
persistence of the harmonic spectra is represented by the persistent 
barcode (Fig. 2) and implemented by GUDHI57. The topological feature 
vectors are generated from the statistics of bar lengths, births and 
deaths. First, bars with lengths lower than 0.1 Å are excluded as they do 
not have a clear physical interpretation. The remaining bars are used to 
construct the featurization vector with: (1) sum, maximum and mean 
for lengths of bars; (2) minimum and maximum for the birth values of 
bars; (3) minimum and maximum for the death values of bars. Each set 
of point clouds leads to a seven-dimensional vector. These features 
are calculated on nine single atomic pairs and one heavy atomic pair. 
The dimension of one- and two-dimensional PST feature vectors for a 
protein is 140. Overall, the dimension of PST embedding is 2,040 by 
concatenating features at different dimensions for wild type, mutant 
and their difference.
In addition to the featurization described above, we also tested dif-
ferent representations of PST and PH features. The persistent landscape 
is a popular vector representation for PH at dimensions one and two. 
It is implemented by GUDHI in this study57. We use 3 landscapes and 
resolution 10 for each point cloud, and it results in a 30-dimensional 
vector. Using the same strategy described for dimensions one and two, 
we generated features at ten point clouds using the element- and site-
specific strategies. The persistent landscape features are calculated 
for wild type, mutant and their difference for both dimensions one and 
two, which results in a 1,800-dimensional vector for each mutational 
entry. We also consider the non-harmonic spectra for p-persistent 
Laplacians at dimensions one and two. They are calculated at 10 fixed 
filtration parameters: 2 Å, 3 Å, ..., 11 Å using the HERMES package58. Five 
statistical values of non-harmonic spectra, minimum, maximum, mean, 
standard deviation and sum, are used for vectorization. This results in 
a 50-dimensional vector for each point cloud. With the same strategy 
used for persistent landscape, this approach results in a 3,000-dimen-
sional vector. These two types of featurization are not used in the main 
results. They are discussed in Supplementary Note 6.
Persistent homology
Similar to PST, PH presents a multiscale analysis via filtration. PH uses 
homology groups to describe the persistence of topological invariants. 
As a result, PH provides only the harmonic spectral information of PST.
The site- and element-specific PH features are generated in a 
similar way to those of PST. They use identical filtration construction. 
For zero dimensions, the filtration parameter is discretized into bins 
with length 1 Å in the range of [2, 11], namely, [2, 3), [3, 4), ..., [10,11) Å. 
For each bin, we count the numbers of persistent bars, resulting in a 
nine-dimensional vector for each point cloud. This type of feature is 
calculated on nine single atomic pairs. Indeed, the dimension of zero-
dimensional PH features for a protein is 81. For one or two dimensions, 
the identical featurization from the statistics of persistent bars in PST 
is used. PH embedding combines features at different dimensions as 
described above and concatenated for wild type, mutant and their 
difference, resulting in a 663-dimensional vector.
Ensemble regression
Following previous work in machine learning-assisted protein engi-
neering32, we employed multiple regressors to build the ensemble 
regression. Features are first normalized by StandardScaler() in scikit-
learn packages59. Each regressor is evaluated on training data by fivefold 
cross-validation using the root mean square error (RMSE). Bayesian 
optimization is performed to find the best-performing hyperparam-
eters for each regressor60. Then, top N models are picked based on the 
RMSE metric. The ensemble regression averages the predictions from 
the top N regressors to predict the fitness. There are 18 regressors, 
including 3 ANNs, 10 kernel models and 5 tree-based models (Fig. 1c).
These 18 models were all used in ensemble regression for the small 
fixed-size training data (that is, from 24 to 240), and top N = 3 models 
were selected and averaged. For the ensemble of the single type of 
regressors, kernel, tree or ANN, N = 3 is also used. For 80/20 train/test 
data split or fivefold cross-validation, we only ensemble three tree-
based models and two ANN models for adaption to large training data, 
and all N = 5 models are averaged to enhance the generalization of the 
model. In addition, the ANN architectures were made wider and deeper 
to fit with large training data. Lists of models, default and optimization 
ranges of hyperparameters can be found in Supplementary Tables 1–3.
Kernel-based models. We employed multiple kernel-based models. 
These types of model have great generalization ability to the small 
size of training data, especially for the linear models. Models imple-
mented by scikit-learn packages59, including ridge regression, kernel 
regression, Bayesian ridge regression, linear support vector regres-
sion, Bayesian ARD regression, cross-validated Lasso, SGDRegressor, 
k-nearest neighbors regressor and ElasticNet. In addition, we also 
included a boosting linear model in XGBoost61.
In particular, for ridge regression and kernel regression, we fol-
lowed the strategy used in augmented models20 to make the evolutionary 
score practically unregularized (using regularization coefficient 10−8).  
Otherwise, features are concatenated and treated equally in the model.
Tree-based models. The tree models can rank and keep important 
features. Their great fitting and generalization ability lead to robust 
and accurate performance for any size of training data. In utilizing the 
tree-based models, we treat all features equally. The important features 
such as evolutionary scores would be recognized by the tree models and 
contribute more to predictions. In this study, we employed four scikit-
learn models59, decision tree, gradient boosting tree regressor, random 
forest regressor and bagging regressor, and one XGBoost tree model61.
ANN models. The ANN models have universal approximation ability 
to learn data. In particular, with large size of training data, ANN models 
show higher accuracy and generalizability than other machine learning 
models. We employed deep and wide neural network architectures to 
build ANN models62. The structure- and sequence-based embeddings 
are concatenated and fed into a series of hidden fully connected layers. 
As embeddings provide relatively lower-level features than evolution-
ary scores, embedding features are fed-forward by several hidden lay-
ers, and evolutionary score concatenates with the last hidden layers, 
and the entire last layer is fed-forward to the output layer.
Each hidden layer except for the final layer sequentially combines 
a fully connected layer, ‘ReLu’ activation function, a batch normaliza-
tion layer and a dropout layer. At the final hidden layer, the hidden 
units are concatenated with the evolutionary score, and a batch nor-
malization layer and a fully connected layer are sequentially used to 
obtain the output. We took the Adam optimizer with maximal epochs 
‘num_epochs’. An early stopping criterion is used where the training 
will be stopped if no decrease of ‘tol’ is observed for training errors 
over ‘patience’ epochs.
For fixed and small sizes of training data 24, 96, 168 and 240; three 
ANN models have 1, 2 and 3 hidden layers. For 80/20 split or fivefold 
cross-validation, the ANN models include more neurons at each layer, 
and the three ANN models have one, three and five hidden layers. 
Specific hyperparameters can be found in Supplementary Table 3.

Nature Computational Science | Volume 3 | February 2023 | 149–163
161
Article
https://doi.org/10.1038/s43588-022-00394-y
Secondary structure predictions
Secondary structures were predicted by the Dictionary of Protein 
Secondary Structure (DSSP)63. Secondary structures are assigned 
based on hydrogen-bonding patterns. We performed DSSP on wild-
type protein for each dataset. DSSP classifies amino acids in a protein 
into eight categories: 3-turn helix (G), 4-turn helix (H), 5-turn helix (I), 
extended strand in parallel and/or anti-parallel β-sheet conformation 
(E), residue in isolated β-bridge (B), bend (S) and coil (C). The experi-
ments usually have low accuracy in measuring the structure of coils 
and the errors will propagate to the entire structure. We then classify 
these eight categories into two types: coil and non-coil. These two types 
of secondary structure can largely indicate the potential accuracy of 
the structure data.
B factors
The B factor provides a direct measurement of the structural flex-
ibility experiment. The B factor, sometimes called the Debye–Waller 
factor, temperature factor or atomic displacement parameter, is used 
in protein crystallography to describe the attenuation of X-ray or neu-
tron scattering caused by thermal motion. In each X-ray wild-type 
structure, the B factor is provided for each atom. In this study, we 
extracted the B factor at alpha carbons to represent the quality of each 
amino acid. Then we have a vector of the B factor with the same length 
as the number of amino acids in a protein. As a high B factor indicates 
low experimental accuracy, the third quartile (Q3), presented in this 
work, indicates the accuracy of those residues with a relatively high  
B factor, and, consequently, reflects the quality of less accurate domain 
in a X-ray structure.
Statistical tests
We used two one-sided statistical tests to compare the performance 
of different methods. For two sets of samples X and Y, we make the 
one-side hypothesis based on their average values. For example,  
if X has larger average than Y, we make the hypothesis that X is greater 
than Y. The P values are calculated to evaluate the statistical signifi-
cance of the null hypothesis.
Mann–Whitney U-test. The Mann–Whitney U-test is a non-parametric 
test of the null hypothesis that, for randomly selected values of X and Y 
from two populations, the probability of X being greater than Y is equal 
to the probability of Y being greater than X. The alternative hypothesis 
is that one population is stochastically greater than the other.
Wilcoxon signed-rank test. The Wilcoxon signed-rank test is also a 
non-parametric test. It is particularly designed for two pairs of depend-
ent samples. For example, we used it to compare the performance of 
two methods over all datasets, which results in paired samples (for 
example, Fig. 4b). Otherwise, the Mann–Whitney U-test is used for 
unpaired samples.
Evaluation metrics
Spearman correlation. Spearman correlation assesses monotonic 
relationships between two variables. The Spearman correlation 
between two variables will be high when observations have a similar 
rank between the two variables and low when observations have a 
dissimilar rank between the two variables. It is defined as the Pearson 
correlation coefficient between the rank variables:
ρ(X, Y) = cov(R(X), R(Y))
σR(X)σR(Y)
,
(28)
where cov is the covariance between two variables, σ is the standard 
deviation, and R(X) is the rank variable of X. The Spearman correlation 
is used to evaluate the rank correlation between ground truth and 
predicted fitness.
Normalized discounted cumulative gain. NDCG is another measure 
of ranking quality64. Highly ranked samples have a higher contribution 
to NDCG. NDCG evaluates the ranking quality by focusing on the top 
samples. In particular, it is useful for greedy search in protein engineer-
ing, which requires selecting the top predicted samples. The predicted 
values are first sorted into a descending order { ̂fi}
N
i=1 and the correspond-
ing ground-truth values are {fi}
N
i=1. The discounted cumulative gain 
(DCG) is defined as
DCG =
N
∑
i=1
fi
log2(i + 1) .
(29)
The ground-truth fitness contributes more to the sum if its pre-
diction has a higher rank. The DCG is then normalized into a range 
between 0 and 1, where 1 indicates perfect ranking quality. The NDCG 
is defined as
NDCG = DCG/
N
∑
i=1
̂fi
log2(i + 1) =
N
∑
i=1
fi
log2(i + 1) /
N
∑
i=1
̂fi
log2(i + 1)
(30)
Root mean square error. RMSE is used to measure the accuracy of the 
regressor. In particular, we used it to rank models in hyperparameter 
optimization and model selection in the ensemble regression. As our 
tasks focus on the ranking quality of the data, RMSE is not used to 
evaluated fitness predictions in this work.
Reporting summary
Further information on research design is available in the Nature Port-
folio Reporting Summary linked to this article.
Data availability
There are 34 DMS datasets with experimentally measured fitness used 
in this work including: 32 DeepSequence datasets8, the avGFP dataset41 
and the GB1 dataset42. The original data sources of the 32 DeepSequence 
datasets are provided in Supplementary Data 1 and Supplementary 
Note 7. Structure data were obtained from PDB database22 and AF38, 
and the specific entry ID is provided in Supplementary Data 1. The data 
analyzed and generated in this work, including sequence-to-fitness 
datasets, optimized structure data, MSAs, fine-tune parameters for 
eUniRep models, predictions from evolutionary scores for individual 
mutations and sequence- and structure-based embeddings are avail-
able at https://github.com/WeilabMSU/TopFit (ref. 65) and our lab 
server https://weilab.math.msu.edu/Downloads/TopFit/. Source data 
for Figs. 3–5 and Extended Data Figs. 1, 3, 5–10 are available with this 
paper. Source data for Extended Data Figs. 2 and 4 are available in 
Supplementary Data 2.
Code availability
All source codes and models are publicly available at https://github.
com/WeilabMSU/TopFit (ref. 65).
References
1.	
Narayanan, H. et al. Machine learning for biologics: opportunities 
for protein engineering, developability, and formulation. Trends 
Pharmacol.Sci. 42, 151–165 (2021).
2.	
Arnold, F. H. Design by directed evolution. Acc. Chem. Res. 31, 
125–131 (1998).
3.	
Karplus, M. & Kuriyan, J. Molecular dynamics and protein 
function. Proc. Natl Acad. Sci. USA 102, 6679–6685 (2005).
4.	
Wittmann, B. J., Johnston, K. E., Wu, Z. & Arnold, F. H. Advances in 
machine learning for directed evolution. Curr. Opin. Struct. Biol. 
69, 11–18 (2021).
5.	
Yang, K. K., Wu, Z. & Arnold, F. H. Machine-learning-guided 
directed evolution for protein engineering. Nat. Methods 16, 
687–694 (2019).

Nature Computational Science | Volume 3 | February 2023 | 149–163
162
Article
https://doi.org/10.1038/s43588-022-00394-y
6.	
Hopf, T. A. et al. The evcouplings python framework for 
coevolutionary sequence analysis. Bioinformatics 35,  
1582–1584 (2019).
7.	
Hopf, T. A. et al. Mutation effects predicted from sequence co-
variation. Nat. Biotechnol. 35, 128–135 (2017).
8.	
Riesselman, A. J., Ingraham, J. B. & Marks, D. S. Deep generative 
models of genetic variation capture the effects of mutations.  
Nat. Methods 15, 816–822 (2018).
9.	
Frazer, J. et al. Disease variant prediction with deep generative 
models of evolutionary data. Nature 599, 91–95 (2021).
10.	 Rao, R. M. et al. MSA transformer. In International Conference on 
Machine Learning 8844–8856 (PMLR, 2021).
11.	
The UniProt Consortium. UniProt: the universal protein knowledge 
base in 2021. Nucleic Acids Res. 49, D480–D489 (2021).
12.	 Rao, R. et al. Evaluating protein transfer learning with tape.  
Adv. Neural Inf. Process. Syst. 32, 9689–9701 (2019).
13.	 Bepler, T. & Berger, B. Learning protein sequence embeddings 
using information from structure. In International Conference on 
Learning Representations (2018).
14.	 Alley, E. C., Khimulya, G., Biswas, S., AlQuraishi, M. & Church, G. 
M. Unified rational protein engineering with sequence-based 
deep representation learning. Nat. Methods 16, 1315–1322 (2019).
15.	 Rives, A. et al. Biological structure and function emerge from 
scaling unsupervised learning to 250 million protein sequences. 
Proc. Natl Acad. Sci. USA 118, e2016239118 (2021).
16.	 Elnaggar, A. et al. ProtTrans: towards cracking the language 
of lifes code through self-supervised deep learning and high 
performance computing. IEEE Trans. Pattern Anal. Mach. Intell. 44, 
7112–7127 (2022).
17.	 Biswas, S., Khimulya, G., Alley, E. C., Esvelt, K. M. & Church, G. M. 
Low-n protein engineering with data-efficient deep learning. Nat. 
Methods 18, 389–396 (2021).
18.	 Meier, J. et al. Language models enable zero-shot prediction 
of the effects of mutations on protein function. Adv. Neural Inf. 
Process. Syst. 34, 29287–29303 (2021).
19.	 Notin, P. et al. Tranception: protein fitness prediction with 
autoregressive transformers and inference-time retrieval. In 
International Conference on Machine Learning 16990–17017 
(PMLR, 2022).
20.	 Hsu, C., Nisonoff, H., Fannjiang, C. & Listgarten, J. Learning protein 
fitness models from evolutionary and assay-labeled data. Nat. 
Biotechnol. 40, 1114–1122 (2022).
21.	 Luo, Y. et al. ECNet is an evolutionary context-integrated deep 
learning framework for protein engineering. Nat. Commun. 12, 
5743 (2021).
22.	 Berman, H. M. et al. The Protein Data Bank. Nucleic Acids Res. 28, 
235–242 (2000).
23.	 Schymkowitz, J. et al. The FoldX web server: an online force field. 
Nucleic Acids Res. 33, W382–W388 (2005).
24.	 Leman, J. K. et al. Macromolecular modeling and design in 
Rosetta: recent methods and frameworks. Nat. Methods 17, 
665–680 (2020).
25.	 Edelsbrunner, H. & Harer, J. Computational Topology: An 
Introduction (American Mathematical Society, 2010).
26.	 Zomorodian, A. & Carlsson, G. Computing persistent homology. 
Discrete Comput. Geom. 33, 249–274 (2005).
27.	 Cang, Z. & Wei, G.-W. Integration of element specific persistent 
homology and machine learning for protein–ligand binding 
affinity prediction. Int. J. Numer. Methods Biomed. Eng. 34,  
e2914 (2018).
28.	 Wang, M., Cang, Z. & Wei, G.-W. A topology-based network tree 
for the prediction of protein–protein binding affinity changes 
following mutation. Nat. Mach. Intell. 2, 116–123 (2020).
29.	 Wang, R., Nguyen, D. D. & Wei, G.-W. Persistent spectral graph.  
Int. J. Numer. Methods Biomed. Eng. 36, e3376 (2020).
30.	 Mémoli, F., Wan, Z. & Wang, Y. Persistent Laplacians:  
properties, algorithms and implications. SIAM J. Math. Data Sci. 4, 
858–884 (2022).
31.	 Meng, Z. & Xia, K. Persistent spectral-based machine learning 
(perspect ML) for protein–ligand binding affinity prediction. Sci. 
Adv. 7, eabc5329 (2021).
32.	 Wittmann, B. J., Yue, Y. & Arnold, F. H. Informed training set design 
enables efficient machine learning-assisted directed protein 
evolution. Cell Systems 12, 1026–1045 (2021).
33.	 Horak, D. & Jost, J. Spectra of combinatorial laplace operators on 
simplicial complexes. Adv. Math. 244, 303–336 (2013).
34.	 Chung, F. R. K. & Graham, F. C. Spectral Graph Theory (American 
Mathematical Society, 1997).
35.	 Brouwer, A. E. & Haemers, W. H. Spectra of Graphs (Springer, New 
York, 2011).
36.	 Eckmann, B. Harmonische funktionen und randwertaufgaben in 
einem komplex. Comment. Math. Helv. 17, 240–255 (1944).
37.	 Kac, M. Can one hear the shape of a drum? Am. Math. Mon. 73, 
1–23 (1966).
38.	 Jumper, J. et al. Highly accurate protein structure prediction with 
AlphaFold. Nature 596, 583–589 (2021).
39.	 Livesey, B. J. & Marsh, J. A. Using deep mutational scanning 
to benchmark variant effect predictors and identify disease 
mutations. Mol. Syst. Biol. 16, e9380 (2020).
40.	 Qiu, Y., Hu, J. & Wei, G.-W. Cluster learning-assisted directed 
evolution. Nat. Comput. Sci. 1, 809–818 (2021).
41.	 Sarkisyan, K. S. et al. Local fitness landscape of the green 
fluorescent protein. Nature 533, 397–401 (2016).
42.	 Olson, C. A., Wu, N. C. & Sun, R. A comprehensive biophysical 
description of pairwise epistasis throughout an entire protein 
domain. Curr. Biol. 24, 2643–2651 (2014).
43.	 Klesmith, J. R., Bacik, J.-P., Michalczyk, R. & Whitehead, T. A. 
Comprehensive sequence-flux mapping of a levoglucosan 
utilization pathway in E. coli. ACS Synth. Biol. 4, 1235–1243 (2015).
44.	 Bubenik, P. et al. Statistical topological data analysis using 
persistence landscapes. J. Mach. Learn. Res. 16, 77–102 (2015).
45.	 Adams, H. et al. Persistence images: a stable vector 
representation of persistent homology. J. Mach. Learn. Res. 18, 
1–35 (2017).
46.	 Romero, P. A., Krause, A. & Arnold, F. H. Navigating the protein 
fitness landscape with Gaussian processes. Proc. Natl Acad. Sci. 
USA 110, E193–E201 (2013).
47.	 Qiu, Y. & Wei, G.-W. Clade 2.0: evolution-driven cluster  
learning-assisted directed evolution. J. Chem. Inf. Model. 62, 
4629–4641 (2022).
48.	 Rollins, N. J. et al. Inferring protein 3D structure from deep 
mutation scans. Nat. Genet. 51, 1170–1176 (2019).
49.	 Georgiev, A. G. Interpretable numerical descriptors of amino acid 
space. J. Comput. Biol. 16, 703–723 (2009).
50.	 Kawashima, S. & Kanehisa, M. AAIndex: amino acid index 
database. Nucleic Acids Res. 28, 374–374 (2000).
51.	 Vaswani, A. et al. Attention is all you need. Adv. Neural Inf. Process. 
Syst. 30, 5998–6008 (2017).
52.	 Devlin, J., Chang, M.-W., Lee, K. & Toutanova, K. BERT: pre-training 
of deep bidirectional transformers for language understanding. 
In Proc. 2019 Conference of the North American Chapter of the 
Association for Computational Linguistics: Human Language 
Technologies 1, 4171–4186 (2019).
53.	 Hochreiter, S. & Schmidhuber, J. Long short-term memory.  
Neural Comput. 9, 1735–1780 (1997).
54.	 Yu, F., Koltun, V. & Funkhouser, T. Dilated residual networks. 
In Proc. IEEE Conference on Computer Vision and Pattern 
Recognition 472–480 (IEEE, 2017).
55.	 Humphrey, W., Dalke, A. & Schulten, K. VMD: visual molecular 
dynamics. J. Mol. Graph. 14, 33–38 (1996).

Nature Computational Science | Volume 3 | February 2023 | 149–163
163
Article
https://doi.org/10.1038/s43588-022-00394-y
56.	 Xiang, Z. & Honig, B. Extending the accuracy limits of prediction 
for side-chain conformations. J. Mol. Biol. 311, 421–430 (2001).
57.	 Maria, C., Boissonnat, J.-D., Glisse, M. & Yvinec, M. The GUDHI 
library: simplicial complexes and persistent homology. In 
International Congress on Mathematical Software 167–174 
(Springer, 2014).
58.	 Wang, R. et al. HERMES: persistent spectral graph software. 
Found. Data Sci. 3, 67 (2021).
59.	 Pedregosa, F. et al. scikit-learn: machine learning in python.  
J. Mach. Learn. Res. 12, 2825–2830 (2011).
60.	 Bergstra, J., Yamins, D. & Cox, D. Making a science of model 
search: hyperparameter optimization in hundreds of dimensions 
for vision architectures. In International Conference on Machine 
Learning 115–123 (PMLR, 2013).
61.	 Chen, T. & Guestrin, C. XGBoost: a scalable tree boosting 
system. In Proc. 22nd ACM SIGKDD International Conference on 
Knowledge Discovery and Data Mining 785–794 (ACM, 2016).
62.	 Cheng, H.-T. et al. Wide and deep learning for recommender 
systems. In Proc.1st Workshop on Deep Learning for Recommender 
Systems 7–10 (ACM, 2016).
63.	 Kabsch, W. & Sander, C. Dictionary of Protein Secondary 
Structure: pattern recognition of hydrogen-bonded and 
geometrical features. Biopolymers 22, 2577–2637 (1983).
64.	 Järvelin, K. & Kekäläinen, J. Cumulated gain-based evaluation of 
IR techniques. ACM Trans. Inf. Syst. 20, 422–446 (2002).
65.	 Qiu, Y. YuchiQiu/TopFit: Nature Computational Science 
publication accompaniment (v1.0.0). Zenodo https://doi.org/ 
10.5281/zenodo.7450235 (2022).
Acknowledgements
This work was supported in part by NIH grants R01GM126189 
and R01AI164266; NSF grants DMS-2052983, DMS-1761320 and 
IIS-1900473; NASA grant 80NSSC21M0023; Michigan Economic 
Development Corporation; MSU Foundation; Bristol-Myers  
Squibb 65109 and Pfizer. We thank C. Hsu and J. Listgarten for  
helpful discussions.
Author contributions
All authors conceived this work, and contributed to the original draft, 
review and editing. Y.Q. performed experiments and analyzed data. G.-
W.W. provided supervision and resources and acquired funding.
Competing interests
The authors declare no competing interests.
Additional information
Supplementary information The online version  
contains supplementary material available at  
https://doi.org/10.1038/s43588-022-00394-y.
Correspondence and requests for materials should be addressed to 
Guo-Wei Wei.
Peer review information Nature Computational Science thanks the 
anonymous reviewers for their contribution to the peer review of this 
work. Primary Handling Editor: Fernando Chirigati, in collaboration 
with the Nature Computational Science team. Peer reviewer reports 
are available.
Reprints and permissions information is available at  
www.nature.com/reprints.
Publisher’s note Springer Nature remains neutral with regard to 
jurisdictional claims in published maps and institutional affiliations.
Springer Nature or its licensor (e.g. a society or other partner) holds 
exclusive rights to this article under a publishing agreement with 
the author(s) or other rightsholder(s); author self-archiving of the 
accepted manuscript version of this article is solely governed by the 
terms of such publishing agreement and applicable law.
© The Author(s), under exclusive licence to Springer Nature America, 
Inc. 2023

Nature Computational Science
Article
https://doi.org/10.1038/s43588-022-00394-y
Extended Data Fig. 1 | The average performance of various models over  
34 datasets. a,b, This is a supplement for Fig. 3a. a, Line plots show identical data 
with Fig. 3a with additional data for two TopFit strategies. b, Results are evaluated 
by NDCG. a,b, Ensemble regression is used, except ridge regression for Georgiev 
and one-hot embeddings. Absolute values of ρ were shown for evolutionary 
scores. The width of shade shows 95% confidence interval from n = 20 repeats. 
Evolutionary scores use absolute values for corresponding quantities.

Nature Computational Science
Article
https://doi.org/10.1038/s43588-022-00394-y
Extended Data Fig. 2 | Spearman correlation for various models on individual datasets. This is a supplement for Fig. 3a. Each line plots show average ρ for each 
dataset over n = 20 repeats. Training data sizes are 24, 96, 168, and 240. The width of the shade shows 95% confidence interval.

Nature Computational Science
Article
https://doi.org/10.1038/s43588-022-00394-y
Extended Data Fig. 3 | The frequency that an embedding is ranked as the best 
across 34 datasets using Spearman correlation. a–d, This is a supplement for 
Fig. 3b including comparisons over different strategies. Histograms show the 
frequency that an embedding is ranked as the best across 34 datasets with 24, 96, 
168 and 240 training data, respectively. For each dataset, the best embedding has 
average ρ over n=20 repeats within the 95% confidence interval of the embedding 
with the highest average ρ. Comparisons were performed for a sequence-based 
embeddings, b structure- and sequence-based embeddings, c structure-based 
embeddings, sequence-based embeddings, and evolutionary scores and d 
structure-based embeddings, sequence-based embeddings, evolutionary scores 
and two sets of TopFit (VAE+PST+ESM and VAE+PST+eUniRep). We showed and 
used absolute values Spearman correlation for evolutionary scores.

Nature Computational Science
Article
https://doi.org/10.1038/s43588-022-00394-y
Extended Data Fig. 4 | NDCG for various models on individual datasets. This is an analog for Extended Data Figure 2 but using NDCG. Each line plots show average 
NDCG for each dataset over n = 20 repeats. Training data sizes are 24, 96, 168 and 240. The width of the shade shows 95% confidence interval.

Nature Computational Science
Article
https://doi.org/10.1038/s43588-022-00394-y
Extended Data Fig. 5 | The frequency that an embedding is ranked as the best 
across 34 datasets using NDCG. a–d, This is an analog for Extended Data Figure 
3 but measured by NDCG. Histograms show the frequency that an embedding 
is ranked as the best across 34 datasets with 24, 96, 168 and 240 training data, 
respectively. For each dataset, the best embedding has average NDCG over n = 
20 repeats within the 95% confidence interval of the embedding with the highest 
average NDCG. Comparisons were performed for a sequence-based embeddings; 
b structure- and sequence-based embeddings; c structure-based embeddings, 
sequence-based embeddings and evolutionary scores and d structure-based 
embeddings, sequence-based embeddings, evolutionary scores and two sets of 
TopFit (VAE+PST+ESM and VAE+PST+eUniRep). We showed and used absolute 
values NDCG for evolutionary scores.

Nature Computational Science
Article
https://doi.org/10.1038/s43588-022-00394-y
Extended Data Fig. 6 | Relationships between quality of wild-type protein 
structure and PST performance. a–b, This is a supplement for Fig. 3c. 
Boxplots show distribution of a percentages of coils for protein structure over 
34 datasets and b third quartile (Q3) of B factors at alpha carbons over 26 X-ray 
datasets. Datasets were classified into two classes depending on whether PST 
embedding is the best embedding. Scatter plots show same data with boxplots 
but for individual datasets. One-sided Mann–Whitney U-test examines the 
statistical significance that two classes have different values. Boxplots display 
five-number summary where center line shows median, upper and lower limits 
of the box show upper and lower quartiles, and upper and lower whiskers show 
the maximum and the minimum by excluding “outliers” outside the interquartile 
range. In a, sample sizes for PST ranked as the best model are n = 21, n = 15, n = 18 
and n = 19 for training data size 24, 96, 168 and 240, respectively. Sample sizes 
for PST not ranked as the best model are n = 13, n = 19, n = 16 and n = 15 for training 
data size 24, 96, 168 and 240, respectively. The p-values are 0.01, 3 × 10−5, 1 × 10−3 
and 1 × 10−3 for training data size 24, 96, 168 and 240, respectively. In b, sample 
sizes for PST ranked as the best model are n = 8, n = 12, n = 14 and n = 15 for training 
data size 24, 96, 168 and 240, respectively. Sample sizes for PST not ranked as the 
best model are n = 18, n = 14, n = 12 and n = 11 for training data size 24, 96, 168 and 
240, respectively. P values are 0.03, 0.02, 0.07 and 0.02 for training data size 24, 
96, 168 and 240, respectively.

Nature Computational Science
Article
https://doi.org/10.1038/s43588-022-00394-y
Extended Data Fig. 7 | Model occurrence in ensemble regression. This is a supplement for Fig. 3e to show model occurrence on individual datasets. For each repeat, 
the top N = 3 regressors were picked and counted. Histograms count the model occurrence over 20 repeats.

Nature Computational Science
Article
https://doi.org/10.1038/s43588-022-00394-y
Extended Data Fig. 8 | Comparisons between TopFit and other methods 
for mutation effects prediction using Spearman correlation. a,b, This is 
an analog for Fig. 4a-b, but TopFit combines VAE score, eUniRep embedding, 
and PST embedding. All supervised models use 240 labeled training data. 
Results are evaluated by Spearman correlation ρ. DeepSequence VAE takes the 
absolute value of ρ. The average ρ from n = 20 repeats is shown. All 34 datasets 
are categorized by their structure modality used: X-ray, nuclear magnetic 
resonance (NMR), AlphaFold (AF) and cryogenic electron microscopy (EM). a, 
Dot plots show results across 34 datasets. b, Dot plots show pairwise comparison 
between TopFit with one method at each plot. Medians of difference for average 
Spearman correlation Δρ across all datasets are shown. One-sided rank-sum test 
determines the statistical significance that TopFit has better performance than 
VAE score, eUniRep embedding and PST embedding with P values 3 × 10−7, 2 × 10−7 
and 4 × 10−7, respectively.

Nature Computational Science
Article
https://doi.org/10.1038/s43588-022-00394-y
Extended Data Fig. 9 | Comparisons between TopFit with other methods. 
TopFit consists of VAE score, ESM embedding and PST embedding. This is a 
supplement for Fig. 4b to include results with various numbers of training data. 
Average Spearman correlation from n = 20 repeats are shown, and all datasets are 
categorized by their structure modality used: X-ray, nuclear magnetic resonance 
(NMR), AlphaFold (AF) and cryogenic electron microscopy (EM). One-sided 
rank-sum test determines the statistical significance that TopFit has better 
performance than other strategies, except we use null hypothesis that TopFit 
has worse performance than VAE with 24 training data. The p-values are shown in 
the corresponding subfigures. They are 1. TopFit versus VAE: P = 4 × 10−6, 2 × 10−5 
and 1 × 10−6; 2. TopFit versus ESM: P = 2 × 10−7, 2 × 10−7 and 2 × 10−7 and 3. TopFit 
versus PST: P = 2 × 10−7, 2 × 10−7 and 2 × 10−7 for training data size 24, 96 and 168, 
respectively.

Nature Computational Science
Article
https://doi.org/10.1038/s43588-022-00394-y
Extended Data Fig. 10 | Comparisons between TopFit with other methods. 
TopFit consists of VAE score, eUniRep embedding and PST embedding. This 
is a supplement for Extended Data Figure 8b to include results with various 
numbers of training data. Average Spearman correlation from n = 20 repeats are 
shown, and all datasets are categorized by their structure modality used: X-ray, 
nuclear magnetic resonance (NMR), AlphaFold (AF) and cryogenic electron 
microscopy (EM). One-sided rank-sum test determines the statistical significance 
that TopFit has better performance than other strategies, except we use null 
hypothesis that TopFit has worse performance than VAE with 24 training data. 
The p-values are shown in the corresponding subfigures. They are 1. TopFit versus 
VAE: P = 3 × 10−6, 3 × 10−5 and 8 × 10−7; 2. TopFit versus eUniRep: P = 4 × 10−7, 2 × 10−7 
and 2 × 10−7; and 3. TopFit versus PST: P = 3 × 10−7, 2 × 10−7 and 3 × 10−7 for training 
data size 24, 96 and 168, respectively.




