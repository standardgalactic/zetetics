 
 
 
 
 
 
 
Folk Psychological Attributions of Consciousness to Large Language Models 
 
 
 
 
Clara Colombatto1*  &  Stephen M. Fleming1,2,3 
 
 
1 Department of Experimental Psychology, University College London, London, UK 
2 Max Planck UCL Centre for Computational Psychiatry and Ageing Research, London, UK 
3 Wellcome Centre for Human Neuroimaging, University College London, London, UK 
 
 
 
 
 
 
 
Running Head: Attributing Consciousness to LLMs 
Correspondence to : *Clara Colombatto, c.colombatto@ucl.ac.uk 
 
 
 
26 Bedford Way, London WC1H 0AP 
Keywords: Phenomenal Consciousness; Subjective Experience; Folk Psychology; Mind   
 
 
       Perception; Artificial Intelligence; Large Language Models 
OSF Repository: https://osf.io/49w7m/?view_only=277d40847f9f4b6cb71af19911c0dd85 
Word Count: 2578 (Main Text) 
Version: 10/26/23 — Submitted Draft 

Attributing Consciousness to LLMs  
 
 
 
 
 
 
p. 2 
Abstract 
 
Technological advances raise new puzzles and challenges for cognitive science and the 
study of how humans think about and interact with artificial intelligence (AI). For example, the 
advent of Large Language Models and their human-like linguistic abilities has raised substantial 
debate regarding whether or not AI could be conscious. Here we consider the question of 
whether AI could have subjective experiences such as feelings and sensations 
(“phenomenological consciousness”). While experts from many fields have weighed in on this 
issue in academic and public discourse, it remains unknown how the general population 
attributes phenomenology to AI. We surveyed a sample of US residents (N=300) and found that 
a majority of participants were willing to attribute phenomenological consciousness to LLMs. 
These attributions were robust, as they predicted attributions of mental states typically associated 
with phenomenology – but also flexible, as they were sensitive to individual differences such as 
usage frequency. Overall, these results show how folk intuitions about AI consciousness can 
diverge from expert intuitions – with important implications for the legal and ethical status of AI. 

Attributing Consciousness to LLMs  
 
 
 
 
 
 
p. 3 
Introduction 
 
One of the most prominent technological advances of the past decade is the development 
of generative large language models (LLMs). With their ability to respond to queries with 
coherent and relevant answers in natural language, LLMs such as ChatGPT are able to provide 
advice, summarise text, write code, and even produce poetry. These human-like capabilities have 
raised profound questions about the nature of artificial intelligence, and in particular, whether 
artificial intelligence (AI) is capable of having subjective experiences or ‘phenomenal 
consciousness’ (Nagel, 1974; Chalmers, 1996). This debate on consciousness in AI has been at 
the forefront of mainstream media and academic discourse (Chalmers, 2023; Shardlow & 
Przybyła 2022; Wiese, 2023) from all areas of cognitive science (LeDoux et al., 2023). 
 
While normative accounts and expert opinions are helpful for developing theories and 
potential tests of AI consciousness, an equally important question remains regarding whether and 
how people attribute phenomenal consciousness to LLMs. Investigating folk attributions of 
consciousness to AI is important for two reasons. First, folk psychological attributions of 
consciousness may mediate future moral concern towards AI, regardless of whether or not they 
are actually conscious (Mazor et al., 2023; Shepherd, 2018).  Second, any current or future 
scientific determination of phenomenal consciousness in AI is likely to be “theory-heavy”, and 
therefore deal in probabilities or credences, rather than definite statements (Butlin et al., 2023). 
The impact of such research on the public perception of AI consciousness is therefore critically 
dependent on a thorough understanding of people’s folk psychological beliefs. 
 
To investigate this question, we drew on insights from a rich tradition in experimental 
philosophy and social psychology showing how people attribute consciousness and other mental 
states to other agents (for a review, see Sytsma, 2014). Indeed, past work has shown that 
nonexperts employ the concept of phenomenal consciousness, and differently ascribe mental 
states that involve phenomenology (e.g., feeling joy, getting depressed) from those that do not 
(e.g., making a decision, forming a belief; Knobe & Prinz, 2008). These findings also mirror 
work showing that attributions of mental states to other agents can be well captured by two 
underlying dimensions related to “Experience” (e.g., the capacity to feel pain, fear) and 
“Agency” (e.g., the capacity to have self-control, morality; Gray, Gray, & Wegner, 2007; for a 
review, see Waytz et al., 2010). 

Attributing Consciousness to LLMs  
 
 
 
 
 
 
p. 4 
 
To investigate folk psychological attributions of consciousness in LLMs, we recruited a 
nationally representative sample of U.S. adults (N=300) and elicited attributions of 
phenomenology via a well-validated measure of phenomenological attributions (Peressini, 2014). 
In particular, we focused on ChatGPT as one of the most well-known and widespread LLMs, and 
asked participants to rate how capable they thought ChatGPT was of having subjective 
experience. We also measured various other attitudes, including confidence in consciousness 
attributions, attributions of other mental states, usage habits, and predictions of public opinion 
regarding AI consciousness. This set of questions thus allowed us to probe the correlates and 
underlying structure of folk psychological intuitions about consciousness in LLMs. 
 
Method and Materials 
This study was approved by the UCL Research Ethics Committee, and was conducted in 
July 2023. Experimental materials, anonymized raw data, and analysis code are openly available 
on the Open Science Framework (OSF) website at 
https://osf.io/49w7m/?view_only=277d40847f9f4b6cb71af19911c0dd85. 
Participants 
A sample of 300 participants from the U.S. was recruited from Prolific Academic 
(Prolific.co). Participants were recruited via proportional stratified random sampling, with age 
and gender quotas representative of the U.S. population based on U.S. Census Bureau data. The 
sample size was chosen arbitrarily to allow for a minimum of 20 participants in each stratum. No 
participants reported having encountered technical difficulties during the experiment, and no 
participants took the survey more than once. All participants were thus included in the analyses 
(N female=152; N male=142; mean age=46.13). 
Procedure 
After consenting to participate in the study, participants were told that they would be 
asked about their opinions regarding ChatGPT, and read a short description of the chatbot: 
“ChatGPT is an artificial intelligence chatbot developed by OpenAI and released in November 
2022. The name ‘ChatGPT’ combines ‘Chat’, referring to its chatbot functionality, and ‘GPT’, 
which stands for Generative Pre-trained Transformer, a type of large language model (LLM).”  

Attributing Consciousness to LLMs  
 
 
 
 
 
 
p. 5 
They were then introduced to the concept of phenomenological experience via a short 
description adapted from a study of folk phenomenality (Peressini, 2014):  
 
“As we all know, each of us as conscious human beings have an ‘inner life.’ We 
are aware of things going on around us and inside our minds. In other words, there 
is something it is like to be each of us at any given moment: the sum total of what 
we are sensing, thinking, feeling, etc. We are experiencers. 
 
On the other hand, things like thermostats, burglar alarms, and bread machines do 
not have an inner life: there is not anything it is like to be these objects, despite 
the fact that they can monitor conditions around them and make appropriate things 
happen at appropriate times. They are not experiencers.” 
 
They were then asked to rate the extent to which ChatGPT is capable of having conscious 
experience on a scale from 1 to 100 (with 1=“clearly not an experiencer”, 50=“somewhat an 
experiencer”, and 100=“clearly an experiencer”). They also reported their confidence in this 
judgment (“How confident are you about your judgment about ChatGPT being an experiencer?”) 
on a scale from 1 (“not confident at all”) to 100 (“very confident”), and their intuitions about 
how other people would judge ChatGPT (“How much of an experiencer do you think most 
people think ChatGPT is?”) on a scale from 1 to 100 (with 1=“most people think it is clearly not 
an experiencer”, 50=“most people think it is somewhat an experiencer”, and 100=“most people 
think it is clearly an experiencer”). 
Next, they answered a series of questions about ChatGPT’s mental capacities. These were 
compiled based on a literature review: we started from a comprehensive review (Sytsma, 2014) 
and identified 22 manuscripts investigating mind perception and consciousness attributions. For 
the full list, see Supplementary References. We then compiled a list of all attributes explored in 
the experiments reported in these previous studies—for a total of 254 stimuli, which we then 
reduced to 65 unique mental states. Participants saw each of these 65 attributes one at a time, and 
rated the extent to which ChatGPT was capable of exhibiting them, from 1 to 100 (with 1=“not at 
all”, 50=“somewhat”, and 100=“very much”), and how confident they were in their response 
from 1 (“not confident at all”) to 100 (“very confident”). 
 
Finally, they answered some questions about their demographics (age and gender), and 
their experience with ChatGPT, namely whether they had heard about ChatGPT prior to the 
experiment (“Yes” or “No”), whether they had used ChatGPT in the past (“Yes” or “No”), how 
often they had used ChatGPT (“More than once a day”, “About once a day”, “About once a 

Attributing Consciousness to LLMs  
 
 
 
 
 
 
p. 6 
week”, “About once every two weeks”, or “About once a month”), and for what purpose they 
had used ChatGPT (“General knowledge”, “Coding”, “Writing”, or “Other”). For full text, see 
materials on the OSF repository.  
 
Results 
 
While a third of participants (33%) reported that ChatGPT was not an experiencer, the 
majority (67%) attributed some phenomenal consciousness (mean[M]=25.56; median=16.00, 
standard deviation[SD]=27.36, range=1-100, where 1=“clearly not an experiencer”, and 
100=“clearly an experiencer”; Figure 1A). Participants who gave more extreme judgments (in 
either direction) were also more confident (quadratic regression B=178.25, SE=24.43, t=7.30, 
p<.001, CI=[130.18, 226.33], with a quadratic relationship between confidence and 
consciousness attributions yielding a better fit than a linear function, F(1, 297)=53.24, p<.001).  
Next, we investigated potential determinants of consciousness attributions, starting with 
familiarity. The majority of participants had heard about ChatGPT (97%), and most had also 
used it at least once before (57%). Participants who had experience using ChatGPT attributed 
higher levels of consciousness (M=29.59) than those who never used it (M=19.37; t(287)=3.33, 
p<.001). Attributions of consciousness were correlated with usage, with a linear increase from 
“never” to “more than once per day” (B=4.94, SE=0.99, t=4.99, p<.001, 95% CI=[2.99, 6.88]; 
Figure 1B). These data thus suggest a strong link between familiarity with a LLM and 
consciousness attributions, such that those who interact with ChatGPT more frequently are also 
more likely to believe it has subjective experiences. 
 

Attributing Consciousness to LLMs  
 
 
 
 
 
 
p. 7 
 
Figure 1. Attributions of Consciousness in a Large Language Model. Participants attributed 
varying levels of consciousness to ChatGPT (A), and these attributions increased with usage 
frequency (B). When asked to predict the extent to which other people on average would think 
ChatGPT is conscious (C), participants consistently overestimated public opinion (D). Error bars 
and bands represent 95% CIs. 
 
We next examined attributions of specific mental states, and their relationship to 
attributions of consciousness. Based on a literature review of experimental investigations of folk 
psychological attributions of consciousness (Sytsma, 2014), we identified 65 mental states 
encompassing various aspects of mental life—from sensory experiences (e.g., seeing or 
smelling) to cognitive processes (e.g., paying attention or exercising self-control), emotions (e.g., 
feeling depressed or relieved), and other complex capacities (e.g., acting morally or self-

Attributing Consciousness to LLMs  
 
 
 
 
 
 
p. 8 
reflecting). Participants’ ratings for each of these 65 traits (Figure S1A) were then reduced via a 
principal component analysis to two main dimensions, which together explained 58% of the 
variance (Figure S1B) and mapped onto previously identified dimensions of “experience” (e.g., 
experiencing pleasure, feeling fearful) and “intelligence” (e.g., knowing things, making choices; 
Grey et al., 2007).  
We then asked which mental state dimensions participants thought ChatGPT was capable 
of having. Overall, ChatGPT was seen as more capable of intelligence than experience: 
attributions of mental states were positively correlated with their loadings on the intelligence 
dimension (r=0.95, p<.001, CI=[0.92, 0.97]), and negatively with the experience dimension (r=-
0.94, p<.001, CI=[-0.96, -0.91]; Figure 2A). Next, we asked which mental states predicted 
consciousness attributions. Here, in contrast, we found a key role for experience: participants 
who attributed more phenomenal consciousness to ChatGPT also attributed more mental states 
related to experience (r=0.65, p<.001, CI=[0.58, 0.71]), but not those related to intelligence 
(r=0.03, p=.596, CI=[-0.08, 0.14]; Figure 2B). In other words, despite ChatGPT being seen on 
average as more capable of intelligence than experience, mental states related to experience were 
still the main driver of consciousness attributions. 
 
 
Figure 2. Structure of Mental State Attributions to ChatGPT. Participants’ ratings of ChatGPT’s 
mental capacities mapped onto two dimensions – Experience and Intelligence. While ChatGPT 
was seen as more capable of mental states related to Intelligence than Experience (A), only those 
related to Experience were predictive of phenomenal consciousness attributions (B). 

Attributing Consciousness to LLMs  
 
 
 
 
 
 
p. 9 
Finally, we probed participants’ intuitions about public attitudes towards consciousness 
in AI by asking them to predict other people’s attributions, using the same scale used to self-
report their own attitudes. As depicted in Figures 1C and 1D, predictions of others’ opinions 
were correlated with participants’ own opinions (r=0.56, p<.001, CI=[0.48, 0.63]), but they were 
also consistently higher (M=41.11; median=39.50, SD=25.28, range=1-100; t(299)=10.90, 
p<.001). In other words, participants systematically overestimated how much other people would 
see ChatGPT as being conscious. 
 
Discussion 
 
Overall, our results reveal that a substantial proportion (67%) of people attribute some 
degree of phenomenal consciousness to ChatGPT, and believe that most other people would as 
well. Strikingly, these attributions of consciousness were positively related to usage frequency, 
such that people who were more familiar with ChatGPT, and used it on a more regular basis 
(whether for assistance with writing, coding, or other activities) were also more likely to attribute 
some degree of phenomenology to the system. Thus, independent of ongoing academic 
discussions about the potential for and possibility of artificial consciousness (eg Butlin et al., 
2023; Chalmers, 2023), the recent emergence and widespread uptake of powerful large language 
models may be associated with a majority of people perceiving some degree of consciousness in 
these systems. 
 
An obvious limitation is that these attributions of consciousness were measured via a 
single question and might differ with different experimental measures and prompts. For example, 
it remains unclear how folk conceptions of phenomenology correspond to the relevant 
philosophical constructs (Huebner, 2010; Talbot, 2012; Peressini, 2014), and how well self-
reported measures capture such intuitions, as opposed to more indirect behavioural markers 
which may be less subject to response biases (Scholl & Gao, 2013). However, an analysis of the 
underlying structure of mental state attributions suggested that attributions of phenomenology 
were a robust feature of our data, and covaried with attributions of other mental states deemed to 
have subjective qualities such as emotions or sensations. We also note converging evidence from 
studies employing different materials and measures (Scott et al., 2023).  
 
Of course, these attitudes were measured in a stratified sample of the U.S. population, 
and it remains unclear whether they would generalize across different samples and cultures. In 

Attributing Consciousness to LLMs  
 
 
 
 
 
 
p. 10 
fact, the effect of usage suggests that consciousness attributions might be higher in participants 
recruited online, who likely use computers on a daily basis, and might be reduced in participants 
who are less familiar with computing and AI. Similarly, the preferences we report reflect 
attitudes at a specific moment in time and may change as LLMs become more widespread and 
advanced. The relationship between usage frequency and consciousness attributions suggests that 
familiarity with the technology may lead to higher attributions of consciousness – or vice versa, 
that higher attributions of consciousness may lead people to make greater use of LLMs. Future 
investigations may probe these attitudes longitudinally or via an experimental intervention, to 
explore the possible causal links between usage of AI and folk psychological attributions of 
consciousness.  
Future work may also investigate specific characteristics of AI and human-AI 
interactions that might influence consciousness attributions. For example, attributions of mental 
states may depend on superficial appearance (Bainbridge et al., 2011) as well as observed 
behavioural profiles (Colombatto & Fleming, 2023). Conversely, future work may also explore 
characteristics of the perceivers - such as a tendency to engage in spontaneous theory of mind - 
that may lead to increased consciousness attributions. Beyond opening up these new avenues for 
future research, our results are also relevant to current controversies in public discourse and 
policy regarding the ethical and legal status of AI, given that folk ascriptions of consciousness, 
both now and in the future, may be a significant driver of societal concern for artificial systems. 
 
Conclusions 
In summary, our investigation of folk psychological attributions of consciousness 
revealed that most people are willing to attribute some form of phenomenology to LLMs: only a 
third of our sample thought ChatGPT did not have subjective experience, while two-thirds of our 
sample thought ChatGPT had varying degrees of phenomenological consciousness. The 
relatively high rates of consciousness attributions in this sample are somewhat surprising, given 
that experts in neuroscience and consciousness science currently estimate that LLMs are highly 
unlikely to be conscious (Butlin et al., 2023; LeDoux et al., 2023). These findings thus highlight 
a discrepancy between folk intuitions and expert opinions on artificial consciousness—with 
significant implications for the ethical, legal, and moral status of AI.  
 

Attributing Consciousness to LLMs  
 
 
 
 
 
 
p. 11 
Acknowledgements 
The authors wish to thank Jonathan Birch, MJ Crockett, Matan Mazor, and Megan Peters 
for helpful discussions. This work was supported by a UKRI/EPSRC Programme Grant 
[EP/V000748/1] and ERC Consolidator Award “ConsciousComputation”. SMF is a CIFAR 
Fellow in the Brain, Mind and Consciousness Program, and is funded by a Sir Henry Dale 
Fellowship jointly funded by the Wellcome Trust and the Royal Society (206648/Z/17/Z). 
 
Author Contributions 
All authors designed research; CC conducted the experiment, analyzed data, and wrote 
the manuscript with input from SMF. 
 
Declaration of Interests 
The authors declare no competing interests. 
 
Open Practices Statement 
Materials, data, and analysis code are openly available on the Open Science Framework 
(OSF) website at this link: 
https://osf.io/49w7m/?view_only=277d40847f9f4b6cb71af19911c0dd85 
 
 

Attributing Consciousness to LLMs  
 
 
 
 
 
 
p. 12 
References 
Bainbridge, W. A., Hart, J. W., Kim, E. S., & Scassellati, B. (2011). The benefits of interactions 
with physically present robots over video-displayed agents. International Journal of 
Social Robotics, 3, 41-52. 
Butlin, P., Long, R., Elmoznino, E., Bengio, Y., Birch, J., Constant, A., ... and VanRullen, R. 
(2023). Consciousness in artificial intelligence: Insights from the science of 
consciousness. Preprint at arXiv, arXiv:2308.08708. 
Chalmers, D. J. (1996). The Conscious Mind. Oxford: Oxford University Press. 
Chalmers, D. J. (2023). Could a large language model be conscious?. Boston Review. 
https://www.bostonreview.net/articles/could-a-large-language-model-be-conscious/ 
(accessed Aug 18, 2023). 
Colombatto, C., & Fleming, S. M. (under review). Illusions of confidence in Artificial Systems. 
Gray, H. M., Gray, K., and Wegner, D. M. (2007). Dimensions of mind perception. Science, 315, 
619–619. 
Huebner, B. (2010). Commonsense concepts of phenomenal consciousness: Does anyone care 
about functional zombies?. Phenomenology and the Cognitive Sciences, 9, 133-155. 
Knobe, J., and Prinz, J. (2008). Intuitions about consciousness: Experimental studies. 
Phenomenology and the Cognitive Sciences, 7, 67–83. 
LeDoux, J., Birch, J., Andrews, K., Clayton, N. S., Daw, N. D., Frith, C., ... & Vandekerckhove, 
M. M. (2023). Consciousness beyond the human case. Current Biology, 33, R832-R840. 
Mazor, M., Brown, S., Ciaunica, A., Demertzi, A., Fahrenfort, J., Faivre, N., ... and Lubianiker, 
N. (2023). The scientific study of consciousness cannot and should not be morally 
neutral. Perspectives on Psychological Science, 18, 535–543. 
Nagel, T. (1974). What is it like to be a bat?. The Philosophical Review, 83, 435–450. 
Peressini, A. (2014). Blurring two conceptions of subjective experience: Folk versus 
philosophical phenomenality. Philosophical Psychology, 27, 862–889. 
Scholl, B. J., & Gao, T. (2013). Perceiving animacy and intentionality: Visual processing or 
higher-level judgment. In M. D. Rutherford, & V. A. Kuhlmeier (Eds.), Social 
perception: Detection and interpretation of animacy, agency, and intention (pp. 197–
229). MIT Press. 
Scott, A. E., Neumann, D., Niess, J. and Woźniak, P. W. (2023). Do you mind? User perceptions 
of machine consciousness. Proceedings of the 2023 ACM CHI Conference on Human 
Factors in Computing Systems, 1–19. 

Attributing Consciousness to LLMs  
 
 
 
 
 
 
p. 13 
Shardlow, M., and Przybyła, P. (2022). Deanthropomorphising NLP: Can a Language Model Be 
Conscious?. Preprint at arXiv, arXiv:2211.11483. 
Shepherd, J. (2018). Consciousness and Moral Status (Taylor & Francis). 
Sytsma, J. (2014). Attributions of consciousness. WIREs Cognitive Science, 5, 635–648. 
Talbot, B. (2012). The irrelevance of folk intuitions to the “hard problem” of consciousness. 
Consciousness and Cognition, 21, 644–650. 
Waytz, A., Gray, K., Epley, N., & Wegner, D. M. (2010). Causes and consequences of mind 
perception. Trends in Cognitive Sciences, 14, 383–388. 
Wiese, W. (2023). Could large language models be conscious? A perspective from the free 
energy principle. Preprint at Philpapers, https://philpapers.org/rec/WIECLL 
 
 
 
 
 

Attributing Consciousness to LLMs  
 
 
 
 
 
 
p. 14 
Supplementary Figure 
 
 
Supplementary Figure 1. Dimensionality Reduction on Mental States. Participants rated 
ChatGPT’s capacity for 65 different mental states (A). A Principal Component Analysis of these 
ratings revealed two underlying dimensions, which seemed to map onto the capacity for feelings 
and sensations (“Experience”; left) and that for thinking and acting (“Intelligence”; right). 

Attributing Consciousness to LLMs  
 
 
 
 
 
 
p. 15 
Supplementary References 
Arico, A. (2010). Folk psychology, consciousness, and context effects. Rev. Philos. Psychol. 1, 
371–393. 
Arico, A., Fiala, B., Goldberg, R.F., and Nichols, S. (2011). The folk psychology of 
consciousness. Mind Lang. 26, 327–352. 
Buckwalter, W., and Phelan, M. (2013). Function and feeling machines: A defense of the 
philosophical conception of subjective experience. Philos. Stud. 166, 349–361. 
Buckwalter, W., and Phelan, M. (2014). Phenomenal consciousness disembodied. In Advances 
in Experimental Philosophy of Mind, J. Sytsma, ed. (London: Bloomsbury), pp. 45–73. 
Fiala, B., Arico, A., and Nichols, S. (2014). You, robot. In Current Controversies in 
Experimental Philosophy, E. Machery and E. O’Neill, eds. (New York: Routledge), pp. 
31–47. 
Gray, H. M., Gray, K., and Wegner, D. M. (2007). Dimensions of mind perception. Science 315, 
619–619. 
Gray, K., and Wegner, D. M. (2009). Moral typecasting: divergent perceptions of moral agents 
and moral patients. J. Pers. Soc. Psychol. 96, 505–520. 
Gray, K., and Wegner, D. M. (2011). To escape blame, don’t be a hero—be a victim. J. Exp. 
Soc. Psychol. 47, 516–519 
Gray, K., Jenkins, A. C., Heberlein, A. S., and Wegner, D. M. (2011). Distortions of mind 
perception in psychopathology. Proc. Natl. Acad. Sci. U.S.A. 108, 477–479. 
Gray, K., Knobe, J., Sheskin, M., Bloom, P., and Barrett, L. F. (2011). More than a body: mind 
perception and the nature of objectification. J. Pers. Soc. Psychol. 101, 1207–1220. 
Haslam, N., Kashima, Y., Loughnan, S., Shi, J., and Suitner, C. (2008). Subhuman, inhuman, 
and superhuman: Contrasting humans with nonhumans in three cultures. Soc. Cogn. 26, 
248–258. 
Huebner, B. (2010). Commonsense concepts of phenomenal consciousness: Does anyone care 
about functional zombies?. Phenomenol. Cogn. Sci. 9, 133–155. 
Huebner, B., Bruno, M., and Sarkissian, H. (2010). What does the nation of China think about 
phenomenal states?. Rev. Philos. Psychol. 1, 225–243. 
Jack, A. I., and Robbins, P. (2012). The phenomenal stance revisited. Rev. Philos. Psychol. 3, 
383–403. 
Knobe, J., and Prinz, J. (2008). Intuitions about consciousness: Experimental studies. 
Phenomenol. Cogn. Sci. 7, 67–83. 

Attributing Consciousness to LLMs  
 
 
 
 
 
 
p. 16 
Peressini, A. (2014). Blurring two conceptions of subjective experience: Folk versus 
philosophical phenomenality. Philos. Psychol. 27, 862–889. 
Phelan, M., Arico, A., and Nichols, S. (2013). Thinking things and feeling things: On an alleged 
discontinuity in folk metaphysics of mind. Phenomenol. Cogn. Sci. 12, 703–725. 
Reuter, K., Phillips, D., and Sytsma, J. (2014). Hallucinating pain. In Advances in Experimental 
Philosophy of Mind, J. Sytsma, ed. (London: Bloomsbury), pp. 1777–1801. 
Sytsma, J. (2014). The robots of the dawn of experimental philosophy of mind. In Current 
Controversies in Experimental Philosophy, E. Machery and E. O’Neill, eds. (New York: 
Routledge), pp. 48–64. 
Sytsma, J., and Machery, E. (2009). How to study folk intuitions about phenomenal 
consciousness. Philos. Psychol. 22, 21–35. 
Sytsma, J., and Machery, E. (2010). Two conceptions of subjective experience. Philos. Stud. 
151, 299–327. 
Sytsma, J., and Machery, E. (2012). On the relevance of folk intuitions: a reply to Talbot. 
Conscious. Cogn. 21, 654–660. 
Sytsma, J. (2014). Attributions of consciousness. WIREs Cogn. Sci. 5, 635–648. 
10.1002/wcs.1320 
 
 

