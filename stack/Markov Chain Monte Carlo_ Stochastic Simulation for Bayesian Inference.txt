

Texts in Statistical Science
Markov Chain 
Monte Carlo
Stochastic Simulation for Bayesian Inference
Second Edition

CHAPMAN & HALL/CRC 
Texts in Statistical Science Series
Series Editors
Bradley P. Carlin, University o f Minnesota, USA 
Chris Chatfield, University o f Bath, UK 
Martin Tanner, Northwestern University, USA 
Jim Zidek, University o f British Columbia, Canada
Analysis o f Failure and Survival Data
Peter J. Smith
T he Analysis and Interpretation o f 
Multivariate Data for Social Scientists
David J. Bartholomew, Fiona Steele,
Irini Moustaki, and Jane Galbraith
T he Analysis o f Tim e Series —
A n  Introduction, Sixth Edition
Chris Chatfield
Applied Bayesian Forecasting and Tim e Series 
Analysis
A . 
Pole, M . W est and J. Harrison
Applied Nonparametric Statistical Methods, 
Third Edition
P. Sprent and N .C . Smeeton
Applied Statistics— Handbook o f G E N ST A T  
Analysis
E.J. Snell and H . Simpson
Applied Statistics —  Principles and Examples
D .R . C ox and E.J. Snell
Bayes and Empirical Bayes Methods for Data
Analysis, Second Edition
Bradley P. Carlin and Thom as A . Louis
Bayesian Data Analysis, Second Edition
Andrew  Gelm an, John B. Carlin,
H al S. Stern, and Donald B. Rubin
Beyond A N O V A — Basics o f Applied Statistics 
R .G . M iller, Jr.
Computer-Aided Multivariate Analysis, 
Fourth Edition
A . A  A fifi and V.A. Clark
A  Course in Categorical Data Analysis
T. Leonard
A  Course in Large Sample Theory
T.S. Ferguson
Data Driven Statistical Methods
P. Sprent
Decision Analysis —  A  Bayesian Approach 
J.Q^ Smith
Elementary Applications o f Probability 
Theory, Second Edition
H .C . Tuckwell
Elements o f Simulation
B.J.T. M organ
Epidemiology—  Study Design and 
Data Analysis, Second Edition
M . W oodw ard
Essential Statistics, Fourth Edition
D .A .G . Rees
Extending the Linear M odel with R: 
Generalized Linear, Mixed Effects and 
Nonparametric Regression Models
Julian J. Faraway
A  First Course in Linear M odel Theory
Nalini Ravishanker and D ipak K. D ey
Generalized Additive Models:
A n Introduction with R
Sim on W ood
Interpreting D ata— A  First Course 
in Statistics
A.J.B. Anderson
A n  Introduction to Generalized 
Linear M odels, Second Edition
A.J. D obson
Introduction to Multivariate Analysis
C. Chatfield and A.J. Collins
Introduction to Optimization Methods and 
Their Applications in Statistics
B.S. Everitt
Large Sample Methods in Statistics 
P.K. Sen and J. da M otta Singer
Linear Models with R
Julian J. Faraway
Markov Chain M onte Carlo —  Stochastic 
Simulation for Bayesian Inference, Second Edition
Dani Gam erm an and Hedibert Freitas Lopes
Mathematical Statistics
K. Knight
Modeling and Analysis o f Stochastic Systems 
V. Kulkarni
Modelling Binary Data, Second Edition
D . Collett
Modelling Survival Data in Medical Research, 
Second Edition
D . Collett
Multivariate Analysis o f Variance and 
Repeated Measures —  A  Practical Approach 
for Behavioural Scientists 
D.J. H and and C .C . Taylor

Practical Data Analysis for Designed 
Experiments
B.S. Yandell
Practical Longitudinal Data Analysis
D.J. H and and M . Crowder
Practical Statistics for Medical Research
D .G . Altm an
Probability —  Methods and Measurement
A. O ’Hagan
Problem Solving— A  Statistician’s Guide, 
Second Edition
C. Chatfield
Randomization, Bootstrap and 
M onte Carlo Methods in Biology,
Second Edition
B.F.J. M anly
Readings in Decision Analysis
S. French
Sampling Methodologies with Applications
Poduri S.R.S. Rao
Statistical Analysis o f Reliability Data
M.J. Crowder, A .C . Kimber,
T.J. Sweeting, and R .L. Smith
Statistical Methods for Spatial Data Analysis
Oliver Schabenberger and Carol A . C otw ay
Statistical Methods for SPC and T Q M
D. 
Bissell
Statistical Methods in Agriculture and 
Experimental Biology, Second Edition
R. M ead, R.N . C um ow , and A .M . Hasted
Statistical Process Control— Theory 
and Practice, Third Edition
G .B . W etherill and D .W . Brown
Statistical Theory, Fourth Edition
B .W . Lindgren
Statistics for Accountants
S. Letchford
Statistics for Epidemiology
Nicholas P. Jewell
Statistics for Technology— A  Course in 
Applied Statistics, Third Edition
C . 
Chatfield
Statistics in Engineering —
A  Practical Approach
A .V . M etcalfe
Statistics in Research and Development, 
Second Edition
R. Caulcutt
Survival Analysis Using S— Analysis of 
Tim e-to-Event Data
Mara Tableman and Jong Sung Kim
T he Theory o f Linear Models
B. 
Jorgensen


Texts in Statistical Science
Markov Chain 
Monte Carlo
Stochastic Simulation for Bayesian Inference
Second Edition
Dani Gamerman
Instituto de Matematica 
Universidade Federal do Rio de Janeiro, Brazil
Hedibert Freitas Lopes
Graduate School of Business 
University of Chicago, U.S.A.
Chapman & Hall/CRC
Taylor & Francis Group
Boca Raton London New York
Chapman & Hall/CRC is an imprint of the 
Taylor & Francis Croup, an informa business

Published in 2006 by
Chapman & Hall/CRC
Taylor & Francis Group
6000 Broken Sound Parkway NW, Suite 300
Boca Raton, FL 33487-2742
©  2006 by Taylor & Francis Group, LLC
Chapman & Hall/CRC is an imprint o f Taylor & Francis Group
No claim to original U.S. Government works
Printed in the United States o f America on acid-free paper
10 9 8 7 6 5
International Standard Book Number-10: 1-58488-587-4 (Hardcover)
International Standard Book Number-13: 978-1-58488-587-0 (Hardcover)
Library o f Congress Card Number 2006044491
This book contains information obtained from authentic and highly regarded sources. Reprinted material is 
quoted with permission, and sources are indicated. A wide variety o f references are listed. Reasonable efforts 
have been made to publish reliable data and information, but the author and the publisher cannot assume 
responsibility for the validity o f all materials or for the consequences of their use.
No part o f this book may be reprinted, reproduced, transmitted, or utilized in any form by any electronic, 
mechanical, or other means, now known or hereafter invented, including photocopying, microfilming, and 
recording, or in any information storage or retrieval system, without written permission from the publishers.
For permission to photocopy or use material electronically from this work, please access www.copyright.com 
(http://www.copyright.com/) or contact the Copyright Clearance Center, Inc. (CCC) 222 Rosewood Drive, 
Danvers, MA 01923, 978-750-8400. CCC is a not-for-profit organization that provides licenses and registration 
for a variety o f users. For organizations that have been granted a photocopy license by the CCC, a separate 
system o f payment has been arranged.
TVademark Notice: Product or corporate names may be trademarks or registered trademarks, and are used only 
for identification and explanation without intent to infringe.
Gamerman, Dani.
Markov chain Monte Carlo : stochastic simulation for Bayesian inference.— 2nd ed. / Dani 
Gamerman, Hedibert F. Lopes.
p. cm. — (Texts in statistical science series ; 68)
Includes bibliographical references and indexes.
ISBN 1-58488-587-4 (acid-free paper)
1. Bayesian statistical decision theory. 2. Markov processes. 3. Monte Carlo method.
I. Lopes, Hedibert Freitas. II. Title. III. Texts in statistical science ; v. 68.
QA279.5.G36 2006
519.5’42—dc22 
2006044491
Library of Congress Cataloging-in-Publication Data
Visit the Taylor & Francis Web site at 
http://www.taylorandfrancis.com
Taylor & Francis Group 
is the Academic Division of Informa pic.
and the C R C Press W eb site at 
http://www.crcpress.com

To Andre (D.G.)
To Neusa, Levy and Anne (H.F.L.)


Contents
Preface to the second edition 
xiii
Preface to the first edition 
xv
Introduction 
1
1 
Stochastic simulation 
9
1.1 Introduction 
9
1.2 Generation of discrete random quantities 
10
1.2.1 Bernoulli distribution 
11
1.2.2 Binomial distribution 
11
1.2.3 Geometric and negative binomial distribution 
12
1.2.4 Poisson distribution 
12
1.3 Generation of continuous random quantities 
13
1.3.1 Probability integral transform 
13
1.3.2 Bivariate techniques 
14
1.3.3 Methods based on mixtures 
17
1.4 Generation of random vectors and matrices 
20
1.4.1 Multivariate normal distribution 
21
1.4.2 Wishart distribution 
23
1.4.3 Multivariate Student’s t distribution 
24
1.5 Resampling methods 
25
1.5.1 
Rejection method 
25
1.5.2 Weighted resampling method 
30
1.5.3 Adaptive rejection method 
32
1.6 Exercises 
34
2 
Bayesian inference 
41
2.1 Introduction 
41
2.2 Bayes’ theorem 
41
2.2.1 Prior, posterior and predictive distributions 
42
2.2.2 Summarizing the information 
47
2.3 Conjugate distributions 
49
2.3.1 Conjugate distributions for the exponential family 
51

X
Contents
2.3.2 
Conjugacy and regression models 
55
2.3.3 
Conditional conjugacy 
58
2.4 Hierarchical models 
60
2.5 Dynamic models 
63
2.5.1 
Sequential inference 
64
2.5.2 
Smoothing 
65
2.5.3 
Extensions 
67
2.6 Spatial models 
68
2.7 Model comparison 
72
2.8 Exercises 
74
3 
Approximate methods of inference 
81
3.1 Introduction 
81
3.2 Asymptotic approximations 
82
3.2.1 
Normal approximations 
83
3.2.2 
Mode calculation 
86
3.2.3 
Standard Laplace approximation 
88
3.2.4 
Exponential form Laplace approximations 
90
3.3 Approximations by Gaussian quadrature 
93
3.4 Monte Carlo integration 
95
3.5 Methods based on stochastic simulation 
98
3.5.1 
Bayes’ theorem via the rejection method 
100
3.5.2 
Bayes’ theorem via weighted resampling 
101
3.5.3 
Application to dynamic models 
104
3.6 Exercises 
106
4 
Markov chains 
113
4.1 Introduction 
113
4.2 Definition and transition probabilities 
114
4.3 Decomposition of the state space 
118
4.4 Stationary distributions 
121
4.5 Limiting theorems 
124
4.6 Reversible chains 
127
4.7 Continuous state spaces 
129
4.7.1 
Transition kernels 
129
4.7.2 
Stationarity and limiting results 
131
4.8 Simulation of a Markov chain 
132
4.9 Data augmentation or substitution sampling 
135
4.10 Exercises 
136
5 
Gibbs sampling 
141
5.1 Introduction 
141
5.2 Definition and properties 
142
5.3 Implementation and optimization 
148

5.3.1 
Forming the sample 
148
5.3.2 
Scanning strategies 
150
5.3.3 
Using the sample 
151
5.3.4 
Reparametrization 
152
5.3.5 
Blocking 
155
5.3.6 
Sampling from the full conditional distributions 
156
5.4 
Convergence diagnostics 
157
5.4.1 
Rate of convergence 
158
5.4.2 
Informal convergence monitors 
159
5.4.3 
Convergence prescription 
161
5.4.4 
Formal convergence methods 
164
5.5 
Applications 
169
5.5.1 
Hierarchical models 
169
5.5.2 
Dynamic models 
172
5.5.3 
Spatial models 
176
5.6 
MCMC-based software for Bayesian modeling 
178
Appendix 5.A: BUGS code for Example 5.7 
182
Appendix 5.B: BUGS code for Example 5.8 
184
5.7 
Exercises 
184
6 
Metropolis-Hastings algorithms 
191
6.1 
Introduction 
191
6.2 
Definition and properties 
193
6.3 
Special cases 
198
6.3.1 
Symmetric chains 
198
6.3.2 
Random walk chains 
198
6.3.3 
Independence chains 
199
6.3.4 
Other forms 
204
6.4 
Hybrid algorithms 
205
6.4.1 
Componentwise transition 
206
6.4.2 
Metropolis within Gibbs 
211
6.4.3 
Blocking 
214
6.4.4 
Reparametrization 
216
6.5 
Applications 
217
6.5.1 
Generalized linear mixed models 
217
6.5.2 
Dynamic linear models 
223
6.5.3 
Dynamic generalized linear models 
226
6.5.4 
Spatial models 
231
6.6 
Exercises 
234
7 
Further topics in M C M C  
237
7.1 
Introduction 
237
7.2 
Model adequacy 
237
7.2.1 
Estimates of the predictive likelihood 
238
Contents 
xi

xii
Contents
7.2.2 
Uses of the predictive likelihood 
248
7.2.3 
Deviance information criterion 
253
7.3 Model choice: MCMC over model and parameter spaces 
257
7.3.1 
Markov chain for supermodels 
258
7.3.2 
Markov chain with jumps 
261
7.3.3 
Further issues related to RJMCMC algorithms 
270
7.4 Convergence acceleration 
271
7.4.1 
Alterations to the chain 
271
7.4.2 
Alterations to the equilibrium distribution 
278
7.4.3 
Auxiliary variables 
282
7.5 Exercises 
284
References 
289
Author index 
311
Subject index
316

Preface to the second edition
Almost a decade has elapsed since the release of the first edition. A large 
amount of recent work was produced on the MCMC subject but made no 
substantial theoretical contribution. As anticipated in the first edition, most 
of the ground work for the theory had been established by then. Subsequent 
literature has basically enabled further understanding and extensions of the 
previous work. In any case, the book has been updated to include the recent 
literature and as a result the number of references has almost doubled. We 
believe to have included at least a reference to most new developments in 
MCMC.
What has really changed in this decade is the depth of understanding 
and amount of applications of MCMC to the solution of inference problems. 
The revision we performed concentrated on this point. The reader will 
hopefully face a much more readable book in terms of practical aspects. The 
numbers of exercises, examples, numerical tables and figures have also been 
considerably increased. We tried to exemplify and illustrate archetypical 
situations to many applied areas to enable a better apprehension of the 
pros and cons of the variety of algorithms available in the MCMC arena.
In line with the modern resources available nowadays, the URL site 
www.ufrj.br/MCMC has been created. It contains the codes (all written 
in R language) used in many of the previously existing and new examples 
and exercises of the book. Readers will have free access to them and will 
be able to reproduce the tables and figures of the book. More importantly, 
the mildly self-explanatory nature of the codes will enable modification of 
the inputs to the codes and variation in many directions will be available 
for further exploration. This internet tool is planned to be constantly being 
updated and can also be used to compensate for any new development not 
included in this edition of the book.
The major changes from the previous edition are as follows. New sections 
on spatial models and model adequacy have been introduced in Chapter 
2. Spatial models is an area that has experienced a huge development in 
statistics during the last decade and the writers of the book have made 
a few contributions there as well. A section on model adequacy should 
have always been there. All that was done was to minimally remedy this 
flaw of the first edition. Chapter 7 is the chapter that has undergone the 
largest change. It has moved away from its speculative flavor to a much

xiv
Preface to the second edition
more detailed description of a number of techniques that are routinely 
used nowadays. Chapters 5 and 6 have also been considerably increased 
by inclusion of more illustrative material. This was done with the sole 
aim of providing better understanding of the MCMC machinery. All other 
chapters have been subjected to additions but to a smaller amount.
In summary, the book has been substantially reinforced as a first reading 
material on MCMC and, consequently, as a textbook on modern Bayesian 
computation and Bayesian inference courses. More advanced derivations 
were not present before and are still not present in this edition.
As usual, this revision could not have been possible without the input 
from a few people. First, Rob Calver from CRC should be mentioned for his 
constant encouragement of this project. Second, our colleague and friend 
Marco Ferreira should be thanked for the suggestion of this author partner­
ship at an evening outing in a bar in Vina del Mar, Chile, during the 2004 
World ISBA meeting. His wisdom has allowed him to foresee (well ahead 
of us!) that we could successfully work together on such an important and 
intense task. We also thank Aline Nobre, Aparecida Souza and Edna Reis 
for helping with the material that appeared in some figures, Glaura Franco, 
Ralph Silva and Michele Guindani for helping with proofreading, Marcia 
Cardoso, from UFRJ, for helping us setting up the URL site at the UFRJ 
domain, Microsoft for enabling through MSN Messenger the cost-free voice 
communication between Rio and Chicago throughout 2005, the research 
support agency CNPq-Brazil for providing funds for the first author for a 
few but very important presential meetings between us, the Universidade 
Federal do Rio de Janeiro and University of Chicago Graduate School of 
Business for fully supporting the authors’ research respectively, and, finally, 
to our family and friends for continuing encouragement and support.
Dani Gamerman 
Hedibert Freitas Lopes
Rio de Janeiro and Chicago, January 2006

Preface to the first edition
The development of statistics as an empirical science for data analysis and 
treatment has always been linked to the computational capabilities of the 
moment. It was not surprising that the widespread dissemination of faster 
computational equipment in the last decades enabled an unprecedented 
growth in the statistical treatment of complex models.
The area that benefited most from this advance was applied Bayesian 
inference. Although the Bayesian approach has always had the support of 
many statistical users, its development has been hampered by the difficulty 
of its implementation in practical problems.
Recently, the story has definitely changed. The rediscovery of relatively 
simple but extremely powerful simulation techniques has enabled the ap­
plication of the Bayesian paradigm to a variety of complex practical sit­
uations. And even better, these techniques are available for general use 
without the need for sophisticated pre-requisites of statistical theory for its 
understanding.
The aim of this book is to describe these techniques, collectively known 
as Markov chain Monte Carlo (MCMC, for short), to a wide-ranging public. 
The mathematical rigor used in the book is minimal and only requires the 
equivalent of a basic undergraduate course on Probability and Statistics. 
Anybody with that mathematical literacy can follow and understand all 
the material contained in this book. This includes not only statisticians 
but also mathematicians, operation researchers, engineers, economists and 
frequent users of statistics in general with some knowledge of the area.
The book grew out of lecture notes in Portuguese prepared for a short 
course on the topic taught at the XII Meeting of Brazilian Statisticians and 
Probabilists, held in Caxambu (MG) in August 1996. I tried to preserve 
the didactic character of the text in the expansion/translation. The book 
can be used as an advanced undergraduate course or graduate course on 
MCMC after at least a Probability course. With that aim, the book includes 
a set of exercises at the end of each chapter. Some of them have a more 
mathematical flavor and are directed to graduate students.
Previous notions of simulation, Bayesian inference and Markov chain are 
useful but are not needed. The first chapters of this book present a basic 
introduction to these topics. The presentation is geared toward applying 
them in the understanding of general MCMC methods. Other approximat­

xvi
Preface to the first edition
ing techniques used in Bayesian inference are also described. They provide a 
useful benchmark for comparison with and the introduction of MCMC. The 
material covered in these chapters prepares the reader for later chapters and 
therefore many relevant points are made throughout these chapters. The 
variety of topics required for full appreciation of MCMC techniques has led 
to the inclusion of an introductory chapter at the beginning of the book. 
This chapter describes without technical details the route I chose to present 
the subject and serves as a guide to the book for the less mathematically 
oriented reader. It also describes the notation used in the book.
The core of the book consists of Chapters 5 and 6 describing Gibbs 
sampling and Metropolis-Hastings algorithms respectively. These are the 
main MCMC methods used nowadays. Examples of inference using these 
methods are provided with emphasis on hierarchical models and dynamic 
models, previously introduced in Chapter 2. Discussions over implementa­
tion, limitations and convergence issues are also included in these chapters. 
Finally, Chapter 7 discusses further topics related to MCMC and more 
advanced material.
The book is an attempt to provide a thorough and concise presentation 
of the subject starting virtually from scratch. The conciseness precludes a 
very detailed coverage of all topics. Important theoretical and methodolog­
ical points could not be skipped and practical data analysis was restricted 
to some of my own limited experience in the area. I concede that more could 
have been done but the extension would have required a substantial enlarge­
ment (and delay) of the book thus removing the slim (and updated) nature 
with which it was conceived. Chapman & Hall has recently released three 
excellent books (Carlin and Louis, 1996; Gelman, Carlin, Stern and Rubin, 
1995; Gilks, Richardson and Spiegelhalter, 1996) dealing with MCMC the­
ory, methodology and applications at a variety of levels that can be used 
to supplement the study of the subject.
The possibilities of expansion of MCMC methods are numerous. Most of 
the relevant literature was published in the 1990s and is providing a fasci­
nating and profitable interaction between statisticians, probabilists, math­
ematicians and physicists. The area should still experience a considerable 
growth in the future both in theory and in implementation and applica­
tions level. This text provides an account of the current situation in simple 
terms so that its readers will also be able to take part in this process.
It is fair to say that MCMC has recently developed mostly with appli­
cations to Bayesian inference as inspiration, but nothing prevents their use 
on problems in frequentist inference. In essence, MCMC deals with draw­
ing random values from a given distribution. The context that gave rise to 
the distribution is not mathematically relevant in general. The presenta­
tion however remains faithful to the main motivation that fuelled the use 
of MCMC techniques and will retain a strong Bayesian flavor.
My introduction to the area was eased by constant interactions with

Preface to the first edition
xvii
Adrian Smith and Jon Wakefield during my visit to their department at 
Imperial College in 1994. My view of the subject and the preparation of this 
text bear their influence but I assume full responsibility for all views ex­
pressed here. Also, most of Chapter 4 in the Portuguese version of this book 
was written by Tereza Benezath, a colleague at my department. The data 
analysis illustrating convergence diagnostics was performed by Aparecida 
Souza. The original idea of preparation of notes on the subject was given 
by Helio S. Migon, also from my department and coauthor of many joint 
research papers. Bent J0rgensen, Pablo Ferrari, Wally Gilks, Peter Green 
and Maria Eulalia Vares were also helpful with suggestions and encourage­
ment. Mark Pollard, Stephanie Harding, and later Richard Whitby, David 
Hemsley and James Rabson provided good editorial support from Chap­
man & Hall’s office. I find it important also to acknowledge the support 
provided by my University and Department through the use of their facili­
ties and by the research supporting agencies CAPES, CNPq and FAPERJ 
through grants that enabled me to continue my research projects. Finally,
I would like to thank my family, relatives, work colleagues and friends in 
general for the support during the difficult period of preparation of this 
book.
Dani Gamerman 
Rio de Janeiro, July 1997


Introduction
Overview of Bayesian Inference
This is a book about statistical inference, the area of science devoted to 
drawing conclusions or inference about data through quantitative measure­
ments. There is often uncertainty associated with measurements, either be­
cause they are made with imprecise devices or because the process under 
which these quantifications become available is not entirely controlled or 
understood. The tool used to quantify uncertainties is probability theory 
and probability distributions are associated with uncertain measurements. 
The specification of probability distributions to the uncertain measure­
ments or random variables in a given problem along with possible deter­
ministic relations between some of them defines a statistical model.
An example considered later in the book (see Example 6.8) is the study 
of the impact advertising expenditure may have on sales of a product or 
some surrogate measurement such as advertising awareness. In this case, 
the uncertain measurements are the results from weekly opinion polls car­
ried out in a given population of interest. The result of an opinion poll is 
given by the percentage y of people who remembered having watched the 
advertisement on TV. It is expected that advertising expenditure x might 
have an effect on awareness. Therefore a deterministic relation is estab­
lished to link its effect on the awareness probability n. The simplest link is 
given by the linear relation tc =  a + (3x. The value of the expenditure should 
take into account the instantaneous expenses but also the downweighted 
values of the expenses over previous weeks. Since n £ [0,1], it is usual in 
such cases to transform it to the real line before equating it to the linear 
form. A very common transformation is
In either case, the larger the value of /3, the more effective the advertisement 
campaign is in boosting awareness. There are, of course, many possible 
relations that can be entertained. The collection of the possible relations 
along with probability specifications for the percentages from the polls 
defines a statistical model.
Once a model is built, there are many ways to proceed with inference. 
The Bayesian approach considers uncertainties associated with all unknown

2
Introduction
quantities whether they are observed or unobserved. Inference is drawn by 
constructing the joint probability distribution of all unobserved quantities 
based on all that is known about them. This knowledge incorporates pre­
vious information about the phenomena under study and is also based on 
values of observed quantities, when they are available. This book assumes 
the general case where both pieces of information are available.
In this case, the distribution of unknowns given the knowns is called 
the posterior distribution because it is obtained after the data is observed. 
The unknown quantities may include future observations (that are cur­
rently unknown). Inference about them is referred to as prediction and 
their marginal distribution is referred to as the predictive distribution. The 
operations required to obtain these distributions are derived in Chapter 2 
and exemplified in a number of typical situations that occur in practice.
For the example, the quantities of interest are a and (3, used to define 
a link between expenditure and awareness. They are unknown, otherwise 
there would be little point in performing the polls. Marketing experience 
may provide some background information on them. Another source of 
information is provided by the result of the polls once they are carried out 
and the percentages become known. Bayesian inference provides the tools 
to combine these pieces of information to obtain the posterior distribution 
of a  and (3 based on previous, background knowledge and the observed 
information from the polls. There may also be interest in predicting future 
results from the polls for an anticipated advertisement expenditure in future 
weeks. In this case, the future results are added to the set of unknown 
quantities of interest.
Obtaining the posterior distribution is an important step but not the 
final one. One must be able to extract meaningful information from this 
distribution and translate it in terms of its impact on the study. This 
is mainly concerned with evaluation of point summaries such as mean, 
median or mode, or interval summaries given by probability intervals. In a 
few examples, this extraction or summarization exercise can be performed 
analytically, which means that an exact appraisal of the situation can be 
made. These cases are also illustrated in Chapter 2 and for them, the 
inferential task is completed.
In the advertisement study, the main interest is the evaluation of the 
value of /3. If its distribution is concentrated with large probability around 
positive values then the study confirms that advertisement boosts aware­
ness. Quantification is also important: the larger the values of /?, the better 
the advertisement campaign is in raising awareness about the product.
In most cases, however, the complexity of the model prevents this simple 
operation from taking place. The complexity is sometimes caused by the 
combination of the sources of information available for a given quantity. 
In other cases, it is caused by the sheer amount of quantities required for 
an adequate description of the phenomena studied. In some cases, it may

Introduction
3
even be caused by a combination of many quantities with many sources of 
information for some of them.
In the example, a more adequate description of the process is provided by 
a model that allows the links between expenditure and awareness to change 
with time. Different habits, changing environment, other rival advertise­
ment campaigns and change in advertisement campaign are all reasons for 
a dynamic modelling. One possible representation is to allow the quanti­
ties a and /3 to vary as time passes. The relation between the awareness 
probability 71\ and the expenditure x t at week t becomes
logit (7rt) = a t +  foxt
where the unknown quantities at and /3t are now allowed to change with 
the week. This automatically leads to a substantial increase in the number 
of the unknown quantities. Also, one must expect some degree of similarity 
between links in adjacent weeks. One convenient form to specify similarities 
is
a t 
=  
a t- i + w u ,
Pt 
=  
P t - i + w 2t-
Note that the number of unknown quantities has risen dramatically from
2 to 2n where n is the number of weeks considered in the study. The in­
corporation of these similarities in the model also means that the structure 
of the model has increased in complexity. The distribution of the unknown 
quantities has consequently become more complex to handle.
One is inevitably led to seek approximations that can provide at least a 
rough guide to the exact but unobtainable answer. There are many ways to 
tackle this problem and a variety of suggestions have been proposed in the 
literature, with more emphasis on this aspect from the 1980s. The timing 
is related to increased computing power enabling more sophisticated and 
computationally based solutions. These solutions can be broadly divided 
into two groups: deterministic and stochastic approximations.
Some of the deterministic methods are based on analytical approxima­
tions whereas others are based on numerical approximations. They have 
received a great deal of attention in the literature and have been applied 
with success in problems where the number of unknown quantities is small. 
Chapter 3 reviews the main approximating techniques, pointing at their 
strengths and weaknesses.
An entirely different perspective to extracting relevant information con­
tained in a given distribution is provided by stochastic simulation. The 
approach here is to use values simulated from the distribution of interest. 
A collection of these values forms a sample and defines a discrete distri­
bution concentrated on the sample values. The distribution of these values 
is an approximation to the parent distribution used for the simulation. 
Then, all relevant calculations with the parent distribution can be approxi­

4
Introduction
mately made with the sample distribution. In particular, the sample can be 
grouped into intervals and the histogram of relative frequencies plotted. If a 
large number of these values is simulated then the resulting histogram will 
be a very close approximation to the density of the distribution of inter­
est. Chapter 3 also describes approximating techniques based on stochastic 
simulation.
Stochastic simulation, or Monte Carlo, techniques have a few attractive 
features that may explain their recent success in Statistical Inference. First, 
they have strong support in probability results such as the law of large 
numbers (Equation (3.9)). It ensures that the approximation becomes in­
creasingly better as the number of simulated values increases. This number 
is controlled by the researcher and only time and cost considerations may 
prevent a virtually error-free approximation. Also, at any stage of the sim­
ulation process, the approximation error may be probabilistically measured 
using the central limit theorem (Equation (3.8)).
The main thrust of the book is the description of techniques devoted to 
perform Bayesian inference based on stochastic simulation, hence its sub­
title Stochastic Simulation for Bayesian Inference. Before applying simula­
tion, it is important to present basic, direct simulation operations to those 
not familiar with them. This is the purpose of Chapter 1. Many of the 
results presented there will be returned to in a more elaborate setting in 
later chapters.
Using these techniques, it is possible to devise simulation schemes to 
draw values from the distribution of a and (3 (in the static model setting) 
but they do not provide adequate solutions to the more elaborate case of 
time-varying a t and [it ■ These techniques will tend to be very inefficient 
as the dimension of unknown quantities increases, and more sophisticated 
simulation techniques will have to be used.
Overview of M C M C
Nowadays, there are many problems of interest that fall into the category 
of large dimension models. Dynamic settings are just an example. Other 
examples also arise in the context of hierarchical or random effects models 
and models for spatial data. The first group roughly deals with unstruc­
tured additional variation whereas the second group deals with variations 
due to a neighboring structure. They will also be considered in later chap­
ters. Models with measurement errors and a mixture or combination of 
models are also settings for large dimension models.
The title of the book, Markov Chain Monte Carlo, refers to an area 
of Statistics, usually referred to as MCMC by taking the first letters of 
each word. MCMC will be described in detail in this book. It provides an 
answer to the difficult problem of simulation from the highly dimensional 
distribution of the unknown quantities that appear in complex models.

Introduction
5
In very broad terms, Markov chains are processes describing trajectories 
where successive quantities are described probabilistically according to the 
value of their immediate predecessors. In many cases, these processes tend 
to an equilibrium and the limiting quantities follow an invariant distribu­
tion. MCMC techniques enable simulation from a distribution by embed­
ding it as a limiting distribution of a Markov chain and simulating from 
the chain until it approaches equilibrium. Before understanding simulation 
through Markov chains, or MCMC in short, it is important that properties 
of Markov chains are well understood. For the sake of those not familiar 
with them, Chapter 4 reviews the most relevant results.
The introduction of Markov chains in the simulation schemes is vital. It 
allows handling of complicated distributions such as those arising in the 
large dimension models mentioned above. It is interesting that introduc­
tion of an additional structure, the Markov chain, into an already complex 
problem ends up solving it! There is also the matter of the extra work 
involved in simulation of a single value by MCMC: a complete sequence 
of values of a chain until it reaches equilibrium is required and only the 
equilibrium value can be taken as a simulated value from the limiting dis­
tribution. Fortunately, there are also analogs of the law of large numbers 
and central limit theorems (Equations (4.6) and (4.11), respectively) for 
Markov chains. They ensure that most simulated values from a chain can 
be used to provide information about the distribution of interest.
There is still the question of how to build a Markov chain whose limiting 
distribution is exactly the distribution of interest, namely the distribution 
of all the unknown quantities of the model. It is amazing that not only is 
this possible but that there are large classes of schemes that provide these 
answers. One such scheme is Gibbs sampling. It is based on a Markov chain 
whose dependence on the predecessor is governed by the conditional distri­
butions that arise from the model. It so happens that many models have a 
complex joint distribution but by construction (some of) their conditional 
distributions are relatively simple. Gibbs sampling explores this point and 
is able to provide simple solutions to many problems. Gibbs sampling is 
presented in Chapter 5 and exemplified in a number of situations including 
models with hierarchical structure and models with a dynamic setting.
There are many ways that MCMC can be used in any given situation. 
The main concern is efficient computation. Efficiency can be measured by 
the ease with which a simulated sample is obtained. It takes many aspects 
into consideration, such as choice of conditional distributions to use, need 
for transformations of the quantities, cost and time of a simulation run 
and stability of the solutions obtained. These matters are also dealt with 
in Chapter 5.
Another scheme is given by the Metropolis-Hastings algorithms, pre­
sented in Chapter 6. They are based on a Markov chain whose dependence 
on the predecessor is split into two parts: a proposal and an acceptance of

6
Introduction
the proposal. The proposals suggest an arbitrary next step in the trajectory 
of the chain and the acceptance makes sure the appropriate limiting direc­
tion is maintained by rejecting unwanted moves of the chain. They provide 
a solution when even the conditional distributions of interest are complex, 
although their use is not restricted to these cases. Metropolis-Hastings al­
gorithms may come in a variety of forms and these can be characterized 
and studied. Some of their forms may be seen as generalizations of Gibbs 
sampling.
Going back to the example, the conditional distributions of at and (3t 
have also proved to be complex and Metropolis-Hastings algorithms seem 
to be a natural choice. Many schemes can be contemplated and a few of 
them are selected for numerical comparison and presented in Chapter 6.
It should also be noted that Bayesian inference is not necessarily com­
pleted after summarizing information about unknown quantities of a given 
model. There may be other relevant operations to perform such as model 
evaluation and model comparison, involving more than one model. O f par­
ticular interest is the joint consideration of a (large) number of possible 
models. Bayesian inference and MCMC can be accommodated to handle 
these questions. Alternative models can also be used as auxiliary devices 
in designing a MCMC method for a particular model. All these points are 
covered in Chapter 7.
Notation
Whenever possible, the same notation is maintained throughout the book. 
Distributions are identified with their density or probability functions and 
variables are generically treated as if they are continuous. Posterior densi­
ties are denoted by n and their approximations (described throughout the 
book) by q, observed quantities by Roman letters x, y, ... and unobserved 
quantities or parameters by Greek letters 9, 
No distinctions are made 
between a random variable and its observed value and between scalar, vec­
tor and matrix quantities although matrices are generally denoted by cap­
ital letters and scalar and vector quantities by lower case letters. Vectors 
are always arranged in a column unless otherwise stated. The transpose of 
a vector x is denoted by s' and its dimension generally denoted by d.
The complement of an event A is denoted by A, the probability of an 
event A is denoted by P r(A ), and expectation and variance of a random 
quantity x are respectively denoted by E (x) and V ar(x). The covariance 
and correlation between random quantities x and y are respectively denoted 
by C ov(x,y) and C or(x,y). The number of elements of a set A is denoted 
by if A. The indicator function is denoted by

Introduction
7
Approximations are denoted by a • superimposed to the relevant symbol. 
Therefore, =  stands for approximately equal, ~  stands for approximately 
distributed as and oc stands for approximately proportional to. - i  refers 
to convergence in distribution and 
refers to almost sure convergence or 
probability one convergence, i.o. denotes infinitely often.
Components of a vector x of fixed dimension are denoted by X\,X2, ■ ■ ■ 
whereas elements of a sequence x  will tend to be denoted by x^\x^2\ ... 
This will help to distinguish between the component and the sequence 
dimensions when dealing with vector sequences. The vector 
=  ( x i,. . . ,  
X i-i, £j+i, • • •, Xd) is the vector x with its ith component removed, i =  
1 ,..., d. The identity and diagonal d x d matrices are respectively denoted 
by
■ 
0 \ 
/  Cl 
0 
■ • ■ 
0 \
(
 
1
Id =
0
0 
1
0
and diag(ci, . . . , c d) =
0 
c2
0
V 0 
• - - 
0 
1 /  
\  o 
o 
Cd )
and the d-dimensional vectors of Is and Os are respectively denoted by 1^ 
and 0d- The absolute value of the determinant of a matrix A is denoted by 
| A | .


C H A P T E R  1
Stochastic simulation
1.1 Introduction
The word simulation refers to the treatment of a real problem through 
reproduction in an environment controlled by the experimenter. The en­
vironment is often provided by computational equipment although it can 
be a smaller scale reproduction of the system under study. An example 
is the study of tides in a laboratory reservoir. In this controlled environ­
ment, the water movement is caused by a device that is governed by an 
electric engine. This constitutes a study of a system that has all its com­
ponents known or at least deductible and whose behavior has an intrinsic 
deterministic character.
In other systems, some or all of its components are subject to random 
fluctuations. They cannot be described by an exact mathematical rule but 
only through the use of probabilistic statements. These systems are the 
subject matter of statistics. Whichever way one chooses to simulate them 
there will be a stochastic component, namely it will be based on probabil­
ity distributions. Stochastic simulation is the area of science dealing with 
simulation of these systems.
From a statistical point of view, these systems may be regarded as a 
(possibly complicated) function of random variables. Our objective is ba­
sically to reproduce these random variables in an environment under our 
control regardless of the complexity of the structure relating these variables. 
Typically, a computer is used for this reproduction exercise.
The starting point for stochastic simulation is the construction of a ran­
dom number generator. Usually, this mechanism generates an integer uni­
formly on an interval [0, M] for a given large value of M . This generation 
can be reduced to a generation of a number in the unit interval [0,1] after 
dividing the result by M .
There are many ways of doing this. The most common is the congruen- 
tial generator (Lehmer, 1951) that generates numbers according to Ui =  
aui- 1  mod M  after a seed Uo is chosen, b mod c denotes the remainder 
of the division of b by c. In fact, there is nothing random about this se­
quence. One would hope that appropriate selection of the constants uo, a 
and M  will provide a sequence that behaves as a sequence of independent 
random draws from a uniform distribution on [0, M\. A common choice of 
M  is 231 -  1 =  2,147,483,647 suggested by Lewis, Goodman and Miller

10
Stochastic simulation
(1969) and used by the International Mathematical and Statistical Library 
(IMSL). Fishman and Moore (1985) made an exhaustive study of these 
generators and came up with a few suggestions for the value of a. The 
Numerical Algorithm Group (NAG) Fortran library uses M  =  259 and 
a =  1313.
Much ingenuity and number theory has been exercised in specifying good 
selections of uo, a and M . Ripley (1987) provides a nice illustrative review 
of the congruential generators, other number generators and the number 
theory results that are relevant for number generation. This text does not 
intend to provide a digression on the subject and the reader is referred 
to the books by Dagpunar (1988), Devroye (1986) and Rubinstein (1981) 
for further reading. Hammersley and Handscomb (1964) and Newman and 
Odell (1971) are useful early references on the subject.
From now on, it will be assumed that the reader has a random number 
generator on the unit interval available to him. The quantity generated will 
be denoted by u and will be the base for all future random generations. 
The words generated, drawn or even sampled are used indistinguishably in 
this book to describe a generation (or draw or sample) from a distribution.
It must be stressed that all methods currently available are based on 
iterative and deterministic mechanisms. It is more accurate to refer to 
them as pseudo-random to clarify that these mechanisms can only generate 
numbers with a similar behavior to truly random numbers. However, if 
the mechanism is not known and examined only through its output, it 
provides numbers with the important basic properties of random numbers: 
uniformity and independence.
The next section describes techniques that can be used for the gener­
ation of some discrete distributions. The same is done in Section 1.3 for 
continuous distributions. Section 1.4 then deals with multivariate distribu­
tions. In Section 1.5, techniques based on an auxiliary generation from an 
approximate distribution are described. These techniques are particularly 
useful if the target distribution has a complicated form and there are no 
direct methods of generation of the random variable or quantity.*
1.2 Generation of discrete random quantities
This section describes methods of generation of quantities with discrete 
probability distribution. This is based on a random quantity u generated 
from a uniform distribution on the unit interval [0,1], denoted by u ~  
C/[0, 1 ],
In the generic case of a quantity x  assuming values on { x i , . . . ,  Xk] with
* T h e  term s q u a n tity  a n d  va ria b le are used in d istin ctly  as in th e co n te x t o f  th is b o o k  
it is p o ss ib le  t o  s p e c ify  p ro b a b ility  d istrib u tion s t o  o b serv a b le variables a n d  fixed  b u t 
u n ob serv a b le q u a n tities. T h is  p oin t is cla rified  in th e  n ext ch a p ter, b u t t o  em p h a size 
it th e  w ord  q u a n tity  w ill b e  used w h en ever p ossib le.

Generation of discrete random quantities
11
respective probabilities p i,...,p k  subject to 
=  1, the interval [0,1] 
is split into k intervals I i , ... Ik with I, =  (Z7, - j , Fi] where F0 =  0 and 
Fi =  pi + ■• ■+  Pi, i =  
Each interval corresponds to a single
value for x and after observing the generated value of u, one verifies the 
interval Ii to which u belongs. The generated value for x is x*. The method 
generates values from the distribution of x because P r(x =  x %') =  Pr(u £ 
I{) =  Fi — Fi-1  — pi, for i =  1 ,..., k. In the case of a discrete uniform 
distribution, it is enough to divide the unit interval into k subintervals of 
equal length 1/k and F, =  i/k. This construction can also be used for 
distributions taking values on a countable set by extending the argument.
In both the finite for large k and countable case, there is an important 
computational problem. It relates to the number of comparisons required 
to find the appropriate interval. Many comparisons may be needed be­
fore the correct interval is chosen. One possible search scheme to speed 
computations is to reorder the possible values of x in decreasing order of 
their probabilities. Most likely values will require less comparisons. Binary 
and indexed searches may also be used (Ripley, 1987). Chapter 6 presents 
alternative methods (see Example 6.5 for an illustration).
For many distributions there is a structure relating the values of the 
probabilities pi to a smaller number of constants. It is possible to obtain 
more efficient schemes without evaluation of the cumulative probabilities 
Fi in these cases. Below, some of these methods are described for the better 
known distributions.
1.2.1 Bernoulli distribution
x has Bernoulli distribution with success probability p if P r(x =  0) =  1 — p 
and P r(x  =  1) =  p, 0 <  p < 1, denoted by x ~  bern{p). In this case, the 
general result above can be used with k =  2, x\ =  1, X2 =  0 and F\ =  p. 
In other words, the unit interval is divided into two pieces of lengths p and 
1 — p. If the value of u is smaller than or equal to p, the value 1 is returned 
for x. Otherwise, the value 0 is returned.
1.2.2 Binomial distribution
x has binomial distribution with index n and success probability p, denoted 
by bin(n,p), if it has probability function
/s('t) =  P r(x  =  i) =  ^ 
~ p )n~\  
* =  0 , 1 , . . . , n. 
■
The generic method can be used here but would involve tedious calculations 
of combinations which may involve overflow difficulties in some computers 
if n is large. A more efficient and easier to code solution is obtained by 
noting that if x\ ,. . . ,  x n form a sample from the bern(p) distribution, that 
is, are independently and identically distributed, then x =  EjXj ~  bin(n,p).

12
Stochastic simulation
So, a value x drawn from a binomial distribution is obtained by drawing a 
sample u i , ..., u n from a U[ 0 ,1] distribution and counting the number x of 
these n generated values that are smaller than or equal to p. The resulting 
value has bin(n,p) distribution.
1.2.3 Geometric and negative binomial distribution
x has negative binomial distribution with index r and success probability 
p, denoted by nb(r,p), if it has probability function
/jv s(n ) =  P r(x =  n) =  ^ 
* ^ p r( l - p ) n _ r, 
n =  r,r +  l ,r  +  2 ,...
If r =  1, the distribution is called geometric and denoted geom(p).
As in Section 1.2.2, relations with the Bernoulli distribution may be 
invoked to simplify the problem. It is easy to see that a nb(r,p) quantity is 
obtained by repeating bern(p) observations until r successes are obtained 
and counting the number of observations used. So, a nb(r,p) quantity is 
drawn through the generation of a sequence U\,U2, . . .  from U [0,1] until 
r of them are smaller than or equal to p and counting the number x of 
generated quantities ul.
1.2.4 Poisson distribution
x has Poisson distribution with mean A, denoted by Poi(X), if it has prob­
ability function
fp {i) =  P r{x =  i) =  e~x ~ij-, 
i =  0, 1 , 2, . . .
The most efficient method of generation of P oi{A) quantities with small to 
moderate values of A is based on generation of exponential random quanti­
ties. x has exponential distribution with parameter A, denoted by E xp(A), 
if it has density f(x ) =  \e~Xx, x > 0. The generation of exponential quanti­
ties will be presented in the next section as this is a continuous distribution. 
For the moment, simply assume that a sample yi, 2/2, • • • from the E xp(A) 
can be drawn.
Results relating the Poisson and exponential distributions are obtained 
in Poisson processes. These are counting processes {N (t) , t >  0} where 
N  (t) counts the number of occurrences of a given event from the origin up 
to time t. The Poisson process is defined through:
(1) N (0) =  0;
(2) N (s) and N (t) — N (s) are independent quantities, for 0 <  s <  t;
(3) N (t) ~  Poi{Xt).
Denote the times between successive occurrences by J/i, 3/2 > - - ■ and the

Generation of continuous random quantities
13
times of the occurrences by t\ =  y1,t 2 =  yi +  y2, • • 
By (2), N (ti),N (t2) — 
N (ti), N its) — N (t2), ■ • ■ are independent quantities. Hence, y i,y 2, ... are 
also independent quantities. For t > 0, Pr(yi >  t) =  P r(N (ti-i +  t) — 
N (ti-i) =  0) =  e~Xt as, by (3), JV(tj_i+t) — N {U -\) ~  Poi(Xt). Therefore, 
F (t) — Pr(yi < t) =  1 — e~xt and the common density of the yi is f(t) =  
Xe~xt, t > 0. Times between occurrences of a Poisson process form a sample 
from the Exp(X) and N (l) ~  Poi(X).
The generation of Poi(X) random quantities is based on generations of 
Exp(X) until the sum of these quantities, playing the role of interoccur­
rences times, goes beyond 1. If the number of generations required to this 
end is k +  1, the number of occurrences N ( 1) and consequently the value 
of x drawn is k.
1.3 
Generation of continuous random quantities
1.3.1 Probability integral transform
The basic result about generation of continuous quantities is the probability 
integral transform stating that if x is continuous with distribution function 
F  then u =  F (x) ~  J/[0,1]. This result is easily shown as
Fu(y) =  Pr(u < y )  =  P r(F (x) <  y) =  P r(x < F - \ y ) )  =  F (F _ 1(y)) =  y,
if 0 < y < 1. Exploring the fact that F  has an inverse, one finds that 
-F-1 (-u) ~  F. So, a quantity with distribution function F  is drawn after 
transforming a uniform quantity according to F ~ x. Below, some applica­
tions of this method are presented.
1) Exponential distribution
This distribution was introduced in the previous section. All that is left 
here is to obtain the expression of F -1 . As F (x) =  1 — e~Xx,
u =  
1 -  e~Xx <=>■
1 
-  u =  
e~Xx <=> 
log (1 — u) =  
-A x  <^=>
x =  
— ^ log (1 — u).
Generation of x may be simplified further by noting that 1 — u ~  u and 
therefore —(1/A) log u leads to a value from E xp{A).
2) Weibull distribution
The Weibull distribution can be obtained as a generalization of the ex­
ponential distribution and is frequently used in reliability studies. If x ~  
Exp(X) then y =  x 1//q has Weibull distribution with parameters a and A. 
Its generation is therefore trivial requiring simply a draw u from a uniform 
distribution and returning a value [—(1/A) log u}1/01.

14
Stochastic simulation
3) Gamma distribution
x has Gamma distribution with parameters a and (3, denoted by G(a,j.3), 
if its density is
If x i , . . . ,  x n is a sample from a E xp(A) distribution then x =  X\H------ \-xn ~
G(n, A) (DeGroot, 1986, p. 289). So, a quantity having Gamma distribution 
with a =  n integer can be generated by drawing a sample of size n from 
the exponential distribution and summing up the sample values.
The x 2 distribution with n degrees of freedom, denoted by x «i is given 
by the G(n/2 ,1/2) distribution. Furthermore it is related to the normal 
distribution by the following result: if x\ ,... ,x n form a sample from the 
standard normal distribution then x — x\ +  ■ • • +  x\ ~  Xn (Mood, Gray- 
bill and Boes, 1974, p. 242). So, generation of a x i  distribution involves 
generation of a standard normal sample of size n and summation of their 
squares. This scheme depends on generation of normal quantities, to be 
presented in the next section. It extends the above generation of Gamma 
distributions by including those with a being an integer multiple of 1/2. 
Generation of Gamma quantities with an arbitrary value of a is deferred 
to Section 1.5.
The parameter j3 of the Gamma distribution is a scale parameter which 
implies that if Xi ~  G(a,(31) and X2 ~  G(a,/32) then (3\Xi and [32X2 have 
the same G (a, 1) distribution. This is a special case of a general result 
valid for any scale model (Exercise 1.5). A quantity y ~  G(a,(3) can be 
generated from x  ~  G (a, 1) by taking y =  x/p.
If x ~  G (a,fi) then x ~ 1 has inverted Gamma distribution with param­
eters a and (3, denoted by IG (a,f3). Generation of an IG quantity simply 
involves generation of a Gamma quantity with the same parameters and 
taking its inverse. The difficulty is therefore the same as that with a Gamma 
generation.
1.3.2 Bivariate techniques
If (x i,x 2) has joint density f x (x 1 ,^ 2) and g (x i ,x 2) =  (y i, 2/2) is a one-to- 
one differentiable transformation with inverse g~ x{y 1, 2/2) =  (^i, X2), then 
the density of (j/1, 2/2) is
x a~ le-^ x , 
x > 0.
fy{yi,V 2) =  fx(g l (yi,V2))J  where J =
dxi/dyi 
d x2/dyi 
dxi/dy-2 
d x2/dy2
Although the result concerns bivariate vectors, it is often useful for gener­
ation of scalar quantities.

Generation of continuous random quantities 
15
1) Normal distribution
x has normal distribution with mean 
and variance a2, denoted by N(fi, a 2), 
if its density is
M x ] f i ' a2) =  
exp 
The standard normal distribution is obtained when /i =  0 and a2 =  1.
As will be seen in the next chapter, it is particularly advantageous in 
the Bayesian context to work with the reparametrization (j> =  1/a2. The 
parameter 4>, the inverse of the variance, is usually referred to as precision. 
For this parametrization, the density is
(h1/2 
f 
cj) 
o'!
f N(x -fi,4> x) =  
exp j - - ( x - / z )  | .
The normal distribution is possibly the most used distribution in statistical 
applications. It is therefore fundamental to have a fast method of generation 
of normal quantities. Unfortunately, all methods available require at least 
two uniform quantities.
Box and Muller (1958) showed that if u\ and u2 are independent U [0,1] 
quantities, x\ =  yj—2 log u\ cos(2ttu2) and x2 =  y j - 2 log u\ sin{2nu2) 
then x\ and x 2 are independent A/’(0 ,1) quantities. The result is proved by 
constructing a transformation
( x i,x 2) =  g(u i,u 2) =  ( \ / - 2  log u\ cos(2ttu2), \J- 2  log u\ sin{2nu2)).
This is clearly a one-to-one transformation taking points from the unit 
square [0, l ]2 into R2 with inverse u\ =  exp{ — (xj +  x 2)/2} and u2 =  
(1/27t) tan- 1 (X2/.T1). As f u(ui, u2) =  1 for (u i: u2) G [0, l]2, and 0 other­
wise, f u(g~1( x i,x 2)) =  1 for (x i,x 2) € R2. Therefore, f x(x i,x 2) =  J for 
(xi, x 2) G R 2 and
J 
=
Xi exp{ — (x2 +  x|)/2} (1/2tt){1/[1 +  (x2/xi)2]}(-x2/xf)
x 2 exp{ — (x2 +  x\ )l2} 
(1/2tt){1/[1 +  (x 2/x i)2]}(1/x i )
x i exp{ —(xf +  x2)/2 } 
—(1/27t)[x2/( x 2 +  x|)]
x2 exp{ —(x2 +  x2)/2 } 
(l/27r)[xi/(xf +  x|)j
8XP
So, f x(x u x 2) =  / ( x i ) / ( x 2) where f ( x l) =  (1/V27r) e x p {-x 2/2 }, i =  1,2, 
proving the result. Another method based on pairs of uniform quantities is 
left as Exercise 1.9.
Finally, it is worth mentioning that a more natural and simpler method 
of normal generation is based on the central limit theorem. Consider a

16
Stochastic simulation
sample u \ ,...,u n from the 17[0,1] distribution. Then, for large enough n,
u 
1 / 2 
1 ^
x — \ fn------ ■== ~  AT(0,1) where u =  — > Ui.
l/\/l2 
n ^ x
A natural choice of n is 12 to simplify the expression of x but this is 
generally too small to yield a reasonable approximation, especially in the 
tails. Despite the historical interest, this method is very inefficient, requiring 
much more computing time than the Box-Muller method.
2) Ratio-of-uniforms method
This is a very general method proposed by Kinderman and Monahan (1977) 
for generation of quantities with an arbitrary density /  that may be known 
up to a proportionality constant. Hence, the method generates values of a 
density from which only the algebraic kernel is known. Let Xi and x 2 be 
quantities uniformly distributed in the region Cf =  {(x 'i, x 2) : 0 <  xi < 
y/f*{x2/x i) }. Then, y =  x 2jx\ has a distribution with density f(y ) =  
f*(y)/  f  f  (u)du.
Again, the result can be obtained by completing the transformation with
2 =  x\. The inverse is given by x\ =  z and x 2 — yz with |J| =  z. As 
f(x \ ,x 2) =  k for (x i ,x 2) € Cf where k~l =  area(C/), then f (y ,z ) =  kz,
0 > z >  y/f*(y). Integrating with respect to z,
/ w
 =  Jf ' / r a 5f e *  =  | A ' / m
 =  | r w .
Therefore, y has density proportional to /* . The uniform generation from 
Cf may be difficult for complicated forms of / .  This may be simplified 
(by graphical techniques) whenever it is possible to find the constants
a =  \/supx f* (x ) > 0, 61 =  ^/infaKo x 2f* (x ) and b2 =  y W x>0 x 2f* (x ) 
such that Cf C [0,a] x [61, 62]- Generation of a pair (y i,y 2) of independent 
uniform quantities on [0,a] x [61, 62] is easy by linear transformation. Pas­
sage from a uniform distribution on [0, a] x [61, 62] to a uniform distribution 
on Cf is achieved by rejection methods (see Section 1.5).
A very simple application of this method is the generation from the 
Cauchy distribution (see definition at the end of this section) by uniform 
generation of points in the unit semi-circle. Other important applications 
are the generation of Gamma (see also Section 1.5 and Exercise 1.15) and 
normal variates.
Wakefield, Gelfand and Smith (1991) studied generalizations of the 
method. They first considered generation of a pair (x i,x 2) uniformly dis­
tributed in the region C ft9 =  {(£ 1, 0:2) : 0 < xi <  g~ 1[cf*(x2/g'(xi))}} 
with a strictly increasing differentiable function g and a constant c >  0 
and showed that y =  x 2/g'{x\) has a distribution with density f(y ) =

f*(y)/  f  f*(u)du. A popular choice for g is the power family g{x) =  x r+ l/  
(r +  1 ) for r >  0 and c =  l /( r  +  1). If r =  1 , the original ratio-of-uniforms 
method is obtained and r =  0 corresponds to a uniform envelope studied 
in Example 1.4 below. They recommend using r =  0.5 which is optimal 
for the normal case in the sense of minimizing expected generation cost 
(Exercise 1.10) and behaves well in some other applications. They also 
show that relocation of /*  to have mode at zero is optimal. A multivariate 
generalization and proof of correctness of the algorithms are also left as 
exercises.
Generation of continuous random quantities 
17
3) Logarithmic distribution
Finally, another interesting, specific application of bivariate techniques is 
presented, x has logarithmic distribution if its density is given by fix )  =  
— log x for 0 < x <  1, and 0 otherwise. The probability integral transform 
does not have analytic solution but if u\ and u2 are independent (/[(), 1] 
quantities then x =  u\u2 has logarithmic distribution. To prove the result, 
complete the transformation g with y =  u\. The inverse g~ 1 is given by 
[y,x/y) with Jacobian
J =
0 
l/y
1 
~x/y2
=  i/y -
So, f {x ,y )  =  l/y, for 0 <  x <  y <  1, and 0 otherwise. Then,
/•i i
f { x ) =  
-d y  =  log y £ =  -  log x 
, 
0 <  x <  1 .
j  x y
1.3.3 Methods based on mixtures
Methods based on mixtures are also bivariate techniques. The motivation 
behind the method, however, is different with the original variables playing 
different roles. In essence, methods based on mixtures explore the simple 
fact that the joint density of {x i,x 2) satisfies
f { x  u x 2) =  f(xi\ x2) f i x 2).
Therefore, the pair ix i,x 2) can be generated in two steps: first, x 2 is gen­
erated and then x\ is generated conditionally on the generated value of 
x 2. This mechanism automatically provides a value x\ from its marginal 
distribution. Despite its simplicity, this result is of great importance in sim­
ulation. By breaking down the joint distribution, one is able to generate a 
value for x\ even when its marginal density is unavailable analytically or 
at least awkward to sample from directly.
The quantity x 2 is the mixing element. When it is discrete with prob­
ability function p(ai) =  Pi, Vi then the marginal density of x\ is given

18
Stochastic simulation
by
f ( x i) =  ^ P i M ^ i )  where f t(x i) =  /(x i| x2 =  a*).
i
If all fi are equal, then they are also equal to /  irrespective of the mixture 
weights Pi. A common choice in this case is pi =  1 jk  where k is the number 
of components in the mixture. This mixing scheme provides another form 
of generation from /  that improves the randomness of the pseudo-random 
generators without altering the target distribution.
A context where the occurrence of mixture is commonplace is in robust­
ness studies. Instead of assuming the usual N(fi, a2) model as appropriate, 
one assumes that the model is appropriate with probability 1 — e, for small 
value of e. There is also a probability e that the appropriate model is cor­
rupted by a shift in level N (ji +  a, a 2) or by an inflated variance N(ji, ka2), 
k > 1. In the first case,
/( x i )  =  (1 -  t ) f N{xi\ n ,a2) + e f N(x 1;n  +  a ,a 2).
A discrete indicator variable x 2 is constructed to indicate the component 
of the mixture to be chosen. If x2 =  0, x\ is drawn from the standard 
component but if x 2 =  1, Xi is drawn from the corrupted component. The 
model is completed by the specification of a bern(e) distribution for x 2. 
Similar calculations can be made for the case of variance contamination.
Discrete mixtures are also used in the fragmentation of a generation in 
many component parts, each having a given probability of being chosen. 
This technique is useful when some components are easy to generate from 
and concentrate most of the probability. In this case, a costly generation 
is replaced by another one that is cheap with high probability and only 
remains costly for low probability components.
Assume that one wishes to draw a value from a density /  but one knows 
how to sample from a density / i  satisfying /( x )  — aif\{x) >  0 for some 
oi >  0. It follows that
J ( f ( x )  -  a ifi(x ))d x  =  1 -  ai
and therefore, a density <?i(x) =  (/(x ) -  a i/i (x )) /( l  -  ai) may be defined. 
This implies a mixture expression /(x ) =  a i/i(x ) 4- (1 -a i)g i(x ). As draws 
from / 1 are easy to obtain, it is advantageous that ai be as large as possible 
subject to the requirements that ai <  1 and /  — a i/i > 0. This implies a 
value for ai given by the minimization of /( x ) //i ( x ) .
All that is left is the problem of generation from gi . Here again the same 
process of decomposition can be applied by choosing a density / 2 easy to 
draw from and satisfying g\(x) — 62/ 2(x) > 0. Again, it is advantageous 
to take the largest possible value for 62 (given by minx (yi (x) / / 2(x)) and 
to create g2(x) =  (gi(x) -  62/ 2(x ))/(l -  b2) such that gx{x) =  b2f 2(x)+

Generation of continuous random quantities
19
(1 -  62)52(3:)- Replacing 51 in the expression of / ,
/(x ) 
=  
fli/i(x ) +  (1 -  a i)62/ 2(x) +  (1 -  a i)(l -  62)52(2:)
=  
a i/i(x ) +  a2/ 2(x) +  (1 -  ai -  a2)52(x)
where 02 =  (1 — 01)62- This process of decomposition of densities may 
proceed indefinitely with successive choices of approximating densities /*+1 
for gi, i =  2 ,3 ,... with weights bi+\. The corresponding weights of f i+ 1 in 
the expression for /  are given by ai+1 =  (1 — a\ — ■ ■ ■ — ai)bi+ i, i =  1 , 2,.. . 
and ai for fi. If the weights ai, 02, - • • are well chosen, the residual densities 
gt, possibly more costly, will have negligible effect on the mixture and will 
not affect the efficiency of the method.
E xam ple 1.1 This technique was applied by Marsaglia and Bray (1964) to 
the generation from the N (0,1) distribution. Let x\ =  2{u\ +  u2+ u s ~  1-5) 
where the Ui are independent U(0,1). Note that E{x\) =  0 and V ar(x 1) =  
1/3 but |xi| <  3. The density of x 1 is
and 0 otherwise. The largest possible value for a\ is given by minx/( x ) //i ( x )  
=  / ( 2) / / i ( 2) =  0.8638. So, X\ may be used to generate N {0,1) variates 
more than 86% of the time. The residual 51 may be approximated by the 
triangular density
and 0 otherwise, which is the density of x 2 =  1 .5(114 +  W5 — 1 ) where the 
Ui are also independent C/(0,1). The largest possible value for 62 is given 
by minx g\(x)/f 2{x) =  5i(0.8739)//i(0.8739) =  0.8139 and a2 =  (1 — 
ai)b2 =  0.1107. So, the components X\ and x 2 that are very easy to obtain 
are responsible for generation at 86.38% +  11.07% =  97.45% of the time. 
The remaining probability is split into two parts: the remaining densities 
fz, defined for \x\ > 3, and f i  in the interval [—3,3]. The density fz is 
given by the N (0 ,1) density restricted to |x| >  3 which has probability
03 =  ii- l>3/jv (x ;0 ,1 )dx =  0.0027. The density /4 is given by
with weight 
=  1 — ai — a2 — 03 =  0.0228. Generation from fz and f\ is 
accomplished via rejection techniques. Details of these generation procedures 
are left as Exercise 1.8.
If the quantity x 2 is continuous, the marginal density of xi is
/ 2(x) =  (6 -4 | x | )/9 , 
|a:| < 1 .5
f , , 
/„ (x ; 0,1) -  O i/i(x) -  a2f 2(x) -  o3/ 3(x)
Ji{x ) = -------------------;-------------------------------------------
1 — ai — a2 — <23

20
Stochastic simulation
An example that appears many times in this book is a scale mixture of nor­
mal densities (Andrews and Mallows, 1974) where f(xi\ x2) =  N (/i,X202). 
Many distributions may be obtained in this way. The most famous one is 
the Student’s t distribution but the double exponential, logistic and stable 
distributions can also be obtained as scale mixtures.
x  has Student’s t distribution with n degrees of freedom (n >  0), mean 
H (fi € R) and scale parameter a 2 (a2 >  0), denoted by tn(n,cr2), if its 
density is
When n =  1, the distribution is called the Cauchy distribution.
The joint distribution of a random pair (xi, x 2) is called normal-Gamma 
with parameters /i,cr2,n  and S, denoted by N G (fi,a 2,n ,S ), if x\\x2 ~  
N(^i,a2/x2) and x 2 ~  G(n/2, nS/2). An alternative representation is x\\x2 
~  N (^ ,a 2x 2) and x 2 ~  IG (n/2,nS/2). It can be shown that the distribu­
tion of x\ is trl(ji, Sa2) for both representations (see, for example, DeGroot, 
1970). So, generation of a t,n(ji, a2) can be made in two steps: first, the scale 
x 2 is drawn from a G(n/2,n/2) (taking 5 = 1 )  and then x\ is drawn from 
a N(fi,cr2/x2) with the value of x 2 generated in the first step.
E xam ple 1.2 Values from some of the distributions described above were 
obtained. Plots of the resulting histograms are shown in Figure 1.1, where 
a very good agreement with the corresponding density is clearly observed.
1.4 G en eration  o f  random  vectors and m atrices
As for univariate quantities, the main generation techniques for random 
vectors and matrices are the probability integral transform and general 
transformations in the style of those in the previous section. The first class 
of transformation is used to generate independent univariate quantities 
on which specific transformations are operated to introduce correlation 
between the elements of a random vector or matrix.
If x  =  ( x i ,... ,x d)' has joint density f x {x\ ,... ,Xd) and g (x \,. . . ,  x d) 
=  (y i,. . . ,  yd)' is a differentiable one-to-one transformation with inverse 
9 
{y i,-- -,yd) =  ( x i ,.. . , x dy, the density of (y i,.. .,y d)' is
T [ ( n + l ) /2]
r(n/2)r(l/2)
f y(yi,---,V d ) =  fx(g 1( y i ,. ..,y d))J
where
d x i /d y i 
••• 
d x d/d y i
J =
d x i /d y d 
••• 
d x d/d y d

Generation of random vectors and matrices
21
(a)
(c)
“I---- 1----1--- 1
-2 
0 
2 
4
_j|
i----1----1----r~
- 4 - 2  
0 
2
i----- 1----- r~
0 
2 
4 
6 
8 
10 
0.0 
0.2 
0.4 
0.6 
0.8 
1 0 
-5 
0 
5 
10
Figure 1.1 Histograms based on 10,000 draws from several distributions, (a) 
N (0 ,1 ) (central limit theorem applied to averages of uniforms), (b) N (0,1) (prob­
ability integral transform), (c) N (0,1) (Box-Muller result), (d) N (0,1) (Box- 
Muller ma rejection, see Exercise 1.9), (e) Poi(5), (f) G (0 .5 ,1), (g) G( 1.5,1), 
(h) Beta(0.5,0.5) (see Exercise 1.18), (i) <5(0,1).
1.4-1 Multivariate normal distribution
x =  ( x i ,... ,XdY has multivariate (or d-variate) normal distribution with 
mean p and variance £, denoted by iV(/z, £ ) or Nd(fi, E) when specification

22
Stochastic simulation
of the dimension is useful, if its density is
/n (x ', (j,, E) =  (2tt) p/2|E| 1/2 exp | - i ( x  - /x)'E ^ x - ^ j
The variance matrix E must have full rank d for the above density to exist. 
Nevertheless, there are some advantages in keeping the notation even when 
E is not of full rank. The standard normal distribution is obtained when 
/i =  0 and E =  Id, the identity matrix of order d. In this case, it is easy to 
see that the components x, are independent standard normal quantities. 
The univariate normal distribution is the special case where d =  1.
The normal distribution possesses many properties that are particularly 
attractive from an applied point of view, making it one of the main proba­
bility distributions. The most important properties for the development of 
this text are
• Linear transformations: if A is a r x d matrix of constants and b is a 
r-dimensional vector of constants and x ~  Nd(ji, E) then
• Marginal distributions: if the vector x is divided into 2 blocks, X\ contain­
ing the first d\ components of x  and x2 containing the other d2 =  d — d\ 
components, then applying a similar partition on /x and E in the form
leads to Xj ~  Ndi{ni, EJ4), i =  1,2.
• Conditional distributions: still using the same partitions on x, /i and E
where/xi.2 =  H\ +  12i2E22(x2- p 2) and E n ,2 =  E n - E i 2E22 E21. Anal­
ogous results are obtained for the distribution of x 2|xi by exchanging 
all indices with values 1 and 2. For these results, it is obviously required 
that the submatrices E22 and E n respectively are of full rank, otherwise 
their inverses will not exist.
• Reconstruction of the joint distribution: if xi|x2 
Ndl(ni +  B i(x 2 -  
M2)) B'l) for di x d2 and d\ x d\ matrices of constants B\ and B 2 respec­
tively and x 2 ~  Nd2(fi2, E22) then
y =  A x +  b ~  Nr(Afi +  b, A ’EA').
(1.1)
X\ |x2 ~  Ndl ini-2, E n .2)
(1.2)
(1.3)
where En — B 2 +  B xE22-B( and S 21 =  E i2 =  B\E22.
• Quadratic forms: (x — /i)'E _1(x -  /i) ~  x d-
These results can be found in standard multivariate analysis textbooks 
such as Anderson (1958).

Generation of random vectors and matrices
23
Generation of a d-variate normal random vector y ~  N(ji, E) uses the 
techniques described at the beginning of this section. Initially, d indepen­
dent A'r(0 ,1) random quantities are generated according to one of the tech­
niques described in the previous section thus forming a d-dimensional vector 
x with a N (0 ,Id) distribution. A N(fi, E) variate is obtained after using 
the transformation (1 .1 ) with b =  fi and E =  A A '. The d x d matrix A is 
the square root matrix of E and there are many algorithms for finding a so­
lution for A. One of them is the singular value decomposition. It prescribes 
how to obtain an orthogonal matrix Q whose columns are the eigenvectors 
of E and a diagonal matrix A with main diagonal containing the eigenval­
ues of E such that E =  QAQ' =  P P 1 where P  — QK1/2. Another algorithm 
with possibly lower computational cost is the Cholesky decomposition that 
finds the unique solution for A in the convenient form of a lower triangular 
matrix (see Chambers (1977) for details of the calculations).
1.4-2 Wishart distribution
A d x d symmetric and positive-definite matrix X  has Wishart distribution 
with a degrees of freedom and parameter /?, where a >  (d — l)/2  and /3 
is a d x d symmetric and positive-definite matrix, denoted by W (a,f3) or 
Wd(a,(3) when specification of the dimension is useful, if its density is
f w {X -,a,P ) = --------------2 
------- —
 \X\a~(d+l)/2 exp { —tr(/3X)}
J W K  
’ 
7rd ( d - l ) /4  J ^ =1 T  (a  —
for positive-definite values of X  and 0 otherwise. Note that if d =  1 and 
therefore X  is scalar, the density above reduces to the G (a, (3) density.
It can be shown that if x i , . . . ,  x n are a sample from a Nd(fi, E) then
n
^ ( x s -  m )(Xi -  M)' -  W d ( n / 2 ,  E - 1/2). 
i= 1
So, an easy generation method for the Wd(a,P ) is available from a sample 
of (integer) size n =  2a from the Nd(fi, (3~1/2) distribution. As in the 
univariate case of Gamma generations, this normal based method only 
allows generation of Wishart matrices with integer multiples of 1/2 degrees 
of freedom.
Defining now yi =  A(xi — ft), i =  1 ,... ,n  where A is the square root 
matrix of £ - 1  implies that y i , . . . , y n are a sample from the Nd(0, Id) and 
Eiyiy'i ~  Wd(n/2, Id/2). On the other hand,
'YjViVi =  
- » ) ] '  =  
-  v )(x i -  vYA '-
z = l 
i—l 
i = 1
So, if x ~  Wd(a,f3) then AxA' ~  Wd{a,A(3A') =  Wd(a ,I d) if /3_1 =  
AA!. The condition is also sufficient in that if y ~  Wd(a ,Id) and /3 is

24
Stochastic simulation
a positive-definite matrix and admits the decomposition (5 =  AA! then 
Ay A' ~  Wd(a,/3).
This result allows generation of Wishart matrices from independent nor­
mal variates. The basic form to draw a d x d matrix x ~  W (n j2, Id/2) is to
generate independent dn N (0, 1 ) quantities i n ........x \n, . . . ,  x^i, . . . ,  x^n,
form a sample of vectors Xi =  [ x u ,... ,Xdi)' ~  N^{0, Id), i =  1 , . . . ,  n and
n
r  =  ^ ® i® ;~ W ,i(n /2, / d/ 2).
i=l
If a matrix X  =  ( x i , ..., x „ )  is constructed then y  =  X X ' and more 
efficient methods using other decompositions and possibly using a smaller 
number of variate generations may be devised.
Odell and Feiveson (1966) explored this idea using the decomposition 
Y =  LL' where L is a d x d lower triangular matrix. They showed that if the 
main diagonal elements la of L are square roots of G((n — i + 1 )/2, 1 /2 ) (or 
X n -i+i) quantities, the off-diagonal elements ltJ of L are N (0,1) quantities 
and if all these variates are independently drawn then Y 
Wd(n/2,Id/2). 
This method only involves d{d — l )/2 normal draws and d Gamma draws 
which implies great computational savings when compared to the brute 
force method that requires dn normal draws. The larger the values of d 
and n, the greater the computational savings. Another side advantage of 
this method is to allow easy calculation of the determinant of Y since
|y| =  |LL'| =  (|L|)2 =  ( a / » ) 2-
If X  has W (a, fi) distribution then X  1 is said to have inverted Wishart 
distribution with the same parameters, denoted by IW (a ,0 ) or IWd{a,(3). 
As in the univariate case with the Gamma distribution, a random matrix 
with IW (a,(3) distribution is obtained by drawing a W (a, [3) matrix and 
inverting it.
1.4-3 Multivariate Student’s t distribution
x has multivariate (or d-variate) Student’s t distribution with n degrees of 
freedom, mean /i and scale parameter E where n >  0, /1 e Rd and E is 
a d x d symmetric and positive-definite matrix, denoted by in(/x, E), if its 
density is
ft(x ;n ,n , E) =
r[(n  +  d)/2] ip i-1/2
r(n/2)(mr)d/2
1
1 H— (x —/u)'E J(x — (i)
The result of the previous section relating normal and Student’s t dis­
tributions can be extended for the multivariate case. The difference here 
is that two possibilities for the scale mixture form are available. In the 
first one, the scale of the normal distribution is altered by a scalar random 
quantity and in the second one it is altered by a random matrix.

Resampling methods
25
A pair (x i,x 2), where x\ is a d-dimensional random vector and x 2 is a 
random variable, has multivariate normal-Gamma distribution with param­
eters /x, £ ,n and S, denoted N G (n ,H ,n ,S ) or NGd(fi, £, n, S), ifxi|x2 ~  
iVd(/x, E /x 2) and x 2 ~  G(n/2,nS/2). This distribution is a multivariate 
extension of the normal-Gamma and also follows that xi ~  tn(fi, ST,).
A pair ( x i,X 2), where X\ is a d-dimensional random vector and X 2 
is a random matrix, has multivariate normal-Wishart distribution with 
parameters n,cr2,n  and S, denoted N W (n ,H ,n ,S ) or N W d (n ,^ ,n , S), 
if x\\X2 ~  Ndin, cP’X ^ 1) and X 2 ~  Wd(n/2,nS/2). This distribution is 
another multivariate extension of the normal-Gamma and also follows that
xi ~  tn(p.,cr2S).
The above results provide two ways of generating multivariate Student’s 
t variates via a scale mixture of normals. In the first one, a quantity x 2 is 
drawn from a G(n/2, n/2) distribution and a vector x\ is generated from a 
N (h,Y,/x2). In the second one, a matrix X 2 is drawn from a W (n/2, n £ /2 ) 
distribution and a vector xi is generated from a N ^ ^ X ^ 1). Both cases 
will require a (Cholesky) decomposition of E. The first method needs one 
generation from a Gamma distribution and d normal generations while the 
second method, even when the efficient technique of Odell and Feiveson 
(1966) is used, needs d Gamma generations and d +  d (d - l)/2  =  d (d + 1)/2 
normal generations. There is a clear computational advantage of the first 
method.
1.5 Resampling methods
Resampling methods can generically be described as generation techniques 
that require sampling of random variables in more than one step. Typically, 
they consist of two steps with the first one providing a value sampled from 
an approximating distribution. The second step is some sort of correction 
mechanism whose function is to redirect the sample so as to make it (at least 
approximately) representative of the distribution of interest. The correction 
mechanism is typically but not necessarily stochastic. In what follows, the 
density of interest will be denoted by 7r and the auxiliary density denoted by 
q. Unless otherwise stated, the generated values x can be scalars, vectors, 
matrices or even higher-dimensional arrays.
1.5.1 Rejection method
This method uses an auxiliary density for generation of random quantities 
from distributions not amenable to analytic treatment. More specifically, 
consider the generation of values from a density 7r for which none of the 
above methods provides a solution. Consider also an auxiliary density q 
from which draws can be made. The idea is to use q to make generations 
from 7r. The only mathematical restriction over q is that there must exist a 
constant A <  00 such that 7r(x) < Aq(x), for every possible value of x. For

that reason, q is usually referred to as a blanketing density or an envelope 
and A is the envelope constant.
26 
Stochastic simulation
Figure 1.2 Enveloping the N(0,1) density (solid line) by a multiple of the 
<i(0,2.5) (Cauchy) density (dashed line). Note that an increased scale of the en­
velope with A = %/1.257T was needed to ensure complete blanketing.
The method is general enough to generate values from 7r without knowing 
the complete expression for 7r. Generation from partially specified densities 
were already presented for the ratio-of-uniforms method. As will be seen 
throughout this book, it is extremely common in statistics to encounter 
such situations where the kernel of 7r is known but the constant ensuring it 
integrates to 1 cannot be obtained analytically. Rejection methods generate 
from 7r by blanketing it with Aq irrespective of the scale used. If one is 
working with n*(x) =  kir(x) for some constant k =  f  ir*(x)dx, one would 
simply need to specify the envelope constant A* ~  kA instead of A to 
ensure a complete envelope. Figure 1.2 shows the blanketing of the standard 
normal density by a Cauchy density.
The method consists of independently drawing x from q and u /NJ f/[0,l] 
and accepting x as a value generated from 7r if Auq(x) < n(x). Otherwise, 
x is not accepted as a value from n and the process must be reinitialized 
until a value x is accepted. Hence the name of the method, which is also 
known as the acceptance/rejection method. The quantity to be generated 
can have any form: scalar, vector or matrix. In each case, the rejection

Resampling methods
27
step is based on a comparison of densities with the aid of a scalar uniform 
variate u.
E xam ple 1.3 In order to sample from a N (0,1) the rejection method was 
implemented and M  =  5000 observations were sampled from a £i(0,2.5) dis­
tribution. A total of 2491 draws were accepted as draws from N (0,1), giving 
an acceptance rate of 49.82%. Figure 1.3(a) shows the ratio ■K(x)/Aq(x), 
which penalizes departures from the center of the N (0,1) distribution, thus 
making the acceptance of tail values (farther from 0 than -3 and 3) virtually 
impossible. Figure 1.3(b) shows the histogram based on the accepted draws, 
which reveals a remarkable agreement with the N (0, 1 ) distribution.
J
Figure 1.3 Acceptance/rejection method, (a) Ratio tt(x)/A q(x). (b) Histogram 
based on the accepted draws and the target N (0,1) distribution. Approximate 
mean and variance are —0.001 and 0.968, respectively.
E xam ple 1.4 A simple use of rejection is sampling from a complicated 
density tt which is non-zero over the interval [01, 02] with ai <  a2 and 0 
otherwise. Also, assume that it is bounded by a constant b >  0. An envelope 
is provided by the U [ai,a2] distribution with density q(x) =  1/(02 — 01), 
for x € [01, 02], and 0 otherwise. Total blanketing is ensured if Aq(x) > b, 
for all possible values of x and for simplicity assume Aq(x) =  b. Hence, a 
draw x from ir is obtained by:
1. drawing x ~  U[a\,a2\ and u ~  U[0,1];

28
Stochastic simulation
2. accepting x if u < tt(x) jb and rejecting it otherwise.
This method was used, for example, to sample from the fourth component 
in the four-part mixture decomposition of the normal distribution described 
in Example 1.1 (see also Exercise 1.8e). Note that this idea can be applied 
when using rejection techniques for d-variate generation. In this case, n is 
non-zero over a region n f= i[aiii a«2] and q is the d-dimensional uniform 
distribution over the above region.
The proof that the rejection procedure effectively generates values from 
7r simply requires one to show that the conditional density of \x\Auq{x) < 
7r(x)] is 7r. The joint density of x, u is f(x , u) =  f(x )f(u ) =  q(x) by the 
independence between x and u and uniformity of u. Applying Bayes’ the­
orem,
The required density is given by the normalized version of 7r. So, the com­
plete knowledge of the integrating constant of 7r is not required; only the 
kernel is needed. When 7r is already the complete expression of a density, 
the normalization is not needed as f  n(x)dx =  1 .
So far nothing has been said about the choice of q. Obviously, it must 
be a density that is easy to draw values from. Otherwise, one is back to 
the original problem of generating from a difficult density. The rejection 
step states that x is accepted if u < ir(x)/Aq(x). When q is close to 7r, the 
envelope constant must be only slightly larger than 1 to ensure a complete 
envelope. In the limit, as 7r and q are the same, A =  1. On the other 
hand, if q(x) is very different from tt(x) for some values of x, A will have 
to be substantially larger than 1 to ensure an envelope. In any case, the 
closer q and 7r are, the smaller the value of A and n(x) j  Aq{x) will be 
close to but smaller than 1. This will lead to a more likely acceptance of 
the proposed value. So, the efficiency of the method is directly related to 
similarity between 7r and q.
The (overall) acceptance probability is
f(x\Auq(x) <  7r(x))
P r(A uq(x) <  7r(x)|x )f(x )
f  P r(A u q(x) <  7r(x)|x )f(x )d x  
[k(x) / Aq{x)\q(x)
f  [7 f(x ) / Aq(x)\q(x)dx 
7 r(x )
/  7T(x)dx

Resampling methods
29
From this expression, A must be chosen as close as possible to /  tx(x)dx, 
which equals 1, if it is already normalized. Also, smaller values of A lead 
to larger acceptance probabilities. So A must be as small as possible and 
the only limitation is that it provides an envelope.
Great care must be exercised for the choice of A, especially when sam­
pling from higher-dimensional densities. Often, two densities are similar in 
the region concentrating the bulk of the probability but their tail behaviors 
are very different. This may lead to very large values of A being required 
for total blanketing. When obtaining A numerically, such dangers that are 
hidden in the tails may go unnoticed and result in a constant that does not 
really provide an envelope. This unfortunately leads to a dilemma where 
a total envelope can only be achieved at the expense of very low accep­
tance probabilities. A more practical approach is to use a smaller constant 
that envelopes tx only with high probability. This approach is only suit­
able for sampling in a subset of the space and should only be used after 
some analytic experimentation has ensured a very large probability for this 
subset.
A limiting case of rejection sampling is given for truncated distributions. 
Let q be any density and n its truncation to the region C. Formally, x 
has density n(x) =  kq(x), x G C where fc- 1  =  Jc q(x)dx and 0 otherwise. 
Taking A =  k and q as the envelope density leads to accepting a generated 
value from q if u <  Tr(x)/kq(x) which is 1 if x  € C  and 0 if x 
C. 
Hence, to draw a value x from q restricted to C, one simply has to draw 
from q and accept it if and only if it is in C, rejecting it otherwise. This 
simple result is heavily used in the ratio-of-uniforms method. Recall from 
Section 1.3 that the method is based on sampling uniformly from a region 
Cf £ [0,a] x [61, 62]- To do that, one simply has to draw from a bivariate 
uniform distribution on [0,a] x [61, 62] and retain values belonging to Cf. 
In some cases, even tighter bounds on Cf can be found but the truncation 
idea remains the same.
Example 1.5 Cheng and Feast (1979) consider the generation of y ~  
G (a, 1) quantity, a >  1, by the ratio-of-uniforms method. They showed that 
the region Cf is given by { (x i,x 2) : 0 <  x\ <  (x2/X\)a~1e~X2/Xl} and that 
Cf C [0, a] x [0,6] where a =  [(a — l ) /e ]^“- 1 ^ 2 and b =  [(a 4- l)/e]^Q+1^ 2. 
After drawing x\ ~  [/[0,a] and x 2 ~  t/[0 ,6] independently, the method re­
duces to accepting y =  x 2/x\ if and only if 2 log X\ — (a — 1) log y +  y < 1 
and rejecting it otherwise. Tighter bounds to Cf can be found when a > 2.5, 
which leads to the more efficient algorithm:
1. draw u\ ~  L/[0, 1] and u2 ~  C/[0, 1] independently and set c =  [a —
(1/6  a)];
2. if a >  2.5, reset U\ =  u2 +  (1 — 1.86ui)/v/a until 0 <  U\ < 1;
3. accept y =  cu2/u\ if 2 log Ui — (a — 1) log y/(a — 1) +  y < 1 and reject
it otherwise.

30
Stochastic simulation
The case a <  1 was tackled by Ahrens and Dieter (1974). Their algorithm
is:
1. draw Mi ~  U[0, 1] and u2 ~  C/[0, 1] independently and set c =  1 +  a/e;
2. ifu\ <  l/c, se tx  =  {cu i)l/a andy =  e~x, otherwise set x =  — log[c(l — 
u{)/a\ and y =  x a~1;
3. accept x if u2 <  y and reject it otherwise.
When a <  1, another sampling scheme using the algorithm for a >  1 
is to take u ~  C/[0,1] and y ~  G (a +  1,1) independently and take x =  
yu1/°c (Exercise 1.17). Whatever the value of a, once a draw x from a 
G (a, 1) is obtained, x/f3 ~  G(a,j3) (Exercise 1.5). This algorithm was 
applied and the resulting Gamma samples is also shown in Figures 1.1(f) 
and 1.1(g), respectively. Many other algorithms for gamma generation have 
been proposed (Ripley, 1987).
If computation of 7r is costly then some other auxiliary function s(x) < 
tt(x ) can be used as a preliminary test (or pretest). Note that 7r gets 
squeezed between s and q. In this case, before verifying acceptance, one 
verifies whether u < s(x)/Aq(x). As s(x)/Aq(x) < ir(x)/Aq(x), the method 
will indicate acceptance of x without needing evaluation of ir(x). Only in 
the negative case, the method proceeds as before. Again, if the pretest is 
efficient (s is close to n) and the rejection method is efficient (q is close 
to 7r) then ■n will need to be evaluated only a few times. This procedure is 
called pretesting or squeezing (Marsaglia, 1977).
1.5.2 Weighted resampling method
Determination of the constant A is vital for the use of rejection methods. 
As already shown, this is by no means an easy task in general. Weighted 
resampling techniques are based on borrowing the good idea of sampling 
from an approximating density q without having to find the constant A. The 
disadvantage of the method is that it only provides values approximately 
distributed according to w.
Assuming the presence of a sample X i,... ,x n from q, weights
ir(xi)/q{xi) 
. _
L ,j= in x j)/ q (x j)
are constructed. A second sample of size to is drawn from the discrete 
distribution on { x i , ... ,x n} with probabilities w i,... ,w „. The resulting 
sample has approximate distribution 7r. It is worth noting that here again 
the complete expression of n is not needed. If only ir*(x) =  kn(x) is known 
without knowledge of the normalizing constant k, the weights Wi do not 
alter. This method was initially proposed by Rubin (1987) in a comment 
and later discussed in more detail by Rubin (1988) and can be applied to 
quantities x of any dimension. The example considered by Rubin (1987)

Resampling methods
31
suggested that q be taken as a product of univariate normal densities for 
the components of x , after transforming them to take values on the real 
line. So, a logarithmic operation would be imposed over positive quantities. 
In the next chapters, better approximations will be presented.
To verify that the method generates approximately from n, consider the 
distribution function of a univariate x (for ease of exposition) generated by 
the algorithm
Fx (a) =  P r(x <  a) =  
^
£ ”= 1 7T(xi)/q(xi)l(xi < a)
{i: Xi<a} 
Z i = l * ( X i ) / q ( X i )  
'
Under appropriate regularity conditions usually satisfied in practice, when 
n —> oo, a large quantity of a^s are available with frequencies according to 
q, and sums become integrals weighted by q. Therefore,
J[tt(x )/q(x)]I(x < a) q(x) dx 
f  n (x )I(x  < a) dx
x a 
f[n (x)/ q (x)]q(x)d x 
fT r(x)d x
that is the distribution function associated with the density tt. The deriva­
tion above can be extended with more cumbersome notation to the case 
of a vector or matrix x. It is clear from above that if the initial sample is 
sufficiently large the resulting sample will have approximate distribution
TT.
As for the rejection method, q must reflect as much as possible the char­
acteristics of tt. Otherwise, the sample will be an inappropriate basis for 
resampling, compromising the method. This point is better understood by 
referring to Figure 1.4 with a bad choice for the starting point q in the uni­
variate case. A sample from q, also marked in the figure, is almost entirely 
concentrated in regions where tt is negligible and, worse still, is almost ab­
sent from the region where n is relevant. As a result, the resample will be 
a very bad representation of the distribution of interest unless the initial 
sample size n is extremely large. For high dimensional problems, this is 
virtually impossible to attain. In summary, q should be as similar to tt as 
possible or at least allow sampling of all possible values of tt.
The weighted resample can have any size, unlike the rejection method 
where the resulting sample is necessarily smaller than the original one. 
The resample can be drawn with or without replacement without affecting 
the results provided the initial sample n is of order of magnitude larger 
than the resulting sample m. The most important requirement is to have 
enough initial points to cover adequately the regions of interest of tt. Rubin 
(1987, 1988) points out that the ratio n/m should be decreased towards 1 
as q and tt get similar and suggests taking n /m  =  20. He uses the name 
sampling/importance resampling (SIR, in short) in analogy with Monte 
Carlo integration (Section 3.5) where sampling-based approximations of 
integrals may be improved by an appropriate choice of the sampling density.

32
Stochastic simulation
Figure 1.4 Example of an inadequate choice of initial density q, with tt(x ) =  
0.7/a?(x ; 5,2 .52) +  0.3/jv(a;; 0, 22) and q(x) =  ft(x; 2 ,—5,1.44). Small negative 
vertical bars indicate the locations of 200 draws from q. Positive bars indicate the 
weights of the 200 draws computed as Wi =  n(xi)/ q(xi).
Weighted resampling is a more concise description although the preferred 
nomenclature is a matter of personal taste.
1.5.3 Adaptive rejection method
The analytic form of the density in complex situations, to be presented 
in the sequel, virtually prevents the establishment of envelopes by simple 
study of their mathematical expressions. Nevertheless, these envelopes may 
be constructed with the aid of graphical techniques. This is certainly vi­
able for the important and encompassing class of log-concave densities. A 
density n(x) is log-concave if the vector of the derivative of its logarithm 
exists and has components that are non-increasing in x. Another defini­
tion is to have a non-positive determinant of the matrix of second deriva­
tives of its logarithm for all values of x. Log-concavity can also be defined 
without using the derivatives. For the univariate case, 7r is log-concave if 
log 7r(aa;i +  (1 — a )x2) > a lo g 7r(xi) +  (1 — a) log ^(2:2), for x\ <  x 2 and 
a € (0, 1 ), or d logir/dx is a non-increasing function of x or d2 logit/dx2 
is a non-positive function of x. Many densities are log-concave including

the normal, the Gamma (a > 1), the Wishart (a > (p +  l)/2 ) and the 
densities arising in the context of generalized linear models with canonical 
link (Dellaportas and Smith, 1993). A list with log-concavity checks for the 
main distributions is given by Gilks and Wild (1992, Table 2).
Resampling methods 
33
Figure 1.5 Adaptive envelope of a log-concave density. The G (2,l) density is 
graphed above in the log scale (solid line) with a starting grid {0.5,1.5, 2.5} of 
size n =  3 (vertical dotted lines). The envelope q is provided by the tangent lines 
at the grid points (dashed lines). Lower bounds can be provided by joining the 
density at these points.
Log-concavity can be explored to construct easy envelopes graphically. 
For the univariate case, Gilks and Wild (1992) developed a method based 
on an approximation of the logarithm of n by line segments that blanket 
the density from above (and also from below if squeezing techniques are 
also applied). Simple forms of envelopes are obtained by intersecting the 
tangent or secant lines at pre-chosen points x i , ... ,x n. Lower bounds are 
provided by the line segments connecting successive points and can be used 
for pretesting. These are illustrated in Figure 1.5 where a reasonable ap­
proximation was obtained with the number of points as small as 3 and it 
improves as the number of points used increases. The use of tangents pro­
vides a better approximation than the use of secants but the only obvious 
choice for lower bounds is provided by the chords. The disadvantage of

34
Stochastic simulation
tangent lines is the need to evaluate the derivatives of log ir (or of 7r) at 
points x i ,x 2, ■ ■. ,x n. The envelope based on secant lines only requires the 
evaluation of ir at these points (Gilks, 1992). Therefore, the choice between 
tangent or secant lines has to do with the computational price of evaluation 
of the derivatives of 7r.
In any case, the function enveloping log 7r is a succession of lines with 
different inclination and hence the function enveloping 7r is a succession of 
exponentials. The density q can be taken as the density of the piecewise 
exponential distributions with parameters given by the derivatives of log n 
at points x i , ... ,x n (in the tangent case) and partition determined by the 
intersection points of adjacent lines. It is not difficult to draw values from 
this distribution (Exercise 1.20) and an envelope is obtained graphically. 
Wild and Gilks (1993) detail the algorithm and a Fortran code is available 
from these authors.
The adaptation referred to in the name of this method has to do with 
the fact that the envelopes can be increasingly improved as more points 
are added to the grid { x i , . . . , x „ }. With such an initial grid, a piecewise 
exponential distribution is obtained for q and a value x* drawn from it. If 
the value is accepted, the process is terminated and x* is a value sampled 
from 7r. If the value is rejected, it can be incorporated to the current grid. 
If x* belongs to (x j,x j+1] for 1 <  j  <  n — 1, a new tangent line is incorpo­
rated to the envelope at that point. As the size of the grid increases, the 
approximation of q to tt becomes better and rejection becomes less likely.
In general 2 to 4 points are enough for the starting grid and on average the 
same number of points are drawn before a value is accepted. Log-concavity 
is essential to ensure that a succession of tangent or secant lines provides 
an envelope and the existence of an envelope is a condition for the rejection 
method. In Chapter 6, a variation of the rejection method that does not 
require the approximating density to blanket the target density completely 
is presented. Gilks, Best and Tan (1995) used this idea to construct an 
adaptive sampling method for cases where the density of interest is not 
log-concave and hence the succession of lines does not envelope it entirely.
The technique of constructing an envelope with tangent lines used by 
the adaptive rejection method in the univariate case may be extended to 
tangent planes (and hyperplanes) in the bivariate (multivariate) case. The 
resulting density obtained by intersecting the planes will still be an en­
velope. The main problem is the difficulty in sampling from this envelope, 
making this technique very unattractive for use beyond the univariate case.
1.6 E xercises
1.1 Show that reordering the possible values Xj of a discrete random quan­
tity x  according to a decreasing order of their respective probabilities speeds 
up generation of x by reducing the average number of comparisons.

Exercises
35
1.2  Describe a scheme to draw values from the following discrete distribu­
tions:
(a) multinomial;
(b) hypergeometric.
1.3 Prove that u ~  U[0,1] if and only if 1 — u ~  £/[0,1].
1.4 Obtain the distribution function of the Weibull distribution and use the 
probability integral transform to confirm that [—(1/A) log u]l/a has Weibull 
distribution with parameters a and X, where u ~  t/[0 ,1].
1.5 x admits a location model if its density has the form f(x\a) =  fo (x -a ), 
admits a scale model if its density has the form f(x\b) — (l/b)fo(x/b) 
and admits a location-scale model if its density has the form f(x\a,b) =  
(l/b)fo[(x — a)/b] where fo(x) is a density depending only on x.
(a) Show that generation of x from a quantity xq generated from fo is 
given by x — xq +  a in the location model, x =  bxo in the scale model 
and x  =  bxo +  a in the location-scale model.
(b) Apply the results in (a) to draw samples from the U[a — 1/2, a +  1/2], 
U[0 ,6] and U[a — b,a +  b] distributions based on samples from the C/[0,1] 
distribution.
1.6 Let ui and u2 be independent t/[0 ,1] variates, x^ — 2u\ — 1, i — 1,2 
and w =  uI +  u2. If w > 1, a new pair (u\,u2) should be considered. If 
w <  1, define yi =  \J—2(log w)/w Xi, i =  1,2. Show that y\ and y2 are 
independent N (0, 1 ) quantities.
1.7 Use the mixture technique to draw values from the density f{x ) =  
1.5(1 — x 2), 0 <  x  <  1, using as approximation f\{x) — 2(1 — x), 0 < 
x < 1, and 0 otherwise. Obtain the value of a\ and the expression of the 
residual density gi(x). Propose alternative sampling schemes and discuss 
advantages and disadvantages of the mixture technique for this particular 
case.
1.8 (Marsaglia and Bray, 1964) A standard normal quantity x may be
generated using mixtures. Assume u \ ,...,u y  are independent C/[0,1] and 
consider components x\ =  2{u\ +  u2 +  U3 — 1.5), x 2 =  1 .5(u4 +  U5 — 1), X3 
providing the value of x when |x| > 3 using the method of Box and Muller 
and rejection and X4 being the component that completes the mixture. Each 
component Xi has density fi and probability ai, i =  1 , 2,3,4.
(a) Show that xi and x 2 have densities
( (3 -  x 2 ) / 8  
, 
|x| < 1
< 
(3 — |x|)2/16 
, 
1 < |x| <  3 
and
[ 0 
, 
otherwise
I  (6 — 4|x|)/9 
, 
|x| <  1.5
[ 0 
, 
otherwise 
'
f i ( x ) 
= 
f 2(x) 
=

36
Stochastic simulation
(b ) Show (analytically) that the largest weight component X\ can have is 
a\ =  0.8638.
(c) Show (numerically or graphically) that the largest weight component 
X2 can have is a2 =  0.1107.
(d) Show that component x 3 has weight a3 =  0.0027 and describe how to 
implement a draw from 13 .
(e) Show that component X4 has weight a4 =  0.0228 and can be generated 
by taking the first value of ue such that f^Qu^ — 3) > 0.3181^7 where
f , s, 
f(x) -  Pifi{x) -  P2/2OE) -  n h { x )
Ji{X) =  ---------------i--------------------------------------  •
1 -  Pi ~ P2 ~ PZ
(f) Certify yourself of the correctness of the algorithm by writing a com­
puter program, drawing a sample of size 1000 and plotting the resulting 
histogram along with the standard normal density.
(g) Discuss advantages and disadvantages of the above algorithm with re­
spect to the method of Box and Muller.
1.9 The polar rejection algorithm generalizes the Box-Muller method to 
generate standard normal variates. The algorithm iterates according to the 
following steps: (i) generate v\ and v2 independently from a U{—1,1] and 
set r 2 =  v2 +  v2; (ii) if r2 >  1, then go back to step (i), otherwise set 
x\ =  V\ [ - 2(logr2) /r 2]1,/2 and x 2 =  v2 [ - 2(logr2) /r 2]
.
(a) Show that x\ and x 2 are independent and identically distributed N (0 ,1) 
variates.
(b) Certify yourself of the correctness of the algorithm by writing a com­
puter program, drawing a sample of size 1000 and plotting the resulting 
histogram along with the standard normal density.
1.10 (Wakefield, Gelfand and Smith, 1991) Consider the generation of a 
pair (x\ ,x2) uniformly distributed in the region C ft9 =  {{x \ ,x 2) : 0 <  x\ < 
<7- 1 [c/* ( ;E2/<7/(a;i))]} with a strictly increasing differentiable function g and 
a constant c > 0.
(a) Show that y =  x 2/g'(xi) has a distribution with density f(y ) =
f*(y)/  f  f*(u)du.
(b) Consider the power family g(x) =  x r+ l/(r +  1) for r > 0 and c =  
l /( r  +  1). Show that if f* (x ) and xr+1 [/*(a;)]'' are bounded then the 
constants a =  supx [/*(x )]1/,*r+1^, bi =  infx<o x [f* (x )]r^ r+1^ and b2 =  
suPx>o 2j[/*(:c)]r/(r+1) are well defined and Cf,g € [0, a] x [61, 62].
(c) Show for the conditions of (b) that the acceptance probability of a point 
uniformly generated in [0,a] x [61, 62] is given by
f  f*(x)d x 
(■r +  l)a (62 -  bi) '

(d) Obtain for the normal case (f*(x) =  e~x2!2) that a =  1, b2 =  —61 =  
y/(r +  I)/re and that the acceptance probability in (c) is maximized for 
r =  0.5 and its value is 0.731.
(e) Consider the generation of a vector ( x i,. . . ,  Xd, Xd+i)' uniformly dis­
tributed in the region C/^g 
=  { ( x i,. . . , x<j,x d+iY 
■ 0 > Xd+i > 
[f*(yi, ■ ■ • ,J/d)]1/(rd+1)}, with yi =  xi/xd+ 1, i =  l , . . . , d .  Show that 
y =  (y i,. . . ,  yd)' has a multivariate distribution with joint density f ( y)  =  
f*(y)/ j r ( u ) d u .
(f) Show that if f*(x) andxrid+1[f*(x)]r are bounded with x =  ( x i,.. . ,  Xd)' 
then the constants
a 
=  
sup[/*(x)]1/(r<i+1) ,
X
bu 
=  
inf x i[/*(x )]r/(rd+1) and
Xi<0
b2t 
=  
s u p x .t /* ( x ) r /^ +1)
Xi>0
are well defined and Cf,g € FIiLi[^i*> ^2i] x[0, a].
1.11 Describe 3 different methods for generation of random quantities with 
Cauchy distribution of density
f ( x ) =  ~~ 
> 
x e R  
7T 1 —f— X
with the first method using probability integral transform, the second one 
using the ratio-of-uniforms from region {(x i,x 2) : x \ > 0 and x\ + x\  < 1 } 
and a third one using scale mixture of normals.
1.12 Show that if a pair (xi,x2) has distribution:
(a) NG(fi, £ ,n , S) then x\ ~  tn(fj,, SH).
(b) N W (/x, a 2, n, S) then xi ~  tn(fi, a2S).
1.13 x has double exponential distribution with parameters ji and a, de­
noted by DE(fi,a), if its density is
Exercises 
37
\ x -  n\
f(x\n,a) =  
exp j - '" _ 
} , 
f o r x e R .
(a) Using the notation of Exercise 1.5, show that the DE distribution 
admits a location-scale model with location parameter n and scale pa­
rameter a.
(b) Show that x can be obtained as a discrete mixture of y and —y with 
equal weights where y ~  Exp( 1) and describe a generation scheme for x 
based on (a) and the above result.
(c) Show that if x\y ~  N(0,y) and y ~  Exp( 1/2) then x ~  D E (0,1) and 
describe a generation scheme for x based on (a) and the above result.

38
Stochastic simulation
(d) Compare the generation schemes described in (b) and (c).
1.14 (Odell and Feiveson, 1966) Let Y =  LL' where L is a d x d  lower tri­
angular matrix. Show that if the main diagonal elements la of L are square 
roots of G((n -  i +  l ) / 2 , 1/2) (or x « -i+ 1) quantities, the off-diagonal ele­
ments lij ofL  are N (0, 1 ) quantities and all these variates are independently 
drawn then Y ~  Wd(n/2,Id/2).
1.15 (Cheng and Feast, 1979) Consider the generation of a x ~  G(a, 1) 
quantity, a >  1, by the ratio-of-uniforms method. Show that:
(a) the region C f is given by {(x i,x 2) : 0 < x f < (x 2/xi)a~ 1e ~X2/ Xl};
(b) the region Cf C [0,a] x [0,6] where a =  [(a — l ) /e ]^“- 1 ^ 2 and b =  
[(a +  l)/e ](“ +1)/2;
(c) after drawing X\ ~  t/[0, a] and x 2 ~  U[0,b\ independently, the method 
reduces to accepting y =  x 2/x\ if and only if 2 log X\ — (a — 1) log y + y  <
1 and rejecting it otherwise;
(d) tighter bounds to Cf can be found when a > 2.5;
(e) using (d) a more efficient algorithm is given by:
i. draw u\ ~  U[0,1] and u2 ~  C/[0, 1] independently;
ii. if a > 2.5, reset u\ =  u2 +  (1 — \.&&ui)/^/a until 0 <  iti < 1;
in. accept y =  cu2/u\ if [2 log u\ — (a — 1 ) log y/{a — 1 ) +  y] < 1  and 
reject it otherwise with c =  [a — ( l / 6a)].
1.16 Describe in detail the generation of a value from a G(a,/3) using the 
rejection method with q =  G(n,c), n integer. Answer in particular:
(a) What values of n and c should be chosen?
(b) Once values for n and c are chosen, what is the value of the constant 
A that optimizes the method?
1.17 Show that to generate x ~  G(a, 1) with a <  1 using the algorithm 
for a >  1 one can take u ~  C/[0,1] and y ~  G (a +  1,1) independently 
and make x =  yux!a . Write a computer program to draw samples from 
a G(a, 1) distribution using this and Ahrens and Dieter algorithms for a 
variety of values of a. Compare the two algorithms.
1.18 (Johnk, 1964) Consider the generation of a x ~  Beta(a, 0) quantity 
with density f(x\a,(3) — katl3Xa_1(l - x ) ^ -1 / ^  € [0,1]), a, f} > 0. When 
a, (3 <  1 , x can be generated by the following algorithm: (i) generate ui and 
u2 independently from a C/[0, 1 ] distribution; (ii) set Vi =  u\^a, v2 =  u\^ 
and w =  Vi +  v2, (Hi) if w >  1 , then go back to step (i), otherwise set 
x =  vi/w.
(a) Certify yourself of the correctness of the algorithm by writing a com­
puter program, drawing a sample of size 10,000 and plotting the resulting 
histogram along with the Beta(0.5,0.5) as shown in Figure 1.1(h).

Exercises
39
(b) If x  ~  G(a, 1), y ~  G{(3,1) and x and y are independent variables, 
show that x j ( x  +  y) ~  Beta{a, (3).
1.19 Suppose one wishes to sample from a N (3,1) distribution using the 
SIR algorithm. There is previous information allowing the specification of 
scale parameter to 1 but little knowledge about the location. Therefore the 
location of importance density is taken as 0 and the heavy-tailed Cauchy 
distribution is chosen to accommodate this uncertainty. In order to form 
a representative sample of size m =  1000 from the distribution of interest, 
one would expect to have around 
(preferably different) values between
3 and 5. How large should an initial sample size n be to allow one to expect 
this many values on the interval [3, 5] ?
1.20  x has piecewise exponential distribution with parameters A i , . . . , A n 
and partition t i , . . . , t n if its density is
f ( x )  =  k
i
, x e 
, i =  1 , . . .  ,n
where to =  0, k\ =  1 and
ki =  exp
i-1
Xj(tj 
tj-\
j =
1
i =  2,. . .  7i .
(a) Show that f(x\x > ti-i) =  A* e Xi^x ti- 1\ x  e 
and, therefore,
min(x -  ti-i,ti -  U-i)\x > U -i ~  Exp(Xi), i =  1 , . . . ,  n.
(b) Use the result in (a) to sample values from this distribution.
1.21 Show with and without evaluating derivatives that the univariate and 
multivariate normal, the Gamma (a > 1) and the Wishart (a > ( p+ l ) / 2 )  
densities are log-concave.


C H A P T E R  2
Bayesian inference
2.1 Introduction
The concepts involved in the Bayesian approach to inference are described 
in this chapter. The treatment and presentation are not exhaustive. The 
main interest is to show how the ingredients required for the construction of 
a model are formulated in a given problem and what techniques are used for 
understanding and extracting relevant information from the process. For a 
more thorough discussion of Bayesian inference, the reader is referred to 
the books by Berger (1985), Migon and Gamerman (1999), O ’Hagan and 
Forster (2004), Robert (2001) and Bernardo and Smith (1994). A more 
philosophical approach is given by De Finetti (1974, 1975).
The techniques will be derived in more detail for classes of models to be 
considered in subsequent chapters. In particular, emphasis will be placed 
on linear regression models and their many generalizations, which include 
generalized linear models, hierarchical models, dynamic linear models, dy­
namic generalized linear models and spatial regression models.
In the next section the standard Bayesian point of view is presented. It 
consists of the combination of historic and data information through Bayes’ 
theorem and the resulting consequences. In Section 2.3, a special family of 
distributions that plays an important role in the specification of models 
will be introduced: the conjugate family of distributions. The use of these 
distributions is illustrated in the context of normal regression models. In the 
next sections, the hierarchical models, the dynamic models and the spatial 
models are described. Inference for the case of normal observations and 
the generalized case of non-normal observations are presented in all cases. 
Section 2.7 focuses on model comparison and/or selection and introduces 
prior and posterior model probabilities as well as Bayes factors.
2.2 Bayes’ theorem
As in the classical or frequentist approach to inference, the Bayesian ap­
proach is developed in the presence of observations x whose values are 
initially uncertain and described through a probability distribution with 
density or probability function* f(x\9). The quantity 9 serves as an index
* A s w ill b e seen in th e  sequ el, th e m a th em a tica l trea tm en t ca n  b e  u n ified w ith ou t differ­
en tia tion  betw een  d iscrete a n d  con tin u ou s qu an tities. T h e  te rm in o lo g y  o f con tin u ou s

42
Bayesian inference
of the family of possible distributions for the observations. It represents 
characteristics of interest one would wish to know in order to obtain a 
complete description of the process.
The canonical situation is one where a random sample x =  ( x i ,. . . ,  x n)' 
is extracted from a population that is distributed according to the density 
f(x\9). Typically in this case, the observations x t are identically distributed 
and independent (conditionally on the knowledge of 9).
E xam ple 2.1 Consider a series of measurements about a physical quantity 
p with measurement errors e* described by the N (0 ,a 2) distribution, where 
the precision of the measurements (controlled by a2) is unknown. In this 
case, Xi =  /i +  ei, i =  1 ,..., n and
f(x\0) =  f [ f N(xi;p,cT2) =  
exp 
j
where 9 =  (p, a 2). It is important to note that the quantity 8 is more than 
a simple index. It is the very reason for drawing measurements as the main 
interest of the study is to determine its value.
E xam ple 2.2 Consider a series of measurements drawn from a specific 
model, characterized here by its distribution function F (x). It can be shown 
that under certain conditions, if X  ~  F  and u is a high threshold then 
the conditional distribution of the excess X  — u given that X  > u can 
be approximated by a generalized Pareto distribution (GPD) (Embrechts, 
Kluppelberg and Mikosch, 1997, p. 162-164). A random variable Y  follows 
a GPD if its distribution function is
<?(»!«.«') =  I
; —( 1 + ? » / ; ) ; 1/s 
(2.i)
' 
\ l - e x p {-y/cr) 
,tf € =  0 
v 
'
for y >  0, when £ > 0, and for 0 <  y <  —cr/£, when £ <  0, where a >  0 and 
£ are the scale and shape parameters, respectively. Therefore, for a sample 
of excesses y =  (y i, . . . ,  yn) from G(-|£, o ) the likelihood function is
£ « . '>  =  n
( i  [ i + « » / * ’ ] ; “ +£>/£) ,
1 = 1  
'
for £ ^  0 and [x]+ =  m ax{0,x}. Similar calculations lead to L(0,er).
2.2.1 Prior, posterior and predictive distributions
The situation of Example 2.1 repeats in more general cases, that is, the 
index 9 is a quantity of interest with a very precise meaning to the prob­
lem under study. Furthermore, it is likely that the researcher has some
varia b les is ch osen  for s im p licity  a n d  /  w ill b e  referred t o  as a  density. W h en ev er 
req u ired , a  d ifferen tia tion  is m a d e in th e text.

Bayes’ theorem
43
knowledge about its value. It is then possible and even scientifically recom­
mended that this body of knowledge should be formally incorporated in the 
analysis. At this point, the Bayesian and frequentist approaches diverge. 
The second one does not admit this information because it has not been 
observed and is therefore not subject to empiric verification. The Bayesian 
approach incorporates this information to the analysis through a density 
p(9) even when this information is not precise.
There used to be much controversy about the appropriateness of incor­
porating this historic information in the analysis; see Efron (1986), Lindley 
(1978) and Smith (1984) for a sample of the discussion about the theme. 
Nowadays, the dominant impression is that this discussion is no longer 
relevant, with the difference between the two approaches well understood 
and elements of each one used wherever it is more suitable to do so. The 
ultimate test is applicability of the theory and the Bayesian approach used 
to fare badly on this account due to the inclusion of more elements into the 
analysis. The techniques described in this text allow the use of Bayesian in­
ference in quite complex problems, virtually removing its main restriction. 
Therefore, from now on this text will assume firm adherence to the Bayesian 
paradigm and this chapter is devoted to presenting its basic elements.
As described above, Bayesian inference contains two ingredients: the ob­
servational (or sampling) distribution f{x\9) and the distribution p(9). This 
latter distribution can also be specified with the help of constants just like 
the distribution of x. Sometimes it is useful to distinguish them from the 
parameter of interest 0. These constants are then called hyperparameters, 
as they are the parameters of the distribution of the parameters. Initially, 
the hyperparameters are assumed to be known.
Looking at the first ingredient of Bayesian inference as a function of 6 
gives the likelihood function of 9, 1(9) =  f(x\9). It provides the chances 
of each value of 9 having led to that observed value of x. The second 
ingredient is called prior density as it contains the probability distribution 
of 9 before the observation of the value of x. Once the problem is cast in this 
form it is only natural that inference should be based on the probability 
distribution of 9 after observing the value of x , that becomes part of the set 
of available information. This distribution is called posterior distribution 
in direct opposition to prior distribution. It can be obtained by Bayes’ 
theorem
As the left hand side is a density for 9, the observation x is simply a constant 
as well as f(x ). Furthermore, the posterior will be denoted from now on by
where
(2 .2)

44
Bayesian inference
7r(6), with the value of the observation upon which it was conditioned im­
plicitly understood. Bayes’ theorem can then be written in a more compact 
form
n(9) (X 1(9) p(9). 
(2.3)
The concepts of prior and posterior are always relative to the observation 
considered at a given moment. It is possible that after observing x and 
obtaining the posterior, a new observation y also related to 9 through an 
eventually different likelihood function becomes available. In this case, the 
posterior (relative to x) is the prior (relative to y) and a new posterior 
can be obtained by a new application of Bayes’ theorem. It can be easily 
shown that the posterior resulting from observations of x and y is the same 
irrespective of the order in which x and y were processed (Exercise 2.1).
E xam ple 2.1 (continued) Suppose that a 2 is known, so 9 — /i. The model 
can be completed with a prior distribution p(p) =  /at(m> Ho, Tq ), where the 
hyperparameters fio and Tq are known constants. The likelihood function is
exp
where x is the arithmetic average of the X*. Therefore, the posterior density
is
7r(/z) 
oc 
exp
a  
exp
f  
1 
~  r i 2 )  exn f  
1 (m -  mo)2 1
I 
2 a 2 fn  
J 
\ 
2 
r02 J
(M-Mi)2 , (a:-Mo)
+
_1cr2 +  77 }
where t1 2 =  n a ~2 +  To2
Hi =  Ti(na 2x +  t 
/io)- The last passage
is obtained using that if x, ai, a2, b\ and b2 are scalars then
(z -  a i)2 
(x -  a2)2 
(z
where d- l
bi 
+
1 + b2 1
b2
d
c)2 
(ai -  Q2)2 
h  +  b2
and c =  d(bi a\ +  b2 a2) with z =  9, ai =  x,
0-2 =  Mo> bi =  a2/n and b2 =  Tq . Incorporating the multiplicative term that 
does not depend on /i to the proportionality constant gives
Mi)2
In other words, the posterior distribution of h is ir(/x) =  /jv(m; Mi> ri )• 
Note that by increasing the value of Tq , the information contained in 
the prior is reduced and so is its influence on the analysis. In the limit 
when Tq —► oo the non-informative prior p(n) oc k is obtained and n(fi) =  
f N(n\x,a2/n).

Bayes’ theorem
45
E xam ple 2.2 (continued) In extreme value analysis, data are usually sparse. 
Therefore, information from experts can (and should) be useful to supple­
ment the information from the data. It is reasonable to hope that experts 
should provide relevant prior information about extremal behavior, since 
they have specific knowledge of the characteristics of the data under study. 
Instead of trying to elicit prior beliefs directly in terms of GPD parameters, 
Coles and Tawn (1996) suggest inverting Equation (2.1) in order to obtain 
the (1 — p)-quantile of the distribution,
(2.4)
where q can be viewed as the return level associated with a return period 
of 1 Ip time units. The prior information is elicited in terms of qi and 
q2, in the case of a G P D (£,a ), for specific values of pi >  p2. Coles and 
Tawn suggest working with the differences di =  g, — Qi-i, i = 1,2 with 
qo (typically zero) being the physical lower bound of the variable under 
consideration. They also suggest setting independent gamma priors for di, 
i.e., di ~  Ga(ai, bi) for i =  1,2.
The prior information is elicited by asking the experts estimates of the 
50% and 90% (or some other) quantiles for specific values of p. Usually, 
10 and 100 time periods are considered, which correspond, respectively, to 
Pi =  0.1 and p2 — 0.01. The prior distribution for a  and £ can be obtained 
by variable transformation of the prior distribution for d\ and d2 and is 
given by
tt( ct, 4 ) 
oc
u +  t ( P i  S -  ! )
e x p
^(p2s -P i* )
02—1
e x p
[(P iP 2) 
? ( l o g p 2 -  lo g  p i )
p2 5 l ° g p 2 + P i 4 l o g p i ]
(2.5)
All the results above are conditional on the choice of threshold u. Recent 
work on model uncertainty about u includes Behrens, Lopes and Gamerman 
(2004), Mendes and Lopes (2004) and Tancredi, 0 ’Hagan and Anderson 
(2002).
There is plenty of controversy among Bayesians about the specification 
of non-informative prior distributions. Part of the disagreement is due to 
an inherent anomaly of these distributions. Often, this specification leads 
to improper distributions. These are distributions that do not integrate to 
1 as prescribed by the theory of probability. Note from Example 2.1 that 
when t 2 —> 00, f  p(9)d9 ^  1. Besides, there are many different definitions of 
non-informative prior distributions, especially in multivariate cases. One of 
the most commonly accepted definitions is Jeffreys’ prior, given by p(9) oc

46
Bayesian inference
\I(0)\1/2 where
d2 log f(x\0)
1(0) =  E
d6d6'
\0
(2.6)
is the expected Fisher information matrix about 0. Jeffreys (1961) formal­
ized a theory of Bayesian inference mostly using this prior and justified it 
on the grounds of invariance under parametric transformations. In general, 
it leads to prior densities in the forms p(0) oc k for location parameters
0 and p(a) oc o ~ x for scale parameters a. When a location 0 and scale a 
are present, Jeffreys (1961) suggested a change from the above rule to a 
product rule that leads to p(0,a) oc cr-1 .
Another pre-eminent specification of vague prior information is provided 
by the reference approach introduced by Bernardo (1979) and later refined 
by Berger and Bernardo (1992) and Berger, Bernardo and Mendoza (1989). 
It is based on expected discrepancy measures of information and under 
asymptotic normality coincides with Jeffreys’ prior in the univariate case. 
In the multidimensional case, the reference approach works on splitting the 
parameter vector into groups and seems to avoid some difficulties of other 
approaches in the multiparameter case, albeit at the cost of a more complex 
derivation of the prior.
The impropriety of some vague prior specifications is a nuisance but in 
general they lead to proper posterior distributions and inference can be 
made without any difficulty. There are exceptions and in some cases the 
posterior remains improper. This is a serious problem as in many complex 
models verification of propriety is far from trivial. For these models, exact 
inference cannot be performed and the approximations used may lead to 
a number of inconsistencies. The recommendation is therefore to avoid 
improper specifications if possible or use them with caution otherwise.
Another important element for Bayesian inference is the predictive or 
marginal distribution of x with density f (x ) given by (2.2). It provides the 
expected distribution for the observation x as f(x ) =  E[f(x\9)\ and the 
expectation is taken with respect to the prior distribution of 0. A similar 
derivation can be applied to the prediction of a future observation y after 
observing x. This prediction should be based on the distribution of y\x, 
that is, on the updated probabilistic description based on the available 
information. If y and x are conditionally independent given 0 then
f(y\x) =  J  f(y,0\x)d6 =  j f ( y \ e )  ir(0) d0
and again the density is obtained as the expectation of the sampling distri­
bution but this time with respect to the posterior of 0. Conditional inde­
pendence between x and y is obtained, for example, if x  =  ( n , . . . ,  x n)' and 
y =  (xn+i! • ■ •,x n+rn)' are samples from f(x\0). The predictive distribution 
is then used to predict future values of this population.

Bayes’ theorem
47
Predictive distributions form the basis of the predictive approach to in­
ference. This approach is described, detailed and applied to a variety of 
problems by Aitchinson and Dunsmore (1975) and Geisser (1993). The 
main thrust of their argument is that the ultimate test of any inferen­
tial procedure is the confrontation against reality. When making inference 
about parameters this will not be possible as they will not be observed. 
Nevertheless, it is still useful to be able to make probabilistic statements 
about meaningful parameters as that will have a direct effect on some 
course of action.
In the case of a multivariate parameter 6 =  (6\,... ,9d)', marginal and 
conditional distributions of the components 0* can be obtained from the 
joint density ir(9i, . . . ,  6d). The marginal posterior density of 9i is
where 6 -i =  (#i,. . . ,  # i-i, 0*+i, ...,0 a ) is the vector 9 with its ith compo­
nent removed, i =  1 , . . . ,  d. For each di the possible conditional distributions 
are
for all C  C { 1 , . . . ,  i — 1, i +  1 , . . . ,  d}. The most important for this text 
is the conditional distribution of 9i\0-i, called full conditional distribution 
of 9i and with density denoted by 7r»(<?»). The above presentation can be 
analogously extended for the case where each of the components di is itself 
a vector.
2.2.2 Summarizing the information
Once the posterior distribution is available, one may seek to summarize its 
information through a few elements. In particular, location and dispersion 
measures may be calculated to provide an idea of possible central values 
and variability associated with the posterior. The main location measures 
are mean, mode and median and the main dispersion measures are vari­
ance, standard deviation, precision, interquartile range and curvature at 
the mode. The posterior mean is the expected value of 9 under 7r, the pos­
terior mode is the most likely value under 7r and the median divides the 
parametric space in two equal probability parts. In the multivariate case, 
the variance is given by a matrix and the standard deviation can be seen 
as the vector of square roots of the diagonal elements of this matrix. The 
precision is given by the inverse of the variance matrix and the curvature 
at the mode given by the matrix of second derivatives of — log ir at the 
mode and provides a local idea about the posterior dispersion. Apart from 
the median, all these measures can be evaluated for joint, marginal and 
conditional distributions. The median only makes sense for univariate dis­
(2.7)
K{8i\6h j  G C) = n (9 i,9 j,j G C)/Tr(9j,j G C)

48
Bayesian inference
tributions. A description of these measures and their relation to decision 
rules is given by Migon and Gamerman (1999).
In multivariate spaces, marginal densities are useful for concentrating in­
ference on a component of the parameter space. These are also obtained by 
integration (Equation (2.7)). Marginal distributions are particularly useful 
when summarizing the information about parameters through credibility 
(or probability) intervals. C is a 100(1 —a)%  credibility interval for a scalar 
parameter 9 if f c  tt(0)(16 =  1 — a. Again, an integration is required. For a 
given value of a , the interval C  of shortest length is given by the interval 
that includes points of higher posterior density than points not included 
in the interval. These intervals are called highest posterior density (HPD) 
intervals. The idea of probability intervals can be extended to parameters 
of higher dimension leading in general to credibility regions constructed in 
exactly the same way.
It is clear that most summarization procedures involve integration of 
7r. Exact inference will only be possible if these integrations can be per­
formed analytically. Otherwise, approximations will have to be used. For 
the remainder of this chapter, a few basic situations where integration is 
possible are presented. These are followed by models of practical interest 
where exact inference is no longer possible. They will be used as motivat­
ing examples for the approximate solution to inference via Markov chain 
simulations introduced later in the book.
E xam ple 2.1 (continued) As it =  A ^ /zi.rf), it is easy to see that the 
posterior mean, mode and median of 9 are given by /ii. As for posterior 
dispersion measures, variance and cunature at mode are given by r f , stan­
dard deviation by t\ and dispersion by t^ 2. In this case, in addition to 
the easy calculation there is an agreement between the different informa­
tion indicators and summarization is straightforward. Figure 2.1 shows this 
density with specific values for no, Tq, x and a 2 jn.
The results of the example are useful but unfortunately are not the rule 
in Bayesian inference. In general, the expression of the posterior even in 
simple models is complex enough not to allow the analytic evaluation of 
these quantities or to provide conflicting indicators.
E xam ple 2.3 Consider again Example 2.1 with a 2 known and 9 =  /x. 
Suppose that the prior distribution is altered now from the normal to the 
Cauchy distribution with the same location no and scale Tq parameters. The 
posterior density becomes
and no additional simplification is possible. Unlike the previous example,

Conjugate distributions
49
no functional form can be recognized and none of the summarizing quan­
tities can be obtained analytically. Besides, the form of the posterior can 
be irregular enough to require extra care when using simple numerical sum­
maries to describe it. Figure 2.1 illustrates a possible form for tt where the 
difficulty in summarizing the information it contains is evident.
The heavy tails of the Cauchy prior are responsible for solving the dis­
agreement between prior and likelihood in favor of the latter in the above 
example. In general, distributions with thick tails are used to represent such 
situations where information is not very reliable. Heavy tails distributions 
were obtained in Chapter 1 as scale mixtures of normal, a device that will 
be used in the sequel.
Figure 2.1 Posterior densities obtained for Examples 2.1 (solid line) and 2.3 
(dashed line) with fj.o =  0, Tq =  1, a2/n  =  4.5 and x =  7. The change in the form 
of the prior causes major disagreement between the resulting posterior densities. 
While for Example 2.1 it is a well behaved JV(1.27, 0.82) density, for Example 2.3 
it is an asymmetric density with a hump to the left of the mode. Approximate 
location values are: mean =  5.05, mode =  5.36, median =  5.10 and approximate 
dispersion values are: variance =  6.10 and inverse curvature at mode =  6.27.
2.3 Conjugate distributions
Example 2.1 illustrates a situation where for a given observational model, 
prior and posterior distributions belong to the same class of distributions, 
the normal family in that case. This brings advantages to the resulting 
inferential procedures due to simplification of the analysis that becomes 
restricted to a subset of all possible distributions. Passage from prior to

50
Bayesian inference
posterior only involves a change in the hyperparameters with no additional 
calculation. The distribution of 9 can then be routinely updated and the 
arrival of new observations does not cause any complication.
E xam ple 2.1 (continued) After observing a sample x\ =  ( i n , ... 
from the N(9, a 2) distribution, the posterior for 9 is N (h i, t2) where r f 2 =  
mcr~2 + t~2, Hi =  T2(n\o~2Xi + r ~ 2/i) and x\ is the arithmetic mean for 
sample x\. If a new sample x 2 =  (x2i , • • •, x 2n2 Y from the same popula­
tion is observed, the posterior for 9 given x i becomes the prior and the 
new posterior of interest for 9 is conditional on x% and x 2. As the prior is 
normal, derivations above guarantee that the new posterior will also be nor­
mal. They also show that the relation between the hyperparameters (h i, t2) 
of the new prior and (h2, t$) of the new posterior is
- 2  
- 2  
i 
- 2  
t2 
=  
n 2a  
+  t x
M2 
=  
T22(n2CT-2 X2 + T f 2M l)
where x 2 is the arithmetic mean for sample x 2.
Furthermore, the prior is in general a well known, easy to understand 
distribution. If the analysis retains the posterior in the same class, the good 
properties of the distribution will be preserved and the analysis based on 
the posterior will be simple to summarize.
This preservation of the distribution after updating in the same class de­
fines conjugacy. A family of distributions P  is conjugate to an observational 
model 
F  if for every prior p € P  and for any observational distribution
/  € F , the posterior tt € P. Obviously this property is almost always valid
for very encompassing classes P, as the class of all continuous distributions, 
and is seldom valid for very restricted classes, as classes containing a single 
distribution. The definition will only be useful for classes of reasonable size 
and is typically used to recognize densities having the same algebraic form 
apart from a few constants, the hyperparameters. Common sense must be 
exercised for recognition of useful conjugate classes. A good example is the 
conjugacy of the normal for normal observations with known variance.
Despite or even because of the advantages of conjugacy, analyses with 
the conjugate prior must be used with care. The easy calculations of this 
specification comes with a price due to the restrictions they impose on the 
form of the prior. In many cases, it is unlikely that the conjugate prior is 
an adequate representation of one’s prior state of uncertainty. The analytic 
tractability associated with conjugate distributions is lost and posterior 
summarization becomes harder. This dichotomy between tractability and 
realism is always present in the applied statistician’s work. There is no 
universal answer for this dilemma and this text aims to provide a solid 
basis with which model building can be approached with the use of a few 
examples.

Conjugate distributions
51
2.3.1 Conjugate distributions for the exponential family
A density or probability function f(x\9) belongs to the one-parameter ex­
ponential family of distributions if
f(x\8) =  a(x) exp{<f>(8)t(x) +  b(8)}. 
(2.8)
The importance of the exponential family is linked to the fact that many 
of the most used distributions belong to the class. This includes:
a) Normal distribution N (8 ,a 2) (with a2 known)
i l  
2er2
v 
82 
f 
X 2 1
<j)(9) =  - ^ ,t ( x )  = x ,  b(9) =  -  ^ 2  , a ( x ) = e x p  
.
b) Binomial distribution bin(n, 9)
4>(8) =  log 
=  x ’ b
=
 n log ^  
^  
=
c) Exponential distribution Exp(9)
<f>(9) =  —9 , t(x) =  x , b(9) =  log 6 , a(x) =  1.
d) Poisson distribution Poi(9)
(p(8) =  log 8, t(x) =  x , b(8) =  — log 8 , a(x) — 1.
Important families of distributions that do not belong to the exponential 
family are the uniform, Student’s t and discrete mixtures of densities.
The definition (2.8) of the exponential family can be modified in many 
essentially similar forms. Note that in particular there is no unique form 
for (f> and t as multiplication of the first by a constant c can be com­
pensated for by division of the second one by c maintaining the product 
4>(9)t(x) unchanged. For the exponential distribution, alternative defini­
tions are (f>(9) =  9 and t(x) =  —x.
A simplifying alteration can be obtained by noting that the distribution 
can be parametrized by 4>(8) instead of 8 and that it is irrelevant whether 
t(x) or x was observed. In many cases, in fact, t(x) =  x. Assuming the 
existence of the inverse transformations x(t) and 8((f)), the density of t is
f*(t\<f>) =  a*(t.) exp{(f>t +  b*(4>)}
where a*(t) =  a(x(t)) 
and b*((f>) =  b(8((f))). Both definitions of the
exponential family may be used indistinctly once these transformations are 
recognized. Whenever possible, the second form is preferred in this text for 
the clear notational simplification. Therefore, a density of the exponential 
family will from now on be written in the form
f(x\8) =  a(x) exp{8x +  b(8)} 
(2.9)

52
Bayesian inference
and 9 will be called the canonical parameter. This family has been well 
studied and
E(x\9) =  n =  ~b'(9) and Var(x\9) =  -b "{9 ) 
(2.10)
where b' (b") denotes the first (second) derivative of the function b, assumed 
to be a twice differentiable function. The function b" is sometimes referred 
to as the variance function (McCullagh and Nelder, 1988). Whenever clear 
from the context, (2.9) will be denoted by x  ~  E F(ji).
Define now a prior for 9 in the form
p(9) — k(a, (3) exp {a9 +  (3b(9)}
where the normalizing constant k typically depends of the hyperparameters 
a and (3. Assuming the presence of an observation from the exponential 
family (2.9) and operating Bayes’ theorem gives
7r(9) 
oc 
f(x\9)p(0)
oc 
exp{&E +  b(9)} exp {ad +  (3b(9)}
=  
exp{(a +  x)9 +  (/? +  1)6(0)}
=  
exp{ai 9 +  (3ib(9)} 
(2-11)
where a± =  a +  x and f3\ =  (3 + 1. Therefore, tt and p have the same form for 
model (2.9) and belong to the same class of distributions. Whenever clear 
from the context, the conjugate prior will be denoted by 9 
C P (a , f3). 
Equation (2.11) shows that 9\x ~  C P (a j, (3\).
The derivation above shows that a search of prior distributions with the 
same algebraic form of the likelihood leads to conjugacy. In specific cases 
it may be simpler to adopt the strategy of recognition of algebraic forms 
than specializing the calculation of distributions for canonical parameters 
that may be harder to handle algebraically.
E xam ple 2.4 Consider the observational model Poi(X) for x and a prior 
G(a,f3) for A. Following Bayes’ theorem,
tt(A) 
oc 
e~ x\x \a~ le - f)x 
oc 
A“ +a:-1e-(/3+1)A
that is the density of a G (a +  x,/3 +  1) distribution. Hence, the Gamma 
family is conjugated to the Poisson observational model. Consideration of 
the canonical parameter log 9 is possible but leads to conjugacy of the log 
Gamma distribution that is more awkward to handle.
There are many other known results about conjugacy inside the exponen­
tial family described in Migon and Gamerman (1999). Also, the sequential 
nature of Bayes’ theorem allows that analysis for a sample can be obtained 
by successive replications where the posterior at the previous step is the 
prior for the next step. If such an updating operation preserves the fam­
ily of distributions after one step it generally will after n steps. Therefore,

Conjugate distributions
53
conjugacy with respect to a single observation is equivalent to conjugacy 
with respect to a sample of size n. At each step of (2.11), updating due 
to observation Xi through Bayes’ theorem reduces to recursively setting 
hyperparameters 
=  a*_i +  Xi and pi =  /?»_i +  1, i =  l , . . . , n  with 
ao =  a and (3q =  (3. After observing a sample of size n, the posterior and 
the prior belong to the same class of distributions with a n =  a +  EjXj and 
Pn =  (3 +  n and therefore the model is conjugate. Again, this can be written 
as 9\xu . .. ,x n ~  C P (a n, j3n).
The definition of exponential family can be extended to the multipa­
rameter case. A density or probability function f(x\9) belongs to the k- 
parameter exponential family of distributions if
where x =  (x \,. . . ,  x^Y and 9 =  (9i, . . . ,  9k)'.
E xam ple 2.5 A random vector x =  ( x i ,... ,Xk)' has multinomial distri­
bution with index n and probabilities p i,...,p k  satisfying EjXj =  n and 
EjP* =  1 if its joint probability function is
This probability function can be placed in the form (2.12) with vector 9 =  
(01,..., 9k-i)' of canonical parameters with Qi =  log(pi/pk), i =  1, ■ •., k — 
1.
A family of conjugate distributions can be constructed in the same way. 
Define a prior p(9) in the form
where the normalizing constant k typically depends on hyperparameters 
a =  ( a i , . . . ,  ak)' and (3. Assuming the presence of an observation from the 
exponential family (2.12), operating Bayes’ theorem gives
(2 .1 2 )
k
7r(0) 
ex 
f{x\9)p(9)

54
Bayesian inference
where a n  — a t +  x,t, i =  
and (3\ =  (3 +  1. The densities tt and
p have the same form for model (2.12) and belong to the same class of 
distributions and therefore the model is conjugate. Again, in specific cases 
it may be simpler to adopt the strategy of recognition of algebraic forms 
than specializing the calculation of distributions for canonical parameters 
that may be harder to handle algebraically.
E xam ple 2.5 (continued) A random vector 9 =  (9 i,... ,0k)' subject to 
the restriction E
=  1 has Dirichlet distribution with parameter a =  
(c*i,... ,c*fc), denoted by D (a), if its density is given by
Formally, 9 cannot have a density, only its subvectors 0-i, but this mathe­
matical formalism may be dropped in favor of notational simplification.
Combining a prior D (a) fo rp  =  (pi, ■ ■ ■ ,PkY 
the multinomial model 
gives via Bayes’ theorem
t=i
Therefore, tt(p ) =  D(a\) where a i =  a +  x and the family of Dirichlet 
distributions is conjugate to the multinomial model.
It was shown at the beginning of this section that the normal family of 
distributions is conjugate to the normal observational model. It is reason­
able to consider whether conjugacy is retained when passing to the multi­
variate case. This result can be established with the help of the properties 
of the multivariate normal distribution described in Section 1.4.
Assume that 9 ~  N (p, B ) and that x\9 ~  N(9, £ ). By the reconstruction 
of the joint distribution (1.3),
The posterior distribution of 9 is given by the conditional distribution of 
9\x, obtained through (1.2) as
7t{9) =  N (p +  B {E +  B ) - \ x  
B(T, +  B )~ 1B) .
k 
k
n (p i,---,p k ) 
oc 
I J p f  1
2=1 
i= 1 
k
Therefore also in the multivariate case, the normal model is conjugate to 
an observational normal model.

Conjugate distributions
55
2.3.2 Conjugacy and regression models
The results above can be extended to the more general case of normal re­
gression. A response or interest variable is observed and its probabilistic 
description is known to be affected by other variables, called explanatory 
variables or covariates. There are many ways these can affect the response. 
In the simplest case, this influence is linear and additive on the mean re­
sponse. These assumptions arise as first order approximations for more 
complicated functions. The influence is taken over the location because it 
is the most pre-eminent element of a distribution. Other location measures 
could also have been used.
If the response variables have normal distribution then a linear regression 
model is obtained with observations y =  (1/1 , . . . ,  yn)' described by
yi ~  N (x ti(3i +  ... +  x ldpd, a2), 
i =  l , . . . , n
where x n , . . . ,  Xid are the values of d explanatory variables for the zth obser­
vation and P i, . . . ,  fid are the regression coefficients associated with these 
variables. For independent observations, the model is written in matrix 
form as
y ~  N (X P , a2In) 
(2.13)
where the n x  d design matrix X  and the d-dimensional parametric vector
(  X u  
■■■ 
X i d  \
\ Xni 
* ■ ■ 
Xnd J
and P
/ Px
\ Pd
It will be assumed in this text that design matrices are always full rank 
d < n. Cases where this does not apply involve redundancies in specifica­
tion of the model that can in general be removed by simple mathematical 
operations. In some cases, the homoscedasticity (equal variances) hypoth­
esis is replaced by a more general form where Var(y) =  < 1 iag(rr^, . . .  ,a 2). 
Even more generally, the generic variance form Var(y) =  E can be used 
where E is any n x  n symmetric positive-definite matrix.
From the Bayesian point of view, the model (2.13) is completed with a 
prior distribution for parameters (p ,a 2). In this case, there is a conjugate 
family of distributions given by
P\a2 ~  N(b0, a2B0) and a2 ~  IG  ^ y ,  
^ .
The unconventional form for the prior hyperparameters (especially from 
the distribution of a2) will be useful below but note that the above is 
equivalent to noS ojo1 ~  Xnu ■ Before establishing this conjugacy, another 
reparametrization will be adopted: a 2 will be replaced by the parameter

56 
Bayesian inference
(j> =  a~2. In terms of (f>, the prior is NG(bo, Bo, no, So), that is,
(3\4> ~  N (b0,4>~1B 0) and <f> ~  G
(and hence noSo4> ~  Xn0) an{^ ^s density is
P{(3,<t>) oc p(P\4>)p{4>)
oc 
<f>d/2 exp | “
(/? -  boy B o \ p  -  £>o) j </>(no/2)~ 1 exp 
—y n 0*S'o
oc 4>[{no+d)/2]~l exp^-^noSo + iP-boYB^^-bo)]^ . 
(2.14)
The likelihood (2.13) can also be written in terms of P and <j> as y\P, <p ~  
N {X p,4> -lIn) and
W
)  = f(y 1/3,0)
oc 
r
n/2e x p { - ^ ( y - X P Y ( y - X f 3 )
= r n/2 exp | -^ [Q (/3 ) +  5 e]| 
(2.15)
where Q (p) =  {(3 -  P)'X 'X (/3  - & ) , & =  (X 'X )~ lX 'y  and 5e =  (y -  
X $ Y (y -  X(3) =  Yh= x ei with ei =  Vi -  A* and fii =  x a Pi +  ... +  x idpd, 
i =  1 ,... , n. Note from (2.15) that p maximizes l(p,(p) as a function of p. 
Combining (2.14) and (2.15) gives the posterior
7X(P,4>) OC l{P,(j))p(P,4>) 
le xp {  
2
oc <p% e x p | - ^ ( /3 - 6 i ) /B 1- 1 ( /? - 6 i ) |  
e x p j - ^ n i S i j  (2.16)
where n\ =  no +  n, n i S i  =  n0S 0 +  {y -  X b i Y y  +  (bo -  b i)'B Q1b0, &i =  
B i(B Q 1bo +  X 'y ) and B j-1 =  B g 1 + X 'X . Comparing (2.14) and (2.16) it is 
clear that posterior and prior have the same form and belong to a conjugate 
family of distributions. The posterior for [P,4>) is N G ( b \ , B i , n i , S \ ) .  If the 
posterior is required in terms of a 2, the same transformation operated over 
the prior can be applied and
P\cr2 ~  N(b\, cr2 B\) and <y2 ~  IG  
■
Another form to rewrite the normal regression form is
yi ~  N (ni, a 2) with /x, =  xnfii +  ... +  xidPd, 
for i =  1 ,..., n. This form is particularly useful for extensions of regression
oc 
0 0+2 + 
1 exp -i -  ^ [n0S0 +  Se +  (P -  b0)'B 0 1 {P -  b0) +  Q(P)}

Conjugate distributions
57
models to other observational distributions. The linear predictor defined 
by rji =  xnPi +  ... +  XidPd brings into the model the joint effect of the 
explanatory variables. It is typically a real number as no restrictions are 
imposed on the values of the covariates. The normal mean /i* can also be 
any real number and there is no problem in equating the two parameters. 
The same does not necessarily hold for other distributions as will be seen 
below.
An extension of regression models still preserving linearity and influence 
of covariates through the mean response is given by generalized linear mod­
els. The observations remain independent but now have distributions in the 
exponential family. The model is
for i =  1 , . . . ,  n, where the link function g is differentiable.
E xam ple 2.6 Consider y^-Ki ~  bin(rii, 7r*), i =  1 , . . . ,  n, and assume that 
the probabilities ir* are determined by the values of a variable x. The 7r, 
lie between 0 and 1 and can be associated to a distribution function. One 
possibility is the normal distribution and in this case
where $  is the distribution function of the N(0,1) distribution and a and
associated respectively to the logistic and extreme-value distributions. Note 
that g\, g2 and g$ take numbers from [0,1] to the real line.
E xam ple 2.7 Consider yt\\i ~  Poi(Xi), i =  1 , . . . ,  n, containing counts of 
events affected by values of variables x\, . . . ,  Xd- As E(yi\Xi) — Xi > 0, it 
is not recommended to equate Xi to the linear predictor rji that can take any 
value in the line. A simple solution is to take the log(Aj) transformation. 
The effects of the variables x\ , . . . ,  x p become multiplicative on the mean 
as
/(s/i 1*0
E {yt\0i)
9{Vi)
Vi
a ( y i )  e x p { y i d i  +  b (8t)}  , 
- b ' (0i) =  fit ,
Vi ,
XilPl +  • • • +  xid(3d ,
7Tj =  <l?(a +  /3xi), 
i =  l , . . . , n
{3 are constants. The binomial distribution belongs to the exponential fam­
ily and the link function g\ =  $ -1 is differentiable. The structure of a 
generalized linear model is completed with the linear predictor
rji =  a +  0Xi, 
i =  1,... ,n.
Other possible links include the logistic and complementary log-log trans­
formations
and gzi^i) =  lo g [- log(l -  7r*)]

58
Bayesian inference
The theory of generalized linear models as well as many applications, 
special cases and maximum likelihood estimation are very well described 
in McCullagh and Nelder (1988). For Bayesian inference, a prior distribu­
tion for the parametric vector (i must be specified. A natural extension 
of the normal regression is to take a N(bo,Bo) prior. Unfortunately, this 
distribution is not conjugate and exact Bayesian inference is not possible. 
The same comment applies to other prior distributions with rare and not 
useful exceptions. This is an example of an important class of models from 
the practical point of view where inference procedures cannot be exactly 
obtained.
2.3.3 Conditional conjugacy
The difficulty in obtaining conjugate distributions increases with the com­
plexity of the model and the dimension of the parametric space. It was 
mentioned in the previous subsection that the important class of general­
ized linear models does not admit analytically tractable conjugacy. In other 
instances, the model is structured in such a way that some elements of the 
parametric vector have conjugate distributions but the vector as a whole 
does not possess conjugacy.
Consider again the normal regression model (2.13) and assume now that 
(3 and cj> are prior independent with distributions ,6 ~  N(bo,Bo) and (p ~  
G (n0/2 , tiqSq/2 ). The change with respect to the previous specification is 
the removal of the dependence between f3 and (p in the conditional variance 
of 
The posterior distribution now has density
and it is not possible to recognize any analytically tractable form as the 
prior. Therefore, there is no conjugacy. Formally, it would still be possible 
to establish conjugacy because the prior is a special case of the posterior. 
This line of thought has little practical value. The notion of conjugacy is 
only useful when it leads to a form that can be treated analytically, which 
is not the case here.
Despite the lack of conjugacy, it is easy to obtain the conditional posterior
7 r (/3 ,0 ) 
cx 
l ( ( 3 , < p ) p ( ( 3 ) p ( 4 > )
OC
(X
X
X

Conjugate distributions
59
distributions of j3\(j) and (f>\(3. The first one is
7 T (m  
<x 
eXp {- ^ [< l > ( p - p y x 'X ( ( 3 - P )  +  ( (3 -b o y B ^ (f 3 - b o )} }
a  
(2-18) 
where b,p =  B^(BQ1bo +  <pX'y) and B ^ 1 =  B q 1 +  cpX'X. Hence, (3\cf> ~  
a posteriori. It has the same form of the conditional prior f3\cf> 
~  N(bo, Bo), by the independence between f3 and 4>. So, /3 is conditionally 
conjugate.
Similarly, the conditional posterior distribution of (p\/3 is 
ir(cf>\f3) oc 
exp { “
[(/? -  fa 'X 'X t f -  0) +  Se +  n050]|
and therefore, a posteriori 4>\f3 ~  G(ni/2,niSfj/2) where n\ =  n +  no and 
Sp =  (ft — P )'X 'X (P  — (3) +  Se +  uqSo- Again, this conditional posterior 
has the same form of the conditional prior. So, there is conjugacy for both 
conditional distributions.
The above specification is of practical relevance as it is conceivable that 
one would want to set initial uncertainty through independent forms for 0 
and 4>. The conditional priors simplify to the marginal priors. Nevertheless, 
it is possible to have conditional conjugacy even when the priors are not 
independent (Exercise 2.11).
In the case of a parametric vector 6 — ($i,. . . ,  6^)', a (not necessarily 
scalar) component 6j, i =  1 ,...,d , exhibits conditional conjugacy if the 
full conditional prior Pi(6i) =  p(9i\9-i) and the full conditional posterior 
Ki(8i) =  7r(0i|<?-i) belong to the same family of distributions. Depending on 
the form of the prior, it is possible that all components of 6 are condition­
ally conjugate. In other cases, only a few components will be conditionally 
conjugate (see Exercise 2.11c).
Conditional conjugacy is present in many complex models where com­
plete probabilistic specification is only possible when the parametric space 
is qualitatively structured before prior quantification takes place. In highly 
dimensional parametric space it is very hard to directly specify a joint 
prior distribution. It makes sense to explore qualitative relations of (condi­
tional) independence between the components of the parameter vector. In 
many cases, this will lead to substantial simplifications in the form of the 
prior. It will be formed by a collection of probabilistic statements about 
components of smaller dimensions. These are easier to specify and quan­
tify. Important examples of this qualitative approach to model building are 
provided in the next sections. Due to the impossibility of exact analysis, 
inference procedures exploring full conditional distributions and, whenever 
possible, conditional conjugacy becomes of great importance.

60
Bayesian inference
The normal regression model was specified in the previous section with the 
aim of establishing relations between the response variable y and a set of 
explanatory variables x i, . . . ,  £<* through regression coefficients gathered 
in the vector fa Many times, the problem is structured in a way that qual­
itative probabilistic statements about (3 can and should be incorporated 
into the model.
E xam ple 2.8 Consider observations yij ~  N (fa ,a 2), j  =  l ,..., rij, i =  
1 ,...,d , collected from d groups with different means fa but the same 
dispersion. This model is a special case of a regression model with ob­
servation vector y =  {yn , ■ ■ ■ ,yim , ■ ■ ■ ,Vdi, ■ ■ ■ ,ydnd)' and design matrix 
X  =  diag( 1 „ , , . . . ,  l „ d) where l m is the m-dimensional vector of Is. The 
model is completed with a prior distribution for (f3,cr2). One possibility is 
to assume prior independence between the means fa, i =  1 ,..., d. If the d 
groups are similar in some sense, a plausible alternative is to assume that 
the means are a sample from a population of means. This population may 
be hypothetical and, to fix ideas, is assumed here to be homogeneous. As­
suming a normal population, fa , . . . ,  (3d is a sample from a N(/j,,t2) where 
fi is the mean and t2 measures the dispersion of the population of means. 
The model is completed with a prior distribution for (/i,r 2). The complete 
prior specification is
2.4 H ierarchical m odels
for independent probability distributions Fa and FT. The prior density for 
the model parameters ((3,fi,a2, r 2) is
Note that the prior in the example was specified in two stages. The 
(two-stage) model of the example can be generalized in many ways. A gen­
eralization towards a normal regression model was considered by Lindley 
and Smith (1972) and is given by
The design matrix with the covariates for the response vector y and the 
regression coefficient were respectively renamed to X\ and fa. This is due 
to the presence of another design matrix containing a further set of ex­
planatory variables X 2 and another regression coefficient fa. This matrix
1st level 
: 
(3\p,r2 ~  N (ld p,T 2Id),
Ind level 
: 
fi ~  N (bo,Bo),
j 2 ~  Fa 
and r 2 ~  FT,
p{(3, fi, a2, t 2) =  p(P\fi, t 2 ) p (h )p (<j 2)p (t 2).
y\fa,cj> 
~  N(X\(3\, <j>~~^In),
f a \ f a  
~  N { X 2f a , C ),
fa 
~  N (b,B ),
(j> 
~  G (n 0/2 , nO0o/2).
(2.19)

Hierarchical models
61
contains the values that explain the variations of the values of j3i, and /?2 
contains the coefficients of this explanation.
The prior in the above model is specified in two stages. Depending on 
the problem, more stages may be required for an appropriate description 
of the model. Its form may remain unchanged with additional equations in 
the form
In general, the higher the stage, the harder is the specification of the distri­
butions. Rarely, models have more than three stages and it is very common 
that the prior at the higher stage is set to be non-informative.
So far, nothing has been said about the matrices C and B being assumed 
to be known. This assumption is not reasonable in general and a modifi­
cation sometimes suggested is the substitution of C and B by (frlC and 
(f>~1B respectively. The variances C  and B will then measure prior disper­
sion relatively to the likelihood dispersion. This dependence on (j> allows the 
use of the results about conjugacy. The analysis still remains conditional 
on the (assumed known) values of the matrices C and B. This procedure 
was adopted by Migon and Gamerman (1999, Chapter 7) but will not be 
pursued here. Details of the relevant distributions are left as an exercise.
The derivations below concentrate on the two-stage model to simplify 
the notation even though there is no technical problem in the extension to 
the k-stage models, k > 2. The first point to mention is that the structure 
imposed upon the joint distribution of all the variables in the problem, that 
is (y,/?i,/32,<£), is
The hierarchical character of the model then becomes clear with the suc­
cessive conditional specifications. Unfortunately, the analysis is not ana­
lytically tractable and it is not possible to obtain the marginal posterior 
distributions of (3\ and <f>. The analysis conditional on knowledge of /32 is 
not new and was performed in the previous section. If /32 is known, the prior 
does not depend on the probabilistic specification of /32. Replacement of bo 
by X 2P2 in the regression model is all that needs to be done. Hence, the 
full conditional posterior for 
is 
and for (j) is G(ni/2,niSp/2)
where 64, =  i?0(C _ 1X 2/32 +  <j>X[y), B ^ 1 — C - 1  +  cj>X[X 1, iii =  n +  no 
and niSp =  no<7o +  (y ~ X i(3i)'(y — X ifti). If /?2 is unknown, it is also 
not possible to obtain its marginal distribution in closed form but its full 
conditional posterior distribution is
Pj I Pj+1 ~  N (X j+i(3j+i,C j).
p(y, P u lh , 4>) =  p(y\Pi,<l>)p(Pi I £2 )p((h)p(4>)-
7r (/32| / ? i ,0 )  
( X  
p ( /3i|/32) p ( / ?2)
oc

62
Bayesian inference
oc 
exp
x 
e x p | -^ (/3 2 - b ) 'B  1(02 - b ) j  
a  
e x p j - ^ - O ' i T - ' O ^ - n }
where b* =  B *(B ~ 1b +  X 2C ~ 10i) and B* =  (B ~ l +  X'2C ~ xX 2)~ x. There­
fore, all model parameters 0i, 02 and (p are conditionally conjugate.
It is interesting to note that the full conditional posterior distribution 
of 02 does not depend on the observations. This somewhat surprising fact 
is a direct consequence of the hierarchical structure of model that passes 
through 0\ all information provided by y to 02. More formally, y and 02 are 
conditionally independent given 0 \. This model-building strategy based on 
conditional independence allows easy derivation of full conditional distri­
butions. In many cases, they are also conditionally conjugate.
There is nothing special about the normal distribution with respect to 
hierarchical models. They can also be defined for other observational dis­
tributions. Example 2.8 can be extended for observations ytJ with density 
f(y\0i, 4>)- The /34s are a sample from a population with density p{0\X) and 
this constitutes the first stage of the prior distribution. Again, the model is 
completed with a second stage prior for A and <p (Deely and Lindley, 1981).
Kass and Steffey (1989) studied this class of models and called them 
conditionally independent hierarchical models. A case of particular interest 
is when the 0iS are conditionally conjugate given the values of 0 and A. In 
the context of exponential family distributions, 
~  E F (jii), the 0t form 
a sample from a C P (a, 0) distribution and the model is completed with a 
prior for a and 0. Then, the full conditional posterior of the 9i is still given 
by a product of independent C P (a * , 0*) distributions. George, Makov and 
Smith (1993) showed that the full conditional posterior distributions of 
a and 0 are log-concave. Hence, despite their awkward functional form 
(Exercise 2.15) they are available for adaptive rejection sampling.
Albert (1996) discussed the possibility of using other non-conjugate spec­
ifications for the first stage prior. In particular, he considered parametric 
transformations taking the 0i to the real line and suggests a normal first 
stage prior to the transformed parameter as done in Example 2.8.
A more general extension of the hierarchical model is considered in Albert 
(1988). The two-stage regression model (2.19) and generalized linear models 
can be combined to give a generalized linear hierarchical model
Vi I Hi 
V
~  
E F (n i), 
i =  1 , . . . ,  n,
=  
x t f u
~  
N (X 202, C),
~  
N (b,B ),
(2 .20)
0 \ I 02 
02
where rj =  (rji,... ,r)ny  and 7]i =  g(Hi), i =  l , . . . , n .  When the EF distri­
bution considered is normal, model (2.19) is obtained. When the regression

Dynamic models
63
structure simply classifies observations by groups, the normal prior model 
for transformed means suggested by Albert (1996) is obtained. Again, there 
is nothing preventing the inclusion of further stages in the hierarchy al­
though this is seldom required.
2.5 D yn am ic m odels
A large class of models with time-varying parameters, adequate to the 
modelling of time series and regression, was presented by Harrison and 
Stevens (1976) and is well described in the book by West and Harrison 
(1997). This section presents some basic aspects of dynamic models with a 
few illustrative examples.
Dynamic linear models are defined by a pair of equations, called the 
observation equation and the evolution or system equation, respectively 
given by
where {y t} is a sequence of observations through time, conditionally inde­
pendent given j3t and o f, Ft is a vector of explanatory variables as in the 
previous sections, pt is a d-dimensional vector of regression coefficients or 
state parameters at time t and Gt is a d x d matrix describing the paramet­
ric evolution. The errors tt and wt are mutually independent and o f and 
Wt are the error variances respectively associated to the univariate obser­
vation and the d-dimensional vector of parameters. The model is completed 
with a prior (3\ ~  N (a , R).
Dynamic linear models provide another nice example of specification of 
a prior for a highly dimensional parameter by combination of qualitative 
and quantitative information. The system equation provides qualitative in­
formation about the relation between successive values of the state param­
eters. Quantitative information is provided by the prior distributions of (3\ 
and evolution errors wt. The complete expression of the prior distribution 
results from the combination of these sources of information.
Dynamic regression models are defined by Gt =  
If, in addition,
Wt =  0, Vt, the static regression model (2.13) is obtained. This is equivalent 
to setting the regression coefficients fit fixed in time.
E xam ple 2.9 The simplest time series model is the first order model. H 
is given by equations
and fit is scalar. The model can be thought of as a first order Taylor se­
ries approximation of a smooth function representing the time trend of the
Ut 
— 
-FfA +  etj 
~  N(0, o f )
fit 
=  
Gt/3t-i+uJt, 
wt ~ N ( 0 , W t)
(2.2 1)
(2.22)
yt 
=  
Pt +  et, 
et ~ N {0  ,o f)
Pt 
=  
P t-i +  
~  N(0, Wt)

64
Bayesian inference
series. This model is useful for stock control, production planning and fi­
nancial data analysis. Observational and system variances may evolve in 
time, offering great scope for modelling the variability of the system.
The linear growth model is slightly more elaborate by incorporation of an 
extra time-varying parameter (32 representing the growth of the level of the 
series. The model becomes
Vt 
=  
Pi,t +  et 
et ~  N (0 ,a t)
P l , t  
=  
+  #2,4 +  w l,t
@2,t 
=  
# 2, t - l + W 2>t, 
Wt =  (wl,t> ^2,t)' ~  N (0, Wt).
This model can be written in the form (2.21)-(2.22) with Ft =  (1,0) and
a
=
( ;  
i ) , « .
The choice of Ft and G t depends on the desired model and the nature 
of the series one wishes to describe. Complete specification of the model 
requires full description of the variances o f and Wt . In general they are 
assumed to be constant in time with o f typically larger than the entries of 
Wt in applications.
Exact inference is only possible when Wt is replaced by o f Wt. The matrix 
Wt becomes a matrix of weights relative to the observational variance. 
Typically it is unknown and must be estimated. This will make analytical 
treatment impossible. To avoid it, West and Harrison (1997) suggested it 
should be subjectively assessed through the concept of discount factors. 
Here, none of these simplifying assumptions are made in preparation for a 
full analysis described in later chapters.
Nevertheless, consider initially that o f and W t are known and let yl =  
{y t, y 1^ 1} with y° describing the initial information available, including the 
values of Ft and Gt, Vt also assumed known here. Observations are inde­
pendent conditionally on the state parameters. This time indexed structure 
matches well with the Bayesian approach by the easy accommodation of 
sequential procedures and subjective specifications.
2.5.1 Sequential inference
One of the main aspects of a dynamic model is that at any time t, inference 
can be based on the updated distribution of pt \yl. Sequential inference then 
carries this through time. There are three basic operations involved here: 
evolution, prediction and updating. These operations are presented here in 
this order.
Consider that at time t — 1, the updated distribution is /3t_i|yt_1 ~  
N (m t- i , C t- i ) . The system equation can be written as (3t \Pt-i ~  N {G tP t-1, 
W t). These specifications can be combined according to (1.3) leading to the

Dynamic models
65
marginal distribution
A Iy1- 1 ~  N (at,R t) 
(2.23)
with at =  Gtmt- i  and Rt =  GtCt-iG 't +  Wt;
One-step-ahead prediction can be made by noting that
pivu P tly*'1) =  /(yt|A M A |yt_1)-
Again, the joint distribution of yt, /3t|yt_1 can be reconstructed using (1.3) 
and the marginal distribution
Vtly1- 1 ~  N {fu Qt) 
(2.24)
with ft =  F^at and Q t =  F(RtFt +  o f is obtained.
Finally, updating is achieved by the standard Bayes’ theorem operation of 
including the observed yt into the set of available information. The updated 
posterior distribution is obtained by
p(A|y‘ ) =P(/3t|2/t,2/-1 ) o c /(y t|/3t)p(/3t]yt_1) .
The resulting posterior distribution is
P ttf ~  N {m t,C t) 
(2.25)
with mt =  at +  A tet and Ct =  Rt -  AtA'tQ t, where A t =  RtFt/Qt and
et =  yt — 
f t. This result follows from (2.18) with prior (2.23) and likelihood
(2.2 1) and from the identity Ct_1 =  Rl~l +  FlFta1~2. It is sometimes referred 
to as the Kalman filter.
By induction, these distributions are valid for all times as (3\ also has 
prior in the form (2.23) and therefore, its posterior must be given by (2.25) 
with t =  1 .
2.5.2 Smoothing
The joint distribution of yn =  ( y i,... ,y n)' and (3 =  (/3i,... ,/3„ )' has den­
sity
n 
n
p{yn,P) = II/ (* IA ) 
p(/31)-
t= 1 
t= 2
Therefore, the full conditional densityt of /3t is 
7i-t(Pt) 
oc 
f(yt\(3t)p(f3t+i\Pt)p{(3t\(3t-i)
oc 
fN(yt\ FtPt, o f) /jv(/?t+i; Gt+iPt, w t+1) fN(Pt\ G tP t-i, Wt)
=  
f N(f3t; bt, B t) 
(2.26)
whereat =  B t(ot 2Ftyt+G't+1W t^ lf3t+ i+ W t lG tftt-1) and Bt =  (crt 2FtF [+ 
G't+1W ^ xGt+1 +  
for t — 2, . . . ,  n — 1 . The endpoint parameters
t T h e  use o f  th e s y m b o l n for a d en sity  a u to m a tica lly  im p lies it w as o b ta in e d  con d itio n a l 
o n  all ava ilab le d a ta  in form a tion , in this case, yn .

66
Bayesian inference
Pi and (3n also have full conditional distributions N (b\,B i) and N (bn, B n) 
wherebx =  B i ^ F i y i + G ' J V ^  fc + R ~ la), B x =  (a^2FiF{+G '2W ^ 1G2+  
R - 1) - 1, bn =  B n( a - 2Fnyn+ W - lGnpn-.i) and Bn =  ( a ^ F ^ + W - 1) - 1.
The distribution of parameters at time t can be revised after data at times 
subsequent to t becomes available. A set of distributions of p(Pt\yt+k), for 
k integer, can be considered. When k > 0, they are called smoothed or 
filtered distributions of the parameters. When k =  0, it is the updated 
distribution and when k <  0, they are prior distributions. In dynamic 
models, the smoothed distribution n(@\yn) is more commonly used. It has 
density
n—1
AP\yn) 
=  
p(Pn\yn) Y [ p m P t +u ...,(3 n ,y n) 
t= 1 
n— 1
=  
p{Pn\yn) n * A I / W
)  
(2-27)
t=i
where the last equality follows from the fact that given (3t+1, Pt is inde­
pendent of all quantities indexed by times larger than t. Integrating (2.27) 
with respect to (/3i,. . . ,  P t-i) gives
n— 1
■K(pt, . . . , p n\yn) = p {P n\yn) JJpiPklPk+uy1)
k=t
for t =  1 , . . . ,  n — 1 and
n{Pt,Pt+i\yn) = p (P t+i\yn)p(Pt\Pt+ i ,y t) 
(2 .28)
for t =  1 , . . .  ,n  — 1 .
Equation (2.28) provides a simple and recursive form to obtain the 
marginal posterior distributions of Pt\yn■ After sequentially obtaining the 
updated distributions of Ptly1 for t =  1 , . . . ,  n, time orientation is reversed 
from the distribution of Pn\yn so as to successively obtain the distributions 
of Pt\yn for t =  n — 1 , . . . ,  1 . It can be shown (Exercise 2.20) that
/3t\yn ~ N (m ? ,C ? ) 
(2.29)
where
m t =  
m t +  C(Gt+1i?t+1(mt+1 — at+i)
C ? 
=  
Ct -  CtG't+1Rt+1(Rt+i -  Ct+i)R t+iG t+iC t.
Other important aspects of dynamic models are the treatment of missing 
observations and interventions. In the first case, updating equations are 
simply not used or the corresponding term f(yt\Pt) not included in the 
calculations. Interventions are accommodated by use of other distributions 
for the disturbances wt with increased uncertainty for times of structural 
changes. Recent observations will be more heavily weighted when used in

Dynamic models
67
the updating step. If the disturbances remain normally distributed, minor 
changes are required. Scale mixtures of normal are again an alternative 
worth considering but exact inferences will no longer be possible.
2.5.3 Extensions
Returning to the case of o f and Wt all unknown, all the above results 
remain valid conditionally on the o f s and Wts. Equation (2.26) provides 
the full conditional posterior of fit and Equation (2.27) provides the full 
conditional posterior of (3.
Assume now that o f =  o 2 =  4>~x and Wt =  W  =  $ _1, 'it. It is not 
possible to obtain analytic expressions for the posterior marginal densities 
of (3, <j> and $. Nevertheless, it is easy to obtain the full conditional densities 
of (j) (or c 2) and $  (or W )
n
7t(</>|/3,$) 
OC 
Y [ f ( , y t \ (3t,<l>) p(ct>\(3,<&), 
t=  1 
n
ir($\(3,4>) 
oc 
Y[p((3t\(3t- i , $ )  p($\/3,4>). 
t=2
If, a priori, 
~  G (na/2,naSa/2) and <1> ~  W (n w / 2,n w S w /2 ) are inde­
pendent then
n
n(4>\f3,$) 
oc 
Y i f N i y u F ^ t , ^ 1) f G{cj>-,na/2,naSa/2) 
t=  1
ex 
f G{4>-,n*j2,n*aS*j2) 
(2.30)
n
7r($|(3,(j>) 
oc 
/n (Pu Gt/3t~i, <I>~1) 
nw/2, nwSw/2)
t=i
n w/2, nw^w/^) 
(2.31)
where n* =  na +  n, n* S* =  naSa +  T,t {yt -  F{/3t)2, n ^  =  nw +  n -  1 and
nw s w =  nw Sw +  'E?=2((3t-G t(3t-i)(f3t-G tl3t-iy. Therefore, a posteriori,
(o2\(3,W) ~  IG(n*/2, n*5*/2) and {W\(3,a2) ~  IW(n*w /2,n*w S*w /2). 4> 
and $  (or equivalently o 2 and W ) exhibit conditional conjugacy.
An extension of dynamic models still preserving linearity and influence 
through mean response is given by generalized linear models where the 
observations now have distribution in the exponential family. The model is
f{.Vt\0t) 
=  
a(yt) exp {yt8t +  b(6t)} with E (yt\6t) =  /xt,
g(fH) 
=  
FlPt, 
(2.32)
(3t 
=  
GtPt+i +  wt with wt ~  N (0, Wt),
where the link function g is again differentiable. The model is completed 
with a prior f3\ ~  N (a ,R ). It combines the prior specification of normal

68
Bayesian inference
dynamic models with the observational structure of generalized linear mod­
els.
Examples include the dynamic logistic regression with a series of bino­
mial observations yt with respective success probabilities 
dynamically 
related to explanatory variables x =  ( zi ,
through the logistic link 
logit(7r() =  x't(3t, and series of Poisson counts with means At dynamically 
related through multiplicative perturbations At =  Xt-iw^. After a logarith­
mic transformation, one obtains log At =  log At_ i +  wt with wt =  log 
which is in the form (2.22).
Dynamic generalized models were introduced by West, Harrison and 
Migon (1985). As with generalized linear models, it is no longer possible 
to perform exact Bayesian inference and they proposed the use of approx­
imations coupled with a conjugate analysis. Again, an important class of 
models of practical relevance is obtained for which inference cannot be 
performed exactly.
2.6 Spatial models
Analysis of spatially distributed data is an area that has been heavily used 
in practical applications, specially over the last decades. The models start 
by considering a region with sites or pixels S i,... ,Sd- For each site, a vari­
able of interest yi is observed with mean h(8t), for some function h. This 
variable can for instance be the number of cases of a disease in a county, 
a radiocarbon count in an archaeological site or an intensity measurement 
such as colour or frequency in an image of a medical exam. The complete set 
of responses forms the observed scene or image. These applications account 
for both discrete, continuous, univariate and multivariate responses.
Depending on the problem, different models can be contemplated. Typi­
cally, it is assumed that there is an underlying pattern formed by the values 
of 0 =  (0i,. . . ,  8d)' associated with the observed scene. This pattern is cor­
rupted by some random observation mechanism. The main effort of the 
inference is to remove this observational noise and recover the underlying, 
unobserved scene.
Most models commonly assume observational independence conditional 
on the unobserved image 9, ie. 1(9) =  f { y i , . . . ,  yd\0i, ■ ■ ■, 9d) =  n* f(Vi\^i)- 
The spatial structure is specified through the prior p(9). Other effects may 
also intervene in the problem. These include the effect of explanatory vari­
ables to account for fixed variation and unstructured pixelwise random 
effects to account for unspecified data heterogeneity.
Concentrating on the spatial structure, Besag, York and Mollie (1991)

Spatial models
69
suggest the specification of a prior in a pairwise difference (PD) form
where the proportionality constant is not uniquely defined due to its degen­
eracy. They present many possibilities and interpretations for the weights 
Wij and for the function h. In particular, they consider h(x) =  x 2/2W 
leading to a singular normal distribution (see Exercise 2.22), denoted by 
9 ~  PD (w , W ), where the proportionality constant now includes the term 
W ~ d/2. The full conditional distribution of 0*|0_ *, W  is given by a N (a*, i?*) 
distribution where
on their spatial neighbors just like in a temporal autoregressive structure. 
Thus, PD forms are usually referred to in the literature as conditional 
autoregressive (CAR, in short) models. The notations CAR and PD are 
used indistinctively. A typical choice of weights is w-ij =  I(sj £ Ni) where 
Ni defines a neighborhood of s*, i =  1 ,..., d. Other choices of 
possibly 
depending on unknown hyperparameters may also be used. Gamerman and 
Moreira (2004) considered i0y oc d“ T where dij is some distance between 
pixels i and j  and r  is an unknown quantity measuring the strength of the 
spatial dependence.
The distribution above can also be identified with Gaussian Markov ran­
dom fields (GMRF). A space-varying process 0(-) taking values in a region 
S follows a Gaussian random field (GRF) if 9 =  (0(si), • • •, 9(sd)) possesses 
a ci-variate normal distribution for any d and any given set of d locations
S i,...,S d  belonging to S. The identity 0(sj) =  8i, for all i, is frequently 
used, especially when the locations st refer to areas contained in S rather 
than point locations belonging to S. If the dependence between them is 
provided by the non-zero weights, a Markovian structure in space is de­
fined (more details about Markov random fields are provided in Chapter 
4). This prior is based on the notion of neighborhood defining the space. 
Thus, it is particularly suited for areal data where observations are mea­
sured or aggregated over an area and pixels are associated with regions in 
the space of interest.
A common choice for observation model is given by the normal distri­
bution. It provides suitable representation (possibly after some transfor­
mation) of crop output for given plots of land, economic indices such as 
production levels in given regions of a country or climatic indicators of
where rij =  S 'jL i wij > f°r * =  1, ■ ■ ■ ,d. This result shows that the 9iS depend

70
Bayesian inference
administrative areas. This leads to the model 
y\9,a2 
~  N (9 ,a 2Id)
0\W 
~  C A R (w ,W ) 
(2.33)
W  
~  Fyy.
The model can be readily extended in a number of directions. Besag 
and Higdon (1999) considered the inclusion of a regression term X(3 in the 
mean structure of the observations. Congdon (1997) introduced the idea 
of allowing spatial variation also to regression coefficients (3. Assungao, 
Gamerman and Assungao (1999) used the multivariate form
p(J3) oc exp j -  X > y ( &  -  p jY W -'t fi -  ft ) j 
(2-34)
for jointly modelling the prior for space-varying regression coefficients (3 =  
(f3i, . . . ,  (3d). Another extension is provided by allowing the observations to 
belong to the larger class of exponential family of distributions. These mod­
els are considered in detail by the books of Banerjee, Carlin and Gelfand 
(2004) and Rue and Held (2005).
E xam ple 2.10 When observing cases of a disease in a given region, an 
appropriate observational model is yi ~  P oi(e6i), i =  1,... ,d. If no covari­
ates are present and weights are given by the neighbor indicators, this leads 
to the posterior distribution for 9 and W
p(W ) (2.35)
n(d,W) oc Y\eyLY>{9iyi- edi} W  5 e
x
p
j
-  Ojf 1 
*= 1 
^ 
i < j  
)
where p (W ) is the prior distribution o fW . The full conditional distribution 
o f 6i is
7ri{9i) oc exp ^9iVi -  e9i -  ^ ( 0 *  -  ^i)2}  
(2-36)
where 
is the number of elements in Ni and 9i =  (l/rij) 
=1} @j-
Here also one may consider the inclusion of a regression term X $  in the
mean structure of the observations.
Another modelling option uses distance-based Gaussian processes or GRF. 
Usual simplifying assumptions of common mean, homoscedasticity (com­
mon variance) and isotropy (correlation depends only on distance) are fre­
quently assumed. Under these assumptions,
9 ~  N (n ld,T2R\) 
where 
R\ =  (p^) 
(2.37)
with pij =  p*(|sj — Sj|), for some suitably defined correlation function 
p\ possibly depending on a parameter A, typically a scalar or low dimen­
sional quantity. In this case, this distribution can be denoted by <?(•) ~

Spatial models
71
GRF(/j,,t2, p\). There are many options for the correlation function. The 
most used ones are exponential p\(u) =  e~“ / A, power exponential, spheri­
cal and Matern (for details, see Cressie, 1993; Stein, 1999).
This prior is based on the notion of distance defining the space. Thus, it is 
better suited for spatially continuous data where observations are measured 
on a specific point location and pixels are associated with points in the 
space of interest. Nevertheless, there is nothing compulsory about these 
guidelines. GMRF may also be used with spatially continuous data (after 
appropriate definition of neighborhood of points) just like distance-based 
GRF may be used with spatially discrete data (after appropriate definition 
of distance between areas).
The study of observation processes defined by GRF forms the basis of 
an area commonly referred to as Geostatistics (Cressie, 1993). The name 
Geostatistics stems from its heavy use by geologists since the 1950s in 
the study geo-chemical properties of the soil. The Bayesian version of the 
canonical model of the area is provided by
y\6,a2 
~  N (e ,a 2Id)
% , t 2,A 
~  G R F (ij,,t2, p\) 
(2.38)
$  
~  F*,
where >3/ =  (fi, a2,r 2, A) is the set of hyperparameters. The model must be 
completed with a prior distribution for 'I'. Usual choices are independent 
priors /z ~  N (a ,R ), 4> ~  G (na/2,naSa/2) and t 2 ~  IG (n T/2,nTST/2). 
The prior for A depends on the choice of correlation function (A can be 
a vector). Typically they depend on a scale parameter that may be inter­
preted as range of the correlation and a shape parameter. Appealing to the 
range interpretation may lead to a uniform prior distribution over the ad­
missible range of distances. Gamma forms are also used. Similar comments 
apply to shape parameters and uniform priors may be used in cases where 
mathematical constraints must be imposed. Reference priors for A in some 
correlation functions were derived by Berger, de Oliveira and Sanso (2001).
Nowadays, the area is very much integrated into Statistics with shared 
knowledge between the communities. GRF provide a flexible tool to allow 
for prediction to unobserved locations or spatial interpolation based on a 
set of observed location. Given the joint normality, the normal conditional 
distribution of 
is easily obtained. This procedure is commonly
referred to by geologists as kriging. Spatial interpolation based on the ob­
served data can be obtained via
f{yiu)\y(obs)) = J  piy^Ko^iO^^ly^ye^de^dy
=  
j  f ( y (u)\6{u),9 (obs\ y , y (obs))p(6 {u)\0{obs\ y , y {obs)) 
xp(6»(obs),'I'|2/(o,,s)) ^ (u)d6>(obs)d4'.

72
Bayesian inference
The above operation can be, and sometimes is, interpreted as Bayesian 
kriging. It provides kriging but in Bayesian fashion, with the incorporation 
of the uncertainty about hyperparameters in the spatial interpolation pro­
cess. The densities in the last integral can be considerably simplified due 
to conditional independence. It is easy to recognize that
/ ( y (u)|6»(u), 0(obs),t f,y (obs)) 
=  
f { y (u)\e(u\cr2) and
=  
p (e{u)\ e{obs\ ^ ).
The model can also be readily extended in a number of directions. Uni­
versal kriging considers the inclusion of a regression term X/3 in the mean 
structure of the observations. Another extension is provided by allowing 
the observations to belong to the larger class of exponential family of dis­
tributions (Diggle, Tawn and Moyeed, 1998). Gelfand et al. (2003) intro­
duced the idea of allowing spatial variation also to regression coefficients 
j3 from a Bayesian perspective. These models are combined with dynamic 
models to account for spatio-temporal data variation in Gelfand, Banerjee 
and Gamerman (2005). These and other hierarchical structures for spatial 
models are covered by Banerjee, Carlin and Gelfand (2004).
2.7 M odel comparison
In the last three sections several fairly complex statistical models were 
introduced for different data structures: hierarchical models for longitudi­
nal/panel data (Section 2.4), dynamic models for time series data (Section 
2.5) and spatial models for scene or image data (Section 2.6). One could 
argue that the data structure itself will narrow down the scope of possi­
ble or feasible models to be entertained. However, the number of possible 
models can be and usually will be relatively large (or even uncountable) 
even after this initial trimming.
In this section the most common approaches to model comparison and/or 
selection within the Bayesian world are briefly introduced. It should be 
emphasized that all criteria point out to the best model within the set of 
entertained ones. They do not by any means identify the correct model, 
should such model exist. Bayesian model comparison is commonly per­
formed by computing posterior model probabilities. More precisely, sup­
pose that the competing models can be enumerated and are represented by 
the set M  =  {M i, M 2, ■ ■.}. Under model M 3 with corresponding parameter 
vector 8j, the posterior distribution of 0j is obtained in the usual manner
where f(y\ 0j,M j) and p(9j\Mj) respectively represent the likelihood and 
the prior distribution of 9j under model M j. Predictive densities f(y\M j) = 
f  f(y\6ji 
\Mj)d9j (j  =  1,2,...) play an important role in Bayesian

Model comparison
73
model selection and comparison. The posterior odds of model Mj relative 
to model Mk is defined as Pr{Mj\y) / Pr(Mk\y), which is the product of 
the prior odds Pr(Mj) / Pr(Mk) of model Mj relative to model Mk by the 
Bayes factor,
f(y\Mj)
3h 
f(y\M k) ' 
( 
}
The Bayes factor can be viewed as the weighted likelihood ratio of Mj to 
Mfc. Hence, the posterior model probability for model j  is
=  j 
J =  1,2........ 
(2.40)
which depends on all prior odds ratios and all Bayes factors involving model 
j. When the prior model probabilities are uniformly distributed, the poste­
rior model probabilities equal the Bayes factor. Jeffreys (1961) recommends 
the use of the following rule of thumb to decide between models j  and k: 
when Bjk is above 100, between 10 and 100 and between 3 and 10, there 
is decisive, strong or substantial evidence against k, respectively.
Bayes factors suffer from at least two major problems. The first prob­
lem is conceptual in that Bayes factors, unlike posterior distributions, are 
extremely sensitive to the specification of the prior distribution. This sen­
sitivity appears for both proper and improper distributions but is more 
dramatically observed in the latter case. More specifically, if p(6j\Mj) and 
p(9k\Mk) are defined only up to arbitrary constants Cj and Cfc, respectively, 
then Bjk is defined only up to an arbitrary constant cj/ck (see Kass and 
Raftery, 1995; Berger and Pericchi, 1996; and their references, for further 
discussion. See also Exercise 2.26 for an illustration). The second prob­
lem is more computational since Bayes factors are functions of predictive 
densities, f(y\Mj), whose integral is usually hard to obtain.
When one of the competing models is nested into the other model and 
the prior distributions for the parameters within models satisfy Dickey’s 
separability conditions (Dickey, 1971,1976), the Bayes factor can be written 
as a posterior/prior ratio under the encompassing model. Verdinelli and 
Wasserman (1995) named Savage-Dickey density ratio this particular form 
of Bayes factor (see Exercises 2.27 and 2.28). Fortunately, for more general 
and usually non-nested models, several computational solutions do exist 
and some will be presented in Section 7.2 (model adequacy) and Section
7.3 (model choice).
E xam ple 2 .1 1  Suppose that, given n observations, y =  (yi, • ■ ■ ,y n)> the 
three alternative normal models of Table 2.1 are considered. Also, a and £2 
and the hyperparameters fio, Co, ^o, 
rfy, tpo, 70 and Sq are known. It can 
be shown (see Exercise 2.25a) that h 1 =  (co + n )~ l (coHo +  ny), c\ =  co+ n , 
V! =  v0 +  n, vxa\ =  vqoI +  ^ ^ (/x o  -  y)2, Vi =  Vo +  n, 
=  r)0ip% +
E I L i (y i  ~
a ) 2 > 6i 2 =  n t ~2 +  5o 2 > 7i =  <*i( n y £~2 +  <^'027o )-

74
Bayesian inference
Mj  
8.j 
p(yi\Qi,Mj) 
p(Gj\Mj) 
Tr(6j \Mj )
Mi 
(»,4>) 
N(fi, <£-1 ) 
N G ( »  o.c-'^ o.cr^  
N G (Mi , c ~l , v u  a 2)
M 2 
r 2 
N (a , r 2)
M3 
7 
W(7,C2) 
JV(7o,«2) 
AT(7i,5?)
Table 2.1 Normal models.
Similarly, prior and posterior predictive densities are as follows (see Ex­
ercise 2.25b):
M j 
yi\Mj 
yn+i\y,M j
M i 
tV0(n o,cj^(1 + C Q 1) )  
+ C ] ; 1) )
M 2 
tVa(a,ii)l) 
tVl(a,i> j)
M 3 
N  (70, £2 +  Sq) 
N f r u e  +  Sf)
Bayes factors are
TT™ 
_____________ r (ilg2~) /1 1 
1 (3/i-Mo)2 \
lli=i r(iai)v/^07rCT2(1+c- i ) V 
• 'o ^ i+ c-1))
12 
“  
,, 
_m±±
n 
r  (2U+1) 
/  
1 ( iii-c ) ^  
2
lli=i r(a .)v^ ^ 2  V ^  VO 
Vo 
)
rr» 
________________________A 
1 (yi- ^ 0)2 \
llj= l F( 
) V^oTrerg (1 +  c~1) V 
v° c,qO-+cq1) )
nr=l 7 ^ « p { -i» 3«}
and B 23 =  B is/B i2-
l/Q + 1
B 13 
=
2.8 E xercises
2.1 Derive and draw the graph of L(0, a) of Example 2.2. Show that the 
qualitative form of the graph remains the same for any given sample and 
is only affected by it in the scale.
2.2 Show that the posterior distribution of 6, p(8\x,y), does not depend on 
the order in which x and y were processed, that is, it is the same whether 
one obtains the posterior of 6\x and uses this posterior as the prior for 
observation y, one obtains the posterior of 8\y and uses this posterior as 
the prior for observation x or one obtains the posterior of0\x,y directly.

Exercises 
75
2.3 Derive expression (2.5).
2.4 Show that if z, ai, a2, bi and b2 are scalars then 
(z -  a i)2 +  (z -  a2)2 _  (z -  c)2 +  («i -  a2)2
b i 
b2 
d 
b i +  62
where d~l =  
+  b2 l and c =  d(b±lai +  b2 1a2). Extend the result for
the multivariate case with d-dimensional vectors z, ai and a2 and d x d 
symmetric positive definite matrices B i and B 2 showing that
(z -  a\)'B^1(z -  ai) +  (z -  a2) 'S ^ 1(^ -  a2) =
(z -  c)'D ~ l (z -  c) +  (ai -  a2)'(B\ +  B 2)~ 1(ai -  a2)
where D ~ l =  B ± l -)- B ^ 1 and c =  D {B ^ la\ +  B 2 la2).
2.5 Consider a sample x \, . . . ,  x n from a distribution with density f ( x |/z, a) 
that admits a location-scale model (Exercise 1.5) with location parameter h 
and scale parameter a. Show that the non-informative prior is:
(a) p(fi) oc k if a is known;
(b) p(a) oc I /a  if 8 is known;
(c) p{n,(j) oc 1/cr2 but reduces to p(ji,a) oc 1/a if independent priors are
chosen. Hint: show that 1(8) =  E
2.6 Consider x  ~  bin(n,8). Obtain Jeffreys’ prior for 6 and show that 
despite being non-informative it is a proper distribution. Obtain the nor­
malizing constant and draw the graph of its density.
2.7 Show that the multinomial distribution with index n and probabilities 
P i,... ,Pk satisfying SjXj =  n and T,iPi =  1 belongs to the k — I-parameter 
exponential family with vector 8 =  (81, ... ,6 k -i)' of canonical parameters 
with components 
=  log(pi/pk), i =  1, ■. ■ ,k — 1. Obtain the functions 
a(x) and b(8).
2.8 Consider the situation of Example 2.1 but assume that the prior is a 
discrete mixture of two normal densities.
(a) Show that this prior is still conjugate, that is, the posterior is a discrete 
mixture of two normal densities.
(b) Generalize the result in (a) for discrete mixtures of k normal densities, 
A; =  2, 3, . . . .
(c) Generalize the result in (b) for discrete mixtures ofk  CP(a.i, Pi) distri­
butions, i =  1 ,..., k, and observations following a EF(/i) distribution.
(d) Calculate the posterior for a prior given by the mixture 0.8Ar(0 ,1) +
0.2iV(5,4) and the data used in Figure 2.1. Plot the posterior density 
and compare it with the densities in that figure.

2.9 Show that once the probability is fixed, HPD credibility intervals are 
always the shortest ones. Obtain (numerically if necessary) the HPD in­
tervals for the posterior densities of 8 considered in Figure 2.1 and for the 
posterior resulting from Exercise 2.8d.
2.10 Prove Equation (2.16), that is, show that
4>\^+n+d)/2]-i exp j _ ^ [ nos 0 +  Se +  ((3 - boYB o'iP -  bo) +  Q(/J)] j
= <pd/2 exp 
bi)'Bil{(3 -  6j) j 4)(ni/2>-1 exp|-|ni5i|
where n x =  n0 +  n, nxSi =  n0So +  (y -  X b i)'y +  (b0 -  bi)'BQlbo, bi =  
B iiB ^ b o  +  X 'y ) and B ^ 1 =  B q 1 +  X 'X .
2.11 Consider the regression model y\(3,<f> ~  N (X B , <t>~xIn) and assume 
that the prior for ((3, <j>) has density proportional to
exp|-i[(/3 - boYB^iP -  b0) + <j>((3 -  h)'B ^{(3 -  h) + n05o^]| .
(a) Obtain the conditional prior distributions of (3\(f> and
(b) Show that the above distributions are conditionally conjugate for (3 and 
for (p.
(c) Assume now that the term noSocp in the expression of the prior density 
is replaced by noSo/(f>. Show that (3 is still conditionally conjugate but 
not (j>.
2.12 Show that in the two-stage hierarchical model, the full conditional
distribution of (32 is iV(/xi, Cg) where 
=  C2{C2 lfi +  
C2 =
( C i 1 +  X!>X2) - \
2.13 Consider the K-stage hierarchical model
y I (3i,4> ~ N (x 1p1, r 1in)
(3k \f3k+i,<t> 
~  N {X k+l(3k + u <j>-lC k l), k =  l , - - - , K - l
(3k \<P 
~  N { H , r lC-Kl)
4> 
~  G{no/2,noal/2) 
.
(a) Obtain the conditional prior distribution of (3k \ <f>, k =  1 ,..., K  — 1.
(b) Obtain the marginal prior distribution of (3k > k =  1 , . . . ,  K  — 1.
(c) Obtain the marginal distribution o fy .
(d) Obtain the conditional posterior distribution of(3k \ 4>>k =  \ ,.. .,K  —
1.
(e) Obtain the marginal posterior distribution of (3k, k =  1 ,..., K  — 1.
76 
Bayesian inference

Exercises
77
2.14 Show that for the one-way analysis of variance model with hierarchical 
prior described in Example 2.8, Cov(f3j, (3y \ (f>) =  Cov(f3j, (3j‘ ) =  Bo and 
Cor(Pj,f3ji | cf>) =  Cor(j3j,Pr ) =  (1 + t 2/B0)~ 1.
2.15 (George, Makov and Smith, 1993) Consider observations yij 
E F (jh), 
j  =  1 ,... ,ni, i =  1 ,... ,d, collected from d groups with different canonical 
parameters 
The hierarchical prior for 9 =  (9\,. . . ,  0d)' assumes they 
form a sample from a C P (a , f3) distribution and the second stage specifies 
a constant (possibly improper) prior for the hyperparameters a and f i.
(a) Obtain the full conditional posterior of 9 and show it splits into a 
product of densities, each depending only on observations from one group 
and the hyperparameters. Apply the results to Poisson and Binomial 
observations.
(b) Obtain the full conditional posterior of the hyperparameters and show 
that it is log-concave and proper. Apply the results to Poisson and Bi­
nomial observations.
2.16 Consider the model described in Example 2.9 with a 2 =  a2 and Wt =
wyt.
(a) Show that the predictive distribution for an horizon k > 0 is
yt+k\y* ~  N (m t,C t +  kW  +  a2)
(b) Obtain the joint predictive distribution of (yt+i, • • •, yt+kly1)-
(c) Obtain the predictive distribution of the sum yt+1 +  • • - +  yt+k of future 
observations. In particular, show for k =  2 that
yt+1 +  yt+2\yt ~  N{2rnt,4C t +  2 < j 2 +  5W ).
2.17 In the conditions of the previous exercise, show that the one-step- 
ahead filtered distribution for P t-i (t =  2 ,... ,n ) is
A - i l y 4 ~  N i m l ^ C l ^ )
with
m\_x 
=  
mt_i +  (Ct-i/ R t)(m t -  TOt- i )  and 
C*_x 
-  
Ct- i  -  (Ct-i/ R t)2(Rt — Ct).
2.18 In the conditions of Exercise 2.17, assume that observation yt is miss­
ing. Obtain the distributions of fitly* andyt+liy1 as functions of m t-\ and
Ct- i .
2.19 Show that for dynamic models the full conditional distribution of (3t 
is N(bt, Bt) for t =  1, . . .  ,n where

and
f ( a ; 2FtF{ +  G’t+1W t~1Gt+1 +  R - ' ) - 1 , t  =  1
{ K - ^ t + ^ + i W
’r'G 't+ i +  W 'r1) - 1 , t =  2 , . . . ,  n — 1 
.
[ (<Jt2FtF( +  W f 1) - 1 
, t =  n
2.20 Show that for normal dynamic models
(a) 7r(/?t|/3t+i,. . . ,  Pn,yn) =ir{Pt\Pt+i,yt) for t =  l , . . . , n - l .
(b) 7r(/?t,. . . ,  /3„|y") = p (/3 n | y")xn felf1p(/3fc|/3fe+i,2/t) for t =  1 , . . . ,  n - 1. 
('c; 7r(/3t,A+i|y” ) =p(A +i|y")p(A |/9t+i,yt) /o r t =  1,... , n -  1.
(d) (3t \(3t+ i,y n ~  N (h ?,H ?) where
ht 
=  
mt “I- CtG t+i-^t+i(/^+i 
Oj_(-i) and
H ? 
=  
C t-Q G j+ ^ -A iit+ iiJ r + iG i+ iC t.
(ej /3t|y" ~  iV (m ",C ” ) where
m” 
=  
m ( +  CtGt+1i ? ^ 1(m"+1 — Ot+i) and
C ? 
=  Ct -  CtG'l+1R-il1(Rt+l -  C?+1)R ;+\G t+1Ct.
2.21 S’/low that for dynamic models the full conditional posterior distribu­
tions of the variances of the disturbances are (a21/3, VK) ~  /G (n * /2 ,n * 5 * /2) 
and (W \P,a2) ~  IW (n ^ / 2 , n ^ S ^ /2 ) with n* =  na +  n, n*S* =  
+
-  F{/3t)2, n*w =  nw +  n -  1 and n ^ S ^  =  n w Sw  +  £?=2(/3t -
G t A - i X A - G t / J t - i ) ' -
2.22 Consider the pairwise difference prior for 8 of Section 2.6 in the form
8 ~  CAR(w, W ).
(a) Show that 8 ~  ^ (O ,/^ -1 ) where K  =  (kij) and
{
m 
, if i =  j  
-w ^  
, if i ~  j  
and n* =  ^
 w ,j.
0 
, otherwise 
j~i
(b) Show that the precision matrix K  is rank deficient. Hint: simply sum 
over the rows.
(c) Show that the full conditional prior distribution of 8i is given by 
8i\8-i, W  
N (ai,R i) where
1 . ^ „ 
1
ai =  —  V"' Wijdj and Ri =  — W,
, 
Tli
3= 1
fo r i =  l ,... ,d .
2.23 Consider again the model (2.33) 
and set 4> =  1/cr2.
(a) Show that the full conditional posterior for di is N(m,i,Ci) wheremi =
Ci(4>yi +  i?J“ 1Oj) and C ~ x =  (fi +  R * 1, with ai and Ri defined in Exercise 
2.22, i =  l ,... ,d .
78 
Bayesian inference

Exercises
79
(b) Show that the full conditional posterior for 9 is N (m ,C ) where m =  
4>Cy and C ~ 1 =  (f>Id +  $ K  and that this is a proper distribution.
(c) Extend the results above to the case of an additional regression term 
X/3 in the mean o fy , ie. y r*j N (9 +  X[3,
(d) Extend the results above to the case of space-varying coefficients (3\,... , 
with prior (2.34) (Gamerman, Moreira and Rue, 2003).
2.24 Consider the spatial model described in Example 2.10. Show that:
(a) the posterior distribution is given by (2.35);
(b) the full conditional distribution of 6i is given by (2.36);
(c) the inverted Gamma distribution for W  is conjugate conditionally on
2.25 Consider again Example 2.11.
(a) Derive the posterior distributions of 9j, ir(8j\Mj) for j  =  1,2,3.
(b) Derive the prior predictive distribution, f(y\M j) and the posterior 
predictive distribution, f ( y n+i\y, M j), for j  =  1,2,3.
2.26 Assume that x \ ,...,x n is a sample of size n and that the only alter­
native model specifications entertained are
M 2 
: 
Xi\(r2 ~  jV(0,<72) 
and a2 ~  JG(^o/2,
where r 2, 1/ and erg are prior hyperparameters.
(a) Show that the Bayes factor is
(c) Assuming that x =  (-0.411,-0.778,0.437,0.723,1.670,0.404,0.444,
0.280,-0.345,1.384), CTq =  0.5 and uq =  5, show that B 12 goes roughly 
from 8 to 1 to 1/8  when r 2 goes from 0.0216 to 1.208 to 2.806.
2.27 Assume that two models M\ and M 2 are entertained and their likeli­
hood functions are
9
M i 
: 
Xi\n ~  N (fi: 1) 
and / i ~ A r(0,r2)
r (f2i)v /^ I^ _
(b) Show that B 12 —> 0 as r 2 —> 00.
M 2 
: 
f(y\92,M 2)
M l 
: 
f(y\0u M l) =  f(y\e21=9°21, 922 =  9U M 2)
where O2 =  (^21^ 22)- to other words, M\ is a restricted version of (nested 
m j J\?f2* Suppose also that the Dickey s condition tt(^022 \92i =  9°21,M 2) =  
7r(#i|Mi) holds.

(a) Show that the Bayes factor is B i2 =  n{82i\y-lM 2)/'K(d2i\M2). This 
Bayes factor is also commonly known as the Savage-Dickey density ratio 
(Dickey, 1971, 1976; Chen, 2005).
(b) Describe conditions under which the Savage-Dickey ratio can be applied 
to compute the Bayes factor for the following two autoregressive models: 
f ( y t\8i,M i) =  f N{yt-,Po +  b m - 1, 02) and p{yt \82,M 2) =  fn iy u  7o +  
JiVt-i +  l 2y t -2,T2).
2.28 (Verdinelli and Wasserman, 1995) Assume that x i , . . . ,  xn are a ran­
dom sample from either one of the following two models
M i 
: 
Xi\6i ~  N(p,,cr2)
M 2 
: 
Xi\02 ~  t\(p/,T2)
where 9i =  (h,ct2), B2 =  (ff2i,0 22), d2i =  to =  1/A and 022 =  (7 , t 2).
(a) Show that M i is a special case of M 2.
(b) Show that the Savage-Dickey formula holds when n(8i\Mi) oc o ~ 2, 
^(02^ 2) a  r -2  and u> ~  U\0,1] and that the Bayes factor is B i2 =  
it(oj =  0|a:, M 2).
(c) Simulate n =  100 observations from N (0 ,l) and compute B v2.
(d) Simulate n — 100 observations from ti (0,1) and compute B\2.
2.29 Consider models M i and M 2 of Exercise 2.27. Assume also that 0 < 
p(82i =  02i,0 22\M2) <  00 and 0 < p(821 =  92i \y,M2) <  00. Show that the 
Bayes factor is
p{02i = 6 o2i\y,M2) 
f 
p(9i\Mi)
12 
p(02i =e°2l\M2) 
\ p { 0 2 i= e o2i\022,M 2)
assuming (finite) expectation with respect to p(922\02i =  82l ,y ,M 2). This 
is the generalized Savage-Dickey density ratio. It generalizes the Savage- 
Dickey density ratio by the introduction of a correction factor (Verdinelli 
and Wasserman, 1995).
80 
Bayesian inference

C H A P T E R  3
Approximate methods of inference
3.1 Introduction
This chapter presents some of the methods proposed for Bayesian inference 
when the necessary calculations cannot be performed analytically. Some of 
these techniques are based on deterministic concepts while others are based 
on non-iterative simulation in opposition to the methods based on iterative 
simulation that form the core of this text. Therefore, only an introduction 
to the subject is presented. A more thorough treatment of the subject with 
comparisons and illustrations of the different techniques is given by Evans 
and Swartz (1995). The books by Carlin and Louis (2000), Gelman et al. 
(2004) and O ’Hagan and Forster (2004) also provide nice reviews of the 
area with the first one also providing a summary of software available.
The main techniques presented in this chapter are normal and Laplace 
approximations based on asymptotics in Section 3.2, quadrature approx­
imations in Section 3.3, Monte Carlo integration in Section 3.4 and re­
sampling techniques in Section 3.5. The last two sections present solutions 
based on stochastic simulation. They generally involve sampling from an 
auxiliary distribution that serves different purposes in the context of each 
approximation.
The deterministic techniques rely upon approximate normality and 
asymptotic results in the sense of the sample size growing to infinity. These 
techniques were mostly developed during the 1980s when the computational 
explosion that enabled computer-intensive methods to be performed was 
only starting. As will be seen, the complexity of the techniques increases 
substantially with the dimension of the parametric space. Similar com­
ments are valid for the simulation techniques presented in this chapter. In 
particular, finding a suitable auxiliary distribution becomes an extremely 
difficult task. As a consequence, their application to a complete Bayesian 
analysis in complex models such as those presented at the last sections 
of the previous chapter is limited. Hierarchical, dynamic and spatial mod­
els have in common highly dimensional parameter spaces that are difficult 
to approach for complete inference with the techniques presented in this 
chapter.
The last sections are more in the spirit of the book with the use of 
stochastic simulation for inference from the posterior distribution. The non­
iterative form of the simulation used restricts its application in complex

82
Approximate methods of inference
models with large numbers of parameters. For these cases, the use of iter­
ative techniques based on Markov chains and described in all subsequent 
chapters of this book.
Before going into the details of the techniques, it is important to recall 
that most summarization operations are provided by integration of the 
form
J m
-n(8)d8. 
(3.1)
The above expression provides the posterior mean of any transformation 
'tp =  t(8). When evaluating the posterior mean of 6, t(8) =  0. When eval­
uating the posterior median c of a scalar 8, t(9) =  1(8 <  c), I — 1/2  and
(3.1) is solved for c. Similarly, credibility regions are obtained by solving
(3.1) for C with t(9) =  1(9 € C) and I  =  1 — a. The posterior variance 
matrix may be obtained by taking t(9) =  98' and previously evaluating the 
posterior mean. Finally, for 8 =  (8i , . . . , 8d)' with components 8t of any 
dimension, the marginal density of 9i is given by (2.7). It can be rewritten 
as
7r(0i) =  J  n W iie -iM e -iW -i 
(3.2)
and again an integration over a posterior density is required with t(8-i) =  
Tr(9i\8-i). As mentioned in Section 2.2, another important integral that 
regularly appears associated with Bayesian model choice and prediction 
procedures is the posterior predictive density
f{y\x) = J  f{y\0)^{0)d9
which can be easily rewritten as the integral in (3.1) with t(8) =  f(y\8).
In very broad terms, experience gathered from previous work suggests 
that deterministic techniques provide good results for low dimensional (say 
single digit) models. Beyond that, they become very complex to handle 
and Monte Carlo techniques have to be used. When the dimension of the 
model becomes increasingly large, then only Markov chain simulation seems 
to provide an adequate solution. Whenever possible, analytical integration 
should be performed. This will reduce the dimension of the model where 
approximate methods are applied. Finally, it is important to mention that 
there is plenty of room for experimentation with combinations of these 
techniques.
3.2 Asymptotic approximations
The approximations here rely on results obtained when the sample size 
n gets large. Consider a parameter 8 =  (8\,.. . ,  8d)' with posterior dis­
tribution 7r. Asymptotic approximations for 7r date back to the work of 
Laplace (1986) in the 18th century. Nowadays, Laplace approximation is

Asymptotic approximations
83
more commonly associated with the approximations presented in the fi­
nal subsection. This nomenclature is kept in this book and the Laplace- 
motivated approximations of the next subsection are only referred to as 
normal approximations.
3.2.1 Normal approximations
These are based on a Taylor series expansion of the logarithm of the pos­
terior density around the (assumedly unique) mode m
log 7t(0) 
=  
log 7T(m) +
1
— (6 — m)
8 log 7r (to) 
89
82 log 7r(m)
(9 -  m)
89 89'
— m) +  R(9)
1
=  
logTr ( m ) - - ( 9 - m )
82 log 7r(m) 
89 89'
(i9 — m) 
(3.3)
where the remainder R(9) contains terms of order 3 or larger in the com­
ponents of (9 — to) and is neglected in the approximation. Commonly, the 
posterior is known up to a proportionality constant, that is, only tt* is 
known where as before n*(9) — l(9)p(9) =  kir(9) where k =  f  tt*(9)d9. The 
expansion of log tt* around m gives analogously
7t*(9) 
=
7r*(m) exp -J — -(9  -  m)
8 2 log 7t*(to)
8989
=  
T r * ( m )  exp -j —^r(9 -  m )V ~ l {9 -  m)
(9 — m)
and therefore
k =  TT*{m)(2Tr)d/2\V\1/2
(3.4)
where
V" =
82 log 7r (m)
- i
82 log 7r*(m)
- l
8989'
8989'
is minus the inverse of the curvature or second derivative matrix of log 7r 
(and log 7r*) at the mode. Therefore, a posteriori, 9~N (m , V) and expecta­
tions evaluated with this approximation are denoted by E,v ■ Approximate 
Bayesian inference proceeds as in the normal case with calculation of point 
estimates, quantiles, credibility intervals and marginal densities.
This approximation is similar to the asymptotic result of the maximum 
likelihood estimator, 9, if the prior is constant. In this case, m — 9 and 
V coincides with the inverse of the observed Fisher information matrix. 
As the sample size n increases, the likelihood becomes dominant and for 
n large, the influence of any non-degenerate prior density becomes negli­
gible. In this case, a further approximation can be made by replacing the

84
Approximate methods of inference
mode m by the maximum likelihood estimate 6 and V by the inverse of 
the observed Fisher information matrix evaluated at 0. Often, the expected 
(rather than the observed) Fisher information matrix is used. The normal 
approximation then becomes 0 N (0,I  J(0)) where 1(0) is given by (2.6). 
This approximation is sometimes called a Bayesian central limit theorem 
(Carlin and Louis, 2000). All these approximations are 0 (n -1 ). The above 
result is formally derived with a full discussion of the conditions for con­
vergence to posterior normality by Heyde and Johnstone (1979).
E xam ple 3.1 Consider the observation of x ~  Poi(X) and a conjugate 
prior A ~  G (a, (3). Example 2-4 shows that the posterior is 7r(A) =  G(ai,(3\) 
where a\ =  a +  x and (3\ =  (3+ 1. Hence, the posterior mean and variance 
are a\/(3\ and o.\j(3\ respectively. The normal approximation to ir has mean 
given by the posterior mode m =  (a\ —I)/f3i and variance V =  (ai — \)/(3\. 
The quality of the approximation can be assessed in Figure 3.1.
Note that the normal approximation does not impose any condition on 
the parameter. It can be applied to any transformation ip =  t(0). This will 
lead to a normal posterior with mean given by the posterior mode of ip 
and variance given by minus the inverse of the Hessian of the log posterior 
of tp evaluated at the mode. If the prior influence is discarded, the normal 
approximation for the posterior of ip becomes ip~N(ip, I ~ l (ip)). Exercise
3.2 shows that this is equivalent to using the delta method for inference.
The normal approximation ignores skewness and secondary modes and it 
will only work well if the posterior is similar in shape to a normal distribu­
tion. When this does not apply, it is still possible to transform the parame­
ter to a more adequate space. There are no optimal rules but good practical 
suggestions are reparametrizations that allow the parameter to vary over 
the real line and those leading to constant (Jeffreys) non-informative priors.
E xam ple 3.1 (continued) A simple transformation taking A to the real 
line is (p =  log A whereas ip =  \/A has constant non-informative prior. The 
respective normal approximations are <p ~  N(m ^, V^) and ip ~  N (m ^, V^) 
with niff, =  log (a i//?i), 
=  aj"1, 
=  y/(ai -  l/2 )//? i and 
=  l/4/?i.
These distributions imply different (non-normal) approximating distribu­
tion for A. They improve over direct application of the normal approxima­
tion for A and provide a reasonable approximation even for a small value 
of n =  1 as can be seen from Figure 3.1.
Approximations to E(ip) can be obtained as Epf(ip) or, when this is 
not possible, via the delta method. Further improvement is obtained by 
including second order terms in the Taylor series expansion of ip =  t(0) 
around m so that
E[t(0)] = Eff[t(0)] 
=  
Epj[t(m) + (d t(m )/d 0 )'(0  — m )

Asymptotic approximations
85
Figure 3.1 Exact and approximated posterior densities for X in Example 3.1 with 
a =  (3 =  x =  3. The exact posterior density /g(A ;6 ,3 ) (solid line) has mean, 
mode and variance equal to 1.5, 1.25 and 0.375, respectively. The normal approx­
imation is /at(A; 1.25, 0.3125) (dashed line). Dotted and long-dashed, lines are the 
implied (non-normal) approximating densities for X based on normal approxi­
mations for <[> =  log A and ip =  y/X, i.e. A- 1 /iv(log A; 0.4055, 0.1667) (mean= 
1.6311, varianee= 0.4819,) and (4A)_1^2/jv(\/A; 1.1726, 0.0625) (mean= 1.4372, 
varianee= 0.3511,), respectively.
+  
( 9 — m )'T(m ){9 — m)/2]
=  
t(m ) +  E N {tr[T (m )(8 — m)(9 -  m)'}/2}
=  
t(m ) +  tr T (m)V/ 2
where T  is the Hessian of t-(9). For scalar ip and 6, the above approxima­
tion becomes E[t(9)\ =  t(m ) +  t"(m )V/2. These approximations are also 
0 {n~l ).
Turning now briefly to multimodality, assume that, in addition to the 
global mode, a few local modes have been previously identified. Denoting 
the r modes by m i , . . . ,  mr, the posterior density can be approximated by

86
Approximate methods of inference
where
Vi
d2 log 7r*(mj)n 1
dOdO'
and/c 1 =  (2/n)dl2 S)^/K*(mi)\Vi\l\ 1/2
n 
\nvij I r I
1=1
This approximation provides 6~Y,ikiN (m i,Vi) with fcj =  ^ “ (m i^Vi!1/ 2, 
i =
3.2.2 Mode calculation
The above methods are simple and only involve the calculation of deriva­
tives of the logarithm of the posterior density. The outstanding problem 
remains the calculation of the mode. It is generally obtained as the solution 
to the equation
d l o g  7r ( f l )  =  
09
In many cases, it is not possible to solve the equation analytically and 
numerical methods must be used. The most common ones are the Newton- 
Raphson algorithm and its deterministic variants and Fisher scoring where 
the Hessian of log tt is replaced by its sampling expectation. These are 
iterative methods that must be repeated until convergence is reached. A 
survey of the Newton-Raphson method in statistics is provided by Thisted 
(1988).
E xam ple 3.2 Consider again generalized linear models with a normal prior 
(3 ~  N (a ,R ) as in Section 2.3.2. West (1985) showed that Fisher scoring 
leads at iteration j  to the construction of adjusted observations
Vi =  yi(/3U~1)) =  s(/4J_1)) +  (Vi ~
with means 
=  - 6 ' ( ^ _1)) and variances Vi =
Vi((3^ ~ ^ ) =  - b ”
) [ g ' ) } 2, where 
are canonical parame­
ters, i =  1 , . . . ,  n, and 
is the value of [3 at iteration j - 1 .  Forming the
vector y =  (yi , . . . ,  yn)' and the matrix V  =  diag (Vi,. . . ,  Vn), the adjusted 
regression model 
)\f3 ~  N (X(3, V(/3^-1 ^)) is obtained. Combining
with the prior (3 ~  N (a ,R ), the adjusted posterior is ir((3) =  N (m ^  , C ^ )  
where the posterior mean is 
=  C ^ (R ~ 1a +  X ' V _1y) and C ^  =  
(i?-1 +  X 'V ~ l X )~ 1. Then, one can set [3^ =  
and move to iteration 
j  +  1 by restarting the above procedure.
After convergence, (3^ is the posterior mode of (3 and it is also conceiv­
able to approximate the posterior of f3 by a N (m ^\ C ^ ). This method is an 
adaptation of the method of iterative reweighted least squares (IRLS) used 
to obtain the maximum likelihood estimator (3 in generalized linear models 
(McCullagh and Nelder, 1988). The main difference is the introduction of a 
prior distribution that leads to the calculation of the posterior mode instead 
of the maximum of the likelihood. The method provides as by-products an

Asymptotic approximations
87
approximation for the variance and a normal approximation for the rele­
vant distribution of (3 in the Bayesian framework and (3 in the frequentist 
framework. The extension of these algorithms to hierarchical and dynamic 
models is presented in Chapter 6.
In the case of more complex models with many components representing 
qualitatively different aspects of the model it is possible to use an itera­
tive algorithm of mode search proposed by Lindley and Smith (1972) in 
the context of hierarchical models. Consider 9 =  {(j),'ip) and let 7r^(^) =  
max^,7r(0, ip) =  n((p,ip(<p)) and n^ip) =  max,£7r(</>, ip) =  n((p(ip),ip). If <p 
(ip) maximizes 
(7r^,) then it maximizes 7r and the value of 9 that maxi­
mizes 
ip) is 9 =  {(j), ip). Note that 4>{ip) {4>{4>)) is the mode of conditional 
posterior of <p (ip) . Many times, it is easier to maximize the conditional den­
sities than the joint density, especially under conditional conjugacy. Lindley 
and Smith (1972) suggest the following iterative algorithm:
1. initialize the iteration counter j  — 1 and set an initial value cp^ for the 
mode of (p\
2. calculate the conditional modes ip^) =  ^(^t?-1 )) and 
=  <p(ip^);
3. change counter from j  to j  +  1 and return to step 2 until convergence is 
reached.
The method moves towards the joint mode along the directions of the 
components. It may be slow if the posterior shows high correlation between 
the components. The convergence criterion can be any measure of distance 
between successive iterations. It is recommended to allow a few additional 
iterations to make sure that a point of maximum and not a saddlepoint 
has been reached. Calculation of posterior mode through iterations of con­
ditional modes always converges to a mode. Additional verifications are 
required to ensure the mode encountered is global. It is recommended that 
a few different starting points are used to make sure that a global (and not 
local) maximum is reached. This method was also studied by Besag (1986) 
in the context of spatial statistics.
::s)-o ?: 
.
N(pip, 1 — p2) and tp\cp ~  N(p<p, 1 — p2). The conditional modes are cp(ip) =  
pip and ip((p) =  p4>. Hence, 1p ^  =  pip^~^ =  p2cp^~l"> =  p2^(p^ and anal­
ogously ipd') =  p2jip(0'). This algorithm will converge to the unique joint 
mode (0,0) for any value of p. The convergence can be slow if the correla­
tion p between 4> and ip is close to 1 in absolute value (0  ’Hagan and Foster, 
2004).
The case of 9 =  (9\,... ,94)' having d components becomes easier to 
understand. The iterative method calculates at each iteration the complete 
set of d full conditional modes 9i{9- \ ) , . . . ,  9d{9-d). Starting with an initial
. By (1.2), <p\ip

value 
it iterates around full conditional modes until convergence is 
reached. It is illustrated below for the one-way model described in Example 
2.8.
E xam ple 3.4 Consider observations yij\Pi, cr2 ~  N (pi, a2), j  =  1 ,..., n*, 
and Pi\fi,r2 ~  N (h ,t 2), i =  1 
Assume independent priors for
</>i =  cr~2 and 02 =  t ~2 with respective distributions G (na/2,naSa/2) 
and G (nT/2,nTST/2). The joint distribution of all variables of the model
(y,P,n,<l>u<h) is
d 
Ui 
d
U U M W M T 1) 
fN (v,b o,B 0)
i= l  
i= 1
xfG(4>i;na/2, n<TS0/2) f G{4'2; nT/2, nrST/2).
The following full conditional distributions are obtained:
a) ir(Pi\P-i,fj.,<j>i,^>2) =  Tr{Pi\n,4>i) =  N(t*i,Ci) where 
+
4>2/i) and Ci =  (ni<pi +  ifc)-1 , i =
b) 7r(/x|/3,0i,02) =  n(fj.\p,<p2) =  iV(6i,Bi) where bi =  B i(B Q lb0 +  fafi) 
and B\ =  (B q 1 +  n02)_1 and n =  £*71*.
c) 7rOi|/?,/z,02) =  tt{4>x\P) =  G (n * /2 ,n*S*/2) where n* =  na +  Y?i=1
flTid naSa =  nc S(j -I- 
/3^) .
d) it(4>2\P,h,4>i) =  7t(02|/3,/x) =  G(n*/2,n*S*/2) where n* =  na +  d and 
n*S* =  nTST +  Ei(Pi -  /x)2.
The respective modes of the distributions above are Pi =  fa, fi =  b\, 
4>i =  (n* -  2)/n*aS* and 02 =  (n* -  2)/n*S*. The joint posterior mode 
can be iteratively obtained through the following cycle:
1. initialize the iteration counter j  =  1 and set initial values for the con­
ditional modes P^  and 
. For example, take 
=  yi and 
=  y;
2a. evaluate the conditional modes (f>^ and ^
 as functions of P^ ^ 
and
2b. evaluate the conditional modes p\^ and fj,^ as functions of <j>^ and 
<t>2 ] ;
3. change the counter from j  to j  +  1 and return to step 2 until convergence 
is reached.
3.2.3 Standard Laplace approximation
One way to improve the approximations is to include higher order terms 
in the Taylor series expansion of the log posterior. Lindley (1961, 1980) 
considered the inclusion of third order terms leading to
7r(0) oc 7r*(m) exp j  — ^(0 — m )V ~ 1(9 — to) +  — 
\
88 
Approximate methods of inference

Asymptotic approximations
89
where
=  Y U 2 J2 7riik^ i~ rni^ eJ~m^ ^ k~ mk')and 7r°'fc =  dJ of)o n o '1 • 
»=ij=ifc=i 
1 
i 
k
The third order term has little influence for regions close to the mode but 
away from the mode it becomes substantial and may cause instability in 
the approximations. This can be alleviated by further replacing the term 
exp {i?(0 )/6} by l +  R(9)/6 and the terms in the exponent will dominate the 
tails. A new problem is caused by the possibility of negative approximation 
values. Again, this is only likely to occur in the tails where the influence of 
this term is negligible.
Integrals and posterior expectations can now be performed. The posterior 
mean of a transformation ip =  t(9) can be approximated by £ V [£(#)( 1 +  
R(9)/6)} and the term associated with R(9) is clearly responsible for skew­
ness corrections. For scalar 9, R(9) =  
— to)3 where 
is the third
derivative of log tt evaluated at m. For t(9) =  9, E(9) =  m +  n ^  Em(9 — 
m)4/6 =  to +  ■k^V2/2.
E xam ple 3.1 (continued) The posterior mean of X can be approximated by 
the above expression. It is straightforward to obtain that 7r('3) =  2(a\ — l ) / m 3 
and that
=  
„ , +  2(Q. - l ) ( a » / / W
TO 
2
_ 
ai -  1 
1
~  
Pi 
+ J i 
-
~  
Pi
and the approximation in this case is exact.
For a vector 9,
d2t(m ) 
dt{m) ^
 ^
 
d3 log 7r(m)
d9id9-j 89i 
^
 
kl d9.-,d9kd9i 
'
J 
k=1/=1 
J 
_
The derivation is left as Exercise 3.3.
These approximations are called standard Laplace approximations by 
Kass, Tierney and Kadane (1988). They are 0 (n ~ 2) which is a qualita­
tive improvement over the first order, normal-based approximations. Their 
main problem is the need to evaluate third order derivatives. It can be a 
substantial overhead if the dimension of 9 is large. An alternative use of 
Laplace approximations that is also 0 (n ~ 2) but requires only calculations 
of first and second derivatives is presented below.
- 
d 
d 
E\t(9)] =  t(m ) — —
2 = 1 ? = 1

90
Approximate methods of inference
3.2.4 Exponential fo rm  Laplace approximations
Calculation of posterior means is given by (3.1) and can be rewritten as a 
ratio of integrals
£[*(»)] =  /  
. 
(3.5)
Equation (3.5) takes explicitly into account that the normalization constant 
may not be available, as is common in many posterior forms. Again the 
same idea of the previous section can be used to approximate integrands 
by quadratic forms in the log scale. The significant difference here is that 
it can be operated to both numerator and denominator in (3.5) which will 
be shown to lead to better approximations.
Assuming that t(6) > 0, define L(9) =  log 1(6) +  log p(0) and L*(6) =  
log t(6) +  log 1(6) +  log p(6), m* as the value that maximizes L* and V* as 
minus the inverse Hessian of L* at the point m*. It follows that E[t(d)\ =  
J exp[L* (6)\dd /  J exp[L(6)]d9.
Taking a Taylor series expansion of L* up to the 2nd order term as in 
(3.3) for L gives
L*(6) 
= 
L * ( m * ) - ^ ( 0 - m * ) V * - 1 ( 0 - m * ) .
Replacing alongside (3.3) in (3.5) gives
E [t(8)]=  
exP 
~ L(m)]. 
(3.6)
Tierney and Kadane (1986) noted that the advantage of this result is that 
despite it being based on approximations that are 0 (n -1 ), the leading 
terms in the numerator and denominator cancel out. Therefore, the result­
ing approximation is 0 ( n ~ 2).
E xam ple 3.1 (continued) (Press, 2003) The posterior m ean o f A m ay also 
be approximated by (3.6). In this case, L ( A) =  (a\ — l)log A — /3iA and 
L*(A) =  a \ log A — 
A. Therefore, m* — a j//3 i and V* =  a \ j 0 { .  Hence 
the approximation is
-^(A) =  \ l/„. 
1 ~i"wo2exP {(a i loS m* -  fam *) -  [(ai -  l)log m -  Pim ]}
(^1/Pi 
(ai -  I)/Pi'
~a[ 
(ax/pxf
ai -  1 [(ai -  l)//?i]“ 1_1e_ (Ql_1)
W hen a  = P = x  =  3, the error is only 0.

Asymptotic approximations
91
The calculations above are based on exponentiating the logarithm of the 
integrands. Kass, Tierney and Kadane (1988) refer to them as exponential 
form approximations to distinguish them from the standard form based on 
third derivatives. They improve upon the standard form by not requiring 
evaluations of third derivatives. The exponential form is usually preferred 
and for that reason is sometimes identified as the Laplace approximation.
The exponential form evaluates the logarithm of t(9) and therefore can 
only be applied to positive functions t(9). If t is not positive, it may be 
possible to define t*{0) =  t(9)+ a where a is a positive constant large enough 
to ensure that t* is positive. Then, the exponential form can be used to 
approximate E[t*(6)\. After that, it is easy to obtain E[t(9)] — E[t*(9)} — a.
Alternatively, the moment generating function of t(9) given by M t{s) =  
E[exp (st(9))] can be used. The function M t is always a positive function 
and can be approximated by M t given by the exponential form (3.6) for 
any value of s. From basic probability theory, E[t(9)] =  dMt(0)/ds and 
the posterior mean of t{9) can be evaluated by differentiating M t at 0. In 
practice, the analytic expression of Mt as a function of s is usually not 
available and numerical differentiation should be used. Tierney, Kass and 
Kadane (1989) showed that working with the moment generating function 
still retains approximation to 0 (n~2) and that it equals the approximation 
based on addition of a large constant a when a —> oo.
In the case of approximation of marginal densities, consider 9 =  {(j), ip) 
and assume the posterior marginal density of <p is required. It is given by 
7r(0) oc /  exp (L^(tp))dxp where L<f,(ip) =  log 1(0, ip) +  log p(<p, ip). Let ip((p) 
be the value of ip that maximizes L$. By the quadratic approximation (3.4),
7r((p) (X
d2L S m  “ 1/2
dipdip'
exp [L(cp,ip((p))\
If the prior p(<p,ip) oc k then L =  log I and exp [L((p, ip(4>))\ =  l((p,ip(<p)), 
the profile likelihood of (p. The resulting expression is the modified profile 
likelihood suggested by Cox and Reid (1987) for likelihood inference af­
ter elimination of the nuisance parameter ip. Tierney and Kadane (1986) 
showed that the order of the approximation error falls to 0 (n -3 / 2) which 
still compares favorably with errors of order 0 (n ~ 1//2) from the normal 
approximation.
A new problem here is the need for calculation of ip((p) for a collection of 
values of (p. When ip is conditionally conjugate, this value can usually be ob­
tained analytically. The approximation for the marginal density may then 
be obtained in closed form. In many higher dimensional or non-conjugate 
models, this is rarely the case and maximizations must be repeated for each 
value of (p and the computational burden becomes substantial. When tfi is 
uni- or bidimensional it is still possible to obtain graphical representations

92
Approximate methods of inference
of its marginal density. For higher dimensions, there is also the additional 
problem associated with the cost of the calculation of the Hessian of L^.
The success of these approximations depends on unimodality of the pos­
terior and similarity of the posterior with the normal forms used. Skew 
distributions are commonly associated with parameters with variation in a 
subset of the line. Again, useful reparametrizations may lead to a parameter 
with a more normal-like posterior. Hills and Smith (1992) suggest graph­
ical techniques to guide the choice of the transformation required. Kass 
and Slate (1992) suggest analytical techniques based on third order deriva­
tives of the posterior density to evaluate the similarity of the considered 
transformation with the normal distribution.
E xam ple 3.5 Consider the model x\9 ~  bin(n, 9) with Jeffreys priorp(9) oc 
9- 1/2(i _  $)-i/2 From Bayes’ theorem
7r(6») a  9X{ 1 -  6>)n_x<r 1/2(1 -  9) - 1/2 =  9x~1/2( 1 -  9)n~x~ 1/2
and, a posteriori, E(9) =  ( x + l / 2 ) / ( n + l )  can be exactly calculated. Assume 
the exact calculation was not possible and E(9) is to be approximated by 
Laplace forms.
Maximization of L and L* give m =  (x -  1/2)/(n  -  1) and m* =  (x + 
1/2)/n. Also, V  =  n(x — l/2)(n — x -  l / 2 ) / ( n -  l ) 3 and V* =  (x +  l/2)(n — 
x — \/2)/n2. Replacing in (3.6) gives
- 
(n — 1)7I+1/'2(x +  l / 2 ) x+1 
{>  
n " + 3 / 2 ( i - 1 / 2 ) »  
‘
When n =  5 and x  =  3, E (8) =  0.583 and E(9) =  0.563 with error of 
3.6%.
The reparametrization with constant Jeffreys prior is A =  arc sin (V9) 
and 9 =  sin2A. The posterior becomes
7t(A) oc (sin2A)x(l — sin2A)"~x.
Applying (3.6) for t(X) =  sin2A gives
nn+l/2(x +  l ) x+1
E(0) =
(n +  l)« + 3/ 2x x
When n =  5 and x  =  3, E(9) =  0.580 with percentaul error of 0.6%.
These results were obtained by Achcar and Smith (1989) and provide 
an indication of the possible gains when calculations are performed in a 
suitable parameterization. Other parametrizations and values of n and x 
were considered as also leading to better results than those obtained with 
the original parametrization.

Approximations by Gaussian quadrature
93
3.3 
Approximations by Gaussian quadrature
Consider for the moment the one-dimensional problem of evaluation of I  =  
j  b g(9)d9. Quadrature rules approximate I hy I — XliLi wi9(@i) f°r some 
weights Wi and grid points 9i,i =  1 ,...,  n. A simple rule is to take n equally 
spaced points with equal weights c =  (b — a)/n. For most one-dimensional 
integrals, taking n of order 102 suffices for a good approximation. This rule 
can then be extended for integrals over the real line when g(9) becomes 
negligible outside some wide limits. This is typically the case in statistical 
applications. Other simple quadrature rules are the trapezium rule where 
the grid endpoints 9\ and 9n receive half weight and Simpson’s rule where 
weights alternate between 4c/3 and 2c/3 apart from the endpoints that 
receive weight c/3.
Improved rules were obtained by taking the form of the integrand into 
consideration. Gaussian rules were developed and tabulated when the in­
tegrand is well approximated by a form h(9)p(6) where h is a polynomial 
function of 9 and p is a density function. When p =  U[—1,1], Gauss-Jacobi 
or Gauss-Legendre rules are obtained. When p =  G (a, 1), Gauss-Laguerre 
rules are obtained and when p =  Ar(0,1), Gauss-Hermite rules result. Each 
of the above rules is appropriate for a different support for 9. Given the 
important part played by the normal distribution in approximating the 
posterior density, the latter are usually preferred for approximating equa­
tions in the form (3.1). Reparametrizations must be made if the support of 
9 is not the real line.
Assuming an integration of the form I =  
exp (—92)h(9)d9, Gauss-
Hermite rules approximate I  by I  =  £ "=1u^/i(xj) where Xi is the ith zero 
of the Hermite polynomial Hn(9) and the respective weights are wt — 
2n~1rb/ir/[nHn-i(x i)]2, i =  1 , . . . ,  n. The approximation is exact and 1 =  1 
if h is a polynomial function of degree 2n — 1 or less. The approximation 
will be accurate if h is well approximated by a polynomial of degree 2n — 1. 
Virtually any function can be well approximated for n sufficiently large. 
In practice, low values of n (around 6 to 8) are enough. The values of the 
weights Wi and zeros x t can be found in mathematical handbooks such as 
Abramowitz and Stegun (1965).
For applications, adjustment must be made to account for a non-standard 
normal distribution. The integral /  =  J^°oog(9)d9 where g(9) is well ap­
proximated by h (9 )fN (9;m ,V ) is given by I  =  £i</j/i(#i) where qi =  
Wiy/(2V )eei and 9i =  m +  y j(2V )xi, for weights wi and zeros Xj, i =
1 ,...,  n (Naylor and Smith, 1982). If the posterior tt(9) =  kTr*(9) is known 
up to a proportionality constant k, Gauss rules can be applied to the nu­
merator and denominator of (3.5) to approximate E\t(9)\ by
Y^= 1qit (9i)Tr*{9j)
Y^= 1qiTr*(9i)

94
Approximate methods of inference
with m and V given by the posterior mean and variance of 9. Consequently, 
the rule also provides estimates of k. Note that the same approximation 
was applied to the numerator and denominator. If the posterior density 
is well approximated by a product of a normal by a polynomial of degree 
2n — 3 then evaluation of the constant k, the mean and the variance can 
be efficiently performed using a single n-point Gaussian rule.
In the multivariate case, consider integration of g{9) where 9 is a d- 
dimensional vector. Then
j m
»  
-  
j
j
 g(9i,9 -i)d 9-id 9 i 
=  J  9i(9i)d0i
where gi(9\) =  f  g{9\, 9-i)d 9-\ . The remaining integral of g\ is one­
dimensional and can be approximated by 
f°r weights
qu and points 6i<n as before. Of course, gi itself is the result of a (d -  1)- 
dimensional integral that must be evaluated. For each of the 
points 
chosen for 9\, it is given by
j  g(9\,9-i)d9-\ 
— J  J  g(9\,92, 9- i 2)d9- i 2d92
=  J  92(9i, 92)d02
where g2(8i , 92) =  J g (9i , 92, 9-i)d 9- i 2 and 6Li2 is a (d -  2)-dimensional 
vector obtained from 9 by removal of its first and second components. Inte­
gration of g2 for any given value of 9i is one-dimensional and can be approx­
imated by ]C"22=i QiiQ'ziOx, 02,i2) for weights ql2 and points #2,i2- Therefore,
I  is approximated by
i*i
*1 — 1 
n i 
n 2
^  , Qii ^  '  Qi292(9l,in 92^) - 
ii = l 
12 =  1
Proceeding by this route for the remaining d — 2 one-dimensional integrals 
gives
n i  
n  d
I  
^  '  Qii * ‘ * ^  ^ *2 id 9 (@ l,ii i • ■ ■ 5 
.
i i =  l  
id =  1
The approximation involves evaluation of the function g at n f= i 
(or
when rules of same size n are used for all components) points.
This integration by Cartesian product rules is very sensitive to the choice 
of m and V and depends on an approximate assumption of independence

Monte Carlo integration
95
between the components. If they are highly correlated, most of the eval­
uations of g will fall in regions of negligible value leading to very poor 
approximations (Smith et al., 1987). They suggest the use of an iterative 
strategy where the parameters are initially transformed to take values on 
the real line. Approximate posterior mean rn and variance V for the result­
ing parameter vector (p are obtained. A new, standardized parameter 'ip is 
constructed according to ip =  L(<fi — m) where L is the square root of V ~ 1 
satisfying LL' =  V -1 . ip will then be a vector with approximately inde­
pendent, standardized components. Cartesian product rules are then used 
to obtain new values of m and V. These are then used to redefine ip and 
the iteration continues until the values of m and V converge. Naylor and 
Smith (1982) recommend that this iteration should be performed on a low 
dimension grid with nd points and after convergence calculations should be 
refined with a larger grid with (n +  l)d points. When convergence between 
different grid sizes is also achieved, posterior quantities of interest are then 
calculated. Iterations can start with n as low as 3.
An outstanding problem with these cartesian product rules is that the 
number of function evaluations increases exponentially with the dimension 
of 6. This limitation restricts the application of Cartesian product quadra­
ture rules to models of single digit dimension. Alternative rules to avoid 
this so-called curse of dimensionality are discussed by Evans and Swartz
(1988), Shaw (1988) and Dellaportas and Wright (1991).
3.4 M onte Carlo integration
Consider as before the problem of solving Equation (3.1). If a sample
6i , . . . , 8n from 7r is available then a natural estimator for /, commonly 
called the simple Monte Carlo (MC) estimator, is
One important application of this result is the derivation of the marginal 
density of 9i given by Equation (3.2). A simple MC estimator of this 
density is obtained by sampling 0i,_i,. . . ,  0n<-i from 7r(6Lj) and setting 
t ( 6 - i )  =  n ( Q i \ Q - i ) . Quite often, sampling from tt( 9 ) (or tt(#-i)) is either 
computationally inefficient or costly. Simple Monte Carlo methods must be 
extended by the use of draws from auxiliary (importance) densities. More 
specifically, let q ( 9 ) be another density for 9 with the same support of tt.
where E f denotes expectation with respect to density / .  If a sample 9\,... ,9n
Then
I  =

96
Approximate methods of inference
from q is available then
(3.7)
is a another estimator of I. Ii is a special case of 12 obtained when q =  n. 
Notice that both 7i and I2 are method of moments estimators of I. These 
estimators enjoy good frequentist properties such as:
• they are unbiased estimators since Eq(Ik) =  I, for k =  1,2;
• their variances are in the form Vq(Ik) =  o^/n, for fc =  1,2, where a\ =  
J[t2(9)Tr(6)}dd — I 2 and a\ =  J[t2(9)TT2(9)/q(9)\d9 — I 2\
• they have central limit theorems stating that
y/n —— -  - i  AT(0,1) as n —» 00,
(3.8)
(7k
for k =  1,2; and
• they are strongly consistent estimators of I  in that
T  
C l- S.  
j
Ik —> I as n —► 00,
(3.9)
for k =  1,2.
The classical nature of the above results leads to objections by Bayesians 
(O ’Hagan, 1987). These results provide important messages, however, and 
in practice they are widely used. Strong consistency follows directly from 
the strong law of large numbers (Feller, 1968). So increasing the size n 
of the sample from q will lead to a virtually error-free estimation at rate 
0 (n -1 / 2). Unlike asymptotic results, this value of n is under the control of 
the researcher and can be increased by drawing more values from q. The 
constant a2 depends on q and can also be estimated by the method of 
moments.
The generating density q is usually called the importance density and 
sampling from q is called importance sampling. There are no restrictions on 
q and the simplest choice is the uniform distribution when the support of 9 
is compact. It can be shown that the optimal choice in terms of minimizing 
a 2 and hence the estimation error is to take q oct x ir. Unfortunately, for 
most cases where (3.1) cannot be evaluated analytically, it will be very 
difficult to draw samples from n. The above results however suggest that q 
should be taken as close as possible to 7r but still available for easy sampling. 
In any case, the importance density q can be chosen to approximate t x -it 
for each required expectation of t(B) or can be chosen to be the same for all 
integrations of interest. Kloek and van Dijk (1978) recommend the latter 
with the importance density q chosen to approximate it.
Geweke (1989) provides a formal proof of the central limit theorem. It 
may be used to assess coverage probabilities by confidence intervals thus

Monte Carlo integration
97
providing error bounds for the estimates unlike the previous estimates pro­
posed. Carlin and Louis (2000) and Evans and Swartz (1995) consider this 
ability as one of the main strengths of approximations based on Monte 
Carlo techniques.
A problem that usually arises in Bayesian applications is that tt is only 
known up to a proportionality constant. Posterior expectations are really 
a problem involving a ratio of two integrals as pointed out in (3.5). The 
resulting approximation is based on the ratio of two Monte Carlo estimators 
of integrals. Using the same importance density q as recommended above, 
the numerator and the denominator are approximated by (3.7) with tt 
replaced by tt* =  I x p and in the case of the denominator t =  1. The form 
of the estimator is then
where the #jS are the same on numerator and denominator and are sampled 
from q. The above estimator is only asymptotically unbiased but is still a 
strongly consistent estimator of I.
Monte Carlo integration has been connected to Bayesian inference after 
its introduction in applied Econometrics by Kloek and van Dijk (1978). 
Medium sized models have been commonly used in this area and their 
paper showed it is a viable technique. Much effort has been devoted since 
then to the specification of suitable importance density functions. It is 
important that it matches tt as close as possible and that it blankets tt 
in the tails. Otherwise, the very few points sampled in the tails may have 
large contributions to I  and estimates will be unstable. This suggests that 
normal distributions should be avoided if possible.
In the multivariate setting, natural choices for importance density are 
the Student’s t distributions with low degrees of freedom. These are easy to 
sample, have thick tails and support over R d. They may therefore require 
transformation of some of the components of 9 to the real line. Geweke
(1989) suggested the use of split-t distributions which are obtained by 
rescaling each component of 9 differently for positive and negative val­
ues to accommodate skewness. Oh and Berger (1993) suggested the use of 
mixtures of ^distributions to accommodate posterior multimodality.
These functions require specification of mean and variance which them­
selves are obtained by integration. This suggests an iterative scheme where 
means and variances are evaluated for a given importance function and 
used to update mean and variance specifications of a new importance func­
tion. The process is repeated until the successive values of means and vari­
ances do not change. Then, integrations of interest are performed. Adaptive 
strategies have been suggested by Kloek and van Dijk (1978) and Smith et 
al. (1987). Oh and Berger (1992) established convergence results of these 
iterative strategies. Examples in Evans and Swartz (1995) suggested that

98
Approximate methods of inference
few (less than 10) iterations are required for convergence with sample sizes 
of order 103 to 104.
3.5 M ethods based on stochastic simulation
Methods of inference based on stochastic simulation use samples from the 
posterior tt to summarize information. Obviously, Monte Carlo integration 
is also a method of inference based on simulation. The difference lies in 
the fact that to perform the necessary integrations there was no need to 
actually produce a sample from ir. This section explores methods designed 
to (at least approximately) produce a sample from ir and shows the use of 
these methods in Bayesian inference. This is the theme of this book and 
this section provides the first formal contact with it.
From the start it should be pointed out that no matter how large the 
sample is it is only a partial substitute for the information contained in 
the expression of a density. These methods provide only an approximation 
to the posterior and should only be used when it is not possible to extract 
information from the posterior analytically. As approximations, they share 
the advantages of Monte Carlo integration techniques: their accuracy is 
controlled by the size of the sample irrespective of the number of obser­
vations in the data and the approximation errors can be probabilistically 
measured.
Once a sample 9\, . . . ,  9n from ir is available, many standard summa­
rization operations can be approximately made. The posterior mean of
9 is estimated by the arithmetic average of sample values. Posterior mo­
ments of 9 and more generally posterior means of transformations t{9) 
are estimated by the average of the corresponding operations on the 9i. 
For example, if 9 =  (#i,. . . ,  9d)', the posterior mean of the parameter 
ip =  9j +  . .. +  91 is estimated by ip =  (1/n) Yh=i i’i where ipt =  9^ +  .. -+92d 
and 9i =  (9n, . . . ,  9d)' is the ith element of the sample, i =  1 ,...,  n. Pos­
terior probabilities of intervals or regions C  are simply approximated by 
the proportions of sample values belonging to C. A 100(1 — a) % credi­
bility interval for a scalar transformation ip of 9 is approximately given by 
\'4){{an/2\)i ^([71(1- 0/ 2)])] where x ^  is the jth  ordered sample value and [x] 
is the smallest integer not larger than x. For example, if n =  1000 and 
a =  0.05, the credibility interval will have endpoints given by the 25th and 
975th largest values of ip. O f course, unequal tail probabilities can also be 
used.
All the above approximations are unbiased, method of moments esti­
mates of their respective posterior quantities and their sampling error can 
be assessed (Exercise 3.14). Also, once a sample from a scalar quantity is 
available, a histogram can be plotted and provides an estimate of the den­
sity. These histograms provide a rough, discrete-like estimate that is often 
adequate to convey relevant aspects of the posterior. However, they may be

Methods based on stochastic simulation
99
smoothed according to some non-parametric smoothing density estimation 
technique (Silverman, 1986). A Bayesian approach to the problem is given 
by West (1992).
This approach can be readily applied to obtain samples from a predictive 
distribution. Recall from Chapter 2 that the predictive distribution for y 
after observing x is given by f(y\x) which itself is the marginal density ob­
tained from the joint posterior f(y , 9\x) =  f(y\9)Tr(9). A sample y i , . . . , y n 
from this distribution can be obtained via mixture techniques (Section 1.3) 
by drawing each j/j from f(y\9i) where 9i is the ith element of the sample 
from the posterior 7T, i =  1 , . . . ,  n.
Availability of a sample for 9 =  (9i, . . . ,  9d)' implies that samples for the 
component 9j, j  =  1,
,d, are given by the jth  components of each sample 
value 9i, i =  1,... ,n. This trivial operation replaces the often intractable 
integration of the posterior over the remaining d — 1 components of 9. More 
generally, a sample of any parametric transformation ip =  t(9) is given by 
ipi, . . . ,  ipn where 4>i =  t(9i), i =  1,... ,n.
The previous sections showed the importance reparametrizations can 
have in techniques for approximate inference. It is nice to know that these 
can be easily handled by simulation-based methods. In some circumstances, 
although the problem is parametrized by 9, it is more convenient to proceed 
sampling through ijj =  t(9), where t is a one-to-one transformation. Once a 
sample 
, • • •, V’n is obtained, a sample from 9 is given by 9i, . . . ,  9n where 
9i =  s(;tpi) and s is the inverse of the function t.
This easy operation replaces possibly difficult operations required to ob­
tain the density of transformations. These densities can only be obtained 
when an explicit expression for the transformation t is available. Sampling- 
based methods can do without it. Press (2003) considers sampling from 
the distribution of the second largest eigenvalue 
of a random matrix 'I'. 
There is no explicit analytic expression form for t such that \2 =  £(*£)• 
Nevertheless, such a function exists and is well defined. Furthermore, it 
can be used to obtain A 2 for any given value of 
by numerical techniques. 
Thus, if a sample ^ i , . . . ,  \I/n is available, so will be a sample A21, • • •, A2n- 
Sampling-based approximate inference will then become possible when an­
alytic methods are hopeless.
All the above derivations are based on a sample from 7r. Unfortunately, 
it is very complicated to directly sample from it for the vast majority of 
problems of practical relevance. Therefore, some ingenuity must be exer­
cised to allow such a task to be accomplished. This book is concerned with 
stochastic simulation via Markov chains but it is important that less elabo­
rate, more limited techniques are described. These are proving to be useful 
in many problems where the parameter is not highly dimensional. This 
section provides a brief introduction to them.
Methods of generation from a density of interest n using samples from 
an auxiliary density q were presented in Section 1.5. Smith and Gelfand

100
Approximate methods of inference
(1992) propose simple forms to give this problem a Bayesian interpretation. 
Their idea was to provide a simulation-based parallel to Bayes’ theorem. 
This can be achieved by taking q as the prior for 8.
Assume that the posterior density 7r is only known up to a proportionality 
constant. Therefore, 7t*(8) =  l(8)p(0) is available where I is the likelihood 
and p is the prior for 8 but 7r =  kn* is not because the value of k is unknown. 
The methods of that section worked just as well with 7r*.
3.5.1 Bayes’ theorem via the rejection method
A value is drawn from 7r by the rejection method by generating a value 
from q and accepting it with probability n*(8)/Aq(8) (Section 1.5.1). The 
constant A <  oo satisfies the blanketing condition tt* (8) / q(0) < A, for all 
8. Efficiency of the method is improved as the value of A gets smaller.
Taking q =  p, the prior for 8, and 7t* — I x q gives 7r*(8)/q(8) =  1(8) and 
the smallest constant A ensuring an envelope is lmax — maxe 1(8) — 1(8) 
where 8 is the maximum likelihood estimator of 8. The acceptance ratio 
becomes
ir*(8) _  l(0)p(8) _  1(8)
Aq(8)
lmaxp(0)
I m a x
which does not exceed 1.
Bayes’ theorem describes the passage from prior to posterior after ob­
serving the data. A sampling version of the theorem describes the passage 
of a sample from the prior to a sample from the posterior. By the rejection 
method, this passage is clear:
1. A value 8 is drawn from the prior density p(8);
2. This value is accepted with probability l(8)/lmax-
Some care must be exercised despite the simplicity of the method. First, 
the method always ends up with a resample smaller than or equal to the 
original sample as some values proposed in Step 1 will not be accepted 
in Step 2. This can be a problem when sequentially updating the prior as 
in dynamic models or when prior and likelihood provide conflicting infor­
mation. The prior density and likelihood function that led to one of the 
posterior densities of Figure 2.1 are reproduced in Figure 3.2. Values gen­
erated according to the prior will have very reduced likelihood values and 
therefore will have low acceptance probabilities. This problem is more likely 
to occur in higher dimensional parameter spaces. Some study of the possi­
ble disagreement between prior and likelihood must be undertaken before 
a blind use of the method. Tachibana (1995) discusses forms to avoid or 
alleviate these problems (see also Newton and Raftery, 1994).
Another problem associated with the method is the need to maximize the 
likelihood to ensure total envelope. This maximization is not always easy 
to perform (Section 3.2) and sometimes requires the use of sophisticated

Methods based on stochastic simulation
101
Figure 3.2 Prior (dotted line), likelihood (dashed line) and posterior (solid line) 
corresponding to the posterior of Figure 2.1 associated with the Cauchy prior. 
The small negative bars indicate a sample drawn from the prior. Positive bars 
indicate the weights of draws.
tools. This may render computationally expensive a method whose main 
attraction is simplicity. Below, an alternative method that does not require 
likelihood maximization is presented.
3.5.2 Bayes’ theorem via weighted resampling
A sample from n can be generated via weighted resampling methods (Sec­
tion 1.5.2) by drawing a sample 0\,. .., 9n from q and resampling from the 
discrete distribution in {9 \,. . . ,  9n} with probabilities Wi given by
Wi =
K{0j)/q{0i)
T . U  < ei)lqW iY
i,
> n.
Substituting the values of q =  p and noting that n/q =  kl gives w, =  
K^*)/ 
i =  
• • ■ >n - Therefore, Bayes’ theorem via weighted re­
sampling methods proceeds as follows:
1. A sample 9\,. . .  , 8 n  is drawn from the prior density p(9);

2. The values of the sample are resampled with probabilities
... 
W i) 
, . 1 
„
w i 
\-^n 
, / n  \ > 
^ 
' ' >n '
2^,j= 1 •'{"j)
Compared with rejection methods, this method does not necessarily re­
duce the original sample size and does not require maximization. This only 
makes more relevant the need for care in case of conflicting information 
between prior and likelihood. In this case, many original sample values are 
equally unlikely under the posterior but this is not considered. Calculation 
of the weights only considers them relative to other sample points without 
accounting for their absolute posterior weight. Once again, higher dimen­
sional parameter spaces will make difficult an efficient use of the method as 
adequate coverage of the relevant posterior regions by the original sample 
becomes harder to achieve.
E xam ple 3.6 (Rao, 1973) A total of n animals are categorized such that 
U — {yuV 2,yz) are the counts per category with cell probabilities given by
0.25(2 +  0,1 — 0 ,9), leading to the probability function p(y\9) oc (2 +  0)J/1(l — 
9)V29m , 6 e (0,1). Combining it with a uniform prior for 0, ie. 9 ~  C/(0,1) 
and data y =  (125,38,34), leads to the posterior density
7r(9) oc (2 +  0)125(1 -  9)38934.
Posterior simulation is performed through the weighted resampling algo­
rithm with proposal density q(9) =  /jv(0; 0.63,0.052) (Section 1.5.2). Based 
on a sample of size M  =  10000 from -k(9), approximations for the poste­
rior mean, variance and 95% credibility interval of 9 are 0.62, 0.0026 and 
(0.517; 0.717), respectively. A histogram approximation to the posterior dis­
tribution appears in Figure 3.3. Other approximations to the posterior dis­
tribution were considered by Tanner (1996).
E xam ple 3.7 (Bates and Watts, 1988) The following non-linear model 
with exponential decay
6
P(y\/3,v2) =  
-  e~02Xi),a 2), 
j3 =  (ft,/3 2)
i= 1
was entertained for description of the demand of oxygen (measured by the 
oxygen concentration y) by microorganisms according to the number of days 
x the experiment is carried out.
Assuming a flat prior distribution and integrating out a 2, gives the marginal 
posterior density
tt(/3)oc ( 5 > - / 3 i +  f c e - ^ ' } 2 
\i= l
mth an uniform prior for (3 in the region [—20,50] x [—2,6] (see Tanner,
102 
Approximate methods of inference

Methods based on stochastic simulation
103
Figure 3.3 Histogram approximation to the posterior density (solid line) based on 
10000 draws from q (dashed line) in Example 3.6.
1996, p. 170, for details). The prior is also used as a proposal distribu­
tion and a weighted resampling (SIR) algorithm generated 10000 draws 
from the posterior distribution ir(/3). Based on data x — (1,2,3,4,5,7) and 
y =  (8.3,10.3,19,16,15.6,19.8), approximations for the posterior mean, 
standard deviation and 95% credibility interval of /3i are 18.10, 6.043 and 
(9.13,33.68), respectively. Similarly, approximations for the posterior mean, 
standard deviation and 95% credibility interval of fe  are 1.78, 1.66 and 
(0.143,5.781), respectively. The resulting approximation is detailed enough 
to pick a minor secondary mode as shown in Figure 3.4- The effect of the 
number of days can also be observed from the figure.
These procedures can also be subject to an adaptive iterative scheme 
where original samples are initially formed with the prior. If this gives 
indications of inadequacy, the original sampling density is replaced by an­
other one that places more weight in the relevant posterior regions. Smith 
and Gelfand (1992) and Liang (2002) also consider this idea. More discus­
sion on the subject with applications to practical problems can be found 
in Stephens and Smith (1992), Mendes (1995), Tachibana (1995), Lopes, 
Moreira and Schmidt (1999), and Schmidt, Gamerman and Moreira (1999).

104
Approximate methods of inference
(«)
<b)
20 
30
tim e (d a y s )
Figure 3.4 The prior p(f3) is used as proposal distribution q{(3) and a weighted 
resampling (SIR) algorithm generated 10000 draws from the posterior distribution 
7r(/3). (a) Contours of 
along with draws (dots) from the posterior distribu­
tion; (b) observed data (o) along with posterior medians (solid lines) and 95% 
credibility intervals (dashed lines) for E{y\x), when x € [1,7].
A more thorough approach involving an iterative procedure for refining 
proposal densities is provided by the so called Population Monte Carlo 
(Cappe et al., 2004).
3.5.3 Application to dynamic models
This section is concluded with an application of sampling-based approaches 
to inference in dynamic models (Section 2.5). It was possible to obtain the 
evolution, prediction and updating equations analytically under linearity 
and normality of the disturbances. Consider now general dynamic models 
with observation and system equations given by
yt 
=  
Ft (/3t,et) where et ~  Fe 
(3.10)
/3t 
=  
G t(f3t- i ,w t) where wt ~  Fw 
(3-11)
where Ft and Fw are any (possibly non-normal) distributions. These distri­
butions may in fact depend on t and the system equation may also depend

Methods based on stochastic simulation
105
on previous /3s without invalidating the calculations below. Nevertheless, 
Equations (3.10)-(3.11) are retained for notational simplicity.
Sequential inference for a dynamic model is based on obtaining for each 
time t the prior, predictive and updated distribution for the system param­
eter (3t. The first two distributions are
p(A |yt_1) 
=  J  p (A |/3 t-iM A -i|y ‘- 1 )d A -i, 
(3.12)
/(yt|yt-1 ) 
=  J f{yt\Pt)p{Pt\yt~ l)d(it 
(3.13)
and the last one is obtained via Bayes’ theorem as
piPtly*) oc /(y tl& M A ly 4-1). 
(3.14)
None of these operations can be completely performed analytically for the 
generality assumed for the model and an alternative is to use a sampling- 
based approach to perform them approximately. A sequential simulation 
procedure follows the steps below:
1. Assume PqX\ ■.. ,/?o"°^ is a sample of po and set the time counter t =  1.
2. Evolution: obtain a sample 
from p(f3t\yt~ 1) by draw­
ing 
from Fw independently and setting /3*^ =
j  =  1,... ,n t-i-
3. Prediction: obtain a sample y^\ ... ,y[nL~^ from /(j/t|yt_1) by draw­
ing 
from Fe independently and setting 
=  Ft(/3t*^\ e ^ ), j  =  
1) ■ • • i nt~\.
4. Updating: obtain a sample 
. . . ,  
from p(f3t \yl) using any of the 
sampling-based versions of Bayes’ theorem to (3.14).
5. Increase t to t +  1 and return to 2 until all time points have been pro­
cessed.
The generations in 2 and 3 follow respectively from (3.12) and (3.13) 
using results about sampling from mixtures (Section 1.3). Generations in
4 can be made via rejection (Muller, 1991a) or weighted resampling (Gor­
don, Salmond and Smith, 1993; Pitt and Shephard, 1999), to name a few 
options. These methods are nowadays known as particle filters (Kitagawa, 
1996) and belong to the class of sequential Monte Carlo methods. For all 
methods there are problems associated with treatment of conflict between 
prior and likelihood. This conflict occurs with outlying observations that 
can indicate either an occasional or structural change in the pattern of 
the series. The practical effect of this conflict is the replication of only a 
few significant particles, which is also known as sample impoverishment, 
and the reduction of the effective sample size. The above papers discuss 
possible treatments of these problems with the particle filter of Pitt and

106
Approximate methods of inference
Shephard (1999) being notoriously better for using f(yt\Pt) as part of the 
weighing function. The methods are somewhat restricted by not evaluating 
smoothing distributions and requiring draws from Ft and Fw. In general, 
these distributions depend on unknown hyperparameters such as variance 
terms. Sampling-based approaches taking all unknown quantities into con­
sideration will be presented in Chapters 5 and 6.
Liu and West (2001), Storvik (2002) and Stroud, Poison and Muller 
(2005) represent a few recent attempts to adapt particle filters to deal 
with unknown hyperparameters. Doucet, Godsill and Andrieu (2000) give 
a detailed overview of several variants of and improvements on the boot­
strap filter of Gordon, Salmond and Smith (1993). Doucet, de Freitas and 
Gordon (2001) collect a broad range of practical applications of sequen­
tial Monte Carlo techniques. Lopes and Migon (2002), Lopes and Carvalho 
(2005), Carvalho and Lopes (2005) and Abanto, Lopes and Migon (2005) 
apply sequential Monte Carlo algorithms to several multivariate stochas­
tic volatility models. Migon et al. (2005) review sequential Monte Carlo 
methods for dynamic models.
E xam ple 3.8 A total of n =  50 observations are simulated from a first 
order model (see example 2.9), with o f =  0.252 and W t =  0.12, for all t, 
and /?o ~  N (m o,Co) for mo =  0 and Co =  100. The level of proximity 
of the sequential Monte Carlo filters can be easily assessed since (/3t\yl) ~  
N (m t,C t) where m t and C t are given in Equation (2.25).
The bootstrap filter was implemented for ct2 and W t known. It performs 
rather well. As an illustration, Figure 3.5 presents the true posterior mean 
and credibility intervals along with the respective approximation based on 
the bootstrap filter with varying number of particles for a sub-sample of the 
time span of the series.
In order to see how sensitive the bootstrap filter is, we have forced y-27 
to be an outlier by replacing its true value —0.473 by 1.00. As it can be 
seen, the particle filter performs considerably well until right before t =  27, 
looses accuracy for the next few observations and resumes its good perfor­
mance around observation t =  32. As expected, the sample impoverishment 
is decreasingly apparent when the number of particles is increased.
3.6 
E xercises
3.1 
Consider a posterior for A in the conditions of Example 3.1 given by a 
G (ai,j3i) distribution. Define the transformations (p =  log A and ip =  VA.
(a) Show by direct likelihood calculation and by exploring transformations 
on Fisher information that the Jeffreys non-informative prior for ip is 
constant.
(b) Obtain the posterior distributions of <p and ip and plot them for the 
values used in Figure 3.1.

Exercises
107
I-----1--1--1-----1---------1----------- 1
-
1
-
1
-
J---1
22 
23 24 25 
26 
27 28 29 30 
31 
32
( C )
(b)
22 
23 
24 
25 
26 
27 
28 
29 
30 
31 
32
<d)
22 
23 
24 
25 
26 
27 
28 
29 
30 
31
Jlme
22 
23 
24 
25 
26 
27 
28 
29 
30 
31 
32
Figure 3.5 Posterior means mt (squares) and 95% credibility intervals m t ±  
l.96\/Ct (solid lines) along with approximations based on the sequential Monte 
Carlo (SMC) filter (circles and dotted lines) with nt =  n particles, for all t: (a) 
n =  100, (b) n =  500, (c) n =  1000 and (d) n =  10000 particles.
(c) Obtain the normal approximations to the posterior distributions of cp 
and %p.
(d) Obtain the expressions for the non-normal posterior densities of X 
derived from the normal approximations to the posterior distributions of 
(p and ip.
(e) Obtain the Lindley approximation for the posterior mean of (p and ip.
3.2 The delta method is a well known approximation technique based on 
first order Taylor series expansions that finds applications in both frequen- 
tist and Bayesian inference. It states that if a random vector x  has vector 
mean a and variance matrix B and t(x) is a differentiable function then 
t(x) has approximate vector mean t(a) and variance matrix
(dt(a) / d x)1 B(dt(a)/dx).

If, in addition, x is normal then t(x) will also be approximately normal.
(a) Use Taylor series expansions up to the linear term to prove the delta 
method.
(b) Show that application of the delta method to a transformation ip =  
t{0) with the normal approximation for the posterior of 9 discarding 
the prior influence coincides with the direct application of the normal 
approximation for the posterior of ip discarding the prior influence.
3.3 (Lindley, 1980) Show that the standard Laplace approximation for the 
posterior mean of ip =  t(6) is given by
d 
d
108 
Approximate methods of inference
E(ip) =  t(m ) -  i2 2=1 J=1
d2t(m ) 
d3log7r(m)
l ^ Z ^ Vkl
dOiddj 
d9i 
d0jd6kd6i
Show also that if 9i is the I th component of 9 then
l ^ ^ v ^ d 3 l°g 7r(m)
( i)=m + j
E
E
E
 d0jd9kd9i zj hl '
i=i j =i jt= i 
j 
K 
‘
3.4 Go through the Fisher scoring algorithm to evaluate the maximum like­
lihood estimator of (3 in a generalized linear model as described for example 
in McCullagh and Nelder (1988) and show that it coincides with the evalu­
ation of the posterior mode when a constant prior is used for (3, by altering 
the relevant steps in Example 3.2.
3.5 Show that in the one-way classification model with a two-stage hier­
archical prior described in Example 3-4, the full conditional distributions 
are:
(a) 7r(/3j|/3_ i,/i,0 i , 02) =  n((3i\n,(pi) =  N (fa,C i) where fa =  Ci{ni(piyi +  
02/i) and Ci =  (Ui<pi +  (pz)~l ,i — l ,... ,d .
(b) 7r(/i|/3,0i,02) =  tt(/x|f},cp2) =  N (b i,B i) where bi =  B i(B Q 1bo + 4 ‘20), 
B\ =  (B q 1 +  ncp2)~ l and n =  EjTij.
(c) tt(<Pi\(3, n,<p2) =  tt(0i 1/3) =  G{n*a/2,n%S*a/2) wheren*a =  na +  J2i= i ni
and naSa =  n^S^ +  
fti) .
(d) 7r(02|/?,M>0i) =  
=  G(n*/2,n*S*/2) where n* =  nc +  d and
n*S* =  nTST +  Ej(/3i -  /t)2.
3.6 Let 9 = (<p,ip) where (p and ip are d-dimensional and r-dimensional 
parameter vectors and 9 has density ir.
(a) Obtain the normalization constants of the Laplace approximation for 
the marginal densities of (p and ip.
(b) Let 7t(9) =  NGd{[i,cr2,n ,S ) and, therefore, r =  1. Show that the 
Laplace approximations for the marginal densities of <p and ip coincide 
with the exact Student’s t and Gamma densities.

3.7 (Achcar and Smith, 1989) Consider Example 3.5 from Section 3.2.
(a) Deduce the expressions of E(6) and E(0).
(b) Using the logistic transformation <j> =  logit(9), obtain another approx­
imation for E(ff) using the fully exponential form of Laplace approxi­
mation and compare it with the approximations obtained for n =  5 and 
x =  3.
3.8 Consider two independent binomial observations Xi\9i ~  bin(ni,0i), 
i =  1,2, with non-informative prior
P (eu o2) oc e ~ l/\ i  -  0 i ) - 1/20 j 1/2(i -  e 2) ~ 112
and define 
=  9\j92.
(a) Show that the posterior mean of ip is E(ip) =  712(2:1 +  l/2 )/[(n i +  
1)(x2 - 1 / 2 ) ] .
(b) Using the exponential form Laplace approximation, show that
(m -  l ) " i + 1/ 2(n2 -  l f 2+1/ 2(^i +  l / 2 ) V l+ 1(y2 -  3/2 )y2~ 1 
W  
(m)n'+3/ 2(n2 - 2 ) " > - 1/2(j,1 _i/2)»i(j/2- i / 2 ) w  
'
(c) Using the parametrization A j =  arc sin(y/6i), i =  1,2, obtain the 
approximation
. (2ni)(2ni+1>/2(2n2)(2n2+1)/2(2yi +  2 )^ +1(2y2 -  2)*'2~ 1 
W  “  
(2m +  2)(2ni+3)/2(2n2 -  2)(2^ - i ) / 2(2yi)yi(2'(/2)2/2 
‘
(d) Using now the parametrization fa =  logit(0i), i =  1,2, obtain the 
approximation
(m +  i)»i + 1/ 2(n2 +  i)»»+i/2(yi +  3 / 2 ) V l+ 1(y2 -  l / 2 ) V2~ l 
[ } 
(ni +  2) "i+ 3/ 2(n2)ra2- i / 2(2;i +  1 /2 ) ^ (y2 +  1 /2 )^  
'
(e) Consider a small set of values for x\ ,x2,n i and n2 and evaluate the 
percentage errors of the approximations above.
3.9 Consider the integral I  =  J^°oog(9)d9 where g(8) is well approximated 
by h(6)fn(9\m ,V ). Show that the adjustment to the Gauss-Hermite rules 
to account for a non-standard normal distribution gives the approximation 
I — Y,iqih(9i) where
Qi =  Wi\/(2V)e~0i 
and 0i =  rn +  
(2V )xi ,
for weights Wi and zeros x it i =  1
,n.
3.10 Show that if 9 is scalar, application of different 1-point Gauss-Hermite 
quadrature rules (w\ =  \J(2n) and x\ =  0) to the numerator and denomi­
nator in (3.5) gives the exponential form Laplace approximation (3.6).
Exercises 
109

3.11 Generate a sample x =  (x i,. . . ,  x „ )  of size n =  50 from the N (n, a2) 
distribution where p, ~  N (p q ,t2) and a2 ~  IG(no/2, So/2) are indepen­
dent. (You have the freedom of choice for the values of the hyperparameters
H, r 2, no and So-)
(a) Obtain the posterior density ir for (p,(j2), for (n,cr~2) and for (/i,log<r2).
(b) Obtain the posterior mode of Tr(p, a 2) using Newton-Raphson and con­
ditional modes iterations.
(c) Calculate E (fi|x) and E (a 2|x) using the approximating methods below: 
cl. Normal approximation;
c2. Standard and exponential form Laplace approximation; 
c3. Gaussian quadrature.
(d) Repeat (c) basing your calculations on a reparametrization h(a2).
(e) Compare the methods about computational aspects and the numeric 
results obtained and discuss an appropriate choice for the transformation 
h.
3.12 Consider the problem of estimating I  =  f  t(9)Tr(9)d8 by
j _ 1 
t(9i)ir(9j)
~ n h i  
^ i )  
where d \ ,...,6 n is a sample from q. Show that
(a) Eq(I) =  I, where Eq denotes expectation with respect to q;
(b) Vq(I) — a 2/n where a 2 =  f [ t 2(9)-n2(9)/q(9)\d9 — I 2;
(c) the optimal choice for q in terms of minimizing a2 is q cut x -k.
3.13 Show that the Monte Carlo ratio estimator of E[t(8)\ =  I2/I1 given 
by
f =  h  =
is asymptotically unbiased and a strongly consistent estimator of I2/11 . 
Also, derive the approximation
V a r(I)=  Var^ —  -  2^ C o v ( i 2,h )  +  ^ V a r (h )
using a Taylor series expansion of I ^ 1 around I\ and propose an estimator 
for Var(I).
3.14 Assume a sample 9\ ,...,6n from 7r is available. Show that the esti­
mators below are unbiased and strongly consistent estimators of their cor­
responding posterior quantities and evaluate the sampling variance of these 
estimators:
110 
Approximate methods of inference

Exercises
111
(a) Quantity of interest: E{6), estimator: 9 — (1/n) XT=i
(b) Quantity of interest: E(<f>) where <p =  t{9), estimator: </>= ( l / n ) ^ ”=10j 
where 4>i =  t(6i), i =  1,... ,n.
(c) Quantity of interest: p =  Pr(9 G  C), estimator: p =  (1/n) ^ ”=1I(9i G
C).


C H A P T E R  4
Markov chains
4.1 Introduction
The presentation of Markov chains in this chapter is far from compre­
hensive. It tries to describe the main results by combining intuition with 
probabilistic reasoning. Its presence in this book is an attempt to provide 
in simple terms the theory governing the iterative simulation techniques 
used in later chapters. There are many excellent books on or including 
a detailed and formal treatment of the subject. Among them, the books 
by Feller (1968), Meyn and Tweedie (1993), Nummelin (1984) and Ross
(1996) can be cited. A more statistically oriented approach is given in Gut- 
torp (1995) and a more thorough treatment is given in Revuz (1975).
Markov dependence is a concept attributed to the Russian mathemati­
cian Andrei Andreivich Markov that at the start of the 20th century in­
vestigated the alternance of vowels and consonants in the poem Onegin by 
Poeshkin. He developed a probabilistic model where successive results de­
pended on all their predecessors only through the immediate predecessor. 
The model allowed him to obtain good estimates of the relative frequency 
of vowels in the poem. Almost at the same time the French mathematician 
Henri Poincare studied sequences of random variables that were in fact 
Markov chains.
A Markov chain is a special type of stochastic process, which deals with 
characterization of sequences of random variables. Special interest is paid 
to the dynamic and the limiting behaviors of the sequence. A stochastic 
process can be defined as a collection of random quantities {0 ^  : t £ T } 
for some set T.
The set { 0 «  : t £ T }, is said to be a stochastic process with state space
S and index (or parameter) set T. Throughout this book the index set T is 
taken as countable, defining a discrete time stochastic process. Without loss 
of generality, it will be assumed to be the set of natural numbers N  and will 
mostly represent the iterations of a simulation scheme. The state space will 
in general be a subset of Rd representing the support of a parameter vector. 
For presentation purposes, it will be assumed to be discrete for the first 
sections of this chapter. Most important results for general Markov chains 
can be obtained with a discrete state space. Limiting results concerning 
ergodic and central limit theorems are especially relevant in this book. 
Later sections consider general state spaces. After the ground work has

114
Markov chains
been done, connections with iterative simulation schemes are outlined and 
illustrated for one such specific scheme.
4.2 
Definition and transition probabilities
In simple terms, a Markov chain is a stochastic process where given the 
present state, past and future states are independent. This property can 
be more formally stated through
Pr(0(n+1) e A \ 8 ^  =  x,0("~1) g A „ _ i , . . . , 0 (o) € j40)
=  Pr(0(n+1) £ A\9{n) =  x) 
(4.1)
for all sets A 0, . . . ,  A n- i , A  C  S and x G S. The Markovian property (4.1) 
can also be established in the equivalent forms:
1. £[ / ( 0(n))
=  £ [ /( 0 (n))|0(m)] for all bounded func­
tions /  and n >  m > 0;
2. Pr(0(”+ 1) =  y|0(") =  x,9^n~ ^  =  x „ _ i , . . . ,  0(o) =  x0) =  Pr(0("+ 1) =  
t/|0(") =  x) for all x o ,. . . ,  xra- i ,  x , y  £ S.
The above form is clearly appropriate only for discrete state spaces. In 
fact, it is more appropriate than (4.1) in this case and is used as defining 
Markov chains for the initial sections of this chapter. In general, the prob­
abilities in (4.1) depend on x, A and n. When they do not depend on n, 
the chain is said to be homogeneous. In this case, a transition function or 
kernel P(x, A) can be defined as:
1. for all x £ S, P(x, ■) is a probability distribution over S;
2. for all A C S, the function x 
P( x , A) can be evaluated.
It is also useful when dealing with discrete state space to identify P ( x , {j/}) 
=  P(x,y). This function is called a transition probability and satisfies:
• P {x ,y ) > 0, Vx,y e 5;
• 
p (x>y) = 1, Vx G 5;
as any probability distribution P(x, ■) should.
Example 4.1 Random, walk
Consider a particle moving independently left and right on the line with 
successive displacements from its current position governed by a probability 
function f  over the integers and 9 ^  representing its position at instant n, 
n G N . Initially, 9i(y> is distributed according to some distribution 
. The 
positions can be related as
Q{n) _  Q(n-1) _|_ Wn _  Wi _|_ W2 _|_ _ _ +  Wn
where the Wi are independent random variables with probability function f . 
So, {0<n> : n £ N } is a Markov chain in Z.

Definition and transition probabilities
115
The position of the chain at instant t =  n is described probabilistically 
by the distribution of wi +  ... +  wn.
/ / / ( I )  =  p, / ( —l) =  q and/(0 ) =  r with p +  q +  r =  1 then the transition 
probabilities are given by
{
p 
, if y =  x +  1 
5 
, ify  =  x - l
r 
, if y =  x
0 
, i f y = £ x - l , x , x  +  l
Example 4.2 Branching processes
Consider particles that can generate new particles of the same type. Each 
of the x particles at generation n gives birth independently to identically 
distributed numbers of descendants £i, i =  l , . . . , x  with discrete distribu­
tion F  and dies. If 0(n'> represents the number of particles at generation n 
then it is a Markov chain with state space S =  { 0 , 1 ,2 ,... }  and transition 
probabilities
P(x, y) =  P r 
& =  y j  .
Note that P (0, 0) =  1 and once state 0 is reached, the chain does not leave 
it. Such states are called absorbing states.
Example 4.3 Ehrenfest model
Consider a total of r balls distributed in two urns with x balls in the first 
urn and r — x in the second urn. Take one of the r balls at random and put 
it in the other urn. Repeat the random selection process independently and 
indefinitely. This procedure was used by Ehrenfest to model the exchange 
of molecules between two containers. If 
represents the number of balls 
in the first urn after n exchanges then { X ^  : n G iV} is a Markov chain 
with state space S =  { 0 , 1,2,..., r} and transition probabilities
{
x/d 
, ify  =  x  +  1 
1 -  x/d 
, if y =  x  -  1 
.
0 
, if \ y - x \ ^ l
Example 4.4 Birth and death processes
Consider a Markov chain that from the state x can only move in the next 
step to one of the neighboring states x — l, representing a death, x or x +  1, 
representing a birth. The transition probabilities are given by
' Px 
, if y =  x +  1
=  J 
■% y = x -
1 
.
rx 
, it y =  x
0 
, if \y -  x\ >  1
where px, qx and rx are non-negative with px +  qx +  rx =  1 and qo =  0. 
The Ehrenfest model is a special case of birth and death processes.

116
Markov chains
In the case of discrete state spaces S =  { x i , x 2, ...}, a transition matrix 
P  with (i, j)th  element given by P(xi,xj)  can be defined. If S is finite with 
r elements, the transition matrix P  is given by
/  P (x i , X i )  . . .  
P (x x,x r) 
\
P =  
: 
: 
.
\  P (xr)x i )  . . .  
P{xr,x r ) J
Transition matrices have all lines summing to one. Such matrices are called 
stochastic and have a few interesting properties. For instance, at least one 
eigenvalue of a stochastic matrix equals one and the product of stochastic 
matrices always produces a stochastic matrix. Of course, countable state 
spaces will lead to an infinite number of eigenvalues.
Transition probabilities from state x  to state y over m steps, denoted 
by P m(x,y), is given by the probability of a chain moving from state x  to 
state y in exactly m steps. It can be obtained for m >  2 as
P m(x,y) =  Pr(<9(m) =  y\6(0) =  x)
=  
Pr(8im) = y,0{m~1) = x m- 1, . . . , 0 {1) = a r 1 |0<°> =  x)
=  
Y , ''' Y I  P r (&{m) =  y\0(m~1] =  Z m -i) • ■ ■ Pr(0{1) =  xi|0(o) -  x)
Xl 
X m - 1
=  
P (x >x l)p (x l ’ x 2 ) ' " p (xm -l,y )
Xm— 1
where the second equality is due to the Markovian property of the process. 
The last equality means that the matrix containing elements P m(x,y) is 
also a stochastic matrix and is given by P m obtained by the matrix product 
of the transition matrix P  m times. Also, for completeness, P 1(x,y) =  
P (x ,y ) and P °(x,y) =  I(x =  y). The above derivation can be used to 
established that
p n+m(x ,y ) =  Y , P r { o (n+m) =  y\e{n) =  z , e {0) =  x ) P r { e (n) =  z\e(0) =  x)
z
=  J 2 p n (x , z ) P m(z>y)- 
(4.2)
z
Equations (4.2) are usually called Chapman-Kolmogorov equations. All 
summations are with respect to the elements of the state space S and results 
are valid for any stage of the chain due to the assumed homogeneity. Higher 
transition matrices can be formed with these higher transition probabilities 
and it can be shown that they satisfy the relation p n+m =  P n P rn and, in 
particular, P n+! =  P nP.
The marginal distribution of the nth stage can be defined by the row 
vector 7r(") with components 7r^l){xi), for all x t 6 S. For finite state spaces,

this is a r-dimensional vector
ttW =  (Tv^n\ xx), • • •, TT^n\ x r)).
When n — 0, this is the initial distribution of the chain. Then, 
v W (y ) 
=  
Pr(9{n)= y )
=  
Y I  P r {0(n) =  y\°{0) =  x)Pr(0w  =  x)
X(zS
=  
^ P n(x,y)7r(0)(a;) .
X(zS
The above equation can be written in matrix notation as 
=  7r(-0*Pn. 
Also, since the same is valid for n — 1, 
=  n(0')pn~1p  =  ^(n~ l)p.
The probability of any event A C  S for a Markov chain starting at x is 
denoted by P rx (A). The hitting time of A is defined as Ta =  min{n > 1 : 
Q(n) £ _4J. jf Q(n) £ a  for some n > 0. Otherwise, Ta — oo. If A — {a}, the 
notation 7{aj =  Ta is used.
E xam ple 4.5 Consider {#(” ) : n > 0}, a Markov chain i t i S = { 0 , l }  with 
initial distribution i g i v e n  by 
=  (7r^°^(0),7r^°^(l)) and transition
matrix P  given by P  =
Using the relation P r(9(-n'1 =  0) =  J2jes P r (0 ^  =  0,6^n~1'1 — j) for 
n >  1 and the Markovian property of the chain,
Pr(9{n) = 0 ) 
=  
(1 -  p)Pr{9(n~ l) =  0) +  qPr(9in- 1'> =  1)
n — 1
=  
( l - p - g ) V 0) ( 0 ) + ^ ( i - p - 5f
k=0
and P r { 9 ^  =  1) =  1 — P r (0 ^  = 0 ) .  If p =  q =  0, the chain never moves 
with probability one and P r ( 9 =  0) =  7To(0) and P r (9 ^  =  1) =  7To(l). 
Ifp  +  q > 0,
p r(e <») =  0) =  ^
 +  (1 -  p -  , r  
( 0 ) - ^ )
and P r ( 9 ^  =  1) =  1 — P r ( 9 ^  =  0). Note that if 7r(°) =  (q,p)/(p +  q), 
then P r (9 ^  =  0) =  q/(p +  q), for all n, and the distributions at all steps 
are the same as the initial distribution.
If, 
in addition, p +  q < 2 then
lim Pr(9{n) =  0) =  —'—  and lim Pr(9(n) 
=  1) =  —
n—too 
p -f q 
n—>oo 
p 
q
and the initial distribution is obtained 
now as a limiting distribution of
the chain. The value of the higher transition matrix _Pn can be found via 
p r (9(n) _  o) =  7r(n)(0) =  J2X P n( x ,0 ) n ^ (x ) , for all x. Taking 7r(°^(0) =  1
Definition and transition probabilities 
117
(
l - p  
p 
\  
\ 
q 
l ~ q  )

118
Markov chains
gives
P «(0 ,0 ) =  - ^ -  +  ( l - p - 9)n- P
p +  q 
p + q
Analogously {or P n(0,1), P "(1 ,0 ) and P ” ( l , l )  gives
pn =  J
_ (  <1 
+  
(  
P 
~ P )  
(43)
P + q \ q p J 
p + q 
\ -q 
q J
Finally Pro(Tx =  n), x =  0,1 can be found by Pro (To =  n) =  P r o (9 ^  =
0,6^') ^  0, 1 < j  < n -  1) =  P (1 ,0 )P (1 ,1 )” - 2P (1,0) =  q( 1 -  q)n~2p. 
Analogously, Pro(T\ =  n) =  p( 1 — p )"- 1 .
It is important to distinguish between P ” (0 ,0) =  P r o (9 ^  =  0), the 
probability of the chain, starting from state 0, hitting state 0 in n steps, and 
Pro(To =  n) =  P r0( 9 ^  =  0, (&'> ^  0, j  =  1 ,2 ,..., n — 1), the probability 
of the chain, starting from state 0, hitting state 0 in n steps for the first 
time.
Markov chains can be extended in many directions. One extension used 
in other chapters of this book is provided by Markov random fields (MRF). 
They arise when the elements of the index set T  are not ordered. More 
formally, a collection { 0 ^ , 9^2\ . . . }  is a MRF if the full conditional distri­
butions of 0 ^  depend only on 9 ^  for j  G  N t , the set of neighbors of i,
i 
=  1, 2 ,... In the special case T  =  TV, 
=  {i — 1, i -f 1}, for i =  1,2,..., 
and a Markov chain is obtained (see Exercise 4.6).
4.3 Decomposition of the state space
A few quantities of interest are important in the classification of states of 
a Markov chain with state space S and transition matrix P:
(i) The probability of the chain starting from state x hitting state y at any 
posterior step is pxy =  P rx(Ty <  oc);
(ii) The number of visits of a chain to a state y is
OO
N(y) =  # {n  >  0 : 0<"> =  y} =  £
 J(0<"> =  y) .
n= 1
It can be shown that E(Ty \ 9 ^  =  x) =  Yl^Lo P rx{Ty > n) and E(N(y) \
0(0) = x )  = J 2 n =1 P n(x,y)-
A state y £ S is said to be recurrent if the Markov chain, starting in y, 
returns to y with probability 1 (pyy =  1) and is said to be transient if it 
has positive probability of not returning to y (pyy <  1). An absorbing state 
y € S is recurrent because P ry(Ty =  1) =  P ry(9 ^  =  y) =  P ( y , y ) =  1 
and therefore pyy =  1. If a Markov chain starts at a recurrent state y, the 
hitting (or return, in this case) time of y, Ty, is a finite random quantity 
whose mean fiy can be evaluated. If this mean is finite, the state y is said

Decomposition of the state space
119
to be positive recurrent and otherwise the state is said to be null recurrent. 
Positive recurrence is a very important property for establishing limiting 
results, as will be seen in the next section.
An important result describing analytically the difference between a re­
current and a transient state is that
• if y € S' is a transient state then, for 
all x £ S,
Prx (N(y) < oo) =  1 and E[N(y) 
| 6>(0) =  x] =  
p*v <  oo.
1 
Pyy
• if y G 5  is a recurrent state then
Pry(N(y) =  oo) =  1 and E[N(y) \ 9^  =  y\ =  oo.
So, recurrent states are infinitely often (i.o.) visited with probability one. 
The expected number of visits is finite if the state is transient.
It is interesting to investigate possible decompositions of S in subsets 
of recurrent and transient states. From this decomposition, probabilities of 
the chain hitting a given set of states can be evaluated. For states x and 
y
in S, x /  y, x is said to hit y, denoted x —> y, if pxy 
> 0. A set C C 
S is
said to be closed if
Pxy =  0 for x G C and y ^ C.
In obvious nomenclature, it is said to be irreducible if x —► y for every 
pair x, y E C . A chain is said to be irreducible if S is irreducible. It is not 
difficult to show that the condition pxy > 0 is equivalent to P n(x,y) >  0 
for some n > 0. This can be used to show that if x £ S is recurrent and 
x —» y then y is also recurrent. In this case, y —> x and one can write x <-> y 
when x —> y and y —> x. In other words, recurrence defines an equivalence 
class with respect to the <-> operation. Also, pxy =  pyx =  1. In fact, a 
stronger result is valid: null recurrence and positive recurrence also define 
equivalence classes (Guttorp, 1995). If C  C S’ is a closed, finite, irreducible 
set of states then all states of C are recurrent.
E xam ple 4.6 Consider the transition matrices over S =  { 0 , 1 , . . . ,  r} be­
low. For each one of them, S can be decomposed into Sr and St , the sets of 
recurrent and transient states respectively. The symbols +  and — associated 
with the pair (x, y) denote x —> y and x ■/* y, respectively.
(  1/2
1/2
°  
\
+
+
+
a)
P =  
1/2
1/4
1/4 
=
*  
+
+
+
V 0
1/3
2/3 )
v +
+
+
Therefore, S is closed and the chain is irreducible. To show that, for in­
stance, 0 —> 2, it suffices to verify that P 2(0,2) >  0. So, S =  Sr .

120
Markov chains
(  1
0
0
0
0 
0 \
/+
1/4
1/2
1/4
0
0 
0
+
+
+  
+
+
+
b )p  =
0
1/5
2/5
1/5
0 
1/5
—V
+
+
+  
+
+
+
0
0
0
1/6
1/3 
1/2
-7*
-
-
-  
+
+
+
0
0
0
1/2
0 
1/2
-
-
-  
+
+
+
\ °
0
0
1/4
CO
o
\ -
—
-  
+
+
+  )
In this case, Sr = {0 } U {3,4 ,5 } and St
= {1,2}.
If the set of recurrent states Sr is not empty then it can be written 
as a (finite or countable) union of disjoint, closed and irreducible sets. 
Whenever the chain hits a closed, irreducible set C of recurrent states, it 
stays in C  forever and with probability one it visits all its elements i.o. It is 
interesting to compute pxy for x  transient and y recurrent. As y belongs to 
an irreducible set C, pxy =  pc(x) =  P r x {Tc <  oc) is called the absortion 
probability. If the chain starts at x  e St , it can hit C  in the first step or 
remain in St in the first step and hit C at a later step. So,
Pc(x) =  
^ 2  P (x ,y )p c (y), 
x € ST- 
(4.4)
y€C 
y€Sr
The uniqueness of solutions of this system is guaranteed if P rx(TsR <  oo) 
=  1, for x G St - In the case of a finite St it is automatically valid as 
transient states are only finitely visited.
E xam ple 4.6 (continued) Let S r =  C\ U C2 where C\ =  {0 } and C2 — 
{3,4,5}. The absortion probabilities pio =  pc\ (1) and p2o =  pch (2) are 
obtained from Equation (4-4) as
1 
1 
1 
^ 
1 
2
Pio =  -  +  - p io +  ~P20 and p2o =  
0 +  - p io +  -zP20 ■
4 
2 
4 
5 
5
Solving for pio and p2Q gives pio =  3/5 
and p2o =  1/5.
Proceeding analogously for C2 gives p c2( 1) =  2/5 and p c2(2) =  4/5. As 
St is finite, the absorption probabilities p c2 (x ) can be evaluated for x  e St 
through Y .iP cA x) =  1.
E xam ple 4.4 (continued) Irreducible chains are obtained when px >  0 
for x > 0 and qx > 0 for x > 0. It is possible to determine if a state y 
is recurrent or transient even for an infinite state space by studying the 
convergence of the series 
ly where
( 
1 if y =  0
7y =  <^ 
i f y > 0  
■
I  
Pl-- Py 
J ^
If the sum diverges, the chain is recurrent. Otherwise, the chain is transient.

If S is finite and 0 is an absorbing state, the absortion probability is
sr^d— 1
p{0}(z) =  Px0 =  
, 
x =  1,... ,d — 1.
Z^y=07y
Details of these calculations are given, for example, in Hoel, Port and Stone 
(1972).
4.4 Stationary d istributions
A fundamental problem for Markov chains in the context of simulation is 
the study of the asymptotic behavior of the chain as the number of steps 
or iterations n —> oo. A key concept is that of a stationary distribution 
7r. A distribution 7r is said to be a stationary distribution of a chain with 
transition probabilities P (x, y) if
Y ^ { x ) P { x , y )  =  ir(y), 
Vy e S. 
(4.5)
x&S
Equation (4.5) can be written in matrix notation as 7r =  irP. The reason 
of the name is clear from the above equation. If the marginal distribution 
at any given step n is tt then the distribution at the next step is ttP  =  w. 
Once the chain reaches a stage where 7r is the distribution of the chain, 
the chain retains this distribution for all subsequent stages. This distribu­
tion is also known as the invariant or equilibrium distribution for similar 
interpretations.
It will be shown below that if the stationary distribution 7r exists and 
limn^oo P n(x,y) =  7r(y) then, independently of the initial distribution of 
the chain, 
will approach n, as n —> oo. In this sense, the distribution 
is also referred to as the limiting distribution.
E xam ple 4.5 (continued) The stationary distribution 7r is the solution of 
the system irP =  7r that gives equations
tt(0)P(0, y) +  tt(1)P(1, y) =  n (y) , 
y =  0,1.
The solution is ir =  (q,p)/(p +  q), a distribution that was shown to be 
invariant for the stages of the chain.
Also, provided p +  q < 2, lim ^ oo P n =  
^ ^ 
^ ^ from (4-3) and
the distribution of 9<'n') converges to 7r at an exponential rate.
The case p +  q =  2 still produces a stationary distribution it but this 
does not provide a unique limiting distribution as P r ( 9 ^  =  0) =  (1/2) +  
(—l ) ” [7r(°) — (1/2)], for all n. This case is somehow different because the 
states are always alternating through the stages. It has a periodic nature 
that will be addressed in the next section.
Stationary distributions 
121

122
Markov chains
E xam ple 4.7 Gibbs sampler (Geman and Geman, 1984)
This example provides a very simple special case of the Gibbs sampler 
and was considered by Casella and George (1992). The complete form of 
the Gibbs sampler will be left for the next chapter. In this special case, the 
state space is S =  {0, l } 2 and define a probability distribution ir over S as
#2
9i
0
1
0
7T00
7T01
1
7T10
7Tn
The probability vector -k contains the above probabilities in any fixed order, 
say (7roo,7Toi,7rio,7rii).
The chain now consists of a bidimensional vector 9^  =  { 9 ^ ,  9 ^ ) -  
Although this introduces some novelties in the presentation they can easily 
be removed by considering a scalar chain ip ^  that assumes values that are 
in correspondence with the 9^  chain, e.g. i p
=  109 ^  +  9^
. This is 
always possible for discrete state spaces and from now on no distinction 
will be made between scalar and vector chains.
Consider the following transition probabilities:
• For the first component 9\, the transition probabilities are given by the 
conditional distribution tt\ of # i|#2 =  j,
7Ti(0|j) =  —
 and 7Ti(l|j) =  —
7r+j 
7r+j
where ir+j =  7Toj +  ttij, j  =  0,1.
• For the second component 92, the transition probabilities are given by 
the conditional distribution 7r2 of 9^18\ =  i,
7r2(01
and 7T2(l|i) =
7I"i+ 
^i+
where ir,+ =  itm +  itn, i =  0,1.
The overall transition probability of the chain is
P ((i,j),(k ,l)) 
=  
Pr(0<"> =  (fc,O|0(” - 1) = ( i , j ) )
=  
P r(9{2n) =  /|6>[n) -  k) Pr(0jn) =  k\9(2 ~ 1] =  j)
_ 
T^kl Kkj
7rfc+ ft+j
for (i , j ), (k,l) e S. Thus, a 4 x 4  transition matrix P  can be formed.
It is evident from the above transitions that (9^ ) n>o forms a Markov

Stationary distributions
123
chain as transition probabilities only depend on the present value (k,l) to 
predict the future value 
It is straightforward to ascertain that tt is
the stationary distribution of the chain. If all elements of tt are positive, 
it is also a limiting distribution. The interesting message is that chains 
formed by the superposition of conditional distributions have a stationary 
distribution given by the joint distribution.
The results can be extended in the same way for cases when 6\ can take 
mi values and 
can take m 2 values. They can similarly be extended to 
cases where the 8 consist of d components and each of them can take jnt 
values, i =  1 , . . . , d . The Gibbs sampler is a simulation scheme that will 
be applied in the next chapter for general (continuous, discrete and mixed) 
d-dimensional state spaces.
E xam ple 4.4 (continued) For irreducible birth and death chains, the sys­
tem of equations Y lxes ^(x)P(x, y) =  ir(y), y € S, is tt(x ) =  px-\ir(x — 
1) + r xTr(x) +  qx+\TT(x +  1) for x > 0 and 7r(0) =  ro7r(0) +  q\Tr(l) for x =  0. 
Using the fact that rx =  1 — px — qx, these equations reduce to
7t(x +  1) =  
- 7r(x), 
x >  0.
Q x + i
Defining
J 
1 
, if x =  0
TTx =  < 
PoP 1 - P . - 1  
i f x  >  l
gives that, if Y^x k x converges, the birth and death chain has stationary 
distribution
n(x) =  
-----. 
x > 0 .
Z_^y=0 7FV
In the case of an Ehrenfest model where P(x, x) =  rx =  0 for all x, 
P n(x,x) =  0 for odd n. Hence, the chain can only return to the same 
state after an even number of transitions. In this case, there is no limiting 
distribution, as will be shown below.
The existence and uniqueness of stationary distributions can be studied 
through weaker results. Let Nn(y) be the number of visits to state y in 
n steps and define Gn(x,y) =  Ex [Nn(y)], the average number of visits of 
the chain to state y and m y — Ey(Ty), the average return time to state 
y. Then, Gn(x,y) =  Y!k=\Pk{x ^y) and limn^oo Gn(x,y)/n provides the 
limiting occupation of state y in a chain observed for an infinitely long 
number of steps. It can be shown that
• If y € S is transient then lircin^oo 
=  0, with probability 1, and
limn_ 00 Gn^ ,v^ =  0 for all x € S.
• If y e S is recurrent then lim „_oo 
=  7^
<0°^, with probability 1, 
and limn^oo Gn(*’v^ =  
for all x £ S.

124
Markov chains
It follows that if 7r is a stationary distribution then ir(x) =  0, if x is 
transient or null recurrent (my =  oo), and tt(x) =  
if x is positive 
recurrent. Intuitively, the stationary probability of any state is given by 
the frequency of visits to the state. As the sets Srp and S'/in of positive 
recurrent and null recurrent states are closed if S is finite, then Sftn =  </>, 
the empty set. Therefore, an irreducible Markov chain is positive recurrent 
if and only if it possesses a stationary distribution 7r such that
Um £ ; .,/* ( * .» )  = lim g.(x.!<) = n(y)
n —+ o o  
n  
n —*oo 
ft
If the chain is null recurrent then Equation (4.5) is still valid but the 
7r(j/)s do not possess a finite sum and therefore do not constitute a proper 
distribution.
The analysis of the limiting behavior of P n is more delicate for technical 
reasons and is considered in the next section. It is clear that if y G  S t or 
y G Sftn then lim ^ oo P n(x,y) =  0, Mx G  S. If y G  Srp, it has only been 
obtained that limfwoo P n(x,y) must equal pxy/my.
4.5 L im iting theorem s
There are situations where stationary distributions are available but lim­
iting distributions are not (Example 4.5). In order to establish limiting 
results, one characterization of states still absent must be introduced. This 
is the notion of periodicity.
The period of a state x, denoted by dX) is the largest common divisor of 
the set
{n  > 1 : P n(x,x) > 0}.
It is obvious that P (x ,x ) >  0 implies that dx =  1 and that if x <-> y 
then dx =  dy. Therefore, the states of an irreducible chain have the same 
period. A state x is aperiodic if dx =  1 and if in addition the state is 
positive recurrent, the state is said to be ergodic. A chain is periodic with 
period d if all its states are periodic with period d > 1 and aperiodic if 
all its states are aperiodic. Finally, a chain is ergodic if all its states are 
ergodic.
Although aperiodicity is superfluous for the existence of an equilibrium 
distribution, it is a condition needed to establish convergence of the transi­
tion probabilities. Let (0^"^)n>o be an irreducible, positive recurrent chain 
with stationary distribution tt and ||£i — &H =  suP,4csl^i(J^) — &(^4)| be 
the total variation distance between two distributions 
and £2-
• If the chain is aperiodic then limn_+00 P ” (x,y) =  n(y) for all x, y €
S. In fact, irreducibility and ergodicity of the chain are equivalent to 
lim„_»00 ||P n(x, •) — 7r(-)|| =  0, for all x G  S (Nummelin, 1984).
• If the chain is periodic with period d then, for every x ,y  G S, there is

Limiting theorems
125
an integer r, 0 < r < d such that P n( x ,y ) =  0 unless n — md +  r for 
some m £ N  and limm_»oo P md+r(x,y) =  dn(y).
E xam ple 4.3 (continued) I f y —x is even then P 2m+1(x,y) =  0, rn > 0 and 
limm_*oo P 2m(x,y) =  2ir(y), and ify  — x  is odd then P 2m(x,y) =  0, m > 0 
and limm_ 00 P 2m+1(x,y) =  2tt(y). The transition matrix P  is periodic with 
period d =  2.
Let S =  {0,1,2,3} (r — 3). From Equation (4-5),
1 3  3 1
and the limiting distributions are obtained from
lim P n =
71— X X )
( \  
0
i  4
V 0
0 \
1 
4 
0 
!
4
for even n and
lim P n
n —► oo
/  o 
1 
4 
0 
1 
4
* \
for odd n.
Once ergodicity of the chain is established, important limiting theorems 
can be stated. The first and most important one is the ergodic theorem. 
The ergodic average of a real-valued function t(9) is the average tn =  
(1 /n) 
If the chain is ergodic and E^tid)] <  oo for the unique
limiting distribution tx then
tn a-->' En[t(9)\ as n —> oo. 
(4-6)
This result is a Markov chain equivalent of the law of large numbers (3.9). 
It states that averages of chain values also provide strongly consistent esti­
mates of parameters of the limiting distribution tt despite their dependence. 
If t{9) =  1(9 =  x) then the ergodic averages are simply counting the rela­
tive frequency of values of xs in realizations of the chain. By the ergodic 
theorem, this relative frequency converges almost surely to n(x) =  l/m x, 
the average frequency of visits to state x.
Just as there is an equivalent of the law of large numbers for Markov 
chains, there are also versions of the central limit theorem (3.8) for Markov 
chains. Many forms are available depending on further conditions on the
* T h e  average co u ld  have in clu d ed  th e term  co rre sp o n d in g  t o  th e initial step  and all 
lim itin g results w o u ld  still follow . In th e sequ el, it w ill b e a ssu m ed  th a t ch ains start 
at s te p  1.

126
Markov chains
chain. A chain is said to be geometrically ergodic if there is a constant
0 < A < 1 and a real, integrable function M (x ) such that
||P"(x,-)-7T(-)|| < M ( x ) \ n 
(4.7)
for all x € S. If the function M  does not depend on x, the ergodicity 
is uniform. Uniform ergodicity implies geometric ergodicity which implies 
ergodicity (Tierney, 1994). The smallest A satisfying (4.7) is called the rate 
of convergence. Roberts (1996) provides a brief discussion showing that this 
rate is bounded by the second largest eigenvalue of P. Of course, a geometric 
rate is desirable for a fast convergence to the limiting distribution. However, 
this speed may be offset by a very large value of M (x) which may slow down 
convergence considerably (Poison, 1996). Also, the rate of convergence can 
be arbitrarily close to 1 (Example 5.5).
Before stating central limit theorems, define the autocovariance of lag k 
(k > 0) of the chain 
=  t ( 9 ^ )  as 7 fc =  Co?;x (^n\ ^ n+fc^), the variance 
of 
as a2 =  7o, the autocorrelation of lag k as pk =  7k/c2 and r 2/n  =  
Var„(tn). It can be shown that
" - 1 „  
7, 
\
(4.8)
and that r 2 —> r 2 as n —> oo where
(4.9)
if the series of autocorrelation is summable. The term between parentheses 
in Equation (4.9) can be called inefficiency factor or integrated autocorre­
lation time (Green and Han, 1992) because it measures how far t ^ 's  are 
from being a random sample and how much Varn(tn) increases because of 
that. The inefficiency factor can be used to derive the effective sample size
n oo 
.  
( 4 .1 0 )
^  
1 +  2 Z k=iPk
which can be thought of as the size of a random sample with the same 
variance (see Liu and Chen, 1995) since Varn(tn) =  cr2/n eg-.
It is important to distinguish between a2 =  Var^ [£(#)], the variance of 
t{9) under the limiting distribution 7r and r 2, the limiting sampling variance 
of y/nt. Note that under independent sampling they are both given by a 2. 
They are both variability measures but the first one is a characteristic of 
the limiting distribution 7r whereas the second is the uncertainty of the 
averaging procedure.
If a chain is uniformly (geometrically) ergodic and t2(0) (t2+t(9)) is in-

Reversible chains
tegrable with respect to tt (for some e > 0) then
127
(4.11)
as n —> oo (Tierney, 1996). Other versions of the central limit theorem 
may be found in Tierney (1994) and the subsequent discussion by Chan 
and Geyer (1994). Just as (4.6) provides theoretical support for the use of 
ergodic averages as estimates, Equation (4.11) provides support for evalu­
ation of approximate confidence intervals. These will require estimation of 
the unknown quantity r 2, a subject that will be treated in Section 4.8.
4.6 R eversible chains
Let (0(n))„>o be an homogeneous Markov chain with transition probabili­
ties P(x, y) and stationary distribution tt. Assume that one wishes to study 
the sequence of states 6^n\ 
... in reversed order. It can be shown
that this sequence satisfies
Pr(9(n) =  y | 0(n+1) =  x, 6>(n+2) =  x 2, ■ ■ ■) =  Pr(B(n) =  y | 6»(n+1) =  x)
and therefore defines a Markov chain. The transition probabilities are
P*{x,y) 
=  
Pr(6{n) =  y | 0(n+1) =  x)
p r ( 0 ( n + l )  =  x  | Q(n) =  y ) P r { Q ( n )  =
=  
Pr(6»("+1) =  x)
_  
7TM (y)P(y,x)
7r(n+i)(;E)
and in general the chain is not homogeneous. If n —> oo or alternatively, 
7r(0) =  n, then P*(x,y) =  P*{x,y) =  n(y)P(y,x)/n{x) and the chain 
becomes homogeneous. If P*(x,y) =  P (x,y) for all x and y £ S, the time 
reversed Markov chain has the same transition probabilities as the original 
Markov chain. Markov chains with such a property are said to be reversible 
and the reversibility condition is usually written as
It can be interpreted as saying that the rate at which the system moves 
from x  to y when in equilibrium, 7r(x)P(x,y), is the same as the rate 
at which it moves from y to x, tt(y)P(y,x). For that reason, (4.12) is 
sometimes referred to as the detailed balance equation (Guttorp, 1995); 
balance because it equates the rates of moves through states and detailed 
because it does it for every possible pair of states.
E xam ple 4.3 (continued) The stationary distribution has already been ob­
tained. There are only 4 possibilities for a pair (x,y) e S:
• if \x — y\ > 2, then P (x ,y ) =  P { y , x ) =  0 and (4-12) is satisfied;
it(x)P(x, y) =  tt(y)P(y,x ), for all x , y  € S.
(4.12)

128
Markov chains
• 
if x  — y, then 
(4-12) is trivially satisfied;
• 
if x =  y — 1 then P(x,y) =  qx and P (y ,x ) =  px-\- It has also been
shown that n(x) =  tx{x — l)px-i/qx for x >  0, which again implies that 
(4-12) is satisfied;
• if x =  y +  1 then P(x,y) =  px and P (y ,x ) — qx- i ■ The same relations 
of the previous case hold and (4-12) is satisfied.
In conclusion, irreducible biHh and death chains are reversible.
Reversible chains are useful because if there is a distribution 7r satis­
fying (4.12) for an irreducible chain, then the chain is positive recurrent, 
reversible with stationary distribution n. This is easily obtained by sum­
ming over y both sides of (4.12) to give (4.5). Construction of Markov 
chains with a given stationary distribution 7r reduces to finding transition 
probabilities P (x ,y ) satisfying (4.12). This is always possible, as the next 
example shows.
E xam ple 4.8 Metropolis algorithm (Metropolis et ai, 1953)
Consider a given distribution px , x € S with ^2x px =  1 where the state 
space S can be a subset of the line or even a d-dimensional subset of R d. The 
problem posed and solved by Metropolis et al. (1953) was how to construct 
a Markov chain with stationary distribution 7r such that tt( x ) =  px, x £ S. 
Let Q be any irreducible transition matrix on S satisfying the symmetry 
condition Q(x, y) =  Q(y, x), for x ,y  £ S.
Define a Markov chain ( 0 ^ ) n>o as having transition from x to y pro­
posed according to the probabilities Q(x,y). This proposed value for 
is accepted with probability m in {l,py/px} and rejected otherwise, leaving 
the chain in state x.
The transition probabilities P (x,y) of the above chain (0t'n^)n>o are
P{x, y) 
=  
P r(0(n+1) =  y, TA\0{n) =  x)
=  
P r(0 (n+1> =  y\0W =  x)Pr(TA)
=  
Q(x,y ) min{l,py/px} 
for y 
x and T A  denotes the event [transition is accepted]. If y =  x, then 
P {x ,x )  =  Pr(6(n+1) =  x,TA\0(n) = x ) +  Pr(0{n+l) ±  x,TA\0(n) =  x) 
=  
Pr(<9(" +1) =  x\0{n) =  x)Pr(TA) +  ^
 Pr(6>(n+1) =  y, TA\6(n) =  x)
yjix
=  
Q{x,x) +  '^ 2 Q (x ,y )[l -  m m {l,p y/px}\ .
yjix
The first step to obtaining the stationary distribution of this chain is to 
prove that the probabilities px satisfy the reversibility condition. For x =  y, 
Equation (4-12) is trivially satisfied. For x  /  y, suppose first that py > px .

Continuous state spaces
129
Then
pxP (x,y) = p xQ { x , y ) =  Q(y,x) min jl, ^ jpy =  PyP(y,x).
Analogous calculations follow for the case py < px . Therefore, the chain is 
reversible and the probabilities px, x € S provide the stationary distribution 
of the chain.
If Q is aperiodic, so will be P  and the stationary distribution is also 
the limiting distribution. It is not difficult to find examples of symmetric 
transition matrices. The random walk chain (Example 4-1) with f  symmet­
ric around 0 is an example. The birth and death model with px — qx-\-\ is 
another example.
4.7 Continuous state spaces
This section considers sequences of random quantities that form a Markov 
chain in R but still retain a discrete parameter space T. Although not ex­
plored in as many textbooks as the case of discrete state spaces, it may 
be found in a few recent books (Gillespie, 1992; Medhi, 1994; Meyn and 
Tweedie, 1993). There are a few changes required with respect to the dis­
crete case but the main results of the previous sections are still valid. In 
particular, convergence to the limiting distribution, the ergodic theorem 
and the central limit theorem need basically technical changes in the con­
ditions of the chain to hold.
4-7.1 Transition kernels
Markov chains are still defined in terms of Equation (4.1). If the condi­
tional probabilities do not depend on the step n, the chain is homogeneous. 
Then the transition kernel P(x, ^4) (Section 4.2) is again used to define the 
chain. The analogy with the discrete case breaks when trying to consider 
P(x, {j/}), which is always null in the continuous case and not useful in this 
context. Therefore, transition matrices cannot be constructed and transi­
tion kernels must be used instead. However, given that P(x, ■) defines a 
probability distribution, the notation P(x, y) can be used as
P(x, y) =  Pr(8^n+1^ <  y | 9
=  x) =  Pr(9^  <  y \ 9(-°'1 =  x), for x ,y  € S,
when P  is absolutely continuous with respect to y. Also associated with 
this conditional distribution, one can obtain the conditional density
/ 
dP{x,y)
p{x,y) =  — -------, for x ,y  e 6.
ay
This density can be used to define the transition kernel of the chain instead 
of P(x, A). The state space S does not need to be the entire line. It can be 
any interval or collection of intervals for results below to hold.

The conditional transition probability over m steps is given by
P m(x, y) =  Pr(6{m+n) < y | <9(n) =  x), for x , y  € S,
the transition kernel over to steps is given by
d P m(x v )
pm(x,y) = ------^ ^ , f o r  x , y £ S
dy
and the equivalent equation to (4.2) has the form
/
OO
P m(z, y)pn(x, z)dz, 
to, n >  0.
•OO
This is the continuous version of the Chapman-Kolmogorov equations. For 
to =  1, it reduces to
pOO
_ , „ ^ 
^ , f iz, 
n > 0.
-O O
The marginal distribution at any step n has density tt*") (n > 0) that can 
be obtained from the marginal distribution at the previous step as
130 
Markov chains
/
OO
P {z,y)pn(x,z)dz
•O O
7T(n)
/
OO
p(x,y)n{n~1)(x)dx. 
(4.13)
•OO
E xam ple 4.1 (continued) Assume now that the random displacements wn 
are continuous quantities with common density f(w ) and 
is some con­
tinuous distribution. As in the discrete case,
0( n + l )  _  g (n )  +  W n + l  =  W l  _|_ _  _ _|_ W n + 1 .
Note that this is the model used for the system equation in Example 2.9 with 
/(•) =  /at(-;0, W ), assuming a constant system variance W . The transition 
probabilities are
P(x, y ) 
=  
P r (0(n) +  wn+\ < y \ 6>(n) =  x)
=  
Pr{wn+1 < y - x )
/
■y - x
f(w)dw
-OO
andp(x,y) =  f ( y  — x). The marginal distribution at each step is recursively 
obtained as
r M l
7T
/
OO
f n(y -  x )n w (x)dx
-OO
where f n is the nth convolution of f . In the case of Example 2.9, these cal­
culations simplify due to the normality assumptions to give 
— N(a, R +  
n W ) if 7r<°> =  N{a,R).
E xam ple 4.9 A numerical sequence y i , y 2,-.- is split by bars whenever

Continuous state spaces
131
Uj >  yj+\ and starts with a bar. A run is a collection of numbers limited 
by bars. For example, the portion 3,6,9,2,3,1,5,2 of a sequence is split as
| 3,6,9 | 2,3 | 1,5 | 2.
and (3,6,9), (2,3) and (1,5) are runs.
Consider a sequence <j>\, <p2, ■ • • of independent random variables with 
identical distribution C/[0,1] and let ( 0 ^ ) n>i be a Markov chain on S =  
(0,1) formed by the initial values of the runs. The transition kernel of the 
chain is obtained from
OO
P (x ,y ) =  
Pr(0^n+1^ <y,4>n =  m \0^ =  x)
m= 1
where ipn is the length of the nth run. It can be shown that this leads to
v(x v) =  f  
e 1~x 
, if y < x
\ e l- x - e » - x 
, if y > x  
'
Assume now that 0^  =  (&[n\ . . . ,  9 ^ ) ' is a random vector in Rd. A 
sequence ( 9 ^ ) n>o in S C Rd is a Markov chain with continuous state 
space if
Pr(8{n+1) <  y | 9in) = x  
= x (n- 1), . . . , 0 (o) = x (0))
=  
Pr(9{n+1) <  y | 9(n) =  x),
where x^°\ ... ,x^n~ y\ x  and y 6 R d and z < w for d-dimensional vectors 
stands for z% < Wi, i =  1 ,. . .  ,d. Homogeneous chains are defined in the 
same way and transition probabilities are given by
P(x,y) =  Pr{9{n+1) < y \ 9{n) =  x) =  Pr(9{1) < y \ 9 ^  =  x).
As this transition defines a d-dimensional conditional distribution, the tran­
sition kernel given by the conditional density associated with this distribu­
tion is
, 
^ 
dP(x, y)
p{x>y) = 
-
4-7.2 Stationarity and limiting results
The stationary or invariant distribution tt of a chain with transition kernel 
p(x, y) must satisfy
/
OO
tt(x ) p(x,y) dx 
(4-14)
-O O
which is the continuous version of Equation (4.5). The interpretation re­
mains the same, as is clear from Equation (4.13).
To study convergence and limiting results, the classification of states

132
Markov chains
must be revisited. Instead of considering hitting time to a given state x, one 
must consider hitting time Ta to a given set A C S and a distribution v. A 
chain is said to be ^-irreducible if for a set A with positive probability under 
v, PxA =  P tx (Ta <  oo) >  0, for all x <E S. This is equivalent to imposing 
the existence of an integer n such that P n(x, A) > 0. A chain is irreducible if 
there is at least one distribution v ensuring that it is ^-irreducible. Usually, 
irreducibility is simpler to verify through this last condition, with u =  7r. 
Also, for most of the chains of interest for simulation, P(x, A) > 0.
The other vital properties for establishment of limiting results are ape- 
riodicity and positive recurrence. These are defined as in the discrete case 
but considering sets A with positive probability under v to replace atoms 
{;y} for which transition probabilities are always null in the continuous 
case. For the specific case of recurrence, a slightly stronger notion of Harris 
recurrence is used to replace positive recurrence (Tierney, 1994). Ergodic 
chains are defined as aperiodic, Harris recurrent chains.
Once these definitions are given, all important convergence results estab­
lished for discrete chains are valid here. For convenience, they are reviewed 
below for a continuous Markov chain 9^  with state space S C R d and 
stationary distribution tt:
• Irreducibility and aperiodicity of the chain is equivalent to ergodicity 
and the uniqueness of 7r as the limiting distribution in total variation 
norm.
• The ergodic averages of real-valued functions t(9) converge almost surely 
to their limiting expectations (when they exist) as stated in (4.6).
• The central limit theorem stated in (4.11) with r 2 given by (4.9) applies 
to ergodic averages.
Verification of ergodicity may be difficult for some chains. Tierney (1994) 
proved that most Markov chains used nowadays for simulation are ergodic 
and the above results can be applied. Finally, the important condition of 
reversibility of a chain is given by
n(x)p(x,y) =  n(y)p(y,x), for all x , y  e  S 
(4.15)
in direct analogy with the discrete case. Note that (4.14) follows directly 
from (4.15) by integrating both sides with respect to x. Reversible chains 
have proved to be very useful in helping specification of a Markov chain 
with limiting distribution 7r.
4.8 Simulation of a Markov chain
Consider an ergodic Markov chain ( 9 ^ ) n>o with state space S C Rd, 
transition kernel p(x, y) and initial distribution 7T^°\ Generation of a value 
of this chain starts with a value for 9i'0> sampled from 7 r^ . The value of 
9 ^  is then distributed with density p(9^°\ •) and can be generated from it

Simulation of a Markov chain
133
(Section 1.3). For 9^2\ this procedure is repeated by drawing from a distri­
bution with density 
•). Iterating this scheme through the steps of the
chain leads to drawing 9 ^  from a distribution with density p(9<^n~ 1\ •), for 
all n.
As the value of n gets large, the draws become increasingly closer to 
draws from the limiting distribution tt and can be considered as approxi­
mate draws from n. Note that all chain values sampled after convergence is 
reached are also draws from tt due to stationarity. Here and throughout this 
book, convergence is assumed to hold approximately for an iteration whose 
marginal distribution is arbitrarily close to the equilibrium distribution tt 
and not in the formal sense.
These simple results have a far-reaching impact on simulation well be­
yond the study of Markov chains. They provide sophisticated machinery 
with which to approach sampling from any (possibly highly dimensional) 
distribution tt. This area of study is collectively known as Markov chain 
Monte Carlo (MCMC) methods. Chapter 2 evidenced the need for sum­
marization of posterior distributions tt. Many examples showed that this 
task is far from trivial in complex models and Chapter 3 presented some 
approximating alternatives, including simulation. It was shown there that 
non-iterative techniques have a limited scope. As the dimension of the 
model gets large, they become less reliable.
This chapter provides the means by which sampling from virtually any 
posterior distribution tt can be approached. One simply has to embed tt as 
the limiting distribution of an ergodic Markov chain with transition kernel 
p. The main requirement from p (x , •) is to provide distributions that can 
be sampled from.
The remainder of this book is devoted to presenting and studying Markov 
chains whose simulation lead to draws from a limiting distribution of in­
terest tt. Many important questions arise and will be tackled during the 
next chapters. The most basic one is whether such chains can always be 
constructed and sampled from. It is remarkable that there are many such 
chains for any posterior distribution tt, no matter how complex the model 
is. Examples 4.7 and 4.8 have provided simple cases that hint at a positive 
answer. Although these examples considered only the discrete context, they 
will be extended in the next chapters to accommodate highly dimensional 
continuous distributions.
Other relevant questions regard the criteria for selecting the iteration to 
stop sampling and choice of kernel among the possible alternatives. The 
first point deals with determination of convergence of the chain. In more 
precise terms, one would wish to ascertain how close the marginal distri­
bution tt^  of the current iteration is to the target distribution tt. Many 
aspects are involved including theoretical bounds on probability distances 
and the computational complexity of calculations required. In broad terms, 
the initial chain values are far from the stationary distribution and should

134
Markov chains
be discarded. This period is referred to as the warm-up or burn-in period 
for obvious reasons. Deletion of these values hopefully improves the accu­
racy of ergodic averages but enlarges their variance by the reduction of the 
sample size. This point will be returned to when convergence diagnostics 
are discussed in the next chapter. In any case, Markov chain simulation 
opens up a host of possibilities for sampling in situations where the di­
rect sampling methods of Chapter 1 and the indirect sampling methods of 
Chapter 3 do not apply.
Before concluding this section, it is important to comment on methods 
for assessing the accuracy of the estimates provided by ergodic averages as 
measured by their variance r^/n or its limiting approximation T2/n. There 
are many methods reviewed by Ripley (1987) and Geyer (1992). They can 
be broadly divided into direct, time series and batching methods. Assume 
that interest lies in the estimation of E^[t{9)] and a stream of simulated 
values tS11'1 =  
) is available from a Markov chain with stationary dis­
tribution 7r.
Direct methods are based on estimates i f  of r 2 obtained by respective 
replacements of a2 and pk in (4.9) by moment estimates a2 =  70 and 
Pk =  % /% , k <  k* where
.j n—k
7k — ~ ^
 
— P  , for k >  0 
(4-16)
n  3=1
and for k >  k*, pk =  0 where k* is chosen to limit the sum to include 
relevant terms. Unfortunately, the resulting estimate i 2 will not be consis­
tent. Many variants of the above estimates considering other multiplying 
constants instead of 1/n have been proposed and are reviewed by Priest­
ley (1981). Consistency is ensured by appropriately downweighting higher 
order autocorrelation (Geyer, 1992).
There are many methods of estimation based on time-series ideas. One 
approach is to fit an autoregressive structure to the time series 
and 
estimate r 2 from the estimated residual variance. More generally, ARM A 
models can be used. Geweke (1992) used estimates based on the spectral 
density S(w) of the series evaluated at frequency w =  0. These are com­
monly used in the study of time series in the frequency domain (Priestley, 
1981). Geyer (1992) considers other estimators based on the autocovariance 
structure of Markov chains.
Batching estimators are based on the simple idea of dividing the stream 
of n =  mk chain values into k batches of m successive values. The ratio­
nale behind it is to seek approximate independence between batches and 
therefore take the batch averages t i , ... ,tk as approximately independent 
quantities. The value of k is chosen to enforce approximate independence 
or at least very low autocorrelation of the sequence. Generally, values of k 
should be between 10 and 30 (Schmeiser, 1982). Then, for large m, each

ti has common approximate mean E^tiO)} and variance r 2/ m  =  kVar(t). 
Hence, the sample variance of the U estimates kVar(t) and r 2 is estimated 
as
| 
X>'< - V
V 
' 1=1
where t =  t. Inference about Ev [t(9)\ is based on an approximate sampling 
distribution y/n{i — jB7r[t(0 )]}/f2 ~  £fc-i(0,1), to account for the extra 
variability due to the estimation of r 2. The simplicity of the method has 
made it a popular choice for estimation of sampling variance in Markov 
chains.
Data augmentation or substitution sampling 
135
4.9 Data augmentation or substitution sampling
This chapter concludes with an example of a Markov chain constructed by 
Tanner and Wong (1987) to have n as a limiting distribution. Assume that
8 =  (</>, ip) has posterior distribution 7r and components (j> and ip can have 
any dimension. Assume also that interest lies mostly in inference about 
<f>, ip being a set of constructed parameters, latent variables or additional 
data. Under the last interpretation data already available is augmented by 
ip, hence the first name of the method.
The marginal posterior densities of (p and tp are obtained as
n(<t>) = J  n((j>\ip)n(ip)dip and
TV (ip) =  
I  Tr(ip\x)Tr(x)dx
where a; is a dummy argument playing the role of the parameter (p. Sub­
stituting the second into the first equation and interchanging integration 
signs leads to
7T(4>)
=  
j  ttW # )
Tr(ip\x)n(x)dx
dip
p(x,(p)Tr(x)dx 
(4-17)
where p(x,<j>) =  f  Tr(<p\ip)Tr(ip\x)dip. Equation (4.17) suggests that succes­
sive substitutions of tr(<j>) form an iterative algorithm. Since the integrations 
involved will generally not be feasible analytically, they can be replaced by 
sampling approximations. Hence the second name of the method. More im­
portantly, Equation (4.17) is Equation (4.14), satisfied by the stationary 
distribution tr(<p) of a Markov chain 
with transition kernel p(x,(f>).
The iterative solution proposed by Tanner and Wong (1987) is to update 
an approximation 7^") of 7r(<f>) to 7r^n+1) as follows:
1. Draw a sample cpi,... ,(pm from -K^n\<p).

2. Draw a sample ipi, . . . ,  iprn from n(ip). This is approximately achieved 
by drawing ipi from n(ip\(pi), i =  1 , . . . ,  m (Section 1.3).
3. Form a Monte Carlo approximation
1 
m
7T(" +1)(0) =  — ]T V (0 | ^ ) . 
i=i
For large m, these steps form a sampling-based approximation to an iter­
ation of the transition kernel p(x, (p), that only depends on the conditional 
distributions. It has been stressed in the previous chapters that despite the 
difficulty in direct sampling from the marginal distributions, sampling from 
the conditionals is generally easy for many models. So, step 2 can in these 
cases be performed. Step 3 informs that sampling required at step 1 will 
be from an approximation to the marginal given by a discrete mixture of 
conditionals. If it is easy to sample from the conditionals, then all sampling 
procedures are easily carried out.
Tanner and Wong (1987) showed that the data augmentation algorithm 
is uniformly ergodic and n(cp) is the unique distribution satisfying (4.17), 
provided the transition kernel p(x, <p) is positive for all pairs of points (x, <p) 
in the support of ir((p) and is uniformly bounded and equicontinuous. These 
results are valid for any choice of m. Also, note that the algorithm is sym­
metric in terms of cp and ip. Therefore, after convergence, samples of size 
m are available from the marginal distributions of both <p and ip.
It is also interesting to consider the case m =  1. An iteration of the 
algorithm simply alternates single draws from the conditional distributions. 
This structure resembles Example 4.7 and is the basis of Gibbs sampling 
scheme, introduced in the next chapter. Also, this iterative algorithm can 
be extended to more than two components (Gelfand and Smith, 1990).
4.10 
E xercises
4.1 Obtain the transition matrices for the chains described in Examples
4-1 to 4-4­
4.2 Consider the problem of sending binary messages of length d through a 
channel consisting of various stages. The transmission through each stage 
has error probability a. Let 9q be the message originally sent and 6n the 
message received at the nth stage.
(a) Obtain the transition matrix.
(b) What is the probability that a signal is correctly received at the second 
stage?
(c) What is the probability that a signal is incorrectly received for the first 
time at the second stage?
136 
Markov chains

P (x,y)
4.3 Let P  be a transition matrix. Prove that P k is stochastic and has at 
least one eigenvalue equal to one, k >  1.
4.4 Consider the Ehrenfest model for r =  3.
(a) Obtain P rx (T0 =  n) for x € S and 1 <  n < 3.
(b) Obtain the matrices P, P 2 and P 3.
(c) If 7r0 =  (1,1,1, l)/4 , calculate -K\, tt2 and ir^.
4.5 Consider a modified Ehrenfest model with S =  {0,1, • • • ,r }  and tran­
sition probabilities given by
(d — x)/2d 
, i i y  =  x  
+  l
1/2 
, if y =  x
x/2d 
, if y =  x 
-  1 '
0 
, i f \ y - x \ ^ l
If initially the first urn has on average d/2 balls, how many balls can be 
expected to lie in the first urn at the next step?
4.6 Show that the full conditional distribution of 9 ^  depends only on
and 9(-i+1\ for i =  1,2,... (You may find it easier to prove the result for 
the special cases of absolutely continuous or discrete state spaces.)
4.7 Show that
OO 
0 0
E(Ty 10<°> = x ) = Y ,  Prx(Ty > n) and E(N(y) \ 9™ = x) = ^  
y) ■
n = 0  
n = 1
4.8 Show that if y G S is a transient state then, for all x E S,
Prx (N (y) <  oo) =  1 and E[N(y) \ (9(0) =  x] =  
^ -v—  < oo .
1 
Pyy
Show also that if y € S is a recurrent state then
P ry(N(y) =  oo) =  1 and E[N(y) \ Xo =  y] =  oo .
4.9 Show that birth and death processes are irreducible Markov chains when 
px >  0 for x > 0 and qx > 0 for x > 0.
4.10 Consider again a birth and death process in S =  {0,1,2, ■■•} with 
Px =  2§ + i y  and Qx =  2(i+ry-
(a) Determine whether the chain is recurrent or transient.
(b) Determine P rx(Ta < Tb) for a < x <  b.
4.11 Show that
(a) pxy > 0 is equivalent to P n(x,y) > 0 for some n 
>  0;
(b) if x £ S is recurrent, x —> y 
and y —> 
x 
then y G S is also recurrent;
(c) if x  G S is recurrent, x / * f /  
and y x then P(x, y) =  0.
Exercises 
137

138
Markov chains
4.12 Consider the 2 x 2  version of the Gibbs sampler presented in Example 
4.7. Obtain the 4 x 4 transition matrix P  and show that tt is the stationary 
distribution of the chain.
4.13 The exports of a country can be modelled, under economic stability, as 
a Markov chain with states +1, 0 and -1 representing, respectively, growth 
of 5% or more, variation smaller than 5% and decline of 5% or more with 
respect to the previous year. Let the transition matrix be
0.8
0.2
0
p  =
0.35
0.3
0.35
V 
0
0.4
0.6
(a) Does this chain have a limiting distribution?
(b) Determine the average return times for all states.
4.14 Points 0,1,2,3 and 4 are marked clockwise in a circle. At each step, 
a particle moves with probability p to the right (clockwise) and 1 - p  to the 
left (anti-clockwise). Let 0^n> be the position of the particle in the circle at 
the nth step. Obtain the transition matrix and the limiting distribution, if 
it exists. What is the expected number of steps for a return to the initial 
state?
4.15 Analyse the limiting behavior of the transition matrices
?) • 
J) 
p* = ( i  i )  ■
4.16 Prove the ergodic theorem for t(9) =  9it i =  1,2, under the conditions 
of Example 4.7 with p00 =  p n  =  p/2 and p0i =  Pio =  (1 -  p)/2. In other 
words, obtain that
P r (  lim -  Y]Ou =  ^ ] =  1, 
i =  l , 2 .
V~*°° 3 
V
4.17 Consider a Markov chain 9 ^  and define the autocovariance of lag 
k (k >  0) of the chain as 7^ =  Covir(9^n\ 9^n+k^), the variance of 9 ^  as 
a 2 =  70, the autocorrelation of lag k as pk =  7fc/cr2 and r 2/n =  Var^(9n). 
Show that
and that t2 -> r 2 as n —*• 00 where r 2 =  ct2(1 +  2 E fcli Pk) if the series of 
autocorrelation is summable.
4.18 Obtain for continuous state spaces that
(a) P n+m(x,y) =  C aoP m{z,y)pn{x,z)dz;
(b) 7r(n)(2/) =  J^°00p(x,y)7rl'n- 1'>(x)dx =  
pn(x,y)7r(0)(x)dx.

Exercises
139
4.19 Consider a chain { 0 ^ ) n>\ formed according to the process described 
in Example 4-9- Show that the transition kernel and the density of the 
limiting distribution of the chain are respectively given by
/ 
N 
f 
e 1~x 
, if V <  x 
, 
t \ 
[2(1 — y) , if 0 <  y < 1
P(x,y) =  jg i-z  _  ey - x t if y > x  and 
“
(  
0 
, otherwise 
'
4.20 Discuss the sense in which an iteration of the data augmentation 
method provides a sampling-based version of the transition kernel in (4-11).


C H A P T E R  5
Gibbs sampling
5.1 Introduction
This chapter introduces the first widely used class of schemes for stochastic 
simulation using Markov chains. It is generically known as Gibbs sampling 
because it originated in the context of image processing. In this context, 
the posterior of interest for sampling is a Gibbs distribution. Borrowing 
concepts from Mechanical Statistics, the density of the Gibbs distribution 
can be written as
f ( x i , . . . , x d) cxexp
(5.1)
where k is a positive constant, T is the temperature of the system, E  is 
the energy of the system, a positive function, and 
is the characteristic 
of interest for the ith component of the system, i =  1,... d. In Mechanical 
Statistics, Xi is the position or perhaps the velocity and position of the ith 
particle and in image processing it is (an indicator of) the colour of the ith 
pixel of an image.
The energy function E is commonly given by a sum of potential functions 
V. These sums operate over collections of subgroups of components over 
which each potential function is evaluated. The subgroups generally obey 
some neighboring relationship in their definition. This leads to a proba­
bility specification based on local properties, useful for modelling spatial 
interaction between components. The main drawback is the difficulty in the 
determination of the global properties, such as the normalizing constant.
Geman and Geman (1984) discuss this modelling problem extensively 
with special regard for sampling schemes and comparison with Markov 
random fields. Their sampling scheme explored the conditional structure 
implied by the local specification. Even though it was a well known and 
influential paper in the area, their paper was not published in a main­
stream statistical journal. This is one of the few possible explanations for 
the delay in the introduction of their powerful results for the solution of 
Bayesian problems in general. Gelfand and Smith (1990) were the first au­
thors to successfully point out to the statistical community at large that 
the sampling scheme devised by Geman and Geman (1984) for Gibbs dis­
tributions could in fact be used for a host of other posterior distributions. 
In that sense, it is somewhat misleading that the scheme retained the name

142
Gibbs sampling
Gibbs sampling and Robert (2001) proposed to change it to Bayesian sam­
pling. The paper by Gelfand and Smith (1990) also compared the Gibbs 
sampling scheme with the data augmentation algorithm (Section 4.9) and 
sampling-importance resampling (Section 3.5).
This chapter tries to describe the development of the area up to now. 
Some questions are still not entirely settled and there is a risk of obso­
lescence involved. The Gibbs sampling algorithm is described in the next 
section and some of its main properties presented. Section 5.3 deals with the 
description of implementation and convergence acceleration techniques. As 
previously discussed, one of the main difficulties when sampling via Markov 
chains is the verification of convergence of the chain. This problem is ad­
dressed in Section 5.4 where statistical techniques for convergence moni­
toring and identification are introduced. Most of the material from these 
two sections can be applied to any MCMC scheme, not just the Gibbs 
sampler. Section 5.5 applies Gibbs sampling to hierarchical, dynamic and 
spatial models. Finally, the chapter provides a brief description of the main 
software available for Bayesian inference using Gibbs sampling.
5.2 Definition and properties
Gibbs sampling is a MCMC scheme where the transition kernel is formed by 
the full conditional distributions. Assume as before that the distribution of 
interest is tt{8) where 8 =  (81, . . . ,  dj)'■ Each one of the components Qi can 
be a scalar, a vector or a matrix.* Consider also that the full conditional 
distributions TTi(&i) =  ir(8i\9-i), i =  I , ... ,d are available. This means that 
they are completely known and can be sampled from.
The problem to be solved is to draw from tt when direct generation 
schemes are costly, complicated or simply unavailable but when genera­
tions from the 7r, are possible. Gibbs sampling provides an alternative gen­
eration scheme based on successive generations from the full conditional 
distributions. It can be described in the following way:
1. Initialize the iteration counter of the chain j  =  1 and set initial values 
6»<°> =  (0'O), . . . , ^ O))';
2. Obtain a new value 8 ^  =  (8^\ 
8 ^ )' from 
through successive
generation of values
o[j) 
~  
7T(01| ^ - 1), . . . , ^ - 1)) ,
6 ?  
~  
tt(02|
~  
tt^
, . . . , ^
) ;
* The reader may prefer to think about them as scalars if that helps. This point is 
readdressed in Section 5.3.4.

3. Change counter j  to j  +  1 and return to step 2 until convergence is
reached.
When convergence is reached, the resulting value 9 ^  is a draw from 7r. 
As the number of iterations increases, the chain approaches its equilibrium 
condition. Convergence is then assumed to hold approximately.
This presentation where each iteration consists of a single change to all 
components is favored by Gelfand and Smith (1990). The original work of 
Geman and Geman (1984) presented a chain with iterations formed by a 
change to a given component. Step 2 is obtained in the special case where 
the components are changed in a fixed and constant order.
The obvious form to obtain a sample of size n from 7r is to replicate n 
chains until convergence. Alternatively, after convergence all draws from a 
chain come from the stationary distribution. Therefore n successive values 
from this chain after the burn-in period will also provide a sample from tt. 
The issue of how to form a sample is readdressed in more detail in the next 
section.
A typical trajectory of a Gibbs sampling chain is illustrated in Figure 
5.1. An iteration is completed after d moves along the coordinate axes of 
the components of 9. Convergence diagnostics are complex as d can be very 
large and will be left for Section 5.4. Note that the convergence must be 
in distribution which means that the joint distribution of all parameter 
components must converge to the joint posterior for all values of 9. This 
exhaustive verification is far from trivial.
E xam ple 5.1 (Carlin, Gelfand and Smith, 1992) Let y i , ... ,yn be a sam­
ple from a Poisson distribution for which there is a suspicion of a change 
point m along the observation process where the means change, m =  1 ,..., n. 
Given m, the observation distributions are yi\X ~  Poi(X), i =  1 ,..., m and 
Vi\4> rsj Poi(4>), i =  m +  1 , . . . ,  n. The model is completed with independent 
prior distributions X ~  G{a,/3), <p ~  G{^,S) and m uniformly distributed 
over { l , . . . , n }  where 
and S are known constants. The posterior
density is
7r(X, (p, m) 
oc f ( y i , . . . , y n\X,cp,m)p(X,(p,m)
m  
n 
^
=  I I / p ( * ; A) II f G { ^ a , 0 ) f G{<f>\i^)-
i =  l  
i= m
+ 1
Definition and properties 
143
(X
J J e - AAVi 
Y [ 
e - V i A“ _1e_/JV y_1e - 4*
i =  1 
1= 771+1
^  
)a+ sru- l e-{l3+m)\(jp+sn- s m- l e-(5+n-m)ct>
where si =  ^ i=1 Vi for I =  1,... , n. It becomes simple to obtain the full 
conditional densities
=  
G (a +  sm, 0  +  m ) ,

144
Gibbs sampling
After 5 iterations 
After 50 iterations
Figure 5.1 Typical trajectories of the Gibbs sampler in a bidimensional paramet­
ric space, d =  2, after 5 and 50 iterations. The concentric curves represent the 
contour lines of the posterior density.
— 
G ( j  +  sn -  sm,8 +  n -  m ) ,
\ a + s m - l e -( f)+ m )\ (p-y+3n - s 7Tl- l e ~(S+n-Tn)<l>
for m =  1 ,..., n. It can be seen that A and <f> are conditionally independent 
given m, a posteriori. Thus, ir(A|m) =  7Ta(A) and n(<f>\m) =  7^(0). All 
these distributions are easily sampled from (see Chapter 1) and the iterative 
scheme repeating steps 1-3 can be operated without difficulty.
In this specific setting, it is possible to obtain the marginal posterior
distributions analytically. It can be shown that (see Exercise 5.1)
Tx(m) oc _____ r ( a  +  Sm) r ( 7  +  S « - S m )
^ 
' 
(m +  /3)Q+ * ™( n - m  +  6)T'+s" - s"* ’ 
^ 
'
for m =  1 
The normalizing constant is obtained by summing up
the posterior probabilities and ensuring sum 1. Therefore 7r(A) and Tr(<j>) 
can be analytically obtained as 7r(A) =  E m = i 7r(A|m)7r(m) and ir(4>) =  
E L i  
For instance, EX(A) =  E l = i £ ( % . ! / ) ’r(m ), while
Varv ((f)) =  E m =i Var((j>\m,y)Tr(m).
This model can be applied to the n — 112 observations in Table 5.1 and

Definition and properties
145
4
5
4
1
0
4
3
4
0
6
3
3
4
0
2
6
3
3
5
4
5
3
1
4
4
1
5
5
3
4
2
5
2
2
3
4
2
1
3
2
'?,
1
1
1
3
0
0
1
0
1
1
0
0
3
1
0
3
2
2
0
1
1
1
0
1
0
1
0
0
0
2
1
0
0
0
1
1
0
2
3
3
1
1
2
1
1
1
1
2
4
2
0
0
0
1
4
0
0
0
1
0
0
0
0
0
1
0
0
1
0
1
Table 5.1 Counts of coal mining disasters in Great Britain by year from 1851 to 
1962 (Jarret, 1979).
True 
Gibbs
Par.
Mean
Var
95% C.I.
Mean
Var
95% C.I.
A
3.120
0.280
(2.571,3.719)
3.131
0.290
(2.582,3.733)
0.923
0.113
(0.684,0.963)
0.922
0.118
(0.703,1.167)
m
1890
2.423
(1886,1895)
1890
2.447
(1886,1896)
Table 5.2 Exact and approximate posterior quantities. Gibbs results are based on 
5000 draws starting at m (0) =  1891. C.I. stands for credibility interval.
Figure 5.2(a). Figure 5.2(b) shows some evidence of a changing pattern 
around 1890 more clearly.
The Gibbs sampler was implemented and run for 5000 iterations, starting 
at m
=  1891 and hyperparameters a =  (3 =  7 =  J =  0.001. Figure 5.1 
exhibits the trajectories at selected iterations.
Inference can be approximately performed via MCMC by using all the 
5000 iterations of the chain. The MCMC approximation to the marginal 
posterior distributions of X, cj> and m appear in Figures 5.2(c) and 5.2(d). It 
can be seen that with high probability a change point occurs around 1891 and 
that the Poisson rates are quite distinct before and after this change point. 
True values and MCMC approximations to the posterior means, posterior 
variances and posterior 95% credibility intervals of X, <fi and m appear in 
Table 5.2.
Effective sample sizes based on X,(J> and m are 4800, 3950 and 4900, 
respectively. They are very close to the actual number of iterations of the 
chain, reflecting the very low autocorrelation structure of this Gibbs sam­
pler. They are calculated with theoretical chain autocorrelations pk used in 
(4-10) approximated by their moment estimates pk (see Equation 4-16).
A few basic facts must be established beforehand. First, the Gibbs sam­
pler does define a Markov chain. This is clearly the case as the probabilistic 
change at iteration j  depends only on chain values at step j  — I. Also, the 
chain is homogeneous as the transitions are only affected by the iteration

146
Gibbs sampling
(a)
(b)
O 
00 oo
CDGD O O O  O  O
OO OO 0 OO O OO 
O
O  
G D O O  
O 
O  
O  O 0 0
O  
OO  
O O O B O  a m  O O  0 0 9  O  O  OOO
O O  O  
CD O O O tX m nBIO
I-------1-------1-------1-------1--------- 1
1860 
1880 
1900 
1920 
1940 
1960
1860 
1880 
1900 
1920 
1940 
1960
years
(C)
A
<d)
1890 
1895
years
Figure 5.2 Poisson observations with change point, (a) counts of coal mining 
disasters in Great Britain by year from 1851 to 1962, (b) cumulative counts,
(c) true (solid lines) and histogram approximations of the marginal posterior 
distributions of A and 4>, (d) true (solid lines) and histogram approximations 
(dashed lines) of the marginal posterior distribution of m.
through the chain values. It is not difficult to obtain the transition kernel 
as
d
P{6,4>) =  
... ,6d) 
(5.4)
i=i
which clearly depends on the iterations only through the chain values 6 and 
4>. This chain with a complete scan over all components is not reversible

although each individual change is reversible. Green (1995) pointed out 
that the chain can be made reversible by taking each iteration of the chain 
to consist of the complete scan through the components followed by another 
scan through the components in reversed order (Besag, 1986).
Another important result is the derivation that the equilibrium distri­
bution of a chain with transition kernel (5.4) is tt. This result was derived 
in a special case of d — 2 discrete components in Example 4.7. In the 
continuous case, the same argument cannot be used but the mechanics of 
the algorithm is the same. If a Markov chain with transition kernel p(9, <p) 
has limiting distribution 7r°°, then the stationarity condition (4.14) must 
be satisfied by p and 7r°° and the chain must be irreducible. Irreducibility 
is easy to verify for each application by checking that P(x, A) > 0 for all 
sets A with positive posterior probability. For statistical applications, it is 
generally satisfied but see Example 5.2 below for a counterexample.
To check stationarity, let 9 =  (91,92) with marginal limiting densities 
7r°°(#i) and ttco(92). The limiting full conditional distribution of 6\ is 
7rf°(0i). The transition kernel (5.4) simplifies to
Definition and properties 
147
where cp =  ((pi, fa) has components with the same dimensions as those of
6. As f  J 7r(falfa)7r00(91j92)d9idfa =  f  n(fajfa)dfa x f  n°°(81l92)d91 =  1,
Integrating (4.14) with respect to fa gives the marginal limiting density of 
<pi as
where the last equality follows from (5.5). The only distribution satisfying
(5.6) must have Tr{fa\(p2) =  
The same argument could be used
to give Tr(fa\(pi) =  7r°°(fa\(pi) and the limiting distribution must have the 
same full conditionals as the posterior. The same argument follows for 9 
divided into d blocks of components as these can always be rearranged in 
two blocks &i and 6L*. This means that all limiting full conditionals are 
given by the posterior full conditionals. This does not in general guarantee 
that 7r°° =  7T (see Exercise 5.3 for an example where it fails). Nevertheless, 
Besag (1974) showed that under very mild conditions, the set of all full
p{9,<p) =  7r(0i|02)7r(<H<M
(5.5)
I I I  n(fa\92)TT{fa\fa)Tr00(9)d9id92dfa
(5.6)

148
Gibbs sampling
conditional distributions determine the joint distribution. Therefore, the 
Markov chain with transition kernel (5.4) converges to the distribution of 
interest 7r and the iterative sampling scheme with steps 1-3 above draws in 
the limit a value from this distribution.
Formal convergence conditions for the Gibbs sampler were established by 
Roberts and Smith (1994) and Tierney (1994). The results are presented in 
terms of continuous parameter spaces but can be extended for combinations 
of continuous and discrete parameters (Example 5.1). A simple example of 
a reducible chain where convergence fails is given below.
E xam ple 5.2 ( 0 ’Hagan and Forster, 2004; Roberts, 1996) Consider 0 =  
(6\, 62) uniformly distributed over two disjoint regions A =  A\ x A2 and 
B  — Bi x B2 of the plane with probabilities pa and pn adding up to 1. 
Consider also that A\ and B\ are disjoint regions on the 0\ axis and A 2 
and JE?2 are disjoint regions on the 02 axis. This implies that full conditionals 
are also uniform but over regions that depend on the starting point. Chains 
that start in Ai will lead to sampling 62 uniformly over A2 which implies 
sampling 0i uniformly over A\ and this situation perpetuates itself. Points 
from B will never be reached. Analogously, chains starting in B\ will never 
reach points in A. This chain is clearly reducible and will have uniform 
limiting distribution over A or B  depending on the starting point.
5.3 
Im p lem en tation  and op tim ization
Despite the theoretical results ensuring the convergence of the Gibbs sam­
pler, its practical implementation may be complicated by the potential 
complexity of the models considered. Convergence of the sampler becomes 
difficult to characterize. Given that it is a numeric and iterative method, 
practical strategies to improve the efficiency of the method may have a 
considerable impact on its computational cost. Efficiency broadly consists 
of reducing the number of burn-in iterations and the amount of arithmetic 
operations required at each iteration. The techniques presented are related 
to the basic MCMC methods as described in the previous chapter and rep­
resented by the Gibbs sampler. More general techniques using other forms 
of chains will be presented in Chapter 7.
5.3.1 Forming the sample
The previous section presented two forms to obtain a sample of size n 
from the posterior distribution ir. The obvious one is to process n chains 
in parallel until convergence, say after m iterations, and take as sample 
elements the rath chain value from each of the n chains. The generation 
procedure will then require mn generations from the chain. If chains are 
initialized independently, the sample consists of independent values from

Implementation and optimization
149
7r. Independence is easier to establish if the initial values are all different 
and preferably with larger dispersion than in the posterior (Section 5.4.3).
Another form is to consider a single chain and explore ergodic results. 
After convergence, all chain values have marginal distribution given by the 
equilibrium distribution 7r. So, a sample of size n may be formed by n suc­
cessive values from this chain. This generation will require m + n  generations 
from the chain. This is substantially less than independent sampling. The 
difficulty here is that the sample elements are no longer independent due 
to chain dependence. Ergodic theorems ensure that inference based on this 
sample is still valid. From a practical point of view, there may be problems 
if the chain autocorrelation is too high and the sample is not large enough 
to acknowledge it. In these cases, chains may take too long to adequately 
cover the entire parameter space appropriately. As a result, some relevant 
regions may be underrepresented in the sample.
An alternative approach accommodating independence is to take for the 
sample chain values at every /cth iteration after the burn-in period. Marko­
vian processes only have first order dependence. As the lag between it­
erations increases, chain values become less and less correlated and are 
virtually independent for a large enough value of the lag k. A sample of 
size n with quasi-independent elements thus requires m +  kn generations 
from the chain. The value of k is typically smaller than m and again an 
improvement over independent sampling is obtained. There is no gain in 
efficiency, however, by this approach and estimation is shown below to be 
always less precise than retaining all chain values. This procedure is ad­
vantageous if computer storage of values is limited. Useful indicators of 
dependence are given by the chain autocorrelations (Section 4.8). A formal 
approach for selection of k is given in Section 5.4.4.
Another compromise is to take a small number I, say less than 10, of 
independent chains, run them until convergence and then retain from each 
of them n/l successive values from the sample. This will lead to a sample 
of size n obtained after l[m +  {n/l)} =  Im +  n generations from the chains. 
Yet another variant is obtained by retaining every fcth chain value after 
convergence with a total of l[m +  (n/l)k\ =  Im +  kn generations. In com­
putational terms, there are efficiency losses with respect to using a single 
chain and gains with respect to independent sampling.
The independent sampling approach was suggested by Gelfand and Smith
(1990) and used by some authors shortly afterwards. The single chain ap­
proach was emphatically advocated by Geyer (1992) backed by ergodic the­
orems. Sampling every kth iteration was discussed by Raftery and Lewis 
(1992). Gelman and Rubin (1992a) recommended the use of a small num­
ber of independent chains backed by an example from Gelman and Rubin 
(1992b) where single chains provide indication of convergence of ergodic 
averages to different limits. Gelman et al. (2004) argue that somehow the

150
Gibbs sampling
benefits from quasi-independent sampling are diluted when running few 
chains.
There is no general agreement on the subject although it is generally 
agreed that running n parallel chains in practice is computationally inef­
ficient and unnecessary. The main debate is whether a few parallel chains 
are needed. If the convergence properties of the chain are well understood 
then clearly a single chain suffices. As these characteristics are hard to ob­
tain, prudence suggests that a few pilot parallel chains should be run. If 
they quickly settle around common values then a single chain can be safely 
used to extract a large sample for inference. Otherwise, there may be mi­
nor characteristics of the posterior distribution such as secondary modes 
far from the mode that require very large samples to be noticed. In this 
case, these parallel chains should be run longer and their values should 
be retained for the sample. Convergence diagnostics are the subject of the 
next section and these points will be returned to in more detail there.
5.3.2 Scanning strategies
The Gibbs sampler described in the previous section involved a complete 
scan over the components. All iterations consisted of visits to update the 
components in the same deterministic order, typically 1 —> 2 —>•••—> d. 
There are many other possible scanning or updating strategies for visiting 
the components of 0.
Geman and Geman (1984) proved convergence to the joint distribution in 
a discrete setting for all visiting schemes that guarantee that all components 
are visited i.o. when the chain is run indefinitely. The reversible Gibbs 
sampler where at each iteration each component is visited in a fixed order 
and then visited again in reversed order satisfies this property. In this case, 
each iteration consists of 2d updates and comparisons between strategies 
should bear that in mind.
Another scheme where an i.o. schedule is guaranteed draws a number i 
from { 1 , . . . ,d } with fixed positive probabilities at each iteration and only 
updates the 8i at that iteration. To make it more comparable with the 
deterministic scan, an iteration of these random scans can be defined by a 
collection of d such updates.
Roberts and Sahu (1997) consider a random permutation scan where at 
each iteration a permutation of { 1 , . . . ,  d} is selected and components are 
visited in that order. Zeger and Karim (1991) describe a Gibbs sampling 
scheme where some components were visited only every kth iteration. This 
also guarantees an i.o. visiting schedule for fixed, finite k.
Assume now that 7r is a multivariate normal distribution with precision 
matrix <3> =  (<ptJ). For this setting, Roberts and Sahu (1997) showed that 
convergence for the deterministic scan is faster than for the random scan 
if $  is tridiagonal (7r(#i|#_j) =  7r(#j|#i_i,6i+\), for all i) or if $  has non­

Implementation and optimization
151
negative partial correlations (<pij < 0). This result is particularly important 
because both dynamic and hierarchical models lead to tridiagonal matrices 
if variances are known. Their results also indicate that more precise distri­
butions lead to faster convergence both for the deterministic and random 
scans.
5.3.3 Using the sample
Whatever the scheme chosen for forming the sample, after it is used a 
sample of vectors 9 i , ... ,0n generated from the posterior distribution tt is 
available. Assume also the more general case where these are successive 
values from a single Markov chain. A sample from the ith component of 0 
is given by On,... ,6ni. Marginal point or interval summaries of any real 
function ip =  t(0) are estimated by their corresponding estimators based on 
the sample. This is always a consistent estimator by the ergodic theorem
(4.6). The quality of this estimator can be judged by the central limit theo­
rem (4.11) from where approximate confidence intervals about the MCMC 
estimates may be formed.
So, the posterior mean of ip is estimated by E(ip) =  ip =  (1/n) J2j=i i’j 
where ipj =  t(8j), j  =  1 ,..., n. The posterior variance of ip is similarly es­
timated by noting that 
— Var(ip) =  E('ip2) — [E(ip)]2. Each expectation 
is estimated by an application of (4.6) and cr2 is estimated by a^ where
=  E{ip2) -  [E(ip)]2 =  ^ j^ iip j -  ip)2 ,
the sample variance. The denominator n may be replaced by n — 1 but this 
change is irrelevant for two reasons. First, typically n is large which makes 
the change immaterial. Second, it does not remove the bias of the estimator 
as usually happens when independent sampling is performed.
Consider again the problem of choosing between a sample of n succes­
sive values and a sample of to =  n/k values obtained by skipping ev­
ery fcth iteration. Note that k such sub-samples with guosi-independent 
draws are formed. Denote by ipi,... ,ipk the averages of the sub-samples 
and ip =  (1 /k) Ylj=i V'j the average over the complete sample. There are 
k +  1 estimators of E(ip) and they are all consistent estimators by the er­
godic theorem. It can be shown that Var (ip) < Var(ipj), for all j  (O ’Hagan 
and Forster, 2004; MacEachern and Berliner, 1994). This means that inde­
pendence sampling comes at the expense of reduced efficiency.
Credibility intervals are similarly obtained by estimating the interval 
limits by the respective sample quantiles. As in Section 3.5, if n =  1000, 
and an equal tails 95% probability interval for ip is required, it can be 
estimated by the interval with limits given by the 25th and 975th largest

152
Gibbs sampling
sample values of ip. Again, these values are consistent estimators of the 
0.025 and 0.975 quantiles of ip by (4.6).
All above estimators have a sampling distribution that is approximated 
by (4.11). The asymptotic variance of these estimators, which is different 
from the posterior variance, can be estimated by the methods described in 
Section 4.8. The central limit theorem ensures that estimation errors are
The marginal densities ir(8i) can be estimated by (a smoothed version 
of) the histogram of sampled values of Qi (Section 3.5). Better estimators 
can be obtained by using conditional distributions. Recalling that 7r(0j) =  
f  Tr(0i\9-i)-K(9-i)d0-i, a Monte Carlo estimator is given by
where the 
j  =  1 , . . . ,  n are a sample from the marginal 7r(9-i). Notice 
that Equation (5.7) is a generalization of the results from Section 3.4 for 
dependent samples. Again, the ergodic theorem ensures that n is a consis­
tent estimator and it obeys a central limit theorem for every value of 9t. 
These estimators are always continuous for continuous parameters. More 
importantly, they are based on information about the form of the poste­
rior. For that reason, Gelfand and Smith (1990) call it a Rao-Blackwellized 
density estimator. This is a reference to the Rao-Blackwell theorem that 
states that estimators are always improved (in the sense of reducing sam­
pling variance) by conditioning on sufficient statistics. They proved the 
result for density estimation in the context of independent sampling. The 
general proof of the result for Markov chain sampling is given by Liu, Wong 
and Kong (1994). The same idea can be used to obtain better estimates of 
moments of t(9i) through
although the gains are not as large here.
5.3.4 Reparametrization
Going back to Figure 5.1, an iteration is formed by moves along the coor­
dinate axes of the components of 9. If there is weak dependence between 
the components, the moves will be ample. The chain will then move freely 
through the parametric space and convergence will be fast. An extreme case 
is posterior independence between the components. The full conditionals 
are equal to the marginals and convergence is immediate.
Often, the posterior structure leads to high correlation between some 
of the components of 9 (Section 5.5.2). Figure 5.3 illustrates this point
0 (n  1//2).
(5.7)
m 9 l)] =  \ Y JE m )  Wi,-*]
j= 1

Implementation and optimization
153
for a bidimensional parameter. The contours of the posterior show strong 
dependence between the components of 9 and chain moves, governed by 
the conditional densities, will be small. The chain will take many iterations 
to adequately cover the parametric space and as a result convergence is 
slow. In this case, the Gibbs sampler will be inefficient. Examples can be 
constructed in larger dimension models where convergence can be slowed 
to any arbitrary amount of iterations (Shephard, 1994).
A simple and sometimes effective way to reduce convergence time is to use 
reparametrizations. This point was discussed in Chapter 3 in the context 
of improving approximations. Adequate transformations in the parameter 
space may produce situations of near independence that are ideal for fast 
convergence of the chain. Unfortunately, there are no rules to determine 
suitable transformations but frequently linear transformations that produce 
a diagonal variance matrix provide good results. Two important classes of 
models where these transformations can be found are presented below.
Figure 5.3 Contour lines of a bivariate posterior density with components highly 
correlated. A possible chain trajectory is also depicted to illustrate slow conver­
gence, with iterations in parentheses. The contours are from a bivariate normal 
distribution with marginal distributions 0\ ~  N (2,1) and 62 ~  N (3,1) and cor­
relation —0.97. The trajectory is obtained by sampling from the full conditional 
distributions 92 |^i and 9i\02-
E xam ple 5.3 For the regression model described in Section 2.3, the con­
ditional posterior was 7r(/?|<j>) =  AT(64,, B^) and therefore posterior correla­
tions depend on the posterior variance B Nu me r i c a l  techniques described 
in Section 1-4 can be applied to obtain the square root matrix A# such that

154
Gibbs sampling
A'pA'^ =  B ^ 1. So, a =  A^P 
N {A <j,b<f>,Id) given 4> and the components of 
vector a are independent a posteriori given <f>.
In the case of simple linear regression yt =  Pi +  p2'£l +  et with non- 
informative prior p(P) oc k, a has components a\ =  P\ +  fax and 
=  
P2. This is equivalent to centering covariates and working with the model 
yi =  c*i +  0L2(xi - x )  +  ei. Depending on how close the values of Xj are, the 
plot of the posterior conditional density of 0\<f> will present contour lines 
that are as concentrated as those exhibited in Figure 5.3. In multiple linear 
regression, centering covariates is usually enough.
The conditional dependence between components of a is removed and the 
only remaining posterior correlation is between a and </>. Sampling com­
ponents of a is as simple as sampling components of P but the chain will 
converge faster. Once a is sampled, a sample value of p is obtained by
E xam ple 5.4 Consider the hierarchical (or random effects) model
where the random effects ai associated with the observation groups have 
distribution a.i ~  iV (0,r2), j  =  1 ,..., n*, i =  1 ,... ,m . Note that by writing 
j3i =  n +  ai and p =  fi, the one-way classification model of Example 2.8 is 
recovered. Assume also a non-informative prior p(fi) oc k. It can be shown 
that given variance components a 2 and t2, posterior correlations between 
model parameters are given by
High posterior correlations occur if a2/rii -C r 2/m . Roberts and Sahu
(1997) showed that asymptotically (as m —> 00)  the deterministic scan 
over the parameters has faster convergence than random scans.
Gelfand, Sahu and Carlin (1995) suggest the hierarchical parametrization 
with the Pi replacing the a j and show that given the variance components
o 2 and t 2 , posterior correlations between model parameters are given by
Now, low correlations are obtained for the conditions of high correlations 
in the original parametrization. So, depending on the data structure, it 
is more appropriate to work on another parametrization. Theoretical sup­
port is provided by Roberts and Sahu (1997). Gelfand, Sahu and Carlin
(1995) argue that in practice the presence of random effects implies excess 
randomness and therefore it is expected that a 2/ni will be smaller than 
T 2 / m ,  which would justify the use of the pi. They extended their approach 
to nested random effects models. Generalized linear models with random
V ij — / i 
OLi ~h 
, 
C ij ~  AT(0 , G  )

Implementation and optimization
155
effects were studied by Gelfand, Sahu and Carlin (1996) and, even though 
analytic results are no longer available, they arrived at the same qualitative 
recommendations.
Vines, Gilks and Wild (1996) suggest the reparametrization v =  fi — a 
and & =  cti — a and show that given the variance components a2 and t2, 
posterior correlations between model parameters are given by
Cor(i/,£i) =  0 and C o rfe, £,•) =  
.
Now, correlations have the advantage of not depending on the variance 
components. Again, the idea can be extended to more general models. For 
this parametrization, convergence with the random permutation scans is 
faster than for all other scanning strategies.
These examples suggest a general strategy based on approximate poste­
rior normality (Section 3.2). The approximate variance V is a first order 
approximation for the posterior variance. Its square root matrix A can be 
calculated and a linear transformation a =  A ~ l9 operated. This will pro­
vide approximate posterior independence to the first order. In more general 
models, in addition to the computational cost of finding A, there is also 
the added cost of sampling a instead of 0. Other approximations to the 
posterior variance may be sought. Hills and Smith (1992) suggest using the 
sample variance obtained from a pilot chain.
Another simple but important point is to observe the structure of the 
model and parameters. For example, these transformations will provide a 
better result if the posterior for each parameter behaves like the normal 
distribution. Variance parameters will not have this behavior unless a large 
number of observations is collected. Otherwise, a logarithmic transforma­
tion is recommended before application of the orthogonalization procedures 
above. Optimal strategies for some parameters should not be blindly ap­
plied for other parameters.
5.3.5 Blocking
So far, nothing has been said about the choice of components that form the 
parameter vector 0. In principle, the way the components are arranged in 
blocks of parameters is completely arbitrary and includes as a special case 
blocks formed by scalar components. The structure of Gibbs sampling, also 
illustrated in Figure 5.1, makes moves according to the coordinate axes of 
the blocks. Scalar blocks lead to moves along each component of 8. Larger 
blocks allow moves in more general directions. This can be very beneficial 
computationally when there is high correlation between components. The 
slow, componentwise moves may be replaced by fast moves incorporating 
the information about dependence between components. These moves are 
dictated by the joint full conditional for the block of parameters considered,

156
Gibbs sampling
which incorporates the correlation structure. This intuitive consideration 
is confirmed by the theoretic results of Liu, Wong and Kong (1994). They 
showed that estimates obtained by blocking components are generally more 
precise than those obtained by treating each component separately.
Derivations of Roberts and Sahu (1997) show that, for random scans, 
convergence improves as the number of blocks decreases. Thus, blocking 
is beneficial. They also proved that blocking is beneficial for non-negative 
partial correlation distributions and more so as the partial correlation of the 
components in the block gets larger. These results were obtained only for 
a multivariate normal 7r and extrapolations should be made with caution. 
They also provided an example where blocking worsens convergence.
Although it is hard to determine optimal blocking strategies, some basic 
rules should be followed. When a parametric vector or matrix is specified in 
block, it generally has joint full conditionals that are easy to sample from. 
The important message is to block as much as possible for sampling. Of 
course, if the complete parameter vector forming a single block could be 
sampled, there would be no need for Gibbs sampling! Therefore, the only 
restriction is the ability to sample from the full conditional distributions 
formed.
5.3.6 Sampling from the full conditional distributions
In some cases, the form of the full conditional distribution is not recog­
nizable which prevents sampling via the conventional algorithms. Chapter 
1 presented a host of other general-purpose options such as (adaptive) re­
jection and reweighted sampling methods. According to Carlin and Louis 
(2000), these situations are an indication that an altogether different ap­
proach should be applied instead of insisting on Gibbs sampling.
Ritter and Tanner (1992) developed yet another sampling scheme from 
difficult full conditionals. Their approach is similar to adaptive rejection 
by being based on the evaluation of the full conditional at a few selected 
points. For that reason, they called it the griddy Gibbs sampler. Let 7r4(#,) 
be a difficult full conditional distribution. Then, sampling from 7r* can be 
approximately performed as follows:
1. Take a grid of points 6 u ,..., f)im, evaluate 7r *(% ), j  =  1, . . . ,  m, and 
normalize them to obtain weights w\,. . . ,  wm.
2. Use the weights w \ ,.. . , wm to construct a simple approximation to the 
distribution function of 7Tj.
3. Draw a value from ni by the probability integral transform method (Sec­
tion 1.3).
There are many possibilities for the construction in step 2. The simplest 
one is to use piecewise constant functions (discrete distribution). Piecewise 
linear functions are also easy to sample from and allow for continuous

Convergence diagnostics
157
sampling. Higher order polynomials and even splines may be used but it is 
important to keep it simple to sample from. Tanner (1996) suggests that 
the number of points m should be kept small for the burn-in period of the 
chain and doubled after convergence for good approximations only when it 
matters. He also suggests an adaptive scheme to revise the selected grid to 
include more points in higher density regions.
5.4 Convergence diagnostics
As previously discussed, a value from the distribution of interest 7r is only 
obtained when the number of iterations of the chain approaches infinity. 
In practice this is not attainable and a value obtained at a sufficiently 
large iteration is taken instead of being drawn from 7r. The difficulty is 
the determination of how large this iteration should be. There is no simple 
answer to this question and most efforts have been directed at studying as 
close as possible the convergence characteristics of the chain. Most results 
below can be applied to any MCMC method although for a few of them 
the use of Gibbs sampling is required.
There are two main ways to approach the study of convergence. The 
first one is more theoretical and tries to measure distances and establish 
bounds on distribution functions generated from a chain. In particular, one 
can study the total variation distance between the distribution of the chain 
at iteration j  and the limiting distribution 7r. Special aspects derived from 
the probabilistic structure of the chain can also be studied. This approach 
was pursued by Meyn and Tweedie (1994), Poison (1996), Roberts and 
Poison (1994), Roberts and Tweedie (1994) and Rosenthal (1993) to cite 
just a few papers (see also the references in those papers). This is an area 
that is certainly going to grow as we increase our understanding of the 
subject. At the moment, however, the results have had little impact on 
practical work (Cowles and Carlin, 1996).
The study of convergence of the chain can also be approached from a 
statistical perspective, i.e., by analyzing the properties of the observed 
output from the chain. This is an empirical as opposed to a theoretical 
treatment of the problem and is obviously more practical. The difficulty 
with this approach is that it can never guarantee convergence because it is 
only based on observations from the chain (see Example 5.5 below).
Although the two approaches to the study of convergence are valid and 
complement each other, theoretical results have proved to be more difficult 
to obtain and apply to practical problems. This book will provide a more 
detailed description of the convergence diagnostics based on the statistical 
properties of the observed chain. Cowles and Carlin (1996) and Brooks 
and Roberts (1998) provide comparative and illustrative reviews of many 
of these methods. Robert (1995) reviews some possibilities involving the 
two approaches.

158
Gibbs sampling
Geman and Geman (1984) showed for the discrete case that the Gibbs sam­
pler is a uniformly ergodic Markov chain. Uniform ergodicity determines 
an exponential rate of convergence of the chain to the limiting distribution 
and could be taken as an indication of fast convergence. However, there is 
no indication of control over the rate of convergence and the Gibbs sampler 
can have an extremely slow convergence in some cases. The example below 
illustrates this point in a very simple context.
E xam ple 5.5 ( 0 ’Hagan and Forster, 2004) Consider again the situation 
of Example 4-1 where 9 =  {9\, 9-i)' is bivariate and ir(9) is given by the 
table of probabilities below
5-4-1 Rate of convergence
________ 02
9i 
0 
1
0 
p/2 
( l - p ) / 2
1 
(1 — p )/2  
p/2
Observe that n(9i) =  bern{\/2), i =  1,2, and the posterior correlation 
between 9\ and 9-2 is p =  2p — 1. Using properties of the Gibbs sampler, 
it is easy to obtain that P r (9 ^  =  l|0p ^ =  1) =  P r (9 ^  =  O|0p ^ =  
0) =  p2 4- (1 — p)2 and, consequently, Pr(9[^ =  1| 9^~1'> =  0) =  P r (9 ^  =  
O l ^ - 1* =  1) =  2p{\ — p). Taking pj =  P r (9 ^  =  1) gives pj =  p2p j-i +  
b where b =  2p(l — p). The solution for pj is pj =  p2<ji+1^po +  b( 1 — 
P2(j+1))/(1 -  P2)-
The transition matrix formed by the marginal chain {9[^)j>o has eigen­
values 1 and p and therefore the rate of convergence is \p\. Ergodicity of the 
chain is ensured if p >  0 but if p is close to 1 or 0, this rate will be close to 
1, the chain will tend not to move and convergence to the limiting distribu­
tion is very slow. In the limit, pj —* 6/(1 — p2) =  1/2 as expected. However, 
if p =  0.999, p =  0.998 and, after 100 iterations, pioo =  0.667po +  0.165 
which is still far from the appropriate limit.
The point raised in this example is far from rare in many applications 
and although correlations as high as 0.998 are not common, a similar effect 
is obtained with high dimensional parameter spaces with much smaller 
correlations. Once again, it seems sensible in these cases to reparametrize 
the model. This point was already discussed in the previous sections and 
will be returned to in Section 5.5.

Convergence diagnostics
159
5.4.2 Informal convergence monitors
Gelfand and Smith (1990) suggested a few informal checks of convergence 
based on graphical techniques. After m iterations in n parallel chains, a 
histogram of the n values of the mth iterates of a given function of 6 
can be plotted. This function can be one of the components of 9 and the 
histogram may be smoothed if desired. The procedure is repeated after 
a further k iterates are obtained in the chains. The value of k does not 
need to be large if one suspects convergence after m iterations. It cannot 
be low as the chain correlation will still be affecting possible similarities 
of the histograms. Typically, values between 10 and 50 are reasonable. 
Convergence is accepted if the histograms cannot be distinguished.
Same ideas can be used with a single chain. A trajectory of the chain 
exhibiting the same qualitative behavior through iterations after a transient 
initial period is an indication of convergence. Similarly, the trajectory of 
the ergodic averages can be evaluated and plotted. An asymptotic behavior 
over many successive iterations indicates convergence. Figure 5.4 shows the 
ergodic averages of variance components in a nested random effects model 
(Gamerman, 1997). The indication of convergence for both components 
seems to be very clear.
Similar ideas can be used with graphical representations of the simulated 
values of a few chosen (transformations of) parameters. The resulting plots 
provide a rough indication of stationarity behavior when the sequence of 
values tends to concentrate around the same pattern. This visual impression 
can be reinforced when chains started at different values oscillate in the 
same region.
E xam ple 5.6 Souza (1999) considers a number of hierarchical and dy­
namic models to describe the nutritional pattern of pregnant women. The 
data depicted in Figure 5.5 consist of the weight gains of I =  68 preg­
nant women at 5 to 7 visits to the Instituto de Puericultura e Pediatria 
Martagao Gesteira from the Universidade Federal do Rio de Janeiro. One 
of the simplest models she adopted was the simple hierarchical regression 
on time where
yij\ati, fa, (j> 
~  
N (a t +  faUj, a2) ,
{oii, /3i)'\a, (3 
~  
N((a,(3)',diag(T~l ,T p1)),
(a, (3)' 
~  
N m 0 Y , d i a g ( P - \ P ^ ) ) ,
prior independent scale parameters a~2, ra and Tp ~  G(a,b) and yij and 
Uj are the jth  weight measurement and visit time of the ith women, j  =
1, . . .  , r i i ,  i =  1 , . . . , / .  Here, n =  YliL 1 ni =  427, Pa =  Pp =  1/1000 and 
a =  b =  0.001.
The Gibbs sampler can be applied after calculation of the full conditional 
distributions. These are all normal and Gamma distributions which can

160
Gibbs sampling
(a)
------------------------------------------------------------------------------ ----
0 
2000 
4000 
6000 
8000 
10000 
iteration
(b)
---------------------------------------------------------------------------------------------------
°  
0 
2000 
4000 
6000 
8000 
10000
iteration
Figure 5.4 Ergodic averages of two parameters with number of iterations of the 
chain. The parameters are standard deviations of random effects at: (a) indi­
vidual; (b) unit level in a longitudinal study of epilepsy treatment (Gamerman, 
1997).
be easily sampled from (see Exercise 5.9). Figure 5.6 shows traces of some 
model parameters for two parallel chains started at different points. It seems 
to indicate convergence after around 1500 iterations.
These techniques must be used with caution and should always be ac­
companied by some theoretical reasoning. Graphical techniques may be 
deceptive indicating constancy that may not be so evident under a dif­
ferent scale. More importantly, there are many chains that exhibit every 
indication of convergence without actually achieving it. They are called 
metastable chains and are the subject of much research in probability the­
ory (Capocaccia, Cassandro and Olivieri, 1977; Cassandro et al., 1984).
E xam ple 5.5 (continued) It was established that ifp  is close to 1 or 0, the 
chain will tend not to move. Therefore, the observed trajectory will have 
long constant stretches indicating a metastable behavior. This is known not 
to be an indication of convergence because ir(6i =  j)  =  1/2, i =  1,2, 
j  =  0,1, and convergence will only be achieved when the chain begins to 
alternate values 0 and 1 with very similar frequencies.

Convergence diagnostics
161
Figure 5.5 Data on the weight gain of pregnant women (Souza, 1999).
5-4-3 Convergence prescription
Raftery and Lewis (1992) proposed a method to establish the length of a 
chain required for a MCMC run. More specifically, the methodology sug­
gests values of m, the number of burn-in iterations, k, the number of it­
erations to be skipped between stored chain values and n, the size of the 
sample values that must be stored to achieve a given Monte Carlo precision 
of estimates.
The setting for these choices is the estimation of u, the q quantile of a 
given function tp =  t(9), i.e. q =  P rv (tp <  u). The method requires that 
the Monte Carlo estimate q satisfies Pr(\q — q\ <  r) =  s. A common choice 
is the tail probability with q =  0.025 in which case u is the lower limit 
of the equal tail 95% posterior credibility interval for tp. One may require 
that the value of this probability be estimated in a MCMC run with error 
smaller than r =  0.01 with confidence s =  0.99. So, 95% posterior intervals 
would be given by intervals with posterior probabilities between 93% and 
97% with 99% confidence. This confidence level is due to the estimation of 
q by MCMC and should not be confused with posterior uncertainty about 
tp, governed by tt.
This problem is tackled at the simpler level of a binary chain 
=  
I(tp^) <  u) where xp^ is the value of tp at the jth  iteration of the MCMC

162
Gibbs sampling
(a )
<b)
(c)
1000 
2000 
3000 
4000
Iteration
i--------1--------1--------1----------1
0 
lOOO 
2000 
3000 
4000
Figure 5.6 Traces of model parameters with number of iterations of the two 
chains. The parameters are: (a) P, the populational growth; (b) a, the obser­
vational standard deviation; (c) ra, the precision of the population of intercepts;
(d) Tp, the precision of the population of regression coefficients. First set of initial 
values: a* =  0 and (3i =  0 for i =  1,..., I, a =  (3 =  0, r =  rQ =  Tp =  1.0. Second 
set of initial values: a* =  — 4 and pi =  0.5 for i =  1
a =  —4, (3 =  0.5, 
T =  Ta =T/3 =  1.0.
for 9. Z &  is derived from a Markov chain but is not a Markov chain. It is 
reasonable to assume however that dependencies between iterations fall off 
quickly with lag and new chains zjf'* may be formed by taking the values 
of Z ^  at every A;th iteration. The value of k is chosen as the smallest lag to 
make the first order Markov chain preferable to the second order Markov 
chain for the chain Z ^ . A test using the BIC (Schwarz, 1978) is used to 
choose between the two models for every value of k (Raftery and Lewis, 
1992).
Once the value of k is chosen, the next step is to determine the num­
ber m =  km* of iterations to be discarded. This is done by choosing m*

(7)
such that at iteration m*, the chain Z yk ' has marginal distribution ar­
bitrarily close to the limiting distribution implied by 7r. This is equiva­
lent to requiring that 
^ =  1
|
=  j)  — P rv (tp < u)\ < e and
|Pr(z[m ^ =  0|Z ^  
=  j)  -  
>  u)| <  
e, j  =  0,1. This is obtained
using the results from Example 
4.5 as
log- ( £(a+0) 
^
,  
I m a x {a ,/3 }
777 
—  
-------------------------------------------------—
log |1 -  Q -  j3\
where a and (3 are the (0,1) and (1,0) elements of Pk and Pk is the tran­
sition matrix of the chain 
(Exercise 5.10).
The value of n =  kn* is chosen using a central limit theorem for the 
chain z jf\  Note that q is estimated by the ergodic average q =  Zk,n =  
(1/n) 
Z^n+:i\ that has asymptotic variance given by r 2 =  a(3{2 —
a — (3)/{a +  (3) 3 . The approximating distribution for q gives
n* =  ( p
^
- ) 2
where z7 is the 7 quantile of the N (0,1) distribution. As an assessment of 
the magnitudes involved, in the most favorable case of independent sam­
pling with k =  1, estimation of the quantile 0.025 with largest error 0.0125 
with 95% confidence level requires n =  600 iterations. When the error is 
reduced to 0.005, the number of iterations required increases to 3746.
So, one must specify the quantile of interest q, the convergence tolerance 
e, the estimation tolerance r and confidence level s in advance. In addition, 
a pilot run must be observed to estimate the values of a and j3. An appro­
priate run length will then be prescribed. This run can be used to refine 
the estimates of a and (3, suggesting an iterative procedure.
This diagnostic obviously depends on the chosen ip andquantile. Raftery 
and Lewis (1996) recommended using it for all quantities of interest with 
q =  0.025 and q =  0.975 as the tails are harder to estimate in general. 
These provide a collection of values of k, m  and n and the largest of each 
is chosen. If I chains are to be used, then each should be run for m  +  n/l 
iterations to ensure convergence. Note that the procedure does not require 
any information about the chain itself, just its output. Brooks and Roberts 
(1998) provided an example of a slow convergence chain where severe under­
and over-estimation of the required length are observed. They pointed out 
the use of marginal indicators and estimation of a and (3 as the weak 
points of the method and suggested its use alongside other convergence 
diagnostics.
E xam ple 5.6 (continued) Two chains were run for 4000 iterations to ob­
tain the prescribed values of the spacing k, the burn-in period m  and the 
sample size n retained for inference. Based on the observed chain values
Convergence diagnostics 
163

164
Gibbs sampling
Parameter
k
TO
n
a
1 (3)
2(18)
3866 (15759)
a
1 (1)
3(2)
4112 (3946)
Ta
1(1)
3(3)
4285 (4112)
T0
2(1)
8(2)
8128 (3787)
Table 5.3 Convergence prescription summary for data on pregnant women. Val­
ues in parentheses refer to the second chain.
these are given for the two chains according to values in Table 5.3 with 
q =  0.025, r =  0.005 and s =  0.95. They seem to indicate that the chain 
lengths used are generally appropriate. Figure 5.6 suggests that the bum-in 
periods have been mostly underestimated. Note also that different starting 
values may provide large variation in the prescribed values.
5-4-4 Formal convergence methods
As in the previous subsection, the methods presented here diagnose con­
vergence based on exploration of the statistical properties of the observed 
chain. The methods here attempt to decide whether convergence can be 
safely assumed to hold rather that prescribing the run length to achieve 
convergence. There have been many methods presented in the literature. 
Most of them are covered by the review papers of Brooks and Roberts
(1998) and Cowles and Carlin (1996). Only a few of the most cited and 
used in the literature are presented here.
Time series analysis
Consider a real function ip =  t(9) and its trajectory ip^  ■> 
... obtained
from ip^  =  t(0^ ) ,  j  =  1, 2,... This trajectory defines a time series and 
ergodic averages of this series can be evaluated. Geweke (1992) suggested 
the use of tests on ergodic averages to verify convergence of the chain based 
on the series ip^).
Assume observation of the chain for to +  n iterations and form averages
- 
m +rib 
- 
m + n
and 
^a — —
Tlh 
na
j = m + 1 
j = m + n —n a + 1
where nt, +  na <  n. If to is the length of the burn-in period, then tpa and 
\pb are the ergodic averages at the end and beginning of the convergence 
period and should behave similarly. As n gets large and the ratios na/n

Convergence diagnostics
165
All 4000 draws
Last 2500 draws
Parameter
1st chain 
.2nd chain
1st chain
2nd chain
P
0.241
-2.936
-0.508
-0.068
a
3.212
- 1.464
-0.552
-0.087
Ta
-0.001
-0.043
-0.063
-0.031
0.069
0.145
0.047
0.044
Table 5.4 Geweke diagnostic zg summary for data on pregnant women.
and rib/n remain fixed then
V 'a  —  
V 'b 
d AT/n 1 ^
zG =  
, 
^ 
-> N (0,1).
yVar(tpa) +  Var(tpb)
So, the standardized difference zg between the ergodic averages at the 
beginning and at the end of the convergence period should not be large 
if convergence has been achieved. Large differences indicate lack of con­
vergence but small differences do not imply convergence. Geweke (1992) 
suggested the use of values nb =  O.ln and na =  0.5n and used spectral 
density estimators for the variances. This is a univariate technique but can 
be applied to posterior density by taking t(9) =  —2 log n{6). As with the 
Raftery and Lewis (1992) diagnostic, it requires only the output from the 
chain and can be used with any MCMC scheme.
E xam ple 5.6 (continued) The values of zq for both chains based 
on all
4000 iterations and on the last 2500 iterations are given in Table 
5.4- Apart
from cr (both chains) and P (second chain), all the other parameters exhibit 
convergence. When the first 1500 iterations are discarded, all parameters 
appear to have converged. Further insight into these values is provided in 
Figure 5.7, displaying the values of zq for each of the four parameters 
after removal of a given number of iterations. The figure seems to indicate 
convergence after around 1200 iterations, confirming results from Figure 
5.6.
Further exploration of the time series structure of the chain to study 
convergence of the chain has been the subject of research of a number 
of authors in the area of Operational Research (Heidelberger and Welch, 
1983; Schruben, Singh and Tierney, 1983; and references therein). These 
techniques investigate similarities between the observed series and their 
expected behavior under stationarity.

166
Gibbs sampling
(a)
”  1
+
(b)
, S '............
-10 
-5
i 
i
+
• ^  + 
♦* /  
-Hf
0 
1000 
2000 
3000 
4000
(c)
0 
1000 
2000 
3000 
4000 
<d>
*
;
 
'
+V
 
7 -
■Hf
I 
I 
i 
i 
i 
0 
1000 
2000 
3000 
4000
................+ .........
■Hf
I 
I 
- - I - 
I 
I 
0 
1000 
2000 
3000 
4000
Figure 5.7 Geweke convergence diagnostic zg for the two chains (1st chain: •, 
2nd chain: + ) and the four chosen parameters for the data on pregnant women 
plotted against the number of discarded iterations: (a) (3; (b) a; (c) rQ; (d) rp.
Multiple chains
Another simple method to check convergence is to use parallel chains 
started at different points. This technique explores the same ideas used in 
iterative optimization to avoid convergence to local maxima. Use of mul­
tiple chains would then prevent chains getting trapped in regions around 
local modes. Also, slow convergence may give rise to metastable behavior of 
the chains and this can be easily detected through parallel chains. Examples 
of this behavior are provided by Gelman and Rubin (1992b) and Gelman 
(1996). After convergence, all chains will have the same quantitative and 
qualitative behavior.
Gelman and Rubin (1992a) elaborated on the idea that the chain tra­
jectories should be the same after convergence using analysis of variance 
techniques. The overall idea is to test whether dispersion within chains is

Convergence diagnostics
167
larger than dispersion between chains. This is equivalent to the histogram 
of all chains being similar to all the histograms of individual chains.
The procedure starts by initializing the chains at points that are overdis­
persed with respect to the posterior distribution. The number of chains does 
not need to be too large to avoid computational waste and is typically given 
by single digit numbers. For components of 0 restricted to an interval, two 
chains initialized close to the limits of the interval is an adequate choice. 
For continuous components, a search for the mode(s) and respective curva­
ture^) (Section 3.2) can be set and initial states of the chains drawn from 
(mixtures of) Student’s t distribution(s) with moment(s) matching mode(s) 
and curvature(s). When there is indication of posterior multimodality, it is 
advisable to start at least one chain from each mode. Gelman (1996) pointed 
out that it is easy to adapt programs for calculation of Gibbs samplers to 
maximization of the posterior. All that is required is the substitution of 
random moves by deterministic moves in the direction of higher posterior 
density.
Considering m parallel chains and a real function ip — t(9), there are 
m trajectories {ip ^ , tp^2\ . . . ,  tp^'1'1}, i =  1 , . . . , m ,  for ip. The variances 
between chains B  and within chains W  are given by
m 
1 
m 
n
B =  
~ ^ )2 and w  =  —(-------tt X !  
2
m — 1 
m(n —
i —1 
v 
’  i= 1 j = 1
where xpi is the average of observations of chain i, i =  1 ,... ,m , and ip is 
the average of these averages. Under convergence, all these mn values are 
drawn from the posterior and a
the variance of tp, can be consistently 
estimated by W , B  and the weighted average 
=  (1 — 1 /n)W  +  (1 /n)B.
If, however, the chains have not yet converged, then initial values will still 
be influencing the trajectories. Due to their overdispersion, they will force 
to overestimate <r2 until stationarity is reached. On the other hand, 
before convergence, W  will tend to underestimate a^ because each chain 
will not have adequately traversed the complete state space. Following this 
reasoning, an indicator of convergence can be formed by the estimator of 
potential scale reduction given by R =  y j^ / W , that is always larger than
1. As n —> oo, both estimators converge to 
by the ergodic theorem 
and R —» 1. Convergence can be evaluated by the proximity of R to 1. 
Gelman (1996) suggested accepting convergence when the value of R is 
below 1.2. The original estimator proposed by Gelman and Rubin (1992a) 
is far more elaborate and its derivation is left as an exercise. It seems that 
the elaboration brings unnecessary complication as there are no formal 
tests applied to the statistic R.
E xam ple 5.6 (continued) The values of R were evaluated for the four pa­

168
Gibbs sampling
rameters previously chosen based on the two chains with 4000 iterations. 
For the scale parameters, a logarithmic transformation was used to improve 
the normality pattern of the posterior sample. They all lie below 1.05 provid­
ing further indication of convergence. At this stage, one can safely assume 
that after 2000 iterations all draws arise from the posterior distribution.
The potential scale reduction should be evaluated for all quantities of 
interest to provide reasonable information about convergence of the chain. 
Note that this is a univariate technique but, again, can be applied to the 
complete posterior density by taking t(9) =  —2 log n {6).
A problem of this method is the dependence on normal theory present 
in the choice of initial states of the chains and formulation of variance esti­
mators. Alternatively, non-parametric estimators of variance can be used. 
Also, reparametrizations may be applied to components expected to have 
non-normal behavior but this increases the complexity of the verification. 
Another problem is the inefficiency associated with multiple chains (Sec­
tion 5.3.1) which should lead to very parsimonious choices of the number 
of chains.
Methods based on conditional distributions
Assume that 9 can be divided in two blocks 6\ and 62- Then, n(9) =  
^(9^62)^[92) =  7r(#2|0i)7r(0i), for all 9. In the applications where Gibbs 
sampling can be used, full conditionals are easy to obtain but the marginal 
distributions are not. However, they can be estimated by (5.7) so let #(0*) 
be the estimate of 7r(0j), z =  1,2.
Zellner and Min (1995) proposed two criteria for verification of conver­
gence of the Gibbs sampler. The difference criterion is based on the statistic
fj =  tt(9i \92)tt(92) -  7r(<92|0i)7r(0i)-
If the chain has converged, then rj will be close to 77 =  0 for all 9. The ratio 
criterium is based on the statistics
~ _  7r(02|0i)7r(0i) 
, 
? _  ^{0i \92)tt(92)
^ - ^ { e * 2\ 9 D m ) 
an
where 9* =  (fl* ,^ ) 7 
another value from the state space. Both £1 and 
£2 are estimates of £ =  7r(0) / 7r(0*). If the chain has converged, then £1 
and £2 will be close. In addition, if they are close to £ then the chain has 
converged to the correct equilibrium distribution. Zellner and Min (1995) 
formalized their approach by assuming a normal sampling distribution for 
the estimates based on (3.8). They proceeded with a Bayesian analysis by 
assuming a vague prior distribution for the estimand, evaluating the criteria 
at a sample of 9 values, constructing credibility intervals and testing the 
hypotheses of interest.

Applications
169
Ritter and Tanner (1992) also proposed to assess convergence of the chain 
by looking at ratio statistics such as £1 and £2- They suggested evaluating 
the ratios at the chain values 0i'n> and plotting the histograms of the ratios. 
As n —► 00, these histograms should become closer to a degenerate distri­
bution at the value of 1. See also Gelfand (1992) for further discussion of 
analysis of histograms and Roberts (1992) for expressions of moments of 
ratio statistics for reversible Gibbs samplers. Again, metastable behavior 
may be a problem and use of multiple chains should remedy the situation. 
Another problem with these methods is the need for the expression of the 
full conditionals which restrict their application to Gibbs samplers. It is 
also not clear how to split the parameter into two blocks. When the pa­
rameter dimension d is large, there are too many ways of splitting them for 
it to be feasible to perform convergence checks in all of them.
Other methods
There are many other methods of convergence diagnostics proposed in the 
literature (see Mengersen, Robert and Cuihenneuc-Jouyaux (1999) for a re­
view). Liu, Liu and Rubin (1992) proposed a method based on control vari­
ables, Garren and Smith (1993) estimated the rate of convergence of chains 
formed by indicator variables and Johnson (1996) used coupled chains with 
overdispersed starting points.
Some of the advantages and disadvantages of the approaches have been 
discussed. The main points to consider are ease of implementation, appli­
cability to MCMC schemes, interpretability, dependence on chain structure 
and availability of software (Section 5.6). As previously mentioned, none of 
the schemes can guarantee convergence. So it is advisable that as many as 
possible are used in any given problem.
5.5 
A pp lication s
Applications of Gibbs sampling have been restricted so far to simple models 
or separate derivations of full conditional distributions. This section will 
provide a more complete treatment for a few special models. Inference 
via Gibbs sampling will be detailed for hierarchical models (Section 2.4), 
dynamic models (Section 2.5) and spatial models (Section 2.6).
5.5.1 Hierarchical models
Consider initially the 2-stage normal hierarchical model described at the 
beginning of Section 2.4 with
y\/3u <l> 
~  
N i X ^ r ' l n )  
0i\p2,C 
~  
N ( X 2fi2, C ~ 1)
~  
N (b,B )

indePendent of 
C ~ W  
nM-y —
where no, nw  and So are positive constants, b is an r-dimensional vector
of constants and B and Sw are r x r and d x d positive definite matrices 
of constants. The parameters of the model are the d- and r-dimensional 
vectors (3i and fi2 respectively, the scalar <p and the dispersion matrix C. 
Typically r <  d although this is not mathematically necessary.
The model includes as special cases the one-way classification model (Ex­
ample 2.8), the random effects model (Example 5.4) and the exchangeable 
regression model
yl\/3u (j> 
~  N{Xipi,<j>~1Ini), 
i =  1 ,... ,m
Pi\/32,C  
~  N(@2, C -1 ), 
j =  
1, . . . ,  m
(32 
~  N (b,B )
independent of 
C  ~  W  
nw^ w j
where yt =  (yli , . . . ,  ym,)\ 
i =  1, . . . ,  m. The analysis for this model using
Gibbs sampling is described and illustrated in Gelfand et al. (1990).
Other versions of this model are possible having, for example, an un­
known observational dispersion matrix or the dispersion matrix C pre­
multiplied by 4>. Also, the prior distributions can be changed to other non­
conjugate forms or more stages can be included.
The full conditional distributions for the blocks 0i, fi2 and 0 were ob­
tained in Section 2.4 as:
1- /?i|/?2i 0, C ~  7V(60, i?^);
2. fo\fafrC ~
3. ^
1,/32,C , ~ G (n 1/2 ,n 15 1/2);
where the expressions of b^, B^, b*, B*, ri\ and S\ were given there.
The novelty here is the assumption that C is unknown which requires 
the evaluation of its full conditional distribution. The density is given by
n(C\(3i,f32,<p) oc /w (/3i;^2/32,C _1) fw (C ;n w /2,nwSw/2) 
oc 
| C f/2exp j-^ tr K /? ! -  X 2/ W
1 - * 2/?2)'C] j
x 
|C|(" - - r+1)/2e x p | -it r [n lv5 vvC]J
oc 
\ C \ ^  exp { - ± t r [ ( n w Sw  +  (ft -  
-  X 2(32)')<?]}
170 
Gibbs sampling

Applications
171
which leads to
4. C\pi,/32,4> ~  W  (n*w j2,n*w S^/2) where n*w =  n w +  1 and 
=
ny/Sw +  (Pi — X 2p2)(Pl — X 2P2)' ■
A complete cycle of the Gibbs sampler involves successive sampling from 
the distributions given in steps 1 - 4 .  Generation from all these distributions 
is described in Chapter 1. Note that the blocks were naturally determined 
by the structure of the model.
E xam ple 5.6 (continued) The model used is a special case of the above 
models and draws from the posterior distribution can be obtained. The ap­
plication indicates convergence after 1500 iterations. Therefore, the remain­
ing 2500 values from the two chains can be taken to form a sample of size 
5000 from the posterior. The resulting histogram can be smoothed and the 
resulting marginal posteriors appear in Figure 5.8. They show a normal­
like form for the populational growth and gamma-like form for the scale 
parameters with more variation in the population of the as than for the Ps.
Another important aspect is the assumed normality of errors at all levels. 
This is an unnecessary restriction now and in particular thicker-tailed dis­
tributions as the Student’s t may be used. If the error distribution can be 
written as a (discrete or scale) mixture of normals, all full conditionals can 
be easily sampled from. In the context of exchangeable regression models, 
one may replace the first level equation by
Pi\p,\i,C ~  N (jj, X~lC ~l) and A, ~  Fa 
, 
i =  l ,...,m .
If F\ is a Gamma distribution, the regression coefficients are t distributed. 
In this case, the full conditional distributions of P =  (P\,. . . ,  pm)' and /x 
alter only by the substitutions of C  by \ C . The full conditional distri­
bution of C  now has 
=  nw Sw  +  ^iK (P i ~ Pi)(Pi ~ P2)'■ If F\ =
G (nx /2,nxSx/2), the full conditional distribution of A =  ( Ai , . . . , Am)' is 
given by
m
m
t t a ( A )  
oc n  M P u l ^ K ' C - 1) H f G(\i-,nx/2,nxSx/2)
171 
( 
\ 
1
«  
I I  A|” A+1)/2 exp 
-^[nxSx +  (pi -  p2)’C (pi -  p2) } j
i=1
(m + l)/2
and, a posteriori, the Ats remain conditionally independent with distribu­
tions G {(n x +  l)/2, [riA^A +  (Pi -  p2)'C(Pi -  /32)]/2 }, i =  1, ..., m. So, 
minor modifications in the already existing steps and the introduction of

172
Gibbs sampling
(b)
1.0 
1.1 
1.2 
1.3
40 
60
Figure 5.8 Smoothed marginal density estimates for data on pregnant women, (a) 
(3; (b) a; (c) ra; (d) Tp.
an additional step with independent Gamma draws are the only changes 
required to robustify the model.
5.5.2 Dynamic models
The dynamic models considered here are given by (2.21) -  (2.22) with 
constant variances of the observation and system disturbances, i.e., o f  =  a2 
and Wt =  W , for all t. This restriction is aimed mainly at presentation 
clarity. It also provides for a more parsimonious model. The extension to 
the general case of unequal variances is not difficult and is left as Exercise 
5.15.
As in hierarchical models, the specification of the model can be taken 
as a basis for blocking parameters. So, the natural choice is to form blocks 
Pi, . . . , p n,a 2 and W .
The full conditional distributions of the pt were obtained in Section 2.5.2

and are given by Tr((3t \0-t, &2, W ) =  N (bt, B t) where
f B t( a - 2Ftyt +  G't+1W - l/3t+l +  R - la) 
bt =  { Bt{ a - 2Ftyt +  G't+1W - lfit+i +  W ~ 1G tPt-\)
( Bt(cr~2Ftyt +  W ~ lGtj3t- i )
and
( 
(a ^ F tF I +  G '^ W -^ G t+ i +  R - 1) - 1 , t 
=  l
Bt = \  
(cr^F tFl +  
G ^ W - 'G t + i  +  W - 1) - '  , t =  2 ,... ,n — 1.
{ 
(a ^ F tF l +  W - 1) - 1 
, t  
=  n
Assuming now independent priors 0 =  a ~2 ~  G (na/2,naSa/2) and 
$  =  W ~ y ~  W (n w / 2 ,n w S w /2 ), their full conditional posterior dis­
tributions were also obtained in Section 2.5.2 as G(n^/2,n*S*/2) and 
^  (n vv'/21 nw S^y/2), respectively. Once again, both parameters are con­
ditionally conjugate.
These full conditional distributions complete a cycle of the Gibbs sam­
pler. They are all easy to sample from (see Chapter 1) and one can proceed
with a sampling-based Bayesian inference. This approach was introduced by 
Carlin, Poison and Stoffer (1992). They also extended the analysis to mod­
els with non-normal disturbances et and Wt and non-linear models. Non­
normality was introduced through scale mixtures of normals and therefore 
the same methods used in Section 5.5.1 can be applied here. Non-linearity 
was introduced with the replacement of the linear forms F[(3t and G tPt-i 
by arbitrary functions Ft((3t) and Gt (fit-1)- In these cases, the authors sug­
gested the use of rejection methods with a normal density q based on the 
linear parts of the model.
Unfortunately, this approach may be very inefficient. The system equa­
tion introduces prior correlation between system parameters (3=((3\,.. . ,  (3n)' 
This correlation is controlled by the system variance matrix W . The smaller 
their elements, the larger in absolute value the correlation between the f3t 
will be. This correlation is partially preserved in the posterior although its 
quantification is more complicated. In the limit, when W  =  0, the prior and 
consequently the posterior correlation is one. This is a highly-dimensional 
version of the same phenomenon depicted in Figure 5.3. The high dimen­
sionality of the state space brings convergence problems to Gibbs sampling.
Typically, the values of W  are much smaller than the values of a2. In 
this case, posterior correlation between state parameters will be high and 
the chain will tend to move slowly across the state space. As a result, a 
large number of iterations is required both for the burn-in period and for 
collecting the sample from the limiting distribution. In the latter case, it is 
advisable to retain draws from every kth iteration for a final sample with 
fixed size. The high chain autocorrelation will tend to force similar values 
at successive iterations and appropriate coverage of the parameter space 
will only be achieved with a large sample or with spacing between draws.
Applications 
173
, t =  1
, t =  2, . . . ,  n — 1 
, t =  n

174
Gibbs sampling
However, if the entries of the system variance matrix W  are large then 
the correlation between the components of f3 will be low and the Gibbs 
sampler will work well. In this case, the system parameters experience large 
variation and the very use of dynamic models becomes questionable; little 
information will be passed through model parameters. Dynamic models 
should be used when there is relevant passage of information through the 
system and in this case the correlation between model parameters will be 
high.
There are two alternative approaches to high correlation, both described 
in Section 5.3: block sampling and reparametrization. As previously dis­
cussed, it is generally preferable to sample correlated parameters in blocks 
when using Gibbs sampling. This is also possible for dynamic models by 
using Equation (2.27). It shows that the full conditional distribution of the 
block (3 is normal and can be decomposed in tractable densities that can 
be obtained and sampled from the updating equations. Incorporating ex­
plicitly the conditional on a2 and W , each term in (2.27) is given by Bayes’ 
theorem as
p(Pt\/3t+i,(r2, W ,yl) 
oc 
p(pt+i\pt,(T2,W ,y t)p(/3t\cT2,W ,y t) 
oc 
/w (/3 t+ i; G t/3t , W )/jv (/3 t ; m t, C t)
where the first term on the right hand side plays the role of the likelihood 
and the second term plays the role of the prior. Prom this perspective, 
the observations (3t+1 form a regression model with design matrix Gt and 
parameters Pt whose prior is given by the updating equations. It becomes 
easy to obtain that
(jS t lA + i.^ .W .j/* ) 
~  
N K G 'tW -'G t +  C ^ r ' i G ' t W - ' p t ^  +  C -'m t ) ,
(G'tW ~ 1G t +  Ct-1 )-1 ] 
(5.8)
for t =  1,... ,n  — 1 using results from Section 2.5 (Exercise 2.20). So, a 
scheme for sampling from the full conditional of the block (3 is given by:
1. 
Sample /?„ from its updated distribution (2.25) and 
set t =  n — 
1.
2. 
Sample /3t from the distribution (5.8).
3. 
Decrease t to t — 1 and return to step 2 until t =  1.
Step 1 is obtained by running the Kalman filter from t =  1 to t =  n with
given values of a 2 and W . When running the filter, the updated means mt 
and variances Ct, t =  1, . . . , n, are stored for use in step 2.
The above sampling scheme draws a value from the full conditional 
7t(/3|<72, W ). It was independently proposed by Carter and Kohn (1994) and 
Friihwirth-Schnatter (1994) and is widely known as the forward filtering- 
backward sampling (FFBS) algorithm, with mt and Ct computed in a for­
ward filtering step and fix,.. ,,/3n sampled backwards from Equation (5.8). 
Examples in these papers and in Shephard (1994) show that convergence

Applications
175
becomes orders of magnitude faster than sampling each fit at a time. Also, 
the computational cost of each iteration is higher but comparable in mag­
nitude to the cost of obtaining and sampling from the full conditionals of 
the fit.
A more numerically advantageous way of obtaining a sample from fi is 
by directly evaluating the prior full conditional of /?, which turns out to be 
N (A , P ~ l), for P  block tridiagonal (see Exercise 5.17). The likelihood is 
now given in matrix form by y\fi, a 2 ~  N (F fi, a2In), where y =  (yi, . . . ,  yn) 
and F  =  diatj(F[,. . . ,  F'J. Combining prior and likelihood leads to the 
posterior 6\a2, W, yn ~  N (M ,Q ~ 1) where Q is a block tridiagonal matrix 
(see Exercise 5.18). Great computational advantages can be obtained from 
the sparseness of Q. In particular, fast inversion algorithms can be used to 
ensure that samples from /? are efficiently drawn (Rue, 2001). See Migon 
et al. (2005) for further details.
In many situations, it is computationally feasible to sample a2 and W  
from their joint marginal posterior tt(a 2,W ). In such cases, block sam­
pling reduces to sampling the whole parameter vector jointly by noticing 
that 7r(/3, o’2, W ) =  n(fi, a2, W)Tt(a2, W ). Thus, a sample from fi, a 2, W  is 
obtained by: (1) sampling (a2,W ) from 7r(cr2, W ), and (2) conditional on 
sampled values of (a2, W ), sampling ft using the FFBS algorithm. Details 
about this sampling scheme are presented in Section 6.5.2.
The other alternative is reparametrization and was explored by Gamer- 
man (1998). The main source of correlation between the fit is induced by 
the system equation. On the other hand, the fit can be completely deter­
mined by the values of fix and disturbances wt. The system equation can be 
rewritten as wt =  fit — G tfit-i, t =  2, . . . ,  n. Setting w\ =  fii and applying 
recursively the system equation leads to the inverse relation
t 
/ t-l
Pt =  Y j ( n
(=1 \fc=l
for t =  2, . . . , n  and fix =  w\. For the majority of dynamic models of 
interest, Gt — G, for all t. In this case, the above expression simplifies to
t
fit =  Y , Gt~lwi ■ 
i=i
The model can be written in terms of the new parameters Wt and their full 
conditionals can be obtained (Exercise 5.16). The disturbances are prior 
independent by construction but are not posterior independent. So, this 
scheme is not as efficient as block sampling fi. Nevertheless, it removes the 
main source of correlation, the system equation, from the sampling scheme. 
This ensures fast convergence of the sampling algorithm although in this 
case the computational cost of each iteration is higher. This reparametriza-

176
Gibbs sampling
tion will be explored in the next chapter in connection with non-normal 
dynamic models.
Reis, Salazar and Gamerman (2006) compare the performance of these 
sampling schemes in the context of the first order normal dynamic linear 
model. Section 6.5.2 gives some details about the comparisons.
5.5.3 Spatial models
The spatial models considered here are the simpler members of the GMRF 
and distance-based GRF families with no covariates and only the intercept 
9 is varying in space. Derivation for the extensions where covariates are 
present and their effects are space-varying are left as Exercises 5.19 and 
5.20. Consider initially the GMRF model (2.33) and, as before, set <fi =  
1/a2. The full set of parameters is 9\,... ,9d,<fi and $  =  W -1 . The full 
conditional posterior for the 9,ts and for 9 were obtained in Exercise 2.23 
as:
1. 9i\0-i,a2, W  ~  N (m i, Cj), for i =  1, . . . ,  d;
2. 0\a2, W  ~  N (m ,C );
where values of rnts, C.ts, m and C are given in the exercise. In practical 
applications the value of d is usually large, making direct sampling of 9 
very slow. Rue (2001) suggested numerical techniques to take advantage of 
the sparseness (presence of many 0s) of C _1 and hence improve sampling 
considerably. These techniques basically involve appropriate reordering of 
the pixels to ensure a minimal diagonal band structure to C -1 followed 
by a numerically efficient Cholesky decomposition that takes advantage of 
this band diagonalization.
Assuming independent Gamma priors
„  ,'n a naSa \ 
( nw n w Sw \
G I —-, — -—  ) and 
~  G
2 ’ 
2 J 
V 2 ’ 
2 
/ ’
the full conditional posterior distributions of (fi and $  are given by
d
t x { 4 > \ 9 , W )  
o c  
Y \ p { y i \ 9 i , 4 > )  p{4>)
1 = 1
d
fG ( < f i \ n cr/ 2 , n a S a / 2 )
i = 1
cx 
/g (0 ; n*a/2,n*aS*/2) and 
7r($|0,</>) 
oc 
p ( 9 i , . . . , 9 d \ $ )  p($>)
CX
|-|5>«<* - 'W2} to (*; n-f.
oc 
/g ($ ; ?ity/2, riwSw/2)

Applications
177
wheren* =  na+d, n*aS* =  naSa +  'Ei(yi- 8i)2, n*w =  nw +d&nd n*w Sw =  
nwSw +  E k j  Wij(6i -  8j)2. Therefore, a posteriori, <p ~  G(n*/2,n*S*/2) 
and $  ~  G (n ^ / 2, n ^ S ^ / 2) and all parameters of this model exhibit con­
ditional conjugacy.
Thus, one can sample each scalar parameter individually or sample jointly 
all components of 6 =  (#1, . . .  , 8d)- It is als° computationally feasible 
to sample all parameters (8,cp,W ) jointly by noting that n(8, <f>, W ) =  
n(6\<p, W)n(<p, W ). Since iv(8\(p, W ) is available in closed form and tt(8, <p, W ) 
is known up to a proportionality constant, 7r(<p, W ) can also be obtained up 
to a proportionality constant. This sampling strategy was also presented 
for dynamic models. Details of this sampling scheme are left for Section 
6.5.4.
Blocking is still beneficial in the spatial context but to an extent smaller 
than for dynamic models. Empirical evidence from Gamerman, Moreira 
and Rue (2003) did not present large differences between these sampling 
schemes but ranked them in the increasing order of blocking. Knorr-Held 
and Rue (2002) also advocated the importance of blocking as much as 
possible in the spatial context based on their empirical evidence.
Consider now the distance-based GRF model (2.38) and, as before, set 
(p =  1 j o 2. The full set of parameters is 8\,. . . ,  84, \I/, with 
=  (fi, <A, t 2, A). 
The full conditional posterior for the model parameters are obtained as
1. 
~  N (m g,Cg), with me =  C $(t~ 2R ^ 1ldP+<py) and C^ 1 =  t ~ 2R ^ 1+  
(pld, and R\ as defined in (2.37) implied by the assumed correlation func­
tion;
2. fi\8, (p, r 2, A ~  N(m ^, Cfj), with mtl =  C^(T~2VdR'^l8-\-R~la) and CM =  
(T -2i>dR-x l i d +  R -'y ,
3. 4 > \ 8 , h , t 2 , \  ~  G(n*/2, n*5*/2),  with n* =  na +  d, n*aS* =  naSa +  
^ [ y t - 8 ( Sl)}2-
4. t 2\8, fi,<p, X ~  IG(n*/2,n*S*/2)i with n* =  nT +  d, n*S* =  nTST +  Q\ 
with Qx =  {8 -  1 dP)'FL\ (8 -  l d/i);
5. X\0,n,(p,T2 ~  F\ with density
tt(A|i9,/x,0 , t2) ocp(A) |i?Ar 1/2e x p {-^ < 5 A }, 
(5.9)
which has no amenable form for direct sampling;
The derivation of the full conditional posterior distributions above is left 
as Exercise 5.20.
A major computational problem arises here when d is large. The inversion 
of the square matrix R\ of order d is required at every iteration, slowing 
down the computations. Nevertheless, it is expected in the case of large d to 
have many (possibly most) entries in R\ with negligible values due to many 
small correlation values. This fact can be used to design approximation

178
Gibbs sampling
methods to band diagonalize this matrix and ensure application of fast 
inversion algorithms (see Rue and Tjelmeland (2004) for more details).
5.6 M C M C -based software for Bayesian modeling
One of the greatest impediments on the development of Bayesian inference 
was the difficulty of its implementation in practical situations. This dif­
ficulty was due to a host of possibilities for the specification of the prior 
distribution and to the difficulty of summarization of the resulting poste­
rior distribution. The first source of difficulty is being eliminated by the 
introduction of symbolic languages that accommodate many specifications 
in a computational system. The second source of difficulty was greatly 
eliminated by the introduction of MCMC methods such as the Gibbs sam­
pler that allow the analysis of complex models through decomposition and 
sampling from full conditional distributions.
Any system capable of specifying a variety of prior distributions for any 
given model and sampling from the resulting full conditionals would solve a 
great number of Bayesian problems. One such system is BUGS (Spiegelhal- 
ter et al., 1995), which stands as an acronym for Bayesian inference Using 
Gibbs Sampling. BUGS is a system developed at the Biostatistics Unit of 
the Medical Research Council, United Kingdom. Since its advent, BUGS has 
been expanded in several fronts with the implementation of Metropolis- 
Hastings steps and its Windows version, WinBUGS. WinBUGS was developed 
jointly with the Imperial College School of Medicine. BUGS consists of a 
set of functions that allows specification of models and probability dis­
tributions for all its random components (observations and parameters). 
Model specification is surprisingly simple given the complexity of models 
that it can tackle. Among those models already analyzed with WinBUGS and 
described in its manual (Spiegelhalter et al., 2003) are generalized linear 
models with random effects, regression analysis of survival data, models for 
spatially dependent data and non-parametric smoothing models.
For each combination of data set and model, BUGS outputs samples of 
model parameters at every k > 1 iterations after m iterations. The values 
of k and m as well as sampled parameters to be stored are chosen by the 
user. In addition, it provides sample-based estimates of posterior mean and 
credibility interval for the parameters. Both the system language as well 
as data input and output follow the syntax of the S -plus and R languages 
thus providing a useful interface for other data manipulations the user may 
wish to entertain. The system is freely available at the BUGS Project official 
web site www. m rc-bsu. cam. a c . uk/bugs/.
The system recognizes conditional conjugacy and uses it to sample effi­
ciently. Failing that, it uses rejection and adaptive rejection methods (Sec­
tion 1.5) or the Metropolis-Hastings algorithm (Chapter 6). The latest re­
lease of WinBUGS at the time of writing, version 1.4, already allows for

MCMC-based software for Bayesian modeling
179
specification of multivariate normal and Wishart distributions, block sam­
pling and graphical model specification. WinBUGS can be run in batch mode 
by using, for example, the R2WinBUGS package (Sturtz, Ligges and Gelman, 
2005). R2WinBUGS is one of the R contributed packages and can be found 
at c r a n .r -p r o je c t .org/src/contrib/D escriptions/R 2W inB U G S.htm l.
E xam ple 5.7 The data in Figure 5.9 describe the evolution in the height 
of teeth of children through time and was introduced by Elston and Grizzle 
(1962). In addition to the dynamic component of the trajectories through 
time, there is also the hierarchical component of similarities between tra­
jectories. Gamerman and Smith (1996) proposed a mixture model that in­
corporates both aspects above while still preserving the individuality of the 
series. Their model was given by
yti 
~  N(Qti, a2)
dtl 
~  ( l - p ) N ( e ^ hi +  Xth W 1)+ p N (fit,V 1)
Xti 
~  ( l - p ) N ( X t- 1,u W2) + p N ( l u V2)
fit 
~  N (n t-i +  It, Wi)
71 
~  N {i t -i ,W 2)
and was completed with independent vague priors 
and 71 ~  N (0 ,103), 
a 2, Vi, V2, Wi and W 2 ~  7G(0.1,0.1).
age (years)
Figure 5.9 Data on the height of the ramus tooth in boys. Each trajectory repre­
sents the evolution of the height for one boy.
The analysis was performed using BUGS. Appendix 5.A below illustrates 
how the model is described in BUGS. It is written as in S (or R) language 
with the added bonus of probabilistic attributions. Note that discrete mixture

180
Gibbs sampling
of normals is not a common distribution and is not directly available. It is 
reproduced in BUGS using the same device of indicator variables described in 
Section 3.2. For this data set, despite the dynamic nature of the model, a 
single long chain was used. The very short time length of the series reduces 
the problems caused by the correlation of successive state parameters. After 
discarding the first 10000 iterations the next 1000 iterations constituted the 
sample used for inference. Part of the results is exhibited in Figure 5.10 
where the mixture character of the model is evidenced.
Figure 5.10 Summary of estimation - posterior histograms of: (a) a; (b) Vi; (c) 
W 2 ; (d) p; (g) 6 2,12; (h) 04,is - Point estimates (full line) and 90% credibility limits 
(broken lines) for populational parameters: (e) (Mt; (f) 71- Note the difference be­
tween the histograms of individual levels in agreement (82,12) and in disagreement 
(84,18)  with the populational levels.
E xam ple 5.8 Nobre, Schmidt and Lopes (2005) proposed a Poisson time 
series version of the spatial model (2.33) when investigating the effects of 
rainfall (x) on malaria incidence (y) in Para, one of Brazil’s largest states, 
for a number of years. They used a Poisson-normal model where malaria 
counts for any two counties (areal data) are conditionally independent and 
Poisson distributed. For counties i =  1, . . . ,  d and years t =  1, . . . ,  n,
yit 
~  P oi(e6ii)
8 u  =  
oct +  (3tX it  +  bu

with a t \at-\ ~  N (a t- i ,  t2), (3t\Pt-i ~  N((3t- i,Tg), a 0 ~  A^(0.001,1000) 
and Po ~  JV(0,tJ). Four model structures were considered for spatio-tem­
poral variation of parameters bn ■
M i : 
bit ~  A^O.of) 
and <x2 ~  JG (1,1) ;
M 2 : 
bu ~  C A R (w ,a 2) 
and o f ~  /G (l, 1 ) ;
M 3 : 
bi t ~ N (0 ,a j) 
and at2 ~  LAT(log(crt2_ 1), t 2) ;
M 4 : 
bu ~  C A R {w ,a?) 
and o f ~  LiV(log(f72_ 1), 
t 2),
w/iere w denotes the weights given by neighbor indicators and L N (n ,a 2) 
is the lognormal distribution such that X  ~  LN(/i,a2) if and only if 
log(X ) 
N {n ,cr2). For models M 3 and M 4) log(crf) ~  N (0 ,t2). The prior 
distributions for the hyperparameters were t ~ 2 ~  G ( l , l ) ,  rg 2 ~  G ( l , l )  
and t 2 ~  G(l, 1 ).
The analysis was performed using BUGS. Appendix 5.B below illustrates 
how the model is described in BUGS. Figure 5.11 shows the posterior me­
dian and associated 95% credibility interval for the rainfall coefficient f3t 
estimated under each of the four models. It is noticeable that the effect of 
rainfall:
(a) is significant for models M\ and M 3 and not significant for models M 2 
and M u
(b) is negative for all models;
(c) drops quite abruptly for models M\ and M 3 around December of 1997, 
probably due to the amount of rainfall predicted in October of 1997.
Convergence diagnotics of a BUGS output can be performed through 
CODA (Best, Cowles and Vines, 1995). The latest version of CODA is 0.5­
3 
and is freely available from www-fis.iarc.fr/coda/. A similar soft­
ware is BOA. Its latest version is 1.1.5 and is also freely available from 
www.public-health.uiowa.edu/boa/. CODA stands for Convergence Diag­
nostics and Output Analysis and BOA stands for Bayesian Output Analysis. 
CODA and BOA are systems that may but do not need to be used in conjunc­
tion with WinBUGS and have versions available for use in R/S-PLUS. CODA 
is being maintained and distributed by the same research group responsi­
ble for BUGS. Both CODA and BOA contain many summarizing statistics and 
the convergence diagnostics of Gelman and Rubin (1992a), Geweke (1992) 
(also available in BUGS), Raftery and Lewis (1992) and Heidelberger and 
Welch (1983). All simulations of Example 5.6 were made with BUGS and 
the convergence diagnostics of Table 5.3 were made with CODA.
Additional software for Bayesian analysis is an increasingly large list. For 
example, BayesX is a software for Bayesian semiparametric regression and 
can be freely downloaded from www. stat .uni-muenchen.de/~bayesx. As 
of now, R contains more than a dozen modules for Bayesian inference via 
MCMC to a broad range of statistical models.
MCMC-based software for Bayesian modeling 
181

182
Gibbs sampling
111111II111111111111111111
1 -9 6  
1 1 -9 6
11111111
9 —97 
6 -9 8
Figure 5.11 Posterior median (solid line) and 95% credibility interval (dashed 
line) for the rainfall coefficient pt under models: M i, M 2, M 3 and M 4.
Appendix 5 .A : BUGS code for Example 5.7
model mixture; 
const
m = 20, # number of series 
n = 4 ; 
# number of time periods
var
y[n,m ],theta[n,m ], lam bda[n,m ],k,kl,m il[n,m ,2],m i2[n,m ,2], 
v il[n ,2 ],v i2 [n ,2 ],m u [n ],gamma[n],eta[n],p,sigm a,taul, 
tau2,wl,w2,w0; 
data in "ramus.dat"; 
in its in "ram us.in";
{
# Prior specification for hyperparameters
sigma ~ dgamma(0.1 ,0 .1 ) ;
taul 
~ dgamma(0.1 ,0 .1 );
tau2 
” dgammaCO.1 ,0 .1 );
wl 
~ dgamma(0.1 ,0 .1 ) ;

MCMC-based software for Bayesian modeling
183
w2 
~ dganuna(0.1,0.1); 
p 
~ dbeta(l,1);
# Prior specification for 2nd. level parameters
wO 
<- 0.001 ;
mu[l] 
~ dnorm(0,w0); 
gamma[l] ~ dnorm(0,w0);
# Prior specification for 1st. level parameters
k 
" dbern(p);
k l  
< -  
1 
+  k ;  
v i l  [1,1] <- wO; 
v
i l [ 1 , 2 ]  
< -  
t a u l ;  
v i 2 [ 1 , 1 ]  <- wO; 
v i 2 [ 1 , 2 ]  
< -  t a u 2 ; 
f o r ( i  
i n  
l : m ) {  
m
i l [ l , i , l ]  < - 0 ; 
m i l [ 1 , i
, 2 ]  
< -  m u [ l ] ; 
m i 2  [ 1 , i
, 1 ]  
< -  
0  
; 
m i 2 [ 1 , i
, 2 ]  
< -  g a m m a [ 1 ] ;
theta[l,i] ~ dnorm(mil[1,i,kl],vil[1,kl]); 
lambda[l,i] ~ dnorm(mi2[1,i,kl],vi2[1,kl]);
# Observation equation
y[l,i] ~ dnorm(theta[l,i],sigma)
>
# Model specification for t=2,...,n
# Evolution for 2nd. level parameters 
for(t in 2:n){
gamma[t] ~ dnorm(gamma[t-l],w2); 
eta[t] <- mu[t-l]+gamma[t] ; 
mu[t] 
" dnorm(eta[t], wl);
# Evolution for 1st. level parameters 
vil [t, 1] <- wl; vil[t,2] <- taul; 
vi2 [t,1] <- w2; vi2[t,2] <- tau2; 
for(i in l:m){
mi2 [t,i, 1] <- lambda[t-1,i]; 
mi2[t,i,2] <- gamma[t];
lambda[t,i] ~ dnorm(mi2[t,i,kl],vi2[t,kl]); 
mil[t,i,l] <- beta[t-1,i]+lambda[t,i]; 
mil[t,i,2] <- mu[t];
theta[t,i] 
" dnorm(mil[t,i,kl],vil [t,kl]);
# Observation equation
y[t,i] ~ dnorm(theta[t,i],sigma)
>
>

184 
Gibbs sampling
Appendix 5.B BUGS code for Example 5.8
# BUGS works with precisions as opposed to variances.
# The prefix " i "  in isigma2, itau2.alpha, itau2.beta
# and itau2.sigma stands for inverse.
# For instance isigma2 is the inverse of sigma2. 
model{
for (t in l:n ) {
b [l:d ,t ] ~ car.normal(adj[ ] ,w eights[],num [],isigm a2[t]) 
for (i in l:n ) {
log(m u [i,t]) <- alp h a[t]+beta[t]*ch u va[i,t]+b[i,t]
Y [i,t]
dpois(mu[i,t ] )
isigma2[1] 
alpha[1] 
beta[l] 
logsigma2 [1] 
for (t in 2 :n ){ 
isigma2[t] 
alpha[t] 
beta[t] 
logsigma2[t]
}
alphaO 
itau2.beta 
itau 2. alpha 
itau 2. sigma
1/exp(logsigma2[1]) 
dnorm(alphaO, itau2. alpha) 
dnorm(0, itau2.beta) 
dnorm(0, itau2. sigma)
<- l/exp(logsigm a2[t])
" 
dnorm (alpha[t-l],itau2.alpha)
~ 
dnorm (beta[t-l],itau2.beta)
~ 
dnorm(logsigma2[t-1], itau2. sigma)
d fla t() 
dgamma(1,1) 
dgamma(l,1) 
dgamma(l,1)
5.7 Exercises
5.1 Consider Example 5.1. Prove Equation (5.3).
5.2 Consider the Gibbs sampler with transition kernel (5.4).
(a) Show that the chain with a complete scan over all components is not 
reversible.
(b) Under what conditions is the chain with transitions formed by individ­
ual changes on a single component reversible?
(c) Show that the chain that takes each iteration to consist of the com­
plete scan through the components followed by another scan through the 
components in reversed order is reversible (Besag, 1986).
5.3 (Casella and George, 1992) Let x and y be random quantities with
conditional densities f(x\y) =  ye~yx, x >  0 and f(y\x) =  xe~ xy, y >  0.

Exercises
185
Show that the only possible solution for f(x ) is f(x ) =  \/x, which is not a 
proper density, and that Gibbs sampling cannot be applied in this case.
5.4 Consider Example 5.1 but assume now that the regions A and B have 
non-null intersection for only one of the axes, for example, the 91 axis. 
Discuss whether conditions for convergence to the joint distribution are 
satisfied.
5.5 (MacEachem and Berliner, 1994; 0 ’Hagan and Forster 2004) Con­
sider the estimation of a real function ip =  t(9) from a stream of n succes­
sive values from a chain. Form k sub-samples of size m =  n/k by skipping 
every k iterations and assume that k is large enough to ensure approximate 
independence between sample values. Denote by ip\, . . . ,  ipk the averages of 
the sub-samples and ip =  (1 /k) Y ^=i 'i’j the average over the complete sam­
ple.
(a) Show that Var(ipj) =  Var(ip)/m, for all j.
(b) Use the Cauchy-Schwarz (or correlation) inequality C ov(x,y) 
< 
y/Var{x)Var(y) to show that Var{ip) <  Var{ipj).
5.6 Consider the simple linear regression model yi =  13i +  02%i +  e* where 
ei ~  N (0, a 2) are independent, i =  1, . . . ,  n, with non-informative marginal 
p(j3) oc k.
(a) Show that if a has components ai =  f3\ +  fo x  and 
=  fa then ai 
and c*2 are conditionally independent a posteriori given a2.
(b) Show that using a is equivalent to centering the covariate and using 
the model yi =  a i +  a 2(xj — x) +  ei-
(c) Generalize the result to multiple linear regression.
5.7 Show that for the random effects model of Example 5.4, posterior cor­
relations are given by
C o r d o n ) =  -  
+ 
and 
C or(au a j) =  ( l  +  ° ^ j^ j
in the original parametrization,
(  
UlT^ \ 
(  
TTIT^ \ 
^
C«iP,l3,) = - (  1 + ^ J  
“ d 
+
in the centered parametrization and
Cor(i/,£i) =  0 
and 
C o r (^ ,^ ) =
in the parametrization of Vines, Gilks and Wild (1996).
5.8 (O ’Hagan and Forster, 2004) Consider the situation of Example 5.5 
where 0 =  (91, 92) is bivariate and 1r(9) is given by the table of probabilities 
below

186
Gibbs sampling
O2
6>i 
0 
1
0 
p /2 
( l - p ) / 2
1 
( l - p ) / 2  
p/2
(a,) Obtain that tt (di) =  bern( 1/2), i =  1,2, and that the posterior corre­
lation between 6\ and 62 is p =  2p — 1.
(b) Show that P r (9 ^  =  1|0[J'-1) =  1) =  P r ^  =  O|0^-1) =  0) =  
p2 +  (1 — p)2 and, consequently, Pr(0[^ =  116*^—
=  0) =  P r(0^  =  
0|6»p_1) =  1) =  2p(l - p ) .
(c) Show that ifpj =  P r(9^  =  1), j  =  0,1, . . thenpj =  p2p j-i+ b  where 
b =  2p(l — p) and derive that pj =  p2^ +1^po 4- 6(1 — p2^ +1^ )/(l -  p2).
(d) Show that the transition matrix formed by the marginal chain ( 9 ^ ) j>0 
has eigenvalues 1 and p.
(e) Show that in the limit, pj —> 6/(1 — p2) =  1/2.
(f) Plot pj x j  for p =  0.999 for a given value ofpo-
5.9 Consider Example 5.6. Show that the full conditional distributions of
a,f3,Ta,Tp,cr~2,ai,(3i and (a,, A ) are:
„  
j\r / Tq  S i = l  
1 
I
a 
~  
iV 1 ----------------  
-------------- 1
Ta I  +  Pa 
’ Ta I  +  Pa
T0 E l l  Pi 
1 
N
T0
a~2
OCi
a 
T!jLi(yij -  Pikj) +  a r a
niO~2 +  ra
a 
AT ( a 2 J 2 % itij(Vij - a i ) + P r 0 
1 
^ __ j
P i 
rvy 
I 
_ q  
*2 1 
’ 
— 2 
+2 
i 
I
I 
O’ 2 E ,  = l t l + T0 
CT 2 £  • 1 t% +

Exercises 
187
where 6 — (cx,/3) , yi — (yn •> - • • > Vini) > ^ — (tn, - •■, tiUi) ; 
(In;> ti),
A -1 =  diag(Ta,Tp) and E» =  (A-1 +
5.10 Consider the chain 
described in Section 5-4 and its transition 
matrix
_  /  1 — a 
a
k -  v 
P 
1 -  P
(a) Show that the equilibrium distribution is (7ro,7ri) =  (P,a)/(a +  p) and 
that
p i _  (  tto 
tti \ 
A 
/ a  
- a
k 
V 7T0 TTl / a + 
P \ ~P 
P
where A =  1 — a — (3.
(b) Show that if P r(Z ^ n') — i\Z ^ =  j) is required to be within e from its 
limiting value 7r*, i.e., \ P r (Z ^  — i\Z^ =  j) — 7Tj| <  e, i ,j  — 0,1, then
( a+/3)e
I m ax(a,0 )
m > m =
log |A|
and therefore m =  m*k should be taken as the bum-in period.
5.11 Still in the conditions of Section 5-4-2, show
(a) using (4-H ) that for large n
7 
• M L  1 (2 - a - P ) a 0 '
Zk,n~N]q,n 
( q  +  /j )3 
_ 
■
(b) that the smallest n satisfying Pr(\Zk,n — q\ <  r) =  s is
„ _  (2 -  a  -  p )a p  f Z(i+ s) /2 -j 2 
71 
(a +  p f  
I 
r 
J
where z7 is the 7 quantile of the N (0, 1 ) distribution.
(c) that n* is minimized in the case of independent sampling in which case
1 — a =  P =  q and is given by
n ,  =  < ? (! ~  t i z ( i + s )/2
5.12 Consider m parallel chains and a real function ip — t(9). There are
m trajectories 
i>[2\ ■ ■ ■, 
i =  I, - ■■ ,m , for tp. The variances
between chains B and within chains W  are given by
m 
, 
m 
n

188
Gibbs sampling
where tpi is the average of observations of chain i, i =  1 , . . . ,  m, and ip is 
the average of these averages.
(a) Use the ergodic theorem to show that W , B and 
=  (1 — 1 /ri)W +  
(1 /n)B —> cr^, the variance of ip, when n —► oo.
(b) (Gelman and Rubin, 1992a) Let V =  a^,+Var(ip) andV =  a^,+B/mn 
be an estimator o fV . Show that E (V ) — V and if the distribution ofV / V  
is approximated by a x i  then v is estimated by the method of moments 
as
~ 
n 
V 2 
v =  2------- — .
V ar(V )
Note: the potential scale reduction estimator originally proposed was 
given by R =  
(V /W )(v/(v — 2)).
5.13 (Zellner and Min, 1995) Derive the central limit theorem for the Rao- 
Blackwellized estimator of marginal densities given by (5.7).
(a) Derive confidence intervals and a test for convergence based on the re­
sult and using the difference criterium statistic fj evaluated at m different 
states 6 i,. . . ,  6m.
(b) Assuming a vague prior for r), obtain its posterior distribution and 
construct credibility intervals for /].
(c) Repeat items (a) and (b) to derive tests and confidence and credibility 
intervals for convergence and for correct convergence based on the ratio 
criterium statistics.
5.14 Specify a version of the 3-stage hierarchical model and obtain the full 
conditional distributions required for implementation of the Gibbs sampler.
5.15 Obtain the full conditional distributions required for implementation 
of the Gibbs sampler for the dynamic model with observational and system 
variances having independent prior distributions IG (n att/^,na^Sa,t/^) and 
I W (n w,t/ 2 , n w,tSw,t/2) respectively, t =  1, . . .  ,n. Compare and discuss in 
this context the relative efficiency of the methods based on separate sampling 
of the fit, block sampling of [3 and sampling via the reparametrizations wt.
5.16 Show that
t 
f t -i
Pt=yi ( n  Gt-k+i 
i=i \fc=i
for t =  2,. . .  ,n  and ji\ =  wi and, if Gt =  G, for all t, the above expression 
simplifies to
t
pt = Y Gt~lwi •
/=i

Exercises
189
Show also that the full conditional distribution of wt is N (bt,B t) where
n 
n
bt =  B t ^
 a~2H ti(yi — kti) 
and 
Bt 1 =  W  1 +  
2HtiH'tl
l=t 
l=t
for t — 2 , . . . ,  n and
b i= B i R 
+  
2Hu{yi -  ku)
i=i
l=i
where H'a =  F^G1^  and ktl =  FI Z l= i,i?t G ^ v n , I > t, t =  1, . . . ,  n.
5.17 Based on the dynamic models developed on Section 5.5.2, show that 
((3\a2,W ) ~  N (A , P " 1) with A! =  (/, G2, G'3G2, ■ ■ ■, rit=2 Gn -t+ 2) a ani^ 
symmetric, block tridiagonal precision matrix P, with main diagonal blocks 
Pu =  R - 1 +  G’2W ^ G 2, Pnn =  W ~ \  Ptt =  W - 1 +  G't+1W - 'G t+i, for 
t =  2,. . .  ,n — 1, and secondary diagonal blocks given by P[tt+i =  Pt+i,t =  
W ~ 1G t, for t =  1,... ,n -  1, where fli ~  N (a ,R ).
5.18 Show that combination of the likelihood y\(3,o2 ~  N (F f3,a2In), where 
y =  {yi,...,yn) and F  =  d iag(F [,. . . ,  F^), with the prior from Exercise
5.17 leads to the full conditional posterior /3|er2, W, yn ~  N (M , Q ~ x) where 
M  =  Q ~1(a~2F y +  PA ) and Q =  P  +  a~2F 'F .
5.19 Derive the expressions for the full conditional posterior distributions 
of parameters of model (2.33) with the presence of a regression term Xf3. 
Repeat the derivations considering now that the regression coefficients j3 
are space-varying.
5.20 Derive the expressions for the full conditional posterior distributions 
of parameters of model (2.38). Repeat the derivations for the model with the 
presence of a regression term X/3. Repeat the derivations considering now 
that the regression coefficients /3 are space-varying according to independent 
GRF for each covariate effect.


C H A P T E R  6
Metropolis-Hastings algorithms
6.1 Introduction
In this chapter, Markov chains known under a generic name of Metropolis- 
Hastings will be presented and discussed. This name stems from papers 
by Metropolis et al. (1953) and Hastings (1970). These are considered as 
basic papers for the characterization of the method, although other pa­
pers including Barker (1965) and Peskun (1973) have also brought relevant 
contributions to the method.
The original paper by Metropolis et al. (1953) deals with the calculation 
of properties of chemical substances and was published in the Journal of 
Chemical Physics. Nevertheless, it later proved itself to have a great impact 
in Statistics and Simulation.
Consider a substance with d molecules positioned at 8 =  (9i, . . . ,  8d)'■ 
In this case, the component (9, is formed by the bidimensional vector of 
positions in the plane of the ith molecule. From Statistical Mechanics, the 
density of these positions is given by Equation (5.1) where a potential V 
between molecules can be defined. The potential energy of the substance 
is then given by E(8) =  Y>tjV (8 i, 8j)/2.
The calculation of the equilibrium value of any chemical property is given 
by the expected value of this property with respect to the distribution of 
the vector of positions. Direct calculation of the expectation is not feasible 
for d large and is replaced by a Monte Carlo estimate. Metropolis et al. 
(1953) suggested the following method to deal with the difficult problem 
of sampling from this density:
1. Start with any initial configuration 8 ^  =  (8[°\ ... ,8 ^ ) ' and set the 
iteration counter j  =  1.
2. Move the particles from previous positions 8^~1^ =  (8[j 1\ . . . ,  8^ 
^)'
according to a uniform distribution centered at these positions in order 
to obtain new positions <fr =  ( 0 i , .. . ,  4>d)' ■
3. Calculate the change A E in the potential energy caused by the move. 
The move in step 2 is accepted with probability m in{l, e~cAE}, with 
c =  1/kT. If the move is accepted, 6 ^  =  4>. Otherwise, 8 ^  =
4. Change the counter from j  to j  +1 and return to step 2 until convergence 
is reached.
After convergence, the vector of positions generated by the method has

192
Metropolis-Hastings algorithms
distribution with density (5.1). It is evident that the above method defines 
a Markov chain as the transitions depend only on the positions at the 
previous stage. However, it is not obvious that the method converges to 
an equilibrium distribution and that this distribution is given by (5.1). 
Metropolis et al. (1953) present a heuristic proof of this result. The same 
proof is valid for the case where the moves to (f> are made according to 
any symmetric distribution centered at previous positions. This defines a 
transition kernel q that depends on (6,<j>) through \<j> -  91. Hastings (1970) 
referred to the above algorithm in this extended form as the Metropolis 
method. In the next section, a more general version of the algorithm and 
the proof of its convergence will be presented.
Note that the above algorithm includes an additional step that was not 
present in the chains previously presented. The transition mechanism now 
depends on a proposed transition q and a subsequent step of evaluation of 
this proposal. Note that the proposed positions are completely unrelated 
from the equilibrium distribution but this is represented in the overall tran­
sition through the acceptance probability because
n(4>) 
exp {—cE(4>)}
Tr{9U-i)) 
e x p {-c£ 7 (0 tf-i))} 
6XP* 
^ '
Another important point is that the resulting chain may remain in a low 
energy (or equivalently, high density) position for many iterations. In this 
case, it is likely that the proposal will lead to very high energy (very low 
density) points and A E  
0 forcing an acceptance probability very close 
to 0. Computationally, this is not desirable and transition kernels must be 
carefully chosen to avoid such low acceptance rates.
The next section presents a more common and complete version of the 
algorithm following the work of Hastings (1970). Important special cases 
are presented in Section 6.3 and in Section 6.4 variations of the method 
are discussed. These variations include blocking and the relationship with 
Gibbs sampling. Finally, application of the algorithm to the context of 
generalized linear models with hierarchical, dynamic and spatial structure 
is discussed. A very nice expository introduction to Metropolis-Hastings 
algorithms is also provided by Chib and Greenberg (1995).
E xam ple 6.1 It is common in pharmacology studies to specify concentra­
tion levels of substances introduced in a system by non-linear equations of 
the form
f(ip ,x ) = tpi +  
(6.1)
1p3 +  X 
v 
'
where %p =  (/ipi,ip2,'4’3) and x is an explanatory variable. Equation (6.1) 
represents a curve starting at ipi when x =  0 and advancing to the asymp­
totic value ipi +V>2 when x  —> oo. Carlin and Louis (2000, p. 233-234) used 
this model to explain the effect of the concentration x of Puyromycin (in 
ppm) on the velocity y of an enzymatic reaction (in counts/min2). Their

Definition and properties
193
x 
0.02 
0.02 
0.06 
0.06 
0.11 
0.11
0.22 
0.22 
0.56 
0.56 
1.10 
1.10
y 
76 
4 7 
97 
107 
123 
139
159 
152 
191 
201 
207 
200
Table 6.1 y : velocity of an enzymatic reaction (in counts/min2), x : substrate 
concentration (in ppm).
model assumes thatyi =  f(ip ,x l) +  ei, i =  l , . . . , n ,  and that observation er­
rors ei are normally distributed with zero mean and variance a 2. Table 6.1 
exhibits the n =  12 observations. Assume that ('0i, -02, cr2) =  (50,170,126) 
and set 
=  9. The prior for 9 is JV(0,100). The posterior distribution for 
9 is
n
7r(0) oc f N(0- 0,100) 11 f N(yu 50 +  170Xi/(0 +  a*), 126). 
(6.2)
i = l
Consider the transition kernel q{9,(f>) =  /jv(u; 0,0.01), where u =  9 — <j>. 
A typical path is presented in Figure 6.1. The horizontal segments of the 
trajectory represent iterations where proposed values were repeatedly not 
accepted. In these cases, the iteration values remained the same until a 
proposed value is accepted. In total, 188 values were accepted out of the 
1000 proposed, thus giving an average acceptance rate for this chain of 
approximately 18%.
In this setting, inference for 9 can be based on the sample obtained from 
running the chain. After 1000 iterations and discarding the first 100 itera­
tions, the posterior mean and standard deviation of 9 are approximately 
given by 0.132 and 0.013. A 95% credibility interval for 9 is given by 
[0.105,0.156].
6.2 Definition and properties
Consider a distribution tt from which a sample must be drawn via Markov 
chains. Again, it is worth stressing that this task will only make sense if 
the non-iterative generation of -k is very complicated or expensive. In this 
case, a transition kernel p(9, tp) must be constructed in a way such that tt 
is the equilibrium distribution of the chain. A simple way to do this is to 
consider reversible chains where the kernel p satisfies
n(9)p(9,4>) =  n(<p)p{(t>, 9), V (0, <t>). 
(6.3)
As previously seen in Section 4.6, this is the reversibility condition of the 
chain. Equation (6.3) is also referred to as the detailed balance equation.

194
Metropolis-Hastings algorithms
600
lOOO
iteration
Figure 6.1 Trajectory of the chain for 9 in Example 6.1 with initial value 
=  
0.4.
Even though this is not a necessary condition for convergence, it is a suffi­
cient condition in order that tt be the equilibrium distribution of the chain.
The kernel p(9, <fi) consists of 2 elements: an arbitrary transition kernel 
q(9,4>) and a probability a(9, <j>) such that
So, the transition kernel defines a density p(9, ■) for every possible value of 
the parameter different from 9. Consequently, there is a positive probability 
left for the chain to remain at 9 given by
for any subset A of the parameter space. So, the transition kernel defines 
a mixed distribution for the new state (j) of the chain. For 4> ^  9, this 
distribution has a density and for <f> =  9, this distribution has a probability 
atom.
Hastings (1970) proposed to define the acceptance probability in such a 
way that when combined with the arbitrary transition kernel, it defines a 
reversible chain. The expression most commonly cited for the acceptance
p(9,0) =  q(9,4>)a(9, <f>), if 9 ^  <j>.
These two forms can be grouped in the general expression
p (Q ,A )=  j  q(9,<p)a(9,(j))d(t> +  1(9 € A) 1 -  f  q(9,tj>)a(9,<f>)d4> 
(6.4)
J A

Definition and properties
195
probability is
<«■»>
Algorithms based on chains with transition kernel (6.4) and acceptance 
probability (6.5) will be referred to as Metropolis-Hastings algorithms from 
now on. This is an acknowledgement of the importance of the contribution 
from both papers. Hastings (1970) referred to the ratio appearing in (6.5) 
as the test ratio. A more general expression for a including (6.5) and the 
acceptance probability used by Barker (1965) as special cases is presented 
in Hastings (1970). Optimality of these choices can be discussed in terms of 
minimization of asymptotic variance of moment estimates. Peskun (1973) 
showed for the discrete case that (6.5) is optimal in a large class of choices. 
He also showed that for suitable choices of the proposal transition q, Markov 
chain sampling can be more precise than independent sampling.
The proof that (6.3) is satisfied by p given in (6.4) and hence defines a 
reversible chain with equilibrium distribution 7r follows directly from (6.5) 
and is left as an exercise. Note that (6.3) is satisfied by p but not by q. 
The proposal transition kernel q has up to now been kept arbitrary and 
is thus a flexible tool for the construction of the algorithm. Roberts and 
Smith (1994) showed that if q is irreducible and aperiodic and a(9, </>) > 0, 
for every possible value of {9,<p), then the algorithm defines an irreducible 
and aperiodic chain with transition kernel p given by (6.4) and limiting 
distribution 7r.
In practical terms, simulation of a draw from n using the Markov chain 
defined by the transition (6.4) can be set up as follows:
1. Initialize the iteration counter j  =  1 and set an arbitrary initial value 
0(°>.
2. Move the chain to a new value <fi generated from the density q(9(j~ l\ •).
3. Evaluate the acceptance probability of the move a(<9(j-1 \ </>) given by
(6.5). If the move is accepted, 9 ^  — (j). If it is not accepted, 9(~j '1 =  9 
and the chain does not move.
4. Change the counter from j  to j  +  l and return to step 2 until convergence 
is reached.
Step 3 is performed after the generation of an independent uniform quan­
tity u. Ifu  < a, the move is accepted and if u >  a the move is not allowed. 
The transition kernel q defines only a possible move that can be confirmed 
according to the value of a. For that reason, q is generally referred to as 
the proposal kernel or proposal (conditional) density when looked upon 
as a (conditional) density q(9, •). Other terms sometimes used are probing 
kernel or density.
In any of the forms of the Metropolis algorithm, q defines a symmet­
ric transition around the previous positions of the molecules. Therefore,

196
Metropolis-Hastings algorithms
q(9,<p) =  q(<f>,9), for every (9, (p) and the acceptance probability becomes
depending only on a simplified test ratio 7r($)/7r(9), the ratio of the poste­
rior density values at the proposed and previous positions of the chain.
Note also that the chain may remain in the same state for many it­
erations. A useful monitoring device of the method is given by the aver­
age percentage of iterations for which moves are accepted. Hastings (1970) 
suggests that this acceptance rate should always be computed in practical 
applications.
The success of the method depends on not having a very low acceptance 
rate. A naive approach to the problem is to make the chain move very 
slowly, i.e., to drive the chain so that its displacements are minute. As­
suming for simplicity that q(-, •) and 7r(-) are continuous, similar values for 
previous and proposed states will lead to a test ratio and hence acceptance 
probability close to 1. Following this strategy, the chain will have very high 
acceptance rates and most proposed moves are accepted. The chain how­
ever must be capable of traversing the whole parameter space in order to 
converge to the equilibrium distribution. Very small moves will make it 
takes many iterations to converge. On the other hand, large displacements 
may be proposed but they are likely to fall in the tails of the posterior 
distribution causing a very low value for the test ratio. The chain moves, 
determined by q, must be paced in such a way as to provide consider­
able displacements from the current state but with substantial probability, 
determined by a, of being accepted.
It is also crucial that the proposal kernels are easy to draw from as the 
method replaces the difficult generation of 7r by many generations proposed 
from q. Another less obvious but equally important requirement to be met 
by q is the correct tuning of the moves it proposes to ensure that moves 
covering the parameter space can be made and accepted in real computing 
time.
Optimization studies in this area are not conclusive and are likely never 
to be. The diversity of models that can be treated and of transitions q that 
can be proposed make it extremely difficult to allow for general results. 
Current reasoning, expressed in an applied context in Bennett, Racine- 
Poon and Wakefield (1995), Besag et al. (1995) and other authors seem 
to indicate to the direction of acceptance rates between 20% to 50%. In 
a specific theoretical context, Gelman, Roberts and Gilks (1996) obtained 
optimal acceptance rates of 24% for high-dimensional problems with nor­
mal densities 7r and q. These values should be looked at only as a generic 
indication rule and never as a compulsory determination. In the final sec­
tion, an application with two sampling schemes with very high acceptance 
rates (larger than 90%) is shown. The performance of the schemes are very

different with one of them showing fast convergence whereas the second 
one has a very slow convergence.
The test ratio can be rewritten as
tt (<t>)/q(9,4>)
ir(6 )/ q {< j),e) '
Acceptance of proposed values is based on the ratio of target and pro­
posed density. So, there is a connection here with the resampling schemes 
described in Section 1.5. There, the proposal density q was to be chosen 
as similar as possible to n to increase acceptance rates but the methods 
were not iterative. Also, for the rejection method, the rejection probability 
depended only on the numerator in (6.6).
The target distribution tt enters the algorithm through the test ratio in 
the form of the ratio 
as in the resampling methods. So again,
the complete knowledge of ix is not required. In particular, proportionality 
constants are not needed. When 7r is a posterior density, even though its 
functional form is always known, the value of the proportionality constant 
is rarely known. So, the algorithm is particularly useful for applications to 
Bayesian inference.
Many of the comments made about Gibbs sampling in the previous chap­
ter are also valid for the Metropolis-Hastings algorithm. So, the discussion 
about single long against multiple chains is just as relevant here. In using 
a single long chain, particular attention must be given to spacing between 
values of the chain taken for the resulting sample. The possible repetition 
of the same state for many iterations does not hinder convergence proper­
ties but makes it more common to produce sequences of repeated values, 
even after some spacing is allowed. A sample must adequately cover the 
complete parameter space so it is important that its values are not unnec­
essarily influenced by their predecessors.
Formal and informal convergence techniques described in Chapter 5 
can all be used here. The exception is made up of those based on com­
plete knowledge of conditional densities. Typically, but not necessarily, 
Metropolis-Hastings algorithms are used when these are not completely 
known and hence difficult to sample from. When the complete conditional 
densities are known, Gibbs sampling is generally used. Besag et al. (1995) 
argued against taking this approach as a general rule. They reasoned that 
Gibbs sampling does not take into account the previous value of the com­
ponent being updated and is therefore restrictive. Questions relative to 
optimization of the algorithm through reparametrization or blocking are 
deferred to Section 6.4.
Definition and properties 
197

198
Metropolis-Hastings algorithms
As described in the previous section, there is total flexibility for the choice 
of the proposal transition q apart from a few technical restrictions. Some 
general considerations have already been made and now we turn to some 
specific classes. It should be pointed out that although a chain is defined by 
its transition kernel p and not by a proposal transition q, the names used 
to categorize the algorithm generally refer to properties of q rather than p.
6.3.1 Symmetric chains
A chain is said to be symmetric if its transition kernel p is symmetric in 
its arguments, namely p(9, (j)) =  p(<p, 9), for every pair (9, </>) of states. For 
the Metropolis-Hastings algorithms, the notion of symmetric chain is ap­
plied to the proposed transition q. An example of a symmetric chain is the 
Metropolis version of the algorithm. If q depends on (9, (f>) only through 
|<j> — 9\ then q(9,<p) — q(4>,9). In this case, the acceptance probability re­
duces to min{l,7r((?i)/7r(0)} and does not depend on q. A computational 
simplification that may well prove to be substantial is thus obtained.
6.3.2 Random walk chains
Again, this characterization refers to the proposal transition q. From Chap­
ter 4, we know that a random walk is a Markov chain with evolution given 
by 9 ^  =  9<J~ 1^ +  wj where wj is a random variable with distribution in­
dependent of the chain. In general, the disturbances Wj are independent 
and identically distributed with density f w. The chain has proposed moves 
according to q(9,<f>) =  
~  8). If f w is symmetric around 0, the chain
is symmetric and all comments above are valid here. The Metropolis algo­
rithm can then be seen as a special case of a random walk chain.
This is a very common option and most practical implementations of 
Metropolis-Hastings algorithms use this scheme. The most used choices for 
f w are the normal (Muller, 1991b) and Student’s t (Geweke, 1992) dis­
tributions centered at the origin. Proposed values are then based around 
the previous values of the chain. An important point still remaining is the 
choice of the dispersion of f w. Large values for the variance allow moves 
that are very distant from previous values but at the likely cost of very 
small acceptance rates. On the other hand, small values for the variance 
only allow moves close to the previous values but with high acceptance 
rates. Tierney (1994) suggested setting the variance matrix of f w as cV 
where c is a multiplying scalar playing the role of a tuning constant and 
V is some form of approximation for the posterior variance (see Chapter 
3). This allows the moves along the components of 9 to be of the same 
size relative to the spread of the posterior distribution. The choice of the
6.3 Special cases

Special cases
199
tuning constant depends on the form of optimization desired (high accep­
tance rates/large moves). Metropolis et al. (1953) discussed this issue in 
the context of their application. Tierney (1994) suggested values between 
1/2 and 1. Bennett, Racine-Poon and Wakefield (1995) reported the use 
of a variety of values in the context of non-linear hierarchical models and 
also obtained the same recommendation based on sample sizes prescribed 
by the technique of Raftery and Lewis (1992). Gelman, Roberts and Gilks
(1996) obtained their optimal acceptance rates for normal random walk 
proposals with c between 2 and 3.
E xam ple 6.2 The performance of the random walk Metropolis algorithm 
is investigated when the target distribution is a two-component mixture of 
bivariate normal densities
n{9) =  0.7f N(e-, Mi, S i) +  0.3f N{0- fi2, S 2).
where
« - ( ; ) ■' - ( « ) - M i : ?  
"»7)-
Figure 6.2 reveals the bimodal aspect ofn(0). Pretend that direct sampling 
from 7r(9) is not possible and instead sample using a random walk Metropo­
lis with proposal q{8, cp) =  f
w
8, v h )- Figure 6.3 shows the effects of the 
initial value and the tuning parameter u in the chains.
Notice that the chains perform poorly for both small and large values of 
the tuning parameter. For small values of the tuning parameter, the chains 
have difficulty moving across the parameter space. As a result, the trajec­
tories provide inappropriate representations of the target distribution. For 
large values of the tuning parameter, the chains visit most of the parameter 
space but have low acceptance rates, leading to a computationally ineffi­
cient sampling scheme. For reasonable values of the tuning parameter the 
chains explore efficiently the parameter space and provide good approxima­
tions to the posterior distribution, with an effective sample size of around 
200 for a sample of 5000 iterations, for v =  1 and t(9) =  #2- The disparity 
between the effective and actual sample sizes is due to the large autocor­
relation structure imposed by the random walk nature of the chain. Figure 
6-4 illustrates this point. See Exercise 6.9 for a univariate version of this 
example.
6.3.3 Independence chains
In this case, the proposed transition is formulated independently of the 
previous position 8 of the chain. So, q(8,4>) =  /(^ )- It may seem that 
the independence from the previous state disagrees with the Markovian 
property of the chain. Once again, it is worth remembering that q is just

200
Metropolis-Hastings algorithms
Figure 6.2 Mixture of normals: ir(0) =  0.7 
fii, Si) + O.3/w(0; M2, £ 2).
a proposal that is combined with an acceptance probability a to give the 
transition kernel p of the algorithm. This transition depends on the previous 
state, preserving the Markovian structure.
Using expression (6.6) for the test ratio, it reduces to w(cf>)/w(9) where 
w =  n/f. The weight function works like the weight function in weighted 
resampling methods (Section 1.5.2). One popular choice for /  is the prior 
density as in Section 3.5.2 (West, 1996; Knorr-Held, 1997). In this case, 
w =  I, the likelihood function and the acceptance probability is a{9,<j>) =  
m in{l, l(4>)/l(9)}.
The use of the prior distribution as the basis for a resampling scheme 
was discussed in Section 3.5 with advantages and disadvantages equally 
relevant here. Computationally, it has the advantages of producing one of 
the simplest expressions for a. The main disadvantage is when there is 
conflict between prior and likelihood information. Values more likely to be 
sampled are those supported by the prior and they will have little posterior

Special cases
201
t u n in g s o .o i 
Initial v a lu e = (4 ,5 ) 
A c c e p ta n c e  rat© =93.8<%
tuning—0.01 
Initial valu«»<0,7) 
Acceptance r 
' '
tuning=100
Figure 6.3 Performance of the random walk Metropolis algorithm. The tuning 
parameter v is set at 0.01,1 and 100 while two different initial values are used:
(4,5) and (0,7). Acceptance rates are based on 5000 draws.
support. Values highly supported by the likelihood (and probably by the 
posterior) will have very little chance of being drawn by such a scheme.
Proposal densities incorporating the likelihood help to avoid this sit­
uation. Normal approximations to the likelihood were used by Bennett, 
Racine-Poon and Wakefield (1995) and Chib and Greenberg (1994) to form 
normal independence proposals with and without combination with the 
prior distribution. Jacquier, Poison and Rossi (1994) use moment matching 
approximation to construct a Gamma proposal in the context of stochastic 
volatility models.
The general rule for independence chains is to avoid large variation in the 
weight function as this increases the chances of a chain being retained for 
many iterations in states with large weights. So, it is recommended that /  
is chosen in order to make the function w as constant as possible, or at least 
bounded. As /  and 7r are both densities, this is equivalent to recommending

202
Metropolis-Hastings algorithms
TuningsO.OI 
Initial value=(4,5)
—i-----1-----1-----r
50 
100 
ISO 
200
Tuning=0.01 
Initial valuft=(0,7)
—I--------1--------1--------r
SO 
100 
ISO 
200
Tuningsl 
Initial value=(4,S)
TuningslOO 
Initial value=(4,5)
0 
SO 
100 
ISO 
200
Tuningsl 
Initial values(0,7)
0 
SO 
100 
150 
200
TuningslOO 
initial value=(0,7)
0 
SO 
100 
ISO 
200
0 
SO 
100 
1 SO 
200
Figure 6.4 Autocorrelation functions for O2 for the random walk Metropolis algo­
rithm.
that /  and 7r are as similar as possible. That rules out the use of a prior 
proposal in case of disagreement between prior and likelihood.
Tierney (1994) suggested avoiding densities /  with thin tails such as the 
normal distribution and to use instead t densities with small numbers of 
degrees of freedom. In this way, the weight function will not be so strongly 
affected by the tails of / .  By doing that, the weight function becomes less 
likely to have large variations.
The very minimum requirement for such /  is to allow sampling from 
all probable values of -k. Otherwise, the resulting chain will almost never 
visit all likely values of -k and resulting sample will be misleading. This 
recommendation was crucial for simpler Monte Carlo schemes (see Figure 
1.4) and remains crucial for MCMC schemes (see example below). Thus, 
overdispersed proposal distributions should always be preferred over un­
derdispersed proposals to ensure appropriate sampling over appropriate 
regions of the parameter space.
E xam ple 6.2 (continued) Consider an independence Metropolis-Hastings 
algorithm with proposal q(9, (f>) =  /jv(0; ^3, v h ), with ^3 =  (3.01,4.55)' the 
mean of the target distribution and v is a tuning parameter.
Figure 6.5 shows the effects of the initial value and the tuning parameter

Special cases
203
in the chains. Again, extremely small and extremely large tuning parameter 
make the chains move slowly and get trapped. For reasonable values of the 
tuning parameter the chains explore adequately the parameter space and 
provide reasonable approximations to the posterior distribution. They do 
that more efficiently than the random walk proposals because of the low 
autocorrelation structure implied by the independence form of the proposal 
(see Figure 6.6). Their acceptance rates are lower but, more importantly, 
their effective sample sizes, for v =  5 and t(0) =  82, a,re between 600 and 
1200, much larger than the effective sample size obtained with the optimized 
random walk proposal. The independence Metropolis chains outperform the 
random walk chains in this case. See again Exercise 6.9 for a univariate 
version of this example.
tu n in g = O .S
tu n ln g = 5  
Initial valuess(4,5) 
A c c e p ta n c e  ra te = 3 0 .9 %
tu n ln g s S O  
in itia l v a lu e s (4 ,S ) 
A c c e p ta n c e  rates5*%
tu n ln g = O .S
Figure 6.5 Performance of the independence Metropolis-Hastings algorithm. The 
tuning parameter v is set at 0.5, 5 and 50 while two different initial values are 
used: (4,5) and (0,7). Acceptance rates are based on 5000 draws.
The message from the above example is that independence proposals are 
better than random walk proposals. This was due to the fact that in this

204
Metropolis-Hastings algorithms
Tunings0.5 
Initial value=(4,5)
t -------1------ j------- 1-------r
0 
60 
100 
150 
200
Tuning=0.5 
Initial valu«^0,7)
Tuning=5 
Initial value={4,5)
Tuning=50 
Initial value=(4,5)
t-------1-------------- 1-------r
0 
50 
100 
150 
200
lag
Tuning=5 
Initial value=(0,7)
Tuning= 50 
Initial value=(0,7)
0 
50 
100 
150 
200
0 
50 
100 
150 
200
Figure 6.6 Autocorrelation functions for O2 for the independence Metropolis al­
gorithm.
simple, bivariate context it was rather simple to envision a good approx­
imation to the target distribution. Such feature was successfully applied 
there. Nonetheless, in more general and higher dimensional problems, it 
is very difficult to find suitable proposals for the independence sampler. 
In those cases, it is easier to design and implement suitable random walk 
proposals.
6.3.4 Other forms
Tierney (1994) presented many other classes of proposal transitions that 
may be used. In particular, he discussed the use of the rejection method 
in independence chains. In the case of sampling from n based on rejection 
sampling from / ,  /  should be an envelope function for -k. This requires 
finding a constant A to achieve this task, which can be difficult due to 
the complicated form of n. Large values of A ensure a proper envelope 
at the expense of high rejection rates and small values of A may not en­
sure a complete envelope. This structure can be put in the context of the 
Metropolis-Hastings algorithm without having to enforce a (possibly dif­
ficult to obtain) specification of the envelope constant A (Exercise 6.6).

Hybrid algorithms
205
Chib and Greenberg (1995) gave the details of this application of rejection 
methods in the Metropolis-Hastings context.
Gilks, Best and Tan (1995) generalized the adaptive rejection sampling 
scheme to non-log-concave densities using the same idea. The approximat­
ing piecewise exponential density does not provide an envelope for these 
densities. The rejection step may then be replaced by a Metropolis-Hastings 
step just as outlined above. Limiting results of Metropolis-Hastings algo­
rithms ensure that sampling is still correct.
Another possibility is the extension of random walk chains to autore­
gressive chains where 8 ^  =  a +  b8^~^ +  Wj (see Section 2.6). Taking the 
value of b =  1 reduces to random walk chains with incorporation of the 
constant a to the distribution f w. Taking b — — 1 produces alternations in 
the chain forcing negative autocorrelations. These two choices of b are com­
pared for the univariate normal distribution by Hastings (1970) and for a 
bivariate normal distribution with high correlation by Chib and Greenberg 
(1995). In both examples, the alternating chain produces better estimates 
of the target distribution. Barone and Frigessi (1989) provided further the­
oretical support for this choice of value of b. This alternating effect is not 
new in simulation and is generally known as antithetic variables with good 
properties in terms of reducing variance of estimates (Ripley, 1987).
The algorithm of Ritter and Tanner (1992) presented in the previous 
chapter is, in fact, based on an approximation to the target distribution. 
This can be improved upon by incorporating it into a Metropolis-Hastings 
algorithm as the proposal kernel. If the kernel admits a density then this 
proposal density will appear in the expression of the acceptance probability.
Finally, further proposal densities that do not fit into any of the above 
schemes will be presented in the applications of Section 6.5. These proposals 
were motivated by the structure of the model and in similar inferential 
procedures already available. The diversity of options presented in this 
section serves the purpose of showing the vast field available for exploration 
when simulating via Markov chains. Most of them have only started to be 
explored.
6.4 Hybrid algorithms
In this chapter, a simulation scheme using Markov chains called the 
Metropolis-Hastings algorithm is being presented. This scheme was intro­
duced in a general form in Section 6.2 and some special cases were presented 
in Section 6.3. The aim of this section is to present some of the capabilities 
of the scheme especially in connection with componentwise transition and 
combinations of different transition schemes.
In the previous chapter, a scheme based on transition by components 
was presented, the Gibbs sampler. The similarity between the componen­
twise sampling schemes will be clarified and schemes combining the two

206
Metropolis-Hastings algorithms
algorithms are presented. Also in this section, a discussion about blocking 
and reparametrization is made.
6-4-1 Componentwise transition
The previous section showed some of the possibilities that are available 
with Metropolis-Hastings algorithms. In all cases presented, the quantity 
of interest for sampling 9 was updated in a single block. Again here, new 
transition forms are available when the components 6\,... ,9d of 9 are used 
separately. Hastings (1970) and Tierney (1994) discussed this possibility. In 
particular, the components of 9 can be updated or changed in the following 
forms:
a) At each iteration, a single component is updated and the choice of the 
component is made at random between the d components.
b) At each iteration, a single component is updated and the choice of
the component is made in a fixed pre-specified order of the d components. 
For example, the components are updated in the order 1 —> 2 
d.
The above forms are examples of mixtures in case (a) and cycles in case
(b) 
of transitions. In the first case, define transitions pm with a common 
equilibrium distribution -k and probabilities or weights wm, to =  1,... ,r, 
satisfying wm >  0 and YZn= l Wln =  1- A mixture transition p is formed by 
taking p =  
l wmPm• Case (a) above is a special case of a mixture with 
r =  d, wm =  1/d and each transition pm moves only the mth component 
of 9, to =  1, . . . ,  d.
Properties of the transitions pm are passed on to the mixture transi­
tion p. First of all, the mixture kernel p defines a transition kernel of a 
Markov chain with equilibrium distribution tt. Also, if one of the compo­
nent transition kernels is irreducible and aperiodic then the mixture kernel 
is irreducible and aperiodic.
In the case of cycle transition kernels with component transition kernels 
pc, c =  1 , . . . ,  r, an iteration of the new chain is performed after undergoing 
all moves dictated by the component kernels. For a move from 9 to ^ in a 
single iteration of a cycle chain, all possible moves to an intermediate state 
i/jc through pc, c = l , . . . , r  — 1, finally leading to ipr — <p through pr must 
be considered. Defining the initial state 1^0 =  $ gives
p{d,(j>)= /■■• 
J | p c(V>c-l,V ’c ) # l - - - # r - l
J 
C = 1
which generalizes results from Section 4.6. Case (b) above is the special 
case of a cycle with r =  d and each transition pm moves only the mth 
component of 9, m =  1 , . . . ,  d.
Many of the properties of the transitions pc are passed on to the cycle 
transition p. First, the cycle kernel p defines a transition kernel of a Markov 
chain with equilibrium distribution tt. Unlike mixture kernels, irreducibility

Hybrid algorithms
207
and aperiodicity of one of the component transition kernels are not in 
general sufficient for irreducibility and aperiodicity of the cycle kernel. If 
all the component kernels are irreducible and aperiodic then the cycle kernel 
is irreducible and aperiodic (Tierney, 1994).
These forms can be integrated in to the Metropolis-Hastings algorithm. 
Each of the transition kernels pi above may be given by a proposal ker­
nel qt and an acceptance probability a.t. Consider now the cycle scheme 
with componentwise transitions, namely, the component transition kernel 
qi(6,4>) proposes a move of the ith component of 9, i =  1 , . . . , r .  From 
Chapter 2, ir(9) =  TTi(9i)ir(9-i) where nj is the full conditional density 
of di. The move determined by ^  only changes di, so 
and
7c(ct>)/ir(d) =  'Ki(<t>i)/'Ki(di). Note that as far as the transition pi is con­
cerned, the other components of d remain fixed and are not affected. Thus, 
it defines a reducible Markov chain. The proposal transition qi may then 
be written in the form qi(di,<j>i) even though it may well depend on the 
value of 0_j. Consequently, the acceptance probability may also be written 
as
&i{di,(pi) =  mm < 1, 
\ . 
6.7)
I 
tti{di)qi{di,(pi) J
Note that each of the component transition kernels above defines a re­
versible chain with equilibrium distribution 7Tj(0j), i — 1 ,...,d . Namely, 
each component transition kernel satisfies the equation
Tr(<pi\9-i) =  J  n(di\9-i)pi(di,4>i )d9i
Considering only two components, a move from 9 =  (#i, #2) to 4> =  
& )
is formed after moves for the two components are operated according to 
their respective kernels. If it is a stationary distribution for this cycle kernel, 
it must satisfy
7t(4>) =  J  J  n(9)pi(dl ,4>i)p2(d2,(j>2)dd . 
(6.8)
The right hand side of (6.8) can be rewritten as
^{di\92)n(d2)pi(9i, (f)i)p2{d2, (p2)d9id92
I I
=  
J  7t(02) J 7r(0i|02)pi(0i,0i)d 0i
=  
/ ^ l | * 2 W
* 2 , ^ 2
=  
f  ir(4>i)n{92\4>i)p2(d2,4>2)dd2 
=  
7>#i) J  n(d2\(f>i)p2(d2,4>2)dd2
P2(92, (p2)d92

208
Metropolis-Hastings algorithms
=  
7r(<M7r(02|<M
=  
7r(</>),
confirming the validity of Equation (6.8). The second and fourth equalities 
above follow from the stationarity of the conditional densities and the oth­
ers follow from basic probability and integration operations. The result can 
be extended to any number of components using induction on the same 
argument. The derivation above was based on absolutely continuous tran­
sition kernels for notational simplicity. All results remain valid with minor 
technical changes for the mixed kernels of the Metropolis-Hastings algo­
rithm. It can also be shown that despite the reducibility of the component 
transition kernels, the cycle kernel is irreducible and aperiodic (Tierney, 
1994). So, the limiting distribution ir is unique.
Once that is done, a new, componentwise version of the Metropolis- 
Hastings algorithm is given by:
1. Initialize the iteration counter 7 =  1 and set the initial value of the chain 
0(°).
2. Initialize the component counter i =  1.
3. Move the ith component of the vector of states of the chain to a new 
value fa generated from the density qi{0[3 
fa).
4. Calculate the acceptance probability of the move c*i(9p  11, fa) given by
(6.7). If the move is accepted, 6 ^  =  fa. If the move is not accepted, 
9 ^  =  
and the chain does not move.
5. Change the counter from i to i +  1 and return to step 3 until i =  d. 
When i =  d, go to step 6.
6. Change the counter from j  to j  +  1 and return to step 2 until convergence 
is reached.
In fact, this was the form of the algorithm originally proposed by Metropo­
lis et al. (1953). The positions of the molecules were modified one by one 
according to a symmetric transition with uniform bivariate distribution 
centered at the previous position of the molecule. The algorithm was intro­
duced in this book with a single global transition to unify the presentation 
with the work of Hastings (1970).
E xam ple 6.3 Bennett, Racine-Poon and Wakefield (1996) compared many 
MCMC schemes in the context of longitudinal data studies with the non­
linear mean structure. The response yij of individual i at time tij is ex­
plained by the non-linear regression model
Vij ~  fifa j tij) +  ^ij
where f(ip,t) is given by (6.1), 
~  N (0 ,a 2) and tpi =  
3») is
the non-linear regression coefficient for individual i, j  =  1, . . . ,  
and i =

Hybrid algorithms
209
1
,m . The common structure relating the individuals is given through a 
hierarchical model where the 'tpi are assumed to be a sample from a N(p, W) 
distribution. The model is completed with a second level where independent 
prior distributions are specified for the hyperparameters ji 
N{b,B), a2 ~  
7G (no/2, noSo/2) a n d W  ~  IW (nw/2,n\yS\y/2).
Bennett, Racine-Poon and Wakefield (1996) showed that the full condi­
tional distributions of the hyperparameters \i, a2 and W  are conjugate and 
easy to sample from but the same is not true for the regression coefficients 
ipi,. . . ,  ipm. For these parameters, they compared sampling directly from the 
full conditionals with the rejection and ratio-of-uniform methods and indi­
rectly with independence proposals based on a normal approximation to the 
likelihood and with normal random walk proposals as described in Section
6.2. They found that Gibbs sampling with the rejection method achieves 
convergence faster but at the expense of many rejections per iteration and 
conclude that the Metropolis-Hastings schemes are easier to implement and 
more efficient in terms of computing time. Their findings seem to provided 
an empirical echo to the intuitive point that whenever the model produces 
awkward full conditional distributions, one should avoid Gibbs sampling in 
favor of other MCM C schemes (see also Example 6.5).
The above description of the algorithm concentrated on the scheme where 
components are updated one by one in the order they are given in the 
parameter vector 9. The mixture and cycle schemes show that it is also 
possible to have many other orderings of the components. This also includes 
schemes where some of the components are systematically updated more 
often than other components. This may be because the less frequently 
updated components are more difficult to generate or because they are 
able to move more freely across the parameter space.
Another possibility worth exploring is to consider mixtures (or cycles) of 
different transition kernels. These typically apply to different components 
of 0 but nothing in the theory prevents the use of conceptually different 
sampling schemes to update the same component or group of components 
in a single Markov chain. This does not generally guarantee faster con­
vergence but Gelfand and Carlin (1995) provide a nice example where 
considerable improvement in convergence is achieved by mixing different 
sampling schemes in a single chain.
The different forms of proposal kernels described in the previous section 
are all relevant here. They may be used to construct each of the proposals
considered here. The same can be said about the adequacy of different 
forms for the situation under study. For some of the components of 9 it 
may be more natural to use independence chains based on the prior dis­
tribution if there is enough information available for that component. For 
other components, the likelihood may suggest more appropriate proposals

210
Metropolis-Hastings algorithms
(Section 6.5). Failing that, remaining components may be updated by a 
simple random walk proposal.
E xam ple 6.4 (Tanner, 1996, p. 67) Times to failure ( f )  of motorettes 
were tested at different temperatures (t). The data is presented in Table
6.2. 
A simple linear regression is fit
Di =  Po + Pl%i +  £»
where £i ~  N (0 ,a 2), Xi =  1000/(^+273.2) andyi =  logio(fi). Without loss 
of generality, assume that the first m =  17 observations are uncensored. For 
simplicity, let cr2 =  0.2592 and p((3) oc 1, where P =  (Po,Pi)- The posterior 
distribution of (3 is
where £i((3) — Vi -  Po ~ P\x%. The second term on the righthand side ac­
counts for the censored portion of the data. Analytical posterior inference 
is unavailable, so the following three variations of the Metropolis-Hastings 
algorithm are implemented:
1. Random walk Metropolis with single move: given the current state of the 
chain P^) =  (Pq \ P i '>), Po is sampled from N(Pq \ t2) and then Pi is 
sampled from N ( P ^ \ t2), for r  =  0.1.
2. Random walk Metropolis with block move: P is sampled from N (P ^ , r 2^ ), 
for t =  0.1.
3. Independence Metropolis: P is sampled from N(p, cr2( X 'X ) _1), with p =
(X ' X ) ~ 1X ' y  where y =  (j/i,. . . ,  ym)' and X  is the design matrix with 
rows (1, Xi), for i =  1, . . . ,  to. The correlation between Po and Pi in the 
proposal is —0.999.
Figure 6.7 exhibits the behavior of three chains for each one of the above 
three schemes for three distinct initial values (—8,5.5), (—8,4) and (—5,3). 
It can be seen that the independence Metropolis scheme works much bet­
ter than both single and block random walk Metropolis schemes, the main 
apparent reason being the careful choice of the proposal density. Effective 
sample sizes based on either Po or Px are roughly around 100 for the ran­
dom walk Metropolis algorithms (schemes 1 and 2). They are around 600 
and 950 for the independence Metropolis algorithm (scheme 3). Posterior 
summaries are based on the 20000 draws from the independence Metropolis 
algorithm. Posterior mean, median and 95% credibility interval for po are 
—6.09, —6.08 and (-7 .8 6 ,-4 .3 0 ). Posterior mean, median and 95% cred­
ibility interval for Pi are 4.34, 4.34 and (3.53,5.15). Posterior correlation 
between Po and Pi is —0.998. Later, in Section 6-4-4, a reparametrization 
strategy is implemented to reduce the high correlation. Consequently, mixing 
of the chains improves considerably for all sampling schemes above.
1 -  $
£i(P)

Hybrid algorithms
211
Temp. 
time to failures
150°
8064
8064
8064
8064
8064
8064
8064
8064
8064
8064
170°
1764
2772
3444
3542
3780
4860
5196
5448
5448
5448
190°
408
408
1344
1344
1440
1680
1680
1680
1680
1680
220°
408
408
504
504
504
528
528
528
528
528
Table 6.2 y: time to failure (in hours) of motorettes for four different tempera­
tures (in Celsius). Bold indicates right censoring.
6-4-2 Metropolis within Gibbs
Transitions are based on the full conditional distributions of the compo­
nents of 6 in the basic componentwise scheme above. The more similar the 
proposal qi and the density tt, or equivalently the full conditional tt,, the 
closer to 1 will the acceptance probabilities (6.7) be. This does not neces­
sarily ensure fast convergence but may reduce the computational burden.
In the basic Metropolis algorithm, it was typically not possible to find 
a sampling kernel q that could approximate rr without error. Were that 
possible, direct sampling from tt would be available. Here, the situation is 
different. It is possible that n has a complicated form that prevents direct 
sampling from it but (some of) its full conditional distributions ni can be 
directly sampled from. In this case, a convenient choice of proposal is to 
take qi =  iti- The proposed value for di is drawn from its full conditional 
and accepted with probability 1. Hence, the computational burden of the 
calculation of (6.7) is avoided. If that can be done for all components of 
9, their values will be all sampled from the corresponding full conditionals 
and accepted. This is the Gibbs sampler!
In general, the simplification obtained is very convenient but again no 
optimality results are available in that direction. Note that the Gibbs sam­
pler only depends on the previous value of the other components, unlike 
the Metropolis-Hastings scheme that also depends on the previous value 
of the component being updated. Besag et al. (1995) pointed out that this 
extra ingredient may allow the construction of a more efficient sampling 
scheme. The example below shows an instance where it is clearly advanta­
geous to sample from a proposal even when the full conditional is available 
for sampling.
Example 6.5 Consider again the Poisson model with change point from 
Example 5.1. Instead of sampling m! from its full conditional irm(m), an 
alternative is to sample m' from a proposal kernel q(m, m!) and accept it

212
Metropolis-Hastings algorithms
Random walk Metropolis (single)
T------1------1------1”
0 
5000 
10000 
15000 
20000
5000 
10000 
15000 
20000
iterations
Random walk Metropolis (block)
Independence Metropolis
5000 
10000 
15000 
20000
iterations
5000 
10000 
15000 
20000
iterations
Figure 6.7 Chain paths: Top frames: random walk Metropolis with single moves. 
Middle frames: random walk Metropolis with block moves. Bottom frames: inde­
pendence Metropolis. 
‘
with probability
a(m, m') =  min | l ,
f(y\X,(p,m') q(m 
f(y\\,(p,m) q(m
V m )l
,ro') J
(6.9)

Hybrid algorithms
213
The main advantage of performing a Metropolis-Hastings step as opposed 
to a Gibbs step is avoiding the evaluation of (5.2) for all possible values 
of m. This advantage is particularly relevant when the number of evalu­
ations of the full conditional is large and/or when their evaluations are 
cumbersome.
Assume thatm' is drawn from the proposal q(m,rn') oc ex p {—T\m' — m\}, 
where t  is a tuning parameter. The approximate posterior distribution ob­
tained with 5000 draws from the algorithm is very similar to the approxi­
mation obtained in Example 5.1 with the Gibbs sampler. Figure 6.8 exhibits 
average acceptance rates and effective sample sizes for several values of r. 
For instance, when t  =  0.15, the average acceptance rate for this chain is 
approximately 33%, while effective sample sizes based on X,(j) and m are 
3455, 3330 and 900, respectively. For comparison with the Gibbs sampler, 
this algorithm was run for a total of 40000 iterations. This task takes only 
around 40% of the time used to run the 5000 iterations of the Gibbs sam­
pler. Effective sample sizes based on X, <fi and m are 25937, 24837 and 5311, 
respectively. They are all larger than the effective sample sizes obtained with 
the 5000 iterations used by the Gibbs sampler. This shows that even though 
the Metropolis algorithm may generate chains with higher autocorrelation 
structures, its computational simplicity compensates and produces more ef­
ficient approximations in the same amount of time.
Going back to direct sampling from full conditionals, in complex models 
it may be possible to establish conditional conjugacy for some but not all 
components of the parameter. In this case, only some components will be 
available for direct sampling. For the components that cannot be directly 
sampled, Muller (1991b) suggested that they are sampled from 7r* by a 
Metropolis-Hasting (sub-)chain inside a Gibbs sampling cycle. So, these 
components are sampled from a proposal qi with possible acceptance gov­
erned by probabilities (6.7). This process would last for T iterations until 
convergence and consequent generation from 7r*, the limiting distribution 
of the sub-chain. This sampling scheme is widely known as Metropolis- 
within-Gibbs. Muller (1991b) used the value T  =  5 in applications but in 
fact the construction of the sub-chain is unnecessary as a single iteration 
is sufficient. Note that the case T  =  1 reproduces the Metropolis-Hastings 
cycle scheme that visits each component once and was described in the 
previous subsection. Nowadays, the version T =  1 is almost always used so 
that the Metropolis-within-Gibbs nomenclature is somewhat misleading. A 
more appropriate name to describe it could be Gibbs-within-Metropolis as 
some Gibbs updating schemes are used inside a componentwise Metropolis- 
Hasting scheme.

214
Metropolis-Hastings algorithms
(c)
<d)
I---------1-------- 1-------- 1---------- 1
0.1 
0.2 
0.3 
0.4 
0.5
I 
1---------1-------- 1----------1
0.1 
0.2 
0.3 
0.4 
0.5
Figure 6.8 Poisson observations with change point. Results from the Metropolis 
chains based on 5000 iterations for several values of tuning parameter t : (a) 
average acceptance rates, (b),(c) and (d) are effective sample sizes based on X, <j> 
andm, respectively. The horizontal lines in (b), (c) and (d) represent the effective 
sample sizes based on 5000 iterations of the Gibbs sampler of Example 5.1.
6.4-3 Blocking
So far, nothing has been said about the choice of blocks 
The
results obtained for the Gibbs sampler do not reproduce with the same 
ease here. In particular, it is not true that the larger the block the faster 
is the convergence. In fact, it is likely that blocking many parameters in a 
single group in highly multidimensional problems is more detrimental than 
beneficial.

Hybrid algorithms
215
Example 6.3 (continued) The parameters of the model are given by ip 1 , .. . ,  
ipm, fi, a2 and W . Note that each of these parameters is itself a collection 
of components with the exception of the scalar a 2. The regression coeffi­
cients tpi and their mean /x are vector parameters and W  is a matrix of 
parameters. Bennett, Racine-Poon and Wakefield (1996) used this block­
ing in all their sampling schemes except for the Gibbs sampler with the 
ratio-of-uniform method where they sample each scalar component of the 
ipi separately.
Consider the most common case of a random walk chain with proposal qi 
for the component Qi formed by a large number of components. As the num­
ber of components gets large, it becomes more likely to have components 
of Oi falling well in the tails of 7r*. This will produce a proposed value with 
very low density 7r.; and hence the test ratio will also be small. As a result, 
very few proposed values will be accepted and convergence will be very 
slow. O f course, decreasing the appropriate entries of the variance matrix 
of the proposal will inhibit such extreme components of 0, being proposed 
but that requires tuning of each component of the variance matrix to avoid 
it. This task is far from easy and can become a time-consuming exercise.
Similar reasoning can be applied to other forms of chains. There is no 
reason to believe that the proposal 
will remain close to 7r* as the dimen­
sion of &i increases. This leads to the same end result that increasing the 
dimension of a block increases the chances of rejection.
Again, there are no theoretical results here but the rule followed in ap­
plied work is to form small groups of correlated parameters that belong to 
the same context in the formulation of the model. A typical example is given 
in Example 6.3 above. The structure of the model made natural the choice 
of the blocks ipi , .. • ,ipm,n ,a 2 and W . With this structure, inference via 
Gibbs sampling or Metropolis-Hastings sampling may successfully proceed. 
Gathering all regression coefficients in a single block ip — (ipi, . . . ,  ipn)' leads 
to the difficulties described above. Small blocks of ^s may also be formed 
and would alleviate these difficulties. In the context of this application, it 
is better to work with each vector of regression coefficient separately be­
cause they are all independent conditionally on /i and W . This point will 
be returned to in more detail below and in the next section.
The important point to make in practice is to block parameters whenever 
it is possible and needed. It is possible to block whenever the acceptance 
rate does not fall to very small values, namely single digit percentages. 
Gelman, Roberts and Gilks (1996) provided some theoretical considerations 
for aiming at an optimal acceptance rate of around 24% for random walk 
chains. This rate should be taken as an indication and not as a rule. Efficient 
chains with higher acceptance rates are also possible.
Blocking of parameters is only needed to break correlations. Parame­
ters that are conditionally independent given other parameters need not

216
Metropolis-Hastings algorithms
be blocked. Remember from Chapter 5 that blocking was used to define 
more appropriate directions of moves for the chain instead of the unrelated 
moves along the axes. When components are (conditionally) independent, 
however, moves will already be made along the individual directions and 
no improvement is made by changing these directions.
6.4-4 Reparametrization
The same comments made about choice of parametrization in Chapter 5 for 
the Gibbs sampler are valid here. Good parametrizations are still useful in 
improving mixing of the chain and accelerating convergence. The apparent 
freedom of choice of proposals is not unrestricted. Very large displacements 
are very likely to be rejected as discussed above. Only moves that are 
compatible with the structure of the model and the form of the posterior 
distribution will be accepted. So, models with highly correlated parameters 
will only allow very small moves.
This is the pattern already observed for the Gibbs sampler. For the 
Metropolis-Hastings algorithm, larger moves may even be proposed but 
they will very likely fall in the tails of the posterior and will force a small 
value for the test ratio. The only possible acceptance of these large moves 
is for the case when they are directed according to the correlation structure 
of the posterior. In high-dimensional problems, the identification of these 
directions may be very difficult or time consuming. So, the large moves 
required for fast convergence will lead to rejected points. If an efficient 
parametrization can be found, then these large moves will no longer be re­
jected and fast convergence may be achieved. The application to dynamic 
models below illustrates this point. Another successful use of reparametriza­
tion is the case of centering in generalized linear mixed models as discussed 
by Gelfand, Sahu and Carlin (1996). They showed that centering in gen­
eralized models reduces correlation and improves convergence just as in 
normal linear mixed models (Section 5.4.3).
E xam ple 6.4 (continued) Assume the following reparametrization of the 
simple linear regression, where the x l is centered around its sample mean
Hi =  A ) +  (3lZi +  £i
for Zi =  Xi — 2.2. This means that Po now corresponds to po — 2.2Pi in 
the original model formulation while Pi remains unchanged (see Example
5.3). Posterior inference is based on samples obtained by performing the 
same three variations of the Metropolis-Hastings algorithm. Figure 6.9 ex­
hibits the behavior of the chains for three distinct initial values (3.3,3), 
(3.7,3) and (3.7,5.5) over 20000 draws. Now all algorithms perform rea­
sonably well. Effective sample sizes based on Po and Pi, when combining 
all 60000 draws, appear in Table 6.3. Posterior inference are virtually the

Applications
217
same across schemes. Based on the random walk Metropolis scheme, for 
instance, posterior mean, median and 95% credibility interval for Po are 
3.47, 3.47 and (3.38,3.57). Posterior mean, median and 95% credibility in­
terval for f}\ are 4.33, 4.32 and (3.50,5.19). Posterior correlation between 
Po and Pi is 0.222.
Algorithm
Po
Pi
RWMS
7524
878
RWMB
6311
597
INDM
2025
2476
Table 6.3 Effective sample sizes.
6.5 
Applications
This section gives some details of applications of the Metropolis-Hastings 
methodology for models commonly used in practice. With only a few ex­
ceptions, as soon as the model gets away from linearity and normality it 
loses conditional conjugacy. Chapter 5 showed that conjugacy is a very use­
ful property in conjunction with the Gibbs sampler. It becomes difficult to 
sample from some of the full conditional distributions without them.
One possibility is to employ some of the resampling techniques, such as 
the rejection method. A very attractive alternative in terms of simplicity 
is the use of a Metropolis-Hastings sampling scheme for these awkward 
sampling distributions. Example 6.3 illustrates this point in the context of 
non-linearity. The applications below detail uses of the algorithm in the 
context of non-normal models.
6.5.1 Generalized linear mixed models
Models with random effects have been described in the previous chapter. 
When they also contain fixed effects, they are called mixed models. A gen­
eral but not unique form for generalized linear mixed models is given by
f(Vi l#i) 
=  a(Vi) exP{yrfi +  H Si)} w it l 1
E (y i\ 0 i)  
=  m  
( 6 .1 0 )
g{m) 
= x'iP + z'fH, 
z =  1 , . . . ,  n
where the link 
function g is differentiable, P is the d-dimensional vector of
regression coefficients and 7* is the r-dimensional random effect associated
with observation y*, i =  1 , . . . ,  n. Associated with these parameters, there

218
Metropolis-Hastings algorithms
Random walk Metropolis (single)
i----1----1--- 1----1----1----r
3.2 
3.3 
3.4 
3.6 
3.6 
3.7 
3.8
“I------1------T
0 
5000 
10000 
15000 
20000
“i------ r
0 
5000 
10000 
15000 
20000
Random walk Metropolis (block)
I 
I 
I 
i 
I 
I 
I
3.2 
3.3 
3.4 
3.5 
3.6 
3.7 
3.8
i------ 1------ 1------ 1------r
0 
5000 
10000 
15000 
20000
n------ r
0 
5000 
10000 
15000 
20000
Independence Metropolis
0 
5000 
10000 
15000 
20000
5000 
10000 
15000 
20000
iterations
Figure 6.9 Chain paths based on 20000 draws. Top frames: random walk Metropo­
lis with single moves (RWMS). Middle frames: random walk Metropolis with block 
moves (RWMB). Bottom frames: independence Metropolis (INDM). Initial val­
ues: (3.3,3), (3.7,3) and (3.7, 5.5).
are d-dimensional and r-dimensional vectors of covariates x t and zt that 
explain the fixed and random variations in the levels of the observations y^ 
i =  1,... , n. The model is completed with independent prior distributions

Applications
219
/3 ~  N (a ,R ), 7*|VF ~  N (0 ,W ), i =  l , . . . , n ,  and W  ~  IW (n/2,nS/2). 
Note that if Xi =  Zi, i =  1, . . . ,  n, the model becomes a special case of the 
generalized linear hierarchical model (2.20) with X i =  (iiag(:/;'1, . . . ,  x'n), 
Pu =  /? +  7i> * =  
X 2 =  In, C = W  and
The natural division of the parameters in blocks is given by (3,71, . . . ,  j n 
and W . Alternative divisions include the block (7 1, . . . ,  7n) or even the 
block (/?, 7 1, . . . ,  7„). The former is unnecessary as the random effects are 
conditionally independent. The latter removes the problems that may be 
associated with correlation between random and fixed components. How­
ever, it is very likely to lead to very low acceptance rates due to its high 
dimension unless complicated search exercises for an adequate proposal are 
undertaken. Even then, the solution is likely to be specific to the problem 
being analyzed and will not be adequate for other data sets.
The only block with full conditional distribution in conjugate form is W . 
The other blocks have full conditional densities
7x0(f3) 
oc 
exp | Y v r f i  +  b(Qi) i fN(/3;a,R)
*1=1
oc 
exp
-  aYR 1(P -  a) +  Y
Vi6i +  
1 • (6-n )
and 
I 
i= 1 
J
tt7; (7i) 
oc 
exp{yidi +  6(6,.i) } / Ar(7i ;0, W )
oc 
exp 
+  yidi +  6(0j)|  , i =  1 , . . .  ,n. 
(6.12)
None of these distributions is easy to sample from. Zeger and Karim (1991) 
use rejection sampling with normal envelopes for each of these full condi­
tional distributions. It is very difficult in this situation to tune the constant 
A to provide a proper envelope over all the parameter space without sacri­
ficing efficiency. Clayton (1996) explored the possibility of sampling each of 
these parameters componentwise. He used the adaptive rejection method 
as in most cases of interest the densities above are log-concave.
Other possibilities are likely to involve some use of Metropolis-Hastings 
methodology. They include sampling from the prior as a proposal in an 
independence chain, or even a random walk chain with variances given by 
prior variance or some measure of likelihood uncertainty as the inverse in­
formation matrix. As previously discussed, these forms require optimization 
of the tuning constant, which may be time consuming.
Gamerman (1997) used proposals based on the IRLS algorithm (Section 
3.2.2). Consider the start of iteration j  of the chain with previous values 
■ ■ ■ ■, 'Yn~1'1 and W
and assume that all blocks are up­
dated at every iteration in the order above.
The first step is the construction of the proposal qp for the block (3 condi­

220
Metropolis-Hastings algorithms
tional on all the other parameters being fixed at current values. A vector of 
adjusted observations y =  y{P {j~ Vy) with corresponding matrix of adjusted 
variances V  =  V(i3l-J~ 1'1) is formed according to the IRLS algorithm and
(6.11). An adjusted regression model is then formed with
Vi ~  N(x'i/3 +  2:i7p _ 1), V ),i =  1
Combining with the prior (3 ~  N (a, R) leads to an adjusted posterior dis­
tribution 7f/3(/3) =  
where 
=  C (ji') (R ~1a +  X 'V ~ ly*)
and 
— (i?_1 +  X 'V ~ iX )~ l . The vector of readjusted observations y*
has components y* =  y, -  
i =  1,... ,n. This additional adjust-
(j— 1)
ment to the observations is caused by the known displacements za\ 
, 
i =  1 
as calculations are conditional on the values of the 7*. In
many cases, frp is a good approximation to up. Therefore, it is reason­
able to take the proposal q
^
— 'Kp(-). A proposed value (3* can 
be generated and qp{(3^~l\{3*) can be calculated. Note that qp{(3^~l\ -) 
depends on p^J~1> but in a very intricate way (through the adjusted obser­
vations) and this proposal does not fit into any of the categories described 
in Section 6.3. To calculate the test ratio, the values of qp{(3*, (3^~^) and 
tt 0((3 * )/ n p (l3 ^ ) are required. The first one is obtained by repeating the 
above procedure with (3* replacing P ^~x\ The second is obtained from
(6.11). Depending on the acceptance stage, (3^ is taken as (3* or (3^~x\
A similar approach is used to construct proposals qlt , i =  1 , . . . , n .
The adjusted observations y» =  y i { l i ~ l)) with adjusted variances V% =  
Vi(7^ -1 ^) form the regression model yi ~  N(x'iP ^  +  
V ). Combin­
ing with the prior 7» ~  A7(0, W ) leads to the adjusted posterior 7r7i (7*) =  
N (m f\ c \ ^ ) where m <f'> =  
ZiV~ly* and 
=  (W ~ 1 +  ZiV~1zi)~ 1. 
Again, the readjusted observation is y* — yi — x^ p ^ . Taking as a proposal 
q7i(7^ _1\-) =  a new value 7 * is proposed and qyi(7^  ^ ,7 *) can
be calculated. Again, q7i(7 *,-) is obtained by repeating the above proce­
dures with 7 * replacing 
■ The value of n7i (7 * )/7r7i (7^ ^ ) is obtained
using (6.12). The test ratio can be calculated and depending on the accep­
tance stage, j.jJ> is taken as 7 * or 
The procedure is repeated for
i =  1 , . . .  ,n.
Finally, the value of 
is drawn directly from the full conditional
distribution of W  or $  =  W ~ 1 given by
n
7r$($) oc ]^ [/w (7 j;0 ,$ _ 1) fw($',nw/2,n\vS\v/2) 
i= 1
oc | $ r /2 exp 
^ 7 - $ 7 i |  |$| w 2 + ) exp ^ — itr(7T-vK■S'vk^>)

Applications
221
OC |$|[ra+nw-(r+l)]/2 exp | _ I t r
This is the expression of the W [(n +  nw)/2, (nw Sw +  Y17=i 7*7i)/2] dis­
tribution. Techniques for generation of a value of $  from the Wishart dis­
tribution above were described in Chapter 1 . A generated value of 
is 
obtained by inversion of the matrix (I>.
An important point is that this solution incorporates the structure of the 
problem into the construction of the proposal transition. This ensures that 
the chain moves will have direction and magnitude governed by the model. 
High acceptance rates are obtained as a result without compromising the 
amplitude of the chain moves and coverage of the relevant regions of the 
parameter space. The price paid in this case is the amount of additional 
calculation required. This may be unnecessary for models with a simpler 
structure but provides a general framework for analysis of any generalized 
linear mixed model.
E xam ple 6.6 Crowder (1978, Table 3) presented a data set with propor­
tions of germinated seeds in n =  21 plates. The data set is influenced by 
the explanatory variables type of seed (s), root extract (r) and an interac­
tion between these covariates. A larger variability than that explained by 
the binomial model was also noted by Crowder (1978). One possible model 
for this overdispersion is obtained with random effects. Breslow and Clay­
ton (1993, Section 6.1) proposed to model the germination probabilities pi 
associated with the ith plate through the logistic relation
logit ( p i) =  x'/3 +  7i
where x' =  (1, Sj, r,, sjfj) and 7j ~  N (0, W ), i =  1 , . . . ,  n, are the univari­
ate random effects modelling the overdispersion. The model is completed 
with a non-informative prior distribution p((3, W ) oc 1 j W . Observe that W  
here is scalar and therefore its full conditional is an IG(n/2, 
7?/2)
distribution. Figure 6.10 presents the marginal histograms for the compo­
nents of j3 for an analysis using the Metropolis-Hastings algorithm above. 
Table 6.4 presents numerical summaries of the posterior distribution along 
the equivalent ones from the penalized quasi-likelihood (PQL) analysis of 
Breslow and Clayton (1993). The estimates are very similar but the uncer­
tainty in the Bayesian inference is always larger. This point had previously 
been noted in the context of generalized linear models by Dellaportas and 
Smith (1993).
Figure 6.11 presents a graphical summary of the inference for the random 
effects 7i. Their posterior mean estimates behave as a sample from a normal 
distribution. So, they appear to confirm in the posterior the distributional 
form assumed for their prior. Some of the posterior correlations between the 
components of (3 were as large as 0.7 in absolute value but the correlation 
between (3 and W  was low. This provides some support for the blocking
ny/Sw +  /

222
intercept
Metropolis-Hastings algorithms 
seed coefficient
-1.0 -0.4 0.0 
extract coefficient
-1.5 
-0.5 
0.5
interaction coefficient
.H.
Ida
0.5
1.5
oLO
- 2.0 
- 1.0 
0.0 
1.0
Figure 6.10 Marginal histograms from the posterior distribution for the regression 
coefficients for the seeds data based on 600 successive draws from a single chain 
with a hurn-in period of 600 iterations.
PQL___________ MCMC
Parameter 
Estimate (SE) 
Estimate (SE)
Intercept
-0.542 (0.190)
-0.543 (0.197)
Seed coef.
0.077 (0.308)
0.074 (0.332)
Extract coef.
1.339 (0.270)
1.313 (0.274)
Interaction coef.
-0.825 (0.430)
-0.755 (0.431)
V w
0.313 (0.121)
0.278 (0.167)
Table 6.4 Estimation summary for Crowder’s seeds data.
scheme adopted, at least for this data set. The analysis presented was based 
on a single long chain. Similar results were however obtained by running 
multiple parallel chains.
This inference procedure can be easily extended to more elaborate forms 
of random effects. In many observation processes, data is obtained in groups. 
There may be random effects associated with groups (7,) and with individ-

Applications
223
uals within groups (Sij). These may be described by the linear predictor
g i f a j )  =  x'i:jl3 +  
+  t'i:j6ij , j  =  1 , . . . ,  n i, i =  1 , . . . ,  to .
An application of the above sampling techniques to this model is also pre­
sented in Gamerman (1997) and illustrated for a real data set. A very 
common special case is the so called Laird and Ware (1982) model where 
there are only random effects associated with groups.
(a)
0.0 
0.2 
0.4 
0.6 
0.8 
1.0 
1.2
(b)
Quantiles of Standard Norm al
Figure 6.11 Summary of inference for the random effects in the seeds data: (a) 
marginal histogram of the posterior distribution of \ / W ; (b) QQ plot for normality 
of random effects. The posterior means for the random effects for each site were 
estimated from the sample and ordered for the construction of graph (b).
6.5.2 Dynamic linear models
The dynamic linear model with Equations (2.21) and (2.22) describing the 
observation and state evolutions, respectively, was introduced in Section 
2.5. It was noted in Section 5.5.2 that pi, . . . ,  pn, a2, W  could be sampled 
jointly by first sampling the pair (a2, W ) from n(a2, W ) and then sampling 
P =  (P i,... ,p n) from tt(P\(72, W ).
From derivations of Section 5.5.2, the joint density of all model parame­
ters can be obtained as ir(P,a2,W ) oc fM (P ;M ,Q ~ 1)f^ (y ;F A ,F P ~ 1F  +  
a2In)p(a2,W ), where p(a2,W ) is the prior for (a 2,W ) and both M  and

224
Metropolis-Hastings algorithms
Q depend on (ct2, W ). Therefore, this density is completely known up to a 
proportionality constant. The distribution of (/3\a2,W ,y n) was derived in 
Section 5.5.2 as N (M ,Q ~ l ). Therefore, the marginal density 7t ( ct2 , W ) oc 
/at(t/; F A , F P ~ 1F  +  a2In)p(cr2, W ) is also known up to a proportionality 
constant. It does not have a known form and direct sampling becomes diffi­
cult, specially if the dimension of the state parameter, and consequently of 
W , is large. Metropolis-Hastings algorithms can be used instead to propose 
values for sampling according to some transition kernel q. Gamerman and 
Moreira (2002) applied this sampling scheme, summarized in the algorithm 
below.
1 . Sample (a2*,W *) from q{a2
2. Set (<t2^ \ W ^ )  =  {cr2*,W *), with probability a and
with probability 1 — a, where
. 
f 
n(a2*,W *) 
a -  mm j  1 , ^ o2{j_ 1} ^ ^ 2( j - 1)) q(a 2*, w
 
- 1), o 2^ - l) , W ^~-1)) J '
Jointly sampling (/?,a 2,W ) avoids MCMC convergence problems asso­
ciated with the posterior correlation between model parameters. The al­
gorithm above uses 7r((i2, W ) only in ratio form. Therefore, the unknown 
proportionality constant of this density is not required. As shown before, 
it is important to specify suitably the proposal density q to avoid chains 
getting trapped or moving slowly. Gamerman and Moreira (2002) used 
product of log random walk forms where q(cr2, W  
a2^ ~ l\ 
'-1) =
qi{a 2\f3^ ~ 1\ a 2^ ~ 1^)q2{W\(3^ ~ l\ W ^ ~ 1'1), with q\ given by an inverse Ga­
mma density centered around (ie with means given by) ct2^ -1 ^ and q2 given 
by an inverse Wishart density centered around 
with shape param­
eters tuned for appropriate chain movements.
E xam ple 6.7 Consider the first order normal dynamic linear model 
Vt 
=  
Pt +  et, 
et ~  N (0, a 2)
Pt 
=  
P t - i + v t ,  
ut ~  N (0, W ),
which is the simplest case of the dynamic model. Regardless of its simplicity, 
this dynamic model retains the main features of a general dynamic model 
and is therefore a suitable representative of the general class for the com­
parison purpose. Fairly vague priors are used: (3\ ~  iV (0,10) and cr2 and 
W  have inverse Gamma distributions with means set at their true values 
and coefficients of variation set at 10. Four combinations of (n ,W ) were 
entertained, (100,0.01), (100,0.5), (1000,0.01) and (1000,0.5), with a2 =  1 
and 100 simulations per combination (a total of 400 simulations). Signal- 
to-noise ratio, W/a2 =  W , are 0.01 (small) and 0.5 (large). Four different 
sampling schemes were implemented for each one of the 400 simulations: 
Scheme I: Sampling P i , , fin, a2 and W  individually from their full con­
ditionals (Carlin, Poison and Stoffer, 1992, and Section 5.5.2), Scheme

Applications
225
Scheme
n=100
n=1000
II
1.7
1.9
III
3.5
3.8
IV
1.9
7.2
Table 6.5 Computing times relative to scheme I. For instance, when n =  100 it 
takes almost 4 times as much to run scheme III and almost 2 times as much to 
run scheme IV. Statistical summaries are based on a total of 20000 iterations. 
Scheme III was run with separate updates for a2 and W  with tuning parameters 
set to 35, leading to 62% and 28% average acceptance rates according to whether 
n =  100 or n =  1000.
W
n
Scheme
I
II
III
IV
0.01
1000
242
8938
3028
2983
0.01
100
3283
13685
2501
12263
0.50
1000
409
3043
2391
963
0.50
100
1694
3404
1182
923
Table 6.6 Effective sample size nejj based on a2: sample mean based on 100 
replications.
II: Sampling (3 jointly (FFBS algorithm) and a 2 and W  from their uni­
variate full conditionals (Carter and Kohn, 1994, Fruhwirth-Schnatter, 
1994, and Section 5.5.2), Scheme III: Reparametrization using the fact 
that the system disturbances wt reproduce in a unique form the state pa­
rameters (Gamerman, 1998, and Section 5.5.2), and Scheme IV: Sampling 
(f3,a2,W ) (Gamerman and Moreira, 2002, and the above algorithm). For 
each one of the 400 simulations computation was based on 20000 itera­
tions. Table 6.5 presents the computing times relative to scheme I. When 
n =  100, schemes I and III are the fastest and slowest ones, respectively, 
with scheme IV  becoming much slower when n =  1000. Effective sample 
sizes are presented in Table 6.6. Figure 6.12 presents effective sample sizes 
based on a2. They are smaller for longer time series and/or larger W . 
Overall, schemes II and III exhibit better performance (see Reis, Salazar 
and Gamerman, 2006, for more details of this comparison study).

226
Metropolis-Hastings algorithms
(a)
(c)
II 
III 
IV
(b)
1
1
i
i
, 
• 
______ 
i
1 
L n r J  
1 
.
1 
.
1 
1 
1 
1 
1 
II 
III 
IV
(d)
1 
1 
1 
h
H
]
-
H
 
0
O
0
1 
I 
1
o
: 
4
t 
i 
r
II 
III 
IV
Figure 6.12 Box plots of the effective sample sizes n ejj for the four schemes and 
the four combinations of W  and T. (a) W  =  0.01, n =  1000, (b) W  =  0.01, n — 
100, (c) W  =  0.5, n =  1000 and (d) W  =  0.5, n =  100.
6.5.3 Dynamic generalized linear models
Dynamic generalized linear models were introduced in Chapter 2. As previ­
ously seen, it is not possible to perform exact inference because the relevant 
marginal distributions cannot be obtained analytically. Assuming that the 
variances of the system disturbances are constant, the model parameters 
are given by the state parameters (3 =  (/3i,. . . ,  /3n)' and the system variance 
W  =  $ _1. The model is specified with the observation and system equa­
tions and completed with the independent prior distributions (3\ ~  N (a, R) 
and $  ~  W (nw /2,n\ySw /2 ). The option to work with the precision ma­
trix instead of the variance matrix is made again. The posterior distribution

Applications
227
is given by
n 
n
7r(/3,$) oc 
■
t= 1 
i—2
The full conditional distributions are given by:
a) for block /3
n 
n
np((3) 
oc 
Y [ f ( y t\Pt)l[[p(Pt\Pt-i,®)p(f3i) 
t= 1 
t= 2
f  
n  
n  
'I
^  
e x p  +  6 ( 6't )] ~  2  ~~ 
“  G t P t - i )  |  
•
b) for block /3t, t =  2, . . . ,  n — 1
**(&) 
/(yt|A )p(/?t|A -i,$)p(/3t+i|A ,^)
oc 
exp {y tet +  b(9t)} exp 
{((3t -  Gt(3t-i)'^ {P t -  G(/3t- i )
+  
( A +1 -  G t+1p t)'<f>(pt+ 1 -  G t+1A )  ]} •
Similar results follow for blocks /3i and /?„.
c) for block $
n
7r$($) 
oc 
]^[p(/3t|/3i_i,$)p(i>)
t= 2
oc 
J J  I^11/2exp | - |tr[(A  -  G t f r - M  -  G tft-O 'fc ]} 
x 
|$|[n^ -(P +1)]/2 e x p | -itr (n w '5 ,w $ )|
o, 
|$|K --(d+1)]/2 e x p | - i t r [ ( n ^ ^ ) $ ] J  . 
(6.13)
that is the density of the W (n :^v /2,n^vS^v /2) distribution with 
=
nw +  n -  1 and 
=  nw Sw +  
~ Gtf3t-i)(0 t -  G tP t-i)'.
The results above show that the full conditional distributions of j3 and Qt 
do not belong to any known class of distributions but the full conditional of 
$  is a known distribution from which samples can be drawn. As a result, the 
state parameters cannot be sampled directly. Again, rejection sampling can 
be used but the same problem of ensuring proper envelopes appears. The 
natural alternative seems to be the use of a Metropolis-Hastings scheme for 
this block. Therefore, transition kernels for the block [i or for the blocks 
/3t, t =  1 ,
,n, are required.
Again, many possibilities are available for the construction of the pro­
posed kernels. Knorr-Held (1997) suggested the use of independence chains

228
Metropolis-Hastings algorithms
with prior proposals. He argued that the fast computing time at each it­
eration partially offsets the slow convergence due to the high correlation 
between state parameters. Shephard and Pitt (1997) used independence 
chains with proposals based on both prior and a normal approximation to 
the likelihood. They also blocked the state parameters (3t in random blocks 
to speed convergence. Ravines (2005) used independence normal proposals 
for the block /3 with moments given by the approximation of West, Har­
rison and Migon (1985). Proposal kernels may again be constructed with 
the IRLS algorithm used for evaluation of the posterior mode.
Singh and Roberts (1982) and Fahrmeir and Wagenpfeil (1997) extended 
to the dynamic setting the method of mode evaluation described in Section
3.2.2 for static regression. They showed that iterating the posterior mode of 
/3 in the adjusted normal dynamic linear model given by (2.2 1) - (2.22) with 
adjusted observations yt and respective adjusted observational variances Vt 
leads to the posterior mode of (3. The expressions of yt and Vt were given 
in Section 3.2.2.
The IRLS algorithm provides the adjusted full conditional distribution 
7r0 for block (3 given by (2.27). From this distribution, adjusted full con­
ditional distributions for subsets of (3 may also be obtained. In particular, 
the adjusted distributions frt for blocks (3t, t =  1 , . . . ,  n, are given by (2.26). 
These distributions may be used as proposal kernels. They define a Marko­
vian process as y and V depend on the value of /3^~lK Note that these 
are all multivariate normal distributions and therefore it is simple to draw 
from them. The draws are proposed and they may either be accepted or 
rejected depending on the acceptance probabilities. The complete sampling 
scheme for (3 and $  is given by:
1. Initialize the iteration counter of the chain j  =  1 and set initial values
^(0) =  (0 (0)^  _ _ ^ p W y  and ^ ( 0)_
2. Draw /3* from the density #/j(/3).
3. Calculate the acceptance probability a((3^~l\ (3*) of the move given 
by (6.7) with q{j3^~l\ (3*) =  frp(f3*). If the move is accepted, (3^ =  (3*. 
If the move is not accepted, (3^ =  /'3(j_1-) and the chain does not move.
4. Draw $  from its full conditional distribution (6.13).
5. Move counter from j  to j  +  1 and return to step 2 until convergence.
When updating with blocks (3i,...,f3 n separately, steps 2 and 3 above 
are replaced by:
2’a. Initialize the component counter t =  1.
2’b. Draw /3* from the density 7Tt((3t)-
2’c. Calculate the acceptance probability a t(/3p_ 1\/3t*) of the move given 
by (6.7) with qt((3^~l\ (3*) =  7rt(/31* )• If the move is accepted, f3^ =  f t . 
If the move is not accepted, (3^ =  /3p~^ and the chain does not move.

Applications
229
3’. Move the counter from t to t +  1 and return to step 2’b until t =  n.
When t =  n, go to step 4.
For normal models, it was shown that the Gibbs sampler operated over 
the block /? is superior to the one operated over the blocks (3t. Although it 
is reasonable to expect the same behavior here, there are important differ­
ences. The block (3 is highly dimensional for time series of large or moderate 
size. The high correlation between its components forces its complete con­
ditional to be concentrated in a small region of the parameter space. It is 
very unlikely that proposed values are in this region and therefore, they 
are likely to fall well into the tails of this distribution. Consequently, their 
acceptance probability will be very low and the chain virtually does not 
move as a result.
That does not happen to blocks (3t of much smaller dimension. The pro­
posal gives very good approximations for the full conditional distributions 
and high acceptance rates result. These high rates are not artificially ob­
tained through small chain moves. They are governed by the structure of 
the model through the IRLS algorithm. The remaining problem is the high 
correlation between the components of /3, making the chain move slowly 
towards equilibrium. Similar problems were found by Knorr-Held (1997).
An alternative previously discussed is the reparametrization in terms of 
the system disturbances wt. The advantage again is that despite the high 
prior correlation between the j3t, the disturbances are independent a priori. 
Again, the components are dealt with separately but as most of the corre­
lation is removed, the scheme is expected to converge at much faster rates. 
The drawback of the approach is the amount of extra calculations required 
by the reparametrization (see Table 6.5). Details about the method may 
be found in Gamerman (1998). Another possibility suggested by Shephard 
and Pitt (1997) is to form blocks containing small collections of j3t- The 
groups are divided at random which seems to improve convergence.
E xam ple 6.8 The data set given in Table 6.7 and Figure 6.13 concerns 
a study on advertising awareness (Migon and Harrison, 1985). Samples of 
nt =  66 people were selected at random every week for an opinion poll and 
asked whether they remembered having seen a given advertising campaign on 
TV. A weekly cumulative measure of campaign expenditure was constructed 
and is also depicted in Figure 6.13. The model used for this problem was a 
dynamic logistic regression
logit(iTt) 
=
A  
=
Vt 
~  
=
bin(nt,nt)
ntn
Pit +  P2 txt =  (1 ,x t)(3t 
(3t- 1 +  wt , wt ~  N (0, W )
The main features of this particular data set are a campaign change before

230
Metropolis-Hastings algorithms
t
x t
Vt
t
x t
Vt
t
Xt
Vt
1
490
29
31
66
05
61
501
09
2
450
20
32
60
15
62
454
05
3
406
21
33
54
07
63
483
11
4
365
20
34
48
10
64
522
13
5
331
*
35
43
10
65
559
09
6
315
*
36
39
15
66
519
09
7
376
*
37
35
07
67
467
11
8
441
*
38
32
09
68
420
08
9
506
22
39
50
13
69
378
08
10
502
32
40
116
11
70
340
12
11
544
27
41
196
11
71
306
09
12
489
29
42
268
15
72
276
07
13
440
27
43
325
10
73
248
06
14
396
23
44
367
13
74
232
08
15
357
25
45
386
23
75
201
09
16
321
25
46
397
21
76
181
05
17
289
15
47
413
*
77
163
10
18
260
20
48
423
*
78
146
09
19
234
14
49
420
*
79
132
04
20
211
15
50
490
10
80
119
03
21
190
17
51
539
15
81
107
12
22
171
15
52
581
19
82
96
13
23
154
09
53
603
23
83
86
06
24
138
14
54
580
15
84
78
08
25
124
11
55
524
11
85
70
05
26
112
13
56
499
08
86
63
05
27
100
05
57
552
15
87
57
07
28
91
17
58
597
07
88
51
03
29
82
11
59
611
19
89
46
05
30
73
14
60
557
14
90
41
05
Table 6.7 Data from Example 6.8. * =  missing value.
week 41 and a few  m issing points fo r weeks during which the poll was not 
made.
The average trajectory o f the expenditure coefficient fo t is plotted in Fig­
ure 6.14 fo r the sam pling schem es based on the state param eters fit and 
the system  disturbances w t . Convergence is clearly fa ster when sam pling 
the disturbances, as expected. The drawback o f higher com putational cost is 
only serious fo r tim e series o f very long size. The conceptual advantages

Applications
231
w e e k
Figure 6.13 Data on advertising awareness. The dots represent the weekly per­
centage of people that recalled having watched the advertising campaign of a given 
product on TV. The vertical bars represent a weekly cumulative measure of ad­
vertising expenditure.
seem stronger and should prevail in the choice of the method. Estimates for 
expenditure coefficients are plotted along with confidence limits in Figure 
6.15. Two distinct levels are observed with a clear reduction after week 41, 
showing that the first campaign was more effective in terms of awareness. 
An increase in the uncertainty levels can be observed for the last weeks of 
the second campaign.
6.5.4 Spatial models
Spatial models were also introduced in Chapter 2 and some MCMC schemes 
based on Gibbs sampling reported in Section 5.5.3. The presentation here 
is concentrated on cases where direct sampling from full conditional distri­
butions is not possible or computationally unfeasible.

232
Metropolis-Hastings algorithms
(a)
O 20 40 60 SO
week
<fc>>
O 20 40 60 SO
Figure 6.14 Average trajectory of @2t in 500 parallel chains with number of iter­
ations for sampling from: (a) system disturbances; (b) state parameters.
“I 
I 
I 
I 
I
0 
20 
40 
60 
80
w eek
Figure 6.15 Point estimate with confidence limits of the expenditure coefficient 
Pa-

Applications
233
Consider first the GMRF model (2.33) with parameters 9, (j) and W . 
These parameters can be sampled separately in a Gibbs cycle. The full 
conditional distributions of 0*, 9, <f> and W  were already derived in Section
5.5.3 and shown to be easily sampled from.
An alternative and possibly more efficient sampling scheme is provided 
by joint sampling all model parameters. Jointly sampling (9, <j>, <I>) was also 
mentioned in that section and is achieved by sampling (<j>, 5>) from their 
marginal posterior distribution and then sampling 9\(<f>, $ ) from its full 
conditional posterior distribution. The latter is easily and efficiently sam­
pled from with the methods of Rue and Tjelmeland (2004) and was given 
in Exercise 2.23. The former is given by
where C _1 =  (j)Id +  ® K.
The density (6.14) cannot easily be sampled directly and Metropolis- 
Hastings algorithms may be used here. Proposal densities should be speci­
fied. In addition to those described earlier in this chapter, other forms can 
be applied. Gamerman, Moreira and Rue (2003) used Gamma proposals 
centered around previous values and shape parameters tuned for appropri­
ate acceptance rates. This is equivalent to a random walk proposal in the 
log scale with log-Gamma proposal densities.
When the observational model is non-Gaussian, then the full conditional 
of 9 is no longer Gaussian and becomes difficult to sample from directly. 
Once again, Metropolis-Hastings algorithms provide a feasible alternative. 
The methods used in the previous section to generate normal proposals for 
state parameters and designed to mimic their posterior distribution could 
be applied. Alternatively, Rue and Tjelmeland (2004) suggested the use 
of quadratic approximations directly to the likelihood thus also generating 
normal proposals.
Analysis of spatially distributed data is an area where simulation meth­
ods using Markov chains have been heavily used. Besag, York and Mollie 
(1991) used the Gibbs sampler with the rejection method to sample from 
(2.36). Green (1991) suggested using instead proposals in the form qi(9i, ■) =  
fiv(s (1 —a)b +  a9i, (1 — a2)c), i =  1, . . . ,  d. The constants a, b and c are cho­
sen so as to make the test ratio as constant as possible thus increasing the 
chances of acceptance of the proposed value. This form of proposal falls 
into the category of autoregressive chains described in Section 6.2. Note 
that a =  1 gives the random walk with variance c. The guidance provided 
for the choice of this variance in the above section may be used here for 
arbitrary values of a.
Once again, the question of blocking arises and different blocking strate-
(6.14)

234
Metropolis-Hastings algorithms
gies can be applied in this setting. One possibility is to sample 9 jointly. In 
this case, proposals transition kernels qe{9, •; $ ) for 9 and 
•; 9) for $
must be specified. Note that both kernels must be conditional on the other 
parameter as was made explicit in their notation.
Knorr-Held and Rue (2002) discussed and compared blocking schemes. 
They favored sampling all parameters in a single block based on empirical 
evidence about the mixing properties of the chains. This means in the 
setting presented here that new values <I>* are generated from a proposal 
<7<j>, then a new value 9* is proposed from its normal proposal qe, conditional 
on $*. In other words, the (joint) transition kernel is q((9, $ ), (9*, $*)) =  
<7<j.($, $*)<?0(0, #*;<!>*)• Then, the value of ((9*,$*) is jointly tested with 
acceptance probability
. 
f 
7r(0*,$*) 
g*(fc%$tt-1>)®,(0*>0t f-1>;$tt-1>)'|
m m \ ’ 7r(^Cj-i),^0--i)) 
g ^ O - D . S * ) ^ ^ - 1) ,# * ;^ )  
J '
Note that when jointly sampling (9, $ ), the (marginal) transition kernel for 
$  does not depend on 9 unlike the (conditional) transition kernel of 9, that 
depends on $.
Similar comments apply to the distance-based GRF models. For normal 
models, almost all full conditional distributions are easily sampled from 
and detailed in Section 5.5.3. The only exception is the full conditional of 
the correlation parameters A given by (5.9). This distribution has no simple 
form and typically Metropolis-Hastings proposals are used, specially when 
A is a vector of hyperparameters.
Full conditional distributions for 9 in non-normal observation models are 
no longer amenable for easy sampling. Many of the proposals described in 
this chapter can be applied. Diggle, Tawn and Moyeed (1998) considered 
these models and proposed MCMC schemes based on single site moves. 
Ideas of Knorr-Held and Rue (2002) can be adapted for this setting and 
used to produce sampling schemes with different blocking pattern.
6.6 Exercises
6.1 Prove that the transition kernel p of the Metropolis-Hastings algorithms 
satisfies the detailed balance Equation (6.3) and hence has stationary dis­
tribution 7r.
6.2 The algorithm proposed by Barker (1965) set
,a 
_  
*•(</>)
’ 
7r(9) +  7x(4>) '
Show that this algorithm produces a reversible chain and has stationary 
distribution ir, if q is symmetric.
6.3 Certify yourself that large moves tend to be rejected and small moves

Exercises
235
are very slow to converge by considering sampling from a N(0,1) distri­
bution. Consider normal proposal transitions with variances ranging from
0.01 to 100. Compare also the independence chains with an arbitrary, fixed 
mean with the random walk chain with proposal means given by the previous 
value of the chain.
6.4 
Certify yourself by graphical and/or analytical terms that ifq(9, ■) and
7r(-) are continuous, the acceptance probability will be close to 1 when moves
proposed by q make the chain move slowly.
6.5 Consider random walk chains. Show that if the distribution f w of the 
disturbances Wj is symmetric around 0, the chain is symmetric. Show also 
that the Metropolis algorithm described in Section 6.1 is an example of a 
random walk chain and specify its distribution f w.
6.6 Consider sampling n by the rejection method with an envelope density 
q for which complete blanketing is not assured and put it into the context 
of Metropolis-Hastings.
(a) Defining the blanketing region C =  {9 \-k(9) <  A q(9)}, obtain that
[ 
l 
, 9 e C
a ( M ) = <  Aq(9)/n(9) 
, 8 ^ C , 0 e C
[ min{l,ty(0)/tu(0)} 
, 9 $ C , < f r $ C
where w =  ir/q.
(b) Discuss the computational advantages of the above scheme over inde­
pendence chains with proposal q and over rejection sampling.
6.7 Consider r Markov chains with transition kernels pi, i =  1, . . . ,  r, with 
a common stationary distribution n. Show that the resulting kernel of the 
mixture of these transitions will also have stationary distribution tt. Repeat 
the exercise for a cycle chain.
6.8 Consider a parameter vector 9 — (9i, . . . ,  Od)' with posterior density 7r 
and the componentwise Metropolis-Hastings algorithm with proposed ker­
nels q.i(9i, (pi) and acceptance probabilities ai given by (6.7), i =  1, . . . ,  d.
(a) Show that the component transition kernels formed define Markov 
chains with stationary distributions given by the full conditional distri­
butions of 9i, i =  1,... ,d.
(b) Extend the results of Section 6.4 to prove that n is a stationary distri­
bution of the cycle kernel defined by the componentwise moves through 
all components of 9.
6.9 Consider the following univariate version of Example 6.2
7r(0) =  0.9f N(9-, 0,1) +  0.1 f N(9; 3.5,0.5)
where again the goal is to sample from it (9) using either the random walk 
Metropolis or the independence Metropolis algorithm. Assume also that the

initial value is either 0(o) =  - 5  or 0<(>> =  - 7 , the random walk Metropolis 
proposal is a normal distribution with standard deviation r =  0.1 or r =  0.5 
and the independent Metropolis proposal is a zero mean normal distribution 
with standard deviation r =  1 or r  =  3. Run the algorithms for at least 
10000 iterations and compute the respective effective sample sizes (Equation 
4-10 from Chapter 4)-
6.10 Consider the non-linear hierarchical model described in Example 6.3. 
Obtain the expressions of the full conditional densities for the hyperparam­
eters /I, a 2 and W  and obtain the expressions of the proposed densities for 
the regression coefficients 
, . . . ,  ipn that were discussed in the text.
6.11 Describe in detail the sampling scheme for the dynamic generalized 
linear model for blocks wi , . . . ,  uin and $  based on the IRLS algorithm. In 
particular, obtain the expressions of the proposal transition kernels and of 
the acceptance probabilities.
6.12 Consider the dynamic linear model of Section 6.5.2. Show that 
tt(/3, ct2, W ) oc f N(!3; M , Q ~ l ) f N{y\FA, F P ~ lF  +  a2In)p(a2, W )
and
n (a2,W ) oc f N(y, F A , F P ^ F  +  a 2In)p(a2, W ).
6.13 Show that the posterior density of all model parameters in the linear 
dynamic model can be written as
7r(/3, ct2, W ) oc M /3 ; M, Q ~ l) f N{y, F A , F P ^ F  +  a2In)p(a2,W ),
where p(cr2, W ) is the prior for (ct2, W ).
6.14 Describe in detail the sampling scheme for the dynamic generalized 
linear model for blocks w\,... ,w n and $  based on the IRLS algorithm. In 
particular, obtain the expressions of the proposal transition kernels and of 
the acceptance probabilities.
236 
Metropolis-Hastings algorithms

CHAPTER 7
Further topics in MCMC
7.1 Introduction
The material presented in the previous chapters covers most of the rel­
evant work on inferential procedures for a given model through Markov 
chain simulation techniques. Chapter 5 presented the Gibbs sampling tech­
nique and Chapter 6 presented the Metropolis-Hastings algorithm. It was 
assumed there that the adopted model was the true one or at least the 
most appropriate one throughout the presentation. Therefore, generation 
of a sample from the posterior distribution was all that was required. The 
techniques presented showed different ways of doing so.
In this final chapter, some points that lie beyond that basic framework 
will be discussed. Initially, the model will be put under scrutiny. Some tech­
niques for evaluation of the model will be discussed. This evaluation may be 
divided into two complementary activities. The adequacy of a given model 
in the light of the observed data is made in Section 7.2. A more encom­
passing treatment is presented in Section 7.3 where different models are 
considered simultaneously. Depending on the cardinality and complexity 
of the set of models considered, alternative methods still based on Markov 
chains must be considered. Alterations in the structure of a given chain in 
order to speed up convergence are discussed in Section 7.4. There are many 
ways of performing these changes, from alterations in the transition ker­
nel to alterations in the target distribution. Other forms of change involve 
alteration in the generated sample. This chapter presents more advanced 
ideas, generalizing the material of the previous chapters.
7.2 M odel adequacy
Recall from Section 2.7 that a basic ingredient for model assessment is 
given by the predictive density
f(y\M ) =  j  f(y\e,M)p(d\M)dd, 
(7.1)
which is the normalizing constant of the posterior distribution (2.3). This 
predictive density can now be viewed as the likelihood of model M . It is 
sometimes referred to as predictive likelihood, because it is obtained after 
marginalization of model parameters.

238
Further topics in MCMC
An important aspect of model evaluation is the calculation of the predic­
tive likelihood. Usually, its expression cannot be analytically obtained due 
to the complexity of the integrand in (7.1) and approximate methods must 
be used. Equation (3.4) provided an analytical approximation supported 
by asymptotic normal theory. In what follows, methods for approximate 
evaluation of (7.1) using simulation techniques will be presented. Different 
uses of these approximations for model evaluation will then be shown.
Although evaluation of a model presupposes the existence (or possibil­
ity) of other models, the calculations of this section will operate on a single 
model M  at any one time. Therefore, the presence of the model in the 
conditioning part of probability statements will be suppressed. Approaches 
that take into consideration different models simultaneously will be con­
sidered in the next section. In those cases, the model must be explicitly 
considered in the conditioning part of the distributions.
7.2.1 Estimates of the predictive likelihood
For any given model, Equation (7.1) can be written as
f(y ) =  E[f(y\0)} 
(7.2)
where expectation is taken with respect to the prior distribution p(9). This 
simple identity and several generalizations can be used to estimate f  (y ). In 
what follows, the well known Monte Carlo and importance function Monte 
Carlo identities as well as the bridge and path identities are introduced. 
Additional estimators derived from normal approximation to the poste­
rior distribution and Chib’s identities are also introduced. In what follows, 
f(y\9) and 1(0) will be used interchangeably.
Normal approximation
Normal approximation to the model likelihood has previously been given in 
Chapter 3. The normal approximation to the posterior gives the estimate 
(3.4) for f(y ), i.e., p(m )l(m )(27r)d/ 2|V’|1/ 2, which is based on the evaluation 
of the values of m, the posterior mode, and V, an asymptotic approxima­
tion for the posterior variance matrix. Sampling-based approximations for 
m and V can be constructed if a sample 9\, . . . ,  0n from the posterior is 
available. The mode m can be estimated as the sample value rh for which 
7r is largest, i.e., n(rh) =  m axj{7r(0j)}. Similarly, estimates for the poste­
rior variance matrix may be given in the case of an independent sample by 
v  =  £ E " = i(6j -  0)(9j -  0)'> where 0 =  £ E "=1 °i- Therefore,
fo(v) =  p(rh)l(m)(2Tt)d/2\V\1/2
is the normal approximation to f(y ). Lewis and Raftery (1997) named 
this estimator the Laplace-Metropolis estimator. Kass and Raftery (1995),

Model adequacy
239
Raftery (1996) and DiCiccio et al. (1997), among others, discussed alter­
native calculations of the value of m when computation of 7r is expensive 
and of the value of V with the use of robust estimators.
Monte Carlo approximations
As it was pointed out at the beginning of this section, the Monte Carlo 
estimate derived from the identity of Equation (7.2) is
where 9 i,... ,9n is a sample from the prior distribution p(9).
Raftery (1996) argued that this estimator does not work well in cases 
of disagreement between prior and likelihood, based also on applications 
by McCulloch and Rossi (1991). Almost all previous chapters contained 
some discussion of the difficulties associated with approximate inferences 
based on the prior, especially with sampling-based approaches. In light of 
this information, it is not surprising to learn that / 1 does not provide a 
sensible estimate. It averages likelihood values that are chosen according 
to the prior. In general, the likelihood is more concentrated than the prior 
and the majority of 9i will be placed in low likelihood regions. Even for 
large values of n, this estimate will be influenced by a few sampled values, 
making it very unstable. For similar problems, see Figures 1.4 and 3.2 and 
the penultimate paragraph of Section 6.3.3.
An alternative is to perform importance sampling with the aim of boost­
ing sampled values in regions where the integrand is large. This approach 
is based on sampling from the importance density y(9) =  kg*(9) where g* 
is the unnormalized form of the density and A; is a normalizing constant. 
When k is known, Equation (7.2) can be rewritten as
where Eg denotes an expectation with respect to the importance distribu­
tion g(9). This form motivates a new estimate
where 9\,. . . ,  9n is a sample from the importance density g(9).
In many cases, the value of k is not known and must be estimated. Noting 
that
leads to the estimator of k given by k =  (,l/n)Y^j= iP(9j)/g*(8j) where,
n

240
Further topics in MCMC
again, the 9t are sampled from g. Replacing this estimate in fa gives
h(y) =
e ; = i  p(03)/g*(ej)
Special cases of g are given by:
1. Importance function: g(9) =  w(9)
These results can be applied in the Bayesian context by taking the pos­
terior density 7r as the importance density. After a (Markov chain) sim­
ulation process, a sample 
from ir(0) =  kl(9)p(9) is available.
These values can be used in fe with g* =  I x p. In this case, the estimator 
simplifies to
- l
h  (y)
(7.4)
This estimator is the harmonic mean of likelihood values originally pro­
posed by Newton and Raftery (1994). It is commonly known as the 
harmonic mean estimator. The simplicity of f i  make it a very appeal­
ing estimator and its use is recommended provided the sample is large 
enough. Despite its consistency, this estimator is strongly affected by 
small likelihood values. Raftery (1996) relates this weakness to the oc­
casional divergence of the variance of the terms in (7.4).
2. Importance function: g(8) =  5p(9) +  (1 — 5)ir(9)
Newton and Raftery (1994) introduced an estimator that is a compro­
mise between / 1, derived from prior draws, and / 4, derived from poste­
rior draws. More precisely, they suggested g(9) — 5p(9) +  (1 — 8)n(6), 
for 0 < 5 <  1 , as the importance function of identity (7.3). There­
fore, when 61, . . . ,  9n is a sample from g(9), f -2 is the estimator of f(y ). 
Unfortunately, f(y ) needs to be known in order to evaluate n(9). This 
dependence suggests the following iterative scheme to estimate f(y ):
? w ^  _  £ " = 1  l(ei M
T
1](v) +  a  -  w
m
-  
f (  0)
for i =  1,2,...  and, say, f f> 
=  ft\- A small number of iterations is 
usually enough for convergence. Even though / 5 avoids the instability of 
fi, with the additional cost of also simulating from the prior.
3. Generalized harmonic mean estimator
Another generalization of the harmonic mean estimator was obtained by 
Gelfand and Dey (1994). For any given density g(0),
1 =

Model adequacy 
241
where, as before, p is the prior and 7r is the posterior density of 9. So,
m
f ( y ) = /
-7T (9)d9
-1
f(y\0)p(9)
Sampling 9\,... ,8n from 7r leads to the estimate
fe{y) =
9 {Qj)
-1
(7.6)
Even though the method is specified for any density g, appropriate 
choices are very important for a good practical implementation. Gelfand 
and Dey (1994) suggested using g as an importance density for the pos­
terior and to take a normal or t distribution that approximates 7r with 
moments based on the sample of 9. Raftery (1996) presented a simple 
example where g was taken in product forms for each parameter compo­
nent. The estimates obtained are highly inaccurate, showing that some 
skill is required in choosing g.
Annealed importance sampling
Neal (2001) introduced a modification of the weighted resampling algorithm 
for sampling from a target distribution (usually the posterior) based on a 
sequence of proposal densities. The algorithm is particularly useful when 
the target density and the first density of the sequence have relatively 
little overlap (vague prior distributions or peaked likelihood functions, for 
instance). As a by product, the derived weights can be used to compute an 
estimate of f(y ).
More specifically, let go be the starting density, gk be the target density,
9j (0) =  cj {go(9)}1~ ^ {g k(9 )}^
be the intermediate densities, where 0 =  Ao < Ai <  • • • < Afc =  1 and 
cj be the normalizing constants. Starting at 6^°\ a sequence 9^ , . . . ,  8 ^  
is sampled as follows. First, sample 8* from go(9) and set 9 ^  =  9* with 
probability
I 
9 i( » 101) » ( » * )  J
Then, for i =  2, . . . ,  k, sample 8* from qi-i(8 ^ ~ 1\ •) and set 8 ^  =  9* with 
probability
’ 
) 
I ' 9 i ( 8 ^ ) )  ^ ( 8 ( ^ , 9 * )  j '
By repeating the previous algorithm n times and keeping the 8ik,s, it can
(k) 
(k)
be shown that the sample 8\ 
and the weights u>i,... ,ujn jointly

242
Further topics in MCMC
summarize the target distribution gk(0). Unnormalized weights are com­
puted as (see Exercise 7.2a)
-  
A
 a,(ef) 
. 
i 
^
- ,  =  1 1 —
J =  
(7.7)
i = 1 9 i — 1 (“j  
)
Weights are obtained as uij =  Cjj/ ^ ”=1 £jj, j  =  1 
When go(0) is
the prior p(0) and gk{9) is the posterior n(0), the annealed importance 
sampling estimator of f(y ) is (see Exercise 7.2b)
M i =  
( 7 -8 )
j= i
As a by product, Egk(0), for instance, can be approximated by the 
weighted average ]T"=1 ujjO ^. When k — I, the algorithm degenerates
into the simple Monte Carlo estimator f\. Neal (2001) argued that the im­
plementation of the annealed importance sampling is commonly straight­
forward and particularly useful when handling isolated modes. Combining 
independent sampling with Markov chain sampling is another attractive 
aspect of the estimator.
Bridge and path identities
Meng and Wong (1996) introduced the bridge sampling to estimate ratios 
of normalizing constants. Notice that f(y ) can be rewritten as
for any arbitrary bridge function a(6) with support encompassing both 
supports of the posterior density n and the proposal density g (see Exercise
7.3). If a(6) =  1 /g(O) then the bridge estimator reduces to the simple Monte 
Carlo estimator /i . Similarly, if a(6) =  {p(0)l(9)g(9)}~ 1 then the bridge 
estimator is a variation of the harmonic mean estimator. If 9 i ,.. .,9 n and 
are samples from ir and g, respectively, then
£  E fe i a(0j)p(0j)l(0j)
is a bridge estimator of f(y ). Meng and Wong (1996) showed that the 
optimal mean square error a function is a(0) =  {g{0) +  (m/n)'ir(0)}~1, 
which depends on f(y ) itself. By letting Wj =  l(0j)p(03)/g(Oj), for j  =  
1 , . . . , n  and u)j =  l(0j)p(0j)/g(0j), for j  =  1 , . . . , m ,  they devised the

Model adequacy
243
iterative scheme below to estimate f(y ):
Hi), , 
+ s 2f^ ~ 1)(y )}-1
/g {V) =  
--------V 
fii-l), 
*
nT ,j=l[*lU j +  *2fs 
\y)] 1
for * =  1,2,..., sj =  n/(m +n), s2 =  m/(m +  n) and, say, /g 0^ =  fa. A small 
number of iterations is usually enough for convergence. Other alternatives 
for a are considered in Meng and Wong (1996).
The bridge sampling efficacy decreases as the distance between tr and 
g increases. Metaphorically speaking, it will be costly (and inefficient) to 
cross the bridge when the length of the bridge is large. Chen and Shao
(1997) extend the bridge identity to situations where g and tt are defined 
over spaces of different dimensionality. For further details about the bridge 
sampler, see also Meng and Schilling (1996).
Gelman and Meng (1998) generalized the bridge sampling by replacing 
one (possibly long) bridge by infinitely many shorter bridges or, as they call 
it, a path. Suppose that instead of using the bridge function a to connect 
p{9) and p(6)l{6), a path function h{9|A) is constructed with h{9|A =  0) =  
p(9) and h(9\\ =  1) =  p(9)l(9). In other words, h(9|A =  0) represents the 
beginning of the path and h{9|A =  1 ) represents the end of the path.
Let c(A) =  J h(9\X)d9 be the normalizing constant of h(91A), so <?(#|A) =  
h(9\\)/c(\) is a normalized density function and c(0) =  1 and c(l) =  f(y ). 
Then, it is easy to see that (see Exercise 7.4)
/ ( y ) = e x p j ^  J ^ ^ ^ -g (0 \ \ )g {\ ) d9 d\^ 
(7.10)
where H(9, A) =  ^  log/i(#|A) and g(A) is any density for A over the unit 
interval [0,1]. Usually the above integrals cannot be solved analytically and 
must be approximated, for instance, by Monte Carlo integration leading to 
the path estimator /g(y).
Candidate’s estimators
A very simple estimate, usually called the candidate’s estimator (Besag, 
1989), can be derived from the fact that f(y ) =  f(y\9)p(9)/it(9). Typically, 
f{y\9) and p{9) are typically easy to calculate but 7r(9) is not. However, if a 
sample of 7r is available, some form of histogram smoothing can be applied 
to get an estimate of tt.
Chib (1995) introduced an alternative estimate of tt when full conditional 
densities are available in closed form, as in Gibbs sampling, for instance. 
Note first that, for 9 =  {9\,. . . ,  9d) and i =  2 , . . . ,  d,
T r ( 8 i \ 9 i , . . . , 9 i -1) = j  
' J
'K(8i\8-i)Tr(9i + i , . . . , 8 d) d 9 i + i - - - d 9 d

244
Further topics in MCMC
7r(0i|0i,...,0i-i) =  
^
, . . . ,  o f )
n j =i
where ( 0 ^ , . . . ,  9^ ) ' ,  j  =  1, . . . , n, is a sample from n(9).
As 7r(0) =  7t((9i) 
7r(0j|0i , ... 
an approximation tt for the pos­
terior is given by
d
ir(0) =  7t(9i ) ]^[ Tt{9i\9\,... , 6>j_j) .
»=i
Once an approximation 7r for 7r is available, it can be used to give another 
estimate
t , x 
f(vW )p(9) 
( 7 , u
M u )  =  
i ( e )  - ■
Note that any value of 9 can be used in the expression of /io  and if 7r 
could have been obtained without error, they would all provide the same 
estimate of f(y ). Obviously, 9 should be chosen so that n has the smallest 
possible estimation error. This narrows the choice of 9 to the central region 
of the posterior where ir is likely to be estimated more accurately. Simple 
choices are the mode and the mean but any value in that region should be 
adequate.
Chib and Jeliazkov (2001) extended the above idea for cases where some
(or none) of the full conditional densities are of unknown form and difficult
to sample from and Metropolis-Hastings output is available. Consider the 
simple case where a value 4> is sampled in a block from the Metropolis- 
Hastings proposal q{9,<j>) and accepted with probability a(9,(j)). For any 
value of <f>, Chib and Jeliazkov (2001) showed that (see Exercise 7.5)
*(4) =  — FT------r , ■ ,nU  
• 
(7-12)
If 6*0^, • • •, #o"u) and 
• • •) 
are samples from 7r(-) and q(4>, •), re­
spectively, then
n o 1Y ^ l 1a(0^\4>)q(e^) ,4>)
Trio) = ------------------------------- -----------
is an estimate of 7r(cj>), which can be used to estimate f ( y)  via
t / \ 
f{y\4>)p{4>)
h i(y ) =  — tt—,—  •
7T(0)
Chib and Jeliazkov (2001) also extended the above idea to multiple pa­
rameter blocks, while Chib and Jeliazkov (2005) estimated marginal like­
lihood based on output from accept-reject Metropolis-Hastings schemes.
suggests approximating 7r(0j|0i,. . . ,  0j_ 1) by 7 r ( 0 * | 0 i , 0 j - i )  given by

Model adequacy
245
By noticing the similarity between Equations (7.9) and (7.12), Mira and 
Nicholls (2004) showed that Chib and Jeliazkov’s estimator is a special case 
of the bridge sampler with a((f>,8) =  a(8)p(8)l(8).
Table 7.1 lists the several estimates of the predictive likelihood intro­
duced in this section, while Examples 7.1 and 7.2 illustrate and compare 
them.
Estimate
Proposal density/method
/o
normal approximation
A
p(8)
h
unnormalized g*(8)
h
unnormalized g(8)
7r(8)
h
Sp(8) +  (1 -  S)w(8)
fa
generalized harmonic mean
h
annealed importance sampling
h
optimal bridge sampling
h
path sampling
h o
candidate’s estimator from Gibbs output
fn
candidate’s estimator from Metropolis output
Table 7.1 List of estimates of the predictive likelihood.
DiCiccio, Kass, Raftery and Wasserman (1997), Han and Carlin (2001) 
and Lopes and West (2004), among others, compared several of estimators 
introduced in this section.
E xam ple 7.1 Consider Example 2.3 where y i, . . . ,  yn are a N(8, a 2) ran­
dom sample, with a2 known and Cauchy prior density p(8) =  7r_1(l +  
6>2) - 1 . Assume that y =  7 and a2/n =  4.5. The likelihood function is 
1(B) =  /Ar(6>;y,<72/n) and the posterior density for 8 is n(8) oc p(8)l(8). A 
very accurate approximation to the normalizing constant was obtained by 
hJ2kJ=i l(0j)p(0j) =  0.00963235, where 81 =  -1 5 ,0 , =  -1 5  +  (j -  1 )h ,j =
2, . . . ,  k — 1,8k =  15, k =  107 and h =  3 x 10~6. 
^
A normal approximation to the posterior density is g(8) =  fN (8\m ,V) 
where m is the posterior mode and V  is a Monte Carlo estimate of the 
posterior variance of 8. The posterior mode, m =  5.384, was obtained by 
a Newton-Raphson-type algorithm that started at 8 ^  =  0.05 and ran for 9 
iterations (see Section 3.2.2).
Subsequently, 100000 draws from a N (m , 22) distribution were used in a 
weighted resampling scheme to generate 100000 draws from the posterior 
distribution n and compute V =  2.492. Figure 7.1(a) exhibits the likelihood 
function as well as the prior and the normal approximation. The resulting 
histogram approximation of tt appears in Figure 7.1(b).

246
Further topics in MCMC
(a)
(b)
- J l
llfrrK—
(c)
<d)
0.01 
0.03 
0.05 
0.07 
0.09
6
I 
I------1------1------1------1------- 1
4.8 
5.0 
5.2 
5.4 
5.6 
5.8 
6.0
Figure 7.1 Comparing several estimators of f(y ). (a) Prior density p(0) (solid 
line), likelihood function 1(0) (dashed line) and normal approximation g(0) (dotted 
line)._ (b) Histogram approximation for tt along with tt. (c) f*,(y) for 5 6 [0.01, 0.1]. 
(d) fn (y) for several values of <p in [4.8,6.0]. Horizontal lines in (c) and (d) 
correspond to 0.00963235, the true value of f(y ).
For f$, 5 was set to 0.1, i.e., an average of 10% of prior draws. For
6 in [0.01, 0.1], the estimation error of / 5 ranged from 0.02% to 2.04% 
(see Figure 7.1(c)). For f 6 and f s, the proposal density g is the normal 
approximation used above. A total of 20000 draws is used to compute f 7, 
where A j =  0.1 j , for j  =  1,... ,5 and Xj =  0.51 +  0.035(j -  6), for j  =
6, . . . ,  20 and qi(9, (p) =  fN(<p] 0,0.12), for i =  1, . . . ,  19. For /§’s iterative 
algorithm, the initial value was set at / 0. For f n , 0 =  m and q(<p,0) =  
f N(0;<p,V). Also, f n =  0.00970631 when (p =  Ev (0) «  5.076856. In fact,

Model adequacy
247
the estimation error of /n  ranged from 0.06% to 1.74%, for <p £ [4.8,6.0] 
(see Figure 7.1(d)). The results of all estimators are given in Table 7.2.
f ( y )
0.00963235
Estimator
Estimate
Error (%)
fo
0.00932328
3.21
fi
0.00960189
0.32
u
0.01055301
9.56
h
0.00957345
0.61
h
0.00962871
0.04
/7
0.01044794
8.47
h
0.00963110
0.01
/11
0.00969942
0.70
Table 7.2 Comparing several estimator of f(y). Error =  100|/(y) — f(y)\/f(y). 
For fs, S =  0.1 and for f n ,  cp =  m.
E xam ple 7.2 Lopes and West (2004) examined the performance of sev­
eral estimators of f(y\k) in a traditional k-factor model. In one case, a 
9-dimensional vector is simulated from a 3-factor model. Subsequently, k- 
factor analysis was entertained for k =  1 ,... ,5, i.e.,
Vi\fki,0k 
~  
N (Pkfki,^k) 
fu  
~  
N (0 ,Ik)
for i =  1,... ,n and 8k =  (Pk, £fc) and £& =  d i a g ( ... , o ^ ). Identifia- 
bility constraints impose that Pku >  0, for i =  1 , . . . ,  k, and Pkij =  0, for 
j  >  i, which characterizes the lower block triangular shape of pk (Lopes,
2000).
The true model parameters are
(  0.99 
0.00 
0.00 0.99 
0.99 
0.00 0.00 
0.00 
0.00 \
p'3 
=  
0.00 
0.95 
0.00 0.00 
0.00 
0.95 0.95 
0.00 
0.00
\ 0.00 
0.00 
0.90 0.00 
0.00 
0.00 0.00 
0.90 
0.90 J
and £3 =  diag(0.02,0.19,0.36,0.02,0.02,0.19,0.19,0.36,0.36). The prior 
distribution of Pkij is normal, i.e., pkij ~  N (0, 1 ), for i =  2,... ,9, j  =
1, . . . ,  i — 1 and k =  1, . . . ,  5, and the prior distribution of 
is inverse 
Gamma, i.e., a^  
IG ( 1.1,0.05), so that E (a 2i) =  0.5, for i =  1, . . . ,  k.
Conditional on k, a Gibbs sampler is promptly available for inference 
about (Pk, Efc) and fki, ■ ■ ■, fkn ( Geweke and Zhou, 1996; Aguilar and West, 
2000; Lopes, 2000; and Lopes and West, 2004). Posterior inference was

248
Further topics in MCMC
based on 10000 iterations as bum-in, followed by a further 10000 iterates 
that were sampled every ten steps to produce a final MCM C sample of size 
1000. For fa, the control parameter S was set at 0.05 and the iterative 
scheme run for 20 iterations.
Uniform prior model probabilities were assumed, i.e., P r(k) =  0.2 for 
k =  1 , . . . ,  5. Table 7.3 shows the frequencies, out of 1000 simulated sets of 
data, at which each k-factor model had the highest posterior model proba­
bility. Several of the approximation methods reliably identify the true model 
structure, which gives some indication of their likely utility in real data 
analysis. Among the approximate Bayesian methods, those based on the 
harmonic mean estimator (fi), the Newton-Raftery estimator (fa) and the 
Chib’s estimator (fao) exhibited worse performances. Sensitivity to prior 
distributions was studied by Lopes (2000, 2003).
Estimator
Number of factors
k =  1
k =  2 
k =  3
k =  4
k =  5
fo
0
1
999
0
0
h
0
0
650
228
122
fa
0
0
615
258
127
fa
0
0
998
2
0
fa
0
11
985
4
0
f  10
0
12
848
138
2
Table 7.3 Number of times (out of 1000 replications) that a particular k-factor 
model was chosen as the best by each one of several estimators.
7.2.2 Uses of the predictive likelihood
When the prior distribution p(0) is informative and proper, there is no 
problem in using f(y ) to evaluate different models. When the prior is im­
proper, f ( y)  cannot be calculated because the integral (7.1) diverges. In 
cases where the prior is proper but not very informative, the use of f (y) 
as a tool for model evaluation should be made with care. So, the estimates 
obtained above for approximating f(y ) with weak prior information are 
likely to be unstable.
Even though the density (7.1) is the canonical form for model evalua­
tion as prescribed by theory, it is not necessarily the only one. Different 
justifications have led to other approaches by several authors. Consider a 
sample y — (y\
a
n
d
 y c  denotes the subset of y containing the

Model adequacy
249
observations with indices in C. So, if C =  { l , n } ,  y c =  (yi,2/n)/ and if 
C =  { 1, . . .  ,i — l , i  -I-1,... ,n }, y c =  y~i■ Gelfand and Dey (1994) showed 
that many densities used for model evaluation can be written in the generic 
form
f (y s 1\ys2) =  J  f (y s 1\o)p{e\ys2)de.
Si =  { 1 , . . .  , n} and S2 =  4>, the empty set, leads to f(y ). If Si =  S2 =  
{ 1 , . . . ,  n}, then the density suggested by Aitkin (1991) for use in the pos­
terior Bayes factor is obtained. The densities appearing in the definitions of 
the intrinsic Bayes factor of Berger and Pericchi (1996) and the fractional 
Bayes factor of O ’Hagan (1995) also fit into this formulation. The main mo­
tivation for these alternative forms is their use with improper priors. An 
adequate choice of S2 removes the impropriety of p(0\ys2) and therefore 
f(ysi\ys2) does not diverge and can be calculated.
Another density extensively used by Gelfand (1996) and Gelfand, Dey 
and Chang (1992) was the cross-validation predictive density f(yi\y~i) 
(Stone, 1974). Geisser and Eddy (1979) suggested the use of the prod­
uct n iL i f(Vi\y-i) ° f these densities as a surrogate indicator of the value 
of the predictive likelihood f (y) through the pseudo Bayes factor
n ? -i/(K | y -i,M 0)
nr=i / (y*l y-*’ M i)
that would approximate the Bayes factor. Gelfand, Dey and Chang (1992) 
suggested many forms of use of the predictive densities through the expec­
tations of functions g(yi)* under f(yi\y-i). Among them is the prediction 
error g\(yi) =  j/* — Vi,obs where y^obs is the observed value of yi. The expec­
tation of g\ is 7ij =  E(yi\y-i) — y^obs■ These values may be standardized 
to 7 ^ =  ■yu/y/Var(yi\y-i). If the model is adequately fitting the data, the 
values of 7^ should be small. Under approximate normality, 95% of them 
should roughly lie between -2 and 2. The quantity
4 =  1
may be constructed and used as an indicator of model fit. The smaller its 
value, the better the fit of the model to the data.
Another useful function is g2(yi) =  /(y* < yi,obs) with expectation 721 =  
P f(yi 
Vi,obs\y-i)- Considering 72i as functions of yi^bs they are f/[0 ,1] 
distributed but they are not independent. So, the behavior of a sample 
from a t/[0 ,1] distribution is expected. A large number of 72iS close to 0 or 
1 indicates observations in the tails of their predictive densities, i.e., poor 
fit of the model, whereas a large number of y2iS close to 1 /2  indicates ob­
* T h e  reason w h y th e  fu n ction s g are con sid ered  in stea d  o f  con sid erin g  d ire ctly  their 
e x p e cta tio n s  w ill b e  m a d e clea r b elow .

250
Further topics in MCMC
servations close to their corresponding predictive medians, i.e., good model 
fit. A possible summarization of the information from the 72jS is obtained 
with
n
G 2 =  £ (7 2 *  -  0.5)2 . 
i=l
Again, the smaller its value, the better the fit of the model to the data.
Similarly, the functions 53(2/*) =  I(Vi e {yi\f{yi\y~i) <  f{Vi,obs\y-i)} 
and 54(yi) =  I(y% € [y^obs —e, J/»,o6*+e])/2e may be used. Their expectations 
are respectively given by 73j =  Pr({yi\f(yi\y-i) <  f ( y it0bs\y-i)}\y-i) and 
74i =  f(yi,obs\y-i), when e —> 0. Again, as functions of yli0bs, the 73ls are a 
sample from a U[0,1] distribution and therefore can be summarized by
n
G3 =  E (7 3 , -  0.5)2 . 
i= 1
As for the 74*s, also known as the conditional predictive ordinate (CPO), 
the natural summarizing quantity is their product
n
g 4 = n  74 i 
1=1
which is also present in the pseudo Bayes factor.
None of the functions of interest 7ji can be obtained analytically for 
most models. However, sampling-based estimates can be obtained as they 
were written as expectations with respect to a distribution. Assuming the 
presence of a sample y n ,..., yin from p(yi\y~i), the 7ji can be estimated 
by
1 
"
7ji =  -  
, j  =  1 , 2,3 ,4.
n 1=1
A sample from p(j/j|y_j) can be obtained by noting that
p(yi\y-i) = 
J  p(yi,o\y-i)do
=  J  p{yi\0)p{Q\y-i)d9
where the last equality follows from the usual assumption of conditional 
independence of the observations given 6. Therefore a draw from p(yi\y-i) 
is obtained by drawing 6, from p(0\y^i) and subsequently drawing yi from 
f{yi\6*) (Section 1.3).
Only a sampling scheme for p(9\y~i) remains to be described. Gelfand, 
Dey and Chang (1992) suggestd the use of a resampling method (Section 
1.5) with some approximating density q(9). A natural choice for q is the 
posterior density 7r(0), for two reasons. For moderate to large samples, the 
exclusion of a single observation is not likely to significantly change the

Model adequacy
251
posterior. Therefore, n(6) is likely to approximate p(9\y-l) well. Also, a 
sample from 7r is already available from an inferential procedure for a given 
model.
Bayes’ theorem with prior p(9\y_i) and observation yi gives 7 r (0 ) 
oc 
f(yi\9) p(6\y-i). Hence, following the construction of a sampling-importance 
resampling scheme in Section 1.5 and assuming the presence of a sample
9 i,... ,9n from 7r, the weights
Tr(flj) 
1
W> K p{ej \y.i) K f ( y t\03)
can be formed. These weights are normalized to add to 1 and used in 
the resampling scheme. The resulting sample has approximate distribution 
p(Q\y-i). A similar scheme can be devised for the rejection method with 
the disadvantage of finding a constant ensuring complete envelope. Gelfand 
(1996) also described an alternative form to estimate the dji directly from 
a sample of 7r. Azevedo (2002), for instance, used CPOs and pseudo Bayes 
factors when combining related studies through hierarchical models with 
Dirichlet process priors (Dey, Muller and Sinha, 1998).
E xam ple 7.3 Data on the temporal evolution of the dry weight of onion 
bulbs (y) is presented in Table 7.4 and Figure 7.2 (Ratkouiski, 1983).
16.08 
33.83 
65.80 
97.20 
191.55 
326.20 
386.87 
520.53
590.03 
651.92 
724-93 
699.56 
689.96 
637.56 
717.41
Table 7.4 Data on the temporal evolution of the dry weight of onion bulbs (y).
Gelfand, Dey and Chang (1992) considered two non-linear models
Logistic model 
: 
yt ~  N(9\/(l +  929\),9\)
Gompertz model 
: 
yt ~  N((j>i +  e^2^3, <p1)
where t =  1, . . . ,  n =  15 and set 9 =  (9i, . . . , 94) and <fi =  (4>i, • • •, 4>i)- 
and <f>i represent asymptotes in both models but the other parameters have 
different meanings under each model with #2 > 0, 0 <  93 < 1, <f>2 >  0 and
0 < fa <  1. Some of the regression parameters in 9 and 4> were transformed 
so that each of them varies over the real line. 02 was transformed to log92, 
93 to log [03/(  1 -  93)\, (j)2 to log $2, <j>3 to log [4>3/( 1 -  (j>3)}. Standard non- 
informative priors were then assumed preventing evaluation of the model 
likelihoods.
Gelfand, Dey and Chang (1992) generated a sample of size 2000 of the 
15 cross-validation predictive densities. They sampled from these densities 
using the weighted resampling technique. Rather than getting an initial sam­
ple from the posterior distribution, they approximated it by a t  distribution

252
Further topics in MCMC
and sampled from it. The parameters of the t distributions were set in ac­
cordance with a previous fit using the non-linear routines from the software 
SAS for each model. The individual values of the 7ji were also provided for 
each model (Gelfand, Dey and Chang, 1992, Table 2). The summarizing 
quantities G\, G 2 and G3 are reproduced here in Table 7.5. The pseudo 
Bayes factor was approximated by 1.6863, indicating preference for the lo­
gistic model. This preference was confirmed by G2 and G$ as shown in 
Table 7.5.
1 1
 1--------1--------1-------1--------1-------1-------1
2 
4 
6 
8 
10 
12 
14
time
Figure 7.2 Onion bulb data.
Criteria
Logistic
Gompertz
Gi
18.27
16.82
g 2
0.92
1.32
g 3
1.57
1.95
Table 7.5 Summarization of model evaluation.
Posterior predictive criterion
Gelfand and Ghosh (1998) introduced a posterior predictive criterion that, 
under squared error loss, favors the model M j which minimizes
D ?  =  P ?  +  G ?
(7.13)

where P G =  Y%= i v (fitly, M j), G f =  £ " = i[yt-£(yt|y,-M j)]2 and {yu ... , 
yn) are predictions/replicates of y. The first term, Pj, is a penalty term for 
model complexity with large values for too simple or too complex models. 
The second term, G j, is a sum of squared residuals and accounts for good­
ness of fit when yt is predicted by yt under model M j. Gelfand and Ghosh
(1998) also derived the criteria for more general error loss functions.
Expectations E(yt\y, Mj) and variances V(yt\y, Mj) are computed under 
posterior predictive densities, ie.
E[h{yt)\y,Mj] 
=  
J  j  Hyt)f(yt\y,&j, MJ)Tr(9j \Mj )d9j dyt
for h(yt) =  yt and h(yt) =  y2. Therefore, when 6^\ ... ,9jM^ is a sample 
from n(Bj\Mj) the above integral can be approximated via Monte Carlo 
(see Section 3.4) by
Model adequacy 
253
1
M
M
Y I 
K yt)f(yt\ y,6jt),M j)d yt .
i= 1
In general, when f ( y t\y,9j,M j) =  f (y t\9j,Mj), the above integral can be 
computed analytically, at least for h(yt) =  yt and h(yt) =  yf-
E xam ple 7.4 Considering again Example 2.11, the two components of the
r r  
=
p g 
-  
r 2 
~
w i a f j l  +  Ci ^ 
v i —2
ni)
7/1 — 2
i f  = n{e + 5'l)
criteria are (see Exercise 7.6a):
and
Gf
n
=  Yl^yi ~ 
-
and
ii
1
to
and
g 2
n
=  
“  7 i ) 2 •
i=1
7.2.3 Deviance information criterion
This section presents different ways to evaluate and use the predictive like­
lihood, obtained by averaging the likelihood with respect to the prior. It is 
also conceivable to use the likelihood function averaged with respect to the 
posterior. Inspired by Dempster’s (1997) suggestion to compute the poste­
rior distribution of the log-likelihood, Spiegelhalter et al. (2002) introduced 
the deviance information criterion (DIC)
D f =  P f +  G Sj 
(7.14)
where P f  =  E[D(9j)\y, Mj] -  D[E(9,\y, Mj)}, G f =  E[D(9j)\y, Mj], and 
D(9j) = —2logf(y\9j,Mj). The DIC is decomposed into two important

254
Further topics in MCMC
components: one responsible for goodness of fit ( G f  ) and one responsible for 
model complexity (P f), just like the previous criterion. P ?  is also currently 
referred to as the effective number of parameters of model Mj.
If 6 ^ \ . .., 
is a sample from n(0j\Mj), then the DIC can be approx­
imated via Monte Carlo by
M  
/  
M  
\
i=l 
\ 
i = 1 
/
DIC is applied for variable selection in van der Linde (2005). Celeux et 
al. (2005) compare different DIC constructions in mixture models, random 
effects models and several missing data models.
This criterion became very popular in the applied Bayesian community 
due to its computational simplicity and, consequently, its availability in 
WinBUGS. Further applications appear, 
amongst many others, in Lopes and
Salazar (2006a,b) (nonlinear time series models), Nobre, Schmidt and Lopes
(2005) and Zhu and Carlin (2000) (space-time hierarchical models) and 
Berg, Meyer and Yu (2002) (stochastic volatility models).
E xam ple 7.4 (continued) The deviance information criteria are (see Ex­
ercise 7.6b)
D f
n 
c, 
+
vial
-  
2 ( &
0
 
+  2nE (^ \ y , 
,
vi=l
^  
<=i 
+  2n £(log a2\y, M i) 
2
a
i=l
D% 
=
T) 1 + 2
Y i V i  - a ) 2 - n l o g
m i’ i
r j i - 2
+  2nE(\ogr2\y,M2) ,
1
D$ 
=  
n lo g ^  +  -r
2 ^ 1 + ^ ( y »  — 7i)2
s 
L 
»=i
with E(\oga2\y,Mi), E(fi/a2\y, M i), E (n 2/a2\y,Mi) and E(\ogT2\y,M2) 
numerically computed. The term n log(27r) was removed from the DIC since 
it appears in all 
, for j  =  1 , 2,3.
E xam ple 7.5 Table 7.6 contains n =  100 cycles-to-failure times for air­
plane yams. For each individual airplane, it has been suggested that an 
exponential model fits the data well (Leonard and Hsu, 1999; Quesenberry 
and Kent, 1982). The following Gamma, log-normal and Weibull models

are compared:
Model adequacy 
255
M\
M 2
M 3
yi~G(a,/3), 
a , 0 > O
yi ~  LN(fi,cr2), 
^ £  R ,a 2 >  0
yi ~  W eibull(7 ,5) 
7,(5 > 0 ,
for i =  1,... , n. Under model M 2, H and a2 are the mean and the variance 
of log yi, respectively. Under model M3, p(yi\l,5) =  7 y'J~1S~'Ye~^yi^s^ . 
Flat priors were considered for B\ =  (log a, log/3), 02 =  (/x,log<r2) and 63 =  
(log7 , logJ). It is also easy to see that, under model M\, E(y\0i) =  a/f3 
andV(y\d\) =a/f32. Similarly, under model M 2, E(y\92) =  exp{/^+0.5<j2} 
and V(y\02) — exp{2/x +  cr2}(e CT -  1), while under model M 3, E (y\63) — 
<5r ( l / 7 )/7  and V(y\9z) =  52 [2r ( 2/ 7 ) -  T ( l / 7 )2/ 7 ] /^. These results will 
be used to compute D °  below.
86
146
251
653
98
249
400
292
131
169
175 
176
76
264
15
364
195
262
88
264
157
220
42
321 
180
198
38
20
61
121
282
224
149
180
325
250
196 
90
229
166
38
337
65
151
341
40
40
135
597
246 
211
180
93
315
353
571
124
279
81
186
497
182
423 
185
229
400
338
290
398
71
246
185
188
568
55
55 
61
244
20
284
393
396
203
829
239
236
286
194
277 
143
198
264
105
203
124
137
135
350
193
188
Table 7.6 One hundred cycles-to-failure times for samples of yarn airplanes.
Weighted resampling schemes, with bivariate normal importance func­
tions, were used to sample from the posterior distributions. For i= 1,2,3, 
the proposals are qi(0i) =  fN(@i',Qi,Vi), where 0\ =  (0.15,0.2)', 02 = 
(5.16,-0.26)', 03 =  (0.47,5.51)', Vi =  diag (0.15,0.2), V2 =  diag (0.087,
0.085) and V3 =  diag (0.087,0.101). Posterior draws and contours of the 
posterior distributions appear in Figure 7.3. Under model M i, the posterior 
means, posterior standard deviations and 95% posterior credibility intervals 
fo r a  and /3 are 2.24,0.21, (1.84, 2.68) and 0.01,0.001, (0.008, 0.012), respec­
tively. Under model M 2, the posterior means, posterior standard deviations 
and 95% posterior credibility intervals for h and a2 are 5.16,0.06,(5.05, 
5.27) and 0.77,0.04,(0.69,0.86), respectively. Under model M 3, posterior 
means, posterior standard deviations and 95% posterior credibility intervals 
for') and 6 are 1.60,0.09, (1.42,1.79) and 248.71,13.88, (222.47, 276.62), re­
spectively.
Model comparison criteria appear in Table 7.7. The criteria indicate 
that both the Gamma and the Weibull models are relatively similar with 
the Weibull model performing slightly better. Similar conclusions appear in 
Figure 7.3(d), where the posterior predictive densities for both Gamma and 
Weibull models are almost identical.

256
Further topics in MCMC
5.0 
5.1
5 .3  
5 .4
■A
\
s-i
i---------i---------1---------r~
6 0 0  
8 0 0
Figure 7.3 Yarns failure data. Posterior draws and true contours: (a) n(a, f3\Mi),
(b) 
a2\M2) and (c) n(y, 5\Ms). Panel (d) exhibits the histogram of the data
along with approximations for the posterior predictive density p(yn+i\y) for the 
Gamma model (solid line), Lognormal model (dashed line) and Weibull (dotted 
line).
Model
A IC
B IC
g 4
D g
D s
M i
1254-489
1259.699
0.2257
1308.370
1253.445
m 2
1267.520
1272.731
0.2212
2268.808
1265.842
m 3
1254-398
1259.608
0.2232
1253.051
1253.051
Table 7.7 Model comparison for the Yarns failure data. For i =  1,2,3, Akaike’s 
information criterion (AIC) and Schwarz information criterion (BIC) are 
—21og/(y|0j) + 2di and —2logf(y\9i) + d,logn, respectively, where 9i is the 
maximum likelihood estimate of 9i. Da s were normalized so that D f =  D f. The 
actual value of D f is 4132798.

7.3 M o d e l choice: M C M C  over m od el and param eter spaces
Results from the previous section deal with evaluation of a given model. 
Comparisons of different models were also contemplated but always within 
the context where calculations were made separately for each model. In 
this section, a more formal treatment is given to the problem of choosing 
between models. This is done by indexing all models under consideration 
and treating this index as another parameter, to be treated jointly with all 
other model parameters.
Essentially, two alternative approaches are presented. The first was intro­
duced by Carlin and Chib (1995) and considers all models in a formation, 
called here a supermodel. The Markov chain simulation scheme for this su­
permodel is presented below. The second approach presents sophisticated 
simulation techniques using Markov chain with jumps between the different 
models (Green, 1995).
It will be assumed throughout this section that y is observed and it can 
be described according to a model M:l with parameter 9j of dimension dj 
taking values in a parameter space Qj C Rdj and j  taking values in a 
countable set J . When J  is finite, it can by identified as J  =
A superparameter 9 =  (Oj,j € J )  taking values in a parameter space
0  =  © j x ©2 • • • and a quantity M  assuming values in J  can be defined. 
M  serves the purpose of indicating a specific model. For instance, M  =  j  
indicates that model Mj is being considered. It is implicitly assumed above 
that different models do not share any component of their respective pa­
rameters. This restrictive assumption is satisfied by many practical appli­
cations. Common components can also be included in this formulation with 
repetition of the components in all 9j that contain them.
Assume for the moment that the posterior distribution n(9,j), the joint 
distribution of the superparameter and the model indicator, is to be ob­
tained. However, the main interest in inference is to obtain the posterior 
distributions of 6j\M =  j, j  £ J , and of M . These distributions respec­
tively provide the posterior inference within each of the models and the 
posterior probabilities of the models. The joint posterior is useful when 
dealing with a component of the parameter, say <fi, that is shared by all 
models. Inference about (f> is based on its marginal posterior distribution 
7t(4>). In any case, it can all be obtained from the joint density-probability 
function ir(9,j). The supermodel approach provides a sample from this 
more general, perhaps unnecessary posterior distribution whereas the ap­
proach with jumps only provides samples from 9j\M =  j, j  £ 3■ and from 
M . The presence of common parameters (for instance, the asymptotes in 
Example 7.3) does not pose any problem here. Samples from them are nat­
urally obtained from the corresponding samples for all models that include 
this shared component.
Model choice: MCMC over model and parameter spaces 
257

258
Further topics in MCMC
The joint probability model for the superparameter vector 9 and M  is 
defined over 0 x^7 , such that the joint distribution of all random quantities 
is given by
p ( v A j )  =  f{v\o,j)p(o\j)Pj
where j  is the value of M  and pj =  P r(M  — j). Godsill (2001) named this 
the composite model and Q x .J  the composite model space. Given that M  =  
j, the distribution of y depends on 9 only through 03, or mathematically,
f{y\o,j) = f(y\0j,j) •
Assume also that the 9j are conditionally independent given the value of 
M . Hence,
P(#\j) =  Y[p(8i\J) ■ 
(7-15)
i e j
Note that the prior distribution p(9i\j), for i /=■ j, does not make much 
sense. It specifies the distribution of the parameters of model i conditioned 
on the fact that this is not the true model. Carlin and Chib (1995) referred 
to these as pseudo prior or linking distributions. Due to the conditional 
independence (7.15), these priors do not interfere in the expressions of the 
marginal predictive densities for each model. Nevertheless, they are relevant 
for the construction of the chain and must be specified.
The full posterior of the supermodel is, therefore,
_ /n 
f ( y \ ° j J ) U i e j P ( 0i\j)Pj 
n , J )  ~  
f ( y )  
'
Assume that (9<'l\ j^ 1')),...,(9 ^ n\j^n')) is a MCMC sample from the su­
permodel 7r(9,j) obtained by one of the algorithms about to be described. 
Comparison between models is based on the marginal posterior distribution 
of M,  7r(j), j  G J . These probabilities are estimated by the proportion of 
values of M  equal to j  in the sample of size n, i.e.,
*U)  =  ~ i t ,  70'(i) =  j)- 
(7-16)
n  (=1
Inference within each model is based on the conditional posterior distri­
bution ir(9j\j), j  € J . Note that the sample available for 9j is a sample 
from the marginal posterior 7i(9j). A sample from the conditional posterior 
is obtained by retaining the 9^l\ I =  1 , . . .  ,n, of the posterior sample for
which the sampled value for M  was j  and discarding all other values of 9 ^ 
for which the sampled value for M  was not j. For instance, E[g(9j)\ can
7.3.1 Markov chain for supermodels

be approximated by
9i =  \ Y , 9 ( 0 f ) I { j U = j ) ,  
l=i
where the expectation is taken with respect to ir(9j\j).
Carlin and Chib sampler
Carlin and Chib (1995) explored the natural 
blocking formed by grouping
each model parameters 
and M  when J" is a finite set of models. The full
conditional distributions for 9\, . . . ,9j and M  are obtained as follows:
• For block 9j, j  =  1, . . . ,  J,
f ( y \ e3 ’ 3 )p(0i\j)  
’  
f o r  M  =  3
K 
| p ^ i )  ) f o r M  =  
i _ ^  
•
• For block M
j
tTM{j) =  k~lf{y\0j, j) Y [p (0 i\ j)p j, j  =
i=1
that is a discrete distribution with proportionality constant
j  
j
( = 1 
i= 1
M  can always be sampled directly because it has a discrete distribution. 
Direct sampling from blocks Oj will depend on the conjugacy structure 
for model M  =  j  and the form of the pseudo prior distributions. When 
direct sampling for some of the OjS is not possible, Metropolis-Hastings 
steps described in Chapter 6 may be used.
The above scheme satisfies the conditions of a conventional Markov chain 
and therefore converges to the target distribution given by the posterior 
7r(0, j). Random draws from this distribution may be generated by iterative 
sampling from the full conditional distributions described above. The end 
result of this process is a sample of size n, say 9[l\ 
, 9 j \ j (-l\ I =  1,... ,n.
The pseudo prior distributions must be carefully chosen as they affect the 
rate of convergence of the chain. Note that as a modelling device they are 
meaningless. Therefore, Carlin and Chib (1995) supported the view that 
total freedom may be given to the specification of these prior distributions. 
They may even include specifications using the data. After experimenting
with 
a few choices, they recommend setting the pseudo prior distributions
p(9j\i), for * /  j  as close as possible to n(8j\j). They suggested the use 
of simple standard approximations based on univariate estimates obtained 
from pilot (model-specific) chains.
Model choice: MCMC over model and parameter spaces 
259

260
Further topics in MCMC
Another difficulty encountered by Carlin and Chib (1995) is connected 
to the prior model probabilities pj. They observed that for some prior 
specifications, the chain does not seem to move between models and, as a 
result, large posterior probabilities are obtained for some of the models. To 
correct that, they set the prior probabilities in their examples to values that 
allow movement between the models. Although this may work in practice, 
it can force practitioners to specify probabilities they may not believe. 
Their example contained fairly vague prior distributions for models with 
different dimensions and Bayes factors are known to be very sensitive in 
these situations. So, this prior setting may need further justification to 
satisfy potential users.
Metropolised Carlin and Chib sampler
The above algorithm is not applicable in the above form when a large or 
countable number of models is considered. Dellaportas, Forster and Nt- 
zoufras (2002) and Godsill (2001) noticed that its major drawback comes 
from the calculation of f(y\0i,l)Yliejp(9i\l)pi for all / e J . They suggested 
replacing the Gibbs sampling step for the model by a Metropolis-type step. 
Therefore, they metropolised the Carlin and Chib algorithm. This idea was 
previously illustrated in the fixed-dimensional case presented in Example
6.5 
where it was shown that replacing a Gibbs step by a Metropolis step 
proved computationally advantageous.
More precisely, if the chain is currently at (9, j), a move to a new pair 
{<j>, k) is made as follows:
• A new model k is sampled from proposal q(j, k);
• ()>k is sampled from the pseudo prior p(9k\j). Thus, only the component 
k of 9 is proposed to move, i.e., <j> =  (&i, . . . ,  9k-i,4>k, 9k+i, ■..);
• Accept the move to (<j>, k) with probability a where
Q =  m i n f 1 f(y\4>k,k)Y^=1p(4>i\k)pk ,, q(k,j)p((f>i\k)}
I ’ f(y\Bj,j)YlLiP(0i\j)Pj 
QU,k)p(6k\j) j
which simplifies to
a- — min 1 1 n^
P
k ~ 
( 7 m
I ’ n(Qj\j)Pj 
q(J,k)p(0k\j) J
since all other pseudo-prior densities cancel out.
E xam ple 7.2 (continued) Lopes and West (2004) proposed a sampler for 
traditional factor analysis that relies upon MCMC-based approximations to 
the posterior distributions of [Jij, Hj) under the j-factor model. In their 
case 9j — (j33 , a j ) where now 
is the vector obtained by stacking the non­
zero elements of the columns of the factor loadings matrix (3j and a 2 is the

Model choice: MCMC over model and parameter spaces
261
vector comprising the diagonal ofT,j. Therefore, (3j is a [9j  — j ( j  — l)/2 ]- 
dimensional vector and a? is a 9-dimensional vector.
Their proposal density is q((9j, j),{4>k,k)) =  q(j, k)qk(cj)k), where q(l, 2) =  
g(5,4) =  1, q(l,l +  1) =  q(l,l -  1) =  0.5, for I =  2,3,4, and qk(4>k) =  
fN0k',bk, PVk)X\kl=1 fiG{Vki’ a ’ av,ki)- The quantities bk and Vk are esti­
mates of the posterior mean and variance of (3k, while 
is an estimate of 
the posterior mean of crh, all under the k-factor model. The tuning parame­
ters a and (3 are problem specific and were set at a =  18 and j3 =  2 in their 
simulated study. The sampler was based on 1000 iterations, after a burn- 
in of 10000 draws. 1000 replications of the 3-factor model were simulated. 
The MCMC algorithms were run and used to identify the model with high­
est posterior probability for each replication. In 99.3% of the replications, 
it chose the correct model (k =  3), while (k =  2) was chosen only in 0.7% 
of the replications. Results based on other model criteria appeared in Ta­
ble 1.3. Further discussion and additional simulated and real data analysis 
appear in Lopes and West (2004).
In the Metropolised Carlin and Chib algorithm, a new model k is pro­
posed according to the transition kernel q(j, k) and then the parameters of 
model k are proposed from the pseudo prior p(9k\j). Natural extensions for 
the above algorithm can be considered. A new model k can be proposed 
according to a transition depending also on the parameter 6j and the pa­
rameter 9k can be proposed according to a general transition kernel that 
depends on j  and 9j.
7.3.2 Markov chain with jumps
An alternative route for general construction of Markovian processes with 
a given stationary distribution is based on the specification of a transition 
kernel satisfying the detailed balance equation
^(0,j)p((0,j),{(p,k)) =  n(<l>,k)p((<l>,k),(9,j))
valid for all points where the move is allowed. This equation ensures re­
versibility of the chain. This is a sufficient condition and hence imposes 
more restrictions than necessary for convergence. Nevertheless, it provides 
a useful basis for specification of appropriate transition kernels. This route 
was used by Hastings (1970) and described in Chapter 6. It led to many 
possibilities for the kernel and is again used here in the more general context 
of a collection of models.
Once again, the transition will be constructed in two stages: a proposal 
transition and an acceptance probability, correcting the proposal to ensure 
balance. The main difference here is that many models are simultaneously 
being considered and therefore many qualitatively different moves are en­
tertained. Green (1995) explored this idea by imposing detailed balance

262
Further topics in MCMC
for all possible moves between models. Detailed balance would then be 
attained globally and convergence to 7r(9,j) would result.
Consider for each possible jump move m between models, an arbitrary 
transition kernel qm({0, j), (<f>, k)) and a yet to be specified acceptance prob­
ability 
(</>, k)). In fact, many different jump moves can be made
from Mj to 
but for simplicity it will be assumed here that each move
to uniquely defines a model Mfc. Many of these moves can be specified and 
for each of them
can be defined. B jm gives the probability that the proposed move makes 
the chain jump from current model Mj to model M k and may depend on
6. In practice, this is typically not the case and dependence on 0 is dropped 
for notational simplicity. If only one type of move is allowed from j  to k 
then one can identify B,jrn — q(j, k) of the previous section. Note now that 
qm{(0,j), (•, k))/Bjm defines a proposal transition density for 6k associated 
with jump move m.
Each proposed move takes the chain from model j  and a corresponding 
value of 0j to model k and a corresponding value of (j>k- Only values of the 
parameters associated with the models involved in the jump are concerned. 
If the move is accepted, only the kth component of 9 is altered and ^_fc =  
6Lfc. Note also that k may be equal to j  and, in this case, the move does 
not involve a jump between models.
The extension considered by Green (1995) also admits that the chain may 
not move at every iteration so that ')2rn B]m =  Bj < 1. Jump moves m 
are proposed according to probabilities B jm and there is also a probability
1 — Bj that the transition does not propose any moves. Naturally, it is 
possible to have Bj =  1 and in this case a move will always be proposed.
Following (6.4), the transition kernel of the chain is given by
+  
I ( ( 0 , j ) e A ) s ( 0 , j )  
where A C  0  x J  and
(7.18)
s(0,j) 
=  
1 -  V
 [  q m ( ( 8 , j ) , { 4 > , k ) ) a m ( (0, j) ,( ( j) ,k )) d( p
™ J®
=  
Y I  [  9m((0,j), (<£,&))[! ~  OCm{{0,j),{4>,k))]d4>
+  
1 — Bj .
(7.19)
As for the Metropolis-Hastings algorithm presented in Chapter 6, the tran­
sition kernel (7.18) defines a mixed distribution for the next state of the 
chain. For points (<fi, k) ^  (0, j), the distribution admits a combination of a

Model choice: MCM C over model and parameter spaces
263
density for <f>k and a probability function for k given by qm((P,j),-)(Xm(P,j),') 
where m is the jump move taking the chain from model Mj to model Mk- 
More specifically, there is a probability Bjm that the proposed move takes 
the chain to model Mk- Given that the proposed move took the chain to 
model Mk, there is a density g describing the chances of moves to 4>k- 
Associated with the jump m to model Mk, the superparameter can be 
partitioned as 9 =  (9 k, 0-fc). The density g is given by
g{4>k) =  qm{{Qk,6-k,j),{<t>k,9-k,k))/Bjm. 
( 7 .2 0 )
This density governs the proposed value for the parameter of model k. This 
value will then be accepted with probability a m((dk,0-k,j),{<f>k,Q-k,k)). 
Note that both the proposal qm and the acceptance probability a m depend 
on 9 only through 9 k', the remaining components are irrelevant once the 
jump defines model Mk for the next state of the chain.
For (4>,k) =  (9,j), this distribution provides a positive probability of no 
moves given by s(6,j). Equation (7.19) informs that this probability is due 
to two distinct events: either the move took the chain to a state that was 
not accepted or no move was proposed.
The imposition of general reversibility of the process leads to
In practical terms, the simulation of a sample from 7r using the Markov 
chain defined by transition (7.18) may be summarized as:
1. Initialize the iterations counter I =  1 and set as arbitrary an initial value
(9(°\jW).
2. Choose a jump move m accordingly with probabilities Bjm, thus defin­
ing the model Mk considered for move proposals. Hence, the proposed 
value for jW  is k. If the move chosen was not to move, set (9^l\ j ^ )  =  
(9<'l~l\ j^ -1 )) and go to step 5. This happens with probability 1 — Bj.
3. Draw (pk from the conditional density (7.20). Hence, the proposed new 
state for 9 changes its fcth component to (f>k while keeping 4>-k =  9^k 
Hence, all other components of 9 are unchanged.
4. Calculate the acceptance probability a m((9(l~ l\ 
(<f>,k)) of the
move given by (7.21). If the move is accepted, (9 ^ \ j^ ) =  (<f>,k). If the 
move is not accepted, 
=  (9('l~1\ j^ -1 -*) and the chain does not
move.
5. Change counter from I to I +  1 and return to step 2 until convergence is 
reached.
Steps 2 and 4 are operated after generation of two independent uniform 
random quantities u\ and u2- The first one is used to determine the model 
to which the chain will propose a move. This choice is made according to the

264
Further topics in MCMC
discrete distribution with probabilities B jm. The second random quantity 
determines the acceptance probability of the proposed move as before: if 
u2 <  ocm, the move is accepted and if u2 > a m the move is not allowed. The 
above algorithm is generically known by the acronym RJMCMC, standing 
for reversible jump MCMC.
This process in fact generates a stream of values 0^t),j^l\ I =  1,2,-----
At each iteration once the value of the model is drawn, only the parameter 
associated with this model is generated. Therefore, samples from 6j\j are 
automatically provided by restricting attention to the chain values associ­
ated with value j  for the model. Likewise, samples from M  are provided by 
the marginal samples over all iterations. At each iteration, the chain can 
be complemented with the current values of the parameters for the other 
models. This forms a larger stream of values 8^l\j^l\ I =  1, 2, —
, as in the 
previous section. There is no harm, of course, in doing it even though this 
is in general irrelevant as most questions of interest are already answered 
by the smaller and variable dimension sequence.
In the case of J models, J +  1 moves can be specified: J — I jumps to 
the other models, one move within a model and one absence of move. As 
J may be large, this may imply too many alternative moves. In general, 
few moves are necessary. The important point is to ensure irreducibility of 
the chain and freedom of movement throughout the parameter space. Typ­
ically, this is achieved by only allowing moves between neighboring models. 
In the case of a special meaning associated with the ordering 1, 2, . . . ,  J 
of the models, Phillips and Smith (1996) and Green (1995) suggested con­
sidering only jumps to models j  — 1 and j  +  1 from model j. Markov 
chains of this kind were studied in Chapter 4 where they were referred to 
as birth and death processes. Note also that in the presence of a single 
model with moves always being proposed, the above structure reduces to 
the Metropolis-Hastings algorithm described in the previous chapter.
The general algorithm introduced by Green (1995) also takes advantage 
of possible similarities amongst competing models when proposing model 
and parameter moves. The supermodel samplers could be thought of as 
globally proposing new models and model parameters. On the other hand, 
Green’s algorithm focuses on local aspects (associated with a single model), 
which in turn introduces more flexibility into the search of the model space.
E xam ple 7.6 Choosing between two competing models
Consider models M i and M 2 with parameters 61 and 02 of dimensions di 
and d2 satisfying d\ + n i =  d2 where n\ > 0. So, model M 2 has a parameter 
space n\ dimensions smaller than model M\. The reversibility condition re­
quires an additional random quantity u of dimension n\ in order to define a 
bijection d2 =  g(9\,u), thus allowing moves in both directions with the same 
probability. There are many other ways to ensure reversibility discussed by 
Green (1995) but this is the simplest one. The transition consists of incor­

Model choice: MCMC over model and parameter spaces
265
porating into the ratio test the information about this bijection and about
the distribution f  of u that may depend on 8\. The acceptance probability 
of the move from model 1 to model 2 is given by
where Bij is the probability that a move from model i to model j  is proposed, 
q(a\8\) is the conditional proposal density used to generate the additional
Details of this derivation are given by Green (1995).
The extension to more than two models follows the same ideas with spec­
ifications of quantities Uij of appropriate dimensions to ensure bijections 
between the parameters of models M* and Mj. So, if 6i (8j) is the param­
eter of model Mi (Mj) with dimension dt (dj) and dj — dt =  ntj > 0, a
-dimensional random quantity 
is defined. Then, a deterministic bi­
jection is created between (8i: 
) and Oj. This simple device ensures that
the moves between any two parameter spaces can be reversed. There is no 
theoretical restriction on the form of the proposal densities used to draw 
the 
and of the bijection. Once again, they should lead to moves that 
are not too small (to render slow convergence) nor too large (to render 
low acceptance probability). Green (1995) applied this idea to models with 
multiple change points, image segmentation and partition models. He also 
discussed some useful forms of bijections and proposals in the context of 
his applications, as shown in Example 7.7 below.
E xam ple 7.7 Non-parametric intensity rate estimation
Consider the observation of a Poisson process over [0,T] with unknown 
intensity rate A(t), 0 <  t < T. Each of countably many possible mod­
els Mk, k =  0, 1 , 2,..., is defined as having a piecewise constant inten­
sity rate taking values A j at intervals Ij =  [tj,tj+1), j  =  0, l , . . . , k ,  with 
to =  0 and tk+i =  T. The parameter of Mk is 8k =  (A(k),t(k))' where 
A (k) =  (Ao, Ai, . . . ,  Afc) and t(k) =  (ti,. ..,tk). The hierarchical prior used 
by Green (1995) assumed that, conditional on k, t(k) consists of the even- 
numbered order statistics of a sample of size 2k +  1 from the U[0,T] dis­
tribution and A(k) is a sample from a G(a,/3) distribution. The prior is 
completed with a second stage k ~  Poi(j) with probability function fp . Ar- 
jas and Heikkinen (1997) replaced the conditional independence of the A(k) 
by a pairwise difference prior accounting for their spatial interaction as in 
Section 2.6.
min < 1{
7t(6>2, 2 )g 2i 
dg(8i,u) 
n(8i , l) Bi 2q(u\6i) 
d(8x,u)
(7.22)
quantity u and 
^  is the matrix of derivatives of the bijection g(8i,u). 
Likewise, the acceptance probability of the reverse move from model 2 to 
model 1 is given by
min < 1 ,

266
Further topics in MCMC
Birth and death processes allow for four different types of moves in these
circumstances:
1. The birth of a new step, thus creating a new interval and moving the 
chain from model Mk to model Mk+i- This move has probability pk■
2. The death of an existing step, thus deleting an existing interval and mov­
ing the chain from model Mk to model M k -1 - This move has probability 
qk■
3. The change of the intensity rate of a given interval. This move has prob­
ability r\k-
4. The change of the length of a given interval. This move has probability
Note that Pk +  qk +  r\k +  r2k =  1 (qo =  0) and that moves of type 3 and 
4 retain model Mk- For these moves, assuming randomly chosen intervals 
for the change, evaluation of acceptance probabilities follows from standard 
theory and is left as an exercise.
A birth move changes model Mk with parameter 9k to model Mk+i with 
parameter 8k+i- Green (1995) suggested starting the move by choosing a 
new endpoint t* uniformly over [0, T] (and this point will lie in the inter­
val [tj,tj+1), say) and choosing the corresponding new rates Xj and A' + 1 
according to A'/A' + 1 =  u/(\ — u) where u ~  t/[0 ,1] but preserving the geo­
metric average so that (tj+ 1 —t*)logX'j+1 +  (t* —tj)logXj =  (tj+ 1 —tj)logXj. 
Note that only two new variables were needed to create the two new pa­
rameters. Reversibility automatically defines the new intensity rate of the 
interval formed by the merging of two adjacent intervals in the death move. 
The endpoint to be deleted is chosen uniformly at random and the merged 
intensity rate should also satisfy the same geometric averaging.
Following (7.22), the acceptance probability of a birth move is
Derivation of the expressions of each of the terms above and of the accep­
tance probability of a death move are left as exercises.
Clyde (1999) showed that pre-existing algorithms, such as the MCMC 
model composition (Raftery, Madigan and Hoeting, 1997) and stochastic 
search variable selection (George and McCulloch, 1992), can be seen as par­
ticular cases of RJMCMC. Other earlier approaches to deal with variable 
dimension appear in Geyer and Moller (1994) and Ripley (1977) for spatial 
processes.
In many situations, there is a natural ordering of the component models, 
usually through the dimensions of their parameter spaces. So, simple chains 
based on irreducible birth and death processes can be used. The approach 
has proved to be useful when treating problems with many possible models
r2k-

Model choice: MCMC over model and parameter spaces
267
such as mixtures with an unknown number of components (Richardson and 
Green, 1997; Lopes, Muller and Rosner, 2003), non-parametric intensity 
rate estimation (Green, 1995; Arjas and Heikkinen, 1997), non-parametric 
regression (Dias and Gamerman, 2002), nonlinear classification and regres­
sion (Dennison et al., 2002), autoregressive time series models (Huerta and 
West, 1999a,b; and Huerta and Lopes, 2000), graphical models (Dellapor- 
tas and Forster, 1999) and genetic mapping (Waagepetersen and Sorensen,
2001).
Proposing from full posterior conditionals
When TT(4>k\k) is available in closed form and it is easy to sample from, 
the proposal transition can be written as q((0j,j), (4>k,k)) =  ir((/)k\k)q(j, k) 
and the acceptance probability (7.21) can be rewritten as
In other words, when posterior inference within model is obtained in 
closed form, the above MCMC scheme searches the space of models with 
moves essentially driven by posterior odds ratios. So, models with higher 
posterior model probability will be visited more often with model M j , for 
instance, visited 1007r(j)% of the time.
Inference about &k is made conditional on k using standard within model 
MCMC algorithms. This result appears, for instance, in MCMC model 
combination (MC3) for graphical models (Madigan and York, 1995). Even 
though 7r(0fc|k) is only rarely known and/or easy to sample from, the above 
results suggest that good proposal densities q should mimic the full poste­
rior conditional densities. This recommendation is in line with those pre­
viously made in Chapter 6.
Partial analytic structure
Even though direct evaluation and/or sampling from n(0j\j) may be unre­
alistic in most situations, there are several instances where ir(0j2\0ji,j) is 
available in closed form, for some partition (0ji,0j2) of 8j under model Mj. 
Suppose that, in addition, there exists a similar partition of <f>k, {<j>ki,<t>k2), 
such that the dimensions of 0j\ and <f>ki are identical. When current and 
proposed models share common parameters, Godsill (2001) suggests an 
MCMC algorithm that proposes a move from (0,j) to (4>,k) by setting 
<pki =  0ji, sampling (f>k2 from its full conditional 7r(0fc2 |0fci, k) and accept­
ing the move with probability
(7.23)

268
Further topics in MCMC
where ni(l\0i2) — J n(l,0n\0i2)d9n, for I — j,k . After that, the common 
component 9ji say, is sampled from its full conditional 7r(0ji \9j2,j).
E xam ple 7.3 (continued) 6\ =  (pi represents the asymptote in both models 
but £L 1 and (p~\ have different meanings under each model. Assume now 
that proper prior distributions are considered. Additionally, assume that the 
prior distribution of 9i and (pi are both N(m o, Co) and independent of the 
other parameters. It is easy to verify that the full conditional distributions 
of 91 and (pi are normal. Therefore, the implementation of the above partial 
analytic structure is straightforward. Suppose that the current model is the 
logistic model, for instance, with parameter value 9 and the proposed model 
is the Gompertz model, then {<p2,<p3,<p4) is sampled from its full conditional 
distribution. The move is accepted with probability
where f{y\92,93,94) =  
xmo, 6»f/n +  xx'C 0) and f(y\<j>2, fo, <M =  
f N( y ;l nm0 +  z,(pj +  l nl'nC0), for x  =  ( x i , . . . , x n)', z =  ( z i , . . . , z n)', 
x t =  (1 +  #2#3)_1 and zt =  exp{(p2cp\}, for t =  1, . . .  ,n. If the move is 
(not) accepted then (pi (81) is sampled from a standard MCMC algorithm 
under the Gompertz (logistic) model.
E xam ple 7.8 Lopes and Salazar (2006a,b) revisited the Canadian lynx 
data (Figure 7-4). They designed RJMCMC algorithms to account for model 
uncertainty in logistic smooth transition autoregressive models of order j
where x t — (y t-i, ■ ■ ■ ,V t-j)' and j3 j i  and (3 j2 are j-dimensional vectors 
of regression coefficients, so that Oj =  (9j i , 8j2), 9ji =  (y j,cj,crj) and 
9j2 =  (P ji,P j2)- The function gj =  g(^ j,C j,st) =  (1 +  e_7^St_c^ ) _1 plays 
the role of a smooth transition continuous function bounded between 0 and
1. When gj =  0; the above model reduces to a simple linear autoregressive 
model. St is called the transition variable, with St — yt-d a common choice. 
One of the prior specification they used is
(Mj)
yt\dj ~  N{x't(3ji + g(yj,Cj, st)x't(3j 2,(T2j)
P ( 9 j )  oc fN(Pj2\ 0 , <r2e7 / j + i ) ( l  +  72) 1aj 2
fo rc j € [F_1(0.15), F _1(0.85)] and F  the empirical cumulative distribution 
function of the data. They showed that the model acceptance probability 
becomes
where
i±i2

Model choice: MCMC over model and parameter spaces 
269
and
mi 
= 
CiaJ~2^ 2 ztyt , 
Q  =  (cr;- 2  ^
 ztz't +  E  
x)
^  
=  
(x't ,gix't ) , 
I T 1 =  d ia g ( 0 ,a l2e~l l li+i),
for I =  j,k . The MCMC algorithm is completed by sampling 9k2 condi­
tional on 9k 1, k and y. Table 7.8 presents the posterior model probabilities 
obtained by their MCMC algorithm. The modal model is an A R (ll) model 
with P r{M n \y) =  0.56. Nonetheless, AR(S), AR(4), AR(12) and AR(13) 
models also exhibit non-negligible posterior model probabilities.
uocsi
I------------- 1--------------1------------- 1------------- 1------------- 1--------------1 
I 
I 
I 
I
1829 
1839 
1849 
1859 
1869 
1879 
1089 
1899 
1909 
1919 
1929
y e a r s
Figure 7.4 Logarithmic transformation of the number of Canadian lynx trapped 
in the Mackenzie River district of Northwest Canada over the period from 1821 
to 1934. The vertical bars represent the posterior mean of g ( j, c , y t-d)-
k
Pr{k\y)
1
0.000
2
0.001
3
0.066
4
0.073
5
0.013
6
0.001
7
0.002
k
Pr(k\y)
8
0.000
9
0.001
10
0.000
11
0.559
12
0.162
13
0.121
Table 7.8 Canadian Lynx: posterior model probabilities.

270 
Further topics in MCMC
1.3.3 Further issues related to RJMCMC algorithms 
Convergence diagnostics
Despite the broad use of reversible jump MCMC algorithms since Green’s 
(1995) seminal paper, literature on assessing convergence is still scarce. 
Brooks and Giudici (1999, 2000) generalized the convergence diagnostics 
of Gelman and Rubin (1992) and Brooks and Gelman (1998) by splitting 
the total variation of parallel chains in two major components: (i) varia­
tion between chains and (ii) variation between models. In order to adapt 
Gelman and Rubin’s convergence diagnostic, a subset of parameters, say 
9o, must keep the same interpretation across models. In such case, which 
appears, for instance, when models are nested, they introduced a two-way 
ANOVA decomposition of the variance of 80. See also Castelloe and Zim­
merman (2002) who proposed unbalanced two-way ANOVA and multivari­
ate ANOVA versions of the diagnostics of Brooks and Giudici (1999,2000).
Another idea, suggested by Brooks, Giudici and Philippe (2003), is to 
monitor the model indicator M k through straightforward implementation of 
nonparametric hypothesis tests. They applied their convergence diagnostic 
to mixture and graphical Gaussian models. In principle two drawbacks are 
apparent. First, care must be taken when assessing convergence based on 
marginal convergence diagnostics, as opposed to assessing for (9, j) jointly. 
Second, it may be restrictive to use univariate test statistics to assess con­
vergence of complex chains over highly dimensional parameter and model 
spaces.
Regardless of the scarcity of research in this area and the limitations of 
the proposed diagnostics, RJMCMC algorithms are being routinely used 
and they heavily depend on reliable convergence assessment. This area will 
benefit from further research.
Choice of the proposal
Green (2003) introduced a transdimensional version of the random walk 
Metropolis algorithm. He suggested running pilot chains within each model 
separately and use the draws to estimate the posterior moments required 
for his extension. An independence Metropolis version of this algorithm was 
presented by Lopes (2000) and Lopes and West (2004) (see Example 7.2 for 
details). This line of approach is practically limited to small or moderately 
small model sets J . Further ideas are presented and discussed in Brooks, 
Giudici and Roberts (2003). See also Ehlers and Brooks (2002) for efficient 
construction of RJMCMC proposal densities for autoregressive models.
RJMCMC and other transdimensional algorithms
A similar treatment to the problem of inference about parameters in dif­
ferent models and model choice was given by Phillips and Smith (1996)

Convergence acceleration
271
based on jump diffusion processes (Grenander and Miller, 1994). In these 
processes, the chain moves within a model following a diffusion process, 
namely a Markovian process at continuous time. Sampling from this pro­
cess requires a discretization of time and consequent approximation of the 
stochastic differential equation governing the process by a difference equa­
tion similar to the one used to define a system equation in dynamic models. 
A jump process is superimposed onto this diffusion to allow for moves be­
tween models. The jumps occur according to the marginal jump intensity. 
This intensity depends on the state (0 ,j) of the chain and is obtained 
by the integration of the jump intensity q{(0,j),(<p,k)) over all possible 
jump points (4>,k). As before, this intensity q is constructed so as to en­
sure reversibility. Many possibilities for q based on Gibbs samplers and 
Metropolis-Hastings algorithms are presented by Phillips and Smith (1996) 
and illustrated in the context of identification of mixture components, ob­
ject recognition, variable selection and identification of change points.
Stephens (2000) proposed and applied a discrete birth and death pro­
cess for model search in mixture models. Later, Cappe, Robert and Ryden 
(2003) showed that Stephen’s processes and Green’s RJMCMC are quite 
similar when continuous-time processes are considered.
Sisson (2005) presented a comprehensive review of RJMCMC and lists 
up to date references and URL for several freely available software im­
plementing RJMCMC and other transdiinensional algorithms. See also 
Waagepetersen and Sorensen (2001) and Green (2003) for further review 
on RJMCMC.
7.4 Convergence acceleration
Previous sections dealing with techniques for improving the convergence of 
the chain basically considered them in the context of blocking parameters 
and reparametrization. There are many other suggestions for improving 
the convergence of the chain to the equilibrium distribution. Some of these 
techniques are described in this section. For presentation purposes they are 
divided in two large groups: alterations in the chain and alterations in the 
equilibrium distribution.
The chain may be altered by specification of alternative transition kernels 
with improved convergence properties or by manipulation of the draws 
generated from the chain. In both cases, the goal is the same: to make the 
chain get to the equilibrium faster.
7-4-1 Alterations to the chain
One of the greatest problems for the convergence of the chain is the fact 
that it moves the components along the directions determined by the com­
ponents of 0. One alteration of this scheme is reparametrization, which

272
Further topics in MCMC
essentially promotes a transformation of the parameter space and can be 
seen as a redefinition of the sampling sixes.
Rather than seeking useful reparametrization that will then define more 
appropriate directions for sampling, new directions can be suggested di­
rectly. Schmeiser and Chen (1991) proposed a chain where the direction e 
of the moves is chosen at random from the unit vectors in R d where d is 
the dimension of 8. Once a direction is chosen, the next state of the chain 
is chosen in that direction according to the probabilities given by w. The 
complete scheme is:
1. Initialize the iteration counter of the chain j  =  1 and set an initial value
and set 8 ^  =  8^ ^ 
.
4. Change counter from j  to j  +  1 and return to step 2 until convergence 
is reached.
The generation of e can be made univariate by drawing r-j from some 
distribution symmetric around 0 and taking et — ri/ J2j rj> ® =  1 
- ,d.
Once the direction e is chosen, the amount c of movement along e is chosen 
according to the posterior distribution. No matter how complicated is the 
form of the posterior, the generation of c is univariate.
E xam ple 7.9 Assume that the posterior distribution of interest is a bi­
variate mixture of normals w N (n i, E) +  (1 — w )N (fi2, E) where 0 <  w <  1 
and
At iteration j  and given direction 
=  (e\,e2)', the generating density
where r =  
E _1e ^ , Sj =  
E -1 ^ -?-1 ) — fii) and ti — (8^~^ —
/ij)/E_1(0^_1* — Hi), i =  1,2. It is clear that g is the density of a mixture 
of normal distributions with means Si/r, i =  1,2, weights w\ and 1 — W\
where w\ =  we^s^ 2r^ (tl/ 2)/[we(si /2r) (*i/2) _|_ ^  _  w )e (s2/2r) (*2/ 2)j am1
0<°>.
2. Choose a direction 
in R d uniformly on { ( x i ,... ,Xd) ■ YliLi X1 — !}■
3. Choose a scalar 
generated from the density g(c) oc 7r(0^~ ^ +  ceS^))
for 
is
oc w exp
common variance r 1. So, draws of c are easily obtained.

Convergence acceleration
273
A generalization of this algorithm was proposed by Phillips and Smith
(1993). Each iteration in their sampling scheme comprises a set of d direc­
tions e\,
,ed- When the e, form the canonical basis of Rd indicating the 
d axes, they reproduce the standard algorithms presented so far. When the 
ei form another basis of Rd, the algorithm corresponds to sampling through 
components after a linear reparametrization. Other choices of direction are 
available. In particular, random choices can be made. Completely random 
choices correspond to the algorithm of Schmeiser and Chen (1991) observed 
at every d iterations. Phillips and Smith (1993) suggested only choosing e\ 
at random and then choosing e2, ■ ■ •, e,/ to be mutually orthogonal and or­
thogonal to e\. The directions can also be tuned to improve sampling in 
every specific setting. This will generally destroy the convergence proper­
ties of the chain so this tuning can only be operated at a transient phase 
of the chain.
Phillips and Smith (1993) also suggested a generalization in the choice 
of c. When sampling c from a general density g
[
c
e*^), the proposed 
value of the chain 9 ^  =  9^~1'1 +  
is only accepted with probability
7r ( ^ ( i ~ 1 ) +  c ( j ) e U ) ) g ( — c U )  | 0 ( j _ 1 ) - ) - c 0 ' ) e 0 ' ) , e 0 ' ) )
The Schmeiser and Chen (1991) algorithm corresponds to g(c\9^~l\ e ^ )  
oc 7t(9^~^ +  c e ^ )  and accepting the proposed value with probability 1.
These algorithms work well for the possibility of large moves when ap­
propriate directions are chosen. This is generally not possible when moving 
along the direction of the components. The performance of these algo­
rithms in highly dimensional models with correlated parameters or with 
concentrated modes may be very poor. The empirical evidence obtained 
by Phillips and Smith (1993) suggested that the orthogonalization of ran­
dom directions improves convergence over sampling from entirely random 
directions which also improves convergence over Gibbs sampling. The im­
provement increases substantially when directions are chosen according to 
a principal component analysis based on (an approximation to) the poste­
rior variance matrix. The comparisons mentioned above were based on the 
convergence prescription of Raftery and Lewis (1992) and on the integrated 
autocorrelation time (Green and Han, 1992).
Another approach to the appropriate selection of the sampling directions 
is given in Gilks, Roberts and George (1994). They proposed adaptive meth­
ods for choosing the direction of sampling. The method is based on a current 
sample 9
^
9
^
 at each iteration of the chain and allows the choice of 
the next direction e^+1^ to be based on the current sample. Dependence on 
a succession of values is not a problem for convergence as the Markov chain 
may be enlarged to contemplate n of the original steps. At each iteration, an 
element from the sample is randomly chosen and replaced by a point chosen

274
Further topics in MCMC
according to a specified direction. This direction may depend on the other 
points in the current sample. When n =  1, the Schmeiser and Chen (1991) 
algorithm is obtained. A variation is to allow only directions connecting 
the chosen point to the other points in the sample. When these choices are 
made with the posterior controlling the displacements along the directions, 
the points are automatically chosen. When the displacements are chosen 
according to some other distribution, the points are only proposed and an 
acceptance probability must be evaluated.
The motivation for the adaptive methods is that the sample points will 
cover adequately regions of high probability of nr near the equilibrium and 
subsequent points will tend to concentrate in these areas. For slow mixing 
chains, however, the sample from initial iterations may be far from these 
regions. Movements of the sample toward high probability regions may be 
slow. Hence, these difficulties and the extra computations required prevent 
a straightforward recommendation of these techniques. Rather, they should 
be used as auxiliary devices.
Tierney and Mira (1999) and Mira and Tierney (2002) introduced the 
delayed rejection Metropolis algorithm. In its simplest version a rejected 
draw is still used to help proposing a new candidate. Suppose that 0 is 
the current state of the Markov chain, 9\ is a candidate draw from q\ (9, •) 
and a (9 ,0 i) is the acceptance probability. Instead of repeating the current 
value 9 when 9\ is rejected, the algorithm suggests delaying the rejection 
and instead sample a second candidate draws 02 from q^AO,9\, ■), which is 
accepted with probability
. 
L  
tt(92) qi(92,9i)  
[1 -  a(92,9i)]q2(92,9u9) \
a - m m \
’ ir(0) q
^
)  
[1 -  a(0,91)]q2(9,91,92) j '
Green and Mira (2001) generalized the delayed rejection algorithm by notic­
ing that a  is derived under the unnecessary assumption that the return path 
has to visit 9\. They pointed out that this restriction may limit the formu­
lation of delayed rejection 
algorithms and completely prevents 
its natural
extension to transdimensional situations. See also Al-Awadhi, Hurn and 
Jennison (2004) for a related algorithm.
E xam ple 7.10 Assume that the target distribution is the following mixture 
of two univariate normal densities:
7t(9) =  0.9f N(9; 0,1) +  0.1 f N{9\ 10,1)
such that 7r is bimodal and the modes are relatively far away from each 
other. The performance of the above delayed rejection Metropolis algorithm 
is compared to the performance of a simple random walk Metropolis algo­
rithm. For both algorithms the proposal density is q(9,91) =  
0 ,rf)
where t\ =  2. For the delayed rejection algorithm q2(9,0i,92) =  /jv(02',#i> 
r | ), where t 2 =  4. Both chains started at 9 ^  =  10.5 and were run for 
15000 iterations. As it can be seen in Figure 7.5, the delayed rejection algo-

Convergence acceleration
275
Random waik Metropolis
UIl.
w
W^nPfl™
Delayed rejection Metropolis
\ 
a-
J L 
= i
L
i 
i 
i----------- 1------- 1 
i-------------- 1----------1------------1------------1
~3i 
0 
36 
7 
10 6 
-3 5 
0 
36 
7 
106
Figure 7.5 Comparison between random walk Metropolis (left panels) and delayed 
rejection Metropolis (right panels): chain paths (top panels); autocorrelation func­
tions (middle panels); and target density versus histogram approximation (bottom 
panels).
rithm induces the appropriate number visits to the lower mode of the target 
distribution and better mixing of the chain to lower chain autocorrelation.
Convergence acceleration can also be achieved by directly changing the 
chain output with resampling techniques. Assume m parallel chains are 
used for sampling and a sample 
dm is available at iteration j.
This is a sample from the marginal distribution of the chain 7r^ at iter­
ation j  and the objective is to use this sample to obtain a sample from 
7T. Successively drawing samples through the chain will attain that but 
Gelfand and Sahu (1994) considered the possibility of making this sample 
an approximate sample from 7r in a single iteration. Methods to achieve 
this goal were described in Section 1.5 where it was shown how an arbi­
trary approximating density q can be used to generate a sample from 7r. 
In the present context, the approximating density 
is not directly avail­

276
Further topics in MCMC
able and must be estimated from its sample. Methods for doing it were 
discussed in Chapter 5. Once this density estimate 
is obtained, resam­
pling techniques can be used. Rejection methods are preferred to weighted 
resampling in principle when calculation of the enveloping constant is fea­
sible. This is rarely the case and Gelfand and Sahu (1995) concentrated on 
weighted resampling.
An approximate sample from n is obtained by forming weights w% oc 
i =  1 ,..., m, and resampling from the discrete distribu­
tion with probabilities w\, . . . ,  wrn. If 7r(^ is a reasonable approximation 
for 7r and to is large, the resulting resample will be a good approximation 
to a sample from 7r. This resample substitutes the current sample with 
consequent approximation to the limiting distribution. Even when the ap­
proximation is not very accurate, the resulting resample should be closer 
to a sample from tt with convergence improvements.
Another suggestion made by Gelfand and Sahu (1995) is the adaptation 
of the transition kernels to forms that speed up the convergence. They work 
in the discrete case with an adaptive initial phase in the chain designed to 
identify better transition matrices among those with the same limiting dis­
tribution. This procedure does not affect the convergence of the chain but 
allows identification of better transitions. The changes between transitions 
in the initial phase are deterministic and do not violate convergence results. 
They apply these results to the choice of tuning parameters in specific pro­
posal transition kernels, thus providing further theoretical justification for 
their choice (see also Section 6.3).
Liu, Liang and Wong (2000) introduced the multiple-try Metropolis algo­
rithm. They claim that the algorithm improves exploration of neighboring 
regions defined by the transition kernel q(9,cf)). They define weights
w{6,4>) =  TT(4>)q(9,(j))\{6,4>),
where A(9, (f>) is a nonnegative symmetric function in 9 and <p with A(9 ,4>) >
0 whenever q(9, <fi) > 0. Suppose that 9 is the current state of the Markov 
chain, then one iteration of the algorithm proceeds according to the follow­
ing steps:
1. Draw a random sample <j>\, ■ ■ ■ 
from q(9, ■);
2. Select <pk from {<f>J,. . . ,  <p*k} with probability proportional to w(4>*j, 9)\
3. Draw 
from q(4>k, •);
4. Calculate the acceptance probability
■ 
J\ 
w{(!>X,9) +  ■ • ■ + w(4>l,9) \ 
a — mm < 1, ——— — --------------- ,, 
, , >.
I 
w(<Pi,<Pk) H--------1- w(<pk,<t>k) J
If the move is accepted, set 4> =  4>k- If the move is not accepted, set 
4> =  B.
Liu, Liang and Wong (2000) pointed out that the multiple try Metropolis

Convergence acceleration
277
algorithm corresponds to Frenkel and Smit’s (1996) orientational-biased 
Monte Carlo when q(9, (fi) is symmetric . Two of the A functions introduced 
by Liu, Liang and Wong (2000) are Ai(9,(fi) =  2{q(6,(fi) +  q((fi,9)}~1 and 
A2(0,(fi) =  {q(8, <fi)q((fi, 9)}~a . When a =  1, w(9,(fi) can be thought of as 
weights obtained in a weighted resampling algorithm with proposal density 
q(8, (fi) and target 7T. Liu, Liang and Wong (2000) recommend using a close 
to one.
E xam ple 7.11 Consider Example 3.7 again, but replacing the independent 
uniform prior distributions for fa and fa by independent normal prior dis­
tributions fa ~  N((31;20,202) and fa ~  N (fa; 0,1.52). This change leads 
to the following posterior distribution (after integrating o 2 out)
7r(/3) oc
Both random walk Metropolis and random walk multiple try Metropolis 
algorithms are implemented. The random walk was run for 10000 iterations, 
while a computationally comparable multiple-try was run for 2000 iterations 
with k =  5. The proposal distributions for both algorithms are q(f3\,fa) =  
/jv (/3i;/3i,252) and q((32, fa) =  fN(fa',fa,32)- Using the random walk al­
gorithm, approximations for the posterior mean, standard deviation and 
95% credibility interval of fa are 20.066, 6.506 and (12.300,38.859), respec­
tively. Similarly, approximations for the posterior mean, standard deviation 
and 95% credibility interval of fa are 0.808, 0.621 and (0.109,2.569), re­
spectively. Using the multiple-try algorithm, approximations for the poste­
rior mean, standard deviation and 95% credibility interval of 18.190, 5.951 
and (9.527,32.912), respectively. Similarly, approximations for the poste­
rior mean, standard deviation and 95% credibility interval of fa are 1.073, 
0.718 and (0.171,2.766), respectively. Results appear in Figure 7.6 showing 
a good agreement between both sampling approaches.
Acceptance rates for fa and fa are 0.1310 and 0.1185 for the random walk 
Metropolis algorithm and 0.5910 and 0.6520 for the multiple try Metropolis 
algorithm, suggesting an efficiency gain with the use of multiple-try algo­
rithms. Effective sample sizes based on fa and fa are 132 and 29 for the 
random walk Metropolis algorithm and 1365 and 577 for the multiple try 
Metropolis algorithm. These results shows a clear superiority of the use of 
multiple try over standard Metropolis algorithms.
Finally, another important development in the area should be mentioned. 
Propp and Wilson (1996) suggested a Markov chain sampling scheme that 
would enable exact draws from the target distribution. For this reason, their 
approach was named perfect sampling. Their work attracted interest from 
the statistical community and has been attracting a considerable amount 
of research effort. Nevertheless, most of the work to date is restricted to 
theoretical aspects and applications only in a limited set of situations.
6
£
i= 1
[yi - f a -  fa e~ 02X'\2 ) 
/jv(/?i; 20,202)/iv(/32; 0 ,1.52)

278
Further topics in MCMC
eooo sooo
2000 4000 eooo 8000
Figure 7.6 Random walk Metropolis and multiple try random walk Metropolis 
with proposal q ( { 3 i ,j 3 i )  =  /jv (y 9 i;/3 i,2 5 2) and q ((3 2 ,@ 2 ) =  /n (/3 2 ;/3 2 ,3 2). Random 
walk Metropolis: (a) trace plot of /3i, (b) trace plot of /?2 and (c) contours of 
7r(/3) against draws from the posterior distribution. Random walk multiple try 
Metropolis: (d) trace plot of f3 \, (e) trace plot of (32 and (f) contours of ir((3 ) 
against draws from the posterior distribution.
According to Sisson (2005), there are indications that this area would not 
be as fruitful as it initially seemed. Future research will indicate if this is 
the case.
7-4-2 Alterations to the equilibrium distribution
The use of resampling described above was applied to intermediate iter­
ations of the chain, before it had reached equilibrium. The same ideas 
could be applied directly to the equilibrium stage. Assume it is possible 
to construct a chain with fast convergence to a limiting distribution q that 
approximates 7r. A sample from q is available after convergence of the chain 
is established but the objective is to obtain a sample from ir. Once again, 
resampling methods can be used. Unlike the previous case, the expression 
for q will typically be available here but the enveloping constant will still 
be difficult to obtain. Weighted resampling seems a better option although 
it provides a sample from an approximation to ir. Weights Wi based on the

Convergence acceleration
279
ratio 7r jq at sampled values can be calculated and the resample drawn from 
the discrete distribution concentrated at the sample from q with respective 
weights w\,... ,w m.
One possibility for q is obtained by heating the target distribution ac­
cording to q(8) oc 7r(#)1//T where the constant T > 1 receives the physical 
interpretation of system temperature, hence the nomenclature used. This 
mechanism was suggested by Jennison (1993). It is based on simulated 
annealing (Kirkpatrick, Gelatt and Vecchi, 1983; Ripley, 1987), an opti­
mization technique designed to find maxima of functions. The equilibrium 
distribution 7r corresponds to the basal temperature T =  1. The heated 
distribution q is flattened with respect to 7r and its density gets closer 
to the uniform distribution. It becomes easier to design a Markov chain 
that converges faster to the equilibrium distribution. This alteration is 
particularly relevant for the case of a distribution with distant modes. It is 
generally difficult to construct chains allowing for frequent moves between 
regions around the modes. Gibbs sampling will make these movements very 
slowly and Metropolis-Hastings algorithms will tend to reject most of these 
moves. By flattening the modes, the moves required to cover adequately 
the parameter space become more likely. Gilks and Roberts (1996) sug­
gested modifying the distribution by the inclusion of what they call step­
ping stones. These are lumps of probability redistributed to regions to ease 
the moves between modes. This is in a way a discrete version of the heat­
ing procedure that redistributes weights continuously over the parameter 
space. Besag and Green (1993) also discussed approaches to multimodality 
in more qualitative terms.
The idea of heated equilibrium distributions was also used by Geyer 
(1991). The difference here is to use m parallel chains, each having equilib­
rium distribution
7i\(9) oc 7r(0)1/Ti 
(7-24)
gradually heated according to the rule Tj =  1 +  X(i — 1), i =  1 ,...,  m, 
for some A > 0. The target distribution is simply one of these chains, 
corresponding to the case i =  1. Chains with higher temperatures will have 
more movement than cooler chains, including the chain of interest i =  1. 
The objective here is to make the low temperature chain benefit from the 
moves from the heated chains. So, jumps between the chains are proposed 
in addition to regular moves within each chain. At iteration j, an exchange 
between the states of chains i and k is proposed. The acceptance probability 
of this swap is
- 
(f)U) nUU 
min
After convergence is reached at the m chains, a sample from chain i =  1

280
Further topics in MCMC
is drawn. The remaining m -  1 are only used to speed the convergence of 
the chain of interest and once that is achieved, they are discarded. This is 
an obvious computational disadvantage of the method. Another disadvan­
tage is that sampling from the heated distributions will generally be more 
difficult.
E xam ple 7.12 The number of positive responses yi out ofni trials at level 
Xi is modelled by the binomial model
Vi|/3 ~  Bin (rii, ^ +  e- ( 01+02Xi) )  > 
* = 1 , 2 , 3 , 4 .
where {3 =  (/3i,/?2)- Let x  =  (—0.863, —0.296, —0.053,0.727), n =  (5,5,5,5) 
and y =  (0,1, 3,5). When a flat prior is adopted for (3, the posterior dis­
tribution becomes
tt(/3) (X J ]  11 {1 + e
^ +
^ } “ n* .
i=l 
1
The objective here is to find the posterior mode. The above simulated 
annealing algorithm is implemented for combinations of four initial val­
ues and two cooling schedules. The proposal distribution is q(f3\(3^) =  
/jv(/?;/?^ ,0 .0 5 2/2). Figure 7.7 exhibits the behavior of the algorithm. It can 
be seen that the convergence is achieved faster when Ti =  1/i for both f3\ and 
(32- For instance, the approximate mode is (0.88,7.99) when 
=
(5,30) was the initial value. The actual mode ofw  is (0.87,7.91), which was 
obtained by a standard Newton-Raphson-like algorithm.
A similar idea was proposed by Marinari and Parisi (1992) under the 
name of simulated tempering. Again, to chains with respective equilibrium 
distributions 7Tj, i =  1 ,..., to, are used, but in series and not in parallel as 
above. One possibility for the nt is given by (7.24). The chain thus formed 
alternates between equilibrium distributions and is formally extended to a 
chain of (9, i) where i denotes the sampling scheme (with respective equi­
librium distribution 7r») considered.
This structure is very similar to that used for model choice based on 
chains with jumps. The only changes are the fact that the parameter is the 
same through all components and the interpretation given to the additional 
component introduced. For model choice, it represents the model consid­
ered. Here, it only specifies the auxiliary chain constructed to accelerate the 
convergence of the chain of interest. Geyer and Thompson (1995) suggested 
that only jumps between neighboring models are allowed and provide fur­
ther discussion on implementation issues. Once again, after convergence is 
reached, only samples of 9 corresponding to i =  1 are retained.
Simulated annealing algorithms can, at least in principle, be used to 
search for modal models or classes of modal models when the number of 
entertained models is large, or even uncountable. Andrieu, de Freitas and

Convergence acceleration
281
Figure 7.7 Simulated annealing paths for four distinct initial values (5,30), 
( —2,40), ( - 4 , - 1 0 )  and (6 ,0 ), two cooling schedules '1\ =  1 /i (left column) 
and Ti =  l/[1 0 1 og (l +  i)] (right column) and proposal distribution q((3\0^) =  
/jv (/? ;/?^ ,0 .0 5 2/2). Joint trajectories for (/3x,p2) appear on the top row along 
with contours of the posterior density 7r(/3), while individual trajectories for p\ 
and /32 appear on the second and third rows, respectively.
Doucet (2000) and Brooks, Friel and King (2003) extended standard sim­
ulated annealing to transverse the model space with limiting distribution
h i f i j , j) oc exp | -  
|
where E(6j, j) is a model-ranking function to be minimized. In their capture- 
recapture applications, King and Brooks (2004) and Sisson and Fan (2004)

282
Further topics in MCMC
adopted AlC-type criteria where E {9j,j) =  —2f(y\9j,j) +  2dj, for dj the 
dimension of Oj. Unfortunately, models that are chosen based on arbitrary 
rankings are not guaranteed to be models with high posterior model prob­
abilities (Sisson, 2005). Nonetheless, from a decision-theoretic perspective, 
the above ideas can be thought of as strategies for selecting models (deci­
sions) that maximize utility functions based on E(6j,j).
7-4-3 Auxiliary variables
Simulated tempering is a scheme where an additional variable was intro­
duced with the aim of reducing slow mixing of the original chain. Slow 
mixing of the chain leads to slow convergence and is normally due to partic­
ularities of the target distribution such as multimodality or high correlation 
between some of the components of 0. Whatever the reason, convergence 
is slow due to high autocorrelation in the chain.
The introduction of auxiliary variables attempts to remove these sources 
of correlation and to hasten convergence of the extended chain. Auxiliary 
variables are also introduced in order to make complicated posterior dis­
tributions more manageable (see Example 7.13 below). Consider the intro­
duction of variables <j> with known (and preferably easy to sample from) 
conditional distribution ir((j>\9). The equilibrium distribution now becomes 
7r(9,<p) =  Tr(9)n((t>\9) where the first term to the right hand side is the tar­
get distribution. The extended chain alternates generations from the full 
conditional distributions ng(9) oc tt(9)tt(<P\9) and 7^(0) =  ir(4>\9) at every 
iteration. If these generations can be made directly, Gibbs sampling can be 
applied to blocks 9 and <p and discarding the (ps once the MCMC sample 
is obtained. Besag and Green (1993) considered chains with more general 
transition kernels with special attention to spatial statistics problems.
E xam ple 7.13 Assume that 9 has a posterior distribution that can be writ­
ten as
i
7t(9) = q { 9 ) Y [ b l(8)
i=  1
where q has an easy to sample distribution and the functions bi are compli­
cated terms involving interactions between the components of the vector 9 
(Edwards and Sokal, 1988). Examples include spatial and temporal corre­
lation. A vector (j> =  
. . . ,  <f>j)' of components conditionally independent
given 9 can be defined with distributions <fri\0 ~  £/[0,6,(#)], i =  1 
Generation from cp\0 is simple and

Convergence acceleration
283
tt(0, fa) 
=  
n(6)Tr((/)\9)
i=l 
j = l  
U  !
I
=  
q ( e ) l [ i ( 0 < < p l < b l( 9 ) ) .
1 = 1
Generation of 9 from ttq involves a generation from q followed by verifica­
tion of conditions bi(9) >  fa. By the rejection method, the generated value 
is retained if they are all satisfied. Otherwise, a new value is generated from 
q until all conditions are satisfied.
E xam ple 7.14 The vector 9 =  (9\,... ,9d)' represents the colors in the 
pixels of a given image. Each position i has color 9i varying in a finite set 
of possibilities { 1 , . . . ,  L }. A frequently adopted distribution for 9 is given 
by the Potts model that seeks to reflect similarities in colors of neighboring 
pixels. Its probability function is given by the Gibbs distribution with energy 
E(9) being the number of neighboring pairs of the same color. So, E(9) =  
E j~k I (ei =£ 9k), where j  
k denotes that the pair i =  (j, k) consists of 
neighboring positions. The probability function can be written as
ir(9) oc 
where bi(9) =  
and (3 =  \/{kT) .
i
If (3 is large, there is high correlation between the components of 9 and the 
use of auxiliary variables is recommended.
Define an auxiliary vector (fi =  (<fii, • • ■, <fii) where I is the number of pairs 
of neighbors and the auxiliary variable fa associated with the pair i =  (j, k) 
has independent bern(bi(9)) distributions, i =  1
If 9j — 9k, fa =  1 
and if 9j ^  9k, fa ~  bern(e~P). Generation from <j>\9 is therefore trivial. 
Generation of 9\<fi is based on a uniform distribution over { 1 , . . . ,  L } d. The 
generated value will be accepted if all pairs of neighbors satisfy the config­
uration given by (p. This generation mechanism was proposed by Swendsen 
and Wang (1987). It is discussed in the statistical context by Besag and 
Green (1993) and Green (1996).
Gilks and Roberts (1996) considered other possible uses of auxiliary vari­
ables. They include the important case of missing observations and a ver­
sion of the rejection method where the auxiliary variable is again an indi­
cator variable controlling the acceptance probability. Gilks, Best and Tan 
(1995) suggested a generalization of the adaptive rejection method where 
the acceptance probability is replaced by a Metropolis step. They show 
that their method can also be seen as another use of auxiliary variables. 
Finally, Besag and Green (1993) discuss other possibilities still preserving 
reversibility of the chain. See Higdon (1998) and Damien, Wakefield and

Walker (1999) for further developments and discussions about auxiliary 
variables.
The slice sampler
Perhaps one of the most popular auxiliary variable MCMC algorithm to 
date, the slice sampler is derived from the Example 7.13 by setting 1 =  1, 
q(0) =  1 and n((f>\6) =  tt~1(9)I(0 < 4 > <  ir(0))- In this case,
7r(0,</>) =  7r(6»)7r(0|6») =  t t ( 0 ) — ^ y / ( 0  <  <f> <  i r ( 0 ) ) ,
suggesting that standard MCMC schemes can be applied to sample from 
n(4>\d) and tt(0\4>) iteratively.
Sampling 4> from 
usually imposes no difficulty. 0 is sampled from
a uniform distribution over the region {0,7r(0) > </>} (see Figure 7.8(a)). 
The name of the sampler derives from the fact that the region is defined 
by slicing the density 7r(0) horizontally at the contour level <f>. Sampling 0 
may not be straightforward because defining the region requires solving the 
inequality 7r(0) >  <p for 0. This can be a computationally challenging task 
in higher dimensional parameter spaces. Neal (2003) proposed an adaptive 
rejection-like algorithm to overcome this difficulty. Silva, Lopes and Migon 
(2006) uses Neal’s adaptive algorithm in the context of generalized inverse 
Gaussian models. Roberts and Rosenthal (1999) and Mira and Tierney 
(2002) examined theoretical properties of the slice sampler.
E xam ple 7.15 Consider Example 3.6 again, but now suppose that the to­
tal number of animals is 22 and that counts are y =  (14,3,5). Adopting an 
uniform prior for 8 leads to ir(0) oc (2 +  0)14(1 — 0)305. Posterior simu­
lation is now performed through the above slice sampling algorithm. Based 
on a sample of size M  =  5000 from 1t(0), approximations for the posterior 
mean, standard deviation and 95% credibility interval of 6 are 0.698, 0.123 
and (0.417,0.908), respectively. Figure 1.8 illustrates the sampler.
7.5 E xercises
7.1 Show that all the estimators of the model likelihood presented in Sec­
tion 1.2 are consistent and obtain their asymptotic variance. Also, discuss 
whether variance of { / 4(y)}~' diverges or not.
7.2 Consider the annealed importance sampling algorithm.
(a) Derive the annealed importance weights (1.7).
(b) Derive the annealed importance sampling estimator of f(y ) of Equation 
(7.8).
7.3 Derive the bridge identity (7.9).
284 
Further topics in MCMC

Exercises
285
0.0 
0.2 
0.4 
0.6 
0.8 
1 .O
1OO 
200 
300 
400 
500
iterations
Figure 7.8 Slice sampling performance after (a) 20 and (b) 500 iterations. The 
first 500 draws appear in (c) and the resulting histogram approximation of -k 
appears in (d).
7.4 Consider the path algorithm.
(a) Derive Equation (7.10).
(b) (Friel and Pettitt, 2005) Consider the power posterior density g(8\\) =  
p(8)lx(8)/c(A) and assume that A ~  U(0,1) and (6\, A i) , . . . ,  (8n, An ) 
is a sample from the joint density g(8\X)g(X). Show that H(8,\) — 
p(8)l(9)x \ogl(8) and the path estimator of f(y )  is
exp |  ^ 
P ( ej ) lXl ( ° j) loS 1 ( O j )
7.5 Derive the identity from Equation (7.12).
7.6 Consider Example 7-4■
(a) Derive the minimum posterior predictive loss criteria D f , D 2 and
D§-
(b) Derive the deviance information criteria D f, D f  and D §.
7.7 Show that the densities used in the intrinsic Bayes factor of Berger and 
Pericchi (1996) and the fractional Bayes factor of O ’Hagan (1995) can be 
written in the form p(ys1 \ys2)> identifying the sets Si and S2 in each case.

286
Further topics in MCMC
7.8 (Gelfand, 1996) Table 7.9 presents measurements of tree trunk circum­
ferences (Draper and Smith, 1981). Let y^ be the time tj measurement of 
the ith tree, for i =  1, . . . , /  =  5 and j  =  1 , . . . ,  J =  7. Consider the fit of 
the following four different random effects (or hierarchical) models:
: 
yij =  po +  bi +  Zij
■ 
yij =  Po +  Pitj + bi +  
£ij
M 3 : 
y^ = Po(l +  p\e@'ltj) 1 +  £ij
M 4 : 
yij =  (Po +  bi)( 1 +  P
\ e
1 +
where £ij are a N(0,<r2) random sample and bi are assumed N ( 0 , t 2). Let 
Bi be the vector of parameters of model Mi, for i =  1 , . . . ,  4. Assume that 
the joint priors are p(9\\Mi) oc ct- 2 t - 3 exp {—2(<r-2  4- t ~ 2}, p(92\M2) oc 
<7- 2T- 3exp{-2(<r-2  + r - 2 }, p(6>3|M3) oc a ~2 exp{—2cr-2 } andp(04|M4) oc 
ct~2t ~3 exp {—2(o~2 +  t - 2 }  . Compute the deviance information criteria 
(D s ) and the pseudo-Bayes factors (G4) and compare them.
Trees
1
2
3
4
5
Days
30
33
30
32
30
118
58
69
51
62
49
484
87
111
75
112
81
664
115
156
108
167
125
1004
120
172
115
179
142
1231
142
203
139
209
174
1372
145
203
140
214
177
1582
Table 7.9 'Drunk circumference (in milimeters) of 5 orange trees measured at 7 
different times (in days) (Draper and Smith, 1981).
7.9 Consider the model choice setting of Section 7.3 with a superparameter 
9 =  (9i,... ,0j) and a quantity M  indicating a model with joint posterior 
n(9,j).
(a) Obtain the posterior distributions of 9j\M =  j, j  =  1 ,...,  J, and M . 
Show that samples from 9j\M =  j  are obtained by retaining the draws 
of Oj associated with a value j  for M .
(b) Consider a component of the parameter, say (j), that is shared by all 
models. Obtain its marginal posterior distribution tt(</>).
7.10 Show that the supermodel approach of Carlin and Chib (1995) cannot 
be applied when the number of models considered is not finite.

Exercises
287
7.11 Show that the reversible jump Markov chain approach for inference 
with a collection of models reduces to the Metropolis-Hastings algorithm if 
only one model is considered and moves are always proposed. What happens 
if there is a positive probability of not proposing a move ?
7.12 Consider the reversible jump Markov chain approach with transition 
kernel qm((9, j), (4>, k)) for each proposed move m. Show that reversibility 
of chain is ensured if the acceptance probability associated with move m is 
given by
c*m((8,j),(4>,k)) =  min
in jl,
7.13 (Green, 1995) Consider again the conditions of Example 7.7 with irre­
ducible birth and death chains moving across models Mk with piecewise con­
stant intensity rates having k steps and model parameter 9k =  (A(k),t(k))' 
where A(k) =  (Ao, A i,. . . ,  Afc) and t(k) =  (ti , . . . ,  tk), k =  0 , 1,2,...
(a) Show that the likelihood for model M k is given by
k
I (6 k, k) 
1 J Ay'f;-A' ('' • 1 '<>
j =o
where dj is the number of occurrences in interval Ij, j  =  0,1,. . .  ,k.
(b) Show that the likelihood ratio between model Mk with parameters 9'k
and 9k is given by
k 
/
\ dj
Zrx = 
( tM  
e^Xj 
1
j = o  
1
if they differ only on the values of A(k) and 
k
j=o
if they differ only on the values oft(k).
(c) 
Consider a move of type 3 which proposes a change of the intensity 
rate of a randomly chosen interval, say j, from A j to A' according to 
a random walk log A' ~  t/[log A j — 1/2, log A j +  1/2]. Show that the 
acceptance probability of this move is
'K  
'
M
a ( | )  exp[—/3(A' — Aj)]
(d) 
Consider a move of type 4 which proposes a change of the endpoint of a 
randomly chosen interval, say j , from tj to £' according to a U [tj-i,tj+ i]

288 
Further topics in MCMC
distribution. Show that the acceptance probability of this move is
mm < 1, lrt------------- -—
------------
(tj+ 1 
t j - 1)
(e) Consider a move of type 1 which proposes the birth of a new endpoint at 
a point uniformly chosen on [0, T ]. Show that the acceptance probability 
of this move is
mm
n(8k+i,k + 1 )  ^ 
B k+ i,fc 
^ 
dOk+1 
)
’ 
ir(Qk,k) 
B k'k+iq(t*,u) 
d(9k,t*,u) J
(f) Show that in the above expression
^{9k+ i,k  +  1) _  l{9k+ i,k  +  1) 
p{X(k +  1 ),t(k +  1 ),k +  1)
7r(dk,k) 
l(0k, k) 
X 
p(X(k),t{k),k)
where
p(X(l),t(l),l) =  
p(X(l)\l)p(t(l)\l)fP ( l ) , l  =  l , 2 , . . . ,
P(X(k +  l )\k +  l) 
=  
^
 
m d
p(X(k)\k) 
r(a) V 
X j  
J
p(t{k +  l)\k +  1) _  
2(k +  1)(2k +  3) ( f  -  tj)(tj+ 1 
-  **)
p(t(/c)|fc) 
T 2 
t j + i - t j  
'
(g) Show that B kik+i =  pk, B k+hk =  qk+1/(k +  1) andq(t*,u) =  1 /T  and 
that the Jacobian is given by (A' +  A'+1 )2/ A j .
(h) Show that the acceptance probability of a death move is
mm
f 
7r(9k,k) 
x Bktk+lq(t*,u) x d(6k ,t*,u) 1
I 
’ 7r(0it+i,fc +  1) 
B k+iik 
d9k+1 
J
7.14 5/iow that the Schmeiser and Chen (1991) algorithm corresponds to 
taking the proposal density g in the Phillips and Smith (1993) algorithm 
as g(c\9^~1\ e ^ )  oc 7r(0^ ^ +  c e ^ ) .  Discuss other possible forms for g, 
commenting on their advantages/disadvantages with respect to the above 
choices.
7.15 Generate samples from the Potts model described in Example 7.13 
varying the values of ft and using both a componentwise Markov chain 
sampler and the method of auxiliary variables. Compare the convergence of 
the generated samples from both approaches.

References
Abanto, C. A., Lopes, H. F. and Migon, H. S. (2005) Simulation-based se­
quential analysis for the bivariate stochastic volatility-volume model. Tech­
nical Report, Graduate School of Business, University of Chicago.
Abramowitz, M. and Stegun, I. A. (eds) (1965) Handbook of Mathemat­
ical Functions, National Bureau of Standards, Washington.
Achcar, J. A. and Smith, A. F. M. (1989) Aspects of reparametrisation 
in approximate Bayesian inference. In Bayesian and Likelihood Methods in 
Statistics and Econometrics: Essays in Honour of George A. Barnard (eds
S. Geisser et al.), North Holland, Amsterdam, 439-52.
Aguilar, O. and West, M. (2000) Bayesian dynamic factor models and 
variance matrix discounting for portfolio allocation. Journal of Business 
and Economic Statistics, 18, 338-57.
Ahrens, J. H. and Dieter, U. (1974) Computer methods for sampling 
gamma, beta, Poisson and binomial distributions. Computing, 12, 223-46.
Aitchinson, J. and Dunsmore, I. R. (1975) Statistical Prediction Analysis, 
Cambridge University Press, Cambridge.
Aitkin, M. (1991) Posterior Bayes factors (with discussion). Journal of 
the Royal Statistical Society, Series B, 53, 111-42.
Al-Awadhi, F., Hurn, M. and Jennison, C. (2004) Improving the accep­
tance rate of reversible jump MCMC proposals. Statistics and Probability 
Letters, 69, 189-98.
Albert, J. H. (1988) Computational methods using a Bayesian hierarchi­
cal generalized linear model. Journal of the American Statistical Associa­
tion, 83, 1037-45.
Albert, J. H. (1996) A MCMC algorithm to fit a general exchangeable 
model. Communications in Statistics - Simulation and Computation, 25, 
573-92.
Anderson, T. W. (1958) An Introduction to Multivariate Statistical Anal­
ysis, Wiley, New York.
Andrews, D. F. and Mallows, C. L. (1974) Scale mixtures of normality. 
Journal of the Royal Statistical Society, Series B, 36, 99-102.
Andrieu, C., de Freitas, J. and Doucet, A. (2000) Reversible jump MCMC 
simulated annealing for neural networks. In Uncertainty in Artificial In- 
teligence (eds C. Boutilier and M. Goldszmidt), Morgan Kaufmann, San 
Francisco, 11-8.

290
References
Arjas, E. and Heikkinen, J. (1997) An algorithm for nonparametric Bayes­
ian estimation of a Poisson intensity. Computational Statistics, 12, 385-402.
Assungao, J. J., Gamerman, D. and Assungao, R. M. (1999) Regional 
differences in factor productivities of Brazilian agriculture: a space varying 
parameter approach. Proceedings of the X V  Latin American Meeting of 
the Econometric Society.
Azevedo, G. M. (2002) Dirichlet process mixture: a hierarchical approach, 
unpublished M.Sc. Thesis, IM-UFRJ (in Portuguese).
Banerjee, S., Carlin, B. P. and Gelfand, A. E. (2004) Hierarchical Mod­
eling and Analysis of Spatial Data, Chapman & Hall/CRC, London.
Barker, A. A. (1965) Monte Carlo calculation of the radial distribution 
functions for a protonelectron plasma. Australian Journal of Physics, 18, 
119-33.
Barone, P. and Frigessi, A. (1989) Improving stochastic relaxation for 
Gaussian random fields. Probability in the Engineering and Informational 
Sciences, 4, 369-89.
Bates, D. M. and Watts, D. G. (1988) Nonlinear Regression Analysis and 
Its Applications, New York. Wiley.
Behrens, C., Lopes, H. F. and Gamerman, D. (2004) Bayesian analysis of 
extreme events with threshold estimation. Statistical Modelling, 4, 227-44.
Bennett, J. E., Racine-Poon, A. and Wakefield, J. C. (1996) MCMC for 
nonlinear hierarchical models. In Markov Chain Monte Carlo in Practice 
(eds W. R. Gilks, S. Richardson and D. J. Spiegelhalter), Chapman & Hall, 
London, 339-57.
Berg, A., Meyer, R. and Yu, J. (2004) DIC as a model comparison cri­
terion for stochastic volatility models. Journal of Business and Economic 
Statistics, 22, 107-20.
Berger, J. O. (1985) Statistical Decision Theory and Bayesian Analysis, 
2nd edn, Springer Verlag, New York.
Berger, J. O. and Bernardo, J. M. (1992) On the development of reference 
priors (with discussion). In Bayesian Statistics 4 (eds J. M. Bernardo et 
al.), Oxford University Press, Oxford, 35-60.
Berger, J. O., Bernardo, J. M. and Mendoza, M. (1989) On priors that 
maximize expected information. In Recent Development in Statistics and 
Their Applications (eds J. P. Klein and J. C. Lee), Freedom Academy 
Publishing: Seoul, 1-20.
Berger, J. O., de Oliveira, V. and Sanso, B. (2001) Objective Bayesian 
analysis of spatially correlated data. Journal of the American Statistical 
Association, 96, 1361-74.
Berger, J. O. and Pericchi, L. R. (1996) The intrinsic Bayes factor for 
model selection and prediction. Journal of the American Statistical Asso­
ciation, 91, 109-22.
Bernardo, J. M. (1979) Reference posterior distributions for Bayesian

References
291
inference (with discussion). Journal of the Royal Statistical Society, Series 
B, 41, 113-47.
Bernardo, J. M. and Smith, A. F. M. (1994) Bayesian Theory, Wiley, 
New York.
Besag, J. (1974) Spatial interaction and the statistical analysis of life 
systems (with discussion). Journal of the Royal Statistical Society, Series 
B, 48, 192-236.
Besag, J. (1986) On statistical analysis of dirty pictures (with discussion). 
Journal of the Royal Statistical Society, Series B, 48, 259-302.
Besag, J. (1989) A candidate’s formula: A curious result in Bayesian 
prediction. Biometrika, 76, 183.
Besag, J. and Green, P. J. (1993) Spatial statistics and Bayesian compu­
tation (with discussion). Journal of the Royal Statistical Society, Series B,
55, 25-37.
Besag, J., Green, P. J., Higdon, D. and Mengersen, K. L. (1995) Bayesian 
computation and stochastic systems (with discussion). Statistical Science, 
10, 3-66.
Besag, J. and Higdon, D. (1999) Bayesian analysis of agricultural field ex­
periments (with discussion). Journal of the Royal Statistical Society, Series 
B, 61, 691-746.
Besag, J., York, J. and Mollie, A. (1991) Bayesian image restoration, 
with two applications in spatial statistics (with discussion). Annals of the 
Institute of Statistical Mathematics, 43, 1-59.
Best, N. G., Cowles, M. K. and Vines, S. K. (1995) CODA: Convergence 
Diagnostics and Output Analysis Software for Gibbs Sampler Output: Ver­
sion 0.3. Technical Report, Biostatistics Unit-MRC, Cambridge.
Box, G. E. P. and Muller, M. E. (1958) A note on the generation of 
random normal deviates. Annals of Mathematical Statistics, 29, 610-1.
Breslow, N. E. and Clayton, D. G. (1993) Approximate inference in gener­
alized linear mixed models. Journal of the American Statistical Association, 
88, 9-25.
Brooks, S. P., Friel, N. and King, R. (2003) Classical model selection via 
simulated annealing. Journal of the Royal Statistical Society, Series B, 65, 
503-20.
Brooks, S. P. and Gelman, A. (1998) General methods for monitoring 
convergence of iterative simulations. Journal of Computational and Graph­
ical Statistics, 7, 434-55.
Brooks, S. P. and Giudici, P. (1999) Diagnosing convergence of reversible 
jump MCMC algorithms. In Bayesian Statistics 6 (eds J. M. Bernardo et 
al.), Oxford University Press, Oxford, 733-42.
Brooks, S. P. and Giudici, P. (2000) MCMC convergence assessment via 
two-way ANOVA. Journal of Computational and Graphical Statistics, 9, 
266-85.
Brooks, S. P., Giudici, P. and Philippe, A. (2003) On non-parametric

292
References
convergence assessment for MCMC model selection. Journal of Computa­
tional and Graphical Statistics, 12, 1-12.
Brooks, S. P., Giudici, P. and Roberts, G. O. (2003) Efficient construction 
of reversible jump MCMC proposal distributions (with discussion). Journal 
of the Royal Statistical Society, Series B, 65, 3-55
Brooks, S. P. and Roberts, G. O. (1998) Assessing convergence of Markov 
chain Monte Carlo algorithms. Statistics and Computing, 8, 319-35.
Capocaccia, D., Cassandro, M. and Olivieri, E. (1977) A study of metasta­
bility in the Ising model. Communications in Mathematical Physics, 39, 
185-205.
Cappe, O., Guillin, A., Marin, J.-M. and Robert, C.P. (2004) Population 
Monte-Carlo. Journal of Computational and Graphical Statistics, 13, 907­
29.
Cappe, O., Robert, C. and Ryden, T. (2003) Reversible jump, birth-and- 
death, and more general continuous time MCMC samplers. Journal of the 
Royal Statistical Society, Series B, 65, 679-700.
Carlin, B. P. and Chib, S. (1995) Bayesian model choice via Markov chain 
Monte Carlo methods. Journal of the Royal Statistical Society, Series B, 
57, 473-84.
Carlin, B. P., Gelfand, A. E. and Smith, A. F. M. (1992) Hierarchical 
Bayesian analysis of changepoint problems. Applied Statistics, 41, 389-405.
Carlin, B. P. and Louis, T. (1996) Bayes and Empirical Bayes Methods 
for Data Analysis, Chapman & Hall, London.
Carlin, B. P. and Louis, T. (2000) Bayes and Empirical Bayes Methods 
for Data Analysis, 2nd edn, Chapman & Hall/CRC, London.
Carlin, B. P., Poison, N. G. and Stoffer, D. S. (1992) A Monte Carlo 
approach to nonnnormal and nonlinear state-space modeling. Journal of 
the American Statistical Association, 87, 493-500.
Carter, C. K. and Kohn, R. (1994) On Gibbs sampling for state space 
models. Biometrika, 81, 541-53.
Carvalho, C. M. and Lopes, H. F. (2005) Simulation-based sequential 
analysis of Markov switching stochastic volatility models. Technical Report, 
Graduate School of Business, University of Chicago.
Casella, G. and George, E. I. (1992) Explaining the Gibbs sampler. The 
American Statistician, 46, 167-74.
Cassandro, M., Galves, A., Olivieri, E. and Vares, M. E. (1984) Metastable 
behaviour of stochastic dynamics: a pathwise approach. Journal of Statis­
tical Physics, 35, 603-34.
Castelloe, J. M. and Zimmerman, D. L. (2002) Convergence assessment 
for reversible-jump MCMC samplers. Technical Report 313, Department of 
Statistics and Actuarial Science, University of Iowa.
Celeux, G., Forbes, F., Robert, C. P. and Titterington, D.M. (2005) 
Deviance information criteria for missing data models (with discussion). 
Bayesian Analysis (to appear).

References
293
Chambers, J. M. (1977) Computational Methods for Data Analysis, W i­
ley, New York.
Chan, K. S. and Geyer, C. J. (1994) Discussion of the paper by Tierney
(1994). Annals of Statistics, 22, 1747-58.
Chen, M.-H. (2005) Computing marginal likelihoods from a single MCMC 
output. Statistica Neerlandica, 59, 16-29.
Chen, M.-H. and Shao, Q.-M. (1997) On Monte Carlo methods for esti­
mating ratios of normalizing constants. Annals of Statistics, 25, 1563-94.
Cheng, R. C. H. and Feast, G. M. (1979) Some simple gamma variate 
generators. Applied Statistics, 28, 290-5.
Chib, S. (1995) Marginal likelihood from the Gibbs output. Journal of 
the American Statistical Association, 90, 773-95.
Chib, S. and Greenberg, E. (1994) Bayes inference for regression models 
with ARM A(p,q) errors. Journal of Econometrics, 64, 183-206.
Chib, S. and Greenberg, E. (1995) Understanding the Metropolis-Hastings 
algorithms. The American Statistician, 49, 327-35.
Chib, S. and Jeliazkov, I. (2001) Marginal likelihood from the Metropolis- 
Hastings output. Journal of the American Statistical Association, 96, 270­
81.
Chib, S. and Jeliazkov, I. (2005) Accept-reject Metropolis-Hastings sam­
pling and marginal likelihood estimation. Statistica Neerlandica, 59, 30-44.
Clayton, D. G. (1996) Generalized linear mixed models. In Markov Chain 
Monte Carlo in Practice (eds W. R. Gilks, S. Richardson and D. J. Spiegel- 
halter), Chapman & Hall, London, 275-301.
Clyde, M. A. (1999) Bayesian model averaging and model search strate­
gies. In Bayesian Statistics 6 (eds J. M. Bernardo et al.), Oxford University 
Press, Oxford, 157-85.
Coles, S. G. and Tawn, J. A. (1996) A Bayesian analysis of extreme 
rainfall data. Applied Statistics, 45, 463-78.
Congdon, P. (1997) Multilevel and clustering analysis of health outcomes 
in small areas. European Journal of Populations, 13, 305-38.
Cowles, M. K. and Carlin, B. P. (1996) Markov chain Monte Carlo con­
vergence diagnostics: a comparative review. Journal of the American Sta­
tistical Association, 91, 883-904.
Cox, D. R. and Reid, N. (1987) Parameter orthogonality and approxi­
mate conditional inference (with discussion). Journal of the Royal Statisti­
cal Society, Series B, 49, 1-39.
Cressie, N. (1993) Statistics for Spatial Data, revised edn, Wiley, New 
York.
Crowder, M. J. (1978) Beta-binomial ANOVA for proportions. Applied 
Statistics, 27, 34-7.
Dagpunar, J. (1988) Principles of Random Variate Generation, Claren­
don Press, Oxford.
Damien, P., Wakefield, J. C. and Walker, S. (1999) Gibbs sampling for

294
References
Bayesian non-conjugate and hierarchical models by using auxiliary vari­
ables. Journal of the Royal Statistical Society, Series B, 61, 331-44.
Deely, J. J. and Lindley, D. V. (1981) Bayes empirical Bayes. Journal of 
the American Statistical Association, 76, 833-41.
De Finetti, B. (1974) The Theory of Probability, Vol. I, Wiley, Chichester.
De Finetti, B. (1975) The Theory of Probability, Vol. II, Wiley, Chich­
ester.
DeGroot, M. H. (1970) Optimal Statistical Decisions, McGraw-Hill, New 
York.
DeGroot, M. H. (1986) Probability and Statistics, 2nd edn, Addison Wes­
ley, Reading.
Dellaportas, P. and Forster, J. (1999) Markov chain Monte Carlo model 
determination for hierarchical and graphical log-linear models. Biometrika, 
86, 615-33.
Dellaportas, P., Forster, J. and Ntzoufras, I. (2002) On Bayesian model 
and variable selection using MCMC. Statistics and Computing, 12, 27-36.
Dellaportas, P. and Smith, A. F. M. (1993) Bayesian inference for gener­
alized linear and proportional hazards models via Gibbs sampling. Applied 
Statistics, 42, 443-60.
Dellaportas, P. and Wright, D. (1991) Positive embedded integration in 
Bayesian analysis. Statistics and Computing, 1, 1-12.
Dempster, A. P. (1997) The direct use of likelihood for significance test­
ing. Statistics and Computing, 7, 247-52.
Dennison, D. G. T., Holmes, C. C., Mallick, B. and Smith, A. F. M. 
(2002) Bayesian Methods for Nonlinear Classification and Regression, W i­
ley, New York.
Devroye, L. (1986) Non-uniform Random Variate Generation, Springer 
Verlag, New York.
Dey, D., Muller, P. and Sinha, D. (eds) (1998) Practical nonparametric 
and semiparametric Bayesian statistics, Lecture Notes in Statistics 133, 
Springer-Verlag, New York.
Dias, R. and Gamerman, D. (2002) A Bayesian approach to hybrid 
splines non-parametric regression. Journal of Statistical Computation and 
Simulation, 72, 175-91.
DiCiccio, T. J., Kass, R. E., Raftery, A. E. and Wasserman, L. (1997) 
Computing Bayes factors by combining simulation and asymptotic approx­
imations. Journal of the American Statistical Association, 92, 903-15.
Dickey, J. (1971) The weighted likelihood ratio linear hypothesis on nor­
mal location parameters. The Annals of Mathematical Statistics, 42, 204­
23.
Dickey, J. (1976) Approximate posterior distributions. Journal of the 
American Statistical Association, 71, 680-9.
Diggle, P. J., Tawn, J. A. and Moyeed, R. A. (1998) Model-based geo­
statistics (with discussion). Applied Statistics, 47, 299-350.

References
295
Doucet, A., de Freitas, N. and Gordon, N. (2001) Sequential Monte Carlo 
Methods in Practice, Springer, New York.
Doucet, A. and Godsill, S. and Andrieu, C. (2000) On sequential Monte 
Carlo sampling methods for Bayesian filtering. Statistics and Computing, 
10, 197-208.
Draper, N. R. and Smith, H. (1981) Applied Regression Analysis, Wiley, 
New York.
Edwards, R. G. and Sokal, A.D. (1988) Generalization of the Fortuin- 
Kasteleyn-Swendsen-Wang representation and Monte Carlo algorithm. Phy­
sical Review, D, 38, 2009-12.
Efron, B. (1986) Why isn’t everyone a Bayesian? American Statistician, 
40, 1-11.
Ehlers, R. S. and Brooks, S. P. (2002) Efficient construction of reversible 
jump MCMC proposals for autoregressive time series models. Technical 
Report, Department of Statistics, UFPR.
Elston, R. C. and Grizzle, J. E. (1962) Estimation of time-response curves 
and their confidence bands. Biometrics, 18, 148-59.
Embrechts, P., Kluppelberg, C. and Mikosch, T. (1997) Modelling Ex­
tremal Events for Insurance and Finance, Springer-Verlag, Berlin.
Evans, M. and Swartz, T. (1988) Sampling from Gauss rules. SIAM Jour­
nal on Scientific and Statistical Computing, 9, 950-61.
Evans, M. and Swartz, T. (1995) Methods for approximating integrals 
in statistics with special emphasis on Bayesian integration problems. Sta­
tistical Science, 10, 254-72.
Fahrmeir, L. and Wagenpfeil, S. (1997) Penalized likelihood estimation 
and iterative Kalman smoothing for non-gaussian dynamic regression mod­
els. Computational Statistics and Data Analysis, 24, 295-320.
Feller, W. (1968) An Introduction to Probability Theory and Its Applica­
tions, Volume I, 3rd edn, Wiley, New York.
Fishman, G. and Moore, L. (1985) An exhaustive analysis of multiplica­
tive congruential random number generators with modulus 231 -  1. SIAM 
Journal of Scientific and Statistical Computing, 7, 24-45.
Frenkel, D. and Smit, B. (1996) Understanding Molecular Simulation, 
Academic Press, New York.
Friel, N. and Pettitt, A. N. (2005) Marginal likelihood estimation via 
power posteriors. Technical Report, Department of Statistics, University 
of Glasgow.
Friihwirth-Schnatter, S. (1994) Data augmentation and dynamic linear 
models. Journal of Time Series Analysis, 15, 183-202.
Gamerman, D. (1997) Efficient sampling from the posterior distribution 
in generalized linear mixed models. Statistics and Computing, 7, 57-68.
Gamerman, D. (1998) Markov Chain Monte Carlo for dynamic general­
ized linear models. Biometrika, 85, 215-27.

296
References
Gamerman, D. and Moreira, A. R. B. (2004) Multivariate spatial regres­
sion models. Journal of Multivariate Analysis, 91, 262-81.
Gamerman, D., Moreira, A. R. B. and Rue, H. (2003) Space-varying 
regression models: specifications and simulation. Computational Statistics 
and Data Analysis, 42, 513-33.
Gamerman, D. and Smith, A. F. M. (1996) Bayesian analysis of longi­
tudinal data studies. In Bayesian Statistics 5 (eds J. M. Bernardo et al.), 
Oxford University Press, Oxford, 587-98.
Garren, S. and Smith, R. L. (1993) Convergence diagnostics for Markov 
chain samplers. Technical Report, Department of Statistics, University of 
North Carolina.
Geisser, S. (1993) Predictive Inference: an Introduction, Chapman & 
Hall, London.
Geisser, S. and Eddy, W. (1979) A predictive approach to model selec­
tion. Journal of the American Statistical Association, 74, 153-60.
Gelfand, A. E. (1992) Discussion of the paper by Gelman and Rubin. 
Statistical Science, 7, 486-7.
Gelfand, A. E. (1996) Model determination using sampling-based meth­
ods. In Markov Chain Monte Carlo in Practice (eds W. R. Gilks, S. Richard­
son and D. J. Spiegelhalter), Chapman & Hall, London, 145-61.
Gelfand, A. E., Banerjee, S. and Gamerman, D. (2005) Spatial process 
modelling for univariate and multivariate dynamic spatial data. Environ- 
metrics, 16, 465-79
Gelfand, A. E. and Carlin, B. P. (1995) Discussion of the paper by Besag, 
Green, Higdon and Mengersen. Statistical Science, 10, 43-6.
Gelfand, A. E. and Dey, D. K. (1994) Bayesian model choice: asymptotics 
and exact calculations. Journal of the Royal Statistical Society, Series B,
56, 501-14.
Gelfand, A. E., Dey, D. K. and Chang, H. (1992) Model determination us­
ing predictive distributions with implementation via sampling-based meth­
ods. In Bayesian Statistics 4 (eds J. M. Bernardo et al.), Oxford University 
Press, Oxford, 147-67.
Gelfand, A. E. and Ghosh, S. K. (1998) Model choice: a minimum pos­
terior predictive loss approach. Biometrika, 85, 1-11.
Gelfand, A. E., Hills, S. E., Racine-Poon, A. and Smith, A. F. M. (1990) 
Illustration of Bayesian inference in normal data models using Gibbs sam­
pling. Journal of the American Statistical Association, 85, 972-85.
Gelfand, A. E., Kim, H.-J., Sirmans, C. F. and Banerjee, S. (2003) Spa­
tial modeling with spatially varying coefficient processes. Journal of the 
American Statistical Association, 98, 387-96.
Gelfand, A. E. and Sahu, S. K. (1994) On Markov chain Monte Carlo 
acceleration. Journal of Computational and Graphical Statistics, 3, 261-7.
Gelfand, A. E., Sahu, S. K. and Carlin, B. P. (1995) Efficient parametri- 
sations for normal linear mixed models. Biometrika, 82, 479-88.

References
297
Gelfand, A. E., Sahu, S. K. and Carlin, B. P. (1996) Efficient parametri- 
sations for generalized linear mixed models (with discussion). In Bayesian 
Statistics 5 (eds J. M. Bernardo et al.), Oxford University Press, Oxford, 
165-80.
Gelfand, A. E. and Smith, A. F. M. (1990) Sampling-based approaches 
to calculating marginal densities. Journal of the American Statistical As­
sociation, 85, 398-409.
Gelman, A. (1996) Inference and monitoring convergence. In Markov 
Chain Monte Carlo in Practice (eds W. R. Gilks, S. Richardson and D. J. 
Spiegelhalter), Chapman & Hall, London, 131-43.
Gelman, A., Carlin, J. B., Stern, H. S. and Rubin, D. B. (1995) Bayesian 
Data Analysis, Chapman & Hall, London.
Gelman, A., Carlin, J. B., Stern, H. S. and Rubin, D. B. (2004) Bayesian 
Data Analysis, 2nd edn, Chapman <fc Hall/CRC, London.
Gelman, A. and Meng, X. L. (1998) Simulating normalizing constants: 
From importance sampling to bridge sampling to path sampling. Statistical 
Science, 13, 163-85.
Gelman, A., Roberts, G. O. and Gilks, W. R. (1996). Efficient Metropolis 
jumping rules. In Bayesian Statistics 5 (eds J. M. Bernardo et al.), Oxford 
University Press, Oxford, 599-607.
Gelman, A. and Rubin, D. R. (1992a) A single series from the Gibbs 
sampler provides a false sense of security. In Bayesian Statistics 4 (eds J. 
M. Bernardo et al.), Oxford University Press, Oxford, 625-31.
Gelman, A. and Rubin, D. R. (1992b) Inference from iterative simulation 
using multiple sequences (with discussion). Statistical Science, 7, 457-511.
Geman, S. and Geman, D. (1984) Stochastic relaxation, Gibbs distri­
butions and the Bayesian restoration of images. IEEE Transactions on 
Pattern Analysis and Machine Intelligence, 6, 721-41.
George, E. I., Makov, U. E. and Smith, A. F. M. (1993) Conjugate like­
lihood distributions. Scandinavian Journal of Statistics, 20, 147-56.
George, E. I. and McCulloch, R.E. (1992) Variable selection via Gibbs 
sampling. Journal of the American Statistical Association, 79, 677-83.
Geweke, J. (1989) Bayesian inference in econometric models using Monte 
Carlo integration. Econometrica, 57, 1317-39.
Geweke, J. (1992) Evaluating the accuracy of sampling-based approaches 
to the calculation of posterior moments (with discussion). In Bayesian 
Statistics 4 (eds J. M. Bernardo et al.), Oxford University Press, Oxford, 
169-93.
Geweke, J. and Zhou, G. (1996) Measuring the pricing error of the arbi­
trage pricing theory. The Review of Financial Studies, 9, 557-87.
Geyer, C. J. (1991) Markov chain Monte Carlo maximum likelihood. In 
Computing Science and Statistics: Proceedings of the 23rd. Symposium on 
the Interface (ed. E. M. Keramidas), Fairfax Station, Interface Foundation, 
156-63.

298
References
Geyer, C. J. (1992) Practical Markov chain Monte Carlo (with discus­
sion). Statistical Science, 7, 473-511.
Geyer, C.J. and Moller, J. (1994) Simulation procedures and likelihood 
inference for spatial point processes. Scandinavian Journal of Statistics, 
21, 359-73.
Geyer, C. J. and Thompson, E. A. (1995) Annealing Markov chain Monte 
Carlo with applications to ancestral inference. Journal of the American 
Statistical Association, 90, 909-20.
Gilks, W. R. (1992) Derivative-free adaptive rejection sampling for Gibbs 
sampling. In Bayesian Statistics 4 (eds J. M. Bernardo et al.), Oxford 
University Press, Oxford, 641-9.
Gilks, W. R., Best, N. G. and Tan, K. K. C. (1995) Adaptive rejection 
Metropolis sampling within Gibbs sampling. Applied Statistics, 44, 455-72.
Gilks, W. R., Richardson, S. and Spiegelhalter, D. J. (eds) (1996) Markov 
Chain Monte Carlo in Practice, Chapman & Hall, London.
Gilks, W. R. and Roberts, G. O. (1996) Strategies for improving MCMC. 
In Markov Chain Monte Carlo in Practice (eds W. R. Gilks, S. Richardson 
and D. J. Spiegelhalter), Chapman & Hall, London, 89-114.
Gilks, W. R., Roberts, G. O. and George, E. I. (1994) Adaptive direction 
sampling. The Statistician, 43, 179-89.
Gilks, W. R. and Wild, P. (1992) Adaptive rejection sampling for Gibbs 
sampling. Applied Statistics, 41, 337-48.
Gillespie, D. T. (1992) Markov Processes: An Introduction for Physical 
Scientists, Academic Press, San Diego.
Godsill, S. J. (2001) On the relationship between Markov chain Monte 
Carlo methods for model uncertainty. Journal of Computational and Graph­
ical Statistics, 10, 1-19.
Gordon, N. J., Salmond, D. J. and Smith, A. F. M. (1993) Novel approach 
to nonlinear/non-Gaussian Bayesian state estimation. IEE Proceedings F, 
140, 107-13.
Green, P. J. (1991) Discussion of the paper by Besag, York and Mollie. 
Annals of the Institute of Statistical Mathematics, 43, 22-4.
Green, P. J. (1995) Reversible jump Markov chain Monte Carlo compu­
tation and Bayesian model determination. Biometrika, 82, 711-32.
Green, P. J. (1996) MCMC in image analysis. In Markov Chain Monte 
Carlo in Practice (eds W. R. Gilks, S. Richardson and D. J. Spiegelhalter), 
Chapman & Hall, London, 381-99.
Green, P. J. (2003) Trans-dimensional Markov chain Monte Carlo. In 
Highly Structured Stochastic Systems (eds P. J. Green, N. L. Hjort and S. 
Richardson), Oxford University Press, Oxford, 179-98.
Green, P. J. and Han, X.-L. (1992) Metropolis methods, Gaussian pro­
posals and antithetic variables. In Lecture Notes in Statistics 74: Stochastic 
Models (eds A. Barone, A. Frigessi and M. Piccioni), Springer Verlag, 142­
64.

References
299
Green, P. J. and Mira, A. (2001) Delayed rejection in reversible jump 
Metropolis-Hastings. Biometrika, 88, 1035-53.
Grenander, U. and Miller, M. I. (1994) Representation of knowledge in 
complex systems (with discussion). Journal of the Royal Statistical Society, 
Series B, 56, 549-603.
Guttorp, P. (1995) Stochastic Modeling of Scientific Data, Chapman & 
Hall, London.
Hammersley, J. M. and Handscomb, D. C. (1964) Monte Carlo Methods, 
Methuen, London.
Han, C. and Carlin, B. P. (2001) Markov chain Monte Carlo methods for 
computing Bayes factors: a comparative review. Journal of the American 
Statistical Association, 96, 1122-32.
Harrison, P. J. and Stevens, C. F. (1976) Bayesian forecasting (with 
discussion). Journal of the Royal Statistical Society, Series B, 38, 205-47.
Hastings, W. K. (1970) Monte Carlo sampling methods using Markov 
chains and their applications. Biometrika, 57, 97-109.
Heidelberger, P. and Welch, P. D. (1983) Simulation run length control 
in the presence of an initial transient. Operations Research, 31, 1109-44.
Heyde, C. C. and Johnstone, I. M. (1979) On asymptotic posterior nor­
mality of stochastic processes. Journal of the Royal Statistical Society, Se­
ries B, 41, 184-9.
Higdon, D. M. (1998) Auxiliary variable methods for Markov chain Monte 
Carlo with applications. Journal of the American Statistical Association, 
93, 585-95.
Higdon, D. M. and Besag, J. (1999) Bayesian analysis of agricultural 
field experiments. Journal of the Royal Statistical Society, Series B, 61, 
691-746.
Hills, S. E. and Smith, A. F. M. (1992) Parametrization issues in Bayesian 
inference (with discussion). In Bayesian Statistics 4 (eds J. M. Bernardo 
et al.), Oxford University Press, Oxford, 227-46.
Hoel, P. G., Port, S. C. and Stone, C. J. (1972) Introduction to Stochastic 
Processes, Houghton Mifflin, Boston.
Huerta, G. and Lopes, H. F. (2000) Bayesian forecasting and inference 
in latent structure for the Brazilian industrial production index. Brazilian 
Review of Econometrics, 20, 1-26.
Huerta, G. and West, M. (1999a) Bayesian inference on periodicities 
and component spectral structure in time series. Journal of Time Series 
Analysis, 20, 401-16.
Huerta, G. and West, M. (1999b) Priors and component structures in 
autoregressive time series models. Journal of the Royal Statistical Society, 
Series B, 61, 881-99.
Jacquier, E., Poison, N. G. and Rossi, P. E. (1994) Bayesian analysis 
of stochastic volatility models (with discussion). Journal of Business and 
Economic Statistics, 12, 371-415.

300
References
Jarret, R. G. (1979) A note on the intervals between coal-mining disas­
ters. Biometrika, 66, 191-3.
Jeffreys, H. (1961) Theory of Probability, 3rd edn, Oxford University 
Press, Oxford.
Jennison, C. (1993) Discussion of the meeting on Gibbs sampling and 
other Markov chain Monte Carlo methods. Journal of the Royal Statistical 
Society, Series B, 55, 54-6.
Johnk, M. D. (1964) Erzeugung von Betaverteilter und Gammaverteilter 
Zufallszahlen. Metrika, 8, 5-15.
Johnson, V. E. (1996) Studying convergence of Markov chain Monte 
Carlo algorithms using coupled sample paths. Journal of the American 
Statistical Association, 91, 154-66.
Kass, R. E. and Raftery, A. E. (1995) Bayes factors. Journal of the Amer­
ican Statistical Association, 90, 773-95.
Kass, R. E. and Slate, E. H. (1992) Reparametrization and diagnostics 
of posterior nonnormality (with discussion). In Bayesian Statistics 4 (eds 
J. M. Bernardo et al.), Oxford University Press, Oxford, 289-305.
Kass, R. E. and Steffey, D. (1989) Approximate Bayesian inference in 
conditionally independent hierarchical models (parametric empirical Bayes 
models). Journal of the American Statistical Association, 84, 717-26.
Kass, R. E., Tierney, L. and Kadane, J. B. (1988) Asymptotic in Bayesian 
computation (with discussion). In Bayesian Statistics 3 (eds J. M. Bernardo 
et al.), Oxford University Press, Oxford, 261-78.
Kinderman, A. J. and Monahan, J. F. (1977) Computer generation of 
random variables using the ratio of random deviates. ACM  Transactions 
in Mathematical Software, 3, 257-60.
King, R. and Brooks, S. P. (2004) A classical study of catch-effort models 
for Hector’s dolphins. Journal of the American Statistical Association, 99, 
325-33.
Kirkpatrick, S., Gelatt, C. D., Jr. and Vecchi, M. P. (1983) Optimization 
by simulated annealing. Science, 220, 671-80.
Kitagawa, G. (1996) Monte Carlo filter and smoother for non-Gaussian 
nonlinear state space models. Journal of Computational and Graphical 
Statistics, 5, 1-25.
Kloek, J. and van Dijk, H. K. (1978) Bayesian estimates of equation 
system parameters: an application of integration by Monte Carlo. Econo- 
metrica, 46, 1-19.
Knorr-Held, L. (1997) Hierarchical Modelling of Discrete Longitudinal 
Data: Applications of Markov Chain Monte Carlo, Herbert Utz Verlag, 
Munich.
Knorr-Held, L. and Rue, H. (2002) On block updating in Markov random 
field models for disease mapping. Scandinavian Journal of Statistics, 29, 
597-614.

References
301
Laird, N. M. and Ware, J. H. (1982) Random effects models for longitu­
dinal data. Biometrics, 38, 963-74.
Laplace, P. S. (1986) Memoir on the probability of causes of events (trans­
lation by S. Stigler). Statistical Science, 1, 364-78.
Lehmer, D. H. (1951) Mathematical methods in large-scale computing 
units. In Proceedings of the Second Symposium on Large-Scale Digital Cal­
culating Machinery, Harvard University Press, Cambridge, 141-6.
Leonard, T. and Hsu, J. S. J. (1999) Bayesian Methods: An Analysis 
for Statisticians and Interdisciplinary Researchers, Cambridge University 
Press, Cambridge.
Lewis, P. A. W., Goodman, A. S. and Miller, J. M. (1969) A pseudo­
random number generator for the System 360. IBM Systems Journal, 8, 
136-45.
Lewis, S. and Raftery, A. E. (1997) Estimating Bayes factors via posterior 
simulation with the Laplace-Metropolis estimator. Journal of the American 
Statistical Association, 92, 648-55.
Liang, F. (2002) Dynamically weighted importance sampling in Monte 
Carlo computation. Journal of the American Statistical Association, 97, 
807-21.
Lindley, D. V. (1961) The use of prior probability distributions in statisti­
cal inference and decision. In Proceedings of the Fourth Berkeley Symposium 
on Mathematical Statistics and Probability, 1, 453-68.
Lindley, D. V. (1978) The Bayesian approach. Scandinavian Journal of 
Statistics, 5, 1-26.
Lindley, D. V. (1980) Approximate Bayesian methods (with discussion). 
In Bayesian Statistics (eds J. M. Bernardo et al.), Valencia University 
Press, Valencia, 223-45.
Lindley, D. V. and Smith, A. F. M. (1972) Bayes estimates for the linear 
model (with discussion). Journal of the Royal Statistical Society, Series B, 
34, 1-41.
Liu, C., Liu, J. S. and Rubin, D. R. (1992) A variational control variable 
for assessing the convergence of the Gibbs sampler. In Proceedings of the 
American Statistical Association, Statistical Computing Section, 74-8.
Liu, J. and West, M. (2001) Combined parameter and state estimation in 
simulation-based filtering. In Sequential Monte Carlo Methods in Practice 
(eds, A. Doucet, N. de Freitas and N. Gordon), Springer, New York, 197­
223.
Liu, J. S. and Chen, R. (1995) Blind deconvolution via sequential impu­
tations. Journal of the American Statistical Association, 90, 567-76.
Liu, J. S., Liang, F. and Wong, W. H. (2000) The use of multiple- 
try method and local optimization in Metropolis sampling. Journal of the 
American Statistical Association, 95, 121-34.
Liu, J. S., Wong, W. H. and Kong, A. (1994) Correlation structure and

302
References
convergence rate of the Gibbs sampler: applications to the comparison of 
estimators and augmentation schemes. Biometrika, 81, 27-40.
Lopes, H. F. (2000) Bayesian analysis in latent factor and longitudinal 
models, unpublished Ph.D. Thesis, Institute of Statistics and Decision Sci­
ences, Duke University, USA.
Lopes, H. F. (2003) Expected posterior priors in factor analysis. Brazilian 
Journal of Probability and Statistics, 17, 91-105.
Lopes, H. F. and Carvalho, C. M. (2005) Factor stochastic volatility with 
time varying loadings and Markov switching regimes. Technical Report, 
Graduate School of Business, University of Chicago.
Lopes, H. F. and Migon, H. S. (2002) Comovements and contagion in 
emergent markets: stock indexes volatilities. In Case Studies in Bayesian 
Statistics 6 (eds C. Gatsonis et al.), Springer-Verlag, New York, 285-300.
Lopes, H. F., Moreira, A. R. B. and Schmidt, A. M. (1999) Hyperparam­
eter estimation in forecasting models. Computational Statistics and Data 
Analysis, 29, 387-410.
Lopes, H. F., Muller, P. and Rosner, G. L. (2003) Bayesian meta-analysis 
for longitudinal data models using multivariate mixture priors. Biometrics,
59, 66-75.
Lopes, H. F. and Salazar, E. (2006a) Time series mean level and stochas­
tic volatility modeling by smooth transition autoregressions: a Bayesian ap­
proach, in Advances in Econometrics: Econometric Analysis of Financial 
and Economic Time Series/Part B, Volume 20 (eds D. Terrell and T. B. 
Fomby), Elsevier, Oxford, 229-42.
Lopes, H. F. and Salazar, E. (2006b) Bayesian model uncertainty in 
smooth transition autoregressions. Journal of Time Series Analysis, 27, 
99-117.
Lopes, H. F. and West, M. (2004) Bayesian model assessment in factor 
analysis. Statistica Sinica, 14, 41-67.
MacEachern, S. N. and Berliner, L. M. (1994) Subsampling the Gibbs 
sampler. The American Statistician, 48, 188-90.
Madigan, D. and York, J. (1995) Bayesian graphical models for discrete 
data. International Statistical Review, 63, 215-32.
Marinari, E. and Parisi, G. (1992) Simulated tempering: a new Monte 
Carlo scheme. Europhysics Letters, 19, 451-8.
Marsaglia, G. (1977) The squeeze method for generating gamma variates. 
Computers and Mathematics with Applications, 3, 321-5.
Marsaglia, G. and Bray, T. A. (1964). A conventional method for gener­
ating normal variables. SIAM Review, 6, 260-4.
McCullagh, P. and Nelder, J. A. (1988) Generalized Linear Models, 2nd 
edn, Chapman & Hall, New York.
McCulloch, R. E. and Rossi, P. E. (1991) A Bayesian approach to testing 
the arbitrage pricing theory. Journal of Econometrics, 49, 141-68.
Medhi, J. (1994) Stochastic Processes, 2nd edn, Wiley, New York.

References
303
Mendes, B. V. M. (1995) Bayesian inference using S-PLUS: the sampling- 
importance resampling technique. Technical Report, Statistical Laboratory, 
UFRJ.
Mendes, B. V. M. and Lopes, H. F. (2004) Data driven estimates for 
mixtures. Computational Statistics and Data Analysis, 47, 583-98.
Meng, X. L. and Schilling, S. (1996) Fitting full-information factor mod­
els and an empirical investigation of bridge sampling. Journal of the Amer­
ican Statistical Association, 91, 1254-67.
Meng, X. L. and Wong, W. H. (1996) Simulating ratios of normalizing 
constants via a simple identity: a theoretical exploration. Statistica Sinica,
6, 831-60.
Mengersen, K. L., Robert, C. P. and Cuihenneuc-Jouyaux, C. (1999) 
MCMC convergence diagnostics: a reviewww. In Bayesian Statistics 6 (eds 
J. M. Bernardo et al.), Oxford University Press, Oxford, 415-40.
Metropolis, N., Rosenbluth, A. W ., Rosenbluth, M. N., Teller, A. H. and 
Teller, E. (1953) Equation of state calculations by fast computing machine. 
Journal of Chemical Physics, 21, 1087-91.
Meyn, S. P. and Tweedie, R. L. (1993) Markov Chains and Stochastic 
Stability, Springer, New York.
Meyn, S. P. and Tweedie, R. L. (1994) Computable bounds for conver­
gence rates of Markov chains. Annals of Applied Probability, 4, 124-48.
Migon, H. S. and Gamerman, D. (1999). Statistical Inference: an Inte­
grated Approach, Arnold, London.
Migon, H. S., Gamerman, D., Lopes, H. F. and Ferreira, M. A. R. (2005) 
Dynamic Models. In Handbook of Statistics (eds D. Dey and C. R. Rao), 
volume 25, 553-88.
Migon, H. S. and Harrison, P. J. (1985) An application of non-linear 
Bayesian forecasting to television advertising. In Bayesian Statistics 2 (eds 
J. M. Bernardo et al.), North Holland, Amsterdam, 681-96.
Mira, A. and Nicholls, G. K. (2004) Bridge estimation of the probability 
density at a point. Statistica Sinica, 14, 603-12.
Mira, A. and Tierney, L. (2002) Efficiency and convergence properties of 
slice samplers. Scandinavian Journal of Statistics, 29, 1-12.
Mood, A. M., Graybill, F. A. and Boes, D. C. (1974) Introduction to the 
Theory of Statistics, 3rd edn, McGraw-Hill, Tokyo.
Muller, P. (1991a) Monte Carlo integration in general dynamic models. 
Contemporary Mathematics, 115, 145-63.
Muller, P. (1991b) Metropolis based posterior integration schemes. Tech­
nical Report, Statistics Department, Purdue University.
Naylor, J. C. and Smith, A. F. M. (1982) Application of a method for 
the efficient computation of posterior distributions. Applied Statistics, 31,
214-25.
Neal, R. M. (2001) Annealed importance sampling. Statistics and Com­
puting, 11, 125-39.

304
References
Neal, R. M. (2003) Slice sampling (with discussion). The Annals of Statis­
tics, 31, 705-67.
Newman, T. G. and Odell, P. L. (1971) The Generation of Random 
Variates, Griffin, London.
Newton, M. A. and Raftery, A. E. (1994) Approximate Bayesian inference 
by the weighted likelihood bootstrap (with discussion). Journal of the Royal 
Statistical Society, Series B, 56, 3-48.
Nobre, A. A., Schmidt, A. M. and Lopes, H. F. (2005) Spatio-temporal 
models for mapping the incidence of malaria in Para. Environmetrics, 16, 
291-304.
Nummelin, E. (1984) General Irreducible Markov Chains and Non­
negative Operators, Cambridge University Press, Cambridge.
Odell, P. L. and Feiveson, A. G. (1966) A numerical procedure to generate 
a sample covariance matrix. Journal of the American Statistical Associa­
tion, 61, 199-203.
Oh, M.-S. and Berger, J. O. (1992) Adaptive importance sampling in 
Monte Carlo integration. Journal of Statistical Computation and Simula­
tion, 41, 143-68.
Oh, M.-S. and Berger, J. O. (1993) Integration of multimodal functions 
by Monte Carlo importance sampling. Journal of the American Statistical 
Association, 88, 450-6.
O ’Hagan, A. (1987) Monte Carlo is fundamentally unsound. The Statis­
tician, 36, 247-9.
O ’Hagan, A. (1995) Fractional Bayes factors (with discussion). Journal 
of the Royal Statistical Society, Series B, 57, 99-138.
O ’Hagan, A. and Forster, J. (2004) Bayesian Inference, Volume 2B of 
Kendall’s Advanced Theory of Statistics, 2nd edn, Edward Arnold, London.
Peskun, P. H. (1973) Optimum Monte Carlo sampling using Markov 
chains. Biometrika, 60, 607-12.
Phillips, D. B. and Smith, A. F. M. (1993) Orthogonal random-direction 
sampling in Markov chain Monte Carlo. Technical Report 93-06, Statistics 
Section, Imperial College London.
Phillips, D. B. and Smith, A. F. M. (1996) Bayesian model comparison 
via jump diffusions. In Markov Chain Monte Carlo in Practice (eds W. R. 
Gilks, S. Richardson and D. J. Spiegelhalter), Chapman & Hall, London,
215-39.
Pitt, M. and Shephard, N. (1999) Filtering via simulation: auxiliary par­
ticle filters. Journal of the American Statistical Association, 94, 590-9.
Poison, N. G. (1996) Convergence of Markov chain Monte Carlo algo­
rithms (with discussion). In Bayesian Statistics 5 (eds J. M. Bernardo et 
al.), Oxford University Press, Oxford, 297-321.
Press, S. J. (2003) Subjective and Objective Bayesian Statistics: Princi­
ples, Models, and Applications, 2nd edn, Wiley, New York.

References
305
Priestley, M. B. (1981) Spectral Analysis and Time Series, Academic 
Press, London.
Propp, J. and Wilson, D. (1996) Exact sampling with coupled Markov 
chains and applications to statistical mechanics. Random Structures and 
Algorithms, 9, 223-52.
Quesenberry, C. P. and Kent, J. (1982) Selecting among probability dis­
tributions used in reliability. Technometrics, 24, 59-65.
Raftery, A. E. (1996) Hypothesis testing and model selection. In Markov 
Chain Monte Carlo in Practice (eds W. R. Gilks, S. Richardson and D. J. 
Spiegelhalter), Chapman & Hall, London, 165-87.
Raftery, A. E. and Lewis, S. (1992) How many iterations in the Gibbs 
sampler?. In Bayesian Statistics 4 (eds J. M. Bernardo et al.), Oxford 
University Press, Oxford, 763-73.
Raftery, A. E. and Lewis, S. (1996) Implementing MCMC. In Markov 
Chain Monte Carlo in Practice (eds W. R. Gilks, S. Richardson and D. J. 
Spiegelhalter), Chapman & Hall, London, 115-30.
Raftery, A. E., Madigan, D. and Hoeting, J. A. (1997) Bayesian model 
averaging for linear regression models. Journal of the American Statistical 
Association, 92, 179-91.
Rao, C. R. (1973) Linear Statistical Inference and Applications, New 
York, Wiley.
Ratkowski, D. A. (1983) Nonlinear Regression Modeling: a Unified Prac­
tical Approach, Marcel Dekker, New York.
Ravines, R. R. (2005) An efficient sampling scheme for generalized dy­
namic models with applications in transfer function models, unpublished 
Ph.D. Qualifying Exam, IM-UFRJ (in Portuguese).
Reis, E. A., Salazar, E. and Gamerman, D. (2006) Comparison of sam­
pling schemes for dynamic linear models. International Statistical Review 
(to appear).
Revuz, D. (1975) Markov Chains, North-Holland, Amsterdam.
Richardson, S. and Green, P. J. (1997) On Bayesian analysis of mixtures 
with an unknown number of components (with discussion). Journal of the 
Royal Statistical Society, Series B, 59, 731-92. Correction, 1998, p. 661.
Ripley, B. D. (1977) Modelling spatial patterns (with discussion). Journal 
of the Royal Statistical Society, Series B, 39, 172-212.
Ripley, B. D. (1987) Stochastic Simulation, Wiley, New York.
Ritter, C. and Tanner, M. A. (1992) Facilitating the Gibbs sampler: the 
Gibbs stopper and the griddy Gibbs sampler. Journal of the American 
Statistical Association, 87, 861-8.
Robert, C. P. (1995) Convergence control methods for Markov chain 
Monte Carlo algorithms. Statistical Science, 10, 231-53.
Robert, C. P. (2001) The Bayesian Choice: From Decision-Theoretic 
Foundations to Computational Implementation, 2nd edn, Springer Verlag, 
New York.

306
References
Roberts, G. O. (1992) Convergence diagnostics of the Gibbs sampler. In 
Bayesian Statistics 4 (eds J. M. Bernardo et al.), Oxford University Press, 
Oxford, 775-82.
Roberts, G. O. (1996) Markov chain concepts related to sampling algo­
rithms. In Markov Chain Monte Carlo in Practice (eds W. R. Gilks, S. 
Richardson and D. J. Spiegelhalter), Chapman & Hall, London, 45-57.
Roberts, G. 0 . and Poison, N. G. (1994) A note on the geometric conver­
gence of the Gibbs sampler. Journal of the Royal Statistical Society, Series 
B, 56, 377-84.
Roberts, G. O. and Rosenthal, J. S. (1999) Convergence of slice sampler 
Markov chains. Journal of the Royal Statistical Society, Series B, 61, 643­
60.
Roberts, G. O. and Sahu, S. K. (1997) Updating schemes, correlation 
structure, blocking and parametrization for the Gibbs sampler. Journal of 
the Royal Statistical Society, Series B, 59, 291-317.
Roberts, G. O. and Smith, A. F. M. (1994) Simple conditions for the 
convergence of the Gibbs sampler and Metropolis-Hastings algorithms. 
Stochastic Processes and their Applications, 49, 207-16.
Roberts, G. O. and Tweedie, R. L. (1994) Geometric convergence and 
central limit theorems, for multidimensional Hastings and Metropolis algo­
rithms. Biometrika, 83, 95-110.
Rosenthal, J. S. (1993) Rates of convergence for data augmentation on 
finite sample spaces. Annals of Applied Probability, 3, 819-39.
Ross, S. (1996) Stochastic Processes, 2nd edn, Wiley, New York.
Rubin, D. B. (1987) A noniterative sampling/importance resampling al­
ternative to the data augmentation algorithm for creating a few imputa­
tions when fractions of missing information are modest: the SIR algorithm, 
comment to a paper by Tanner and Wong. Journal of the American Sta­
tistical Association, 82, 543-6.
Rubin, D. B. (1988) Using the SIR algorithm to simulate posterior dis­
tributions (with discussion). In Bayesian Statistics 3 (eds J. M. Bernardo 
et al.), Oxford University Press, Oxford, 395-402.
Rubinstein, R. Y. (1981) Simulation and the Monte Carlo Method, Wiley, 
New York.
Rue, H. (2001). Fast sampling of Gaussian Markov random fields. Journal 
of the Royal Statistical Society, Series B, 63, 325-38.
Rue, H. and Held, L. (2005) Gaussian Markov Random Fields: Theory 
and Applications, CRC/Chapman k. Hall. Chapman & Hall/CRC, Boca 
Raton.
Rue, H. and Tjelmeland, H. (2004). Fitting Gaussian random fields to 
Gaussian fields. Scandinavian Journal of Statistics, 29, 31-49.
Schmeiser, B. (1982) Batch size effects in the analysis of simulation out­
put. Operation Research, 30, 556-68.
Schmeiser, B. and Chen, M. H. (1991) General hit-and-run Monte Carlo

References
307
sampling for evaluating multidimensional integrals. Operations Research 
Letters, 19, 161-9.
Schmidt, A. M., Gamerman, D. and Moreira, A. R. B. (1999) An adaptive 
resampling scheme for cycle estimation. Journal of Applied Statistics, 26, 
619-41.
Schruben, L. W ., Singh, H. and Tierney, L. (1983) Optimal tests for 
initialization bias in simulation output. Operations Research, 31, 1167-78.
Schwarz, G. (1978) Estimating the dimension of a model. Annals of 
Statistics, 6, 461-4.
Shaw, J. E. H. (1988) Aspects of numerical integration and summarisa­
tion (with discussion). In Bayesian Statistics 3 (eds J. M. Bernardo et al.), 
Oxford University Press, Oxford, 411-28.
Shephard, N. (1994) Partial non-Gaussian state space. Biometrika, 81, 
115-31.
Shephard, N. and Pitt, M. (1997) Likelihood analysis of non-Gaussian 
measurement time series. Biometrika, 84, 653-67.
Silva, R., Lopes, H. F. and Migon, H. S. (2006) The extended generalized 
inverse Gaussian distribution for log-linear and stochastic volatility models. 
Brazilian Journal of Probability and Statistics (to appear).
Silverman, B. W. (1986) Density Estimation for Statistics and Data 
Analysis, Chapman & Hall, London.
Singh, A. C. and Roberts, G. R. (1992) State space modelling of cross­
classified time series of counts. International Statistical Review, 60, 321-36.
Sisson, S. A. (2005) Transdimensional Markov chains: a decade of progress 
and future perspectives. Journal of the American Statistical Association, 
100, 1077-89.
Sisson, S. A. and Fan, Y. (2004) Towards automating model selection 
for a mark-recapture-recovery analysis. Technical Report, School of Math­
ematics, University of New South Wales.
Smith, A. F. M. (1984) Bayesian statistics. Present position and potential 
developments: some personal views (with discussion). Journal of the Royal 
Statistical Society, Series A, 147, 245-59.
Smith, A. F. M. and Gelfand, A. E. (1992) Bayesian statistics without 
tears: a sampling-resampling perspective. American Statistician, 46, 84-8.
Smith, A. F. M., Skene, A. M., Shaw, J. E. H. and Naylor, J. C. (1987) 
Progress with numerical and graphical methods for practical Bayesian statis­
tics. The Statistician, 36, 75-82.
Souza, A. D. P. (1999) Approximate Methods in Bayesian Dynamic 
Hierarchical Models, unpublished Ph.D. Thesis, COPPE-UFRJ (in Por­
tuguese).
Spiegelhalter, D. J., Best, N.G., Carlin, B.P., and van der Linde, A. 
(2002) Bayesian measures of model complexity and fit (with discussion and 
rejoinder). Journal of the Royal Statistical Society, Series B, 64, 583-639.
Spiegelhalter, D. J., Thomas, A., Best, N. G. and Gilks, W. R. (1995)

308
References
BUGS: Bayesian Inference Using Gibbs Sampling: Version 0.5. Technical 
Report, Biostatistics Unit-MRC, Cambridge.
Spiegelhalter, D. J., Thomas, A., Best, N. G. and Lunn, D. (2003) Win- 
BUGS User Manual: Version I.4.I. http://w w w .m rc-bsu.cam .ac.uk/bugs.
Stein, M. L. (1999) Statistical Interpolation of Spatial Data: Some Theory 
for Kriging, Springer, New York.
Stephens, D. A. and Smith, A. F. M. (1992) Sampling-resampling tech­
niques for the computation of posterior densities in normal mean problems. 
Test, 1, 1-18.
Stephens, M. (2000) Bayesian analysis of mixture models with an un­
known number of components -  an alternative to reversible jump methods. 
Annals of Statistics, 28, 40-74.
Stone, M. (1974) Cross-validatory choice and assessment of statistical 
predictions (with discussion). Journal of the Royal Statistical Society, Se­
ries B, 36, 111-47.
Storvik, G. (2002) Particle filters in state space models with the presence 
of unknown static parameters. In IEEE Trans, on Signal Processing, 50,
281-9.
Stroud, J. R., Poison, N. G. and Muller, P. (2004) Practical filtering for 
stochastic volatility models. In State Space and Unobserved Components 
Models (eds A. Harvey, S. J. Koopmans and N. Shephard), Oxford Univer­
sity Press, Oxford, 236-47.
Sturtz, S., Ligges, U. and Gelman, A. (2005) R2WinBUGS: A package 
for running WinBUGS from R. Journal of Statistical Software, 12, 1-16.
Swendsen, R. H. and Wang, J. S. (1987) Nonuniversal critical dynamics 
in Monte Carlo simulations. Physics Review Letters, 58, 86-8.
Tachibana, V. (1995) Approximate methods in Bayesian models of ran­
domized response and logistic regression, unpublished Ph.D. Thesis, COPPE- 
UFRJ (in Portuguese).
Tancredi, A., Anderson, C. W. and O ’Hagan, A. (2002) Accounting for 
threshold uncertainty in extreme value estimation. Technical Report, De­
partment of Probability and Statistics, University of Sheffield.
Tanner, M. A. (1996) Tools for Statistical Inference: Methods for the 
Exploration of Posterior Distributions and Likelihood Functions, 3rd edn, 
Springer Verlag, New York.
Tanner, M. A. and Wong, W. (1987) The calculation of posterior distri­
butions by data augmentation (with discussion). Journal of the American 
Statistical Association, 82, 528-50.
Thisted, R. A. (1988) Elements of Statistical Computing, Chapman and 
Hall, New York.
Tierney, L. (1994) Markov chains for exploring posterior distributions 
(with discussion). Annals of Statistics, 22, 1701-62.
Tierney, L. (1996) Introduction to general state-space Markov chain the-

References
309
ory. In Markov Chain Monte Carlo in Practice (eds W. R. Gilks, S. Richard­
son and D. J. Spiegelhalter), Chapman & Hall, London, 59-74.
Tierney, L. and Kadane, J. B. (1986) Accurate approximations for pos­
terior moments and marginal densities. Journal of the American Statistical 
Association, 81, 82-6.
Tierney, L., Kass, R. E. and Kadane, J. B. (1989) Fully exponential 
Laplace approximations for expectations and variances of nonpositive func­
tions. Journal of the American Statistical Association, 84, 710-6.
Tierney and Mira (1999) Some adaptive Monte Carlo methods for Bayesian 
inference. Statistics in Medicine, 18, 2507-15.
van der Linde, A. (2005) DIC in variable selection. Statistica Neerlandica, 
59, 45-56.
Verdinelli, I. and Wasserman, L. (1995) Computing Bayes factor using a 
generalization of the Savage-Dickey density ratio. Journal of the American 
Statistical Association, 90, 614-8.
Vines, S. K., Gilks, W. R. and Wild, P. (1996) Fitting Bayesian multiple 
random effects models. Statistics Computing, 6, 337-46.
Waagepetersen, R. and Sorensen, D. (2001) A tutorial on reversible jump 
MCMC with a view toward applications in QTL-mapping. International 
Statistical Review, 69, 49-61.
Wakefield, J. C., Gelfand, A. E. and Smith, A. F. M. (1991) Efficient 
computation of random variates via the ratio-of-uniforms method. Statistics 
and Computing, 1, 129-34.
West, M. (1985) Generalized linear models: outlier accommodation, scale 
parameters and prior distributions (with discussion). In Bayesian Statistics
2 (eds J. M. Bernardo et al.), North Holland, Amsterdam, 531-58.
West, M. (1992) Modelling with mixtures (with discussion). In Bayesian 
Statistics 4 (eds J. M. Bernardo et al.), Oxford University Press, Oxford, 
503-24.
West, M. (1996) Some statistical issues in paleoclimatology (with discus­
sion). In Bayesian Statistics 5 (eds J. M. Bernardo et al.), Oxford University 
Press, Oxford, 461-84.
West, M. and Harrison, P. J. (1997) Bayesian Forecasting and Dynamic 
Models, 2nd edn, Springer Verlag, New York.
West, M., Harrison, P. J. and Migon, H. S. (1985) Dynamic generalized 
linear models and Bayesian forecasting (with discussion). Journal of the 
American Statistical Association, 80, 73-83.
Wild, P. and Gilks, W. R. (1993) AS 287. Adaptive rejection sampling 
from log-concave density functions. Applied Statistics, 42, 701-9.
Zeger, S. L. and Karim, M. R. (1991) Generalized linear models with 
random effects: a Gibbs sampling approach. Journal of the American Sta­
tistical Association, 86, 79-86.
Zellner, A. and Min, C. K. (1995) Gibbs sampler convergence criteria. 
Journal of the American Statistical Association, 90, 921-7.

310
References
Zhu, L. and Carlin, B. (2000) Comparing hierarchical models for spatio- 
temporally misaligned data using the deviance information criterion. Statis­
tics in Medicine, 94, 1275-85.

Author index
Abanto, C. A. 106 
Abramowitz, M. 93 
Achcar, J. A. 92, 109 
Aguilar, O. 247 
Ahrens, J. H. 30, 38 
Aitchinson, J. 47 
Aitkin, M. 249 
Al-Awadhi, F. 247 
Albert, J. H. 62-3 
Anderson, C. W . 45 
Anderson, T. W . 58 
Andrews, D. F. 20 
Andrieu, C. 106, 280 
Arjas, E. 265, 267 
Assungao, J. J. 70 
Assungao, R. M. 70 
Avezedo, G. M. 251
Banerjee, S. 70, 72 
Barker, A. A. 191, 195, 234 
Barone, P. 205 
Bates, D. M. 102 
Behrens, C. 45
Bennett, J. E. 196, 199, 201, 208-9, 
215 
Berg, A. 254
Berger, J. O. 41, 46, 71, 73, 97, 249, 
285
Berliner, L. M. 151, 185 
Bernardo, J. M. 41, 46 
Besag, J. 68, 70, 87, 147, 185, 196-7, 
211, 233, 243, 279, 282-3 
Best, N. G. 34, 178, 181, 205, 253, 283 
Boes, D. C. 14 
Box, G. E. P. 15-6, 21, 35-6 
Bray, T. A. 19, 35 
Breslow, N. E. 221 
Brooks, S. P. 157,163-4, 270, 281
Capocaccia, D. 160 
Cappe, O. 102, 271 
Carlin, B. P. xiv, 70, 72, 81, 84, 97, 
143, 149, 154-7, 164, 173, 192, 209, 
216, 224, 245, 253-4, 257-261, 286 
Carter, C. K. 174, 225 
Carvalho, C. M. 106 
Casella, G. 122, 185 
Cassandro, M. 160 
Castelloe, J. M. 270 
Celeux, G. 254 
Chambers, J. M. 23 
Chan, K. S. 127 
Chang, H. 29, 38 
Chen, M.-H. 80, 243, 272-4, 288 
Chen, R. 126 
Cheng, R. C. H. 29, 38 
Chib, S. 192, 201, 205, 238, 243-5, 
248, 257-261, 286 
Clayton, D. G. 219, 221 
Clyde, M. A. 266 
Coles, S. G. 45 
Congdon, P. 70 
Cowles, M. K. 157, 164, 181 
Cox, D. R. 91 
Cressie, N. 71 
Crowder, M. J. 221-2 
Cuihenneuc-Jouyaux, C. 169
Dagpunar, J. 10
Damien, P. 283
De Finetti, B. 41
de Freitas, J. 106, 280
Deely, J. J. 62
DeGroot, M. H. 14, 20
Dellaportas, P. 33, 95, 221, 260, 267
Dempster, A. P. 253
Dennison, D. G. T. 267

312
Author index
de Oliveira, V. 71 
Devroye, L. 10 
Dey, D. 240-241, 249-252 
Dias, R. 267 
DiCiccio, T. J. 239, 245 
Dickey, J. 73, 79-80 
Dieter, U. 30, 38 
Diggle, P. J. 72, 234 
Doucet, A. 106, 281 
Draper, N. R. 286 
Dunsmore, I. R. 47
Eddy, W . 249 
Edwards, R. G. 282 
Efron, B. 43 
Ehlers, R. S. 270 
Elston, R. C. 179 
Embrechts, P. 42 
Evans, M. 90, 95, 97
Fahrmeir, L. 228 
F an ,Y . 281 
Feast, G. M. 29, 38 
Feiveson, A. G. 24-5, 38 
Feller, W . 96, 113 
Ferreira, M. A. R. 106, 175 
Fishman, G. 10 
Forbes, F. 254
Forster, J. J. 41, 81, 148, 151, 158, 
185-6, 260, 267 
Frenkel, D. 277 
Friel, N. 281, 285 
Frigessi, A. 205
Friihwirth-Schnatter, S. 174, 225 
Galves, A. 160
Gamerman, D. 41, 45, 48, 52, 61, 
69-70, 72, 79, 103, 106, 159-160, 
175-7, 179, 219, 223-5, 229, 233, 267 
Garren, S. 169 
Geisser, S. 47, 249 
Gelatt, C. D., Jr. 279 
Gelfand, A. E. 16, 36, 70, 72, 99, 103, 
136, 141-3, 149, 152, 154-5, 159, 
169-170, 209, 216, 240-1, 249-253,
275-6
Gelman, A. xiv, 81, 149, 166-7, 179, 
181, 188, 196, 199, 215, 243, 270 
Geman, D. 122, 141, 143, 150, 158 
Geman, S. 122, 141, 143, 150, 158 
George, E. I. 62, 77, 122, 185, 266, 273 
Geweke, J. 96-7, 134, 164-6, 181, 198, 
247
Geyer, C. J. 127, 134,149,266, 279-280 
Ghosh, S. K. 252-3 
Gilks, W . R. xiv-xv, 33-4, 155, 178, 
185, 196, 199, 205, 215, 273, 279,
283
Gillespie, D. T. 129 
Giudici, P. 270 
Godsill, S. 106, 258, 260, 267 
Goodm an, A. S. 9 
Gordon, N. J. 105-6 
Graybill, F. A. 14 
Green, P. J. 126, 147, 233, 257, 
261-262, 264-7, 270-1, 273-4, 279,
282-3, 287 
Greenberg, E. 192, 201, 205 
Grenander, U. 271 
Grizzle, J. E. 179 
Guillin, A. 102 
Guttorp, P. 113, 119, 127
Hammersley, J. M. 10 
Han, C. 254 
Han, X.-L. 126, 273 
Handscomb, D. C. 10 
Haxrison, P. J. 63-64, 68, 228-9 
Hastings, W . K. 191-2, 195-6, 205-6,
208, 
261 
Heidelberger, P. 165, 181 
Heikkinen, J. 265, 267 
Heyde, C. C. 84 
Higdon, D. 70, 283 
Hills, S. E. 92, 155, 170 
Hoel, P. G. 121 
Holmes, C.C. 267 
Hsu, J. S. J. 254 
Huerta, G. 267 
Hurn, M. 274
Jacquier, E. 201 
Jarret, R. G. 145

Author index
313
Jeffreys, H. 46 
Jeliazkov, I. 244-5 
Jennison, C. 274, 279 
Johnk, M. D. 38 
Johnson, V. E. 169 
Johnstone, I. M. 84
Kadane, J. B. 89-91
Karim, M. R. 150, 219
Kass, R.E. 62, 73, 89, 91-2, 238, 245
Kent, J. 254
Kim, H.-J. 72
Kinderman, A. J. 16
King, R. 281
Kirkpatrick, S. 279
Kitagawa, G. 105
Kloek, J. 96-7
Kluppelberg, C. 42
Knorr-Held, L. 177, 200, 227, 229
Kohn, R. 174, 225
Kong, A. 152, 156
Laird, N. M. 223 
Laplace, P. S. 82 
Lehmer, D. H. 9 
Leonard, T. 254 
Lewis, P. A. W . 9 
Lewis, S. 149, 161-3, 165, 181, 199, 
238, 273 
Liang, F. 103, 276-7 
Ligges, U. 179
Lindley, D. V. 43, 60, 62, 87-8, 108 
Liu, C. 169 
Liu, J. 106
Liu, J. S. 126, 152, 156, 169, 276-7 
Lopes, H. F. 45, 103, 106, 175, 180, 
245, 247-8, 254, 260-1, 267-8, 270,
284
Louis, T. xiv, 81, 84, 97, 156, 192 
Lunn, D. 178
MacEachern, S. N. 151, 185 
Madigan, D. 266-7 
Makov, U. E. 62, 77 
Mallick, B. 267 
Mallows, C. L. 20
Marin, J.-M. 102 
Marinari, E. 280 
Marsaglia, G. 19, 30, 35 
McCullagh, P. 52, 58, 86, 108 
M cCulloch, R. E. 239, 266 
Medhi, J. 129 
Mendes, B. V. M. 45, 103 
Mendoza, M. 46 
Meng, X. L. 242-3 
Mengersen, K. L. 169 
Metropolis, N. 128, 191-2, 199, 208 
Meyer, R. 254 
Meyn, S. P. 113, 129, 157 
Migon, H. S. 41, 48, 52, 61, 68, 106, 
175, 228-9, 284 
Mikosch, T. 42 
Miller, J. M. 9 
Miller, M. I. 271 
Min, C. K. 168, 188 
Mira, A. 245, 274, 284 
Moller, J. 266 
Mollie, A. 68, 233 
Monahan, J. F. 16 
M ood, A. M. 14 
Moore, L. 10
Moreira, A. R. B. 69, 79, 103, 177, 
224-5, 233 
Moyeed, R. A. 72, 234 
Muller, M. E. 15
Muller, P. 105-6, 198, 213, 251, 267
Naylor, J. C. 93, 95, 97 
Neal, R. M. 241-2, 284 
Nelder, J. A. 52, 58, 86, 108 
Newman, T. G. 10 
Newton, M. A. 100, 240 
Nicholls, G. K. 245 
Nobre, A. A. 180, 254 
Ntzoufras, I. 260 
Nummelin, E. 113, 124
Odell, P. L. 10, 24, 25, 38 
Oh, M.-S. 97
O ’Hagan, A. 41, 45, 81, 87, 96, 148, 
151, 158, 185-6, 249, 285 
Olivieri, E. 160

314
Author index
Parisi, G. 280 
Pericchi, L. R. 73, 249, 285 
Peskun, P. H. 191, 195 
Pettitt, A. N. 285 
Philippe, A. 270
Phillips, D. B. 264, 270-1, 273, 288 
Pitt, M. 105, 228-229 
Poison, N. G. 106, 126, 157, 173, 201,
224
Port, S. C. 121 
Press, S. J. 90, 99 
Priestley, M. B. 134 
Propp, J. 277
Quesenberry, C. P. 254
Racine-Poon, A. 170, 196, 199, 201, 
208-9, 215 
Raftery, A. E. 73, 100, 149, 161-3, 165, 
181, 199, 238-241, 245, 248, 266, 273 
Rao, C. R. 102 
Ratkowski, D. A. 251 
Ravines, R. 228 
Reid, N. 91 
Reis, E. A. 176, 225 
Revuz, D. 113 
Richardson, S. xiv, 267 
Ripley, B. D. 10-1, 30, 134, 205, 266 
Ritter, C. 156, 169, 205 
Robert, C. P. 41, 102, 142, 157, 169,
271
Roberts, G. O. 126, 148, 150, 154, 
156-157, 163-4, 169, 195-6, 199, 215, 
228, 254, 270, 273, 279, 283, 284 
Rosenbluth, A. W . 128, 191, 192, 199, 
208
Rosenbluth, M. N. 128, 191, 192, 199, 
208
Rosenthal, J. S. 157, 284 
Rosner, G.L. 267 
Ross, S. 113 
Rossi, P. E. 201, 239 
Rubin, D. B. xiv, 30-1, 81, 149, 166-7, 
169, 181, 188, 270 
Rubinstein, R. Y. 10 
Rue, H. 70, 79, 175-8, 233-4 
Ryden, T. 271
Sahu, S. K. 150, 154-6, 216
Salazar, E. 176, 225, 254, 268
Salmond, D. J. 105-6
Sanso, B. 71
Schilling, S. 243
Schmeiser, B. 134, 272-4, 288
Schmidt, A. M. 103, 180, 254
Schruben, L. W . 165
Schwarz, G. 162
Shao, Q.-M . 243
Shaw, J. E. H. 95, 97
Shephard, N. 105-6, 153, 174, 228-9
Silva, R. 284
Silverman, B. W . 99
Singh, A. C. 228
Singh, H. 165
Sinha, D. 251
Sirmans, C. F. 72
Sisson, S. A. 271, 278, 281
Skene, A. M. 95, 97
Slate, E. H. 92
Smit, B. 277
Smith, A. F. M. 16, 33, 36, 41, 43, 60, 
62, 77, 87, 92-93, 95, 97, 99, 103, 
105-6, 109, 136, 141-3, 148-9, 152, 
155, 159, 170, 179, 195, 221, 264, 
267, 270-1, 273, 288 
Smith, H. 286 
Smith, R. L. 169 
Sokal, A. D. 282 
Sorensen, D. 267, 271 
Souza, A. D. P. 159, 161 
Spiegelhalter, D. J. xiv, 178, 253 
Steffey, D. 62 
Stegun, I. A. 93 
Stein, M. L. 71 
Stephens, D. A. 103 
Stephens, M. 271 
Stern, H. S. xiv, 81, 149 
Stevens, C. F. 63 
Stoffer, D. S. 173, 224 
Stone, C. J. 121 
Stone, M. 249 
Storvik, G. 106 
Stroud, J.R. 106 
Sturtz, S. 179 
Swartz, T. 81, 95, 97

Author index
315
Swendsen, R. H. 283
Tachibana, V. 100, 103 
Tan, K. K. C. 34, 205, 283 
Tancredi, A. 45
Tanner, M. A. 102, 135-6, 156-7, 169, 
205, 210 
Tawn, J. A. 45, 72, 234 
Teller, A. H. 128, 191, 192, 199, 208 
Teller, E. 128, 191, 192, 199, 208 
Thisted, R. A. 86 
Thomas, A. 178 
Thompson, E. A. 280 
Tierney, L. 89-91, 126-7, 132, 148,
165, 198-9, 202, 204, 206-8, 274, 284 
Titterington, D.M. 254 
Tjelmeland, H. 178, 233 
Tweedie, R. L. 113, 129, 157
van Dijk, H. K. 96-7 
van der Linde, A. 253-4 
Vares, M. E. 160 
Vecchi, M. P. 279 
Verdinelli, I. 73, 80 
Vines, S. K. 155, 181, 185
Waagepetersen, R. 267, 271 
Wagenpfeil, S. 228
Wakefield, J. C. 16, 36, 196, 199, 201, 
208-9, 215, 283 
Walker, S. 284 
Wang, J. S. 283 
Ware, J. H. 223 
Wasserman, L. 73, 80, 245 
Watts, D.G. 102 
Welch, P. D. 165, 181 
West, M. 63-4, 68, 86, 99, 106, 200, 
228, 245-6, 248, 260-1, 267, 270 
W ild, P. 33 
W ilson, D. 277
W ong, W . H. 135-6, 152, 156, 242-3,
276-7 
Wright, D. 95
York, J. 68, 233, 267 
Yu, J. 254
Zeger, S. L. 150, 219 
Zellner, A. 168, 188 
Zhou, G. 247 
Zhu, L. 254 
Zimmerman, D.L. 270

Subject index
Page numbers representing figures are 
in b o ld ; those representing tables are 
in italics.
Acceptance
probability 28, 36-7, 192, 194-6,
198, 
200, 205, 207-8, 228-9, 234-5, 
261-268, 274, 276, 279, 283, 287-8 
rate 27, 192-3, 196-9, 201, 203, 
213-5, 219, 221, 225, 229, 233,
277
Adaptive rejection 32, 34, 62, 156,
178, 205, 219, 283-4 
envelopes 32-4, 34 
Adjusted
posterior 86, 220 
regression model 83, 220 
Advertising 229-31, 230 , 231 
Algorithms
data augmentation 135-36 
forward filtering backward sampling 
(FFBS), 174-5, 225 
hybrid 205-17 
iterative 87, 135-6 
Metropolis-Hastings 128-9, 191-236 
Population Monte Carlo 104 
reversible jum p 264, 270 
sequential Monte Carlo 105 
Aperiodicity 124-5, 132, 207 
Approximate normality 81, 249 
Approximations 
asymptotic 82-92 
Laplace 88-91 
exponential 90-2 
standard 88-9 
marginal densities 92 
normal 83-6 
target distribution 205
unbiased 98 
Asym ptotic approximations 82-92 
Autocorrelation time 126, 273 
A utoregressi ve 
chains 205 
conditional 69
logistic smooth transition 268 
models 80, 268, 270 
Auxiliary 
density 25 
variables 282-284
Bayes factors 72-4 
posterior 249 
pseudo 249 
Bayes’ theorem 28, 41-9, 52-4 
probability function 41-2 
rejection method 100-1, 101 
sequential nature 52 
weighted resampling 101-4 
Bayesian
central limit theorem 83-4 
inference, literature 41 
solutions 141 
Bernoulli distribution 11 
Binomial distribution 11, 57, 68 
conjugate 51 
negative 12 
Birth and death chains 123, 271 
reversible 128 
Birth and death processes 115, 266 
Bivariate techniques 14-20 
Blocking 155-6, 214-6
division of parameters 219 
Gibbs sampling 155-6 
Metropolis-Hastings sampling 214-6 
Bootstrap filter 106 
Box-Muller method 15

Subject index
317
Branching processes 115 
BUGS, software 178-84
Canonical parameter 52 
Cartesian product rules 94-95 
Cauchy distribution 20 
prior 49, 101 
Central limit theorem 4, 5, 15, 84, 96, 
125-7, 132, 151, 163, 188, 21 
Chain trajectory, Gibbs sampling 153, 
153 
Chains
dispersion 167 
Chapman-Kolmogorov equations 116, 
130
Cholesky decomposition 23, 25, 176 
CODA, software 181 
Componentwise transition 206-11 
Metropolis-Hastings 208 
Conditional distributions 
multivariate normal 22 
posterior 58-9 
Conditional predictive ordinates 
(C PO ) 251 
Congruential generator 9-10 
Conjugacy 55-8 
conditional 58-9 
Conjugate distributions 49-59, 53 
binomial 51 
exponential 51 
exponential family 51-4 
normal 51 
Poisson 51 
prior 50 
Continuous random quantities, 
generation o f 13-20 
Continuous state spaces 129-32 
Convergence
acceleration 271-84 
chain alteration 271-8 
reparametrization 271 
resampling 275 
diagnostics 157-69 
literature 169 
formal 164-9, 166 , 165 
Gibbs sampling 169 
prescription 161-4, 164
rate of 158, 259 
time 153 
tolerance 163 
Credibility intervals 83, 106, 145, 151, 
169, 255, 107 
Curse of dimensionality 95
Data augmentation 135-6 
algorithm 135-6 
Decomposition of state space 118-21 
Definitions
Gibbs sampling 142-8 
Metropolis-Hastings 193-7 
Poisson process 12 
stationary distributions 121 
stochastic processes 113 
Delayed rejection 274-5, 275 
Densities
full conditional 65, 66-7 
marginal 47, 152, 172 
non-log-concave 205 
posterior distribution 58 
power posterior 285 
triangular 19 
Detailed balance equation 127, 194, 
261 
Dirichlet
distribution 54 
process 251 
Discrete
mixtures 17-20 
probability distribution 10-13 
random quantities, generation of 
10-13
Dispersion, chains 167 
Distributions
binomial 11, 57, 68
Cauchy 20
conjugate 49-59, 53
Dirichlet 54
double exponential 20
equilibrium 121, 143, 191-2, 278-82
exponential 12
extreme value 57
full conditional 142, 147, 156-7, 170, 
172, 211, 259, 153 
Gamma 14, 16, 23-5, 30, 33

318
Subject index
generalized Pareto (G PD ) 42 
Geometric 12 
heavy tails 49 
invariant 121
limiting 117, 121, 125, 129, 147, 
157, 281 
logarithmic 17 
logistic 20, 57
marginal 17, 22, 46-8, 130, 153 
observational 43 
Poisson 12, 51, 68, 143 
posterior 43, 81, 133, 141, 193, 237 
full conditional 61, 176 
predictive 42-7 
prior 42-7
probability 1, 9-10, 22, 60, 178 
stable 20 
stationary 121-4 
Student’s t distribution 20 
multivariate 24 
Dynamic models 63-8 
Gibbs sampling 172-6 
inference 104-7 
linear 63
Metropolis-Hastings 223-6 
regression 63
Ease of sampling 173 
Effective sample size 105, 126, 145,
199, 203, 210, 213, 214, 226, 217,
225
Ehrenfest model 115, 123 
Envelopes
adaptive rejection 32-4, 34 
Equations 
evolution 63 
observation 63 
system 63 
Equilibrium distribution 121,143, 
191-2, 278-82 
heated 279-80 
Ergodic 124-5, 132 
averages 132, 160 
accuracy o f 134 
geometrically 126 
theorem 125, 149, 151-2, 167 
uniformly 126, 136, 158
verifying 132 
Estimators 246, 247-8
annealed importance sampling 242 
batching 134 
bridge 242 
candidate 243-5 
consistent 96 
Gibbs sampling 151-2 
harmonic mean 240 
generalized 240 
Laplace-Metropolis 238 
maximum likelihood 83 
Monte Carlo 95-9 
path 243
Rao Blackwellized density 152 
sampling based 238-41 
literature 243, 245 
unbiased 96 
Evolution 
equations 63
sequential inference 64-5, 105 
Exponential 
distribution 12
conjugate distributions 51 
family 51
conjugate distributions 51-4 
Laplace approximations 90-2 
Extensions
dynamic models 67-8
Factor model 247-8, 248 
Filter
particle 105-7, 107 
bootstrap 106 
Fisher information matrix 46, 83 
Fisher scoring 86 
Formal convergence 148 
Forward filtering backward sampling 
(FFBS), 174-5, 225 
Full conditional distributions, Gibbs 
sampling 156
Gamma distribution, random 
quantities 14, 29-30 
Gaussian quadrature 93-5 
rules 93 
Generalized

Subject index
319
inverse Gaussian 284 
linear models 57-8, 86 
Pareto distribution (G PD ) 42 
Generation procedure, Gibbs sampling 
148-9 
Generators
congruential 9-10 
random number 9-10 
Geometric distribution 12 
Geostatistics 71
Gibbs sampling 122-3, 141-89, 233 
applications 169-78 
blocking 155-6 
chain trajectory 153, 153 
choosing sample 151 
convergence 148, 169 
definition 142-8 
dynamic models 172-6 
estimators 151-2 
forming sample 148-50 
full conditional distributions 156 
generation procedure 148-9 
hierarchical models 169-72 
implementation 148-57 
independent 149 
Metropolis-Hastings 211-3 
properties 142-8 
reparametrization 152-5 
reversible 150 
structure 155 
Gompertz model 251, 268 
Griddy Gibbs sampler 156
Heavy tails distributions 49 
Hierarchical
models 60-3, 154-5, 219
conditionally independent 61-2 
extending 62 
generalized linear 62 
Gibbs sampling 169-72 
parametrization 154 
prior 265
regression on time 159 
Highest posterior density (H PD) 48 
Homogeneous chains 114 
Homoscedasticity 55 
Hybrid algorithms 205-17
Importance density 96-7, 239-41 
Independence chains
Metropolis-Hastings 199-204 
rejection method 204 
Inference, dynamic models 104-7 
Informal convergence monitors 159-60, 
135-7 
Information criteria 
Akaike (AIC ) 257, 282 
Bayesian (BIC) 162, 257 
deviance (DIC) 254, 257 
minimum posterior predictive 253, 
257
Initial density, choice of 32 
Intensity rate estimation 265-6 
Interval
credibility 48
highest posterior density (H PD) 48 
Irreducibility 24, 132, 147, 207, 264 
Irreducible chains 119-20 
Iterative algorithm 87, 135-6
Jeffreys prior 45-6 
Joint
density 14, 17, 20, 28 
distributions, multivariate normal 
distribution 22, 54 
probability function 53 
Jump
diffusion 271
Markov chains with 261-9
Kalman filter 65 
Kernels
choice of 133 
constructing 228 
cycle transition 206 
Kriging 71-2
Law of large numbers 4, 5, 96, 125 
Likelihood function 42-4 
Limiting
distribution 117, 121, 125, 129, 147, 
157, 281
results, and stationarity 131-2, 205 
theorems 124-7

320
Subject index
Linear predictor 57 
Linear transformations, multivariate 
normal distribution 20 
Literature
Bayesian inference 41 
Markov chains 113 
Log-concavity 33, 33 
Logistic regression 
dynamic 229 
Log-log transformations 57
Marginal densities 47, 152, 172 
approximation 91 
Marginal distributions 17, 22, 46-8, 
130, 153
multivariate normal 20-3 
Marginal posterior densities 135 
Markov 
chains
with reversible jum ps 261-9 
supermodels 257-60 
transition kernel 129-31, 261 
dependence 113 
random fields 69, 118, 142 
Gaussian 69 
Matrix, stochastic 116 
Maximum likelihood estimate 58, 83 
Means, prior independence 60 
Metastable chains 160 
Metropolis-Hastings 128-9, 191-236 
algorithms 128-9, 191-236 
applications 217-34 
blocking 214-6 
definition 193-7 
dynamic models 223-6 
Gibbs sampling 211-3 
independence chains 199-204 
properties 193-7 
random walk chains 198-9 
reparametrization 216-7 
reversible chains 193-7 
symmetric chains 198 
transition kernel 192-4 
Mixed models 217-23 
Mixing element 17 
Mode calculation 86-8
Models
adequacy 237-56 
autoregressive 80, 268, 270 
choice 256-271 
comparing 258 
dynamic 63-8
evaluation 237-56, 248, 252, 257, 
269
exchangeable regression 170-1 
factor 247-8, 248 
first order 63
generalized inverse Gaussian 284 
generalized linear 57-8, 86 
mixed 217-23 
Gompertz 251, 251, 268 
hierarchical 60-3, 154-5, 219 
highly dimensional 273 
indexing 256 
linear growth 64 
logistic 251, 251 
logistic sm ooth transition 
autoregressive 268 
mixed 217-23 
one-way classification 154 
random effects 154-5, 169-70 
regression 55-8 
spatial 68-72, 176-8, 231-4 
super 257-61 
M onte Carlo integration 95-8 
Multimodality, posterior 85, 97, 167 
Multiple chains 166-8 
Multiple try 276-7, 277 
Multivariate
normal distribution 20-3 
conditional distributions 22 
joint distributions 22 
linear transformations 22 
marginal distributions 20-3 
quadratic forms 22 
Student’s t distribution 24-5
Negative binomial distribution 12 
Neighborhood 69 
Newton-Raphson algorithm 86-7 
Non-informative prior distributions 
44-5, 84, 154, 221

Subject index
321
Normal
densities 15, 22
scale mixture 19-20 
distribution 15 
conjugate 51 
standard 15 
generation 15 
mean 56-7
Observation equations 63
Parallel chains 150, 159-60, 270, 275, 
279, 232 
Parametric space, dimensions 58-9 
Particle 114-5, 141, 191 
filters 105-7, 107 
Periodicity 124-5, 132, 207 
Pharmacology 192 
Poisson
distribution 12
conjugate distributions 51 
random quantities 12-3 
observational model 52-3 
process, definition 12 
series of counts 68 
Population Monte Carlo 104 
Posterior
approximation 98 
contours 153, 153 
correlation 153 
density 48, 48, 85 
highest (H PD) 48 
distributions 42-7 
conditional 58-9 
density 58 
marginal 66 
multimodality 85, 97, 167 
Power 285 
variance 153 
Power posterior 285 
Prediction, sequential inference 64-5 
Predictive
density 237, 249, 256 
distributions 42-7 
likelihood
estimating 237-47, 247, 246, 248 
uses o f 248-53
Pretesting 30, 33 
Prior
conditional 59 
density 44
Dirichlet process 251 
distributions 42-7 
Jeffreys 45-6 
non-informative 45 
marginal 59 
pseudo 258-61 
Probability
acceptance 28, 36-7, 192, 194-6,
198, 200, 205, 207-8, 228-9, 234-5, 
261-268, 274, 276, 279, 283, 287-8 
distributions 1, 9-10, 22, 60, 178 
integral transform 13 
intervals 48 
Properties
Gibbs sampling 142-8 
Metropolis-Hastings 193-7 
Proposal transition 195, 198, 204, 
261-2, 267 
Pseudo
Bayes factors 249 
prior 258-61
Quadratic forms, multivariate normal 
distribution 22 
Quadrature 93-5 
rules 93
R, software 15, 178-9, 181 
Random effects 223 
model 154-5 
Random fields 
Gaussian 69 
Gaussian Markov 69 
Markov 118 
Random number generator 9-10 
Random quantities
exponential distribution 13 
Gamma distribution 14 
Poisson distribution 12-3 
Weibull distribution 13 
Random vectors, generation of 20-5 
Random walk 114-5 
chains 198-9, 205, 215

322
Subject index
Metropolis-Hastings 198-9 
Rate o f convergence 158 
Ratio statistics 169 
Ratio-of-uniform m ethod 16-7, 26, 29,
209, 
215 
Recurrence 119, 132 
Harris 132 
null 119
positive 119, 132 
Recurrent state 118-20 
Regression
dynamic logistic 68, 229 
models 55-8
on time, hierarchical 159 
Rejection method
adaptive 178, 205, 219, 283-4 
Bayes theorem 100-1, 101 
sampling 219 
Reparametrization 91, 99, 216-7 
dynamic models 152-5 
convergence acceleration 271 
Gibbs sampling 152-5 
Resampling 25-34
convergence acceleration 275 
rejection m ethod 25-30 
sam pling/im portance SIR 31 
weighted 30-2, 106 
Reversibility 127-8, 132, 261, 263 
Reversible chains 127-9 
Metropolis-Hastings 193-5 
Reversible jum p 264, 270 
Rules
Cartesian product 94-5 
Gaussian 93 
quadrature 93, 95 
Simpson 93
Sample impoverishment 105 
Sample size 31, 81-3
effective 105, 126, 145, 199, 203, 
210, 213, 214, 226, 217, 225 
Sampling 
block 174-5 
bridge 242-3 
directions 273 
ease o f 173 
from conditionals 135
path 242-3 
rejection 25-30, 219 
slice 284 
Sam pling/im portance resampling 
(SIR) 31 
Scanning 150-1 
Search schemes 11 
Sequential inference 64-5 
Sequential Monte Carlo 105 
evolution 64 
prediction 64 
updating 64-5 
Simpson rule 93 
Simulated tempering 280, 282 
Smoothing 105-6 
Software 178-84 
BUGS 178-84 
CODA 181 
R 15, 178-9, 181 
Spatial
models 68-72, 176-8, 231-4 
interpolation 71-2 
structure 168 
Spatially distributed data 68 
Squeezing 30, 33 
States, classifying 131 
Stationarity, and limiting results 131-2 
Stationary distributions 121-4 
definition 121 
Stochastic processes, definition 113 
Stochastic simulation 93-106 
Stochastic volatility 106, 201, 254 
Structure
Gibbs sampling 155 
spatial 168 
Substitution sampling 135-6 
Supermodels 257-61 
Markov chains 258-60 
Symmetric chains,
Metropolis-Hastings 198 
System equations 63
Target distribution, approximation 
205
Tim e series analysis 164-6 
Transformations, density o f 99 
Transient chains 118-21, 123-4

Subject index
323
Transition
componentwise 206-11 
kernel 129-31, 142, 147 
Metropolis-Hastings 193-5 
matrices 116, 119, 129 
probabilities 114-8, 122, 127 
proposal 262
Truncated distributions 29
Uniform random quantities 10
Updating, sequential inference 64-5
Variance function 52
Weibull distribution, random 
quantities 13
Weighted resampling, Bayes theorem 
101-4
Wishart distribution 23-4

