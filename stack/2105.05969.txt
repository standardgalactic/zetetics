SPUX Framework: a Scalable Package for Bayesian
Uncertainty Quantiﬁcation and Propagation
Jonas Šukys and Marco Bacci
May 14, 2021
We present SPUX - a modular framework for Bayesian inference enabling uncertainty
quantiﬁcation and propagation in linear and nonlinear, deterministic and stochastic mod-
els, and supporting Bayesian model selection. SPUX can be coupled to any serial or
parallel application written in any programming language, (e.g. including Python, R, Ju-
lia, C/C++, Fortran, Java, or a binary executable), scales eﬀortlessly from serial runs on
a personal computer to parallel high performance computing clusters, and aims to provide
a platform particularly suited to support and foster reproducibility in computational sci-
ence. We illustrate SPUX capabilities for a simple yet representative random walk model,
describe how to couple diﬀerent types of user applications, and showcase several readily
available examples from environmental sciences. In addition to available state-of-the-art
numerical inference algorithms including EMCEE, PMCMC (PF) and SABC, the open
source nature of the SPUX framework and the explicit description of the hierarchical par-
allel SPUX executors should also greatly simplify the implementation and usage of other
inference and optimization techniques.
1. Introduction
For centuries, human intuition and curiosity toward natural phenomena have been the main driving
forces behind the discovery of the fundamental laws of physics, and behind the formulations of
mathematical models capable of describing past and forecasting future behavior of various complex
systems. In evironmental sciences, in particular, there are strong justiﬁcations, and thus a strong
trend, for using stochastic models, such as stochastic diﬀerential equations (SDEs) and individual
based models (IBMs), to simulate the dynamical systems of interest.
Indeed, these models are
especially useful when intrinsic uncertainties are present as in, for instance, the modeling of inter-
connected systems such as climate, weather, ocean and lake dynamics, subsurface ground water ﬂows,
hydrological catchments, urban ﬂoods, and ecological communities, to name a few.
In recent years, as reviewed in [GDB+19], two additional important inﬂuencing factors have arisen
and have become available to scientists: considerable computational power and a massive increase
in data availability. The steady increase in computational power has allowed models to reduce their
level of approximation to reality by increasing complexity and/or by achieving faster convergence
towards the exact solution. Concurrently, the recent technological advances in sensing and imaging
have initiated the so-called era of big data, allowing one to complement mechanistic modeling based
on ﬁrst principles (e.g. conservation laws) and human ingenuity with observational data [KBBP16].
However, the opportunity to use high performance computing (HPC) infrastructures to enable an
eﬃcient coupling of complex models and/or of large data-sets, is posing signiﬁcant challenges to the
so-called scientiﬁc programming and computing practices. Indeed, nowadays users are often required
to run their forward simulations on HPC clusters, while developers need to be able to exploit diﬀerent
types of parallelism to ensure the feasibility of model calibration and uncertainty propagation. It is
also worth noting, that while for some complex models already a single forward simulation can be
computationally expensive, the statistical inference methodologies for assimilating datasets can be
extremely demanding already for models of intermediate complexity.
1
arXiv:2105.05969v1  [stat.CO]  12 May 2021

By building on these necessities, the focus of our contribution is on describing a new software
framework, called SPUX, which stands for "Scalable Package for Uncertainty Quantiﬁcation in X",
that aims to abstract and simplify the access to modern computing infrastructure for reproducible
uncertainty quantiﬁcation and propagation. The remainder of this section is dedicated to exposing
basic information about Bayesian inference, which is at the core of SPUX (a more detailed exposure
is provided in section 2), and to brieﬂy reviewing similar existing computational suites.
To advance the scientiﬁc understanding of complex systems, statistical inference techniques such
as Bayesian inference [GCS+14] can be used for probabilistic quantiﬁcation (i.e. including uncertain-
ties) of model parameters and (past, present and future) model states, and for comparing several
available models using Bayesian model selection. Bayesian inference conditions the prior distributions
of model parameters and (stochastic model) states (which probabilistically describe any prior infor-
mation regarding model parameter and output values) on the data to get the corresponding posterior
distributions. For instance, it can be based on the so-called likelihood function for a given model,
which formulates the model as a probability distribution of observations for given model parameter
values (to which model’s inputs and outputs are associated), and on the prior distribution of model
parameter values. Posterior probability distribution of model parameters can then be inferred from
combining such prior knowledge about the model and its parameters with the likelihood function (see
Bayes theorem and section 2 for more details).
Historically, the successful application of Bayesian inference for stochastic generative models with
realistic datasets has been hindered by the lack of eﬃcient sampling techniques for posterior model
trajectories and for the computationally expensive evaluation of the likelihood (as a high-dimensional
integration).
The development of methodologies to address such challenges has been an active
research topic in recent years. Relevant methods include Particle Filter (PF) estimation (with op-
tional trajectory "smoothing" techniques [DJ09]) coupled with Markov Chain Monte Carlo (MCMC)
sampling - also known as Particle Markov Chain Monte Carlo (PMCMC) [ADH10], Gibbs sampling
- including Conditional Ornstein-Uhlenbeck Sampling (COUS) [RM09], the Approximate Bayesian
Computation (ABC) methodologies such as Simulated Annealing ABC (SABC) [AKS15], Hamilto-
nian Monte Carlo [AUS16], and stochastic variational (Bayesian) inference (SVI) methods [HBWP13].
In recent years, the number of computational inference frameworks implementing the above-
mentioned methodologies to study not only deterministic but also stochastic models has been growing
considerably. An attempt to provide a thorough review of those suites would certainly fall short, unless
carried out as a dedicated review, and is therefore beyond the scope of this contribution. Instead, we
brieﬂy mention the existing approaches that we are aware of to provide a context and to highlight their
speciﬁcities, referring the reader to the individual articles. In particular, the ﬁrst table overviews UQ
suites targeted at "static" stochastic models, where uncertainty is speciﬁed by means of hierarchical
Bayesian networks, or incorporated into boundary (and initial) conditions or forcing terms:
Name
Language
Type
Type
Methodologies
Reference
BUGS
R/SAS
partial
specialized
Gibbs sampler
[LSTB09]
JAGS
Python/R
partial
specialized
Gibbs sampler
[Plu04]
MUQ
C++
partial
framework
optimization and UQ
[PCDM14]
Pest
proprietary
partial
program
optimization and UQ
[DMRT14]
STAN
Python
partial
framework
MCMC and HMC
[CGH+17]
emcee
Python
partial
specialized
EMCEE sampler
[FMHLG13]
PyMC3
Python
partial
framework
optimization and UQ
[SWF16]
UQpy
Python
partial
framework
optimization and UQ
[uqp20]
SPOTPY
Python
parallel
framework
optimization and UQ
[HKCCB15]
P4U
C/C++
parallel
framework
optimization and UQ
[HAPK15]
Dakota
C++
parallel
framework
optimization and UQ
[ABD+09]
Korali
Python/C++
parallel
framework
optimization and UQ
[WME+20]
The second table overviews UQ suites additionally supporting more complex "dynamic" stochastic
models (e.g. driven by SDEs and/or SPDEs):
2

Name
Language
Type
Type
Methodologies
Reference
PPF
Java
partial
specialized
Particle Filtering (PF)
[DSN+14]
timedeppar
R
partial
specialized
COUS
[RM09]
EasyABC
R
partial
specialized
ABC methods including SABC
[eas20]
pyro
Python
partial
framework
deep learning, UQ, SVI
[BCJ+18]
ABCpy
Python
parallel
specialized
ABC-type samplers
[DSU+17]
LibBi
C++
parallel
specialized
Particle Filtering (PF)
[Mur13]
PDAF
Fortran
parallel
specialized
Kalman/Particle Filtering (K/PF)
[NHS05]
SPUX
Python
parallel
framework
EMCEE, PF, PMCMC, SABC
[current]
In both tables, "partial" indicates only partial parallelization - either only the UQ algorithms (usually
only the outer loops over independent sampling tasks) are parallelized (manually or natively) or only
parallel external user applications are supported as models, in addition to standard serial models.
Full support (at the time of writing of this manuscript) for hierarchical parallelization of all, possibly
nested, algorithms (sampler, aggregator for multiple datasets, marginal likelihood estimator, model)
is indicated by plain "parallel".
As evinced by the large list of available frameworks, Bayesian inference is a ﬁeld that evolves fast,
especially when considering stochastic models.
Despite the strong commonalities in the naming,
the target applications of most suites fall into several insigniﬁcantly overlapping problem classes,
hindering the possibility of a direct comparison.
All of the established uncertainty quantiﬁcation
suites listed above provide users access to very sophisticated methodologies and are of great value
to the scientiﬁc community; however, all of them also have one or several shortcomings. Possible
deﬃciencies include no support for "dynamic" stochastic models or parallel models, limited choice of
inference algorithms (for instance, either only Markov- or ABC-type), and user interfaces based on a
rather technical programming language (C/C++/Fortran). Unfortunately, among all suites reviewed
here and actually supporting full parallelism and "dynamic" stochastic models, none implement both
classes of inference algorithms (MCMC-type and ABC-type) and, at the same time, provide an
interface in high-level programming language (e.g. Python) for both programming use cases: coupling
a user’s model and implementing a new inference algorithm. In addition, since plain embarrassingly
parallel Bayesian inference algorithms are being outcompeted by more eﬃcient and adaptive, but also
more communication intensive methodologies, modern computing inference suites are required to
evolve to incorporate and oﬀer these new functionalities despite their higher algorithmic complexity.
In contrast to the well-established suites for "static" stochastic models, a niche for UQ suites with a
focus on "dynamic" stochastic models, easy user interface, support for diﬀerent classes of inference
methodologies, and full parallelization was so far relatively empty. In other words, the above overview
provides motivation and sets speciﬁc goals for any new uncertainty quantiﬁcation framework.
With the SPUX framework we aim to oﬀer a framework that is open to a large class of model struc-
tures, and does not pose any limitations to the programming language. For instance, models can be
serial or parallel, and can be written in any language (e.g. Python, R, Julia, C/C++, Fortran, Java),
or can be available just as a binary executable. We choose Python as programming language for
SPUX as it is simple, popular, ﬂexible, and yet suited to exploit modern HPC architectures by means
of, for example, the mpi4py package [DPKC11]. More speciﬁcally, our framework mitigates high
computational costs by adaptively distributing model evaluations (for diﬀerent parameters, data-sets,
and stochastic trajectories) over multiple computational units in a parallel compute environment. It
does so according to a multilevel parallel programming approach, which allows SPUX to overcome
the standard paradigm of map-reduce workﬂows in favor of a more ﬂexible design tailored towards
algorithmic eﬃciency, as it is suggested in a recent review [LLM19]. Indeed, the ﬂexible parallelization
paradigm used in SPUX is based on a continuous management of multiple parallel workers, while their
internal states are maintained remotely to signiﬁcantly improve the eﬃciency of complex algorithms,
such as the Particle Filtering method.
SPUX can scale eﬀortlessly from laptops to large parallel
compute clusters, where it is particularly suited. SPUX already natively supports multiple inference
approaches, namely, Aﬃne Invariant Markov chain Monte Carlo Ensemble with or without Particle
Filtering (with memory-eﬃcient "rejection" particle smoothing [JMR15]), Simulated Annealing ABC,
3

and also standard Metropolis-Hastings MCMC. To the best of our knowledge, SPUX might be the
ﬁrst uncertainty quantiﬁcation framework that gathers all these capabilities in a single implemen-
tation. Finally, we want to mention that the open source nature of the SPUX framework and the
explicit description of the hierarchical parallel SPUX executors should allow to greatly simplify any ad-
ditional implementation and usage of other (existing or future) numerical inference and optimization
algorithms for deployment on parallel clusters.
The scope of the manuscript is to present the most recent version (1.0) of SPUX – a prototype of
which was already introduced in the earlier publication [ŠK17]. Theory and numerical methods for
Bayesian inference are brieﬂy reviewed in section 2. The purpose, design speciﬁcation, and available
modular components of SPUX are introduced in section 3, while section 4 showcases the framework
for a simple example model. A detailed overview of the most common use case – coupling of a
scientiﬁc application to the SPUX framework, including a novel proposed adaptive sampling strategy,
is provided in section 5. Finally, section 6 describes the design of the SPUX framework based on the
parallel SPUX executors, and the outlook to future developments is provided in section 7.
2. Mathematical concepts and numerical algorithms
In this section, we start with a brief review of the mathematical concepts for the underlying scientiﬁc
problem addressed by our framework. In particular, we introduce generic and hidden Markov models
and Bayesian inference for them, followed by several widely used numerical techniques, and a brief
summary for subsequent uncertainty propagation (forecasting) of future model predictions.
2.1. Generic and hidden-Markov (state-space) models
Within the scope of this uncertainty quantiﬁcation framework, we will consider two classes of predictive
models: a wide class of "generic" models and a specialized class of hidden-Markov models.
state 𝑚𝑠𝑁~ 𝑀𝑠𝑁(𝜃)
state 𝑚𝑡0 ∼𝑀𝑡0(𝜃)
state 𝑚𝑠1~ 𝑀𝑠1(𝜃)
𝑡= 𝑠1
initial 𝑀𝑡0
parameters 𝜃
output 𝑦𝑠1 = ℎ(𝑚𝑠1, 𝜃)
observation 𝑜𝑠1~ 𝒪(⋅|𝑦𝑠1, 𝜃)
𝑡= 𝑠𝑁
output 𝑦𝑠𝑁= ℎ(𝑚𝑠𝑁, 𝜃)
observation 𝑜𝑠𝑁~ 𝒪(⋅|𝑦𝑠𝑁, 𝜃)
state 𝑚~ 𝑀(𝜃)
parameters 𝜃
output 𝑦= ℎ(𝑚, 𝜃)
observation 𝑜~ 𝒪(⋅|𝑦, 𝜃)
Figure 1: Scheme of a generic model (left) and a hidden-Markov model (right), mapping parameters
θ to the state m, output y and observation o of the model M.
In a generic model M, a set of model parameters within a vector θ is mapped to the model
"prediction", as depicted in the left part of Figure 1. We further categorize such model prediction into
the full model "state" m = M(θ), and its hidden part (deﬁned by function h, for instance, extracting
only surface values from a three-dimensional lake model or accummulating only the number of adult
individuals in an ecological community) as model "output" y = h(m, θ). If the model M is stochastic
(e.g. driven by a stochastic process and hence attaining a non-unique state m) then, given a suitable
probability measure (denoted by P), its state is characterized by a probability distribution m ∼M(θ),
as a shorthand notation for state m having a probability P(m|θ, M). The ensemble of all possible
"predictions" of such stochastic models are sometimes also referred to as "trajectories".
Many realistic models have an explicit temporal dimension (denoted in this manuscript by time t)
as depicted in the right part of Figure 1. In such case, for each time t, we denote the model by Mt,
a speciﬁc model state by mt, and the model output as yt = h(mt, θ). A stochastic time-dependent
model M is called a hidden-Markov model, if, for any increasing sequence of times s1 < · · · < sN,
4

its corresponding states mt ∼Mt(θ) (but not necessarily its outputs yt) satisfy the Markov property
for all θ and all 1 ≤k ≤N:
P(msk|msk−1, . . . , ms1, θ, M) = P(msk|msk−1, θ, M).
(1)
In a realistic scenario (to be modeled by model M), the value of the output y, which we could
refert to as the "exact" or "true" output, is often not measured completely accurately during the
observation process (independentent of a chosen model type). In particular, the corresponding data
"observations" o (see Figure 1) are instead assumed to follow a probabilistic distribution O(·|y, θ),
sometimes referred to as the observational error model, potentially also depending on some uncertain
parameters, included within the same vector θ for simplicity and brevity of the exposition.
For time-dependent models, the corresponding data observations ot at "snapshots" t = s1, . . . , s ¯
N
are assumed to be mutually independent and each follow a given O(·|ysn, θ) distribution.
Con-
sequently, the entire observation sequence (os1, . . . , osN ) follows a tensorized distribution with a
shorthand notation O(·|y, θ) = ⊗N
n=1O(·|ysn, θ). In the following we refer to the observational error
model simply by "error".
Finally, hierarchical Bayesian networks (as examples of "static" probabilistic models introduced in
section 1) are supported as distributions (see subsection 5.6) for observational error and priors of
model parameters and intial model states (see subsection 2.2), are hence are not incorporated among
the "model" concept whithin manuscript.
2.2. Bayesian inference
Bayesian inference [GCS+14] can be used for statistical quantiﬁcation (including uncertainties) of
model parameters and (past, present and future) model states by conditioning the corresponding
prior distributions on the data to get the corresponding posterior distributions. In particular, for a
given model M mapping the parameters vector θ to (possibly probabilistic) model state m ∼M(θ),
the so-called likelihood L(D|θ, M) = P(D|θ, M) of the model M deﬁnes a probability distribution of
observations D for given model parameter values θ. In addition to the likelihood, initial information
about parameters θ is described probabilistically by the so-called prior distribution π(θ|M) = P(θ|M).
This prior knowledge on the model and its parameters is combined with the observed data D via the
likelihood L(D|θ, M) to obtain the posterior distribution P(θ|D, M) of model parameters θ:
P(θ|D, M) = L(D|θ, M)π(θ|M)
P(D|M)
∝L(D|θ, M)π(θ|M),
(2)
where the Bayesian model evidence term P(D|M), useful for model selection, is independent of the
parameters θ.
For a deterministic model M, model output y = h(m, θ) = h(M(θ), θ) can be obtained for
any arbitrary θ, and hence the likelihood can be evaluated explicitly by applying the error, i.e.
L(D|θ, M) = O(D|y, θ).
In addition to the posterior distribution P(θ|D, M) of model parame-
ters θ given by (2), the posterior distribution P(m|D, M) of model states m is given directly by
propagating P(θ|D, M) through model M using the procedures described in later sections.
For a stochastic model M, a conditional (on θ) prior distribution π(m|θ, M) of the model states
m is also required (for simplicity, the same notation π is used for the prior distributions of model
parameters θ and of model states m).
For instance, in a time-dependent stochastic model with
state mt ∼Mt(θ), a prior distribution Mt0(θ) of the initial model state mt0 needs to be speciﬁed
(possibly conditional on θ), and then the prior distribution of the later model states mt at t > t0
is determined by propagating Mt0(θ) through model M.
Given π(m|θ, M), the conditional (on
θ) posterior distribution P(m|θ, D, M) of the (stochastic) model state m can be inferred jointly
with the posterior distribution P(θ|D, M) of model parameters θ by evaluating their joint posterior
5

P(θ, m|D, M):
P(θ, m|D, M) = P(D|θ, m, M)π(θ, m|M)
P(D|M)
∝P(D|θ, m, M)π(θ|M)π(m|θ, M).
(3)
Note, that marginalization of (3) over stochastic model states m recovers (2), where the evaluation
of the likelihood L(D|θ, M) entails a marginalization over all possible model states m ∼M(θ):
L(D|θ, M) =
Z
P(D|θ, m, M)π(m|θ, M)dm.
(4)
In the remaining of this manuscript the dependence of L(D|θ, M) on prior model states distribution
π(m|θ, M) will be understood implicitly via the dependency on model M which is assumed to provide
a prior distribution Mt0(θ) for the initial model state mt0. The information of the prior distribution for
later model states π(mt>t0|θ, M) is usually incorporated as a speciﬁc evolution structure within the
model M and hence is also implicitly taken into account as a dependency for likelihood L(D|θ, M).
2.3. Numerical methods for Bayesian inference
Usually, Bayesian inference cannot be solved analytically for posteriors P(θ|D, M), and in the case
of stochastic model M, usually not even for likelihood L(D|θ, M) in Equation 4 and hence also not
for posterior of model states P(m|D, M). Therefore, numerical methodologies have been developed
to sample from the posterior distribution of model parameters θ (and states m, if the model is
stochastic), obtained by running numerous corresponding simulations of the model M.
Existing
non-intrusive methods include the Metropolis or Metropolis-Hastings Markov chain Monte Carlo
(Markov-type for short) [GL06, GCS+14, Has70, MRRT53], Gibbs-type samplers for modifying one
parameter at a time [RM09], and the Approximate Bayesian Computation (ABC-type for short)
[AKS15].
Alterantive partially intrusive (but usually faster) methods include Hamiltonian Monte
Carlo (HMC-type for short) and Variational Bayesian Inference (SVI-type for short), based on exact
analytical solution to an approximation of the posterior. Sampling of posterior model states m in
stochastic models can be achieved, for instance, using Conditional Ornstein-Uhlenbeck Sampling
COUS [RM09] within Gibbs-type samplers, Particle Filtering [ADH10] within Markov-type samplers,
or by recasting model M to consider all its states m as parameters as well [AUS16] in HMC. ABC-
type methods require minimal model structure restrictions and are able to reliably sample from model
parameters posterior P(θ, |D, M), but are often very ineﬃcient in sampling posterior of model states
P(m|θ, D, M), since the high-dimensional model output y is usually compressed to a low-dimensional
suﬃcient summary statistic S(y). HMC and Variational Inference methodologies usually have the
best performance, but are intrusive (problem reformulations and/or derivatives are required). Next,
we brieﬂy describe Markov-type and ABC-type samplers, depicted by simpliﬁed algorithm ﬂowcharts
in Figure 2 and already available within the SPUX framework.
2.3.1. Markov-type sampling
In the Markov-type sampling, samples from the posterior distribution of model parameters are gen-
erated iteratively. In each iteration, model parameters are proposed, for which the prior and the
likelihood are evaluated (up to an arbitrary factor) by considering model output (if needed). These
are then either accepted or rejected (in the latter case, the parameters from the previoius step are
kept). Acceptance or rejection is based on the ratio of current and previous posterior density estimates
(i.e. the product of likelihood and prior as evinced from Bayes theorem). In the ABC-type sampling,
the initial set of model parameters is drawn from the prior distribution. Then, an iterative procedure
consist of multiple tolerance steps (converging to zero), evaluating the distance metric between the
model output and the observations (data), re-drawing a subset of the model parameters and accepting
part of them based on the distance and the adjusted tolerance, this way gradually tuning the initial
6

Sampler
set initial (e.g. from prior 𝜋) 
batch of parameters 𝜃
if sampler is not converged to posterior
PF
for all observations at times 𝑠1, … , 𝑠𝑁:
evaluate obs. errors 
𝒪𝑛
𝑝= 𝒪𝐷𝑠𝑛|𝑦𝑠𝑛
𝑝, 𝜃
(Markov)
(ABC)
compute likelihood 𝐿
of observed dataset 𝐷
given parameters 𝜃:
𝑚= 𝑀𝜃→𝐿= 𝒪𝐷|𝑦, 𝜃
accept or reject
𝜃based on 𝐿and 𝜋
compute distance 𝜌
between dataset 𝐷
and model output 𝑦
of trajectory 𝑚∼𝑀(𝜃)
resample 𝜃based on 
𝜌, tolerance, and 𝜋
initialize P trajectories
𝑚𝑠0
𝑝=1,…,𝑃∼𝑀𝑡0(𝜃)
estimate likelihood
𝐿≈ෑ
𝑛=1
𝑁
1
𝑃෍
𝑝=1
𝑃
𝒪𝑛
𝑝
propose new 𝜃samples
adjust tolerance
resample trajectories 
w.r.t. normalized 𝒪𝑛
𝑝
evolve P trajectories
𝑚𝑠𝑛
𝑝∼ℙ𝑛(⋅|𝑚𝑠𝑛−1
𝑝
, 𝜃, 𝑀)
𝑚∼𝑀(𝜃)
Figure 2: Left: a simpliﬁed ﬂowchart of the numerical posterior sampling algorithms of the Markov-
and ABC-types. Right: a simpliﬁed ﬂowchart of the Particle Filter (PF) algorithm sampling
posterior trajectories (states) of a stachastic model by ﬁltering them w.r.t. dataset and the
observation error model and estimating the likelihood deﬁned as an integral in Equation 4.
Note, that ABC-type samplers estimate posterior of model parameters (but not states).
prior model parameter samples to the posterior model parameter samples.
2.3.2. Posterior trajectories sampling and likelihood estimation for stochastic models
For stochastic models, in addition to posterior distribution of the model parameters θ, sampling of
the posterior distribution of model states m is also required. In particular, the estimation of the
(marginalized) likelihood (4) required in the Markov-type samplers is most often estimated numeri-
cally. Nonlinear ﬁltering numerical schemes, such as Particle Filter (PF) with or without smoothing
(also known as Particle Markov Chain Monte Carlo (PMCMC) technique [ADH10]) or a (Seam-
less) multi-level (Ensemble Transform) Particle Filter ((S)ML(ET)PF) [GC17] can be employed. Any
Markov-type sampler can be combined with the (ML)PF method for likelihood estimations, where
the hidden-Markov structure of the underlying stochastic model Mt is exploited for eﬃcient marginal
likelihood approximations using time-series observations [ADH10, KR17]. In particular, for observa-
tions D consisting of time-series data D = {Dsn : n = 1, . . . , N} at time snapshots t = s1, . . . , sN,
the marginal likelihood in (4) can be rewritten (using the Markov structure, see [KR17]) as
L(D|θ, M) =
Z
π (ms0|θ, M)
N
Y
n=1
Pn
 msn|msn−1, θ, M

O(Dsn|ysn, θ)dms0 . . . dmsn.
(5)
Here, for given parameters θ, probability distributions Pn(msn|msn−1, θ) characterize random model
state vector msn given previous state msn−1, representing propagation of a given model state
msn−1 to the next state msn, The observational likelihood is evaluated using the (abbreviated)
error On(D|y, θ) = O(Dsn|ysn, θ) for model output ysn = h(msn, θ) and provides a probabilistic
model for the data observation process [KR17]. The PMCMC algorithm [ADH10, KR17] uses PF to
provide an unbiased statistical estimate of the proposal parameter marginal likelihood L(D|θ, M) with
structure given in equation 5. As depicted in Figure 2 (right), the numeric approximation of equation
5 involves sampling model trajectories ("particles") in terms of model states mp (with p = 1, . . . , P)
of the underlying model M with parameters θ. At each measurement time sn in the observations
time series, model simulations are paused and all particles are re-sampled (bootstrapped) according
to their (abbreviated) observational likelihoods Op
n(D|θ) = On(D|yp, θ) with yp
sn = h(mp
sn, θ). Such
periodic re-sampling increases algorithmic complexity due to the required destruction and replication
of existing particles, however, provides an eﬃcient way of sampling "intermediate" posterior model
7

states (i.e. P(msn|θ, Ds1,...,sn, M) conditioned only on the partial dataset Ds1,...,sn up to the ﬁltering
time sn). At the end of the PF, an unbiased estimate ˆL(D|θ, M) of marginal likelihood L(D|θ, M)
as in equation 5 is evaluated by
L(D|θ, M) ≈ˆL(D|θ, M) =
N
Y
n=1
 
1
P
P
X
p=1
Op
n(D|y, θ)
!
=
N
Y
n=1
 
1
P
P
X
p=1
O(Dsn|yp
sn, θ)
!
.
(6)
In implementation, the evaluation fo ˆL(D|θ, M) is performed in log-scale to mitigate numerical round-
oﬀerrors. The accuracy (namely, the variance) of the PF likelihood estimate clearly depends on the
number of used particles P. At the initial burn-in stage, the sampling acceptace procedure is often
dominated by the low likelihood values and hence the inaccuracy of the PF estimator is of secondary
importance. However, when sampler is converged towards the posterior, a larger number of particles
is prefered to ensure low relative approximation error in likelihood estimator. In supplementary Ap-
pendix G, we describe an adaptive procedure to automatically set the number of particles throughout
the sampling procedure based on the feedback containing historical estimator accuracies and param-
eters ﬁtness. To guarantee the convergence of the posterior, the particle adaptivity is "locked" after
the speciﬁed period of sampling, which should be smaller or equal to the burn-in phase.
Additional methodologies, often refered to by "smoothing" [DJ09], are often employed to ob-
tain "smoothed" trajectories (conditioned on the entire dataset D) from posterior model states
P(m|θ, D, M). One way to achieve this is to resample already available "intermediate" posterior
trajectories (conditioned only on the partial dataset Ds1,...,sn). In particular, a simple yet very com-
putationally eﬃcient (w.r.t. to both runtime and memory usage) "rejection" based smoothing (see
sections 2.3 and 5 [DJ09] and [JMR15]) sequentially iterates sn from s1 to sN to generate trajectories
from posteriors P(ms1, . . . , msn|θ, Ds1,...,sn, M) by an additional re-ﬁltering step (after the main PF
ﬁlter step only for sn) for all preceeding snapshots s1, . . . , sn−1 as well. Note, that such "rejection"
smoothing, unlike the PF ﬁlter for likelihood estimation, is prone to particle degeneracy (i.e. collapses
to a single trajectory for each sample of posterior parameter θ) for n ≪N [DJ09] and hence should
be used with care for non-illustrative purposes. More sophisticated (but also more computationally
expensive) techniques to prevent such particle degeneracy have been also reviewed in [DJ09].
2.3.3. Approximate Bayesian Computation
If the model M does not have a hidden-Markov structure, or if the error O(·|y, θ) is not explicitly
available as a probabilistic distribution (i.e. only a direct sampling of o = y = h(m, θ) by sampling m
from a probabilistic distribution M(θ) is possible), then the eﬃcient numerical likelihood estimation
methods from subsubsection 2.3.2 cannot be directly applied. Note, that if data D consists only of a
single observation, there are obviously no eﬃciency gains in using the (temporally) adaptive likelihood
estimation using Particle Filtering.
In such cases, a more general (but potentially less eﬃcient due to the lack of adaptive temporal
ﬁltering) Approximate Bayesian Computation (ABC) methodology [AKS15] can be used to sample
from the posterior distribution, without requiring the evaluation of the likelihood as in Equation 4.
In particular, in Boltzmann-type ABC methods, the joint posterior of model parameters and states is
approximated by the following family of distributions
Pτ(θ, m|D, M) =
1
Z(τ)L(o|θ, M)π(θ|M)e−ρ(D,o)
τ
where
o ∼O(·|y, θ),
y = h(m, θ)
(7)
and where 1/Z(τ) is a normalization factor, τ is the selected tolerance level, and ρ(D, o) measures
how close the observational dataset D is to the model observation o. If the error O is not available
explicitly, o = y = h(m, θ) is used instead. Given distance ρ, an initial tolerance level τ0, and an
initial distribution (usually prior π(θ|M)), ABC-type samplers use a sequence of tolerances τi →0
to generate a sequence of approximations to P(θ, m|D, M).
In the following, dependencies of a
8

particular distance value ρ(D, o) on the parameters θ and the model M (and, if available, also of the
error O) are explicitly represented by using a supplementary notation ρ(D|θ, M) for ρ(D, o) with a
realization for o obtained from θ and M as described above.
2.4. Uncertainty propagation and forecasting
For time-dependent models M(θ) = Mt(θ), once the joint posterior distribution P(θ, m[t0,T ](θ)|D, M)
of model parameters and states (up to the last snapshot time T = sN) is available, the distribution of
the "future" (forecast) model states P(m[T,∞)(θ)|D, M) is given by propagating P(θ, mT (θ)|D, M)
to the "future" times t > T using the model M. In practice, this is achieved with a Monte Carlo (MC)
sampler, where samples from P(m[T,TF ]|θ, D, M) are obtained by sampling from P(mT |θ, D, M) and
propagating them for t ∈[T, TF ] for some future time TF > T.
Such an MC sampler can also be used for somewhat less diﬃcult direct propagation of uncertainty
from prior distributions, when dataset is not available.
For very challenging priors, Markov-type
samplers from subsubsection 2.3.1 can be employed by using prior density instead of the likelihood.
3. SPUX framework
In this section we introduce the SPUX framework, focusing on the purpose and design speciﬁcation in
subsection 3.1, available modular components and built-in services in subsection 3.2, and paralleliza-
tion capabilities in subsection 3.3. A collection of continously updated current and past examples of
SPUX applications is illustrated at the end, in subsection 3.4.
3.1. SPUX purpose and design speciﬁcation
The purpose of the SPUX framework is to provide a seamless high-level interface to perform Bayesian
inference with a free choice of methodologies, algorithms, and computational environments.
To
achieve such ﬂexible customization and eﬀortless adaptivity, the SPUX framework harnesses the pow-
erful dynamic typing and runtime polymorphism oﬀered by the modern Python programming language,
which has recently become one of the most popular programming languages for scientiﬁc computing.
In essence, the SPUX framework is a collection of carefully selected, designed and optimized modu-
lar components. The modularity of SPUX components extends beyond the conventional restrictive
patterns and instead follows a "duck typing" design philosophy [duc18], namely, the suitability of an
object to perform a function is not determined by the object’s type, rather by the support of certain
methods and properties by the object itself. An overview of the basic key components currently
implemented in SPUX, each with the purpose of representing a particular mathematical concept as
introduced in section 2, together with available speciﬁc numerical methods for each component type,
is provided in the table below:
Concept
Component
Numerical method / algorithm
Description
P(θ, m|D, M)
Sampler
EMCEE, SABC, MCMC, MC
parameters sampling
L(D|θ, M)
Likelihood
Direct, PF
states sampling and likelihood
ρ(D|θ, M)
Distance
Norm, Regression
distance for ABC sampler
mt ∼Mt(θ)
Model
Randomwalk, External, . . .
model for user’s application
Π(·), Σ(·), etc.
Aggregator
Trajectories, Replicates
aggregator of components
3.2. SPUX component assignments and built-in services
All available SPUX components can be assigned to each other following the required dependencies.
An example scheme for such assignment of the components required by any Markov-type sampling
algorithm for Bayesian inference is provided in Figure 3 (the left part), together with the associated
9

mathematical objects introduced in section 2. The right part of this same scheme depicts an as-
signment of the components together with the associated mathematical objects for the a posteriori
forecast stage, introduced in subsection 2.4. Each SPUX component (for both stages: hindcast and
sampler
model
likelihood
𝜋(𝜃|𝑀)
𝐷
𝒪
ℙ(𝜃|𝐷, 𝑀)
ℙ(𝑚[𝑡0,𝑇]|𝜃, 𝐷, 𝑀)
… infer uncertainty (hindcast)
propagate uncertainty (forecast)…
sampler
model
ℙ(𝜃|𝐷, 𝑀)
ℙ(𝑚[𝑇,𝑇𝐹)|𝜃, 𝐷, 𝑀)
𝑀𝑇𝜃= ℙ(𝑚𝑇|𝜃, 𝐷, 𝑀)
𝑀𝑡0(𝜃)
Figure 3: Component assignment and execution ﬂow scheme for the SPUX framework using Markov-
type sampler and likelihood components for the inferenece (hindcast up to the last time
T in dataset D) and the resulting bootstrapped posterior parameters and model states
distributions for the forecast (uncertainty propagation beyond the last time T in dataset
D). Boxes with thin outlines indicate the associated mathematical objects introduced in
section 2, and boxes with thick outlines and thick arrows indicate SPUX components and
their internal assignments, respectively. Thin solid and dotted arrows represent component
inputs and outputs, respectively.
White background indicates built-in components and
inference outputs, whereas gray background indicates anticipated framework inputs.
forecast) is described in detail in section 4. Such modularity in SPUX allows easy implementation of
diﬀerent numerical approaches for Bayesian inference. For instance, subsection 5.12 explain how to
use a structurally diﬀerent ABC-type method (see subsubsection 2.3.3) within SPUX.
3.3. SPUX parallelization capabilities
One of the key advantage of SPUX is its very transparent yet very ﬂexible parallelization sub-system.
In particular, multiple parallel workers can be attached to each spux component listed in subsection 3.2
and depicted in Figure 3. An example of such parallelization scheme with only three components (sam-
pler, likelihood, and model) is provided in Figure 4, where sampler samples, likelihood particles, and
multiple tasks of the (optionally) parallel user’s application/model (more details in subsection 5.10)
are distributed over available attached workers. Additional parallel components can be incorporated
when needed; for instance, the Replicates aggregator is designed to assimilate multiple independent
observational data sources in parallel. Note, that neither the model (i.e. the associated user’s appli-
cation) nor any other SPUX component is strictly required to be parallelized; instead, any of these
components might also be serial (i.e. without any parallel workers attached). For a more detailed
description of parallel hierarchically stackable SPUX executors, refer to the technical section 6.
3.4. SPUX gallery
Inspired by the "Demo Data as Code" concept [Lim19], SPUX documentation website hosts a gallery
listing examples of user’s applications, including source codes, authors, scientiﬁc ﬁelds, model pro-
gramming languages, used computational environment and conﬁguration, ﬁgures with representative
results, and associated scientiﬁc publications. At the time of this writing, example ﬁelds include
hydrology, aquatic ecology, urban hydrology, limnology, physics and data science, but the generality
of the SPUX framework does certainly extend beyond. In particular, SPUX is currently being actively
used for Bayesian inference in realistic individual-based modelling of riverine macro invertebrates,time-
dependent conceptual hydrological modeling of catchments,stochastic-input driven hydrological mod-
eling of the rainfall runoﬀsystems,and high resolution three-dimensional hydrological (and, eventually,
ecological) operational modeling of Lake Geneva.
10

sampler
parameters
parameters
parameters
parameters
likelihood
trajectory
trajectory
trajectory
trajectory
model
application
task
application
task
worker
worker
worker
worker
worker
worker
Figure 4: An example of parallelization capabilities for various compontents of the SPUX framework
described in subsection 3.2 and depicted in Figure 5. Three levels of hierarchical paralleliza-
tion are used here: parallelization over multiple model parameters samples of the EMCEE
sampler, over multiple particles of the PF likelihood, and over multiple independent tasks
of a parallel user’s application. For each SPUX component (depicted using thick lines), the
thin solid arrows represent the required (possibly independent) tasks to be executed, the
thick solid arrows represent the internally called methods, and the dotted lines represent
the remaining ommited parts of the scheme.
4. SPUX framework showcase with a random walk model
This section guides the reader through an example model and usage pattern of SPUX. An overview of
diﬀerent SPUX installation methods can be found on the SPUX documentation page, where we also
provide the links to access the source code, and a pre-conﬁgured SPUX Jupyter notebook (oﬀered on
a best-eﬀort basis only). After a brief overview of model types in subsection 4.1, an example model
of a random walk and its setup in SPUX are described in subsection 4.2. The serial model execution
procedure with a brief overview of resuts is described in subsection 4.3. In subsection 4.4, minor
auxiliary setup and execution steps needed to run SPUX in parallel on workstations and high perfor-
mance clusters are addressed. A detailed automatic PDF report compiling inference setup, results,
diagnostics and performance is introduced and interpreted in subsection 4.5. Finally, subsection 4.6
and subsection 4.7 describe procedures building upon already estimated posteriors for model parame-
ters and states: re-executing the best (or any other) model trajectory and forecasting to future times
or performing sequential Bayesian updating for a new dataset.
4.1. Deterministic and stochastic models
SPUX supports all types of models for Bayesian inference introduced in section 2: deterministic,
where model evaluation is uniquely determined by parameters θ, and stochastic, where model state
depends on random variable(s) (e.g. initial data) and/or is driven by stochastic process(es).
In Bayesian inference for deterministic models M, a simple Direct likelihood can be analytically
computed using the speciﬁed error: L(D|θ, M) = O(D|y, θ) with y = h(m, θ) and m = M(θ). An
example of such model, Straightwalk, is available at examples/straightwalk.
For stochastic models M, in addition to uncertain model parameters θ, also the uncertain model
states m ∼M(θ) need to be inferred. In such cases, the error O(D|y, θ) by itself is often not suﬃcient
to analytically compute the likelihood L(D|θ, M) in (4) for given model parameters. Hence, additional
approximation techniques are required, as discussed in section 2. An example of a stochastic time-
independent model (left part of Figure 3) is provided in examples/gaussian-sabc. Another built-in
model in SPUX is a stochastic version of the Straightwalk model, called Randomwalk, where the
direction and size of each (time) step of the walker is a random variable. Since the SPUX framework
is tailored to Bayesian inference with stochastic time-dependent models, in the following section we
showcase the Randomwalk model. Files mentioned throughout next section are located in SPUX
repository at examples/randomwalk (assumed to be the current working directory).
11

4.2. Randomwalk model
The Randomwalk model describes a stochastic walk on a line (i.e. a set of real numbers). Its goal
is to provide the simplest possible conceptual model, which has just enough complexity to illustrate
most of the functionality in SPUX, while addressing the majority of requirements in the environmental
scientiﬁc modeling. Given a prescribed time step size ∆t (in seconds for example, to ﬁx the ideas) as a
numerical discretization parameter, together with an initial time t0 and an initial (possibly uncertain)
position mt0 ∼Mt0 [m], the model iteratively takes random steps on a one-dimensional line every
∆t time units. The direction and the size of each step depend on the time step size, on two models
parameters, the drift µ [m/s] and the volatility σ [ms−1/2], and on an independent standard normal
random variable Nt:
mt+∆t ∼P(mt+∆t|mt) = mt + ∆tµ +
√
∆tσNt.
(8)
The Randomwalk model is a built-in SPUX model class with model initialization and evolution
(according to (8)) implemented in the corresponding class methods:
init(...)
initialize model with initial state mt0 and model parameters θ
run(...)
run model up to the speciﬁed time t and return model prediction output yt
For implementation details of this demonstrational model, focused on simplicity and code readability
(no vectorization, explicit for-loops), refer to Listing 12 in Appendix D. In this particular model, the
initial argument contains the initial time t0 and the initial position mt0. Alternatively, initial
contents might as well be assigned in the model constructor directly, as for the time step size ∆t.
However, such explicit speciﬁcation of the model input (i.e., using initial) at the initialization
provides more ﬂexibility in the cases where the initial position Mt0 is uncertain (see subsection 4.3)
and/or when multiple observational datasets are available (see subsection 5.9). Finally, we assume
h(m, θ) = m, i.e. model state coincides with the model output.
4.3. Inference for Randomwalk model
As brieﬂy described in subsection 3.2, all available SPUX components can be assigned among each
other following the required dependencies. An extended version of the assignment scheme in Fig-
ure 3 for this example, together with the associated mathematical objects introduced in section 2, is
provided in Figure 5. The ﬁnal assigment assigns the top component, in this case (and also usually)
the sampler, to the built-in main SPUX framework component. The SPUX framework component
provides the hierarchical sandboxing (isolation to dedicated directories) and seeding (controlling the
independence of random number streams and ensuring reproducibility independently of the chosen
computational environment), manages runtime checkpointing, diagnostics, and framework setup op-
tions (see later subsection 5.1). In the following subsections we provide an elaborate description
of the remaining SPUX framework prerequisites, supplementary capabilities, and a general inference
execution workﬂow, using the Randomwalk model (introduced in subsection 4.2) as the underlying
example.
4.3.1. SPUX conﬁguration prerequisites
To perform Bayesian inference for the given model, we also need to specify several conﬁguration ﬁles
for the remaining essential prerequisites (besides the sampler, the likelihood and the model). In
particular, for the Randomwalk example, these prerequisites are: the available observational data D,
a statistical error O(D|M(θ), θ), prior distributions π(θ|M) for all the parameters that we intend
to infer, and the initial model state Mt0(θ). Note, that the parameters θ contain both the parame-
ters of the model M and, if present, the hyper-parameters used for constructing the distributions of
the error O(·|ys, θ) and/or of the initial model states Mt0(θ). Prior initial model state distribution
Mt0(θ) is then attached to the speciﬁed model component (representing M(θ)), error O(D|M(θ), θ)
12

sampler
spux
model
likelihood
prior
dataset
initial
error
application
parameters
sandbox
seed
aggregator
𝑚𝑡0 ∼𝑀𝑡0(𝜃)
𝑚𝑠𝑛∼𝑀𝑠𝑛𝜃
𝑦𝑠𝑛= ℎ(𝑚𝑠𝑛, 𝜃)
𝑀𝑡0(𝜃)
parameters
time
𝑠𝑛
driver
𝜋(𝜃|𝑀)
𝐷
𝒪
𝐿(𝐷|𝜃, 𝑀)
ℙ(𝜃|𝐷, 𝑀)
𝜃
ℙ(𝐷|𝑀)
posterior
evidence
ℙ(𝑚|𝜃, 𝐷, 𝑀)
ensemble
report
output
𝜃
𝑀𝑡𝜃
setup
𝐿(𝐷|𝜃, 𝑀)
Figure 5: Component assignment and execution ﬂow scheme for the SPUX framework using Markov-
type sampler and likelihood components (an extension of the summary Figure 3). Boxes
with thin outlines indicate the associated mathematical objects introduced in section 2, and
boxes with thick outlines and thick arrows indicate SPUX components and their assignments
for internal evaluation, respectively.
For each SPUX component, thin solid and dotted
arrows represent component inputs and outputs, repspectively. White background indicates
built-in components and inference outputs, whereas gray background indicates anticipated
framework inputs. Dashed boxes indicate optional SPUX components for special sampler,
likelihood, model or dataset requirements and optional output or reporting capabilities.
is attached to likelihood component (representing L(D|θ, M)), and prior model parameters distri-
bution π(θ|M) is attached to sampler component (representing P(θ, m|D, M)), see subsection 3.2
and Figure 5.
Marginal prior distributions πµ, πσ, πε for each parameter θ = (µ, σ, ε) of the joint prior distribution
π(θ|M), with ε being the standard deviation (as a hyper-parameter) of the (assumed) Gaussian error
(described below), are assumed to be independent and are given by (for plots, refer to Figure 6):
πµ = U(−1, 1) [m/s],
πσ = U(5, 15) [ms−1/2],
πε = LN(1, 1) [m].
(9)
This multivariate prior distribution is deﬁned in prior.py using a built-in Tensor SPUX distribution
(see subsection 5.6 for an overview of all SPUX distributions).
The Tensor combines multiple
statistically independent distributions (provided as a Python dictionary of the corresponding univariate
distributions - for instance, from the scipy.stats package) to a multivariate SPUX distribution.
The initial position mt0 of the randomwalk is also uncertain, and hence a prior distribution Mt0 for
the initial position mt0 at (deterministic) time t0 = 0 is deﬁned in initial.py:
mt0 ∼Mt0(θ) = N(10, 2) [m],
t0 ∼δ0 [s].
(10)
We note, that mt0 could alternatively be included as a model parameter in the prior deﬁned in (9).
Actual dataset ﬁles are located in the datasets directory.
The default container for dataset
management in SPUX is a DataFrame of the pandas package [McK10], which is very similar to the
dataframe in the R programming language. The example script to load the dataset into a DataFrame,
is located in dataset.py. The dataset provides inaccurate observations Ds of the position ms at
(snapshot) times s = s1, . . . , sN. N/A values are allowed, however, no column (quantity of interest)
or row (snapshot) should contain only N/A values.
The inaccuracies in observational data are modeled with an error, which is a statistical distribution
of the observations (from the dataset) conditional on the speciﬁed model output prediction (from
the simulation). In this example this distribution is assumed to be normally distributed with mean
equal to the position ms predicted by the model, and with standard deviation given by an additional
13

uncertain parameter ε used as a hyper-parameter:
Ds ∼O(·|ys, θ) = N(ys, ε) [m].
(11)
All observations are assumed to be statistically independent.
The observational error is deﬁned
in error.py as a function (or a collable object) which, given the model output prediction and
parameters, constructs the statistical distribution above as a SPUX Distribution instance.
An (optional) dictionary specifying units (as LaTeX-supported strings) for each parameter, model
output and time dimension is speciﬁed in units.py. Its contents are used only in the SPUX report.
Within the context of this illustrative example, we also make use of the (optional) exact (loaded
in exact.py) parameter values available at datasets/exact.dat and the exact (synthetic) model
outputs (without the observational noise) available at datasets/predictions.dat.
4.3.2. SPUX conﬁguration and execution - user interface (UI)
The quickest way to run SPUX inference and post-processing (discussed in the following sections)
is using the SPUX UI conﬁguration ﬁle spux.cfg. There, arguments required for the construction,
conﬁguration, initialization, and execution of each SPUX component (either built-in or manually
imported) can be speciﬁed, including any prerequisite script (see subsubsection 4.3.1) deﬁning required
(mandatory or optional) options. All such components and options can be conﬁgured as depicted
in Listing 1, where an adaptive Particle Filter (PF with the default "rejection" smoothing) likelihood
with the speciﬁed maximum number of particles is assigned to an Aﬃne Invariant Ensemble sampler
(EMCEE) with the speciﬁed number of concurrent chains.
Additional framework options (i.e. not
speciﬁc to any component) can also be speciﬁed in spux.cfg to control various aspects of the SPUX
framework, as depicted in Listing 2. For instance, optional units of time, model parameters and
observations can be speciﬁed (see also subsection 5.6). Built-in component and framework options
are described throughout the following sections with a summary available in subsection 5.1.
Listing 1: Example spux.cfg for components
model
Randomwalk
model.dt 0.1
likelihood PF
likelihood.particles
256
sampler
EMCEE
sampler.chains 32
sampler.samples
10000
Listing 2: Additional options in spux.cfg
prior
prior.py
error
error.py
dataset
dataset.py
initial
initial.py
burnin "half"
units
units.py
exact
exact.py
Using SPUX UI, inference and post-processing can be setup and performed by simply executing:
spux spux.cfg --execute --all
This automatically generates the required SPUX scripts (described in the following sections) and
automatically executes testing, synthesis, inference, reporting, and re-execution of the best trajectory.
The inference process can be terminated at any time, since the output is periodically checkpointed
(see subsection 5.1). Additional runtime arguments can be speciﬁed to customize the inference:
--dry
"dry run" mode - inspect conﬁguration without actual sampling
--continue
continue the inference process starting from the latest checkpoint
--no-repro
disable reproducibility information (stored in randomwalk_reprozip.rpz)
The list of all built-in components and options is also retrievable by executing spux --help.
14

4.3.3. SPUX conﬁguration and execution - application programming interface (API)
The most ﬂexible way to conﬁgure SPUX is to use a conﬁguration script in which the prior, error
model, and dataset are explicitely imported and assigned (using the SPUX API) to the selected SPUX
components, such as likelihood (or distance) and sampler. A summary of an example configure.py
(excluding trivial module imports) is provided in Listing 3.
The mandatory spux.assign(...)
assigns all hierarchically ordered components to the SPUX framework.
Listing 3: Example configure.py (excluding imports)
model = Randomwalk (dt =0.1)
model.configure (initial)
likelihood = PF (particles =256)
likelihood.configure (dataset , error)
sampler = EMCEE (chains =32)
samper.configure (prior , samples =10000)
spux.configure (units)
spux.assign (model , likelihood , sampler)
Listing 4: Example infer.py
from
configure
import *
spux.setup ()
spux.init ()
sampler.init ()
sampler ()
spux.exit ()
The execution script infer.py provided in Listing 4 imports the components from configure.py,
setups the SPUX framework (mandatory spux.setup(...), see subsection 5.1), initializes the sam-
pler (mandatory sampler.init(...) for EMCEE), and performs the posterior sampling of the model
parameters and states for the speciﬁed number of samples. The mandatory framework initialization
spux.init(...) and ﬁnalization spux.exit(...) methods manage the required computational
resources. For the EMCEE sampler, initial model parameters are drawn by default from the speciﬁed
prior. The script can be executed by typing python infer.py in the console. Analogously to the
UI in subsubsection 4.3.2, the --dry, --continue and --no-repro runtime arguments can be used
for infer.py to enable "dry mode", continuation or disable reproducibility package.
4.3.4. SPUX results
The estimated marginal posteriors of model parameters are provided in Figure 6 and the estimated
marginal posteriors of model predictions are provided in Figure 7.
1.0
0.5
0.0
0.5
1.0
drift
0.0
0.5
1.0
1.5
2.0
2.5
pdf of drift
0
1
2
3
4
5
6
error
0
1
2
3
4
pdf of error
0.2
0.4
0.6
0.8
1.0
1.2
volatility
0.0
0.5
1.0
1.5
2.0
pdf of volatility
Figure 6: Marginal posterior (orange) and prior (blue) distributions of model parameters. The red
dashed line indicates the best found parameters values. The black dotted line represents
the exact parameter values.
For the inference results, the default burnin period (half of all samples) was selected to remove the
initial sampler bias. The adaptive number of particles described later in Appendix G is locked after
the speciﬁed lock batches. By default the burnin is also set to this value to avoid any potential bias
due to the adaptivity process. For post-processing, only every thin-th sample (of each sampler chain)
is selected in order to obtain a sequence of statistically independent posterior samples. In particular,
15

0
5
10
15
20
25
time [s]
8
10
12
14
16
18
20
22
position [m]
best
exact model predictions
dataset
Figure 7: Posterior distribution of model predictions for the observational dataset. The shaded or-
ange regions indicate the log-density of the posterior model predictions distribution at the
respective time points, the red line represents the best found model prediction, the black
line represents the exact model prediction values.
the default "auto" value was used for the thin period, which uses the median of optimal thinning
periods obtained by estimating multivariate eﬀective posterior sample sizes for each sampler chain.
4.4. Parallel inference for Randomwalk model
With minimal eﬀort, the above example conﬁguration can be parallelized either for a local machine
or for a remote high performance computing (HPC) cluster. We emphasize, that no modiﬁcations
are needed for this particular "Randomwalk" model class. For HPC cluster, consider placing (see
subsection 5.1) the output directory in a parallel high performance "scratch" ﬁlesystem, if available.
For a more detailed discussion regarding models not written in pure Python, refer to section 5.
4.4.1. Attaching parallel workers
To enable parallel execution, a required number of parallel workers can be attached to each SPUX
component, as depicted in Figure 3. Examples are provided in Listing 5 (for the UI conﬁguration ﬁle
spux.cfg as in Listing 1) and in Listing 6 (for the API inference script infer.py as in Listing 4, in
this case these lines should be placed before the calls to framework setup and initialization).
Listing 5: Parallel workers in spux.cfg.
likelihood.workers 8
sampler.workers 16
Listing 6: Parallel workers in infer.py.
likelihood.attach (workers =8)
sampler.attach (workers =16)
A separate dedicated core is used for the manager process of each group of parallel workers. For an
advice regarding worker allocation strategies across multiple parallel executors, refer to Appendix B.
Advanced explicit attaching of SPUX executors to components is described in subsection 6.1.
4.4.2. Launching parallel SPUX
Assuming a library for the Message Passing Interface (MPI) [Mes15] is installed, parallel scripts need
to be launched through the Python mpi4py module. For the execution using the UI, specify --mpi
runtime argument. For infer.py using the API:
mpiexec -n 1 python -m mpi4py infer.py.
16

The required worker MPI processes will be spawned automatically (i.e. according to the resources
table).
For HPC systems not supporting dynamical spawning of new MPI processes, the required number
of MPI ranks (workers) needs to be explicitly speciﬁed for mpiexec (after "-n"). The cumulative
number of required workers is indicated in the bottom right cell of the computational resources table
(see example in Table R9), which is printed to the console already during the "dry run" mode (for
which one core is suﬃcient, i.e. no MPI is required). For convenience (e.g. to automate parallel job
submission process), this number is also written to the dedicated workers.txt ﬁle. Parallelization
can be temporarily disabled with --serial runtime argument, which ignores all workers attachments.
SPUX documentation outlines speciﬁcs regarding diﬀerent MPI libraries and useful advice to address
any potential issues.
4.5. SPUX report
All tables and ﬁgures generated by the SPUX framework, such as previous Figure 6 and Figure 7,
are automatically included (see Appendix A for technical details) in a PDF report (A4 and "slides"
layouts), described in later sections. Such PDF report is also provided to support this section on
SPUX usage as Suplementary Material. All of the tables, ﬁgures, and even the conﬁguration scripts
(in section R8) referenced within this section are available in this report, hence it is strongly advisable
to have a separate copy of the Suplementary Material at hand.
Additionally, the "reproducible
package" spux.rpz is generated using the "reprozip" tool [CRSF16]. The SPUX report and the
spux.rpz provide the highest level of reproducibility (excluding containerization techniques) for an
inference or forecast run, independently of the chosen computational environment and/or hardware.
4.5.1. Conﬁguration and setup section
SPUX conﬁguration and setup is summarized in the ﬁrst section of the SPUX report (see section R1),
which is generated by executing the example report.py script and contains the following:
conﬁguration
Table R1
SPUX conﬁguration: component classes and their options
setup
Table R2
framework options from spux.setup(...), see subsection 5.1
units
Table R3
units for parameters, observations, and time
exact
Table R4
exact model parameters (if speciﬁed)
evaluations
Table R5
total number of anticipated model evaluations across components
datasets
Figure R1
dataset(s) D and exact model predictions (if speciﬁed)
errors
Figure R2
marginal error models O distributions for the speciﬁed θ, y
prior
Figure R3
marginal prior distributions of model parameters - π(θ, M)
initials
Figure R4
marginal prior distributions of initial model states - Mt0(θ)
In particular, for the Randomwalk example, at most 256 particles were used in the PF likelihood
(with adaptivity enabled, see Appendix G), and 32 chains were used in the EMCEE sampler. In total,
10’000 samples were requested, locking particle adaptivity after 75 sample batches (2’400 samples).
4.5.2. Results and diagnostics sections
To load and visualize inference results and diagnostics using the API, the report.py script is executed
with the additional --results runtime argument.
This report.py script uses built-in plotting
routines available in spux.reports.mpl module. The user can freely choose to use the reconstructed
results and diagnostics with other established data visualization libraries, including the specialized
pandas.plotting module and arviz [KCHM19] package.
The report script generates multiple
tables and ﬁgures of the results and diagnostics, and updates the SPUX report accordingly.
The inference results tables and ﬁgures included in section R2 of the SPUX report provide posterior
distributions for model parameters and predictions (i.e. model outputs y or even model states m):
17

best-parameters
Table R6
best found (e.g. maximum a posteriori) model parameters
parameters
Figure R5
marginal posteriors P(θ|D, M) of model parameters θ
predictions
Figure R6
marginal posteriors P(m|D, M) of model state m
dependencies2d
Figure R7
dependencies among pairs of posterior parameters θ or states m
parameters2d
Figure R8
joint posterior for the most dependent model parameters θ pair
In particular Figure R5 and Figure R6, are included here as Figure 6 and Figure 7, respectively.
Additional "diagnostics" tables and ﬁgures are included in section R3 of the SPUX report, providing
quality assesments of the inference results and the algorithmic technicalities for the Markov chain
sampling (EMCEE in this case), as well as the likelihood estimation (PF in this case):
status
Table R7
information about loaded SPUX status, see Appendix A
metrics
Table R8
metrics such as eﬀective sample size, thinning period, etc.
residuals
Figure R9
residuals (diﬀerences between the dataset and outputs)
QQ
Figure R10
quantile-quantile comparison of residuals and O distributions
successfuls
Figure R11
tracking of the failed or skipped likelihood evaluations
samples
Figure R12
progress of model parameters sampling (including burnin)
samples-cutoﬀ
Figure R13
progress of model parameters sampling (excluding burnin)
acceptances
Figure R14
progress of the instantaneous sampler acceptance rate
resets
Figure R15
tracking likelihood re-estimations due to stuck chains
autocorrelations
Figure R16
autocorrelations of Markov chain parameters samples
likelihoods
Figure R17
progress of prior/likelihood/posterior (including burnin)
likelihoods-cutoﬀ
Figure R18
progress of prior/likelihood/posterior (excluding burnin)
ﬁtscores
Figure R19
progress of ﬁtscores as in subsection G.1 (including burnin)
accuracies
Figure R20
progress of likelihood accuracy as in subsection G.2
particles
Figure R21
progress of likelihood adaptivity as in subsection G.3
redraw
Figure R22
progress of the particle redraw fraction in PF (see Figure 2)
redraw-temporal
Figure R23
temporal progress of the redraw fraction in PF (see Figure 2)
From these diagnostic plots, also included in section R3 of the SPUX report, we determine that the
inference was relatively successful. In particular, the eﬀective sample size (computed using the esti-
mated autocorrelations as in Figure R16) is not much smaller than the actual request by the sampler,
the posterior residuals distribution is consistent with the theoretical distribution prescribed in the error
model, not many failed (NaN - where model did not return an output) or skipped (due to at least
one proposed parameter laying outside the support of its prior) likelihood evaluations, and converged
sampling of the parameters space due to the stationarity of the Markov chains. Additionally, the
average acceptance rate is relatively satisfactory (considering there were 3 model parameters), total
chain resets (likelihood re-estimations) due to stuck chains are negligible, and chain autocorrelations
lengths are relatively short. The adaptivity within the PF (described in Appendix G) is also successful:
ﬁtscores below the prescribed threshold, accuracies in the prescribed interval, the number of particles
steadily adapted within the speciﬁed limits during the burnin stage, and the average redraw rate (the
fraction of unique particles in particle ﬁlter after each resampling) well above half the total number
of particles (indicating the absence of any critical degeneration, e.g., collapsing on a single particle,
of the PF resampling procedure).
Finally, various (approximate) criterions for model suitability [HWN18] are provided in Table R8:
with O
Bayesian Model Evidence (BME), Kashyap/Baysian Information Criterions (KIC/BIC)
w/o O
Bayesian Cross Validation (BCV), Deviance/Akaike Information Criterions (DIC/AIC)
Bayesian factors K(Ma, Mb) = P(D|Ma)/P(D|Mb) for models Ma/b from above metrics, determine
if model Ma (relative to model Mb) is strongly supported (K > 10) by the observational dataset(s).
4.5.3. Computational environment and performance sections
In section R4 of the SPUX report, the computational environment and attached computational re-
sources are provided:
18

environment
Table R9
computational environment (date, time, hardware, software versions)
resources
Table R10
required computational resources in terms of workers (e.g. cores)
In particular, for the Randomwalk example, we used 145 cores in total, with 16 parallel workers for
the EMCEE sampler, and 8 parallel workers for the PF likelihood.
In section R5, tables with measured runtimes of the entire inference run are included:
runtimes
Table R11
total inference runtimes (wall-clock and serial equivalent)
runtimes-latest
Table R12
latest inference runtimes (wall-clock and serial equivalent)
Optional computational performance plots for section R5 of the SPUX report, providing additional
insight into the computational and algorithmical eﬃciency of the inference process, can be generated
by specifying --performance runtime argument for the report.py script. In particular, "runtimes"
of key SPUX routines are measured by default (see "performance" keyword for "informative" option in
subsection 5.1). This allows to generate the "runtimes" plot for the entire sampling progress or more
easily interpretable "runtime" plots for speciﬁc sampler batches. Optionally, if "timestamps" keyword
is requested for "informative" option, the respective "timestamps" plots can be generated, providing
an insight into the detailed SPUX performance proﬁles. Additionally, plots for parallel eﬃciencies for
the entire sampling progress and strong scaling (multiple SPUX executions using diﬀerent number of
parallel workers) can be generated (refer to the SPUX documentation). Since the current Randomwalk
example takes virtually no time to be executed, it is of little value to investigate the computational
performance of SPUX here; such detailed investigations will be included in the subsequent publications
focused on the application of the SPUX framework to realistic models and datasets.
It is, however, worthwhile to inspect the parallelization performance in terms of the measured
"traﬃc" types and amounts within the adaptive PF resampling process, available in:
traﬃc
Figure R24
progress of copied/moved particles and communication cost
traﬃc-temporal
Figure R25
temporal progress of copied/moved particles and comm. cost
Note, that due to the communication-avoiding load balancing in the resampling parallelization routing
(see subsubsection 6.2.2), only a small fraction of all copied particles needs to be moved, and the
associated communication costs are even lower due to the exploitation of the node-level aﬃnity of
the parallel workers (to optimize processing of the "stateﬁles", see subsubsection 5.5.1). The initial
period is dominated by the "move" traﬃc, since the number of parallel workers equals the initial
number of particles, allowing only the exploitation of the node-level aﬃnity (if "stateﬁles" are used).
4.6. Executing a selected (e.g. "best") model trajectory
If required, the best (or any other) model trajectory, corresponding to the best model parameters, and
the best model predictions (which, for stochastic models, are not determined only by the best model
parameters), can be explicitly executed using the auto-generated best.py script. Such a-posteriori
explicit execution of a speciﬁed model trajectory allows to conﬁgure the model for richer output,
that is otherwise not required during the inference (i.e. for comparison with the datasets) or not
accessible using the functionality of the history option (see subsection 5.1). For instance, instead of
only the model output y, more of the hidden model state m could be returned as model predictions.
Additionally, an explicit trajectory directory is used for the model’s sandbox (see subsection 5.2)
instead of potentially inaccessible node-local ﬁlesystems (see subsubsection 5.3.5).
4.7. Forecast (uncertainty propagation) and sequential Bayesian updating
In many use cases, datasets could be structured into multiple time periods, allowing to perform the
Bayesian inference sequentially, and providing an optional future forecasts in-between such datasets.
Firstly, a forecast of the model predictions can be obtained by simply propagating the inferred
posterior distributions from a preceeding time period [t0, T], for which a dataset of observed data is
available, to a future time period [T, TF ], see subsection 2.4 and Figure 3. This can be achieved by
19

specifying, within the UI conﬁguration ﬁle of the SPUX framework, the location of the "past" inference
(with states=True, see subsection 5.1) root directory as pastdir and the list of "future" times as
timeset (see the example provided in Appendix C, Listing 9).
The UI automatically generates
the corresponding prior.py and initial.py scripts for the bootstrap distribution of samples from
posterior model parameters and the associated bootstrap distribution of samples from posterior model
predictions at the last snapshot of the dataset, respectively. These prior and initial prerequisites are
then assigned to an MC sampler, which is used to generate probabilistic (i.e. including uncertaitainty
quantiﬁcation) forecasts.
Additionally, if a validation dataset is also available (i.e. not used for the preceeding inference), it
can be used for the evaluation of the error in order to compute the predictive cross validation likeli-
hood or distance. For an example refer to examples/randomwalk-forecast. An analogous SPUX
conﬁguration could also be obtained (see subsection 2.4) by explicitly specifying prior distributions of
parameters and intial state instead of providing ’pastdir’.
Secondly, upon aquisition of an additional dataset (for a time period [Ti, Ti+1] following an already
inferred time period [Ti−1, Ti], as depicted in Figure 8), posterior distributions of model parameters
and ﬁnal model states at time Ti can be used as prior distributions of model parameters and initial
model state for the Bayesian inference within the succeeding time period [Ti, Ti+1], respectively. In
𝜋(𝜃|𝑀)
𝐷[𝑇0,𝑇1]
ℙ[𝑇0,𝑇1](𝜃|𝐷, 𝑀)
ℙ(𝑚[𝑇0,𝑇1]|𝜃, 𝐷, 𝑀)
𝑀𝑇1 𝜃= ℙ(𝑚𝑇1|𝜃, 𝐷, 𝑀)
𝑀𝑇0(𝜃)
𝐷[𝑇1,𝑇2]
dataset period
parameters
predictions
ℙ[𝑇0,𝑇1](𝜃|𝐷, 𝑀)
ℙ(𝑚[𝑇1,𝑇2]|𝜃, 𝐷, 𝑀)
ℙ[𝑇0,𝑇2](𝜃|𝐷, 𝑀)
𝑇0
𝑇1
𝑇2
Figure 8: Sequential Bayesian updating with SPUX for datasets split across multiple time periods.
Gray and white backgrounds indicate anticipated inputs and inferred outputs, respectively.
such a case, the location of the "past" inference and the additional dataset (for period [Ti, Ti+1])
need to be speciﬁed; unless explicitly speciﬁed otherwise, the error and units will be reused from the
conﬁguration of the speciﬁed "past" inference. If model parameters are not expected to be inﬂuenced,
the model states for the next time period [Ti, Ti+1] can be inferred by using the PF likelihood while
keeping the model parameters distribution unchanged by using the MC sampler (see the example
provided in Appendix C (Listing 10) and examples/randomwalk-assimilate). Alternatively, if not
only the model state but also the model parameters need to be updated when processing the next
time period [Ti, Ti+1], a full sequential Bayesian update can be conﬁgured analogously to the example
provided in subsubsection 4.3.2, but with the pastdir speciﬁed instead of the prior and initial.
For an example, please refer to examples/randomwalk-update.
5. SPUX framework usage and customization
This section starts with an overview of the available framework setup options in subsection 5.1 includ-
ing an overview of "sandboxing" strategies on subsection 5.2, and continues by describing key SPUX
customization guidelines for the most common necessities. Those are: how to couple an application
with the framework by deﬁning a new model (subsection 5.3), including potential options to include
stochastic processes for model input/parameters (subsection 5.4); how to handle model state seri-
alization (subsection 5.5); how to specify a prior distribution for all (model and observational error)
parameters (subsection 5.6); how to deﬁne an error for the available dataset(s) (subsection 5.7). In
addition, subsection 5.8 describes how optional "auxiliary" output and datasets, that are not in a
form of a pandas.DataFrame, can be incorporated into the model, error, and distribution classes.
Multiple independent datasets can be combined using the Replicates aggregator, introduced in
subsection 5.9. Instructions on possible built-in parallelization techniques for Python applications or,
alternatively, on executing existing parallel user applications within the SPUX model environment
20

are presented in subsection 5.10. Finally, subsection 5.11 provides some guidelines on advanced cus-
tomization options such as writing a custom SPUX component (e.g. sampler, aggregator, likelihood,
distance, etc.), with an example of SABC sampler described in subsection 5.12. SPUX documentation
might incorporate improvements made after the publication of this manuscript.
5.1. Options for framework setup, component conﬁguration and report
Multiple (non-mandatory) options to conﬁgure components, framework’s "global" setup, and the
report can be speciﬁed directly in UI conﬁguration ﬁle spux.cfg as indicated in Listing 1 and Listing 2.
In the following, we describe the corresponding API methods and list examples of such options.
In particular, an optional configure(...) method is available for each component, implementing
functionalities usually shared by all the components of a given component type. The following table
provides a brief overview of all available configure-options (excluding options already introduced in
the preceeding sections) for model, likelihood/distance, and sampler component types:
templatedir
None
directory with intial sandbox contents for the model
statefiles
None
sandbox ﬁles relevant to the model state (see subsubsection 5.5.1)
ignore
None
list of non-serializable model attribute names (see subsubsection 5.3.3)
timeset
4
integer: points in-between dataset snapshots; iterable: predictions times
auxset
None
auxiliary observational datasets (see subsection 5.8)
lock
None
batch index to lock sampler’s feedback to likelihood or distance
An iterable (e.g. list or array) of times t0, . . . , t ¯
N can be used for timeset to select corresponding
model outputs ytn for later post-processing, providing additional intermediate time points (among
dataset snapshots) for a larger temporal resolution (i.e. with ¯N > N).
The framework itself can be customized by the optional spux.configure(...) (see Listing 3)
and the mandatory spux.setup(...) (see Listing 4) methods. The following basic arguments (with
default values and descriptions) are available:
seed
0
integer seed (for hierarchical seeding of RNG libraries)
verbosity
2
hierarchical verbosity level (integer) for SPUX components
informative
["performance"]
to save: "performance" "timestamps" "infos" "rejections"
sandboxdir
"sandbox"
directory for the "root" sandbox (see subsection 5.2)
trace
"none"
sandboxes to keep (if used): "none" "best" "posterior" "all"
outputdir
"output"
directory for SPUX output ﬁles (see Appendix A)
history
"none"
store "stateﬁles"/"auxiliary": "none" "best" "posterior" "all"
states
False
store ﬁnal model states for forecasting or sequential updating
checkpoint
600
minimal time period (in seconds) between checkpoints
Note, that including more keywords in informative will consequently also increase expected inference
runtime, required operational memory, and the total size of written output ﬁles (see Appendix A). To
keep diﬀerent "stateﬁles" copies and archived auxiliary model predictions, a dictionary with respective
"statefiles" and "auxiliary" entries can be speciﬁed for history. Refer to SPUX documen-
tation for the advanced options (redirect, cache, setupdir) and for the additional options of
functions within the test.py, synthesize.py, report.py, and best.py scripts.
5.2. Sandbox - ﬁlesystems and post-execution accessiblity
As indicated in subsection 5.1, the "root" sandbox directory sandboxdir contains nested sandboxes
for each (if used) batch, chain, replicate, and model (particle or trajectory). By default, the "root"
sandboxdir is placed in (node-local) fast virtual node-local RAM-based Linux ﬁlesystem called tmpfs
(by default mounted at /dev/shm) to avoid network and I/O overheads. If the amount of system
memory is a limiting factor, alternative (node-local) ﬁlesystems can be used to avoid network (but
not I/O) overhead. For inference runs on high performance computing clusters, if neither option
21

is possible, (shared) "scratch" ﬁle system can be used instead (with associated network and I/O
overheads).
If sandboxes located on node-local (not shared) ﬁlesystems are inaccessible and the
functionality of the history option is not suﬃcient, the best (or any other) model trajectory can be
re-executed (even after inference) within a speciﬁed accessible trajectorydir for model’s sandbox
(see subsection 4.6).
5.3. Adding a model
In the most common use case scenario of SPUX, a user will wrap an existing application as the SPUX
model either by conﬁguring an existing SPUX model or by implementing a new SPUX model as
Python class. To avoid confusion, we will refer to a user’s existing application (in any programming
language) as the "application", and to the (built-in or custom) Python class coupling such application
to the SPUX framework as the "model". In this section we review available model testing routines
and discuss two scenarios in detail: using the built-in External model to manage the (appropriately
modiﬁed) application and writing a new SPUX model class to explicitly wrap an (unmodiﬁed) appli-
cation. In both cases, the incremental model execution must be possible, i.e. a corresponding output
is required for each speciﬁed time from an increasing list of times.
5.3.1. Model testing and dataset synthesis
Automatically generated test.py (and synthesize.py) scripts (using the cleaned up SPUX UI
spux.cfg ﬁle to only deﬁne the model component and its options) could be used to continuously
test the development of a new model class. In spux.cfg, model parameters can be speciﬁed as the
parametersfile option, deﬁning the path to a text ﬁle containing rows with parameter names and
values (separated by some white space), and an array of times (e.g. using utils.period(...)) for
model evaluation can be speciﬁed as the snapshots option. The optional synthesize.py script uses
the speciﬁed (or drawn from the prior) exact model parameters to generate exact model outputs and
selected observations (with error, if speciﬁed) at the speciﬁed snapshots, including the corresponding
exact.py and dataset.py scripts to load them (for instance, to generate the conﬁguration section
of the SPUX report). Such synthetically generated datasets provide an invaluable resource for making
sure the correctness of your implementation, especially because the posteriors for model parameters
(and outputs) obtained from the Bayesian inference can be compared to their exact values.
5.3.2. Using External model for user’s application
The External SPUX model relies on an application execution command, with which users are already
familiar from using the console (shell), making this a good starting option for the ﬁrst coupling of the
application to the SPUX framework. In particular, the application execution command can be used to
conﬁgure SPUX with an External model, as indicated by examples in Listing 7 and Listing 8.
Listing 7: External model example in spux.cfg.
model
External
model.command r"./ myapp <TIME >"
Listing 8: External model example in infer.py.
command = r"./ myapp <TIME >"
model = External (command)
The External model automatically isolates the application to a unique sandbox directory, where
the corresponding initial model state is written to initial.txt ﬁle if the initial model state is
speciﬁed. Subsequently, the command can be executed to evolve the current model state mtn−1 to the
next mtn by reading the automatically generated input ﬁles (parameters.txt for θ, time.txt for tn
and seed.txt for the seed), and writing output.txt (see subsubsection 5.3.8 for details). Optionally,
the contents of the corresponding ﬁles can be also passed to the application as runtime arguments
using <PARAMETERS>, <TIME> or <SEED> keywords (for substitution) within the application command.
An alternative "direct" mode of the External model is available (see later sections for reasoning) by
22

setting model attribute direct to True. In this mode, instead of executing the command sequentially
for each required time tn, the command is executed only once to evolve model from the initial state
mt0 to the ﬁnal state mT by reading automatically generated input ﬁles times.txt and seeds.txt
(or using corresponding <TIMES> and <SEEDS> keywords), and writing outputs.txt.
5.3.3. Implementing a new SPUX model class - execution control
The functionality of the simple External model might be insuﬃcient; for instance, it might be
inadequate (binary executable), inconvenient (requires changes in application interface) or ineﬃcient
(requires storing model state inbetween run(...) calls) to rely only on the application modiﬁcations.
A new SPUX model class can be written instead, allowing unrestricted capabilities for interfacing
the application to the SPUX model, possibly even without any modiﬁcations to the application itself.
Such model class can be speciﬁed as the "model" component within respective SPUX conﬁguration
ﬁles in Listing 1 and Listing 3. New model classes need to be derived from the base Model class deﬁned
in the spux.components.models.model module. The model execution ﬂow scheme is provided in
Figure 9, where arguments and built-in internal variables (introduced in subsubsection 5.3.4) are
explicitly indicated for each model method.
init
seed
sandbox
run
run
run
save
state
load
exit
time
initial
parameters
sandbox
seed
time
time
seed
seed
output
templatedir
statefiles
statefiles
output
output
statefiles
statefiles
write
read
statefiles
Figure 9: Execution ﬂow scheme for the SPUX model, controlling the user’s application. The middle
row indicates order of model methods calls (with multiple run and save/load calls). The
top row indicates the arguments that are provided to these model methods, and the bottom
row indicates before which methods the built-in variables sandbox and seed are updated.
A new SPUX model class needs to have an appropriately implemented run(...) method:
run(self,time)
run model from current time (tn−1) until time tn, return model output ytn
In the method declaration above, self is a handle to the model class instance, time corresponds to
times in snapshots, timeset, dataset or auxset, and the model output (see subsubsection 5.3.8)
consists of a mandatory array of labelled values (and, if needed, an optional "auxiliary" object).
If run(...)
fails (invalid parameters, invalid trajectory evolution, etc.), nothing (None) can be
returned instead of raising an error, if the user would like the inference to continue. Optionally,
model initialization and ﬁnalization methods can be implemented:
init(self,initial,parameters)
initialize model with initial mt0 and parameters θ
exit(self)
ﬁnalize (cleanup) model after the last run(...) call
In some cases, performing the model evaluation m ∼M(θ) for all timesteps in a single function
call (analogously to direct mode of the External model) instead of making incremental steps
mtn ∼Pn(·|mtn−1, θ, M) for each time tn might be an easier way to wrap the user application
and/or a faster way to execute the model in cases where the incremental model execution is not
required (currently it is required only by the PF likelihood). This can be achieved by implementing a
custom __call__(...) method, returning model output for all times at once as a dataframe:
__call__(self,parameters,times)
given parameters and times, return model output y
23

The built-in implementation of __call__(...) relies on using init/run/exit methods and re-
turns model outputs dataframe as well associated info and (optional) timings dictionaries (see
Appendix A). Additionally, self.diagnostics attribute might be set (at runtime, by other compo-
nents) to a function returning a dictionary of model output (ytn at time tn) diagnostics (e.g. O(ytn|θ)):
diagnostics(time,prediction,parameters,rng)
return diagnostics of prediction ytn
The info in __call__(...) then needs to be updated with returned diagnostics (if not None).
The implementation of model methods could rely on direct imports of modules corresponding
to existing applications written in Python, could rely on customized versions of the corresponding
methods found in External or Randomwalk (or in the other models from the built-in examples), or
could be a combination thereof. Three choices are available to control the application execution:
basic
execute application command in a shell (an extension of the External model)
advanced
call application methods from Python using "drivers" (an eﬃcient SPUX’onic way)
custom
implement any custom interface between the model and the application
The "basic" execution control allows extending the capabilities of the External model (see List-
ing 11 in Appendix D), for instance, by storing model inputs in a diﬀerent format, reading model
output from a custom output ﬁle, supporting parallel applications (see subsubsection 6.1.3), imple-
menting a custom init(...) method to account for the speciﬁed initial model state.
The "advanced" and "custom" execution control allows the control of multiple execution stages
in the associated application directly from within the Python model methods, avoiding the unneces-
sary overhead of application initialization and ﬁnalization in-between consecutive calls of the model’s
run(...) method. The computational eﬃciency gains are particularly large for a long time series
datasets, where run(...) needs to be called multiple times, and for models with small stochastic
volatility (including deterministic models), where model states ﬁltering is infrequent (or absent) in-
between consecutive run(...) calls. In order to directly call routines within the user’s non-Python
application, appropriate application-to-Python bindings can be used, with built-in examples including:
R
Python module "rpy2"
examples/runoff
Fortran
Python modules "f2py" or "ctypes" (C-ISO DLL)
examples/superflex
C/C++
Swig (swig.org) code wrapper
examples/hydro
Java
Python module "JPype1"
examples/IBM_2species
In addition to these examples, the most common practices of using the functionality of such Python
bindings for SPUX are combined into so-called "drivers" under spux/drivers directory. Note, that
loaded bindings or driver instances are usually non-serializable (see subsection 5.5), and hence their
names as model attributes (under self) need to be included in ignore, see subsection 5.1. Next,
we brieﬂy describe built-in framework functionalities that can be used in such a new model class.
5.3.4. Model scope attributes
Any model class instance has the following relevant internal attributes accessible in all methods:
self.sandbox()
path to an isolated sandbox directory (supports ﬁle name argument)
self.parameters
model parameters θ (available if init(...) is not speciﬁed)
self.seed()
list of integer seeds for each SPUX component (and iteration)
self.seed.one()
one integer seed, reduced from an array of hierarchical integer seeds
self.rng
numpy.random.RandomState as random_state for scipy.stats
self.verbosity
integer indicating verbosity level, e.g. for custom print(...) usage
self.print(string)
print string to standard output, taking into account the verbosity
self.replace(string)
substitute <PARAMETERS>, <SEED> and <TIME> keywords in string
self.shell(command)
execute the speciﬁed command using shell in the sandbox directory
24

The sandboxing and seeding systems are described in more detail in the following sections. Sum-
mary implementations of methods for an external application model (External) and a Python
(Randomwalk) model are provided in Appendix D, Listing 11 and Listing 12, respectively. For an
alternative to self.shell(command) for parallel applications, see subsubsection 6.1.3.
5.3.5. Model sandbox
From within any method of the model, the path to the corresponding dedicated "local" sandbox can
be retrieved by executing self.sandbox(), which might be diﬀerent for each run(...) call (e.g. if
PF likelihood is used). This is a path to a unique working directory for each model class instance,
isolating multiple models executed in parallel and preventing race conditions and conﬂicts for their
input/output ﬁles.
Any ﬁle access within the model class can be conveniently performed using
the ﬁle path returned by self.sandbox(filename). If a user’s model requires certain common
ﬁles to be present in every "local" sandbox, a template sandbox directory templatedir can be
speciﬁed in model.configure(...), see subsection 5.1 and Figure 9. The templatedir is best
placed in parallel high performance "scratch" ﬁlesystems, since contents of the template sandbox are
automatically copied (using eﬃcient local caching) to each "local" sandbox.
5.3.6. Model seeding
Note, that self.seed(), self.seed.one() and self.rng can be diﬀerent for the initialization call
init(...) and for each run(...) call. The reseed() method in the test.py script checks if the
model supports such frequent updates of the seeding and of the random number generator.
5.3.7. Initial model state (deterministic or stochastic)
Since the initial model state Mt0(θ) can depend on the model parameters, it is expected to be speciﬁed
as a function initial(parameters), the return value of which is passed to model’s init(...) call.
Alternatively, initial(parameters) can return a prior SPUX distribution for the initial model state
Mt0(θ), a draw from which is passed to model’s init(...)
call.
In such case, the posterior
distribution of the initial model state is then also inferred.
5.3.8. Model output
The run(...) method of the model needs to return labelled model output ytn = h(mtn, θ) con-
structed by calling the built-in self.output(values,names,time) method. For the sake of sim-
plicity and to keep the amount of data manageable, only a list or an array of plain datatypes (e.g.
ﬂoats, integers, strings, etc.) is allowed to be speciﬁed in the argument values, with corresponding
names in a list (or an array) of strings speciﬁed in the argument names. Alternatively, values can
be a pandas.DataFrame with labels as index and values as the ﬁrst column. For inference, all
dataset (columns) labels must be present among the model output labels. For later reporting, the
(extended) model ouput can contain additional labelled quantities of interest from the full model state
mtn. Optional __call__(...) method of the model needs to return three objects: a dataframe for
predictions y, an info dictionary with supporting information (including diagnostics, if requested),
and timing information (or None, if not available). The predictions dataframe rows need to be
indexed by times and columns labeled by names - for examples, see Model and External model
classes. For the (extended) model output of a stochastic model, it could consist of multiple labelled
quantities to enable, for example, additional diagnostics plots to be included in the SPUX report (see
subsection 4.5):
predictions2d
summary of statistical dependencies among (extended) model output ytn pairs
prediction2d
joint posteriors for the most correlated (extended) model output ytn pair
25

In some complex models (for instance, in computational ﬂuid dynamics), even the output prediction
yt of the full state mt might consist of large multi-dimensional arrays (for instance, observed surface
values) instead of just a few scalar values. In order to beneﬁt from the built-in reporting capabilities
and at the same time have an eﬃcient method for the evaluation of the error, a diverse handful of
important quantities of interest can be cherry-picked for the annotation and the (remaining or entire)
output prediction yt can be speciﬁed as "auxiliary" (see subsection 5.8).
5.4. Processes (stochastic and deterministic)
Built-in processes for use within the SPUX model classes are available in spux.library.processes.
For instance, OrnsteinUhlenbeck (i.e. bounded standard Gaussian) process with a temporal correla-
tion length τ is available, often used to facilitate inference of time-dependent input and/or parameters
for deterministic models [RM09]. In such case, the model input and/or parameters are stochastically
sampled from the process evaluate(time,rng) method with rng set to model’s self.rng.
5.5. Model state serialization
The "PF" likelihood estimator for stochastic models requires the user’s model (and the underlying
application) to have the capability of cloning its state ms at any given time "snapshot" s available in
the speciﬁed dataset. Additionally, the posterior forecast and sequential Bayesian updating described
in subsection 2.4 and subsection 4.7 also rely on loading the ﬁnal model state saved during the
preceding inference run. Such capabilities of the model are also tested by the automatically generated
test.py script mentioned earlier. In "clone" test, the script runs the speciﬁed model up to the
speciﬁed clone time and makes a clone of the original model by saving its state. Then, a second
model is created by loading the saved state of the original model and both models are run using
the same seed up to the speciﬁed compare time. The outputs of both models must be identical
(up to numerical roundoﬀerrors). The "move" test checks if the model outputs are consistent in
case its sandbox is moved in-between consecutive model runs. Note, that a freshly cloned model,
resulting from the execution of the load method as depicted in Figure 9, does not execute the
init(...) method; in particular, any required corresponding application initialization procedures
need to be part of the load functionality.
Furthermore, application bindings or driver instances
(see subsubsection 5.3.3) are usually non-serializable and hence their names (if assigned as models
attributes) need to be speciﬁed in ignore (see subsection 5.1).
In SPUX cloning is based on the concept of the model "state" serialization to a binary stream
(array). There are two potential sources for application’s state information: the model class instance
and, if sandboxing is used, the contents of the associated sandbox directory.
5.5.1. Model state serialization using "stateﬁles"
If sandboxing is used and some ﬁles in sandboxes are relevant to the model state (often corresponding
to the "restart" or "pickup" ﬁles of the underlying application), then the list of all such "stateﬁles"
(wildcards such as "*" and directories are allowed) must to be speciﬁed in model.configure(...),
see subsection 5.1 and Figure 9.
Such "stateﬁles" might be dynamically generated during the
init(...) and run(...) methods of the model, and hence are not necessarily already present
in the optional templatedir. If the model state is completely determined by such sandbox "state-
ﬁles" (i.e. those ﬁles are all that is required for a successful call of the run(...) method, in addition
to templatedir contents), then the built-in model state serialization functionality is already suf-
ﬁcient, independently of the application’s programming language (including the External model
from subsubsection 5.3.2). However, writing relatively large "stateﬁles" (in every init(...) and
run(...)) as a strategy for model serialization might be ineﬃcient due to the resulting I/O and
application initialization overheads if the model state is needed only very infrequently (or if PF is not
used - for instance, during forecast runs). To avoid such overhead, instead of writing and reading
26

"stateﬁles" in init(...) and run(...) methods, explicit model methods for "stateﬁles" writing
and reading can be implemented by the user:
write(self)
write "stateﬁles" representing model state mt to the model sandbox directory
read(self)
set model state mt by reading "stateﬁles" from the model sandbox directory
Note, that model sandbox is designed to contain only the "stateﬁles" corresponding to the current
model time tn. Any "stateﬁles" of the preceeding times (no longer required by run(...) method)
should be removed from the model sandbox. If "stateﬁles" are actually required for all times (e.g. in
post-processing), the history option needs to be set (see subsection 5.1) to enable automatic (within
self.output(...)) "stateﬁles" copies to the output directory (see Appendix A).
5.5.2. Model state serialization from model class instance
If, in addition to (or instead of) the sandbox "stateﬁles", the model state depends on the information
in the model class instance (or in the memory of the driven application), then the model serialization
requirements depend on the application’s programming language.
In particular, for (pickle’able)
applications written in pure Python or R (using r2py bindings), the built-in model state serialization
functionality is already suﬃcient. For applications written in other programming languages, custom
methods need to be implemented to serialize the model into its binary representation (state) and to
de-serialize such state into the model again (including write() and read() for "stateﬁles", if used):
save(self)
return a bytearray (binary array) as the current model state mt
load(self,state)
load the speciﬁed model state mt represented in the bytearray
For some of the most common programming languages, built-in driver modules described in subsub-
section 5.3.3 can be used to implement the above model state saving and loading.
5.6. Adding a distribution
SPUX requires all joint statistical distributions, such as model parameters prior π(·|M), error O(·|M(θ), θ),
or model initial state prior Mt0(θ) to be deﬁned as a SPUX Distribution. The mandatory func-
tionality of a Distribution X includes providing methods
(log)pdf(values)
evaluate the joint (log) probability density P(x) of values vector x ∼X
If the provided values are invalid, 0 and −∞should be returned, respectively. Optional capabilities,
such as marginal probability density and quantile-quantile plots or initial sampler parameters drawn
from π(θ, M), require implementation of additional Distribution methods:
(log)mpdf(label,value)
evaluate marginal (log) probability density of speciﬁc x[label]
intervals(alpha)
support intervals (mass alpha) of marginal probability densities
draw(rng)
draw random values vector x ∼X using the provided rng
Each element of the vector x ∼X has an associated label, i.e., a string, with supported La-
TeX syntax (for tables and ﬁgures), such as, for example, r’$\mu$’, and an associated type (by
default, random variables in Distribution are of type float). A dictionary of explicit types for
each random variable label (for instance, loaded from a text ﬁle with rows of "name type" using
loader.read_types(...) from spux.utils.io) can be speciﬁed in configure(types=...) of
any distribution derived from the base Distribution class. Highly complex distributions represented
by hierarchical Bayesian networks, where each random variable can have conditional dependencies on
other variables via a directed acyclic graph, can be also constructed as a SPUX Distribution by
relying on already existing respective packages, such as PyMC3 [SWF16]. In the remaining of this
section we describe how to construct a Tensor distribution from univariate distributions of probabilis-
tic libraries (such as scipy.stats) and outline available built-in transformation methods for further
customization.
27

5.6.1. Tensor distribution
Already introduced in the Randomwalk model example in subsubsection 4.3.1, a Tensor distribution
class from spux.library.distributions.tensor module provides the easiest and by far the most
common way to specify a joint distribution of statistically independent univariate random variables.
Only a label-indexed dictionary of required univariate distributions, for example, constructed from
univariate distributions of the scipy.stats module, is needed to construct a Tensor distribution.
5.6.2. Distribution transformations
In some application scenarios, either truncated distributions might be needed (for instance, for non-
negative variables), or some observations (the outputs and/or the dataset) need to be transformed
before the density of the distribution from the error can be evaluated (for instance, in heteroscedastic
errors). For such purposes, the built-in transformation classes for univariate distributions are available
at spux.library.distributions.utils, with example usage in examples/hydro/error.py.
Truncate
truncate tail(s) of a probability density function at the speciﬁed location(s)
Concentrate
concentrate tail(s) of probability density function to an atomic part
Transform
transform any continuous distribution by an invertible function
5.7. Adding an observational error model
In SPUX, an error is deﬁned using a function (or a callable object) which takes model output
prediction y and parameters θ as arguments and returns a corresponding SPUX distribution (see
subsection 5.6) D ∼O(·|y, θ). Heteroscedastic errors can often be easily implemented using the
distribution transformation described in subsubsection 5.6.2. The SPUX framework aims at removing
systematic bias in the error by considering stochastic models instead of deterministic couterparts, and
hence by default temporally independent errors are assumed, covering most of the realistic use cases.
If required, temporally correlated errors can be setup by including past model outputs in output and
adding lagged dataframe columns in the dataset(s), see examples/superflex.
5.8. Adding auxiliary model outputs and observational datasets
Arbitrary (pandas.DataFrame-incompatible) model outputs and datasets, such as multi-dimensional
arrays or unstructured relational sets, can be also used in SPUX model, error, and distribution classes.
In particular, an arbitrary "auxiliary" object can be passed in run(...) as part of the model output
using output(...,auxiliary=...). Correspondingly, the prediction.auxiliary attribute will
be available, for instance, in the observational error model. There, the SPUX distribution for "stan-
dard" observations can be merged with a SPUX distribution (deﬁned using prediction.auxiliary)
for "auxiliary" observations using the built-in Merge distribution from spux.distributions.merge
module. An auxset dictionary with "snapshots" as time points and a "loader" as function map-
ping a speciﬁed snapshot to the corresponding dataset object (usually stored on a high-performance
ﬁlesystem, see subsubsection 4.4.2) needs to be speciﬁed, see subsection 5.1. Note, that "auxiliary"
model outputs are not returned by the model __call__(...) method; if required internally, use the
model diagnostics attribute functionality instead. If history option is set (see subsection 5.1),
"auxiliary" model outputs are serialized and packed into archives in output directory (see Appendix A).
5.9. Aggregating multiple datasets
In some applications (for instance, examples/hydro), multiple observational datasets are available
for aggregation into the inference. Each dataset corresponds to the same model parameters θ, but is
a result of stochastic model evaluation with diﬀerent initial model states and/or diﬀerent (mutually
independent) stochastic trajectories. The Replicates aggregator can be used to aggregate datasets
by packing the corresponding datasets in a dictionary indexed by dataset names. The aggregator
28

can optionally conﬁgure initial model states from an analogously indexed dictionary of corresponding
initial functions, errors from a dictionary of corresponding error functions, and "auxsets" from a
dictionary of corresponding auxset dictionaries.
5.10. Parallel models and parallel user applications
The example Randomwalk model introduced in section 3 does not support parallelization, and hence
no executor was attached to it. However, an external user application might be very computationally
expensive and might be parallel (as a stand-alone executable or as a library) or might be splittable
into multiple independent tasks. In this section we introduce built-in executors for already parallel
user applications and review diﬀerent ways to support direct model parallelization in SPUX. The
supplementary testing and synthesis scripts mentioned earlier, can also be launched for parallel models
(i.e., with multiple workers attached), as described in subsubsection 4.4.2.
Since the application processes might be running on a diﬀerent compute node, in local ﬁlesystems
the sandbox directory can only be used for operations inside the SPUX model (not the coupled parallel
user application). If the sandbox directory is explicitly required inside parallel user application, then
the sandbox must be located on a shared ﬁlesystem, as described in subsection 5.2.
For applications parallelized using threads for shared memory architectures, such as multi-core
CPUs or GPUs, it is suﬃcient (for any execution method from subsubsection 5.3.3) to properly
allocate enough computational resources and properly bind SPUX MPI workers (ranks), such that
each application has a dedicated part of a multi-core CPU or a GPU. If the (not yet parallelized)
algorithms within model’s init/run/__call__ methods can be split into independent tasks, consider
distribution across parallel workers of an attached "Pool" or "Ensemble" SPUX executor, see section 6.
In the next sections we outline available SPUX functionalities for applications parallelized using
MPI (distributed memory parallelism), where the built-in parallel model executor is attached to the
user’s model with the speciﬁed number of workers. In particular, "basic" mode replacing the built-in
self.shell(...) is described in subsubsection 5.10.1. Despite its generality and simplicity, the
"basic" model executor mode could be ineﬃcient due to multiple application execution calls. Alter-
native model executor modes ("advanced" and "custom") are introduced in subsubsection 5.10.2 and
subsubsection 5.10.3 (with supplementary subsubsection 5.10.4), requiring only a single application
launch (e.g. loading a library or executing a process) and relying on application execution control
within init/run/__call__ methods. Such model executor modes allow one to avoid unnecessary
ﬁlesystem related operations and excessively large number of state saving/loading or "stateﬁles" writ-
ing/reading by implementing custom write/read or save/load methods (see subsection 5.5). A
guideline scheme for selecting the appropriate executor mode for the model based on the available
parallel features of the user’s application and of the MPI library is available in Figure 10.
basic
driver to directly call
application methods
yes
no
spawning new MPI
processes is supported
application support to 
replace ”MPI_COMM_WORLD”
yes
no
advanced
custom
unsupported
yes
no
efficient model state
serialization needed
yes
no
Figure 10: A guideline scheme to select the appropriate parallel model executor mode based on the
characteristics of the application and of the MPI library.
29

5.10.1. Parallel model executor for parallel user applications - "basic" mode (execute)
As an extension of the "basic" execution mode in subsubsection 5.3.3, the "basic" parallel model
executor replaces the built-in self.shell(...) in init/run/__call__ methods with
self.executor.execute (r’application’, args = [’arg1’, ’arg2’])
This way, each application is executed (spawned) in parallel (in the respective sandbox directory),
analogously to the manual launch using MPI. For correct model serialization, model (and application)
state within init(...) and run(...) methods can be directly stored as model instance (self)
attributes and/or as ﬁles (e.g. "stateﬁles") in its sandbox (without explicit write/read methods).
5.10.2. Parallel model executor for parallel user applications - "advanced" mode (instruct)
As an extension of the "advanced" model execution method from subsubsection 5.3.3, the "advanced"
parallel model executor mode relies on direct (but minimally intrusive) application control. In partic-
ular, on each parallel application worker in the peers intra-communicator, a function to establish and
return (e.g. using drivers, see subsubsection 5.3.3 and Appendix E) an interface to the application
interface = method (manager, peers)
can be speciﬁed for the built-in parallel model executor establish(...) method:
self.executor.establish (method)
User-deﬁned instruction functions are required to call application method(s) on each parallel worker
(with the MPI intra-communicator peers instead of MPI_COMM_WORLD) using the binded interface:
result = instruction (interface, peers)
SPUX model acts as a "manager" to control application: specify parameters, retrieve output, and
save/load model state or write/read "stateﬁles". To achieve this, custom instruction functions
(see also Appendix E for a practical example) can be deﬁned (even at runtime within model methods,
if speciﬁed model parameters, time, seed, etc. need to be taken into account). An instruction
function can be dispatched from (any method of) the SPUX model (as the "manager") to all parallel
application processes (as "workers") for execution and retrieval of returned instruction results by
calling:
results = self.executor.instruct (instruction)
Application interface can be terminated by self.executor.demolish() in the model exit().
5.10.3. Parallel model executor for parallel user applications - "custom" mode (connect)
As an extension of the "custom" model execution mode introduced in subsubsection 5.3.3, a "custom"
parallel model executor mode can be used to launch (spawn) the application, e.g. in init(...), with
self.executor.launch (r’application’, args = [’arg1’, ’arg2’, ’<PORT>’])
where strings <PORT> among the list of arguments will be replaced by the actual port, described in
later subsubsection 5.10.4. Afterwards, the manager-side (i.e. SPUX model) MPI inter-communicator
to the parallel workers (i.e. user application) can be obtained and released by, respectively:
workers = self.executor.connect() and self.executor.disconnect(workers)
executor method calls in any model method, as many times as required. Note, that the MPI commu-
nicators are not serializable and hence must be added to the ignore list (see subsection 5.1) if stored
as model (i.e. self) attributes. The manager-side MPI inter-communicator to workers can be used
for simulation control, parameters speciﬁcation, output retrieval, saving/loading of the model state
or writing/reading of "stateﬁles", and for termination of application execution in model exit().
30

5.10.4. Application-side communicators for parallel user application in "custom" mode
In the "custom" model executor mode, in addition to a standard intra-communicator among other
application workers (usually MPI_COMM_WORLD, as in a manual MPI run), each parallel worker (of the
user’s application) also retrieves a corresponding worker-side inter-communicator with the "manager"
(the model) by calling MPI_Comm_Connect(port,...). The port can be passed as the command
line argument (or, alternatively, as a ﬁle in the model sandbox) to the application executable. The
worker-side MPI inter-communicator to "manager" can be used to manage the execution ﬂow within
the user’s application according to the requests from the manager side (parameters acquisition, output
reporting, saving/loading model state and writing/reading "stateﬁles", etc.).
5.11. Adding an arbitrary SPUX component
In this section we brieﬂy overview guidelines for adding new SPUX component algorithms (e.g. for
sampler, aggregator, likelihood or distance component types), or even new SPUX component types.
A new SPUX component algorithm class inherits the corresponding component type class, and
can potentially re-deﬁne mandatory require dictionary and/or optional configure method (as in
configure.py), see Appendix F, Listing 15. The require dictionary contains "executor" with
an attachable executor type name (from subsection 6.1 with default being "Pool") and (optionally)
"tasks" with a list of assignable component type names (absent by default). Optional init and
mandatory __call__ methods of the component algorithm class are responsible for the actual com-
putations (as in infer.py) and can access self.sandbox, self.seed and self.rng instances.
A new SPUX component type class (i.e. a row in the components table, subsection 3.1) inherits the
Component class, deﬁnes the mandatory component attribute (a string for type name), and can re-
deﬁne the require attribute and the optional configure method, see Appendix F, Listing 16. The
Component class from the spux.components.component module implements default attributes and
methods common among all SPUX component type classes, such as name, require, evaluations,
assign, attach, copy, setup, isolate, plant, spawn, consensus, feedback, feed, prepare.
5.12. Approximate Bayesian Computation type samplers
In ABC-type samplers (introduced in subsubsection 2.3.3), the (approximate-)likelihood based sam-
plers are replaced by approximate samplers based on an empirical distance between the appropriate
summary statistics S(y) of model output y and S(D) of the observational dataset D, as depicted in
the adapted SPUX framework scheme in Figure 11. If, in addition, the (optional) observation error
model is available, then it will be incorporated into the model output before the distance evaluations.
Alternatively, the user’s SPUX model might already contain all required uncertainties arising from
both the generative (deterministic or stochastic) model and the stochastic error, in which case the
observational errors are also incorporated for the distance evaluations. However, the explicitly deﬁned
error model (i.e., not part of the generative model) allows for future forecast of true model output,
whereas hiding the error within the generative model only allows for future forecast of observed (i.e.,
including observational noise) model output. For the ABC-type samplers, the number of required
Markov
ABC
sampler
model
distance
𝜋(𝜃|𝑀)
𝒮(𝐷)
𝒪
ℙ(𝜃|𝐷, 𝑀)
ℙ(𝑚[𝑡0,𝑇]|𝜃, 𝐷, 𝑀)
𝑀𝑡0(𝜃)
sampler
model
likelihood
𝜋(𝜃|𝑀)
𝐷
𝒪
ℙ(𝜃|𝐷, 𝑀)
ℙ(𝑚[𝑡0,𝑇]|𝜃, 𝐷, 𝑀)
𝑀𝑡0(𝜃)
Figure 11: Diﬀerences in the components conﬁguration for the SPUX framework using ABC-type
sampler (right part). The components that are diﬀerent from the Markov-type samplers
(left part, refer to Figure 5 for a detailed description) are highlighted in red.
31

posterior samples can be set by specifying the batchsize for the sampler constructor, and the num-
ber of iterations for convergence can be set by specifying the batches (instead of the samples for
the Markov-type samplers) for the call sampler(...). For an example using the SABC sampler,
please refer to examples/randomwalk-sabc.
6. SPUX framework parallelization
In this section we describe three built-in executor types and their load balancing techniques. For the
design and implementation of the hierarchical parallelization sub-system supporting multiple nested
executors for simultanous parallelization of multiple nested SPUX components, see Appendix H.
6.1. SPUX executors - types
Any set of independent tasks within any of the SPUX components can be executed in parallel using
built-in SPUX executors. The parallel design of SPUX is centered on three main types of executors,
each supporting diﬀerent functionality and (usually) meant to be used in diﬀerent SPUX components:
pool
dynamically executes a set of independent tasks; task "states" are discarded
ensemble
adaptively executes a set of independent ensemble tasks (maintaining task "states")
model
dynamically executes a user’s application (discarding or maintaining its "state")
Executors can be attached to SPUX components without any restrictions on the executor type, how-
ever, the list of available executor capabilities (methods) is checked at the attach(...) method
of the corresponding SPUX component. As described in the tutorial, the default executors of each
type are the serial executors with rather straight-forward implementations, namely SerialPool,
SerialEnsemble, SerialModel.
Currently, parallel executors of each type are implemented in
SPUX using the Message Passing Interface (MPI) library [Mes15] via the object-oriented Python
MPI bindings package mpi4py [DPKC11]. MPI dynamic process management for parallel workers is
employed for deploying nested parallelization. Pickle-based communication of arbitrary serializable
Python objects is used for execution workﬂow management across managers and workers, as the
memory and processing time overheads are negligible. Additionally, for multiple subsequent executor
calls (e.g. for iterative sampling within sampler), executor task caching allows to perform task serial-
ization and communication only once. Finally, memory-eﬃcient raw binary arrays (bytearray) are
used for eﬃcient communication of model states.
6.1.1. SPUX "pool" executors
A "pool" type executor can be used by calling its map(functions,parameters) method, where
either argument (or both) can also be iterable (for instance, a list) over corresponding parallel tasks.
For lower communication overhead, consider providing lists of sandboxes and/or seeds as arguments
in map(...) instead of using lists of functions. An optional list of arguments can be speciﬁed in
map(...) as args and will be passed to the execution of the function(s) by function(...,*args).
6.1.2. SPUX "ensemble" executors
An "ensemble" type executor does not accept tasks directly, but requires an instance derived from
the Ensemble base class deﬁned in the spux.library.ensembles.ensemble module. Currently
the only implemented ensemble class is for a Particles ensemble of SPUX models (particles to
be used in the PF likelihood). For example, the "ensemble" executor, attached to an instance of
the PF likelihood, manages the parallel initialization, method execution and resampling of (indexed)
ensemble tasks. In particular, in-between the ensemble executor connect(ensemble,indices) and
disconnect() methods, call(method,args) and/or resample(indices) methods can be called
multiple times, each time executing the speciﬁed ensemble method and/or performing ensemble
32

resampling, respectively. In the resample method, ensemble tasks can be cloned (duplicate indices)
and deleted (missing indices), including the load re-balancing across the resampled sub-ensembles. A
detailed description of the parallel resamplig procedure is available in the earlier manuscript [ŠK17].
6.1.3. SPUX "model" executors
A "model" type executor is needed to execute a parallel user application. As described in subsec-
tion 5.10, three parallel model executor modes to manage parallel application workers are supported:
basic
execute(command) by spawning a new process (analogously to mpiexec)
advanced
instruct(...) to call instruction(...) with MPI intra-communicator peers
custom
launch(application) and connect()/disconnect() MPI inter-communicators
Options, such as args list of runtime arguments, can be speciﬁed in execute(...) and launch(...)
methods. For serial executors (workers set to None or 0), both methods fall back to shell(...).
6.2. SPUX executors - load balancing
In all parallel executors, a collection of multiple tasks must be carefully dispatched to (or re-distributed
among) parallel workers to ensure the minimal needed wall-clock runtime to execute all tasks. In this
section we describe such load balancing algorithms for the pool and ensemble type executors. For
"model" type executors, the load balancing is federated to the user’s application itself.
6.2.1. Load balancing for pool-type executors
All tasks requested in the "map" method of the "pool" type executor are executed dynamically, with
pending tasks being constantly dispatched to workers as they become available. For parallel runs with
multiple datasets (e.g. examples/hydro example), the Replicates aggregator performs guided load
balancing by evaluating the lengths of the associated datasets and sorting component evaluations,
taking into account the component adaptivity as well (for instance, the replicate-dependent adap-
tive number of particles in the PF likelihood). Higher priorities are assigned to components with
longer datasets and large number of particles (if applicable), and lower priorities are assigned to the
components with shorter datasets and smaller number of particles (if applicable).
6.2.2. Load balancing for ensemble-type executors
During routing, re-sampled tasks are instructed to be moved from over-utilized workers to the neigh-
boring under-utilized workers across their intra-communicator. Routing objects include worker-speciﬁc
information regarding task identiﬁcation, source (present worker address), destination (re-routed
worker address), re-identiﬁcation (for post-routing re-seeding), and aﬃnity (local or remote node).
An empirical greedy algorithm is employed for constructing routings based on re-sampled task counts,
current task distribution across workers, and the maximal worker load. For each worker, resident re-
sampled tasks are kept in place provided the worker loads do not exceed the maximal worker load.
Remaining tasks are routed either to a under-utilized worker on the same compute node (determined
by aﬃnity), or, in the worst case, to the closest (w.r.t. proximity of worker ranks) under-utilized
remote worker. To further reduce communication overhead, identical tasks are moved together by
moving only a single particle and then replicating it on the destination worker. Such load balancing
strategy, however, does not take into account possibly heterogeneous model runtimes for each task,
and hence there could be potential gains from dynamic balancing frameworks using task stealing in-
stead of an adaptive routing approach. Sandboxes, associated to the corresponding tasks, are assumed
to be isolated from each other, unless local aﬃnity (determined automatically) is set in the routing
information. For local aﬃnity, sandboxes exist on the same compute node (with access to the same
ﬁlesystem), and hence a simple and eﬃcient sandbox renaming (internally consisting of stashing and
fetching) is suﬃcient. Such "hybrid" parallelization, inspired by existing MPI + OpenMP paradigms,
33

harnesses the node-local connectivity to minimize the required sandbox-related communication. For
remote aﬃnity, sandboxes are "pseudo-moved" from one remote ﬁlesystem to another, by copying
template sandbox on the destination, and, if sandbox "stateﬁles" are speciﬁed, saving the sandbox
state, sending it to the destination, and loading it into the newly created sandbox. After removal of
all extinct particles, at the expense of memory usage the resampling process prioritizes overlapping
the communication synchronization overhead with any other pending task that does not require infor-
mation exchange with other parallel ensemble workers. In particular, after all asynchronous sends and
receives for task exchange according to routings are launched, all orphan particles (already sent, but
not to be kept) are removed, remaining particles are stashed, ensembles are synchronzed to prevent
race conditions, stashed particles are fetched using re-identiﬁcation, local particles are replicated, and
only in the last step received remote particles are stored and replicated, while still waiting in the back-
ground for all local particles (if any) to be sent. For a more detailed description and an illustrative
scheme of such adaptive ensemble task resampling, refer to the earlier manuscript [ŠK17].
7. Future developments
In the near future, multi-level uncertainty quantiﬁcation methods (e.g. ML(ET)PF, MLCV) will be
investigated. These methods can greatly reduce the amount of required computational resources for
Bayesian inference and forecasting by conﬁguring applications for multiple diﬀerent accuracies (e.g.
resolutions). These capabilities will be oﬀered in SPUX through the the foreseen MLCV aggregator
component. Furthermore, samplers optimized for multi-modal posteriors will be investigated. Addi-
tionally, shared memory optimizations for nodel-level model state communication, dynamic balancing
using task stealing in ensemble tasks resampling, and machine learning based dynamical process
scheduling for pool executors are foreseen as further improvements of an already very well performing
SPUX parallelization suite. Depending on the level of support for general purpose distributed tasking
libraries, new parallel executors could be added.
8. Author contributions and acknowledgments
Author contributions. JŠ designed and implemented the prototype framework, plotting, automatic
report generation, compiled the documentation, contributed examples for built-in and "hydro" models,
and led the preparation of this manuscript. JŠ and MB co-designed and implemented the paralleliza-
tion speciﬁcation, all main SPUX components, and SPUX interfaces (UI and API). MB also designed
and implemented the legacy MPI connector, distribution variable types, and contributed examples for
"superﬂex" and "ibm" models.
Acknowledgments. Authors would like to thank Uwe Schmitt and Mikołaj Rybiński (ETH Zurich)
for support with CI/CD and unit testing on EULER and Daint clusters at the Swiss Supercomputing
Center (CSCS), Artur Saﬁn, Marco Dal Molin and Lorenz Amman (Eawag) for contributing exam-
ple models, Mira Kattwinkel (U Koblenz-Landau) for helping to design the initial SPUX framework
prototype, Alexander Madsen (PSI), Anthony Ebert (USI and Eawag), Simone Ulzega (ZHAW) for
contributing and testing SABC sampler, Peter Reichert (Eawag), Andreas Scheidegger (Eawag), Carlo
Albert (Eawag), Fabrizio Fenicia (Eawag), Vilma Šukien˙e for providing feedback to this manuscript
and framework usage, Panagiotis Hadjidoukas (IBM Research Zurich) for advice regarding task-based
parallelism design, and Siddhartha Mishra (ETH Zurich) for access to EULER cluster.
Funding. We also acknowledge the Eawag Directorate Discretionary Funding for granting ﬁnan-
cial resources and Swiss Supercomputing Center (CSCS) for granting computational resources in the
development project d97.
34

References
[ABD+09]
Brian M Adams, WJ Bohnhoﬀ, KR Dalbey, JP Eddy, MS Eldred, DM Gay, K Haskell,
Patricia D Hough, and Laura P Swiler. Dakota, a multilevel parallel object-oriented
framework for design optimization, parameter estimation, uncertainty quantiﬁcation,
and sensitivity analysis: version 5.0 user’s manual. Sandia National Laboratories, Tech.
Rep. SAND2010-2183, 2009.
[ADH10]
Christophe Andrieu, Arnaud Doucet, and Roman Holenstein.
Particle Markov chain
Monte Carlo methods. Journal of the Royal Statistical Society: Series B (Statistical
Methodology), 72(3):269–342, 6 2010.
[AKS15]
Carlo Albert, Hans R Künsch, and Andreas Scheidegger. A simulated annealing approach
to approximate bayes computations. Statistics and computing, 25(6):1217–1232, 2015.
[AUS16]
Carlo Albert, Simone Ulzega, and Ruedi Stoop. Boosting bayesian parameter inference of
nonlinear stochastic diﬀerential equation models by hamiltonian scale separation. Phys.
Rev. E, 93:043313, Apr 2016.
[BCJ+18]
Eli Bingham, Jonathan P. Chen, Martin Jankowiak, Fritz Obermeyer, Neeraj Pradhan,
Theofanis Karaletsos, Rohit Singh, Paul Szerlip, Paul Horsfall, and Noah D. Goodman.
Pyro: Deep Universal Probabilistic Programming. Journal of Machine Learning Research,
2018.
[CGH+17]
Bob Carpenter, Andrew Gelman, Matthew D Hoﬀman, Daniel Lee, Ben Goodrich,
Michael Betancourt, Marcus Brubaker, Jiqiang Guo, Peter Li, and Allen Riddell. Stan:
A probabilistic programming language. Journal of statistical software, 76(1), 2017.
[CRSF16]
Fernando Chirigati, Rémi Rampin, Dennis Shasha, and Juliana Freire. Reprozip: Com-
putational reproducibility with ease. In Proceedings of the 2016 International Conference
on Management of Data, SIGMOD ’16, pages 2085–2088. ACM, 2016.
[DJ09]
A. Doucet and A.M. Johansen. A tutorial on particle ﬁltering and smoothing: Fifteen
years later. Handbook of nonlinear ﬁltering, 12:656–704, 2009.
[DMRT14]
J Doherty, C Muﬀels, J Rumbaugh, and M Tonkin. Pest, model independent parameter
estimation and uncertainty analysis, 2014.
[DPKC11]
Lisandro D. Dalcin, Rodrigo R. Paz, Pablo A. Kler, and Alejandro Cosimo. Parallel
distributed computing using Python. Advances in Water Resources, 34(9):1124–1139,
2011.
[DSN+14]
Omer Demirel, Ihor Smal, Wiro J Niessen, Erik Meijering, and Ivo F Sbalzarini. Ppf: A
parallel particle ﬁltering library. 2014.
[DSU+17]
Ritabrata Dutta, Marcel Schoengens, Avinash Ummadisingu, Nicole Widmer, Jukka-
Pekka Onnela, and Antonietta Mira. Abcpy: A high-performance computing perspective
to approximate bayesian computation. arXiv preprint arXiv:1711.04694, 2017.
[duc18]
Glossary — python 3.7.1 documentation, 2018.
[eas20]
Easyabc, 2020.
[EMD17]
V. Elvira, J. Míguez, and P. M. Djurić. Adapting the number of particles in sequential
monte carlo methods through an online scheme for convergence assessment.
IEEE
Transactions on Signal Processing, 65(7):1781–1794, 2017.
35

[FMHLG13] Daniel Foreman-Mackey, David W Hogg, Dustin Lang, and Jonathan Goodman. em-
cee:
the mcmc hammer.
Publications of the Astronomical Society of the Paciﬁc,
125(925):306, 2013.
[GC17]
A. Gregory and C. J. Cotter. A seamless multilevel ensemble transform particle ﬁlter.
SIAM Journal on Scientiﬁc Computing, 39(6):A2684–A2701, 2017.
[GCS+14]
Andrew Gelman, J. Carlin, H. Stern, D. Dunson, A. Vehtari, and D Rubin. Bayesian
Data Analysis, third ed. Chapman & Hall, 2014.
[GDB+19]
Carla Gomes, Thomas Dietterich, Christopher Barrett, Jon Conrad, Bistra Dilkina, Ste-
fano Ermon, Fei Fang, Andrew Farnsworth, Alan Fern, Xiaoli Fern, Daniel Fink, Douglas
Fisher, Alexander Flecker, Daniel Freund, Angela Fuller, John Gregoire, John Hopcroft,
Steve Kelling, Zico Kolter, Warren Powell, Nicole Sintov, John Selker, Bart Selman,
Daniel Sheldon, David Shmoys, Milind Tambe, Weng-Keen Wong, Christopher Wood,
Xiaojian Wu, Yexiang Xue, Amulya Yadav, Abdul-Aziz Yakubu, and Mary Lou Zeeman.
Computational sustainability: Computing for a better world and a sustainable future.
Commun. ACM, 62(9):56–65, August 2019.
[GL06]
D. Gamerman and H.F. Lopes. Markov Chain Monte Carlo: Stochastic Simulation for
Bayesian Inference. Taylor & Francis Inc., 2006.
[HAPK15]
Panagiotis E Hadjidoukas, Panagiotis Angelikopoulos, Costas Papadimitriou, and Petros
Koumoutsakos. π4u: A high performance computing framework for bayesian uncertainty
quantiﬁcation of complex models. Journal of Computational Physics, 284:1–21, 2015.
[Has70]
W. K. Hastings. Monte Carlo sampling methods using Markov chains and their applica-
tions. Biometrika, 57(1):97–109, 1970.
[HBWP13]
Matthew D. Hoﬀman, David M. Blei, Chong Wang, and John Paisley. Stochastic vari-
ational inference. J. Mach. Learn. Res., 14(1):1303–1347, May 2013.
[HKCCB15] Tobias Houska, Philipp Kraft, Alejandro Chamorro-Chavez, and Lutz Breuer. Spotting
model parameters using a ready-made python package. PloS one, 10(12):e0145180,
2015.
[HWN18]
Marvin Höge, Thomas Wöhling, and Wolfgang Nowak. A primer for model selection:
The decisive role of model complexity. Water Resources Research, 54(3):1688–1715,
2018.
[JMR15]
P.E. Jacob, L.M. Murray, and S. Rubenthaler. Path storage in the particle ﬁlter. Statistics
and Computing, 25:487–496, 2015.
[KBBP16]
J Nathan Kutz, Steven L Brunton, Bingni W Brunton, and Joshua L Proctor. Dynamic
mode decomposition: data-driven modeling of complex systems. SIAM, 2016.
[KCHM19]
Ravin Kumar, Colin Carroll, Ari Hartikainen, and Osvaldo A. Martin. ArviZ a uniﬁed
library for exploratory analysis of Bayesian models in Python. The Journal of Open
Source Software, 2019.
[KR17]
Mira Kattwinkel and Peter Reichert. Bayesian parameter inference for individual-based
models using a Particle Markov Chain Monte Carlo method. Environmental Modelling
& Software, 87:110–119, 2017.
[Lim19]
Thomas A. Limoncelli. Demo data as code. Commun. ACM, 62(10):39–41, September
2019.
36

[LLM19]
Adam Lev-Libfeld and Alexander Margolin. Fast data: Moving beyond from big data’s
map-reduce, 2019.
[LSTB09]
David Lunn, David Spiegelhalter, Andrew Thomas, and Nicky Best. The bugs project:
Evolution, critique and future directions. Statistics in medicine, 28(25):3049–3067, 2009.
[McK10]
Wes McKinney. Data structures for statistical computing in python. In Stéfan van der
Walt and Jarrod Millman, editors, Proceedings of the 9th Python in Science Conference,
pages 51 – 56, 2010.
[Mes15]
Message Passing Interface Forum. MPI: Message-Passing Interface Standard. Version
3.1. Technical report, 2015.
[MRRT53]
Nicholas Metropolis, Arianna W. Rosenbluth, Marshall N. Rosenbluth, and Augusta H.
Teller. Equation of State Calculations by Fast Computing Machines. The Journal of
Chemical Physics, 21(6), 1953.
[Mur13]
Lawrence M Murray.
Bayesian state-space modelling on high-performance hardware
using libbi. arXiv preprint arXiv:1306.3277, 2013.
[NHS05]
Lars Nerger, Wolfgang Hiller, and Jens Schröter. Pdaf-the parallel data assimilation
framework: experiences with kalman ﬁltering. In Use Of High Performance Computing
In Meteorology, pages 63–83. World Scientiﬁc, 2005.
[PCDM14]
Matthew Parno, P Conrad, A Davis, and YM Marzouk. Mit uncertainty quantiﬁcation
(muq) library, 2014.
[Plu04]
Martyn Plummer. Jags: Just another gibbs sampler, 2004.
[RM09]
P. Reichert and J. Mieleitner. Analyzing input and structural uncertainty of nonlinear
dynamic models with stochastic, time-dependent parameters. Water Resources Research,
45:W10402, doi:10.1029/2009WR007814, 2009.
[ŠK17]
Jonas
Šukys
and
Mira
Kattwinkel.
SPUX:
Scalable
Particle
Markov
Chain
Monte
Carlo
for
uncertainty
quantiﬁcation
in
stochastic
ecological
models.
http://arxiv.org/abs/1711.01410, 11 2017.
[SWF16]
John Salvatier, Thomas V Wiecki, and Christopher Fonnesbeck. Probabilistic program-
ming in python using pymc3. PeerJ Computer Science, 2:e55, 2016.
[uqp20]
Uqpy, 2020.
[WME+20] D. Wälchli, S. Martin, A. Economides, L. Amoudruz, G. Arampatzis, X. Bian, and
P. Koumoutsakos. Load balancing in large scale bayesian inference. In [To be published
at] Proceedings of the Platform for Advanced Scientiﬁc Computing Conference PASC20,
2020.
A. SPUX setup and output
The setup directory contains generated raw (binary, text, and LaTeX) information regarding SPUX
framework conﬁguration and setup, corresponding to configure.py and infer.py, respectively. The
output directory is generated incrementally using checkpointing and contains multiple subdirectories:
37

samples
samples of posterior parameters, outputs, supporting "infos", timings
best
the "best" trajectory (parameters and output); also see Table R13
diagnostics
diagnostics information (e.g. metrics)
specifications
structure speciﬁcations of the outputs/infos ﬁles and the "best" trajectory
pickup
framework (e.g. sampler) pickup information (for --continue)
states
ﬁnal states samples of posterior model trajectories
statefiles
(only if used) copies of "stateﬁles" for all times
auxiliary
(only if used) auxiliary model outputs
Contents of the samples subdirectory can be loaded and used for any custom post-processing:
parameters-*.csv
a CSV ﬁle containing posterior samples of model parameters
predictions-*.dat
a binary ﬁle containing (posterior) of model output samples
infos-*.dat
a binary ﬁle containing supporting information (e.g. acceptance rates)
timings-*.dat
a binary ﬁle containing framework timings (runtimes and timestamps)
Refer to report.py for an example of loading all samples ﬁles into dataframes (for CSV) and
lists of dictionaries using loader.reconstruct(...) from spux.utils.io. The corresponding
speciﬁcations tables are also listed in the section R6 of the SPUX report as Table R14, Table R15, and
??. In particular, each model predictions (output) sample is a dictionary of either dataframes (default)
or Trajectories from spux.utils.trajectories module (using memory-eﬃcient compressed
data structures [JMR15], e.g. for PF).
Infos contain additional (component-speciﬁc) information for "infos" keyword requested in informative
and/or for a manually speciﬁed component (such as PF likelihood or Norm distance) diagnostics
list. Unless "rejections" keyword is requested in informative, model outputs and infos corresponding
to the rejected samples are excluded (set to None), and the estimates of prior and likelihood/distance
are overwritten by the values of the accepted samples.
To ensure consistency, we strongly advice to access any SPUX framework output (or conﬁguration
and setup options) using the SPUX status, which can be constructed using the Status class from
the spux.utils.status module, as is done in report.py. Attribute status.parameters is a
pandas.DataFrame loaded directly from parameters-*.csv, and additional status methods can
be called with any required batch/chain/time arguments:
info(key,...)
respective key value from loaded infos-*.dat
prediction(...)
respective model prediction from loaded predictions-*.dat
timing(...)
respective timing measurement from loaded timings-*.dat
sandbox(...)
respective model sandbox path (relative to the "root" sandbox)
auxiliary(...)
respective "auxiliary" model output (see subsection 5.8)
statefiles(...)
respective paths to directories storing all "stateﬁles" (see subsubsection 5.5.1)
Internally, these methods use "origins" stored in sampler "info" to locate the corresponding accepted
samples and functionality of the Trajectories class to follow trajectory ﬁltering (if PF was used).
The figures directory contains all raw ﬁgure ﬁles in multiple formats (default: PDF (vector),
PNG (raster)) and with the associatiated caption ﬁles *.cap containing the description of the ﬁgure
contents. The contents of the setup and figures directories are used to generate the LaTeX source
ﬁles in the report directory, which are compiled into the SPUX report (A4 and "slides" layouts).
B. Remark on workers for parallel SPUX executors
The freedom to attach arbitrary many parallel workers for every SPUX component provides a lot of
ﬂexibility, but also leaves ample space for computationally ineﬃcient parallel conﬁgurations. Hence,
for large production runs, we strongly advice to follow these two guidelines:
• Attach most workers to the outer-most SPUX component(s) (e.g. sampler).
38

• Avoid few parallel workers (less than 4) - replace them with the default workers = None.
SPUX will report the percentage of the number of manager cores w.r.t. the total number of cores.
Additionally, parallel performance and scaling plots could be used to investigate the model runtimes
homogeneity and the synchronization overheads. For highly heterogeneous model runtimes (for in-
stance, when model runtime strongly depends on the proposed model parameters), or for the cases
where sampler often proposes parameters outside the speciﬁed prior parameters distribution, consider
attaching more workers to the PF likelihood instead in order to avoid very few tasks for each parallel
worker in sampler (and aggregator) SPUX components.
C. Example forecast and sequential assimilation scripts
Listing 9: Example spux.cfg for forecast
model
Randomwalk
model.dt 0.1
aggregator
Trajectories
aggregator.trajectories 64
sampler MC
sampler.chains 64
sampler.samples
500
pastdir "../ randomwalk"
timeset
utils.period (25, 30)
Listing 10: Example spux.cfg for assimilation
model
Randomwalk
model.dt 0.1
likelihood PF
likelihood.particles
256
sampler MC
sampler.chains 64
sampler.samples
1000
pastdir "../ randomwalk"
dataset
dataset -T_1 -T_2.py
D. Example model scripts
Listing 11: External model methods
from spux.utils.io import
loader
from spux.utils.io import
dumper
def run (self ,time ):
p = self.sandbox("parameters.txt")
t = self.sandbox("time.txt")
s = self.sandbox("seed.txt")
dumper.txt(p,self.parameters)
dumper.txt(t,self.time)
dumper.txt(s,self.seed ())
cmd = self.replace(self.command)
code = self.shell(cmd)
if code is None: return
None
o = self.sandbox("output.txt")
return
self.output(loader.txt(o))
Listing 12: Randomwalk model methods
def init (self ,initial ,parameters ):
self.d = parameters["drift"]
self.v = parameters["volatility"]
self.m = initial["position"]
self.time = initial["time"]
def run (self ,time ):
while
self.time < time:
dt = min(time -self.time ,self.dt)
n = self.rng.normal ()
self.m += dt*self.d
self.m += sqrt(dt)* self.v*n
self.time += dt
return
self.output ([ self.m],[’p’])
E. Example parallel model executor scripts for Fortran
In Listing 13, the application module is obtained by compiling user’s Fortran application with f2py.
On parallel application workers, the returned interface is passed to instruction in Listing 14.
39

Listing 13: Application interface example
def
interface (manager ,peers ):
import
application
interface = application
return
interface
Listing 14: Executor instruction example
def
instruction (interface ,peers ):
comm = peers.py2f ()
result = interface.main(comm)
return
result
F. Example SPUX component scripts
Listing 15: Component algorithm class example
class
NewAlgorithm (NewType ):
def init (self ,...):
...
def
__call__ (self ,...):
...
Listing 16: Component type class example
class
NewType (Component ):
component = "new_type"
requires = {"executor":"Pool"}
def
configure (self ,...):
...
G. Adaptivity in the Particle Filter
For the SPUX framework, we are also introducing an empirical adaptivity technique to dynamically
control the number of particles used for the PF likelihood.
The proposed technique, as already
common among complex adaptive methodologies such as [EMD17], relies on empirical metrics; in
this case, on "ﬁtscore", which describes the convergence progress of the posterior sampling, and on
"accuracy", which provides an estimate for the statistical error in the estimated likelihood.
G.1. Fitscores of model parameters
A ﬁtscore indicator is simply the likelihood normalized (purely for easier interpretation and gener-
alization) w.r.t. the dataset length, dimensionality of the observations, and the maximum of the
corresponding error distribution densities. As such, ﬁtscore measures (analogously to likelihood) how
consistent the model output is compared to the observational dataset. Recalling the notations
On(D|y, θ) = O(Dtn|ytn, θ),
Op
n(D|y, θ) = On(D|yp, θ),
On(D|y, θ) = 1
P
P
X
p=1
Op
n(D|y, θ),
(12)
we deﬁne ﬁtscore as the average (over multiple dataset points N and multiple particles P of the
Particle Filter) logarithm of the normalized (with respect to probability density function value of
the model prediction and the dimensions d of the observations) residuals (observational model error
probability density of posterior model output):
r(θ, D, M) = 1
N
N
X
n=1
1
P
P
X
p=1
log Op
n(D|y, θ) −log Op
n(y|y, θ)
d
.
(13)
The numerical value of the ﬁtscore empirically tracks progress of the sampler convergence, with higher
values indicating that the sampler is most probably out of burn-in phase.
40

G.2. Accuracies of likelihood estimates
The variance σ2(log ˆL) of the estimated marginal log-likelihood log ˆL was found to be an important
indicator controlling the convergence of the Markov chain sampler [KR17]. It can be bounded (with
equality only for statistically independent On over all n = 1, . . . , N) by the sum of log-likelihood
variances from individual snapshots (for brevity, we drop (D|y, θ) notation from Op
n and On):
σ2(log ˆL) = σ2
 
log
 N
Y
n=1
On
!!
= σ2
 N
X
n=1
log Op
n
!
≤
N
X
n=1
σ2(log On).
(14)
Motivated by this upper bound, the empirical "accuracy" of the PF likelihood is deﬁned as the average
(over multiple dataset points) standard deviations for the estimated observational log-errors (treated
as random statistical estimates depending on p):
δ(θ, D, M) = 1
N
N
X
n=1
σ(log On).
(15)
The numerical value of the accuracy empirically measures the statistical accuracy of the marginal
likelihood estimator computed by the Particle Filter, normalized with respect to the length n of the
dataset. The standard deviations σ(log On) for the estimated observational log-errors of the current
snapshot, are dynamically (i.e., during runtime) estimated using 1st order Taylor-series approximation
and the central limit theorem. In particular, the variance of the logarithm of a random variable a can
be approximated using Taylor series around the mean µ(a):
σ2(log a) ≈σ2(a)
µ2(a),
provided
σ2(a) ≪µ2(a).
(16)
The summands σ(log On) in equation 15 can be approximated by such Taylor-series by setting a = On
and then applying a central limit theorem to approximate µ(On) and σ(On) by ˆµ(On) and ˆσ(On):
µ(On) = µ(Op
n) ≈ˆµ(Op
n) = 1
P
P
X
p=1
Op
n = On,
σ2(On) = σ2(Op
n)
P
≈ˆσ2(Op
n)
P
.
(17)
In Equation 17, ˆµ(Op
n) and ˆσ(Op
n) are the empirical estimators for the mean and the variance as
computed from the available p samples O1
n, . . . , OP
n of the random variables On. Applying both
approximations, the ﬁnal estimate for the "accuracy" is then given by
δ ≈1
N
N
X
n=1
σ(On)
µ(On) ≈1
N
N
X
n=1
1
√
P
ˆσ(Op
n)
On
.
(18)
G.3. Adaptivity procedure and customization
Initially, all particle ﬁlters start with the minimal number of particles (default is 4 or the number
of parallel workers for PF). Fitscore values above the customizable threshold (with the default value
at -2) indicate that the sampler is most probably out of the initialization phase, and hence the
particle ﬁlter adaptivity is activated.
The number of particles is then halved or doubled in each
likelihood estimation step, depending on whether the accuracy is above or below the requested
accuracy envelope. Requested accuracy envelope is determined by the accuracy and margins speciﬁed
within the adaptive PF likelihood. The maximum number of particles is given by particles option
of PF. To guarantee the convergence of the posterior, the particle adaptivity is suspended after the
user-speciﬁed "lock" sample batches.
41

H. SPUX executors - design
The schemes outlining designs of the SPUX executor (the manager side) and the corresponding SPUX
contract (the worker side) are listed in Figure 12 and Figure 13, respectively. In Figure 12, during
init
rootcall
connector
peers
connector.init
root
prepare
task.executor.setup
bootup
connector.bootup
exit
shutdown
connector.disconnect
connector.shutdown
map [pool]
connect
call [ensemble]
disconnect
execute [model]
contract
setup
rootcall
owner
task
addresscall
address
resources
port
task(s)
results
Figure 12: Execution ﬂow scheme for the SPUX executor base class (the manager side). The thick
solid arrows represent the sequence of the called methods (the gray arrow indicates a call
at the top-level executor only), the thin solid arrows represent the required inputs, and the
dotted arrows represent the outputs (or inputs provided to subsequently called methods).
The tasks-to-results pipeline can then be executed multiple times, with the ﬁnal executor
exit() deallocating all computational resources.
the conﬁguration stage when executors are attached to SPUX components, only the setup of each
executor is called and only with the owner argument. Such pre-setup executors are then used to
compute the total required resources in framework init(...). After the conﬁguration stage, the
init(...)
of the executor attached to the main component (sampler) is called by framework
init(...) and initializes the connector (i.e. allocates computational resources). The same executor
init(...) method then starts a chain of recursive hierarchical initializations for all other executors.
In particular, the setup(...) of the executor attached to the task of the owner of the current
executor is called, where the task of the executor owner becomes the owner of the task executor,
and the addresscall() of the executor is used for the rootcall of the task executor. During
the bootup(...), each parallel worker is issued a contract describing required worker behavior, as
explained in Figure 13. In Figure 13, the manager and peers communicators are provided as the
peers
executor.rootcall
executor
port
taskroot
executor.init
taskport
connector.connect
manager
task.executor.bind
task(s)
executor.exit
manager
results
manager.Disconnect
manager.Disconnect
Figure 13: Execution ﬂow scheme for the SPUX contract (the worker side) required in SPUX execu-
tors. The boxes with dashed outline represent the communicators and the dashed arrows
represent the information exchange over the network. The thick solid arrows represent
the sequence of the called methods, the thin solid arrows represent the required inputs,
and the dotted arrows represent the outputs (or inputs provided to subsequently called
methods). The pipeline part highlighted with the gray outline can be executed multiple
times.
arguments of the contract. Initially, the port (to connect back to manager) and the executor (to be
42

attached to incoming tasks) for each worker’s contract is received by the workers from the manager
communicator. The taskroot (integer or string) is determined by the rootcall(...) of the task
executor, which is set in task.executor.setup (...), see Figure 12. The taskport (integer or
string) is determined by calling init(...) of the received task executor, completing this way the next
recursive step of hierarchical executor initializations. The contract then waits for further instructions
from the manager, for instance, to execute some tasks. The determined taskroot and taskport
are unique among all other peer task executors (initialized in the contracts of parallel workers at the
same hierarchical level), and hence can be used for binding any requested task to the task executor
initialized speciﬁcally for this contract. The results obtained by executing the received tasks are then
sent back to the manager.
43

