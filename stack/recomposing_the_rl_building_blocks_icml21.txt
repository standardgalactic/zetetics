Recomposing the Reinforcement Learning Building Blocks with
Hypernetworks
Elad Saraﬁan* 1 Shai Keynan* 1 Sarit Kraus 1
Abstract
The Reinforcement Learning (RL) building
blocks, i.e. Q-functions and policy networks, usu-
ally take elements from the cartesian product of
two domains as input. In particular, the input of
the Q-function is both the state and the action,
and in multi-task problems (Meta-RL) the policy
can take a state and a context. Standard architec-
tures tend to ignore these variables’ underlying
interpretations and simply concatenate their fea-
tures into a single vector. In this work, we argue
that this choice may lead to poor gradient estima-
tion in actor-critic algorithms and high variance
learning steps in Meta-RL algorithms. To con-
sider the interaction between the input variables,
we suggest using a Hypernetwork architecture
where a primary network determines the weights
of a conditional dynamic network. We show that
this approach improves the gradient approxima-
tion and reduces the learning step variance, which
both accelerates learning and improves the ﬁnal
performance. We demonstrate a consistent im-
provement across different locomotion tasks and
different algorithms both in RL (TD3 and SAC)
and in Meta-RL (MAML and PEARL).
1. Introduction
The rapid development of deep neural-networks as general-
purpose function approximators has propelled the recent
Reinforcement Learning (RL) renaissance (Zai and Brown,
2020). RL algorithms have progressed in robustness, e.g.
from (Lillicrap et al., 2016) to (Fujimoto et al., 2018); explo-
ration (Haarnoja et al., 2018); gradient sampling (Schulman
et al., 2017; 2015a); and off-policy learning (Fujimoto et al.,
*Equal contribution: authors’ order was randomly selected
1Department of Computer Science, Bar-Ilan University, Ramat-
Gan, Israel. Correspondence to: Elad Saraﬁan, Shai Keynan
<elad.saraﬁan@gmail.com, shai.keynan@gmail.com>.
Proceedings of the 38 th International Conference on Machine
Learning, PMLR 139, 2021. Copyright 2021 by the author(s).
Dynamic net
Hidden 
layer
Output
base 
variable 
meta variable
Primary net
Linear 
layer
Dynamic
weights
Block
Head
Linear
+
ReLU
bias
gain
weights
Linear
ReLU
Head 
ReLU
Res Block
Linear
Res Block
Res Block
Block 512
Block 1024
Head 
Block 256
Output 
layer
Figure 1. The Hypernetwork architecture
2019; Kumar et al., 2019). Many actor-critic algorithms
have focused on improving the critic learning routines by
modifying the target value (Hasselt et al., 2016), which
enables more accurate and robust Q-function approxima-
tions. While this greatly improves the policy optimization
efﬁciency, the performance is still bound by the networks’
ability to represent Q-functions and policies. Such a con-
straint calls for studying and designing neural models suited
for the representation of these RL building blocks.
A critical insight in designing neural models for RL is the
reciprocity between the state and the action, which both
serve as the input for the Q-function. At the start, each
input can be processed individually according to its source
domain. For example, when s is a vector of images, it
is common to employ CNN models (Kaiser et al., 2019),
and when s or a are natural language words, each input
can be processed separately with embedding vectors (He
et al., 2016). The common practice in incorporating the
state and action learnable features into a single network
is to concatenate the two vectors and follow with MLP to
yield the Q-value (Schulman et al., 2017). In this work, we
argue that for actor-critic RL algorithms (Grondman et al.,
2012), such an off-the-shelf method could be signiﬁcantly
improved with Hypernetworks.
In actor-critic methods, for each state, sampled from the

Recomposing the Reinforcement Learning Building-Blocks with Hypernetworks
dataset distribution, the actor’s task is to solve an optimiza-
tion problem over the action distribution, i.e. the policy.
This motivates an architecture where the Q-function is ex-
plicitly modeled as the value function of a contextual bandit
(Lattimore and Szepesv´ari, 2020) Qπ(s, a) = Qπ
s (a) where
s is the context. While standard architectures are not de-
signed to model such a relationship, Hypernetworks were
explicitly constructed for that purpose (Ha et al., 2016).
Hypernetworks, also called meta-networks, can represent
hierarchies by transforming a meta variable into a context-
dependent function that maps a base variable to the required
output space. This emphasizes the underlying dynamic be-
tween the meta and base variables and has found success
in a variety of domains such as Bayesian neural-networks
(Lior Deutsch, 2019), continual learning (von Oswald et al.,
2019), generative models (Ratzlaff and Li, 2019) and ad-
versarial defense (Sun et al., 2017). The practical success
has sparked interest in the theoretical properties of Hyper-
networks. For example, it has recently been shown that
they enjoy better parameter complexity than classical mod-
els which concatenate the base and meta-variables together
(Galanti and Wolf, 2020a;b).
When analyzing the critic’s ability to represent the Q-
function, it is important to notice that in order to optimize
the policy, modern off-policy actor-critic algorithms (Fu-
jimoto et al., 2018; Haarnoja et al., 2018) utilize only the
parametric neural gradient of the critic with respect to the
action input, i.e., ∇aQπ
θ (s, a).1 Recently, (Ilyas et al., 2019)
examined the accuracy of the policy gradient in on-policy
algorithms. They demonstrated that standard RL implemen-
tations achieve gradient estimation with a near-zero cosine
similarity when compared to the “true” gradient. Therefore,
recovering better gradient approximations has the potential
to substantially improve the RL learning process. Motivated
by the need to obtain high-quality gradient approximations,
we set out to investigate the gradient accuracy of Hyper-
networks with respect to standard models. In Sec. 3 we
analyze three critic models and ﬁnd that the Hypernetwork
model with a state as a meta-variable enjoys better gradient
accuracy which translates into a faster learning rate.
Much like the induced hierarchy in the critic, meta-policies
that optimize multi-task RL problems have a similar struc-
ture as they combine a task-dependent context and a state
input. While some algorithms like MAML (Finn et al.,
2017) and LEO (Rusu et al., 2019) do not utilize an explicit
context, other works, e.g. PEARL (Rakelly et al., 2019) or
MQL (Fakoor et al., 2019), have demonstrated that a context
improves the generalization abilities. Recently, (Jayakumar
et al., 2019) have shown that Multiplicative Interactions
(MI) are an excellent design choice when combining states
1This is in contrast to the REINFORCE approach (Williams,
1992) based on the policy gradient theorem (Sutton et al., 2000)
which does not require a differentiable Q-function estimation.
and contexts. MI operations can be viewed as shallow Hy-
pernetwork architectures. In Sec. 4, we further explore
this approach and study context-based meta-policies with
deep Hypernetworks. We ﬁnd that with Hypernetworks,
the task and state-dependent gradients are disentangled s.t.
the state-dependent gradients are marginalized out, which
leads to an empirically lower learning step variance. This is
speciﬁcally important in on-policy methods such as MAML,
where there are fewer optimization steps during training.
The contributions of this paper are three-fold. First, in
Sec. 3 we provide a theoretical link between the Q-function
gradient approximation quality and the allowable learning
rate for monotonic policy improvement. Next, we show
empirically that Hypernetworks achieve better gradient ap-
proximations which translates into a faster learning rate and
improves the ﬁnal performance. Finally, in Sec. 4 we show
that Hypernetworks signiﬁcantly reduce the learning step
variance in Meta-RL. We summarize our empirical results
in Sec. 5, which demonstrates the gain of Hypernetworks
both in single-task RL and Meta-RL. Importantly, we ﬁnd
empirically that Hypernetwork policies eliminate the need
for the MAML adaptation step and improve the Out-Of-
Distribution generalization in PEARL.
2. Hypernetworks
A Hypernetwork (Ha et al., 2016) is a neural-network ar-
chitecture designed to process a tuple (z, x) ∈Z × X and
output a value y ∈Y . It is comprised of two networks, a
primary network wθ : Z →Rnw which produces weights
wθ(z) for a dynamic network fwθ(z) : X →Y . Both net-
works are trained together, and the gradient ﬂows through
f to the primary networks’ weights θ. During test time or
inference, the primary weights are ﬁxed while the z input
determines the dynamic network’s weights.
The idea of learnable context-dependent weights can be
traced back to (McClelland, 1985; Schmidhuber, 1992).
However, only in recent years have Hypernetworks gained
popularity when they have been applied successfully with
many dynamic network models, e.g. recurrent networks (Ha
et al., 2016), MLP networks for 3D point clouds (Littwin and
Wolf, 2019), spatial transformation (Potapov et al., 2018),
convolutional networks for video frame prediction (Jia et al.,
2016) and few-shot learning (Brock et al., 2018). In the
context of RL, Hypernetworks were also applied, e.g., in
QMIX (Rashid et al., 2018) to solve Multi-agent RL tasks
and for continual model-based RL (Huang et al., 2020).
Fig. 1 illustrates our Hypernetwork model. The primary
network wθ(z) contains residual blocks (Srivastava et al.,
2015) which transform the meta-variable into a 1024 sized
latent representation. This stage is followed by a series
of parallel linear transformations, termed “heads”, which

Recomposing the Reinforcement Learning Building-Blocks with Hypernetworks
output the sets of dynamic weights. The dynamic network
fwθ(z)(x) contains only a single hidden layer of 256 which
is smaller than the standard MLP architecture used in many
RL papers (Fujimoto et al., 2018; Haarnoja et al., 2018) of
2 hidden layers, each with 256 neurons. The computational
model of each dynamic layer is
xl+1 = σReLU
 (1 + gl(z)) ⊙xlW l(z) + bl(z)

(1)
where the non-linearity is applied only over the hidden layer
and gl is an additional gain parameter that is required in
Hypernetwork architectures (Littwin and Wolf, 2019). We
defer the discussion of these design choices to Sec. 5.
3. Recomposing the Actor-Critic’s
Q-Function
3.1. Background
Reinforcement Learning concerns ﬁnding optimal policies
in Markov Decision Processes (MDPs). An MDP (Dean
and Givan, 1997) is deﬁned by a tuple (S, A, P, R) where
S is a set of states, A is a set of actions, P is a set of
probabilities to switch from a state s to s′ given an action
a, and R : S × A →R is a scalar reward function. The
objective is to maximize the expected discounted sum of
rewards with a discount factor γ > 0
J(π) = E
" ∞
X
t=0
γtR(st, at)
at ∼π(·|st)
#
.
(2)
J(π) can also be written, up to a constant factor 1 −γ, as
an expectation over the Q-function
J(π) = Es∼dπ 
Ea∼π(·|s) [Qπ(s, a)]

,
(3)
where the Q-function is the expected discounted sum of re-
wards following visitation at state s and execution of action
a (Sutton and Barto, 2018), and dπ is the state distribution
induced by policy π.
Actor-critic methods maximize J(π) over the space of pa-
rameterized policies. Stochastic policies are constructed as
a state dependent transformation of an independent random
variable
πφ(a|s) = µφ(ε|s)s.t.ε ∼pε,
(4)
where pε is a predeﬁned multivariate distribution over Rna
and na is the number of actions.2 To maximize J(πφ) over
the φ parameters, actor-critic methods operate with an itera-
tive three-phase algorithm. First, they collect into a replay
buffer D the experience tuples (s, a, r, s′) generated with
the parametric πφ and some additive exploration noise pol-
icy (Zhang and Sutton, 2017). Then they ﬁt a critic which is
2Deterministic policies, on the other hand, are commonly de-
ﬁned as a deterministic transformation of the state’s feature vector.
a parametric model Qπ
θ for the Q-function. For that purpose,
they apply TD-learning (Sutton and Barto, 2018) with the
loss function
Lcritic(θ) =
Es,a,r,s′∼D
hQπ
θ (s, a) −r −γEa′∼πφ(·|s′)[Qπ
¯θ (s′, a′)]
2i
,
where ¯θ is a lagging set of parameters (Lillicrap et al., 2016).
Finally, they apply gradient descent updates in the direction
of an off-policy surrogate of J(πφ)
φ ←φ + η∇φJactor(φ)
∇φJactor(φ) = E{
s∼D
ε∼pε} [∇φµφ(ε|s)∇aQπ
θ (s, µφ(ε|s))] .
(5)
Here, ∇φµφ(ε|s) is a matrix of size nφ × na where nφ is
the number of policy parameters to be optimized.
Two well-known off-policy algorithms are TD3 (Fujimoto
et al., 2018) and SAC (Haarnoja et al., 2018). TD3 op-
timizes deterministic policies with additive normal explo-
ration noise and double Q-learning to improve the robust-
ness of the critic part (Hasselt et al., 2016). On the other
hand, SAC adopts stochastic, normally distributed policies
but it modiﬁes the reward function to include a high entropy
bonus ˜R(s, a) = R(s, a) + αH(π(·|s)) which eliminates
the need for exploration noise.
3.2. Our Approach
The gradient of the off-policy surrogate ∇φJactor(φ) dif-
fers from the true gradient ∇φJ(π) in two elements: First,
the distribution of states is the empirical distribution in the
dataset and not the policy distribution dπ; and second, the
Q-function gradient is estimated with the critic’s parametric
neural gradient ∇aQπ
θ ≃∇aQπ. Avoiding a distribution
mismatch is the motivation of many constrained policy im-
provement methods such as TRPO and PPO (Schulman
et al., 2015a; 2017). However, it requires very small and
impractical steps. Thus, many off-policy algorithms ignore
the distribution mismatch and seek to maximize only the
empirical advantage
A(φ′, φ) = Es∼D [Ea∼π′ [Qπ(s, a)] −Ea∼π [Qπ(s, a)]] .
In practice, a positive empirical advantage is associated with
better policies and is required by monotonic policy improve-
ment methods such as TRPO (Kakade and Langford, 2002;
Schulman et al., 2015a). Yet, ﬁnding positive empirical
advantage policies requires a good approximation of the
gradient ∇aQπ. The next proposition suggests that with a
sufﬁciently accurate approximation, applying the gradient
step as formulated in the actor update in Eq. (5) yields
positive empirical advantage policies.
Proposition 1. Let π(a|s)
=
µφ(ε|s) be a stochas-
tic parametric policy with ε
∼
pε, and µφ(·|s) a

Recomposing the Reinforcement Learning Building-Blocks with Hypernetworks
transformation with a Lipschitz continuous gradient
and a Lipschitz constant κµ.
Assume that its Q-
function Qπ(s, a) has a Lipschitz continuous gradient
in a, i.e.
|∇aQπ(s, a1) −∇aQπ(s, a2)| ≤κq∥a1 −
a2∥.
Deﬁne the average gradient operator ∇φ · f =
Es∼D [Eε∼pε[∇φµφ(ε|s) · f(s, µφ(ε|s))]]. If there exists
a gradient estimation g(s, a) and 0 < α < 1 s.t.
∥∇φ · g −∇φ · ∇aQπ∥≤α∥∇φ · ∇aQπ∥
(6)
then the ascent step φ′ ←φ + η∇φ · g with η ≤1
˜k
1−α
(1+α)2
yields a positive empirical advantage policy.
We deﬁne ˜k and provide the proof in the appendix. It fol-
lows that a positive empirical advantage can be guaranteed
when the gradient of the Q-function is sufﬁciently accurate,
and with better gradient models, i.e. smaller α, one may
apply larger ascent steps. However, instead of ﬁtting the gra-
dient, actor-critic algorithms favor modeling the Q-function
and estimate the gradient with the parametric gradient of
the model ∇aQπ
θ . It is not obvious whether better mod-
els for the Q-functions, with lower Mean-Squared-Error
(MSE), provide better gradient estimation. A more direct
approach could be to explicitly learn the gradient of the
Q-function (Saraﬁan et al., 2020; Saremi, 2019); however,
in this work, we choose to explore which architecture re-
covers more accurate gradient approximation based on the
parametric gradient of the Q-function model.
We consider three alternative models:
1. MLP network, where state features ξ(s) (possibly
learnable) are concatenated into a single input of a
multi-layer linear network.
2. Action-State Hypernetwork (AS-Hyper) where the ac-
tions are the meta variable, input of the primary net-
work w, and the state features are the base variable,
input for the dynamic network f.
3. State-Action Hypernetwork (SA-Hyper), which re-
verses the order of AS-Hyper.
To develop some intuition, let us ﬁrst consider the simplest
case where the dynamic network has a single linear layer
and the MLP model is replaced with a plain linear model.
Starting with the linear model, the Q-function and its gradi-
ent take the following parametric model:
Qπ
θ (s, a) = [ws, wa] · [ξ(s), a]
∇aQπ
θ (s, a) = wa
(7)
where θ = [ws, wa]. Clearly, in this case, the gradient is not
a function of the state, therefore it is impossible to exploit
this model for actor-critic algorithms. For the AS-Hyper we
obtain the following model
Qπ
θ (s, a) = w(a) · ξ(s)
∇aQπ
θ (s, a) = ∇aw(a)ξ(s)
(8)
f
q
a
f
w
q
a
f
w
q
a
s
(a)
(b)
(c)
Figure 2. Illustrating three alternatives for combining states and ac-
tions: (a) MLP; (b) AS-Hyper; and (c) SA-Hyper. The blue arrows
represent the backpropagation calculation of the actions gradient.
Notice that in the SA-Hyper, the gradient ﬂows only through the
dynamic network, which enables more efﬁcient implementation as
the dynamic network is much smaller than the primary network.
Usually, the state feature vector ξ(s) has a much larger
dimension than the action dimension na. Thus, the matrix
∇aw(a) has a large null-space which can potentially hamper
the training as it may yield zero or near-zero gradients even
when the true gradient exists.
On the other hand, the SA-Hyper formulation is
Qπ
θ (s, a) = w(s) · a
∇aQπ
θ (s, a) = w(s)
(9)
which is a state-dependent constant model of the gradient
in a. While it is a relatively naive model, it is sufﬁcient for
localized policies with low variance as it approximates the
tangent hyperplane around the policy mean value.
Moving forward to a multi-layer architecture, let us ﬁrst
consider the AS-Hyper architecture. In this case the gra-
dient is ∇aQπ
θ (s, a) = ∇aw(a)∇wfw(s). We see that the
problem of the single layer is exacerbated since ∇aw(a) is
now a na × nw matrix where nw ≫na is the number of
dynamic network weights.
Next, the MLP and SA-Hyper models can be jointly ana-
lyzed. First, we calculate the input’s gradient of each layer
xl+1 = f l(xl) = σ
 xlW l + bl
(10)
∇axl+1 = (∇axl)∇xlf l(xl) = (∇axl)W lΛl(xl)
(11)
Λl(xl) = diag
 σ′  xlW l + bl
,
(12)
where σ is the activation function and W l and bl are the
weights and biases of the l-th layer, respectively. By the
chain rule, the input’s gradient of an L-layers network is the
product of these expressions. For the MLP model we obtain
∇aQπ
θ (s, a) = W aΛ1(s, a)
 L−1
Y
l=2
W lΛl(s, a)
!
W L.
(13)

Recomposing the Reinforcement Learning Building-Blocks with Hypernetworks
Figure 3. Comparing the Cosine-Similarity of different critic models: (a) The percentage of states with CS better than a τ threshold.
(b-c) The percentage of states with CS better than τ = 0.25 and τ = 0.75 with respect to the learning step. (d) The mean CS over time
averaged over all seeds and environments. The shaded area is the interquartile range Q3 −Q1. In all cases, the CS was evaluated every
10K steps with Ns = 15 states and Nr = 15 independent trajectories for each state.
On the other hand, in SA-Hyper the weights are the outputs
of the primary network, thus we have
∇aQπ
θ (s, a) =
W 1(s)Λ1(s, a)
 L−1
Y
l=2
W l(s)Λl(s, a)
!
W L(s).
(14)
Importantly, while the SA-Hyper’s gradient conﬁguration
is controlled via the state-dependent matrices W l(s), in
the MLP model, it is a function of the state only via the
diagonal elements in Λl(s, a). These local derivatives of
the non-linear activation functions are usually piecewise
constant when the activations take the form of ReLU-like
functions. Also, they are required to be bounded and smaller
than one in order to avoid exploding gradients during train-
ing (Philipp et al., 2017). These restrictions signiﬁcantly
reduce the expressiveness of the parametric gradient and its
ability to model the true Q-function gradient. For example,
with ReLU, for two different pairs (s1, a1) and (s2, a2) the
estimated gradient is equal if they have same active neurons
map (i.e. the same ReLUs are in the active mode). Follow-
ing this line of reasoning, we postulate that the SA-Hyper
conﬁguration should have better gradient approximations.
Empirical analysis To test our hypothesis, we trained TD3
agents with different network models and evaluated their
parametric gradient ∇aQθ(s, a). To empirically analyze the
gradient accuracy, we opted to estimate the true Q-function
gradient with a non-parametric local estimator at the policy
mean value, i.e. at aµ = Eε∼pε [µφ(ε|s)]. For that purpose,
we generated Nr independent trajectories with actions sam-
pled around the mean value, i.e. a = aµ + ∆a, and ﬁt with
a Least-Mean-Square (LMS) estimator a linear model for
the empirical return of the sampled trajectories. The “true”
gradient is therefore the linear model’s gradient. Additional
technical details of this estimator are found in the appendix.
As our Q-function estimator is based on Temporal-
Difference (TD) learning, it bears bias. Hence, in practice
we cannot hope to reconstruct the true Q-function scale.
Thus, instead of evaluating the gradient’s MSE, we take
the Cosine Similarity (CS) as a surrogate for measuring the
gradient accuracy.
cs(Qπ
θ ) = Es∼D

∇aQπ
θ (s, aµ) · ∇aQπ(s, aµ)
∥∇aQπ
θ (s, aµ)∥∥∇aQπ(s, aµ)∥

,
Fig. 3 summarizes our CS evaluations with the three model
alternatives averaged over 4 Mujoco (Todorov et al., 2012)
environments. Fig. 3d presents the mean CS over states
during the training process. Generally, the CS is very low,
which indicates that the RL training is far from optimal.
While this ﬁnding is somewhat surprising, it corroborates
the results in (Ilyas et al., 2019) which found near-zero CS
in policy gradient algorithms. Nevertheless, note that the
impact of the CS accuracy is cumulative as in each gradient
ascent step the policy accumulates small improvements.
This lets even near-zero gradient models improve over time.
Overall, we ﬁnd that the SA-Hyper CS is higher, and unlike
other models, it is larger than zero during the entire training
process. The SA-Hyper advantage is speciﬁcally signiﬁcant
at the ﬁrst 100K learning steps, which indicates that SA-
Hyper learns faster in the early learning stages.
Assessing the gradient accuracy by the average CS can be
somewhat confounded by states that have reached a local
equilibrium during the training process. In these states the
true gradient has zero magnitude s.t. the CS is ill-deﬁned.
For that purpose, in Fig. 3a-c we measure the percentage
of states with a CS higher than a threshold τ. This indi-
cates how many states are learnable where more learnable
states are attributed to a better gradient estimation. Fig.
3a shows that for all thresholds τ ∈[0, 1] SA-Hyper has
more learnable states, and Fig. 3b-c present the change
in learnable states for different τ during the training pro-
cess. Here we also ﬁnd that the SA-Hyper advantage is
signiﬁcant particularly at the ﬁrst stage of training. Finally,
Fig. 4 demonstrates how gradient accuracy translates to
better learning curves. As expected, we ﬁnd that SA-Hyper
outperforms both the MLP architecture and the AS-Hyper
conﬁguration which is also generally inferior to MLP.

Recomposing the Reinforcement Learning Building-Blocks with Hypernetworks
(a) Hopper
(b) Walker2d
(c) Ant
(d) HalfCheetah
Figure 4. Learning curves of the TD3 algorithm with different critic models. SA-Hyper refers to Qπ
θ = fwθ(s)(a), AS-Hyper refers to
Qπ
θ = fwθ(a)(s) and MLP refers to Qπ
θ = fθ(s, a), which concatenates both inputs.
In the next section, we discuss the application of Hypernet-
works in Meta-RL for modeling context conditional policies.
When such a context exists, it also serves as an input vari-
able to the Q-function. In that case, when modeling the
critic with a Hypernetwork, one may choose to use the con-
text as a meta-variable or alternatively as a base variable.
Importantly, when the context is the dynamic’s input, the
dynamic weights are ﬁxed for each state, regardless of the
task. In our PEARL experiments in Sec. 5 we always used
the context as a base variable of the critic. We opted for
this conﬁguration since: (1) we found empirically that it is
important for the generalization to have a constant set of
weights for each state; and (2) As the PEARL context is
learnable, we found that when the context gradient back-
propagates through three networks (primary, dynamic and
the context network), it hampers the training. Instead, as a
base variable, the context’s gradient backpropagates only
via two networks as in the original PEARL implementation.
4. Recomposing the Policy in Meta-RL
4.1. Background
Meta-RL is the generalization of Meta-Learning (Mishra
et al., 2018; Sohn et al., 2019) to the RL domain. It aims at
learning meta-policies that solve a distribution of different
tasks p(T ). Instead of learning different policies for each
task, the meta-policy shares weights between all tasks and
thus can generalize from one task to the other (Sung et al.,
2017). A popular Meta-RL algorithm is MAML (Finn et al.,
2017), which learns a set of weights that can quickly adapt
to a new task with a few gradient ascent steps. To do so,
for each task, it estimates the policy gradient (Sutton et al.,
2000) at the adaptation point. The total gradient is the sum
of policy gradients over the task distribution p(T ):
∇φJmaml(φ) = E{
Ti∼p(T )
πφi
}
" ∞
X
t=0
ˆAi,t∇φ log πφi(at|st)
#
φi = φ + ηEπφ
" ∞
X
t=0
ˆAi,t∇φ log πφi(at|st)
#
,
(15)
where ˆAi,t is the empirical advantage estimation at the t-th
step in task i (Schulman et al., 2015b). On-policy algorithms
tend to suffer from high sample complexity as each update
step requires many new trajectories sampled from the most
recent policy in order to adequately evaluate the gradient
direction.
Off-policy methods are designed to improve the sample com-
plexity by reusing experience from old policies (Thomas
and Brunskill, 2016). Although not necessarily related, in
Meta-RL, many off-policy algorithms also avoid the MAML
approach of weight adaptation. Instead, they opt to condi-
tion the policy and the Q-function on a context which distin-
guishes between different tasks (Ren et al., 2019; Sung et al.,
2017). A notable off-policy Meta-RL method is PEARL
(Rakelly et al., 2019). It builds on top of the SAC algorithm
and learns a Q-function Qπ
θ (s, a, z), a policy πφ(s, z) and
a context z ∼qν(z|cTi). The context, which is a latent rep-
resentation of task Ti, is generated by a probabilistic model
that processes a trajectory cTi of (s, a, r) transitions sam-
pled from task Ti. To learn the critic alongside the context,
PEARL modiﬁes the SAC critic loss to
Lcritic
pearl (θ, ν) =
ET

Eqν(z|cTi)

Lcritic
sac
(θ, ν) + DKL
 qν(z|cTi)
p(z)

,
where p(z) is a prior probability over the latent distribution
of the context. While PEARL’s context is a probabilistic
model, other works (Fakoor et al., 2019) have suggested that
a deterministic learnable context can provide similar results.
In this work, we consider both a learnable context and also
the simpler approach of an oracle-context cTi which is a
unique, predeﬁned identiﬁer for task i (Jayakumar et al.,
2019). It can be an index when there is a countable number
of tasks or a continuous number when the tasks are sam-
pled from a continuous distribution. In practice, the oracle
identiﬁer is often known to the agent. Moreover, sometimes,
e.g., in goal-oriented tasks, the context cannot be recovered
directly from the transition tuples without prior knowledge,
since there are no rewards unless the goal is reached, which
rarely happens without policy adaptation.

Recomposing the Reinforcement Learning Building-Blocks with Hypernetworks
Figure 5. Visualizing gradient noise in MAML: The statistical population of the performance after 50 uncorrelated update steps is plotted
for 4 different time steps. Hyper-MAML refers to Hypernetwork where the oracle-context is the meta variable and the state features are
the base variable. Context-MAML refers to MLP policy where the oracle-context is concatenated with the state features.
4.2. Our Approach
Hypernetworks naturally ﬁt into the meta-learning formu-
lation where the context is an input to the primary network
(von Oswald et al., 2019; Zhao et al., 2020). Therefore, we
suggest modeling meta-policies s.t. the context is the meta
variable and the state is the dynamic’s input
πφ(a|s, c) = µw(c)(ε|s) s.t. ε ∼pε.
(16)
Interestingly, this modeling disentangles the state dependent
gradient and the task dependent gradient of the meta-policy.
To see that, let us take for example the on-policy objective of
MAML and plug in a context dependent policy πφ(a|s, c) =
µφ(ε|s, c). Then, the objective in Eq. (15) becomes
J(φ) =
X
Ti
X
sj∈Ti
ˆAi,j
∇φµφi(εj|sj, ci)
µφi(εj|sj, ci)
.
(17)
Applying the Hypernetwork modeling of the meta-policy in
Eq. (16), this objective can be written as
J(φ) =
X
Ti
∇φw(ci) ·
X
sj∈Ti
ˆAi,j
∇wµw(ci)(εj|sj)
µw(ci)(εj|sj)
(18)
In this form, the state-dependent gradients of the dynamic
weights ∇wµw(ci)(εj, sj) are averaged independently for
each task, and the task-dependent gradients of the primary
weights ∇φw(ci) are averaged only over the task distribu-
tion and not over the joint task-state distribution as in Eq.
(17). We postulate that such disentanglement reduces the
gradient noise for the same number of samples. This should
translate to more accurate learning steps and thus a more
efﬁcient learning process.
To test our hypothesis, we trained two different meta-policy
models based on the MAML algorithm: (1) an MLP model
where a state and an oracle-context are joined together; and
(2) a Hypernetwork model, as described, with an oracle-
context as a meta-variable. Importantly, note that, other
than the neural architecture, both algorithms are identical.
For four different timestamps during the learning process,
we constructed 50 different uncorrelated gradients from
different episodes and evaluating the updated policy’s per-
formance. We take the performance statistics of the updated
policies as a surrogate for the gradient noise. In Fig. 5, we
plot the performance statistics of the updated meta-policies.
We ﬁnd that the variance of the Hypernetwork model is
signiﬁcantly lower than the MLP model across all tasks
and environments. This indicates more efﬁcient improve-
ment and therefore we also observe that the mean value is
consistently higher.
5. Experiments
5.1. Experimental Setup
We conducted our experiments in the MuJoCo simulator
(Todorov et al., 2012) and tested the algorithms on the bench-
mark environments available in OpenAI Gym (Brockman
et al., 2016). For single task RL, we evaluated our method
on the: (1) Hooper-v2; (2) Walker2D-v2; (3) Ant-v23; and
(4) Half-Cheetah-v2 environments. For meta-RL, we eval-
uated our method on the: (1) Half-Cheetah-Fwd-Back and
(2) Ant-Fwd-Back, and on velocity tasks: (3) Half-Cheetah-
Vel and (4) Ant-Vel as is done in (Rakelly et al., 2019).
We also added the Half-Cheetah-Vel-Medium environment
as presented in (Fakoor et al., 2019), which tests out-of-
distribution generalization abilities. For Context-MAML
and Hyper-MAML we adopted the oracle-context as dis-
cussed in Sec. 4. For the forward-backward tasks, we
provided a binary indicator, and for the velocity tasks, we
adopted a continuous context in the range [0, 3] that maps
to the velocities in the training distribution.
In the RL experiments, we compared our model to SAC
and TD3, and in Meta-RL, we compared to MAML and
PEARL. We used the authors’ ofﬁcial implementations (or
open-source PyTorch (Ketkar, 2017) implementation when
the ofﬁcial one was not available) and the original base-
lines’ hyperparameters, as well as strictly following each
algorithm evaluation procedure. The Hypernetwork training
was executed with the baseline loss s.t. we changed only
the networks model and adjusted the learning rate to ﬁt the

Recomposing the Reinforcement Learning Building-Blocks with Hypernetworks
different architecture. All experiments were averaged over
5 seeds. Further technical details are in the appendix.
5.2. The Hypernetwork Architecture
Our Hypernetwork model is illustrated in Fig. 1 and in Sec.
2. When designing the Hypernetwork model, we did not
search for the best performance model, rather we sought a
proper comparison to the standard MLP architecture used in
RL (denoted here as MLP-Standard). To that end, we used
a smaller dynamic network than the MLP model (single
hidden layer instead of two layers and the same number
of neurons (256) in a layer). With this approach, we wish
to show the gain of using dynamic weights with respect to
a ﬁxed set of weights in the MLP model. To emphasize
the gain of the dynamic weights, we added an MLP-Small
baseline with equal conﬁguration to the dynamic model (one
hidden layer with 256 neurons).
Unlike the dynamic network, the role of the primary net-
work is missing from the MLP architecture. Therefore, for
the primary network, we used a high-performance ResNet
model (Srivastava et al., 2015) which we found apt for gener-
ating the set of dynamic weights (Glorot and Bengio, 2010).
To make sure that the performance gain is not due to the
expressiveness of the ResNet model or the additional num-
ber of learnable weights, we added three more baselines:
(1) ResNet Features: the same primary and dynamic ar-
chitecture, but the output of the primary is a state feature
vector which is concatenated to the action as the input for an
MLP-Standard network; (2) MLP-Large: two hidden layers,
each with 2900 neurons which sum up to 9M weights as
in the Hypernetwork architecture; and (3) Res35: ResNet
with 35 blocks to yield the Q-value, which sum up to 4.5M
weights. In addition, we added a comparison to the Q-D2RL
model: a deep dense architecture for the Q-function which
was recently suggested in (Sinha et al., 2020).
One important issue with Hypernetworks is their numerical
stability. We found that they are speciﬁcally sensitive to
weight initialization as bad primary initialization may am-
plify into catastrophic dynamic weights (Chang et al., 2019).
We solved this problem by initializing the primary s.t. the
average initial distribution dynamic weights resembles the
Kaiming-uniform initialization (He et al., 2015). Further
details can be found in the appendix.
5.3. Results
The results and the comparison to the baselines are summa-
rized in Fig. 6. In all four experiments, our Hypernetwork
model achieves an average of 10% - 70% gain over the MLP-
Standard baseline in the ﬁnal performance and reaches the
3We reduced the control cost as is done in PEARL (Rakelly
et al., 2019) to avoid numerical instability problems.
(a)
SAC
(b)
TD3
(c)
MAML
(d)
PEARL
Figure 6. The mean normalized score with respect to different base-
line models: (a) SAC; (b) TD3; (c) MAML; and (d) PEARL. The
Hypernetwork consistently improves all baselines in all algorithms.

Recomposing the Reinforcement Learning Building-Blocks with Hypernetworks
baseline’s score, with only 20%-70% of the total training
steps. As described in Sec. 5.2, for the RL experiments, in
addition to the MLP-Standard model, we tested ﬁve more
baselines: (1) MLP-Large; (2) MLP-Small; (3) ResNet Fea-
tures; (4) ResNet35; and (5) Q-D2RL. Both on TD3 and
SAC, we ﬁnd a consistent improvement over all baselines
and SA-Hyper outperforms in all environments with two
exceptions: where MLP-Large or Q-D2RL achieve a better
score than SA-Hyper in the Ant-v2 environment (the learn-
ing curves for each environment are found in the appendix).
While it may seem like the Hypernetwork improvement is
due to its large parametric dimension or the ResNet design
of the primary model, our results provide strong evidence
that this assumption is not true. The SA-Hyper model out-
performs other models with the same number of parameters
(MLP-Large and ResNet Features4) and also models that em-
ploy ResNet architectures (ResNet Features and Res35). In
addition, it is as good (SAC) or better (TD3) than Q-D2RL,
which was recently suggested as an architecture tailored for
the RL problem (Sinha et al., 2020). Please note that as
discussed in Sec. 5.2 and unlike D2RL, we do not optimize
the number of layers in the dynamic model.5
In Fig. 6c we compared different models for MAML: (1)
Vanilla-MAML; (2) Context-MAML, i.e. a context-based
version of MAML with an oracle-context; and (3) Hyper-
MAML, similar to context-MAML but with a Hypernetwork
model. For all models, we evaluated both the pre-adaptation
(pre-ad) as well as the post-adaptation scores. First, we
verify the claim in (Fakoor et al., 2019) that context beneﬁts
Meta-RL algorithms just as Context-MAML outperforms
Vanilla-MAML. However, we ﬁnd that Hyper-MAML out-
performs Context-MAML by roughly 50%. Moreover, un-
like the standard MLP models, we ﬁnd that Hyper-MAML
does not require any adaptation step (no observable dif-
ference between the pre- and post-adaptation scores). We
assume that this result is due to the better generalization
capabilities of the Hypernetwork architecture as can also be
seen from the next PEARL experiments.
In Fig. 6d we evaluated the Hypernetwork model with the
PEARL algorithm. The context is learned with a proba-
bilistic encoder as presented in (Rakelly et al., 2019) s.t.
the only difference with the original PEARL is the policy
and critic neural models. The empirical results show that
4Interestingly, The Resnet Features baseline achieved very low
scores even as compared to the MLP-Standard baseline. Indeed,
this result is not surprising as the action gradient model of Resnet
Features is identical to the action gradient model of MLP-Small
(single hidden layer with 256 neurons). While ResNet generated
state features may improve the Q-function estimation, they do not
necessarily improve the gradient estimation ∇aQπ as the network
is not explicitly trained to model the gradient.
5We do not compare to the full D2RL model which also modi-
ﬁes the policy architecture as our SA-Hyper model only changes
the Q-net model.
Hyper-PEARL outperforms the MLP baseline both in the ﬁ-
nal performance (15%) and in sample efﬁciency (70% fewer
steps to reach the ﬁnal baseline score). Most importantly,
we ﬁnd that Hyper-PEARL generalizes better to the unseen
test tasks. This applies both to test tasks sampled from the
training distribution (as the higher score and lower variance
of Hyper-PEARL indicate) and also to Out-Of-Distribution
(OOD) tasks, as can be observed in Fig. 7.
HalfCheetah-Vel-Medium (OOD)
Figure 7. PEARL results in an Out Of Distribution environment,
HalfCheetah-Vel-Medium, where the training tasks’ target is
[0,2.5] and the test tasks’ target is [2.5,3]. The Hypernetwork
achieved slightly lower returns over the training tasks, yet it gener-
alizes better over the OOD test tasks.
6. Conclusions
In this work, we set out to study neural models for the RL
building blocks: Q-functions and meta-policies. Arguing
that the unique nature of the RL setting requires unconven-
tional models, we suggested the Hypernetwork model and
showed empirically several signiﬁcant advantages over MLP
models. First, Hypernetworks are better able to estimate
the parametric gradient signal of the Q-function required to
train actor-critic algorithms. Second, they reduce the gradi-
ent variance in training meta-policies in Meta-RL. Finally,
they improve OOD generalization and they do not require
any adaptation step in Meta-RL training, which signiﬁcantly
facilitates the training process.
7. Code
Our Hypernetwork PyTorch implementation is found at
https://github.com/keynans/HypeRL.

Recomposing the Reinforcement Learning Building-Blocks with Hypernetworks
References
Brock, A., Lim, T., Ritchie, J., and Weston, N. (2018).
Smash: One-shot model architecture search through hy-
pernetworks. In International Conference on Learning
Representations.
Brockman, G., Cheung, V., Pettersson, L., Schneider, J.,
Schulman, J., Tang, J., and Zaremba, W. (2016). Openai
gym. CoRR, abs/1606.01540.
Chang, O., Flokas, L., and Lipson, H. (2019). Principled
weight initialization for hypernetworks. In International
Conference on Learning Representations.
Dean, T. and Givan, R. (1997). Model minimization in
markov decision processes. In AAAI/IAAI, pages 106–
111.
Fakoor, R., Chaudhari, P., Soatto, S., and Smola, A. J.
(2019). Meta-q-learning. In International Conference on
Learning Representations.
Finn, C., Abbeel, P., and Levine, S. (2017). Model-agnostic
meta-learning for fast adaptation of deep networks. In
ICML.
Fujimoto, S., Meger, D., and Precup, D. (2019). Off-policy
deep reinforcement learning without exploration. In Inter-
national Conference on Machine Learning, pages 2052–
2062.
Fujimoto, S., Van Hoof, H., and Meger, D. (2018). Address-
ing function approximation error in actor-critic methods.
arXiv preprint arXiv:1802.09477.
Galanti, T. and Wolf, L. (2020a). Comparing the parameter
complexity of hypernetworks and the embedding-based
alternative. arXiv preprint arXiv:2002.10006.
Galanti, T. and Wolf, L. (2020b). On the modularity of hy-
pernetworks. Advances in Neural Information Processing
Systems, 33.
Glorot, X. and Bengio, Y. (2010). Understanding the difﬁ-
culty of training deep feedforward neural networks. In
Proceedings of the thirteenth international conference
on artiﬁcial intelligence and statistics, pages 249–256.
JMLR Workshop and Conference Proceedings.
Grondman, I., Busoniu, L., Lopes, G. A., and Babuska, R.
(2012). A survey of actor-critic reinforcement learning:
Standard and natural policy gradients. IEEE Transactions
on Systems, Man, and Cybernetics, Part C (Applications
and Reviews), 42(6):1291–1307.
Ha, D., Dai, A., and Le, Q. V. (2016). Hypernetworks.
arXiv, pages arXiv–1609.
Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S. (2018).
Soft actor-critic: Off-policy maximum entropy deep rein-
forcement learning with a stochastic actor. arXiv preprint
arXiv:1801.01290.
Hasselt, H. v., Guez, A., and Silver, D. (2016). Deep re-
inforcement learning with double q-learning. In Pro-
ceedings of the Thirtieth AAAI Conference on Artiﬁcial
Intelligence, pages 2094–2100.
He, J., Chen, J., He, X., Gao, J., Li, L., Deng, L., and
Ostendorf, M. (2016). Deep reinforcement learning with
a natural language action space. In ACL (1).
He, K., Zhang, X., Ren, S., and Sun, J. (2015). Delving
deep into rectiﬁers: Surpassing human-level performance
on imagenet classiﬁcation. In ICCV.
Huang, G., Liu, Z., Van Der Maaten, L., and Weinberger,
K. Q. (2017). Densely connected convolutional networks.
In Proceedings of the IEEE conference on computer vi-
sion and pattern recognition, pages 4700–4708.
Huang, Y., Xie, K., Bharadhwaj, H., and Shkurti, F. (2020).
Continual model-based reinforcement learning with hy-
pernetworks. arXiv preprint arXiv:2009.11997.
Ilyas, A., Engstrom, L., Santurkar, S., Tsipras, D., Janoos,
F., Rudolph, L., and Madry, A. (2019). A closer look at
deep policy gradients. In International Conference on
Learning Representations.
Ioffe, S. and Szegedy, C. (2015).
Batch normalization:
Accelerating deep network training by reducing internal
covariate shift. In International conference on machine
learning, pages 448–456. PMLR.
Jayakumar, S. M., Czarnecki, W. M., Menick, J., Schwarz,
J., Rae, J., Osindero, S., Teh, Y. W., Harley, T., and
Pascanu, R. (2019). Multiplicative interactions and where
to ﬁnd them. In International Conference on Learning
Representations.
Jia, X., De Brabandere, B., Tuytelaars, T., and Gool, L. V.
(2016). Dynamic ﬁlter networks. In Advances in neural
information processing systems, pages 667–675.
Kaiser, Ł., Babaeizadeh, M., Mił]]os, P., Osi´nski, B., Camp-
bell, R. H., Czechowski, K., Erhan, D., Finn, C., Koza-
kowski, P., Levine, S., et al. (2019). Model based rein-
forcement learning for atari. In International Conference
on Learning Representations.
Kakade, S. and Langford, J. (2002). Approximately optimal
approximate reinforcement learning. In In Proc. 19th
International Conference on Machine Learning. Citeseer.
Ketkar, N. (2017). Introduction to pytorch. In Deep learning
with python, pages 195–208. Springer.

Recomposing the Reinforcement Learning Building-Blocks with Hypernetworks
Kumar, A., Fu, J., Soh, M., Tucker, G., and Levine, S.
(2019). Stabilizing off-policy q-learning via bootstrap-
ping error reduction. In Advances in Neural Information
Processing Systems, pages 11784–11794.
Lattimore, T. and Szepesv´ari, C. (2020). Bandit algorithms.
Cambridge University Press.
Lillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez, T.,
Tassa, Y., Silver, D., and Wierstra, D. (2016). Continu-
ous control with deep reinforcement learning. In ICLR
(Poster).
Lior Deutsch, Erik Nijkamp, Y. Y. (2019).
A genera-
tive model for sampling high-performance and diverse
weights for neural networks. CoRR.
Littwin, G. and Wolf, L. (2019). Deep meta functionals for
shape representation. In Proceedings of the IEEE Interna-
tional Conference on Computer Vision, pages 1824–1833.
McClelland, J. L. (1985). Putting knowledge in its place: A
scheme for programming parallel processing structures
on the ﬂy. Cognitive Science, 9(1):113–146.
Mishra, N., Rohaninejad, M., Chen, X., and Abbeel, P.
(2018). A simple neural attentive meta-learner. In Inter-
national Conference on Learning Representations.
Miyato, T., Kataoka, T., Koyama, M., and Yoshida, Y.
(2018). Spectral normalization for generative adversarial
networks. arXiv preprint arXiv:1802.05957.
Philipp, G., Song, D., and Carbonell, J. G. (2017). The
exploding gradient problem demystiﬁed-deﬁnition, preva-
lence, impact, origin, tradeoffs, and solutions. arXiv
preprint arXiv:1712.05577.
Potapov, A., Shcherbakov, O., Zhdanov, I., Rodionov, S.,
and Skorobogatko, N. (2018). Hypernets and their ap-
plication to learning spatial transformations. In Interna-
tional Conference on Artiﬁcial Neural Networks, pages
476–486. Springer.
Rakelly, K., Zhou, A., Finn, C., Levine, S., and Quillen, D.
(2019). Efﬁcient off-policy meta-reinforcement learning
via probabilistic context variables. In International con-
ference on machine learning, pages 5331–5340. PMLR.
Rashid, T., Samvelyan, M., Schroeder, C., Farquhar, G.,
Foerster, J., and Whiteson, S. (2018). Qmix: Mono-
tonic value function factorisation for deep multi-agent
reinforcement learning. In International Conference on
Machine Learning, pages 4295–4304.
Ratzlaff, N. and Li, F. (2019). Hypergan: A generative
model for diverse, performant neural networks. CoRR,
abs/1901.11058.
Ren, H., Garg, A., and Anandkumar, A. (2019). Context-
based meta-reinforcement learning with structured latent
space. Skills Workshop NeurIPS 2019.
Rusu, A. A., Rao, D., Sygnowski, J., Vinyals, O., Pascanu,
R., Osindero, S., and Hadsell, R. (2019). Meta-learning
with latent embedding optimization. In International
Conference on Learning Representations.
Salimans, T. and Kingma, D. P. (2016). Weight normaliza-
tion: A simple reparameterization to accelerate training of
deep neural networks. arXiv preprint arXiv:1602.07868.
Saraﬁan, E., Sinay, M., Louzoun, Y., Agmon, N., and Kraus,
S. (2020). Explicit gradient learning for black-box opti-
mization. In International Conference on Machine Learn-
ing, pages 8480–8490. PMLR.
Saremi, S. (2019). On approximating \∇f with neural
networks. arXiv preprint arXiv:1910.12744.
Schmidhuber, J. (1992). Learning to control fast-weight
memories: An alternative to dynamic recurrent networks.
Neural Computation, 4(1):131–139.
Schulman, J., Levine, S., Abbeel, P., Jordan, M., and Moritz,
P. (2015a). Trust region policy optimization. In Interna-
tional conference on machine learning, pages 1889–1897.
Schulman, J., Moritz, P., Levine, S., Jordan, M., and Abbeel,
P. (2015b).
High-dimensional continuous control us-
ing generalized advantage estimation. arXiv preprint
arXiv:1506.02438.
Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and
Klimov, O. (2017). Proximal policy optimization algo-
rithms. arXiv preprint arXiv:1707.06347.
Sinha, S., Bharadhwaj, H., Srinivas, A., and Garg, A. (2020).
D2rl: Deep dense architectures in reinforcement learning.
arXiv preprint arXiv:2010.09163.
Sohn, S., Woo, H., Choi, J., and Lee, H. (2019). Meta
reinforcement learning with autonomous inference of
subtask dependencies. In International Conference on
Learning Representations.
Srivastava, R. K., Greff, K., and Schmidhuber, J. (2015).
Training very deep networks. In NIPS.
Sun, Z., Ozay, M., and Okatani, T. (2017). Hypernetworks
with statistical ﬁltering for defending adversarial exam-
ples. arXiv preprint arXiv:1711.01791.
Sung, F., Zhang, L., Xiang, T., Hospedales, T., and Yang, Y.
(2017). Learning to learn: Meta-critic networks for sam-
ple efﬁcient learning. arXiv preprint arXiv:1706.09529.

Recomposing the Reinforcement Learning Building-Blocks with Hypernetworks
Sutton, R. S. and Barto, A. G. (2018). Reinforcement learn-
ing: An introduction. MIT press.
Sutton, R. S., McAllester, D. A., Singh, S. P., and Mansour,
Y. (2000). Policy gradient methods for reinforcement
learning with function approximation. In Advances in
neural information processing systems, pages 1057–1063.
Thomas, P. and Brunskill, E. (2016). Data-efﬁcient off-
policy policy evaluation for reinforcement learning. In
International Conference on Machine Learning, pages
2139–2148. PMLR.
Todorov, E., Erez, T., and Tassa, Y. (2012).
Mujoco:
A physics engine for model-based control.
In 2012
IEEE/RSJ International Conference on Intelligent Robots
and Systems, pages 5026–5033. IEEE.
von Oswald, J., Henning, C., Sacramento, J., and Grewe,
B. F. (2019). Continual learning with hypernetworks. In
International Conference on Learning Representations.
Williams, R. J. (1992). Simple statistical gradient-following
algorithms for connectionist reinforcement learning. Ma-
chine learning, 8(3-4):229–256.
Zai, A. and Brown, B. (2020). Deep reinforcement learning
in action. Manning Publications.
Zhang, S. and Sutton, R. S. (2017). A deeper look at experi-
ence replay. arXiv preprint arXiv:1712.01275.
Zhao, D., von Oswald, J., Kobayashi, S., Sacramento, J., and
Grewe, B. F. (2020). Meta-learning via hypernetworks.
4th Workshop on Meta-Learning at NeurIPS 2020.

Recomposing the Reinforcement Learning Building-Blocks with Hypernetworks
A. Proof of Proposition 1
Proposition 1. Let π(a|s) = µφ(ε|s) be a stochastic parametric policy with ε ∼pε and µφ(·|s) a transformation
with a Lipschitz continuous gradient and a Lipschitz constant κµ. Assume that its Q-function Qπ(s, a) has a Lipschitz
continuous gradient in a, i.e. |∇aQπ(s, a1) −∇aQπ(s, a2)| ≤κq∥a1 −a2∥. Deﬁne the average gradient operator
∇φf = Es∼D [Eε∼pε[∇φµφ(ε|s) · f(s, µφ(ε|s))]]. If there exists a gradient estimation g(s, a) and 0 < α < 1 s.t.
∥∇φ · g −∇φ · ∇aQπ∥≤α∥∇φ · ∇aQπ∥
(19)
then the ascent step φ′ ←φ + η∇φ · g with η ≤1
˜k
1−α
(1+α)2 yields a positive empirical advantage policy.
Proof. First, recall the objective to be optimized:
J(φ) = Es∼D [Eε∼pε [Qπ(s, µφ(ε; s))]]
∇φJ(φ) = Es∼D [Eε∼pε [∇φµφ(ε; s) · ∇aQπ(s, µφ(ε; s))]] = ∇φ · ∇aQπ
(20)
Notice that as Qπ is bounded by the maximal reward and its gradient is Lipschitz continuous, the gradient ∇aQπ is therefore
bounded. Similarly, since the action space is bounded, and the terministic transformation µπ has a Lipschitz continuous
gradient, it follows that ∇φµπ is also bounded. Deﬁne ∥∇aQπ∥≤σq and ∥∇φµπ∥≤σµ.
Lemma 1. Let A(x) : Rn →M k×l s.t. ∥A(x)∥≤Ma and ∥A(x1)−A(x2)∥≤α∥x1 −x2∥and ∥·∥is the induced vector
norm. And let b(x) : Rn →Rl s.t. ∥b(x)∥≤Mb and ∥b(x1) −b(x2)∥≤β∥x1 −x2∥. The operator c(x) = A(x) · b(x) :
Rn →Rk is Lipschitz with constant κc ≤αMa + βMb.
Proof.
∥c(x1) −c(x2)∥= ∥A(x1) · b(x1) −A(x2) · b(x2)∥
= ∥A(x1) · b(x1) −A(x1) · b(x2) + A(x1) · b(x2) −A(x2) · b(x2)∥
≤∥A(x1) · (b(x1) −b(x2))∥+ ∥(A(x1) −A(x2)) · b(x2)∥
≤∥A(x1)∥∥b(x1) −b(x2)∥+ ∥A(x1) −A(x2)∥∥b(x2)∥
≤(βMa + αMb) ∥x1 −x2∥
The Lipschitz constant of the objective gradient is bounded by
∥∇φJ(φ1) −∇φJ(φ2)∥= ∥E [∇φQπ(s, µφ1(ε; s)) −∇φQπ(s, µφ2(ε; s))]∥≤
E [∥∇φµφ1(ε; s) · ∇aQπ(s, µφ1(ε; s)) −∇φµφ2(ε; s) · ∇aQπ(s, µφ2(ε; s))∥]
Applying Lemma 1, we obtain
∥∇φJ(φ1) −∇φJ(φ2)∥≤(κqσµ + κµσq)∥φ1 −φ2∥.
Therefore, J(φ1) is also Lipschitz. Hence, applying Taylor’s expansion around φ, we have that
J(φ′) ≥J(φ) + (φ′ −φ) · ∇φJ(φ) −κ2
J
2 ∥φ′ −φ∥2 ≥J(φ) + (φ′ −φ) · ∇φJ(φ) −(κqσµ + κµσq)2
2
∥φ′ −φ∥2.
Plugging in the iteration φ′ ←φ + η∇φ · g we obtain
J(φ′) ≥J(φ) + η
 ∇φ · g

·
 ∇φ · Qπ
−η2(κqσµ + κµσq)2
2
∥∇φ · g∥2.
(21)
Taking the second term on the right-hand side,
 ∇φ · g

·
 ∇φ · Qπ
=
 ∇φ · Qπ −(∇φ · g −∇φ · Qπ)

·
 ∇φ · Qπ
≥
∇φ · Qπ2 −
 ∇φ · g −∇φ · Qπ
·
 ∇φ · Qπ
≥
∇φ · Qπ2 −
∇φ · g −∇φ · Qπ ·
∇φ · Qπ
≥(1 −α)
∇φ · Qπ2 .

Recomposing the Reinforcement Learning Building-Blocks with Hypernetworks
For the last term we have
∇φ · g
2 =
∇φ · Qπ −(∇φ · g −∇φ · Qπ)
2
=
∇φ · Qπ2 −2
 ∇φ · Qπ
· (∇φ · g −∇φ · Qπ) +
∇φ · g −∇φ · Qπ2
≤
∇φ · Qπ2 + 2
∇φ · Qπ ·
∇φ · g −∇φ · Qπ + α2 ∇φ · Qπ2
≤(1 + 2α + α2)
∇φ · Qπ2 .
Plugging both terms together into Eq. (21) we get
J(φ′) ≥J(φ) +
∇φ · Qπ2 
η(1 −α) −1
2η2(κqσµ + κµσq)2(1 + α)2

.
To obtain a positive empirical advantage we need
η(1 −α) −1
2η2(κqσµ + κµσq)2(1 + α)2 ≥0
Thus the sufﬁcient requirement for the learning rate is
η ≤1
˜k
1 −α
(1 + α)2 .
where ˜k = 1
2(κqσµ + κµσq).

Recomposing the Reinforcement Learning Building-Blocks with Hypernetworks
B. Cosine Similarity Estimation
To evaluate the averaged Cosine Similarity (CS)
cs(Qπ
θ ) = Es∼D

∇aQπ
θ (s, aµ) · ∇aQπ(s, aµ)
∥∇aQπ
θ (s, aµ)∥∥∇aQπ(s, aµ)∥

,
(22)
we need to estimate the local CS for each state. To that end, we estimate the “true” Q-function Qπ(s, a) at the vicinity of
a = aµ with a non-parametric local linear model
Qπ(s, a) ≃fs,aµ(a) = a · g
where g ∈RNa s.t. the Q-function gradient is constant ∇aQπ(s, a) ≃g. To ﬁt the linear model, we sample Nr unbiased
samples of the Q-function around aµ, i.e. qi = ˆQπ(s, ai). These samples are the empirical discounted sum of rewards
following execution of action ai = aµ + ∆i at state s and then applying policy π.
To ﬁt the linear model we directly ﬁt the constant model g for the gradient. Recall that applying the Taylor’s expansion
around aµ gives
Qπ(s, a) = Qπ(s, aµ) + (a −aµ) · ∇aQπ
θ (s, aµ) + O
 ∥a −aµ∥2
, therefore
Qπ(s, a2) −Qπ(s, a1) −(a2 −a1) · ∇aQπ
θ (s, aµ) = O
 ∥a2 −a1∥2
for a1, a2 at the vicinity of aµ.
To ﬁnd the best ﬁt g ≃∇aQπ
θ (s, aµ) we minimize averaged the quadratic error term over all pairs of sampled trajectories
g∗= arg min
g
Nr
X
i
Nr
X
j
|(aj −ai) · g −qj + qi|2.
This problem can be expressed in a matrix notation as
g∗= arg min
g
 ˜Xg −δ

2
,
where ˜X ∈RN 2
r ×Na is a matrix with N 2
r rows of all the vectors aj −ai and δ is a N 2
r element vector of all the differences
qj −qi. Its minimization is the Least-Mean-Squared Estimator (LMSE)
g∗= ( ˜XT ˜X)−1 ˜XT δ.
In our experiments we evaluated the CS every K = 104 learning steps and used Ns = 15, Nr = 15 and ∆i ∼N(0, 0.3) for
each evaluation. This choice trades off somewhat less accurate local estimators with more samples during training. To test
our gradient estimator, we ﬁrst applied it to the outputs of the Q-function network (instead of the true returns) and calculated
the CS between a linear model based on the network outputs and the network parametric gradient. The results in Fig. 8 show
that our g∗estimator obtains a high CS between the Q-net outputs of the SA-Hyper and MLP models and their respective
parametric gradients. This indicates that these networks are locally (∆∝0.3) linear. On the other hand, the CS between the
linear model based on the AS-Hyper outputs and its parametric gradient is lower, which indicates that the network is not
necessarily close to linear with ∆∝0.3. We assume that this may be because the action in the AS-Hyper conﬁguration
plays the meta-variable role which increases the non-linearity of the model with respect to the action input. Importantly,
note that this does not indicate that the true Q-function of the AS-Hyper model is more non-linear than other models.
In Fig. 9 we plot the CS for 4 different environments averaged with a window size of W = 20. The results show that on
average the SA-Hyper conﬁguration obtains a higher CS, which indicates that the policy optimization step is more accurate
s.t. the RL training process is more efﬁcient.

Recomposing the Reinforcement Learning Building-Blocks with Hypernetworks
0
100K 200K 300K 400K 500K 600K
steps
(a) Hopper
0.2
0.4
0.6
0.8
1.0
Cosine-Similarity
SA-Hyper(Ours)
MLP
AS-Hyper
0
100K 200K 300K 400K 500K 600K
steps
(b) Walker2d
0.0
0.2
0.4
0.6
0.8
1.0
SA-Hyper(Ours)
MLP
AS-Hyper
0
100K 200K 300K 400K 500K 600K
steps
(c) Ant
0.2
0.4
0.6
0.8
1.0
SA-Hyper(Ours)
MLP
AS-Hyper
0
100K 200K 300K 400K 500K 600K
steps
(d) HalfCheetah
0.2
0.4
0.6
0.8
1.0
SA-Hyper(Ours)
MLP
AS-Hyper
Figure 8. The Cosine-Similarity between the LMSE estimator of the Q-net outputs and the parametric gradient averaged with a window
size of W = 20.
0
100K 200K 300K 400K 500K 600K
steps
(a) Hopper
0.6
0.4
0.2
0.0
0.2
0.4
0.6
Cosine-Similarity
SA-Hyper(Ours)
MLP
AS-Hyper
0
100K 200K 300K 400K 500K 600K
steps
(b) Walker2d
0.4
0.2
0.0
0.2
0.4
SA-Hyper(Ours)
MLP
AS-Hyper
0
100K 200K 300K 400K 500K 600K
steps
(c) Ant
0.3
0.2
0.1
0.0
0.1
0.2
0.3
SA-Hyper(Ours)
MLP
AS-Hyper
0
100K 200K 300K 400K 500K 600K
steps
(d) HalfCheetah
0.4
0.3
0.2
0.1
0.0
0.1
0.2
0.3
0.4
SA-Hyper(Ours)
MLP
AS-Hyper
Figure 9. The Cosine-Similarity between the LMSE estimator of the empirical sum of rewards and the parametric gradient averaged with
a window size of W = 20.

Recomposing the Reinforcement Learning Building-Blocks with Hypernetworks
C. Gradient Step Noise Statistics in MAML
Hypernetworks disentangle the state-dependent gradient and the task-dependent gradient. As explained in the paper, we
hypothesized that these characteristics reduce the gradient ascent step noise during policy updates
φ ←φ −η b∇φJ
where b∇φJ is the gradient step estimation and η is the learning rate. It is not obvious how to deﬁne the gradient noise
properly as any norm-based measure depends on the network’s structure and size. Therefore, we take an alternative approach
and deﬁne the gradient noise as the performance statistics after applying a set of independent gradient steps. In simple
words, this deﬁnition essentially corresponds to how noisy the learning process is.
To estimate the performance statistics, we take N = 50 different independent policy gradients based on independent
trajectories at 4 different time steps during the training process. For each gradient step, we sampled 20 trajectories with a
maximal length of 200 steps (identical to a single policy update during the training process) out of 40 tasks. After each
gradient step, we evaluated the performance and restored the policy’s weights s.t. the gradient steps are independent.
We compared two different network architectures, both with access to an oracle context: (1) Hyper-MAML; and (2)
Context-MAML. We did not evaluate Vanilla-MAML as it has no context and the gradient noise, in this case, might also be
due to higher adaptation noise as the context must be recovered from the trajectories’ rewards. In the paper, we presented
the performance statistics after N = 50 different updates. In Table 1 we present the variance of those statistics.
Table 1. The gradient coefﬁcient of variation
σ
|µ| and the variance (in brackets) in MAML. Hyper-MAML refers to Hypernetwork policy
where the oracle-context is the meta-variable and the state features are the base-variable. Context-MAML refers to the MLP model policy
where the oracle-context is concatenated with the state features. To compare between different policies with different reward scales, we
report both the coefﬁcient of variation and the variance in brackets.
Envrionment
50 iter
150 iter
300 iter
450 iter
HalfCheetah-Fwd-Back
Context-MAML
1.184 (774)
4.492 (2595)
2.590 (1891)
0.822 (3689)
Hyper MAML (Ours)
0.027 (26)
0.017 (43)
0.021 (96)
0.014 (53)
HalfCheetah-Vel
Context-MAML
0.035 (122)
0.050 (208)
0.093 (520)
0.066 (161)
Hyper MAML (Ours)
0.009 (5)
0.005 (1)
0.008 (2)
0.009 (2)
Ant-Fwd-Back
Context-MAML
0.274 (3)
0.199 (5)
0.400 (12)
0.285 (20)
Hyper MAML (Ours)
0.073 (1)
0.047 (2)
0.050 (6)
0.047 (11)
Ant-Vel
Context-MAML
0.379 (52)
0.377 (8)
0.628 (109)
0.418 (117)
Hyper MAML (Ours)
0.252 (5)
0.159 (2)
0.080 (2)
0.057 (2)

Recomposing the Reinforcement Learning Building-Blocks with Hypernetworks
D. Models Design
D.1. Hypernetwork Architecture
The Hypernetwork’s primary part is composed of three main blocks followed by a set of heads. Each block contains an
up-scaling linear layer followed by two pre-activation residual linear blocks (ReLU-linear-ReLU-linear). The ﬁrst block
up-scales from the state’s dimension to 256 and the second and third blocks grow to 512 and 1024 neurons respectively. The
total number of learnable parameters in the three blocks is ∼6.5M. The last block is followed by the heads which are a set
of linear transformations that generate the ∼2K dynamic parameters (including weights, biases and gains). The heads have
∼2.5M learnable parameters s.t. the total number of parameters in the primary part is ∼9M.
D.2. Primary Model Design: Negative Results
In our search for a primary network that can learn to model the weights of a state-dependent dynamic Q-function, we
experimented with several different architectures. Here we outline a list of negative results, i.e. models that failed to learn
good primary networks.
1. we tried three network architecture: (1) MLP; (2) Dense Blocks (Huang et al., 2017); and (3) ResNet Blocks (He et al.,
2016). The MLP did not converge and the dense blocks were sensitive to the initialization with spikes in the policy’s
gradient which led to an unstable learning process.
2. We found that the head size (the last layer that outputs all the dynamic network weights) should not be smaller than
512 and the depth should be at least 5 blocks. Upsampling from the low state dimension can either be done gradually
or at the ﬁrst layer.
3. We tried different normalization schemes: (1) weight normalization (Salimans and Kingma, 2016); (2) spectral
normalization (Miyato et al., 2018); and (3) batch normalization (Ioffe and Szegedy, 2015). All of them did not help
and slowed or stopped the learning.
4. For the non-linear activation functions, we tried RELU and ELU which we found to have similar performances.
D.3. Hypernetwork Initialization
A proper initialization for the Hypernetwork is crucial for the network’s numerical stability and its ability to learn. Common
initialization methods are not necessarily suited for Hypernetworks (Chang et al., 2019) since they fail to generate the
dynamic weights in the correct scale. We found that some RL algorithms are more affected than others by the initialization
scheme, e.g, SAC is more sensitive than TD3. However, we leave this question of why some RL algorithms are more
sensitive than others to the weight initialization for future research.
To improve the Hypernetwork weight initialization, we followed (Lior Deutsch, 2019) and initialized the primary weights
with smaller than usual values s.t. the initial dynamic weights were also relatively small compared to standard initialization
(Fig. 10). As is shown in Fig. 11, this enables the dynamic weights to converge during the training process to a relatively
similar distribution of a normal MLP network.
The residual blocks in the primary part were initialized with a fan-in Kaiming uniform initialization (He et al., 2015) with a
gain of
1
√
12 (instead of the normal gain of
√
2 for the ReLU activation). We used ﬁxed uniform distributions to initialize the
weights in the heads: U(−0.05, 0.05) for the ﬁrst dynamic layer, U(−0.008, 0.008) for the second dynamic layer and for
the standard deviation output layer in the PEARL meta-policy we used the U(−0.001, 0.001) distribution.
In Fig. 10 and Fig. 11 we plot the histogram of the TD3 critic dynamic network weights with different primary initializations:
(1) our custom primary initialization; and (2) The default Pytorch initialization of the primary network. We compare the
dynamic weights to the weights of a standard MLP-Small network (the same size as the dynamic network). We take two
snapshots of the weight distribution: (1) in Fig. 10 before the start of the training process; and (2) after 100K training steps.
In Table 2 we also report the total-variation distance between each initialization and the MLP-Small weight distribution.
Interestingly, the results show that while the dynamic weight distribution with the Pytorch primary initialization is closer
to the MLP-Small distribution at the beginning of the training process, after 100K training steps our primary initialized
weights produce closer dynamic weight distribution to the MLP-Small network (also trained for 100K steps).

Recomposing the Reinforcement Learning Building-Blocks with Hypernetworks
0.3
0.2
0.1
0.0
0.1
0.2
0.3
0
2
4
6
8
10
Hopper- Weights of Layer 1
0.8
0.6
0.4
0.2
0.0
0.2
0.4
0.6
0.8
0
2
4
6
Walker2d- Weights of Layer 1
0.20
0.15
0.10
0.05 0.00
0.05
0.10
0.15
0.20
0
2
4
6
8
10
12
Ant- Weights of Layer 1
0.4
0.2
0.0
0.2
0.4
0
2
4
6
8
10
HalfCheetah- Weights of Layer 1
0.3
0.2
0.1
0.0
0.1
0.2
value
0
10
20
30
40
50
Hopper- Weights of Layer 2
0.8
0.6
0.4
0.2
0.0
0.2
0.4
0.6
value
0
10
20
30
Walker2d- Weights of Layer 2
0.15
0.10
0.05
0.00
0.05
0.10
0.15
value
0
10
20
30
40
50
60
Ant- Weights of Layer 2
0.4
0.3
0.2
0.1
0.0
0.1
0.2
0.3
value
0
10
20
30
40
50
HalfCheetah- Weights of Layer 2
Ours Hyper init
Pytorch Hyper init
Pytorch MLP init
Figure 10. Dynamic network weight distribution of different initialization schemes at the beginning of the training. The “Hyper init”
refers to a primary network initialized with our suggested initialization scheme. ’Pytorch Hyper init’ refers to the Pytorch default
initialization of the primary network and “Pytorch MLP init” refers to the Pytorch default initialization of the MLP-Small model (same
architecture as the dynamic network).
4
2
0
2
4
6
0.0
0.2
0.4
0.6
0.8
1.0
1.2
Hopper- Weights of Layer 1
3
2
1
0
1
2
3
0.00
0.25
0.50
0.75
1.00
1.25
1.50
Walker2d- Weights of Layer 1
2
1
0
1
2
0
1
2
3
4
Ant- Weights of Layer 1
10.0
7.5
5.0
2.5
0.0
2.5
5.0
7.5
0.0
0.5
1.0
1.5
HalfCheetah- Weights of Layer 1
6
4
2
0
2
value
0.0
0.2
0.4
0.6
0.8
1.0
1.2
Hopper- Weights of Layer 2
6
4
2
0
2
4
value
0.0
0.2
0.4
0.6
0.8
1.0
Walker2d- Weights of Layer 2
4
2
0
2
4
value
0.0
0.5
1.0
1.5
2.0
2.5
Ant- Weights of Layer 2
6
4
2
0
2
4
value
0.00
0.25
0.50
0.75
1.00
1.25
1.50
HalfCheetah- Weights of Layer 2
Ours Hyper init
Pytorch Hyper init
Pytorch MLP init
Figure 11. Dynamic network weight distribution of different initialization schemes after 100K training steps. The “Hyper init” refers to
a primary network initialized with our suggested initialization scheme. ’Pytorch Hyper init’ refers to the Pytorch default initialization of
the primary network and ’Pytorch MLP init’ weight distribution of the MLP-Small model (same architecture as the dynamic network).
D.4. Baseline Models for the SAC and TD3 algorithms
In our TD3 and SAC experiments, we tested the Hypernetwork architecture with respect to 7 different baseline models.
D.4.1. MLP-STANDARD
A standard MLP architecture, which is used in many RL papers (e.g. SAC and TD3) with 2 hidden layers of 256 neurons
each with ReLU activation function.
D.4.2. MLP-SMALL
The MLP-Small model helps in understanding the gain of using context-dependent dynamic weights. It is an MLP network
with the same architecture as our dynamic network model, i.e. 1 hidden layer with 256 neurons followed by a ReLU
activation function. As expected, although the MLP-Small and MLP-Standard conﬁgurations are relatively similar with
only a different number of hidden layers (1 and 2 respectively), the MLP-Small achieved close to half the return of the
MLP-Standard. However, our experiments show that when using even a shallow MLP network with context-dependent
weights (i.e. our SA-Hyper model), it can signiﬁcantly outperform both shallow and deeper standard MLP models.

Recomposing the Reinforcement Learning Building-Blocks with Hypernetworks
Table 2. Total-variation distance between the dynamic weight distribution and the “Pytorch MLP init” weight distribution: at the beginning
of the training process we ﬁnd that ’Pytorch Hyper init’ is closer to the ’Pytorch MLP init’ weight distribution while after 100K training
steps we ﬁnd that our initialization is closer to the ’Pytorch MLP init’ weights (also trained for 100K steps).
Primary Initialization Scheme
Hopper
Walker2d
Ant
HalfCheetah
First Layer
Ours Hyper init
31.4
23.9
13.6
29.4
Pytorch Hyprer init
16.3
20.5
9.2
8.8
Second Layer
Ours Hyper init
34.8
30.77
37.7
36.9
Pytorch Hyprer init
24.7
39.6
11.2
29.3
First Layer After 100K Steps
Ours Hyper init
14.4
19.6
29.9
16.4
Pytorch Hyprer init
24.9
22.6
34.0
22.4
Second Layer After 100K Steps
Ours Hyper init
31.2
28.5
30.6
21.1
Pytorch Hyprer init
32.11
20.8
30.7
31.1
D.4.3. MLP-LARGE
To make sure that the performance gain is not due to the large number of weights in the primary network, we evaluated
MLP-Large, an MLP network with 2 hidden layers as the MLP-Standard but with 2,900 neurons in each layer. This yields a
total number of ∼9M learnable parameters, as in our entire primary model. While this large network usually outperformed
other baselines, in almost all environments it still did not reach the Hypernetwork performance with one exception in the
Ant-v2 environment in the TD3 algorithm. This provides another empirical argument that Hypernetworks are more suited
for the RL problem and their performance gain is not only due to their larger parametric space.
D.4.4. RESNET FEATURES
To test whether the performance gain is due to the expressiveness of the ResNet model, we evaluated ResNet-Features: an
MLP-Small model but instead of plugging in the raw state features, we use the primary model conﬁguration (with ResNet
blocks) to generate 10 learnable features of the state. Note that the feature extractor part of ResNet-Features has a similar
parameter space as the Hypernetwork’s primary model except for the head units. The ResNet-Features was unable to learn
on most environments in both algorithms, even though we tried several different initialization schemes. This shows that
the primary model is not suitable for a state’s features extraction, and while it may be possible to ﬁnd other models with
ResNet that outperform this ResNet model, it is yet further evidence that the success of the Hypernetwork architecture is not
attributed solely to the ResNet expressiveness power in the primary network.
D.4.5. AS-HYPER
This is the reverse conﬁguration of our SA-Hyper model. In this conﬁguration, the action is the meta-variable and the state
serves as the base-variable. Its lower performance provides another empirical argument (alongside the lower CS, see Sec. B)
that the “correct” Hypernetwork composition is when the state plays the context role and the action is the base-variable.
D.4.6. EMB-HYPER
In this conﬁguration, we replace the input of the primary network with a learnable embedding of size 5 (equal to the PEARL
context size) and the dynamic part gets both the state and the action as its input variables. This produces a learnable set of
weights that is constant for all states and actions. However, unlike MLP-Small, the weights are generated via the primary
model and are not independent as in normal neural network training. Note that we did not include this experiment in the

Recomposing the Reinforcement Learning Building-Blocks with Hypernetworks
main paper but we have added it to the results in the appendix. This is another conﬁguration that aims to validate that the
Hypernetwork gain is not due to the over-parameterization of the primary model and that the disentanglement of the state
and action is an important ingredient of the Hypernetwork performance.
D.4.7. RESNET 35
To validate that the performance gain is not due to a large number of weights in the primary network combined with the
expressiveness of the residual blocks, we evaluated a full ResNet architecture: The state and actions are concatenated and
followed by 35 ResNet blocks. Each block contains two linear layers of 256 size (and an identity path). This yields a a total
number of ∼4.5M learnable parameters, which is half of the 9M parameters in the Hypernetwork model. In almost all
environments it underperformed both with respect to SA-Hyper and also with respect to the MLP-Standard baseline.
D.4.8. Q-D2RL
The Deep Dense architecture (D2RL) (Sinha et al., 2020) suggests to add skip connections from the input to each hidden
layer. In the original paper this applies both to the Q-net model, where states and actions are concatenated and added to each
hidden layer, and to policies where only states are added to each hidden layer. According to the paper, the best performing
model contains 4 hidden layers. Here, we compared to Q-D2RL which only modiﬁes the Q-net as our SA-Hyper model
but does not alter the policy network. Q-D2RL shows an inconsistent performance between SAC and TD3. In the SAC
algorithm, it performs close to the SA-Hyper in all environments. On the other hand, in the TD3 algorithm, Q-D2RL was
unable to reach the SA-Hyper performance in any environment.
D.5. Complexity and Run Time Considerations
Modern deep learning packages such as Pytorch and Tensorﬂow currently do not have optimized implementation of
Hypernetworks as opposed to conventional neural architectures such as CNN or MLP. Therefore, it is not surprising that
the training of Hypernetwork can take a longer time than MLP models. However, remarkably, in MAML we were able
to reduce the training time as the primary weights and gradients are calculated only once for each task and the dynamic
network is smaller than the Vanilla-MAML MLP network. Therefore, within each task, both data collection and gradient
calculation with the dynamic model requires less time than the Vanilla-MAML network. In Table D.5 we summarize the
average training time of each algorithm and compare the Hyper and MLP conﬁgurations.
Table 3. Comparing the algorithms’ average running time between Hyper and MLP models: Single iteration training time for the MAML
algorithm and 5K steps training time for all other algorithms. Note that each agent was trained using a single NVIDIA® GeForce® RTX
2080 Ti GPU with a 11019 MiB memory.
Algorithm
MLP
Hyper
SAC
120s
200s
TD3
40s
140s
PEARL
450s
700s
MAML
150s
145s
Multi-Task MAML
-
120s

Recomposing the Reinforcement Learning Building-Blocks with Hypernetworks
E. Experiments
In this section, we report the training results of all tested algorithms as well as the hyperparameters used in these experiments.
For each algorithm, we plot the mean reward and standard deviation over ﬁve different seeds. The evaluation procedure of
single task RL algorithms was done every 5K training steps, with a mean calculated over ten independent trajectory roll-outs,
without exploration, as described in (Fujimoto et al., 2018). The evaluation procedure of the Meta-RL algorithms was done
after every algorithm’s iteration, with a mean calculated over all test tasks’ roll-outs, as was done in (Rakelly et al., 2019).
In ’Velocity’ tasks in Meta-RL, we sample training and test tasks from [0, 3] except for the HalfCheetah-Vel-Medium(OOD)
environment which the training tasks sample from [0, 2.5] and the test tasks sample from [2.5, 3]. We used 100 training
tasks and 30 tests tasks for both algorithms (PEARL and MAML) on “Velocity” tasks and 2 tasks for the “Direction” tasks,
forward and backward.
E.1. TD3
(a) Hopper
(b) Walker2d
(d) HalfCheetah
(c) Ant
Figure 12. TD3 performance of different MLP architectures compared to the SA-Hyper. SA-Hyper shows consistent high performance in
all environments and outperforms all other architectures except for the Ant environment.
(a) Hopper
(b) Walker2d
(c) Ant
(d) HalfCheetah
Figure 13. TD3 Performance with Hypernetwork critic compared to MLP critic over different ’Mujoco’ environments. In all environments,
Hypernetwork outperforms all the baselines.

Recomposing the Reinforcement Learning Building-Blocks with Hypernetworks
Table 4. TD3 highest rewards.
Network
Hopper
Walker2D
Ant
HalfCheetah
MLP-Standard
3256 ± 211
3449 ± 730
3524 ± 617
10384 ± 923
MLP-Large
3156 ± 368
4527 ± 397
6042 ± 731
9467 ± 1978
MLP-Small
1756 ± 926
1799 ± 538m
3215 ± 267
6071 ± 256
ResNet-Features
307 ± 173
343 ± 349
1001 ± 1
2474 ± 2184
ResNet35
2213 ± 1431
4411 ± 703
4042 ± 215
9621 ± 1072
Q-D2RL
3347 ± 270
4408 ± 473
3736 ± 881
10023 ± 867
AS-Hyper
2633 ± 391
1905 ± 985
4513 ± 759
7669 ± 667
Emb-Hyper
2261 ± 728
2446 ± 676
2949 ± 741
6915 ± 374
SA-Hyper (Ours)
3418 ± 318
5412 ± 445
4660 ± 1194
11423 ± 560
Table 5. TD3 Hyper Parameters
Hyper-parameter
TD3
Hyper TD3 (Ours)
Actor Learning Rate
3e−4
3e−4
Critic Learning Rate
3e−4
5e−5
Optimizer
Adam
Adam
Batch Size
100
100
Policy update frequency
2
2
Discount Factor
0.99
0.99
Target critic update
0.005
0.005
Target policy update
0.005
0.005
Reward Scaling
1
1
Exploration Policy
N(0, 0.1)
N(0, 0.1)

Recomposing the Reinforcement Learning Building-Blocks with Hypernetworks
E.2. SAC
(a) Hopper
(b) Walker2d
(d) HalfCheetah
(c) Ant
Figure 14. SAC Performance of different critic models.
Table 6. SAC highest rewards.
Network
Hopper
Walker2D
Ant
HalfCheetah
MLP-Standard
3160 ± 327
4258 ± 413
3323 ± 389
10225 ± 324
MLP-Large
3549 ± 160
3550 ± 936
2100 ± 1322
8853 ± 1663
MLP-Small
2806 ± 425
2629 ± 804
2735 ± 589
6229 ± 475
ResNet-Features
3038 ± 1129
2936 ± 896
1002 ± 1
2755 ± 2114
ResNet35
3525 ± 40
2923 ± 1369
1138 ± 252
10096 ± 468
Q-D2RL
3612 ± 51
4638 ± 441
3684 ± 1207
10224 ± 1090
SA-Hyper (Ours)
3527 ± 40
4844 ± 254
3385 ± 983
10600 ± 950
Table 7. SAC Hyper Parameters
Hyper-parameter
SAC
Hyper SAC (Ours)
Actor Learning Rate
3e−4
2e−5, 1e−4 for ’HalfCheetah’
Critic Learning Rate
3e−4
5e−5
Optimizer
Adam
Adam
Batch Size
256
256
Discount Factor
0.99
0.99
Target critic update
0.005
0.005
Reward Scaling
5
5

Recomposing the Reinforcement Learning Building-Blocks with Hypernetworks
E.3. MAML
0
50
100
150
200
250
300
350
400
350
300
250
200
150
return
Half-Cheetah-Vel
0
50
100
150
200
250
300
350
400
0
100
200
300
400
500
600
Half-Cheetah-Fwd-Back
0
50
100
150
200
250
300
350
400
iteration
600
550
500
450
400
350
300
Half-Cheetah-Vel-Medium(OOD)
0
50
100
150
200
250
300
350
400
iteration
60
40
20
0
20
40
return
Ant-Vel
0
50
100
150
200
250
300
350
400
iteration
0
10
20
30
40
50
60
70
Ant-Fwd-Back
Vanilla MAML pre-ad
Vanilla MAML
Contex MAML pre-ad
Contex MAML
Hyper MAML pre-ad
Hyper MAML(Ours)
Figure 15. MAML Performance over test tasks with a Hypernetwork policy compared to MLP policy with and without a given context
of the tasks by an oracle. The oracle-context improves the MAML performance but Hyper-MAML outperforms Context-MAML and,
importantly, it does not require an adaptation step.
0
50
100
150
200
250
300
350
400
350
300
250
200
150
return
Half-Cheetah-Vel
0
50
100
150
200
250
300
350
400
0
100
200
300
400
500
600
Half-Cheetah-Fwd-Back
0
50
100
150
200
250
300
350
400
iteration
300
250
200
150
Half-Cheetah-Vel-Medium(OOD)
0
50
100
150
200
250
300
350
400
iteration
40
20
0
20
40
return
Ant-Vel
0
50
100
150
200
250
300
350
400
iteration
0
20
40
60
80
Ant-Fwd-Back
Vanilla MAML pre-ad
Vanilla MAML
Contex MAML pre-ad
Contex MAML
Hyper MAML pre-ad
Hyper MAML(Ours)
Figure 16. MAML Performance over training tasks with a Hypernetwork policy compared to MLP policy with and without a given
context of the tasks by an oracle. The oracle-context improves the MAML performance but Hyper-MAML outperforms Context-MAML
and, importantly, it does not require an adaptation step.

Recomposing the Reinforcement Learning Building-Blocks with Hypernetworks
E.3.1. ELIMINATING THE ADAPTATION STEP
Our experiments show that taking the MAML adaptation step is unnecessary when using the Hyper-MAML model with
an oracle-context (as opposed to Context-MAML which uses an oracle-context but still beneﬁts from the adaptation step).
We further investigate whether we can also eliminate the adaptation step during training s.t. the gradient of each task is
calculated with the policy current weights as opposed to MAML which calculates the gradient at the policy’s adapted
weights. We term this method as Multi-Task Hyper-MAML (following (Fakoor et al., 2019) which termed the Meta-RL
objective without adaptation as a multi-task objective). In Fig. 17 we ﬁnd that Multi-Task Hyper-MAML outperforms
the Hyper-MAML with adaptation. Moreover, Table D.5 shows that it also requires less training time as it removes the
unnecessary complexity of the MAML adaptation training.
0
50
100
150
200
250
300
350
400
350
300
250
200
150
return
Half-Cheetah-Vel
0
50
100
150
200
250
300
350
400
0
100
200
300
400
500
600
Half-Cheetah-Fwd-Back
0
50
100
150
200
250
300
350
400
iteration
600
550
500
450
400
350
300
Half-Cheetah-Vel-Medium(OOD)
0
50
100
150
200
250
300
350
400
iteration
60
40
20
0
20
40
return
Ant-Vel
0
50
100
150
200
250
300
350
400
iteration
0
10
20
30
40
50
60
70
Ant-Fwd-Back
Vanilla MAML
Contex MAML
Hyper MAML(Ours)
Multi Task Hyper MAML (Ours)
Figure 17. Multi-Task Hyper-MAML performance over the test tasks with a Hypernetwork policy and a multi-task objective. Using a
multi-task objective matches or outperforms the MAML objective without the need for the adaption step training.
Table 8. MAML highest rewards.
Algorithm
Cheetah-Vel
Cheetah-Fwd-Back
Cheetah-Vel-Med
Ant-Vel
Ant-Fwd-Back
MAML
−231 ± 40
183 ± 51
−423 ± 33
−8 ± 10
25 ± 13
Context MAML
−207 ± 25
315 ± 93
−374 ± 79
6±19
41 ± 15
Hyper Multi-Task (Ours)
−178 ± 21
539 ± 102
−332 ± 7
30 ± 13
72 ± 3
Hyper MAML (Ours)
−182 ± 25
558 ± 49
−344 ± 10
27 ± 14
70 ± 5
E.4. PEARL

Recomposing the Reinforcement Learning Building-Blocks with Hypernetworks
Table 9. MAML Hyperparameters
Hyperparameter
MAML
Hyper MAML (Ours)
Batch Size
20
20
Meta batch Size
40
40
Discount Factor
0.95
0.95
Num of Iterations
400
400
Max KL
1e−2
1e−2
LS Max Steps
20
20
Episode Max Steps
200
200
0.25
0.50
0.75
1.00
1.25
1.50
1.75
2.00
2.25
1e6
400
350
300
250
200
150
100
50
return
Half-Cheetah-Vel
0.0
0.5
1.0
1.5
2.0
2.5
3.0
3.5
4.0
1e6
0
250
500
750
1000
1250
1500
1750
2000
Half-Cheetah-Fwd-Back
0.25
0.50
0.75
1.00
1.25
1.50
1.75
2.00
2.25
steps
1e6
500
400
300
200
100
Half-Cheetah-Vel-Medium(OOD)
0.25
0.50
0.75
1.00
1.25
1.50
1.75
2.00
2.25
steps
1e6
200
100
0
100
200
return
Ant-Vel
0.00
0.25
0.50
0.75
1.00
1.25
1.50
1.75
2.00
steps
1e6
0
200
400
600
800
1000
1200
Ant-Fwd-Back
PEARL
Hyper PEARL (Ours)
Figure 18. PEARL Performance over the test tasks with policy and critic Hypernetworks compared to MLP policy and critic. Hypernet-
work outperforms or matches MLP in all environments.
Table 10. PEARL highest rewards.
Algorithm
Cheetah-Vel
Ant-Vel
Cheetah-Vel-Med
Cheetah-Fwd-Back
Ant-Fwd-Back
PEARL
−142 ± 82
1636 ± 210
−325 ± 109
93 ± 115
943 ± 146
Hyper PEARL (Ours)
−119 ± 82
1828 ± 203
−206 ± 104
115 ± 54
1026 ± 62

Recomposing the Reinforcement Learning Building-Blocks with Hypernetworks
0.25
0.50
0.75
1.00
1.25
1.50
1.75
2.00
2.25
1e6
400
350
300
250
200
150
100
50
0
return
Half-Cheetah-Vel
0.0
0.5
1.0
1.5
2.0
2.5
3.0
3.5
4.0
1e6
0
500
1000
1500
2000
Half-Cheetah-Fwd-Back
0.25
0.50
0.75
1.00
1.25
1.50
1.75
2.00
2.25
steps
1e6
350
300
250
200
150
100
50
0
Half-Cheetah-Vel-Medium(OOD)
0.25
0.50
0.75
1.00
1.25
1.50
1.75
2.00
2.25
steps
1e6
150
100
50
0
50
100
150
200
return
Ant-Vel
0.00
0.25
0.50
0.75
1.00
1.25
1.50
1.75
2.00
steps
1e6
0
200
400
600
800
1000
1200
Ant-Fwd-Back
PEARL
Hyper PEARL (Ours)
Figure 19. PEARL Performance over training tasks with policy and critic Hypernetworks compared to MLP policy and critic.
Table 11. PEARL Hyperparameters
Hyperparameter
PEARL
Hyper PEARL
Actor Learning Rate
3e−4
1e−4
Critic Learning Rate
3e−4
5e−5
Context Learning Rate
3e−4
3e−4
Value Learning Rate
3e−4
5e−5
Optimizer
Adam
Adam
Batch Size
256
256
’Dir’ Tasks Meta batch Size
4
4
’Vel’ Tasks Meta batch Size
16
16
Target critic update
0.005
0.005
Discount Factor
0.99
0.99
Num of Iterations
400
400
Reward Scaling
5
5
Episode Max Steps
200
200

