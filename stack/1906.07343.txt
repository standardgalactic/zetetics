Language as an Abstraction
for Hierarchical Deep Reinforcement Learning
Yiding Jiang∗, Shixiang Gu, Kevin Murphy, Chelsea Finn
Google Research
{ydjiang,shanegu,kpmurphy,chelseaf}@google.com
Abstract
Solving complex, temporally-extended tasks is a long-standing problem in rein-
forcement learning (RL). We hypothesize that one critical element of solving such
problems is the notion of compositionality. With the ability to learn concepts
and sub-skills that can be composed to solve longer tasks, i.e. hierarchical RL,
we can acquire temporally-extended behaviors. However, acquiring effective yet
general abstractions for hierarchical RL is remarkably challenging. In this paper,
we propose to use language as the abstraction, as it provides unique composi-
tional structure, enabling fast learning and combinatorial generalization, while
retaining tremendous ﬂexibility, making it suitable for a variety of problems. Our
approach learns an instruction-following low-level policy and a high-level policy
that can reuse abstractions across tasks, in essence, permitting agents to reason
using structured language. To study compositional task learning, we introduce an
open-source object interaction environment built using the MuJoCo physics engine
and the CLEVR engine. We ﬁnd that, using our approach, agents can learn to
solve to diverse, temporally-extended tasks such as object sorting and multi-object
rearrangement, including from raw pixel observations. Our analysis reveals that
the compositional nature of language is critical for learning diverse sub-skills and
systematically generalizing to new sub-skills in comparison to non-compositional
abstractions that use the same supervision.2
1
Introduction
Deep reinforcement learning offers a promising framework for enabling agents to autonomously
acquire complex skills, and has demonstrated impressive performance on continuous control prob-
lems [35, 56] and games such as Atari [41] and Go [59]. However, the ability to learn a variety of
compositional, long-horizon skills while generalizing to novel concepts remain an open challenge.
Long-horizon tasks demand sophisticated exploration strategies and structured reasoning, while
generalization requires suitable representations. In this work, we consider the question: how can we
leverage the compositional structure of language for enabling agents to perform long-horizon tasks
and systematically generalize to new goals?
To do so, we build upon the framework of hierarchical reinforcement learning (HRL), which offers
a potential solution for learning long-horizon tasks by training a hierarchy of policies. However,
the abstraction between these policies is critical for good performance. Hard-coded abstractions
often lack modeling ﬂexibility and are task-speciﬁc [63, 33, 26, 47], while learned abstractions
often ﬁnd degenerate solutions without careful tuning [5, 24]. One possible solution is to have the
higher-level policy generate a sub-goal state and have the low-level policy try to reach that goal
state [42, 36]. However, using goal states still lacks some degree of ﬂexibility (e.g. in comparison
∗Work done as a part of the Goolge AI Residency program
2Code and videos of the environment, and experiments are at https://sites.google.com/view/hal-demo
33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.
arXiv:1906.07343v2  [cs.LG]  18 Nov 2019

(a) Goal is g0: “There is a red ball; are there
any matte cyan sphere right of it?". Currently
Ψ(st, g0) = 0
(b) Agent performs actions and interacts with
the environment and tries to satisfy goal.
(c) Resulting state st+1 does not satisfy g0,
so relabel goal to g′: “ There is a green
sphere; are there any rubber cyan balls be-
hind it?" so Ψ(st+1, g′) = 1
Figure 1: The environment and some instructions that we consider in this work, along with an illustration of
hindsight instruction relabeling (HIR), which we use to enable the agent to learn from many different language
goals at once (Details in Section 4.2).
to goal regions or attributes), is challenging to scale to visual observations naively, and does not
generalize systematically to new goals. In contrast to these prior approaches, language is a ﬂexible
representation for transferring a variety of ideas and intentions with minimal assumptions about
the problem setting; its compositional nature makes it a powerful abstraction for representing
combinatorial concepts and for transferring knowledge [22].
In this work, we propose to use language as the interface between high- and low-level policies in
hierarchical RL. With a low-level policy that follows language instructions (Figure 1), the high-level
policy can produce actions in the space of language, yielding a number of appealing beneﬁts. First,
the low-level policy can be re-used for different high-level objectives without retraining. Second,
the high-level policies are human-interpretable as the actions correspond to language instructions,
making it easier to recognize and diagnose failures. Third, language abstractions can be viewed as
a strict generalization of goal states, as an instruction can represent a region of states that satisfy
some abstract criteria, rather than the entirety of an individual goal state. Finally, studies have also
suggested that humans use language as an abstraction for reasoning and planning [21, 49]. In fact, the
majority of knowledge learning and skill acquisition we do throughout our life is through languages.
While language is an appealing choice as the abstraction for hierarchical RL, training a low-level
policy to follow language instructions is highly non-trivial [20, 6] as it involves learning from binary
rewards that indicate completion of the instruction. To address this problem, we generalize prior work
on goal relabeling to the space of language instructions (which instead operate on regions of state
space, rather than a single state), allowing the agent to learn from many language instructions at once.
To empirically study the role of language abstractions for long-horizon tasks, we introduce a new
environment inspired by the CLEVR engine [28] that consists of procedurally-generated scenes of
objects that are paired with programatically-generated language descriptions. The low-level policy’s
objective is to manipulate the objects in the scene such that a description or statement is satisﬁed
by the arrangement of objects in the scene. We ﬁnd that our approach is able to learn a variety
of vision-based long-horizon manipulation tasks such as object reconﬁguration and sorting, while
outperforming state-of-the-art RL and hierarchical RL approaches. Further, our experimental analysis
ﬁnds that HRL with non-compositional abstractions struggles to learn the tasks, even when the
non-compositional abstraction is derived from language instructions themselves, demonstrating the
critical role of compositionality in learning. Lastly, we ﬁnd that our instruction-following agent is
able to generalize to instructions that are systematically different from those seen during training.
In summary, the main contribution of our work is three-fold:
1. a framework for using language abstractions in HRL, with which we ﬁnd that the structure
and ﬂexibility of language enables agents to solve challenging long-horizon control problems
2. an open-source continuous control environment for studying compositional, long-horizon
tasks, integrated with language instructions inspired by the CLEVR engine [28]
3. empirical analysis that studies the role of compositionality in learning long-horizon tasks and
achieving systematic generalization
2
Related Work
Designing, discovering and learning meaningful and effective abstractions of MDPs has been studied
extensively in hierarchical reinforcement learning (HRL) [15, 46, 63, 16, 5]. Classically, the work
2

on HRL has focused on learning only the high-level policy given a set of hand-engineered low-level
policies [60, 38, 9], or more generically options policies with ﬂexible termination conditions [63, 52].
Recent HRL works have begun to tackle more difﬁcult control domains with both large state spaces
and long planning horizons [26, 34, 65, 17, 42, 43]. These works can typically be categorized into
two approaches. The ﬁrst aims to learn effective low-level policies end-to-end directly from ﬁnal task
rewards with minimal human engineering, such as through the option-critic architecture [5, 24] or
multi-task or meta learning [18, 58]. While appealing in theory, this end-to-end approach relies solely
on ﬁnal task rewards and is shown to scale poorly to complex domains [5, 42], unless distributions of
tasks are carefully designed [18]. The second approach instead augments the low-level learning with
auxiliary rewards that can bring better inductive bias. These rewards include mutual information-
based diversity rewards [13, 17], hand-crafted rewards based on domain knowledge [33, 26, 34, 65],
and goal-oriented rewards [15, 55, 4, 69, 42, 43]. Goal-oriented rewards have been shown to balance
sufﬁcient inductive bias for effective learning with minimal domain-speciﬁc engineering, and achieve
performance gains on a range of domains [69, 42, 43]. Our work is a generalization on these lines of
work, representing goal regions using language instructions, rather than individual goal states. Here,
region refers to the sets of states (possibly disjoint and far away from each other) that satisfy more
abstract criteria (e.g. “red ball in front of blue cube" can be satisﬁed by inﬁnitely many states that are
drastically different from each other in the pixel space) rather than a simple ϵ-ball around a single
goal state that is only there for creating a reachable non-zero measure goal. Further, our experiments
demonstrate signiﬁcant empirical gains over these prior approaches.
Since our low-level policy training is related to goal-conditioned HRL, we can beneﬁt from algo-
rithmic advances in multi-goal reinforcement learning [29, 55, 4, 50]. In particular, we extend the
recently popularized goal relabeling strategy [29, 4] to instructions, allowing us to relabel based on
achieving a language statement that describes a region of state space, rather than relabeling based on
reaching an individual state.
Lastly, there are a number of prior works that study how language can guide or improve reinforcement
learning [37, 19, 30, 6, 20, 12, 10]. While prior work has made use of language-based sub-goal
policies in hierarchical RL [57, 14], the instruction representations used lack the needed diversity
to beneﬁt from the compositionality of language over one-hot goal representations. In a concurrent
work, Wu et al. [70] show that language can help with learning difﬁcult tasks where more naive
goal representations lead to poor performance, even with hindsight goal relabeling. While we are
also interested in using language to improve learning of challenging tasks, we focus on the use
of language in the context of hierarchical RL, demonstrating that language can be further used to
compose complex objectives for the agent. Andreas et al. [3] leverage language descriptions to
rapidly adapt to unseen environments through structured policy search in the space of language;
each environment is described by one sentence. In contrast, we show that a high-level policy can
effectively leverage the combinatorially many sub-policies induced by language by generating a
sequence of instructions for the low-level agent. Further, we use language not only for adaptation but
also for learning the lower level control primitives, without the need for imitation learning from an
expert. Another line of work focuses on RL for textual adventure games where the state is represented
as language descriptions and the actions are either textual actions available at each state [25] or all
possible actions [45] (even though not every action is applicable to all states). In general, these
works look at text-based games with discrete 1-bit actions, while we consider continuous actions in
physics-based environments. One may view the latter as a high-level policy with oracular low-level
policies that are speciﬁc to each state; the discrete nature of these games entails limited complexity of
interactions with the environment.
3
Preliminaries
Standard reinforcement learning. The typical RL problem considers a Markov decision process
(MDP) deﬁned by the tuple (S, A, T, R, γ) where S where S is the state space, A is the action space,
the unknown transition probability T : S × A × S →[0, ∞) represents the probability density of
reaching st+1 ∈S from st ∈S by taking the action a ∈A, γ ∈[0, 1) is the discount factor, and the
bounded real-valued function R : S × A →[rmin, rmax] represents the reward of each transition.
We further denote ρπ(st) and ρπ(st, at) as the state marginal and the state-action marginal of the
trajectory induced by policy π(at|st). The objective of reinforcement learning is to ﬁnd a policy
π(at|st) such that the expected discounted future reward P
t E(st,at)∼ρπ[γtR(st, at)] is maximized.
3

Goal conditioned reinforcement learning. In goal-conditioned RL, we work with an Augmented
Markov Decision Process, which is deﬁned by the tuple (S, G, A, T, R, γ). Most elements represent
the same quantities as a standard MDP. The additional tuple element G is the space of all possible goals,
and the reward function R : S ×A×G →[rmin, rmax] represents the reward of each transition under
a given goal. Similarly, the policy π(at|st, g) is now conditioned on g. Finally, pg(g) represents
a distribution over G. The objective of goal directed reinforcement learning is to ﬁnd a policy
π(at|st, g) such that the expected discounted future reward P
t Eg∼pg,(st,at)∼ρπ[γtR(st, at, g)] is
maximized. While this objective can be expressed with a standard MDP by augmenting the state
vector with a goal vector, the policy does not change the goal; the explicit distinction between goal
and state facilitates discussion later.
Q-learning. Q-learning is a large class of off-policy reinforcement learning algorithms that focuses
on learning the Q-function, Q∗(st, at), which represents the expected total discounted reward that
can be obtained after taking action at in state st assuming the agent acts optimally thereafter. It can
be recursively deﬁned as:
Q∗(st, at) = Est+1[R(st, at) + γ max
a∈A(Q∗(st+1, a))]
(1)
The optimal policy learned can be recovered through π∗(at|st) = δ(at = arg maxa∈A Q∗(st, a)).
In high-dimensional spaces, the Q-function is usually represented with function approximators and
ﬁt using transition tuples, (st, at, st+1, rt), which are stored in a replay buffer [41].
Hindsight experience replay (HER). HER [4] is a data augmentation technique for off-policy goal
conditioned reinforcement learning. For simplicity, assume that the goal is speciﬁed in the state
space directly. A trajectory can be transformed into a sequence of goal augmented transition tuples
(st, at, sg, st+1, rt). We can relabel each tuple’s sg with st+1 or other future states visited in the
trajectory and adjust rt to be the appropriate value. This makes the otherwise sparse reward signal
much denser. This technique can also been seen as generating an implicit curriculum of increasing
difﬁculty for the agent as it learns to interact with the environment more effectively.
4
Hierarchical Reinforcement Learning with Language Abstractions
Figure 2: HAL: The high-level pol-
icy πh produces language instruc-
tions g for the low level policy πl.
In this section, we present our framework for training a 2-layer
hierarchical policy with compositional language as the abstrac-
tion between the high-level policy and the low-level policy. We
open the exposition with formalizing the problem of solving tem-
porally extended task with language, including our assumptions
regarding the availability of supervision. We will then discuss
how we can efﬁciently train the low-level policy, πl(a|st, g) con-
ditioned on language instructions g in Section 4.2, and how a
high-level policy, πh(g|st), can be trained using such a low-level
policy in Section 4.3. We refer to this framework as Hierarchical
Abstraction with Language (HAL, Figure 2, Appendix C.1).
4.1
Problem statement
We are interested in learning temporally-extended tasks by leveraging the compositionality of lan-
guage. Thus, in addition to the standard reinforcement learning assumptions laid out in Section 3,
we also need some form of grounded language supervision in the environemnt E during training. To
this end, we also assume the access to a conditional density ω(g|s) that maps observation s to a
distribution of language statements g ∈G that describes s. This distribution can take the form of a
supervised image captioning model, a human supervisor, or a functional program that is executed on
st similar to CLEVR. Further, we deﬁne Ω(st) to be the support of ω(g|st). Moreover, we assume
the access to a function Ψ that maps a state and an instruction to a single Boolean value which
indicates whether the instruction is satisﬁed or not by the s, i.e. Ψ : S × G →{0, 1}. Once again,
Ψ can be a VQA model, a human supervisor or a program. Note that any goal speciﬁed in the state
space can be easily expressed by a Boolean function of this form by checking if two states are close
to each other up to some threshold parameter. Ψ can effectively act as the reward for the low-level
policy.
An example for the high-level tasks is arranging objects in the scene according to a speciﬁc spatial
relationship. This can be putting the object in speciﬁc arrangement or ordering the object according
4

(a) Object
arrangement
(b) Object
ordering
(c) Object
sorting
(d) Color
ordering
(e) Shape
ordering
(f) Color & shape
ordering
Figure 3: Sample goal states for the high-level tasks in the standard (a-c) and diverse (d-f) environments. The
high-level policy only receives reward if all constraints are satisﬁed. The global location of the objects may vary.
the the colors (Figure 3) by pushing the objects around (Figure 1). Details of these high-level tasks
are described in Section 5. These tasks are complex but can be naturally decomposed into smaller
sub-tasks, giving rise to a naturally deﬁned hierarchy, making it an ideal testbed for HRL algorithms.
Problems of similar nature including organizing a cluttered table top or stacking LEGO blocks to
construct structures such as a castle or a bridge. We train the low-level policy πl(a|s, g) to solve an
augmented MDP described Section 3. For simplicity, we assume that Ω’s output is uniform over G.
The low-level policy receives supervision from Ωand Ψ by completing instructions. The high-level
policy πh(g|s) is trained to solve a standard MDP whose state space is the same S as the low-level
policy, and the action space is G. In this case, the high-level policy’s supervision comes from the
reward function of the environment which may be highly sparse.
We separately train the high-level policy and low-level policy, so the low-level policy is agnostic to
the high-level policy. Since the policies share the same G, the low-level policy can be reused for
different high-level policies (Appendix C.3). Jointly ﬁne-tuning the low-level policy with a speciﬁc
high-level policy is certainly a possible direction for future work (Appendix C.1).
4.2
Training a language-conditioned low-level policy
To train a goal conditioned low-level policy, we need to deﬁne a suitable reward function for training
such a policy and a mechanism for sampling language instructions. A straightforward way to represent
the reward for the low-level policy would be R(st, at, st+1, g) = Ψ(st+1, g) or, to ensure that at is
inducing the reward:
R(st, at, st+1, g) =
0
if Ψ(st+1, g) = 0
Ψ(st+1, g) ⊕Ψ(st, g)
if Ψ(st+1, g) = 1
However, optimizing with this reward directly is difﬁcult because the reward signal is only non-zero
when the goal is achieved. Unlike prior work (e.g. HER [4]), which uses a state vector or a task-
relevant part of the state vector as the goal, it is difﬁcult to deﬁne meaningful distance metrics in
the space of language statements [8, 53, 61], and, consequently, difﬁcult to make the reward signal
smooth by assigning partial credit (unlike, e.g., the ℓp norm of the difference between 2 states). To
overcome these difﬁculties, we propose a trajectory relabeling technique for language instructions:
Instead of relabeling the trajectory with states reached, we relabel states in the the trajectory τ with
the elements of Ω(st) as the goal instruction using a relabeling strategy S. We refer to this procedure
as hindsight instruction relabeling (HIR). The details of S is located in Algorithm 4 in Appendix C.4.
Pseudocode for the method can be found in Algorithm 2 in Appendix C.2 and an illustration of the
process can be found in Figure 1.
The proposed relabeling scheme, HIR, is reminiscent of HER [4]. In HER, the goal is often the state
or a simple function of the state, such as a masked representation. However, with high-dimensional
observation spaces such as images, there is excessive information in the state that is irrelevant to
the goal, while task-relevant information is not readily accessible. While one can use HER with
generative models of images [54, 51, 44], the resulting representation of the state may not effectively
capture the relevant aspects of the desired task. Language can be viewed as an alternative, highly-
compressed representation of the state that explicitly exposes the task structure, e.g. the relation
between objects. Thus, we can readily apply HIR to settings with image observations.
4.3
Acting in language with the high-level policy
We aim to learn a high-level policy for long-horizon tasks that can explore and act in the space
of language by providing instructions g to the low-level policy πl(a|st, g). The use of language
abstractions through g allows the high-level policy to structurally explore with actions that are
semantically meaningful and span multiple low-level actions.
5

In principle, the high-level policy, πh(g|s), can be trained with any reinforcement learning algorithm,
given a suitable way to generate sentences for the goals. However, generating coherent sequences
of discrete tokens is difﬁcult, particularly when combined with existing reinforcement learning
algorithms. We explore how we might incorporate a language model into the high level policy in
Appendix A, which shows promising preliminary results but also signiﬁcant challenges. Fortunately,
while the size of the instruction space G scales exponentially with the size of the vocabulary, the
elements of G are naturally structured and redundant – many elements correspond to effectively
the same underlying instruction with different synonyms or grammar. While the low-level policy
understands all the different instructions, in many cases, the high-level policy only needs to generate
instruction from a much smaller subset of G to direct the low-level policy. We denote such subsets of
G as I.
If I is relatively small, the problem can be recast as a discrete-action RL problem, where one action
choice corresponds to an instruction, and can be solved with algorithms such as DQN [41]. We adopt
this simple approach in this work. As the instruction often represents a sequence of low-level actions,
we take T ′ actions with the low-level policy for every high-level instruction. T ′ can be a ﬁxed number
of steps, or computed dynamically by a terminal policy learned by the low-level policy like the option
framework. We found that simply using a ﬁxed T ′ was sufﬁcient in our experiments.
5
The Environment and Implementation
Environment. To empirically study how compositional languages can aid in long-horizon reasoning
and generalization, we need an environment that will test the agent’s ability to do so. While prior
works have studied the use of language in navigation [1], instruction following in a grid-world [10],
and compositionality in question-answering, we aim to develop a physical simulation environment
where the agent must interact with and change the environment in order to accomplish long-horizon,
compositional tasks. These criteria are particularly appealing for robotic applications, and, to the best
of our knowledge, none of the existing benchmarks simultaneously fulﬁlls all of them. To this end,
we developed a new environment using the MuJoCo physics engine [66] and the CLEVR language
engine, that tests an agents ability to manipulate and rearrange objects of various shapes and colors.
To succeed in this environment, the agent must be able to handle varying number of objects with
diverse visual and physical properties. Two versions of the environment of varying complexity are
illustrated in Figures 3 and 1 and further details are in Appendix B.1.
High-level tasks. We evaluate our framework 6 challenging temporally-extended tasks across two
environments, all illustrated in Figure 3: (a) object arrangement: manipulate objects such that 10
pair-wise constraints are satisﬁed, (b) object ordering: order objects by color, (c) object sorting:
arrange 4 objects around a central object, and in a more diverse environment (d) color ordering: order
objects by color irrespective of shape, (e) shape ordering: order objects by shape irrespective of color,
and (f) color & shape ordering: order objects by both shape and color. In all cases, the agent receives
a binary reward only if all constraints are satisﬁed. Consequently, this makes obtaining meaningful
signal in these tasks extremely challenging as only a very small number of action sequences will
yield non-zero signal. For more details, see Appendix B.2.
Action and observation parameterization. The state-based observation is s ∈R10 that represents
the location of each object and |A| = 40 which corresponds to picking an object and pushing it in
one of the eight cardinal directions. The image-based observation is s ∈R64×64×3 which is the
rendering of the scene and |A| = 800 which corresponds to picking a location in a 10 × 10 grid and
pushing in one of the eight cardinal directions. For more details, see Appendix B.1.
Policy parameterization. The low-level policy encodes the instruction with a GRU and feeds the
result, along with the state, into a neural network that predicts the Q-value of each action. The
high-level policy is also a neural network Q-function. Both use Double DQN [68] for training. The
high-level policy uses sets of 80 and 240 instructions as the action space in the standard and diverse
environments respectively, a set that sufﬁciently covers relationships between objects. We roll out the
low-level policy for T ′ = 5 steps for every high-level instruction. For details, see Appendix B.3.
6
Experiments
To evaluate our framework, and study the role of compositionality in RL in general, we design the
experiments to answer the following questions: (1) as a representation, how does language compare
6

to alternative representations, such as those that are not explicitly compositional? (2) How well does
the framework scale with the diversity of instruction and dimensionality of the state (e.g. vision-
based observation)? (3) Can the policy generalize in systematic ways by leveraging the structure of
language? (4) Overall, how does our approach compare to state-of-the-art hierarchical RL approaches,
along with learning ﬂat, homogeneous policies?
To answer these questions, in Section 6.1, we ﬁrst evaluate and analyze training of effective low-
level policies, which are critical for effective learning of long-horizon tasks. Then, in Section 6.2,
we evaluate the full HAL method on challenging temporally-extended tasks.
Finally, we apply
our method to the Crafting environment from Andreas et al. [2] to showcase the generality of our
framework. For details on the experimental set-up and analysis, see Appendix D and E.
6.1
Low-level Policy
Role of compositionality and relabeling. We start by evaluating the ﬁdelity of the low-level
instruction-following policy, in isolation, with a variety of representations for the instruction. For
these experiments, we use state-based observations. We start with a set of 600 instructions, which we
paraphrase and substitute synonyms to obtain more than 10,000 total instructions which allows us to
answer the ﬁrst part of (2). We evaluate the performance all low-level policies by the average number
of instructions it can successfully achieve each episode (100 steps), measured over 100 episodes. To
answer (1), and evaluate the importance of compositionality, we compare against:
• a one-hot encoded representation of instructions where each instruction has its own row in a
real-valued embedding matrix which uses the same instruction relabeling (see Appendix D.1)
• a non-compositional latent variable representation with identical information content. We train
an sequence auto-encoder on sentences, which achieves 0 reconstruction error and is hence a
lossless non-compositional representation of the instructions (see Appendix D.2)
• a bag-of-words (BOW) representation of instructions (see Appendix D.3)
In the ﬁrst comparison, we observe that while one-hot encoded representation works on-par with or
better than the language in the regime where the number of instructions is small, its performance
quickly deteriorates as the number of instruction increases (Fig.4, middle). On the other hand,
language representation of instruction can leverage the structures shared by different instructions and
does not suffer from increasing number of instructions (Fig.4, right blue); in fact, an improvement
in performance is observed. This suggests, perhaps unsurprisingly, that one-hot representations and
state-based relabeling scale poorly to large numbers of instructions, even when the underlying number
of instructions does not change, while, with instruction relabeling (HIR), the policy acquires better,
more successful representations as the number of instructions increases.
In the second comparison, we observe that the agent is unable to make meaningful progress with
this representation despite receiving identical amount of supervision as language. This indicates that
the compositionality of language is critical for effective learning. Finally, we ﬁnd that relabeling is
critical for good performance, since without it (no HIR), the reward signal is signiﬁcantly sparser.
Finally, in the comparison to a bag-of-words representation (BOW), we observe that, while the BOW
agent’s return increases at a faster rate than that of the language agent at the beginning of training –
likely due to the difﬁculty of optimizing recurrent neural network in language agent – the language
agent achieves signiﬁcantly better ﬁnal performance. On the other hand, the performance of the BOW
agent plateaus at around 8 instructions per episode. This is expected as BOW does not consider the
sequential nature of an instruction, which is important for correctly executing an instruction.
Vision-based observations. To answer the second part of (2), we extend our framework to pixel ob-
servations. The agent reaches the same performance as the state-based model, albeit requiring longer
convergence time with the same hyper-parameters. On the other hand, the one-hot representation
reaches much worse relative performance with the same amount of experience (Fig.4, right).
Visual generalization. One of the most appealing aspects of language is the promise of combinatorial
generalization [7] which allows for extrapolation rather than simple interpolation over the training
set. To evaluate this (i.e. (3)), we design the training and test instruction sets that are systematically
distinct. We evaluate the agent’s ability to perform such generalization by splitting the 600 instruction
sets through the following procedure: (i) standard: random 70/30 split of the instruction set; (ii)
systematic: the training set only consists of instructions that do not contain the words red in the
ﬁrst half of the instructions and the test set contains only those that have red in the ﬁrst half of the
7

Figure 4: Results for low-level policies in terms of goals accomplished per episode over training steps for
HIR. Left: HIR with different number of instructions and results with non-compositional representation and
with no relabeling. Middle: Results for one-hot encoded representation with increasing number of instructions.
Since the one-hot cannot leverage compositionality of the language, it suffers signiﬁcantly as instruction sets
grow, while HIR on sentences in fact learns even faster when instruction sets increase. Right: Performance of
image-based low-level policy compared against one-hot and non-compositional instruction representations.
instructions. We emphasize that the agent has never seen the words red in the ﬁrst part of the sentence
in training; in other words, the task is zero-shot as the training set and the test set are disjoint (i.e. the
distributions do not share support). From a pure statistical learning theoretical perspective, the agent
should not do better than chance on such a test set. Remarkably, we observe that the agent generalizes
better with language than with non-compositional representation (table 1). This suggests that the
agent recognizes the compositional structure of the language, and achieves systematic generalization
through such understanding.
Standard
train
Standard
test
Standard
gap
Systematic
train
Systematic
test
Systematic
gap
Language
21.50 ± 2.28
21.49 ± 2.53
0.001
20.09 ± 2.46
8.13 ± 2.34
0.596
Non-Compositional
6.26 ± 1.18
5.78 ± 1.44
0.077
7.54 ± 1.14
0.76 ± 0.69
0.899
Random
0.17 ± 0.20
0.21 ± 0.17
-
0.11 ± 0.19
0.18 ± 0.22
-
Table 1: Final performance of the low-level policy on different training and test instruction distributions (20
episodes). Language outperforms the non-compositional language representation in both absolute performance
and relative generalization gap for every setting. Gap is equal to one minus the ratio of between mean test
performance and mean train performance; this quantity can be interpreted as the generalization gap. For
instructions with language representation, the generalization gap increases by approximately 59.5% from
standard generalization to zero-shot generalization while for non-compositional representation the generalization
gap increases by 82.2%
Figure 5: Results for high-level policy on tasks (a-c). Blue curves for HAL include the steps for training the
low-level policy (a single low-level policy is used for all 3 tasks). In all settings, HAL demonstrates faster
learning than DDQN. Means and standard deviations of 3 random seeds are plotted.
6.2
High-level policy
Now that we have analyzed the low-level policy performance, we next evaluate the full HAL algorithm.
To answer (4), we compare our framework in the state space against a non-hierarchical baseline
DDQN and two representative hierarchical reinforcement learning frameworks HIRO [42] and Option-
Critic (OC) [5] on the proposed high-level tasks with sparse rewards (Sec.5). We observe that neither
HRL baselines are able to learn a reasonable policy while DDQN is able to solve only 2 of the 3 tasks.
HAL is able to solve all 3 tasks consistently and with much lower variance and better asymptotic
performance (Fig.5). Then we show that our framework successfully transfers to high-dimensional
8

observation (i.e. images) in all 3 tasks without loss of performance whereas even the non-hierarchical
DDQN fails to make progress (Fig.6, left). Finally, we apply the method to 3 additional diverse tasks
(Fig.6, middle). In these settings, we observed that the high-level policy has difﬁculty learning from
pixels alone, likely due to the visual diversity and the simpliﬁed high-level policy parameterization.
As such, the high-level policy for diverse setting receives state observation but the low-level policy
uses the raw-pixel observation. For more details, please refer to Appendix B.3.
Figure 6: Left: Results for vision-based hierarchical RL. In all settings, HAL demonstrates faster and more
stable learning while the baseline DDQN cannot learn a non-trivial policy. In this case, the vision-based low-level
policy needs longer training time (~5 × 106 steps) so we start the x-axis there. Means and standard deviations of
3 seeds are plotted (Near 0 variance for DDQN). Middle: Results of HRL on the proposed 3 diverse tasks (d-e).
In this case, the low-level policy used is trained on image observation for (~4 × 106 steps). 3 random seeds are
plotted and training has not converged. Right: HAL vs policy sketches on the Crafting Environment. HAL is
signiﬁcantly more sample efﬁcient since it is off-policy and uses relabeling among all modules.
6.3
Crafting Environment
To show the generality of the proposed framework, we apply our method to the Crafting environments
introduced by Andreas et al. [2] (Fig. 6, right). We apply HAL to this environment by training separate
policy networks for each module since there are fewer than 20 modules in the environment. These low-
level policies receive binary rewards (i.e. one-bit supervision analogous to completing an instruction),
and are trained jointly with HIR. Another high-level policy that picks which module to execute for
a ﬁxed 5 steps and is trained with regular DDQN. Note that our method uses a different form of
supervision compared to policy sketch since we provide binary reward for the low-level policy – such
supervision can sometimes be easier to specify than the entire sketch.
7
Discussion
We demonstrate that language abstractions can serve as an efﬁcient, ﬂexible, and human-interpretable
representation for solving a variety of long-horizon control problems in HRL framework. Through re-
labeling and the inherent compositionality of language, we show that low-level, language-conditioned
policies can be trained efﬁciently without engineered reward shaping and with large numbers of
instructions, while exhibiting strong generalizations. Our framework HAL can thereby leverage these
policies to solve a range of difﬁcult sparse-reward manipulation tasks with greater success and sample
efﬁciency than training without language abstractions.
While our method demonstrates promising results, one limitation is that the current method relies on
instructions provided by a language supervisor which has access to the instructions that describe a
scene. The language supervisor can, in principle, be replaced with an image-captioning model and
question-answering model such that it can be deployed on real image observations for robotic control
tasks, an exciting direction for future work. Another limitation is that the instruction set used is
speciﬁc to our problem domain, providing a substantial amount of pre-deﬁned structure to the agent.
It remains an open question on how to enable an agent to follow a much more diverse instruction
set, that is not speciﬁc to any particular domain, or learn compositional abstractions without the
supervision of language. Our experiments suggest that both would likely yield an HRL method
that requires minimal domain-speciﬁc supervision, while yielding signiﬁcant empirical gains over
existing domain-agnostic works, indicating an promising direction for future research. Overall, we
believe this work represents a step towards RL agents that can effectively reason using compositional
language to perform complex tasks, and hope that our empirical analysis will inspire more research
in compositionality at the intersection of language and reinforcement learning.
9

Acknowledgments
We would like to thank anonymous reviewers for the valuable feedback. We would also like to thank
Jacob Andreas, Justin Fu, Sergio Guadarrama, Oﬁr Nachum, Vikash Kumar, Allan Zhou, Archit
Sharma, and other colleagues at Google Research for helpful discussion and feedback on the draft of
this work.
References
[1] Peter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark Johnson, Niko Sünderhauf, Ian
Reid, Stephen Gould, and Anton van den Hengel. Vision-and-language navigation: Interpreting
visually-grounded navigation instructions in real environments. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), volume 2, 2018.
[2] Jacob Andreas, Dan Klein, and Sergey Levine. Modular multitask reinforcement learning with
policy sketches, 2016.
[3] Jacob Andreas, Dan Klein, and Sergey Levine. Learning with latent language, 2017.
[4] Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder,
Bob McGrew, Josh Tobin, OpenAI Pieter Abbeel, and Wojciech Zaremba. Hindsight experience
replay. In Advances in Neural Information Processing Systems, pages 5048–5058, 2017.
[5] Pierre-Luc Bacon, Jean Harb, and Doina Precup. The option-critic architecture. In AAAI, pages
1726–1734, 2017.
[6] Dzmitry Bahdanau, Felix Hill, Jan Leike, Edward Hughes, Pushmeet Kohli, and Edward
Grefenstette. Learning to understand goal speciﬁcations by modelling reward, 2018.
[7] Peter W Battaglia, Jessica B Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Vinicius
Zambaldi, Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan
Faulkner, et al. Relational inductive biases, deep learning, and graph networks. arXiv preprint
arXiv:1806.01261, 2018.
[8] Chris Callison-Burch, Miles Osborne, and Philipp Koehn. Re-evaluation the role of bleu in
machine translation research. In 11th Conference of the European Chapter of the Association
for Computational Linguistics, 2006.
[9] Nuttapong Chentanez, Andrew G Barto, and Satinder P Singh. Intrinsically motivated rein-
forcement learning. In Advances in neural information processing systems, pages 1281–1288,
2005.
[10] Maxime Chevalier-Boisvert, Dzmitry Bahdanau, Salem Lahlou, Lucas Willems, Chitwan Sa-
haria, Thien Huu Nguyen, and Yoshua Bengio. BabyAI: First steps towards grounded language
learning with a human in the loop. In International Conference on Learning Representations,
2019. URL https://openreview.net/forum?id=rJeXCo0cYX.
[11] Kyunghyun Cho, Bart van Merrienboer, Dzmitry Bahdanau, and Yoshua Bengio. On the
properties of neural machine translation: Encoder-decoder approaches, 2014.
[12] John D Co-Reyes, Abhishek Gupta, Suvansh Sanjeev, Nick Altieri, John DeNero, Pieter
Abbeel, and Sergey Levine. Meta-learning language-guided policy learning. In International
Conference on Learning Representations, 2019. URL https://openreview.net/forum?
id=HkgSEnA5KQ.
[13] Christian Daniel, Gerhard Neumann, and Jan Peters. Hierarchical relative entropy policy search.
In Artiﬁcial Intelligence and Statistics, pages 273–281, 2012.
[14] Abhishek Das, Georgia Gkioxari, Stefan Lee, Devi Parikh, and Dhruv Batra. Neural modular
control for embodied question answering. arXiv preprint arXiv:1810.11181, 2018.
[15] Peter Dayan and Geoffrey E Hinton. Feudal reinforcement learning. In Advances in neural
information processing systems, pages 271–278, 1993.
10

[16] Thomas G Dietterich. Hierarchical reinforcement learning with the maxq value function
decomposition. Journal of Artiﬁcial Intelligence Research, 13:227–303, 2000.
[17] Carlos Florensa, Yan Duan, and Pieter Abbeel. Stochastic neural networks for hierarchical
reinforcement learning. arXiv preprint arXiv:1704.03012, 2017.
[18] Kevin Frans, Jonathan Ho, Xi Chen, Pieter Abbeel, and John Schulman. Meta learning shared
hierarchies. International Conference on Learning Representations (ICLR), 2018.
[19] Daniel Fried, Ronghang Hu, Volkan Cirik, Anna Rohrbach, Jacob Andreas, Louis-Philippe
Morency, Taylor Berg-Kirkpatrick, Kate Saenko, Dan Klein, and Trevor Darrell. Speaker-
follower models for vision-and-language navigation. In Advances in Neural Information
Processing Systems, pages 3314–3325, 2018.
[20] Justin Fu, Anoop Korattikara, Sergey Levine, and Sergio Guadarrama. From language to
goals: Inverse reinforcement learning for vision-based instruction following. In International
Conference on Learning Representations, 2019. URL https://openreview.net/forum?
id=r1lq1hRqYQ.
[21] Lila Gleitman and Anna Papafragou. Language and thought. Cambridge handbook of thinking
and reasoning, pages 633–661, 2005.
[22] H Paul Grice. Logic and conversation. 1975, pages 41–58, 1975.
[23] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-
policy maximum entropy deep reinforcement learning with a stochastic actor. arXiv preprint
arXiv:1801.01290, 2018.
[24] Jean Harb, Pierre-Luc Bacon, Martin Klissarov, and Doina Precup. When waiting is not an
option: Learning options with a deliberation cost. arXiv preprint arXiv:1709.04571, 2017.
[25] Ji He, Jianshu Chen, Xiaodong He, Jianfeng Gao, Lihong Li, Li Deng, and Mari Ostendorf. Deep
reinforcement learning with a natural language action space. arXiv preprint arXiv:1511.04636,
2015.
[26] Nicolas Heess, Greg Wayne, Yuval Tassa, Timothy Lillicrap, Martin Riedmiller, and David Sil-
ver. Learning and transfer of modulated locomotor controllers. arXiv preprint arXiv:1610.05182,
2016.
[27] Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax.
arXiv preprint arXiv:1611.01144, 2016.
[28] Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Li Fei-Fei, C Lawrence Zitnick,
and Ross Girshick. Clevr: A diagnostic dataset for compositional language and elementary
visual reasoning. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 2901–2910, 2017.
[29] Leslie Pack Kaelbling. Hierarchical learning in stochastic domains: Preliminary results. In
Proceedings of the tenth international conference on machine learning, volume 951, pages
167–173, 1993.
[30] Russell Kaplan, Christopher Sauer, and Alexander Sosa. Beating atari with natural language
guided reinforcement learning, 2017.
[31] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
[32] Diederik P Kingma and Max Welling.
Auto-encoding variational bayes.
arXiv preprint
arXiv:1312.6114, 2013.
[33] George Konidaris and Andrew G Barto. Building portable options: Skill transfer in reinforce-
ment learning. In IJCAI, volume 7, pages 895–900, 2007.
11

[34] Tejas D Kulkarni, Karthik Narasimhan, Ardavan Saeedi, and Josh Tenenbaum. Hierarchical
deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation. In
Advances in neural information processing systems, pages 3675–3683, 2016.
[35] Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. End-to-end training of deep
visuomotor policies. The Journal of Machine Learning Research, 17(1):1334–1373, 2016.
[36] Andrew Levy, Robert Platt, and Kate Saenko. Hierarchical reinforcement learning with hindsight.
In International Conference on Learning Representations, 2019. URL https://openreview.
net/forum?id=ryzECoAcY7.
[37] Jelena Luketina, Nantas Nardelli, Gregory Farquhar, Jakob Foerster, Jacob Andreas, Edward
Grefenstette, Shimon Whiteson, and Tim Rocktäschel. A survey of reinforcement learning
informed by natural language. arXiv preprint arXiv:1906.03926, 2019.
[38] Shie Mannor, Ishai Menache, Amit Hoze, and Uri Klein. Dynamic abstraction in reinforcement
learning via clustering. In Proceedings of the twenty-ﬁrst international conference on Machine
learning, page 71. ACM, 2004.
[39] Luke Metz, Julian Ibarz, Navdeep Jaitly, and James Davidson. Discrete sequential prediction of
continuous actions for deep rl. arXiv preprint arXiv:1705.05035, 2017.
[40] Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization
for generative adversarial networks. arXiv preprint arXiv:1802.05957, 2018.
[41] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G
Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al.
Human-level control through deep reinforcement learning. Nature, 518(7540):529, 2015.
[42] Oﬁr Nachum, Shixiang Gu, Honglak Lee, and Sergey Levine. Data-efﬁcient hierarchical
reinforcement learning, 2018.
[43] Oﬁr Nachum, Shixiang Gu, Honglak Lee, and Sergey Levine. Near-optimal representation
learning for hierarchical reinforcement learning. International Conference on Learning Repre-
sentations (ICLR), 2019.
[44] Ashvin Nair, Vitchyr Pong, Murtaza Dalal, Shikhar Bahl, Steven Lin, and Sergey Levine.
Visual reinforcement learning with imagined goals. CoRR, abs/1807.04742, 2018. URL
http://arxiv.org/abs/1807.04742.
[45] Karthik Narasimhan, Tejas Kulkarni, and Regina Barzilay. Language understanding for text-
based games using deep reinforcement learning. arXiv preprint arXiv:1506.08941, 2015.
[46] Ronald Parr and Stuart J Russell. Reinforcement learning with hierarchies of machines. In
Advances in neural information processing systems, pages 1043–1049, 1998.
[47] Xue Bin Peng, Glen Berseth, KangKang Yin, and Michiel van de Panne. Deeploco: Dynamic lo-
comotion skills using hierarchical deep reinforcement learning. ACM Transactions on Graphics
(Proc. SIGGRAPH 2017), 36(4), 2017.
[48] Ethan Perez, Florian Strub, Harm De Vries, Vincent Dumoulin, and Aaron Courville. Film:
Visual reasoning with a general conditioning layer. In Thirty-Second AAAI Conference on
Artiﬁcial Intelligence, 2018.
[49] Steven T Piantadosi, Joshua B Tenenbaum, and Noah D Goodman. Bootstrapping in a language
of thought: A formal model of numerical concept learning. Cognition, 123(2):199–217, 2012.
[50] Vitchyr Pong, Shixiang Gu, Murtaza Dalal, and Sergey Levine. Temporal difference models:
Model-free deep rl for model-based control, 2018.
[51] Vitchyr H. Pong, Murtaza Dalal, Steven Lin, Ashvin Nair, Shikhar Bahl, and Sergey Levine.
Skew-ﬁt: State-covering self-supervised reinforcement learning. CoRR, abs/1903.03698, 2019.
URL http://arxiv.org/abs/1903.03698.
12

[52] Doina Precup. Temporal abstraction in reinforcement learning. University of Massachusetts
Amherst, 2000.
[53] Ehud Reiter. A structured review of the validity of bleu. Computational Linguistics, 44(3):
393–401, 2018.
[54] Himanshu Sahni, Toby Buckley, Pieter Abbeel, and Ilya Kuzovkin. Visual hindsight experience
replay. CoRR, abs/1901.11529, 2019. URL http://arxiv.org/abs/1901.11529.
[55] Tom Schaul, Daniel Horgan, Karol Gregor, and David Silver. Universal value function approxi-
mators. In International Conference on Machine Learning, pages 1312–1320, 2015.
[56] John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High-
dimensional continuous control using generalized advantage estimation.
arXiv preprint
arXiv:1506.02438, 2015.
[57] Tianmin Shu, Caiming Xiong, and Richard Socher. Hierarchical and interpretable skill acquisi-
tion in multi-task reinforcement learning. arXiv preprint arXiv:1712.07294, 2017.
[58] Olivier Sigaud and Freek Stulp. Policy search in continuous action domains: an overview. arXiv
preprint arXiv:1803.04706, 2018.
[59] David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur
Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of
go without human knowledge. Nature, 550(7676):354, 2017.
[60] Martin Stolle and Doina Precup. Learning options in reinforcement learning. In International
Symposium on abstraction, reformulation, and approximation, pages 212–223. Springer, 2002.
[61] Elior Sulem, Omri Abend, and Ari Rappoport. Bleu is not suitable for the evaluation of text
simpliﬁcation. arXiv preprint arXiv:1810.05995, 2018.
[62] Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. Sequence to sequence learning with neural
networks, 2014.
[63] Richard S Sutton, Doina Precup, and Satinder Singh. Between mdps and semi-mdps: A
framework for temporal abstraction in reinforcement learning. Artiﬁcial intelligence, 112(1-2):
181–211, 1999.
[64] Arash Tavakoli, Fabio Pardo, and Petar Kormushev. Action branching architectures for deep
reinforcement learning. In Thirty-Second AAAI Conference on Artiﬁcial Intelligence, 2018.
[65] Chen Tessler, Shahar Givony, Tom Zahavy, Daniel J Mankowitz, and Shie Mannor. A deep
hierarchical approach to lifelong learning in minecraft. In AAAI, volume 3, page 6, 2017.
[66] Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based
control. In Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ International Conference on,
pages 5026–5033. IEEE, 2012.
[67] Aäron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. Neural discrete representation
learning. CoRR, abs/1711.00937, 2017. URL http://arxiv.org/abs/1711.00937.
[68] Hado van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double
q-learning, 2015.
[69] Alexander Sasha Vezhnevets, Simon Osindero, Tom Schaul, Nicolas Heess, Max Jaderberg,
David Silver, and Koray Kavukcuoglu. Feudal networks for hierarchical reinforcement learning.
arXiv preprint arXiv:1703.01161, 2017.
[70] Yuhuai Wu, Harris Chan, Jamie Kiros, Sanja Fidler, and Jimmy Ba. ACTRCE: Augmenting expe-
rience via teacher’s advice, 2019. URL https://openreview.net/forum?id=HyM8V2A9Km.
13

A
Language Model as the High-level Policy
Picking an instruction from a pre-deﬁned set of instructions can limit the ﬂexibility of the high-level
policy and introduces scaling issues for more complex language instructions. In this section, we
present a method for using a language model as the high-level policy, thereby removing the constraints
of ﬁxed instruction set and potentially enabling much more powerful and general-purpose high-level
policy. Naively, directly using language as the action space is difﬁcult for RL agent as language
has extremely high-dimension and presents difﬁcult exploration challenges. An insight that we can
leverage is that many language statements, especially instructions, lie in a much lower dimension
space. To that end, we can learn a low-dimension embedding space of the instructions and then set
the action space of the high-level RL agent to be the embedding space. Note that naively training a
latent variable model such as a VAE [32] is unlikely work well since the latent space does not have
the structure necessary for being a good action space – an indirect example that demonstrates this fact
is that the non-compositional representation for the low-level policy, which is also a latent variable
model completely fails to learn a reasonable policy, even though it contains perfect information about
the instruction. We hypothesize that to facilitate learning, the embedding space must be sufﬁciently
disentangled. Learning a disentangled representation of language without any supervision is very
much still an open problem and we present a candidate solution that can learn an well disentangled
embedding space (for a relatively domain-speciﬁc language) that can efﬁciently be used as the action
space for the high-level policy.
A.1
Learning a Disentangled Representation of Language
Unlike modeling the density of language with architectures such as GPT or Transformers, we want to
learn an disentangled representation of language via an embedding space. To this end, we leverage
an InfoGAN-like auto-regressive architecture that turns an embedding to a sentence, and use a
mutual information based objective to encourage disentanglement. Further, since the generated
tokens are discrete, we explore several method for efﬁciently training the generator with gradients
and ﬁnd that using vector quantization [67] empirically outperforms unbiased methods such as the
Gumbel-softmax trick [27].
Notations:
1. The latent vector ez = (z, c) ∈R10 is a concatenation of a noise vector z ∈R4 sampled
from an isotropic Gaussian and a latent code c ∈R6 sampled uniformly from the hypercube
[−1, 1]6. With a little abuse of notations, we use ezc to denote the latent code components of
the latent vector ez.
2. V = {vi}NV
i=1 is the set of all vocabularies with size NV
3. E = {ei}NV
i=1 is a set embedding vector for each vocabulary where e ∈Rde. Further, for
stability, each ei is normalize to be unit vector such that ||ei||2 = 1 (constrained to a unit
sphere).
4. x = [v1, v2, . . . , vm] is real instruction datum, assuming all instructions are of length m
(padded with ⟨EOS⟩if shorter).
5. xi is the ith example in a collection of sampled instructions X = {xi}NX
i=1.
6. G(ez; θG) is the generator that maps a latent code to a vector xgen ∈Rm×de. Each row of
x(i)
gen can be mapped to some
e(x(i)
gen) = arg min
ej
||ej −x(i)
gen||2
7. D(x ; θD) maps a sample to a scalar; D is constrained to be 1-Lipschitz.
8. Q(xgen ; θQ) maps a generated sample to its latent code ec. Q shares the parameters all
layers except for the last layer with D
9. sg denotes stop gradient operator
Then the training objective of the generator is deﬁned as:
ℓV Q(ez) = 1
m
m
X
i=1
|| sg[G(ez)(i)] −e(G(ez)(i)) ||2
2 + β|| G(ez)(i) −sg[e(G(ez)(i))] ||2
2
(2)
14

ℓG = Eez[−D(G(ez)) + huber(Q(G(ez)), ezc) + ℓV Q(ez)]
(3)
(E∗, θ∗
G) = arg min
E,θG
ℓG
(4)
β is a hyperparameter for the commitment loss (second term in the VQ objective) and huber denotes
the Huber loss. Note that Huber loss is used instead of ℓ2 loss which corresponds to true maximum
likelihood objective as ℓ2 loss is very unstable.
Then the training objective of the discriminator is deﬁned as:
ℓD = Eez[ max(0, 1 + D(G(ez))) ] + Ex∼U(X)[ max(0, 1 −D(x)) ]
(5)
θ∗
D =
arg min
θD s.t D∈C1,1 ℓD
(6)
Note that the discriminator does not optimize E. Including E causes instability likely from competition
between the generator and discriminator. The generator cannot cheat by forcing all embeddings to be
the same (which will fool the discriminator that does not have control over the embedding) because
of the mutual information constraint below.
The training objective of the decoder is deﬁned as:
ℓQ = Eez[ huber(Q(G(ez)), ezc) ]
(7)
θ∗
Q = arg min
θQ
ℓQ
(8)
For the components shared with D, the 1-Lipschitz constraint is also enforced.
A.2
Embedding as Action
After training the architecture above, we have a function G that maps a noise vector ez to a sentence.
We train a SAC [23] agent πh that outputs action a ∈R10. This action is mapped to a instruction
g = G(a) which is in turn interpreted and executed by the low-level policy πl. The policy is trained
as a regular reinforcement learning agent on the high-level tasks.
A.3
Architecture Details
The generator network consists of a GRU cell with 128 hidden units. There are 50 available tokens
for all the sentences in the environment and we choose the embedding size to be 64 so the embedding
vector is of dimension 50×32. The hidden state at each time step of the generator is linearly projected
to dimension 32.
The discriminator architecture is a feed-forward 1D convolutional neural network with residual
connections. The components shared by the discriminator and the decoder consists of 2 identical
residual blocks, each of which consists of an identity block plus the output of convolution with
window size 1 and output channel 32, leaky relu, convolution with window size 3 and output channel
64, and leaky relu. All of the convolution layers are spectrally normalized [40]. Following these are
branches of two identical residual blocks – one for the discriminator and the other for the decoder.
Finally, the outputs of both are reduced to a single scalar.
A.4
Experiments
We trained the above language model with 10000 randomly sampled instructions and trained a SAC
agent using the resulting generator on the ColorSort task from the diverse setting. The diverse setting
has much more possible instructions compared to the ﬁxed object setting and can beneﬁt more from
having access to more instructions at the high-level policy. The comparison between language model
and ﬁxed instruction set (Discrete) is shown below in Figure 7.
Currently the language model agent performs comparably to the discrete agent. This may be due the
fact that learning a disentangled generative model of language is difﬁcult – in particular, we observed
that the model tends to drop modes which can affect the ﬁnal policy negatively. We expect the result
to signiﬁcantly improve with better language models or even visual-linguistic models. Further, the
ﬁxed instruction baseline shown above under-performs the one used in the main text because it does
not use prioritized replay but instead uses uniform replay buffer. We observed decreased performance
for SAC when prioritized replay is used; as such, we decided to use a uniform replay buffer for fair
comparison.
15

Figure 7: Results of an agent using a language model vs an agent picking from a ﬁxed set of instructions.
B
Environment, Model Architectures, and Implementation
In this section, we describe the environment and tasks we designed, and various implementation
details such as the architectures of the policy networks.
B.1
CLEVR-Robot Environment
We want an environment where we can evaluate the agent’s ability to learn policies for long-horizon
tasks that have compositional structure. In robotics, manipulating and rearranging objects is a
fundamental way through which robots interact with the environment, which is often cluttered
and unstructured. Humans also tend to decompose complex tasks into smaller sub-tasks in the
environment (e.g. putting together a complicated model or write a piece of program). While previous
works have studied the use of language in navigation domains, we aim to develop an environment
where the agent can physically interact with and change the environment. To that end, we designed a
new environment for language and manipulation tasks in MuJoCo where the agents must interact
with the objects in the scene. To succeed in this environment, the agents must be able to handle a
variety of objects with diverse visual and physical properties. Since our environment is inspired by
the CLEVR dataset, we name our environment CLEVR-Robot Environment.
We will refer to all the elements in the environment collectively as the world state. The environment
can contain up to 5 object. Each object is represented by a vector oi that is the concatenation of 3d
coordinate, pi, of its center of mass, and a one-hot representation of its 4 properties: color, shape,
size, and material. The environment keeps an internal relation graph Gadj for all the objects currently
in the scene. The relation graph is stored as an adjacency list whose ith entry is a nested array storing
oi’s neighbors in 4 cardinal directions left, right, front and behind. The criterion for oj to be the
neighbor of oi in certain direction is if ||pj −pi|| ≤rmax and the angle between pj −pi and the
cardinal vector of that direction is smaller than βmax. After every interaction between the agent and
the environment, oi and the relation graph are updated to reﬂect the current world state.
The agent takes the form of a point mass that can teleport around, which is a mild assumption for
standard robotic arms (Other agents are possible as well). Before each interaction, the environment
stores a set of language statements that are not satisﬁed by the current world state. These statements
are re-evaluated after the interaction. The statements whose values change to True during the
interaction can be used as the goals or instructions for relabeling the trajectories (cf. pre and post
conditions used in classical AI planning). Assuming the low-level policy only follows a single
instruction at any given instant, the reward for every transition is 1 if the goal is achieved and 0
otherwise. The action space we use in this work consists of a point mass agent pushing one object in
1 of the 8 cardinal directions for a ﬁxed number of frames, so the discrete action space has size 8kt,
where kt ≤5 is the number of objects.
B.2
Tasks
The high-level policy’s reward function can be tailored towards the task of interests. We propose
three difﬁcult benchmark tasks with extremely sparse rewards.
16

B.2.1
Five Object Settings (Standard)
In this setting, we have a ﬁxed set of 5 spheres of different colors cyan, purple, green, blue, red.
The ﬁrst task we consider is object arrangement. We sample a random set of statements that can
be simultaneously satisﬁed and, at every time step, the agent receives a reward of -10.0 if at least 1
statement is not satisﬁed satisﬁed and 0.0 only if all statements are satisﬁed. At the beginning of every
episode, we reset the environment until none of the statements is satisﬁed. The exact arrangement
constraints are: (1) red ball to the right of purple ball; (2) green ball to the right of red ball; (3) green
ball to the right of cyan ball; (4) purple ball to the left of cyan ball; (5) cyan ball to the right of purple
ball; (6) red ball in front of blue ball; (7) red ball to the left of green ball; (8) green ball in front of
blue ball; (9) purple ball to the left of cyan ball; (10) blue ball behind the red ball
The second task is object ordering. An example of such a task is “arrange the objects so that their
colors range from red to blue in the horizontal direction, and keep the objects close vertically". In
this case, the conﬁguration can be speciﬁed with 4 pair-wise constraint between the objects. We reset
the environment until at most 1 pair-wise constraint is satisﬁed involving the x-coordinate and the
y-coordinate. At every time step, the agent receives a reward of -10.0 if at least 1 statement is not
satisﬁed and 0.0 only if all statements are satisﬁed. The ordering of color is: cyan, purple, green,
blue, red from left to right.
The third task is object sorting. In this task, the agent needs to sort 4 object around a central object;
furthermore, the 4 objects cannot be too far away from the central object. Once again, the agent
receives a reward of -10.0 if at least 1 constraint is violated, and 0.0 only if all constraints are satisﬁed.
Environment is reset until at most 1 constraint is satisﬁed. Images of the end goal for each high-level
tasks are show in Figure 3.
B.2.2
Diverse Object Settings
Here, instead of 5 ﬁxed objects, we introduce 3 different shapes cube, sphere and cylinder in
combinations with 5 colors. Both colors and shapes can repeat but the same combination of color and
shape does not repeat. In this setting, there are
 15
5

= 3003 possible object conﬁgurations. In this
setting, we deﬁne the color hierarchy to be red, green, blue, cyan, purple from left to right and the
shape hierarchy to be sphere, cube, cylinder from left to right. Sample goal states of each task are
shown in 3.
The ﬁrst task is color ordering where the agent needs to manipulate the objects such that their colors
are in ascending order.
The second task is shape ordering where the agent needs to manipulate the object such that their
shapes are in ascending order.
Finally, the last task is color & shape ordering where the agent needs to manipulate the object such
that the color needs to be in ascending order, and within each color group the shapes are also in
ascending order.
Like in the ﬁxed object setting, the agent only receives 0 reward when the objects are completely
ordered; otherwise, the reward is always -10.
B.3
Implementation details
Language supervisor. In this work, each language statement generated by the environment is
associated with a functional program that can be executed on the environment’s relation graph to
yield an answer that reﬂects the value of that statement on the current scene. The functional programs
are built from simple elementary operation such as querying the property of objects in the scene, but
they can represent a wide range of statements of different nature and can be efﬁciently executed on
the relation graph. This scheme for generating language statements is reminiscent of the CLEVR
dataset [28] whose code we drew on and modiﬁed for our use case. Note that a language statement
that can be evaluated is equivalent to a question, and the instructions we use also take the form of
questions. For simplicity and computational efﬁciency, we use a smaller subset of question family
deﬁned in CLEVR that only involves pair-wise relationships (one-hop) between the objects. We plan
to scale up to full CLEVR complexity and possibly beyond in future works.
17

State based low-level policy. When we have access to the ground truth states of the objects in the
scene, we use an object-centric representation of states by assuming st = {oi}kt
i=1, where oi ∈Rdo
is the state representation of object i, and kt is the number of objects (which can change over time).
We also assume at = {αi}kt
i=1, where each αi ∈Rdα acts on individual object oi.
We implement a specialized universal value function approximator [55] for this case. To handle
a variable number of relations between the different objects, and their changing properties, we
built a goal-conditioned self attention policy network. Given a set of k object {oi}k
i=1, we ﬁrst
create pair-wise concatenation of the objects, O = {oi∥oj}k,k
j=1,i=1. Then we transform every
pair-wise vectors with a single neural network f1 into Z = {f1(oi∥oj)}k,k
j=1,i=1. A recurrent neural
network with GRU [11], f2, embeds the instruction g into a real valued vector eg = f2(g). We
use the embedding to attend over every pair of objects to compute weights {wi = ⟨eg, zi⟩|zi ∈Z}.
We then compute a weighted combination of all pi where the weights are equal to the softmax
weights exp(wi)/ Pk2
j=1 exp(wj). This combination transforms the elements of Z into a single
vector ¯z of ﬁxed size. Each oi is concatenated with eg and ¯z into o′
i = (oi∥eg∥¯z). Then each o′
i is
transformed with the another neural network f3 whose output is of dimension dα. The ﬁnal output
Q = {f3(oi∥eg∥¯z)}k
i=1 is in Rk×dα which represents all state-action values of the state. Illustration
of the architecture is shown in Figure 8.
Figure 8: Computation graph of the state-based low level policy.
Image based low-level policy. In reality, we often do not have access to the state representation of
the scene. For many application, a natural alternative is images. We assume st ∈[0, 1]W ×H×C is
the available image representation of the scene (in all experiemnts, W=64, H=64, C=3). Further, we
need to adopt a more general action space since we no longer have access to the state representation
(e.g. coordinates of the location). To that end, we discretize the 2D space into 10 × 10 grids and an
action involves picking an starting location out of the 100 available grid and a direction out of the the
8 cardinal direction to push. This induces an 800 dimensional discrete action space.
It is well-known that reinforcement learning from raw image observation is difﬁcult, and even off-
policy methods require millions of interaction on Atari games where the discrete action space is small.
Increasing the action space would understandably make the already difﬁcult exploration problem
harder. A potential remedy is found in the fact that these high-dimensional action space can often
be factorized into semantically meaningful groups (e.g. the pushing task can be break down into
discrete bins of the x and y axes as well as a pushing direction). Previous works attempted to leverage
this observation by using auto-regressive models or assuming conditional independence between the
groups [39, 64]. We offer a new approach that aims to make the least assumptions. Following the
group assumption, we assume there exists m = 3 groups and each group consists of km discrete
sub-actions (i.e. Am = {a(1)
m , a(2)
m , . . . a(km)
m
}). Following this deﬁnition, we can build a bijective
18

look-up map ζ between A to tuples of sub-actions:
B =
m
Y
n=1
{1, 2, . . . , kn}
(9)
A
ζ
=⇒{(a(i1)
1
, . . . , a(im)
m
) | ∀(i1, . . . , im) ∈B}
(10)
We overload the notion a(i)
m (s) to the action feature of a(i)
m conditioned on the state and goal and
ζ(a, s) to be a tuple of the corresponding action features. Then the value function each action can be
represented as:
Q(s, a) = fψ(ζ(a, s))
(11)
where fψ is a single neural network parameterized by ψ that is shared between all a. This model
does not require picking an order like the auto-regressive model and does not assume conditional
independence between the groups. Most importantly, the number of parameters scales sublinearly
with the dimension of the actions. The trade-off of this model is that it can be memory-expensive
to model the full joint distribution of actions at the same time; however, we found that this model
performs empirically well for the pushing tasks considered in this work. We will refer to this operation
as Tensor Concatenation.
The overall architecture for the UVFA is as follows: the image input is fed through 3 convolution
layers with kernel size {8, 5, 3}, stride {2, 2, 1}, and channel {46, 128, 64}; each convlution block is
FiLM’d [48] with the instruction embedding. Then the activation is ﬂattened spatially to 256 × 64
and projected to 28 × 64. This is further split into 3 action group of sizes {10 × 64, 10 × 64, 8 × 64}
and fed through tensor concatenation. fψ is parameterized by a 2-layer MLP with 512 hidden units at
each layer and output dimension of 1 which is the Q-value.
Figure 9: Computation graph of the vision-based low level policy.
Both policy networks are trained with HIR. Training mini-batches are uniformly sampled from the
replay buffer. Each episode lasts for 100 steps. When the current instruction is accomplished, a new
one that is currently not satisﬁed will be sampled. To accelerate the initial training and increase the
diversity of instructions, we put a 10 step time limit on each instruction, so the policy does not get
stuck if it is unable to ﬁnish the current instruction.
High-level policy. For simplicity, we use a Double DQN [68] to train the high-level policy. We use
an instruction set that consists of 80 instructions (|I| = 80 for standard and |I| = 240 for diverse)
that can sufﬁciently cover all relationships between the objects. We roll out the low-level policy
for 5 steps for every high-level instruction (T ′ = 5). Training mini-batches are uniformly sampled
from the replay buffer. The state-based high-level policy uses the same observation space as the
low-level policy; the image-based high-level policy uses extracted visual features from the low-level
policy and then extracts salient spatial points with spatial softmax [35]. The convolutional weights
are frozen during training. This design choice makes natural sense since humans also use a single
visual cortex to process all initial visual signals, but training from scratch is certainly possible, if not
ideal, should computation budget not matter. For the diverse visual tasks, we found that using the
convolutional features with spatial softmax could not yield a good representation for the downstream
tasks. Due to time constraints, the experiments shown for the diverse high-level tasks use the ground
truth state, namely position, one-hot encoded colors, and shapes for the high-level policy; however,
the low-level policy only has access to the image. We believe a learned convolutional layer would
19

solve this problem. Finally, we note that the high-level policy picks each sentence independent and
therefore does not leverage the structure of language. While generating language is generally hard, a
generative model would have more natural resemblance to thought. We think this is an extremely
important and exciting direction for future work.
20

C
Algorithms
In this section we elaborate on our proposed algorithm and lay out ﬁner details.
C.1
Overall algorithm
The overall hierarchical algorithm is as follows:
Algorithm 1 Overall Hierarchical Training
1: Inputs: Low level RL algorithm Al; High-level RL algorithm Ah; Environment E; other relevant
inputs of Algorithms 2 and 3
2: πl(a|s, g) ←low-level policy trained with Al and other appropriate inputs (Algorithm 2)
3: πh(g|s) ←high-level policy trained with Ah, πl(a|s, g) and other appropriate inputs (Algorithm
3)
4: return πl(a|s, g) and πh(g|s)
C.2
Training the low-level policy
For both state-based and vision-based experiments, we use DDQN as Al. For the state-based
experiments, the agent receives binary reward based on whether the action taken completes the
instruction; for the vision-based experiments, we found it instrumental to add an object movements
bonus, i.e. if the agents change the position of the objects by some minimum threshold, the agetn
receives a 0.25 reward. This alleviates the exploration problem in high dimensional action space
(R800), but the algorithm is capable of learning without the exploration bonus. (Algorithm 2). We
adopt the similar setting as HER where unit of time consists of epochs, cycles and episode. Each cycle
consists of 50 episode and each episode consists of 100 steps. While we set the number of epoch to
50, in practice we never actually reach there. We adopt an epsilon greedy exploration strategy where
at the beginning of every cycle we multiply the exploration by a factor of 0.993, starting from 1 but
at the beginning we 10 cycles to populate the buffer. The minimum epsilon is 0.1. We use γ = 0.9
and replay buffer of size 2e6. The target network we use is a 0.95 moving average of the model
parameters, updated at the beginning of every cycle. Every episode, we update the network for 100
steps with the Adam Optimizer at minibatch of size 128 randomly sampled from the replay buffer.
C.3
Training the high-level policy
Ah is also a DDQN. One single set of hyperparameter is used for standard experiments and another
for the diverse experiments. DDQN is trained for 2e6 steps, uses uniform replay buffer of size 1e5,
linearly anneals epsilon from 1 to 0.05 in 3e5 steps, Adam and batch size 256. T ′ = 5 for all our
experiments, but the experience the network sees is equivalent of 1 step. For the diverse settings, due
to time constraint, we use priority replay buffer of size 1e6 for all diverse experiment including the
DDQN baselines. (Algorithm 3) We use the Adam Optimizer [31] with initial learning rate of 0.0001.
Discount factor γ = 0.9. Learning starts after 100 episodes each of which lasts for 100 steps (100
high-level actions).
C.4
Relabeling Strategy
HER [4] demonstrated that the relabeling strategy for trajectories can have signiﬁcant impacts on the
performance of the policy. The most successful relabeling strategy is the “k-future” strategy where
the goal state and the reward are relabeled with k states in the trajectories that are reached after the
current time step and the reward is discounted based on the discount factor γ and how far away the
current state is from the future state in ℓ2 distance. We modify this strategy for relabeling a language
conditioned policy. One challenge with language instruction is that the notion of distance is not well
deﬁned as the instruction is under-determined and only captures a part of the information about the
actual state. As such, conventional metrics for describing distance between sequences of tokens (e.g.
edit distance) do not actually capture the information we are interested in. Instead, we adopt a more
“greedy” approach to relabeling by putting more focus on 1-step transition where the instruction is
actually fulﬁlled. Namely, we add all transition tuples in Vt to the replay buffer B (Algorithm 2). For
21

Algorithm 2 RL with hindsight instruction relabeling (HIR)
1: Inputs: off-policy RL algorithm Al; instruction relabeling strategy S; language supervisor Ω;
Environment E; number of relabeled future K
2: Initialize replay buffer B and πl(a|s, g)
3: for episode i = 1 to M do
4:
s0 ←reset E
5:
g ∼U({g ∈Ω(s0) | Ψ(s0, g) = 0})
6:
τ ←[ ]
7:
for step t = 0 to T do
8:
Ut ←{g ∈Ω(st) | Ψ(st, g) = 0}
9:
at ∼πA(a|st, g)
10:
st+1 ←Take action at from st
11:
rt ←Ψ(st+1, g)
12:
Vt ←Ut \ {g ∈Ω(st+1) | Ψ(st+1, g) = 0}
13:
add (st, at, g, rt, st+1, at+1, Vt) to τ
14:
if rt = 1 then
15:
g ∼U({g ∈Ω(st+1) | Ψ(st+1, g) = 0})
16:
end if
17:
end for
18:
for step t = 0 to T do
19:
B ←B ∪{(st, at, g, rt, st+1, at+1)}
20:
for g′ ∈Vt do
21:
B ←B ∪{(st, at, g′, 1, st+1, at+1)}
22:
end for
23:
W ←S(τ, t, K)†
24:
for (g′, r′) ∈W do
25:
B ←B ∪{st, at, g′, r′, st+1, at+1)}
26:
end for
27:
end for-
28:
Update πl(a|s, g) with Al using minibtach from B
29: end for
30: return πl(a|s, g)
31: † Details in Appendix C.4; U(·) denotes uniform sampling from the given set.
future relabeling, we simply use the reward discounted by time steps into the future to relabel the
trajectory. While the discounted reward does not usually capture the “optimal” or true discounted
reward, we found it provides sufﬁcient learning signal. Detailed steps are shown below (Algorithm
4). In our experiments, we use K = 4.
In additon to relabeling, if an object is moved (using 800 dimensional action space), we add to replay
buffer a transition where the instruction is the name of the object such as “large rubber red ball” with
associated reward of 1.0. We found this helps the agent to learn the concept of objects. We refer to
this operation as Unary Relabeling.
D
Experimental Details
D.1
One-hot encoded representation
We assign each instruction a varying number of bins in the one-hot vector. Concretely, we give
each instruction of the 600 instruction 1, 4, 10, and 20 bins in the one-hot vector, which means the
effective size of the one-hot vector is 600, 2400, 6000 and 12000. When sampling goals, each goal is
uniformly dropped into one of its corresponding bins. The one-hot vector is embedded with a 2 layer
MLP with 64 hidden units at each layer.
22

Algorithm 3 Training high-level policy
1: Inputs: Any RL algorithm Ah; reward function R : S →[rmin, rmax]∗; instruction set I;
instruction encoder φ; low-level policy πl(a|s, g)
2: Initialize A
3: for episode i = 1 to M do
4:
s0 ←reset E
5:
for step t = 0 to T do
6:
g ←Sample from I using πh(g|st)
7:
s′ ←st
8:
for substep t′ = 1 to T ′ do
9:
a′ ∼πl(a|s′, g)
10:
s′ ←Take action a′ from s′
11:
end for
12:
st+1 ←s′
13:
Store experience
14:
end for
15:
Update Ah accordingly with experience collected
16: end for
17: *Here we assume the reward is only based on the new state for simplicity
Algorithm 4 Future Instruction Relabeling Strategy (S)
1: Inputs: Trajectory τ; current time step t; number of relabeled future K
2: ∆←[ ]
3: count ←0
4: while count < K do
5:
future ∼U({t + 1, . . . , |τ|})
6:
(s, a, g, r, s′, a′, V) ←τ[future]
7:
if |V| > 0 then
8:
g′ ∼U(V)
9:
r′ ←r · γfuture−t
10:
Store (g′, r′) in ∆
11:
count ←count + 1
12:
end if
13: end while
14: return ∆
D.2
Non-compositional representation
To faithfully evaluate the importance of compositionality, we want a representation that carries the
identical information as the language instruction but without the explicit compositional property
(but perhaps still to some degree compositional). To this end, we use a Seq2Seq [62] autoencoder
with 64 hidden units to compress the 600 instructions into real-valued continuous vectors. The
original tokens are fully recovered which indicates that the compression is lossless and the latent’s
information content is the same as the original instruction. This embedding is used in place of
the GRU instruction embedding. We also observed that adding regularization to the autoencoder
decreases the performance of the resulting representation. For example, decreasing the bottleneck size
leads to worse performance, as does adding dropout. Figure 4 uses an autoencoder with dropout of
0.5 while 1 uses one with no dropout. As shown in the experiments, the performance without dropout
is better than the one with. We hypothesize that adding regularization decreases the compositionality
of the representation.
D.3
Bag-of-Words representation
Bag-of-words is a popular technique in natural language processing where a sentence or corpus of
texts are treated as a histogram of tokens – i.e. the order of the tokens are discarded and only the
frequency at which a token occurs is considered. For each instruction, we count the occurrence of
23

each token available in the vocabulary and normalize the count by the length of the instruction. This
vector is then embedded with a 2 layer MLP with 64 hidden units at each layer.
D.4
Non-hierarchical baseline
We use the Double DQN implementation from OpenAI baselines3. We use a 2 layer MLP with 512
hidden units at each layer with the same action dimension as the low-level policy.
D.5
HRL baselines
In general, we note that it is difﬁcult to compare different HRL algorithms in an apples-to-apples
manner. HIRO assumes a continuous goal space so we modiﬁed the goal to be an R10 vector
representing the locations of each object rather than language. In this regime, we observed HIRO was
unable to make good progress. We hypothesize that the highly sparse reward might be the culprit. It
is also worth noting that HIRO uses a goal space in R2 for navigation (which is by itself a choice of
abstraction because the actual agent state space is much higher) while ours is of higher dimensionality.
The Tensorﬂow implementation of HIRO we used can be found at the ofﬁcial Tensorﬂow repository4.
(This is the implementation from the original author).
Option-critic aims to learn everything in a complete end-to-end manner which means it does not use
the supervision from language. It is unsurprising that the sparse tasks do not provide sufﬁcient signal
for OC. In other words, our method enjoys the beneﬁt of a ﬂexible but ﬁxed abstraction while OC
needs to learn such abstraction. We tried 8, 16, and 32 options for OC but our method has much more
sub-policies due to the combinatorial nature of language. The OC implementation in Tensorﬂow we
used is open-sourced5.
D.6
Hardware Specs and Training time
All of our experiments are performed on a single Nvidia Tesla V100. We are unable to verify the
specs of the virtual CPU. The low-level policy for state-based observation takes about 2 days to train
and, for image-based observation, 6 days. The high-level policies for the state-based observation takes
about 2 days to train and 3 days for image-based observations (wall-clock time). The implementations
are not deliberately optimized for performance as the major bottleneck is actually the language
supervisor so it is very likely the time could be dramatically shortened.
E
More Experimental Results and Discussions
E.1
Low-level policy for a diverse environment
Figure 10 shows the training instruction per episode on the diverse environment. We see that the
performance is worse than a ﬁxed number of objects with the same amount of experience. This is
perhaps not surprising considering the visual tasks are much more diverse and hence more challenging.
Figure 10: Results training the low-level policy on the diverse environment.
3https://github.com/openai/baselines/tree/master/baselines/deepq
4https://github.com/tensorﬂow/models/tree/master/research/efﬁcient-hrl
5https://github.com/yadrimz/option-critic
24

E.2
Proposed environment, sparse reward, and structured exploration
While DDQN worked on 2 cases in the state-based environment, it is unable to solve any of the
problems in visual domain. We hypothesize that the pixel observations and increase in action space
(20× increase) makes the exploration difﬁcult for DDQN. The difﬁculty of the tasks – in particular the
3 standard tasks – is reﬂected in the fact that the reward from non-hierarchical random action is stably
0 with small variance, meaning that under the sparse reward setting the agent almost never visits the
goal states (states with non-zero reward). On the other hand, the random exploration reward is much
higher for our method as the exploration in the space of language is structured as the exploration is
aware of the task-relevant structure of the environment.
25

