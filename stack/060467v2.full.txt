Neurobiology as information physics 
 
Sterling Street 
Independent Researcher 
 
 
 
Abstract  
 
This article reviews the preliminary results of an analysis of the brain that may supplement our 
current understanding of its structure and function, even if by falsification. This analysis seems to 
reveal that the information in a system of the brain can be quantified to an appreciable degree of 
objective precision, that many qualitative properties of the information content of a system of the 
brain can be inferred by observing matter and energy, and that many general features of the 
brain’s anatomy and architecture illustrate simple information-energy relationships. It is possible 
that the brain provides a unique window into the relationship between matter, energy, and 
physical information. 
 
Introduction  
 
That information is physical has been suggested by evidence for over a century (S Lloyd 2006). 
In recent years, Landauer’s principle (R Landauer 1961, CH Bennett et al. 2003), which relates 
informational entropy to thermodynamic entropy, has been confirmed (A Bérut et al. 2012; Y 
Jun et al. 2014; AO Orlov et al. 2012; B Piechocinska 2000), and information-energy 
equivalence (A Alfonso-Faus 2013) has been shown to qualify the existence of thermodynamic 
demons (S Toyabe et al. 2010; K Maruyama et al. 2009). The finding that information is 
conserved as event horizon area has led to the tentative resolution of major paradoxes in black 
hole thermodynamics (L Susskind and J Lindesay 2005; K Bradler and C Adami 2014, SW 
Hawking 2005), and the holographic principle is currently well-supported by evidence (R 
Bousso 2002, T Asselmeyer-Maluga 2015). Many classic mysteries of quantum mechanics, 
including entanglement and double-slit interference, may demonstrate properties of physical 
information (Č Brukner and A Zeilinger 2003; A Zeilinger 2004), and information-interpretive 
approaches in quantum gravity research (JW Lee et al. 2013; E Verlinde 2011) could even lead 
to the unification of quantum mechanics and general relativity. Surprisingly, however, “a 
formalization of the relationship between information and energy is currently lacking in 
neuroscience” (G Collell and J Fauquet 2015). The purpose of this article is to explore a few 
different sides of this relationship and, along the way, to suggest that many competing models 
and theories in neuroscience can be unified through the physics of information.   
 
 
 
 
 
 
.
CC-BY 4.0 International license
a
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under 
The copyright holder for this preprint (which was not
this version posted July 6, 2016. 
; 
https://doi.org/10.1101/060467
doi: 
bioRxiv preprint 

I. Information bounds  
 
“How can the events in space and time which take place within the spatial boundary of a living 
organism be accounted for by physics and chemistry?” – (E Schrödinger 1944) 
 
Information in any physical system is finite, and its flow is inevitably tied to thermodynamics. 
As a result, the information content of any system in the brain at any level of structure can be 
quantified by applying a modified form of the universal (JD Bekenstein 1981, 1984, 2001, 2005; 
J Oppenheim 2015) information-entropy bound:  
 
Isys  = ζ √� �� 
ħ�   
 
where Isys is the quantity of units of information in a physical system, A is area in m2, E is energy in joules, ħ is the 
reduced Planck constant, c is the speed of light in meters per second, and ζ is a factor of information saturation such 
that 0 ≤ ζ ≤ 1  
 
In short, a system approaches its information bound as it approaches a state of maximum 
information saturation. In other words, the bound quantifies the absolute information present in a 
system, while the portion of this information available for inference (available for performing 
logical operations or receiving in an influence network node or observatorial apparatus) is 
necessarily observer-dependent and generally far lower than quantity given by the bound. It is 
critical to make this distinction; a number of researchers have used the bound to estimate the 
information capacity of the brain as a hypothetical upper limit rather than as an absolute quantity 
of information, and a central argument among their critics is that the bound overestimates the 
quantity of information that is functionally relevant in cognition. Further, in the case that 
quantum effects play a meaningful role in cognition, this bound includes both classical 
information in bits and quantum information in qubits. The bound quantifies units of information 
of any base or classification and varies only with the area and energy content of a system.  
 
The factor ζ  partitions absolute information from observer-dependent information by separating 
entropic information from relative information. In other words, while absolute saturation (ζ = 1) 
specifies the absolute quantity of information present in a thermodynamic universe, relative 
saturation (0 ≤ ζ ≤ 1) specifies the quantity of absolute information that is available for influence 
into a referential system. Since entropy is hidden information (C Adami 2011; L Susskind 2008; 
S Lloyd 2006) with respect to a referential system, a system transferring none of its information 
to a referential system (resulting in maximal KL entropy) carries an entropic saturation factor of 
1 and an informational saturation factor of 0. On the other hand, a system transferring the 
entirety of its information to a referential system (resulting in minimal KL entropy) carries an 
entropic saturation factor of 0 and an informational saturation factor of 1. The flow of 
information from an index to a referential system would parallel the crossing of Markov 
boundaries (KJ Friston 2013) and Fisher information spaces (BR Frieden 2015).  
 
 
 
.
CC-BY 4.0 International license
a
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under 
The copyright holder for this preprint (which was not
this version posted July 6, 2016. 
; 
https://doi.org/10.1101/060467
doi: 
bioRxiv preprint 

II. Stratification  
 
“If information forms the most basic layer of reality, what does it refer to?” – (A Aguirre et al. 2015) 
 
Consider the information processed by the four distinct bases of DNA. The quantity of 
information contained in a single base molecule includes the information in its constituent atoms 
and subatomic particles, all the way down to the quantum information evaluated in their spin 
states, angular momenta, vibrational quanta, and so forth. How is it that we can ignore this 
information when analyzing base pair sequences? Although the absolute information in a single 
DNA base far exceeds the relative information it transfers to its referents in a process such as 
protein synthesis, DNA lies within an effectively base-four layer of influence – the alternative 
evaluation of a qubit that determines the spin of a proton in an adenine molecule is very unlikely 
to alter the effective structure of adenine, much less a resultant base pair sequence or the 
structure of a protein, cell, or organism. So, information is stratified between layers of influence: 
only the value of a nucleotide’s macrostate is informative relative to a referential amino acid.  
 
On the other hand, while binary operations in an electronic device are nominally performed in a 
base-two layer of influence, a single bit of information registered by the presence or absence of 
electrons in a capacitor also contains the information within these electrons, and even in the 
atoms and subatomic particles of the capacitor itself. The absolute quantity of information 
processed by the entire device over a period of time would similarly include all of the 
information in any of its structural metals and plastics, in the photons that exit the screen, in the 
information lost as heat to the surroundings, and in the photons, air molecules, and external 
systems that interact with the device. The absolute information processed by the classical 
computing device clearly exceeds the quantity of information used in performing electronic 
computations. This stratification is arguably what allows the brain to contain a far greater 
quantity of relative information than an electronic device with equivalent physical dimensions.  
 
Some other properties of information should be noted in the context of stratification. Each 
degree of freedom in a physical system is essentially an informational influence space, so that 
the realization of one state or elimination of one degree of freedom (i.e., the induction of many 
microstates into a single macrostate, the occurrence of a probabilistic event over its complement, 
the transition from a prior to a posterior probability, the collapse of a wavefunction, etc.) 
registers a single unit of physical information. It is for this reason that the finite number of 
quantum states imposed on any physical system ensures its containing and processing of finite 
information (S Lloyd 2006; C Rovelli 2015), that storing many degrees of freedom in one 
computational macrostate confers greater statistical stability (S Lloyd 2000), that the occurrence 
of an unlikely physical event registers more Shannon information, and that a relative quantity of 
information is measurable as entropy (both as H = log2 N and S = k loge W). In any case, the 
information in any layer of influence can be quantified even if the content of its internal 
evaluation is unattainable by an external system.   
 
 
 
 
 
 
 
 
 
.
CC-BY 4.0 International license
a
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under 
The copyright holder for this preprint (which was not
this version posted July 6, 2016. 
; 
https://doi.org/10.1101/060467
doi: 
bioRxiv preprint 

III. Information-energy equivalence  
 
“It is important to realize that in physics today, we have no knowledge what energy is.” – (RP 
Feynman 1964) 
 
Because a state of maximal internal energy corresponds to a state of maximal information 
saturation (TL Duncan et al. 2004), any transitions between energy levels occur as transitions 
between informational extrema. These transitions enclose a finite quantity of information, so the 
flow of information through a referential system is followed by an energy change:     
 
�����= �����
�� 
 
where k is Boltzmann’s constant, E is energy in joules, and T is ambient temperature in kelvin  
 
Reintroducing the universal information-entropy bound as  
 
Iuniv  = ζu 
√� ��
ħ� 
 
where A is the area of a thermodynamic universe in meters squared, E is energy in joules and ζu = 1  
 
now allows us to partition the universal information saturation factor into a relative information 
component (ζi  = 1 – ζs ) and an entropic information component (ζs = 1 – ζi ),  
 
Isys = ζi  
√� ��
ħ� = (1 – ζs )  
√� ��
ħ�  
 
so that, in the case that information enters a macroscopic physical system with an effectively 
constant mass and surface area, we can write   
 
ΔIsys = Δζi = ΔEsys 
 
and, in the case that information exits a system as heat,  
 
ΔSsys = Δζs = ΔEsurr 
 
Various forms of similar relationships have been considered in neuroscience (P Sterling and S 
Laughlin 2015). This article simply proposes their restatement as fundamental physical 
relationships in the brain requiring further investigation and critique. On a side note, these 
relationships also seem to be consistent with the possibility that the quantum of gravity is 
(roughly) the energy of one bit of entropy (A Alfonso-Faus 2011), the possibility that the Planck 
constant is the minimal unit of information (C Rovelli 2015), and the possibility that the 
asymmetry of time is an entropy gradient (L Mlodinow and TA Brun 2014).  
 
.
CC-BY 4.0 International license
a
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under 
The copyright holder for this preprint (which was not
this version posted July 6, 2016. 
; 
https://doi.org/10.1101/060467
doi: 
bioRxiv preprint 

IV. An optimization principle  
 
"To heat also are due the vast movements which take place on the earth. It causes the agitations 
of the atmosphere, the ascension of clouds, the fall of rain and of meteors, the currents of water 
which channel the globe… It is necessary to establish principles applicable not only to steam 
engines but to all imaginable heat engines, whatever the working substance and whatever the 
method by which it is operated." — (S Carnot 1824)  
 
The observation that biological systems use a steady influx of energy to maintain order and 
structural integrity seems to show that, since entropy can be considered a referent-dependent 
measure of inaccessible information, the continuous influence of energy through the information 
bound of a biological system necessarily decreases entropy and increases internal energy. Since 
the maximization of internal energy and the maximization of relative information occur as 
coupled, mutually-reinforcing processes – and both processes offset the transition into 
equilibrium – living systems can be seen as thermodynamic demons that locally maximize 
information and internal energy. It has indeed been suggested that there should be no distinction 
made between formalized information and physical information in biology (C Adami 2004), that 
defining biological systems in terms of their management of information may condense a great 
deal of experimental evidence into a small number of theoretical models (S Brenner 2012, PM 
Binder et al. 2011), and that the development of a unified theory of biology may require the 
application of information thermodynamics (DR Brooks et al. 1989, BH Weber et al. 1989, L 
Demetrius 2000, LM Martyushev et al. 2006, ED Schneider et al. 2006, JL England 2013, DF 
Styer 2008). It appears that, as a general trend, living systems conform to a principle of 
information-energy maximization, or optimization. Interestingly, because this maximization 
would lead to the minimization of informational free energy, this principle is describing one side 
of the same relationship represented by the variational free energy principle (KJ Friston 2010, B 
Sengupta et al. 2013). In other words, this is simply an alternative way of considering the free 
energy principle as an informational principle of least action.  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
.
CC-BY 4.0 International license
a
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under 
The copyright holder for this preprint (which was not
this version posted July 6, 2016. 
; 
https://doi.org/10.1101/060467
doi: 
bioRxiv preprint 

V. Neurobiology   
 
“… classical thermodynamics… is the only physical theory of a universal content concerning 
which I am convinced that, within the framework of applicability of its basic concepts, will never 
be overthrown.” – (A Einstein 1949, from JD Bekenstein 2001) 
 
Neurons 
Neuron activity is, to some extent, reducible to chemical and electrical activity, and such activity 
is necessarily accompanied by an exchange of energy that follows the transfer of information 
between systems. The occurrence of depolarization as a transition to Emax and the maintenance of 
a resting state as Emin may reflect coupled information-energy dynamics, and the maintenance of 
a negative resting membrane potential may show that depolarization requires the input of 
external information-energy. In addition, while the graded potential and analog influence is 
clearly important, a quintessential feature of neurons is their propagation of all-or-nothing action 
potentials. This feature seems to mirror the discrete, granular all-or-nothing mathematical 
structure of physical information. Although information is processed in the activity of the ions 
and neurotransmitters that generate an EPSP or IPSP, even at subthreshold levels, any transition 
into one state from two or more alternatives or degrees of freedom could be interpreted as 
carrying a single unit of information regardless of underlying dynamics. Further, that spatial and 
temporal summation of numerous dendritic spikes results in the propagation of a single axonal 
action potential – one macrostate – even suggests a compression algorithm. This operation may 
represent an analog of feature binding at the level of the neuron, particularly suitable for the 
layer III and IV pyramidal and stellate cells whose projections connect higher cortical areas. 
Additionally, the observation that the information in a sequence of action potentials is carried 
only by spike count and temporal spacing (E Kandel et. al 2000) may show that a frequency 
distribution code is partly an error-minimizing redundancy feature of neuronal information 
processing mechanisms. That signal strength increases linearly with frequency and is solely 
determined by frequency also seems to suggest that spikes represent discrete quantities of 
physical information. The possibility that bursting activity and repeated spiking responses may 
have evolved partly to allow error correction by repetition – perhaps the simplest and most 
rudimentary error correction algorithm, present everywhere from the organization of DNA (V 
Vedral 2010) to the structure of speech (J MacCormick 2012) – is supported by the observation 
that signal-to-noise ratios tend to increase with signal repetition (W Bialek and F Rieke 1992).  
 
Networks 
The general features of the various network architectures in the brain also appear to show 
information-energy relationships. Networks naturally stabilize by seeking energy minima, and 
the various attractor states in one of these networks define the geometry of its energy landscape 
(JJ Hopfield 1982). As a result, transitions into spontaneous states follow transitions into 
information-energy maxima, and such transitions have been associated with the generation of an 
informational entity such as a memory or a decision (G Deco et al. 2009). In this way, the local 
energy basins of attractor networks may be closely analogous to the resting potentials of neurons 
and receptors; a transition between any two distinctive energy states follows the passage of a 
.
CC-BY 4.0 International license
a
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under 
The copyright holder for this preprint (which was not
this version posted July 6, 2016. 
; 
https://doi.org/10.1101/060467
doi: 
bioRxiv preprint 

finite quantity of information. Even in the case of a massively integrated network managing a 
great deal of complex information, the observation that any energy landscape contains a finite 
number of basins at any moment may show that each energy transition in such a network carries 
a finite quantity of information. This architecture may even parallel the action potential as an 
mechanism for promoting stability (in the sense of minimizing prediction errors) simply by 
using many degrees of neuronal freedom to effect the transition into one macrostate from a large 
number of microstates. In other words, where the degrees of freedom governing the dynamics of 
a neuron rest on the degrees of freedom of its structural molecules, neurotransmitters, and ions, 
the degrees of freedom of a network rest (emergently) on the degrees of freedom in underlying 
neurons and their possible states and configurations. Of course, a network in the brain is still 
prone to chaos and nonlinear behavior, and the function of a single neuron could result in an 
altered network macrostate. For the very reason that this is true, a vastly connected network 
whose behavior is governed by the activity of large assemblies of integrated neurons is 
minimally likely to be altered by the macrostate of an individual subsystem (e.g., a single 
potassium ion) at a lower level of stratification. That a network’s number of degrees of freedom 
is limited by connection density is also suggested by the observation that vocabulary size and 
memory capacity appear to correspond to a network’s number of connections (ET Rolls 2012). If 
entities such as words and memories are considered to be units of information, there is clearly a 
correspondence between a network’s number of connections and its number of effective degrees 
of freedom as possible units of information. Network competition may also restate the 
importance of increasing outcome stability by maximizing conditional network responses, since 
inhibiting more weakly activated surrounding networks would reduce the overall number of 
degrees of freedom of a network at a higher level of architecture, and for the brain as a whole.   
 
Inhibition  
It is possible that inhibition only nominally reduces information processing in the brain. Since 
maintaining proper membrane potentials, energy gradients, neurotransmitter and signaling 
protein concentrations requires the expenditure of internal energy, it is advantageous for the 
brain to process only a minimal quantity of environmental information necessary for survival. 
For this reason, it is possible that inhibition in the brain still results in optimization, both by the 
brain itself and the organism as a whole. This possibility even seems to align with some very 
general observations at the level of behavior and cognition. For instance, the ability of a verbal 
command to rapidly inhibit the flow of information through non-salient networks (F Grabenhorst 
and ET Rolls 2008) may show that the affective nature of language parallels inhibitory 
neuromodulation: minimizing the expenditure of internal energy at any level of structure seems 
to maximize the internal information content of each influence node. In the case of social 
networks, integration enhances the efficiency of inhibition: the peripheral mediation of another’s 
inner state is clearly advantageous for a function such as calling attention to a stimulus in order 
to coordinate a group response, and the rapid evaluation of another’s inner state is inversely 
beneficial for the same reasons. The ability of disintegrated information flow to prevent the 
emergence of an optimized influence network may be further revealed by the observation that 
the maintenance of a stable internal state requires constant inhibitory modulation (K Lehmann et 
al. 2012), as well as the observation that large-scale network hyperexcitation can lead to the 
.
CC-BY 4.0 International license
a
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under 
The copyright holder for this preprint (which was not
this version posted July 6, 2016. 
; 
https://doi.org/10.1101/060467
doi: 
bioRxiv preprint 

rapid loss of consciousness. The “attentional spotlight” of awareness noted by Global 
Workspace Theory (BJ Baars 2005) may even describe a form of inhibition mediated largely by 
frontal and parietal network competition. In the case of volitional attention, the willful selection 
of a salient perceptual stream (i.e., through a chosen communication channel) requires the 
activation of large assemblies of inhibitory interneurons.  
 
Learning  
Lowering the firing threshold in order to increase the likelihood of individual cellular response 
to a stimulus (DO Hebb 1952) may optimize the activation of an engram by minimizing the 
quantity of energy needed to elicit maximal influence from its connections. Interestingly, the 
influence of a minimal energetic stimulus in an optimally integrated recurrent network has even 
been reported to enable a full energetic activation and an effectively perfect recall of a memory 
(ET Rolls 2012). For this reason, it is possible that a learned engram is not the storage of 
information as a dormant memory or response per se but rather a mechanism allowing the 
temporal convergence of otherwise separate units of information upon the engram’s future 
activation. It is also possible that neurons tend to form composite systems only if the internal 
energy and information management capacity of the composite system is likely to exceed that of 
the individual neurons in their prior state of separation. This possibility seems to be supported by 
the general molecular and cellular energetics of learning and plasticity. The minimization of free 
energy achieved through learning (KJ Friston 2010) would then maximize internal information, 
and the continual formation and disassembly of engrams during learning and forgetting would 
optimize the growth and pruning of a network in response to external states.  
 
Noise 
Any physical system whose state values are coupled to the exchange of energy and the transfer 
of information is inherently noisy, even if effectively non-stochastic and roughly predictable. 
Noise does appear to be an innate and unavoidable feature of information processing in the brain 
(ET Rolls 2012), scaling in proportion to influence network size (K Josic et al. 2009) and 
varying inversely with the coupling of network components (DJ Mar et al. 1999; B Kia et al. 
2015). Even in the absence of any potential forms of stochastic resonance (F Moss et al. 2004), 
the noise-driven exploration of various state spaces could enhance the ability of a system in the 
brain to reach an optimal information-energy extremum. Interestingly, noise has been found to 
enhance learning and serve beneficial functions (MD McDonnell and LM Ward 2011); the 
finding that double-stranded DNA breaking (E Suberbielle et al. 2013) and reversible 
methylation (P Tognini et al. 2015) both occur as normal, everyday processes in the living brain 
could perhaps be explained by considering that any stochastically optimized structure would be 
more likely to survive over time as a strengthened connection or increased integration in a 
modifiable network. So, noise could enhance brain function in much the same way that noise at 
the level of the genome (i.e., recombination) enhances adaptation and evolution of species (JV 
Stone 2015; S Lloyd 2006). In other words, it possible that noise in the brain can be interpreted 
as informational entropy, which necessarily decreases when any systems integrate, couple their 
influence with mutual information, and jointly reduce KL and thermodynamic entropy. The 
signal-to-noise ratio could then be seen as a measurement of relative information.  
.
CC-BY 4.0 International license
a
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under 
The copyright holder for this preprint (which was not
this version posted July 6, 2016. 
; 
https://doi.org/10.1101/060467
doi: 
bioRxiv preprint 

 
Brain  
The brain’s dependence on energy (P Sterling and S Laughlin 2015) is readily apparent in its 
disproportionate consumption of energy substrates (e.g., oxygen, glucose, and ATP), in its 
vulnerability to hypoxic-ischemic damage, and in the rapid loss of consciousness conferred by 
the onset of an energy restriction. It is also interesting to consider that the information content of 
a large-scale system in the brain can change while mass and surface area remain effectively 
constant, and that the brain consumes a greater quantity of energy expected from its mass alone 
(ME Raichle and DA Gusnard 2002). These findings may even relate to the tradeoff of these 
parameters predicted by the information bound: a change in the information content of a system 
with constant mass and surface area must produce a change in internal energy. All fMRI and 
PET interpretation rests on the assumption that changes in the information content of a structure 
can be inferred by observing internal energy changes, and it is well known that the information 
processing capacities of systems in the brain are limited by energy supply. The living brain 
appears to dynamically maintain a state of optimization, while brain death could be said to occur 
as a rapid transition into a local information-energy minimum.  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
.
CC-BY 4.0 International license
a
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under 
The copyright holder for this preprint (which was not
this version posted July 6, 2016. 
; 
https://doi.org/10.1101/060467
doi: 
bioRxiv preprint 

VI. Consciousness and free will  
 
“… science appears completely to lose from sight the large and general questions; but all the 
more splendid is the success when, groping in the thicket of special questions, we suddenly find a 
small opening that allows a hitherto undreamt of outlook on the whole.” – (L Boltzmann 1886) 
 
Unity  
At any given moment, awareness is experienced as a unified whole. It is possible that physical 
information is the substrate of consciousness (A Annila 2016) and it appears that any unit of 
information – the bit, trit, nat, qubit, and so forth – is necessarily transferred into a referential 
system as a discrete and temporally unitary quantity. In other words, it is possible that the 
passage of time itself occurs secondarily to the transfer of information, and that the information 
present in any system at any time is always cohesive and temporally unified. From this 
perspective, the dilation or contraction of time would vary as a rate of information influence and 
entropy generation – the flow of absolute information into a relative information space would 
increase the internal energy of a referential demon by kT J for every state observed, while the 
inverse would occur with the flow of relative information into an absolute information space 
(i.e., the generation of entropy). The possible requirement for any information to be unified at its 
time of influence into a referential system of the brain may even originate in fundamental 
conservation laws; any quantity of information is fully conserved during its course of influence, 
and the influence of unified quantities of information necessarily occurs alongside the influence 
of conserved quantities of energy. Although it is possible that all physical systems exchange 
information-energy in temporally unitary quantities, it is likely that many of the familiar features 
of unity in phenomenological human consciousness require the structure and function of neural 
networks in the brain. It appears that this unity, as an emergent informational structure (F 
Dretske 1981) requires an integration of thalamocortical and localized neocortical influence (S 
Dehaene et al. 2011; C Koch and S Greenfield 2007; S Greenfield and TFT Collins 2005). It is 
also possible that an emergent informational structure in the claustrum generates some of this 
experiential unity (FC Crick and C Koch 2005, MZ Koubeissi et al. 2014). However, it has been 
reported that complete unilateral resection of the claustrum performed in patients with neoplastic 
lesions of the region appears not produce any significant change in subjective awareness or 
observable functionality (Duffau et al. 2007). It appears unlikely that the flow of information 
into any isolated or compartmentalized network of the brain is responsible for producing all 
aspects of the unified nature of conscious experience.  
 
Complexity  
The subjective complexity of conscious experience may reveal that extensive influence network 
integration is needed for maximizing mutual information and internal energy (JS Torday et al. 
2016). An exemplary structure enabling such integration, likely one of many that account for the 
sensory complexity of consciousness, is the cluster of projections that form the internal capsule. 
A structure such as this may show that, at any given moment in the influence space of a living 
brain, a wide range of integrated referential networks are sharing mutual sources of sensory 
information. This pattern of structure may reveal that the perceptual depth and complexity of 
.
CC-BY 4.0 International license
a
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under 
The copyright holder for this preprint (which was not
this version posted July 6, 2016. 
; 
https://doi.org/10.1101/060467
doi: 
bioRxiv preprint 

consciousness is an inevitable emergent effect of influence rather than a deliberate construction 
of the brain per se. It also seems that extensive localized regional cortical consolidation of 
sensory information is necessary for producing a refined and coherent complex sensorium within 
a referential network, which could then pass into other higher-order networks through 
commissural and association fibers. In addition, the dynamics of two-state attractor networks at 
these levels of network structure may also show that quantities of complex sensory-perceptual 
information can be observed as changes in cortical energy landscapes, with a single transition 
between states following the transfer of a finite quantity of information between any two 
systems. The complexity of the information encapsulated by such a transition would reflect the 
degree of consolidation of information-energy from underlying influence networks. In short, it is 
possible that the sensory and perceptual complexity of consciousness is, to at least some extent, 
apparent in the properties of observable structures that are fairly well understood. The structural 
analogs of the complexity of conscious experience may be observable as integration between 
large-scale influence networks that receive and share surroundings-sourced information.  
 
Self-awareness  
It is possible that the informational structure of self-awareness arose as a necessity for survival 
rather than as an accident of evolution, and rudimentary forms of self-awareness likely began to 
appear early in the course of brain evolution as various forms of sensory self-environment 
separation. The extent of this perceived separation would likely vary as a function of Markov 
boundary density and internal information content (enclosed by the information bound). An 
example of this division is the tickle response, which clearly (DJ Linden 2007) requires the 
ability to partition self-produced (i.e., non-threatening) tactile sensory states from those 
produced by external influence (i.e., potentially threatening). The early need for the ability to 
cancel self-produced tickles may be further elucidated by the observation that the cancellation 
process is largely mediated by the cerebellum (SJ Blakemore et al. 1998). The perception of a 
well-defined self is clearly advantageous (PS Churchland 2002, MS Graziano 2013), and 
conditions involving neglect do show that a reduced awareness of self-ownership of motor skills 
or perceptions can result in significant impairment even if potential sensory or motor function 
remains intact (A Parton et al. 2004). While it is possible that the perceptual self arose from 
early subcortical origins, the complex syntactical and conceptual self structure present in the 
modern brain likely requires the neocortical influence networks that began integrating with (i.e., 
essentially grew over) pre-existing rudimentary subcortical self networks. The innate capacity 
for developing sophisticated self-modeling informational structures may have evolved, along 
with language, as a requirement for coordinating group behavior as social dynamics began to 
grow increasingly less predictable. Further, in the same way that the specific state values of 
informational language components (e.g., graphemes, phonemes, words, constructions, etc.) are 
the products of external informational structures (i.e., a learning environment) it is likely that 
states of self-awareness are learned. This possibility is supported by the finding that self-touch 
seems to enhance body ownership in stroke patients with acquired body awareness disorders 
(HE Van Stralen 2011) and that mirror therapy in such groups can often alleviate self-awareness 
deficits (A Fotopoulou et al. 2009; AS Rothgangel et al. 2011). It is also notable that self 
structures and language structures in the brain both traverse a wide range of cortical and 
subcortical regions (G Northoff 2006, W Penfield and L Roberts 2014), and that self-referential 
.
CC-BY 4.0 International license
a
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under 
The copyright holder for this preprint (which was not
this version posted July 6, 2016. 
; 
https://doi.org/10.1101/060467
doi: 
bioRxiv preprint 

thought informs the content of inner speech (A Morin and J Michaud 2007). The continual 
experience of a first-person syntactical self thus enables the rapid, thorough evaluation and 
influence of another’s inner state, which could be said to optimize social networks.  
 
Continuity  
While perceptual time likely results from a group of related processes rather than a single, global 
function localized to any specific influence network in the brain, it is possible that some of the 
perceptual continuity of conscious experience results from the effectively continuous flow of 
discrete quantities of physical information into and out of systems of the brain. In such a 
framework, the “quantum” (M Prokopenko et al. 2014) of perceptual time would be the minimal 
flux of information, and an effectively continuous influence of relative information would 
appear as as an effectively continuous influence of energy into an internal space. This possibility 
seeems to be supported by the observation that the transition of a large-scale attractor network is 
regressively more continuous than the activation of a small-scale engram, the propagation of an 
action potential, the release of a vesicle, or the passage of an ion through a membrane. Likewise, 
electroencephalography shows that the ongoing summation of a large number of neuronal 
potentials can converge into an effectively continuous wave as a network field potential (SJM 
Smith 2005) whose disruption may cause discontinuity of experience and induction of 
unconsciousness (H Blumenfeld and J Taylor 2003, B Haider et al. 2006). In addition, higher 
frequency network oscillations are generally indicative of wakeful states and active awareness 
(M Teplan 2002), while lower frequency oscillations tend to be associated with states of lesser 
passage of perceptual time, such as sleep and unconsciousness. The possibility that the 
perceptual arrow of time and the external arrow of time share a common origin in the flow of 
physical information is supported by general models of time in cognitive neuroscience and the 
interpretation of time in physics as a statistically asymmetrical increase in relative entropy (L 
Maccone 2009; L Mlodinow and TA Brun 2014; OC Stoica 2008; T Asselmeyer-Maluga 2015).  
 
Free will  
Evidence suggests that the brain is often predictable within reason, and the performance of an 
action can be predicted before a decision is reported to have been made (B Libet et al. 1983). 
Evidence suggests that entities such as ideas, feelings, and beliefs exist as largely deterministic, 
even if stochastically mutable, evaluations of information processed in the brain. Whether this 
flow is subject to its volitional alteration or retroactively assigned as an “illusory" feeling or 
belief, evidence also shows that information in the brain is subjectively real to its influence 
space, even if this information is inconsistent with an external reality or the experienced reality 
of an external system. That an influence space of the brain can contain a subjectively true and 
independently real but objectively inaccurate and externally inconsistent reality is exemplified 
by phenomena such as confabulation, neglect, commissurotomy effects, placebo and nocebo 
effects, hallucinations, prediction errors, the suspension of disbelief during dreaming, the 
requirement for channels in the form of synapses and white matter tracts, the function of social 
communication in minimizing divergence between individual realities, the quality of many kinds 
of realistic drug-induced experiences, and the observable effects of many cognitive conditions. 
The fact that subjective reality is an active construction of the physical brain has even led to the 
.
CC-BY 4.0 International license
a
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under 
The copyright holder for this preprint (which was not
this version posted July 6, 2016. 
; 
https://doi.org/10.1101/060467
doi: 
bioRxiv preprint 

proposal of model-dependent realism (SW Hawking and L Mlodinow 2011) as a theoretical 
approach in developing a unified physical “theory of everything” (ignoring unsolved problems 
in other fields). Even the scientific method itself seems to hint some elements of information 
processing (ET Jaynes 2003), such as error correction by experimental replication, updating of 
priors with the falsification of predictions, and the information-energy maximization that could 
be said to result in the integrated social networks that benefit from its presence and application. 
In any case, it is likely that beliefs, including those in free will, exist as physical information, 
and that their internal reality is a restatement of its inherently observer-dependent nature. It is 
likely that a scientific debate over the existence of objective, external free will independent of 
reference is analogous to a debate over the existence of a similar informational entity such as a 
perception, experience, or quale. It is likely that the internal reality of informational entities in 
the brain is further revealed by their existence as physical information, their inevitably stochastic 
nature, their effectively deterministic evaluation, their requirement for physical media, and their 
ability to determine the states of physical systems. It is likely that a simple inquiry currently 
offers the most accurate, valid, and reliable test for the presence of free will. 
 
Conclusion  
This article has presented information-energy relationships in the hope that they may eventually 
provide a general framework for uniting theory and experiment in neuroscience. This analysis has 
been very superficial and preliminary, and largely limited to speculation and suggestion. 
Nevertheless, it is possible that these or similar relationships are present, to at least some extent, 
in all brain structure and function. Refinement or falsification is needed. 
 
Acknowledgements  
I am grateful for the constructive discussions, friendly criticism, and generous insight and 
assistance provided by Baroness Susan Greenfield, Dr. Francesco Fermani, Dr. Karl Friston,    
Dr. Biswa Sengupta, Dr. Chris Adami, Dr. Roy Frieden, Dr. Bernard Baars, Dr. Brett Clementz, 
Dr. Cristi Stoica, Dr. Satoru Suzuki, Dr. Paul King, Guillem Collell Talleda, and Dr. Jordi 
Fauquet. I am also grateful to Dr. Shanta Dhar and her team for introducing me to the research 
process.  
 
References  
Adami, C. (2004). Information theory in molecular biology. Physics of Life Reviews, 1(1), 3-22. 
Adami, C. (2011). Toward a fully relativistic theory of quantum information. arXiv:1112.1941. 
Alfonso-Faus, A. (2011). Quantum gravity and information theories linked by the physical 
properties of the bit. arXiv preprint arXiv:1105.3143 
Alfonso-Faus, A. (2013). Fundamental Principle of Information-to-Energy Conversion. 
arXiv:1401.6052. 
Asselmeyer-Maluga, T. (2015). Spacetime Weave—Bit as the Connection Between Its or the 
Informational Content of Spacetime. It From Bit or Bit From It? Springer.  
Attwell, D., & Laughlin, S. B. (2001). An energy budget for signaling in the grey matter of the 
brain. Journal of Cerebral Blood Flow & Metabolism, 21(10), 1133-1145. 
.
CC-BY 4.0 International license
a
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under 
The copyright holder for this preprint (which was not
this version posted July 6, 2016. 
; 
https://doi.org/10.1101/060467
doi: 
bioRxiv preprint 

Baars, B. J. (2005). Global workspace theory of consciousness: toward a cognitive neuroscience 
of human experience. Progress in Brain Research, 150, 45-53. 
Bérut, A., et al. (2012). Experimental verification of Landauer's principle linking information and 
thermodynamics. Nature, 483(7388), 187-189. 
Bekenstein, J. D. (1981). Universal upper bound on the entropy-to-energy ratio for bounded 
systems. Physical Review D, 23(2), 287. 
Bekenstein, J. D. (1984). Entropy content and information flow in systems with limited energy. 
Physical Review D, 30(8), 1669. 
Bekenstein, J. D. (2001). The limits of information. Studies In History and Philosophy of Science 
Part B: Studies In History and Philosophy of Modern Physics, 32(4), 511-524. 
Bekenstein, J. D. (2004). Black holes and information theory. Contemporary Physics, 45(1). 
Bennett, C. H. (2003). Notes on Landauer's principle, reversible computation, and Maxwell's 
Demon. Studies In History and Philosophy of Science Part B: Studies In History and Philosophy 
of Modern Physics, 34(3), 501-510. 
Bennett, C. H., & Landauer, R. (1985). The fundamental physical limits of computation. 
Scientific American, 253(1), 48-56. 
Bialek, W., & Rieke, F. (1992). Reliability and information transmission in spiking neurons. 
Trends in Neurosciences, 15(11), 428-434.     
Binder, P. M., & Danchin, A. (2011). Life's demons: information and order in biology. EMBO 
Reports, 12(6), 495-499. 
Blakemore, S. J., et al. (1998). Central cancellation of self-produced tickle sensation. Nature 
Neuroscience, 1(7), 635-640. 
Blumenfeld, H., & Taylor, J. (2003). Why do seizures cause loss of consciousness? The 
Neuroscientist, 9(5), 301-310. 
Boltzmann, L. (2012). Theoretical Physics and Philosophical Problems: Selected Writings 
(Volume 5). Springer. 24.  
Bousso, R. (2002). The holographic principle. Reviews of Modern Physics, 74(3), 825. 
Bradler K. & Adami C. (2014). The capacity of black holes to transmit quantum information 
Journal of High Energy Physics. 5(95). 1405.  
Bremermann, H. J. (1982). Minimum energy requirements of information transfer and computing. 
International Journal of Theoretical Physics, 21(3), 203-217. 
Brenner, S. (2012). Turing centenary: Life's code script. Nature, 482(7386), 461-461. 
Brooks, D. R., et al. (1989). Entropy and information in evolving biological systems. Biology and 
Philosophy, 4(4), 407-432. 
Brukner, Č., & Zeilinger, A. (2003). Information and fundamental elements of the structure of 
quantum theory. Springer Berlin Heidelberg. 323-354. 
Churchland, P. S. (2002). Brain-Wise: Studies in Neurophilosophy. MIT press.   
Collell, G., & Fauquet, J. (2015). Brain activity and cognition: a connection from 
thermodynamics and information theory. Frontiers in Psychology, 6. 
Collier, J. (1986). Entropy in evolution. Biology and Philosophy, 1(1), 5-24. 
.
CC-BY 4.0 International license
a
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under 
The copyright holder for this preprint (which was not
this version posted July 6, 2016. 
; 
https://doi.org/10.1101/060467
doi: 
bioRxiv preprint 

Crick, F., & Koch, C. (2002). The problem of consciousness. Scientific American, 267(3).  
Crick, F. C., & Koch, C. (2005). What is the function of the claustrum? Philosophical 
Transactions of the Royal Society B: Biological Sciences, 360(1458), 1271-1279. 
Deco, G., et al. (2009). Stochastic dynamics as a principle of brain function. Progress in 
Neurobiology, 88(1), 1-16. 
Demetrius, L. (2000). Thermodynamics and evolution. Journal of Theoretical Biology, 206(1), 1. 
Dretske, F. (1981). Knowledge and the Flow of Information. 
Duffau, H., et al. (2007). Functional compensation of the claustrum: lessons from low-grade 
glioma surgery. Journal of Neuro-oncology, 81(3), 327-329. 
Duncan, T. L., & Semura, J. S. (2004). The deep physics behind the second law: information and 
energy as independent forms of bookkeeping. Entropy, 6(1), 21-29. 
England, J. L. (2013). Statistical physics of self-replication. The Journal of Chemical Physics, 
139(12), 121923. 
Feynman, R. P., et al. (1965). The Feynman Lectures on Physics, Volume 1. American Journal of 
Physics, 33(9), 750-752. 
Fotopoulou, A., et al. (2009). Self-observation reinstates motor awareness in anosognosia for 
hemiplegia. Neuropsychologia, 47(5), 1256-1260. 
Fox, D. (2011). The limits of intelligence. Scientific American, 305(1), 36-43. 
Frieden, B. R. (2015). Estimating a Repeatable Statistical Law by Requiring Its Stability During 
Observation. Entropy, 17(11), 7453-7467. 
Friston, K. (2010). The free-energy principle: a unified brain theory? Nature Reviews 
Neuroscience, 11(2), 127-138. 
Friston, K. (2013). Life as we know it. Journal of The Royal Society Interface, 10(86), 20130475. 
Greenfield, S. (2002). Mind, brain and consciousness. The British Journal of Psychiatry, 181(2).  
Greenfield, S. A., & Collins, T. F. (2005). A neuroscientific approach to consciousness. Progress 
in Brain Research, 150, 11-587. 
Haider, B., et al. (2006). Neocortical network activity in vivo is generated through a dynamic 
balance of excitation and inhibition. The Journal of Neuroscience, 26(17), 4535-4545. 
Hawking, S. W. (2005). Information loss in black holes. Physical Review D, 72(8), 084013. 
Hawking, S. W. and Mlodinow, L. (2011). The Grand Design. Random House LLC.  
Hebb, D. O. (1952). The Organization of Behavior: A Neuropsychological Theory. Wiley.  
Hobson, J. A., & Friston, K. J. (2012). Waking and dreaming consciousness: neurobiological and 
functional considerations. Progress in Neurobiology, 98(1), 82-98. 
Hopfield, J. J. (1982). Neural networks and physical systems with emergent collective 
computational abilities. Proceedings of the National Academy of Sciences, 79(8), 2554-2558. 
Jaynes, E. T. (2003). Probability Theory: the Logic of Science. Cambridge University Press. 
Josic, K., et al. (2009). Coherent Behavior in Neuronal Networks: 3 (Springer Series in 
Computational Neuroscience). 2009 Edition. Springer. 222-225. 
.
CC-BY 4.0 International license
a
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under 
The copyright holder for this preprint (which was not
this version posted July 6, 2016. 
; 
https://doi.org/10.1101/060467
doi: 
bioRxiv preprint 

Jun, Y., Gavrilov, M., & Bechhoefer, J. (2014). High-precision test of Landauer’s principle in a 
feedback trap. Physical Review Letters, 113(19), 190601. 
Kandel, E. R., et al. (2000). Principles of Neural Science.  New York: McGraw-Hill.  
Kia, B., et al. (2015). Nonlinear dynamics based digital logic and circuits. Frontiers in 
Computational Neuroscience, 9. 
Koubeissi, M. Z., (2014). Electrical stimulation of a small brain area reversibly disrupts 
consciousness. Epilepsy & Behavior, 37, 32-35. 
Knuth, K. H. (2015). Information-based physics and the influence network. It From Bit or Bit 
From It? Springer. 65-78.  
Landauer, R. (1961). Irreversibility and heat generation in the computing process. IBM Journal of 
Research and Development, 5(3), 183-191. 
Landauer, R. (1996). The physical nature of information. Physics Letters A, 217(4), 188-193. 
Laughlin, S. B. (2001). Energy as a constraint on the coding and processing of sensory 
information. Current Opinion in Neurobiology, 11(4), 475-480. 
Lavis, D. A., & Streater, R. F. (2002). Physics from Fisher information. Studies in History and 
Philosophy of Science Part B: Studies in History and Philosophy of Modern Physics, 33(2). 
Lehmann, K., Steinecke, A., & Bolz, J. (2012). GABA through the ages: regulation of cortical 
function and plasticity by inhibitory interneurons. Neural Plasticity, 2012. 
Lee, J.W., et al. (2013). Gravity from quantum information. Journal of the Korean Physical 
Society, 63(5), 1094-1098 
Leifer, M. S. (2015). “It from bit” and the quantum probability rule. It From Bit or Bit From It? 
Springer.   
Libet, B., et al. (1983). Time of conscious intention to act in relation to onset of cerebral activity 
(readiness-potential). Brain, 106(3), 623-642. 
Linden, D. J. (2007). The Accidental Mind: How Brain Evolution Has Given Us Love, Memory, 
Dreams, and God.  
Liu, K. C., Oztaskin, M., & Burchiel, K. J. (2012). Basics of neurosurgical techniques and 
procedures. Essentials of Neurosurgical Anesthesia & Critical Care. Springer New York. 145. 
Lloyd, S. (2000). Ultimate physical limits to computation. Nature, 406(6799), 1047-1054. 
 
Lloyd, S. (2006). Programming the Universe.  
Maccone, L. (2009). Quantum solution to the arrow-of-time dilemma. Physical Review Letters, 
103(8), 080401. 
MacCormick, J. (2012). Nine Algorithms that Changed the Future. Princeton University Press.  
Mar, D. J., et al. (1999). Noise shaping in populations of coupled model neurons. Proceedings of 
the National Academy of Sciences, 96(18), 10450-10455. 
Martyushev, L. M. (2013). Entropy and entropy production: Old misconceptions and new 
breakthroughs. Entropy, 15(4), 1152-1170. 
Martyushev, L. M., & Seleznev, V. D. (2006). Maximum entropy production principle in physics, 
chemistry and biology. Physics Reports, 426(1), 1-45. 
.
CC-BY 4.0 International license
a
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under 
The copyright holder for this preprint (which was not
this version posted July 6, 2016. 
; 
https://doi.org/10.1101/060467
doi: 
bioRxiv preprint 

Maruyama, K., et al. (2009). Colloquium: The physics of Maxwell’s demon and information. 
Reviews of Modern Physics, 81(1), 1. 
McDonnell, M. D., & Ward, L. M. (2011). The benefits of noise in neural systems: bridging 
theory and experiment. Nature Reviews Neuroscience, 12(7), 415-426. 
Morin, A., & Michaud, J. (2007). Self-awareness and the left inferior frontal gyrus: inner speech 
use during self-related processing. Brain Research Bulletin, 74(6), 387-396. 
Mlodinow, L., & Brun, T. A. (2014). Relation between the psychological and thermodynamic 
arrows of time. Physical Review E, 89(5), 052102. 
Northoff, G., et al. (2006). Self-referential processing in our brain—a meta-analysis of imaging 
studies on the self. Neuroimage, 31(1), 440-457. 
Orlov, A. O., et al. (2012). Experimental test of Landauer's Principle at the sub-kBT level. 
Japanese Journal of Applied Physics, 51(6S), 06FE10. 
Penfield, W., & Roberts, L. (2014). Speech and Brain Mechanisms. Princeton University Press. 
Piechocinska, B. (2000). Information erasure. Physical Review A, 61(6), 062314. 
Raichle, M. E., & Gusnard, D. A. (2002). Appraising the brain's energy budget. Proceedings of 
the National Academy of Sciences, 99(16), 10237-10239. 
Rolls, E. T. (2012). Neuroculture. Oxford University Press.  
Rolls, E. T., et al. (2008). Selective attention to affective value alters how the brain processes 
olfactory stimuli. Journal of Cognitive Neuroscience, 20(10), 1815-1826. 
Rothgangel, A. S., et al. (2011). The clinical aspects of mirror therapy in rehabilitation: a 
systematic review of the literature. International Journal of Rehabilitation Research, 34(1), 1-13. 
Rovelli, C. (2015). Relative information at the foundation of physics. It From Bit or Bit From It?  
Schneider, E. D., & Kay, J. J. (1994). Life as a manifestation of the second law of 
thermodynamics. Mathematical and Computer Modelling, 19(6), 25-48. 
Schrödinger, E. (1992). What is life? Cambridge University Press.  
Sengupta, B., et al. (2013). Information and efficiency in the nervous system – a synthesis. PLoS 
Computational Biology, 9(7), e1003157. 
Shannon, C. E., & Weaver, W. (2015). The mathematical theory of communication.  
Smith, S. J. M. (2005). EEG in the diagnosis, classification, and management of patients with 
epilepsy. Journal of Neurology, Neurosurgery & Psychiatry, 76(2), ii2-ii7. 
Stoica, O. C. (2008). “Flowing with a frozen river.” Foundational Questions Institute, “The 
Nature of Time” essay contest. 
Stoica, O. C. (2015). The Tao of It and Bit. It From Bit or Bit From It? Springer.  
Stone, J. V. (2015). Information Theory: A Tutorial Introduction. Sebtel Press. 188-193.  
Styer, D. F. (2008). Entropy and evolution. American Journal of Physics, 76(11), 1031-1033. 
Suberbielle, E., et al. (2013). Physiologic brain activity causes DNA double-strand breaks in 
neurons, with exacerbation by amyloid-β. Nature Neuroscience, 16(5), 613-621. 
Susskind, L. (2008). The Black Hole War. Little, Brown and Company.  
Susskind, L., & Hrabovsky, G. (2013). The Theoretical Minimum. Basic Books.  
.
CC-BY 4.0 International license
a
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under 
The copyright holder for this preprint (which was not
this version posted July 6, 2016. 
; 
https://doi.org/10.1101/060467
doi: 
bioRxiv preprint 

Susskind, L., & Lindesay, J. (2005). An Introduction to Black Holes, Information and the String 
Theory Revolution. World Scientific, Singapore, 10(5), 10. 
Teplan, M. (2002). Fundamentals of EEG measurement. Measurement science review, 2(2), 1-11. 
Tognini, P., et al. (2015). Dynamic DNA methylation in the brain: a new epigenetic mark for 
experience-dependent plasticity. Frontiers in Cellular Neuroscience, 9. 
Torday, J. S., & Miller Jr, W. B. (2016). On the evolution of the mammalian brain. Frontiers in 
Systems Neuroscience, 10. 
Toyabe, S., et al. (2010). Information heat engine: converting information to energy by feedback 
control. Nature Physics, 6(12), 988-992.  
Umpleby, S. A. (2007). Physical relationships among matter, energy and information. Systems 
Research and Behavioral Science, 24(3), 369-372. 
Van Stralen, H. E., et al. (2011). The role of self-touch in somatosensory and body representation 
disorders after stroke. Philosophical Transactions of the Royal Society B: Biological Sciences, 
366(1581), 3142-3152. 
Vedral, V. (2010). Decoding Reality. Oxford University Press.  
Verlinde, E. (2011). On the Origin of Gravity and the Laws of Newton. Journal of High Energy 
Physics, 2011(4), 1-27. 
Weber, B. H., et al. (1989). Evolution in thermodynamic perspective: an ecological approach. 
Biology and Philosophy, 4(4), 373-405. 
Wheeler, J. A. (1989). Proceedings of the 3rd International Symposium on the Foundations of 
Quantum Mechanics. Physical Society of Japan. 354-368.  
Wicken, J. S. (1987). Evolution, thermodynamics, and information: extending the Darwinian 
program. Oxford University Press. 
Zeilinger, A. (2004). Why the quantum? It’from bit? A participatory universe? Three far-reaching 
challenges from John Archibald Wheeler and their relation to experiment. In Science and 
Ultimate Reality: Quantum Theory, Cosmology, and Complexity. (J Barrow et al., Cambridge 
University Press).  
 
 
.
CC-BY 4.0 International license
a
certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under 
The copyright holder for this preprint (which was not
this version posted July 6, 2016. 
; 
https://doi.org/10.1101/060467
doi: 
bioRxiv preprint 

