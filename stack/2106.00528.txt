Transformation Models for Flexible Posteriors in Variational Bayes
Sefan H¬®ortling 1 Daniel Dold 1 Oliver D¬®urr 1 Beate Sick 2
Abstract
The main challenge in Bayesian models is to de-
termine the posterior for the model parameters.
Already, in models with only one or few param-
eters, the analytical posterior can only be deter-
mined in special settings. In Bayesian neural net-
works, variational inference is widely used to ap-
proximate difÔ¨Åcult-to-compute posteriors by vari-
ational distributions. Usually, Gaussians are used
as variational distributions (Gaussian-VI) which
limits the quality of the approximation due to their
limited Ô¨Çexibility. Transformation models on the
other hand are Ô¨Çexible enough to Ô¨Åt any distribu-
tion. Here we present transformation model-based
variational inference (TM-VI) and demonstrate
that it allows to accurately approximate complex
posteriors in models with one parameter and also
works in a mean-Ô¨Åeld fashion for multi-parameter
models like neural networks.
1. Introduction
Uncertainty quantiÔ¨Åcation is important, especially if model
predictions are used to support high stake decision-making.
Quantifying uncertainty in statistical or machine learning
models is often achieved by Bayesian approaches, where
the uncertainty of the estimated model parameters are rep-
resented by posterior distributions. Determining these pos-
terior distributions analytically is usually impossible if the
posterior takes a complex shape or if the model has many
parameters, such as a neural network (NN). Variational in-
ference (VI) is a well established method to approximate
difÔ¨Åcult-to-compute distributions through optimization (Jor-
dan et al., 1999; Wainwright & Jordan, 2008; Blei et al.,
2017). In VI the posterior is approximated by a variational
distribution by minimizing the Kullback-Leibler divergence
between the variational distribution and the posterior. Usu-
1IOS, Konstanz University of Applied Sciences, Germany
2IDP, Zurich University of Applied Sciences, Switzerland,
and EBPI, University of Zurich, Switzerland.
Correspon-
dence to: Stefan H¬®ortling <stefan.hoertling@htwg-konstanz.de>,
Oliver D¬®urr <oliver.duerr@htwg-konstanz.de>, Beate Sick
<sick@zhaw.ch>.
ally, this is done by Ô¨Årst choosing a family of parametric
distributions, usually Gaussians, and then tuning the param-
eters of the variational distribution until its distance to the
posterior is minimized. Obviously, the quality of the VI
approximation depends on the similarity of the true poste-
rior with the optimized member of the chosen distribution
family. In cases where the posterior takes a complex shape,
a simple variational distribution, such as a Gaussian, can
never yield a good approximation.
We propose to use transformation models (TMs) (Hothorn
et al., 2014) to approximate complex posteriors via VI. The
main advantage of TMs is their guarantee that any distribu-
tion shape can be achieved without predeÔ¨Åning the family
of the distribution. In this paper, we show how to combine
TMs and VI to accurately approximate Ô¨Çexible posteriors
for all parameters in variational Bayes models via a com-
putational efÔ¨Åcient optimization process. Moreover, we
benchmark our approach against exact Bayesian models,
MCMC-Simulations, and Gaussian-VI approximations.
2. Related Work
TMs were developed in the statistics community and have
the focus on modeling a potentially complex conditional
outcome distributions in regression models (Hothorn et al.,
2014). The main idea of TMs is to learn a potentially com-
plex transformation function that transforms a simple base
distribution, such as the Standard Gaussian N(0, 1), to a
potentially complex outcome distribution under which the
likelihood of the observed outcomes is maximized. The
choice of the simple base distribution is unimportant for
prediction purposes but gets crucial for inference (Hothorn
et al., 2018). Up to now, TMs were used to model uncondi-
tional distributions or conditional outcome distributions in
statistical or deep learning regression models (Kook et al.,
2020; Buri et al., 2020; Sick et al., 2021; Baumann et al.,
2020). Recently, a Ô¨Årst Bayesian version of TMs were pro-
posed (Carlan et al., 2020), which yields exact posteriors
by Hamilton Monte Carlo sampling, but is restricted to rela-
tively small models and requires experience with designing
priors.
Independently to TMs, normalizing Ô¨Çows were developed
in the deep learning community (Dinh et al., 2014) and
are based on the same idea as TMs. Normalizing Ô¨Çows
arXiv:2106.00528v1  [stat.ML]  1 Jun 2021

Transformation Models for Flexible Posteriors in Variational Bayes
learn a chain of many simple, bijective transformations and
are mainly used to model unconditional high-dimensional
distributions for generative models (Kobyzev et al., 2020).
In generative deep learning models, VI was used to ap-
proximate the posterior of the latent variables (Rezende &
Mohamed, 2015) and, indirectly, by constructing a Ô¨Çexible
mixing density, to build the variational distribution of the
weights from multivariate Gaussians (Louizos & Welling,
2017).
To the best of our knowledge, TM based or normalizing
Ô¨Çow based VI that directly approximates the posteriors of
all model parameters, such as the weights in a NN model,
were not yet developed.
When using VI for models with many parameters, such as
NNs, it is usually not possible to optimize a joint variational
distribution over all parameters that accounts for all poten-
tial dependencies between the parameters. In such cases,
mean-Ô¨Åeld VI is used where the variational distributions of
the parameters are optimized independently from each other.
For deeper NNs it has been demonstrated that Gaussian-VI
achieves the same quality in terms of the modeled outcome
distribution regardless if the posteriors were optimized in-
dependently from each other or if correlations between the
parameters were taken into account (Farquhar et al., 2020).
3. Methods
In the following, we describe our proposed TM-VI approach,
where we use transformation models in variational inference
to achieve accurate approximations to potentially complex
posteriors for parameters in Bayesian models. The code is
publicly available on github1. The main idea is to enable
the VI procedure to approximate the posterior of the model
parameters by a Ô¨Çexible variational distribution. In TMs a
complex target distribution of interest is Ô¨Åtted by learning a
bijective transformation function h that transforms between
latent variable z following a Ô¨Åxed simple distribution, e.g.
z ‚àºN(0, 1), and a variable of interest following a poten-
tially complex target distribution (see Figure 1). Our target
distribution of interest is the variational distribution that
approximates the posterior of model parameters (e.g. the
weights w of a NN).
3.1. Transformation model
The complete transformation function h(z) = w consisting
of a chain of transformations h = f3 ‚ó¶f2 ‚ó¶œÉ ‚ó¶f1 as shown
in Figure 1. To achieve a bijective overall transformation
h, it is sufÔ¨Åcient that each transformation fi is a strictly
monotone increasing transformation. A Ô¨Årst scale and shift
transformation f1(z) = a ¬∑ z + b followed by a sigmoid
function transforms the standard Normal distributed z into
1https://github.com/stefan1893/TM-VI
[0, 1]. To ensure a strictly monotonic increase of f1, we
constrain the slope a to be positive.
The core of the transformation is the Ô¨Çexible Bernstein
polynomial of degree M:
f2(z‚Ä≤) =
M
X
i=0
Bei(z‚Ä≤)
œëi
M + 1
(1)
With Bei(z‚Ä≤) being densities of Beta-functions which are
deÔ¨Åned on z‚Ä≤ ‚àà[0, 1].
It is known that the Bernstein
polynomials can uniformly approximate every function in
z ‚àà[0, 1] (BernÀásteƒ±n, 1912), see (Farouki, 2012) for a fur-
ther discussion. An additional beneÔ¨Åt of the Bernstein poly-
nomials is that a strict a monotonic increase of f2(z‚Ä≤) w.r.t.
z‚Ä≤ can be achieved by simply enforcing that the Bernstein
coefÔ¨Åcients are increasing, i.e. œë0 < œë1 < . . . < œëM. The
last transformation is again a scale and shift transformation
f3(w‚Ä≤) = Œ±¬∑w‚Ä≤ +Œ≤ for which we constrain Œ± to be positive
to ensure a monotone increasing transformation. Altogether
the complete transformation h(z) is described by M + 5
variational parameters Œª = (a, b, œë0, . . . œëM, Œ±, Œ≤).
ùëì1
ùëì2
ùëì3
ùëì2
ùúé 
¬¥
¬¥
~
Figure 1. Overview of the transformation h for modeling a poten-
tially complex variational distribution qŒª(w) of a unconstrained
weight w in a Bayesian NN (lower left). The transformation h is a
chain of transformations, h = f3 ‚ó¶f2 ‚ó¶œÉ ‚ó¶f1, that starts from a
simple distribution, here N(0, 1) (depicted in red), and ends with
the complex distribution qŒª(w) (depicted in green). The Ô¨Årst part
of the Ô¨Çow, œÉ ‚ó¶f1, transforms N(0, 1) into a distribution with
support [0, 1], the Ô¨Çexible Bernstein polynomial used in f2 allow
for the creation of a complex shaped distribution, and f3 yields the
variational distribution qŒª(w).
We apply the following manipulations to the unrestricted pa-
rameters Œª‚Ä≤ = (a‚Ä≤, b, œë‚Ä≤
0, . . . œë‚Ä≤
M, Œ±‚Ä≤, Œ≤) to ensure the above

Transformation Models for Flexible Posteriors in Variational Bayes
described constrains of the parameters that guarantee a bijec-
tive transformation: a = softplus(a‚Ä≤), Œ± = softplus(Œ±‚Ä≤),
œë0 = œë‚Ä≤
0, and œëi = œëi‚àí1 + softplus(œë‚Ä≤
i) for i = 1, . . . , M.
To facilitate the Ô¨Åtting of distributions with potentially com-
plex shapes or long tails, we use the fact that f2(0) = œë0
and f2(1) = œëM (see (Ramasinghe et al., 2021)) to ini-
tialize our weights such that the Bernstein transformation
f2 yields a distribution which assigns substantial proba-
bility mass over the support of w‚Ä≤ before optimization.
This can be achieved by deÔ¨Åning a range on w‚Ä≤ with
w‚Ä≤
min and w‚Ä≤
max and initializing œë‚Ä≤ with œë‚Ä≤
0 = w‚Ä≤
min and
œë‚Ä≤
i = softplus‚àí1((w‚Ä≤
max ‚àíw‚Ä≤
min)/M) for i = 1, . . . , M.
Thus, they have the same initial support regardless of the
degree of the Bernstein polynomial.
3.2. Transformation model based variational inference
In VI the variational parameters Œª = (a, b, œë0, . . . œëM, Œ±, Œ≤)
are tuned such that the resulting variational distribution
qŒª(w) is as close to the posterior p(w|D) as possible. This
is done by minimizing the KL divergence between the
variational distribution and the (unknown) posterior:
KL(qŒª(w) ‚à•p(w|D)) =
Z
qŒª(w) log
 qŒª(w)
p(w|D)

dw
= log(D)‚àí(Ew‚àºqŒª(log(p(D|w))) ‚àíKL(qŒª(w) ‚à•p(w)))
|
{z
}
ELBO(Œª)
(2)
Instead of minimizing equation (2) usually only the evi-
dence lower bound ELBO (see e.g. (Blundell et al., 2015))
is maximized which consists of the expected value of the
log-likelihood, Ew‚àºqŒª(log(p(D|w))), minus the KL di-
vergence between the variational distribution qŒª(w) and
the known prior p(w). In practice, the negative ELBO is
minimized by gradient descent. We approximate the ex-
pected log-likelihood by averaging over T weight samples
wt ‚àºqŒª(w). To get these weight samples we Ô¨Årst draw T
samples zt ‚àºN(0, 1) and then compute the corresponding
weight samples via wt = h(zt). We can approximate the
expected log-likelihood for the training data by:
Ew‚àºqŒª(log(p(D|w))) ‚âà1
T
X
t,i
log (p(Di|wt))
(3)
We use the same weight samples wt to approximate the
Kullback-Leibler divergence between the variational distri-
bution and the prior via:
KL(qŒª(w) ‚à•p(w)) ‚âà1
T
X
t
log
qŒª(wt)
p(wt)

(4)
where the probability density qŒª(wt) can be calculated, from
the samples zt using the change of variable function as:
qŒª(wt) = p(zt) ¬∑

‚àÇhŒª(zt)
‚àÇz

‚àí1
(5)
4. Results and discussion
We performed a couple of experiments to benchmark
our TM-VI approach versus exact Bayesian solutions and
Gaussian-VI.
4.1. Models with a single parameter
We Ô¨Årst discuss two experiments with models containing
only one parameter. This has the advantage that we can
rule out the mean Ô¨Åeld assumption as a potential reason for
observed deÔ¨Åcits of the achieved variational distribution.
Bernoulli example
We Ô¨Årst look at an unconditional Bayesian model for
a random variable y following a Bernoulli distribution
y ‚àºBer(œÄ) which we Ô¨Åt based on only two samples
(y1 = 1, y2 = 1). In this simple Bernoulli model, it is
possible to determine the Bayesian solution analytically.
Since the parameter œÄ can only take values between zero
and one we choose a Beta-distribution p(œÄ) = Beta(Œ± =
1.1, Œ≤ = 1.1) as prior which leads to the conjugated pos-
terior p(œÄ|data) = Beta(Œ± + P yi, Œ≤ + n ‚àíP yi) (see
analytical posterior in Figure 2).
0.0
0.2
0.4
0.6
0.8
1.0
0
1
2
3
p( |y)
Analytical posterior
TM-VI M=1; KL=7.30e-03
TM-VI M=10; KL=9.87e-04
TM-VI M=30; KL=8.13e-04
Gauss-VI; KL=2.22e-02
Figure 2. Comparison of the analytical posterior for the parameter
œÄ in the Bernoulli model y ‚àºBer(œÄ) with variational distribu-
tions achieved via TM-VI and Gaussian-VI. The blue lines show
the results from our TM-VI model with different degrees M of the
Bernstein polynomial shown in different line styles. In addition
the KL divergence between the variational distributions and the
analytical posterior KL(qŒª(w) ‚à•p(w|D)) is shown in the legend.
For an animated version, showing the training process for TM-VI
with M = 10 see: https://youtu.be/_RA7QirjXMM
We now use our TM-VI method to approximate the pos-
terior. To investigate how the Ô¨Çexibility of the Bernstein
polynomial impacts the quality of the achieved variational
distribution, we have used Bernstein polynomials with dif-
ferent degree M. To enforce the modeled variational dis-
tribution to be restricted on [0, 1] we pipe the result of f3
(see Figure 1) through a sigmoid transformation to get the

Transformation Models for Flexible Posteriors in Variational Bayes
variational distribution of the parameter œÄ. Figure 2 shows
the achieved variational distributions after minimizing the
negative ELBO via gradient descent as described in section
3.2. With increasing degree M of the Bernstein polynomial,
the variational distributions gets closer to the posterior (see
KL(qŒª(w) ‚à•p(w|D)) in the legend of Figure 2). Using
the TM-VI with M = 30 yields a variation distribution that
approximates the posterior very accurately. As expected,
the Gaussian based VI has not enough Ô¨Çexibility to lead to
a variational distribution that approximates the analytical
posterior nicely (see Figure 2).
Cauchy example
For the next experiment we follow an example from (Yao
et al., 2020) and we Ô¨Åt an unconditional Cauchy model
y ‚àºCauchy(Œæ, Œ≥) to six samples which we have drawn from
mixture-Cauchy distribution y ‚àºCauchy(Œæ1 = ‚àí2.5, Œ≥) +
Cauchy(Œæ2 = 2.5, Œ≥). Because of the miss-speciÔ¨Åcation
of the model, the true posterior of the parameter Œæ has a
bi-modal shape which we have determined via MCMC (see
Figure 3). We have used TM-VI and Gaussian-VI to ap-
proximate the posterior of the Cauchy parameter Œæ by a
variational distribution. Because the possible range of Œæ
is not restricted we can use the result of f3 (see Figure 1)
as variational distribution for Œæ. While TM-VI has enough
Ô¨Çexibility to capture the bi-modal shape of the posterior,
Gauss-VI fails as expected.
4
3
2
1
0
1
2
3
4
0.0
0.2
0.4
0.6
p( |D)
MCMC
TM-VI, M=1
TM-VI, M=10
TM-VI, M=30
Gauss-VI
Figure 3. Posterior distribution of the parameter Œæ in the miss-
speciÔ¨Åed Cauchy model y ‚àºCauchy(Œæ, Œ≥). Comparing the bi-
modal true posterior resulting from MCMC with variational dis-
tributions estimated via Gaussian-VI or TM-VI shows that the
Ô¨Çexibility of TM-VI is needed to capture the bi-modal shape of the
posterior.
4.2. Multi-weight neural networks
We now investigate how our TM-VI approach performs in
conditional multi-parameter models like NNs. For this ex-
periment, we sample 16 data points clustered in two regions
of a noisy sinus wave (see points in Figure 4). We then use
MCMC, TM-VI, and Gaussian-VI to determine the solu-
tion of a Bayesian NN which controls the conditional mean
¬µ(x) of the conditional outcome distribution N(¬µ(x), œÉ).
We use two different NNs: one with 1 hidden layer and
3 neurons per layer model, and one with 2 hidden layers
and 10 neurons per layer. The variational distributions for
the weights propagate to the distribution of the conditional
mean ¬µ(x). Figure 4 demonstrates, that in the smaller 1-
hidden-layer NN, mean-Ô¨Åeld TM-VI slightly outperforms
mean-Ô¨Åeld Gaussian-VI. In the larger 2-hidden-layer NN,
both mean-Ô¨Åeld VI approaches perform comparably. Both,
mean-Ô¨Åeld Gaussian-VI and TM-VI, fail to capture the un-
certainty in-between the two clusters of data points. We
attribute both observations to the used mean-Ô¨Åeld approach,
which is known to underestimate the uncertainty (Blei et al.,
2017) and probably also masks the beneÔ¨Åt of the Ô¨Çexible
TM-VI.
MCMC
1-hidden-layer network 
2-hidden-layer network 
TM-VI
Gauss-VI
Figure 4. Posterior distribution for the conditional mean ¬µ(x) of
the conditional outcome distribution N(¬µ(x), œÉ) modeled by a
Bayesian NN with 1 hidden layer (left panel) or 2 hidden layers
(right panel). The Bayesian solution was determined via MCMC
(Ô¨Årst row) or via mean-Ô¨Åeld TM-VI (second row) or via mean-Ô¨Åeld
Gaussian-VI (third row).
5. Conclusion and outlook
We have introduced TM-VI to achieve Ô¨Çexible variational
distributions that accurately approximate potential complex
parameter posteriors. In single-parameter models with a
complex posterior, we have shown that TM-VI perfectly ap-
proximates the posterior while Gaussian-VI fails. In multi-
parameter NN models, we have demonstrated that TM-VI
can be used in a mean-Ô¨Åeld fashion. For small NNs, our TM-
VI approach produces slightly superior results compared
to the Gaussian-VI, but the limitations of the mean-Ô¨Åeld
approach are clearly visible. In the future, we plan to extend
TM-VI for Bayesian inference in models with few inter-
pretable parameters by dropping the mean-Ô¨Åeld assumption
to get accurate posterior approximations.

Transformation Models for Flexible Posteriors in Variational Bayes
6. Acknowledgements
Part of the work has been founded by the Federal Ministry of
Education and Research of Germany (BMBF) in the project
DeepDoubt (grant no. 01IS19083A).
References
Baumann, P. F., Hothorn, T., and R¬®ugamer, D.
Deep
conditional transformation models.
arXiv preprint
arXiv:2010.07860, 2020.
BernÀásteƒ±n, S. D¬¥emonstration du th¬¥eoreme de weierstrass
fond¬¥ee sur le calcul des probabilities. Comm. Soc. Math.
Kharkov, 13:1‚Äì2, 1912.
Blei, D. M., Kucukelbir, A., and McAuliffe, J. D. Varia-
tional inference: A review for statisticians. Journal of
the American statistical Association, 112(518):859‚Äì877,
2017.
Blundell, C., Cornebise, J., Kavukcuoglu, K., and Wierstra,
D. Weight uncertainty in neural network. In Interna-
tional Conference on Machine Learning, pp. 1613‚Äì1622.
PMLR, 2015.
Buri, M., Curt, A., Steeves, J., and Hothorn, T. Baseline-
adjusted proportional odds models for the quantiÔ¨Åcation
of treatment effects in trials with ordinal sum score out-
comes. BMC medical research methodology, 20:1‚Äì14,
2020.
Carlan, M., Kneib, T., and Klein, N. Bayesian conditional
transformation models. arXiv preprint arXiv:2012.11016,
2020.
Dinh, L., Krueger, D., and Bengio, Y. Nice: Non-linear
independent components estimation.
arXiv preprint
arXiv:1410.8516, 2014.
Farouki, R. T. The bernstein polynomial basis: A centennial
retrospective. Computer Aided Geometric Design, 29(6):
379‚Äì419, 2012.
Farquhar, S., Smith, L., and Gal, Y. Liberty or depth: Deep
bayesian neural nets do not need complex weight pos-
terior approximations. arXiv e-prints, pp. arXiv‚Äì2002,
2020.
Hothorn, T., Kneib, T., and B¬®uhlmann, P. Conditional trans-
formation models. Journal of the Royal Statistical Soci-
ety: Series B: Statistical Methodology, pp. 3‚Äì27, 2014.
Hothorn, T., Moest, L., and Buehlmann, P. Most likely
transformations. Scandinavian Journal of Statistics, 45
(1):110‚Äì134, 2018.
Jordan, M. I., Ghahramani, Z., Jaakkola, T. S., and Saul,
L. K. An introduction to variational methods for graphical
models. Machine learning, 37(2):183‚Äì233, 1999.
Kobyzev, I., Prince, S., and Brubaker, M. Normalizing
Ô¨Çows: An introduction and review of current methods.
IEEE Transactions on Pattern Analysis and Machine In-
telligence, 2020.
Kook, L., Herzog, L., Hothorn, T., D¬®urr, O., and Sick, B.
Deep and interpretable regression models for ordinal out-
comes. arXiv preprint arXiv:2010.08376, 2020.
Louizos, C. and Welling, M. Multiplicative normalizing
Ô¨Çows for variational bayesian neural networks. In Interna-
tional Conference on Machine Learning, pp. 2218‚Äì2227.
PMLR, 2017.
Ramasinghe, S., Fernando, K., Khan, S., and Barnes, N. Ro-
bust normalizing Ô¨Çows using bernstein-type polynomials.
arXiv preprint arXiv:2102.03509, 2021.
Rezende, D. and Mohamed, S. Variational inference with
normalizing Ô¨Çows. In International Conference on Ma-
chine Learning, pp. 1530‚Äì1538. PMLR, 2015.
Sick, B., Hathorn, T., and D¬®urr, O. Deep transformation
models: Tackling complex regression problems with neu-
ral network based transformation models. In 2020 25th
International Conference on Pattern Recognition (ICPR),
pp. 2476‚Äì2481. IEEE, 2021.
Wainwright, M. J. and Jordan, M. I. Graphical models,
exponential families, and variational inference. Now
Publishers Inc, 2008.
Yao, Y., Vehtari, A., and Gelman, A. Stacking for non-
mixing bayesian computations: The curse and blessing of
multimodal posteriors. arXiv preprint arXiv:2006.12335,
2020.

