Deep Position-Aware Hashing for Semantic Continuous Image Retrieval
Ruikui Wang1,2, Ruiping Wang1,2, Shishi Qiao1,2, Shiguang Shan1,2, Xilin Chen1,2
1 Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS),
Institute of Computing Technology, CAS, Beijing, 100190, China
2 University of Chinese Academy of Sciences, Beijing, 100049, China
{ruikui.wang, shishi.qiao}@vipl.ict.ac.cn, {wangruiping, sgshan, xlchen}@ict.ac.cn
Abstract
Preserving the semantic similarity is one of the most im-
portant goals of hashing. Most existing deep hashing meth-
ods employ pairs or triplets of samples in training stage,
which only consider the semantic similarity within a mini-
batch and depict the local positional relationship in Ham-
ming space, leading to intermittent semantic similarity p-
reservation. In this paper, we propose Deep Position-Aware
Hashing (DPAH) to ensure continuous semantic similarity
in Hamming space by modeling global positional relation-
ship. SpeciÔ¨Åcally, we introduce a set of learnable class cen-
ters as the global proxies to represent the global informa-
tion and generate discriminative binary codes by constrain-
ing the distance between data points and class centers. In
addition, in order to reduce the information loss caused by
relaxing the binary codes to real-values in optimization, we
propose kurtosis loss (KT loss) to handle the distribution of
real-valued features before thresholding to be double-peak,
and then enable the real-valued features to be more binary-
like. Comprehensive experiments on three datasets show
that our DPAH outperforms state-of-the-art methods.
1. Introduction
Owing to fast retrieval speed and low memory cost,
learning to hash [11, 41, 31, 19, 16, 28, 25, 12, 35, 1]
has been widely used in large-scale image retrieval. With
CNN‚Äôs powerful representation capabilities, deep hashing
methods [48, 49, 45, 36, 2, 13, 7, 3, 30, 47] have attracted
more and more attention in recent years.
Generally speaking, hashing is a kind of function that
maps data from high-dimensional real-valued space to low-
dimensional Hamming space and keeps the semantic sim-
ilarities as good as possible. Although the existing deep
hashing methods, such as [43, 18, 22, 24, 21, 4, 37] have
reached excellent performance on benchmarks, most of
them can only give a local description to the positional rela-
tionship of samples in Hamming space. As shown in Figure
(a) pair-wise
(b) triplet-wise
(c) softmax
(d) position awareness
ùõº
ùõº
ùõº
ùõº
ùõº
ùõº
ùõº
ùõº
push
pull
sample
class center
Figure 1. An illustration of Hamming positional relationship us-
ing different supervised manners. Different colors denote different
categories.
1(a)(b), the supervised manner they adopt either belongs to
pair-wise [41, 27, 32, 24, 51, 4] or triplet-wise [18, 50, 40]
which only consider the local similarity within a mini-batch
and has no communication between different batches. This
will lead to intermittent semantic similarity from the global
perspective, especially on multi-label retrieval where sim-
ilarity among samples is more continuous and the task is
more common in real world scenarios than single-label re-
trieval. A couple of works have tried to consider the global
positional relationship in Hamming space, e.g. the point-
wise methods [34, 22, 20, 44, 37]. They aim to classify
in Hamming space using softmax loss, max-margin loss or
L2-loss as the supervised signal. Nevertheless, their binary
codes will not be further optimized as soon as the classi-
Ô¨Åcation results are all correct, which leads to insufÔ¨Åcient
discriminability. This can be seen from Figure 1(c).
In order to preserve continuous semantic similarity as
well as to enhance the discriminability of the binary codes,
we introduce a kind of learnable class center as the global
proxy of one category. This class center is like the origin
of a coordinate system, and once the origin is Ô¨Åxed, then
the coordinates of each data point are determined. In other
words, the global position of all data points within one cat-
egory will be aware. It is so-called position awareness (PA)
whose learning process is vividly shown in Figure 2. Then
we constrain the Hamming distance between data points to
make the binary codes more discriminative. SpeciÔ¨Åcally, a
2493

sample should be close to its relevant class centers (e.g. a
data point may belong to multiple categories in some sce-
narios) and be closer to them by a margin than to the n-
earest irrelevant class center. By doing so, the continuous
semantic similarity can be characterized by different Ham-
ming distance between data points and corresponding class
centers. Figure 1(d) demonstrates the target state of our ap-
proach.
Apart from semantic similarity preservation, the opti-
mization of discrete binary codes is another challenge in
hash learning. As validated in most existing methods, to
make the network trainable in an end-to-end manner, we
choose to relax the discrete codes to real-values by replac-
ing the threshold function sign by sigmoid. However, this
relaxation incurs that the variance of the real-valued fea-
tures before thresholding will become quite large, whose
distribution can be seen from the blue area of Figure 3. Each
half of the blue area will then be thresholded as discrete val-
ues (e.g. 0/1) later, which inevitably leads to heavy infor-
mation loss. On the contrary, if one can constrain the distri-
bution of the real-valued features to be within the red area
in Figure 3, the information loss can be reduced intuitively.
Based on such Ô¨Ånding, we propose a novel kurtosis loss (K-
T loss), a simple approach used for making real-valued fea-
tures distribution be a steeper double-peak. In this way, the
data structure before and after thresholding becomes more
similar, and thus the information loss can be mitigated.
In summary, this paper has three main contributions:
‚Ä¢ We explicitly model the global positional relationship in
Hamming space to preserve the continuous semantic sim-
ilarity and enhance the discriminability of binary codes
by introducing a Hamming margin;
‚Ä¢ Kurtosis loss is proposed to reduce the information loss
caused by thresholding and further narrow the gap be-
tween real-valued space and desired Hamming space;
‚Ä¢ Comprehensive experiments are conducted to show that
our methodology signiÔ¨Åcantly outperforms existing hash-
ing methods for fast image retrieval in both single-label
and multi-label databases.
2. Related Works
Hashing is a potent weapon for fast data retrieval. Com-
pared with real-valued feature based methods [6, 9], hash-
ing with binary codes is matching efÔ¨Åcient and storage
free. The classical literature contains the family of method-
s known as Locality Sensitive Hashing (LSH) [11] and its
variants [31]. Comprehensive survey of traditional hashing
methods can be found in [39].
More recently, in light of the progress of deep neural net-
work, deep hashing methods gradually enter the front stage.
Simultaneously learning image feature and hash function in
an end-to-end manner is the key advantage of deep hashing
Position Awareness
learning
Figure 2. Position awareness learning: the data points‚Äô positions in
Hamming space are rambling before learning, with the help of PA,
they gradually become organized like a coordinate whose origins
correspond the learned class centers. In the Ô¨Ågure, different col-
or dots denote data points from different class. The dashed pen-
tagrams and solid pentagrams denote learnable class centers and
learned class centers respectively. The black dashed lines denote
the data points‚Äô position vectors.
over traditional methods. Most deep hashing methods are
analogous in terms of feature learning module, hash func-
tion learning manner reÔ¨Çects difference instead. Deep based
supervised hashing methods can be divided into four cate-
gories from the perspective of semantic similarity preserva-
tion.
Pair-wise. This kind of methods intend to pull the codes
of similar images together and push the codes of dissimi-
lar images away from each other. This idea is widely used
in metric learning. Representative methods include, ADSH
[14], DPSH [21], COSDISH [15], DHN [51], DSH [24] and
HashNet [4]. The commonality of these methods are that
they do not require fully supervised information and only
needs an afÔ¨Ånity matrix. However, they need to generate
dense pairs online, which results in much consumption of
training time. On the other hand, since deep learning is a
batch-based training method, there may be no communica-
tion between different batches, which results in insufÔ¨Åcient
sample mining. In addition, from the perspective of bina-
ry feature distribution, this type of methods only consider
the relative positional relationship between sample pairs, so
that the original semantic relevance is not well preserved.
Triplet-wise. Compared to the previous one, this type
of methods consider the local positional relationship within
a sample triplet. Representative methods include DNNH
[18], DSRBH [50], DTSH [40]. Similar with the pair-wise
method, this one require online generation of triplets, and
even hard negative mining during training, which costs a lot
of time. And only the local position relationship between
samples is considered.
List-wise. This kind of methods are based on learning to
rank. Representative methods include [5], [38], [46]. They
decompose a sort of retrieved samples into multiple triplet-
s for processing, which is more elaborate than the triplet-
wise, but still inherits the lack of triplet-wise.
Point-wise. Different from the several previous method-
s, this type of methods need no sample pairs or triplets, only
2494

need aware label information for each sample to guide the
learning of the hash functions. This trait make it more scal-
able to large-scale data retrieval [44]. Representative works
include DLBHC [22], SSDH [44] and Greedy Hash [37].
Compared with the above approaches, this kind of methods
can describe the global information with the classiÔ¨Åcation
hyperplanes optimized adaptively as the training data Ô¨Çows
and our DPAH belongs to point-wise.
Minimizing quantization error is another important chal-
lenge in learning to hash. ITQ [12] minimizes quantiza-
tion error by Ô¨Ånding an optimal rotation matrix; SDH [34],
ADSH [14] and other methods optimize the binary code dis-
cretely, so that there is no quantization error; DPSH [21]
and DSH [24] control the quantization error by imposing
a regularizer; HashNet [4] approximates the sign function
by incrementing the slope of the tanh function; Greedy
hash [37] decreases the gap between binary coding learning
and feature representation learning by redeÔ¨Åning the back
propagation of the sign function. This paper novelly re-
duces the gap by constraining the feature distribution of the
real-valued space before Hamming space. Since there is
no direct manipulation of discrete binary features, complex
and time-consuming column-by-column discrete optimiza-
tion processes are avoided, which is very beneÔ¨Åcial for end-
to-end optimization.
3. Approach
Our goal is to learn discriminative binary codes for im-
ages such that: (a) the global continuous semantic similarity
should be preserved in Hamming space, i.e. the more se-
mantic information shared by two images, the closer they
will be in Hamming space; (b) the information loss be-
fore and after thresholding should be small. The training
pipeline is described as follow: taking into the training im-
ages from different classes, our method Ô¨Årst extracts the
real-valued features with the stacked convolution layers and
several fully connected layers. Then with the help of the
PA (Position Awareness) module, the global positional re-
lationships among images are built to ensure continuous se-
mantic similarity in Hamming space. At the same time, the
information loss is reduced by adjusting the distribution of
real-valued features with the proposed KT loss. Finally, the
thresholded outputs are quantized to generate binary codes
for these images.
3.1. Hamming Position Awareness
Given N tuples of training points {Ii, yi}N
i=1, each rep-
resented by image Ii ‚ààRH√óW √ó3 and a C-dimensional
label vector yi ‚àà{0, 1}C (either one-hot or multi-labeled),
we aim to learn a mapping from the RGB space to Ham-
ming space: f : RH√óW √ó3 ‚Üí{0, 1}K, which encodes
Ii into K-bit binary code bi such that the more semantic
information shared by two images, the closer they will be
in Hamming space. Firstly, we model the global relation-
ship by introducing C class centers {Œ∏i}C
i=1 (Œ∏i ‚àà{0, 1}K)
which are learnable during training, i.e. they can record
the historical data information and be updated dynamical-
ly driven by the new data, thus it is suitable to capture the
global information of categories. Secondly, we embed con-
tinuous semantic similarity by constraining the distance be-
tween data points and class centers. SpeciÔ¨Åcally, if Ii be-
longs to one category ci, then corresponding bi should be
very close to Œ∏ci and closer to it by a margin than the nearest
irrelevant class center. Furthermore, if Ii is multi-labeled,
i.e. label{i} = {ci1, ci2, ...cini }, then corresponding bi
should be very close to the center ¬ØŒ∏i of the label set i.e.
¬ØŒ∏i = 1
ni
X
j‚ààlabel{i}
Œ∏j and closer to it by a margin than the
nearest one which is excluded from this label set. It is not d-
ifÔ¨Åcult to note that the single-label case is a special example
of the multi-label case mentioned above. Such constraints
can be formulated as:
Li
inter=max{dH(bi, ¬ØŒ∏i)(1 + Œ±) ‚àí
min
j /‚ààlabel{i} dH(bi, Œ∏j), 0}
s.t.
bi, Œ∏i ‚àà{0, 1}K, i ‚àà{1, ..., N}
(1)
where Li
inter denotes inter-class loss, dH(¬∑, ¬∑) is the Eu-
clidean distance, deÔ¨Åned as dH(x, y) = 1
2 ‚à•x ‚àíy‚à•2
2, which
is equivalent to Hamming distance between two binary
codes, Œ± is the margin hyperparameter, and ni denotes the
size of the label{i}. It is difÔ¨Åcult to optimize directly with
back propagation algorithm since the min(¬∑) and max(¬∑)
functions will easily cause the loss instable. Hence we turn
to optimize the smooth upper bound of Eq.(1) which is mag-
niÔ¨Åed as:
Li
inter = ‚àílog{
e{‚àídH (bi,¬Ø
Œ∏i)(1+Œ±)}
e{‚àídH (bi,¬Ø
Œ∏i)(1+Œ±)}+
P
j /
‚ààlabel{i}
e{‚àídH (bi,Œ∏j )} }
s.t. bi, Œ∏i ‚àà{0, 1}K
(2)
The detailed derivation from Eq.(1) to Eq.(2) can be
found in Appendix A. At the same time, considering the
discriminability in Hamming space and the position com-
pactness within one class clusters, we impose a tight con-
straint like [42] as Eq.(3).
Li
intra = 1
2
bi ‚àí¬ØŒ∏i
2
2
s.t. bi, Œ∏i ‚àà{0, 1}K
(3)
In Eq.(3), Li
intra denotes the intra-class loss.
Actual-
ly the idea of intra compactness has been widely used in
many Ô¨Åelds, such as metric learning, face recognition. Our
method also contains the idea of intra compactness. Dif-
ferent from [42], we consider both the intra compactness
and inter discriminability, i.e. intra-class samples are clos-
2495

35
30
25
20
15
10
5
0
5
10
15
20
25
30
35
0
10000
20000
30000
40000
50000
w/o KT loss
w/ KT loss
Figure 3. Distribution of real-valued outputs before thresholding.
er and inter-class samples will be kept away from a certain
distance by introducing the margin Œ±.
The above two loss functions will push each binary code
to its reasonable position and gradually converge to the state
shown in Figure 1(d). Combining the Eq.(2) with Eq.(3), we
get Eq.(4) as follows:
Ldis = 1
N
N
X
i=1
(Li
inter + ŒªLi
intra)
s.t. bi, Œ∏i ‚àà{0, 1}K
(4)
However, directly optimizing Eq.(4) in an end-to-end
manner is infeasible since the binary constraints will make
it intractable to train the network with back propagation al-
gorithm. To this end, a natural scheme is used by relaxing
the integer constraint of bi to real constraint. SpeciÔ¨Åcal-
ly, we adopt sigmoid as the threshold function to approxi-
mate sign, therefor bi can be relaxed as bi = œÉ(ui). Here
œÉ(¬∑) and ui denote the sigmoid and real-valued output of
the last fully connected layer respectively. Then Eq.(4) is
rewritten as :
Ldis = 1
N
N
X
i=1
(Li
inter + ŒªLi
intra)
s.t. bi, Œ∏i ‚àà(0, 1)K
(5)
In order to reduce the cost of relaxation, as advocated in
[44], an additional regularizer is imposed on bi as:
Lreg =
1
2N
N
X
i=1
‚à•mean(bi) ‚àí0.5‚à•2
2
‚àí1
2N
N
X
i=1
‚à•bi ‚àí0.5I‚à•2
2
s.t. bi ‚àà(0, 1)K
(6)
where the Ô¨Årst term is used for making the hash bits bal-
anced, and the second one is for controlling the quantization
error.
3.2. Kurtosis Loss
The similarity would be well preserved with Eq.(5).
However, some issues still exist caused by the approxi-
mation of the threshold function adopted in Eq.(5) which
has not been noticed in previous works. SpeciÔ¨Åcally, we
Ô¨Ånd that as the training progresses, the variance of the real-
valued network output (i.e. ui) will gradually increase and
its distribution is present in the blue area in Figure 3. Each
half of the blue area will be thresholded as discrete values
(e.g. 0/1) later, which inevitably leads to information loss,
i.e. the large variance of the half of the blue area cannot be
preserved after thresholding. In order to minimize such in-
formation loss, directly optimizing the discrete binary codes
is generally NP-hard [34], we turn to constrain the variance
of ui to be smaller. In speciÔ¨Åc, a threshold t is set, once the
maximum value of ui exceeds the threshold, it will cause
loss. We form it as follows:
Lktl =
1
2N
N
X
i=1
K
X
k=1
[max2(ui(k) ‚àít, 0)
+ max2(‚àíui(k) ‚àít, 0)]
(7)
It is not diffcult to realize from the curve of the Eq.(7)
that the real-valued feature will be pulled back with Lktl,
which presents a form, larger kurtosis of ui. The new dis-
tribution of ui is shown as the red area which is steeper than
the blue area in Figure 3. In mathematics, kurtosis is used
to describe the steepness of a distribution. Therefor we call
Lktl kurtosis loss. By doing so, the distribution structure
of features before thresholding is much like the one after
thresholding, and thus the information loss and the gap be-
fore and after thresholding can be reduced. It should be
noted that we impose an additional regularize on features
before thresholding, which is different from [24, 21] whose
goal is to constrain the real-valued feature to be binary-like.
Now, we rewrite the overall loss function as follows:
L = Ldis + Œ≤Lreg + Œ≥Lktl
s.t. bi, Œ∏i ‚àà(0, 1)K
(8)
where Œ≤ and Œ≥ are pre-deÔ¨Åned weighting parameters to bal-
ance the Lreg and Lktl.
3.3. Discussions
Comparison with metric learning. As mentioned in
Introduction section, semantic similarity preservation is one
of the challenges in hashing. Seeking intra compactness and
inter separability, metric learning is a nice choice for solv-
ing such problem. It has been widely adopted in many ex-
isting hashing methods, such as pair-wise methods, triplet-
wise methods which have been thoroughly introduced in re-
lated work section. Different from point-wise metric learn-
ing methods [42, 26, 10], our DPAH uniÔ¨Åes single-label
retrieval and multi-label retrieval in one formulation and
shows distinctive superiority in multi-label scenarios, which
is veriÔ¨Åed in later experiment section. Furthermore, opti-
mization in discrete Hamming space is an extra challenge
in hashing compared to metric learning of which the analy-
sis lies in Euclidean space. To this end, we propose KT loss
2496

to constrain the feature distribution to be double-peak and
devote to alleviate such trouble.
Comparison with existing point-wise hashing meth-
ods. As mentioned in related work section, our DPAH be-
longs to point-wise type, which is more practical in real ap-
plications. Most recent point-wise hashing methods, such
as SDH [34], SSDH [44], Greedy Hash [37] use inner prod-
uct as metrics in Hamming space. Our DPAH adopts Eu-
clidean distance which is equivalent to Hamming distance
instead. There are two advantages using this scheme: Ô¨Årstly,
Euclidean distance in binary space has an intuitive meaning
and can directly reÔ¨Çect the role of optimization; secondly, it
becomes easier to introduce margin to enhance the discrim-
inability of binary code.
Extensive experiments demon-
strate our DPAH has clear superiority over those point-wise
hashing methods.
4. Experiments
4.1. Datasets and Experimental Settings
We conduct experiments on three datasets: (1) Ima-
geNet [33] consists of about 1.2M images belonging to
1,000 mutually exclusive categories. Following the same
setting in [4], we randomly select 100 categories and use all
images of these categories in training set as the database,
using all images of these categories in validation set to form
query set. Moreover, 130 images per class randomly select-
ed from the database are used to train the deep hashing net-
work. (2) MS COCO [23] contains 82,783 training images
and 40,504 validation images. Following the same setting
in [4], we randomly select 5,000 images to form the query
set, and the rest as the database. Further, 10,000 images
were randomly selected from the database as the training
set. (3) NUS-WIDE [8] contains 269,648 images collected
from Flickr and associated with 81 concepts. Following the
similar protocols as [48], we randomly select 2,100 images
from 21 most happened semantic labels as the query set and
the rest as the training set.
Same as most of the previous hashing methods, the
ground truth is deÔ¨Åned by class-level labels. For ImageNet,
images from the same class are considered semantically rel-
evant and vice versa. For MS COCO and NUS-WIDE, if
two images share at least one positive label, they are con-
sidered relevant, and irrelevant otherwise. As for evalua-
tion metrics, we use the mean Average Precision (mAP) for
different code lengths and precision-recall curves (48-bit).
For fair comparison with state-of-the-art, we use mAP@1K
for ImageNet, mAP@5K for MS COCO and mAP@50K
for NUS-WIDE respectively. Our DPAH method is imple-
mented with Pytorch [29] framework1. The AlexNet [17] is
adopted as our backbone. During training, we set the batch
size as 256, momentum as 0.9, weight decay as 5e-4. The
1Our source codes are available at http://vipl.ict.ac.cn/resources/codes.
Method
NUS-WIDE
MS COCO
ImageNet
mAP@50K
mAP
mAP@5K
mAP
mAP@1K
mAP
contrastive loss
0.7056
0.6414
0.6618
0.5963
N/C
N/C
triplet loss
0.7916
0.7451
0.7183
0.6406
0.5710
0.4655
maximum-margin
0.7393
0.6735
0.7193
0.5654
N/C
N/C
softmax+CE
0.7544
0.6736
0.7334
0.5770
0.7014
0.6218
softmax+PA
0.8038
0.7171
0.7549
0.6266
0.7014
0.6218
HPA(Œª = 0)
0.8279
0.7591
0.7731
0.6634
0.7027
0.6316
HPA(Œª = 0.01)
0.8307
0.7634
0.7756
0.6682
0.7050
0.6334
Table 1. Comparison of retrieval performance with baselines. The
results are obtained with 48-bit binary codes.
0.795
0.800
0.805
0.810
0.815
0.820
0.825
0.830
0.835
0.840
16bits 32bits 48bits 64bits
w/o KT loss
w/ KT loss
(a) NUS-WIDE
0.70
0.72
0.74
0.76
0.78
0.80
16bits 32bits 48bits 64bits
w/o KT loss
w/ KT loss
(b) MS COCO
0.58
0.60
0.62
0.64
0.66
0.68
0.70
0.72
0.74
16bits 32bits 48bits 64bits
w/o KT loss
w/ KT loss
(c) ImageNet
Figure 4. Comparison of the model with and without KT loss on
three databases under different code lengths.
learning rate is Ô¨Åxed as 0.001. We train DPAH model 200
epochs in total.
4.2. Evaluation of Hamming Position Awareness
In order to verify the effectiveness of PA module, we
compare it with several methods corresponding to differ-
ent supervised manner: (1) contrastive loss, a representa-
tive implementation of pair-wise method; (2) triplet loss,
a representative implementation of triplet-wise method; (3)
softmax+CE [44], softmax followed by cross entropy loss
which is a representative implementation of point-wise
method; (4) maximum-margin, which is proposed in [44]
and reproduced carefully by us; (5) softmax+PA, softmax
followed by PA module in which the metric is inner product,
consisting with softmax; (6) our Hamming Position Aware-
ness (HPA) module with different hyperparameter, i.e. Œª.
Without loss of generality, we only test the case with 48-bit
and set Œ≤ = 1 and Œ± = 0.2 in our model according to Sec-
tion3.1. Note that we aim to verify the effectiveness of PA
independently, therefor, no KT loss is used in this section.
The performance of all methods are listed in Table 1. The
N/C in Table 1 denotes the model cannot converge under
the same experimental settings with others.
We have four observations from the comparison table:
First, with the help of PA module, HPA and softmax+PA
are generally better than their corresponding baseline meth-
ods on the three databases, which indicates that out PA mod-
ule is very effective for the semantic similarity preserva-
tion in Hamming space; Second, the superiority of HPA is
obvious compared with baselines especially on multi-label
databases, which veriÔ¨Åed our argument proposed in intro-
duction. Generally speaking, multi-label data means more
complex semantic similarities. Based on the distance con-
straint, our PA can measure the degree of similarity, i.e. the
2497

Method
VGG16
ResNet50
ResNet101
mAP@1K
mAP
mAP@1K
mAP
mAP@1K
mAP
softmax
0.8327
0.7729
0.8134
0.7375
0.8370
0.7717
softmax+KT loss
0.8438
0.7929
0.8302
0.7613
0.8528
0.7938
DPAH\KT loss
0.8449
0.7980
0.8516
0.7944
0.8710
0.8266
DPAH
0.8487
0.8063
0.8566
0.8029
0.8765
0.8361
Table 2. Retrieval performance of the model with KT loss under
different backbones. The results are obtained with 48-bit binary
codes on ImageNet.
35
30
25
20
15
10
5
0
5
10
15
20
25
30
35
0
10000
20000
30000
40000
50000
(a) without KT loss
35
30
25
20
15
10
5
0
5
10
15
20
25
30
35
0
10000
20000
30000
40000
50000
(b) t = 15
35
30
25
20
15
10
5
0
5
10
15
20
25
30
35
0
10000
20000
30000
40000
50000
(c) t = 10
35
30
25
20
15
10
5
0
5
10
15
20
25
30
35
0
10000
20000
30000
40000
50000
(d) t = 5
Figure 5. Distribution of real-valued network outputs under differ-
ent settings of t.
number for categories shared by two images. This can be
further demonstrated in visulization part of Section 4.4. On
the contrary, pair-wise and triplet-wise methods (i.e. the
Ô¨Årst two rows of Table 1) are based solely on a similarity
matrix so that they cannot distinguish pairs whether they
share one or more categories. Third, from the fourth row
and second last row of the Table 1, we can see that soft-
max+CE is not performing as well as HPA. This may re-
sult from that softmax+CE focuses on classiÔ¨Åcation accura-
cy leading to insufÔ¨Åcient discriminability. Fourth, from the
last two rows of the Table 1, we notice that the retrieval per-
formance will further increase with Eq.(3), demonstrating
the effectiveness of making intra-class compact.
4.3. Evaluation of the KT loss
In this part, we evaluate KT loss from the following three
aspects: multiple datasets, different network structures and
parameter sensitivity. Firstly, we test the role of KT loss
on three datasets (i.e.
both single-label and multi-label
dataset). Then, we test the performance of KT loss with
VGG16, ResNet50 and ResNet101 as backbone respective-
ly. Finally, 16-bit is taken as an example to test the sensitiv-
ity of the hyperparameter t. As mentioned in Section 3.2,
the variance of the network real-valued output will gradual-
ly increase without KT loss, we use the maximum value it
can reach as the upper bound of the hyperparameter t.
Firstly, we can see from Figure 4 that KT loss will gener-
ally improve the model‚Äôs retrieval ability compared with the
baselines on all datasets; Secondly, from Table 2, in which
5
10
15
20
25
t
0.810
0.811
0.812
0.813
0.814
0.815
0.816
0.817
mAP@50K
(a) NUS-WIDE
5
10
15
20
25
t
0.728
0.729
0.730
0.731
0.732
0.733
mAP@5K
(b) MS COCO
5
10
15
20
25
t
0.635
0.640
0.645
0.650
0.655
0.660
mAP@1K
(c) ImageNet
Figure 6. The sensitivity to t. The dotted line indicates the result
without KT loss.
0
0.2
0.4
0.6
0.8
0.79
0.80
0.81
0.82
0.83
mAP@50K
(a) NUS-WIDE
0
0.2
0.4
0.6
0.8
0.762
0.764
0.766
0.768
0.770
0.772
0.774
mAP@5K
(b) MS COCO
0
0.2
0.4
0.6
0.8
0.696
0.698
0.700
0.702
0.704
0.706
0.708
mAP@1K
(c) ImageNet
Figure 7. The sensitivity to Œ±. The dotted line indicates the result
without margin.
DPAH\KT loss stands for dropping KT loss from DPAH,
we observe that KT loss have a stable effect with differ-
ent backbones. The above two experiments show that KT
loss is robust. Recalling that KT loss does not utilize su-
pervisory information, and it can theoretically be used for
different discriminative losses, such as softmax. The Ô¨Årst
two rows of Table 2 shows that KT loss also have obvious
effects on softmax loss, thus indicating the versatility of KT
loss; Thirdly, Figure 6 reveals that KT loss is not very sensi-
tive to hyperparameter t. The retrieval performances can be
improved stably when setting t under a reasonable range.
(e.g. [5, 10]). These quantitative results validate that KT
loss reduces information loss caused by the approximation
of the threshold function. As shown in Figure 5, the smaller
t is, the larger the kurtosis of the real-valued network output
is, and the smaller the information loss is. However, when t
is too small, the discriminability of binary codes will be af-
fected. In other words, we should make a tradeoff between
information loss and discriminability. Fortunately, the rea-
sonable range of hyperparameter t is relatively large.
4.4. Empirical Analysis
Parameter Sensitivity Analysis: In the Ô¨Åeld of metric
learning, the use of the margin in the training of the model
tends to increase its generalization ability. Inspired by this,
we believe that once the position of binary codes in Ham-
ming space is aware, imposing a margin allows it to further
consolidate its position (the position becomes more aware),
thereby increasing the generalization ability of the retrieval
model. To verify this, we test the impact of different values
of the margin on three databases. It can be Ô¨Ånded from the
Figure 7, compared to the baseline, the performance is sig-
niÔ¨Åcantly improved after the margin is applied. But when
the margin is too large, optimal hash functions are diffcult
to learn.
Visualization of Hash Codes and Retrieval Samples:
2498

Method
NUS-WIDE
MS COCO
ImageNet
16-bit
32-bit
48-bit
64-bit
16-bit
32-bit
48-bit
64-bit
16-bit
32-bit
48-bit
64-bit
LSH [11]
0.3915
0.4169
0.4114
0.4323
0.4118
0.4689
0.4992
0.5076
0.0699
0.1562
0.2205
0.2732
ITQ [12]
0.5754
0.5840
0.5873
0.5943
0.6293
0.6654
0.6793
0.6903
0.3245
0.4655
0.5219
0.5539
SDH [34]
0.7019
0.7158
0.7157
0.7256
0.5447
0.5857
0.6025
0.6127
0.4001
0.5515
0.6196
0.6516
KSH [25]
0.5693
0.5736
0.5754
0.5822
0.5924
0.6180
0.6345
0.6422
0.3600
0.4803
0.5327
0.5544
SITQ [12]
0.6312
0.6693
0.6808
0.6888
0.6300
0.6760
0.7047
0.7163
0.3250
0.4750
0.5369
0.5779
DSH [24]
0.7014
0.7275
0.7261
0.7289
0.6218
0.6292
0.6383
0.6380
0.4526
0.5563
0.6062
0.6235
HashNet [4]
0.7260
0.7704
0.7797
0.7731
0.6578
0.7121
0.7316
0.7374
0.4643
0.5925
0.6558
0.6544
DHN [51]
0.7443
0.7490
0.7486
0.7482
0.6888
0.7158
0.7221
0.7274
0.2671
0.4367
0.4933
0.5347
DNNH [18]
0.7847
0.8002
0.8053
0.8062
0.6732
0.7137
0.7298
0.7362
0.4946
0.5805
0.6035
0.6143
SSDH [44]
0.7188
0.7225
0.7393
0.7460
0.6970
0.7250
0.7410
0.7440
0.6342
0.6915
0.7014
0.7069
DPAH
0.8162
0.8266
0.8346
0.8280
0.7325
0.7675
0.7777
0.7815
0.6517
0.7001
0.7149
0.7138
Table 3. Comparison of retrieval performance of our DPAH method and the other hashing methods on three benchmark datasets.
0
0.2
0.4
0.6
0.8
1
Recall
0.4
0.6
0.8
Precision
LSH
ITQ
SDH
KSH
SITQ
DSH
HashNet
DHN
DNNH
SSDH
DPAH
0
0.2
0.4
0.6
0.8
1
Recall
0.4
0.6
0.8
Precision
LSH
ITQ
SDH
KSH
SITQ
DSH
HashNet
DHN
DNNH
SSDH
DPAH
0
0.2
0.4
0.6
0.8
1
Recall
0.4
0.6
0.8
Precision
LSH
ITQ
SDH
KSH
SITQ
DSH
HashNet
DHN
DNNH
SSDH
DPAH
(a) NUS-WIDE
(b) MS COCO
(c) ImageNet
Figure 8. Precision-recall curves (48-bit) of our DPAH method and the other hashing methods on three benchmark datasets.
60 40 20 0
20 40 60 80
80
60
40
20
0
20
40
60
80
(a) DNNH
100 75 50 25 0
25 50 75
80
60
40
20
0
20
40
(b) HashNet
60 40 20 0
20 40 60 80
20
0
20
40
60
80
100
(c) DPAH
Figure 9. Comparison of 48-bit Hamming space visualization of
DNNH, HashNet and our DPAH.
With the help of t-SNE, we visualize the 48-bit binary codes
generated by DNNH, HashNet and DPAH on ImageNet in
Figure 9 (for ease of distinction, we sample 20 categories).
These methods belong to triplet-wise, pair-wise and point-
wise respectively. It can be seen from Figure 9(a) that the
binary codes‚Äô position from different class is overlapped se-
riously, which results in pool discriminative ability. Then
the overlap is weakened by HashNet in that the weight of
data pairs is considered. Finally, we can see from Figure
9(c) that binary codes from different categories are well
separated and vice versa, which validates that DPAH can
effectively preserve semantic similarity in Hamming space.
Apart from this, We can see from the Figure 10 and Figure
11 that our DPAH tends to return the images as relevant as
possible.
4.5. Comparison with the State-of-the-arts
Comparative methods: We compare DPAH with ten
classical hashing methods:
unsupervised methods LSH
[11], ITQ [12], supervised shallow methods SITQ [12],
KSH [25], SDH [34] and supervised deep methods DNNH
[18], DHN [51], DSH [24], HashNet [4], SSDH [44]. In or-
der to compare more recent methods, we also made a sup-
plementary experiment on CIFAR-10 in the Appendix B.
For all shallow hashing methods, we use the fc7 layer of the
AlexNet [17] pre-trained on ImageNet2012 as input. For
all deep hashing methods, we use the AlexNet model pre-
trained on ImageNet2012 as the backbone. In our DPAH,
we set the hyperparamter t as 10, Œ± as 0.2, Œª as 0.01, Œ≤ as 1
and Œ≥ as 0.01 respectively.
Results: Table 3 shows the retrieval performance com-
parison of our method against the others and Figure 8 gives
the precision recall curves on three datasets with 48-bit bi-
nary codes. In general, supervised hashing method perfor-
m better than unsupervised methods, validating the impor-
tance of supervised information for learning discriminative
binary codes. In addition, those CNN-based methods out-
perform the conventional hashing methods with deep fea-
tures on both datasets by a large margin, suggesting that
learning discriminative image representations and compact
binary codes simultaneously in an end-to-end manner is ad-
2499

DPAH(ours)
HashNet
DNNH
(a) Case:1
DPAH(ours)
HashNet
DNNH
(b) Case:2
Figure 10. Two retrieval cases on MS COCO, only the top-5 feedbacks are shown due to space limitation. Results were obtained with
64-bit binary codes. The Ô¨Çoating point number with blue background below the image indicates the Jaccard similarity between sample and
the query. The larger Ô¨Çoating number indicates the more relevant the query is with retrieval sample.
DPAH(ours)
HashNet
DNNH
(a) Case:1
DPAH(ours)
HashNet
DNNH
(b) Case:2
Figure 11. Two retrieval cases on NUS-WIDE, only the top-5 feedbacks are shown due to space limitation. Results were obtained with
64-bit binary codes. The Ô¨Çoating point number with blue background below the image indicates the Jaccard similarity between sample and
the query. The larger Ô¨Çoating number indicates the more relevant the query is with retrieval sample.
vantageous. Among the CNN-based methods, HashNet get-
s the best performance in pair-wise methods. It is because
HashNet weights the training pairs that this method is more
robust to deal with imbalanced similarity data; DNNH sam-
ples triplet online, although the training time is relatively
long, the sequence relationship of the data can be guaran-
teed, which is more consistent with the target of the im-
age retrieval. It achieves a near performance as HashNet;
It is worth noting that SSDH, as a representative method of
point-wise type, even outperforms HashNet on the single-
label database, e.g. ImageNet, but not as good as Hash-
Net on multi-label databases, e.g.
MS COCO or NUS-
WIDE. Our DPAH models global positional relationships to
preserve continuous semantic similarities, showing superior
performance on both single-label and multi-label databases.
5. Conclusion
In this paper, we propose a novel deep hashing frame-
work named DPAH for image retrieval task. We attribute
the promising performance to two aspects: First, the pro-
posed Position awareness module that ensures continuous
semantic information in binary codes; Second, the novel
KT loss for reducing the information loss between the real-
valued feature space and the desired Hamming space. S-
ince DPAH is a relatively general hashing method, it has
wide potential applications in other tasks like information
retrieval.
Acknowledgements. This work is partially supported
by Natural Science Foundation of China under contracts
Nos. 61922080, U19B2036, 61772500, and CAS Frontier
Science Key Research Project No. QYZDJ-SSWJSC009.
References
[1] A. Andoni, P. Indyk, T. Laarhoven, I. Razenshteyn, and
L. Schmidt. Practical and optimal lsh for angular distance. In
Advances in Neural Information Processing Systems, pages
1225‚Äì1233, 2015.
[2] F. Cakir, K. He, S. Adel Bargal, and S. Sclaroff. Mihash: On-
line hashing with mutual information. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion, pages 437‚Äì445, 2017.
2500

[3] F. Cakir, K. He, and S. Sclaroff. Hashing with binary ma-
trix pursuit. In Proceedings of the European Conference on
Computer Vision, pages 332‚Äì348. Springer, 2018.
[4] Z. Cao, M. Long, J. Wang, and P. S. Yu. Hashnet: Deep
learning to hash by continuation. In Proceedings of the IEEE
International Conference on Computer Vision, pages 5608‚Äì
5617, 2017.
[5] Z. Cao, T. Qin, T.-Y. Liu, M.-F. Tsai, and H. Li. Learning to
rank: from pairwise approach to listwise approach. In Pro-
ceedings of the 24th International Conference on Machine
Learning, pages 129‚Äì136. ACM, 2007.
[6] K. ChatÔ¨Åeld, R. Arandjelovi¬¥c, O. Parkhi, and A. Zisserman.
On-the-Ô¨Çy learning for visual search of large-scale image and
video datasets. International Journal of Multimedia Infor-
mation Retrieval, 4(2):75‚Äì93, 2015.
[7] Z. Chen, X. Yuan, J. Lu, Q. Tian, and J. Zhou. Deep hashing
via discrepancy minimization. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition,
pages 6838‚Äì6847, 2018.
[8] T.-S. Chua, J. Tang, R. Hong, H. Li, Z. Luo, and Y. Zheng.
Nus-wide: a real-world web image database from national
university of singapore. In Proceedings of the ACM Inter-
national Conference on Image and Video Retrieval, page 48.
ACM, 2009.
[9] E. J. Crowley, O. M. Parkhi, and A. Zisserman. Face paint-
ing: querying art with photos. In Proceedings of the British
Machine Vision Conference, pages 65.1‚Äì65.13, 2015.
[10] J. Deng, J. Guo, N. Xue, and S. Zafeiriou. Arcface: Additive
angular margin loss for deep face recognition. In Proceed-
ings of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 4690‚Äì4699, 2019.
[11] A. Gionis, P. Indyk, R. Motwani, et al. Similarity search in
high dimensions via hashing. In VLDB, volume 99, pages
518‚Äì529, 1999.
[12] Y. Gong, S. Lazebnik, A. Gordo, and F. Perronnin. Itera-
tive quantization: A procrustean approach to learning bina-
ry codes for large-scale image retrieval. IEEE Transactions
on Pattern Analysis and Machine Intelligence, 35(12):2916‚Äì
2929, 2013.
[13] K. He, F. Cakir, S. Adel Bargal, and S. Sclaroff. Hashing as
tie-aware learning to rank. In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition, pages
4023‚Äì4032, 2018.
[14] Q.-Y. Jiang and W.-J. Li. Asymmetric deep supervised hash-
ing. In Proceedings of the Thirty-Second AAAI Conference
on ArtiÔ¨Åcial Intelligence, 2018.
[15] W.-C. Kang, W.-J. Li, and Z.-H. Zhou. Column sampling
based discrete supervised hashing.
In Proceedings of the
Thirtieth AAAI conference on ArtiÔ¨Åcial Intelligence, 2016.
[16] W. Kong and W. jun Li. Isotropic hashing. In Advances in
Neural Information Processing Systems, pages 1646‚Äì1654.
2012.
[17] A. Krizhevsky, I. Sutskever, and G. E. Hinton.
Imagenet
classiÔ¨Åcation with deep convolutional neural networks. In
Advances in Neural Information Processing Systems, pages
1097‚Äì1105, 2012.
[18] H. Lai, Y. Pan, Y. Liu, and S. Yan. Simultaneous feature
learning and hash coding with deep neural networks. In Pro-
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pages 3270‚Äì3278, 2015.
[19] P. Li, A. Shrivastava, J. L. Moore, and A. C. K¬®onig. Hashing
algorithms for large-scale learning. In Advances in Neural
Information Processing Systems, pages 2672‚Äì2680, 2011.
[20] Q. Li, Z. Sun, R. He, and T. Tan. Deep supervised discrete
hashing. In Advances in Neural Information Processing Sys-
tems, pages 2482‚Äì2491. 2017.
[21] W. J. Li, S. Wang, and W. C. Kang. Feature learning based
deep supervised hashing with pairwise labels. In Proceed-
ings of the Twenty-Fifth International Joint Conference on
ArtiÔ¨Åcial Intelligence, 2016.
[22] K. Lin, H.-F. Yang, J.-H. Hsiao, and C.-S. Chen. Deep learn-
ing of binary hash codes for fast image retrieval. In Proceed-
ings of the IEEE Conference on Computer Vision and Pattern
Recognition workshops, pages 27‚Äì35, 2015.
[23] T. Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ra-
manan, P. Dollr, and C. L. Zitnick. Microsoft coco: Common
objects in context. 2014.
[24] H. Liu, R. Wang, S. Shan, and X. Chen. Deep supervised
hashing for fast image retrieval. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition,
June 2016.
[25] W. Liu, J. Wang, R. Ji, Y. G. Jiang, and S. F. Chang. Super-
vised hashing with kernels. In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition, 2012.
[26] W. Liu, Y. Wen, Z. Yu, M. Li, B. Raj, and L. Song.
Sphereface: Deep hypersphere embedding for face recogni-
tion. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pages 212‚Äì220, 2017.
[27] M. Norouzi and D. M. Blei. Minimal loss hashing for com-
pact binary codes. In Proceedings of the 28th Internation-
al Conference on Machine Learning, pages 353‚Äì360. ACM,
2011.
[28] M. Norouzi, D. J. Fleet, and R. R. Salakhutdinov. Hamming
distance metric learning. In Advances in Neural Information
Processing Systems, pages 1061‚Äì1069, 2012.
[29] A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. De-
Vito, Z. Lin, A. Desmaison, L. Antiga, and A. Lerer. Auto-
matic differentiation in pytorch. 2017.
[30] Q. Qiu, J. Lezama, A. Bronstein, and G. Sapiro. Foresthash:
Semantic hashing with shallow random forests and tiny con-
volutional networks. In Proceedings of the European Con-
ference on Computer Vision, pages 432‚Äì448. Springer, 2018.
[31] M. Raginsky and S. Lazebnik.
Locality-sensitive binary
codes from shift-invariant kernels. In Advances in Neural
Information Processing Systems, pages 1509‚Äì1517, 2009.
[32] M. Rastegari, A. Farhadi, and D. Forsyth. Attribute discov-
ery via predictable discriminative binary codes. In Proceed-
ings of the European Conference on Computer Vision, pages
876‚Äì889. Springer, 2012.
[33] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,
S. Ma, Z. Huang, A. Karpathy, A. Khosla, and M. Bernstein.
Imagenet large scale visual recognition challenge. Interna-
tional Journal of Computer Vision, 115(3):211‚Äì252, 2015.
2501

[34] F. Shen, C. Shen, W. Liu, and H. Tao Shen.
Supervised
discrete hashing.
In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pages 37‚Äì45,
2015.
[35] A. Shrivastava and P. Li.
Asymmetric LSH (ALSH) for
sublinear time maximum inner product search (MIPS). In
Advances in Neural Information Processing Systems, pages
2321‚Äì2329, 2014.
[36] S. Su, G. Chen, X. Cheng, and R. Bi.
Deep supervised
hashing with nonlinear projections. In Proceedings of the
Twenty-Sixth International Joint Conference on ArtiÔ¨Åcial In-
telligence, pages 2786‚Äì2792, 2017.
[37] S. Su, C. Zhang, K. Han, and Y. Tian. Greedy hash: To-
wards fast optimization for accurate hash coding in cnn. In
Advances in Neural Information Processing Systems, pages
806‚Äì815. 2018.
[38] J. Wang, W. Liu, A. X. Sun, and Y.-G. Jiang. Learning hash
codes with listwise supervision. In Proceedings of the IEEE
International Conference on Computer Vision, pages 3032‚Äì
3039, 2013.
[39] J. Wang, H. T. Shen, J. Song, and J. Ji. Hashing for similarity
search: A survey. arXiv preprint arXiv:1408.2927, 2014.
[40] X. Wang, Y. Shi, and K. M. Kitani. Deep supervised hashing
with triplet labels. In Asian Conference on Computer Vision,
pages 70‚Äì84. Springer, 2016.
[41] Y. Weiss, A. Torralba, and R. Fergus. Spectral hashing. In
Advances in Neural Information Processing Systems, 2008.
[42] Y. Wen, K. Zhang, Z. Li, and Y. Qiao. A discriminative fea-
ture learning approach for deep face recognition. In Proceed-
ings of the European Conference on Computer Vision, pages
499‚Äì515. Springer, 2016.
[43] R. Xia, Y. Pan, H. Lai, C. Liu, and S. Yan. Supervised hash-
ing for image retrieval via image representation learning. In
Proceedings of the Twenty-Eighth AAAI Conference on Arti-
Ô¨Åcial Intelligence, 2014.
[44] H.-F. Yang, K. Lin, and C.-S. Chen. Supervised learning of
semantics-preserving hash via deep convolutional neural net-
works. IEEE Transactions on Pattern Analysis and Machine
Intelligence, 40(2):437‚Äì451, 2018.
[45] T. Yao, F. Long, T. Mei, and Y. Rui.
Deep semantic-
preserving and ranking-based hashing for image retrieval. In
Proceedings of the Twenty-Fifth International Joint Confer-
ence on ArtiÔ¨Åcial Intelligence, pages 3931‚Äì3937, 2016.
[46] Z. Yu, F. Wu, Y. Zhang, S. Tang, J. Shao, and Y. Zhuang.
Hashing with list-wise learning to rank. In Proceedings of
the 37th international ACM SIGIR conference on Research
& development in information retrieval, pages 999‚Äì1002.
ACM, 2014.
[47] X. Yuan, L. Ren, J. Lu, and J. Zhou. Relaxation-free deep
hashing via policy gradient. In Proceedings of the European
Conference on Computer Vision, pages 134‚Äì150. Springer,
2018.
[48] R. Zhang, L. Lin, R. Zhang, W. Zuo, and L. Zhang. Bit-
scalable deep hashing with regularized similarity learning for
image retrieval and person re-identiÔ¨Åcation. IEEE Transac-
tions on Image Processing, 24(12):4766‚Äì4779, 2015.
[49] Z. Zhang, Y. Chen, and V. Saligrama. EfÔ¨Åcient training of
very deep neural networks for supervised hashing. In Pro-
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pages 1487‚Äì1495, 2016.
[50] F. Zhao, Y. Huang, L. Wang, and T. Tan. Deep semantic rank-
ing based hashing for multi-label image retrieval. In Pro-
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pages 1556‚Äì1564, 2015.
[51] H. Zhu, M. Long, J. Wang, and Y. Cao. Deep hashing net-
work for efÔ¨Åcient similarity retrieval. In Proceedings of the
Thirtieth AAAI Conference on ArtiÔ¨Åcial Intelligence, 2016.
2502

