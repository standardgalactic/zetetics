Modelling operational risk using a Bayesian
approach to extreme value theory
Mar´ıa Elena Rivera Manc´ıa
Doctor of Philosophy
Deparment of Mathematics and Statistics
McGill University
Montr´eal, Qu´ebec
February 2014
A thesis submitted to McGill University in partial fullﬁlment of the
requirements of the degree of Doctorate of Philosophy
Copyright ©Elena Rivera, 2014

i
DEDICATION
I dedicate this thesis to the strongest woman I have ever met in my life: My mom,
who passed away in 2008. Her love and encouragement are still with me today.
I also dedicate this work to my family, for all the love and support they have given me
throughout my life.

ii
ACKNOWLEDGEMENTS
I would like to express my deep gratitude to my supervisors, Professor David Stephens and
Professor Johanna Neˇslehov´a, for their invaluable support and patience for the completion
of this work. This thesis would not have been possible without their continued feedback
and encouragement throughout this long journey.
I thank my PhD committee members, Professor Christian Genest, Professor Jean-
Fran¸cois Plante and Professor Russell Steele, for reading my thesis and for their insightful
comments. I would also like to thank my external examiner, Professor Val´erie Chavez-
Demoulin, for her willingness to review my thesis and for her valuable comments and
suggestions.
I am very thankful to Professor Christian Genest for kindly helping me with the French
translation of the thesis abstract.
I would also like to thank the Department of Mathematics and Statistics for their
ﬁnancial support throughout my studies and for making my stay at McGill more enjoyable.
Special thanks to the Bank of Mexico for all that I learned during the two summer
internships and for giving me the opportunity to work with real data and enrich my
research.
I wish to thank my friends in Montr´eal for all the moments that turned my mind away
from work and brought joy to my life. I must also thank those friends who are far in
distance but close in heart, for their constant encouragement.
Above all I am very grateful to my family, for their trust and love during my whole
life, and for teaching me what really matters in life. I cannot ﬁnd the words to express all
my gratitude to them.
Finally, I want to thank to all those people who have believed in me.

iii
ABSTRACT
Extreme-value theory is concerned with the tail behaviour of probability distributions.
In recent years, it has found many applications in areas as diverse as hydrology, actuarial
science, and ﬁnance, where complex phenomena must often be modelled from a small
number of observations.
Extreme-value theory can be used to assess the risk of rare events either through the
block maxima or peaks-over-threshold method. The choice of threshold is both inﬂuential
and delicate, as a balance between the bias and variance of the estimates is required. At
present, this threshold is often chosen arbitrarily, either graphically or by setting it as
some high quantile of the data.
Bayesian inference is an alternative to deal with this problem by treating the threshold
as a parameter in the model. In addition, a Bayesian approach allows for the incorporation
of internal and external observations in combination with expert opinion, thereby providing
a natural probabilistic framework to evaluate risk models.
This thesis presents a Bayesian inference framework for extremes.
We focus on a
model proposed by Behrens et al. (2004), where an analysis of extremes is performed
using a mixture model that combines a parametric form for the centre and a Generalized
Pareto Distribution (GPD) for the tail of the distribution. Our approach accounts for all
the information available in making inference about the unknown parameters from both
distributions, the threshold included. A Bayesian analysis is then performed by using
expert opinions to determine the parameters for prior distributions; posterior inference is
carried out through Markov Chain Monte Carlo methods. We apply this methodology to
operational risk data to analyze its performance.
The contributions of this thesis can be outlined as follows:
 Bayesian models have been barely explored in operational risk analysis. In Chap-
ter 3, we show how these models can be adapted to operational risk analysis using
fraud data collected by diﬀerent banks between 2007 and 2010. By combining prior

iv
information to the data, we can estimate the minimum capital requirement and risk
measures such as the Value-at-Risk (VaR) and the Expected Shortfall (ES) for each
bank.
 The use of expert opinion plays a fundamental role in operational risk modelling.
However, most of time this issue is not addressed properly. In Chapter 4, we consider
the context of the problem and show how to construct a prior distribution based on
measures that experts are familiar with, including VaR and ES. The purpose is to
facilitate prior elicitation and reproduce expert judgement faithfully.
 In Section 4.3, we describe techniques for the combination of expert opinions. While
this issue has been addressed in other ﬁelds, it is relatively recent in our context. We
examine how diﬀerent expert opinions may inﬂuence the posterior distribution and
how to build a prior distribution in this case. Results are presented on simulated
and real data.
 In Chapter 5, we propose several new mixture models with Gamma and Generalized
Pareto elements. Our models improve upon previous work by Behrens et al. (2004)
since the loss distribution is either continuous at a ﬁxed quantile or it has continuous
ﬁrst derivative at the blend point. We also consider the cases when the scaling is
arbitrary and when the density is discontinuous.
 Finally, we introduce two nonparametric models.
The ﬁrst one is based on the
fact that the GPD model can be represented as a Gamma mixture of exponential
distributions, while the second uses a Dirichlet process prior on the parameters of
the GPD model.

v
ABR´EG´E
La th´eorie des valeurs extrˆemes concerne l’´etude du comportement caudal de lois de
probabilit´e. Ces derni`eres ann´ees, elle a trouv´e de nombreuses applications dans des do-
maines aussi vari´es que l’hydrologie, l’actuariat et la ﬁnance, o`u l’on doit parfois mod´eliser
des ph´enom`enes complexes `a partir d’un petit nombre d’observations.
La th´eorie des valeurs extrˆemes permet d’´evaluer le risque d’´ev´enements rares par la
m´ethode des maxima bloc par bloc ou celle des exc`es au-del`a d’un seuil. Le choix du seuil
est `a la fois inﬂuent et d´elicat, vu la n´ecessit´e de trouver un ´equilibre entre le biais et la
pr´ecision des estimations. `A l’heure actuelle, ce seuil est souvent choisi arbitrairement,
soit `a partir d’un graphique ou d’un quantile ´elev´e des donn´ees.
L’inf´erence bay´esienne permet de contourner cette diﬃcult´e en traitant le seuil comme
un param`etre du mod`ele. L’approche bay´esienne permet en outre d’incorporer des obser-
vations internes et externes en lien avec l’opinion d’experts, fournissant ainsi un cadre
probabiliste naturel pour l’´evaluation des mod`eles de risque.
Cette th`ese d´ecrit un cadre d’inf´erence bay´esien pour les extrˆemes. Ce cadre est ins-
pir´e des travaux de Behrens et coll. (2004), dans lesquels l’´etude des extrˆemes est r´ealis´ee
au moyen d’un mod`ele de m´elange alliant une forme param´etrique pour le cœur de la
distribution et une loi de Pareto g´en´eralis´ee (LPG) pour sa queue. L’approche propos´ee
exploite toute l’information disponible pour le choix des param`etres des deux lois, y com-
pris le seuil. Une analyse bay´esienne tenant compte d’avis d’experts sur les param`etres
des lois a priori est ensuite eﬀectu´ee ; l’inf´erence a posteriori s’appuie sur une chaˆıne de
Markov Monte-Carlo. Nous appliquons cette approche `a des donn´ees relatives aux risques
op´erationnels aﬁn d’analyser sa performance.
Les principales contributions de cette th`ese sont les suivantes :
 On fait rarement appel aux mod`eles bay´esiens pour l’analyse du risque op´erationnel.
Au chapitre 3, nous montrons comment adapter ces mod`eles `a l’analyse du risque
op´erationnel au moyen de statistiques de fraudes recueillies par des banques entre

vi
2007 et 2010. L’int´egration d’information a priori aux donn´ees nous permet d’estimer
le capital minimal requis pour chaque banque, ainsi que diverses mesures de risque
telles que la valeur-`a-risque (VaR) et le d´eﬁcit pr´evu (DP).
 Les avis d’experts jouent un rˆole clef dans la mod´elisation du risque op´erationnel.
Toutefois, cette question est souvent trait´ee de fa¸con incorrecte. Au chapitre 4, nous
examinons le probl`eme dans son contexte et montrons comment choisir une loi a
priori `a partir de mesures que les experts connaissent bien, dont la VaR et le DP. Le
but est de faciliter le choix de la loi a priori et de mieux reﬂ´eter l’avis des experts.
 `A la section 4.3, nous d´ecrivons diverses techniques de synth`ese d’opinions d’experts.
Bien que ce probl`eme ait d´ej`a ´et´e abord´e dans d’autres domaines, il est relativement
nouveau dans notre contexte. Nous montrons comment ´elaborer une loi a priori `a
partir d’avis d’experts et mesurons leur inﬂuence sur la loi a posteriori. Des donn´ees
r´eelles et simul´ees sont utilis´ees aux ﬁns d’illustration.
 Au chapitre 5, nous proposons plusieurs nouveaux mod`eles faisant intervenir des
m´elanges de lois gamma et de Pareto g´en´eralis´ees. Ces mod`eles ´etendent les travaux
de Behrens et coll. (2004) dans la mesure o`u la loi des pertes peut ˆetre continue `a un
quantile donn´e ou avoir une premi`ere d´eriv´ee continue au point de jonction. Nous
traitons aussi les cas o`u l’´echelle est arbitraire et la densit´e est discontinue.
 Enﬁn, nous pr´esentons deux mod`eles non param´etriques. Le premier s’appuie sur
le fait que le mod`ele LPG peut ˆetre repr´esent´e comme un m´elange gamma de lois
exponentielles ; dans le second, l’information a priori sur les param`etres du mod`ele
LPG est repr´esent´ee par un processus de Dirichlet.

Contents
Contents
vii
List of Figures
xi
List of Tables
xvi
1
Introduction
1
2
Extreme Value Theory in operational risk
4
2.1
Operational risk . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4
2.1.1
Introduction to operational risk . . . . . . . . . . . . . . . . . . . .
4
2.1.2
Basel II and operational risk . . . . . . . . . . . . . . . . . . . . . .
5
2.1.2.1
Implementation . . . . . . . . . . . . . . . . . . . . . . . .
7
2.1.3
Loss Distribution Approach (LDA) . . . . . . . . . . . . . . . . . .
8
2.1.4
Operational risk data . . . . . . . . . . . . . . . . . . . . . . . . . .
8
2.1.5
Bank fraud
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
9
2.1.6
Challenges in operational risk . . . . . . . . . . . . . . . . . . . . .
10
2.2
Extreme Value Theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
11
2.2.1
Basic deﬁnitions and results . . . . . . . . . . . . . . . . . . . . . .
11
2.2.1.1
Generalized Extreme Value Distribution . . . . . . . . . .
11
2.2.1.2
Threshold Exceedances . . . . . . . . . . . . . . . . . . . .
12
2.2.2
The Peaks Over the Threshold and the Point Process Approach . .
14
vii

Contents
viii
2.2.3
Threshold selection . . . . . . . . . . . . . . . . . . . . . . . . . . .
17
2.2.3.1
Threshold choice plot
. . . . . . . . . . . . . . . . . . . .
17
2.2.3.2
Mean residual life plot . . . . . . . . . . . . . . . . . . . .
18
2.2.3.3
Dispersion index plot . . . . . . . . . . . . . . . . . . . . .
20
2.2.4
Measuring operational risk . . . . . . . . . . . . . . . . . . . . . . .
22
2.2.5
Limitations of Extreme Value Theory . . . . . . . . . . . . . . . . .
24
2.3
Bayesian inference in Extreme Value Theory . . . . . . . . . . . . . . . . .
25
2.3.1
Bayesian framework
. . . . . . . . . . . . . . . . . . . . . . . . . .
26
2.3.2
Basics of Bayesian inference for extremes . . . . . . . . . . . . . . .
27
2.3.3
Combining diﬀerent data sources
. . . . . . . . . . . . . . . . . . .
29
2.3.3.1
Ad-hoc procedures . . . . . . . . . . . . . . . . . . . . . .
29
2.3.3.2
Bayesian methods
. . . . . . . . . . . . . . . . . . . . . .
31
2.3.4
Prior elicitation . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
32
2.3.4.1
The elicitation process . . . . . . . . . . . . . . . . . . . .
33
2.3.4.2
Elicitation and extreme events
. . . . . . . . . . . . . . .
34
2.3.4.3
Distribution ﬁtting . . . . . . . . . . . . . . . . . . . . . .
35
2.3.5
Advantages and challenges of Bayesian inference in EVT . . . . . .
36
2.4
Conclusions of the Chapter . . . . . . . . . . . . . . . . . . . . . . . . . . .
37
3
A Bayesian model for operational risk
39
3.1
General model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
40
3.1.1
Priors
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
41
3.1.2
Posterior inference
. . . . . . . . . . . . . . . . . . . . . . . . . . .
43
3.1.3
Performance in simulations . . . . . . . . . . . . . . . . . . . . . . .
44
3.2
An application to operational risk data . . . . . . . . . . . . . . . . . . . .
47
3.2.1
Data description
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
47
3.2.2
Exploratory analysis . . . . . . . . . . . . . . . . . . . . . . . . . .
48
3.2.3
Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
50

Contents
ix
3.2.4
Operational risk measurement . . . . . . . . . . . . . . . . . . . . .
55
3.2.5
Grouped data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
60
3.2.5.1
Introducing a Reversible Jump Markov Chain Monte Carlo
algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . .
65
3.2.5.2
Results
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
67
3.2.5.3
An alternative for FIL and BAC data
. . . . . . . . . . .
71
3.3
Conclusions of the Chapter . . . . . . . . . . . . . . . . . . . . . . . . . . .
76
4
Prior elicitation and analysis
78
4.1
The non-informative prior . . . . . . . . . . . . . . . . . . . . . . . . . . .
79
4.2
Elicitation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
81
4.3
Elicitation from multiple experts
. . . . . . . . . . . . . . . . . . . . . . .
89
4.3.1
Prior on σ and ξ for multiple experts . . . . . . . . . . . . . . . . .
90
4.3.2
Multiple experts and real data . . . . . . . . . . . . . . . . . . . . .
91
4.3.3
A new prior for multiple experts . . . . . . . . . . . . . . . . . . . .
96
4.3.4
Posterior analysis for more than two experts . . . . . . . . . . . . .
98
4.3.5
Updating the weighting of the experts’ opinions . . . . . . . . . . . 101
4.3.6
Prior weights updating . . . . . . . . . . . . . . . . . . . . . . . . . 101
4.3.7
Posterior distribution updating
. . . . . . . . . . . . . . . . . . . . 105
4.4
Conclusions of the Chapter . . . . . . . . . . . . . . . . . . . . . . . . . . . 111
5
Extending the GPD mixture model
112
5.1
Blended Gamma-GPD model
. . . . . . . . . . . . . . . . . . . . . . . . . 113
5.1.1
A model with continuous density at the qth quantile
. . . . . . . . 114
5.1.2
A model with continuous ﬁrst derivative at the blend point . . . . . 114
5.1.3
A model with continuous ﬁrst derivative with arbitrary scaling . . . 116
5.1.4
A model with discontinuous density with arbitrary scaling
. . . . . 117
5.1.5
Real data example
. . . . . . . . . . . . . . . . . . . . . . . . . . . 118

Contents
x
5.2
A Bayesian nonparametric model . . . . . . . . . . . . . . . . . . . . . . . 124
5.2.1
Simulation from the DPM model
. . . . . . . . . . . . . . . . . . . 125
5.2.2
Posterior inference under the DPM model
. . . . . . . . . . . . . . 126
5.2.2.1
Sampling the θ parameters
. . . . . . . . . . . . . . . . . 126
5.2.2.2
Sampling the GPD parameters
. . . . . . . . . . . . . . . 127
5.2.2.3
Sampling the DP precision parameter
. . . . . . . . . . . 128
5.2.3
Introducing the oﬀset u
. . . . . . . . . . . . . . . . . . . . . . . . 128
5.3
A second Bayesian nonparametric model . . . . . . . . . . . . . . . . . . . 130
5.3.1
Priors
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 131
5.3.2
Inference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 131
5.3.2.1
Sampling the ζ parameters
. . . . . . . . . . . . . . . . . 131
5.3.2.2
Sampling the hyperparameters
. . . . . . . . . . . . . . . 133
5.3.3
Extending the second Bayesian nonparametric model . . . . . . . . 133
5.4
A nonparametric version of the model from Section 5.1.4 . . . . . . . . . . 134
5.4.1
Implementing the DPM in the model from Section 5.1.4 . . . . . . . 134
5.4.2
Updating the remaining parameters . . . . . . . . . . . . . . . . . . 136
5.4.2.1
Parameters of the density fH
. . . . . . . . . . . . . . . . 136
5.4.2.2
Threshold parameter u and scaling parameter ω . . . . . . 137
5.4.2.3
Dirichlet process parameters . . . . . . . . . . . . . . . . . 137
5.4.3
Real data example
. . . . . . . . . . . . . . . . . . . . . . . . . . . 137
5.5
Conclusions of the Chapter . . . . . . . . . . . . . . . . . . . . . . . . . . . 141
6
Conclusions and future research
142
6.1
Contributions of the thesis . . . . . . . . . . . . . . . . . . . . . . . . . . . 142
6.2
Future research . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 144
6.2.1
Finite mixture distributions and EVT . . . . . . . . . . . . . . . . . 144
6.2.1.1
Simulation study . . . . . . . . . . . . . . . . . . . . . . . 147
6.2.1.2
GPD scale and shape parameters for mixture distributions 147

6.2.1.3
High quantiles estimation . . . . . . . . . . . . . . . . . . 151
6.2.1.4
Mixtures of distributions with disjoint supports. . . . . . . 156
6.3
Concluding remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 157
A Markov Chain Monte Carlo methods
159
A.1 Metropolis–Hastings sampling . . . . . . . . . . . . . . . . . . . . . . . . . 160
A.2 The Gibbs sampler . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 160
A.3 Metropolis-within-Gibbs . . . . . . . . . . . . . . . . . . . . . . . . . . . . 161
A.4 Convergence diagnostics
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 162
A.4.1
Gelman and Rubin diagnostic . . . . . . . . . . . . . . . . . . . . . 162
A.4.2
Heidelberg and Welch diagnostic
. . . . . . . . . . . . . . . . . . . 163
A.4.3
Eﬀective Sample Size . . . . . . . . . . . . . . . . . . . . . . . . . . 164
A.5 Reversible jump MCMC . . . . . . . . . . . . . . . . . . . . . . . . . . . . 164
A.5.1
Model formulation
. . . . . . . . . . . . . . . . . . . . . . . . . . . 164
B Jeﬀreys prior for the GPD
167
C Algorithm for the Bayesian model
173
D Non-informative prior plots
175
Bibliography
180
List of Figures
2.1
Threshold choice plot with threshold around 0.98
. . . . . . . . . . . . . . . . . .
18
2.2
Mean residual life plot with threshold around 2.5
. . . . . . . . . . . . . . . . . .
20
xi

List of Figures
xii
2.3
Dispersion index plot with threshold around 5 . . . . . . . . . . . . . . . . . . . .
21
3.1
Representation of the mixture model . . . . . . . . . . . . . . . . . . . . . . . . .
41
3.2
Trace plots of the MCMC samples from the posterior density for n=1000 simulated
data from a Gamma-GPD mixture, with ξ = −0.1. 100,000 iterations after burn-in
46
3.3
Histograms of the MCMC samples from the posterior density for n=1000 simulated
data from a Gamma-GPD mixture, with ξ = −0.1. 100,000 iterations after burn-in
47
3.4
Top left: Monthly fraud losses (scaled by the asset size) in 41 banks from 01/2007
to 04/2010. Top right: Histogram of scaled fraud losses from 01/2007 to 04/2010.
Bottom: Exponential Q-Q plot for n=626 scaled fraud losses.
We can infer the
heavy-tailedness of the data in all cases.
. . . . . . . . . . . . . . . . . . . . . .
49
3.5
Mean excess plot (fraud data) Circled area represents the posterior range of u . . . .
51
3.6
Trace plots of the MCMC samples from the posterior density for n=626 fraud losses
in 41 banks, recorded from 01/2007 to 04/2010. Two chains for 100,000 iterations
were run in R: Gray colour is the ﬁrst chain and black colour is the second chain . .
52
3.7
Histograms of the MCMC samples from the posterior density for n=626 fraud losses
in 41 banks, recorded from 01/2007 to 04/2010. 100,000 iterations . . . . . . . . .
53
3.8
Gelman plots for the MCMC posterior estimates for n=626 fraud losses in 41 banks,
recorded from 01/2007 to 04/2010. 100,000 iterations . . . . . . . . . . . . . . . .
54
3.9
Top: Bar plots of the Minimum capital requirement for fraud losses in each bank,
using the Basic Indicator Approach (Dark gray) and the Bayesian approach (Gray).
Bottom: Minimum capital requirement trend for fraud losses in each bank, using the
Basic Indicator Approach (Dark gray) and the Bayesian approach (Gray)
. . . . .
59
3.10 Minimum capital requirement for fraud losses in each bank using the Basic Indicator
Approach vs. Bayesian approach
. . . . . . . . . . . . . . . . . . . . . . . . . .
60
3.11 Scaled losses for fraud data subgroups: BAC (75 observations), FIL (50 observa-
tions), G-7 (270 observations) and MED (231 observations). (Dashed gray line is
the threshold value)
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
62

List of Figures
xiii
3.12 Original data and ﬁtted densities using the MCMC posterior estimates of the Gamma
and GPD parameters (dashed gray line is the threshold value) . . . . . . . . . . . .
64
3.13 Jumps between Models 1 and 2 for the diﬀerent fraud data subgroups . . . . . . . .
68
3.14 GSM for BAC data
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
75
3.15 GSM for FIL data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
76
4.1
Prior distributions: σ ∼LN(0, 1.25) ; u ∼TN(min (fraud data)); ξ ∼Jeﬀreys prior
80
4.2
Loss distribution used in the elicitation process
. . . . . . . . . . . . . . . . . . .
82
4.3
Trace plots and histograms of prior samples, using the opinion of a ﬁctitious expert
and the prior in Equation (4.5)
. . . . . . . . . . . . . . . . . . . . . . . . . . .
85
4.4
Prior samples (left) and contour plot (right) of the joint distribution of σ and ξ
(truncated at 0), using the opinion of a ﬁctitious expert and the prior in Equation 4.5 85
4.5
Posterior samples (left) and contour plots (right) of the joint posterior distribution of
σ and ξ, using the opinion of a ﬁctitious expert for n=626 fraud losses in 41 banks,
recorded from 01/2007 to 04/2010 . . . . . . . . . . . . . . . . . . . . . . . . . .
86
4.6
Histograms of posterior samples, using the opinion of a ﬁctitious expert for n=626
fraud losses in 41 banks, recorded from 01/2007 to 04/2010 . . . . . . . . . . . . .
87
4.7
VaR0.99 and ES0.99 prior (left) and posterior (right) distribution, using the opinion
of a ﬁctitious expert for n=626 fraud losses in 41 banks, recorded from 01/2007 to
04/2010
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
88
4.8
Fraud data: VaR0.99 and ES0.99 prior (left) and posterior (right) distribution for
the linear opinion pool for two ﬁctitious experts with similar opinions and weights
w1 = w2 = 0.5 (10,000 runs-1,000 after thinning) . . . . . . . . . . . . . . . . . .
92
4.9
Fraud data: VaR0.99 and ES0.99 prior (left) and posterior (right) distribution for
the linear opinion pool for two ﬁctitious experts with diﬀerent opinions and weights
w1 = w2 = 0.5 (10,000 runs-1,000 after thinning) . . . . . . . . . . . . . . . . . .
93

List of Figures
xiv
4.10 Fraud data: Posterior distribution of VaR0.99 and ES0.99 for the linear opinion pool
for two ﬁctitious experts with very diﬀerent opinions and weights w1 = w2 = 0.5
(10,000 runs-1,000 after thinning) . . . . . . . . . . . . . . . . . . . . . . . . . .
94
4.11 VaR0.99 and ES0.99 posterior distribution (linear opinion pool for two ﬁctitious experts
with very diﬀerent opinions) for n=1000 simulated data from a Gamma(100,0.09) .
95
4.12 Prior (left) and posterior (right) distribution of VaR0.99, using the prior from Equa-
tion (4.13) and the linear opinion pool for two ﬁctitious experts with very diﬀerent
opinions. Expert 1 (black), expert 2 (red) and combined distribution (blue) . . . . .
97
4.13 Prior (left) and posterior (right) distribution of ES0.99, using the prior from Equa-
tion (4.13) and the linear opinion pool for two ﬁctitious experts with very diﬀerent
opinions. Expert 1 (black), expert 2 (red) and combined distribution (blue) . . . . .
97
4.14 Posterior distribution of VaR0.995 and ES0.995 for the ﬁrst set (top), second set (mid-
dle) and third set (bottom) of hyperparameters for n=1000 simulated data from a
Gamma(100,0.09), using the opinion of ﬁve experts (E1,E2,E3,E4,E5) with equal
weights . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
99
4.15 Posterior distribution of VaR0.995 and ES0.995 for diﬀerent experts in the ﬁrst year
(top), second year (middle) and third year (bottom). n=1000 simulated data from
a Gamma(100,0.09), using the opinion of ﬁve experts (E1,E2,E3,E4,E5) with equal
weights . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107
4.16 Fraud data: Posterior distribution of VaR0.99 and ES0.99 for diﬀerent experts in 2007 109
4.17 Fraud data: Posterior distribution of VaR0.99 and ES0.99 for diﬀerent experts in 2008 109
4.18 Fraud data: Posterior distribution of VaR0.99 and ES0.99 for diﬀerent experts in 2009 110
4.19 Fraud data: Posterior distribution of VaR0.99 and ES0.99 for diﬀerent experts in 2010 110
5.1
Fraud data: Histograms of data with cut-oﬀs at 500 (left) and 50 (right) . . . . . . 119
5.2
Fraud data: Fitted densities for the model with continuous ﬁrst derivative at the blend
point . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 120

List of Figures
xv
5.3
Fraud data: Posterior histograms for the model with continuous ﬁrst derivative at
the blend point. Top: α and β. Middle: u and σ Bottom: ξ . . . . . . . . . . . . . 121
5.4
Fraud data: Posterior histograms for ω = 1 . . . . . . . . . . . . . . . . . . . . . 122
5.5
Fraud data: Posterior histograms for ω varying . . . . . . . . . . . . . . . . . . . 122
5.6
Fraud data: Posterior histogram for ω in model (b) based on fB . . . . . . . . . . . 123
5.7
P-P plots for data using Bayesian estimates from ﬁtted models. Left panel is model
(a) (ω = 1), right panel is model (b) (ω varying)
. . . . . . . . . . . . . . . . . . 123
5.8
Fraud data: Posterior samples of the precision parameter for the ﬁrst Bayesian non-
parametric model. The blue line is a Gamma prior for ν in terms of frequencies . . 138
5.9
Fraud data: Histograms of the posterior MCMC estimates for the ﬁrst Bayesian
nonparametric model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 138
5.10 Fraud data: P-P plot using the posterior MCMC estimates from the ﬁrst Bayesian
nonparametric model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 139
5.11 Fraud data: Posterior samples of the precision parameter for the nonparametric ver-
sion of the model with discontinuous density with arbitrary scaling. The blue line is
a Gamma prior for ν in terms of frequencies
. . . . . . . . . . . . . . . . . . . . 139
5.12 Fraud data: Posterior parameter samples from the nonparametric version of the
model with discontinuous density with arbitrary scaling
. . . . . . . . . . . . . . . 140
5.13 Fraud data: P-P plot using the posterior MCMC estimates from the nonparametric
version of the model with discontinuous density with arbitrary scaling
. . . . . . . 140
6.1
Simulated mixture densities
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 149
6.2
Mixture of Student-t distributions (2,000 observations). . . . . . . . . . . . . . . . 151
D.1
Fraud data: σ = 1, 10, 100, respectively; u ∼TN(min (data)); ξ ∼Jeﬀreys prior
. . 175
D.2
Fraud data: σ ∼LN(0, 1.25); u = Q0.5 (19.89) , Q0.7 (43.77) , Q0.9 (124) , respec-
tively; ξ ∼Jeﬀreys prior . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 175
D.3
Fraud data: σ ∼LN(0, 1.25) ; u ∼TN(min (data)); ξ = −0.3, 0.05, 0.45, respectively 176

D.4
Exponential(0.1): σ = 1, 10, 100, respectively; u ∼TN(min (data)),ξ ∼Jeﬀreys prior 176
D.5
Exponential(0.1): σ ∼LN(0, 1.25); u = Q0.5 (6.89) , Q0.7 (11.35) , Q0.9 (23.03) , re-
spectively; ξ ∼Jeﬀreys prior . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 176
D.6
Exponential(0.1): σ ∼LN(0, 1.25) ; u ∼TN(min (data)); ξ = −0.3, 0.05, 0.45, re-
spectively . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 177
D.7
Log-Normal(0,1): σ = 1, 10, 100, respectively; u ∼TN(min (data)), ξ ∼Jeﬀreys prior 177
D.8
Log-Normal(0,1): σ ∼LN(0, 1.25); u = Q0.5 (1.009) , Q0.7 (1.69) , Q0.9 (3.49) , re-
spectively; ξ ∼Jeﬀreys prior . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 177
D.9
Log-Normal(0,1): σ ∼LN(0, 1.25) ; u ∼TN(min (data)); ξ = −0.3, 0.05, 0.45, re-
spectively . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 178
D.10 Gamma(2,0.5): σ = 1, 10, 100, respectively; u ∼TN(min (data)), ξ ∼Jeﬀreys prior
178
D.11 Gamma(2,0.5): σ ∼LN(0, 1.25); u = Q0.5 (3.4) , Q0.7 (4.85) , Q0.9 (7.91) , respec-
tively; ξ ∼Jeﬀreys prior . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 178
D.12 Gamma(2,0.5): σ ∼LN(0, 1.25) ; u ∼TN(min (data)); ξ = −0.3, 0.05, 0.45, respec-
tively . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 179
List of Tables
2.1
Historical operational risk losses in ﬁnancial institutions worldwide . . . . . . . . .
5
3.1
Posterior MCMC estimates for n=1000 simulated data from a Gamma-GPD mixture,
with ξ = −0.1
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
45
3.2
Posterior MCMC estimates for n=1000 simulated data from a Gamma-GPD mixture,
with ξ = 0.2
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
46
xvi

List of Tables
xvii
3.3
Descriptive statistics for n=626 fraud losses in 41 banks, recorded from 01/2007 to
04/2010
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
50
3.4
Posterior MCMC estimates for n=626 fraud losses in 41 banks, recorded from 01/2007
to 04/2010 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
50
3.5
Gelman-Rubin statistic for the MCMC posterior estimates for n=626 fraud losses in
41 banks, recorded from 01/2007 to 04/2010. (Scale reduction factor and its 97.5%
quantile) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
52
3.6
Eﬀective sample size for the MCMC posterior estimates for n=626 fraud losses in
41 banks, recorded from 01/2007 to 04/2010 . . . . . . . . . . . . . . . . . . . . .
53
3.7
Heidelberg and Welch diagnostic for the MCMC posterior estimates for n=626 fraud
losses in 41 banks, recorded from 01/2007 to 04/2010 . . . . . . . . . . . . . . . .
55
3.8
Minimum capital requirement (Millions of pesos) for fraud losses in 41 banks . . . .
56
3.9
VaR at diﬀerent levels (Millions of pesos) for fraud losses in 41 banks
. . . . . . .
57
3.10 Expected Shortfall at diﬀerent levels (Millions of pesos) for fraud losses in 41 banks
58
3.11 Posterior MCMC estimates for BAC data (n=75) . . . . . . . . . . . . . . . . . .
61
3.12 Posterior MCMC estimates for FIL data (n=50)
. . . . . . . . . . . . . . . . . .
63
3.13 Posterior MCMC estimates for G7 data (n=270) . . . . . . . . . . . . . . . . . .
63
3.14 Posterior MCMC estimates for MED data (n=231) . . . . . . . . . . . . . . . . .
63
3.15 Posterior RJMCMC estimates for BAC data (n=75) . . . . . . . . . . . . . . . .
67
3.16 Posterior RJMCMC estimates for FIL data (n=50) . . . . . . . . . . . . . . . . .
69
3.17 Posterior RJMCMC estimates for G7 data (n=270) . . . . . . . . . . . . . . . . .
69
3.18 Posterior RJMCMC estimates for MED data (n=231)
. . . . . . . . . . . . . . .
69
3.19 VaR at diﬀerent levels before and after RJMCMC-BAC data
. . . . . . . . . . . .
70
3.20 VaR at diﬀerent levels before and after RJMCMC-FIL data . . . . . . . . . . . . .
70
3.21 VaR at diﬀerent levels before and after RJMCMC-G7 data
. . . . . . . . . . . . .
70
3.22 VaR at diﬀerent levels before and after RJMCMC-MED data . . . . . . . . . . . .
71
3.23 Estimates of θ from the GSM procedure . . . . . . . . . . . . . . . . . . . . . . .
74

List of Tables
xviii
3.24 VaR at diﬀerent levels using the RJMCMC and GSM procedure-BAC data . . . . .
74
3.25 VaR at diﬀerent levels using the RJMCMC and GSM procedure-FIL data . . . . . .
75
4.1
Gamma parameters obtained from elicited quantiles (ﬁctitious expert) . . . . . . . .
83
4.2
Hyperparameters for a ﬁctitious expert . . . . . . . . . . . . . . . . . . . . . . . .
84
4.3
Fraud data: Prior MCMC estimates, using the opinion of a ﬁctitious expert and the
prior in Equation (4.5) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
84
4.4
Posterior MCMC estimates, using the opinion of a ﬁctitious expert for n=626 fraud
losses in 41 banks, recorded from 01/2007 to 04/2010 . . . . . . . . . . . . . . . .
84
4.5
Eﬀective Sample Size of prior samples, using the opinion of a ﬁctitious expert and
the prior in Equation (4.5) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
86
4.6
Heidelberg and Welch diagnostics for prior samples, using the opinion of a ﬁctitious
expert and the prior in Equation 4.5 . . . . . . . . . . . . . . . . . . . . . . . . .
87
4.7
Eﬀective Sample Size of posterior samples, using the opinion of a ﬁctitious expert
for n=626 fraud losses in 41 banks, recorded from 01/2007 to 04/2010 . . . . . . .
88
4.8
Heidelberg and Welch diagnostics for posterior samples, using the opinion of a ﬁcti-
tious expert for n=626 fraud losses in 41 banks, recorded from 01/2007 to 04/2010 .
88
4.9
Sets of hyperparameters for two ﬁctitious experts with similar, diﬀerent and very
diﬀerent opinions
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
91
4.10 Fraud data: Prior MCMC estimates for the linear opinion pool for two ﬁctitious
experts with similar opinions and weights w1 = w2 = 0.5 (10,000 runs-1,000 after
thinning) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
92
4.11 Fraud data: Posterior MCMC estimates for the linear opinion pool for two ﬁctitious
experts with similar opinions and weights w1 = w2 = 0.5 (10,000 runs-1,000 after
thinning) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
92
4.12 Fraud data: Prior MCMC estimates for the linear opinion pool for two ﬁctitious
experts with diﬀerent opinions and weights w1 = w2 = 0.5 (10,000 runs-1,000 after
thinning) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
93

List of Tables
xix
4.13 Fraud data: Posterior MCMC estimates for the linear opinion pool for two ﬁctitious
experts with diﬀerent opinions and weights w1 = w2 = 0.5 (10,000 runs-1,000 after
thinning))
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
93
4.14 Fraud data: Prior MCMC estimates for the linear opinion pool for two ﬁctitious
experts with very diﬀerent opinions and weights w1 = w2 = 0.5 (10,000 runs-1,000
after thinning) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
94
4.15 Fraud data: Posterior MCMC estimates for the linear opinion pool for two ﬁctitious
experts with very diﬀerent opinions and weights w1 = w2 = 0.5 (10,000 runs-1,000
after thinning) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
94
4.16 Set of hyperparameters for two experts with very diﬀerent opinions for n=1000 sim-
ulated data from a Gamma(100,0.09)
. . . . . . . . . . . . . . . . . . . . . . . .
95
4.17 VaR0.99 estimates and true value (linear opinion pool for two ﬁctitious experts with
very diﬀerent opinions) for n=1000 simulated data from a Gamma(100,0.09) . . . .
96
4.18 Sets of hyperparameters for ﬁve diﬀerent experts (E1,E2,E3,E4,E5) . . . . . . . . .
98
4.19 Parameter estimates of VaR0.995 for n=1000 simulated data from a Gamma(100,0.09),
using the opinion of ﬁve experts (E1,E2,E3,E4,E5) with equal weights (10,000 runs)
100
4.20 Parameter estimates of VaR0.995 for the ﬁrst subset of n=1000 simulated data from
a Gamma(100,0.09), using the opinion of ﬁve experts (E1,E2,E3,E4,E5) with equal
weights (10,000 runs)
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100
4.21 Parameter estimates of VaR0.995 for the second subset of n=1000 simulated data from
a Gamma(100,0.09), using the opinion of ﬁve experts (E1,E2,E3,E4,E5) with equal
weights (10,000 runs)
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100
4.22 Prior weights updating for diﬀerent sets of hyperparameters for n=1000 simulated
data from a Gamma(100,0.09), using the opinion of ﬁve experts (E1,E2,E3,E4,E5)
with equal weights
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 104

List of Tables
xx
4.23 Prior weights updating for diﬀerent sets of hyperparameters for the ﬁrst subset of
n=1000 simulated data from a Gamma(100,0.09), using the opinion of ﬁve experts
(E1,E2,E3,E4,E5) with equal weights
. . . . . . . . . . . . . . . . . . . . . . . . 104
4.24 Prior weights updating for diﬀerent sets of hyperparameters for the second subset of
n=1000 simulated data from a Gamma(100,0.09), using the opinion of ﬁve experts
(E1,E2,E3,E4,E5) with equal weights
. . . . . . . . . . . . . . . . . . . . . . . . 105
4.25 Prior distributions for diﬀerent years . . . . . . . . . . . . . . . . . . . . . . . 105
4.26 Prior weights for diﬀerent experts in a 3-year period. n=1000 simulated data from
a Gamma(100,0.09), using the opinion of ﬁve experts (E1,E2,E3,E4,E5) with equal
weights . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 106
4.27 Posterior distribution for diﬀerent experts in a 3- year period. n=1000 simulated
data from a Gamma(100,0.09), using the opinion of ﬁve experts (E1,E2,E3,E4,E5)
with equal weights
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 108
4.28 Prior weights for diﬀerent experts in a 3-year period. n=626 fraud losses in 41 banks,
recorded from 01/2007 to 04/2010 . . . . . . . . . . . . . . . . . . . . . . . . . . 108
4.29 Posterior distribution for diﬀerent experts in a 3- year period. n=626 fraud losses
in 41 banks, recorded from 01/2007 to 04/2010
. . . . . . . . . . . . . . . . . . . 111
6.1
Classic GPD estimation for the mixture of normals. . . . . . . . . . . . . . . . . . 150
6.2
Bayesian parameter estimates for the mixture of normals. . . . . . . . . . . . . . . 150
6.3
Classic GPD estimation for the Normal-Gamma mixture. . . . . . . . . . . . . . . 150
6.4
Bayesian parameter estimates for the Normal-Gamma mixture. . . . . . . . . . . . 150
6.5
Classic GPD estimation for the mixture of Student-t distributions (1,000 observations).151
6.6
Bayesian parameter estimates for the mixture of Student-t distributions (1,000 ob-
servations). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 151
6.7
Classic GPD estimation for the mixture of Student-t distributions (2,000 observations).152
6.8
Bayesian parameter estimates for the mixture of Student-t distributions (2,000 ob-
servations). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 152

List of Tables
xxi
6.9
Parameter estimates for a mixture of GPDs . . . . . . . . . . . . . . . . . . . . . 153
6.10 VaR for a mixture of GPDs, n = 150
. . . . . . . . . . . . . . . . . . . . . . . . 154
6.11 VaR for a mixture of GPDs, n = 500
. . . . . . . . . . . . . . . . . . . . . . . . 154
6.12 VaR for a mixture of GPDs, n = 1000 . . . . . . . . . . . . . . . . . . . . . . . . 154
6.13 VaR for a mixture of GPDs, n = 150
. . . . . . . . . . . . . . . . . . . . . . . . 154
6.14 VaR for a mixture of GPDs, n = 500
. . . . . . . . . . . . . . . . . . . . . . . . 155
6.15 VaR for a mixture of GPDs, n = 1000 . . . . . . . . . . . . . . . . . . . . . . . . 155
6.16 VaR for a mixture of GPDs, n = 150
. . . . . . . . . . . . . . . . . . . . . . . . 155
6.17 VaR for a mixture of GPDs, n = 500
. . . . . . . . . . . . . . . . . . . . . . . . 155
6.18 VaR for a mixture of GPDs, n = 1000 . . . . . . . . . . . . . . . . . . . . . . . . 156

Chapter 1
Introduction
Since its emergence, the study of operational risk has presented many issues: the combi-
nation of data sources from internal and external data and expert opinion; the elicitation
of information; and since rare events occur infrequently, small data sets. Extreme Value
Theory has emerged as a natural tool for quantifying operational risk. However, its ap-
plication involves several challenges, mainly the characterization of tail behaviour and
the inclusion of expert knowledge. To overcome these limitations, attention has turned
recently to Bayesian methods which can handle all these problems in a single framework.
In this thesis, we propose a Bayesian approach that provides an appropriate framework
to address the threshold1 selection issue and allows for the inclusion of multiple expert
opinion and external data, while taking into account the theoretical foundations of Extreme
Value Theory.
The thesis is organized as follows. Chapter 2 reviews relevant background and concepts
concerning operational risk, Extreme Value Theory and Bayesian inference. This part of
the thesis has the intention to give the reader a basis for what will be discussed in later
chapters. We present some historical losses that motivated the study of operational risk
and gave rise to the Basel II and Basel III Accords in 2004 and 2010, respectively. We
1The threshold is loosely deﬁned as the point above which losses are considered extremes and it is
chosen such that the population tail can be well approximated by an extreme value model.
1

Introduction
2
highlight the importance of these Accords, since they pave the way for the analysis of
operational risk in a formal fashion.
We then move on to a review of Extreme Value Theory. This section introduces the
foundations of Extreme Value Theory and its main results, such as the Fisher-Tippet-
Gnedenko and Pickands-Balkema-de Haan theorems. In the end, the theory and methods
presented in this section lead us to study the limitations of Extreme Value Theory, one
of the main concerns of this thesis. We focus our attention on the Peaks Over Threshold
(POT) method and the point processes approach. We ﬁnd that these approaches suﬀer
from two intrinsic limitations: Subjectivity about the threshold choice, and not accounting
for threshold uncertainty in inference.
Once these problems are clearly stated, our attention turns to the range of tools avail-
able, in particular Bayesian inference. We present the Bayesian framework and the mod-
elling strategy behind it: Formulate our beliefs in terms of the so-called prior distribution,
observe the data and update our beliefs via Bayes’ rule, giving place to the so-called
posterior distribution. We then examine this framework in the context of extremes and
emphasize its advantages compared to maximum likelihood estimation methods. In addi-
tion, we introduce some ideas about the elicitation process that will be studied in detail
in Chapter 4.
In Chapter 3, we concentrate on the model introduced by Behrens et al. (2004). This
model is based on a mixture distribution, which combines a parametric form for the cen-
ter and a Generalized Pareto Distribution (GPD) for the tail, using all observations for
inference about the unknown parameters from both distributions, the threshold included.
Next, we provide various ways of choosing prior distributions for the diﬀerent parame-
ters involved, while posterior inference is carried out through Markov Chain Monte Carlo
(MCMC) methods. In order to test the performance of this model, we apply this method-
ology to simulated data. To complete the analysis, we apply the proposed algorithm to
real data in Section 3.2, consisting of bank frauds from 2007 to 2010. We start by doing an

Introduction
3
exploratory analysis of the data to determine the presence of extremes, and then proceed
to perform the Bayesian analysis using an algorithm adapted to the context of operational
risk. Subsequently, we estimate the minimum capital requirement, the Value-at-Risk and
the Expected Shortfall, and present some interesting ﬁndings related to these risk mea-
sures. In the second part of our analysis, we consider grouped data. Banks are classiﬁed
according to their size and some other characteristics. The analysis is performed using a
Reversible Markov Chain Monte Carlo (RJMCMC) algorithm and a Bayesian approach
based on a mixture of Gamma distributions in which the mixing occurs over the shape
parameter.
The work described in Chapters 4 and 5 attempts to ﬁll some of the gaps in inference
for extremes and to improve the Bayesian methodology presented previously. Chapter
4 is dedicated to prior analysis. We address the problem of prior sensitivity, prior elic-
itation and multiple expert opinion. In the ﬁrst part, we study the diﬀerence of using
non-informative and informative priors for extreme data. After that, we introduce an in-
formative prior that captures expert opinion in an intuitive way, making use of quantities
that experts are familiar with, and that are realistic in the context of our problem. Lastly,
we explore multiple expert opinion and propose a methodology for combining multiple
opinions and for updating our beliefs when new information becomes available.
Finally, in Chapter 5, we introduce a new model that we call the Blended model. This
model is constructed using Gamma and Generalized Pareto elements and has the objective
of improving the discontinuity problem in the model of Behrens et al. (2004). We consider
diﬀerent approaches according to the type of discontinuity, for instance the qth quantile or
the ﬁrst derivative at the blend point. In order to complete the analysis, we also consider
nonparametric models and propose two diﬀerent ways to construct them: (1) Represent
the GPD as a mixture of Exponential distributions and use a Dirichlet process mixture
formulation and (2) use a Dirichlet process prior on the parameters of the GPD model.
We conclude this thesis with a discussion of our results in Chapter 6 .

Chapter 2
Extreme Value Theory in operational
risk
2.1
Operational risk
2.1.1
Introduction to operational risk
During the 1980s, several catastrophic losses ocurred in ﬁnancial institutions worldwide,
giving rise to a ﬁrst international cooperation agreement known as Basel I Accord, issued
in 1988, that sets minimum capital requirements for banks to hedge their risk exposure,
particularly credit risk.
Faced with a changing environment, Basel I had limitations, so that in 2004 the Basel
Committee1 stated the New Capital Accord “Basel II”, which aims to strengthen the
soundness and stability of the international banking system, with more risk-sensitive cap-
ital requirements.
Basel II incorporates operational risk to the already considered credit and market risks,
highlighting the importance of allocating capital for operational losses. Table 2.1 shows
1The Basel Committee on Banking Supervision is a committee of banking supervisory authorities which
was established by the central bank Governors of the Group of Ten countries in 1975. Its recommendations
have become the standard and guidelines in banking supervision and regulation in the rest of the world.
4

2.1.
Operational risk
5
Year
Institution
Estimated
Losses
(Millions of
USD)
Event
1995
Barings Bank
$1,300
In Singapore, a trader accumulated
not reported losses for two years.
1995
Dalwa Bank
$1,100
For eleven years, a trader
accumulated not reported losses.
1996
Morgan
Grenfell
$640
For two years, a fund manager
invested in shares of a “junk
company”.
1997
Natwest
Bank
$145
Incorrect valuation of options.
2002
All First
$691
For three years, losses in foreign
exchange trading were hidden.
2008
Soci´et´e
G´en´erale
$7,000
A broker hid losses from
unauthorized transactions in the
futures market.
Table 2.1: Historical operational risk losses in ﬁnancial institutions worldwide
some recent losses that motivated the study of operational risk.
A new Accord was introduced in 2010 in response to the deﬁciencies in ﬁnancial regu-
lation revealed by the late-2000s ﬁnancial crisis. The Basel III Accord is supposed to
strengthen bank capital requirements by increasing bank liquidity and bank leverage.
However, the new Accord does not incorporate relevant changes regarding operational
risk. Hence, we will focus on the Basel II Accord.
2.1.2
Basel II and operational risk
The Basel II Capital Accord of 2004 2 deﬁnes operational risk as “the risk of loss resulting
from inadequate or failed internal processes, people and systems, or from external events”.
Risk events are classiﬁed into seven categories, namely:
 Internal fraud
2To record losses, institutions should: classify units and business lines, identify and classify the diﬀerent
types of loss events, and keep a historical database containing records of losses and their cost.

2.1.
Operational risk
6
 External fraud
 Employment practices and workplace safety
 Clients, products, and business practice
 Damage to physical assets
 Business disruption and systems failures
 Execution, delivery and process management
The Basel II Accord also sets capital requirements to face operational losses and established
2007 as the year by which ﬁnancial institutions should have implemented the calculation
of capital requirements for operational risk.
The Accord provides three methods for calculating capital requirements for operational
risk, which are presented in order of sophistication and risk sensitivity:
1. The Basic Indicator Approach. Capital is allocated using the gross income as a proxy
for an institution’s overall operational risk exposure, with each bank holding capital
for operational risk equal to the amount of a ﬁxed percentage α (15%), multiplied
by its individual amount of positive gross income. Assume that in n out of the
last 3 years the gross income was positive. Relabeling these years as 1, ..., n, the
corresponding gross income is GI1, ..., GIn. The resulting capital charge KBIA under
the Basic Indicator approach is given as follows.
KBIA =
nP
j=1
GIj × α
n
.
(2.1)
This approach is simple and easy to implement, however, it does not consider speciﬁc
needs and characteristics of each bank.

2.1.
Operational risk
7
2. The Standardised Approach. Capital is determined based on predeﬁned percentages
of the average gross income of the last three years for eight business lines 3. After
determining the capital required for each business line, the aggregate is the total
capital requirement. This approach diﬀers from the Basic Indicator Approach in
that bank’s activities are divided into a number of standardised business units and
business lines.
3. Advanced Measurement Approach (AMA): The regulatory capital is determined us-
ing models developed by each institution. It considers two approaches: the internal
measurement and the loss distribution approach (LDA). Under the LDA, banks use
their internal data, estimate the frequency and severity distributions of operational
risk events and, based on these two distributions, compute the probability distribu-
tion function of the cumulative operational loss. The capital charge is usually based
on the Value-at-Risk measure (VaR), which is deﬁned in the next section, Equation
(2.3).
2.1.2.1
Implementation
In 2010, the Financial Stability Institute (FSI) carried out a survey on Basel II implemen-
tation. The survey results indicate that 112 countries have implemented or are currently
planning to implement Basel II.
For operational risk the survey indicated that 80% of respondents that have adopted
the Accord expected to adopt the Basic Indicator Approach.
3Banks’ activities are divided into eight business lines by considering that gross income is a broad
indicator that serves as a proxy for the scale of business operations and thus the likely scale of operational
risk exposure within each of these business lines: corporate ﬁnance, trading & sales, retail banking,
commercial banking, payment & settlement, agency services, asset management, and retail brokerage.

2.1.
Operational risk
8
2.1.3
Loss Distribution Approach (LDA)
The Loss Distribution Approach (LDA) is a statistical approach which is very popular in
actuarial sciences for computing aggregate loss distributions. Under this approach, banks
adjust statistical distributions to loss data by modelling: i) the frequency of loss events
and ii) their severity, then combining them to obtain the distribution of total losses.
The loss distribution is modelled as follows:
Zt =
J
X
j=1
Z(j)
t ;
Z(j)
t
=
N(j)
t
X
i=1
X(j)
i
(t) ,
(2.2)
where
 t = 1, 2, ... is discrete time in annual units.
 Z(j)
t
is the annual loss in risk cell j, modelled as an aggregate loss over one year,
with frequency N (j)
t .
 N (j)
t is a counting process and for each t, X(j)
i
(t) are positive random variables rep-
resenting severities, with i = 1, ..., N (j)
t .
Under this model, the capital is deﬁned as the Value-at-Risk at the 99.9% level quantile
of the distribution for the next year annual loss Zt+1
VaRq (Zt+1) = inf {z ∈R : Pr (Zt+1 > z) ≤1 −q}
(2.3)
at level q = 0.999.
2.1.4
Operational risk data
In the General Standards for Advanced Measurement Approaches, the Basel II Accord
speciﬁes that internal data, external data and expert opinion data should be considered
into the analysis. In addition, internal control indicators and factors aﬀecting the busi-
nesses should be used.
Operational risk data should meet speciﬁc criteria:

2.1.
Operational risk
9
 Internal data. Internal measures must be based on a minimum ﬁve-year observation
period of internal loss data, or three years when the bank ﬁrst moves to the AMA.
Loss data must capture all material activities and exposures from all appropriate
sub-systems and geographic locations and the collected information should include
the date of the risk event, any recoveries of gross loss amounts, as well as some
descriptive information about its causes.
 External data: Banks must use relevant external data (either public data and/or
pooled industry data), especially when the bank is exposed to infrequent, yet po-
tentially severe, losses. These data should include actual loss amounts, information
on the scale of business operations where the event occurred and information on the
causes and circumstances of the loss events. External data are diﬃcult to use due
to diﬀerent volumes and other factors.
 Expert opinion: A bank must use scenario analysis of expert opinion in conjunction
with external data to evaluate its exposure to high-severity events. These expert
assessments could be expressed as parameters of a statistical loss distribution.
2.1.5
Bank fraud
Any activity related to money entails the risk of fraud, however, in the ﬁnancial sector,
particularly banking, the exposure is higher. “A fraud is an action to achieve a proﬁt or
gain something illegally through deception or exploitation of a mistake made by others.
Deception occurs when an individual displays a series of machinations and artiﬁces in
order to make one or more people have a false perception of reality to obtain goods or
property rights of others” 4.
In some countries, most of banking fraud losses correspond to credit and debit cards.
However, they also occur with other means of payment such as cheques and Internet
transfers.
4Source: Condusef http://www.condusef.gob.mx

2.1.
Operational risk
10
2.1.6
Challenges in operational risk
As it is said in McNeil (2005), “nobody doubts the importance of operational risk for the
ﬁnancial and insurance sector, but much less agreement exists on how to measure this
risk”. Undoubtedly, the study of operational risk is accompanied by several disadvantages:
Possible inconsistencies in its deﬁnition, data gaps and limitations to allocate capital for it.
As a result, there have been several criticisms of Basel II on operational risk, which
shows diﬀerences with respect to market and credit risks: It is more linked to process rather
than product, and hence operational risk does not always arise through transactions; thus,
most of the time it is not reported in the income statement. Furthermore, operational
risk cannot always be reduced by diversiﬁcation, or objectively assigned to a particular
business line, so it is assumed inevitably as part of the company’s business rather than
pursuit of proﬁt.
Additionally, there are several criticisms of the methods of measurement, mainly the
questionable choice of the gross income as an indicator of risk exposure, since there may
be institutions with high income and low risk, and institutions with lower income but
high risk, however, the basic approach will require more capital to the ﬁrst group. Some
additional criticisms are the linear relationship between the indicator and risk, together
with the coexistence of methodologies in which all parameters are predeﬁned.
Because of these criticisms, in recent years the academic community has placed special
emphasis on advanced measurement methods, by considering the loss distribution approach
and providing a theoretical basis to implement more advanced methods.
It should be noticed that beyond the regulation for operational risk, the losses as-
sociated with such events may aﬀect the results of the institutions and, therefore, their
capitalization levels, so that the measurement of this risk should not be ignored.

2.2.
Extreme Value Theory
11
2.2
Extreme Value Theory
Extreme Value Theory (EVT) is used to model and measure tail events that occur with
small probability. Its main utility is that it is a quantitative method that takes into account
the frequency and severity of losses.
Over the past two decades, EVT has had an important development and has been
recognized as an extremely useful statistical tool for modelling“rare events”. It has relevant
applications in insurance, ﬁnance, risk management and meteorology, among others.
2.2.1
Basic deﬁnitions and results
Most of the results presented in this section can be found in McNeil et al. (2005, Chapter
7); proofs are given in Embrechts et al. (1997, Chapter 3).
2.2.1.1
Generalized Extreme Value Distribution
Theorem 2.1. (Fisher-Tippett). Let Xn be a sequence of iid random variables and Mn =
max (X1, ..., Xn) be the maximum of the ﬁrst n members of the sequence. If there exist
norming constants cn > 0, dn ∈R and some nondegenerate cdf H such that
Mn −dn
cn
d⇒H,
(2.4)
then H belongs to the type of one of the following cdfs:















































Type I (Gumbel):
Type II (Fr´echet):
Type III (Weibull) :
H(x) = exp (−e−x)
x ∈R
H(x) =







0
x ≤0
exp (−x−α)
x > 0
α > 0
H(x) =







exp (−(−x)α)
x ≤0
1
x > 0
α > 0
(2.5)

2.2.
Extreme Value Theory
12
where
d⇒denotes convergence in distribution.
Deﬁnition 2.2. (maximum domain of attraction). If (2.4) holds for some nondegenerate
cdf H and sequences of constants cn and dn, then F is said to be in the maximum domain
of attraction of H, written F ∈MDA(H).
The deﬁnition below provides a uniﬁed parameterization for the class of limit distribu-
tions: Gumbel, Fr´echet and Weibull.
Deﬁnition 2.3. (the generalized extreme value (GEV) distribution). The distribution
function of the GEV distribution is given by
Hξ,µ,β (x) =







exp

−

1 + ξ x−µ
β
−1
ξ 
,
ξ ̸= 0
exp
h
−exp

1 −x−µ
β
i
,
ξ = 0
(2.6)
for 1 + ξ(x −µ)/σ > 0, where µ ∈R is the location parameter, β > 0 the scale parameter
and ξ = 1/α ∈R the shape parameter.
The parameter ξ in the above Deﬁnition deﬁnes the type of distribution: when ξ > 0
the distribution is a Fr´echet distribution; when ξ = 0 it is a Gumbel distribution; when
ξ < 0 it is a Weibull distribution.
2.2.1.2
Threshold Exceedances
The Generalized Pareto Distribution (GPD) is used to model the tails of distributions
based on theoretical arguments. Usually, it is expressed through a distribution function
that depends on two parameters.
Deﬁnition 2.4. The cdf of the GPD is given by
Gξ,σ (x) =







1 −
 1 + ξ x
σ
−1
ξ ,
ξ ̸= 0
1 −exp
 −x
σ

,
ξ = 0
(2.7)

2.2.
Extreme Value Theory
13
for x ≥0 for ξ ≥0 and for 0 ≤x ≤−σ/ξ for ξ < 0. In (2.7), σ > 0 and ξ ∈R are the
scale and shape parameters, respectively.
When ξ > 0, the Generalized Pareto distribution is a reparameterized version of the
ordinary Pareto distribution with parameter α = 1/ξ and κ = σ/ξ; if ξ < 0, we have a
Pareto type II distribution; if ξ = 0 we have the exponential distribution.
Deﬁnition 2.5. The right endpoint of a distribution is deﬁned as
xF = sup {x ∈R : F (x) < 1} .
Deﬁnition 2.6. (excess distribution over threshold u). Let X be a rv with cdf F. The
excess distribution over the threshold u is given by
Fu (x) = P (X −u ≤x | X > u) = F (x + u) −F (u)
1 −F (u)
,
(2.8)
for 0 < x < xF −u, where xF ≤∞is the right endpoint of F.
Deﬁnition 2.7. (mean excess function). The mean excess function of a random variable
X with ﬁnite mean is given by e (u) = E (X −u | X > u).
Theorem 2.8. (Pickands-Balkema-de Haan). Let F be a distribution function with end-
point xF. We can ﬁnd a positive measurable function σ (u) such that:
lim
u→xF
sup
0≤x<xF −u
Fu(x) −Gξ,σ(u) (x)
 = 0
(2.9)
if and only if F ∈MDA (Hξ) , ξ ∈R.
Hence, for high thresholds, the excess distribution function can be approximated by
Gξ,σ(u) (x) for some values of ξ and σ. Thus, we may try to ﬁt the generalized Pareto
distribution to data which exceed high thresholds.

2.2.
Extreme Value Theory
14
2.2.2
The Peaks Over the Threshold and the Point Process
Approach
There are two inference methodologies related to Extreme Value Theory: Block Maxima
(BM) and Peaks Over the Threshold (POT). The ﬁrst technique consists of ﬁrst dividing
identically distributed observations into block of equal size and modelling the maxima of
each block. On the other hand, the POT method considers models for all observations
that exceed some high level.
Both techniques are based on limit results: The Block
Maxima method uses the Fisher–Tippett Theorem 2.1, while the POT approach uses the
Pickands–Balkema–de Haan Theorem 2.8.
POT models are generally considered to be the most useful for practical applications,
due to their more eﬃcient use of the data on extreme outcomes. Moreover, these models
provide insights into excess distributions over high thresholds, which are of particular
interest in operational risk. Hence, we will focus on the POT method throughout the
thesis.
For POT models, there are two types of analysis: Semiparametric models based on
the Hill estimator, and complete parametric models based on the Generalized Pareto
distribution (GPD). When used correctly, both methods are theoretically and empirically
justiﬁed. However, the approach based on the Hill estimator has limited applications since
the heavy-tailedness assumption is necessary, as its focus is on the case ξ> 0; while GPD-
based models are applicable to any type of distribution, heavy tailed or not, provided it
is in the MDA of Hξ.
Under this last approach, the exceedances, i.e. observations that are larger than a
threshold, are considered as a point process of exceedances, which converges weakly to
a Poisson point process that allows for inference on the intensity of occurrence of such
exceedances. On the other hand, the Generalized Pareto distribution provides a model for
the excesses over an appropriate threshold, i.e. the magnitudes of the diﬀerences of the
exceedances and the threshold.

2.2.
Extreme Value Theory
15
The POT method is described in McNeil et al.(2005) as follows: Given loss data
X1, ..., Xn from F, a random number Nu will exceed our threshold u; it will be conve-
nient to relabel these data ˜X1, ..., ˜XNu. For each of these exceedances we calculate the
amount Yj = ˜Xj −u of the excess loss. We wish to estimate the parameters of a GPD
model by ﬁtting this distribution to the Nu excess losses. According to Ribatet (2006),
there are currently seventeen estimators available to ﬁt a Generalized Pareto distribution.
Among the most important are: the method of moments, maximum likelihood, proba-
bility weighted moments (biased and unbiased), median, Pickands, penalized maximum
likelihood and moment generating function estimators. The maximum likelihood method
is more commonly used and is easy to implement if the excess data can be assumed to be
realizations of independent rvs, since the joint density will then be a product of marginal
GPD densities.
Denoting gξ,σ the density of the GPD, the log-likelihood may be easily calculated to be
ln L (ξ, σ; Y1, ..., YNu) =
Nu
X
j=1
ln gξ,σ (Yj) = −Nuln σ −

1 + 1
ξ

Nu
X
j=1
ln

1 + ξYj
σ

, (2.10)
which must be maximized subject to the parameter constraints σ > 0 and 1 + ξYj/σ > 0
for all j.
Solving the maximization problem yields a GPD model Gˆξ,ˆσ for the excess
distribution Fu. If ξ > −0.5, the maximum likelihood estimators of ξ and σ are regular
(Smith, 1985).
Next, note that F ∈MDA (Hξ) if and only if there exist sequences of constants cn > 0,
dn ∈R such that for all x ∈R,
lim
n→∞n · (1 −F (cnx + dn)) = lim
n→∞n · ¯F (cnx + dn) = −ln Hξ (x) ,
(2.11)
where the limit is interpreted as ∞if Hξ (x) = 0; see Embrechts et al. (1997, Proposition
3.3.2). Assuming that F ∈MDA (Hξ) and setting un (x) = cnx + dn we thus have
lim
n→∞n · ¯F (un (x)) = λ (x) ,
(2.12)

2.2.
Extreme Value Theory
16
where λ (x) = −ln Hξ (x). Deﬁning the point process of exceedances as
Nn (A) =
n
X
i=1
I
 I(Xi>un(x)) · (i/n) ∈A

(2.13)
for any Borel set A ⊆(0, 1], it can be shown (Embrechts et al.(1997), Theorem 5.3.2) that
if λ (x) ∈(0, ∞), Nn converges weakly to a homogeneous Poisson point process on (0, 1]
with intensity λ (x).
Now assume that X1, ..., Xn are iid observations from F ∈MDA (Hξ) and that u is a
high threshold of the form u = cny + dn for some y ∈R. We assume that the process of
exceedances above u is well approximated by a Poisson point process with intensity
λ = −ln Hξ (y) = −ln Hξ
y −dn
cn

.
(2.14)
Since dn and cn are unknown, we can replace them by the location and scaling parameters
µ and β, respectively. This gives
λ = −ln Hξ
u −µ
β

= −ln Hξ,µ,β =

1 + ξu −µ
β
−1/ξ−1
(2.15)
provided 1 + ξ(u −µ)/β > 0, and by λ = 0 otherwise.
To take the sizes of the exceedances into account, this model can be extended to a
(non-homogeneous) two-dimensional Poisson point process where the points (t, x) in the
two-dimensional space X = (0, 1]×(u, ∞) are record times and magnitudes of exceedances.
For a set of the form A = (t1, t2) × (x, ∞) ⊂X, the intensity measure is
Λ (A) =
ˆ t2
t1
ˆ ∞
x
λ (y) dy dt = −(t2 −t1) ln Hξ,µ,σ (x) .
(2.16)
It follows from (2.16) that for any x ≥u the implied one-dimensional process of ex-
ceedances of the level x is a homogeneous Poisson process with rate τ(x) := −ln Hξ,µ,β(x).
Now consider the excess amounts over the threshold u. The tail of the excess distribution
function over the threshold u, denoted ¯Fu(x) before, can be calculated as the ratio of the
rates of exceeding the levels u + x and u. We obtain
¯Fu(x) = τ (u + x)
τ (u)
=

1 +
ξx
β + ξ (u −µ)
−1/ξ
= ¯Gξ,σ (x)
(2.17)

2.2.
Extreme Value Theory
17
for a positive scaling parameter σ = β + ξ(u −µ). This is precisely the tail of the GPD
model for excesses over the threshold u used previously.
2.2.3
Threshold selection
Most of the methods involved in choosing the threshold, u, make use of the Pickands-
Balkema-de Haan Theorem 2.8. In general, we face two conﬂicting issues. If the threshold
is chosen too low it is possible to get biased estimates because the theorem does not apply.
On the other hand, if the threshold is set too high then only few data points will be
available and estimates will be prone to high standard errors. We therefore reduce bias by
lowering the number of observations in the tail and reduce variance by increasing it. This
is known as the bias-variance tradeoﬀ.
The main objective is to choose a threshold so that enough events are selected to reduce
the variance, without inducing bias. The theory does not propose any objective method
for threshold determination; there are mainly graphical ad-hoc approaches. Some other
approaches include setting the threshold as some high percentile of the data; for instance,
DuMouchel (1983) suggests ﬁtting a Generalized Pareto model to the data outside the
10th and 90th percentiles. Here, we concentrate on the most commonly used graphical
methods.
2.2.3.1
Threshold choice plot
Through this graph, we analyze the stability of the model estimation based on the ﬁt of
diﬀerent models, using thresholds in a given range.
Let X −u0 | X > u0 ∼GPDξ0,σ0. Let u1 be any other threshold such that u1 > u0.
The random variable X −u1 | X > u1 is also GPD with updated parameters σ1 =
σ0 + ξ0 (u1 −u0) and ξ1 = ξ0 (Lemma 7.22, McNeil et al. 2005). Setting
σ∗= σ1 −ξ1u1,
(2.18)

2.2.
Extreme Value Theory
18
Figure 2.1: Threshold choice plot with threshold around 0.98
σ∗is independent of u1. Thus, the estimates σ∗and ξ1 are constant for every u1 > u0 if
u0 is an appropriate threshold for the asymptotic approximation.
The threshold choice plot represents the points deﬁned by:
{(u1, σ∗) : u1 ≤xmax}
and
{(u1, ξ1) : u1 ≤xmax}
(2.19)
where xmax is the maximum of the observations. Thus, we select the threshold at the point
where estimates remain roughly constant.
Figure 2.1 shows an example of the threshold choice plot for simulated uniform data,
where a threshold around 0.98 is a reasonable choice.
2.2.3.2
Mean residual life plot
This plot is also called the mean excess plot and it is based on the theoretical mean of a
generalized Pareto distribution. If X is GPDξ,σ, then, provided ξ < 1,
E [X] =
σ
1 −ξ.
(2.20)

2.2.
Extreme Value Theory
19
When ξ ≥1 the theoretical mean of X is inﬁnite. Moreover, for any u > 0, the excess
distribution function of X is easily calculated to be
Fu (x) = Gξ,σ(u),
σ (u) = σ + ξu.
(2.21)
In particular, by equation (2.20), the mean excess function of X introduced in Deﬁnition
2.7 is then
e (u) = σ (u)
1 −ξ = σ + ξu
1 −ξ ,
(2.22)
provided ξ < 1, for 0 ≤u < ∞if 0 ≤ξ < 1 and 0 ≤u ≤−σ/ξ if ξ < 0. It may
be observed that the mean excess function is thus linear in u, which is a characterizing
property of the GPD.
In practice, if X represents the excess over a threshold u0 and if a GPD approximation
above that threshold is good enough, the excess distribution over a higher threshold u > u0
is again GPD with the same parameter ξ and scale parameter σ + ξ(u −u0) (Lemma 7.22,
McNeil et al. 2005). Therefore, by equation (2.20) we have:
e (u) = σ + ξ (u −u0)
1 −ξ
=
ξu
1 −ξ + σ −ξu0
1 −ξ ,
(2.23)
where u0 ≤u < ∞if 0 ≤ξ < 1 and u0 ≤u ≤u0 −σ/ξ if ξ < 0.
The mean e(u) is thus linear in u and can easily be estimated using the empirical
sample mean.
This fact is commonly used as a diagnostic for data admitting a GPD
model for the excess distribution.
Given a sample X1, ..., Xn, the estimator is given by
en (u) =
nP
i=1
(Xi −u) I{Xi>u}
nP
i=1
I{Xi>u}
,
(2.24)
where I{·} is the indicator function and the sum in the denominator represents the number
of observations over the threshold u.
Thus, the mean residual life plot represents points deﬁned by:
{(Xi,n, en (Xi,n)) : 2 ≤i ≤n}
(2.25)

2.2.
Extreme Value Theory
20
Figure 2.2: Mean residual life plot with threshold around 2.5
where Xi,n denotes the i−th order statistic.
If the GPD model is appropriate for data exceeding a high threshold, (2.23) suggests
that the mean excess plot should become increasingly “linear” for higher values of u. In
general, a linear upward trend indicates a GPD model with positive shape parameter ξ;
a plot tending towards the horizontal indicates a GPD with approximately zero shape
parameter; a linear downward trend indicates a GPD with negative shape parameter.
An example of the mean excess plot for simulated standard normal data is displayed
in Figure 2.2. The threshold is selected to be around 2.5.
2.2.3.3
Dispersion index plot
Let X be a random variable with Poisson distribution with parameter λ. Then
P [X = k] = e−λλk
k!
,
k ∈N
(2.26)
and E [X] = Var [X]. If the number of events follows a Poisson distribution, the ratio of
the variance to the mean equals 1.

2.2.
Extreme Value Theory
21
Figure 2.3: Dispersion index plot with threshold around 5
Under suitable conditions (i.e. F ∈MDA(H)) the exceedances over a threshold can
be approximated by a GPD. Moreover, EVT also shows that the occurrences of these
exceedances may be represented as a homogeneous Poisson point process. In particular,
if the Poisson point process approximation above a high threshold u is valid, the number
of exceedances above u in disjoint blocks of equal size constitutes an approximately i.i.d.
sample from the Poisson distribution. The so-called dispersion index DI, which is the ratio
of the sample variance to the sample mean, viz.
DI(u) = s2
λ
(2.27)
should thus be close to 1. The dispersion index plot constitutes of points
{(Xi,n, DI (Xi,n)) : 2 ≤i ≤n}
(2.28)
and we select a threshold at the point where the plot becomes close to 1. Figure 2.3 displays
the dispersion index plot for the data set ardieres included in the R POT package, where
a threshold around 5 seems to be reasonable.

2.2.
Extreme Value Theory
22
2.2.4
Measuring operational risk
The POT model can be used to quantify operational risk. We will assume that the chosen
threshold u satisﬁes a bias-variance tradeoﬀand that such u may be termed an unexpected
loss threshold. Following the procedure described by Medova et al. (2002), we have:
 The severity of the losses is modelled by the Generalized Pareto distribution. The
expectation of the excess loss distribution, i.e., the expected severity is a coherent5
risk measure given by:
E (X −u | X > u) = σ + ξu
1 −ξ .
(2.29)
 The number of exceedances Nu over the threshold u and the corresponding ex-
ceedance times follow a homogeneous Poisson point process with intensity given
by:
λ (u) :=

1 + ξu −µ
β
−1
ξ
.
(2.30)
 The extra capital provision for operational risk over the unexpected loss threshold u
is estimated as the expectation of the excess loss distribution, scaled by the intensity
λ (u) of the Poisson process:
λ (u) E (X −u | X > u) = λ (u) σ + ξu
1 −ξ ,
(2.31)
where u, σ, ξ and λ are the POT model parameters and time is measured in the same
units as the frequency of data collection (years, months, days, etc.).
 The total amount of capital provided against extreme operational risks for a time
period of length T will be calculated by:
uT + λ (u) T E (X −u | X > u) = uT + λ (u) T σ + ξu
1 −ξ
(2.32)
5A risk measure R (·) is coherent if it satisﬁes:
1. Monotonicity: If X ≤Y ⇒R (X) ≤R (Y ).
2. Positive homogeneity: For all λ ≥0 R (λX) = λR (X).
3. Translation invariance: For all κ ∈R, R (X + κ)=R (X) + κ.
4. Sub-additivity: R (X + Y ) ≤R (X) + R (Y ).

2.2.
Extreme Value Theory
23
where uT may in the ﬁrst instance be considered equal to u, under the assumption
of max-stability6.
Assuming that the excess distribution above u is GPD, we have that:
¯F (x) = P (X > u) P (X > x | X > u)
= ¯F (u) P (X −u > x −u | X > u)
= ¯F (u) ¯Fu (x −u)
= ¯F (u)

1 + ξx −u
σ
−1/ξ
.
(2.33)
This formula may be inverted to obtain a high quantile of the underlying distribution,
which we interpret as a Value-at-Risk (VaR). For q ≥F (u) we have that VaR at level q
is equal to
VaRq = u + σ
ξ
 1 −q
¯F (u)
−ξ
−1
!
.
(2.34)
When VaR is exceeded, the actual loss can be much higher than VaR. To employ a
coherent risk measure, we consider the Expected Shortfall or conditional VaR (CVaR), i.e.
the expected loss once the VaR is exceeded. This measure is given by:
ESq = E (X | X > VaRq) =
1
1 −q
ˆ 1
q
VaRx dx.
(2.35)
Assuming the GPD approximation above u, the Expected Shortfall can be calculated using
(2.20) as
ESq = VaRq + E (X −VaRq | X > VaRq) = VaRq
1 −ξ + σ −ξu
1 −ξ .
(2.36)
We can obtain estimates of both VaR and ES. Replacing ¯F (u) by its empirical estimate
Nu/n in (2.34), and replacing ξ and σ by their estimates, we get:
d
VaRq = u + ˆσ
ˆξ
 n (1 −q)
Nu
−ˆξ
−1
!
(2.37)
6Max-stability means that if X1, ..., Xd are iid copies of X,
max (X1, ..., Xd)
d= X −ad
bd

2.2.
Extreme Value Theory
24
and
c
ESq =
d
VaRq
1 −ˆξ
+ ˆσ −ˆξu
1 −ˆξ
.
(2.38)
2.2.5
Limitations of Extreme Value Theory
Extreme Value Theory (EVT) models are based upon an asymptotic approximation for
the tail distribution, which are very ﬂexible in terms of the allowable tail shape behaviour.
The attraction of EVT based methods is that they can provide mathematically and statis-
tically justiﬁable parametric models for the tails of any distribution which can give reliable
extrapolations beyond the range of the observed data.
The Pickands-Balkema-de Haan theorem states that for suﬃciently large values of the
threshold, under certain regularity conditions, the generalized Pareto distribution (GPD)
is the limit distribution of exceedances. However, one of the main issues in applying the
classical GPD approach is the threshold selection (i.e. at which level of extremity into the
tails of the data the GPD is a good model).
The threshold selection is a balance between reliability of the asymptotic approximation
versus the sample variance of estimators. The threshold must be suﬃciently high to ensure
the threshold excesses are approximately GPD. However, the threshold cannot be too high
as this will reduce the sample size available for inference and thus increase variability of
the estimators.
As we saw in the previous sections, traditionally, data analysis with such models is
performed in two steps. In the ﬁrst one, the threshold u, is chosen graphically by looking
at the mean excess plot (or some other graphical tools as described in Section 2.2.3) or
simply setting it at some high percentile of the data. Once a suitable value has been
determined, the threshold is then treated as a known ﬁxed constant in later inference
and the remaining parameters are estimated. This approach suﬀers from concerns over
subjectivity about the threshold choice and not accounting for threshold uncertainty in

2.3.
Bayesian inference in Extreme Value Theory
25
inference. Moreover, only the observations above the threshold are used in the second
step.
Threshold selection is by no means an easy task, as observed by Davison and Smith
(1990) and Coles and Tawn (1994), among others. Choosing the threshold through a mean
excess plot or by choosing a certain percentile does not guarantee that an appropriate
selection was made in order to prevent model bias which is crucial for the use of the
asymptotic distribution as a model.
Most of the literature has shown how threshold
selection inﬂuences parameter estimation (see Smith (1987) and Coles and Tawn (1996)).
Although some approaches have been developed to deal with these issues, the problem
remains.
Finally, for many applications, threshold selection may be critical for the extrapolated
tail behaviour, so the extra uncertainty associated with the threshold choice needs to be
accounted for. In this setting, Bayesian inference oﬀers an alternative framework which
allows to overcome this uncertainty.
2.3
Bayesian inference in Extreme Value Theory
Bayesian inference is a powerful and increasingly popular statistical approach, which allows
one to deal with complex problems in a conceptually simple and uniﬁed way. In Bayesian
inference, parameters are random variables. Uncertainty or degree of belief with respect
to the parameters is quantiﬁed by probability distributions. The basic idea of Bayesian
inference is to set up a full probability model for both observed and unobserved quantities.
Inference is then based on the so-called posterior density, i.e. the conditional density of
the unobserved quantity conditional on the observed quantity. Additionally, it can be used
to incorporate expert opinions into data analysis and to combine diﬀerent data sources.

2.3.
Bayesian inference in Extreme Value Theory
26
2.3.1
Bayesian framework
Bayesian techniques oﬀer an alternative way to draw inferences from the likelihood func-
tion. As in the non-Bayesian setting, we assume data x = (x1, ..., xn) to be realizations of
a random variable whose density falls within a parametric family F = {f (x; θ) : θ ∈Θ} (θ
is perhaps a collection of several parameters). However, parameters of a distribution are
now treated as random variables, more precisely we assume that Θ is distributed according
to the so-called prior density π (θ). The speciﬁcation of this prior distribution enables us
to supplement the information provided by the data—which, in extreme value analysis, is
often very limited—with other sources of information.
Given Θ = θ we model our observed data x using the probability density function
f(x; θ). The likelihood function for θ is therefore L(θ | x) = f(x; θ).
We can combine both the prior and the likelihood using Bayes Theorem, which states
that
π (θ | x) = π (θ) L(θ | x)
f (x)
,
(2.39)
where
f (x) =







´
Θ π (θ) L(θ | x)dθ
if θ is continuous,
P
Θ
π (θ) L(θ | x)
if θ is discrete.
(2.40)
Since f (x) is not a function of θ, calculations (numerical and algebraic) are usually
required only up to a proportionality constant. Therefore, we write Bayes theorem as
π (θ | x) ∝π (θ) × L(θ | x),
(2.41)
i.e.,
posterior ∝prior × likelihood.
π (θ | x) is the posterior distribution of the parameter vector θ, θ ∈Θ , i.e. the distribution
of θ after the inclusion of the data. This posterior distribution is often of great interest,
since the prior-posterior changes represent the changes in our beliefs after the data has

2.3.
Bayesian inference in Extreme Value Theory
27
been included in the analysis, hence the posterior density can be interpreted as our updated
knowledge about Θ after having observed x. Inference is typically based on reproducing all
or parts of the posterior density graphically (as graphs or contour plots). Another option
is to report e.g. posterior mean, mode, and quantiles.
However, many problems in Bayesian inference leave us with intractable distributions
that cannot be expressed in a closed form. The posterior or joint distribution that we are
interested in is often of high dimensionality, and in cases like mixture models can exhibit
an exponentially increasing number of modes. Therefore, we need a way of understanding
posterior densities which does not rely on being able to analytically integrate the kernel
of the posterior.
To solve this problem, numerous simulation-based methods have been developed and
implemented within the Bayesian paradigm, e.g.
importance sampling (Ripley, 1987),
Markov Chains Monte Carlo (MCMC) algorithms (Casella and Robert, 1999) and particle
ﬁltering (Doucet et al., 2001).
For this purpose, we will focus on Markov chain Monte Carlo (MCMC) methods,
which can be used to generate samples from the posterior distribution. A more detailed
explanation of MCMC methods is provided in Appendix A.
2.3.2
Basics of Bayesian inference for extremes
There are several reasons for preferring a Bayesian analysis of extremes over the more tra-
ditional likelihood approach. Since extreme data are (by their very nature) quite scarce,
the ability to incorporate other sources of information through a prior distribution has
obvious appeal. Bayes’ Theorem also leads to an inference that comprises a complete
distribution, meaning that the variance of the posterior distribution, for example, can be
used to summarize the precision of the inference, without having to rely upon asymp-
totic theory. Furthermore, the concept of the predictive distribution is implicit in the
Bayesian framework. This distribution describes how likely are diﬀerent outcomes of a

2.3.
Bayesian inference in Extreme Value Theory
28
future experiment. The predictive probability density function is given by
f (y | x) =
ˆ
Θ
f (y | θ) π (θ | x) dθ
(2.42)
when θ is continuous, and analogously when θ is discrete.
From equation (2.42), we can see that the predictive distribution is formed by weighting
the possible values for θ in the future experiment f (y | θ) by how likely we believe they
are to occur after observing the data.
For example, a suitable model for threshold excess Y = X −u is Y ∼GPD(σ, ξ). Es-
timation of θ = (σ, ξ) could be made on the basis of previous observations x = (x1, ..., xn).
Thus, in the Bayesian framework, we would have
P (Y ≤y | x1, ..., xn) =
ˆ
Θ
P (Y ≤y | θ) π (θ | x) dθ.
(2.43)
Equation (2.43) gives the distribution of a future threshold excess, allowing for both
parameter uncertainty and randomness in future observations.
Solving
P (Y ≤y | x1, ..., xn) = q
(2.44)
for y therefore gives an estimate of the Value-at-Risk at level q ∈(0, 1) that incorporates
uncertainty due to model estimation.
Although (2.42) may seem analytically intractable, it can be approximated if the pos-
terior distribution has been estimated using, for example, MCMC. After removal of the
“burn–in” period, the MCMC procedure gives a sample θ1, ..., θB that can be regarded as
realizations from the stationary distribution π (θ | x). Thus
P (Y ≤y | x1, ..., xn) ≈1
B
B
X
i=1
P (Y ≤y | θi) ,
(2.45)
which we can solve for y using a numerical solver.
Another reason lending appeal to Bayesian inference for extremes is that it is less
dependent on the regularity assumptions required by the theory of maximum likelihood.

2.3.
Bayesian inference in Extreme Value Theory
29
For example, when ξ < −0.5, maximum likelihood estimation breaks down (Smith, 1985).
In this case a Bayesian approach provides a feasible alternative.
2.3.3
Combining diﬀerent data sources
As mentioned in Section 2.1.4, Basel II requires that operational risk models include the use
of several diﬀerent sources of information: internal data, relevant external data, scenario
analysis and factors reﬂecting the business environment and internal control systems.
Additionally, it is widely recognized that estimation of operational risk distributions
cannot be done exclusively using historical data given the limited number of data available
and the diﬃculty of predicting future losses in a banking environment which is constantly
changing.
Thus, combining diﬀerent sources of information is critical for estimation of operational
risk, especially for low-frequency/high-severity risks.
Shevchenko (2011) considers three ways that have been proposed to process diﬀerent
data sources of information:
1. Ad-hoc procedures.
2. Bayesian methods.
3. General non-probabilistic methods.
We focus on the ﬁrst two procedures only. General non-probabilistic methods are mainly
based on the Dempster-Shafer theory and probabilistic boxes. More on these subjects can
be found in Ferson et al. (2003).
2.3.3.1
Ad-hoc procedures
One suggested method for the ad-hoc combining is based on mixing of distributions, where
the loss distribution is expressed as:

2.3.
Bayesian inference in Extreme Value Theory
30
w1FSA (x) + w2Fint (x) + (1 −w1 −w2)Fext (x) ,
(2.46)
where FSA (x), Fint (x) and Fext (x) are the distributions identiﬁed by scenario analysis,
internal data and external data, respectively, using expert speciﬁed weights w1 and w2.
After that, we can apply the minimum variance principle, where the combined esti-
mator is a linear combination of the individual estimators obtained from internal data,
external data and expert opinion separately, with the weights chosen to minimize the
variance of the combined estimator.
Consider two unbiased independent estimates ˆθ1 and ˆθ2 for a parameter θ, i.e. E
h
ˆθm
i
=
θ; Var
h
ˆθm
i
= σ2
m, m = 1, 2.
The combined unbiased estimator is:
ˆθtot = w1ˆθ1 + w2ˆθ2, w1 + w2 = 1
(2.47)
and
Var

ˆθtot

= w2
1σ2
1 + (1 −w1)2 σ2
2
(2.48)
Then choose weights to minimize Var

ˆθtot

, namely
ˆw1 =
σ2
2
σ2
1 + σ2
2
,
ˆw2 =
σ2
1
σ2
1 + σ2
2
.
(2.49)
This technique can be easily extended to combine three or more estimators.
ˆθtot = w1ˆθ1 + · · · + wK ˆθK,
w1 + · · · + wK = 1
(2.50)
and
wi =
1/σ2
i
K
P
k=1
(1/σ2
k)
.
(2.51)
Heuristically, this can be applied to almost any quantity, including a distribution pa-
rameter or distribution characteristic such as mean, variance or quantile. The assumption
that the estimators are unbiased estimators for θ is probably reasonable when combining

2.3.
Bayesian inference in Extreme Value Theory
31
estimators from diﬀerent experts (or from expert and internal data). However, it is cer-
tainly questionable if applied to combine estimators from the external and internal data
(Shevchenko, 2011).
2.3.3.2
Bayesian methods
Diﬀerent methodologies have been proposed in the literature in order to combine diﬀer-
ent sources of information. Here, we only show part of the methodology suggested by
Lambrigger et al. (2007). They consider the following assumptions:
 Loss frequency and severity are modelled by parametric distributions (e.g. Poisson
for the frequency or Pareto, lognormal, etc.
for the severity).
In any case, the
parameter vector θ has to be estimated.
 Before we have any internal data, only external data are available and the best pre-
diction for the parameter θ is given by the belief in the available external knowledge.
The parameter of interest is modelled by a prior distribution corresponding to a
random vector Θ.
 The true speciﬁc parameter θ0 is treated as a realization of a random vector Θ, where
Θ corresponds to the whole data (including external and internal data) and θ stands
for the unknown parameter of the speciﬁc entity being considered.
 As time passes, internal data X = (X1, ..., XK)′ and expert opinions ∆= (∆1, ..., ∆M)
about θ become available.
 X and ∆are assumed to be conditionally independent given Θ, with joint density
h (x, δ | θ) = h1 (x | θ) h2 (δ | θ) ,
(2.52)
where h1 and h2 are the conditional densities of X and ∆given Θ.

2.3.
Bayesian inference in Extreme Value Theory
32
 Assuming that observations and expert opinions are conditionally independent and
identically distributed, given Θ = θ, we have that
h1 (x | θ) =
K
Y
k=1
f1 (xk | θ) ,
(2.53)
h2 (δ | θ) =
M
Y
m=1
f2 (δm | θ) ,
(2.54)
where f1 and f2 are the marginal densities of a single internal observation and a
single expert opinion, respectively.
Now, by considering the unconditional joint density of X and ∆, denoted by h (x, δ) and
using Bayes’ theorem,
h (x, δ | θ) π (θ) = π (θ | x, δ) h (x, δ)
(2.55)
and hence the posterior density satisﬁes
π (θ | x, δ) ∝π (θ) h (x, δ | θ) .
(2.56)
The posterior density is then
π (θ | x, δ) ∝π (θ)
K
Y
k=1
f1 (xk | θ)
M
Y
m=1
f2 (δm | θ) .
(2.57)
2.3.4
Prior elicitation
Elicitation is another part of the process of statistical modelling. Garthwaite et al. (2004)
deﬁne elicitation as “the process of formulating a person’s knowledge and beliefs about one
or more uncertain quantities into a (joint) probability distribution for those quantities”. In
a Bayesian context, it arises most usually as a method for specifying the prior distribution
for one or more unknown parameters of a statistical model. However, there are some other
contexts in which elicitation is important.

2.3.
Bayesian inference in Extreme Value Theory
33
2.3.4.1
The elicitation process
The elicitation of information is not an easy task, even if it only aims to extract the
expert’s beliefs about an event or particular hypothesis. Even when the expert is familiar
with probabilities and their meaning, it is not easy to assign a probability value to an
event accurately.
It is convenient to think of the elicitation process as a task that involves a facilitator
who assists the expert to formulate his knowledge in a probabilistic way. Sometimes, if
the expert is familiar with statistical concepts, then there may be no formal need for a
facilitator, although this is rare in practice.
To carry out this process, four stages have been identiﬁed:
1. Setup. It consists of selecting and training the expert(s), identifying what aspects
of the problem to elicit, etc.
2. Elicitation. It is the extraction of information from the expert and it is the core of
the process.
3. Fit. A probability distribution is ﬁtted to the information obtained in the second
stage.
4. Evaluation. This stage involves assessing the adequacy of the elicitation, with the
option of returning to the second stage and eliciting more summaries from the ex-
pert(s).
Despite the diﬃculties to carry out the elicitation process, it is a valuable tool since
it facilitates decision-making and allows to make inference. Furthermore, it brings the
analysis closer to the application by demanding attention to what is being modelled, and
what is reasonable to believe about it.
Similarly, it helps to summarize the posterior
distribution by meaningful quantities.

2.3.
Bayesian inference in Extreme Value Theory
34
To decide whether the process has been performed successfully, it is important to
distinguish between the quality of an expert’s knowledge and the accuracy with which this
knowledge has been translated into a probabilistic form.
A successful elicitation process faithfully represents the opinion of the person being
elicited, which does not necessarily mean that it is the correct view.
2.3.4.2
Elicitation and extreme events
It is reasonable to hope that experts should provide relevant prior information about
extremal behaviour, since they have speciﬁc knowledge of the characteristics of the data
under study.
Unfortunately, when we consider unlikely events, these are hard to evaluate. In this
case, expert opinion plays an important role due to the scarcity of data. It has been
observed that in general people are capable of estimating proportions, modes and medians
of samples. However, when dealing with highly skewed distributions several errors may
occur.
To avoid these problems, the questions should be formulated in an intuitive way. Stein-
hoﬀand Baule (2006) suggest the following questions:
1. Which events of severity between x and y do you remember?
2. How many of those events have happened in a year on average?
3. How many of those events happened in a good year (minimum) and in a bad year
(maximum)?
They also point out that these questions are quite easy if, on average, there is more than
one event a year. However, the crucial losses are usually the low-frequency, high-impact
events that occur very rarely. In such case, they suggest some changes to the questions as
follows:
2a. How many extreme events happened in the past 10 or xx years?

2.3.
Bayesian inference in Extreme Value Theory
35
3a. How many extreme events can happen in the chosen time range in worst- and
best-case scenarios?
To obtain more accurate results, the last two questions might be changed into:
 How many years will we have to wait, all things being equal, to observe an event of
severity x (or above)?
2.3.4.3
Distribution ﬁtting
As we saw in the previous paragraphs, expressing prior beliefs directly in terms of the
distribution parameters is not a simple job. Depending on the questions we ask, expert
opinions on potential losses and corresponding probabilities are often expressed as:
 Opinion on the distribution parameter;
 Opinions on the number of losses with the amount to be within some ranges;
 Separate opinions on the frequency of the losses and quantiles of the severity;
 Opinion on how often the loss exceeding some level may occur.
Usually, if the expected values for the quantiles (or mean) and their uncertainties are
estimated by the expert then it is possible to ﬁt the priors.
It is very unusual that experts express their beliefs in terms of the distribution param-
eters. Instead, they provide some quantities such as quantiles or other risk characteristics.
If we consider this situation, it might be better to assume some priors for these quantities
that will imply a prior for the parameters.
Let θ = (θ1, ..., θn) be the model parameters and di = gi (θ), i = 1, ..., n be risk
characteristics that can be elicited from experts, such as speciﬁc quantiles, expected values,
return level, etc. Assuming that experts specify the joint prior π (d1, ..., dn), we can obtain
the prior for θ1, ..., θn by using the transformation method as follows:

2.3.
Bayesian inference in Extreme Value Theory
36
π (θ) = π (g1 (θ) , ..., gn (θ))

∂(g1 (θ) , ..., gn (θ))
∂(θ1, ..., θn)
 ,
(2.58)
where the second factor on the right-hand side is the Jacobian determinant of the trans-
formation.
Since there is possible dependence between parameters, it is helpful to choose charac-
teristics such that independence can be assumed.
Coles and Tawn (1996) and Coles and Powell (1996) elicit prior information in terms
of extreme quantiles, arguing that this is a scale on which an expert is most likely to be
able to accurately quantify their prior beliefs about extremal behaviour.
If the prior for quantiles q1 < · · · < qn (for speciﬁc probability values p1 < · · · < pn) is
to be speciﬁed, in order to respect the ordering we can work instead with the diﬀerences
d1 = q1 −e1, d2 = q2 −q1, ..., dn = qn −qn−1,
where e1 is a physical lower end point for the process variable.
Under this setting it is reasonable to assume independence between these diﬀerences
and impose constraints di > 0, i = 2, ..., n.
If the experts assign marginal priors π (d1) , ..., π (dn) then the full joint prior is
π (d1, ..., dn) = π (d1) × · · · × π (dn) .
(2.59)
Hence, the prior for θ1, ..., θn can be calculated using (2.58).
2.3.5
Advantages and challenges of Bayesian inference in EVT
Recently, Bayesian inference has been focused on the development of mixture type models,
which typically treat the threshold as a model parameter to be estimated, and so also
automatically accounts for the uncertainty associated with the threshold selection. For
example, Behrens et al. (2004) use a truncated Gamma distribution for observations below
the threshold and the GPD approach for observations exceeding the threshold. Tancredi et

2.4.
Conclusions of the Chapter
37
al. (2006) propose to model extreme and non-extreme data with a distribution composed
of a piecewise constant density from a low threshold up to an unknown end point and a
GPD with threshold for the remaining tail part.
In both examples, Bayesian inference is used for ﬁtting the mixture model as it can take
advantage of any expert prior information, which can be important in tail estimation due to
the inherent sparsity of extremal data. There are distinct beneﬁts and potential drawbacks
to the mixture modelling approach when compared to the classical ﬁxed threshold method.
The principal advantages are that the threshold is estimated avoiding the often subjective
choice in the classical approach and the uncertainty associated with the estimation is
accounted for in inference, which is rather challenging for the ﬁxed threshold method.
The automated threshold estimation is a major beneﬁt when trying to automate ﬁtting
the GPD to multiple datasets.
The principal drawbacks are the added complexity of estimating the additional param-
eters and the ﬁt in the bulk of the distribution (or the alternate tail) may have an inﬂuence
on the tail ﬁt. It is clear that diﬀerent parameters values could give similar model ﬁts.
However, in most of the cases, Bayesian inference provides reliable parameter estimates,
the threshold included. This will be studied more closely in the next chapters.
2.4
Conclusions of the Chapter
In this chapter we have studied the importance of measuring operational risk, as well
as the integrated risk framework established by Basel II, which deﬁnes the guidelines to
measure the capital adequacy of ﬁnancial institutions. Furthermore, we have presented
the fundamentals of Extreme Value Theory through various deﬁnitions and theorems,
along with practical aspects for estimating and assessing statistical models for heavy-tailed
events.
One of the most important aspects that has been addressed in this chapter is the

2.4.
Conclusions of the Chapter
38
subjectivity in the threshold selection by using graphical methods. This is one of the most
controversial aspects of Extreme Value Theory and, possibly, its main weakness.
Moreover, we have shown how Bayesian inference is a powerful alternative to handle
the threshold issue and to include expert opinion, as stated in Basel II. Similarly, we have
introduced the concept of elicitation and showed how this process may be carried out,
playing a fundamental role in the estimation due to the scarcity of data.
All the material presented in this chapter will help us to understand the work presented
in subsequent chapters.

Chapter 3
A Bayesian model for operational
risk
The use of Bayesian methods in extreme value modelling has recently become more com-
mon. Several models have been proposed in the literature. For example, Pickands (1994)
discusses Bayesian estimation of extreme quantiles assuming independent non-informative
priors among parameters. Bermudez et al. (2001) propose a Bayesian predictive approach
for the choice of the threshold. Behrens et al. (2004) develop an extreme value mixture
model by combining a parametric bulk model below the threshold with a GPD above the
threshold. Several models have been derived from this last mixture model. For instance,
Mendes and Lopes (2004) and Zhao et al. (2009) propose a mixture with a normal distri-
bution for the bulk, with both tails represented by separate threshold models. Carreau and
Bengio (2009) propose a hybrid Pareto by splicing a normal distribution with a GPD and
setting continuity constraints on the density and on its ﬁrst derivative at the threshold.
Cabras and Castellanos (2010) consider a semiparametric bulk model spliced with a GPD
upper tail. Do Nascimento et al. (2011) extended the model of Behrens et al. (2004) by
deﬁning the bulk distribution as a weighted mixture of Gamma densities.
Due to its notable inﬂuence, for the purposes of this thesis, we focus our attention
39

3.1.
General model
40
on the model developed by Behrens et al. (2004). We refer the reader to Scarrott and
MacDonald (2012) for a detailed review of Bayesian methods in extreme value modelling.
3.1
General model
The model proposed in this chapter is based on the work of Behrens et al. (2004), where
the uncertainty in the threshold selection is incorporated by choosing a prior, possibly ﬂat,
for it to compose the model.
Consider X1, ..., Xn independent and identically distributed observations and u the
threshold. We assume that observations below the threshold come from a certain distri-
bution with parameters η, denoted by FH (· | η), while those above the threshold come
from a GPD, denoted by FG (x | u, σ, ξ). Therefore, the distribution function FB, of any
observation X, can be written as:
FB (x | η, u, σ, ξ) =







FH (x | η) ,
x < u,
FH (u | η) + [1 −FH (u | η)] FG (x | u, σ, ξ) ,
x ≥u.
(3.1)
The likelihood is:
L (θ; x) =
Y
A
fH (x | η)
Y
B
[1 −FH (u | η)]
"
1
σ

1 + ξ (xi −u)
σ
−1+ξ
ξ
+
#
(3.2)
for ξ ̸= 0 and
Y
A
fH (x | η)
Y
B
[1 −FH (u | η)]
 1
σexp
xi −u
σ

(3.3)
for ξ = 0, where θ = (η, u, σ, ξ), x = (x1, ..., xn), A = {i : xi < u} and B = {i : xi ≥u}.
Figure 3.1 is a representation of this model. We can observe that the density has a
discontinuity at the threshold u. Depending on the parameters, the density jump can be
larger or smaller, and in each case the choice of which observations will be considered as
exceedances can be more obvious or less evident.

3.1.
General model
41
Figure 3.1: Representation of the mixture model
3.1.1
Priors
Prior for parameters below the threshold
To model observations below the threshold through a parametric form, it is always better
to try to obtain a conjugate prior to simplify the problem analytically.
Given that operational losses are non-negative, a convenient choice is the Gamma distri-
bution. We have η = (α, β) and reparameterizing as µ = α/β, we can set α ∼Gamma (a, b)
and µ ∼Gamma (c, d) where a, b, c and d are known hyperparameters. Therefore the joint
prior of η = (α, β) is:
π (η) =
ba
Γ (a)αa−1e−bα dc
Γ (c)
α
β
c−1
e
−dα/β
 α
β2

.
(3.4)
Prior for the threshold
To set up a prior distribution for the threshold, we can assume that u follows a truncated
normal distribution with parameters (µu, σu), truncated from below at e1 (the minimum
of the data). Setting µu at some high data percentile and σu large enough to represent a

3.1.
General model
42
fairly non-informative prior, the density becomes, for u > e1,
π
 u | µu, σ2
u, e1

=
1
p
2πσ2
u
exp
(
−1
2
u −µu
σu
2)
Φ

−
e1 −µu
σu

.
(3.5)
A continuous uniform prior is an alternative. A discrete distribution can also be as-
sumed. In this case, u could take any value between certain high data percentiles which
is convenient for applications as it facilitates posterior computation.
Prior for the GPD parameters
Coles and Tawn (1996) point out that increasing σ or ξ leads to a longer-tailed distribution,
so a priori negative dependence between these parameters is expected.
To avoid the
assumption of independent priors on the GPD parameters, we use an elicitation method
as outlined in Section 2.3.4.3.
In their paper, Behrens et al. (2004) call q = u + σ
ξ
 p−ξ −1

the return level1 and
elicitation of the prior information is done in terms of (q1, q2, q3), in the case of location–
scale parameterization of the GPD, for speciﬁc values of p1 > p2 > p3.
We consider
more appropriate to treat q as the Value-at-Risk, since it is derived in terms of the GPD
parameters. We should also notice that in this case p is a ﬁxed value and the probability
of exceeding the threshold is not considered. We will return to this issue later in Chapter
4, when we discuss prior sensitivity. For now, our main concern is to provide some insight
into how the priors are chosen.
In Behrens et al. (2004), the prior for ξ and σ is computed by choosing two levels
q1 < q2 and setting d1 = q1 and d2 = q2 −q1 to be Gamma. Concretely, we have the
following Gamma distributions with known hyperparameters: d1 = q1 ∼Gamma(a1, b1)
1The return level is the level that is expected to be exceeded, on average, once every 1/p years (called
the return period), where p is the probability of the extreme event occurring. It is obtained by computing
the inverse of the GEV, although in their paper Behrens et al. (2004) deﬁne the return level in terms of
the GPD parameters.

3.1.
General model
43
and d2 = q2 −q1 ∼Gamma(a2, b2). The hyper parameters a1, b1, a2 and b2 are typically
obtained from the experts’ information; this will be treated in detail in Chapter 4.
In order to derive the marginal prior distribution for σ and ξ, we have:
π (σ, ξ) ∝π (d1) π (d2)

∂(d1, d2)
∂(σ, ξ)
 =

∂d1
dσ × ∂d2
dξ
 −

∂d1
dξ × ∂d2
dσ
 .
(3.6)
To ﬁnd the Jacobian

∂(d1, d2)
∂(σ, ξ)
 :
∂d1
dσ
=
p−ξ
1
ξ
−1
ξ ,
(3.7)
∂d2
dσ
=
p−ξ
2
ξ
−p−ξ
1
ξ ,
∂d1
dξ
=
−σp−ξ
1
ξ2
+ σp−ξ
1 ln p1
ξ
+ σ
ξ2,
∂d2
dξ
=
−σp−ξ
2
ξ2
+ σp−ξ
2 ln p2
ξ
+ σp−ξ
1
ξ2
−σp−ξ
1 ln p1
ξ
.
Hence
∂d1
dσ ×∂d2
dξ −∂d1
dξ ×∂d2
dσ = σ (p1p2)−ξ ln p2
ξ2
−σp−ξ
2 ln p2
ξ2
+σp−ξ
1 ln p1
ξ2
−σ (p1p2)−ξ ln p1
ξ2
, (3.8)
and therefore
π (σ, ξ) ∝
h
u + σ
ξ

p−ξ
1 −1
ia1−1
exp
h
−b1
n
u + σ
ξ

p−ξ
1 −1
oi
×
h
u + σ
ξ

p−ξ
2 −p−ξ
1
ia2−1
exp
h
−b2
n
σ
ξ

p−ξ
2 −p−ξ
1
oi
×
−σ
ξ2
h
(p1p2)−ξ (log p2 −log p1) −p−ξ
2 log p2 + p−ξ
1 log p1
i
.
(3.9)
If we consider the case ξ = 0, it is possible to assign a positive probability to this
point and then the prior distribution would be a mixture between the prior distribution
corresponding to the case ξ = 0 and the prior distribution corresponding to the case ξ ̸= 0.
In ﬁnance, there are some risk measures used in practice and it would be convenient
to specify priors in terms of these measures. This will be discussed in the next chapter.
3.1.2
Posterior inference
From the likelihood (3.2) and the prior distributions speciﬁed before, using the Bayes
theorem we can obtain the posterior distribution, which is given as follows (Section 3.4,

3.1.
General model
44
Behrens et al. 2004):
log p(θ | x) =
K +
nP
i=1
I (xi < u) [αlogβ −logΓ (α) + (α −1) log xi −βxi]
nP
i=1
I (xi ≥u) log

1 −
u´
0
βα
Γ(α)tα−1e−βtdt

−
nP
i=1
I (xi ≥u) log σ
−1+ξ
ξ
nP
i=1
I (xi ≥u) log
h
1 + ξ(xi−u)
σ
i
+(a −1)log α −bα + (c −1)log

α
β

=d

α
β

+ log

α
β2

+ 1
2σu (uµu)2 + (a1 −1)log
h
u + σ
ξ

p−ξ
1 −1
i
−b1
σ
ξ

p−ξ
1 −1

+ (a2 −1)log u + σ
ξ

p−ξ
2 −p−ξ
1

−b2
σ
ξ

p−ξ
2 −p−ξ
1

+log
−σ
ξ2
h
(p1p2)−ξ (log p2 −log p1) −p−ξ
2 log p2 + p−ξ
1 log p1
i
.
(3.10)
In (3.10) K is the normalizing constant and θ = (η, u, σ, ξ). We only show the posterior
with the likelihood for the case ξ ̸= 0 and with a normal prior for the threshold. However,
the case ξ = 0 can be considered in the model as well.
As expected, the posterior has no closed form and the implementation of Markov Chain
Monte Carlo methods is required. Computations will be done via the Metropolis–Hastings
steps within a blockwise algorithm. The algorithm is shown in Appendix A.
3.1.3
Performance in simulations
The algorithm was tested ﬁrst with simulated data, generated for ﬁxed values of α, β, u, σ
and ξ, based on diﬀerent characteristics such as skewness, tail behaviour and number of
observations available for estimation of these parameters.
For each scenario we considered a sample size of n = 1000 and diﬀerent combinations
of parameters, by varying the value of the shape parameter (ξ); data below the threshold
were simulated using a Gamma distribution.

3.1.
General model
45
100,000 Iterations
Parameter
Posterior
mean
Posterior
median
Posterior
std
Credible
interval
True
value
α
1.342
1.342
0.002
(1.268, 1.438)
1.350
β
0.292
0.290
0.003
(0.257, 0.330)
0.290
u
1.492
1.389
0.068
(1.211, 2.221)
1.400
σ
3.684
3.677
0.072
(3.170, 4.209)
3.500
ξ
-0.080
-0.083
0.003
(-0.182, 0.035)
-0.100
Table 3.1: Posterior MCMC estimates for n=1000 simulated data from a Gamma-GPD mixture,
with ξ = −0.1
We display here the results for the following combination of parameters.
p
=
0.1,
α
=
1.35, 0.5,
β
=
0.29, 0.2,
σ
=
3.5,
ξ
=
−0.1, 0.2.
The value of u is deﬁned automatically by the sample size and the quantile p. We started
by drawing a number n of observations from a Gamma(α, β) and u was deﬁned as the 1−p
quantile of this sample; n1 observations below this value were retained, and the sample
size was completed by drawing n2 = n −n1 observations from u + GPDσ,ξ.
Results for ξ = −0.1 are shown in Table 3.1 and Figures 3.2 and 3.3. In all cases, the
estimates are very close to the true values and convergence seems to be achieved. We also
display the estimates for ξ = 0.2 in Table 3.2. Again, the estimates are close to the true
values.

3.1.
General model
46
100,000 Iterations
Parameter
Posterior
mean
Posterior
median
Posterior
std
Credible
interval
True
value
α
0.510
0.508
0.020
(0.493, 0.512)
0.5
β
0.203
0.202
0.014
(0.197, 0.212)
0.2
u
5.944
6.312
2.319
(5.998, 6.451)
6.0
σ
3.625
3.629
0.665
(3.484, 3.810)
3.5
ξ
0.187
0.178
0.121
(0.177, 0.191)
0.2
Table 3.2: Posterior MCMC estimates for n=1000 simulated data from a Gamma-GPD mixture,
with ξ = 0.2
Figure 3.2: Trace plots of the MCMC samples from the posterior density for n=1000 simulated
data from a Gamma-GPD mixture, with ξ = −0.1. 100,000 iterations after burn-in

3.2.
An application to operational risk data
47
Figure 3.3: Histograms of the MCMC samples from the posterior density for n=1000 simulated
data from a Gamma-GPD mixture, with ξ = −0.1. 100,000 iterations after burn-in
3.2
An application to operational risk data
In this section, the objective is to make use of the Bayesian model presented in the previous
section to analyze operational risk data.
3.2.1
Data description
Unfortunately, in many countries the record of operational risk losses has not yet been
formalized. The number of institutions that report their operational losses and the avail-

3.2.
An application to operational risk data
48
ability of information are still limited, so obtaining data to carry out this work was not
an easy task.
In an ideal scenario, we should have a database containing all risk events and their
corresponding Basel II categories. However, the available data are still insuﬃcient and
most of them have been collected for a short period of time.
In spite of this fact, it was possible to collect fraud data, one of the most frequent
operational risk events that has been a constant concern in the ﬁnancial sector.
The
data consist of 626 observations for fraud losses in 41 banks in Mexico, recorded between
January 2007 and April 2010. These data were obtained during a summer internship at
the Bank of Mexico and were used for research purposes only. The names of the banks
were changed into capital letters to preserve their anonymity.
Due to the large diﬀerences in banks sizes and their respective losses, these were scaled
by the asset size of each bank at the time of the event considered and multiplied by one
million2.
3.2.2
Exploratory analysis
As a ﬁrst approach, we determined the main descriptive statistics (Table 3.3). Similarly,
the behaviour of the data over time was observed by using graphical tools (Figure 3.4).
In all cases, one may identify the presence of possible extreme events. Also, we observe
a concave shape in the quantile-quantile plot when it is compared to the exponential
2To perform the scaling of data it is important to consider the work of Shih et al. (2000); they found
a relationship between the size of the institution and the magnitude of its losses. This paper highlights
some aspects:
 The size of an institution is related to the magnitude of its loss, but the relationship is not necessarily
linear.
 The size represents a very small proportion (about 5%) of the variability in the loss severity.
The scaling performed is a simple approach to the relationship between the size of the institution and the
magnitude of its losses, as this is a subject of study itself, due to the heterogeneity between the banks
considered. Particularly, for the fraud risk, the types of frauds and their eﬀects may diﬀer widely from
one institution to another.
The use of scaled data allowed that not only losses corresponding to large banks exceeded the threshold,
but also smaller banks were incorporated into the analysis.

3.2.
An application to operational risk data
49
distribution, indicating heavy-tailedness. This fact is supported by the value of the kurtosis
and appearance of the histogram of losses.
Figure 3.4: Top left: Monthly fraud losses (scaled by the asset size) in 41 banks from 01/2007
to 04/2010. Top right: Histogram of scaled fraud losses from 01/2007 to 04/2010. Bottom:
Exponential Q-Q plot for n=626 scaled fraud losses. We can infer the heavy-tailedness of the
data in all cases.

3.2.
An application to operational risk data
50
Number of observations: 626
Standard deviation: 110.793
Mean: 51.930
Median: 19.890
Minimum: 0.0002
Skewness: 6.604
Maximum: 1509.410
Kurtosis: 66.678
Percentile (q)
Percentile value (Xq)
q = 0.25
X0.25 = 3.241
q = 0.50
X0.50 = 19.892
q = 0.75
X0.75 = 54.134
q = 1.00
X1.00 = 1509.416
Table 3.3: Descriptive statistics for n=626 fraud losses in 41 banks, recorded from 01/2007 to
04/2010
Runs
Parameter
10,000
Mean, Median, Std
25,000
Mean, Median, Std
100,000
Mean, Median, Std
α
0.553, 0.552, 0.029
0.553, 0.553, 0.029
0.552, 0.552, 0.029
β
0.014, 0.014, 0.001
0.014, 0.014, 0.001
0.014, 0.014,0.001
u
70.176, 72.980, 17.703
70.011, 72.980, 19.231
71.310, 72.980, 19.072
σ
93.033, 93.922, 19.847
93.392, 94.141, 20.673
94.173, 94.593,21.109
ξ
0.097, 0.092, 0.059
0.097, 0.091, 0.058
0.095, 0.089, 0.060
λ
38.147, 33.934, 13.255
38.411, 33.934, 13.538
37.752, 33.933, 13.523
Table 3.4: Posterior MCMC estimates for n=626 fraud losses in 41 banks, recorded from 01/2007
to 04/2010
3.2.3
Results
Two chains were run in R with 10,000, 25,000 and 100,000 iterations. The initial values
for the ﬁrst chain were chosen by using classical estimators: α and β were set as the
maximum likelihood estimators, u = data 70th percentile, and σ and ξ the maximum
likelihood estimates (MLE’s). For the second chain, starting values were chosen far from
the maximum likelihood estimates used for the ﬁrst chain. Run time varied from half an
hour to ﬁve hours in R.
The posterior mean, median and standard deviation of α, β, σ, u, ξ are shown in Table
3.4. We also estimate the rate of exceedances, i.e., the number of observations exceeding
the threshold u per year. This quantity is denoted by λ and it will not be used in further
analysis, although it is displayed in Table 3.4 as another model parameter.

3.2.
An application to operational risk data
51
Figure 3.5: Mean excess plot (fraud data) Circled area represents the posterior range of u
Figure 3.5 shows the mean excess plot used in classical Extreme Value Theory for the
threshold selection. From the plot, one can see that the posterior range (circled area)
contains the value of u where the function becomes linear, about u ≈110.
In order to compare both chains, trace plots are displayed in Figure 3.6. Notice that
regardless of the starting values, the chains show convergence to the same value. A visual
examination of the ergodic mean behaviour seems to indicate that the chain actually
converges. Also, the histogram shows the distribution of the parameters for the ﬁrst chain
(Figure 3.7).
Although the estimates seem to be stable, in order to study convergence in a more
formal way, we used the Gelman–Rubin statistic to compare the ﬁrst and second chains
(Table 3.5 and Figure 3.8). Also, the eﬀective sample size and the Heidelberg and Welch
diagnostics were used for the ﬁrst chain (the one corresponding to the MLE’s as initial
values, Tables 3.6 and 3.7).

3.2.
An application to operational risk data
52
Figure 3.6: Trace plots of the MCMC samples from the posterior density for n=626 fraud losses
in 41 banks, recorded from 01/2007 to 04/2010. Two chains for 100,000 iterations were run in
R: Gray colour is the ﬁrst chain and black colour is the second chain
Parameter
Runs
α
β
u
σ
ξ
λ
10,000
1.000
(1.000)
1.010
(1.020)
1.070
(1.150)
1.050
(1.090)
1.050
(1.100)
1.140
(1.400)
25,000
1.000
(1.000)
1.000
(1.000)
1.000
(1.010)
1.000
(1.000)
1.000
(1.000)
1.000
(1.000)
100,000
1.000
(1.000)
1.000
(1.000)
1.000
(1.010)
1.000
(1.010)
1.000
(1.010)
1.000
(1.010)
Table 3.5: Gelman-Rubin statistic for the MCMC posterior estimates for n=626 fraud losses in
41 banks, recorded from 01/2007 to 04/2010. (Scale reduction factor and its 97.5% quantile)

3.2.
An application to operational risk data
53
Figure 3.7: Histograms of the MCMC samples from the posterior density for n=626 fraud losses
in 41 banks, recorded from 01/2007 to 04/2010. 100,000 iterations
Parameter
Runs
α
β
u
σ
ξ
λ
10,000
639.790
573.720
122.370
128.560
158.040
82.210
25,000
1535.950 1417.400 309.280
327.280
342.040
232.110
100,000
5196.060 4262.690 1239.950
1409.310
1450.460
926.520
Table 3.6: Eﬀective sample size for the MCMC posterior estimates for n=626 fraud losses in 41
banks, recorded from 01/2007 to 04/2010

3.2.
An application to operational risk data
54
Figure 3.8: Gelman plots for the MCMC posterior estimates for n=626 fraud losses in 41 banks,
recorded from 01/2007 to 04/2010. 100,000 iterations
Results reported in Table 3.5 clearly indicate that the behaviour of both chains is
basically the same.
The value of the Gelman–Rubin statistic is very close to 1 in all
cases, which occurs when the pooled within-chain variance dominates the between-chain
variance, meaning that at that point, all chains have escaped the inﬂuence of their starting
points and have traversed all of the target distribution.
The Heidelberg and Welch diagnostic, reported in Table 3.7, also indicates that the
ﬁrst chain, which will be used in the analysis, achieves stationarity.
These results are essential to achieve an adequate estimation of the parameters.

3.2.
An application to operational risk data
55
Result (p-value/ Halfwidth)
Runs
Test
Parameter
α
β
u
σ
ξ
λ
10,000
Stat
passed
(0.321)
passed
(0.151)
passed
(0.911 )
passed
(0.838)
passed
(0.883)
passed
(0.953)
Halfw
passed
(0.002)
passed
(0.001)
passed
(2.687)
passed
(2.967)
passed
(0.008)
passed
(1.957)
25,000
Stat
passed
(0.475)
passed
(0.611 )
passed
(0.383 )
passed
(0.276 )
passed
(0.225 )
passed
(0.217 )
Halfw
passed
(1.320e-03)
passed
(7.590e-05 )
passed
(2.780)
passed
(3.220)
passed
(9.440e-03 )
passed
(2.130)
100,000
Stat
passed
(0.852 )
passed
(0.939 )
passed
(0.884 )
passed
(0.854 )
passed
(0.897 )
passed
(0.859 )
Halfw
passed
(8.200e-04 )
passed
(4.470e-05 )
passed
(1.240)
passed
(1.320)
passed
(3.680e-03 )
passed
(1.000)
Table 3.7: Heidelberg and Welch diagnostic for the MCMC posterior estimates for n=626 fraud
losses in 41 banks, recorded from 01/2007 to 04/2010
3.2.4
Operational risk measurement
Once the estimates are suﬃciently accurate, we can compute the minimum capital require-
ment for diﬀerent banks. The operational risk VaR and ES at levels 0.95, 0.99 and 0.999
can be computed as well from the MCMC samples, using formulas (2.37) and (2.38). Since
data were scaled, to determine the capital requirement for each bank, data were returned
to their original scale by multiplying them by the asset size and dividing by a million.
The values obtained with the diﬀerent methods were compared to the current capital
requirement, which in most banks is calculated following the Basic Indicator Approach
(BIA). Tables 3.8–3.10 display the results for all banks.
Notice that, in general, the estimate of the minimum capital requirement obtained
from both the classical Peaks Over the Threshold and Bayesian inference is similar. How-
ever, in the Bayesian approach we have incorporated the threshold uncertainty and prior
information, leading to smaller estimates. Also notice that the capital requirement using
the BIA seems to overestimate (underestimate) the requirement. Figure 3.9 compares the
capital requirement using the BIA and the Bayesian estimate.

3.2.
An application to operational risk data
56
Capital requirement
Proportion of equity capital
Bank
BIA
POT
Bayesian (runs)
Equity
BIA
POT
Bayesian(runs)
10000
25000
100000
capital
10000
25000
100000
A
107
169
154.44
154.46
154.72
2422.19
4.42%
6.98%
6.38%
6.38%
6.39%
B
112
67
60.91
60.92
61.02
2554.29
4.38%
2.62%
2.38%
2.38%
2.39%
C
2
3
2.69
2.69
2.69
412. 97
0.48%
0.73%
0.65%
0.65%
0.65%
D
5
7
6.19
6.19
6.20
420.63
1.19%
1.66%
1.47%
1.47%
1.47%
E
282
266
242.43
242.48
242.88
4520.69
6.24%
5.88%
5.36%
5.36%
5.37%
F
322
314
285.97
286.02
286.51
9388.67
3.43%
3.34%
3.05%
3.05%
3.05%
G
4805
4904
4471.30
4472.09
4479.60
153078.65
3.14%
3.20%
2.92%
2.92%
2.93%
H(*)
2
5
4.57
4.57
4.58
439.00
0.46%
1.14%
1.04%
1.04%
1.04%
I
6239
4470
4076.17
4076.90
4083.75
103496.22
6.03%
4.32%
3.94%
3.94%
3.95%
J
14
29
26.86
26.87
26.91
762.68
1.84%
3.80%
3.52%
3.52%
3.53%
K(*)
71
126
114.63
114.65
114.84
3497.53
2.03%
3.60%
3.28%
3.28%
3.28%
L(*)
1
3
2.77
2.77
2.78
690.29
0.14%
0.43%
0.40%
0.40%
0.40%
M
2272
2232
2035.11
2035.47
2038.89
40489.16
5.61%
5.51%
5.03%
5.03%
5.04%
N
147
157
143.22
143.25
143.49
2200.09
6.68%
7.14%
6.51%
6.51%
6.52%
O
36
71
65.02
65.03
65.14
1052.59
3.42%
6.75%
6.18%
6.18%
6.19%
P(*)
39
64
58.71
58.73
58.82
1523.23
2.56%
4.20%
3.85%
3.86%
3.86%
Q
53
39
35.84
35.84
35.90
4298.29
1.23%
0.91%
0.83%
0.83%
0.84%
R
9
28
25.82
25.82
25.86
689.20
1.31%
4.06%
3.75%
3.75%
3.75%
S(*)
22
52
47.75
47.76
47.84
1182.59
1.86%
4.40%
4.04%
4.04%
4.05%
T(*)
37
147
134.33
134.35
134.58
2101.42
1.76%
7.00%
6.39%
6.39%
6.40%
U
4
1
1.06
1.06
1.06
148.81
2.69%
0.67%
0.71%
0.71%
0.71%
V
38
43
39.00
39.00
39.07
1508.55
2.52%
2.85%
2.59%
2.59%
2.59%
W
2049
1644
1498.64
1498.91
1501.43
40099.08
5.11%
4.10%
3.74%
3.74%
3.74%
X
843
868
791.35
791.49
792.82
43254.84
1.95%
2.01%
1.83%
1.83%
1.83%
Y(*)
237
322
294.06
294.11
294.60
7371.61
3.22%
4.37%
3.99%
3.99%
4.00%
Z
175
300
273.30
273.35
273.81
3365.10
5.20%
8.92%
8.12%
8.12%
8.14%
AA
88
102
92.60
92.62
92.78
1993.36
4.41%
5.12%
4.65%
4.65%
4.65%
BB
196
256
233.37
233.41
233.80
4196.14
4.67%
6.10%
5.56%
5.56%
5.57%
CC(*)
111
130
118.62
118.64
118.84
4302.47
2.58%
3.02%
2.76%
2.76%
2.76%
DD
60
130
118.38
118.41
118.60
1058.72
5.67%
12.28%
11.18%
11.18%
11.20%
EE(*)
17
82
74.58
74.59
74.72
1680.43
1.01%
4.88%
4.44%
4.44%
4.45%
FF
27
59
53.99
54.00
54.09
1601.32
1.69%
3.68%
3.37%
3.37%
3.38%
GG(*)
8
15
13.58
13.58
13.60
513.85
1.56%
2.92%
2.64%
2.64%
2.65%
HH
3527
2496
2276.19
2276.59
2280.42
80529.98
4.38%
3.10%
2.83%
2.83%
2.83%
II
1086
678
618.04
618.15
619.19
25590.18
4.24%
2.65%
2.42%
2.42%
2.42%
JJ
23
44
39.93
39.94
40.01
798.56
2.88%
5.51%
5.00%
5.00%
5.01%
KK(*)
19
26
23.32
23.32
23.36
724.57
2.62%
3.59%
3.22%
3.22%
3.22%
LL(*)
1
8
7.16
7.17
7.18
385.56
0.26%
2.07%
1.86%
1.86%
1.86%
MM
43
53
48.27
48.28
48.36
925.07
4.65%
5.73%
5.22%
5.22%
5.23%
NN
7
7
6.65
6.65
6.67
495.64
1.41%
1.41%
1.34%
1.34%
1.34%
OO
2
7
6.69
6.69
6.70
992.76
0.20%
0.71%
0.67%
0.67%
0.67%
Table 3.8: Minimum capital requirement (Millions of pesos) for fraud losses in 41 banks

3.2.
An application to operational risk data
57
VaR0.95
VaR0.99
VaR0.999
Bank
POT
Bayesian (Runs)
POT
Bayesian (Runs)
POT
Bayesian (Runs)
10000
25000
100000
10000
25000
100000
10000
25000
100000
A
8.26
8.25
8.26
8.28
18.55
15.78
15.79
15.82
46.13
28.69
28.67
28.68
B
3.26
3.25
3.26
3.27
7.32
6.22
6.23
6.24
18.19
11.32
11.31
11.31
C
0.14
0.14
0.14
0.14
0.32
0.27
0.27
0.28
0.80
0.50
0.50
0.50
D
0.33
0.33
0.33
0.33
0.74
0.63
0.63
0.63
1.85
1.15
1.15
1.15
E
12.97
12.95
12.96
13.00
29.13
24.77
24.78
24.83
72.41
45.04
45.01
45.03
F
15.30
15.28
15.29
15.33
34.36
29.22
29.24
29.29
85.42
53.13
53.09
53.12
G
239.28
238.87
239.12
239.69
537.16
456.86
457.11
458.03
1335.51
830.67
830.13
830.49
H(*)
0.24
0.24
0.24
0.25
0.55
0.47
0.47
0.47
1.37
0.85
0.85
0.85
I
218.13
217.76
217.99
218.51
489.70
416.49
416.72
417.55
1217.50
757.27
756.77
757.10
J
1.44
1.44
1.44
1.44
3.23
2.74
2.75
2.75
8.02
4.99
4.99
4.99
K(*)
6.13
6.12
6.13
6.14
13.77
11.71
11.72
11.74
34.24
21.30
21.28
21.29
L(*)
0.15
0.15
0.15
0.15
0.33
0.28
0.28
0.28
0.83
0.51
0.51
0.51
M
108.91
108.72
108.83
109.10
244.49
207.94
208.05
208.47
607.86
378.08
377.83
378.00
N
7.66
7.65
7.66
7.68
17.21
14.63
14.64
14.67
42.78
26.61
26.59
26.60
O
3.48
3.47
3.48
3.49
7.81
6.64
6.65
6.66
19.42
12.08
12.07
12.08
P(*)
3.14
3.14
3.14
3.15
7.05
6.00
6.00
6.01
17.54
10.91
10.90
10.91
Q
1.92
1.91
1.92
1.92
4.31
3.66
3.66
3.67
10.70
6.66
6.65
6.66
R
1.38
1.38
1.38
1.38
3.10
2.64
2.64
2.64
7.71
4.80
4.79
4.80
S(*)
2.56
2.55
2.55
2.56
5.74
4.88
4.88
4.89
14.26
8.87
8.87
8.87
T(*)
7.19
7.18
7.18
7.20
16.14
13.72
13.73
13.76
40.12
24.96
24.94
24.95
U
0.06
0.06
0.06
0.06
0.13
0.11
0.11
0.11
0.32
0.20
0.20
0.20
V
2.09
2.08
2.09
2.09
4.68
3.98
3.99
3.99
11.65
7.24
7.24
7.24
W
80.20
80.06
80.14
80.34
180.04
153.13
153.21
153.52
447.62
278.42
278.23
278.35
X
42.35
42.28
42.32
42.42
95.07
80.86
80.90
81.06
236.37
147.02
146.92
146.98
Y(*)
15.74
15.71
15.73
15.76
35.33
30.05
30.06
30.12
87.83
54.63
54.59
54.62
Z
14.63
14.60
14.62
14.65
32.83
27.93
27.94
28.00
81.63
50.77
50.74
50.76
AA
4.96
4.95
4.95
4.96
11.13
9.46
9.47
9.49
27.66
17.20
17.19
17.20
BB
12.49
12.47
12.48
12.51
28.04
23.84
23.86
23.91
69.70
43.36
43.33
43.35
CC(*)
6.35
6.34
6.34
6.36
14.25
12.12
12.13
12.15
35.43
22.04
22.02
22.03
DD
6.34
6.32
6.33
6.35
14.22
12.10
12.10
12.13
35.36
21.99
21.98
21.99
EE(*)
3.99
3.98
3.99
4.00
8.96
7.62
7.62
7.64
22.28
13.86
13.85
13.85
FF
2.89
2.88
2.89
2.89
6.49
5.52
5.52
5.53
16.13
10.03
10.02
10.03
GG(*)
0.73
0.73
0.73
0.73
1.63
1.39
1.39
1.39
4.05
2.52
2.52
2.52
HH
121.81
121.60
121.73
122.02
273.45
232.57
232.70
233.17
679.87
422.87
422.59
422.77
II
33.07
33.02
33.05
33.13
74.25
63.15
63.18
63.31
184.60
114.82
114.74
114.79
JJ
2.14
2.13
2.14
2.14
4.80
4.08
4.08
4.09
11.93
7.42
7.41
7.42
KK(*)
1.25
1.25
1.25
1.25
2.80
2.38
2.38
2.39
6.96
4.33
4.33
4.33
LL(*)
0.38
0.38
0.38
0.38
0.86
0.73
0.73
0.73
2.14
1.33
1.33
1.33
MM
2.58
2.58
2.58
2.59
5.80
4.93
4.93
4.94
14.42
8.97
8.96
8.97
NN
0.36
0.36
0.36
0.36
0.80
0.68
0.68
0.68
1.99
1.24
1.24
1.24
OO
0.36
0.36
0.36
0.36
0.80
0.68
0.68
0.68
2.00
1.24
1.24
1.24
Table 3.9: VaR at diﬀerent levels (Millions of pesos) for fraud losses in 41 banks

3.2.
An application to operational risk data
58
ES0.95
ES0.99
ES0.999
Bank
POT
Bayesian (Runs)
POT
Bayesian (Runs)
POT
Bayesian (Runs)
10000
25000
100000
10000
25000
100000
10000
25000
100000
A
11.60
12.16
12.16
12.18
21.89
19.69
19.69
19.73
49.46
32.60
32.58
32.59
B
4.57
4.79
4.80
4.81
8.63
7.76
7.77
7.78
19.51
12.86
12.85
12.85
C
0.20
0.21
0.21
0.21
0.38
0.34
0.34
0.34
0.86
0.57
0.57
0.57
D
0.46
0.49
0.49
0.49
0.88
0.79
0.79
0.79
1.98
1.31
1.31
1.31
E
18.21
19.08
19.10
19.13
34.36
30.9
30.92
30.97
77.65
51.17
51.14
51.16
F
21.48
22.51
22.53
22.56
40.53
36.45
36.47
36.53
91.59
60.36
60.33
60.35
G
335.82
351.95
352.20
352.77
633.71
569.94
570.19
571.11
1432.06
943.75
943.21
943.57
H(*)
0.34
0.36
0.36
0.36
0.65
0.58
0.58
0.58
1.46
0.97
0.96
0.96
I
306.15
320.85
321.07
321.60
577.71
519.57
519.80
520.64
1305.51
860.36
859.86
860.19
J
2.02
2.11
2.12
2.12
3.81
3.42
3.43
3.43
8.60
5.67
5.67
5.67
K(*)
8.61
9.02
9.03
9.04
16.25
14.61
14.62
14.64
36.71
24.19
24.18
24.19
L(*)
0.21
0.22
0.22
0.22
0.39
0.35
0.35
0.35
0.89
0.58
0.58
0.58
M
152.85
160.19
160.30
160.57
288.43
259.41
259.52
259.94
651.80
429.55
429.30
429.46
N
10.76
11.27
11.28
11.30
20.30
18.26
18.26
18.29
45.87
30.23
30.21
30.22
O
4.88
5.12
5.12
5.13
9.21
8.29
8.29
8.30
20.82
13.72
13.72
13.72
P(*)
4.41
4.62
4.62
4.63
8.32
7.48
7.49
7.50
18.81
12.39
12.39
12.39
Q
2.69
2.82
2.82
2.83
5.08
4.57
4.57
4.58
11.48
7.56
7.56
7.56
R
1.94
2.03
2.03
2.04
3.66
3.29
3.29
3.30
8.27
5.45
5.45
5.45
S(*)
3.59
3.76
3.76
3.77
6.77
6.09
6.09
6.10
15.29
10.08
10.07
10.08
T(*)
10.09
10.57
10.58
10.6
19.04
17.12
17.13
17.16
43.02
28.35
28.34
28.35
U
0.08
0.08
0.08
0.08
0.15
0.14
0.14
0.14
0.34
0.22
0.22
0.22
V
2.93
3.07
3.07
3.08
5.53
4.97
4.97
4.98
12.49
8.23
8.23
8.23
W
112.56
117.96
118.05
118.24
212.40
191.03
191.11
191.42
479.98
316.32
316.13
316.26
X
59.44
62.29
62.33
62.44
112.16
100.87
100.92
101.08
253.45
167.03
166.93
167.00
Y(*)
22.09
23.15
23.16
23.2
41.68
37.48
37.5
37.56
94.18
62.07
62.03
62.05
Z
20.53
21.51
21.53
21.56
38.73
34.84
34.85
34.91
87.53
57.69
57.65
57.67
AA
6.96
7.29
7.29
7.31
13.12
11.80
11.81
11.83
29.66
19.55
19.53
19.54
BB
17.53
18.37
18.38
18.41
33.08
29.75
29.76
29.81
74.74
49.26
49.23
49.25
CC(*)
8.91
9.34
9.34
9.36
16.81
15.12
15.13
15.15
37.99
25.04
25.02
25.03
DD
8.89
9.32
9.32
9.34
16.78
15.09
15.10
15.12
37.92
24.99
24.97
24.98
EE(*)
5.60
5.87
5.87
5.88
10.57
9.51
9.51
9.53
23.89
15.74
15.73
15.74
FF
4.06
4.25
4.25
4.26
7.65
6.88
6.89
6.90
17.29
11.40
11.39
11.39
GG(*)
1.02
1.07
1.07
1.07
1.92
1.73
1.73
1.73
4.35
2.87
2.86
2.86
HH
170.96
179.17
179.29
179.59
322.60
290.14
290.26
290.73
729.01
480.43
480.16
480.34
II
46.42
48.65
48.68
48.76
87.59
78.78
78.81
78.94
197.95
130.45
130.37
130.42
JJ
3.00
3.14
3.15
3.15
5.66
5.09
5.09
5.10
12.79
8.43
8.42
8.43
KK(*)
1.75
1.84
1.84
1.84
3.30
2.97
2.97
2.98
7.47
4.92
4.92
4.92
LL(*)
0.54
0.56
0.56
0.57
1.02
0.91
0.91
0.92
2.29
1.51
1.51
1.51
MM
3.63
3.80
3.80
3.81
6.84
6.15
6.16
6.17
15.46
10.19
10.18
10.19
NN
0.50
0.52
0.52
0.52
0.94
0.85
0.85
0.85
2.13
1.40
1.40
1.40
OO
0.50
0.53
0.53
0.53
0.95
0.85
0.85
0.85
2.14
1.41
1.41
1.41
Table 3.10: Expected Shortfall at diﬀerent levels (Millions of pesos) for fraud losses in 41 banks

3.2.
An application to operational risk data
59
Figure 3.9: Top: Bar plots of the Minimum capital requirement for fraud losses in each bank,
using the Basic Indicator Approach (Dark gray) and the Bayesian approach (Gray). Bottom:
Minimum capital requirement trend for fraud losses in each bank, using the Basic Indicator
Approach (Dark gray) and the Bayesian approach (Gray)

3.2.
An application to operational risk data
60
Figure 3.10: Minimum capital requirement for fraud losses in each bank using the Basic Indicator
Approach vs. Bayesian approach
For the estimates of Value-at-Risk for operational risk, we obtained similar values with
both methods for VaR 95%. However, for VaR 99% and 99.9%, the POT method provides
much higher estimates.
Finally, in Figure 3.9, we conﬁrm that in some cases the current capital estimate is
much higher with respect to the Bayesian one.
Figure 3.10 shows the current capital
estimate (Bayesian Indicator Approach) against the Bayesian estimate.
3.2.5
Grouped data
The forty-one banks considered were classifed into four groups according to their size and
origin: BAC, FIL, G-7 and MED. Diﬀerent models were ﬁtted to each group.
The ﬁrst group includes those banks linked to department stores while the second one
corresponds to subsidiary banks. G-7 groups the seven largest banks and MED consists
of medium size banks.

3.2.
An application to operational risk data
61
100,000 Runs
Parameter
BAC
Mean, Median, Std
All
Mean, Median, Std
α
0.682, 0.678, 0.097
0.552, 0.552, 0.029
β
0.012, 0.011, 0.002
0.014, 0.014,0.001
u
178.683, 182.513, 32.320
71.31, 72.980, 19.072
σ
322.281, 307.589, 127.466
94.173, 94.593, 21.109
ξ
-0.673, -0.645, 0.315
0.095, 0.089, 0.060
Table 3.11: Posterior MCMC estimates for BAC data (n=75)
If we observe the scaled losses in the diﬀerent groups (Figure 3.11), we may notice
that there are much fewer threshold exceedances in the ﬁrst two groups than in the last
two. Also, we should highlight the fact that we have only 75 observations for BAC and
50 for FIL, while the number of observations is larger for G-7 and MED (270 and 231,
respectively). This makes the distribution of BAC and FIL not really heavy-tailed when
all data are pooled.
We might expect that the ﬁtted distributions diﬀer from group to group, as it is
shown in Tables 3.11–3.14, where the shape parameter of BAC clearly indicates that
its distribution is not heavy-tailed as it is for the other groups. Figure 3.12 shows the
ﬁtted densities using the estimated parameters of the Gamma and GPD. In the ﬁrst two
cases, the use of the GPD does not appear appropriate, however for the last two groups
the estimated densities seem to ﬁt the data well. Something similar happens when we
estimate the parameters using all data together, in this case the estimated density shows
a good ﬁt with the data.
Among other things, one might think that a bad ﬁt is due to the fact that ξ < 0
imposes an upper bound on the density. This does not seem to be appropriate for the
BAC data. For FIL data, the threshold seems a bit too low. Additionally, in both cases,
we have a small number of observations.

3.2.
An application to operational risk data
62
Figure 3.11: Scaled losses for fraud data subgroups: BAC (75 observations), FIL (50 observa-
tions), G-7 (270 observations) and MED (231 observations). (Dashed gray line is the threshold
value)

3.2.
An application to operational risk data
63
100,000 Runs
Parameter
FIL
Mean, Median, Std
All
Mean, Median, Std
α
0.971, 0.956, 0.203
0.552, 0.552, 0.029
β
0.005, 0.005, 0.001
0.014, 0.014, 0.001
u
177.035, 176.351, 11.984
71.310, 72.980, 19.072
σ
56.023, 53.075, 19.651
94.173, 94.593, 21.109
ξ
0.147, 0.145, 0.065
0.095, 0.089, 0.060
Table 3.12: Posterior MCMC estimates for FIL data (n=50)
100,000 Runs
Parameter
G7
Mean, Median, Std
All
Mean, Median, Std
α
0.839, 0.834, 0.080
0.552, 0.552, 0.029
β
0.019, 0.019, 0.003
0.014, 0.014,0.001
u
38.024, 34.264, 11.710
71.31, 72.980, 19.072
σ
34.918, 34.482, 4.679
94.173, 94.593, 21.109
ξ
0.180, 0.179, 0.049
0.095, 0.089, 0.060
Table 3.13: Posterior MCMC estimates for G7 data (n=270)
100,000 Runs
Parameter
MED
Mean, Median, Std
All
Mean, Median, Std
α
0.864, 0.860, 0.111
0.552, 0.552, 0.029
β
0.149, 0.147, 0.036
0.014, 0.014, 0.001
u
6.930, 6.742, 2.502
71.310, 72.980, 19.072
σ
18.102, 16.024, 8.695
94.173, 94.593, 21.109
ξ
0.351, 0.354, 0.093
0.095, 0.089, 0.060
Table 3.14: Posterior MCMC estimates for MED data (n=231)

3.2.
An application to operational risk data
64
Figure 3.12:
Original data and ﬁtted densities using the MCMC posterior estimates of the
Gamma and GPD parameters (dashed gray line is the threshold value)

3.2.
An application to operational risk data
65
3.2.5.1
Introducing a Reversible Jump Markov Chain Monte Carlo
algorithm
Due to the poor ﬁt of the GPD for BAC and FIL data, we consider two models :
1. A mixture of a Gamma distribution and the GPD (Original model, M).
2. A Gamma distribution (M ′).
To determine which of these models is more appropriate for the diﬀerent data sets, we
introduce a reversible jump step. The algorithm to do so is as follows:
1. Update the parameters θ and θ′ conditional on the model M or M ′, respectively,
using the Metropolis–Hastings algorithm.
2. Update the model conditional on the current parameter values using the following
steps:
a) Propose to move from model M to M ′.
b) Accept this proposed move with probability A.
where:
M = Model 1,
M ′ = Model 2,
θ = {α, β, u, σ, ξ} ,
θ′ = {α′, β′} .
Auxiliary variables:
u = {u1, u2, u3} .

3.2.
An application to operational risk data
66
A function such that:
α′ = α,
β′ = β,
u1 = u,
u2 = σ,
u3 = ξ.
This is the identity function and therefore |J| = 1.
In order to preserve the support of u, σ and ξ we can set:
ui ∼TN (ai, bi, µi, σi) ,
i = 1, 2
and
u3 ∼N (0, σ3) ,
where TN denotes the truncated normal distribution with mean µi, standard deviation σi,
and lower and upper truncation points ai and bi, respectively.
The acceptance probability is
min {1, A}
with
A = π (M ′, θ′ | x) P (M | M ′) p (u1, u2, u3)
π (M, θ | x) P (M ′ | M)
,
(3.11)
where
 π denotes the posterior distribution over parameter and model space.
 P is the probability of proposing to move to model M given the current state of the
chain is M ′, and viceversa.
 p is a proposal distribution.
See section A.5 of the Appendix for more details about RJMCMC methods.

3.2.
An application to operational risk data
67
For performing the RJ step:
α = α′,
β = β′,
u = u1,
σ = u2,
ξ = u3.
with acceptance probability:
min

1, A−1	
.
3.2.5.2
Results
Results for 20,000 iterations are shown in Tables 3.15–3.18. Figure 3.13 shows the be-
haviour of the chain when introducing the reversible jump step. We may notice that in
most cases the chain explores Model 2 (M ′) and immediately goes to Model 1 (M) and
stays there, except for the FIL data.
For G7 and MED data we expect Model 1 to be more suitable than Model 2. For
FIL and BAC data, Model 2 would seem to be a more suitable choice. Nonetheless, for
BAC data, the algorithm indicates that Model 1 is more appropriate. That might be a
consequence of the chosen distribution for the auxiliary variables u1, u2 and u3.
20,000 Runs
Parameter
BAC
Mean, Median, Std
α
0.582, 0.580, 0.057
β
0.009, 0.009, 8.364e-05
u
180.947, 182.513, 30.576
σ
309.573, 330.354, 79.067
ξ
-0.651, -0.648, 0.218
Table 3.15: Posterior RJMCMC estimates for BAC data (n=75)

3.2.
An application to operational risk data
68
Figure 3.13: Jumps between Models 1 and 2 for the diﬀerent fraud data subgroups

3.2.
An application to operational risk data
69
20,000 Runs
Parameter
FIL
Mean, Median, Std
α
0.897, 0.880, 0.171
β
0.054, 0.054, 0.001
u
194.388, 196.087, 5.543
σ
58.737, 58.442, 1.806
ξ
0.167, 0.169, 0.197
Table 3.16: Posterior RJMCMC estimates for FIL data (n=50)
20,000 Runs
Parameter
G7
Mean, Median, Std
α
0.856, 0.854, 0.071
β
0.019, 0.019, 0.002
u
54.914, 54.408, 8.646
σ
29.465, 29.478, 1.254
ξ
0.193, 0.198, 0.007
Table 3.17: Posterior RJMCMC estimates for G7 data (n=270)
20,000 Runs
Parameter
MED
Mean, Median, Std
α
0.874, 0.890, 0.105
β
0.158, 0.146, 0.025
u
7.142, 6.946, 5.223
σ
21.997, 26.316, 8.555
ξ
0.360, 0.350, 0.099
Table 3.18: Posterior RJMCMC estimates for MED data (n=231)
Tables 3.19–3.22 display the estimates of VaR and ES at diﬀerent levels, for each group,
before and after introducing the reversible jump (RJ) step. We can observe that the RJ
step introduces some variation in the estimates and leads to higher estimates of VaR at
diﬀerent levels for all the groups, except for G7.

3.2.
An application to operational risk data
70
Before RJMCMC
After RJMCM
Bank
VaR0.95
VaR0.99
VaR0.999
VaR0.95
VaR0.99
VaR0.999
E
16.97
33.44
40.42
17.35
34.95
42.34
J
1.88
3.71
4.48
1.92
3.87
4.69
U
0.07
0.15
0.18
0.08
0.15
0.19
V
2.73
5.38
6.50
2.79
5.62
6.81
OO
0.47
0.92
1.11
0.47
0.96
1.17
Table 3.19: VaR at diﬀerent levels before and after RJMCMC-BAC data
Before RJMCMC
After RJMCMC
Bank
VaR0.95
VaR0.99
VaR0.999
VaR0.95
VaR0.99
VaR0.999
B
5.10
7.25
11.34
5.39
7.84
12.72
K(*)
9.61
13.65
21.34
10.15
14.75
23.93
L(*)
0.23
0.33
0.52
0.25
0.36
0.58
P(*)
4.92
6.99
10.93
5.20
7.55
12.26
S(*)
4.00
5.69
8.89
4.23
6.14
9.97
T(*)
11.26
15.99
25.01
11.89
17.28
28.04
Y(*)
24.64
35.01
54.75
26.03
37.83
61.39
CC(*)
9.94
14.12
22.09
10.50
15.26
24.76
JJ
3.35
4.75
7.44
3.54
5.14
8.34
KK(*)
1.95
2.78
4.34
2.06
3.00
4.87
LL(*)
0.60
0.85
1.33
0.63
0.92
1.50
NN
0.56
0.79
1.24
0.59
0.86
1.39
Table 3.20: VaR at diﬀerent levels before and after RJMCMC-FIL data
Before RJMCMC
After RJMCMC
Bank
VaR0.95
VaR0.99
VaR0.999
VaR0.95
VaR0.99
VaR0.999
G
149.87
262.95
493.66
131.43
222.90
416.50
I
136.63
239.71
450.03
119.82
203.20
379.70
M
68.22
119.68
224.69
59.82
101.45
189.57
W
50.23
88.13
165.46
44.05
74.71
139.60
HH
76.30
133.86
251.31
66.91
113.47
212.03
X
26.53
46.54
87.37
23.26
39.45
73.71
II
20.72
36.35
68.24
18.17
30.81
57.57
Table 3.21: VaR at diﬀerent levels before and after RJMCMC-G7 data

3.2.
An application to operational risk data
71
Before RJMCMC
After RJMCMC
Bank
VaR0.95
VaR0.99
VaR0.999
VaR0.95
VaR0.99
VaR0.999
A
2.09
4.90
13.07
3.11
7.58
20.45
C
0.04
0.09
0.23
0.05
0.13
0.36
D
0.08
0.20
0.52
0.12
0.30
0.82
F
3.86
9.08
24.20
5.76
14.04
37.87
H(*)
0.06
0.15
0.39
0.09
0.22
0.61
N
1.94
4.55
12.12
2.89
7.03
18.97
O
0.88
2.06
5.50
1.31
3.19
8.61
Q
0.48
1.14
3.03
0.72
1.76
4.75
R
0.35
0.82
2.18
0.52
1.27
3.42
Z
3.69
8.68
23.13
5.51
13.42
36.19
AA
1.25
2.94
7.84
1.87
4.55
12.26
BB
3.15
7.41
19.75
4.70
11.46
30.90
DD
1.60
3.76
10.02
2.39
5.81
15.68
EE(*)
1.01
2.37
6.31
1.50
3.66
9.88
FF
0.73
1.71
4.57
1.09
2.65
7.15
GG(*)
0.18
0.43
1.15
0.27
0.67
1.80
MM
0.65
1.53
4.09
0.97
2.37
6.39
Table 3.22: VaR at diﬀerent levels before and after RJMCMC-MED data
3.2.5.3
An alternative for FIL and BAC data
This section is based on the paper by Venturini et al. (2008), where a Bayesian approach
for the estimation of tail probabilities of heavy-tailed distributions is proposed, based on
a mixture of Gamma distributions in which the mixing occurs over the shape parameter.
The procedure is as follows.
Let Y be a positive random variable. The Gamma Shape Mixture (GSM) model is
deﬁned as:
f (y | w1, ..., wJ, θ) =
J
X
j=1
wj fj (y | θ) ,
(3.12)
where fj (y | θ) =
θj
Γ(j)yj−1e−θy, is the density function of a Gamma(j, θ) random variable,
with parameters j > 0 (shape) and θ > 0 (scale). We assume that the number of com-
ponents J is known and ﬁxed, whereas w = (w1, ..., wJ) is an unknown vector of mixture
weights. The GSM model has two useful properties:
1. 1/θ is a scale parameter for the whole model, since f (y | w1, ..., wJ, θ) = θ · f(θ ·

3.2.
An application to operational risk data
72
y|w1, ..., wJ, 1).
2. Its moments are convex combinations of the moments of Gamma(j, θ) variables, so
that the mth moment is given by
E [Y m | w1, ..., wJ, θ] =
J
X
j=1
wjE

Y m
j
| θ

=
J
X
j=1
wj
Qm
l=1 (j + l −1)
θm
.
We assume that θ and w are independent a priori and we specify the following conjugate
prior distributions:
θ ∼Gamma (α, β) ,
w = (w1, ..., wJ) ∼DJ
 1
J , ..., 1
J

,
where DJ (a1, ..., aJ) denotes the Dirichlet distribution with parameters a1, ..., aJ > 0.
In practice, setting the prior hyperparameters equal to 1/J tends to produce posterior
distributions where only a small subset of the J mixture weights will have high prior
probability to be selected at each iteration of the MCMC.
Given a sample y = (y1, ..., yn) of i.i.d. observations from (3.12), the likelihood is
L (w, θ | y) =
n
Y
i=1
J
X
j=1
wj fj (yi | θ) .
(3.13)
This expression is however intractable because it includes Jn diﬀerent terms.
Given y = (y1, ..., yn) from (3.12), we can associate to each yi an integer xi between 1
and J that identiﬁes the component of the mixture generating observation yi . Thus, the
variable xi takes value j with prior probability wj, 1 ≤j ≤J. The vector x = (x1, ..., xn)
of component labels is the missing data part of the sample since it is not observed.
Suppose the missing data x1, ..., xn were available. Then the model could be written as
p (y1, ..., yn | x1, ..., xn, θ) =
θ
Pn
i=1 xi
Qn
i=1 Γ (xi)
 
n
Y
i=1
yxi−1
i
!
e−θ Pn
i=1 yi.
(3.14)
Thus, using (3.14) and the priors, the posterior distribution is
p (w1, ..., wJ, θ | y1, ..., yn, x1, ..., xn) ∝


JY
j=1
w
(1/J)+nj−1
j

θα+Pn
i=1 xie−(β+Pn
i=1 yi)θ,
(3.15)

3.2.
An application to operational risk data
73
where nj = Pn
i=1 I (xi = j) , j = 1, ..., J. The main consequence of this conditional de-
composition is that, for a given missing data vector (x1, ..., xn), the conjugacy is preserved
and, therefore, the simulation can be easily performed, conditional on the missing data
x1, ..., xn.
To compute the posterior distribution, after having integrated out θ, the full conditional
distribution of the mixture weights is given by
p (w1, ..., wJ | y1, ..., yn, x1, ..., xn) ∝
JY
j=1
w
(1/J)+nj−1
j
,
(3.16)
that is, the Dirichlet distribution DJ
  1
J + n1, ..., 1
J + nJ

.
The full conditional probability of the ith missing label is then given by
p
 xi | y, x(−i), w

=
JP
j=1
wj
yj−1
i

α + P
(−i) xr

j
Γ (j) (β + Pn
r=1 yr)j I (xi = j)
JP
k=1
wk
yk−1
i

α + P
(−i) xr

k
Γ (k) (β + Pn
r=1 yr)k
,
(3.17)
where x(−i) is the x = (x1, ..., xn) vector with the ith element deleted, P
(−i) xr denotes the
sum of all the component labels except for the ith one, and (n)k = n(n + 1) · · · (n + k −1)
is the Pochhammer symbol. We assume that α is an integer, for computation speed, and
to avoid overﬂow errors. The integration of θ implies that the missing data are no longer
independent.
For a given value of J, a strategy for choosing α and β is as follows:
1. Compute eθ = J/max (y1, ..., yn) and check that 1/eθ ≤min(y1, ..., yn); the idea is
that, on average, θ should take values that allow the set of Gamma distributions in
(3.12) to completely span the range of observed values (the last Gamma distribu-
tion should have a mean not smaller than the maximum observation and the ﬁrst
Gamma distribution a mean not greater than the minimum observation). Hence eθ
is a candidate for the prior mean α/β.

3.2.
An application to operational risk data
74
2. Choose a value ω for the weight of the prior information. Values between 0.2 and
0.5 are usually reasonable choices. Set β to
ω ·
n
X
i=1
yi/1 −ω.
(3.18)
3. Set α to be the closest integer to the quantity eθ · β.
Regarding the choice of J, a small value of J can create a severe limitation to the model,
as the set of densities available in the class being mixed may not be suﬃciently rich with
elements that have a large mean. On the other hand, too large a value does not cause
serious diﬃculties as the ﬁt is often robust when there are several Gamma distributions in
the class that can serve as building blocks for a particular mixture component. However,
too large a J may cause numerical problems.
θ
Mean, median
BAC
0.685, 0.642
FIL
0.462, 0.462
Table 3.23: Estimates of θ from the GSM procedure
RJMCMC
GSM
Bank
VaR0.95
VaR0.99
VaR0.999
VaR0.95
VaR0.99
VaR0.999
E
16.97
33.44
40.42
20.41
34.09
37.28
J
1.88
3.71
4.48
2.26
3.78
4.13
U
0.07
0.15
0.18
0.09
0.15
0.16
V
2.73
5.38
6.50
3.28
5.48
6.00
OO
0.47
0.92
1.11
0.56
0.94
1.03
Table 3.24: VaR at diﬀerent levels using the RJMCMC and GSM procedure-BAC data
For the BAC and FIL data, the above model was ﬁtted using a Gibbs sampler. J
was chosen by trial and error, the best value was about 380 and 700, respectively. This
value was used to compute eθ, α and β, following the strategy above. Results are shown
in Table 3.23. VaR estimates at diﬀerent levels for the RJ step and the GSM procedure
are displayed in tables 3.24–3.25, while ﬁtted densities are shown in Figures 3.14 and 3.15.

3.2.
An application to operational risk data
75
RJMCMC
GSM
Bank
VaR0.95
VaR0.99
VaR0.999
VaR0.95
VaR0.99
VaR0.999
B
5.10
7.25
11.34
9.73
17.73
21.05
K(*)
9.61
13.65
21.34
18.31
33.37
39.61
L(*)
0.23
0.33
0.52
0.44
0.81
0.96
P(*)
4.92
6.99
10.93
9.38
17.09
20.29
S(*)
4.00
5.69
8.89
7.63
13.90
16.50
T(*)
11.26
15.99
25.01
21.46
39.11
46.42
Y(*)
24.64
35.01
54.75
46.98
85.61
101.61
CC(*)
9.94
14.12
22.09
18.95
34.53
40.99
JJ
3.35
4.75
7.44
6.38
11.63
13.80
KK(*)
1.95
2.78
4.34
3.72
6.79
8.06
LL(*)
0.60
0.85
1.33
1.14
2.09
2.48
NN
0.56
0.79
1.24
1.06
1.94
2.30
Table 3.25: VaR at diﬀerent levels using the RJMCMC and GSM procedure-FIL data
Figure 3.14: GSM for BAC data
The GSM procedure yields higher estimates of VaR for both groups and the density is well
approximated by the mixture. It appears that, in this case, the GSM model represents a
good alternative for ﬁtting the data.

3.3.
Conclusions of the Chapter
76
Figure 3.15: GSM for FIL data
3.3
Conclusions of the Chapter
In this chapter we have introduced a mixture model developed by Behrens et al. (2004)
that has signiﬁcantly inﬂuenced the development of new Bayesian models. This model
uses a Gamma distribution for observations below the threshold, and a GPD for those
above it. One of its main features is that the threshold is considered as another parameter
in the model.
By using real and simulated data, we have identiﬁed the potential of this model for the
estimation of operational risk measures, particularly the Value-at-Risk and the Expected
Shortfall. At the same time, we have conﬁrmed the poor performance of the Basic Indicator
approach.
Furthermore, we have explored model selection using RJMCMC methods. As we have
seen, these methods allow us to select the most suitable model while performing parameter
estimation. It is worth pointing out that the selection of the “best” model does not mean
that it is generally the most appropriate to model our data. Thus, in some cases, other
alternatives should be considered, as it was the case for FIL and BAC data, where the
GSM model presented a better overall performance than previous models.

3.3.
Conclusions of the Chapter
77
This leads us to consider that Bayesian models can be used in many diﬀerent ways in
the study of extremes, with the advantage of considering the uncertainty implicit in the
parameters and the prior information available.

Chapter 4
Prior elicitation and analysis
An important question that arises in our Bayesian analysis is: How sensitive are the
posterior results to variations in the prior?
The issue of prior selection is one of the most controversial aspects of Bayesian theory.
An interesting discussion about these issues can be found in Irony and Singpurwalla (1997).
It is well-known that the prior can have an impact on posterior results when obser-
vations are scarce, as it is the case for operational risk data. Diﬀerent priors may lead
to diﬀerent posterior estimates and hence it is essential to ﬁnd an appropriate prior by
considering the context of the problem.
In the Bayesian analysis of extremes, authors such as Cabras et al. (2010) or Castel-
lanos and Cabras (2007) have used non-informative priors for the GPD parameters, specif-
ically the Jeﬀreys prior (see Appendix B), because of their simplicity. Jeﬀreys priors are
useful in situations in which we would like the observations to “speak for themselves”;
however, this assumption is not always realistic, especially when a considerable part of the
analysis is based on expert opinion.
In this chapter we consider two important risk measures which are based on the shape
and location parameters of the GPD: The Value-at-Risk (VaR) and the Expected Shortfall
(ES). Both measures were deﬁned in Chapter 2. Recall that if the excess distribution of
78

4.1.
The non-informative prior
79
a loss variable X above a high threshold u is approximated by a GPD with parameters σ
and ξ, and if 1 −q < P (X > u),
VaRq
=
u + σ
ξ
 1 −q
¯F (u)
−ξ
−1
!
,
(4.1)
ESq
=
VaRq
1 −ξ + σ −ξu
1 −ξ .
(4.2)
These measures are commonly used in ﬁnance and risk management. Therefore, experts
are familiar with them.
In Section 4.1, we study the prior and posterior distributions of these two risk measures
when using the Jeﬀreys prior. In Section 4.2, we study the prior elicitation from expert
opinion in terms of VaR.
4.1
The non-informative prior
In this section, we study the performance of the Jeﬀreys prior for the GPD parameters.
A complete derivation of this prior is provided in Appendix B and leads to
π (σ, ξ) ∝σ−1 (1 + ξ)−1 (1 + 2ξ)−1/2 , −0.5 < ξ, σ > 0.
(4.3)
For ξ = 0, π (σ, ξ) corresponds to the Jeﬀreys’s prior for the scale parameter of the expo-
nential distribution.
Equation (4.3) implies that σ and ξ are a priori independent. Notice that only the
marginal prior for σ is improper.
We obtained samples from the prior distribution of VaR0.99 and ES0.99 by considering
diﬀerent scenarios. In all cases we set a truncated normal for the threshold, using diﬀerent
data sets for the lower bound e1 in (3.5): The fraud data described in Section 3.2.1, and
samples from Exponential(0.1), Log-normal(0,1) and Gamma(2,0.5) distributions. The
improper prior of σ was approximated by a Log-normal(0, 1.25).
The prior for ξ was
derived from Equation (4.3) by introducing an appropriate normalizing constant.

4.1.
The non-informative prior
80
Figure 4.1: Prior distributions: σ ∼LN(0, 1.25) ; u ∼TN(min (fraud data)); ξ ∼Jeﬀreys prior
To study the eﬀect of these parameters on the prior distribution of VaR and ES, we
ﬁxed one of them and kept the priors speciﬁed above unchanged for the rest.
We ﬁxed values of 1, 10 and 100 for σ; quantiles 0.5, 0.7 and 0.9 for the threshold u;
and −0.3, 0.05 and 0.45 for ξ. Then, we combined all these scenarios and compared the
prior distributions obtained. Results for VaR0.99 and ES0.99 can be observed in Figure 4.1
and Figures D.1–D.12 shown in Appendix D.
From the graphs, one can note the same pattern in all data sets when we ﬁx a speciﬁc
parameter.
The choice of the prior for the scale σ and shape ξ parameters strongly
inﬂuences the distribution of VaR and ES.
The Jeﬀreys prior for ξ (Figure 4.1) yields values of ξ very close to −0.5 and most of
them are concentrated around this number, which is completely unrealistic in practice.
On the other hand, although the prior for σ is reasonable, we should notice that for the

4.2.
Elicitation
81
prior in equation (4.3), σ and ξ are independent. As it was pointed out in Section 3.1.1, a
priori negative dependence between these parameters is expected (Coles and Tawn, 1996).
This shows us that, at least in the context of extreme data, it is not appropriate to adopt
a non-informative prior.
In the next section, we propose a subjective prior, based on the VaR measure.
4.2
Elicitation
As it has been mentioned in the previous chapters, one of the requirements of the Ad-
vanced Measurement Approach (AMA) is the inclusion of expert opinion. Additionally, the
scarcity of data has prompted us to turn to experts through the elicitation of information,
which is not an easy task.
In this section, the objective is to facilitate prior elicitation and use a prior distribution
that reﬂects expert opinion faithfully, providing realistic values for the parameters involved.
For that purpose, we have chosen the Value-at-Risk. Based on the prior for σ and ξ
proposed in the previous chapter, we introduce the following prior.
Consider two high quantiles q1 < q2 (usually q1 = 0.99 and q2 = 0.999) and the
Value-at-Risk at those levels. Recall that if the GPD approximation is used above a high
threshold u and 1 −q1 < P (X > u),
VaRq1 = u + σ
ξ

k−ξ
1
−1

, VaRq2 = u + σ
ξ

k−ξ
2
−1

,
(4.4)
where ki = (1 −qi) /P (X > u) , i = 1, 2.
Since we need to guarantee that u < VaRq1 < VaRq2, we can follow a similar approach
to the one discussed in Section 3.1.1. More speciﬁcally, we work with the diﬀerences
d1 = (VaRq1 −u) = σ
ξ

k−ξ
1
−1

, d2 = (VaRq2 −u) −(VaRq1 −u) = σ
ξ

k−ξ
2
−k−ξ
1

and assume that π (di) ∼Gamma (ai, bi) , i = 1, 2 and that d1 is independent of d2. In
comparison to the prior for ξ and σ from Section 3.1.1, this approach guarantees that

4.2.
Elicitation
82
Figure 4.2: Loss distribution used in the elicitation process
indeed VaRq1 > u. The derivation of π (σ, ξ) is analogous to the transformation method
from Section 3.1.1. We get:
π (σ, ξ) ∝
h
σ
ξ

k−ξ
1
−1
ia1−1
exp
h
−b1
n
σ
ξ

k−ξ
1
−1
oi
×
h
σ
ξ

k−ξ
2
−k−ξ
1
ia2−1
exp
h
−b2
n
σ
ξ

k−ξ
2
−k−ξ
1
oi
×
−σ
ξ2
h
(k1k2)−ξ (log k2 −log k1) −k−ξ
2 log k2 + k−ξ
1 log k1
i .
(4.5)
The hyperparameters ai and bi are determined by measures of location and variability
in prior belief. Experts are asked for estimates of the median and 90% quantiles of each
of the di’s. Parameter estimates may be obtained by solving numerically for ai and bi.
We illustrate the elicitation process with a simple example. Suppose we have an expert
who can provide us with information about quantiles. We could start by showing him/her
the plot in Figure 4.2. This plot displays the loss distribution and three diﬀerent points:
1. The point above which a loss is considered extreme (EL).
2. The VaR at level q1=0.99.
3. The VaR at level q2=0.999.

4.2.
Elicitation
83
Elicited Value
ai
bi
q0.5,1 = 10, q0.9,1 = 100
0.339
0.001
q0.5,2 = 5, q0.9,2 = 45
0.362
0.023
Table 4.1: Gamma parameters obtained from elicited quantiles (ﬁctitious expert)
Next, we ask him/her to think of how diﬀerent these quantities are from one another.
That is, how large are the intervals [EL, VaRq1] and [VaRq1, VaRq2].
Then, we ask him/her for the median and 90% quantiles of these diﬀerences.
We
denote these values by: q0.5,1(median of the ﬁrst diﬀerence), q0.5,2 (median of the second
diﬀerence), q0.9,1 (0.9 quantile of the ﬁrst diﬀerence) and q0.9,2 (0.9 quantile of the second
diﬀerence). Once we know these quantiles, we can obtain the values of ai and bi by solving
the following equations:
Fx (q0.5,1, a1, b1) = 0.5
and Fx (q0.9,1, a1, b1) = 0.9,
(4.6)
Fx (q0.5,2, a2, b2) = 0.5
and Fx (q0.9,2, a2, b2) = 0.9,
(4.7)
where Fx is a Gamma(ai, bi). This can be solved numerically. For instance, we may use
the function get.gamma.par from the rriskDistributions R package. Table 4.1 shows
the results for some speciﬁc quantiles.
Now, keeping the priors previously proposed for the rest of the parameters, we can
derive the posterior distribution:
log p(θ | x) =
K +
nP
i=1
I (xi < u) [αlogβ −logΓ (α) + (α −1) log xi −βxi]
nP
i=1
I (xi ≥u) log

1 −
u´
0
βα
Γ(α)tα−1e−βtdt

−
nP
i=1
I (xi ≥u) log σ
−1+ξ
ξ
nP
i=1
I (xi ≥u) log
h
1 + ξ(xi−u)
σ
i
+(a −1)log α=bαa + (c −1)log

α
β

−d

α
β

+ log

α
β2

+ 1
2σu (uµu)2 + (a1 −1)log
h
σ
ξ

k−ξ
1
−1
i
−b1
σ
ξ

k−ξ
1
−1

+ (a2 −1)log σ
ξ

k−ξ
2
−k−ξ
1

−b2
σ
ξ

k−ξ
2
−k−ξ
1

+log
 σ
ξ2
h
(k1k2)−ξ (log k2 −log k1) −k−ξ
2 log k2 + p−ξ
1 log k1
i ,
(4.8)

4.2.
Elicitation
84
Hyperparameter
a1
b1
a2
b2
Expert 1
0.1
0.005
0.9
0.0302
Table 4.2: Hyperparameters for a ﬁctitious expert
10,000 runs
Parameter
Mean
Median
Std
σ
66.494
65.533
15.056
ξ
0.074
0.048
0.077
VaR0.99
171.740
141.086
97.729
ES0.99
254.513
223.150
114.117
Table 4.3: Fraud data: Prior MCMC estimates, using the opinion of a ﬁctitious expert and the
prior in Equation (4.5)
10,000 runs
Parameter
Mean
Median
Std
α
0.559
0.558
0.031
β
0.015
0.014
0.002
u
49.404
43.777
21.685
σ
69.061
64.355
19.288
ξ
0.202
0.207
0.068
VaR0.99
363.684
360.392
34.825
ES0.99
532.386
527.596
54.286
Table 4.4: Posterior MCMC estimates, using the opinion of a ﬁctitious expert for n=626 fraud
losses in 41 banks, recorded from 01/2007 to 04/2010
where α, β, a1, a2, b1 and b2 are hyperparameters from the Gamma distribution and K is
a normalizing constant.
To study the performance of the prior, we obtained samples of VaR0.99 and ES0.99
from the prior and the corresponding posterior. Again, we used the real data described
in Section 3.2.1. This time the main purpose is to study the behaviour of the posterior
when the prior is elicited from an expert. For simplicity, we pooled all the data together
rather than look at each bank separately, as in Chapter 3. It is important to mention that
since real experts were not available, we chose the hyperparameters so that the MCMC
algorithm was reasonably eﬃcient. Table 4.2 shows the hyperparameters of a ﬁctitious
expert while Tables 4.3 and 4.4 and Figures 4.3–4.7 display the results of this exercise.

4.2.
Elicitation
85
Figure 4.3: Trace plots and histograms of prior samples, using the opinion of a ﬁctitious expert
and the prior in Equation (4.5)
Figure 4.4: Prior samples (left) and contour plot (right) of the joint distribution of σ and ξ
(truncated at 0), using the opinion of a ﬁctitious expert and the prior in Equation 4.5

4.2.
Elicitation
86
Figure 4.5: Posterior samples (left) and contour plots (right) of the joint posterior distribution
of σ and ξ, using the opinion of a ﬁctitious expert for n=626 fraud losses in 41 banks, recorded
from 01/2007 to 04/2010
Parameter
Eﬀective Sample Size
σ
9357.655
ξ
9735.053
VaR0.99
8878.431
ES0.99
9122.453
Table 4.5: Eﬀective Sample Size of prior samples, using the opinion of a ﬁctitious expert and
the prior in Equation (4.5)
From the ﬁgures, the contour plot of the prior is similar to that of a non-informative
prior; however, when look at the posterior, we can observe how the prior inﬂuences the
result. We can notice that there has been a slight reduction in uncertainty from prior
to posterior. The location of the density has also changed and the posterior distribution
captures the negative correlation between parameters σ and ξ.
Some convergence diagnostics are also shown in Tables 4.5–4.8. In all cases convergence
seems to be achieved and the eﬀective sample size is large enough.

4.2.
Elicitation
87
Figure 4.6: Histograms of posterior samples, using the opinion of a ﬁctitious expert for n=626
fraud losses in 41 banks, recorded from 01/2007 to 04/2010
Stationarity
Halfwidth Mean
Parameter
Result
p-value
Result
Halfwidth
u
passed
0.889
passed
1.029
σ
passed
0.916
passed
0.280
ξ
passed
0.240
passed
0.001
VaR0.99
passed
0.909
passed
2.103
ES0.99
passed
0.913
passed
2.348
Table 4.6: Heidelberg and Welch diagnostics for prior samples, using the opinion of a ﬁctitious
expert and the prior in Equation 4.5

4.2.
Elicitation
88
Figure 4.7: VaR0.99 and ES0.99 prior (left) and posterior (right) distribution, using the opinion
of a ﬁctitious expert for n=626 fraud losses in 41 banks, recorded from 01/2007 to 04/2010
Parameter
Eﬀective Sample Size
α
4636.1564
β
3881.205
u
949.799
σ
874.672
ξ
1188.032
VaR0.99
1941.437
ES0.99
6504.822
Table 4.7: Eﬀective Sample Size of posterior samples, using the opinion of a ﬁctitious expert for
n=626 fraud losses in 41 banks, recorded from 01/2007 to 04/2010
Stationarity
Halfwidth Mean
Parameter
Result
p-value
Result
Halfwidth
α
passed
0.788
passed
8.240e-04
β
passed
0.844
passed
5.110e-05
u
passed
0.427
passed
1.410
σ
passed
0.319
passed
1.290
ξ
passed
0.393
passed
4.060e-03
VaR0.99
passed
0.206
passed
1.550
ES0.99
passed
0.255
passed
1.320
Table 4.8: Heidelberg and Welch diagnostics for posterior samples, using the opinion of a ﬁcti-
tious expert for n=626 fraud losses in 41 banks, recorded from 01/2007 to 04/2010

4.3.
Elicitation from multiple experts
89
4.3
Elicitation from multiple experts
As pointed out in the paper by Jenkinson (2005), whilst understanding the opinions of one
expert is useful, there is an underlying statistical principle that the more information we
have got, the better the results will be. Hence, it can be preferable to elicit the opinions
of several experts.
According to Genest (1984b), Genest and Zidek (1986) and McConway (1981), among
others, there are two possible ways of combining multiple opinions: Elicit the distributions
from each expert and combine them mathematically (mathematical approach), or elicit a
consensus distribution (behavioural approach).
Mathematical methods are divided into two types: axiomatic approaches and Bayesian
approaches. The two main axiomatic approaches are:
1. The linear opinion pool.
2. The logarithmic opinion pool.
The linear opinion pool.
Let πj (θ) , j = 1, 2, ..., k be the j-th expert’s probability
density function and wj be the weight attached to the j-th expert’s opinion, with wj > 0
and
kP
j=1
wj = 1. The linear opinion pool is the weighted arithmetic mean of the densities
π (θ) =
k
X
j=1
wjπj (θ) .
(4.9)
This method satisﬁes the marginalization property (Genest, 1984b and McConway,
1981), which states that for a multivariate θ, the marginal probability from the combined
density for any of the components is the same as what would be achieved if the elicited
marginal distributions for that component were combined.

4.3.
Elicitation from multiple experts
90
The logarithmic opinion pool.
This method consist of the weighted geometric mean
of the densities:
π (θ) = n (w)
kY
j=1
πj (θ)wj ,
(4.10)
where n (w) is a normalizing constant, i.e.:
n−1 (w) =
ˆ
Θ
kY
j=1
πj (θ)wj dθ
and the weigthts wj are nonnegative and sum up to one.
This approach satisﬁes the external Bayesian (EB) principle (Genest, 1984a), which
refers to how a decision maker updates the combined distribution when new information
becomes available.
For simplicity, in this work we will focus on the linear opinion pool approach.
4.3.1
Prior on σ and ξ for multiple experts
We had previously deﬁned the prior distribution for the GPD parameters (σ and ξ) in
terms of the Value-at-Risk (Equation 4.5).
We can now use the two axiomatic approaches to obtain a prior for multiple experts.
Under this assumption, we can elicit the VaR and work with the diﬀerences as follows:
d1j =
 VaRq1j −u

, d2j =
 VaRq2j −u

−
 VaRq1j −u

for each expert j.
We can keep the assumption π (dij) ∼Gamma (aij, bij) , i = 1, 2; j = 1, ..., k.
Let πj (σ, ξ) be the prior for expert j as deﬁned in Equation (4.5). Then, under the
linear opinion pool, the prior for k experts can be written as:
π (σ, ξ) =
k
X
j=1
wjπj (σ, ξ) ,
(4.11)
where wj is the weight assigned to expert j. Under the logarithmic pool approach, the
density is given by:
π (σ, ξ) = n
kY
j=1
πj (σ, ξ)wj .
(4.12)

4.3.
Elicitation from multiple experts
91
Opinion
Similar
Diﬀerent
Very diﬀerent
Hyp.
Expert 1
Expert 2
Expert 1
Expert 2
Expert 1
Expert 2
a1i
0.1
0.15
0.1
5
0.1
10.5
b1i
0.005
0.007
0.005
0.252
0.005
0.53
a2i
0.9
1
0.9
9
0.9
20
b2i
0.03
0.033
0.03
0.302
0.03
0.671
Table 4.9: Sets of hyperparameters for two ﬁctitious experts with similar, diﬀerent and very
diﬀerent opinions
4.3.2
Multiple experts and real data
In order to compare the behaviour of the posterior density of VaR0.99 and ES0.99 under
the linear opinion pool, we analyze the real data from Section 3.2.1 for diﬀerent scenarios.
Recall that, for simplicity, we pooled all the data together and not looking at each bank
separately. We assume that we have two experts and three diﬀerent cases:
1. Two experts with similar opinions and weights w1 = w2 = 0.5
2. Two experts with diﬀerent opinions and weights w1 = w2 = 0.5
3. Two experts with very diﬀerent opinions and weights w1 = w2 = 0.5
Similarities and diﬀerences in opinions are expressed through the hyperparameters. Table
4.9 shows three diﬀerent sets of hyperparameters, according to the case considered.
Tables 4.10–4.15 and Figures 4.8–4.10 show the results for diﬀerent scenarios. When
both experts have similar opinions, prior and posterior parameter estimates do not show
great variation with respect to the original estimates (using only one expert opinion); if
experts have diﬀerent opinions, the estimates vary slightly; however, when experts have
very diﬀerent opinion, the estimates vary considerably. In this last case, one may notice
that the combined posterior resembles closely one of the prior densities (Figure 4.10).

4.3.
Elicitation from multiple experts
92
10,000 runs
Parameter
Mean
Median
Std
σ
66.409
65.399
14.824
ξ
0.073
0.049
0.075
VaR0.99
173.455
144.803
95.923
ES0.99
255.781
227.924
110.288
Table 4.10: Fraud data: Prior MCMC estimates for the linear opinion pool for two ﬁctitious
experts with similar opinions and weights w1 = w2 = 0.5 (10,000 runs-1,000 after thinning)
Figure 4.8: Fraud data: VaR0.99 and ES0.99 prior (left) and posterior (right) distribution for the
linear opinion pool for two ﬁctitious experts with similar opinions and weights w1 = w2 = 0.5
(10,000 runs-1,000 after thinning)
10,000 runs
Parameter
Mean
Median
Std
α
0.559
0.558
0.031
β
0.015
0.014
0.002
u
50.147
43.777
20.217
σ
69.477
65.102
17.740
ξ
0.201
0.208
0.0643
VaR0.99
364.224
361.466
33.656
ES0.99
532.409
525.920
53.799
Table 4.11: Fraud data: Posterior MCMC estimates for the linear opinion pool for two ﬁctitious
experts with similar opinions and weights w1 = w2 = 0.5 (10,000 runs-1,000 after thinning)

4.3.
Elicitation from multiple experts
93
10,000 runs
Parameter
Mean
Median
Std
σ
27.805
26.499
3.328
ξ
0.004e-01
9.3e-07
0.009
VaR0.99
107.827
87.858
63.369
ES0.99
135.712
114.484
63.778
Table 4.12: Fraud data: Prior MCMC estimates for the linear opinion pool for two ﬁctitious
experts with diﬀerent opinions and weights w1 = w2 = 0.5 (10,000 runs-1,000 after thinning)
Figure 4.9: Fraud data: VaR0.99 and ES0.99 prior (left) and posterior (right) distribution for the
linear opinion pool for two ﬁctitious experts with diﬀerent opinions and weights w1 = w2 = 0.5
(10,000 runs-1,000 after thinning)
10,000 runs
Parameter
Mean
Median
Std
α
0.561
0.561
0.033
β
0.015
0.015
0.002
u
38.081
36.228
16.606
σ
59.627
57.432
12.075
ξ
0.185
0.187
0.048
VaR0.99
327.787
325.256
25.771
ES0.99
467.983
465.486
40.546
Table 4.13: Fraud data: Posterior MCMC estimates for the linear opinion pool for two ﬁctitious
experts with diﬀerent opinions and weights w1 = w2 = 0.5 (10,000 runs-1,000 after thinning))

4.3.
Elicitation from multiple experts
94
10,000 runs
Parameter
Mean
Median
Std
σ
65.747
64.972
15.602
ξ
0.072
0.047
0.075
VaR0.99
170.194
140.679
95.520
ES0.99
251.542
223.487
111.087
Table 4.14: Fraud data: Prior MCMC estimates for the linear opinion pool for two ﬁctitious
experts with very diﬀerent opinions and weights w1 = w2 = 0.5 (10,000 runs-1,000 after thinning)
10,000 runs
Parameter
Expert 1
Expert 2
Both experts
α
0.567
0.561
0.555
β
0.0152
0.015
0.014
u
25.012
39.786
45.399
σ
49.898
56.734
62.127
ξ
0.152
0.387
0.369
VaR0.99
279.074
445.949
448.077
ES0.99
384.766
795.887
773.989
Table 4.15: Fraud data: Posterior MCMC estimates for the linear opinion pool for two ﬁctitious
experts with very diﬀerent opinions and weights w1 = w2 = 0.5 (10,000 runs-1,000 after thinning)
Figure 4.10: Fraud data: Posterior distribution of VaR0.99 and ES0.99 for the linear opinion
pool for two ﬁctitious experts with very diﬀerent opinions and weights w1 = w2 = 0.5 (10,000
runs-1,000 after thinning)

4.3.
Elicitation from multiple experts
95
Opinion
Very diﬀerent
Hyperparameters
Expert 1
Expert 2
a1i
24
0.005
b1i
1.21
2e-04
a2i
26
0.008
b2i
0.87
3e-04
Table 4.16: Set of hyperparameters for two experts with very diﬀerent opinions for n=1000
simulated data from a Gamma(100,0.09)
To determine if this problem is related to the data set, we simulated 1000 observations
from a Gamma(100,0.09). Again, the diﬀerences in opinions are expressed through the
hyperparameters. Table 4.16 shows the hyperparameters used in this analysis for ﬁctitious
experts.
Figure 4.11: VaR0.99 and ES0.99 posterior distribution (linear opinion pool for two ﬁctitious
experts with very diﬀerent opinions) for n=1000 simulated data from a Gamma(100,0.09)
In this case the posterior does not exhibit the same behaviour; however, it is still close
to one of the prior densities. (Figure 4.11 and Table 4.17).

4.3.
Elicitation from multiple experts
96
Gamma(100,0.09)-10,000 runs
VaR0.99
Mean
Median
Std
Posterior Expert 1
1381.758
1381.749
6.891
Posterior Expert 2
1392.442
1392.782
6.820
Posterior w1 = 0.5, w2 = 0.5
1390.320
1390.025
7.468
True value
1385.806
Table 4.17: VaR0.99 estimates and true value (linear opinion pool for two ﬁctitious experts with
very diﬀerent opinions) for n=1000 simulated data from a Gamma(100,0.09)
4.3.3
A new prior for multiple experts
In order to improve the prior proposed in Section 4.2, we consider a diﬀerent prior, based
on the same elicited quantities. Consider the ratio
r1 = (VaRq1 −u)
(VaRq2 −u) =
σ
ξ

k−ξ
1
−1

σ
ξ

k−ξ
2
−1
 = k−ξ
1
−1
k−ξ
2
−1
.
Then
dr1
dξ
=
−

k−ξ
1 ln k1
 
k−ξ
2
−1

+

k−ξ
1
−1
 
k−ξ
2 ln k2


k−ξ
2
−1
2
= −(k1k2)−ξ ln k1 + k−ξ
1 ln k1 + (k1k2)−ξ ln k2 −k−ξ
2 ln k2

k−ξ
2
−1
2
= −(k1k2)−ξ (ln k1 −ln k2) + k−ξ
1 ln k1 −k−ξ
2 ln k2

k−ξ
2
−1
2
.
If we choose a Beta distribution for the ratio r1, we have:
π (ξ | u, σ) = π (r1)

∂r1
∂ξ
 ∝ra1−1
1
 1 −rb1−1
1
 
∂r1
∂ξ
 .
(4.13)
We also choose an Inverse Gaussian for σ and a truncated normal for u.
For the simulated data (Gamma(100,0.09)), using the prior speciﬁed above and the
hyperparameters for ﬁctitious experts in Table 4.16, we obtain the densities shown in
Figures 4.12 and 4.13.

4.3.
Elicitation from multiple experts
97
Figure 4.12: Prior (left) and posterior (right) distribution of VaR0.99, using the prior from
Equation (4.13) and the linear opinion pool for two ﬁctitious experts with very diﬀerent opinions.
Expert 1 (black), expert 2 (red) and combined distribution (blue)
Figure 4.13: Prior (left) and posterior (right) distribution of ES0.99, using the prior from Equa-
tion (4.13) and the linear opinion pool for two ﬁctitious experts with very diﬀerent opinions.
Expert 1 (black), expert 2 (red) and combined distribution (blue)

4.3.
Elicitation from multiple experts
98
4.3.4
Posterior analysis for more than two experts
So far, we have considered only the case when the prior is based on two experts’ opinion.
We have observed how the diﬀerences between these opinions may inﬂuence the prior and
posterior behaviour. To conclude this chapter, we study the posterior behaviour when
several experts express their opinions. We consider ﬁve experts with diﬀerent opinions.
Again, we resort to the Gamma simulated data, using three sets of hyperparameters (Table
4.18). The results obtained are shown in Figure 4.14.
As can be seen from the ﬁgures, even when the posteriors for each expert are diﬀerent,
all lead to similar posterior distributions.
Table 4.19 shows the estimates of the Value-at-Risk at 99.5% level. It can be seen from
this table that experts whose opinion is far from the true value inﬂuence the combined
posterior in all cases. However, the combined estimate is still acceptable.
To complete this study, we perform the analysis in two diﬀerent subsets of the
Gamma(100, 0.09) data, using the same hyperparameters. Results are shown in Tables
4.20 and 4.21. This time, the results depend on the subset we are working with; however,
we see again how experts whose opinion is far from the true value inﬂuence the combined
posterior.
The results of this analysis indicate that when dealing with several opinions, one should
pay special attention to prior speciﬁcation.
Set 1
Set 2
Set 3
Hyp.
E1
E2
E3
E4
E5
E1
E2
E3
E4
E5
E1
E2
E3
E4
E5
a1i
0.001
25
12
8
1
3
28
13
11
3
0.001
0.28
0.15
0.11
0.3
b1i
0.3
13
14
7
2
3.3
16
17
10
4
0.3
0.16
0.17
0.1
0.4
a2i
10
6
8
8
0.9
12
9
10
11
2.9
0.12
0.9
0.1
0.11
0.29
b2i
0.3
1
2
5
0.7
2.3
4
4
8
2.7
0.23
0.4
0.4
0.8
0.27
Table 4.18: Sets of hyperparameters for ﬁve diﬀerent experts (E1,E2,E3,E4,E5)

4.3.
Elicitation from multiple experts
99
Figure 4.14:
Posterior distribution of VaR0.995 and ES0.995 for the ﬁrst set (top), second
set (middle) and third set (bottom) of hyperparameters for n=1000 simulated data from a
Gamma(100,0.09), using the opinion of ﬁve experts (E1,E2,E3,E4,E5) with equal weights

4.3.
Elicitation from multiple experts
100
Set 1
Set 2
Set 3
Expert
Mean, Median, Std
Mean, Median, Std
Mean, Median, Std
E1
1411.990, 1411.600, 13.680
1412.450, 1412.050, 12.230
1416.880, 1413.290, 15.860
E2
1427.560, 1427.710, 9.410
1424.060, 1422.710, 9.190
1425.120, 1424.370, 11.850
E3
1416.900, 1415.200, 11.370
1415.940, 1415.400, 11.210
1418.020, 1417.360, 12.680
E4
1421.230, 1421.320, 12.290
1422.240, 1423.200, 9.010
1421.380, 1420.520, 9.840
E5
1427.770, 1427.410, 10.610
1426.070, 1426.540, 10.710
1423.240, 1422.370, 12.390
Combined
1423.330, 1421.310, 11.030
1422.690, 1421.970, 11.740
1422.930, 1422.540, 10.080
True value
1418.134
Table 4.19:
Parameter estimates of VaR0.995
for n=1000 simulated data from a
Gamma(100,0.09), using the opinion of ﬁve experts (E1,E2,E3,E4,E5) with equal weights (10,000
runs)
Set 1
Set 2
Set 3
Expert
Mean, Median, Std
Mean, Median, Std
Mean, Median, Std
E1
1371.820, 1370.186, 6.989
1443.776, 1441.065, 55.335
1405.031, 1397.473, 31.126
E2
1391.487, 1389.019, 11.620
1395.270, 1393.940, 10.823
1399.606, 1396.492, 17.876
E3
1386.927, 1383.362, 8.986
1388.259, 1385.408, 16.212
1400.394, 1397.394, 24.305
E4
1385.866, 1385.433, 6.877
1392.034, 1389.880, 13.309
1396.940, 1394.319, 17.206
E5
1396.547, 1397.347, 13.535
1395.127, 1393.501, 11.729
1399.752, 1397.355, 17.270
Combined
1389.755, 1387.520, 7.271
1398.320, 1393.878, 16.025
1400.570, 1393.845, 18.116
Table 4.20: Parameter estimates of VaR0.995 for the ﬁrst subset of n=1000 simulated data from a
Gamma(100,0.09), using the opinion of ﬁve experts (E1,E2,E3,E4,E5) with equal weights (10,000
runs)
Set 1
Set 2
Set 3
Expert
Mean, Median, Std
Mean, Median, Std
Mean, Median, Std
E1
1431.522, 1423.112, 48.875
1452.904, 1445.230, 46.153
1463.631, 1457.973, 52.967
E2
1429.530, 1427.390, 19.159
1432.792, 1431.263, 21.051
1439.916, 1434.600, 30.502
E3
1414.734, 1413.327, 19.829
1413.011, 1410.485, 17.976
1426.850, 1420.401, 27.519
E4
1430.454, 1429.945, 25.231
1424.375, 1425.571, 19.345
1431.188, 1427.123, 24.997
E5
1439.861, 1438.333, 21.899
1434.694, 1435.101, 19.474
1442.770, 1441.365x, 23.989
Combined
1438.283, 1433.558, 23.646
1432.958, 1427.611, 23.489
1436.812, 1434.325, 27.353
Table 4.21: Parameter estimates of VaR0.995 for the second subset of n=1000 simulated data
from a Gamma(100,0.09), using the opinion of ﬁve experts (E1,E2,E3,E4,E5) with equal weights
(10,000 runs)

4.3.
Elicitation from multiple experts
101
4.3.5
Updating the weighting of the experts’ opinions
The Bayesian model we introduced before allows the incorporation of expert opinion into
the analysis (via the prior distributions) for model parameters. These parameters can be
updated as new data become available. Experts may reassess their opinion to incorporate
new information (for example, new policies or controls) and this can be done as follows.
Using equation (2.39), let πj (θ) be the prior density for θ and L (θ | x) be the likelihood.
Then, the posterior density is:
pj (θ | x) = πj (θ) L (θ | x)
Kj
,
(4.14)
where Kj =
´ ∞
−∞πj (θ) L (θ | x) dθ.
Now, let π (θ) =
JP
j=1wjπj (θ) be a mixture prior given. Hence, the posterior density is
p (θ | x) =
JP
j=1wjπj (θ) L (θ | x)
K
=
JP
j=1
wjKjπj (θ) L (θ | x)
Kj
K
=
JP
j=1wjKjpj (θ | x)
K
.
(4.15)
From (4.15), we require
JP
j=1wjKj/K=1, which implies K =
JP
j=1wjKj. Therefore, the
posterior distribution is given by
p (θ | x) =
J
X
j=1
w
′
jpj (θ | x) ,
(4.16)
where w
′
j = {wjKj}/ PJ
h=1 whKh.
4.3.6
Prior weights updating
Once we have performed the analysis and observed the results, we might want to evaluate
how accurate experts are in their prediction, and adjust the weight assigned to each expert
in future exercises, based on their past performance. To do so, one can use a measure of
divergence, such as the Kullback-Leibler divergence1.
1Let g (x) and h (x) be two probability density functions deﬁned over the same support S.
The
Kullback and Leibler divergence between the two distributions is deﬁned as:
D (g q h) =
ˆ
S
g (x) ln g (x)
h (x)dx

4.3.
Elicitation from multiple experts
102
Suppose that, for each risk assessment, we base our prior information on the opinion
of n experts. In each exercise we might do the following:
 Follow the opinion of the majority of experts.
 After a pre-determined number of assessments, see which of the experts get it right
most of the time and then follow their advice.
Although these strategies work well in some cases, the ﬁrst one fails when only a few experts
make good predictions. The second one fails when there is an expert that performs well
for the ﬁrst evaluations but is wrong after that.
In this section, we will consider the Multiplicative Weights approach (Arora et al.,
2012), which allows us to consider the opinion of all experts, but weighting each expert’s
opinion according to his/her past performance.
Multiplicative update algorithms were proposed in game theory in the early ﬁfties. One
of the ﬁrst ideas was that at each step each player observes actions taken by his/her op-
ponent in previous stages, updates his beliefs about his opponents’ strategies, and chooses
myopic pure best responses against these beliefs. However, the multiplicative update rule
was rediscovered in Computational Geometry in the late 1980s, while the weighted ma-
jority algorithm has been independently discovered in operations research and statistical
decision making.
Initially, the algorithm assigns equal weights to all experts. As time goes on, some
experts are seen as making better predictions than others, and the algorithm increases
their weight proportionally. The algorithm is as follows.
Weighted majority algorithm.
At every step t, we have a weight wt
i assigned to expert
i. Initially equal weights for all i. At step t + 1, for each i such that expert i was found
to have predicted the quantity of interest incorrectly, we set
wt+1
i
= (1 −ϵ) wt
i
(update rule)

4.3.
Elicitation from multiple experts
103
Our prediction for step t + 1 is the opinion of a weighted majority of the experts.
The following result (Theorem 1.1, Arora et al.
2012) shows that the number of
mistakes the algorithm makes is bounded above.
Theorem 4.1. After t steps, let mt
i be the number of mistakes of expert i and mt be the
number of mistakes our algorithm has made. Then we have the following bound for every i:
mt ≤2 ln n
ϵ
+ 2 (1 + ϵ) mt
i.
We have adapted this algorithm to our problem by considering the KL divergence. We
propose the following procedure to perform the updating:
1. Set w0
j = 1/n for i = 1, ..., n.
2. After assessment t, update the weights as wt
i =
(1−ϵi)wt−1
i
P
j(1−ϵj)wt−1
j
. Here, ϵj = KL/10N,
where KL corresponds to the Kullback-Leibler divergence between the posterior
distribution of VaR of expert j and the combined posterior distribution (including
the opinion of all experts), and N is the number of digits left of the decimal point
of maxj(KLj).
This procedure assigns more weight to experts whose opinion is closer to the mixture
of opinions and penalizes those who are far from the majority, based on the the KL
divergence.
We can apply this algorithm to the simulated data Gamma(100,0.09), and to the the
posterior distribution of VaR0.99 for diﬀerent sets of hyperparameters (Table 4.18). In this
case, n = 5 and w0
j = 1/5 = 0.2 for j = 1, ..., 5.
Table 4.22 summarizes the results for the diﬀerent sets of hyperparameters. From it, we
can notice that as KL increases, w1
j decreases. That is, those experts whose KL divergence
is large are penalized in the next risk assessment by assigning them smaller weights. On
the other hand, experts with smaller KL divergence gain more credibility and get larger
weights in the new assessment.

4.3.
Elicitation from multiple experts
104
Set 1
Set 2
Set 3
Expert
w0
j
KLj
ϵj
w1
j
KLj
ϵj
w1
j
KLj
ϵj
w1
j
E1
0.2
0.418
0.042
0.205
0.176
0.176
0.220
0.450
0.450
0.159
E2
0.2
1.638
0.164
0.179
0.155
0.155
0.225
0.330
0.330
0.194
E3
0.2
0.663
0.066
0.199
0.355
0.355
0.172
0.147
0.147
0.246
E4
0.2
0.293
0.029
0.208
0.335
0.335
0.177
0.029
0.029
0.281
E5
0.2
0.237
0.024
0.209
0.227
0.227
0.206
0.584
0.584
0.120
Table 4.22: Prior weights updating for diﬀerent sets of hyperparameters for n=1000 simulated
data from a Gamma(100,0.09), using the opinion of ﬁve experts (E1,E2,E3,E4,E5) with equal
weights
Set 1
Set 2
Set 3
Expert
w0
j
KLj
ϵj
w1
j
KLj
ϵj
w1
j
KLj
ϵj
w1
j
E1
0.2
0.178
0.178
0.235
0.062
0.062
0.205
0.061
0.061
0.204
E2
0.2
0.357
0.357
0.183
0.041
0.041
0.208
0.054
0.054
0.205
E3
0.2
0.199
0.199
0.227
0.093
0.093
0.197
0.030
0.030
0.211
E4
0.2
0.609
0.609
0.111
0.106
0.106
0.194
0.202
0.202
0.173
E5
0.2
0.139
0.139
0.244
0.097
0.097
0.196
0.044
0.044
0.207
Table 4.23:
Prior weights updating for diﬀerent sets of hyperparameters for the ﬁrst sub-
set of n=1000 simulated data from a Gamma(100,0.09), using the opinion of ﬁve experts
(E1,E2,E3,E4,E5) with equal weights
In order to study the variation of the weights for diﬀerent data, we obtain new weights
for the two subsets of the Gamma(100,0.09) that were used in Section 4.3.4. Tables 4.23
and 4.24 display the results. Once again, one may notice that w1
j is inversely proportional
to KL, and therefore experts whose KL divergence is large get smaller weights, while those
with small KL divergence are assigned larger weights.
According to Basel II, operational risk has to be measured on a yearly basis. The
method presented in this section allows us not only to update prior information as new
information becomes available, but also the weights assigned to experts are based on the
accuracy of their past opinions.

4.3.
Elicitation from multiple experts
105
Set 1
Set 2
Set 3
Expert
w0
j
KLj
ϵj
w1
j
KLj
ϵj
w1
j
KLj
ϵj
w1
j
E1
0.2
0.330
0.330
0.167
0.182
0.182
0.214
0.109
0.109
0.218
E2
0.2
0.022
0.022
0.243
0.144
0.144
0.224
0.401
0.401
0.146
E3
0.2
0.039
0.039
0.239
0.319
0.319
0.178
0.292
0.292
0.174
E4
0.2
0.190
0.190
0.202
0.190
0.190
0.213
0.065
0.065
0.228
E5
0.2
0.401
0.401
0.149
0.348
0.348
0.171
0.042
0.042
0.234
Table 4.24: Prior weights updating for diﬀerent sets of hyperparameters for the second sub-
set of n=1000 simulated data from a Gamma(100,0.09), using the opinion of ﬁve experts
(E1,E2,E3,E4,E5) with equal weights
Prior distribution
Expert
Year 0
Year 1
Year 2
...
Year n
E1
π1 (θ)
p1 (θ | x1)
p1 (θ | x1, x2)
...
p1 (θ | x1, x2, ..., xn)
E2
π2 (θ)
p2 (θ | x1)
p2 (θ | x1, x2)
...
p2 (θ | x1, x2, ..., xn)
E3
π3 (θ)
p3 (θ | x1)
p3 (θ | x1, x2)
...
p3 (θ | x1, x2, ..., xn)
E4
π4 (θ)
p1 (θ | x1)
p4 (θ | x1, x2)
...
p4 (θ | x1, x2, ..., xn)
E5
π5 (θ)
p1 (θ | x1)
p5 (θ | x1, x2)
...
p5 (θ | x1, x2, ..., xn)
Table 4.25: Prior distributions for diﬀerent years
4.3.7
Posterior distribution updating
We will assume now that the risk assessment is performed every year and, as time passes,
more data become available. This aﬀects the resulting posterior distribution and some
adjustments have to be made. The more information we have, the better we are able to
predict.
Under this situation, the prior distribution of each expert for the next year should
be based on their posterior distribution from the previous year. That is, we start with
opinions expressed in terms of the prior distribution πi (θ) for year 0. For the next year, the
prior of expert i is replaced by pi (θ | x1), which is the posterior distribution after including
the data collected during the ﬁrst year. We can continue this procedure for the upcoming
years, as it is ilustrated in Table 4.25. We expect that after many years, as the number of
observations increases, all of the experts will have similar posterior distributions.
To explore this, we use the simulated data Gamma(100,0.09) and the real data de-
scribed in Section 3.2.1. For the simulated data, we assume that new information becomes

4.3.
Elicitation from multiple experts
106
Year 0
Year 1
Year 2
Year 3
Expert
w0
j
KLj
ϵj
w1
j
KLj
ϵj
w2
j
KLj
ϵj
w3
j
E1
0.2
0.418
0.042
0.205
0.319
0.319
0.169
0.067
0.007
0.206
E2
0.2
1.638
0.164
0.179
0.043
0.043
0.237
1.201
0.120
0.182
E3
0.2
0.663
0.066
0.199
0.314
0.314
0.170
0.111
0.011
0.205
E4
0.2
0.293
0.029
0.208
0.228
0.228
0.191
0.278
0.028
0.201
E5
0.2
0.237
0.024
0.209
0.056
0.056
0.233
0.077
0.008
0.206
Table 4.26: Prior weights for diﬀerent experts in a 3-year period. n=1000 simulated data from
a Gamma(100,0.09), using the opinion of ﬁve experts (E1,E2,E3,E4,E5) with equal weights
available every year during 3 years. We start with 1,000 observations for year 0. For
the next year, we add 400 observations to the data set. In the second year, we also add
400 observations. Finally, in the third year, 500 observations are added to the data set.
Following the notation in Table 4.25, we have:
 x1: Observations for year 0 (1000);
 x2: Observations for year 1 (400);
 x3: Observations for year 2 (400);
 x4: Observations for year 3 (500).
Tables 4.26 and 4.27 show the results for the period considered, while Figure 4.15 displays
the posterior distributions. We can observe that every year, the estimated VaR is closer
to the true value. Additionally, the posterior distributions present less variability and look
more similar to each other.
The same procedure is applied to the real data set from Section 3.2.1 to analyze the
posterior behaviour when new data are added. We have 626 observations available from
year 2007 to 2010. We divide our data into four diﬀerent data sets:
 x1: Observations for 2007 (154);
 x2: Observations for 2008 (193);
 x3: Observations for 2009 (212);

4.3.
Elicitation from multiple experts
107
Figure 4.15: Posterior distribution of VaR0.995 and ES0.995 for diﬀerent experts in the ﬁrst
year (top), second year (middle) and third year (bottom).
n=1000 simulated data from a
Gamma(100,0.09), using the opinion of ﬁve experts (E1,E2,E3,E4,E5) with equal weights

4.3.
Elicitation from multiple experts
108
Year 0
Year 1
Year 2
Year 3
Expert
Mean, Median, Std
Mean, Median, Std
Mean, Median, Std
Mean, Median, Std
E1
1411.990, 1411.600, 13.680
1410.843, 1409.158, 11.276
1407.612, 1406.914, 8.211
1419.249, 1419.307, 9.353
E2
1427.560, 1427.710, 9.410
1420.800, 1420.384, 9.77
1420.044, 1419.911, 7.914
1417.47, 1417.476, 6.057
E3
1416.900, 1415.200, 11.370
1429.660, 1426.730, 22.121
1408.145, 1406.73, 14.041
1414.015, 1411.566, 11.477
E4
1421.230, 1421.320, 12.290
1418.614, 1419.563, 9.414
1414.167, 1413.947, 6.911
1417.266, 1416.81, 7.299
E5
1427.770, 1427.410, 10.610
1421.409, 1421.879, 8.259
1416.877, 1416.652, 7.586
1422.44, 1420.701, 7.881
Comb.
1423.330, 1421.310, 11.030
1416.476, 1416.917, 9.658
1418.539, 1417.987, 7.229
1418.561, 1418.467, 8.679
True
value
1418.134
Table 4.27: Posterior distribution for diﬀerent experts in a 3- year period. n=1000 simulated
data from a Gamma(100,0.09), using the opinion of ﬁve experts (E1,E2,E3,E4,E5) with equal
weights
2007
2007-2008
2007-2009
2007-2010
Expert
w0
j
KLj
ϵj
w1
j
KLj
ϵj
w2
j
KLj
ϵj
w3
j
E1
0.2
2.141
0.214
0.167
0.029
0.029
0.201
0.031
0.031
0.214
E2
0.2
0.037
0.004
0.211
0.030
0.030
0.200
0.117
0.117
0.195
E3
0.2
0.493
0.049
0.202
0.009
0.009
0.205
0.117
0.117
0.195
E4
0.2
0.052
0.005
0.211
0.078
0.078
0.190
0.182
0.182
0.180
E5
0.2
0.127
0-013
0.209
0.0135
0.0135
0.204
0.023
0.023
0.216
Table 4.28: Prior weights for diﬀerent experts in a 3-year period. n=626 fraud losses in 41
banks, recorded from 01/2007 to 04/2010
 x4: Observations for 2010 (67).
We can observe the results for diﬀerent years in Tables 4.28–4.29. Posterior distributions
are shown in Figures 4.16–4.19. From the plots one may notice that, at the beginning,
expert 4 is far from the rest. However, as more data become available during the next
years, his distribution gradually resembles the combined distribution. Something similar
happens to the other expert distributions. As expected, in the analysis corresponding to
the year 2010, we can observe how the distribution of VaR and ES for all of the experts
are more uniform than in previous years.

4.3.
Elicitation from multiple experts
109
Figure 4.16: Fraud data: Posterior distribution of VaR0.99 and ES0.99 for diﬀerent experts in
2007
Figure 4.17: Fraud data: Posterior distribution of VaR0.99 and ES0.99 for diﬀerent experts in
2008

4.3.
Elicitation from multiple experts
110
Figure 4.18: Fraud data: Posterior distribution of VaR0.99 and ES0.99 for diﬀerent experts in
2009
Figure 4.19: Fraud data: Posterior distribution of VaR0.99 and ES0.99 for diﬀerent experts in
2010

4.4.
Conclusions of the Chapter
111
2007
2007-2008
2007-2009
2007-2010
Expert
Mean, Median, Std
Mean, Median, Std
Mean, Median, Std
Mean, Median, Std
E1
428.748, 364.665, 214.425
480.516, 462.743, 114.047
590.205, 567.618, 116.473
587.406, 569.175, 122.972
E2
355.307, 336.308, 81.129
464.560, 447.699, 95.808
570.429, 559.874, 104.638
564.206, 549.007, 104.701
E3
422.933, 395.480, 140.720
482.356, 469.746, 109.211
589.436, 537.179, 144.232
570.985, 541.690, 121.582
E4
318.049, 306.768, 46.8490
409.576, 396.502, 63.460
507.235, 496.151, 82.829
513.787, 497.401, 98.330
E5
356.672, 334.556, 84.574
459.366, 439.861, 96.358
562.389, 550.084, 94.883
548.625, 527.648, 104.754
Combined
371.900, 336.464, 89.656
471.559, 455.680, 98.280
562.042, 542.345, 114.736
547.475, 535.806, 78.192
Table 4.29: Posterior distribution for diﬀerent experts in a 3- year period. n=626 fraud losses
in 41 banks, recorded from 01/2007 to 04/2010
4.4
Conclusions of the Chapter
The conclusions drawn by the discussion of this chapter reveal the importance of consider-
ing expert opinion in the analysis of extremes. The ﬁrst important ﬁnding in this chapter
was that non-informative priors are not appropriate for modelling extreme data when few
observations are available. To handle this issue, we have proposed two diﬀerent ways of
constructing a subjective prior, based on risk measures that experts are familiar with.
In all cases considered, we could observe how expert opinion inﬂuences the posterior
behaviour and how the diﬀerences in these opinions may lead to diﬀerent conclusions.
Hence, the relevance of an appropriate elicitation process.
Additionally, in Section 4.3.4, we have provided the tools for prior analysis when opin-
ions from multiple experts are available, particularly some methods to combine diﬀerent
distributions and assign weights to experts according to their past performance. This be-
comes particularly important in situations where data sets are limited, as it is typically
the case of operational risk data. However, as we saw in the previous analysis, as more
data become available, we have less uncertainty about the unknown parameters and our
estimates seem to be more accurate. Moreover, the analysis is carried out using all the
information available, either expert opinion or collected data.

Chapter 5
Extending the GPD mixture model
In Chapter 3 we presented a Bayesian model that takes into account observations above
and below the threshold u. However, in that model the density has a discontinuity at the
threshold. The jump can be larger or smaller depending on the parameters.
In this chapter, we introduce a new model that allows to handle the discontinuity issue
in diﬀerent ways. We consider diﬀerent approaches according to the sort of discontinuity
we are dealing with. We start by introducing a model where the threshold u plays the role
of the qth quantile and that has a continuous density at that point. We also consider a
second model where the threshold plays the role of the quantile of the overall distribution
at which the Gamma tail is replaced by the GPD tail, not a ﬁxed quantile for a pre-
speciﬁed q. This time we require the derivative of the density to be continuous at the
blend point. Next, we present a model with arbitrary scaling for the GPD density and,
again, we require this model to have continuous derivative at the blend point. Lastly, we
introduce a more general model where the scaling is arbitrary but that can be implemented
even if the density and its derivative are discontinuous.
Finally, we explore a Bayesian nonparametric framework. We start by considering a
model where the GPD is represented as a mixture of Exponentials and use a Dirichlet
process mixture formulation that allows for a ﬂexible density speciﬁcation. We provide
112

5.1.
Blended Gamma-GPD model
113
the details for simulation from the DPM model using the P´olya urn scheme and MCMC
sampling. After that, we introduce a second Bayesian nonparametric model that uses a
Dirichlet process prior on the parameters of the GPD model. We also provide the sampling
scheme and a possible extension of this model.
We ﬁnally introduce a nonparametric
version of the model with arbitrary scaling that can be implemented even if the density
and its ﬁrst derivative are discontinuous. As for the other models, we provide the details
of its implementation.
It is worth pointing out that we have implemented most of these models for diﬀerent
data sets; however, in order to be brief, we only present the most interesting ﬁndings.
Nonetheless, we provide the details of all the models as extra information. Another ex-
tension for mixture models is provided in the next chapter.
5.1
Blended Gamma-GPD model
Recall the blended model with Gamma and Generalized Pareto elements blended to be
continuous at an upper quantile, q (where q = 0.9, 0.99 say), which is a special case
of the general model introduced in Section 3.1.
Suppose that u is deﬁned such that
FH(u; α, β) = q, with FH(x; α, β) denoting the Gamma cdf, where for x >0,
FH(x; α, β) =
ˆ x
0
fH(t; α, β)dt,
(5.1)
with
fH(x; α, β) =
βα
Γ (α)xα−1e−βx,
x > 0.
The blended cdf, FB(x; u, σ, ξ), takes the form
FB(x; u, σ, ξ) =







FH(x; α, β),
x ≤u,
FH(u; α, β) + (1 −FH(u; α, β)) FG (x; u, σ, ξ) ,
x > u,
(5.2)
where FG(x; u, σ, ξ) is the shifted generalized Pareto cdf viz.
FG (x; u, σ, ξ) = 1 −

1 + ξ(x −u)
σ
−1/ξ
,
x > u

5.1.
Blended Gamma-GPD model
114
with density
fG (x; u, σ, ξ) = 1
σ

1 + ξ (x −u)
σ
−1
ξ −1
,
where the support is







x > u
if
ξ ≥0,
u ≤x ≤u −σ/ξ
if
ξ < 0,
with parameters u, σ > 0, and ξ. We may assume that ξ ≥0 if we require the support of
the blended distribution to be unbounded from above.
5.1.1
A model with continuous density at the qth quantile
Observe that the blended model satisﬁes
FH(u; α, β) = q.
(5.3)
In this model, we require the density to be continuous at x = u. The density is
fB(x; u, σ, ξ) =







fH(x; α, β),
x ≤u,
(1 −q) fG (x; u, σ, ξ) ,
x > u.
(5.4)
and therefore we require
fH(u; α, β) = (1 −q) fG (u; u, σ, ξ) = 1 −q
σ
.
(5.5)
This deﬁnes a second equality constraint to reduce the number of free parameters. We
regard the GPD parameters (u, σ, ξ) as the working parameters, and thus solve equations
(5.3) and (5.5) simultaneously for (α, β). This must be done numerically, and leaves (α, β)
determined by q, u and σ.
5.1.2
A model with continuous ﬁrst derivative at the blend
point
In this version of the model, u plays the role of the quantile of the overall distribution
at which the Gamma tail is replaced by the GPD tail; it is not a ﬁxed qth quantile for a

5.1.
Blended Gamma-GPD model
115
pre-speciﬁed q. Again, we require the blended model to have continuous density at x = u.
Up to proportionality, the density is
fB(x; u, σ, ξ) ∝







fH(x; α, β),
x ≤u,
(1 −FH(u; α, β)) fG (x; u, σ, ξ) ,
x > u.
(5.6)
where α and β are such that
fH(u; α, β) = (1 −FH(u; α, β)) fG (u; u, σ, ξ) = 1 −FH(u; α, β)
σ
.
(5.7)
This deﬁnes one equality constraint to reduce the number of free parameters. For a further
constraint, we also require the derivative of the density to be continuous at x = u. Up to
proportionality, the derivative is
˙fB(x; u, σ, ξ) ∝







˙fH(x; α, β),
x ≤u,
(1 −FH(u; α, β)) ˙fG (x; u, σ, ξ) ,
x > u.
(5.8)
Hence we require
˙fH(u; α, β) = (1 −FH(u; α, β)) ˙fG (u; u, σ, ξ) = −(1 −FH(u; α, β)) (1 + ξ)
σ2
.
(5.9)
Note that
˙fH(x; α, β) =
βα
Γ (α)

(α −1) xα−2e−βx −βxα−1e−βx
=
α −1
x
−β

fH(x; α, β).
(5.10)
Because of (5.7), (5.9) thus reduces to
α −1
u
−β

= −(1 + ξ)
σ
,
yielding that
β = α −1
u
+ 1 + ξ
σ
= β (α, u, σ, ξ) ,
say. Equation (5.7) then becomes
fH (u; α, β (α, u, σ, ξ)) = 1 −FH (u; α, β (α, u, σ, ξ))
σ
,
(5.11)

5.1.
Blended Gamma-GPD model
116
which deﬁnes the second equality constraint. We regard the parameters (u, σ, ξ) as the
working parameters, and thus solve equation (5.11) for α. The proportionality constant is
deﬁned by requiring fB(x), deﬁned by (5.6), to integrate to 1. This gives the normalizing
constant
1
FH (u) + (1 −FH(u)) = 1
so that
fB(x; u, σ, ξ) =







fH(x; α, β),
x ≤u,
(1 −FH(u; α, β)) fG (x; u, σ, ξ) ,
x > u.
(5.12)
We note the necessary constraint σ > u(1 + ξ) which guarantees that β(α, u, σ, ξ) > 0 for
any value of α > 0.
5.1.3
A model with continuous ﬁrst derivative with arbitrary
scaling
In this version of the model, the density is
fB(x; u, σ, ξ) ∝







fH(x; α, β),
x ≤u,
ω (1 −FH(u; α, β)) fG (x; u, σ, ξ) ,
x > u,
(5.13)
for some ω > 0 which is a further parameter of the model. To assure the continuity of fB,
fH(u; α, β) = ω (1 −FH(u; α, β)) fG (u; u, σ, ξ) = ω (1 −FH(u; α, β))
σ
.
(5.14)
This deﬁnes one equality constraint to reduce the number of free parameters. For a further
constraint, we also require the derivative of the density to be continuous at x = u. Up to
proportionality, the derivative is
˙fB(x; u, σ, ξ) ∝







˙fH(x; α, β),
x ≤u,
ω (1 −FH(u; α, β)) ˙fG (x; u, σ, ξ) ,
x > u.
(5.15)

5.1.
Blended Gamma-GPD model
117
As before, we have that
β = α −1
u
+ 1 + ξ
σ
= β (α, u, σ, ξ) ,
say. Equation (5.14) then becomes
fH (u; α, β (α, u, ξ, σ)) = ω (1 −FH(u; α, β (α, u, σ, ξ)))
σ
,
(5.16)
which deﬁnes the second equality constraint. We regard the GPD parameters (u, σ, ξ) as
the working parameters, and thus solve equation (5.16) for α. The proportionality constant
is deﬁned by requiring fB(x) to integrate to 1: with the density deﬁned by (5.13), we have
that the normalizing constant is
1
FH (u; α, β) + ω (1 −FH(u; α, β))
so that
fB(x; u, σ, ξ) ∝









fH(x; α, β)
FH (u) + ω (1 −FH(u; α, β)),
x ≤u,
ω (1 −FH(u; α, β)) fG (x; u, σ, ξ)
FH (u) + ω (1 −FH(u; α, β))
,
x > u,
(5.17)
again with the constraint σ > u(1 + ξ) which guarantees that β(α, u, σ, ξ) > 0 for any
α > 0. At x = u, we have that
FB (u; u, σ, ξ) =
FH (u)
FH (u) + ω (1 −FH(u; α, β)).
5.1.4
A model with discontinuous density with arbitrary
scaling
The scaling from the model from Section 5.1.3 can be implemented even if the density and
its derivative are discontinuous, as in the original version of the model. Suppose that fB
takes the form
fB(x; u, σ, ξ) ∝







fH(x; θ),
x ≤u,
ω (1 −FH(u; θ)) fG (x; u, σ, ξ) ,
x > u,
(5.18)

5.1.
Blended Gamma-GPD model
118
where fH is the density for the left-hand component (not necessarily Gamma), active when
x < u, and for some ω > 0. The proportionality constant is deﬁned by requiring fB(x)
deﬁned in (5.18) to integrate to 1. The normalizing constant can be computed to be
1
FH (u; θ) + ω (1 −FH(u; θ))
so that
fB(x; u, θ, σ, ξ) ∝









fH(x; θ)
FH (u; θ) + ω (1 −FH(u; θ)),
x ≤u,
ω (1 −FH(u; θ)) fG (x; u, σ, ξ)
FH (u; θ) + ω (1 −FH(u; θ)) ,
x > u.
(5.19)
At x = u, we have that
FB (u; u, θ, σ, ξ) =
FH (u; θ)
FH (u; θ) + ω (1 −FH(u; θ)).
From (5.19), it is apparent that this model can be thought of as resulting from a
mixture representation
fB (x) = π ˜
fH (x) + (1 −π) fG (x) ,
where fH has support (0, u) and fG has support (u, ∞) with
˜
fH (x) = fH(x; θ)
FH(u; θ)
x ≤u
and
π =
FH (u; θ)
FH (u; θ) + ω (1 −FH (u; θ)).
In the conventional version of this model, we ﬁx ω =1.
5.1.5
Real data example
The four models proposed above were tested on diﬀerent data sets. In this section, we
illustrate the results for the fraud data described in Section 3.2.1 (Figure 5.1). We con-
centrate our attention on the model with continuous ﬁrst derivative at the blend point
(Section 5.1.2) and the model with discontinuous density with arbitrary scaling (Section
5.1.4), due to their performance. MCMC was used to sample the parameters from these

5.1.
Blended Gamma-GPD model
119
Figure 5.1: Fraud data: Histograms of data with cut-oﬀs at 500 (left) and 50 (right)
models. Using uniform priors on all parameters, 2000 samples from the posterior for the
parameters were obtained.
Figure 5.2 displays the ﬁtted densities for the model from Section 5.1.2, while Figure
5.3 shows the corresponding posterior distributions. As we can see from the ﬁgures, the
model has a good performance overall.
Now, for the model from Section 5.1.4, we can assume a Gamma model for fH, and
that
(a) ω is ﬁxed to be 1. This is the usual model.
(b) ω is allowed to vary as a free parameter.
For model (a), with ω = 1 , and model (b), where ω is allowed to vary, we obtain the
posteriors shown in Figures 5.4–5.6.

5.1.
Blended Gamma-GPD model
120
Data with fitted densities
Y
Frequency
0
500
1000
1500
0
50
100
150
200
250
Figure 5.2: Fraud data: Fitted densities for the model with continuous ﬁrst derivative at the
blend point
For these parameters, the results are broadly similar; the most diﬀerent are the poste-
rior for β in the Gamma model, and the posterior for u. In model (b), the posterior for ω
is as follows: It is clear from Figure 5.6 that ω > 1 is heavily supported in the analysis.
Figure 5.7 uses an estimated distribution function formed from the Bayesian analysis to
carry out a goodness of ﬁt assessment using a P-P plot; it seems that the model where ω
varies is preferred.

5.1.
Blended Gamma-GPD model
121
Posterior for α
α
Frequency
0.45
0.50
0.55
0.60
0.65
0.70
0
100
200
300
400
Posterior for β
β
Frequency
0.0002
0.0004
0.0006
0.0008
0.0010
0.0012
0.0014
0
200
400
600
800
Posterior for u
u
Frequency
5
10
15
20
0
100
200
300
400
500
600
Posterior for σ
σ
Frequency
20
25
30
35
40
45
50
0
100
200
300
400
Posterior for ξ
ξ
Frequency
0.3
0.4
0.5
0.6
0.7
0.8
0.9
0
100
200
300
400
Figure 5.3: Fraud data: Posterior histograms for the model with continuous ﬁrst derivative at
the blend point. Top: α and β. Middle: u and σ Bottom: ξ

5.1.
Blended Gamma-GPD model
122
Figure 5.4: Fraud data: Posterior histograms for ω = 1
Figure 5.5: Fraud data: Posterior histograms for ω varying

5.1.
Blended Gamma-GPD model
123
Figure 5.6: Fraud data: Posterior histogram for ω in model (b) based on fB
Figure 5.7: P-P plots for data using Bayesian estimates from ﬁtted models. Left panel is model
(a) (ω = 1), right panel is model (b) (ω varying)

5.2.
A Bayesian nonparametric model
124
5.2
A Bayesian nonparametric model
It is well-known that the GPD model can be represented as a Gamma mixture of Expo-
nential distributions. Recall that if
fX|θ (x | θ) ≡Exponential (θ) x > 0
fθ (θ | γ, β) ≡Gamma (γ, β)
then for x > 0
fX(x|γ, β)
=
ˆ ∞
0
θe−θx βγ
Γ(γ)θγ−1e−βθ dθ
=
βγ
Γ(γ)
ˆ ∞
0
θ(γ+1)−1e−(β+x)θ dθ
=
βγ
Γ(γ)
Γ(γ + 1)
(β + x)γ+1
=
γ
β
1
(1 + x/β)γ+1
so setting γ = 1/ξ and β = σ/ξ yields the GPD(σ, ξ). Adding the ﬁxed location shift
parameter u is straightforward, this will be discussed later.
This suggests a natural Bayesian nonparametric model based on a Dirichlet process
mixture formulation. Consider a random sample X1, ..., Xn where Xi ∼Exponential(θi),
i = 1, ..., n and
θ1, ..., θn
i.i.d
∼Fθ,
where Fθ is constructed as follows. Let DP(ν, G0) be a Dirichlet process with parameter ν
and base distribution G0, taken to be the Gamma distribution with parameters(1/ξ, σ/ξ).
The parameter ν is a precision hyperparameter. The CDF Fθ is then a random (almost
surely discrete) CDF with prior specifying that for any set B,
E [Fθ (B)] = G0 (B) ,
V [Fθ (B)] = G0 (B) (1 −G0 (B))
ν + 1
.
As ν →∞, the unconditional distribution of X becomes the GPD(σ, ξ) distribution. For
ﬁnite ν however, the Bayesian nonparametric speciﬁcation imparts additional ﬂexibility.

5.2.
A Bayesian nonparametric model
125
This model is termed a Dirichlet process mixture (DPM) model; it allows for a ﬂexible
density speciﬁcation.
The parameter ν determines the degree of “clustering” in the Dirichlet Process model,
that is, the number of distinct values in the collection θ1, ..., θn. If K is the (random)
number of distinct values, recalling the result of Antoniak (1974), we have that
p (K | n, ν) = Sn,Kn!νKΓ (ν)
Γ (ν + n)
,
K = 1, ..., n
where Sn,k denotes the Stirling number of the ﬁrst kind. It holds that (Teh, 2010)
E [K | n, ν] = ν (ψ (ν + n) −ψ (ν)) ,
V [K | n, ν] = ν (ψ (ν + n) −ψ (ν)) + ν2 (ψ′ (ν + n) −ψ′ (ν)) ,
where ψ (·) and ψ′ (·) are the digamma and trigamma functions, respectively.
5.2.1
Simulation from the DPM model
To simulate from the DPM model (Mahmoud, 2008), we ﬁrst must specify how to simulate
the sample θ1, ..., θn; this is achieved via the P´olya urn scheme operating on the implicit
clusters that form part of the Dirichlet process speciﬁcation. The algorithm proceeds as
follows:
Step 1.
Set i = 1; set c1 = 1 as the ﬁrst cluster label.
Step 2.
For i = 2, 3, ..., n: let Ki−1 denote the number of distinct values in c1, ..., ci−1,
and let n1(i−1), ..., nKi−1(i−1) denote the counts of each of the distinct cluster
labels. Then generate
ci | c1, ..., ci−1 ∼
ν
ν + i −1δKi−1+1 +
1
ν + i −1
Ki−1
X
k=1
nk (i −1) δk,
that is, with probability ν/ (ν + i −1) generate a new cluster label Ki−1 + 1,
but with probability
nk (i −1)
ν + i −1 ,

5.2.
A Bayesian nonparametric model
126
set ci = k.
When i = n, the values c1, ..., cn form Kn clusters of labels
1, 2, ..., Kn, with n1(n), ..., nKn(n) in each cluster respectively.
Step 3.
Let K ≡Kn denote the number of distinct values in c1, ..., cn. For k = 1, ..., K,
generate ϑk ∼G0 independently, and for i = 1, ..., n set
θi =
K
X
k=1
ϑk1k (ci) .
The ϑ1, ..., ϑK values represent the cluster “locations” for the K clusters to
which the n data belong. The ﬁnal step involves generating Xi | θi.
Step 4.
Simulate Xi | θi ∼Exponential(θi).
5.2.2
Posterior inference under the DPM model
Given data x1, . . . , xn that are assumed to arise from a data generating process to be
modelled using a DPM, we seek a MCMC sampling scheme to perform posterior inference.
This can be achieved using a standard strategy that samples in turn
(i) the parameters θ1, . . . , θn;
(ii) the GPD parameters σ, ξ;
(iii) the Dirichlet process precision parameter ν;
from their full conditional distributions.
5.2.2.1
Sampling the θ parameters
In this step, we deﬁne the sampling weights as follows.
fX (x | σ, ξ) =
ˆ
∞
0
fX|θ (x | θ) fθ (θ | σ, ξ) dθ
(5.20)
is the marginal density for X after integrating out over the mixing distribution, and
pi (θ | xi, σ, ξ) is the posterior density derived from datum xi, formed as
pi (θ | xi, σ, ξ) ∝fX|θ (xi | θ) πθ (θ | σ, ξ) .
(5.21)

5.2.
A Bayesian nonparametric model
127
In the conjugate formulation we have utilized, both equations (5.20) and (5.21) can be
computed analytically: we have that
fX (x | σ, ξ) ≡GPD (x | σ, ξ) ,
(5.22)
as in the initial calculation, and
pi(θ|xi, ξ, σ) ∝θe−θxiθ1/ξ−1e−σθ/ξ = θ1/ξ+1−1e−(σ/ξ+xi)θ,
(5.23)
so that
pi(θ|xi, ξ, σ) ≡Gamma(1/ξ + 1, σ/ξ + xi).
(5.24)
Next, using a P´olya urn scheme that samples the parameter for datum i from its full
conditional posterior distribution we have that
θi | xi, θ(−i), σ, ξ, ν ∼w0pi (θ | xi, σ, ξ) +
X
j̸=i
wjδθj,
(5.25)
where
w0 =
νfX (xi | σ, ξ)
νfX (xi | σ, ξ) + P
j̸=i
fX|θ (xi | θj),
wj =
fX|θ (xi | σ, ξ)
νfX (xi | σ, ξ) + P
j̸=i
fX|θ (xi | θj),
j ̸= i.
In summary, for θi, we ﬁrst sample from the set {0, 1, 2, ..., i −1, i + 1, ..., n} with
probabilities {w0, w1, w2, . . . , wi−1, wi+1, . . . , wn}; if the value 0 is obtained, we sample
from θi from (5.24), whereas if index j is sampled, we set θi equal to θj.
In this scheme, the clustering of the θ values is not utilized in the sampling, although
it is of course present; obtaining index 0 and sampling from (5.24) generates a completely
new θ and a new cluster, but otherwise an existing value of θ is used. As before, we let
K denote the number of distinct values of θ, ϑ1, ..., ϑK denote these distinct values, and
c1, ..., cn denote the cluster labels denoting which datum belongs to which cluster.
5.2.2.2
Sampling the GPD parameters
The full conditional for (σ, ξ) given all other parameters is proportional to
(
K
Y
k=1
fθ (ϑk | σ, ξ)
)
π (σ, ξ) .
(5.26)

5.2.
A Bayesian nonparametric model
128
where fθ (· | σ, ξ) ≡Gamma (1/ξ, σ/ξ), that is, proportional to the likelihood of the dis-
tinct θ values multiplied by the prior π (σ, ξ). The distribution cannot be sampled directly,
so Metropolis-Hastings updates must be used.
5.2.2.3
Sampling the DP precision parameter
The full conditional for ν given ϑ1, ..., ϑK can be updated using an auxiliary variable
method designed by Escobar and West (1995). Suppose that the prior distribution for ν
is Gamma(a, b). To update ν, we introduce η with conditional distribution
η | ν ∼Beta (ν + 1, n) ,
(5.27)
and then update ν as
ν | η, K ∼









Gamma (a + K, b −log (η))
with prob.
a + K −1
a + K −1 + n (b −log (η)),
Gamma (a + K −1, b −log (η))
with prob.
n (b −log (η))
a + K −1 + n (b −log (η)).
(5.28)
5.2.3
Introducing the oﬀset u
If the GPD model pertains beyond a threshold u, then the model can be readily extended
for the right tail model. We have X1, ..., Xn independent where for Xi > u we have
Xi | u, θi ∼Shifted exponential (θi, u)
with density θie−θi(x−u), x > u. As before, we assume a Gamma (1/ξ, σ/ξ) model for θ.
For Xi ≤u, we might assume that Xi ∼Gamma(α, β), independent of θi. That is, we are
appealing to the continuous mixture representation
fX (x | u) =
ˆ
∞
0
fX|θ (x | u, θ) π (θ) dθ ≡







Gamma (α, β) ,
x ≤u,
GPD (u, σ, ξ) ,
x > u.
(5.29)

5.2.
A Bayesian nonparametric model
129
For a Bayesian nonparametric speciﬁcation, we again assume
θ1, ..., θn
i.i.d.
∼Fθ,
Fθ ∼DP (ν, G0 (σ, ξ)) .
(5.30)
where G0 (σ, ξ) is a GPD base distribution. The generative model is therefore represented
by the following simulation scheme:
1. Simulate θ1, ..., θn from the DP model using the P´olya urn scheme.
2. For i = 1, ..., n, generate zi ∼Uniform(0, 1).
a) If zi ≤FG(u; α, β), then simulate Xi from the Gamma(α, β) distribution trun-
cated at u; this may be done by cdf inversion or by rejection sampling.
b) If zi > FG(u; α, β), then simulate Vi ∼Exponential(θi), and set Xi = Vi + u.
For inference, the previous algorithm can be implemented with amendments. The most
notable changes occur in the sampling of the θis from θi | xi, θ(−i) in the P´olya urn scheme.
The two cases need separate consideration:
 xi ≤u: we have that the sampling weights are deﬁned as proportional to
νfG (xi | α, β, u)
for w0, where fG(· | α, β, u) is the Gamma(α, β) density truncated at u, and wj is
proportional to fG(xi | α, β, u) for each j ̸= i. Therefore the sum of the sampling
weights is (ν + n −1) fG(xi | α, β, u).
The posterior measure pi (θ | xi, u, σ, ξ) is
given by
pi (θ | xi) ∝fX (xi | u) πθ (θ | σ, ξ) ∝πθ (θ | σ, ξ) .
(5.31)
That is, we may sample θi in this case using a prior P´olya urn scheme
– with probability ν/ (ν + n −1) , sample θi ∼Gamma (1/ξ, σ/ξ);
– with probability wj = 1/ (ν + n −1) , j ∈{1, 2, ..., i −1, i + 1, ..., n} set θi = θj;

5.3.
A second Bayesian nonparametric model
130
 xi > u: in this case, the scheme reverts to the one from Section 5.2.2.1.
Conditional on θ1, ..., θn, the sampling of the Generalized Pareto parameters (σ, ξ) and
precision parameter ν proceeds as before. To sample the Gamma parameters (α, β) from
their full conditional distribution given the data and u, the likelihood
( Y
i : xi ≤u
fG(xi; α, β)
)
{1 −FG(u; α, β)}n−n1
(5.32)
is used, in conjunction with a suitable prior. Finally, to sample the parameter u from its
full conditional density given the other parameters, we utilize the likelihood
( Y
i : xi ≤u
fG(xi; α, β)
) ( Y
i : xi > u
θi exp{−θi(xi −u)}
)
.
(5.33)
5.3
A second Bayesian nonparametric model
A second Bayesian nonparametric approach uses a Dirichlet process prior on the parame-
ters of the GPD model. We assume that for a ﬁxed threshold u,
fX(x|ϕ, u) =
¨
Ω
fX (x | u, σ, ξ) F(dξ, dσ),
(5.34)
where Ω= (−1/2, ∞) × R+, and
F ∼DP (ν, G0 (ϕ))
and G0 (ϕ) is some base measure with hyperparameters ϕ and ν is the precision parameter.
For example, we might choose that G0 is a product of two Gamma distributions with
parameters (αξ, βξ) and (ασ, βσ) respectively, with the prior for ξ relocated to (−1/2, ∞).
The parametric analysis is recovered when ν →∞. An equivalent analysis is recovered
when the scale mixture of Exponentials is used, that is,
fX(x|ϕ, u) =
¨
Ω
ˆ ∞
0
fX|θ(x|θ, u)π(θ|σ, ξ)dθ

F(dξ, dσ),
(5.35)
where fX|θ (x | θ, u) is Exponential (θ) shifted by u, and π (θ | σ, ξ) is Gamma (1/ξ, σ/ξ).
In the initial formulation, we consider the case u = 0.

5.3.
A second Bayesian nonparametric model
131
5.3.1
Priors
Reasonable choices for the hyperparameters seem to be
(αξ, βξ) = (6, 5) ,
(ασ, βσ) = (3, 0.1) .
Note that the Jeﬀreys’ prior for ξ (see Section 4.1), given by
1
π
1
(1 + ξ) √1 + 2ξ,
−1
2 < ξ < ∞
(5.36)
is reasonably well approximated by Gamma(1/4, 1/4) relocated to the range (−1/2, ∞).
5.3.2
Inference
Given data x1, ..., xn assumed to arise from a data generating process to be modelled using
a DPM, we seek a MCMC sampling scheme to perform posterior inference. This can be
achieved using the strategy from Section 5.2.2. We sample
(i) the parameters (σ1, ξ1) , ..., (σn, ξn),
(ii) the hyperparameters (ασ, βσ) and (αξ, βξ),
(iii) the Dirichlet process precision parameter ν,
from their full conditional distributions. We use the notation ζi = (ξi, σi).
5.3.2.1
Sampling the ζ parameters
This step is achieved using a P´olya urn scheme that samples the pair of parameters for
datum i from its full conditional posterior distribution
ζi | xi, ζ(−i) ∼w0pi (ζ | xi) +
X
j̸=i
wjδζj,
(5.37)
suppressing the dependence on the hyperparameters, where
w0 =
νfX (xi)
νfX (xi) + P
j̸=i
fX|ζ (xi | ζj)
and
wj =
fX|ζ (xi | ζj)
νfX (xi) + P
j̸=i
fX|ζ (xi | ζj),
j ̸= i
(5.38)

5.3.
A second Bayesian nonparametric model
132
deﬁne the sampling weights, where
fX (x) =
ˆ
∞
−1/2
ˆ
∞
0
fX|ζ (x | ζ) πζ (ζ) dζ
(5.39)
is the marginal density for X after integrating out over the mixing distribution, and
pii (ζ | xi) is the posterior density derived from datum xi, formed as
pi (ζ | xi) ∝fX|ζ (xi | ζ) πζ (ζ) .
(5.40)
Here, equations (5.39) and (5.40) cannot be computed analytically. For (5.39) we can
use numerical integration eﬀectively, as this is merely a bivariate integral. This can be
achieved using quadrature, or using Monte Carlo, using the following ‘Rao-Blackwellized’
estimation strategy:
 Sample ζl = (σl, ξl), l = 1, . . . , L independently from πζ(·).
 Compute the density estimate
bfX(x) = 1
L
L
X
l=1
fX|ζ(x|ζl)
on a ﬁxed, ﬁne grid of x values, to form a look-up table.
 For any desired xi, use interpolation from the look-up table.
For (5.40), we must use MCMC sampling.
As before, for ζi, we ﬁrst sample from the set {0, 1, 2, ..., i −1, i + 1, ..., n} with prob-
abilities given by w0, w1, w2, ..., wi−1, wi+1, ..., wn; if the value 0 is obtained, we sample ζi
from (5.40) using MCMC, whereas if index j > 0 is sampled, we set ζi, equal to ζj .
As before, the clustering of the ζ values is not utilized in the sampling, although it is
of course present; obtaining index 0 and sampling from (5.40) generates a completely new
ζ and a new cluster, but otherwise an existing value of ζ is used. As before, we let K
denote the number of distinct values of ζ. Denote these distinct values by ϑ1, ..., ϑK, and
denote the cluster labels by c1, ..., cn, indicating which datum belongs to each cluster. Let
ϑk ⊂(σ∗
k, ξ∗
k).

5.3.
A second Bayesian nonparametric model
133
5.3.2.2
Sampling the hyperparameters
If these parameters are to be updated (which is not really necessary or advisable) we have
from our formulation that (ασ, βσ) and (αξ, βξ) are conditionally independent given all
other parameters. The full conditional for (αξ, βξ) is proportional to
( K
Y
k=1
fξ(ξ∗
k|αξ, βξ)
)
π(αξ, βξ),
(5.41)
where fξ(·|αξ, βξ) is Gamma(αξ, βξ) that is, proportional to the likelihood of the distinct
values ξ∗multiplied by the prior for these parameters, π(αξ, βξ). The distribution cannot
be sampled directly, so Metropolis-Hastings updates must be used.
5.3.3
Extending the second Bayesian nonparametric model
For a ﬁxed u > 0 the formulation proceeds as before. We have
fX(x|ϕ, u) =
¨
Ω
fX(x|ξ, σ, u)F(dξ, dσ),
where Ω= (−1/2, ∞)×R+, and F ∼DP(α, G0(ϕ)) and G0(ϕ) is some base measure with
hyperparameters ϕ. The support of this density is clearly (u, ∞).
Most of the details go through as before for implementing the DP components, but we
additionally condition on u. To sample the ζs, we again consider a P´olya urn scheme that
samples the pair of parameters for datum i from its full conditional posterior distribution
ζi|xi, ζ(−i), u ∼w0pi(ζ|xi, u) +
X
j̸=i
wjδζj,
suppressing the dependence on the hyperparameters, where
w0 =
νfX(xi|u)
νfX(xi|u) + P
l̸=i
fX|ζ(xi|ζl, u)
and
wj =
fX|ζ(xi|ζj, u)
νfX(xi|u) + P
l̸=i
fX|ζ(xi|ζl, u), j ̸= i
deﬁne the sampling weights, where
fX(x|u) =
ˆ ∞
−1/2
ˆ ∞
0
fX|ζ(x|ζ, u)πζ(ζ) dζ
(5.42)

5.4.
A nonparametric version of the model from Section 5.1.4
134
is the marginal density for X after integrating out over the mixing distribution, and pi(ζ|xi)
is the posterior density derived from datum xi, formed as
pi(ζ|xi, u) ∝fX|ζ(xi|ζ, u)πζ(ζ).
(5.43)
For (5.42) we can again use numerical integration eﬀectively, as this is merely a bivariate
integral; to compute for each u, note that
fX(x|u) = fX(x −u),
where the right hand side is the density computed for u = 0 from (5.39). For (5.43),
we again use MCMC sampling.
Sampling the hyperparameters and the DP precision
parameter proceed as before.
5.4
A nonparametric version of the model from
Section 5.1.4
Suppose again, as in Section 5.1.4, that the data density fB takes the form
fB(x; u, φ, ξ, σ) =











fH(x; φ)
FH(u; φ) + ω(1 −FH(u; φ)),
x ≤u,
ω(1 −FH(u; φ))fG(x; u, ξ, σ)
FH(u; φ) + ω(1 −FH(u; φ)) ,
x > u.
A nonparametric speciﬁcation for the component fG of this model can be considered,
speciﬁcally the DPM from Section 5.3.3.
5.4.1
Implementing the DPM in the model from Section 5.1.4
The only complicated step involves sampling the ζ = (σ, ξ) from their full conditional
density as indicated by Equations (5.37) and (5.38). Recall that we need to sample ζi
from its full conditional density
ζi|xi, ζ(−i), u ∼w0pi(ζ|xi, u) +
X
j̸=i
wjδζj

5.4.
A nonparametric version of the model from Section 5.1.4
135
for each i = 1, . . . , n, where
w0 =
νfX(xi|u)
νfX(xi|u) + P
l̸=i
fX|ζ(xi|ζl, u)
and
wj =
fX|ζ(xi|ζj, u)
νfX(xi|u) + P
l̸=i
fX|ζ(xi|ζl, u), j ̸= i
with
fX(x|u) =
ˆ ∞
−1/2
ˆ ∞
0
fX|ζ(x|ζ, u)πζ(ζ) dζ
and
pi(ζ|xi, u) ∝fX|ζ(xi|ζ, u)πζ(ζ),
the full conditional posterior for ζ given xi.
The two cases xi ≤u and xi > u need separate consideration:
 xi ≤u: in this case, the density for x does not depend on ζ, so using the same logic
as in Section 5.2.3, we have that the sampling weights are proportional to
νfH(xi|φ, u)
for w0, where fH(.|φ, u) is the density fH truncated to (0, u), and wj is fH(xi|φ, u)
for each j ̸= i. Therefore the sum of the sampling weights is (ν + n −1)fH(xi|φ, u).
The posterior measure pi(ζ|xi, u, ) is given by
pi(ζ|xi, u) ∝fH(xi|φ, u)πζ(ζ) ∝πζ(ζ).
That is, we may sample ζi in this case using a prior P´olya urn scheme
– with probability ν/(ν + n −1), sample ζi ∼πζ(·)
– with probability wj = 1/(ν + n −1), j ∈{1, ..., i −1, i + 1, ..., n}, set ζi = ζj.
 xi > u: in this case, the standard scheme applies. We have by assumption that the
data density is
ω(1 −FH(u; φ))fG(x; u, ζ)
FH(u; φ) + ω(1 −FH(u; φ)) = MfG(x; ζ, u)

5.4.
A nonparametric version of the model from Section 5.1.4
136
say, so therefore
w0 ∝νMfG(xi|u) = νM
ˆ ∞
−1/2
ˆ ∞
0
fX|ζ(xi|ζ, u)πζ(ζ) dζ
and
wj ∝MfG(xi|ζj, u),
j ̸= i.
That is, the constant M cancels, and we may sample ζi as follows. Let
Si =
X
j̸=i
fX|ζ(xi|ζj, u)
then
– with probability νfG(xi|u)/(νfG(xi|u)+Si), sample ζi ∼pi(ζ|xi, u) using Metropolis-
Hastings;
– with probability wj = 1/(νfG(xi|u) + Si), j ∈{1, 2, ..., i −1, i + 1, ..., n} set
ζi = ζj.
5.4.2
Updating the remaining parameters
5.4.2.1
Parameters of the density fH
The parameters φ of the component fH can be updated in the usual way for a paramet-
ric analysis. Conditional on the parameters ζ1, . . . , ζn, ω and u, we have that the full
conditional likelihood takes the form
( Y
i : xi≤u
fH(xi; φ)
FH(u; φ) + ω(1 −FH(u; φ))
) ( Y
i : xi>u
ω(1 −FH(u; φ))fG(xi; ζi, u)
FH(u; φ) + ω(1 −FH(u; φ))
)
(5.44)
which, if nG denotes the number of xi > u, is proportional to
( Y
i : xi≤u
fH(xi; φ)
)
(1 −FH(u; φ))nG
[FH(u; φ) + ω(1 −FH(u; φ))]n
A prior on φ completes the speciﬁcation.

5.4.
A nonparametric version of the model from Section 5.1.4
137
5.4.2.2
Threshold parameter u and scaling parameter ω
The parameter u is updated using the conditional likelihood in equation (5.44), in con-
junction with a suitable prior. The same is true for scaling parameter ω. Both updates
are achieved using Metropolis-Hastings.
5.4.2.3
Dirichlet process parameters
The hyperparameters of the Dirichlet process base measure, and the Dirichlet process
precision parameter, are updated as in previous sections.
5.4.3
Real data example
As before, we tested the Bayesian nonparametric models on diﬀerent data sets. We only
show the results for the fraud data from Section 3.2.1. This time we focus on the ﬁrst
Bayesian nonparametric model and on the nonparametric version of the model from Section
5.1.4.
Following the procedure from Sections 5.2.1 and 5.4.1, we obtained 2000 samples from
the posterior distributions using MCMC. Again, we assume a Gamma model for fH.
Results for the ﬁrst Bayesian nonparametric model are shown in Figures 5.8–5.10, while
Figures 5.11–5.13 display the results for the nonparametric version of the model from
Section 5.1.4. In Figure 5.12, we can observe samples of u, ω and fH model parameters.
We can also assess the model performance using the P-P plots. From the ﬁgures one may
notice that the model ﬁt is good in general, although it can be improved by introducing
the ω parameter.

5.4.
A nonparametric version of the model from Section 5.1.4
138
Figure 5.8: Fraud data: Posterior samples of the precision parameter for the ﬁrst Bayesian
nonparametric model. The blue line is a Gamma prior for ν in terms of frequencies
σ
Frequency
50
100
150
200
0
50
100
150
200
ξ
Frequency
0
2
4
6
8
10
0
100
200
300
400
500
600
700
Figure 5.9: Fraud data: Histograms of the posterior MCMC estimates for the ﬁrst Bayesian
nonparametric model

5.4.
A nonparametric version of the model from Section 5.1.4
139
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
c(1:n)/(n + 1)
Pest
Figure 5.10: Fraud data: P-P plot using the posterior MCMC estimates from the ﬁrst Bayesian
nonparametric model
Precision parameter
ν
Frequency
0
5
10
15
20
0
100
200
300
400
500
Figure 5.11: Fraud data: Posterior samples of the precision parameter for the nonparametric
version of the model with discontinuous density with arbitrary scaling. The blue line is a Gamma
prior for ν in terms of frequencies

5.4.
A nonparametric version of the model from Section 5.1.4
140
0
500
1000
1500
2000
5
10
15
20
Samples
u
0
500
1000
1500
2000
0.8
1.0
1.2
1.4
Samples
α
0
500
1000
1500
2000
0.1
0.3
0.5
Samples
β
0
500
1000
1500
2000
0
10
20
30
40
50
Samples
ω
Figure 5.12: Fraud data: Posterior parameter samples from the nonparametric version of the
model with discontinuous density with arbitrary scaling
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
P−P plot
i/(n+1)
F(q)
Figure 5.13: Fraud data: P-P plot using the posterior MCMC estimates from the nonparametric
version of the model with discontinuous density with arbitrary scaling

5.5.
Conclusions of the Chapter
141
5.5
Conclusions of the Chapter
In this chapter, we have proposed several new models. The ﬁrst model has a density
continuous at the qth quantile, ﬁxing the discontinuity issue in the model by Behrens et
al. (2004) in a simple way. Nonetheless, q is a ﬁxed quantity. Thus, in the second model
we require the ﬁrst derivative to be continuous at the blend point, so that q is not ﬁxed
anymore. The third model has a continuous ﬁrst derivative, but this time the scaling is
arbitrary, i.e., the density is scaled by some ω > 0, which gives greater ﬂexibility. The
last Bayesian model is based on a discontinuous density with arbitrary scaling. One of
the main features of this model is that it can be implemented even if the density and its
derivative are discontinuous.
As for the Bayesian nonparametric models, we have recalled some properties of the
GPD, in particular that it can be represented as a Gamma mixture of Exponentials,
suggesting a model based on a Dirichlet Process Mixture representation. We have also
introduced a second Bayesian nonparametric model that diﬀers from the previous one in
that it uses a Dirichlet process prior on the parameters of the GPD. Similarly, we have
applied this last model to the case when the density is discontinuous and the scaling is
arbitrary.
As we have seen in our analysis, all these models can be easily implemented using
MCMC methods. For the fraud data, we have observed that the proposed models have a
good overall performance, providing a powerful alternative to deal with the discontinuity
issue in the model introduced by Behrens et al. (2004).

Chapter 6
Conclusions and future research
6.1
Contributions of the thesis
Extreme Value Theory based models have been widely used in many ﬁelds as they supply
a statistically justiﬁable and ﬂexible method for extrapolating tail distributions. In the
context of operational risk, EVT oﬀers an interesting view on the quantitative measure-
ment of risk. As Embrechts (2002) pointed out: “The main virtue of EVT is that it gives
the user a critical view on and a methodological toolkit for issues like skewness, fat tails,
rare events, stress scenarios,...”; however, as we have discussed through this thesis, there
are still serious limitations that have to be overcome to make it viable; for instance, this
methodology requires a good choice of a threshold to have a good performance, which most
of the times is problematic. Furthermore, the Basel II requirements for implementing the
Advanced Measurement Approach are not completely met.
This dissertation has investigated how a Bayesian analysis of extremes oﬀers and al-
ternative to the standard classical one, in the sense that it is able to account for the
uncertainty associated with the threshold choice by considering it as another parameter of
the model to be estimated. It also oﬀers the ability to supplement information provided by
the data with other sources of information, through the prior distribution. In addition, the
142

6.1.
Contributions of the thesis
143
output of a Bayesian analysis provides posterior information which can also be exploited
for extrapolation, via the posterior predictive distribution.
The ﬁrst contribution of this work was presented in Chapter 3, where both the model
proposed and its application to operational risk data showed how Bayesian inference allows
for parameter estimation to determine the loss distribution, by using all data and prior
information through a Bayesian model. Our study has shown the importance of considering
the threshold uncertainty in the estimation, as the GPD parameters are very sensitive with
respect to this value. We also observed how the inclusion of expert opinion is an important
input, particularly when we have small data sets, as shown in Chapter 3.
Another interesting discovery was that the Basic Indicator Approach –the most com-
monly used method for estimating the minimum capital requirement–, although is a good
start, rarely provides a good estimate, since it does not take into account either the data
or expert opinion.
One of the most signiﬁcant ﬁndings to emerge from this study was that expert opinion
represents a valuable but underestimated source of information. In many cases, to simplify
the problem, it is assumed that the information experts can provide is not signiﬁcant and
statisticians tend to use non-informative priors instead of priors based on expert judgment.
Nevertheless, as shown in Chapter 4, many of the non-informative prior distributions
are unrealistic. It was further noted that expert opinion can dramatically inﬂuence the
estimates in the context of extreme events and hence, one should pay special attention to
the elicitation process.
Furthermore, we addressed a common problem in practice: the combination of mul-
tiple expert opinions. Although this topic has been widely studied, little research has
focused on operational risk. Our work thus makes a noteworthy contribution in the sense
that it provides the tools for combining and updating multiple opinions on extreme data.
Moreover, we could notice how the inclusion of diﬀerent opinions may inﬂuence the results
and only after some years, when more data become available, the inﬂuence of experts is

6.2.
Future research
144
limited.
The last contribution of this thesis was the introduction of several new models that
allow to deal with the discontinuity problem observed in previous models. Our models
are a modiﬁcation on a previously introduced model (Behrens et al., 2004). In this last
Chapter, we provided the theory and the sampling scheme for all the diﬀerent versions of
our model. This can be used in order to improve the performance of Bayesian models and
parameter estimates, providing a more realistic approach to the loss distribution. We also
explored Bayesian nonparametric models, giving more ﬂexibility to the analysis of extreme
events and laying the foundation for future work in such models.
6.2
Future research
The ﬁndings of this study have a number of important implications for future practice,
and we strongly advocate that Bayesian approaches to operational risk modelling should
be considered as a serious alternative for practitioners in banks and ﬁnancial institutions.
It is worth pointing out that these models may be improved and extended in a number
of directions. Currently, the use of Bayesian models in extremes is an active research
area. For instance, some authors have extended these models to include covariates in a
GPD model formulation (Cabras et al., 2011) while others are treating this problem in a
nonparametric setting (Wang et al., 2011).
As part of our future research, we have considered the problem of dealing with losses
caused by diﬀerent eﬀects. In the next section we develop some ideas and present some of
our ﬁndings for the study of mixture distributions in EVT.
6.2.1
Finite mixture distributions and EVT
Extreme value theory is usually applied to single loss distributions; however, there is the
case where losses are caused by two or more independent eﬀects, each happening with a

6.2.
Future research
145
certain probability. In this situation, we should consider mixture models, i.e. distributions
of the form F (x) = P
i∈I
wiFi (x) for x ∈R, where wi ≥0, i ∈I and P
i∈I wi = 1; the index
set I is assumed to be ﬁnite or countable and {Fi : i ∈I} is a family of univariate
distribution functions. But this leads to a diﬀerent problem. Recently, some authors have
investigated the performance of the POT method under these circumstances. For instance,
Neˇslehov´a et al. (2006) point out the potential shortcomings of the classical POT model
in the presence of “data contamination”, that is, observations within the sample, which
do not follow the same distribution as the rest of the data. They provide some examples
based on mixtures of Pareto distributions, showing that very high losses are driven for one
of the mixture components, which leads to incorrect estimates of high quantiles.
In this section we explore whether extreme value theory can be applied to mixture
distributions. We start by reviewing some results from the literature, particularlly a theo-
rem in the paper by Kang and Serfozo (1999), which shows that for mixture distributions,
the limiting distribution is determined by one or more distributions among the mixtures
whose tails dominate the other tails. They also derive bounds and rates of convergence.
Deﬁnition 6.1. Suppose F is a mixture of the form
F (x) =
X
i∈I
wiFi (x) , x ∈R,
(6.1)
where {Fi : i ∈I} is a ﬁnite or countable collection of distributions and the wi are non-
negative constants such that P
i∈I wi = 1.
We say that the tail of the distribution F ∗with right endpoint x∗dominates those of
{Fi : i ∈I} if, for each i ∈I, the right endpoint of Fi is x∗and
lim
x→x∗
¯Fi (x)
¯F ∗(x) = ri,
for some ﬁnite ri ≥0, and this limit is uniform in i in case I is an inﬁnite set. The
coeﬃcients ri are called the tail ratios. F ∗and Fi are called tail equivalent if ri > 0, and
the tail of F ∗strictly dominates that of Fi when ri = 0.

6.2.
Future research
146
Theorem 6.2. Suppose F is a mixture of the form (6.1) and there is a distribution
F ∗∈{Fi : i ∈I} whose tail dominates those of {Fi : i ∈I} with tail ratios such that
γ ≡P
i wiri is positive. Then the following statements are equivalent.
(i) F ∈MDA(H) with normalizing constants cn, dn.
(ii) F ∗∈MDA(H) with normalizing constants c∗
n, d∗
n.
When these statements hold, the normalizing constants are related as follows.
cn = c∗
n,
dn = d∗
n + c∗
nlog γ
if H is Gumbel
cn = γ
1/αc∗
n,
dn = d∗
n = 0
if H is Fr´echet
cn = γ−1/αc∗
n,
dn = d∗
n
if H is Weibull











(6.2)
Proof of Theorem 6.2. For simplicity, let xn = cnx + dn. We ﬁrst show that statement
(ii) is equivalent to
n ¯F ∗(xn) →−γ−1 log H (x) , x ∈R,
(6.3)
where cn, dn are given by (6.2). Suppose that (ii) holds and note that it is equivalent to
n ¯F ∗(c∗
nx + d∗
n) →−log H (x) .
(6.4)
by (2.11). If H is Gumbel, i.e. H (x) = e−e−x, then xn = c∗
n (x + log γ) + d∗
n and so by
(6.4),
n ¯F ∗(xn) →−log H (x + log γ) = −γ−1log H (x) .
If H is Fr´echet or Weibull, it follows by a similar argument that
xn =







c∗
n
 xγ
1/α
+ d∗
n
if H is Fr´echet,
c∗
n
 xγ−1/α
+ d∗
n
if H is Weibull,
respectively, and hence
n ¯F ∗(xn) →





−log H
 xγ
1/α
−log H
 xγ−1/α





= −γ−1log H (x) .
The last equality is due to the two forms of H. This proves that (ii) implies (6.3). Con-
versely, suppose that (6.3) holds and deﬁne c∗
n, d∗
n by (6.2). Then arguing as above, it
follows that, for the three forms of H,

6.2.
Future research
147
n ¯F ∗(c∗
nx + d∗
n) →











−γ−1log H (x −log γ)
−γ−1log H
 xγ−1/α
−γ−1log H
 xγ
1/α











= −log H (x) .
Thus F ∗∈MDA (H) , which proves that (ii) is equivalent to (6.3).
We now consider the equivalence of (i) and (ii). Because of (2.11), this is equivalent
to showing that (6.3) is equivalent to which is equivalent to
n ¯F (xn) →−log H (x) .
(6.5)
By the deﬁnition of F,
n ¯F (xn) = n ¯F ∗(xn)
X
i∈I
wi
¯Fi(xn)
¯F ∗(xn).
(6.6)
Since the tail of F ∗dominates those of the Fi, we have that ¯Fi(xn)/ ¯F ∗(xn) →ri as n →∞
for i ∈I. Furthermore, this convergence is uniform in i when I is an inﬁnite set. Thus,
by the dominated convergence theorem, we have
X
i
wi
¯Fi(xn)
¯F ∗(xn) →
X
i
wiri = γ.
(6.7)
This implies that (6.3) is equivalent to (6.5) by (6.6).
6.2.1.1
Simulation study
In the previous section we studied the theoretical foundations of extreme value theory for
mixture distributions. In order to compare the diﬀerences between theory and practice, we
carry out a simulation study. We divide our study in two parts: 1. Behaviour of the GPD
scale and shape parameters when dealing with mixture distributions and 2. Estimation of
high quantiles for a particular mixture distribution.
6.2.1.2
GPD scale and shape parameters for mixture distributions
In this part of our study, we simulate data from three diﬀerent (two-component) mixtures
with I = {1, 2}:

6.2.
Future research
148
1. F1 is N(100, 202) and F2 is N(150, 252).
2. F1 is N(700, 502) and F2 is Gamma(100, 0.1).
3. F1 is t(2,10) and F2 is t(5,15).
We start by exploring the tail dominance of these distributions. For a mixture of Gaussians
we have:
lim
x→∞
¯Fi (x)
¯Fj (x)
=
lim
x→∞
fi (x)
fj (x) = lim
x→∞
1
σi
√
2πexp
h
−(x−µi)2
2σ2
i
i
1
σj
√
2πexp
h
−(x−µj)2
2σ2
j
i
=
σj
σi
exp

−µ2
i
2σ2
i
+ µ2
j
2σ2
j

lim
x→∞exp
 1
σ2
j
−1
σ2
i
 x2
2 +
 µi
σ2
i
−µj
σ2
j

x

.
Thus ¯Fi (x) / ¯Fj (x) converges to 0 as x →∞if
 1
σ2
j
−1
σ2
i

< 0 or µi < µj
for σi = σj.
Therefore, the tail of Fj strictly dominates that of Fi when σi < σj or µi < µj for σi = σj.
Hence, for the mixture N(100, 202) -N(150, 252), the tail is dominated by N(150, 252).
Similar results can be derived for the other mixtures.
In order to observe the behaviour of these distributions in practice, we simulate 1,000
observations from each mixture and assign equal weights wi = 0.5 i = 1, 2 to each com-
ponent. The corresponding densities are shown in Figure 6.1.
We approximate the tail of each mixture using the POT method and maximum like-
lihood for the estimation of the scale (σ) and shape (ξ) parameters. The threshold is set
as the 0.7, 0.8 and 0.9 empirical quantile. Results are displayed in Tables 6.1-6.6.
From Table 6.1, we can observe that, under the classic GPD estimation, the estimate
of the ξ parameter—denoted by ˆξ—is always larger for the mixture than for the single
normals. The Bayesian estimates present a similar behaviour and, again, the ˆξ parameter
is larger for the mixture (Table 6.2). In most cases, ˆξ is far from the theoretical value of
0, since the normal distribution belongs to the Gumbel domain of attraction.

6.2.
Future research
149
0
50
100
150
200
250
0.000
0.005
0.010
0.015
0.020
0.025
0.030
Density
N(100,202)
N(150,252)
Mixture
0
500
1000
1500
2000
0.000
0.001
0.002
0.003
0.004
0.005
Normal−Gamma mixture
Density
N(750,100)
Ga(100,0.1)
Mixture
0
50
100
150
200
0.00
0.02
0.04
0.06
0.08
0.10
Density
Student−t(2,10)
Student−t(5,15)
Mixture
Figure 6.1: Simulated mixture densities
For the Normal-Gamma mixture we can see in Table 6.3 that the shape parameter of
the mixture is larger in almost all cases under the classical GPD estimation, except for
u = 0.7 quantile. However, for the Bayesian estimates, the ˆξ parameter of the mixture is
smaller compared to the shape parameter of the single distributions.
On the other hand, for the mixture of noncentral-t distributions, from Tables 6.5 and
6.6 we can see that ˆξ behaves as previously, being larger for the mixture in all cases,
except for u = 0.9 quantile. The theoretical values of ξ = 0.5, 0.2, respectively, are better
approximated under the Bayesian approach.
Now, in order to observe the eﬀect of increasing the sample size, we simulate 2,000
observations from the previous noncentral t-distributions (Figure 6.2). Results for the

6.2.
Future research
150
Distribution
u = q0.7
σ
ξ
u = q0.8
σ
ξ
u = q0.9
σ
ξ
Mixture
146.685
24.522
-0.226
157.513
19.737
-0.179
172.163
13.005
-0.043
N(100, 202)
109.585
15.897
-0.307
115.648
14.258
-0.318
124.307
11.859
-0.336
N(150, 252)
164.468
18.652
-0.260
172.338
15.619
-0.226
181.785
14.622
-0.276
Table 6.1: Classic GPD estimation for the mixture of normals.
Mixture
N(100, 202)
N(150, 252)
Parameter
Mean, median, std
Mean, median, std
Mean, median, std
u
152.839, 152.777, 1.042
100.610, 100.444, 1.498
170.189, 172.163, 4.969
σ
12.242, 11.541, 2.615
18.330, 18.461, 1.507
22.127, 22.127, 1.789
ξ
0.0168, 0.012, 0.109
-0.286, -0.291, 0.051
-0.259, -0.264, 0.056
Table 6.2: Bayesian parameter estimates for the mixture of normals.
Distribution
u = q0.7
σ
ξ
u = q0.8
σ
ξ
u = q0.9
σ
ξ
Mixture
974.868
130.720
-0.365
1030.631
98.825
-0.299
1092.69
79.450
-0.293
N(700,
√
50)
806.842
94.180
-0.385
841.525
82.837
-0.402
889.185
74.334
-0.519
Gamma(100,0.1)
806.842
86.709
-0.260
1083.393
84.352
-0.307
1131.591
80.534
-0.402
Table 6.3: Classic GPD estimation for the Normal-Gamma mixture.
Mixture
N(700,50)
Gamma(100,0.1)
Parameter
Mean, median, std
Mean, median, std
Mean, median, std
u
910.686, 910.141, 3.024
779.329, 778.683, 2.625
1008.302, 1008.372, 0.638
σ
119.698, 118.807, 6.255
60.753, 60.330, 6.067
64.387, 64.130, 4.883
ξ
-0.212, -0.209, 0.040
-0.007, -0.011, 0.091
0.012, 0.008, 0.070
Table 6.4: Bayesian parameter estimates for the Normal-Gamma mixture.
classic GPD estimation are displayed in Table 6.7. This time we can notice that the shape
parameter value of the mixture is between the other two parameters’ values.
Bayesian estimates are presented in Table 6.8. Again, the shape parameter value of
the mixture is between the other two parameters’ values.
Hence, one may infer that
increasing the sample size decreases the eﬀect of the dominant tail and leads to better
approximations. Nonetheless, in practice, we usually have a small number of observations.

6.2.
Future research
151
Distribution
u = q0.7
σ
ξ
u = q0.8
σ
ξ
u = q0.9
σ
ξ
Mixture
18.149
6.728
0.511
21.099
8.203
0.524
27.940
14.188
0.372
t(2,10)
17.131
8.664
0.414
20.169
12.919
0.266
30.490
11.656
0.460
t(5,15)
19.273
5.658
0.223
21.790
5.879
0.254
26.042
7.460
0.220
Table 6.5: Classic GPD estimation for the mixture of Student-t distributions (1,000 observa-
tions).
Mixture
t(2,10)
t(5,15)
Parameter
Mean, median, std
Mean, median, std
Mean, median, std
u
17.571, 17.270, 1.520
17.63, 17.471, 1.430
19.084, 17.751, 0.632
σ
6.278, 6.183, 0.713
6.378, 6.319, 0.716
5.645, 5.568, 0.910
ξ
0.521, 0.518, 0.079
0.519, 0.528, 0.094
0.232, 0.221, 0.116
Table 6.6: Bayesian parameter estimates for the mixture of Student-t distributions (1,000 ob-
servations).
0
100
200
300
400
500
0.00
0.02
0.04
0.06
0.08
0.10
Density
Student−t(2,10)
Student−t(5,15)
Mixture
Figure 6.2: Mixture of Student-t distributions (2,000 observations).
6.2.1.3
High quantiles estimation
In this section we explore the behaviour of high quantiles when taking into account the
mixture of components instead of a single distribution. To do so, we consider a mixture
of Generalized Pareto distributions:
w1GPD (σ1, ξ1) + w2GPD (σ2, ξ2) ,
w2 = 1 −w1.
Our aim is to estimate the scale and location parameters and the corresponding weights.
By doing so, we will be able to compute the Value-at-Risk for diﬀerent parameter sets and

6.2.
Future research
152
Distribution
u = q0.7
σ
ξ
u = q0.8
σ
ξ
u = q0.9
σ
ξ
Mixture
18.611
7.276
0.384
22.213
7.187
0.484
27.979
9.792
0.512
t(2,10)
16.504
9.913
0.589
21.117
11.803
0.636
30.910
16.495
0.742
t(5,15)
18.943
6.393
0.108
21.659
6.502
0.124
26.465
7.411
0.082
Table 6.7: Classic GPD estimation for the mixture of Student-t distributions (2,000 observa-
tions).
Mixture
t(2,10)
t(5,15)
Parameter
Mean, median, std
Mean, median, std
Mean, median, std
u
20.821, 20.801, 2.498
11.893, 11.722, 0.245
17.312, 17.270, 1.064
σ
7.342, 7.254, 0.624
7.912, 7.855, 0.633
6.002, 5.976, 0.509
ξ
0.431, 0.423, 0.082
0.555, 0.552, 0.072
0.130, 0.121, 0.060
Table 6.8: Bayesian parameter estimates for the mixture of Student-t distributions (2,000 ob-
servations).
sample sizes.
Before doing the estimation, note that inference for mixtures of the classical Pareto
distributions is diﬃcult because the EM algorithm cannot be applied; see Bee et al. (2013).
These authors point out the theoretical and computational diﬃculties in carrying out the
estimation using MLE. They show that when we have diﬀerent shapes and scales, the
likelihood is non-regular and the EM algorithm breaks down. That is, the estimator of the
scale parameter is not asymptotically eﬃcient since the distributions of ˆσ1 and ˆσ2 do not
have common support, violating one of the regularity conditions for asymptotic eﬃciency
of MLEs.
Whether similar results apply also to mixtures of the Generalized Pareto distribution
remains to be investigated. Keeping this fact in mind, we decided to perform a simple
MLE estimation using the function optim in R. The estimation was carried out for diﬀerent
data sets, keeping σ1, σ2, ξ1 and ξ2 ﬁxed, and varying the weights wi and sample size
n = 150, 500 and 1000. The data used in this study come from the following distributions:
1. 0.2 GPD(1,0.5)+0.8 GPD(3,0.2)
2. 0.5 GPD(1,0.5)+0.5 GPD(3,0.2)

6.2.
Future research
153
3. 0.7 GPD(1,0.5)+0.3 GPD(3,0.2)
Table 6.9 shows the parameter estimates. From the table, we can see that most of the time,
values are not estimated correctly. Notwithstanding, the essential purpose of this work
is to compare the performance of the classical POT method with respect to the mixture
of GPDs. To do so, we compute the Value-at-Risk using both methods. In general, the
Value-at-Risk of mixture distributions is no longer given by a simple formula and has to
be calculated numerically.
Tables 6.10–6.18 display the estimates of the Value-at-Risk at diﬀerent levels. We can
see that in general, the mixture produces good estimates of VaR0.9, VaR0.95 and VaR0.99,
as the mean square error (MSE) is small; although, for higher quantiles (0.999 and 0.9999)
neither the mixture nor the classical POT method give accurate estimates and the MSE
increases considerably.
However, the mixture estimates are slightly closer to the true
values than the classical POT estimates in almost all cases. It is worth to mention that
the estimates are very sensitive to the sample size and we have to be very careful with our
conclusions.
This analysis does not pretend to be exhaustive, but may be considered as a tentative,
to be extended to more general situations, in the aim to improve the application of extreme
value theory methods to mixture distributions, as more sophisticated methods become
available to estimate the parameters of the mixture of GPDs.
0.2 GPD(1,0.5)+0.8 GPD(3,0.2)
0.5 GPD(1,0.5)+0.5 GPD(3,0.2)
0.7 GPD(1,0.5)+0.3 GPD(3,0.2)
Par.
True
value
Estimate
True
value
Estimate
True
value
Estimate
n = 150
n = 500
n = 1000
n = 150
n = 500
n = 1000
n = 150
n = 500
n = 1000
w1
0.2
0.047
0.658
0.733
0.5
0.547
0.056
0.729
0.7
0.100
0.999
0.799
σ1
1
0.088
2.492
2.073
1
1.173
0.207
1.057
1
0.103
1.167
1.105
ξ1
0.5
-0.002
0.287
0.327
0.5
0.443
0.559
0.487
0.5
0.815
0.478
0.329
σ2
3
2.542
3.545
5.931
3
3.852
1.964
4.036
3
1.744
2.348
5.211
ξ2
0.2
0.269
-0.077
-0.092
0.2
-0.108
0.347
-0.047
0.2
0.422
0.348
0.172
Table 6.9: Parameter estimates for a mixture of GPDs

6.2.
Future research
154
0.2 GPD(1,0.5)+0.8 GPD(3,0.2)
Mixture of GPD’s
Classical POT
γ
True value
V aRγ
MSE
V aRγ
MSE
0.9
8.109
7.877
0.053
7.756
0.125
0.95
11.594
11.430
0.027
11.594
6.671e-07
0.99
22.116
22.746
0.397
25.043
8.569
0.999
46.945
50.376
11.774
63.575
276.563
0.9999
102.650
101.721
0.862
149.458
2191.039
Table 6.10: VaR for a mixture of GPDs, n = 150
0.2 GPD(1,0.5)+0.8 GPD(3,0.2)
Mixture of GPD’s
Classical POT
γ
True value
V aRγ
MSE
V aRγ
MSE
0.9
8.109
7.847
0.068
8.124
2.091e-04
0.95
11.594
10.798
0.632
11.356
0.056
0.99
22.116
20.328
3.197
20.738
1.898
0.999
46.945
47.188
0.059
40.325
43.826
0.9999
102.650
99.468
10.123
70.738
1018.368
Table 6.11: VaR for a mixture of GPDs, n = 500
0.2 GPD(1,0.5)+0.8 GPD(3,0.2)
Mixture of GPD’s
Classical POT
γ
True value
V aRγ
MSE
V aRγ
MSE
0.9
8.109
9.068
0.919
8.697
0.345
0.95
11.594
12.706
1.236
12.571
0.955
0.99
22.116
22.272
0.024
24.950
8.030
0.999
46.945
48.608
2.765
55.325
70.223
0.9999
102.650
110.469
61.137
112.092
89.142
Table 6.12: VaR for a mixture of GPDs, n = 1000
0.5 GPD(1,0.5)+0.5 GPD(3,0.2)
Mixture of GPD’s
Classical POT
γ
True value
V aRγ
MSE
V aRγ
MSE
0.9
6.927
6.621
0.093
6.872
0.003
0.95
10.284
8.998
1.651
9.830
0.205
0.99
21.050
15.207
34.135
19.006
4.176
0.999
51.308
40.610
114.440
40.431
118.298
0.9999
142.295
117.351
622.179
78.261
4100
Table 6.13: VaR for a mixture of GPDs, n = 150

6.2.
Future research
155
0.5 GPD(1,0.5)+0.5 GPD(3,0.2)
Mixture of GPD’s
Classical POT
γ
True value
V aRγ
MSE
V aRγ
MSE
0.9
6.927
6.689
0.0563
6.610
0.100
0.95
10.284
10.049
0.055
10.111
0.029
0.99
21.050
21.815
0.585
23.146
4.392
0.999
51.308
55.527
17.799
64.510
174.303
0.9999
142.295
130.846
131.071
167.985
659.994
Table 6.14: VaR for a mixture of GPDs, n = 500
0.5 GPD(1,0.5)+0.5 GPD(3,0.2)
Mixture of GPD’s
Classical POT
γ
True value
V aRγ
MSE
V aRγ
MSE
0.9
6.927
6.226
0.492
5.917
1.019
0.95
10.284
9.0813
1.446
9.176
1.228
0.99
21.050
17.514
12.506
21.751
0.491
0.999
51.308
51.604
0.087
64.117
164.084
0.9999
142.295
162.827
421.575
177.479
1237.956
Table 6.15: VaR for a mixture of GPDs, n = 1000
0.7 GPD(1,0.5)+0.3 GPD(3,0.2)
Mixture of GPD’s
Classical POT
γ
True value
V aRγ
MSE
V aRγ
MSE
0.9
5.981
6.344
0.132
6.249
0.072
0.95
9.176
9.910
0.539
10.294
1.250
0.99
20.108
23.639
12.471
28.481
70.116
0.999
54.934
70.086
229.608
107.564
2769.970
0.9999
166.313
198.979
1067.089
388.878
4.953e+04
Table 6.16: VaR for a mixture of GPDs, n = 150
0.7 GPD(1,0.5)+0.3 GPD(3,0.2)
Mixture of GPD’s
Classical POT
γ
True value
V aRγ
MSE
V aRγ
MSE
0.9
5.981
4.901
1.165
5.488
0.243
0.95
9.176
7.785
1.93
8.762
0.171
0.99
20.108
19.628
0.230
22.380
5.164
0.999
54.934
63.873
79.914
74.375
377.947
0.9999
166.313
196.823
930.841
234.413
4637.639
Table 6.17: VaR for a mixture of GPDs, n = 500

6.2.
Future research
156
0.7 GPD(1,0.5)+0.3 GPD(3,0.2)
Mixture of GPD’s
Classical POT
γ
True value
V aRγ
MSE
V aRγ
MSE
0.9
5.981
6.253
0.074
5.921
0.003
0.95
9.176
10.022
0.715
9.562
0.148
0.99
20.108
22.089
3.927
25.145
25.372
0.999
54.934
48.067
47.143
87.557
1064.293
0.9999
166.313
89.764
5859.797
290.186
1.534e+04
Table 6.18: VaR for a mixture of GPDs, n = 1000
6.2.1.4
Mixtures of distributions with disjoint supports.
To complete this study, we consider the case of mixtures of distributions with disjoint
supports. Castellacci (2012) provides some formulas for calculating high quantiles for this
type of mixtures:
Theorem 6.3. Consider a proper (wi > 0 for all i) mixture distribution F of the form
(6.1) with I = {1, . . . , n} and let Ai = supp(fi) be the support of fi, where fi denotes the
PDF of Fi. Assume sup Ai ≤inf Ai+1 for i = 1, ..., n. Then the quantile Q(p) of F at
level p equals
Q (p) = F −1 (p) = F −1
1
 p
w1

1B1 (p) + · · · + F −1
n
 
p −Pn−1
i=1 wi
wn
!
1Bn (p)
for any p ∈[0, 1] , where
Bi :=


i−1
X
j=0
wj,
i
X
j=0
wj


and
Bn :=


n−1
X
j=0
wj, 1


and we have deﬁned w0 = 0.
Details of the proof can be found in Castellacci (2012).
If X ∼F represents portfolio losses over a prescribed time horizon, and F is a mixture
as in Theorem 6.3, the Value-at-Risk at level c is
VaRc (X) =
n
X
i=1
VaRci (Xi) 1Ci (c) ,

6.3.
Concluding remarks
157
where Xi is a loss random variable with distribution Fi corresponding to the ith component
of the mixture, i ∈{1, . . . , n} and
ci =
c−1+Pi
j=1 wj
wi
,
Ci =

1 −Pi
j=1 wj, 1 −Pi−1
j=1 wj
i
.
This reformulation of VaR implies that the higher the conﬁdence level, the smaller the
ci of the component whose VaR is selected for the computation. The following example is
provided. If c ∈C1 = (1 −w1, 1],
VaRc (X) = VaRc1 (X1) = −F −1
1
1 −c
w1

.
Conversely, given a conﬁdence level c one can pick a weight w1 such that 1 −c < w1. In
this case, only the leftmost risk contributes to the VaR of the mixture.
6.3
Concluding remarks
To conclude, we suggest some other directions for future work:
 Inclusion of covariates in the analysis. Extremes are non-stationary in general and
they vary with respect to covariates. Several authors have shown the importance
of including covariates in the analysis.
For example, Davison and Smith (1990)
characterize extreme value model parameters in terms of one or more covariates.
Chavez-Demoulin and Davison (2005) and Coles (2001) describe a non-homogeneous
Poisson model in which ocurrence rates and extremal properties are modelled as
functions of covariates. Moreover, as we saw in Chapter 2, threshold selection is
an important issue to handle, and it is even more diﬃcult when covariate eﬀects
are present, since the threshold may itself be a function of covariates. Thus, it is
essential to accommodate covariate eﬀects in the model.
 Extension to the multivariate case. Modelling of multivariate extremes is increas-
ingly important in diﬀerent ﬁelds. Contrary to what happens in the univariate case,

6.3.
Concluding remarks
158
the whole family of MEVD cannot be described parametrically. Due to their ﬂexibil-
ity, nonparametric estimation has been proposed in the literature (see Beirlant et al.,
2004), however, these models are diﬃcult to implement due to the increasing num-
ber of parameters needed to characterize the joint dependence structure accurately.
Hence, this is still an active area of research that requires new approaches.
 The use of more sophisticated methods to combine expert opinion. In Chapter 4
we have used basic methodologies for aggregating expert opinion. Nonetheless, ag-
gregation methods range from the simple to the complex. Some authors favor the
theoretical elegance of the more sophisticated approaches, while others advocate for
the simple methodologies, on the basis of the greater probability of obtaining mean-
ingful results (Jenkinson, 2005). Therefore, we leave open the possibility of using
more sophisticated elicitation methods in the analysis of extremes.
 The elicitation of information in the new model framework. Models introduced in
Chapter 5 provide diﬀerent alternatives to handle the discontinuity problem in the
model of Behrens et al. (2004); however, we have not supplied information from
experts. Thus, it would be interesting to assess the eﬀect of incorporating expert
opinion into the analysis.

Appendix A
Markov Chain Monte Carlo methods
The recent explosion in Markov chain Monte Carlo (MCMC) techniques owes largely to
their application in Bayesian inference. In MCMC simulation, one constructs a Markov
chain long enough for the distribution of the elements to stabilize to a stationary distri-
bution, which is the distribution of interest. By repeatedly simulating steps of the chain,
the method simulates draws from the distribution of interest.
MCMC has been widely applied for exploring posterior distributions. That is, through
MCMC, one can simulate the entire joint posterior distribution of the unknown quantities
and obtain simulation-based estimates that can be used directly for parameter inference
and prediction.
There are many ways of constructing the appropriate Markov chain, but all of them,
including the Gibbs sampler are special cases of the general framework of Metropolis et
al. (1953) and Hastings (1970), or the Metropolis-Hastings algorithm.
A detailed introduction to the Metropolis-Hastings and the Gibbs sampler algorithms
can be found in Robert and Casella (1999) and Brooks et al. (2011), among others. In
Sections A.1–A.3, we summarize the algorithms presented in those books.
159

A.1.
Metropolis–Hastings sampling
160
A.1
Metropolis–Hastings sampling
Suppose we wish to draw θ = (θ1, ..., θd)′ from a density π (θ) (usually the posterior
density). Further, suppose that we have some arbitrary transition kernel p (θi+1, θi) (which
is easy to simulate from) for iterative simulation of successive values. Then consider the
following algorithm:
1. Initialize the iteration counter to k = 1, and initialize the chain to θ(0) at some value;
2. Generate a proposed value θ′ using the kernel p
 θ(k−1), θ′
;
3. Evaluate the acceptance probability A
 θ(k), θ′
of the proposed move, where
A
 θ(k), θ′
= min

1, π (θ′) L (θ′ | x) p (θ′, θ)
π (θ) L (θ | x) p (θ, θ′)

;
(A.1)
4. Put θ(k) = θ′ with probability A
 θ(k−1), θ′
, and put θ(k) = θ(k−1) otherwise;
5. Change the counter from k to k + 1 and return to step 2.
So at each stage, a new value is generated from the proposal distribution. This is either
accepted, in which case the chain moves, or rejected, in which case the chain stays where it
is. Whether or not the move is accepted or rejected depends on the acceptance probability
which itself depends on the relationship between the density of interest and the proposal
distribution.
A.2
The Gibbs sampler
Suppose again that π (θ) is the density of interest. If we can draw from various conditional
distributions of π (θ), then we can construct a Markov chain that eventually converges to
the joint distribution. This is, suppose that
π (θi | θ1, ..., θi−1, θi+1, ..., θd) = π (θi | θ−i) = πi (θi) ,
i = 1, ..., d
(A.2)

A.3.
Metropolis-within-Gibbs
161
are available for simulating from (θ−i denotes the parameter vector excluding θi). The
Gibbs sampler uses the following algorithm:
1. Initialize the iteration counter to k = 1. Initialize the state of the chain to θ(0) =

θ(0)
1 , ..., θ(0)
d
′
at some arbitrary values;
2. Obtain a new value θ(k) from θ(k−1) by successive generation of values
θ(k)
1
∼
π

θ1 | θ(k−1)
2
, ..., θ(k−1)
d

,
θ(k)
2
∼
π

θ2 | θ(k)
1 , θ(k−1)
3
, ..., θ(k−1)
d

,
.
.
.
.
.
.
θ(k)
d
∼
π

θd | θ(k)
1 , ..., θ(k)
d−1

.
3. Change counter k to k + 1, and return to step 2.
Each simulated value depends only on the previous simulated value, and not on any other
previous values or the iteration counter k. The Gibbs sampler can be used in isolation if we
can readily simulate from the full conditional distributions; however, this is not always the
case. Fortunately, the Gibbs sampler can be combined with Metropolis-Hastings schemes
when the full conditionals are diﬃcult to simulate from.
A.3
Metropolis-within-Gibbs
The Metropolis-within-Gibbs idea retains the idea of sequential sampling, but uses a
Metropolis step on some or all variables rather than attempting to sample from the exact
conditional distribution.
That is, we propose a move of the parameter in question from its current position
to a new position in the state space (keeping all the other variables ﬁxed); calculate the
acceptance probability using the conditional distribution of that variable; decide whether

A.4.
Convergence diagnostics
162
to accept or reject, and then move onto the next variable. We can mix “pure” Gibbs
steps where we sample from the desired conditional distribution of some variables, with
Metropolis steps on other variables.
A.4
Convergence diagnostics
Convergence refers to the idea that eventually the simulated Markov chain reaches the
stationary distribution. Although, it is never possible to say with certainty that a ﬁnite
sample from an MCMC algorithm is representative of an underlying stationary distribu-
tion, convergence diagnostics oﬀer a worthwhile check on the algorithm’s progress. In this
section we present some general methods for monitoring convergence. We refer the reader
to Brooks and Gelman (1997), Cowles and Carlin (1996), Gelman and Rubin (1992) and
Heidelberger and Welch (1983) for a detailed explanation of these methods.
A.4.1
Gelman and Rubin diagnostic
This statistic is based on the idea that after simulating multiple sequences for over-
dispersed starting points, the behaviour of all of the chains should be basically the same
(Gelman and Rubin, 1992). That is, the variance within the chains should be the same as
the variance across the chains.
The procedure to calculate the Gelman-Rubin statistic is as follows:
1. Estimate the model with a variety of diﬀerent initial values and iterate for an n-
iteration burn-in and an n-iteration monitored period.
2. Take the n-monitored draws of m parameters and calculate the statistic
√
R =
q
ˆV (θ)
W , where
 W =
1
m(n−1)
m
P
j=1
nP
i=1

θ(i)
j −θj
2
is the within-chain variance.

A.4.
Convergence diagnostics
163
 B =
n
m−1
m
P
j=1
 θj −θ
2 is the between-chain variance.
 ˆV (θ) =
 1 −1
n

W + 1
nB is the estimated variance.
If convergence has been achieved, W and ˆV (θ) should be almost equivalent, so R should
approximately equal to 1.
A.4.2
Heidelberg and Welch diagnostic
The Heidelberg and Welch statistic (Heidelberger and Welch, 1983) is based on the Cram´er-
von Mises statistic and is used to test the hypothesis that the Markov chain is from a
stationary distribution. The diagnostic consists of two parts.
 Part I
1. Generate a chain of N iterations and deﬁne an α ∈(0, 1) level.
2. Calculate the test statistic on the whole chain to accept or reject the null hypothesis
of stationarity.
3. If the null hypothesis is rejected, discard the ﬁrst 10% of the chain and calculate the
test statistic to accept or reject the null.
4. If the null hypothesis is rejected, discard the next 10% and calculate the test statistic.
5. Repeat until the null hypothesis is accepted or 50% of the chain is discarded. If the
test still rejects the null hypothesis, then the chain fails the test and needs to be run
longer.
 Part II
1. If the chain passes the ﬁrst part of the diagnostic, then it takes the part of the chain
not discarded from the ﬁrst part to test the second part.

A.5.
Reversible jump MCMC
164
2. The halfwidth test calculates half the width of the (1−α)% credible interval around
the mean.
3. If the ratio of the halfwidth and the mean is lower than some ϵ, then the chain passes
the test. Otherwise, the chain must be run out longer.
A.4.3
Eﬀective Sample Size
For a series of N observations from a dependent stochastic process, the eﬀective sample
size is given by (Chapter 1, Brooks et al. 2011)
Neﬀ=
N
1 + 2
∞
P
k=1
ρ (k)
,
(A.3)
where ρ (k) is the true lag-k autocorrelation for the Markov chain.
This quantity gives an estimate of the equivalent number of independent iterations
that the chain represents.
A.5
Reversible jump MCMC
The reversible jump MCMC (RJMCMC) algorithm was introduced by Green in 1995 (see
Green (1995) and Damien et al. (2013, Chapter 7)), and enables us to get a handle on both
model selection and parameter estimation in one single algorithm. This is, we are able to
traverse the posterior model space, in terms of the models and corresponding parameters,
in a single Markov chain.
This algorithm can be seen as an extension of the MH algorithm, with an additional
step which moves between the diﬀerent models.
A.5.1
Model formulation
The model and notation presented in this section are based on Damien et al.
(2013,
Chapter 7).

A.5.
Reversible jump MCMC
165
Denote the generic Markov transition kernel Q
 θ, dθ
′
= P
 θ ∈dθ
′ | x, θ

and the
modiﬁed transition kernel P
 θ, dθ
′ | x

which has π∗as its unique stationary distribution.
Let π and q denote the densities of π∗and Q with respect to Lebesgue measure.
Consider a countable collection of Bayesian models, {Mk, k = 1, 2, ...} , where model
Mk is parameterized by θk with parameter space Θk ⊂Rdk . For data x the full posterior
can be written as
π (Mk, θk | x) =
f (x | θk, Mk) p (θk, | Mk) p (Mk)
P
j
´
f (x | θj, Mj) p (θj, | Mj) dθj
	
p (Mj).
(A.4)
The objective is to construct an aperiodic and irreducible Markov chain on the union
parameter space Θ = S
k Θk.
We assume that at a speciﬁc iteration the chain is in model M with a d-dimensional
parameter value θ, and the proposal is to move to model M
′ with a d′-dimensional pa-
rameter value θ
′. Suppose that, when in model M, the move between the two models is
selected with probability r
 M, M
′
and consider the introduction of collections of latent
variables u and v of dimension du and dv, respectively, so that d + du = d
′ + dv. The
proposal density q is then considered for the extended parameter vectors (θ, u) and (θ
′, v)
such that q
 (θ, u) , (θ
′, v) | x

is reversible. This can be seen as a bijective diﬀerentiable
mapping
(θ
′, v) = g (θ, u) ⇐⇒(θ, u) = h(θ
′, v).
(A.5)
Let ψ and ψ
′ be the latent-augmented parameter vectors and denote the augmented pos-
terior density by
˜πm (ψ | x) = ˜πm (M, θ, u | x) = π (M, θ | x) pu (u) .
(A.6)
Let αm
 ψ, ψ
′ | x

denote the acceptance probability for move type m, so that for arbitrary
sets A and B,
ˆ
A
˜π∗
m (dψ | x)
ˆ
B
αm

ψ, ψ
′ | x

Qm

ψ, dψ
′ | x

=
ˆ
B
˜π∗
m (dψ′ | x)
ˆ
A
αm

ψ, ψ
′ | x

Qm

ψ, dψ
′ | x

(A.7)

A.5.
Reversible jump MCMC
166
which implies that
αm

ψ, ψ
′ | x

fm

ψ, ψ
′ | x

= αm

ψ
′, ψ | x

fm

ψ
′, ψ | x

,
(A.8)
where fm is deﬁned with respect to a common, symmetric measure on the product space.
For the forward move, we have
fm

ψ, ψ
′ | x

= ˜πm (ψ | x) qm

ψ, ψ
′ | x

= π (M, θ | x) pu (u)⃗rm,
(A.9)
where ⃗rm represents the probability of choosing to make a move m from M to M
′.
For the reverse move, we must set
fm

ψ
′, ψ | x

= ˜πm

ψ
′ | x

qm

ψ
′, ψ | x

= π

M
′, θ
′ | x

pv (v)

∂
 θ
′, v

∂(θ, u)

←−
rm, (A.10)
where

∂
 θ
′, v

∂(θ, u)
 =

∂g (t1, t2)
∂g (θ, u)

t1=θ,t2=u
(A.11)
is the Jacobian associated with the bijection g : (θ, u) 7→
 θ
′, v

and ←−
rm represents the
probability of choosing to make a move m from M
′ to M. So the augmented posterior
becomes
˜π

M
′, θ
′, v | x

= ˜π

M, h

θ
′, v

| x
 J

θ
′, v
 = ˜π (M, θ, u | x) |J (θ, u)|−1 .
(A.12)
The acceptance probability for move type m is
αm

(M, θ) ,

M
′, θ
′
| x

= min
(
1, π
 M
′, θ
′ | x

pv (v) ←−r m
π (M, θ | x) pu (u)⃗rm
) 
∂
 θ
′, v

∂(θ, u)
 .
(A.13)
This probability can be simpliﬁed depending on whether dim (θ) > dim (θ′) or dim (θ) <
dim (θ′).
In general, within each iteration of the Markov chain, the algorithm involves two steps:
1. Update the parameters, conditional on the model using the MH algorithm and
2. Update the model, conditional on the current parameter values by proposing to move
to a diﬀerent model with some given parameter values and accepting this proposed
move with the probability above.
Further details on this algorithm can be found in Green (1995) and Tierney (1998).

Appendix B
Jeﬀreys prior for the GPD
Let Y ∼GPD (σ, ξ). The Jeﬀreys prior for a GPD with parameter θ = (σ, ξ) is given by
J (θ) ∝
p
|I (θ)|,
(B.1)
where I (θ) = −E [∂2ln f (y | θ) /∂2θ].
The density of the GPD(σ, ξ) is
f (y | θ) = 1
σ

1 + y ξ
σ
−1
ξ −1
,
where the support is







y > 0
if
ξ ≥0,
0 ≤y ≤−σ/ξ
if
ξ < 0.
The log of f (y | θ) is thus given by
L (y | θ) = ln f (y | θ) = −ln σ −
1
ξ + 1

ln

1 + ξ
σy

.
(B.2)
The ﬁrst derivative with respect to ξ is
∂L (y | θ)
∂ξ
=
 1
ξ2

ln

1 + ξ
σy

+

−1
ξ −1
 y
σ
 
1 + ξ
σy
−1
.
(B.3)
We can write
y = −σ
ξ

1 −

1 + ξ
σy

.
(B.4)
167

Jeﬀreys prior for the GPD
168
Substituting (B.4) in (B.3) we get:
∂L (y | θ)
∂ξ
=
 1
ξ2

ln

1 + ξ
σy

+

−1
ξ −1
 
−1
σ
 σ
ξ
 
1 −

1 + ξ
σy
 
1 + ξ
σy
−1
=
 1
ξ2

ln

1 + ξ
σy

+

−1
ξ −1
 
−1
ξ + 1
ξ

1 + ξ
σy
 
1 + ξ
σy
−1
=
 1
ξ2

ln

1 + ξ
σy

+

−1
ξ −1
 1
ξ
 "
1 −

1 + ξ
σy
−1#
.
(B.5)
Now the ﬁrst derivative with respect to σ is, using (B.4),
∂L (y | θ)
∂σ
= −1
σ +

−1
ξ −1
 
−ξy
σ2
 
1 + ξ
σy
−1
= −1
σ +

−1
ξ −1
  ξ
σ2
 σ
ξ
 
1 −

1 + ξ
σy
 
1 + ξ
σy
−1
= −1
σ +
 1
σ
 
−1
ξ −1
 
1 −

1 + ξ
σy
 
1 + ξ
σy
−1
= −1
σ +

−1
σξ −1
σ
 "
1 + ξ
σy
−1
−1
#
= −1
σ −1
σξ

1 + ξ
σy
−1
−1
σ

1 + ξ
σy
−1
+ 1
σ + 1
σξ
= 1
σξ + 1
σ

−1
ξ −1
 
1 + ξ
σy
−1
.
(B.6)
Setting k = 1 + ξ
σy, the second derivative with respect to ξ is as follows
∂2L (y | θ)
∂ξ2
= ∂
∂ξ
1
ξ2ln k −1
ξ2 −1
ξ + 1
ξ2k−1 + 1
ξ k−1.
(B.7)
The derivative of k with respect to ξ is, using (B.4),
dk
dξ = y
σ = 1
σ

−σ
ξ (1 −k)

= −1
ξ (1 −k) .
(B.8)

Jeﬀreys prior for the GPD
169
Using B.8,
∂2L (y | θ)
∂ξ2
= −2
ξ3 ln k + 1
ξ2

−1
ξ (1 −k)

k
+ 2
ξ3 + 1
ξ2 −2
ξ3k−1 −1
ξ2k−2

−1
ξ (1 −k)

−1
ξ2k−1 −1
ξ k−2

−1
ξ (1 −k)

= −2
ξ3 ln k −1
ξ3k−1 + 1
ξ3 + 2
ξ3 + 1
ξ2 −2
ξ3k−1 + 1
ξ3k−2 −1
ξ3k−1 −1
ξ2k−1
+ 1
ξ2k−2 −1
ξ2k−1
= −2
ξ3 ln k + 1
ξ2 + 3
ξ3 −4
ξ3k−1 −2
ξ2k−1 + 1
ξ2k−2 + 1
ξ3k−2
= −2
ξ3 ln k + 3 + ξ
ξ3
−2 (2 + ξ)
ξ3
k−1 + (1 + ξ)
ξ3
k−2
= −2
ξ3 ln

1 + ξ
σy

+ 3 + ξ
ξ3
−2 (2 + ξ)
ξ3

1 + ξ
σy
−1
+ (1 + ξ)
ξ3

1 + ξ
σy
−2
.
(B.9)
The second derivative with respect to σ is
∂2L (y | θ)
∂σ
= ∂
∂σ
1
σξ +

−1
ξ −1
 1
σk−1.
(B.10)
The derivative of k with respect to σ is, using (B.4),
dk
dσ = −ξy
σ2 = −ξ
σ2

−σ
ξ (1 −k)

= 1
σ (1 −k) .
(B.11)
Using (B.11)
∂2L (y | θ)
∂σ
= −1
σ2ξ +

−1
ξ −1
 
−1
σ2k−1 −1
σk−2
 1
σ (1 −k)

= −1
σ2ξ +

−1
ξ −1
 
−1
σ2k−2

= −1
σ2ξ −1
σ2

−1
ξ −1
 
1 + ξ
σy
−2
.
(B.12)

Jeﬀreys prior for the GPD
170
Furthermore,
∂2L (y | θ)
∂σ∂ξ
= ∂
∂ξ
 1
σξ −1
σξk−1 −1
σk−1

= 1
σ

−1
ξ2

+ 1
σ
1
ξ2k−1 −1
σξ

−k−2

−1
ξ (1 −k)

−1
σ

−k−2

−1
ξ (1 −k)

= −1
σξ2 +
1
σξ2k−1 −
1
σξ2k−2 +
1
σξ2k−1 −1
σξk−2 + 1
σξk−1
= −1
σξ2 +
1
σξ2 (2 + ξ) k−1 −
1
σξ2 (1 + ξ) k−2
= −1
σξ2 +
1
σξ2 (2 + ξ)

1 + ξ
σy
−1
−
1
σξ2 (1 + ξ)

1 + ξ
σy
−2
.
(B.13)
Next, we calculate
I (θ) = −E
∂2ln f (y | θ)
∂2θ

.
For ξ < 0.5, we have the following results (Davison and Smith,1990)
E

1 + ξ
σY
r
=
1
1 −rξ
(B.14)
and
E

ln

1 + ξ
σY

= ξ.
(B.15)

Jeﬀreys prior for the GPD
171
Then
E
∂2L (Y | θ)
∂2ξ

= −2
ξ3E

ln
 1 + ξ
σY

+ 3 + ξ
ξ3
−2 (2 + ξ)
ξ3
E
"
1 + ξ
σY
−1#
+(1 + ξ)
ξ3
E
"
1 + ξ
σY
−2#
= −2
ξ3 (ξ) + 3 + ξ
ξ3
−2 (2 + ξ)
ξ3
1
1 + ξ + (1 + ξ)
ξ3
1
1 + 2ξ
= −2ξ
ξ3 + 1
ξ3
"
(3 + ξ) (1 + ξ) (1 + 2ξ) −2 (2 + ξ) (1 + 2ξ) + (1 + ξ)2
(1 + ξ) (1 + 2ξ)
#
= −2ξ
ξ3 + 1
ξ3
2ξ3 + 9ξ2 + 10ξ + 3 −4ξ2 −10ξ −4 + ξ2 + 2ξ + 1
(1 + ξ) (1 + 2ξ)

= −2ξ
ξ3 + 1
ξ3
 2ξ3 + 6ξ2 + 2ξ
(1 + ξ) (1 + 2ξ)

= 1
ξ3
−2ξ (1 + ξ) (1 + 2ξ) + (2ξ3 + 6ξ2 + 2ξ)
(1 + ξ) (1 + 2ξ)

= 1
ξ3
−4ξ3 −6ξ2 −2ξ + 2ξ3 + 6ξ2 + 2ξ
(1 + ξ) (1 + 2ξ)

= 1
ξ3

−2ξ3
(1 + ξ) (1 + 2ξ)

= −
2
(1 + ξ) (1 + 2ξ)
(B.16)
and
E
∂2L (Y | θ)
∂2σ

= −1
σ2ξ −1
σ2

−1
ξ −1

E
"
1 + ξ
σY
−2#
= −1
σ2ξ +
 1
σ2ξ + 1
σ2
 
1
1 + 2ξ

= −1
σ2ξ +
1
σ2ξ (1 + 2ξ) +
1
σ2 (1 + 2ξ)
= 1
σ2

−1
ξ +
1
ξ (1 + 2ξ) +
1
1 + 2ξ

= 1
σ2

−1
ξ +
1
ξ (1 + 2ξ) +
1
1 + 2ξ

= 1
σ2
−1 −2ξ + 1 + ξ
ξ (1 + 2ξ)

= 1
σ2

−ξ
ξ (1 + 2ξ)

= −
1
σ2 (1 + 2ξ).
(B.17)

Jeﬀreys prior for the GPD
172
E
∂2L (Y | θ)
∂ξ∂σ

= −1
σξ2 +
1
σξ2 (2 + ξ) E
"
1 + ξ
σy
−1#
−
1
σξ2 (1 + ξ) E
"
1 + ξ
σy
−2#
= −1
σξ2 +
1
σξ2 (2 + ξ)

1
1 + ξ

−
1
σξ2 (1 + ξ)

1
1 + 2ξ

= −1
σξ2

1 + (1 + ξ)

1
1 + 2ξ

−(2 + ξ)

1
1 + ξ

= −1
σξ2
"
(1 + 2ξ) (1 + ξ) + (1 + ξ)2 −(2 + ξ) (1 + 2ξ)
(1 + 2ξ) (1 + ξ)
#
= −1
σξ2
2ξ2 + 3ξ + 1 + ξ2 + 2ξ + 1 −2ξ2 −5ξ −2
(1 + 2ξ) (1 + ξ)

= −1
σξ2

ξ2
(1 + 2ξ) (1 + ξ)

= −
1
σ (1 + 2ξ) (1 + ξ).
(B.18)
From these equations,
I (θ) = I (σ, ξ) =


2
(1 + ξ) (1 + 2ξ)
1
σ (1 + 2ξ) (1 + ξ)
1
σ (1 + 2ξ) (1 + ξ)
1
σ2 (1 + 2ξ)


(B.19)
and
|I (σ, ξ)|
=
2
(1 + ξ) (1 + 2ξ)
1
σ2 (1 + 2ξ) −

1
σ (1 + 2ξ) (1 + ξ)
2
=
2
σ2 (1 + ξ) (1 + 2ξ)2 −
1
σ2 (1 + ξ)2 (1 + 2ξ)2
=
1
σ2 (1 + ξ)2 (1 + 2ξ)2 [2 (1 + ξ) −1]
=
1
σ2 (1 + ξ)2 (1 + 2ξ)
.
(B.20)
Hence, the Jeﬀreys prior is
J (θ) ∝
p
|I (θ)| =
s
1
σ2 (1 + ξ)2 (1 + 2ξ)
=
1
σ (1 + ξ) √1 + 2ξ
= σ−1 (1 + ξ)−1 (1 + 2ξ)−1/2 .
(B.21)

Appendix C
Algorithm for the Bayesian model1
Simulations are done via Metropolis-Hastings steps within blockwise MCMC. Suppose that
at iteration j −1, the chain is positioned at θ(j−1) = (α(j−1), β(j−1), u(j−1), σ(j−1), ξ(j−1)).
Then, at iteration j, the algorithm cycles through the following steps:
 ξ(j)
ξ∗is sampled from the following candidate distribution
ξ(j) ∼N
 ξ(j−1), Vξ(j−1)

I (Ξ) ,
where Vξ(j−1) is an approximation based on the curvature at the conditional posterior mode
and Ξ =

−σ(j−1)/
 M −u(j−1)
, ∞

is given by the support of the GPD function, and
M = max(x1, ..., xn), the maximum value of the data.
 σ(j)
The candidate distribution of σ depends on the current value of ξ. If ξ is positive, no
restrictions are imposed on σ. Otherwise, σ must be drawn in an appropriate region given
by the support of the GPD. That is,
If ξ(j) ≥0 : σ(j) ∼Ga (aσ, bσ), where aσ/bσ = σ(j−1) and aσ/b2
σ = Vσ(j−1). So, σ(j) is
centered around σ(j−1) with some variance Vσ(j−1) ;
1This algorithm has been taken from Behrens et al. (2004).
173

Algorithm for the Bayesian model
174
If ξ(j) < 0 : σ(j) ∼N
 σ(j−1), Vσ(j−1)

I (Σ), a truncated normal distribution, where Σ
is given by the support of the GPD with lower bound at Σ =

−ξ(j−1)  M −u(j−1), ∞

,
Vσ(j−1) is an approximation for the concavity in the conditional posterior mode.
 u(j)
As for σ, the candidate distribution used for u depends on the current value of ξ as well
as on σ. When we consider a continuous prior, the candidate distributions are
If ξ(j) ≥0 : u(j) ∼N
 u(j−1), Vu(j−1)

I (A), a normal distribution truncated on A =
(m, M), with M being the largest, as before, and m the smallest observations, respectively.
If ξ(j) < 0 : u(j) ∼N
 u(j−1), Vu(j−1)

I (A), a normal distribution truncated on A =
(au, M), with au = M + σ(j−1)/ξ(j−1). Again, Vu(j−1) is a value for the variance which is
tuned to allow appropriate chain movements.
If u has a discrete prior, we have to follow the same model restrictions as before.
Therefore, the candidate distribution is:
If ξ(j) ≥0 : u(j) ∼Ud (q1, q2), a discrete uniform distribution on data quantiles from q1
to q2, where q2 can be any high quantile, like the largest observation M, and q1 can be any
quantile below q2. It is important to keep in mind that q1 must be reasonable to prevent
the model bias and to respect the asymptotic properties of the model.
If ξ(j) < 0 : u(j) ∼Ud (q1, q2), a discrete uniform distribution with q2 as before, but q1
have to respect the model restrictions as shown above: q1 ≥M + σ(j−1)/ξ(j−1)
 α(j), β(j)
α(j) ∼log N
 α(j−1), Vα(j−1)

, with Vα(j−1) given by an approximation for the curvature
at the conditional posterior mode and β(j) ∼GI (aβ, bβ), an inverse Gamma distribution
centered at β(j−1) and with variance Vβ(j−1) given by the approximation for the curvature
at the conditional posterior mode.

Appendix D
Non-informative prior plots
Figure D.1: Fraud data: σ = 1, 10, 100, respectively; u ∼TN(min (data)); ξ ∼Jeﬀreys prior
Figure D.2: Fraud data: σ ∼LN(0, 1.25); u = Q0.5 (19.89) , Q0.7 (43.77) , Q0.9 (124) , respec-
tively; ξ ∼Jeﬀreys prior
175

Non-informative prior plots
176
Figure D.3: Fraud data: σ ∼LN(0, 1.25) ; u ∼TN(min (data)); ξ = −0.3, 0.05, 0.45, respec-
tively
Figure D.4:
Exponential(0.1):
σ
=
1, 10, 100, respectively; u
∼
TN(min (data)),ξ
∼
Jeﬀreys prior
Figure D.5: Exponential(0.1): σ ∼LN(0, 1.25); u = Q0.5 (6.89) , Q0.7 (11.35) , Q0.9 (23.03) ,
respectively; ξ ∼Jeﬀreys prior

Non-informative prior plots
177
Figure D.6: Exponential(0.1): σ ∼LN(0, 1.25) ; u ∼TN(min (data)); ξ = −0.3, 0.05, 0.45, re-
spectively
Figure D.7:
Log-Normal(0,1):
σ = 1, 10, 100, respectively; u ∼TN(min (data)), ξ ∼
Jeﬀreys prior
Figure D.8: Log-Normal(0,1): σ ∼LN(0, 1.25); u = Q0.5 (1.009) , Q0.7 (1.69) , Q0.9 (3.49) , re-
spectively; ξ ∼Jeﬀreys prior

Non-informative prior plots
178
Figure D.9: Log-Normal(0,1): σ ∼LN(0, 1.25) ; u ∼TN(min (data)); ξ = −0.3, 0.05, 0.45, re-
spectively
Figure D.10:
Gamma(2,0.5):
σ
= 1, 10, 100, respectively; u ∼TN(min (data)), ξ
∼
Jeﬀreys prior
Figure D.11: Gamma(2,0.5): σ ∼LN(0, 1.25); u = Q0.5 (3.4) , Q0.7 (4.85) , Q0.9 (7.91) , respec-
tively; ξ ∼Jeﬀreys prior

Non-informative prior plots
179
Figure D.12: Gamma(2,0.5): σ ∼LN(0, 1.25) ; u ∼TN(min (data)); ξ = −0.3, 0.05, 0.45, re-
spectively

Bibliography
[1]
Agostini, A., Talamo, P. and Vecchione, V. (2010): “Combining operational loss data
with expert opinions through advanced credibility theory”. The Journal of Operational
Risk, 5(1): 2–28.
[2]
Antoniak, C. (1974): “Mixtures of Dirichlet processes with applications to Bayesian
nonparametric problems”. The Annals of Statistics 2: 1152–1174.
[3]
Arora, S., Hazan, E. and Kale, S. (2012): “The multiplicative weights update method:
a meta-algorithm and applications”. Theory of Computing, 8(1): 121–164.
[4]
Basel Committee on Banking Supervision (2004): “International convergence of cap-
ital measurement and capital standards”. Available online: http://www.bis.org/
publ/bcbs107.pdf.
[5]
Bee, M., Espa, G. and Benedetti, R. (2013): “On maximum likelihood estimation of
a Pareto mixture” . Computational Statistics and Data Analysis, 28(1): 161–178.
[6]
Behrens, C. N., Lopes, H. F., and Gamerman, D. (2004): “Bayesian analysis of
extreme events with threshold estimation”. Statistical Modelling, 4: 227–244.
[7]
Beirlant, J., Goegebeur, Y., Segers, J. and Teugels, J. (2004): “Statistics of extremes:
Theory and applications”. Wiley, London.
[8]
Beirlant, J., Vynckier, P. and Teugels, J. (1996): “Excess functions and estimation of
the extreme-value index”. Bernoulli, 2(4): 293–318.
180

Bibliography
181
[9]
Bermudez, P., Turkman, M.A. and Turkman, K.F. (2001): “A predictive approach to
tail probability estimation”. Extremes, 4: 295–314.
[10] Bernardo, J.M. (1997): “Noninformative priors do not exist: A discussion”. Journal
of Statistical Planning and Inference, 65: 159–189.
[11] Bortot, P. and Gaetan, C. (2013): “A latent process model for temporal extremes”.
Scandinavian Journal of Statistics, forthcoming.
[12] Brooks, S. and Gelman, A. (1997): “General methods for monitoring convergence of
iterative simulations”. Journal of Computational and Graphical Statistics, 7: 434–455.
[13] Brooks, S., Gelman, A., Jones, G. L., and Meng, X.L. (2011): “Handbook of Markov
Chain Monte Carlo”. Chapman and Hall, London.
[14] Cabras, S. and Castellanos, M.E. (2010): “An objective Bayesian approach for thresh-
old estimation in the peaks over the threshold model”. Improving Risk Management.
Technical Report TR2010.10.
[15] Cabras, S., Castellanos, M. E., and Gamerman, D. (2011): “A default Bayesian ap-
proach for regression on extremes”. Statistical Modelling, 11 (6): 557–580.
[16] Carreau, J. and Bengio, Y. (2009): “A hybrid Pareto model for asymmetric fat-tailed
data: The univariate case”. Extremes, 12: 53–76.
[17] Castellacci, G. (2012): “A formula for the quantiles of mixtures of distributions with
disjoint supports”. Available online: http://ssrn.com/abstract=2055022.
[18] Castellanos, M.E. and Cabras, S. (2007): “A default Bayesian procedure for the gen-
eralized Pareto distribution”. Journal of Statistical Planning and Inference, 137(2):
473–483.
[19] Chavez-Demoulin, V. and Davison, A.C. (2005): “Generalized additive modelling of
sample extremes”. Journal of the Royal Statistical Society C, 54: 207–222.

Bibliography
182
[20] Chavez-Demoulin, V., Embrechts, P. and Neˇslehov´a, J. (2006): “Quantitative models
for operational risk: Extremes, dependence and aggregation”. Journal of Banking and
Finance, 30(9): 2635–2658.
[21] Clemen, R.T. and Winkler, R.L. (1997): “Combining probability distributions from
experts in risk analysis”. Risk Analysis, 19: 187–203.
[22] Coles, S.G. (2001): “An introduction to statistical modelling of extreme values”.
Springer, London.
[23] Coles, S.G. and Powell, E.A. (1996): “Bayesian methods in extreme value modelling:
A review and new developments”. International Statistical Review, 64: 119–136.
[24] Coles, S.G. and Tawn, J.A. (1994): “Statistical methods for multivariate extremes:
An application to structural design”. Applied Statistics, 43: 1–48.
[25] Coles, S.G. and Tawn, J.A. (1996): “A Bayesian analysis of extreme rainfall data”.
Applied Statistics, 45: 463–478.
[26] Cowles, M.K. and Carlin, B.P. (1994): “Markov chain Monte Carlo convergence di-
agnostics: A comparative review”. Technical report 94-008, Division of Biostatistics,
School of Public Health, University of Minessota.
[27] Cruz, M.G. (2002): “Modelling, measuring and hedging operational risk”. Wiley,
Chichester.
[28] Damien, P., Dellaportas, P., Polson, N. and Stephens, D.A. (2013): “Bayesian theory
and applications”. Oxford University Press.
[29] Danielsson,
J.
and
de
Vries,
C.
(2002):
“Where
do
extremes
matter?”.
Working
Paper.
Available
online:
http://www.riskresearch.org/files/
CdV-JD-00-6-10-960592460-4.pdf.

Bibliography
183
[30] Davison, A.C. and Smith, R.L. (1990): “Models for exceedances over high thresholds”.
Journal of the Royal Statistical Society B, 52: 393–442.
[31] Degen, M., Embrechts, P. and Lambrigger, D.D. (2007): “The quantitative modelling
of operational risk: Between g-and-h and EVT”. Astin Bulletin, 37(2): 265–291.
[32] Diaconis, P. and Ylvisaker, D. (1985): “Quantifying prior opinion (with discussion)”.
Bayesian Statistics 2 (J.-M. Bernardo, M. H. DeGroot, D. V. Lindley and A. F. M.
Smith, eds.): 133–156. North-Holland, Amsterdam.
[33] Do Nascimento, F.F., Gamerman, D. and Lopes, H.F. (2012): “A semiparametric
Bayesian approach to extreme value estimation”. Statistics and Computing, 22(2):
661–675.
[34] Doucet, A., De Freitas, J.F.G. and Gordon, N.J. (2001): “Sequential Monte Carlo
Methods in Practice”. Springer, New York.
[35] DuMouchel, W. H. (1983): “Estimating the stable index in order to measure tail
thickness: a critique”. The Annals of Statistics, 11(4): 1019–1031.
[36] Dupuis, D.J. (2000): “Exceedances over high thresholds: A guide to threshold selec-
tion”. Extremes, 1: 251–261.
[37] Dupuis, D. J. and Field, C. A. (1998): “Robust estimation of extremes”. The Canadian
Journal of Statistics, 26(2): 199–215.
[38] Dutta, K. and Perry, J. (2006): “A tale of tails: an empirical analysis of loss distribu-
tion models for estimating operational risk capital”. Federal Reserve Bank of Boston.
Working paper No. 06–13.
[39] Embrechts, P. (2000): “Extreme Value Theory: Potential and limitations as an inte-
grated risk management tool”. Derivatives Use, Trading and Regulation, 6: 449–456.

Bibliography
184
[40] Embrechts, P. , Kl¨uppelberg, C. and Mikosch, T. (1997): “Modelling extremal events”.
Springer, Berlin.
[41] Embrechts, P. and Puccetti, G. (2006): “Aggregating risk capital, with an application
to operational risk”. The Geneva Risk and Insurance Review, 31(2): 71–79.
[42] Embrechts, P. and Puccetti, G.(2008): “Aggregation operational risk across matrix
structured loss data”. The Journal of Operational Risk, 3(2): 29–44.
[43] Escobar, M.D. and West, M. (1995): “Bayesian density-estimation and inference using
mixtures”. Journal of the American Statistical Association, 90: 577–588.
[44] Falk, M., H¨usler, J. and Reiss, D. (2010): “Laws of small numbers: Extremes and rare
events”. Springer, Basel.
[45] Ferson, S., Kreinovich, V., Ginzburg, L., Myers, D.S. and Sentz, K.(2003): “Con-
structing probability boxes and Dempster-Shafer structures”. Sandia National Labo-
ratories, Albuquerque, New Mexico 87185 and Livermore, California 94550. SAND
report: SAND2002- 4015.
[46] Garthwaite, P.H., Kadane, J.B. and O’Hagan, A. (2005): “Statistical methods for
eliciting probability distributions”. Journal of the American Statistical Association,
100(470): 680–701.
[47] Gelfand, A. E. and Kottas, A. (2002): “A computational approach for full nonpara-
metric Bayesian inference under Dirichlet process mixture models”. Journal of Com-
putational and Graphical Statistics, 11: 289–305.
[48] Gelman, A. and Rubin, D.B. (1992): “Inference from iterative simulation using mul-
tiple sequences”. Statistical Science, 7: 457–511.
[49] Genest, C. (1984a): “A characterization theorem for externally Bayesian groups”. The
Annals of Statistics, 12: 1100–1105.

Bibliography
185
[50] Genest, C. (1984b): “Pooling operators with the marginalization property”. The Cana-
dian Journal of Statistics, 12: 153–163.
[51] Genest, C. and McConway, K.J. (1990): “Allocating the weights in the linear opinion
pool”. Journal of Forecasting, 9: 53–73.
[52] Genest, C. and Schervish, M.J. (1985): “Modeling expert judgment for Bayesian up-
dating”. The Annals of Statistics, 13: 1198–1212.
[53] Genest, C. and Zidek, J.V. (1986): “Combining probability distributions: A critique
and an annotated bibliography (with discussion)”. Statistical Science, 1: 114–148.
[54] Green, P.J. (1995): “Reversible jump Markov chain Monte Carlo computation and
Bayesian model determination”. Biometrika, 82: 711–732.
[55] Hastings, W.K. (1970): “Monte Carlo sampling methods using Markov chains, and
their applications”. Biometrika, 57: 97–109.
[56] Heidelberger, P. and Welch, P.D.(1983): “Simulation run length control in the pres-
ence of an initial transient”. Operations Research, 31: 1109–1144.
[57] Irony, T. Z. and Singpurwalla, N. D. (1997): “Noninformative priors do not exist”,
Journal of Statistical Planning and Inference, 65: 159–89.
[58] Jenkinson, D. (2005): “The elicitation of probabilities: A review of the statistical
literature”. Beep working paper, Department of Probability and Statistics, University
of Sheﬃeld.
[59] Kang, S., Serfozo, R.F. (1999): “Extreme values of phase-type and mixed random
variables with parallel processing examples”. Journal of Applied Probability, 36: 194–
210.
[60] Kiefer, N. M. (2010): “Default estimation and expert information”. Journal of Business
and Economic Statistics, 28(2): 320–328.

Bibliography
186
[61] Lambrigger, D.D., Shevchenko and P.V., W¨uthrich, M.V. (2007): “The quantiﬁcation
of operational risk using internal data, relevant external data and expert opinions”.
The Journal of Operational Risk, 2(3): 3–27.
[62] MacDonald, A., Scarrott, C.J., Lee, D., Darlow, B., Reale, M. and Russell, G. (2011):
“A ﬂexible extreme value mixture model”. Computational Statistics and Data Analysis,
55: 2137–2157.
[63] Mahmoud, H. M. (2008): “P´olya Urn Models”. Chapman-Hall, Florida, USA.
[64] McConway, K. J. (1981): “Marginalization and linear opinion pools”. Journal of the
American Statistical Association, 76: 410–414.
[65] McNeil, A., Frey, R. and Embrechts, P. (2005): “Quantitative risk management”.
Princeton University Press, Princeton.
[66] Medova, E.A. (2007): “Bayesian analysis and Markov chain Monte Carlo simulation”.
Judge Business School Working Papers, No.10/2007. University of Cambridge.
[67] Medova E.A. and Kyriacou M.N. (2002): “Extremes in operational risk management”.
In Dempster, M.A.H. (ed.): Risk Management: Value-at-Risk and Beyond. Cam-
bridge University Press, 247–274.
[68] Mendes, B. and Lopes, H.F. (2004): “Data driven estimates for mixtures”. Computa-
tional Statistics and Data Analysis, 47: 583–598.
[69] Metropolis, N., Rosenbluth, A.W., Rosenbluth, M.N., Teller, A.H. and Teller, E.
(1953): “Equations of state calculations by fast computing machines”. Journal of
Chemical Physics, 21(6): 1087–1092.
[70] Neˇslehov´a, J., Embrechts, P. and Chavez-Demoulin, V. (2006): “Inﬁnite mean models
and the LDA for operational risk”. Journal of Operational Risk, 1(1): 3–25.

Bibliography
187
[71] Pickands III, J. (1994): “Bayes quantile estimation and threshold selection for the
generalized Pareto family”. In Galambos, J., Leigh, S., and Simiu, E. (eds.), Extreme
Value Theory and Applications, 123–138, Kluwer, Amsterdam.
[72] Reiss, R.D. and Thomas, M. (1997): “Statistical analysis of extreme values with ap-
plications to insurance, ﬁnance, hydrology and other ﬁelds”. Birkhauser Verlag, Basel.
[73] Resnick, S. I. (1987):
“Extreme values, regular variation and point processes”.
Springer, New York.
[74] Ribatet, M. A. (2006):
“A user’s guide to the POT package”. http://cran.
r-project.org/.
[75] Ripley B. D. (1987). “Stochastic simulation”. Wiley, New York.
[76] Robert, C. P., and Casella, G. (1999): “Monte Carlo statistical methods”. New York:
Springer-Verlag.
[77] Scarrott, C. and MacDonald, A. (2012): “A review of extreme value threshold estima-
tion and uncertainty quantiﬁcation”. REVSTAT: Statistical Journal, 10(1): 33–60.
[78] Shevchenko, P.V. (2010): “Implementing loss distribution approach for operational
risk”. Applied Stochastic Models in Business and Industry, 26(3): 277–307.
[79] Shevchenko P.V. (2011):
“Modelling operational risk using Bayesian inference”.
Springer, Berlin.
[80] Shevchenko, P.V. and Temnov, G. (2009) : “Modelling operational risk data reported
above a time-varying threshold”. The Journal of Operational Risk, 4(2): 19–42.
[81] Shevchenko, P.V. and W¨uthrich, M.V. (2006): “The structural modelling of opera-
tional risk via Bayesian inference: combining loss data with expert opinions”. The
Journal of Operational Risk, 1(3): 3–26.

Bibliography
188
[82] Shi J., Samad-Khan, A. and Medapa P. (2000): “Is the size of an operational loss
related to ﬁrm size?”. Operational Risk Magazine, 1(2): 22–25.
[83] Smith, R.L. (1987): “Estimating tails of probability distributions”. The Annals of.
Statistics, 15: 1174–1207.
[84] Smith, R.L. (1985): “Maximum likelihood estimation in a class of non-regular cases”.
Biometrika, 72: 67–90.
[85] Steinhoﬀ, C., Baule, R.( 2006): “How to validate op risk distributions”. OpRisk and
Compliance, 1(8): 36–39.
[86] Tancredi, A., Anderson, C. W. and O’Hagan, A. (2006): “Accounting for threshold
uncertainty in extreme value estimation”. Extremes 9: 87–106.
[87] Teh, Y.W. (2010): “Dirichlet Processes”. Encyclopedia of Machine Learning. Springer,
New York.
[88] Tierney, L. (1998): “A note on Metropolis-Hastings kernels for general state spaces”.
The Annals of Applied Probability, 8(1): 1–9.
[89] Tversky, A. (1974): “Assessing uncertainty”. Journal of the Royal Statistical Society
B, 36: 148–159.
[90] Venturini, S., Dominici, F. and Parmigiani, G. (2008): “Gamma shape mixtures for
heavy-tailed distributions”. The Annals of Applied Statistics, 2(2): 756–776.
[91] Wang, Z., Rodriguez, A. and Kottas, A. (2011): “A nonparametric mixture modelling
framework for extreme value analysis”. SIGFIRM Working Paper No. 13.
[92] Zhao, X., Scarrott, C.J., Oxley, L. and Reale, M. (2009): “Bayesian extreme value
mixture modelling for estimating VaR”. Working Papers in Economics 09/15, Univer-
sity of Canterbury, Department of Economics and Finance.

