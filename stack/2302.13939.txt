SpikeGPT: Generative Pre-trained Language Model
with Spiking Neural Networks
Rui-Jie Zhu
Department of Electrical and Computer Engineering
University of California, Santa Cruz
ridger@live.cn
Qihang Zhao
Kuaishou Technology Co. Ltd
zhaoqihang@kuaishou.com
Jason K. Eshraghianâˆ—
Department of Electrical and Computer Engineering
University of California, Santa Cruz
jeshragh@ucsc.edu
Abstract
As the size of large language models continue to scale, so does the computational
resources required to run it. Spiking neural networks (SNNs) have emerged as an
energy-efï¬cient approach to deep learning that leverage sparse and event-driven
activations to reduce the computational overhead associated with model inference.
While they have become competitive with non-spiking models on many computer
vision tasks, SNNs have also proven to be more challenging to train. As a result,
their performance lags behind modern deep learning, and we are yet to see the
effectiveness of SNNs in language generation. In this paper, inspired by the RWKV
language model, we successfully implement â€˜SpikeGPTâ€™, a generative language
model with pure binary, event-driven spiking activation units. We train the proposed
model on three model variants: 45M, 125M and 260M parameters. To the best
of our knowledge, this is 4Ã— larger than any functional backprop-trained SNN to
date. We achieve this by modifying the transformer block to replace multi-head self
attention to reduce quadratic computational complexity to linear with increasing
sequence length. Input tokens are instead streamed in sequentially to our atten-
tion mechanism (as with typical SNNs). Our preliminary experiments show that
SpikeGPT remains competitive with non-spiking models on tested benchmarks,
while maintaining 5Ã— less energy consumption when processed on neuromorphic
hardware that can leverage sparse, event-driven activations. Our code implementa-
tion is available at https://github.com/ridgerchu/SpikeGPT.
1
Introduction
Artiï¬cial Neural Networks (ANNs) have recently achieved widespread, public-facing impact in
Natural Language Processing (NLP), but has come with a signiï¬cant computational and energy
consumption burden across training and deployment. As examples, training GPT-3 was projected
to use 190,000 kWh of energy [3; 9; 1]. Deploying ChatGPT into every modern word processor
will witness millions of users in need of on-demand inference of large language models [34]. SNNs,
inspired by neuroscientiï¬c models of neuronal ï¬ring, offer a more energy-efï¬cient alternative by
using discrete spikes to compute and transmit information [25]. Spike-based computing combined
with neuromorphic hardware holds great potential for low-energy AI [8; 30; 40], and its effectiveness
in integration with deep learning has been demonstrated through numerous studies [38; 37; 15; 13].
âˆ—Corresponding author
Preprint. Under review.
arXiv:2302.13939v2  [cs.CL]  28 Feb 2023

At this stage, the performance of SNNs in NLP and generation tasks remains relatively under-
investigated. While SNNs have shown competitiveness in computer vision tasks such as classiï¬cation
and object detection [2; 21; 5], they have yet to attain similar success in generation tasks. The
parallelization of input tokens, a widely-used and highly effective method in the transformer block,
cannot be readily integrated with recurrent SNNs [42]. Although previous research has indicated that
the conversion of ANNs to SNNs can lead to competitive performance in NLP tasks, direct training of
SNNs results in a performance loss of approximately 20% compared to the conversion approach[24].
The sequential structure of linguistic data presents a unique advantage for the utilization of SNNs,
notwithstanding the difï¬culties faced by recurrent networks in NLP. The beneï¬ts of SNNs are that
they provide a more energy-efï¬cient alternative to conventional models because of their sparsely
active neurons, event-driven embedding of data, and binarized spiking activations.
The drawbacks of SNNs in an NLP context include the vanishing gradient problem where long-range
dependencies can no longer be extracted, the total absence of learning in excessively sparsiï¬ed
models [11], and the extreme constraint on layer-to-layer bandwidth, where activations are bina-
rized spikes [12]. These issues means that training large-scale SNNs via error backpropagation
is extremely challenging, leading to an absence of performant SNNs in language generation. Our
proposed SpikeGPT language model provides solutions to these challenges, thus combining the high
performance of large-scale language models with the computational efï¬ciency of SNNs.
1.1
Contributions
To the best of our knowledge, SpikeGPT is the ï¬rst generative SNN language model and the largest
SNN trained to date in terms of parameter count, with the largest version at 260M parameters (4Ã—
more than the previous largest SNN) [45]. Our results demonstrate that a small-scale variant with
45M parameters performs competitively against similar transformer models, with approximately 22Ã—
less synaptic operations that rely on expensive memory accesses.
The implementation of SpikeGPT is based on integrating recurrence into the Transformer block such
that it is compatible with SNNs and eliminates quadratic computational complexity, allowing for the
representation of words as event-driven spikes. Combining recurrent dynamics with linear attention
enables our network to stream incoming data word-by-word, and commence computation before
a sentence has been completed, while still retaining long-range dependencies present in complex
syntactic structures.
Our experiments show that SpikeGPT achieves competitive performance on all tested datasets while
consuming signiï¬cantly less energy compared to traditional artiï¬cial neural network models. Our
contributions in the ï¬eld of NLP and language generation can be succinctly described as follows:
1. We provide the ï¬rst demonstration of language-generation using direct-SNN training;
2. We achieve performance comparable to that of ANNs, while preserving the energy efï¬ciency
of spike-based computations;
3. We have successfully combined the powerful Transformer architecture with SNNs, with-
out the need for additional simulation time-steps, by utilizing linearization and recurrent
Transformer blocks.
This work can pave the way for effectively training large-scale SNNs.
2
Related Works
Although language generation has not previously been achieved with SNNs, this section provides an
overview of how SNNs have been used in basic NLP tasks, and the ways in which transformers have
been adopted for SNNs.
2.1
Spiking Neural Networks for Natural Language Processing
Ref. [43] proposes a bi-directional SNN for sentiment classiï¬cation and machine translation tasks.
Their approach uses spiking encoders, which replace costly multiplication operations with much
cheaper additive operations to signiï¬cantly reduce computational energy consumption. Similarly,
2

Input
Binary 
Embedding
Token-shift
Spiking RWKV
Add & Norm
Spiking RFFN
Add & Norm
Linear
N Ã—
Token-shift
Spiking RWKV
Spiking RFFN
Ã—ğ‘€!
Ã—ğ‘€"
Ã—ğ‘€#
ğ‘¹
K
ğ‘½
ğ‘¿
ğ‘¾
ğœ(ğ‘¹)
âŠ™
ğ¸Ã—ğ‘‡
âŠ›
ğ‘½
âŠ™
(
)
âŠ›
ğ’†ğ‘¾
ğ’†ğ‘¾
ğ’†ğ‘²
ğ’†ğ‘²
ğœ: Sigmoid Function
âŠ›: 1D Convolution
âŠ™: Hadamard Product
=
ğ’€
ğ’€
Spiking
Neuron
Feed Forward in 
Temporal dimension
Spike Output
Ã—ğ‘€!
ğ‘¿
ğ¸Ã—ğ‘‡
ğ‘®â€²
ğ‘®
ğœº(ğ‘®) ğŸ
Ã—ğ‘€"
S
4ğ¸Ã—ğ‘‡
4ğ¸Ã—ğ‘‡
ğ¸Ã—ğ‘‡
Ã—ğ‘€#
ğ¸Ã—ğ‘‡
ğ‘·
ğ’
Spiking
Neuron
Feed Forward in 
Temporal dimension
ğœ(ğ‘·)
âŠ™
S
=
Z
ğœ: Sigmoid Function
ğœ€: ReLU Function
âŠ™: Hadamard Product
Spike Output
Figure 1: Model Architecture.
Ref. [24] presents a two-step method to train SNNs for text classiï¬cation, with a simple and effective
way to encode pre-trained word embeddings as spike trains. Their results indicate that the converted
SNNs achieve comparable results to their ANN counterparts and are more robust against adversarial
attacks. Furthermore, Ref. [10] demonstrate the train-and-constrain methodology that enables the
mapping of machine-learned recurrent neural networks (RNNs) on a substrate of spiking neurons.
The authors achieve 74% accuracy on a question classiï¬cation task using less than 0.025% of the
cores on one TrueNorth chip [30], showcasing the potential for SNNs in classiï¬cation tasks in NLP.
2.2
Transformer in Spiking Neural Networks
The Transformer model, ï¬rst introduced in [42], has shown signiï¬cant success in various NLP tasks.
However, the application of the Transformer model to spiking neural networks (SNNs) has been
relatively limited.
The ï¬rst Spiking Transformer model was proposed by [45], which proposes spiking self-attention to
model visual features using sparse Query, Key and Value matrices. Ref. [23] proposes another variant
on Transformer-based SNNs, adopting spatio-temporal attention instead of spatial or temporal-wise
attention to better incorporate the attention mechanism within the Transformer.
While Transformers were initially proposed to solve NLP tasks, the SNN-based Transformers were
only applies to vision tasks. We believe this is because the computational complexity of self-
attention scales quadratically with sequence length (O(T 2)), and the extra temporal dimension
further increases this to the cubic order (O(T 3)). The additional challenges of extreme sparsity,
non-differential operators, approximate gradients, and single-bit activations that are characteristic of
SNNs make training convergence more challenging.
The demonstrated image classiï¬cation tasks have a far smaller number of output classes, which
shrinks the scale of demonstrated networks. Image classiï¬cation also does not exploit the inherent
long-range learning capacity of self-attention. Therefore, there is underexplored potential in the
application of Transformer models in other SNN-based applications beyond vision tasks. In the
following sections, we demonstrate how we reduce this computational complexity to enable scaled-up
models that are capable of language generation.
3

3
Methods
3.1
Model Architecture
The high-level architecture of SpikeGPT is shown in Fig. 1. The following sections formalize the
various components of the model.
3.2
Binary Embedding
To maintain consistency with the binary activations of SNNs, we propose a binary embedding step
to convert the continuous outputs of the embedding layer into binary spikes. The conversion is
performed using a Heaviside function for feed-forward propagation, which maps the continuous
values to binary spikes. As this is a non-differentiable functino, the arctangent function (a sigmoid-
like shape) is applied as a â€˜surrogate gradientâ€™ for backward propagation to provide a biased gradient
estimator [15; 32], which can be represented as:
Ïƒâ€²(x) = 1
Ï€ arctan(Ï€x) + 1
2
(1)
This allows us to convert continuous embedding values into spikes using non-differentiable functions,
while still being able to perform backpropagation and update the weights of the embedding layer
[32].
3.3
Token Shift
Given an input X, we perform a token shift operation on it as follows:
Xs = ZeroPad[0,0,âˆ’1,1](X)
Wshift = [( i
E )n/N], i = 1, Â· Â· Â· , E
X = Wshift âŠ™X + (1 âˆ’Wshift) âŠ™Xs
(2)
where E is the embedding size of each token, ZeroPad denotes the zero padding operation2, n is the
current block, and N is the total number of blocks.
The token shift operator combines information from the global context with information of the original
token to provide the token with better contextual information. This strengthens the connection between
the token and its neighboring tokens, making it easier for the model to learn the token combinations
that have appeared before. This is similar to the induction head [33]. To some extent, token shift is a
lightweight and inexpensive alternative to the attention mechanism.
3.4
Spiking RWKV (SRWKV)
3.4.1
Recall Self-Attention
The self-attention operation lies at the heart of Transformers. In Transformers, self-attention takes an
input sequence X, and applies a scaled dot product attention. Formally, self-attention is deï¬ned as:
f(X) = Ïƒ(Q(K)T
âˆšdk
)V, s.t. Q = XMQ, K = XMK, V = XMV
(3)
where MQ âˆˆRdÃ—dk, MK âˆˆRdÃ—dk, MV âˆˆRdÃ—dv are linear transformations, and Ïƒ is the non-
linearity function by default set as the softmax (applied to each row of a matrix). dk, dv are dimensions
for key and value, respectively. Self-attention enables the model to learn the dependencies between
any two tokens in a sequence.
2The subscript [0, 0, âˆ’1, 1] is written with PyTorch syntax in mind, where âˆ’1 clips the top row and 1
zero-pads the bottom row.
4

3.4.2
Receptance Weighted Key Value (RWKV)
In this section, we introduce vanilla RWKV in natural language generation [36]. Inspired by
the Attention Free Transformer [44], RWKV acts as a replacement for self-attention. It reduces
computational complexity by swapping matrix-matrix multiplication with a convolution that sweeps
along the time dimension. We subsequently modify this step to instead operate recurrently on input
data. This modiï¬cation enables compatibility with recurrent SNNs, thus making it more manageable
to run on limited resources.
Vanilla RWKV: Given an input token-shifted embedding vector X, similar to self-attention, RWKV
ï¬rst applies a linear transform R = XMR, K = XMK, V = XMV . X is a time-varying embedding
(varying over the sequence), and so R, K, V are also time-varying. Fig. 1 depicts the sequence
unrolled into a set of 2-D matrices.
MR, MK and MV consist of learnable parameters, where K and V can be likened to the key and
value matrices of self-attention. R is referred to as the receptance matrix, where each element
indicates the acceptance of past information.
Next, the following operation is applied:3
Yt = Ïƒ(Rt) âŠ™
Pt
i=1 exp(W(T âˆ’i+1)) âŠ™exp(Ki) âŠ™Vi
Pt
i=1 exp(W(T âˆ’i+1)) âŠ™exp(Ki)
(4)
where âŠ™is the element-wise product, T is the sequence length, Ïƒ is the nonlinearity applied to R
with the default being sigmoid; W âˆˆRT Ã—E is the positional weight decay matrix. W encodes
the sequential importance of a given word on subsequent words. It is not directly learnable, but is
determined by other learnable parameters.
Intuitively, as time t increases, the vector Yt is dependent on a longer history, represented by the
summation of an increasing number of terms. For the target position t, RWKV performs a weighted
summation in the positional interval of [1, t], and takes the Hadamard product of the weighted result
with the receptance Ïƒ(Rt). By taking the sigmoid of Rt, the receptance acts as a â€˜forget gateâ€™ by
eliminating unnecessary historical information.
Similarity to Multi-Headed Self-Attention: Distinct from the method of calculating the matching
degree4 between tokens by the self-attention mechanism, RWKV decomposes the calculation of
matching degree into: Î±ij = Ïƒ(Ri) âŠ™exp(WT âˆ’i+1) âŠ™exp(Kj), where Î±ij âˆˆRE is a vector. Each
element in Î±ij, that is Î±ijk, represents the matching degree at the k-th position of the embedding of
the i-th and j-th tokens. In other words, it can be seen as a multi-headed RWKV with E heads, each
of which has a hidden size=1, which is similar to the multi-headed self-attention (MHA) mechanism.
Positional Weight Decay: The positional weight bias W is a function of both learnable parameters
and pre-calculated matrices formalized below. In general, for a given word, the elements of W decay
over the sequence. When this rule of thumb does not hold, this likely means the model is embedding
long-range dependencies across a sequence. The positional weight bias matrix W is determined by
three matrices, Wd, Wc and Wf:
Wd = ln(Ws), Ws âˆˆREÃ—1
(5)
Wc = [(âˆ’T + 2)
(âˆ’T + 3)
(âˆ’T + 4)
Â· Â· Â·
âˆ’1
0] âˆˆR1Ã—(T âˆ’1)
(6)
Wf = [ln(0.3) ln(0.3) Â· Â· Â· ln(0.3)] âˆˆREÃ—1
(7)
where Ws is a pre-calculated matrix dependent on the layer and size of E, Wd and Wf are both
learnable, and Wc is a static, pre-calculated matrix based on a decay prior. The ï¬nal matrix W is
calculated below:
W = exp(concat(Wd Ã— exp(Wc), Wf)), W âˆˆREÃ—T
(8)
3{MR, MK, MV } âˆˆREÃ—H, where H denotes hidden size. In RWKV, we set E = H.
4A scalar in self-attention, Î±ij = QiKT
j
5

where concat denotes the concatenation of two tensors in the temporal dimension, and the operator
â€˜Ã—â€™ is the outer-product of two vectors.
RWKV as a 1-D Convolution: Eq. 4 only calculates the weighted summation across target positions
t. On the basis of Eq. 4, the values of all target positions can be represented as a 1-D convolution:
Y = Ïƒ(R) âŠ™exp(W) âŠ—(LeftPad(exp(K) âŠ™V ))
exp(W) âŠ—LeftPad(exp(K))
(9)
where âŠ—denotes the 1-D convolution operation, LeftPad applies zero-padding to all columns preced-
ing the T âˆ’1th position.
Consider W to be a large convolutional kernel, performing a convolution with the matrix exp(K) (or
exp(K) âŠ™V ). The computational complexity of the complete convolution is O(ET 2) (assuming the
number of ï¬lters matches the sequence length, and E is the embedding size). This can be further
optimized by adopting the Fast Fourier Transform (FFT) to reduce the time complexity of the whole
convolution operation to O(ETlogT).
Compatibility with SNNs: The behavior of individual spiking neurons in an SNN is often described
using differential equations, which cannot be solved analytically in closed-form expressions. In the
context of recurrent networks, these equations must be solved numerically, which typically requires
iterative methods that calculate the systemâ€™s behavior step-by-step over time. Fortunately, from Eq. 4,
we are able to derive a recurrent form of RWKV, which is perfectly compatible with recurrent SNNs.
3.4.3
RWKV Enabled SNN
The serial RNN form of RWKV is expressed as follows:
Y [t + 1] = Ïƒ(RX[t]) Â· exp(KY [t]) Â· (V Y [t]) + exp(W) Â· A[t]
exp(KY [t]) + exp(W) Â· B[t]
(10)
where t represents the time step index, and variables R, W, K, V are the same as Eq. 9. The hidden
states A and B are represented by
A[t] = exp(KY [t âˆ’1]) Â· (V Y [t âˆ’1]) + exp(W) Â· A[t âˆ’1]
(11)
and
B[t] = exp(KY [t âˆ’1]) + exp(W) Â· B[t âˆ’1]
(12)
Finally, we integrate the spiking neuron model into the Spiking-RWKV module. As RWKV has been
serialized, not only does the computational complexity decrease from O(ETlogT) to O(ET), but the
output of RWKV can be sequentially passed directly to spiking neurons without having to unsqueeze
dimensionality for feed-forward. This is in stark contrast to prior SNN-based Transformer methods
which combine matrix-matrix multiplications along with recurrence. This leads to computational
complexity scaling cubically with sequence length, without enhancing the networkâ€™s ability to learn
sequential information. Consequently, we achieve a more streamlined approach in our feed-forward
process, allowing us to effectively process data in a streaming manner.
We employ the Leaky Integrate-and-Fire (LIF) neuron as the default spiking neuron of our model, a
widely used model for SNNs often trained via error backpropagation [25]. The formula is represented
as follows:
( U[t] = H[t] + Î²(Y [t] âˆ’(H[t âˆ’1] âˆ’Ureset))
S[t] = Î˜(U[t] âˆ’Uthreshold)
H[t] = U[t] Â· (1 âˆ’S[t])
(13)
where Î² is a decay factor, U is the membrane potential (or hidden state) of the neuron, S is the
spiking tensor with binarized elements, Y denotes the output of the previous series RWKV block (see
Eq. 10), Î˜(Â·) denotes the Heaviside function, and H represents the reset process after spike emission.
We set Uthreshold = 1 and Ureset = 0 as done in Refs. [46; 27; 28].
To overcome the non-differentiable problem during the back-propagation caused by the Heaviside
step function Î˜(Â·), we employ the surrogate gradient approach. As with the binary embedding in
Sec. 3.2, we utilize the arctangent surrogate function (Eq. 1) during the backward pass.
6

3.5
Spiking Receptance Feed-Forward Networks (SRFFN)
Each block in our model contains a fully connected feed-forward network with a gating mechanism
(SRFFN), which is applied to normalized and token-shifted output of each spiking-RWKV module.
This SRFFN module consists of three linear transformations with ReLU 2 activations as follows:
Y â€²[t] = Ïƒ(MP X[t]) âŠ™MS(ReLU 2(MGX[t]))
(14)
where Y â€²[t] denotes the output of SRFFN at time-step t which is then passed to the spiking neuron
(Eq. 13). {MP , MG, MS} âˆˆREÃ—H are learnable parameters of the linear transformations. SRFFN
is a variant of the Gated Linear Unit (GLU) [7], which can control the degree of information ï¬‚owing
into the model by Ïƒ(MP X[t]). In order to maintain the consistency of SRFFN and GEGLU [39]
parameters, we set the size of H from the SRFFN to 4E.
3.6
Training & Inference
This section will be updated with details for each model once all SpikeGPT variants have completed
training. Until then, we refer the reader to the SpikeGPT repository.
4
Experiments
We conduct a series of experiments to optimize the performance of our SpikeGPT model by training
it with three varying parameter scales: 45 million, 125 million, and 260 million parameters. Our code
is based on PyTorch [35] and SpikingJelly [14].
4.1
Datasets
We test two variants of the 45 million parameter model; one where T = 1024 and another where
T = 3, 072. We used the Enwik8 dataset to conduct both training and testing. The ï¬ndings of
this experiment are presented in Table 1. To explore the efï¬ciency of our 125 million parameter
scale, we trained our model using the BookCorpus [47] dataset, and text generated samples are
provided in Fig. 3. Our most extensive model with 260 million parameters was trained using the
OpenWebText2 [17] dataset. Text samples of this experiment are shown in Fig. 2. At present, we are
conducting additional experiments on the larger models and will update this preprint once completed.
All experiments were conducted on four NVIDIA V100 graphic cards. For the models of 45M, 120M
and 260M, we trained them for 12, 24 and 48 hours respectively.
Table 1: Enwik8 results, measured in bits per character (bpc): the lower the better. Baseline
comparisons are made with Reformer [22], Synthesizer [41] (the best performing dense version),
Linear Transformer [20], Performer [4], Stacked LSTM [18] and SHA-LSTM [29]. L, d, and T
denote the number of blocks (network depth), dimension of features, and sequence length, respec-
tively. Both Linear Transformer and Performer are implemented with customized CUDA kernels
(github.com/idiap/fast-transformers), and all other models are implemented in native Pytorch. (Note:
Interim results. Still in training; to be updated.)
Method
Binary
L
d
T
Train bpc
Test bpc
SynOps
Transformer

12
512
1024
0.977
1.137
9.6 Ã— 1010
Transformer

24
256
1024
1.039
1.130
-
Reformer

12
512
1024
1.040
1.195
-
Synthesizer

12
512
1024
0.994
1.298
-
Linear Transformer

12
512
1024
0.981
1.207
Performer

12
512
1024
1.002
1.199
-
Stacked LSTM

7
-
-
1.420
1.670
-
SHA-LSTM (no attention)

4
1024
1024
-
1.330
-
SpikeGPT 45M

12
512
1024
1.113
1.283
4.35 Ã— 109
SpikeGPT 45M

12
512
3072
0.864
1.262
1.30 Ã— 1010
7

Context â†’
Who are you?
I am the AI.
Who are you?
I am Jack.
Who are you?
I am a woman.
Who are you?
â€“â€“â€“â€“ Generated Answer 1 â€“â€“â€“â€“
â€“â€“â€“â€“ Generated Answer 2 â€“â€“â€“â€“
Amy.
I am the woman who is,
The amazing thing is,
in my book.
you can tell the stories about what happened to me.
Figure 2: Example of text generated by SpikeGPT 260M. The model is trained on OpenWebText2.
Context â†’
<start>
â€“â€“â€“â€“ Generated Answer 1 â€“â€“â€“â€“
â€so what do you think of me?
â€
I asked.
he was standing behind me, still looking at the posters,
and I suddenly knew what he was asking.
â€œ what do you think of me?
â€
I looked up at him , the corners of his lips pressed together.
â€i donâ€™t know.â€
he looked at me and i was staring at him, not even realizing what he was doing.
all I could do was watch him,
look at him, listen to his voice,
and then I would fall back to the stage and look at him.
Figure 3: Example of text generated by SpikeGPT 120M. The model is trained on BookCorpus.
4.2
Comparisons
With the Enwik8 dataset, we use the same training/test splits and pre-processing conventions as
Ref. [6]. A 12 layer, 512-dimensional, 8-head architecture with 1024-neuron dense layer serves as our
Transformer benchmark. We also compare with a number of effective, similarly sized Transformer
baselines, including Reformer [22], Synthesizer [41], Linear Transformer [20], Performer [4]. In
addition, as we are using recurrent structures, we included representative LSTM variants [19]: Stacked
LSTM [18], and SHA-LSTM [29]. From Tab. 1, we see that with the L = 12, d = 512, T = 3072
architecture, the proposed model achieves the lowest training bits per character (bpc), which is an
indicator for high model capacity.
4.3
Results
While our modelâ€™s test performance is slightly less than that of the standard Transformer and several
other Transformer variations, it nonetheless remains similar in performance with 22Ã— less synaptic
operations (SynOps). SynOps is a metric that accounts for activation sparsity, where only multiply-
accumulate operations using non-zero activations are counted. The Transformer is measured using
full precision (ï¬‚t32) SynOps, whereas SpikeGPT uses binarized SynOps. Therefore, a given SynOp
for SpikeGPT is substantially cheaper in terms of energy consumption compared to a SynOp of the
Transformer.
Neuromorphic hardware is able to exploit activation sparsity by skipping memory access and compu-
tation when no spikes are emitted [8; 31; 26; 16]. We continue to optimize the larger-scale models
and will update this preprint as we develop more detailed performance metrics.
8

5
Conclusion
Our preliminary results demonstrate that event-driven spiking activations are not only capable of
language generation, but they can do so with fewer high-cost operations. We develop techniques
that promote lightweight models for the NLP community, and make large-scale models for the
neuromorphic and SNN community more effective. We demonstrate how large SNNs can be trained
in a way that harnesses advances in transformers and our own serialized version of the attention
mechanisms. In the meantime, we continue to test and validate our larger scale models and will
continue to update this preprint and provide our code implementation here: https://github.com/
ridgerchu/SpikeGPT.
Acknowledgements
We are grateful to Bo Peng for his fruitful comments, corrections and inspiration in making large-
language models more accessible.
References
[1] Lasse F Wolff Anthony, Benjamin Kanding, and Raghavendra Selvan. Carbontracker: Track-
ing and predicting the carbon footprint of training deep learning models. arXiv preprint
arXiv:2007.03051, 2020.
[2] Sami Barchid, JosÃ© Mennesson, Jason Eshraghian, Chaabane DjÃ©raba, and Mohammed Ben-
namoun. Spiking neural networks for frame-based and event-based single object localization.
arXiv preprint arXiv:2206.06506, 2022.
[3] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are
few-shot learners. arXiv preprint arXiv:2005.14165, 2020.
[4] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane,
Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, David Belanger,
Lucy Colwell, and Adrian Weller. Rethinking attention with performers, 2020.
[5] LoÃ¯c Cordone, BenoÃ®t Miramond, and Philippe Thierion. Object detection with spiking neural
networks on automotive event data. In 2022 International Joint Conference on Neural Networks
(IJCNN), pages 1â€“8. IEEE, 2022.
[6] Zihang Dai, Z. Yang, Yiming Yang, J. Carbonell, Quoc V. Le, and R. Salakhutdi-
nov.
Transformer-xl: Attentive language models beyond a ï¬xed-length context.
ArXiv,
abs/1901.02860, 2019.
[7] Yann N. Dauphin, Angela Fan, Michael Auli, and David Grangier. Language modeling with
gated convolutional networks. In Proceedings of the 34th International Conference on Machine
Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, volume 70 of Proceedings of
Machine Learning Research, pages 933â€“941. PMLR, 2017.
[8] Mike Davies, Narayan Srinivasa, Tsung-Han Lin, Gautham Chinya, Yongqiang Cao, Sri Harsha
Choday, Georgios Dimou, Prasad Joshi, Nabil Imam, Shweta Jain, et al. Loihi: A neuromorphic
manycore processor with on-chip learning. Ieee Micro, 38(1):82â€“99, 2018.
[9] Payal Dhar. The carbon impact of artiï¬cial intelligence. Nature Machine Intelligence, 2:423â€“5,
2020.
[10] Peter U Diehl, Guido Zarrella, Andrew Cassidy, Bruno U Pedroni, and Emre Neftci. Conversion
of artiï¬cial recurrent neural networks to spiking neural networks for low-power neuromorphic
hardware. In 2016 IEEE International Conference on Rebooting Computing (ICRC), pages 1â€“8.
IEEE, 2016.
[11] Jason K Eshraghian and Wei D Lu. The ï¬ne line between dead neurons and sparsity in binarized
spiking neural networks. arXiv preprint arXiv:2201.11915, 2022.
9

[12] Jason K Eshraghian, Xinxin Wang, and Wei D Lu. Memristor-based binarized spiking neural
networks: Challenges and applications. IEEE Nanotechnology Magazine, 16(2):14â€“23, 2022.
[13] Jason K Eshraghian, Max Ward, Emre Neftci, Xinxin Wang, Gregor Lenz, Girish Dwivedi,
Mohammed Bennamoun, Doo Seok Jeong, and Wei D Lu. Training spiking neural networks
using lessons from deep learning. arXiv preprint arXiv:2109.12894, 2021.
[14] Wei Fang, Yanqi Chen, Jianhao Ding, Ding Chen, Zhaofei Yu, Huihui Zhou, TimothÃ©e
Masquelier, Yonghong Tian, and other contributors. Spikingjelly. https://github.com/
fangwei123456/spikingjelly, 2020. Accessed: 2022-05-21.
[15] Wei Fang, Zhaofei Yu, Yanqi Chen, Tiejun Huang, TimothÃ©e Masquelier, and Yonghong Tian.
Deep residual learning in spiking neural networks. Advances in Neural Information Processing
Systems, 34:21056â€“21069, 2021.
[16] Charlotte Frenkel. Sparsity provides a competitive advantage. Nature Machine Intelligence,
3(9):742â€“743, 2021.
[17] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason
Phang, Horace He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse
text for language modeling. arXiv preprint arXiv:2101.00027, 2020.
[18] Alex Graves.
Generating sequences with recurrent neural networks.
arXiv preprint
arXiv:1308.0850, 2013.
[19] Sepp Hochreiter and JÃ¼rgen Schmidhuber. Long short-term memory. Neural computation,
9(8):1735â€“1780, 1997.
[20] A. Katharopoulos, A. Vyas, N. Pappas, and F. Fleuret. Transformers are rnns: Fast autoregressive
transformers with linear attention. In Proceedings of the International Conference on Machine
Learning (ICML), 2020.
[21] Seijoon Kim, Seongsik Park, Byunggook Na, and Sungroh Yoon. Spiking-yolo: spiking neural
network for energy-efï¬cient object detection. In Proceedings of the AAAI conference on artiï¬cial
intelligence, volume 34, pages 11270â€“11277, 2020.
[22] Nikita Kitaev, L. Kaiser, and Anselm Levskaya. Reformer: The efï¬cient transformer. ArXiv,
abs/2001.04451, 2020.
[23] Yudong Li, Yunlin Lei, and Xu Yang. Spikeformer: A novel architecture for training high-
performance low-latency spiking neural network. arXiv preprint arXiv:2211.10686, 2022.
[24] Changze Lv, Jianhan Xu, and Xiaoqing Zheng. Spiking convolutional neural networks for text
classiï¬cation. In The Eleventh International Conference on Learning Representations, 2023.
[25] Wolfgang Maass. Networks of spiking neurons: the third generation of neural network models.
Neural networks, 10(9):1659â€“1671, 1997.
[26] Joshua Mack, Ruben Purdy, Kris Rockowitz, Michael Inouye, Edward Richter, Spencer Valan-
cius, Nirmal Kumbhare, Md Sahil Hassan, Kaitlin Fair, John Mixter, et al. Ranc: Reconï¬gurable
architecture for neuromorphic computing. IEEE Transactions on Computer-Aided Design of
Integrated Circuits and Systems, 40(11):2265â€“2278, 2020.
[27] Yao Man, Huanhuan Gao, Guangshe Zhao, Wang Dingheng, Yihan Lin, Zhaoxu Yang, and
Guoqi Li. Temporal-wise attention spiking neural networks for event streams classiï¬cation.
pages 10201â€“10210, 10 2021.
[28] Yao Man, Guangshe Zhao, Hengyu Zhang, Hu Yifan, Lei Deng, Yonghong Tian, Bo Xu, and
Guoqi Li. Attention spiking neural networks. IEEE Transactions on Pattern Analysis and
Machine Intelligence, PP:1â€“18, 01 2023.
[29] Stephen Merity. Single headed attention rnn: Stop thinking with your head. arXiv preprint
arXiv:1911.11423, 2019.
10

[30] Paul A Merolla, John V Arthur, Rodrigo Alvarez-Icaza, Andrew S Cassidy, Jun Sawada, Filipp
Akopyan, Bryan L Jackson, Nabil Imam, Chen Guo, Yutaka Nakamura, et al. A million
spiking-neuron integrated circuit with a scalable communication network and interface. Science,
345(6197):668â€“673, 2014.
[31] Farhad Modaresi, Matthew Guthaus, and Jason K Eshraghian. OpenSpike: An OpenRAM SNN
Accelerator. arXiv preprint arXiv:2302.01015, 2023.
[32] Emre O Neftci, Hesham Mostafa, and Friedemann Zenke. Surrogate gradient learning in spiking
neural networks: Bringing the power of gradient-based optimization to spiking neural networks.
IEEE Signal Processing Magazine, 36(6):51â€“63, 2019.
[33] Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom
Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Dawn Drain,
Deep Ganguli, Zac Hatï¬eld-Dodds, Danny Hernandez, Scott Johnston, Andy Jones, Jackson
Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Ka-
plan, Sam McCandlish, and Chris Olah. In-context learning and induction heads. CoRR,
abs/2209.11895, 2022.
[34] OpenAI. ChatGPT: Optimizing language models for dialogue. https://openai.com/blog/
chatgpt/. Accessed: 2023-02-18.
[35] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,
Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative
style, high-performance deep learning library. Advances in neural information processing
systems, 32, 2019.
[36] Bo PENG. RWKV-LM. https://github.com/BlinkDL/RWKV-LM, 8 2021.
[37] Michael Pfeiffer and Thomas Pfeil. Deep learning with spiking neurons: Opportunities and
challenges. Frontiers in Neuroscience, 12:774, 2018.
[38] Kaushik Roy, Akhilesh Jaiswal, and Priyadarshini Panda.
Towards spike-based machine
intelligence with neuromorphic computing. Nature, 575(7784):607â€“617, 2019.
[39] Noam Shazeer. GLU variants improve transformer. CoRR, abs/2002.05202, 2020.
[40] Pao-Sheng Vincent Sun, Alexander Titterton, Anjlee Gopiani, Tim Santos, Arindam Basu,
Wei D Lu, and Jason K Eshraghian. Intelligence processing units accelerate neuromorphic
learning. arXiv preprint arXiv:2211.10725, 2022.
[41] Yi Tay, Dara Bahri, Donald Metzler, Da-Cheng Juan, Zhe Zhao, and Che Zheng. Synthesizer:
Rethinking self-attention in transformer models, 2020.
[42] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Åukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information
processing systems, pages 5998â€“6008, 2017.
[43] Rong Xiao, Yu Wan, Baosong Yang, Haibo Zhang, Huajin Tang, Derek F Wong, and Boxing
Chen. Towards energy-preserving natural language understanding with spiking neural networks.
IEEE/ACM Transactions on Audio, Speech, and Language Processing, 31:439â€“447, 2022.
[44] Shuangfei Zhai, Walter Talbott, Nitish Srivastava, Chen Huang, Hanlin Goh, Ruixiang Zhang,
and Josh Susskind. An attention free transformer. arXiv preprint arXiv:2105.14103, 2021.
[45] Zhaokun Zhou, Yuesheng Zhu, Chao He, Yaowei Wang, Shuicheng YAN, Yonghong Tian,
and Li Yuan. Spikformer: When spiking neural network meets transformer. In The Eleventh
International Conference on Learning Representations, 2023.
[46] Rui-Jie Zhu, Qihang Zhao, Tianjing Zhang, Haoyu Deng, Yule Duan, Malu Zhang, and Liang-
Jian Deng. Tcja-snn: Temporal-channel joint attention for spiking neural networks. arXiv
preprint arXiv:2206.10177, 2022.
[47] Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba,
and Sanja Fidler. Aligning books and movies: Towards story-like visual explanations by
watching movies and reading books. In Proceedings of the IEEE international conference on
computer vision, pages 19â€“27, 2015.
11

