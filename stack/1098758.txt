Bayesian model inference
in dynamic biological systems
using Markov Chain Monte Carlo methods
Daniel Schmidl
February 2012


TECHNISCHE UNIVERSIT¨AT M¨UNCHEN
Lehrstuhl M12 (Biomathematik)
“Bayesian model inference in dynamic biological
systems using Markov Chain Monte Carlo
methods”
Daniel Schmidl
Vollst¨andiger Abdruck der von der Fakult¨at f¨ur Mathematik der Technischen Univer-
sit¨at M¨unchen zur Erlangung des akademischen Grades eines
Doktors der Naturwissenschaften (Dr. rer. nat.)
genehmigten Dissertation.
Vorsitzender:
Univ.-Prof. Dr. Oliver Junge
Pr¨ufer der Dissertation:
1. Univ.-Prof. Dr. Dr. Fabian J. Theis
2. Univ.-Prof. Claudia Czado, Ph.D.
3. Univ.-Prof. Dr. Achim Tresch, Universit¨at zu K¨oln (nur schriftliche Beurteilung)
Die Dissertation wurde am 14.03.2012 bei der Technischen Universit¨at M¨unchen ein-
gereicht und durch die Fakult¨at f¨ur Mathematik am 21.08.2012 angenommen.


To my parents.


Acknowledgments
Working on Monte Carlo methods I was able to gain quite some expertise in
the ﬁeld of educated guessing. This is why I can almost surely say that for
a few people these are the most interesting pages of the entire work. Here is
where you ﬁnd out who incited me to write this thesis and who supported,
pushed and accompanied me over the last few years. I am deeply grateful
that all of them made this a very valuable time of my life.
The work in this thesis has been supervised by Prof. Dr. Dr. Fabian Theis.
First and foremost, I would like to thank him for putting trust in me and
giving me the opportunity to work almost independently on the various
projects. Not only was he able to push me scientiﬁcally, but also in various
running competitions. I must admit that there might still be some potential
left on my side.
Thanks for the chance to take part in building up an
extraordinary interdisciplinary group on computational modeling in biology.
I am also very grateful to Prof. Dr. Claudia Czado, who never tired in
explaining details on copula distributions and (grapeless) vines. It was a
pleasure to work with her.
Thank you Prof. Dr. Achim Tresch for digging yourself through this thesis
and evaluation this work.
The entire CMB group: Thanks for many valuable discussions, but moreover
for putting fun into science. It was really nice to work with every single one
of you. Thank you Jan for almost reinventing computer science in order to
solve some of my computer problems and Dom and Florian for teaching me
basics on biology and cows. A special thank to Dominik for always taking
time to discuss mathematical problems and for making our conference trips
a fun time. Andreas, thank you for teaching me how to get things done in
a quick and diligent way. Our time in Boston and New York was legendary!
I want to express my gratitude and appreciation to Sabine for putting a lot
of eﬀort into the zirconium processing project. It was really nice to work
with you! Moreover, thank you for organizing an endless number of group

events, for carefully proof-reading this thesis and for introducing me to pink
colored corrections.
Eike and Ulf, thank you for your support on vine estimation. I am grate-
ful to the whole statistics group at the Technische Universit¨at M¨unchen.
Teaching with you taught me a lot as well.
Andi, Cathi, Kerstin, Markus, and Tom, my dear companions, I wish anyone
could have friends like you! Knowing you is without exaggeration one of
the greatest gifts in life and I am truly looking forward to all the fun times
to come.
Nina, talking to you always makes my day! Your eagerness along with your
open-hearted nature is unmatched. Ironically, we still have to work on your
understanding of the word moaning. Thank you for always having an open
ear. I can say you are in many ways a pure source of motivation.
Richard, I really enjoyed going through PhD with you, showing me that
the grass is not greener on the other side. Thank you for always listening
carefully to a friend’s needs! Your jolly company is true inspiration.
Finally, a very special thank goes to my parents!
Although not exactly
familiar with my scientiﬁc way of life, their lifelong encouragement and
unconditional support always kept me on track and taught me what being
true to yourself really means. Words cannot convey my gratitude!

Abstract
Dynamical systems are a valuable tool for exploring the regulatory organi-
zation of living organisms on a molecular level. Here, profound knowledge
about underlying reaction rate parameters is essential for biological predic-
tions and hypotheses. Bayesian methods provide a sophisticated approach
to infer these parameters including statistical dependencies and uncertain-
ties. Moreover, they are able to determine an appropriate model structure
by model selection. As the posterior distributions involved are generally
analytically intractable, Markov Chain Monte Carlo (MCMC) methods are
used for inference. However, owing to posterior complexity, these methods
are often ineﬃciently sampling from the according parameter spaces. In this
thesis we develop a Copula based Independence/random walk Metropolis-
Hastings (CIMH) sampling scheme for eﬃcient model inference of diﬀeren-
tial equation based dynamical systems. The concept exploits a vine copula
decomposition of the estimated posterior distribution in order to generate
proposals that are distributed according to an approximation of the true
posterior distribution, which yields high acceptance rates. The basic CIMH
algorithm is furthermore extended to an Adaptive MCMC scheme (ACIMH)
to speed up convergence in complex systems. We thoroughly compare CIMH
and ACIMH to existing methods on various examples. Furthermore, in ap-
plications from the ﬁeld of systems biology, we examine the mechanism of
nuclear phosphorylated STAT3 dimer import in the JAK1-STAT3 signaling
pathway. Here, ACIMH infers a pathway model based on mouse hepato-
cyte data. In addition, using CIMH, we analyze a biokinetic compartment
model for zirconium processing in the human body that can readily be used
in radiation protection. Transfer rates (including credible intervals) for an
average individual are provided and form the basis for analyses considering
retrospective dosimetry and bone retention of zirconium.

Zusammenfassung
Dynamische Systeme sind n¨utzliche Hilfsmittel zur Erforschung der moleku-
laren Funktionsweise lebender Organismen. Fundiertes Wissen ¨uber Reak-
tionsparameter ist hierbei entscheidend f¨ur biologische Modellvorhersagen
und Hypothesen. Bayesianische Verfahren bieten einen differenzierten An-
satz zur Sch¨atzung dieser Parameter. Dar¨uber hinaus erm¨oglichen sie die
Inferenz einer geeigneten Modellstruktur durch Modellselektion. Da die zu-
geh¨origen Posteriori-Verteilungen i.Allg. analytisch unl¨osbar sind, verwen-
det man approximative Markov-Chain-Monte-Carlo-Verfahren (MCMC-Ver-
fahren). Im Falle komplexer Posteriori-Verteilungen generieren diese jedoch
h¨auﬁg inefﬁziente Stichprobenvorschl¨age. Wir entwickeln in dieser Arbeit
einen Metropolis-Hastings-Algorithmus (CIMH) zur efﬁzienten Modellinfe-
renz von dynamischen Systemen, welche auf Differentialgleichungen basie-
ren. Das Konzept nutzt die Vine-Copula-Zerlegung einer approximativen
Posteriori-Verteilung zur Erzeugung von Stichprobenvorschl¨agen, welche
¨ahnlich der wahren Posteriori-Verteilung verteilt sind. Der CIMH-Algorith-
mus wird anschließend zu einem adaptiven Verfahren erweitert (ACIMH).
Dies beschleunigt die Konvergenz in komplexen Systemen. Wir vergleichen
CIMH und ACIMH mit etablierten Verfahren anhand verschiedener Bei-
spiele.
Dar¨uber hinaus wenden wir die Algorithmen auf Fragestellungen
aus dem Bereich der Systembiologie, wie etwa den Ablauf des STAT3-Di-
mertransports in den Zellkern f¨ur den JAK1-STAT3-Signalweg, an. Hier
benutzen wir ACIMH, um ein auf Maushepatozyten basierendes, mathe-
matisches Signalwegmodell zu inferieren. Zudem verwenden wir CIMH zur
Analyse biokinetischer Modelle f¨ur die Verarbeitung von Zirkonium (Zr) im
menschlichen K¨orper. ¨Ubertragungsraten (mit Kredibilit¨atsintervallen) f¨ur
eine Durchschnittsperson werden berechnet und bilden die Grundlage f¨ur
retrospektive Dosimetrie und die Analyse von Zr-Einlagerungen in Knochen.

Contents
List of Figures
xiii
List of Tables
xv
1
Introduction
1
2
Prerequisites
9
2.1
Basics on probability theory . . . . . . . . . . . . . . . . . . . . . . . . .
9
2.2
Copula distributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
15
2.2.1
Pair copula decomposition
. . . . . . . . . . . . . . . . . . . . .
19
2.2.2
Vines
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
22
2.3
Markov chains
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
26
2.4
A short introduction on molecular biology . . . . . . . . . . . . . . . . .
35
2.4.1
Signaling pathways . . . . . . . . . . . . . . . . . . . . . . . . . .
37
2.4.2
The JAK-STAT pathway
. . . . . . . . . . . . . . . . . . . . . .
37
2.5
Dynamical systems in molecular biology . . . . . . . . . . . . . . . . . .
38
2.5.1
Compartment models
. . . . . . . . . . . . . . . . . . . . . . . .
45
2.5.2
Parameter estimation in dynamical systems . . . . . . . . . . . .
46
2.5.3
Parameter identiﬁability in dynamical systems
. . . . . . . . . .
47
3
Bayesian model inference
51
3.1
Bayesian parameter inference . . . . . . . . . . . . . . . . . . . . . . . .
51
3.2
Prior distributions
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
54
3.3
Bayesian parameter identiﬁability . . . . . . . . . . . . . . . . . . . . . .
56
3.4
Bayes factors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
56
3.4.1
The prior arithmetic mean estimate
. . . . . . . . . . . . . . . .
60
ix

CONTENTS
3.4.2
The posterior harmonic mean estimate . . . . . . . . . . . . . . .
61
3.4.3
Thermodynamic integration
. . . . . . . . . . . . . . . . . . . .
63
3.4.4
Example: A Gaussian mixture model
. . . . . . . . . . . . . . .
66
4
Markov Chain Monte Carlo (MCMC) methods
69
4.1
The Metropolis-Hastings (MH) algorithm
. . . . . . . . . . . . . . . . .
70
4.2
Independent identically distributed samples from a Markov chain
. . .
74
4.3
A measure for independence
. . . . . . . . . . . . . . . . . . . . . . . .
75
4.4
Convergence to the stationary distribution
. . . . . . . . . . . . . . . .
78
4.5
Reversible jump MCMC
. . . . . . . . . . . . . . . . . . . . . . . . . .
80
4.6
The simulated annealing algorithm
. . . . . . . . . . . . . . . . . . . .
83
5
Extensions to the Metropolis-Hastings algorithm
85
5.1
Simpliﬁed Riemann Manifold Metropolis Adjusted
Langevin Algorithm (SMALA)
. . . . . . . . . . . . . . . . . . . . . . .
85
5.2
Adaptive MCMC . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
88
5.3
Metropolis Gaussian Adaption algorithm (M-GaA)
. . . . . . . . . . .
90
6
Improving the Metropolis-Hastings algorithm using copulas
93
6.1
Copula based Independence MH algorithm (CIMH)
. . . . . . . . . . .
94
6.1.1
The basic copula MH sampling procedure . . . . . . . . . . . . .
95
6.1.2
CIMH as adaptive sampling scheme
. . . . . . . . . . . . . . . .
99
6.2
Adaptive Copula based Independence MH algorithm (ACIMH) . . . . . 101
6.3
Performance of CIMH and ACIMH . . . . . . . . . . . . . . . . . . . . . 102
6.3.1
Sampling from a strongly correlated 2-dim. normal distribution . 106
6.3.2
Performance on a steady state model with nonlinear parameter
dependency . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 109
6.3.3
Performance on a small compartment model . . . . . . . . . . . . 115
6.3.4
Performance on a JAK2-STAT5 signaling pathway model . . . . 118
6.3.5
Robustness with respect to the choice of the pair copula decom-
position and cdf’s for prerun sample transformation
. . . . . . . 125
6.4
Conclusions on CIMH and ACIMH . . . . . . . . . . . . . . . . . . . . . 126
7
Model inference of the JAK1-STAT3 pathway
127
7.1
Experimental JAK1-STAT3 data . . . . . . . . . . . . . . . . . . . . . . 128
x

CONTENTS
7.2
Mathematical models for the JAK1-STAT3 pathway . . . . . . . . . . . 128
7.3
Inference of the JAK1-STAT3 model . . . . . . . . . . . . . . . . . . . . 131
8
Inference of biokinetic models for zirconium processing in humans
137
8.1
Experimental zirconium data . . . . . . . . . . . . . . . . . . . . . . . . 139
8.2
Mathematical models for zirconium processing
. . . . . . . . . . . . . . 139
8.3
Prior information for zirconium processing and
algorithmic set up
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 143
8.4
Inference of the zirconium models . . . . . . . . . . . . . . . . . . . . . . 146
8.4.1
Investigation speciﬁcity of transfer rates . . . . . . . . . . . . . . 146
8.4.2
Parameter correlations . . . . . . . . . . . . . . . . . . . . . . . . 147
8.4.3
Bayesian model comparison of the HMGU and ICRP models
. . 150
8.4.4
Diﬀerences in radioactive 95Zr retention in bone predicted by the
HMGU and ICRP models . . . . . . . . . . . . . . . . . . . . . . 152
8.4.5
Retrospective dose assessment . . . . . . . . . . . . . . . . . . . . 154
9
Conclusions and outlook
157
A Important univariate density functions
161
B Important bivariate copulas
163
C Calculations for the Bayes factor of the Gaussian mixture model
167
C.1
Power posterior and marginal likelihood for the one-component Gaussian
(mixture) model
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 167
C.2
Power posterior for the two-component Gaussian (mixture) model
. . . 168
C.3
Expected value of the log likelihood w.r.t. the power posterior for the
one-component Gaussian (mixture) model . . . . . . . . . . . . . . . . . 169
D Transformation of the JAK2-STAT5 DDE system
173
E Geometric tensor for the JAK2-STAT5 DDE system
175
F Parameters for prior distributions of the zirconium models
181
G Investigation speciﬁc time courses for the ICRP and HMGU models183
References
187
xi

CONTENTS
Index
199
xii

List of Figures
1.1
Posterior distribution of the JAK1-STAT3 model (7.1) marginalized on
the k1 and k6 dimension. . . . . . . . . . . . . . . . . . . . . . . . . . . .
2
2.1
Bivariate copula densities. . . . . . . . . . . . . . . . . . . . . . . . . . .
19
2.2
Examples of vines. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
25
2.3
Realizations of various random processes.
. . . . . . . . . . . . . . . . .
28
2.4
Dynamics of the elementary biochemical reactions of example 2.10. . . .
44
2.5
SIR model. Graphical representation and time courses. . . . . . . . . . .
45
2.6
Schematic representation and time courses of model (2.33).
. . . . . . .
49
3.1
Example path for thermodynamic integration. . . . . . . . . . . . . . . .
64
4.1
Realizations and according histograms of a Markov chain. . . . . . . . .
73
5.1
Graphical representation of the discrete state space models. . . . . . . .
89
6.1
Thinned Markov chain samples of the strongly correlated bivariate nor-
mal distribution. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 108
6.2
Markov chains of the strongly correlated bivariate normal distribution. . 109
6.3
Results for the strongly correlated bivariate normal distribution.
. . . . 110
6.4
Autocorrelation functions of the strongly correlated bivariate normal dis-
tribution.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 110
6.5
Toy data, posterior median solution, and 95% credible interval as well
as copula data and thinned MCMC samples of the steady state model. . 113
6.6
Unthinned Markov chains of the steady state model. . . . . . . . . . . . 113
6.7
Results for the steady state model. . . . . . . . . . . . . . . . . . . . . . 114
xiii

LIST OF FIGURES
6.8
Schematic representation, toy data, posterior median solution, and 95%
credible interval as well as copula data of the small compartment model. 117
6.9
Results for the compartment model.
. . . . . . . . . . . . . . . . . . . . 118
6.10 Schematic representation as well as data, posterior median solutions,
and 95% credible intervals for phosphorylated and total STAT5 in the
cytoplasm. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 120
6.11 Copula data and density plot of the (˘u2, ˘u7) copula data pair of the
JAK2-STAT5 model. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 123
6.12 Results for the JAK2-STAT5 model. . . . . . . . . . . . . . . . . . . . . 125
7.1
Schematic representation of the JAK1-STAT3 pathway models. . . . . . 130
7.2
Data as well as posterior median solutions and 95% credible intervals for
the JAK1-STAT3 pathway.
. . . . . . . . . . . . . . . . . . . . . . . . . 132
7.3
Pairwise density plots for all posterior parameter-pairs of the ﬁrst JAK1-
STAT3 pathway model.
. . . . . . . . . . . . . . . . . . . . . . . . . . . 133
8.1
Schematic representation of the ICRP and HMGU models.
. . . . . . . 140
8.2
Plasma and urine data for investigations 1-16 on log-log-scale. . . . . . . 146
8.3
Pairwise density plots for all parameter-pairs of the HMGU posterior.
. 148
8.4
Pairwise density plots for all parameter-pairs of the ICRP posterior. . . 149
8.5
Posterior median solutions and 95% credible intervals for the ICRP and
HMGU models. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 153
8.6
Posterior median as well as 90% credible intervals for bone retention of
95Zr according to the ICRP and HMGU models.
. . . . . . . . . . . . . 154
A.1 Various univariate distributions.
. . . . . . . . . . . . . . . . . . . . . . 162
B.1
Various copula density functions. . . . . . . . . . . . . . . . . . . . . . . 165
B.2
Various rotated copula density functions.
. . . . . . . . . . . . . . . . . 166
xiv

List of Tables
6.1
Sampling results and residual diﬀerences between the average posterior
mean, standard deviation, and correlation coeﬃcient estimates for the
strongly correlated bivariate normal distribution. . . . . . . . . . . . . . 111
6.2
Sampling results and average posterior mean estimates for the steady
state model. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 112
6.3
Sampling results and estimated marginal posterior means, modes, and
90% posterior quantile based credible intervals for the small compart-
ment model. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 119
6.4
Estimated marginal posterior means, modes, and 90% posterior quantile
based credible intervals of the JAK2-STAT5 pathway model.
. . . . . . 124
6.5
Sampling results for the JAK2-STAT5 pathway model. . . . . . . . . . . 125
7.1
Estimated marginal posterior means, modes, and 90% posterior quantile
based credible intervals of the JAK1-STAT3 pathway model.
. . . . . . 135
8.1
Overview of priors for the zirconium models.
. . . . . . . . . . . . . . . 145
8.2
Bayes factors and sampling results for the HMGU versus the ICRP model
based on plasma and urine data.
. . . . . . . . . . . . . . . . . . . . . . 151
8.3
Bayes factors and sampling results for the HMGU versus the ICRP model
based on plasma data. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 151
8.4
Bayes factors and sampling results for the HMGU versus the ICRP model
based on urine data. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 152
8.5
Posterior median and according 95% quantile based credible intervals for
the HMGU parameters.
. . . . . . . . . . . . . . . . . . . . . . . . . . . 153
8.6
Retrospective urine predictions for the HMGU model. . . . . . . . . . . 155
xv

LIST OF TABLES
B.1
A selection of Archimedean copulas.
. . . . . . . . . . . . . . . . . . . . 164
xvi

1
Introduction
Dynamical systems are used in various ﬁelds of science. Frequently modeled by para-
metrized ordinary or delay diﬀerential equations they ﬁnd application in physics, engi-
neering, computational biology, and many more. In systems biology diﬀerential equa-
tion driven dynamical systems are commonly used to study the evolution and mainte-
nance of cellular functionality over time. Here, scarce and noisy data for the mostly
large and complex models hamper the unraveling of molecular interaction mechanisms.
However, determining the underlying reaction rate parameters is crucial for profound
model based predictions. Extensive research has therefore been done on the inference
of these systems. The issue is typically addressed by initial value or maximum like-
lihood approaches (Horbelt et al. [2002]), which yield the supposedly most probable
systematic parameter values. However, as the data and models are generally impre-
cise, determining single best values is often inadequate.
In the last few years fully
statistical Bayesian approaches were considered for parameter estimation in diﬀerential
equation systems (Brown & Sethna [2003]; Lawrence et al. [2010]; Wilkinson [2006]).
These Bayesian methods provide a nice way of combining the parameters of interest
with the underlying data and a priori information by means of a posterior distribution,
even when dealing with very complex models or partially unobserved quantities. More-
over they can readily be applied for model selection purposes and provide an overall
model inference scheme that naturally corrects for overﬁtting – an issue classical model
selection approaches are subject to.
1

1. INTRODUCTION
Figure 1.1: Posterior distribution of the JAK1-STAT3 model (7.1) introduced in Chapter
7 marginalized on the k1 and k6 dimension.
As analytic inference of the posterior distributions quickly becomes intractable, Markov
Chain Monte Carlo (MCMC) methods are nowadays widely used to tackle the problem
(Brooks [1998]; Gamerman & Lopes [2006]). Despite of their computationally costly na-
ture MCMC methods have of late drawn major interest in the scientiﬁc community. One
of the most successful and inﬂuential (Beichl & Sullivan [2000]; Wilkinson [2007]) algo-
rithms for Markov chain sampling was developed by Metropolis and Hastings (Hastings
[1970]; Metropolis et al. [1953]). It can draw samples from any probability distribu-
tion, given that a function proportional to the according probability density function
is available. Throughout the sampling process a random walk proposal function based
on the current Markov chain sample is used to generate a possible subsequent Markov
chain candidate.
Fine-tuning the Metropolis-Hastings (MH) algorithm for performing eﬃcient posterior
inference is nevertheless a daunting task: It is easy to see that an eﬃcient MH pro-
posal function should ideally be (at least locally) very similar to the actual posterior
distribution of interest. Figure 1.1 shows the ”banana”-shaped posterior distribution
2

of the JAK1-STAT3 model (7.1) introduced in Chapter 7 marginalized on two dimen-
sions (i.e. for the parameters k1 and k6). Such spiky, complex distributions are hard to
mimic using a globally constant random walk proposal scheme as applied in the classical
MH algorithm. More generally, strong parameter dependencies and complex sampling
spaces limit MH algorithms to conservative parameter update schemes (Ramsay et al.
[2007]). In other words, vast traversals in the parameter space call for huge amounts
of MCMC iterations in situations with complex sampling spaces. Towards this end,
a variety of algorithms based on techniques from mathematical optimization theory
have been developed by Duane et al. [1987], Girolami & Calderhead [2011], Roberts &
Stramer [2002], or Ter Braak & Vrugt [2008] in order to improve the MCMC sampling
eﬃciency. These approaches propose Markov chain candidates using local information
about the posterior manifold structure and with this attain higher proposal acceptance
rates compared to the classical random walk MH algorithm. Another Ansatz that has
of late drawn a lot of interest in the MCMC community is based on the successive
adaption of the MH proposal function in order to amplify the sampling eﬃciency. Var-
ious algorithms were proposed by Haario et al. [1999], Haario et al. [2001], Roberts &
Rosenthal [2007], Holden et al. [2009], or M¨uller & Sbalzarini [2010]. This can speed
up the inference process severely as pointed out by Rosenthal [2011], and Gilks et al.
[1998].
In this thesis we extend the classical MH algorithm by a novel estimated MH proposal
function which generates Markov chain candidates almost independent of the current
Markov chain state while taking into account the full estimated parameter dependency
structure. This results in a proposal function that is ideally close to the actual posterior
distribution and is therefore able to eﬃciently sample the often times complex distri-
butions of the dynamical systems mentioned above. The sampling scheme is based on
mimicking the posterior distribution by means of a D-vine copula decomposition. Up-
dating the proposal copula during the sampling process leads to an adaptive sampling
scheme. We employ this approach for the inference of two biological systems: ﬁrst of
all we analyze a model of the JAK1-STAT3 signaling pathway in order to investigate
the eﬀectiveness of tyrosine-phosphorylated STAT3 homodimers working as transcrip-
tion factors for gene regulation. Secondly, we infer a biokinetic model for zirconium
3

1. INTRODUCTION
processing in the human body, which can readily be used to derive limiting values of
detrimental eﬀects in radiation protection.
Overview of this thesis
In Chapter 2 we ﬁrst introduce some basic notions and notations from probability
theory used throughout this thesis. Then the concept of vine copula decomposition
of probability densities is outlined. In addition, we shortly summarize the theory of
random processes and Markov chains in particular. We focus on the important aspects
regarding Markov Chain Monte Carlo methods.
Moreover, a short introduction on
biological signaling pathways and dynamical systems is given.
Chapter 3 considers the concept of Bayesian model inference. Here, Bayes’ theorem
yields posterior distributions that are proportional to the likelihood in combination with
prior distributions for a series of given observations. More precisely, Bayesian model
inference comprises two main aspects: On the one hand we introduce Bayes factors,
which are capable of inferring the best model structure for a given set of parametrized
models, i.e. we deduce the model M with the highest probability that the observations
were generated according to M.
For this model selection task various approaches,
such as the prior arithmetic mean estimate, the posterior harmonic mean estimate,
and thermodynamic integration are presented. On the other hand the second strain
of Bayesian model inference is treated: Given a speciﬁc parametrized model M, we
summarize the concept of posterior parameter inference.
Drawing samples from a posterior distribution is essential for Bayesian inference. As the
posteriors are generally non-standard distributions, Chapter 4 addresses the concept
of Markov Chain Monte Carlo (MCMC) sampling. Interestingly, both Bayesian model
inference aspects can be simultaneously covered by MCMC sampling. We introduce the
basic version of the Metropolis-Hastings sampling scheme (Hastings [1970]; Metropolis
et al. [1953]) and discuss dependency and convergence diagnostics of the generated
Markov chains. The latter ﬁnd use in all applications throughout this thesis. Finally,
extensions to optimization problems via simulated annealing (Kirkpatrick et al. [1983])
and the direct application to the model selection issue via reversible jump MCMC
(Green [1995]) are given.
4

Due to the aforementioned complex posterior surfaces MCMC approaches often struggle
with sampling eﬃciency. Especially the inference of parametrized diﬀerential equations
likes to trap MCMC samplers between high proposal rejection rates and strong autocor-
relation structures – both leading to a low number of independent samples drawn over
time. In Chapter 5 we review two current approaches addressing the issue of improv-
ing the Metropolis-Hastings proposal function. Considered are a successive proposal
function adaption scheme (M¨uller & Sbalzarini [2010]) and the simpliﬁed Riemann Man-
ifold Metropolis adjusted Langevin algorithm (Girolami & Calderhead [2011]), which
exploits the geometric posterior parameter structure for proposal generation.
In Chapter 6 we develop a novel vine copula based (adaptive) MCMC approach for
eﬃcient parameter inference in complex dynamic systems. Although copulas are well
established in various ﬁelds of science, we are the ﬁrst to exploit this concept for ﬁne
tuning the Metropolis-Hastings algorithm. Copulas are capable of handling asymmetric
dependency structures (Aas et al. [2009]; Kurowicka & Cooke [2006a]; Kurowicka & Joe
[2011]) – a characteristic also inherent to most posterior densities subject to MCMC
sampling. The basic version of our novel hybrid Copula based Independence/random
walk Metropolis-Hastings algorithm (CIMH) is presented and subsequently extend to
an adaptive sampling scheme (ACIMH). This is the ﬁrst major contribution of this
thesis. The chapter is based on Schmidl et al. [2012a] and in part even identical.
In Chapter 7 we apply ACIMH to infer a model of the JAK1-STAT3 signaling pathway.
Using thermodynamic integration we analyze the eﬀect of direct tyrosine-phosphorylated
STAT3 dimer import into the nucleus as compared to a model considering tyrosine-
serine-phosphorylated STAT3 dimer import only. The estimated maximum a posteriori
rate constants of the JAK1-STAT3 pathway are provided.
We conclude with the second major contribution of this thesis by analyzing a model for
zirconium processing in the human body in Chapter 8. Again, using thermodynamic
integration, this time in combination with CIMH, we compare a biokinetic model re-
cently put forward by Greiter et al. [2011] to the model established by the International
Commission on Radiological Protection. The latter is currently used for radiation pro-
tection. The former turns out to be superior based on in vivo plasma and urine data of
16 investigations in humans. Transfer rates (including credible intervals) for an average
5

1. INTRODUCTION
individual are given. We also provide an estimation of initially ingested amounts of zir-
conium for ex post measurements, which is crucial for determining detrimental eﬀects
at occupational exposure. Furthermore, zirconium retention in bone is analyzed. The
chapter is based on Schmidl et al. [2012b] and in part even identical.
Main scientiﬁc contributions
The main scientiﬁc contributions of this thesis are (i) the development of a novel vine
copula based (adaptive) MCMC approach for eﬃcient parameter inference in complex
dynamic systems and (ii) the in-depth analysis of a model for zirconium processing in
the human body. The latter includes the estimation of initially ingested amounts of zir-
conium for ex post measurements and zirconium retention in bone. These contributions
are contained in the following two manuscripts:
• D. Schmidl, C. Czado, and F.J. Theis. A vine-copula based adaptive MCMC
sampler for eﬃcient inference of dynamical systems. Under revision at Bayesian
Analysis.
• D. Schmidl, S. Hug, W.B. Li, M.B. Greiter, and F.J. Theis.
Bayesian model
selection validates a biokinetic model for zirconium processing in humans. BMC
Systems Biology, 6(95), 2012.
Further scientiﬁc contributions
The following publications are not contained in this thesis. They present the results of
various collaborations and projects in computational systems biology.
• D.M. Wittmann, D. Schmidl, F. Bl¨ochl, and F.J. Theis. Reconstruction of graphs
based on random walks. Journal of Theoretical Computer Science, 410 (38-40),
2009.
• A. Ruepp, A. Kowarsch, D. Schmidl, F. Buggenthin, B. Brauner, I. Dunger, G.
Fobo, G. Frishman, C. Montrone, and F.J. Theis. PhenomiR: a knowledgebase
for microRNA expression in diseases and biological processes. Genome Biology
11(1):R6, 2010.
6

• A. Kowarsch, C. Marr, D. Schmidl, A. Ruepp, and F.J. Theis. Tissue-speciﬁc
target analysis of disease-associated microRNAs in human signaling pathways.
PLoS one, 5(6), 2010.
• A. Kowarsch, D. Schmidl, S. Braun, S. Bohl, R. Merkle, U. Klingm¨uller, and F.J.
Theis. MicroRNA-mediated regulation has an impact on the dynamic behavior
of the JAK-STAT pathway. Manuscript in preparation.
7

1. INTRODUCTION
8

2
Prerequisites
The following chapter holds the basic notions and notations from probability theory,
the theory of Markov chains, copula constructions, and dynamical systems. These will
be used in subsequent chapters.
2.1
Basics on probability theory
We start with the introduction of basics on probability theory, mostly following Chap-
ter 1.3 of Theis [2002] with some extensions. For proofs see e.g. the book of Klenke
[2008]. Throughout this thesis we denote vectors/matrices by bold letters, while non
bold letters with subscript indices denote vector/matrix elements. Markov chains are
displayed as sets, such as {X(t)}t∈I for some index set I, where the superscript (t)
denotes the tth element.
Deﬁnition 2.1 (Probability space). A probability space is a triplet (Ω, F, P) consisting
of a non-empty set Ω, a sigma-algebra F on Ωand a probability measure P on F with
P(Ω) = 1.
The sets A ∈F are called events, while P(A) is called the probability of the event A.
By deﬁnition we have P(A) ∈[0, 1]. An event A ∈F is said to occur almost surely, if
the probability of A to not occur is zero.
9

2. PREREQUISITES
Deﬁnition 2.2 (Random variable/random vector). Suppose (Ω, F, P) is a probability
space and (E′, E′) a measurable space. Suppose furthermore X : Ω−→E′ is a measur-
able mapping. Then X is called a random variable with values in E′. An n-dimensional
random vector with values in E is a vector-valued function X = (X1, . . . , Xn)⊤:
Ω−→E into a measurable space (E, E) whose components Xi are random variables
on (Ω, F, P).
For each ω ∈Ωwe call x = X(ω) ∈E a realization of X. As we allow n = 1, we will
further on speak of random vectors only. In our applications (E, E) = (Rn, B(Rn)),
where B(Rn) denotes the Borel sigma algebra on Rn. In this case we call X a real-
valued random vector or simply random vector.
For A ∈E we write X(P)(A) :=
P(X−1(A)) := P(X ∈A) := PX(A). This is a probability measure X(P) : E −→
[0, 1], A 7−→PX(A), since PX(E) = P(Ω) = 1. The function X(P) is called distribution
of X with respect to P. We write X ∼PX and call X to be PX distributed.
For a ﬁnite sequence X(1), . . . , X(T) of T ∈N random vectors on (Ω, F, P) with values
in (E, E) we deﬁne the function X(1) ⊗. . . ⊗X(T) by
X(1) ⊗. . . ⊗X(T) : Ω−→E × . . . × E
ω 7−→(X(1)(ω)⊤, . . . , X(T)(ω)⊤)⊤.
The function X(1) ⊗. . . ⊗X(T) also constitutes a random vector called product ran-
dom vector of X(1), . . . , X(T). The according distribution PX(1)⊗...⊗X(T ) is called joint
(product) distribution of X(1), . . . , X(T).
Deﬁnition 2.3 (Distribution function). For a real-valued random vector X : Ω−→Rn
on (Ω, F, P) the function
FX : Rn −→[0, 1]
(x1, . . . , xn)⊤7−→PX((−∞, x1]) × . . . × (−∞, xn])
is called distribution function of X with respect to P, or likewise, cumulative distribution
function (cdf) of X.
The opposite direction also holds true, which allows us to identify a real-valued random
vector with its distribution:
10

2.1 Basics on probability theory
Proposition 2.1. For each distribution function F there exists a real-valued random
vector X with FX = F.
Deﬁnition 2.4 (Density function). If f : Rn −→[0, ∞) is a non-negative Lebesgue-
integrable function, such that the distribution function FX : Rn −→[0, 1] with respect
to P can be written as
FX(x1, . . . , xn) =
Z x1
−∞
. . .
Z xn
−∞
f(u1, . . . , un) dun . . . du1
then f is called density function with respect to P, or likewise, probability density func-
tion (pdf) of X. We also write fX for f.
Clearly, if a distribution function F is suﬃciently diﬀerentiable at a point (x1, . . . , xn)⊤∈
Rn, the according probability density function is given by
f(x1, . . . , xn) =
∂n
∂x1 . . . ∂xn
F(x1, . . . , xn).
(2.1)
In this situation we also write X ∼f(x) instead of X ∼PX.
Following are two very important examples of distributions: (i) the uniform distribution
will play a key role when dealing with copula densities as their margins are uniformly
distributed random variates and (ii) the normal distribution, which is a common pro-
posal density for the prominent Metropolis-Hastings algorithm deﬁned in Chapter 4.1.
A selection of important univariate density functions is given in Appendix A.
Example 2.1 (Uniform distribution). Suppose A ⊂Rn is a non-empty measurable set
and 1A the indicator function on A, this is
1A : Rn −→R
x −→



1, if x ∈A
0, otherwise.
A random vector X : Ω−→Rn is called uniformly distributed on A, if for the n-
dimensional Lebesgue measure λn the density function
fX(x) =
1
λn(A)1A(x)
exists. We then write X ∼U[A].
11

2. PREREQUISITES
Example 2.2 (Normal distribution). A random vector X : Ω−→Rn is said to be
normally distributed (or Gaussian), if there exists a vector µ ∈Rn along with a positive-
semideﬁnite symmetric matrix Σ ∈Mat(n × n, R), such that the density function
fX(x) =
1
p
(2π)n det (Σ)
exp

−1
2(x −µ)⊤Σ−1(x −µ)

exists. We then write X ∼Nn(µ, Σ). The quantities µ and Σ are called mean and
covariance matrix of Nn(µ, Σ), respectively. An example of a bivariate normal density
function can be seen in Figure 2.1(b).
Heading towards Markov chains, we need the notions of marginal and conditional dis-
tribution functions. While the former averages over a subset of random variables of
X, the latter ﬁxes this subset to a speciﬁc value and in some sense cuts through the
graph of the joint probability distribution function along this value. More precisely:
Suppose we are given a random vector X = (X1, . . . , Xn)⊤: Ω−→Rn together with
its density function fX : Rn −→R. Setting Y := (Xn1 . . . , Xnk)⊤for a non-empty
subset {n1, . . . , nk} ⊊{1, . . . , n}, then for {n′
1, . . . , n′
l} := {1, . . . , n} \ {n1, . . . , nk}, the
vector Y is a random vector with distribution function
FY (xn1 . . . , xnk) =
Z xn1
−∞
. . .
Z xnk
−∞
Z ∞
−∞
. . .
Z ∞
−∞
fX(u1, . . . , un) dun′
l . . . dun′
1 dunk . . . dun1.
The function FY is said to be the marginal distribution function of Y . Assuming (after
variable permutation) without loss of generality that {n1, . . . , nk} = {1, . . . , k} and
deﬁning the complement of Y with respect to X as Z = (Xk+1, . . . , Xn)⊤then the
marginal density function of Y is given by
fY (x1 . . . , xk) =
Z ∞
−∞
. . .
Z ∞
−∞
fX(x1, . . . , xk, uk+1, . . . , un) dun . . . duk+1.
We can now turn to conditional distribution functions.
Deﬁnition 2.5 (Conditional distribution function). Using the deﬁnitions from above
the conditional distribution function of Y = (X1, . . . , Xk)⊤given Z = (xk+1, . . . , xn)⊤
for some xk+1, . . . , xn ∈R is for fZ(xk+1 . . . , xn) > 0 given by
FY |Z(x1, . . . , xk|xk+1, . . . , xn) =
Z x1
−∞
. . .
Z xk
−∞
fX(u1, . . . , uk, xk+1, . . . , xn)
fZ(xk+1 . . . , xn)
duk . . . du1.
12

2.1 Basics on probability theory
Deﬁnition 2.6 (Conditional density function). The conditional density function of Y
given Z is for fZ(xk+1 . . . , xn) > 0 given by
fY |Z(x1, . . . , xk|xk+1, . . . , xn) = fX(x1, . . . , xn)
fZ(xk+1 . . . , xn).
Suppose a, b ∈Rn and A is the open n-dimensional cube with lower limits a and upper
limits b, i.e A = (a1, b1) × . . . × (an, bn). Then P(X ∈A) computes to
P(X ∈A) =
Z b1
a1
. . .
Z bn
an
f(u1, . . . , un) dun, . . . , du1.
With this the notion of a conditional distribution function generalizes in the sense of
the Borel measure in the usual way for all measurable sets A ⊂Rn that are non-
cubic (such as unions of n-dimensional cubes). This gives a sound deﬁnition for the
probability P(Y ∈B|xn′
1, . . . , xn′
l) for any set B ∈B(Rk). For these we also write
P(Y ∈B|Xn′
1 = xn′
1, . . . , Xn′
l = xn′
l) or P(B|xn′
1, . . . , xn′
l).
In order to simplify notation later on we also write for Y = (Xn1 . . . , Xnk)⊤and
Z = (Xn′
1 . . . , Xn′
l)⊤
Fn1...nk, fn1...nk, Fn1...nk|n′
1...n′
l, and fn1...nk|n′
1...n′
l
instead of
FY , fY , FY |Z, and fY |Z,
respectively.
Deﬁnition 2.7 (Independent random vector). Let X(1), . . . , X(T) be a ﬁnite sequence
of random vectors on some probability space (Ω, F, P). Let furthermore FX(1)⊗...⊗X(T )(·)
be the distribution function of X(1) ⊗. . . ⊗X(T). Then X(1), . . . , X(T) are called in-
dependent, if
FX(1)⊗...⊗X(T )(x(1), . . . , x(T)) =
T
Y
t=1
FX(t)(x(t)).
In Markov Chain Monte Carlo methods a sequence of realizations of random vectors is
drawn in order to approximate a distribution PX for some random vector X. The law
of large numbers will lay the basis for this procedure. We therefore deﬁne:
13

2. PREREQUISITES
Deﬁnition 2.8 (Expectation). For an integrable random vector X : Ω−→Rn on a
probability space (Ω, F, P)
EPX[X] =
Z
Rn x PX
is referred to as the expectation or mean of X.
In case there exists a density function f(x) corresponding to PX, we also write Ef(x)[X]
instead of EPX[X], or, where clear without ambiguity, simply E[X]. The expectation
behaves nicely with respect to the product of independent random vectors:
Proposition 2.2. For a sequence of independent integrable random vectors X(1), . . . , X(T)
the component-wise product QT
j=1 X(j) is integrable and
E


T
Y
j=1
X(j)

=
T
Y
j=1
E[X(j)].
We denote a series of realizations x(1), . . . , x(T) from the very same random vector X
to be independent identically distributed ( i.i.d.) of the distribution PX. This leads us
to:
Theorem 2.1 (Strong law of large numbers). For an i.i.d. sequence x(1), . . . , x(T) of
realizations of X
lim
T→∞
1
T
T
X
i=1
(x(i) −E[X]) = lim
T→∞
 
1
T
T
X
i=1
x(i)
!
−E[X] = 0 ∈Rn
almost surely.
This shows that the expected mean
¯x = 1
T
T
X
i=1
x(i)
of a set of i.i.d. realizations x(1), . . . , x(T) truly converges to the expectation of X. The
approximation of E[X] by ¯x gets better as T increases.
For a random vector X an unbiased approximation for the variance
Var[X] = E

(X −E[X])2
,
14

2.2 Copula distributions
if existent, is given by the expected variance
sx =
1
T −1
T
X
i=1
(x(i) −¯x)2,
where the square is taken component-wise.
For two random vectors X = (X1, . . . , Xn)⊤and Y = (Y1, . . . , Yn)⊤with existing
variance the covariance cov[X, Y ] is deﬁned by the matrix
cov[X, Y ] = (covi,j)i,j=1,...,n
with
covi,j = E[(Xi −E[Xi])(Yj −E[Yj])].
Moreover, Pearson’s correlation matrix of X and Y is given by
corr[X, Y ] = (corri,j)i,j=1,...,n
with
corri,j =
cov[Xi, Xj]
p
Var[Xi] ·
p
Var[Yj]
.
Pearson’s correlation matrix contains the linear dependence between X and Y . Since
the dependence between X and Y can also be nonlinear, we also deﬁne a rank-based
dependence measure: For two random vectors X and Y , Kendall’s τ is given by the
component-wise diﬀerence of the probability of concordance and the probability of
discordance of X and Y , this is
τ[X, Y ] = (τi,j)i,j=1,...,n
with
τi,j = P((Xi −X′
i)(Yj −Y ′
j ) ≥0) −P((Xi −X′
i)(Yj −Y ′
j ) < 0)
for the joint probability P of X and Y and two independent identically distributed
copies X′ of X and Y ′ of Y .
Lastly, in order to compare the distance between two distributions PX and PY for two
random vectors X and Y with values in (E, E), we deﬁne the total variation norm as
∥PX −PY ∥TV = sup
A∈E
|PX(A) −PY (A)|.
2.2
Copula distributions
We now turn to the task of characterizing a continuous n-variate distribution function
F(x) with given margins by its according univariate pdf’s f(xi), i = 1, . . . , n, and
a second n-variate distribution function C(u) with uniformly distributed margins on
15

2. PREREQUISITES
[0, 1], called copula (see below for a rigorous deﬁnition). This concept allows for eﬃcient
modeling of multivariate distributions with asymmetric tail dependencies. It splits up
F(x) into one part containing the dependency structure and another one containing
all marginal informations on the according random vector X.
Copulas have been
successfully applied in the ﬁelds of economics, ﬁnance, or geology.
For a thorough
introduction on the theory (including proofs) and applications see for example Joe
[1997] or Nelsen [2006].
Deﬁnition 2.9 (Copula). A function C : [0, 1]n −→[0, 1] is called n-dimensional
copula, if the following properties hold:
(i) C(u) = 0 for all u = (u1, . . . , un)⊤∈[0, 1]n with ui = 0 for some i ∈{1, . . . , n}.
(ii) C(u) = ui for all u = (u1, . . . , un)⊤∈[0, 1]n with uj = 1 for all i ̸= j.
(iii) C(u) is n-increasing, i.e. for all cubes A = ×n
i=1[ai, bi] ⊆[0, 1]n the volume of A
with respect to C is non-negative:
VC(A) :=
X
u∈×n
i=1{ai,bi}
sgn(u)C(u) ≥0.
Here, sgn is a sign function with value one, if ui = ai for an even number of i’s
and minus one for an odd number of i’s.
The following fundamental theorem by Sklar [1959] uniquely links multivariate cdf’s
on R
n with its univariate margins by means of a copula.
Here, R
n denotes the n-
dimensional cartesian product of R := R ∪{−∞, ∞}.
Theorem 2.2 (Sklar). Suppose F is an n-dimensional distribution function with con-
tinuous univariate margins F1, . . . , Fn. Then there exists a unique copula C, such that
for all x = (x1, . . . , xn)⊤∈R
n
F(x) = C(F1(x1), . . . , Fn(xn)).
(2.2)
Conversely, for any copula C and univariate distribution functions F1, . . . , Fn the func-
tion F deﬁned by Equation (2.2) is a multivariate distribution function with margins
F1, . . . , Fn.
16

2.2 Copula distributions
The relation (2.2) furthermore deﬁnes the density of a copula C: Suppose the necessary
derivatives exist, then the chain rule yields
f(x) = ∂nC(F1(x1), . . . , Fn(xn))
∂x1 . . . ∂xn
= ∂nC(F1(x1), . . . , Fn(xn))
∂F1(x1) . . . ∂Fn(xn)
n
Y
i=1
fi(xi)
(2.3)
at some point x = (x1, . . . , xn)⊤∈Rn. As suggested by Equation (2.1) we set
c(F1(x1), . . . , Fn(xn)) := ∂nC(F1(x1), . . . , Fn(xn))
∂F1(x1) . . . ∂Fn(xn)
.
(2.4)
Hence, every probability density function f can be decomposed as product
f(x) = c(F1(x1), . . . , Fn(xn)) · f1(x1) · . . . · fn(xn).
(2.5)
for the corresponding copula density c to f. For strictly positive marginal distribution
functions f1, . . . , fn
c(F1(x1), . . . , Fn(xn)) =
f(x)
f1(x1) · . . . · fn(xn).
The copula density thus contains the dependency structure, while the marginal infor-
mation is “divided” out.
Sklar’s theorem also yields a natural recipe for the construction of copulas based on dis-
tribution functions F with invertible margins F1, . . . , Fn: we set for u = (u1, . . . , un)⊤∈
[0, 1]n
C(u) = F(F −1
1 (u1), . . . , F −1
n (un)).
(2.6)
This method is called inversion method (Nelsen [2006]) and lays the basis for sampling
from copula distributions. In order to familiarize the reader with the concept of copulas
we consider two very prominent classes of multivariate copula densities: these are for
one Archimedean and for the other elliptical copulas.
Theorem 2.3 (Archimedean copula). Suppose ϕ : [0, 1] −→[0, ∞] is a continuous
strictly decreasing function with ϕ(0) = ∞and ϕ(1) = 0. Suppose furthermore that
the inverse ϕ−1 of ϕ is completely monotonic, i.e. ϕ−1 is continuous and satisﬁes
(−1)k dk
dxk ϕ−1(x) ≥0 for all x ∈(0, ∞) and all k ∈N0. Then
C(u) = ϕ−1(ϕ(u1) + . . . + ϕ(un))
is a copula.
17

2. PREREQUISITES
The copula C deﬁned by Theorem 2.3 is called Archimedean copula. The function ϕ is
said to be the generator of C. A proof of Theorem 2.3 is given in Nelsen [2006].
Example 2.3 (Independence copula). The independence copula is deﬁned as
CI : [0, 1]n −→[0, 1]
(u1, . . . , un) 7−→
n
Y
i=1
ui.
It is an Archimedean copula with generator ϕ(x) = −log(x). A plot of a bivariate
independence copula density function is shown in 2.1(a). The term “independence”
originates from the corresponding copula density function cI: For any multivariate
continuous distribution function F with according density function f Equation (2.4)
yields
cI(F1(x1), . . . , Fn(xn)) = ∂nCI(F1(x1), . . . , Fn(xn))
∂F1(x1) . . . ∂Fn(xn)
= 1.
Equation (2.5) therefore gives
f(x) =
n
Y
i=1
fi(xi).
This means the corresponding random variables X1, . . . , Xn are independent.
The inversion method of Equation (2.6) yields the class of elliptical copulas for elliptical
density functions f(x), this is
f(x) =
cn
p
det(Σ)
g((x −µ)⊤Σ−1(x −µ))
for some constant cn ∈R, a univariate function g, a mean vector µ ∈Rn, and covariance
matrix Σ ∈Rn×n.
Example 2.4 (Gaussian copula). Suppose f(x) is the n-variate Gaussian density func-
tion from Example 2.2 with mean vector µ and covariance matrix Σ. For the corre-
sponding correlation matrix S the Gaussian copula is given by
C(u) = ΦS(Φ−1(u1), . . . , Φ−1(un)),
where Φ−1 is the inverse cdf of the univariate standard normal distribution N(0, 1) and
ΦS the n-variate normal cdf with covariance matrix S. The density is given by
c(u) =
1
p
det(S)
exp

−1
2x⊤(S−1 −In)x

18

2.2 Copula distributions
0
0.5
1
0
0.5
1
0
1
2
u1
u2
(a)
−2
0
2
4
−2
0
2
4
0.05
0.1
x1
x2
(b)
0
0.5
1
0
0.5
1
1
2
3
u1
u2
(c)
Figure 2.1: (a) Independence copula density function. (b) Correlated Gaussian density
function with correlation parameter ρ = 0.5. (c) Gaussian copula density function for the
correlated Gaussian density from (b).
with x := (Φ−1(u1), . . . , Φ−1(un))⊤for u = (u1, . . . , un)⊤∈[0, 1]n and the n-dimensional
identity matrix In (Arbenz [2011]). A plot of a bivariate Gaussian density with
µ =
 
1
1
!
and
Σ =
 
1
p
3/4
p
3/4
3
!
is shown in Figure 2.1(b). Here, the according correlation parameter ρ = 0.5. The
corresponding copula density function is depicted in Figure 2.1(c).
Although there are multivariate copulas that are neither Archimedean, nor elliptic,
the task of ﬁtting a parametrized1 multivariate copula to some vector of observations
can be daunting. In the following we introduce a more ﬂexible approach for ﬁtting
parametrized copulas to a given vector of observations.
2.2.1
Pair copula decomposition
The class of classical multivariate copulas has been considerably extended by Joe [1996].
Joe showed that a decomposition involving only bivariate copula densities and marginal
densities provides a valid multivariate density.
We follow Aas et al. [2009] for the
introduction of these pair copula decompositions.
1We call a copula parametrized, if it depends on some parameter vector θ i.e. C(·) = C(·|θ).
19

2. PREREQUISITES
Suppose X = (X1, . . . , Xn)⊤is a random vector with distribution function F(x1, . . . , xn)
and probability density function f(x1, . . . , xn). Then, except for permutation of the
variables, we are given the unique decomposition
f(x1, . . . , xn) = f(xn) · f(xn−1|xn) · f(xn−2|xn−1, xn) · . . . · f(x1|x2, . . . , xn),
(2.7)
for the respective conditioned distribution functions F(·|·) and density functions f(·|·).
We iteratively derive a pair copula decomposition of f starting at n = 2: Using Sklar’s
theorem (Theorem 2.2), there exists a unique bivariate copula density function c1,2,
such that
f(x1, x2) = c1,2(F1(x1), F2(x2)) · f1(x1) · f2(x2).
Thus, it follows from Equation (2.7) that, using c1,2, the conditional density f(x1|x2)
can be written as
f(x1|x2) = c1,2(F1(x1), F2(x2)) · f1(x1).
(2.8)
For n = 3 we consider the conditional density function f(x1, x2|x3) of f(x1, x2, x3).
Again, by Sklar’s theorem there exists a copula density function c1,2|3, such that
f(x1|x2, x3) = c1,3|2(F1|2(x1|x2), F3|2(x3|x2)|x2) · f(x1|x2).
(2.9)
We already see that this decomposition is not unique as there also exists a copula
density function c1,2|3 with
f(x1|x2, x3) = c1,2|3(F1|3(x1|x3), F2|3(x2|x3)|x3) · f(x1|x3).
(2.10)
Nevertheless, using Equation (2.9) in combination with (2.8)
f(x1|x2, x3) = c1,3|2(F1|2(x1|x2), F3|2(x3|x2)|x2) · c1,2(F1(x1), F2(x2)) · f1(x1).
For n > 3 we now see that the decomposition of the conditional density function
f(xt|xt+1, . . . , xn) is for t ≤n −2 given by
f(xt|v) = ct,j|D−j(F(xt|v−j), F(vj|v−j)|v−j) · f(xt|v−j),
(2.11)
where j ∈D for D := {1, . . . , t −1} with D−j := D \ j, and v−j denotes the vector
v = (x1, . . . , xt−1)⊤with missing jth component. This allows us to decompose f into
20

2.2 Copula distributions
the product of a series of bivariate copula density and marginal density functions using
Equation (2.7).
In general the conditional pair copula densities in (2.11) depend on the conditioning
values v−j. However, we will assume the restriction that the ct,j|Dj(·, ·|v−j) do not
depend on v−j. This means that the decomposition (2.11) captures the dependency
on the conditioning values solely through the arguments F(xt|v−j) and F(vj|v−j).
Hobæk Haﬀet al. [2010] showed that this restriction is not severe. In the Gaussian
and multivariate Student-t-case the conditional pair copula densities are independent
of the conditioning values.
Aas et al. [2009] were the ﬁrst to consider standard estimation methods for parameters
of vine copulas such as stepwise and maximum likelihood estimation (MLE). Since we
have an explicit expression for the joint density, the likelihood is easily derived (see
e.g. Aas et al. [2009]). These expressions however involve conditional cdf’s: Joe [1996]
showed that the conditional distribution functions corresponding to Equation (2.11)
can be computed by
F(xt|v) =
∂Ct,j|D−j(F(xt|v−j), F(vj|v−j))
∂F(vj|v−j)
,
(2.12)
where Ct,j|D−j is the copula distribution function corresponding to ct,j|D−j.
There-
fore, the required conditional cdf’s can be computed recursively. Equation (2.12) is
furthermore used for sampling from copulas as shown in Aas et al. [2009].
For bivariate Gaussian copulas as building blocks and Gaussian marginals the resulting
joint density is multivariate Gaussian. Here, the bivariate copula parameters corre-
spond to partial correlations, which can be chosen arbitrarily between -1 and 1 and
still induce a positive deﬁnite correlation matrix (Joe [1996]). Similarly, if one uses
bivariate t-copulas together with a restriction on the degree of freedom parameters for
diﬀerent numbers of conditioning variables, a multivariate t-distribution emerges.
Example 2.5 (Trivariate copula decomposition). Suppose X = (X1, X2, X3)⊤is a nor-
mally distributed random vector with standard normally distributed marginals f1(x1),
f2(x2), f3(x3) and pdf f(x1, x2, x3). Then there exist bivariate Gaussian copula densi-
ties c1,2, c2,3, and c1,3|2 with respective correlation parameters ρ12, ρ23 and ρ13|2 such
21

2. PREREQUISITES
that
f(x1, x2, x3) =c1,3|2(F1|2(x1|x2), F3|2(x3|x2))
· c1,2(F1(x1), F2(x2)) · c2,3(F2(x2), F3(x3)) ·
3
Y
i=1
fi(xi).
(2.13)
Conversely (c.f. Aas et al. [2009]), for given correlation parameters ρ12, ρ23 and ρ13|2,
Equation (2.13) deﬁnes a trivariate normal distribution with correlation matrix
S =



1
ρ12
ρ13
ρ12
1
ρ23
ρ13
ρ23
1


.
Here, ρ13 is given by
ρ13 = ρ13|2
q
(1 −ρ2
12)(1 −ρ2
23) + ρ12ρ23.
In summary, pair copula decomposition constitutes a natural and ﬂexible tool for mod-
eling complex multivariate distribution functions.
Fundamental properties, such as
asymmetric or tail dependencies are broken down to the direct interactions of pairs of
random variables (Joe et al. [2010]). A list of important bivariate copulas is given in
Appendix B.
2.2.2
Vines
We already saw from Equations (2.9) and (2.10) that the pairwise decomposition of
a density function f into the product of bivariate copulas and marginal distributions
is by no means unique.
Bedford & Cooke [2001, 2002] came up with a graphical
representation to classify general pair copula models, called regular vines or R-vines.
Essentially, they are represented by a collection of linked trees. In our applications we
will focus on a subclass of these R-vines, called D-vines. This ﬁxes the decomposition
structure to some extent and by that reduces the number of possible decompositions
severely. However, before we can characterize D-vines, we need some deﬁnitions from
graph theory:
An (undirected) graph is a pair G = (V, E), where for k, k′ ∈N, V = {v1, . . . , vk} is
a set of nodes (also called vertices) and E = {e1, . . . , ek′} is a set of edges, such that
22

2.2 Copula distributions
e = (v, v′) ∈E for some v, v′ ∈V . The order of v and v′ in e is of no importance here,
i.e. we identify e = (v, v′) = (v,′ v). A path P = (V ∗, E∗) in a graph G = (V, E) is a
graph with V ∗= {v∗
1, . . . , v∗
l } ⊆V and E∗= {(v∗
1, v∗
2), (v∗
2, v∗
3), . . . , (v∗l−1, v∗
l )} ⊆E. If
v∗
l = v∗
1, then G is said to be a cycle. A graph G is called acyclic, if it does not contain
cycles. An acyclic graph T = (V, E) is called a tree.
Deﬁnition 2.10 (Regular vine). A regular vine on n ∈N nodes is a collection of (n−1)
trees V = (T1, . . . , Tn−1) such that:
(i) T1 = (V1, E1) has the set of nodes V1 = {v1, . . . , vn}.
(ii) For i = 2, . . . , n −1 the set of nodes of Ti = (Vi, Ei) is given by Vi = Ei−1.
(iii) For i = 2, . . . , n−1 every element (vi, v′
i) ∈Ei consists of two elements (vi−1, v′
i−1)
and (wi−1, w′
i−1) ∈Ei−1 where exactly one of the v’s coincides with one of the
w’s.
Loosely speaking, condition (ii) says that every edge e in a tree Ti becomes a node in the
subsequent tree Ti+1, while condition (iii) states that two adjacent nodes in tree Ti are
connected to a common node in Ti−1. In the following the nodes of tree T1 = (V1, E1)
will correspond to some random vector X = (X1, . . . , Xn)⊤, i.e. vi = Xi for all vi ∈V1.
On the other hand, we depict the edges of tree Ti by an ordered labeling kl|D with k < l
for vk = Xk and vl = Xl. Here, D is the set of conditioning variables, which is ordered
increasingly as well. This results in a unique representation for every R-vine. The
labeling corresponds directly to the set of varying and conditioning variables k, l and D
of the copula ck,l|D. An example of a graphical representation of a six-dimensional R-
vine is shown in Figure 2.2(a). The class of R-vines is suﬃcient to represent pair copula
decompositions. However, this is no longer true for higher dimensional copulas as their
graphical representation can contain loops (see e.g. Diestel [2000] for a deﬁnition).
Since the notion of R-vines does not impose much structure on the number of resulting
decompositions, we later on restrict ourselves to D-vines.
Deﬁnition 2.11 (D-vine). A regular vine is called D-vine, if the degree of each node
v in T1 is at most two, i.e. v is contained in at most two edges of E1.
23

2. PREREQUISITES
For completeness we need to point out another very prominent class of vines: We call a
regular vine a canonical vine or C-vine, if there exists one node v in T1, which is directly
connected to all other nodes. Such a graph is also often called a star. Note that for
i = 2, . . . , n −1 all trees Ti in a C-vine are stars as well. An example of a graphical
representation of a six-dimensional C-vine and a six-dimensional D-vine is depicted
in Figure 2.2(b) and 2.2(c), respectively. While the number of diﬀerent R-vines on n
nodes computes to
 n
2

· (n −2)! · 2(n−2
2 ) (Morales-N´apoles et al. [2010]), the number
of possible C- or D-vines on n nodes is given by n!
2 (Aas et al. [2009]). For n = 6 we
can build 23.040 diﬀerent R-vines and 320 diﬀerent C- or D-vines. Hence, the notion
of C- and D-vines clearly imposes some structure on the pair copula decomposition.
A comprehensive introduction to vines is for instance given in Kurowicka & Cooke
[2006b].
For distinct indices i, j, i1, . . . , ik ∈{1, . . . , n} with k ≤(n −2), i < j and i1 < · · · < ik
we abbreviate
ci,j|i1,...,ik := ci,j|i1,...,ik(F(xi|xi1, . . . , xik), F(xj|xi1, . . . , xik)).
Using the variable order of Equation (2.7) we get the D-vine decomposition
f(x1, . . . , xn) =


n−1
Y
j=1
n−j
Y
i=1
ci,i+j|i+1,...,i+j−1

·
" n
Y
k=1
fk(xk)
#
.
(2.14)
For example, the joint density function of a ﬁve dimensional D-vine is given by
f(x1, . . . , x5) =
" 5
Y
k=1
fk(xk)
#
·c1,2·c2,3·c3,4·c4,5·c1,3|2·c2,4|3·c3,5|4·c1,4|2,3·c2,5|3,4·c1,5|2,3,4.
For a C-vine we on the other hand have the decomposition
f(x1, . . . , xn) =


n−1
Y
j=1
n−j
Y
i=1
cj,i+j|1,...,j−1

·
" n
Y
k=1
fk(xk)
#
.
(2.15)
And the joint density function of a ﬁve dimensional C-vine is given by
f(x1, . . . , x5) =
" 5
Y
k=1
fk(xk)
#
·c1,2·c1,3·c1,4·c1,5·c2,3|1·c2,4|1·c2,5|1·c3,4|1,2·c3,5|1,2·c4,5|1,2,3.
Note that the decompositions (2.14) and (2.15) of the joint density consist of pair copula
densities ci,j|i1,...,ik(·, ·) evaluated at conditonal distribution functions F(xi|xi1, . . . xik)
and F(xj|xi1, . . . , xik) for speciﬁed indices i, j, i1, . . . , ik and marginal densities fk.
24

2.2 Copula distributions
(a)
(b)
(c)
Figure 2.2: (a) Example of an R-vine on six nodes. (b) Example of a D-vine on six nodes.
(c) Example of a C-vine on six nodes.
25

2. PREREQUISITES
2.3
Markov chains
In the following we introduce the concept of Markov chains and address some of their
properties. We do not go into full detail, but rather focus on the important aspects
regarding Markov Chain Monte Carlo (MCMC) methods.
The reader may consult
Wilkinson [2006] for an easily readable and Meyn et al. [1996] for a thorough intro-
duction on the topic. We here take an approach similar to Robert & Casella [2004]
and Tierney [1994]. Loosely speaking a Markov chain is a series of random vectors
in which every element depends on its very last predecessor only. This can be seen
as a generalization of a ﬁrst-order autoregressive process allowing for non-linear de-
pendency functions and non-Gaussian noise. Before we proceed to the deﬁnition of
Markov chains, we introduce the more general concept of random vectors evolving on
some arbitrary non-empty index set I. In later applications I ⊆N0 indexes a series of
model parameters ξ(t) ∈Rn. Throughout this chapter (Ω, F, P) denotes a probability
space, and all random vectors are functions X : Ω−→E onto a measurable space
(E, E) ⊆(Rn, B(Rn)). Without loss of generality we furthermore assume that the den-
sity function for each random vector exists and that it is positive for any realization
x ∈E mentioned.
Deﬁnition 2.12 (Stochastic/random process). Given a probability space (Ω, F, P) to-
gether with the measurable space (E, E), a stochastic (or random) process {X(t)}t∈I on
some index set I is a function
X : Ω× I −→E
(ω, t) 7−→X(t)(ω),
such that the functions
X(t) : Ω−→E
ω 7−→X(t)(ω),
are (F, E)-measurable, i.e. X(t) : Ω−→E are random vectors living on the same
probability space. If I ⊆R, we call {X(t)}t∈I a time-continuous random process and if
I ⊆Z (naturally including I = N) a time-discrete random process.
26

2.3 Markov chains
For a ﬁnite subset If ⊆I the set {x(t)}t∈If is a realization or sample path of {X(t)}t∈I,
if for all t ∈If, x(t) is a realization of X(t).
In our applications the realizations
{x(t)}t∈If consist of samples x(t) of a posterior distribution dependent on some vector
of observations.
Example 2.6 (Time-discrete random process.). Suppose I = N and {X(t)}t∈N are inde-
pendent univariate random vectors with
P(X(t) = +1) = P(X(t) = −1) = 1
2.
Setting X(0) = 0 and Y (k) = Pk
t=1 X(t), the process Y := {Y (k)}k∈N deﬁnes a time-
discrete (and even space-discrete) random process. A realization of {Y (k)}k∈N can be
seen in Figure 2.3(a). For standard normally distributed X(t)’s we get a time-discrete
random process on a continuous sample space. A realization is given in Figure 2.3(b).
Example 2.7 (Time-continuous random process). A random process {W (t)}t∈R+
0 with
W (t) = (W (t)
1 , . . . , W (t)
n )⊤and
(a) W (0) = 0
(b) W (t) is almost surely continuous and
(c) W (t) has independent increments with W (t)
j
−W (s)
j
∼N(0, t −s) (for j = 1, . . . , n
and 0 ≤s < t)
is called a Wiener process or standard Brownian motion. This is a time- and space-
continuous random process. A one dimensional realization is given in Figure 2.3(c).
By prerequisite (c) above each random vector W (t) in a Wiener process depends on
the history of all preceding random vectors W (s) for 0 ≤s < t. On the contrary,
a random process for which the transition probability between diﬀerent states in the
state-space only depends on the current state is termed a Markov process. Mathemati-
cally this means that for the joint distributions PX(0)⊗...⊗X(t+1) of X(0), . . . , X(t+1) and
PX(t)⊗X(t+1) of X(t) and X(t+1) we have
PX(t+1)|X(0)⊗...⊗X(t)(X(t+1) ∈A|x(0), . . . , x(t)) = PX(t+1)|X(t)(X(t+1) ∈A|x(t)),
for any measurable A ⊆E and x(0), . . . , x(t) ∈E.
27

2. PREREQUISITES
0
5
10
15
20
−5
0
5
index t
Y
(a)
0
5
10
15
20
−2
−1
0
1
2
index t
Y
(b)
0
5
10
15
20
−4
−2
0
2
4
time t
Y
(c)
Figure 2.3: (a) Realization of a time-discrete and space-discrete random process. (b)
Realization of a time-discrete and space-continuous random process. (c) Realization of a
Wiener process.
Hence, while ignoring historic events, a Markov process predicts future events solely
based on the current state. Note that the transition probabilities between two states
X(t) and X(t+1) can depend on t ∈I. As our realizations of Markov chain elements
x(t) later on are always drawn from the very same distribution for all t ∈I, we deﬁne:
Deﬁnition 2.13 (Stationary process). Let I = R/Z/N; let furthermore {X(t)}t∈I be
a stochastic process on I. We call {X(t)}t∈I stationary, if for all {t1, . . . , tk} ⊆I and
all τ ∈I the joint distributions of
X(t1+τ), . . . , X(tk+τ)
and
X(t1), . . . , X(tk)
are equal, i.e.
PX(t1+τ )⊗...⊗X(tk+τ )(X(t1+τ), . . . , X(tk+τ)) = PX(t1)⊗...⊗X(tk)(X(t1), . . . , X(tk)).
This means that a shift in time has no eﬀect on the joint statistics of any order. In
other words, the joint distribution is time independent. Markov processes {X(i)}i∈I
on the continuous index set I = R are commonly termed diﬀusion processes. Diﬀusion
processes are an important modeling technique when dealing with single entity pro-
cesses, such as molecule or protein interactions. Here, random eﬀects often determine
the modeling results and therefore call for a stochastic approach (see Dargatz [2010] or
Øksendal [2003] for nice introductions). These are modeled by stochastic diﬀerential
28

2.3 Markov chains
equations of the form
dX(t)
dt
= µ(X(t), t) + Σ(X(t), t)η(t),
(2.16)
where µ : E × I −→Rn and Σ : E × I −→Rn×m are jointly measurable functions
for the n-dimensional random vectors X(t) and m-dimensional Gaussian white noise
η : I −→Rm, such that η(t) i.i.d.
∼Nm(0, 1) ∀t ∈I. While the function µ determines
the so-called systematic (non-stochastic) drift, the Σ-term regulates the strength of the
diﬀusion. Equation (2.16) is also commonly written as
dX(t) = µ(X(t), t)dt + Σ(X(t), t)dW (t),
where dW (t) = η(t)dt is the notation for standard Brownian motion (compare e.g.
Dargatz [2010]). For a thorough introduction to Markov processes on continuous index
sets the reader may be referred to Meyn et al. [1996]. In this work all measurements are
taken at ﬁxed time points. Therefore we will restrict ourselves to time-discrete Markov
processes, called Markov chains.
Deﬁnition 2.14 (Markov chain). Let (Ω, F, P) be a probability space. A stochastic
process {X(t)}t∈I with values in E is called a Markov chain, if the index set I = N0 and
for any measurable set A ⊆E, any T ∈I \ {0, 1}, and any realization x(0), . . . , x(T) of
X(0), . . . , X(T), the random vector X(T+1) does not depend on x(0), . . . , x(T−1), that
is
PX(T +1)|X(0)⊗...⊗X(T )(X(T+1) ∈A|x(0), . . . , x(T)) = PX(T +1)|X(T )(X(T+1) ∈A|x(T)).
Furthermore, a stationary Markov chain is also called (time) homogeneous. From this
point on, we consider all Markov chains to be time homogeneous. In the next step,
we want to simplify the notation of the joint distribution PX(0)⊗...⊗X(t) for the ran-
dom vectors X(0), . . . , X(t): When dealing with Markov chains the joint probability
PX(0)⊗...⊗X(t+1) is often seen as transition probability from a state X(t) = x to some
A ⊆E. Put diﬀerently, it contains the distribution for X(t+1) given X(t) = x.
Deﬁnition 2.15 (Transition kernel). For a Markov chain {X(t)}t∈I on a probability
29

2. PREREQUISITES
space (Ω, F, P) with values in E and a measurable set A ⊆E the distribution
k(A|x) := PX(t+1)|X(t)(X(t+1) ∈A|X(t) = x)
=
Z
A
PX(t+1)|X(t)(dy|x)
is called (time homogeneous) transition kernel (or transition probability) from x ∈E to
A ⊆E.
The transition kernel is a time independent function
k : E × E −→[0, 1],
where (c.f. Robert & Casella [2004])
(i) k(·|x) is a probability measure for all x ∈E and
(ii) k(A|·) is measurable for all A ∈E.
Technically, it can be expressed via a function p : E × E −→[0, ∞) as
k(dy|x) = p(y|x)dy + r(x)1x(dy),
(2.17)
where 1x(dy) is the indicator function, p(x|x) = 0 and r(x) = 1 −
R
E p(y|x)dy (c.f.
Tierney [1994]). Here, the function p governs the transition from x to y while r(x)
holds the probability for X(t+1) to remain at X(t) = x. We will see later that in fact
r(x) needs to be positive for Markov Chain Monte Carlo methods.
Proposition 2.3. Given an initial value x(0) ∈E, the transition kernel k fully deter-
mines the respective Markov chain.
Proof. For all x(0) ∈E and Ai ∈E
PX(1)|X(0)(A1|x(0)) = k(A1|x(0))
PX(1)⊗X(2)|X(0)((X(1), X(2)) ∈A1 × A2|x(0)) =
Z
A1
k(A2|y1)k(dy1|x(0))
...
...
PX(1)⊗...⊗X(t)|X(0)((X(1), . . . , X(t)) ∈A1 × . . . × At|x(0)) =
Z
A1
. . .
Z
At−1
k(At|yt−1)
× k(dyt−1|yt−2) . . . k(dy1|x(0)).
30

2.3 Markov chains
This allows us to not specify the diﬀerent (conditional) probability distributions explic-
itly, but simply denote the distribution on any set X(t1), . . . , X(ts) for t1, . . . , ts ⊂I
by P. For reasons of readability we will in the following make use of this notation.
Note that the joint distribution P = PX(t1)⊗...⊗X(ts) for X(t1), . . . , X(ts) should not be
confused with the probability measure P on the probability space (Ω, F, P).
The primary goal of Markov Chain Monte Carlo (MCMC) methods lies in the inference
of a distribution π by means of a Markov chain {X(t)}t∈I. Towards this end we need
to make sure that the chain essentially converges towards π, regardless of where it
begins. The following deﬁnitions form the basis for proper MCMC methods: We call a
distribution π invariant or stationary for the transition kernel k(·|·), if
π(A) =
Z
E
k(A|x)π(dx)
=
Z
E
k(A|x)πd(x) dx,
∀A ∈E
(2.18)
where πd is the probability density function to π with respect to the Lebesque measure.
A stationary Markov chain is reversible, if for A ∈E,
P(X(t+1) ∈A|X(t+2) = x) = P(X(t+1) ∈A|X(t) = x).
(2.19)
Reversibility essentially states that the direction of the evolution on I does not inﬂuence
the dynamics of the chain. A suﬃcient condition for invariance and reversibility is given
by:
Deﬁnition 2.16 (Detailed balance condition). A Markov chain with transition kernel
k(dy|x) = p(y|x)dy + r(x)1x(dy) as introduced in (2.17) satisﬁes the detailed balance
condition, if there exists a probability density function πd, such that
p(x|y)πd(y) = p(y|x)πd(x).
(2.20)
Theorem 2.4. If the detailed balance condition holds for a Markov chain with transi-
tion kernel k and density function πd, then
(i) the associated distribution π is invariant with respect to k and
(ii) the Markov chain is reversible.
31

2. PREREQUISITES
Proof. Part (i) follows directly by checking Equation (2.18):
Z
E
πd(x)k(A|x) dx =
Z
E
Z
A
πd(x)p(y|x) dy dx +
Z
A
πd(x)r(x) dx
=
Z
A
Z
E
πd(x)p(y|x) dx dy +
Z
A
πd(x)r(x) dx
=
Z
A
πd(y) (1 −r(y)) + πd(y)r(y) dy
=
Z
A
π(dy).
Part (ii): As π is invariant with respect to k it follows that if X(0) ∼π, then X(t) ∼
π ∀t ∈I. Together with Bayes’ theorem this yields
p(y|x) + r(x)1x(y) = (p(x|y) + r(y)1y(x)) · πd(y)
πd(x)
= (p(y|x) + r(y)1y(x)) · πd(x)
πd(x)
= p(y|x) + r(y)1y(x).
Hence, for A ∈E
P(X(t+1) ∈A|X(t) = x) =
Z
A
p(y|x) + r(x)1y(x) dy
=
Z
A
p(y|x) dy
=
Z
A
p(x|y) dy
= P(X(t+1) ∈A|X(t+2) = x).
We have to point out that the detailed balance condition is suﬃcient but not necessary
for the existence of an invariant distribution π. However, its simplicity makes it easy
to check and it is therefore frequently assumed in most MCMC algorithms. Up to now
we laid grounds for the existence of a reversible invariant distribution π. This invariant
distribution might nonetheless be non-unique. If every Markov chain governed by the
transition kernel k is converging to the same invariant distribution π, independent of
the starting value x(0) ∈E, we call π an equilibrium distribution.
In terms of the
32

2.3 Markov chains
m-step transition kernel km(A|x) =
R
E km−1(A|y)k(dy|x) for the transition from x to
A in m ∈N steps this means
lim
m→∞km(A|x(0)) = π(A)
for almost all x(0) ∈E. Naturally, k1(A|x) := k(A|x). We need some more deﬁnitions
to characterize equilibrium distributions.
Luckily these are easy to prove for most
practical applications.
A Markov chain {X(t)}t∈I with transition kernel k is called π-irreducible for a σ-ﬁnite
π, if for any x ∈E and A ∈E with π(A) > 0 there exists an m ∈N such that
km(A|x) > 0.
Here, km(A|x) denotes the associated m-step transition kernel. This means the Markov
chain can get from any state x ∈E to any other state in E within a ﬁnite number of
steps. If m = 1, we call the chain strongly π-irreducible.
A π-irreducible Markov chain with transition kernel k is periodic, if for some integer
s ≥2 there exists a sequence (A0, A1, . . . , As−1) of pairwise disjoint non-empty subsets
Ai ∈E, such that for all i = 0, . . . , s −1 and all x ∈Ai
k(Aj|x) = 1
for j = i + 1 mod s.
We call the chain aperiodic, if it is not periodic. Frankly spoken, aperiodic Markov
chains do not contain deterministic cycles.
Suppose Px(A) reﬂects the probability that, starting at x ∈E, we obtain for the
number c(t)
A := card{x(s) ∈A|0 ≤s ≤t} of visits to some subset A ∈E up to t, that
c(t)
A →∞for t →∞. A Markov chain is Harris recurrent, if there exists an invariant
distribution π, such that for every A ∈E with π(A) > 0
Px(A) = 1
for all x ∈E.
As pointed out in Tierney [1994], it follows from Corollary 5.2 of Nummelin [2004]
that there exists an invariant measure ν on E for every π-irreducible Harris recurrent
Markov chain. The measure ν is unique up to a multiplicative constant. The chain is
called positive Harris recurrent, if ν(E) < ∞. We want to point out that there exists
33

2. PREREQUISITES
also the somewhat weaker notion of a recurrent – as opposed to Harris recurrent –
Markov chain. However, as all MCMC methods introduced in Chapter 4 start at some
arbitrary random vector x(0) ∈Rn, we have to make sure that the algorithms converge
to a unique invariant distribution independent of x(0). This is shown in Theorem 2.5
for Harris recurrent chains claiming some weak assumptions. It summarizes the neces-
sary and suﬃcient conditions for the convergence of a Markov chain to an equilibrium
distribution (Tierney [1994]). For a thorough proof see Sethuraman et al. [1992]. In
the case of recurrent chains the theorem only holds for almost all x(0) ∈E.
Theorem 2.5. Suppose {X(t)}t∈I is a π-irreducible, aperiodic and Harris recurrent
Markov chain with transition kernel k and invariant distribution π. Then
(i) k is positive Harris recurrent,
(ii) π is the (unique) equilibrium distribution and
(iii) k is ergodic for π, i.e. {X(t)}t∈I converges regardless of its starting value x(0) ∈
E, i.e. for every x ∈E and every A ∈E
∥km(A|x) −π(A)∥TV →0
for m →∞.
Hence, we later on only need to test for the existence of an invariant distribution π,
along with π-irreducibility, aperiodicity, and Harris recurrence in order to establish
a valid MCMC method. A corollary to Theorem 3.6 in Chapter 4 of Revuz [1984]
in combination with Corollary 1 of Tierney [1994] describes the limiting behavior of
averages. It states a law of large numbers and can be derived from the Chacon-Ornstein
or ergodic theorem (c.f. Tierney [1994]).
Theorem 2.6. Suppose {X(t)}t∈I is a positive Harris recurrent aperiodic Markov chain
with invariant distribution π. Suppose furthermore f : E −→R is π-integrable with
R
E|f(x)|π(dx) < ∞. Then for a realization {x(t)}t∈I the sample mean
¯fm =
1
m + 1
m
X
t=0
f(x(t)) −→
Z
E
f(x)π(dx) = Eπ [f(E)]
almost surely as m →∞.
Example 2.8 (Discrete state space). For a discrete state space E = {x1, . . . , xS} the
transition kernel k(X(t+1) = {xj}|X(t) = xi) := k(xj|xi) := ki,j can be written in
34

2.4 A short introduction on molecular biology
matrix notation as
K =




k(x1|x1)
. . .
k(xS|x1)
...
...
...
k(x1|xS)
. . .
k(xS|xS)



.
The matrix K is right stochastic, i.e. PS
j=1 ki,j = 1. Every starting distribution can
be written as vector π(0) ∈[0, 1]S. Suppose the Markov chain starts at x(0) = xj for
some j ∈{1, . . . , S}, then π(0)
j
= 1 while π(0)
i
= 0 for i ̸= j. The probability for moving
to state xs at t = 1 is given by
π(1) = π(0)K.
Applying the m-step transition kernel Km =: (k[m]
i,j )i,j=1,...,S, i.e. k[m]
i,j = P(X(m+t) =
xj|X(t) = xi) we iteratively obtain
π(t) = π(s)Kt−s
(2.21)
for 1 ≤s < t. Equation (2.21) is known as the discrete Chapman-Kolmogorov equation.
A continuous version is given in Lemma 4.1. It essentially states that a move from state
x(t) to x(t+2) passes through any of the states x1, . . . , xS with the respective probability.
The Markov chain {X(t)}t∈I is π-invariant, if π = πK. Thus, π needs to be a left-
eigenvector for the eigenvalue 1 of K. For ˜ki,j = P(X(t+1) = xj|X(t+2) = xi) and a
stationary distribution π = (π1, . . . , πS)⊤Bayes’ theorem yields
˜ki,j =
˜kj,i · πj
πi
= πj
πi
kj,i.
(2.22)
Hence, the detailed balance condition emerges naturally by Equation (2.22) for a re-
versible chain in the discrete case.
The Markov chain is irreducible, if every state
can be reached from any other state within a certain number of steps, i.e if for all
i, j ∈{1, . . . , S} there exists a natural number mij with k[mij]
i,j
> 0. It is aperiodic, if
for all m > 0 k[m]
i,i
̸= 1 for all i ∈{1, . . . , S}. For characterizing recurrence we ﬁnally
need the hitting times Ti = inf{t ≥1|X(t) = xi given X(0) = xi} for i ∈{1, . . . , S}.
The chain is recurrent, if the probability Pr(Ti < ∞) = 1 ∀i. It is positive (Harris)
recurrent, if the expected value of the hitting time is ﬁnite.
2.4
A short introduction on molecular biology
It is an amazing fact that almost every cell in a living organism contains a full blueprint
for the development and functionality of the entire organism. This information is stored
35

2. PREREQUISITES
in the so-called Deoxyribonucleic acid (DNA) within the cell nucleus. A gene is a short
sequence of this DNA strain holding information for the construction of a protein. While
there are approximately 20,000-25,000 genes in the human genome (i.e. the entity of
all genes in an organism), plants often times endow more than twice this number. The
pure amount of genes is therefore no indicator for the complexity of a life form. An
organism is in fact regulated by a complex network of protein interactions.
For a gene to code for a protein the processes of transcription and translation need to
take place. In transcription RNA Polymerase enzymes (proteins) along with various
transcription factor proteins identify a speciﬁc gene on the DNA strain. This gene then
gets synthesized and transformed into Ribonucleic acid (RNA), the basis for protein
construction. There are various types of RNA molecules that play an important role
in the actual protein building process, which itself takes place in the cytoplasm during
the translation step: Mitochondrial ribosomes consisting inter alia of ribosomal RNAs
(rRNAs) convert the information stored on messenger RNAs (mRNAs) into a protein.
This protein consist of diﬀerent amino acids that are transported to the ribosomes by
transfer RNAs (tRNAs). Moreover, short RNA sequences called silencing RNAs (siR-
NAs) and micro RNAs (miRNAs) control the protein coding mechanism. In addition,
there are also non-coding RNAs (ncRNAs). Although these do not contain informa-
tion for the translation process, it is believed that they control processes, such as gene
regulation. Much of their functionality has however not been inferred yet.
Proteins, protein complexes, or peptides (i.e. short amino acid sequences generally
built from larger precursor proteins) regulate the majority of cellular processes. These
comprise amongst others structural proteins used for all rigid components of the cell
(such as cell membranes), transcription factors that control the transcription process,
enzymes responsible for regulating the metabolism and the activation of proteins by
transferring phosphate groups (such as kinases), or growth factors (such as cytokines or
hormones) that trigger proliferation and cellular growth. For a thorough introduction
to cellular design and functionality the reader may be referred to Alberts et al. [2002].
Although the Human Genome project deciphered the entire human genome in 2003,
we are yet far from fully understanding its mechanistic interplay. The dynamics of all
of the underlying mechanisms are highly complex. They can be and are classiﬁed and
studied as dynamical systems (see Chapter 2.5).
36

2.4 A short introduction on molecular biology
2.4.1
Signaling pathways
We saw above that proteins are essential entities for the functionality of an organism,
but how does a cell know which proteins to express when? This is mostly regulated by
cellular signaling. On the molecular level these signals are again mainly mediated by
proteins, small peptides, single amino acids, or lipids. Speciﬁcally, intercellular signals
are either transmitted by direct cell-to-cell exchange of molecules or by the secretion
of molecules from the signaling cell; these in turn provoke a reaction on the surface
receptor proteins of a receiving cell. In the latter mechanism a signaling protein binds
to an extracellular receptor and induces a series of biochemical reactions that trans-
port the information through the cell membrane. Within the receiving cell a receptor
associated kinase or kinase domain is thereupon activated. This induces the activation
of diverse intracellular proteins or other signaling molecules which ﬁnally transmit the
signal to a speciﬁc cellular compartment, such as the nucleus. Generally, the alter-
ation of various proteins by a series of phosphorylation and dephosphorylation steps
is responsible for signal maintenance (Kowarsch [2011]). In the nucleus the transmit-
ted signal controls processes like transcription. These are in turn responsible for cell
growth, diﬀerentiation, apoptosis, or protein synthesis to name just a few. The mecha-
nism of transmitting an extracellular signal to a cellular compartment for the induction
of a speciﬁc response is called cellular signaling pathway or simply signaling pathway.
In summary, cellular signaling pathways are processing and transmitting intercellular
signals in order to control cellular processes.
2.4.2
The JAK-STAT pathway
An important representative for signaling pathways in mammals is the so-called JAK-
STAT pathway (JAK stands for Janus Kinase, and STAT for Signal Transducer and
Activator of Transcription). It is utilized by more than 50 diﬀerent cytokines, hor-
mones, and other growth factors and plays a key role in gene regulation (Subramaniam
et al. [2001]). Scientiﬁcally it is therefore of major interest. Malfunctioning results
in diseases like leukemia or bronchial asthma (Igaz et al. [2001]). In the JAK-STAT
pathway a cellular transmembrane receptor is triggered by diﬀerent molecules of the cy-
tokine or growth factor families. Examples include the epidermal growth factor (EGF),
37

2. PREREQUISITES
erythropoietin (EPO), interferones (INFα, INFβ, INFγ), and Interleukin-6 (IL-6). Cy-
toplasmic STAT proteins are inactive in unstimulated cells. Upon receptor activation
the receptor-bound JAK proteins catalyze auto-phosphorylation and build tyrosine
residues for tyrosine-phosphorylation of STAT proteins between STAT Src-homology 2
(SH2) domains and the tyrosine residues (Aaronson & Horvath [2002]). The tyrosine-
activated STAT proteins homo- and heterodimerize and get rapidly transported into the
nucleus subsequent to a possible second serine phosphorylation step of the dimer (Wen
et al. [1995]). In the nucleus the activated STAT dimer dramatically upregulates the
transcription rate of the target promoter. After the transcription process inactive, i.e.
unphosphorylated, STAT is released back into the cytoplasm. Four evolutionarily con-
served JAK proteins (JAK1, JAK2, JAK3, TYK2) and seven STAT coding genes with
corresponding proteins (STAT1, STAT2, STAT3, STAT4, STAT5A, STAT5B, STAT6)
are known for mammals (Aaronson & Horvath [2002]). These however only work in
speciﬁc combinations coordinated by SH2-phosphotyrosine interactions.
In Chapter 6.3.4, we use a delay diﬀerential equation model of the JAK2-STAT5 path-
way in order to evaluate the performance of the copula based Metropolis-Hastings
algorithms introduced in this thesis. In Chapter 7 model inference helps to address the
question, whether tyrosine phosphorylated STAT3 can work as transcription factor in
the JAK1-STAT3 pathway.
2.5
Dynamical systems in molecular biology
Understanding the mechanisms of cellular functionality is a key challenge in the ﬁeld
of systems biology. In recent years much eﬀort has gone into the inference of gene
regulatory, metabolic and signaling networks, which inter alia govern gene expression,
cellular communication, or intra cellular molecular transfer (De Jong [2002]; Palsson
[2006]). All of these processes may be modeled by a system of biochemical reactions of
the form
R1 : a11A11 + . . . + a1r1A1r1 −→b11B11 + . . . + b1s1B1s1
...
Rm : am1Am1 + . . . + amrmAmrm −→bm1Bm1 + . . . + bmsmBmsm
(2.23)
38

2.5 Dynamical systems in molecular biology
where the reactants Ai1, . . . , Airi are transformed into the products Bi1, . . . , Bisi. The
natural numbers ai1, . . . , airi and bi1, . . . , bisi hold the number of reactants and products
involved in the reactions (2.23). Conventionally for i = 1, . . . , m the greatest common
divisor of these quantities is equal to one. Biochemical reactions solely control cellular
activity. Generally, each reaction Ri obeys the law of mass action, which states that
the probability of Ri to occur is proportional to the product of the concentrations of
all reactants.
Example 2.9 (Elementary biochemical reactions). Wilkinson [2006] analyzes a simple
auto-regulatory gene network in prokaryotes: A protein P is coded for by a gene g, i.e.
g is transcribed into the transcript r, which is subsequently translated into the protein
P. After translation the protein P builds a protein complex P2 consisting of two copies
of P. This homodimer P2 ﬁnally inhibits the transcription of gene g. The network is
based on the interaction of the following biochemical reactions:
transcription:
g
−→
g + r
mRNA translation:
r
−→
r + P
repression:
g + P2
←→
g · P2
dimerization:
2P
←→
P2
mRNA degradation:
r
−→
∅
protein degradation:
P
−→
∅
Here, products connected by a dot represent a gene-protein complex. The empty set ∅
indicates that the reactant on the left hand side of the reaction is degraded. Reactions
with a double sided arrow are reversible, which means that the right hand side of the
equation can act as reactant producing the left hand side as product as well.
There are various approaches for modeling the dynamics of the reactions (2.23) over
time. Gillespie [2007] presented a nice review on the topic. We shortly summarize its
key aspects in the following.
Biochemical reactions occur, when a molecule transforms itself to another isomeric
form or two or more molecules form a molecular complex. While the ﬁrst scenario is
governed by quantum mechanics, the latter depends on the chance of these molecules
to come within a certain distance to each other. The dynamics of molecular systems
thus exhibit some stochasticity.
Let us assume here that the system is well-stirred
and has a constant volume and temperature throughout the modeling process. These
39

2. PREREQUISITES
assumption guarantee that the positions and velocities of the individual molecules have
no eﬀect on the system’s dynamics. As bottom line we want to estimate the state
vector X(t) = (X(t)
1 , . . . , X(t)
d )⊤based on some initial conﬁguration X(t0), where X(t)
i
denotes the number of molecules of species i at time point t and d is the number of
species present. Each reaction Rj (j ∈{1, . . . , m}) is then characterized by
(i) a state change vector vj = (v1,j, . . . vd,j)⊤and
(ii) a propensity function aj(·).
The elements vi,j hold the change in the copy number of species i, if reaction Rj occurs,
i.e. reaction Rj updates the current conﬁguration X(t) = x to x + vj. On the other
hand, aj(x)dt gives the probability that reaction Rj occurs in the inﬁnitesimal time
interval [t, t + dt) while the system is in the conﬁguration x.
For instance, let us
consider a unimolecular reaction Rj : Aj,i −→B, where Aj,i represents a molecule of
species i (reactant) and B is some product. Then, due to the laws of physics, there
exists a rate constants (also called reaction rate) kj, such that the probability for any
of the molecules of type Aj,i to react in the inﬁnitesimal time interval [t, t + dt) is
given by kjdt.
Hence, the propensity function reads aj(x) = kjxi (i ∈{1, . . . , d})
for the conﬁguration X(t) = x = (x1, . . . , xd)⊤. Similarly, for a bimolecular reaction
Rj′ : Aj′,i1 + Aj′,i2 −→B′ there exists a rate constant kj′, such that aj′(x) = kj′xi1xi2
(i1 ̸= i2, i1, i2 ∈{1, . . . , d}). In the case i1 = i2 we instead have aj′(x) = 1
2kj′xi1(xi1−1).
Since all biochemical reactions can be built using a combination of uni- and bimolecular
reactions, we do not consider higher order reactions for now.
The propensity functions aj(x) (j = 1, . . . , m) allow to describe the evolution over time
of the probability P(x, t|x0, t0) that the system is at conﬁguration x at time point t,
given it started in x0 at t0. The result (see e.g. Gillespie [1992] for a derivation) is the
so-called chemical master equation
P(x, t|x0, t0)
dt
=
m
X
j=1
(aj(x −vj)P(x −vj, t|x0, t0) −aj(x)P(x, t|x0, t0)) .
(2.24)
Equation (2.24) completely determines P(x, t|x0, t0).
Unfortunately it can only be
solved analytically in the most simplest scenarios and even a computational approxi-
mation is often times too costly in larger systems.
40

2.5 Dynamical systems in molecular biology
Algorithm 1: The stochastic simulation algorithm.
Input: System conﬁguration x0 at time t0, ﬁnal simulation time tf > t0, state change vectors
v1 . . . , vm, and propensity functions a1(·), . . . , am(·).
Output: Realization of the process X(t) as ﬁnite series of pairs {(x0, t0), (x1, t1), (x2, t2), . . .}.
Initialize k ←0
while tk < tf do
for i ←1 to m do
bi ←ai(xk)
Set b0 ←Pm
i=1 bi.
Sample r ∼U[0, 1] and set τ ←−b−1
0
log(r).
Sample s ∼U[0, 1] and set j ←smallest l such that Pl
l′=1 bl′ > sb0.
Update k ←k + 1.
Set tk ←tk−1 + τ and xk ←xk−1 + vj.
Instead of solving the chemical master equation explicitly, we can also try to simulate
a realization of X(t) over time. The key to this approach is to consider the probability
P(τ, j|x, t)dτ of the reaction Rj to occur as next reaction in the inﬁnitesimal time
interval [t + τ, t + τ + dτ), given that the system is in conﬁguration X(t) = x at time
point t. Here, according to Gillespie [1992], we have
P(τ, j|x, t) = aj(x) exp

−


m
X
j′=1
aj′(x)

τ

.
(2.25)
This forms the basis for the stochastic simulation algorithm depicted in Algorithm 1.
Extensions to Algorithm 1 e.g. for the case of large m and d were given by Gibson &
Bruck [2000] or Cao et al. [2004].
We now want to approximate the sometimes computationally expensive outcome of
the stochastic simulation algorithm by means of a stochastic diﬀerential equation. For
this we assume that for some τ > 0 all propensity functions a1(·), . . . , am(·) are close
to constant on the time interval [t, t + τ). Then the number of reactions Rj within
[t, t + τ) is Poisson distributed with mean aj(x)τ. We obtain the discrete update rule
X(t + τ) ≈x +
m
X
j=1
ηjvj
for the conﬁguration X(t) = x and m independent Poisson distributed random variables
ηj, j = 1, . . . , m, with according means aj(x)τ. Assuming furthermore that for all
41

2. PREREQUISITES
j = 1, . . . , m, aj(x)τ ≫1, we can approximate the Poisson distributions using the
m normal distributions N(aj(x)τ, aj(x)τ) with means aj(x)τ and variances aj(x)τ
(j = 1, . . . , m). For small τ’s this leads to the chemical Langevin equation
dX(t)
dt
=
m
X
j=1
aj(X(t))vj +
m
X
j=1
q
aj(X(t))η(t)
j ,
(2.26)
where η(t)
j
denotes some independent Gaussian white noise process (Dargatz [2010]).
For a thorough introduction to stochastic diﬀerential equations see for instance Øksendal
[2003]. Equation (2.26) can now be used to speed up the simulation of X(t) over time.
In the thermodynamic limit, i.e. if the copy number of species i and the volume simul-
taneously approach inﬁnity while their quotient stays constant, the second term on the
right hand side of Equation (2.26) becomes negligible compared to the ﬁrst term (Gille-
spie [1992]). Hence, if the substances involved in the biochemical reactions have numer-
ous copy numbers and the cellular volume is large compared to the sizes of its molecules
chemical kinetics can be modeled by sets of ordinary diﬀerential equations. Allowing ad-
ditionally time delays for transcription, translation or diﬀusion processes (c.f. De Jong
[2002]) we end up with a set of delay diﬀerential equations. In both cases we are deal-
ing with continuous vectors of concentrations x(t) = (x1(t), . . . , xd(t))⊤≥0 ∈Rd of
biochemical substances x1(t), . . . , xd(t) within a given time interval [0, T]. These sub-
stances can be proteins, RNA’s, small molecules, and the like. As seen above, the rate
of change for their concentrations is given by a set of diﬀerential equations
dx(t)
dt
= gξ(x1(t), . . . , xd(t), x1(t −τ1), . . . , xd(t −τd), u(t), t),
(2.27)
linking the solution x(t) via a ξ-parametrized (nonlinear) Lipschitz-continuous func-
tion gξ : R2d
+ × Rk+1 −→R2d to the derivative of x(t) with respect to time t. For
readability we generally omit the dependence of x(t) on the parameter vector ξ ∈Rn.
The latter can contain reaction rates, initial values to (2.27), but also noise parameters
of the measurements or further constants as will become clear later. While u(t) ∈Rk
represents an input vector of externally-supplied energy or input signals, the constants
τ1, . . . , τd denote discrete time delays. For τ1 = . . . = τd = 0 we call (2.27) a system of
(nonlinear) ordinary diﬀerential equations (ODE’s). Otherwise (2.27) is denoted as a
system of (nonlinear) delay diﬀerential equations (DDE’s). We also write
dx(t)
dt
= gξ(x(t), τ, u(t), t),
(2.28)
42

2.5 Dynamical systems in molecular biology
for the system (2.27), where τ = (τ1, . . . , τd)⊤denotes the vector of time delays. Ex-
amples for systems of diﬀerential equations in computational biology are the modeling
of mRNA synthesis (Goodwin [1963]) or the modeling of cell cycles in Caulobacter
crescentus (Li et al. [2008]) and yeast (Chen et al. [2004]).
Remark 2.1. We have to point out that there are other approaches for modeling the
dynamics of cellular processes, too. For instance, especially in large systems we are
often given no particular information about the number of reactants and products
involved, i.e. we only have qualitative information such as “a gene is expressed at time
point t”. This gives a qualitative view on the system’s behavior which is frequently
modeled by a so-called Boolean network. The dynamics are given by simple logic-driven
recombinations of binary ON/OFF states for the substances involved (c.f. Bornholdt
[2008]; Kauﬀman [1969]; Thomas [1991] for an introduction). As Boolean networks
do not contain information about molecular copy-numbers or concentrations they only
provide a rather crude view on the system’s evolution. This modeling technique might
be essential if the number of species is very large. However, in general it is avoided in
smaller systems.
In the following we will only deal with ordinary and delay diﬀerential equation models,
which is what we call a dynamical system in this thesis. From the derivation of these
systems above we saw that a large abundance of entities is necessary throughout the
modeling process in order to justify the approach. This also implies that rate con-
stants are constant at all times. We therefore consider the model parameters to be
time-independent. We furthermore assume that the system is well-stirred and external
inﬂuences such as the temperature or the osmotic pressure are constant throughout the
modeling process. This allows us to ignore any spacial constraints that would e.g. call
for models involving hard-to-handle partial diﬀerential equations. Raia et al. [2011]
showed that the number of STAT5 and STAT6 molecules contained in human lym-
phoma cells (L1236) is ∼2 · 105 each. For the JAK-STAT pathways of Chapters 6.3.4
and 7 the assumption of being close to the thermodynamic limit is hence well justiﬁed
and we can in fact approximate the dynamics by applying a diﬀerential equation model.
Example 2.10 (ODE representation of biochemical reactions). According to the law of
43

2. PREREQUISITES
0
10
20
30
0
0.5
1
1.5
2
time
concentration
 
 
g
r
P
P2
gP2
(a)
0
10
20
30
0
2
4
6
8
time
concentration
 
 
g
r
P
P2
gP2
(b)
Figure 2.4: Time courses for Equation (2.29). (a) All rate constants were set to 0.5.
The initial concentrations were g(0) = 1 and r(0) = P(0) = P2(0) = gP2(0) = 0. (b)
Again, all rate constants were set to 0.5. The initial concentrations were g(0) = 5 and
r(0) = P(0) = P2(0) = gP2(0) = 0. Both systems are close to individual steady states at
time point t = 30.
mass action the corresponding ODE-system to Example 2.9 is given by
dg(t)
dt
= k1gP2(t) −k2g(t)P2(t)
dr(t)
dt
= k3g(t) −k4r(t)
dP(t)
dt
= k5r(t) + k6P2(t) −k7P(t)2 −k8P(t)
dP2(t)
dt
= 1
2k7P(t)2 + k1gP2(t) −1
2k6P2(t) −k2g(t)P2(t)
dgP2(t)
dt
= k2g(t)P2(t) −k1gP2(t)
(2.29)
for the concentrations g(t), r(t), P(t), P2(t), gP2(t) of g, r, P, P2, gP2 at time point t and
some non-negative reaction rates k1, . . . , k8. The corresponding parameter vector is
given by ξ = (k1, . . . , k8)⊤∈R8
+. Note that the dimerization process needs two proteins
P to form one dimer P2. This is reﬂected in the second to last equation by multiplying
the rate constants k6 and k7 by one half. There is no basal production rate for any of
the elements. Various examples for possible dynamics of Equation (2.29) are shown in
Figure 2.4.
The solution of (nonlinear) ordinary diﬀerential equations can be numerically approxi-
mated using e.g. Matlab’s ode15s (Shampine & Reichelt [1997]), or SUNDIALS’ CVODEs
44

2.5 Dynamical systems in molecular biology
	






β1 
β2 
(a)
0
200
400
600
0
0.2
0.4
0.6
0.8
1
time
concentration
 
 
SIR
(b)
Figure 2.5:
(a) The SIR model. (b) Time course for the SIR model with transfer rates
β1 = 0.1, β2 = 0.01 and initial conditions s(0) = 0.9, i(0) = 0.1, and r(0) = 0.
(Serban & Hindmarsh [2005]) solvers. For delay diﬀerential equations Matlab’s dde23
solver (Shampine & Thompson [2001]) can be used.
2.5.1
Compartment models
A compartment model consists of a ﬁnite set of mutually exclusive compartments. Each
compartment holds a group of objects unambiguously identiﬁable with the respective
compartment (Jacquez [1985]). The interaction of compartments is governed by transi-
tion equations, which control the exchange of objects between compartments. The net
ﬂow between compartments is based on the density of its objects. All compartments
are assumed to be well-mixed and homogeneous with constant volume. This implies
that all objects distribute instantly after transition. In this thesis compartment models
are also seen as dynamical systems since transition equations are deﬁned by a system of
diﬀerential equations. In contrast to biochemical reactions we consider closed systems
only, i.e. there is no external ﬂow of objects into or out of the system.
Example 2.11 (SIR model). Probably the most prominent example for a compartment
model stems from epidemiology. It estimates the spread of an epidemic, such as measles,
in large populations (Anderson & May [1992]). A simple version contains the three com-
partments S (susceptible subjects), I (infectious subjects) and R (recovered subjects),
which are governed by the system of ODE’s:
45

2. PREREQUISITES
ds(t)
dt
= −β1 · i(t) · s(t)
di(t)
dt
= β1 · i(t) · s(t) −β2 · i(t)
dr(t)
dt
= β2 · i(t)
where β1 and β2 are transfer rates and s(t), i(t), r(t) the concentrations corresponding
to the compartments S, I and R at time t. The hazard of an individual for an infection
depends on the concentration of infected individuals and the transfer rate β1. The
chance for recovery on the other hand is solely controlled by β2. A time course of the
model is shown in Figure 2.5.
2.5.2
Parameter estimation in dynamical systems
Parameter inference of diﬀerential equation systems is a prominent topic in the ﬁeld
of computational systems biology. Despite the arrival of new, high-throughput mea-
surement techniques, compared to model complexity most systems suﬀer from very low
observation numbers and noisy measurements. Moreover, as biological organisms need
to be able to quickly adjust to various environmental conditions, we hence expect these
models to be somewhat insensitive to parameter variations. Some may even show two
ranges of functionality (Kaplan et al. [2008]).
Given the general dynamical system
dx(t)
dt
= gξ(x(t), τ, u(t), t),
(2.30)
the vector ξ ∈Rn in (2.30) holds the parameters deﬁning gξ. As mentioned above, it can
contain rate constants, initial values to (2.30) or other constants e.g. necessary for the
link functions deﬁned in the following. In practical applications ξ needs to be inferred
from a set of given observations {y1, . . . , ym}, where yi was observed at the time
points ti ∈[0, T] for i = 1, . . . , m. It is assumed that there exists a parameter vector
ξ such that the simulation of the diﬀerential equation trajectory of (2.30) contains
the true dynamics of the particular biological system. More precisely, the observation
yi = (yi,1, . . . , yi,li)⊤with li ∈N is supposed to satisfy the equation
yi,j = h(i,j)
ξ
(x(ti)) + εi,j,
j = 1, . . . , li
(2.31)
46

2.5 Dynamical systems in molecular biology
for the realizations εi,j of some independent normally distributed random variable with
mean zero and unknown variance. Note that the li’s can vary between observations
as we do not require the measurements to be of the same dimension at every time
point ti. The functions h(i,j)
ξ
: Rd −→R denote ξ-dependent link functions, where
h(i,j)
ξ
can correspond to a simple projection on the jth component of x(ti), to sums
xj1(ti)+. . .+xjr(ti) (j1, . . . , jr ∈{1, . . . , li}) thereof, or their rescaled versions s1·xj(ti),
or s2 ·(xj1(ti)+. . .+xjr(ti)), where s1 and s2 are unknown scaling constants contained
in ξ. These link functions arise since technical limitations frequently prevent to observe
each concentration xi individually.
On the basis of Equation (2.31) the parameter vector ξ of 2.30 is generally estimated
by minimizing the squared error loss function
χ2(ξ) =
m
X
i=1
li
X
j=1

yi,j −h(i,j)
ξ
(x(ti))
2
σ2
i,j
,
(2.32)
with respect to ξ (see Horbelt et al. [2002] or Maiwald & Timmer [2008]), where σ2
i,j
denote known measurement errors. Very promising approaches for the minimization
process include global nonlinear optimization methods, such as the simulated annealing
algorithm (see ˇCern`y [1985]; Kirkpatrick et al. [1983] or Chapter 4.6), the genetic
algorithm (Fraser & Burnell [1970]), or coupled local minimizers (Suykens & Vandewalle
[2002]; Suykens et al. [2002]). These techniques have shown to work well in practice as
they try to avoid getting trapped in local minima. Nevertheless, deterministic methods,
such as steepest decent algorithms (Fletcher [1987]) started various times at diﬀerent
initial ξ-values can also be applied.
2.5.3
Parameter identiﬁability in dynamical systems
Technical limitations generally prevent experimentalists from individually measuring
every substance involved in a biological process. This means that we are dealing with
incomplete data when modeling a particular system. Moreover, there might even be
a considerable amount of noise on the measurements. Therefore, we need to raise the
question, whether a model – or rather its parameters – can at all be identiﬁed based
on some noisy, incomplete observations y := {y1, . . . , ym}?
47

2. PREREQUISITES
An approach to address this issue for dynamical systems has been proposed by Raue
et al. [2009] (see also Murphy & Van der Vaart [2000] and Venzon & Moolgavkar [1988]).
The authors consider sets of the form
CRα := {ξ|χ2(ξ) −χ2(ˆξ) < ∆α},
where χ2(ξ) is the squared error loss function of Equation (2.32), ˆξ is the according
estimated argmin, i.e. χ2(ξ) > χ2(ˆξ) for all deﬁned ξ’s, and ∆α is the α-quantile of the
χ2-distribution with one degree of freedom (for details see Press et al. [1986]). Meeker
& Escobar [1995] showed that the borders of CRα represent conﬁdence regions for ˆξ in
linear systems.
A parameter ξi of ξ = (ξ1, . . . , ξn)⊤is then said to be identiﬁable, if the conﬁdence
interval [li, ui] (of the estimate ˆξi) deﬁned by li := min{ξi|∃ξ, s.t. χ2(ξ)−χ2(ˆξ) < ∆α}
and ui := max{ξi|∃ξ, s.t. χ2(ξ)−χ2(ˆξ) < ∆α} (if existent and −∞/+∞otherwise), is
ﬁnite. Moreover, Raue et al. [2009] propose even ﬁner notions of identiﬁability: They
call a system structurally identiﬁable, if χ2(ξ) possesses a unique minimum. Further-
more, a system is named practically identiﬁable, if it possesses a unique minimum and
none of the conﬁdence intervals [li, ui] (i ∈{1, . . . , n}) has inﬁnite size. Frankly spoken,
structural identiﬁability issues arise, if there exists a functional relationship between
individual model parameters. Practical identiﬁability issues, on the other hand, are
caused by too noisy measurements; although the measurements allow for a minimum of
χ2(ξ) at ˆξ, the χ2(ξ) function is too ﬂat around ˆξ to consider the estimate signiﬁcant.
The following example gives an instance of a structurally non-identiﬁable model:
Example 2.12 ( Structural non-identiﬁability). Let us consider the compartment model
inspired by the models of Chapter 8:
dx1(t)
dt
= −β1x1(t) −β3x1(t) −β4x1(t) + β2x2(t) + β5x3(t)
dx2(t)
dt
= β1x1(t) −β2x2(t)
dx3(t)
dt
= β4x1(t) −β5x3(t)
(2.33)
where (without loss of generality) β1, . . . , β5 are positive rate constants controlling
the ﬂow between the compartments x1, x2, and x3. The rate β3 corresponds to the
degradation of the elements in x1.
Suppose x1(0) = 10 and x2(0) = x3(0) = 0 in
48

2.5 Dynamical systems in molecular biology



β5 
β1 
β2 
β3 
β4 
Ø 
(a)
0
0.5
1
0
2
4
6
8
10
12
time
concentration
 
 
β1 = β4 = 2
β1 = 3β4 = 3
(b)
Figure 2.6:
(a) Schematic representation of model (2.33). (b) Time course for com-
partment x1 of the model in (a) for x1(0) = 10 and x2(0) = x3(0) = 0.
The blue
line corresponds to β1 = β4 = 2, β2 = β3 = β5 = 0.5, the dashed black line to
β1 = 3, β4 = 1, β2 = β3 = β5 = 0.5.
arbitrary units. For the vector of concentrations x(t) = (x1(t), x2(t), x3(t))⊤the linear
ODE (2.33) has the solution
x(t) = exp(A·t)·



10
0
0



for the matrix
A =



−β1 −β3 −β4
β2
β5
β1
−β2
0
β4
0
−β5


.
Setting β2 = β5 the characteristic polynomial in λ is
χ(λ) = (β2 + λ)2(c + β3) −(β2 + λ)β2c,
where c = β1 + β4 > 0. This shows that the parameters β1 and β4 are not identiﬁable,
since for any β1 ̸= c we get the same solution to (2.33) setting β4 = c−β1. A schematic
representation of (2.33) including a time course can be found in Figure 2.6.
For checking the identiﬁability of a particular model the Matlab based PottersWheel
software (Maiwald & Timmer [2008]) can be used. Here, the ﬁniteness of the conﬁdence
intervals [li, ui] is estimated via the so-called proﬁle likelihood function
χ2
PL(ξ0
i ) := min
A χ2(ξ)
for A = {ξ = (ξ1, . . . , ξn)⊤|ξi = ξ0
i }. The proﬁle likelihood checks whether for i =
1, . . . , n the bounds li and ui are ﬁnite by proceeding into the direction of the least
increase of χ2(ξ) starting at the estimated argmin ˆξ of χ2(ξ).
49

2. PREREQUISITES
Various other methods for parameter identiﬁability analysis were proposed by Hengl
et al. [2007]; Lecourtier et al. [1987]; Ljung & Glad [1994] or D. et al. [2003]. All of
these approaches do however not test for practical identiﬁability.
In summary, before performing parameter estimation in dynamic systems, an iden-
tiﬁability analysis has to be performed in order to ensure the validity of parameter
estimates.
50

3
Bayesian model inference
We now turn to the concept of Bayesian model inference. For the application in dynam-
ical systems this comprises two aspects: On the one hand, given a speciﬁc parametrized
model M = M(ξ), we want to infer the parameter values ξ ∈Rn and their correspond-
ing uncertainties based on a series of observations y = {y1, . . . , ym}. This can be seen
as extension to the parameter estimation approach described in Chapter 2.5.2. On
the other hand, given a set of parametrized models M1, . . . , Mk with according param-
eter vectors ξ1 ∈Rn1, . . . , ξk ∈Rnk, we want to infer the best model structure Mi
resembling the observations y, i.e. we want to deduce the model Mi with the highest
probability that the observations y were generated by Mi. Interestingly, both aspects
can be covered by the technique of Markov Chain Monte Carlo (MCMC) sampling
addressed in Chapter 4.
3.1
Bayesian parameter inference
First, we address the issue of inferring the parameter vector ξ ∈Rn for a given
parametrized model M based on observations y = {y1, . . . , ym}. Taking a frequen-
tistic approach, y is considered the outcome of one of inﬁnitely many repetitions of the
same experiment based on ξ. Conversely, in the Bayesian paradigm ξ is considered as a
realization of a random parameter vector (Robert & Casella [2004]). Every data point
yi is here seen as realization of a random vector X ∼f(x|ξ) for a density function
51

3. BAYESIAN MODEL INFERENCE
f(·|ξ) conditioned on the parameter vector ξ. Given f(·|ξ) we deﬁne the likelihood
function (or simply likelihood) for y = {y1, . . . , ym} by
L(ξ|y) = L(ξ|y1, . . . , ym)
=
m
Y
i=1
f(yi|ξ).
This is a function of the parameter vector ξ.
In combination with a prior density
function π(ξ), containing information about ξ before observing y, Bayes’ theorem gives
rise to a probability distribution function
π(ξ|y) =
L(ξ|y)π(ξ)
R
Rn L(ξ|y)π(ξ) dξ
(3.1)
(see for instance Berger [1985] or Bernardo et al. [1994] for the foundations of this
approach). The term π(ξ|y) in (3.1) is called posterior distribution of ξ and represents
the core of Bayesian inference. It contains the distribution of ξ while taking into account
both the data y itself as well as prior knowledge about the parameters (by means of
π(ξ)). Note that we abusively use ξ to denote a realization of the random parameter
vector and the variable of the density function (3.1). The integral in the denominator
π(y) =
Z
Rn L(ξ|y)π(ξ) dξ
(3.2)
is called marginal likelihood (or model evidence). As we will see in Chapter 3.4, π(y) is
subject to model inference, but generally analytically intractable. In high dimensions
numerical approximations of π(y) are diﬃcult and mostly inaccurate. Sophisticated
approaches such as the ones introduced in Section 3.4.3 have to be applied. The infer-
ence of the posterior distribution of (3.1) is often done by MCMC sampling (Chapter
4). These methods exploit the fact that π(y) does not depend on the parameter vector
ξ: As the vector of observations y is ﬁxed, the inference is solely based on the relation
π(ξ|y) ∝L(ξ|y)π(ξ).
(3.3)
Analogously to Chapter 2.5.2 we want to infer the parameter distribution of a parametrized
diﬀerential equation of the form
dx(t)
dt
= gξ(x(t), τ, u(t), t),
(3.4)
52

3.1 Bayesian parameter inference
based on noisy observations y = {y1, . . . , ym} at the time points t1, . . . , tm.
For
Bayesian parameter estimation we assume that the observations contain independent
measurement errors, i.e. for yi = (yi,1, . . . , yi,li)⊤we have
yi,j = h(i,j)
ξ
(x(ti)) + εi,j,
j = 1, . . . , li
(3.5)
where εi,j are realizations distributed according to some time-independent noise model
density functions f(i,j)(·|ξ). Here, h(i,j)
ξ
again denote link functions introduced in Equa-
tion (2.31). We hence infer the distribution
π(ξ|y) ∝
m
Y
i=1
li
Y
j=1
f(i,j)(yi,j −h(i,j)
ξ
(x(ti))|ξ)π(ξ)
for some prior π(ξ) and the solution x(t) of Equation (3.4) corresponding to the param-
eter vector ξ. The vector ξ can contain rate constants, time delays, or initial conditions
of the diﬀerential equation, but also parameters of the applied noise model, such as the
standard deviation in case of a Gaussian error model.
We deﬁne the maximum likelihood estimator (MLE) for a given likelihood function
L(ξ|y) as the parameter vector ˆξML ∈Rn at which L(ξ|y) attains its maximum,
if existent. Furthermore, the maximum a posteriori estimate (MAP) for a posterior
function π(ξ|y) is the value ˆξMAP ∈Rn at which π(ξ|y) attains its maximum, if
existent. Clearly, both estimators coincide, if the prior density π(ξ) is uniform and its
support contains ˆξML. It is easy to see that for Gaussian noise model density functions
f(i,j)(·|ξ) with known measurement errors σ2
(i,j), the estimate ˆξML coincides with the
minimization result of Equation (2.32). Note however that contrary to the classical
parameter estimation approach unknown measurement errors σ2
(i,j) can be estimated
simultaneously in the Bayesian paradigm, this is, ξ can contain the σ2
(i,j)’s.
Analogously to conﬁdence intervals, we can compute credible intervals for each compo-
nent ξj of ξ: For j ∈{1, . . . , n} and conﬁdence level α the 100%(1−α) credible interval
for ξj is the interval Ij = [Ilow
j
, Iup
j ] with lower bound
Ilow
j
= Π−1
j (α/2|y)
and upper bound
Iup
j
= Π−1
j (1 −α/2|y),
where Π−1
j (·|y) is the univariate quantile function corresponding to the marginal pos-
terior density function π(ξj|y) for ξj, i.e. Π−1
j (·|y) is the inverse of the corresponding
53

3. BAYESIAN MODEL INFERENCE
posterior distribution function Π(ξj|y). Clearly,
Z
Ij
π(ξj|y) dξj = 1 −α.
(3.6)
As in classical approaches α = 0.1 and α = 0.05 are popular, although arbitrary,
conﬁdence levels. We may point out that there exist generic cases for which the jth
component (j ∈{1, . . . , n}) of the MLE or MAP is not contained in the credible interval
Ij. However, in practical applications this is generally not an issue.
While in the frequentistic paradigm conﬁdence intervals hold the probability for the
“true” parameter value to be contained within the conﬁdence interval of a parameter ξ,
in the Bayesian language credible intervals hold the probability that ξ itself is contained
in this interval. Credible intervals are thus considered one of the strongest points of
Bayesian inference.
3.2
Prior distributions
A crucial issue in Bayesian statistics is the correct choice of prior distributions. When-
ever we are given information about the model or observations y1, . . . , ym, it should
(and must) be contained in the inference process. However, since priors can have con-
siderable inﬂuence on the obtained results, the topic has to be treated with care. A very
popular choice for prior distributions constitutes the class of conjugate priors: When
the likelihood is of the exponential family, this is, the likelihood has the form
L(ξ|y) = h(y) exp(ξ · R(y) −Ψ(ξ))
with functions h : Rl −→R, R : Rl −→Rn, Ψ : Rn −→R – the product ξ · R(y) is to
be understood as scalar product in Rn – then for ζ ∈Rn, λ > 0 the conjugate prior is
deﬁned as
π(ξ|ζ, λ) = k(y) exp(ζ · ξ −λΨ(ξ))
where k : Rl −→R is a function dependent on y only (Marin & Robert [2007]). Since
the data y is ﬁxed, k can also be considered as constant. The prior parameters ζ and
λ are called hyperparameters, as are all parameters inﬂuencing the prior distribution.
For a conjugate prior the posterior can be written as
π(ξ|y) = π(ξ|ζ + R(y), λ + 1).
54

3.2 Prior distributions
In this case the posterior distribution just “updates” the prior parameters. Robert &
Casella [2004] pointed out that using conjugate priors for computational reasons might
introduce eﬀects unrelated to reality on the inference process. Hence, in this thesis we
frequently use non-informative priors1
π(ξ) ∝1S(ξ)
on some support set S ⊆Rn. Although the choice of a ﬁnite support S already in-
troduces prior information on ξ, the restriction is usually very mild: Rate constants
of biological systems are generally considered to be non-negative and setting an upper
bound can also be easily done without considerably inﬂuencing the posterior distribu-
tion.
Interestingly, we can choose any non-negative function π : Rn −→R with
Z
Rn π(ξ) dξ = c
for a constant c ∈(0, ∞] as prior distribution, as long as the marginal likelihood fulﬁlls
π(y) =
Z
Rn L(ξ|y)π(ξ) dξ < ∞
(3.7)
almost surely with respect to y (Marin & Robert [2007]). This becomes clear as c is
independent of ξ, recalling that we are only interested in the proportionality (3.3). In
case c ̸= 1, we call π(ξ) improper and proper otherwise.
In summary, while the use of non-informative priors might be necessary for unknown
parameter values, there needs to be a suﬃcient amount of observations in order to gain
valuable inference results. The term “suﬃcient amount” is generally hard to specify and
whenever possible the modelers expertise, information from related experiments or the
literature should guide the choice in prior selection. Moreover, the use of very distinct
priors can help weighting down the inﬂuence of the data in the posterior distribution
(Bernardo et al. [1994]), a situation possibly aspired for error prone observations. In
general we make use of independent prior distributions, i.e. π(ξ) = Qn
i=1 π(ξi), for the
marginal density functions π(xi) of ξi. This, however, is not a necessity!
1The notion of non-informative priors is only vaguely deﬁned in the literature. Essentially, the
inﬂuence of the prior distribution needs to be as small as possible (Marin & Robert [2007]). Therefore,
although there are other prior types with little inﬂuence on the posterior, we stick to the deﬁnition via
the indicator function.
55

3. BAYESIAN MODEL INFERENCE
3.3
Bayesian parameter identiﬁability
The MAP of high dimensional dynamical systems is oftentimes the center of inter-
est when performing parameter inference.
However, especially for non-informative
priors π(ξ) = 1S(ξ) this might inherit some issues as seen in the structural non-
identiﬁability Example 2.12.
While for uni- or bivariate posterior distributions a
graphical visualization of the posterior density can help to identify these issues this
approach is not possible for higher dimensional systems.
However, analogously to
Meeker & Escobar [1995], exchanging the squared error loss function (2.32) by twice
the negative logarithm −2 log(π(ξ|y)) of the posterior density function π(ξ|y) natu-
rally extends the identiﬁability analysis of Chapter 2.5.3 to the Bayesian paradigm:
We call a dynamical system identiﬁable with respect to the MAP or simply identiﬁable,
if for i = 1, . . . , n the conﬁdence intervals [li, ui] (of the ith component of the esti-
mate ˆξMAP ) deﬁned by li := min{ξi|∃ξ, s.t. 2 log(π(ˆξMAP |y)) −2 log(π(ξ|y)) < ∆α}
and ui := max{ξi|∃ξ, s.t. 2 log(π(ˆξMAP |y)) −2 log(π(ξ|y)) < ∆α} (if existent and
−∞/ + ∞otherwise), is ﬁnite. With this, structural and practical identiﬁability can
also be deﬁned as in Chapter 2.5.3. Note that the posterior distribution only needs to
be known up to a scaling constant as scaling constants cancel out in the diﬀerences of
the deﬁnitions of the li’s and ui’s. For Gaussian noise functions the Bayesian deﬁnition
of identiﬁability coincides with the deﬁnition in Chapter 2.5.3.
3.4
Bayes factors
We now turn to the issue of Bayesian model selection. Generally, statistical modeling
of any kind of data crucially depends on the modeler’s expertise in model construction.
Inferring parameter values based on the wrong model can have severe eﬀects on the
outcome of the model’s predictions.
Towards this end the task of model selection
has ever since been a very important step in the process of data analysis.
A well
established approach to address this problem is the likelihood ratio test. For general
applications it is based on the ratio of the maximal likelihood value of a model with
restricted parameter space and the maximal likelihood value of the same model on
the full parameter space (Casella & Berger [2001]). Since the latter always performs
56

3.4 Bayes factors
at least as good as the former one, the test analyzes how much more likely the full
model compared to the restricted one is with respect to the data. However, the need
to restrict the parameter space limits this approach to nested models only, i.e. the
simpler model can be transformed into the more complex one by the introduction of
additional parameters. A more sophisticated test for non-nested models was established
by Vuong (Vuong [1989]). As the likelihood ratio test, the Vuong test is also based on
the likelihood ratio of two competing models, but additional corrects by a Kulback-
Leibler information criterion driven term. In any case, all of the approaches above
suﬀer from two major drawbacks: (i) they are based on two single maximally likely
values – one for each model – not taking into account any kind of uncertainty in the
parameters and, moreover, (ii) due to the maximality restriction the test statistic is
deﬁned on the probability of extreme events, anticipating that the data produces events
at least as extreme as the tested ones.
The Bayesian way of model selection – the second strain in Bayesian model inference
– naturally circumvents these problems by taking into account full parameter distri-
butions rather than single maximal parameter values. In the following, we introduce
the concept of Bayesian model selection including a practical approach for comput-
ing the so-called Bayes factor, a statistic used for pairwise model comparison. A nice
comprehensive discussion about this topic is given in Kass & Raftery [1995].
As we have seen in Section 3.1, the posterior distribution π(ξ|y) contains the full
structural dependency of all parameters involved in the model.
In Bayesian model
selection we simply extend π(ξ|y) by an additional model parameter, this is, given
a set of (possibly non-nested) models M1, . . . , Mk with according parameter vectors
ξ1 ∈Rn1, . . . , ξk ∈Rnk we consider the distribution deﬁned by
π(ξi, Mi|y) =
Li(ξi|y)π(ξi, Mi)
Pk
j=1
R
Rnj Lj(ξj|y)π(ξj, Mj) dξj
(3.8)
where Li(ξi|y) denotes the likelihood function and π(ξi, Mi) the joint prior density for
model Mi and parameter vector ξi. Clearly, conditioning Equation (3.8) on model Mi
gives the relation (3.3), where the proportionality is to be understood with respect to
the data and model.
57

3. BAYESIAN MODEL INFERENCE
In the case of two models M1 and M2, we – similarly to classical hypothesis testing –
get the posterior odds ratio via integration over the corresponding parameter spaces:
π(M1|y)
π(M2|y) =
R
Rn1 π(ξ1, M1|y) dξ1
R
Rn2 π(ξ2, M2|y) dξ2
=
 
π(M1)
R
Rn1 L1(ξ1|y)π(ξ1|M1) dξ1
P2
j=1 π(Mj)
R
Rnj Lj(ξj|y)π(ξj|Mj) dξj
!
·
 
π(M2)
R
Rn2 L2(ξ2|y)π(ξ2|M2) dξ2
P2
j=1 π(Mj)
R
Rnj Lj(ξj|y)π(ξj|Mj) dξj
!−1
= π(y|M1)
π(y|M2)
π(M1)
π(M2),
(3.9)
where
π(y|Mj) =
Z
Rnj Lj(ξj|y)π(ξj|Mj) dξj = Eπ(ξj|Mj)[Lj(ξj|y)]
(3.10)
(j = 1, 2) is the model evidence of model Mj deﬁned in Equation (3.2). The expression
B12 := π(y|M1)
π(y|M2)
(3.11)
from Equation (3.9) is called Bayes factor of model M1 versus M2. We have to em-
phasize that contrary to Bayesian parameter inference the Bayes factor can not handle
improper prior distributions. This can be seen as follows: Assume (without loss of
generality) we are given the improper prior distributions π(ξ1|M1) for model M1. Let
π(y|M1) be the marginal distribution corresponding to model M1 and c > 0 an arbi-
trary constant. Then π∗(ξ1|M1) = c · π(ξ1|M1) is a valid prior distribution and
π∗(y|M1) =
Z
Rn1
L1(ξ1|y)π∗(ξ1|Mj) dξ1 = c · π(y|M1).
Hence, Bayes factors are not well deﬁned for improper prior distributions as they can
be varied by arbitrary constants.
Typically we do not favor any model a priori. The prior odds ratio π(M1)/π(M2) is then
simply one and the Bayes factor coincides with the ratio of the posterior probabilities
of model M1 and M2. As Kass & Raftery [1995] pointed out, it is possible for nested
models to avoid specifying the prior density functions π(ξi, Mi) from Equation (3.8)
using the so-called Schwarz criterion
SC = log(π(y|ˆξ1, M1)) −log(π(y|ˆξ2, M2)) −1
2(n1 −n2) log(N)
58

3.4 Bayes factors
where log is the natural logarithm, N the number of samples used to compute SC, ˆξj
the MLE of model j, and nj the dimension of ˆξj, respectively. For N →∞
SC −log(B12)
log(B12)
→0.
Hence, exp(SC) is an approximation of the Bayes factor B12. Although, the approxi-
mation is not very accurate even for large sample sizes N, a famous statistic derived
from the Schwarz criterion is the Bayesian information criterion (BIC). It is given by
BIC = −2SC. Harold Jeﬀreys established a widely used interpretation of the Bayes
factor in Jeﬀreys [1961]. He suggested to classify the evidence in favor of model M1 by
log10-half-scale units as:
log10(B12)
B12
Evidence in favor of model M1
0 – 0.5
1 – 3.2
Not worth more than a bare mention
0.5 – 1.0
3.2 – 10
Substantial
1.0 – 1.5
10 – 32.6
Strong
1.5 – 2.0
32.6 – 100
Very strong
2.0 – ∞
100 – ∞
Decisive
This grouping is known as Jeﬀreys’ scale of evidence. Certainly some applications chal-
lenge this classiﬁcation (see e.g. Evett [1991]). Nevertheless, Jeﬀreys’ scale of evidence
is well established and widely used in the Bayesian community.
The Bayes factor has several advantages compared to classical odds ratio tests: Due
to its construction it naturally holds both the evidence in favor of model M1 and the
evidence in favor of model M2. The latter is simply given by
B21 = π(y|M2)
π(y|M1) =
1
B12
as we did not make any assumptions about the parameter space of model M1 and M2,
i.e. the Bayes factor is able to handle non-nested models. Furthermore, for k models
M1, . . . , Mk with uniform model prior densities π(Mj) (j = 1, . . . , k) and Bayes factors
B1j = π(y|M1)
π(y|Mj)
we have the posterior probability that an observation y originates from model Mi given
by (c.f. Kass & Raftery [1995])
π(Mi|y) =
Bi1
1 + Pk
j=2 Bj1
.
59

3. BAYESIAN MODEL INFERENCE
A huge advantage of Bayes factors is their ability to naturally correct for overﬁtting
issues (Lodewyckx et al. [2011]; Myung & Pitt [1997]; Pitt et al. [2002]). Since we
marginalize rather than maximize with respect to the according parameter spaces, the
Bayes factor can compensate for areas that extraordinarily pander a speciﬁc model (see
Spiegelhalter & Smith [1982] as well as Jeﬀerys & Berger [1992] and references therein).
Although there will never be any “certainty” we picked the “true” model, the process
of model selection is still an important issue and draws lots of interest. Naturally, there
has been extensive research on the computation of the marginal likelihood (see e.g.
Kass & Raftery [1995]; Lartillot & Philippe [2006]; Newton & Raftery [1994] or Friel
& Pettitt [2008]) in settings where no analytical solution is tractable. In the following
we give a brief overview about the most prominent approaches. All of them are based
on sampling – i.e. generating a number of realizations of– the parameter vector ξj of
model Mj.
3.4.1
The prior arithmetic mean estimate
The simplest approach to approximate a marginal likelihood π(y) – for reasons of
clarity we drop the model dependency for now – is based on drawing a total of T
samples ξ(1), . . . , ξ(T) i.i.d.
∼π(ξ) from the prior distribution π(ξ). Equation (3.10) then
suggests
π(y) = Eπ(ξ)[L(ξ|y)] ≈1
T
T
X
j=1
L(ξ(j)|y),
(3.12)
where L(ξ|y) denotes the likelihood function. The right hand side of Equation (3.12)
is known as the prior arithmetic mean estimate (Lartillot & Philippe [2006]). When
the number of samples tend to inﬁnity the strong law of large numbers (almost surely)
guarantees convergence. However, in practical applications with complex and often
times spiky posterior distributions the prior arithmetic mean estimate can be very in-
eﬃcient as many samples might fall in regions with comparatively low likelihood values
(Gamerman & Lopes [2006]). A large number of samples is needed for accurate results.
Especially in high-dimensional systems this issue aggravates. Thus, the application of
the prior arithmetic mean estimate can aﬀord high computational power in order to
obtain acceptable results (Lewis [1994]).
60

3.4 Bayes factors
3.4.2
The posterior harmonic mean estimate
Newton & Raftery [1994] suggested an alternative to the prior arithmetic mean esti-
mator: Instead of sampling from the prior distribution, they proposed to sample from
the posterior directly. Explicitly, for the samples ξ(1), . . . , ξ(T) i.i.d.
∼
π(ξ|y) from the
posterior distribution π(ξ|y), the posterior harmonic mean estimate is deﬁned via the
right hand side of
π(y) ≈

1
T
T
X
j=1
1
L(ξ(j)|y)


−1
(3.13)
where L(ξ|y) is the likelihood with respect to the observations y. To see the relation
in (3.13), we recall that the samples are drawn from the posterior distribution. Fur-
thermore, the expectation of the inverse of the likelihood function with respect to the
posterior distribution is given by
Eπ(ξ|y)

1
L(ξ|y)

=
Z
Rn
1
L(ξ|y)π(ξ|y) dξ
=
Z
Rn
1
L(ξ|y)
L(ξ|y)π(ξ)
π(y)
dξ
=
1
π(y)
Z
Rn π(ξ) dξ
=
1
π(y),
which leads directly to the approximation of Equation (3.13) (remember that all prior
distributions should be proper). The strong law of large numbers guarantees almost
sure convergence. Intuitively, this approach circumvents the aforementioned problem
of spiky likelihood functions as the samples are generated by means of this spiky dis-
tribution itself. However, the posterior harmonic mean estimate suﬀers severe variance
issues, which can be seen from the following simple example given by Neal [2008].
Example 3.1 (Neal [2008]). Suppose we are given the single data point y ∼N(ξ, σ2
1)
with a posterior distribution π(ξ|y) for a parameter ξ ∈R.
We want to infer the
marginal likelihood π(y) taking the prior distribution ξ ∼N(0, σ2
2) and assuming that
the variances σ2
1 and σ2
2 are known. This is a conjugate prior as we will see below. The
posterior density function is then for
σ :=
 1
σ2
1
+ 1
σ2
2
−1
2
61

3. BAYESIAN MODEL INFERENCE
given by
π(ξ|y) =
1
π(y)
1
√
2πσ1
exp

−1
2σ2
1
(ξ −y)2

·
1
√
2πσ2
exp

−1
2σ2
2
ξ2

=
1
π(y)
1
2πσ1σ2
exp

−1
2
 ξ2
σ2
2
+ ξ2
σ2
1
−2ξ y
σ2
1
+ y2
σ2
1

= Z(y)
π(y)
1
√
2πσ exp
 
−1
2σ2

ξ −yσ2
σ2
1
2!
(3.14)
where
Z(y) =
σ
√
2πσ1σ2
exp
 
−1
2
 y2
σ2
1
−y2σ2
σ4
1
2!
=
1
p
2π(σ2
1 + σ2
2)
exp

−
1
2(σ2
1 + σ2
2)y2

.
(3.15)
The calculation yields two things: (i) Equation (3.14) shows that the posterior distribu-
tion for ξ given y is N

yσ2
σ2
1 , σ2
, which allows us to sample directly from the posterior;
(ii) Integrating both sides of Equation (3.14) over R yields Z(y) = π(y). According to
Equation (3.15) the true marginal likelihood for y can be computed via the probability
density function corresponding to N
 0, σ2
1 + σ2
2,

.
Now, as we have inferred all relevant terms, let us assume
y = 1, σ2
1 = 1 and σ2
2 = 100.
For T = 106 random samples from the posterior N
  100
101, 100
101

the estimated mean (in-
cluding one standard error) for the marginal likelihood based on R =1,000 runs in
Matlab is 0.0946±3.22·10−7. The harmonic mean constantly overestimates the “true”
value of π(y = 1) = 0.0395 more than twice. With increasing σ2
2 this gap increases.
The corresponding prior arithmetic mean estimate for T = 106 samples from the prior
N(0, 101) is 0.0395 ± 1.06 · 10−11 and approximates the “true” value extraordinarily
well.
Newton & Raftery [1994] also proposed a weighted combination of the prior arithmetic
mean estimator and posterior harmonic mean estimator called the stabilized harmonic
mean estimator. This helps to reduce the issues of the individual estimators.
62

3.4 Bayes factors
3.4.3
Thermodynamic integration
Contrary to the two approaches described above we employ a method based on path
sampling (Gelman & Meng [1998]) in this thesis: The principle of thermodynamic inte-
gration for marginal likelihood estimation was introduced by Lartillot & Philippe [2006]
and Friel & Pettitt [2008] and applied to problems from systems biology by Calderhead
& Girolami [2009]. These methods are founded on the integral representation of the
natural logarithm of the marginal likelihood by means of the power posterior
πt(ξ|y) = L(ξ|y)tπ(ξ)
πt(y)
,
(3.16)
where t ∈[0, 1] and for the prior π(ξ)
πt(y) =
Z
Rn L(ξ|y)tπ(ξ) dξ.
(3.17)
Note that the power posterior πt(ξ|y) is truly a probability density function. For t = 0
and t = 1 the marginal π0(y) is equal to one and π1(y) is the marginal likelihood; for
any other t, πt(y) is weighing the inﬂuence of the data y on the posterior πt(ξ|y) via
the inﬂuence of the likelihoods L(ξ|y)t. Taking the derivative of the logarithm of πt(y)
with respect to t yields
d
dt log(πt(y)) =
1
πt(y)
d
dtπt(y)
=
1
πt(y)
Z
Rn
d
dtL(ξ|y)tπ(ξ) dξ
=
Z
Rn log(L(ξ|y))L(ξ|y)tπ(ξ)
πt(y)
dξ
= Eπt(ξ|y)[log(L(ξ|y))].
The thermodynamic integral is then given by integration of t on [0, 1] as
log(π(y)) =
Z 1
0
Eπt(ξ|y)[log(L(ξ|y))] dt.
(3.18)
This approach tackles the problem of spiky likelihoods by considering a path (from
0 to 1) that gradually puts more and more weight on the likelihood function.
An
illustrative example for the two-dimensional posterior distribution of Figure 1.1 of
the introduction (depicting the k1-k6 marginalized posterior distribution of the JAK1-
STAT3 model (7.1)) is shown in Figure 3.1. Unfortunately, Equation (3.18) can only
63

3. BAYESIAN MODEL INFERENCE
t=0	  
t=0.25	  
t=0.5	  
t=0.75	  
t=1	  
t=0.1	  
Figure 3.1: Example path for thermodynamic integration based on the posterior distri-
bution of the JAK1-STAT3 model (7.1) introduced in Chapter 7 marginalized on the k1
and k6 dimension.
64

3.4 Bayes factors
be solved analytically in the most simplest situations.
In general log(π(y)) is ob-
tained by numerical integration using a ﬁnite number of evaluations between t = 0 and
t = 1 applying the trapezium rule (Friel & Pettitt [2008]). Using the discretization
0 = t0 < t1 < . . . < tT = 1 Calderhead & Girolami [2009] have shown that
log(π(y)) =1
2
T−1
X
i=0
(ti+1 −ti)

Eπti+1(ξ|y)[log(L(ξ|y))] + Eπti(ξ|y)[log(L(ξ|y))]

+ 1
2
T−1
X
i=0
 KL(πti(ξ|y)||πti+1(ξ|y)) −KL(πti+1(ξ|y)||πti(ξ|y))

(3.19)
where
KL(πti(ξ|y)||πti+1(ξ|y)) =
Z
Rn πti(ξ|y) log
 πti(ξ|y)
πti+1(ξ|y)

dξ
is the Kullback-Leibler divergence between πti(ξ|y) and πti+1(ξ|y) (Kullback & Leibler
[1951]). It is non-negative and can be seen as measure for the asymmetric diﬀerence
between the two distributions as in the limit KL(πti(ξ|y)||πti+1(ξ|y)) →0 whenever
ti →ti+1. Equation (3.19) can easily be obtained based on the relation
L(ξ|y)ti+1
L(ξ|y)ti πti(ξ|y) = L(ξ|y)ti+1π(ξ)
L(ξ|y)tiπ(ξ) · L(ξ|y)tiπ(ξ)
πti(y)
= πti+1(y)
πti(y) πti+1(ξ|y).
(3.20)
With t ∈[0, 1] we get
log(π(y)) = log(π1(y)) −log(π0(y)) =
T−1
X
i=0
log
πti+1(y)
πti(y)

=
T−1
X
i=0
Z
Rn log
πti+1(y)
πti(y) πti+1(ξ|y)

πt(ξ|y) dξ −
Z
Rn log(πti+1(ξ|y))πt(ξ|y) dξ

=
T−1
X
i=0
Z
Rn log
L(ξ|y)ti+1
L(ξ|y)ti πti(ξ|y)

πt(ξ|y) dξ −
Z
Rn log(πti+1(ξ|y))πt(ξ|y) dξ

=
T−1
X
i=0
Z
Rn log
L(ξ|y)ti+1
L(ξ|y)ti

πt(ξ|y) dξ +
Z
Rn log
 πti(ξ|y)
πti+1(ξ|y)

πt(ξ|y) dξ

.
Applying the trapezium rule for t = ti and t = ti+1 yields Equation (3.19). Naturally,
this leads to the approximation
log π((y)) ≈
T−1
X
i=0
1
2(ti+1−ti)

Eπti+1(ξ|y)[log(L(ξ|y))] + Eπti(ξ|y)[log(L(ξ|y))]

, (3.21)
65

3. BAYESIAN MODEL INFERENCE
as the second expression involving the Kullback-Leibler divergence introduces small er-
rors for πti+1 ≈πti only. This approximation coincides with the one in Friel & Pettitt
[2008]. The Kullback-Leibler term can be seen as bias for discretely approximating
the integral in (3.18). Taking the expectation in (3.21) at each evaluation index ti is
adding to the numerical stability of the marginal likelihood estimate. Nevertheless, for
estimating the expectation Eπt(ξ|y)[log(L(ξ|y))] at an arbitrary index t, a series of sam-
ples drawn from the power posterior πt(ξ|y) is necessary. This makes thermodynamic
integration computationally expensive. Hence, the discretization schedule of the unit
interval is of great relevance in order to quickly obtain numerically stable estimations
for the marginal likelihood: Friel & Pettitt [2008] propose a power law like division of
the unit interval via
ti = (i/T)c
(3.22)
(i = 0, . . . , T) where T ∈N and c > 0. Calderhead & Girolami [2009] then show that
this scheme also minimizes the Kullback-Leibler bias for linear regression models in
the approximation of the logarithm of the marginal likelihood. The application to dy-
namical systems yielded good results in Calderhead & Girolami [2009] which is why we
mostly use their proposals of T = 30 and c = 5 in this thesis. Although thermodynamic
integration is computationally more expensive than the methods in Chapters 3.4.1 and
3.4.2, it performs well on most statistical models including diﬀerential equations based
systems (Calderhead & Girolami [2009]).
3.4.4
Example: A Gaussian mixture model
We now apply the techniques introduced above to a two component Gaussian mix-
ture model.
This rather simple setting is analytically tractable and we are able to
depict and practically verify the diﬀerent concepts.
Suppose we are given observa-
tions y = {y1, . . . , ym} with y1, . . . , ym ∈R, such that y1, . . . , ym1
i.i.d.
∼N(µ1, σ2) and
ym1+1, . . . , ym
i.i.d.
∼
N(µ2, σ2). For known variance σ2 our two competing models are
deﬁned by
(i) a model with µ := µ1 = µ2, designated M1. This essentially is a single univariate
normal distribution model with y1, . . . , ym
i.i.d.
∼N(µ, σ2) for the mean µ = µ1 = µ2
and standard deviation σ.
66

3.4 Bayes factors
(ii) a model with possibly µ1 ̸= µ2, designated M2,
This leaves us with one free parameter µ for model M1 and two free parameters µ1 and
µ2 for model M2. The prior distributions are chosen to be conjugate with µ = µ1 =
µ2 ∼N(0, σ2) in M1 and µ1 ∼N(2, σ2) and µ2 ∼N(−2, σ2) in M2. The likelihood for
both models is
L(µ1, µ2|y) =

1
√
2πσ
m
exp

−1
2σ2


m1
X
i=1
(yi −µ1)2 +
m
X
j=m1+1
(yj −µ2)2




yielding after some simple calculations (Appendix C.1 and C.2) the posterior distribu-
tions
(i) N
 Pm
i=1 yi
m+1 ,
σ2
m+1

for model M1,
(ii) N2




2+Pm1
i=1 yi
m1+1
−2+Pm
j=m1+1 yj
m−m1+1

,
 
σ2
m1+1
0
0
σ2
m−m1+1
!
for model M2.
In order to compute the Bayes factor, we need the marginal likelihoods π(y|M1) and
π(y|M2). As computed in Appendix C.1 for ¯y = 1
m
Pm
i=1 yi:
π(y|M1) =
1
√m + 1

1
√
2πσ
m
exp
 
1
2σ2
 
m2¯y2
m + 1 −
m
X
i=1
y2
i
!!
.
Similarly, (c.f. Appendix C.2) we have for ¯y1 =
1
m1
Pm1
i=1 yi and ¯y2 =
1
m2
Pm
j=m1+1 yj,
where m2 = m −m1
π(y|M2) =
1
p
(m1 + 1)(m2 + 1)

1
√
2πσ
m
· exp
 
−1
2σ2
 m
X
i=1
y2
i + 8 −(m1¯y1 + 2)2
m1 + 1
−(m2¯y2 −2)2
m2 + 1
!!
.
The Bayes factor B21 for model M2 versus model M1 is hence
B21 = π(y|M2)
π(y|M1)
(3.23)
=
√m + 1
√m1 + 1√m2 + 1 exp

−1
2σ2

8 + (m¯y)2
m + 1 −(m1¯y1 + 2)2
m1 + 1
−(m2¯y2 −2)2
m2 + 1

.
Recall that the Bayes factor for M1 versus M2 is simply B−1
12 . For the use in ther-
modynamic integration we exemplary compute the expected value of the log-likelihood
67

3. BAYESIAN MODEL INFERENCE
log L(µ|y) of model M1 under the power posterior πt(µ|y, M1). As shown in Appendix
C.3 we have
Eπt(µ|y,M1) (log L(µ|y)) =
Z
R
log L(µ|y)
L(µ, |y)tπ(µ)
R
R L(µ, |y)tπ(µ) dµ dµ
= −1
2σ2
(  m
X
i=1

y2
i + 2σ2 log(
√
2πσ)
!
+ mσ2 −2¯y2m2t
mt + 1
+
m3¯y2t2
(mt + 1)2
)
.
The computations for model M2 are very similar, but a lot more cumbersome. They
hold no more insights and we leave this to the considerate reader.
Now, setting σ2 = 1 and sampling ten observations
y1, . . . , y5
i.i.d.
∼N(1, 12)
and
y6, . . . , y10
i.i.d.
∼N(−1, 12)
the prior arithmetic mean estimate (including one standard error) of the Bayes factor
B21 based on ten runs with 100,000 samples per model each was 77.68 ± 0.11. This
is close to the true value of 77.47 computed via Equation (3.23). On the other hand,
the corresponding posterior harmonic mean estimate (211.50 ± 0.08) overestimated the
true value quite strongly. Using the power law division ti = (i/T)c with c = 5 and
T = 25, we also computed B21 applying thermodynamic integration. At each ti, we
used 4,000 samples to estimate Eπt(·|y,·) (log L(·|y)), again resulting in a total of 100,000
samples per model for the approximation of B21. Thermodynamic integration yielded
the closest result of 77.34±0.16. The average expected value of Eπt(µ|y,M1) (log L(µ|y)),
−25.40, was very closely approximated by −24.77 ± 10−2.
68

4
Markov Chain Monte Carlo
(MCMC) methods
We have seen in the previous chapter that sampling from some posterior distribution
π(ξ|y) lies at the core of Bayesian inference. Up to this point, we were able to directly
draw from π(ξ|y) using e.g. conjugate prior distributions.
However, in most appli-
cations π(ξ|y) is not a standard sampling distribution and we have to turn to more
advanced techniques.
A solution to this problem is given by Markov Chain Monte
Carlo (MCMC) methods: As the name implies, MCMC methods attempt to generate
a Markov chain directly drawing from some complex posterior distribution. With the
advent of MCMC methods Bayesian inference has skyrocketed in various ﬁelds of sci-
ence and is likely to continue spreading in the future. One of the most successful and
inﬂuential (Beichl & Sullivan [2000]; Wilkinson [2006]) algorithms was developed by
Metropolis and Hastings (Hastings [1970]; Metropolis et al. [1953]). In the following
we ﬁrst introduce the basic version of the Metropolis-Hasting (MH) algorithm. Subse-
quently, Chapter 4.5 introduces its direct application to model selection. Interestingly,
the MH algorithm can also be applied for optimization problems via simulated an-
nealing (Chapter 4.6). Finally, we address the issues of dependency and convergence
diagnostics of the Markov chains generated in Chapters 4.2 to 4.4.
69

4. MARKOV CHAIN MONTE CARLO (MCMC) METHODS
4.1
The Metropolis-Hastings (MH) algorithm
Historically the Metropolis-Hastings algorithm was introduced by Metropolis and Hast-
ings in Metropolis et al. [1953] and Hastings [1970] for integration of complex functions
by random sampling: The integral
Z b
a
h(ξ) dξ
for some function h : Rn −→R can be computed by decomposing h(ξ) into the product
f(ξ)π(ξ), where f : Rn −→R is a function deﬁned over (a, b) and π(ξ) a probability
density function on (a, b). The integral is then
Z b
a
h(ξ) dξ =
Z b
a
f(ξ)π(ξ) dξ = Eπ(ξ)[h(ξ)].
Now, given a number of samples ξ(0), . . . , ξ(T) of π(ξ), Monte Carlo integration approx-
imates
Z b
a
h(ξ) dξ = Eπ(ξ)[h(ξ)] ≈1
T
T
X
i=1
f(ξ(i)).
We already applied this principle for the prior harmonic mean estimate (see Equation
(3.13)), where f was the likelihood function. For sampling from the density function
π(ξ), Metropolis and Hastings used the algorithm depicted in Algorithm 2. Here, the
realization {ξ(j)}j=0,...,T of a Markov chain {X(t)}t∈N0 is sampled as follows: In each
iteration the algorithm generates a proposal ξp according to some transition density
function q(ξ|ξ(j)) that (possibly) depends on the current element ξ(j) of the Markov
chain. It is accepted with the Metropolis-Hastings acceptance probability
α(ξp|ξ(j)) = min
(
π(ξp)q(ξ(j)|ξp)
π(ξ(j))q(ξp|ξ(j))
, 1
)
.
(4.1)
If ξp is accepted, the Markov chain element ξ(j+1) is deﬁned by ξ(j+1) = ξp and by
ξ(j+1) = ξ(j) otherwise. The transition density function q(ξ|ξ′) is also called proposal
function and, like the density function π(ξ), only needs to be explicitly available up to
a multiplicative constant independent of ξ′; this is due to the fact that these constants
cancel out in the Metropolis-Hastings acceptance probability. We call the percentage
of accepted MH steps the acceptance rate of a realization ξ(0), . . . , ξ(T).
70

4.1 The Metropolis-Hastings (MH) algorithm
Algorithm 2: The Metropolis-Hastings algorithm
Input: Initial value ξ(0) ∈Rn, (transition) density function q(ξ|ξ′) on Rn such that q(ξ|ξ(0))
exists, arbitrary target density function π : Rn −→R, and chain length T ∈N.
Output: Markov chain realization {ξ(j)}j=0,...,T .
for j ←0 to T −1 do
Generate proposal ξp ∼q(ξ|ξ(j))
Set
ξ(j+1) ←



ξp
with probability α(ξp|ξ(j)),
ξ(j)
with probability 1 −α(ξp|ξ(j)),
where
α(ξp|ξ(j)) = min
(
π(ξp)q(ξ(j)|ξp)
π(ξ(j))q(ξp|ξ(j))
, 1
)
.
In Bayesian model inference we use the MH algorithm for sampling from complex
posterior distributions π(ξ|y).
Since the algorithm is quite general, we impose the
regularity condition q(ξ|ξ′) > 0 for all ξ, ξ′ in the support of π(ξ|y) in order to avoid
convergence issues. This means the proposal function q(ξ|ξ′) allows to proceed from
any point to any other point on the support of π(ξ|y).
Theorem 4.1 (Convergence of the MH algorithm ). Let {X(t)}t∈N0 be a Markov chain
governed by Algorithm 2 with target density function π(ξ). Suppose the proposal func-
tion q(ξ|ξ′) fulﬁlls the regularity condition. Then π is the equilibrium distribution of
{X(t)}t∈N0.
Proof. According to Theorem 2.5 we have to show that {X(t)}t∈N0 is (i) π-irreducible,
(ii) Harris recurrent and (iii) aperiodic with (iv) invariant distribution π.
(i) Due to the regularity condition for q, (i) is naturally satisﬁed.
(ii) According to Lemma 7.3 in Robert & Casella [2004] π-irreducibility of {X(t)}t∈N0
also implies Harris recurrence (a proof involves further theory on bounded har-
monic functions and tail events, which, however, is beyond the scope of this thesis.
Nevertheless, the interested reader may be referred to Robert & Casella [2004] or
Nummelin [2004]).
71

4. MARKOV CHAIN MONTE CARLO (MCMC) METHODS
(iii) The aperiodicity condition follows as for each t ∈N, X(t) = X(t+1) with positive
probability, this is, the Markov chain can stay in place in every step.
(iv) For proving invariance, we show that the kernel corresponding to {X(t)}t∈N0 ful-
ﬁlls the detailed balance condition: The transition kernel density function associ-
ated with {X(t)}t∈N0 is given by
k(ξ′|ξ) = α(ξ′|ξ)q(ξ′|ξ) + r(ξ)1ξ(ξ′),
where for the the support of S of π, r(ξ) = 1 −
R
S q(ξ′|ξ) dξ′, as introduced in
Chapter 2.3. Clearly,
r(ξ)1ξ(ξ′)p(ξ) = r(ξ′)1ξ′(ξ)p(ξ′)
for any two realizations ξ and ξ′ of X(t) and X(t+1) (t ∈N0).
Furthermore,
without loss of generality let α(ξ′|ξ) < 1 (while the case α(ξ′|ξ) = 1 is trivial,
exchanged roles of ξ and ξ′ yield the case α(ξ|ξ′) < 1). Then
α(ξ′|ξ) = π(ξ′)q(ξ|ξ′)
π(ξ)q(ξ′|ξ)
and
α(ξ|ξ′) = 1.
Hence,
α(ξ′|ξ)q(ξ′|ξ)π(ξ) = π(ξ′)q(ξ|ξ′)
π(ξ)q(ξ′|ξ) q(ξ′|ξ)π(ξ)
= q(ξ|ξ′)π(ξ′)
= α(ξ|ξ′)q(ξ|ξ′)π(ξ′)
and k(ξ′|ξ) satisﬁes the detailed balance condition. Theorem 2.4 now provides
that π is an invariant distribution for the Markov chain, which ﬁnalizes the proof.
A very popular choice for the proposal density q(ξ|ξ′) is the n-dimensional normal
distribution Nn(ξ′, Σ) with mean ξ′ and covariance matrix Σ. In each MCMC step
a new proposal is generated based on the current sample ξ(c) as ξp = ξ(c) + ε, where
ε ∼Nn(0, Σ). We refer to this scheme as Random Walk Metropolis-Hastings (RWMH)
algorithm. In case there is no knowledge about the covariance structure of the pa-
rameters, Σ = kRW In is typically chosen, where kRW is the step-size tuning or scaling
parameter and In the n-dimensional identity matrix. If the proposal function in each it-
eration is independent of the current sample, i.e. if q(ξ|ξ′) = q(ξ), the sampling scheme
72

4.1 The Metropolis-Hastings (MH) algorithm
200 400 600 8001000
0.4
0.5
0.6
0.7
index
sample value
(a)
0
0.5
1
1.5
0
1
2
3
4
 
probability density
 
 
samples
posterior
(b)
200 400 600 8001000
0.4
0.5
0.6
0.7
index
sample value
(c)
0
0.5
1
1.5
0
1
2
3
4
 
probability density
 
 
samples
posterior
(d)
Figure 4.1: (a) Realization of a Markov chain generated by the random walk proposal
function q1; (b) corresponding histogram for the samples in (a) and “true” posterior density.
(c) Realization of a Markov chain generated by the independence chain proposal function
q2; (d) corresponding histogram for the samples in (c) and “true” posterior density.
is called an Independence chain Metropolis-Hastings (IMH) algorithm. There are gen-
erally no limitations regarding the choice of q as long as it is positive on the support of
the posterior distribution π(ξ|y). Its choice however is very crucial for the performance
of the algorithm: An eﬃcient MH algorithm shows high acceptance rates for the pro-
posed samples at simultaneously low autocorrelation between generated Markov chain
elements (see Chapter 4.3). Especially in high dimensions this is hard to attain, because
small update step sizes result in high acceptance rates, but also in highly correlated
Markov chain samples and vice versa.
Example 4.1 (Mean of a normal distribution). Suppose we want to generate the real-
ization of a Markov chain from the posterior distribution of the one-component model
M1 of Chapter 3.4.4 using the MH algorithm with (i) a random walk proposal func-
tion q1 and (ii) an independence proposal function q2. Given the i.i.d. observations
y1, . . . , y10 ∼N(µ = 0, 12) and the prior distribution N(0, 12) for the parameter µ the
posterior distribution is then given by N
 P10
i=1 yi
11
, 1
11

(Chapter 3.4.4).
Starting at
ξ(0) = 0.5 we generate the proposal ξp based on the current Markov chain sample ξ(c)
as
ξp ∼N(ξ(c), 0.52) in case of q1
and
ξp ∼N(1, 12) in case of q2.
Figures 4.1(a) and 4.1(c) hold 1,000 realizations for q1 and q2, respectively.
The
according histograms of these realizations as well as the “true” posterior distribution
and depicted in Figures 4.1(b) and 4.1(d). Note that our samples are no independent
realizations of the posterior distribution as the Markov chain inherits some intrinsic
dependency.
73

4. MARKOV CHAIN MONTE CARLO (MCMC) METHODS
4.2
Independent identically distributed samples from a
Markov chain
From a general point of view, we try to draw i.i.d. samples from a density π by means
of a Markov chain {X(t)}t∈N0. At ﬁrst sight, this seems to be a conﬂicting goal as any
realization of {X(t)}t∈N0 can at best only approximate π and generating independent
samples based on a dependent process seems unintuitive. However, in order to address
the dependency issue we can make use of the limited memory of the Markov chain
in combination with the so-called Chapman-Kolmogorov equations (see Papoulis et al.
[1965] for a proof):
Lemma 4.1 (Chapman-Kolmogorov equations). Suppose {X(t)}t∈N0 is a Markov chain
on a probability space (Ω, F, P) with values in a measurable space (E, E).
Suppose
further A ∈E, x ∈E and km(A|x) denotes the m-step transition kernel corresponding
to {X(t)}t∈N0. It holds for every (m1, m2) ∈N2
km1+m2(A|x) =
Z
E
km2(A|y)km1(dy|x).
(4.2)
This means in order to get from x to A in m1 + m2 steps, we need to pass through
some y on the mth
2 step. Thus, for the realization of a Markov chain {x(j)}j∈{0,...,T}
and any two natural numbers i′, i with 0 < i ≤T
ki′(A|x(i)) ≤
Z
E
ki′(A|y)k(dy|x(i−1)) = ki′+1(A|x(i−1))
The chain in some sense “loses” its memory on the states visited a long time back in
the past. Therefore, taking every ith sample only, provides more or less independent
samples from the distribution π. How to infer i is derived in Chapter 4.3 with the help
of the autocorrelation function, a statistic commonly used for analyzing time series data
in the ﬁeld of signal processing.
As far as the convergence of a Markov chain to a limiting distribution π is concerned, we
need to keep in mind that computational methods are always discrete due to calculation
accuracy. Thus, all we can hope for solving a continuous problem using a computational
approach is a good approximation of the problem at hand. Recalling that we are dealing
with dependent Markov chains, we nevertheless want to start the sampling procedure
74

4.3 A measure for independence
in an area where π possesses a considerable amount of mass. This can e.g. be achieved
using the simulated annealing algorithm introduced in Chapter 4.6 or the convergence
statistics of Chapter 4.4.
4.3
A measure for independence
We now turn to the problem of determining the right amount of thinning in order to
resolve the dependency issue of a Markov chain. For the rest of this chapter, we assume
that the ﬁrst and second moment of the posterior distribution π(·|·) exist. To introduce
the basic concept of thinning, we for now assume that the elements of the following
random processes are univariate.
Deﬁnition 4.1 (Autocorrelation and autocovariance). Let {X(t)}t∈N0 be a Markov
chain. The autocovariance is deﬁned as function
γ : N0 × N0 −→R, (s, t) 7→E[(X(t) −µt)(X(s) −µs)],
where µt and µs denote the means of X(t) and X(s).
If furthermore {X(t)}t∈N0 is
non-Dirac, i.e. for all t the random vector X(t) is not constant (we exclude random
varibles with Dirac distribution and therefore standard deviation 0), the autocorrelation
is deﬁned as
ρ : N0 × N0 −→R, (s, t) 7→E[(X(t) −µt)(X(s) −µs)]
σtσs
,
where σt and σs are the respective standard deviations of X(t) and X(s).
The autocovariance takes on values in [−∞, +∞] and the autocorrelation in [−1, 1],
similar to the covariance and correlation statistics. Our goal is to generate samples
from the probability density function π(·|·) based on a Markov chain {X(t)}t∈N0. Here,
{X(t)}t∈N0 needs to be a stationary stochastic process. This means X(t) is independent
of the index t and
µ := E[X(t)] = µt = µs = E[X(s)]
75

4. MARKOV CHAIN MONTE CARLO (MCMC) METHODS
for all t, s ∈N0. In this setting, we somewhat abuse the notation of the autocorrelation
and autocovariance above and write
γ(t, s) = E[(X(t) −µt)(X(s) −µs)] = E[(X(t) −µ)(X(s) −µ)]
= E[(X(t) −µ)(X(t+τ) −µ)] = γ(τ),
and also
ρ(t, s) = E[(X(t) −µt)(X(s) −µs)]
σtσs
= E[(X(t) −µ)(X(s) −µ)]
σ2
= E[(X(t) −µ)(X(t+τ) −µ)]
σ2
= ρ(τ),
for τ = s −t and σ2 = σ2
t = σ2
s.
The autocorrelation describes the correlation between all possible pairs (X(t), X(s)) in
the Markov chain. This deﬁnition also includes negative lags τ < 0, which will not be
of further interest to us, since γ and ρ are clearly symmetric, this is,
γ(τ) = γ(−τ)
and
ρ(τ) = ρ(−τ).
We now determine the thinning rate r for a realization {ξ(t)}t=1,...,T of the Markov chain.
More precisely, we want to infer the parameter r, such that the chain {ξ(t′)}t′∈J with
J = {0, r, 2r, . . . , ⌊T
r ⌋·r} has a very low autocorrelation value for all lags τ ̸= 0 (compare
Neal [1993]). Note that ρ(τ = 0) = 1. For estimating r only very few approaches exist.
For one, visual inspection of the sample autocorrelation function of the Markov chain
{ξ(t)}t can help to determine the size of r. A sharp decrease in all dimensions should
here be obtained within the ﬁrst few lags after thinning. A widely used estimate for r,
the so-called INEFﬁciency Factor (INEFF), is also based on autocorrelation functions
(Bartlett [1966]; Kass [1993]; Kass et al. [1998]):
Lemma 4.2 (Ineﬃciency factor (INEFF)). For the realization of a stationary Markov
chain {ξ(t)}t∈{1,...,T} and the autocorrelation function ρ(τ) we can consider samples that
are at least
1 + 2
T
X
τ=1

1 −
τ
T + 1

ρ(τ)
indices apart as independent.
76

4.3 A measure for independence
Proof. The idea is to monitor the eﬀect of thinning on the expected variance of ξ :=
{ξ(t)}t. For the expected mean
¯ξ =
1
T + 1
T
X
t=0
ξ(t),
(4.3)
the law of large numbers guarantees ¯ξ −→E[ξ] almost surely for T →∞. Now, if the
elements of the chain were pairwise independent, we would get the well-known result
Var[¯ξ] = Var
"
1
T + 1
T
X
t=0
ξ(t)
#
=
1
(T + 1)2
T
X
t=0
Var
h
ξ(t)i
=
σ2
T + 1.
Due to the autocorrelation in the chain the variance of the estimator ¯ξ is given by
Var[¯ξ] = E[(¯ξ −E[ξ])2]
(4.3)
= E


 
1
T + 1
T
X
t=0
(ξ(t) −E[ξ])
!2

=
1
(T + 1)2
T
X
t,t′=0
E
h
(ξ(t) −E[ξ])(ξ(t′) −E[ξ])
i
=
1
(T + 1)2
T
X
t,t′=0
γ(t′ −t)
=
1
(T + 1)2


X
−T≤τ≤T
(γ(τ) (T + 1 −|τ|)) + (T + 1)γ(0)


=
1
T + 1
X
−T ≤τ ≤T
τ ̸= 0

1 −
|τ|
T + 1

γ(τ) +
σ2
T + 1
=
σ2
T + 1 ·
 
1 + 2
T
X
τ=1

1 −
τ
T + 1

ρ(τ)
!
,
which proofs the claim.
Generally, the autocorrelation function is not known explicitly and we have to use the
sample autocorrelation function
ˆρ(τ) =
1
(T + 1 −τ)ˆσ2
T−τ
X
j=0
(ξ(j) −¯ξ)(ξ(j+τ) −¯ξ)
77

4. MARKOV CHAIN MONTE CARLO (MCMC) METHODS
as estimator for ρ(τ) instead.
Here, ˆσ2 =
1
T
PT
j=0(ξ(j) −¯ξ)2 denotes the expected
variance. For an n-dimensional Markov chain we compute the sample autocorrelation
function ˆρi(τ) along each dimension i and estimate the INEFF by
ˆr := max
i=1,...,n
( 
1 + 2
T
X
τ=1

1 −
τ
T + 1

ˆρi(τ)
!)
.
A related, frequently used statistic is the Eﬀective Sampling Size (ESS) deﬁned by
ESS = T/ˆr.
4.4
Convergence to the stationary distribution
Another important issue for sampling from a Markov chain is its starting value ξ(0).
Choosing ξ(0) arbitrarily can lead to convergence issues as can be seen in the following
simple example:
Example 4.2 (Sampling from a one dimensional normal distribution). Say we want to
generate samples from the univariate standard normal distribution N(0, 12) using a
simple random walk Metropolis-Hastings sampler.
We compare the convergence of
the chain to N(0, 12) taking ξ(0) = 0.1 and ξ(0) = 20 as initial values. In both cases
we tune the step-size of the Metropolis-Hastings algorithm to 2.4, which is optimal
for convergence as shown by Gelman et al. [1996], i.e. the Metropolis-Hastings update
function is given by ξp ∼N(x(c), 2.42). Our quality measure is to approximate the true
mean µ = 0 and the standard deviation σ = 1 within an [−0.05, 0.05] and [0.95, 1.05]
error bound, respectively. Each of the 100 runs is thinned by computing the INEFF.
While starting at ξ(0) = 0.1 needs in average 137 steps to converge, starting at ξ(0) = 20
takes 7,257 steps. This is more than 50 times as much as for ξ(0) = 0.1. The reason
for this behavior is the sometime slow proceeding of the Markov chain towards regions
with high mass in the probability mass function.
In the following we introduce convergence statistics in order to remedy this eﬀect. These
can be split into two classes: While the ﬁrst class is based on multiple chains, i.e. a
set of Markov chains is run in parallel, generally starting at diﬀerent initial values, the
second class is based on a single chain. Both determine the number of samples to be
removed before we can consider the Markov chain to be stationary. This limit is called
burn-in period of the Markov chain.
78

4.4 Convergence to the stationary distribution
We ﬁrst consider one of the most prominent single chain methods: The Geweke test
(Geweke [1992]) splits the realization of a Markov chain {ξ(j)}j=0,...,T into two distinct
subsamples. It generally takes the ﬁrst 10% and last 50% of the samples. For a station-
ary Markov chain the mean of these parts should be approximately equal. Therefore,
in order to remove the burn-in period of {ξ(j)}j=0,...,T a z-score can be used, i.e. for
given z0 (generally z0 = 2 is chosen) we monitor the number of samples m, such that
for am, bm ∈N with am + bm + m ≤T

¯ξam −¯ξbm
p
ˆσam + ˆσbm
 < z0
where | · | denotes the absolute value, ˆσ· the according expected variances, and ¯ξam =
1
am
Pm+am
j=m+1 ξ(j) and ¯ξbm =
1
bm
Pm+T
j=m+T−bm+1 ξ(j). Here, the natural number am is the
index corresponding to the ﬁrst 10% of the reduced Markov chain {ξ(j)}j=m,...,T , bm
the index corresponding to the last 50% of this chain.
Let us now turn to the most prominent multiple chains method: The Gelman-Rubin
statistic ˆR (Brooks & Gelman [1998]; Gelman & Rubin [1992]) compares the variances
within each chain to the variance between the chains. For a stationary Markov chain,
these two statistics should coincide. More precisely, suppose we are given L realizations
{ξ(j)
l }j=0,...,T of the Markov chain {X(t)}t∈N starting at diﬀerent initial values ξ(0)
l
(l = 1, . . . , L). We monitor the number of samples m, such that for the between-chain
variance B
B(m) = T −m
L −1
L
X
l=1
(¯ξl(m) −¯ξ(m))2
where
¯ξl(m) =
1
T −m
T
X
j=m+1
ξ(j)
l
and
¯ξ(m) = 1
L
L
X
l=1
¯ξl(m)
and the within-chain variance W
W(m) = 1
L
L
X
l=1
ˆσ2
l (m)
where
ˆσ2
l (m) =
1
T −m −1
L
X
j=m+1
(ξ(j)
l
−¯ξl(m))2
79

4. MARKOV CHAIN MONTE CARLO (MCMC) METHODS
the Gelman-Rubin statistic
ˆR(m) =
s
ˆσ(m)
W(m)
is close to one (generally a limit of 1.2 is chosen), where
ˆσ(m) = (1 −1/(T −m))W(m) + B(m)/(T −m).
The Gelman-Rubin statistic can also be computed using a single realization {ξ(j)}j=0,...,T
only. For this, we split apart {ξ(j)}j=0,...,T into a number of subsamples of equal length.
The subsamples are then considered as parallel chains and the Gelman-Rubin statistic
is computed as described above. This raises the question, whether a single chain or
multiple chains starting at diﬀerent initial values should be used for posterior infer-
ence? In this thesis we will follow Geyer [1992] who argues that a single longer chain is
superior to multiple chain approaches as various smaller chains may not reach conver-
gence during the sampling process. We therefore do not consider multi-chain MCMC
methods any further.
4.5
Reversible jump MCMC
We have seen in Chapter 3.4 how to apply Bayesian methods for the purpose of model
selection using the posterior odds ratio π(M1|y)
π(M2|y) for two competing models M1 and M2
based on the observations y. Given a series of models M1, . . . , Mk, pairwise application
of the posterior odds ratio can be used in order to infer the best model for y. For each
Mi the MH algorithm from Chapter 4.1 may be applied for inference of the according
posterior distribution. The sampling is thereby restricted on the speciﬁc parameter
space Ξi of Mi. Naturally, there has been extensive research in order to simultaneously
cover the issue of model selection and model inference (see e.g. Geyer & Møller [1994];
Ripley [1977]). This is of importance, if the number of models is denumerable, as could
be the case for Gaussian mixture models (compare Chapter 3.4.4) with an unknown
number of components, or for the application in model averaging (see e.g. Hastie et al.
[2009] Chapter 8.8). However, a general formalization has only rather recently been
presented by Green [1995]:
80

4.5 Reversible jump MCMC
Suppose we are given a series of models {Mi}i∈I on some discrete index set I with
according proper parameter prior distributions π(ξi|Mi) on the parameter space Ξi for
model Mi and a model prior distribution π(Mi) holding the probability distribution for
model Mi. Then the joint prior for the parameter ξi in model Mi on the parameter
space
Ξ =
[
i
{i} × Ξi
is given by
π(ξi, Mi) = π(ξi|Mi)π(Mi)
with respective Lebesgue measure induced by π(ξi|Mi). For the jump between two
parameter spaces Ξi and Ξj, Green’s approach is based on the bijective identiﬁcation
of artiﬁcial extensions of these spaces. More precisely, we have to deﬁne a series of
artiﬁcial sets {Uij}ij∈I×I together with deterministic bijections
Tij : Ξi × Uij −→Ξj × Uji
(ξi, uij) 7−→(ξj, uji).
This is known as dimension matching condition. Note that Tij ̸= T −1
ji
is possible, albeit
not common. In general, a move from ξi ∈Ξi to some ξj ∈Ξj is proposed by sampling
ui ∼gi(ui) according to some proper proposal function gi(·) and setting
(ξj, uj) = Tij(ξi, ui).
The reverse move is given by sampling uj ∼gj(uj) for some proper proposal func-
tion gj(·) such that (ξi, ui) = T −1
ij (ξj, uj). Green [1995] showed that the according
acceptance probability for the move from Mi to Mj is then
α(j, ξj|i, ξi) = min
Lj(ξj|y)π(ξj, j)pjigj(uj)
Li(ξi|y)π(ξi, i)pijgi(ui)

∂Tij(ξi, ui)
∂(ξi, ui)
 , 1

,
where Lk(ξk|y) is the likelihood function for model Mk given the observations y,
 ∂Tij(ξi,ui)
∂(ξi,ui)
 is the Jacobian of the transformation Tij at (ξi, uij), pkl is the probability
for jumping from Mk to Ml, and gk is the density function of uk. As in the classical MH
algorithm the posterior distribution needs to be known up to a multiplicative constant.
However, all prior distributions π(ξi|Mi) need to be normalized up to the same mul-
tiplicative constant. The according sampling scheme is called reversible jump MCMC
81

4. MARKOV CHAIN MONTE CARLO (MCMC) METHODS
(RJMCMC) algorithm. It explores the joint model/parameter space posterior distribu-
tion π(ξi, Mi|y) based on the observations y and can therefore be used for simultaneous
model selection and inference. For a ﬁnite set of models {Mi}i=1,...,k RJMCMC draws
samples from the posterior distribution
π(ξi, Mi|y) = Li(ξi|y)π(ξi|Mi)π(Mi)
π(y)
with normalizing constant
π(y) =
k
X
j=1
Z
Ξj
Lj(ξj|y)π(ξj|Mj)π(Mj) dξj.
The relation
π(Mi|y) =
Z
Ξi
π(ξi, Mi|y) dξi
=
1
π(y)π(Mi)
Z
Ξi
Li(ξi|y)π(ξi|Mi) dξi
naturally provides the Bayes factor by RJMCMC via
Bij = π(Mi|y)π(Mj)
π(Mi|y)π(Mi) =
R
Ξi Li(ξi|y)π(ξi|Mi) dξi
R
Ξj Lj(ξj|y)π(ξj|Mj) dξj
(4.4)
which can be approximated using the parameter samples of the models Mi and Mj.
Hence, there is a one-to-one relation between Bayes factors and RJMCMC with respect
to Bayesian model selection. However, for a denumerable number of models RJMCMC
is an essential Bayesian modeling tool. Using a uniform model prior π(Mi) = π(Mj)
for all i, j ∈{1, . . . , k}, Equation (4.4) can simply be approximated by the quotient ni
nj
of instances nk the Markov chain visited model Mk (Bartolucci et al. [2006]).
Example 4.3 (RJMCMC for the Gaussian mixture model). We again turn to the Gaus-
sian mixture example from Chapter 3.4.4 reusing the notation from above. The pa-
rameter space is given by
Ξ = {1} × R ∪{2} × R2.
As proposed by Robert & Casella [2004] we deﬁne the bijections
T12(µ, u) = (µ1, µ2) := (µ−u, µ+u) and T21(µ1, µ2) = (µ, u) =
µ2 + µ1
2
, µ2 −µ1
2

,
82

4.6 The simulated annealing algorithm
where u ∼N(0, 12). The Jacobians are then

∂T12(µ, u)
∂(µ, u)
 = 2
and

∂T21(µ1, µ2)
∂(µ1, µ2)
 = 1
2.
We set the jump probabilities p12 = p21 = 1, i.e. we propose a model jump in each
RJMCMC step. With the model prior distribution ρ(1) = ρ(2) = 1
2, the acceptance
probabilities are given by
α(2, µ1, µ2|1, µ, u) = min
L2(µ2, µ1|y)ϕ2(µ1)ϕ−2(µ2)
L1(µ|y)ϕ0(µ)ϕ0(u)
2, 1

and
α(1, µ, u|2, µ1, µ2) = min

L1(µ|y)ϕ0(µ)ϕ0(u)
L2(µ1, µ2|y)ϕ2(µ1)ϕ−2(µ2)
1
2, 1

,
where ϕ0(·), ϕ2(·), and ϕ−2(·) are the probability density functions corresponding to
N(0, 12), N(2, 12) and N(−2, 12), respectively. Reusing the observations y from the
Gaussian mixture example of Chapter 3.4.4 and running the RJMCMC algorithm for
ten runs on N =100,000 proposals each, the Bayes factor was computed as 63.21±11.27
(including one standard error). Although this is close to the “true” value of 77.47,
thermodynamic integration yielded better results!
4.6
The simulated annealing algorithm
For completeness we want to point out that the MH algorithm can also be directly
applied to global optimization problems: Consider a real valued function h : E −→R
on a ﬁnite set E – for inﬁnite sets E convergence is very problematic in practical
applications – along with the minimization problem
min
ξ∈E h(ξ).
(4.5)
Minimization is not a restriction here as we can maximize h by minimizing the func-
tion −h. Generally, h is a function with various local minima making deterministic
minimization algorithms inapplicable. Now, for T ∈(0, 1] we can use Algorithm 2 to
sample from the stationary distribution
π(ξ) ∝exp(−h(ξ)/T).
The MH acceptance probability then computes to
α(ξ|ξ(c)) = min{exp((h(ξ(c)) −h(ξ))/T), 1}
83

4. MARKOV CHAIN MONTE CARLO (MCMC) METHODS
for some proposal function q(ξ|ξ(c)) where ξ(c) is the current Markov chain element.
Naturally, ξ is accepted, if h(ξ) < h(ξ(c)).
However, there is a chance (inversely
proportional to T) for proposals ξ to get accepted even if h(ξ(c)) < h(ξ). The Markov
chain can hence escape local minima. Setting up a cooling schedule, i.e. deﬁning a
series of “temperatures” T1 = 1 > T2 > . . . Tk > 0, the iterative application of the MH
algorithm for these Ti’s solves the minimization problem (4.5). The according algorithm
is called simulated annealing algorithm and was introduced by Kirkpatrick et al. [1983].
Although the theory of time-homogeneous Markov chains introduced in Chapter 2.3
does not provide the necessary convergence properties (c.f. Robert & Casella [2004]),
convergence can nevertheless be shown for slow cooling schedules (see e.g. Mitra et al.
[1986]).
84

5
Extensions to the
Metropolis-Hastings algorithm
In the last few years various extensions to the classical random walk MH algorithm have
been proposed. Examples include adaptive MCMC, population MCMC, hybrid Monte
Carlo and tempering methods (see Liu [2008] for an overview). All of these approaches
try to generate MH proposals that have a high chance of getting accepted by the MH
acceptance probability (4.1).
This increases the overall sampling eﬃciency.
In the
current chapter we introduce two prominent MH extensions. The ﬁrst exploits the ge-
ometric posterior parameter structure by generating proposals guided by the Jacobian
matrix. The second successively improves the algorithmic parameters of the transi-
tion density function during the sampling process. Chapter 5.3 also introduces some
notation needed for the deﬁnition of the copula based Metropolis-Hastings algorithms
introduced in Chapter 6.
5.1
Simpliﬁed Riemann Manifold Metropolis Adjusted
Langevin Algorithm (SMALA)
An extension to the classical MH algorithm was derived from diﬀusion theory (Grenan-
der & Miller [1994]): For the stationary target distribution π(·) the Langevin diﬀusion
85

5. EXTENSIONS TO THE METROPOLIS-HASTINGS ALGORITHM
is deﬁned by the stochastic diﬀerential equation
dX(t) = 1
2∇X(t) log(π(X(t)))dt + dW (t),
(5.1)
where ∇X(t) is the nabla operator with respect to X(t) and W (t) denotes n-dimensional
standard Brownian motion. The stationary distribution of Equation (5.1) is given by
π(·). A ﬁrst order Euler discretization of (5.1) then leads to the Metropolis Adjusted
Langevin Algorithm (MALA) proposal function q(ξ|ξ′) (used in an MH algorithm) with
ξ = ξ′ + ε2
2 ∇ξ′ log(π(ξ′)) + εη
(5.2)
for η ∼Nn(0, In). The scaling parameter ε can be used to ﬁne tune the algorithmic step
size, which opens up the possibility to control MH acceptance rates. Since the nabla
operator ∇ξ only considers the directional derivatives with respect to the parameter
vector ξ, strong parameter correlations can disturb the eﬃcacy of MALA. Roberts &
Stramer [2002] suggested to circumvent this issue using a preconditioning matrix G
such that
ξ = ξ′ + ε2
2 G∇ξ′ log(π(ξ′)) + ε
√
Gη,
where
√
G is the square root of G obtained by eigen- or Cholesky decomposition.
However, ﬁnding an appropriate preconditioning matrix G needs structural information
about the target distribution. Regarding Bayesian inference Girolami & Calderhead
[2011] were the ﬁrst to consider a parameter dependent preconditioning matrix G(ξ)
as geometric tensor that takes into account the local geometric structure of the joint
data and parameter distribution π(y, ξ). The concept exploits the distance between
two ξ-parametrized distributions π(y|ξ) and π(y|ξ + δξ) given by the quadratic form
δξ⊤G(ξ)δξ for some positive deﬁnite metric tensor G(ξ) (Rao [1945]). Rao noted that
G(ξ) by deﬁnition yields the metric of a Riemann manifold. Given that log(π(y|ξ)) is
twice absolutely continuously diﬀerentiable with respect to ξ (as is the case in almost
all practical applications, Girolami & Calderhead [2011] ﬁnally suggested to take G(ξ)
86

5.1 Simpliﬁed Riemann Manifold Metropolis Adjusted
Langevin Algorithm (SMALA)
to be the expected Fisher information matrix minus the Hessian of the log-prior, i.e.
G(ξ) = −Eπ(y|ξ)
 ∂2
∂ξ2 log(π(y, ξ))

= −Eπ(y|ξ)
 ∂2
∂ξ2 log(π(y|ξ)π(ξ))

= −Eπ(y|ξ)
 ∂2
∂ξ2 log(π(y|ξ))

|
{z
}
expected Fisher information matrix
−
∂2
∂ξ2 log(π(ξ))
|
{z
}
Hessian of log-prior
= cov
" ∂
∂ξ log(π(y|ξ))
⊤
,
 ∂
∂ξ log(π(y|ξ))
⊤#
−∂2
∂ξ2 log(π(ξ)).
The last equation follows assuming Fisher regularity for the target distribution (Schervish
[1995], Prop. 2.84). Generally, a Langevin diﬀusion with invariant distribution π(·) can
be deﬁned on a Riemann manifold via its metric tensor G(·) by
dX(t) = 1
2
˜∇X(t) log(π(X(t)))dt + d ˜
W
(t),
(5.3)
where
˜∇X(t) log(π(X(t))) = G−1(X(t))∇X(t) log(π(X(t)))
for the natural gradient ∇X(t) on Rn (Amari & Nagaoka [2007]). On the other hand,
the Brownian motion on the manifold is given by
d ˜
W
(t)
i
= det(G(X(t)))−1
2
n
X
j=1
∂
∂X(t)

G(X(t))i,j det(G(X(t)))
1
2

dt
+
q
G−1(X(t))dW (t)

i
,
(5.4)
i = 1, . . . , n (Chung [1982]). Expanding the diﬀerential in Equation (5.4) in combina-
tion with a ﬁrst order Euler discretization of Equation (5.3) ﬁnally yields the Riemann
Manifold MALA (MMALA) proposal function q(ξ|ξ′) via
ξi =ξ′
i + ε2
2
 G−1(ξ′)∇ξ′ log π(y|ξ′)

i −ε2
n
X
j=1
 
G−1(ξ′)∂G(ξ′)
∂ξ′
j
G−1(ξ′)
!
i,j
+ ε2
2
n
X
j=1
 G−1(ξ′)

i,j tr
 
G−1(ξ′)∂G(ξ′)
∂ξ′
j
!
+

ε
q
G−1(ξ′)η

i
(5.5)
for i = 1, . . . , n and η ∼Nn(0, In). In summary, MMALA exploits the local Riemann
manifold structure of the parameter space at stake in order to eﬃciently explore the
87

5. EXTENSIONS TO THE METROPOLIS-HASTINGS ALGORITHM
target distribution π(ξ|y). This becomes clear as the MH proposal function proposes
moves with respect to the Riemannian metric deﬁned by G(ξ) rather than with respect
to the standard Euclidian distance on Rn. Sampling results in Girolami & Calderhead
[2011] showed that simplifying the proposal (5.5) by assuming a constant curvature
throughout the manifold, i.e.
∂2
∂ξ2 log(π(y, ξ)) = const. and hence
∂
∂ξG(ξ) = 0, drasti-
cally decreases the computational demand for proposal generation. Although the ef-
fective sample size decreases, the number of i.i.d. samples drawn per second increases.
This holds true especially for the dynamic systems considered in the paper. The MH
sampling scheme assuming constant manifold curvature is called Simpliﬁed MMALA
(SMALA) algorithm. The according proposal function simpliﬁes to
ξ = ξ′ + ε2
2 G−1(ξ′)∇ξ′ log π(y|ξ′) + ε
q
G−1(ξ′)η.
(5.6)
for η ∼Nn(0, In). Clearly, even though the manifold curvature is generally not con-
stant, the proposal scheme (5.6) deﬁnes a valid MCMC sampler in combination with
the MH acceptance rule. Here, the proposals are somewhat generated in the direction
of highest local improvement with respect to the posterior distribution. Compared to
random walk proposals this can improve the chance of acceptance.
5.2
Adaptive MCMC
Another very tempting approach to attain high proposal acceptance rates is the ﬁne
tuning of algorithmic parameters, such as the scaling parameter kRW of the RWMH
algorithm, during the sampling process. This algorithmic parameter adaption is gener-
ally done based on preceding Markov chain realizations. A major pitfall of this attempt
is, that the stochastic process {X(t)}t∈N0 is no longer Markovian, i.e.
P(X(t)|X(t−1), . . . , X(0)) ̸= P(X(t)|X(t−1)).
Thus, the convergence, or rather the ergodicity of the MCMC sampler at hand is no
longer guaranteed (Rosenthal [2011]). More generally spoken, the application of diﬀer-
ent Markov chain transition kernels in the very same inference process of an arbitrary
probability distribution can destroy the ergodicity constraint, even if all of these tran-
sition kernels have the same equilibrium distribution. This can be seen in the following
simple (discrete) example inspired by Roberts & Rosenthal [2007]:
88

5.2 Adaptive MCMC
(a)
(b)
Figure 5.1: Graphical representation of the discrete state space models. Depicted are
the transition probabilities for (a) the transition kernel k1(·|·) and (b) the transition kernel
k2(·|·). Here, black arrows correspond to a transition probability of 1
2 and red arrows to a
transition probability of 1.
Example 5.1. Suppose E = {1, 2, 3, 4} is a discrete state space and we want to sample
from the uniform distribution π(·) with
π(1) = π(2) = π(3) = π(4) = 1
4.
Suppose furthermore we are given the irreducible and aperiodic transition kernels k1(·|·)
and k2(·|·) with
k1(2|1) = k2(1|2) = 1,
k1(3|2) = k1(4|2) = k1(1|3) = k1(4|3) = k1(1|4) = k1(3|4) = 1
2,
k2(3|1) = k2(4|1) = k2(2|3) = k2(4|3) = k2(2|4) = k2(3|4) = 1
2
(see Figure 5.1). Following Example 2.8 π(·) is the unique stationary distribution for
k1 and k2. Now, iteratively applying k1 and k2 starting at ξ(0) = {1} yields a Markov
chain that exclusively visits the states {1} and {2}. Hence, the chain does not converge
to π(·).
Adaptive MCMC methods preserving ergodicity have been proposed amongst others by
Gilks et al. [1998] or Holden et al. [2009]. Gilks allows updating the proposal function
whenever the Markov chain reaches a set A ⊂E of the state space (E, E), such that
π(A) > 0 and X(t+1), X(t+2), . . . is conditionally independent of X(0), . . . , X(t) given
X(t) ∈A. The set A is called a proper atom of the Markov chain and whenever the
89

5. EXTENSIONS TO THE METROPOLIS-HASTINGS ALGORITHM
chain enters A, it is said to regenerate. For continuous state spaces (E, E) proper atoms
might not exist. Although a technique due to Nummelin [2004] allows to construct
regenerating Markov chains on some augmented state space also for continuous state
spaces, the practical application of regenerating Markov chains remains an engineering
art form. Holden and coworkers, on the other hand propose an adaptive independence
sampling scheme that takes into account almost the full history of the proposed samples.
They show convergence as long as the proposal distribution satisﬁes the strong Doeblin
condition that we introduce in Remark 6.1.
Theoretical results with respect to convergence of adaptive MCMC samplers have been
derived by Roberts & Rosenthal [2007]. They all require that the transition kernels used
are uniformly bounded. Furthermore, the amount of modiﬁcation of the algorithmic
parameters has to either diminish as t →∞– this is e.g. the case for the Adaptive
Metropolis algorithm of Haario et al. [2001] introduced in the following – or the adaption
process is applied with diminishing probability PA(t), i.e. PA(t) →0 for t →∞.
5.3
Metropolis Gaussian Adaption algorithm (M-GaA)
Haario et al. [1999] introduced a rather simple but eﬃcient adaptive Monte Carlo sam-
pler called Adaptive Proposal (AP) algorithm. The basic idea is to apply a RWMH
algorithm using a multivariate Gaussian proposal function whose covariance matrix is
continuously updated based on a ﬁxed number of previously accepted Markov chain
samples. With this the MCMC process (locally) adapts to the target distribution and
provides an eﬀective proposal scheme. As the covariance matrix can be updated se-
quentially (Haario et al. [2001]), the additional computational cost is by far outweighed
by the improvement in eﬃciency. Although the AP algorithm has empirically proven to
outperform the classical (non-adaptive) Metropolis-Hastings algorithm, rigorous proofs
of convergence are yet still missing. In fact, Haario et al. [1999] pointed out that AP
algorithm might be slightly biased. Nevertheless, the paper also shows that the diﬀer-
ence between the AP sampled and true limiting distribution is very small for practical
applications.
The AP algorithm was extended by M¨uller & Sbalzarini [2010] such that the adapted
covariance matrix of the RWMH proposal function maximizes the entropy of the target
90

5.3 Metropolis Gaussian Adaption algorithm (M-GaA)
distribution π(·) under the constraint that the proposed MCMC samples are accepted
with a predeﬁned theoretical MH acceptance rate α0 ∈(0, 1). In contrast to the classical
AP algorithm the covariance matrix is based on all previously accepted Markov chain
samples. This sampling scheme was named Metropolis Gaussian Adaption (M-GaA)
algorithm. Without going into detail we shortly review the basic sampling procedure
applying the strategic algorithmic parameter setting of M¨uller & Sbalzarini [2010]:
Starting at some initial value ξ(0) ∈Rn, the empirical proposal covariance matrix of
step j is decomposed as
Σ(j) = r2p
Σ(j)p
Σ(j)⊤
where
p
Σ(j) denotes the normalized square root of Σ(j) found by eigen- or Cholesky
decomposition, i.e. det
p
Σ(j)
= 1. The parameter r is of no importance for the rest
of the proposal scheme. The algorithm uses the n-dimensional identity matrix In as
proposal covariance matrix Σ(i) as long as ξ(i) = ξ(0) for i ∈N. An MCMC proposal
is then generated based on the current sample ξ(j) via
ξp = ξ(j) + r(j)p
Σ(j)η(j)
for some η(j) ∼Nn(0, In) and some step size parameter r(j) deﬁned below. Due to the
symmetry in the proposal function we accept ξp according to the Metropolis-Hastings
acceptance probability
α(ξp|ξ(j)) = min
(
π(ξp)
π(ξ(j))
, 1
)
with respect to the target density π(·). The proposal covariance matrix Σ(j) is updated
based on the Markov chain sample ξ(j+1) via
Σ(j+1) = (1 −s) ξ(j+1) + s(ξ(j+1) −ξ(j)) · (ξ(j+1) −ξ(j))⊤
for some algorithmic parameter s.
M¨uller & Sbalzarini [2010] suggested to choose
s = log(n+1)
(n+1)2
< 1 as s directly controls the inﬂuence of the n2 values of Σ(j). Finally,
the step size parameter r(j) was suggested to be updated as
r(j+1) =





(1 + s (1 −α0)) · r(j)
if ξp was accepted and
(1 −sα0) · r(j)
otherwise,
91

5. EXTENSIONS TO THE METROPOLIS-HASTINGS ALGORITHM
where α0 denotes the predeﬁned acceptance rate of the M-GaA algorithm. Note that
r(j+1) is reduced in case ξp was rejected and increased otherwise. We will see in Chap-
ter 6.3.4 that the actual acceptance rate of M-GaA can fail to meet α0 in complex
applications.
Remark 5.1 ( Adaptive Metropolis algorithm). An adaptive MCMC scheme very similar
to the AP and M-GaA algorithms was introduced in Haario et al. [2001]: The Adaptive
Metropolis (AM) algorithm updates the multivariate Gaussian RWMH proposal func-
tion with covariance matrix Σ based on all previously accepted Markov chain samples.
The explicit covariance adaption at the jth MCMC step is given by
Σ(j) =



Σ(0)
for j ≤j0
sdCov(ξ(0), . . . ξ(j−1)) + sdεIn
for j > j0.
where Σ(0) is some initial covariance matrix applied up to step j0 with ξ(i) ̸= ξ(i−1)
for some i ≤j0. Cov(ξ(0), . . . ξ(j−1)) denotes the empirical covariance matrix based on
the Markov chain realizations ξ(0), . . . ξ(j−1) and In the n-dimensional identity matrix.
The parameter sd is a predeﬁned scaling constant and ε > 0 an auxiliary constant
introduced to avoid singularities in the covariance matrices Σ(j). Haario et al. [2001]
provided a proof of convergence for the AM algorithm for bounded target distributions
π(·) on a bounded support S ⊂Rn, this is, there exists a constant M ∈R such that
π(x) ≤M for all x ∈S and π(X) ≡0 for all x ∈Rn \ S.
While the SMALA and M-GaA algorithms try to locally improve the MH transition
kernel, we took a global approach in order to elevate the MH sampling eﬃciency. Here,
an approximation of the posterior distribution generates samples that are distributed
similar to the true posterior. The concept is introduced in the next chapter.
92

6
Improving the
Metropolis-Hastings algorithm
using copulas
We have seen in Figure 1.1 that even in two dimensions posterior complexity can be
quite severe. For an independent posterior parameter distribution π(ξ1, . . . , ξn|y) =
π(ξ1|y) · . . . · π(ξn|y) the inference process can be focused on the distributions π(ξi|y),
i = 1, . . . , n, individually, avoiding the so-called curse of dimensionality (Hastie et al.
[2009]) that aggravates the inference of the joint distribution π(ξ1, . . . , ξn|y). Especially
model inference of parametrized diﬀerential equations likes to trap Markov Chain Monte
Carlo samplers between high proposal rejection rates and strong autocorrelation struc-
tures within the sampled Markov chains – both leading to a low number of independent
samples drawn over time. A crucial issue for the eﬃcacy of an MH algorithm is the
choice of the proposal function. Clearly, the optimal proposal function is given by the
actual posterior density, which transforms the Metropolis acceptance probability to
α(ξp|ξ(c)) = min
(
π(ξp|y)π(ξ(c)|y)
π(ξ(c)|y)π(ξp|y)
, 1
)
= 1.
However, if direct sampling from the posterior is possible, the MH algorithm becomes
uncalled-for. The best we can hope for is to use a proposal function that approxi-
mates the posterior density as close as possible. We addressed this issue by developing
93

6. IMPROVING THE METROPOLIS-HASTINGS ALGORITHM
USING COPULAS
a novel (adaptive) MCMC approach for eﬃcient parameter inference in highly depen-
dent systems: The Copula based Independence MH algorithm (CIMH) and its extension,
the Adaptive Copula based Independence MH algorithm (ACIMH), exploit the concept
of a vine copula decomposition of the posterior distribution densities.
They allow
to generate problem speciﬁc proposals for a hybrid independence chain/random walk
Metropolis-Hastings sampler. The key advantage of this approach is a reduced auto-
correlation structure in the sampled Markov chain and with this an increased number
of independent samples drawn over time. In ACIMH all copula densities are updated
during the sampling procedure for ﬁne-tuning.
The performance of our method(s) is assessed on three small scale examples and ﬁnally
evaluated on a DDE model for the JAK2-STAT5 signaling pathway ﬁtted to time-
resolved western blot data. In the ﬁrst three examples we compare our copula based
sampler to a random walk MH algorithm (RWMH), an independence chain sampler
(IMH) and a second order moment based random walk MH algorithm (CovRWMH),
as introduced in Chapter 4.1 and further speciﬁed below. Due to the simplicity of the
systems we do not consider the SMALA and M-GaA algorithms here. However, they
additionally come into play for performance evaluation on the complex JAK2-STAT5
pathway.
6.1
Copula based Independence MH algorithm (CIMH)
We now turn to the deﬁnition of CIMH. As mentioned above, it is based on a vine
copula proposal function similar to the posterior density. However, since the copula
may be based on insuﬃcient data, we extend this proposal function by two additional
transition functions, the ﬁrst of which is a random walk density and the second a
heavy-tailed independence density. The latter is essential to safeguard convergence.
Overall, we end up with a hybrid copula based independence/random walk proposal
function. The sampling scheme consists of four steps: (i) a prerun, (ii) a uniformization
step of the prerun samples, (iii) a D-vine copula decomposition of the dependent prerun
samples, and (iv) the generation of a Markov chain by means of the hybrid copula based
independence chain/random walk sampler. Throughout, we assume that the sampling
space is a Borel measurable subset of Rn.
94

6.1 Copula based Independence MH algorithm (CIMH)
6.1.1
The basic copula MH sampling procedure
(i) Prerun: Our goal is to eﬃciently sample an independent Markov chain realization
{ξ(j)}j=0,...,T with ξ(j) ∈Rn from the posterior distribution π(ξ|y) based on the ob-
servations y.
For this, we ﬁrst generate an initial Markov chain {˘ξ
(j)}j=0,...,T ′, the
so-called prerun samples, using e.g. RWMH or any other sampling algorithm.
The
chain length T ′ +1 should ideally be large enough for the prerun samples to suﬃciently
cover the support of π(ξ|y). Determining T ′ can either be left to the modeler’s expe-
rience or monitored utilizing convergence statistics such as the Gelman-Rubin statistic
introduced in Chapter 4.4. Although too small values of T ′ have a negative eﬀect on the
performance of CIMH, to our experience we can generally choose T ′ a lot smaller than
the chain length T + 1 of the ﬁnal Markov chain {ξ(j)}j=0,...,T . The prerun samples
{˘ξ
(j)}j=0,...,T ′ form the basis for the copula proposal function. Note that we do not
demand independence in the realization {˘ξ
(j)}j.
(ii) Uniformization: Based on {˘ξ
(j)}j, we ﬁt a θ-parametrized D-vine copula c1,...,n(u|θ)
in step (iii). As seen in Chapter 2.2, copulas are deﬁned on the n-dimensional unit cube
[0, 1]n. Hence, each prerun sample ˘ξ
(j) needs to be transformed to [0, 1]n. Depending
on the shape of the histograms of the n sample marginals ˘ξi := (˘ξ(1)
i
, . . . , ˘ξ(T ′)
i
)⊤, we
ﬁt for i = 1, . . . , n γi-parametrized continuous cdf’s Gi(ξ|γi) to the respective sample
marginal. Clearly, the support of the marginal posterior density function π(ξi|y) needs
to be covered by the support of Gi(ξ|γi). This is not a limitation as the support of
π(ξi|y) is controlled by the support of the prior distributions and the claim can easily
be satisﬁed. Each ˘ξ
(j) is then transformed to ˘u(j) := (G1(˘ξ(j)
1 |ˆγ1), . . . , Gn(˘ξ(j)
n |ˆγn))⊤∈
[0, 1]n based on the estimates ˆγi of γi. In the following we refer to {˘u(j)}j=0,...,T ′ as
copula data. Let us consider a simple example: Say, for instance, n = 2 and the sample
marginals of {˘ξ
(j)}j are normally distributed. Based on the estimated means ˆµ1, ˆµ2
and variances ˆσ2
1, ˆσ2
2 of {˘ξ
(j)}j we transform
˘u(j)
1
= Φ
 ˘ξ(j)
1
−ˆµ1
ˆσ1
!
and
˘u(j)
2
= Φ
 ˘ξ(j)
2
−ˆµ2
ˆσ2
!
,
where Φ(·) is the cdf of a standard normal random variable. Step (ii) does not change
the dependency structure inherent to the prerun samples {˘ξ
(j)}j, which is exclusively
modeled by the D-vine copula (see Aas et al. [2009]). This implies that the estimated
95

6. IMPROVING THE METROPOLIS-HASTINGS ALGORITHM
USING COPULAS
Kendall’s τ’s for {˘ξ
(j)}j are identical to the estimated Kendall’s τ’s for {˘u(j)}j. The
procedure works for more general vine structures, such as R-vines, as well, but for the
application to dynamical systems a sequential dependency structure seems appropriate.
(iii) Copula decomposition: The user deﬁned D-vine structure ﬁxes the variable order
for the D-vine copula, i.e. a permutation function ι : {1, . . . , n} −→{1, . . . , n}, i 7→ι(i)
rearranges each sample ˘u(j) = (˘u(j)
1 , . . . , ˘u(j)
n )⊤to ˜u(j) := (˘u(j)
ι(1), . . . , ˘u(j)
ι(n))⊤. Deﬁning
ι such that for i = 1, . . . , (n −1) the pairs (ι(i), ι(i) + 1) cover the highest pairwise
absolute dependency works well in our simulations. For determining the dependency
structure, estimated pairwise Kendall’s τ’s for {˘u(j)}j can be used. Based on {˜u(j)}j
we then ﬁt1 a θ-parametrized D-vine copula density function
c1,...,n(u|θ) =
n−1
Y
j=1
n−j
Y
i=1
cj,j+i|j+1,...,j+i−1(F(uj|uj+1,...,j+i−1, θ), F(uj+i|uj+1,...,j+i−1, θ)|θ),
(6.1)
where F(uℓ|uD, θ) is the θ-parameterized conditional cdf of uℓgiven U D = uD and
uD is a set of [0, 1] valued variables.
Here, the order of the variables in (6.1) cor-
responds to the permutation ι chosen above.
In our notation the parameter θ =
{θi,j+i|(j+1),...,(j+i−1)} for j = 1, . . . , (n −1) and i = 1, . . . , (n −j) contains the copula
parameters and types. However, all bivariate copulas cj,j+i|(j+1),...,(j+i−1) depend only
on θi,j+i|(j+1),...,(j+i−1).
(iv) Generation of the Markov chain: The copula proposal function is deﬁned as follows:
For generating n-dimensional copula proposals ˜ξ ∈Rn, we sample ˜u ∼c1,...,n(u|ˆθ)
from the estimated copula c1,...,n(u|ˆθ). The sample ˜u is then transformed by ˜ξi :=
G−1
ι−1(i)(˜uι−1(i)|ˆγι−1(i)) to yield ˜ξ = (˜ξ1, . . . , ˜ξn)⊤. In the setting of the example above,
say, we choose ι to be the identity function. The corresponding samples ˜ξ on R2 are
then for i = 1, 2 given by
˜ξi = G−1
ι−1(i)(˜uι−1(i)|ˆµι−1(i), ˆσ2
ι−1(i)) = Φ−1(˜uι−1(i))ˆσι−1(i) + ˆµι−1(i) = Φ−1(˜ui)ˆσi + ˆµi.
Thus, all copula proposals ˜ξ are generated according to the joint proposal function
q1(ξ|ˆγ, ˆθ) := c1,...,n(G1(ξ1|ˆγ1), . . . , Gn(ξn|ˆγn)|ˆθ) ·
n
Y
i=1
gi(ξi|ˆγi)
(6.2)
1See Remark 6.2.
96

6.1 Copula based Independence MH algorithm (CIMH)
where gi(ξ|ˆγi) are the density functions corresponding to Gi(ξ|ˆγi). Now, let q2(ξ|ξ′) be
a random Metropolis-Hastings proposal function of choice and q3(ξ) a (compared to the
posterior density π(ξ|y)) heavy-tailed independence proposal function with q2(ξ|ξ′) > 0
and q3(ξ) > 0 on the support of the prior distribution. Let furthermore ξ(0) ∈Rn be
an initial sample. For ﬁxed constants r1 ∈[0, 1) and r2 ∈[0, 1) with r1 + r2 < 1, we
deﬁne the copula based hybrid independence/random walk proposal function for CIMH
via the density function
qcop(ξ|ξ′, ˆγ, ˆθ) := r1q1(ξ|ˆγ, ˆθ) + r2q2(ξ|ξ′) + (1 −r1 −r2)q3(ξ).
(6.3)
Pseudo-code for CIMH is depicted in Algorithm 3. For readability, we write qcop(ξ|ξ′)
for qcop(ξ|ξ′, ˆγ, ˆθ). With this, the Metropolis-Hastings acceptance probability is given
by
αcop(ξ|ξ′) = min
 π(ξ|y)qcop(ξ′|ξ)
π(ξ′|y)qcop(ξ|ξ′), 1

.
(6.4)
We need to point out that although qcop(ξ|ξ′) is independent of the current Markov
chain sample ξ′ for r2 = 0, the acceptance probability nevertheless depends on ξ′.
Hence, the Markov chain generally inherits some autocorrelation structure even for
r2 = 0. The constants r1 and r2 are generally chosen such that r1 + r2 is close to one
in order to ”waste” as few samples as possible.
Proposition 6.1 (Convergence of CIMH ). The CIMH sampling scheme converges to
the posterior equilibrium distribution.
Proof. The proof is identical to the one of Theorem 4.1 recalling that for i = 1, . . . , n
the support of the marginal posterior density function π(ξi|y) is covered by the support
of the transformation functions Gi(ξ|γi). Hence, with q2(ξ|ξ′) > 0 and q3(ξ) > 0 on
the support of the prior distribution, qcop(ξ|ξ′) > 0 on the support of π(ξ|y), which
yields the regularity condition of Theorem 4.1.
Remark 6.1 (Strong Doeblin condition ). The heavy-tailed independent proposal func-
tion q3(ξ) guarantees that the proposal distribution qcop(ξ|ξ′) has uniformly heavier
tails than the posterior distribution π(ξ|y). Hence, the strong Doeblin condition1 holds,
i.e. there exists an integer s > 0 and a constant as ∈(0, 1] such that
(qcop)s(ξ, ξ′) ≥asπ(ξ|y)
for all ξ, ξ′ ∈Rn.
(6.5)
1See e.g. Holden [2000] for details.
97

6. IMPROVING THE METROPOLIS-HASTINGS ALGORITHM
USING COPULAS
Algorithm 3: The CIMH algorithm
(i) Input: RWMH prerun samples {˘ξ
(j)}j=0,...,T ′ with ˘ξ
(j) = (˘ξ(j)
1 , . . . , ˘ξ(j)
n )⊤, sampling
parameters r1, and r2, variable permutation function ι, chain length T, starting value
ξ0, and transition densities q2 and q3.
Output: Markov chain {ξ(j)}j=0,...,T .
Set ξ(0) ←ξ0
(ii) for i ←1 to n do
Fit ˆγi for parametrized cdf Gi(·|γi) based on {˘ξ(k)
i
}k=0,...,T ′
for k ←0 to T ′ + j do
Set ˘u(k)
i
←Gi(˘ξ(k)
i
|ˆγi)
(iii) Fit ˆθ for D-vine copula c1,...,n(u1, . . . , un|θ) based on {(˘u(k)
ι(1), . . . , ˘u(k)
ι(n))⊤}k=0,...,T ′
for j ←1 to T do
(iv)
Sample r ∼U[0, 1]
if r ≤r1 then
Generate (˜u1, . . . , ˜un)⊤with density c1,...,n(u1, . . . , un|ˆθ)
for i ←1 to n do
Set ˜ξi ←G−1
ι(i)(˜uι−1(i)|ˆγι(i))
Deﬁne ˜ξ = (˜ξ1, . . . , ˜ξn)⊤
else if r1 < r ≤r1 + r2 then
Sample ˜ξ ∼q2(ξ|ξ(j−1))
else if r > r1 + r2 then
Sample ˜ξ ∼q3(ξ)
Set
ξ(j) ←



˜ξ
with probability αcop(˜ξ|ξ(j−1))
ξ(j−1)
with probability 1 −αcop(˜ξ|ξ(j−1))
As pointed out by Holden et al. [2009], if the strong Doeblin condition does not hold
for some states in the sampling space, the MCMC algorithm will tend to undersample
these areas. This is not crucial, if further inference does not depend on tail behavior,
but may become problematic for questions of extreme value theory.
Remark 6.2 (Pairwise copula estimation ). The estimation of the copula decomposition
parameters of step (iii) for a given parameter order ι can be done as follows: Since the
number of parameters grows quadratically in the dimension n, it is useful to consider
a stepwise estimation approach, where we estimate the parameters from pair copulas
98

6.1 Copula based Independence MH algorithm (CIMH)
with no conditioning to the ones with n −2 conditioning variables. In an initial step,
we estimate the parameters corresponding to the pair copulas with no conditioning.
For the copula parameters with a single conditioning value, we transform the data
with the appropriate conditional cdf’s using the estimated parameters to determine
pseudo realizations needed in the pair copulas with a single conditioning variable. We
proceed as before until all parameters have been estimated. These so-called sequen-
tial estimates have shown to be consistent and asymptotically normally distributed
(Hobæk Haﬀ[2010]). They are then used as starting values for numerically determin-
ing the maximum likelihood estimates. When several bivariate copula families for a
pair copula term are available, the family is chosen according to Akaike’s information
criterion (AIC). Brechmann [2010] shows that AIC performs well with regard to several
alternatives. The R package CDVine of Brechmann & Schepsmeier [2011] can be used
to ﬁt the according D-vines.
Remark 6.3 (Likelihood based copula parameter estimation ). Likelihood based copula
parameter estimation was ﬁrst proposed by Aas et al. [2009] and current developments
in this active area can be found in Czado [2010] and Kurowicka & Joe [2011]. Bayesian
analyses of D-vines using MCMC are also available (see Min & Czado [2010]). Further-
more, Bayesian model selection methods are implemented using indicator variables by
Smith et al. [2010] and using reversible jump MCMC by Min & Czado [2011], respec-
tively.
6.1.2
CIMH as adaptive sampling scheme
Interestingly, CIMH can also be seen as an adaptive MCMC sampler. Before we elab-
orate this, we need to introduce some more notation. We follow Roberts & Rosenthal
[2007]: As usual, let X(t) be a real valued random variable for t ∈N0. Moreover, let
Γ(t) be a K-valued random variable representing the choice of the transition kernel for
updating X(t) to X(t+1). The random variable (X(t), Γ(t)) generates a ﬁltration
Gt = σ(X(0), . . . , X(t), Γ(0), . . . , Γ(t))
i.e. Gt is the smallest σ-algebra with respect to which (X(t), Γ(t)) is measurable for all
s < t. Clearly, Gs ⊆Gt for all s < t. The Markov chain transition kernel is then
kγ(A|ξ) = P(X(t+1) ∈A|X(t) = ξ, Γ(t) = γ, Gt−1)
for some ξ ∈Rn, A ⊆Rn, and γ ∈K.
99

6. IMPROVING THE METROPOLIS-HASTINGS ALGORITHM
USING COPULAS
Deﬁnition 6.1 (Independent adaption). An adaptive MCMC algorithm is called in-
dependent adaption algorithm, if for all t ∈N the random variable Γ(t) is independent
of X(0), . . . , X(t).
Now, let us interpret CIMH as an independent adaption algorithm: Suppose {k1, k2, k3}
is the set of Markov chain kernels induced1 by the proposal functions q1, q2 and q3
of CIMH, respectively.
Then for i = 1, . . . , 3, ki(ξ|ξ′) is ergodic for the posterior
distribution π(ξ|y), that is, each ki(ξ|ξ′) converges to the equilibrium distribution
π(ξ|y). Nevertheless, the speed of convergence can diﬀer severely. Instead of using
the proposal function qcop introduced above, we can in each MCMC step j sample the
proposal function qi, i ∈{1, 2, 3}, according to
P(Γ(j) = 1) = r1,
P(Γ(j) = 2) = r2,
and
P(Γ(j) = 3) = 1 −r1 −r2.
The Metropolis-Hastings acceptance probability is then computed with respect to qi of
step j instead of qcop, i.e.
α(ξp|ξ(j)) = min
(
π(ξp|y)qi(ξ(j)|ξp)
π(ξ(j)|y)qi(ξp|ξ(j))
, 1
)
.
This somewhat reduces the computational cost and speeds up the inference process as
the proposed sample ξp ∼qi of step j does not need to be evaluated by the proposal
density functions qj for j ̸= i in the Metropolis-Hastings acceptance probability. We
call this sampling scheme independent adaption CIMH.
Proposition 6.2 (Convergence of CIMH as independent adaption algorithm). The
independent adaption CIMH sampling scheme converges to the posterior equilibrium
distribution.
Proof. As qi(·|·) > 0 for i = 1, 2, 3 on the support of the prior distributions, the
corresponding Markov chain {X(t)}t∈N0 of the independent adaption CIMH is π(·|y)-
irreducible, and Harris recurrent for the posterior π(·|y). Again, since for each t ∈N,
X(t) = X(t+1) with positive probability the aperiodicity condition holds. We are left
to prove invariance of the posterior distribution, i.e.
Z
ξ∈Rn P(X(t+1) ∈A|X(t) = ξ, Gt−1)π(dξ|y) = π(A|y)
for all A ⊆Rn.
1See e.g. the proof of Theorem 4.1.
100

6.2 Adaptive Copula based Independence MH algorithm (ACIMH)
Since Γ(t) is independent of X(t′) for all t, t′ ∈N0 we have
Z
ξ∈Rn P(X(t+1) ∈A|X(t) = ξ, Gt−1)π(dξ|y)
=
Z
ξ∈Rn
3
X
γ=1
P(X(t+1) ∈A|X(t) = ξ, Γ(t) = γ, Gt−1)P(Γ(t) = γ|X(t) = ξ, Gt−1)π(dξ|y)
=
Z
ξ∈Rn
3
X
γ=1
P(Γ(t) = γ|Gt−1)kγ(A|ξ)π(dξ|y) = π(A|y)
3
X
γ=1
P(Γ(t) = γ|Gt−1) = π(A|y).
For more details on adaptive MCMC sampling we refer the reader to Fearnhead [2008].
6.2
Adaptive Copula based Independence MH algorithm
(ACIMH)
Based on short preruns {˘ξ
(j)}j=1,...T ′, it is sometimes diﬃcult to guarantee suﬃcient
sampling from the posterior’s marginals’ tails in order to ﬁt an eﬃcient proposal copula.
To avoid setting r1 + r2 ≪1 and thus generating rather ineﬀective proposals, we
propose an extension of the basic CIMH algorithm by sequentially updating the copula
functions based on preceding Markov chain samples. This changes the proposal function
qcop during the sampling process and leads to a limited adaption scheme: For integers
R, S > 0 we set the copula update probability for the jth MCMC step, Pu(j), to
Pu(j) =
(
1,
if j mod R ≡0 and j < R · S and
0,
otherwise.
(6.6)
This is, the estimated copula parameters ˆγ and ˆθ become dependent on the proposal
step j, resulting in a step dependent proposal function qcop(ξ|ξ(j), ˆγ(j), ˆθ
(j)), where
ˆγ(j) and ˆθ
(j) are updated based on the concatenated prerun samples and the samples
generated up to step j according to (6.6). We refer to the hybrid Adaptive Copula
update Independence chain/random walk MH algorithm as ACIMH. The pseudo code
for ACIMH is shown in Algorithm 4.
Proposition 6.3 (Convergence of ACIMH). The ACIMH sampling scheme converges
to the posterior equilibrium distribution.
101

6. IMPROVING THE METROPOLIS-HASTINGS ALGORITHM
USING COPULAS
Proof. The update probability (6.6) initiates a total of S update steps (including the
initial copula estimation after the prerun). Hence, K = {1, . . . , S}. Setting
d(ξ, γ, s) = ∥ks
γ(·|ξ) −π(·|y)∥TV
for the posterior distribution π(·|y), ξ ∈Rn, γ ∈{1, . . . , S}, and the s step transition
kernel
ks
γ(A|ξ) = P(X(t+s+1) ∈A|X(t) = ξ, Γ(t) = γ, Gt−1),
(A ⊆Rn),
we have lims→∞d(ξ, γ, s) = 0 independent of ξ ∈Rn.
After R′ = (S −1) · R
Markov chain steps Γ(R′+s) = Γ(R′) for all s ≥0. For any realization ξ(0), . . . , ξR′ and
γ(0), . . . , γR′ of the Markov chain generated by ACIMH, lims→∞d(ξ(R′), γ(R′), s) = 0
independent of ξ(0), . . . , ξR′ and γ(0), . . . , γR′.
Therefore, for all A ⊆Rn and any
ξ(R′), ξ(0) ∈Rn
0 = lim
s→∞|ks
γ(A|ξ) −π(A|y)| = lim
s→∞|P(X(s+1) ∈A|X(0) = ξ(0), Γ(0) = γ(0)) −π(A|y)|.
6.3
Performance of CIMH and ACIMH
For benchmarking CIMH and ACIMH, the algorithms were tested on four examples:
First, we draw samples from a strongly correlated two dimensional normal distribution.
This is a simple proof-of-concept example of an analytically tractable system. Subse-
quently, we turn to dynamic systems deﬁned by diﬀerential equations (DEs). More
precisely, examples 2 and 3 examine the performance for ordinary nonlinear parame-
ter dependencies and parameter distributions with non-symmetric tail dependencies.
Finally, we apply our samplers to a delay diﬀerential equation (DDE) model of the
JAK2-STAT5 signaling pathway as published by Swameye et al. [2003]. Here, a so-
phisticated proposal generation is crucial as there exists no closed form solution of the
DDE system, calling for a computationally very expensive numerical solution for ev-
ery evaluation of the likelihood. Moreover, the seven parameters involved show high
dependency, which additionally complicates the posterior inference.
We evaluated the following performance indices: (I1) the quotient of acceptance rate
and INEFF. This was motivated by the antagonistic behavior of high acceptance rates
102

6.3 Performance of CIMH and ACIMH
Algorithm 4: The ACIMH algorithm
(i) Input: RWMH prerun samples {˘ξ
(j)}j=0,...,T ′ with ˘ξ
(j) = (˘ξ(j)
1 , . . . , ˘ξ(j)
n )⊤, update and
sampling parameters R, S, r1, and r2, variable permutation function ι, chain length
T, starting value ξ0, and transition densities q2 and q3.
Output: Markov chain {ξ(j)}j=0,...,T .
Initialize s ←0
Set ξ(0) ←ξ0
for j ←0 to T do
if j mod R ≡0 and j < R · S then
Update s ←s + 1
(ii)
for i ←1 to n do
Fit ˆγ(s)
i
for parametrized cdf Gi(·|γi) based on {˘ξ(k)
i
}k=0,...,T ′+j
for k ←0 to T ′ + j do
Set ˘u(k)
i
←Gi(˘ξ(k)
i
|ˆγ(s)
i )
(iii)
Fit ˆθ
(s) for D-vine copula c1,...,n(u1, . . . , un|θ) based on {(˘u(k)
ι(1), . . . , ˘u(k)
ι(n))⊤}k=0,...,T ′+j
(iv)
Sample r ∼U[0, 1]
if j > 0 then
if r ≤r1 then
Generate (˜u1, . . . , ˜un)⊤with density c1,...,n(u1, . . . , un|ˆθ
(s))
for i ←1 to n do
Set ˜ξi ←G−1
ι(i)(˜uι−1(i)|ˆγ(s)
ι(i))
Deﬁne ˜ξ = (˜ξ1, . . . , ˜ξn)⊤
else if r1 < r ≤r1 + r2 then
Sample ˜ξ ∼q2(ξ|ξ(j−1))
else if r > r1 + r2 then
Sample ˜ξ ∼q3(ξ)
Set
ξ(j) ←



˜ξ
with probability αcop(˜ξ|ξ(j−1))
ξ(j−1)
with probability 1 −αcop(˜ξ|ξ(j−1))
and ˘ξ
(T ′+j) ←ξ(j)
versus high INEFF’s as the Markov chain converges slowly for small proposal variances
and the MH algorithm conversely rejects a large amount of its proposed moves for too
high variances (see Roberts et al. [1997], Liu [2008], or Girolami & Calderhead [2011]).
103

6. IMPROVING THE METROPOLIS-HASTINGS ALGORITHM
USING COPULAS
Clearly (I1) ∈[0, 1], with higher values being superior. Furthermore, we monitor the
number (I2) of i.i.d. samples generated per second. As all algorithms were implemented
in Matlab using the same underlying MH code, (I2) is well justiﬁed. Time here denotes
the CPU-time on a two Six-Core Opteron 2427 (2.2 GHz) machine.
Using (I1) and (I2), the performance of CIMH and ACIMH was compared to (i) a regu-
lar RWMH, (ii) an IMH, and (iii) a random walk MH algorithm with a covariance based
proposal function (CovRWMH) in the ﬁrst three examples. The JAK2-STAT5 pathway
was additionally evaluated by means of the (iv) SMALA and (v) M-GaA algorithms.
In each example jointly updating all parameters of RWMH at once outperformed sin-
gle parameter updates with respect to the acceptance rates. Therefore, a joint update
scheme was used for all algorithms. Since there is generally little to no knowledge about
the underlying parameter dependency, we applied an uncorrelated normal update func-
tion in RWMH, i.e. a new proposal ξp was generated based on the current sample ξ(c)
by ξp = ξ(c) + ε for ε ∼N(0, ΣRW ) with ΣRW deﬁned as follows: We determined
the maximum a posteriori estimate for all n parameters using a simulated annealing
algorithm. Denoting these estimates by si, the ith diagonal element of ΣRW was set
to kRW · si, where kRW was adjusted in each example to yield an acceptance rate of
approximately 23% as suggested in Roberts et al. [1997] – our exact limits were set to
10% and 36%. This approach tries to compensate the sometimes large diﬀerences in
parameter magnitude and therefore sensitivity. The unthinned Markov chains sampled
by RWMH were directly taken as prerun samples for IMH, CovRWMH, CIMH, and
ACIMH. The sampling times for RWMH were added to the respective sampling times
of IMH, CovRWMH, CIMH, and ACIMH.
Two major issues of RWMH are (P1) a rather strong autocorrelation between subse-
quent MCMC iterations and (P2) its lack of incorporating any information about the
limiting distribution when proposing new samples. To address (P1) we set up the IMH
whose proposals are generated independently of the current state of the Markov chain:
as for the copula based algorithms we ﬁt one-dimensional parametrized cdf’s Gi(ξ|γi) to
each of the n empirical marginal parameter distributions sampled in the prerun. In fact,
these were identical for IMH, CIMH, and ACIMH. The IMH proposals ˜ξ(j)
i
were jointly
generated by sampling n · (T + 1) independent samples u(j)
i
∼U[0, 1] (i = 1, . . . , n,
j = 0, . . . , T), which are subsequently transformed to ˜ξ(j)
i
= G−1
i (u(j)
i |ˆγi). In other
104

6.3 Performance of CIMH and ACIMH
words, IMH generates proposals assuming an independent parameter structure. The
MH acceptance probability is given by
α = min
(
π(ξp|y) · Q
i gi(ξ(c)
i |ˆγi)
π(ξ(c)|y) · Q
i gi(ξp
i |ˆγi)
, 1
)
,
where gi(ξ|ˆγi) denotes the pdf to Gi(ξ|ˆγi). This somewhat reduces the autocorrelation
compared to the Markov chain of the RWMH since all proposals are independent of
any foregoing MCMC elements.
Nevertheless, the IMH Markov chain can still be
strongly autocorrelated and needs to be thinned.
Note that with independence in
π(ξ|y) = Q
i π(ξi|y) and gi(ξi|ˆγi) = π(ξi|y) for all i, the MH acceptance probability
collapses to α = 1, allowing the IMH to generate an independent sample in each MCMC
step.
Rather than directly reducing the autocorrelation in the Markov chain, CovRWMH
exploits the expected covariance matrix ˆC of the prerun and addresses (P2): The reg-
ular RWMH proposal function is changed to ξp = ξ(c) + ε with ε ∼N(0, kCovRW · ˆC).
Simulations show that the acceptance rate of CovRWMH outperforms the acceptance
rate of RWMH for kCovRW = kRW . Thus, ﬁne tuning kCovRW to yield an approxi-
mate acceptance rate of 23% generally decreases the autocorrelation in the CovRWMH
Markov chain by increasing the increments ξp
i −ξ(c)
i
compared to the regular RWMH.
While IMH and CovRWMH can in some sense be seen as antagonistic approaches
with respect to (P1) and (P2), CIMH and ACIMH address both issues at once. For
performance assessment CIMH and ACIMH were applied as introduced in Chapter
6.1 and 6.2. Throughout, the Metropolis-Hastings proposal function q2 was taken to
be identical with the one of the CovRWMH, reusing the tuning parameter kCovRW ;
q3 and the proposal probabilities r1 and r2 were adjusted individually (see Chapter
6.3.1 - 6.3.4).
For thorough performance evaluation, the ﬁrst three examples were
each run 100 times for 50,000 MCMC iterations, the last one 10 times for 50,000
MCMC iterations. In all examples the copula update parameters for ACIMH were
set to R =10,000 and S =4. While the copulas were ﬁtted on 1,000 prerun samples
in the ﬁrst three examples, we used 3,000 samples for the JAK2-STAT5 inference,
owing to the complexity of the system. The time for the prerun was added to the
sampling times of IMH, CovRWMH, CIMH, and ACIMH, respectively. Note that we
do not need to generate independent samples from the prerun’s MCMC chain since
105

6. IMPROVING THE METROPOLIS-HASTINGS ALGORITHM
USING COPULAS
ﬁtting the copula to dependent data only slightly eﬀects the eﬃciency of the proposal
distribution. This also holds for ﬁtting Gi(ξ|γi) and the estimation of the covariance
matrix in the CovRWMH proposal function. Nevertheless, insuﬃcient coverage of the
sampling space by the prerun samples decreases the performance of all samplers. For
copula ﬁtting and copula sample generation the CDVine R-package (Brechmann &
Schepsmeier [2011]) was used. Here, sampling from c1,...,n(u|ˆθ) is done sequentially
as proposed by Aas et al. [2009]. For the copula type of each pair copula term 32
types were available: Implemented are the independence copula, Gaussian copula (N),
Student-t copula (S), Clayton copula (C), Gumbel copula (G), Frank copula (F),
Joe copula (J), BB1 copula, BB6 copula, BB7 copula, BB8 copula, as well as the
corresponding 90◦, 180◦, and 270◦rotated versions of S, C, G, F, J, BB1, BB6, BB7,
and BB8 (see Appendix B for details). Copula type and the corresponding parameter(s)
for each copula term in (6.1) are estimated by a sequential likelihood based approach
described in Dißmann et al. [2011]. Finally, the SMALA and M-GaA algorithms were
directly applied as introduced in Chapter 5.1 and 5.3. In both cases the respective free
algorithmic parameters ε and α0 were tuned to yield a Metropolis-Hastings acceptance
rate between 10% −36% and of 23%, respectively. The SMALA geometric tensor G(·)
for the JAK2-STAT5 DDE system is computed in Appendix E.
6.3.1
Sampling from a strongly correlated 2-dim. normal distribution
In our ﬁrst example we want to draw samples from a strongly correlated bivariate
normal distribution N2 (µ, Σ) with respective mean and covariance matrix
µ =
µ1
µ2

=
0
0

and Σ =
 σ2
1
ρσ1σ2
ρσ1σ2
σ2
2

=

1
0.95 ·
√
3
0.95 ·
√
3
3

.
Here, ρ = 0.95. The MH acceptance probability for this example is given by
α = min



Φ2 (ξp | µ, Σ) · q(ξ(c)|ξp)
Φ2

ξ(c) | µ, Σ

· q(ξp|ξ(c))
, 1


,
where Φ2 (ξ | µ, Σ) is the density function of N2 (µ, Σ) and q(ξ|ξ′) the proposal func-
tion.
We chose this example as it is both illustrative as well as analytically tractable. Canon-
ically, the cdf’s of N(0, 1) and N(0, 3) were used to transform the prerun samples to
106

6.3 Performance of CIMH and ACIMH
[0, 1]2. The independence proposal density q3 was taken to be a bivariate student-t
distribution with location parameter (0, 1)⊤and identity scale matrix. Furthermore,
we set r1 = 0.99 and r2 = 0. Table 6.1 (Lower table) nicely shows that all samplers
approximate the two dimensional normal distribution with negligible errors. This can
also be seen from the thinned Markov chain samples depicted in Figure 6.1. The sam-
ples are based on the (unthinned) Markov chains from Figure 6.2. The chains exhibit
a better mixing behavior when generated by a copula based algorithm compared to
the non-copula based ones. Although the sampling times for IMH and CovRWMH
were about twice as long as for RWMH (Table 6.1 (Upper table)), IMH is comparable
with RWMH and CovRWMH even outperforms RWMH with respect to (I1) and (I2)
(Figure 6.3(a) and 6.3(b)). The most eﬃcient of all algorithms turned out to be CIMH.
Caused by the additional time needed for copula reﬁtting, ACIMH in average produced
slightly less independent samples per second than CovRWMH. We have to point out
that (I1) is very close to one for CIMH and ACIMH. This means that in almost every
MCMC iteration an independent sample was generated (see also Figure 6.4 for the
autocorrelation functions after thinning by INEFF). At ﬁrst sight this might almost
seem too good a result, but clearly, due to the simplicity of the problem, the copula
was ﬁt almost perfectly (Figure 6.3(c)) leading to an independent proposal function
qcop(ξ|ξ′) = qcop(ξ) that is very close to the true sampling distribution N2 (µ, Σ). This
pushes the MH acceptance probability close to one. We will see in later examples that
this index generally attains high values. It can also be seen as combined goodness-of-ﬁt
index for the ﬁtted marginal cdf’s and vine copula decomposition.
The copula families for ACIMH did not change in any of the 100 runs, meaning that the
dependency structure was already well covered by the preruns. The bivariate copula
c1,2(u1, u2|θ) of the ﬁrst of the 100 runs was ﬁt to be Gaussian with an estimated
parameter value of ˆθ = 0.953. This is very close to the actual correlation value of
ρ = 0.95. The corresponding Kendall’s τ for the copula parameters was estimated to
be ˆτm = 0.805, which coincides with the Kendall’s τ estimated for the prerun (Figure
6.3(c)). All other runs showed similar outcomes. Note that the time needed for ﬁtting
a single copula is almost identical to the time needed for the complete RWMH run (c.f.
Table 6.1 (Upper table)). All MCMC runs were started at the origin.
107

6. IMPROVING THE METROPOLIS-HASTINGS ALGORITHM
USING COPULAS
−5
0
5
−5
0
5
ξ1
ξ2
(a)
−5
0
5
−5
0
5
ξ1
ξ2
(b)
−5
0
5
−5
0
5
ξ1
ξ2
(c)
−5
0
5
−5
0
5
ξ1
ξ2
(d)
−5
0
5
−5
0
5
ξ1
ξ2
(e)
Figure 6.1: Thinned Markov chain samples of the ﬁrst run of the (a) ACIMH, (b) CIMH,
(c) CovRWMH, (d) IMH, and (e) RWMH. The red lines display the p · 10% quantiles of
the normal distribution for p = 1, . . . , 9.
Thinning was performed according to the INEFF and caused a nice decrease in the
autocorrelation functions as depicted in Figure 6.4. Interestingly, the INEFF slightly
underestimated the thinning rate for the last three algorithms, which can be seen from
the rather slow decreases in the autocorrelation functions. A nice analogy between
CovRWMH and the copula based algorithms is given by the fact that all three were
using the same Gaussian copula. However, while CovRWMH was applying it for lo-
cally proposing new samples, the (I1) index was quite low compared to CIMH and
ACIMH. On the other hand, CovRWMH was taking less than half the time of the
copula algorithms resulting in a very good performance on (I2).
108

6.3 Performance of CIMH and ACIMH
25000
50000
−5
0
5
ξ1
25000
50000
−5
0
5
ξ2
(a)
25000
50000
−5
0
5
ξ1
25000
50000
−5
0
5
ξ2
(b)
25000
50000
−5
0
5
ξ1
25000
50000
−5
0
5
ξ2
(c)
25000
50000
−5
0
5
ξ1
25000
50000
−5
0
5
ξ2
(d)
25000
50000
−5
0
5
ξ1
25000
50000
−5
0
5
ξ2
(e)
Figure 6.2:
Unthinned Markov chains of the ﬁrst run of the (a) ACIMH, (b) CIMH, (c)
CovRWMH, (d) IMH, and (e) RWMH. While the x-axis holds the step number, the y-axis
displays the parameter value. The copula based samplers show a slightly better mixing
behavior compared to the non-copula algorithms.
6.3.2
Performance on a steady state model with nonlinear parameter
dependency
We will now consider posterior inference in dynamic systems. We ﬁrst take a look at a
simple, but completely unidentiﬁable1 steady state example: Consider the system
dx(t)
dt
= 0
with
x(0) := x0
unknown.
(6.7)
We are interested in the behavior of y(t) = kx(t), where x(t) is the solution to (6.7).
The parameter k is introduced since biological experiments very often only yield relative
concentrations. Again, k is unknown. At the time points t = ti = i (i = 1, . . . , 5) we
1Note that MCMC algorithms are able to deal with unidentiﬁable systems as such. The inference
of meaningful reaction rates is however not possible in this scenario.
109

6. IMPROVING THE METROPOLIS-HASTINGS ALGORITHM
USING COPULAS
(a)
(b)
 
 
0  
0.5
1  
˘u2
0  
0.5
1  
0  
0.5
1  
τe = 0.80
˘u2
˘u1
0  
0.5
1  
˘u1
(c)
Figure 6.3:
Results for the 2-dim. normal distribution. Figure (a): Quotient of accep-
tance rate versus INEFF (I1). Figure (b): Number of i.i.d. samples drawn per second (I2).
Error bars show the estimated standard errors based on 100 runs. Figure (c): Marginal
copula data (c.f. Chapter 6.1.1 (ii)) to ﬁt the CIMH/ACIMH copula of run one – for
uniformization the cdf’s of N(0, 1) and N(0, 3) were applied. The diagonal holds the his-
tograms of the MCMC sample marginals and τe is the corresponding empirical Kendall’s
τ.
0
5
10
15
0
1
ξ1
0
5
10
15
0
1
ξ2
(a)
0
5
10
15
0
1
ξ1
0
5
10
15
0
1
ξ2
(b)
0
5
10
15
0
1
ξ1
0
5
10
15
0
1
ξ2
(c)
0
5
10
15
0
1
ξ1
0
5
10
15
0
1
ξ2
(d)
0
5
10
15
0
1
ξ1
0
5
10
15
0
1
ξ2
(e)
Figure 6.4: Autocorrelation functions of the ﬁrst run of the (a) ACIMH, (b) CIMH, (c)
CovRWMH, (d) IMH, and (e) RWMH. All plots range over 15 lags on the x-axis. The
INEFF is slightly underestimated in the last three algorithms.
110

6.3 Performance of CIMH and ACIMH
Sampler
i.i.d. samples
INEFF
AR (%)
time (sec.)
ACIMH
48250.0± 641.1
1.1±0.03
95.3±0.6
51.1± 0.3
CIMH
48750.0± 547.6
1.1±0.02
95.2±1.1
25.8± 0.2
CovRWMH
21051.5±1318.3
3.6±0.30
21.9±0.4
17.6±4 · 10−3
IMH
7821.3± 566.4
14.9±1.70
24.3±0.4
14.6±7 · 10−3
RWMH
4836.9± 316.2
17.9±1.70
23.5±0.2
8.3±2 · 10−3
Sampler
ˆµ1 −µ1
ˆµ2 −µ2
ˆσ2
1 −σ2
1
ˆσ2
2 −σ2
2
ˆρ −ρ
(×10−3)
(×10−3)
(×10−3)
(×10−3)
(×10−5)
ACIMH
0.40±0.46
0.27±0.77
−0.06±0.77
−1.35± 2.17
−692.84±0.04
CIMH
−0.33±0.50
−0.46±0.86
−0.86±0.62
−1.54± 1.68
−692.90±0.05
CovRWMH
−1.91±1.37
−3.80±2.41
0.48±2.17
2.44± 6.51
−692.85±0.13
IMH
1.74±3.19
3.82±5.57
−4.92±7.25
−13.88±20.96
−693.69±0.38
RWMH
4.12±3.07
6.94±5.41
1.52±3.67
5.04±11.92
−692.79±3.07
Table 6.1: Two-dimensional normal example. Upper table: Depicted are the average
number of i.i.d. samples per run (ESS), INEFF’s, acceptance rates (AR), and sampling
times based on 100 runs including estimated standard errors. All samplers ran for 50,000
MCMC proposals. Lower table: Residual diﬀerences between the average posterior mean,
standard deviation, and correlation coeﬃcient estimates and the true parameter values
based on 100 runs including estimated standard errors.
The true values are µ1 = 0,
µ2 = 0, σ2
1 = 1, σ2
2 = 3, and ρ = 0.95.
measure noisy observations yi of y(t), which are assumed to follow yi ∼N(kxi, 0.12),
with xi = x(ti). For simplicity we chose k = x0 = 1 for toy data generation (see Figure
6.5(a)). We thus have x(t) = kx0 = 1. Although there is clearly no way to determine
any of the parameters k or x0 = 1/k here, models such as the one of the JAK2-
STAT5 pathway introduced by Swameye et al. [2003] suﬀer a similar kind of practical
unidentiﬁability (see Section 6.3.4). Nevertheless, the system provides a nice benchmark
for the performance of our algorithms on nonlinear parameter dependencies. Since no
prior knowledge other than x0 > 0 and k > 0 is available we assume independent
uniform prior distributions x0 ∼U[0, 2.5] and k ∼U[0, 2.5]. The system’s posterior
distribution is then given by
π(x0, k|y) = 1
Z
5
Y
i=1
Φ(yi|kxi, 0.12)1[0,2.5](x0)1[0,2.5](k)
with normalizing constant
Z =
2.5
Z
0
2.5
Z
0
5
Y
i=1
Φ(yi|kxi, 0.12) dx0 dk.
(6.8)
111

6. IMPROVING THE METROPOLIS-HASTINGS ALGORITHM
USING COPULAS
Here, Φ(·|µ, σ2) denotes the density function of a univariate normal random variable
with mean µ and variance σ2 and 1[a,b] the density function corresponding to U[a, b].
Note that xi = x(ti) depends on the initial condition x0.
Sampler
i.i.d. samples
INEFF
AR (%)
time (sec.)
ACIMH
35446.3±1511.5
1.8± 0.1
70.40±0.05
43.948± 1.3
CIMH
35224.9±1501.6
1.8± 0.1
70.20±0.09
23.043± 0.3
CovRWMH
1594.4± 134.5
56.4± 4.5
10.48±0.04
13.847±6 · 10−3
IMH
1576.6± 128.9
58.9± 5.5
5.43±0.02
11.129±6 · 10−3
RWMH
335.8±
23.5
241.5±18.9
21.14±0.03
6.532±3 · 10−3
ACIMH
CIMH
CovRWMH
IMH
RWMH
E[k|y]
1.129±0.001
1.128±0.001
1.125±0.003
1.129±0.003
1.124±0.007
E[x0|y]
1.129±0.001
1.130±0.001
1.132±0.003
1.128±0.003
1.135±0.008
Table 6.2: Steady state model. Upper table: Depicted are the average number of i.i.d.
samples per run (ESS), INEFF’s, acceptance rates (AR), and sampling times based on 100
runs including standard errors. All samplers ran for 50,000 MCMC proposals. Lower table:
Average posterior mean estimates for k and x0 based on MCMC samples. Standard errors
are estimated based on 100 runs. The numerical estimate is 1.129.
We chose the independence proposal density q3 to be uniform on [0, 2.5]2, which co-
incides with the joint prior distribution. Again, r1 = 0.99 and r2 = 0. The prerun
samples ˘ξ
(j) = (k(j), x(j)
0 )⊤(j = 1, . . . ,50,000) shown in Figure 6.5(c) indicate that
the sample marginals for k and x0 are close to being lognormally distributed. Hence,
we used two lognormal distributions gi(·|µi, σi) (i = 1, 2) for the uniformization step.
Although for i = 1, 2 and ﬁtted parameters ˆµi, ˆσi for µi, σi the transformed samples
˘
u(j)i = gi( ˘
ξ(j)i|ˆµi, ˆσi) (j = 1, . . . ,50,000) are not exactly uniformly distributed (c.f.
histograms in Figure 6.5(b)), the CIMH and ACIMH proposals were accepted with a
probability of about 70% (Table 6.2 (Upper table)). The approximation of the copula
proposal function to the posterior of equation (6.8) was nevertheless very good as the
index (I1) had values around 0.5 for CIMH and ACIMH (Figure 6.7(a)). This can
also be seen from the excellent mixing behavior of the Markov chains for the copula
based sampling schemes (Figure 6.6). Moreover, these algorithms drew an indepen-
dent sample of the posterior in approximately every second MCMC iteration. We also
tested uniformization by exponential and Gamma distributions ending up with com-
parable results (not shown). While the performance of CovRWMH with respect to
112

6.3 Performance of CIMH and ACIMH
(a)
 
 
0.2 0.5 0.8
˘u2
0.2 0.5 0.8
0.2
0.5
0.8
τe = −0.95
˘u2
˘u1
0.2
0.5
0.8
˘u1
(b)
 
 
0.5 1 1.5 2
x0
0.5 1 1.5 2
0.5
1
1.5
2
τe = −0.95
x0
k
0.5
1
1.5
2
k
(c)
Figure 6.5: (a) Toy data used for sampling the steady state model. Depicted are the true
underlying concentration x(t) (solid line), the posterior median solution (dashed line) as
well as the according 95% credible interval (shaded area) of the thinned ﬁrst ACIMH run.
The dots depict noisy data yi including 95% conﬁdence intervals. (b) Copula data (c.f.
Section 6.1.1 (ii)) of the ﬁrst run used to ﬁt the CIMH copula. For uniformization of k
and x0 the cdf’s of LN(µ1= -0.03, σ1=0.54) and LN(µ2=0.01, σ2=0.54) were applied. The
diagonal displays the histograms of the MCMC sample marginals and τe the respective
empirical Kendall’s τ. (c) Thinned MCMC samples of the ﬁrst RWMH run.
25000
50000
0
2.5
k
25000
50000
0
2.5
x0
(a)
25000
50000
0
2.5
k
25000
50000
0
2.5
x0
(b)
25000
50000
0
2.5
k
25000
50000
0
2.5
x0
(c)
25000
50000
0
2.5
k
25000
50000
0
2.5
x0
(d)
25000
50000
0
2.5
k
25000
50000
0
2.5
x0
(e)
Figure 6.6:
Unthinned Markov chains of the ﬁrst run of the (a) ACIMH, (b) CIMH, (c)
CovRWMH, (d) IMH, and (e) RWMH. While the x-axis holds the step number, the y-axis
displays the parameter value. The copula based samplers show a superior mixing.
113

6. IMPROVING THE METROPOLIS-HASTINGS ALGORITHM
USING COPULAS
(a)
(b)
Figure 6.7:
Results for the steady state model. Figure (a): Quotient of acceptance rate
and INEFF (I1). Figure (b): Number of independent samples drawn per second (I2). Error
bars show the estimated standard errors based on 100 runs.
(I2) was similar to the performance of ACIMH in the ﬁrst example, it here ran into
problems generating adequate proposals due to the nonlinear parameter dependency.
The estimated Pearson correlation coeﬃcient used by CovRWMH lay around ˆρ = −0.9,
which is close to the rank based estimated Kendall’s τ, given as τe = −0.95 (Figure
6.5(b)). Nevertheless, ACIMH outperformed CovRWMH as well as RWMH and IMH
more than 5-fold with respect to (I2). All ACIMH copulas were ﬁtted to be of Gaussian
type with a correlation parameter of ˆθ = −0.99 and did not change throughout the
sampling process. Saving the time for copula reﬁtting, CIMH even outperformed the
non copula based algorithms more than 10-fold (Figure 6.7(b)). The MCMC samples’
based solutions to equation (6.7) nicely approximate the data: Figure 6.5(a) depicts
the posterior median solution with corresponding 95% credible interval for the thinned
ﬁrst ACIMH run, i.e. at time point t equation (6.7) was solved numerically for all
ACIMH MCMC samples (after thinning); subsequently the pointwise median over all
solutions – called posterior median solution – as well as the 95% credible interval were
computed. Although in general neither the posterior median solution, nor the upper
or lower boundary of the 95% credible interval need to be a solution to a diﬀerential
equation system, the steady state property in this example guarantees that all three
are in fact solutions to (6.7).
For analytically verifying the sampling results, we numerically evaluated the expected
114

6.3 Performance of CIMH and ACIMH
posterior means of k and x0. The means are given by
E[x0|y] = E[k|y] =
2.5
R
0
k
2.5
R
0
5Q
i=1
Φ(yi|kxi, 0.12)dx0dk
2.5
R
0
2.5
R
0
5Q
i=1
Φ(yi|kxi, 0.12)dx0dk
= 1.129.
(6.9)
As Table 6.2 (Lower table) shows, all samplers closely approximated (6.9).
6.3.3
Performance on a small compartment model
Our last toy example is motivated by a model for the biokinetic behavior of zirconium
(Zr) in the human body (see Chapter 8). Compartmentalizing major organs, Li et al.
[2011a,b] analyzed the circulation of Zr in the human body.
The paper compares
transfer rates of two competing compartment models with respect to sensitivity and
predictability in order to establish a new model for radiation risk analysis. Both models
are structurally identical as far as the interaction of the compartments “Small intestine”
and “Transfer” is concerned, which is what our toy model is based on: After ingestion
Zr passes through the “Small intestine”. Subsequently it is either excreted directly
or passes through the “Transfer” compartment as depicted in Figure 6.8(a).
Since
taking accurate measurements of Zr in the “Small intestine” compartment is technically
not possible, we chose to generate data for the “Transfer” compartment only. The
diﬀerential equations underlying the data are
dx1(t)
dt
= −k2x1(t) −k3x1(t)
and
dx2(t)
dt
= k2x1(t) −k1x2(t)
(6.10)
with x1(0) = 100 and x2(0) = 0 in arbitrary units, making our model similar to
the ones proposed in Li et al. [2011a,b]. Note that the concentrations xi(t) depend
on k1, k2, and k3.
However, for readability the dependency on these parameters is
omitted. Our data was generated for k1 = 1, k2 = 1, and k3 = 20 at the time points
ti = 0, 0.1, 0.2, . . . , 1.0 as yi = x2(ti) + εi with εi
i.i.d.
∼N(0, 12) for i = 1, . . . , 11. We
assume the initial concentrations x1(0) = 100 and x2(0) = 0 to be known.
Under
independent prior distributions k1 ∼N[0,1000](1, 12), k2 ∼N[0,1000](1, 12), and k3 ∼
N[0,1000](20, 202), where N[a,b](µ, σ2) denotes the [a, b]-truncated normal distribution,
the posterior is proportional to
π(k1. . . . , k3|y1, . . . , y11) ∝
11
Y
i=1
Φ(yi|x2(ti), 12)
2
Y
j=1
Φ[0,1000](kj|1, 12)Φ[0,1000](k3|20, 202)
115

6. IMPROVING THE METROPOLIS-HASTINGS ALGORITHM
USING COPULAS
for the truncated normal density function Φ[a,b](·|µ, σ2) corresponding to N[a,b](µ, σ2).
The data is depicted in Figure 6.8(b).
As independence proposal density q3 we chose a uniform distribution on [0, 1000]3 and
set r1 = 0.99 and r2 = 0.
There is an interesting dependency structure between
the parameters k1, k2, and k3 inherent to the system. While k2 and k3 show strong
positive but non-symmetric dependency, k1 is almost independent of k2 and k3 (compare
Figure 6.8(c) and recall that the dependency structure between the ki’s and the ˘ui’s is
identical). This can be explained as follows: An increase in k3 at constant k2 results
in a decrease of the Zr concentration in the “Transfer” compartment.
In order to
compensate this eﬀect k2 needs to be increased simultaneously, resulting in a positive
dependence between k2 and k3. The degradation rate k1 on the other hand is pairwise
almost independent from k2, and k3 since it only depends on the concentration x2(t),
which itself depends directly on the data (y1, . . . , y11)⊤.
After uniformization of the prerun samples by ﬁtted normal distributions the pairwise
scatterplots in Figure 6.8(c) indicate some lower tail dependence between the param-
eters ˘u2 and ˘u3 (corresponding to k2 and k3).
Using the notation of Chapter 2.2
the proposal copula for the ﬁrst of the 100 runs was decomposed as c1,2,3(u1,2,3|θ) =
c1,2(u1,2|θ) · c2,3(u2,3|θ) · c1,3|2(F(u1|u2, θ), F(u3|u2, θ)|θ) with the following estimated
pair copulas: (i) ˆc1,2 a 90◦rotated Clayton copula with estimated parameter ˆθ1,2 =
−0.19 and corresponding estimated Kendall’s ˆτ = −0.08, (ii) ˆc2,3 a 180◦rotated
BB6 copula with estimated parameters ˆθ2,3 = (1.24, 3.07) and corresponding esti-
mated Kendall’s ˆτ = 0.71, and (iii) ˆc1,3|2 a Gaussian copula with estimated parameter
ˆθ1,3|2 = −0.79 and corresponding estimated Kendall’s ˆτ = −0.58. Note that the es-
timated Kendall’s τ’s of the ˆc1,2 and ˆc2,3 copula nicely coincided with the estimated
Kendall’s τ’s of the copula sample (Figure 6.8(c)). This indicates a good parameter
dependency coverage by the proposal copula, at least on the unconditioned copula
decomposition level. All other runs yielded similar decompositions. The order ι was
chosen to be the identity function. Lower tail dependence of ˘u2 and ˘u3 was covered
by the rotated 180◦BB6 copula ˆc2,3. Interestingly, the estimated pairwise dependency
between k1 and k3 (τe = −0, 28, Figure 6.8(c)) more than doubles when conditioning on
k2, as the estimated Kendall’s τ corresponding to ˆθ1,3|2 is given by ˆτ = −0.58. Loosely
speaking Zr is to be excreted within a ﬁxed time period. Moreover, Figure 6.8(c) gives
116

6.3 Performance of CIMH and ACIMH
Transfer
Small 
intestine
x1
x2
k1
k3
k2
(a)
(b)
 
 
0.20.6 1
˘u3
0.20.6 1
0.20.6 1
0.2
0.6
1
τe = 0.71 ˘u2
0.2
0.6
1
τe = −0.28
˘u3
˘u1
τe = −0.06
˘u2
0.2
0.6
1
˘u1
(c)
Figure 6.8: (a) Schematic representation of the small compartment model. The con-
centration of the shaded “Transfer” compartment is measured at the eleven time points
ti = 0, 0.1, . . . , 1.0. The rates k1 and k3 lead to unobserved downstream compartments and
are therefore considered as degradation rates. (b) Toy data used for sampling the small
compartment model. Depicted are the true underlying concentration x2(t) of the “Trans-
fer” compartment (solid line), the posterior median solution (dashed line) as well as the
according 95% credible interval (shaded area) of the thinned ﬁrst ACIMH run. The dots
depict noisy data yi including 95% conﬁdence intervals and the unobserved concentration
x1(t) of the “Small intestine” compartment is shown as dashed-dotted line. (c) Copula
data (c.f. Section 6.1.1 (ii)) of the ﬁrst run used to ﬁt the CIMH copula. For uniformiza-
tion of k1, k2, and k3 the cdf’s of N(1.33, 0.552), N(29.40, 13.032), and N(1.29, 0.472) were
applied. The diagonal displays the histograms of the MCMC sample marginals and τe the
respective empirical Kendall’s τ.
a hint at the order for arranging the sequence of the copula variables. Here, ˘u2 and ˘u3
show a signiﬁcant dependency and were hence modeled as a direct pair within the pair
copula decomposition.
The ﬁne-tuning parameter kcovRW in CovRWMH could be set to a relatively high value
guaranteeing large jumps in the parameter space and therefore low INEFF’s (Table 6.3
(Upper table)). CovRWMH hence outperformed RWMH and IMH on both (I1) and
(I2) as can be seen from Figure 6.9(a) and 6.9(b). The dependency between k1, k2,
and k3 is signiﬁcant enough to cause IMH to perform even worse than RWMH on (I2).
Although taking in average more than 1.3 times as long as any other sampler, the
copula based algorithms nicely detected the prerun dependency structure and yielded
the best results. The INEFF slightly decreased when updating the copulas. This means
that the copula structure is – although only slightly – recursively adjusted to better
117

6. IMPROVING THE METROPOLIS-HASTINGS ALGORITHM
USING COPULAS
(a)
(b)
Figure 6.9: Results for the compartment model. Figure (a): Quotient of acceptance rate
and INEFF. Figure (b): Number of independent samples drawn per second. Error bars
show the estimated standard errors based on 100 runs.
ﬁt the true underlying dependency structure of k1, k2, and k3. However, compared to
CIMH the additional time for re-ﬁtting the copula lowered the eﬃciency with respect
to (I2). For the inference of the marginal MAP estimates, we applied a kernel density
estimator to the respective sampled Markov chains. The posterior mean and mode
estimates including 90% credible intervals are given in Table 6.3 (Lower table). All
predicted modes slightly overestimated the true values k1 = k2 = 1 and k3 = 20.
6.3.4
Performance on a JAK2-STAT5 signaling pathway model
In this section, we apply our sampling schemes to a DDE model of the JAK2-STAT5
signaling pathway1. Here, STAT5 denotes either one of the STAT5A or STAT5B pro-
teins. The system is based on a number of phosphorylation and dephosphorylation steps
within a complex protein interaction network. In case of the JAK2-STAT5 pathway
the Erythropoietin (Epo) hormone ﬁrst binds to the transmembrane receptor phos-
phorylating Janus Kinase 2 (JAK2). Monomeric Signal Transducer and Activator of
Transcription 5 (STAT5) is thereafter tyrosine phosphorylated by the JAK2/receptor
complex. After a dimerization step the phosphorylated STAT5 homodimer enters the
nucleus and binds to the promoter region of its target gene. It dephosphorylates in the
1For a thorough introduction to the JAK-STAT pathway see Chapter 2.4.2.
118

6.3 Performance of CIMH and ACIMH
Sampler
i.i.d. samples
INEFF
AR (%)
time (sec.)
ACIMH
25255.7±1509.4
3.9±0.9
75.07±0.12
75.10±0.28
CIMH
22414.5±1434.1
5.1±1.0
75.07±0.11
54.20±0.31
CovMH
8756.8± 491.8
7.9±0.5
23.38±0.06
40.94±0.02
IMH
4532.2± 406.9
23.5±2.9
20.79±0.04
40.14±0.02
RWMH
2963.5± 200.5
27.6±2.4
23.46±0.03
19.33±0.01
Sampler
ACIMH
CIMH
CovRWMH
IMH
RWMH
E[k1|y]
1.25
1.24
1.25
1.25
1.25
M[k1|y]
1.17
1.16
1.17
1.20
1.19
CI[k1|y]
(0.74;1.79)
(0.74;1.79)
(0.74;1.79)
(0.74;1.78)
(0.74;1.79)
E[k2|y]
1.54
1.54
1.54
1.53
1.53
M[k2|y]
1.33
1.35
1.34
1.31
1.29
CI[k2|y]
(0.86;2.31)
(0.86;2.31)
(0.86;2.31)
(0.86;2.30)
(0.85;2.30)
E[k3|y]
27.30
27.29
27.30
27.24
27.21
M[k3|y]
23.30
23.73
21.97
22.13
23.87
CI[k3|y]
(14.09;42.07)
(14.08;42.08)
(14.05;42.10)
(14.24;41.83)
(14.01;41.92)
Table 6.3: Small compartment model. Upper table: Depicted are the average number of
i.i.d. samples per run (ESS), INEFF’s, acceptance rates (AR), and sampling times based
on 100 runs including estimated standard errors.
All samplers ran for 50,000 MCMC
proposals. Lower table: Estimated marginal posterior means E[·|y], modes M[·|y] (MAP
estimates), and 90% posterior quantile based credible intervals CI[·|y] for k1, k2, and k3
for the concatenated data of 100 runs.
process and gets subsequently exported to the cytoplasm (Aaronson & Horvath [2002]
and Hou et al. [2002]). A schematic representation can be seen in Figure 6.10(c).
We want to point out that although posterior parameter estimates are given in Table 6.4
the focus of this section lies clearly on the performance evaluation of CIMH and ACIMH
on a complex dynamical system, rather than on novel biological insights. Due to the
complexity of the system caused by high parameter dependencies MCMC sampling is
a daunting task in this scenario. We therefore evaluated the performance of SMALA
and M-GaA (introduced in Chapter 5.1 and Chapter 5.3) in addition to the RWMH,
IMH, and CovRWMH algorithms.
119

6. IMPROVING THE METROPOLIS-HASTINGS ALGORITHM
USING COPULAS
(a)
(b)
Epo
target gene
x1
x2
x3
JAK2
P
P
P
P
P
P
P
cell membrane
nucleus
cytoplasm
P
P
P
P
STAT5
P
k1
k2
k3
k4
STAT5
STAT5
STAT5
STAT5
STAT5
STAT5
(c)
Figure 6.10: (a) Time courses for the numerical solution of phosphorylated STAT5 in
the cytoplasm (y1(t)). Depicted are the posterior median solution (dashed line) as well as
the according 95% credible interval (shaded area) of the thinned ﬁrst ACIMH run. The
dots represent given measurements yi including 95% conﬁdence intervals. (b) Similarly to
(a), the measurements, the median (dashed line), and the 95% credible interval (shaded
area) for the numerical solution of y2(t). (c) Schematic representation of the JAK2-STAT5
pathway: Erythropoietin (Epo) binds to the transmembrane receptor. Monomeric STAT5
(x1) is tyrosine phosphorylated (x2) by the activated JAK2/receptor complex in the cy-
toplasm. After dimerizing the phosphorylated STAT5 homodimer, (x3) enters the nucleus
and binds to the promoter target gene region. It is then dephosphorylated and released to
the cytoplasm.
Our analysis is based on the data and mass-action DDE model of Swameye et al. [2003]:
dx1(t)
dt
= −k1x1(t)Epo(t) + 2k4x3(t + τ)
dx2(t)
dt
= −k2x2
2(t) + k1x1(t)Epo(t)
dx3(t)
dt
= −k3x3(t) + 1
2k2x2
2(t)
dx4(t)
dt
= −k4x3(t + τ) + k3x3(t),
(6.11)
with x1(0) = 1 and x2(0) = x3(0) = x4(0) = 0, where Epo(t) denotes the time-
dependent Epo stimulation function and τ the time lag between STAT5 entering the
nucleus and dephosphorylated cytoplasmic release. Furthermore, x1(t), x2(t), x3(t) are
the concentrations of unphosphorlated, tyrosine-phosphorylated, and dimerized STAT5,
respectively, while x4(t) is the concentration of STAT5 in the nucleus. Note that the sys-
tem inherits a dimerization step as introduced in Example 2.10. Due to the law of mass
conservation, we need to claim k3 ≥k4. The data we used for inference was provided
120

6.3 Performance of CIMH and ACIMH
by J.Timmer at http://webber.physik.uni-freiburg.de/∼jeti/PNAS Swameye Data/. It
contains (including 95% conﬁdence intervals) the amount of phosphorylated STAT5,
yε
1(ti) = k5(x2(ti)+2x3(ti)+ε1(ti)) and the total concentration of cytoplasmic STAT5,
yε
2(ti) = k6(x1(ti)+x2(ti)+2x3(ti)+ε2(ti)), at 16 time points t1, . . . , t16 (in minutes) in
the interval [0, 60]. Here, k5 and k6 are introduced since all measurements are relative.
The errors εj(ti) are measurement errors included in the data, which are assumed to be
N(0, σ2
i,j) distributed where σ2
i,j was estimated from various experiments. We performed
a Kolmogorov-Smirnov test (Davison [2003]) on normality to ensure that the combined
measurement/model error of the likelihood can not possibly be non-Gaussian. For this,
the residuals between the data points and a simulated annealing MLE based time course
were considered. The null-hypothesis of non-Gaussian noise could not be rejected on
an α = 5% signiﬁcance level. All seven parameters ξ = (k1, k2, k3, k4, τ, k5, k6)⊤are
time-independent. Again, for readability the dependence of the solutions xi(t) to (6.11)
on ξ is omitted. A picture of the data can be seen in Figure 6.10(a) and 6.10(b). Simi-
larly to Swameye et al. [2003] we reparametrized the DDE system (see Appendix D) in
order to resolve structural parameter identiﬁability issues. A discussion on the struc-
tural parameter identiﬁability issues of the particular system can be found in Timmer
et al. [2004] and Raue et al. [2009]. Due to the lack of knowledge we chose the in-
dependent prior distributions k1, k2, k4, τ, k5, k6
i.i.d.
∼U[0, 50] and k3 ∼U[k4, 50]. The
lower limit 0 was canonically introduced by the non-negativity constraint for reaction
rates. For y := {yε
1(t1), . . . , yε
1(t16), yε
2(t1), . . . , yε
2(t16)} and the prior π(ξ) this leads to
the posterior
π(ξ|y) ∝
16
Y
i=1
Φ(yε
1(ti)|y1(ti), σ2
i,1) · Φ(yε
2(ti)|y2(ti), σ2
i,2) · π(ξ).
Here, y1(t) = k5(x2(t) + 2x3(t)) and y2(t) = k6(x1(t) + x2(t) + 2x3(t)) for the solutions
x1(t), x2(t), and x3(t) of the DDE. Since there is no analytical solution to (6.11), we
applied Matlab’s dde23 solver to numerically derive xi(t) (i = 1, 2, 3) in case of the
RWMH, IMH, CovRWMH, M-GaA, CIMH, and ACIMH algorithms. SMALA used
Matlab’s ode15s solver for the geometric tensor derived in Appendix E. As dde23 and
ode15s are quite time consuming, generating good proposals is essential for eﬃcient
sampling from the highly dependent seven dimensional parameter distribution.
121

6. IMPROVING THE METROPOLIS-HASTINGS ALGORITHM
USING COPULAS
We started the inference by choosing the independence proposal density q3 to be uniform
on [0, 50]7 and setting r1 = 0.7 and r2 = 0.25. The outcome of a simulated annealing
run was taken as starting value for RWMH, making a correction for a burn-in phase
obsolete. A look at the copula data from Figure 6.11(a) reveals that ﬁtting standard
pair copulas to the data – at least on the unconditioned level – is rather involved: the
density plot of the (˘u2, ˘u7)-pair of the ﬁrst run (Figure 6.11(b)), for instance, has a non-
standard bent ridge shape with a very dense region at high ˘u7 and low ˘u2 values. The
ﬁtting issue results in rather low acceptance rates for the copula based algorithms (Table
6.5). Nevertheless, both copula algorithms had again comparatively better INEFF’s and
generated far more independent samples than RWMH, IMH, CovRWMH, M-GaA, or
SMALA (Figure 6.12(a) and 6.12(b) and Table 6.5). Except for the M-GaA algorithm,
ACIMH outperformed all non-copula based sampling schemes more than 2.5-fold with
respect to (I2). The prerun samples were transformed to [0, 1]7 using ﬁtted normal
densities for the margins of k1, k2, k3, k4, τ, and k5 and a ﬁtted lognormal density for
the margin of k6. Owing to the complexity of the system, we used 3,000 samples to
ﬁt all copulas involved. By sequential adjustment of the proposal function during the
sampling process ACIMH could increase (I1) and (I2) compared to CIMH. The average
number of pair copula family updates in every ACIMH run was 48%, i.e.
almost
every second pair copula was ﬁtted to have diﬀerent copula types compared to the
ﬁt before.
The order of the variables ι was chosen to best capture strong pairwise
dependencies in the samples. More precisely we set ι(1) = 3, ι(2) = 1, ι(3) = 2, ι(4) =
4, ι(5) = 5, ι(6) = 6, ι(7) = 7. SMALA had in average rather high INEFF’s and was
even more time consuming than the copula based samplers, i.e. the time needed for
the computation of the geometric tensor could not be compensated by vast traversals
through the parameter space. On the other hand, the second order moments based
CovRWMH and M-GaA algorithms yielded already good sampling performances with
respect to (I2) as Figure 6.12(b) shows. Clearly, since the covariance matrix ˆC is based
on a total of 3,000 prerun samples it covers the second order moment of the posterior
in average better than the initial covariance matrices of the M-GaA proposal function.
CovRWMH therefore outperforms M-GaA with respect to (I1). Conversely, the prerun-
time needed to tune ˆC slows down CovRWMH and gives M-GaA an advantage with
respect to (I2).
As mentioned earlier the actual acceptance rate of M-GaA drops
to 14.86% ± 4.79% (Table 6.5) and failed to meet the predeﬁned acceptance rate of
122

6.3 Performance of CIMH and ACIMH
 
 
0.51
˘u7
0.5 1
0.51
0.51
0.5 1
0.5 1
0.5 1
0.5
1
0.89
˘u6
0.5
1
0.17
˘u5
0.20
0.5
1
0.05
˘u4
0.04
−0.43
0.5
1
0.05
˘u3
0.04
−0.43
0.97
0.5
1
−0.92 ˘u2
−0.87
−0.13
−0.04
−0.04
0.5
1
−0.04
˘u7
˘u1
−0.10
˘u6
−0.42
˘u5
0.38
˘u4
0.38
˘u3
0.05
˘u2
0.5
1 ˘u1
(a)
 
 
˘u7
0.2
0.5
0.8
0.2
0.5
0.8
0.2
0.5
0.8
τe = −0.92
˘u7
˘u2
˘u2
0.2
0.5
0.8
(b)
Figure 6.11: Copula data (c.f. Section 6.1.1 (ii)) of the ﬁrst run used to ﬁt the CIMH
and initial ACIMH copula on 3,000 of the 50,000 MCMC iterations. For uniformization
normal distributions were chosen for k1, k2, k3, k4, τ, and k5 and a lognormal distribution
for k6. The diagonal displays the histograms of the sample marginals and the numbers in
the upper right triangle the estimated Kendall’s τ’s. (b) Density plot of the (˘u2, ˘u7) copula
data pair corresponding to (k2, k6) of the ﬁrst run. Red areas depict higher, blue areas
lower density values.
α0 = 23%. It is nevertheless higher than the ones of IMH, SMALA, CIMH, or ACIMH.
In summary, although CIMH and ACIMH somewhat struggle to cover the complex
posterior distribution, which is indicated by the low (I1) index (Figure 6.12(a)), they
nevertheless yield a superior sampling performance with respect to (I2) compared to
all other algorithms considered (Figure 6.12(b)).
Our Bayesian MCMC approach revealed a strong indeterminacy with respect to the
parameters of the system.
There e.g. exist strong dependencies between k2 and k6
and k5 and k6 (compare pairs (˘u2, ˘u7) and (˘u6, ˘u7) of Figure 6.11(a)). Table 6.4 shows
the marginal posterior means, modes (MAP estimates), and 90% posterior quantile
based credible intervals for the concatenated data of 10 thinned runs for the respective
algorithms RWMH, IMH, CovRWMH, CIMH, and ACIMH. The MAP estimates of the
time τ a STAT5 molecule remains in the nucleus is ≈4 minutes. This means that
the cytoplasmic release turns out to be a bit faster than the value of ≈6.4 minutes
123

6. IMPROVING THE METROPOLIS-HASTINGS ALGORITHM
USING COPULAS
Sampler
ACIMH
CIMH
SMALA
M-GaA
CovRWMH
IMH
RWMH
E[k1|y]
0.03
0.03
0.03
0.03
0.03
0.03
0.03
M[k1|y]
0.03
0.03
0.04
0.03
0.03
0.03
0.03
CI[k1|y]
(0.03;0.04)
(0.03;0.04)
(0.02; 0.05)
(0.03; 0.04)
(0.03;0.04)
(0.02;0.04)
(0.02;0.03)
E[k2|y]
2.46
3.11
2.37
1.03
2.41
2.81
2.56
M[k2|y]
1.31
2.14
2.36
0.86
2.16
2.02
2.15
CI[k2|y]
(1.11; 4.43)
(1.32; 6.12)
(2.33; 2.42)
(0.76; 1.34)
(1.38;3.57)
(1.76;4.10)
(1.52;3.73)
E[k3|y]
0.17
0.16
0.11
0.24
0.17
0.15
0.15
M[k3|y]
0.15
0.14
0.11
0.20
0.15
0.15
0.14
CI[k3|y]
(0.13;0.23)
(0.13;0.20)
(0.10; 0.11)
(0.16; 0.34)
(0.13;0.22)
(0.12;0.19)
(0.11;0.19)
E[k4|y]
0.17
0.16
0.11
0.24
0.17
0.15
0.15
M[k4|y]
0.15
0.14
0.11
0.21
0.15
0.13
0.14
CI[k4|y]
(0.13;0.23)
(0.13;0.20)
(0.10; 0.11)
(0.16; 0.34)
(0.12;0.22)
(0.12;0.18)
(0.11;0.18)
E[τ|y]
3.97
4.18
5.56
3.61
4.06
4.31
4.52
M[τ|y]
3.82
4.23
5.59
3.79
3.86
4.28
4.03
CI[τ|y]
(3.12; 4.86)
(3.43; 4.94)
(5.15; 6.08)
(2.83; 4.36)
(3.08;5.02)
(3.51;5.25)
(3.47;6.40)
E[k5|y]
35.90
36.06
34.11
35.93
36.10
35.44
36.08
M[k5|y]
35.63
36.02
33.82
35.29
36.57
35.43
36.78
CI[k5|y]
(33.90; 37.93)
(34.19; 37.97)
(33.32; 35.26)
(33.71; 38.27)
(34.24;37.93)
(32.42;38.36)
(34.08;37.68)
E[k6|y]
0.95
0.95
0.40
0.0.94
0.95
0.94
0.96
M[k6|y]
0.95
0.95
0.36
0.93
0.96
0.94
0.94
CI[k6|y]
(0.92;0.98)
(0.92;0.98)
(0.34; 0.46)
(0.91; 0.97)
(0.92;0.98)
(0.91;0.99)
(0.92;1.00)
Table 6.4: JAK2-STAT5 pathway model. Estimated marginal posterior means E[·|y], modes M[·|y] (MAP estimates), and 90%
posterior quantile based credible intervals CI[·|y] for the parameters k1, k2, k3, k4, τ, k5 and k6 for the concatenated data of 10 runs.
124

6.3 Performance of CIMH and ACIMH
(a)
(b)
Figure 6.12:
Results for the JAK2-STAT5 model. Figure (a): Quotient of acceptance
rate and INEFF. Figure (b): Number of independent samples drawn per second. Error
bars show the estimated standard errors based on 10 runs.
Sampler
i.i.d. samp.
INEFF
AR (%)
time (sec.)
ACIMH
105.9± 19.5
658.4± 128.8
8.38±
1.38
6635.7± 197.7
CIMH
93.0± 48.1
1859.1± 591.1
8.03±
1.17
6339.7± 195.8
SMALA
47.0± 26.9
3242.1± 968.4
13.92±
4.96
7175.3±2211.3
M-GaA
40.3± 19.1
2922.3±1023.4
14.86±
4.79
7041.9±2346.3
CovRWMH
35.7± 15.6
3573.4± 887.0
29.04±
3.40
5574.8±
34.8
IMH
10.9± 2.4
5366.2± 597.6
0.04±4·10−3
4105.8±
40.3
RWMH
7.6± 0.6
6406.9± 463.3
21.47±
0.60
2328.9±
26.8
Table 6.5:
JAK2-STAT5 pathway model.
Depicted are the average number of i.i.d.
samples per run (ESS), INEFF’s, acceptance rates (AR), and sampling times based on 10
runs including estimated standard errors. All samplers ran for 50,000 MCMC proposals.
computed by Swameye et al. [2003]. Nevertheless τ ≈4 minutes is contained in their
conﬁdence interval of (3.8, 6.9) minutes. All other results coincide well. Overall, the
system represents a very challenging example for MH sampling schemes and is thus a
good benchmark for performance evaluation.
6.3.5
Robustness with respect to the choice of the pair copula decom-
position and cdf’s for prerun sample transformation
Two very crucial factors for the performance of CIMH and ACIMH are the goodness-
of-ﬁt of (i) the pair copula decomposition and (ii) the cdf’s for the transformation
125

6. IMPROVING THE METROPOLIS-HASTINGS ALGORITHM
USING COPULAS
of the prerun samples. We already inferred the eﬀect of applying oversimpliﬁed cop-
ula decompositions: as the independence copula is deﬁned by C : [0, 1]n −→[0, 1],
(u1, . . . , un) 7→Qn
i=1 ui, the IMH is essentially a CIMH algorithm with an indepen-
dence copula based proposal function. The JAK2-STAT5 example showed that we can
run into serious problems to ﬁt an appropriate decomposition. More involved tech-
niques such as ﬁtting mixtures of pair copulas as well as non or semi parametric copula
density estimation and sample generation (Hu [2006]) are therefore needed in future
applications. As Kim et al. [2007] showed, the latter can in some cases considerably
improve the robustness against misspeciﬁcation of the marginal distribution types for
Gi(ξ|γi). To assess the misspeciﬁcation eﬀect, we resampled the example of Chapter
6.3.2 over 10 runs using ﬁtted lognormal distribution functions on the one hand and
ﬁtted normal distribution functions on the other hand. The amount of independent
samples per second dramatically dropped by a factor of 9 when applying lognormal dis-
tributions. The index (I1) even decreased by a factor of 45. The issue of modeling the
dependency structure therefore needs to be considered carefully. As mentioned above,
(I1) can be taken as index for the goodness-of-ﬁt of the copula proposal function. A
value close to 1 guarantees an eﬃcient sampling performance.
6.4
Conclusions on CIMH and ACIMH
In summary, we saw that the vine copula based sampling schemes CIMH and ACIMH
showed superior performance compared to RWMH, IMH, and CovRWMH in every
example. They outperformed the MCMC algorithms SMALA and M-GaA on complex
systems, such as the JAK2-STAT5 pathway. Especially in the ﬁrst three examples the
copula based approaches covered the dependency structure of the posterior very well,
which resulted in high sampling eﬃciencies. Avoiding computationally costly copula
updates CIMH is doing best on these simple systems. However, in the complex JAK2-
STAT5 case these updates could improve the performance of ACIMH by ﬁne-tuning
the transition function. The copula data of the JAK2-STAT5 posterior indicated that
non-standard distributions for marginalization might improve the sampling eﬃciency
even more; a topic that should be addressed in further research. Nevertheless, CIMH
and ACIMH are yet promising concepts for the inference of dynamical systems.
126

7
Model inference of the
JAK1-STAT3 pathway
We already used a mathematical model of the JAK2-STAT5 pathway in Chapter 6.3.4 to
show that ACIMH is capable of eﬃciently inferring complex dynamical systems of this
type. In the current chapter we focus on the question, whether tyrosine-phosphorylated
STAT3 dimers can directly work as transcription factors in the JAK1-STAT3 pathway1?
From a biological point of view this is of relevance since IL-6 stimulation controls the
spreading of microbes in inﬂamed cells and endorses cell regeneration after injury as
could be shown in mouse hepatocytes (Bonizzi & Karin [2004]).
Similarly to the JAK2-STAT5 case, STAT3 is activated, i.e. phosphorylated, by JAK1
upon association with the IL-6 stimulated glycoprotein 130 (gp130) transmembrane
receptor.
To reach full transcription factor activity the STAT3 dimer can also be
serine phosphorylated (see Wen et al. [1995]). The latter is however not necessary for
the STAT3 dimer to work as transcription factor in the nucleus. In the following we
therefore infer the eﬀect of tyrosine-phosphorylated STAT3 dimer transcription factors.
1For a thorough introduction to the JAK-STAT pathway see Chapter 2.4.2.
127

7. MODEL INFERENCE OF THE JAK1-STAT3 PATHWAY
7.1
Experimental JAK1-STAT3 data
Our data is based on primary mouse hepatocytes (liver cells) stimulated with 1nm
IL-6. Measured were the protein concentrations of total cytoplasmic STAT3, tyrosine-
phosphorylated gp130, tyrosine phosphorylated STAT3, as well as tyrosine-serine-phos-
phorylated STAT3 dimers on a 90 minute time scale using quantitative immunoblotting
as described in Schilling et al. [2005] and Bohl [2009]. The data was kindly provided
by Prof. Dr. U. Klingm¨uller from the Deutsches Krebsforschungszentrum (DKFZ) in
Heidelberg, Germany. Unfortunately, the strength of measurement errors is unknown.
According to Bohl [2009], phosphorylated gp130 can be directly identiﬁed with phos-
phorylated JAK1 proteins. All measurements were normalized using calibrator or nor-
malizer proteins and therefore contain arbitrary units.
7.2
Mathematical models for the JAK1-STAT3 pathway
For inference of the JAK1-STAT3 pathway we adapted the JAK2-STAT5 mass action
DDE model of Chapter 6.3.4 by including an additional serine phosphorylation step of
the STAT3 dimer. Applying the linear chain trick to convert the deﬁning DDE system
into an ODE system (Appendix E, Equation (E.3)), the model reads
dx1(t)
dt
= −k1x1(t)pgp130(t) + 2k5x7(t)
dx2(t)
dt
= −k2x2
2(t) + k1x1(t)pgp130(t)
dx3(t)
dt
= −k3x3(t) + 1
2k2x2
2(t)
dx4(t)
dt
= −k4x4(t) + k3x3(t)
dx5(t)
dt
= −k5x7(t) + k4x4(t)
dx6(t)
dt
= 2
τ (x4(t) −x6(t))
dx7(t)
dt
= 2
τ (x6(t) −x7(t)),
(7.1)
where x1(0) = 1 and x2(0) . . . = x7(0) = 0 in arbitrary units. The concentrations
x5(t), x6(t), x7(t) are auxiliary and represent the concentration of STAT3 in the nu-
128

7.2 Mathematical models for the JAK1-STAT3 pathway
cleus. However, with respect to inference they are of no further interest. pgp130(t)
denotes the time-dependent tyrosine-phosphorylated gp130 stimulation function and
τ the time lag between STAT3 entering the nucleus and dephosphorylated cytoplas-
mic release. Furthermore, x1(t), x2(t), x3(t), x4(t) are the concentrations of unphos-
phorylated, tyrosine-phosphorylated, tyrosine-phosphorylated dimerized, and tyrosine-
serine-phosphorylated dimerized STAT3, respectively. The law of mass conservation
again claims k2 ≥k3 and k4 ≥k5. A schematic representation of the model can be seen
in Figure 7.1(a). We are given the following observations:
yε
1(ti) = k6(x2(ti) + 2x3(ti) + ε1(ti)), the amount of tyrosine-phosphorylated STAT3
yε
2(ti) = k7(2x4(ti) + ε2(ti)), the amount of tyrosine-serine-phosphorylated STAT3
yε
3(ti) = k8(x2(ti) + 2x3(ti) + 2x4(ti) + ε3(ti)), the total cytoplasmic STAT3,
at 30 time points t1, . . . , t30 (in minutes) in the interval [0, 90]. Here, k6, k7 and k8
are introduced due to the relativity of the measurements.
It has to be noted that
k1 cannot be inferred without any further knowledge about the gp130 measurements
as it includes a relativity term of the normalized gp130 measurements. The quantities
εj(ti) denote normally distributed measurement errors included in the data. We assume
that all tyrosine-phosphorylated STAT3 molecules are converted into tyrosine-serine-
phosphorylated STAT3 (Bohl [2009]). Therefore, k7 can be approximated via k6 times
the estimated mean quotient r of yε
1(ti) and yε
2(ti) which eliminates one parameter.
The remaining eight parameters ξ = (k1, k2, k3, k4, k5, τ, k6, k8)⊤are time-independent.
Again, for readability we omitted the dependence of the solutions xi(t) to (7.1) on ξ.
Since the JAK2-STAT5 pathway is directly comparable to the JAK1-STAT3 pathway
(Aaronson & Horvath [2002]), we used the rates given in Timmer et al. [2004] as
prior information, i.e. we chose k1 ∼N0(0.021, (0.021
2 )2), k2 ∼Nk3(2.46, (2.46
2 )2), k4 ∼
Nk5(0.107, (0.107
2 )2), k5 ∼N0(0.107, (0.107
2 )2) and τ ∼N0(6.4, (6.4
2 )2), where Na(·, ·)
denotes the a left-truncated univariate normal distribution.
The rates k3, k6, and
k8 were chosen to be uniform on the interval [0, 1000]. Here, the lower limit 0 was
canonically introduced by the non-negativity constraint for reaction rates. For y :=
{yε
1(t1), . . . , yε
1(t30), yε
2(t1), . . . , yε
2(t30), yε
3(t1), . . . , yε
3(t30)} and the prior π(ξ) this leads
to the posterior distribution
π(ξ|y) ∝
30
Y
i=1
Φ(yε
1(ti)|y1(ti), σ2
1) · Φ(yε
2(ti)|y2(ti), σ2
2) · Φ(yε
3(ti)|y3(ti), σ2
3) · π(ξ),
129

7. MODEL INFERENCE OF THE JAK1-STAT3 PATHWAY
IL-6
target gene
x1
x2
x4
JAK1
P
P
P
P
P
P
P
cell membrane
nucleus
cytoplasm
P
P
P
P
STAT3
P
k1
k3
k4
k5
STAT3
STAT3
STAT3
STAT3
gp130
x3
P
k2
STAT3
STAT3
P
P
P
P
STAT3
P
STAT3
(a)
IL-6
target gene
x1
x2
x4
JAK1
P
P
P
P
P
P
P
cell membrane
nucleus
cytoplasm
P
P
P
P
STAT3
P
k1
k3
k4
k5
STAT3
STAT3
STAT3
STAT3
gp130
x3
P
k2
STAT3
STAT3
P
P
P
P
STAT3
P
STAT3
k9
(b)
Figure 7.1: (a) Schematic representation of the ﬁrst JAK1-STAT3 pathway model (7.1):
Interleukin 6 (IL-6) binds to the gp130 transmembrane receptor. Monomeric STAT3 (x1)
is tyrosine phosphorylated (x2) by the activated JAK1/gp130 complex in the cytoplasm.
After dimerizing the tyrosine-phosphorylated STAT3 dimer (x3) serine phosphorylates (x4),
gets transported to the nucleus and binds to the promoter target gene region. Subsequently
it is dephosphorylated and released back into the cytoplasm. (b) Graphical representation
of the alternative JAK1-STAT3 pathway model (7.2): The model contains an additional
transfer of tyrosine-phosphorylated STAT3 dimers (x3) into the nucleus.
where y1(t) = k6(x2(t) + 2x3(t)), y2(t) = k6r(2x4(t)) and y3(t) = k8(x1(t) + x2(t) +
2x3(t) + 2x4(t)) for the solutions x1(t), x2(t), x3(t), and x4(t) of (7.1). The standard
deviations σ1, σ2, and σ3 of the measurement errors were inferred by simulated anneal-
ing (Chapter 4.6) before starting posterior inference. Like in the JAK2-STAT5 pathway
we assume normally distributed measurement/model errors for the likelihood. Again,
we performed a Kolmogorov-Smirnov test on normality to ensure that the combined
measurement/model error can not possibly be non-Gaussian. The null-hypothesis of
non-Gaussian noise could not be rejected on an α = 5% signiﬁcance level for tyrosine-
phosphorylated STAT3, tyrosine-serine-phosphorylated STAT3, or the total cytoplas-
mic STAT3. The test was based on the corresponding time course of the MLE of model
(7.1). The MLE was obtained by simulated annealing.
As mentioned above it is biologically unclear whether the tyrosine-phosphorylated
STAT3 dimer has a strong potential to directly work as transcription factor. We there-
fore compared our ﬁrst model (7.1) to a model including an additional direct x3(t)
130

7.3 Inference of the JAK1-STAT3 model
nucleus import model: once again applying the linear chain trick we have
dx1(t)
dt
= −k1x1(t)pgp130(t) + 2k5x8(t) + 2k5x10(t)
dx2(t)
dt
= −k2x2
2(t) + k1x1(t)pgp130(t)
dx3(t)
dt
= −k3x3(t) −k9x3(t) + 1
2k2x2
2(t)
dx4(t)
dt
= −k4x4(t) + k3x3(t)
dx5(t)
dt
= −k5x8(t) + k4x4(t)
dx6(t)
dt
= −k5x10(t) + k9x3(t)
dx7(t)
dt
= 2
τ (x5(t) −x7(t))
dx8(t)
dt
= 2
τ (x7(t) −x8(t))
dx9(t)
dt
= 2
τ ′ (x6(t) −x9(t))
dx10(t)
dt
= 2
τ ′ (x9(t) −x10(t)),
(7.2)
with x1(0) = 1 and x2(0) . . . = x10(0) = 0 in arbitrary units and time delays τ, τ ′
for the nuclear time spent by the tyrosine-serine-phosphorylated STAT3 dimer and the
tyrosine-phosphorylated STAT3 dimer, respectively. Here, x5(t), . . . , x10(t) are auxil-
iary. We claim k2 ≥k3, and k4 + k9 ≥k5. The posterior distribution remains almost
unchanged, except that the deﬁning parameter vector of the solutions xi(t) of (7.2) is
given by ξ = (k1, k2, k3, k4, k5, τ, τ ′, k6, k8, k9)⊤and the prior distribution of model one
is extended by τ ′ ∼N0(6.4, (6.4
2 )2) and k9 ∼Nk5−k4(0.107, (0.107
2 )2) for the additional
parameter variables. Due to a diﬀerent STAT3 dimer transcription activity (Wen et al.
[1995]), we allow τ ̸= τ ′. All rates shared by both models are governed by the very
same prior distributions. This means the choice of priors does not inﬂuence the model
inference process. A schematic representation of (7.2) is depicted in Figure 7.1(b).
7.3
Inference of the JAK1-STAT3 model
We now want to analyze the eﬀect of direct tyrosine-phosphorylated STAT3 dimer im-
port into the nucleus, i.e. we compare the models (7.1) and (7.2) using thermodynamic
131

7. MODEL INFERENCE OF THE JAK1-STAT3 PATHWAY
(a)
(b)
(c)
(d)
Figure 7.2: JAK1-STAT3 data. (a) pgp130 data (green dots) and according ﬁtted pgp130
stimulating function (solid green line).
(b) Time courses for the numerical solution of
tyrosine-phosphorylated STAT3 in the cytoplasm (y1(t)). Depicted are the posterior me-
dian solution (dashed blue line) as well as the according 95% credible interval (shaded
area) of the thinned T30 = 1 ACIMH run for model (7.1). Blues dots represent given mea-
surements yε
1(ti). (c) Similarly to (b), the measurements (red dots), the posterior median
solution (dashed red line), and the 95% credible interval (shaded area) for the numerical
solution of y2(t), i.e. tyrosine-serine-phosphorylated STAT3 in the cytoplasm. (d) Similarly
to (b), the measurements (black dots), the posterior median solution (dashed black line),
and the 95% credible interval (shaded area) for the numerical solution of y3(t), i.e. total
STAT3 in the cytoplasm.
132

7.3 Inference of the JAK1-STAT3 model
k8
3.5
4
30 45
1 2.5
0.15 0.22
0.15 0.22
0.7 1.3
1.5
3.5
0.01
0.03
3.5
4
0.10
k6
30
45
0.24
τ
0.26
1
2.5
−0.02
k5
0.27
−0.06
0.15
0.22
0.02
k4
0.30
−0.05
0.91
0.15
0.22
0.05
k3
0.29
0.04
0.30
0.30
0.7
1.3
0.03
k2
0.24
0.03
0.28
0.29
−0.20
1.5
3.5
−0.04
k8
k1
−0.63
k6
−0.21
τ
−0.05
k5
−0.08
k4
−0.18
k3
−0.09
k2
k 1
Figure 7.3: JAK1-STAT3 pathway model. Pairwise density plots for all parameter-pairs
of the posterior. Red areas depict higher, blue areas lower density values. The diagonal
displays the histograms of the sample marginals and the numbers in the upper right triangle
the estimated Kendall’s τ’s.
integration (Chapter 3.4.3). For inference we chose the independence proposal density
q3 of ACIMH to be uniform on [0, 1000]8 for model (7.1) and uniform on [0, 1000]10 for
model (7.2). Furthermore, r1 = 0.7 and r2 = 0.29, i.e. the copula sampling scheme q1
was used for ca. 70%, the CovRWMH sampling scheme q2 for ca. 29%, and q3 for ca.
1% of the proposals. Following Calderhead & Girolami [2009] the thermodynamic in-
tegration schedule Ti = (i/30)5, i = 0, ..., 30, was applied (see Equation (3.22)). Based
on 1,000,000 prerun samples for each Ti we computed the Bayes factor of model (7.2)
133

7. MODEL INFERENCE OF THE JAK1-STAT3 PATHWAY
versus model (7.1) using 1,000,000 ACIMH proposals with copula update parameters
of R =100,000 and S =4 (see Equation (6.6)) in each Ti. The model prior probabil-
ity of each model was set to 1
2. All copulas were ﬁtted on a total of 3,000 samples.
Throughout the permutation function ι was chosen to be the identity function. For
uniformization we applied ﬁtted normal distributions. Each ACIMH run was started
at an arbitrarily drawn sample from the according prior distribution. We applied the
Geweke test (Chapter 4.4) to correct for a burn-in phase in every chain. The resulting
Bayes factor computed to
B1,2 = 1.5 · 103
(7.3)
in favor of model (7.1). Including one standard error, this result is based on 453 ± 59
and 922 ± 149 eﬀective samples for each Ti at average acceptance rates 9.11 ± 0.87%
and 8.10 ± 0.53% for model (7.1) and model (7.2), respectively. The solutions to (7.1)
and (7.2) were derived numerically applying the transformation of Appendix D as done
for the JAK2-STAT5 system. The time course for pgp130 was ﬁtted prior to inference
using a rescaled lognormal density function (Figure 7.2(a)).
According to (7.3) the ﬁrst model is favored decisively on Jeﬀreys’ scale of evidence.
The eﬀect of direct tyrosine-phosphorylated STAT3 dimer import into the nucleus is
thus negligible. Henceforth we focus on model (7.1) for further inference. Model (7.1) is
identiﬁable as it diﬀers by a second linear transformation step compared to the JAK2-
STAT5 model. The (posterior) time courses are depicted in Figure 7.2. Despite the
error prone total STAT data (Figure 7.2(d)) the model can cover the phosphorylated
STAT measurements well (Figure 7.2(b) and 7.2(c)). Due to the restriction of k4 ≥k5
there is a strong correlation between theses two parameters (Figure 7.3). Based on k2 ≥
k3 higher order dependencies arise as can be seen in the pairwise density plot. Especially
k8 seems to inherit many nonlinear dependencies (last row of Figure 7.3). In contrast to
the JAK2-STAT5 pathway the nuclear abidance time decreases drastically (Table 7.1).
The MAP estimate for τ computed to 0.252 minutes = 15.12 seconds, while the upper
bound of the 95% credible interval is 2 minutes. This is unnaturally short. Here, the
error prone total STAT data might eﬀect the result. The estimated marginal posterior
means, modes (MAP estimates), as well as the 90% posterior quantile based credible
intervals are given in Table 7.1. An identiﬁability analysis as introduced in Chapter 3.3
134

7.3 Inference of the JAK1-STAT3 model
showed that all parameters are identiﬁable. Note that k1 contains a relativity constant
as mentioned above and is therefore not meaningful as such.
E[k1|y]
0.024
E[k5|y]
0.176
M[k1|y]
0.023
M[k5|y]
0.176
CI[k1|y]
(0.014; 0.035)
CI[k5|y]
(0.131; 0.226)
E[k2|y]
1.869
E[τ|y]
0.625
M[k2|y]
1.436
M[τ|y]
0.252
CI[k2|y]
(1.014; 3.432)
CI[τ|y]
(0.017; 2.082)
E[k3|y]
0.871
E[k6|y]
28.656
M[k3|y]
0.871
M[k6|y]
26.305
CI[k3|y]
(0.356; 1.596)
CI[k6|y]
(22.900; 38.625)
E[k4|y]
0.182
E[k8|y]
3.674
M[k4|y]
0.178
M[k8|y]
3.657
CI[k4|y]
(0.136;0.235)
CI[k8|y]
(3.394; 3.964)
Table 7.1: JAK1-STAT3 pathway model. Estimated marginal posterior means E[·|y],
modes M[·|y] (MAP estimates), and 90% posterior quantile based credible intervals CI[·|y]
for the parameters k1, k2, k3, k4, k5, τ, k6 and k8. The according units are [min−1] for all
ki’s and [min] for τ.
135

7. MODEL INFERENCE OF THE JAK1-STAT3 PATHWAY
136

8
Inference of biokinetic models for
zirconium processing in humans
Radioactive zirconium (Zr) isotopes are produced in large quantities in nuclear ﬁssion
reactors; one of the most common fragments in uranium ﬁssion is Zirconium-95 (95Zr).
For example, the estimated released 95Zr activity of the Fukushima and Chernobyl ac-
cidents is considered to have detrimental health eﬀects not only via irradiation, but also
via the intake of edibles (Eidgen¨ossisches Nuklearsicherheitsinspektorat Informations-
dienst [2011]; UNSCEAR [2008]). The estimation of radiation doses is indispensable
for risk analysis for humans exposed to radioactive substances (ICRP [1979, 1988]).
They provide limiting values of detrimental eﬀects and build the basis for applications
in internal dosimetry (ICRP [2007]), the prediction for radioactive zirconium retention
in various organs (ICRP [1998]) as well as retrospective dosimetry, i.e. the estimation
of ingested amounts of zirconium for ex post measurements. This is crucial for occupa-
tional exposure (ICRP [1979]), and for patients undergoing diagnostic and therapeutic
nuclear medicine (ICRP [1988]).
In order to calculate the radiation dose and quantify the deposition of radioactivity
from the incorporated radionuclide inside the human body, the International Commis-
sion on Radiological Protection (ICRP) in ICRP [1989] recommends a compartment
model. It incorporates basic processes in the human physiological system (Guyton &
Hall [2006]; ICRP [1975, 1979, 1989]). All major organs and tissues are simpliﬁed in
the model structure as separate compartments that represent kinetically homogeneous
137

8. INFERENCE OF BIOKINETIC MODELS FOR ZIRCONIUM
PROCESSING IN HUMANS
amounts of radionuclides; the connections between organs and tissues are described
via transfer rates, i.e. model parameters that represent the exchange rates between
the individual mutually exclusive compartments. These multi-compartmental systems
along with their transfer parameters describing the kinetic behavior of radionuclides in
the human body are called biokinetic models (ICRP [1989]). Throughout we use the
terms biokinetic model and compartment model interchangeably. The transfer of sub-
stances into and out of compartments is governed by the law of mass balance. Transfer
parameters are frequently evaluated on the basis of experimental data obtained from
laboratory animals and, to a lesser extent, human beings (ICRP [1975]). Although
animal data is not directly comparable to human data, the former can nevertheless
be very helpful as prior information. In order to obtain more reliable dose estimates
for humans, Greiter and coworkers developed a novel biokinetic model (Greiter et al.
[2011]). It is based on the processing of non-radioactive Zr isotopes in 16 investigations
with 12 healthy human subjects. In our case in vivo measurements were taken in urine
and plasma (see Chapter 8.1). Although a global statistical analysis of the HMGU
model was provided in Li et al. [2011a,b], a thorough comparison of the ICRP and
HMGU model by a model selection approach was yet missing.
Applying thermodynamic integration in combination with CIMH we compared the
HMGU and ICRP models based on in vivo plasma and urine data of the 16 investiga-
tions. More precisely, the models were evaluated for each investigation individually and
for the concatenated data of all investigations. The latter allowed to infer transfer rates
(including credible intervals) for an average subject. We also provide an analysis based
on the (i) plasma data and (ii) urine data individually. Furthermore, the diﬀerence in
accretion of zirconium in bones is inferred. The Bayesian framework also yields credible
bounds for retrospective dose assessment of an average subject, this is, based on the
concatenated data of all 16 investigations. We provide a simple to use estimation table
for the prediction of initially ingested zirconium mass for ex post measurements. This
impacts the estimation of intake and radiation dose, especially the bone dose, when
aiming for personalized occupational monitoring data.
138

8.1 Experimental zirconium data
8.1
Experimental zirconium data
The human biokinetic data was measured in a stable tracer study executed at the
Helmholtz Zentrum M¨unchen (HMGU) (Greiter et al. [2011]). It includes 16 investi-
gations with ingestion of a investigation-speciﬁc amount of isotopically enriched stable
zirconium. The administered amount was based on the individuals weight, aiming at
a dose of 0.09mg stable tracer per kg body weight. Tracer concentrations were deter-
mined in blood plasma and urine. For the plasma data, samples were taken several
times during the ﬁrst day in increasing intervals, and more scarcely later on. Urine
was collected completely in 12-24h periods on several days.
The last samples were
taken at 100d after tracer administration. Tracer concentrations were normalized to
the respective tracer amount ingested in each investigation, such that the total ingested
amount corresponds 100% at t = 0 in the stomach. Concentrations in blood plasma
were expressed as % per kg plasma. The plasma concentrations were scaled by the total
amount of plasma in the body to get absolute concentrations (Alberts et al. [2002]).
Urine data was expressed as excretion rate in % per day.
8.2
Mathematical models for zirconium processing
The currently used compartmental model was recommended by the ICRP in ICRP
[1975, 1989, 1993] (Figure 8.1(a)). It consists of eleven compartments and 15 transfer
rates. Zirconium enters the body via the stomach compartment x9 and is processed
until it reaches any of the two ﬁnal compartments urine, x7, or feces, x8. The transfer
compartment was taken to be identical with blood plasma. Some of the transfer rates
and compartments of the ICRP model are however physiologically questionable: The
direct mass transport from the two bone compartments to the urinary bladder contents
and upper large intestine compartments or the distinction between trabecular bone
surface and cortical bone surface as such.
In order to address these shortcomings
Greiter et al. [2011] recently proposed an alternative HMGU model combining the two
bone compartments into one single compartment and replacing several mass ﬂows by
physiologically more plausible transfer rates (Figure 8.1(b)). Altogether both models
139

8. INFERENCE OF BIOKINETIC MODELS FOR ZIRCONIUM
PROCESSING IN HUMANS
Small 
Intestine
x10
Stomach
x9
Urinary 
Bladder 
Contents
x4
Urine
x7
Feces
x8
Transfer
x1
C Bone S
x2
Lower Large 
Intestine
x6
Upper Large 
Intestine
x5
k16
k13
k3
k8
k1
k14
k7
k15
k6
k5
k4
k2
 T Bone S
x11
k17
k18
k19
Other 
Tissue
x3
(a)
Small 
Intestine
x10
Stomach
x9
Urinary 
Bladder 
Contents
x4
Urine
x7
Feces
x8
Bone
x2
Transfer
x1
Other 
Tissue
x3
Lower Large 
Intestine
x6
Upper Large 
Intestine
x5
k9
k3
k8
k10
k7
k11
k1
k6
k5
k4
k2
k12
(b)
Figure 8.1: (a) Schematic representation of the ICRP model. The model consists of eleven
compartments x1,. . . ,x11 and 15 time independent transfer rates k1,. . . ,k8,k13,. . . ,k19. (b)
Schematic representation of the HMGU model. The model consists of ten compartments
x1,. . . ,x10 and twelve transfer rates k1,...,k12. In both models zirconium enters the body
in the stomach compartment x9 and diﬀuses through the system until it reaches either one
of the two ﬁnal compartments urine, x7, or feces, x8. The gray compartments x1 and x7
are directly related to the datasets measured.
share eight transfer rates, which we denote by k1, . . . , k8. Transfers present in just one
of the models have a unique index to facilitate distinction.
The dynamics of both models are described by a system of coupled linear ﬁrst-order
ordinary diﬀerential equations (ODEs). The ICRP model reads
140

8.2 Mathematical models for zirconium processing
dx1(t)
dt
= (−k1 −k2 −k13) x1(t) + k7x10(t)
dx2(t)
dt
= k1x1(t) + (−k14 −k15) x2(t)
dx3(t)
dt
= k2x1(t) + (−k16 −k17) x3(t)
dx4(t)
dt
= k14x2(t) + k16x3(t) −k3x4(t) + k18x11(t)
dx5(t)
dt
= k15x2(t) + k17x3(t) −k4x5(t) + k8x10(t) + k19x11(t)
dx6(t)
dt
= k4x5(t) −k5x6(t)
dx7(t)
dt
= k3x4(t)
dx8(t)
dt
= k5x6(t)
dx9(t)
dt
= −k6x9(t)
dx10(t)
dt
= k6x9(t) + (−k7 −k8) x10(t)
dx11(t)
dt
= k13x1(t) + (−k18 −k19) x11(t).
(8.1)
The HMGU model on the other hand is deﬁned by
dx1(t)
dt
= (−k1 −k2 −k9 −k10) x1(t) + k11x2(t) + k12x3(t) + k7x10(t)
dx2(t)
dt
= k1x1(t) −k11x2(t)
dx3(t)
dt
= k2x1(t) −k12x3(t)
dx4(t)
dt
= k9x1(t) −k3x4(t)
dx5(t)
dt
= k10x1(t) −k4x5(t) + k8x10(t)
dx6(t)
dt
= k4x5(t) −k5x6(t)
dx7(t)
dt
= k3x4(t)
dx8(t)
dt
= k5x6(t)
dx9(t)
dt
= −k6x9(t)
dx10(t)
dt
= k6x9(t) + (−k7 −k8) x10(t).
(8.2)
In both models x9(0) = 100% and therefore xj̸=9(0) = 0% at time point t = 0, this is,
141

8. INFERENCE OF BIOKINETIC MODELS FOR ZIRCONIUM
PROCESSING IN HUMANS
the complete amount of zirconium is initially contained in the stomach compartment.
For each investigation i we assume that the data
yi = {xi,1
1 , xi,2
1 , . . . , x
i,np
i
1
, ˙xi,1
7 , ˙xi,2
7 , . . . , ˙xi,nu
i
7
}
follows the solution xξm(t) of the diﬀerential equation given in (8.1) and (8.2) for any
of the two models Mm and some corresponding parameter vector ξm. The model index
m ∈{H, I}, where MI is the ICRP model and MH the HMGU model. Corresponding
to the notation in Figure 8.1(a) and 8.1(b), ξI = (k1, . . . , k8, k13, . . . , k19) and ξH =
(k1, . . . , k12). While for investigation i, xi,·
1 indicate measurements in plasma, i.e. in the
transfer compartment x1, ˙xi,·
7 designate measurements of the excretion rate in the urine
compartment x7. The expressions np
i denote the number of measurements in plasma
and nu
i the number of measurements in urine for investigation i. Assuming furthermore
that all data points contain normally distributed measurement errors, the investigation
i and model Mm speciﬁc likelihood function has the form
Li(ξm|yi, m) =
np
i
Y
α=1
Φ

xi,α
1 |xp
ξm(tα), σp
i

|
{z
}
Lp
i (ξm|yi,m)
nu
i
Y
β=1
Φ

˙xi,β
7 | d
dtxu
ξm(tβ), σu
i

|
{z
}
Lu
i (ξm|yi,m)
,
where xp
ξm(tα) denotes the solution for the transfer compartment x1 at time point tα of
the according ODE system, corresponding to the measurement at xi,α
1 , for the parame-
ter vector ξm. Furthermore, d
dtxu
ξm(tβ) is the derivative of the solution for the urine com-
partment x7 at time point tβ, corresponding to the measurement ˙xi,β
7 , while Φ(·|µ, σ)
is the univariate probability density function of the normal distribution with mean µ
and standard deviation σ. In order to take into account the biological variability, the
combined model/measurement errors for plasma, σp
i , and for urine, σu
i , are ﬁtted inves-
tigation speciﬁcally by simulated annealing (Chapter 4.6) before starting the MCMC
sampling process. We tested all 16 investigations for non-Gaussian measurement/model
error using individual MLE based time courses and applying the Kolmogorov-Smirnov
test on normality. The null-hypothesis of non-Gaussian noise could not be rejected on
an α = 5% signiﬁcance level for any investigation. The complete data (i.e. concatenated
data y = {by1, . . . , y16}) likelihood is given by LALL(ξm|yi, m) = Q16
i=1 Li(ξm|yi, m)
where in all Li(ξm|yi, m) the same ﬁtted investigation independent σp
i = σp and
142

8.3 Prior information for zirconium processing and
algorithmic set up
σu
i = σu are used. For the calculation of the likelihood L·(ξm|y·, m) we solved the
according ODE system semi-analytically: Writing the ODE system as
dxξm(t)
dt
= A(ξm) · xξm(t),
where xξm(t) is the vector of all the compartments of model Mm and the time in-
dependent matrix A(ξm) represents all the interactions between these compartments,
depending on the transfer rate values ξm, the corresponding analytical solution is given
by
xξm(t) = eA(ξm)t · xξm(t = 0).
We computed the matrix exponential eA(ξm)t by eigenvalue decomposition, i.e.
eA(ξm)t = U(ξm)





ed1(ξm)t
0
· · ·
0
0
ed2(ξm)t
0
...
...
...
0
0
· · ·
edV (ξm)t




U(ξm)−1
for the eigenvalues d1(ξm), d2(ξm), . . . , dV (ξm) of A(ξm) and some orthonormal matrix
U(ξm). The eigenvalues were estimated using MATLAB’s eig function. As we have
no initial preference for any of the models we chose a uniform model prior. The model
speciﬁc, investigation independent prior distributions π(ξm|m) are based on combined
human/animal data as speciﬁed in the following section.
8.3
Prior information for zirconium processing and
algorithmic set up
Since the problem of radiation protection is of great relevance, quite a large number
of animal studies have been conducted. These yield excessive prior knowledge for a
Bayesian modeling approach. As the ICRP model is the recommended model used for
dose estimation, there exists information on the distribution types of the parameters
involved in the model along with conﬁdence intervals (Li et al. [2011a], and Table 8.1).
The prior informations are based on a large number of studies and well-established
over the years. Even for the HMGU model, detailed prior information is available from
previous studies (Li et al. [2011a,b], and Table 8.1). Here, the prior informations are
in part directly derived from ICRP recommendations, plus information gained from
143

8. INFERENCE OF BIOKINETIC MODELS FOR ZIRCONIUM
PROCESSING IN HUMANS
additional experiments based on injected zirconium doses (Li et al. [2011a]). As we
do not use the injection data for our analysis, the HMGU prior informations are not
biasing the sampling outcome.
It is noteworthy that of the eight parameters shared in both models, k8 is the only
one having diﬀerent distributions in the ICRP and HMGU model. Due to a lack of
knowledge of the dependency structure between the parameters, the multivariate prior
distribution π(ξm|m) of model Mm was taken to be the product of the univariate prior
distributions π(ξm|m) for each parameter km
r , i.e. π(ξm|m) = Q
r π(km
r |m). Each uni-
variate prior distribution was truncated at zero. While Bayes factors were computed
for each investigation separately (see Chapter 8.4.3), the same prior information was
applied throughout all investigations. This is reasonable, as the priors contain infor-
mation from a huge variety of diﬀerent preceding experiments.
We again used thermodynamic integration (see Chapter 3.4.3) in combination with the
CIMH algorithm in order to compare the ICRP model with the HMGU model based on
Bayes factors. For all our applications the thermodynamic integration schedule Ti =
(i/29)5, i = 0, ..., 29, was applied (see Equation (3.22)). The independence proposal
function q3 of CIMH was chosen to be the product of the according prior distributions.
Furthermore, r1 = 0.89 and r2 = 0.1, i.e. the copula sampling scheme q1 was used for ca.
89%, the CovRWMH sampling scheme q2 for ca. 10%, and q3 for ca. 1% of the proposals.
Throughout the permutation function ι was chosen to be the identity function. Fitting
copula distributions was done in preruns containing 1,000,000 unthinned samples each.
They were generated for each investigation and model separately. For uniformization of
the prerun samples, we naturally applied the according prior distributions of the models
at hand. Before starting the MCMC sampling procedure, the maximum a posteriori
parameter estimates were computed by simulated annealing and used as initial MCMC
sampling values. This makes a burn-in period dispensable. Finally, all Bayes factors
were computed based on 30,000 proposals of the CIMH algorithm at each Ti throughout
all applications.
144

8.3 Prior information for zirconium processing and
algorithmic set up
ICRP model
Par.
Compartments
Med. (d−1)
99.7% CI
Distribution
µ/a
σ/c
b
k1
TC →CBS
0.69
[0.086, 5.52]
LN(µ, σ)
-0.3711
0.6931
k2
TC →Other
1.39
[0.174, 11.12]
LN(µ, σ)
0.3293
0.6931
k3
UBC →Urine
12
T(a, b, c)
6
8
24
k4
UpLI →LoLI
1.8
T(a, b, c)
0.9
1.2
3.6
k5
LoLI →Feces
1
T(a, b, c)
0.3
1
1.7
k6
Stomach →SI
24
T(a, b, c)
12
16
48
k7
SI →TC
0.06
[0.0075, 0.48]
LN(µ, σ)
-2.8134
0.6931
k8
SI →UpLI
6
T(a, b, c)
3
4
12
k13
TC →TBS
0.69
[0.086, 5.52]
LN(µ, σ)
-0.3711
0.6931
k14
CBS →UBC
5.8 · 10−5
[5.8 · 10−6, 1.1 · 10−4]
N(µ, σ)
5.8 · 10−5
1.7 · 10−5
k15
CBS →UpLI
1.2 · 10−5
[1.2 · 10−6, 2.2 · 10−5]
N(µ, σ)
1.2 · 10−5
3.5 · 10−6
k16
Other →UBC
0.083
[0.0083, 0.158]
N(µ, σ)
0.083
0.025
k17
Other →UpLI
0.0165
[0.00165, 0.0314]
N(µ, σ)
0.0165
0.00495
k18
TBS →UBC
5.8 · 10−5
[5.8 · 10−6, 1.1 · 10−4]
N(µ, σ)
5.8 · 10−5
1.7 · 10−5
k19
TBS →UpLI
1.2 · 10−5
[1.2 · 10−6, 2.2 · 10−5]
N(µ, σ)
1.2 · 10−5
3.5 · 10−6
HMGU model
Par.
Compartments
Med. (d−1)
99.7% CI
Distribution
µ/a
σ/c
b
k1
TC →Bone
0.10
[0.013, 0.8]
LN(µ, σ)
-2.3026
0.6931
k2
TC →Other
1.35
[0.17, 10.8]
LN(µ, σ)
0.3001
0.6931
k3
UBC →Urine
12.0
T(a, b, c)
6.0
8.0
24.0
k4
UpLI →LoLI
1.8
T(a, b, c)
0.9
1.2
3.6
k5
LoLI →Feces
1.0
T(a, b, c)
0.3
1.0
1.7
k6
Stomach →SI
24.0
T(a, b, c)
12.0
16.0
48.0
k7
SI →TC
0.03
[1.1 · 10−3, 0.81]
LN(µ, σ)
-3.5066
1.0986
k8
SI →UpLI
17.21
[0.64, 464.67]
LN(µ, σ)
2.8455
1.0986
k9
TC →UBC
0.031
[0.0011, 0.8370]
LN(µ, σ)
-3.4738
1.0986
k10
TC →UpLI
0.0062
[0.0002, 0.1674]
LN(µ, σ)
-5.0832
1.0986
k11
Bone →TC
6.9 · 10−5
[8.7 · 10−6, 5.6 · 10−4]
LN(µ, σ)
-9.5769
0.6931
k12
Other →TC
0.53
[0.066, 4.24]
LN(µ, σ)
-0.6349
0.6931
Table 8.1: Overview of priors for the zirconium models. The tables are based on Li et al.
[2011a], where the conﬁdence intervals (CI), the medians (Med.) as well as the parameters
of the normal and lognormal distributions were calculated according to Appendix F. Ab-
breviations are: LN(µ, σ) for a lognormal distribution with location parameter µ and scale
parameter σ, T(a, b, c) for a triangular distribution with lower limit a, upper limit b, and
mode c, as well as N(µ, σ) for a normal distribution with mean µ and standard deviation σ.
Furthermore TC= Transfer compartment; CBS = Cortical Bone Surface; Other = Other
Tissues; UBC = Urinary Bladder Contents; UpLi = Upper Large Intestine; LoLI = Lower
Large Intestine; SI = Small Intestine; TBS = Trabecular Bone Surface.
145

8. INFERENCE OF BIOKINETIC MODELS FOR ZIRCONIUM
PROCESSING IN HUMANS
8.4
Inference of the zirconium models
We now present the results of our analysis, which primarily addresses the question
of model selection between the HMGU and ICRP models. First several general re-
sults, such as investigation dependency of the Bayes factor and eﬀects of parameter
correlations are shown, before turning to the results of the model selection, and their
consequences for the HMGU and ICRP models.
10−2
10−1
100
101
102
10−2
100
concentration [%]
time [d]
   Plasma data   
 
100
101
102
10−2
100
excretion rate [%/d]
time [d]
   Urine data   
 
Inv. 1
Inv. 2
Inv. 3
Inv. 4
Inv. 5
Inv. 6
Inv. 7
Inv. 8
Inv. 9
Inv. 10
Inv. 11
Inv. 12
Inv. 13
Inv. 14
Inv. 15
Inv. 16
Figure 8.2: Plasma and urine data for investigations 1-16 on log-log-scale.
8.4.1
Investigation speciﬁcity of transfer rates
In radiation protection the transfer rates for the biokinetics of radionuclides in the hu-
man body are derived from data collected in various independent experiments (ICRP,
2008). We here used plasma and urine measurements of 16 diﬀerent investigations.
This poses the question whether the models should be compared based on the com-
plete dataset y = {y1, . . . , y16}, or whether statistical evaluation should be done for
each investigation individually on yi for i = 1, . . . , 16.
While the former approach
146

8.4 Inference of the zirconium models
results in one overall Bayes factor, the latter yields 16 investigation speciﬁc, not di-
rectly comparable Bayes factors. Figure 8.2 shows that all investigations follow the
same pulse-like time courses in the transfer compartment x1 while the excretion rates
in the urine compartment x7 exhibit an exponential decay behavior. However, zirco-
nium tracer concentrations showed up to a 50-fold diﬀerence between maximal plasma
concentrations, i.e. for investigation 10 (1.616%, ) and 6 (0.033%).
To test the hypothesis whether the diversity in concentration also eﬀects transfer rates
and therefore the estimated Bayes factors, we pairwise compared the posterior sam-
ple marginals of the MCMC run (corresponding to the samples of T29 = 1) for the
parameter k7 of the ICRP model between all investigations by means of a Kolmogoroﬀ-
Smirnov test.
Here k7 was chosen as it directly aﬀects the observed plasma levels
(Li et al. [2011b]). Except for one pair, all p-values were < 6 · 10−8, meaning that
the chance of falsely rejecting the hypothesis of comparable marginals is negligible.
Therefore, as the posterior marginal distributions are quite diﬀerent, it can be deduced
that the basis for the Bayes factor, the joint posterior distribution, can diﬀer quite
strongly with respect to the individuals. This indicated that each investigation should
be treated separately. Nevertheless, in order to infer the transfer rates of an average
subject (Table 8.5) the concatenated data has to be used. We therefore compared the
HMGU and ICRP model based on both the concatenated data y = {y1, . . . , y16} and,
in order to account for the biological diversity, the individual patient speciﬁc datasets
yi (i = 1, . . . , 16). This could also be the basis for further analysis of inﬂuence factors,
such as weight or gender.
8.4.2
Parameter correlations
The posterior probabilities of both the HMGU and ICRP model show strong cor-
relation between the parameters k7 and k8 throughout all investigations.
The esti-
mated Kendall’s τ’s based on the preruns were ˆτHMGU = 0.8027 ± 0.01 and ˆτICRP =
0.3452 ± 0.02. This can be explained as follows: At time point t = 0 the stomach com-
partment x9 is the only compartment with non-zero Zr concentration. It is exclusively
connected to the small intestines x10 in both models. Therefore, all Zr compounds
have to pass through x10, which further on distributes them to the observed transfer
compartment x1 via k7 or to the upper large intestines x5 via k8. Aberrations in one of
147

8. INFERENCE OF BIOKINETIC MODELS FOR ZIRCONIUM
PROCESSING IN HUMANS
k 1 2
0.4 1.4
1e−46e−4
0.1
0.1 0.3
50 250
0.3 1.3
25 40
0.71.3
1.5 3
10 20
1
3
0.2 0.8
0.4
1.4
−0.01 k 1 1
1e−4
6e−4
0.03
k 1 0
−0.00
0.04
0.1
−0.11
k 9
0.00
−0.00
0.1
0.3
−0.01
k 8
−0.01
0.01
0.01
50
250
0.01
k 7
−0.01
0.01
0.02
0.84
0.3
1.3
−0.03
k 6
0.01
−0.00
0.00
−0.21
−0.27
25
40
−0.01
k 5
0.00
−0.02
−0.00
−0.00
−0.01
−0.01
0.7
1.3
−0.02
k 4
0.01
−0.01
−0.01
−0.01
−0.01
0.02
−0.01
1.5
3
−0.06
k 3
−0.00
−0.01
0.05
0.00
−0.01
0.00
−0.02
0.00
10
20
0.09
k 2
−0.01
−0.01
0.38
−0.16
−0.05
−0.10
−0.00
−0.01
−0.07
1
3
0.18
k 1 2
k 1
−0.01
k 1 1
−0.01
k 1 0
0.01
k 9
0.02
k 8
0.02
k 7
−0.00
k 6
−0.02
k 5
0.01
k 4
0.01
k 3
−0.09
k 2
k 1
Figure 8.3: Pairwise density plots for all parameter-pairs of the HMGU posterior. Red
areas depict higher, blue areas lower density values. The diagonal displays the histograms
of the sample marginals and the numbers in the upper right triangle the estimated Kendall’s
τ’s.
148

8.4 Inference of the zirconium models
k 1 9
1e−5
5e−5
0.01
0.02
0.1
1e−5
7e−5
5 10
6 9
0.1
2540
0.71.2
2 3
10 20
2 4
2
8
1e−5
2e−5
0.00 k 1 8
3e−5
8e−5
0.03 k 1 7
0.05
0.01
0.02
−0.01 k 1 6
−0.01
0.02
0.05
0.15
0.03 k 1 5
−0.00
−0.01
0.03
1e−5
2e−5
−0.01 k 1 4
−0.03
−0.00
0.02
0.01
5e−5
1e−4
0.01 k 1 3
−0.02
−0.01
0.09
0.02
−0.02
5
10
0.04
k 8
0.01
0.01
0.02
0.00
0.00
−0.11
6
9
−0.00 k 7
−0.01
−0.01
0.02
−0.03
0.02
0.19
0.16
0.07
0.12
−0.02 k 6
−0.03
−0.00
0.02
−0.01
0.02
0.00
−0.00
−0.08
25
40
0.02
k 5
−0.03
0.00
0.01
0.03
−0.01
0.02
0.02
−0.01
−0.04
0.7
1.2
−0.01 k 4
−0.01
−0.02
−0.01
−0.01
0.00
−0.02
0.01
−0.03
0.04
−0.00
2
3
0.06
k 3
−0.00
0.02
0.00
0.02
−0.02
0.02
−0.02
0.02
0.01
0.04
−0.02
10
20
0.01
k 2
−0.03
0.02
−0.42
−0.04
0.01
0.02
0.01
0.18
−0.01
−0.00
−0.01
0.03
2
4
0.00
k 1 9
k 1
0.02
k 1 8
0.03
k 17
0.06
k 1 6
0.00
k 1 5
0.02
k 1 4
−0.36
k 1 3
−0.07
k 8
0.20
k 7
−0.04
k 6
−0.03
k 5
0.00
k 4
−0.00
k 3
0.06
k 2
k 1
Figure 8.4: Pairwise density plot for all parameter-pairs of the ICRP posterior. Red areas
depict higher, blue areas lower density values. The diagonal displays the histograms of the
sample marginals and the numbers in the upper right triangle the estimated Kendall’s τ’s.
149

8. INFERENCE OF BIOKINETIC MODELS FOR ZIRCONIUM
PROCESSING IN HUMANS
the parameters k7 or k8 thus have a direct eﬀect on the amount of zirconium predicted
for x1. This aﬀects the according posterior distributions. The same eﬀect is found
for the complete data y (see pairwise scatterplots in Figure 8.4 and 8.3). Despite the
parameter dependencies, the posterior distributions of the ICRP and HMGU model
are identiﬁable for all 16 investigations, this is, the investigation speciﬁc maximum a
posteriori estimates are well deﬁned and inferable (Figure 8.4 and 8.3).
8.4.3
Bayesian model comparison of the HMGU and ICRP models
Applying thermodynamic integration in combination with the CIMH algorithm we
compared the HMGU and the ICRP model based on (i) the concatenated data y =
{y1, . . . , y16} and (ii) the individual investigation speciﬁc datasets yi (i = 1, . . . , 16).
This resulted in a total of 17 Bayes factors. We found that all Bayes factors favored the
HMGU model; in 14 out of the 17 cases even decisively (Table 8.2). Throughout, the
analysis was based on 30,000 proposals for each of the 30 Ti-levels in all 17 cases. Based
on 30,000 proposals the average ESS of the HMGU model including one standard error,
i.e. taken over all Ti levels and investigations, was 5832 ± 405. In case of the ICRP
model we obtained in average 5808 ± 252 (approximately independent) samples from
the Markov chains. This implied high acceptance rates for both models. The sampling
procedure thus captured the power posteriors very well.
In order to take a closer look at the contribution of the plasma and urine data to
the above results, we computed additional Bayes factors based on the likelihoods
Lp
i (ξm|yi, m) and Lu
i (ξm|yi, m) individually. Here, i = 1, . . . , 16, ALL and m ∈{I, H},
where I represents the ICRP and H the HMGU model. The time courses already sug-
gested better coverage of plasma data by the HMGU model (Figure 8.5, and individual
time courses in Appendix G); for urine the situation is not that clear. This was con-
ﬁrmed by the Bayes factors: all 17 Bayes factors based on plasma data favored the
HMGU model; in ten cases even decisively (Table 8.3). For the urine data, three inves-
tigations slightly favored the ICRP model (Table 8.4). In summary, all decisive Bayes
factors are in favor of the HMGU model. This means the HMGU model was never
decisively rejected. On the other hand the ICRP model was decisively rejected in the
majority of cases. Hence, the HMGU model is superior over the ICRP model with
150

8.4 Inference of the zirconium models
Inv.
BH,I
AR HMGU
AR ICRP
ESS HMGU
ESS ICRP
[min, med, max]
[min, med, max]
[min, med, max]
[min, med, max]
ALL
1.2010 · 1011
[0.54, 0.57, 0.64]
[0.27, 0.57, 0.60]
[1500, 4643, 10000]
[639, 4643, 10000]
1
71.7283
[0.56, 0.65, 0.70]
[0.55, 0.63, 0.67]
[1000, 6000, 15000]
[2308, 6000, 10000]
2
114.6109
[0.59, 0.65, 0.69]
[0.54, 0.56, 0.58]
[2000, 7500, 15000]
[1035, 5000, 10000]
3
59532.2127
[0.42, 0.61, 0.66]
[0.47, 0.60, 0.66]
[181, 5500, 10000]
[1667, 6000, 15000]
4
1065.3125
[0.63, 0.66, 0.68]
[0.55, 0.62, 0.64]
[2500, 7500, 15000]
[811, 6000, 15000]
5
219.0939
[0.60, 0.66, 0.69]
[0.56, 0.62, 0.65]
[3750, 7500, 15000]
[1579, 6000, 15000]
6
4642.8755
[0.61, 0.63, 0.68]
[0.57, 0.62, 0.66]
[1072, 6000, 15000]
[1765, 6000, 10000]
7
218.0765
[0.62, 0.66, 0.71]
[0.60, 0.64, 0.67]
[1667, 7500, 15000]
[4286, 7500, 15000]
8
37.5182
[0.47, 0.61, 0.71]
[0.59, 0.64, 0.69]
[2308, 7500, 15000]
[3334, 7500, 15000]
9
462.3241
[0.48, 0.57, 0.71]
[0.41, 0.63, 0.67]
[770, 6000, 15000]
[698, 5500, 15000]
10
861.7574
[0.43, 0.60, 0.71]
[0.44, 0.64, 0.68]
[2000, 7500, 15000]
[126, 5000, 10000]
11
117250.4521
[0.38, 0.49, 0.63]
[0.49, 0.57, 0.59]
[1072, 4286, 15000]
[698, 4286, 7500]
12
177.9964
[0.26, 0.61, 0.72]
[0.46, 0.62, 0.68]
[313, 5000, 10000]
[2308, 6000, 15000]
13
718.7546
[0.10, 0.44, 0.70]
[0.53, 0.58, 0.60]
[169, 4018, 15000]
[2308, 4643, 10000]
14
35.8079
[0.09, 0.41, 0.69]
[0.56, 0.64, 0.69]
[345, 3000, 15000]
[1500, 7500, 15000]
15
6287.6538
[0.22, 0.53, 0.70]
[0.46, 0.64, 0.68]
[121, 5500, 15000]
[1000, 5000, 15000]
16
622.4126
[0.23, 0.56, 0.64]
[0.51, 0.58, 0.59]
[417, 3000, 10000]
[1765, 5000, 10000]
Table 8.2: Bayes factors for the HMGU versus the ICRP model (BH,I) for investigation
1, . . . , 16 and the complete data model (ALL). Green color indicates a Bayes factor in favor
of the HMGU model. Shown are also the minimal, median, and maximal acceptance rates
(AR) and eﬀective sampling sizes (ESS) for both models.
Inv.
Bp
H,I
AR HMGU
AR ICRP
ESS HMGU
ESS ICRP
[min, med, max]
[min, med, max]
[min, med, max]
[min, med, max]
ALL
34283.1711
[0.56, 0.61, 0.64]
[0.41, 0.57, 0.60]
[1000, 6750, 10000]
[1875, 5000, 10000]
1
71.1549
[0.53, 0.65, 0.69]
[0.56, 0.62, 0.66]
[834, 6000, 30000]
[455, 6000, 15000]
2
293.4270
[0.58, 0.64, 0.67]
[0.59, 0.62, 0.66]
[338, 7500, 15000]
[546, 7500, 10000]
3
52297.4330
[0.45, 0.62, 0.66]
[0.55, 0.61, 0.65]
[1200, 6000, 15000]
[1765, 6000, 15000]
4
2639.9965
[0.56, 0.60, 0.63]
[0.50, 0.56, 0.57]
[2308, 6000, 15000]
[968, 3334, 10000]
5
473.1182
[0.59, 0.65, 0.69]
[0.59, 0.63, 0.68]
[3334, 8750, 15000]
[1429, 7500, 15000]
6
3926.9639
[0.62, 0.65, 0.70]
[0.48, 0.62, 0.66]
[577, 7500, 10000]
[698, 6000, 10000]
7
229.9698
[0.55, 0.64, 0.72]
[0.59, 0.64, 0.68]
[968, 6000, 15000]
[2143, 6750, 15000]
8
127.7723
[0.38, 0.57, 0.72]
[0.56, 0.64, 0.69]
[667, 6750, 15000]
[2308, 7500, 15000]
9
231.8086
[0.50, 0.57, 0.65]
[0.58, 0.65, 0.69]
[653, 4286, 15000]
[3334, 7500, 15000]
10
115.6091
[0.53, 0.61, 0.65]
[0.52, 0.58, 0.60]
[215, 4643, 15000]
[1667, 6000, 10000]
11
18.0543
[0.56, 0.65, 0.71]
[0.59, 0.65, 0.69]
[1000, 5500, 15000]
[3750, 6750, 15000]
12
5.4764
[0.55, 0.61, 0.64]
[0.56, 0.58, 0.60]
[750, 6000, 15000]
[2728, 5000, 15000]
13
14.1274
[0.50, 0.67, 0.71]
[0.60, 0.65, 0.67]
[1154, 7500, 15000]
[4286, 7500, 15000]
14
7.4250
[0.59, 0.67, 0.72]
[0.62, 0.65, 0.69]
[2500, 7500, 15000]
[2728, 8750, 15000]
15
21.6865
[0.56, 0.61, 0.65]
[0.55, 0.58, 0.59]
[750, 7500, 15000]
[2000, 5000, 10000]
16
13.4114
[0.56, 0.66, 0.70]
[0.59, 0.66, 0.68]
[625, 7500, 30000]
[3334, 7500, 15000]
Table 8.3: Bayes factors for the HMGU versus the ICRP model (Bp
H,I) for investigation
1, . . . , 16 and the complete data model (ALL) based on plasma data only. Green color
indicates a Bayes factor in favor of the HMGU model. Shown are also the minimal, median,
and maximal acceptance rates (AR) and eﬀective sampling sizes (ESS) for both models.
151

8. INFERENCE OF BIOKINETIC MODELS FOR ZIRCONIUM
PROCESSING IN HUMANS
Inv.
Bu
H,I
AR HMGU
AR ICRP
ESS HMGU
ESS ICRP
[min, med, max]
[min, med, max]
[min, med, max]
[min, med, max]
ALL
47303749.2905
[0.42, 0.58, 0.65]
[0.37, 0.57, 0.60]
[35, 4286, 15000]
[600, 4286, 10000]
1
1.0460
[0.66, 0.70, 0.73]
[0.64, 0.66, 0.68]
[5000, 10000, 15000]
[3750, 7500, 15000]
2
3940.3951
[0.35, 0.54, 0.64]
[0.54, 0.57, 0.60]
[770, 4643, 10000]
[2500, 5000, 7500]
3
1.3352
[0.59, 0.70, 0.73]
[0.60, 0.67, 0.70]
[2143, 8750, 15000]
[4286, 7500, 15000]
4
34.7362
[0.46, 0.65, 0.72]
[0.59, 0.65, 0.69]
[380, 6000, 15000]
[2143, 7500, 15000]
5
133.8984
[0.43, 0.59, 0.64]
[0.55, 0.58, 0.60]
[244, 4018, 15000]
[2308, 5000, 10000]
6
2384.2435
[0.13, 0.48, 0.63]
[0.58, 0.61, 0.62]
[257, 3000, 15000]
[1667, 4286, 7500]
7
1335.8332
[0.13, 0.50, 0.63]
[0.58, 0.61, 0.62]
[136, 3167, 10000]
[2500, 4286, 10000]
8
0.2221
[0.57, 0.69, 0.72]
[0.58, 0.66, 0.68]
[3750, 10000, 15000]
[3334, 7500, 15000]
9
0.1753
[0.33, 0.62, 0.70]
[0.46, 0.63, 0.68]
[235, 4286, 10000]
[770, 7500, 15000]
10
0.1992
[0.48, 0.68, 0.71]
[0.58, 0.64, 0.69]
[1154, 10000, 30000]
[2000, 6750, 15000]
11
2936.7417
[0.33, 0.48, 0.63]
[0.52, 0.57, 0.60]
[273, 3542, 15000]
[2143, 5000, 7500]
12
11.4359
[0.51, 0.59, 0.64]
[0.50, 0.57, 0.60]
[546, 5500, 10000]
[1072, 5000, 7500]
13
4.4105
[0.48, 0.64, 0.71]
[0.59, 0.65, 0.69]
[1000, 6750, 15000]
[2728, 7500, 15000]
14
9.7741
[0.43, 0.54, 0.63]
[0.53, 0.57, 0.60]
[968, 5000, 15000]
[1875, 5000, 10000]
15
160.0045
[0.40, 0.61, 0.71]
[0.52, 0.63, 0.68]
[320, 5000, 15000]
[1875, 6000, 15000]
16
12003.8714
[0.30, 0.50, 0.63]
[0.54, 0.61, 0.62]
[366, 3334, 7500]
[1500, 3750, 10000]
Table 8.4: Bayes factors for the HMGU versus the ICRP model (Bu
H,I) for investigation
1, . . . , 16 and the complete data model (ALL) based on urine data only.
Green color
indicates a Bayes factor in favor of the HMGU model and red color a Bayes factor in favor
of the ICRP model. Shown are also the minimal, median, and maximal acceptance rates
(AR) and eﬀective sampling sizes (ESS) for both models.
respect to zirconium processing in the human body. This not only holds investigation
speciﬁcally, but also based on the complete data.
The posterior median (MAP) as well as the according 95% credible intervals for the
HMGU parameter values based on the complete data y are given in Table 8.5. An
identiﬁability analysis as introduced in Chapter 3.3 showed that all parameter rates
are in fact identiﬁable.
From a comparison with table 8.1, one can see that some
parameters are slightly shifted.
Since these parameter values are derived from the
concatenated data, they are valid for all subjects and thus represent the parameters of
choice for an average subject.
8.4.4
Diﬀerences in radioactive 95Zr retention in bone predicted by
the HMGU and ICRP models
In internal exposure monitoring, biokinetic models are used to predict the organ reten-
tion or daily excretion of incorporated radionuclides (ICRP, 1998). With an interval of
152

8.4 Inference of the zirconium models
Figure 8.5: Posterior median solution (black line) and according 95% credible interval
(shaded area) for the plasma and urinary excretion rate time courses based on the posterior
HMGU and ICRP samples for the data y.
Colored markers are the data points.
For
readability we truncated the plasma plot at 1 · 10−5[%] and the urine plot at 1 · 10−6[%].
Param.
k1
k2
k3
k4
95% CI
[0.03,0.42]
[0.63,2.99]
[7.14,20.91]
[1.03,3.18]
MAP
0.08
1.48
9.54
1.28
Param.
k5
k6
k7
k8
95% CI
[0.47,1.55]
[17.57,45.15]
[0.10,0.61]
[19.58,134.48]
MAP
1.03
37.43
0.19
41.86
Param.
k9
k10
k11
k12
95% CI
[0.12,0.28]
[6.75 · 10−4,0.06]
[1.86 · 10−5,2.57 · 10−4]
[0.14,0.82]
MAP
0.20
0.0028
3.57 · 10−5
0.27
Table 8.5: Posterior median (MAP) and according 95% credible intervals (CI) for the
HMGU parameters based on the complete data y = {y1, . . . , y16}.
120 days the radioactivity of 95Zr possibly incorporated by occupational workers is rou-
tinely monitored by whole body counters. Depending on the intake route, the radiation
dose of bone surfaces or colon is taken as regulatory limit for a decision if an individual
is requested for person-speciﬁc monitoring (BMU, 2007). In this monitoring procedure,
the biokinetic model structure and parameters are used implicitly in the background.
The organ retention function is the solution of the model in each compartment; the
organ doses are directly related to the integral of radioactivity of 95Zr in source organs
over 50 years.
In order to compare the retention of 95Zr as predicted by the ICRP and HMGU models,
the 90% credible intervals for the time courses in the bone compartments were calcu-
lated based on the posterior samples. It is found that there is a signiﬁcant diﬀerence
153

8. INFERENCE OF BIOKINETIC MODELS FOR ZIRCONIUM
PROCESSING IN HUMANS
Figure 8.6: Posterior median (solid lines) as well as 90% credible intervals (shaded areas)
for the retention of 95Zr in the bone compartments as predicted by the HMGU (blue) and
ICRP (red) models with radioactive decay taken into account.
between both models (Figure 8.6), where for the ICRP model we added the concentra-
tions in the two bone compartments. The time courses were derived for stable isotopes
of Zr and thus the radioactive decay of 95Zr with half-life of 64.032d (ICRP, 2008) had
to be taken into account. The decrease of retention in bone using the HMGU model
consequently reduces the radiation dose in bone in comparison to the ICRP bone dose
which is currently used in monitoring.
8.4.5
Retrospective dose assessment
Internal doses due to incorporated radionuclides have to be estimated with the help
of biokinetic models based on indirect measurements, using for example bioassays for
blood or urinary excretion. Normally, bioassay or in vivo data (e.g. radioactivity ac-
cumulated in skull or knee detected by a partial body counter) are measured after an
accidental intake of radionuclides. Uncertainties of estimated doses are signiﬁcant and
have a large impact on remediation and thus action costs. In contrast to conventional
uncertainty analysis (Li et al., 2011a), our Bayesian Ansatz naturally integrates the un-
certainties of measured data and parameters simultaneously. This trait of the Bayesian
approach is helpful as it provides an estimate for the intake and its credible intervals.
For example, if the urinary excretion after accidental exposure is measured, we are able
to compute credible intervals for the initial intake of radionuclide 95Zr by exploiting
154

8.4 Inference of the zirconium models
the posterior distribution together with the linearity of the HMGU model. In order
to be as general as possible we use the posterior samples based on the complete data
y. Given a posterior sample ξH, a measurement ˙xt
7 in [µg/d] for the urinary excretion
rate of zirconium at time point t corresponds to a unique solution xξH(t) of the HMGU
ODE system.
Due to the linearity of the ODE’s, the initial concentration xξH(0)
is by deﬁnition zero for all except the stomach compartment x9.
The latter reads
x9(0) = ˙xt
7 · 100%/x9
ξH(t) where x9
ξH(t) denotes the value of xξH(t) in the stomach
compartment at time point t. Now, assuming that for arbitrary posterior samples ξH
the measurement ˙xt
7 is contained in the 90% credible interval of the solution xξH(t)
with initial condition x9(0) as given above, lower and upper bounds for credible regions
of the initial amount of zirconium taken in at t0 = 0h emerge. These are based on the
posterior samples. The estimated extrapolation factors for multiplication with a urine
measurement (in [µg/d]) after time t (in [h]) are contained in Table 8.6 and yield the
initially amount of zirconium contained in the stomach at t0 = 0h. For example, if an
amount of ˙x2d
7 = 50µg/d was measured after two days, we ﬁnd from Table 8.6 that the
90% credible interval for the ingested amount lies between 0.029g and 0.059g. Since the
above calculations are based on non-radioactive Zr isotopes, the results have to take
into account the radioactive decay of the radionuclide in question, i.e. in many cases
95Zr, for dose assessment.
Time t
6h
12h
18h
24h
30h
lbf for IC
1233.91
1820.44
2614.48
3369.70
4100.16
mf for IC
1763.73
2225.90
3153.70
4228.19
5340.23
ubf for IC
2512.54
2832.49
3978.27
5650.86
7516.00
Time t
36h
42h
48h
54h
60h
lbf for IC
4778.27
5352.64
5800.77
6153.80
6450.74
mf for IC
6364.76
7250.67
7977.31
8557.87
9006.97
ubf for IC
9122.11
10655.01
11878.81
12960.61
13903.07
Table 8.6: Retrospective urine predictions for the HMGU model. Shown are the lower
bound factor (lbf), median factor (mf), and upper bound factor (ubf) for multiplication
with a urine measurement (in [µg /d]) after time t (in [h]) on a 60h grid yielding the initial
intake concentration (IC) at t0 = 0h.
Concluding we have seen that transfer rates can diﬀer quite heavily for the various
investigations. However, the HMGU model is able to outperform the current ICRP
155

8. INFERENCE OF BIOKINETIC MODELS FOR ZIRCONIUM
PROCESSING IN HUMANS
model based on the complete data (corresponding to an average individual) and inves-
tigation speciﬁcally. It can hence improve predictions in internal dosimetry compared
to the current ICRP model.
156

9
Conclusions and outlook
In this thesis we have introduced two novel MCMC sampling schemes: The hybrid
vine copula based independence/random walk Metropolis-Hastings algorithm (CIMH)
and the adaptive vine copula based independence/random walk Metropolis-Hastings
algorithm (ACIMH). A vine copula decomposition of the posterior distribution here
exploits higher order parameter dependencies in order to generate eﬃcient problem
speciﬁc MCMC proposals. The algorithms were applied for parameter inference and
model selection in various dynamical systems.
We tested the performance of CIMH and ACIMH on four examples: First of all, we
inferred the (i) mean and covariance matrix of a strongly correlated two dimensional
normal distribution. The system was analytically tractable and provided a simple proof-
of-concept example. Subsequently, an (ii) ordinary diﬀerential equations driven steady
state as well as an (iii) ordinary diﬀerential equations driven compartment model were
considered. Finally an existing (iv) delay diﬀerential equations model of the JAK2-
STAT5 signaling pathway (Swameye et al. [2003]) has been inferred.
Throughout, both algorithms were evaluated on the basis of the quotient of acceptance
rate versus ineﬃciency factor (I1) and the number of independent samples generated per
second (I2). Here, (I1) was motivated by the antagonistic behavior of high acceptance
rates versus high INEFF’s, while (I2) provided an easily interpretable performance
statistic.
As competing samplers a simple random walk Metropolis-Hastings, a co-
variance based random walk Metropolis-Hastings, and an independence chain sampler
157

9. CONCLUSIONS AND OUTLOOK
were chosen for the ﬁrst three examples. The JAK2-STAT5 pathway was additionally
evaluated by SMALA and M-GaA. Our copula based approach generally covered the
dependency structure of the posterior very well and outperformed all other sampling
schemes in every example. It turned out that the basic CIMH algorithm is doing best on
simple systems as it does not lose time on extra copula updates. However, in very com-
plex situations, such as the inference of the JAK-STAT5 pathway, copula updates were
needed to ﬁne-tune the proposal distribution and thereby improve the performance.
We applied ACIMH to infer a model of the JAK1-STAT3 signaling pathway. Ther-
modynamic integration provided a Bayes factor that rejected a model covering di-
rect tyrosine-phosphorylated STAT3 dimer import into the nucleus as compared to a
model considering tyrosine-serine-phosphorylated STAT3 dimer import only. The esti-
mated maximum a posteriori estimate for nuclear abidance time of the tyrosine-serine-
phosphorylated STAT3 dimer turned out to be unnaturally short (0.252 minutes). The
error prone total STAT-data might here aﬀect the result. Additional measurements of
total cytoplasmic STAT3 concentrations could supposedly remedy this issue.
Moreover, we evaluated two competing biokinetic models for zirconium processing in
the human body after ingestion for in vivo plasma and urine measurements. In order to
obtain reliable Monte Carlo sampling results, we again combined the numerically stable
thermodynamic integration, this time with CIMH. Based on individual Bayes factors
for 16 investigations as well as a Bayes factor based on the concatenated dataset the
HMGU model was unequivocally superior when compared to the current ICRP model.
Also, when restricting the data on plasma and urine measurements only, we found that
the HMGU model was clearly favored.
In contrast to the ICRP model, the HMGU model predicted a delayed accumulation
of zirconium in bones. Furthermore, we showed that the HMGU model can be applied
for retrospective dose assessment, where the initially ingested amount of zirconium
can be reconstructed (including credible intervals) from ex post urine measurements.
This provided estimates that facilitate the decision if measures have to be taken in
case of accidental exposure. In future applications the HMGU model together with its
posterior samples can readily be used as basis for dose assessment in internal dosimetry.
158

In this thesis we primarily focused on the issues of model selection and parameter infer-
ence in dynamic systems governed by ordinary or delay diﬀerential equations. Typically,
a closed form solution to the diﬀerential equation system is unavailable in real world
applications. The computationally expensive numerical solution for every likelihood
evaluation calls for a sophisticated MCMC proposal generation scheme. However, the
ﬁelds of application of CIMH and ACIMH is not limited to this scenario. Both vine
copula based algorithms can be applied to any MCMC inference problem, such as
Bayesian inference of ARMA or GARCH models used in economics and ﬁnance. They
are expected to work well on highly dependent posterior distributions, but also very
eﬃciently in simple systems.
Nevertheless, further research is needed to improve the algorithms for sampling from
highly complex posterior distributions. A simple ﬁrst step in this direction could be to
apply automated cdf type detection for sample uniformization in each copula adaption
step. Monitoring Markov chain convergence by means of convergence statistics could
moreover lead to variable adaption of the proposal function rates r1, r2, and r3. The
ﬁnite copula update scheme of ACIMH might even be generalized to an inﬁnite update
scheme. Clearly, this requires a thorough proof of convergence.
CIMH and ACIMH can readily be extended to population MCMC sampling schemes.
Much in the sense of thermodynamic integration and path sampling (Gelman & Meng
[1998]), tempered MCMC approaches (Liu [2008]) might help to explore the sampling
spaces more quickly. This would possibly produce apt copula decomposition of the
posterior distribution in a much faster way.
The JAK2-STAT5 pathway analysis indicates that non-standard copula and marginal
distributions might be needed to guarantee eﬃcient sampling performances. Fitting
non-parametric cdf’s for sample uniformization as well as non-parametric pair copula
distributions could be a further step to improve the eﬃciency of CIMH and ACIMH.
However, as proposal generation might become computationally more expensive it has
to be checked whether speed advantages with respect to (I2) would still be retained.
A similar issue constitutes the choice of the copula decomposition: Although the order
of the variables was rather canonical for our examples, introducing more general vine
structures, such as R-vines, could further increase the sampling eﬃciency. Additional
vine structure selection methods would however be needed in this case.
159

9. CONCLUSIONS AND OUTLOOK
160

Appendix A
Important univariate density
functions
The following univariate density functions are used for the copula based independence
chain Metropolis-Hastings algorithm:
Normal density function
A random variable X is called normally distributed, if its density function is for µ ∈R
and σ > 0 given by
f(x) =
1
√
2πσ2 exp

−1
2σ2 (x −µ)2

.
We write X ∼N(µ, σ). A plot of the normal density function is shown in Figure A.1(a).
Lognormal density function
A random variable X is called lognormally distributed, if its density function is for
µ ∈R and σ > 0 given by
f(x) =
1
x
√
2πσ2 exp

−1
2σ2 (log(x) −µ)2

1(0,∞)(x),
where 1(0,∞)(x) denotes the indicator function on (0, ∞). We write X ∼LN(µ, σ). A
plot of the lognormal density function is shown in Figure A.1(b).
161

A. IMPORTANT UNIVARIATE DENSITY FUNCTIONS
−5
0
5
0
0.2
0.4
0.6
0.8
1
x
f(x)
 
 
µ = 0, σ2 = 1
µ = 0, σ2 = 4
µ = 0, σ2 = 0.6
µ = 2, σ2 = 1
(a)
0
5
10
15
0
0.2
0.4
0.6
0.8
x
f(x)
 
 
µ = 2, σ2 = 1
µ = 2, σ2 = 4
µ = 2, σ2 = 0.36
µ = 0, σ2 = 1
(b)
0
5
10
15
0
0.2
0.4
0.6
0.8
x
f(x)
 
 
q = 0.8, λ = 0.5
q = 3,
λ = 0.5
q = 5,
λ = 0.5
q = 3,
λ = 2
(c)
0
2
4
0
0.5
1
1.5
2
x
f(x)
 
 
λ = 0.5
λ = 1
λ = 2
(d)
Figure A.1: Various univariate (a) normal, (b) lognormal, (c) gamma, and (d) exponential
density functions.
Gamma density function
A random variable X is called gamma distributed, if its density function is for q, λ > 0
given by
f(x) =
λq
Γ(q)xq−1 exp(−λx)1(0,∞)(x),
where 1(0,∞)(x) denotes the indicator function on (0, ∞) and
Γ(q) :=
Z ∞
0
tq−1e−t dt
the Gamma function. We write X ∼Γ(q, λ). A plot of the Gamma density function is
shown in Figure A.1(c).
Exponential density function
A random variable X is called exponentially distributed, if its density function is for
λ > 0 given by
f(x) = λ exp(−λx)1(0,∞)(x),
where 1(0,∞)(x) denotes the indicator function on (0, ∞). We write X ∼Exp(λ). A
plot of the exponential density function is shown in Figure A.1(d).
162

Appendix B
Important bivariate copulas
The following bivariate copulas are used for the copula based independence chain
Metropolis-Hastings algorithm:
Bivariate elliptical copula density functions
The copula density function of the bivariate Gaussian copula (c.f. Aas et al. [2009]) is
given by
c(u1, u2|ρ) =
1
p
1 −ρ2 exp
ρ2(x2
1 + x2
2) −2ρx1x2
2(1 −ρ2)

.
Here, ρ ∈(−1, 1) denotes the (correlation) parameter and xi = Φ−1(ui) for the inverse
Φ−1(·) of the standard normal distribution function. Figure 2.1(c) in Chapter 2.2 shows
a plot of the Gaussian copula density function for ρ = 0.5.
The copula density function of the bivariate Student’s t copula (c.f. Aas et al. [2009])
is for the copula parameters ν > 2 and ρ ∈(−1, 1) given by
c(u1, u2|ρ, ν) =
Γ(ν/2 + 1)/Γ(ν/2)
νπtdν(x1)tdν(x2)
p
1 −ρ2

1 + x2
1 + x2
2 −2ρx1x2
ν(1 −ρ2)
−ν/2−0.5
.
Here, Γ(·) denotes the Gamma function
Γ(x) =
Z ∞
0
yx−1e−y dy
163

B. IMPORTANT BIVARIATE COPULAS
and xi = t−1
ν (ui) for the inverse t−1
ν
of the univariate standard student’s t distribution
function with ν degrees of freedom deﬁned via the corresponding density function
td
ν(x) = Γ(n/2 + 0.5)/Γ(n/2)
√νπ

1 + x2
ν
−n/2−0.5
.
Figure B.1(a) shows a plot of the student’s t copula density function for ρ = 0.7 and
ν = 1.
Bivariate Archimedean copulas
Archimedean copulas are deﬁned via their generators (see Theorem 2.3). The following
table holds a selection of the most prominent Archimedean copulas (c.f. Brechmann &
Schepsmeier [2011]):
Name
Generator
Parameter(s)
Independence (I)
−log(t)
–
Clayton (C)
(t−θ −1)/θ
θ > 0
Gumbel (G)
(−log(t))θ
θ ≥1
Frank (F)
−log ((exp(−θt) −1)/(exp(−θ) −1))
θ ∈R \ {0}
Joe (J)
−log
 1 −(1 −t)θ
θ > 1
BB1
(t−θ −1)δ
θ > 0 , δ ≥1
BB6
(−log(1 −(1 −t)θ))δ
θ ≥1, δ ≥1
BB7
(1 −(1 −t)θ)−δ −1
θ ≥1, δ > 0
BB8
−log
 (1 −(1 −δt)θ)((1 −(1 −δ)θ)

θ ≥1, δ ∈(0, 1]
Table B.1:
A selection of Archimedean copulas.
Figure 2.1(a) shows a plot of the independence copula density function.
All other
copula density types are depicted in Figure B.1.
We furthermore get for each elliptical or Archimedean copula C the 90◦, 180◦and 270◦
rotated copulas C90, C180, and C270 by setting
C90(u1, u2) = u2 −C(1 −u1, u2),
C180(u1, u2) = u1 + u2 −1 + C(1 −u1, 1 −u2),
C270(u1, u2) = u1 −C(u1, 1 −u2).
164

0
0.5
1
0
0.5
10
1
2
3
x 10
4
u1
u2
(a)
0
0.5
1
0
0.5
10
2
4
x 10
4
u1
u2
(b)
0
0.5
1
0
0.5
10
1
2
3
x 10
4
u1
u2
(c)
0
0.5
1
0
0.5
1
0
5000
10000
u1
u2
(d)
0
0.5
1
0
0.5
10
1
2
3
x 10
4
u1
u2
(e)
0
0.5
1
0
0.5
10
2
4
x 10
4
u1
u2
(f)
0
0.5
1
0
0.5
10
2
4
x 10
4
u1
u2
(g)
0
0.5
1
0
0.5
10
2
4
x 10
4
u1
u2
(h)
0
0.5
1
0
0.5
1
0
5000
10000
u1
u2
(i)
Figure B.1:
Copula density functions for the (a) student’s t (0.7,1), (b) Clayton (2), (c)
Gumbel (2), (d) Frank (5), (e) Joe (2), (f) BB1 (2,2), (g) BB6 (2,2), (h) BB7 (2,2), and
(i) BB8 (2,0.8) copula. The bracketed values denote the according parameter values in the
order given in Table B.1.
165

B. IMPORTANT BIVARIATE COPULAS
0
0.5
1
0
0.5
10
1
2
3
x 10
4
u1
u2
(a)
0
0.5
1
0
0.5
10
2
4
x 10
4
u1
u2
(b)
0
0.5
1
0
0.5
1
0
5000
10000
u1
u2
(c)
Figure B.2: Copula density functions for the 90◦rotated (a) student’s t (0.7,1), (b) Clay-
ton (2), and (c) Frank (5) copula. The bracketed values denote the according parameter
values in the order given in Table B.1.
Note that some copula types coincide with some of their rotated versions due to reasons
of symmetry.
Figures B.2(a), B.2(b), and B.2(c) show a plot of the 90◦rotated t,
Clayton, and Frank copula density functions.
166

Appendix C
Calculations for the Bayes factor
of the Gaussian mixture model
For our Gaussian mixture example from Chapter 3.4.4 we explicitly compute the power
posterior at t ∈[0, 1] as well as the marginal likelihoods. Subsequently, we exemplary
infer the expected value of the log-likelihood with respect to the power posterior for
the one component case. We use the notations introduced in Chapter 3.4.4.
C.1
Power posterior and marginal likelihood for the one-
component Gaussian (mixture) model
Setting ¯y = 1
m
Pm
i=1 yi, the t-powered product of the likelihood times prior for M1 given
the observations y = {y1, . . . , ym} and the prior distribution µ ∼N(0, σ2) computes to
L(µ|y)tπ(µ|M1) =

1
√
2πσ
mt+1
exp
 
−1
2σ2
 
t
m
X
i=1
y2
i −2µt
m
X
i=1
yi + (mt + 1)µ2
!!
=

1
√
2πσ
mt+1
exp
 
1
2σ2
 
(mt¯y)2
mt + 1 −t
m
X
i=1
y2
i
!!
· exp
 
−mt + 1
2σ2
 mt¯y
mt + 1 −µ
2!
∝exp
 
−mt + 1
2σ2

µ −t Pm
i=1 yi
mt + 1
2!
,
167

C. CALCULATIONS FOR THE BAYES FACTOR OF THE GAUSSIAN
MIXTURE MODEL
where the proportionality is to be understood with respect to µ. The power posterior
for t ∈[0, 1] and µ ∈R is therefore given by N
 t Pm
i=1 yi
mt+1 ,
σ2
mt+1

. Furthermore, setting
t = 1 and integrating over the real line yields the marginal likelihood
π(y|M1) =
Z
R

1
√
2πσ
m+1
exp
 
1
2σ2
 
m2¯y2
m + 1 −
m
X
i=1
y2
i
!!
· exp
 
−m + 1
2σ2
 m¯y
m + 1 −µ
2!
dµ
=
1
√m + 1

1
√
2πσ
m
exp
 
1
2σ2
 
m2¯y2
m + 1 −
m
X
i=1
y2
i
!!
·
Z
R
√m + 1
√
2πσ exp
 
−m + 1
2σ2
 m¯y
m + 1 −µ
2!
dµ
=
1
√m + 1

1
√
2πσ
m
exp
 
1
2σ2
 
m2¯y2
m + 1 −
m
X
i=1
y2
i
!!
.
C.2
Power posterior for the two-component Gaussian (mix-
ture) model
Similarly to the one-component case, setting ¯y1 =
1
m1
Pm1
i=1 yi and ¯y2 =
1
m2
Pm
j=m1+1 yj
for m2 := m −m1, the t-powered product of the likelihood times prior for the two-
component model M2 given the observations y = {y1, . . . , ym, ym1+1, . . . , ym} and the
independent prior distributions µ1 ∼N(2, σ2) and µ2 ∼N(−2, σ2) computes to
L(µ|y)tπ(µ1|M2)π(µ2|M2) =

1
√
2πσ
mt+2
exp
 
−
1
2σ2
 
t
m
X
i=1
y2
i + 8 −2(m1t¯y1 + 2)µ1
−2(m2t¯y2 −2)µ2 + (m1t + 1)µ2
1 + (m2t + 1)µ2
2
!!
∝exp
 
−m1t + 1
2σ2

µ1 −2 + t Pm1
i=1 yi
m1t + 1
2!
· exp

−m2t + 1
2σ2
 
µ2 −
−2 + t Pm
j=m1+1 yj
m2t + 1
!2
,
168

C.3 Expected value of the log likelihood w.r.t. the power posterior for the
one-component Gaussian (mixture) model
where the proportionality is to be understood with respect to µ1 and µ2. The power
posterior for t ∈[0, 1] and (µ1, µ2)⊤∈R2 is hence given by
N2




2+t Pm1
i=1 yi
m1t+1
−2+t Pm
j=m1+1 yj
mt−m1t+1

,
 
σ2
m1t+1
0
0
σ2
mt−m1t+1
!
.
For t = 1 integration over R2 yields the marginal likelihood
π(y|M2) =
Z
R
Z
R

1
√
2πσ
m+2
exp
 
−
1
2σ2
 m
X
i=1
y2
i + 8 −2(m1¯y1 + 2)µ1
−2(m2¯y2 −2)µ2 + (m1 + 1)µ2
1 + (m2 + 1)µ2
2
!!
dµ1dµ2
=
1
p
(m1 + 1)(m2 + 1)

1
√
2πσ
m
· exp
 
−1
2σ2
 m
X
i=1
y2
i + 8 −(m1¯y1 + 2)2
m1 + 1
−(m2¯y2 −2)2
m2 + 1
!!
.
C.3
Expected value of the log likelihood w.r.t. the power
posterior for the one-component Gaussian (mixture)
model
Exemplary we compute for the power posterior
πt(µ|y, M1) = L(µ|y)t · π(µ|M1)
πt(y|M1)
the normalizing constant
πt(y|M1) =
Z
R
L(µ|y)t · π(µ|M1) dµ
=
Z
R

1
√
2πσ
mt+1
exp
 
−1
2σ2
 
t
m
X
i=1
y2
i −2m¯ytµ + (mt + 1)µ2
!!
dµ
=
1
√mt + 1

1
√
2πσ
mt
exp
 
−1
2σ2
 
t
m
X
i=1
y2
i −(mt¯y)2
mt + 1
!!
.
In order to compute the expectation of the data’s log-likelihood, log L(µ|y, M1), with
respect to the power posterior πt(µ|y, M1), we ﬁrst have to do the calculations of
169

C. CALCULATIONS FOR THE BAYES FACTOR OF THE GAUSSIAN
MIXTURE MODEL
•
Z
R
A(µ) dµ :=
 m
X
i=1
(y2
i + 2σ2 log(
√
2πσ))
! Z
R
exp
 
−mt + 1
2σ2
 mt¯y
mt + 1 −µ
2!
dµ
•
Z
R
B(µ) dµ := −2m¯y
Z
R
µ exp
 
−mt + 1
2σ2
 mt¯y
mt + 1 −µ
2!
dµ
•
Z
R
C(µ) dµ := m
Z
R
µ2 exp
 
−mt + 1
2σ2
 mt¯y
mt + 1 −µ
2!
dµ.
For a := mt+1
2σ2
and z :=
mt
mt+1 ¯y the three integrals are
Z
R
A(µ) dµ =
 m
X
i=1
(y2
i + 2σ2 log(
√
2πσ))
! Z
R
exp
 
−mt + 1
2σ2
 mt¯y
mt + 1 −µ
2!
dµ
=
 m
X
i=1

y2
i + 2σ2 log(
√
2πσ)
!
√
2πσ
√mt + 1,
Z
R
B(µ) dµ = −2m¯y
Z
R
µ exp
 
−mt + 1
2σ2
 mt¯y
mt + 1 −µ
2!
dµ
= −2m¯y
Z
R
µ exp

−a (z −µ)2
dµ
= −2m¯y
Z
R
(µ −z) exp
 −a(z −µ)2
+ z exp
 −a(z −µ)2
dµ
= −2m¯y
 Z ∞
−∞
y exp(−ay2) dy + z
√
2πσ
√mt + 1
!
= −2m¯yz
√
2πσ
√mt + 1 = −2¯y2
√
2πσm2t
(mt + 1)
3
2
,
and
170

C.3 Expected value of the log likelihood w.r.t. the power posterior for the
one-component Gaussian (mixture) model
Z
R
C(µ) dµ = m
Z
R
µ2 exp
 
−mt + 1
2σ2
 mt¯y
mt + 1 −µ
2!
dµ
= m
Z
R
µ2 exp

−a (z −µ)2
dµ
= m
Z
R
(µ −z)2 exp

−a (z −µ)2
−z2 exp

−a (z −µ)2
+ 2zµ exp

−a (z −µ)2
dµ
= m
Z
R
y2 exp
 −ay2
dy −mz2
Z
R
exp

−a (z −µ)2
dµ
+ 2mz
Z
R
µ exp

−a (z −µ)2
dµ
= 2m
Z ∞
0
y2 exp
 −ay2
dy −m

mt
mt + 1 ¯y
2
√
2πσ
√mt + 1
+ 2mz
Z
R
(µ −z) exp

−a (z −µ)2
dµ
+
Z
R
z exp

−a (z −µ)2
dµ
= 2mΓ(3
2)
2a
3
2
−m3t2¯y2√
2πσ
(mt + 1)
5
2
+ 2z2m
√
2πσ
√mt + 1
= m
√
2πσ3
(mt + 1)
3
2
+ m3t2¯y2√
2πσ
(mt + 1)
5
2
.
Deﬁning furthermore
α(y|t) := −
1
Z(y|t)
1
2σ2

1
√
2πσ
mt+1
,
β(y|t) := α(y|t) exp
 
−1
2σ2
 
t
m
X
i=1
y2
i −(mt¯y)2
mt + 1
!!
= −1
2σ2 ,
we ﬁnally get:
171

C. CALCULATIONS FOR THE BAYES FACTOR OF THE GAUSSIAN
MIXTURE MODEL
Eπt(µ|y,M1) (log L(µ|y))
=
Z
R
log L(µ|y)L(µ|y)tπ(µ)
Z(y|t, M1) dµ
=
1
Z(y|t, M1)
Z
R
 m
X
i=1

−1
2σ2 (y2
i −2µyi + µ2)

−m log(
√
2πσ)
!
·

1
√
2πσ
mt+1
exp
 
−1
2σ2
 
t
m
X
i=1
y2
i −2m¯ytµ + (mt + 1)µ2
!!
dµ
= α(y|t) ·
Z
R
 m
X
i=1
(y2
i −2µyi + µ2) + 2mσ2 log(
√
2πσ)
!
· exp
 
−1
2σ2
 
t
m
X
i=1
y2
i −(mt¯y)2
mt + 1
!!
· exp
 
−mt + 1
2σ2
 mt¯y
mt + 1 −µ
2!
dµ
= β(y|t) ·
Z
R
 m
X
i=1

y2
i −2µyi + µ2 + 2σ2 log(
√
2πσ)
!
· exp
 
−mt + 1
2σ2
 mt¯y
mt + 1 −µ
2!
dµ
= β(y|t) ·
Z
R
A(µ) dµ +
Z
R
B(µ) dµ +
Z
R
C(µ) dµ

β(y|t)
(  m
X
i=1

y2
i + 2σ2 log(
√
2πσ)
!
·
√
2πσ
√mt + 1
−2¯y2
√
2πσm2t
(mt + 1)
3
2
+ m
√
2πσ3
(mt + 1)
3
2
+ m3t2¯y2√
2πσ
(mt + 1)
5
2
)
= β(y|t)
√
2πσ
√mt + 1
(  m
X
i=1

y2
i + 2σ2 log(
√
2πσ)
!
+ mσ2 −2tm2¯y2
mt + 1
+
m3t2¯y2
(mt + 1)2
)
=
= −1
2σ2
(  m
X
i=1

y2
i + 2σ2 log(
√
2πσ)
!
+ mσ2 −2¯y2m2t
mt + 1
+
m3¯y2t2
(mt + 1)2
)
172

Appendix D
Transformation of the
JAK2-STAT5 DDE system
In order to resolve structural identiﬁability issues of the non-linear JAK2-STAT5 DDE
system
dx1(t)
dt
= −k1x1(t)Epo(t) + 2k4x3(t + τ)
dx2(t)
dt
= −k2x2
2(t) + k1x1(t)Epo(t)
dx3(t)
dt
= −k3x3(t) + 1
2k2x2
2(t)
dx4(t)
dt
= −k4x3(t + τ) + k3x3(t),
(D.1)
with observables
y1(t) = k5(x2(t) + 2x3(t))
and
y2(t) = k6(x1(t) + x2(t) + 2x3(t))
of Chapter 6.3.4 we follow Timmer et al. [2004] and deﬁne
zi(t) = k2xi(t)
for i = 1, . . . , 4,
k′
i = ki
k2
for i = 5, 6.
(D.2)
173

D. TRANSFORMATION OF THE JAK2-STAT5 DDE SYSTEM
The transformed DDE system then reads
dz1(t)
dt
= −k1z1(t)Epo(t) + 2k4z3(t + τ)
dz2(t)
dt
= −z2
2(t) + k1z1(t)Epo(t)
dz3(t)
dt
= −k3z3(t) + 1
2z2
2(t)
dz4(t)
dt
= −k4z3(t + τ) + k3z3(t),
(D.3)
with observables
y1(t) = k′
5(z2(t) + 2z3(t))
and
y2(t) = k′
6(z1(t) + z2(t) + 2z3(t)).
Note that this system is structurally identiﬁable (Timmer et al. [2004]) and has the
same observables y1(t) and y2(t) as the original system (D.1). The posterior distribution
with respect to (D.1) can hence be directly transformed into the posterior distribution
with respect to (D.3). Since x1(0) = 1 and x2(0) = x2(0) = x4(0) = 0, the initial
conditions for (D.3) are given by z1(0) = k2 and z2(0) = z2(0) = z4(0) = 0, i.e.
k2 corresponds directly to the initial condition for z1(t). The transformed system is
therefore parametrized by
ξ = (k1, k2, k3, k4, τ, k′
5, k′
6)⊤.
Inference of the JAK2-STAT5 parameters was done using (D.3). All estimated pa-
rameter marginal posterior means, modes, and 90% posterior quantile based credible
intervals of Table 6.4 in Chapter 6.3.4 are given with respect to the original parameters
k1, . . . , k4, τ, k5, k6 by application of the inverse transformation to (D.2).
174

Appendix E
Geometric tensor for the
JAK2-STAT5 DDE system
For the observations y := {yε
1(t1), . . . , yε
1(t16), yε
2(t1), . . . , yε
2(t16)} on the time grid
t1, . . . , t16 and the parameter vector ξ = (k1, k2, k3, k4, τ, k′
5, k′
6)⊤we want to infer the
posterior distribution
π(ξ|y) ∝
16
Y
i=1
Φ(yε
1(ti)|y1(ti), σ2
i,1) · Φ(yε
2(ti)|y2(ti), σ2
i,2) · π(ξ),
(E.1)
where Φ denotes the pdf of the univariate normal distribution for the known measure-
ment errors σ2
i,j, i = 1, . . . , 16, j = 1, 2, and π(ξ) = Q
j̸=3
1[0,50](ξj) · π(ξ3|ξ4) – recall
k3 ≥k4. Furthermore, y1(t) = k′
5(z2(t) + 2z3(t)) and y2(t) = k′
6(z1(t) + z2(t) + 2z3(t))
for the solutions z1(t), z2(t), and z3(t) of the DDE system (D.3) of Appendix D. The
geometric tensor, as introduced in Chapter 5.1, is given by the expected Fisher infor-
mation matrix of the log-likelihood minus the Hessian of the log-prior. It involves the
partial derivatives of y1(t) and y2(t) with respect to the parameters k1, k2, k3, k4, τ, k′
5
and k′
6. These, however, are not analytically tractable. We therefore (i) approximated
the DDE system (D.3) by means of an ODE system and (ii) subsequently apply the
so-called sensitivity equations to circumvent this issue.
(i) Approximation of the DDE system using the linear chain trick: Simply spoken the
linear chain trick approximates the time delay of a DDE system by a sequence of linear
175

E. GEOMETRIC TENSOR FOR THE JAK2-STAT5 DDE SYSTEM
segments.
More precisely, suppose we are given the diﬀerential equation with time
delay τ
dx(t)
dt
= gξ(x(t), x(t −τ), u(t), t)
(E.2)
where t ∈R, x ∈Rr and – with respect to x(t) and x(t −τ) – Lipschitz-continuous
ξ-parametrized function gξ(x(t), x(t −τ), u(t), t) with gξ(0, 0, 0, t) = 0 and external
stimulus u(t). Then we can transform (E.2) into a corresponding ODE system using
m ∈N segments xi(t):
dx(t)
dt
= gξ(x(t), xm(t), u(t), t),
dxi(t)
dt
= m
τ (xi−1(t) −xi(t)),
i = 1, . . . , m
by exchanging the delayed elements x(t −τ) with xm(t) (Fall [2002]). In this sense we
introduce two auxiliary functions z5(t) and z6(t) for the system (D.3) of Appendix D
and deﬁne the ODE
dz1(t)
dt
= −k1z1(t)Epo(t) + 2k4z6(t)
dz2(t)
dt
= −z2
2(t) + k1z1(t)Epo(t)
dz3(t)
dt
= −k3z3(t) + 1
2z2
2(t)
dz4(t)
dt
= −k4z6(t) + k3z3(t)
dz5(t)
dt
= 2
τ (z3(t) −z5(t))
dz6(t)
dt
= 2
τ (z5(t) −z6(t)),
(E.3)
with z5(0) = z6(0) = 0. Nikolov et al. [2007] discuss the asymptotic stability of (E.3)
and show that the solutions of the original DDE and the approximating ODE model
are very close. We can thus use the much simpler diﬀerential equation (E.3) to compute
the geometric tensor for the JAK2-STAT5 system. Note that the approximation error
does not eﬀect the validity but only the eﬃciency of the posterior inference process in
the SMALA algorithm.
(ii) Sensitivity equations: Suppose we are given an arbitrary ξ-parametrized ODE
system
dx(t)
dt
= gξ(x(t), u(t), t)
(E.4)
176

with initial conditions x(0) = x0. Using the chain rule the derivative with respect to ξ
and x(0) yields the following sensitivity equations for S(t) = ∂x(t)
∂ξ
and S0(t) = ∂x(t)
∂x0 :
d
dtS(t) = ∂gξ(x(t), u(t), t)
∂x(t)
S(t) + ∂gξ(x(t), u(t), t)
∂ξ
with S(0) = 0
d
dtS0(t) = ∂gξ(x(t), u(t), t)
∂x(t)
S0(t)
with S0(0) = Im
where Im is the m-dimensional identity matrix (Wu et al. [2008]). This means we can
numerically solve the extended diﬀerential equations system
dx(t)
dt
= gξ(x(t), u(t), t)
d
dtS(t) = ∂gξ(x(t), u(t), t)
∂x(t)
S(t) + ∂gξ(x(t), u(t), t)
∂ξ
d
dtS0(t) = ∂gξ(x(t), u(t), t)
∂x(t)
S0(t)
(E.5)
in order to obtain the solutions for S(t) and S0(t).
This eventually allows us to compute the geometric tensor for the JAK2-STAT5 system:
For z(t) = (z1(t), . . . , z6(t))⊤and ξ = (k1, k2, k3, k4, τ, k′
5, k′
6)⊤let
dz(t)
dt
= gξ(z(t), Epo(t))
be the ODE system deﬁned by (E.3) with y1(t) = k′
5(z2(t) + 2z3(t)) and y2(t) =
k′
6(z1(t) + z2(t) + 2z3(t)) intrinsically dependent on the parameter vector ξ. Using the
covariance matrices Σ1 = diag(σ2
1,1, . . . , σ2
16,1), Σ2 = diag(σ2
1,2, . . . , σ2
16,2) along with the
vectors v1 = (yε
1(t1)−y1(t1), . . . , yε
1(t16)−y1(t16))⊤, v2 = (yε
2(t1)−y2(t1), . . . , yε
2(t16)−
y2(t16))⊤we can rewrite the posterior (E.1) as
π(ξ|y) ∝
2
Y
i=1
Φ16(vi|Σi) · π(ξ),
for the pdf’s Φ16(·|Σi) of the 16 dimensional normal distribution N16(0, Σi). Deﬁning
vi
j := ∂vj
∂ξi the partial derivative of the log-likelihood log(L(ξ|y)) = log
 2Q
i=1
Φ16(vi|Σi)

177

E. GEOMETRIC TENSOR FOR THE JAK2-STAT5 DDE SYSTEM
with respect to the ith parameter ξi computes to
∂
∂ξi
log(L(ξ|y)) = ∂
∂ξi
2
X
j=1

−1
2v⊤
j Σ−1
j vj

=
2
X
j=1

−1
2(vi
j)⊤Σ−1
j vj −1
2v⊤
j Σ−1
j vi
j

= −
n
X
j=1
(vi
j)⊤Σ−1
j vj
It is easy to see that (vi
j)⊤Σ−1
j vj = v⊤
j Σ−1
j vi
j. According to Chapter 5.1 the (i, j)th
element (i, j = 1, . . . , 7) of the geometric tensor is given as
Gi,j(ξ) = cov
 ∂
∂ξi
log(L(ξ|y))⊤, ∂
∂ξj
log(L(ξ|y))⊤

−
∂2
∂ξi∂ξj
log(π(ξ))
= cov
" 2
X
k=1
(vi
k)⊤Σ−1
k vk,
2
X
l=1
v⊤
l Σ−1
l
vj
l
#
−
∂2
∂ξi∂ξj
log(π(ξ))
=
2
X
k=1
(vi
k)⊤Σ−1
k vj
k −
∂2
∂ξi∂ξj
log(π(ξ))
(E.6)
(see Girolami & Calderhead [2011]).
The kth element of vi
j is ∂yj(tk)
∂ξi
, where i = 1, . . . , 7 is the parameter index, j = 1, 2 the
species index, k = 1, . . . , 16 the time index, and yj(t) the solution to the ODE system
(E.3). Setting vi
j(t) := ∂yj(t)
∂ξi
we now have for i = 1, . . . , 7
dvi
1(t)
dt
=
6
X
l=1

∂
∂zl(t)
dy1(t)
dt
 ∂zl(t)
∂ξi

+ ∂
∂ξi
dy1(t)
dt
=
6
X
l=1
∂k′
5(k1z1(t)Epo(t) −2k3z3(t))
∂zl(t)
∂zl(t)
∂ξi

+ ∂k′
5(k1z1(t)Epo(t) −2k3z3(t))
∂ξi
dvi
2(t)
dt
=
6
X
l=1

∂
∂zl(t)
dy2(t)
dt
 ∂zl(t)
∂ξi

+ ∂
∂ξi
dy2(t)
dt
=
6
X
l=1
∂k′
6(2k4z6(t) −2k3z3(t))
∂zl(t)
∂zl(t)
∂ξi

+ ∂k′
6(2k4z6(t) −2k3z3(t))
∂ξi
.
(E.7)
with vi
1(0) = 0 for all i, vi
2(0) = 0 for i = 1, 3, 4, 5, 6, v2
2(0) = k′
6, and v7
2(0) = k2 which
can be seen by straightforward application of the deﬁnition of y1(t) and y2(t). The
178

expressions ∂zl(t)
∂ξi
can be computed via the sensitivity equations
d
dt
∂zl(t)
∂ξi
=
6
X
m=1
∂gξ(z(t), Epo(t))
∂zm(t)
 ∂zm(t)
∂ξi

+ ∂gξ(z(t), Epo(t))
∂ξi(t)
for i = 1, 3, . . . , 7, l = 1, . . . , 6
d
dt
∂zl(t)
∂ξ2
=
6
X
m=1
∂gξ(z(t), Epo(t))
∂zm(t)
 ∂zm(t)
∂ξ2

for l = 1, . . . , 6
∂z2(0)
∂ξ2
= ∂zl(0)
∂k2
= 1
∂zl(0)
∂ξi
= 0
for i = 1, . . . , 7, l = 1, . . . , 6, (i, j) ̸= (2, 2).
(E.8)
Hence, vi
j can be numerically computed by means of the extended ODE system (E.3),
(E.7), and (E.8). We used Matlab’s ode15s solver for the solution of the extended ODE
system, which is faster than the dde23 solver applied for the RWMH, IMH, CovRWMH,
M-GaA, CIMH, and ACIMH algorithms. SMALA might thus have a slight advantage
in speed when solving the diﬀerential equation systems (D.1).
For the second term of the right hand side of Equation (E.6) we have due to
k1, k2, k4, τ, k5, k6
i.i.d.
∼U[0, 50] and k3 ∼U[k4, 50] that
∂2
∂ξi∂ξj
log(π(ξ)) = 0
for all i, j except i = j = 4.
On the other hand
∂2
∂k4∂k4
log(π(ξ)) = −6 log(50)
∂2
∂k4∂k4
log(π(k3|k4))
= −6 log(50)
∂2
∂k4∂k4
log

1
50 −k4

= −6 log(50) ∂
∂k4
1
50 −k4
= 6 log(50) ∂
∂k4
1
(50 −k4)2
179

E. GEOMETRIC TENSOR FOR THE JAK2-STAT5 DDE SYSTEM
180

Appendix F
Parameters for prior
distributions of the zirconium
models
The prior distributions of the HMGU and ICRP model were computed in Li et al.
[2011a]. Since estimated conﬁdence intervals as well as estimated medians were provided
only, we need to infer the location (µ) and scale (σ) parameters for all lognormal
distributions LN(µ, σ). Furthermore, the means µ and standard deviations σ for all
normal distributions N(µ, σ) need to be computed. The formulas are derived in the
following.
Location and scale parameters for the lognormal distribution given the
estimated median and geometric standard deviation
Suppose we are given the estimation ˆm of the median m of a univariate lognormal
distribution LN(µ, σ). According to Johnson et al. [1994], m = exp(µ) and therefore
µ = log(m) ≈log( ˆm).
Furthermore, the geometric standard deviation (GSD) is provided for all lognormally
distributed parameters and is either GSD = 2 or GSD = 3. Since we have GSD =
181

F. PARAMETERS FOR PRIOR DISTRIBUTIONS OF THE
ZIRCONIUM MODELS
exp(σ) in case of a lognormal distribution, this naturally yields for the scale parameter:
σ = ln(2) or σ = ln(3)
Mean and standard deviation for the normal distribution given the
estimated median and the coeﬃcient of variation
Since for the normal distribution N(µ, σ) the mean and median coincide, we do not
further distinguish between the two of them and simply denote their estimates by ˆµ.
Clearly, we have
µ ≈ˆµ.
Also, for the normally distributed parameters, we are given a coeﬃcient of variation of
cV = 0.3. Since cV = σ
µ, we obtain
σ = 0.3µ.
182

Appendix G
Investigation speciﬁc time
courses for the ICRP and
HMGU models
We here present the investigation speciﬁc time courses (TC) based on the posterior
samples.
Depicted are the time courses for the transfer compartment and for the
excretion rate in the urine compartment together with the corresponding data. The
black lines are the posterior median solutions of the time courses, while the shaded areas
denote the 90 % posterior credible intervals. In addition, the investigation speciﬁc zero-
truncated measurement errors, ﬁtted by simulated annealing, are depicted. Note that
neither the upper nor the lower conﬁdence bound, nor the median needs to represent
a solution to the according ordinary diﬀerential equation. While plasma time courses
are generally covered well by both models, especially the ICRP model struggles from
time to time with the urinary data. The coloring corresponds to the coloring of the
individuals in Figure 8.2 and Figure 8.5.
183

G. INVESTIGATION SPECIFIC TIME COURSES FOR THE ICRP
AND HMGU MODELS
184

185

G. INVESTIGATION SPECIFIC TIME COURSES FOR THE ICRP
AND HMGU MODELS
186

References
Aaronson, D. & Horvath, C. (2002). A road map for those who don’t know JAK-STAT.
Science, 296, 1653 – 1655. 38, 119, 129
Aas, K., Czado, C., Frigessi, A. & Bakken, H. (2009). Pair-copula constructions of
multiple dependence. Insurance, Mathematics and Economics, 44, 182–198. 5, 19, 21, 22,
24, 95, 99, 106, 163
Alberts, B., Johnson, A., Lewis, J., Raff, M., Roberts, K. & Walter, P. (2002).
Molecular Biology of the Cell, vol. 4. Garland Science. 36, 139
Amari, S. & Nagaoka, H. (2007). Methods of information geometry, vol. 191. American
Mathematical Society. 87
Anderson, R. & May, R. (1992). Infectious diseases of humans: dynamics and control,
vol. 26. Wiley Online Library. 45
Arbenz, P. (2011). Bayesian copulae distributions, with application to operational risk man-
agement – some comments. Methodology and Computing in Applied Probability, 1–4. 19
Bartlett, M. (1966). An introduction to stochastic processes. Cambridge University Press.
76
Bartolucci, F., Scaccia, L. & Mira, A. (2006). Eﬃcient Bayes factor estimation from the
reversible jump output. Biometrika, 93, 41–52. 82
Bedford, T. & Cooke, R. (2001). Probability density decomposition for conditionally depen-
dent random variables modeled by vines. Annals of Mathematics and Artiﬁcial Intelligence,
32, 245–268. 22
Bedford, T. & Cooke, R. (2002). Vines – a new graphical model for dependent random
variables. Annals of Statistics, 30, 1031–1068. 22
Beichl, I. & Sullivan, F. (2000). The Metropolis algorithm. Computing in Science & Engi-
neering, 2, 65–69. 2, 69
Berger, J. (1985). Statistical decision theory and Bayesian analysis. Springer. 52
187

REFERENCES
Bernardo, J., Smith, A. & Berliner, M. (1994). Bayesian theory, vol. 62. Wiley. 52, 55
BMU (2007). Richtlinie f¨ur die physikalische Strahlenschutzkontrolle zur Ermittlung der
K¨orperdosis. Teil 2: Ermittlung der K¨orperdosis bei innerer Strahlenexposition (Inkorpo-
rations¨uberwachung) (§§40, 41 und 42 StrlSchV). Bonn: Bundesministerium f¨ur Umwelt,
Naturschutz und Reaktorsicherheit. 153
Bohl, S. (2009). Dynamic modeling of signal processing for IL-6-induced STAT3 signal trans-
duction in primary mouse hepatocytes. Ph.D. thesis, Ruperto-Carola University of Heidel-
berg, Germany. 128, 129
Bonizzi, G. & Karin, M. (2004). The two nf-κb activation pathways and their role in innate
and adaptive immunity. Trends in immunology, 25, 280–288. 127
Bornholdt, S. (2008). Boolean network models of cellular regulation: prospects and limita-
tions. Journal of the Royal Society Interface, 5, S85–S94. 43
Brechmann, E. (2010). Truncated and simpliﬁed regular vines and their applications, diploma
thesis, Technische Universit¨at M¨unchen, Germany. 99
Brechmann, E. & Schepsmeier, U. (2011). Dependence modeling with C- and D-vine cop-
ulas: The R-package CDVine, Preprint (http://www-m4.ma.tum.de/Papers/index.html). 99,
106, 164
Brooks, S. (1998). Markov chain Monte Carlo method and its application. Journal of the
Royal Statistical Society: Series D (The Statistician), 47, 69–100. 2
Brooks, S. & Gelman, A. (1998). General methods for monitoring convergence of iterative
simulations. Journal of Computational and Graphical Statistics, 434–455. 79
Brown, K. & Sethna, J. (2003). Statistical mechanical approaches to models with many
poorly known parameters. Physical Review E, 68, 021904–1–021904–9. 1
Calderhead, B. & Girolami, M. (2009). Estimating Bayes factors via thermodynamic
integration and population MCMC. Computational Statistics & Data Analysis, 53, 4028–
4045. 63, 65, 66, 133
Cao, Y., Li, H. & Petzold, L. (2004). Eﬃcient formulation of the stochastic simulation
algorithm for chemically reacting systems. The journal of chemical physics, 121, 4059. 41
Casella, G. & Berger, R. (2001). Statistical inference. Duxbury Press. 56
ˇCern`y, V. (1985). Thermodynamical approach to the traveling salesman problem: An eﬃcient
simulation algorithm. Journal of Optimization Theory and Applications, 45, 41–51. 47
Chen, K., Calzone, L., Csikasz-Nagy, A., Cross, F., Novak, B. & Tyson, J. (2004).
Integrative analysis of cell cycle control in budding yeast. Molecular Biology of the Cell, 15,
3841–3862. 43
188

REFERENCES
Chung, K. (1982). Lectures from Markov processes to Brownian motion. Springer-Verlag. 87
Czado, C. (2010). Pair-copula constructions of multivariate copulas. In P. Jaworki, F. Durante,
W. H¨ardle & T. Rychlik, eds., Copula Theory and its Applications, 93–110, Springer-Verlag.
99
D., K.Z.Y.P., Shaw, B., Kou, B., McAuley, K. & Bacon, D. (2003). Modeling ethy-
lene/butene copolymerization with multi-site catalysts: parameter estimability and experi-
mental design. Polymer Reaction Engineering, 11, 563–588. 50
Dargatz, C. (2010). Bayesian Inference for Diﬀusion Processes with Applications in Life
Sciences. Ph.D. thesis, Ludwig-Maximilians-Universit¨at M¨unchen, Germany. 28, 29, 42
Davison, A. (2003). Statistical models, vol. 11. Cambridge University Press. 121
De Jong, H. (2002). Modeling and simulation of genetic regulatory systems: a literature
review. Journal of Computational Biology, 9, 67–103. 38, 42
Diestel, R. (2000). Graph theory (graduate texts in mathematics vol 173). 23
Dißmann, J., Brechmann, E., Czado, C. & Kurowicka, D. (2011). Selecting and esti-
mating regular vine copulae and application to ﬁnancial returns. Preprint. 106
Duane, S., Kennedy, A., Pendleton, B. & Roweth, D. (1987). Hybrid Monte Carlo.
Physics letters B, 195, 216–222. 3
Eidgen¨ossisches Nuklearsicherheitsinspektorat Informationsdienst (2011). Radiol-
ogische Auswirkungen aus den kerntechnischen Unf¨allen in Fukushima vom 11.3.2011. Eid-
gen¨ossisches Nuklearsicherheitsinspektorat Informationsdienst. 137
Evett, I. (1991). Implementing bayesian methods in forensic science. In Fourth Valencia In-
ternational Meeting on Bayesian Statistics. 59
Fall, C. (2002). Computational cell biology, vol. 20. Springer-Verlag. 176
Fearnhead, P. (2008). Editorial: Special issue on adaptive Monte Carlo methods. Statistics
and Computing, 18, 341–342. 101
Fletcher, R. (1987). Practical methods of optimization, volume 1. Wiley. 47
Fraser, A. & Burnell, D. (1970). Computer models in genetics. McGraw-Hill Book Com-
pany. 47
Friel, N. & Pettitt, A. (2008). Marginal likelihood estimation via power posteriors. Journal
of the Royal Statistical Society: Series B (Statistical Methodology), 70, 589–607. 60, 63, 65,
66
Gamerman, D. & Lopes, H. (2006). Markov chain Monte Carlo: stochastic simulation for
Bayesian inference, vol. 68. Chapman & Hall/CRC. 2, 60
189

REFERENCES
Gelman, A. & Meng, X. (1998). Simulating normalizing constants: From importance sam-
pling to bridge sampling to path sampling. Statistical Science, 163–185. 63, 159
Gelman, A. & Rubin, D. (1992). Inference from iterative simulation using multiple sequences.
Statistical Science, 7, 457–472. 79
Gelman, A., Roberts, G. & Gilks, W. (1996). Eﬃcient Metropolis jumping rules. Bayesian
statistics, 5, 599–608. 78
Geweke, J. (1992). Evaluating the Accuracy of Sampling-Based Approaches to the Calculation
of Posterior Moments. Oxford University Press. 79
Geyer, C. (1992). Practical Markov chain Monte Carlo. Statistical Science, 7, 473–483. 80
Geyer, C. & Møller, J. (1994). Simulation procedures and likelihood inference for spatial
point processes. Scandinavian Journal of Statistics, 359–373. 80
Gibson, M. & Bruck, J. (2000). Eﬃcient exact stochastic simulation of chemical systems
with many species and many channels. The journal of physical chemistry A, 104, 1876–1889.
41
Gilks, W., Roberts, G. & Sahu, S. (1998). Adaptive Markov chain Monte Carlo through
regeneration. Journal of the American Statistical Association, 1045–1054. 3, 89
Gillespie, D. (1992). A rigorous derivation of the chemical master equation. Physica A: Sta-
tistical Mechanics and its Applications, 188, 404–425. 40, 41, 42
Gillespie, D. (2007). Stochastic simulation of chemical kinetics. Annual Review of Physical
Chemistry, 58, 35–55. 39
Girolami, M. & Calderhead, B. (2011). Riemann manifold Langevin and Hamiltonian
Monte Carlo methods. Journal of the Royal Statistical Society: Series B (Statistical Method-
ology), 73, 123–214. 3, 5, 86, 88, 103, 178
Goodwin, B. (1963). Temporal organization in cells. Academic Press New York. 43
Green, P. (1995). Reversible jump Markov chain Monte Carlo computation and Bayesian
model determination. Biometrika, 82, 711–732. 4, 80, 81
Greiter, M., Giussani, A., H¨ollriegl, V., Li, W. & Oeh, U. (2011). Human biokinetic
data and a new compartmental model of zirconium – a tracer study with enriched stable
isotopes. Science of the Total Environment, 409, 3701–3710. 5, 138, 139
Grenander, U. & Miller, M. (1994). Representations of knowledge in complex systems.
Journal of the Royal Statistical Society. Series B (Methodological), 549–603. 85
Guyton, A. & Hall, J. (2006). Textbook of Medical Physiology, vol. 11. Elsevier Saunders.
137
190

REFERENCES
Haario, H., Saksman, E. & Tamminen, J. (1999). Adaptive proposal distribution for ran-
dom walk Metropolis algorithm. Computational Statistics, 14, 375–396. 3, 90
Haario, H., Saksman, E. & Tamminen, J. (2001). An adaptive Metropolis algorithm.
Bernoulli, 223–242. 3, 90, 92
Hastie, T., Tibshirani, R. & Friedman, J.J.H. (2009). The elements of statistical learning.
Springer-Verlag. 80, 93
Hastings, W. (1970). Monte Carlo sampling methods using Markov chains and their applica-
tions. Biometrika, 57, 97–109. 2, 69, 70
Hengl, S., Kreutz, C., Timmer, J. & Maiwald, T. (2007). Data-based identiﬁability
analysis of non-linear dynamical models. Bioinformatics, 23, 2612. 50
Hobæk Haff, I. (2010). Parameter estimation for pair-copula constructions. Preprint. 99
Hobæk Haff, I., Aas, K. & Frigessi, A. (2010). On the simpliﬁed pair-copula construction–
simply useful or too simplistic? Journal of Multivariate Analysis, 101, 1296–1310. 21
Holden, L. (2000). Convergence of Markov chains in the relative supremum norm. Journal of
Applied Probability, 37, 1074–1083. 97
Holden, L., Hauge, R. & Holden, M. (2009). Adaptive independent Metropolis-Hastings.
The Annals of Applied Probability, 19, 395–413. 3, 89, 98
Horbelt, W., Timmer, J. & Voss, H. (2002). Parameter estimation in nonlinear delayed
feedback systems from noisy data. Physics Letters A, 299, 513–521. 1, 47
Hou, S., Zheng, Z., Chen, X. & Perrimon, N. (2002). The JAK/STAT pathway in model
organisms: Emerging roles in cell movement. Developmental Cell, 3, 765–778. 119
Hu, L. (2006). Dependence patterns across ﬁnancial markets: a mixed copula approach. Applied
Financial Economics, 16, 717–729. 126
ICRP (1975). ICRP publication 23. Report on the task group on reference man. Annals of the
ICRP. 137, 138, 139
ICRP (1979). ICRP publication 30. Limits for intakes of radionuclides by workers (part 1).
Annals of the ICRP, 8. 137
ICRP (1988). ICRP publication 53. Radiation dose to patients from radiopharmaceuticals.
Annals of the ICRP, 18. 137
ICRP (1989). ICRP publication 56. Age-dependent doses to members of the public from intake
of radionuclides (part 1: Ingestion dose coeﬃcients). Annals of the ICRP, 20. 137, 138, 139
ICRP (1993). ICRP publication 67. Age-dependent doses to members of the public from intake
of radionuclides (part 2: Ingestion dose coeﬃcients). Annals of the ICRP, 23. 139
191

REFERENCES
ICRP (1998). ICRP publication 78. Individual monitoring for internal exposure of workers.
Annals of the ICRP, 27. 137, 152
ICRP (2007). ICRP publication 103. The 2007 Recommendations of the International Com-
mission on Radiological Protection. Annals of the ICRP, 37, 2. 137
ICRP (2008). ICRP publication 107. Nuclear decay data for dosimetric calculations. Annals of
the ICRP, 38. 146, 154
Igaz, P., Toth, S. & Falus, A. (2001). Biological and clinical signiﬁcance of the JAK-STAT
pathway; lessons from knockout mice. Inﬂammation Research, 50, 435–441. 37
Jacquez, J. (1985). Compartmental analysis in biology and medicine. University of Michigan
Press Ann Arbor, MI. 45
Jefferys, W. & Berger, J. (1992). Ockham’s razor and Bayesian analysis. American Sci-
entist, 80, 64–72. 60
Jeffreys, H. (1961). Theory of probability. Clarendon Press. 59
Joe, H. (1996). Families of m-variate distributions with given margins and m(m-1)/2 bivari-
ate dependence parameters. In L. R¨uschendorf and B. Schweizer and M. D. Taylor, ed.,
Distributions with Fixed Marginals and Related Topics, vol. 28. 19, 21
Joe, H. (1997). Multivariate Models and Dependence Concepts. Chapman & Hall. 16
Joe, H., Li, H. & Nikoloulopoulos, A. (2010). Tail dependence functions and vine copulas.
Journal of Multivariate Analysis, 101, 252–270. 22
Johnson, N., Kotz, S. & Balakrishnan, N. (1994). Continuous univariate distributions,
vol. 1. 181
Kaplan, S., Bren, A., Dekel, E. & Alon, U. (2008). The incoherent feed-forward loop
can generate non-monotonic input functions for genes. Molecular Systems Biology, 4, 1–9.
46
Kass, R. (1993). Probabilistic inference using Markov chain Monte Carlo methods. Technical
Report. 76
Kass, R. & Raftery, A. (1995). Bayes factors. Journal of the American Statistical Associa-
tion, 773–795. 57, 58, 59, 60
Kass, R., Carlin, B., Gelman, A. & Neal, R. (1998). Markov chain Monte Carlo in
practice: A roundtable discussion. American Statistician, 52, 93–100. 76
Kauffman, S. (1969). Metabolic stability and epigenesis in randomly constructed genetic nets.
Journal of Theoretical Biology, 22, 437–467. 43
192

REFERENCES
Kim, G., Silvapulle, M. & Silvapulle, P. (2007). Comparison of semiparametric and
parametric methods for estimating copulas. Computational Statistics & Data Analysis, 51,
2836–2850. 126
Kirkpatrick, S., Gelatt, C.D. & Vecchi, M.P. (1983). Optimization by simulated an-
nealing. Science, 220, 671–680. 4, 47, 84
Klenke, A. (2008). Wahrscheinlichkeitstheorie. Springer-Verlag. 9
Kowarsch, A. (2011). The impact of microRNAs on signaling pathways: From general per-
spectives to a computational model of the JAK-STAT pathway. Ph.D. thesis, Technische
Universit¨at M¨unchen, Germany. 37
Kullback, S. & Leibler, R. (1951). On information and suﬃciency. The Annals of Mathe-
matical Statistics, 22, 79–86. 65
Kurowicka, D. & Cooke, R. (2006a). Completion problem with partial correlation vines.
Linear Algebra and its Applications, 418, 188–200. 5
Kurowicka, D. & Cooke, R. (2006b). Uncertainty Analysis with High Dimensional Depen-
dence Modelling. Wiley. 24
Kurowicka, D. & Joe, H. (2011). Dependence Modeling - Handbook on Vine Copulae. World
Scientiﬁc Publishing Co. 5, 99
Lartillot, N. & Philippe, H. (2006). Computing Bayes factors using thermodynamic inte-
gration. Systematic biology, 55, 195–207. 60, 63
Lawrence, N., Girolami, M., Rattray, M. & Sanguinetti, G. (2010). Learning and
Inference in Computational Systems Biology. The MIT Press. 1
Lecourtier, Y., Lamnabhi-Lagarrigue, F. & Walter, E. (1987). Volterra and generating
power series approaches to identiﬁability testing. Identiﬁability of parametric models, 50–66.
50
Lewis, S. (1994). Multilevel modeling of discrete event history data using Markov chain Monte
Carlo methods. Ph.D. thesis, University of Washington, USA. 60
Li, S., Brazhnik, P., Sobral, B. & Tyson, J. (2008). A quantitative study of the division
cycle of Caulobacter crescentus stalked cells. PLoS Comput Biol, 4, e9. 43
Li, W., Greiter, M., Oeh, U. & Hoeschen, C. (2011a). Reliability of a new biokinetic
model of zirconium in internal dosimetry. Part I. Parameter uncertainty analysis. Health
Physics, 101, 660. 115, 138, 143, 144, 145, 154, 181
Li, W., Greiter, M., Oeh, U. & Hoeschen, C. (2011b). Reliability of a new biokinetic
model of zirconium in internal dosimetry. Part II. Parameter sensitivity analysis. Health
Physics, 101, 677. 115, 138, 143, 147
193

REFERENCES
Liu, J. (2008). Monte Carlo Strategies in Scientiﬁc Computing. Springer-Verlag. 85, 103, 159
Ljung, L. & Glad, T. (1994). On global identiﬁability for arbitrary model parametrizations.
Automatica, 30, 265–276. 50
Lodewyckx, T., Kim, W., Lee, M., Tuerlinckx, F., Kuppens, P. & Wagenmakers,
E. (2011). A tutorial on Bayes factor estimation with the product space method. Journal of
Mathematical Psychology, 55. 60
Maiwald, T. & Timmer, J. (2008). Dynamical modeling and multi-experiment ﬁtting with
potterswheel. Bioinformatics, 24, 2037–2043. 47, 49
Marin, J. & Robert, C. (2007). Bayesian core:
a practical approach to computational
Bayesian statistics. Springer-Verlag. 54, 55
Meeker, W. & Escobar, L. (1995). Teaching about approximate conﬁdence regions based
on maximum likelihood estimation. American Statistician, 48–53. 48, 56
Metropolis, N., Rosenbluth, A., Rosenbluth, M., Teller, A., Teller, E. et al.
(1953). Equation of state calculations by fast computing machines. The Journal of Chemical
Physics, 21, 1087–1092. 2, 69, 70
Meyn, S., Tweedie, R., Glynn, P. & Corporation, E. (1996). Markov chains and stochas-
tic stability. Springer-Verlag. 26, 29
Min, A. & Czado, C. (2010). Bayesian inference for multivariate copulas using pair-copula
constructions. Journal of Financial Econometrics, 8(4), 511–546. 99
Min, A. & Czado, C. (2011). Bayesian model selection for multivariate copulas using pair-
copula constructions. Canadian Journal of Statistics, 39, 239–258. 99
Mitra, D., Romeo, F. & Sangiovanni-Vincentelli, A. (1986). Convergence and ﬁnite-
time behavior of simulated annealing. Advances in Applied Probability, 747–771. 84
Morales-N´apoles, O., Cooke, R. & Kurowicka, D. (2010). About the number of vines
and regular vines on n nodes. Submitted for publication. 24
M¨uller, C. & Sbalzarini, I. (2010). Gaussian Adaptation as a unifying framework for con-
tinuous black-box optimization and adaptive Monte Carlo sampling. In Evolutionary Com-
putation (CEC), 2010 IEEE Congress on, 1–8, IEEE. 3, 5, 90, 91
Murphy, S. & Van der Vaart, A. (2000). On proﬁle likelihood. Journal of the American
Statistical Association, 449–465. 48
Myung, I. & Pitt, M. (1997). Applying Occam’s razor in modeling cognition: A Bayesian
approach. Psychonomic Bulletin & Review, 4, 79–95. 60
Neal, R. (1993). Probabilistic inference using Markov chain Monte Carlo methods. Tech. Rep.
CRG-TR-93-1, University of Toronto. Department of Computer Science. 76
194

REFERENCES
Neal,
R.
(2008).
The
Harmonic
Mean
of
the
Likelihood:
Worst
Monte
Carlo
Method
Ever,
http://radfordneal.wordpress.com/2008/08/17/
the-harmonic-mean-of-the-likelihood-worst-monte-carlo-method-ever. 61
Nelsen, R. (2006). An Introduction to Copulas. Springer. 16, 17, 18
Newton, M. & Raftery, A. (1994). Approximate Bayesian inference with the weighted like-
lihood bootstrap. Journal of the Royal Statistical Society: Series B (Statistical Methodology),
56, 3–48. 60, 61, 62
Nikolov, S., Kotev, V. & Petrov, V. (2007). Stability analysis of a time delay model for
the JAK-STAT signaling pathway. Series on Biomechanics, 23, 52–65. 176
Nummelin, E. (2004). General irreducible Markov chains and non-negative operators, vol. 83.
Cambridge University Press. 33, 71, 90
Øksendal, B. (2003). Stochastic diﬀerential equations: an introduction with applications.
Springer-Verlag. 28, 42
Palsson, B. (2006). Systems Biology: Properties of Reconstructed Networks. Cambridge Uni-
versity Press. 38
Papoulis, A., Pillai, S. & Unnikrishna, S. (1965). Probability, random variables, and
stochastic processes, vol. 196. McGraw-Hill. 74
Pitt, M., Myung, I. & Zhang, S. (2002). Toward a method of selecting among computational
models of cognition. Psychological Review, 109, 472. 60
Press, W., Flannery, B., Teukolsky, S., Vetterling, W. et al. (1986). Numerical
recipes, vol. 547. Cambridge Univ Press. 48
Raia, V., Schilling, M., B¨ohm, M., Hahn, B., Kowarsch, A., Raue, A., Sticht, C.,
Bohl, S., Saile, M., M¨oller, P. et al. (2011). Dynamic mathematical modeling of IL13-
induced signaling in Hodgkin and primary mediastinal B-cell lymphoma allows prediction of
therapeutic targets. Cancer research, 71, 693. 43
Ramsay, J., Hooker, G., Campbell, D. & Cao, J. (2007). Parameter estimation for
diﬀerential equations: a generalized smoothing approach. Journal of the Royal Statistical
Society: Series B (Statistical Methodology), 69, 741–796. 3
Rao, C. (1945). Information and accuracy attainable in the estimation of statistical parameters.
Bulletin Calcutta Mathematical Society, 37, 81–91. 86
Raue, A., Kreutz, C., Maiwald, T., Bachmann, J., Schilling, M., Klingm¨uller, U.
& Timmer, J. (2009). Structural and practical identiﬁability analysis of partially observed
dynamical models by exploiting the proﬁle likelihood. Bioinformatics, 25, 1923–1929. 48,
121
195

REFERENCES
Revuz, D. (1984). Markov chains, vol. 11. North Holland. 34
Ripley, B. (1977). Modelling spatial patterns. Journal of the Royal Statistical Society: Series
B (Statistical Methodology), 172–212. 80
Robert, C. & Casella, G. (2004). Monte Carlo statistical methods. Springer-Verlag. 26, 30,
51, 55, 71, 82, 84
Roberts, G. & Rosenthal, J. (2007). Coupling and ergodicity of adaptive Markov chain
Monte Carlo algorithms. Journal of Applied Probability, 44, 458–475. 3, 88, 90, 99
Roberts, G. & Stramer, O. (2002). Langevin diﬀusions and Metropolis-Hastings algorithms.
Methodology and computing in applied probability, 4, 337–357. 3, 86
Roberts, G., Gelman, A. & Gilks, W. (1997). Weak convergence and optimal scaling of
random walk Metropolis algorithms. The Annals of Applied Probability, 7, 110–120. 103, 104
Rosenthal, J. (2011). Optimal proposal distributions and adaptive MCMC. Chapman &
Hall/CRC Press. 3, 88
Schervish, M.J. (1995). Theory of statistics. Springer-Verlag. 87
Schilling, M., Maiwald, T., Bohl, S., Kollmann, M., Kreutz, C., Timmer, J. &
Klingm¨uller, U. (2005). Computational processing and error reduction strategies for stan-
dardized quantitative data in biological networks. FEBS Journal, 272, 6400–6411. 128
Schmidl, D., Czado, C. & Theis, F. (2012a). A vine copula based adaptive MCMC sampler
for eﬃcient inference of dynamical systems. Bayesian Analysis, under revision. 5
Schmidl, D., Hug, S., Li, W., Greiter, M. & Theis, F. (2012b). Bayesian model selection
validates a biokinetic model for zirconium processing in humans. BMC Systems Biology, 6,
95. 6
Serban, R. & Hindmarsh, A. (2005). Cvodes: the sensitivity-enabled ode solver in Sundials.
In Proceedings of IDETC/CIE, vol. 24. 45
Sethuraman, J., Athreya, K. & Ddoss, H. (1992). A Proof of convergence of the Markov
Chain simulation Method. Technical Report 868. 34
Shampine, L. & Reichelt, M. (1997). The Matlab ode suite. SIAM journal on scientiﬁc
computing, 18, 1–22. 44
Shampine, L. & Thompson, S. (2001). Solving DDEs in Matlab. Applied Numerical Mathe-
matics, 37, 441–458. 45
Sklar, A. (1959). Fonctions d´e repartition ´a n dimensions et leurs marges. Publ. Inst. Stat.
Univ. Paris, 8, 229–231. 16
196

REFERENCES
Smith, M., Min, A., Almeida, C. & Czado, C. (2010). Modeling longitudinal data us-
ing a pair-copula construction decomposition of serial dependence. Journal of the American
Statistical Association, 105, 1467–1479. 99
Spiegelhalter, D. & Smith, A. (1982). Bayes factors for linear and log-linear models with
vague prior information. Journal of the Royal Statistical Society: Series B (Statistical Method-
ology), 377–387. 60
Subramaniam, P., Torres, B. & Johnson, H. (2001). So many ligands, so few transcription
factors: a new paradigm for signaling through the STAT transcription factors. Cytokine, 15,
175–187. 37
Suykens, J. & Vandewalle, J. (2002). Coupled local minimizers: alternative formulations
and extensions. In Neural Networks, 2002. IJCNN’02. Proceedings of the 2002 International
Joint Conference on, vol. 3, 2039–2043, IEEE. 47
Suykens, J., Vandewalle, J. & De Moor, B. (2002). Intelligence and cooperative search
by coupled local minimizers. Arxiv Preprint cs/0210030. 47
Swameye, I., M¨uller, T., Timmer, J., Sandra, O. & Klingm¨uller, U. (2003). Iden-
tiﬁcation of nucleocytoplasmic cycling as a remote sensor in cellular signaling by databased
modeling. Proceedings of the National Academy of Sciences of the United States of America,
100, 1028–1033. 102, 111, 120, 121, 125, 157
Ter Braak, C. & Vrugt, J. (2008). Diﬀerential evolution Markov chain with snooker updater
and fewer chains. Statistics and Computing, 18, 435–446. 3
Theis, F. (2002). Mathematics in Independent Component Analysis. Ph.D. thesis, University
of Regensburg, Germany. 9
Thomas, R. (1991). Regulatory networks seen as asynchronous automata: a logical description.
Journal of Theoretical Biology, 153, 1–23. 43
Tierney, L. (1994). Markov chains for exploring posterior distributions. The Annals of Statis-
tics, 1701–1728. 26, 30, 33, 34
Timmer, J., M¨uller, T., Swameye, I., Sandra, O. & Klingm¨uller, U. (2004). Modeling
the nonlinear dynamics of cellular signal transduction. International Journal of Bifurcation
and Chaos, 14, 2069–2079. 121, 129, 173, 174
UNSCEAR (2008). Sources and Eﬀects of Ionizing Radiation, vol. 2. United Nations Publica-
tions. 137
Venzon, D. & Moolgavkar, S. (1988). A method for computing proﬁle-likelihood-based
conﬁdence intervals. Applied Statistics, 87–94. 48
Vuong, Q. (1989). Likelihood ratio tests for model selection and non-nested hypotheses. Econo-
metrica: Journal of the Econometric Society, 307–333. 57
197

REFERENCES
Wen, Z., Zhong, Z. & Darnell, J. (1995). Maximal activation of transcription by Stat1
and Stat3 requires both tyrosine and serine phosphorylation. Cell, 82, 241–250. 38, 127, 131
Wilkinson, D. (2006). Stochastic Modelling for Systems Biology. Chapman & Hall/CRC. 1,
26, 39, 69
Wilkinson, D. (2007). Bayesian methods in Bioinformatics and computational Systems Biol-
ogy. Brieﬁngs in Bioinformatics, 8, 109–116. 2
Wu, W., Wang, F. & Chang, M. (2008). Dynamic sensitivity analysis of biological systems.
BMC Bioinformatics, 9, S17. 177
198

Index
m-step transition kernel, 33
acceptance rate, 70
acyclic graph, 23
Adaptive CIMH (ACIMH), 101
adaptive MCMC, 88
Adaptive Metropolis algorithm (AM), 92
Adaptive Proposal algorithm (AP), 90
almost surely, 9
aperiodic transition kernel, 33
Archimedean copula, 18, 164
autocorrelation, 75
autocovariance, 75
Bayes
factor, 58
theorem, 52
Bayesian Information Criterion (BIC), 59
BB1 copula, 164
BB6 copula, 164
BB7 copula, 164
BB8 copula, 164
biochemical reactions, 38
biokinetic model, 138
bone retention, 152
Boolean network, 43
Brownian motion, see Wiener process
burn-in, 78
C-vine, 24
CDVine, 99
cellular signaling pathway, 37
Chapman-Kolmogorov equations, 74
chemical
Langevin equation, 42
master equation, 40
Clayton copula, 164
compartment model, 45
conditional
density function, 13
distribution function, 12
conjugate prior, 54
convergence statistics, 78
copula, 16
data, 95
decomposition, 96
Copula based Independence MH algorithm (CIMH),
94
covariance, 15
Covariance based RWMH (CovRWMH), 105
credible interval, 53
cumulative distribution function (cdf), 10
cycle, 23
D-vine, 23
delay diﬀerential equation, 42
density function, see probability density function
detailed balance condition, 31
diﬀerential equation, 42
diﬀusion process, 28
dimension matching condition, 81
distribution, 10
Doeblin condition, see strong Doeblin condition
dynamical system, 43
edge, 22
Eﬀective Sampling Size (ESS), 78
elliptical copula, 18, 163
equilibrium distribution, 32
ergodic Markov chain, 34
event, 9
expectation, 14
199

INDEX
expected
mean, 14
variance, 15
exponential distribution, 162
Fisher information matrix, 87
Frank copula, 164
gamma distribution, 162
Gaussian
copula, 163
distribution, 12
Gelman-Rubin statistic, 79
gene, 36
generator of a copula, 18
geometric tensor, 86
Geweke test, 79
graph, 22
Gumbel copula, 164
Harris recurrent Markov chain, 33
heavy-tailed independence proposal function, 94
HMGU model, 140
homogeneous Markov chain, 29
hyperparameter, 54
ICRP model, 140
identiﬁability w.r.t. the MAP, 56
identiﬁable parameter, 48
improper prior, 55
Independence chain Metropolis-Hastings algorithm
(IMH), 73
independence copula, 18, 164
independent
adaption algorithm, 100
identically distributed (i.i.d.), 14
random vector, 13
indicator function, 11
INEFﬁciency Factor (INEFF), 76
invariant distribution, 31
inversion method, 17
irreducible
Markov chain, 33
transition kernel, 33
JAK-STAT pathway, 37, 118
Jeﬀreys’ scale of evidence, 59
Joe copula, 164
Kendall’s τ, 15
Kullback-Leibler divergence, 65
Langevin diﬀusion, 85
law of mass action, 39
likelihood function, 52
linear chain trick, 175
link functions, 47
lognormal distribution, 161
Manifold MALA (MMALA), 87
marginal
density function, 12
distribution function, 12
likelihood, 52
Markov
chain, 29
process, 27
Markov Chain Monte Carlo (MCMC), 69
Maximum
A Posterior estimate (MAP), 53
Likelihood Estimator (MLE), 53
mean, 14
Metropolis Adjusted Langevin Algorithm (MALA),
86
Metropolis Gaussian Adaption algorithm (M-GaA),
91
Metropolis-Hastings acceptance probability, 70
model
averaging, 80
evidence, 52
inference, 51
selection, 56, 57
Monte Carlo integration, 70
node, 22
non-informative prior, 55
normal distribution, 12, 161
Ordinary Diﬀerential Equation (ODE), 42
parameter inference, 51
path, 23
200

INDEX
Pearson’s correlation matrix, 15
periodic transition kernel, 33
positive Harris recurrent Markov chains, 33
posterior
distribution, 52
harmonic mean estimate, 61
median solution, 114
odds ratio, 58
power posterior, 63
practical identiﬁability, 48
prerun, 95
prior
arithmetic mean estimate, 60
distribution, 52
odds ratio, 58
probability
density function (pdf), 11
measure, 9
space, 9
product, 39
proﬁle likelihood, 49
propensity function, 40
proper
atom, 89
prior, 55
proposal function, 70
protein, 36
R-Vine, 22
random
process, 26
variable, 10
vector, 10
Random Walk Metropolis-Hastings (RWMH), 72
rate constant, 40
reactant, 39
reaction rate, see rate constant
realization, 10, 27
recurrent Markov chain, 34
regenerating Markov chain, 90
regular vine, 23
regularity condition, 71
retrospective dosimetry, 137
Reversible Jump MCMC (RJMCMC), 81
reversible Markov chain, 31
rotated copula, 164
sample
autocorrelation function, 77
path, 27
scaling parameter, 72
Schwarz criterion, 58
sensitivity equations, 176
signaling pathway, see cellular signaling pathway
Simpliﬁed MMALA (SMALA), 88
simulated annealing algorithm, 84
Sklar’s theorem, 16
stabilized harmonic mean estimator, 62
star, 24
state change vector, 40
stationary
distribution, 31
process, 28
step size tuning parameter, 72
stochastic
process, 26
simulation algorithm, 41
Stochastic Diﬀerential Equation (SDE), 29
strong
Doeblin condition, 97
law of large numbers, 14
structural identiﬁability, 48
Student’s t copula, 163
thermodynamic
integration, 63
limit, 42
time-continuous random process, 26
time-discrete random process, 26
total variation norm, 15
transcription, 36
transcription factor, 36
transfer rates, 138
transition
equations, 45
kernel, 30
probability, 30
translation, 36
tree, 23
uniform distribution, 11
201

INDEX
uniformization, 95
vertex, see node
vine, 22
Wiener process, 27
zirconium, 137
202

