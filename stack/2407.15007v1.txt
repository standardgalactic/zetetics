Is Behavior Cloning All You Need?
Understanding Horizon in Imitation Learning
Dylan J. Foster
Microsoft Research
Adam Block
MIT
Dipendra Misra
Microsoft Research
Abstract
Imitation learning (IL) aims to mimic the behavior of an expert in a sequential decision making task
by learning from demonstrations, and has been widely applied to robotics, autonomous driving, and
autoregressive text generation. The simplest approach to IL, behavior cloning (BC), is thought to incur
sample complexity with unfavorable quadratic dependence on the problem horizon, motivating a variety
of different online algorithms that attain improved linear horizon dependence under stronger assumptions
on the data and the learner’s access to the expert.
We revisit the apparent gap between offline and online IL from a learning-theoretic perspective, with
a focus on general policy classes up to and including deep neural networks. Through a new analysis of
behavior cloning with the logarithmic loss, we show that it is possible to achieve horizon-independent
sample complexity in offline IL whenever (i) the range of the cumulative payoffs is controlled, and (ii)
an appropriate notion of supervised learning complexity for the policy class is controlled. Specializing
our results to deterministic, stationary policies, we show that the gap between offline and online IL is
not fundamental: (i) it is possible to achieve linear dependence on horizon in offline IL under dense
rewards (matching what was previously only known to be achievable in online IL); and (ii) without further
assumptions on the policy class, online IL cannot improve over offline IL with the logarithmic loss, even
in benign MDPs. We complement our theoretical results with experiments on standard RL tasks and
autoregressive language generation to validate the practical relevance of our findings.
1
Introduction
Imitation learning (IL) is the problem of emulating an expert policy for sequential decision making by
learning from demonstrations. Compared to reinforcement learning (RL), the learner in IL does not observe
reward-based feedback, and must imitate the expert’s behavior based on demonstrations alone; their objective
is to achieve performance close to that of the expert on an unobserved reward function.
Imitation learning is motivated by the observation that in many domains, demonstrating the desired behavior
for a task (e.g., robotic grasping) is simple, while designing a reward function to elicit the desired behavior
can be challenging. IL is also often preferable to RL because it removes the need for exploration, leading
to empirically reduced sample complexity and often much more stable training. Indeed, the relative ease
of applying IL (over RL methods) has led to extensive adoption, ranging from classical applications in au-
tonomous driving (Pomerleau, 1988) and helicopter flight (Abbeel and Ng, 2004) to contemporary works that
leverage deep learning to achieve state-of-the-art performance for self-driving vehicles (Bojarski et al., 2016;
Bansal et al., 2018; Hussein et al., 2017), visuomotor control (Finn et al., 2017; Zhang et al., 2018), navigation
(Hussein et al., 2018), and game AI (Ibarz et al., 2018; Vinyals et al., 2019). Imitation learning also offers
a conceptual framework through which to study autoregressive language modeling (Chang et al., 2023; Block
et al., 2024a), and a number of useful empirical insights have arisen as a result of this perspective. However,
a central challenge limiting broader real-world deployment is to understand and improve the reliability and
stability properties of algorithms that support general-purpose (deep/neural) function approximation.
In more detail, imitation learning algorithms can be loosely grouped into offline and online approaches.
Offline imitation learning algorithms only require access to a dataset of logged trajectories from the expert,
making them broadly applicable. The most widely used approach, behavior cloning, reduces imitation learning
1
arXiv:2407.15007v1  [cs.LG]  20 Jul 2024

0
200
400
Number of Trajectories
−0.10
−0.05
0.00
0.05
0.10
0.15
0.20
Expected Regret
MuJoCo
horizon 50
horizon 100
horizon 200
horizon 300
horizon 400
horizon 500
(a)
0
50
100
150
200
Number of Trajectories
0.05
0.10
0.15
0.20
0.25
0.30
Expected Regret
Atari
horizon 50
horizon 100
horizon 200
horizon 300
(b)
Figure 1: Suboptimality of a policy learned with log-loss behavior cloning (LogLossBC) as a function of
the number of expert trajectories, for varying values of horizon H. In each environment, an imitator is
trained according to LogLossBC and the regret with respect to the expert is reported, with reward normalized
to be horizon-independent. (a) Continuous control with MuJoCo environment Walker2d-v4. (b) Discrete
control with Atari environment BeamRiderNoFrameskip-v4. For both environments, we find that the regret
is independent of horizon (or in the case of Atari, slightly improving with horizon), as predicted by our
theoretical results. Full experimental details are provided in Section 5.
to a standard supervised learning problem in which the learner attempts to predict the expert’s actions from
observations given the collected trajectories. The simplicity of this approach allows the learner to leverage
the considerable machinery developed for supervised learning and readily incorporate complex function
approximation with deep models (Beygelzimer et al., 2005; Ross and Bagnell, 2010). On the other hand,
BC seemingly ignores the problem of distribution shift, wherein small deviations from the expert policy early
in rollout lead the learner off-distribution to regions where they are less able to accurately imitate. This
apparent error amplification phenomenon has been widely observed empirically (Ross and Bagnell, 2010;
Laskey et al., 2017; Block et al., 2024a), and motivates online or interactive approaches to imitation learning
(Ross and Bagnell, 2010; Ross et al., 2011; Ross and Bagnell, 2014; Sun et al., 2017), which avoid error
amplification by interactively querying the expert and learning to correct mistakes on-policy.
In theory, online imitation learning enables sample complexity guarantees with improved (linear, as opposed
to quadratic) dependence on horizon for favorable MDPs. Yet, while online imitation learning has found
some empirical success (Ross et al., 2013; Kim et al., 2013; Gupta et al., 2017; Kelly et al., 2019), online
access to the expert can be costly or infeasible in many applications, and offline imitation learning remains
the dominant empirical paradigm. Motivated by this disconnect between theory and practice, we we aim to
understand whether the apparent gap between offline and online imitation learning is fundamental. We ask:
Is online imitation learning truly more sample-efficient than offline imitation learning, or can existing
algorithms or analyses be improved?
1.1
Background: Offline and Online Imitation Learning
To motivate our results, we begin by formally introduce the offline and online imitation learning frame-
works, highlighting gaps in current sample complexity guarantees concerning horizon dependence. We take
a learning-theoretic perspective, with a focus on general policy classes.
Markov decision processes.
We study imitation learning in episodic Markov decision processes. Formally,
a Markov decision process M = (X, A, P, r, H) consists of a (potentially large) state space X, action space
A, horizon H, probability transition function P = {Ph}H
h=0, where Ph : X × A →∆(X), and reward function
2

r = {rh}H
h=1, where rh : X × A →R. A (randomized) policy is a sequence of per-timestep functions
π = {πh : X →∆(A)}H
h=1. The policy induces a distribution over trajectories (x1, a1, r1), . . . , (xH, aH, rH)
via the following process. The initial state is drawn via x1 ∼P0(∅),1 then for h = 1, . . . , H: ah ∼π(xh),
rh = rh(xh, ah), and xh+1 ∼Ph(xh, ah). For notational convenience, we use xH+1 to denote a determin-
istic terminal state with zero reward. We let E
π[·] and Pπ[·] denote expectation and probability law for
(x1, a1), . . . , (xH, aH) under this process, respectively.2
The expected reward for policy π is given by J(π) := EπPH
h=1 rh

, and the value functions for π are given
by
V π
h (x) := EπhPH
h′=h rh′ | xh = x
i
,
and
Qπ
h(x, a) := EπhPH
h′=h rh′ | xh = x, ah = a
i
.
Reward normalization.
To study the role of horizon in imitation learning in a way that disentangles the
effects of reward scaling from other factors, we assume that rewards are normalized such that PH
h=1 rh ∈[0, R]
for a parameter R > 0 (Jiang and Agarwal, 2018; Wang et al., 2020; Zhang et al., 2021; Jin et al., 2021).
We refer to the setting in which rh ∈[0, 1] for all h ∈[H], which is the focus of most prior work (Ross and
Bagnell, 2010; Ross et al., 2011; Ross and Bagnell, 2014; Rajaraman et al., 2020, 2021a,b; Swamy et al.,
2022), as the dense reward setting, which has R ≤H; we will frequently specialize our results to this setting.
1.1.1
Offline Imitation Learning: Behavior Cloning
Let π⋆= {π⋆
h : X →∆(A)}H
h=1 denote the expert policy. In the offline imitation learning setting, we receive a
dataset D = {oi}n
i=1 of (reward-free) trajectories oi = (xi
1, ai
1), . . . , (xi
H, ai
H) obtained by executing π⋆in the
underlying MDP M ⋆. Using these trajectories, our goal is to learn a policy bπ such that the rollout regret J(π⋆)−
J(bπ) to π⋆is as small as possible. We emphasize that π⋆is an arbitrary policy, and is not assumed to be optimal.
Behavior cloning.
Behavior cloning, which reduces the imitation learning problem to supervised pre-
diction, is the dominant offline imitation learning paradigm. To describe the algorithm in its simplest
form, consider the case where π⋆:= {π⋆
h : X →A}H
h=1 is deterministic. For a user-specified policy class
Π ⊂{πh : X →∆(A)}H
h=1, the most basic version of behavior cloning (Ross and Bagnell, 2010) solves the
supervised classification problem
bπ = arg min
π∈Π
n
X
i=1
1
H
H
X
h=1
I{πh(x
i
h) ̸= a
i
h}
|
{z
}
=:bLbc(π)
.
(1)
Naturally, other classification losses (e.g., square loss, logistic loss, or log loss) may be used in place of the
indicator loss.3 To provide sample complexity bounds for this algorithm, we make a standard realizability
assumption (e.g., Agarwal et al. (2019); Foster and Rakhlin (2023)).
Assumption 1.1 (Realizability). The policy class Π contains the expert policy, i.e. π⋆∈Π.
This assumption asserts that Π is expressive enough to represent the expert policy;4 depending on the
application, Π might be parameterized by simple linear models, or by flexible models such as convolutional
neural networks or transformers. To simplify presentation, we adopt a standard convention in RL theory and
focus on finite classes with |Π| < ∞(Agarwal et al., 2019; Foster and Rakhlin, 2023). A standard uniform
convergence argument implies that if we define Lbc(π) = 1
H
PH
h=1 Pπ⋆[π(xh) ̸= π⋆(xh)], then with probability
1We use the convention that P0(∅) denotes the initial state distribution.
2To simplify presentation, we assume that X and A are countable, but our results trivially extend to general spaces with an
appropriate measure-theoretic treatment.
3Behavior cloning for stochastic expert policies has received limited attention in theory (Rajaraman et al., 2020), but the
logarithmic loss is widely used in practice. One contribution of our work is to fill this lacuna.
4We restrict our attention to the realizable setting to simplify presentation as much as possible, but extension to misspecified
policy classes is straightforward, and we remark on the misspecified case at various points.
3

at least 1 −δ, behavior cloning has
Lbc(bπ) ≲log(|Π|δ−1)
n
.
Meanwhile, a standard error analysis for BC leads to the following bound on rollout performance:
J(π⋆) −J(bπ) ≲RH · Lbc(bπ).
(2)
Combining these bounds, we conclude that
J(π⋆) −J(bπ) ≲RH · log(|Π|δ−1)
n
.
(3)
For the dense reward setting where R = H, this leads to quadratic dependence on horizon; that is, Ω(H2)
trajectories are required to achieve constant accuracy. Unfortunately, both steps in this argument are tight in
general:
• The generalization bound Lbc(bπ) ≲log(|Π|δ−1)
n
is tight even when |Π| = 2 (this is true not just for the
indicator loss, but for other standard losses such as square loss, absolute loss, and hinge loss). Since the
amount of information in a trajectory grows with H, one might hope a-priori that the generalization
error would decrease with H; alas, this does not occur due to the dependence between samples in each
trajectory.
• Ross and Bagnell (2010) show that the inequality J(π⋆) −J(bπ) ≲RH · Lbc(bπ) is tight for MDPs with
3 states; the quadratic scaling in H this induces under dense rewards is often attributed to error
amplification or distribution shift incurred by passing from error under the state distribution of π⋆to
the state distribution of bπ.
Combining, these observations, Ross and Bagnell (2010) conclude that offline imitation learning is fundamen-
tally harder than supervised classification, where linear dependence on horizon might be expected (e.g., if we
considered H independent prediction tasks).
1.1.2
Online Imitation Learning and Recoverability
The aforementioned limitations of behavior cloning have motivated online approaches to IL (Ross and
Bagnell, 2010; Ross et al., 2011; Ross and Bagnell, 2014; Sun et al., 2017). In the online framework, learning
proceeds in n episodes in which the learner can directly interact with the underlying MDP M ⋆and query the
expert advice. Concretely, for each episode i ∈[n], the learner executes a policy πi = {πi
h : X →∆(A)}H
h=1
and receives a trajectory ot = (xi
1, ai
1, a⋆,i
1 ), . . . , (xi
H, ai
H, a⋆,i
H ), in which ai
h ∼πi
h(xi
h), a⋆,i
h
∼π⋆(xt
h), and
xi
h+1 ∼Ph(xi
h, ai
h); in other words, the trajectory induced by the learner’s policy is annotated by the expert’s
action a⋆
h ∼π⋆
h(xh) at each state xh encountered.5 After all n episodes conclude, the learner produces a final
policy bπ whose regret to π⋆should be small. Online imitation learning can avoid error amplification and
achieve improved dependence on horizon for MDPs that satisfy a recoverability condition (Ross et al., 2011;
Rajaraman et al., 2021a).
Definition 1.1 (Recoverability parameter). The recoverability parameter for an MDP M ⋆and expert π⋆is
given by 6
µ =
max
x∈X,a∈A,h∈[H]
n
(Qπ⋆
h (x, π⋆
h(x)) −Qπ⋆
h (x, a))+
o
∈[0, R].
Under recoverability, the Dagger algorithm of Ross et al. (2011) leverages online interaction by interactively
querying the expert and learning to correct mistakes on-policy, leading to sample complexity
J(π⋆) −J(bπ) ≲µH · log|Π|
n
(4)
5All of the lower bounds in this paper continue to hold when the learner is allowed to select ai
h based on the sequence
(xi
1, ai
1, a⋆,i
1 ), . . . , (xi
h−1, ai
h−1, a⋆,i
h−1), (xi
h, a⋆,i
h ) at training time; we adopt the present formulation to keep notation compact.
6For stochastic policies, we overload notation and write f(π(x)) as shorthand for Ea∼π(x)[f(a)].
4

Parameter Sharing
(Corollary 2.1)
No Parameter Sharing (Π = Π1 × · · · ΠH)
(e.g., (Ross and Bagnell, 2010))
Sparse Rewards
O

R log(|Π|)
n

O

HR log(maxh |Πh|)
n

Dense Rewards (R = H)
O

H log(|Π|)
n

O

H2 log(maxh |Πh|)
n

Table 1: Summary of upper bounds for deterministic experts; lower bounds are more nuanced, and discussed
in Section 2.2. Each cell denotes the regret of a policy learned with log-loss behavior cloning (LogLossBC),
which is optimal in each setting. Here, Π is the policy class, R is the reward range, H is the horizon, and
n is the number of expert trajectories. In the dense-reward setting, we set R = H.
for any finite class Π and deterministic expert policy π⋆, when configured appropriately (for completeness,
we include an analysis in Appendix C.2; see Propositions C.1 and C.2).
For the dense reward setting where R = H, we can have µ = H in the worst case, in which case Eq. (4)
matches the quadratic horizon dependence of behavior cloning, but when µ = O(1) (informally, this means
it is possible to “recover” from a bad action that deviates from π⋆), the bound in Eq. (4) achieves linear
dependence on horizon. Other online IL algorithms such as Forward, Smile (Ross and Bagnell, 2010), and
Aggrevate (Ross and Bagnell, 2014) achieve similar guarantees (we are not aware of another approach that
improves upon Eq. (4) for general finite classes).
The improvements of online IL notwithstanding, Eq. (3) is known to be tight for BC, but this is an algorithm-
dependent (as opposed to information-theoretic) lower bound, and does not preclude the existence of more
sample-efficient, purely offline algorithms. In this context, our central question can be restated as: Can offline
imitation learning algorithms achieve sub-quadratic horizon dependence for general policy classes Π? While
prior work has investigated this question for tabular and linear policies (Rajaraman et al., 2020, 2021a,b), we
approach the problem from a new (learning-theoretic) perspective by considering general policy classes.
1.2
Contributions
We present several new results that clarify the role of horizon in offline and online imitation learning.
1. Horizon-independent analysis of log-loss behavior cloning. Through a new analysis of behavior
cloning with the logarithmic loss (LogLossBC), we show that it is possible to achieve horizon-
independent sample complexity (Jiang and Agarwal, 2018; Wang et al., 2020; Zhang et al., 2021,
2022) in offline imitation learning whenever (i) the range of the cumulative payoffs is normalized, and
(ii) an appropriate notion of supervised learning complexity for the policy class is controlled. Our result
is facilitated by a novel information-theoretic analysis which controls policy behavior at the trajectory
level, supporting both deterministic and stochastic expert policies.
2. Deterministic policies: Closing the gap between offline and online IL. Specializing LogLossBC to
deterministic stationary policies (more generally, policies with parameter sharing) and cumulative rewards in
the range [0, H], we show that it is possible to achieve sample complexity with linear dependence on horizon
in offline IL in arbitrary MDPs, matching was was previously only known of online IL. We complement
this result with a lower bound showing that, without further structural assumptions on the policy class
(e.g., no parameter sharing (Rajaraman et al., 2020)), online IL cannot improve over offline IL with
LogLossBC, even for benign MDPs. Our results are summarized in Table 1. Nonetheless, as observed in prior
work (Rajaraman et al., 2020), online imitation learning can still be beneficial for non-stationary policies.
3. Stochastic policies: Tight understanding of optimal sample complexity. For stochastic expert
policies, our analysis of LogLossBC gives the first variance-dependent sample complexity bounds for imita-
tion learning with general policy classes, which we prove to be tight in a problem-dependent and minimax
sense. Using this result, we show that for stochastic stationary experts, (i) quadratic dependence on the
horizon is necessary when cumulative rewards lie in the range [0, H], in contrast to the deterministic
setting, but (ii) LogLossBC—through our variance-dependent analysis—can sidestep this hardness and
achieve linear dependence on horizon under a recoverability-like condition. Finally, we show that, as in the
5

deterministic case, online IL cannot improve over offline IL with LogLossBC without further assumptions
on the policy class. Our results are summarized in Table 2.
Toward a learning-theoretic understanding of imitation learning.
Our findings call into question
the conventional wisdom around the benefits of online imitation learning, and highlight the need to develop
a fine-grained, problem-dependent understanding of algorithms and complexity for IL. Indeed, instabilities
of offline IL (Block et al., 2024a) and benefits of online IL (Ross et al., 2013) may indeed arise in practice,
but existing assumptions in theoretical research are often too coarse to give insights into the true nature of
these phenomena, leading to an important gap between theory and practice. As a first step in this research
program, we highlight several under-explored mechanisms through which online IL can lead to improved
sample complexity, including representational benefits and exploration (Section 4). We also complement our
theoretical results with empirical demonstrations of the phenomena we describe (Section 5).
Experiments.
In Section 5, we complement our theoretical results with an empirical demonstration of the
horizon-independence of LogLossBC predicted by our theory (under parameter sharing and sparse rewards).
We consider tasks where the horizon H can be naturally scaled up and down—for example, an agent walking
for a set number of timesteps—and use an expert trained according to RL to generate expert trajectories,
before training a policy using LogLossBC. We consider both continuous action space (MuJoCo environment
Walker2d) and discrete action space (Atari environment Beamrider) tasks to demonstrate the broad applicability
of our theoretical results. As can be seen in Figure 1, the performance of the learned policy is independent
or improving with horizon, consistent with our theoretical results. We also perform simplified experiments
on autoregressive language generation with transformers. Here, we find that the performance of the imitator
is largely independent of H, as predicted by our results, though the results are more nuanced.
1.3
Paper Organization
Section 2 presents the first of our main results, a horizon-independent sample complexity analysis for LogLossBC
for deterministic experts, and discusses implications regarding the gap between offline and online IL as it
concerns horizon. Section 3 presents analogous results and implications for stochastic experts. Section 4
discusses mechanisms through which online IL can have benefits over offline IL, beyond horizon dependence,
highlighting directions for future research. Section 5 presents an empirical validation, and we conclude with
open problems and further directions for future research in Section 6.3. Proofs and additional results are
deferred to the appendix.
Notation.
For an integer n ∈N, we let [n] denote the set {1, . . . , n}. For a set X, we let ∆(X) denote
the set of all probability distributions over X. We use Ix ∈∆(X) to denote the direct delta distribution,
which places probability mass 1 on x. We adopt standard big-oh notation, and write f = eO(g) to denote that
f = O(g · max{1, polylog(g)}) and a ≲b as shorthand for a = O(b).
2
Horizon-Independent Analysis of Log-Loss Behavior Cloning
This section presents the first of our main results, a horizon-independent sample complexity analysis of
log-loss behavior cloning for the case of deterministic experts. Our second main result, handles the case of
stochastic experts, builds on our results here, and is presented in Section 3.
2.1
Log-Loss Behavior Cloning and Supervised Learning Guarantees
The workhorse for all of our results (both for deterministic and stochastic experts), is the following simple mod-
ification to behavior cloning. For a class of (potentially stochastic) policies Π, we minimize the logarithmic loss:
bπ = arg min
π∈Π
n
X
i=1
H
X
h=1
log

1
π(ai
h | xi
h)

.
(5)
6

This scheme is ubiquitous in practice (Hussein et al., 2018; Florence et al., 2022), and forms the basis for
autoregressive language modeling (Radford et al., 2019); we refer to it as LogLossBC. We will show that this
seemingly small change—moving from indicator loss to log loss—has significant benefits.7
Following the classical tradition of imitation learning (Ross and Bagnell, 2010; Ross et al., 2011; Ross and
Bagnell, 2014), our analysis proceeds via reduction to supervised learning. We first show that LogLossBC
satisfies an appropriate supervised learning guarantee, then translate this into rollout performance. Our
starting point is to observe that LogLossBC, via Eq. (5), can be interpreted as performing maximum likelihood
estimation over the set {Pπ}π∈Π in order to estimate the law Pπ⋆over trajectories under π⋆(see Appendix C.1
for details). As a result, standard guarantees for maximum likelihood estimation (van de Geer, 2000; Zhang,
2006) imply convergence in distribution whenever π⋆∈Π. To be precise, define the squared Hellinger distance
for probability measures P and Q with a common dominating measure ω by
D2
H(P, Q) =
Z r
dP
dω −
r
dQ
dω
2
dω.
Then for any finite policy class Π, we have the following guarantee.8
Proposition 2.1 (Supervised learning guarantee for LogLossBC (special case of Theorem C.1)). For any (poten-
tially stochastic) expert π⋆∈Π, the LogLossBC algorithm in Eq. (5) ensures that with probability at least 1 −δ,
D2
H
 Pbπ, Pπ⋆
≤2log(|Π|δ−1)
n
.
That is, by performing LogLossBC, we are implicitly estimating the law Pπ⋆; note that this result holds even
if π⋆is stochastic, as long as π⋆∈Π. We will focus on finite, realizable policy classes throughout this section
to simplify presentation as much as possible, but guarantees for infinite classes under misspecification are
given in Appendix C.1.
2.2
Horizon-Independent Analysis of LogLossBC for Deterministic Experts
We first consider the case where the expert π⋆is deterministic. Our main result is the following theorem,
which translates the supervised learning error D2
H
 Pbπ, Pπ⋆
into a bound on rollout performance in a horizon-
independent fashion.
Theorem 2.1 (Horizon-independent regret decomposition (deterministic case)). For any deterministic policy
π⋆and potentially stochastic policy bπ,
J(π⋆) −J(bπ) ≤4R · D2
H
 Pbπ, Pπ⋆
.
(6)
This result shows that horizon-independent bounds on rollout performance are possible whenever (i) rewards
are appropriately normalized, and (ii) the supervised learning error D2
H
 Pbπ, Pπ⋆
is appropriately controlled.
It is proven using novel trajectory-level control over deviations between bπ and π⋆; we will elaborate upon this
in the sequel. We emphasize that this result would be trivial if squared Hellinger distance were replaced by
total variation distance in (6); that the bound scales with squared Hellinger distance is crucial for obtaining
fast 1/n-type rates and linear horizon dependence. We further remark that this reduction is not specific to
LogLossBC, and can be applied to any IL algorithm for which we can bound the Hellinger distance. Combining
Theorem 2.1 with Proposition 2.1, we obtain the following guarantee for finite policy classes.
7Beginning from Foster and Krishnamurthy (2021), a recent line of work (Wang et al., 2023, 2024; Ayoub et al., 2024) shows
that the logarithmic loss can be beneficial for deriving problem-dependent bounds for various reinforcement learning settings.
We build upon the information-theoretic machinery of Foster and Krishnamurthy (2021); Foster et al. (2021), but use it show
that for imitation learning, the log-loss is beneficial even in a minimax sense.
8While unfamiliar readers might expect a bound on KL divergence, Hellinger distance turns out to be more natural due to a
connection to the MGF of the log-loss (van de Geer, 2000; Zhang, 2006). This facilitates scale-free generalization guarantees in
spite of the potential unboundedness of the log-loss. .
7

Corollary 2.1 (Regret of LogLossBC (deterministic case)). For any deterministic expert π⋆∈Π, the
LogLossBC algorithm in Eq. (5) ensures that with probability at least 1 −δ, it holds that
J(π⋆) −J(bπ) ≤8R · log(2|Π|δ−1)
n
.
(7)
To the best of our knowledge, this is the tightest available sample complexity guarantee for offline imitation
learning with general policy classes. This bound improves upon the guarantee for indicator-loss behavior
cloning in Eq. (3) by an O(H) factor, and improves upon the guarantee for Dagger in Eq. (4) (replacing H
with R ≤H under rh ∈[0, 1]) in the typical regime where µ = Ω(1).
2.3
Interpreting the Sample Complexity of LogLossBC
To understand the behavior of the bound for LogLossBC in Corollary 2.1 in more detail, we consider two
special cases (summarized in Table 1).
Stationary policies and parameter sharing.
If log|Π| = O(1), the bound in Eq. (7) is independent
of horizon in the case of sparse rewards (R = O(1)), and linear in horizon in the case of dense rewards
(R = O(H)). In other words, our work establishes for the first time that:
O(H) sample complexity can be achieved in offline IL under dense rewards for general Π,
as long as log|Π| is appropriately controlled. This runs somewhat counter to intuition expressed in prior work
(Ross and Bagnell, 2010; Ross et al., 2011; Ross and Bagnell, 2014; Rajaraman et al., 2020, 2021a,b; Swamy
et al., 2022), but we will show in the sequel that there is no contradiction.
Generally speaking, we expect to have log|Π| = O(1) if Π consists of stationary policies or more broadly,
policies with parameter sharing across steps h ∈[H] (as is the case in transformers used for autoregressive text
generation). As an example, for a tabular (finite state/action) MDP, if Π consists of all stationary policies,
we have log|Π| = |X| log|A|, so Eq. (7) gives J(π⋆) −J(bπ) ≲R|X| log(|A|δ−1)
n
; that is, stationary policies can
be learned with horizon-independent samples complexity under sparse rewards and linear dependence on
horizon under dense rewards.
Similar behavior holds for non-stationary policies with parameter sharing. For example, we show (Ap-
pendix C.1) that for linear policy classes of the form πh(a | x) ∝exp(⟨ϕh(x, a), θ⟩) for a feature map
ϕh(x, a) ∈Rd, one can take D2
H
 Pbπ, Pπ⋆
= eO( d
n), so that Theorem 2.1 gives J(π⋆) −J(bπ) ≤eO( Rd
n ).
Non-stationary policies or no parameter sharing.
For non-stationary policies or policies with no
parameter sharing across steps h (e.g., product classes where Π = Π1 ×Π2 · · ·×ΠH), we expect log|Π| = O(H)
(more generally, D2
H
 Pbπ, Pπ⋆
= eO(H/n)). For example, in a tabular MDP, if Π consists of all non-stationary
policies, we have log|Π| = H|X| log|A|. In this case, Eq. (7) gives linear dependence on horizon for sparse
rewards (J(π⋆) −J(bπ) ≲RH|X| log(|A|δ−1)
n
) and quadratic dependence on horizon for dense rewards (J(π⋆) −
J(bπ) ≲H2|X| log(|A|δ−1)
n
). The latter bound is known to be optimal (Rajaraman et al., 2020) for offline IL.
2.4
Optimality and Consequences for Online versus Offline Imitation Learning
We now investigate the optimality of Theorem 2.1 and discuss implications for online versus offline imitation
learning, as well as connections to prior work. Our main result here shows that in the dense-reward regime
where rh ∈[0, 1] and R = H, Theorem 2.1 cannot be improved when log|Π| = O(1)—even with online access,
recoverability, and known dynamics.
Theorem 2.2 (Lower bound for deterministic experts). For any n ∈N and H ∈N, there exists a (reward-free)
MDP M ⋆with |X| = |A| = 2, a class of reward functions R with |R| = 2, and a class of deterministic policies
Π with |Π| = 2 with the following property. For any (online or offline) imitation learning algorithm, there
8

exists a deterministic reward function r = {rh}H
h=1 with rh ∈[0, 1] (in particular, R ≤H) and (optimal)
expert policy π⋆∈Π with µ = 1 such that the expected suboptimality is lower bounded as
E[J(π⋆) −J(bπ)] ≥c · H
n
for an absolute constant c > 0. In addition, the dynamics, rewards, and expert policies are stationary.
Together, Theorems 2.1 and 2.2 show that without further assumptions on Π, online imitation learning cannot
improve upon offline imitation learning. That is, even if recoverability is satisfied, there is no online imitation
learning algorithm that improves upon Theorem 2.1 uniformly for all policy classes. See Appendix G.1 for
further lower bounds.
Benefits of online IL for policies with no parameter sharing.
How can we reconcile our results
with the claim found throughout prior work (Ross and Bagnell, 2010; Ross et al., 2011; Ross and Bagnell,
2014; Rajaraman et al., 2020, 2021a,b; Swamy et al., 2022) that online IL improves the horizon dependence
of offline IL? The important distinction here is that online IL can still improve on a policy-class dependent
basis. In particular, methods like Dagger can still lead to improved sample complexity for policy classes with
no parameter sharing across steps h ∈[H]. Let Πh := {πh | π ∈Π} denote the projection of Π onto step
h. In Appendix C.2, we prove the following refined guarantee for a variant of Dagger based on the log-loss
(LogLossDagger).
Proposition 2.2 (Special case of Proposition C.2). When π⋆∈Π is deterministic, LogLossDagger ensures
that with probability at least 1 −δ,
J(π⋆) −J(bπ) ≲µ ·
H
X
h=1
log(|Πh|Hδ−1)
n
.
For classes with no parameter sharing (i.e., product classes where Π = Π1 × Π2 · · · × ΠH), we have
PH
h=1 log|Πh| = log|Π|. In this case, Proposition 2.2 scales as J(π⋆) −J(bπ) ≲µ · log(|Π|Hδ−1)
n
, improving on
the bound for LogLossBC in Theorem 2.1 by replacing R with µ ≤R. Thus, online imitation learning can
indeed improve over offline IL for classes with no parameter sharing. This is consistent with Rajaraman et al.
(2020, 2021a), who proved a µH vs. H2 gap between online and offline IL for the special case of non-stationary
tabular policies (where Π is a product class with log|Π| ∝H) under dense rewards. However, for classes with
parameter sharing (i.e., where log|Πh| ∝log|Π|), the bound in Proposition 2.2 scales as µH log|Π|
n
, which does
not improve over Theorem 2.1 unless µ ≪1. Since virtually all empirical work on imitation learning uses
parameter sharing across steps h ∈[H], we believe the finding that online IL does not improve over offline IL
in this regime is quite salient.
Remark 2.1 (Known dynamics/inverse RL). Complementary to our results, various works show improved
horizon dependence in offline IL under the assumption that the MDP dynamics are known (Rajaraman et al.,
2020; Swamy et al., 2021); see Appendix A for discussion.
2.5
Proving Theorem 2.1: How Does LogLossBC Avoid Error Amplification?
The central object in the proof of Theorem 2.1 is the following trajectory-level distance function between
policies. For a pair of potentially stochastic policies π and π′, define
ρ(π ∥π′) := Eπ Ea′
1:H∼π′(x1:H)[I{∃h : ah ̸= a′
h}],
(8)
where we use the shorthand a′
1:H ∼π′(x1:H) to indicate that a′
1 ∼π′(x1), . . . , a′
H ∼π′(xH). We begin by
showing (Lemma D.2) that for all (potentially stochastic) policies π⋆and bπ,
J(π⋆) −J(bπ) ≤R · ρ(π⋆∥bπ).
(9)
9

We then show (Lemma D.3) that whenever π⋆is deterministic, Hellinger distance satisfies9
D2
H

Pbπ, Pπ⋆
≥1
4 · ρ(bπ ∥π⋆).
Finally, we show (Lemma D.1) that the trajectory-level distance is symmetric, i.e.
ρ(bπ ∥π⋆) = ρ(π⋆∥bπ).
This step is perhaps the most critical: by considering trajectory-level errors, we can switch from the state
distribution induced by bπ to that of π⋆for free, without incurring error amplification or spurious horizon
factors. Combining the preceding inequalities yields Theorem 2.1; see Appendix D for the full proof.
This analysis is closely related to a result in Rajaraman et al. (2021a). For the special case of deterministic,
linearly parameterized policies with parameter sharing, Rajaraman et al. (2021a) consider an algorithm
that minimizes an empirical analogue of the trajectory-wise distance in Eq. (8), and show that it leads to a
bound similar to Eq. (7) (i.e., linear-in-H sample complexity under dense rewards). Relative to this work, our
contributions are threefold: (i) we show that horizon-independent sample complexity can be achieved for
arbitrary policy classes with parameter sharing, not just linear classes; (ii) we show that said guarantees can
be achieved by a natural algorithm, LogLossBC, which is already widely used in practice; and (iii), by virtue
of considering the log loss, our results readily generalize to encompass stochastic expert policies, as we will
show in the sequel.10
3
Horizon-Independent Analysis of LogLossBC for Stochastic Experts
In this section, we turn out attention to the general setting in which the expert policy π⋆is stochastic.
Stochastic policies are widely used in practice, where they are useful for modeling multimodal behavior
(Shafiullah et al., 2022; Chi et al., 2023; Block et al., 2024b), but have received relatively little exploration in
theory beyond the work of Rajaraman et al. (2020) for tabular policies.11
Our main result for this section, Theorem 3.1, is a regret decomposition based on the supervised learning
error D2
H
 Pbπ, Pπ⋆
that is horizon-independent and variance-dependent (Zhou et al., 2023; Zhao et al., 2023;
Wang et al., 2024).
To state the guarantee, we define the following notion of variance for the expert policy:
σ2
π⋆:=
H
X
h=1
Eπ⋆h
(Qπ⋆
h (xh, π⋆(xh)) −Qπ⋆
h (xh, ah))2i
.
(10)
We can equivalently write this as σ2
π⋆= PH
h=1 Eπ⋆
(V π⋆
h (xh) −Qπ⋆
h (xh, ah))2
.
Our main result is as follows.
Theorem 3.1 (Horizon-independent regret decomposition). Assume R ≥1. For any pair of (potentially
stochastic) policies π⋆and bπ and any ε ∈(0, e−1),
J(π⋆) −J(bπ) ≤
q
6σ2
π⋆· D2
H
 Pbπ, Pπ⋆
+ O
 R log(Rε−1)

· D2
H
 Pbπ, Pπ⋆
+ ε.
(11)
Applying this result with LogLossBC leads to the following guarantee.
Corollary 3.1 (Regret of LogLossBC). For any expert π⋆∈Π, the LogLossBC algorithm in Eq. (5) ensures
that with probability at least 1 −δ,
J(π⋆) −J(bπ) ≤O(1) ·
r
σ2
π⋆log(|Π|δ−1)
n
+ O(R log(n)) · log(|Π|δ−1)
n
.
(12)
9In fact, the opposite direction of this inequality holds as well, up to an absolute constant.
10A fourth benefit is that our analysis supports the setting in which π⋆is deterministic, yet Π contains stochastic policies. This
is a natural setting which can arise when, for example, Π is parameterized by softmax policies. Guarantees under misspecification,
which support this setting, are given in Appendix C.1.
11As discussed at length in Rajaraman et al. (2020), many prior works (Ross and Bagnell, 2010; Ross et al., 2011) state
results in a level of generality that allows for stochastic experts, but the notions of supervised learning error found in these
works (e.g., TV distance) do not lead to tight rates when instantiated for stochastic experts.
10

Worst-case
Low-noise
eµ-recoverable
Sparse
Rewards
eO

R
q
log(|Π|)
n

eO
q
σ2
π⋆log(|Π|)
n
+ R log(|Π|)
n

N/A
Dense
Rewards
eO

H
q
log(|Π|)
n

eO
q
σ2
π⋆log(|Π|)
n
+ H log(|Π|)
n

eO

eµ
q
H log(|Π|)
n
+ H log(|Π|)
n

Table 2: Summary of upper bounds for stochastic experts (Corollary 3.1). Each cell denotes the expected
regret of a policy learned with LogLossBC; lower bounds are more nuanced and discussed in Section 3. Here Π
is the policy class, R is the cumulative reward range, H is the horizon, n is the number of expert trajectories,
σ2
π⋆is the variance of the expert policy (Eq. (10)), and eµ is the signed recoverability parameter (Eq. (13)).12
As we show in the sequel, when the expert policy is stochastic, we can no longer hope for a “fast” 1/n-type
rate, and must instead settle for a “slow” 1/√n-type rate. The slow term in Eq. (12) is controlled by the
variance σ2
π⋆for the optimal policy. In particular, if π⋆is deterministic, then σ2
π⋆= 0, and Eq. (12) recovers
our bound for the deterministic setting in Corollary 2.1 up to a log(n) factor.
3.1
Horizon-Independence and Optimality for Stochastic Experts
To understand the dependence on horizon in Corollary 3.1, we restrict our attention to the “parameter sharing”
case where log|Π| = O(1), and separately discuss the sparse and dense reward settings (results summarized in
Table 2).
Consider the sparse reward setting where R = O(1). Here, at first glance it would appear that the variance
σ2
π⋆should scale with the horizon. Fortunately, this is not the case: The following result—via a law-of-total-
variance-type argument (Azar et al., 2017)—implies that Corollary 3.1 is fully horizon-independent, with no
explicit dependence on horizon when R = O(1) and log|Π| = O(1). For a function f(x1:H, a1:H), let Varπ[f]
denote the variance of f under (x1, a1), . . . , (xH, aH) ∼π .
Proposition 3.1. We have that σ2
π⋆≤Varπ⋆PH
h=1 rh

≤R2.
For the dense-reward regime where R = H, Proposition 3.1 gives J(π⋆) −J(bπ) ≲H
q
log(|Π|)
n
. This is
somewhat disappointing, as we now require Ω(H2) trajectories (quadratic sample complexity) to learn a
non-trivial policy, even when log|Π| = O(1). The following result shows that the dependence on the variance
in Corollary 2.1 cannot be improved in general, which implies that the quadratic horizon dependence in this
regime is tight.
Theorem 3.2 (Lower bound for stochastic experts). Consider the dense reward setting where rh ∈[0, 1]
and R = H. For any n ∈N, H ∈N and σ2 ∈[H, H2], there exists a reward-free MDP M ⋆with |X| = 3 and
|A| = 2, a class of reward functions R with |R| = 2, and a class of policies Π with |Π| = 2 with the following
property. For any (online or offline) imitation learning algorithm, there exists a deterministic reward function
r = {rh}H
h=1 and expert policy π⋆∈Π such that σ2
π⋆≤σ2 and eµ ≤σ2/H (Eq. (13)), and for which
P
 
J(π⋆) −J(bπ) ≥c ·
r
σ2
n
!
≥1
8
for an absolute constant c ≥1.
Beyond showing that a slow 1/√n rate is required for stochastic policies,13 when specializing to σ2 = H2,
this result shows that Ω(H2) trajectories are required to learn a non-trivial policy under a stochastic expert,
even when log|Π| = O(1); this reveals a fundamental difference between deterministic and stochastic experts,
since O(H) sample complexity is sufficient in the former case.
13Rajaraman et al. (2020) show that for the tabular setting, it is possible to achieve a 1/n-type rate in-expectation for
stochastic policies. Their result critically exploits the assumption that |X| and |A| are small and finite to argue that it is possible
to build an unbiased estimator for π⋆. Theorem 3.2 shows that such a result cannot hold with even constant probability for the
same setting. We believe the fact that a 1/n-type rate is possible in expectation is an artifact of the tabular setting, and unlikely
to hold for general policy classes.
11

Nonetheless, it is possible to obtain linear-in-H sample complexity for dense rewards under a recoverability-like
condition. Let us define the signed recoverability constant via
eµ =
max
x∈X,a∈A,h∈[H]
(Qπ⋆
h (x, π⋆
h(x)) −Qπ⋆
h (x, a)
.
(13)
Note that eµ ∈[0, R], and that eµ ≥µ, since this version counts actions a that outperform π⋆, not just those
that underperform. It is immediate to see that σ2
π⋆≤eµ2H. Hence, even if R = H, as long as eµ = O(1),
Corollary 3.1 yields J(π⋆) −J(bπ) ≲
q
H log(|Π|)
n
+ H log(|Π|)
n
, so that O
  H log|Π|
ε2

trajectories suffice to learn
an ε-optimal policy.14
See Appendix G for further results concerning tightness of Theorem 3.1, including instance-dependent lower
bounds.
Consequences for online versus offline IL.
The lower bound in Theorem 3.2 holds even for online
imitation learning algorithms. Thus, similar to the deterministic setting, there is no online IL algorithm that
improves upon Theorem 3.1 uniformly for all policy classes. This means that even for stochastic experts,
online imitation learning cannot improve upon offline imitation learning without further assumptions (e.g.,
no parameter sharing) on the policy class under consideration.
3.2
Proof Sketch for Theorem 3.1
When the expert is stochastic, the trajectory-wise distance in Eq. (8), is no longer useful (i.e., ρ(π⋆∥π⋆) ̸= 0),
which necessitates a more information-theoretic analysis. Our starting point is the following scale-sensitive
change-of-measure lemma for Hellinger distance.
Lemma 3.1 (Change-of-measure for Hellinger distance (Foster et al., 2021, 2022)). Let P and Q be probability
distributions over a measurable space (X, F). Then for all functions h : X →R,
|EP[h(X)] −EQ[h(X)]| ≤
q
1
2(EP[h2(X)] + EQ[h2(X)]) · D2
H(P, Q).
(14)
In particular, if h ∈[0, R] almost surely, then
EP[h(X)] ≤2 EQ[h(X)] + R · D2
H(P, Q).
(15)
We first sketch how to use this result to prove a weaker version of Theorem 3.1, then explain how to strengthen
this argument. Define the sum of advantages for a trajectory o = (x1, a1), . . . , (xH, aH) via
∆(o) =
H
X
h=1
Qπ⋆
h (xh, π⋆(xh)) −Qπ⋆
h (xh, ah) =
H
X
h=1
V π⋆
h (xh) −Qπ⋆
h (xh, ah).
By the performance difference lemma, we can write J(π⋆) −J(bπ) = Ebπ[∆(o)], so applying Eq. (14) yields
J(π⋆) −J(bπ) = Ebπ[∆(o)] ≲Eπ⋆[∆(o)]
|
{z
}
=0
+
q
(Ebπ[∆2(o)] + Eπ⋆[∆2(o)]) · D2
H(Pbπ, Pπ⋆).
From here, we observe that Eπ⋆[∆(o)] = 0 and Eπ⋆
∆2(o)

= σ2
π⋆(this follows because advantages are a
martingale difference sequence under Pπ⋆), so all that remains is to bound the term Ebπ
∆2(o)

. A crude
approach is to observe that |∆(o)| ≤eµH, so that applying Eq. (15) gives
Ebπ
∆2(o)

≲Eπ⋆
∆2(o)

|
{z
}
=σ2
π⋆
+(eµH)2 · D2
H

Pbπ, Pπ⋆
,
(16)
14An interesting question for future work is to understand if a similar conclusion holds if we replace eµ with µ.
12

and consequently
J(π⋆) −J(bπ) ≲
q
σ2
π⋆· D2
H
 Pbπ, Pπ⋆
+ eµH · D2
H
 Pbπ, Pπ⋆
.
This falls short of Eq. (18) due to the suboptimal lower-order term, which does not recover Theorem 2.1 when
π⋆is deterministic (σ2
π⋆= 0).
To address this, we make use of the following advantage concentration lemma.
Lemma 3.2 (Concentration for advantages). Assume that rh ≥0 and PH
h=1 rh ∈[0, R] almost surely for
some R > 0. Then for any (potentially stochastic) policy π, it holds that for all δ ∈(0, e−1),
Pπ

∃H′ :

H′
X
h=1
Qπ
h(xh, ah) −V π
h (xh)

≥c · R log(δ−1)

≤δ,
for an absolute constant c > 0.
This result shows that even though the sum of advantages ∆(o) could be as large as eµH for a given realization
of the trajectory o = (x1, a1), . . . , (xH, aH), the range is bounded as |∆(o)| ≲R (i.e., horizon-independent)
with high probability under π⋆. From here, the crux of the proof is a stopping time argument, which we use
to argue that—up to negligible approximation error—we can truncate ∆(o) to order R, facilitating a tighter
application of the change-of-measure argument in Eq. (16). The stopping time argument is quite subtle and
somewhat involved, owing to the fact that while ∆(o) concentrates well under Pπ⋆, it is not guaranteed
(a-priori) to concentrate under Pbπ.
4
To What Extent is Online Interaction Beneficial?
Our results in Sections 2 and 3 show that the benefits of online interaction in imitation learning—to the
extent that horizon is concerned—are more limited than previously thought. We expect that ’in practice,
online interaction may still lead to benefits, but in a problem-dependent sense. To this end, we now highlight
several special cases in which online interaction does indeed lead to benefits over offline imitation learning,
but in a policy class-dependent fashion not captured by existing theory. In particular, we identify three
phenomena which lead to improved sample complexity: (i) representational benefits; (ii) value-based feedback;
and (iii) exploration. Our results in this section can serve as a starting point toward developing a more
fine-grained understanding of algorithms and sample complexity of imitation learning.
Representational benefits.
The classical intuition behind algorithms like Dagger and Aggrevate (which
Definition 1.1 attempts to quantify) is recoverability: through online access, we can learn to correct the
mistakes of an imperfect policy. Our results in Sections 2 and 3 show that recoverability has limited benefits
for stationary policy classes as far as horizon is concerned. In spite of this, the following proposition shows
that recoverability can have pronounced benefits for representational reasons, even with constant horizon.
Proposition 4.1 (Representational benefits of online IL). For any N ∈N, there exists a class M of MDPs
with H = 2 and a policy class Π with log|Π| = O(N) such that
• There is an online imitation learning algorithm that achieves J(π⋆) −J(bπ) = 0 with probability at least
1 −δ using O(log(δ−1)) episodes for any MDP M ⋆∈M and expert policy π⋆∈Π. In particular, this
can be achieved by Dagger.
• Any proper offline imitation learning algorithm requires n = Ω(N) trajectories to learn a non-trivial
policy with J(π⋆) −J(bπ) ≤c for an absolute constant c > 0.15
The idea behind this construction is as follows: The behavior of the (stochastic) expert policy at step h = 1 is
very complex, and learning to imitate it well in distribution (e.g., with respect to total variation or Hellinger
distance) is a difficult representation learning problem (in the language of Section 2, e.g., Theorem 2.1, we
15We expect that this result extends to improper offline IL algorithms for which bπ /∈Π, but a more complicated construction
is required; we leave this for the next version of the paper.
13

must take log|Π1| very large in order to realize π⋆
1). For offline imitation learning, we have no choice but to
imitate π⋆
1 well at h = 1, leading to the lower bound in Proposition 4.1. With online access though, we can
give up on learning π⋆
1 well, and instead learn to correct our mistake at step h = 2. For the construction in
Proposition 4.1, this a much easier representation learning problem, and requires very low sample complexity
(i.e., we can realize π⋆
2 with a class Π2 for which log|Π2| is small. We conclude that Dagger can indeed lead
to substantial benefits over offline IL, but for representational reasons unrelated to horizon, and not captured
by existing theory. While this example is somewhat contrived, it suggests that potential to develop a deeper
understanding of representational benefits in imitation learning, which we leave as a promising direction for
future work.
Benefits of value-based feedback.
Beginning with the work of Ross and Bagnell (2014) on Aggrevate,
many works (e.g., Sun et al. (2017)) consider a value-based feedback variant of the online IL framework
(Section 1.1) where in addition to (or instead of) observing a⋆
h, the learner observes the expert’s advantage
function Aπ⋆
h (xh, ·) := Qπ⋆
h (xh, π⋆
h(xh)) −Qπ⋆
h (xh, ·) or value function Qπ⋆
h (xh, ·) at every state visited by the
learner (see Appendix F.2 for details, which are deferred to the appendix for space). While such feedback
intuitively seems useful, existing theoretical guarantees—to the best of our knowledge—(Ross and Bagnell,
2014; Sun et al., 2017) only show that algorithms like Aggrevate are no worse than non-value based methods
like Dagger, and do not quantify situations in which value-based feedback actually leads to improvement.16
The following result shows that i) value-based feedback can lead to arbitrarily large improvement over
non-value based feedback for representational reasons similar to Proposition 4.1 (that is for a complicated
stochastic expert, learning to optimize a fixed value function can be much easier than learning to imitate the
expert well in TV distance), but ii) it is only possible to exploit value-based feedback in this fashion under
online interaction (that is, even if we annotate the trajectories for offline imitation learning with Aπ⋆
h (xh, ·)
for the visited states, this cannot lead to improvement in sample complexity).
Proposition 4.2 (Benefits of value-based feedback (informal)). For any N ∈N, there is a class of MDPs
M with H = 2 and a policy class Π with log|Π| = O(N) such that
• There is an online imitation learning algorithm with value-based feedback that achieves J(π⋆) −J(bπ) = 0
with probability at least 1 −δ using O(log(δ−1)) episodes for every MDP M ⋆∈M and expert π⋆∈Π.
In particular, this can be achieved by Aggrevate.
• Any proper offline imitation learning algorithm (with value-based feedback) or proper online imitation
learning algorithm (without valued-based feedback) requires n = Ω(N) trajectories to learn a non-trivial
policy with J(π⋆) −J(bπ) ≤c for an absolute constant c > 0.17
As with Proposition 4.1, this example calls for a fine-grained policy class-dependent theory, which we hope to
explore more deeply in future work.
Benefits from exploration.
A final potential benefit of online interaction arises in exploration. One might
hope that with online access, we can directly guide the MDP to informative states that will help to identify
the optimal policy faster. The following proposition gives an example in which deliberate exploration can
lead to arbitrarily large improvement over offline imitation learning, as well as over naive online imitation
learning algorithms like Dagger that do not deliberately explore.
Proposition 4.3 (Benefits of exploration for online IL). For any n ∈N and H ∈N, there exists an MDP
M ⋆and a class of deterministic policies Π with |Π| = 2 with the following properties.
1. There exists an online imitation learning algorithm that returns a policy bπ such that J(π⋆)−J(bπ) = 0 with
probability at least 1−δ using O(log(δ−1)) episodes, for all possible reward functions (i.e., even if µ = H).
16These results are reductions which bound regret in terms of different notions of supervised learning performance, which
makes it somewhat difficult to compare them or derive concrete end-to-end guarantees.
17As with Proposition 4.1, we expect that this lower bound can be extended to improper learners, but a more complicated
construction is required.
14

2. For any offline imitation learning algorithm, there exists a deterministic reward function r = {rh}H
h=1
and expert policy π⋆∈Π with µ = 1 such that any algorithm must have E[J(π⋆) −J(bπ)] ≥Ω(1) · H
n .
In addition, Dagger has regret E[J(π⋆) −J(bπ)] ≥Ω(1) · H
n .
The idea behind this construction is simple: We take the lower bound construction from Theorem 2.2 and
augment it with a “revealing” which directly reveals the identity of the underlying expert. The true expert
never visits this state, so offline imitation learning algorithms cannot exploit it (standard online IL algorithms
like Dagger and relatives do not exploit the revealing state for the same reason),18 but a well-designed online
IL algorithm that deliberately navigates to the revealing state can use it to identify π⋆extremely quickly.
As with the previous examples, this construction is somewhat contrived, but it suggests that directly
maximizing information acquisition may be a useful algorithm design paradigm for online IL, and we hope to
explore this more deeply in future work.
5
Experiments
In this section, we validate our theoretical results empirically. We first provide a detailed overview of our
experimental setup, including the control and natural language tasks we consider, then present empirical
results for each task individually.
5.1
Experimental Setup
We evaluate the effect of horizon on the performance of LogLossBC in three environments. We begin by
describing our training and evaluation protocol (which is agnostic to the environment under consideration),
then provide details for each environment.
In each experiment, we begin with an expert policy π⋆(which is always a neural network; details below) and
construct an offline dataset by rolling out with it n times for H timesteps per episode. To train the imitator
policy bπ, we use the same architecture as the expert, but randomly initialize the weights and use stochastic
gradient descent with the Adam optimizer to minimize the LogLossBC objective for the offline dataset. We
repeat this entire process for varying values of H.
To evaluate the regret J(π⋆) −J(bπ) after training, we approximate the average reward of the imitator
policy bπ by selecting new random seeds and collecting n trajectories of length H by rolling out with bπ; we
approximate the average reward of the expert π⋆in the same fashion, and we also compute several auxiliary
performance measures (details below) that aim to capture the distance between bπ and π⋆. In all environments,
we normalize rewards so that the average reward of the expert is at most 1, in order to bring us to the
sparse reward setting in Section 1.1 and keep the range of the possible rewards constant as a function of the
(varying) horizon.
We consider four diverse environments, with the aim of evaluating LogLossBC in qualitatively different domains:
(i) Walker2d, a classical continuous control task from MuJoCo (Towers et al., 2023; Todorov et al., 2012)
where the learner attempts to make a stick figure-like agent walk to the right by controlling its joints; (ii)
Beamrider, a standard discrete-action RL task from the Atari suite (Bellemare et al., 2013), where the learner
attempts to play the game of Beamrider; (iii) Car, a top-down discrete car racing environment where the car
has to avoid obstacles to reach a goal, and (iv) Dyck, an autoregressive language generation task where the
agent is given a sequence of brackets in {{, }, [, ], (, )} and has to close all open brackets in the correct order.
We emphasize diversity in task selection in order to demonstrate the generality of our results, covering
discrete and continuous actions spaces, as well as both control and language generation. For some of the
environment (Walker2d, Beamrider), the task is intended to be “stateless”, in the sense that varying the horizon
H does not change the difficulty of the task itself (e.g., complexity of the expert policy π⋆), allowing for an
honest evaluation of the difficulty of the learning problem as we vary the horizon H. For other domains, such
18This phenomenon is also distinct from “active” online imitation learning algorithms (Sekhari et al., 2024) which can obtain
improved sampling complexity under strong distributional assumptions in the vein of active learning (Hanneke, 2014), but still
do not deliberately explore.
15

200
400
600
Horizon
−0.10
−0.05
0.00
0.05
0.10
Expected Regret
5 trajectories
10 trajectories
20 trajectories
50 trajectories
100 trajectories
500 trajectories
MuJoCo
(a)
100
200
300
Horizon
0.05
0.10
0.15
0.20
0.25
0.30
Expected Regret
5 trajectories
10 trajectories
50 trajectories
100 trajectories
200 trajectories
Atari
(b)
Figure 2: Dependence of expected regret on the horizon for multiple choices for the number of imita-
tor trajectories n.
(a) Continuous control environment Walker2d-v4.
(b) Discrete Atari environment
BeamriderNoFrameskip-v4. For both environments, increasing the horizon does not lead to a significant increase
in regret, as predicted by our theory.
as Dyck, horizon dependence is more nuanced, as here the capacity required to represent the expert grows
as the horizon increases; this manifests itself in our theoretical results through the realizability condition
(Assumption 1.1), which necessitates a more complex function class Π as H increases.
We now provide details for our experimental setup for each environment.
Walker2d.
We use the Gymnasium (Towers et al., 2023) environment Walker2d-v4, which has continuous
state and action spaces of dimensions 17 and 6 respectively. The agent is rewarded for moving to the right
and staying alive as well as being penalized for excessively forceful actions; because we vary the horizon H, in
order to make the comparison fair, we normalize the rewards so that our trained expert always has average
reward 1. Our expert is a depth-2 MLP with width 64. We use the Stable-Baselines3 (Raffin et al., 2021)
implementation of the Proximal Policy Optimization (PPO) algorithm (Schulman et al., 2017) with default
settings to train the expert for 500K steps. The policy’s action distribution is Gaussian, with the mean and
covariance determined by the MLP; we use this for computation of the logarithmic loss. For data collection,
we enforce a deterministic expert by always playing the mean of the Gaussian distribution produced by their
policy. Our imitator policy uses the same architecture as the expert policy, with the weights re-initialized
randomly. We train the imitator using the logarithmic loss by default, but as an ablation, we also evaluate
the effect of training with the mean-squared-error loss on the Euclidean norm over the actions. We train
using the Adam optimizer (Kingma and Ba, 2015) with a learning rate of 10−3 and a batch size of 128. We
stop training early based on the validation loss on a held out set of expert trajectories. Note that the expert
and imitator policies above are both stationary policies.
Beamrider.
We use the Gymnasium environment BeamRiderNoFrameskip-v4, which has 9 discrete actions and
a 210x160x3 image as the state; the rewards are computed as a function of how many enemies are destroyed.
As in the case of the previous setup, we account for the varying of H by normalizing expert rewards to be
1. Here we do not train our expert ourselves, but instead use the trained PPO agent provided by Raffin
(2020), which is a convolutional neural network. We use the same architecture for our imitator policy, with
the weights re-initialized randomly. Here, the expert (and imitator) policies map the observation to a point
on the probability simplex over actions, and so logarithmic loss computation is immediate. Similar to the case
of Walker2d, we enforce a deterministic expert for collecting trajectories by taking the action with maximal
probability. We then train our imitators using the same setup as in the Walker2d environment. As with
Walker2d, the expert and imitator here are both stationary policies.
16

2000
4000
6000
8000
10000
Number of Trajectories
0.0
0.2
0.4
0.6
0.8
1.0
Expected Regret
Dyck Language
horizon 10
horizon 20
horizon 30
horizon 40
(a)
10
20
30
40
Horizon
0.9
1.0
1.1
1.2
1.3
Log Prod Norm
Log Prod Norm
Car
Dyck
(b)
Figure 3: (a) Relationship between the number of expert trajectories and expected regret for the Dyck
environment multiple choices of horizon H. The expert is trained to produce valid Dyck words of length H,
and the imitator’s ability to generate a valid word is evaluated. We find that regret increases as a function of
H. (b) Logarithm of the product of weight matrix norms for the expert policy network as a function of H, for
Dyck and Car environments. The log-product-norm acts as a proxy for complexity for the class Π; we rescale
such that log-product-norm at H = 10 is 1.0 for both domains. For Dyck, we find that as H increases, the
complexity of Π required to represent the expert policy (as measured by the log-product-norm) also increases,
explaining the increasing regret in (a). However, the gain in log-product-norm for the Car domain is much
lower, which is in line with the fact that the regret for the Car domain exhibits only mild scaling with horizon.
Car.
We introduce a simple top-down navigation task where the agent is a “car” that always moves forward
by one step, but can take actions to move left, right, or remain in its lane to avoid obstacles and reach the
desired destination. There are M possible lanes. At timestep h ∈[H + 1], if the agent is in lane i ∈[M],
then the agent’s state is (i, h). We view the state space as a M × (H + 1) grid; a given point (i, j) in the grid
can be empty, or contain an obstacle, or contain the agent. The agent’s action space consists of 3 possible
actions: stay in the current lane ((i, h) 7→(i, h + 1)), move one step left ((i, h) 7→(i −1, h + 1)), or move one
step right ((i, h) 7→(i + 1, h + 1)). If the agent’s action causes it to collide with an obstacle or the boundary
of the grid, it is sent to an absorbing state. The agent gets a reward of 1 for reaching the goal state for the
first time, and a reward of 0 otherwise. When the agent occupies a state (i, h), it observes an image-based
observation xh showing the state of all lanes for V steps ahead where V is the size of the viewing field. At
the start of each episode, we randomly sample obstacles positions, the start position, and the goal position.
The goal can be reached after H actions, and it is always possible to reach the goal.
Dyck.
In addition to the RL environments above, we evaluate LogLossBC for autoregressive language
generation with transformers (cf. Appendix A.3), where the goal of the “agent” is to complete a valid word of
a given length in a Dyck language; this has emerged as a popular sandbox for understanding the nuances
of autoregressive text generation in theory (Yao et al., 2021; Hahn, 2020; Bhattamishra et al., 2020) and
empirically (Liu et al., 2022; Wen et al., 2024). We recall that a Dyck language Dyckk consists of 2k matched
symbols thought of as open and closed parentheses, with concatenations being valid words if the parentheses
are closed in the correct order. For example, if we define the space of characters as ‘()’, ‘[]’, and ‘{}’, then
‘([()]){}’ is a valid word, whereas ‘([)]’ and ‘((({}’ are not.
Our experiments use the Dyck language Dyck3. For our expert, we train an instance of GPT-2 small (Radford
et al., 2019) with 6 layers, 3 heads, and 96 hidden dimensions from scratch to produce valid Dyck words. In
particular, the training dataset consists of random Dyck prefixes that require exactly H actions (symbols) to
complete. To imitate this expert, we train a GPT-2 small model with the same architecture, but with randomly
initialized weights on an offline dataset of sequences generated by the expert. We assign a reward 1 to each
trajectory if the generated word is valid, and assign reward 0 otherwise. We use Adam optimization for training,
with our experts trained for 40K iterations in order to ensure their quality. Note that in this environment, the
17

5000
10000
15000
20000
Number of Trajectories
0.00
0.05
0.10
0.15
0.20
0.25
0.30
Expected Regret
Car (Log Loss)
horizon 10
horizon 20
horizon 30
horizon 40
(a)
5000
10000
15000
20000
Number of Trajectories
0.00
0.05
0.10
0.15
0.20
0.25
0.30
Expected Regret
Car (MSE Loss)
horizon 10
horizon 20
horizon 30
horizon 40
(b)
Figure 4: Dependence of expected regret on the number of expert trajectories for Car environment under varying
values for horizon H for log-loss (a) and mean-squared loss (b). The expert policy network is trained on a set of
2×104 episodes generated by an optimal policy via behavior cloning. We use LogLossBC to train imitator policy
for varying values of the horizon H and number of trajectories n. For both losses, we find that the expected
regret goes down as the number of expert trajectories increases, but degrades slightly as a function of H.
expert and imitator policies are non-stationary, but use parameter sharing via the transformer architecture.
5.2
Results
We summarize our main findings below.
Effect of horizon on regret.
Figures 1 and 2 plot the relationship between expected regret and the number
of expert trajectories for the Walker2d (MuJoCo), and BeamriderNoFrameskip (Atari) environments, as the
horizon H is varied from 50 to 500. For both environments, we find regret is largely independent of the horizon,
consistent with our theoretical results. In fact, in the case of BeamriderNoFrameskip, we find that increasing
the horizon leads to better regret. To understand this, note that our theory provides horizon-agnostic
upper bounds independent of the environment. Our lower bounds are constructed for specific worst-case
environments, and not rule out the possibility of improved performance with longer horizons environments
with favorable structure. We conjecture that this phenomenon is related to the fact that longer horizons yield
fundamentally more data, as the total number of state-action pairs in the expert dataset is equal to nH.19
Figure 3(a) plots our findings for the Dyck environment. Here, we see that with the number of trajectories n
fixed, regret does increase with H, which might appear to contradict our theory at first glance. However, we
note that the policy class itself must become larger as H increases, as the task itself becomes more difficult
(equivalently, the supervised learning error D2
H
 Pπ⋆, Pbπ
must grow with H). As a result, the regret is not
expected to be independent of H for this environment, in spit of parameter sharing. To verify whether
supervised learning error is indeed the cause for horizon dependence for Dyck, Figure 3(b) plots the logarithm
of the product of the Frobenius norms of the weight matrices of the expert for varying values of H, as a
proxy for supervised learning performance (Bartlett et al., 2017; Golowich et al., 2018).20 We find that the
log-product-norms do in fact grow with H, consistent with the fact that the regret grows with H in this case.
For the Car environment, we observe similar behavior to the Dyck environment, visualized in Figure 4. We
find that performance degrades slightly as a function of the horizon H, but that this increase in regret can be
explained by an increase in the log-product-norm (Figure 3(b)). However, the effect is mild compared to Dyck.
19For example, if we repeat a fixed contextual bandit instance H times across the horizon and train a stationary policy, it is
clear that regret should decrease with H under sparse rewards. Less trivial instances where increasing horizon provably leads to
better performance are known in some special cases (Tu et al., 2022a).
20We only include log-product-norm plots for Dyck and Car because for the other environments (Walker2d and Beamrider-
NoFrameskip), we do not change the expert as a function of H.
18

0
200
400
Number of Trajectories
−0.10
−0.05
0.00
0.05
0.10
Expected Regret
Log Loss
0
200
400
Number of Trajectories
−0.10
−0.05
0.00
0.05
0.10
Expected Regret
MSE Loss
horizon 50
horizon 100
horizon 200
horizon 300
horizon 400
horizon 500
Figure 5: Dependence of expected regret on the number of expert trajectories for continuous control
environment Walker2d-v4 under varying choices for horizon H. (Left) Behavior cloning with logarithmic loss
(LogLossBC); (Right) Behavior cloning with mean squared error (MSE) Loss. Both losses lead to similar
performance for this environment, possibly due to Gaussian policy parameterization.
0.2
0.4
D2
H to Expert
1200
1300
1400
1500
Expected Reward
−5
0
Validation Loss
MuJoCo (Log Loss)
(a)
0.2
0.4
D2
H to Expert
800
1000
1200
1400
Expected Reward
0.01
0.02
0.03
Validation Loss
MuJoCo (MSE Loss)
(b)
Figure 6: Evaluation of the quality of (i) Hellinger distance D2
H
 Pπ⋆, Pbπ
, and (ii) validation loss as a proxy for
rollout reward. We plot Hellinger distance and validation loss against mean reward for a over a single training
run for Walker2d environment with H = 500 and n = 500. (a) Results for LogLossBC, where the validation
loss and Hellinger distance D2
H are highly correlated, and serve as good proxies for the expected reward of the
policy. (b) Results for MSE loss, where the validation loss is less well correlated with the expected reward
(note the cluster in the upper left hand corner), but the Hellinger distance D2
H remains a good proxy.
Comparison between log loss and square loss.
As an ablation, Figures 4 and 5 compare LogLossBC
to the original behavior cloning objective of Pomerleau (1988), which uses the mean squared error (MSE)
to regress expert actions to observations in the offline dataset. Focusing on the Walker2d environment
(Figure 5) and Car environment (Figure 4) (other environments presented difficulties in training21), we find
that performance with the MSE loss is comparable to that of the logarithmic loss. For Walker2d, a possible
explanation is that under the Gaussian policy parameterization we use, the MSE loss is the same as the
logarithmic loss up to state-dependent heteroskedasticity.22 Another possible explanation is that this is an
instance of the phenomenon described in Section 6.1.
21In particular, we attempted a similar result in the Atari environment, using MSE loss being between vectors on the probability
simplex over |A| actions. For MSE loss, we found that the imitator did not train, in the sense that even with 500 expert
trajectories, the performance of the cloner did not improve. We suspect this was due to numerical instability in optimization for
the MSE loss in this setup or a failure of hyperparameter optimization.
22In theory, the MSE loss can still underperform the logarithmic loss when the heteroskedasticity is severe (Foster and
Krishnamurthy, 2021), but this may not manifest for this environment.
19

Relationship between regret and Hellinger distance to expert.
Finally, we directly evaluate the
quality of (i) Hellinger distance D2
H
 Pπ⋆, Pbπ
, and (ii) validation loss as proxies for rollout performance. We
estimate the Hellinger distance using sample trajectories. Figure 6 displays our findings for Walker2d with
H = n = 500, where we observe that both metrics, particularly the Hellinger distance, are well correlated with
rollout performance, as measured by average reward. In Figure 6a, we see that under LogLossBC, Hellinger
distance and validation loss are highly correlated with each other, and negatively correlated with expected
reward, thereby acting as excellent proxies for rollout performance. Meanwhile, in Figure 6b, we find that
under behavior cloning with the MSE loss, validation error is less well correlated with the expected reward of
the imitator policy, as evinced by the cluster in the upper left corner, where there are policies with roughly
the same validation loss, but variable expected reward. On the other hand, the Hellinger distance D2
H still
appears to predict the performance of the policy well, as is consistent with our theoretical results.
6
Discussion
We conclude with additional technical remarks and directions for future research.
6.1
When is Indicator-Loss Behavior Cloning Suboptimal?
Our discussion in Section 1.1.1 suggests that indicator-loss behavior cloning, which solves bπ = arg minπ∈Π bLbc(π) :=
Pn
i=1
1
H
PH
h=1 I{πh(xi
h) ̸= ai
h}, can have suboptimal horizon dependence compared to LogLossBC. This turns
out to be a subtle point. Suppose that π⋆is deterministic, that Π exactly satisfies realizability in the sense
that π⋆∈Π, and that Π only contains deterministic policies. In this case, we observe that bLbc(bπ) = 0 (i.e.,
bπ agrees with π⋆on every instance in the dataset). Consequently, bπ can also be viewed as minimizing an
empirical version of the trajectory-wise loss in Eq. (8), i.e.
n
X
i=1
I{∃h : bπh(x
i
h) ̸= a
i
h} = 0.
(17)
From here, a standard uniform convergence argument implies that ρ(π⋆∥bπ) ≲log(|Π|δ−1)
n
, and by combining
this with Eq. (9), we obtain the following result.
Proposition 6.1. For any deterministic expert π⋆∈Π, the indicator loss behavior cloning policy bπ =
arg minπ∈Π bLbc(π) ensures that with probability at least 1 −δ,
J(π⋆) −J(bπ) ≤O(R) · log(|Π|δ−1)
n
.
This result, which shows that indicator-loss BC attains a similar horizon-independent rate toLogLossBC under
the conditions of Corollary 2.1, is novel to our knowledge. While this would seem to suggest that indicator-loss
BC can match the performance of LogLossBC, there are number of important caveats.
First, Proposition 6.1 is not robust to optimization errors or misspecification errors. For example, if bπ only
minimizes the indicator loss bLbc(π) up to error εopt, i.e.
bLbc(bπ) ≤inf
π∈Π
bLbc(π) + εopt · n,
then by adapting the construction of Ross and Bagnell (2010), one can show that in general the algorithm
can have J(π⋆) −J(bπ) ≥RH · εopt, meaning it no longer achieves horizon dependence. Indeed, in this case,
it is no longer possible to translate the bound on bLbc(bπ) to a bound on the trajectory-level loss in Eq. (17)
without incurring an H factor. On the other hand, as we show in Appendix C, if the LogLossBC objective is
solved only approximately, i.e.
n
X
i=1
H
X
h=1
log

1
bπ(ai
h | xi
h)

≤inf
π∈Π
n
X
i=1
H
X
h=1
log

1
π(ai
h | xi
h)

+ εopt · n,
20

the regret of the algorithm degrades only to J(π⋆) −J(bπ) ≲R ·

log(|Π|δ−1)
n
+ εopt

, and thus remains
horizon-independent. Similar remarks apply to the case of misspecification. Of course, perhaps the greatest
advantage of LogLossBC is that it readily supports stochastic policies, and is far more practical to implement.
6.2
The Role of Misspecification
This paper (for both deterministic and stochastic experts) focuses on the realizable setting in which π⋆∈Π. It is
natural to ask how the role of horizon in imitation learning changes under misspecification. This is a subtle issue,
as there are various incomparable notions of misspecification error which can lead to different forms of horizon
dependence. For example, for deterministic experts, if Π is misspecified in the sense that infπ∈Π Lbc(π) ≤εapx,
the indicator-loss behavior cloning algorithm in Eq. (1) achieves J(π⋆)−J(bπ) ≲RH·

log(|Π|δ−1)
n
+ εapx

, which
is tight in general. In other words, the dependence on εapx is not horizon-independent. On the other hand, as we
show in Appendix C, if we assume that infπ∈Π Dχ2 Pπ⋆∥Pπ
≤εapx, a stronger notion of misspecification error,
then LogLossBC achieves a horizon-independent guarantee of the form J(π⋆) −J(bπ) ≲R ·

log(|Π|δ−1)
n
+ εapx

.
We leave a detailed investigation of tradeoffs between misspecification and horizon (as well as interplay with
online versus offline IL) for future work; by giving the first horizon-independent treatment for the realizable
setting, we hope that our results can serve as a starting point.
6.3
Conclusion and Future Work
Our results clarify the role of horizon in offline and online imitation learning, and show that—at least under
standard assumptions in theoretical research into imitation learning—the gap between online and offline IL is
smaller than previously thought. Instabilities of offline IL (Block et al., 2024a) and benefits of online IL (Ross
et al., 2013) may indeed arise in practice, but existing assumptions in theoretical research on imitation learning
appear be too coarse to give insights into the true nature of these phenomena, highlighting the need to develop
a fine-grained, problem-dependent understanding of algorithms and complexity for IL. To this end, natural
directions include (i) Building upon the initial results in Section 4, and investigating new mechanisms such as
exploration and representational benefits through which online IL can improve over offline IL; (ii) Developing
and analyzing imitation learning algorithms under control-theoretic assumptions that more directly capture
practical notions of instability (Pfrommer et al., 2022; Tu et al., 2022b; Block et al., 2024a,b); (iii) Developing a
more refined theory in the context of language models, via the connection in Appendix A.3. For the latter two
directions, an important question is to understand whether the notion of supervised learning error D2
H
 Pbπ, Pπ⋆
we consider is a suitable proxy for real-world performance, or whether more refined notions are required.
Our results also highlight the importance of developing learning-theoretic foundations for imitation learning
that support general, potentially neural function classes. To this end, a natural question left open by our work
is to develop complexity measures (analogous to VC dimension or Rademacher complexity) that characterize
the minimax sample complexity of online and offline IL for any policy class.
Additional Results
Secondary results deferred to the appendix for space include (i) examples and additional guarantees for
LogLossBC and LogLossDagger (Appendix C); and (ii) additional lower bounds and results concerning the
tightness of Theorems 2.2 and 3.1 (Appendix G).
Acknowledgements
We thank Jordan Ash, Audrey Huang, Akshay Krishnamurthy, Max Simchowitz, and Cyril Zhang for many
helpful discussions.
21

References
Pieter Abbeel and Andrew Y Ng. Apprenticeship learning via inverse reinforcement learning. In Proceedings
of the twenty-first international conference on Machine learning, page 1, 2004.
Alekh Agarwal, Nan Jiang, and Sham M Kakade. Reinforcement learning: Theory and algorithms. 2019.
Alex Ayoub, Kaiwen Wang, Vincent Liu, Samuel Robertson, James McInerney, Dawen Liang, Nathan Kallus,
and Csaba Szepesvári. Switching the loss reduces the cost in batch reinforcement learning. arXiv preprint
arXiv:2403.05385, 2024.
Mohammad Gheshlaghi Azar, Ian Osband, and Rémi Munos. Minimax regret bounds for reinforcement
learning. In International Conference on Machine Learning, pages 263–272, 2017.
Gregor Bachmann and Vaishnavh Nagarajan.
The pitfalls of next-token prediction.
arXiv preprint
arXiv:2403.06963, 2024.
Mayank Bansal, Alex Krizhevsky, and Abhijit Ogale. Chauffeurnet: Learning to drive by imitating the best
and synthesizing the worst. arXiv preprint arXiv:1812.03079, 2018.
Peter L. Bartlett, Dylan J. Foster, and Matus J. Telgarsky. Spectrally-normalized margin bounds for neural
networks. In Advances in Neural Information Processing Systems, 2017.
Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning environment: An
evaluation platform for general agents. Journal of Artificial Intelligence Research, 47:253–279, 2013.
Alina Beygelzimer, Varsha Dani, Tom Hayes, John Langford, and Bianca Zadrozny. Error limiting reductions
between classification tasks. In Proceedings of the 22nd international conference on Machine learning,
pages 49–56, 2005.
Alina Beygelzimer, John Langford, Lihong Li, Lev Reyzin, and Robert Schapire. Contextual bandit algorithms
with supervised learning guarantees. In Proceedings of the Fourteenth International Conference on Artificial
Intelligence and Statistics, pages 19–26, 2011.
Satwik Bhattamishra, Kabir Ahuja, and Navin Goyal. On the ability and limitations of transformers to
recognize formal languages. In Proceedings of the 2020 Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 7096–7116, 2020.
Adam Block, Dylan J Foster, Akshay Krishnamurthy, Max Simchowitz, and Cyril Zhang. Butterfly effects
of sgd noise: Error amplification in behavior cloning and autoregression. International Conference on
Learning Representations (ICLR), 2024a.
Adam Block, Ali Jadbabaie, Daniel Pfrommer, Max Simchowitz, and Russ Tedrake. Provable guarantees
for generative behavior cloning: Bridging low-level stability and high-level behavior. Advances in Neural
Information Processing Systems, 36, 2024b.
Mariusz Bojarski, Davide Del Testa, Daniel Dworakowski, Bernhard Firner, Beat Flepp, Prasoon Goyal,
Lawrence D Jackel, Mathew Monfort, Urs Muller, and Jiakai Zhang. End to end learning for self-driving
cars. arXiv preprint arXiv:1604.07316, 2016.
Kiante Brantley, Wen Sun, and Mikael Henaff. Disagreement-regularized imitation learning. In International
Conference on Learning Representations, 2019.
Mark Braverman, Xinyi Chen, Sham Kakade, Karthik Narasimhan, Cyril Zhang, and Yi Zhang. Calibration,
entropy rates, and memory in language models. In International Conference on Machine Learning, pages
1089–1099. PMLR, 2020.
Clément L Canonne. A short note on learning discrete distributions. arXiv preprint arXiv:2002.11457, 2020.
Nicolò Cesa-Bianchi and Gábor Lugosi. Prediction, Learning, and Games. Cambridge University Press, New
York, NY, USA, 2006. ISBN 0521841089.
22

Jonathan D Chang, Masatoshi Uehara, Dhruv Sreenivas, Rahul Kidambi, and Wen Sun. Mitigating covariate
shift in imitation learning via offline data without great coverage. Advances in Neural Information Processing
Systems, 2021.
Jonathan D Chang, Kiante Brantley, Rajkumar Ramamurthy, Dipendra Misra, and Wen Sun. Learning to
generate better than your llm. arXiv preprint arXiv:2306.11816, 2023.
Ching-An Cheng and Byron Boots. Convergence of value aggregation for imitation learning. In International
Conference on Artificial Intelligence and Statistics, pages 1801–1809. PMLR, 2018.
Ching-An Cheng, Xinyan Yan, Evangelos Theodorou, and Byron Boots. Accelerating imitation learning with
predictive models. In The 22nd International Conference on Artificial Intelligence and Statistics, pages
3187–3196. PMLR, 2019.
Ching-An Cheng, Andrey Kolobov, and Alekh Agarwal. Policy improvement via imitation of multiple oracles.
Advances in Neural Information Processing Systems, 33:5587–5598, 2020.
Cheng Chi, Siyuan Feng, Yilun Du, Zhenjia Xu, Eric Cousineau, Benjamin Burchfiel, and Shuran Song.
Diffusion policy: Visuomotor policy learning via action diffusion. arXiv preprint arXiv:2303.04137, 2023.
David L Donoho and Richard C Liu. Geometrizing rates of convergence, II. The Annals of Statistics, pages
633–667, 1991.
Jesse Farebrother, Jordi Orbay, Quan Vuong, Adrien Ali Taïga, Yevgen Chebotar, Ted Xiao, Alex Irpan,
Sergey Levine, Pablo Samuel Castro, and Aleksandra Faust. Stop regressing: Training value functions via
classification for scalable deep rl. arXiv preprint arXiv:2403.03950, 2024.
Chelsea Finn, Tianhe Yu, Tianhao Zhang, Pieter Abbeel, and Sergey Levine. One-shot visual imitation
learning via meta-learning. In Conference on robot learning, pages 357–368. PMLR, 2017.
Pete Florence, Corey Lynch, Andy Zeng, Oscar A Ramirez, Ayzaan Wahid, Laura Downs, Adrian Wong,
Johnny Lee, Igor Mordatch, and Jonathan Tompson. Implicit behavioral cloning. In Conference on Robot
Learning, pages 158–168. PMLR, 2022.
Dylan J Foster and Akshay Krishnamurthy. Efficient first-order contextual bandits: Prediction, allocation,
and triangular discrimination. Neural Information Processing Systems (NeurIPS), 2021.
Dylan J Foster and Alexander Rakhlin. Foundations of reinforcement learning and interactive decision making.
arXiv preprint arXiv:2312.16730, 2023.
Dylan J Foster, Sham M Kakade, Jian Qian, and Alexander Rakhlin. The statistical complexity of interactive
decision making. arXiv preprint arXiv:2112.13487, 2021.
Dylan J Foster, Alexander Rakhlin, Ayush Sekhari, and Karthik Sridharan. On the complexity of adversarial
decision making. Advances in Neural Information Processing Systems, 35:35404–35417, 2022.
Dylan J Foster, Yanjun Han, Jian Qian, and Alexander Rakhlin. Online estimation via offline estimation: An
information-theoretic framework. arXiv preprint arXiv:2404.10122, 2024.
Noah Golowich, Alexander Rakhlin, and Ohad Shamir. Size-independent sample complexity of neural
networks. In Conference On Learning Theory, pages 297–299. PMLR, 2018.
Saurabh Gupta, James Davidson, Sergey Levine, Rahul Sukthankar, and Jitendra Malik. Cognitive mapping
and planning for visual navigation. In Proceedings of the IEEE conference on computer vision and pattern
recognition, pages 2616–2625, 2017.
Michael Hahn. Theoretical limitations of self-attention in neural sequence models. Transactions of the
Association for Computational Linguistics, 8:156–171, 2020.
Steve Hanneke. Theory of disagreement-based active learning. Foundations and Trends® in Machine
Learning, 7(2-3):131–309, 2014.
23

Aaron Havens and Bin Hu. On imitation learning of linear control policies: Enforcing stability and robustness
constraints via lmi conditions. In 2021 American Control Conference (ACC), pages 882–887. IEEE, 2021.
Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning. Advances in neural information
processing systems, 29, 2016.
Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text degeneration.
arXiv preprint arXiv:1904.09751, 2019.
Ahmed Hussein, Mohamed Medhat Gaber, Eyad Elyan, and Chrisina Jayne. Imitation learning: A survey of
learning methods. ACM Computing Surveys (CSUR), 50(2):1–35, 2017.
Ahmed Hussein, Eyad Elyan, Mohamed Medhat Gaber, and Chrisina Jayne. Deep imitation learning for 3d
navigation tasks. Neural computing and applications, 29:389–404, 2018.
Borja Ibarz, Jan Leike, Tobias Pohlen, Geoffrey Irving, Shane Legg, and Dario Amodei. Reward learning
from human preferences and demonstrations in atari. Advances in neural information processing systems,
31, 2018.
Nan Jiang and Alekh Agarwal. Open problem: The dependence of sample complexity lower bounds on
planning horizon. In Conference On Learning Theory, pages 3395–3398. PMLR, 2018.
Chi Jin, Qinghua Liu, and Sobhan Miryoosefi. Bellman eluder dimension: New rich classes of RL problems,
and sample-efficient algorithms. Neural Information Processing Systems, 2021.
Liyiming Ke, Sanjiban Choudhury, Matt Barnes, Wen Sun, Gilwoo Lee, and Siddhartha Srinivasa. Imitation
learning as f-divergence minimization. In Algorithmic Foundations of Robotics XIV: Proceedings of the
Fourteenth Workshop on the Algorithmic Foundations of Robotics 14, pages 313–329. Springer, 2021a.
Liyiming Ke, Jingqiang Wang, Tapomayukh Bhattacharjee, Byron Boots, and Siddhartha Srinivasa. Grasping
with chopsticks: Combating covariate shift in model-free imitation learning for fine manipulation. In 2021
IEEE International Conference on Robotics and Automation (ICRA), pages 6185–6191. IEEE, 2021b.
Michael Kelly, Chelsea Sidrane, Katherine Driggs-Campbell, and Mykel J Kochenderfer. Hg-dagger: Interactive
imitation learning with human experts. In 2019 International Conference on Robotics and Automation
(ICRA), pages 8077–8083. IEEE, 2019.
Beomjoon Kim, Amir-massoud Farahmand, Joelle Pineau, and Doina Precup.
Learning from limited
demonstrations. Advances in Neural Information Processing Systems, 26, 2013.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. International Conference
on Learning Representations, 2015.
Michael Laskey, Jonathan Lee, Roy Fox, Anca Dragan, and Ken Goldberg. Dart: Noise injection for robust
imitation learning. In Conference on robot learning, pages 143–156. PMLR, 2017.
Yann LeCun. Do large language models need sensory grounding for meaning and understanding. In Workshop
on Philosophy of Deep Learning, NYU Center for Mind, Brain, and Consciousness and the Columbia
Center for Science and Society, 2023.
Yichen Li and Chicheng Zhang. On efficient online imitation learning via classification. Advances in Neural
Information Processing Systems, 35:32383–32397, 2022.
Yunzhu Li, Jiaming Song, and Stefano Ermon.
Infogail: Interpretable imitation learning from visual
demonstrations. Advances in neural information processing systems, 30, 2017.
Bingbin Liu, Jordan T Ash, Surbhi Goel, Akshay Krishnamurthy, and Cyril Zhang. Transformers learn
shortcuts to automata. In The Eleventh International Conference on Learning Representations, 2022.
Daniel Pfrommer, Thomas Zhang, Stephen Tu, and Nikolai Matni. Tasil: Taylor series imitation learning.
Advances in Neural Information Processing Systems, 35:20162–20174, 2022.
Yury Polyanskiy and Yihong Wu. Lecture notes on information theory. 2014.
24

Dean A Pomerleau. Alvinn: An autonomous land vehicle in a neural network. Advances in neural information
processing systems, 1, 1988.
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models
are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.
Antonin Raffin. Rl baselines3 zoo. https://github.com/DLR-RM/rl-baselines3-zoo, 2020.
Antonin Raffin, Ashley Hill, Adam Gleave, Anssi Kanervisto, Maximilian Ernestus, and Noah Dormann.
Stable-baselines3: Reliable reinforcement learning implementations. Journal of Machine Learning Research,
22(268):1–8, 2021. URL http://jmlr.org/papers/v22/20-1364.html.
Nived Rajaraman, Lin Yang, Jiantao Jiao, and Kannan Ramchandran. Toward the fundamental limits of
imitation learning. Advances in Neural Information Processing Systems, 33:2914–2924, 2020.
Nived Rajaraman, Yanjun Han, Lin Yang, Jingbo Liu, Jiantao Jiao, and Kannan Ramchandran. On the
value of interaction and function approximation in imitation learning. Advances in Neural Information
Processing Systems, 34:1325–1336, 2021a.
Nived Rajaraman, Yanjun Han, Lin F Yang, Kannan Ramchandran, and Jiantao Jiao. Provably breaking the
quadratic error compounding barrier in imitation learning, optimally. arXiv preprint arXiv:2102.12948,
2021b.
Stéphane Ross and Drew Bagnell. Efficient reductions for imitation learning. In Proceedings of the thirteenth
international conference on artificial intelligence and statistics, pages 661–668. JMLR Workshop and
Conference Proceedings, 2010.
Stephane Ross and J Andrew Bagnell. Reinforcement and imitation learning via interactive no-regret learning.
arXiv preprint arXiv:1406.5979, 2014.
Stéphane Ross, Geoffrey Gordon, and Drew Bagnell. A reduction of imitation learning and structured
prediction to no-regret online learning. In Proceedings of the fourteenth international conference on artificial
intelligence and statistics, pages 627–635. JMLR Workshop and Conference Proceedings, 2011.
Stéphane Ross, Narek Melik-Barkhudarov, Kumar Shaurya Shankar, Andreas Wendel, Debadeepta Dey,
J Andrew Bagnell, and Martial Hebert. Learning monocular reactive uav control in cluttered natural
environments. In 2013 IEEE international conference on robotics and automation, pages 1765–1772. IEEE,
2013.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization
algorithms. arXiv preprint arXiv:1707.06347, 2017.
Ayush Sekhari, Karthik Sridharan, Wen Sun, and Runzhe Wu. Selective sampling and imitation learning via
online regression. Advances in Neural Information Processing Systems, 36, 2024.
Nur Muhammad Shafiullah, Zichen Cui, Ariuntuya Arty Altanzaya, and Lerrel Pinto. Behavior transformers:
Cloning k modes with one stone. Advances in neural information processing systems, 35:22955–22968, 2022.
Jonathan Spencer, Sanjiban Choudhury, Arun Venkatraman, Brian Ziebart, and J Andrew Bagnell. Feedback
in imitation learning: The three regimes of covariate shift. arXiv preprint arXiv:2102.02872, 2021.
Wen Sun, Arun Venkatraman, Geoffrey J Gordon, Byron Boots, and J Andrew Bagnell. Deeply aggrevated:
Differentiable imitation learning for sequential prediction. In International conference on machine learning,
pages 3309–3318. PMLR, 2017.
Gokul Swamy, Sanjiban Choudhury, J Andrew Bagnell, and Steven Wu. Of moments and matching: A
game-theoretic framework for closing the imitation gap. In International Conference on Machine Learning,
pages 10022–10032. PMLR, 2021.
Gokul Swamy, Nived Rajaraman, Matt Peng, Sanjiban Choudhury, J Bagnell, Steven Z Wu, Jiantao Jiao,
and Kannan Ramchandran. Minimax optimal online imitation learning via replay estimation. Advances in
Neural Information Processing Systems, 35:7077–7088, 2022.
25

Umar Syed and Robert E Schapire. A game-theoretic approach to apprenticeship learning. Advances in
neural information processing systems, 20, 2007.
Umar Syed and Robert E Schapire. A reduction from apprenticeship learning to classification. Advances in
neural information processing systems, 23, 2010.
Umar Syed, Michael Bowling, and Robert E Schapire. Apprenticeship learning using linear programming. In
Proceedings of the 25th international conference on Machine learning, pages 1032–1039, 2008.
Daniil Tiapkin, Denis Belomestny, Daniele Calandriello, Eric Moulines, Alexey Naumov, Pierre Perrault,
Michal Valko, and Pierre Menard. Demonstration-regularized rl. International Conference on Learning
Representations, 2024.
Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In 2012
IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 5026–5033. IEEE, 2012. doi:
10.1109/IROS.2012.6386109.
Mark Towers, Jordan K. Terry, Ariel Kwiatkowski, John U. Balis, Gianluca de Cola, Tristan Deleu, Manuel
Goulão, Andreas Kallinteris, Arjun KG, Markus Krimmel, Rodrigo Perez-Vicente, Andrea Pierré, Sander
Schulhoff, Jun Jet Tai, Andrew Tan Jin Shen, and Omar G. Younis. Gymnasium, March 2023. URL
https://zenodo.org/record/8127025.
Stephen Tu, Roy Frostig, and Mahdi Soltanolkotabi. Learning from many trajectories. arXiv preprint
arXiv:2203.17193, 2022a.
Stephen Tu, Alexander Robey, Tingnan Zhang, and Nikolai Matni. On the sample complexity of stability
constrained imitation learning. In Learning for Dynamics and Control Conference, pages 180–191. PMLR,
2022b.
Sara A. van de Geer. Empirical Processes in M-Estimation. Cambridge University Press, 2000.
Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Michaël Mathieu, Andrew Dudzik, Junyoung Chung,
David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmaster level in starcraft ii using
multi-agent reinforcement learning. Nature, 575(7782):350–354, 2019.
Martin J Wainwright. High-dimensional statistics: A non-asymptotic viewpoint, volume 48. Cambridge
University Press, 2019.
Kaiwen Wang, Kevin Zhou, Runzhe Wu, Nathan Kallus, and Wen Sun. The benefits of being distributional:
Small-loss bounds for reinforcement learning. Advances in Neural Information Processing Systems, 36,
2023.
Kaiwen Wang, Owen Oertell, Alekh Agarwal, Nathan Kallus, and Wen Sun.
More benefits of being
distributional: Second-order bounds for reinforcement learning. arXiv preprint arXiv:2402.07198, 2024.
Ruosong Wang, Simon S Du, Lin F Yang, and Sham M Kakade. Is long horizon reinforcement learning more
difficult than short horizon reinforcement learning? Neural Information Processing Systems (NeurIPS),
2020.
Kaiyue Wen, Yuchen Li, Bingbin Liu, and Andrej Risteski. Transformers are uninterpretable with myopic
methods: a case study with bounded dyck grammars. Advances in Neural Information Processing Systems,
36, 2024.
David Williams. Probability with martingales. Cambridge university press, 1991.
Wing Hung Wong and Xiaotong Shen. Probability inequalities for likelihood ratios and convergence rates of
sieve mles. The Annals of Statistics, pages 339–362, 1995.
Xinyan Yan, Byron Boots, and Ching-An Cheng. Explaining fast improvement in online imitation learning.
In Uncertainty in Artificial Intelligence, pages 1874–1884. PMLR, 2021.
Shunyu Yao, Binghui Peng, Christos Papadimitriou, and Karthik Narasimhan. Self-attention networks can
process bounded hierarchical languages. In Proceedings of the 59th Annual Meeting of the Association for
26

Computational Linguistics and the 11th International Joint Conference on Natural Language Processing
(Volume 1: Long Papers), pages 3770–3785, 2021.
Andrea Zanette and Emma Brunskill. Tighter problem-dependent regret bounds in reinforcement learning
without domain knowledge using value function bounds. In International Conference on Machine Learning,
pages 7304–7312. PMLR, 2019.
Tianhao Zhang, Zoe McCarthy, Owen Jow, Dennis Lee, Xi Chen, Ken Goldberg, and Pieter Abbeel. Deep
imitation learning for complex manipulation tasks from virtual reality teleoperation. In 2018 IEEE
international conference on robotics and automation (ICRA), pages 5628–5635. IEEE, 2018.
Tong Zhang. From ϵ-entropy to KL-entropy: Analysis of minimum information complexity density estimation.
The Annals of Statistics, 34(5):2180–2210, 2006.
Zihan Zhang, Xiangyang Ji, and Simon Du.
Is reinforcement learning more difficult than bandits?
a
near-optimal algorithm escaping the curse of horizon. In Conference on Learning Theory, pages 4528–4531.
PMLR, 2021.
Zihan Zhang, Xiangyang Ji, and Simon Du. Horizon-free reinforcement learning in polynomial time: the
power of stationary policies. In Conference on Learning Theory, pages 3858–3904. PMLR, 2022.
Heyang Zhao, Jiafan He, Dongruo Zhou, Tong Zhang, and Quanquan Gu. Variance-dependent regret bounds
for linear bandits and reinforcement learning: Adaptivity and computational efficiency. In The Thirty Sixth
Annual Conference on Learning Theory, pages 4977–5020. PMLR, 2023.
Runlong Zhou, Zhang Zihan, and Simon Shaolei Du. Sharp variance-dependent bounds in reinforcement
learning: Best of both worlds in stochastic and deterministic environments. In International Conference on
Machine Learning, pages 42878–42914. PMLR, 2023.
Brian D Ziebart, Andrew L Maas, J Andrew Bagnell, and Anind K Dey.
Maximum entropy inverse
reinforcement learning. In Aaai, volume 8, pages 1433–1438. Chicago, IL, USA, 2008.
27

Contents of Appendix
A Additional Related Work
29
A.1 Theory of Imitation Learning and Reinforcement Learning . . . . . . . . . . . . . . . . . . . .
29
A.2 Empirical Research on Imitation Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
30
A.3 Autoregressive Language Modeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
31
B Technical Tools
31
B.1 Tail Bounds . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
31
B.2
Information Theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
32
B.3
Reinforcement Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
32
B.4
Maximum Likelihood Estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
33
I
Proofs and Supporting Results
35
C Examples and Supporting Results from Section 2 and Section 3
35
C.1 General Guarantees and Examples for Log-Loss Behavior Cloning . . . . . . . . . . . . . . . .
35
C.2 Online IL Framework and Sample Complexity Bounds for Log-Loss Dagger
. . . . . . . . . .
38
D Proofs from Section 2
41
D.1 Proof of Theorem 2.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
41
D.2 Proof of Theorem 2.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
43
E Proofs from Section 3
45
E.1
Proof of Theorem 3.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
45
E.2
Proof of Theorem 3.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
52
E.3
Additional Proofs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
54
F Proofs from Section 4
55
F.1
Proof of Proposition 4.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
55
F.2
Background and Proof for Proposition 4.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
55
F.3
Proof of Proposition 4.3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
57
II
Additional Results
58
G Additional Lower Bounds
58
G.1 Lower Bounds for Online Imitation Learning in Active Interaction Model
. . . . . . . . . . .
58
G.2 An Instance-Dependent Lower Bound for Stochastic Experts . . . . . . . . . . . . . . . . . . .
60
G.3 Tightness of the Hellinger Distance Reduction . . . . . . . . . . . . . . . . . . . . . . . . . . .
63
28

A
Additional Related Work
A.1
Theory of Imitation Learning and Reinforcement Learning
Classical theoretical works in imitation learning, beginning from the work of Ross and Bagnell (2010) observes
that behavior cloning (for the specific indicator loss in Eq. (1)) can incur quadratic dependence on horizon,
and shows that online interaction, via algorithms like Dagger and Aggrevate, can obtain improved sample
complexity under recoverability-type conditions (Ross and Bagnell, 2010; Ross et al., 2011; Ross and Bagnell,
2014; Sun et al., 2017). Further works along this line include Cheng and Boots (2018); Cheng et al. (2020,
2019); Yan et al. (2021); Spencer et al. (2021).
These papers can be thought of as supervised learning reduction, in the sense that—in the vein of Eq. (2)—they
guarantee that the imitation learning performance is controlled by an appropriate notion of supervised learning
performance. Notably, this holds for any policy bπ, which means that in practice, the rollout performance
is good whenever supervised learning succeeds, even if we do not necessarily have a provable guarantee for
the generalization of bπ (e.g., for neural networks, where understanding generalization is an active area of
research). However, as noted throughout this paper and elsewhere (Rajaraman et al., 2020, 2021a,b), these
works typically state regret guarantees in terms of different, often incomparable notions of supervised learning
performance, and avoid giving concrete, end-to-end guarantees for specific policy classes of interest. This
can make it challenging to objectively evaluate optimality, and to understand whether limitations of specific
algorithms are due to suboptimal design choices versus information-theoretic limitations. For example, Li
and Zhang (2022) show that in some cases, supervised learning oracles that satisfy assumptions required by
prior work do not actually exist.
Minimax sample complexity of imitation learning.
More recently, a line of work beginning with
Rajaraman et al. (2020) revisits the minimax sample complexity of imitation learning, aiming to provide
end-to-end sample complexity guarantees and lower bounds, but primarily focused on tabular MDPs and
policies (Rajaraman et al., 2020, 2021a,b; Swamy et al., 2022). Notably, Rajaraman et al. (2020) show that
when Π is the set of all non-stationary policies in a tabular MDP and R = H, online IL methods can achieve
O(µH) sample complexity, while offline IL methods must pay Ω(H2); this is consistent with our findings in
Section 2, as log|Π| = Ω(H) for this setting. Other interesting findings from this line of work include the
observation that when the MDP dynamics are known, the sample complexity for offline IL with non-stationary
tabular policies can be brought down to O(H3/2). As noted in Section 2, Rajaraman et al. (2021a) show that
offline IL methods can obtain O(H) sample complexity for linearly parameterized policies under parameter
sharing; our analysis of LogLossBC for the special case of deterministic policies shows that it can be viewed
as implicitly minimizing the objective they consider.
Compared to the works above, we focus on general finite classes Π. Various works on theoretical reinforcement
learning (Agarwal et al., 2019; Foster and Rakhlin, 2023) have observed that finite classes are a useful test
case for general function approximation, because they are arguably the simplest type of policy class from
a generalization perspective, yet do not have any additional structure (e.g., linearity) that could lead to
spurious conclusions that do not extend to rich function classes like neural networks.
Recent work of Tiapkin et al. (2024) provides generalization guarantees for behavior cloning with the
logarithmic loss, but their results scale linearly with the horizon, and thus cannot give tight guarantees for
policy classes with parameter sharing. In addition, their results are stated in terms of KL-divergence and,
as a consequence, require a lower bound on the action densities for the policy class under consideration.
We expect that both of these limitations are inherent to KL divergence. Tiapkin et al. (2024) also give
variance-dependent bounds on rollout performance similar to Theorem 3.1, but their results require a bound
on KL divergence (which is stronger than a bound on Hellinger distance), and thus are unlikely to meaningfully
capture optimal horizon dependence. These bounds on rollout performance also do not recover the notion of
variance in Theorem 3.1.
We also mention in passing Sekhari et al. (2024), who consider active imitation learning algorithms, and focus
on obtaining improved sample complexity with respect to dependence on the accuracy ε (as opposed to H),
under strong distributional assumptions in the vein of active learning (Hanneke, 2014).
29

Inverse reinforcement learning.
A long line of research on inverse reinforcement learning and related
techniques considers a setting in which either a) the dynamics of the MDP M ⋆are known, or b) it is possible
to interact with M ⋆online (without expert feedback), with empirical (Abbeel and Ng, 2004; Ziebart et al.,
2008) and theoretical results (Syed and Schapire, 2007; Syed et al., 2008; Syed and Schapire, 2010; Brantley
et al., 2019; Chang et al., 2021). This setting encompasses generative adversarial imitation learning and
related moment matching methods (Ho and Ermon, 2016; Li et al., 2017; Ke et al., 2021a; Swamy et al.,
2021). A detailed discussion is out of scope for the present work, but we believe this framework can improve
over the sample complexity of offline IL in some but not all situations (e.g., Rajaraman et al. (2020)).
Benefits of logarithmic loss.
Our work draws inspiration from Foster and Krishnamurthy (2021), who
observed that the logarithmic loss can have benefits over square loss when outcomes are heteroskedastic,
and used this observation to derive first-order regret bounds for contextual bandits. Subsequent works have
extended their analysis technicals to derive first-order regret bounds in various reinforcement learning settings
(Wang et al., 2023, 2024; Ayoub et al., 2024).23 To the best of our knowledge, our work is the first to
uncover a decision making setting in which switching to the logarithmic loss is beneficial even in a minimax
sense. We emphasize that while our analysis uses the information-theoretic machinery introduced in Foster
and Krishnamurthy (2021) and related work (Foster et al., 2021, 2022), our results are quite specialized to
structure of the imitation learning setting, and cannot directly be derived from any of the results in Foster
and Krishnamurthy (2021); Wang et al. (2023, 2024); Ayoub et al. (2024).
Horizon-free reinforcement learning.
Our results also take inspiration from the line of research on
horizon-independent sample complexity bounds for reinforcement learning (Jiang and Agarwal, 2018; Zanette
and Brunskill, 2019; Wang et al., 2020; Zhang et al., 2021, 2022), as well as a closely related line of research
on variance-dependent regret bounds (Zhou et al., 2023; Zhao et al., 2023; Wang et al., 2024).24 These papers
provide sample complexity bounds for reinforcement learning that have little or no explicit dependence on
horizon whenever rewards are normalized such that PH
h=1 rh ∈[0, 1]. We consider a simpler setting (imitation
learning), but provide guarantees that hold under general function approximation, while the works above
are restricted to either tabular MDPs or MDPs with linear/low-rank structure. Nonetheless, our proof of
Theorem 3.1 makes use of concentration arguments inspired by Zhang et al. (2021, 2022).
A.2
Empirical Research on Imitation Learning
Many empirical works have observed compounding error in behavior cloning. Outside of online imitation
learning, mitigations include noise injection at data collection time (Laskey et al., 2017; Ke et al., 2021b) or
inverse RL methods that assume knowledge of system dynamics (Ziebart et al., 2008). Other works take a
control-theoretic perspective (Tu et al., 2022b; Havens and Hu, 2021; Pfrommer et al., 2022; Block et al.,
2024b), and augment behavior cloning with techniques designed to ensure incremental stability (or other
control-theoretic notions of stability) of system.
Online imitation learning.
Many empirical works have noted benefits of online imitation learning methods
like Dagger over classical behavior cloning (Ross and Bagnell, 2010; Ross et al., 2011, 2013; Kim et al., 2013;
Gupta et al., 2017; Kelly et al., 2019). These results are not in contradiction to our findings, as they typically
do not ablate the effect of the loss function (e.g., Ross and Bagnell (2010) uses the squared hinge loss, Ross
et al. (2011) uses the hinge loss, and Ross et al. (2013) uses the square loss). It is also possible that the
perceived benefits arise from factors beyond horizon (e.g., representational benefits), as discussed in Section 4.
23We also mention in passing the work of Farebrother et al. (2024), which observes that switching to the log-loss is beneficial
empirically for approximate value iteration methods in offline reinforcement learning.
24Compared to variance-dependent bounds for RL in Zhou et al. (2023); Zhao et al. (2023); Wang et al. (2024) an interesting
feature of Theorem 3.1 is that it only depends on variance for π⋆, whereas these works typically depend on worst-case variance
over all policies or similar quantities.
30

A.3
Autoregressive Language Modeling
Autoregressive language modeling with the standard next-token prediction objective (Radford et al., 2019)
can be viewed as an instance of behavior cloning with the logarithmic loss. In this setting, M ⋆corresponds
to a token-level MDP. Here A is a space or vocabulary of tokens The initial state is x1 = z ∼P0, where z is
a prompt or context. Given the prompt, for each h = 1, . . . , H the action ah ∈A is a new token, which is
concatenated to the state via the deterministic dynamics xh+1 ←(z, a1:h). Via Bayes’ rule, an expert policy
π⋆(a1:H | z) =
H
Y
h=1
π⋆
h(ah | z, a1:h−1) =
H
Y
h=1
π⋆
h(ah | xh)
can represent an arbitrary conditional distribution over sequences, from which a training set D = {oi} with
oi = (zi, ai
1, . . . , ai
H) is generated. With this setup, log-loss behavior cloning
bπ = arg max
π∈Π
n
X
i=1
H
X
h=1
log(π(a
i
h | z
i, a
i
1:h−1))
is equivalent to the standard next-token prediction objective for unsupervised language model pre-training
(Radford et al., 2019), with the class Π parameterized by a transformer or a similar neural net architecture.
In this context, long-range error amplification arising from the next-token prediction objective (often referred
to as exposure bias) has been widely observed by prior work (Holtzman et al., 2019; Braverman et al., 2020;
Block et al., 2024a), and in some cases speculated to be a fundamental limitation (LeCun, 2023; Bachmann
and Nagarajan, 2024).
Applying our results.
To apply our results, consider a fixed reward function r = {rh}H
h=1, which might
measure performance for a particular task of interest (e.g., question answering or commonsense reasoning).
Then, for a model π, J(π) corresponds to rollout performance at the task for an autoregressively generated
sequence (i.e., given z ∼P0, we sample ah ∼πh(· | z, a1:h−1) for all h ∈[H]). For this setting, Theorem 3.1
states that
J(π⋆) −J(bπ) ≤eO
q
σ2
π⋆· D2
H
 Pbπ, Pπ⋆
+ R · D2
H
 Pbπ, Pπ⋆
,
(18)
where σ2
π⋆= PH
h=1 Eπ⋆
(Qπ⋆
h (z, a1:h) −V π⋆
h (z, a1:h−1))2
. In particular, as long as the cumulative reward for
the task is bounded by R = O(1) (e.g., if we receive an episode-level reward rH = 1 if a question is answered
correctly, and receive zero reward otherwise), the rollout performance has no explicit dependence on the
sequence length, except through the generalization error D2
H
 Pbπ, Pπ⋆
. In light of this result, we expect that
error amplification observed in practice may arise from challenges in minimizing the generalization error
D2
H
 Pbπ, Pπ⋆
itself (e.g., architecture, data generation process, optimization (Braverman et al., 2020; Block
et al., 2024a)), rather than fundamental limits of next-token prediction.
B
Technical Tools
B.1
Tail Bounds
Lemma B.1 (e.g., Foster et al. (2021)). For any sequence of real-valued random variables (Xt)t≤T adapted
to a filtration (Ft)t≤T , it holds that with probability at least 1 −δ, for all T ′ ≤T,
T ′
X
t=1
−log
 Et−1

e−Xt
≤
T ′
X
t=1
Xt + log(δ−1).
Lemma B.2 (Time-uniform Freedman-type inequality). Let (Xt)t≤T be a real-valued martingale difference
sequence adapted to a filtration (Ft)t≤T . If |Xt| ≤R almost surely, then for any η ∈(0, 1/R), with probability
at least 1 −δ, for all T ′ ≤T.
T ′
X
t=1
Xt ≤η
T ′
X
t=1
Et−1

X2
t

+ log(δ−1)
η
.
31

Proof of Lemma B.2.
Let St = Pt
s=1 Xt and Vt = Pt
s=1 Et=1

X2
t

. Let Zt = exp(ηSt −η2Vt). As shown
in Beygelzimer et al. (2011) (see proof of Theorem 1), as long as η ≤1/R,
Et−1[exp(ηXt)] ≤exp(η2 Et−1

X2
t

),
and so
Et−1[Zt] = Et−1

exp
 ηXt −η2 Et−1

X2
t

· Zt−1 ≤Zt−1.
It follows that (Zt) is a non-negative supermartingale. Hence, by Ville’s inequality, for any η ∈(0, 1/R), we
have that for any τ > 0,
P[∃t : St −ηVt ≥τ] = P[∃t : Zt ≥eητ] ≤e−ητ E[ZT ] ≤e−ητ.
We conclude by setting τ = log(δ−1)/η.
The following result is a standard consequence of Lemma B.2.
Lemma B.3. Let (Xt)t≤T be a sequence of random variables adapted to a filtration (Ft)t≤T . If 0 ≤Xt ≤R
almost surely, then with probability at least 1 −δ, for all T ′ ≤T,
T ′
X
t=1
Xt ≤3
2
T ′
X
t=1
Et−1[Xt] + 4R log(2δ−1),
and
T ′
X
t=1
Et−1[Xt] ≤2
T ′
X
t=1
Xt + 8R log(2δ−1).
B.2
Information Theory
For a pair of probability measures P and Q, we define the total variation distance as DTV(P, Q) = 1
2
R
|dP−dQ|,
and define the χ2-divergence by Dχ2(P ∥Q) =
R (dQ−dQ)2
dQ
if P ≪Q and Dχ2(P ∥Q) = +∞otherwise. We
define KL divergence by DKL(P ∥Q) =
R
dP log
  dP
dQ

if P ≪Q and DKL(P ∥Q) = +∞otherwise.
The following lemma states some basic inequalities between divergences.
Lemma B.4 (e.g., Polyanskiy and Wu (2014)). The following inequalities hold:
• D2
TV(P, Q) ≤D2
H(P, Q) ≤2DTV(P, Q).
•
1
6D2
H(P, Q) ≤Dχ2 P ∥1
2(P + Q)

≤D2
H(P, Q).
B.3
Reinforcement Learning
The following lemma is a somewhat standard result; see, e.g., Lemma 15 in Zanette and Brunskill (2019). We
include a proof for completeness.
Lemma B.5 (Law of total variance). For any (potentially stochastic) policy π, we have
Varπ
" H
X
h=1
rh
#
= Eπ
" H
X
h=0
Varπ
rh + V π
h+1(xh+1) | xh

#
,
with the convention that x0 is a deterministic dummy state (so that P0(x1 = · | x0, a = ·) is the initial state
distribution) and r0 = 0.
32

Proof of Lemma B.5.
Let h ∈{0, . . . , H} be fixed. We can expand
Varπ
" H
X
ℓ=h
rℓ| xh
#
= Eπ


 H
X
ℓ=h
rℓ−V π
h (xh)
!2
| xh


= Eπ


 
H
X
ℓ=h+1
rℓ−V π
h+1(xh+1) + (rh + V π
h+1(xh+1) −V π
h (xh))
!2
| xh


= Eπ


 
H
X
ℓ=h+1
rℓ−V π
h+1(xh+1)
!2
| xh

+ Eπ
(rh + V π
h+1(xh+1) −V π
h (xh))2 | xh

+ 2 Eπ
" 
H
X
ℓ=h+1
rℓ−V π
h+1(xh+1)
!
 rh + V π
h+1(xh+1) −V π
h (xh)

| xh
#
= Eπ


 
H
X
ℓ=h+1
rℓ−V π
h+1(xh+1)
!2
| xh

+ Eπ
(rh + V π
h+1(xh+1) −V π
h (xh))2 | xh

= Eπ
"
Varπ
"
H
X
ℓ=h+1
rℓ| xh+1
#
| xh
#
+ Varπ
(rh + V π
h+1(xh+1) | xh

.
We conclude inductively that for all h ∈{0, . . . , H},
Varπ
" H
X
ℓ=h
rℓ| xh
#
=
H
X
ℓ=h
Eπ
Varπ
(rℓ+ V π
ℓ+1(xℓ+1) | xℓ

| xh

.
To obtain the final expression, we note that
Varπ
" H
X
h=1
rh
#
= Varπ
" H
X
h=0
rh | x0
#
,
under the convention that x0 is a deterministic dummy state (so that P1(x1 = · | x0, a) is the initial state
distribution) and r0 = 0.
B.4
Maximum Likelihood Estimation
This section presents a self-contained analysis of the maximum likelihood estimator (MLE) for density
estimation. The results are somewhat standard (e.g., Wong and Shen (1995); van de Geer (2000); Zhang
(2006)), but we include proofs for completeness.
Consider a setting where we receive {zi}n
i=1 i.i.d. from z ∼g⋆, where g⋆∈∆(Z). We have a class G ⊆∆(Z)
that may or may not contain g⋆. We analyze the following maximum likelihood estimator:
bg = arg max
g∈G
n
X
i=1
log(g(z
i)).
(19)
To provide sample complexity guarantees that support infinite classes, we appeal to the following notion of
covering number (e.g., Wong and Shen (1995)), which tailored to the log-loss.
Definition B.1 (Covering number). For a class G ⊂∆(Z), we set that a class G′ ⊂∆(Z) is an ε-cover
if for all g ∈G, there exists g′ ∈G′ such that for all z ∈Z, log(g(z)/g′(z)) ≤ε. We denote the size of the
smallest such cover by Nlog(G, ε).
33

We also allow for optimization errors, and concretely assume that bg satisfies
n
X
i=1
log(bg(z
i)) ≥max
g∈G
n
X
i=1
log(g(z
i)) −εopt · n
for a parameter εopt ≥0; the case εopt = 0 coincides with Eq. (19). Our main guarantee for MLE is as follows.
Proposition B.1. The maximum likelihood estimator in Eq. (19) has that with probability at least 1 −δ,
D2
H(bg, g⋆) ≤inf
ε>0
6 log(2Nlog(G, ε)/δ−1)
n
+ 4ε

+ 2 inf
g∈G log(1 + Dχ2(g⋆∥g)) + 2εopt.
In particular, if G is finite, the maximum likelihood estimator satisfies
D2
H(bg, g⋆) ≤6 log(2|G|/δ−1)
n
+ 2 inf
g∈G log(1 + Dχ2(g⋆∥g)) + 2εopt.
Note that the term infg∈G log(1 + Dχ2(g⋆∥g)) corresponds to misspecification error, and is zero if g⋆∈G.
Proof of Proposition B.1. Let Gε denote a minimal ε-cover for G, and let eg ∈Gε denote any element that
covers bg in the sense of Definition B.1. Going forward, we will use that eg satisfies
D2
H(g⋆, eg) ≤DKL(g⋆∥eg) ≤ε.
(20)
Let ℓi(g) = −log(g(zi)), and set bL(g) = −Pn
i=1 log(g(zi)). Set Xi(g) = 1
2(ℓi(g) −ℓi(g⋆)). By applying
Lemma B.1 with the sequence (Xi(g))n
i=1 for each g ∈Gε and taking a union bound, we have that with
probability at least 1 −δ, for all g ∈Gε
−n · log

Ez∼g⋆
h
e
1
2 log(g(z)/g⋆(z))i
≤1
2

bL(g) −bL(g⋆)

+ log(|Gε|δ−1).
Using a standard argument (Zhang, 2006), we have that
−log

Ez∼g⋆
h
e
1
2 log(g(z)/g⋆(z))i
= −log

1 −1
2D2
H(g, g⋆)

≥1
2D2
H(g, g⋆).
In particular, this implies that
D2
H(eg, g⋆) ≤2 log(|G|/δ−1)
n
+ 1
n

bL(eg) −bL(g⋆)

,
and so
D2
H(bg, g⋆) ≤2D2
H(bg, eg) + 2D2
H(eg, g⋆) ≤4 log(|G|/δ−1)
n
+ 2
n

bL(eg) −bL(g⋆)

+ 2ε,
by the triangle inequality for Hellinger distance and Eq. (20).
It remains to bound the right-hand-side. Let g ∈G be arbitrary. We can bound
bL(eg) −bL(g⋆) ≤bL(eg) −bL(bg) + bL(bg) −bL(g⋆) ≤bL(eg) −bL(bg) + bL(g) −bL(g⋆) + εoptn,
(21)
by the definition of the maximum likelihood estimator. For the first term in Eq. (21), we observe that
bL(eg) −bL(g⋆) =
n
X
i=1
log(g⋆(z
i)/eg(z
i)) ≤εn,
by Definition B.1.
34

To bound the second term in Eq. (21), set Yi = −(ℓt(g) −ℓt(g⋆)). Applying Lemma B.1 with the sequence
(Yi)n
i=1, we have that with probability at least 1 −δ,
bL(g) −bL(g⋆) ≤n · log

Ez∼g⋆
h
elog(g⋆(z)/g(z))i
+ log(δ−1).
Finally, note that
log

Ez∼g⋆
h
elog(g⋆(z)/g(z))i
= log

Ez∼g⋆
g⋆(z)
g(z)

= log(1 + Dχ2(g⋆∥g)).
The result follows by choosing g ∈G to minimize this quantity.
Part I
Proofs and Supporting Results
C
Examples and Supporting Results from Section 2 and Section 3
This section contains supporting results from Sections 2 and 3:
• Appendix C.1 presents general sample complexity guarantees for log-loss behavior cloning that support
infinite policy classes and misspecification, as well as concrete examples.
• Appendix C.2 formally introduces the online imitation learning framework, and gives sample complexity
guarantees for a log-loss variant of Dagger.
C.1
General Guarantees and Examples for Log-Loss Behavior Cloning
In this section, we give bounds on the generalization error D2
H
 Pbπ, Pπ⋆
for log-loss behavior cloning for
concrete classes Π of interest. To do so, we observe that the log-loss behavior cloning objective
bπ = arg max
π∈Π
n
X
i=1
H
X
h=1
log(π(a
i
h | x
i
h)).
is equivalent to performing maximum likelihood estimation over the density class P = {Pπ}π∈Π. Indeed, for
any π ∈Π, we have
n
X
i=1
log(Pπ(o
i)) =
n
X
i=1
log
 
P0(x
i
1)
H
Y
h=1
Ph(x
i
h+1 | x
i
h, a
i
h)πh(a
i
h | x
i
h)
!
=
n
X
i=1
H
X
h=1
log(πh(a
i
h | x
i
h)) + C(D),
where C(D) is a constant that depends on the dataset D but not on π. It follows that both objectives have
the same maximizer. Consequently, we can prove sample complexity bounds for log-loss behavior cloning by
specializing sample complexity bounds for MLE given in Appendix B.4.
To give guarantees that support infinite policy classes, we appeal to the following notion of covering number.
Definition C.1 (Policy covering number). For a class Π ⊂{πh : X →∆(A)}, we set that Π′ ⊂{πh : X →∆(A)}
is an ε-cover if for all π ∈Π, there exists π′ ∈Π′ such that for all x ∈X, a ∈A, and h ∈[H],
log(πh(a | x)/π′
h(a | x)) ≤ε. We denote the size of the smallest such cover by Npol(Π, ε).
35

In addition, to allow for optimization errors, we replace Eq. (5) with the assumption that bπ satisfies
n
X
i=1
H
X
h=1
log(bπ(a
i
h | x
i
h)) ≥max
π∈Π
n
X
i=1
H
X
h=1
log(π(a
i
h | x
i
h)) −εopt · n
(22)
for a parameter εopt > 0; Eq. (5) is the special case in which εopt = 0. With these definitions, specializing
Proposition B.1 leads to the following result.
Theorem C.1 (Generalization bound for LogLossBC). The LogLossBC policy in Eq. (22) has that with
probability at least 1 −δ,
D2
H

Pbπ, Pπ⋆
≤inf
ε>0
6 log(2Npol(Π, ε/H)δ−1)
n
+ 4ε

+ 2 inf
π∈Π log

1 + Dχ2 Pπ⋆∥Pπ
+ 2εopt.
In particular, if Π is finite, the log-loss behavior cloning policy satisfies
D2
H

Pbπ, Pπ⋆
≤6 log(2|Π|δ−1)
n
+ 2 inf
π∈Π log

1 + Dχ2 Pπ⋆∥Pπ
+ 2εopt.
Let us make two remarks.
• First, the only explicit dependence on the horizon H is through the precision ε/H through which we
evaluate the covering number: Npol(Π, ε/H). As a result, for parametric classes where Npol(Π, ε) ≍
log(ε−1) (we will give examples in the sequel), the result will scale at most logarithmically in H, but
for nonparametric classes the dependence can be polynomial.
• Second, the remainder term infπ∈Π log(1 + Dχ2 Pπ⋆∥Pπ
) corresponds to misspecification error, and
is zero if π⋆∈Π. We remark that when π⋆is deterministic, this expression can be simplified to
infπ∈Π log

Eπ⋆h
1
QH
h=1 πh(ah|xh)
i
.
Proof of Theorem C.1.
This follows by applying Proposition B.1 with the class {Pπ}π∈Π, and noting
that if π′ covers π in the sense of Definition C.1, then for all o ∈(X × A)H, we have log(Pπ(o)/Pπ′(o)) ≤εH,
meaning that an ε-cover in the sense of Definition C.1 yields an εH-cover in the sense of Definition B.1.
C.1.1
Example: Tabular Policies
We now instantiate Theorem C.1 to give generalization bounds for specific policy classes of interest.
Consider a tabular MDP in which |X|, |A| < ∞are small and finite. Here, choosing Π to be the set of all
stationary policies leads to a bound independent of H.
Corollary C.1 (Stationary tabular policies). When Π is the set of all deterministic stationary policies, the
log-loss behavior cloning policy Eq. (5) has that with probability at least 1 −δ,
D2
H
 Pbπ, Pπ⋆
≤O
|X| log(|A|δ−1)
n

.
Meanwhile, if Π is the set of all stochastic stationary policies, the log-loss behavior cloning policy Eq. (5) has
that with probability at least 1 −δ,
D2
H
 Pbπ, Pπ⋆
≤eO
|X||A| log(Hnδ−1)
n

.
Proof of Corollary C.1.
This follows by noting that we have log|Π| ≤|X| log|A| in the deterministic case
and log Npol(Π, ε) ≤eO
 |X||A| log(ε−1)

in the stochastic case (this follows from a standard discretization
36

argument, e.g., Wainwright (2019)).
Naturally, we can also give generalization guarantees for non-stationary tabular policies, though the sample
complexity will scale with H in this case.
Corollary C.2 (Non-stationary tabular policies). When Π is the set of all deterministic non-stationary
policies, the log-loss behavior cloning policy Eq. (5) has that with probability at least 1 −δ,
D2
H
 Pbπ, Pπ⋆
≤O
H|X| log(|A|δ−1)
n

.
Meanwhile, if Π is the set of all stochastic non-stationary policies, the log-loss behavior cloning policy Eq. (5)
has that with probability at least 1 −δ,
D2
H
 Pbπ, Pπ⋆
≤eO
H|X||A| log(Hnδ−1)
n

.
Proof of Corollary C.2.
This follows because we have log|Π| ≤H|X| log|A| in the deterministic case and
log Npol(Π, ε) ≤eO
 H|X||A| log(ε−1)

in the stochastic case.
C.1.2
Example: Softmax Policies
Next, we give an example of a general family of policy classes based on function approximation for which the
sample complexity is at most polylogarithmic in H.
For a vector v ∈RA, let σ : RA →∆(A) be the softmax function, which is given by
σa(v) =
exp(va)
P
a′∈A exp(va′).
Let F ⊂{fh : X × A →R}H
h=1 be a class of value functions, and define the induced class of softmax policies
via
ΠF = {πf | f ∈F},
where
πf,h(x) := σa(fh(x, a)).
We give sample complexity guarantees based on covering numbers for the value function class F.
Definition C.2 (Value function covering number). For a class F ⊂{fh : X × A →R}, we set that F′ ⊂
{fh : X × A →R} is an ε-cover if for all f ∈F, there exists f ′ ∈F′ such that for all x ∈X, a ∈A, and
h ∈[H], |fh(x, a) −f ′
h(x, a)| ≤ε. We denote the size of the smallest such cover by Nval(Π, ε).
Corollary C.3 (Softmax policies). When Π = ΠF is the softmax policy class for a value function class F,
the log-loss behavior cloning policy Eq. (5) has that with probability at least 1 −δ,
D2
H

Pbπ, Pπ⋆
≤O(1) · inf
ε>0
log(Nval(F, ε/H)δ−1)
n
+ ε

+ 2 inf
π∈ΠF log

1 + Dχ2 Pπ⋆∥Pπ
.
Proof of Corollary C.3.
Consider a pair of functions f, f ′ with |fh(x, a) −f ′
h(x, a)| ≤ε for all x ∈X,
a ∈A, and h ∈[H]. The induced softmax policies satisfy
log(πf,h(a | x)/πf ′,h(a | x)) = fh(x, a) −f ′
h(x, a) + log
P
a′∈A exp(f ′
h(x, a′))
P
a∈A exp(f ′
h(x, a′))

.
37

Clearly we have fh(x, a) −f ′
h(x, a) ≤ε, and we can bound
log
P
a′∈A exp(f ′
h(x, a′))
P
a∈A exp(fh(x, a′))

= log
P
a′∈A exp(fh(x, a′)) · exp(f ′
h(x, a′) −fh(x, a′))
P
a∈A exp(fh(x, a′))

≤log
P
a′∈A exp(fh(x, a′)) · maxa′′∈A exp(f ′
h(x, a′′) −fh(x, a′′))
P
a∈A exp(fh(x, a′))

≤max
a′′∈A{f ′
h(x, a′′) −fh(x, a′′)} ≤ε.
Hence, an ε-cover in the sense of Definition C.2 implies a 2ε-cover in the sense of Definition C.1.
no Whenever F is parametric in the sense that log Nval(F, ε) ∝log(ε−1), Corollary C.3 leads to polyloga-
rithmic dependence on H. The following result gives such an example.
Linear softmax policies.
Consider the set of stationary linear softmax policies given by
F = {(x, a, h) 7→⟨ϕh(x, a), θ⟩| ∥θ∥2 ≤B},
where ϕh(x, a) ∈Rd is a known feature map with ∥ϕh(x, a)∥≤B. Here, we have log Nval(F, ε) ∝d log(Bε−1)
(e.g., Wainwright (2019)), which yields the following generalization guarantee.
Corollary C.4. When Π is the set of stationary linear softmax policies and π⋆∈Π, the log-loss behavior
cloning policy Eq. (5) has that with probability at least 1 −δ,
D2
H
 Pbπ, Pπ⋆
≤O
d log(BHnδ−1)
n

.
C.2
Online IL Framework and Sample Complexity Bounds for Log-Loss Dagger
In this section, we give sample complexity bounds for a variant of the Dagger algorithm for online IL (Ross
et al., 2011) that uses the logarithmic loss. The main purpose of including this result is to give end-to-end
sample complexity guarantees for general policy classes, which we use in Sections 2 and 3 to compare the
optimal rates for online and offline IL. For this comparison, we are be mainly interested in the case of
deterministic expert policies, but our analysis supports stochastic policies, which may be of independent
interest.
Online imitation learning framework.
In the online imitation learning framework, learning proceeds in
n episodes in which the learner can directly interact with the underlying MDP M ⋆and query the expert advice.
Concretely, for each episode i ∈[n], the learner executes a policy πi = {πi
h : X →∆(A)}H
h=1 and receives a
trajectory ot = (xi
1, ai
1, a⋆,i
1 ), . . . , (xi
H, ai
H, a⋆,i
H ), in which ai
h ∼πi
h(xi
h), a⋆,i
h ∼π⋆(xt
h), and xi
h+1 ∼Ph(xi
h, ai
h);
in other words, the trajectory induced by the learner’s policy is annotated by the expert’s action a⋆
h ∼π⋆
h(xh)
at each state xh encountered. After all n episodes conclude, they can use all of the data collected to produce
a policy bπ such that J(π⋆) −J(bπ) is small.
Dagger algorithm.
We consider a general version of the Dagger algorithm. The algorithm is parameterized
by an online learning algorithm AlgEst, which attempts to estimate the expert policy in a sequential fashion
based on trajectories.
Set D1 = ∅. For i = 1, . . . , n:
• Query online learning algorithm AlgEst with Di and receive policy bπ.
• Execute bπ and observe oi = (xi
1, ai
1, a⋆,i
1 ), . . . , (xi
H, ai
H, a⋆,i
H ).
• Update Di+1 ←Di ∪{oi}.
At the end, we output bπ = unif(π1, . . . , πn) as the final policy.
38

To measure the performance of the estimation oracle, we define the online estimation error as:
Eston
H (n) = 1
n
n
X
i=1
H
X
h=1
Ebπi
D2
H(bπ
i
h(xh), π⋆(xh))

.
As we will show in a moment, this notion of estimation error is well-suited for online learning algorithms that
estimate π⋆using the logarithmic loss.
Our following result gives a general guarantee for Dagger that holds for any choice of online learning algorithm.
To state the result, let Pπ⋆|π denote the law of o = (x1, a1, a⋆
1), . . . , (xH, aH, a⋆
H) when π⋆is the expert policy
and we execute π. Let
σ2
π⋆|π =
H
X
h=1
Eπ◦hπ⋆h
(Qπ⋆
h (xh, ah) −V π⋆
h (xh))2i
,
so that σ2
π⋆= σ2
π⋆|π⋆and define σ2
π⋆= supπ σ2
π⋆|π. Note that σ2
π⋆= 0 whenever π⋆is deterministic, but in
general, σ2
π⋆≥σ2
π⋆.
Proposition C.1 (Regret for Dagger). For any MDP M ⋆with signed recoverability parameter eµ (Eq. (13))and
any online learning algorithm AlgEst, Dagger ensures that
J(π⋆) −J(bπ) ≲
q
σ2
π⋆· Eston
H (n) + eµ · Eston
H (n).
Furthermore, whenever π⋆is deterministic, Dagger ensures that
J(π⋆) −J(bπ) ≲µ · Eston
H (n).
(23)
To instantiate the bound above, we choose AlgEst by applying the exponential weights algorithm (e.g.,
Cesa-Bianchi and Lugosi (2006)) with the logarithmic loss. Let Πh := {πh | π ∈Π} denote the projection
of Π onto step h. The algorithm proceeds as follows. At step i ∈[n], given the dataset Di, for each layer
h ∈[H] we define a distribution µi
h ∈∆(Πh) via
µ
i
h(π) ∝exp

X
j<j
log(πh(a⋆,j
h
| x
j
h))

=
Y
j<j
πh(a⋆,j
h
| x
j
h).
We then set
bπ
i
h(a | x) = Eπh∼µi
h[πh(a | x)].
We refer to the resulting algorithm as LogLossDagger. This leads to the following guarantee for finite classes.
Proposition C.2 (Regret for LogLossDagger). When π⋆∈Π, the log-loss exponential weights algorithm
ensures that with probability at least 1 −δ,
Eston
H (n) ≤2
n
H
X
h=1
log(|Πh|Hδ−1).
Consequently, LogLossDagger ensures that with probability at least 1 −δ,
J(π⋆) −J(bπ) ≲
v
u
u
tσ2
π⋆·
H
X
h=1
log(|Πh|Hδ−1)
n
+ eµ ·
H
X
h=1
log(|Πh|Hδ−1)
n
,
and when π⋆is deterministic,
J(π⋆) −J(bπ) ≲µ ·
H
X
h=1
log(|Πh|Hδ−1)
n
.
39

We note that for many parameter regimes, the sample complexity bound in Proposition C.1 can be worse
than that of LogLossBC in Theorem 3.1 (for stationary policies, Proposition C.1 has spurious dependence on
H, and the variance-like quantity in the leading order term is weaker). It would be interesting to get the best
of both worlds, though this may require changing the algorithm.
Proof of Proposition C.1.
Consider an arbitrary policy bπ. Begin by writing
J(π⋆) −J(bπ) =
H
X
h=1
Ebπ|bπh
Qπ⋆
h (xh, π⋆
h(xh)) −Qπ⋆
h (xh, ah)
i
.
Fix a layer h. By Lemma 3.1, we have
Ebπ|bπh
Qπ⋆
h (xh, π⋆
h(xh)) −Qπ⋆
h (xh, ah)
i
≤Eπ⋆|bπh
Qπ⋆
h (xh, π⋆
h(xh)) −Qπ⋆
h (xh, ah)
i
+
q Ebπ|bπ[(Qπ⋆
h (xh, π⋆
h(xh)) −Qπ⋆
h (xh, ah))2] + Eπ⋆|bπ[(Qπ⋆
h (xh, π⋆
h(xh)) −Qπ⋆
h (xh, ah))2]

Ebπ[D2
H(bπh(xh), π⋆
h(xh))]
=
q Ebπ|bπ[(Qπ⋆
h (xh, π⋆
h(xh)) −Qπ⋆
h (xh, ah))2] + Eπ⋆|bπ[(Qπ⋆
h (xh, π⋆
h(xh)) −Qπ⋆
h (xh, ah))2]

Ebπ[D2
H(bπh(xh), π⋆
h(xh))].
Furthermore, using Lemma 3.1, we have
Ebπ|bπh
(Qπ⋆
h (xh, π⋆
h(xh)) −Qπ⋆
h (xh, ah))2i
≲
H
X
h=1
Eπ⋆|bπh
(Qπ⋆
h (xh, π⋆
h(xh)) −Qπ⋆
h (xh, ah))2i
+ eµ2
H
X
h=1
Ebπ
D2
H(bπh(xh), π⋆
h(xh))

,
so that
Ebπ|bπh
Qπ⋆
h (xh, π⋆
h(xh)) −Qπ⋆
h (xh, ah)
i
(24)
≲
q
Eπ⋆|bπ[(Qπ⋆
h (xh, π⋆
h(xh)) −Qπ⋆
h (xh, ah))2] · Ebπ[D2
H(bπh(xh), π⋆
h(xh))] + eµ · Ebπ
D2
H(bπh(xh), π⋆
h(xh))

.
Recall that the Dagger policy satisfies
J(π⋆) −J(bπ) = 1
n
n
X
i=1
J(π⋆) −J(bπ
i).
Applying Eq. (24) to each policy bπi, summing over all layer h, and applying Cauchy-Schwarz yields
J(π⋆) −J(bπ) ≲
v
u
u
t 1
n
n
X
i=1
σ2
π⋆|πi · Eston
H (n) + eµ · Eston
H (n)
≲
q
σ2
π⋆· Eston
H (n) + eµ · Eston
H (n).
In the deterministic case, we tighten the argument above by applying the following improved change-of-measure
argument based on Lemma 3.1:
Eπ⋆|bπh
Qπ⋆
h (xh, π⋆
h(xh)) −Qπ⋆
h (xh, ah)
i
≤Eπ⋆|bπh
(Qπ⋆
h (xh, π⋆
h(xh)) −Qπ⋆
h (xh, ah))+
i
≤2 Eπ⋆|bπh
(Qπ⋆
h (xh, π⋆
h(xh)) −Qπ⋆
h (xh, ah))+
i
+ µ · Ebπ
D2
H(bπh(xh), π⋆
h(xh))

= µ · Ebπ
D2
H(bπh(xh), π⋆
h(xh))

,
This leads to Eq. (23).
40

Proof of Proposition C.2.
Since π⋆∈Π, a standard guarantee for exponential weights with the log-loss
(e.g., Cesa-Bianchi and Lugosi (2006)) ensures that for all h ∈[H], the following bound holds almost surely:
n
X
i=1
log(1/bπ
i
h(a⋆,i
h | x
i
h)) ≤
n
X
i=1
log(1/π⋆
h(a⋆,i
h | x
i
h)) + log|Πh|.
From here, for each h ∈[H], Lemma A.14 of Foster et al. (2021) implies that with probability at least 1 −δ,
n
X
i=1
Ebπi
D2
H(bπ
i
h(xh), π⋆(xh))

≤log|Πh| + 2 log(δ−1).
The result now follows by taking a union bound.
D
Proofs from Section 2
D.1
Proof of Theorem 2.1
Proof of Theorem 2.1.
We begin by defining the following trajectory-wise semi-metric between policies.
For a pair of potentially stochastic policies π and π′, define
ρ(π ∥π′) := Eπ Ea′
1:H∼π′(x1:H)[I{∃h : ah ̸= a′
h}],
where we use the shorthand a′
1:H ∼π′(x1:H) to indicate that a′
1 ∼π′(x1), . . . , a′
H ∼π′(xH). Despite being
defined in an asymmetric fashion, the following lemma shows that the trajectory-wise distance ρ(· ∥·) is
symmetric, from which it follows that it is indeed a semi-metric.
Lemma D.1. For all (potentially stochastic) policies π and π′, it holds that
ρ(π ∥π′) = ρ(π′ ∥π).
Next, we show that it is possible to bound the difference in reward for any pair of policies in terms of the
trajectory-wise distance ρ(· ∥·).
Lemma D.2. For all (potentially stochastic) policies π and π′, it holds that
J(π) −J(π′) ≤R · ρ(π ∥π′).
Finally, using Lemma D.1, we show that when one of the policies is deterministic, the trajectory-wise distance
is equivalent to Hellinger distance up to an absolute constant.
Lemma D.3. Let π⋆be a deterministic policy and π be an arbitrary stochastic policy. Then we have that
1
4 · ρ(π⋆∥π) ≤D2
H

Pπ, Pπ⋆
≤2 · ρ(π⋆∥π).
Combining Lemmas D.2 and D.3, we conclude that for any deterministic policy π⋆and stochastic policy bπ,
J(π⋆) −J(bπ) ≤4R · D2
H
 Pbπ, Pπ⋆
.
41

Proof of Lemma D.1.
This follows by noting that we can write
ρ(π ∥π′) = 1 −Eπ Ea′
1:H∼π′(x1:H)[I{ah = a′
h ∀h}]
= 1 −
X
x1:H,a1:H,a′
1:H
P0(x1)
H
Y
h=1
Ph(xh+1 | xh, ah)πh(ah | xh)π′
h(a′
h | xh)I{ah = a′
h}
= 1 −
X
x1:H,a1:H,a′
1:H
P0(x1)
H
Y
h=1
Ph(xh+1 | xh, a′
h)πh(ah | xh)π′
h(a′
h | xh)I{ah = a′
h}
= 1 −Eπ′ Ea′
1:H∼π(x1:H)[I{ah = a′
h ∀h}] = ρ(π′ ∥π).
Proof of Lemma D.2.
Observe that since PH
h=1 rh ∈[0, R], we can bound the reward for π as
J(π) ≤Eπ
" H
X
h=1
rh
!
Ea′
1:H∼π′(x1:H)[I{a′
h = ah ∀h}]
#
+ R · Eπ Ea′
1:H∼π′(x1:H)[I{∃h : a′
h ̸= ah}]
= Eπ
" H
X
h=1
rh
!
Ea′
1:H∼π′(x1:H)[I{a′
h = ah ∀h}]
#
+ R · ρ(π ∥π′).
We can bound the first term as
Eπ
" H
X
h=1
rh
!
Ea′
1:H∼π′(x1:H)[I{a′
h = ah ∀h}]
#
= Eπh
f(x1:H, a1:H) Ea′
1:H∼π′(x1:H)[I{a′
h = ah ∀h}]
i
,
where f(x1:H, a1:H) := PH
h=1 E[rh | xh, ah]. We now observe that for any function f,
Eπh
f(x1:H, a1:H) Ea′
1:H∼π′(x1:H)[I{a′
h = ah ∀h}]
i
=
X
x1:H,a1:H,a′
1:H
f(x1:H, a1:H) · P0(x1)
H
Y
h=1
Ph(xh+1 | xh, ah)πh(ah | xh)π′
h(a′
h | xh)I{ah = a′
h}
=
X
x1:H,a1:H,a′
1:H
f(x1:H, a′
1:H) · P0(x1)
H
Y
h=1
Ph(xh+1 | xh, a′
h)πh(ah | xh)π′
h(a′
h | xh)I{ah = a′
h}
≤
X
x1:H,a′
1:H
f(x1:H, a′
1:H) · P0(x1)
H
Y
h=1
Ph(xh+1 | xh, a′
h)π′
h(a′
h | xh)
= Eπ′[f(x1:H, a1:H)].
We conclude that
Eπ
" H
X
h=1
rh
!
Ea′
1:H∼π′(x1:H)[I{a′
h = ah ∀h}]
#
≤J(π′),
so that
J(π) −J(π′) ≤R · ρ(π ∥π′).
42

Proof of Lemma D.3.
Define the triangular discrimination via D∆(P, Q) :=
R (dP−dQ)2
dP+dQ , and recall that
1
2D∆(P, Q) ≤D2
H(P, Q) ≤D∆(P, Q) (e.g., Foster and Krishnamurthy (2021)). Next, define the shorthand
P(x1:H | a1:H) := QH−1
h=0 P(xh+1 | xh, ah) and P π(a1:H | x1:H) := QH
h=1 πh(ah | xh) (these quantities do not
have an interpretation as conditional probability measures in the way the notation might suggest, but this
will not be relevant to the proof). For any deterministic policy π⋆, we can write
D∆

Pπ, Pπ⋆
=
X
x1:H
X
a1:H
P(x1:H | a1:H−1) · (P π(a1:H | x1:H) −P π⋆(a1:H | x1:H))2
P π(a1:H | x1:H) + P π⋆(a1:H | x1:H)
=
X
x1:H
X
a1:H=π⋆(x1:H)
P(x1:H | a1:H−1) · (P π(a1:H | x1:H) −P π⋆(a1:H | x1:H))2
P π(a1:H | x1:H) + P π⋆(a1:H | x1:H)
+
X
x1:H
X
a1:H̸=π⋆(x1:H)
P(x1:H | a1:H−1) · (P π(a1:H | x1:H) −P π⋆(a1:H | x1:H))2
P π(a1:H | x1:H) + P π⋆(a1:H | x1:H) .
Since π⋆is deterministic, P π⋆(a1:H | x1:H) = 1 if a1:H = π⋆(x1:H), and is P π⋆(a1:H | x1:H) = 0 otherwise.
Using this, we can write the second term above as
X
x1:H
X
a1:H̸=π⋆(x1:H)
P(x1:H | a1:H−1) · (P π(a1:H | x1:H) −0)2
P π(a1:H | x1:H) + 0
=
X
x1:H
X
a1:H̸=π⋆(x1:H)
P(x1:H | a1:H−1)P π(a1:H | x1:H)
= Pπ[∃h : ah ̸= π⋆(xH)] = ρ(π ∥π⋆).
This proves that D∆
 Pπ, Pπ⋆
≥ρ(π ∥π⋆). For the upper bound, we use that π⋆is deterministic once more
to write the first term above as
X
x1:H
X
a1:H=π⋆(x1:H)
P(x1:H | a1:H−1) · (P π(a1:H | x1:H) −1)2
P π(a1:H | x1:H) + 1
= Eπ⋆(P π(a1:H | x1:H) −1)2
P π(a1:H | x1:H) + 1

≤Eπ⋆
(P π(a1:H | x1:H) −1)2
.
We further note that
Eπ⋆
(P π(a1:H | x1:H) −1)2
= Eπ⋆Ea′
1:H∼π(x1:H)[1 + (P π(a′
1:H | x1:H) −2)I{a′
1:H = a1:H}]
≤Eπ⋆Ea′
1:H∼π(x1:H)[1 −I{a′
1:H = a1:H}]
= Eπ⋆Ea′
1:H∼π(x1:H)[I{∃h : a′
1:H ̸= a1:H}] = ρ(π⋆∥π).
By Lemma D.1, we conclude that D∆
 Pπ, Pπ⋆
≤ρ(π ∥π⋆) + ρ(π⋆∥π) = 2ρ(π⋆∥π).
D.2
Proof of Theorem 2.2
Proof of Theorem 2.2.
For this proof, we consider a slightly more general online imitation learning model
in which the learner is allowed to select ai
h based on the sequence (xi
1, ai
1, a⋆,i
1 ), . . . , (xi
h−1, ai
h−1, a⋆,i
h−1), (xi
h, a⋆,i
h )
at training time; this subsumes the offline imitation learning model. Let n ∈N and H ∈N be fixed. Let
∆∈(0, 1/3) be a parameter whose value will be chosen later.
We first specify the dynamics for the reward-free MDP M ⋆and the policy class Π. Set X = {x, y} and
A = {a, b}. The initial state distribution sets P0(x) = 1 −∆and P0(y) = ∆. The transition dynamics are
43

Ph(x′ | x, a) = I{x′ = x} for all h; that is, x, y are self-looping terminal states. We set Π = {πa, πb}, where the
expert policies are πa, which sets πa
h(x) = a for all h and x, and πb, which sets πb
h(x) = a and sets πb
h(y) = b.
Let a problem instance I = (M ⋆, r, π⋆) refer to a tuple consisting of the reward-free MDP M ⋆, a reward
function r = {rh}H
h=1, and an expert policy π⋆. We consider two problem instances, Ia = (M ⋆, ra, πa) and
Ib = (M ⋆, rb, πb):
• For problem instance Ia, the expert policy is πa. We set ra
h(x, ·) = 0, ra
h(y, a) = I{a = a} for all h.
• For problem instance Ib, the expert policy is πb. We set rb
h(x, ·) = 0, rb
h(y, a) = I{a = b} for all h.
Note that both of these instances satisfy µ = 1, and that πa and πb are optimal policies for their respective
instances. Let J a denote the expected reward function for instance Ia, and likewise for Ib.
Going forward, we fix the online imitation learning algorithm under consideration and let Pa denote the
law of o1, . . . , on when we execute the algorithm on instance a, and likewise for b; let E
a[·] and Eb[·]
denote the corresponding expectations. In addition, for any policy π, let Pπa|π denote the law of o =
(x1, a1, a⋆
1), . . . , (xH, aH, a⋆
H) when we execute π in the online imitation learning framework and the expert
policy is π⋆= πa, and define Pπb|π analogously.
We first observe that for any policy bπ,
J
a(π
a) −J
a(bπ) = ∆·
H
X
h=1
Eah∼bπh(y)[I{ah ̸= π
a
h(y)}],
and that J b(πb)−J b(bπ) = ∆· PH
h=1 Eah∼bπh(y)[I{ah ̸= πb
h(y)}]. Defining ρ(π, π′) = PH
h=1 Eah∼πh(y),a′
h∼π′
h(y) I{ah ̸= a′
h}
as a metric, we note that ρ(πa, πb) = H, and hence by the standard Le Cam two-point argument (e.g.,.
Wainwright (2019)), the algorithm must have
max{E
a[J
a(π
a) −J
a(bπ)], E
b[J
b(π
b) −J
b(bπ)]} ≥∆H
4 (1 −DTV(P
a, P
b)),
where DTV(·, ·) denotes total variation distance. Next, using Lemma D.2 of Foster et al. (2024), we can bound
D2
TV(P
a, P
b) ≤D2
H(P
a, P
b) ≤7 E
a
" n
X
i=1
D2
H

Pπa|πi, Pπb|πi#
.
Since, the feedback the learner receives for a given episode i is identical under instances Ia and Ib unless
x1 = y (regardless of how πi is chosen), we can bound
D2
H

Pπa|πi, Pπb|πi
≤2∆,
and hence
D2
TV(P
a, P
b) ≤14∆n.
We set ∆= 1/56n, and conclude that any algorithm must have
max{E
a[J
a(π
a) −J
a(bπ)], E
b[J
b(π
b) −J
b(bπ)]} ≥∆H
8
= c · H
n
for an absolute constant c > 0.
44

E
Proofs from Section 3
E.1
Proof of Theorem 3.1
Proof of Theorem 3.1. Assume without loss of generality that R = 1. Let o = (x1, a1), . . . , (xH, aH), and
for each h ∈[H], define the sum of advantages up to step h via
∆h(o) =
h
X
ℓ=1

Qπ⋆
ℓ(xℓ, π⋆
ℓ(xℓ)) −Qπ⋆
ℓ(xℓ, aℓ)

,
which has |∆(o)| ≤H almost surely. Consider the filtration Fh := σ(x1, a1, . . . , xh, ah). Fix a parameter
L ≥1 whose value will be chosen later, and define a random variable
H⋆:= min{h | |∆h(o)| > L},
with H⋆:= H +1 if there is no h such that |∆h(o)| > L; we will adopt the convention that Qπ⋆
H+1 = V π⋆
H+1 = 0.
Lemma E.1. H⋆is a stopping time with respect (Fh)h≥1,25 and has |∆H⋆(o)| ≤L + 1 almost surely.
The following lemma, which is one of the central technical components of this proof, gives a bound on regret
in terms of the expected advantage at the stopping time H⋆. We use the stopping time to keep the sum of
advantages ∆H⋆bounded, which facilitates a strong change-of-measure argument in the sequel.
Lemma E.2 (Regret decomposition for stopped advantages). If rh ≥0 and PH
h=1 rh ∈[0, R], then for all
policies bπ, we have that
J(π⋆) −J(bπ) ≤Ebπ[∆H⋆(o)] + R · Pbπ[H⋆≤H].
(25)
Note that even though we assume R = 1 throughout this proof, we state this lemma for general R for the
sake of keeping it self-contained.
We proceed to bound the right-hand-side of Eq. (25) using change-of-measure based on Hellinger distance
(Lemma 3.1). For the second term in Eq. (25), Lemma 3.1 gives
Pbπ[H⋆≤H] ≤2Pπ⋆[H⋆≤H] + D2
H

Pbπ, Pπ⋆
= 2Pπ⋆[∃h : |∆h(o)| > L] + D2
H

Pbπ, Pπ⋆
.
For the first term in Eq. (25), Lemma 3.1, gives that
Ebπ[∆H⋆(o)] ≤Eπ⋆[∆H⋆(o)] +
r
1
2

Ebπ[∆2
H⋆(o)] + Eπ⋆[∆2
H⋆(o)]

· D2
H(Pbπ, Pπ⋆).
To bound the first moment and second moment of ∆H⋆(o) under π⋆, we use the following lemma, which
follows from elementary properties of stopped martingale difference sequences.
Lemma E.3. We have that
Eπ⋆[∆H⋆(o)] ≤0,
and
Eπ⋆
∆2
H⋆(o)

≤4σ2
π⋆.
It remains to bound the second moment under bπ. Here, since |∆H⋆(o)| ≤L + 1 almost surely by Lemma E.1,
we note that Lemma 3.1 gives
Ebπ
∆2
H⋆(o)

≤2 Eπ⋆
∆2
H⋆(o)

+ (L + 1)2D2
H

Pbπ, Pπ⋆
.
25That is, for all h, I{h = H⋆} is a measurable function of (x1, a1), . . . , (xh, ah).
45

Combining these developments, we have that
Ebπ[∆H⋆(o)] ≤
q
3
2 Eπ⋆[∆2
H⋆(o)] · D2
H(Pbπ, Pπ⋆) + (L + 1)D2
H

Pbπ, Pπ⋆
≤
q
6σ2
π⋆· D2
H(Pbπ, Pπ⋆) + (L + 1)D2
H

Pbπ, Pπ⋆
,
and thus
J(π⋆) −J(bπ) ≤
q
6σ2
π⋆· D2
H(Pbπ, Pπ⋆) + (L + 2)D2
H

Pbπ, Pπ⋆
+ 2Pπ⋆[∃h : |∆h(o)| > L].
To wrap up, we appeal to the second of our main technical lemmas, Lemma 3.2.
Let ε ∈(0, e−1) be fixed. If
we define
L = c · log(ε−1),
where c > 1 is a sufficiently large absolute constant, then by Lemma 3.2, we have that
Pπ⋆[∃h : |∆h(o)| > L] ≤ε.
This proves the result.
Proof of Lemma E.1.
To prove that H⋆is a stopping time, we observe that for all h ≤H, we have
I{h = H⋆} = I{|∆h(o)| > L, |∆h′(o)| ≤L ∀h′ < h},
and ∆h(o) is a measurable function of (x1, a1), . . . , (xh, ah). Likewise, we have
I{h = H⋆+ 1} = I{|∆h(o)| ≤L ∀h ≤H},
which is a measurable function of (x1, a1), . . . , (xH, aH).
For the second claim, we observe that
|∆H⋆(o)| ≤|∆H⋆−1(o)| +
Qπ⋆
H⋆(xH⋆, π⋆
H⋆(xH⋆)) −Qπ⋆
H⋆(xH⋆, aH⋆)

≤L + 1
almost surely.
Proof of Lemma E.3.
Define Xh := Qπ⋆
h (xh, π⋆
h(xh)) −Qπ⋆
h (xh, ah), and Fh = σ(x1, a1, . . . , xh, ah), with
XH+1 := 0. Since H⋆is a stopping time with respect to (Fh) and Xh is a martingale difference sequence
(under π⋆), the optional stopping theorem (e.g., Williams (1991)) implies that26
Eπ⋆[∆H⋆(o)] = Eπ⋆
" H⋆
X
h=1
Xh
#
= 0.
We now bound the second moment. Recall Doob’s maximal inequality (e.g., Williams (1991)).
26To give self-contained proof, note that we can write Eπ⋆hPH⋆
h=1 Xh
i
= Eπ⋆hPH
h=1 XhI{H⋆≥h}
i
We claim that
I{H⋆≥h} is a measurable function of Fh−1, since I{H⋆≥h} = 1 −I{H⋆< h}, and I{H⋆= h′} is a measurable func-
tion of (x1, a1), . . . , (xh′, ah′) ⊂(x1, a1), . . . , (xh−1, ah−1) for h′ < h.
We conclude that Eπ⋆[XhI{H⋆≥h} | Fh−1] =
Eπ⋆[Xh | Fh−1]I{H⋆≥h} = 0.
46

Lemma E.4. If (Sh)h∈[H] is a non-negative submartingale, then
E

max
h∈[H] S2
h

≤4 E

S2
H

.
We claim that |∆h(o)| is a submartingale, since a convex function of a martingale is a submartingale.27 As a
result, Lemma E.4 gives that
E

∆2
H⋆(o)

≤E

max
h∈[H] ∆2
h(o)

≤4 E

∆2
H(o)

.
Finally, we note that
Eπ⋆
∆2
H(o)

= Eπ⋆


 H
X
h=1

Qπ⋆
h (xh, π⋆
h(xh)) −Qπ⋆
h (xh, ah)
!2

=
H
X
h=1
Eπ⋆h
(Qπ⋆
h (xh, π⋆
h(xh)) −Qπ⋆
h (xh, ah))2i
= σ2
π⋆,
where we have once more used that Xh = Qπ⋆
h (xh, π⋆
h(xh)) −Qπ⋆
h (xh, ah) is a martingale difference sequence.
E.1.1
Proof of Lemma E.2 (Regret Decomposition for Stopped Advantages)
Proof of Lemma E.2. Consider the following non-Markovian policy:
eπh(· | x1:h, a1:h−1) =
 bπh(· | xh)
h ≤H⋆,
π⋆
h(· | xh)
h > H⋆.
This is a well-defined policy, since we can write I{h > H⋆} = maxh′<h I{h′ = H⋆}, and I{h′ = H⋆} is a
measurable function of (x1, a1), . . . , (xh′, ah′) ⊂(x1, a1), . . . , (xh−1, ah−1) for h′ < h.
We begin by writing
J(π⋆) −J(bπ) = J(π⋆) −J(eπ) + J(eπ) −J(bπ).
(26)
For the second pair of terms in Eq. (26), we use the following lemma.
Lemma E.5. Under the same assumptions as Lemma E.2, it holds that
J(eπ) −J(bπ) ≤R · Pbπ[H⋆≤H].
27For completeness, note that E[|∆h(o)| | Fh−1] = E[|∆h−1(o) + Xh| | Fh−1] ≥|∆h−1(o) + E[Xh | Fh−1]| = |∆h−1(o)|.
47

For the first pair of terms in Eq. (26), using the performance difference lemma, we can write28
J(π⋆) −J(eπ) = Eeπ
" H
X
h=1
Qπ⋆
h (xh, π⋆
h(xh)) −Qπ⋆
h (xh, ah)
#
= Eeπ
" H
X
h=1
Eh−1
h
Qπ⋆
h (xh, π⋆
h(xh)) −Qπ⋆
h (xh, ah)
i#
= Eeπ
" H
X
h=1
Eh−1
h
Qπ⋆
h (xh, π⋆
h(xh)) −Qπ⋆
h (xh, ah)
i
I{h ≤H⋆}
#
= Eeπ
" H
X
h=1
Eh−1
h
Qπ⋆
h (xh, π⋆
h(xh)) −Qπ⋆
h (xh, ah)

I{h ≤H⋆}
i#
= Eeπ
" H⋆
X
h=1
Qπ⋆
h (xh, π⋆
h(xh)) −Qπ⋆
h (xh, ah)
#
= Eeπ[∆H⋆(o)],
where the third equality uses that eπh(· | x1:h, a1:h−1) = π⋆
h(· | xh) for h > H⋆, and the fourth equality uses
that I{h ≤H⋆} is Fh−1-measurable. We now appeal to the following lemma, proven in the sequel.
Lemma E.6. Under the same assumptions as Lemma E.2, it holds that
Eeπ[∆H⋆(o)] = Ebπ[∆H⋆(o)].
Altogether, we conclude that
J(π⋆) −J(bπ) ≤Ebπ[∆H⋆(o)] + R · Pbπ[H⋆≤H].
Proof of Lemma E.5.
Let us define f(o) = PH
h=1 E[rh | xh, ah] and g(o) = I{H⋆> H}; note that
g(o) is indeed a measurable function of o = (x1, a1), . . . , (xH, aH), since I{H⋆> H} = 1 −I{H⋆≤H},
{H⋆≤H} = ∪h≤H{H⋆= h}, and {H⋆= h} is a measurable function of (x1, a1), . . . , (xh, ah). We can write
J(eπ) ≤Eeπ
" H
X
h=1
rh
!
I{H⋆> H}
#
+ R · Peπ[H⋆≤H].
(27)
Let us adopt the shorthand P(x1:H | a1:H−1) := QH−1
h=0 Ph(xh+1 | xh, ah). We can bound the first term in
Eq. (27) via
Eeπ
" H
X
h=1
rh
!
I{H⋆> H}
#
=
X
o=x1:H,a1:H
f(o)g(o)P(x1:H | a1:H−1)
H
Y
h=1
eπh(ah | x1:h, a1:h−1)
=
X
o=x1:H,a1:H
f(o)g(o)P(x1:H | a1:H−1)
H
Y
h=1
bπh(ah | xh)
≤
X
o=x1:H,a1:H
f(o)P(x1:H | a1:H−1)
H
Y
h=1
bπh(ah | xh)
= Ebπ
" H
X
h=1
rh
#
= J(bπ),
28Since eπ is non-Markovian, we need to expand the state space to x′
h = x1:h, a1:h−1 to apply the performance difference
lemma, but since π⋆itself is Markovian, this results in the claimed expression.
48

where the second equality uses that eπ(· | x1:h, a1:h−1) = bπ(· | xh) for all h ∈[H] whenever g(o) = 1.
To bound the second term in Eq. (27), we can write
Peπ[H⋆≤H] =
H
X
h=1
Peπ[H⋆= h].
For each h, let oh := (x1, a1), . . . , (xh, ah) and gh(oh) := I{H⋆= h} (recall that I{H⋆= h} is a measurable
function of (x1, a1), . . . , (xh, ah)). Note that for each h, if we define P(x1:h | a1:h−1) := Qh−1
h=0 Pℓ(xℓ+1 | xℓ, aℓ),
then
Peπ[H⋆= h] =
X
oh=x1:h,a1:h
gh(oh)P(x1:h | a1:h−1)
h
Y
ℓ=1
eπℓ(aℓ| x1:ℓ, a1:ℓ−1)
=
X
oh=x1:h,a1:h
gh(oh)P(x1:h | a1:h−1)
h
Y
ℓ=1
bπℓ(aℓ| xℓ)
= Pbπ[H⋆= h],
where the second inequality uses that eπ(· | x1:ℓ, a1:ℓ−1) = bπ(· | xℓ) whenever ℓ≤H⋆.
Proof of Lemma E.6.
We start by writing
Eeπ[∆H⋆(o)] =
H+1
X
h=1
Eeπ[I{H⋆= h}∆h(o)].
For each h ≤H + 1, let oh := (x1, a1), . . . , (xh, ah) and gh(oh) := I{H⋆= h} (recall that I{H⋆= h} is
a measurable function of (x1, a1), . . . , (xh, ah)).
For each h ≤H + 1, if we define P(x1:h | a1:h−1) :=
Qh−1
h=0 Pℓ(xℓ+1 | xℓ, aℓ), then
Eeπ[I{H⋆= h}∆h(o)] =
X
oh=x1:h,a1:h
gh(oh)∆h(oh)P(x1:h | a1:h−1)
h
Y
ℓ=1
eπℓ(aℓ| x1:ℓ, a1:ℓ−1)
=
X
oh=x1:h,a1:h
gh(oh)∆h(oh)P(x1:h | a1:h−1)
h
Y
ℓ=1
bπℓ(aℓ| xℓ)
= Ebπ[I{H⋆= h}∆h(o)],
where the second inequality uses that eπ(· | x1:ℓ, a1:ℓ−1) = bπ(· | xℓ) whenever ℓ≤H⋆.
E.1.2
Proof of Lemma 3.2 (Concentration for Advantages)
Lemma 3.2 is proven using arguments similar to those in Zhang et al. (2021, 2022), but requires non-trivial
modifications to accommodate the fact that π is an arbitrary, potentially suboptimal policy.
Proof of Lemma 3.2.
Let us abbreviate Q = Qπ and V = V π. Assume without loss of generality that
R = 1, and note that this implies that rh ∈[0, 1] and Qh, Vh ∈[0, 1], which we will use throughout the proof.
Define a filtration Fh−1 := σ((x1, a1, r1), . . . , (xh−1, ah−1, rh−1), xh). Since
Eh−1[Qh(xh, ah) −Vh(xh)] = 0,
two applications of Lemma B.2 and a union bound imply that with probability at least 1 −δ, for all H′ ∈[H]

H′
X
h=1
Qh(xh, ah) −Vh(xh)

≤
H′
X
h=1
Eπ
(Qh(xh, ah) −Vh(xh))2 | xh

+ log(2δ−1).
49

Since Eπ[Qh(xh, ah) | xh] = Vh(xh), we can write
H′
X
h=1
Eπ
(Qh(xh, ah) −Vh(xh))2 | xh

=
H′
X
h=1
Eπ
(Q2
h(xh, ah) | xh

−V 2
h (xh)
=
H′
X
h=1
 Eπ
(Q2
h(xh, ah) | xh

−V 2
h+1(xh+1)

+ V 2
H′+1(xH′+1) −V 2
1 (x1)
≤
H′
X
h=1
 Eπ
(Q2
h(xh, ah) | xh

−V 2
h+1(xh+1)

+ 1.
Observe that by Jensen’s inequality, we have
Eπ
(Q2
h(xh, ah) | xh

≤Eπ
(rh + Vh+1(xh+1))2 | xh

= Eπ
V 2
h+1(xh+1) | xh

+ Eπ
r2
h | xh

+ 2 Eπ[rhVh+1(xh+1) | xh]
≤Eπ
V 2
h+1(xh+1) | xh

+ 3 Eπ[rh | xh],
so that
H′
X
h=1
Eπ
(Qh(xh, ah) −Vh(xh))2 | xh

≤
H′
X
h=1
Eπ
V 2
h+1(xh+1) | xh

−V 2
h+1(xh+1) + 3
H′
X
h=1
Eπ[rh | xh] + 1.
By Lemma B.3, we have that with probability at least 1 −δ, for all H′ ∈[H],
H′
X
h=1
Eπ[rh | xh] ≤3
2
H′
X
h=1
rh + 4 log(2δ−1)
≤3
2 + 4 log(2δ−1).
Likewise, by Lemma B.2, we have that with probability at least 1 −δ, for all H′ ∈[H],
H′
X
h=1
Eπ
V 2
h+1(xh+1) | xh

−V 2
h+1(xh+1) ≤
H′
X
h=1
Eπh V 2
h+1(xh+1) −Eπ
V 2
h+1(xh+1) | xh
2 | xh
i
+ log(δ−1)
=
H′
X
h=1
Varπ
V 2
h+1(xh+1) | xh

+ log(δ−1)
≤4
H′
X
h=1
Varπ[Vh+1(xh+1) | xh] + log(δ−1),
where the last line uses the following lemma, proven in the sequel.
Lemma E.7. If X is a random variable with |X| ≤1, then
Var(X2) ≤4Var(X).
We now appeal to the following lemma, also proven in the sequel.
Lemma E.8. Under the same setting as Lemma 3.2, we have that for any δ ∈(0, 1), with probability at least
1 −2δ, for all H′ ∈[H],
H′
X
h=1
Varπ
V π
h+1(xh+1) | xh

≤8 + 32 log(2δ−1).
50

Putting together all of the developments so far, we have that with probability at least 1 −5δ, for all H′ ∈[H],

H′
X
h=1
Qh(xh, ah) −Vh(xh)

≤4
H′
X
h=1
Varπ[Vh+1(xh+1) | xh] + 6 + 14 log(2δ−1)
≤38 + 142 log(2δ−1).
Proof of Lemma E.7.
Note that we have
Var(X2) = E

(X2 −E

X2
)2
≤E
h
(X2 −E[X]2)2i
≤4 E

(X −E[X])2
,
where the last line uses that
a2 −b2 ≤2|a −b| for a, b ∈[−1, 1].
Proof of Lemma E.8.
Abbreviate V ≡V π. By telescoping, we can write
ZH′ :=
H′
X
h=1
Varπ[Vh+1(xh+1) | xh]
=
H′
X
h=1
Eπ
V 2
h+1(xh+1) | xh

−(Eπ[Vh+1(xh+1) | xh])2
=
H′
X
h=1
Eπ
V 2
h+1(xh+1) | xh

−V 2
h+1(xh+1) +
H′
X
h=1
V 2
h (xh) −(Eπ[Vh+1(xh+1) | xh])2 + V 2
H′+1(xH′+1) −V 2
1 (x1)
≤
H′
X
h=1
Eπ
V 2
h+1(xh+1) | xh

−V 2
h+1(xh+1) +
H′
X
h=1
V 2
h (xh) −(Eπ[Vh+1(xh+1) | xh])2 + 1.
For the latter term, since
a2 −b2 ≤2|a −b| for a, b ∈[0, 1], we have that
H′
X
h=1
V 2
h (xh) −(Eπ[Vh+1(xh+1) | xh])2 ≤2
H′
X
h=1
|Vh(xh) −Eπ[Vh+1(xh+1) | xh]|
= 2
H′
X
h=1
|Eπ[rh | xh]| ≤2
H′
X
h=1
Eπ[rh | xh],
By Lemma B.3, we have that with probability at least 1 −δ, for all H′ ∈[H],
H′
X
h=1
Eπ[rh | xh] ≤3
2
H′
X
h=1
rh + 4 log(2δ−1) ≤3
2 + 4 log(2δ−1).
For the first term, by Lemma B.2, we have that for all η ∈(0, 1), with probability at least 1 −δ, for all
H′ ∈[H],
H′
X
h=1
Eπ
V 2
h+1(xh+1) | xh

−Vh+1(xh+1) ≤η
H′
X
h=1
Eπh V 2
h+1(xh+1) −Eπ
V 2
h+1(xh+1) | xh
2 | xh
i
+ η−1 log(δ−1)
= η
H′
X
h=1
Varπ
V 2
h+1(xh+1) | xh

+ η−1 log(δ−1)
≤4η
H′
X
h=1
Varπ[Vh+1(xh+1) | xh] + η−1 log(δ−1)
= 4ηZH′ + η−1 log(δ−1),
51

where the last inequality uses Lemma E.7. Putting everything together and setting η = 1/8, we conclude
that with probability at least 1 −2δ, for all H′ ∈[H]
ZH′ ≤1
2ZH′ + 16 log(2δ−1) + 4,
which yields the result after rearranging.
E.2
Proof of Theorem 3.2
Proof of Theorem 3.2.
For this proof, we consider a slightly more general online imitation learning model
in which the learner is allowed to select ai
h based on the sequence (xi
1, ai
1, a⋆,i
1 ), . . . , (xi
h−1, ai
h−1, a⋆,i
h−1), (xi
h, a⋆,i
h )
at training time; this subsumes the offline imitation learning model. Let H ∈N, n ∈N, and σ2 ∈[H, H2] be
given. Fix a parameter K ∈N such that H/K is an integer and a parameter ∆∈(0, 1/2) be fixed; both
parameters will be chosen at the end of the proof.
We first specify the dynamics for the reward-free MDP M ⋆and the policy class Π. Let A = {a, b}, and
let X = {s, a, b}. We consider the following (deterministic) dynamics. For h ∈H := [1, K + 1, 2K + 1, . . .],
always the state is always xh = s. For such a step h ∈H, choosing ah = a sets xh = a for the next K −1
steps until returning to s at time h + K, and choosing ah = b sets xh = b until returning to s at time h + K
(that is, the action has no effect for h /∈H).
We consider a class Π = {πa, πb} consisting of two experts πa and πb. πa sets πa
h(a | s) = 1
2 + ∆for h ∈H and
sets πh(x) = a for all h /∈H and x ∈X. Meanwhile, πb sets πb(b | s) = 1
2 + ∆for h ∈H and sets πh(x) = a
for all h /∈H and x ∈X.
We consider two choices of reward function, ra and rb. ra sets ra
h(s, a) = 1 and ra
h(s, b) = 0 for h ∈H, and
sets ra
h(a, ·) = 1 and ra
h(b, ·) = 0 for h /∈H. Meanwhile, rb sets rb
h(s, b) = 1 and rb
h(s, a) = 0 for h ∈H and
sets rb
h(a, ·) = 0 and rb
h(b, ·) = 1 for h /∈H.
Let a problem instance I = (M ⋆, r, π⋆) refer to a tuple consisting of the reward-free MDP M ⋆, a reward
function r = {rh}H
h=1, and an expert policy π⋆. We consider four problem instances altogether: (M ⋆, ra, πa),
(M ⋆, rb, πa), (M ⋆, ra, πb), and (M ⋆, rb, πb).
Let Pa denote the law of o1, . . . , on when a when we execute the algorithm on the underlying instance, and
likewise for b (recall that the law does not depend on the choice of reward function, since this is not observed);
let E
a[·] and Eb[·] denote the corresponding expectations. In addition, for any policy π, let Pπa|π denote the
law of o = (x1, a1, a⋆
1), . . . , (xH, aH, a⋆
H) when we execute π in the online imitation learning framework and
the expert policy is π⋆= πa, and define Pπb|π analogously.
We begin by lower bounding the regret. Consider a fixed policy bπ = {bπh : X →∆(X)}, and let π(a) :=
1
|H|
P
h∈H bπh(a | s). Observe that for instance (M ⋆, ra, πa), we have
Jra(π
a) −Jra(bπ) =
1
2 + ∆

H −K
X
h∈H
bπh(a | s) =
1
2 + ∆

H −Hπ(a)
and for instance (M ⋆, rb, πa),
Jrb(π
a) −Jrb(bπ) =
1
2 −∆

H −K
X
h∈H
bπh(b | s) = Hπ(a) −
1
2 + ∆

H.
Likewise, for instance (M ⋆, rb, πb), we have
Jrb(π
b) −Jrb(bπ) =
1
2 + ∆

H −K
X
h∈H
bπh(b | s) = π(a)H −
1
2 −∆

H
52

and for instance (M ⋆, ra, πb),
Jra(π
b) −Jra(bπ) =
1
2 −∆

H −K
X
h∈H
bπh(a | s) =
1
2 −∆

H −π(a)H.
We conclude that for any ε > 0, since the law of the dataset is independent of the choice of the reward
function,
max{P
a[Jra(π
a) −Jra(bπ) ≥εH], P
a[Jrb(π
a) −Jrb(bπ) ≥εH], P
b[Jrb(π
b) −Jrb(bπ) ≥εH], P
b[Jra(π
b) −Jra(bπ) ≥εH]}
≥max







P
a
1
2 + ∆

H −π(a)H ≥εH

, P
a

π(a)H −
1
2 + ∆

H ≥εH

,
P
b

π(a)H −
1
2 −∆

H ≥εH

, P
b
1
2 −∆

H −π(a)H ≥εH








≥1
2 max

P
a

1
2 + ∆

−π(a)
H ≥εH

, P
b
π(a) −
1
2 −∆
H ≥εH

= 1
2 max

P
a

1
2 + ∆

−π(a)
 ≥ε

, P
b
π(a) −
1
2 −∆
 ≥ε

≥1
4

P
a

1
2 + ∆

−π(a)
 ≥ε

+ P
b
π(a) −
1
2 −∆
 ≥ε

≥1
4

1 −P
a

1
2 + ∆

−π(a)
 ≤ε

+ P
b
π(a) −
1
2 −∆
 ≥ε

≥1
4

1 −P
a

1
2 −∆

−π(a)
 ≥ε

+ P
b
π(a) −
1
2 −∆
 ≥ε

≥1
4(1 −DTV(P
a, P
b)),
where the second inequality uses the union bound (i.e. P[|x| ≥ε] = P[x ≥ε ∪−x ≥ε] ≤P[x ≥ε]+P[−x ≥ε]),
and the second-to-last inequality holds as long as ε < ∆. In particular, this implies that
max







P
a

Jra(π
a) −Jra(bπ) ≥∆H
2

, P
a

Jrb(π
a) −Jrb(bπ) ≥∆H
2

,
P
b

Jrb(π
b) −Jrb(bπ) ≥∆H
2

, P
b

Jra(π
b) −Jra(bπ) ≥∆H
2








≥1
4(1 −DTV(P
a, P
b)).
Next, using Lemma D.2 of Foster et al. (2024), we can bound
D2
TV(P
a, P
b) ≤D2
H(P
a, P
b) ≤7 E
a
" n
X
i=1
D2
H

Pπa|πi, Pπb|πi#
.
Observe that for a given episode i, regardless of how the policy πi is selected:
• The feedback for steps h /∈H is identical under Pa and Pb.
• The feedback at step h ∈H differs only in the distribution of a⋆
h ∼πa(s) versus a⋆
h ∼πb(s). This is
equivalently to Ber(1/2 + ∆) feedback versus Ber(1/2 −∆) feedback.
As a result, using Lemma D.2 of Foster et al. (2024) once more, we have
D2
H

Pπa|πi, Pπb|πi
≤7
X
h∈H
D2
H(Ber(1/2 + ∆), Ber(1/2 −∆))
Since ∆∈(0, 1/2), we have D2
H(Ber(1/2 + ∆), Ber(1/2 −∆)) ≤O(∆2) (e.g., Foster et al. (2021, Lemma A.7)).
We conclude that
D2
TV(P
a, P
b) ≤O
 n · |H| · ∆2
= O

n · H
K · ∆2

53

We set ∆2 = c ·
K
Hn for c > 0 sufficiently small so that D2
TV(Pa, Pb) ≤1/2, and conclude that on at least one
of the four problem instances, the algorithm must have
J(π⋆) −J(bπ) ≥Ω(∆H) = Ω
 r
HK
n
!
with probability at least 1/8.
Finally, we compute the variance and choose the parameter K. Observe that for all of the choices of expert
policy and reward function described above, we have Qπ⋆
h (xh, π⋆(xh)) −Qπ⋆
h (xh, a) = 0 for h /∈H, while
Qπ⋆
h (xh, π⋆(xh)) −Qπ⋆
h (xh, a)
 ≤K
for h ∈H, so we can take eµ ≤K. Consequently, we have
σ2
π⋆=
H
X
h=1
Eπ⋆h
(Qπ⋆
h (xh, π⋆(xh)) −Qπ⋆
h (xh, ah))2i
≤
X
h∈H
Eπ⋆h
(Qπ⋆
h (s, π⋆(s)) −Qπ⋆
h (s, ah))2i
≤H
K · K2 = HK.
We conclude by setting K = σ2/H, which is admissible for σ2 ∈

H, H2
(up to a loss in absolute constants,
we can assume that σ2/H is an integer without loss of generality).
E.3
Additional Proofs
Proof of Proposition 3.1.
We have
σ2
π⋆=
H
X
h=1
Eπ⋆h
(Qπ⋆
h (xh, ah) −V π⋆
h (xh))2i
.
Note that Qπ⋆
h (xh, ah) = E

rh + V π⋆
h (xh+1) | xh, ah

. Hence, by Jensen’s inequality we can bound
Eπ⋆h
(Qπ⋆
h (xh, ah) −V π⋆
h (xh))2i
≤Eπ⋆h
E
h
(rh + V π⋆
h+1(xh+1) −V π⋆
h (xh))2 | xh, ah
ii
= Eπ⋆h
Eπ⋆h
(rh + V π⋆
h+1(xh+1) −V π⋆
h (xh))2 | xh
ii
= Eπ⋆h
Varπ⋆h
rh + V π⋆
h+1(xh+1) | xh
ii
,
so that
σ2
π⋆≤Eπ⋆
" H
X
h=1
Varπ⋆h
rh + V π⋆
h+1(xh+1) | xh
i#
≤Eπ⋆
" H
X
h=0
Varπ⋆h
rh + V π⋆
h+1(xh+1) | xh
i#
= Varπ⋆
" H
X
h=1
rh
#
≤R2,
where the second to last inequality follows from Lemma B.5.
54

F
Proofs from Section 4
F.1
Proof of Proposition 4.1
Proof of Proposition 4.1.
Let N ∈N be given. We set X = {x, y, z}, A = [N] ∪{a, b}, and H = 2. We
consider a family of problem instances {(M, π⋆, r)} indexed by a subset S ⊂[N] with |S| = N/2 and an
action a⋆∈{a, b} as follows. For a given pair (S, a⋆):
• The dynamics are as follows. We have x1 = x deterministically. For simplicity, we assume that only
actions in [N] are available at step h = 1. If a1 ∈S, then x2 = y, otherwise x2 = z.
• The reward at step 1 is r1(·, ·) = 0, and the reward at step 2 is given by r2(y, ·) = 1 and r2(z, a) =
I{a = a⋆}.
• The expert π⋆sets π⋆(x) = unif(S), π⋆(y) = unif({a, b}), and π⋆(z) = a⋆.
Let us refer to the problem instance above as IS,a⋆=

(MS,a⋆, π⋆
S,a⋆, rS,a⋆)
	
, and let JS,a⋆(π) denote the
expected reward under this instance.
Upper bound for online imitation learning.
Consider the algorithm that sets bπi
1 = unif([N]) for each
i ∈[n]. If we play for n = log2(δ−1) episodes, we will see x2 = z in at least one episode with probability at least
1 −δ, at which point we will observe a⋆= π⋆(z), and we can return the policy bπ that sets bπ1(x) = unif([N])
and bπ2(·) = a⋆; this policy has zero regret.
Note that if we define Π =

π⋆
S,a⋆
	
|S|=N/2,a⋆∈{a,b} as the natural policy class for the family of instances
above, then the algorithm above is equivalent to running Dagger with the online learning algorithm that, at
iteration i, sets
bπ
i
h = unif
 
π ∈Πh | π2(z) = a⋆,j
2
∀j < i : x
j
2 = z
	
,
and choosing the final policy as bπ = bπi for any iteration i after x2 = z is encountered.
Lower bound for offline imitation learning.
Consider the offline imitation learning setting. When
the underlying instance is IS,a⋆, we observe a dataset D consisting of n trajectories generated by executing
π⋆
S,a⋆in MS,a⋆. The trajectories never visit the state z, so a⋆is not identifiable, and we can do no better
than guessing a⋆uniformly in this state. Letting ES,a⋆denote the law of D under instance IS,a⋆, we have
JS,a⋆(bπ) = bπ1(S | x) + bπ1(Sc | x)bπ2(a⋆| z). It follows that for any S, since the law of D does not depend on
a⋆,
max
a⋆∈{a,b} ES,a⋆
JS,a⋆(π⋆
S,a⋆) −JS,a⋆(bπ)

≥ES,a[1 −bπ1(S | x) −bπ1(Sc | x)/2]
= 1
2 ES,a[1 −bπ1(S | x)].
Note that if bπ is proper in the sense that bπ1(·x) = unif(bS) for some bS ⊂[N] with |bS| = N/2, we have
1−bπ1(S | x) = 1−2
N |bS∪S|. We conclude that if ES,a⋆
JS,a⋆(π⋆
S,a⋆) −JS,a⋆(bπ)

≤1
8, then ES,a⋆
|bS∩S|

≥3
8N.
From here, it follows from standard lower bounds for discrete distribution estimation (e.g., Canonne (2020))
that any such estimator bS requires n = Ω(N) samples for a worst-case choice of S.
F.2
Background and Proof for Proposition 4.2
Before proving Proposition 4.2, we first formally introduce the value-based feedback model we consider.
Background on value-based feedback.
We can consider two models for imitation learning with value-
based feedback, inspired by Ross and Bagnell (2014); Sun et al. (2017).
55

• Offline setting.
In the offline setting, we receive n trajectories (x1, a1), . . . , (xH, aH) generated
by executing π⋆in M ⋆.
For each state in each such trajectory, we observe Aπ⋆
h (xh, ·), where
Aπ⋆
h (x, a) = Qπ⋆
h (x, π⋆(x)) −Qπ⋆
h (x, a) is the advantage function for π⋆.29
• Online setting. The online setting is as follows. There are n at episodes. For each episode i, we
execute a policy bπi, and receive a “trajectory” oi = (xi
1, ai
1, a⋆,i
1 ), . . . , (xi
H, ai
H, a⋆,i
H ), where ai
h ∼bπi(xi
h)
and a⋆,i
h
∼π⋆(xi
h). In addition, for each state in the trajectory, we observe Aπ⋆
h (xh, ·). After the n
episodes conclude, we output a final policy bπ on which performance is evaluated.
Proof of Proposition 4.2.
We only sketch the proof, as it is quite similar to Proposition 4.1. Let N ∈N
be given. We set S = {x, y, z}, A = [N], and H = 2. We consider a class of problem instances {(M, π⋆, r)}
indexed by sets S1, S2 ⊂[N] with |S1| = |S2| = N/2 defined as follows. For a given pair (S1, S2):
• The dynamics are as follows. We have x1 = x deterministically. If a1 ∈S1, then x2 = y, otherwise
x2 = z.
• The reward function sets r1(x, ·) = 0, r2(y, ·) = 1, and r2(z, a) = I{a ∈S2}.
• The expert π⋆sets π⋆(x) = unif(S1), π⋆(z) = unif(S2), and π⋆(y) = unif([N])
We refer to the problem instance above as IS1,S2 = (MS1,S2, π⋆
S1,S2, rS1,S2), and let JS1,S2(π) denote the
expected reward under this instance.
Upper bound for online imitation learning with value-based feedback.
Consider an algorithm that
sets bπi
1 = unif([N]) for each i ∈[n]. If we play for n = log2(δ−1) episodes, we will see x2 = z in at least one
episode with probability at least 1 −δ, at which point we will observe Aπ⋆
2 (z, ·). We can pick an arbitrary
action with Aπ⋆
2 (z, ·) = 0 and return the policy bπ that sets bπ1(x) = unif([N]) and bπ2(·) = a; this policy has
zero regret.
Note that if we define Π =

π⋆
S1,S2
	
|S1|=|S2|=N/2 as the natural policy class for the family of instances above,
then the algorithm above is equivalent to running Aggrevate with the online learning algorithm that, at
iteration i, sets
bπ
i
h = unif
 
π ∈Πh | π2(z) ∈arg max
a
Aπ⋆
2 (x
j
2, a) ∀j < i : x
j
2 = z
	
,
and choosing the final policy as bπ = bπi for any iteration i after x2 = z is encountered.
Lower bound for offline imitation learning.
Consider the offline imitation learning setting. When the
underlying instance is IS1,S1, we observe a dataset D consisting of n trajectories generated by executing
π⋆
S1,S2 in MS1,S2. The trajectories never visit the state z, so S2 is not identifiable, and we can do no better
than guessing uniformly in this state. Letting ES1,S2 denote the law of D under instance IS1,S2, we have
JS1,S2(bπ) = bπ1(S1 | x) + bπ1(Sc
1 | x)bπ2(S2 | z). It follows that for any (S1, S2), since the law of D does not
depend on S2,
max
S2:|S2|=N/2 ES1,S2

JS1,S2(π⋆
S1,S2) −JS1,S2(bπ)

≥ES1[1 −bπ1(S1 | x) −bπ1(Sc
1 | x)/2]
= 1
2 ES1[1 −bπ1(S1 | x)],
with the convention that ES1 denotes the law of D for an arbitrary choice of S2. If bπ is proper in the sense
that bπ1(·x) = unif(c
S1) for some c
S1 ⊂[N] with |c
S1| = N/2, we have 1 −bπ1(S1 | x) = 1 −2
N |c
S1 ∪S1|. We
conclude that if ES1,S2

JS1,S2(π⋆
S1,S2) −JS1,S2(bπ)

≤1
8, then ES1,

|c
S1 ∩S1|

≥3
8N. From here, it follows
from standard lower bounds for discrete distribution estimation (e.g., Canonne (2020)) that any such estimator
bS requires n = Ω(N) samples for a worst-case choice of S.
29Our results are not sensitive to whether the learner observes the advantage function or the value function itself; we choose
this formulation for concreteness.
56

Lower bound for online imitation learning without value-based-feedback.
Consider an online
imitation learning algorithm that does not receive value-based feedback. We claim, via an argument similar
to the one above, that if the algorithm that ensures
ES1,S2

JS1,S2(π⋆
S1,S2) −JS1,S2(bπ)

≤c
on all instances for a sufficiently small absolute constant c, then it can be used to produce estimators
c
S1, c
S2 ⊂[N] such that with constant probability, either
c
S1∩S1
 ≥3
8N or
c
S2∩S2
 ≥3
8N. From here, it should
follow from standard arguments that this requires n = Ω(N) samples for a worst-case choice of S1 and S2.
F.3
Proof of Proposition 4.3
Proof of Proposition 4.3. We consider a slight variant of the construction from Theorem 2.2. Let n and H
be given, and let ∆∈(0, 1/3) be a parameter whose value will be chosen later. We first specify the dynamics
for M ⋆. Set X = {x, y, z} and A = {a, b, c}. The initial state distribution sets P0(x) = 1 −∆and P0(y) = ∆.
The transition dynamics are:
• Ph(x′ = · | x = x, a) = Ix · I{a ∈{a, b}} + Iz · I{a = c}.
• Ph(x′ | x, a) = I{x′ = x} for x ∈{y, z}.
In other words, y and z are terminal states. For state x, actions a and b are self-loops, but action c transitions
to z.
The expert policies are πa, which sets πa
h(x) = a for all h and x ∈X, and πb, which sets πb
h(x) = a and sets
πb
h(y) = πb
h(z) = b. We have Π = {πa, πb}.
We consider two problem instances for the lower bound, Ia = (M ⋆, πa, ra), and Ib = (M ⋆, πb, rb). For problem
instance Ia, the expert policy is πa. We set ra
h(x, ·) = ra
h(z, ·) = 0, ra
h(y, a) = I{a = a} for all h. On the other
hand, for problem instance Ib, the expert policy is πb. We set rb
h(x, ·) = rb
h(z, ·) = 0, rb
h(y, a) = I{a = b} for all
h. Note that both of these choices for the reward function satisfy µ = 1, and that πa and πb are optimal policies
for the respective instances. Let J a denote the expected reward function for instance a, and likewise for b.
Upper bound on online sample complexity.
We consider the following online algorithm. For episodes
t = 1, . . . ,:
• If x1 ̸= x, proceed to the next episode.
• If x1 = x, take action c, and observe a2 = π⋆(z). If a2 = a, return bπ = πa, and if a2 = b, return bπ = πb.
For any ∆≤e−1, this algorithm will terminate after log(1/δ) episodes with probability at least 1 −δ, and
whenever the algorithm terminates, it is clear that bπ = π⋆. In particular, this leads to zero regret for any
choice of reward function.
Lower bound on offline sample complexity.
By setting ∆∝1
n, an argument essentially identical to
the proof of Theorem 2.2 shows that any offline imitation learning algorithm must have
max{E
a[J
a(π
a) −J
a(bπ)], E
b[J
b(π
b) −J
b(bπ)]} ≳∆H ≳H
n .
For the sake of avoiding repetition, we omit the details. Finally, we observe that since neither policy in Π
takes the action c, Dagger—when equipped with any online learning algorithm that predicts from a mixture
of policies in Π, such as in Proposition C.2)—will never take the action c, and hence is subject to the H
n
lower bound from Theorem 2.2 as well.
57

Part II
Additional Results
G
Additional Lower Bounds
This section contains additional lower bounds that complement the results in Sections 2 and 3:
• Appendix G.1 shows that the conclusion of Theorem G.1 continues to hold even for online imitation
learning in an active sample complexity framework.
• Appendix G.2 presents an instance-dependent lower bound for stochastic experts, complementing the
minimax lower bound in Theorem 3.2.
• Appendix G.3 investigates the extent to which Theorems 2.1 and 3.1 are tight on a per-policy basis.
G.1
Lower Bounds for Online Imitation Learning in Active Interaction Model
For the online imitation learning setting introduced in Section 1.1, we measure sample complexity in terms of
the total number of episodes of online interaction, and expert feedback is available in every episode. In this
section, we consider a more permissive sample complexity framework inspired by active learning (Hanneke,
2014; Sekhari et al., 2024). Here, as in Section 1.1, the learner interacts with the underlying MDP M ⋆
through multiple episodes. At each episode i ∈[n] the learner executes a policy πi = {πi
h : X →∆(A)}H
h=1,
and at any step h in the episode, they can decide whether to query the expert for an action a⋆
h ∼π⋆
h(xh) at
the current state xh. We set M i = 1 if the learner queries the expert at any point during episode i and set
M i = 0 otherwise, and define the active sample complexity M := Pn
i=1 M i as the total number of queries.
It is clear that the active sample complexity satisfies m ≤n, and in some cases we might hope for it to be much
smaller than the total number of episodes, at least for a well-designed algorithm. While this can indeed be the
case for MDPs that satisfies (fairly strong) distributional assumptions (Sekhari et al., 2024), we will show that
the lower bound in Theorem 2.2 continues to hold in this framework (up to a logarithmic factor), meaning
that online interaction in the active sample complexity framework cannot improve over LogLossBC in general.
Theorem G.1 (Lower bound for deterministic experts in active sample complexity framework). For any
m ∈N and H ∈N, there exists a reward-free MDP M ⋆with |X| = |A| = m + 1, a class of reward functions R
with |R| = m+1, and a class of deterministic policies Π with log|Π| = log(m) with the following property. For
any online imitation learning algorithm in the active sample complexity framework that has sample complexity
E[M] ≤c·m for an absolute constant c > 0, there exists a deterministic reward function r = {rh}H
h=1 with rh ∈
[0, 1] and (optimal) expert policy π⋆∈Π with µ = 1 such that the expected suboptimality is lower bounded as
E[J(π⋆) −J(bπ)] ≥c · H
m
for an absolute constant c > 0. In addition, the dynamics, rewards, and expert policies are all stationary.
Since this example has log|Π| = log(M), it follows that the sample complexity bound for LogLossBC in
Theorem 2.1 (which uses M = n) can be improved by no more than a log(n) factor through online interaction
in the active framework.
Proof of Theorem G.1.
Let m ∈N and H ∈N be fixed. We first specify the dynamics for the reward-free
MDP M ⋆. Set X = {x1, . . . , xm} and A = {a, b}. The initial state distribution is P0 = unif(x1, . . . , xm). The
transition dynamics are Ph(x′ | x, a) = I{x′ = x} for all h; that is, x1, . . . , xm are all self-looping terminal
states.
Let a problem instance I = (M ⋆, r, π⋆) refer to a tuple consisting of the reward-free MDP M ⋆, a reward
function r = {rh}H
h=1, and an expert policy π⋆. We consider m+1 problem instances I0, . . . , Im parameterized
by a collection of policies Π = {π0, . . . , πm} and reward functions R = {r0, . . . , rm}.
58

• For problem instance I0 = (M ⋆, r0, π0), the expert policy is π0, which sets π0
h(x) = a for all x ∈X and
h ∈[H]. The reward function r0 sets rh(x, a) = I{a = a} for all x ∈X and h ∈[H].
• For each problem instance Ij = (M ⋆, rj, πj), the expert policy is πj, which for all h ∈[H] sets π
j
h(x) = a
for x ̸= xj and sets πh(xj) = b. The reward function rj sets rh(x, a) = I{a = a, x ̸= xj}+I{a = b, x = xj}
for all h ∈[H].
Let J j denote the expected reward under instance j. Note that all instances satisfy µ = 1, and that πj is an
optimal policy for each instance j.
Going forward, we fix the online imitation learning algorithm under consideration and let Pj denote the
law of o1, . . . , on when a when we execute the algorithm on instance Ij; let E
j[·] denote the corresponding
expectation. In addition, for any policy π, let Pπj|π denote the law of o = (x1, a1, a⋆
1), . . . , (xH, aH, a⋆
H)
when we execute π in the online imitation learning framework when the underlying instance is Ij, with the
convention that a⋆
h =⊥if the learner does not query the expert in episode j.
Our aim is to lower bound
max
j∈{0,...,m} E
j[J
j(π
j) −J
j(bπ)]
To this end, define ρj(π, π′) = PH
h=1 Eah∼πh(xj),a′
h∼π′
h(xj) I{ah ̸= a′
h} and ρ(π, π′) = 1
mρj(π, π′), and observe
that
E
0[J
0(π
0) −J
0(bπ)] = E
0

1
m
m
X
j=1
H
X
h=1
Eah∼bπh(xj)[I{ah ̸= π
0
h(xj)}]


= E
0[ρ(bπ, π
0)] ≥H
2m · P
0

ρ(bπ, π
0) ≥H
2m

.
Next, note that for any i ∈[m], if ρ(bπ, π0) <
H
2m, then ρj(bπ, π0) < H
2 , which means that ρj(bπ, πj) ≥H
2 . It
follows that
E
j[J
j(π
j) −J
j(bπ)] = E
j
 1
mρj(bπ, π
j)

≥H
2mP
j

ρ(bπ, π
0) < H
2m

,
and if we define P = Ej∼unif([m]) Pj, then
Ej∼unif([m]) E
j[J
j(π
j) −J
j(bπ)] ≥H
2mP

ρ(bπ, π
0) < H
2m

.
Combining these observations, we find that
max
i∈{0,...,m} E
j[J
j(π
j) −J
j(bπ)] ≥H
4m

P
0

ρ(bπ, π
0) ≥H
2m

+ P

ρ(bπ, π
0) < H
2m

≥H
4m(1 −DTV
 P
0, P

).
It remains to bound the total variation distance. Next, using Lemma D.2 of Foster et al. (2024), we can
bound
D2
TV
 P
0, P

≤D2
H
 P
0, P

≤Ej∼unif[m]

D2
H(P
0, P
j)

≤7 Ej∼unif[m] E
0
" n
X
t=1
D2
H

Pπ0|πt, Pπj|πt#
.
Since the feedback the learner receives for a given episode t is identical under instances I0 and Ij is identical
unless i) x1 = xj, and ii) the learner decides to query the expert for feedback (i.e., M t = 1), we can bound
D2
H

Pπ0|πt, Pπj|π0
≤2Pπ0|πt[x
t
1 = xj, M
t = 1]
59

and hence
Ej∼unif[m] E
0
" n
X
t=1
D2
H

Pπ0|πt, Pπj|πt#
≤2 Ej∼unif[m] E
0
" n
X
t=1
Pπ0|πt[x
t
1 = xj, M
t = 1]
#
= 2
m E
0


n
X
t=1
m
X
j=1
Pπ0|πt[x
t
1 = xj, M
t = 1]


= 2
m E
0
" n
X
t=1
Pπ0|πt[M
t = 1]
#
= 2
m E
0[M].
It follows that if E
0[M] ≤m/56, then DTV
 P0, P

≤1/2, so that the algorithm must have
max
i∈{0,...,m} E
j[J
j(π
j) −J
j(bπ)] ≥H
8m.
G.2
An Instance-Dependent Lower Bound for Stochastic Experts
In this section, we further investigate the optimality of LogLossBC for stochastic experts (Theorem 3.1).
Recall that when log|Π| = O(1) the leading-order term in Theorem 3.1 scales as roughly
p
σ2
π⋆/n, where the
salient quantity is the variance
σ2
π⋆:=
H
X
h=1
Eπ⋆h
(Qπ⋆
h (xh, π⋆(xh)) −Qπ⋆
h (xh, ah))2i
for the expert policy π⋆. Theorem 3.2 shows that this is optimal qualitatively, in the sense that for any value
σ2, there exists a class of MDPs where the σ2
π⋆≤σ2, and where the minimax rate is at least
p
σ2/n.
In what follows, we will prove that for the special case of autoregressive MDPs (that is, the special case of the
imitation learning problem in which the state takes the form xh = a1:h−1; cf. Appendix A.3), Theorem 3.2 is
optimal on a per-policy basis. Concretely, we prove a local minimax lower bound (Donoho and Liu, 1991)
which states that for any policy π⋆and any reward function r⋆, there exists a difficult “alternative” policy eπ,
such that in worst case over rewards r ∈{−r⋆, +r⋆} and expert policies π ∈{π⋆, eπ}, any algorithm must
have regret at least
p
σ2/n.
Theorem G.2. Consider the offline imitation learning setting, and let M ⋆be an autoregressive MDP. Let a
reward function r⋆with PH
h=1 r⋆
h ∈[0, R] almost surely be fixed, and let an expert policy π⋆be given. For any
n ∈N, there exists an alternative policy eπ such that
min
Alg
max
π∈{π⋆,eπ}
max
r∈{r⋆,−r⋆} P
"
J(π) −J(bπ) ≥c ·
r
σ2
π⋆
n
#
≥1
4
for all n ≥c′ · R2
σ2
π⋆, where c, c′ > 0 are absolute constants.
Theorem G.2 suggests that the leading term in Theorem 3.1 cannot be improved substantially without
additional assumptions, on a (nearly) per-instance basis. The restriction to n ≥c′ · R2
σ2
π⋆in Theorem G.2 is
somewhat natural, as this corresponds to the regime in which the
p
σ2
π⋆/n term in Theorem 3.1 dominates
the lower-order term.
60

Proof of Theorem G.2.
We begin by observing that for any ∆> 0,
min
Alg
max
π∈{π⋆,eπ}
max
r∈{r⋆,−r⋆} P[Jr(π) −Jr(bπ) ≥∆] ≥min
Alg
max
π∈{π⋆,eπ} P[|Jr⋆(π) −Jr⋆(bπ)| ≥∆].
with the convention that Jr(π) denotes the expected reward under r; we abbreviate J(π) ≡Jr⋆(π) going
forward. Let Pπ
n denote the law of the offline imitation learning dataset under π. If we set ∆= |J(π⋆)−J(eπ)|/2,
then by the standard Le Cam two-point argument, we have that
max
n
Pπ⋆
n [|J(π⋆) −J(bπ)| ≥∆], Peπ
n[|J(eπ) −J(bπ)| ≥∆]
o
≥1
2

1 −Pπ⋆
n [|J(π⋆) −J(bπ)| < ∆] + Peπ
n[|J(eπ) −J(bπ)| ≥∆]

≥1
2

1 −Pπ⋆
n [|J(eπ) −J(bπ)| ≥∆] + Peπ
n[|J(eπ) −J(bπ)| ≥∆]

≥1
2

1 −DTV

Pπ⋆
n , Peπ
n

≥1
2

1 −
q
n · D2
H(Pπ⋆, Peπ)

,
where the final inequality uses the standard tensorization property for Hellinger distance (e.g., Wainwright
(2019)).
We will proceed by showing that
ωπ⋆(ε) := sup
π
n
|J(π) −J(π⋆)| | D2
H

Pπ⋆, Pπ
≤ε2o
≥Ω(1) ·
q
σ2
π⋆· ε2,
(28)
for any ε > 0 sufficiently small, from which the result will follow by setting ε2 ∝1/n and
eπ = arg max
π
n
|J(π) −J(π⋆)| | D2
H

Pπ⋆, Pπ
≤ε2o
≥Ω(1) ·
q
σ2
π⋆· ε2.
To prove this, we will appeal to the following technical lemma.
Lemma G.1. For any distribution Q and function h with |h| ≤R almost surely, it holds that for all
0 ≤ε2 ≤VarQ[h]
4R2 , there exists a distribution P such that
1. EP[h] −EQ[h] ≥2−3p
VarQ[h] · ε2
2. DKL(Q ∥P) ≤ε2.
Since stochastic policies π in the autoregressive MDP M ⋆are equivalent to arbitrary joint laws over
the sequence a1:H (via Bayes’ rule) and J(π) = EπhPH
h=1 r⋆
h
i
, Lemma G.1 implies that for any ε2 ≤
Varπ⋆hPH
h=1 r⋆
h
i
/4R2, there exists a policy eπ such that (i) D2
H
 Pπ⋆, Peπ
≤DKL
 Pπ⋆∥Peπ
≤ε2, and (ii)
J(eπ) −J(π⋆) ≥2−3
v
u
u
tVarπ⋆
" H
X
h=1
r⋆
h
#
· ε2.
This establishes Eq. (28). The result now follows by setting ε2 =
c
n for an absolute constant c > 0 so
that
p
n · D2
H(Pπ⋆, Peπ) ≤1/2, which is admissible whenever n ≥c′ · R2
σ2
π⋆. Finally, we observe that for any
deterministic MDP, by Lemma B.5,
Varπ⋆
" H
X
h=1
rh
#
= Eπ⋆
" H
X
h=1
Varπ⋆h
rh + V π⋆
h+1(xh+1) | xh
i#
= Eπ⋆
" H
X
h=1
(Qπ⋆
h (xh, ah) −V π⋆
h (xh))2
#
= σ2
π⋆,
since deterministic MDPs satisfy
Qπ⋆
h (xh, ah) = rh(xh, ah) + V π⋆
h+1(xh+1)
61

almost surely, and since Eπ⋆
Qπ⋆
h (xh, ah) | xh

= V π⋆
h (xh).
Proof of Lemma G.1.
Recall that we assume the domain is countable, so that Q admits a probability
mass function q. We will define P via the probability mass function
p(x) =
q(x)eηh(x)
P
x′ q(x′)eηh(x′)
for a parameter η > 0. We begin by observing that
DKL(Q ∥P) = log
 EQ

eηh
−η EQ[h] = log

EQ
h
eη(h−EQ[h])i
.
We now use the following lemma.
Lemma G.2. For any random variable X with |X| ≤R almost surely and any η ∈(0, (2R)−1),
η2
8 Var[X] ≤log

E
h
eη(X−E[X])i
≤η2Var[X].
Hence, as long as η ≤(2R)−1,
DKL(Q ∥P) ≤η2VarQ[h].
We set η = min
nq
ε2
VarQ[h],
1
2R
o
so that DKL(Q ∥P) ≤ε2.
Next, we compute that
0 ≤DKL(P ∥Q) = η EP[h] −log
 EQ

eηh
,
so that
EP[h] −EQ[h] ≥η−1 log
 EQ

eηh
−EQ[h] = η−1 log

EQ
h
eη(h−EQ[h])i
.
Since η ≤(2R)−1, Lemma G.2 yields
EP[h] −EQ[h] ≥η
8VarQ[h] = 1
8
q
VarQ[h] · ε2
as long as ε2 ≤VarQ[h]
4R2 .
Proof of Lemma G.2.
Note that ex ≤1 + x + (e −2)x2 ≤1 + x + x2 whenever |x| ≤1, and similarly
ex ≥1 + x + x2
4 for |x| ≤1. It follows that if η ≤(2R)−1,
1 + η2
4 Var(X) ≤E
h
eη(X−E[X])i
≤1 + η2Var(X).
We conclude by using that x
2 ≤log(1 + x) ≤x for x ∈[0, 1].
62

G.3
Tightness of the Hellinger Distance Reduction
Theorem 2.1 and Theorem 3.1 are supervised learning reductions that bound the regret of any policy bπ in
terms of its Hellinger distance D2
H
 Pbπ, Pπ⋆
to the expert policy π⋆. The following result shows that these
reductions are tight in a fairly strong instance-dependent sense: Namely, for any pair of policies bπ and π⋆,
and for any reward-free MDP M ⋆, it is possible to design a reward function r = {rh}H
h=1 for which each term
in Eq. (18) of Theorem 3.1 is tight, and such that Theorem 2.1 is tight; the only caveat is that we require the
reward function to be non-Markovian, in the sense that rh depends on the full history x1:h and a1:h.
Theorem G.3 (Converse to Theorems 2.1 and 3.1). Let a reward-free MDP M ⋆and a pair of (potentially
stochastic) policies bπ and π⋆be given.
1. For any R > 0, there exists a non-Markovian reward function r = {rh}H
h=1 with PH
h=1 rh ∈[0, R] such
that
J(π⋆) −J(bπ) ≥R
6 · D2
H
 Pbπ, Pπ⋆
.
(29)
2. For any σ2 > 0, there exists a non-Markovian reward function r = {rh}H
h=1 for which σ2
π⋆:=
PH
h=1 Eπ⋆
(Qπ⋆
h (x1:h, a1:h) −V π⋆
h (x1:h, a1:h−1))2
≤σ2, and such that30
J(π⋆) −J(bπ) ≥1
9
q
σ2 · D2
H(Pbπ, Pπ⋆).
(30)
3. For any R > 0 and σ2 > 0, there exists a non-Markovian reward function r = {rh}H
h=1 with PH
h=1 rh ∈
[0, R] and σ2
π⋆≤σ2 simultaneously such that
J(π⋆) −J(bπ) ≥1
9 min
q
σ2 · D2
H(Pbπ, Pπ⋆), R · D2
H
 Pbπ, Pπ⋆
.
Eq. (29) shows that there exist reward functions with bounded range for which Theorem 2.1 and the lower-order
term in Eq. (18) of Theorem 3.1 are tight, while Eq. (30) shows that there exist reward functions with bounded
variance (but not necessarily bounded range) for which the leading term in Eq. (18) or Theorem 3.1 is tight.
Note that for some MDPs, the state xh already contains the full history x1:h−1, a1:h−1, so the assumption of
non-Markovian rewards is without loss of generality. For MDPs that do not have this property, Theorem G.3
leaves open the possibility that Theorems 2.1 and 3.1 can be improved on a per-MDP basis.
Proof of Theorem G.3.
Consider a pair of measures P and Q, and set P := 1
2(P + Q). Consider the
function
h = 1 −1
2
Q
P ∈[0, 1].
Using Lemma B.4, we observe that
EP[h] −EQ[h] = 2
 EP[h] −EQ[h]

= EQ
Q
P

−EP
Q
P

= Dχ2 Q ∥P

≥1
6D2
H(Q, P).
(31)
We also observe that by concavity of variance,
1
2(VarP[h] + VarQ[h]) ≤VarP[h] = 1
4 EP
Q
P −EP
Q
P
2
= Dχ2 Q ∥P

≤D2
H(Q, P).
(32)
To apply this observation to the theorem at hand, let a parameter B > 0 be given, let P := 1
2(Pπ⋆+ Pbπ), and
consider the non-Markov reward function r that sets r1, . . . , rh−1 = 0 and
rH(τ) = B ·

1 −1
2
Pbπ
P

∈[0, B].
30Note that since the reward function under consideration is non-Markovian, the value functions Qπ⋆
h
and V π⋆
h
depend on the
full history x1:h, a1:h−1.
63

Then by Eq. (31), we have that
J(π⋆) −J(bπ) ≥B
6 · D2
H

Pbπ, Pπ⋆
.
At the same time, by Eq. (32), we have that
Varπ⋆
" H
X
h=1
rh
#
= Varπ⋆[rH] ≤2B2 · D2
H

Pbπ, Pπ⋆
,
and by Proposition 3.1,
σ2
π⋆≤Varπ⋆
" H
X
h=1
rh
#
.
To conclude, note that if we set B2 = R2, then PH
h=1 rh ∈[0, R] and
J(π⋆) −J(bπ) ≥R
6 · D2
H

Pbπ, Pπ⋆
.
Meanwhile, if we set
B2 =
σ2
2D2
H(Pbπ, Pπ⋆),
then σ2
π⋆≤σ2 and
J(π⋆) −J(bπ) ≥1
9
q
σ2 · D2
H(Pbπ, Pπ⋆).
Finally, if we set
B2 =
σ2
2D2
H(Pbπ, Pπ⋆) ∧R2.
Then PH
h=1 rh ∈[0, R], σ2
π⋆≤σ2, and
J(π⋆) −J(bπ) ≥B
6 · D2
H

Pbπ, Pπ⋆
≥min
1
9
q
σ2 · D2
H(Pbπ, Pπ⋆), R
6 · D2
H

Pbπ, Pπ⋆
.
64

