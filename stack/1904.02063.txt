arXiv Preprint, December 13, 2019.
Generalized Variational Inference:
Three arguments for deriving new Posteriors
Jeremias Knoblauch
j.knoblauch@warwick.ac.uk
The Alan Turing Institute
Dept. of Statistics
University of Warwick
Coventry, CV4 7AL, UK
Jack Jewson
j.e.jewson@warwick.ac.uk
The Alan Turing Institute
Dept. of Statistics
University of Warwick
Coventry, CV4 7AL, UK
Theodoros Damoulas
t.damoulas@warwick.ac.uk
The Alan Turing Institute
Depts. of Computer Science & Statistics
University of Warwick
Coventry, CV4 7AL, UK
Editor: Leslie Pack Kaelbling
Abstract
In this paper we advocate an optimization-centric view on Bayesian statistics and
introduce a novel generalization of Bayesian inference. On both counts, our inspiration is
the representation of Bayes’ rule as an inﬁnite-dimensional optimization problem as shown
independently by Csiszár (1975); Donsker and Varadhan (1975); Zellner (1988). First, we use
this representation to prove a surprising optimality result of standard Variational Inference
(VI) methods: Under the proposed view, the standard Evidence Lower Bound (ELBO)
maximizing VI posterior is always preferable to alternative approximations of the Bayesian
posterior. Next, we argue for an optimization-centric generalization of standard Bayesian
inference. The need for this generalization arises in situations of severe misalignment between
reality and three assumptions underlying the standard Bayesian posterior: (1) Well-speciﬁed
priors, (2) well-speciﬁed likelihood models and (3) the availability of inﬁnite computing
power. In response to this observation, our generalization is deﬁned by three arguments and
named the Rule of Three (RoT). Each of its three arguments relaxes one of the assumptions
underlying standard Bayesian inference. We axiomatically derive the RoT and recover
existing methods as special cases, including the Bayesian posterior and its approximation
by standard Variational Inference (VI). In contrast, alternative approximations to the
Bayesian posterior maximizing other ELBO-like objectives violate these axioms. Finally, we
introduce a special case of the RoT that we call Generalized Variational Inference (GVI).
GVI posteriors are a large and tractable family of belief distributions speciﬁed by three
arguments: A loss, a divergence and a variational family. GVI posteriors possess appealing
theoretical properties, including consistency and an interpretation as an approximate ELBO.
c⃝2019 Jeremias Knoblauch, Jack Jewson and Theo Damoulas.
arXiv:1904.02063v4  [stat.ML]  12 Dec 2019

Knoblauch, Jewson and Damoulas
The last part of the paper explores some attractive applications of GVI in popular machine
learning models, including robustness and more appropriate marginals. After deriving black
box inference schemes for GVI posteriors, their predictive performance is investigated on
Bayesian Neural Networks and Deep Gaussian Processes, where GVI can comprehensively
improve upon existing methods.
Keywords:
Bayesian Inference, Generalized Bayesian Inference, Variational Inference,
Bayesian Neural Networks, Deep Gaussian Proceses
1. Introduction
Though famously ﬁrst discovered in Bayes (1763), the version of Bayes’ Theorem that a
modern audience would be familiar with is much closer to the one in De Laplace (1774).
Bayes’ Theorem (or Bayes’ rule) is one of the most fundamental results in probability theory
and states that for a probability measure P and two events A, B, it holds that
P (A|B) = P (B|A) P (A)
P (B)
.
As usual, P (A|B) denotes the conditional probability of event A given that event B occured.
It would take nearly two more centuries for this mathematical result to be used as the basis
for an entire school of statistical inference (Fienberg, 2006). More precisely, Fisher (1950)
makes the ﬁrst mention of the term Bayesian in our modern understanding (David, 1998).
Bayesian statistics uses Bayes’ Theorem to conduct inference on an unknown and unob-
servable event A. Speciﬁcally, suppose that one can compute for an observable event B the
probability P(B|A) and has a prior belief P(A) about the event A before observing B. In this
situation, Bayes’ rule tells us that we should be able to draw probabilistic inferences on A|B
by computing the probability P(A|B). In practice, the events A quantify the uncertainty
about a parameter of interest θ ∈Θ and so are of the form A ⊂Θ. The prior beliefs
about events A are usually speciﬁed by some probability density π : Θ →R+ inducing the
probability measure P(A) =
R
A dπ(θ). This leaves us with the need to specify a probability
distribution P (B|A) that relates the (unobserved) parameter θ to the (observable) event B.
In practice, one typically sets B = x1:n to correspond to n observations x1:n. The next step
is to deﬁne a distribution of B|A. This amounts to positing a likelihood function pn(x1:n|θ)
and setting P(B|A) = pn(x1:n|θ). Put together, this yields the standard Bayesian posterior
that we denote as q∗
B(θ) throughout the paper and which is given by
q∗
B(θ) = pn(x1:n|θ)π(θ)
Z
.
Here, Z =
R
Θ pn(x1:n|θ)dπ(θ) is the normalizing constant—also known as partition function—
whose computation generally makes the Bayesian posterior intractable.
Bayesian inference is appealing both conceptually and practically: Unlike frequentist
inference, Bayesian methods allow inferences to be informed by domain expertise in form of a
carefully speciﬁed prior belief π(θ). Further, Bayesian inference produces belief distributions
(rather than point estimates) over the parameter of interest θ ∈Θ that best ﬁts the observed
data x1:n while taking into account a prior belief π(θ) about appropriate values of θ. As
a consequence, Bayesian inferences automatically quantify uncertainty about θ. This is
practically useful in many situations, but especially if one uses θ predictively: Integrating
2

Generalized Variational Inference
over q∗
B(θ) avoids being over-conﬁdent about the best value of θ, substantially improving
predictive performance (see e.g. Aitchison, 1975). Amongst other beneﬁts, it is this enhanced
predictive performance that has cast Bayesian inference as one of the predominant paradigms
in contemporary large-scale statistical inference and machine learning.
While Bayesian methods automatically quantify the uncertainty about their inferences,
this comes at a cost: In the translation of Bayes’ rule into the Bayesian posterior q∗
B(θ),
we have made three implicit but crucial assumptions. Firstly, we have assumed that the
modeller has a prior belief π(θ) which is worth being taken into account and which the
modeller is capable of writing out mathematically. Secondly, we speciﬁed the likelihood
function pn(x1:n|θ) as a conditional probability. In other words, we have assumed that the
model is correctly speciﬁed, which is to say that pn(x1:n|θ∗) = dP(x1:n) for some unknown
value of θ∗∈Θ. Thirdly, we have assumed the availability of enough computational power
to compute and perform exact inference based upon the generally intractable posterior q∗
B(θ).
In many situations, these three assumptions built into q∗
B(θ) are harmless. For modern
large-scale statistical machine learning tasks however, they are frequently violated.
To address this, the current paper takes a step back from Bayes’ Theorem and the
standard Bayesian posterior q∗
B(θ) to deﬁne a generalized class of posterior belief distributions.
Throughout, we motivate this with the tension between the three main assumptions underlying
standard Bayesian inference on the one hand and the requirement of many contemporary
statistical applications on the other hand. To resolve this tension, we deﬁne a generalization
of Bayesian inference that we call the Rule of Three (RoT). The RoT is speciﬁed by an
optimization problem over the space of probability measures P(Θ) on Θ with three arguments.
These arguments are a loss function ℓ, a divergence D measuring the deviation of the posterior
from the prior and a space Π ⊆P(Θ) of feasible solutions. Together, these three ingredients
deﬁne posterior beliefs of the form
q∗(θ) = arg min
q∈Π
(
Eq(θ)
" n
X
i=1
ℓ(θ, xi)
#
+ D(q∥π)
)
def
= P(ℓ, D, Π).
(1)
This recovers previous generalizations of Bayesian inference, including those inspired by Gibbs
posteriors (e.g. Ghosh and Basu, 2016; Bissiri et al., 2016; Jewson et al., 2018; Nakagawa and
Figure 1: A taxonomy of some important belief distributions as special cases of the RoT.
3

Knoblauch, Jewson and Damoulas
Hashimoto, 2019; Chérief-Abdellatif and Alquier, 2019), tempered posteriors (e.g. Grünwald,
2011, 2012; Holmes and Walker, 2017; Grünwald and Van Ommen, 2017; Miller and Dunson,
2019), as well as PAC-Bayesian approaches (for a recent overview, see Guedj, 2019). we
illustrate this taxonomy in Figure 1. Unlike any of these previous generalizations however,
posteriors taking the form P(ℓ, D, Π) may be non-multiplicative. One of the most important
implications of this is that in contrast to previous generalizations, the RoT can recover
standard Variational Inference (VI) posteriors based on minimizing the Kullback-Leibler
Divergence (KLD) to q∗
B(θ). Notably, this is true even though standard VI is derived as
an approximation to the Bayesian posterior q∗
B(θ).
Even more remarkably, variational
approximations to the Bayesian posterior that are constructed by minimizing divergences
other than the KLD are not recovered by the RoT. This inspires us to deﬁne and investigate
Generalized Variational Inference (GVI), the tractable special case for the RoT in which
Π = Q = {q(θ|κ) : κ ∈K} ⊂P(Θ) is chosen to be a variational family. Various theoretical
and empirical ﬁndings lead us to conclude that GVI posteriors are well-suited to real world
inference problems and are an exciting ﬁrst step on the way to derive generalized and tractable
posterior belief distributions. The paper draws these conclusions in ﬁve steps.
Section 2: We recapitulate the standard approach to Bayesian inference and various
variational approximation schemes for q∗
B(θ). Unconventionally, we do so through the
lens of inﬁnite-dimensional optimization. This view provides a number of interesting
insights: For example, it enables a natural breakdown of variational approximation
methods. Further, it reveals that relative to the inﬁnite-dimensional optimization
problem whose solution is the Bayesian posterior, the standard Variational Inference
(VI) posterior is the optimal solution in its ﬁnite-dimensional variational family. Per-
haps surprisingly, this also implies that for any ﬁxed variational family, alternative
approximations in the same variational family are sub-optimal.
Section 3: We explain why a generalized view on Bayesian inference is necessary. To
this end, we ﬁrst give a brief overview over the three assumptions that justify Bayesian
inference: The availability of both an appropriately speciﬁed prior belief and likelihood
as well as suﬃcient computational power to address the intractability of q∗
B(θ). We then
proceed to contrast these three assumptions with the realities of modern day large-scale
statistical inference and use three examples to explain the real world problems arising
from this misalignment between assumptions and reality.
Section 4: We derive a generalized representation of Bayesian inference that we
call the Rule of Three (RoT) based on three simple axioms. The RoT is inspired
by our optimality ﬁnding regarding standard VI in Section 2. Thus, unlike previous
generalizations it deﬁnes an optimization-centric outlook on Bayesian inference. We
discuss the RoT and explain how it can address the adverse eﬀects of violating the
assumptions underlying standard Bayesian inference. Further, we connect the RoT
to existing Bayesian methods, the information bottleneck method and PAC-Bayesian
approaches.
Section 5: Translating the conceptual contribution of the RoT into a methodological
one, we introduce Generalized Variational Inference (GVI). We explain how to use
GVI for robust inference and more appropriate marginal variances. We also point to
4

Generalized Variational Inference
some theoretical ﬁndings, including frequentist consistency and an interpretation of
GVI as approximate evidence lower bound. Lastly, we discuss computation of GVI
posteriors. While special cases permit closed form objectives, one generally needs to
rely on stochastic Black Box GVI (BBGVI).
Section 6: We reinforce the conceptual and methodological appeal of GVI with two
large-scale inference applications: Bayesian Neural Networks (BNNs) and Deep Gaussian
Processes (DGPs). In diﬀerent ways, both model classes are representative for the
diﬀerent ways in which contemporary large-scale inference is often misaligned with the
assumptions underlying the standard Bayesian posterior. We show that appropriately
addressing this misalignment dramatically improves performance.
Throughout, we radically simplify the presentation for improved readability: For example,
we do not incorporate latent variables into our notation in spite of demonstrating GVI on
a Deep Gaussian Process (DGP) latent variable model in Section 6.2. Further, we assume
that losses are additive, homogeneous and such that the i-th loss term ℓ(θ, xi) only depends
on xi. None of these assumptions are necessary, and one could replace ℓ(θ, xi) by ℓi(θ, x1:i)
or ℓi(θ, xnbh(i)) for some neighbourhood nbh(i) ⊂{1, 2, . . . n} throughout the paper without
violating the principles of the RoT or GVI1.
2. An optimization-centric view on Bayesian inference
Before presenting our main ﬁndings, we set the stage by introducing an optimization-centric
view on (generalized) Bayesian inference. Speciﬁcally, we draw attention to an isomorphism
between the Bayesian posterior and an inﬁnite-dimensional optimization problem and discuss
three implications of this relationship.
Section 2.2: Committing to any exact Bayesian posterior is equivalent to committing
to a particular optimization problem over the space of probability measures
Section 2.3: Taking an optimization-centric view of Bayesian inference and holding
the variational family ﬁxed, standard Variational Inference (VI) produces optimal
approximations of the exact Bayesian posterior.
Section 2.4: non-standard VI methods based on alternative divergences are suboptimal
approximations of the exact Bayesian posterior.
2.1 Preliminaries
Given a prior belief π(θ) about the parameter and observations x1:n linked to θ via a
likelihood function p(xi|θ), the standard Bayesian posterior belief q∗
B(θ) is computed
through a multiplicative updating rule with ℓ(θ, xi) = −log p(xi|θ) as
q∗
B(θ) ∝π(θ)
n
Y
i=1
exp{−ℓ(θ, xi)}.
(2)
1.
For the interested reader, we note that the appropriate notational extension of GVI to latent variables
and non-homogeneous losses is formalized in Knoblauch (2019a), see e.g. Assumption 1 and Remarks 1,2
and 3 therein.
5

Knoblauch, Jewson and Damoulas
While this way of writing Bayes rule might seem cumbersome, it reveals that the multiplicative
structure is in principle applicable to any loss function. This leads to the development of
a generalized Bayesian posterior by replacing the negative log likelihood with any loss
ℓ: Θ × X →R. If the normalizer of eq. (2) exists, such treatment provides a coherent and
principled way to update beliefs about an arbitrary parameter θ (Bissiri et al., 2016).
To make this generalization tangible, imagine that θ denotes the median for the data
generating mechanism that produced x1:n and that one wishes to update beliefs about it
in a Bayesian manner. A loss-based Bayesian treatment of this problem would combine
a prior belief π about the median with the loss ℓ(θ, xi) = |θ −xi|1. Together, these two
ingredients yield a generalized Bayesian posterior belief about the median as given above.
For some applications with ℓ(θ, xi) ̸= −log p(xi|θ), see Ghosh and Basu (2016); Alquier et al.
(2016); Grünwald and Van Ommen (2017); Jewson et al. (2018); Knoblauch et al. (2018);
Chérief-Abdellatif and Alquier (2019) or Nakagawa and Hashimoto (2019).
Throughout this paper, we do not notationally distinguish standard and generalized
Bayesian posteriors. Unless we make the distinction explicit, we subsume both types of belief
distributions under the name Bayesian posterior and denote any posterior belief computed
as in eq. (2) by q∗
B(θ). The asterisk superscript in q∗
B(θ) emphasizes an observation we
make next and that will be a recurrent theme throughout the paper: Any posterior belief
distribution is the result of an appropriately speciﬁed optimization problem.
2.2 Bayesian inference as inﬁnite-dimensional optimization
The traditional perspective on Bayesian posteriors derives from the basic laws of probability
and in particular Bayes’ Theorem: If p(xi|θ) denotes the conditional density of xi given θ
and π(θ) denotes the density of θ, the conditional probability of θ given x1:n is given by
q∗
B(θ) in eq. (2) with ℓ(θ, xi) = −log p(xi|θ). This multiplicative update rule is motivated
slightly diﬀerently for generalized Bayesian posteriors, but the inherent logic largely remains
the same: By imposing coherence, one forces the priors and losses into an exponentially
additive relationship (see also Section 4.4.2). One interpretation of this is that one treats
the loss terms exp{−ℓ(θ, xi} as quasi-likelihoods, rendering the resulting posteriors at very
strongly inspired by conditionalization and the fundamental rules of probability that underlie
Bayes’ rule2.
While Bayes’ rule and eq. (2) are well-known, there is a conceptually rather diﬀerent
path for arriving at q∗
B(θ): Dating back at least to Csiszár (1975) and Donsker and Varadhan
(1975), it was shown that Bayesian inference can be recast as the solution to an inﬁnite-
dimensional optimization problem. This result was rediscovered in statistics by Zellner (1988)
and states that for P(Θ) denoting the space of all probability measures on Θ, the Bayesian
posterior is given by
q∗
B(θ) = arg min
q∈P(Θ)
(
Eq(θ)
"
−
n
X
i=1
log(p(xi|θ))
#
+ KLD (q||π)
)
,
2. Note that the literature on PAC-Bayesian procedures can provide diﬀerent justiﬁcations for multiplicative
update rules (see e.g. Germain et al., 2016; Guedj, 2019).
6

Generalized Variational Inference
where KLD is the Kullback-Leibler divergence (Kullback and Leibler, 1951) given by
KLD(q||π) = Eq(θ)

log
 q(θ)
π(θ)

= Eq(θ) [log q(θ)] −Eq(θ) [log π(θ)] .
Similarly, the generalized Bayesian posteriors of Bissiri et al. (2016) solve the optimization
problem given by
q∗
B(θ) = arg min
q∈P(Θ)
(
Eq(θ)
" n
X
i=1
ℓ(θ, xi)
#
+ KLD (q||π)
)
.
(3)
This objective allows a re-interpretation of Bayesian inference as regularized optimization:
As in maximum likelihood inference or other empirical risk minimization tasks, one wishes
to minimize some loss function over the data. Unlike with frequentist methods however, one
wishes to quantify uncertainty and obtain a belief distribution rather than a point estimate.
Consequently, one adds the KLD regularization term. Note that if this KLD term were absent
from eq. (3), the solution of the optimization problem would simply be a Dirac mass δbθn(θ)
at the empirical risk minimizer bθn, since δbθn(θ) ∈P(Θ).
For completeness’ sake, we now provide a self-contained proof of eq. (3) which is based on
the one in the supplementary material of Bissiri et al. (2016). This encompasses the original
result of (Csiszár, 1975) and (Donsker and Varadhan, 1975) for ℓ(θ, xi) = −log p(xi|θ).
Theorem 1 If Z =
R
Θ exp {−Pn
i=1 ℓ(θ, xi)} π(θ)dθ < ∞, then the solution of eq. (3) exists
and is equivalent to the generalized Bayesian posterior q∗
B(θ) as given in eq. (2).
Proof One may rewrite the objective in eq. (3) as
q∗(θ) = arg min
q∈P(Θ)
(Z
Θ
"
log
 
exp
( n
X
i=1
ℓ(θ, xi)
)!
+ log
 q(θ)
π(θ)
#
q(θ)dθ
)
= arg min
q∈P(Θ)
Z
Θ
log

q(θ)
π(θ) exp {−Pn
i=1 ℓ(θ, xi)}

q(θ)dθ

.
As one only cares about the minimizer q∗(θ) (and not the objective value), it also holds that
for any constant Z > 0, the above is equal to
q∗(θ) = arg min
q∈P(Θ)
Z
Θ
log

q(θ)
π(θ) exp {−Pn
i=1 ℓ(θ, xi)} Z−1

q(θ)dθ −log Z

= arg min
q∈P(Θ)
(
KLD
 
q(θ)
π(θ) exp
(
−
n
X
i=1
ℓ(θ, xi)
)
Z−1
!)
.
Lastly, one sets Z =
R
θ exp {−Pn
i=1 ℓ(θ, xi)} π(θ)dθ and notes that as the KLD is a statistical
divergence, it is minimized uniquely if its two arguments are the same, so q∗(θ) = q∗
B(θ).
The result in Theorem 1 implies an important isomorphism that drives much of the current
paper’s development: Any commitment to a (standard or generalized) Bayesian posterior is
always a commitment to an optimization objective. In other words, the Bayesian posterior
for a given inference problem is adequate if and only if the objective in eq. (3) is.
7

Knoblauch, Jewson and Damoulas
Observation 1 (Isomorphism) Suppose an agent A wishes to conduct inference based
upon the Bayesian posterior q∗
B(θ) in eq. (2). Then agent A conducts inference by solving an
optimization problem, namely the one in eq. (3).
The above observation implies that computing the Bayesian posterior q∗
B(θ) is equivalent to
assuming that the objective of eq. (3) is appropriate for producing belief distributions. In
Section 3, we will explain why the usefulness of the standard Bayesian posterior—and thus
of the objective in eq. (3)—is at least doubtful for many contemporary statistical machine
learning problems.
Remark 2 The sceptical reader may notice that given the Bayesian posterior, eq. (3) is not
the unique solution-inducing problem. Speciﬁcally, suppose that D is a statistical divergence.
Then D(q∥p) ≥0 and D(q∥p) = 0 if and only if q(θ) = p(θ) almost everywhere. Hence, one
could object that in fact any optimization problem of the form
q∗(θ) = arg min
q∈P(Θ)
D(q∥q∗
B)
(4)
will be solved setting q∗(θ) = q∗
B(θ). While true, this is tautological: In particular, such
optimization problems shed no light on how q∗
B(θ) was arrived at. By their very construction,
problems of this form pre-suppose that q∗
B(θ) is the posterior belief one wishes to obtain.
Thus, while there exist inﬁnitely many optimization problems whose solution is q∗
B(θ),
some are more meaningful than others. Speciﬁcally, whenever one seeks to solve an objective
of the form given in eq. (4), the Bayesian posterior appears deus ex machina. This does not
allow us any interpretation about what q∗(θ) itself stands for and why it is a desirable belief
distribution to target. In contrast, eq. (3) shows that the Bayesian posterior arises as the
solution of a clearly interpretable optimization problem.
2.3 Optimality of standard Variational Inference
While Bayesian posteriors of the form given in eq. (2) are analytically available up to a
normalizing constant, this is not immediately useful. Speciﬁcally, (asymptotically) exact
computations of expectations and integrals with respect to these posteriors are in general only
possible through sampling methods and incur a large computational burden. To alleviate this
problem, numerous approximations to the exact Bayesian posterior have been proposed. The
principal idea of any such approximation is to force the posterior belief into some parametric
form. Speciﬁcally, one seeks to approximate q∗
B(θ) ≈q∗
A(θ), where q∗
A(θ) ∈Q and
Q = {q(θ|κ) : κ ∈K}
(5)
denotes a family of distributions parameterized by κ. It is obvious that this signiﬁcantly
reduces the computational burden, as it transforms the optimization from an inﬁnite-
dimensional into a ﬁnite-dimensional one.
The literature on approximations of this sort is large and has diverse origins. Their
development arguably started with Laplace Approximations (see e.g. the seminal papers of
Tierney and Kadane, 1986; Shun and McCullagh, 1995; MacKay, 1998), which have recently
been reﬁned substantially into Integrated Nested Laplace Approximations (Rue et al., 2009).
8

Generalized Variational Inference
(a) DVI interpretation of VI
(b) Interpretation of VI as in Theorem 3
Figure 2: Best viewed in color. Depicted is a schematic to clarify the conceptual distinction
between two interpretations of VI. DVI methods interpret VI as the KLD-projection of
q∗
B(θ) into the variational family Q. New methods are then derived by replacing the KLD
with alternative projection operators.
In contrast, Theorem 3 interprets VI posteriors
as best solutions to a constrained optimization problem. Rather than ﬁnding the global
optimum q∗
B(θ) of the optimization problem in eq. (3), VI ﬁnds the best solution in the
subset Q ⊂P(Θ). This optimization-centric view on variational methods is also the logic
underlying GVI posteriors.
A second family of approximation methods known as Expectation Propagation (Opper and
Winther, 2000; Minka, 2001) was motivated through factor graphs and message passing
(Minka, 2005). The third and arguably most successful approach originated by connecting the
Expectation-Maximization algorithm (Dempster et al., 1977) and the variational free energy
from physics (Neal and Hinton, 1998), culminating in Variational Inference (VI) (Jordan
et al., 1999; Beal, 2003). For these methods, Q in eq. (5) is called the variational family.
Traditionally, two main interpretations of VI prevail. Firstly, one can derive its objective
function as an Evidence Lower Bound (ELBO). Secondly, one can show that the same
objective function minimizes the KLD between Q and q∗
B(θ). In this paper, we introduce and
advocate a third–to the best of our knowledge novel—view on VI: Relative to the objective
in eq. (3), VI corresponds to the best Q-constrained solution.
2.3.1 VI as log evidence bound
A standard way of deriving VI is the following: Since one wishes to obtain the posterior
maximizing the evidence in the data, a reasonable objective is to pick the element in the
approximating family Q that maximizes the evidence. This logic is operationalized by
9

Knoblauch, Jewson and Damoulas
observing that for Z the normalizing constant (or partition function) and any q(θ) ∈Q,
log p(x1:n) = log
 Z
Θ
Z−1 exp{−
n
X
i=1
ℓ(θ, xi)}π(θ)dθ
!
= log
 Z
Θ
exp{−
n
X
i=1
ℓ(θ, xi)}π(θ)
q(θ) q(θ)dθ
!
−log Z
JIE
≥
Z
Θ
log
 
exp
(
−
n
X
i=1
ℓ(θ, xi)
)
π(θ)
q(θ)
!
q(θ)dθ −log Z
(6)
where we have applied Jensen’s Inequality in the last step. The right hand side of eq. (6)
is called the Evidence Lower Bound (ELBO), and maximizing it over Q is independent of
Z. With this, one ﬁnally obtains the standard VI posterior as solution to an optimization
problem given by
q∗
VI(θ) = arg min
q∈Q
(
Eq(θ)
" n
X
i=1
ℓ(θ, xi)
#
+ KLD (q||π)
)
.
(7)
Here, q∗
VI(θ) = q(θ|κ∗) for some optimal parameter κ∗∈K.
Taking inspiration from the Evidence Lower Bound interpretation of the VI objective,
alternative approximation methods produce VI posteriors maximizing generalized Evidence
Lower Bounds (e.g. Chen et al., 2018; Domke and Sheldon, 2018; Burda et al., 2016). For
some bound log p(x1:n) ≤G-ELBO(q) on the evidence, such methods produce posteriors as
q∗
G−ELBO(θ) = arg min
q∈Q
{G-ELBO(q)} .
2.3.2 VI as KLD-minimization & Discrepancy VI (DVI)
A second well-known perspective on standard VI posteriors is motivated by rewriting the
objective in eq. (7) in terms of the Kullback-Leibler Divergence (KLD) as follows:
q∗
VI(θ) = arg min
q∈Q
n
KLD

q(θ)
q∗
B(θ)
o
The relevant algebraic arguments are similar to the ones used in the proof of Theorem 1. In
eﬀect, the resulting re-arrangement of terms above shows that standard VI ﬁnds the element
q∗
VI(θ) ∈Q closest to the Bayesian posterior belief in the KLD-sense.
This insight has produced a growing body of literature seeking to minimize (local or
global) discrepancies D between Q and q∗
B(θ) that are diﬀerent from the KLD (e.g. Minka,
2001; Opper and Winther, 2000; Li and Turner, 2016; Saha et al., 2019; Dieng et al., 2017;
Hernández-Lobato et al., 2016; Yang et al., 2019; Cichocki and Amari, 2010; Ranganath
et al., 2016; Wang et al., 2018). For a disrepancy measure D : P(Θ) × P(Θ) →R, methods
of this kind compute approximations to q∗
B(θ) based on objectives of the form
q∗
DVI(θ) = arg min
q∈Q
n
D

q(θ)
q∗
B(θ)
o
.
Throughout the remainder of this paper, we call procedures of this nature with D ̸= KLD
Discrepancy Variational Inference (DVI) methods. Their logic and intuitive appeal is
summarized in Figure 2a.
10

Generalized Variational Inference
2.3.3 VI as constrained optimization
While the interpretations of VI as optimizing over an evidence lower bound and as minimizing
a discrepancy are well-known, this paper presents a third interpretation: VI posteriors are
also the Q-optimal solutions to the optimization problem in eq. (3) characterizing Bayesian
inference. To spot this, simply compare eq. (3) to eq. (7) and notice that the optimization
problem only diﬀers through the space over which optimization is performed: As opposed to
the set of all probability measures P(Θ) as in eq. (3), eq. (7) only considers the parameterized
subset Q. This observation is rather signiﬁcant and bears important implications summarized
in the following Theorem and Figure 2b.
Theorem 3 (Optimality of standard VI) Relative to the inﬁnite-dimensional optimiza-
tion problem over P(Θ) characterizing Bayesian inference and a ﬁxed ﬁnite-dimensional
variational family Q, standard VI produces the optimal posterior belief in Q.
Proof First, notice that the VI posterior belief distribution q∗
VI(θ) and the Bayesian posterior
belief distribution q∗
B(θ) both seek to minimize
Eq(θ)
" n
X
i=1
ℓ(θ, xi)
#
+ KLD(q∥π)
over q(θ). Second, notice that q∗
VI(θ) is the minimizer of this objective relative to Q while
q∗
B(θ) is the minimizer relative to P(Θ). Third, note that Q ⊂P(Θ).
Remark 4 The result of Theorem 3 is important: As Observation 1 explained, committing
to a Bayesian posterior q∗
B(θ) is equivalent to committing to the objective function in eq. (3).
In other words, if we judge the posterior belief q∗
B(θ) to be desirable, we are also saying that
the objective function in eq. (3) encodes properties that we want our posterior belief to adhere
to. Accordingly, once we restrict our posterior beliefs to be in a subset Q ⊂P(Θ), we should
want to compute the best possible solution to the same objective in Q. As Theorem 3 shows,
this is exactly what VI does. Importantly, this has two implications: Firstly, it gives another
meaningful interpretation to standard VI approximations to q∗
B(θ). Secondly, it provides
important insights into the suboptimality of alternative approximation methods summarized
in the following Corollary.
Corollary 5 (Suboptimality of alternative methods) Relative to the inﬁnite-dimensional
optimization problem over P(Θ) characterizing Bayesian inference and a ﬁxed ﬁnite-dimensional
variational family Q, methods diﬀerent from standard VI produce sub-optimal posterior beliefs.
Proof We prove this by contradiction: Suppose the posterior belief q∗
A(θ) was produced by
some alternative method A that is not equivalent to standard VI. Suppose also that q∗
A(θ) is
the Q-optimal posterior relative to eq. (3). By deﬁnition of standard VI, it then holds that
that for any sequence of observations x1:n and for all n,
Eq∗
VI(θ)
" n
X
i=1
ℓ(θ, xi)
#
+ KLD (q∗
VI||π) ≤Eq∗
A(θ)
" n
X
i=1
ℓ(θ, xi)
#
+ KLD (q∗
A||π) .
11

Knoblauch, Jewson and Damoulas
Suppose the inequality is strict. This immediately yields a contradiction with the supposition
that q∗
A(θ) is the Q-optimal posterior relative to eq. (3). Alternatively, suppose the inequality
is an equality for any sequence of observations x1:n and for all n. This too immediately yields
a contradiction: In particular, it violates the supposition that the method producing q∗
A(θ)
is not equivalent to standard VI. Thus, the desired contradiction is obtained.
Remark 6 Corollary 5 implies that once the variational family Q is ﬁxed, producing an
approximation to q∗
B(θ) which does not correspond to standard VI posteriors is sub-optimal
under an optimization-centric view on Bayesian inference. This notion of optimality is
important because Theorem 1 and Observation 1 expose the isomorphic relationship between the
optimization problem in eq. (3) and the exact Bayesian posterior. An immediate consequence
of Corollary 5 is thus that methods built around generalized evidence lower bound formulations,
alternative Discrepancy Variational Inference ( DVI) methods or Expectation Propagation
( EP) approaches (e.g. Minka, 2001; Opper and Winther, 2000) are sub-optimal relative to
standard VI: If one wishes to minimize the objective that deﬁnes the Bayesian posterior
q∗
B(θ), it is irrational to pick any q ∈Q not produced by standard VI.
2.4 Reconciling (sub)optimality with empirical evidence
At ﬁrst glance, the conclusions from Corollary 5 and Remark 5 seem to contradict numerous
landmark ﬁndings in the area of approximate Bayesian inference: Firstly, there are various
issues with standard VI that are well-known and hinder its eﬀectiveness in certain situations
(see e.g. Turner and Sahani, 2011). For this reason, various alternative approximations have
proven successful in practice (e.g. Minka, 2001; Rue et al., 2009) and often produce more
desirable posterior inferences. All this seems to contradict the (sub)optimality results in
Theorem 3 and Corollary 5.
This contradiction resolves itself upon closer examination. Speciﬁcally, the practical
relevance of any optimality result hinges on two crucial assumptions that are typically
violated in practice: Firstly, one needs to assume that the original objective in eq. (3) is
appropriately speciﬁed. Secondly, one needs the variational family Q to be rich enough so
that the statement q∗
VI(θ) ≈q∗
B(θ) is not completely vacuous. Conversely, this means that
q∗
A(θ) can produce more desirable approximations to q∗
B(θ) than q∗
VI(θ) whenever one of the
following holds:
(i) the original objective in eq. (3) is misspeciﬁed and does not reﬂect the belief distribution
we wish to compute;
(ii) The approximating family Q is inappropriately speciﬁed so that the statement q(θ) ≈
q∗
B(θ) is vacuous for any q ∈Q.
Under (i), q∗
A(θ) will outperform q∗
VI(θ) whenever its objective implicitly encodes desirable
properties for the posterior belief distribution that are not part of the objective in eq. (3).
Similarly, if q∗
A(θ) is designed to accommodate speciﬁc choices of Q that q∗
VI(θ) struggles
with, it will perform well under (ii).
For example, virtually all posteriors produced within the DVI family (e.g. Li and Turner,
2016; Hernández-Lobato et al., 2016; Dieng et al., 2017; Regli and Silva, 2018) are designed
12

Generalized Variational Inference
to address (ii): In particular, these methods prevent unimodal approximations from focusing
too strongly around the empirical risk minimizer of θ. For standard VI, this phenomenon
is common whenever Q is the mean ﬁeld variational family, which explains why DVI often
empirically outperforms standard VI for this popular choice of Q. Taking the optimization-
centric view on posterior beliefs, this implies that in spite of being sub-optimal relative
to eq. (3), DVI methods pose objectives that are often better-suited to produce belief
distributions in Q. This raises an interesting question: Rather than thinking of inference in
a subset Q ⊂P(Θ) as approximate, can we adapt a radical optimization-centric view and
directly design appropriately speciﬁed objectives to generate posterior beliefs with desirable
properties? The remainder of this paper gives an aﬃrmative answer to this question in the
form of Generalized Variational Inference (GVI).
Taking inspiration from (i) and (ii), the next section ﬁrst takes a step back and explores
the conditions under which such alternative GVI posteriors could be desirable. As we shall
see, the isomorphic relationship between eq. (3) and the Bayesian posterior provides a
comprehensive answer to this question: Speciﬁcally, we explain the ways in which the assump-
tions underpinning the traditional Bayesian paradigm giving rise to the Bayesian posterior
q∗
B(θ) and eq. (3) are often misaligned with the realities of contemporary statistical machine
learning. This misalignment problem has three important dimensions: The information
contained in the prior belief (P), the role of the likelihood model (L), and the availability of
computational resources (C).
3. A reality check: Re-examining the traditional Bayesian paradigm
In the following section, we illuminate the misalignment between the assumptions underlying
the traditional Bayesian paradigm and the way in which modern statistical machine learning
uses (approximate) Bayesian posteriors to conduct inference.
First, Section 3.1 recalls and elaborates on the three crucial assumptions underlying
the standard Bayesian posterior: An appropriate prior (P) and likelihood (L) and an
inﬁnite computational budget (C).
Next, Section 3.2 exposes the misalignment of these three assumptions with inferential
practices in contemporary statistical machine learning and large-scale inference.
Lastly, Sections 3.3–3.5 illustrates the adverse real-world consequences arising from
violating these assumptions.
3.1 The traditional Bayesian paradigm
Due to their direct correspondence with the fundamental rules of probability, Bayesian
posteriors q∗
B(θ) are desirable objects to be basing inference on. To see why, suppose the
following three conditions hold true.
(P) The Prior π(θ) is correctly speciﬁed: It encodes the best available judgement about θ
based on all information available to the modeller. Crucially, the distribution π(θ) is
assumed to reﬂect this prior belief exactly. This implies that π(θ) should completely
reﬂect all information available to the modeller such as previously observed observations
13

Knoblauch, Jewson and Damoulas
x−m:0 of the same phenomenon or domain expertise relating to the problem domain
and the statistical model.
(L) There exists an (unknown but ﬁxed) θ∗making the Likelihood model equivalent to
the data generating mechanism of xi. This is to say that xi ∼p(xi|θ∗).3
(C) The budget for Computation is inﬁnite, so the complexity of computing the belief
q∗
B(θ) can be ignored.
If (L), (P) and (C) are satisﬁed, it immediately follows that the best belief for the event
{θ∗= θ}|{x1:n = x1:n} is simply given by the analytically available posterior
dP (θ|x1:n) ∝dP (θ)
n
Y
i=1
dP (xi|θ) = π(θ)
n
Y
i=1
p(xi|θ) = q∗
B(θ).
(8)
Note that (P) and (L) lend a meaningful interpretation to Bayes’ rule in form of conditional
probability updates. Complementing this, (C) ensures that it is feasible to compute the
generally intractable solution q∗
B(θ) of eq. (3). Accordingly, (C) generally is interpreted to
mean that a Markov Chain Monte Carlo algorithm can be run for long enough to accuratley
represent? q∗
B(θ). In summary, if (P), (L) and (C) hold, q∗
B(θ) is the only desirable posterior
belief distribution.
But how well does reality align with (P), (L) and (C)? Turning attention to (C) ﬁrst,
most traditional scientiﬁc disciplines have little need to worry about computational complexity
and will resort to sampling schemes for two reasons: Firstly, the models are often relatively
simple and thus straightforward to infer. Secondly, even for more complicated models the
experimental setup and data collection typically outweighs the cost of computation by orders
of magnitude. As for (P) and (L), neither prior nor likelihood are ever perfect reﬂections
of one’s full prior beliefs (see e.g. Goldstein, 1990; O’Hagan and Oakley, 2004; Goldstein,
2006) or the data generating mechanism (see e.g. Bernardo, 2000). In other words, (P) and
(L) are invariably violated when interpreted literally. However and as enshrined in Box’s
aphorism that all models are wrong, but some are useful, this is not a problem so long as
these violations are suﬃciently small. In traditional statistics, ensuring that these violations
are small has typically been enforced through a simple recursion (e.g. Box, 1980; Berger
et al., 1994). Speciﬁcally, until you are conﬁdent that both (P) and (L) are close enough
to the truth, repeat the following: Check if (L) or (P) are violated severely. If they are,
3.
We note here that to keep the presentation simpler, we are giving conditions that are stricter than
what is required for Bayesian analysis. In particular, (L) corresponds to an objectivist treatment of the
likelihood and can be weakened under the subjectivist paradigm for Bayesian analysis. In this paradigm,
the treatment of the likelihood mirrors that of the prior: It now simply corresponds to the modeller’s
belief about the process that generated the data. While this ﬁrst sounds like a weaker requirement, it
ends up producing the same misspeciﬁcation problems as (L). Speciﬁcally, a subjectivist treatment of
the likelihood requires the modeller to express her beliefs about the likelihood function exactly. This
forces her to make more probability statements than she realistically has time or introspection for (see
e.g. Goldstein, 1990; O’Hagan and Oakley, 2004; Goldstein, 2006). The result is that the likelihood
function supplied by the modeller is at best going to be an approximate description of the modeller’s
beliefs. This provides the subjectivist interpretation of misspeciﬁcation. Notice that it directly mirrors
the objectivist interpretation of misspeciﬁcation in (L): The likelihood function supplied is at best going
to be an approximate description of the true data generating mechanism.
14

Generalized Variational Inference
choose a more appropriate likelihood and prior. In order to operationalize this iterative logic,
batteries of descriptive statistics, tests and model selection criteria have been developed over
the years.
In summary then, ignoring the computational overhead and iteratively reﬁning likelihoods
and priors is rightfully the predominant inferential strategy for traditional scientiﬁc endeavours.
Not only is domain expertise relevant for designing priors and likelihood, but the process of
ﬁnding an appropriate model often provides valuable insights in itself. Further, the expensive
part of the analysis is typically data collection. Consequently, it is typically not prohibitive
to perform inference even with the most computationally expensive of sampling schemes. In
line with this, most methodological contributions in statistical sciences rely to a substantial
degree on (P), (L) and (C).
3.2 Machine Learning: Challenging the traditional paradigm
Contemporary large-scale inference applications have frequently turned the traditional
schematic of statistical model design upside down: Rather than carefully designing an
appropriate likelihood model p(xi|θ) for a speciﬁc data domain, statistical machine learning
research is typically characterized by the search of a ﬂexible algorithm that can ﬁt any
data set x1:n well enough to produce useful inferences. The resulting likelihood models are
typically not attempting to describe any data generating processes in the sense of (L). Rather,
they are highly over-parameterized functions of θ and typically un-identiﬁable, meaning that
θ∗is neither interpretable nor unique. Such statistical machine learning models have three
major issues under the traditional paradigm of Bayesian inference that are readily identiﬁed:
(EP) Invariably, the Prior is misspeciﬁed. Two factors compound this issue: Firstly, the
large number of parameters over-parameterizing the likelihoods of many statistical
machine learning models are no longer interpretable. This often prohibits domain
experts to carry out carefully guided prior elicitation. Secondly, priors are typically
selected at least in part for their computational feasibility. This fundamentally alters
the interpretation of the prior: Rather than the result of an attempt to capture the
modeller’s knowledge before observing the data, the prior takes the role of a reference
measure or regularizer. To make matters worse, the number of parameters is often
large relative to n. In turn, this implies that the priors have a disproportional eﬀect on
inference via q∗
B(θ), a problem we will discuss in Example 1 in the context of Bayesian
Neural Networks.
(EL) Clearly, the Likelihood is misspeciﬁed. This often has adverse side eﬀects: While
using an oﬀ-the-shelf and often over-parameterized likelihood function can provide a
good ﬁt for the typical behaviour of the data, it often causes severe problems with
heterogeneous or untypical data points. We will demonstrate this phenomenon on a
changepoint problem in Example 2.
(EC) With increasingly complex statistical models, (C) has proven an increasingly infeasible
description of reality. Accordingly, this problem has inspired numerous directions
of research, including variational methods and Laplace approximations. Example 3
illustrates this for the case of Gaussian Processes.
15

Knoblauch, Jewson and Damoulas
Under the challenges outlined in (EP), (EL) and (EC), standard Bayesian posteriors often
do not provide appropriate belief distributions. In the remainder, we will explain how and
why this is the case for many parts of modern large-scale inference.
0
2
misspecified
d=50
d=100
d=250
KLD
D( )
AR, 
= 0.5
0
2
well-specified
KLD
D( )
AR, 
= 0.5
KLD
D( )
AR, 
= 0.5
qGVI(
true
1
1)
GVI posteriors and prior specification
Figure 3: Best viewed in color. Taken from Knoblauch (2019a), the plot shows the impact
of diﬀerent prior beliefs on inference in a Bayesian normal mixture model with n = 50
observations and mixture components in Rd for diﬀerent choices of d. Speciﬁcally, the
plot compares inference outcomes under a misspeciﬁed prior (Top) against those under
a well-speciﬁed prior (Bottom). It does so by depicting the average absolute diﬀerence
between the true parameter values and their MAP estimate on the y-axis. Here, the solid
whiskers’ length corresponds to one standard deviation of the underlying posterior. The plot
shows that using the KLD as uncertainty quantiﬁer as in standard Variational Inference
(VI) will produce undesirable uncertainty quantiﬁcation under misspeciﬁed prior beliefs.
In contrast, Generalized Variational Inference (GVI) with Rényi’s α-divergence as
uncertainty quantiﬁer produces desirable uncertainty quantiﬁcation in both settings.
3.3 Prior misspeciﬁcation
For most ﬁnite-dimensional parameters, even severely misspeciﬁed priors can often be harmless.
Sometimes, this expresses itself in theoretical results: For example, prior misspeciﬁcation is
typically no problem in the asymptotic sense. Speciﬁcally, so long as (L) holds, it suﬃces
that π(θ∗) > 0 for the standard Bayesian posterior to contract around θ∗at rate O(n−1/2)
(see e.g. Ghosal, 1998; Ghosal et al., 2000; Shen and Wasserman, 2001; Walker, 2004, and
references therein).
Oftentimes, these results are used as an apology to neglect the role of prior speciﬁcation.
While it is reassuring that the sequence of standard Bayesian posteriors shrinks to the
population-optimum as n →∞, this does not describe the real world: In particular, n is
usually ﬁxed and only a single posterior is computed. Note that whenever n is ﬁxed, it is
possible to specify an arbitrarily bad prior belief. This means that once one departs from
assuming that (P) is at least approximately correct, the standard Bayesian posterior belief
about θ∗can be made arbitrarily inappropriate—even if (L) still holds. Figure 3 illustrates
this on a Bayesian Mixture Model and also shows how Generalized Variational Inference
16

Generalized Variational Inference
(GVI) can solve this problem. As the Figure shows, ﬁnite data can make prior misspeciﬁcation
a more serious issue, even more so if (i) the parameter space is large relative to n or (ii) it is
impossible to specify priors in a principled way. As we discuss in the next example, a model
invariably aﬀected by both problems is the Bayesian Neural Network (BNN).
Example 1 (Deep Bayesian models as violations of (P)) Bayesian Neural Networks
( BNNs) (MacKay, 1996; Neal, 2012) seek to combine Deep Learning models with Bayesian
uncertainty quantiﬁcation. For the parameter vector θ of weights, let F(θ) be the non-linear
composition of activation functions speciﬁed by a Neural Network. A conceptually appealing
way of thinking about BNNs is as an arbitrarily ﬂexible likelihood function with a large
number of parameters d = |Θ|. This is to say that one believes that (at least approximately),
xi ∼p(xi|F(θ∗)) for some θ∗∈Θ. For a prior π(θ) about θ, this means that BNNs seek to
do inference on the posterior given by
q∗(θ) ∝π(θ)
n
Y
i=1
p(xi|F(θ)).
At ﬁrst, this approach seems conceptually appealing: Not only does one circumvent most
issues with ( L) by making the likelihood function almost arbitrarily ﬂexible, but one also
quantiﬁes uncertainty in the usual Bayesian manner. While both observations are correct,
they mask a potentially severe issue with this approach: Namely, specifying the prior π(θ) in
a principled way and in (approximate) accordance with ( P) is impossible in practice: Firstly,
because the vector θ indexes a black box model, its entries do not correspond to interpretable
quantities. Accordingly, building prior beliefs based on domain expertise about the data x1:n
is not feasible. Secondly, since computational aspects are a major concern for BNNs, one is
typically constrained to choosing priors that factorize over θ. As a consequence, practitioners
often resort to choosing default priors which are not motivated as prior beliefs in the original
sense or by an attempt to approximately satisfy ( P). Speciﬁcally, one typically just picks
π(θ) = Qd
j=1 πj(θj), where πj(θj) is a standard normal distribution for all j. Choosing
priors in this ad-hoc fashion violates the principles underlying classical Bayesian modelling
(see also Section 5.2.1). This is especially problematic whenever n is small relative to d: In
these situations, prior inﬂuence serves as a strong source of information about θ. Thus, if the
prior is misspeciﬁed and n is small relative to d, the (incorrect) information contained in the
prior often overshadows the information in the data. At the same time, reliable uncertainty
quantiﬁcation is most important whenever n is small relative to d. Indeed, this is a well-known
issue and is addressed in various contributions by up-weighting the likelihood (down-weighting
the KLD term in the ELBO), see Bowman et al. (2016); Zhang et al. (2018); Rossi et al.
(2019a,b); Sønderby et al. (2016).
For completeness, we note that the current paper does not discuss uninformative and
so-called objective priors (see, e.g. Jeﬀreys, 1961; Zellner, 1977; Bernardo, 1979; Berger
and Bernardo, 1992; Jaynes, 2003; Berger, 2006). Priors of this kind are constructed to be
as uninformative as possible and thus in some ways objective. In many ways, they are a
principled and natural response to the critique of ill-informed priors. Generally however,
their construction results in so-called improper priors–densities that do not correspond to a
ﬁnite measure and thus do not integrate to one. While this is not generally prohibitive, it
17

Knoblauch, Jewson and Damoulas
would severely complicate the developments of Section 4 because most divergences are not
well-deﬁned for improper priors4.
3.4 Likelihood Misspeciﬁcation
While prior misspeciﬁcation aﬀects inference adversely, the issue for inferential practice
is even more serious if (L) is violated thoroughly: Whenever the likelihood model for xi
is severely misspeciﬁed, inference outcomes suﬀer dramatically. Moreover, not even the
asymptotic regime oﬀers a remedy and the adverse eﬀects of misspeciﬁcation persist as
n →∞. The traditional approach to addressing this issue is straightforward: If the likelihood
model p(xi|θ) is misspeciﬁed, simply investigate why exactly it ﬁts the data poorly. After
residual analysis, intense study of descriptive statistics and consultation with domain experts,
redesign it to arrive at a likelihood model p′(xi|θ′), which hopefully provides a better ﬁt to
the data and (approximately) satisﬁes (L). In other words, the traditional view is that any
problem with misspeciﬁcation is really a problem with careless modelling.
As outlined in Section 3.2, this strategy is neither practiced nor feasible with contemporary
large-scale models. The naive interpretation of likelihood functions as corresponding to an
appropriately good description of the true data generating process in the sense of (L) is thus
wholly inappropriate. This is especially important as many large-scale models are mainly
interested in capturing the typical behaviour of the data—rather than fully modelling every
aspect of a population. While this may appear to be a minor point at ﬁrst glance, it has
serious consequences for inferential practice. To see why, suppose a population contains
a small number of outlying observations, local heterogeneities or spiky noise. The naive
interpretation of the likelihood as in (L) assumes that these untypical aspects are encoded
in the likelihood function. Hence, if xi is an outlier so that p(xi|θ) is very close to zero
for some value of θ constructed to ﬁt the rest of the data, the inference machinery of
traditional statistics interprets this as a strong signal: After all, if the likelihood model is an
approximately correct description of the data generating mechanism, the most informative
observations are those that do not ﬁt the model ﬁtted to the rest of the data. It follows that
aberrant parts of the data will have a disproportional impact on inference outcomes—leading
standard inference methods to break down (see also Jewson et al., 2018).
While it is in general hard to visualize this issue, inﬂuence functions provide a concise
way of showcasing the problem. Roughly speaking, inﬂuence functions in a Bayesian context
quantify the impact the (n + 1)-th observation xn+1 has on the posterior distribution q∗
B(θ)
constructed using the ﬁrst n observations (Peng and Dey, 1995). This discrepancy is measured
by computing a divergence between the posteriors based on x1:n and on x1:(n+1). Using the
Fisher-Rao divergence (for its geometric properties as explained in Kurtek and Bharath,
2015), Figure 4 compares the inﬂuence of a standard Bayesian posterior with that of a
posterior belief computed using Generalized Variational Inference (GVI). The left side of the
Figure formalizes the intuition we have just developed: In the standard Bayesian case, the
inﬂuence of xn+1 on the posterior belief grows stronger and stronger the more untypical it is
relative to previously observed data. Similarly, the right side shows the adverse eﬀect this
4. The KLD is the exception to this rule: As it depends on the log normalizer of π(θ) in an additive fashion,
improper priors can still be admissible so long as eq. (3) yields a solution for the unnormalized version of
the KLD as given in Cichocki and Amari (2010).
18

Generalized Variational Inference
has on the posterior predictive. To make the implications of inﬂuence functions for inferential
practice more tangible, we additionally demonstrate the outlier problem in Example 2.
0
2
4
6
8
10
0.0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
Standard Deviations from the Posterior Mean
Inﬂuence/Density
−log(p(x; θ))
Lβ
p(x, θ), β = 1.05
Lβ
p(x, θ), β = 1.1
Lβ
p(x, θ), β = 1.25
N(0,1)
-5
0
5
10
15
0.0
0.1
0.2
0.3
0.4
x
Density
(1 −ϵ)N(0, 1)
ϵN(8, 1)
VI, −log(p(x; θ))
GVI, Lβ
p(x, θ)
Contamination
Figure 4: Best viewed in color. The plots compare inﬂuence functions (Left) and predictive
posteriors (Right) of a standard Bayesian against a GVI posterior. Left: The inﬂuence
functions of scoring the normal likelihood with a standard negative log likelihood against
a robust scoring rule derived from β-divergences. Right: A univariate normal is ﬁtted
using all the data depicted, including the outlying contamination. The posterior predictive
corresponding to the robust scoring rule is able to ignore these outliers. This stands in
contrast to the posterior predictive based on standard, which assigns increasingly large
inﬂuence to outlying observations.
Example 2 (Outliers as violations of (L)) While there exist many formalizations of the
notion of an outlier, the conceptually most useful one is probably the ε-contamination model.
In the ε-contamination model, the data points xi are generated according to a contaminated
density composed additively as
ptrue(xi) = (1 −ε) · p(xi|θ∗) + ε · o(xi),
for some small and ﬁxed 0 < ε < 1, a ﬁxed parameter value θ∗of interest and a contaminating
outlier-generating density o. An obvious violation of ( L) for this case would be ﬁtting the
data only to the non-contaminated component p(xi|θ) in order to infer θ∗.
Considering this type of model misspeciﬁcation is especially poignant in Bayesian On-line
Changepoint Detection ( BOCPD), a well-studied family of models that yield computationally
eﬃcient algorithms (see e.g. Adams and MacKay, 2007; Fearnhead and Liu, 2007; Wilson
et al., 2010; Saatçi et al., 2010; Caron et al., 2012; Turner et al., 2013; Knoblauch and
19

Knoblauch, Jewson and Damoulas
Damoulas, 2018; Knoblauch et al., 2018). BOCPD aims to segment a data stream in real
time and achieves this via an eﬃcient recursion updating the Bayesian posterior with each
newly arriving observation. A canonical application example of BOCPD is the well-log data
set ﬁrst discussed by O’Ruanaidh (1994). Its observations describe the abruptly changing
nuclear responses of rock stratiﬁcation during the course of drilling a well. Generally, the
diﬀerent rock strata are clearly distinguishable from one another. However, rock formation
processes are noisy and sometimes interrupted by extraordinary events such as tsunamis,
earth quakes or eruptions. Accordingly, the data points generated are surprisingly close to
an ε-contaminated normal distribution within each of the clearly distinguishable rock strata.
Figure 5 is taken from Knoblauch et al. (2018) and shows how this phenomenon renders
vanilla BOCPD an unreliable algorithm. It also shows that this issue can be remedied by
constructing alternative posterior belief distributions via a Generalized Variational Inference
( GVI) procedure relying on a robust loss function derived from the β-divergence.
0
500
1000
1500
2000
2500
3000
3500
4000
Time
80000
100000
120000
140000
Nuclear Response
Figure 5: Best viewed in color. Inference outcomes of BOCPD on the well log data set
using the standard Bayesian posterior and a GVI posterior constructed with robust
losses based on the β-divergence. Solid vertical lines correspond to Maximum A Posteriori
(MAP) segmentation of GVI posterior, dashed vertical lines mark incorrect changepoints
additionally detected under standard Bayesian inference.
3.5 Computation mismatch
As Theorem 1 shows, the Bayesian posterior q∗
B(θ) is the result of an optimisation problem
over the inﬁnite-dimensional space P(Θ).
Generally therefore, the posterior itself also
does not have a closed form expressed in a ﬁnite-dimensional parameter.In fact, the only
case in which q∗
B(θ) can be represented through a ﬁnite-dimensional parameter is when
prior and likelihood are conjugate to one another. Accordingly, performing inference with
q∗
B(θ) is in general a hard problem, which manifests itself through the need to compute the
generally intractable normalizing constant. To circumvent this problem, it is common to
leverage Markov Chain Monte Carlo algorithms that produce an exact representation of
q∗
B(θ) if the chain runs indeﬁnitely and collects inﬁnitely many (correlated) samples. In
practice, collecting a ﬁnite number of samples from the chain will often yield a reasonable
approximation to the posterior so long as d = |Θ| is not too large. For large values of d
however, the number of samples required to make the approximation useful is often too
large to make sampling a computationally viable strategy: For example, in the best case
scenario, Random Walk Metropolis Hastings scales like O(d2) (Roberts et al., 1997), the
20

Generalized Variational Inference
Metropolis-adjusted Langevin algorithm like O(d4/3) (Roberts and Rosenthal, 1998) and
Hamiltonian Monte Carlo like O(d5/4) (Beskos et al., 2013). Note that these results assume
independence and Gaussiantiy—so in practice scaling rates are typically much worse.
An alternative way of avoiding the computation of a normalizing constant are various
approximation strategies seeking to project q∗
B(θ) into some parameterized subset Q ⊂P(Θ).
This strategy will produce approximations q∗
A(θ) of high quality only if the set Q is chosen
to be suﬃciently rich so that the statement q∗
A(θ) ≈q∗
B(θ) is not completely vacuous.
Importantly however, most posterior belief distributions q∗
A(θ) that are regularly computed
this way barely deserve to be called approximations to q∗
B(θ). For example, consider the
mean ﬁeld normal variational family given by
QMFN =



d
Y
j=1
N(θj|µj, σ2
j ) : µj ∈R, σ2
j ∈R>0 for all j


.
(9)
For most interesting non-trivial posterior distributions q∗
B(θ), there will not exist any element
in QMFN that could be considered an approximation to q∗
B(θ) in any meaningful way: After all,
this variational family directly assumes O(d2) independence relationships in the approximate
posterior belief for θ. Worse still: As approximations are particularly attractive when |Θ| = d
is large, in practice we will resort to these insuﬃciently expressive “approximations” to q∗
B(θ)
precisely when the elements in QMFN are structurally most dissimilar from q∗
B(θ). As we
proceed to explain in the remainder of the paper, we think it is often unhelpful to think of
posterior beliefs q∗
A(θ) computed in this way as approximations to q∗
B(θ). Rather, we think
of them as deﬁning a new and distinct posterior belief distribution in their own right.
To make the preceding discussion more tangible and highlight the importance that the
frequent violation of (C) has played in research on statistical machine learning, Example 3
illuminates the importance of computational considerations for Gaussian Processes.
Example 3 (large-scale Gaussian processes as violations of (C)) Many Bayesian ma-
chine learning models prohibit exact computation. One particularly interesting case are
Gaussian Processes ( GPs): Even in the special cases where they admit closed form pos-
teriors, it may well be impossible to compute them exactly for suﬃciently large inference
problems. The reason is that for n observations, direct computation of the associated GP
posterior takes O(n3) time. As a consequence, an entire literature is dedicated to bringing
down this prohibitive computational complexity (see for instance Williams and Seeger, 2001;
Quiñonero-Candela and Rasmussen, 2005; Snelson and Ghahramani, 2006; Titsias, 2009)
and developing software or computer-architecture speciﬁc methods geared towards inference
with GPs (e.g. Matthews et al., 2017; Gardner et al., 2018; Balandat et al., 2019; Wang
et al., 2019). Furthermore, with deep (i.e., hierarchical) approaches to GPs introduced in
Damianou and Lawrence (2013) and extended in various directions (e.g. Dai et al., 2016;
Hegde et al., 2019), this challenge has only become more important (see e.g. Bui et al., 2016;
Cutajar et al., 2017b; Salimbeni and Deisenroth, 2017).
4. The Rule of Three: A new Bayesian paradigm
As the last sections have shown, the assumptions which form the core of traditional Bayesian
inference are often not a good basis for modern large-scale statistical inference. In response
21

Knoblauch, Jewson and Damoulas
to this observation, the following section seeks to generalize the Bayesian paradigm. As we
shall see, the way in which we do so is strongly inspired by the preceding development in two
important ways: Firstly, as we did in Section 2, we take inspiration from an optimization-
centric view on Bayesian inference. Secondly, since we saw in Section 3 that there are
three potentially problematic assumptions underlying Bayesian inference, we construct our
generalization in order to address each of these concerns directly and modularly. This
development proceeds in three steps.
Section 4.1 sets out axioms that are a minimal requirement for any posterior belief
distribution. In accordance with these axioms, we derive the Rule of Three (RoT).
Sections 4.2 & 4.3 discuss the RoT as a recipe for producing posterior belief distri-
butions and elaborate on its three interpretable ingredients. We also show how the
RoT can directly address the concerns associated with imposing (P), (L) and (C).
Section 4.4 demonstrates that the axiomatic development is both helpful and useful
by comparing the RoT with existing methods that generate belief distributions.
4.1 An axiomatic foundation for Bayesian inference
In this section, we set out to produce a novel axiomatic foundation for Bayesian inference
that is ﬂexible enough to deal with contemporary real-world challenges. Before doing so, we
deﬁne two core concepts required in their development.
Deﬁnition 7 (Loss Function) Losses are functions ℓ: Θ × X →R which for any obser-
vation sequence x1:n ∈X n have empirical risk minimizers
bθn = arg min
θ∈Θ
( n
X
i=1
ℓ(θ, xi)
)
.
Deﬁnition 8 (Statistical Divergence) Statistical divergences are functions D : P(Θ) ×
P(Θ) →R≥0 which satisfy that D(q∥π) ≥0 and D(q∥π) = 0 if and only if q(θ) = π(θ)
almost everywhere.
In the axiomatic development to follow, we will avoid introducing measure-theoretic notation.
Thus, for simplicity we will assume that all densities are deﬁned with respect to the Lebesgue
measure on Rd. To the same end, we will also slightly abuse notation in two ways. Firstly,
we will write that q∗∈P(Θ) for probability densities q∗(θ) on Θ, even though probability
densities are not in P(Θ). However, q∗(θ) induces a measure µq∗∈P(Θ) as µq∗(A) =
R
A dq∗(θ) for any measurable set A ⊂Θ. Thus, whenever we write q∗∈P(Θ), what we
mean is that µq∗∈P(Θ). Similarly, we will sometimes write q∗
1 ̸= q∗
2 to mean that there
exists a measurable set A ∈Θ such that µq∗
1(A) ̸= µq∗
2(A). We are now ﬁnally ready to state
the axiomatic foundations. To avoid confusion, note that Axioms will build on each other in
the order in which they are stated.
Axiom I
(Representation)
The posterior q∗∈P(Θ) is constructed by solving an
optimization problem over some space Π ⊆P(Θ). The optimization seeks to minimize exactly
two criteria that do not interact:
22

Generalized Variational Inference
(i) The in-sample loss Pn
i=1 ℓ(θ, xi) to be expected under q∗(θ).
(ii) The deviation from the prior π(θ) as measured by the magnitude of some statistical
divergence D.
Remark 9 By inspecting eq. (3), it becomes clear that the above axiom is a generalization
of traditional Bayesian inference: Firstly, eq. (3) reveals that standard Bayesian inference
solves an optimization problem over Π = P(Θ). Secondly, eq. (3) also shows that (i) of the
above Axiom also holds for standard Bayesian posteriors. In traditional Bayesian inference,
the loss to be minimized is a negative log likelihood, while more recent iterations have allowed
for broader classes of losses (e.g. Bissiri et al., 2016; Jewson et al., 2018). Thirdly, eq. (3)
demonstrates that (ii) is also satisﬁed for standard Bayesian inference: The Kullback-Leibler
Divergence ( KLD) penalizes large deviations of q∗
B(θ) from the prior π(θ).
Reiterating the essence of Observation 1, the previous axiom formalizes our understanding
of Bayesian inference as an optimization problem over a potentially inﬁnite-dimensional
function space. In fact, it already tells us that for a ﬁxed prior, posterior beliefs derived
under our new axiomatic approach have three distinct ingredients: The loss ℓ, the divergence
D(·∥π) and the space Π. Making this insight more precise immediately yields the following
representation Theorem.
Theorem 10 (Form 1) Under Axiom I, posterior belief distributions can be written as
q∗(θ) = arg min
q∈Π
(
f
 
Eq(θ)
" n
X
i=1
ℓ(θ, xi)
#
, D(q||π)
!)
,
where f : R2 →R is some function which is non-decreasing in both its arguments.
Proof This follows directly from Axiom I: Firstly, any posterior belief distribution q∗(θ)
is the solution to an optimization problem over Π. Thus, for an appropriately structured
objective Obj, one can write
q∗(θ) = arg min
q∈Π
{Obj(q)} .
Hence, the question becomes what the objective looks like. The answer is provided by parts
(i) and (ii) of Axiom I: By (i), the optimization’s objective depends on Eq(θ)[ℓ(θ, xi)]. Further,
by (ii) it also depends on the divergence D between prior and q, i.e. on D(q||π). Clearly
then, for some function f : R2 →R,
Obj(q) = f
 
Eq(θ)
" n
X
i=1
ℓ(θ, xi)
#
, D(q||π)
!
.
Noting that both arguments are not allowed to interact and are to be minimized, it is clear
that f is non-decreasing in both arguments, which completes the proof.
This result is a ﬁrst and helpful step, but in itself does not suﬃce to yield objectives that are
useful in practice. Speciﬁcally, we need to get a handle on the function f. It is clear that
23

Knoblauch, Jewson and Damoulas
under Axiom I alone, very little can be said about f. The next two axioms seek to address
this very issue. The Axiom II is imposed for a simple purpose: We want posteriors to be
invariant to uninformative components of the loss ℓas well as the divergence term D. In
other words, adding a constant C to the loss or the divergence from the prior should not
change our inferences about θ.
Axiom II (Information Equivalence) Take any two constants C, M ∈R>0 and let the
posteriors q∗
1, q∗
2 ∈P(Θ) be computed based on the same optimization problem, but with two
diﬀerent loss functions ℓ(1) and ℓ(2) as well as two diﬀerent divergences D(1), D(2) so that
D(1) = D(2) + M and so that ℓ(1) = ℓ(2) + C. Then, q∗
1(θ) = q∗
2(θ) (almost everywhere).
Remark 11 At this point, one may pause and wonder if one would not like the above axiom
to be stronger. Speciﬁcally, would one want diﬀerent posteriors if ℓ(1) = w · ℓ(2) for some
w ̸= 1? Since we want our methods to recover existing Bayesian inference techniques as
special cases, this question is readily answered. In particular, notice that many Bayesian
inference techniques do attach information to pre-multiplying losses with a constant. For
example, in the Power Bayesian framework (e.g. Grünwald, 2011, 2012; Holmes and Walker,
2017; Grünwald and Van Ommen, 2017; Miller and Dunson, 2019) one wishes to down-weight
the information in the likelihood terms by considering the pseudo-likelihood terms p(xi|θ)w for
some w ∈(0, 1) instead of p(xi|θ). This procedure is attractive since it produces alternative
posterior beliefs that contract to a point mass at a slower rate than the standard posteriors.
Re-examining eq. (3), it becomes clear that relative to standard Bayes rule, power likelihoods
are in fact a simple re-weighting scheme. Speciﬁcally, one replaces ℓ(θ, xi) = −log p(xi|θ) by
the weighted version ℓ(θ, xi) = −w log p(xi|θ).
We are now ready to state the last axiom. Observe that Axiom III determines conditions
under which the posterior must not be aﬀected by changing inputs. Complementing this, we
still require suﬃcient conditions under which the posteriors are guaranteed to diﬀer.
Axiom III (Generalized Likelihood Principle) Suppose the posteriors q∗
n, q∗
n+m ∈Π
are computed based on an optimization problem satisfying Axiom I. Assume that the same
optimization problem is used for both posteriors, except for a diﬀerence in the samples x1:n
and x1:n+m and potentially diﬀerent loss functions ℓ(1) and ℓ(2) that are used, respectively.
(i) Provided that there is an information diﬀerence between x1:n and x1:n+m for m > 0, the
posteriors are diﬀerent. In other words, Pn
i=1 ℓ(1) (θ, xi) ̸= Pn+m
i=1 ℓ(2) (θ, xi) implies
that q∗
n ̸= q∗
n+m, even if ℓ(1) = ℓ(2).
(ii) Provided that there is a diﬀerence in the measure of information between ℓ(1) and ℓ(2),
the posteriors are diﬀerent. In other words, ℓ(1) ̸= ℓ(2) implies that q∗
n ̸= q∗
n+m, even if
m = 0 so that x1:n = x1:n+m.
Remark 12 This axiom has a particularly interesting interpretation as a generalized version
of the well-known likelihood principle associated with standard Bayesian inference. Roughly
speaking, the likelihood principle says that all information in x1:n relevant to conducting
inference on the model parameters is contained in the likelihood functions evaluated at x1:n.
Similarly, Axiom III says that all information the sample x1:n contains about θ is contained
in the loss function evaluated on the relevant data sample.
24

Generalized Variational Inference
Remark 13 Notice that part (ii) of Axiom III requires that ℓ(1) and ℓ(2) measure information
diﬀerently, which precludes that ℓ(1) = ℓ(2) + C for some C ∈R>0 by Axiom II.
Recalling the discussion of Remark 11, we next state a result showing that our axiomatic
approach respects the logic of Power Bayes and related procedures. Speciﬁcally, re-weighting
the likelihood (or the losses more generally) will yield diﬀerent posteriors.
Corollary 14 If Axiom III holds, and if the posteriors q∗
1 ∈Π and q∗
2 ∈Π are based on the
same observations and losses ℓand w · ℓfor w ∈R>0 \ {1}, respectively, then q∗
1 ̸= q∗
2.
Proof This follows from part (ii) of Axiom III.
Finally, we investigate how the axiomatic developments set out above simplify the structure
of objectives producing posterior belief distributions. To achieve this, it is clear that we need
to re-investigate f : R2 →R as in Theorem 10. Since we want f to recover the standard
Bayesian posterior of eq. (3) as well as the standard VI posterior in eq. (2.3.1), it is natural
to restrict attention to elementary operations. Doing so produces the following representation
result that is both stricter and more useful than Theorem 10.
Theorem 15 Suppose the posterior belief q∗∈P(Θ) satisﬁes Axiom I. Recall from Theorem
10 that this is equivalent to saying that for some Π ⊆P(Θ), some loss function ℓ: Θ×X →R
and some divergence D(·∥π) : P(Θ) →R≥0, the posterior can be written as
q∗(θ) = arg min
q∈Π
(
f
 
Eq(θ)
" n
X
i=1
ℓ(θ, xi)
#
, D(q∥π)
!)
,
where f is some function f : R2 →R. If f(x, y) = x ◦y is an elementary operation on R
and q∗∈P(Θ) satisﬁes Axiom II, this objective is uniquely identiﬁed as
q∗(θ) = arg min
q∈Π
(
Eq(θ)
" n
X
i=1
ℓ(θ, xi)
#
+ D(q∥π)
)
.
(10)
Proof The elementary operations are addition, subtraction, multiplication and division.
Consider the losses ℓ(1) and ℓ(2) = ℓ(1) + C for C ∈R>0 a constant. It is straightforward to
see that Axiom II is violated if ◦is multiplication:
arg min
q∈Π
(
Eq(θ)
" n
X
i=1
ℓ(2)(θ, xi)
#
· D(q∥π)
)
= arg min
q∈Π
(
Eq(θ)
" n
X
i=1
ℓ(1)(θ, xi)
#
· D(q∥π) + C · D(q∥π)
)
̸= arg min
q∈Π
(
Eq(θ)
" n
X
i=1
ℓ(1)(θ, xi)
#
· D(q∥π)
)
and similarly if ◦is division. However, it is straightforward to see that the optimization
problem is invariant to adding constants to the loss or the divergence if ◦is addition or
25

Knoblauch, Jewson and Damoulas
subtraction. Finally, subtracting the prior regularizer is a direct and obvious violation of
part (ii) in Axiom I, it follows that addition is the only elementary operation on R satisfying
both Axioms and the result follows.
The last result is of crucial importance for the further development of the paper: Speciﬁcally,
eq. (10) provides a generic and ﬂexible recipe for the design of novel posterior distributions.
In fact, it is this equation that we discuss next.
4.2 The Rule of Three
Following from the axiomatic developments of the last section that culminated in Theorem 15,
we next discuss the interpretations and theoretical properties of posterior belief distributions
generated from objectives as in eq. (10). To simplify the representation throughout the
remainder, we ﬁrst deﬁne notation for posteriors of this form.
Deﬁnition 16 (Rule of Three (RoT)) Take observations x1:n, a prior π(θ), a space Π ⊆
P(Θ), a loss function ℓ: Θ × X →R and a divergence D(·∥π) : Π →R≥0. With this in
hand, we say that posteriors have been constructed via the Rule of Three ( RoT) if they
can be written as
q∗(θ) = arg min
q∈Π
(
Eq(θ)
" n
X
i=1
ℓ(θ, xi)
#
+ D(q∥π)
)
= P(ℓ, D, Π).
Here, P(ℓ, D, Π) is a short-hand notation for the RoT suppressing dependence on n and π.
Remark 17 Before moving on, note that in the RoT, D determines the shape of the uncer-
tainty, i.e. how exactly uncertainty is quantiﬁed. To see that this is true, consider eq. (3) but
leave out the KLD from the optimization problem. This yields the (non-RoT) problem
bq(θ) = arg min
q∈P(Θ)
(
Eq
" n
X
i=1
ℓ(θ, xi)
#)
.
(11)
Denoting bθn = arg minθ∈Θ {Pn
i=1 ℓ(θ, xi)} as the empirical risk minimizer (maximum likeli-
hood estimate if the loss is a negative log likelihood) and δy(x) as the Dirac measure at y, it
is immediately clear that bq(θ) = δbθn(θ), which holds as δbθn ∈P(Θ).
The RoT is a corner stone of our contribution: Based on the axiomatic development, we
argue that posteriors should take the form P(ℓ, D, Π). The most practically feasible versions
of these problems correspond to the case where Π is a κ-parameterized family of distributions
Q = {q(θ|κ) : κ ∈K}. Beliefs of this form are Generalized Variational Inference (GVI)
posteriors. They are a special case of the RoT that we explore in Section 5.
On top of the axiomatic foundation from which the RoT directly originates, the next
two subsections provide additional reasons for designing posterior beliefs according to the
form P(ℓ, D, Π). First, we give an interpretation of the three components of the RoT. In
particular, we show that they directly address the three issues (EP), (EL) and (EC) via an
intuitive modularity result. Second, we recover some existing Bayesian methods as special
cases of the RoT. We discuss the meaning of a Bayesian method (not) being representable
via P(ℓ, D, Π) and use this as a springboard to motivate GVI.
26

Generalized Variational Inference
4.3 Modularity of the Rule of Three
Taking another look at the constituent parts of P(ℓ, D, Π), it becomes clear that each
component of the optimization problem serves a speciﬁc and separate purpose. In particular,
posteriors generated by the RoT have three ingredients.
(­L) A loss ℓ: Θ × X →R. The loss deﬁnes the parameter of interest θ by linking it to the
observations x1:n. Throughout, we make a number of assumptions on the loss. None of
these assumptions are required, but they signiﬁcantly simplify the presentation: Firstly,
we assume that the losses are additive and identical over all observations5. Secondly, we
assume that the loss depends on a parameter θ rather than a latent variable6. Thirdly,
we assume that the losses are deterministic and do not depend on unknown (local or
global) latent variables7.
(­P) A divergence D : P(Θ) × P(Θ) →R+ that imposes a cost for the posterior to deviate
too much from the prior π(θ). Recalling Remark 17 and speciﬁcally eq. (11), it is
clear that D determines how uncertainty about θ is quantiﬁed with the posterior.
Accordingly, we also call D the uncertainty quantiﬁer.
(­C) A set of feasible posteriors Π ⊆P(Θ) the objective speciﬁed by the RoT is minimized
over. The word “feasible” here is used in the optimization sense: As the form of
P(ℓ, D, Π) reveals, any q(θ) ∈Π is a feasible solution (i.e. posterior).
From this, it is clear that P(ℓ, D, Π) has a modular interpretation and decomposes into three
parts with distinct functions. Moreover, taking another glance at the problems (EP), (EL)
and (EC), observe that each of the arguments of P(ℓ, D, Π) addresses one of the concerns
raised in Section 3: Firstly, as the loss ℓdetermines the parameter, it can be used to tackle
model misspeciﬁcation and other violations of (L). Secondly—and assuming one has already
speciﬁed the best possible prior belief that is available and/or computationally feasible—the
uncertainty quantiﬁer D can be deployed to change the way in which priors inﬂuence inference,
directly tackling issues surrounding (P). Accordingly, changing D shifts the uncertainty
quantiﬁcation about θ. Thirdly, the space of potential posterior beliefs Π can be chosen in
such a way as to address the problems with (C): The more computational power is available,
the more complex Π can become. In fact, making this modularity conceptually more precise,
we arrive at the following result:
Theorem 18 (RoT modularity) Hold n, π(θ) and Π ﬁxed and take q∗
1(θ) ∈Π as a pos-
terior computed via P(ℓ, D, Π). If one wishes to derive an alternative posterior q∗
2(θ) ∈Π
through the RoT
(1) which avoids or is robust to model misspeciﬁcation, this amounts to changing ℓ.
5.
The losses are in fact not required to be identical and we can easily replace ℓ(θ, xi) by ℓi(θ, xi). For
example, one can set ℓi(θ, xi) = ℓ(θ, xi|x1:i−1). Here, the xi-th observation is conditioned on the ﬁrst i−1
observations as is common in time series models. More generally, any conditional dependence structure is
easily incorporated into the RoT at the expense of complicating notation, see also Knoblauch (2019a).
6.
Except in the experiments on Deep Gaussian Processes in Section 6. Here, the losses are in fact directly
deﬁned relative to latent variables.
7. While latent variable models are not the focus of the current paper, the RoT and GVI are easily extended
to the latent variable case, see Knoblauch (2019a).
27

Knoblauch, Jewson and Damoulas
(2) which is robust to prior misspeciﬁcation without changing the parameter of interest,
this amounts to changing D.
(3) which aﬀects uncertainty quantiﬁcation without changing the parameter of interest, this
amounts to changing D.
Remark 19 The proof can be found in the Appendix C. While it is easy to prove, it requires
carefully deﬁning robustness to model misspeciﬁcation as in Tukey (1960) and thus is somewhat
laborious.
Method
ℓ(θ, xi)
D
Π
Standard Bayes
−log p(xi|θ)
KLD
P(Θ)
Power Likelihood Bayes1
−log p(xi|θ)
1
w KLD, w < 1
P(Θ)
Composite Likelihood Bayes2
−wi log p(xi|θ)
KLD
P(Θ)
Divergence-based Bayes3
divergence-based ℓ
KLD
P(Θ)
PAC/Gibbs Bayes4
any ℓ
KLD
P(Θ)
VAE5,†
−log pζ(xi|θ)
KLD
Q
β-VAE6,†
−log pζ(xi|θ)
β · KLD, β > 1
Q
Bernoulli-VAE7,†
continuous Bernoulli
KLD
Q
Standard VI
−log p(xi|θ)
KLD
Q
Power VI8
−log p(xi|θ)
1
w KLD, w < 1
Q
Utility VI9
−log p(xi|θ) + log u(h, xi)
KLD
Q
Regularized Bayes10
−log p(xi|θ) + φ(θ, xi)
KLD
Q
Gibbs VI11
any ℓ
KLD
Q
Generalized VI
any ℓ
any D
Q
Table 1: Relationship of P(ℓ, D, Q) to a selection of existing methods. 1(e.g. Grünwald, 2011,
2012; Holmes and Walker, 2017; Grünwald and Van Ommen, 2017; Miller and Dunson, 2019),
2(e.g. Varin et al., 2011; Pauli et al., 2011; Ribatet et al., 2012; Hamelijnck et al., 2019),
3(e.g. Hooker and Vidyashankar, 2014; Ghosh and Basu, 2016; Futami et al., 2018; Jewson
et al., 2018; Chérief-Abdellatif and Alquier, 2019), 4(Bissiri et al., 2016; Germain et al., 2016;
Guedj, 2019), 5(Kingma and Welling, 2013), 6(Higgins et al., 2017), 7(Loaiza-Ganem and
Cunningham, 2019) 8(e.g. Yang et al., 2017; Huang et al., 2018) 9(e.g. Kuśmierczyk et al.,
2019; Lacoste-Julien et al., 2011) 10(Ganchev et al. (2010), but only if the regularizer can
be written as Eq(θ) [φ(θ, x)] as in Zhu et al. (2014)), 11(e.g. Alquier et al., 2016) †For the
VAE entries in the table, we abuse notation by denoting the local latent variable for xi as θ.
Further, ζ denote the generative parameters.
4.4 Connecting the Rule of Three to existing methods
Beyond axiomatic foundations and the interpretable modularity result of the last section,
the form P(ℓ, D, Π) also sheds light on existing methods. As we will see, most existing
28

Generalized Variational Inference
Bayesian methods are special cases of the RoT. However, certain posterior beliefs derived as
approximations to q∗
B(θ) are not. This section explains this distinction, how it is relevant and
why it provides a theoretical argument for the construction of posterior belief distributions
through Generalized Variational Inference (GVI).
As Table 1 shows, an impressive array of Bayesian methods are recovered by the RoT.
This includes a wide range of approxiate Bayesian methods and in particular standard VI.
In contrast, alternative approximation methods such as Laplace Approximations, Discrepancy
Variational Inference (DVI), Expectation Propagation (EP) or VI posteriors derived from
Generalized Evidence Lower Bounds do not satisfy these axiomatic desiderata.
4.4.1 Methods as special cases of the RoT
Unsurprisingly, as the RoT was constructed as a strict generalization of standard Bayesian
inference methods, most existing Bayesian methods are recovered as special cases. Further,
by virtue of incorporating the space Π into the objective, P(ℓ, D, Π) also recovers many
posterior beliefs that are constructed as approximations to the Bayesian posterior. Table 1
gives a non-exhaustive overview over some of belief distributions the RoT recovers as special
cases.
One of the key ﬁndings of the table is that standard VI satisﬁes the axiomatic foundations
underlying the RoT. In other words, the RoT does not judge the belief distribution of eq. (2)
derived from Bayes rule to be preferable to certain classes of approximations by default. The
reason for this is simple: Unlike the traditional Bayesian paradigm, the RoT can explicitly
encode the availability of ﬁnite or inﬁnite computational resources through the argument
Π. Hence, the moment the computational resources are scarce and posterior beliefs can
only be computed over a parameterized subset Q ⊂P(Θ), standard VI produces the best
computationally feasible posteriors relative to the objective in eq. 3. In this sense, the RoT
respects the optimality result of standard VI presented in Theorem 3.
4.4.2 Coherence and the RoT
Another insightful observation is that unlike previous generalizations such as the Generalized
Bayesian update in eq. (2), posterior belief distributions generated under the RoT are allowed
to break a property referred to as coherence or Bayesian additivity (e.g. Bissiri et al., 2016;
Fong and Holmes, 2019). In a nutshell, coherence says that posterior beliefs have to be
generated according to some function ψ : R2 →R which for the prior π(θ) and loss terms
ℓ(θ, x1), ℓ(θ, x2) behaves as
ψ (ℓ(θ, x2), ψ (ℓ(θ, x1), π(θ))) = ψ (ℓ(θ, x1), ℓ(θ, x2), π(θ)) .
Eﬀectively, this property ends up enforcing a multiplicative update rule and exponential
additivity as in eq. (2). Recalling the construction of the standard Bayesian posterior in
Section 3.1, it should be clear that the desirability of coherence is a direct result of assuming
(P) and (C). To see that this intuition is accurate, note that treating the prior belief according
to (P) and assuming inﬁnite computational power via (C) is exactly equivalent to setting
D = KLD and Π = P(Θ). Next, solving eq. (3) with these speciﬁcations as in the proof of
Theorem 1, one obtains the coherent exponentially additive update rule in eq. (2). In other
words, generating posteriors that can violate coherence is identical to generating posteriors
29

Knoblauch, Jewson and Damoulas
that do not have to rely on (P) and (C). Since this is precisely what we set out to do in the
ﬁrst place, this is in fact a desirable property for posteriors generated by the RoT.
4.4.3 Links to information theory and PAC-Bayes
It will be worthwhile exploring the interesting links between the RoT and alternative methods
for the construction of belief distributions in the future. Here, we simply note in passing
that special cases of the form P(ℓ, D, Π) were arrived at with strong theoretical arguments
starting from at least two completely diﬀerent paradigmatic bases.
PAC-Bayes:
While PAC-Bayesian results often have intimate links with Bayesian inference
(see e.g. Germain et al., 2016; Grünwald and Van Ommen, 2017), their motivations and origins
are distinct (see e.g. Shawe-Taylor and Williamson, 1997; Guedj, 2019): Unlike Bayesian
inference, PAC-Bayesian results are not constructed based on (P) and (L). Rather, their aim
is the derivation of generalization bounds for belief distributions q(θ) ∈P(Θ) deﬁned over
some hypothesis space (corresponding to the parameter space Θ) relative to a loss function
(corresponding to ℓ). For example, under a prior belief π(θ), a loss ℓand a data generating
mechanism for x1:n satisfying appropriate regularity conditions and for any q(θ) ∈P(Θ)
as well as for any ﬁxed value of ε > 0, a rescaled version of McAllester’s seminal bound
(McAllester, 1999a,b) says that with probability at least 1 −ε,
Eq(θ)
"
n · Ex1:n
"
1
n
n
X
i=1
ℓ(θ, xi)
##
≤Eq(θ)
" n
X
i=1
ℓ(θ, xi)
#
+
s
KLD(q, π) + log 2√n
ε
2n−0.5
.
Minimizing the right hand side of this bound with respect to q(θ) over some set Π ⊆P(Θ)
immediately recovers a special case for the RoT given by P(ℓ, DMcAllester, Π). Here, DMcAllester
is just the KLD term of the original bound with a subtracted constant:
DMcAllester(q∥π) =
s
KLD(q, π) + log 2√n
ε
2n−0.5
−
s
log 2√n
ε
2n−0.5
This is done to ensure that DMcAllester(q∥π) = 0 if and only if π(θ) = q(θ) (almost everywhere).
Note that it does not eﬀect the minimizer of the right hand side. A similar logic can be applied
to virtually all PAC-Bayesian bounds, including bounds based on alternative divergences
(Bégin et al., 2016; Alquier and Guedj, 2018).
This suggests that there are insightful
connections between PAC-Bayes and the RoT. Further, GVI—the tractable case when Π = Q
is a parameterized subset of P(Θ)—is a promising way forward to scale and operationalize
PAC-Bayesian learning. In fact, Letarte et al. (2019) constitutes the ﬁrst step in this direction
and we will explore these connections in future work.
Information Theory:
Another strong link exists between the RoT and results derived
from Information Theory and speciﬁcally the predictive Information Bottleneck (see Tishby
et al., 2000; Bialek et al., 2001; Alemi, 2019). The name arises from the fact that this
problem seeks to compress all information of the (possibly inﬁnitely many) observations in
xP into the ﬁnite-dimensional quantity θ which maximizes the amount of information about
(possibly inﬁnitely many) future observations denoted as xF . More concisely, the predictive
30

Generalized Variational Inference
Information Bottleneck is given by the inﬁnite-dimensional optimization problem
max
p(θ|xP ) I(θ; xF )
s.t.
I(θ; xP ) = I0.
Here, I denotes predictive information of some kind—typically mutual information—so that
I(θ; xF ) quantiﬁes the amount of information the quantity θ can tell us about the future
while I(θ; xP ) is a measure of its complexity. As shown in Alemi (2019), a straightforward
application of Lagrangian optimization together with a natural variational bound readily
transforms this problem into an optimization problem of the form P(ℓ, D, Π).
5. Generalized Variational Inference (GVI)
The next section ﬁnally introduces a version of the RoT that is feasible for real-world inference
and that we call Generalized Variational Inference (GVI). As the name suggests, GVI is any
posterior distribution generated from P(ℓ, D, Q), where Q is a parameterized subset of P(Θ)
as given in eq. (5). The remainder proceeds as follows:
Section 5.1 motivates why GVI procedures generate conceptually appealing and
coherent posterior belief distributions.
Section 5.2 focuses on the main applications of GVI in the current paper. In particular,
we explain how GVI can be used for inferences that (i) are robust to model misspeci-
ﬁcation, (ii) are robust to prior misspeciﬁcation and (iii) produce more appropriate
marginal distributions.
Section 5.3 discusses two strong theoretical guarantees for GVI: Frequentist consistency
and an interpretation as an approximate lower bound on the evidence of a (generalized)
Bayesian posterior.
Section 5.4 focuses on strategies for inference. First, we derive a closed form objective
for a particular set of GVI posteriors that are robust to model misspeciﬁcation. Second,
we introduce black box inference for GVI and prove more results on closed form
expressions.
5.1 Advantages of GVI over alternative VI methods
As Table 1 shows, posteriors derived via P(ℓ, D, Π) recover many Bayesian methods motivated
as approximations to the Bayesian posterior.
Crucially however, a large collection of
approximation strategies for the standard Bayesian posterior are not special cases of the
RoT. Unsurprisingly, these are the exact same methods that are suboptimal relative to the
objective deﬁning Bayesian inference (see Corollary 5). They encompass a variety of strategies
we elaborated on in Section 2.3.1 and 2.3.2. The most prominent of these include VI based
on generalized ELBO formulations, Laplace approximations, Expectation Propagation (EP)
as well as Discrepancy Variational Inference (DVI).
We stress that we do not claim that these alternative posterior approximations are
always performing worse than VI in practice—this is not what Corollary 5 says. What
we do claim and prove is that such alternative approximations do not provide the optimal
31

Knoblauch, Jewson and Damoulas
posterior relative to eq. (3)—an equation that is isomorphic with the Bayesian posterior.
This is an important distinction because it means that the empirical success of alternative
approximations has a theoretically appealing interpretation: As explained in some detail
in Section 2.4, if alternative approximations q∗
A(θ) to the Bayesian posterior q∗
B(θ) perform
better than the standard variational approximation q∗
VI(θ), the objective underlying q∗
A(θ)
must be targeting a more appropriate posterior belief. Thus, undesirable inference outcomes
with standard VI are synonymous with an inappropriately designed objective. Following
this line of reasoning, the most transparent way to improve poor performance of standard
VI posteriors is a direct adjustment of the objective that generated them. Conveniently,
the axiomatic development of Section 4 provides a solution of precisely this kind: The RoT,
which deﬁnes a versatile and modular family of objectives useful for interpretably adapting
the standard VI objective, a technique we call Generalized Variational Inference (GVI).
Deﬁnition 20 (Generalized Variational Inference (GVI)) Solving any RoT of form
P(ℓ, D, Q) for Q = {q(θ|κ) : κ ∈K} a parameterized subset of P(Θ) (also called a
variational family) constitutes a procedure we call Generalized Variational Inference ( GVI).
Remark 21 Notice that GVI posteriors satisfy Axioms I, II and III by deﬁnition. It
immediately follows that they also inherit the modularity result in Theorem 18. By the same
token, approximations q∗
A(θ) to the Bayesian posterior that cannot be rewritten as a GVI
procedure will violate the Axioms set out in Section 4.1. Clearly, this need not be a problem
if Q is a suﬃciently rich set of approximating distributions: In this case, the approximation
q∗
A(θ) ≈q∗
B(θ) is very reliable. As q∗
B(θ) itself satisﬁes the Axioms in Section 4.1, q∗
A(θ) will
not violate the Axioms in a practically meaningful way.
In practice however, Q is typically hopelessly restrictive. Consequently, it does not contain
any qualitatively good approximations to q∗
B(θ). In this case, violation of the Axioms is often
a serious problem. Example 4 and Figure 6 illustrate this on a Bayesian Mixture Model
( BMM). We also revisit this issue with our experiments in Section 6.1, where we observe its
real world consequence on Bayesian Neural Networks.
Example 4 (Label switching and multi-modality) A recurrent theme in the research
on variational approximations q∗
A(θ) to q∗
B(θ) is the observation that standard VI with a mean
ﬁeld normal family will center closely around the maximum likelihood estimate (e.g. Turner
and Sahani, 2011). This phenomenon is often referred to as the zero-forcing behaviour of
the KLD (Minka, 2005). Its eﬀect are undesirably overconﬁdent variational posteriors q∗
VI(θ).
Moreover, this problem is especially pronounced when the approximated posterior beliefs q∗
B(θ)
are multi-modal. Popular approaches to address this issue are Expectation Propagation ( EP)
(Minka, 2001; Opper and Winther, 2000) and Divergence Variational Inference ( DVI) methods
as introduced in Section 2.3.2 (e.g. Hernández-Lobato et al., 2016; Li and Turner, 2016; Dieng
et al., 2017). All of these approaches seek to (locally or globally) minimize an alternative zero-
avoiding divergence D between q∗
A(θ) and q∗
B(θ). Unfortunately, none of these approaches
satisfy the Axioms set out in 4.1. An immediate consequence is that the modularity encoded
by Theorem 18 does not apply to EP or DVI. This may seem inconsequential, but it really is
not: Unlike with GVI, changing the divergence in the DVI-sense no longer aﬀects
uncertainty quantiﬁcation alone. In other words, we may accidentally interfere with the
loss and warp the way the goodness of a parameter value θ is assessed in undesirable ways.
32

Generalized Variational Inference
Using Bayesian mixture models ( BMMs), we show that problems associated with violating
the Axioms or Section 4.1 indeed occur in practice. BMMs produce multi-modal posteriors
because the likelihood function is invariant to switching parameter labels. In other words,
BMMs have multiple parameter values that constitute equally good ﬁts to the data. With this
in mind, we simulate n = 100 observations from the model
p (x|θ = (µ1, µ2)) = 0.5 · N(x|µ1, 0.652) + 0.5 · N(x|µ2, 0.652)
for two diﬀerent settings of θ = (µ1, µ2). For inference, we use the well-speciﬁed prior
belief µj ∼N(0, 22), j = 1, 2. Using the correctly speciﬁed likelihood function ℓ(θ, xi) =
−log p (xi|θ = (µ1, µ2)), we then compare the standard Bayesian posterior q∗
B(θ), the standard
VI posterior, the DVI posterior based on Rényi’s α-divergence ( D(α)
AR) (Li and Turner, 2016)
and the GVI posterior using D = D(α)
AR as uncertainty quantiﬁer. For all posteriors produced
by variational methods, we use the mean ﬁeld normal family for Q.
Figure 6 shows the results. Clearly, q∗
B(θ) is multi-modal because there are two equally
good parameter values describing the data by virtue of the fact that p (x|θ = (µ1, µ2)) =
p (x|θ = (µ2, µ1)). Because we enforce uni-modality through Q, the variational posteriors
have a clear interpretation: Firstly, their means have the interpretation of (one of the two)
best parameter values of θ = (µ1, µ2). Secondly, their variances quantify the uncertainty
about this best value. For both settings of the true value for θ, DVI produces a posterior that
reﬂects a highly undesirable belief: In particular, the mode of the DVI posterior is located
at a (locally) worst value of θ. Unsurprisingly and as the bottom right plot shows, this has
adverse consequences for predictive performance. This behaviour is entirely attributable to
the fact that unlike GVI posteriors, DVI posteriors violate the axiomatic foundation set out
in Section 4.1 and thus do not inherit the modularity result of Theorem 18. In other words:
In the GVI framework, changing the KLD to another divergence only changes uncertainty
quantiﬁcation and does not aﬀect the way the best parameter is found. In sharp contrast,
the DVI framework comes with no such guarantee. Accordingly, posteriors produced with
DVI may conﬂate uncertainty quantiﬁcation and the way the best parameter is found. In
this context, Figure 6 serves as a morality tale: The modularity of standard VI and GVI
ensures transparency. In contrast, DVI methods have no such guarantees and can easily end
up conﬂating the search for the best parameter with uncertainty quantiﬁcation8.
5.2 GVI use cases: Robust inference & better marginals
Summarizing the ﬁndings of the last paragraphs, saying that the standard VI posterior
produces undesirable inferences or inappropriate uncertainty quantiﬁcation amounts to
saying that the objective in eq. (7) is inappropriate for the inference task at hand. By virtue
of the modularity result in Theorem 18, it is also clear that GVI is uniquely positioned to
deﬁne alternative objectives to address this in a direct and targeted fashion. Inspired by this,
we next focus on three situations that cause problems for standard VI, but are easily solved
by GVI: Prior misspeciﬁcaton (EP), model misspeciﬁcation (EL), and marginal variances.
8. Clearly, this problem is not guaranteed to occur: As Remark 21 explains, the conﬂation of loss and
uncertainty quantiﬁcation will not be a problem so long as Q is suﬃciently rich to produce approximations
that endow the statement q∗
A(θ) ≈q∗
B(θ) with meaning.
33

Knoblauch, Jewson and Damoulas
-0.5
0.0
0.5
1.0
0.0
1.0
2.0
3.0
µ1
Density
Exact Posterior
VI
F-VI, F = D(0.5)
AR
GVI, α = 0.25
MLE
-0.5
0.0
0.5
1.0
0.0
1.0
2.0
3.0
µ2
Density
-1
0
1
2
3
0.0
1.0
2.0
3.0
µ1
Density
-2
-1
0
1
2
3
4
0.0
0.1
0.2
0.3
0.4
0.5
x
Density
Figure 6: Best viewed in color. Depicted are the outcomes of inference in a BMM model,
namely the (multimodal) exact Bayesian inference posterior, standard VI, DVI with D(α)
AR
(Li and Turner, 2016) and GVI with D(α)
AR as uncertainty quantiﬁer. Top: Posterior marginals
for µ1 = 0, µ2 = 0.75. The Maximum A Posteriori estimate of the DVI posterior yields a
locally worst value for θ relative to the exact Bayesian posterior. In contrast, standard
VI and GVI respect the loss: They produce a posterior belief centered around one (of the
two) values of θ minimizing the loss. Bottom left: Posterior marginal for µ1 = 0, µ2 = 2.
The eﬀects of the top row become even stronger as the modes move further apart. Bottom
right: Posterior predictive for µ1 = 0, µ2 = 2 against the histogram depicting the actual
data. VI, GVI and exact Bayesian inference perform well and almost identically. DVI
performs poorly and captures none of the two mixture components of the BMM.
5.2.1 GVI and prior misspecification
Section 3.3 outlines the problems associated with violating (P): If the prior does not even
approximately reﬂect the best available judgement, inference outcomes are adversely aﬀected.
This phenomenon is particularly pronounced whenever the prior is speciﬁed according to
some default setting. For example, in the case of Bayesian Neural Networks (BNNs), a
typical choice of prior is a standard normal distribution that factorizes over the network
weights. While this may seem harmless or even uninformative, a supposedly uninformative
prior speciﬁcation of this kind actually encompasses a large degree of information, e.g.
34

Generalized Variational Inference
(U) The prior belief is unimodal. In other words, we believe that there exists a uniquely
most likely parameterization of the network before observing any data.
(I) The prior belief is that all network weights of a BNN are uncorrelated. In fact, we even
believe that all network weights of a BNN are both pairwise and mutually independent.9
The above implications are in direct and strong contradiction to our best possible judgements
about BNNs and thus violate (P):
(EU) Neural Networks are well-understood to have multiple parameter settings that are
equally good (e.g. Choromanska et al., 2015). The unimodality assumption outlined in
(U) is thus clearly not a reﬂection of the best judgement available: A prior belief in
accordance with (P) would encode multimodality.
(EI) By construction, Neural Networks encode a signiﬁcant degree of dependence in their
parameters: The best values for parameters in the l-th layer will strongly depend on
the best values for parameters in the (l −1)-th layer (and vice versa). Hence, assuming
uncorrelatedness (much less so independence!) directly contradicts our best judgement.
From this, it is obvious that a fully factorized normal distribution is hardly an appropriate
default prior for BNNs in the sense of (P) in Section 3.1. At the same time, it is often
prohibitive or computationally infeasible to construct alternative prior beliefs that reﬂect
our best judgements more accurately. In other words, we are stuck with a sub-optimal prior.
Under the standard Bayesian paradigm, this is not an acceptable position. In contrast, the
new paradigm outlined in Section 4.1 does not require the prior to be ﬂawless. Using GVI,
we can thus use our very imperfect prior to design more appropriate posterior beliefs: Simply
adapt the argument D which regularizes the posterior belief against the prior. In particular,
we want to adapt D such that the resulting posteriors satisfy two criteria: Firstly, they
should be more robust to priors which strongly contradict the observed data. Secondly, they
should still provide reliable uncertainty quantiﬁcation.
As the toy example in Figure 3 shows, this can be achieved by picking a robust replacement
Drobust for the KLD. A recent overview of robust divergences was provided by Cichocki and
Amari (2010). Such divergences are constructed with a hyperparameter that regulates
the degree of robustness and which recovers the KLD as a limiting case. In the current
paper, all robust divergences are parameterized such that one recovers the KLD as the
hyperparameter approaches unity. For example, Rényi’s α-divergence—henceforth denoted
D(α)
AR and introduced by Rényi (1961)—is such a robust alternative to the KLD. Using the
parameterization in Cichocki and Amari (2010), it is given by
D(α)
AR(q∥π) =
1
α(α −1) log
 
Eq(θ)
"π(θ)
q(θ)
1−α#!
.
(12)
Originally, Rényi’s α-divergence was motivated as the geometric mean information to dis-
criminate between the two hypotheses θ ∼π(θ) and θ ∼q(θ) of order α, for some α ∈(0, 1).
Similarly, the original motivations for the KLD was its interpretation as the arithmetic mean
information to discriminate between θ ∼π(θ) and θ ∼q(θ) (Kullback and Leibler, 1951).
9. For joint normal distributions, variables are uncorrelated if and only if they are independent.
35

Knoblauch, Jewson and Damoulas
Since geometric means are more robust measures of central tendency than arithmetic
means, the D(α)
AR will generally be a more robust measure of discrepancy between π(θ)
and q(θ) so long as α ∈(0, 1). Clearly, the degree of this robustness will depend on α.
Generally, picking lower values of α ∈(0, 1) will produce more prior-robust measures of
discrepancy. Indeed, D(α)
AR(q∥π) recovers KLD(q∥π) as α →1 and KLD(π∥q) as α →0. A
host of other divergences behave similarly, including α-, β- and γ-divergences as well as
their generalizations (Cichocki and Amari, 2010). We plot their magnitude for two Normal
Inverse Gamma distributions with diﬀerent divergence hyperparameter values in Figure
7. The plot illustrates that (i) hyperparameter values below (above) unity impose larger
(smaller) penalties than the KLD and that (ii) all robust divergences recover the KLD as
their hyperparameters approach one. Importantly, a larger regularization term does not
necessarily imply that a misspeciﬁed prior dominates inference: In fact, Figure 3 shows that
the contrary holds using the example of D(α)
AR with α = 0.5. As Appendix B demonstrates
in great detail, Rényi’s α-divergence is representative of the behaviour displayed by various
robust divergences. To keep things simple, we thus focus on D(α)
AR in the remainder of the
paper.
5.2.2 GVI and model misspecification
Section 3.4 explains how and why (EL) can severely impede the usefulness of the Bayesian
posterior: Assuming that the likelihood is an accurate reﬂection of the data generating
mechanism makes inferences susceptible to outliers, heterogeneity and other adversarial
aspects of the data. Further recalling the isomorphism between eq. (3) and eq. (8), it also is
0.00
0.25
0.50
0.75
1.00
1.25
1.50
1.75
2.00
hyperparameter value
0
10
20
30
40
50
magnitude of D(q| )
D =
-divergence (DB)
D = -divergence (DG)
D = Renyi's -divergence (D( )
AR)
KLD
Figure 7: Depicted is the magnitude D(q∥π) for diﬀerent robust divergences D and the
KLD for two Normal Inverse Gamma distributions given by q(θ) = NI−1(θ; µq, Vq, aq, bq)
and π(θ) = NI−1(θ; µπ, Vπ, aπ, bπ) with µπ = (0, 0)T , Vπ = 25 · I2, aπ = 500, bπ = 500 and
µq = (2.5, 2.5)T , Vq = 0.3 · I2, aq = 512, bq = 543.
36

Generalized Variational Inference
clear that treating the likelihood model as (approximately) correct is synonymous to using
the log score ℓ(θ, xi) = −log p(xi|θ) to assess how well the model ﬁts the data.
Based on this observation, a promising line of work has sought to produce more robust
scoring rules for probability models (see e.g. Dawid et al., 2016, and references therein).
While there are other ways to derive proper scoring rules, the conceptually most appealing
constructions are arguably based on statistical divergences.
The intuition behind this
approach consists in two observations: First, if n is large enough, one can invoke the law of
large numbers and rewrite the parameter value bθn minimizing the log score as
bθn =
min
θ∈Θ
(
1
n
n
X
i=1
−log p(xi|θ)
)
n→∞
≈
min
θ∈Θ {EPx [−log p(x|θ)]}
= min
θ∈Θ

EPx

−log
 p(x|θ)
dPx(x)

=
min
θ∈Θ
KLD (dPx∥p(x|θ)) .
In other words, the log score targets the parameter value θ which is best in the KLD-sense.
Second, as it is well-known that the KLD is not robust (e.g. Cichocki and Amari, 2010), it is
often advantageous to ﬁnd a robust alternative divergence Drobust to reverse-engineer the
above derivation and arrive at some robust scoring rule Lrobust(θ, xi).
The ﬁrst developments in this direction started with the family of β-divergences by
Basu et al. (1998); Mihoko and Eguchi (2002), but the idea has since been extended to
various other discrepancies. This includes the Fisher divergence (Hyvärinen, 2005), the family
of γ-divergences (e.g. Fujisawa and Eguchi, 2008; Hung et al., 2018) as well as Minimum
Stein Discrepancies (Barp et al., 2019). The Generalized Bayesian posterior associated with
this procedure then arises from replacing the negative log likelihood in eqs. (2) and (3)
by Lrobust(θ, xi). A growing literature has focused on using the scoring rules derived from
these divergences to perform Generalized Bayesian inference of precisely this sort (see e.g.
Hooker and Vidyashankar, 2014; Ghosh and Basu, 2016; Chérief-Abdellatif and Alquier,
2019). Jewson et al. (2018) provides a recent survey of this ﬁeld and connects it to the
idea of Bissiri et al. (2016). As Figures 4 and 5 demonstrate for the scoring rule Lβ(θ, xi)
derived from the β-divergence, this produces reliably robust posterior inferences. We state
the analytic form of Lβ(θ, xi) and another robust scoring rule Lγ(θ, xi) derived from the
γ-divergence (see Hung et al., 2018) in Section 6.2, where we show how they can be used to
robustify Deep Gaussian Processes.
5.2.3 GVI and marginal variances
The standard VI objective can be inappropriate even in situations where assuming appropri-
ately speciﬁed priors (P) and likelihood functions (L) underlying the traditional Bayesian
paradigm are a useful working assumption. For instance, the uncertainty quantiﬁcation of
standard VI is often inappropriate when Q is a mean ﬁeld variational family factorizing
dimension-wide over θ and the individual entries of θ exhibit strong dependence (e.g. Turner
and Sahani, 2011). Oftentimes, this phenomenon is referred to as mode seeking behaviour
(e.g. Minka, 2005). The name itself also reveals that this problem is intimately linked to
unimodal—and thus in practice mean ﬁeld normal—variational families Q. Again, the
modularity result of Theorem 18 can be helpful: Provided that one has no ﬂexibility about
the choice of Q and has ﬁxed the prior π(θ), one can adapt the GVI posterior’s uncertainty
37

Knoblauch, Jewson and Damoulas
0
1
2
3
4
5
1
0.00
0.25
0.50
0.75
1.00
1.25
1.50
Density
D = Renyi's -divergence (D( )
AR)
Exact posterior
= 1.25
Standard VI
= 0.5
= 0.025
MLE
0
1
2
3
4
5
1
D = -divergence (DG)
Exact posterior
= 1.5
Standard VI
= 0.75
= 0.15
MLE
0
1
2
3
4
5
1
D =
-divergence (DB)
Exact posterior
= 1.5
Standard VI
= 0.75
= 0.5
MLE
Figure 8: Best viewed in color. Marginal VI compared to diﬀerent GVI posteriors for the
coeﬃcient θ1 of data simulated from a Bayesian linear model (see Appendix B for details).
For all posteriors, the loss ℓis the correctly speciﬁed negative log likelihood of the true data
generating mechanism. Further, for all variational posteriors the belief is constrained to lie
inside a mean ﬁeld normal family Q. Due to high correlation between the coeﬃcients for the
exact posterior, standard VI produces undesirably over-concentrated belief distributions.
In contrast, appropriately choosing the hyperparameters of alternative robust divergences
D ̸= KLD provides more desirable uncertainty quantiﬁcation.
quantiﬁcation properties by changing D = KLD to an alternative divergence. Figure 8
illustrates this ﬂexibility of GVI by comparing the uncertainty quantiﬁcation properties of
three diﬀerent robust divergences to standard VI.
5.3 Theoretical properties of GVI
Clearly, the principal appeal of GVI lies in its modularity and the associated subjective choices
of ℓ, D and Q. Because of this increased need for the statistical modeller to provide sensible
problem speciﬁcation, one may worry about producing inferences that are non-sensical. The
following section studies two theoretical ﬁndings that impose meaningful limits to the damage
a badly speciﬁed GVI posterior can do: Firstly, we point to novel results showing that GVI
posteriors are consistent in the frequentist sense: As more and more data are observed,
the GVI posterior will collapse to the population optimum regardless of D. Secondly, we
show that GVI posteriors with uncertainty quantiﬁers based on robust divergences can be
interpreted as approximations to Bayesian posteriors with a power likelihood.
5.3.1 Frequentist consistency
Knoblauch (2019a) shows that GVI posteriors are consistent in the Frequentist sense. In
other words, they collapse to a point mass at the population-optimal value as the number of
observations tends to inﬁnity. This holds under a wide range of extremely mild regularity
conditions on the arguments ℓ, D and Q. Here, we state a simple version of the result for
independent data with the mean ﬁeld normal variational family.
38

Generalized Variational Inference
KLD
D( )
AR, 
= 0.5
JD
Reverse KLD
FD
10
8
6
4
2
0
2
qGVI(
true
1
1)
n=10
KLD
D( )
AR, 
= 0.5
JD
Reverse KLD
FD
n=25
KLD
D( )
AR, 
= 0.5
JD
Reverse KLD
FD
n=50
KLD
D( )
AR, 
= 0.5
JD
Reverse KLD
FD
n=500
KLD
D( )
AR, 
= 0.5
JD
Reverse KLD
FD
n=2500
KLD
D( )
AR, 
= 0.5
JD
Reverse KLD
FD
n=25000
 
 
 
 
 
 
 
Figure 9: Best viewed in color. Marginal VI and diﬀerent GVI posteriors for the ﬁrst
coeﬃcient of a simulated 20-dimensional Bayesian Linear Model based on n observations.
The loss ℓis the correctly speciﬁed negative log likelihood of the true data generating
mechanism and the uncertainty quantiﬁer is varied along the x-axis. Depicted uncertainty
quantiﬁers are the forward and reverse KLD, Rényi’s α-divergence (D(α)
AR), Jeﬀrey’s Divergence
(JD) as well as the Fisher Divergence (FD). All posteriors are members of the mean ﬁeld
normal family Q. Because all inferred posterior beliefs are normals, dots are used to mark out
the posterior mean and whiskers to denote the posterior standard deviation. All posteriors
are re-centered around the true value of the coeﬃcient, so that the y-axis shows how far the
posterior belief is from the truth.
Theorem 22 (Frequentist consistency of GVI) Suppose that Assumption 1 in Knoblauch
(2019a) holds. Choosing Q to be the mean ﬁeld normal family and letting D be lower-semi-
continuous in its ﬁrst argument, suppose that D(q∥π) < ∞for all q ∈Q. Further, let Px be
the true probability measure of some random variable x and suppose that the observations
x1:n are independent and identically distributed draws from x. If the prior is not inﬁnitely
bad for the population of x (which is to say that Eπ [EPx [ℓ(θ, x)]] < ∞), then
q∗
GVI,n(θ)
D
−→δθ∗(θ),
where q∗
GVI,n(θ) is the GVI posterior corresponding to the problem P(ℓ, D, Q) based on n
observations and θ∗= arg minθ [EPx [ℓ(θ, x)]] is the population-optimal parameter value.
Remark 23 This Theorem is a straightforward invocation of Corollary 1 in Knoblauch
(2019a). Assumption 1 guarantees a number of conditions that are required to make GVI a
well-deﬁned optimization problem. For example, it ensures that the sum of the losses has
minimizers for any ﬁnite n and in the large data limit and that the loss expected under Px is
ﬁnite.
This ﬁnding is illustrated in Figure 9, which is taken from Knoblauch (2019a). The plot
shows that as the theory suggests, the posteriors collapse to a point mass under mild
regularity conditions on the uncertainty quanitﬁer D. Unsurprisingly, speed and nature of
the convergence depends on the choice of D.
39

Knoblauch, Jewson and Damoulas
5.3.2 GVI as a posterior approximation
Although the axiomatic development in Section 4.1 shows that GVI produces a posterior
belief distribution that is valid in its own right, one can also interpret certain GVI posteriors
as approximations to (generalized) Bayesian posteriors as in eq. (2). In particular, we
show that for robust divergences D(ρ)
robust parameterized by some hyperparameter ρ so that
limρ→1 D(ρ)
robust = KLD, the GVI objective constitutes a lower bound on the evidence of a
(generalized) Bayesian posterior. Results of this form can be shown to hold for Rényi’s
α-divergence (D(α)
AR), the γ-divergence (D(γ)
G ) as well as the β-divergence (D(β)
B ). As they are
structurally similar, we only state the bound corresponding to D = D(α)
AR and defer the results
for D(γ)
G and D(β)
B
as well as all proofs to Appendix D.
Theorem 24 (GVI as approximate Evidence Lower bound with D = D(α)
AR) The ob-
jective of a GVI posterior based on P(ℓ, D(α)
AR, Q) has an interpretation as lower bound on the
c(α)-scaled (generalized) evidence lower bound of P(w(α) · ℓ, KLD, P(Θ)):
Eq(θ)
" n
X
i=1
ℓ(θ, xi)
#
+ D(α)
AR(q||π) ≥−c(α) · ELBOw(α)ℓ(q) + S1(α, q, π)
(13)
where ELBOw(α)ℓdenotes the Evidence Lower Bound associated with standard VI relative to
the generalized Bayesian posterior given by
qw(α)ℓ
B
(θ) ∝π(θ) exp
 
−w(α)
n
X
i=1
ℓ(θ, xi)
!
,
where c(α) = min{1, α−1}, w(α) = max{1, α} and S1(α, q, π) is an interpretable slack term.
Remark 25 The take-away from the bound in eq. (13) is that the slack term S1(α, q, π)
introduces the main diﬀerence between P(ℓ, D(α)
AR, Q) and P(w(α) · ℓ, KLD, Q). As Appendix
D shows, is possible but rather tedious to make analytically more concise statements about
S1(α, q, π). Speciﬁcally, the main function of the slack term is the introduction of more
conservative uncertainty quantiﬁcation. Studying this empirically reveals that for Rényi’s
α-divergence, this has the eﬀect of enabling prior robust inference. This point is demonstrated
in Figure 3 and elaborated upon in Section 5.2.1, but is perhaps best summarized in Figure 10:
Since c(α) = 1 for α ∈(0, 1), the left hand side of the plot corresponds to P(ℓ, D(α)
AR, Q) and the
right-hand side to P(w(α) · ℓ, KLD, Q). The plot shows the diﬀerence between minimizing the
GVI and the ELBO objectives in eq. (13): relative to the ELBO objective P(w(α) · ℓ, KLD, Q),
the belief distributions P(ℓ, D(α)
AR, Q) based on the D(α)
AR are much less sensitive to badly speciﬁed
priors. In fact, the GVI posteriors implicitly determine if it is worth taking the prior into
account: On the one hand, the uncertainty quantiﬁcation with D = D(α)
AR is less aﬀected
than standard VI for badly speciﬁed priors. On the other hand, GVI posteriors with D = D(α)
AR
are still more conservative than standard VI for well speciﬁed priors.
40

Generalized Variational Inference
1
0
1
2
3
4
5
1
0.0
0.2
0.4
0.6
0.8
1.0
1.2
Density
Standard VI with  D = KLD
= 3.0
= -5.0
= -20.0
= -50.0
MLE
1
0
1
2
3
4
5
1
D = Renyi's -divergence (D( )
AR) 
= 0.75
= 3.0
= -5.0
= -20.0
= -50.0
MLE
1
0
1
2
3
4
5
1
D = Renyi's -divergence (D( )
AR) 
= 0.5
= 3.0
= -5.0
= -20.0
= -50.0
MLE
Figure 10: Best viewed in color. Marginal VI compared to diﬀerent GVI posteriors for the
coeﬃcient θ1 of data simulated from a d-dimensional Bayesian linear model with diﬀerent
priors (see Appendix B for details). The prior for the coeﬃcients is a Normal Inverse Gamma
distribution given by µ ∼NI−1(µπ · 1d, vπ · Id, aπ, bπ) with vπ = 4 · Id, aπ = 3, bπ = 5
and various values for µπ. For all posteriors, the loss ℓis the correctly speciﬁed negative
log likelihood of the true data generating mechanism. Further, all variational posteriors
are constrained to lie inside a mean ﬁeld normal family Q. Notice that the standard VI
posterior corresponds to the ELBO component on the right hand side of the bound in eq.
(13). In contrast, the GVI posteriors are obtained by maximizing the left hand side of the
same bound.
5.4 Inference with Generalized Variational Inference (GVI)
This section outlines two powerful inference strategies for GVI: Quasi-conjugate and fully
black box inference. Built on earlier ﬁndings in Knoblauch et al. (2018), we show that a
class of GVI posteriors based on robust likelihood scoring rules admits closed form objectives.
Because this closed form objective emerges when the non-robust counterpart of the likelihood
is conjugate to the prior, we call the resulting inference procedure quasi-conjugate. For more
complicated models, closed form objectives will not be available. To address this, we also
introduce a black box inference procedure for arbitrary choices of ℓand D.
5.4.1 Quasi-conjugate inference
This paper so far has given little attention to specifying the constraining family for a GVI
problem. One interesting interdependence between loss function and variational family was
studied in Knoblauch et al. (2018): When applying the robust scoring rule Lβ (Basu et al.,
1998) derived from the β-divergence (D(β)
B ) to a likelihood associated with a conjgate prior
π(θ|κ0), there is a considerable advantage in taking Q to be the family of the conjugate prior.
Since Lβ(θ, xi) →−log p(xi|θ) as β →1, the (generalized) Bayesian posterior
qβ
B(θ) ∝π(θ)
n
Y
i=1
exp
n
−Lβ(θ, xi)
o
41

Knoblauch, Jewson and Damoulas
is contained in Q as β →1. The results in Knoblauch et al. (2018) and intuition thus
suggest that so long as |β −1| < ε for some suﬃciently small value ε > 0 and D = KLD,
constraining the posterior to be in Q still produces excellent approximations to qβ
B(θ). Beyond
the approximation quality, choosing the quasi-conjugate variational family oﬀers tangible
computational advantages: As Theorem 2 in Knoblauch et al. (2018) shows, under these
circumstances the objective deﬁning P(Lβ, KLD, Q) is available in closed form. Consequently,
no sampling or approximation is required and the optimum is usually found within a very
small number of iterations.
Complementing this ﬁnding, Proposition 26 extends quasi-conjugacy to the robust scoring
rule Lγ derived from the γ-divergence (D(γ)
G ) as provided for in Hung et al. (2018). Similarly
to Lβ, Lγ(θ, xi) →−log p(xi|θ) as γ →1, so that the same intuition that applied to Lβ also
applies here. Note that the conditions for Lγ in Proposition 26 are slightly more restrictive
than those derived for Lβ. This is due to the fact that while the same integral term appears
in both, it is additive for Lβ but multiplicative for Lγ. While the proof is conceptually
straightforward, it is notationally cumbersome and thus deferred to Appendix E.
Proposition 26 (Closed form GVI objectives with Lγ) Let Lγ(θ, ·) be the γ-divergence
based scoring rule for likelihood p(·|θ). Suppose p(·|θ) admits conjugacy relative to the expo-
nential distributions given by Q and let the conjugate prior π(θ|κ0) ∈Q. Writing
p(x|θ) = h(x) exp

g(x)T T(θ) −B(x)
	
,
q(θ|κ) = h(θ) exp

η(κ)T T(θ) −A(η(κ))
	
,
N = {κ : exp{A(η(κ))} < ∞} ,
the objective of P(Lγ, KLD, Q) has closed form if for observations x1:n and all q ∈Q
I(γ)(θ) =
Z
X
p(x|θ)γdx,
F1(κ) =
Z
Θ
T(θ)q(θ|κ)dθ,
F2(κ) =
Z
Θ
I(γ)(θ)
1−γ
γ q(θ|κ)dθ
are closed form functions of θ and κ and if for all xi, (η(κ) + (γ −1)g(xi)) ∈N.
5.4.2 Additional details on Black-Box GVI (BBGVI)
Standard VI is scalable using doubly stochastic, model-agnostic optimization techniques
(e.g. Paisley et al., 2012; Hoﬀman et al., 2013; Titsias and Lázaro-Gredilla, 2014; Salimans
and Knowles, 2014; Wu et al., 2019) collectively known as black box VI (Ranganath et al.,
2014). We extend these methods to black box GVI (BBGVI), an inference algorithm directly
inheriting the modularity of the posteriors deﬁned by P(ℓ, D, Q). This makes it easy to
build BBGVI into existing software: For example, adapting the Deep Gaussian Process
implementation of Salimbeni and Deisenroth (2017) required <100 lines of Python code.
Suppose Q = {q(θ|κ) : κ ∈K} and that for all (κ, θ) ∈(K, Θ), one can sample
θ ∼q(θ|κ). Suppose also that the derivatives ∇κ log(q(θ|κ)) and ∇κD(q||π) exist (q-almost
surely). For many choices of D, Q and π, ∇κD(q||π) is available in closed form. In this case,
BBGVI is particularly attractive and GVI posteriors can be computed through an unbiased
gradient estimate given as
∇κ ˆL(q|ℓ, D, Q) = 1
S
S
X
s=1
( n
X
i=1
ℓ(θ(s), xi) · ∇κ log(q(θ(s)|κ))
)
+ ∇κD(q||π)
(14)
42

Generalized Variational Inference
and relying on an independent sample θ(1:S) i.i.d
∼q(θ|κ). If a closed form for ∇κD(q||π) is
not available but D(q||π) = Eq(θ|κ)

ℓD
κ,π(θ)

for a function ℓD
κ,π : Θ →R, one can use the
alternative unbiased gradient estimate
∇κ ˆL(q|ℓ, D, Q) = 1
S
S
X
s=1
(" n
X
i=1
ℓ(θ(s), xi) + ℓD
κ,π(θ(s))
#
· ∇κ log(q(θ(s)|κ)) + ∇κℓD
κ,π(θ(s))
)
.(15)
This can be deployed for most divergences of interest, including the family of f-divergences.
In some cases however, divergences will not be linear in the argument θ so that one has
D(q∥π) = τ
 Eq(θ|κ)

ℓD
κ,π(θ)

for some non-linear function τ : R →R. In this case, BBGVI
can be performed based on the biased gradient estimate
∇κ ˆL(q|ℓ, D, Q) = 1
S
S
X
s=1
( n
X
i=1
ℓ(θ(s), xi) · ∇κ log(q(θ(s)|κ))
)
+
τ
 
1
S
S
X
s=1
ℓD
κ,π(θ(s))
!
· 1
S
S
X
s=1
∇κℓD
κ,π(θ(s)).
(16)
Any gradient form admits black box variance reduction through some control variate h
(Ranganath et al., 2014; Wu et al., 2019), see Appendix F for details. Algorithm 1 summarizes
a generic BBGVI procedure.
As gradient estimates based on eq. (14) will have strictly lower variance than estimates
based on eq. (15) or eq. (16), one may wonder under which conditions closed forms for
∇κD(q∥π) are available. Proposition 27 clariﬁes this for robust divergences.
Proposition 27 (Closed form D) Let q, π with natural parameters ηq, ηπ be in the expo-
nential family Q = {q(θ|η) = h(θ) exp {η′T(θ) −A(η)} : η ∈N} with natural parameter
space N = {η : exp{A(η)} < ∞}. Then,
(1) D(α)
A (q||π) and D(α)
AR(q||π) have a closed form if α ∈(0, 1) or if αηq + (1 −α)ηπ ∈N
(2) D(β)
B (q||π) has a closed form if h(θ) = h does not depend on θ and additionally,
(β −1) · η1 + η2 ∈N for any η1, η2 ∈N (amongst others, this holds for Beta, Gamma,
Gaussian, exponential or Laplace distributions)
(3) D(γ)
G (q||π) has closed form if D(β)
B (q||π) does for β = γ.
6. Experiments
Having introduced an inference strategy that is generic enough to work on high-dimensional,
black box Bayesian models, the remainder of the paper studies GVI on two applications of
interest in Bayesian Deep Learning. Before doing so, notice that as indicated in Table 1,
previous work constitutes various interesting special cases of GVI with other strong empirical
results (e.g., Futami et al., 2018; Knoblauch et al., 2018; Higgins et al., 2017; Chérief-
Abdellatif and Alquier, 2019; Jankowiak et al., 2019) We add to this body of evidence by
deploying GVI on Bayesian Neural Networks (BNNs) and Deep Gaussian Processes (DGPs) to
address the particular ways in which these two models challenge the assumptions underlying
the standard Bayesian paradigm. All code used for generating the experiments is available
from https://github.com/JeremiasKnoblauch/GVIPublic.
43

Knoblauch, Jewson and Damoulas
Algorithm 1 Black box GVI (BBGVI)
Input: x1:n, π, D, ℓ, Q, h, StoppingCriterion, κ0, K, S, t = 0, LearningRate
done ←False
while not done do
// STEP 1: Get a subsample from x1:n of size K
ρ1:K ←SampleWithoutReplacement(1 : n, K)
x(t)1:K ←xρ1:K
// STEP 2: Sample from q(θ|κt) and compute losses
θ(1:S) i.i.d.
∼q(θ|κt)
ℓi,s ←ℓ(θ(s), x(t)i) · ∇κt log q(θ(s)|κt) for all s = 1, 2, . . . S and i = 1, 2, . . . , K
ℓs ←n
K
PK
i=1 ℓi,s for all s = 1, 2, . . . S
// STEP 3: Compute uncertainty quantiﬁer
if D(q∥π) admits closed form then
ℓs ←ℓs + ∇κD(q∥π) for all s = 1, 2, . . . S
else if D(q∥π) = Eq[ℓD
κ,π(θ)] then
ℓs ←ℓs + ℓD
κ,π(θ(s))∇κt log q(θ(s)|κt) + ∇κtℓD
κt,π(θ(s)) for all s = 1, 2, . . . S
else if D(q∥π) = τ
 Eq[ℓD
κ,π(θ)]

then
ℓs ←ℓs + τ

1
S
PS
s=1 ℓD
κt,π(θ(s))

· ∇κtℓD
κt,π(θ(s)) for all s = 1, 2, . . . S
// STEP 4: Apply variance reduction via h if desired
if h ̸= None then
hs ←h(θ(s), ℓs)
ℓs ←ℓs −hs for all for all s = 1, 2, . . . S
// STEP 5: Update κt and stopping criterion
ρt ←LearningRate(t)
L ←1
S
PS
s=1 ℓs
κt+1 ←κt + ρt · L
done ←StoppingCriterion(κt+1, κt, t)
t ←t + 1
6.1 Bayesian Neural Network Regression
As alluded to in Example 1 and Section 5.2.1, BNN models should be expected to suﬀer from
prior misspeciﬁcation. Focusing on the regression case, we wish to alleviate this problem
using GVI’s modularity and thus focus on varying D. Accordingly, we ﬁx the loss function
44

Generalized Variational Inference
to the usual negative log likelihood ℓ(θ, yi, xi, σ2) = −log pN (yi|xi, σ2, F(θ)) for
pN (yi|xi, σ2, F(θ)) = N(yi|F(θ), xi, σ2),
and choose Q = QMFN as the normal mean ﬁeld variational family given in eq. (9). With
this in hand, we compare three diﬀerent constructions of posterior beliefs:
(1) Standard VI as described in Section 2.3;
(2) DVI methods introduced as approximations to the standard Bayesian posterior q∗
B(θ)
that ﬁnd q∗
A(θ) = arg minq∈Q D(q∥q∗
B(θ)) with D being the α-divergence (Hernández-
Lobato et al., 2016)10 and Rényi’s α-divergence (Li and Turner, 2016);
(3) GVI with D = D(α)
AR.
To make comparisons as fair as possible, our implementation is built on top of that used
for the results of Li and Turner (2016) and only changes the objective being optimized.
Similarly, all settings and data sets for which the methods are compared are unchanged
and taken directly from Li and Turner (2016) and Hernández-Lobato et al. (2016): We
use a single-layer network with 50 ReLU nodes on all experiments. Inference is performed
via probabilistic back-propagation (Hernández-Lobato and Adams, 2015) and the ADAM
optimizer (Kingma and Ba, 2014) with its default settings, 500 epochs and a batch size of
32. Priors and variational posteriors are both fully factorized normal distributions. Further,
the results are also evaluated on the same selection of UCI data sets (Lichman, 2013) and
in the same way as they were in Li and Turner (2016) and Hernández-Lobato et al. (2016):
Using 50 random splits of the relevant data into training (90%) and test (10%) sets, the
inferred models are evaluated predictively on the test sets using the average negative log
likelihood (NLL) as well as the average root mean square error (RMSE). For each of the 50
splits, predictions are computed based on 100 samples from the variational posterior.
We summarize the two main results of our experiments as follows: First, Figure 11
depicts what appears to be the most typical relationship between VI, DVI and GVI on
BNNs. Second, Figure 12 explores a surprising ﬁnding about the typical relationship further
and connects it back to the modularity result in Theorem 18. The Appendix contains a small
number of further results.
6.1.1 Typical patterns (Figure 11)
As Figure 11 demonstrates, several ﬁndings form a consistent pattern across a range of data
sets. Three ﬁndings are most poignant.
(A) DVI can often achieve a performance gain for the NLL relative to standard VI, but
much less so for RMSE. On both metrics, there is no clear pattern of improvement.
(B) Relative to standard VI, GVI signiﬁcantly improves performance for both NLL and
RMSE if α > 1. Conversely, GVI worsens performance if α ∈(0, 1). In other words,
larger posterior variances adversely aﬀect predictive quality.
10. We align the parameterization of the D(α)
A
with the current paper, meaning 1−αcurrent = αH.-L. et al. (2016)
45

Knoblauch, Jewson and Damoulas
4.00
4.05
4.10
D(0.5)
AR
KLD (VI)
D(1.5)
AR
D(2.0)
AR
D(2.5)
AR
D(0.5)
AR
D(0.5)
A
D(0.0)
A
DVI
GVI
power
1.2
1.4
1.6
energy
3.0
3.2
3.4
boston
5.00
5.25
5.50
5.75
concrete
0.8
1.0
1.2
yacht
2.80
2.82
D(0.5)
AR
KLD (VI)
D(1.5)
AR
D(2.0)
AR
D(2.5)
AR
D(0.5)
AR
D(0.5)
A
D(0.0)
A
DVI
GVI
1.6
1.8
2.0
2.5
2.6
2.7
3.05
3.10
3.15
3.20
1.7
1.8
1.9
Figure 11: Best viewed in color. Top row depicts RMSE, bottom row the NLL across a range
of data sets using BNNs. Dots correspond to means, whiskers to standard errors. The further
to the left, the better the predictive performance. For the depicted selection of data sets,
a clear common pattern exists for the performance diﬀerences between standard VI, DVI
and GVI.
(C) GVI performance is a clear banana-shaped function of α across all data sets: While
predictive performance beneﬁts as α gets larger than one, the improvement ﬂattens
out and bends back in a banana shape as α grows too large.
Finding (B) has a straightforward interpretation: Since it holds that D(α)
AR ≤KLD for α > 1
(see Van Erven and Harremos (2014)11 and Figure 7), the GVI posteriors associated with D(α)
AR
for α > 1 are more concentrated than the standard VI posteriors, a phenomenon also depicted
on toy models in Figure 8. In other words: Ignoring more of the poorly speciﬁed prior and
consequently being closer to a point mass at the empirical risk minimizer is beneﬁcial for
predictive performance. As alluded to in Example 1, this is perhaps to be expected: Not
only is the likelihood function of a BNN extremely ﬂexible so that even a point estimate is
likely to produce decent predictions, but it is also doubtful if a literal interpretation of the
prior as in (P) is appropriate for BNNs. As ﬁnding (C) shows however, this does not mean
that point estimates are preferable to posterior beliefs: Increasing the value of α shrinks the
variances too much, eventually impeding predictive performance.
6.1.2 The surprising benefits of modularity (Figure 12)
While ﬁndings (B) and (C) should not come as a surprise by themselves, they do raise
an interesting question: In particular, GVI for D(α)
AR with α = 0.5 is the worst-performing
setting across the board. This is remarkable because this setting also constructs the only
11.
Note that their result holds for a diﬀerent parameterization of the D(α)
AR, but it is easy to show that our
parameterization is strictly smaller than theirs for α > 1.
46

Generalized Variational Inference
0.0
0.1
0.2
GVI,  D(0.5)
AR
0.0
0.1
0.2
Standard VI
0.0
0.1
0.2
DVI,  D(0.5)
AR
0.0
0.1
0.2
DVI,  D(0.5)
A
7
14
21
28
0.0
0.1
0.2
DVI,  D(0.0)
A
7
14
21
28
7
14
21
28
14
21
28
35
p(yi|xi) (posterior predictives)
0.0
0.12
0.24
0.36
GVI,  D(0.5)
AR
0.0
0.12
0.24
0.36
Standard VI
0.0
0.12
0.24
0.36
DVI,  D(0.5)
AR
0.0
0.12
0.24
0.36
DVI,  D(0.5)
A
7
14
21
28
0.0
0.12
0.24
0.36
DVI,  D(0.0)
A
7
14
21
28
7
14
21
28
14
21
28
35
p( |xi) (parameter posterior pushforwards)
Figure 12: Best viewed in color.
Depicted are test set predictions based on posterior
predictives (top panel) and parameter posterior pushforwards (bottom panel) with four
observations in the boston data set. Each column shows one observation (dashed line). The
predictive distributions (histogram) and their means (solid line) for each row correspond to
standard VI, DVI and GVI.
47

Knoblauch, Jewson and Damoulas
GVI posteriors in our experiments with wider variances than standard VI. At the same time,
producing wider variances and more conservative uncertainty quantiﬁcation is one of the
main motivations for Expectation Propagation (EP) the presented DVI methods, see for
example Figure 1(a) in Li and Turner (2016) or Figure 8 in Hernández-Lobato et al. (2016).
This is puzzling: Are wider variances for θ somehow beneﬁcial for DVI posteriors’ predictive
performance while damaging that of GVI posteriors? As it turns out, this is not the case.
Rather, while both GVI with α = 0.5 and all DVI produce parameter posteriors with larger
variances, in the case of DVI this does not translate into predictive uncertainty.
This phenomenon is depicted in Figure 12, which clearly shows that the additional
uncertainty in the DVI parameter posteriors q∗
DVI(θ|κ∗) is completely overshadowed by an
extreme degree of variance shrinkage in the corresponding posterior predictives. In other
words, the increased uncertainty in θ is outweighed by extremely small values for σ2. The
plot demonstrates this by comparing the push-forward F#q∗
DVI(·|κ∗) with the posterior
predictives. Formally, the push-forward is given by
p(µ|xi) = (F#q∗
DVI(·|κ∗)) (µ),
where the operation # is simply a formalization of the following two operations: (i) sample
θ ∼q∗
DVI(θ|κ∗), (ii) compute µ = F(θ).
The posterior predictive then integrates the
push-forward measure p(µ|xi) over the likelihood function as
p(yi|xi) =
Z
Θ
pN (yi|xi, σ2, F(θ))q∗
DVI(θ|κ∗)dθ =
Z
R
pN (yi|xi, σ2, µ)p(µ|xi)dµ.
As Figure 12 shows, the push-forward behaves as expected for both GVI and DVI. For DVI,
the same cannot be said for the posterior predictive: Interestingly, they generally have much
less variance than for standard VI.
This surprising phenomenon is due to hyperparameter optimization for σ2 and has an
intimate link with the modularity result of Theorem 18. Since variational inference on σ2
complicates the DVI objectives, both Hernández-Lobato et al. (2016) and Li and Turner
(2016) do not infer σ2 probabilistically. Instead, it is optimized over their objectives. This
approach poses an optimization problem which for D = D(α)
A
and D = D(α)
AR is given by
bσ2, q∗
DVI(θ|κ∗) = arg min
σ2
(
arg min
q∈Q
D(q(θ|κ)||q∗
B(θ|σ2, x1:n, y1:n))
)
.
(17)
Crucially, the inner part of this objective conditions on the exact Bayesian posterior for a
ﬁxed value of σ2 and then seeks to approximate the posterior belief given by
q∗
B(θ|σ2, x1:n, y1:n) ∝π(θ)
n
Y
i=1
pN (yi|xi, σ2, F(θ)).
At the same time however, the outer part of the objective seeks to ﬁnd a value for σ2 which
makes the posterior q∗
B(θ|σ2, x1:n, y1:n) as easily approximable as possible. In other words,
an objective which is explicitly motivated as a projection of q∗
B(θ|σ2, x1:n, y1:n) into Q also
changes the very point from which to project into Q.
48

Generalized Variational Inference
Though it would be computationally easy to perform probabilistic inference on σ2 within
GVI, we also optimize σ2 as a hyperparameter for comparability. Thus, we pose the alternative
optimization problem
bσ2, q∗
GVI(θ|κ∗) = arg min
σ2
(
arg min
q∈Q
(
Eq
" n
X
i=1
−log pN (yi|xi, σ2, F(θ))
#
+ D(α)
AR(q||π)
))
.(18)
As Figure 12 shows, the outcomes are drastically diﬀerent: Unlike in the DVI case, the
predictive uncertainty for the GVI posteriors move in the same direction as parameter
uncertainty as α varies. The modularity of GVI makes it obvious what the optimization over
σ2 corresponds to in eq. (18): Rather than choosing the best posterior q∗
B(θ|σ2, x1:n, y1:n)
from which to project into Q, the optimization problem simply seeks to ﬁnd the best possible
loss ℓσ2(yi|xi, F(θ)) = −log p(yi|xi, σ2, F(θ)) over all σ2 ∈R+.
6.2 Deep Gaussian Processes
Deep Gaussian Processes (DGPs) were introduced by Damianou and Lawrence (2013) and
extend the logic of deep learning to the nonparametric Bayesian setting. The principal idea
is to construct a hierarchy of Gaussian Process (GP) priors over latent spaces. Unlike with
the BNNs presented in the last section, the priors in DGPs are usually reﬁned at run-time
by using various hyperparameter optimization schemes. This is in fact crucial for DGPs to
provide good inferences: Indeed, it ensures that the inputs X are mapped into latent spaces
which are informative for the outputs Y . As a consequence, unlike with BNNs we expect
there to be comparatively little merit in varying the uncertainty quantiﬁer D for DGPs—a
suspicion we experimentally conﬁrm in Appendix H.2.3. Accordingly, we instead focus on
experiments that vary the loss ℓ. More speciﬁcally, we consider replacing the negative log
score with a robust scoring rule for the likelihood which is derived from the γ-divergence
(Hung et al., 2018), which drastically improves predictive performance.
In the remainder, we ﬁrst introduce DGPs (Section 6.2.1). Next, we provide a brief
overview of the doubly stochastic inference procedure in Salimbeni and Deisenroth (2017)
(Section 6.2.2) and show how to adapt DGPs to GVI (Section 6.2.3). Lastly, we present
numerical experiments and their results (Section 6.2.4). These ﬁndings are also summarized
with a higher level of detail in a separate technical report (Knoblauch, 2019b).
6.2.1 Preliminaries for DGPs
Given observations (X, Y ) where X ∈Rn×Dx and Y ∈Rn×p, a DGP of L layers introduces
L latent functions {F l}L
l=1. Here, F l is matrix-valued and of dimension Dl × Dl+1. Setting
F 0 = X, D0 = Dx and Dl+1 = p, one can write the DGP construction as
Y |F L
∼
p
 Y
 F L
F L|F L−1 = fL(F L−1)
∼
GP
 µL(F L−1), KL(F L−1, F L−1)

. . .
F 1|F 0 = f1(F 0)
∼
GP
 µ1(F 0), K1(F 0, F 0)

,
where the mean and covariance functions are µl : RDl →RDl+1 and Kl : RDl×Dl →
RDl+1×Dl+1. Scalable inference strategies for this model generally rely on VI (Damianou and
49

Knoblauch, Jewson and Damoulas
Lawrence, 2013; Dai et al., 2016; Salimbeni and Deisenroth, 2017; Hensman and Lawrence,
2014), Monte Carlo methods (Vafa, 2016; Wang et al., 2016) or more specialized approaches
(Cutajar et al., 2017a).
In the remainder, we discuss the implications of Generalized
Variational Inference (GVI) in relation to the arguably most promising VI approach of
Salimbeni and Deisenroth (2017).
Unlike previous VI methods, it encodes conditional
dependence into the variational family Q and comprehensively outperformed Expectation
Propagation (EP) based alternatives (Bui et al., 2016).
6.2.2 Doubly stochastic VI in DGPs
First, deﬁne m inducing points Zl = (zl
1, zl
2, . . . , zl
m)T and their function values U l =
(fl(zl
1), fl(zl
2), . . . , fl(zl
m))T (for details on inducing points, see Snelson and Ghahramani,
2006; Titsias, 2009; Bonilla et al., 2019; Matthews et al., 2016). For improved readability, we
drop X and Zl from the conditioning sets and denote the i-th row of F l as f L
i . With this,
the joint distribution of the DGP is
p

Y , {F l}L
l=1, {U l}L
l=1

=
n
Y
i=1
p(yi|f L
i )
|
{z
}
likelihood
×
L
Y
l=1
p

F l U l, F l−1, Zl−1
p

U l Zl−1
|
{z
}
(DGP) prior
.
The posteriors p
 {F l}L
l=1, {U l}L
l=1

and p
 {F l}L
l=1

are intractable. The VI method proposed
in Salimbeni and Deisenroth (2017) overcomes this with the variational family given by
q

{F l}L
l=1, {U l}L
l=1

=
L
Y
l=1
p

F l U l, F l−1, Zl−1
q

U l
;
q

U l
= N

U l ml, Sl

.
This allows for exact integration over the inducing points {U l}L
l=1, yielding
q

{F l}L
l=1

=
L
Y
l=1
N

F l µl, Σl

.
As shown in Salimbeni and Deisenroth (2017), this enables a doubly stochastic minimization
of the negative Evidence Lower Bound (ELBO) given by
Eq(F L)
" n
X
i=1
−log p(yi|F L)
#
+ KLD

q({F l}L
l=1, {U l}L
l=1)
p({F l}L
l=1, {U l}L
l=1)

= −
n
X
i=1
Eq(f L
i )

log p(yi|f L
i )

+
L
X
l=1
KLD(q(U l)||p(U l)).
(19)
For optimization, the samples for F l are drawn using the variational posteriors from the
previous layers so that approximating the expectations over q(f L
i ) induces the ﬁrst layer of
stochasticity. The second layer is due to drawing mini-batches from X = F 0 and Y . Because
of this large degree of stochasticity, it is appealing if Eq(f L
i )

log p(yi|f L
i )

is available in
closed form, which is for instance the case if p = pN is a normal likelihood.
50

Generalized Variational Inference
6.2.3 Adaption to GVI
The objective in eq. (19) suggests itself naturally to a GVI variant. This raises two questions:
(D) Is it theoretically coherent with the meaning and function of D in the axiomatic
development of Section 4.1 to simply replace the KLD-terms layer-wise?
(ℓ) Can one derive closed forms for the expectations when the log scoring rule is replaced
by robust alternatives Lβ or Lγ derived from the β- and γ-divergence?
As shown next, we can give a positive answer to both these questions.
(D)
Conveniently and as shown in Salimbeni and Deisenroth (2017),
KLD

q({F l}L
l=1, {U l}L
l=1)
p({F l}L
l=1, {U l}L
l=1)

=
L
X
l=1
KLD(q(U l)||p(U l)).
(20)
A natural question is whether one can reverse-engineer this ﬁnding: If we simply pick a collec-
tion of other divergences Dl(q(U l)||p(U l)) for each layer l and combine them additively, does
the result deﬁne a valid divergence between q({F l}L
l=1, {U l}L
l=1) and p({F l}L
l=1, {U l}L
l=1)?
As the next Corollary shows, one can prove that reverse-engineering prior regularizers in-
spired by eq. (20) is feasible so long as the layer-speciﬁc divergences Dl are f-divergences or
monotonic transformations of f-divergences. The proof relies on a technical Lemma and is
given in Appendix H.2.1
Corollary 28 In the DGP construction of eq. (19), replacing the sum of KLD-terms by
L
X
l=1
Dl(q(U l)||p(U l))
deﬁnes a valid divergence between q({F l}L
l=1, {U l}L
l=1) and p({F l}L
l=1, {U l}L
l=1) so long as Dl
is an f-divergence or a divergence obtained as a monotonic transform g of an f-divergence
for all l = 1, 2, . . . L.
(ℓ)
Next, we turn attention to modifying the loss terms in eq. (19). First, note that
Eq(F L)
" n
X
i=1
−log p(yi|F L)
#
= −
n
X
i=1
Eq(f L
i )

log p(yi|f L
i )

.
This identity still holds if one replaces the negative log with other scoring rules. As the next
Proposition shows, we even retain closed forms for the regression case and the scoring rules
Lβ
p(f L
i , yi) = −
1
β −1p(yi|f L
i )β−1 + Ip,β(f L
i )
β
Lγ
p(f L
i , yi) = −
1
γ −1p(yi|f L
i )γ−1 ·
γ
Ip,γ(f L
i )−γ−1
γ
.
Crucially, the integral term Ip,c(f L
i ) =
R
p(y|f L
i )cdy is generally available in closed form for
exponential families. As the notation suggests, Lβ
p is linked to the β-divergence in the same
51

Knoblauch, Jewson and Damoulas
way we linked the log score to the KLD in Section 5.2.2, see also Basu et al. (1998). Similarly,
Lγ
p is derived from the γ-divergence as explained in Hung et al. (2018). As also alluded to in
Section 5.4.1, Lγ
p (Lβ
p) recovers the log score as γ →1 (β →1) and produces robust inferences
for γ > 1 (β > 1). Figure 4 depicts this for Lβ
p, and the behaviour is very similar for Lγ
p.
Proposition 29 (Closed forms for robust DGP regression) If it holds that yi ∈Rd,
p(yi|f L
i ) = N
 yi; f L
i , σ2Id

;
q(f L
i ) = N(f L
i ; µ, Σ),
then for the quantities given by
eΣ−1 =
 c
σs Id + Σ−1
;
eµ =
 c
σ2 yi + Σ−1µ

;
I(c) = (2πσ2)−0.5dcc−0.5d
and for
E(c) = 1
c
 2πσ2−0.5dc |eΣ|0.5
|Σ|0.5 exp

−1
2
 c
σ2 yT
i yi + µT Σ−1µ −eµT eΣeµ

the following expectations are available in closed form:
Eq(f L
i )

Lβ
p(f L
i , yi)

= −E(β −1) + I(β)
β
Eq(f L
i )

Lβ
p(f L
i , yi)

= −E(γ −1) ·
γ
I(γ)
γ
γ−1
As shown in Appendix H.2.2, it is easy but tedious to derive this result. While the results of
using Lβ
p and Lγ
p are often virtually identical (see for instance Figure 22 in Appendix H.1),
our experiments on DGPs will exclusively use the Lγ
p. This is done because unlike for Lβ
p,
computations with Lγ
p can be performed in its numerically more stable log form.
6.2.4 Results
As with the experiments on BNNs in the previous section, we make comparisons as fair
as possible by using the gpflow (Matthews et al., 2017) implementation of Salimbeni and
Deisenroth (2017). Further, we use the same settings, meaning that all experiments use
20,000 iterations of the ADAM optimizer (Kingma and Ba, 2014) with a learning rate of 0.01
and default settings for all other hyperparameters. We perform inference for each of the UCI
data sets (Lichman, 2013) after normalization using the RBF kernel with dimension-wise
lengthscales, 100 inducing points, with batch sizes of min(1000, n) and Dl = min(Dx, 30).
As before, we use 50 random splits with 90% training and 10% test data to assess predictive
performance in terms of negative log likelihood (NLL) and root mean square error (RMSE).
With this, we compare two inference schemes:
(1) The state of the art standard VI techniques of Salimbeni and Deisenroth (2017);
(2) A GVI variant of the same inference method which replaces the log score with the
robust γ-divergence based scoring rule Lγ
p.
52

Generalized Variational Inference
2.8
3.0
3.2
L = 3
L = 2
L = 1
= 1.01, L = 3
= 1.01, L = 2
= 1.01, L = 1
= 1.05, L = 3
= 1.05, L = 2
= 1.05, L = 1
GVI
VI
boston
4.5
5.0
5.5
concrete
0.50
0.75
1.00
1.25
1.50
energy
0.06
0.07
0.08
0.09
kin8mn
2.4
2.5
2.6
L = 3
L = 2
L = 1
= 1.01, L = 3
= 1.01, L = 2
= 1.01, L = 1
= 1.05, L = 3
= 1.05, L = 2
= 1.05, L = 1
GVI
VI
2.9
3.0
3.1
0.75
1.00
1.25
1.50
1.75
1.4
1.3
1.2
1.1
1.0
0.62
0.63
L = 3
L = 2
L = 1
= 1.01, L = 3
= 1.01, L = 2
= 1.01, L = 1
= 1.05, L = 3
= 1.05, L = 2
= 1.05, L = 1
GVI
VI
wine
0.00025 0.00030 0.00035
naval
0.4
0.6
0.8
yacht
3.8
4.0
power
1.80
1.85
1.90
protein
0.93
0.94
0.95
0.96
L = 3
L = 2
L = 1
= 1.01, L = 3
= 1.01, L = 2
= 1.01, L = 1
= 1.05, L = 3
= 1.05, L = 2
= 1.05, L = 1
GVI
VI
6.8
6.6
0.5
1.0
2.75
2.80
2.0
2.1
2.2
2.3
Figure 13: Best viewed in color. Top rows depict RMSE, bottom rows the NLL across a
range of data sets using DGPs. Dots correspond to means, whiskers to standard errors. The
further to the left, the better the predictive performance. For the depicted selection of data
sets, GVI comprehensively outperforms standard VI.
For choosing γ, we note that inferences are robust for γ > 1 and that Lγ
p recovers the log
score as γ →1. At the same time, the scoring rule will grow increasingly happy to ignore
virtually all of the data as γ →∞. Accordingly, one will typically want to pick
γ = 1 + ε
53

Knoblauch, Jewson and Damoulas
for a small ε > 0.
Choosing γ in this way encodes the intuition that a good scoring
rule will behave like the log score for all but the most extreme outliers. We thus pick
ε ∈{0.01, 0.05}, a range of values also successfully used in (Jankowiak et al., 2019). We note
that hyperparameter optimization might appear to be the natural choice for picking γ, but
will not perform well in practice: Rather than producing robust inferences, this will select
for a value of γ generally producing the smallest GVI objective values across Q12.
The results are depicted in Figure 13 and conﬁrm our two main intuitions about robustness:
Firstly, the robust scoring rule provides a signiﬁcant performance improvement. Secondly,
the smaller value of γ (which will be closer to the log score) generally outperforms the larger
value of γ, though both choices are equally good in many data sets13. We believe that the
performance gains of the robust scoring rule is due to large parts of the latent spaces being
non-informative. This implies that it is beneﬁcial to implicitly down-weight the inﬂuence
of these non-informative parts of the latent space. It is clear that robust scoring rules do
exactly that (see for instance Figure 4), which explains their superior performance in the
DGP experiments. This intuition is further bolstered by the following observation: Generally,
performance improves with a larger number of layers L under the robust score Lγ
p, but
worsens under the log score. In other words: The more dispersed the prior over the latent
space (i.e., the DGP) becomes, the more inferential outcomes beneﬁt from implicitly ignoring
its non-informative regions. In Appendix H.2 we provide a small batch of additional results
showing that as expected, modifying the uncertainty quantiﬁer D is less beneﬁcial for DGPs
than it is for BNNs. Most likely, this is due to hyperparameter optimization for the kernels
of the DGP. Together with the fact that Gaussian Processes are far more informative priors
than fully factorized normals, careful selection of the hyperparameters ensures that unlike in
the BNN case, the prior is well-speciﬁed.
7. Discussion & Conclusion
In this work, we re-examined the working assumptions that have proven powerful and useful
in spreading Bayesian inference into virtually all domains of scientiﬁc endeavour. Studying
the challenges of contemporary inference, we concluded that the traditional assumptions
underlying Bayesian statistics are misaligned with the realities of modern large-scale problems.
At the same time, we adopted an optimization-centric view on Bayesian inference that allows
us to prove a novel optimality result for standard Vatiational Inference (VI). In spite of
this theoretical result, we pointed out that belief distributions computed as alternative
approximations to the Bayesian posterior often perform better in practice. We explained
that this is because standard VI is optimal only relative to a particular objective function—
speciﬁcally, an objective function whose origins are the very assumptions that are misaligned
with reality. Inspired by this insight, we proceed to derive a new class of posterior belief
distributions that do not rely on these assumptions. To do so, we ﬁrst set out a new axiomatic
foundation for Bayesian inference culminating in the Rule of Three (RoT). The RoT is an
12. In practice, this means that hyperparameter optimization pushes γ →1 or γ →∞, depending on the
magnitudes of {p(yi|f L
i )}n
i=1.
13.
We expect this second ﬁnding about γ to generalize to new settings so long as the inputs are normalized
and the outputs are not high-dimensional (see also Figure 22 for some empirical evidence of this on
BNNs), which would make γ = 1.01 a decent default choice in such scenarios.
54

Generalized Variational Inference
optimization problem with three arguments, each of which addresses one of the shortcomings
of the standard Bayesian assumptions. Yet, while it deﬁnes a much larger family of posteriors,
the RoT also recovers the standard Bayesian update rule as a special case. Building on this
novel generalized class of posteriors, we introduce Generalized Variational Inference (GVI).
In essence, GVI restricts attention to the tractable subset of RoT posterior beliefs contained
within a variational family. Next, we show that GVI satisﬁes a number of desirable theoretical
properties: It is modular (in the sense of Theorem 18) and consistent in the frequentist sense.
Moreover, the objectives for a sub-class of GVI posteriors form an approximate evidence
lower bound on a generalized Bayesian posterior. On the practical side, we show three
generic applications of GVI in the broad context of customized uncertainty quantiﬁcation and
robustness. Speciﬁcally, we demonstrate how GVI can be used to adjust posterior variances
and produce inferences that are robust to model and prior misspeciﬁcation. Lastly, we
demonstrate GVI’s power, usefulness and versatility on two model classes that encapsulate
the misalignment between the assumptions underlying the traditional Bayesian paradigm
and the realities of modern large-scale Bayesian inference: Bayesian Neural Networks (BNNs)
and Deep Gaussian Processes (DGPs).
The current work makes two major contributions. The ﬁrst of these is conceptual: We
propose a generalization of Bayesian inference through the Rule of Three (RoT). This aspect
of our work stands in the tradition of previous generalizations of Bayesian inference such
as the one in Bissiri et al. (2016) and Jewson et al. (2018). Unlike previous work however,
we take the ﬁrst step in the development of Bayesian inference procedures that generalize
beyond multiplicative belief updates. Indeed, this step is natural once one observes the
intimate connection between Bayesian inference and inﬁnite-dimensional optimization as
set out in Observation 1. As explained in Sections 2 and 4, an immediate consequence of
this generalization is a meaningful taxonomy of various variational inference procedures:
Unlike most other variational approximations to the Bayesian posterior, standard Variational
Inference (VI) is a special case of the RoT. This special standing of standard VI also
expresses itself in Theorem 3, which endows standard VI with an (objective-dependent)
quality guarantee that is absent from alternative approximation procedures.
The second contribution is methodological and consists in making the RoT useful for real
world inference problems via Generalized Variational Inference (GVI). We show that GVI
modularly addresses the three shortcomings associated with traditional Bayesian inference.
This is done by linking it to the literature on generalized Bayesian inference and robust
scoring rules as well as to the literature on robust divergences. As Section 6 shows with two
applications on large-scale inference problems, GVI posteriors of this form can yield signiﬁcant
predictive performance improvements in modern statistical machine learning models.
With the provision of a new optimization-centric generalization on Bayesian inference, the
current paper is only the ﬁrst step on a long road to designing posteriors that conform with
the demands of contemporary models and inferential problems. In the wake of this, several
important questions have been left unanswered. For example, it is unclear how to choose
hyperparameters occurring in the loss or uncertainty quantiﬁer beyond simplistic (albeit well-
working) rules of thumb. Further, we have not characterized the class of posteriors satisfying
the axioms in Section 4.1 uniquely. Though the RoT is unique when restricting attention to
elementary functions and is arguable the most desirable form due to its relationship to the
standard Bayesian and variational posteriors, we are convinced that the uniqueness result
55

Knoblauch, Jewson and Damoulas
of Theorem 15 can be derived under much stronger conditions. GVI also has an obvious
intimate connections with PAC-Bayesian approaches that we will be exploring in the near
future. Moreover, the ﬂexibility in choosing diﬀerent uncertainty quantiﬁers D brings about
another interesting question: Given that frequentist consistency holds, what impact does D
have on the contraction rate? And do certain special cases of D endow GVI with compelling
geometric interpretations?
In summary, the current work is but the start of an investigation into the theoretical,
methodological and applied consequences of the RoT and GVI. It is clear that the ideas
introduced in the current paper—while barely scratching the surface of the possible—have
produced valuable insights and shown much promise in all three of these regards. Consequently,
it is with much excitement that we look forward to future contributions on questions of
theory, methodology and practice surrounding the RoT and GVI.
Acknowledgments
We would like to cordially thank Edwin Fong, Benjamin Guedj, Chris Holmes, David Dunson,
Mark van der Wilk, Giles Hooker and Alex Alemi for fruitful discussions, insights, comments
and pointers that were invaluable for improving the paper. JK and JJ are funded by EPSRC
grant EP/L016710/1 as part of the Oxford-Warwick Statistics Programme (OxWaSP). JK is
additionally funded by the Facebook Fellowship Programme and the London Air Quality
project at the Alan Turing Institute for Data Science and AI. TD acknowledges funding
from EPSRC grant EP/T004134/1, the Lloyd’s Register Foundation programme on Data
Centric Engineering, and the London Air Quality project at the Alan Turing Institute for
Data Science and AI. This work was furthermore supported by The Alan Turing Institute for
Data Science and AI under EPSRC grant EP/N510129/1 in collaboration with the Greater
London Authority.
56

Generalized Variational Inference
Appendix A. Deﬁnitions for robust divergences
The following is an overview of deﬁnitions for the most important divergences that are used
throughout the paper.
Deﬁnition 30 (The αβγ-divergence D(α,β,r)
G
(Cichocki and Amari, 2010)) The αβγ-
divergence D(α,β,r)
G
Cichocki and Amari (2010) takes the form
D(α,β,r)
G
(q(θ)||π(θ)) =
1
α(β −1)(α + β −1)r
h
˜D(α,β)
G
(q(θ)||π(θ)) + 1
r
−1
i
where r > 0, α ̸= 0, β ̸= 1 and
˜D(α,β)
G
(q(θ)||π(θ)) =
Z 
αq(θ)α+β−1 + (β −1)π(θ)α+β−1 −(α + β −1)q(θ)απ(θ)β−1
dθ
Below we list some well-known special cases of the family of divergences deﬁned by D(α,β,r)
G
that we use in the main paper. This exposition is a summary of the review conducted in
Cichocki and Amari (2010).
Deﬁnition 31 (The α-divergence (D(α)
A ) (Chernoﬀ, 1952; Amari, 2012)) The α-divergence
is deﬁned as
D(α)
A (q(θ)||π(θ)) =
1
α(1 −α)

1 −
Z
q(θ)απ(θ)1−αdθ

,
where α ∈R \ {0, 1}. Note that D(α)
A
is recovered from D(α,β,r)
G
when r = 1 and β = 2 −α.
D(α)
A
is also a member of the f-divergence family.
Deﬁnition 32 (Rényi’s α-divergence (D(α)
AR) (Rényi, 1961)) Rényi’s α-divergence is de-
ﬁned as
D(α)
AR(q(θ)||π(θ)) =
1
α(α −1) log
Z
q(θ)απ(θ)1−αdθ

,
where α ∈R \ {0, 1}. D(α)
AR is recovered from D(α,β,r)
G
in the limit as r →0 and β = 2 −α.
Note that we use the rescaled version proposed by Liese and Vajda (1987); Cichocki and
Amari (2010) rather than the original parameterization of Rényi (1961) because it links the
divergence more closely to other robust alternatives of the KLD.
Deﬁnition 33 (The β-divergence (D(β)
B ) (Basu et al., 1998; Mihoko and Eguchi, 2002))
The β-divergence (Mihoko and Eguchi, 2002) was originally introduced under the name ”den-
sity power divergence“ and is deﬁned as
D(β)
B (q(θ)||π(θ)) =
1
β(β −1)
Z
q(θ)βdθ + 1
β
Z
π(θ)βdθ −
1
β −1
Z
q(θ)π(θ)β−1dθ,
where β ∈R \ {0, 1}. D(β)
B
is recovered from D(α,β,r)
G
when r = α = 1. D(β)
B
is a member of the
Bregman-divergence family.
57

Knoblauch, Jewson and Damoulas
Deﬁnition 34 (The γ-divergence (D(γ)
G ) (Fujisawa and Eguchi, 2008)) The γ-divergence
(Fujisawa and Eguchi, 2008) is deﬁned as
D(γ)
G (q(θ)||π(θ)) =
1
γ(γ −1) log
 R
q(θ)γdθ
  R
π(θ)γdθ
γ−1
 R
q(θ)π(θ)γdθ
γ
,
where γ ∈R \ {0, 1}. D(γ)
G
is recovered from D(α,β,r)
G
in the limit as r →0, α = 1 and β = γ.
The D(γ)
G
can be shown to be generated from the D(β)
B
applying the following transformation
c0
Z
g(x)c1f(x)c2dx →c0 log
Z
g(x)c1f(x)c2dx
to all three of the D(β)
B
terms. This is of interest because the D(α)
AR is generated by the D(α)
A
by
applying the same transformation of its two terms.
Remark 35 (Recovering the KLD) The D(α)
A , D(α)
AR, D(β)
B
and D(γ)
G
all recover the KLD in
the limit as α = β = γ →1. This can be shown using the replica trick:
lim
x→0
Zx −1
x
= log(Z).
Appendix B. Comparing robust divergences as uncertainty quantiﬁers
In order to understand the impact the choice of divergence used for regularization and its
hyperparameter have on the inference, this section studies variations in the argument D.
This investigation is conducted on a simple Bayesian linear regression example with two
highly correlated predictors given by
σ2 ∼IG(a0, b0)
θ|σ2 ∼N2
 µ0, σ2V0

(21)
yi|θ, σ2 ∼N
 Xiθ, σ2
.
We choose this example because it provides a closed form exact Bayesian posteriors and closed
form objectives for the variational objectives of VI and GVI. Consequently, no sampling is
required—neither for calculating the exact posterior nor for the optimization of the GVI and
VI posteriors—so that numerical errors and uncertainties are kept to a minimum.
Studying the exact closed form Bayesian (normal) posterior for θ = (θ1, θ2)T , one observes
that if the two predictors are correlated, then the posterior covariance of θ will inherit this
correlation. As we wish to investigate the underestimation of marginal variances for standard
VI as well as the way in which GVI can address this, our numerical studies leverage this
ﬁnding. In particular, we simulate the highly correlated predictors
(x1, x2)T ∼N2
0
0

,
 1
0.9
0.9
1

58

Generalized Variational Inference
0
1
2
3
4
5
1
0.0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
Density
D =
-divergence (D( )
A )
Exact posterior
= 1.25
Standard VI
= 0.95
= 0.5
= 0.01
MLE
Figure 14: Best viewed in color. Marginal VI and GVI posterior for the θ1 coeﬃcient
of a Bayesian linear model under the D(α)
A
prior regularizer for diﬀerent values of α. The
boundedness of the D(α)
A causes GVI posteriors to severely over-concentrate if α is not carefully
speciﬁed. Prior Speciﬁcation: σ2 ∼IG(20, 50), θ1|σ2 ∼N(0, 25σ2) and θ2|σ2 ∼N(0, 25σ2).
and compare the performance of the diﬀerent GVI and VI posteriors on the resulting Bayesian
linear regression. All posteriors are based on the the mean ﬁeld normal variational family
Q = {q(θ1|σ2, κn)q(θ2|σ2, κn)q(σ2|κn)} so that κn = (an, bn, µ1,n, µ2,n, v1,n, v2,n)T
with an, bn, v1,n, v2,n > 0 and µ1,n, µ2,n ∈R
q(σ2|κn) = IG(σ2|an, bn)
q(θ1|σ2, κn) = N
 θ1|µ1,n, σ2v1,n

q(θ2|σ2, κn) = N
 θ2|µ2,n, σ2v2,n

.
For all experiments, n = 25 observations are simulated from eq. (22) with θ = (2, 3) and
σ2 = 4. We use the negative log-likelihood of the correctly speciﬁed model as given in eq.
(22) as loss function. To investigate GVI’s behaviour across diﬀerent uncertainty quantiﬁers,
we vary its choice as D ∈

D(α)
A , D(β)
B , D(α)
AR, D(γ)
G
	
. The results are depicted in Figs. 14 and
16-20. We summarize the most interesting results from these plots in the following three
subsections.
B.1 A cautionary tale: The boundedness of the α-divergence (D(α)
A )
Of the alternative divergences to the KLD contained within the D(α,β,r)
G
family deﬁned in
Appendix A, D(α)
A
is arguably the most well known. Our results in Figure 14 show that
in spite of its popularity in other contexts, the D(α)
A
is not a reliable uncertainty quantiﬁer
59

Knoblauch, Jewson and Damoulas
0.00
0.25
0.50
0.75
1.00
1.25
1.50
1.75
2.00
hyperparameter value
0
10
20
30
40
50
magnitude of D(q| )
D =
-divergence (D( )
A )
KLD
Figure 15: A comparison of the size of D(α)
A
for various values of α between two bivariate
Normal Inverse Gamma distributions with an = 512, bn = 543, µn = (2.5, 2.5), Vn =
diag(0.3, 2) and a0 = 500, b0 = 500, µ0 = (0, 0), V0 = diag(25, 2).
within GVI, at least for α ∈(0, 1). In particular, the plot shows that the solutions to
P(ℓ, D(α)
A , Q) can produce essentially degenerate posteriors if α ∈(0, 1). Note also that this
happens in spite of the relatively small sample size of n = 25. For example, when α = 0.5,
P(ℓ, D(α)
A , Q) is visually indistinguishable from a point mass at the maximum likelihood
estimate. This is a consequence of the boundedness of D(α)
A
for α ∈(0, 1): Speciﬁcally, it
holds that D(α)
A
≤(α(1 −α))−1 for α ∈(0, 1). As α decreases from 1, this upper-bound
initially also decreases until reaching its minimum for α = 0.5. As a result, decreasing α
from unity to 0.5 signiﬁcantly decreases the maximal penalty for posterior beliefs far from
the prior. In turn, this forces the posterior to focus mostly on minimising the in-sample loss.
This phenomenon is depicted in Figure 15, which also shows that the divergence magnitude
increases again as α approaches zero or if α > 1. Comparing the plot with that in Figure
7, it is clear why hyperparameter selection for the other members of the D(α,β,r)
G
family of
divergences is a less complicated endeavour than for the α-divergence. This does not mean
that the D(α)
A
cannot be used for producing GVI posteriors: For example, in Figure 14, the
D(α)
A
is able to achieve marginal variances that more closely correspond to the exact posterior
for α = 1.25 and α = 0.01. Generally speaking, for values of α close to zero or above
unity, it is possible to achieve more conservative uncertainty quantiﬁcation. Yet, the D(α)
A
also functions primarily as a cautionary tale: Without understanding the properties of the
uncertainty quantiﬁer D suﬃciently well, GVI may well yield unsatisfactory posteriors.
B.2 Larger divergences produce larger marginal variances
In this section, we summarize the impact that a selection of robust divergences have on the
marginal variances of the solution to P(ℓ, D, Q), again using the Bayesian Linear regression
model from before. For a range of robust divergences, Figure 16 illustrates the impact that
60

Generalized Variational Inference
changes in D have on the marginal variances of the resulting posteriors. As one should expect
from re-examining Figure 7, the plot shows that D(β)
B , D(α)
AR and D(γ)
G
are able to produce
larger posterior variances for β, α, γ < 1 and smaller posterior variances for β, α, γ > 1. This
0.00
0.25
0.50
0.75
1.00
1.25
1.50
1.75
Density
D = w
1 KLD
Exact posterior
w = 2.0
Standard VI
w = 0.5
w = 0.125
MLE
D = Renyi's -divergence (D( )
AR)
Exact posterior
= 1.25
Standard VI
= 0.5
= 0.025
MLE
0
1
2
3
4
5
1
0.00
0.25
0.50
0.75
1.00
1.25
1.50
1.75
Density
D = -divergence (DG)
Exact posterior
= 1.5
Standard VI
= 0.75
= 0.15
MLE
0
1
2
3
4
5
1
D =
-divergence (DB)
Exact posterior
= 1.5
Standard VI
= 0.75
= 0.5
MLE
Figure 16: Best viewed in color. Marginal VI and GVI posterior for the ﬁrst coeﬃcient of a
Bayesian linear model under the D(α)
AR, D(β)
B , D(γ)
G and
1
wKLD prior regularizers for diﬀerent
uncertainty quantiﬁers. Correlated covariates cause dependency in the exact Bayesian
posterior of the coeﬃcients and as a result VI underestimates marginal variances. GVI
has the ﬂexibility to produce wider marginal variances. Prior Speciﬁcation: σ2 ∼IG(20, 50),
θ1 ∼N(0, 52) and θ2 ∼N(0, 52).
61

Knoblauch, Jewson and Damoulas
is a manifestation of the posterior being penalized more heavily (β, α, γ < 1) or less heavily
(β, α, γ > 1) for deviating from the prior than under the traditional VI. It follows that by
choosing the divergence appropriately, GVI can allow greater control over the uncertainty
quantiﬁcation characteristics of the resulting posterior than what is possible under standard
VI. Note that Figure 16 also compares the robust divergences against the re-weighted
KLD. While the re-weighted KLD can prove a successful alternative for producing desirable
variational posteriors with larger variances robust divergences if the prior is well-speciﬁed,
this is no longer the case if the information contained in the prior cannot be relied upon. We
study this and related ﬁndings surrounding robustness to the prior in the next section.
B.3 Robustness to the prior
Next, we compare the impact of changing the uncertainty quantiﬁer on the posterior’s
sensitivity to appropriate speciﬁcation of the prior. Speciﬁcally, we consider and compare
D(β)
B , D(α)
AR, D(γ)
G
and
1
w KLD. When comparing
1
w KLD with D(α)
AR and D(γ)
G , we ﬁxed α = γ = w.
Setting the values of these various hyperparameters to be the same is intuitively appealing
for comparison due to GVI’s interpretation as approximate Evidence Lower Bound (ELBO),
see Theorems 24 and 40. For the D(β)
B , diﬀerent values of β had to be selected to ensure its
availability in a closed form.
B.3.1 weighted KLD ( 1
w KLD)
Figure 17 examines how changing the weight w aﬀects the posteriors P(ℓ, 1
w KLD, Q). Notice
that this is equivalent to changing the negative log likelihood to a power likelihood with power
w. Further, it should be clear that choosing w < 1 leads to posteriors that encourage larger
variances, making them amenable to conservative uncertainty quantiﬁcation. Unfortunately
and again unsurprisingly, this comes at the price of making posteriors more sensitive to the
prior: After all, one up-weights the term penalizing deviations from the prior. Conversely,
w > 1 will result in posteriors that are less sensitive to the prior than standard VI. At the
same time, they will also be more concentrated around the Maximum Likelihood Estimator.
This makes D =
1
w KLD less attractive than it could be: Setting w to smaller values will yield
larger posteriors variances (at the expense of not being robust to the prior), while setting
w to larger values will make the posterior more robust to misspeciﬁed priors (but at the
expense of far more concentrated posteriors). As we shall see, this undesirable trade-oﬀis
not shared by the other (robust) divergences considered in this section. Unlike the
1
w KLD,
they often provide a way to have your cake and eat it, too.
B.3.2 Rényi’s α-divergence (D(α)
AR)
Figure 18 demonstrates the sensitivity of P(ℓ, D(α)
AR, Q) to prior speciﬁcation. For 0 < α < 1,
the posterior exhibits the kind of behaviour that is diﬃcult to attain with standard VI: It
both produces larger marginal variances and is robust to badly speciﬁed priors. This is no
longer true if α > 1: For α > 1, D(α)
AR ≤KLD, so that it is more sensitive to the prior than
the KLD. This ﬂip in robustness as α crosses from (0, 1) into values larger than one may
seem strange, but can be understood by investigating the form of the D(α)
AR:
D(α)
AR(q(θ)||π(θ)) =
1
α(α −1) log
Z
q(θ)απ(θ)1−αdθ =
1
α(α −1) log
Z
q(θ)α
π(θ)α−1 dθ.
62

Generalized Variational Inference
0.0
0.2
0.4
0.6
0.8
1.0
1.2
Density
D = w
1 KLD w = 1.25
= 3.0
= -5.0
= -20.0
= -50.0
MLE
D = w
1 KLD w = 1
= 3.0
= -5.0
= -20.0
= -50.0
MLE
1
0
1
2
3
4
5
1
0.0
0.2
0.4
0.6
0.8
1.0
1.2
Density
D = w
1 KLD w = 0.75
= 3.0
= -5.0
= -20.0
= -50.0
MLE
1
0
1
2
3
4
5
1
D = w
1 KLD w = 0.5
= 3.0
= -5.0
= -20.0
= -50.0
MLE
Figure 17: Best viewed in color. Marginal VI and GVI posterior for the coeﬃcient of a
Bayesian linear model under diﬀerent priors using D =
1
w KLD as uncertainty quantiﬁer
( 1
w KLD recovers KLD for w = 1). The prior speciﬁcation is given by θ1|σ2 ∼N(µπ, σ2) with
σ2 ∼IG(3, 5).
63

Knoblauch, Jewson and Damoulas
0.0
0.2
0.4
0.6
0.8
1.0
1.2
Density
D = Renyi's -divergence (D( )
AR) 
= 1.25
= 3.0
= -5.0
= -20.0
= -50.0
MLE
D = Renyi's -divergence (D( )
AR) 
= 1
= 3.0
= -5.0
= -20.0
= -50.0
MLE
1
0
1
2
3
4
5
1
0.0
0.2
0.4
0.6
0.8
1.0
1.2
Density
D = Renyi's -divergence (D( )
AR) 
= 0.75
= 3.0
= -5.0
= -20.0
= -50.0
MLE
1
0
1
2
3
4
5
1
D = Renyi's -divergence (D( )
AR) 
= 0.5
= 3.0
= -5.0
= -20.0
= -50.0
MLE
Figure 18: Best viewed in color. Marginal VI and GVI posterior for the coeﬃcient of
a Bayesian linear model under diﬀerent priors using D = D(α)
AR as uncertainty quantiﬁer
(D(α)
AR recovers KLD as α →1). The prior speciﬁcation is given by θ1|σ2 ∼N(µπ, σ2) with
σ2 ∼IG(3, 5).
64

Generalized Variational Inference
It is clear that the magnitude of the divergence is determined by a ratio of two densities.
Glancing closer, for α > 1 this means that if q(θ) is large in an area where π(θ) is not, then
a severe penalty is incurred. This limits how far the q(θ) can move from the prior and thus
results in lack of prior robustness. Conversely, if α ∈(0, 1), then π(θ)α−1 > π(θ) for regions
where π(θ) < 1, which allows the posterior to spread its mass in a less concentrated way
than for α > 1. In fact, this very ﬁnding is also implicitly stated in Theorem 24.
B.3.3 β-divergence (D(β)
B )
Figure 19 demonstrates the sensitivity of P(ℓ, D(β)
B , Q) to prior speciﬁcation. The plot shows
that β > 1 is able to achieve extreme robustness to the prior, while β < 1 causes extreme
sensitivity to the prior. This phenomenon is a result of the fact that the D(β)
B decomposes into
three integrals, one containing just the prior, one containing just q(θ) and one containing an
interaction between them.
D(β)
B (q(θ)||π(θ)) = 1
β
Z
π(θ)βdθ −
1
β −1
Z
π(θ)β−1q(θ)dθ +
1
β(β −1)
Z
q(θ)βdθ (22)
The integral depending only on the prior does not depend q(θ), so we can ignore it (since the
prior is ﬁxed across the diﬀerent values of β). If β increases substantially above 1, the second
term which expresses an interaction between π(θ) and q(θ) will have a relatively smaller
weight in the optimisation than the term only involving q(θ). As a result, the optimisation
will focus on decreasing
R
qβ(θ)dθ rather than increasing
R
πβ−1(θ)q(θ)dθ. We note that
this is closely linked to the so-called ignorance to the data phenomenon as discussed in
Jewson et al. (2018). The uncertainty quantiﬁcation for large values of β is therefore largely
controlled by the third term, which only depends on q(θ). This third integral will become
very large if the variance of q(θ) gets very small, which prevents it from quickly converging
to a point mass at the maximuml likelihood estimate. As a consequence, the D(β)
B
is able to
provide virtually prior-invariant uncertainty quantiﬁcation for beta > 1. For β ∈(0, 1), the
opposite eﬀect is observed: Here, the third integral term depending only on q(θ) has smaller
weight relative to the interaction between π(θ) and q(θ) given by
R
πβ−1(θ)q(θ)dθ. As a
result, the corresponding posterior will be very close to the prior. (In fact, notice that that
two of the four posteriors for β = 0.75 favour the prior so much that the density around the
maximum likelihood estimate is virtually zero.)
B.3.4 γ-divergence (D(γ)
G )
Lastly, Fig. 20 demonstrates the sensitivity of P(ℓ, D(γ)
G , Q) to prior speciﬁcation. For γ < 1
it appears as though the D(γ)
G reacts similarly to the
1
w KLD for w < 1. The D(γ)
G with γ > 1
produces greater robustness to the prior than the
1
w KLD uncertainty quantiﬁer with w > 1,
but this robustness is less extreme as it was for D = D(β)
B . The reason for this is that although
the D(γ)
G consists of the same three integral terms as the D(β)
B , these terms are now transformed
into the logarithmic scale. This means that the three integrals are combined multiplicatively
(in the D(γ)
G ) rather than additively (in the D(β)
B ), which makes the variation across γ much
smoother than across β: Unlike for the D(β)
B , minimising the D(γ)
G no longer disregards any
one term in order to minimise the others.
65

Knoblauch, Jewson and Damoulas
0.0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
Density
D =
-divergence (DB) 
= 1.25
= 3.0
= -5.0
= -20.0
= -50.0
MLE
D =
-divergence (DB) 
= 1
= 3.0
= -5.0
= -20.0
= -50.0
MLE
1
0
1
2
3
4
5
1
0.0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
Density
D =
-divergence (DB) 
= 0.9
= 3.0
= -5.0
= -20.0
= -50.0
MLE
1
0
1
2
3
4
5
1
D =
-divergence (DB) 
= 0.75
= 3.0
= -5.0
= -20.0
= -50.0
MLE
Figure 19: Best viewed in color. Marginal VI and GVI posterior for the coeﬃcient of
a Bayesian linear model under diﬀerent priors using D = D(β)
B
as uncertainty quantiﬁer
(D(β)
B
recovers KLD as β →1). The prior speciﬁcation is given by θ1|σ2 ∼N(µπ, σ2) with
σ2 ∼IG(3, 5).
66

Generalized Variational Inference
0.0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
Density
D = -divergence (DG) = 1.25
= 3.0
= -5.0
= -20.0
= -50.0
MLE
D = -divergence (DG) = 1
= 3.0
= -5.0
= -20.0
= -50.0
MLE
1
0
1
2
3
4
5
1
0.0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
Density
D = -divergence (DG) = 0.75
= 3.0
= -5.0
= -20.0
= -50.0
MLE
1
0
1
2
3
4
5
1
D = -divergence (DG) = 0.5
= 3.0
= -5.0
= -20.0
= -50.0
MLE
Figure 20: Best viewed in color. Marginal VI and GVI posterior for the coeﬃcient of
a Bayesian linear model under diﬀerent priors using D = D(γ)
G
as uncertainty quantiﬁer
(D(γ)
G
recovers KLD as γ →1). The prior speciﬁcation is given by θ1|σ2 ∼N(µπ, σ2) with
σ2 ∼IG(3, 5).
67

Knoblauch, Jewson and Damoulas
Appendix C. Proof of Theorem 18
Before we can prove Theorem 18, we ﬁrst formally deﬁne the notion of robustness to model
misspeciﬁcation. Our understanding of robustness to model misspeciﬁcation is aligned with
Hampel et al. (2011) and Tukey (1960). In the words of the latter, robustness stands for
a tacit hope in ignoring deviations from ideal models was that they would not matter;
that statistical procedures which are optimal under the strict model would still be
approximately optimal under the approximate model. Unfortunately, it turned out that
this hope was often drastically wrong; even mild deviations often have much larger
eﬀects than were anticipated by most statisticians.
Formalizing this, we arrive at the following deﬁnition.
Deﬁnition 36 (Robustness) Let Mj = P(Dj, ℓj, Π) with θ∗
j = arg minθ {EX [ℓj(θ, X)]}
for j = 1, 2. Then, M1 is more robust for θ than M2 relative to the (implicit) assumptions
A on the data generating mechanism of X if (i) θ∗
1 is a better result than θ∗
2 if A is untrue
and (ii) θ∗
1 = θ∗
2 if A is true.
Remark 37 It is hard to say what a better result means, but we note that regardless of
its precise meaning, this deﬁnition requires that robust inference directly aﬀects θ∗, i.e.
that θ∗
1 ̸= θ∗
2 unless A is true. While one could substantially strengthen this deﬁnition by
formalizing what exactly a better result means, this would necessarily be context-dependent,
complicate matters substantially and obfuscate the point of robustness.
Proof
First, we prove claim (i) about robustness to model misspeciﬁcation: By Def-
inition 36, robustness implies a change in θ∗= arg minθ {EX [ℓ(θ, X)]} if distributional
assumptions about X are incorrect.
Notice that θ∗is not aﬀected by D or Π, but is
aﬀected by ℓ.
Next, we turn to the claims (ii) and (iii) about uncertainty quantiﬁ-
cation and prior robustness: First, note that Π and π are not allowed to change by
assumption and so cannot aﬀect uncertainty quantiﬁcation. Next, while ℓis allowed to
change, the parameter of interest it not allowed to change. In other words, ℓmay only
be changed in a ways that leave ˆθn and θ∗unaﬀected. Notice that changing ℓto ℓ′ will
aﬀect ˆθn = arg minθ
 1
n
Pn
i=1 ℓ(θ, xi)
	
and θ∗= EX [ℓ(θ, x)] unless ℓ′ = C + w · ℓfor some
constants C and w > 0. Since P(ℓ, D, Π) = P(ℓ+ C, D, Π) for any C by Axiom II, we can
disregard C and turn to w. Indeed, the uncertainty quantiﬁcation of P(ℓ, D, Π) will be
diﬀerent from that of P(w · ℓ, D, Π) for any constant w ̸= 1. However, dividing by w in eq.
(10) shows that P(w · ℓ, D, Π) = P(ℓ, 1
wD, Π). Hence, any change in the loss that does not
aﬀect ˆθn and θ∗can be rewritten as a change in D. It follows that changing the uncertainty
quantiﬁcation or making the RoT robust to the prior belief must amount to changing D.
Appendix D. Proof of Theorem 24 and additional lower bounds
This section of the Appendix provides proofs for the lower bound interpretation of certain
GVI objectives. First, we prove the result stated in the main paper. Next, we show equivalent
results for the case of the β-divergence (D(β)
B ) and γ-divergence (D(γ)
G ). While the following
68

Generalized Variational Inference
results and corresponding proofs are somewhat tedious to read, they are conceptually simple:
In fact, all that is needed to derive the results is some basic algebra, Jensen’s inequality and
a further inequality involving the logarithm, see Lemma 38.
D.1 Proof for D(α)
AR (Theorem 24)
Firstly, we provide explicit forms for the function quoted in Theorem 24
S1(α, q, π) =
(
D(α)
AR(q(θ)||π(θ) −KLD(q(θ)||π(θ))
if 0 < α < 1
0
if α > 1.
(23)
Next we provide a proof of the Theorem
Proof For this proof we have to consider two cases for α as the positivity and negativity of
α −1 aﬀect the results that can be used.
Case 1) α > 1: Jensen’s inequality and the concavity of the natural logarithm give us that
Eq(θ)
" n
X
i=1
ℓ(θ, xi)
#
+
1
α(α −1) log Eq(θ)
" q(θ)
π(θ)
α−1#
≥Eq(θ)
" n
X
i=1
ℓ(θ, xi)
#
+ 1
αEq(θ)

log
 q(θ)
π(θ)

= 1
α
KLD(q(θ)||παℓ(θ|x)) −1
α log
Z
π(θ) exp
 
−α
n
X
i=1
ℓ(θ, xi)
!
dθ.
Case 2) 0 < α < 1: Here the negativity of
1
α(α−1) means we cannot apply Jensen’s inequality
in the above way. Instead, we can write
Eq(θ)
" n
X
i=1
ℓ(θ, xi)
#
+ D(α)
AR(q(θ)||π(θ))
= Eq(θ) [log(π(θ))] −Eq(θ)

log

π(θ) exp(−Pn
i=1 ℓ(θ, xi))
R
π(θ) exp(−Pn
i=1 ℓ(θ, xi))dθ

−log
Z
π(θ) exp(−
n
X
i=1
ℓ(θ, xi))dθ + D(α)
AR(q(θ)||π(θ))
= KLD(q(θ)||πℓ(θ|x)) −log
Z
π(θ) exp(−
n
X
i=1
ℓ(θ, xi))dθ
+D(α)
AR(q(θ)||π(θ)) −KLD(q(θ)||π(θ)).
Combined these two cases provides the term in Eq. (13) and (23)
Next we state, prove and interpret equivalent results for the D(β)
B
and D(γ)
G
prior regularisers.
But before we do so we need the following lemma
Lemma 38 (A Taylor series bound for the natural logarithm) The natural logarithm
of a positive real number Z can be bounded as follows
(
log(Z) ≤Zx−1
x
if x > 0
log(Z) ≥Zx−1
x
if x < 0.
69

Knoblauch, Jewson and Damoulas
Proof Using the series expansion of exp(x) and the Lagrange form of the remainder we see
that
Zx −1
x
= exp (x log Z) −1
x
= (x log Z) + 1
2! (x log Z)2 + 1
3! (x log Z)3 + . . .
x
= (x log Z) + 1
2 exp(c) (x log Z)2
x
= log Z +
1
2! exp(c) (x log Z)2
x
where c ∈[0, x log(Z)]. Now the numerator of the remainder term
1
2! exp(c)(x log Z)2
x
is always
positive and therefore the sign of x determines whether this remainder term forms an upper
or lower bound for log(Z).
D.2 The D(β)
B
prior regulariser
Theorem 39 (GVI as approximate Evidence Lower bound with D = D(β)
B ) The ob-
jective of a GVI posterior based on P(ℓ, D(β)
B , Q) has an interpretation as lower bound on the
c(β)-scaled (generalized) evidence lower bound of P(w(β) · ℓ, KLD, P(Θ)):
Eq(θ)
" n
X
i=1
ℓ(θ, xi)
#
+ D(β)
B (q(θ)||π(θ)) ≥−c(β)ELBOw(β)ℓ(q) + S1(β, q, π)
(24)
where ELBOw(β)ℓdenotes the Evidence Lower Bound associated with standard VI relative to
the generalized Bayesian posterior given by
qw(β)ℓ
B
(θ) ∝π(θ) exp
 
−w(β)
n
X
i=1
ℓ(θ, xi)
!
,
where c(β) = min{1, β−1}, w(β) = max{1, β} and where S1(β, q, π) is a closed form slack
term with
S1(β, q, π) =
(
1
β(β−1)Eq(θ)

q(θ)β−1
−Eq(θ) [log q(θ)] −
1
β−1
if 0 < β < 1
1
βEq(θ) [log π(θ)] −
1
β−1Eq(θ)

π(θ)β−1
−
1
β(β−1)
if β > 1.
(25)
Proof Firstly we note that the objective function associated with the RoT P(D(β)
B , ℓn, Q)
can be simpliﬁed by removing the terms in the D(β)
B
that don’t depend on q(θ)
arg min
q∈Q
(
Eq(θ)
" n
X
i=1
ℓ(θ, xi)
#
+ D(β)
B (q(θ)||π(θ))
)
=arg min
q∈Q
(
Eq(θ)
" n
X
i=1
ℓ(θ, xi)
#
+
1
β(β −1)Eq(θ)
h
q(θ)β−1i
−
1
(β −1)Eq(θ)
h
π(θ)β−1i)
.
We have to consider two cases for β as the positivity and negativity of β −1 aﬀect which
part of Lemma 38 we use.
70

Generalized Variational Inference
Case 1) 0 < β < 1: Lemma 38 gives us that for β −1 < 0, Zβ−1
β−1 ≤log(Z) +
1
β−1 therefore
= Eq(θ)
" n
X
i=1
ℓ(θ, xi)
#
+
1
β(β −1)Eq(θ)
h
q(θ)β−1i
−
1
(β −1)Eq(θ)
h
π(θ)β−1i
≥Eq(θ)
" n
X
i=1
ℓ(θ, xi)
#
+
1
β(β −1)Eq(θ)
h
q(θ)β−1i
−Eq(θ) [log(π(θ))] −
1
β −1
= KLD(q(θ)||πℓ(θ|x)) −log
Z
exp(−
n
X
i=1
ℓ(θ, xi))π(θ)dθ
+
1
β(β −1)Eq(θ)
h
q(θ)β−1i
−Eq(θ) [log(q(θ))] −
1
β −1.
Case 2) β > 1: Lemma 38 gives us that for β −1 > 0, Zβ−1
β−1 ≥log(Z) +
1
β−1 therefore
= Eq(θ)
" n
X
i=1
ℓ(θ, xi)
#
+
1
β(β −1)Eq(θ)
h
q(θ)β−1i
−
1
(β −1)Eq(θ)
h
π(θ)β−1i
≥Eq(θ)
" n
X
i=1
ℓ(θ, xi)
#
+ 1
β

Eq(θ)

log

q(θ)π(θ)
π(θ)

+
1
β −1

−
1
(β −1)Eq(θ)
h
π(θ)β−1i
= 1
β
KLD(q(θ)||πβℓ(θ|x)) −1
β log
Z
π(θ) exp(−β
n
X
i=1
ℓ(θ, xi))dθ
+ 1
β Eq(θ) [log(π(θ))] −
1
(β −1)Eq(θ)
h
π(θ)β−1i
+
1
β(β −1).
Combined these two cases provides the term in Eq. (24) and (25)
D.3 The D(γ)
G
prior regulariser
Theorem 40 (GVI as approximate Evidence Lower bound with D = D(γ)
G ) The ob-
jective of a GVI posterior based on P(ℓ, D(γ)
G , Q) has an interpretation as lower bound on the
c(γ)-scaled (generalized) evidence lower bound of P(w(γ) · ℓ, KLD, P(Θ)):
Eq(θ)
" n
X
i=1
ℓ(θ, xi)
#
+ D(γ)
G (q(θ)||π(θ)) = −c(γ)ELBOw(γ)ℓ(q) + S(γ, q, π)
(26)
where ELBOw(γ)ℓdenotes the Evidence Lower Bound associated with standard VI relative to
the generalized Bayesian posterior given by
qw(γ)ℓ
B
(θ) ∝π(θ) exp
 
−w(γ)
n
X
i=1
ℓ(θ, xi)
!
,
where c(γ) = min{1, γ−1}, w(γ) = max{1, γ} and where S1(γ, q, π) is a closed form slack
term with
S1(γ, q, π) =
(
1
γ(γ−1) log Eq(θ)

q(θ)γ−1
−Eq(θ) [log q(θ)]
if 0 < γ < 1
1
γ Eq(θ) [log π(θ)] −
1
γ−1 log Eq(θ)

π(θ)γ−1
if γ > 1.
(27)
71

Knoblauch, Jewson and Damoulas
Proof
Firstly we note that the objective function associated with P(D(γ)
G , ℓn, Q) can be
simpliﬁed by removing the terms in the D(γ)
G that don’t depend on q(θ)
arg min
q∈Q
(
Eq(θ)
" n
X
i=1
ℓ(θ, xi)
#
+ D(γ)
G (q(θ)||π(θ))
)
=
arg min
q∈Q
(
Eq(θ)
" n
X
i=1
ℓ(θ, xi)
#
+
1
γ(γ −1) log Eq(θ)

q(θ)γ−1
−
1
(γ −1) log Eq(θ)

π(θ)γ−1
)
.
We have to consider two cases for γ as the positivity and negativity of γ −1 aﬀect the results
we can use.
Case 1) 0 < γ < 1: Jensen’s inequality and the concavity of the natural logarithm applied
to Eq(θ)

π(θ)γ−1
provides
= Eq(θ)
" n
X
i=1
ℓ(θ, xi)
#
+
1
γ(γ −1) log Eq(θ)

q(θ)γ−1
−
1
(γ −1) log Eq(θ)

π(θ)γ−1
≥Eq(θ)
" n
X
i=1
ℓ(θ, xi)
#
+
1
γ(γ −1) log Eq(θ)

q(θ)γ−1
−Eq(θ) [log π(θ)]
= KLD(q(θ)||πℓ(θ|x)) −log
Z
π(θ) exp
 
−
n
X
i=1
ℓ(θ, xi)
!
dθ
+
1
γ(γ −1) log Eq(θ)

q(θ)γ−1
+ −Eq(θ) [log q(θ)] .
Case 2) γ > 1: Jensen’s inequality and the concavity of the natural logarithm applied to
Eq(θ)
h
q(θ)γ−1 π(θ)γ−1
π(θ)γ−1
i
provides
= Eq(θ)
" n
X
i=1
ℓ(θ, xi)
#
+
1
γ(γ −1) log Eq(θ)

q(θ)γ−1
−
1
(γ −1) log Eq(θ)

π(θ)γ−1
≥Eq(θ)
" n
X
i=1
ℓ(θ, xi)
#
+ 1
γ Eq(θ)

log q(θ)π(θ)
π(θ)

−
1
(γ −1) log Eq(θ)

π(θ)γ−1
= 1
γ
KLD(q(θ)||πγℓ(θ|x)) −1
γ
Z
π(θ) exp
 
−γ
n
X
i=1
ℓ(θ, xi)
!
dθ
+1
γ Eq(θ) [log π(θ)] −
1
(γ −1) log Eq(θ)

π(θ)γ−1
.
Combined these two cases provides the term in Eq. (26) and (27)
D.3.1 Interpretation
Theorems 39 and 40 provide a lower bound on an objective function that is to be minimised
so that interpreting this lower bound provides some insight into the behaviour of the GVI
72

Generalized Variational Inference
posterior. First, we investigate the case where the hyperparameters β and γ are in (0, 1). As
expected, the form of the GVI objective leads us to conclude that the variance will be larger
than that for standard VI within this range of values. Next, we investigate the case where
the hyperparameters β and γ are > 1. Again unsurprisingly, this leads to a shrinkage of the
posterior variance relative to standard VI.
Case 1: 0 < β = γ < 1.
For 0 < β = γ < 1 the terms c(β) = c(γ) and w(β) = w(γ)
produce an objective equivalent to standard VI. This suggests that GVI continues to minimise
the KLD between the variational and standard Bayesian posterior. Unlike standard VI
however, GVI with D = D(β)
B
or D = D(γ)
G additionally minimises the slack terms S1(β, q, π)
or S1(γ, q, π). It is easy to show that the these adjustment terms encourage the solution to
P(D(β)
B , ℓ, Q) with 0 < β < 1 and P(D(γ)
G , ℓ, Q) with 0 < γ < 1 to have greater variance than
the standard VI posterior given by P(KLD, ℓn, Q). For the D(β)
B , we can see this by rewriting
S(0,1)(β, q, π) = −1
β h(β)
T (q(θ)) + hKLD(q(θ)) + 1 −β
β
.
Here, hKLD(q(θ)) is the Shannon entropy of q(θ) and h(β)
T (q(θ)) is the Tsallis entropy of q(θ)
with parameter β. Again applying Lemma 38, we ﬁnd that for 0 < β < 1, h(β)
T (q(θ)) >
hKLD(q(θ)). It immediately follows that minimising −1
βh(β)
T (q(θ))+hKLD(q(θ)) for 0 < β < 1
will make h(β)
T (q(θ)) large—an eﬀect that is achieved by increasing the variance of q(θ).
Applying the same type of logic to the D(γ)
G , one can rewrite
S(0,1)(γ, q, π) = −1
γ h(γ)
R (q(θ)) + hKLD(q(θ)).
As before, hKLD(q(θ)) is the Shannon entropy of q(θ), but unlike before h(γ)
R (q(θ)) is now the
Rényi entropy of q(θ) with parameter γ. With this, one can extend Theorem 3 in Van Erven
and Harremos (2014) to show that h(γ)
R (q(θ)) is decreasing in γ. Since it is also well-known
that limγ→1 h(γ)
R (q(θ)) = hKLD(q(θ)), it follows that minimising −1
γ h(γ)
R (q(θ)) + hKLD(q(θ))
for 0 < γ < 1 will make h(γ)
R (q(θ)) large—an eﬀect that is again achieved by increasing the
variance of q(θ).
Case 2: β = γ = k > 1.
For k = γ = β > 1, c(k) =
1
k and w(k) = k.
Minimis-
ing KLD(q||q∗
k) for k > 1 will encourage P(D(β)
B , ℓ, Q) or P(D(γ)
G , ℓ, Q) to be more concen-
trated around the empirical risk minimizer ˆθn of ℓthan the standard VI posterior given by
P(KLD, ℓ, Q). Additionally, one can show that minimising the adjustment term also favours
shrinking the variance of q(θ). To see this for the case of D(β)
B , rewrite
S(1,∞)(β, q, π) = 1
β Eq(θ) [log(π(θ))] −
1
β −1Eq(θ)
h
π(θ)β−1 −1
i
−1
β .
(28)
Applying Lemma 38 then shows that for β > 1,
1
β −1Eq(θ)
h
π(θ)β−1 −1
i
≥Eq(θ) [log(π(θ))] ≥1
β Eq(θ) [log(π(θ))] .
From this, it follows that minimising Eq. (28) will make
1
β−1Eq(θ)

π(θ)β−1
large. Fixing
π(θ), maximising
1
β−1Eq(θ)

π(θ)β−1
plus 1
β× the Tsallis entropy of q(θ) is equivalent to
73

Knoblauch, Jewson and Damoulas
minimising D(β)
B (q(θ)||π(θ)). Because D(β)
B
is a divergence, this maximization would naturally
seek to choose q(θ) close to π(θ). The Tsallis entropy term in this formulation would have
acted to increase the variance of q(θ). But since we maximize only
1
β−1Eq(θ)

π(θ)β−1
—i.e.
without adding the Tsallis entropy of q(θ)—choices of β > 1 will lead to shrinking the
variance of q(θ) relative to standard VI.
For the D(γ)
G , Jensen’s inequality shows that for γ > 1,
1
γ −1 log Eq(θ)

π(θ)γ−1
≥Eq(θ) [log(π(θ))] ≥1
γ Eq(θ) [log(π(θ))] .
As a result, minimising S(1,∞)(γ, q, π) will seek to make
1
γ−1 log Eq(θ)

π(θ)γ−1
large. Fixing
again π(θ), maximising
1
γ−1 log Eq(θ)

π(θ)β−1
plus 1
γ × the Rényi entropy of q(θ) is equiva-
lent to minimising D(γ)
G (q(θ)||π(θ)), and thus seeks q(θ) close to π(θ). The Rényi entropy
term would have acted to increase the variance of q(θ). Therefore and similarly to the case
of D(β)
B , maximising
1
γ−1 log Eq(θ)

π(θ)γ−1
without adding the Rényi entropy will lead to
shrinkage of the variance of q(θ).
Appendix E. Proof of Proposition 26
Proof Proposition 26 considers the following forms of the prior and likelihood
π(θ|κ0) = h(θ) exp

η(κ0)T T(θ) −A(η(κ0))
	
q(θ|κ) = h(θ) exp

η(κ)T T(θ) −A(η(κ))
	
p(x|θ) = h(θ) exp(g(x)T T(θ) −B(x)),
where A(η(κ)) = log
R
h(θ) exp

η(κ)T T(θ))
	
dθ and h(θ) =
1
R
exp(g(x)T T(θ)−B(x))dx.
The GVI objective function in this scenario, which we term an ELBO as we use the KLD
prior regulariser is
ELBO(κ) = Eq(θ|κ)
" n
X
i=1
ℓ(γ)
G (θ, xi)
#
+ KLD(q(θ|κ)||q(θ|κ0))
=
n
X
i=1
Z
ℓ(γ)
G (θ, xi)
|
{z
}
C1(κ,θ,xi)
q(θ|κ)dθ
|
{z
}
C2(κ,xi)
+ KLD(q(θ|κ)||π(θ|κ0))
|
{z
}
C3(κ,κ0)
.
We have decomposed this into three terms that we need to check are closed forms of κ.
Firstly
C1(κ, θ, xi) = ℓ(γ)
G (xi, θ) = −
1
γ −1p(xi; θ)γ−1
γ
R
p(z; θ)γdz
 γ−1
γ
,
and in order for this to be a closed form function of κ, θ, and xi requires that
I(γ)(θ) =
Z
p(z|θ)γdz =
Z
h(θ)γ exp(γg(z)T T(θ) −γB(z))dz,
74

Generalized Variational Inference
where the theorem statement ensures that I(γ)(θ) is a closed form function of θ. Next
C2(κ, xi)
= −
γ
γ −1
Z
h(θ)γ−1 exp((γ −1)g(xi)T T(θ) −(γ −1)B(xi))
1

h(θ)γI(γ)(θ)
 γ−1
γ
q(θ|κ)dθ
= −
γ
γ −1
exp ((1 −γ)B(xi) + A (η(κ) + (γ −1)g(xi)))
exp (A(η(κ)))
Eq(θ|(η(κ)+(γ−1)g(xi)))
h
I(γ)(θ)
1−γ
γ
i
,
where the theorem statement ensures that (η(κn) + (γ −1)g(xi)) ∈N for all xi and that
F2(κ∗) = Eq(θ|κ∗)
h
I(γ)(θ)
1−γ
γ
i
is closed form function of κ∗for all κ∗∈N. Lastly
C3(κ, κ0) =
Z
h(θ) exp

η(κ)T T(θ) −A(η(κ))
	
log h(θ) exp

η(κ)T T(θ) −A(η(κ))
	
h(θ) exp {η(κ0)T T(θ) −A(η(κ0))}dθ
=A(η(κ0)) −A(η(κ)) + (η(κ) −η(κ0))T Eq(θ|κ) [T(θ)] ,
where the theorem statement ensures that F1(κ∗) = Eq(θ|κ∗) [T(θ)] is a closed form function
of κ∗for all κ∗∈N .
Appendix F. Black Box GVI (BBGVI)
The following sections ﬁrst recall the (implicit and explicit) assumptions one typically makes
for black box VI. They are then compared to assumptions that are reasonable for black box
GVI (BBGVI). The corresponding methods, their special cases and the relevant black box
variance reduction techniques are then derived and elaborated upon. While there are many
black box VI strategies, we center attention on the framework provided for by Ranganath
et al. (2014). Throughout, we denote q(θ) = q(θ|κ) as a posterior distribution in a set of
variational families Q and parameterized by some parameter κ ∈K.
F.1 Preliminaries and assumptions
The variance reduction techniques of Ranganath et al. (2014) crucially rely on three implicit
assumptions that are reasonable for many applications of standard VI.
(A1) Structured mean-ﬁeld variational inference is used, which means that we can factorize
the variational family as Q = {q(θ|κ) = Qk
j=1 qj(θj|κj) : κj ∈Kj for all j}.
(A2) For all factors θj, we have a Markov blanket θ(j) for which we can additively decompose
ℓ(θ, xi) = ℓ(j)(θj, θ(j), xi) + ℓ(−j)(θ−j, xi). Here, ℓ(j) is an additive component of the
loss ℓthat only depends on the j-th factor and its Markov blanket, while ℓ(−j) is
an additive component of the loss that may depend on all of θ except for its j-th
factor. Note that such additivity holds for standard VI for which the likelihood and
the prior are such that the components θj are conditionally independent. In this case,
the conditioning set is the Markov blanket.
(A3) D = 1
w · KLD (with w = 1 for standard VI).
75

Knoblauch, Jewson and Damoulas
Note that (A1) is always satisﬁed for both standard VI and GVI, because any variational
family factorizes into at least a single factor. In contrast, note that (A2) does not even
necessarily hold for standard VI unless one imposes some conditional independence structure
on the θj. For GVI, both (A2) and (A3) do not necessarily hold. If they do however, they
can greatly simplify BBGVI or improve its numerical performance. In the remainder of this
section, we discuss diﬀerent constellations of assumptions and their consequences for BBGVI.
F.2 Standard black box VI with (A2) and (A3)
If the regularizer used is still a rescaled version of the KLD, one recovers an internally rescaled
version of the objective in (Ranganath et al., 2014). Namely, the gradient is given by
Eq(θ|κ)

∇κ log(q(θ|κ))

−
n
X
i=1
ℓ(θ, xi) −wπ(θ) −w log(q(θ|κ))

.
and can be approximated in a smart way by sampling from q(θ|κ), see for instance Ranganath
et al. (2014) for details and the viable strategies for variance reduction. Next, we turn attention
to the cases that are more interesting: If (A3) does not hold (so that D ̸= KLD) and when the
losses are not necessarily negative log likelihoods, meaning that (A2) requires more careful
consideration.
F.3 BBGVI under (A2)
If the losses are decomposable along the factors, two cases need to be distinguished:
(D1) ∇κD(q∥π) has closed form for all q ∈Q;
(D2) D(q∥π) = Eq(θ|κ)

ℓD
κ,π(θ)

for some function ℓD
κ,π : Θ →R.
Under each condition, we ﬁnd a diﬀerent solution using as much of the available information
as possible to improve inference outcomes. For simplicity, we ﬁrst explain how the derivation
works without using the additional information that (A2). In a second step, we shall see how
this additional information can be used for variance reductions in the Rao-Blackwellization
spirit also used by Ranganath et al. (2014).
F.3.1 Gradients if (D1) holds, not using (A2)
In this case, we can obtain the objective given in the main paper. Deﬁne L(q) to be the GVI
objective function of q(θ|κ). It holds that
∇κL(q) = ∇κ
"Z
θ
n
X
i=1
ℓ(θ, xi)q(θ|κ)dθ + D(q||π)
#
=
Z
θ
ℓn(θ, x)∇κq(θ|κ)dθ + ∇κD(q||π)
= Eq(θ|κ)
" n
X
i=1
ℓ(θ, xi)∇κ log(q(θ|κ))
#
+ ∇κD(q||π).
76

Generalized Variational Inference
Correspondingly, the gradient can then be estimated without bias and computing the
corresponding sample average 1
S
PS
s=1 G(θ(s)), where the individual terms are given by
G(θ(s)) =
n
X
i=1
ℓ(θ(s), xi)∇κ log(q(θ(s))) + ∇κD(q||π)
F.3.2 Gradients if (D2) holds, not using (A2)
If the uncertainty quantiﬁer is not available in closed form, one instead can rely on
∇κL(q) = ∇κ
"Z
θ
" n
X
i=1
ℓ(θ, xi) + ℓD
κ,π(θ)
#
q(θ|κ)dθ
#
=
Z
θ
" n
X
i=1
ℓ(θ, xi) + ℓD
κ,π(θ)
#
∇κq(θ|κ)dθ +
Z
θ

∇κℓD
κ,π(θ)

q(θ|κ)dθ
= Eq(θ|κ)
" n
X
i=1
ℓ(θ, xi) + ℓD
κ,π(θ)
!
∇κ log(q(θ|κ))
#
+ Eq(θ|κ)

∇κℓD
κ,π(θ)

.
This derivation is a more general case of the one given in Ranganath et al. (2014), but further
simpliﬁes to the one therein if D = KLD. The gradient is estimated without bias by sampling
θ(1:S) from q(θ|κ) and again computing 1
S
PS
s=1 G(θ(s)) for the slightly diﬀerent
G(θ(s)) =
" n
X
i=1
ℓ(θ(s), xi) + ℓD
κ,π(θ(s))
#
∇κ log(q(θ(s)|κ)) + ∇κℓD
κ,π(θ(s)).
F.3.3 Rao-Blackwellization for variance reduction, using (A2)
If the losses deﬁne a markov blanket over the factors θj, one can employ Rao-Blackwellization
for variance reduction. This is done by rewriting for q−j(θ−j|κ−j) = Qk
l=1,l̸=j ql(θl|κl) the
partial derivatives as
∇κjL(q) = ∇κjEqj(θj|κj)
h
Eq−j(θ−j|κ−j) [L(q)|θj]
i
.
The hope is then to get around computing as many of the inner expectations over q−j(θ−j|κ−j)
as possible. Assume for the moment that at least (D2) holds. Further, denote q−j(θ−j|κ−j) =
q−j, qj(θj|κj) = qj, and in similar fashion the distributions q(j), q−(j), q. Moreover, denote
ℓi = ℓ(θ, xi), ℓD = ℓD
κ,π(θ) and in a similar fashion ℓ(j)
i , ℓ−(j)
i
. Now, assuming that (A2)
holds relative to the factors θj of the variational family Q, one ﬁnds
∇κjL(q) = Eqj
"
∇κj log(qj)
 
Eq−j
" n
X
i=1
ℓ(j)
i
#
+ Eq−j[ℓ−j
n ] + Eq−j[ℓD]
!#
+ Eq−j[∇κjℓD].
Observing that Eqj[∇κj log(qj)] = 0 and that Eq−j[ℓ−(j)] is constant in θj by (A2), this
drastically simpliﬁes to
∇κjL(q) = Eqj
"
∇κj log(qj)Eq−j
" n
X
i=1
ℓ(j)
i
#
+ Eq−j

ℓD + ∇κjℓD
#
.
77

Knoblauch, Jewson and Damoulas
Next, observe that by virtue of how ℓ(j) was constructed, it holds that we can also simplify
Eqj
"
∇κj log(qj)Eq−j
" n
X
i=1
ℓ(j)
i
##
= Eq(j)
" n
X
i=1
ℓ(j)
i
#
.
Putting the above together, we ﬁnally arrive at
∇κjL(q) = Eqj
"
∇κj log(qj)
 
Eq−j
" n
X
i=1
ℓ(j)
i
#
+ Eq−j[ℓD]
!
+ Eq−j[∇κjℓD]
#
= Eq(j)
"
∇κj log(qj)
n
X
i=1
ℓ(j)
i
#
+ Eq

∇κj log(qj)ℓD + ∇κjℓD
.
which is the ﬁnal form under (D1). Should (D1) to hold, one can instead use the lower
variance estimate
∇κjL(q) = Eq(j)
"
∇κj log(qj)
n
X
i=1
ℓ(j)
i
#
+ ∇κjD(q∥π).
These derivations are very similar to the ones in the supplement of Ranganath et al. (2014),
but importantly the former are restricted to negative log likelihood losses. The more general
version presented here holds for arbitrary decomposable losses. The J terms ∇κjL(q) can be
combined into a global gradient estimate simply by setting
∇κL(q) = (∇κ1L(q), ∇κ2L(q), . . . ∇κJL(q))T .
To make the meaning of (A2) more tangible for the case of general losses, we next provide a
short example in the context of multivariate regression.
Example 5 (Markov blankets without conditional independence) Suppose each xi =
(xi,1, xi,2, xi,3)′ consists of three measurements that we wish to relate to some other observables
yi through
xi,1 = a + yib + ξ1
xi,2 = b + yic + ξ2
xi,3 = d + ξ3
where ξj are unknown slack variables (or errors), the parameters of interest are θ =
(a, b, c, d, e) and we wish to produce a belief distribution over θ that is informative about good
values of θ relative to some prediction loss
ℓ(θ, xi) = ∥f1
1 (θ1, θ(1), yi) −xi,1∥p
p + ∥f2
2 (θ1, θ(1), yi) −xi,2∥p
p + ∥f3
2 (θ2, θ(2), yi) −xi,3∥p
p,
where ∥· ∥p
p denotes some p-norm for p ≥1 and fj
l seeks to predict only the l-th dimension
of xi by means of the l-th factor of θ and its blanket. Suppose that fj
l will correspond to the
78

Generalized Variational Inference
l-th row written down in the above model for xi (excluding of course the error term), which
means that
f1
1 (θ1, θ(1)) = a + yib
f2
2 (θ1, θ(1), yi) = b + yic
f3
2 (θ2, θ(2), yi) = d
In this case, the two factors of θ will clearly be given by
θ1 = (a, b, c)T ,
θ2 = (d).
As before, one will in practice need to approximate the gradients with a sample θ(1:S) drawn
from q(θ|κ). For one of the ﬁxed samples θ(s), the relevant terms are computed as
Gj(θ(s)) = ∇κj log(qj(θ(s)
j |κj))
n
X
i=1
ℓ(j)(θ(s)
j , θ(s)
(j), xi) + eD(s, j)
for some function eD(s, j). If (D2) holds and there is no closed form for the uncertainty
quantiﬁer, this function is given by
eD(s, j) = ∇κj log(qj(θ(s)
j |κj))ℓD
π,κ(θ(s)) + ∇κjℓD
π,κ(θ(s))
and in case the stricter requirement (D1) holds, it is simply given by the closed form
eD(s, j) = ∇κjD(q∥π).
F.4 BBGVI if neither (A2) nor (A3) hold
It is of course possible that neither (A2) nor (A3) hold.
Alternatively, it may simply
be convenient to build an implementation that can work reliably without imposing any
assumptions. In this case, one will have to use the naive version of BBGVI that is given in
the main paper and only depends on the distinction between (D2) and (D1). However—even
though we do not do so in our experiments–there still are valid black box variance reduction
techniques for this case. The next section presents these techniques, again by adapting
notation and logic from Ranganath et al. (2014).
F.5 Generically applicable variance reduction
While the Rao-Blackwellization variance reduction will generally be more eﬀective, some
variance reduction techniques can work in circumstances where Rao-Blackwellization does
not. Conversely, this means that if the Rao-Blackwellization is applicable, one can actually
deploy two variance reduction schemes at once to substantially speed up convergence. The
control variate we use is simply
h(θ) = ∇κ log q(θ|κ)
with an optimal scaling parameter that can be estimated as
ˆa∗=
PS
s=1 d
Cov(L(θ(s)), h(θ(s)))
PS
s=1 d
Var(h(θ(s)))
.
79

Knoblauch, Jewson and Damoulas
Based on this, one may now compute the variance reduced term GVR(θ(s)) from G(θ(s)) as
GVR(θ(s)) = G(θ(s)) −ˆa∗· h(θ(s)).
Of course, the exact same logic can be applied to the Rao-Blackwellized terms Gj(θ(s)) to
reduce the variance a second time.
Appendix G. Closed forms for divergences & proof of Proposition 27
This section proves various closed forms for the uncertainty quantiﬁers in the GVI problem.
We do so by proving conditions for closed forms of the αβγ-divergence (D(α,β,r)
G
) introduced
in Appendix A. Note that the special case of these results for the D(α)
AR has been derived
before (see Gil et al., 2013; Gil, 2011; Liese and Vajda, 1987). Unlike previous work, our
results apply to a range of other divergences, too. This is convenient because all other robust
divergences we discuss throughout the paper are special cases of D(α,β,r)
G
.
G.1 High-level overview of results and preliminaries
Summarizing some of the most important ﬁndings of this section, we ﬁnd that if both q(θ)
and π(θ) are in the same exponential variational family Q,
• D(α)
AR(q||π) and D(α)
A (q|π) are always available in closed form if α ∈(0, 1) (see Corollary
45)
• D(α)
AR(q||π) and D(α)
A (q|π) are available in closed form if α > 1 for most exponential
families (see again Corollary 45)
• D(β)
B (q||π) and D(γ)
G (q||π) are available in closed form for β > 1 and γ > 1 for most
exponential families (See Corollary 51).
We note that these ﬁndings are interesting because closed forms for the divergence term
drastically reduce the variance of black box GVI, see also Appendix F. The remainder of this
section is devoted to tedious but rigorous derivations of these ﬁndings. Before stating any
results, it is useful to state the deﬁnition of an exponential family and its natural parameter
space upon which the proofs rely.
Deﬁnition 41 (Exponential families) Object θ ∈Θ ⊂Rd, d ≥1 has an exponential
family distribution with parameters κ ∈K ⊂Rp′, p′ ≥1 if there exist functions η : K →
N ⊂Rp, p ≥1, T : Θ →T ⊂Rp, h : Θ →R≥0 and A : N →R such that
p(θ|η(κ)) = h(θ) exp

η(κ)T T(θ) −A(η(κ))
	
,
where A(η(κ)) = −log
 R
h(θ) exp

η(κ)T T(θ)
	
dθ

.
The set N is called the natural
parameter space and is deﬁned to ensure p(θ|η(κ)) is a normalised probability density,
N = {η(κ) : A(η(κ)) < ∞}.
Throughout the rest of this section, we assume that the following condition holds for both
the prior and the variational family Q.
Condition 1 (The prior and variational families) It holds that
80

Generalized Variational Inference
i) the variational family Q = {q(θ|η(κ))} is an exponential family of the form given by
Deﬁnition 41
ii) the prior π(θ|η(κ0)) is a member of that variational family.
Amongst other things, this implies that the log-normalising constant is a closed form function
of the natural parameters and that we can derive generic conditions for closed forms by using
the canonical representation of exponential families.
To showcase the implications of the derived results, we use the Mulitvariate Gaussian (MVN)
to provide examples along the way.
Deﬁnition 42 (The MVN exponential family) The density of the MVN exponential
family for vector θ of dimension d is p(θ|η(κ)) = h(θ) exp

η(κ)T T(θ) −A(η(κ))
	
where
η(κ) =
 V −1µ
−1
2V −1

T(θ)=
 θ
θθT

h(θ) = (2π)−d/2
A(η(κ))=
1
2 log |V | + 1
2µV −1µ

and the natural parameter space requires that µ is a real valued vector of the same dimension
as θ and V is a d × d symmetric semi-positive deﬁnite matrix.
G.2 Results, proofs & examples
The remainder of this section is structued as follows: First, we give the main result for the
αβγ-divergence (D(α,β,r)
G
) in Proposition 43. This “master result” is then applied to various
special cases for D(α,β,r)
G
that are of practical interest, namely the α-divergence (D(α)
A ), Rényi’s
α-divergence (D(α)
AR), the β-divergence (D(β)
B ) as well the γ-divergence (D(γ)
G ).
G.2.1 Master result for D(α,β,r)
G
While the following result and corresponding proof are somewhat tedious to read, they are
conceptually simple: In fact, all that is needed to derive the results is some basic algebra
and the canonical form of the exponential family.
Proposition 43 (Closed form D(α,β,r)
G
between exponential families) The D(α,β,r)
G
be-
tween a variational posterior q(θ|κn) and prior π(θ|κ0) is available in closed form under the
following conditions
i) η(κ0), η(κn) ∈N ⇒(αη(κ0) + (β −1)η(κn)) ∈N;
ii) Ep(θ|η(κ))

h(θ)α+β−2
is a closed form function of η(κ) ∈N.
If these conditions hold the D(α,β,r)
G
can be written as
˜D(α,β)
G
(q(θ|κn)||π(θ|κ0))
= αB(κn, (α + β −1))E(κn, (α + β −1)) + (β −1)B(κ0, (α + β −1))E(κ0, (α + β −1))
−(α + β −1)C(κn, κ0, α, (β −1)) ˜E(κn, κ0, α, (β −1))
81

Knoblauch, Jewson and Damoulas
where
B(κ, δ) = exp {A(δη(κ))}
exp {A(η(κ))}δ ,
C(κ1, κ2, δ1, δ2) =
exp {A (δ1η(κ1) + δ2)η(κ2))}
exp {A(η(κ1))}δ1 exp {A(η(κ2))}δ2
E(κ, δ) = Ep(θ|δη(κ))
h
h(θ)δ−1i
,
˜E(κ1, κ2, δ1, δ2) = Ep(θ|δ1η(κ1)+δ2η(κ2))
h
h(θ)δ1+δ2−1i
we suppress the dependence of these functions on A(·) and h(·) as these derive form the
deﬁnition of the exponential family (Deﬁnition 41).
Proof The D(α,β,r)
G
is a closed form function of ˜D(α,β)
G
given in Deﬁnition 30. Hence if ˜D(α,β)
G
is available in closed form, then so is D(α,β,r)
G
. In order to ensure that ˜D(α,β)
G
(q(θ|κn)||π(θ|κ0))
has closed form, we need to make sure the three integrals below are available in closed form
for the exponential family.
G1 :=
Z
q(θ|κn)α+β−1dθ,
G2 :=
Z
π(θ|κ0)α+β−1dθ,
G3 :=
Z
q(θ|κn)απ(θ|κ0)β−1dθ.
First we tackle G1.
G1 =
Z
h(θ)α+β−1 exp

(α + β −1)η(κn)T T(θ) −(α + β −1)A(η(κn))
	
dθ
= exp {A((α + β −1)η(κn)) −(α + β −1)A(η(κn))} Ep(θ|(α+β−1)η(κn)
h
h(θ)α+β−2i
,
where condition (i) with η(κ0) = η(κn) ensures that
A((α + β −1)η(κn)) =
Z
h(θ) exp

(α + β −1)η(κn)T T(θ)
	
dθ < ∞,
which in turn ensures that p(θ|(α + β −1)η(κn) is a normalised probability density and that
Ep(θ|(α+β−1)η(κn)

h(θ)α+β−2
is a valid expectation. Now, condition (ii) guarantees this is a
closed form function of η(κn). Similarly for G2,
G2 =
Z
h(θ)α+β−1 exp

(α + β −1)η(κ0)T T(θ) −(α + β −1)A(η(κ0))
	
dθ
= exp {A((α + β −1)η(κ0)) −(α + β −1)A(η(κ0))} Ep(θ|(α+β−1)η(κ0)
h
h(θ)α+β−2i
,
where in analogy to G1, conditions (i) and (ii) with η(κk) = η(κ0) ensure this has a closed
form. Lastly for G3,
G3 =
Z
h(θ)α exp

αη(κn)T T(θ) −αA(η(κn))
	
·h(θ)β−1 exp

(β −1)η(κ0)T T(θ) −(β −1)A(η(κ0))
	
dθ
= exp {A (αη(κn) + (β −1)η(κ0)) −αA(η(κn)) −(β −1)A(η(κ0))}
·Ep(θ|(αη(κn)+(β−1)η(κ0))
h
h(θ)α+β−2i
,
where once again in analogy to G1 and G2, conditions (i) and (ii) ensure this is a closed form
function of η(κn) and η(κ0).
82

Generalized Variational Inference
Therefore, provided conditions (i) and(ii) hold, the integrals G1, G2 and G3 are available
in closed form, implying that the same holds for D(α,β,r)
G
(q(θ|κn)||π(θ|κ0)).
Remark 44 (Conditions of Proposition 43 for the MVN exponential family) In or-
der to illuminate the meaning and generality of the conditions of Theorem 43, we apply them
to the MVN exponential family described in Deﬁnition 42. In this case the two conditions
become:
i) For µ∗:=
(
µ1 + µ2 −
  1
αV1
−1 +

1
β−1V2
−1−1   1
αV1
−1 µ2 +

1
β−1V2
−1
µ1
)
we require that
α
V −1
1
µ1
−1
2V −1
1

+ (β −1)
V −1
2
µ2
−1
2V −1
2

=



  1
αV1
−1 µ1 +

1
β−1V2
−1
µ2
−1
2
  1
αV1
−1 +

1
β−1V2
−1



=




  1
αV1
−1 +

1
β−1V2
−1
µ∗
−1
2
  1
αV1
−1 +

1
β−1V2
−1



∈N
ii) Ep(θ|η(κ))
h
(2π)−d/2(α+β+2)i
= (2π)−d/2(α+β+2) = f(η(κ)) where f is a closed form
function.
Part ii) shows that the second condition is trivially satisﬁed for the MVN exponential family.
Part i) shows that for the MVN exponential family, the ﬁrst condition is satisﬁed provided
(V ∗)−1 =
  1
αV1
−1 +

1
β−1V2
−1
is a positive deﬁnite matrix. This condition is enough
to ensure that V ∗is invertible and thus that µ∗is well-deﬁned. We elaborate further on what
this means for certain parametrisations below.
G.2.2 Corollary: The special cases of D(α)
A , D(α)
AR
Next, we consider the D(α)
A
and D(α)
AR special cases of the D(α,β,r)
G
family. Deﬁnitions 31 and 32
can be used to show that the D(α)
AR is available as the following closed form function of the
D(α)
A . In particular, it holds that
D(α)
AR(q(θ)||π(θ)) =
1
α(α −1) log

1 + α(1 −α)D(α)
A (q(θ)||π(θ))
	
.
(29)
Thus, as demonstrated in Corollary 46 below, the D(α)
A
being available in closed form
immediately provides the D(α)
AR in closed form. Before stating these results, we note that Gil
et al. (2013); Gil (2011); Liese and Vajda (1987) have shown our closed form results for the
D(α)
AR (and thus implicitly the D(α)
A ) before. We nevertheless think there is merit in stating
them, since our results refer to the D(α,β,r)
G
and thus are more general, recovering both the
D(α)
A
and D(α)
AR only as a special case.
83

Knoblauch, Jewson and Damoulas
Corollary 45 (Closed form D(α)
A
for exponential families) The D(α)
A
between a varia-
tional posterior q(θ|κn) and prior π(θ|κ0) is available in closed form under the following
conditions
i) (αη(κn) + (1 −α)η(κ0)) ∈N
and in this case the D(α)
A
can be written as
D(α)
A (q(θ|κn)||π(θ|κ0) =
1
α(1 −α) [1 −C(κn, κ0, α, (1 −α))] ,
where C(κ1, κ2, δ1, δ2) was deﬁned in Proposition 43 .
Proof Following Cichocki and Amari (2010) the single-parameter D(α)
A
is recovered as a
member of the D(α,β,r)
G
family when r = 1 and β = 2 −α. In this situation, Condition (ii) of
Theorem 43 holds automatically and we are left with Condition (i). Substituting β = 2 −α
provides Condition (i) of the Theorem above.
If α ∈(0, 1) then the convexity of the natural parameter space ensures that providing
η(κn) ∈N and η(κ0) ∈N then αη(κn) + (1 −α)η(κ0) ∈N. If α < 0 or α > 1, then this
can no longer be guaranteed.
Corollary 46 is then an immediate consequence of Corollary 45.
Corollary 46 (Closed form D(α)
AR for exponential families) The D(α)
AR between a varia-
tional posterior q(θ|κn) and prior π(θ|κ0) will have closed form providing the D(α)
A
between
the same two densities for the same value of α has closed form.
Proof The proof of this follows immediately from the fact that the D(α)
AR can be recovered
using the closed form function of the D(α)
A
shown in eq. (29)
Remark 47 (Conditions for Corollary 45 for the MVN exponential family) The con-
dition that αη(κn) + (1 −α)η(κ0) ∈N can only be guaranteed for α ∈(0, 1). However we
can see from Remark 44 that provided V ∗=
  1
αV1
−1 +

1
β−1V2
−1−1
is a symmetric
semi-positive deﬁnite ( SPD) matrix for β = 2 −α then this condition will be satisﬁed. For
α > 1 or α < 0 we cannot guarantee that V ∗is SPD. However, we implement the D(α)
AR to
quantify uncertainty for α > 1 in the main paper. Corollary 45 demonstrates that these
parameters will still produce a closed form divergence provided the prior has suﬃciently large
variance, which can always be guaranteed to hold in practice.
G.2.3 Corollary: The special cases of D(β)
B , D(γ)
G
Next, we turn attention to the β- and γ-divergence families. Deﬁnition 34 shows that the D(γ)
G
can be recovered as a closed form function of the terms of the D(β)
B
and thus, as demonstrated
in Corollary 49 below, the D(β)
B
being available in closed form immediately provides that the
D(γ)
G is available in closed form While the conditions for these are slightly more restrictive
than they were for the D(α)
A
and D(α)
AR, one can still obtain closed form uncertainty quantiﬁers
for a large range of settings.
84

Generalized Variational Inference
Corollary 48 (Closed form D(β)
B
for exponential families) The D(β)
B
between a varia-
tional posterior q(θ|κn) and prior π(θ|κ0) is available in closed form under the following
conditions
i) η(κ1), η(κ2) ∈N ⇒((β −1)η(κ1) + η(κ2)) ∈N
ii) Ep(θ|η(κ))

h(θ)β−1
is a closed form function of η(κ) ∈N.
and in this case the D(β)
B
can be written as
D(β)
B (q(θ|κn)||π(θ|κ0)) =
1
β(β −1)B(κn, β)E(κn, β) + 1
β B(κ0, β)E(κ0, β)
−
1
(β −1)C(κn, κ0, 1, (β −1)) ˜E(κn, κ0, 1, (β −1)),
where the functions B(κ, δ), C(κ1, κ2, δ1, δ2), E(κ, δ) and ˜E(κ1, κ2, δ1, δ2) are deﬁned in
Proposition 43.
Proof Following Cichocki and Amari (2010), the single-parameter D(β)
B
is recovered as a
member of the D(α,β,r)
G
family when r = 1 and α = 1. In this situation, Condition (i)-(ii) of
Theorem 43 become (i)-(ii) above.
Corollary 49 is then an immediate consequence of Corollary 48.
Corollary 49 (Closed form D(γ)
G
for exponential families) The D(γ)
G
between a varia-
tional posterior q(θ|κn) and prior π(θ|κ0) will have closed form providing the D(β)
B
between
the same two densities with β = γ has closed form.
Proof The proof of this follows immediately from the fact that the D(γ)
G can be recovered
from the D(β)
B
using closed form function as outlined in Deﬁnition 34.
Remark 50 (Conditions for Corollary 48 under the MVN exponential family)
Fol-
lowing Remark 44, Corollary 48 is satisﬁed providing V ∗=

(Vn)−1 +

1
β−1V0
−1−1
is a
symmetric SPD matrix. The sum of two symmetric SPD matrices is symmetric SPD and
additionally the inverse of a symmetric SPD matrix is also SPD. Therefore provided β > 1 we
can be sure that Condition iii) will be satisﬁed. Similarly to Remark 47, when β < 1 closed
forms will require that the prior has a suﬃciently large variance.
In fact Remark 50 can be extended to many other exponential families if we constrain
β = γ > 1, this is formalised in Corollary 51.
Corollary 51 (Closed form D(β)
B
and D(γ)
G
for exponential families when β = γ > 1)
When β = γ > 1, the conditions for Corollary 48 are satisﬁed by any exponential family
whose h(θ) is a constant function of θ and whose natural parameter space is closed under
addition and scalar multiplication. This includes the Beta, Gamma, Gaussian, exponential
and Laplace families.
Proof The proof of Corollary 51 follows straight from that of Corollary 48.
85

Knoblauch, Jewson and Damoulas
0.07750
0.07875
0.08000
D(0.5)
AR
KLD (VI)
D(2.0)
AR
D(2.5)
AR
D(0.5)
AR
D(0.5)
A
D(0.0)
A
DVI
GVI
kin8mn
0.630
0.635
0.640
wine
0.000
0.002
0.004
0.006
naval
1.20
1.18
1.16
1.14
1.12
D(0.5)
AR
KLD (VI)
D(2.0)
AR
D(2.5)
AR
D(0.5)
AR
D(0.5)
A
D(0.0)
A
DVI
GVI
0.95
0.96
0.97
0.98
6.5
6.0
5.5
5.0
4.5
4.0
Figure 21: Top row depicts RMSE, bottom row the NLL across a range of data sets using
BNNs. Dots correspond to means, whiskers the standard errors. The further to the left,
the better the predictive performance. For the depicted selection of data sets, no common
pattern exists for the performance diﬀerences between standard VI, DVI and GVI.
Appendix H. Experiments
While the most interesting ﬁndings of our numerical studies can be found in the main paper,
here we give a brief overview over additional results. More importantly, we state the proofs
for the theoretical groundwork necessary to deploy GVI on DGPs.
H.1 Bayesian Neural Networks (BNNs)
We provide two more sets of experiments for further insights into BNNs. The ﬁrst set consists
in three more data sets with the same settings as used in the main paper. While these ﬁndings
do not change the overall picture, they do require more careful analysis and dissemination.
The second set of results investigates the interaction between robustifying inference relative
to the loss with robustifying it relative to the prior. The results suggest a clear relationship
for predictive performance as measured by the root mean square error: If robust losses are
used, the KLD generally performs better. Moreover, the combination of robust loss and
D = KLD outperforms VI and the investigated DVI methods on all data sets studied. The
relationship is less clear for the predictive negative log likelihood, both between loss and
uncertainty quantiﬁer as well as between the performance to be expected under GVI, VI and
DVI.
H.1.1 First set of additional experiments (Figure 21)
Figure 21 provides the predictive outcomes on three more data sets using the exact same
settings and experimental setup as described in the main paper. The ﬁndings generally
86

Generalized Variational Inference
2.9
3.0
3.1
3.2
KLD (VI)
D(0.5)
AR ,
= 1.01
KLD,
= 1.01
D(0.5)
AR ,
= 1.01
KLD,
= 1.01
D(0.5)
AR
D(0.5)
A
D(0.0)
A
DVI
GVI
boston
5.0
5.1
5.2
5.3
5.4
concrete
1.2
1.3
1.4
energy
0.8
1.0
1.2
yacht
2.5
2.6
2.7
KLD (VI)
D(0.5)
AR ,
= 1.01
KLD,
= 1.01
D(0.5)
AR ,
= 1.01
KLD,
= 1.01
D(0.5)
AR
D(0.5)
A
D(0.0)
A
DVI
GVI
3.1
3.2
3.3
1.5
1.6
1.7
1.8
1.70
1.75
1.80
Figure 22: Top row depicts RMSE, bottom row the NLL across a range of data sets using
BNNs. Dots correspond to means, whiskers the standard errors. The further to the left, the
better the predictive performance. For the depicted selection of data sets, patterns exists for
the interplay between the loss and uncertainty quantiﬁer for GVI.
reinforce the ﬁndings of the main paper. First, while the GVI methods with α > 1 still
perform as good as or better than standard VI on the kin8mn data set, DVI methods show
a clear performance gain relative to either of the two. Crucially, it is not clear what leads
to this improvement gain, though the fact that the best-performing DVI method is the one
recovering EP (D(α)
A
for α = 0) suggests that there is tangible merit in producing mass-
covering approximations to the posterior of θ on this data set. While the deployment of
DVI methods looks tempting on the kin8mn data set, the results on the naval data set are a
reminder that the behaviour of these methods is in many ways unpredictable. Moreover, it
shows that the risks we identiﬁed in Example 4 readily translate into real world applications:
By using DVI methods, we may accidentally conﬂate the role of the loss and the role of
uncertainty quantiﬁcation. If the loss is well-suited for the data at hand—as the RMSE
panel suggests it is in the naval case—the mass-covering behaviour of DVI methods can
be extremely detrimental. Lastly, the wine data set provides a very similar picture to the
results in Figure 11: Varying α introduces a banana-shaped curve for the GVI methods. As
it so happens, the ideal choice of α on the wine data set appears to be around α = 1 (i.e.,
standard VI). Taking into account the predictive uncertainty in form of the whiskers, it is
doubtful if any of the methods is dominating another one on wine. Presumably, the reason
for this is that the true posterior is relatively well approximated with the mean ﬁeld normal
family, yielding very similar results across all settings.
87

Knoblauch, Jewson and Damoulas
H.1.2 Second set of additional experiments (Figure 22)
In a second set of additional experiments, we varied the loss function to be a robust scoring
rule. Speciﬁcally, we used scoring rules based on the β-divergence and the γ-divergence.
See Section 6.2.3 for the deﬁnition and more detail on these robust scoring rules. As for
the DGP examples, we choose values of the scoring rule that are close to the log score, but
suﬃciently far to induce robust behaviour. All settings for optimization, initialization as
well as the code are the same as for the results provided in the main paper. Figure 22 shows
the results: For the RMSE, the results are unambiguous: Combining a robust scoring rule
with the standard uncertainty quantiﬁer D = KLD appears to be the winning combination
across all four data sets. The picture is less clear for the NLL: Relative to both VI and DVI,
the performance gains depend on the data set. Even within the class of GVI posteriors, it is
data-set dependent which uncertainty quantiﬁer should be chosen: For example, it is clearly
beneﬁcial to choose the D(α)
AR as uncertainty quantiﬁer in the boston and concrete data sets,
but the opposite is true on the yacht data set. Above all other things, this highlights the
need for a good selection strategy of GVI hyperparameters: Oftentimes, intuitions about the
correct uncertainty quantiﬁer or the appropriate loss may be incorrect.
H.2 Deep Gaussian Processes (DGPs)
Unlike BNNs, DGPs require some theoretical groundwork before they are amenable to changes
in the loss and uncertainty quantiﬁer. Speciﬁcally, we need to show that it is valid to deﬁne
new divergences layer-wise. Moreover, while not required it is beneﬁcial if one can obtain
closed forms for the robustiﬁed likelihood terms. The following sections proceed to do both.
Thereafter, we also show an additional short example to illustrate the eﬀect of changing the
uncertainty quantiﬁer in DGPs.
H.2.1 Proof of Corollary 28
We ﬁrst prove a Lemma that plays a key role in the proof of Corollary 28.
Lemma 52 (Divergence recombination) Let Dl be divergences and cl > 0 scalars for
l = 1, 2, . . . , L.
Further, denote θ−l = θ1:l−1,l+t:L and let ql(θl|θ′
−l) and πl(θl|θ′
−l) be
the conditional distributions of θl for q(θ) and π(θ) conditioned on θ−l = θ′
−l.
Then,
Dθ′(q||π) = PL
l=1 clDl
 ql(θl|θ′
−l)||πl(θl|θ′
−l)

is a divergence between q(θ) and π(θ) if (i)
Dθ◦(q||π) = Dθ′(q||π) for all conditioning sets θ◦, θ′ and (ii) a Hammersley-Cliﬀord Theorem
holds for the collection of conditionals πl(θl|θ′
−l) and ql(θl|θ′
−l).
Proof First, observe by deﬁnition of a divergence, Dl(ql(θl|θ′−l)||πl(θl|θ′−l)) = 0 for all l
and over all potential conditioning sets θ′ holds if and only if ql(θl|θ′−l) = πl(θl|θ′−l). Next,
note that we have assumed that Dθ′(q||π) = Dθ◦(q||π) for all conditioning sets θ′, θ◦. In other
words, if Dθ′(q||π) = 0 for some θ′, then it will also be 0 for any conditioning set θ◦. This
immediately entails that for arbitrary θ′, Dθ′(q||π) = 0 if and only if ql(θl|θ′
−l) = πl(θl|θ′
−l)
for all l and for any choice of θ′−l. In other words, the conditionals are the same. Since
the positivity condition holds, we can then apply the Hammersley-Cliﬀord Theorem to
conclude that the conditionals fully specify the joint. This ﬁnally yields the desired result:
Dθ′(q||π) = 0 if and only if q(θ) = π(θ).
88

Generalized Variational Inference
With this technical result in hand, one can now prove Corollary 28, which shows that reverse-
engineering prior regularizers inspired by eq. (20) is feasible so long as the layer-speciﬁc
divergences Dl are f-divergences or monotonic transformations of f-divergences.
Proof Suppressing again Zl and X for readability, ﬁrst recall that
q({U l}L
l=1, {F l}L
l=1) =
L
Y
l=1
p(F l|U l, F l−1)q(U l)
p({U l}L
l=1, {F l}L
l=1) =
L
Y
l=1
p(F l|U l, F l−1)p(U l)
and write for a ﬁxed conditioning set {F l
◦}L
l=1 the new divergence
D{F l
◦}L
l=1  q({Ul}L
l=1, {Fl}L
l=1)∥p({Ul}L
l=1, {Fl}L
l=1)

=
L
X
l=1
Dl 
p(F l|U l, F l−1
◦
)q(U l)∥p(F l|U l, F l−1
◦
)p(U l)

=
L
X
l=1
Dl 
q(U l)∥p(U l)

The ﬁrst equality is simply the deﬁnition of the new divergence. The second equality follows
by virtue of Dl being a monotonic function of an f-divergences or an f-divergence for all l,
which ensures that the l-th term is given by
Dl 
p(F l|U l, F l−1
◦
)q(U l)∥p(F l|U l, F l−1
◦
)p(U l)

(30)
= g

Ep(F l|Ul,F l−1
◦
)p(Ul)

f
p(F l|U l, F l−1
◦
)q(U l)
p(F l|U l, F l−1
◦
)p(U l)

.
= g

Ep(Ul)

f
q(U l)
p(U l)

= Dl 
q(U l)∥p(U l)

.
Now note that we can invoke Lemma 52: The ﬁrst condition is satisﬁed because the derivation
was independent of the chosen {F l
◦}L
l=1. The second condition is satisﬁed as both conditionals
satisfy the positivity condition required for the Hammersley-Cliﬀord Theorem to hold.
H.2.2 Proof of Proposition 29
Proof The likelihood is Gaussian with a ﬁxed variance parameter σ2, i.e. for yi ∈Rd with
i = 1, 2, . . . , n
p(yi|f L
i ) = (2πσ2)−0.5d exp

−1
2σ2 (yi −f L
i )T (yi −f L
i )

With this, note that integrating out the normal density yields
Ip,c(f L
i ) = (2πσ2)−0.5dcc−0.5d.
(31)
Note in particular that this is a constant and does not depend on f, which makes computing
the expectation over q(f L
i ) depend only on the power likelihood. Next, we show that the
89

Knoblauch, Jewson and Damoulas
power likelihood is also available in closed form. This is laborious but not diﬃcult and relies
on the same algebraic tricks in the Appendix of Knoblauch et al. (2018). To simplify notation,
we write f = f L
i . Note also that the variational parameters µ and Σ are (stochastic)
functions of the draws of f 1:L−1
i
from the previous layers, but we suppress this dependency,
again for readability.
Eq(f|µ,Σ)
1
cp(yi|f)c

= 1
c(2πσ2)−0.5dc · Eq(f|µ,Σ)
h
exp
n
−c
2σ2 (yT
i yi + f T f −2f T yi)
oi
= 1
c(2πσ2)−0.5dc exp
n
−c
2σ2 yT
i yi
o
· Eq(f|µ,Σ)
h
exp
n
−c
2σ2 (f T f −2f T yi)
oi
= 1
c(2πσ2)−0.5dc(2πσ2)−0.5d|Σ|−0.5 exp
n
−c
2σ2 yT
i yi
o
×
Z
exp

−1
2
 c
σ2 f T f −2c
σ2 f T yi + (f −µ)T Σ−1(f −µ)

df
= 1
c(2πσ2)−0.5dc(2π)−0.5d|Σ|−0.5 exp

−1
2
 c
σ2 yT
i yi + µT Σ−1µ

×
Z
exp

−1
2
 c
σ2 f T f −2c
σ2 f T yi + f T Σ−1f −2f T Σ−1µ

df
The integral suggests one can obtain a closed form through the Gaussian integral by completing
the squares. Deﬁning eΣ−1 =
  c
σs Id + Σ−1
, eµ =
  c
σ2 yi + Σ−1µ

, bµ = eΣeµ, one indeed has
c
σ2 f T f −2c
σ2 f T yi + f T Σ−1f −2f T Σ−1µ = f T 
Id
c
σ2 + Σ−1
f −2f T  c
σ2 yi + Σ−1µ

= (f −bµ)T eΣ−1 (f −bµ) −eµT eΣeµ,
which allows us to ﬁnally rewrite the integral as
Z
exp

−1
2
 c
σ2 f T f −2c
σ2 f T yi + f T Σ−1f −2f T Σ−1µ

df
= exp

−1
2 eµT eΣeµ
 Z
exp

−1
2 (f −bµ)T eΣ−1 (f −bµ)

df = exp
1
2 eµT eΣeµ

(2π)0.5d|eΣ|0.5.
Putting everthing together and simplifying expressions, this means that
Eq(f|µ,Σ)
1
cp(yi|f)c

= 1
c
 2πσ2−0.5dc |eΣ|0.5
|Σ|0.5 exp

−1
2
 c
σ2 yT
i yi + µT Σ−1µ −eµT eΣeµ

Depending on whether one uses the β- or γ-divergence for robustifying the loss, one thus
obtains the closed form expressions
Eq(f|µ,Σ)

−
1
β −1p(yi|f)β−1 + Ip,β(f)
β

= Eq(f|µ,Σ)

−
1
β −1p(yi|f)β−1

+ Ip,β(f)
β
Eq(f|µ,Σ)
"
−
1
γ −1p(yi|f)γ−1 ·
γ
Ip,γ(f)
γ−1
γ
#
= Eq(f|µ,Σ)

−
1
γ −1p(yi|f)γ−1

·
γ
Ip,γ(f)
γ−1
γ
,
with the expectation over q(f|µ, Σ) as in and the integrals Ip,β(f), Ip,γ(f) as deﬁned above.
Note that we have derived the general case for yi ∈Rd, where Σ, f and µ are matrix- and
90

Generalized Variational Inference
4.5
5.0
5.5
6.0
6.5
D3 = D(0.5)
AR
1/w=2.0
VI (1/w = 1.0)
1/w=0.5
p,
= 1.01, D3 = D(0.5)
AR
p,
= 1.01, 1/w = 2.0
p,
= 1.01, 1/w = 1.0
p,
= 1.01, 1/w = 0.5
p,
= 1.05, D3 = D(0.5)
AR
p,
= 1.05, 1/w = 2.0
p,
= 1.05, w = 1.0
p,
= 1.05, 1/w = 0.5
concrete
0.50
0.75
1.00
1.25
1.50
1.75
energy
0.060 0.061 0.062 0.063 0.064 0.065 0.066
kin8mn
3.0
3.5
4.0
4.5
5.0
5.5
D3 = D(0.5)
AR
1/w=2.0
VI (1/w = 1.0)
1/w=0.5
p,
= 1.01, D3 = D(0.5)
AR
p,
= 1.01, 1/w = 2.0
p,
= 1.01, 1/w = 1.0
p,
= 1.01, 1/w = 0.5
p,
= 1.05, D3 = D(0.5)
AR
p,
= 1.05, 1/w = 2.0
p,
= 1.05, w = 1.0
p,
= 1.05, 1/w = 0.5
concrete
0.75
1.00
1.25
1.50
1.75
2.00
energy
1.38
1.36
1.34
1.32
1.30
1.28
kin8mn
Figure 23: Best viewed in color. Top row depicts RMSE, bottom row the NLL across a range
of data sets using DGPs with L = 3 layers. Dots correspond to means, whiskers the standard
errors. The further to the left, the better the predictive performance.
vector-valued.
In fact, we can simplify everything even further in the univariate case. We summarize this in
the next part.
Remark 53 Since the derivation of Salimbeni and Deisenroth (2017) shows that one in
fact only needs to integrate over the marginals f L
i , if d = 1 (as in all experiments in both
this paper and (Salimbeni and Deisenroth, 2017)), the computation corresponding to the
expression above simpliﬁes considerably as no matrix inverses and determinants are needed.
In particular, denoting the uni-variate mean and variance parameters as µ, Σ and deﬁning
eΣ =
1
c
σs + 1
Σ and eµ =
  cyi
σ2 + µ
Σ

, the expectation term over the posterior q simpliﬁes to
Eq(f|µ,Σ)
1
cp(yi|f)c

= 1
cs
 2πσ2−0.5c
s
eΣ
Σ · exp

−1
2
cy2
i
σ2 + µ2
Σ −eµ2eΣ

.
91

Knoblauch, Jewson and Damoulas
H.2.3 Additional experiments varying D (Figure 23)
While we showed that DGPs allow for the variation of both losses and uncertainty quantiﬁers,
the main paper did not use the ﬂexibility aﬀorded by varying D. The main reason for this is
that much like for the BNNs, the results when jointly varying loss and uncertainty quantiﬁer
are less intuitively interpretable. We showcase this in Figure 23, which compares a number
of diﬀerent GVI posteriors for DGPs with L = 3 layers. The loss is either the robust loss Lγ
p
for γ ∈{1.01, 1.05} (top 8 entries in each row) or the standard log score (bottom 4 entries
in each row). We also compare D = 1
w KLD for w = 2.0, 1.0, 0.5 as well as the composite
layer-wise divergence
D(q∥π) =
3
X
l=1
Dl(ql∥πl),
D1 = D2 = KLD, D3 = D(α)
AR for α = 0.5.
Aligned with the intuition that the priors in DGPs are rather informative due to various
hyperparameter optimization schemes, changing the uncertainty quantiﬁer from the KLD
to the D(α)
AR generally typically has either fairly little or even adverse impact. Similarly, up-
or down-weighting the KLD seems not to be beneﬁcial across the board and will depend on
the loss function. For the case of the log score however, we ﬁnd a consistent improvement
for down-weighting the KLD: Predictively, it improves the predictions on both metrics
and across all data sets relative to standard VI. Similarly, up-weighting the KLD term is
counterproductive under the log score and yields a performance deterioration across all data
sets. This indicates that despite best eﬀorts to the contrary, DGPs are probably violating (P)
so that their predictive performance can be enhanced by ignoring more prior information,
ensuring posteriors that are concentrated around the empirical risk minimizer.
References
Ryan Prescott Adams and David J. C. MacKay. Bayesian online changepoint detection.
arXiv preprint arXiv:0710.3742, 2007.
James Aitchison. Goodness of prediction ﬁt. Biometrika, 62(3):547–554, 1975.
Alexander A. Alemi. Variational predictive information bottleneck. In Workshop on Infor-
mation Theory, Advances in Neural Information Processing Systems, 2019.
Pierre Alquier and Benjamin Guedj. Simpler PAC-Bayesian bounds for hostile data. Machine
Learning, 107(5):887–902, 2018.
Pierre Alquier, James Ridgway, and Nicolas Chopin.
On the properties of variational
approximations of Gibbs posteriors. The Journal of Machine Learning Research, 17(1):
8374–8414, 2016.
Shun-ichi Amari. Diﬀerential-geometrical methods in statistics, volume 28. Springer Science
& Business Media, 2012.
Maximilian Balandat, Brian Karrer, Daniel R. Jiang, Samuel Daulton, Benjamin Letham, An-
drew Gordon Wilson, and Eytan Bakshy. BoTorch: Programmable Bayesian optimization
in PyTorch. arXiv preprint arXiv:1910.06403, 2019.
92

Generalized Variational Inference
Alessandro Barp, Francois-Xavier Briol, Andrew B. Duncan, Mark Girolami, and Lester
Mackey. Minimum Stein discrepancy estimators. Advances in Neural Information Processing
Systems, 2019.
Ayanendranath Basu, Ian R. Harris, Nils L. Hjort, and M. C. Jones. Robust and eﬃcient
estimation by minimising a density power divergence. Biometrika, 85(3):549–559, 1998.
Thomas Bayes. An essay towards solving a problem in the doctrine of chances. Philosophical
Transactions of the Royal Society of London, 53:370–418, 1763.
Matthew James Beal. Variational algorithms for approximate Bayesian inference. University
College London, 2003.
Luc Bégin, Pascal Germain, François Laviolette, and Jean-Francis Roy. PAC-Bayesian bounds
based on the Rényi divergence. In Artiﬁcial Intelligence and Statistics, pages 435–444,
2016.
James O. Berger. The case for objective Bayesian analysis. Bayesian analysis, 1(3):385–402,
2006.
James O Berger and José M Bernardo. On the development of the reference prior method.
Bayesian statistics, 4(4):35–60, 1992.
James O. Berger, Elías Moreno, Luis Raul Pericchi, M. Jesús Bayarri, José M. Bernardo,
Juan A. Cano, Julián De la Horra, Jacinto Martín, David Ríos-Insúa, Dasgupta A. Betrò,
Bruno, Paul Gustafson, Larry Wasserman, Joseph B. Kadane, Cid Srinivasan, Michael
Lavine, Anthony O’Hagan, Wolfgang Polasek, Christian P. Robert, Constantinos Goutis,
Fabrizio Ruggeri, Gabriella Salinetti, and Siva Sivaganesan. An overview of robust Bayesian
analysis. Test, 3(1):5–124, 1994.
Jose M. Bernardo. Reference posterior distributions for Bayesian inference. Journal of the
Royal Statistical Society: Series B (Methodological), 41(2):113–128, 1979.
José M. Bernardo. Bayesian theory. Wiley Series in Probability and Statistics. 23 cm. 586
p., 2000.
Alexandros Beskos, Natesh Pillai, Gareth Roberts, Jesus-Maria Sanz-Serna, and Andrew
Stuart. Optimal tuning of the hybrid Monte Carlo algorithm. Bernoulli, 19(5A):1501–1534,
2013.
William Bialek, Ilya Nemenman, and Naftali Tishby. Predictability, complexity, and learning.
Neural computation, 13(11):2409–2463, 2001.
Pier Giovanni Bissiri, Chris Holmes, and Stephen Walker. A general framework for up-
dating belief distributions. Journal of the Royal Statistical Society: Series B (Statistical
Methodology), 78(5):1103–1130, 2016.
Edwin V. Bonilla, Karl Krauth, and Amir Dezfouli. Generic inference in latent Gaussian
process models. Journal of Machine Learning Research, 20(117):1–63, 2019.
93

Knoblauch, Jewson and Damoulas
Samuel R. Bowman, Luke Vilnis, Oriol Vinyals, Andrew Dai, Rafal Jozefowicz, and Samy
Bengio. Generating sentences from a continuous space. In Proceedings of The 20th SIGNLL
Conference on Computational Natural Language Learning, pages 10–21, Berlin, Germany,
August 2016. Association for Computational Linguistics.
George E. P. Box. Sampling and Bayes’ inference in scientiﬁc modelling and robustness.
Journal of the Royal Statistical Society. Series A (General), pages 383–430, 1980.
Thang Bui, Daniel Hernández-Lobato, Jose Hernandez-Lobato, Yingzhen Li, and Richard
Turner. Deep Gaussian processes for regression using approximate expectation propagation.
In International Conference on Machine Learning, pages 1472–1481, 2016.
Yuri Burda, Roger Grosse, and Ruslan Salakhutdinov. Importance weighted autoencoders.
In International Conference on Learning Representations, 2016.
François Caron, Arnaud Doucet, and Raphael Gottardo. On-line changepoint detection and
parameter estimation with application to genomic data. Statistics and Computing, 22(2):
579–595, 2012.
Liqun Chen, Chenyang Tao, Ruiyi Zhang, Ricardo Henao, and Lawrence Carin Duke.
Variational inference and model selection with generalized evidence bounds. In International
Conference on Machine Learning, pages 892–901, 2018.
Badr-Eddine Chérief-Abdellatif and Pierre Alquier. MMD-Bayes: Robust Bayesian estimation
via maximum mean discrepancy. arXiv preprint arXiv:1909.13339, 2019.
Herman Chernoﬀ. A measure of asymptotic eﬃciency for tests of a hypothesis based on the
sum of observations. The Annals of Mathematical Statistics, 23(4):493–507, 1952.
Anna Choromanska, Mikael Henaﬀ, Michael Mathieu, Gérard Ben Arous, and Yann LeCun.
The loss surfaces of multilayer networks. In Artiﬁcial Intelligence and Statistics, pages
192–204, 2015.
Andrzej Cichocki and Shun-ichi Amari. Families of alpha-beta-and gamma-divergences:
Flexible and robust measures of similarities. Entropy, 12(6):1532–1568, 2010.
Imre Csiszár. I-divergence geometry of probability distributions and minimization problems.
The Annals of Probability, pages 146–158, 1975.
Kurt Cutajar, Edwin V Bonilla, Pietro Michiardi, and Maurizio Filippone. Random feature
expansions for deep Gaussian processes. In Proceedings of the 34th International Conference
on Machine Learning-Volume 70, pages 884–893. JMLR, 2017a.
Kurt Cutajar, Edwin V Bonilla, Pietro Michiardi, and Maurizio Filippone. Random feature
expansions for deep Gaussian processes. In Proceedings of the 34th International Conference
on Machine Learning-Volume 70, pages 884–893. JMLR. org, 2017b.
Z. Dai, A. Damianou, J. Gonzalez, and N. Lawrence. Variational auto-encoded deep Gaussian
processes. In International Conference on Learning Representations, 2016.
94

Generalized Variational Inference
Andreas Damianou and Neil Lawrence. Deep Gaussian processes. In Artiﬁcial Intelligence
and Statistics, pages 207–215, 2013.
Herbert Aron David. First (?) occurrence of common terms in probability and statistics—a
second list, with corrections. The American Statistician, 52(1):36–40, 1998.
A Philip Dawid, Monica Musio, and Laura Ventura.
Minimum scoring rule inference.
Scandinavian Journal of Statistics, 43(1):123–138, 2016.
Pierre-Simon De Laplace. Mémoire sur la probabilité des causes par les événements. Mém.
de math. et phys. présentés à l’Acad. roy. des sci, 6:621–656, 1774.
Arthur P Dempster, Nan M Laird, and Donald B Rubin. Maximum likelihood from in-
complete data via the EM algorithm. Journal of the Royal Statistical Society: Series B
(Methodological), 39(1):1–22, 1977.
Adji Bousso Dieng, Dustin Tran, Rajesh Ranganath, John Paisley, and David Blei. Variational
inference via χ upper bound minimization. In Advances in Neural Information Processing
Systems, pages 2732–2741, 2017.
Justin Domke and Daniel R. Sheldon. Importance weighting and variational inference. In
Advances in neural information processing systems, pages 4470–4479, 2018.
Monroe D Donsker and SR Srinivasa Varadhan. Asymptotic evaluation of certain Markov
process expectations for large time, i. Communications on Pure and Applied Mathematics,
28(1):1–47, 1975.
Paul Fearnhead and Zhen Liu. On-line inference for multiple changepoint problems. Journal
of the Royal Statistical Society: Series B (Statistical Methodology), 69(4):589–605, 2007.
Stephen E Fienberg. When did Bayesian inference become "Bayesian"? Bayesian analysis, 1
(1):1–40, 2006.
Ronald Aylmer Fisher. Contributions to mathematical statistics. 1950.
Edwin Fong and Chris Holmes. On the marginal likelihood and cross-validation. arXiv
preprint arXiv:1905.08737, 2019.
Hironori Fujisawa and Shinto Eguchi. Robust parameter estimation with a small bias against
heavy contamination. Journal of Multivariate Analysis, 99(9):2053–2081, 2008.
Futoshi Futami, Issei Sato, and Masashi Sugiyama. Variational inference based on robust
divergences. In Proceedings of the Twenty-First International Conference on Artiﬁcial
Intelligence and Statistics, volume 84 of Proceedings of Machine Learning Research, pages
813–822. PMLR, 2018.
Kuzman Ganchev, Jennifer Gillenwater, and Ben Taskar. Posterior regularization for struc-
tured latent variable models. Journal of Machine Learning Research, 11:2001–2049, 2010.
95

Knoblauch, Jewson and Damoulas
Jacob Gardner, GeoﬀPleiss, Kilian Q. Weinberger, David Bindel, and Andrew G. Wilson.
Gpytorch: Blackbox matrix-matrix Gaussian process inference with GPU acceleration. In
Advances in Neural Information Processing Systems, pages 7576–7586, 2018.
Pascal Germain, Francis Bach, Alexandre Lacoste, and Simon Lacoste-Julien. PAC-Bayesian
theory meets Bayesian inference. In Advances in Neural Information Processing Systems,
pages 1884–1892, 2016.
Subhashis Ghosal. A review of consistency and convergence rates of posterior distributions.
In Proc. Varanasi Symp. on Bayesian Inference, 1998.
Subhashis Ghosal, Jayanta K Ghosh, and Aad W Van Der Vaart. Convergence rates of
posterior distributions. Annals of Statistics, 28(2):500–531, 2000.
Abhik Ghosh and Ayanendranath Basu. Robust Bayes estimation using the density power
divergence. Annals of the Institute of Statistical Mathematics, 68(2):413–437, 2016.
Manuel Gil. On Rényi divergence measures for continuous alphabet sources. PhD thesis,
2011.
Manuel Gil, Fady Alajaji, and Tamas Linder. Rényi divergence measures for commonly used
univariate continuous distributions. Information Sciences, 249:124–131, 2013.
M Goldstein. Inﬂuence and belief adjustment. Inﬂuence Diagrams, Belief Nets and Decision
Analysis, pages 143–174, 1990.
Michael Goldstein. Subjective Bayesian analysis: principles and practice. Bayesian Analysis,
1(3):403–420, 2006.
Peter Grünwald. Safe learning: bridging the gap between Bayes, MDL and statistical learning
theory via empirical convexity. In Proceedings of the 24th Annual Conference on Learning
Theory, pages 397–420, 2011.
Peter Grünwald. The safe Bayesian. In International Conference on Algorithmic Learning
Theory, pages 169–183. Springer, 2012.
Peter Grünwald and Thijs Van Ommen. Inconsistency of Bayesian inference for misspeciﬁed
linear models, and a proposal for repairing it. Bayesian Analysis, 12(4):1069–1103, 2017.
Benjamin Guedj. A primer on PAC-Bayesian learning. arXiv preprint arXiv:1901.05353,
2019.
Oliver Hamelijnck, Theodoros Damoulas, Kangrui Wang, and Mark Girolami. Multi-resolution
multi-task gaussian processes. In Advances in Neural Information Processing Systems,
2019.
Frank R Hampel, Elvezio M Ronchetti, Peter J Rousseeuw, and Werner A Stahel. Robust
statistics: the approach based on inﬂuence functions, volume 196. John Wiley & Sons,
2011.
96

Generalized Variational Inference
Pashupati Hegde, Markus Heinonen, Harri Lähdesmäki, and Samuel Kaski. Deep learning
with diﬀerential gaussian process ﬂows. In The 22nd International Conference on Artiﬁcial
Intelligence and Statistics, pages 1812–1821, 2019.
James Hensman and Neil D. Lawrence. Nested variational compression in deep Gaussian
processes. stat, 1050:3, 2014.
José Miguel Hernández-Lobato and Ryan Adams. Probabilistic backpropagation for scalable
learning of Bayesian neural networks. In International Conference on Machine Learning,
pages 1861–1869, 2015.
José Miguel Hernández-Lobato, Yingzhen Li, Mark Rowland, Daniel Hernández-Lobato,
Thang D Bui, and Richard E Turner. Black-box α-divergence minimization. In Proceedings
of the 33rd International Conference on International Conference on Machine Learning-
Volume 48, pages 1511–1520, 2016.
Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew
Botvinick, Shakir Mohamed, and Alexander Lerchner.
beta-vae: Learning basic vi-
sual concepts with a constrained variational framework. In International Conference on
Learning Representations, volume 3, 2017.
Matthew D Hoﬀman, David M Blei, Chong Wang, and John Paisley. Stochastic variational
inference. The Journal of Machine Learning Research, 14(1):1303–1347, 2013.
Chris Holmes and Stephen Walker. Assigning a value to a power likelihood in a general
bayesian model. Biometrika, 104(2):497–503, 2017.
Giles Hooker and Anand N Vidyashankar. Bayesian model robustness via disparities. Test,
23(3):556–584, 2014.
Chin-Wei Huang, Shawn Tan, Alexandre Lacoste, and Aaron C. Courville. Improving
explorability in variational inference with annealed variational objectives. In Advances in
Neural Information Processing Systems, pages 9724–9734, 2018.
Hung Hung, Zhi-Yu Jou, and Su-Yun Huang. Robust mislabel logistic regression without
modeling mislabel probabilities. Biometrics, 74(1):145–154, 2018.
Aapo Hyvärinen. Estimation of non-normalized statistical models by score matching. Journal
of Machine Learning Research, 6:695–708, 2005.
Martin Jankowiak, GeoﬀPleiss, and Jacob R Gardner. Sparse gaussian process regression
beyond variational inference. arXiv preprint arXiv:1910.07123, 2019.
Edwin T. Jaynes. Probability theory: The logic of science. Cambridge university press, 2003.
H. Jeﬀreys. Theory of probability: Oxford Univ. Press (earlier editions 1939, 1948), 1961.
Jack Jewson, Jim Smith, and Chris Holmes. Principles of Bayesian inference using general
divergence criteria. Entropy, 20(6):442, 2018.
97

Knoblauch, Jewson and Damoulas
Michael I Jordan, Zoubin Ghahramani, Tommi S Jaakkola, and Lawrence K Saul. An
introduction to variational methods for graphical models. Machine learning, 37(2):183–233,
1999.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In
International Conference on Learning Representations, 2014.
Diederik P Kingma and Max Welling. Auto-encoding variational Bayes. In International
Conference on Learning Representations, 2013.
Jeremias Knoblauch. Frequentist consistency of generalized variational inference. arXiv
preprint arXiv:1912.04946, 2019a.
Jeremias Knoblauch. Robust deep Gaussian processes. arXiv preprint arXiv:1904.02303,
2019b.
Jeremias Knoblauch and Theodoros Damoulas. Spatio-temporal Bayesian on-line changepoint
detection with model selection. In Proceedings of the 27th International Conference on
Machine Learning (ICML), 2018.
Jeremias Knoblauch, Jack Jewson, and Theodoros Damoulas. Doubly robust Bayesian
inference for non-stationary streaming data using β-divergences. In Advances in Neural
Information Processing Systems (NeurIPS), pages 64–75, 2018.
Solomon Kullback and Richard A Leibler. On information and suﬃciency. The annals of
mathematical statistics, 22(1):79–86, 1951.
Sebastian Kurtek and Karthik Bharath. Bayesian sensitivity analysis with the Fisher–Rao
metric. Biometrika, 102(3):601–616, 2015.
Tomasz Kuśmierczyk, Joseph Sakaya, and Arto Klami. Variational Bayesian decision-making
for continuous utilities. In Advances in Neural Information Processing Systems, 2019.
Simon Lacoste-Julien, Ferenc Huszár, and Zoubin Ghahramani. Approximate inference for
the loss-calibrated Bayesian. In Proceedings of the Fourteenth International Conference on
Artiﬁcial Intelligence and Statistics, pages 416–424, 2011.
Gaël Letarte, Pascal Germain, Benjamin Guedj, and François Laviolette. Dichotomize and
generalize: Pac-bayesian binary activated deep neural networks. In Advances in Neural
Information Processing Systems, 2019.
Yingzhen Li and Richard E Turner. Rényi divergence variational inference. In Advances in
Neural Information Processing Systems, pages 1073–1081, 2016.
Moshe Lichman. UCI machine learning repository, 2013.
F Liese and I Vajda. Convex statistical distances, volume 95 of teubner texts in mathematics.
BSB BG Teubner Verlagsgesellschaft, Leipzig, 1987.
98

Generalized Variational Inference
Gabriel Loaiza-Ganem and John P. Cunningham. The continuous Bernoulli: ﬁxing a pervasive
error in variational autoencoders. In Advances in Neural Information Processing Systems,
2019.
David J. C. MacKay. Bayesian methods for backpropagation networks. In Models of neural
networks III, pages 211–254. Springer, 1996.
David J. C. MacKay. Choice of basis for Laplace approximation. Machine learning, 33(1):
77–86, 1998.
Alexander G. de G. Matthews, James Hensman, Richard Turner, and Zoubin Ghahramani.
On sparse variational methods and the Kullback-Leibler divergence between stochastic
processes. Journal of Machine Learning Research, 51:231–239, 2016.
Alexander G. de G. Matthews, Mark Van Der Wilk, Tom Nickson, Keisuke Fujii, Alexis
Boukouvalas, Pablo León-Villagrá, Zoubin Ghahramani, and James Hensman. Gpﬂow: A
Gaussian process library using tensorﬂow. The Journal of Machine Learning Research, 18
(1):1299–1304, 2017.
David A. McAllester. Some PAC-Bayesian theorems. Machine Learning, 37(3):355–363,
1999a.
David A. McAllester. PAC-Bayesian model averaging. In Proceedings of the twelfth annual
conference on Computational learning theory, pages 164–170. ACM, 1999b.
Minami Mihoko and Shinto Eguchi. Robust blind source separation by beta divergence.
Neural computation, 14(8):1859–1886, 2002.
Jeﬀrey W. Miller and David B. Dunson. Robust Bayesian inference via coarsening. Journal
of the American Statistical Association, 114(527):1113–1125, 2019.
Thomas Minka. Divergence measures and message passing. Technical report, Technical
report, Microsoft Research, 2005.
Thomas P Minka. Expectation propagation for approximate Bayesian inference. In Proceedings
of the Seventeenth conference on Uncertainty in artiﬁcial intelligence, pages 362–369.
Morgan Kaufmann Publishers Inc., 2001.
Tomoyuki Nakagawa and Shintaro Hashimoto. Robust Bayesian inference via γ-divergence.
Communications in Statistics-Theory and Methods, pages 1–18, 2019.
Radford M Neal. Bayesian learning for neural networks, volume 118. Springer Science &
Business Media, 2012.
Radford M Neal and Geoﬀrey E Hinton. A view of the EM algorithm that justiﬁes incremental,
sparse, and other variants. In Learning in graphical models, pages 355–368. Springer, 1998.
Anthony O’Hagan and Jeremy E Oakley. Probability is perfect, but we can’t elicit it perfectly.
Reliability Engineering & System Safety, 85(1):239–248, 2004.
99

Knoblauch, Jewson and Damoulas
Manfred Opper and Ole Winther. Gaussian processes for classiﬁcation: Mean-ﬁeld algorithms.
Neural computation, 12(11):2655–2684, 2000.
Joseph J. K. O’Ruanaidh. Numerical Bayesian methods applied to signal processing. PhD
thesis, University of Cambridge, 1994.
John Paisley, David M. Blei, and Michael I. Jordan. Variational Bayesian inference with
stochastic search. In Proceedings of the 29th International Coference on International
Conference on Machine Learning, pages 1363–1370, 2012.
Francesco Pauli, Walter Racugno, and Laura Ventura. Bayesian composite marginal likeli-
hoods. Statistica Sinica, pages 149–164, 2011.
Fengchun Peng and Dipak K Dey. Bayesian analysis of outlier problems using divergence
measures. Canadian Journal of Statistics, 23(2):199–213, 1995.
Joaquin Quiñonero-Candela and Carl Edward Rasmussen. A unifying view of sparse ap-
proximate Gaussian process regression. Journal of Machine Learning Research, 6(Dec):
1939–1959, 2005.
Rajesh Ranganath, Sean Gerrish, and David Blei. Black box variational inference. In
Artiﬁcial Intelligence and Statistics, pages 814–822, 2014.
Rajesh Ranganath, Dustin Tran, Jaan Altosaar, and David Blei. Operator variational
inference. In Advances in Neural Information Processing Systems, pages 496–504, 2016.
Jean-Baptiste Regli and Ricardo Silva. Alpha-beta divergence for variational inference. arXiv
preprint arXiv:1805.01045, 2018.
Alfréd Rényi. On measures of entropy and information. In Proceedings of the Fourth Berkeley
Symposium on Mathematical Statistics and Probability, Volume 1: Contributions to the
Theory of Statistics. The Regents of the University of California, 1961.
Mathieu Ribatet, Daniel Cooley, and Anthony C Davison. Bayesian inference from composite
likelihoods, with an application to spatial extremes. Statistica Sinica, pages 813–845, 2012.
Gareth O Roberts and Jeﬀrey S Rosenthal. Optimal scaling of discrete approximations
to Langevin diﬀusions. Journal of the Royal Statistical Society: Series B (Statistical
Methodology), 60(1):255–268, 1998.
Gareth O Roberts, Andrew Gelman, and Walter R Gilks. Weak convergence and optimal
scaling of random walk Metropolis algorithms. The annals of applied probability, 7(1):
110–120, 1997.
Simone Rossi, Sebastien Marmin, and Maurizio Filippone. Walsh-Hadamard variational
inference for Bayesian deep learning. arXiv preprint arXiv:1905.11248, 2019a.
Simone Rossi, Pietro Michiardi, and Maurizio Filippone. Good initializations of variational
Bayes for deep models. In Proceedings of the 36th International Conference on Machine
Learning, volume 97, pages 5487–5497, 2019b.
100

Generalized Variational Inference
Håvard Rue, Sara Martino, and Nicolas Chopin. Approximate Bayesian inference for latent
Gaussian models by using integrated nested Laplace approximations. Journal of the royal
statistical society: Series b (statistical methodology), 71(2):319–392, 2009.
Yunus Saatçi, Ryan D. Turner, and Carl E. Rasmussen. Gaussian process change point
models. In Proceedings of the 27th International Conference on Machine Learning, pages
927–934, 2010.
Abhijoy Saha, Karthik Bharath, and Sebastian Kurtek. A geometric variational approach to
bayesian inference. Journal of the American Statistical Association, pages 1–25, 2019.
Tim Salimans and David A Knowles. On using control variates with stochastic approximation
for variational Bayes and its connection to stochastic linear regression. arXiv preprint
arXiv:1401.1022, 2014.
Hugh Salimbeni and Marc Deisenroth. Doubly stochastic variational inference for deep
Gaussian processes. In Advances in Neural Information Processing Systems, pages 4588–
4599, 2017.
John Shawe-Taylor and Robert C Williamson. A PAC analysis of a Bayesian estimator. In
Annual Workshop on Computational Learning Theory: Proceedings of the tenth annual
conference on Computational learning theory, volume 6, pages 2–9, 1997.
Xiaotong Shen and Larry Wasserman. Rates of convergence of posterior distributions. The
Annals of Statistics, 29(3):687–714, 2001.
Zhenming Shun and Peter McCullagh. Laplace approximation of high dimensional integrals.
Journal of the Royal Statistical Society: Series B (Methodological), 57(4):749–760, 1995.
Edward Snelson and Zoubin Ghahramani. Sparse Gaussian processes using pseudo-inputs.
In Advances in neural information processing systems, pages 1257–1264, 2006.
Casper Kaae Sønderby, Tapani Raiko, Lars Maaløe, Søren Kaae Sønderby, and Ole Winther.
Ladder variational autoencoders. In Advances in neural information processing systems,
pages 3738–3746, 2016.
Luke Tierney and Joseph B Kadane. Accurate approximations for posterior moments and
marginal densities. Journal of the american statistical association, 81(393):82–86, 1986.
Naftali Tishby, Fernando C Pereira, and William Bialek. The information bottleneck method.
arXiv preprint physics/0004057, 2000.
Michalis Titsias. Variational learning of inducing variables in sparse Gaussian processes. In
Artiﬁcial Intelligence and Statistics, pages 567–574, 2009.
Michalis Titsias and Miguel Lázaro-Gredilla. Doubly stochastic variational Bayes for non-
conjugate inference. In International Conference on Machine Learning, pages 1971–1979,
2014.
John W Tukey. A survey of sampling from contaminated distributions. Contributions to
probability and statistics, pages 448–485, 1960.
101

Knoblauch, Jewson and Damoulas
R. E. Turner and M. Sahani. Two problems with variational expectation maximisation for
time-series models. In Bayesian time series models. Cambridge University Press, 2011.
Ryan D. Turner, Steven Bottone, and Clay J. Stanek. Online variational approximations
to non-exponential family change point models: with application to radar tracking. In
Advances in Neural Information Processing Systems, pages 306–314, 2013.
Keyon Vafa. Training deep Gaussian processes with sampling. In NIPS 2016 Workshop on
Advances in Approximate Bayesian Inference, 2016.
Tim Van Erven and Peter Harremos. Rényi divergence and Kullback-Leibler divergence.
IEEE Transactions on Information Theory, 60(7):3797–3820, 2014.
Cristiano Varin, Nancy Reid, and David Firth. An overview of composite likelihood methods.
Statistica Sinica, pages 5–42, 2011.
Stephen Walker. New approaches to Bayesian consistency. The Annals of Statistics, 32(5):
2028–2043, 2004.
Dilin Wang, Hao Liu, and Qiang Liu. Variational inference with tail-adaptive f-divergence.
In Advances in Neural Information Processing Systems, pages 5742–5752, 2018.
Ke Alexander Wang, GeoﬀPleiss, Jacob R Gardner, Stephen Tyree, Kilian Q Weinberger,
and Andrew Gordon Wilson. Exact Gaussian processes on a million data points. arXiv
preprint arXiv:1903.08114, 2019.
Yali Wang, Marcus Brubaker, Brahim Chaib-Draa, and Raquel Urtasun. Sequential inference
for deep Gaussian process. In Artiﬁcial Intelligence and Statistics, pages 694–703, 2016.
Christopher KI Williams and Matthias Seeger. Using the Nyström method to speed up kernel
machines. In Advances in neural information processing systems, pages 682–688, 2001.
Robert C Wilson, Matthew R Nassar, and Joshua I Gold. Bayesian online learning of the
hazard rate in change-point problems. Neural computation, 22(9):2452–2476, 2010.
Mike Wu, Noah Goodman, and Stefano Ermon. Diﬀerentiable antithetic sampling for variance
reduction in stochastic variational inference. In Proceedings of Machine Learning Research,
volume 89, pages 2877–2886, 2019.
Yue Yang, Ryan Martin, and Howard Bondell. Variational approximations using Fisher
divergence. arXiv preprint arXiv:1905.05284, 2019.
Yun Yang, Debdeep Pati, and Anirban Bhattacharya. α-variational inference with statistical
guarantees. arXiv preprint arXiv:1710.03266, 2017.
Arnold Zellner. Maximal data information prior distributions. New developments in the
applications of Bayesian methods, pages 211–232, 1977.
Arnold Zellner.
Optimal information processing and Bayes’s theorem.
The American
Statistician, 42(4):278–280, 1988.
102

Generalized Variational Inference
Guodong Zhang, Shengyang Sun, David Duvenaud, and Roger Grosse. Noisy natural gradient
as variational inference. In Jennifer Dy and Andreas Krause, editors, Proceedings of the
35th International Conference on Machine Learning, volume 80, pages 5852–5861, 2018.
Jun Zhu, Ning Chen, and Eric P Xing. Bayesian inference with posterior regularization and
applications to inﬁnite latent svms. The Journal of Machine Learning Research, 15(1):
1799–1847, 2014.
103

