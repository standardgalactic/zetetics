Model-Advantage and Value-Aware Models for Model-Based
Reinforcement Learning: Bridging the Gap in Theory and Practice
Nirbhay Modhe 1 Harish Kamath 1 Dhruv Batra 1 Ashwin Kalyan 2
Abstract
This work shows that value-aware model learn-
ing, known for its numerous theoretical beneﬁts,
is also practically viable for solving challenging
continuous control tasks in prevalent model-based
reinforcement learning algorithms. First, we de-
rive a novel value-aware model learning objective
by bounding the model-advantage i.e. model per-
formance difference, between two MDPs or mod-
els given a ﬁxed policy, achieving superior perfor-
mance to prior value-aware objectives in most con-
tinuous control environments. Second, we iden-
tify the issue of stale value estimates in naively
substituting value-aware objectives in place of
maximum-likelihood in dyna-style model-based
RL algorithms. Our proposed remedy to this issue
bridges the long-standing gap in theory and prac-
tice of value-aware model learning by enabling
successful deployment of all value-aware objec-
tives in solving several continuous control robotic
manipulation and locomotion tasks. Our results
are obtained with minimal modiﬁcations to two
popular and open-source model-based RL algo-
rithms – SLBO and MBPO, without tuning any
existing hyper-parameters, while also demonstrat-
ing better performance of value-aware objectives
than these baseline in some environments.
1. Introduction
Reinforcement Learning (RL), with its many success sto-
ries (Mnih et al., 2015; Silver et al., 2016; 2017b; Levine
et al., 2016; Gu et al., 2016), has emerged as a promising
learning paradigm. These milestones are largely due to
model-free RL approaches that come at a signiﬁcant price
in terms of sample efﬁciency. In ﬁelds like robotics or
healthcare, obtaining large amounts of data is both impracti-
cal and expensive, making these methods ill-suited despite
1Georgia Institute of Technology 2Allen Institute for AI. Corre-
spondence to: Nirbhay Modhe <nirbhaym@gatech.edu>.
their successes in complex game environments. As a result,
the alternate data-efﬁcient approach of Model-based Rein-
forcement Learning (MBRL), has become an increasingly
important direction for the research community. However,
the status quo of model learning is conﬁned to mimicking
real world data as opposed to learning representations that
can induce optimal behavior; thereby reducing the scope of
current MBRL approaches.
Traditional MBRL approaches seek to accurately learn the
dynamics of the environment and in practice, employ max-
imum likelihood estimation (MLE) to achieve this – i.e.
minimizing the KL divergence between predicted and ob-
served next state distributions. A drawback of this approach
is the issue of an objective mismatch between the model-
learning objective and the ultimate purpose of using the
model to ﬁnd an optimal policy (Wang et al., 2019; Lambert
et al., 2020). More recent research in MBRL has focused
on efforts to overcome these shortcomings – including opti-
mizing for auxiliary objectives (Lee et al., 2020; Nair et al.,
2020; Tomar et al., 2021), augmenting model-learning with
exploration strategies (Janner et al., 2019; Kidambi et al.,
2020), meta-learning to closely intertwine the two objectives
(Nagabandi et al., 2018) and introducing inductive biases
to the model-learning objective (Lu et al., 2020). However,
these MBRL approaches still employ MLE.
In this work, we revisit Value Aware Model Learning
(VAML) (Farahmand et al., 2017; Farahmand, 2018), an
alternate objective for learning dynamics. Instead of pre-
dicting the exact next state, VAML seeks to predict states
that have similar value as the observed next state. For in-
stance, suppose states s′ and s′′ are distinct but have the
same value i.e. V π(s′) = V π(s′′) under some policy π.
MLE-based model learning would penalize predicting s′′
instead of the observed state s′ since they are not identi-
cal. In VAML, s′ and s′′ are equivalent since they have
the same value. This objective is appealing as it factors
in the utility of the model in ﬁnding the optimal policy
(through the value function) and does not require exact pre-
diction of observed trajectories. Value-aware model-based
RL has recently witnessed several theoretical advancements
in the form of guarantees of convergence (Farahmand et al.,
2017; Farahmand, 2018), the value-equivalence principle
arXiv:2106.14080v2  [cs.LG]  28 Jan 2022

(Grimm et al., 2020) and use optimistic model-based RL
for regret minimization (Ayoub et al., 2020). The MuZero
algorithm (Schrittwieser et al., 2019) is also an example of a
value-aware (or ‘value-equivalent’) model-based approach
for solving discrete action environments while leveraging
Monte-Carlo tree search.
Despite the intuitive and theoretical appeal of existing value-
aware model learning objectives, their utility has thus far
remained under-explored beyond toy settings for contin-
uous control. In our experiments, we found that existing
value-aware objectives perform poorly with model-based
RL frameworks, independently replicating recent negative
results (Lovatto et al., 2020). In this work, we ﬁrst derive an
upper bound on the expected model performance difference
of two MDPs or models for a ﬁxed policy, using triangle
inequality on the L1-norm. In contrast, prior value-aware
approaches (Farahmand, 2018), though inspired by the min-
imization of (normed) model performance difference, do
not upper bound the model performance difference with
their use of the L2-norm, which may explain their infe-
rior performance compared to our proposed objective in
most of our continuous control experiments.
Secondly,
we discover the key issue of stale value estimates in the
naive application of value-aware losses in the dyna-style
model-based RL algorithmic framework. Upon correcting
for the stale value estimates by intermittently ﬁtting the
value network during model learning, we obtain signiﬁcant
performance improvements on the more challenging contin-
uous control environments. The resulting general purpose
dyna-style MBRL algorithm is, to the best of our knowl-
edge, the ﬁrst known practical deployment of value-aware
objectives in challenging continuous control domains in-
cluding MuJoCo (Todorov et al., 2012) robotic simulation
environments (Brockman et al., 2016).
We empirically test our proposed algorithm and novel upper
bound on two recent dyna-style MBRL algorithms – SLBO
(Luo et al., 2018) and MBPO (Janner et al., 2019). We ﬁnd
that our algorithm, without tuning any existing hyperparam-
eter, successfully bridges the gap in theory and practice
by reaching near-matching performance w.r.t. MLE-based
baselines in most continuous control simulation tasks and
outperforming them in some others.
We hope that these
encouraging results spur wider interest in the community
leading to both adoption and further study of value-aware
methods for practical model-based RL.
2. Related Work
MLE-based MBRL. Maximum likelihood estimation
(MLE) is the most prevalent and straight-forward objec-
tive for model learning in an MBRL framework (Sutton,
1990; Sutton et al., 2012), with the goal of modeling state
transitions accurately. Unlike our proposed value-aware
objective, minimizing MLE error minimizes a looser upper
bound on the model performance difference (Farahmand
et al., 2017). Therefore, multiple MBRL approaches that
minimize various deﬁnitions of dynamics error have been
proposed. For instance, Azar et al. (2012; 2013; 2017) use
na¨ıve empirical frequencies. More sophisticated approaches
use function approximators and minimize various statistical
distances – e.g. KL (Ross & Bagnell, 2012), total-variation
(Janner et al., 2019) or Wasserstein metrics (Wu et al., 2019).
Non-MLE based MBRL. Methods that inform model
learning via the value function, reward or policy have re-
cently gained popularity (Oh et al., 2017; Silver et al., 2017a;
Schrittwieser et al., 2019; Abachi, 2020). In particular, Hes-
sel et al. (2021), Schrittwieser et al. (2019), and Schrit-
twieser et al. (2021) explore learning dynamics implicitly
using the estimated value for a given state, and using a
Monte-Carlo tree search algorithm to plan with this learned
model – their method, MuZero, is an instance of a value-
equivalent (Grimm et al., 2020) model learning approach.
However, these works learn a joint model for directly esti-
mating future values and actions (policy) without any ex-
plicit future predictions in the state space. In contrast, we
focus on the class of MBRL methods that explicitly make
predictions in the state space, allowing for simple adap-
tations on top of of well-known MBRL frameworks e.g.
Dyna-style algorithms (Sutton, 1990) such as SLBO (Luo
et al., 2018) and MBPO (Janner et al., 2019).
Value-aware Model Learning. Farahmand et al. (2017),
Farahmand (2018), Ayoub et al. (2020) and Grimm et al.
(2020) are the closest prior works that study the theoretical
properties of value-aware objectives, with experiments re-
stricted to pedagogical settings with small state spaces or
the cart-pole environment. Lovatto et al. (2020) demonstrate
negative empirical results for their practical instantiation
of value aware model learning (Farahmand, 2018) with an
actor-critic learner in continuous control environments such
as Pusher-v2 and InvertedPendulum-v2. Their al-
gorithm employs a sparse model update, occurring only
once every few policy and critic updates – different from
our algorithm that builds on top of a standard Dyna-style
MBRL algorithm with multiple model updates in between
every sequence of agent updates.
3. Preliminaries
3.1. Markov Decision Processes
In this work, we consider a discrete-time inﬁnite-horizon
RL problem characterized by Markov Decision Processes
(MDPs) M deﬁned as (S,A,P,R,P0,γ). Here, S is the
state space, A, the action space, P ∶S × A × S →R the
transition probabilities or dynamics, R ∶S ×A →[0,Rmax]
the reward function, P0 the starting state distribution and

Algorithm 1 Model Based Reinforcement Learning
(MBRL)
Randomly initialize policy π, model M
Initialize replay buffer D ←∅
for nouter iterations do
// model update step
for Kmodel updates do
D ←D ⋃{n samples from true environment M ′
collected by π}
Update M ′ using model-learning objective on D //
e.g. ED [KL(ˆs∣∣s)]
end for
end for
// policy update step
for Kpolicy updates do
D′ ←{Samples collected in learned model M using
π.}
Update π using policy learning method // e.g. TRPO
(Schulman et al., 2015)
end for
ﬁnally, γ the discount factor. The goal is to ﬁnd the optimal
policy π⋆∈Π that maximizes the (discounted) total return
J ∶π →R i.e. J(π) = Eρπ [∑t R(st,at)]. where ρπ is the
distribution of trajectories (s0,a0,s1,...), s0 ∼P0, when
acting according to policy π.
The Q-function and the value function under policy π are
given by Qπ(s,a) = Eρπ [∑t R(st,at) ∣π,s0 = s,a0 = a]
and V π(s) = Eρπ [∑t R(st,at) ∣π,s0 = s]. A more useful
version of the value function, and therefore the RL objective
itself, is obtained by deﬁning the future state distribution
P π
s,t(s′) = Pr(st = s′ ∣π,s0 = s) and γ-discounted sta-
tionary state distribution ds,π(s′) = (1−γ)∑∞
t=0 γtP π
s,t(s′),
where we drop the dependency on start state distribution
when it is implicitly assumed to be known and ﬁxed. Using
these deﬁnitions, we write the value function as:
V π(s0) =
∞
∑
t=0
γtE(at,st)∼(π,P π
s0,t)[R(st,at)]
=
1
1 −γ Ea,s∼π,ds0,π[R(s,a)]
(1)
3.2. Model-Advantage and MBRL
MBRL algorithms work by iteratively learning an approxi-
mate model and then deriving an optimal policy from this
model either by planning with MPC (model-predictive con-
trol) or learning a separate policy (actor-critic) with imag-
ined experience. The latter case refers to the family of
Dyna-style MBRL algorithms (Sutton, 1990) that we adopt
in this work – see Algorithm 1 for a representative algorithm
from this family. Model-advantage1, proposed by (Metelli
et al., 2018; Modhe et al., 2020), is a key quantity that can
be used to compare the utility of transitioning according
to the approximate model M as opposed to the true model
M ′. Speciﬁcally, model-advantage denoted by Aπ
M(s,s′)
compares the utility of moving to state s′ and thereafter
following the trajectory governed by model M as opposed
to following M from state s itself; while acting according
to policy π. The following deﬁnition in Eq. (2) captures this
intuition. We denote model-dependent quantities with the
model as subscript: transition probability distribution of M
is denoted by PM and value function as V π
M.
Aπ
M(s,s′) ∶= γ [V π
M(s′) −Es′′∼PM(s,π)V π
M(s′′)]
(2)
Here, V π
M is the model-dependent value function deﬁned as:
V π
M(s) = Eρπ
M [
∞
∑
t=0
γtRM(st,at) ∣π,M,s0 = s]
We are now ready to restate the well-known simulation
lemma (Kearns & Singh, 2002) that quantiﬁes the model
performance difference using model-advantage.
Lemma 1. (Simulation Lemma) Let M and M ′ be
two different MDPs.
Further,
deﬁne Rπ
M(s)
=
Ea∼π(⋅∣s)[RM(s,a)] and Rπ
δM,M′(s) = Rπ
M(s) −Rπ
M ′(s).
For a policy π ∈Π we have:
JM(π) = JM ′(π) + Es∼dM,π[Rπ
δM,M′(s)]
´¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¸¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¶
reward difference
+
1
1 −γ Es∼dM,πEs′∼PM(s,π) [Aπ
M ′(s,s′)]
´¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¸¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¶
expected model-advantage
(3)
Here, we use a model-dependent stationary state distribution
dM,π(s) (dropping the dependence on start state distribu-
tion) where the dynamics PM are used. To simplify nota-
tion, we will write the expected model advantage term as
E(s,s′)∼M [Aπ
M ′(s,s′)] or simply EM [Aπ
M ′] . A slightly
different form of Lemma 1 can be obtained by explicitly
indicating the model in the Bellman operator as follows.
T π
MV (s) ∶= Ea∼π [RM(s,a) + γEs′∼PM(s,a)[V (s′)]]
(4)
This leads to the following corollary that provides an alter-
nate view of the model-advantage term (see Appendix for
the proof).
Corollary 2. Let M and M ′ be two different MDPs. For
any policy π ∈Π we have:
JM(π) = JM ′(π)
+
1
1 −γ Es∼dM,π [T π
MV π
M ′(s) −T π
M ′V π
M ′(s)]
´¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¸¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¶
deviation error
(5)
1Name follows policy-advantage that compares the utility of
two actions (Kakade & Langford, 2002)

Note that the term on the right that includes the deviation
error is exactly equal to model-advantage when the reward
functions of the two MDPs are identical2. Therefore, setting
aside the reward-error term in Lemma 1, model advantage
can be viewed as the deviation resulting from acting accord-
ing to different MDPs. Minimizing the deviation error is
the basis of the objective proposed in Value-Aware Model
Learning (VAML) (Farahmand et al., 2017; Farahmand,
2018). More recent work (Grimm et al., 2020) shows that
various MBRL methods can be thought of as minimizing the
deviation error – a direct consequence of the close relation-
ship between the deviation error and the model performance
difference.
4. Approach: Model Advantage Based
Objective
In this section, we ﬁrst introduce the basis of value-aware
model learning where the objective is to minimize the per-
formance difference of a policy in the true vs approximate
model. From Eqn. 3, this translates to optimization of ex-
pected model advantage EM [Aπ
M ′] , for which we show
an empirical estimation strategy with samples from the true
MDP and gradient based updates for a parametrized dynam-
ics model. We then derive a novel upper bound on expected
model advantage and introduce a general purpose algorithm
for value-aware model-based RL.
4.1. Optimizing Model Advantage
For the model-learning step in MBRL, we are interested in
an objective for ﬁnding model parameters φ corresponding
to the dynamics of the approximate MDP i.e. Pφ(⋅∣s,a)
that eventually lead to the learning of an optimal policy
in the true MDP M ⋆. By looking at the model-advantage
version of the simulation lemma (i.e. Lemma 1), a natu-
ral choice for a loss function is the absolute value of the
expected model advantage. For brevity, we replace the ex-
pectation over st ∼P π
M ⋆,t,st+1 ∼P π
M ⋆,t+1 with ˜dt.
L1(φ) ∶= ∣JMφ(π) −JM ⋆(π)∣
=
RRRRRRRRRRR
∞
∑
t=0
γt E
˜dt
[γV π
Mφ(st+1) −γ
E
s′′∼Pπ
Mφ(st,π)[V π
Mφ(s′′)]]
RRRRRRRRRRR
(6)
This objective can be empirically estimated via trajectories
(s0,a0,...,aT −1,sT ) ∈Dm where dataset Dm is sampled
2A common assumption for MBRL works proposing to learn
dynamics (e.g. (Luo et al., 2018)). We make this assumption as
well.
from the true MDP M ⋆. We omit the input of π in Pπ
Mφ.
̂
L1(φ) =
RRRRRRRRRRR
∑
Dm
T −1
∑
t=0
γt
mγt(V π
Mφ(st+1) −
E
s′∼Pπ
Mφ(st)[V π
Mφ(s′)])
RRRRRRRRRRR
(7)
In Eqn. 7, the value function V π
Mφ has a complex dependency
on parameters φ which is difﬁcult to optimize. In practice,
as observed by Farahmand (2018), this value can be esti-
mated in any Dyna-style (Sutton, 1990) model-based RL
algorithm with a parametrized value function (with parame-
ters included in θ of policy πθ i.e. as part of an actor-critic
pair) for estimating this value. We estimate the value func-
tion without modeling its dependency on φ i.e. we replace
V π
Mφ with a learned value network V πθ, such that the V πθ
is updated during the policy-update step of our algorithm
(using imagined experience from Mφ) to match the true tar-
get V π
Mφ. This results in a simple stochastic gradient update
rule3 for reparametrized samples from P π
Mφ(st) (typically
Gaussian). Finally, our empirical objective is as follows.
̂
L1(φ) =
RRRRRRRRRRR
∑
Dm
T −1
∑
t=0
γt
m(V πθ(st+1) −
E
s′∼P π
Mφ(st)[V πθ(s′)])
RRRRRRRRRRR
(8)
4.2. Model-Advantage Upper Bound
In practice, the objective in Eqn. 8 is undesirable as it re-
quires full length trajectory samples to compute the dis-
counted sum and therefore, provides a sparse learning sig-
nal i.e. a single gradient update step from an entire trajec-
tory. This limitation is overcome by further upper bounding
Eq. (6) via the triangle inequality as shown below (with
abbreviated notation).
L1(φ) ∶= ∣
∞
∑
t=0
γt E
M ⋆,t[Aπ
M]∣≤
∞
∑
t=0
γt E
M ⋆,t[∣Aπ
M∣]
´¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¸¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¶
=∶LU
1 (φ)
(9)
Observe that this form of the objective is now compatable
with experiences i.e. (s,a,s′,r) sampled from the true MDP
M ⋆as opposed to ensure trajectories – thereby providing
a denser learning signal. We further make this objective
amenable to minibatch training by replacing the the dis-
counted sum over timesteps ∑∞
t=0 γtEst(⋅) with the policy’s
discounted stationary state distribution Es∼ρπ,M⋆(⋅) – this
is estimated empirically with a ﬁnite dataset of sampled
3Note that this objective can be optimized via gradient updates
as long as the value function V πθ can be differentiated w.r.t. its
inputs i.e. states, which is the case for neural networks.

Algorithm 2 Value-Aware MBRL
Initialize θ = (θp,θv), the policy/value function parame-
ters and model parameters φ randomly
Initialize replay buffer D,D′,D′′ ←∅
for K iterations do
D ←D ⋃{n samples from true environment M ⋆col-
lected by πθp}
// model update step
for Kmodel updates do
Update φ using value-aware model-learning objec-
tive on D; //e.g. Eq. (10)
if every Kinterval model updates then
// update stale value parameters
D′′ ←{m samples collected in learned model M
using πθp}
Update θv to estimate discounted return with D′′
end if
end for
end for
// policy update step
for Kpolicy updates do
D′ ←{m samples collected with πθp in model M}
Update θp,θv using policy learning method //e.g.
TRPO (Schulman et al., 2015)
end for
experiences. Similar to Eq. (8), the empirical estimation
version of the objective is as follows, where the summation
is over trajectories (st,at,st+1) ∈Dn.
̂
LU
1 (φ) = 1
n ∑
Dn
∣V πθ(st+1) −
E
s′∼pπ
Mφ(st)[V πθ(s′)]∣(10)
In Section 5.1 we observe the beneﬁts of the denser learning
signal provided by this upper bound in contrast with Equa-
tion (8) on discrete environments where the both objectives
converge successfully.
Connection to VAML. Eqn. 9 is similar to the L2 norm
value-aware objective introduced in (Farahmand et al., 2017;
Farahmand, 2018). In our framework, the VAML objective,
LVAML
2
, can be obtained by using the L2 norm in Eq. (9)
i.e. LVAML
2
∶= ∑∞
t=0 γt EM ⋆,t [(Aπ
M)2]. Importantly, owing
to the properties of L2 norm, note that LVAML
2
does not
upper bound its corresponding L2 normed model advantage
objective L2. We ﬁnd in our experiments that LU
1 has better
overall performance in conjunction with SLBO, potentially
hinting at the importance of this relationship with model-
advantage.
4.3. General Algorithm for Value-aware Objectives
Value-aware objectives such as (Farahmand, 2018; Grimm
et al., 2020) enjoy several theoretical beneﬁts, but remain
isolated from practical use beyond small, ﬁnite state toy
MDPs. We ﬁnd that with a naive substitution of value-
aware objectives in place of maximum likelihood (Figure 4)
in existing model-based RL algorithms (i.e. Algorithm 1
in Algorithm 2) worked well only for the easy continu-
ous control environments, namely Cartpole, Pendulum
and Acrobot.
This supports the evidence in Lovatto
et al. (2020) where negative results were demonstrated for
their choice of simple environments – Pusher-v2 and
InvertedPendulum-v2, and value-aware errors alone
were examined for Hopper-v3 and Walker2d-v3.
Next, we describe our algorithm with which we ﬁnd posi-
tive results in conjunction with the SLBO algorithm (Luo
et al., 2018) on Swimmer-v1, Hopper-v1 and Ant-v1
environments and in conjunction with the MBPO algorithm
(Janner et al., 2019) on Walker-v2 and Hopper-v2 en-
vironments.
4.3.1. CORRECTING STALE VALUE ESTIMATION
Algorithm 1 represents the standard framework of a Dyna-
style algorithm, where the model is trained in a model-
update step with ground truth experience (samples from
M ⋆) and the policy and value parameters πθ,Vθ ∶= V πθ
are trained in a policy update step with virtual or imagined
experiences (samples from Mφ).
Value-aware objectives have an additional dependency on
value estimates in model learning in the from of V πθ
(Eq. (8)) which play the role of V π
Mφ in simplifying Eq. (7).
However, dropping the dependency on φ in V π
Mφ from Eq. 7
to 8 leads to an issue of stale value estimates in the default
dyna-style algorithm, described as follows. For every model
update, the parameter φ of Mφ is changed and as a result,
the value function term in Eq. (7) no longer corresponds
to the same Mφ. This implies that for multiple consecu-
tive model updates with a ﬁxed V πθ, the target that V πθ is
supposed to estimate has moved – making it a stale estimate.
In Algorithm 2 we remedy this issue by updating the value
function intermittently (while keeping policy ﬁxed) between
model updates. Such an intermittent update is relatively
cheap to perform as (i) it does not rely on any additional
ground truth experience, (ii) it updates solely the value
network and not the policy network, and (iii) the frequency
of intermittent updates need not be very high – controlled by
the new hyperparameter Kinterval in Algorithm 2. We found
that setting Kinterval to 20 for SLBO and 5 for MBPO works
well in practice. Intuitively, such intermittent value updates
allow for a novel interplay in the form of a joint optimisation
of model estimates and value estimates (keeping policy
ﬁxed) in conjunction with any value aware model learning

Figure 1: (Left) A sample of the 8x8 size gridworld environment from Gym MiniGrid (Chevalier-Boisvert et al., 2018).
(Right) Return curves over 5 random seeds on MiniGrid-Empty environments with grid sizes 5x5, 8x8, 12x12 and
16x16 using 4 value aware methods and an MLE baseline. Increasing grid size (up to 16x16) negatively affects MLE
performance most, while our proposed upper bound and VAML (Farahmand, 2018) are affected the least. The direct versions
of L1 and L2 model-advantage based objectives (MA Direct L1 and MA Direct L2) are further slower to converge than MA
Upper Bound L1 and MA VAML L2.
objective. We hypothesize that this interplay adds stability
to the optimisation of value aware objectives and in the
next section, we verify it’s role in the same with an ablation
experiment (Fig. 4).
5. Experiments
In this section, we investigate our model learning objective
in the context of model based reinforcement learning in
two settings. First, we evaluate our algorithm in a discrete-
state MDP where we optimize expected model advantage
directly or indirectly via Eqns. 8 , 10, with the purpose of
establishing the performance and convergence relationship
among the selected value-aware objectives and a maximum-
likelihood baseline in a pedagogical setting. Second, we
evaluate Algorithm 2 together with our proposed and a prior
value aware objective on several continuous control tasks,
with two recent dyna-style MBRL algorithms – SLBO (Luo
et al., 2018) and MBPO (Janner et al., 2019).
5.1. Discrete State and Control
We ﬁrst establish the efﬁcacy of value-aware objectives in
a ﬁnite state setting with increasing state space size. For
this experiment, we use a discrete-state episodic gridworld
MDP with cardinal actions {North, South, East,
West}, an N ×N grid, deterministic transitions and a ﬁxed,
absorbing goal state located at the bottom right of the grid
and agent spawning at the top left. A dense reward is pro-
vided for improvement in L2 distance to the goal square
and an additional decaying reward is provided upon reach-
ing the goal. The environment is empty except for walls
along all edges. Since the values of the optimal policy are
symmetric around the major diagonal of the grid it should
provide a slight advantage for value-aware methods that
learn the value-equivalence of such states. We use four con-
ﬁgurations of grid sizes: 5 × 5, 8 × 8, 12 × 12 and 16 × 16.
The dynamics model’s prediction space for the next state is
a discrete set of the total number of states or grid cells in
each environment – the increasing grid size quadratically
increases the total number of states and hence, makes model
learning challenging.
Methods. We denote MA Direct L1 and MA Direct
L2 as methods that optimize L1 and L2 objectives respec-
tively (Eq. 8).
MA Upper Bound L1 optimizes our
proposed upper bound LU
1 and MA VAML L2 optimizes
LVAML
2
(IterVAML from (Farahmand et al., 2017)). In com-
puting the objectives from equations 8 and 10, the expecta-
tion over predicted states is computed exactly as a summa-
tion over all states. MLE denotes the maximum likelihood
baseline. For all methods, we use A2C as the policy update
protocol in the MBRL algorithm.
Results. Figure 1 shows return curves for all methods and
environment conﬁgurations. Return greater than 1 corre-
sponds to reaching the goal (green square) and solving the
task successfully and higher returns correspond to fewer
steps taken to reach the goal. We observe that MLE sample
efﬁciency decreases with increase in grid size (left to right in
Fig. 1) and all value based methods outperform this baseline
on the largest grid size of 16x16. We observe that the upper
bounds on expected model advantage MA Upper Bound
L1 and MA Upper Bound L2 achieve better sample ef-
ﬁciency than the direct counterparts MA L1 and MA L2,
which is expected due to the sparser learning signal from
the norm of the summation over value differences in the
direct computation as opposed to sum of normed value dif-
ferences in the upper bounds. In conclusion, we ﬁnd that
value-aware methods do outperform maximum-likelihood
in discrete state settings with increasing number of states.
5.2. Continuous Control
We select two commonly adopted dyna-style MBRL algo-
rithms – SLBO (Luo et al., 2018) and MBPO (Janner et al.,
2019) as a foundation for evaluating value-aware approaches
in continuous control. For SLBO, we use their open source
code4 and for MBPO, we use the open source PyTorch im-
4https://github.com/facebookresearch/slbo

Figure 2: Evaluation on continuous control environments for value aware methods and baselines with SLBO (Luo et al.,
2018), without tuning existing parameters, over 5 seeds. Our objective MA-L1 achieves better return and sample efﬁciency
in comparison to MA-VAML on most environments (Ant-v1 being the exception) and in comparison to MLE on all
environments. On the Swimmer-v1 and Hopper-v1 environments, we also outperform or are competitive with SLBO.
plementation by MBRL-Lib5 (Pineda et al., 2021). In both
cases, we implement two modiﬁcations – (1) the option to
swap out MLE with value-aware losses for model-learning
and (2) the option to turn on correction of stale value esti-
mates as per Algorithm 2. We tune a single new parameter –
the scaling of the value-aware losses (which is ﬁxed across
all environments once selected We maintain existing value
for all other hyperparameters in order to attribute evaluation
differences solely to Algorithm 2 and value-aware losses,
although further improvements could be achieved through
tuning other hyperparameters.
5.3. Methods
For SLBO, we denote the original SLBO model-learning
objective as SLBO. We denote two value-aware variants as
MA-L1 and MA-VAML which correspond to the empirical
versions of LU
1 and LVAML
2
(IterVAML from (Farahmand
et al., 2017)) as model learning objectives respectively. Both
these variants use Algorithm 2 i.e. the proposed stale value
estimate correction. In Figure 4, we isolate the beneﬁts of
this correction by testing the LU
1 objective without Algo-
rithm 2, which we denote as MA-L1 Naive. Due to the
nature of the SLBO model learning objective in Luo et al.
(2018), it admits decomposition into two components – an
MLE term and a second smoothness term which minimizes
the difference of consecutive state differences. We denote
an MLE-only baseline as MLE, which corresponds to keep-
ing just the MLE term of this objective. Intuitively, this
should be a weaker baseline than SLBO as it represents a
bare bones dyna-style MBRL algorithm. We select the same
5https://github.com/facebookresearch/mbrl-lib
MuJoCo (Todorov et al., 2012) environments provided in
the open source code by SLBO, shown in Figure 2. Addi-
tionally, we show results on two OpenAI Gym (Brockman
et al., 2016) environments Pendulum and Acrobot.
For MBPO, we denote the original MBPO algorithm as
MBPO MLE. Two value-aware variants are obtained similar
to SLBO, which we denote as MBPO MA-L1 and MBPO
MA-VAML which again correspond to the LU
1 and LVAML
2
re-
spectively. We select the same environments provided in the
open source code by MBPO, shown in Figure 3. Note that
the MuJoCo environments for MBPO use the “v2” variants
as opposed to the “v1” variants in SLBO.
5.4. Results
We present return curves for SLBO variants in Figure 2, and
MBPO variants in Figure 3.
Among the SLBO variants,
we ﬁnd that our proposed objective MA-L1 outperforms
MA-VAML on all environments except Ant-v1 (where
MA-VAML performs best) but still achieves performance
comparable to SLBO. We ﬁnd a signiﬁcant improvement
in performance of value-aware methods over the MLE base-
line in most environments. This is an important positive
result for value-aware methods in general – they succeed
in solving continuous control tasks where MLE alone fails
(in all environments except Reacher), demonstrating that
striving for learning an accurate model (with zero MLE
loss) may in fact be practically sub-optimal to minimizng
value-aware loss functions. We also observe that on a few en-
vironments, namely Swimmer-v1 and Hopper-v1, our
MA-L1 objective outperforms or is competitive with SLBO
– a baseline that beneﬁts from the smoothness regularizer in
its second term, in addition to MLE.

Figure 3: Evaluation on continuous control environments for value aware methods and baselines with MBPO (Janner et al.,
2019), without tuning existing parameters, over 5 random seeds. The two value-aware objectives MBPO MA-L1 and MBPO
MA-VAML obtain near-matching performance with MBPO MLE in several environments but under-performing in others.
Figure 4: Return snapshots taken after convergence of SLBO, evaluated on three other variants. MLE corresponds to the
SLBO algorithm with just an MLE model learning objective. MA-L1 Naive corresponds to the SLBO algorithm where
the model learning is objective is replaced with a value-aware objective LU
1 . MA-L1 further uses Algorithm 2 for stale value
estimate correction. In most environments, MA-L1 outperforms MA-L1 Naive and MLE, indicating that the stale value
estimate correction of Algorithm 2 is the reason for improved performance.
Among the MBPO variants, we ﬁnd that both value-aware
variants MBPO MA-L1 and MBPO MA-VAML obtain com-
parable but not excess return compared to MBPO MLE
on CartPole, Swimmer-v2, Pusher, Walker-v2
and Hopper-v2 environments – indicating that they are
solving the continuous control task but with lesser return.
We ﬁnd that value-aware methods are not performant on
Ant-v2 and HalfCheetah-v2, obtaining very low re-
wards (while still being positive for HalfCheetah-v2).
In Figure 4, we ﬁnd that the ablation MA-L1 Naive
is outperformed by MA-L1 in all environments except
Pendulum and Acrobot. The MA-L1 Naive in most
cases fails to exceed the performance of the MLE baseline,
corroborating the negative results by Lovatto et al. (2020)
and highlighting the importance of correcting stale value
estimates in Algorithm 2.
6. Conclusion
In this work, we bridge the gap in theory and practice of
value-aware model learning for model-based RL. We present
a novel value-aware objective inspired by bounding the
model-advantage between an approximate and true model
given a ﬁxed policy, demonstrating superior performance in
comparison to prior value-aware objectives in conjunction
with SLBO (Luo et al., 2018). We identify the issue of stale
value estimates that hamper performance of all value-aware
methods in general if used as-is in the dyna-style MBRL
framework. Our proposed algorithm enables successful de-
ployment of value-aware objectives in complex continuous
control environments, representing the ﬁrst positive result in
the path to bringing value-aware objectives, well-known for
their theoretical beneﬁts, closer to practice. We hope that
these successful experimental results spur wider interest in
value-aware model learning.

References
Abachi, R. Policy-aware model learning for policy gradient
methods. PhD thesis, University of Toronto (Canada),
2020.
Ayoub, A., Jia, Z., Szepesvari, C., Wang, M., and Yang, L.
Model-based reinforcement learning with value-targeted
regression.
In International Conference on Machine
Learning, pp. 463–474. PMLR, 2020.
Azar, M. G., Munos, R., and Kappen, B. On the sample
complexity of reinforcement learning with a generative
model. arXiv preprint arXiv:1206.6461, 2012.
Azar, M. G., Munos, R., and Kappen, H. J. Minimax pac
bounds on the sample complexity of reinforcement learn-
ing with a generative model. Machine learning, 91(3):
325–349, 2013.
Azar, M. G., Osband, I., and Munos, R. Minimax regret
bounds for reinforcement learning. In Proceedings of
the 34th International Conference on Machine Learning-
Volume 70, pp. 263–272. JMLR. org, 2017.
Brockman, G., Cheung, V., Pettersson, L., Schneider, J.,
Schulman, J., Tang, J., and Zaremba, W. Openai gym.
arXiv preprint arXiv:1606.01540, 2016.
Chevalier-Boisvert, M., Willems, L., and Pal, S. Minimalis-
tic gridworld environment for openai gym. https://
github.com/maximecb/gym-minigrid, 2018.
Farahmand, A.-m. Iterative value-aware model learning. In
Advances in Neural Information Processing Systems, pp.
9072–9083, 2018.
Farahmand, A.-m., Barreto, A., and Nikovski, D. Value-
aware loss function for model-based reinforcement learn-
ing. In Artiﬁcial Intelligence and Statistics, pp. 1486–
1494, 2017.
Grimm, C., Barreto, A., Singh, S., and Silver, D. The
value equivalence principle for model-based reinforce-
ment learning. arXiv preprint arXiv:2011.03506, 2020.
Gu, S., Lillicrap, T., Sutskever, I., and Levine, S. Con-
tinuous deep q-learning with model-based acceleration.
In International Conference on Machine Learning, pp.
2829–2838, 2016.
Hessel, M., Danihelka, I., Viola, F., Guez, A., Schmitt,
S., Sifre, L., Weber, T., Silver, D., and van Hasselt, H.
Muesli: Combining improvements in policy optimization.
CoRR, abs/2104.06159, 2021. URL https://arxiv.
org/abs/2104.06159.
Janner, M., Fu, J., Zhang, M., and Levine, S. When to
trust your model: Model-based policy optimization. In
Advances in Neural Information Processing Systems, pp.
12498–12509, 2019.
Kakade, S. and Langford, J. Approximately optimal approx-
imate reinforcement learning. In ICML, volume 2, pp.
267–274, 2002.
Kearns, M. and Singh, S. Near-optimal reinforcement learn-
ing in polynomial time. Machine learning, 49(2-3):209–
232, 2002.
Kidambi, R., Rajeswaran, A., Netrapalli, P., and Joachims,
T. Morel: Model-based ofﬂine reinforcement learning.
arXiv preprint arXiv:2005.05951, 2020.
Lambert, N., Amos, B., Yadan, O., and Calandra, R. Ob-
jective mismatch in model-based reinforcement learning.
arXiv preprint arXiv:2002.04523, 2020.
Lee, K., Seo, Y., Lee, S., Lee, H., and Shin, J. Context-
aware dynamics model for generalization in model-based
reinforcement learning. In International Conference on
Machine Learning, pp. 5757–5766. PMLR, 2020.
Levine, S., Finn, C., Darrell, T., and Abbeel, P. End-to-
end training of deep visuomotor policies. The Journal of
Machine Learning Research, 17(1):1334–1373, 2016.
Lovatto, ˆA. G., Bueno, T. P., Mau´a, D. D., and Barros, L. N.
Decision-aware model learning for actor-critic methods:
When theory does not meet practice. In Proceedings on
”I Can’t Believe It’s Not Better!” at NeurIPS Workshops,
2020. PMLR, 2020.
Lu, X., Lee, K., Abbeel, P., and Tiomkin, S. Dynamics
generalization via information bottleneck in deep rein-
forcement learning. arXiv preprint arXiv:2008.00614,
2020.
Luo, Y., Xu, H., Li, Y., Tian, Y., Darrell, T., and Ma, T.
Algorithmic framework for model-based deep reinforce-
ment learning with theoretical guarantees. arXiv preprint
arXiv:1807.03858, 2018.
Metelli, A. M., Mutti, M., and Restelli, M. Conﬁgurable
markov decision processes. In International Conference
on Machine Learning, pp. 3491–3500. PMLR, 2018.
Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness,
J., Bellemare, M. G., Graves, A., Riedmiller, M., Fidje-
land, A. K., Ostrovski, G., et al. Human-level control
through deep reinforcement learning. Nature, 518(7540):
529–533, 2015.
Modhe, N., Kamath, H. K., Batra, D., and Kalyan, A.
Bridging worlds in reinforcement learning with model-
advantage. 2020.

Nagabandi, A., Clavera, I., Liu, S., Fearing, R. S., Abbeel,
P., Levine, S., and Finn, C. Learning to adapt in dynamic,
real-world environments through meta-reinforcement
learning. arXiv preprint arXiv:1803.11347, 2018.
Nair, S., Savarese, S., and Finn, C. Goal-aware prediction:
Learning to model what matters. In International Con-
ference on Machine Learning, pp. 7207–7219. PMLR,
2020.
Oh, J., Singh, S., and Lee, H. Value prediction network.
arXiv preprint arXiv:1707.03497, 2017.
Pineda, L., Amos, B., Zhang, A., Lambert, N. O., and Ca-
landra, R. Mbrl-lib: A modular library for model-based
reinforcement learning. arXiv preprint arXiv:2104.10159,
2021.
Ross, S. and Bagnell, J. A. Agnostic system identiﬁcation
for model-based reinforcement learning. arXiv preprint
arXiv:1203.1007, 2012.
Schrittwieser, J., Antonoglou, I., Hubert, T., Simonyan, K.,
Sifre, L., Schmitt, S., Guez, A., Lockhart, E., Hassabis,
D., Graepel, T., Lillicrap, T. P., and Silver, D. Mastering
atari, go, chess and shogi by planning with a learned
model. CoRR, abs/1911.08265, 2019. URL http://
arxiv.org/abs/1911.08265.
Schrittwieser, J., Hubert, T., Mandhane, A., Barekatain,
M., Antonoglou, I., and Silver, D. Online and ofﬂine
reinforcement learning by planning with a learned model.
CoRR, abs/2104.06294, 2021. URL https://arxiv.
org/abs/2104.06294.
Schulman, J., Levine, S., Abbeel, P., Jordan, M., and Moritz,
P. Trust region policy optimization. In International
conference on machine learning, pp. 1889–1897, 2015.
Silver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L.,
Van Den Driessche, G., Schrittwieser, J., Antonoglou, I.,
Panneershelvam, V., Lanctot, M., et al. Mastering the
game of go with deep neural networks and tree search.
nature, 529(7587):484–489, 2016.
Silver, D., Hasselt, H., Hessel, M., Schaul, T., Guez, A.,
Harley, T., Dulac-Arnold, G., Reichert, D., Rabinowitz,
N., Barreto, A., et al. The predictron: End-to-end learning
and planning. In International Conference on Machine
Learning, pp. 3191–3199. PMLR, 2017a.
Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou,
I., Huang, A., Guez, A., Hubert, T., Baker, L., Lai, M.,
Bolton, A., et al. Mastering the game of go without
human knowledge. Nature, 550(7676):354–359, 2017b.
Sutton, R. S. Integrated architectures for learning, plan-
ning, and reacting based on approximating dynamic pro-
gramming. In Machine learning proceedings 1990, pp.
216–224. Elsevier, 1990.
Sutton, R. S., Szepesv´ari, C., Geramifard, A., and Bowl-
ing, M. P. Dyna-style planning with linear function ap-
proximation and prioritized sweeping. arXiv preprint
arXiv:1206.3285, 2012.
Todorov, E., Erez, T., and Tassa, Y. Mujoco: A physics
engine for model-based control. In 2012 IEEE/RSJ Inter-
national Conference on Intelligent Robots and Systems,
pp. 5026–5033. IEEE, 2012.
Tomar, M., Zhang, A., Calandra, R., Taylor, M. E.,
and Pineau, J.
Model-invariant state abstractions for
model-based reinforcement learning.
arXiv preprint
arXiv:2102.09850, 2021.
Wang, T., Bao, X., Clavera, I., Hoang, J., Wen, Y., Lan-
glois, E., Zhang, S., Zhang, G., Abbeel, P., and Ba,
J. Benchmarking model-based reinforcement learning.
arXiv preprint arXiv:1907.02057, 2019.
Wu, Y.-H., Fan, T.-H., Ramadge, P. J., and Su, H. Model
imitation for model-based reinforcement learning. arXiv
preprint arXiv:1909.11821, 2019.

Appendix
A. Performance Difference
A.1. Proof of Lemma 1 (Simulation Lemma)
We restate Lemma 1 below, followed by the proof.
Lemma 1. (Simulation Lemma) Let M and M ′ be two dif-
ferent MDPs. Further, deﬁne Rπ(s) = Ea∼π(⋅∣s)[R(s,a)]
and Rπ
ϵ (s) = Rπ
M(s) −Rπ
M ′(s). For any policy π ∈Π we
have:
JM(π) = JM ′(π) + Es∼dM,π[Rϵ(s)]
+
1
1 −γ Es∼dM,πEs′∼PM(s′∣s,π) [Aπ
M ′(s,s′)]
Proof. Let P0 be the start state distribution for both MDPs,
P π
M,t be the state distribution at time t starting from s0 ∼P0
in M, and dM,π denote the stationary state distribution
under MDP M, policy π and start state s0 ∼P0. We use
the following slightly modiﬁed version of the deﬁnition of
value function which has a normalization of 1 −γ:
V π
M(s0) = (1 −γ)
∞
∑
t=0
γtEat,st∼πPM,t[RM(st,at)]
Then, we have:
JM(π) −JM ′(π)
= Es0∼P0 [V π
M(s0) −V π
M ′(s0)]
= (1 −γ)
∞
∑
t=0
γtEst∼P π
M,t Eat∼π(⋅∣st) [RM(st,at)]
´¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¸¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¶
Rπ
M(st)
−Es0∼P0 [V π
M ′(s0)]
= (1 −γ)
∞
∑
t=0
γtEst∼P π
M,t [Rπ
M(st)] −Es0∼P0 [V π
M ′(s0)]
=
∞
∑
t=0
γtEst∼P π
M,t [(1 −γ)Rπ
M(st) + V π
M ′(st) −V π
M ′(st)]
−Es0∼P0 [V π
M ′(s0)]
Cancelling the ﬁrst element in the summation, and shifting
the series by 1 step:
=
∞
∑
t=0
γt
E
st∼P π
M,t
st+1∼P π
M,t+1
[(1 −γ)Rπ
M(st) + γV π
M ′(st+1)
−V π
M ′(st)]
Expanding V π
M ′(st) with a one-step bellman evaluation op-
erator:
=
∞
∑
t=0
γt
E
st∼P π
M,t
st+1∼P π
M,t+1
[(1 −γ)Rπ
M(st) + γV π
M ′(st+1)
−((1 −γ)Rπ
M ′(st) + γEs′′∼Pπ
M′(st,π) [V π
M ′(s′′)])]
=
∞
∑
t=0
γt
E
st∼P π
M,t
st+1∼P π
M,t+1
[(1 −γ)(Rπ
M(st) −Rπ
M ′(st))
+ γV π
M ′(st+1) −γEs′′∼Pπ
M′(st,π) [V π
M ′(s′′)]]
=
∞
∑
t=0
γt
E
st∼P π
M,t
st+1∼P π
M,t+1
[(1 −γ)Rπ
ϵ (st)
+ γV π
M ′(st+1) −γEs′′∼Pπ
M′(st,π) [V π
M ′(s′′)]]
Using deﬁnition of Aπ
M ′:
=
∞
∑
t=0
γt
E
st∼P π
M,t
st+1∼P π
M,t+1
[(1 −γ)Rπ
ϵ (st) + Aπ
M ′(st,st+1)]
=
1
1 −γ Es∼dM,πEs′∼PM(s,π) [Aπ
M ′(s,s′)]
+ Es∼dM,π [Rπ
ϵ (s)]
A.2. Proof of Corollary 2 (Deviation Error)
Restating Corollary 2: Corollary 2. Let M and M ′ be two
different MDPs. For any policy π ∈Π we have:
JM(π) = JM ′(π)
+
1
1 −γ Es∼dM,π [T π
MV π
M ′(s) −T π
M ′V π
M ′(s)]
´¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¸¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¶
deviation error
(11)
Proof.
JM(π) −JM ′(π)
= Es0∼P0 [V π
M(s0) −V π
M ′(s0)]
= ⋯

Proceeding similar to the previous proof upto the following
line:
=
∞
∑
t=0
γt
E
st∼P π
M,t
st+1∼P π
M,t+1
[(1 −γ)Rπ
M(st) + γV π
M ′(st+1)
−((1 −γ)Rπ
M ′(st) + γEs′′∼Pπ
M′(st,π) [V π
M ′(s′′)])]
=
∞
∑
t=0
γtEst∼P π
M,t[T π
MV π
M ′(st)
−((1 −γ)Rπ
M ′(st) + γEs′′∼Pπ
M′(st,π) [V π
M ′(s′′)])]
=
∞
∑
t=0
γtEst∼P π
M,t [T π
MV π
M ′(st) −T π
M ′V π
M ′(st)]
=
1
1 −γ Es∼dπ,M [T π
MV π
M ′(s) −T π
M ′V π
M ′(s)]
This concludes the proof of Corollary 2. Note that we can
further upper bound the difference in values across MDPs
for a policy as follows, which will be useful in subsequent
proofs. We compute this bound at an arbitrary start state s0,
and it will then hold for any start state. Let dM,s0,π be the
stationary state distribution of following policy π in MDP
M, starting at state s0.
V π
M(s0) −V π
M ′(s0)
= ⋯
(12)
Proceeding similar to the proof of Corollary 2, we get the
following:
=
1
1 −γ EdM,s0,π [T π
MV π
M ′ −T π
M ′V π
M ′]
≤
1
1 −γ ∥T π
MV π
M ′ −T π
M ′V π
M ′∥∞
(13)
≤
1
1 −γ [∥Rπ
M ′ −Rπ
M∥∞
+ γ max
s
∑
s′∈S
V π
M ′(s′)Ea∼π(s) [pM ′(s′∣s,a) −pM(s′,a)]]
≤
1
1 −γ [ϵR
+ γ ∥V π
M ′∥∞max
s
max
a
∥pM ′(s′∣s,a) −pM(s′,a)∥1
´¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¸¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¶
ϵP
]
≤
1
1 −γ [ϵR + γ ∥V π
M ′∥∞ϵP ]
≤
1
1 −γ [ϵR + γϵP Rmax
1 −γ
]
(14)

