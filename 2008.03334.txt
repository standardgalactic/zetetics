Bayesian inference of network structure from unreliable data
Jean-Gabriel Young,1 George T. Cantwell,2 and M. E. J. Newman1, 2
1Center for the Study of Complex Systems, University of Michigan, Ann Arbor, Michigan 48109, USA
2Department of Physics, University of Michigan, Ann Arbor, Michigan 48109, USA
Most empirical studies of complex networks do not return direct, error-free measurements of
network structure. Instead, they typically rely on indirect measurements that are often error-prone
and unreliable.
A fundamental problem in empirical network science is how to make the best
possible estimates of network structure given such unreliable data. In this paper we describe a fully
Bayesian method for reconstructing networks from observational data in any format, even when
the data contain substantial measurement error and when the nature and magnitude of that error
is unknown. The method is introduced through pedagogical case studies using real-world example
networks, and speciﬁcally tailored to allow straightforward, computationally eﬃcient implementation
with a minimum of technical input. Computer code implementing the method is publicly available.
I.
INTRODUCTION
Networks are widely used as a convenient mathemati-
cal representation of the relationships between elements
of a complex system [1].
Network methods have been
fruitfully applied to aid our understanding of systems in
physics, biology, computer and information sciences, the
social sciences, and many other areas. A typical empirical
network study will ﬁrst determine the structure of a net-
work of interest, using experiments, ﬁeld observations,
or archival data, then analyze that structure to reveal
features of interest, using any of the many quantitative
analysis techniques developed for this purpose [2].
In this paper we focus on the ﬁrst part of this process,
on how we determine the structure of a network from
appropriate empirical observations. At ﬁrst sight, this
appears to be a straightforward task. Most studies aim
to measure the presence or absence of edges in a network,
either singly or in some collective fashion, and then as-
semble a picture of the complete network by aggregating
many such measurements. Upon further consideration,
however, it is apparent that the situation is not so sim-
ple, because most network measurements do not tell us
unambiguously about the presence or absence of edges.
At best, they give us a noisy indication, and in many
cases they merely hint obliquely at the network struc-
ture [3].
As an example, consider a protein-protein interaction
network representing the pattern of physical interactions
between proteins in the cell. Such networks can be mea-
sured in a relatively direct manner. Techniques such as
aﬃnity puriﬁcation and two-hybrid screens can be used
to determine whether a given protein interacts with oth-
ers [4, 5]. These techniques are notoriously unreliable,
however, returning high rates of both false positives and
false negatives, so that individual measurements do not
themselves tell us the structure of the network [6]. Mea-
surements may have to be repeated many times in order
to separate the signal from the noise [7].
A more complex example is the measurement of a so-
cial network such as a network of who is friends with
whom.
Such networks can be measured by conduct-
ing surveys in which people are asked to identify their
friends [8]. Here, however, things can get complicated.
For instance, it happens often that person A identiﬁes
person B as a friend, but person B does not identify per-
son A. In some communities fully a half of all friendships
are “one-way” friendships of this kind [9, 10].
Should
two such people be considered friends? Why do they dis-
agree? Is one of the individuals mistaken, or forgetful, or
not telling the truth? Perhaps the two are using diﬀer-
ent deﬁnitions of friendship? Things become harder still
when we consider other data that are commonly gathered
in such surveys, such as age, gender, race, occupation,
income, or educational level of participants. Friendships
are well known to depend strongly on such personal char-
acteristics and in cases where we are uncertain about a
hypothesized friendship between two people we may be
able to use their traits to make a better informed deci-
sion about whether they are really friends [11]. The best
estimate of whether two people are friends may thus be
the result of a complex calculation that combines many
inputs.
Most empirical studies of network structure, however,
do not take such an approach. Even though network data
are known to be noisy and imperfect [6, 12–17], many
experimenters nonetheless rely on simple direct measure-
ments of the presence of edges and eﬀectively make the
assumption that the resulting data are the network. In a
protein-protein interaction network they assume an inter-
action to be present if, for example, it is observed to exist
enough times when measured. In a friendship network
two people are assumed to be friends if one identiﬁes the
other as a friend. And yet it is clear from the discussion
above that the true situation is more complicated than
this. In realistic circumstances, we have a heterogeneous
body of data, potentially of many diﬀerent types, poten-
tially unreliable, and varying in its relationship to the
actual network structure. In such circumstances, tradi-
tional methods for inferring network structure from data
may be inadequate and in some cases outright mislead-
ing.
In this paper we present a general framework for infer-
ring network structure from measurements using meth-
arXiv:2008.03334v2  [cs.SI]  9 Mar 2021

2
ods of Bayesian inference, along with a complete software
pipeline implementing that framework. The methods we
describe take empirical measurements of a system and re-
turn a posterior distribution over possible network struc-
tures, thereby telling us not only what the likely structure
is but also giving us an estimate of our certainty about
that structure. Our approach requires a minimum of in-
put from the experimenter, the only information they
need supply (other than the data) being a description of
how the measurements depend on the underlying network
structure, which is speciﬁed at a high level in the form
of probability distributions—we give a number of exam-
ples in this paper. The rest of the calculation, from data
to posterior distribution and ﬁnal network structure, is
performed automatically. Application of Bayesian infer-
ence methods often requires experimentation to ﬁnd the
best approach for a particular application. The level of
automation in our framework makes this a straightfor-
ward task, allowing one to easily experiment with diﬀer-
ent strategies and quickly see the results [18].
The problem we address falls in the general area of
network reconstruction, the problem of determining the
structure of a network from the available data. There
has been a considerable amount of previous work on net-
work reconstruction, much of it in the subject-speciﬁc
literature and directed at the reconstruction of par-
ticular types of networks or using particular types of
data. Methods have been proposed, for example, for ge-
ographic co-location data [19–22], social networks [23],
brain scans [24–26], and biochemical networks [5, 27–
29]. Perhaps the closest precursor to our work is that of
Butts [23], who developed Bayesian methods and Gibbs
sampling techniques for estimating social network struc-
ture from unreliable social surveys. Priebe and collab-
orators [30] have developed similar ideas, incorporating
structured priors over networks to improve inference, an
approach that has been echoed in statistics [24–26] and
network science [31]. A key diﬀerence between these ap-
proaches and our own, however, is that they were de-
veloped with particular measurement processes in mind,
whereas our methods can accommodate almost arbitrary
ones. The approach we propose also diﬀers from our own
related previous work [3, 32, 33] in putting forward an es-
timation algorithm that works essentially automatically
with arbitrary models, using ideas borrowed from the lit-
erature on ﬁnite mixture models [34, 35].
Looking more broadly, there is also a considerable vol-
ume of work that tackles other problems concerning the
reliability of network data, many of which could also
be addressed using variants or extensions of the meth-
ods proposed here. The problem of link prediction, for
instance, involves predicting missing edges in partially
observed networks and a range of algorithms have been
proposed that achieve good performance [36–40]. Also
much studied is the problem of inferring network struc-
ture from non-network data [41], such as time-series [42],
gene expression data [43], the spread of information or
disease [44, 45], and various local network features and
node properties [46, 47]. A separate literature deals with
problems of network sampling, addressing how choices
such as which nodes or edges we measure can aﬀect our
estimates of network properties and how best to make
those choices [2, 48–50]. Disambiguation or entity reso-
lution is the process of accurately identifying the nodes
in a network in the presence of potential sources of error
such as duplicate nodes or nodes that have been inad-
vertently combined [16, 51, 52]. Some methods have also
been proposed that aim to perform more than one of
these tasks at once, such as simultaneous link prediction
and disambiguation [53]. Finally, there is also a growing
literature on making best estimates of derived network
metrics starting from probabilistic representations of net-
work structure [54–58]. These methods use the kinds of
structural estimates we develop in this paper as input to
calculations of further quantities of interest, such as cen-
trality measures, path lengths, correlations, community
structure, or core decompositions [55, 56].
This manuscript is organized as follows. In Section II
we ﬁrst give an overview of the framework we propose
for inference of network structure from unreliable data.
Then in Section III we describe in depth two example
applications, illustrating the construction of appropriate
measurement models and the practical implementation of
the method. In Section IV we give our conclusions and
suggest some directions for future work. An appendix
gives technical details of the workings of our method.
II.
DESCRIPTION OF THE METHOD
Suppose we have made some measurements of a net-
worked system. They may include direct measurements
of network structure, such as measurements of the pres-
ence or absence or edges, but they may also include other
quantities that could have indirect bearing on network
structure, such as measurements of node properties or
traits. Our objective is to make the best possible estimate
of the structure of the network from these measurements
and to do so eﬃciently and almost automatically, requir-
ing a minimum of technical eﬀort, so that practitioners
can focus on making the measurements and interpreting
the results. In this section we give a broad overview of the
concepts and essential equations underlying our method.
A complete derivation and accompanying technical dis-
cussion can be found in Appendix A.
In a nutshell, we posit that there exists some underly-
ing network whose structure aﬀects measurements made
on the system of interest. This network is represented by
its adjacency matrix A, which is initially unknown. Our
goal is to estimate this matrix. We assume that the ob-
servational data depend on the adjacency matrix but in a
potentially noisy way: even exact repetition of the same
experiment could produce diﬀerent observations.
This
means that, even though the network has a well-deﬁned
and unambiguous structure, it will not in general be pos-
sible to tell exactly what that structure is from the mea-

3
surements. To accommodate this uncertainty, we adopt
a Bayesian point of view. Instead of inferring the exact
network structure itself from measurements, we instead
infer a probability distribution over possible structures
compatible with the observations.
Apart from the data themselves, the only input our
method requires of the user is the speciﬁcation of a model
that represents, in general terms, how the data depend
on the network structure.
This model can take a va-
riety of diﬀerent forms: networks and the experiments
used to measure them vary widely, so no single model
applies in all cases. Our method allows the user to spec-
ify the model that is most appropriate to their situation.
The model may contain parameters (sometimes many of
them) but it is not necessary to know the values of these
parameters: our method automatically infers the best
values from the data.
For the sake of concreteness, we concentrate in this
paper primarily on the most common situation encoun-
tered in network studies, in which the network is simple,
undirected, and unweighted, and the data consist of indi-
vidual measurements of the presence or absence of edges.
The methods we describe are applicable to other situa-
tions as well, but this one covers many cases of interest
and will allow us to use a more transparent and explicit
notation. The measurements themselves can be as simple
as observed interactions between pairs of nodes, but can
also take more complex forms, such as paths across the
network, time-series, tags associated with relationships,
or any of a variety of other possibilities. We encode the
measurements in an array X, whose entry Xij contains
all the information we have about the interaction of nodes
i and j.
We also make a further crucial assumption about the
model, namely that observations of diﬀerent node pairs
are conditionally independent, which in this case means
that the observations Xij of the interaction between i
and j depend only on the adjacency matrix element Aij
and not on any other elements. This assumption is not
strictly necessary for the method work but, as shown
in Appendix A, it improves the computational eﬃciency
substantially.
And, since it is true of most commonly
used models anyway, it is in practice not a signiﬁcant
restriction.
There are exceptions:
in certain (“ﬁxed
choice”) social network studies, for instance, participants
are limited to naming a maximum number of friends or
contacts, such as ten.
This means that every time a
participant names a contact, contacts with other indi-
viduals becomes less likely because the participant has
fewer “slots” left to ﬁll, and hence contacts are (weakly)
negatively correlated. In this paper we neglect such de-
pendencies and assume that contacts are independent.
(However, see Refs. [3, 32] for a discussion of methods
that can handle dependencies.)
With these assumptions, selecting a model boils down
to making three basic choices. The ﬁrst and most crucial
of these is specifying how the pairwise measurements Xij
depend on the underlying network, as represented by the
adjacency matrix A.
Speciﬁcally, for a given pair of
nodes i, j we need to specify the expected distribution
of values Xij for the case where i and j are connected
by an edge and for the case where they are not. We will
write these two distributions respectively as
µij(1, θ) = P(Xij|Aij = 1, θ)
(1)
and
µij(0, θ) = P(Xij|Aij = 0, θ),
(2)
where θ denotes any parameters of the distribution. (We
will in some cases drop θ from the notation where it is
clearly understood.) We will refer to Eqs. (1) and (2) as
the data model.
The second modeling choice is the speciﬁcation of the
prior probability ascribed to the edge between i and j.
What is the probability that the edge exists before we
make any measurements of it? Do all edges have an equal
chance of existing a priori, or are some more likely than
others? Again we assume that diﬀerent edges are inde-
pendent, although again this assumption, while computa-
tionally convenient, is not strictly necessary. Mimicking
the notation introduced for the data model, we write the
prior probability of an edge between i and j as
νij(1, θ) = P(Aij = 1|θ)
(3)
and the probability of no edge is νij(0, θ) = 1 −νij(1, θ).
Again θ collectively denotes the set of parameters (if
any).
We call this second set of probabilities the net-
work model.
The third and last modeling choice is the speciﬁcation
of the prior distribution on the parameters θ. Our frame-
work does not place any restriction on the possible form
of the prior on θ. In many cases a simple uniform (max-
imum entropy) prior works well, but the prior can also
be chosen for example to encode speciﬁc prior knowledge
about the system or to rule out unphysical values of the
parameters.
Once these three choices are made, the rest of the pro-
cedure is essentially automatic. Given the model choices
and a set of measurements, our method will generate a
string of pairs (Ar, θr) of networks and parameters com-
patible with the measurements. By inspecting these net-
works and parameters we can determine any other net-
work properties we might care about. For example, if
we want to determine whether i and j are connected by
an edge, we can inspect the matrix element A(r)
ij for all r
and compute the fraction of the time that A(r)
ij = 1, which
gives us (a Monte Carlo estimate of) the posterior prob-
ability of the edge’s existence.
More precisely, the sample networks and parameter
sets returned by our algorithm are drawn from the joint
posterior distribution P(A, θ|X), which allows us to
compute an estimate of the expected value of any func-

4
tion f(A, θ) of the network and parameters thus:
⟨f(A, θ)⟩=
X
A
Z
f(A, θ)P(θ, A|X) dθ
≃1
N
N
X
r=1
f(Ar, θr),
(4)
where N is the number of samples generated.
Our computer code implementing these calculations is
freely available online with accompanying tutorials ex-
plaining its use.1
III.
EXAMPLES
To demonstrate how our method operates in practice,
we present two detailed case studies. The ﬁrst involves a
network of animal interactions.
A.
Network of dolphin companionship
Many animals form lasting social networks, includ-
ing monkeys, deer, horses, cattle, dolphins, and kanga-
roos [14]. Here we analyze data from Connor et al. [59] of
interactions among a small (n = 13) group of male bot-
tlenose dolphins as they swim in a shallow lagoon. This
is a typical example of an animal observational study
that aims to determine social ties indirectly by observing
behavior.
Standard techniques of social network anal-
ysis would typically be used to transform the observa-
tions into association indices that quantify the level of
interaction between pairs of individuals [60]. These in-
dices, however, can can be hard to interpret and their
deﬁnition relies on somewhat ad hoc assumptions. Our
methods give us a more principled way to infer connec-
tions by interpreting the data as noisy measurements of
an underlying social network.
1.
Basic model
In this particular study the recorded data Xij, shown
in Fig. 1, represent the number of times each pair of dol-
phins is observed swimming in close proximity. We can
use these data to reconstruct the underlying network as
follows. First, it is reasonable to assume that the num-
ber of interactions between two dolphins will depend on
whether they have a network connection or not. But also
we expect there to be some randomness in the numbers,
both because of circumstances and because of observa-
tional error. We thus model the number of interactions
1 The code can be downloaded from https://github.com/jg-you/
noisy-networks-measurements.
2 4 6 8 10 12
Dolphin
2
4
6
8
10
12
Dolphin
(a)
0
10
20
30
# of interactions
0
10
20
30
Number of interactions
10
2
10
1
10
0
Fraction of pairs
(b)
FIG. 1. Data on interactions among a group of dolphins, from
Connor et al. [59]. Thirteen male dolphins were observed as
they swam in a shallow lagoon and tabulations were made of
pairs that swam together. (a) Observed frequency of interac-
tion for each pair. (b) Histogram of frequencies.
as a Poisson random variable with mean λ1 or λ0 depend-
ing on whether there is or is not a network connection,
respectively. That is,
µij(0, λ0) = λXij
0
Xij! e−λ0,
(5a)
µij(1, λ1) = λXij
1
Xij! e−λ1.
(5b)
We assume that λ1 > λ0, i.e., that individuals interact
more often if they have a network connection than if they
do not.
We also need to choose our network model and prior
on the model parameters. Having no prior information
about the probabilities of individual network edges, we
assume that all edges are a priori equally likely, which
implies
νij(0, ρ) = 1 −ρ,
(6a)
νij(1, ρ) = ρ.
(6b)
where ρ is the probability of an edge.
For the priors on the parameters we make minimal
assumptions. For ρ, which is a probability, we assume a
uniform prior on the interval [0, 1]. For λ0 and λ1, which
have semi-inﬁnite support, we cannot use a uniform prior,
so instead we assume a slowly varying probability over a
wide range of plausible values. In keeping with modern
Bayesian practice we use a semi-normal distribution with
large variance (i.e., the right half of a normal distribution
centered on zero):
P(λk) ∝e−λ2
k/2σ2,
(7)
where σ ≫1 is a ﬁxed hyperparameter and P(λk) = 0 if
λk < 0. In our work we use σ = 100.
With the model speciﬁed, we now run the algorithm
described in Appendix A and obtain a series of samples
(Ar, θr) from the joint distribution P(A, θ|X), where
θ in this case collectively refers to the parameters λ0,
λ1, and ρ, and θr is one realization of these parameters.
Two examples of sampled network structures are shown
in Fig. 2a, out of thousands generated.
As the ﬁgure

5
(a)
1
2
3
4
5
6
7
8
9
10
11
12
13
1
2
3
4
5
6
7
8
9
10
11
12
13
2
4
6
8
10 12
Dolphin
2
4
6
8
10
12
Dolphin
(b)
10
3
10
2
10
1
1
Connection probability
FIG. 2. Reconstruction of the network of dolphins from the
data shown in Fig. 1.
(a) Two examples of sampled net-
work structures. (b) Matrix of posterior edge probabilities,
obtained by averaging over 4000 network samples. Entries in
the matrix corresponding to the edges highlighted in panel (a)
are shown in matching colors.
shows, the two structures are similar but not identical.
This is expected: we should see variability from sample
to sample. When looking over the entire sample set, for
example, the edge between nodes 9 and 10, highlighted
in orange, appears almost always, whereas the edge be-
tween 1 and 8, in blue, appears only rarely. To capture
this variability, we average over samples and compute
the posterior probability of every edge as the fraction of
samples in which the edge occurs. The result is shown in
matrix form in Fig. 2b. Comparing with Fig. 1, we see
that these probabilities are quite diﬀerent from the distri-
bution of number of interactions between dolphin pairs.
Moreover, while the numbers of interactions span a wide
range of values, the probabilities of edges are clustered
around zero and one. This is good news. Edge probabili-
ties near zero and one indicate edges about which we are
relatively certain, either that they exist or that they do
not. Thus we have turned a data set with considerable
variability into a network structure about which we have
high conﬁdence. We discuss this point further below.
In conjunction with these estimates of the network’s
structure, we also compute estimates of the parame-
ters θ, by averaging the parameter samples, just as we
did with the network samples. For instance, we ﬁnd that
λ0 = 0.63 ± 0.22 and λ1 = 14.4 ± 1.5, meaning that dol-
phin pairs interacted an average of 14.4 times during the
experiment if they shared a network connection, but only
0.6 times if they did not. In other words, the eﬀect of
network connections is very pronounced and highly sta-
tistically signiﬁcant. We also ﬁnd that ρ = 0.26 ± 0.06,
indicating that the network is quite dense. (This would
be an unusually high value in human social networks, al-
though the network in this example is small, which tends
to increase density.)
FIG. 3. Statistics of samples drawn from the posterior dis-
tribution P(λ0, λ1, ρ|X) for the data shown in Fig. 1 and the
measurement model given in Eqs. (5)–(6) over four runs with
randomly chosen initial conditions. Only 500 of the 4000 total
samples taken are shown for the sake of visual clarity. Col-
ors indicate the four runs. (a) The logarithm of the posterior
probability. Diﬀerent runs are separated by vertical dotted
lines.
(b) Pair plot showing the relation between sampled
values of parameters, as well as the distribution of individual
parameters on the diagonal.
2.
Sampling quality and goodness of ﬁt
While these results are promising, there is further work
to be done if we are to be conﬁdent in them. In particular,
as is standard with Bayesian calculations, there are two
speciﬁc things we need to check [61]. First, we need to be
sure that the Monte Carlo algorithm is sampling correctly
from the posterior distribution and, second, we need to
test whether our proposed model is in fact a good ﬁt to
the data.
In Monte Carlo calculations of this kind the posterior
probability distribution is typically rugged, meaning it
has multiple local optima, and the sampling algorithm
can as a result get stuck for periods of time in subopti-
mal portions of the sampling space. To test for this issue
we plot in Fig. 3a the log probability of our samples as a
function of time over four diﬀerent runs of the algorithm.
The plot shows that the distribution of values appears
roughly consistent within each run and across diﬀerent
runs, which we take as a sign that the samples have not
failed in obvious ways. In Fig. 3b we compare how the
sampled values for the parameters λ0, λ1, and ρ relate
to one another. These plots, colloquially known as pair
plots, are conventional in Bayesian analysis. In addition
to showing the distribution of values for the individual
parameters on the diagonal (which are more informative
than the simple averages reported in the text above),
these plots can help verify the quality of the samples and
provide some sanity checks.
In our case, the plots re-
veal that all the samples come from approximately the
same region of parameter space regardless of the diﬀer-
ent initialization of the four runs, which tallies with the
uniform sample quality found in Fig. 3a and gives further
evidence that the algorithm is sampling consistently.
Another function of pair plots is to diagnose problems

6
0
20
Observed interactions
0
10
20
30
Predicted interactions
(a)
2000
4000
D(X;
)
2000
4000
D(X;
)
(b)
FIG. 4. Posterior-predictive tests of the model used for the
dolphin network. (a) Average predicted number of interac-
tions between dolphin pairs as a function of the actual ob-
served number of interactions, for ﬁve random data sets gen-
erated from the model.
Colors of the data points indicate
the numbers of samples and R2 = 0.50.
(b) Scatter plot
of discrepancy values.
Each point in this plot corresponds
to one of 500 sets of parameter values, selected at random
from a total of 4000 such sets drawn from the posterior dis-
tribution P(λ0, λ1, ρ|X) during the calculation. Sets having
higher model–model discrepancy D( ˜
X, λ0, λ1, ρ) than data–
model discrepancy D(X, λ0, λ1, ρ) are highlighted in blue,
above the diagonal.
The fraction of such sets gives us the
p-value, which in this case is p = 0.136.
with the model speciﬁcation. Two parameters that can-
not be independently determined from the data, for in-
stance, will have strong correlation, and other issues like
permutation symmetries will also show up in these plots.
In the case of Fig. 3b we ﬁnd the parameters to be only
modestly correlated and there are no major red ﬂags at
this stage.
Having conﬁrmed that the samples are plausibly drawn
from a correctly speciﬁed posterior distribution, the other
thing we need to check is whether our model is actually
a good ﬁt to the data. If the model is a poor one, then
even the best ﬁt it provides may not actually be good ﬁt.
To test the goodness of ﬁt we use two Bayesian tests of
the type known as posterior-predictive assessments. (See
Appendix A and Refs. [18, 62] for discussions.) In these
tests we use the data model P(X| ˆ
A, ˆθ) with parameter
values ˆ
A and ˆθ drawn from our Monte Carlo sample to
generate new data
˜
X, as if we were making measure-
ments on a system that obeyed the ﬁtted model exactly.
Then we compare these new data to the original inputs.
If the model is a good ﬁt, the two should look similar.
An example of such a comparison is shown in Fig. 4.
Panel (a) in the ﬁgure shows the number of times a pair
of dolphins interacts in the simulated data as a function
of the number of times they are observed to interact in
the original data, in ﬁve artiﬁcial data sets ˜
X generated
as above.
If data and model agreed well, most of the
points in this scatter plot would concentrate along the
diagonal line, but in this case they do not. This is our
ﬁrst indication that the ﬁt we have found may not be as
good as we would like. We will see shortly how to make
the ﬁt much better, but let us proceed for the moment
with what we have as an illustration of our goodness-of-
ﬁt analysis.
To more accurately quantify the performance of the
model we can calculate a discrepancy between data
sets [62]:
D(X, θ) =
X
ij
Xij log
Xij
˜Xij(θ)
,
(8)
where ˜Xij(θ) are the data generated from the model with
parameters θ. The discrepancy is essentially a Kullback-
Leibler divergence between the distribution of real and
synthetic data for one sampled value of the parameters.
It functions in this situation as a goodness-of-ﬁt measure:
the smaller the discrepancy, the closer the synthetic data
are to the input.
The discrepancy is not very informative by itself, how-
ever, since it is not clear what kind of values we should
expect to see. For example, even if the model is a per-
fect ﬁt, we should not expect the discrepancy to be zero,
since the randomness of both the data and the model
mean that they are unlikely to agree exactly. To obtain
a point of comparison, therefore, we compute discrepan-
cies between a large number of pairs of simulated data
sets drawn from realizations of the model with the same
parameter values used for the observed data. If the model
were correct, so that simulated and observed data have
the same distribution, then these values would tell us the
typical magnitude we should expect the discrepancy to
have. If the values are mostly smaller than the observed
discrepancy then it indicates the model is unlikely to be
correct; if they are larger then the model is not ruled
out. The fraction of generated discrepancy values that
are larger than the observed discrepancy gives us the p-
value for the model, i.e., the probability that the observed
discrepancy would have been generated if the model were
correct.
Note that the use of the p-value in this situation is
diﬀerent from the way it is used in traditional frequentist
statistics, where it represents the probability of getting a
particular observed value if a null hypothesis were true.
In the traditional scenario a small p-value leads us to
reject the null hypothesis, and, since this is often the
goal of an experiment, small p-values are “good.” In the
present case, the p-value is applied to the model we are
ﬁtting (there is no null/alternative hypothesis) and it just
counts the fraction of artiﬁcial data sets for which we ﬁnd
discrepancies more extreme than the value found when
ﬁtting the model to the true data. So in this situation
small p-values are “bad.”
Figure 4b shows values of the discrepancy for both ar-
tiﬁcial and real data, for 500 sampled values of the model
parameters. Instances where the artiﬁcial discrepancy is
greater than the observed one appear above the diagonal
in the plot and the fraction of such points tells us the
p-value. In this case we ﬁnd p = 0.136. While it is not
appropriate to apply arbitrary cutoﬀs to this (or any) p-
value [62], the value is not as high as we would like it to

7
be, and though we cannot completely rule out the model
the evidence suggests that the ﬁt is not ideal.
3.
Improved model
What can we do to improve the ﬁt?
The standard
approach is to adopt a more elaborate model that is ca-
pable of representing a wider range of data distributions.
In the model we have used so far connections are binary:
dolphin pairs either have a connection or they don’t. We
can create a more nuanced model by allowing three levels
of connection, corresponding to no tie, a weak tie, or a
strong tie. Denoting the three levels by adjacency matrix
elements Aij = 0, 1, and 2, we introduce a new distribu-
tion for Xij when Aij = 2 which is Poisson as before but
with mean λ2:
µij(2, λ2) = λXij
2
Xij! e−λ2
(9)
where λ2 ≥λ1 ≥λ0, and the prior of Eq. (6) becomes
νij(0, ρ) = ρ0 = 1 −ρ1 −ρ2,
(10a)
νij(1, ρ) = ρ1,
(10b)
νij(2, ρ) = ρ2.
(10c)
The ﬁtting and model veriﬁcation procedures follow the
same lines as previously.
This modiﬁed model now ﬁts the data signiﬁcantly bet-
ter, as shown in Fig. 5a. It divides the observed num-
bers of pair interactions into three clear groups centered
around values of about 0, 5, and 25, and the p-value is
now a very respectable 0.722, indicating that there is no
statistical basis to reject this model at all: the model
truly captures the structure of the observed behavior.
Indeed a p-value signiﬁcantly larger than this could be
a sign of problems, indicating overﬁtting.
Unless the
distribution of the discrepancy is strongly skewed, the
expected p-value will normally be around 0.5 when the
model is a perfect ﬁt.
Having found a model that ﬁts the data well, we ex-
amine the inferred network structure, which is shown in
Fig. 5b. The network has three disconnected subgroups
of dolphins, two comprised of strong connections only and
one, the largest of the three, having a mix of strong and
weak connections. The posterior probabilities on all of
the interaction types are close to one, indicating high con-
ﬁdence in the structure of the network. For instance, the
model predicts that nodes 8 and 9 are not connected with
probability 0.99(6), that nodes 7 and 8 share a strong
connection with probability 0.99(9), and that nodes 1
and 8 share a weak connection with probability 0.99(9).
There is just one pair of nodes (1 and 2) whose connec-
tion is hard to classify.
The model indicates that the
tie between these nodes is either weak or strong, with
probabilities 0.51(4) and 0.48(5), respectively. This pair
of dolphins was observed swimming together 12 times, a
0
10
20
30
Observed interactions
0
10
20
30
Predicted interactions
(a)
(b)
1
2
3
4
5
6
7
8
9
10
11
12
13
FIG. 5. Network inference with multiple edge types for the
data shown in Fig. 1 and the measurement model given in
Eqs. (5)–(10). The estimated mean numbers of interactions
are λ0 ≃0.02 when there is no edge between a pair of
dolphins, λ1 = 5.13 when the pair shares a weak tie, and
λ2 = 21.97 when they share a strong one. The corresponding
prior probabilities of edge types are ρ0 = 0.58, ρ1 = 0.28, and
ρ2 = 0.14. (a) Posterior predicted number of interactions be-
tween dolphins as a function of observed number for ﬁve ran-
dom data sets generated from the ﬁtted model (R2 = 0.82).
The highlighted regions correspond, from left to right, to dol-
phins having no tie, a weak tie, and a strong tie. (b) The in-
ferred structure of the network, with weak ties represented by
thin gray edges and strong ties by thicker blue edges. (Nodes 1
and 2 are connected by a thin blue line to reﬂect the fact that
the calculation is ambiguous about the type of this tie.)
number that falls between the weak and strong domains
in the ﬁtted model (see Fig. 5a).
B.
Friendship network of school students
For our second example, we revisit a network analyzed
previously using diﬀerent methods in Refs. [3, 32], a net-
work of friendships between high-school students taken
from the US National Longitudinal Study of Adolescent
to Adult Health (the “AddHealth” study). Although the
method proposed in this paper is mathematically more
complex than that of [3, 32], it is arguably easier to ap-
ply since our analysis pipeline performs most of the cal-
culation automatically. The most demanding step is the
formulation of the model, but once we have a plausible
model the rest of the process is straightforward and me-
chanical.
The AddHealth study is a large study of networks of so-
cial contact among students in schools across the United
States. Students in participating schools were asked to
identify their friends and the basic unit of resulting data
is a friendship nomination: one student says they are
friends with another. Our data matrix X in this case
is thus a non-symmetric one: Xij = 1 if i names j as
a friend and 0 otherwise. There is no guarantee that j
will also name i and in fact there are many instances
in which reported friendships only run in one direction.
If we assume that friendship is fundamentally a bidirec-
tional interaction then this lack of symmetry indicates

8
that the data are necessarily unreliable.
As is done in Refs. [3, 23, 32], we ﬁt the data us-
ing a model in which each student i has an individual
true-positive rate αi and false-positive rate βi. The true-
positive rate is the probability that i names as a friend
another student who is in fact a friend, as determined by
the adjacency matrix. The false-positive rate is the prob-
ability of naming someone who is not actually a friend.
We explicitly allow for diﬀerent true- and false-positive
rates for diﬀerent individuals, since it is widely accepted
that survey respondents vary in the accuracy of their re-
sponses.
In the notation of this paper the equations for the
model are:
µij(1, αi, αj) = αXij
i
(1 −αi)1−XijαXji
j
(1 −αj)1−Xji,
(11a)
µij(0, βi, βj) = βXij
i
(1 −βi)1−XijβXji
j
(1 −βj)1−Xji.
(11b)
For instance, supposing that i and j truly are friends,
the probability of i saying that they are (Xij = 1) while
j says they are not (Xji = 0) is µij(1) = αi(1 −αj).
Conversely, if they are not in fact friends then we instead
get µij(0) = βi(1 −βj).
For the priors we again make the assumption of Eq. (6)
that all edges are a priori equally likely, and assume
a uniform prior on the edge probability ρ and a uni-
form distribution over all values of αi and βi that satisfy
βi < 1
2 < αi. (One could simply assume a uniform prior
on both αi and βi in the range [0, 1] but this leaves some
ambiguity in the model because of the inherent symmet-
ric between edges and non-edges: if we exchange the val-
ues of all αi and all βi and set ρ to 1 −ρ the model re-
mains the same. By making the reasonable assumption
that αi > βi we break this symmetry. The assumption
that βi < 1
2 < αi is not strictly necessary, but turns out
to be helpful for narrowing down the parameter space
and hence improving the speed and convergence of the
calculation [23].)
Figures 6 and 7 show the results of ﬁtting this model
to the data for a single school from the AddHealth data
set. We use one of the smaller schools as our example,
with 521 students who completed a survey and 2182 de-
clared ties, primarily in order to make visualization of
the results easier. We ﬁnd that the Monte Carlo algo-
rithm converges well and gives samples that appear to
accurately characterize the posterior distribution. Fig-
ure 6 shows discrepancy values in a manner analogous to
Fig. 3b for the dolphin network and all values are well
above the diagonal, indicating a good ﬁt to the data.
The inferred network structure is shown in Fig. 7a.
By contrast with the dolphin network example, the pos-
terior probabilities of edges now vary more widely, as
represented by the thickness of the edges in the ﬁgure.
Figure 7b shows the distribution of edge probabilities as
a histogram and many probabilities are again close to
either 1 or 0, indicating a high degree of certainty that
0
10000
20000
D(X;
)
0
50000
100000
150000
D(X;
)
FIG. 6.
Goodness of ﬁt testing for the AddHealth model.
Each point in this plot corresponds to a random parame-
ter set drawn from the posterior distribution P(α, β, ρ|X).
Samples associated with a higher model–model discrepancy
D( ˜
X, θ) than data–model discrepancy D(X, θ) appear above
the dotted line, indicating a good ﬁt between data and model.
these edges either exist or do not, but there are also a sig-
niﬁcant number of edges with intermediate probabilities,
edges about which we are less certain.
The ﬁt also returns values of the true- and false-
positive rates αi and βi for each node, which allow us
to make quantitative statements about how accurately
each individual reports his or her friendships. The av-
erage value of αi over all individuals and all samples
is 0.76, meaning that an estimated 24% of friendships
are going unreported. The average false-positive rate is
0.0065, which sounds small but this result is somewhat
misleading. The network is very sparse, meaning that
almost all edges that could exist do not. It takes only a
small fraction of false-positives among these many non-
edges to generate a signiﬁcant number of errors.
Ar-
guably a more informative measure of false positives is
the precision, which is the fraction of reported friend-
ships that are actually present and is given in this case
by ραi/[ραi + (1 −ρ)βi] [32]. The distribution of values
of the precision is shown in Fig. 7c and ranges from a lit-
tle under 0.2 to a little over 0.75, indicating that in fact
a signiﬁcant fraction of reported friendships—anywhere
from 25% to 80%—are false positives.
These results are largely in agreement with previous
work [32], although there are modest diﬀerences in es-
timated parameter values and network structure, due to
the diﬀerent methodology. We would argue that the fully
Bayesian methodology employed here is more correct in
that it accounts for intrinsic uncertainty in the parame-
ter values. The methodology of [32], which makes use of
an expectation-maximization (EM) algorithm, might be
described as “semi-Bayesian,” computing a full posterior
distribution over the network structure but relying on
point estimates of the parameters. Because the model
used here is a large one, having O(n) parameters, we
expect there to be signiﬁcant uncertainty in the param-
eter values, which is captured by our Bayesian sampling
method. That said, in practice the two methods do lead
to qualitatively consistent conclusions. The key beneﬁts
of the current approach in this case are that it is simpler

9
(a)
0.0
0.2
0.4
0.6
0.8
1.0
Edge probability
10
4
10
2
100
Fraction of edges
(b)
0.0
0.2
0.4
0.6
0.8
1.0
Precision
0
0.05
0.10
Fraction of nodes
(c)
FIG. 7. Inference of a school friendship network from noisy
data. (a) The 521 nodes in this ﬁgure represent the students
at a single school in the AddHealth study and inferred friend-
ships are shown as edges whose thickness indicates the esti-
mated probability that they exist. The size and color of the
nodes indicates the estimated precision of friendship reports
by the corresponding individual, i.e., the fraction of their re-
ported friendships that are inferred to actually exist. Darker
shades indicated less precise individuals and correspond to the
shades in the histogram in panel (c). The average values of
the parameters of the model are ⟨α⟩≃0.7605, ⟨β⟩≃0.0065,
and ⟨ρ⟩≃0.004. (b) The distribution of the probability of
existence of edges. Many values are close to zero or one, in-
dicating conﬁdence that the corresponding edge does or does
not exist, although a signiﬁcant number fall at intermediate
values. (c) The distribution of estimated precision values for
participants.
to implement using standard software, is formally more
correct, and incorporates a natural means for checking
the goodness of ﬁt.
One potential issue with the results is the fact that the
discrepancy values in Fig. 6 are all well above the dotted
line, indicating close ﬁts of the model to the data and
a p-value near 1. A p-value this large can be a warning
sign for overﬁtting, which is a possibility given the large
number of parameters in the model. Such an issue could
not be diagnosed with the methods previously used in
Refs. [3, 32], but our approach makes this possible. One
could address the problem by changing the model, say by
using a more complex model in which instead of ﬁtting
the true- and false-positive parameters we instead draw
them from a hyperprior distribution, such as a beta dis-
tribution, with an associated (small) set of hyperparam-
eters that are ﬁt using Monte Carlo. This approach can
reduce the chances of overﬁtting and would be a good
direction for future work.
IV.
CONCLUSIONS
In this paper we have introduced a general Bayesian
framework for reconstructing networks from observa-
tional data in the case where the data are error prone,
even when the magnitude of the errors is unknown. Our
methods work by ﬁtting a suitable model of the mea-
surement process to the data and there is a large class of
models that is both expressive enough to represent real
data sets accurately and yet simple enough to allow for
easy and automatic statistical inference. The output of
the ﬁtting process is a complete Bayesian posterior dis-
tribution over possible network structures and possible
values of model parameters. We have demonstrated our
methods with two case studies showing how to formulate
suitable models, ﬁt them, assess goodness of ﬁt, and infer
reliable estimates of network structure.
With this work, we hope to promote the adoption of
more rigorous methods for handling measurement error
in network data in a principled manner. The methods
we propose not only achieve this but do so in a man-
ner that is straightforward and requires a minimum of
technical expertise on the part of the user. Practitioners
can use the framework we propose to apply appropriate,
application-speciﬁc models to their data and obtain esti-
mates of network structure in a matter of minutes.
ACKNOWLEDGMENTS
We thank Alec Kirkley for helpful discussions. This
work was funded in part by the James S. McDonnell
Foundation (JGY) and the US National Science Foun-
dation under grant DMS–1710848 (MEJN).
Appendix A: Methods
In this appendix we describe the mathematical and
statistical foundations of our method in detail.

10
1.
Generative models of measurement
Consider an experimental setting in which we have
measurements X of a network’s structure.
The mea-
surements could be as simple as a number of observed
interactions between pairs of nodes, but could also incor-
porate time-series, vector measurements, etc. In general
these measurements do not tell us the exact structure
of the network, but instead give us indirect and poten-
tially noisy information. Our goal is to make the best
estimate we can of the true network structure given the
measurements.
In the general framework we consider here, two nodes
i and j can share connections of various types. In the
simplest case there are just two types: nodes can be either
connected by an edge (type 1) or not (type 0). In a more
complex three-type case the connection could be absent
(type 0), weak (type 1), or strong (type 2), and so on. For
a network of n nodes we encode these connections by an
n × n adjacency matrix A where the matrix element Aij
records the type of connection between nodes i and j. We
can also represent directed networks using an asymmetric
adjacency matrix with Aij being the type of the directed
connection from j to i and Aji being the type from i to j.
Our approach rests on the hypothesis that the ma-
trix X of pairwise measurements is dependent, in a prob-
abilistic fashion, on the adjacency matrix A.
Both A
and X can be either symmetric (for undirected networks)
or asymmetric (for directed ones) and they need not be
of the same type. In friendship networks, for example,
the symmetric relationship of being friends is commonly
probed using asymmetric measurements (person i says
they are friends with person j).
It is this dependence between network and measure-
ment that we exploit to estimate A from X. We formal-
ize the relation using a generative model that speciﬁes
the probability P(X|A, θ) of making the measurements
given the network, plus optionally some additional pa-
rameters represented collectively by θ. Then, applying
Bayes’ rule, we can write the probability of the unknown
quantities A and θ given the measurements as
P(A, θ|X) = P(X|A, θ)P(A|θ)P(θ)
P(X)
.
(A1)
Our goal is to use this equation to infer the network struc-
ture A from the measurements X and to quantify the
errors we might make in doing so.
2.
A ﬂexible class of models
To further simplify the discussion and improve the ef-
ﬁciency of the numerical calculations we make some ad-
ditional assumptions about the model, while keeping the
approach as broad as possible to allow users to easily
adapt it to various types of data and experimental set-
tings.
Of the four probabilities that appear on the right-hand
side of Eq. (A1) one of them P(X) is a constant (since
it depends only on X which is ﬁxed by the experiment)
and hence will play no part in our calculations. The oth-
ers must be speciﬁed to deﬁne our model. We refer to
these three probabilities as the data model P(X|A, θ),
the network model P(A|θ), and the prior on the param-
eters P(θ). Let us consider each of these in turn.
a.
Data model
The data model P(X|A, θ) speciﬁes the probability of
making a particular set of measurements X given the
network and the model parameters.
In specifying this
probability we will make two key assumptions.
First,
we assume that the measurement Xij is only inﬂuenced
by the corresponding element Aij of the adjacency ma-
trix and not by any other elements. Second, we assume
that, conditioned on the network structure A and param-
eters θ, the measurements Xij for diﬀerent node pairs are
independent. Thus, for instance,
P(Xij, Xkl|A, θ) = P(Xij|Aij, θ)P(Xkl|Akl, θ).
The notation here is a bit unwieldy, so for clarity we
introduce the notation µij(Aij, θ) to denote the probabil-
ity P(Xij|Aij, θ) of making the measurement Xij given
the type Aij of the connection between nodes i and j
and given the parameter values θ.
(Where the mean-
ing is clear we may drop the explicit dependence on θ
to simplify our expressions.) With this notation and our
assumption of conditional independence, the probabil-
ity P(X|A, θ) for the data model is simply
P(X|A, θ) =
Y
(i,j)
µij(Aij, θ).
(A2)
The product Q
(i,j) is taken over all unordered pairs of
nodes when the network is undirected and over all or-
dered pairs when it is directed.
Table I gives a selection of possible forms for the data
model for networks with only a single edge type. Gener-
alization to multiple edge types is straightforward. (See
also Ref. [32] for a discussion of a range of models.)
b.
Network model
The network model P(A|θ) can be thought of as our
prior expectation of what the network should look like,
before we make the measurements. By analogy with the
factorized form of the data model in Eq. (A2), we con-
sider network models with the factorized form
P(A|θ) =
Y
(i,j)
νij(Aij, θ),
(A3)
where
we
deﬁne
νij(Aij, θ)
in
a
similar
manner
to µij(Aij, θ), as the prior probability P(Aij|θ) that

11
nodes i and j share a connection of type Aij, given the
parameters θ.
Many standard network models can be
written in this form, including the Erd˝os–R´enyi random
graph, the conﬁguration model, and the stochastic block
model. Some examples of network models are given in
Table II and Ref. [32].
c.
Prior on the parameters
The third component of our generative model, the prior
P(θ) on the parameters, is the simplest.
Our method
does not place any signiﬁcant constraints on the form
of this probability, so one is free to choose almost any
form appropriate to problem at hand, ranging from sim-
ple ﬂat priors or factorized forms to ones that incorpo-
rate complex correlations between parameters. The only
stipulation we make is that the parameters should be
continuous-valued variables (not discrete-valued), which
allows for more eﬃcient sampling procedures (see Sec-
tion A 4).
3.
Inference in theory
Gathering the elements deﬁned above and substituting
them into Eq. (A1) we obtain the complete joint posterior
distribution for the model:
P(A, θ|X) = P(X|θ, A)P(A|θ)P(θ)
P(X)
,
∝P(θ)
Y
(i,j)
µij(Aij)νij(Aij).
(A4)
This distribution tells us the probability of a network
structure and a set of parameter values given the ob-
served measurements. From it we can derive a variety of
further useful quantities, such as the probability of the
network structure independent of the parameters, which
is given by
P(A|X) =
Z
P(A, θ|X) dθ.
(A5)
Even more useful, perhaps, is the probability of having
an edge of a given type between two speciﬁc nodes i, j:
P(Aij = k|X) =
Z
P(Aij = k, θ|X) dθ
∝
Z
µij(k, θ)νij(k, θ)P(θ) dθ,
(A6)
where we have used Eq. (A4).
If we instead want to learn something about a param-
eter φ ∈θ then we can compute its distribution as
P(φ|X) =
X
A
Z
P(θ′, φ, A|X) dθ′,
(A7)
where θ′ is the parameter set with φ excluded.
Each of these quantities can be considered a special
case of the posterior average of a general function f(A, θ)
of network structure and parameters, thus:
⟨f(A, θ)⟩=
X
A
Z
f(A, θ)P(θ, A|X) dθ.
(A8)
There are a number of approaches we could take to
computing expectations of this form [35].
One possi-
bility is to use an expectation–maximization (EM) al-
gorithm to compute the distribution over potential net-
works P(A|θ, X) as well as a point-estimate of θ [3, 32].
Alternatively, following [31, 63], we can integrate out the
parameters θ analytically to derive the marginal distri-
bution P(A|X) over the networks alone. Both these ap-
proaches, however, only allow us to compute averages
over network structures and not over parameters. They
are moreover not in line with our goal of providing al-
most automatic inference for arbitrary choices of models,
the EM approach because it calls for the solution of (of-
ten non-linear) equations speciﬁc to the model, and the
marginal-based approach because it works only for mod-
els amenable to closed-form integration.
Instead, therefore, we employ a generalization of a
method introduced in [33], which harnesses standard
mixture-modeling techniques, adapting them to the net-
work context.
4.
Inference in practice
The general idea behind our method is to compute
expectations of the form (A8) in two manageable steps
by factorizing the joint posterior as
P(A, θ|X) = P(A|θ, X)P(θ|X).
(A9)
This factorization tells us that we can draw samples from
the joint posterior by ﬁrst sampling sets of parameter val-
ues θ from the marginal distribution P(θ|X) and then
sampling networks A from P(A|θ, X) with these param-
eter values. If we sample m diﬀerent parameter sets and
then n networks for each set, we end up with mn net-
work/parameter pairs, which we number r = 1 . . . mn.
Then we can estimate the average in Eq. (A8) as
⟨f(A, θ)⟩=
X
A
Z
f(A, θ)P(A|θ, X)P(θ|X) dθ
≃
1
mn
mn
X
r=1
f(Ar, θr).
(A10)
This expression is completely general and holds for any
posterior, but for the class of models we consider here
there are, as we now show, particularly eﬃcient methods
that can help us quickly generate the samples we need.

12
Model
Parameters
Data probability
Binomial with
uniform errors
True positive rate α ∈[0, 1]
µij(1) = αXij(1 −α)Nij−Xij
False positive rate β ∈[0, 1]
µij(0) = βXij(1 −β)Nij−Xij
Binomial with
node-dependent
errors
True positive rate αi ∈[0, 1] for node i
µij(1) = α
Xij
i
(1 −αi)Nij−Xij
False positive rate βi ∈[0, 1] for node i
µij(0) = β
Xij
i
(1 −βi)Nij−Xij
Poisson with
uniform errors
Means λ1, λ0 for edges and non-edges
µij(1) = λ
Xij
1
e−λ1/Xij!
µij(0) = λ
Xij
0
e−λ0/Xij!
Poisson with
node propensity
Normalized node propensity 0 < ηi < 1
(P ηi = 1) and base rates λ1, λ0
µij(1) = (λ1ηiηj)Xije−λ1ηiηj/Xij!
µij(0) = (λ0ηiηj)Xije−λ0ηiηj/Xij!
TABLE I. Example data models for undirected networks with one edge type. Here Nij represents the number of times the
node pair i, j was measured and Xij represents how many of those times an edge was observed to exist.
Model
Parameters
Edge probability
Random graph
Edge probability ρ
νij(1) = ρ
“Soft” conﬁguration model
Node pseudo-degree λi
νij(1) = 1/(1 + e−λiλj)
Stochastic block model
Node i belongs to group gi and edge
probability between groups r and s is ωrs
νij(1) = ωgigj
Random graph with
multiple edge types
Probability of type-k edge ρk
νij(k) = ρk
Poisson multigraph
Mean edge number ω
νij(k) = ωke−ω/k!
TABLE II. Network models for the prior probability νij of an edge between nodes i and j.
a.
Generating parameter samples
The ﬁrst step of the sampling algorithm draws values
of the parameters θ from the marginal distribution
P(θ|X) =
X
A
P(θ, A|X),
(A11)
where the sum runs over all the possible matrices A. For
models with the factorized form (A4) we have
P(θ|X) ∝P(θ)
X
A
Y
(i,j)
µij(Aij, θ)νij(Aij, θ)
∝P(θ)
Y
(i,j)
X
k
µij(k, θ)νij(k, θ).
(A12)
Modern probabilistic programming languages make it
easy
to
generate
random
samples
from
factorized
marginals of this kind. Our code is written in the prob-
abilistic language Stan, which implements the technique
known as Hamiltonian Monte Carlo to generate samples
automatically and eﬃciently—see Refs. [64, 65] for an in-
troduction. Evaluating P(θ|X) involves a product over
pairs (i, j) of nodes, of which there are O(n2), meaning
that in general generating a sample takes O(n2) time. In
many cases, however, the time complexity can be reduced
to O(n) by pooling terms in the product, as discussed in
Sec. A 6 b.
b.
Generating network samples
Given sampled values θ1, . . . , θm of the parameters, the
next step is to generate samples of the network A from
the distribution P(A|θ, X) for these parameter values.
This is straightforward for the factorized model assumed
here, since node pairs are independent and we can sample
each one separately.
Speciﬁcally, using Eqs. (A4) and
(A12), we have
P(A|θ, X) = P(θ, A|X)
P(θ|X)
=
Q
(i,j) µij(Aij)νij(Aij)
Q
(i,j)
P
k µij(k)νij(k)
=
Y
(i,j)
Qij(Aij, θ),
(A13)
where
Qij(k, θ) =
µij(k)νij(k)
P
k′ µij(k′)νij(k′)
(A14)
is the posterior probability that nodes i and j are joined
by an edge of type k.
Generating networks is simply
a matter of drawing a value Aij = k for each node
pair independently from the distribution over k implied
by Qij(k). Again, naively this takes time O(n2) for all
node pairs, but on a sparse network the speed can be
improved by sampling only those edges with k > 0 and
assuming k = 0 for all others.

13
To estimate the average ⟨f(A, θ)⟩, we generate a series
of parameter sets θ using Eq. (A12) and for each of these
a series of networks using Eq. (A13), then evaluate the
average with Eq. (A10).
5.
Assessing goodness of ﬁt
The method described above is simple, eﬃcient, and
often gives good results. As described in the main text,
however, the method can fail if the model itself is faulty—
if the model is a poor representation of the system, failing
to ﬁt the data for any parameter values. It’s important
therefore to verify that the ﬁt between model and data is
good, which can be done with the standard technique of
posterior-predictive assessment. As described in the main
text, this involves generating synthetic data ˜
X from the
distribution implied by the ﬁtted model:
P( ˜
X|X) =
Z X
A
P( ˜
X|θ, A)P(θ, A|X) dθ.
(A15)
This distribution weights all the possible parameters θ
and networks A with their appropriate posterior proba-
bilities and tells us the probability that a new data set ˜
X
would have if it were truly generated by the model with
these inputs. The idea of the posterior-predictive assess-
ment is to compare these synthetic data with the original
input X. If the two look alike then the model has cap-
tured the data well; otherwise, it has not.
There are a number of ways to quantify the similarity
of ˜
X and X. For instance, one can compute the average
⟨˜Xij⟩=
X
˜
X
P( ˜
X|X) ˜Xij,
(A16)
and compare the result with Xij. Visualizing the matrix
of residues ⟨˜
X⟩−X, the distribution of these residues,
or how they depend on Xij allows one to easily spot sys-
tematic issues with the model [18]. Such calculations are
not costly in practice: the distribution (A15) is just an
average of a known function of A, θ over the posterior
distribution and has the same general form as Eq. (A10),
so it can be evaluated numerically by the same methods.
In this particular case, however, we can do even better,
skipping the network sampling step altogether and mak-
ing an estimate directly from the parameter samples. To
do this, we write the distribution of Eq. (A15) in the form
P( ˜
X|X) =
Z X
A
P( ˜
X|θ, A)P(A|θ, X)P(θ|X) dθ
=
Z
P(θ|X)
X
A
Y
(i,j)
˜µij(Aij, θ) Qij(Aij, θ) dθ
=
Z
P(θ|X)
Y
(i,j)
X
k
˜µij(k, θ) Qij(k, θ) dθ,
(A17)
where have used Eqs. (A2) and (A13) in the second line,
and ˜µij(k, θ) is the probability of generating a synthetic
measurement ˜Xij given that (i, j) is an edge of type k.
This expression is now independent of A and only re-
quires an average over θ to evaluate.
Using this expression for P( ˜
X|X), we can write the
average ⟨˜Xij⟩in Eq. (A16) as
⟨˜Xij⟩=
Z
P(θ|X)
X
k

˜µij(k, θ)

Qij(k, θ) dθ,
(A18)
which we evaluate numerically as
⟨˜Xij⟩≃1
m
m
X
r=1
X
k

˜µij(k, θr)

Qij(k, θr).
(A19)
Note that

˜µij(k, θr)

usually has a simple closed form,
since it is just the mean of ˜Xij within the data model
with parameters θr.
A visual inspection of the residues between ˜
X and X
is often enough to reveal issues with goodness of ﬁt, but
one can carry out a more formal model assessment using
any of a variety of discrepancy measures that quantify
the distance between the synthetic data ˜
X and the orig-
inal X [62]. The average value of such a discrepancy will
always be greater than zero, since one does not expect the
synthetic and original data to agree perfectly even with
a perfect model. To obtain a baseline against which dis-
crepancy values can be compared, we therefore compute
the discrepancy between synthetic measurements ˜
X and
their associated predictions, calculating a model-versus-
model discrepancy distribution.
In the calculations presented here we make use of the
log-likelihood ratio discrepancy:
D(X, θr) =
X
(i,j)
Xij log
Xij
˜Xij(θr)
,
(A20)
where ˜Xij(θr) is evaluated using Eq. (A19) with the sam-
pled parameter values θr. This discrepancy is reminiscent
of a Kullback-Leibler divergence, with the primary dif-
ference being that it compares unnormalized quantities
rather than normalized probability distributions. That
said, the norm of the two sets of measurements should
be similar, since the whole purpose of the calculation is
to reproduce the original observations. Hence, one can
usually interpret the discrepancy in more or less the same
way as a divergence: the smaller the divergence the bet-
ter the ﬁt (although values slightly less than zero can
occur, which is not true of a true divergence).
We compute the distribution of the discrepancy and
the reference distribution
˜
X simultaneously using the
method introduced in Ref. [62]. We go through each net-
work/parameter sample Ar, θr and generate a single real-
ization ˜
X of the synthetic data from the data model, then
compute the two discrepancies D( ˜
X, θr) and D(X, θr)
using the analog of Eq. (A19). From the resulting sets

14
of discrepancy values one can then compute the p-value
p = P[D(X, θr) > D( ˜
X, θr)], which is the fraction of
artiﬁcial data sets with discrepancy at least as large as
the observed value. The model is rejected if the p-value
is too small. This calculation does not cost much com-
putation time since we are merely reusing the samples
already generated for estimation purposes.
6.
Implementation
In this section we discuss details of implementation
of the algorithm, including a number of techniques for
improving speed and numerical accuracy which can be
useful with large data sets.
a.
Sampling networks
One of the more computationally costly steps in the
algorithm is the generation of sample networks from the
conditional posterior distribution P(A|θ, X).
Naively
generating the network by ﬂipping a biased coin for every
node pair i, j takes time O(n2) on a network of n nodes.
For some models on sparse networks this time can be
reduced by explicitly sampling only the edges that exist.
That is, all edges are assumed not to exist, except for
a sparse sample that are generated in accordance with
the ﬁtted model. For instance, with the simple “uniform
error” model of Table I, the posterior probabilities Qij
of edges are a unique function Q(X) of the number of
observations Xij of the edge in question. With this in
mind we deﬁne Σ = P
(i,j) Qij = P
X n(X)Q(X) where
n(X) = P
(i,j) δ(X, Xij) is the number of node pairs with
X observations and δ(x, y) is the Kronecker delta.
The value of Σ can be calculated rapidly once n(X)
is known, then we can generate a sampled network by
ﬁrst drawing an integer M ∼Poisson(Σ) to represent
the number of edges in the network, and then generat-
ing M random edges with probabilities Qij with stan-
dard “roulette wheel” proportional sampling using binary
search.
The complete process takes time O(M log n),
which on a sparse network will be much faster than the
O(n2) of the naive algorithm.
In other cases we may be able to skip the process of
network sampling altogether, although at the price of still
having to perform O(n2) operations. Speciﬁcally, when
we want to calculate the average of a function f that
factorizes over node pairs thus
f(A, θ) =
Y
(i,j)
gij(Aij, θ),
(A21)
we can write the average as
⟨f(A, θ)⟩=
X
A
Z
f(A, θ)P(A|θ, X)P(θ|X) dθ
=
Z
P(θ|X)
Y
(i,j)
X
k
[gij(k, θ)Qij(k, θ)] dθ. (A22)
Now we sample m sets of parameter values θr as usual,
but generate no networks A, and the average we want is
given by
⟨f(A, θ)⟩≃1
m
m
X
r=1
Y
(i,j)
X
k
gij(k, θr)Qij(k, θr).
(A23)
b.
Sampling parameters
Generating sample values of the parameters also takes
time O(n2) in general, because the right-hand side of
Eq. (A12) involves a product over pairs of nodes. For
some models, however, we may be able to evaluate this
product more rapidly by methods similar to those de-
scribed for sampling networks above. Taking again the
example of the “uniform error” model from Table I, the
probability µij(k) is a function µ(X, k, θ) only of the
number of observations Xij of the corresponding edge
(and k and θ) and νij(k) is a function of k and θ only.
This means we can group terms in the product and write
Y
(i,j)
X
k
µij(k, θ)νij(k, θ) =
Y
X
X
k
µ(X, k, θ)ν(k, θ)
n(X)
,
(A24)
which saves considerable time.
[1] M. Newman, Networks. Oxford University Press, Oxford,
2nd edition (2018).
[2] E. D. Kolaczyk, Statistical Analysis of Network Data.
Springer, New York, NY (2009).
[3] M. E. J. Newman, Network structure from rich but noisy
data. Nat. Phys. 14, 542–545 (2018).
[4] T. Ito et al., Toward a protein–protein interaction map
of the budding yeast: A comprehensive system to exam-
ine two-hybrid interactions in all possible combinations
between the yeast proteins. Proc. Natl. Acad. Sci. USA
97, 1143–1147 (2000).
[5] N. J. Krogan et al., Global landscape of protein com-
plexes in the yeast Saccharomyces cerevisiae. Nature 440,
637–643 (2006).
[6] E. Sprinzak, S. Sattath, and H. Margalit, How reliable
are experimental protein-protein interaction data?
J.
Mol. Biol. 327, 919–923 (2003).
[7] T. Rolland et al., A proteome-scale map of the human
interactome network. Cell 159, 1212–1226 (2014).
[8] S. Wasserman and K. Faust, Social Network Analysis.
Cambridge University Press, Cambridge (1994).

15
[9] E. Vaquera and G. Kao, Do you like me as much as I
like you? Friendship reciprocity and its eﬀects on school
outcomes among adolescents. Soc. Sci. Res. 37, 55–72
(2008).
[10] B. Ball and M. E. J. Newman, Friendship networks and
social status. Network Science 1, 16–30 (2013).
[11] M. McPherson, L. Smith-Lovin, and J. M. Cook, Birds of
a feather: Homophily in social networks. Annual Review
of Sociology 27, 415–444 (2001).
[12] P. W. Holland and S. Leinhardt, The structural implica-
tions of measurement error in sociometry. J. Math. So-
ciol. 3, 85–111 (1973).
[13] L. Amini, A. Shaikh, and H. Schulzrinne, Issues with
inferring internet topological attributes. Comput. Com-
mun. 27, 557–567 (2004).
[14] H. Whitehead, Analyzing Animal Societies: Quantita-
tive Methods for Vertebrate Social Analysis. University
of Chicago Press, Chicago, IL (2008).
[15] O. Sporns, Networks of the Brain. MIT Press, Cam-
bridge, MA (2010).
[16] D. J. Wang, X. Shi, D. A. McFarland, and J. Leskovec,
Measurement error in network data: A re-classiﬁcation.
Soc. Netw. 34, 396–409 (2012).
[17] J. Wiese, J.-K. Min, J. I. Hong, and J. Zimmerman,
You never call, you never write: Call and SMS logs do
not always indicate tie strength. In Proceedings of the
18th ACM Conference on Computer Supported Coopera-
tive Work & Social Computing, pp. 765–774 (2015).
[18] A. Gelman and C. R. Shalizi, Philosophy and the practice
of Bayesian statistics. Br. J. Math. Stat. Psychol. 66, 8–
38 (2013).
[19] N. Eagle and A. Pentland, Reality mining: Sensing com-
plex social systems. J. Pers. Ubiquitous Comput. 10,
255–268 (2006).
[20] N. Eagle, A. S. Pentland, and D. Lazer, Inferring friend-
ship network structure by using mobile phone data. Proc.
Natl. Acad. Sci. USA 106, 15274–15278 (2009).
[21] D. J. Crandall, L. Backstrom, D. Cosley, S. Suri, D. Hut-
tenlocher, and J. Kleinberg, Inferring social ties from ge-
ographic coincidences. Proc. Natl. Acad. Sci. USA 107,
22436–22441 (2010).
[22] J. Cranshaw, E. Toch, J. Hong, A. Kittur, and N. Sadeh,
Bridging the gap between physical location and online
social networks. In Proceedings of the 12th ACM Interna-
tional Conference on Ubiquitous Computing, pp. 119–128
(2010).
[23] C. T. Butts, Network inference, error, and informant
(in)accuracy: A Bayesian approach. Soc. Netw. 25, 103–
140 (2003).
[24] C. M. Le, K. Levin, and E. Levina, Estimating a network
from multiple noisy realizations. Electron. J. Stat. 12,
4697–4740 (2018).
[25] R. Tang et al., Connectome smoothing via low-rank ap-
proximations. IEEE Trans. Med. Imaging. 38, 1446–1456
(2018).
[26] L. Wang, Z. Zhang, and D. Dunson, Common and indi-
vidual structure of brain networks. Ann. Appl. Stat. 13,
85–112 (2019).
[27] R. Jansen et al., A Bayesian networks approach for pre-
dicting protein-protein interactions from genomic data.
Science 302, 449–453 (2003).
[28] X. Jiang and E. D. Kolaczyk, A latent eigenprobit model
with link uncertainty for prediction of protein-protein in-
teractions. Stat. Biosci. 4, 84–104 (2012).
[29] A. Birlutiu, F. d’Alch´e Buc, and T. Heskes, A Bayesian
framework for combining protein and network topology
information for predicting protein-protein interactions.
IEEE/ACM Transactions on Computational Biology and
Bioinformatics 12, 538–550 (2014).
[30] C. E. Priebe, D. L. Sussman, M. Tang, and J. T. Vogel-
stein, Statistical inference on errorfully observed graphs.
J. Comput. Graph. Stat 24, 930–953 (2015).
[31] T. P. Peixoto, Reconstructing networks with unknown
and heterogeneous errors. Phys. Rev. X 8, 041011 (2018).
[32] M. E. J. Newman, Estimating network structure from un-
reliable measurements. Phys. Rev. E 98, 062321 (2018).
[33] J.-G. Young, F. S. Valdovinos, and M. E. J. Newman,
Reconstruction of plant–pollinator networks from obser-
vational data. Preprint bioRxiv:754077 (2019).
[34] D. M. Titterington, A. Smith, and U. E. Makov, Statisti-
cal analysis of ﬁnite mixture distributions. Wiley, (1985).
[35] G. J. McLachlan and D. Peel, Finite mixture models. Wi-
ley (2004).
[36] D. Liben-Nowell and J. Kleinberg, The link-prediction
problem for social networks. J. Assoc. Inf. Sci. Technol.
58, 1019–1031 (2007).
[37] A. Clauset, C. Moore, and M. E. J. Newman, Hierarchical
structure and the prediction of missing links in networks.
Nature 453, 98–101 (2008).
[38] R. Guimer`a and M. Sales-Pardo, Missing and spurious
interactions and the reconstruction of complex networks.
Proc. Natl. Acad. Sci. USA 106, 22073–22078 (2009).
[39] M. Huisman, Imputation of missing network data: Some
simple procedures. J. Soc. Struct. 10, 1–29 (2009).
[40] M. Kim and J. Leskovec, The network completion prob-
lem: Inferring missing nodes and edges in networks. In
B. Liu, H. Liu, C. Clifton, T. Washio, and C. Kamath
(eds.), Proceedings of the 2011 SIAM International Con-
ference on Data Mining, pp. 47–58, Society for Industrial
and Applied Mathematics, Philadephia, PA (2011).
[41] I. Brugere, B. Gallagher, and T. Y. Berger-Wolf, Network
structure inference, a survey: Motivations, methods, and
applications. ACM Comput. Surv. 51, 24:1–24:39 (2018).
[42] Q. Li, Y. Zheng, X. Xie, Y. Chen, W. Liu, and W.-Y. Ma,
Mining user similarity based on location history. In Pro-
ceedings of the 16th ACM Sigspatial International Con-
ference on Advances in Geographic Information Systems
(2008).
[43] M. Bansal, V. Belcastro, A. Ambesi-Impiombato, and
D. Di Bernardo, How to infer gene networks from ex-
pression proﬁles. Mol. Syst. Biol. 3, 78 (2007).
[44] M. Gomez-Rodriguez, J. Leskovec, and A. Krause, In-
ferring networks of diﬀusion and inﬂuence. ACM Trans.
Knowl. Discov. Data 5, 21 (2012).
[45] P. Netrapalli and S. Sanghavi, Learning the graph of epi-
demic cascades. In Proceedings of the 12th ACM SIG-
METRICS Joint International Conference on Measure-
ment and Modeling of Computer Systems, pp. 211–222,
Association of Computing Machinery, New York (2012).
[46] T. Squartini and D. Garlaschelli, Maximum-Entropy Net-
works: Pattern Detection, Network Reconstruction and
Graph Combinatorics. Springer, Berlin (2017).
[47] M. Yuan and Y. Lin, Model selection and estimation
in the Gaussian graphical model. Biometrika 94, 19–35
(2007).
[48] P. Orbanz, Subsampling large graphs and invariance in
networks. Preprint arXiv:1710.04217 (2017).

16
[49] M. P. H. Stumpf, C. Wiuf, and R. M. May, Subnets of
scale-free networks are not scale-free: Sampling proper-
ties of networks. Proc. Natl. Acad. Sci. USA 102, 4221–
4224 (2005).
[50] S. H. Lee, P.-J. Kim, and H. Jeong, Statistical properties
of sampled networks. Phys. Rev. E 73, 016102 (2006).
[51] C. T. Butts, Revisiting the foundations of network anal-
ysis. Science 325, 414–416 (2009).
[52] A. A. Ferreira, M. A. Goncalves, and A. H. F. Laender,
A brief survey of automatic methods for author name
disambiguation. SIGMOD Record 41, 15–26 (2012).
[53] G. M. Namata, B. London, and L. Getoor, Collective
graph identiﬁcation. ACM Trans. Knowl. Discov. Data
10, 25 (2016).
[54] J. J. Pfeiﬀer and J. Neville, Methods to determine node
centrality and clustering in graphs with uncertain struc-
ture. In Fifth International AAAI Conference on Weblogs
and Social Media (2011).
[55] F. Bonchi, F. Gullo, A. Kaltenbrunner, and Y. Volkovich,
Core decomposition of uncertain graphs. In Proceedings
of the 20th ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining, pp. 1316–1325
(2014).
[56] T. Martin, B. Ball, and M. E. J. Newman, Structural in-
ference for uncertain networks. Phys. Rev. E 93, 012306
(2016).
[57] T. Poisot, A. R. Cirtwill, K. Cazelles, D. Gravel, M.-J.
Fortin, and D. B. Stouﬀer, The structure of probabilistic
networks. Methods Ecol. Evol. 7, 303–312 (2016).
[58] A. Khan, Y. Ye, and L. Chen, On Uncertain Graphs.
Morgan & Claypool, San Rafael, CA (2018).
[59] R. C. Connor, R. A. Smolker, and A. F. Richards, Dol-
phin alliances and coalitions. In Coalitions and Alliances
in Humans and Other Animals, p. 443. Oxford University
Press, Oxford (1992).
[60] J. B. Brask, S. Ellis, and D. P. Croft, Animal social
networks-an introduction for complex systems scientists.
Preprint arXiv:2005.09598 (2020).
[61] A. Gelman, H. S. Stern, J. B. Carlin, D. B. Dunson,
A. Vehtari, and D. B. Rubin, Bayesian Data Analysis,
3rd edition. Chapman and Hall/CRC Press, New York,
NY (2013).
[62] A. Gelman, X.-L. Meng, and H. Stern, Posterior predic-
tive assessment of model ﬁtness via realized discrepan-
cies. Stat. Sin. 6, 733–760 (1996).
[63] T. P. Peixoto, Network reconstruction and community
detection from dynamics. Phys. Rev. Lett. 123, 128301
(2019).
[64] M. Betancourt, A conceptual introduction to Hamilto-
nian Monte Carlo. Preprint arXiv:1701.02434 (2017).
[65] B. Carpenter et al., Stan: A probabilistic programming
language. J. Stat. Softw. 76, 1–32 (2017).

