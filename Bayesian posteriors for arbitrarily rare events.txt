Bayesian posteriors for arbitrarily rare events
The MIT Faculty has made this article openly available. Please share 
how this access benefits you. Your story matters.
Citation
Fudenberg, Drew, Kevin He, and Lorens A. Imhof. â€œBayesian
Posteriors for Arbitrarily Rare Events.â€ Proceedings of the National
Academy of Sciences 114, no. 19 (April 25, 2017): 4925â€“4929. Â© 2018
National Academy of Sciences
As Published
http://dx.doi.org/10.1073/pnas.1618780114
Publisher
National Academy of Sciences
Version
Final published version
Citable link
http://hdl.handle.net/1721.1/113241
Terms of Use
Article is made available in accordance with the publisher's
policy and may be subject to US copyright law. Please refer to the
publisher's site for terms of use.

ECONOMIC
SCIENCES
STATISTICS
Bayesian posteriors for arbitrarily rare events
Drew Fudenberga,1, Kevin Heb,1, and Lorens A. Imhofc,d,1
aDepartment of Economics, Massachusetts Institute of Technology, Cambridge, MA 02139; bDepartment of Economics, Harvard University, Cambridge, MA
02138; cDepartment of Statistics, Bonn University, 53113 Bonn, Germany; and dHausdorff Center for Mathematics, Bonn University, 53113 Bonn, Germany
Contributed by Drew Fudenberg, March 27, 2017 (sent for review November 14, 2016; reviewed by Keisuke Hirano, Demian Pouzo, and Bruno Strulovici)
We study how much data a Bayesian observer needs to correctly
infer the relative likelihoods of two events when both events
are arbitrarily rare. Each period, either a blue die or a red die is
tossed. The two dice land on side 1 with unknown probabilities
p1 and q1, which can be arbitrarily low. Given a data-generating
process where p1 â‰¥cq1, we are interested in how much data are
required to guarantee that with high probability the observerâ€™s
Bayesian posterior mean for p1 exceeds (1 âˆ’Î´)c times that for q1.
If the prior densities for the two dice are positive on the interior
of the parameter space and behave like power functions at the
boundary, then for every Ïµ > 0, there exists a ï¬nite N so that the
observer obtains such an inference after n periods with probabil-
ity at least 1 âˆ’Ïµ whenever np1 â‰¥N. The condition on n and p1 is
the best possible. The result can fail if one of the prior densities
converges to zero exponentially fast at the boundary.
rare event | Bayes etimate | uniform consistency | multinomial
distribution | signaling game
S
uppose a physician is deciding between a routine surgery and
a newly approved drug for her patient. Either treatment can,
in rare cases, lead to a life-threatening complication. She adopts
a Bayesian approach to estimate the respective probability of
complication, as is common among practitioners in medicine
when dealing with rare events; see, for example, refs. 1 and 2
on the â€œzero-numerator problem.â€ She reads the medical litera-
ture to learn about n patient outcomes associated with the two
treatments and chooses the new drug if and only if her poste-
rior mean regarding the probability of complication due to the
drug is lower than (1 âˆ’Î´) times that of the surgery. As the true
probability of complication becomes small for both treatments,
how quickly does n need to increase to ensure that the physician
will correctly choose surgery with probability at least 1 âˆ’Ïµ when
surgery is in fact the safer option?
Phrased more generally, we study how much data are required
for the Bayesian posterior means on two probabilities to respect
an inequality between them in the data-generating process,
where these true probabilities may be arbitrarily small. Each
period, one of two dice, blue or red, is chosen to be tossed. The
choices can be deterministic or random, but have to be indepen-
dent of past outcomes. The blue and red dice land on side k with
unknown probabilities pk and qk, and the outcomes of the tosses
are independent of past outcomes. Say that the posterior beliefs
of a Bayesian observer satisfy (c, Î´) monotonicity for side Â¯k if his
posterior mean for pÂ¯k exceeds (1 âˆ’Î´)c times that for qÂ¯k when-
ever the true probabilities are such that pÂ¯k â‰¥cqÂ¯k. We assume
the prior densities are continuous and positive on the interior
of the probability simplex and behave like power functions at the
boundary. Then we show that, under a mild condition on the fre-
quencies of the chosen colors, for every Ïµ > 0, there exists a ï¬nite
N so that the observer holds a (c, Î´)-monotonic belief after n
periods with probability at least 1 âˆ’Ïµ whenever npÂ¯k â‰¥N . This
condition means that the expected number of times the blue die
lands on side Â¯k must exceed a constant that is independent of
the true parameter. Examples show that the sample size condi-
tion is the best possible and that the result can fail if one of the
prior densities converges to zero exponentially fast at the bound-
ary. A crucial aspect of our problem is the behavior of estimates
when the true parameter value approaches the boundary of the
parameter space, a situation that is rarely studied in a Bayesian
context.
Suppose that in every period, the blue die is chosen with the
same probability and that outcome Â¯k is more likely under the
blue die than under the red one. Then, under our conditions, an
observer who sees outcome Â¯k but not the die color is very likely
to assign a posterior odds ratio to blue vs. red that is not much
below the prior odds ratio. That is, the observer is unlikely to
update her beliefs in the wrong direction. This corollary is used
in ref. 3 to provide a learning-based foundation for equilibrium
reï¬nements in signaling games.
The best related result known so far is a consequence of
the uniform consistency result of Diaconis and Freedman in
ref. 4. Their result leads to the desired conclusion only under
the stronger condition that the sample size is so large that the
expected number of times the blue die lands on side Â¯k exceeds a
threshold proportional to 1/pÂ¯k. That is, the threshold obtained
from their result explodes as pÂ¯k approaches zero.
Our improvement of the sample size condition is made pos-
sible by a pair of inequalities that relate the Bayes estimates to
observed frequencies. Like the bounds of ref. 4, the inequalities
apply to all sample sequences without exceptional null sets and
they do not involve true parameter values. Our result is related to
a recent result of ref. 5, which shows that, under some conditions,
the posterior distribution converges faster when the true param-
eter is on the boundary. Our result is also related to ref. 6, which
considers a half space not containing the maximum-likelihood
estimate of the true parameter and studies how quickly the pos-
terior probability assigned to the half space converges to zero.
Bayes Estimates for Multinomial Probabilities
We ï¬rst consider the simpler problem of estimating for a sin-
gle K-sided die the probabilities of landing on the various
sides. Suppose the die is tossed independently n times. Let
X n
k denote the number of times the die lands on side k. Then
Signiï¬cance
Many decision problems in contexts ranging from drug safety
tests to game-theoretic learning models require Bayesian com-
parisons between the likelihoods of two events. When both
events are arbitrarily rare, a large data set is needed to reach
the correct decision with high probability. The best result in
previous work requires the data size to grow so quickly with
rarity that the expectation of the number of observations of
the rare event explodes. We show for a large class of pri-
ors that it is enough that this expectation exceeds a prior-
dependent constant. However, without some restrictions on
the prior the result fails, and our condition on the data size is
the weakest possible.
Author contributions: D.F., K.H., and L.A.I. designed research, performed research, and
wrote the paper.
Reviewers: K.H., Pennsylvania State University; D.P., University of California, Berkeley;
and B.S., Northwestern University.
The authors declare no conï¬‚ict of interest.
1To whom correspondence should be addressed. Email: drew.fudenberg@gmail.com.
This article contains supporting information online at www.pnas.org/lookup/suppl/doi:10.
1073/pnas.1618780114/-/DCSupplemental.
www.pnas.org/cgi/doi/10.1073/pnas.1618780114
PNAS
|
May 9, 2017
|
vol. 114
|
no. 19
|
4925â€“4929

X n = (X n
1 , . . . , X n
K ) has a multinomial distribution with param-
eter n âˆˆN and unknown parameter p = (p1, . . . , pK ) âˆˆâˆ†, where
N is the set of positive integers and âˆ†= {p âˆˆ[0, 1]K : p1 + Â· Â· Â· +
pK = 1}. Let N0 = N âˆª{0}. Let Ï€ be a prior density on âˆ†
with respect to the Lebesgue measure Î» on âˆ†, normalized by
Î»(âˆ†) = 1/(K âˆ’1)!. Let Ï€(Â·|X n) be the posterior density after
observing X n.
Motivated by applications where some of the pk can be arbi-
trarily small, we are interested in whether the relative error of
the Bayes estimator Ë†pk(X n) =
R
pkÏ€(p|X n)dÎ»(p) is small with
probability close to 1, uniformly on large subsets of âˆ†. Specif-
ically, given k âˆˆ{1, . . . , K} and Ïµ > 0, we seek conditions on n
and p and the prior, so that
Pp(|Ë†pk(X n) âˆ’pk| < pkÏµ) â‰¥1 âˆ’Ïµ.
[1]
A subscript on P or E indicates the parameter value under which
the probability or expectation is to be taken.
For a wide class of priors, we show in Theorem 1 that there is
a constant N that is independent of the unknown parameter so
that [1] holds whenever Ep(X n
k ) â‰¥N . Denote the interior of âˆ†
by int âˆ†.
Condition P: We say that a density Ï€ on âˆ†satisï¬es Condition
P(Î±), where Î± = (Î±1, . . . , Î±K ) âˆˆ(0, âˆ)K , if
Ï€(p)
QK
k=1 pÎ±k âˆ’1
k
is uniformly continuous and bounded away from zero on int âˆ†.
We say that Ï€ satisï¬es Condition P if there exists Î± âˆˆ(0, âˆ)K so
that Ï€ satisï¬es Condition P(Î±).
For example, if K = 2, then Ï€ satisï¬es Condition P(Î±) if and
only if Ï€ is positive and continuous on int âˆ†and the limit
limpk â†’0 Ï€(p)/pÎ±k âˆ’1
k
exists and is positive for k = 1, 2. For every
K â‰¥2, every Dirichlet distribution has a density that satisï¬es
Condition P. Note that Condition P does not require that the
density is bounded away from zero and inï¬nity at the boundary.
The present assumption on the behavior at the boundary is simi-
lar to Assumption P of ref. 5.
Theorem 1. Suppose Ï€ satisï¬es Condition P. Then for every Ïµ > 0,
there exists N âˆˆN so that
Pp(|Ë†pk(X n) âˆ’pk| â‰¥pkÏµ) â‰¤Ïµ
[2]
if npk â‰¥N .
The proofs of the results in this section are given in SI
Appendix.
The proof of Theorem 1 uses bounds on the posterior means
given in Proposition 1 below. These bounds imply that there is an
N âˆˆN so that if npk â‰¥N and the maximum-likelihood estimator
1
n X n
k is close to pk, then |Ë†pk(X n) âˆ’pk| < pkÏµ. It follows from
Chernoffâ€™s inequality that the probability that 1
n X n
k is not close
to pk is at most Ïµ.
Inequality 2 shows a higher accuracy of the Bayes estima-
tor Ë†pk(X n) when the true parameter pk approaches 0. To
explain this fact in a special case suppose that K = 2 and the
prior is the uniform distribution. Then Ë†pk(X n) = (X n
k + 1)/(n +
2) and the mean squared error of Ë†pk(X n) is [npk(1 âˆ’pk) +
(1 âˆ’2pk)2]/(n + 2)2, which converges to 0 like
1
n when pk âˆˆ
(0, 1) is ï¬xed and like
1
n2 when pk = 1
n . Moreover, by Markovâ€™s
inequality, the probability in [2] is less than (npk + 1)/(n2p2
k Ïµ2),
so that in this case we can choose N = 2/Ïµ3. In general, we do not
have an explicit expression for the threshold N , but in Remark 2
we discuss the properties of the prior that have an impact on the
N we construct in the proof.
Condition P allows the prior density to converge to zero at
the boundary of âˆ†like a power function with an arbitrarily large
exponent. The following example shows that the conclusion of
Theorem 1 fails to hold for a prior density that converges to 0
exponentially fast.
Example 1: Let K = 2, Ï€(p) âˆeâˆ’1/p1, and Î´ > 0. Then for every
N âˆˆN, there exist p âˆˆâˆ†and n âˆˆN with n
1
2 +Î´p1 â‰¥N so that
Pp(|Ë†p1(X n) âˆ’p1|>p1) = 1.
The idea behind this example is that the prior assigns very
little mass near the boundary point where p1 = 0, so if the true
parameter p1 is small, the observer needs a tremendous amount
of data to be convinced that p1 is in fact small. The prior den-
sity in our example converges to 0 at an exponential rate as
p1 â†’0, and it turns out that the amount of data needed so that
Ë†p1(X n)/p1 is close to 1 grows quadratically in 1/p1. For every
ï¬xed N âˆˆN and Î´ > 0, the pairs (n, p1) satisfying the relation
n
1
2 +Î´p1 = N involve a subquadratic growth rate of n with respect
to 1/p1. So we can always pick a small enough p1 such that the
corresponding data size n is insufï¬cient.
The next example shows that the sample size condition of
Theorem 1, npk â‰¥N , cannot be replaced by a weaker con-
dition of the form Î¶(n)pk â‰¥N
for some function Î¶ with
lim supnâ†’âˆÎ¶(n)/n = âˆ. Put differently, the set of p for which
[2] can be proved cannot be enlarged to a set of the form {p :
pk â‰¥Ï†Ïµ(n)} with Ï†Ïµ(n) = o(1/n).
Example 2: Suppose Ï€ satisï¬es Condition P. Let Î¶ : N â†’(0, âˆ)
be so that lim supnâ†’âˆÎ¶(n)/n = âˆ. Then for every N âˆˆN, there
exist p âˆˆâˆ†and n âˆˆN with Î¶(n)p1 â‰¥N so that
Pp(|Ë†p1(X n) âˆ’p1| > p1) = 1.
The following proposition gives fairly sharp bounds on the pos-
terior means under the assumption that the prior density satisï¬es
Condition P. The result is purely deterministic and applies to all
possible sample sequences. The bounds are of interest in their
own right and also play a crucial role in the proofs of Theorems 1
and 2.
Proposition 1. Suppose Ï€ satisï¬es Condition P(Î±). Then for every
Ïµ > 0, there exists a constant Î³ > 0 such that
(1 âˆ’Ïµ)nk + Î±k
n + Î³ â‰¤
R
pk
QK
i=1 pni
i

Ï€(p) dÎ»(p)
R QK
i=1 pni
i

Ï€(p) dÎ»(p)
â‰¤(1 + Ïµ)nk + Î³
n + Î³
[3]
for k = 1, . . . , K and all n, n1, . . . , nK âˆˆN0 with PK
i=1 ni = n.
Remark 1: If Ï€ is the density of a Dirichlet distribution with
parameter Î± âˆˆ(0, âˆ)K , then the inequalities in [3] hold with
Ïµ = 0 and Î³ = PK
k=1 Î±k, and the inequality on the left-hand side
is an equality. If Ï€ is the density of a mixture of Dirichlet distribu-
tions and the support of the mixing distribution is included in the
interval [a, A]K , 0 â‰¤a â‰¤A < âˆ, then for all k and n1, . . . , nK
with PK
i=1 ni = n,
nk + a
n + KA â‰¤
R
pk
QK
i=1 pni
i

Ï€(p) dÎ»(p)
R QK
i=1 pni
i

Ï€(p) dÎ»(p)
â‰¤nk + A
n + Ka .
[4]
The proofs of our main results, Theorems 1 and 2, apply to all
priors whose densities satisfy inequalities 3 or 4. In particular,
the conclusions of these theorems and of their corollaries hold if
the prior distribution is a mixture of Dirichlet distributions and
the support of the mixing distribution is bounded.
Remark 2: Condition P(Î±) implies that the function Ï€(p)/
QK
k=1 pÎ±k âˆ’1
k
, p âˆˆintâˆ†, can be extended to a continuous function
ËœÏ€(p) on âˆ†. The proof of Proposition 1 relies on the fact that ËœÏ€ can
be uniformly approximated by Bernstein polynomials. An inspec-
tion of the proof shows that the constant Î³ in [3] can be taken to
4926
|
www.pnas.org/cgi/doi/10.1073/pnas.1618780114
Fudenberg et al.

ECONOMIC
SCIENCES
STATISTICS
be m + PK
k=1 Î±k, where m is so large that hm, the mth-degree
Bernstein polynomial of ËœÏ€, satisï¬es
max{|hm(p) âˆ’ËœÏ€(p)| : p âˆˆâˆ†} â‰¤min{ËœÏ€(p) : p âˆˆâˆ†}
1 + 2Ïµâˆ’1
.
Hence, in addition to a small value of Ïµ, the following properties
of the density Ï€ result in a large value of Î³: (i) if PK
k=1 Î±k is large,
(ii) if Ï€ is a â€œroughâ€ function so that ËœÏ€ is hard to approximate and
m needs to be large, and (iii) if ËœÏ€ is close to 0 somewhere. The
threshold N in Theorem 1 depends on the prior through the con-
stant Î³ from Proposition 1 and the properties of Ï€ just described
will also lead to a large value of N .
In particular, N â†’âˆif PK
k=1 Î±k â†’âˆ. For example, consider
a sequence of priors Ï€(j) for K = 2, where Ï€(j) is the density of
the Dirichlet distribution with parameter (j, 1), so that Ï€(j) satis-
ï¬es Condition P(Î±) with Î±1 = j. As j â†’âˆ, Ï€(j) converges faster
and faster to 0 as p1 â†’0, although never as fast as in Example
1, where no ï¬nite N can satisfy the conclusion of Theorem 1. If
n = 4j and p1 = 1
12, then under Ï€(j), Ë†p1(X n) = (X n
1 + j)/(n +
j + 1) â‰¥2p1, so for every Ïµ âˆˆ(0, 1), the probability in Theorem
1 is 1. Thus, the smallest N for which the conclusion holds must
exceed 4j Ã— 1
12 = j
3.
Remark 3: Using results on the degree of approximation by
Bernstein polynomials, one may compute explicit values for the
constants Î³ in Proposition 1 and N in Theorem 1. Details are
given in in SI Appendix, Remarks 3â€² and 3â€²â€².
Remark 4: Suppose K > 2 and the statistician is interested in
only one of the probabilities pk, say pÂ¯k. Then, instead of using
Ë†pÂ¯k(X n), he may ï¬rst reduce the original (K âˆ’1)-dimensional
estimation problem to the problem of estimating the one-
dimensional parameter (pÂ¯k, P
kÌ¸=Â¯k pk) of the Dirichlet distribu-
tion of (X n
Â¯k , P
kÌ¸=Â¯k X n
k ). He will then distinguish only whether
or not the die lands on side Â¯k and will use the induced one-
dimensional prior distribution for the parameter of interest.
If the original prior is a Dirichlet distribution on âˆ†, both
approaches lead to the same Bayes estimators for pÂ¯k, but in gen-
eral, they do not. SI Appendix, Proposition 2 shows that whenever
the original density Ï€ satisï¬es Condition P, then the induced den-
sity satisï¬es Condition P as well. However, it may happen that the
induced density satisï¬es Condition P even though the original
density does not. For example, if K = 3 and Ï€(p) âˆeâˆ’1/p1 + p2,
then Ï€ does not satisfy Condition P, but for each Â¯k = 1, 2, 3, the
induced density does.
Comparison of Two Multinomial Distributions
Here we consider two dice, blue and red, each with K â‰¥2 sides.
In every period, a die is chosen. We ï¬rst consider the case where
the choice is deterministic and ï¬xed in advance. We later allow
the choice to be random. The chosen die is tossed and lands
on the kth side according to the unknown probability distribu-
tions p = (p1, . . . , pK ) and q = (q1, . . . , qK ) for the blue and the
red die, respectively. The outcome of the toss is independent of
past outcomes. The parameter space of the problem is âˆ†2. The
observerâ€™s prior is represented by a product density Ï€(p)Ï±(q)
over âˆ†2; that is, he regards the parameters p and q as realiza-
tions of independent random vectors.
Let X n be a random vector that describes the outcomes, i.e.,
colors and sides, of the ï¬rst n tosses. Let bn denote the number
of times the blue die is tossed in the ï¬rst n periods. Let Ï€(Â·|X n)
and Ï±(Â·|X n) be the posterior densities for the blue and the red
die after observing X n. Let Ë†pk(X n) =
R
pkÏ€(p|X n)dÎ»(p) and
Ë†qk(X n) =
R
qkÏ±(q|X n)dÎ»(q). The product form of the prior
density ensures that the marginal posterior distribution for either
die is completely determined by the observations on that die and
the marginal prior for that die.
We study the following problem. Fix a side Â¯k âˆˆ{1, . . . , K}
and a constant c âˆˆ(0, âˆ). Consider a family of environments,
each characterized by a data-generating parameter vector Ï‘ =
(p, q) âˆˆâˆ†2 and an observation length n. In each environment,
we have pÂ¯k â‰¥cqÂ¯k, and we are interested in whether the Bayes
estimators reï¬‚ect this inequality. In general, one cannot expect
that the probability that Ë†pÂ¯k(X n) â‰¥cË†qÂ¯k(X n) is much higher than
1
2 when pÂ¯k = cqÂ¯k. We therefore ask whether in all of the environ-
ments, the observer has a high probability that Ë†pÂ¯k(X n) â‰¥c(1 âˆ’
Î´)Ë†qÂ¯k(X n) for a given constant Î´ âˆˆ(0, 1).
Clearly, as pÂ¯k approaches 0, we will need a larger observation
length n for the data to overwhelm the prior. But how fast must
n grow relative to pÂ¯k? Applying the uniform consistency result of
ref. 4 to each Bayes estimator separately leads to the condition
that n must be so large that the expected number of times the
blue die lands on side Â¯k, that is, bnpÂ¯k, exceeds a threshold that
explodes when pÂ¯k approaches zero. The following theorem shows
that there is a threshold that is independent of p, provided the
prior densities satisfy Condition P.
Theorem 2. Suppose that Ï€ and Ï± satisfy Condition P. Let Â¯k âˆˆ
{1, . . . , K}, c âˆˆ(0, âˆ), and Ïµ, Î´, Î· âˆˆ(0, 1). Then there exists N âˆˆN
so that for every deterministic sequence of choices of the dice to be
tossed,
PÏ‘(Ë†pÂ¯k(X n) â‰¥c(1 âˆ’Î´)Ë†qÂ¯k(X n)) â‰¥1 âˆ’Ïµ
[5]
for all Ï‘ = (p, q) âˆˆâˆ†2 with pÂ¯k â‰¥cqÂ¯k and all n âˆˆN with bnpÂ¯k â‰¥
N and bn/n â‰¤1 âˆ’Î·.
We prove Theorem 2 in the next section.
Note that the only constraints on the sample size here are that
the product of bn with pÂ¯k be sufï¬ciently large and the propor-
tion of periods in which the red die is chosen be not too small.
However, pÂ¯k and qÂ¯k can be arbitrarily small. This is useful in ana-
lyzing situations where the data-generating process contains rare
events.
In the language of hypothesis testing, Theorem 2 says that
under the stated condition on the prior, the test that rejects the
null hypothesis pÂ¯k â‰¥cqÂ¯k if and only if Ë†pÂ¯k(X n) < c(1 âˆ’Î´)Ë†qÂ¯k(X n)
has a type I error probability of at most Ïµ provided pÂ¯k â‰¥N /bn
(and bn/n < 1 âˆ’Î·). For every n, the bound on the error proba-
bility holds uniformly on the speciï¬ed parameter set. Note that
such a bound cannot be obtained for a test that rejects the
hypothesis whenever Ë†pÂ¯k(X n) < cË†qÂ¯k(X n).
We now turn to the case where the dice are randomly chosen.
The probability of choosing the blue die need not be constant
over time but must not depend on the unknown parameter Ï‘.
Let the random variable Bn denote the number of times the blue
die is tossed in the ï¬rst n periods.
Corollary 1. Suppose that Ï€ and Ï± satisfy Condition P. Let Â¯k âˆˆ
{1, . . . , K}, c âˆˆ(0, âˆ), and Ïµ, Î´ âˆˆ(0, 1). Suppose that in every
period, the die to be tossed is chosen at random, independent of
the past, and that
lim inf
nâ†’âˆ
E(Bn)
n
> 0,
lim sup
nâ†’âˆ
E(Bn)
n
< 1.
[6]
Then there exists N âˆˆN so that
PÏ‘(Ë†pÂ¯k(X n) â‰¥c(1 âˆ’Î´)Ë†qÂ¯k(X n)) â‰¥1 âˆ’Ïµ
[7]
for all Ï‘ = (p, q) âˆˆâˆ†2 with pÂ¯k â‰¥cqÂ¯k and all n âˆˆN with npÂ¯k â‰¥N .
The proof of Corollary 1 is given at the end of the next section.
In the decision problem described at the beginning, Theorem 2
and Corollary 1 ensure that whenever surgery is the safer option,
the probability that the physician actually chooses surgery is at
least 1 âˆ’Ïµ unless the probability of complication due to the drug
is smaller than N /n. Except for this last condition, the bound
1 âˆ’Ïµ holds uniformly over all possible parameters.
Fudenberg et al.
PNAS
|
May 9, 2017
|
vol. 114
|
no. 19
|
4927

In the rest of this section we assume that in every period the
blue die is chosen at random with the same probability ÂµB. The
value of ÂµB need not be known; we assume only that 0 < ÂµB < 1,
so that condition 6 is met.
The following example shows that the conditions on the prior
densities cannot be omitted from Corollary 1.
Example 3: Suppose K = 2 and 0 < ÂµB < 1. Suppose Ï€ satisï¬es
Condition P and Ï±(q) âˆeâˆ’1/q1. Let c > 0. Then for every N âˆˆN,
there exist Ï‘ = (p, q) âˆˆâˆ†2 with p1 â‰¥cq1 and n âˆˆN with np1 â‰¥N
so that
PÏ‘

Ë†p1(X n)< c
2 Ë†q1(X n)

>1
2.
The next example shows that the sample size condition of
Corollary 1, npÂ¯k â‰¥N , is the best possible for small pÂ¯k. It can-
not be replaced by a weaker condition of the form nÎ¶(pÂ¯k) â‰¥N
for some function Î¶ with limtâ†’0+ Î¶(t)/t = âˆ. In particular, tak-
ing Î¶ to be a constant function shows that there does not exist
N âˆˆN so that n â‰¥N implies that [7] holds uniformly for all Ï‘
with pk â‰¥cqk.
Example 4: Suppose that 0 < ÂµB < 1 and that Ï€ and Ï± satisfy Con-
dition P. Let c > 0. Let Î¶ be a nonnegative function on [0, 1] with
limtâ†’0+ Î¶(t)/t = âˆ. Then there exists Ïµ0 > 0 so that for every
N âˆˆN, there exist Ï‘ = (p, q) âˆˆâˆ†2 with p1 â‰¥cq1 and n âˆˆN with
nÎ¶(p1) â‰¥N so that
PÏ‘

Ë†p1(X n)<c
2 Ë†q1(X n)

> Ïµ0.
Examples 3 and 4 are proved in SI Appendix.
Suppose that after data X n the observer was told that the next
outcome was Â¯k but not which die was used. Then Bayesâ€™ rule
implies the posterior odds ratio for â€œblueâ€ relative to â€œredâ€ is
ÂµB
R
pÂ¯kÏ€(p|X n) dÎ»(p)
ÂµR
R
qÂ¯kÏ±(q|X n) dÎ»(q) ,
where ÂµR = 1 âˆ’ÂµB.
Corollary 2. Suppose that Ï€ and Ï± satisfy Condition P. Then there
exists N âˆˆN such that whenever pÂ¯k â‰¥qÂ¯k and npÂ¯k â‰¥N , there is
probability at least 1 âˆ’Ïµ that the posterior odds ratio of blue rel-
ative to red exceeds (1 âˆ’Ïµ) Â· ÂµB
ÂµR when the (n + 1)th die lands on
side Â¯k.
Corollary 2 is used by Fudenberg and He in ref. 3, who pro-
vide a learning-based foundation for equilibrium reï¬nements in
signaling games. They consider a sequence of learning environ-
ments, each containing populations of blue senders, red senders,
and receivers. Senders are randomly matched with receivers
each period and communicate using one of K messages. There
is some special message Â¯k, whose probability of being sent by
blue senders always exceeds the probability of being sent by red
senders in each environment. Suppose the common prior of the
receivers satisï¬es Condition P and, in every environment, there
are enough periods that the expected total observations of blue
sender playing Â¯k exceed a constant. Then at the end of every
environment, by Corollary 2 all but Ïµ fraction of the receivers
will assign a posterior odds ratio for the color of the sender not
much less than the prior odds ratio of red vs. blue, if they were
to observe another instance of Â¯k sent by an unknown sender,
regardless of how rarely the message Â¯k is observed. A leading
case of receiver prior satisfying Condition P is ï¬ctitious play, the
most commonly used model of learning in games, which corre-
sponds to Bayesian updating from a Dirichlet prior, but Corol-
lary 2 shows that the Dirichlet restriction can be substantially
relaxed.
Proofs of Theorem 2 and Corollary 1
We begin with two auxiliary results needed in the proof of Theo-
rem 2. Lemma 1 is a large deviation estimate that gives a bound
on the probability that the frequency of side Â¯k in the tosses of
the red die exceeds an afï¬ne function of the frequency of side Â¯k
in the tosses of the blue die. Lemma 2 implies that, with proba-
bility close to 1, the number of times the blue die lands on side
Â¯k exceeds a given number when bnpÂ¯k is sufï¬ciently large. The
proofs of Lemmas 1 and 2 are in SI Appendix.
Lemma 1. Let Sn be a binomial random variable with parameters
n and p, and let Tm be a binomial random variable with param-
eters m and q. Let 0 < câ€² < c and d > 0. Suppose Sn and Tm are
independent, and p â‰¥cq. Then
P
Tm
m â‰¥1
câ€²
Sn
n +
d
n âˆ§m

â‰¤
câ€²
c
câ€²d/(câ€²+1)
.
Lemma 2. Let M < âˆand Ïµ > 0. Then there exists N âˆˆN so that
if Sn is a binomial random variable with parameters n and p and
np â‰¥N , then
Pp(Sn â‰¤M ) â‰¤Ïµ.
Proof of Theorem 2: Let rn = n âˆ’bn be the number of times the
red die is tossed in the ï¬rst n periods. Let Yn and Zn be the
respective number of times the blue and the red die land on side
Â¯k. Choose Î² > 0 and câ€² âˆˆ(0, c) so that
1 âˆ’Î²
(1 + Î²)(1 âˆ’Î´)> c
câ€² + Î´.
[8]
By Proposition 1, there exists Î³ > 0 so that for every n âˆˆN,
Ë†pÂ¯k(X n) â‰¥Ï†(bn, Yn),
Ë†qÂ¯k(X n) â‰¤Ïˆ(rn, Zn),
[9]
where
Ï†(b, y) = (1 âˆ’Î²)
y
b + Î³ ,
Ïˆ(r, z) = (1 + Î²)z + Î³
r
.
Let d > 0 be
so that
the bound
in Lemma
1 satisï¬es
(câ€²/c)câ€²d/(câ€²+1) â‰¤Ïµ
2.
We now show that for all b, r âˆˆN, y = 0, . . . , b, and z =
0, . . . , r, the inequalities
z
r < 1
câ€²
y
b +
d
b âˆ§r ,
2cÎ³
câ€²Î´ <b<r
Î· ,
y >M := 3c(d + Î³)
Î´Î·
[10]
imply that
Ï†(b, y)>c(1 âˆ’Î´)Ïˆ(r, z).
[11]
It follows from the ï¬rst and the third inequality in [10] that
Ïˆ(r, z) < Ïˆ

r, ry
câ€²b +
rd
b âˆ§r

= (1 + Î²)
 y
câ€²b +
d
b âˆ§r + Î³
r

â‰¤(1 + Î²)
 y
câ€²b + Î´M
3bc

.
Applying this result, inequality 8, twice the second, and ï¬nally
the fourth inequality in [10] we get
Ï†(b, y) âˆ’c(1 âˆ’Î´)Ïˆ(r, z)
(1 âˆ’Î´)(1 + Î²)
>
y
b + Î³

1 âˆ’Î²
(1 âˆ’Î´)(1 + Î²) âˆ’c
câ€² âˆ’cÎ³
câ€²b

âˆ’Î´M
3b
â‰¥
y
b + Î³

Î´ âˆ’Î´
2

âˆ’Î´M
3b
â‰¥2
3b
Î´
2M âˆ’Î´M
3b = 0,
proving [11].
4928
|
www.pnas.org/cgi/doi/10.1073/pnas.1618780114
Fudenberg et al.

ECONOMIC
SCIENCES
STATISTICS
Let N = {n âˆˆN : bn/n â‰¤1 âˆ’Î·}, N1 = âŒˆ2cÎ³/(câ€²Î´)âŒ‰, and for
every n âˆˆN with bn â‰¥N1 deï¬ne the events
Fn =
Zn
rn < 1
câ€²
Yn
bn +
d
bn âˆ§rn

,
Gn = {Yn>M }.
For all n âˆˆN, bn < rn/Î·. Thus, if n âˆˆN and bn â‰¥N1, the impli-
cation [10] â‡’[11] yields that
Fn âˆ©Gn âŠ‚{Ï†(bn, Yn) > c(1 âˆ’Î´)Ïˆ(rn, Zn)}.
Therefore, by inequalities 9,
Fn âˆ©Gn âŠ‚{Ë†pÂ¯k(X n) â‰¥c(1 âˆ’Î´)Ë†qÂ¯k(X n)} .
It follows from Lemma 1 and the deï¬nition of d that for every
Ï‘ = (p, q) with pÂ¯k â‰¥cqÂ¯k, PÏ‘(F c
n) â‰¤Ïµ
2. By Lemma 2, there exists
N2 âˆˆN so that PÏ‘(Gc
n) â‰¤Ïµ
2 for all n with bnpÂ¯k â‰¥N2. Thus, if
pÂ¯k â‰¥cqÂ¯k, n âˆˆN, and bnpÂ¯k â‰¥N := max(N1, N2), then
PÏ‘(Ë†pÂ¯k(X n) â‰¥c(1 âˆ’Î´)Ë†qÂ¯k(X n)) â‰¥1 âˆ’PÏ‘(F c
n) âˆ’PÏ‘(Gc
n) â‰¥1 âˆ’Ïµ.
Note that N does not depend on the sequence of the choices of
the dice. â–¡
Remark 5: If K = 2, then for every n â‰¥1 and every ï¬xed number
of times the red die is chosen in the ï¬rst n periods, the Bayes
estimate of qÂ¯k can be shown to be an increasing function of the
number of times the red die lands on side Â¯k. This fact can be
combined with Theorem 1 to give an alternative proof of Theorem
2 for the case K = 2. The monotonicity result does not hold for
K > 2 and our Proof of Theorem 2 does not use Theorem 1.
Proof of Corollary 1: By Chebyshevâ€™s inequality, [Bn âˆ’E(Bn)]/n
converges in probability to 0. Thus, by condition 6, there exists
Î· > 0 and N1 âˆˆN so that the event Fn = {Î· â‰¤Bn/n â‰¤1 âˆ’Î·} has
probability P(Fn) â‰¥1 âˆ’Ïµ
2 for all n â‰¥N1. By Theorem 2, there
exists N2 âˆˆN so that
PÏ‘(Ë†pÂ¯k(X n) â‰¥c(1 âˆ’Î´)Ë†qÂ¯k(X n)|Bn = bn) â‰¥1 âˆ’Ïµ
2
for all Ï‘ = (p, q) âˆˆâˆ†2 with pÂ¯k â‰¥cqÂ¯k and all n âˆˆN and bn âˆˆ
{1, . . . , n} with P(Bn = bn) > 0 and bnpÂ¯k â‰¥N2 and bn/n â‰¤
1 âˆ’Î·. Let N = max(N1, âŒˆN2/Î·âŒ‰). Then for every Ï‘ with pÂ¯k â‰¥cqÂ¯k
and every n âˆˆN with npÂ¯k â‰¥N , Fn âŠ‚{BnpÂ¯k â‰¥N2, Bn/n â‰¤
1 âˆ’Î·}, so that
PÏ‘(Ë†pÂ¯k(X n) â‰¥c(1 âˆ’Î´)Ë†qÂ¯k(X n)|Fn) â‰¥1 âˆ’Ïµ
2,
which implies [7] because P(Fn) â‰¥1 âˆ’Ïµ
2. â–¡
ACKNOWLEDGMENTS. We thank three referees for many useful sugges-
tions. We thank Gary Chamberlain, Martin Cripps, Ignacio Esponda, and
Muhamet Yildiz for helpful conversations. This research is supported by
National Science Foundation Grant SES 1558205.
1. US Food and Drug Administration (2000) Guidance for the Use of Bayesian Statistics
in Medical Device Clinical Trials (FDA, Rockville, MD), Tech Rep 2006Dâ€“0191.
2. ThompsonLA(2014)Bayesian Methods for Making Inferences about Rare Diseases in
Pediatric Populations. Presentation at the Food and Drug Administration (FDA,
Rockville,MD).
3. Fudenberg D, He K (2017) Type-compatible equilibria in signalling games. arXiv:
1702.01819.
4. Diaconis P, Freedman D (1990) On the uniform consistency of Bayes estimates
for multinomial probabilities. Ann Stat 18:1317â€“1327.
5. Bochkina NA, Green PJ (2014) The Bernsteinâ€“von Mises theorem and nonregular
models. Ann Stat 42:1850â€“1878.
6. Dudley R, Haughton D (2002) Asymptotic normality with small relative errors of
posterior probabilities of half-spaces. Ann Stat 30:1311â€“1344.
Fudenberg et al.
PNAS
|
May 9, 2017
|
vol. 114
|
no. 19
|
4929

