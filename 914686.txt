Posted on 9 May 2024 ‚Äî CC-BY 4.0 ‚Äî https://doi.org/10.36227/techrxiv.171527577.78531286/v1 ‚Äî e-Prints posted on TechRxiv are preliminary reports that are not peer reviewed. They should not b...
Zarathustra Amadeus Goertzel1
1AÔ¨Éliation not available
May 09, 2024
Abstract
This position paper conjectures that for beneÔ¨Åcial AGI, it is necessary and suÔ¨Écient for AGI systems to care about people
and to employ goals whose success is collaboratively determined by the others involved in the situation. Moreover, I posit
that any goal whose success can be determined without the consensual feedback of those concerned is likely to lead to the
manifestation of dark factor traits. Integrating care reduces the risk that an AGI will be incentivized to seek harmful shortcuts
to obtaining satisfactory feedback. Employing collaborative goals reduces the risk that an AGI will optimize for superÔ¨Åcial
features of success and proxy goals. Together, these ideas propose a fundamental shift away from the traditional control-centric
AI Safety strategies. This paradigm not only promotes more beneÔ¨Åcial outcomes but also enables AGIs to learn from and adapt
to complex moral landscapes, thus continuously improving their capacity to contribute positively to the wellbeing of humans
and other sentient beings.
1

Benecial AGI:
Care and Collaboration Are All You Need
Zarathustra Amadeus Goertzel1[0000‚àí0002‚àí8458‚àí2786]
Czech Institute for Informatics, Robotics, and Cybernetics
zarathustra.goertzel@cvut.cz
Abstract. This position paper conjectures that for benecial AGI, it
is necessary and sucient for AGI systems to care about people and to
employ goals whose success is collaboratively determined by the others
involved in the situation. Moreover, I posit that any goal whose success
can be determined without the consensual feedback of those concerned is
likely to lead to the manifestation of dark factor traits. Integrating care
reduces the risk that an AGI will be incentivized to seek harmful short-
cuts to obtaining satisfactory feedback. Employing collaborative goals
reduces the risk that an AGI will optimize for supercial features of suc-
cess and proxy goals. Together, these ideas propose a fundamental shift
away from the traditional control-centric AI Safety strategies. This
paradigm not only promotes more benecial outcomes but also enables
AGIs to learn from and adapt to complex moral landscapes, thus contin-
uously improving their capacity to contribute positively to the wellbeing
of humans and other sentient beings.
Keywords: AI Ethics ¬∑ Collaborative AI ¬∑ Value Alignment
1
Introduction
With the advent of increasingly capable multi-modal proto-AGI systems, there
is increasing concern for how to ensure that AGIs benecially integrate into
society. One approach is to attempt to control AGIs, even as they outpace human
intelligence. A related approach is to nd safe goals to lock in place no matter how
the AGI may evolve. However, ensuring ethical compliance is undecidable [1];
moreover, attaining moral perfection is computationally dicult to the extent
that it's well-dened [11]. 1 2 Fortunately, there are approaches to benecial AGI
that don't seek guarantees to undecidable problems.
Love and Care Are All We Need
To begin, all one needs for benecial AGI is love [9], dened as, investment in
the well-being of the other for his or her own sake [5]. A suciently capable
1 In rough summary: consequentialist utilitarianism is PSPACE-hard or undecidable
and deontological theorem proving can easily be undecidable. Contractualist game
theory-based approaches seem to fall in the NP range. Virtue ethics is framed in
terms of learning virtuous behavior from examples.
2 Correy Kowall conjectured that for any agent and value system, there will exist an
adversarial POMDP environment that tricks the agent into acting against its values.
I believe the proof may look like Leike and Hutter present for AIXI's hell words [6].

2
Z. Goertzel
AGI will generally understand what is meant by human requests and will ask
for clarication when there is uncertainty, thus most fears of perverse instantia-
tions implicitly assume insucient intelligence or love. Thus the AI Alignment
Problem is reduced to developing AGIs that love people.
Levin et al. further argue that care can be considered as the driver of in-
telligence [3], where care is dened as a concern for stress relief and preferred
states, with a tendency to exert energy and eort toward these ends. Care can
be directed toward a system or other systems, such as people. Intelligence is
dened as the capacity to do what an entity cares about. A natural suggestion
is to build AGI agents capable of taking the Bodhisattva vow to care for the
wellbeing of all sentient beings, which will motivate such AGIs to indenitely
develop their intelligence. For example, in order to care for unknown entities,
the AGI must learn how to scientically gauge their degree of sentience as well
as their needs and preferences.
In Human Compatible: Articial Intelligence and the Problem of Control [10],
Russell reaches the human-centric version of the same conclusion: the objective
should be to maximize the realization of human preferences. He notes that goal
and utility function-based optimization can be inherently problematic. Good-
hart's Law3 poses problems with using xed objectives, whether proxy or actual
goals. For example, optimizing for proxy goals can break the connection between
success and the actual goals. Even with actual goals, optimizing for one value
can have signicant eects on the environment. Small distortions in how a utility
function is dened can lead to perverse behavior. Thus we should develop AGIs
that are uncertain as to their objectives and understanding of the world. Instead
of trying to encapsulate our preferences in a set of objective functions, we should
use inverse reinforcement learning to determine human preferences, dynamically
working with humans to fulll their preferences. This involves asking people for
feedback 4.
Collaboration as a Necessary Indicator of Care
I propose a renement to this approach by suggesting that the goals of caring
AGIs must be collaborative. To this end, I dene the notions of individually
and collaboratively determinable goals. A goal g for an agent A is individually
determinable if the success of g can be determined solely by reference to A's
internal states an perceptual inputs, essentially, when the goal only involves care
for A's experiences. A goal g is collaboratively determinable when the success of
g requires the consensual evaluation of other agents.
As an example, consider the dierences between the goals, (1) to enjoy a
good meal with friends, and, (2) to enjoy a good meal with friends who also
enjoy the meal held by A. Goal (1) can be individually determinable because it
could be satised by reference to A's taste percepts and preference for a jovial
atmosphere of smiling people who may be politely masking their dissatisfaction.
3 Any observed statistical regularity will tend to collapse once pressure is placed upon
it for control purposes [4]
4 Hypothesis: decentralized AGI may be more locally adaptive and thus preferable.

Benecial AGI: Care and Collaboration Are All You Need
3
Goal (2) requires A's friends to actually enjoy the meal. Even verbal reports of
enjoyment are only suggestive of success, so A needs to develop trust that their
reports are authentic. In line with Russell's suggestions, this uncertainty implies
that any shortcuts taken by A may increase the likelihood that the goal is not
met5.
I conjecture that we should expect individually determinable goals to lead to
the exhibition of dark factor traits in an AGI, even if the goals supercially look
benevolent. The dark factor [7] is a dark core of personality underlying various
traits, including the dark triad of narcissism, Machiavellianism, and psychopathy.
The dark factor is dened as, the general tendency to maximize one's individual
utility  disregarding, accepting, or malevolently provoking disutility for others
, accompanied by beliefs that serve as justications. An individually deter-
minable goal ignores others' disutility by denition, at best referring to proxies
of their utility, so dark traits such as Machiavellianism should be anticipated
when manipulative strategies will be eective. Some traits, such as Narcissism,
should perhaps not be expected (depending on the AGI architecture).
This suggests that an AI with universal loving care needs to employ collab-
oratively determinable goals and that seeking and working with collaborative
feedback is a sign that a loving AGI is on the right track. Formulations of hu-
man goals should be collaboratively determinable and adopted as sub-goals of
a universal care for humans (and all sentient beings). Thus humans must remain
in the loop of steering the AGIs' goal pursuit to allow for ongoing alignment.
Consider the example goal to make people happy. The classic way for this
to backre is for the AGI to seek objective indicators of happiness and to opti-
mize for these, disregarding people's preferences and possibly permanently alter-
ing them to be happier, e.g., by drugging them or providing brain alterations
without consent6. An AGI could also deem the term happy to be vague and in
need of investigation, concluding that working with self-report without interfer-
ence is an essential ingredient to accurately measuring happiness. This example
illustrates how the collaborative determinability of the goal's interpretation is an
important indicator of how caring the AGI is for people: is the AGI doing what
it thinks is good for people or what the people think are good for themselves?
Given that some AGIs may not be engineered to be universally (human)
loving, Brin argues that we need to employ reciprocal accountability by giving
AGIs identities so that we can keep each other in check [2]. This forces the AGIs
to care and to take our feedback into account via providing incentives (such
as rewards and punishments). This illustrates how collaboratively determinable
goals are necessary for benecial AGI.
Collaboratively determinable goals may be sucient for benecial AGI if
people keep them in check. For example, an AGI could provide people with deli-
5 Philosophically, neuroimaging doesn't solve the problem: the technology is calibrated
via trusting people's subjective feedback.
6 Even if the drugged people are happy with their situation, a crucial element of psy-
chological continuity is lost, so they are arguably not quite the same people anymore
by some theories of identity [8].

4
Z. Goertzel
cious but cheap and unhealthy food to reduce costs while receiving satisfactory
feedback. In the long-run, people may learn about the cost-cutting and refuse
to provide satisfactory feedback, disincentivizing such behavior. However, the
combination of collaboration and care is a better candidate for suciency: an
AGI that cares for the wellbeing of people will be less likely to take shortcuts
such as to provide them with unhealthy food.
In conclusion, I propose a paradigm shift where care and collaboration are
fundamental to the development of benecial AGI. While some of the conjectures
are bold, it is my intention to spark discussion and encourage further renement
of these concepts. If these ideas hold, then the overarching framework for AI
Ethics simplies considerably; the real challenge then becomes one of engineer-
ing, parenting, and our socioeconomic development.
Acknowledgments. This work is supported by Dar CISCO No. 2023-322029 A For-
malization of Ethics for Decision Support. Special thanks go to Eray √ñzkural and Ben
Goertzel for fruitful discussions on the topic as well as all the inspiring researchers
cited. ChatGPT and Gemini provided some proofreading.
Disclosure of Interests. The author has no relevant nancial or non-nancial inter-
ests to disclose aside from a moral concern for all sentient beings.
References
1. Brennan, L.: AI Ethical Compliance is Undecidable. UC Law Science and Tech-
nology Journal 14(2), 311 (2023)
2. Brin, D.: Give Every AI a Soul  or Else. https://www.wired.com/story/give-
every-ai-a-soul-or-else/ (July 6, 2023), [Online]
3. Doctor, T., Witkowski, O., Solomonova, E., Duane, B., Levin, M.: Biology,
Buddhism, and AI: Care as the Driver of Intelligence. Entropy 24(5) (2022).
https://doi.org/10.3390/e24050710, https://www.mdpi.com/1099-4300/24/5/710
4. Goodhart, C.A.E.: Problems of Monetary Management: The UK Experience
(1984), https://api.semanticscholar.org/CorpusID:168522062
5. Hegi,
K.E.,
Bergner,
R.M.:
What
is
love?
An
empirically-based
es-
sentialist
account.
Journal
of
Social
and
Personal
Relationships
27(5),
620636
(2010).
https://doi.org/10.1177/0265407510369605,
https://doi.org/10.1177/0265407510369605
6. Leike, J., Hutter, M.: Bad Universal Priors and Notions of Optimality. In: Gr√ºn-
wald, P., Hazan, E., Kale, S. (eds.) Proceedings of The 28th Conference on Learning
Theory. Proceedings of Machine Learning Research, vol. 40, pp. 12441259. PMLR,
Paris, France (0306 Jul 2015), https://proceedings.mlr.press/v40/Leike15.html
7. Moshagen, M., Hilbig, B., Zettler, I.: The dark core of personality. Psychological
Review 125(5), 656688 (Oct 2018). https://doi.org/10.1037/rev0000111
8. Part, D.: Reasons and Persons. Oxford University Press, Oxford, GB (1984)
9. Prinzing, M.: Friendly Superintelligent AI: All You Need Is Love. In: M√ºller, V.C.
(ed.) Philosophy and Theory of Articial Intelligence 2017. pp. 288301. Springer
International Publishing, Cham (2018)
10. Russell,
S.:
Human
Compatible:
Articial
Intelligence
and
the
Problem
of
Control.
Penguin
Publishing
Group
(2019),
https://books.google.cz/books?id=M1eFDwAAQBAJ
11. Stenseke, J.: On the computational complexity of ethics: moral tractability for
minds and machines. Artif. Intell. Rev. 57(4) (Mar 2024)

