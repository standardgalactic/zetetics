Variational Quantum State Diagonalization
Ryan LaRose,1, 2 Arkin Tikku,1, 3 Étude O’Neel-Judy,1 Lukasz Cincio,1 and Patrick J. Coles1
1Theoretical Division, Los Alamos National Laboratory, Los Alamos, NM 87545, USA.
2Department of Computational Mathematics, Science,
and Engineering & Department of Physics and Astronomy,
Michigan State University, East Lansing, MI 48823, USA.
3Department of Physics, Blackett Laboratory, Imperial College London,
Prince Consort Road, London SW7 2AZ, United Kingdom
Variational hybrid quantum-classical algorithms are promising candidates for near-term imple-
mentation on quantum computers. In these algorithms, a quantum computer evaluates the cost of
a gate sequence (with speedup over classical cost evaluation), and a classical computer uses this
information to adjust the parameters of the gate sequence. Here we present such an algorithm for
quantum state diagonalization. State diagonalization has applications in condensed matter physics
(e.g., entanglement spectroscopy) as well as in machine learning (e.g., principal component analy-
sis). For a quantum state ρ and gate sequence U, our cost function quantiﬁes how far UρU † is from
being diagonal. We introduce novel short-depth quantum circuits to quantify our cost. Minimizing
this cost returns a gate sequence that approximately diagonalizes ρ. One can then read out approx-
imations of the largest eigenvalues, and the associated eigenvectors, of ρ. As a proof-of-principle,
we implement our algorithm on Rigetti’s quantum computer to diagonalize one-qubit states and on
a simulator to ﬁnd the entanglement spectrum of the Heisenberg model ground state.
I.
INTRODUCTION
The future applications of quantum computers, assum-
ing that large-scale, fault-tolerant versions will eventually
be realized, are manifold. From a mathematical perspec-
tive, applications include number theory [1], linear alge-
bra [2–4], diﬀerential equations [5, 6], and optimization
[7].
From a physical perspective, applications include
electronic structure determination [8, 9] for molecules
and materials and real-time simulation of quantum dy-
namical processes [10] such as protein folding and photo-
excitation events. Naturally, some of these applications
are more long-term than others.
Factoring and solv-
ing linear systems of equations are typically viewed as
longer term applications due to their high resource re-
quirements. On the other hand, approximate optimiza-
tion and the determination of electronic structure may
be nearer term applications, and could even serve as
demonstrations of quantum supremacy in the near fu-
ture [11, 12].
A major aspect of quantum algorithms research is to
make applications of interest more near term by reducing
quantum resource requirements including qubit count,
circuit depth, numbers of gates, and numbers of mea-
surements. A powerful strategy for this purpose is al-
gorithm hybridization, where a fully quantum algorithm
is turned into a hybrid quantum-classical algorithm [13].
The beneﬁt of hybridization is two-fold, both reducing
the resources (hence allowing implementation on smaller
hardware) as well as increasing accuracy (by outsourcing
calculations to “error-free” classical computers).
Variational hybrid algorithms are a class of quantum-
classical algorithms that involve minimizing a cost func-
tion that depends on the parameters of a quantum gate
sequence. Cost evaluation occurs on the quantum com-
puter, with speedup over classical cost evaluation, and
the classical computer uses this cost information to adjust
the parameters of the gate sequence. Variational hybrid
algorithms have been proposed for Hamiltonian ground
state and excited state preparation [8, 14, 15], approx-
imate optimization [7], error correction [16], quantum
data compression [17, 18], quantum simulation [19, 20],
and quantum compiling [21]. A key feature of such algo-
rithms is their near-term relevance, since only the sub-
routine of cost evaluation occurs on the quantum com-
puter, while the optimization procedure is entirely clas-
sical, and hence standard classical optimization tools can
be employed.
In this work, we consider the application of diagonaliz-
ing quantum states. In condensed matter physics, diago-
nalizing states is useful for identifying properties of topo-
logical quantum phases—a ﬁeld known as entanglement
spectroscopy [22]. In data science and machine learning,
diagonalizing the covariance matrix (which could be en-
coded in a quantum state [2, 23]) is frequently employed
for principal component analysis (PCA). PCA identiﬁes
features that capture the largest variance in one’s data
and hence allows for dimensionality reduction [24].
Classical methods for diagonalization typically scale
polynomially in the matrix dimension [25].
Similarly,
the number of measurements required for quantum state
tomography—a general method for fully characterizing
a quantum state—scales polynomially in the dimension.
Interestingly, Lloyd et al. proposed a quantum algo-
rithm for diagonalizing quantum states that can poten-
tially perform exponentially faster than these methods
[2].
Namely, their algorithm, called quantum princi-
pal component analysis (qPCA), gives an exponential
speedup for low-rank matrices. qPCA employs quantum
phase estimation combined with density matrix exponen-
tiation. These subroutines require a signiﬁcant number
of qubits and gates, making qPCA diﬃcult to implement
arXiv:1810.10506v2  [quant-ph]  26 Jun 2019

2
in the near term, despite its long-term promise.
Here, we propose a variational hybrid algorithm for
quantum state diagonalization. For a given state ρ, our
algorithm is composed of three steps: (i) Train the pa-
rameters α of a gate sequence Up(α) such that ˜ρ =
Up(αopt)ρUp(αopt)† is approximately diagonal, where
αopt is the optimal value of α obtained (ii) Read out
the largest eigenvalues of ρ by measuring in the eigen-
basis (i.e., by measuring ˜ρ in the standard basis), and
(iii) Prepare the eigenvectors associated with the largest
eigenvalues. We call this the variational quantum state
diagonalization (VQSD) algorithm. VQSD is a near-term
algorithm with the same practical beneﬁts as other vari-
ational hybrid algorithms. Employing a layered ansatz
for Up(α) (where p is the number of layers) allows one
to obtain a hierarchy of approximations for the eigeval-
ues and eigenvectors. We therefore think of VQSD as an
approximate diagonalization algorithm.
We carefully choose our cost function C to have the
following properties: (i) C is faithful (i.e, it vanishes if
and only if ˜ρ is diagonal), (ii) C is eﬃciently computable
on a quantum computer, (iii) C has operational meanings
such that it upper bounds the eigenvalue and eigenvector
error (see Sec. II A), and (iv) C scales well for training
purposes in the sense that its gradient does not vanish
exponentially in the number of qubits. The precise deﬁ-
nition of C is given in Sec. II A and involves a diﬀerence
of purities for diﬀerent states. To compute C, we intro-
duce novel short-depth quantum circuits that likely have
applications outside the context of VQSD.
To illustrate our method, we implement VQSD on
Rigetti’s 8-qubit quantum computer.
We successfully
diagonalize one-qubit pure states using this quantum
computer. To highlight future applications (when larger
quantum computers are made available), we implement
VQSD on a simulator to perform entanglement spec-
troscopy on the ground state of the one-dimensional (1D)
Heisenberg model composed of 12 spins.
Our paper is organized as follows. Section II outlines
the VQSD algorithm and presents its implementation.
In Sec. III, we give a comparison to the qPCA algo-
rithm, and we elaborate on future applications. Section
IV presents our methods for quantifying diagonalization
and for optimizing our cost function.
II.
RESULTS
A.
The VQSD Algorithm
1.
Overall Structure
Figure 1 shows the structure of the VQSD algorithm.
The goal of VQSD is to take, as its input, an n-qubit den-
sity matrix ρ given as a quantum state and then output
approximations of the m-largest eigenvalues and their as-
sociated eigenvectors. Here, m will typically be much less
than 2n, the matrix dimension of ρ, although the user is
free to increase m with increased algorithmic complex-
ity (discussed below). The outputted eigenvalues will be
in classical form, i.e., will be stored on a classical com-
puter. In contrast, the outputted eigenvectors will be in
quantum form, i.e., will be prepared on a quantum com-
puter. This is necessary because the eigenvectors would
have 2n entries if they were stored on a classical com-
puter, which is intractable for large n. Nevertheless, one
can characterize important aspects of these eigenvectors
with a polynomial number of measurements on the quan-
tum computer.
Similar to classical eigensolvers, the VQSD algorithm
is an approximate or iterative diagonalization algorithm.
Classical eigenvalue algorithms are necessarily iterative,
not exact [26].
Iterative algorithms are useful in that
they allow for a trade-oﬀbetween run-time and accuracy.
Higher degrees of accuracy can be achieved at the cost of
more iterations (equivalently, longer run-time), or short
run-time can be achieved at the cost of lower accuracy.
This ﬂexibility is desirable in that it allows the user of the
algorithm to dictate the quality of the solutions found.
The iterative feature of VQSD arises via a layered
ansatz for the diagonalizing unitary. This idea similarly
appears in other variational hybrid algorithms, such as
the Quantum Approximate Optimization Algorithm [7].
Speciﬁcally, VQSD diagonalizes ρ by variationally updat-
ing a parameterized unitary Up(α) such that
˜ρp(α) := Up(α)ρU †
p(α)
(1)
is (approximately) diagonal at the optimal value αopt.
(For brevity we often write ˜ρ for ˜ρp(α).) We assume a
layered ansatz of the form
Up(α) = L1(α1)L2(α2) · · · Lp(αp) .
(2)
Here, p is a hyperparameter that sets the number of layers
Li(αi), and each αi is a set of optimization parameters
that corresponds to internal gate angles within the layer.
The parameter α in (1) refers to the collection of all αi
for i = 1, ..., p. Once the optimization procedure is ﬁn-
ished and returns the optimal parameters αopt, one can
then run a particular quantum circuit (shown in Fig. 1(c)
and discussed below) Nreadout times to approximately de-
termine the eigenvalues of ρ. The precision (i.e, the num-
ber of signiﬁcant digits) of each eigenvalue increases with
Nreadout and with the eigenvalue’s magnitude. Hence for
small Nreadout only the largest eigenvalues of ρ will be
precisely characterized, so there is a connection between
Nreadout and how many eigenvalues, m, are determined.
The hyperparameter p is a reﬁnement parameter, mean-
ing that the accuracy of the eigensystem (eigenvalues and
eigenvectors) typically increases as p increases. We for-
malize this argument as follows.
Let C denote our cost function, deﬁned below in (10),
which we are trying to minimize. In general, the cost
C will be non-increasing (i.e., will either decrease or stay
constant) in p. One can ensure that this is true by taking
the optimal parameters learned for p layers as the starting

3
(b)
(c)
(a)
(e)
(d)
FIG. 1. Schematic diagram showing the steps of the VQSD algorithm. (a) Two copies of quantum state ρ are provided as an
input. These states are sent to the parameter optimization loop (b) where a hybrid quantum-classical variational algorithm
approximates the diagonalizing unitary Up(αopt). Here, p is a hyperparameter that dictates the quality of solution found. This
optimal unitary is sent to the eigenvalue readout circuit (c) to obtain bitstrings z, the frequencies of which provide estimates of
the eigenvalues of ρ. Along with the optimal unitary Up(αopt), these bitstrings are sent to the eigenvector preparation circuit
(c) to prepare the eigenstates of ρ on a quantum computer. Both the eigenvalues and eigenvectors are the outputs (d) of the
VQSD algorithm.
point for the optimization of p + 1 layers and by setting
αp+1 such that Lp+1(αp+1) is an identity. This strategy
also avoids barren plateaus [27, 28] and helps to mitigate
the problem of local minima, as we discuss in Appendix C
of Supplementary Material (SM).
Next, we argue that C is closely connected to the ac-
curacy of the eigensystem. Speciﬁcally, it gives an up-
per bound on the eigensystem error. Hence, one obtains
an increasingly tighter upper bound on the eigensystem
error as C decreases (equivalently, as p increases). To
quantify eigenvalue error, we deﬁne
∆λ :=
d
X
i=1
(λi −˜λi)2 ,
(3)
where d = 2n, and {λi} and {˜λi} are the true and inferred
eigenvalues, respectively. Here, i is an index that orders
the eigenvalues in decreasing order, i.e., λi ⩾λi+1 and
˜λi ⩾˜λi+1 for all i ∈{1, ..., d−1}. To quantify eigenvector
error, we deﬁne
∆v :=
d
X
i=1
⟨δi|δi⟩,
with |δi⟩= ρ|˜vi⟩−˜λi|˜vi⟩= Π⊥
i ρ|˜vi⟩.
(4)
Here, |˜vi⟩is the inferred eigenvector associated with ˜λi,
and Π⊥
i = 11 −|˜vi⟩⟨˜vi| is the projector onto the subspace
orthogonal to |˜vi⟩. Hence, |δi⟩is a vector whose norm
quantiﬁes the component of ρ|˜vi⟩that is orthogonal to
|˜vi⟩, or in other words, how far |˜vi⟩is from being an
eigenvector of ρ.
As proven in Sec. IV A, our cost function upper bounds
the eigenvalue and eigenvector error up to a proportion-
ality factor β,
∆λ ⩽βC ,
and
∆v ⩽βC .
(5)
Because C is non-increasing in p, the upper bound in (5)
is non-increasing in p and goes to zero if C goes to zero.
We remark that ∆v can be interpreted as a weighted
eigenvector error, where eigenvectors with larger eigen-
values are weighted more heavily in the sum. This is a
useful feature since it implies that lowering the cost C
will force the eigenvectors with the largest eigenvalues to
be highly accurate. In many applications, such eigenvec-
tors are precisely the ones of interest. (See Sec. II B 2 for
an illustration of this feature.)
The various steps in the VQSD algorithm are shown
schematically in Fig. 1. There are essentially three main
steps: (1) an optimization loop that minimizes the cost
C via back-and-forth communication between a classical
and quantum computer, where the former adjusts α and
the latter computes C for Up(α), (2) a readout procedure
for approximations of the m largest eigenvalues, which
involves running a quantum circuit and then classically

4
(a)
(b)
FIG. 2.
(a) Layered ansatz for the diagonalizing unitary
Up(α).
Each layer Li, i = 1, ..., p, consists of a set of op-
timization parameters αi. (b) The two-qubit gate ansatz for
the ith layer, shown on four qubits. Here we impose periodic
boundary conditions on the top/bottom edge of the circuit so
that G3 wraps around from top to bottom. Appendix B of
SM discusses an alternative approach to the construction of
Up(α), in which the ansatz is modiﬁed during the optimiza-
tion process.
analyzing the statistics, and (3) a preparation procedure
to prepare approximations of the eigenvectors associated
with the m largest eigenvalues. In the following subsec-
tions, we elaborate on each of these procedures.
2.
Parameter Optimization Loop
Naturally, there are many ways to parameterize Up(α).
Ideally one would like the number of parameters to grow
at most polynomially in both n and p. Figure 2 presents
an example ansatz that satisﬁes this condition.
Each
layer Li is broken down into layers of two-body gates
that can be performed in parallel. These two-body gates
can be further broken down into parameterized one-body
gates, for example, with the construction in Ref. [29].
We discuss a diﬀerent approach to parameterize Up(α)
in Appendix B of SM.
For a given ansatz, such as the one in Fig. 2, parameter
optimization involves evaluating the cost C on a quantum
computer for an initial choice of parameters and then
modifying the parameters on a classical computer in an
iterative feedback loop. The goal is to ﬁnd
αopt := arg min
α
C(Up(α)) .
(6)
The classical optimization routine used for updating the
parameters can involve either gradient-free or gradient-
based methods. In Sec. IV B, we explore this further and
discuss our optimization methods.
In Eq. (6), C(Up(α)) quantiﬁes how far the state ˜ρp(α)
is from being diagonal. There are many ways to deﬁne
such a cost function, and in fact there is an entire ﬁeld
of research on coherence measures that has introduced
various such quantities [30]. We aim for a cost that is
eﬃciently computable with a quantum-classical system,
and hence we consider a cost that can be expressed in
terms of purities. (It is well known that a quantum com-
puter can ﬁnd the purity Tr(σ2) of an n-qubit state σ
with complexity scaling only linearly in n, an exponen-
tial speedup over classical computation [31, 32].) Two
such cost functions, whose individual merits we discuss
in Sec. IV A, are
C1(Up(α)) = Tr(ρ2) −Tr(Z(˜ρ)2) ,
(7)
C2(Up(α)) = Tr(ρ2) −1
n
n
X
j=1
Tr(Zj(˜ρ)2) .
(8)
Here, Z and Zj are quantum channels that dephase (i.e.,
destroy the oﬀ-diagonal elements) in the global standard
basis and in the local standard basis on qubit j, respec-
tively. Importantly, the two functions vanish under the
same conditions:
C1(Up(α)) = 0 ⇐⇒C2(Up(α)) = 0 ⇐⇒˜ρ = Z(˜ρ) .
(9)
So the global minima of C1 and C2 coincide and cor-
respond precisely to unitaries Up(α) that diagonalize ρ
(i.e., unitaries such that ˜ρ is diagonal).
As elaborated in Sec. IV A, C1 has operational mean-
ings: it bounds our eigenvalue error, C1 ⩾∆λ, and it is
equivalent to our eigenvector error, C1 = ∆v. However,
its landscape tends to be insensitive to changes in Up(α)
for large n.
In contrast, we are not aware of a direct
operational meaning for C2, aside from its bound on C1
given by C2 ⩾(1/n)C1. However, the landscape for C2 is
more sensitive to changes in Up(α), making it useful for
training Up(α) when n is large. Due to these contrasting
merits of C1 and C2, we deﬁne our overall cost function
C as a weighted average of these two functions
C(Up(α)) = qC1(Up(α)) + (1 −q)C2(Up(α)) ,
(10)
where q ∈[0, 1] is a free parameter that allows one to
tailor the VQSD method to the scale of one’s problem.
For small n, one can set q ≈1 since the landscape for
C1 is not too ﬂat for small n, and, as noted above, C1 is
an operationally relevant quantity. For large n, one can
set q to be small since the landscape for C2 will provide
the gradient needed to train Up(α).
The overall cost
maintains the operational meaning in (5) with
β = n/(1 + q(n −1)) .
(11)
Appendix D illustrates the advantages of training with
diﬀerent values of q.
Computing C amounts to evaluating the purities of
various quantum states on a quantum computer and then
doing some simple classical post-processing that scales
linearly in n. This can be seen from Eqns. (7) and (8).
The ﬁrst term, Tr(ρ2), in C1 and C2 is independent of

5
Up(α). Hence, Tr(ρ2) can be evaluated outside of the op-
timization loop in Fig. 1 using the Destructive Swap Test
(see Sec. IV A for the circuit diagram). Inside the loop,
we only need to compute Tr(Z(˜ρ)2) and Tr(Zj(˜ρ)2) for
all j. Each of these terms are computed by ﬁrst preparing
two copies of ˜ρ and then implementing quantum circuits
whose depths are constant in n. For example, the cir-
cuit for computing Tr(Z(˜ρ)2) is shown in Fig. 1(b), and
surprisingly it has a depth of only one gate. We call it
the Diagonalized Inner Product (DIP) Test. The circuit
for computing Tr(Zj(˜ρ)2) is similar, and we call it the
Partially Diagonalized Inner Product (PDIP) Test. We
elaborate on both of these circuits in Sec. IV A.
3.
Eigenvalue Readout
After
ﬁnding
the
optimal
diagonalizing
unitary
Up(αopt), one can use it to readout approximations of
the eigenvalues of ρ. Figure 1(c) shows the circuit for
this readout. One prepares a single copy of ρ and then
acts with Up(αopt) to prepare ˜ρp(αopt). Measuring in the
standard basis {|z⟩}, where z = z1z2...zn is a bitstring
of length n, gives a set of probabilities {˜λz} with
˜λz = ⟨z|˜ρp(αopt)|z⟩.
(12)
We take the ˜λz as the inferred eigenvalues of ρ.
We
emphasize that the ˜λz are the diagonal elements, not the
eigenvalues, of ˜ρp(αopt).
Each run of the circuit in Fig. 1(c) generates a bitstring
z corresponding to the measurement outcomes. If one
obtains z with frequency fz for Nreadout total runs, then
˜λest
z
= fz/Nreadout
(13)
gives an estimate for ˜λz. The statistical deviation of ˜λest
z
from ˜λz goes with 1/√Nreadout.
The relative error ϵz
(i.e., the ratio of the statistical error on ˜λest
z
to the value
of ˜λest
z ) then goes as
ϵz =
1
√Nreadout˜λest
z
=
√Nreadout
fz
.
(14)
This implies that events z with higher frequency fz have
lower relative error. In other words, the larger the in-
ferred eigenvalue ˜λz, the lower the relative error, and
hence the more precisely it is determined from the ex-
periment.
When running VQSD, one can pre-decide
on the desired values of Nreadout and a threshold for
the relative error, denoted ϵmax.
This error thresh-
old ϵmax will then determine m, i.e., how many of the
largest eigenvalues that get precisely characterized. So
m = m(Nreadout, ϵmax, {˜λz}) is a function of Nreadout,
ϵmax, and the set of inferred eigenvalues {˜λz}. Precisely,
we take m = |˜λ
est| as the cardinality of the following set:
˜λ
est = {˜λest
z
: ϵz ⩽ϵmax} ,
(15)
which is the set of inferred eigenvalues that were esti-
mated with the desired precision.
4.
Eigenvector Preparation
The ﬁnal step of VQSD is to prepare the eigenvectors
associated with the m-largest eigenvalues, i.e., the eigen-
values in the set in Eq. (15). Let Z = {z : ˜λest
z
∈˜λ
est}
be the set of bitstrings z associated with the eigenval-
ues in ˜λ
est. (Note that these bitstrings are obtained di-
rectly from the measurement outcomes of the circuit in
Fig. 1(c), i.e., the outcomes become the bitstring z.) For
each z ∈Z, one can prepare the following state, which
we take as the inferred eigenvector associated with our
estimate of the inferred eigenvalue ˜λest
z ,
|˜vz⟩= Up(αopt)†|z⟩
(16)
= Up(αopt)†(Xz1 ⊗· · · ⊗Xzn)|0⟩.
(17)
The circuit for preparing this state is shown in Fig. 1(d).
As noted in (17), one ﬁrst prepares |z⟩by acting with X
operators raised to the appropriate powers, and then one
acts with Up(αopt)† to rotate from the standard basis to
the inferred eigenbasis.
Once they are prepared on the quantum computer,
each inferred eigenvector |˜vz⟩can be characterized by
measuring expectation values of interest.
That is, im-
portant physical features such as energy or entanglement
(e.g., entanglement witnesses) are associated with some
Hermitian observable M, and one can evaluate the ex-
pectation value ⟨˜vz|M|˜vz⟩to learn about these features.
B.
Implementations
Here we present our implementations of VQSD, ﬁrst
for a one-qubit state on a cloud quantum computer to
show that it is amenable to currently available hardware.
Then, to illustrate the scaling to larger, more interesting
problems, we implement VQSD on a simulator for the
12-spin ground state of the Heisenberg model. See Ap-
pendices A and B of SM for further details. The code
used to generate some of the examples presented here
and in SM can be accessed from [33].
1.
One-Qubit State
We now discuss the results of applying VQSD to the
one-qubit plus state ρ = |+⟩⟨+| on the 8Q-Agave quan-
tum computer provided by Rigetti [34].
Because the
problem size is small (n = 1), we set q = 1 in the cost
function (10). Since ρ is a pure state, the cost function
is
C(Up(α)) = C1(Up(α)) = 1 −Tr(Z(˜ρ)2).
(18)
For Up(α), we take p = 1, for which the layered ansatz
becomes an arbitrary single qubit rotation.
The results of VQSD for this state are shown in Fig. 3.
In Fig. 3(a), the solid curve shows the cost versus the

6
Iteration
(a)
(b)
2
4
6
10
8
0
0.0
0.2
0.4
0.6
0.8
1.0
FIG. 3.
The VQSD algorithm run on Rigetti’s 8Q-Agave
quantum computer for ρ = |+⟩⟨+|. (a) A representative run
of the parameter optimization loop, using the Powell opti-
mization algorithm (see Sec. IV B for details and Appendix A
for data from additional runs). Cost versus iteration is shown
by the black solid line.
The dotted lines show the two in-
ferred eigenvalues. After four iterations, the inferred eigen-
values approach {0, 1}, as required for a pure state. (b) The
cost landscape on a noiseless simulator, Rigetti’s noisy simu-
lator, and Rigetti’s quantum computer. Error bars show the
standard deviation (due to ﬁnite sampling) of multiple runs.
The local minima occur roughly at the theoretically predicted
values of π/2 and 3π/2. During data collection for this plot,
the 8Q-Agave quantum computer retuned, after which its cost
landscape closely matched that of the noisy simulator.
number of iterations in the parameter optimization loop,
and the dashed curves show the inferred eigenvalues of ρ
at each iteration. Here we used the Powell optimization
algorithm, see Section IV B for more details. As can be
seen, the cost decreases to a small value near zero and
the eigenvalue estimates simultaneously converge to the
correct values of zero and one. Hence, VQSD successfully
diagonalized this state.
Figure 3(b) shows the landscape of the optimiza-
tion problem on Rigetti’s 8Q-Agave quantum computer,
Rigetti’s noisy simulator, and a noiseless simulator. Here,
we varied the angle α in the diagonalizing unitary U(α) =
Rx(π/2)Rz(α) and computed the cost at each value of
this angle. The landscape on the quantum computer has
local minima near the optimal angles α = π/2, 3π/2 but
the cost is not zero. This explains why we obtain the
correct eigenvalues even though the cost is nonzero in
Fig. 3(a). The nonzero cost can be due to a combination
of decoherence, gate inﬁdelity, and measurement error.
As shown in Fig. 3(b), the 8Q-Agave quantum computer
FIG. 4. Implementing VQSD with a simulator for the ground
state of the 1D Heisenberg model, diagonalizing a 4-spin sub-
system of a chain of 8 spins. We chose q = 1 for the cost
in (10) and employed a gradient-based method to ﬁnd αopt.
(a) Largest inferred eigenvalues ˜λj versus 1/p, where p is the
number of layers in our ansatz, which in this example takes
half-integer values corresponding to fractions of layers shown
in Fig. 2. The exact eigenvalues are shown on the y-axis (along
1/p = 0 line) with their degeneracy indicated in parenthe-
ses. One can see the largest eigenvalues converge to their cor-
rect values, including the correct degeneracies. Inset: overall
eigenvalue error ∆λ versus 1/p. (b) Largest inferred eigen-
values resolved by the inferred ⟨Sz⟩quantum number of their
associated eigenvector, for p = 5. The inferred data points
(red X’s) roughly agree with the theoretical values (black cir-
cles), particularly for the largest eigenvalues. Appendix B of
SM discusses Heisenberg chain of 12 spins.
retuned during our data collection, and after this retun-
ing, the landscape of the quantum computer matched
that of the noisy simulator signiﬁcantly better.
2.
Heisenberg Model Ground State
While current noise levels of quantum hardware limit
our implementations of VQSD to small problem sizes,

7
we can explore larger problem sizes on a simulator. An
important application of VQSD is to study the entangle-
ment in condensed matter systems, and we highlight this
application in the following example.
Let us consider the ground state of the 1D Heisenberg
model, the Hamiltonian of which is
H =
2n
X
j=1
S(j) · S(j+1) ,
(19)
with S(j) = (1/2)(σ(j)
x ˆx + σ(j)
y ˆy + σ(j)
z ˆz) and periodic
boundary conditions, S(2n+1) = S(1). Performing entan-
glement spectroscopy on the ground state |ψ⟩AB involves
diagonalizing the reduced state ρ = TrB(|ψ⟩⟨ψ|AB). Here
we consider a total of 8 spins (2n = 8). We take A to
be a subset of 4 nearest-neighbor spins, and B is the
complement of A.
The results of applying VQSD to the 4-spin reduced
state ρ via a simulator are shown in Fig. 4. Panel (a)
plots the inferred eigenvalues versus the number of layers
p in our ansatz (see Fig. 2). One can see that the inferred
eigenvalues converge to their theoretical values as p in-
creases. Panel (b) plots the inferred eigenvalues resolved
by their associated quantum numbers (z-component of
total spin).
This plot illustrates the feature we noted
previously that minimizing our cost will ﬁrst result in
minimizing the eigenvector error for those eigenvectors
with the largest eigenvalues. Overall our VQSD imple-
mentation returned roughly the correct values for both
the eigenvalues and their quantum numbers. Resolving
not only the eigenvalues but also their quantum num-
bers is important for entanglement spectroscopy [22], and
clearly VQSD can do this.
In Appendix B of SM we discuss an alternative ap-
proach employing a variable ansatz for Up(α), and we
present results of applying this approach to a 6-qubit
reduced state of the 12-qubit ground state of the Heisen-
berg model.
III.
DISCUSSION
We emphasize that VQSD is meant for states ρ that
have either low rank or possibly high rank but low en-
tropy H(ρ) = −Tr(ρ log ρ). This is because the eigen-
value readout step of VQSD would be exponentially com-
plex for states with high entropy. In other words, for high
entropy states, if one eﬃciently implemented the eigen-
value readout step (with Nreadout polynomial in n), then
very few eigenvalues would get characterized with the
desired precision. In Appendix F of SM we discuss the
complexity of VQSD for particular example states.
Examples of states for which VQSD is expected to be
eﬃcient include density matrices computed from ground
states of 1D, local, gapped Hamiltonians.
Also, ther-
mal states of some 1D systems in a many-body localized
phase at low enough temperature are expected to be diag-
onalizable by VQSD. These states have rapidly decaying
spectra and are eigendecomposed into states obeying a
1D area law [35–37]. This means that every eigenstate
can be prepared by a constant depth circuit in alternat-
ing ansatz form [36], and hence VQSD will be able to
diagonalize it.
A.
Comparison to Literature
Diagonalizing quantum states with classical methods
would require exponentially large memory to store the
density matrix, and the matrix operations needed for
diagonalization would be exponentially costly.
VQSD
avoids both of these scaling issues.
Another quantum algorithm that extracts the eigen-
values and eigenvectors of a quantum state is qPCA [2].
Similar to VQSD, qPCA has the potential for exponen-
tial speedup over classical diagonalization for particular
classes of quantum states. Like VQSD, the speedup in
qPCA is contingent on ρ being a low-entropy state.
We performed a simple implementation of qPCA to get
a sense for how it compares to VQSD, see Appendix G in
SM for details. In particular, just like we did for Fig. 3,
we considered the one-qubit plus state ρ = |+⟩⟨+|. We
implemented qPCA for this state on Rigetti’s noisy sim-
ulator (whose noise is meant to mimic that of their 8Q-
Agave quantum computer). The circuit that we imple-
mented applied one controlled-exponential-swap gate (in
order to approximately exponentiate ρ, as discussed in
[2]). We employed a machine-learning approach [38] to
compile the controlled-exponential-swap gate into a novel
short-depth gate sequence (see Appendix G in SM). With
this circuit we inferred the two eigenvalues of ρ to be ap-
proximately 0.8 and 0.2. Hence, for this simple example,
it appears that qPCA gave eigenvalues that were slightly
oﬀfrom the true values of 1 and 0, while VQSD was able
to obtain the correct eigenvalues, as discussed in Fig. 3.
B.
Future Applications
Finally we discuss various applications of VQSD.
As noted in Ref. [2], one application of quantum state
diagonalization is benchmarking of quantum noise pro-
cesses, i.e., quantum process tomography. Here one pre-
pares the Choi state by sending half of a maximally en-
tangled state through the process of interest. One can ap-
ply VQSD to the resulting Choi state to learn about the
noise process, which may be particular useful for bench-
marking near-term quantum computers.
A special case of VQSD is variational state prepa-
ration.
That is, if one applies VQSD to a pure state
ρ = |ψ⟩⟨ψ|, then one can learn the unitary U(α) that
maps |ψ⟩to a standard basis state. Inverting this uni-
tary allows one to map a standard basis state (and hence
the state |0⟩⊗n) to the state |ψ⟩, which is known as state
preparation. Hence, if one is given |ψ⟩in quantum form,
then VQSD can potentially ﬁnd a short-depth circuit

8
that approximately prepares |ψ⟩. Variational quantum
compiling algorithms that were very recently proposed
[21, 39] may also be used for this same purpose, and
hence it would be interesting to compare VQSD to these
algorithms for this special case. Additionally, in this spe-
cial case one could use VQSD and these other algorithms
as an error mitigation tool, i.e., to ﬁnd a short-depth
state preparation that achieves higher accuracy than the
original state preparation.
In machine learning, PCA is a subroutine in super-
vised and unsupervised learning algorithms and also has
many direct applications. PCA inputs a data matrix X
and ﬁnds a new basis such that the variance is maxi-
mal along the new basis vectors.
One can show that
this amounts to ﬁnding the eigenvectors of the covari-
ance matrix E[XXT ] with the largest eigenvalues, where
E denotes expectation value. Thus PCA involves diago-
nalizing a positive-semideﬁnite matrix, E[XXT ]. Hence
VQSD can perform this task provided one has access to
QRAM [23] to prepare the covariance matrix as a quan-
tum state. PCA can reduce the dimension of X as well
as ﬁlter out noise in data. In addition, nonlinear (kernel)
PCA can be used on data that is not linearly separable.
Very recent work by Tang [40] suggests that classical al-
gorithms could be improved for PCA of low-rank matri-
ces, and potentially obtain similar scaling as qPCA and
VQSD. Hence future work is needed to compare these
diﬀerent approaches to PCA.
Perhaps the most important near-term application of
VQSD is to study condensed matter physics.
In par-
ticular, we propose that one can apply the variational
quantum eigensolver [8] to prepare the ground state of
a many-body system, and then one can follow this with
the VQSD algorithm to characterize the entanglement
in this state. Ultimately this approach could elucidate
key properties of condensed matter phases. In particu-
lar, VQSD allows for entanglement spectroscopy, which
has direct application to the identiﬁcation of topological
order [22]. Extracting both the eigenvalues and eigenvec-
tors is useful for entanglement spectroscopy [22], and we
illustrated this capability of VQSD in Fig. 4. Finally, an
interesting future research direction is to check how the
discrepancies in preparation of multiple copies aﬀect the
performance of the diagonalization.
IV.
METHODS
A.
Diagonalization Test Circuits
Here we elaborate on the cost functions C1 and C2 and
present short-depth quantum circuits to compute them.
1.
C1 and the DIP Test
The function C1 deﬁned in (7) has several intuitive in-
terpretations. These interpretations make it clear that
C1 quantiﬁes how far a state is from being diagonal. In
particular, let DHS(A, B) := Tr
 (A −B)†(A −B)

de-
note the Hilbert-Schmidt distance. Then we can write
C1 = min
σ∈D DHS(˜ρ, σ)
(20)
= DHS(˜ρ, Z(˜ρ))
(21)
=
X
z,z′̸=z
|⟨z|˜ρ|z′⟩|2 .
(22)
In other words, C1 is (1) the minimum distance between
˜ρ and the set of diagonal states D, (2) the distance from
˜ρ to Z(˜ρ), and (3) the sum of the absolute squares of the
oﬀ-diagonal elements of ˜ρ.
C1 can also be written as the eigenvector error in (4)
as follows.
For an inferred eigenvector |˜vz⟩, we deﬁne
|δz⟩= ρ|˜vz⟩−˜λz|˜vz⟩and write the eigenvector error as
⟨δz|δz⟩= ⟨˜vz|ρ2|˜vz⟩+ ˜λ2
z −2˜λz⟨˜vz|ρ|˜vz⟩
(23)
= ⟨˜vz|ρ2|˜vz⟩−˜λ2
z ,
(24)
since ⟨˜vz|ρ|˜vz⟩= ˜λz. Summing over all z gives
∆v =
X
z
⟨δz|δz⟩=
X
z
⟨˜vz|ρ2|˜vz⟩−˜λ2
z
(25)
= Tr(ρ2) −Tr(Z(˜ρ)2) = C1 ,
(26)
which proves the bound in (5) for q = 1.
In addition, C1 bounds the eigenvalue error deﬁned in
(3). Let ˜λ = (˜λ1, ..., ˜λd) and λ = (λ1, ..., λd) denote the
inferred and actual eigenvalues of ρ, respectively, both
arranged in decreasing order. In this notation we have
∆λ = λ · λ + ˜λ · ˜λ −2λ · ˜λ
(27)
C1 = λ · λ −˜λ · ˜λ
(28)
= ∆λ + 2(λ · ˜λ −˜λ · ˜λ) .
(29)
Since the eigenvalues of a density matrix majorize its
diagonal elements, λ ≻˜λ, and the dot product with an
ordered vector is a Schur convex function, we have
λ · ˜λ ⩾˜λ · ˜λ .
(30)
Hence from (29) and (30) we obtain the bound
∆λ ⩽C1 ,
(31)
which corresponds to the bound in (5) for the special case
of q = 1.
For computational purposes, we use the diﬀerence of
purities interpretation of C1 given in (7).
The Tr(ρ2)
term is independent of Up(α). Hence it only needs to
be evaluated once, outside of the parameter optimization
loop. It can be computed via the expectation value of the
swap operator S on two copies of ρ, using the identity
Tr(ρ2) = Tr((ρ ⊗ρ)S) .
(32)

9
FIG. 5. Diagonalization test circuits used in VQSD. (a) The Destructive Swap Test computes Tr(στ) via a depth-two circuit.
(b) The Diagonalized Inner Product (DIP) Test computes Tr(Z(σ)Z(τ)) via a depth-one circuit. (c) The Partially Diagonalized
Inner Product (PDIP) Test computes Tr(Zj(σ)Zj(τ)) via a depth-two circuit, for a particular set of qubits j. While the DIP
test requires no postprocessing, the postprocessing for the Destructive Swap Test and the Partial DIP Test scales linearly in n.
This expectation value is found with a depth-two quan-
tum circuit that essentially corresponds to a Bell-basis
measurement, with classical post-processing that scales
linearly in the number of qubits [38, 41]. This is shown
in Fig. 5(a). We call this procedure the Destructive Swap
Test, since it is like the Swap Test, but the measurement
occurs on the original systems instead of on an ancilla.
Similarly, the Tr(Z(˜ρ)2) term could be evaluated by
ﬁrst dephasing ˜ρ and then performing the Destructive
Swap Test, which would involve a depth-three quantum
circuit with linear classical post-processing.
This ap-
proach was noted in Ref. [42].
However, there exists
a simpler circuit, which we call the Diagonalized Inner
Product (DIP) Test. The DIP Test involves a depth-one
quantum circuit with no classical post-processing.
An
abstract version of this circuit is shown in Fig. 5(b), for
two states σ and τ. The proof that this circuit computes
Tr(Z(σ)Z(τ)) is given in Appendix H of SM. For our ap-
plication we will set σ = τ = ˜ρ, for which this circuit
gives Tr(Z(˜ρ)2).
In summary, C1 is eﬃciently computed by using the
Destructive Swap Test for the Tr(ρ2) term and the DIP
Test for the Tr(Z(˜ρ)2) term.
2.
C2 and the PDIP Test
Like C1, C2 can also be rewritten in terms of of the
Hilbert-Schmidt distance. Namely, C2 is the average dis-
tance of ˜ρ to each locally-dephased state Zj(˜ρ):
C2 = 1
n
n
X
j=1
DHS(˜ρ, Zj(˜ρ)) .
(33)
where Zj(·) = P
z(|z⟩⟨z|j ⊗11k̸=j)(·)(|z⟩⟨z|j ⊗11k̸=j). Nat-
urally, one would expect that C2 ⩽C1, since ˜ρ should be
closer to each locally dephased state than to the fully
dephased state. Indeed this is true and can be seen from:
C2 = C1 −1
n
n
X
j=1
min
σ∈D DHS(Zj(˜ρ), σ) .
(34)
However, C1 and C2 vanish under precisely the same con-
ditions, as noted in Eq. (9). One can see this by noting
that C2 also upper bounds (1/n)C1 and hence we have
C2 ⩽C1 ⩽nC2 .
(35)
Combining the upper bound in (35) with the relations in
(26) and (31) gives the bounds in (5) with β deﬁned in
(11). The upper bound in (35) is proved as follows. Let
z = z1...zn and z′ = z′
1...z′
n be n-dimensional bitstrings.
Let S be the set of all pairs (z, z′) such that z ̸= z′, and
let Sj be the set of all pairs (z, z′) such that zj ̸= z′
j.
Then we have C1 = P
(z,z′)∈S |⟨z|˜ρ|z′⟩|2, and
nC2 =
n
X
j=1
X
(z,z′)∈Sj
|⟨z|˜ρ|z′⟩|2
(36)
⩾
X
(z,z′)∈SU
|⟨z|˜ρ|z′⟩|2 = C1 ,
(37)
where SU = Sn
j=1 Sj is the union of all the Sj sets. The
inequality in (37) arises from the fact that the Sj sets
have non-trivial intersection with each other, and hence
we throw some terms away when only considering the
union SU. The last equality follows from the fact that

10
SU = S, i.e, the set of all bitstring pairs that diﬀer from
each other (S) corresponds to the set of all bitstring pairs
that diﬀer for at least one element (SU).
Writing C2 in terms of purities, as in (8), shows how
it can be computed on a quantum computer. As in the
case of C1, the ﬁrst term in (8) is computed with the De-
structive Swap Test. For the second term in (8), each pu-
rity Tr(Zj(˜ρ)2) could also be evaluated with the Destruc-
tive Swap Test, by ﬁrst locally dephasing the appropriate
qubit. However, we present a slightly improved circuit to
compute these purities that we call the Partially Diago-
nalized Inner Product (PDIP) Test. The PDIP Test is
shown in Fig. 5(c) for the general case of feeding in two
distinct states σ and τ with the goal of computing the in-
ner product between Zj(σ) and Zj(τ). For generality we
let l, with 0 ⩽l ⩽n, denote the number of qubits being
locally dephased for this computation. If l > 0, we deﬁne
j = (j1, . . . , jl) as a vector of indices that indicates which
qubits are being locally dephased. The PDIP Test is a
hybrid of the Destructive Swap Test and the DIP Test,
corresponding to the former when l = 0 and the latter
when l = n. Hence, it generalizes both the Destructive
Swap Test and the DIP Test. Namely, the PDIP Test
performs the DIP Test on the qubits appearing in j and
performs the Destructive Swap Test on the qubits not
appearing in j. The proof that the PDIP Test computes
Tr(Zj(σ)Zj(τ)), and hence Tr(Zj(˜ρ)2) when σ = τ = ˜ρ,
is given in Appendix H of SM.
3.
C1 versus C2
Here we discuss the contrasting merits of the functions
C1 and C2, hence motivating our cost deﬁnition in (10).
As noted previously, C2 does not have an operational
meaning like C1.
In addition, the circuit for comput-
ing C1 is more eﬃcient than that for C2. The circuit in
Fig. 5(b) for computing the second term in C1 has a gate
depth of one, with n CNOT gates, n measurements, and
no classical post-processing. The circuit in Fig. 5(c) for
computing the second term in C2 has a gate depth of
two, with n CNOT gates, n −1 Hadamard gates, 2n −1
measurements, and classical post-processing whose com-
plexity scales linearly in n. So in every aspect, the circuit
for computing C1 is less complex than that for C2. This
implies that C1 can be computed with greater accuracy
than C2 on a noisy quantum computer.
On the other hand, consider how the landscape for C1
and C2 scale with n. As a simple example, suppose ρ =
|0⟩⟨0| ⊗· · · ⊗|0⟩⟨0|. Suppose one takes a single parameter
ansatz for U, such that U(θ) = RX(θ) ⊗· · · ⊗RX(θ),
where RX(θ) is a rotation about the X-axis of the Bloch
sphere by angle θ. For this example,
C1(θ) = 1 −Tr(Z(˜ρ)2) = 1 −x(θ)n
(38)
where x(θ)
=
Tr(Z(RX(θ)|0⟩⟨0|RX(θ)†)2)
=
(1 +
cos2 θ)/2.
If θ is not an integer multiple of π, then
x(θ) < 1, and x(θ)n will be exponentially suppressed
for large n. In other words, for large n, the landscape for
x(θ)n becomes similar to that of a delta function: it is
zero for all θ except for multiples of π. Hence, for large n,
it becomes diﬃcult to train the unitary U(θ) because the
gradient vanishes for most θ. This is just an illustrative
example, but this issue is general. Generally speaking,
for large n, the function C1 has a sharp gradient near
its global minima, and the gradient vanishes when one is
far away from these minima. Ultimately this limits C1’s
utility as a training function for large n.
In contrast, C2 does not suﬀer from this issue. For the
example in the previous paragraph,
C2(θ) = 1 −x(θ) ,
(39)
which is independent of n. So for this example the gra-
dient of C2 does not vanish as n increases, and hence C2
can be used to train θ. More generally, the landscape
of C2 is less barren than that of C1 for large n. We can
argue this, particularly, for states ρ that have low rank or
low entropy. The second term in (8), which is the term
that provides the variability with α, does not vanish even
for large n, since (as shown in Appendix I of SM):
Tr(Zj(˜ρ)2) ⩾2−H(ρ)−1 ⩾1
2r .
(40)
Here, H(ρ) = −Tr(ρ log2 ρ) is the von Neumann entropy,
and r is the rank of ρ. So as long as ρ is low entropy
or low rank, then the second term in C2 will not vanish.
Note that a similar bound does not exist for second term
in C1, which does tend to vanish for large n.
B.
Optimization Methods
Finding αopt in (6) is a major component of VQSD.
While many works have benchmarked classical optimiza-
tion algorithms (e.g., Ref. [43]), the particular case of op-
timization for variational hybrid algorithms [44] is limited
and needs further work [45]. Both gradient-based and
gradient-free methods are possible, but gradient-based
methods may not work as well with noisy data. Addi-
tionally, Ref. [27] notes that gradients of a large class
of circuit ansatze vanish when the number of parameters
becomes large. These and other issues (e.g., sensitivity to
initial conditions, number of function evaluations) should
be considered when choosing an optimization method.
In our preliminary numerical analyses (see Appendix E
in SM), we found that the Powell optimization algorithm
[46] performed the best on both quantum computer and
simulator implementations of VQSD. This derivative-free
algorithm uses a bi-directional search along each param-
eter using Brent’s method.
Our studies showed that
Powell’s method performed the best in terms of conver-
gence, sensitivity to initial conditions, and number of
correct solutions found.
The implementation of Pow-
ell’s algorithm used in this paper can be found in the
open-source Python package SciPy Optimize [47].
Fi-
nally, Appendix C of SM shows how our layered ansatz

11
for Up(α) as well as proper initialization of Up(α) helps
in mitigating the problem of local minima.
DATA AVAILABILITY
Data generated and analyzed during current study are
available from the corresponding author upon reasonable
request.
CODE AVAILABILITY
The code used to generate some of the examples pre-
sented here and in Supplementary Material can be ac-
cessed from [33].
ACKNOWLEDGMENTS
We thank Rigetti for providing access to their quantum
computer. The views expressed in this paper are those
of the authors and do not reﬂect those of Rigetti. RL,
EO, and AT acknowledge support from the U.S. Depart-
ment of Energy through a quantum computing program
sponsored by the LANL Information Science & Technol-
ogy Institute. RL acknowledges support from an Engi-
neering Distinguished Fellowship through Michigan State
University. LC was supported by the U.S. Department of
Energy through the J. Robert Oppenheimer fellowship.
PJC was supported by the LANL ASC Beyond Moore’s
Law project. LC and PJC were also supported by the
LDRD program at Los Alamos National Laboratory, by
the U.S. Department of Energy (DOE), Oﬃce of Science,
Oﬃce of Advanced Scientiﬁc Computing Research, and
also by the U.S. DOE, Oﬃce of Science, Basic Energy
Sciences, Materials Sciences and Engineering Division,
Condensed Matter Theory Program.
AUTHOR CONTRIBUTION
RL, AT and LC implemented the algorithms and per-
formed numerical analysis.
LC and PJC designed the
project. PJC proposed the cost function and proved the
analytical results.
RL, AT, ÉO-J, LC, and PJC con-
tributed to data analysis, as well as writing and editing
the ﬁnal manuscript.
COMPETING INTERESTS
The authors declare no competing interests.
[1] P. W. Shor, “Polynomial-time algorithms for prime fac-
torization and discrete logarithms on a quantum com-
puter,” SIAM review 41, 303–332 (1999).
[2] S. Lloyd, M. Mohseni,
and P. Rebentrost, “Quantum
principal component analysis,” Nature Physics 10, 631–
633.
[3] A. W. Harrow, A. Hassidim, and S. Lloyd, “Quantum al-
gorithm for linear systems of equations,” Physical Review
Letters 103, 150502 (2009).
[4] P. Rebentrost, A. Steﬀens, I. Marvian,
and S. Lloyd,
“Quantum singular-value decomposition of nonsparse
low-rank matrices,”
Physical Review A 97, 012327
(2018).
[5] S. K. Leyton and T. J. Osborne, “A quantum al-
gorithm
to
solve
nonlinear
diﬀerential
equations,”
arXiv:0812.4423 (2008).
[6] D. W. Berry, “High-order quantum algorithm for solv-
ing linear diﬀerential equations,” Journal of Physics A:
Mathematical and Theoretical 47, 105301 (2014).
[7] E. Farhi, J. Goldstone,
and S. Gutmann, “A quantum
approximate optimization algorithm,” arXiv:1411.4028
(2014).
[8] A. Peruzzo, J. McClean, P. Shadbolt, M.-H. Yung, X.-Q.
Zhou, P. J. Love, A. Aspuru-Guzik, and J. L. O’Brien,
“A variational eigenvalue solver on a photonic quantum
processor,” Nature Communications 5, 4213 (2014).
[9] A. Kandala, A. Mezzacapo, K. Temme, M. Takita,
M.
Brink,
J.
M.
Chow,
and
J.
M.
Gambetta,
“Hardware-eﬃcient variational quantum eigensolver for
small molecules and quantum magnets,” Nature 549, 242
(2017).
[10] D. W. Berry, A. M. Childs, R. Cleve, R. Kothari,
and
R. D. Somma, “Simulating Hamiltonian dynamics with
a truncated taylor series,” Physical Review Letters 114,
090502 (2015).
[11] J. Preskill, “Quantum computing and the entanglement
frontier,” arXiv:1203.5813 (2012).
[12] A. W. Harrow and A. Montanaro, “Quantum computa-
tional supremacy,” Nature 549, 203 (2017).
[13] S. Bravyi, G. Smith, and J. A. Smolin, “Trading classical
and quantum computational resources,” Physical Review
X 6, 021043 (2016).
[14] O. Higgott, D. Wang, and S. Brierley, “Variational Quan-
tum Computation of Excited States,” arXiv:1805.08138
(2018).
[15] T. Jones, S. Endo, S. McArdle, X. Yuan,
and S. C.
Benjamin, “Variational quantum algorithms for discover-
ing Hamiltonian spectra,” Physical Review A 99, 062304
(2019).
[16] P. D. Johnson, J. Romero, J. Olson, Y. Cao,
and
A. Aspuru-Guzik, “QVECTOR: an algorithm for device-
tailored quantum error correction,” arXiv:1711.02249
(2017).
[17] J. Romero, J. P. Olson, and A. Aspuru-Guzik, “Quantum
autoencoders for eﬃcient compression of quantum data,”
Quantum Science and Technology 2, 045001 (2017).
[18] A. Khoshaman, W. Vinci, B. Denis, E. Andriyash, and
M. H. Amin, “Quantum variational autoencoder,” Quan-
tum Science and Technology 4, 014001 (2018).

12
[19] Y. Li and S. C. Benjamin, “Eﬃcient variational quan-
tum simulator incorporating active error minimization,”
Physical Review X 7, 021050 (2017).
[20] C. Kokail, C. Maier, R. van Bijnen, T. Brydges, M. K.
Joshi, P. Jurcevic, C. A. Muschik, P. Silvi, R. Blatt, C. F.
Roos, and P. Zoller, “Self-verifying variational quantum
simulation of the lattice Schwinger model,” Nature 569,
355 (2019).
[21] S. Khatri, R. LaRose, A. Poremba, L. Cincio, A. T. Sorn-
borger,
and P. J. Coles, “Quantum-assisted quantum
compiling,” Quantum 3, 140 (2019).
[22] H. Li and F. D. M. Haldane, “Entanglement spectrum as
a generalization of entanglement entropy: Identiﬁcation
of topological order in non-abelian fractional quantum
Hall eﬀect states,” Physical Review Letters 101, 010504
(2008).
[23] V. Giovannetti, S. Lloyd,
and L. Maccone, “Quantum
random access memory,” Physical Review Letters 100,
160501 (2008).
[24] K. Pearson, “On lines and planes of closest ﬁt to systems
of points in space,” The London, Edinburgh, and Dublin
Philosophical Magazine and Journal of Science 2, 559–
572 (1901).
[25] L. N. Trefethen and D. Bau, Numerical Linear Algebra
(SIAM, Philadelphia, PA, 1997).
[26] This can be seen by noting that computing eigenvalues is
equivalent to computing roots of a polynomial equation
(namely the characteristic polynomial of the matrix) and
that no closed-form solution exists for the roots of general
polynomials of degree greater than or equal to ﬁve [25].
[27] J. R. McClean, S. Boixo, V. N. Smelyanskiy, R. Bab-
bush, and H. Neven, “Barren plateaus in quantum neural
network training landscapes,” Nature Communications 9,
4812 (2018).
[28] E.
Grant,
L.
Wossnig,
M.
Ostaszewski,
and
M. Benedetti, “An initialization strategy for address-
ing barren plateaus in parametrized quantum circuits,”
arXiv:1903.05076 (2019).
[29] F. Vatan and C. Williams, “Optimal quantum circuits for
general two-qubit gates,” Physical Review A 69, 032315
(2004).
[30] T. Baumgratz, M. Cramer,
and M. B. Plenio, “Quan-
tifying coherence,” Physical Review Letters 113, 140401
(2014).
[31] H. Buhrman, R. Cleve, J. Watrous,
and R. De Wolf,
“Quantum ﬁngerprinting,” Physical Review Letters 87,
167902 (2001).
[32] D. Gottesman and I. Chuang, “Quantum digital signa-
tures,” quant-ph/0105032 (2001).
[33] “VQSD source code,” https://github.com/rmlarose/
vqsd.
[34] R.
S.
Smith,
M.
J.
Curtis,
and
W.
J.
Zeng,
“A Practical Quantum Instruction Set Architecture,”
arXiv:1608.03355 (2016).
[35] M. B. Hastings, “An area law for one-dimensional quan-
tum systems,” Journal of Statistical Mechanics: Theory
and Experiment 2007, 08024 (2007).
[36] B. Bauer and C. Nayak, “Area laws in a many-body lo-
calized state and its implications for topological order,”
Journal of Statistical Mechanics:
Theory and Experi-
ment 2013, 09005 (2013).
[37] T. Grover, “Certain General Constraints on the Many-
Body Localization Transition,” arXiv:1405.1471 (2014).
[38] L. Cincio, Y. Subaşı, A. T. Sornborger, and P. J. Coles,
“Learning the quantum algorithm for state overlap,” New
Journal of Physics 20, 113022 (2018).
[39] T. Jones and S. C. Benjamin, “Quantum compila-
tion and circuit optimisation via energy dissipation,”
arXiv:1811.03147 (2018).
[40] E. Tang, “Quantum-inspired classical algorithms for prin-
cipal component analysis and supervised clustering,”
arXiv:1811.00414 (2018).
[41] J. C. Garcia-Escartin and P. Chamorro-Posada, “Swap
test and Hong-Ou-Mandel eﬀect are equivalent,” Physical
Review A 87, 052330 (2013).
[42] G. Smith, J. A. Smolin, X. Yuan, Q. Zhao, D. Girolami,
and X. Ma, “Quantifying coherence and entanglement via
simple measurements,” arXiv:1707.09928 (2017).
[43] L. M. Rios and N. V. Sahinidis, “Derivative-free opti-
mization: a review of algorithms and comparison of soft-
ware implementations,” Journal of Global Optimization
56, 1247–1293 (2013).
[44] G. G. Guerreschi and M. Smelyanskiy, “Practical op-
timization for hybrid quantum-classical algorithms,”
arXiv:1701.01450 (2017).
[45] J. R. McClean, J. Romero, R. Babbush, and A. Aspuru-
Guzik, “The theory of variational hybrid quantum-
classical algorithms,” New Journal of Physics 18, 023023
(2016).
[46] M. J. D. Powell, “A fast algorithm for nonlinearly con-
strained optimization calculations,” in Numerical Analy-
sis, Lecture Notes in Mathematics, edited by G. A.Editor
Watson (Springer Berlin Heidelberg, 1978) pp. 144–157.
[47] “Scipy optimization and root ﬁnding,” https://docs.
scipy.org/doc/scipy/reference/optimize.html
(2018).
[48] M. J. D. Powell, “Direct search algorithms for optimiza-
tion calculations,” Acta Numerica 7, 287–336 (1998).
[49] M. J. D. Powell, “The BOBYQA algorithm for bound
constrained optimization without derivatives,” (2009).
[50] F. Gao and L. Han, “Implementing the Nelder-Mead
simplex algorithm with adaptive parameters,” Compu-
tational Optimization and Applications 51, 259–277
(2012).
[51] J. Nocedal and S. Wright, Numerical Optimization, 2nd
ed., Springer Series in Operations Research and Financial
Engineering (Springer-Verlag, 2006).
[52] C. Cartis, J. Fiala, B. Marteau,
and L. Roberts, “Im-
proving the Flexibility and Robustness of Model-Based
Derivative-Free Optimization Solvers,” arXiv:1804.00154
(2018).

1
Supplementary Material for
“Variational Quantum State Diagonalization”
Appendix A: Details on VQSD Implementations
Here we provide further details on our implementa-
tions of VQSD in Sec. II B. This includes further details
about the optimization parameters as well as additional
statistics for our runs on ns on the quantum computer.
1.
Optimization Parameters
First, we discuss our implementation on a quantum
computer (data shown in Fig. 3).
Figure S.1 displays
the circuit used for this implementation. This circuit is
logically divided into three sections. First, we prepare
two copies of the plus state ρ = |+⟩⟨+| = H|0⟩⟨0|H
by doing a Hadamard gate H on each qubit.
Next,
we implement one layer of a unitary ansatz, namely
U(θ) = Rx(π/2)Rz(θ). This ansatz was chosen because
each gate can be natively implemented on Rigetti’s quan-
tum computer.
To simplify the search space, we re-
stricted to one parameter instead of a universal one-qubit
unitary. Last, we implement the DIP Test circuit, de-
scribed in Fig. 5, which here consists of only one CNOT
gate and one measurement.
For the parameter optimization loop, we used the Pow-
ell algorithm mentioned in Sec. IV B. This algorithm
found the minimum cost in less than ten objective func-
tion evaluations on average. Each objective function eval-
uation (i.e., call to the quantum computer) sampled from
10,000 runs of the circuit in Fig. S.1. As can be seen in
Fig. 3(b), 10,000 runs was suﬃcient to accurately esti-
mate the cost function (10) with small variance. Because
the problem size was small, we took q = 1 in (10), which
provided adequate variability in the cost landscape.
Because of the noise levels in current quantum com-
puters, we limited VQSD implementations on quantum
hardware to only one-qubit states. Noise aﬀects the com-
putation in multiple areas. For example, in state prepa-
ration, qubit-speciﬁc errors can cause the two copies of
ρ to actually be diﬀerent states.
Subsequent gate er-
rors (notably two-qubit gates), decoherence, and mea-
surement errors prevent the cost from reaching zero even
though the optimal value of θ is obtained. The eﬀect of
these various noise sources, and in particular the eﬀect
of discrepancies in preparation of two copies of ρ, will be
important to study in future work.
Next, we discuss our VQSD implementation on a sim-
ulator (data shown in Fig. 4). For this implementation
we again chose q = 1 in our cost function. Because of
the larger problem size (diagonalizing a 4-qubit state),
we employed multiple layers in our ansatz, up to p = 5.
The simulator directly calculated the measurement prob-
ability distribution in the DIP Test, as opposed to deter-
mining the desired probability via sampling.
This al-
State Preperation
Unitary Ansatz
DIP Test
FIG. S.1. Circuit used to implement VQSD for ρ = |+⟩⟨+| on
Rigetti’s 8Q-Agave quantum computer. Vertical dashed lines
separate the circuit into logical components.
lowed us to use a gradient-based method to optimize our
cost function, reducing the overall runtime of the opti-
mization. Hence, our simulator implementation for the
Heisenberg model demonstrated a future application of
VQSD while alleviating the optimization bottleneck that
is present for all variational quantum algorithms on large
problem sizes, an area that needs further research [45].
We explore optimization methods further in Appendix E.
2.
Additional statistics for the Quantum Computer
Implementation
Here, we present statistics for several runs of the
VQSD implementation run on Rigetti’s 8Q-Agave quan-
tum computer. One example plot of cost vs. iteration
for diagonalizing the plus state ρ = |+⟩⟨+| is shown in
Figure 3(a). Here, we present all data collected for this
implementation of VQSD, shown in Figure S.2. The fol-
lowing table displays the ﬁnal costs achieved as well the
associated inferred eigenvalues.
VQSD Run min(C) min( ˜
λest
z ) max( ˜
λest
z )
1
0.107
0.000
1.000
2
0.090
0.142
0.858
3
0.099
0.054
0.946
4
0.120
0.079
0.921
5
0.080
0.061
0.939
6
0.090
0.210
0.790
7
0.65
0.001
0.999
Avg.
0.093
0.078
0.922
Std.
0.016
0.070
0.070
TABLE I. Minimum cost and eigenvalues achieved after per-
forming the parameter optimization loop for seven indepen-
dent runs of VQSD for the example discussed in Sec. II B.
The ﬁnal two rows show average values and standard devia-
tion across all runs.

2
Iteration
Sampled Cost
0
5
10
15
20
25
0.05
0.10
0.15
0.20
0.25
0.30
0.35
0.40
0.45
FIG. S.2.
Cost vs iteration for all attempts of VQSD on
Rigetti’s 8Q-Agave computer for diagonalizing the plus state
ρ = |+⟩⟨+|. Each of the seven curves represents a diﬀerent
independent run. Each run starts at a random initial angle
and uses the Powell optimization algorithm to minimize the
cost.
Appendix B: Alternative Ansatz and the Heisenberg
Model Ground State
In this Appendix, we describe a modiﬁcation of the
layered ansatz discussed in Section II A. Figure 2 in the
main text shows an example of a layered ansatz in which
every layer has the same, ﬁxed structure consisting of
alternating two-qubit gates acting on nearest-neighbor
qubits. The modiﬁed approach presented here may be
useful in situations where there is no natural choice of
the structure of the layered ansatz.
Here, instead of working with a ﬁxed structure for the
diagonalizing unitary U(α), we allow it to vary during
the optimization process. The algorithm used to update
the structure of U(α) is probabilistic and resembles the
one presented in [38].
In the examples studied here, the initial U(α) consists
of a small number of random two-qubit gates with ran-
dom supports (i.e. the qubits on which a gate acts). An
optimization step involves minimizing the cost function
by changing parameters α as well as a small random
change to the structure of U(α).
This change to the
structure typically amounts to a random modiﬁcation of
support for a limited number of gates. The new struc-
ture is accepted or rejected following the usual simulated
annealing schemes. We refer the reader to Section II D
of [38] for further details on the optimization method.
The gate sequence representing U(α) is allowed to
grow. If the algorithm described above cannot minimize
the cost function for a speciﬁed number of iterations, an
identity gate (spanned by new variational parameters) is
randomly added to U(α). This step is similar in spirit
to adding a layer to U(α) as discussed in Section II A of
the main text.
FIG. S.3. Comparison of two approaches to obtaining the di-
agonalizing unitary U(α): (i) based on a ﬁxed layered ansatz
shown in Fig. 2 in the main text (black line) and (ii) based on
random updates to the structure of U(α) (red line). The plot
shows eigenvalue error ∆λ versus 1/D, where D is the number
of gates in U(α). For the same D, the second approach found
a more optimal gate sequence.
We compared the current method with the one based
on the layered ansatz and found that it produced diago-
nalizing circuits involving signiﬁcantly fewer gates. Fig-
ure S.3 shows the eigenvalue error ∆λ, deﬁned in Eq. (3),
as a function of 1/D, where D is the total number of gates
of U(α). Here, VQSD is used to diagonalize a 4-qubit
reduced state of the ground state of the one-dimensional
Heisenberg model deﬁned on 8 qubits, see Eq. (19). For
every number of gates D, the current algorithm outper-
forms the one based on the ﬁxed, layered ansatz. It ﬁnds
a sequence of gates that results in a smaller eigenvalue
error ∆λ.
Finally, we use the current optimization approach to
ﬁnd the spectrum of a 6-qubit reduced state ρ of the
12-qubit ground state of a one-dimensional Heisenberg
model. The results of performing VQSD on ρ are shown
in Fig. S.4. Panel (a) shows the convergence of the 11
largest inferred eigenvalues ˜λj of ρ to their exact values.
We can see that the quality of the inferred eigenvalues
increases quickly with the number of gates D used in
the diagonalizing unitary U(α). In panel (b), we show
the dominant part of the spectrum of ρ resolved in the
z-component of the total spin.
The results show that
VQSD could be used to accurately obtain the dominant
part of the spectrum of the density matrix together with
the associated quantum numbers.
Appendix C: Optimization and local minima
In this Appendix we describe a strategy to avoid lo-
cal minima that is used in the optimization algorithms
throughout the paper and detailed in Appendix B. We

3
FIG. S.4. VQSD applied to the ground state of the Heisenberg
model. Here we consider a 6-qubit reduced state ρ of the 12-
qubit ground state. (a) Largest inferred eigenvalues ˜λj of ρ
as a function of 1/D, where D is the total number of gates
in the diagonalizing unitary U(α). The inferred eigenvalues
converge to their exact values shown along the 1/D = 0 line
recovering the correct degeneracy. Inset: Eigenvalue error ∆λ
as a function of 1/D. (b) The largest inferred eigenvalues ˜λj
of ρ resolved in the ⟨Sz⟩quantum number. We ﬁnd very good
agreement between the inferred eigenvalues (red crosses) and
the exact ones (black circles), especially for large eigenvalues.
The data was obtained for D = 150 gates.
adapt the optimization involved in the diagonalization of
the 6-qubit density matrix described in Appendix B as
an illustrative example.
We note that the classical optimization problem asso-
ciated with VQSD is potentially very diﬃcult one. In the
example studied in Appendix B the diagonalizing unitary
consisted of 150 two-qubit gates. This means that in or-
der to ﬁnd that unitary one has to optimize over at least
150 · 13 continuous parameters (every two-qubit gate is
spanned by 15 parameters, but there is some reduction
in the total number of parameters when two consecutive
gates have overlapping supports).
Initiated randomly,
oﬀ-the-shelf techniques will most likely return subopti-
mal solution due to the presence of multiple local minima
FIG. S.5.
Cost function C versus 1/D for three indepen-
dent optimization runs. Here, D is the total number of gates
in the diagonalizing unitary UD(α). Every optimization run
got stuck at local minimum at some point during the mini-
mization but thanks to the growth of the ansatz for UD(α)
described in the text, the predeﬁned small value of C was
eventually attained. The data was obtained for a 6-qubit re-
duced state of the 12-qubit ground state of the Heisenberg
model.
and the rough cost function landscape.
Let UD(α) denote a diagonalizing unitary that is built
by D two-qubit gates parametrized by α. Our optimiza-
tion method begins with a shallow circuit consisting of
few gates only.
Since there is only a small number of
variational parameters, the local minimum is quickly at-
tained.
After this initial step, the circuit that imple-
ments the unitary UD(α) is grown by adding an iden-
tity gate (either randomly as discussed in this Appendix
or by means of a layer of identity gates as presented in
the main text). This additional gate contains new vari-
ational parameters that are initiated such that the uni-
tary UD(α) = UD+1(α) and hence the value of the cost
function are not changed. After the gate was added, the
unitary UD+1(α) contains more variational parameters
which allows for further minimization of the cost func-
tion. In summary, the optimization of a deeper circuit
UD+1(α) is initialized by previously obtained UD(α) as
opposed to random initialization. What is more, even if
the unitary UD(α) was not the most optimal one for a
given D, the growth of the circuit allows the algorithm to
escape the local minimum and eventually ﬁnd the global
one, as illustrated by an example below and shown in
Fig. S.5. For a similar discussion, see [28].
To clarify the above analysis, let us consider an ex-
ample of diagonalizing a 6-qubit reduced state of the
12-qubit ground state of the Heisenberg model, see Ap-
pendix B for comparison.
Figure S.5 shows the value
of the cost function C as a function of 1/D for three
independent optimization runs. Each optimization was
initialized randomly and we applied the same optimiza-
tion scheme described above to each of them.
We see

4
that despite getting stuck in local minima, every opti-
mization run managed to minimize the cost function to
the predeﬁned small value (which was set to 2 · 10−6 in
this example). For instance, at D = 28, optimization run
no. 2 clearly returns suboptimal solution (optimization
run no. 3 gives lower cost function by a factor of 6) but
after adding several identity gates, it manages to escape
the local minimum and continue towards the global one.
Appendix D: Optimization runs with various q
values
In this Appendix we present some numerical results for
training our overall cost function for various values of q.
Recall from Eq. (10) that q is the weighting parameter
that weights the contributions of C1 and C2 in the overall
cost, as follows:
C(Up(α)) = qC1(Up(α)) + (1 −q)C2(Up(α)) ,
(D1)
where
C1(Up(α)) = Tr(ρ2) −Tr(Z(˜ρ)2) ,
(D2)
C2(Up(α)) = Tr(ρ2) −1
n
n
X
j=1
Tr(Zj(˜ρ)2) .
(D3)
As argued in Section II, C1 is operationally meaning-
ful, while C2 has a landscape that is more amendable to
training when n is large. In particular, one expects that
for large n, the gradient of C1 is sharp near the global
minima but vanishes exponentially in n away from these
minima. In contrast, the gradient of C2 is not expected
to exponentially vanish as n increases, even away from
the minima.
Here, we numerically study the performance for dif-
ferent q values for a simple example where ρ is a ten-
sor product of qubit pure states.
Namely, we choose
ρ = Nn
j=1 Vj|0⟩⟨0|V †
j , where Vj = RX(θj) with θj ran-
domly chosen. Such tensor product states are diagonal-
izable by a single layer ansatz: U(α) = Nn
j=1 RX(αj).
We consider three diﬀerent problem sizes: n = 6, 8, and
10. Figure S.6 shows our numerical results.
Directly training the C1 cost (corresponding to q = 1)
sometimes fails to ﬁnd the global minimum. One can see
this in Fig. S.6, where the green curve fails to fully reach
zero cost. In contrast, the red and blue curves in Fig. S.6,
which correspond to q = 0.5 and q = 0 respectively,
approximately go to zero for large iterations.
Even more interesting are the purple and yellow curves,
which respectively correspond to evaluating the C1 cost
at the angles α obtained from training the q = 0.5 and
q = 0 costs. It is remarkable that both the purple and
yellow curves perform better (i.e., achieve lower values)
than the green curve. This implies that one can indirectly
train the C1 cost by training the q = 0.5 or q = 0 costs,
and this indirect training performs better than directly
training C1. Since C1 is operationally meaningful, this
indirect training with q < 1 is performing better in an
operationally meaningful way.
We expect that direct training of C1 will perform worse
as n increases, due to the exponential vanishing of the
gradient of C1. The particular runs shown in Fig. S.6
do not show this trend, although this can be explained
by the fact that the gradient of C1 depends signiﬁcantly
on the initial values of the α, and indeed we saw large
variability in the performance of the green curve even for
a ﬁxed n. Nevertheless, it is worth noting that we were
always able to directly train C1 (i.e., to make the green
curve go to zero) for n < 6, which is consistent with our
expectations.
Overall, Fig. S.6 provides numerical justiﬁcation of the
deﬁnition of our cost function as a weighted average, as
in Eq. (10). Namely, it shows that there is an advantage
to choosing q < 1.
Appendix E: Comparison of Optimization Methods
As emphasized previously,
numerical optimization
plays a key role in all variational hybrid algorithms,
and further research in optimization methods is needed.
In VQSD, the accuracy of the inferred eigenvalues are
closely tied to the performance of the optimization al-
gorithm used in the parameter optimization loop. This
issue becomes increasingly important as one goes to large
problem sizes (large n), where the number of parameters
in the diagonalizing unitary becomes large.
Here, we compare the performance of six diﬀerent
optimization algorithms when used inside the param-
eter optimization loop of VQSD. These include Pow-
ell’s algorithm [46], Constrained Optimization BY Linear
Approximation (COBYLA) [48], Bound Optimization
BY Quadratic Approximation (BOBYQA) [49], Nelder-
Mead [50], Broyden-Fletcher-Goldfarb-Shanno (BFGS)
[51], and conjugate gradient (CG) [51]. As mentioned in
the main text, Powell’s algorithm is a derivative-free opti-
mizer that uses a bi-directional search along each param-
eter. The COBYLA and BOBYQA algorithms are both
trust region or restricted-step methods, which approxi-
mate the objective function by a model function. The re-
gion where this model function is a good approximation is
known as the trust region, and at each step the optimizer
attempts to expand the trust region. The Nelder-Mead
algorithm is a simplex method useful for derivative-free
optimization with smooth objective functions.
Lastly,
the BFGS and CG algorithms are both gradient-based.
The BFGS method is a quasi-Newton method that uses
ﬁrst derivatives only, and the CG method uses a non-
linear conjugate gradient descent. The implementations
used in our study can be found in the open-source Python
package SciPy Optimize [47] and in Ref. [52].
For this study, we take the input state ρ to be a six-

5
FIG. S.6. Cost versus iteration for diﬀerent values of q, when ρ is a tensor product of pure states on n qubits. Here we consider
(a) n = 6, (b) n = 8, and (a) n = 10. We employed the COBYLA optimization method for training (see Appendix E for
discussion of this method). For each call to the quantum simulator (i.e., classical simulator of a quantum computer), we took
500 shots for statistics. The green, red, and blue curves respectively correspond to directly training the cost with q = 1, q = 0.5,
and q = 0. The purple and yellow curves respectively correspond to evaluating the q = 1 cost for the angles α obtained by
training the q = 0.5 and q = 0 costs.
qubit pure product state:
ρ =
6
O
j=1
|ψj⟩⟨ψj| ,
where
|ψj⟩= Vj|0⟩.
(E1)
Here, the state preparation unitary is
Vj = Rx(α(j)
x )Ry(α(j)
y )Rz(α(j)
z )
(E2)
where the angles (α(j)
x , α(j)
y , α(j)
z ) are randomly chosen.
Using each algorithm, we attempt to minimize the cost
by adjusting 36 parameters in one layer of the unitary
ansatz in Fig. 2.
For fairness of comparison, only the
objective function and initial starting point were input
to each algorithm, i.e., no special options such as con-
straints, bounds, or other information was provided. The
results of this study are shown in Fig. S.7 and Table II.
Figure S.7 shows cost versus iteration for each of the
six algorithms. Here, we deﬁne one iteration by a call
to the objective function in which the cost decreases. In
particular, the number of iterations is diﬀerent than the
number of cost function evaluations (see Table II), which
is not set a priori but rather determined by the opti-
mizer. Plotting cost per each function evaluation would
essentially produce a noisy curve since the optimizer is
trying many values for the parameters. Instead, we only
plot the cost for each parameter update in which the cost
decreases. Both the number of iterations, function evalu-
ations, and overall runtime are important features of the
optimizer.
In this study, as well as others, we found that the Pow-
ell optimization algorithm provides the best performance
in terms of lowest minimum cost achieved, sensitivity to
Alg. Powell COBYLA BOBYQA Nelder-Mead BFGS CG
r.r.
13.20
1
2.32
23.65
3.83
2.89
f.ev.
4474
341
518
7212
1016 1045
TABLE II. Relative average run-times (r.r.)
and absolute
number of function evaluations (f.ev.) of each optimization
algorithm (Alg.) used for the data obtained in Fig. S.7. For
example, BOBYQA took 2.32 times as long to run on average
than COBYLA, which took the least time to run out of all
algorithms. Absolute run-times depend on a variety of factors
and computer performance. For reference, the COBYLA al-
gorithm takes approximately one minute for this problem on
a laptop computer. The number of cost function evaluations
used (related to run-time but also dependent on the method
used by the optimizer) is shown in the second row.
initial conditions, and fraction of correct solutions found.
The trust-region algorithms COBYLA and BOBYQA
were the next best methods. In particular, although the
Powell algorithm consistently obtained lower minimum
costs, the COBYLA method ran thirteen times faster on
average (see Table II). Indeed, both trust region meth-
ods provided the shortest runtime. The gradient-based
methods BFGS and CG had comparable run-times but
were unable to ﬁnd any minima. Similarly, the Nelder-
Mead simplex algorithm was unable to ﬁnd any minima.
This method also had the longest average run-time of all
algorithms tested.
This preliminary analysis suggests that the Powell al-
gorithm is the best method for VQSD. For other vari-
ational quantum algorithms, this may not necessarily
be the case.
In particular, we emphasize that the op-

6
(b) COBYLA
(a) Powell
Cost
0
20
40
60
80
100
(c) BOBYQA
Cost
0
2
4
6
8
10
12
(d) Nelder-Mead
0
2
4
6
8
10
(f) BFGS
Iterations
Cost
0
2
4
6
8
10
(c) Conjugate Gradient (CG)
Iterations
0.0
0.2
0.4
0.6
0.8
1.0
0
2
4
6
8
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
0
5
10
15
20
25
FIG. S.7. Optimization tests on six-qubit product states in
the VQSD algorithm. Each plot shows a diﬀerent optimiza-
tion algorithm (described in main text) and curves on each
plot show optimization attempts with diﬀerent (random) ini-
tial conditions. Cost refers to the C1 cost function (q = 1 in
(10)), and each iteration is deﬁned by a decrease in the cost
function. As can be seen, the Powell algorithm is the most
robust to initial conditions and provides the largest number
of solved problem instances.
timization landscape is determined by both the unitary
ansatz and the cost function deﬁnition, which may vary
drastically in diﬀerent algorithms. While we found that
gradient-based methods did not perform well for VQSD,
they may work well for other applications. Additionally,
optimizers that we have not considered here may also
provide better performance. We leave these questions to
further work.
Appendix F: Complexity for particular examples
1.
General Complexity Remarks
In what follows we discuss some simple examples of
states to which one might apply VQSD. There are several
aspects of complexity to keep in mind when considering
these examples, including:
(C1) The gate complexity of the unitary that diagonal-
izes ρ. (It is worth remarking that approximate diago-
nalization might be achieved with a less complex unitary
than exact diagonalization.)
(C2) The complexity of searching through the search
space to ﬁnd the diagonalizing unitary.
(C3) The statistical complexity associated with read-
ing out the eigenvalues.
Naturally, (C1) is related to (C2). However, being eﬃ-
cient with respect to (C1) does not guarantee that (C2)
is eﬃcient.
2.
Example States
In the simplest case, suppose ρ = |ψ1⟩⟨ψ1| ⊗· · · ⊗
|ψn⟩⟨ψn| is a tensor product of pure states. This state can
be diagonalized by a depth-one circuit U = U1 ⊗· · ·⊗Un
composed of n one-qubit gates (all done in parallel). Each
Uj diagonalizes the associated |ψj⟩⟨ψj| state. Searching
for this unitary within our ansatz can be done by setting
p = 1, i.e., with a single layer L1 shown in Fig. 2. A
single layer is enough to ﬁnd the unitary that exactly di-
agonalizes ρ in this case. Hence, for this example, both
complexities (C1) and (C2) are eﬃcient. Finally, note
that the eigenvalue readout, (C3), is eﬃcient because
there is only one non-zero eigenvalue. Hence, ˜λest
z
≈1
and ϵz ≈1/√Nreadout for this eigenvalue. This implies
that Nreadout can be chosen to be constant, independent
of n, in order to accurately characterize this eigenvalue.
A generalization of product states are classically cor-
related states, which have the form
ρ =
X
z
pz|b(1)
z1 ⟩⟨b(1)
z1 | ⊗· · · ⊗|b(n)
zn ⟩⟨b(n)
zn |
(F1)
where {|b(j)
0 ⟩, |b(j)
1 ⟩} form an orthonormal basis for
qubit j. Like product states, classically correlated states
can be diagonalized with a depth-one circuit composed
of one-body unitaries. Hence (C1) and (C2) are eﬃcient
for such states. However, the complexity of eigenvalue
readout depends on the {pz} distribution; if it is high
entropy then eigenvalue readout can scale exponentially.
Finally, we consider pure states of the form ρ = |ψ⟩⟨ψ|.
For such states, eigenvalue readout (C3) is eﬃcient be-
cause Nreadout can be chosen to be independent of n, as
we noted earlier for the example of pure product states.
Next we argue that the gate complexity of the diago-
nalizing unitary, (C1), is eﬃcient. The argument is sim-
ply that VQSD takes the state ρ as its input, and ρ must
have been prepared on a quantum computer. Let V be
the unitary that was used to prepare |ψ⟩= V |0⟩on the
quantum computer. For large n, V must have been ef-
ﬁcient to implement, otherwise the state |ψ⟩could not
have been prepared. Note that V †, which is constructed
from V by reversing the order of the gates and adjoint-
ing each gate, can be used to diagonalize ρ. Because V is
eﬃciently implementable, then V † is also eﬃciently im-
plementable. Hence, ρ can be eﬃciently diagonalized. A
subtlety is that one must compile V † into one’s ansatz,
such as the ansatz in Fig. 2. Fortunately, the overhead
needed to compile V † into our ansatz grows (at worst)
only linearly in n. An explicit construction for compiling
V † into our ansatz is as follows. Any one-qubit gate di-
rectly translates without overhead into our ansatz, while
any two-qubit gate can be compiled using a linear number
of swap gates to make the qubits of interest to be nearest

7
neighbors, then performing the desired two-qubit gate,
and ﬁnally using a linear number of swap gates to move
the qubits back to their original positions.
Let us now consider the complexity (C2) of searching
for U. Since there are a linear number of parameters in
each layer, and p needs only to grow polynomially in n,
then the total number of parameters grows only poly-
nomially in n. But this does not guarantee that we can
eﬃciently minimize the cost function, since the landscape
is non-convex. In general, search complexity for problems
such as this remains an open problem. Hence, we cannot
make a general statement about (C2) for pure states.
Appendix G: Implementation of qPCA
In the main text we compared VQSD to the qPCA al-
gorithm. Here we give further details on our implemen-
tation of qPCA. Let us ﬁrst give an overview of qPCA.
1.
Overview of qPCA
The qPCA algorithm exploits two primitives: quan-
tum phase estimation and density matrix exponentiation.
Combining these two primitives allows one to estimate
the eigenvalues and prepare the eigenvectors of a state ρ.
Density matrix exponentiation refers to generating the
unitary V (t) = e−iρt for a given state ρ and arbi-
trary time t.
For qPCA, one actually needs to apply
the controlled-V (t) gate (CV (t)).
Namely, in qPCA,
the CV (t) gate must be applied for a set of times,
{t, 2t, 22t, ..., 2xt}, as part of the phase-estimation algo-
rithm. Here we deﬁne tmax := 2xt.
Ref. [2] noted that V (t) can be approximated with a
sequence of k exponential swap operations between a tar-
get state σ and k copies of ρ. That is, let SJK be the swap
operator between systems J and K, and let σ and ρ⊗k
be states on systems A and B = B1...Br, respectively.
Then one performs the transformation
τAB = σ ⊗(ρ⊗k)
→
τ ′
AB = W(σ ⊗(ρ⊗k))W † ,
(G1)
where
W = UABk · · · UAB1,
and
UJK = e−iSJK∆t .
(G2)
The resulting reduced state is
τ ′
A = TrB(τ ′
AB) ≈V (t)ρV (t)†
(G3)
where t = k∆t. Finally, by turning each UJK in (G2)
into a controlled operation:
CUJK = |0⟩⟨0| ⊗11 + |1⟩⟨1| ⊗e−iSJK∆t ,
(G4)
and hence making W controlled, one can then construct
an approximation of CV (t).
apply k times 
FIG. S.8. Circuit for our qPCA implementation. Here, the
eigenvalues of a one-qubit pure state ρ are estimated to a
single digit of precision. We use k copies of ρ to approximate
CV (t) by applying the controlled-exponential-swap operator k
times for a time period ∆t = t/k. The bottom panel shows
our compilation of the controlled-exponential-swap gate into
one- and two-qubit gates.
If one chooses the input state for quantum phase es-
timation to be ρ = P
z λz|vz⟩⟨vz| itself, then the ﬁnal
state becomes
X
z
λz|vz⟩⟨vz| ⊗|ˆλz⟩⟨ˆλz|
(G5)
where ˆλz is a binary representation of an estimate of
the corresponding eigenvalue λz. One can then sample
from the state in (G5) to characterize the eigenvalues and
eigenvectors.
The approximation of V (t) in (G3) can be done with
accuracy ϵ provided that one uses O(t2ϵ−1) copies of ρ.
The time tmax needed for quantum phase estimation to
achieve accuracy ϵ is tmax = O(ϵ−1). Hence, with qPCA,
the eigenvalues and eigenvectors can be obtained with
accuracy ϵ provided that one uses O(ϵ−3) copies of ρ.
2.
Our Implementation of qPCA
Figure S.8 shows our strategy for implementing qPCA
on an arbitary one-qubit state ρ. The circuit shown corre-
sponds to the quantum phase estimation algorithm with

8
FIG. S.9. The largest inferred eigenvalue for the one-qubit
pure state ρ = |+⟩⟨+| versus application time of unitary e−iρt,
for our implementation of qPCA on Rigetti’s noisy and noise-
less QVMs. Curves are shown for k = 1 and k = 2, where
k indicates the number of controlled-exponential-swap opera-
tors applied.
one bit of precision (i.e., one ancilla qubit). A Hadamard
gate is applied to the ancilla qubit, which then acts as
the control system for the CV (t) gate, and ﬁnally the
ancilla is measured in the x-basis. The CV (t) is approx-
imated (as discussed above) with k applications of the
controlled-exponential-swap gate.
To implement qPCA, the controlled-exponential-swap
gate in (G4) must be compiled into one- and two-body
gates. For this purpose, we used the machine-learning
approach from Ref. [38] to obtain a short-depth gate
sequence for controlled-exponential-swap. The gate se-
quence we obtained is shown in Fig. S.8 and involves 7
CNOTs and 8 one-qubit gates.
Most of the one-qubit
gates are z-rotations and hence are error-free (imple-
mented via a clock change), including the following gates:
u1 = u7 = Rz(−(π + ∆t)/2)
(G6)
u3 = Rz((π −∆t)/2)
(G7)
u4 = Rz(∆t/2)
(G8)
u5 = Rz((π + ∆t)/2)
(G9)
u8 = Rz(π/2) .
(G10)
The one-qubit gates that are not z-rotations are:
u2 =
1
√
2
 
1
1
e−i(π−∆t)/2 ei(π+∆t)/2
!
(G11)
u6 =
1
√
2
 
1
e−i(π+∆t)/2
−i
e−i∆t/2
!
.
(G12)
We implemented the circuit in Fig. S.8 using both
Rigetti’s noiseless simulator, known as the Quantum Vir-
tual Machine (QVM), as well as their noisy QVM that
QVM
k = 1
k = 2
noiseless
≈{1, 0}
≈{1, 0}
noisy
≈{0.8, 0.2} ≈{0.7, 0.3}
TABLE III. Estimated eigenvalues for the ρ = |+⟩⟨+| state
using qPCA on both the noiseless and the noisy QVMs of
Rigetti.
utilizes a noise model of their 8Q-Agave chip. Because
the latter is meant to mimic the noise in the 8Q-Agave
chip, our qPCA results on the noisy QVM can be com-
pared to our VQSD results on the 8Q-Agave chip in
Fig. 3. (We remark that lack of availability prevented us
from implementing qPCA on the actual 8Q-Agave chip.)
For our implementation, we chose the one-qubit plus
state, ρ = |+⟩⟨+|. Implementations were carried out us-
ing both one and two controlled-exponential-swap gates,
corresponding to k = 1 and k = 2. The time t for which
the unitary e−iρt was applied was increased.
Figure S.9 shows the raw data, i.e., the largest in-
ferred eigenvalue versus t. In each case, small values of
t gave more accurate eigenvalues. In the noiseless case,
the eigenvalues of ρ = |+⟩⟨+| were correctly estimated to
be ≈{1, 0} already for k = 1 and consequently also for
k = 2. In the noisy case, the eigenvalues were estimated
to be ≈{0.8, 0.2} for k = 1 and ≈{0.7, 0.3} for k = 2,
where we have taken the values for small t. Table III
summarizes the diﬀerent cases.
Already for the case of k = 1, the required resources
of qPCA (3 qubits + 7 CNOT gates) for estimating
the eigenvalue of an arbitary pure one-qubit state ρ are
higher than those of the DIP test (2 qubits + 1 CNOT
gate) for the same task.
Consequently, the DIP test
yields more accurate results as can be observed by com-
paring Fig.
3 to Fig.
S.9.
Increasing the number of
copies to k = 2 only decreases the accuracy of the esti-
mation, since the CV (t) gate is already well approximated
for short application times t when k = 1 in the noiseless
case. Thus, increasing the number of copies does not oﬀer
any improvement in the noiseless case, but instead leads
to poorer estimation performance in the noisy case. This
can be seen for the k = 2 case (see Fig. S.9 and Table
III), due to the doubled number of required CNOT gates
relative to k = 1.
Appendix H: Circuit derivation
1.
DIP test
Here we prove that the circuit in Fig. S.10(a) computes
Tr(Z(σ)Z(τ)) for any two density matrices σ and τ.
Let σ and τ be states on the n-qubit systems A and
B, respectively. Let ωAB = σ⊗τ denote the initial state.

9
FIG. S.10. Test circuits used to compute the cost function in
VQSD. (a) DIP test (b) PDIP test. (These circuits appear in
Fig. 5 and are also shown here for the reader’s convenience.)
The action of the CNOTs in Fig. S.10(a) gives the state
ω′
AB =
X
z,z′
XzσXz′ ⊗|z⟩⟨z|τ|z′⟩⟨z′|,
(H1)
where the notation Xz means Xz1 ⊗Xz2 ⊗· · · ⊗Xzn.
Partially tracing over the B system gives
ω′
A =
X
z
τz,zXzσXz ,
(H2)
where τz,z = ⟨z|τ|z⟩. The probability for the all-zeros
outcome is then
⟨0|ω′
A|0⟩=
X
z
τz,z⟨0|XzσXz|0⟩=
X
z
τz,zσz,z, (H3)
which follows because Xz|0⟩= |z⟩. Hence the probabil-
ity for the all-zeros outcome is precisely the diagonalized
inner product, Tr(Z(σ)Z(τ)). Note that in the special
case where σ = τ = ˜ρ, we obtain the sum of the squares
of the diagonal elements, P
z ˜ρ2
z,z = Tr(Z(˜ρ)2).
2.
PDIP test
We prove that the circuit in Fig. S.10(b) computes
Tr(Zj(σ)Zj(τ)) for a given set of qubits j.
Let j′ denote the complement of j. Let σ and τ, re-
spectively, be states on the n-qubit systems A = Ajj′
and B = Bjj′. The initial state ωAB = σ ⊗τ evolves,
under the action of the CNOTs associated with the DIP
Test and then tracing over the control systems, to
ω′
ABj′ =
X
z
(Xz ⊗11)σ(Xz ⊗11) ⊗TrBj((|z⟩⟨z| ⊗11)τ),
(H4)
where Xz and |z⟩⟨z| act non-trivially only on the j sub-
systems of A and B, respectively. Measuring system Aj
and obtaining the all-zeros outcome would leave systems
Aj′Bj′ in the (unnormalized) conditional state:
TrAj((|0⟩⟨0| ⊗11)ω′
ABj′) =
X
z
σz
j′ ⊗τ z
j′,
(H5)
where σz
j′ := TrAj((|z⟩⟨z|⊗11)σ) and τ z
j′ := TrBj((|z⟩⟨z|⊗
11)τ). Finally, computing the expectation value for the
swap operator (via the Destructive Swap Test) on the
state in (H5) gives
X
z
Tr((σz
j′ ⊗τ z
j′)S) =
X
z
Tr(σz
j′τ z
j′) = Tr(Zj(σ)Zj(τ)) .
(H6)
The last equality can be veriﬁed by noting that Zj(σ) =
P
z |z⟩⟨z|⊗σz
j′ and Zj(τ) = P
z |z⟩⟨z|⊗τ z
j′. Specializing
(H6) to σ = τ = ˜ρ gives the quantity Tr(Zj(˜ρ)2).
Appendix I: Proof of Eq. (40)
Let H2(σ) = −log2[Tr(σ2)] be the Renyi entropy of
order two. Then, noting that H2(σ) ⩽H(σ), we have
Tr(Zj(˜ρ)2) = 2−H2(Zj(˜ρ)) ⩾2−H(Zj(˜ρ)) .
(I1)
Next, let A denote qubit j, and let B denote all the other
qubits. This allows us to write ρ = ρAB and ˜ρ = ˜ρAB.
Let C be a purifying system such that ρABC and ˜ρABC
are both pure states. Then we have
H(Zj(˜ρ)) = H(Zj(˜ρAB))
(I2)
= H(Zj(˜ρAC))
(I3)
⩽H(Zj(˜ρA)) + H(˜ρC)
(I4)
where the inequality in (I4) used the subadditivity of von
Neumann entropy. Finally, note that
H(˜ρC) = H(˜ρAB) = H(ρAB) = H(ρ)
(I5)
and H(Zj(˜ρA)) ⩽1, which gives
H(Zj(˜ρ)) ⩽1 + H(ρ) .
(I6)
Substituting (I6) into (I1) gives
Tr(Zj(˜ρ)2) ⩾2−1−H(ρ) ,
(I7)
and (40) follows from H(ρ) ⩽log2 r.

