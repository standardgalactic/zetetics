Nicolas Peltier
Viorica Sofronie-Stokkermans (Eds.)
 123
LNAI 12166
10th International Joint Conference, IJCAR 2020
Paris, France, July 1–4, 2020
Proceedings, Part I
Automated Reasoning

Lecture Notes in Artiﬁcial Intelligence
12166
Subseries of Lecture Notes in Computer Science
Series Editors
Randy Goebel
University of Alberta, Edmonton, Canada
Yuzuru Tanaka
Hokkaido University, Sapporo, Japan
Wolfgang Wahlster
DFKI and Saarland University, Saarbrücken, Germany
Founding Editor
Jörg Siekmann
DFKI and Saarland University, Saarbrücken, Germany

More information about this series at http://www.springer.com/series/1244

Nicolas Peltier
• Viorica Sofronie-Stokkermans (Eds.)
Automated Reasoning
10th International Joint Conference, IJCAR 2020
Paris, France, July 1–4, 2020
Proceedings, Part I
123

Editors
Nicolas Peltier
CNRS, LIG, Université Grenoble Alpes
Saint Martin d’Hères, France
Viorica Sofronie-Stokkermans
University Koblenz-Landau
Koblenz, Germany
ISSN 0302-9743
ISSN 1611-3349
(electronic)
Lecture Notes in Artiﬁcial Intelligence
ISBN 978-3-030-51073-2
ISBN 978-3-030-51074-9
(eBook)
https://doi.org/10.1007/978-3-030-51074-9
LNCS Sublibrary: SL7 – Artiﬁcial Intelligence
© Springer Nature Switzerland AG 2020
The chapter “Constructive Hybrid Games” is licensed under the terms of the Creative Commons Attribution
4.0 International License (http://creativecommons.org/licenses/by/4.0/). For further details see license
information in the chapter.
This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part of the
material is concerned, speciﬁcally the rights of translation, reprinting, reuse of illustrations, recitation,
broadcasting, reproduction on microﬁlms or in any other physical way, and transmission or information
storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar methodology now
known or hereafter developed.
The use of general descriptive names, registered names, trademarks, service marks, etc. in this publication
does not imply, even in the absence of a speciﬁc statement, that such names are exempt from the relevant
protective laws and regulations and therefore free for general use.
The publisher, the authors and the editors are safe to assume that the advice and information in this book are
believed to be true and accurate at the date of publication. Neither the publisher nor the authors or the editors
give a warranty, express or implied, with respect to the material contained herein or for any errors or
omissions that may have been made. The publisher remains neutral with regard to jurisdictional claims in
published maps and institutional afﬁliations.
This Springer imprint is published by the registered company Springer Nature Switzerland AG
The registered company address is: Gewerbestrasse 11, 6330 Cham, Switzerland

Preface
These volumes contain the papers presented at the 10th International Joint Conference
on Automated Reasoning (IJCAR 2020) initially planned to be held in Paris, but – due
to the COVID-19 pandemic – held by remote conferencing during July 1-4, 2020.
IJCAR is the premier international joint conference on all aspects of automated
reasoning, including foundations, implementations, and applications, comprising sev-
eral leading conferences and workshops. IJCAR 2020 united CADE, the Conference on
Automated Deduction, TABLEAUX, the International Conference on Automated
Reasoning with Analytic Tableaux and Related Methods, FroCoS, the International
Symposium on Frontiers of Combining Systems, and ITP, the International Conference
on Interactive Theorem Proving. Previous IJCAR conferences were held in Siena
(Italy) in 2001, Cork (Ireland) in 2004, Seattle (USA) in 2006, Sydney (Australia) in
2008, Edinburgh (UK) in 2010, Manchester (UK) in 2012, Vienna (Austria) in 2014,
Coimbra (Portugal) in 2016, and Oxford (UK) in 2018.
150 papers were submitted to IJCAR: 105 regular papers, 21 system description,
and 24 short papers, describing interesting work in progress. Each submission was
assigned to three Program Committee (PC) members; in a few cases, a fourth additional
review was requested. A rebuttal phase was added for the authors to respond to the
reviews before the ﬁnal deliberation. The PC accepted 62 papers, resulting in an
acceptance rate of about 41%: 46 regular papers (43%), 11 system descriptions (52%),
and 5 short papers (20%).
In addition, the program included three invited talks, by Clark Barrett, Elaine
Pimentel, and Ruzica Piskac, plus two additional invited talks shared with the con-
ference on Formal Structures for Computation and Deduction (FSCD), by John
Harrison and René Thiemann (the abstract of the invited talk by René Thiemann is
available in the proceedings of FSCD 2020).
The Best Paper Award was shared this year by two papers: “Politeness for The
Theory of Algebraic Datatypes” by Ying Sheng, Yoni Zohar, Christophe Ringeissen,
Jane Lange, Pascal Fontaine, and Clark Barrett, and “The Resolution of Keller’s
Conjecture” by Joshua Brakensiek, Marijn Heule, John Mackey, and David Narvaez.
IJCAR acknowledges the generous sponsorship of the CNRS (French National
Centre for Scientiﬁc Research), Inria (French Institute for Research in Computer
Science and Automation), the Northern Paris Computer Science (LIPN: Laboratoire
d’Informatique de Paris Nord) at the University of Paris North (Université Sorbonne
Paris Nord), and of the Computer Science Laboratory of Ecole Polytechnique (LIX:
Laboratoire d’Informatique de l’École Polytechnique) in the École Polytechnique.
The EasyChair system was extremely useful for the reviewing and selection of
papers, the organization of the program, and the creation of this proceedings volume.
The PC chairs also want to thank Springer for their support of this publication.

We would like to thank the organizers of IJCAR, the members of the IJCAR PC,
and the additional reviewers, who provided high-quality reviews, as well as all authors,
speakers, and attendees.
The COVID-19 pandemic had a strong impact on the organization of IJCAR
and signiﬁcantly weighted the burden on authors, reviewers, and organizers. We are
very grateful to all of them for their hard work under such difﬁcult and unusual
circumstances.
April 2020
Nicolas Peltier
Viorica Sofronie-Stokkermans
vi
Preface

Organization
Program Committee Chairs
Nicolas Peltier
Université Grenoble Alpes, CNRS, LIG, France
Viorica
Sofronie-Stokkermans
University of Koblenz-Landau, Germany
Program Committee
Takahito Aoto
Niigata University, Japan
Carlos Areces
Universidad Nacional de Córdoba (FaMAF), Argentina
Jeremy Avigad
Carnegie Mellon University, USA
Franz Baader
TU Dresden, Germany
Peter Baumgartner
CSIRO, Australia
Christoph Benzmüller
Freie Universität Berlin, Germany
Yves Bertot
Inria, France
Armin Biere
Johannes Kepler University Linz, Austria
Nikolaj Bjørner
Microsoft, USA
Jasmin Blanchette
Vrije Universiteit Amsterdam, The Netherlands
Maria Paola Bonacina
Università degli Studi di Verona, Italy
James Brotherston
University College London, UK
Serenella Cerrito
IBISC, Université d'Évry, Université Paris-Saclay,
France
Agata Ciabattoni
Vienna University of Technology, Austria
Koen Claessen
Chalmers University of Technology, Sweden
Leonardo de Moura
Microsoft, USA
Stéphane Demri
LSV, CNRS, ENS Paris-Saclay, France
Gilles Dowek
Inria, ENS Paris-Saclay, France
Marcelo Finger
University of São Paulo, Brazil
Pascal Fontaine
Université de Liège, Belgium
Didier Galmiche
Université de Lorraine, CNRS, LORIA, France
Silvio Ghilardi
Università degli Studi di Milano, Italy
Martin Giese
University of Oslo, Norway
Jürgen Giesl
RWTH Aachen University, Germany
Valentin Goranko
Stockholm University, Sweden
Rajeev Gore
The Australian National University, Australia
Stefan Hetzl
Vienna University of Technology, Austria
Marijn Heule
Carnegie Mellon University, USA
Cezary Kaliszyk
University of Innsbruck, Austria
Deepak Kapur
University of New Mexico, USA
Laura Kovacs
Vienna University of Technology, Austria
Andreas Lochbihler
Digital Asset GmbH, Switzerland

Christopher Lynch
Clarkson University, USA
Assia Mahboubi
Inria, France
Panagiotis Manolios
Northeastern University, USA
Dale Miller
Inria, LIX/Ecole Polytechnique, France
Cláudia Nalon
University of Brasília, Brazil
Tobias Nipkow
Technical University of Munich, Germany
Albert Oliveras
Universitat Politècnica de Catalunya, Spain
Jens Otten
University of Oslo, Norway
Lawrence Paulson
University of Cambridge, UK
Nicolas Peltier
Université Grenoble Alpes, CNRS, LIG, France
Frank Pfenning
Carnegie Mellon University, USA
Andrei Popescu
Middlesex University London, UK
Andrew Reynolds
University of Iowa, USA
Christophe Ringeissen
Inria, France
Christine Rizkallah
The University of New South Wales, Australia
Katsuhiko Sano
Hokkaido University, Japan
Renate A. Schmidt
The University of Manchester, UK
Stephan Schulz
DHBW Stuttgart, Germany
Roberto Sebastiani
University of Trento, Italy
Viorica
Sofronie-Stokkermans
University of Koblenz-Landau, Germany
Matthieu Sozeau
Inria, France
Martin Suda
Czech Technical University in Prague, Czech Republic
Geoff Sutcliffe
University of Miami, USA
Soﬁène Tahar
Concordia University, Canada
Cesare Tinelli
The University of Iowa, USA
Christian Urban
King’s College London, UK
Josef Urban
Czech Technical University in Prague, Czech Republic
Uwe Waldmann
Max Planck Institute for Informatics, Germany
Christoph Weidenbach
Max Planck Institute for Informatics, Germany
Conference Chair
Kaustuv Chaudhuri
Inria, Ecole Polytechnique Paris, France
Workshop Chairs
Giulio Manzonetto
Université Paris-Nord, France
Andrew Reynolds
University of Iowa, USA
IJCAR Steering Committee
Franz Baader
TU Dresden, Germany
Kaustuv Chaudhuri
Inria, Ecole polytechnique Paris, France
Didier Galmiche
Université de Lorraine, CNRS, LORIA, France
Ian Horrocks
University of Oxford, UK
viii
Organization

Jens Otten
University of Oslo, Norway
Lawrence Paulson
University of Cambridge, UK
Pedro Quaresma
University of Coimbra, Portugal
Viorica
Sofronie-Stokkermans
University of Koblenz-Landau, Germany
Christoph Weidenbach
Max Planck Institute for Informatics, Germany
Additional Reviewers
Abdelghany, Mohamed
Abdul Aziz, Mohammad
Ahmad, Waqar
Ahmed, Asad
Alassaf, Ruba
Baanen, Anne
Baldi, Paolo
Balyo, Tomas
Barbosa, Haniel
Barnett, Lee
Bartocci, Ezio
Berardi, Stefano
Bobillo, Fernando
Boﬁll, Miquel
Boutry, Pierre
Boy de La Tour, Thierry
Brain, Martin
Bromberger, Martin
Brown, Chad
Casal, Filipe
Ceylan, Ismail Ilkan
Chaudhuri, Kaustuv
Chen, Joshua
Cialdea Mayer, Marta
Cohen, Cyril
D’Agostino, Marcello
Dahmen, Sander
Dalmonte, Tiziano
Dang, Thao
Dawson, Jeremy
De, Michael
Del-Pinto, Warren
Dumbrava, Stefania
Eberl, Manuel
Ebner, Gabriel
Echenim, Mnacho
Egly, Uwe
Esen, Zafer
Fleury, Mathias
Fuhs, Carsten
Gammie, Peter
Gauthier, Thibault
Genestier, Guillaume
Gianola, Alessandro
Girlando, Marianna
Graham-Lengrand, Stéphane
Hirokawa, Nao
Holliday, Wesley
Hustadt, Ullrich
Jaroschek, Maximilian
Kaminski, Benjamin Lucien
Kappé, Tobias
Ketabii, Milad
Kiesel, Rafael
Kim, Dohan
Kocsis, Zoltan A.
Kohlhase, Michael
Konev, Boris
Koopmann, Patrick
Korovin, Konstantin
Lammich, Peter
Lange, Martin
Larchey-Wendling, Dominique
Leitsch, Alexander
Lellmann, Bjoern
Lewis, Robert Y.
Li, Wenda
Lin, Hai
Litak, Tadeusz
Liu, Qinghua
Organization
ix

Lyon, Tim
Magaud, Nicolas
Mahmoud, Mohamed Yousri
Manighetti, Matteo
Mansutti, Alessio
Marti, Johannes
Martins, Ruben
Mazzullo, Andrea
McKeown, John
Mclaughlin, Craig
Metcalfe, George
Mineshima, Koji
Méry, Daniel
Nagashima, Yutaka
Negri, Sara
Nishida, Naoki
Norrish, Michael
Nummelin, Visa
Olarte, Carlos
Omori, Hitoshi
Overbeek, Roy
Pattinson, Dirk
Peñaloza, Rafael
Preiner, Mathias
Preto, Sandro
Ramanayake, Revantha
Rashid, Adnan
Robillard, Simon
Roux, Pierre
Roveri, Marco
Rowe, Reuben
Sakr, Mostafa
Samadpour Motalebi, Kiana
Sato, Haruhiko
Seﬁdgar, Seyed Reza
Thiemann, René
Théry, Laurent
Toluhi, David
Traytel, Dmitriy
Trevizan, Felipe
van Oostrom, Vincent
Vierling, Jannik
Vigneron, Laurent
Virtema, Jonni
Wahl, Thomas
Wansing, Heinrich
Wassermann, Renata
Weber, Tjark
Wiedijk, Freek
Wies, Thomas
Winkler, Sarah
Wojtylak, Piotr
Wolter, Frank
Yamada, Akihisa
Zamansky, Anna
Zivota, Sebastian
Zohar, Yoni
x
Organization

Abstracts of Invited Talks

Domain-Speciﬁc Reasoning with Satisﬁability
Modulo Theories
Clark Barrett
Department of Computer Science, Stanford University
Abstract. General-purpose automated reasoning approaches such as automated
theorem proving (ATP) and Boolean satisﬁability (SAT) are extremely versatile.
Many interesting and practical problems can be reduced to ATP or SAT
problems. On the other hand, these techniques cannot easily incorporate cus-
tomized domain-speciﬁc reasoning. In contrast, this is one of the strengths of
approaches based on Satisﬁabiilty Modulo Theories (SMT). This talk showcases
recent work in which domain-speciﬁc customization of SMT solvers plays a key
role in making otherwise intractable problems tractable. The domains discussed
include reasoning about neural networks and reasoning about string constraints.

Adventures in Verifying Arithmetic
John Harrison
Amazon Web Services
jrh013@gmail.com
Abstract. I have focused a lot of the applied side of my work over the last 20
years on formal veriﬁcation of arithmetic in some sense. Originally my main
focus was on veriﬁcation of ﬂoating-point algorithms for division, square root,
transcendental functions, etc. More recently my interests have shifted to discrete
arithmetical primitives, large integer arithmetic, and elliptic curve operations. As
well as many contrasts and special problems, there are a number of common
themes running through all this work: the challenges of veriﬁcation at the
unstructured machine-code level or indeed even getting adequate speciﬁcations
for machine instruction sets, the countervailing beneﬁt of generally having clear
and incontrovertible speciﬁcations of the functions themselves, and the value of
special customized decision procedures in making veriﬁcations of this kind
practical. I will give an overview of some of the highlights of this work, as well
as talking in more detail about my current project.

Focusing, Axioms and Synthetic Inference
Rules (Extended Abstract)
Elaine Pimentel
, Sonia Marin, Dale Miller
, Marco Volpe
1 Departamento de Matemática, UFRN, Brazil
elaine.pimentel@gmail.com
2 Department of Computer Science, University College London, UK
s.marin@ucl.ac.uk
3 Inria-Saclay & LIX, Ecole Polytechnique, Palaiseau, France
dale@lix.polytechnique.fr
4 Fortiss, Munich, Germany
emme.volpe@gmail.com
Proving a sequent in sequent-based systems often involves many choices. For example,
at every node of a tree-derivation one could: apply an introduction rule over a non-
atomic formula; apply a structural rule; introduce a lemma; apply initial rules, etc.
Hence, there is a need for discipline in structuring such choices and taming proof-
search. One such discipline is focusing [1].
Focused proof systems combines two devices: polarization and focused rule
application. In classical/intuitionistic ﬁrst order systems, polarized formulas are built
using atomic formulas and polarized versions of logical connectives and constants. The
positive and negative versions of connectives and constants have identical truth con-
ditions but have different inference rules inside the polarized proof systems. For
example, left introduction rules for positive connectives are invertible while left
introduction rules for negative connectives are not necessarily invertible. The polarity
of a non-atomic formula is determined by its top-level connective. Since every
polarized formula is classiﬁed as positive or negative, a polarity to atomic formulas
must also be provided. As it turns out, this assignment of polarity to atomic formulas
can be arbitrary [1].
When focusing on a formula, the focus is transferred to the active formulas in the
premises (focused rule applications). This process goes on in all branches of the
derivation, until: an initial rule/introduction rule on constants is applied (and the
derivation ends at that branch); either the polarity of the focused formula changes or the
side (left/right) of the focus ﬂips (but not both). In this case, focus is released and the
formula is eagerly decomposed into its negative-left, positive-right and/or atomic
subformulas, that are stored in the context. Reading derivations from the root upwards,
this forces a sequent derivation to be organized into focused phases, each of them
corresponding to an application of a synthetic inference rule [2], where the focused
formula is rewritten into (some of) its subformulas.
There is a class of formulas corresponding to particularly interesting synthetic rules:
the bipolars. Bipolars are negative formulas in which polarity can change at most once
among its subformulas. This means that left-focusing on a bipolar A gives rise to

(possibly many) synthetic inference rules having simple shape, with leaves involving
only atomic subformulas of A. We call a synthetic inference rule corresponding to the
bipolar A a bipole for A.
In this talk, we will present a careful study of bipoles, giving a fresh view to an old
problem: how to incorporate inference rules encoding axioms into proof systems for
classical and intuitionistic logics.
We start by considering LKF and LJF [6,7] as the basic focused proof systems for
classical and intuitionistic logics, respectively. In such systems, leafs of focused phases
can be composed of either: (i) a conclusion-sequent of the application of introduction
rule on constants; (ii) a (focused) conclusion-sequent of the application of the initial
rule; (iii) an (unfocused) sequent after the storage of the remaining formulas. As an
example, consider the following ﬁrst order formula, that relates the subset and mem-
bership predicates in set theory:
A ¼ 8yz:ð8x:x 2 y  x 2 zÞ  yz:
Assuming that the predicate  is given negative polarity, in the focused phase given by
(left) focusing on A
the right leaf has shape (ii) while the left one is of the form (iii). The formula between
the + and ‘ is the focus of that sequent.
Observe that it must be the case that yz 2 D (since yz is atomic, negative and
under focus), while x 2 y; x 2 z end-up being stored into contexts. This is not by
chance: restricted to bipoles, leaves of the shape (ii) forces atoms to belong to the
context, while leaves of the shape (iii) adds atoms to the context. This implies that
principal and active formulas in bipoles for A (if any) are atomic formulas. That is:
bipoles can be seen, in a sense, as introduction rules for atoms. For example, the bipolar
above corresponds to the (unpolarized) synthetic rule
which introduces yz from x 2 y and x 2 z, where x is an eigenvariable.
Using such synthetic inference rules is one method for systematically generating
proof systems for axiomatic extensions of classical/intuitionistic logics: focusing on a
bipolar axiom yields a bipole.
A key step in transforming a formula into synthetic inference rules involves
attaching a polarity to atomic formulas and to some logical connectives. Since there are
Γ, x
y
x
z, Δ
Γ
x
y
x
z
Δ
storel, storer
Γ
x.x
y
x
z
Δ ∀r, ⊃r
Γ
x.x
y
x
z
Δ
releaser
Γ
y
z
Δ initl
Γ
( x.x
y
x
z)
y
z
Δ
⊃l
Γ
yz.( x.x
y
x
z)
y
z
Δ ∀l
x
y, Γ
x
z, Δ
Γ
y
z, Δ
.
xvi
E. Pimentel et al.

different choices for assigning polarities, it is possible to produce different synthetic
inference rules for the same formula. Indeed, if in our running example the predicate 
is given positive polarity, the corresponding (unpolarized) synthetic rule is
with x an eigenvariable.
We show that this ﬂexibility allows for the generalization of different approaches
for transforming axioms into sequent rules present in the literature, such as the works in
[4, 5, 11, 12]. In particular, bipolars correspond to (the ﬁrst-order version of) the N 2
class presented in [3], which subsumes the class of geometric axioms studied in
[11,10].
We ﬁnish the talk by showing how to emulate precisely rules for modalities in
labeled modal systems as synthetic connectives [8, 9]. Such tight emulation means that
proof search/proof checking on the focused version of the translated formulas models
exactly proof search/proof checking in the correspondent labeled system. As a result,
we are able to show that we can use focused proofs to precisely emulate modal proofs
whenever Kripke frames are characterized by bipolar properties.
References
1. Andreoli, J.: Logic programming with focusing proofs in linear logic. J. Log. Comput. 2(3),
297–347 (1992)
2. Chaudhuri, K.: Focusing Strategies In The Sequent Calculus Of Synthetic Connectives. In:
Cervesato I., Veith H., Voronkov A. (eds.) LPAR 2008. LNCS, vol 5330, pp. 467–481.
Springer, Heidelberg (2008). https://doi.org/10.1007/978-3-540-89439-1_33
3. Ciabattoni, A., Galatos, N., Terui, K.: From axioms to analytic rules in nonclassical logics.
In: LICS, pp. 229–240 (2008)
4. Ciabattoni, A., Maffezioli, P., Spendier, L.: Hypersequent and labelled calculi for interme-
diate logics. In: Galmiche, D., Larchey-Wendling, D. (eds.) TABLEAUX 2013. LNCS, vol
8123, pp. 81–96. Springer, Heidelberg (2013). https://doi.org/10.1007/978-3-642-40537-2_9
5. Dyckhoff, R., Negri, S.: Geometrisation of ﬁrst-order logic. Bull. Symb. Logic 21(2),
123–163 (2015)
6. Liang, C., Miller, D.: Focusing and polarization in intuitionistic logic. In: Duparc, J.,
Henzinger, T.A. (eds.) CSL 2007. LNCS, vol 4646, pp. 451–465. Springer, Heidelberg
(2007). https://doi.org/10.1007/978-3-540-74915-8_34
7. Liang, C., Miller, D.: Focusing and polarization in linear, intuitionistic, and classical logics.
Theor. Comput. Sci. 410(46), 4747–4768 (2009)
8. Marin, S., Miller, D., Volpe, M.: A focused framework for emulating modal proof systems.
In: Beklemishev, L.D., Demri, S., Maté, A. (eds.) Advances in Modal Logic, vol. 11,
pp. 469–488. College Publications (2016)
9. Miller, D., Volpe, M.: Focused labeled proof systems for modal logic. In: Davis, M.,
Fehnker, A., McIver, A., Voronkov, A. (eds.) LPAR 2015. LNCS, vol. 9450, pp. 266–280.
Springer, Heidelberg (2015). https://doi.org/10.1007/978-3-662-48899-7_19
x
y, Γ
x
z, Δ
y
z, Γ
Δ
Γ
Δ
.
Focusing, Axioms and Synthetic Inference Rules (Extended Abstract)
xvii

10. Negri, S.: Contraction-free sequent calculi for geometric theories with an application to
Barr’s theorem. Arch. Math. Log. 42(4), 389–401 (2003). https://doi.org/10.1007/
s001530100124
11. Simpson, A.K.: The Proof Theory and Semantics of Intuitionistic Modal Logic. Ph.D. thesis,
School of Informatics, University of Edinburgh (1994)
12. Viganò, L.: Labelled Non-Classical Logics. Kluwer Academic Publishers (2000)
xviii
E. Pimentel et al.

Efﬁcient Automated Reasoning About Sets
and Multisets with Cardinality Constraints
Ruzica Piskac
Yale University
ruzica.piskac@yale.edu
Abstract. When reasoning about container data structures that can hold dupli-
cate elements, multisets are the obvious choice for representing the data struc-
ture abstractly. However, the decidability and complexity of constraints on
multisets has been much less studied and understood than for constraints on sets.
In this presentation, we outline an efﬁcient decision procedure for reasoning
about multisets with cardinality constraints. We describe how to translate, in
linear time, multisets constraints to constraints in an extension of quantiﬁer-free
linear integer arithmetic, which we call LIA*. LIA* extends linear integer
arithmetic with unbounded sums over values satisfying a given linear arithmetic
formula. We show how to reduce a LIA* formula to an equisatisﬁable linear
integer arithmetic formula. However, this approach requires an explicit com-
putation of semilinear sets and in practice it scales poorly even on simple
benchmarks. We then describe a recent more efﬁcient approach for checking
satisﬁability of LIA*. The approach is based on the use of under- and
over-approximations of LIA* formulas. This way we avoid the space overhead
and explicitly computing semilinear sets. Finally, we report on our prototype
tool which can efﬁciently reason about sets and multisets formulas with cardi-
nality constraints.
Keywords: Multisets • Cardinality constraints • Linear interger arithmetic.
This work is partially supported by the National Science Foundation under GrantNo. CCF-1553168
and No. CCF-1715387.

Contents – Part I
Invited Paper
Efficient Automated Reasoning About Sets and Multisets
with Cardinality Constraints . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3
Ruzica Piskac
SAT, SMT and QBF
An SMT Theory of Fixed-Point Arithmetic . . . . . . . . . . . . . . . . . . . . . . . .
13
Marek Baranowski, Shaobo He, Mathias Lechner, Thanh Son Nguyen,
and Zvonimir Rakamarić
Covered Clauses Are Not Propagation Redundant . . . . . . . . . . . . . . . . . . . .
32
Lee A. Barnett, David Cerna, and Armin Biere
The Resolution of Keller’s Conjecture . . . . . . . . . . . . . . . . . . . . . . . . . . . .
48
Joshua Brakensiek, Marijn Heule, John Mackey, and David Narváez
How QBF Expansion Makes Strategy Extraction Hard. . . . . . . . . . . . . . . . .
66
Leroy Chew and Judith Clymo
Removing Algebraic Data Types from Constrained Horn Clauses Using
Difference Predicates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
83
Emanuele De Angelis, Fabio Fioravanti, Alberto Pettorossi,
and Maurizio Proietti
Solving Bitvectors with MCSAT: Explanations from Bits and Pieces. . . . . . .
103
Stéphane Graham-Lengrand, Dejan Jovanović, and Bruno Dutertre
Monadic Decomposition in Integer Linear Arithmetic . . . . . . . . . . . . . . . . .
122
Matthew Hague, Anthony W. Lin, Philipp Rümmer, and Zhilin Wu
Scalable Algorithms for Abduction via Enumerative
Syntax-Guided Synthesis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
141
Andrew Reynolds, Haniel Barbosa, Daniel Larraz, and Cesare Tinelli
Decision Procedures and Combination of Theories
Deciding the Word Problem for Ground Identities with Commutative
and Extensional Symbols . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
163
Franz Baader and Deepak Kapur

Combined Covers and Beth Definability . . . . . . . . . . . . . . . . . . . . . . . . . .
181
Diego Calvanese, Silvio Ghilardi, Alessandro Gianola, Marco Montali,
and Andrey Rivkin
Deciding Simple Infinity Axiom Sets with One Binary Relation by Means
of Superpostulates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
201
Timm Lampert and Anderson Nakano
A Decision Procedure for String to Code Point Conversion . . . . . . . . . . . . .
218
Andrew Reynolds, Andres Nötzli, Clark Barrett, and Cesare Tinelli
Politeness for the Theory of Algebraic Datatypes . . . . . . . . . . . . . . . . . . . .
238
Ying Sheng, Yoni Zohar, Christophe Ringeissen, Jane Lange,
Pascal Fontaine, and Clark Barrett
Superposition
A Knuth-Bendix-Like Ordering for Orienting Combinator Equations . . . . . . .
259
Ahmed Bhayat and Giles Reger
A Combinator-Based Superposition Calculus for Higher-Order Logic . . . . . .
278
Ahmed Bhayat and Giles Reger
Subsumption Demodulation in First-Order Theorem Proving . . . . . . . . . . . .
297
Bernhard Gleiss, Laura Kovács, and Jakob Rath
A Comprehensive Framework for Saturation Theorem Proving . . . . . . . . . . .
316
Uwe Waldmann, Sophie Tourret, Simon Robillard,
and Jasmin Blanchette
Proof Procedures
Possible Models Computation and Revision – A Practical Approach . . . . . . .
337
Peter Baumgartner
SGGS Decision Procedures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
356
Maria Paola Bonacina and Sarah Winkler
Integrating Induction and Coinduction via Closure Operators
and Proof Cycles . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
375
Liron Cohen and Reuben N. S. Rowe
Logic-Independent Proof Search in Logical Frameworks (Short Paper). . . . . .
395
Michael Kohlhase, Florian Rabe, Claudio Sacerdoti Coen,
and Jan Frederik Schaefer
Layered Clause Selection for Theory Reasoning (Short Paper) . . . . . . . . . . .
402
Bernhard Gleiss and Martin Suda
xxii
Contents – Part I

Non Classical Logics
Description Logics with Concrete Domains and General Concept
Inclusions Revisited. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
413
Franz Baader and Jakub Rydval
A Formally Verified, Optimized Monitor for Metric First-Order
Dynamic Logic. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
432
David Basin, Thibault Dardinier, Lukas Heimes, Srđan Krstić,
Martin Raszyk, Joshua Schneider, and Dmitriy Traytel
Constructive Hybrid Games . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
454
Rose Bohrer and André Platzer
Formalizing a Seligman-Style Tableau System for Hybrid Logic
(Short Paper) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
474
Asta Halkjær From, Patrick Blackburn, and Jørgen Villadsen
NP Reasoning in the Monotone l-Calculus . . . . . . . . . . . . . . . . . . . . . . . .
482
Daniel Hausmann and Lutz Schröder
Soft Subexponentials and Multiplexing . . . . . . . . . . . . . . . . . . . . . . . . . . .
500
Max Kanovich, Stepan Kuznetsov, Vivek Nigam, and Andre Scedrov
Mechanised Modal Model Theory. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
518
Yiming Xu and Michael Norrish
Author Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
535
Contents – Part I
xxiii

Contents – Part II
Interactive Theorem Proving/HOL
Competing Inheritance Paths in Dependent Type Theory: A Case Study in
Functional Analysis. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3
Reynald Affeldt, Cyril Cohen, Marie Kerjean, Assia Mahboubi,
Damien Rouhling, and Kazuhiko Sakaguchi
A Lean Tactic for Normalising Ring Expressions with Exponents
(Short Paper) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
21
Anne Baanen
Practical Proof Search for Coq by Type Inhabitation . . . . . . . . . . . . . . . . . .
28
Łukasz Czajka
Quotients of Bounded Natural Functors . . . . . . . . . . . . . . . . . . . . . . . . . . .
58
Basil Fürer, Andreas Lochbihler, Joshua Schneider, and Dmitriy Traytel
Trakhtenbrot’s Theorem in Coq: A Constructive Approach to Finite Model
Theory. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
79
Dominik Kirst and Dominique Larchey-Wendling
Deep Generation of Coq Lemma Names Using Elaborated Terms . . . . . . . . .
97
Pengyu Nie, Karl Palmskog, Junyi Jessy Li, and Milos Gligoric
Extensible Extraction of Efficient Imperative Programs with Foreign
Functions, Manually Managed Memory, and Proofs . . . . . . . . . . . . . . . . . .
119
Clément Pit-Claudel, Peng Wang, Benjamin Delaware, Jason Gross,
and Adam Chlipala
Validating Mathematical Structures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
138
Kazuhiko Sakaguchi
Teaching Automated Theorem Proving by Example: PyRes 1.2
(System Description) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
158
Stephan Schulz and Adam Pease
Beyond Notations: Hygienic Macro Expansion for Theorem
Proving Languages . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
167
Sebastian Ullrich and Leonardo de Moura

Formalizations
Formalizing the Face Lattice of Polyhedra . . . . . . . . . . . . . . . . . . . . . . . . .
185
Xavier Allamigeon, Ricardo D. Katz, and Pierre-Yves Strub
Algebraically Closed Fields in Isabelle/HOL. . . . . . . . . . . . . . . . . . . . . . . .
204
Paulo Emílio de Vilhena and Lawrence C. Paulson
Formalization of Forcing in Isabelle/ZF . . . . . . . . . . . . . . . . . . . . . . . . . . .
221
Emmanuel Gunther, Miguel Pagano, and Pedro Sánchez Terraf
Reasoning About Algebraic Structures with Implicit Carriers
in Isabelle/HOL . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
236
Walter Guttmann
Formal Proof of the Group Law for Edwards Elliptic Curves . . . . . . . . . . . .
254
Thomas Hales and Rodrigo Raya
Verifying Faradžev-Read Type Isomorph-Free Exhaustive Generation . . . . . .
270
Filip Marić
Verification
Verified Approximation Algorithms. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
291
Robin Eßmann, Tobias Nipkow, and Simon Robillard
Efficient Verified Implementation of Introsort and Pdqsort . . . . . . . . . . . . . .
307
Peter Lammich
A Fast Verified Liveness Analysis in SSA Form. . . . . . . . . . . . . . . . . . . . .
324
Jean-Christophe Léchenet, Sandrine Blazy, and David Pichardie
Verification of Closest Pair of Points Algorithms . . . . . . . . . . . . . . . . . . . .
341
Martin Rau and Tobias Nipkow
Reasoning Systems and Tools
A Polymorphic Vampire (Short Paper). . . . . . . . . . . . . . . . . . . . . . . . . . . .
361
Ahmed Bhayat and Giles Reger
N-PAT: A Nested Model-Checker (System Description). . . . . . . . . . . . . . . .
369
Hadrien Bride, Cheng-Hao Cai, Jin Song Dong, Rajeev Gore, Zhé Hóu,
Brendan Mahony, and Jim McCarthy
HYPNO: Theorem Proving with Hypersequent Calculi for Non-normal
Modal Logics (System Description). . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
378
Tiziano Dalmonte, Nicola Olivetti, and Gian Luca Pozzato
xxvi
Contents – Part II

Implementing Superposition in iProver (System Description) . . . . . . . . . . . .
388
André Duarte and Konstantin Korovin
MOIN: A Nested Sequent Theorem Prover for Intuitionistic Modal Logics
(System Description) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
398
Marianna Girlando and Lutz Straßburger
Make E Smart Again (Short Paper) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
408
Zarathustra Amadeus Goertzel
Automatically Proving and Disproving Feasibility Conditions . . . . . . . . . . . .
416
Raúl Gutiérrez and Salvador Lucas
MU-TERM: Verify Termination Properties Automatically
(System Description) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
436
Raúl Gutiérrez and Salvador Lucas
ENIGMA Anonymous: Symbol-Independent Inference Guiding Machine
(System Description) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
448
Jan Jakubův, Karel Chvalovský, Miroslav Olšák, Bartosz Piotrowski,
Martin Suda, and Josef Urban
The Imandra Automated Reasoning System (System Description) . . . . . . . . .
464
Grant Passmore, Simon Cruanes, Denis Ignatovich, Dave Aitken,
Matt Bray, Elijah Kagan, Kostya Kanishev, Ewen Maclean,
and Nicola Mometto
A Programmer’s Text Editor for a Logical Theory: The SUMOjEdit Editor
(System Description) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
472
Adam Pease
Sequoia: A Playground for Logicians (System Description) . . . . . . . . . . . . .
480
Giselle Reis, Zan Naeem, and Mohammed Hashim
Prolog Technology Reinforcement Learning Prover (System Description). . . .
489
Zsolt Zombori, Josef Urban, and Chad E. Brown
Author Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
509
Contents – Part II
xxvii

Invited Paper

Eﬃcient Automated Reasoning About
Sets and Multisets with Cardinality
Constraints
Ruzica Piskac(B)
Yale University, New Haven, USA
ruzica.piskac@yale.edu
Abstract. When reasoning about container data structures that can
hold duplicate elements, multisets are the obvious choice for representing
the data structure abstractly. However, the decidability and complexity
of constraints on multisets has been much less studied and understood
than for constraints on sets. In this presentation, we outline an eﬃ-
cient decision procedure for reasoning about multisets with cardinality
constraints. We describe how to translate, in linear time, multisets con-
straints to constraints in an extension of quantiﬁer-free linear integer
arithmetic, which we call LIA*. LIA* extends linear integer arithmetic
with unbounded sums over values satisfying a given linear arithmetic for-
mula. We show how to reduce a LIA* formula to an equisatisﬁable linear
integer arithmetic formula. However, this approach requires an explicit
computation of semilinear sets and in practice it scales poorly even on
simple benchmarks. We then describe a recent more eﬃcient approach
for checking satisﬁability of LIA*. The approach is based on the use of
under- and over-approximations of LIA* formulas. This way we avoid
the space overhead and explicitly computing semilinear sets. Finally, we
report on our prototype tool which can eﬃciently reason about sets and
multisets formulas with cardinality constraints.
Keywords: Multisets · Cardinality constraints · Linear interger
arithmetic
1
Introduction
In the veriﬁcation of container data structures one often needs to reason about
sets of objects – for example, abstracting the content of a container data structure
as a set. The need for cardinality constraints naturally arises in order to reason
about the number of the elements in the data structure. We have all witnessed to
the success of the BAPA logic [4,5] that was, among others, used for veriﬁcation
of distributed algorithms [1].
This work is partially supported by the National Science Foundation under Grant No.
CCF-1553168 and No. CCF-1715387.
c
⃝Springer Nature Switzerland AG 2020
N. Peltier and V. Sofronie-Stokkermans (Eds.): IJCAR 2020, LNAI 12166, pp. 3–10, 2020.
https://doi.org/10.1007/978-3-030-51074-9_1

4
R. Piskac
Similarly, when reasoning about container data structures that can hold
duplicate elements, multisets are the obvious choice of an abstraction. Multisets
are collections of objects where an element can occur several times. They can be
seen as “sets with counting”. Although multisets are interesting mathematical
objects that can be widely used in veriﬁcation, there was no eﬃcient reasoner for
multisets and sets with cardinality constraints until recently [6]. Moreover, for a
long time it was not known if the logic of multisets with cardinality constraints
is even decidable [7]. Nevertheless, researchers have recognized the importance
of this logic and they have been studied multisets in combination with other
theories.
Zarba [13] investigated decision procedures for quantiﬁer-free multisets but
without the cardinality operator. He showed how to reduce a multiset formula
to a quantiﬁer-free deﬁning each multiset operation pointwise on the elements
of the set. Adding the cardinality operator makes such a reduction impossible.
Lugiez studied multiset constraints in the context of a more general result
on multitree automata [7] and proved the decidability of multiset formulas with
a weaker form of cardinality operator that counts only distinct elements in a
multiset.
1.1
Multisets with Cardinality Constraints
In this paper we revive the ﬁrst decision procedure for multisets with cardi-
nality constraints [9,10]. We represent multisets (bags) with their characteristic
functions. A multiset m is a function E →N, where E is the universe used for
populating multisets and N is the set of non-negative integers. The value m(e) is
the multiplicity (the number of occurrences) of an element e in a multiset m. We
assume that the domain E is ﬁxed and ﬁnite but of an unknown size. We consider
the logic of multisets constraints with the cardinality operator (MAPA), given
in Fig. 1. An atom in MAPA is either a multiset comparison, or it is a stan-
dard quantiﬁer-free linear integer arithmetic atom, or it is a quantiﬁed formula
(∀e.Fin), or it is a collecting sum formula. We allow only universal quantiﬁcation
over all elements of E. This way we can express, for example, that for a multiset
k it holds ∀e.k(e) = 0 ∨k(e) = 1 – in other words, k is a set. A collecting sum
atom is used to group several formulas involving sums into a single atom. This
is needed for the next step of the decision procedure. The sums are used in the
deﬁnition of the cardinality operator:
|m| =

e∈E
m(e)
Piskac and Kuncak [9] showed that every MAPA formula can be translated
to an equisatisﬁable LIA⋆formula. The translation is linear and described in [9].
This way reasoning about MAPA formulas reduces to reasoning about LIA⋆
formulas.

Eﬃcient Automated Reasoning About Sets and Multisets
5
F ::= A | F ∧F | ¬F
A ::= M=M | M ⊆M | ∀e.Fin | Aout
outer linear arithmetic formulas:
Fout ::= Aout | Fout ∧Fout | ¬Fout
Aout ::= tout ≤tout | tout=tout | (tout, . . . , tout)= 
Fin(tin, . . . , tin)
tout ::= k | |M| | C | tout + tout | C · tout | ite(Fout, tout, tout)
inner linear arithmetic formulas:
Fin ::= Ain | Fin ∧Fin | ¬Fin
Ain ::= tin ≤tin | tin=tin
tin ::= m(e) | P | tin + tin | P · tin | ite(Fin, tin, tin)
multiset expressions:
M ::= m | ∅| M ∩M | M ∪M | M ⊎M | M \ M | M \\ M | set(M)
terminals:
m - multiset variables; e - index variable (ﬁxed)
k - integer variable; C - integer constant; P - non-negative integer constant
top-level formulas:
Fig. 1. The logic of multiset constraints with Presburger Arithmetic (MAPA)
1.2
Reasoning About LIA⋆Formulas
The LIA⋆logic [10] is a standard linear integer arithmetic extended with a new
operator: the star operator, which is deﬁned over a set of integer vectors as
follows:
S⋆
≜
 n

i=1
si
| ∀i.1 ≤i ≤n. si ∈S

(1)
The result of the star operator applied to set S is a set if all linear additive
combinations of vectors from S. Its syntax is given in Fig. 2.
LIA⋆formulas: ϕ ::= F1 ∧x1 ∈{x2 | F2}⋆
such that dim(x1) = dim(x2) and free-vars(F2) ⊆x2
LIA formulas:
F ::= A | F1 ∧F2 | F1 ∨F2 | ¬F1 | ∃x. F | ∀x. F
A ::= T1 ≤T2 | T1 = T2
T ::= x | C | T1 + T2 | C · T1 | ite(F, T1, T2)
terminals: x - integer variable; C - integer constant
Fig. 2. Linear integer arithmetic (LIA) and an extension with the Star Operator.
To check a satisﬁability of a LIA⋆formula, we use the semilinear set charac-
terization of solutions of integer linear arithmetic formulas.

6
R. Piskac
Deﬁnition 1 (Semilinear sets). A linear set LS(a, B) is deﬁned by an integer
vector a and a ﬁnite set of integer vectors B = {b1, . . . , bn}, all of the same
dimension, as follows:
LS(a, B)
≜

a +
n

i=1
λibi
|
n

i=1
λi ≥0

(2)
Sometimes, as a shorthand, we use λB = n
i=1 λibi.
A semilinear set SLS(ls1, . . . , lsn) is a ﬁnite union of linear sets ls1, . . . , lsn,
i.e., SLS(ls1, . . . , lsn) = n
i=1 lsi.
Ginsburg and Spanier showed (Theorem 1.3 in [3]) that a solution set for
every Presburger arithmetic formula is a semilinear set, and we use that result
to eliminate the star operator.
Theorem 1 (Lemmas 2 and 3 in [10]). Given a LIA⋆atom x1 ∈{x2 | F2}⋆,
let SLS(LS(a1, B1), . . . , LS(ak, Bk)) be a semilinear set describing the set of
the solutions of formula F2. The atom x1 ∈{x2 | F2}⋆is equisatisﬁable to the
following LIA formula:
∃μ1 ≥0, . . . , μk ≥0, λ1 ≥0, . . . λk ≥0.
x1 =
k

i=1
μiai + λiBi ∧
k
i=1
(μi = 0 →λi = 0)
By applying Theorem 1, checking satisﬁability of a LIA⋆formula reduces to
reasoning about linear integer arithmetic. Note, however, that this approach
results in automatically constructing a formula might be really large, depending
on the size of a semilinear set. In addition, this approach relies on computing
semilinear sets explicitly, both of which make it scale poorly even on simple
benchmarks.
2
Illustrating Example
We illustrate now how is a decision procedure for MAPA working on the following
simple multiset formula: for two multisets X and Y , the size of their disjoint
union is the sum of their respective sizes. In other words, we need to prove the
validity of the following formula
|X ⊎Y | = |X| + |Y |
As usual, we prove the unsatisﬁability of the formula |X ⊎Y | ̸= |X| + |Y |.
The ﬁrst step is to reduce this formula into an equisatisﬁable LIA⋆formula. To
do that, we perform a sequence of steps that resemble the puriﬁcation step in
the Nelson-Oppen combination procedure [8]. In a nutshell, we introduce a new
variable for every non-terminal expression.

Eﬃcient Automated Reasoning About Sets and Multisets
7
We ﬁrst introduce a multiset variable M deﬁning multiset expression X ⊎Y
and then we introduce integer variables k1, k2, k3 for each of the cardinality
expressions. This way the formula becomes:
k1 ̸= k2 + k3 ∧k1 = |M| ∧k2 = |X| ∧k3 = |Y | ∧M = X ⊎Y
We next apply the point-wise deﬁnitions of the cardinality and ⊎operators
and we obtain the following formula:
k1 ̸= k2 + k3 ∧k1 =

e∈E
M(e) ∧k2 =

e∈E
X(e) ∧k3 =

e∈E
Y (e)
∧∀e.M(e) = X(e) + Y (e)
Grouping all the sum expressions together results in the formula:
k1 ̸= k2 + k3 ∧(k1, k2, k3) =

e∈E
(M(e), X(e), Y (e)) ∧∀e.M(e) = X(e) + Y (e)
Piskac and Kuncak have shown in [9] that every multiset formula can be
reduced to this form. They call it the sum normal form. It consists of three
conjuncts. One is a pure LIA formula, the other is the summation and the third
part is a universally quantiﬁed formula. By applying Theorem 2 from [9], the
above MAPA formula is translated into an equisatisﬁable LIA⋆formula, where
m, x and y are non-negative integer variables:
k1 ̸= k2 + k3 ∧(k1, k2, k3) ∈{(m, x, y)|m = x + y}⋆
To check the satisﬁability of this formula, we ﬁrst need to eliminate the star
operator, which is done by computing a semilinear set describing the set of solu-
tions of m = x + y. In this particular case, the semilinear set is actually a linear
set, consisting of the zero vector and two vectors deﬁning linear combinations:
{(m, x, y)|m = x + y} = LS((0, 0, 0), {(1, 1, 0), (1, 0, 1)})
Having the semilinear set representation, we can apply Theorem 1. In partic-
ular, only one linear set and the zero vector can signiﬁcantly simplify the corre-
sponding equisatisﬁable formula. As the result of applying Theorem 1, we obtain
that formula (k1, k2, k3) ∈{(m, x, y)|m = x+y}⋆is equisatisﬁable to the formula
(k1, k2, k3) = λ{(1, 1, 0), (1, 0, 1)} ⇔(k1, k2, k3) = λ1(1, 1, 0) + λ2(1, 0, 1).
This way we have eliminated the star operator from the given LIA⋆formula.
It is now reduced to an equisatisﬁable linear integer arithmetic formula:
k1 ̸= k2 + k3 ∧k1 = λ1 + λ2 ∧k2 = λ1 ∧k3 = λ2
The resulting LIA formula is unsatisﬁable.

8
R. Piskac
3
Eﬃcient Reasoning About LIA⋆Formulas
The described decision procedure is sound and complete. However, its crucial
component is a computation of semilinear sets. While it is possible to com-
pute Hilbert basis using the z3 [2] SMT solver, to the best of our knowledge
there are no eﬃcient tools for computing semilinear sets. Moreover, Pottier [12]
showed that a semilinear set might contain an exponential number of vectors.
To overcome the explicit computation of semilinear sets, Piskac and Kuncak [10]
developed a new decision procedure for LIA⋆which eliminates the star oper-
ator from the atom x1 ∈{x2 | F}⋆by showing that x1 is a linear combina-
tion of O(n2 log n) solution vectors of F, where n is the size of the input for-
mula. Although this new decision procedure avoids computing semilinear sets,
it instantly produces a very large formula that could not be solved in practice
by existing tools, not even for the most simple benchmarks.
Levatich et al. [6] used those insights to develop a new eﬃcient and scal-
able approach for solving LIA⋆formulas. The approach is based on the use of
under- and over-approximations of LIA⋆formulas. This way one avoids the space
overhead and explicitly computing semilinear sets.
The key insight of their approach is to construct a solution or a proof of unsat-
isﬁability “on demand”. Given a LIA⋆formula F1(x1) ∧x1 ∈{x2 | F2(x2)}⋆,
we ﬁrst ﬁnd any solution vector for formula F2, let us name it u1. We next
check if formula F1(x1) ∧x1 = λ1 ∗u1 is satisﬁable. If this is the case, the given
LIA⋆formula is satisﬁable as well. However, if this is not the case, we cannot
conclude anything about the satisiﬁability of the given LIA⋆formula, so we ﬁnd
a new diﬀerent solution of formula F2, denoted by u2: F2(u2) ∧u1 ̸= u2. Next,
we check if the vector x1 is a linear combination of those two solution vectors:
F1(x1) ∧x1 = λ1 ∗u1 + λ2 ∗u2. If this newly constructed formula is satisﬁable,
so is the original LIA⋆formula, otherwise we repeat the process. This way, by
ﬁnding and checking solution vectors of F2, we construct underapproximations
of the set {x2 | F2(x2)}⋆. Moreover, we know that this process will terminate
once we check suﬃciently many solution vectors, as shown in [10].
However, if the given LIA⋆formula is unsatisﬁable, this approach will result
in an equally large formula as in [10], and again it does not scale. Therefore, in
parallel to ﬁnding an under-approximation of the set {x2 | F2(x2)}⋆, we are also
constructing a sequence of its over-approximation. The properties, that such an
overapproximation should have, are encoded as a set of Constraint Horn Clauses
and we use existing solvers to compute them. Such an overapproximation, if
exists, is an interpolant that separates two conjuncts in the given LIA⋆formula,
proving this way that the formula is unsatisﬁable.
Finally, we have implemented the presented decision procedure and the tool
is publicly available at https://github.com/mlevatich/sls-reachability. Because
there were no MAPA benchmarks available, we had to create our own bench-
marks. In addition, we also treated 240 BAPA benchmarks about sets, available
in [1], as MAPA benchmarks. While the full report on the empirical results is
available in [6], our general assessment is that the presented algorithm is eﬀec-
tive on both SAT and UNSAT benchmarks. Our tool solved 83% of benchmarks

Eﬃcient Automated Reasoning About Sets and Multisets
9
in less than 50 seconds, and over 75% of those in under 3 seconds. We believe
that this tool is the ﬁrst eﬃcient reasoner for multisets and sets with cardinality
constraints.
4
Conclusions
The presented work describes a sequence of decision procedures that has lead
towards an eﬃcient reasoner for multisets and sets with cardinality constraints.
We noticed that some constraints arising in formal veriﬁcation of protocols and
data structures could have been expressed more succinctly and naturally, were
they using multisets as the underlying abstract datatype in the speciﬁcation.
Nevertheless, due to the lack of tool support they use sets, resulting in more
complex constraints. While there was an older tool for reasoning about multisets
with cardinality constraints [11], that tool served mainly as a proof of concept
and was evaluated only on a handful of manually written formulas. We here
presented a recent tool for reasoning about sets and multisets and we showed
empirically that this tool scales well and can easily reason about complex mul-
tiset formulas. We hope that this work will lead to a renewed research interest
in multisets and encourage their use in software analysis and veriﬁcation.
Acknowledgments. This presentation is based on the previously published results
on reasoning about multisets with cardinality constraints [6,9–11]. We sincerely thank
the collaborators on these projects: Nikolaj Bjørner, Maxwell Levatich, Viktor Kunˇcak
and Sharon Shoham, without whom this work would not be possible.
References
1. Berkovits, I., Lazi´c, M., Losa, G., Padon, O., Shoham, S.: Veriﬁcation of threshold-
based distributed algorithms by decomposition to decidable logics. In: Dillig, I.,
Tasiran, S. (eds.) CAV 2019. LNCS, vol. 11562, pp. 245–266. Springer, Cham
(2019). https://doi.org/10.1007/978-3-030-25543-5 15
2. de Moura, L.M., Bjørner, N.: Z3: an eﬃcient SMT solver. In: Ramakrishnan, C.R.,
Rehof, J. (eds.) TACAS 2008. LNCS, vol. 4963, pp. 337–340. Springer, Heidelberg
(2008). https://doi.org/10.1007/978-3-540-78800-3 24
3. Ginsburg, S., Spanier, E.H.: Semigroups, Presburger formulas, and languages.
Paciﬁc J. Math. 16(2), 285–296 (1966)
4. Kuncak, V., Nguyen, H.H., Rinard, M.C.: An algorithm for deciding BAPA:
Boolean algebra with Presburger arithmetic. In: Nieuwenhuis, R. (ed.) CADE 2005.
LNCS (LNAI), vol. 3632, pp. 260–277. Springer, Heidelberg (2005). https://doi.
org/10.1007/11532231 20
5. Kuncak, V., Nguyen, H.H., Rinard, M.C.: Deciding Boolean algebra with Pres-
burger arithmetic. J. Autom. Reason. 36(3), 213–239 (2006)
6. Levatich, M., Bjørner, N., Piskac, R., Shoham, S.: Solving LIA⋆using approxi-
mations. In: Beyer, D., Zuﬀerey, D. (eds.) VMCAI 2020. LNCS, vol. 11990, pp.
360–378. Springer, Cham (2020). https://doi.org/10.1007/978-3-030-39322-9 17
7. Lugiez, D.: Multitree automata that count. Theor. Comput. Sci. 333(1–2), 225–263
(2005)

10
R. Piskac
8. Nelson, G., Oppen, D.C.: Fast decision procedures based on congruence closure. J.
ACM 27(2), 356–364 (1980)
9. Piskac, R., Kuncak, V.: Decision procedures for multisets with cardinality con-
straints. In: Logozzo, F., Peled, D.A., Zuck, L.D. (eds.) VMCAI 2008. LNCS,
vol. 4905, pp. 218–232. Springer, Heidelberg (2008). https://doi.org/10.1007/978-
3-540-78163-9 20
10. Piskac, R., Kuncak, V.: Linear arithmetic with stars. In: Gupta, A., Malik, S. (eds.)
CAV 2008. LNCS, vol. 5123, pp. 268–280. Springer, Heidelberg (2008). https://doi.
org/10.1007/978-3-540-70545-1 25
11. Piskac, R., Kuncak, V.: MUNCH - automated reasoner for sets and multisets. In:
Giesl, J., H¨ahnle, R. (eds.) IJCAR 2010. LNCS (LNAI), vol. 6173, pp. 149–155.
Springer, Heidelberg (2010). https://doi.org/10.1007/978-3-642-14203-1 13
12. Pottier, L.: Minimal solutions of linear diophantine systems: bounds and algo-
rithms. In: Book, R.V. (ed.) RTA 1991. LNCS, vol. 488, pp. 162–173. Springer,
Heidelberg (1991). https://doi.org/10.1007/3-540-53904-2 94
13. Zarba, C.G.: Combining multisets with integers. In: Voronkov, A. (ed.) CADE-18.
LNCS (LNAI), vol. 2392, pp. 363–376. Springer, Heidelberg (2002). https://doi.
org/10.1007/3-540-45620-1 30

SAT, SMT and QBF

An SMT Theory of Fixed-Point Arithmetic
Marek Baranowski1, Shaobo He1(B), Mathias Lechner2, Thanh Son Nguyen1,
and Zvonimir Rakamarić1
1 School of Computing, University of Utah, Salt Lake City, UT, USA
{baranows,shaobo,thanhson,zvonimir}@cs.utah.edu
2 IST Austria, Klosterneuburg, Austria
mathias.lechner@ist.ac.at
Abstract. Fixed-point arithmetic is a popular alternative to ﬂoating-
point arithmetic on embedded systems. Existing work on the veriﬁcation
of ﬁxed-point programs relies on custom formalizations of ﬁxed-point
arithmetic, which makes it hard to compare the described techniques
or reuse the implementations. In this paper, we address this issue by
proposing and formalizing an SMT theory of ﬁxed-point arithmetic. We
present an intuitive yet comprehensive syntax of the ﬁxed-point theory,
and provide formal semantics for it based on rational arithmetic. We also
describe two decision procedures for this theory: one based on the theory
of bit-vectors and the other on the theory of reals. We implement the
two decision procedures, and evaluate our implementations using exist-
ing mature SMT solvers on a benchmark suite we created. Finally, we
perform a case study of using the theory we propose to verify properties
of quantized neural networks.
Keywords: SMT · Fixed-point arithmetic · Decision procedure
1
Introduction
Algorithms based on real arithmetic have become prevalent. For example, the
mathematical models in machine learning algorithms operate on real numbers.
Similarly, signal processing algorithms often implemented on embedded sys-
tems (e.g., fast Fourier transform) are almost always deﬁned over real numbers.
However, real arithmetic is not implementable on computer systems due to its
unlimited precision. Consequently, we use implementable approximations of real
arithmetic, such as ﬂoating-point and ﬁxed-point arithmetic, to realize these
algorithms in practice.
Floating-point arithmetic is the dominant approximation of real arithmetic
that has mature hardware support. Although it enjoys the beneﬁts of being able
to represent a large spectrum of real numbers and high precision of arithmetic
This work was supported in part by NSF awards CCF 1552975, CCF 1704715, and the
Austrian Science Fund (FWF) under grant Z211-N23 (Wittgenstein Award).
c
⃝Springer Nature Switzerland AG 2020
N. Peltier and V. Sofronie-Stokkermans (Eds.): IJCAR 2020, LNAI 12166, pp. 13–31, 2020.
https://doi.org/10.1007/978-3-030-51074-9_2

14
M. Baranowski et al.
operations over small numbers, ﬂoating-point arithmetic, due to its complex-
ity, can be too expensive in terms of speed and power consumption on cer-
tain platforms. These platforms are often deployed in embedded systems such
as mobile phones, video game consoles, and digital controllers. Recently, the
machine learning community revived the interest in ﬁxed-point arithmetic since
popular machine learning algorithms and models can be implemented using (even
very low bit-width) ﬁxed-points with little accuracy loss [11,27,37]. Therefore,
ﬁxed-point arithmetic has been a popular alternative to ﬂoating-point arithmetic
on such platforms since it can be eﬃciently realized using integer arithmetic.
There are several software implementations of ﬁxed-point arithmetic in diﬀer-
ent programming languages [22,28,34]; moreover, some programming languages,
such as Ada and GNU C, have built-in ﬁxed-point types.
While ﬁxed-point arithmetic is less popular in mainstream applications than
ﬂoating-point arithmetic, the systems employing the former are often safety-
critical. For example, ﬁxed-point arithmetic is often used in medical devices,
cars, and robots. Therefore, there is a need for formal methods that can rigor-
ously ensure the correctness of these systems. Although techniques that perform
automated veriﬁcation of ﬁxed-point programs already exist [1,3,15], all of them
implement a custom dedicated decision procedure without formalizing the details
of ﬁxed-point arithmetic. As a result, it is hard to compare these techniques, or
reuse the implemented decision procedures.
On the other hand, ever since the SMT theory of ﬂoating-point numbers
was formalized [8,44] in SMT-LIB [46], there has been a ﬂurry of research in
developing novel and faster decision procedures for the theory [6,7,14,29,35,
50]. Meanwhile, the ﬂoating-point theory has also been used by a number of
approaches that require rigorous reasoning about ﬂoating-point arithmetic [2,
36,39,41]. The published formalization of the theory enables fair comparison
between the decision procedures, sharing of benchmarks, and easy integration of
decision procedures within tools that need this functionality. In this paper, we
propose and formalize an SMT theory of ﬁxed-point arithmetic, in the spirit of
the SMT theory of ﬂoating-point arithmetic, with the hope that it will lead to
similar outcomes and advances.
Contributions. We summarize our main contributions as follows:
– We present an intuitive and comprehensive syntax of ﬁxed-point arithmetic
(Sect. 3) that captures common use cases of ﬁxed-point operations.
– We provide formal semantics of the ﬁxed-point theory based on rational arith-
metic (Sect. 4).
– We propose and implement two decision procedures for the ﬁxed-point theory:
one that leverages the theory of ﬁxed-size bit-vectors and the other the theory
of real numbers (Sect. 5).
– We evaluate the two decision procedures on a set of benchmarks using mature
SMT solvers (Sect. 6), and perform a case study of verifying quantized neural
networks that uses our theory of ﬁxed-point arithmetic (Sect. 7).

An SMT Theory of Fixed-Point Arithmetic
15
2
Background
Fixed-point arithmetic, like ﬂoating-point arithmetic, is used as an approxima-
tion for computations over the reals. Both ﬁxed-point and ﬂoating-point num-
bers (excluding the special values) can be represented using rational numbers.
However, unlike ﬂoating-point numbers, ﬁxed-point numbers in a certain format
maintain a ﬁxed divisor, hence the name ﬁxed-point. Consequently, ﬁxed-point
numbers have a reduced range of values. However, this format allows for custom
precision systems to be implemented eﬃciently in software—ﬁxed-point arith-
metic operations can be implemented in a much smaller amount of integer arith-
metic operations compared to their ﬂoating-point counterparts. For example, a
ﬁxed-point addition operation simply amounts to an integer addition instruc-
tion provided that wrap-around is the intended behavior when overﬂows occur.
This feature gives rise to the popularity of ﬁxed-point arithmetic on embedded
systems where computing resources are fairly constrained.
A ﬁxed-point number is typically interpreted as a fraction whose numerator
is an integer with ﬁxed bit-width in its two’s complement representation and
denominator is a power of 2. Therefore, a ﬁxed-point format is parameterized by
two natural numbers—tb that deﬁnes the bit-width of the numerator and fb that
deﬁnes the power of the denominator. A ﬁxed-point number in this format can be
treated as a bit-vector of length tb that is the two’s complement representation
of the numerator integer and has an implicit binary point between the fb + 1th
and fbth least signiﬁcant bits. We focus on the binary format (as opposed to
decimal, etc.) of ﬁxed-point arithmetic since it is widely adopted in hardware
and software implementations in practice. Moreover, depending on the intended
usage, developers leverage both signed and unsigned ﬁxed-point formats. The
signed or unsigned format determines whether the bit pattern representing the
ﬁxed-point number should be interpreted as a signed or unsigned integer, respec-
tively. Therefore, signed and unsigned ﬁxed-point formats having the same tb
and fb have diﬀerent ranges ([ −2tb−1
2fb
, 2tb−1−1
2fb
] and [0, 2tb−1
2fb ]), respectively.
Fixed-point addition (resp. subtraction) is typically implemented by adding
(resp. subtracting) the two bit-vector operands (i.e., two’s complements),
amounting to a single operation. Because the denominators are the same between
the two operands, we do not need to perform rounding. However, we still have to
take care of potential overﬂows that occur when the result exceeds the allowed
range of the chosen ﬁxed-point format. Fixed-point libraries typically implement
two methods to handle overﬂows: saturation and wrap-around. Saturation entails
ﬁxing overﬂowed results to either the minimal or maximal representable value.
The advantage of this method is that it ensures that the ﬁnal ﬁxed-point result is
the closest to the actual result not limited by ﬁnite precision. Wrap-around allows
for the overﬂowing result to wrap according to two’s complement arithmetic. The
advantage of this method is that it is eﬃcient and can be used to ensure the sum
of a set of (signed) numbers has a correct ﬁnal value despite potential overﬂows

16
M. Baranowski et al.
(if it falls within the supported range). Note that addition is commutative under
both methods, but only addition using the wrap-around method is associative.
The multiplication and division operations are more involved since they have to
include the rounding step as well.
3
Syntax
In this section, we describe the syntax of our proposed theory of ﬁxed-point
arithmetic. It is inspired by the syntax of the SMT theory of ﬂoating-points [8,44]
and the ISO/IEC TR 18037 standard [23].
Fixed-Points. We introduce the indexed SMT nullary sorts (_ SFXP tb fb) to rep-
resent signed ﬁxed-point sorts, where tb is a natural number specifying the total
bit-width of the scaled integer in its two’s complement form and fb is a natural
number specifying the number of fractional bits; tb is greater than or equal to fb.
Similarly, we represent unsigned ﬁxed-point sorts with (_ UFXP tb fb). Following
the SMT-LIB notation, we deﬁne the following two functions for constructing
ﬁxed-points literals:
((_ sfxp fb) (_ BitVec tb) (_ SFXP tb fb))
((_ ufxp fb) (_ BitVec tb) (_ UFXP tb fb))
where (_ sfxp fb) (resp. (_ ufxp fb)) produces a function that takes a bit-vector
(_ BitVec tb) and constructs a ﬁxed-point (_ SFXP tb fb) (resp. (_ UFXP tb fb)).
Rounding Modes. Similarly to the theory of ﬂoating-point arithmetic, we also
introduce the RoundingMode sort (abbreviated as RM) to represent the rounding
mode, which controls the direction of rounding when an arithmetic result cannot
be precisely represented by the speciﬁed ﬁxed-point format. However, unlike the
ﬂoating-point theory that speciﬁes ﬁve diﬀerent rounding modes, we only adopt
two rounding mode constants, namely roundUp and roundDown, as they are
common in practice.
Overﬂow Modes. We introduce the nullary sort OverflowMode (abbreviated as
OM) to capture the behaviors of ﬁxed-point arithmetic when the result of an
operation is beyond the representable range of the used ﬁxed-point format. We
adopt two constants, saturation and wrapAround, to represent the two com-
mon behaviors. The saturation mode rounds any out-of-bound results to the
maximum or minimum values of the representable range, while the wrapAround
mode wraps the results around similar to bit-vector addition.

An SMT Theory of Fixed-Point Arithmetic
17
Comparisons. The following operators return a Boolean by comparing two ﬁxed-
point numbers:
(sfxp.geq (_ SFXP tb fb) (_ SFXP tb fb) Bool)
(ufxp.geq (_ UFXP tb fb) (_ UFXP tb fb) Bool)
(sfxp.gt (_ SFXP tb fb) (_ SFXP tb fb) Bool)
(ufxp.gt (_ UFXP tb fb) (_ UFXP tb fb) Bool)
(sfxp.leq (_ SFXP tb fb) (_ SFXP tb fb) Bool)
(ufxp.leq (_ UFXP tb fb) (_ UFXP tb fb) Bool)
(sfxp.lt (_ SFXP tb fb) (_ SFXP tb fb) Bool)
(ufxp.lt (_ UFXP tb fb) (_ UFXP tb fb) Bool)
Arithmetic. We support the following binary arithmetic operators over ﬁxed-
point sorts parameterized by tb and fb:
(sfxp.add OM (_ SFXP tb fb) (_ SFXP tb fb) (_ SFXP tb fb))
(ufxp.add OM (_ UFXP tb fb) (_ UFXP tb fb) (_ UFXP tb fb))
(sfxp.sub OM (_ SFXP tb fb) (_ SFXP tb fb) (_ SFXP tb fb))
(ufxp.sub OM (_ UFXP tb fb) (_ UFXP tb fb) (_ UFXP tb fb))
(sfxp.mul OM RM (_ SFXP tb fb) (_ SFXP tb fb) (_ SFXP tb fb))
(ufxp.mul OM RM (_ UFXP tb fb) (_ UFXP tb fb) (_ UFXP tb fb))
(sfxp.div OM RM (_ SFXP tb fb) (_ SFXP tb fb) (_ SFXP tb fb))
(ufxp.div OM RM (_ UFXP tb fb) (_ UFXP tb fb) (_ UFXP tb fb))
Note that we force the sorts of operands and return values to be the same.
The addition and subtraction operations never introduce error into computa-
tion according to our semantics in Sect. 4. Hence, these operators do not take a
rounding mode as input like multiplication and division.
Conversions. We introduce two types of conversions between sorts. First, the
conversions between diﬀerent ﬁxed-point sorts:
((_ to_sfxp tb fb) OM RM (_ SFXP tb′ fb′) (_ SFXP tb fb))
((_ to_ufxp tb fb) OM RM (_ UFXP tb′ fb′) (_ UFXP tb fb))
Second, the conversions between the real and ﬁxed-point sorts:
((_ to_sfxp tb fb) OM RM Real (_ SFXP tb fb))
((_ to_ufxp tb fb) OM RM Real (_ UFXP tb fb))
(sfxp.to_real (_ SFXP tb fb) Real)
(ufxp.to_real (_ UFXP tb fb) Real)

18
M. Baranowski et al.
4
Semantics
In this section, we formalize the semantics of the ﬁxed-point theory by treating
ﬁxed-points as rational numbers. We ﬁrst deﬁne ﬁxed-points as indexed subsets
of rationals. Then, we introduce two functions, rounding and overﬂow, that are
crucial for the formalization of the ﬁxed-point arithmetic operations. Finally,
we present the formal semantics of the arithmetic operations based on rational
arithmetic and the two aforementioned functions.
Let Ffb = { n
2fb | n ∈Z} be the inﬁnite set of rationals that can be repre-
sented as ﬁxed-points using fb fractional bits. We interpret a signed ﬁxed-point
sort (_ SFXP tb fb) as the ﬁnite subset Stb,fb = { n
2fb | −2tb−1 ≤n < 2tb−1, n ∈Z}
of Ffb. We interpret an unsigned ﬁxed-point sort (_ UFXP tb fb) as the ﬁnite sub-
set Utb,fb = { n
2fb | 0 ≤n < 2tb, n ∈Z} of Ffb. The rational value of an unsigned
ﬁxed-point constant constructed using (ufxp bv fb) is bv2nat(bv)
2fb
, where function
bv2nat converts a bit-vector to its unsigned integer value. The rational value of
its signed counterpart constructed using (sfxp bv fb) is bv2int(bv)
2fb
, where func-
tion bv2int converts a bit-vector to its signed value. Since we treat ﬁxed-point
numbers as subsets of rational numbers, we interpret ﬁxed-point comparison
operators, such as =, fxp.le, fxp.leq, as simply their corresponding rational
comparison relations, such as =, <, ≤, respectively. To be able to formalize the
semantics of arithmetic operations, we ﬁrst introduce the round and overﬂow
helper functions.
We interpret the rounding mode sort RoundingMode as the set rmode =
{ru, rd}, where roundUp = ru and roundDown = rd. Let rndFfb : rmode×R →
Ffb be a family of round functions parameterized by fb that map a rounding mode
and real number to an element of Ffb. Then, we deﬁne rndFfb as
rndFfb(ru, r) = min({x | x ≥r, x ∈Ffb})
rndFfb(rd, r) = max({x | x ≤r, x ∈Ffb})
We interpret the overﬂow mode sort OverflowMode as the set omode =
{sat, wrap}, where saturation = sat and wrapAround = wrap. Let ovf F :
omode × Ffb →F be a family of overﬂow functions parameterized by F that
map a rounding mode and element of Ffb to an element of F; here, F is either
Stb,fb or Utb,fb depending on whether we are using signed or unsigned ﬁxed-point
numbers, respectively. Then, we deﬁne ovf F as
ovf F(sat, x) =
⎧
⎪
⎨
⎪
⎩
x
if x ∈F
max(F)
if x > max(F)
min(F)
if x < min(F)
ovf F(wrap, x) = y such that y · 2fb ≡x · 2fb (mod 2tb) ∧y ∈F
Note that x · 2fb, y · 2fb ∈Z according to the deﬁnition of F, and also there is
always exactly one y satisfying the constraint.

An SMT Theory of Fixed-Point Arithmetic
19
Now that we introduced our helper round and overﬂow functions, it is easy
to deﬁne the interpretation of ﬁxed-point arithmetic operations:
sfxp.add(om, x1, x2) = ovf Stb,fb(om, x1 + x2)
ufxp.add(om, x1, x2) = ovf Utb,fb(om, x1 + x2)
sfxp.sub(om, x1, x2) = ovf Stb,fb(om, x1 −x2)
ufxp.sub(om, x1, x2) = ovf Utb,fb(om, x1 −x2)
sfxp.mul(om, rm, x1, x2) = ovf Stb,fb(om, rndFfb(rm, x1 · x2))
ufxp.mul(om, rm, x1, x2) = ovf Utb,fb(om, rndFfb(rm, x1 · x2))
sfxp.div(om, rm, x1, x2) = ovf Stb,fb(om, rndFfb(rm, x1/x2))
ufxp.div(om, rm, x1, x2) = ovf Utb,fb(om, rndFfb(rm, x1/x2))
Note that it trivially holds that ∀x1, x2 ∈Ffb . x1 + x2 ∈Ffb ∧x1 −x2 ∈Ffb.
Therefore, we do not need to round the results of the addition and sub-
traction operations. In the case of division by zero, we adopt the semantics
of other SMT theories such as reals: (=
x (sfxp.div om rm y 0)) and
(= x (ufxp.div om rm y 0)) are satisﬁable for every x, y ∈F, om ∈omode,
rm ∈rmode. Furthermore, for every x, y ∈F, om ∈omode, rm ∈rmode,
if (=
x y) then (=
(sfxp.div om rm x 0) (sfxp.div om rm y 0)) and
(= (ufxp.div om rm x 0) (ufxp.div om rm y 0)).
Note that the order of applying the rnd and ovf functions to the results
in real arithmetic matters. We choose rnd followed by ovf since it matches
the typical real-world ﬁxed-point semantics. Conversely, reversing the order can
lead to out-of-bound results. For example, assume that we extend the signa-
ture of the ovf function to omode × R →R while preserving its semantics as
a modulo operation over 2tb−fb. Then, ovf U3,2(wrap, 7.5) evaluates to 7.5
4 , and
applying rndF2 to it when the rounding mode is ru evaluates to 8
4; this is greater
than the maximum number in U3,2, namely 7
4. On the other hand, evaluating
ovf U3,2(wrap, rndF2(ru, 7.5)) produces 0, which is the expected result. We could
apply the ovf function again to the out-of-bound results, but the current seman-
tics achieves the same without this additional operation.
Let castF,Ffb : omode × rmode × R →F be a family of cast functions param-
eterized by F and Ffb that map an overﬂow mode, rounding mode, and real
number to an element of F; as before, F is either Stb,fb or Utb,fb depending on
whether we are using signed or unsigned ﬁxed-point numbers, respectively. Then,
we deﬁne castF,Ffb(om, rm, r) = ovf F(om, rndFfb(rm, r)), and the interpretation
of the conversions between reals and ﬁxed-points as
(_ to_sfxp tb fb)(om, rm, r) = castStb,fb,Ffb(om, rm, r)
(_ to_ufxp tb fb)(om, rm, r) = castUtb,fb,Ffb(om, rm, r)
sfxp.to_real(r) = r
ufxp.to_real(r) = r

20
M. Baranowski et al.
5
Decision Procedures
In this section, we propose two decision procedures for the ﬁxed-point theory by
leveraging the theory of ﬁxed-size bit-vectors in one and the theory of reals in
the other.
Bit-Vector Encoding. The decision procedure based on the theory of ﬁxed-size
bit-vectors is akin to the existing software implementations of ﬁxed-point arith-
metic that use machine integers. More speciﬁcally, a ﬁxed-point sort parameter-
ized by tb is encoded as a bit-vector sort of length tb. Therefore, the encoding of
the constructors of ﬁxed-point values simply amounts to identity functions. Simi-
larly, the encoding of the comparison operators uses the corresponding bit-vector
relations. For example, the comparison operator sfxp.lt is encoded as bvslt.
The essence of the encoding of the arithmetic operations is expressing the numer-
ator of the result, after rounding and overﬂow handling, using bit-vector arith-
metic. We leverage the following two observations in our encoding. First, round-
ing a real value v to the value in the set Ffb amounts to rounding v·2fb to an inte-
ger following the same rounding mode. This observation explains why rounding
is not necessary for the linear arithmetic operations. Second, we can encode the
wrap-around of the rounded result as simply extracting tb bits from the encoded
result thanks to the wrap-around nature of the two’s complement SMT represen-
tation. We model the behavior of division-by-zero using uninterpreted functions
of the form (RoundingMode OverflowMode (_ BitVec tb) (_ BitVec tb)), with
one such function for each ﬁxed-point sort appearing in the query. The result
of division-by-zero is then the result of applying this function to the numera-
tor, conditioned on the denominator being equal to zero. This ensures that all
divisions-by-zero with equal numerators produce equal results when the overﬂow
and rounding modes are also equal.
Real Encoding. The decision procedure based on the theory of reals closely mim-
ics the semantics deﬁned in Sect. 4. We encode all ﬁxed-point sorts as the real
sort, while we represent ﬁxed-point values as rational numbers. Therefore, we
can simply encode ﬁxed-point comparisons as real relations. For example, both
sfxp.lt and ufxp.lt are translated into < relation. We rely on the ﬁrst obser-
vation above to implement the rounding function rndfb using an SMT real-to-
integer conversion. We implement the overﬂow function ovftb,fb using the SMT
remainder function. Note that the encodings of both functions involve non-linear
real functions, such as the real-to-int conversion. Finally, we model division as
the rounded, overﬂow-corrected result of the real theory’s division operation.
Since the real theory’s semantics ensures that equivalent division operations pro-
duce equivalent results, this suﬃces to capture the ﬁxed-point division-by-zero
semantics.
Implementation.
We implemented the two decision procedures within the
pySMT framework [25]: the two encodings are rewriting classes of pySMT. We

An SMT Theory of Fixed-Point Arithmetic
21
made our implementations publicly available.1 We also implemented a random
generator of queries in our ﬁxed-point theory, and used it to perform thorough
diﬀerential testing of our decision procedures.
6
Experiments
We generated the benchmarks we use to evaluate the two encodings described
in Sect. 5 by translating the SMT-COMP non-incremental QF_FP bench-
marks [45]. The translation accepts benchmarks that contain only basic arith-
metic operations deﬁned in both theories. Moreover, we exclude all the bench-
marks in the wintersteiger folder because they are mostly simple regressions to
test the correctness of an implementation of the ﬂoating-point theory. In the
end, we manage to translate 218 QF_FP benchmarks in total.
We translate each QF_FP benchmark into 4 benchmarks in the ﬁxed-point
theory, which diﬀer in the conﬁgurations of rounding and overﬂow modes. We
denote a conﬁguration as a (rounding mode, overﬂow mode) tuple. Note that
changing a benchmark conﬁguration alters the semantics of its arithmetic opera-
tions, which might aﬀect its satisﬁability. Our translation replaces ﬂoating-point
sorts with ﬁxed-point sorts that have the same total bit-widths; the number of
fractional bits is half of the bit-width. This establishes a mapping from single-
precision ﬂoats to Q16.16 ﬁxed-points implemented by popular software libraries
such as libﬁxmath [34]. It translates arithmetic operations into their correspond-
ing ﬁxed-point counterparts using the chosen conﬁguration uniformly across a
benchmark. The translation also replaces ﬂoating-point comparison operations
with their ﬁxed-point counterparts. Finally, we convert ﬂoating-point constants
by treating them as reals and performing real-to-ﬁxed-point casts. We made our
ﬁxed-point benchmarks publicly available.2
The SMT solvers that we use in the evaluation are Boolector [9] (version
3.1.0), CVC4 [4] (version 1.7), MathSAT [13] (version 5.5.1), Yices2 [19] (version
2.6.1), and Z3 [17] (version 4.8.4) for the decision procedure based on the theory
of bit-vectors. For the evaluation of the decision procedure based on the theory
of reals, we use CVC4, MathSAT, Yices2, and Z3. We ran the experiments on
a machine with four Intel E7-4830 sockets, for a total of 32 physical cores, and
512GB of RAM, running Ubuntu 18.04. Each benchmark was limited to 1200 s
of wall time and 8GB of memory, and no run of any benchmark exceeded the
memory limit. We set processor aﬃnity for each solver instance in order to reduce
variability due to cache eﬀects.
Table 1 shows the results of running the SMT solvers on each conﬁguration
with both encodings (bit-vector and real). We do not observe any inconsistencies
in terms of satisﬁability reported among all the solvers and between both encod-
ings. The performance of the solvers on the bit-vector encoding is typically better
than on the real encoding since it leads to fewer timeouts and crashes. More-
over, all the solvers demonstrate similar performance for the bit-vector encoding
1 https://github.com/soarlab/pysmt/tree/ﬁxed-points.
2 https://github.com/soarlab/QF_FXP.

22
M. Baranowski et al.
Table 1. The results of running SMT solvers on the four diﬀerent conﬁgurations of
the benchmarks using both encodings. Boolector and MathSAT are denoted by Btor
and MSAT, respectively. Column “All” indicates the number of benchmarks for which
any solver answered sat or unsat; benchmarks for which no solver gave an answer are
counted as unknown.

An SMT Theory of Fixed-Point Arithmetic
23
Table 2. Comparison of the number of benchmarks (considering all conﬁgurations)
solved by a solver but not solved by another solver. Each row shows the number of
benchmarks solved by the row’s solver but not solved by the column’s solver. We mark
the bit-vector (resp. real) encoding with B (resp. R).
Btor-B CVC4-B MSAT-B Yices2-B Z3-B CVC4-R MSAT-R Yices2-R Z3-R
Btor-B
—
33
37
11
52
165
183
86
185
CVC4-B
19
—
46
6
57
154
160
70
170
MSAT-B
8
31
—
4
39
141
174
78
160
Yices2-B
35
44
57
—
79
194
198
95
208
Z3-B
31
50
47
34
—
151
189
103
168
CVC4-R
2
5
7
7
9
—
113
49
41
MSAT-R
17
8
37
8
44
110
—
23
118
Yices2-R
28
26
49
13
66
154
131
—
162
Z3-R
3
2
7
2
7
22
102
38
—
across all the conﬁgurations, whereas they generally produce more timeouts for
the real encoding when the overﬂow mode is wrap-around. We believe that this
can be attributed to the usage of nonlinear operations (e.g., real to int casts) in
the handling of wrap-around behaviors. This hypothesis could also explain the
observation that the bit-vector encoding generally outperform the real encoding
when the overﬂow mode is wrap-around since wrap-around comes at almost no
cost for the bit-vector encoding (see Sect. 5).
Column “All” captures the performance of the solvers when treated as one
portfolio solver. This improves the overall performance since the number of solved
benchmarks increases, indicating that each solver has diﬀerent strengths and
weaknesses. Table 2 further analyzes this behavior, and we identify two reasons
for it when we consider unique instances solved by each individual solver. First,
when the overﬂow mode is saturation, Yices2 is the only solver to solve unique
instances for both encodings. Second, when the overﬂow mode is wrap-around,
the uniquely solved instances come from solvers used on the bit-vector encoding,
except one that comes from Yices2 on the real encoding. These results provide
further evidence that the saturation conﬁgurations are somewhat easier to solve
with reals, and that wrap-around is easier with bit-vectors.
Figure 1 uses quantile plots [5] to visualize our experimental results in terms
of runtimes. A quantile plot shows the minimum runtime on y-axis within which
each of the x-axis benchmarks is solved. Some characteristics of a quantile plot
are helpful in analyzing the runtimes. First, the rightmost x coordinate is the
number of benchmarks that a solver returns meaningful results for (i.e., sat or
unsat). Second, the uppermost y coordinate is the maximum runtime of all the
benchmarks. Third, the area under a line approximates the total runtime.
Although the semantics of the benchmarks vary for each conﬁguration, we can
observe that the shapes of the bit-vector encoding curves are similar, while those
of the real encoding diﬀer based on the chosen overﬂow mode. More precisely,
solvers tend to solve benchmarks faster when their overﬂow mode is saturation

24
M. Baranowski et al.
(a) (RoundUp, Saturation)
(b) (RoundUp, WrapAround)
(c) (RoundDown, Saturation)
(d) (RoundDown, WrapAround)
Fig. 1. Quantile plots of our experimental results.
as opposed to wrap-around. We observe the same behavior in Table 1, and it is
likely due to the fact that we introduce nonlinear operations to handle wrap-
around behaviors when using the real encoding.
7
Case Study: Veriﬁcation of Quantized Neural Networks
Neural networks have experienced a signiﬁcant increase in popularity in the past
decade. Such networks that are realized by a composition of non-linear layers
are able to eﬃciently solve a large variety of previously unsolved learning tasks.
However, neural networks are often viewed as black-boxes, whose causal structure
cannot be interpreted easily by humans [40]. This property makes them unﬁt for
applications where guaranteed correctness has a high priority. Advances in formal
methods, in particular SMT solvers, leveraging the piece-wise linear structure
of neural networks [20,31,47], have made it possible to verify certain formal
properties of neural networks of reasonable size. While these successes provide
an essential step towards applying neural networks to safety-critical tasks, these

An SMT Theory of Fixed-Point Arithmetic
25
Fig. 2. Satisﬁability of speciﬁcations of our cart-pole controller.
methods leave out one crucial aspect—neural networks are usually quantized
before being deployed to production systems [30].
Quantization converts a network that operates over 32-bit ﬂoating-point
semantics into a fewer-bit ﬁxed-point representation. This process serves two
goals: compressing the memory requirement and increasing the computational
eﬃciency of running the network. Quantization introduces additional non-linear
rounding operations to the semantics of a neural network. Recently, Giacobbe
et al. [26] have shown that, in practice, this can lead to situations where a net-
work that satisﬁes formal speciﬁcations might violate them after the quantization
step. Therefore, when checking formal properties of quantized neural networks,
we need to take their ﬁxed-point semantics into account.
We derive a set of example ﬁxed-point problem instances based on two
machine learning tasks to demonstrate the capabilities of our ﬁxed-point SMT
theory on realistic problems. For all tasks, we train multi-layer perceptron mod-
ules [43] with ReLU-7 activation function [32] using quantization-aware train-
ing [30]. This way we avoid that quantization results in a considerable loss of
accuracy. To encode a neural network into an SMT formula, we rely on the Gia-
cobbe et al.’s [26] approach for encoding the summations and activation func-
tions. We quantize all neural networks using the signed ﬁxed-point format with
8 bits total and 4 fractional bits. We are using the bit-vector encoding decision
procedure in combination with the Boolector SMT solver.
7.1
Cart-Pole Controller
In our ﬁrst task, we train a neural network controller using the cart-pole envi-
ronment of OpenAI’s “gym” reinforcement learning suite. In this task, an agent
has to balance a pole mounted on a movable cart in an upright position. The cart
provides four observation variables x, ˙x,ϕ, ˙ϕ to the controller, where x is the posi-
tion of the cart and ϕ the angle of the pole. The controller then steers the cart by

26
M. Baranowski et al.
discrete actions (move left or right). Our neural network agent, composed of three
layers (4,8,1), solves the task by achieving an average score of the maximal 500
points. We analyze what our black-box agent has learned by using our decision
procedure. In particular, we are interested in how much our agent relies on the
input variable x compared to ϕ for making a decision. Moreover, we are interested
in which parts of the input space the agent’s decision is constant. We assume
the dynamics of the cart is bounded, i.e., −0.3 ≤˙x ≤0.3, −0.02 ≤˙ϕ ≤0.2,
and partition the input space of the remaining two input variables into a grid
of 64 tiles. We then check for each tile whether there exists a situation when
the network would output a certain action (left, right) by invoking our decision
procedure.
Figure 2 shows that the agent primarily relies on the input variable ϕ for
making a decision. If the angle of the pole exceeds a certain threshold, the
network is guaranteed to make the vehicle move left; on the other hand, if the
angle of the pole is below a diﬀerent threshold, the network moves the vehicle
right. Interestingly, this pattern is non-symmetric, despite the task being entirely
symmetric.
7.2
Checking Fairness
Table 3. Satisﬁability of speciﬁcations
of our fairness example.
Score DiﬀStatus Runtime
11.25
sat
10s
11.5
sat
9s
11.75
unsat
200s
12
unsat
706s
For our second task, we checked the fair-
ness speciﬁcation proposed by Giacobbe
et al. [26] to evaluate the maximum inﬂu-
ence of a single input variable on the deci-
sion of a network. We train a neural net-
work on student data to predict the score
on a math exam. Among other personal
features, the gender of a person is fed into
the network for making a decision. As the
training data contains a bias in the form
of a higher average math score for male
participants, the network might learn to underestimate the math score of female
students. We employ our decision procedure to compute the maximum inﬂuence
of the gender of a person to its predicted math score. First, we create encodings
of the same network (3 layers of size 6, 16, and 1) that share all input variables
except the gender as a single ﬁxed-point theory formula. We then constrain the
predicted scores such that the one network outputs a score that is c higher than
the score predicted by the other network. Finally, we perform binary search by
iteratively invoking our decision procedure to ﬁnd out at what bias c the formula
changes from satisﬁable to unsatisﬁable.
Table 3 shows that there exists a hypothetical person whose predicted math
score would drop by 11.5 points out of 100 if the person is female instead of
male. Moreover, our results also show that for no person the math score would
change by 11.75 points if the gender would be changed.

An SMT Theory of Fixed-Point Arithmetic
27
8
Related Work
Ruemmer and Wahl [44] and Brain et al. [8] propose and formalize the SMT
theory of the IEEE-754 ﬂoating-point arithmetic. We were inspired by these
papers both in terms of the syntax and the formalization of the semantics of
our theory. There are several decision procedures for the ﬂoating-point theory.
In particular, Brain et al. [7] present an eﬃcient and veriﬁed reduction from the
theory of ﬂoating-points to the theory of bit-vectors, while Leeser et al. [33] solve
the ﬂoating-point theory by reducing it to the theory of reals. These two decision
procedures are much more complicated than the ones we describe in Sect. 5 due
to the more complex nature of ﬂoating-point arithmetic.
In the rest of this section, we introduce related approaches that perform ver-
iﬁcation or synthesis of programs that use ﬁxed-point arithmetic. Many of these
approaches, and in particular the SMT-based ones, could beneﬁt from our uniﬁed
formalization of the theory of ﬁxed-point arithmetic. For example, they could
leverage our decision procedures instead of developing their own from scratch.
Moreover, having the same format allows for easier sharing of benchmarks and
comparison of results among diﬀerent decision procedures.
Eldib et al. [21] present an SMT-based method for synthesizing optimized
ﬁxed-point computations that satisfy certain acceptance criteria, which they
rigorously verify using an SMT solver. Similarly to our paper, their approach
encodes ﬁxed-point arithmetic operations using the theory of bit-vectors. Anta
et al. [3] tackle the veriﬁcation problem of the stability of ﬁxed-point controller
implementations. They provide a formalization of ﬁxed-point arithmetic seman-
tics using bit-vectors, but unlike our paper they do not formalize rounding and
overﬂows. Furthermore, they encode the ﬁxed-point arithmetic using unbounded
integer arithmetic, arguing that unbounded integer arithmetic is a better ﬁt
for their symbolic analysis. We could also reduce our bit-vector encoding to
unbounded integers following a similar scheme as Anta et al.
Bounded model checker ESMBC [15,24] supports ﬁxed-point arithmetic and
has been used to verify safety properties of ﬁxed-point digital controllers [1].
Like us, it also employs a bit-vector encoding. However, it is unclear exactly
which ﬁxed-point operations are supported. UppSAT [50] is an approximating
SMT solver that leverages ﬁxed-point arithmetic as an approximation theory to
ﬂoating-point arithmetic. Like the aforementioned work, UppSAT also encodes
ﬁxed-point arithmetic using the theory of bit-vectors. Its encoding ignores round-
ing modes, but adds special values such as inﬁnities.
In addition to SMT-based veriﬁcation, another important aspect of reason-
ing about ﬁxed-point computations is error bound analysis, which is often used
for the synthesis of ﬁxed-point implementations. Majumdar et al. [38] synthesize
Pareto optimal ﬁxed-point implementations of control software in regard to per-
formance criteria and error bounds. They reduce error bound computation to an
optimization problem solved by mixed-integer linear programming. Darulova et
al. [16] compile real-valued expressions to ﬁxed-point expressions, and rigorously
show that the generated expressions satisfy given error bounds. The error bound
analysis is static and based on aﬃne arithmetic. Volkova et al. [48,49] propose

28
M. Baranowski et al.
an approach to determine the ﬁxed-point format that ensures the absence of
overﬂows and minimizes errors; their error analysis is based on Worst-Case Peak
Gain measure. TAFFO [12] is an LLVM plugin that performs precision tuning by
replacing ﬂoating-point computations with their ﬁxed-point counterparts. The
quality of precision tuning is determined by a static error propagation analysis.
9
Conclusions and Future Work
In this paper, we propose an SMT theory of ﬁxed-point arithmetic to facili-
tate SMT-based software veriﬁcation of ﬁxed-point programs and systems by
promoting the development of decision procedures for the proposed theory. We
introduce the syntax of ﬁxed-point sorts and operations in the SMT-LIB for-
mat similar to that of the SMT ﬂoating-point theory. Then, we formalize the
semantics of the ﬁxed-point theory, including rounding and overﬂow, based on
the exact rational arithmetic. We develop two decision procedures for the ﬁxed-
point theory that encode it into the theory of bit-vectors and reals. Finally, we
study the performance of our prototype decision procedures on a set of bench-
marks, and perform a realistic case study by proving properties of quantized
neural networks.
As future work, we plan to add more complex operations to the ﬁxed-point
theory, such as conversions to/from ﬂoating-points and the remainder operation.
Moreover, we would like to apply the ﬁxed-point theory to verify existing software
implementations of ﬁxed-point arithmetic in diﬀerent programming languages.
We plan to do this by integrating it into the Boogie intermediate veriﬁcation
language [18] and the SMACK veriﬁcation toolchain [10,42].
References
1. Abreu, R.B., Gadelha, M.Y.R., Cordeiro, L.C., de Lima Filho, E.B., da Silva, W.S.:
Bounded model checking for ﬁxed-point digital ﬁlters. J. Braz. Comput. Soc. 22(1),
1:1–1:20 (2016). https://doi.org/10.1186/s13173-016-0041-8
2. Andrysco, M., Nötzli, A., Brown, F., Jhala, R., Stefan, D.: Towards veriﬁed,
constant-time ﬂoating point operations. In: Proceedings of the ACM Conference on
Computer and Communications Security (CCS), pp. 1369–1382 (2018). https://
doi.org/10.1145/3243734.3243766
3. Anta, A., Majumdar, R., Saha, I., Tabuada, P.: Automatic veriﬁcation of con-
trol system implementations. In: Proceedings of the International Conference
on Embedded Software (EMSOFT), pp. 9–18 (2010). https://doi.org/10.1145/
1879021.1879024
4. Barrett, C., et al.: CVC4. In: Gopalakrishnan, G., Qadeer, S. (eds.) CAV 2011.
LNCS, vol. 6806, pp. 171–177. Springer, Heidelberg (2011). https://doi.org/10.
1007/978-3-642-22110-1_14
5. Beyer, D.: Software veriﬁcation and veriﬁable witnesses. In: Baier, C., Tinelli, C.
(eds.) TACAS 2015. LNCS, vol. 9035, pp. 401–416. Springer, Heidelberg (2015).
https://doi.org/10.1007/978-3-662-46681-0_31

An SMT Theory of Fixed-Point Arithmetic
29
6. Brain, M., D’Silva, V., Griggio, A., Haller, L., Kroening, D.: Deciding ﬂoating-
point logic with abstract conﬂict driven clause learning. Formal Methods Syst.
Des. 45(2), 213–245 (2013). https://doi.org/10.1007/s10703-013-0203-7
7. Brain, M., Schanda, F., Sun, Y.: Building better bit-blasting for ﬂoating-point
problems. In: Vojnar, T., Zhang, L. (eds.) TACAS 2019. LNCS, vol. 11427, pp.
79–98. Springer, Cham (2019). https://doi.org/10.1007/978-3-030-17462-0_5
8. Brain, M., Tinelli, C., Rümmer, P., Wahl, T.: An automatable formal semantics
for IEEE-754 ﬂoating-point arithmetic. In: Proceedings of the IEEE International
Symposium on Computer Arithmetic (ARITH), pp. 160–167 (2015). https://doi.
org/10.1109/ARITH.2015.26
9. Brummayer, R., Biere, A.: Boolector: an eﬃcient SMT solver for bit-vectors and
arrays. In: Kowalewski, S., Philippou, A. (eds.) TACAS 2009. LNCS, vol. 5505, pp.
174–177. Springer, Heidelberg (2009). https://doi.org/10.1007/978-3-642-00768-
2_16
10. Carter, M., He, S., Whitaker, J., Rakamarić, Z., Emmi, M.: SMACK software
veriﬁcation toolchain. In: Proceedings of the International Conference on Software
Engineering (ICSE), pp. 589–592 (2016). https://doi.org/10.1145/2889160.2889163
11. Cherkaev, A., Tai, W., Phillips, J.M., Srikumar, V.: Learning in practice: reason-
ing about quantization. CoRR abs/1905.11478 (2019). http://arxiv.org/abs/1905.
11478
12. Cherubin, S., Cattaneo, D., Chiari, M., Bello, A.D., Agosta, G.: TAFFO: tuning
assistant for ﬂoating to ﬁxed point optimization. Embed. Syst. Lett. 12(1), 5–8
(2020). https://doi.org/10.1109/LES.2019.2913774
13. Cimatti, A., Griggio, A., Schaafsma, B.J., Sebastiani, R.: The MathSAT5 SMT
solver. In: Piterman, N., Smolka, S.A. (eds.) TACAS 2013. LNCS, vol. 7795, pp. 93–
107. Springer, Heidelberg (2013). https://doi.org/10.1007/978-3-642-36742-7_7
14. Conchon, S., Iguernlala, M., Ji, K., Melquiond, G., Fumex, C.: A three-tier strategy
for reasoning about ﬂoating-point numbers in SMT. In: Majumdar, R., Kunčak, V.
(eds.) CAV 2017. LNCS, vol. 10427, pp. 419–435. Springer, Cham (2017). https://
doi.org/10.1007/978-3-319-63390-9_22
15. Cordeiro, L., Fischer, B., Marques-Silva, J.: SMT-based bounded model checking
for embedded ANSI-C software. In: Proceedings of the International Conference
on Automated Software Engineering (ASE), pp. 137–148 (2009). https://doi.org/
10.1109/ASE.2009.63
16. Darulova, E., Kuncak, V., Majumdar, R., Saha, I.: Synthesis of ﬁxed-point
programs. In: Proceedings of the International Conference on Embedded Soft-
ware (EMSOFT), pp. 22:1–22:10 (2013). https://doi.org/10.1109/EMSOFT.2013.
6658600
17. de Moura, L., Bjørner, N.: Z3: an eﬃcient SMT solver. In: Ramakrishnan, C.R.,
Rehof, J. (eds.) TACAS 2008. LNCS, vol. 4963, pp. 337–340. Springer, Heidelberg
(2008). https://doi.org/10.1007/978-3-540-78800-3_24
18. DeLine, R., Leino, K.R.M.: BoogiePL: a typed procedural language for checking
object-oriented programs. Technical report, MSR-TR-2005-70, Microsoft Research
(2005)
19. Dutertre, B.: Yices 2.2. In: Biere, A., Bloem, R. (eds.) CAV 2014. LNCS, vol. 8559,
pp. 737–744. Springer, Cham (2014). https://doi.org/10.1007/978-3-319-08867-
9_49
20. Ehlers, R.: Formal veriﬁcation of piece-wise linear feed-forward neural networks.
In: D’Souza, D., Narayan Kumar, K. (eds.) ATVA 2017. LNCS, vol. 10482, pp.
269–286. Springer, Cham (2017). https://doi.org/10.1007/978-3-319-68167-2_19

30
M. Baranowski et al.
21. Eldib, H., Wang, C.: An SMT based method for optimizing arithmetic computa-
tions in embedded software code. IEEE Trans. Comput. Aided Des. Integr. Circ.
Syst. 33(11), 1611–1622 (2014). https://doi.org/10.1109/TCAD.2014.2341931
22. The ﬁxed crate. https://gitlab.com/tspiteri/ﬁxed
23. Programming languages – C – extensions to support embedded processors. Stan-
dard 18037, ISO/IEC (2008). https://www.iso.org/standard/51126.html
24. Gadelha, M.R., Monteiro, F.R., Morse, J., Cordeiro, L.C., Fischer, B., Nicole,
D.A.: ESBMC 5.0: an industrial-strength C model checker. In: Proceedings of the
International Conference on Automated Software Engineering (ASE), pp. 888–891.
https://doi.org/10.1145/3238147.3240481
25. Gario, M., Micheli, A.: PySMT: a solver-agnostic library for fast prototyping of
SMT-based algorithms. In: International Workshop on Satisﬁability Modulo The-
ories (SMT) (2015)
26. Giacobbe, M., Henzinger, T.A., Lechner, M.: How many bits does it take to quan-
tize your neural network? In: Biere, A., Parker, D. (eds.) TACAS 2020. LNCS,
vol. 12079, pp. 79–97. Springer, Cham (2020). https://doi.org/10.1007/978-3-030-
45237-7_5
27. Gupta, S., Agrawal, A., Gopalakrishnan, K., Narayanan, P.: Deep learning with
limited numerical precision. In: Proceedings of the International Conference on
Machine Learning (ICML), pp. 1737–1746 (2015)
28. Signed 15.16 precision ﬁxed-point arithmetic. https://github.com/ekmett/ﬁxed
29. He, S., Baranowski, M., Rakamarić, Z.: Stochastic local search for solving ﬂoating-
point constraints. In: Zamani, M., Zuﬀerey, D. (eds.) NSV 2019. LNCS, vol. 11652,
pp. 76–84. Springer, Cham (2019). https://doi.org/10.1007/978-3-030-28423-7_5
30. Jacob, B., et al.: Quantization and training of neural networks for eﬃcient integer-
arithmetic-only inference. In: Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), pp. 2704–2713 (2018). https://doi.org/
10.1109/CVPR.2018.00286
31. Katz, G., Barrett, C., Dill, D.L., Julian, K., Kochenderfer, M.J.: Reluplex: an
eﬃcient SMT solver for verifying deep neural networks. In: Majumdar, R., Kunčak,
V. (eds.) CAV 2017. LNCS, vol. 10426, pp. 97–117. Springer, Cham (2017). https://
doi.org/10.1007/978-3-319-63387-9_5
32. Krizhevsky, A.: Convolutional deep belief networks on CIFAR-10 (2010, unpub-
lished manuscript)
33. Leeser, M., Mukherjee, S., Ramachandran, J., Wahl, T.: Make it real: eﬀec-
tive ﬂoating-point reasoning via exact arithmetic. In: Proceedings of the Design,
Automation and Test in Europe Conference and Exhibition (DATE), pp. 1–4
(2014). https://doi.org/10.7873/DATE.2014.130
34. Cross platform ﬁxed point maths library. https://github.com/PetteriAimonen/
libﬁxmath
35. Liew, D., Cadar, C., Donaldson, A.F., Stinnett, J.R.: Just fuzz it: solving ﬂoating-
point constraints using coverage-guided fuzzing. In: Proceedings of the ACM Joint
European Software Engineering Conference and Symposium on the Foundations of
Software Engineering (ESEC/FSE), pp. 521–532 (2019). https://doi.org/10.1145/
3338906.3338921
36. Liew, D., Schemmel, D., Cadar, C., Donaldson, A.F., Zähl, R., Wehrle, K.:
Floating-point symbolic execution: a case study in n-version programming. In:
Proceedings of the International Conference on Automated Software Engineering
(ASE), pp. 601–612 (2017). https://doi.org/10.1109/ASE.2017.8115670

An SMT Theory of Fixed-Point Arithmetic
31
37. Lin, D.D., Talathi, S.S., Annapureddy, V.S.: Fixed point quantization of deep con-
volutional networks. In: Proceedings of the International Conference on Machine
Learning (ICML), pp. 2849–2858 (2016)
38. Majumdar, R., Saha, I., Zamani, M.: Synthesis of minimal-error control soft-
ware. In: Proceedings of the International Conference on Embedded Software
(EMSOFT), pp. 123–132 (2012). https://doi.org/10.1145/2380356.2380380
39. Menendez, D., Nagarakatte, S., Gupta, A.: Alive-FP: automated veriﬁcation of
ﬂoating point based peephole optimizations in LLVM. In: Rival, X. (ed.) SAS
2016. LNCS, vol. 9837, pp. 317–337. Springer, Heidelberg (2016). https://doi.org/
10.1007/978-3-662-53413-7_16
40. Olah, C., et al.: The building blocks of interpretability. Distill (2018). https://doi.
org/10.23915/distill.00010
41. Paganelli, G., Ahrendt, W.: Verifying (in-)stability in ﬂoating-point programs by
increasing precision, using SMT solving. In: Proceedings of the International Sym-
posium on Symbolic and Numeric Algorithms for Scientiﬁc Computing (SYNASC),
pp. 209–216 (2013). https://doi.org/10.1109/SYNASC.2013.35
42. Rakamarić, Z., Emmi, M.: SMACK: decoupling source language details from veri-
ﬁer implementations. In: Biere, A., Bloem, R. (eds.) CAV 2014. LNCS, vol. 8559,
pp. 106–113. Springer, Cham (2014). https://doi.org/10.1007/978-3-319-08867-
9_7
43. Rumelhart, D.E., Hinton, G.E., Williams, R.J.: Learning representations by back-
propagating errors. Nature 323(6088), 533–536 (1986). https://doi.org/10.1038/
323533a0
44. Rümmer, P., Wahl, T.: An SMT-LIB theory of binary ﬂoating-point arithmetic.
In: Informal Proceedings of the International Workshop on Satisﬁability Modulo
Theories (SMT) (2010)
45. SMT-LIB benchmarks in the QF_FP theory. https://clc-gitlab.cs.uiowa.edu:2443/
SMT-LIB-benchmarks/QF_FP
46. SMT-LIB: the satisﬁability modulo theories library. http://smtlib.cs.uiowa.edu
47. Tjeng, V., Xiao, K.Y., Tedrake, R.: Evaluating robustness of neural networks with
mixed integer programming. In: International Conference on Learning Represen-
tations (ICLR) (2019)
48. Volkova, A., Hilaire, T., Lauter, C.: Determining ﬁxed-point formats for a digital
ﬁlter implementation using the worst-case peak gain measure. In: Proceedings of
the Asilomar Conference on Signals, Systems and Computers, pp. 737–741 (2015).
https://doi.org/10.1109/ACSSC.2015.7421231
49. Volkova, A., Hilaire, T., Lauter, C.Q.: Arithmetic approaches for rigorous design
of reliable ﬁxed-point LTI ﬁlters. IEEE Trans. Comput. 69(4), 489–504 (2020).
https://doi.org/10.1109/TC.2019.2950658
50. Zeljić, A., Backeman, P., Wintersteiger, C.M., Rümmer, P.: Exploring approxi-
mations for ﬂoating-point arithmetic using UppSAT. In: Galmiche, D., Schulz, S.,
Sebastiani, R. (eds.) IJCAR 2018. LNCS (LNAI), vol. 10900, pp. 246–262. Springer,
Cham (2018). https://doi.org/10.1007/978-3-319-94205-6_17

Covered Clauses Are Not Propagation
Redundant
Lee A. Barnett(B), David Cerna, and Armin Biere
Johannes Kepler University Linz, Altenbergerstraße 69, 4040 Linz, Austria
{lee.barnett,david.cerna,armin.biere}@jku.at
Abstract. Propositional proof systems based on recently-developed
redundancy properties admit short refutations for many formulas tra-
ditionally considered hard. Redundancy properties are also used by pro-
cedures which simplify formulas in conjunctive normal form by removing
redundant clauses. Revisiting the covered clause elimination procedure,
we prove the correctness of an explicit algorithm for identifying cov-
ered clauses, as it has previously only been implicitly described. While
other elimination procedures produce redundancy witnesses for com-
pactly reconstructing solutions to the original formula, we prove that
witnesses for covered clauses are hard to compute. Further, we show
that not all covered clauses are propagation redundant, the most gen-
eral, polynomially-veriﬁable standard redundancy property. Finally, we
close a gap in the literature by demonstrating the complexity of clause
redundancy itself.
Keywords: SAT · Clause elimination · Redundancy
1
Introduction
Boolean satisﬁability (SAT) solvers have become successful tools for solving rea-
soning problems in a variety of applications, from formal veriﬁcation [6] and
security [27] to pure mathematics [10,19,24]. Signiﬁcant recent progress in the
design of SAT solvers has come as a result of exploiting the notion of clause
redundancy (for instance, [14,16,18]). For a propositional formula F in con-
junctive normal form (CNF), a clause C is redundant if it can be added to, or
removed from, F without aﬀecting whether F is satisﬁable [23].
In particular, redundancy forms a basis for clausal proof systems. These
systems refute an unsatisﬁable CNF formula F by listing instructions to add
or delete clauses to or from F, where the addition of a clause C is permitted
only if C meets some criteria ensuring its redundancy. By eventually adding the
empty clause, the formula is proven to be unsatisﬁable. Crucially, the redundancy
Supported by the Austrian Science Fund (FWF) under project W1255-N23, the LogiCS
Doctoral College on Logical Methods in Computer Science, as well as the LIT Artiﬁcial
Intelligence Lab funded by the State of Upper Austria.
c
⃝Springer Nature Switzerland AG 2020
N. Peltier and V. Sofronie-Stokkermans (Eds.): IJCAR 2020, LNAI 12166, pp. 32–47, 2020.
https://doi.org/10.1007/978-3-030-51074-9_3

Covered Clauses Are Not Propagation Redundant
33
criteria of a system can also be used as an inference rule by a solver searching
for such refutations, or for satisfying assignments.
Proof systems based on the recently introduced PR (Propagation Redun-
dancy) criteria [15] have been shown to admit short refutations of the famous
pigeonhole formulas [11,17]. These are known to have only exponential-size refu-
tations in many systems, including resolution [9] and constant-depth Frege sys-
tems [1], but have polynomial-size PR refutations. In fact, many problems typi-
cally considered hard have short PR refutations, spurring interest in these sys-
tems from the viewpoint of proof complexity [4]. Further, systems based on PR
are strong even without introducing new variables, and have the potential to
aﬀord substantial improvements to SAT solvers (such as in [16,18]).
The PR criteria is very general, encompassing nearly all other established
redundancy criteria, and it is NP-complete to decide whether it is met by a
given clause [18]. However, when the clause is given alongside a witness, a partial
assignment providing additional evidence for the clause’s redundancy, the PR
criteria can be polynomially veriﬁed [15]. SAT solvers producing refutations in
the PR system must ﬁnd and record a witness for each PR clause addition.
Redundancy is also a basis for clause elimination procedures, which simplify
a CNF formula by removing redundant clauses [12,14]. These are useful pre-
processing and inprocessing techniques that also make use of witnesses, but for
the task of solution reconstruction: correcting satisfying assignments found after
simplifying to ensure they solve the original formula. A witness for a clause C
details how to ﬁx assignments falsifying C without falsifying other clauses in
the formula [15,17], so solvers using elimination procedures that do not preserve
formula equivalence typically provide a witness for each removed clause.
Covered clause elimination (CCE) [13] is a strong procedure which removes
covered clauses, a generalization of blocked clauses [20,25], and has been imple-
mented in various SAT solvers (for example, [2,3,8]) and the CNF preprocessing
tool Coprocessor [26]. CCE does not preserve formula equivalence, but provides
no witnesses for the clauses it removes. Instead, it uses a complex technique to
reconstruct solutions in multiple steps, requiring at times a quadratic amount of
space to reconstruct a single clause [14,21]. CCE has so far only been implicitly
described, and it is not clear how to produce witnesses for covered clauses.
In this paper we provide an explicit algorithm for identifying covered clauses,
and show that their witnesses are diﬃcult to produce. We also demonstrate that
although covered clauses are redundant, they do not always meet the criteria
required by PR. This suggests it may be beneﬁcial to consider redundancy prop-
erties beyond PR which allow alternative types of witnesses. There has already
been some work in this direction with the introduction of the SR (Substitution
Redundancy) property by Buss and Thapen [4].
The paper is organized as follows. In Sect. 2 we provide necessary background
and terminology, while Sect. 3 reviews covered clause elimination, provides the
algorithm for identifying covered clauses, and proves that this algorithm and its
reconstruction strategy are correct. Section 4 includes proofs about witnesses for
covered clauses, and shows that they are not encompassed by PR. In Sect. 5 we

34
L. A. Barnett et al.
consider the complexity of deciding clause redundancy in general, followed by a
conclusion and discussion of future work in Sect. 6.
2
Preliminaries
A literal is a boolean variable x or its negation ¬x. A clause is a disjunction
of literals, and a formula is a conjunction of clauses. We often identify a clause
with the set of its literals, and a formula with the set of its clauses. For a set of
literals S we write ¬S to refer to the set ¬S = {¬l | l ∈S}. The set of variables
occurring in a formula F is written var(F). The empty clause is represented by
⊥, and the satisﬁed clause by ⊤.
An assignment is a function from a set of variables to the truth values true
and false. An assignment is total for a formula F if it assigns a value for every
variable in var(F), otherwise it is partial. An assignment is represented by the
set of literals it assigns to true. The composition of assignments τ and υ is
τ ◦υ(x) =

τ(x)
if x, ¬x ̸∈υ
υ(x)
otherwise
for a variable x in the domain of τ or υ. For a literal l, we write τl to represent
the assignment τ ◦{l}. An assignment satisﬁes (resp., falsiﬁes) a variable if
it assigns that variable true (resp., false). Assignments are lifted to functions
assigning literals, clauses, and formulas in the usual way.
Given an assignment τ and a clause C, the partial application of τ to C is
written C|τ and is deﬁned as follows: C|τ = ⊤if C is satisﬁed by τ, otherwise,
C|τ = {l | l ∈C and ¬l ̸∈τ}. Likewise, the partial application of the assignment
τ to a formula F is written F|τ and deﬁned by: F|τ = ⊤if σ satisﬁes F, otherwise
F|τ = {C|τ | C ∈F and C|τ ̸= ⊤}. Unit propagation refers to the iterated
application of the unit clause rule, replacing F by F|{l} for each unit clause
(l) ∈F, until there are no unit clauses left.
We write F ⊨G to indicate that every assignment satisfying F, and which is
total for G, satisﬁes G as well. Further, we write F ⊢1 G to mean F implies G
by unit propagation: for every D ∈G, unit propagation on ¬D ∧F produces ⊥.
A clause C is redundant with respect to a formula F if the formulas F \ {C}
and F ∪{C} are satisﬁability-equivalent: both satisﬁable, or both unsatisﬁ-
able [23]. The following theorem provides a characterization of clause redundancy
based on logical implication.
Theorem 1 (Heule, Kiesel, and Biere [17]).
A non-empty clause C is
redundant with respect to a formula F (with C ̸∈F) if and only if there is
a partial assignment ω such that ω satisﬁes C, and F|α ⊨F|ω, where α = ¬C.
As a result, redundancy can be shown by providing a witnessing assignment ω
(or witness) and demonstrating that F|α ⊨F|ω. When the logical implication
relation “⊨” is replaced with “⊢1,” the result is the deﬁnition of a propagation
redundant or PR clause, and ω is called a PR witness [15]. Determining whether

Covered Clauses Are Not Propagation Redundant
35
a clause is PR with respect to a formula is NP-complete [18], but since it can be
decided in polynomial time whether F ⊢1 G for arbitrary formulas F and G, it
can be eﬃciently decided whether a given assignment is a PR witness.
A clause elimination procedure iteratively identiﬁes and removes clauses sat-
isfying a particular redundancy property from a formula, until no such clauses
remain. A simple example is subsumption elimination, which removes any clauses
C ∈F that are subsumed by another clause D ∈F; that is, D ⊆C. Subsump-
tion elimination is model-preserving, as it only removes clauses C such that any
assignment satisfying F \ {C} also satisﬁes F ∪{C}.
Some clause elimination procedures are not model-preserving. Blocked clause
elimination [20,25] iteratively removes from a formula F any clauses C satisfying
the following property: C is blocked by a literal l ∈C if for every clause D ∈F
containing ¬l, there is some other literal k ∈C with ¬k ∈D. For a blocked
clause C, there may be assignments satisfying F \ {C} which falsify F ∪{C}.
However, blocked clauses are redundant, so if F ∪{C} is unsatisﬁable, then so
is F \ {C}, thus blocked clause elimination is still satisﬁability-preserving.
Clause elimination procedures which are not model-preserving must provide
a way to reconstruct solutions to the original formula out of solutions to the
reduced formula. Witnesses provide a convenient framework for reconstruction:
if C is redundant with respect to F, and τ is a total or partial assignment
satisfying F but not C, then τ ◦ω satisﬁes F ∪{C}, for any witness ω for C
with respect to F [7,17]. For reconstructing solutions after removing multiple
clauses, a sequence σ of witness-labeled clauses (ω : C), called a reconstruction
stack, can be maintained and used as follows [7,21].
Deﬁnition 1. Given a sequence σ of witness-labeled clauses, the reconstruction
function (w.r.t. σ) is deﬁned recursively as follows, for an assignment τ:1
Rϵ(τ) = τ,
Rσ·(ω:D)(τ) =

Rσ(τ)
if τ(D) = ⊤
Rσ(τ ◦ω)
otherwise.
For a set of clauses S, a sequence σ of witness-labeled clauses satisﬁes the recon-
struction property for S, or is a reconstruction sequence for S, with respect to a
formula F if Rσ(τ) satisﬁes F ∪S for any assignment τ satisfying F \S. As long
as a witness is recorded for each clause C removed by a non-model-preserving
procedure, even combinations of diﬀerent clause elimination procedures can be
used to simplify the same formula. Speciﬁcally, σ = (ω1 : C1) · · · (ωn : Cn) is
a reconstruction sequence for {C1, . . . , Cn} ⊆F if ωi is a witness for Ci with
respect to F \ {C1, . . . , Ci}, for all 1 ≤i ≤n [7].
The following lemma results from the fact that the reconstruction function
satisﬁes Rσ·σ′(τ) = Rσ(Rσ′(τ)), for any sequences σ, σ′ and assignment τ [7].
Lemma 1. If σ is a reconstruction sequence for a set of clauses S with respect
to F ∪{C}, and σ′ is a reconstruction sequence for {C} with respect to F, then
σ · σ′ is a reconstruction sequence for S with respect to F.
1 This improved variant over [7] is due to Christoph Scholl (3rd author of [7]).

36
L. A. Barnett et al.
3
Covered Clause Elimination
This section reviews covered clause elimination (CCE) and its asymmetric vari-
ant (ACCE), introduced by Heule, J¨arvisalo, and Biere [13], and presents an
explicit algorithm implementing the more general ACCE procedure. The deﬁni-
tions as given here diﬀer slightly from the original work, but are generally equiv-
alent. A proof of correctness for the algorithm and its reconstruction sequence
are given.
CCE is a clause elimination procedure which iteratively extends a clause by
the addition of so-called “covered” literals. If at some point the extended clause
becomes blocked, the original clause is redundant and can be eliminated. To
make this precise, the set of resolution candidates in F of C upon l, written
RC(F, C, l), is deﬁned as the collection of clauses in F with which C has a
non-tautological resolvent upon l (where “⊗l” denotes resolution):
RC(F, C, l) = {C′ ∨¬l ∈F | C′ ∨¬l ⊗l C ̸≡⊤}.
The resolution intersection in F of C upon l, written RI(F, C, l), consists of
those literals occurring in each of the resolution candidates, apart from ¬l:
RI(F, C, l) =
 
RC(F, C, l)

\ {¬l}.
If RI(F, C, l) ̸= ∅, its literals are covered by l and can be used to extend C.
Deﬁnition 2. A literal k is covered by l ∈C with respect to F if k ∈RI(F, C, l).
A literal is covered by C if it is covered by some literal in C.
Covered literals can be added to a clause in a satisﬁability-preserving manner,
meaning that if the extended clause C ∪RI(F, C, l) is added to F, then C is
redundant. In fact, C is a PR clause.
Proposition 1. C is PR with respect to F ′ = F ∧(C ∪RI(F, C, l)) with witness
ω = αl, for l ∈C and α = ¬C.
Proof. Consider a clause D|ω ∈F ′|ω, for some D ∈F ′. We prove that ω is a PR
witness by showing that F ′|α implies D|ω by unit propagation. First, we know
l ̸∈D, since otherwise D|ω = ⊤would vanish in F|ω. If also ¬l ̸∈D, this means
D|ω = D|α, and therefore F ′|α ⊢1 D|ω. Now, suppose ¬l ∈D. Notice that D
contains no other literal k such that ¬k ∈C, since otherwise D|ω = ⊤here as
well. As a result D ∈RC(F, C, l), so RI(F, C, l) ⊂D and RI(F, C, l) \ C ⊆D|ω.
Notice RI(F, C, l) \ C = (C ∪RI(F, C, l))|α ∈F ′|α, therefore F ′|α ⊢1 D|ω.
⊓⊔
Consequently, C is redundant with respect to F ∪{C′} for any C′ ⊇C con-
structed by iteratively adding covered literals to C. In other words, F and
(F \ {C}) ∪{C′} are satisﬁability-equivalent, so that C could be replaced by
C′ in F without aﬀecting the satisﬁability of the formula. Thus if some such
extension C′ would be blocked in F, that C′ would be redundant, and therefore
C is redundant itself. CCE identiﬁes and removes such clauses from F.

Covered Clauses Are Not Propagation Redundant
37
Deﬁnition 3. A clause C is covered in F if an extension of C by iteratively
adding covered literals is blocked.
CCE refers to the following procedure: while some clause C in F is covered,
remove C (that is, replace F with F \ {C}).
ACCE strengthens this procedure by extending clauses using a combination
of covered literals and asymmetric literals. A literal k is asymmetric to C with
respect to F if there is a clause C′ ∨¬k ∈F such that C′ ⊆C. The addition of
asymmetric literals to a clause is model-preserving, so that the formulas F and
(F \ {C}) ∪{C ∨k} are equivalent, for any k which is asymmetric to C [12].
Deﬁnition 4. A clause C′ ⊇C is an ACC extension of C with respect to F if
C′ can be constructed from C by the iterative addition of covered and asymmetric
literals. If some ACC extension of C is blocked or subsumed in F, then C is an
asymmetric covered clause (ACC).
ACCE performs the following: while some C in F is an ACC, remove C from F.
Solvers aiming to eliminate covered clauses more often implement ACCE than
plain CCE, since asymmetric literals can easily be found by unit propagation,
and ACCE is more powerful than CCE, eliminating more clauses [12,13].
The procedure ACC(F, C) in Fig. 1 provides an algorithm identifying whether
a clause C is an ACC with respect to a formula F. This procedure diﬀers in some
ways, and includes optimizations over the original procedure as implicitly given
by the deﬁnition of ACCE. Notably, two extensions of the original clause C are
maintained: E consists of C and any added covered literals, while α tracks C
and all added literals, both covered and asymmetric. The literals in α are kept
negated, so that E ⊆¬α, and the clause represented by ¬α is the ACC extension
of the original clause C being computed.
The E and α extensions are maintained separately for two purposes. First,
the covered literal addition loop (lines 9–16) needs to iterate only over those
literals in E, and can ignore those in (¬α) \ E, as argued below.
Lemma 2. If k is covered by l ∈(¬α) \ E, then k ∈¬α already.
Proof. If l belongs to ¬α but not to E, then there is some clause D ∨¬l in F
such that D ⊆¬α. But then D ∨¬l occurs in RC(F, ¬α, l), and consequently
RI(F, ¬α, l) ⊆D ⊆¬α. Thus k ∈RI(F, ¬α, l) implies k ∈¬α.
⊓⊔
Notice that the computation of the literals covered by l ∈E also prevents any
of these literals already in ¬α from being added again.
The second reason for separating E and α is as follows. When a covered literal
is found, or when the extended clause is blocked, the algorithm appends a new
witness-labeled clause to the reconstruction sequence σ (lines 11 and 14). Instead
of (¬αl : α), the procedure adds the shorter witness-labeled clause (¬El : E).
The proof of statement (3) in Lemma 3 below shows that this is suﬃcient.
Certain details are omitted, especially concerning the addition of asymmetric
literals (lines 6–7), but notice that it is never necessary to recompute F|α entirely.
Instead the assignment falsifying each u newly added to α can simply be applied

38
L. A. Barnett et al.
ACC(F, C)
1
σ := ε
2
E :=
C
3
α := ¬C
4
repeat
5
if ⊥∈F|α then return (true, σ)
6
if there are unit clauses in F|α then
7
α := α ∪{u} for each unit u
8
else
9
for each l ∈E
10
G := {D|α | (D ∨¬l) ∈F and D|α ̸= ⊤}
11
if G = ∅then return
true, σ · (¬El : E)

12
Φ :=

G
13
if Φ ̸= ∅then
14
σ := σ · (¬El : E)
15
E := E ∪
Φ
16
α := α ∪¬ Φ
17
until no updates to α
18
return (false, ε)
Fig. 1. Asymmetric Covered Clause (ACC) Identiﬁcation. The procedure ACC(F, C)
maintains a sequence σ of witness-labeled clauses, and two sets of literals E and α. The
main loop iteratively searches for literals which could be used to extend C and adds
their negations to α, so that the clause represented by ¬α is an ACC extension of C.
The set E records only those which could be added as covered literals. If C is an ACC,
then ACC(F, C) returns (true, σ): in line 5 if the extension ¬α becomes subsumed in F,
or in line 11 if it becomes blocked. In either case, the witness-labeled clauses in σ form
a reconstruction sequence for the clause C. Note that lines 5–7 implement Boolean
constraint propagation (over the partial assignment α) and can make use of eﬃcient
watched clause data structures, while line 10 has to collect all clauses containing ¬l,
which are still unsatisﬁed by α, and thus requires full occurrence lists.
to the existing F|α. In contrast, the for each loop (lines 9–16) should re-iterate
over the entirety of E each time, as added literals may allow new coverings:
Example 1. Let C = a ∨b ∨c and
F = (¬a ∨¬x1) ∧(¬a ∨x2) ∧(¬b ∨¬x1) ∧(¬b ∨¬x2) ∧(¬c ∨x1)
Initially, neither a nor b cover any literals, but c covers x1, so it can be added
to the clause. After extending, a in C ∨x1 covers x2, and b blocks C ∨x1 ∨x2.

Covered Clauses Are Not Propagation Redundant
39
The following lemma supplies invariants for arguing about ACC(F, C).
Lemma 3. After each update to α, for the clauses represented by ¬α and E:
(1) ¬α is an ACC extension of C,
(2) F ∪{¬α} ⊨F ∪{E}, and
(3) σ is a reconstruction sequence for {C} with respect to F ∪{¬α}.
Proof. Let αi, σi, and Ei refer to the values of α, σ, and E, respectively, after
i ≥0 updates to α (so that αi ⊊αi+1 for each i, but possibly σi = σi+1 and
Ei = Ei+1). Initially, (1) and (2) hold as E = ¬α0 = C. Further, σ0 = ϵ is a
reconstruction sequence for {C} with respect to F ∪{C}, so (3) holds as well.
Assuming these claims hold after update i, we show that they hold after i + 1.
First suppose update i + 1 is the result of executing line 7.
(1) αi+1 = αi ∪U, where u ∈U implies (u) is a unit clause in F|αi. Then ¬αi+1
is the extension of ¬αi by the addition of asymmetric literals ¬U. Assuming
¬αi is an ACC extension of C, then so is ¬αi+1.
(2) Asymmetric literal addition is model-preserving, so F ∪{¬αi+1} ⊨F ∪{¬αi}.
Since E was not updated, Ei+1 = Ei. Assuming F ∪{¬αi} ⊨F ∪{Ei}, we
get F ∪{¬αi+1} ⊨F ∪{Ei+1}.
(3) Again, asymmetric literal addition is model-preserving. Assuming σi is a
reconstruction sequence for {C} with respect to F ∪{αi}, then Lemma 1
implies σi+1 = σi · ε = σi reconstructs {C} with respect to F ∪{¬αi+1}.
Now, suppose instead update i + 1 is executed in line 16.
(1) αi+1 = αi ∪Φ, for some set of literals Φ ̸= ∅constructed for l ∈E ⊆¬α.
Notice for k ∈Φ that k ∈RI(F, ¬α, l), so k is covered by ¬α. Thus assuming
¬αi is a ACC extension of C, then ¬αi+1 is as well.
(2) Consider an assignment τ satisfying F ∪{¬αi+1}. If τ satisﬁes ¬αi ⊂¬αi+1
then τ satisﬁes F ∪{¬αi} and by assumption, F ∪{Ei}. Since Ei ⊂Ei+1
in this case, τ satisﬁes F ∪{Ei+1}. If instead τ satisﬁes some literal in
¬αi+1 \ ¬αi then τ satisﬁes Φ ⊆Ei+1, so τ satisﬁes F ∪{Ei+1}. Thus
F ∪{¬αi+1} ⊨F ∪{Ei+1} in this case as well.
(3) Proposition 1 implies ((αi)l : ¬αi) is a reconstruction sequence for {¬αi}
with respect to F ∪{¬αi+1}. As Ei ⊆¬αi, and F ∪{¬αi} ⊨F ∪{Ei}
by assumption, then any τ falsiﬁes ¬αi if and only if τ falsiﬁes Ei. Since
l ∈Ei as well, then ((¬Ei)l : Ei) is, in fact, also a reconstruction sequence
for {¬αi} with respect to F ∪{¬αi+1}. Finally, with the assumption σi is a
reconstruction sequence for C with respect to F ∪{¬αi} and Lemma 1, then
σi+1 = σi ·((¬Ei)l : Ei) is a reconstruction sequence for {C} in F ∪{¬αi+1}.
Thus both updates maintain invariants (1)–(3).
⊓⊔
With the help of this lemma we can now show the correctness of ACC(F, C):
Theorem 2. For a formula F and a clause C, the procedure ACC(F, C) returns
(true, σ) if and only if C is an ACC with respect to F. Further, if ACC(F, C)
returns (true, σ), then σ is a reconstruction sequence for {C} with respect to F.

40
L. A. Barnett et al.
Proof. (⇒)
Suppose (true, σ) is returned in line 5. Then ⊥∈F|α, so there
is some D ∈F such that D ⊆¬α; that is, ¬α is subsumed by D. By Lemma 3
then an ACC extension of C is subsumed in F, so C is an ACC with respect
to F. Further, subsumption elimination is model-preserving, so that Lemmas 1
and 3 imply σ is a reconstruction sequence for C with respect to F.
Suppose now that (true, σ) is returned in line 11. Then for α and some l ∈E,
all clauses in F with ¬l are satisﬁed by α. Since E ⊆¬α, then ¬α is blocked
by l. By Lemma 3 then C is an ACC with respect to F. Now, αl is a witness
for ¬α with respect to F, and (αl : ¬α) is a reconstruction sequence for {¬α}
in F. Further, E ⊆¬α, and Lemma 3 gives F ∪{¬α} ⊨F ∪{E}, therefore
((¬Ei)l : Ei) is a reconstruction sequence for {¬α} in F as well. Then Lemma 1
implies σ · (¬El : E) is a reconstruction sequence for C with respect to F.
(⇐)
Suppose C is an ACC; that is, some C′ = C ∨k1 ∨· · · ∨kn is blocked
or subsumed in F, where k1 is an asymmetric or covered literal for C, and ki
is an asymmetric or covered literal for C ∨k1 ∨· · · ∨ki−1 for i > 1. Towards a
contradiction, assume ACC(F, C) returns (false, ε). Then for the ﬁnal value of
α, the clause represented by ¬α is not blocked nor subsumed in F, and hence,
C′ ̸⊆¬α. As C ⊆¬α, there must be some values of i such that ¬ki ̸∈α.
Let m refer to the least such i; that is, ¬km ̸∈α, but ¬ki ∈α for all 1 ≤i < m.
Thus km is asymmetric, or covered by, Cm−1 = C ∨k1 ∨· · · ∨km−1.
If km is asymmetric to Cm−1, there is some clause D ∨¬km in F such that
D ⊆Cm−1. By assumption, ¬km ̸∈α but ¬Cm−1 ⊆α. Further, km ̸∈α, as
otherwise (D ∨¬km)|α = ⊥and ACC(F, C) would have returned true. But
(D ∨¬km)|α = ¬km would be a unit in F|α and added to α by line 7.
If instead km is covered by Cm−1, then km ∈RI(F, Cm−1, l) for some literal
l ∈Cm−1 ⊆¬α. In fact l ∈E, by Lemma 2. During the lth iteration of the for
each loop, then km ∈Φ, and ¬km would be added to α by line 16.
⊓⊔
ACC(F, C) produces, for any asymmetric covered clause C in F, a reconstruc-
tion sequence σ for C with respect to F. This allows ACCE to be used during
preprocessing or inprocessing like other clause elimination procedures, append-
ing this σ to the solver’s main reconstruction stack whenever an ACC is removed.
However, the algorithm does not produce redundancy witnesses for the clauses
it removes. Instead, σ consists of possibly many witness-labeled clauses, starting
with the redundant clause C, and reconstructs solutions for C in multiple steps.
In contrast, most clause elimination procedures produce a single witness-
labeled clause (ω : C) for each removed clause C. In practice, only the part of
ω which diﬀers from ¬C must be recorded; for most procedures this diﬀerence
includes only literals in C, so that reconstruction for {C} needs only linear space
in the size of C. In contrast, the size of σ produced by ACC(F, C) to reconstruct
{C} can be quadratic in the length of the extended clause.
Example 2. Consider C = x0 and
Fn = (¬xn−2 ∨xn−1 ∨xn) ∧(¬xn−1 ∨¬xn) ∧
n−2

i=1
(¬xi−1 ∨xi).

Covered Clauses Are Not Propagation Redundant
41
The extended clause ¬α = x0 ∨x1 ∨· · · ∨xn is blocked in Fn by xn−1. Then
ACC(Fn, C) returns the pair with true and the reconstruction sequence2
σ = (x0 ≀x0)·(x1 ≀x0∨x1) · · · (xn−2 ≀x0∨x1∨· · ·∨xn−2)·(xn−1 ≀x0∨x1∨· · ·∨xn).
The extended clause includes n literals, and the size of σ is O(n2).
4
Witnesses for Covered Clauses
In this section, we consider the speciﬁc problem of ﬁnding witnesses for (asym-
metric) covered clauses. As these clauses are redundant, such witnesses are guar-
anteed to exist by Theorem 1, though they are not produced by ACC(F, C). More
precisely, we are interested in the witness problem for covered clauses.
Deﬁnition 5. The witness problem for a redundancy property P is as follows:
given a formula F and a clause C, if P is met by C with respect to F then return
a witness for C, or decide that P is not met by C.
For instance, the witness problem for blocked clauses is solved as follows: test
each l ∈C to see if l blocks C in F. As soon as a blocking literal l is found then
αl is a witness for C, where α = ¬C. If no blocking literal is found, then C is
not blocked. For blocked clauses, this polynomial procedure decides whether C
is blocked or not and also determines a witness ω = αl for C.
Solving the witness problem for covered clauses is not as straightforward,
as it is not clear how a witness could be produced when deciding a clause is
covered, or from a sequence σ constructed by ACC(F, C). The following theorem
shows that this problem is as diﬃcult as producing a satisfying assignment for
an arbitrary formula, if one exists. In particular, we present a polynomial time
reduction from the search analog of the SAT problem: given a formula F, return
a satisfying assignment of F, or decide that F is unsatisﬁable.
Speciﬁcally, given a formula G, we construct a pair (F, C) as an instance to
the witness problem for covered clauses. In this construction, C is covered in F
and has some witness ω. Moreover, any witness ω for this C necessarily provides
a satisfying assignment to G, if there is one.
Proposition 2. Given a formula G = D1 ∧· · · ∧Dn, let G′ = D′
1 ∧· · · ∧D′
n
refer to a variable-renamed copy of G, containing v′ everywhere G contains v,
so that var(G) ∩var(G′) = ∅. Further, let C = k ∨l and construct the formula:
F = (x ∨¬k) ∧(¬x ∨¬y) ∧(y ∨¬l) ∧
(x ∨D1) ∧
· · ·
∧(x ∨Dn) ∧
(y ∨D′
1) ∧
· · ·
∧(y ∨D′
n)
for variables x, y, k, l ̸∈var(G) ∪var(G)′. Finally, let ω be a witness for C with
respect to F. Either ω satisﬁes at least one of G or G′, or G is unsatisﬁable.
2 In order to simplify the presentation, only the part of the witness diﬀering from the
negated clause is written, so that (l ≀C) actually stands for (¬Cl : C). The former
is in essence the original notation used in [21], while set, super or globally blocked,
as well as PR clauses [15,22,23] require the more general one used in this paper.

42
L. A. Barnett et al.
Proof. First notice for C that x is covered by k and y is covered by l, so that
the extension (k ∨l ∨x ∨y) is blocked in F (with blocking literal x or y). Thus
C is redundant in F, so a witness ω exists.
We show that ω satisﬁes G or G′ if and only if G is satisﬁable.
(⇒)
If ω satisﬁes G then surely G is satisﬁable. If ω satisﬁes G′ but not G then
the assignment ωG = {x ∈var(G) | x′ ∈G′ and x ∈ω} satisﬁes G.
(⇐)
Assume G is satisﬁable, and without loss of generality3 further assume
ω = {k} ◦ω′ for some ω′ not assigning var(k). Then F|α ⊨(F|{k})|ω′; that is,
F|α ⊨

(x) ∧(¬x ∨¬y) ∧(y ∨¬l) ∧(x ∨D1) ∧· · · ∧(y ∨D′
n)

|ω′.
G is satisﬁable, so there are models of F|α in which ¬x is true. However, x
occurs as a unit clause in F|{k}, so it must be the case that x ∈ω′. Therefore
ω = {k, x} ◦ω′′ for some ω′′ assigning neither var(k) nor var(x) such that
F|α ⊨

(¬y) ∧(y ∨¬l) ∧(y ∨D′
1) ∧· · · ∧(y ∧D′
n)

|ω′′.
By similar reasoning, ω′′ must assign y to false, so now ω = {k, x, ¬y} ◦ω′′′ for
some ω′′′, assigning none of var(k), var(x), or var(y), such that
F|α ⊨

(¬l) ∧(D′
1) ∧· · · ∧(D′
n)

|ω′′′.
Finally, consider any clause D′
i ∈G′. We show that ω satisﬁes D′
i. As ω is
a witness, F|α ⊨F|ω, so that (D′
i)|ω is true in all models of F|α, including
models which assign y to true. In particular, let τ be a model of G; then (D′
i)|ω
is satisﬁed by τ ∪{¬x, y} ∪ν, for every assignment ν over var(G′). Because
var(D′
i) ⊆var(G′), then (D′
i)|ω ≡⊤. Therefore G′|ω ≡⊤.
⊓⊔
Proposition 2 suggests there is likely no polynomial procedure for computing
witnesses for covered clauses. The existence of witnesses is the basis for solution
reconstruction, but witnesses which cannot be eﬃciently computed make the use
of non-model-preserving clause elimination procedures more challenging; that is,
we are not aware of any polynomial algorithm for generating a compact (sub-
quadratic) reconstruction sequence (see also Example 2).
As PR clauses are deﬁned by witnesses, procedures deciding PR generally
solve the witness problem for PR. For example, the PR reduct [16] provides a for-
mula whose satisfying assignments encode PR witnesses, if they exist. However,
this does not produce witnesses for covered clauses, which are not encompassed
by PR. In other words, although any clause extended by a single covered literal
addition is a PR clause by Proposition 1, this is not true for covered clauses.
Theorem 3. Covered clauses are not all propagation redundant.
3 If k ̸∈ω, then ω = {l} ◦ω′ for some ω′ not assigning var(l) and the argument is
symmetric, ending with G|ω = ⊤. Note that by deﬁnition ω satisﬁes C thus k or l.

Covered Clauses Are Not Propagation Redundant
43
Proof. By counterexample. Consider the clause C = k ∨l and the formula
F = (x ∨¬k)
∧
(¬x ∨¬y)
∧(y ∨¬l)
∧
(x ∨a ∨b) ∧(x ∨a ∨¬b) ∧(x ∨¬a ∨b) ∧(x ∨¬a ∨¬b) ∧
(y ∨c ∨d) ∧(y ∨c ∨¬d) ∧(y ∨¬c ∨d) ∧(y ∨¬c ∨¬d).
The extension C ∨x ∨y is blocked with respect to F, so C is covered. However,
C is not PR with respect to F. To see this, suppose to the contrary that ω is a
PR witness for C. Similar to the reasoning in the proof of Theorem 2, assume,
without loss of generality, that ω = {k} ◦ω′ for some ω′ not assigning k. Notice
that (x) ∈F|k, but unit propagation on ¬x ∧F|α stops without producing ⊥.
Therefore x ∈ω′, and ω = {k, x} ◦ω′′ for some ω′′ assigning neither k nor x. By
similar reasoning, it must be the case that ¬y ∈ω′′, so that ω = {k, x, ¬y}◦ω′′′.
Now, (c ∨d) ∈F|{k,x,¬y}, but once more, unit propagation on F|α ∧¬c ∧¬d
does not produce ⊥, so either c or d belongs to ω′′′. Without loss of generality,
assume c ∈ω′′′ so that ω = {k, x, ¬y, c}◦ω′′′′. Finally, both d and ¬d are clauses
in F ′|{k,x,¬y,c}, but neither are implied by F|α by unit propagation. However, if
either d or ¬d belongs to ω, then ⊥∈F|ω. As unit propagation on F|α alone
does not produce ⊥, this is a contradiction.
⊓⊔
Notice the formula in Theorem 3 can be seen as an instance of the formula in
Proposition 2, with G as (a ∨b) ∧(a ∨¬b) ∧(¬a ∨b) ∧(¬a ∨¬b). In fact, as long
as unit propagation on G does not derive ⊥, then G could be any, arbitrarily
hard, unsatisﬁable formula (such as an instance of the pigeonhole principle).
5
Complexity of Redundancy
In the previous section we introduced the witness problem for a redundancy
property (Deﬁnition 5) and showed that it is not trivial, even when the redun-
dancy property itself can be eﬃciently decided. Further, the witness problem for
PR clauses is solvable by encoding it into SAT [16].
Note that PR is considered to be a very general redundancy property. The
proof of theorem 1 in [15,17] shows that if F is satisﬁable and C redundant,
then F ∧C is satisﬁable by deﬁnition. In addition, any satisfying assignment τ
of F ∧C is a PR witness for C with respect to F. This yields the following:
Proposition 3. Let F be a satisﬁable formula. A clause C is redundant with
respect to F if and only if it is a PR clause with respect to F.
While not all covered clauses are PR, this motivates the question of whether
witnesses for all redundancy properties can be encoded as an instance to SAT,
and solved similarly. In this section we show that this is likely not the case by
demonstrating the complexity of the redundancy problem: given a clause C and
a formula F, is C redundant with respect to F?
Deciding whether a clause is PR belongs to NP: assignments can be chosen
non-deterministically and eﬃciently veriﬁed as PR witnesses, since the relation
⊢1 is polynomially decidable [18]. For clause redundancy in general, it is not
clear that this holds, as the corresponding problem is co-NP-complete.

44
L. A. Barnett et al.
Proposition 4. Deciding whether an assignment ω is a witness for a clause C
with respect to a formula F is complete for co-NP.
Proof. The problem belongs to co-NP since F|α ⊨F|ω whenever ¬(F|α) ∨F|ω
is a tautology. In the following we show a reduction from the tautology problem.
Given a formula F, construct the formula F ′ as below, for x ̸∈var(F). Further,
let C′ = x, so that α = ¬x, and let also ω = x.
F ′ =

C∈F
(C ∨¬x)
Then F ′|α = ⊤and F ′|ω = F. Therefore F ′|α ⊨F ′|ω if and only if ⊤⊨F.
⊓⊔
Theorem 4 below shows that the irredundancy problem, the complement of
the redundancy problem, is complete for the class DP, the class of languages
that are the intersection of a language in NP and a language in co-NP [28]:
DP = {L1 ∩L2 | L1 ∈NP and L2 ∈co-NP}.
This class was originally introduced to classify certain problems which are hard
for both NP and co-NP, but do not seem to be complete for either, and it
characterizes a variety of optimization problems. It is the second level of the
Boolean hierarchy over NP, which is the completion of NP under Boolean oper-
ations [5,29]. We provide a reduction from the canonical DP-complete problem,
SAT-UNSAT: given formulas F and G, is F satisﬁable and G unsatisﬁable?
Theorem 4. The irredundancy problem is DP-complete.
Proof. Notice that the irredundancy problem can be expressed as
IRR = {(F, C) | F is satisﬁable, and F ∧C is unsatisﬁable}
= {(F, C) | F ∈SAT} ∩{(F, C) | F ∧C ∈UNSAT}.
That is, IRR is the intersection of a language in NP and a language in co-NP,
and so the irredundancy problem belongs to DP.
Now, let (F, G) be an instance to SAT-UNSAT. Construct the formula F ′ as
follows, for x ̸∈var(F) ∪var(G):
F ′ =

C∈F
(C ∨x) ∧

D∈G
(D ∨¬x).
Further, let C′ = x. We demonstrate that (F, G) ∈SAT-UNSAT if and only if
C′ is irredundant with respect to F ′.
(⇐)
Suppose C′ is irredundant with respect to F ′. In other words, F ′ is satisﬁ-
able but F ′∧C′ is unsatisﬁable. Since F ′∧C′ is unsatisﬁable, it must be the case
that F ′|{x} is unsatisﬁable; however, F ′ is satisﬁable, therefore F ′|{¬x} must be
satisﬁable. Since F ′|{¬x} = F and F ′|{x} = G, then (F, G) ∈SAT-UNSAT.
(⇒)
Now, suppose F is satisﬁable and G is unsatisﬁable. Then some assign-
ment τ over var(F) satisﬁes F. As a result, τ ∪{¬x} satisﬁes F ′. Because G

Covered Clauses Are Not Propagation Redundant
45
is unsatisﬁable, there is no assignment satisfying F ′|{x} = G. This means there
is no σ satisfying both F ′ and C′ = x, and so F ′ ∧C′ is unsatisﬁable as well.
Therefore C′ is irredundant with respect to F ′.
⊓⊔
Consequently the redundancy problem is complete for co-DP. This suggests
that suﬃcient SAT encodings of the clause redundancy problem, and its corre-
sponding witness problem, are not possible.
6
Conclusion
We revisit a strong clause elimination procedure, covered clause elimination,
and provide an explicit algorithm for both deciding its redundancy property and
reconstructing solutions after its use. Covered clause elimination is unique in that
it does not produce redundancy witnesses for clauses it eliminates, and uses a
complex, multi-step reconstruction strategy. We prove that while witnesses exist
for covered clauses, computing such a witness is as hard as ﬁnding a satisfying
assignment for an arbitrary formula.
For PR, a very general redundancy property used by strong proof systems,
witnesses can be found through encodings into SAT. We show that covered
clauses are not described by PR, and SAT encodings for ﬁnding general redun-
dancy witnesses likely do not exist, as deciding clause redundancy is hard for
the class DP, the second level of the Boolean hierarchy over NP.
Directions for future work include the development of redundancy properties
beyond PR, and investigating their use for solution reconstruction after clause
elimination, as well as in proof systems. Extending redundancy notions by using
a structure for witnesses other than partial assignments may provide more gen-
erality while remaining polynomially veriﬁable.
We are also interested in developing notions of redundancy for adding or
removing more than a single clause at a time, and exploring proof systems and
simpliﬁcation techniques which make use of non-clausal redundancy properties.
References
1. Ajtai, M.: The complexity of the pigeonhole principle. Combinatorica 14(4), 417–
433 (1994)
2. Biere, A.: CaDiCaL, Lingeling, Plingeling, Treengeling and YalSAT entering the
SAT competition 2018. In: Heule, M.J.H., J¨arvisalo, M., Suda, M. (eds.) Proceed-
ings of SAT Competition 2018, pp. 13–14. Department of Computer Science Series
of Publications B, University of Helsinki (2018)
3. Biere, A.: CaDiCaL at the SAT race 2019. In: Heule, M.J.H., J¨arvisalo, M., Suda,
M. (eds.) Proceedings of SAT Race 2019, pp. 8–9. Department of Computer Science
Series of Publications B, University of Helsinki (2019)
4. Buss, S., Thapen, N.: DRAT proofs, propagation redundancy, and extended res-
olution. In: Janota, M., Lynce, I. (eds.) SAT 2019. LNCS, vol. 11628, pp. 71–89.
Springer, Cham (2019). https://doi.org/10.1007/978-3-030-24258-9 5

46
L. A. Barnett et al.
5. Cai, J.Y., Hemachandra, L.: The Boolean hierarchy: hardware over NP. In: Selman,
A.L. (ed.) Structure in Complexity Theory. LNCS, vol. 223, pp. 105–124. Springer,
Heidelberg (1986). https://doi.org/10.1007/3-540-16486-3 93
6. Clarke, E., Biere, A., Raimi, R., Zhu, Y.: Bounded model checking using satisﬁa-
bility solving. Formal Methods Syst. Des. 19(1), 7–34 (2001)
7. Fazekas, K., Biere, A., Scholl, C.: Incremental inprocessing in SAT solving. In:
Janota, M., Lynce, I. (eds.) SAT 2019. LNCS, vol. 11628, pp. 136–154. Springer,
Cham (2019). https://doi.org/10.1007/978-3-030-24258-9 9
8. Gableske, O.: BossLS preprocessing and stochastic local search. In: Balint, A.,
Belov, A., Diepold, D., Gerber, S., J¨arvisalo, M., Sinz, C. (eds.) Proceedings of SAT
Challenge 2012, pp. 10–11. Department of Computer Science Series of Publications
B, University of Helsinki (2012)
9. Haken, A.: The intractability of resolution. Theor. Comput. Sci. 39(2–3), 297–308
(1985)
10. Heule, M.J.H.: Schur number ﬁve. In: Proceedings of the 32nd AAAI Conference
on Artiﬁcial Intelligence (AAAI 2018), pp. 6598–6606 (2018)
11. Heule, M.J.H., Biere, A.: What a diﬀerence a variable makes. In: Beyer, D., Huis-
man, M. (eds.) TACAS 2018. LNCS, vol. 10806, pp. 75–92. Springer, Cham (2018).
https://doi.org/10.1007/978-3-319-89963-3 5
12. Heule, M.J.H., J¨arvisalo, M., Biere, A.: Clause elimination procedures for CNF
formulas. In: Ferm¨uller, C.G., Voronkov, A. (eds.) LPAR 2010. LNCS, vol. 6397, pp.
357–371. Springer, Heidelberg (2010). https://doi.org/10.1007/978-3-642-16242-
8 26
13. Heule, M.J.H., J¨arvisalo, M., Biere, A.: Covered clause elimination. In: Voronkov,
A., Sutcliﬀe, G., Baaz, M., Ferm¨uller, C. (eds.) Logic for Programming, Artiﬁcial
Intelligence and Reasoning - LPAR 17 (short paper). EPiC Series in Computing,
vol. 13, pp. 41–46. EasyChair (2010)
14. Heule, M.J.H., J¨arvisalo, M., Lonsing, F., Seidl, M., Biere, A.: Clause elimination
for SAT and QSAT. J. Artif. Intell. Res. 53(1), 127–168 (2015)
15. Heule, M.J.H., Kiesl, B., Biere, A.: Short proofs without new variables. In: de
Moura, L. (ed.) CADE 2017. LNCS (LNAI), vol. 10395, pp. 130–147. Springer,
Cham (2017). https://doi.org/10.1007/978-3-319-63046-5 9
16. Heule, M.J.H., Kiesl, B., Biere, A.: Encoding redundancy for satisfaction-driven
clause learning. In: Vojnar, T., Zhang, L. (eds.) TACAS 2019. LNCS, vol. 11427,
pp. 41–58. Springer, Cham (2019). https://doi.org/10.1007/978-3-030-17462-0 3
17. Heule, M.J.H., Kiesl, B., Biere, A.: Strong extension-free proof systems. J. Autom.
Reason. 64, 533–554 (2020)
18. Heule, M.J.H., Kiesl, B., Seidl, M., Biere, A.: PRuning through satisfaction. In:
Strichman, O., Tzoref-Brill, R. (eds.) HVC 2017. LNCS, vol. 10629, pp. 179–194.
Springer, Cham (2017). https://doi.org/10.1007/978-3-319-70389-3 12
19. Heule, M.J.H., Kullmann, O., Marek, V.W.: Solving and verifying the Boolean
Pythagorean triples problem via Cube-and-Conquer. In: Creignou, N., Le Berre,
D. (eds.) SAT 2016. LNCS, vol. 9710, pp. 228–245. Springer, Cham (2016). https://
doi.org/10.1007/978-3-319-40970-2 15
20. J¨arvisalo, M., Biere, A., Heule, M.J.H.: Blocked clause elimination. In: Esparza,
J., Majumdar, R. (eds.) TACAS 2010. LNCS, vol. 6015, pp. 129–144. Springer,
Heidelberg (2010). https://doi.org/10.1007/978-3-642-12002-2 10
21. J¨arvisalo, M., Heule, M.J.H., Biere, A.: Inprocessing rules. In: Gramlich, B., Miller,
D., Sattler, U. (eds.) IJCAR 2012. LNCS (LNAI), vol. 7364, pp. 355–370. Springer,
Heidelberg (2012). https://doi.org/10.1007/978-3-642-31365-3 28

Covered Clauses Are Not Propagation Redundant
47
22. Kiesl, B., Heule, M.J.H., Biere, A.: Truth assignments as conditional autarkies. In:
Chen, Y.-F., Cheng, C.-H., Esparza, J. (eds.) ATVA 2019. LNCS, vol. 11781, pp.
48–64. Springer, Cham (2019). https://doi.org/10.1007/978-3-030-31784-3 3
23. Kiesl, B., Seidl, M., Tompits, H., Biere, A.: Super-blocked clauses. In: Olivetti,
N., Tiwari, A. (eds.) IJCAR 2016. LNCS (LNAI), vol. 9706, pp. 45–61. Springer,
Cham (2016). https://doi.org/10.1007/978-3-319-40229-1 5
24. Konev, B., Lisitsa, A.: Computer-aided proof of Erd˝os discrepancy properties.
Artif. Intell. 224, 103–118 (2015)
25. Kullmann, O.: On a generalization of extended resolution. Discret. Appl. Math.
96(97), 149–176 (1999)
26. Manthey, N.: Coprocessor 2.0 – a ﬂexible CNF simpliﬁer. In: Cimatti, A., Sebas-
tiani, R. (eds.) SAT 2012. LNCS, vol. 7317, pp. 436–441. Springer, Heidelberg
(2012). https://doi.org/10.1007/978-3-642-31612-8 34
27. Mironov, I., Zhang, L.: Applications of SAT solvers to cryptanalysis of hash func-
tions. In: Biere, A., Gomes, C.P. (eds.) SAT 2006. LNCS, vol. 4121, pp. 102–115.
Springer, Heidelberg (2006). https://doi.org/10.1007/11814948 13
28. Papadimitriou, C., Yannakakis, M.: The complexity of facets (and some facets of
complexity). J. Comput. Syst. Sci. 28(2), 244–259 (1984)
29. Wechsung, G.: On the Boolean closure of NP. In: Budach, L. (ed.) FCT 1985.
LNCS, vol. 199, pp. 485–493. Springer, Heidelberg (1985). https://doi.org/10.1007/
BFb0028832

The Resolution of Keller’s Conjecture
Joshua Brakensiek1, Marijn Heule2(B), John Mackey2, and David Narv´aez3
1 Stanford University, Stanford, CA, USA
2 Carnegie Mellon University, Pittsburgh, PA, USA
marijn@cmu.edu
3 Rochester Institute of Technology, Rochester, NY, USA
Abstract. We consider three graphs, G7,3, G7,4, and G7,6, related to
Keller’s conjecture in dimension 7. The conjecture is false for this di-
mension if and only if at least one of the graphs contains a clique of
size 27 = 128. We present an automated method to solve this conjecture
by encoding the existence of such a clique as a propositional formula.
We apply satisﬁability solving combined with symmetry-breaking tech-
niques to determine that no such clique exists. This result implies that
every unit cube tiling of R7 contains a facesharing pair of cubes. Since
a faceshare-free unit cube tiling of R8 exists (which we also verify), this
completely resolves Keller’s conjecture.
1
Introduction
In 1930, Keller conjectured that any tiling of n-dimensional space by translates
of the unit cube must contain a pair of cubes that share a complete (n −1)-
dimensional face [13]. Figure 1 illustrates this for the plane and the 3-dimensional
space. The conjecture generalized a 1907 conjecture of Minkowski [24] in which
the centers of the cubes were assumed to form a lattice. Keller’s conjecture was
proven to be true for n ≤6 by Perron in 1940 [25,26], and in 1942 Haj´os [6]
showed Minkowski’s conjecture to be true in all dimensions.
Fig. 1. Left, a tiling of the plane (2-dimensional space) with unit cubes (squares). The
bold black edges are fully face-sharing edges. Right, a partial tiling of the 3-dimensional
space with unit cubes. The only way to tile the entire space would result in a fully face-
sharing square at the position of the black squares.
c
⃝Springer Nature Switzerland AG 2020
N. Peltier and V. Sofronie-Stokkermans (Eds.): IJCAR 2020, LNAI 12166, pp. 48–65, 2020.
https://doi.org/10.1007/978-3-030-51074-9_4

The Resolution of Keller’s Conjecture
49
In 1986 Szab´o [28] reduced Keller’s conjecture to the study of periodic tilings.
Using this reduction Corr´adi and Szab´o [3] introduced the Keller graphs: the
graph Gn,s has vertices {0, 1, . . . , 2s −1}n such that a pair are adjacent if and
only if they diﬀer by exactly s in at least one coordinate and they diﬀer in at
least two coordinates. The size of cliques in Gn,s is at most 2n [5] and the size
of the largest clique in Gn,s is at most the size of the largest clique in Gn,s+1.
A clique in Gn,s of size 2n demonstrates that Keller’s conjecture is false
for dimensions greater than or equal to n. Lagarias and Shor [19] showed that
Keller’s conjecture is false for n ≥10 in 1992 by exhibiting clique of size 210 in
G10,2. In 2002, Mackey [22] found a clique of size 28 in G8,2 to show that Keller’s
conjecture is false for n ≥8. In 2011, Debroni, Eblen, Langston, Myrvold, Shor,
and Weerapurage [5] showed that the largest clique in G7,2 has size 124.
In 2015, Kisielewicz and Lysakowska [14,16] made substantial progress on
reducing the conjecture in dimension 7. More recently, in 2017, Kisielewicz [15]
reduced the conjecture in dimension 7 as follows: Keller’s conjecture is true in
dimension 7 if and only if there does not exist a clique in G7,3 of size 27 [21].
The main result of this paper is the following theorem.
Theorem 1. Neither G7,3 nor G7,4 nor G7,6 contains a clique of size 27 = 128.
Although proving this property for G7,3 suﬃces to prove Keller’s conjecture
true in dimension 7, we also show this for G7,4 and G7,6 to demonstrate that our
methods need only depend on prior work of Kisielewicz and Lysakowska [14,16].
In particular, the argument for G7,6 [14] predates and is much simpler than the
one for G7,4 [16] (although the publication dates indicate otherwise). It is not
explicitly stated in either that it suﬃces to prove that G7,4 or G7,6 lacks a clique
of size 128 to prove Keller’s conjecture. We show this in the Appendix of the
extended version, available at https://arxiv.org/abs/1910.03740.
We present an approach based on satisﬁability (SAT) solving to show the ab-
sence of a clique of size 128. SAT solving has become a powerful tool in computer-
aided mathematics in recent years. For example, it was used to prove the Erd˝os
discrepancy conjecture with discrepancy 2 [17], the Pythagorean triples prob-
lem [10], and Schur number ﬁve [7]. Modern SAT solvers can also emit proofs of
unsatisﬁability. There exist formally veriﬁed checkers for such proofs as devel-
oped in the ACL2, Coq, and Isabelle theorem-proving systems [4,20].
The outline of this paper is as follows. After describing some background con-
cepts in Sect. 2, we present a compact encoding whether Gn,s contains a clique
of size 2n as a propositional formula in Sect. 3. Without symmetry breaking,
these formulas with n > 5 are challenging for state-of-the-art tools. However,
the Keller graphs contain many symmetries. We perform some initial symmetry
breaking that is hard to express on the propositional level in Sect. 4. This al-
lows us to partially ﬁx three vertices. On top of that we add symmetry-breaking
clauses in Sect. 5. The soundness of their addition has been mechanically ver-
iﬁed. We prove in Sect. 6 the absence of a clique of size 128 in G7,3, G7,4 and
G7,6. We optimize the proofs of unsatisﬁability obtained by the SAT solver and
certify them using a formally veriﬁed checker. Finally we draw some conclusions
in Sect. 7 and present directions for future research.

50
J. Brakensiek et al.
00
02
33
30
10
11
23
32
22
12
21
20
01
13
31
03
Fig. 2. Illustration of G2,2. The coordinates of the vertices are compactly represented
by a sequence of the digits.
2
Preliminaries
We present the most important background concepts related to this paper and
introduce some properties of Gn,s. First, for positive integers k, we deﬁne two
sets: [k] := {1, 2, . . . , k} and ⟨k⟩:= {0, 1, . . . , k −1}.
Keller Graphs. The Keller graph Gn,s consists of the vertices ⟨2s⟩n. Two vertices
are adjacent if and only if they diﬀer by exactly s in at least one coordinate and
they diﬀer in at least two coordinates. Figure 2 shows a visualization of G2,2.
As noted in [5], {sw + ⟨s⟩n : w ∈{0, 1}n} is a partition of the vertices of
Gn,s into 2n independent sets. Consequently, any clique in Gn,s has at most 2n
vertices. For example, V (G2,2) is partitioned as follows:
{{2(0, 0) + {0, 1}2, 2(0, 1) + {0, 1}2, 2(1, 0) + {0, 1}2, 2(1, 1) + {0, 1}2} =
{{(0, 0), (0, 1), (1, 0), (1, 1)}, {(0, 2), (0, 3), (1, 2), (1, 3)},
{(2, 0), (2, 1), (3, 0), (3, 1)}, {(2, 2), (2, 3), (3, 2), (3, 3)}}.
We use the above observation for encoding whether Gn,s has a clique of
size 2n. Instead of searching for such a clique on the graph representation of
Gn,s, which consists of (2s)n vertices, we search for 2n vertices, one from each
sw + ⟨s⟩n, such that every pair is adjacent.
For every i ∈⟨2n⟩, we let w(i) = (w1, w2, . . . , wn) ∈{0, 1}n be deﬁned by
i = n
k=1 2k−1 · wk. Given a clique of size 2n, we let ci be its unique element in
sw(i) + ⟨s⟩n and we let ci,j be the jth coordinate of ci.

The Resolution of Keller’s Conjecture
51
Useful Automorphisms of Keller Graphs. Let Sn be the set of permutations of
[n] and let Hs be the set of permutations of ⟨2s⟩generated by the swaps (i i+s)
composed with any permutation of ⟨s⟩which is identically applied to s + ⟨s⟩.
The maps
(x1, x2, . . . , xn) →(τ1(xσ(1)), τ2(xσ(2)), . . . , τn(xσ(n))),
where σ ∈Sn and τ1, τ2, . . . , τn ∈Hs are automorphisms of Gn,s. Note that
applying an automorphism to every vertex of a clique yields another clique of
the same size.
Propositional Formulas. We consider formulas in conjunctive normal form (CNF),
which are deﬁned as follows. A literal is either a variable x (a positive literal) or
the negation x of a variable x (a negative literal). The complement l of a literal
l is deﬁned as l = x if l = x and l = x if l = x. For a literal l, var(l) denotes the
variable of l. A clause is a disjunction of literals and a formula is a conjunction
of clauses.
An assignment is a function from a set of variables to the truth values 1 (true)
and 0 (false). A literal l is satisﬁed by an assignment α if l is positive and
α(var(l)) = 1 or if it is negative and α(var(l)) = 0. A literal is falsiﬁed by an as-
signment if its complement is satisﬁed by the assignment. A clause is satisﬁed by
an assignment α if it contains a literal that is satisﬁed by α. A formula is satisﬁed
by an assignment α if all its clauses are satisﬁed by α. A formula is satisﬁable if
there exists an assignment that satisﬁes it and unsatisﬁable otherwise.
Clausal Proofs. Our proof that Keller’s conjecture is true for dimension 7 is
predominantly a clausal proof, including a large part of the symmetry breaking.
Informally, a clausal proof system allows us to show the unsatisﬁability of a
CNF formula by continuously deriving more and more clauses until we obtain
the empty clause. Thereby, the addition of a derived clause to the formula and
all previously derived clauses must preserve satisﬁability. As the empty clause
is trivially unsatisﬁable, a clausal proof shows the unsatisﬁability of the original
formula. Moreover, it must be checkable in polynomial time that each derivation
step does preserve satisﬁability. This requirement ensures that the correctness
of proofs can be eﬃciently veriﬁed. In practice, this is achieved by allowing only
the derivation of speciﬁc clauses that fulﬁll some eﬃciently checkable criterion.
Formally, clausal proof systems are based on the notion of clause redundancy.
A clause C is redundant with respect to a formula F if adding C to F preserves
satisﬁability. Given a formula F = C1 ∧· · · ∧Cm, a clausal proof of F is a
sequence (Cm+1, ωm+1), . . . , (Cn, ωn) of pairs where each Ci is a clause, each ωi
(called the witness) is a string, and Cn is the empty clause [9]. Such a sequence
gives rise to formulas Fm, Fm+1, . . . , Fn, where Fi = C1 ∧· · · ∧Ci. A clausal
proof is correct if every clause Ci (i > m) is redundant with respect to Fi−1,
and if this redundancy can be checked in polynomial time (with respect to the
size of the proof) using the witness ωi.

52
J. Brakensiek et al.
An example for a clausal proof system is the resolution proof system, which
only allows the derivation of resolvents (with no or empty witnesses). How-
ever, the resolution proof system does not allow to compactly express symmetry
breaking. Instead we will construct a proof in the resolution asymmetric tautol-
ogy (RAT) proof system. This proof system is also used to validate the results
of the SAT competitions [11]. For the details of RAT, we refer to the original
paper [9]. Here, we just note that (1) for RAT clauses, it can be checked eﬃ-
ciently that their addition preserves satisﬁability, and (2) every resolvent is a
RAT clause but not vice versa.
3
Clique Existence Encoding
Recall that Gn,s has a clique of size 2n if and only if there exist vertices ci ∈
sw(i)+⟨s⟩n for all i ∈⟨2n⟩such that for all i ̸= i′ there exists at least two j ∈[n]
such that ci,j ̸= ci′,j and there exists at least one j ∈[n] such that ci,j = ci′,j ±s.
Our CNF will encode the coordinates of the ci. For each i ∈⟨2n⟩, j ∈[n],
k ∈⟨s⟩, we deﬁne Boolean variables xi,j,k which are true if and only if ci,j =
sw(i)j + k. We therefore need to encode that exactly one of xi,j,0, xi,j,1, . . .,
xi,j,s−1 is true. We use the following clauses
∀i ∈⟨2n⟩, ∀j ∈[n], (xi,j,0 ∨xi,j,1 ∨· · · ∨xi,j,s−1) ∧

k<k′∈⟨s⟩
(xi,j,k ∨xi,j,k′).
(1)
Next we enforce that every pair of vertices ci and ci′ in the clique diﬀer in at
least two coordinates. For most pairs of vertices, no clauses are required because
w(i) and w(i′) diﬀer in at least two positions. Hence, a constraint is only required
for two vertices if w(i) and w(i′) diﬀer in exactly one coordinate.
Let ⊕be the binary XOR operator and ej be the indicator vector of the jth
coordinate. If w(i) ⊕w(i′) = ej, then in order to ensure that ci and ci′ diﬀer
in at least two coordinates we need to make sure that there is some coordinate
j′ ̸= j for which ci,j′ ̸= ci′,j′
∀i ̸= i′ ∈⟨2n⟩s.t. w(i) ⊕w(i′) = ej,

j′∈[n]\{j},k∈⟨s⟩
(xi,j′,k ̸= xi′,j′,k).
(2)
We use the Plaisted-Greenbaum [27] encoding to convert the above constraint
into CNF. We refer to the auxiliary variables introduced by the encoding as
yi,i′,j′,k, which if true imply xi,j′,k ̸= xi′,j′,k, or written as an implication
yi,i′,j′,k →(xi,j′,k ̸= xi′,j′,k)
The following two clauses express this implication
(yi,i′,j′,k ∨xi,j′,k ∨xi′,j′,k) ∧(yi,i′,j′,k ∨xi,j′,k ∨xi′,j′,k)
(3)

The Resolution of Keller’s Conjecture
53
Notice that the implication is only in one direction as Plaisted-Greenbaum
takes the polarity of constraints into account. The clauses that represent the
other direction, i.e., (yi,i′,j′,k ∨xi,j′,k ∨xi′,j′,k) and (yi,i′,j′,k ∨xi,j′,k ∨xi′,j′,k) are
redundant (and more speciﬁcally, they are blocked [18]).
Using the auxiliary variables, we can express the constraint (2) using clauses
of length s · (n −1)
∀i ̸= i′ ∈⟨2n⟩s.t. w(i) ⊕w(i′) = ej,

j′∈[n]\{j},k∈⟨s⟩
yi,i′,j′,k.
(4)
The last part of the encoding consists of clauses to ensure that each pair of
vertices in the clique have at least one coordinate in which they diﬀer by exactly
s. Observe that ci,j = ci′,j ± s implies that ci,j ̸= ci′,j and xi,j,k = xi′,j,k for all
k ∈⟨s⟩. We use auxiliary variables zi,i′,j, whose truth implies ci,j = ci′,j ± s, or
written as an implication
∀i ̸= i′ ∈⟨2n⟩, ∀j ∈[n] s.t. ci,j ̸= ci′,j,
zi,i′,j →

(xi,j,0 = xi′,j,0) ∧· · · ∧(xi,j,s−1 = xi′,j,s−1)

.
Notice that the implication is again in one direction only. Below we enforce
that some zi,i′,j variables must be true, but there are no constraints that enforce
zi,i′,j variables to be false.
This can be written as a CNF using the following clauses:

k∈⟨s⟩

(zi,i′,j ∨xi,j,k ∨xi′,j,k) ∧(zi,i′,j ∨xi,j,k ∨xi′,j,k)

(5)
Finally, to make sure that ci,j = ci′,j ± s for some j ∈[n], we specify
∀i ̸= i′ ∈⟨2n⟩,

j:ci,j̸=ci′,j
zi,i′,j.
(6)
The variables and clauses, including precise formulas for their counts, are
summarized in Table 1. The sizes of the CNF encodings (before the addition of
symmetry breaking clauses) of G7,3, G7,4, and G7,6 are listed in Table 2. Notice
that for ﬁxed n, the dependence on s is quadratic, which is better than the
s2n dependence one would get in the naive encoding of Gn,s as a graph. This
compact encoding, when combined with symmetry breaking, is a core reason
that we were able to prove Theorem 1.
The instances with n = 7 are too hard for state-of-the-art SAT solvers
if no symmetry breaking is applied. We experimented with general-purpose
symmetry-breaking techniques, similar to the symmetry-breaking predicates pro-
duced by shatter [1]. This allows solving the formula for G7,3, but the com-
putation takes a few CPU years. The formulas for G7,4 and G7,6 with these
symmetry-breaking predicates are signiﬁcantly harder.
Instead we employ problem-speciﬁc symmetry breaking by making use of
the observations in Sects. 4 and 5. This allows solving the clique of size 2n
existence problem for all three graphs in reasonable time.

54
J. Brakensiek et al.
Table 1. Summary of variable and clause counts in the CNF encoding.
Clauses
New Variable Count
Clause Count
(1)
2n · n · s
2n · n · (1 +
s
2

)
(3)
2n−1 · n · s · (n −1)
2n · n · s · (n −1)
(4)
2n−1 · n
(5)
22n−2 · n
22n−1 · n · s
(6)
2n
2

Total
2n−1 · n · (s(n + 1) + 2n−1)
2n · n ·
 3
2 +
s
2

+ n · s −s

+ 22n−1ns +
2n
2

4
Initial Symmetry Breaking
Our goal is to prove that there exists no clique of size 128 in G7,s for s ∈{3, 4, 6}.
In this section, and the subsequent, we assume that such a clique exists and adapt
some of the arguments of Perron [25,26] to show that it may be assumed to have
a canonical form. We will use ⋆i to denote an element in ⟨i⟩.
Lemma 1. If there is a clique of size 128 in G7,s, then there is a clique of size
128 in G7,s containing the vertices (0, 0, 0, 0, 0, 0, 0) and (s, 1, 0, 0, 0, 0, 0).
Proof. Let K be a clique of size 128 in G7,s. Consider the following sets of
vertices in G6,s:
K<s := {(v2, . . . , v7) | ∃v1 ∈⟨s⟩s.t. (v1, . . . , v7) ∈K}
and
K≥s := {(v2, . . . , v7) | ∃v1 ∈s + ⟨s⟩s.t. (v1, . . . , v7) ∈K}.
Every pair of vertices in K<s diﬀers by exactly s in at least one coordinate,
because the corresponding pair of vertices in K can’t diﬀer by exactly s in the
ﬁrst coordinate. Similarly, every pair of vertices in K≥s diﬀers by exactly s in at
least one coordinate. Although K<s and K≥s are not necessarily cliques in G6,s,
they satisfy the ﬁrst condition of the adjacency requirement. The partition of
Sect. 2 can thus be applied to deduce that |K<s| ≤64 and |K≥s| ≤64. Since
|K<s| + |K≥s| = 128, we conclude that |K<s| = 64 and |K≥s| = 64.
By the truth of Keller’s conjecture in dimension 6, K<s is not a clique in G6,s.
Thus, some pair of vertices in K<s are identical in ﬁve of the six coordinates.
Table 2. Summary of variable and clause counts of the CNF encoding for G7,3, G7,4,
and G7,6. These counts do not include the clauses introduced by the symmetry breaking.
Keller Graph
Variable Count
Clause Count
G7,3
39 424
200 320
G7,4
43 008
265 728
G7,6
50 176
399 232

The Resolution of Keller’s Conjecture
55
After application of an automorphism, we may without loss of generality assume
that this pair is (s, 0, 0, 0, 0, 0) and (0, 0, 0, 0, 0, 0). Since the pair comes from K<s,
there exist v1 ̸= v′
1 ∈⟨s⟩such that (v1, s, 0, 0, 0, 0, 0) and (v′
1, 0, 0, 0, 0, 0, 0) are
in the clique.
After application of an automorphism that moves v1 to 1 and v′
1 to 0, we de-
duce that without loss of generality (1, s, 0, 0, 0, 0, 0) and (0, 0, 0, 0, 0, 0, 0) are in
the clique. Application of the automorphism that interchanges the ﬁrst two co-
ordinates yields a clique of size 128 containing the vertices c0 = (0, 0, 0, 0, 0, 0, 0)
and c1 = (s, 1, 0, 0, 0, 0, 0).
□
Theorem 2. If there is a clique of size 128 in G7,s, then there is a clique of
size 128 in G7,s containing the vertices (0, 0, 0, 0, 0, 0, 0), (s, 1, 0, 0, 0, 0, 0), and
(s, s + 1, ⋆2, ⋆2, 1, 1, 1).
Proof. Using the preceding lemma, we can choose from among all cliques of size
128 that contain c0 = (0, 0, 0, 0, 0, 0, 0) and c1 = (s, 1, 0, 0, 0, 0, 0), one in which
c3 has the fewest number of coordinates equal to 0. Let λ be this least number.
Observe that the ﬁrst two coordinates of c3 must be (s, s + 1) in order for it
to be adjacent with both c0 and c1. Thus, we have
c0 = (0 ,
0
, 0 , 0 , 0 , 0 , 0 )
c1 = (s ,
1
, 0 , 0 , 0 , 0 , 0 )
c3 = (s , s + 1 , ⋆s , ⋆s , ⋆s , ⋆s , ⋆s)
In the above, we can apply automorphisms that ﬁx 0 in the last ﬁve coor-
dinates to replace ⋆s by ⋆2. We can apply an automorphism that permutes the
last ﬁve coordinates to assume that the 0’s and 1’s in c3 are sorted in increasing
order. Notice that not all of the ⋆2 coordinates in c3 can be 0, because c1 and
c3 are adjacent and must therefore diﬀer in at least two coordinates. Hence at
least the last coordinate of c3 is 1.
Case 1) λ = 4. In this case c3 = (s, s + 1, 0, 0, 0, 0, 1). In order for c67 to be
adjacent with c0, c1, and c3, it must start with (s, s + 1) and end with s + 1:
c0 = (0 ,
0
, 0 , 0 , 0 , 0 ,
0
)
c1 = (s ,
1
, 0 , 0 , 0 , 0 ,
0
)
c3 = (s , s + 1 , 0 , 0 , 0 , 0 ,
1
)
c67 = (s , s + 1 , ⋆s , ⋆s , ⋆s , ⋆s , s + 1)
Not all ⋆s elements in c67 can be 0, because c3 and c67 diﬀer in at least two
coordinates. However, if one of the ⋆s elements in c67 is nonzero, then we can
swap 1 and s + 1 in the last coordinate to obtain a clique in which c3 has three
or fewer coordinates equal to 0, contradicting λ = 4. Thus, λ ≤3.
Case 2) λ = 3, in which case c3 = (s, s + 1, 0, 0, 0, 1, 1):
c0 = (0 ,
0
, 0 , 0 , 0 ,
0
,
0
)
c1 = (s ,
1
, 0 , 0 , 0 ,
0
,
0
)
c3 = (s , s + 1 , 0 , 0 , 0 ,
1
,
1
)
c35 = (s , s + 1 , ⋆s , ⋆s , ⋆s , s + 1 , ⋆s )
c67 = (s , s + 1 , ⋆s , ⋆s , ⋆s , ⋆s
, s + 1)

56
J. Brakensiek et al.
Since c67 is adjacent with c0, c1, and c3, it must start with (s, s + 1) and
end with s + 1. Similarly, since c35 is adjacent with c0, c1, and c3, it must start
with (s, s + 1) and have s + 1 as its penultimate coordinate. Since c35 and c67
are adjacent, either the last coordinate of c35 must be 1, or the penultimate
coordinate of c67 must be 1. Without loss of generality we can assume that the
penultimate coordinate of c67 is 1 as we can permute the last two coordinates
which would swap c35 and c67 without involving the other cubes. The remaining
three ⋆s elements in c67 cannot all be 0, since c3 and c67 diﬀer in at least two
coordinates. However, if one of the ⋆s elements is non-zero, then we can swap
1 and s + 1 in the last coordinate to obtain a clique in which c3 has two or
fewer coordinates equal to 0, contradicting λ = 3. Thus, we have λ ≤2 and
c3 = (s, s + 1, ⋆2, ⋆2, 1, 1, 1), as desired.
□
Notice that most of the symmetry breaking discussed in this section is chal-
lenging, if not impossible, to break on the propositional level: The proof of
Lemma 1 uses the argument that Keller’s conjecture holds for dimension 6,
while the proof of Theorem 2 uses the interchangeability of 1 and s + 1, which
is not a symmetry on the propositional level. We will break these symmetries
by adding some unit clauses to the encoding. All additional symmetry breaking
will be presented in the next section and will be checked mechanically.
5
Clausal Symmetry Breaking
Our symmetry-breaking approach starts with enforcing the initial symmetry
breaking: We assume that vertices c0 = (0, 0, 0, 0, 0, 0, 0), c1 = (s, 1, 0, 0, 0, 0, 0)
and c3 = (s, s + 1, ⋆s, ⋆s, 1, 1, 1) are in our clique K, which follows from Theo-
rem 2. We will not use the observation that ⋆s occurrences in c3 can be reduced
to ⋆2 and instead add and validate clauses that realize this reduction.
We ﬁx the above initial vertices by adding unit clauses to the CNF encoding.
This is the only part of the symmetry breaking that is not checked mechanically.
Let Φ7,s be the formula obtained from our encoding in Sect. 3 together with
the unit clauses corresponding to the 19 coordinates ﬁxed among c0, c1 and c3.
In this section we will identify several symmetries in Φ7,s that can be further
broken at the CNF level by adding symmetry breaking clauses. The formula
ultimately used in Sect. 6 for the experiments is the result of adding these
symmetry breaking clauses to Φ7,s. Symmetry breaking clauses are added in an
incremental fashion. For each addition, a clausal proof of its validity with respect
to Φ7,s and the clauses added so far is generated, as well. Each of these clausal
proofs has been validated using the drat-trim proof checker.
Our approach can be described in general terms as identifying groups of co-
ordinates whose assignments exhibit interesting symmetries and calculating the
equivalence classes of these assignments. Given a class of symmetric assignments,
it holds that one of these assignments can be extended to a clique of size 128
if and only if every assignment in that class can be extended as well. It is then
enough to pick a canonical representative for each class, add clauses forbidding

The Resolution of Keller’s Conjecture
57
every assignment that is not canonical, and ﬁnally determine the satisﬁability
of the formula under the canonical representative of every class of assignments:
if no canonical assignment can be extended to a satisfying assignment for the
formula, then the formula is unsatisﬁable. In order to forbid assignments that
are not canonical, we use an approach similar to the one described in [8].
5.1
The Last Three Coordinates of c19, c35 and c67
The reasoning in the proof of Theorem 2 leads to the following forced settings,
once we assign c3 = (s, s + 1, ⋆s, ⋆s, 1, 1, 1) and apply unit propagation:
(c19,1, c19,2, c19,5) = (s, s + 1, s + 1),
(c35,1, c35,2, c35,6) = (s, s + 1, s + 1),
(c67,1, c67,2, c67,7) = (s, s + 1, s + 1).
Let’s now focus on the 3 × 3 matrix of the coordinates below and do a case
split on all of the s6 possible assignments of coordinates labeled with ⋆s.
5
6
7
c19 s + 1
⋆s
⋆s
c35
⋆s
s + 1
⋆s
c67
⋆s
⋆s
s + 1
Notice, however, that since the only positions in which c19 and c35 can diﬀer
by exactly s are positions 5 and 6, and since c19,5 and c35,6 are already set to
s+1, at least one of c19,6 and c35,5 has to be set to 1. Similarly, it is not possible
for both c35,7 and c67,6 to not be 1 and for both c67,5 and c19,7 to not be 1.
By the inclusion-exclusion principle, this reasoning alone discards 3(s −1)2s4 −
3(s −1)4s2 + (s −1)6 cases. All of these cases can be blocked by adding the
binary clauses: (x19,6,1 ∨x35,5,1) ∧(x35,7,1 ∨x67,6,1) ∧(x67,5,1 ∨x19,7,1). These
three clauses are RAT clauses [12] with respect to the formula Φ7,s.
Furthermore, among the remaining (2s −1)3 cases, several assignment pairs
are symmetric. For example, the following two assignments are symmetric be-
cause one can be obtained from the other by swapping columns and rows:
5
6
7
c19 s + 1
1
2
c35
2
s + 1
2
c67
1
1
s + 1
5
6
7
c19 s + 1
1
1
c35
2
s + 1
1
c67
2
2
s + 1
As with many problems related to symmetries, we can encode each assign-
ment as a vertex-colored graph and use canonical labeling algorithms to de-
termine a canonical assignment representing all the symmetric assignments of
each equivalence class, and which assignments are symmetric to each canonical
form. Our approach is similar to the one by McKay and Piperno for isotopy of
matrices [23].
This additional symmetry breaking reduces the number of cases for the last
three coordinates of the vertices c19, c37, and c67 from the trivial s6 to 25 cases
for s = 3 and 28 cases for s ≥4. Figure 3 shows the 25 canonical cases for s = 3.

58
J. Brakensiek et al.
(0, 0, 1, 0, 1, 1)
(0, 0, 1, 1, 1, 1)
(0, 0, 1, 1, 1, 2)
(0, 1, 1, 0, 0, 1)
(0, 1, 1, 0, 1, 1)
(0, 1, 1, 0, 2, 1)
(0, 1, 1, 1, 0, 2)
(0, 1, 1, 1, 1, 0)
(0, 1, 1, 1, 1, 1)
(0, 1, 1, 1, 1, 2)
(0, 1, 1, 1, 2, 0)
(0, 1, 1, 1, 2, 1)
(0, 1, 1, 1, 2, 2)
(0, 1, 1, 2, 1, 1)
(0, 1, 1, 2, 2, 1)
(0, 2, 1, 1, 1, 1)
(0, 2, 1, 1, 1, 2)
(0, 2, 1, 2, 1, 1)
(1, 1, 1, 1, 1, 1)
(1, 1, 1, 1, 1, 2)
(1, 1, 1, 1, 2, 2)
(1, 1, 1, 2, 2, 1)
(1, 1, 2, 1, 2, 1)
(1, 1, 2, 1, 2, 2)
(1, 2, 2, 1, 1, 2)
Fig. 3. The 25 canonical cases for s = 3. Each vector corresponds to the values of the
coordinates (c19,6, c19,7, c35,5, c35,7, c67,5, c67,6).
5.2
Coordinates Three and Four of Vertices c3, c19, c35 and c67
The symmetry breaking in the previous subsection allows us to ﬁx, without loss
of generality, the last coordinate of c19 to 1. It also constrains the third and
fourth coordinates of c3 to take values in ⟨2⟩instead of ⟨s⟩.
We break the computation into further cases by enumerating over choices for
the third and fourth coordinates of vertices c3, c19, c37, and c67 (Fig. 4). Up to this
point, our description of the partial clique is invariant under the permutations
of ⟨s −1⟩in the third and fourth coordinates as well as swapping the third and
fourth coordinates. With respect to these automorphisms, for s = 3 there are
only 861 equivalence classes for how to ﬁll in the ⋆s cases for these four vertices.
For s = 4 there are 1326 such equivalence classes, and for s = 6 there are 1378
such equivalence classes. This gives a total of 25 × 861 = 21 525 cases to check
for s = 3, 28 × 1326 = 37 128 cases to check for s = 4, and 28 × 1378 = 38 584
cases to check for s = 6.
5.3
Identifying Hardest Cases
In initial experiments we observed for each s ∈{3, 4, 6} that out of the many
thousands of subformulas (cases), one subformula was signiﬁcantly harder to
solve compared to the other subformulas. Figure 5 shows the coordinates of the
key vertices of this subformula for s ∈{3, 4, 6}. Notice that the third and fourth
coordinates are all 0 for all the key vertices. We therefore applied additional
symmetry breaking in case all of these coordinates are 0. Under this case, the
third and the fourth coordinates of vertex c2 can be restricted to (0, 0), (0, 1),
c0 = (0 ,
0
, 0 , 0 ,
0
,
0
,
0
)
c1 = (s ,
1
, 0 , 0 ,
0
,
0
,
0
)
c3 = (s , s + 1 , ⋆2 , ⋆2 ,
1
,
1
,
1
)
c19 = (s , s + 1 , ⋆3 , ⋆3 , s + 1 ,
⋆3
,
1
)
c35 = (s , s + 1 , ⋆4 , ⋆4 ,
⋆3
, s + 1 ,
⋆3 )
c67 = (s , s + 1 , ⋆5 , ⋆5 ,
⋆4
,
⋆4
, s + 1)
Fig. 4. Part of the symmetry breaking on the key vertices. The bold coordinates show
the (unveriﬁed) initial symmetry breaking. The bold s and s + 1 coordinates in c1 and
c3 are also implied by unit propagation. The additional symmetry breaking is validated
by checking a DRAT proof expressing the symmetry breaking clauses.

The Resolution of Keller’s Conjecture
59
c0
c1
c3
c19
c35
c67
(0, 0, 0, 0, 0, 0, 0)
(3, 1, 0, 0, 0, 0, 0)
(3, 4, 0, 0, 1, 1, 1)
(3, 4, 0, 0, 4, 0, 1)
(3, 4, 0, 0, 1, 4, 0)
(3, 4, 0, 0, 0, 1, 4)
(0, 0, 0, 0, 0, 0, 0)
(4, 1, 0, 0, 0, 0, 0)
(4, 5, 0, 0, 1, 1, 1)
(4, 5, 0, 0, 5, 0, 1)
(4, 5, 0, 0, 1, 5, 0)
(4, 5, 0, 0, 0, 1, 5)
(0, 0, 0, 0, 0, 0, 0)
(6, 1, 0, 0, 0, 0, 0)
(6, 7, 0, 0, 1, 1, 1)
(6, 7, 0, 0, 7, 0, 1)
(6, 7, 0, 0, 1, 7, 0)
(6, 7, 0, 0, 0, 1, 7)
Fig. 5. The hardest instance for s = 3 (left), s = 4 (middle), and s = 6 (right).
and (1, 1), and the last three coordinates of c2 can only take values in ⟨3⟩. Fur-
thermore, any assignment (a, b, c) to the last three coordinates of c2 is symmetric
to the same assignment “shifted right”, i.e. (c, a, b), by swapping columns and
rows appropriately. These symmetries deﬁne equivalence classes of assignments
that can also be broken at the CNF level. Under the case shown in Fig. 5,
there are only 33 non-isomorphic assignments remaining for vertex c2 for s ≥3.
We replace the hard case for each s ∈{3, 4, 6} by the corresponding 33 cases,
thereby increasing the total number of cases mentioned above by 32.
5.4
SAT Solving
Each of the cases was solved using a SAT solver, which produced a proof of
unsatisﬁability that was validated using a formally veriﬁed checker (details are
described in the following section). To ensure that the combined cases cover the
entire search space, we constructed for each s ∈{3, 4, 6} a tautological formula
in disjunctive normal form (DNF). The building blocks of a DNF are conjuctions
of literals known as cube. We will use α as a symbol for cubes as they can also
be considered variable assignments. For each cube α in the DNF, we prove that
the formula after symmetry breaking under α is unsatisﬁable. Additionally, we
mechanically check that the three DNFs are indeed tautologies.
6
Experiments
We used the CaDiCaL1 SAT solver developed by Biere [2] and ran the simula-
tions on a cluster of Xeon E5-2690 processors with 24 cores per node. CaDiCaL
supports proof logging in the DRAT format. We used DRAT-trim [29] to opti-
mize the emitted proof of unsatisﬁability. Afterwards we certiﬁed the optimized
proofs with ACL2check, a formally veriﬁed checker [4]. All of the code that
we used is publicly available on GitHub.2 We have also made the logs of the
computation publicly available on Zenodo.3
1 Commit 92d72896c49b30ad2d50c8e1061ca0681cd23e60 of
https://github.com/arminbiere/cadical.
2 https://github.com/marijnheule/Keller-encode.
3 https://doi.org/10.5281/zenodo.3755116.

60
J. Brakensiek et al.
10−1
100
101
102
103
104
0
5000
10000
15000
20000
runtime in seconds
subformulas sorted based on y-axis
solving
proof optimization
proof certiﬁcation
Fig. 6. Cactus plot of the runtime in seconds (logscale) to solve the 21 557 subformulas
of G7,3 as well as the times to optimize and certify the proofs of unsatisﬁability.
6.1
Results for Dimension 7
Table 3 summarizes the running times are for experiment. The subformula-
solving runtimes for s = 3, 4 and 6 are summarized in cactus plots in Figs. 6,
7 and 8. The combined size of all unsatisﬁability proofs of the subformulas of
s = 6 is 224 gigabyte in the binary DRAT proof format. These proofs contained
together 6.18 · 109 proof steps (i.e., additions of redundant clauses). The DRAT-
trim proof checker only used 6.39·108 proof steps to validate the unsatisﬁability
of all subformulas. In other words, almost 90% of the clauses generated by CaD-
iCaL are not required to show unsatisﬁability. It is therefore likely that a single
DRAT proof for the formula after symmetry breaking can be constructed that
is about 20 gigabytes in size. That is signiﬁcantly smaller compared to other
recently solved problems in mathematics that used SAT solvers [7,10].
Table 3. Summary of solve times for s = 3, 4, 6. Times without a unit are in CPU
hours. “No. Hard” is the number of subformulas which required more than 900 seconds
to solve. “Hardest” is the solve time of the hardest subformula in CPU hours.
s
Tot. Solve
Avg. Solve
Proof Opt.
Proof Cert.
No. Hard
Hardest
3
43.27
7.23 s
22.46
4.98
28 form.
≈1.2
4
77.00
7.46 s
44.00
9.70
62 form.
≈2.7
6
81.85
7.63 s
34.84
14.53
63 form.
≈1.25

The Resolution of Keller’s Conjecture
61
10−1
100
101
102
103
104
0
5000
10000
15000
20000
25000
30000
35000
runtime in seconds
subformulas sorted based on y-axis
solving
proof optimization
proof certiﬁcation
Fig. 7. Cactus plot of the runtime in seconds (logscale) to solve the 37 160 subformulas
of G7,4 as well as the times to optimize and certify the proofs of unsatisﬁability.
We ran all three experiments simultaneously on 20 nodes on the Lonestar
5 cluster and computing on 24 CPUs per node in parallel. All instances were
reported unsatisﬁable and all proofs of unsatisﬁability were certiﬁed by the for-
mally veriﬁed checker. This proves Theorem 1.
6.2
Refuting Keller’s Conjecture in Dimension 8
To check the accuracy of the CNF encoding, we veriﬁed that the generated
formulas for G8,2, G8,3, G8,4 and G8,6 are satisﬁable — thereby conﬁrming that
Keller’s conjecture is false for dimension 8. These instances, by themselves, have
too many degrees of freedom for the solver to ﬁnish. Instead, we added to the
CNF the unit clauses consistent with the original clique found in the paper of
Mackey [22] (as suitably embedded for the larger graphs). Speciﬁcation of the
vertices was per the method in Sect. 3 and 4. These experiments were run on
Stanford’s Sherlock cluster and took less than a second to conﬁrm satisﬁability.
Figure 9 shows an illustration of a clique of size 256 in G8,2. This is the
smallest counterexample for Keller’s conjecture, both in the dimension (n = 8)
as in the number of coordinates (s = 2). The illustration uses a black circle,
black square, white circle, or white square to represent a coordinate set to 0,
1, 2, or 3, respectively. Notice that for each pair of vertices it holds that they
have a complementary (black vs white) circle or square and at least one other
diﬀerent coordinate (black/white or circle/square or both).

62
J. Brakensiek et al.
10−1
100
101
102
103
104
0
5000
10000
15000
20000
25000
30000
35000
runtime in seconds
subformulas sorted based on y-axis
solving
proof optimization
proof certiﬁcation
Fig. 8. Cactus plot of the runtime in seconds (logscale) to solve the 38 616 subformulas
of G7,6 as well as the times to optimize and certify the proofs of unsatisﬁability.
7
Conclusions and Future Work
In this paper, we analyzed maximal cliques in the graphs G7,3, G7,4, and G7,6 by
combining symmetry-breaking and SAT-solving techniques. For the initial sym-
metry breaking we adapt some of the arguments of Perron. Additional symmetry
breaking is performed on the propositional level and this part is mechanically
veriﬁed. We partitioned the resulting formulas into thousands of subformulas
and used a SAT solver to check that each subformula cannot be extended to a
clique of size 128. Additionally, we optimized and certiﬁed the resulting proofs
of unsatisﬁability. As a result, we proved Theorem 1, which resolves Keller’s
conjecture in dimension 7.
In the future, we hope to construct a formally veriﬁed argument for Keller’s
conjecture, starting with a formalization of Keller’s conjecture down to the re-
lation of the existence of cliques of size 2n in Keller graphs and ﬁnally the
correctness of the presented encoding. This eﬀort would likely involve formally
verifying most of the theory discussed in the Appendix of the extended version
of the paper. On top of that, we would like to construct a single proof of un-
satisﬁability that incorporates all the clausal symmetry breaking and the proof
of unsatisﬁability of all the subformulas and validate this proof using a formally
veriﬁed checker.
Furthermore, we would like to extend the analysis to G7,s, including com-
puting the size of the largest cliques for various values of s. Another direction
to consider is to study the maximal cliques in G8,s in order to have some sort of
classiﬁcation of all maximal cliques.

The Resolution of Keller’s Conjecture
63
Fig. 9. Illustration of a clique of 256 vertices in G8,2. Each “dice” with eight dots
represents a vertex, and each dot represents a coordinate. A black circle, black square,
white circle, and white square represent a coordinate set to 0, 1, 2, and 3, respectively.
Acknowledgments. The authors acknowledge the Texas Advanced Computing Cen-
ter (TACC) at The University of Texas at Austin, RIT Research Computing, and
the Stanford Research Computing Center for providing HPC resources that have con-
tributed to the research results reported within this paper. Joshua is supported by an
NSF graduate research fellowship. Marijn and David are supported by NSF grant CCF-
1813993. We thank Andrzej Kisielewicz and Jasmin Blanchette for valuable comments
on an earlier version of the manuscript. We thank William Cooperman for helpful dis-
cussions on a previous attempt at programming simulations to study the half-integral
case. We thank Alex Ozdemir for helpful feedback on both the paper and the codebase.
We thank Xinyu Wu for making this collaboration possible.

64
J. Brakensiek et al.
References
1. Aloul, F.A., Markov, I.L., Sakallah, K.A.: Shatter: eﬃcient symmetry-breaking
for Boolean satisﬁability. In: Proceedings of the 40th Annual Design Automation
Conference, DAC 2003, pp. 836–839. ACM, Anaheim (2003)
2. Biere, A.: CaDiCaL, Lingeling, Plingeling, Treengeling and YalSAT entering the
SAT competition 2018. In: Proceedings of SAT Competition 2018 – Solver and
Benchmark Descriptions. Department of Computer Science Series of Publications
B, vol. B-2018-1, pp. 13–14. University of Helsinki (2018)
3. Corr´adi, K., Szab´o, S.: A combinatorial approach for Keller’s conjecture. Period.
Math. Hungar. 21, 91–100 (1990)
4. Cruz-Filipe, L., Heule, M.J.H., Hunt Jr., W.A., Kaufmann, M., Schneider-Kamp,
P.: Eﬃcient certiﬁed RAT veriﬁcation. In: de Moura, L. (ed.) CADE 26. LNCS
(LNAI), vol. 10395, pp. 220–236. Springer, Cham (2017)
5. Debroni, J., Eblen, J., Langston, M., Myrvold, W., Shor, P.W., Weerapurage, D.:
A complete resolution of the Keller maximum clique problem. In: Proceedings
of the Twenty-Second Annual ACM-SIAM Symposium on Discrete Algorithms,
pp. 129–135. SIAM, Society for Industrial and Applied Mathematics, Philadelphia
(2011)
6. Haj´os, G.: Uber einfache und mehrfache Bedeckung des n-dimensionalen Raumes
mit einen Wurfelgitter. Math. Z. 47, 427–467 (1942)
7. Heule, M.J.H.: Schur number ﬁve. In: Proceedings of the 32nd AAAI Conference
on Artiﬁcial Intelligence (AAAI 2018), pp. 6598–6606. AAAI Press (2018)
8. Heule, M.J.H., Hunt Jr., W.A., Wetzler, N.D.: Expressing symmetry breaking in
DRAT proofs. In: Felty, A.P., Middeldorp, A. (eds.) CADE 2015. LNCS (LNAI),
vol. 9195, pp. 591–606. Springer, Cham (2015)
9. Heule, M.J.H., Kiesl, B., Biere, A.: Short proofs without new variables. In: de
Moura, L. (ed.) CADE 26. LNCS (LNAI), vol. 10395, pp. 130–147. Springer, Cham
(2017)
10. Heule, M.J.H., Kullmann, O., Marek, V.W.: Solving and verifying the Boolean
Pythagorean Triples problem via Cube-and-Conquer. In: Creignou, N., Le Berre,
D. (eds.) SAT 2016. LNCS, vol. 9710, pp. 228–245. Springer, Cham (2016)
11. Heule, M.J.H., Schaub, T.: What’s hot in the SAT and ASP competition. In:
Twenty-Ninth AAAI Conference on Artiﬁcial Intelligence 2015, pp. 4322–4323.
AAAI Press (2015)
12. J¨arvisalo, M., Heule, M.J.H., Biere, A.: Inprocessing rules. In: Gramlich, B., Miller,
D., Sattler, U. (eds.) IJCAR 2012. LNCS (LNAI), vol. 7364, pp. 355–370. Springer,
Heidelberg (2012)
13. Keller, O.H.: ¨Uber die l¨uckenlose Erf¨ullung des Raumes mit W¨urfeln. Journal f¨ur
die reine und angewandte Mathematik 163, 231–248 (1930)
14. Kisielewicz, A.P.: Rigid polyboxes and Keller’s conjecture. Adv. Geom. 17(2), 203–
230 (2017)
15. Kisielewicz, A.P.: Towards resolving Keller’s cube tiling conjecture in dimension
seven. arXiv preprint arXiv:1701.07155 (2017)
16. Kisielewicz, A.P., Lysakowska, M.: On Keller’s conjecture in dimension seven. Elec-
tron. J. Comb. 22(1), P1–16 (2015)
17. Konev, B., Lisitsa, A.: Computer-aided proof of Erd˝os discrepancy properties.
Artif. Intell. 224(C), 103–118 (2015)
18. Kullmann, O.: On a generalization of extended resolution. Discret. Appl. Math.
96–97, 149–176 (1999)

The Resolution of Keller’s Conjecture
65
19. Lagarias, J.C., Shor, P.W.: Keller’s cube-tiling conjecture is false in high dimen-
sions. Bull. Am. Math. Soc. 27(2), 279–283 (1992)
20. Lammich, P.: Eﬃcient veriﬁed (UN)SAT certiﬁcate checking. In: de Moura, L. (ed.)
CADE 26. LNCS (LNAI), vol. 10395, pp. 237–254. Springer, Cham (2017)
21. Lysakowska, M.: Extended Keller graph and its properties. Quaestiones Mathe-
maticae 42(4), 551–560 (2019)
22. Mackey, J.: A cube tiling of dimension eight with no facesharing. Discret. Comput.
Geom. 28(2), 275–279 (2002)
23. McKay, B.D., Piperno, A.: Nauty and Traces User’ Guide (version 2.6). http://
users.cecs.anu.edu.au/∼bdm/nauty/nug26.pdf
24. Minkowski, H.: Diophantische Approximationen. B.G. Teubner, Leipzig (1907)
25. Perron, O.: ¨Uber l¨uckenlose ausf¨ullung desn-dimensionalen raumes durch kongru-
ente w¨urfel. Math. Z. 46(1), 1–26 (1940)
26. Perron, O.: ¨Uber l¨uckenlose ausf¨ullung desn-dimensionalen raumes durch kongru-
ente w¨urfel. ii. Math. Z. 46(1), 161–180 (1940)
27. Plaisted, D.A., Greenbaum, S.: A structure-preserving clause form translation. J.
Symb. Comput. 2(3), 293–304 (1986)
28. Szab´o, S.: A reduction of Keller’s conjecture. Period. Math. Hung. 17(4), 265–277
(1986)
29. Wetzler, N.D., Heule, M.J.H., Hunt, W.A.: DRAT-trim: eﬃcient checking and trim-
ming using expressive clausal proofs. In: Sinz, C., Egly, U. (eds.) SAT 2014. LNCS,
vol. 8561, pp. 422–429. Springer, Cham (2014)

How QBF Expansion Makes Strategy
Extraction Hard
Leroy Chew1 and Judith Clymo2(B)
1 Computer Science Department, Carnegie Mellon University, Pittsburgh, PA, USA
lchew@andrew.cmu.edu
http://www.leroychew.wordpress.com
2 School of Computing, University of Leeds, Leeds, UK
scjc@leeds.ac.uk
Abstract. In this paper we show that the QBF proof checking for-
mat QRAT (Quantiﬁed Resolution Asymmetric Tautologies) by Heule,
Biere and Seidl cannot have polynomial time strategy extraction unless
P=PSPACE. In our proof, the crucial property that makes strategy
extraction PSPACE-hard for this proof format is universal expansion,
even expansion on a single variable.
While expansion reasoning used in other QBF calculi can admit poly-
nomial time strategy extraction, we ﬁnd this is conditional on a property
studied in proof complexity theory. We show that strategy extraction on
expansion based systems can only happen when the underlying proposi-
tional calculus has the property of feasible interpolation.
Keywords: QBF · Proof complexity · QRAT · Strategy extraction ·
Quantiﬁer expansion · Feasible interpolation
1
Introduction
Quantiﬁed Boolean logic is an extension of propositional logic in which variables
may be existentially or universally quantiﬁed. This can allow problems to be
represented more succinctly than is possible in propositional logic. Deciding the
truth of a quantiﬁed Boolean formula (QBF) is PSPACE-complete. Propositional
proof systems can be lifted to the QBF situation by the addition of rules to
handle the universal quantiﬁcation.
In addition to deciding whether a given QBF is true or false it is desirable
that algorithms for solving QBFs can provide veriﬁcation by outputting a proof.
The QRAT proof system [14] is suﬃciently strong to simulate the reasoning steps
of all current QBF solvers and preprocessors and is a candidate for a standard
format for veriﬁcation of solvers.
In many settings it is not simply desirable to know that a QBF is true or false
but also to ﬁnd functions that witness to this. For example, QBFs may be used
to model safety veriﬁcation so that if the QBF is false then the modelled system
is able to reach an unsafe state. It may be important to also know how this state
c
⃝Springer Nature Switzerland AG 2020
N. Peltier and V. Sofronie-Stokkermans (Eds.): IJCAR 2020, LNAI 12166, pp. 66–82, 2020.
https://doi.org/10.1007/978-3-030-51074-9_5

How QBF Expansion Makes Strategy Extraction Hard
67
is reached. If a QBF is true (resp. false) then there must exist Skolem (resp.
Herbrand) functions for the existentially (resp. universally) quantiﬁed variables
that certify this. Substituting the certifying Skolem functions into the original
QBF yields a tautology. Equivalently, substituting Herbrand functions results in
an unsatisﬁable propositional formula. The ability to eﬃciently extract Skolem
or Herbrand functions from the proof output by a QBF solver is called strategy
extraction.
There are generally two main paradigms in QBF solving: QCDCL (Conﬂict
Driven Clause Learning) and QBF expansion. Both of these paradigms borrow
techniques from propositional satisﬁability solving for existential variables, but
they diﬀer in how they handle universal variables. The performance and limi-
tations of these solvers can be analysed by studying proof systems that follow
the solver steps. QCDCL adds the universal reduction rule, such as in the Q-Res
proof system [20]. QBF expansion, on the other hand, adds the universal expan-
sion rule such as in the proof system ∀Exp+Res [16]. Both Q-Res and ∀Exp+Res
are based on the resolution system in propositional logic.
The relationship between the two systems has been studied extensively in
both QBF theory and practice. In [4,16] it was shown that Q-Res and ∀Exp+Res
are incomparable. However the picture becomes much more nuanced on cer-
tain fragments of QBF. Lonsing and Egly ran experiments on QBFs which were
parametrised by the number of quantiﬁer alternations and found better perfor-
mance in the expansion based solvers on formulas with a low number of alterna-
tions [23]. This observation was conﬁrmed in proof complexity in [3] where the
expansion calculus ∀Exp+Res was shown to polynomially simulate the QCDCL
calculus Q-Res on bounded quantiﬁer alternations.
As well as using the calculi Q-Res and ∀Exp+Res to compare the strengths of
the two types of QBF solvers, other properties can be studied for each of these
systems. One very important property is the aforementioned strategy extraction.
Often the strategies are just as important as whether a QBF is true or false. Many
QBF proof systems with the universal reduction rule (from QCDCL) have been
studied and shown to have polynomial time strategy extraction using a technique
from [1] and later generalised in [2,7]. For QBF systems with universal expansion
some strategy extraction results are known using a diﬀerent technique [4,12].
QRAT is a very diﬀerent kind of proof system, not only can it simulate both
the universal reduction and expansion rules but it draws from a stronger form
of propositional reasoning than resolution. With this power it has been shown
to simulate a number of diﬀerent QBF proof systems [17,18].
Strategy extraction on a universal checking format like QRAT would have
certain beneﬁts for the solving community. One could extract a QRAT proof from
a solver and then from that proof separately extract the Skolem/Herbrand func-
tions that give the winning strategy. This would avoid having to extract strate-
gies directly from solvers while they are running which may aﬀect performance.
Conversely, the property of strategy extraction can actually provide a source
of weakness in QBF proof systems. In particular Q-Res can always extract strate-
gies as bounded depth circuits. This means that QBFs with winning strategies

68
L. Chew and J. Clymo
that cannot be expressed in small bounded depth circuits necessarily have large
Q-Res proofs [4]. This is similar to the proof size lower bound technique based on
feasible interpolation [21,24] where if a propositional proof system can extract
Craig interpolants in polynomial time then super-polynomial interpolant size
lower bounds become super-polynomial proof size lower bounds.
It was shown in [13] that Skolem functions to certify that a QBF is true can
be extracted in polynomial time from a QRAT proof. In [8] a partial result was
presented showing that Herbrand functions may be extracted from proofs in a
restricted version of refutational QRAT. Here, we show that it is not possible
in general to eﬃciently extract Herbrand functions certifying falsity from proofs
in QRAT. This is due to QRAT providing short proofs to formulas that have
PSPACE-hard strategies. Thus we show an asymmetry between the refutation
of false QBF and proof of true QBF in the QRAT system. We demonstrate that
this is due to the presence of universal expansion steps which manifest from the
powerful reduction rules in the full QRAT proof system [14,18].
The universal expansion reasoning technique is present in QBF proof sys-
tems other than QRAT, but does not always exhibit the same hardness issues
that we demonstrate for QRAT regarding strategy extraction. For example, the
proof system ∀Exp+Res [16] uses expansion, but allows polynomial time strat-
egy extraction [4]. In this paper we strengthen the important connection, ﬁrst
explored in [5], between strategy extraction and feasible interpolation.
This paper is organised as follows: Sect. 2 introduces the main concepts used
in this paper. We show that strategy extraction in QRAT is PSPACE-hard
in Sects. 3 and 4. In Sect. 5 we look at expansion based systems that do have
strategy extraction. We show that it is necessary for their underlying proof
systems to have feasible interpolation. A suﬃcient condition with a relationship
to feasible interpolation is also shown.
2
Preliminiaries
2.1
Proof Complexity
Let Γ be an alphabet, with Γ∗being the set of strings over Γ. Let L be a language
over Γ, in other words L ⊂Γ∗. A proof system [9] for L is a polynomial time
computable partial function f : Γ⋆⇁Γ⋆with range L. The size |π| of a proof
π in Γ⋆is the number of characters it contains. Proof system f maps a proof
to the theorem it proves (or refutes, in the case of a refutational proof system).
Soundness of f requires that rng(f) ⊆L and completeness that rng(f) ⊇L.
Given a family {ci | i ∈N} of formulas fi ∈L, and a family of f-proofs P =
{pi |, i ∈N} such that f(pi) = ci we can say that pi is polynomially bounded if
|pi| ≤|ci|O(1). An even stronger property is that P is said to be uniform if there
is a polynomial-time function h such that h(ci) = pi.
In propositional logic a literal is a Boolean variable x or its negation ¬x. A
clause is a disjunction of literals. A formula in conjunctive normal form (CNF)
is a conjunction of clauses. Let l be a literal. If l = x then ¯l = ¬x, if l = ¬x

How QBF Expansion Makes Strategy Extraction Hard
69
then ¯l = x. A CNF is naturally understood as a set of clauses, and a clause as a
set of literals. Where it is convenient to do so we will therefore use set notation
C ∈φ and l ∈C to state that clause C appears in φ and literal l appears in
C. It is often convenient to notationally treat clauses as unordered disjunctions
and sets simultaneously, so we can use C ∨l to denote the clause that contains
all literals of clause C and also the literal l if it is not already included, and
D ∪E to denote the disjunction of all literals that appear in either clause D or
E. An assignment τ for a formula φ over n variables is a partial function from
the variables of φ to {0, 1}n. τ(C) is the result of evaluating clause C under
assignment τ, and φ|τ = {τ(C) | C ∈φ}. For formula (or circuit) φ, we deﬁne
φ[b/x] so that all instances of variable x in φ are replaced with b ∈{0, 1}.
2.2
Quantiﬁed Boolean Formulas
Quantiﬁed Boolean formulas (QBF) extend propositional logic by allowing for
Boolean variables to be universally or existentially quantiﬁed [19]. ∀x Ψ is satis-
ﬁed by the same truth assignments as Ψ[0/x]∧Ψ[1/x] and ∃x Ψ is satisﬁed by the
same truth assignments as Ψ[0/x] ∨Ψ[1/x]. In a closed QBF all variables must
be quantiﬁed. A prenex QBF Ψ consists of a preﬁx Π deﬁning how each variable
is quantiﬁed and a propositional part φ called the matrix. We write Ψ = Πφ.
The preﬁx Π has a linearly ordered structure. A PCNF is a QBF in prenex form
and with the propositional part in conjunctive normal form. We consider only
closed PCNFs.
Starting from the left we can assign each variable x a level, denoted lv(x). The
ﬁrst variable has level 1. The level is incremented by 1 every time the quantiﬁer
type changes and otherwise remains the same. It is often convenient to write
quantiﬁers in a preﬁx only when the level changes. When QBF are written in
this way we can think of entire levels quantifying blocks of variables.
2.3
Winning Strategies
A closed prenex QBF is analogous to a two player game with perfect informa-
tion, in which one player is responsible for assigning values to the existentially
quantiﬁed variables and the other to the universally quantiﬁed variables. The
players make assignments according to the quantiﬁer preﬁx, so each level of the
preﬁx corresponds to one turn in the game, moving from left to right. The exis-
tential player wins the game if the formula evaluates to true once all assignments
have been made, the universal player wins if the formula evaluates to false.
A strategy for the universal player on QBF Πφ is a set of rules for making
the assignments to each universal variable u. The rule for setting u must depend
only on variables earlier than (to the left of) u in Π, respecting the idea that
when u is being decided the universal player cannot know what choices will be
made in future turns. If this strategy ensures the universal player always wins
games on Πφ (however the existential player makes assignments), then it is
called a winning strategy. A QBF is false if and only if the universal player has a
winning strategy. Strategies for the existential player are deﬁned analogously. A

70
L. Chew and J. Clymo
refutational proof system is said to admit strategy extraction if and only if it is
possible to eﬃciently (i.e. in polynomial time in the size of the proof) construct a
circuit representing a winning strategy for the universal player from a refutation
of a QBF.
2.4
Expansion Based Proof Systems
Since QBF includes and extends all propositional formulas, proving (or refuting)
QBFs typically involves adapting existing propositional proof systems to deal
with variables that are now quantiﬁed.
One such approach is to take the semantic deﬁnition of the universal quan-
tiﬁer ∀uΨ ≡Ψ[0/u] ∧Ψ[1/u], which can be used as a rule to eliminate universal
quantiﬁers. If Ψ is a QBF then Ψ[0/u] and Ψ[1/u] each contain their own quan-
tiﬁers, so the variables bound by these quantiﬁers would have to be renamed to
avoid repeating the other’s variables. We take a convention of renaming these
variables by putting a partial assignment in the superscript such that the vari-
ables X = {xi | i ∈I} bound in Ψ are renamed X0/u = {x0/u
i
| i ∈I} in
Ψ[0/u] and X1/u = {x1/u
i
| i ∈I} in Ψ[1/u]. Repeated expansions create a
larger superscript e.g.
∀u∃x∀v∃y (¬u ∨x ∨v ∨¬y)
≡∀u∃x∃y0/v ∃y1/v (¬u ∨x ∨0 ∨¬y0/v) ∧(¬u ∨x ∨1 ∨¬y1/v)
≡∀u∃x∃y0/v ∃y1/v (¬u ∨x ∨¬y0/v)
≡∃x0/u ∃x1/u ∃y0/u,0/v ∃y0/u,1/v ∃y1/u,0/v ∃y1/u,1/v
(1 ∨x0/u ∨¬y0/u,0/v) ∧(0 ∨x1/u ∨¬y1/u,0/v)
≡∃x0/u ∃x1/u ∃y0/u,0/v ∃y0/u,1/v ∃y1/u,0/v ∃y1/u,1/v
(x1/u ∨¬y1/u,0/u)
Note that because we started here with a prenex formula, we can maintain
that throughout the expansions. In the end, once we expand all universal vari-
ables we have a prenex QBF with only existential quantiﬁers, this is known
as a full expansion. Deciding the truth of a closed PCNF with only existential
quantiﬁers is simply a propositional satisﬁability problem. If we use a refutation
system S we can attempt to refute the expanded formula.
In fact for any refutational propositional proof system S we can create a
refutational QBF proof system (that is refutationally complete) by taking the
full expansion and showing a contradiction using propositional system S. Such a
system would easily have many exponential lower bounds due to the explosion
caused by the full expansion on a linear number of universal variables.
In practice we can often do better than this. The full expansion gives a large
conjunction and we may only need to use some of the conjuncts in order to prove
a contradiction. This can be tightened up further when the original QBF is a

How QBF Expansion Makes Strategy Extraction Hard
71
prenexed conjunction (like a PCNF), checking whether a conjunct is in the full
expansion can be decided in polynomial time. We deﬁne this formally below.
S+∀Exp0,1 We start with a propositional proof system S and prenex QBF
Ψ = Πφ, where Π is the quantiﬁer preﬁx and φ is a propositional matrix in
variables of Π. We treat φ as a conjunction of formulas.
Let τ be a full assignment to all universal variables and let l be an existentially
quantiﬁed literal. We deﬁne restrictl(τ) to be the partial assignment of τ for all
universal variables whose level (in the preﬁx) is less than that of the variable of
l. Now let us use that to deﬁne Cτ, where C is a propositional formula in both
existential and universal variables.
Cτ is the same as C except that we replace every existential literal l with the
annotated literal lrestrictl(τ) and every universal variable u with its value τ(u).
Deﬁnition 1. The refutational QBF proof system S+∀Exp0,1 allows the instan-
tiation of axiom Cτ, whenever C is a conjunct from the matrix and τ is an
assignment to all universal variables; it generates an S refutation of the con-
junction of the axioms, treating diﬀerently annotated variables as diﬀerent.
An S+∀Exp0,1 proof [2] π of QBF Ψ therefore consists of a propositional S
proof of a sub-conjunction of the full expansion. We denote this conjunction as
subexpπ(Ψ).
A well-known example of S+∀Exp0,1 is ∀Exp+Res [16] which is obtained when
S is propositional resolution. This is the proof system that underlies the reasoning
in the competitive QBF solver RAReQS [15]. Resolution is chosen because of its
use in SAT solving.
2.5
QRAT
The QRAT proof system [14] was introduced as a universal proof checking format
for QBF. It is able to express many QBF preprocessing techniques and proof
systems. QRAT works on a QBF Πφ in PCNF which is modiﬁed throughout the
proof by satisﬁability preserving rules. Clauses may be added, altered or deleted
depending on the current status of Πφ.
QRAT may be used either to prove that a QBF is true or to refute it.
The refutational version of QRAT uses the following rules: Asymmetric Tau-
tology Addition (ATA), Quantiﬁed Resolution Asymmetric Tautology Addition
(QRATA), Quantiﬁed Resolution Asymmetric Tautology Universal (QRATU),
Extended Universal Reduction (EUR), and Clause Deletion. We deﬁne only the
rules that are relevant for this paper, for a full deﬁnition of the QRAT system
please refer to [14].
If C is a clause, then ¯C is the conjunction of the negation of the literals in
C. Unit propagation is a procedure on a formula φ in CNF that builds a partial
assignment τ. This assignment is applied to φ and then for any literal l that
appears in a singleton (unit) clause in the resulting formula, the assignment
satisfying l is added to τ. This is repeated until reaching ﬁx-point, which must
happen in polynomial time in the number of clauses in φ. Unit propagation is

72
L. Chew and J. Clymo
used extensively in QRAT for deciding whether a derivation rule may be applied.
We denote by φ ⊢1 ⊥that unit propagation derives the empty clause from φ.
In the following deﬁnitions, Πφ is a closed PCNF and C a clause not in φ.
Π′ is a preﬁx including the variables of φ and C, Π is identical to Π′ except
that it contains the variables of φ only.
Deﬁnition 2 (Asymmetric Tautology Addition (ATA)).
Suppose φ ∧
¯C ⊢1 ⊥. Then we can make the following inference
Πφ
(ATA)
Π′φ ∧C
Deﬁnition 3 (Outer Clause). Suppose C contains a literal l. Consider all
clauses D in φ with ¯l ∈D. The outer clause Ol
D of D is {k ∈D | lv(k) ≤Π
lv(l), k ̸= ¯l}.
Deﬁnition 4 (Quantiﬁed Resolution Asymmetric Tautology Addition
(QRATA)).
If C contains an existential literal l such that for every D ∈φ
with ¯l ∈D, φ ∧¯C ∧¯Ol
D ⊢1 ⊥then we can derive
Πφ
(QRATA w.r.t. l)
Π′φ ∧C
Note that for Deﬁnition 4 we have used the original deﬁnition of QRATA as
it appears in [14], where it is explicitly stated that new variables can appear
anywhere in the preﬁx. In [13] another deﬁnition is used where new variables
only appear at the end of the preﬁx and are necessarily existential. This paper
is in line with the original paper and recent papers such as [8,17,18].
Deﬁnition 5 (Extended Universal Reduction (EUR)).
Given a clause
C ∨u with universal literal u, consider extending C by
C ←C ∪{k ∈D | lv(k) >Π lv(u) or k = ¯u},
where D ∈φ is any clause with some p : lv(p) >Π lv(u), p ∈C and ¯p ∈D,
until we reach a ﬁx-point denoted C′. If ¯u /∈C′ then we can perform the following
rule.
Πφ ∧(C ∨u) (EUR)
Π′φ ∧C
Π′ diﬀers from Π only in that it does not contain u if u /∈φ ∪C.
We can also deﬁne a weaker version of QRAT, QRAT(UR), which uses uni-
versal reduction instead of EUR.
Deﬁnition 6 (Universal Reduction (UR)). Given a clause C ∨u with uni-
versal literal u such that lv(u) > lv(x) for all existentially quantiﬁed variables x
in C we can apply the following rule.
Πφ ∧(C ∨u) (UR)
Π′φ ∧C

How QBF Expansion Makes Strategy Extraction Hard
73
3
Cheating a QBF Game
It is rumoured that the famous chess players Alekhine and Bogoljubov were
once both separately challenged to a game of correspondence chess by an anony-
mous opportunist. The third player had deviously remembered the moves of
each opponent to play Alekhine’s and Bogoljubov’s moves against each other,
eﬀectively removing themselves from the game. The player was guaranteed to
win or draw in at least one game, and with the money odds against them, they
stood to make a proﬁt.
We see that this devious idea can also be used in the conjunction of QBF two-
player games. We will show that these conjunctions have short QRAT proofs. We
take a QBF and conjunct it with its negation in new variables. We interleave the
preﬁxes so that the existential player plays ﬁrst and the universal player is able
to copy the moves at the right time. The universal player has to win on only one
of the conjuncts and an easy winning strategy is to copy the opponent’s move
for the other side. The easy winning strategy is essential for the short proofs,
but despite the guaranteed win, it is PSPACE-hard to ﬁnd out which game the
universal player wins prior to playing it. In the next section we add an extra
universal variable that requires the calculation of who wins in order to make the
game hard. However we see that expansion allows us to quickly return to the
original easy problem.
In this section we will deﬁne these formulas that conjunct a QBF and its
negation and show how a short QRAT proof can be uniformly obtained.
3.1
Duality Formulas
Let X be the set of variables {x1, . . . , x2n} and φ(X) a CNF in the variables
of X. Then Πφ(X) with preﬁx Π = ∀x1∃x2∀x3 . . . ∃x2n is a closed PCNF. We
also deﬁne a second set of 2n variables X′ = {x′
1, . . . , x′
2n} and an alternative
preﬁx Π′ = ∃x′
1∀x′
2∃x′
3 . . . ∀x′
2n. The QBF Πφ(X) ∧Π′¬φ(X′) is necessarily
false. However this QBF is not in PCNF, which many proof systems require.
Firstly we will transform ¬φ(X′) into a CNF ¯φ(X′, T) via the use of Tseitin
variables T = {tK | K ∈φ(X)}. We overload the ′ notation:
– For literal l if l = xi then l′ = x′
i and if l = ¬xi then l′ = ¬x′
i.
– For each clause K in φ(X) we denote the corresponding clause in φ(X′) as
K′ so that K′ = 
l∈K l′.
We require that ¯φ(X′, T) is true precisely when φ(X′) is false. We will intro-
duce clauses stating that variable tK is true if and only if clause K′ is satisﬁed.
Then φ(X′) is false if and only if at least one tK is false, so we will also add a
clause specifying that this must hold.
¯φ(X′, T) contains the following clauses:
– (¬tK ∨K′) for each clause K in φ(X)
– (¬l′ ∨tK) for each literal l in K and each K in φ(X)
–

K∈φ(X) ¬tK


74
L. Chew and J. Clymo
The next part is the most important – the prenexing of the QBF. We place
every universal variable to the right of its existential counterpart. The auxiliary
T variables must be placed at the end of the preﬁx. Thus, from any PCNF
Ψ = Πφ we generate a formula Duality(Πφ) encoding in PCNF the claim that
both Ψ and its negation are true:
Duality(Πφ) = ∃x′
1∀x1∃x2∀x′
2 . . . ∃x′
2n−1∀x2n−1∃x2n∀x′
2n∃T φ(X)∧¯φ(X′, T)
3.2
Short Proofs of Duality Formulas
In [6], Beyersdorﬀet al. showed short Frege + ∀red proofs of a family of QBFs
that take an input of a graph and state that there is a k-clique (Clique) and
dually that there is no k-clique (Co-Clique). The short proofs exploited the
fact that the Co-Clique part of the formula was structured in a similar way to
the Clique part.
We generalise this approach here for short proofs of the Duality formulas.
First we will give a sketch proof of how this can be done using Frege + ∀red
rules before we show those short proofs formally in QRAT. Frege + ∀red is sim-
ply a propositional Frege system augmented with the ∀red rule for removing
universally quantiﬁed variables. ∀red allows to substitute a Boolean value for
universally quantiﬁed u in a previously derived line, provided that lv(u) > lv(x)
for all existentially quantiﬁed x in the proof line.
The clauses in Duality(Πφ) state 
K (tK ↔K′), 
K ¬tK and  K.
Recall that clause K is identical to clause K′ with all instances of x′
i replaced
with xi (for all i). From assumption 2n
i=1 (xi ↔x′
i) we would ﬁnd a contradiction
in polynomially many Frege steps. The outline of the derivation is given below:

K ¬tK
 K

K (tK ↔K′)
2n
i=1 (xi ↔x′
i)

K (tK ↔K)

K tK
⊥
We therefore conclude that 2n
i=1 ¬(xi ↔x′
i).
Now, starting from the variables quantiﬁed innermost in the preﬁx, we per-
form ∀red on all universally quantiﬁed x′
2j and x2j+1:
¬(x2n ↔0) ∨
2n−1

i=1
¬(xi ↔x′
i) = x2n ∨
2n−1

i=1
¬(xi ↔x′
i)
Reduction can also be done with x′
2j = 1
¬(x2n ↔1) ∨
2n−1

i=1
¬(xi ↔x′
i) = ¬x2n ∨
2n−1

i=1
¬(xi ↔x′
i)
We can resolve these two disjunctions together and conclude 2n−1
i=1
¬(xi ↔x′
i).

How QBF Expansion Makes Strategy Extraction Hard
75
Now x2n−1 is the innermost universally quantiﬁed variable. The same
sequence of steps is applied for each universal variable leading to a contradiction
which completes the proof.
This proof idea works for showing short proofs in QRAT. In fact these proofs
have a uniform structure.
Theorem 1. Given a formula Duality(Πφ) we can in polynomial time construct
a quadratic size QRAT(UR) refutation of Duality(Πφ).
Proof. Let |K| be the number of literals in clause K ∈φ, then | Duality(Πφ)| ≥
|φ| ≥ΣK∈φ|K|. Recall Πφ has 2n variables.
Extension Variables. The refutation begins by using QRATA to introduce exten-
sion variable eqxi for each xi ∈X. Every eqxi is existentially quantiﬁed and
is introduced to the preﬁx so that lv(eqxi) > lv(xi), lv(x′
i) and lv(eqxi) <
lv(xj), lv(x′
j) for all j > i (which is possible since lv(xi), lv(x′
i) < lv(xj), lv(x′
j)
in Duality(Πφ) whenever j > i). For each xi ∈X we use QRATA to add four
clauses:
– (¬xi ∨x′
i ∨¬eqxi)
– (xi ∨¬x′
i ∨¬eqxi)
– (¬xi ∨¬x′
i ∨eqxi)
– (xi ∨x′
i ∨eqxi)
Recall that adding a clause by QRATA requires that we have an existential
literal l in the new clause C such that φ∧¯C ∧¯Ol
D ⊢1 ⊥for all D with ¯l ∈D. For
the ﬁrst two clauses this is vacuously satisﬁed with l = ¬eqxi since eqxi does not
appear positively anywhere in the formula. To add the latter clauses we have
l = eqxi and must consider the two outer clauses (¬xi ∨x′
i) and (xi ∨¬x′
i). The
QRATA condition is satisﬁed for (¬xi∨¬x′
i∨eqxi) because xi∧x′
i∧xi∧¬x′
i ⊢1 ⊥
and xi ∧x′
i ∧¬xi ∧x′
i ⊢1 ⊥, and similarly for the ﬁnal clause.
For each of the original 2n variables in Πφ we have added four clauses of
constant size. Following O(n) steps the formula has increased in length by O(n)
characters.
Non-Equivalence of Xand X′. The next three ATA steps are equivalent to those
in the derivation of 2n
i=1 ¬(xi ↔x′
i) in the sketch proof above.
– (2n
i=1 ¬eqxi ∨tK ∨¯l) for every K ∈φ(X) and every l ∈K
– (2n
i=1 ¬eqxi ∨tK) for every K ∈φ(X)
– (2n
i=1 ¬eqxi)
Each clause has O(n) literals and there are at most |φ| clauses of each type. In
O(|φ|) proof steps the formula has increased in length by O(n|φ|).

76
L. Chew and J. Clymo
Removing the Universal Variables. Finally, we want to derive Aj = (j−1
i=1 ¬eqxi)
for j = 2n . . . 1 (thus j = 1 means that we have derived the empty clause).
Assuming that we already have Aj+1 we can use ATA to add:
– (j−1
i=1 ¬eqxi ∨xj ∨x′
j)
– (j−1
i=1 ¬eqxi ∨¬xj ∨¬x′
j)
In these clauses whichever of xj and x′
j is universally quantiﬁed is innermost
by the construction of Duality(Πφ) and the decision of where to introduce the
variables eqxi in the preﬁx. Without loss of generality, assume x′
j is universally
quantiﬁed so we can use UR to derive clauses (j−1
i=1 ¬eqxi∨xj) and (j−1
i=1 ¬eqxi∨
¬xj), then ATA allows to add the resolvent Aj.
For each of the 2n variables from φ there are ﬁve proof steps in this ﬁnal
part of the refutation, each introducing a new clause of size O(n), and in total
the formula has increased in length by O(n2). The whole refutation therefore
has size O(| Duality(Πφ)|2).
⊓⊔
4
Making Strategies Hard
The formulas Duality(Πφ) have short winning strategies for the universal player,
namely to always play so that xi = x′
i. We know also that one of Πφ(X) or
Π′ ¯φ(X′, T) is false and so has a winning strategy for the universal player. Decid-
ing which subformula is false is PSPACE-hard and the winning strategy for the
false formula could be much more complicated than the strategy for Duality(Πφ).
We introduce formulas exploiting this hardness:
Select(Πφ) = ∀u Q ∃T (φu(X)) ∧(¯φ¬u(X′, T))
where φl(X) =

K∈φ(X)
(K ∨l)
and Q = ∃x′
1∀x1∃x2∀x′
2 . . . ∃x′
2n−1∀x2n−1∃x2n∀x′
2n
4.1
Short Proofs of Select Formulas in QRAT
It was shown in [13] that satisfaction QRAT has strategy extraction, and in [8]
that refutational QRAT(UR) has strategy extraction. In this section we use the
formulas Select(Πφ) to show that refutational QRAT does not have strategy
extraction under a strong complexity assumption.
Theorem 2. QRAT has short uniform proofs of Select(Πφ) for any QBF Πφ.
Proof. The ﬁrst step in the proof is to use Extended Universal Reduction (EUR)
to remove u from all clauses in φu(X) and ¬u from all clauses in ¯φ¬u(X′, T).
Using EUR to reduce l in C requires that ¯l does not appear in C′ (the ﬁx-
point of the inner expansion as given in Deﬁnition 5). In other words, there is
no inner resolution path between any clauses containing the removed literal and

How QBF Expansion Makes Strategy Extraction Hard
77
its negation. We can only add literals to the inner expansion from clauses that
share variables in common with the current inner expansion. However u and ¬u
appear in sections of the formula that have no other variables in common. Hence
we can always reduce u (and ¬u) in Select(Πφ).
Having performed these (polynomially many) EUR steps the formula is iden-
tical to Duality(Πφ), which is uniformly refuted as in Theorem 1.
⊓⊔
Corollary 1. Refutational QRAT does not have strategy extraction unless P =
PSPACE.
Proof. If QRAT has strategy extraction we can decide the truth of closed QBF
in polynomial time – a PSPACE-complete problem.
Given a QBF Πφ, with Π a preﬁx and φ a propositional formula in the
variables of Π, we create the formula Select(Πφ) and then in polynomial time
we can output the proofs as in Theorem 2. Then from the proof of Select(Πφ) we
can extract the strategy for u. Since u is outermost in the preﬁx, this strategy
must be constant. If the strategy sets u = 0 then all clauses in ¯φ¬u(X′, T) are
immediately satisﬁed so we know that the rest of the extracted strategy is a
strategy for Πφ, showing that Πφ is false. Similarly, if the strategy sets u = 1
then it must be the case that ¯φ¬u(X′, T) is false and so, by construction, Πφ is
true. Therefore we have a polynomial time decision procedure for an arbitrary
QBF.
⊓⊔
In fact, the full power of EUR is not required. QRAT(UR) is capable of
refuting the formulas Duality(Πφ), and the initial EUR step can be replaced
by universal expansion of u, producing a formula equivalent to Duality(Πφ)
with renamed variables. Even QBF solvers whose underlying proof system uses
universal reduction to handle universally quantiﬁed variables often employ a
preprocessing stage that includes universal expansion. Our Select(Πφ) formulas
show that a single initial expansion step may be suﬃcient to prevent strategy
extraction.
5
Relation to Feasible Interpolation
The results of the previous section indicate that expansion steps may prevent
strategy extraction. However we have seen many proof systems and solvers that
admit strategy extraction despite using universal expansion. It is clear that the
other rules of the calculus play an important role on whether or not strategy
extraction is admissible.
If we wish to guarantee strategy extraction in our proof systems and solvers,
it may be important for future work to explore suﬃcient conditions for strategy
extraction when using expansion. In this section we instead explore a necessary
condition for strategy extraction.
In Corollary 1 the necessary condition for strategy extraction in QRAT was
that we needed eﬃcient circuits that calculated the truth of Πφ in order to have
strategy extraction for Duality(Πφ). We can think of a strategy for u acting

78
L. Chew and J. Clymo
as a circuit deciding between Πφ and ¬(Πφ). In propositional logic the eﬃ-
cient extraction of these deciding circuits (known as interpolating circuits or
interpolants) from a proof is a well studied technique known as feasible interpo-
lation. In this section, we will use our lower bound technique to place necessary
conditions for strategy extraction on a large class of QBF proof systems.
Given a true propositional implication A(P, Q) →B(P, R) (or, equivalently,
a false conjunction A(P, Q)∧¬B(P, R)) Craig’s interpolation theorem [11], states
that there is an interpolant C(P) in only the joint variables P. Feasible interpo-
lation is a property of proof systems. A proof system has feasible interpolation
[21,24] if and only if there is a polynomial time procedure that takes a proof of
A(P, Q) →B(P, R) as an input and extracts an interpolating circuit C(P).
In [5] Beyersdorﬀet al. lifted a version of the feasible interpolation lower
bound technique from propositional logic to QBF. In Sect. 5 of [5] feasible inter-
polation was linked to strategy extraction by adding an extra universal variable
with similarities to Sect. 4 and how the Select formulas are created from the
Duality formulas.
Theorem 3. Given any propositional refutation system S, if the refutational
QBF proof system S+∀Exp0,1 has strategy extraction then S must have feasible
interpolation.1
Proof. Suppose S+∀Exp0,1 has strategy extraction and we have an S-refutation
π of A(P, Q) ∧B(P, R) with P, Q, R disjoint sets of variables. We will show that
we can ﬁnd an interpolant in polynomial time.
We consider the following QBF
∃P∀u∃Q∃R(A(P, Q) ∨u) ∧(B(P, R) ∨¯u)
We can refute this formula in S+∀Exp0,1 using π. Expansion gives us
(A(P, Q0/u) ∨0) ∧(B(P, R0/u) ∨1) ∧(A(P, Q1/u) ∨1) ∧(B(P, R1/u) ∨0)
but this immediately simpliﬁes to A(P, Q0/u) ∧B(P, R1/u).
We can now refute this using π using Q0/u variables instead of Q, and using
R1/u variables instead of R. The provision here is important for this as one could
make S a pathological proof system that disallowed steps using variables named
as in R0/u, but allowed them named as in R.
We can then extract a strategy for u as a circuit in the variables P. However
this circuit is also an interpolant for A(P, Q) ∧B(P, R).
⊓⊔
In regards to making suﬃcient conditions for strategy extraction using fea-
sible interpolation we can look at the results that have come before. We see that
∀Exp+Res has strategy extraction. This is done by a “round-based” strategy
extraction. This technique gives a winning response for the universal player by
1 Provided the refutations of S work independently of the variable names. This is usu-
ally the case but one could create a pathological proof proof system where annotated
variables are treated diﬀerently to normal ones.

How QBF Expansion Makes Strategy Extraction Hard
79
taking the outermost block of existentially quantiﬁed variables and applying a
restriction to the ∀Exp+Res proof on that block in correspondence to the existen-
tial player’s moves. The universal player can then “read oﬀ”, from the restricted
proof, which universal assignment to the next block of variables is useful in the
proof. The proof can be restricted by the universal assignment and we repeat
until we end up with a complete set of universal responses and a falsiﬁed formula.
The “reading oﬀ” which clauses actually contribute to the proof is a weak
form of feasible interpolation and so we can say we have strategy extraction
for S+∀Exp0,1 whenever refutational proof system S satisﬁes two conditions.
Note that because of Theorem 3 feasible interpolation is implied by these two
conditions (although this can be shown without Theorem 3). The extraction
technique is inspired by the one used in [12], instead here we use it for expansion
systems.
Theorem 4. S+∀Exp0,1 has strategy extraction whenever:
1. S is closed under restriction, meaning that from a refutation πof φ one can
extract in polynomial time an S refutation πϵ of φ|ϵ for any assignment ϵ with
|πϵ| ≤|π|.
2. From any refutation ρ1 in S of A(Q) ∧B(R) where Q, R share no common
variables another refutation ρ2 of either A(Q) or B(R) can be extracted in
polynomial time with |ρ2| ≤|ρ1|.
Resolution would be an example of such a system with both properties. It is
not possible to say for certain that any proof system lacks one or both of these
properties without a separation of P and NP. The ﬁrst property is fairly common,
even in stronger systems, but we do not expect systems such as bounded-depth
Frege to have the second property.
Proof. Suppose we have a closed prenex QBF ∃X∀Y Πφ where Π is a preﬁx in
variables Z and φ is a propositional matrix with variables in X,Y and Z. Now
suppose we have an S+∀Exp0,1 refutation π of ∃X∀Y Πφ. This gives an S proof
π′ of subexpπ(∃X∀Y Πφ), a subset of the full expansion of ∃X∀Y Πφ using π.
We will show that under conditions 1 and 2 we have a polynomial time
procedure that takes any assignment ϵ to X and outputs a response μ in Y and
a Πφ|ϵ,μ refutation in S+∀Exp0,1.
From π′, we can extract π′
ϵ in polynomial time, using condition 1, which
provides an S refutation of subexpπ(∃X∀Y Πφ)|ϵ. For D ∈φ we have that C =
D|ϵ is in φ|ϵ, so every conjunct Dτ|ϵ of subexpπ(∃X∀Y Πφ)|ϵ is also an axiom
Cτ of ∀Y Πφ|ϵ. Therefore, π′
ϵ becomes an S+∀Exp0,1 refutation πϵ of ∀Y Πφ|ϵ.
Now we ﬁnd the universal response in universal variables Y . We separate
Y = {y1 . . . ym} and we can start with a response c to y1 and then ﬁnd an
S+∀Exp0,1 refutation of ∀y2 . . . ymΠφ|ϵ,c/y1. We make sure the proofs do not
increase in size. Then we can repeat this for each variable in Y in turn.
Suppose we have an S+∀Exp0,1 refutation πi of the QBF ∀yi . . . ymΠφ|ϵ,μi,
where μi 1 ≤i ≤m is a Boolean assignment to variables {y1, . . . , yi−1}. The
variables of subexpπi(∀yi . . . ymΠφ|ϵ,μi) can be partitioned into Z0/yi = {zα | z ∈

80
L. Chew and J. Clymo
Z, α(yi) = 0} and Z1/yi = {zα | z ∈Z, α(yi) = 1}. This completely partitions
the variables because yi is leftmost in the preﬁx.
Conjunct C ∈subexpπi(Πφ)|ϵ,μi cannot mix variables Z0/yi and Z1/yi since
the axiom rule in Deﬁnition 1 substitutes one or the other everywhere in the
conjunct. Therefore subexpπi(∀yi . . . ymΠφ|ϵ,μi) can be written as A(Z0/yi) ∧
B(Z1/yi) with S refutation π′
i (based on the S+∀Exp0,1 refutation πi).
We deﬁne a new partial assignment μi+1, which is deﬁned as μi+1(yj) =
μi(yj) for 1 ≤j < i. Now we can use condition 2 to extract from π′
i an S
refutation π′
i+1 of either A(Z0/yi) or B(Z1/yi) in polynomial time. If it is A(Z0/yi)
then we let μi+1(yi) = 0 and if it is B(Z1/yi) then we let μi+1(yi) = 1. π′
i+1 can
be used as part of an S+∀Exp0,1 refutation πi+1 of ∀yi+1 . . . ymΠφ|ϵ,μi+1 as
subexpπi+1(∀yi+1 . . . ymΠφ|ϵ,μi+1) is equal to A(Z0/yi) or B(Z1/yi). Condition 2
guarantees |π′
i+1| ≤|π′
i| so |πi+1| ≤|πi| as well.
Once we get to μm we have a complete assignment to Y and a guarantee
that the remaining QBF game on Πφ|ϵ,μm is false by the S+∀Exp0,1 refutation
πm, with |πm| ≤|π|.
We can repeat this procedure for every universal block and we end up with
the false proposition ⊥and since our proofs are non-increasing in size in each
step we guarantee this can be done in a polynomial time procedure.
⊓⊔
6
Conclusion
We have answered an open question in QBF proof complexity by showing that
refutational QRAT does not have strategy extraction, and have introduced a
family of QBFs witnessing this fact. We have also formalised one condition for
strategy extraction to be present in QBF proof systems using universal expan-
sion. This adds to an existing awareness of the trade-oﬀbetween strength of QBF
proof systems and the ability to oﬀer explanation via winning strategies [4].
In current QBF solvers that use inference from propositional SAT solvers
implementing CDCL the propositional inference used is Resolution and the fea-
sible interpolation property applies. As such it may be possible (as it is indeed
possible in ∀Exp+Res) to have both expansion solving and strategy extraction
together. There is also hope that, because the Cutting Planes proof system [10]
has feasible interpolation [24], strategy extraction may still be compatible with
QBF expansion if we were to use pseudo-Boolean solvers as a propositional com-
ponent in QBF solvers.
However, if SAT solvers are developed with more power than both Resolution
and Cutting Planes, then the problem of having both strategy extraction and
expansion in QBF solvers becomes more serious. Extended Resolution is strong
enough to cause these issues [22], but there is a degree of non determinism
required in choosing the correct extension variables in practice. In the case of
our Select and Duality formulas this task would be to ﬁnd the eq extension
variables, despite the fact we are linearly increasing the number of variables this
way and increasing the search space for the solver.

How QBF Expansion Makes Strategy Extraction Hard
81
References
1. Balabanov, V., Jiang, J.-H.R.: Resolution proofs and skolem functions in QBF
evaluation and applications. In: Gopalakrishnan, G., Qadeer, S. (eds.) CAV 2011.
LNCS, vol. 6806, pp. 149–164. Springer, Heidelberg (2011). https://doi.org/10.
1007/978-3-642-22110-1 12
2. Beyersdorﬀ, O., Bonacina, I., Chew, L.: Lower bounds: from circuits to QBF proof
systems. In: Proceedings of the ACM Conference on Innovations in Theoretical
Computer Science (ITCS 2016), pp. 249–260. ACM (2016)
3. Beyersdorﬀ, O., Chew, L., Clymo, J., Mahajan, M.: Short proofs in QBF expansion.
In: Janota, M., Lynce, I. (eds.) SAT 2019. LNCS, vol. 11628, pp. 19–35. Springer,
Cham (2019). https://doi.org/10.1007/978-3-030-24258-9 2
4. Beyersdorﬀ, O., Chew, L., Janota, M.: Proof complexity of resolution-based QBF
calculi. In: Proceedings of the Symposium on Theoretical Aspects of Computer
Science, pp. 76–89. LIPIcs series (2015)
5. Beyersdorﬀ, O., Chew, L., Mahajan, M., Shukla, A.: Feasible interpolation for QBF
resolution calculi. Logical Methods Comput. Sci. 13(2) (2017). https://doi.org/10.
23638/LMCS-13(2:7)2017. https://lmcs.episciences.org/3702
6. Beyersdorﬀ, O., Chew, L., Mahajan, M., Shukla, A.: Understanding cutting planes
for QBFs. Inf. Comput. 262, 141–161 (2018). https://doi.org/10.1016/j.ic.2018.08.
002. http://www.sciencedirect.com/science/article/pii/S0890540118301184
7. Beyersdorﬀ, O., Hinde, L., Pich, J.: Reasons for hardness in QBF proof systems.
In: Electronic Colloquium on Computational Complexity (ECCC) 24, Report no.
44 (2017). https://eccc.weizmann.ac.il/report/2017/044
8. Chew, L., Clymo, J.: The equivalences of refutational QRAT. In: Janota, M., Lynce,
I. (eds.) SAT 2019. LNCS, vol. 11628, pp. 100–116. Springer, Cham (2019). https://
doi.org/10.1007/978-3-030-24258-9 7
9. Cook, S.A., Reckhow, R.A.: The relative eﬃciency of propositional proof systems.
J. Symb. Logic 44(1), 36–50 (1979)
10. Cook, W.J., Coullard, C.R., Tur´an, G.: On the complexity of cutting-plane proofs.
Discret. Appl. Math. 18(1), 25–38 (1987)
11. Craig, W.: Three uses of the Herbrand-Gentzen theorem in relating model theory
and proof theory. J. Symb. Logic 22(3), 269–285 (1957)
12. Goultiaeva, A., Van Gelder, A., Bacchus, F.: A uniform approach for generat-
ing proofs and strategies for both true and false QBF formulas. In: Walsh, T.
(ed.) International Joint Conference on Artiﬁcial Intelligence IJCAI, pp. 546–553.
IJCAI/AAAI (2011)
13. Heule, M., Seidl, M., Biere, A.: Eﬃcient extraction of Skolem functions from QRAT
proofs. In: Formal Methods in Computer-Aided Design, FMCAD 2014, Lausanne,
Switzerland, 21–24 October 2014, pp. 107–114 (2014). https://doi.org/10.1109/
FMCAD.2014.6987602
14. Heule, M.J.H., Seidl, M., Biere, A.: A uniﬁed proof system for QBF preprocess-
ing. In: Demri, S., Kapur, D., Weidenbach, C. (eds.) IJCAR 2014. LNCS (LNAI),
vol. 8562, pp. 91–106. Springer, Cham (2014). https://doi.org/10.1007/978-3-319-
08587-6 7
15. Janota, M., Klieber, W., Marques-Silva, J., Clarke, E.: Solving QBF with coun-
terexample guided reﬁnement. In: Cimatti, A., Sebastiani, R. (eds.) SAT 2012.
LNCS, vol. 7317, pp. 114–128. Springer, Heidelberg (2012). https://doi.org/10.
1007/978-3-642-31612-8 10

82
L. Chew and J. Clymo
16. Janota, M., Marques-Silva, J.: Expansion-based QBF solving versus Q-resolution.
Theor. Comput. Sci. 577, 25–42 (2015)
17. Kiesl, B., Heule, M.J.H., Seidl, M.: A little blocked literal goes a long way. In:
Gaspers, S., Walsh, T. (eds.) SAT 2017. LNCS, vol. 10491, pp. 281–297. Springer,
Cham (2017). https://doi.org/10.1007/978-3-319-66263-3 18
18. Kiesl, B., Seidl, M.: QRAT polynomially simulates ∀-Exp+Res. In: Janota, M.,
Lynce, I. (eds.) SAT 2019. LNCS, vol. 11628, pp. 193–202. Springer, Cham (2019).
https://doi.org/10.1007/978-3-030-24258-9 13
19. Kleine B¨uning, H., Bubeck, U.: Theory of quantiﬁed Boolean formulas. In: Biere,
A., Heule, M., van Maaren, H., Walsh, T. (eds.) Handbook of Satisﬁability, Fron-
tiers in Artiﬁcial Intelligence and Applications, vol. 185, pp. 735–760. IOS Press
(2009)
20. Kleine B¨uning, H., Karpinski, M., Fl¨ogel, A.: Resolution for quantiﬁed Boolean
formulas. Inf. Comput. 117(1), 12–18 (1995)
21. Kraj´ıˇcek, J.: Interpolation theorems, lower bounds for proof systems and indepen-
dence results for bounded arithmetic. J. Symb. Logic 62(2), 457–486 (1997)
22. Kraj´ıˇcek, J., Pudl´ak, P.: Some consequences of cryptographical conjectures for S1
2
and EF. Inf. Comput. 140(1), 82–94 (1998)
23. Lonsing, F., Egly, U.: QRAT+: generalizing QRAT by a more powerful QBF redun-
dancy property. In: Galmiche, D., Schulz, S., Sebastiani, R. (eds.) IJCAR 2018.
LNCS (LNAI), vol. 10900, pp. 161–177. Springer, Cham (2018). https://doi.org/
10.1007/978-3-319-94205-6 12
24. Pudl´ak, P.: Lower bounds for resolution and cutting planes proofs and monotone
computations. J. Symb. Logic 62(3), 981–998 (1997)

Removing Algebraic Data Types from
Constrained Horn Clauses Using
Diﬀerence Predicates
Emanuele De Angelis1,3(B)
, Fabio Fioravanti1(B)
, Alberto Pettorossi2,3
,
and Maurizio Proietti3
1 DEC, University ‘G. d’Annunzio’, Chieti-Pescara, Pescara, Italy
fabio.fioravanti@unich.it
2 DICII, University of Rome ‘Tor Vergata’, Rome, Italy
pettorossi@info.uniroma2.it
3 IASI-CNR, Rome, Italy
{emanuele.deangelis,maurizio.proietti}@iasi.cnr.it
Abstract. We address the problem of proving the satisﬁability of Con-
strained Horn Clauses (CHCs) with Algebraic Data Types (ADTs), such
as lists and trees. We propose a new technique for transforming CHCs
with ADTs into CHCs where predicates are deﬁned over basic types, such
as integers and booleans, only. Thus, our technique avoids the explicit
use of inductive proof rules during satisﬁability proofs. The main exten-
sion over previous techniques for ADT removal is a new transformation
rule, called diﬀerential replacement, which allows us to introduce auxil-
iary predicates corresponding to the lemmas used when making induc-
tive proofs. We present an algorithm that applies the new rule, together
with the traditional folding/unfolding rules, for the automatic removal of
ADTs. We prove that if the set of the transformed clauses is satisﬁable,
then so is the set of the original clauses. By an experimental evaluation,
we show that the use of the new rule signiﬁcantly improves the eﬀective-
ness of ADT removal, and that our approach is competitive with respect
to a state-of-the-art tool that extends the CVC4 solver with induction.
1
Introduction
Constrained Horn Clauses (CHCs) constitute a fragment of the ﬁrst order pred-
icate calculus, where the Horn clause format is extended by allowing constraints
on speciﬁc domains to occur in clause premises. CHCs have gained popularity as
a suitable logical formalism for automatic program veriﬁcation [3]. Indeed, many
veriﬁcation problems can be reduced to the satisﬁability problem for CHCs.
Satisﬁability of CHCs is a particular case of Satisﬁability Modulo Theories
(SMT), understood here as the general problem of determining the satisﬁabil-
ity of (possibly quantiﬁed) ﬁrst order formulas where the interpretation of some
function and predicate symbols is deﬁned in a given constraint (or background)
This work has been partially supported by GNCS-INdAM, Italy.
c
⃝Springer Nature Switzerland AG 2020
N. Peltier and V. Sofronie-Stokkermans (Eds.): IJCAR 2020, LNAI 12166, pp. 83–102, 2020.
https://doi.org/10.1007/978-3-030-51074-9_6

84
E. De Angelis et al.
theory [2]. Recent advances in the ﬁeld have led to the development of a number
of very powerful SMT (and, in particular, CHC) solvers, which aim at solving sat-
isﬁability problems with respect to a large variety of constraint theories. Among
SMT solvers, we would like to mention CVC4 [1], MathSAT [5], and Z3 [14],
and among solvers with specialized engines for CHCs, we recall Eldarica [22],
HSF [20], RAHFT [26], and Spacer [29].
Even if SMT algorithms for unrestricted ﬁrst order formulas suﬀer from
incompleteness limitations due to general undecidability results, most of the
above mentioned tools work well in practice when acting on constraint theo-
ries, such as Booleans, Uninterpreted Function Symbols, Linear Integer or Real
Arithmetic, Bit Vectors, and Arrays. However, when formulas contain univer-
sally quantiﬁed variables ranging over inductively deﬁned algebraic data types
(ADTs), such as lists and trees, then the SMT/CHC solvers often show a very
poor performance, as they do not incorporate induction principles relative to the
ADT in use.
To mitigate this diﬃculty, some SMT/CHC solvers have been enhanced by
incorporating appropriate induction principles [38,43,44], similarly to what has
been done in automated theorem provers [4]. The most creative step which is
needed when extending SMT solving with induction, is the generation of the
auxiliary lemmas that are required for proving the main conjecture.
An alternative approach, proposed in the context of CHCs [10], consists in
transforming a given set of clauses into a new set: (i) where all ADT terms are
removed (without introducing new function symbols), and (ii) whose satisﬁabil-
ity implies the satisﬁability of the original set of clauses. This approach has the
advantage of separating the concern of dealing with ADTs (at transformation
time) from the concern of dealing with simpler, non-inductive constraint theories
(at solving time), thus avoiding the complex interaction between inductive rea-
soning and constraint solving. It has been shown [10] that the transformational
approach compares well with induction-based tools in the case where lemmas
are not needed in the proofs. However, in some satisﬁability problems, if suitable
lemmas are not provided, the transformation fails to remove the ADT terms.
The main contributions of this paper are as follows.
(1) We extend the transformational approach by proposing a new rule, called
diﬀerential replacement, based on the introduction of suitable diﬀerence
predicates, which play a role similar to that of lemmas in inductive proofs.
We prove that the combined use of the fold/unfold transformation rules [17]
and the diﬀerential replacement rule is sound, that is, if the transformed set
of clauses is satisﬁable, then the original set of clauses is satisﬁable.
(2) We develop a transformation algorithm that removes ADTs from CHCs by
applying the fold/unfold and the diﬀerential replacement rules in a fully
automated way.
(3)
Due to the undecidability of the satisﬁability problem for CHCs, our tech-
nique for ADT removal is incomplete. Thus, we evaluate its eﬀectiveness
from an experimental point of view, and in particular we discuss the results
obtained by the implementation of our technique in a tool, called AdtRem.

Removing ADTs from CHCs Using Diﬀerence Predicates
85
We consider a set of CHC satisﬁability problems on ADTs taken from various
benchmarks which are used for evaluating inductive theorem provers. The
experiments show that AdtRem is competitive with respect to Reynolds
and Kuncak’s tool that augments the CVC4 solver with inductive reason-
ing [38].
The paper is structured as follows. In Sect. 2 we present an introductory, moti-
vating example. In Sect. 3 we recall some basic notions about CHCs. In Sect. 4
we introduce the rules used in our transformation technique and, in particular,
the novel diﬀerential replacement rule, and we show their soundness. In Sect. 5
we present a transformation algorithm that uses the transformation rules for
removing ADTs from sets of CHCs. In Sect. 6 we illustrate the AdtRem tool
and we present the experimental results we have obtained. Finally, in Sect. 7 we
discuss the related work and make some concluding remarks.
2
A Motivating Example
Let us consider the following functional program Reverse, which we write using
the OCaml syntax [31]:
type list = Nil | Cons of int * list;;
let rec append l ys = match l with
| Nil -> ys
| Cons(x,xs) -> Cons(x,(append xs ys));;
let rec rev l = match l with
| Nil -> Nil
| Cons(x,xs) -> append (rev xs) (Cons(x,Nil));;
let rec len l = match l with
| Nil -> 0
| Cons(x,xs) -> 1 + len xs;;
The functions append, rev, and len compute list concatenation, list reversal,
and list length, respectively. Suppose we want to prove the following property:
∀xs, ys. len (rev (append xs ys)) = (len xs) + (len ys)
(1)
Inductive theorem provers construct a proof of this property by induction on the
structure of the list l, by assuming the knowledge of the following lemma:
∀x, l. len (append l (Cons(x, Nil))) = (len l) + 1
(2)
The approach we follow in this paper avoids the explicit use of induction princi-
ples and also the knowledge of ad hoc lemmas. First, we consider the translation
of Property (1) into a set of constrained Horn clauses [10,43], as follows1:
1 In the examples, we use Prolog syntax for writing clauses, instead of the more verbose
SMT-LIB syntax. The predicates \= (diﬀerent from), = (equal to), < (less-than),
>= (greater-than-or-equal-to) denote constraints between integers. The last argument
of a Prolog predicate stores the value of the corresponding function.

86
E. De Angelis et al.
1. false :- N2\=N0+N1, append(Xs,Ys,Zs), rev(Zs,Rs),
len(Xs,N0), len(Ys,N1), len(Rs,N2).
2. append([],Ys,Ys).
3. append([X|Xs],Ys,[X|Zs]) :- append(Xs,Ys,Zs).
4. rev([],[]).
5. rev([X|Xs],Rs) :- rev(Xs,Ts), append(Ts,[X],Rs).
6. len([],N) :- N=0.
7. len([X|Xs],N1) :- N1=N0+1, len(Xs,N0).
The set of clauses 1–7 is satisﬁable if and only if Property (1) holds. However,
state-of-the-art CHC solvers, such as Z3 or Eldarica, fail to prove the satisﬁability
of clauses 1–7, because those solvers do not incorporate any induction principle
on lists. Moreover, some tools that extend SMT solvers with induction [38,43]
fail on this particular example because they are not able to introduce Lemma (2).
To overcome this diﬃculty, we apply the transformational approach based on
the fold/unfold rules [10], whose objective is to transform a given set of clauses
into a new set without occurrences of list variables, whose satisﬁability can be
checked by using CHC solvers based on the theory of Linear Integer Arithmetic
only. The soundness of the transformation rules ensures that the satisﬁability of
the transformed clauses implies the satisﬁability of the original ones. We apply
the Elimination Algorithm [10] as follows. First, we introduce a new clause:
8. new1(N0,N1,N2) :- append(Xs,Ys,Zs), rev(Zs,Rs),
len(Xs,N0), len(Ys,N1), len(Rs,N2).
whose body is made out of the atoms of clause 1 which have at least one list
variable, and whose head arguments are the integer variables of the body. By
folding, from clause 1 we derive a new clause without occurrences of lists:
9. false :- N2\=N0+N1, new1(N0,N1,N2).
We proceed by eliminating lists from clause 8. By unfolding clause 8, we replace
some predicate calls by their deﬁnitions and we derive the two new clauses:
10. new1(N0,N1,N2) :- N0=0, rev(Zs,Rs), len(Zs,N1), len(Rs,N2).
11. new1(N01,N1,N21) :- N01=N0+1, append(Xs,Ys,Zs), rev(Zs,Rs),
len(Xs,N0), len(Ys,N1), append(Rs,[X],R1s), len(R1s,N21).
We would like to fold clause 11 using clause 8, so as to derive a recursive deﬁni-
tion of new1 without lists. Unfortunately, this folding step cannot be performed
because the body of clause 11 does not contain a variant of the body of clause 8,
and hence the Elimination Algorithm fails to eliminate lists in this example.
Thus, now we depart from the Elimination Algorithm and we continue our
derivation by observing that the body of clause 11 contains the subconjunc-
tion ‘append(Xs,Ys,Zs), rev(Zs,Rs), len(Xs,N0), len(Ys,N1)’ of the body of
clause 8. Then, in order to ﬁnd a variant of the whole body of clause 8, we
may replace in clause 11 the remaining subconjunction ‘append(Rs,[X],R1s),
len(R1s,N21)’ by the new subconjunction ‘len(Rs,N2), diff(N2,X,N21)’, where
diff is a predicate, called diﬀerence predicate, deﬁned as follows:
12. diff(N2,X,N21) :- append(Rs,[X],R1s), len(R1s,N21), len(Rs,N2).

Removing ADTs from CHCs Using Diﬀerence Predicates
87
From clause 11, by performing that replacement, we derive the following clause:
13. new1(N01,N1,N21) :- N01=N0+1, append(Xs,Ys,Zs), rev(Zs,Rs),
len(Xs,N0), len(Ys,N1), len(Rs,N2), diff(N2,X,N21).
Now, we can fold clause 13 using clause 8 and we derive a new clause without
list arguments:
14. new1(N01,N1,N21) :- N01=N0+1, new1(N0,N1,N2), diff(N2,X,N21).
At this point, we are left with the task of removing list arguments from clauses 10
and 12. As the reader may verify, this can be done by applying the Elimination
Algorithm without the need of introducing additional diﬀerence predicates. By
doing so, we get the following ﬁnal set of clauses without list arguments:
false :- N2\=N0+N1, new1(N0,N1,N2).
new1(N0,N1,N2) :- N0=0, new2(N1,N2).
new1(N0,N1,N2) :- N0=N+1, new1(N,N1,M), diff(M,X,N2).
new2(M,N) :- M=0, N=0.
new2(M1,N1) :- M1=M+1, new2(M,N), diff(N,X,N1).
diff(N0,X,N1) :- N0=0, N1=1.
diff(N0,X,N1) :- N0=N+1, N1=M+1, diff(N,X,M).
The Eldarica CHC solver proves the satisﬁability of this set of clauses by
computing the following model (here we use a Prolog-like syntax):
new1(N0,N1,N2) :- N2=N0+N1, N0>=0, N1>=0, N2>=0.
new2(M,N) :- M=N, M>=0, N>=0.
diff(N,X,M) :- M=N+1, N>=0.
Finally, we note that if in clause 12 we substitute the atom diff(N2,X,N21) by
its model computed by Eldarica, namely the constraint ‘N21=N2+1, N2>=0’, we get
exactly the CHC translation of Lemma (2). Thus, in some cases, the introduction
of the diﬀerence predicates can be viewed as a way of automatically introducing
the lemmas needed when performing inductive proofs.
3
Constrained Horn Clauses
Let LIA be the theory of linear integer arithmetic and Bool be the theory of
boolean values. A constraint is a quantiﬁer-free formula of LIA ∪Bool. Let C
denote the set of all constraints. Let L be a typed ﬁrst order language with
equality [16] which includes the language of LIA ∪Bool. Let Pred be a set of
predicate symbols in L not occurring in the language of LIA ∪Bool.
The integer and boolean types are said to be the basic types. For reasons
of simplicity we do not consider any other basic types, such as real number,
arrays, and bit-vectors, which are usually supported by SMT solvers [1,14,22].
We assume that all non-basic types are speciﬁed by suitable data-type declara-
tions (such as the declare-datatypes declarations adopted by SMT solvers), and
are collectively called algebraic data types (ADTs).

88
E. De Angelis et al.
An atom is a formula of the form p(t1, . . . , tm), where p is a typed predicate
symbol in Pred, and t1, . . . , tm are typed terms constructed out of individual
variables, individual constants, and function symbols. A constrained Horn clause
(or simply, a clause, or a CHC) is an implication of the form A ←c, B (for clauses
we use the logic programming notation, where comma denotes conjunction). The
conclusion (or head) A is either an atom or false, the premise (or body) is the
conjunction of a constraint c ∈C, and a (possibly empty) conjunction B of
atoms. If A is an atom of the form p(t1, . . . , tn), the predicate p is said to be a
head predicate. A clause whose head is an atom is called a deﬁnite clause, and a
clause whose head is false is called a goal.
We assume that all variables in a clause are universally quantiﬁed in front,
and thus we can freely rename those variables. Clause C is said to be a variant
of clause D if C can be obtained from D by renaming variables and rearranging
the order of the atoms in its body. Given a term t, by vars(t) we denote the set
of all variables occurring in t. Similarly for the set of all free variables occurring
in a formula. Given a formula ϕ in L, we denote by ∀(ϕ) its universal closure.
Let D be the usual interpretation for the symbols in LIA ∪Bool, and let
a D-interpretation be an interpretation of L that, for all symbols occurring
in LIA ∪Bool, agrees with D.
A set P of CHCs is satisﬁable if it has a D-model and it is unsatisﬁable,
otherwise. Given two D-interpretations I and J, we say that I is included in
J if for all ground atoms A, I |= A implies J |= A. Every set P of deﬁnite
clauses is satisﬁable and has a least (with respect to inclusion) D-model, denoted
M(P) [24]. Thus, if P is any set of constrained Horn clauses and Q is the set of
the goals in P, then we deﬁne Deﬁnite(P)=P \ Q. We have that P is satisﬁable
if and only if M(Deﬁnite(P)) |= Q.
We will often use tuples of variables as arguments of predicates and write
p(X, Y ), instead of p(X1, . . . , Xm, Y1, . . . , Yn), whenever the values of m (≥0)
and n (≥0) are not relevant. Whenever the order of the variables is not relevant,
we will feel free to identify tuples of distinct variables with ﬁnite sets, and we will
extend to ﬁnite tuples the usual operations and relations on ﬁnite sets. Given
two tuples X and Y of distinct elements, (i) their union X ∪Y is obtained
by concatenating them and removing all duplicated occurrences of elements,
(ii) their intersection X ∩Y is obtained by removing from X the elements which
do not occur in Y , (iii) their diﬀerence X\Y is obtained by removing from X the
elements which occur in Y , and (iv) X ⊆Y holds if every element of X occurs in
Y . For all m ≥0, (u1, . . . , um) = (v1, . . . , vm) iﬀm
i=1 ui =vi. The empty tuple
() is identiﬁed with the empty set ∅.
By A(X, Y ), where X and Y are disjoint tuples of distinct variables, we
denote an atom A such that vars(A) = X ∪Y . Let P be a set of deﬁnite clauses.
We say that the atom A(X, Y ) is functional from X to Y with respect to P if
(F1) M(P) |= ∀X, Y, Z. A(X, Y ) ∧A(X, Z) →Y =Z
The reference to the set P of deﬁnite clauses is omitted, when understood from
the context. Given a functional atom A(X, Y ), we say that X and Y are its input

Removing ADTs from CHCs Using Diﬀerence Predicates
89
and output (tuples of) variables, respectively. The atom A(X, Y ) is said to be
total from X to Y with respect to P if
(F2) M(P) |= ∀X∃Y. A(X, Y )
If A(X, Y ) is a total, functional atom from X to Y , we may write A(X; Y ),
instead of A(X, Y ). For instance, append(Xs,Ys,Zs) is a total, functional atom
from (Xs,Ys) to Zs with respect to the set of clauses 1–7 of Sect. 2.
Now we extend the above notions from atoms to conjunctions of atoms. Let F
be a conjunction A1(X1; Y1), . . . , An(Xn; Yn) such that: (i) X = (n
i=1 Xi) \
(n
i=1 Yi), (ii) Y = (n
i=1 Yi), and (iii) for i = 1, . . . , n, Yi is disjoint from
(i
j=1 Xj) ∪(i−1
j=1 Yj). Then, the conjunction F is said to be a total, func-
tional conjunction from X to Y and it is also written as F(X; Y ). For F(X; Y ),
the above properties (F1) and (F2) hold if we replace A by F. For instance,
append(Xs,Ys,Zs), rev(Zs,Rs) is a total, functional conjunction from (Xs,Ys) to
(Zs,Rs) with respect to the set of clauses 1–7 of Sect. 2.
4
Transformation Rules for Constrained Horn Clauses
In this section we present the rules that we propose for transforming CHCs, and
in particular, for introducing diﬀerence predicates, and we prove their soundness.
We refer to Sect. 2 for examples of how the rules are applied.
First, we introduce the following notion of a stratiﬁcation for a set of clauses.
Let N denote the set of the natural numbers. A level mapping is a function
ℓ: Pred →N. For every predicate p, the natural number ℓ(p) is said to be the
level of p. Level mappings are extended to atoms by stating that the level ℓ(A)
of an atom A is the level of its predicate symbol. A clause H ←c, A1, . . . , An is
stratiﬁed with respect to ℓif, for i=1, . . . , n, ℓ(H) ≥ℓ(Ai). A set P of CHCs is
stratiﬁed w.r.t. ℓif all clauses in P are stratiﬁed w.r.t. ℓ. Clearly, for every set P
of CHCs, there exists a level mapping ℓsuch that P is stratiﬁed w.r.t. ℓ[33].
A transformation sequence from P0 to Pn is a sequence P0 ⇒P1 ⇒. . . ⇒Pn
of sets of CHCs such that, for i = 0, . . . , n−1, Pi+1 is derived from Pi, denoted
Pi ⇒Pi+1, by applying one of the following Rules R1–R7. We assume that P0
is stratiﬁed w.r.t. a given level mapping ℓ.
(R1) Deﬁnition Rule. Let D be the clause newp(X1, . . . , Xk) ←c, A1, . . . , An,
where: (i) newp is a predicate symbol in Pred not occurring in the sequence P0 ⇒
P1 ⇒. . . ⇒Pi, (ii) c is a constraint, (iii) the predicate symbols of A1, . . . , An
occur in P0, and (iv) (X1, . . . , Xk) ⊆vars(c, A1, . . . , An). Then, Pi+1 = Pi∪{D}.
We set ℓ(newp) = max {ℓ(Ai) | i = 1, . . . , n}.
For i = 0, . . . , n, by Defsi we denote the set of clauses, called deﬁnitions,
introduced by Rule R1 during the construction of P0 ⇒P1 ⇒. . . ⇒Pi. Thus,
∅= Defs0 ⊆Defs1 ⊆. . . . However, by using Rules R2–R7 we can replace a
deﬁnition in Pi, for i>0, and hence it may happen that Defsi+1 ̸⊆Pi+1.
(R2) Unfolding Rule. Let C: H ←c, GL, A, GR be a clause in Pi, where A is an
atom. Without loss of generality, we assume that vars(C) ∩vars(P0) = ∅. Let
Cls: {K1 ←c1, B1, . . . , Km ←cm, Bm}, m ≥0, be the set of clauses in P0, such

90
E. De Angelis et al.
that: for j = 1, . . . , m, (1) there exists a most general uniﬁer ϑj of A and Kj, and
(2) the conjunction of constraints (c, cj)ϑj is satisﬁable. Let Unf (C, A, P0) =
{(H ←c, cj, GL, Bj, GR)ϑj | j = 1, . . . , m}. Then, by unfolding C w.r.t. A,
we derive the set Unf (C, A, P0) of clauses and we get Pi+1 = (Pi \ {C}) ∪
Unf (C, A, P0).
When we apply Rule R2, we say that, for j = 1, . . . , m, the atoms in the con-
junction Bjϑj are derived from A, and the atoms in the conjunction (GL, GR)ϑj
are inherited from the corresponding atoms in the body of C.
(R3) Folding Rule. Let C: H ←c, GL, Q, GR be a clause in Pi, and let D:
K ←d, B be a clause in Defsi. Suppose that: (i) either H is false or ℓ(H) ≥ℓ(K),
and (ii) there exists a substitution ϑ such that Q=Bϑ and D |= ∀(c →dϑ). By
folding C using deﬁnition D, we derive clause E: H ←c, GL, Kϑ, GR, and we
get Pi+1 = (Pi \ {C}) ∪{E}.
(R4) Clause Deletion Rule. Let C: H ←c, G be a clause in Pi such that the
constraint c is unsatisﬁable. Then, we get Pi+1 = Pi \ {C}.
(R5) Functionality Rule. Let C: H ←c, GL, F(X; Y ), F(X; Z), GR be a clause
in Pi, where F(X; Y ) is a total, functional conjunction in Deﬁnite(P0) ∪Defsi.
By functionality, from C we derive clause D: H ←c, Y = Z, GL, F(X; Y ), GR,
and we get Pi+1 = (Pi \ {C}) ∪{D}.
(R6) Totality Rule. Let C: H ←c, GL, F(X; Y ), GR be a clause in Pi such that
Y ∩vars(H ←c, GL, GR) = ∅and F(X; Y ) is a total, functional conjunction in
Deﬁnite(P0) ∪Defsi. By totality, from C we derive clause D : H ←c, GL, GR
and we get Pi+1 = (Pi \ {C}) ∪{D}.
Since the initial set of clauses is obtained by translating a terminating func-
tional program, the functionality and totality properties hold by construction
and we do not need to prove them when we apply Rules R5 and R6.
(R7) Diﬀerential Replacement Rule. Let C: H ←c, GL, F(X; Y ), GR be a clause
in Pi, and let D: diﬀ(Z) ←d, F(X; Y ), R(V ; W) be a deﬁnition clause in Defsi,
where: (i) F(X; Y ) and R(V ; W) are total, functional conjunctions with respect
to Deﬁnite(P0)∪Defsi, (ii) W ∩vars(C) = ∅, (iii) D |= ∀(c →d), and (iv) ℓ(H)>
ℓ(diﬀ). By diﬀerential replacement, we derive E: H ←c, GL, R(V ;W), diﬀ(Z), GR
and we get Pi+1 = (Pi \ {C}) ∪{E}.
Rule R7 has a very general formulation that eases the proof of the Soundness
Theorem, which extends to Rules R1–R7 correctness results for transformations
of (constraint) logic programs [17,18,39,42] (see [13] for a proof). In the trans-
formation algorithm of Sect. 5, we will use a speciﬁc instance of Rule R7 which
is suﬃcient for ADT removal (see, in particular, the Diﬀ-Introduce step).
Theorem 1 (Soundness).
Let P0 ⇒P1 ⇒. . . ⇒Pn be a transformation
sequence using Rules R1–R7. Suppose that the following condition holds :
(U) for i=1, . . . , n−1, if Pi ⇒Pi+1 by folding a clause in Pi using a deﬁnition
D : H ←c, B in Defsi, then, for some j ∈{1, . . . , i−1, i+1, . . . , n−1},
Pj ⇒Pj+1 by unfolding D with respect to an atom A such that ℓ(H) = ℓ(A).

Removing ADTs from CHCs Using Diﬀerence Predicates
91
If Pn is satisﬁable, then P0 is satisﬁable.
Thus, to prove the satisﬁability of a set P0 of clauses, it suﬃces: (i) to con-
struct a transformation sequence P0 ⇒. . . ⇒Pn, and then (ii) to prove that
Pn is satisﬁable. Note, however, that if Rule R7 is used, it may happen that
P0 is satisﬁable and Pn is unsatisﬁable, that is, some false counterexamples to
satisﬁability, so-called false positives, may be generated, as we now show.
Example 1. Let us consider the following set P1 of clauses derived by adding the
deﬁnition clause D to the initial set P0 ={C, 1, 2, 3} of clauses:
C. false :- X=0, Y>0, a(X,Y).
1. a(X,Y) :- X=<0, Y=0.
2. a(X,Y) :- X>0, Y=1.
3. r(X,W) :- W=1.
D. diff(Y,W) :- a(X,Y), r(X,W).
where: (i) a(X,Y) is a total, functional atom from X to Y, (ii) r(X,W) is a total,
functional atom from X to W, and (iii) D is a deﬁnition in Defs1. By applying
Rule R7, from P1 we derive the set P2 ={E, 1, 2, 3, D} of clauses where:
E. false :- X=0, Y>0, r(X,W), diff(Y,W).
Now we have that P0 is satisﬁable, while P2 is unsatisﬁable.
5
An Algorithm for ADT Removal
Now we present Algorithm R for eliminating ADT terms from CHCs by using the
transformation rules presented in Sect. 4 and automatically introducing suitable
diﬀerence predicates. If R terminates, it transforms a set Cls of clauses into a new
set TransfCls where the arguments of all predicates have basic type. Theorem 1
guarantees that if TransfCls is satisﬁable, then also Cls is satisﬁable.
Algorithm R (see Fig. 1) removes ADT terms starting from the set Gs of
goals in Cls. R collects these goals in InCls and stores in Defs the deﬁnitions of
new predicates introduced by Rule R1.
Fig. 1. The ADT removal algorithm R.
Before describing the procedures used by Algorithm R, let us ﬁrst introduce
the following notions.

92
E. De Angelis et al.
Given a conjunction G of atoms, bvars(G) (or adt−vars(G)) denotes the
set of variables in G that have a basic type (or an ADT type, respectively).
We say that an atom (or clause) has basic types if all its arguments (or atoms,
respectively) have a basic type. An atom (or clause) has ADTs if at least one of
its arguments (or atoms, respectively) has an ADT type.
Given a set (or a conjunction) S of atoms, SharingBlocks(S) denotes the
partition of S with respect to the reﬂexive, transitive closure ⇓S of the rela-
tion ↓S deﬁned as follows. Given two atoms A1 and A2 in S, A1 ↓S A2 holds
iﬀadt-vars(A1) ∩adt-vars(A2)̸=∅. The elements of the partition are called the
sharing blocks of S.
A generalization of a pair (c1, c2) of constraints is a constraint α(c1, c2) such
that D |= ∀(c1 →α(c1, c2)) and D |= ∀(c2 →α(c1, c2)) [19]. In particular, we
consider the following generalization operator based on widening [7]. Suppose
that c1 is the conjunction (a1, . . . , am) of atomic constraints, then α(c1, c2) is
deﬁned as the conjunction of all ai’s in (a1, . . . , am) such that D |= ∀(c2 →
ai). For any constraint c and tuple V of variables, the projection of c onto V
is a constraint π(c, V ) such that: (i) vars(π(c, V )) ⊆V , and (ii) D |= ∀(c →
π(c, V )). In our implementation, π(c, V ) is computed from ∃Y.c, where Y =
vars(c) \ V , by a quantiﬁer elimination algorithm in the theory of booleans and
rational numbers. This implementation is safe in our context, and avoids relying
on modular arithmetic, as is often done when eliminating quantiﬁers in LIA [37].
For two conjunctions G1 and G2 of atoms, G1 ∼◁G2 holds if G1 =(A1, . . . , An)
and there exists a subconjunction (B1, . . . , Bn) of G2 (modulo reordering) such
that, for i = 1, . . . , n, Bi is an instance of Ai. A conjunction G of atoms is
connected if it consists of a single sharing block.
■Procedure Diﬀ-Deﬁne-Fold (see Fig. 2). At each iteration of the body of the
for loop, the Diﬀ-Deﬁne-Fold procedure removes the ADT terms occurring
in a sharing block B of the body of a clause C : H ←c, B, G′ of InCls. This is
done by possibly introducing some new deﬁnitions (using Rule R1) and applying
the Folding Rule R3. To allow folding, some applications of the Diﬀerential
Replacement Rule R7 may be needed. We have the following four cases.
• (Fold). We remove the ADT arguments occurring in B by folding C using a
deﬁnition D introduced at a previous step. Indeed, the head of each deﬁnition
introduced by Algorithm R is by construction a tuple of variables of basic type.
• (Generalize). We introduce a new deﬁnition GenD : genp(V ) ←α(d, c), B
whose constraint is obtained by generalizing (d, c), where d is the constraint
occurring in an already available deﬁnition whose body is B. Then, we remove
the ADT arguments occurring in B by folding C using GenD.
• (Diﬀ-Introduce). Suppose that B partially matches the body of an avail-
able deﬁnition D: newp(U) ←d, B′, that is, for some substitution ϑ, B =
(M, F(X; Y )), and B′ϑ = (M, R(V ; W)). Then, we introduce a diﬀerence predi-
cate through the new deﬁnition D: diﬀ(Z) ←π(c, X), F(X; Y ), R(V ; W), where
Z = bvars(F(X; Y ), R(V ; W)) and, by Rule R7, we replace the conjunction
F(X; Y ) by (R(V ; W), diﬀ(Z)) in the body of C, thereby deriving C′. Finally,

Removing ADTs from CHCs Using Diﬀerence Predicates
93
Fig. 2. The Diﬀ-Deﬁne-Fold procedure.
we remove the ADT arguments in B by folding C′ using either D or a clause
GenD whose constraint is a generalization of the pair (dϑ, c) of constraints.
The example of Sect. 2 allows us to illustrate this (Diﬀ-Introduce) case.
With reference to that example, clause C: H ←c, G that we want to fold
is clause 11, whose body has the single sharing block B: ‘append(Xs,Ys,Zs),

94
E. De Angelis et al.
rev(Zs,Rs), len(Xs,N0), len(Ys,N1), append(Rs,[X],R1s), len(R1s,N21)’. Block B
partially
matches
the
body
‘append(Xs,Ys,Zs), rev(Zs,Rs), len(Xs,N0), len
(Ys,N1),
len(Rs,N2)’ of
clause 8 of
Sect. 2
which
plays the
role of
deﬁni-
tion D: newp(U)←d, B′ in this example. Indeed, we have that:
M= (append(Xs,Ys,Zs), rev(Zs,Rs), len(Xs,N0), len(Ys,N1)),
F(X; Y )=(append(Rs,[X],R1s), len(R1s,N21)), where X=(Rs,X), Y =(R1s,N21),
R(V ; W) = len(Rs,N2), where V = (Rs), Y = (N2).
In this example, ϑ is the identity substitution. Morevover, the condition on
the level mapping ℓrequired in the Diﬀ-Deﬁne-Fold Procedure of Fig. 2 can be
fulﬁlled by stipulating that ℓ(new1) > ℓ(append) and ℓ(new1) > ℓ(len). Thus, the
deﬁnition D to be introduced is:
12. diff(N2,X,N21) :- append(Rs,[X],R1s), len(R1s,N21), len(Rs,N2).
Indeed, we have that: (i) the projection π(c, X) is π(N01=N0+1, (Rs,X)), that is, the
empty conjunction, (ii) F(X; Y ), R(V ; W) is the body of clause 12, and (iii) the
head variables N2, X, and N21 are the integer variables in that body. Then, by
applying Rule R7, we replace in clause 11 the conjunction ‘append(Rs,[X],R1s),
len(R1s,N21)’ by the new conjunction ‘len(Rs,N2), diff(N2,X,N21)’, hence deriv-
ing clause C′, which is clause 13 of Sect. 2. Finally, by folding clause 13 using
clause 8, we derive clause 14 of Sect. 2, which has no list arguments.
• (Project). If none of the previous three cases apply, then we introduce a
new deﬁnition ProjC: newp(V ) ←π(c, V ), B, where V = bvars(B). Then, we
remove the ADT arguments occurring in B by folding C using ProjC.
The Diﬀ-Deﬁne-Fold procedure may introduce new deﬁnitions with ADTs
in their bodies, which are added to NewDefs and processed by the Unfold pro-
cedure. In order to present this procedure, we need the following notions.
The application of Rule R2 is controlled by marking some atoms in the body
of a clause as unfoldable. If we unfold w.r.t. atom A clause C: H ←c, L, A, R
the marking of the clauses in Unf (C, A, Ds) is handled as follows: the atoms
derived from A are not marked as unfoldable and each atom A′′ inherited from an
atom A′ in the body of C is marked as unfoldable iﬀA′ is marked as unfoldable.
An atom A(X; Y ) in a conjunction F(V ; Z) of atoms is said to be a source
atom if X ⊆V . Thus, a source atom corresponds to an innermost function call
in a given functional expression. For instance, in clause 1 of Sect. 2, the source
atoms are append(Xs,Ys,Zs), len(Xs,N0), and len(Ys,N1). Indeed, the body of
clause 1 corresponds to len(rev(append xs ys)) =/ (len xs)+(len ys).
An atom A(X; Y ) in the body of clause C: H ←c, L, A(X; Y ), R is a head-
instance w.r.t. a set Ds of clauses if, for every clause K ←d, B in Ds such that:
(1) there exists a most general uniﬁer ϑ of A(X; Y ) and K, and (2) the constraint
(c, d)ϑ is satisﬁable, we have that Xϑ=X. Thus, the input variables of A(X; Y )
are not instantiated by uniﬁcation. For instance, the atom append([X|Xs],Ys,Zs)
is a head-instance, while append(Xs,Ys,Zs) is not.
In a set Cls of clauses, predicate p immediately depends on predicate q, if in Cls
there is a clause of the form p(. . .) ←. . . , q(. . .), . . . The depends on relation is

Removing ADTs from CHCs Using Diﬀerence Predicates
95
the transitive closure of the immediately depends on relation. Let ≺be a well-
founded ordering on tuples of terms such that, for all terms t, u, if t ≺u, then,
for all substitutions ϑ, tϑ ≺uϑ. A predicate p is descending w.r.t. ≺if, for all
clauses, p(t; u) ←c, p1(t1; u1), . . . , pn(tn; un), for i=1, . . . , n, if pi depends on p
then ti ≺t. An atom is descending if its predicate is descending. The well-founded
ordering ≺we use in our implementation is based on the subterm relation and
is deﬁned as follows: (x1, . . . , xk)≺(y1, . . . , ym) if every xi is a subterm of some
yj and there exists xi which is a strict subterm of some yj. For instance, the
predicates append, rev, and len in the example of Sect. 2 are all descending.
■Procedure Unfold (see Fig. 3) repeatedly applies Rule R2 in two phases. In
Phase 1, the procedure unfolds the clauses in NewDefs w.r.t. at least one source
atom. Then, in Phase 2, clauses are unfolded w.r.t. head-instance atoms. Unfold-
ing is repeated only w.r.t descending atoms. The termination of the Unfold pro-
cedure is ensured by the fact that the unfolding w.r.t. a non-descending atom is
done at most once in each phase.
Fig. 3. The Unfold procedure.
■Procedure Replace simpliﬁes some clauses by applying Rules R5 and R6 as long
as possible. Replace terminates because each application of either rule decreases
the number of atoms.
Thus, each execution of the Diﬀ-Deﬁne-Fold, Unfold, and Replace procedures
terminates. However, Algorithm R might not terminate because new predicates
may be introduced by Diﬀ-Deﬁne-Fold at each iteration of the while-do of R.
Soundness of R follows from soundness of the transformation rules [13].
Theorem 2 (Soundness of Algorithm R). Suppose that Algorithm R ter-
minates for an input set Cls of clauses, and let TransfCls be the output set of
clauses. Then, every clause in TransfCls has basic types, and if TransfCls is
satisﬁable, then Cls is satisﬁable.

96
E. De Angelis et al.
Algorithm R is not complete, in the sense that, even if Cls is a satisﬁable set
of input clauses, then R may not terminate or, due to the use of Rule R7, it may
terminate with an output set TransfCls of unsatisﬁable clauses, thereby gener-
ating a false positive (see Example 1 in Sect. 4). However, due to well-known
undecidability results for the satisﬁability problem of CHCs, this limitation can-
not be overcome, unless we restrict the class of clauses we consider. The study
of such restricted classes of clauses is beyond the scope of the present paper and,
instead, in the next section, we evaluate the eﬀectiveness of Algorithm R from
an experimental viewpoint.
6
Experimental Evaluation
In this section we present the results of some experiments we have performed for
assessing the eﬀectiveness of our transformation-based CHC solving technique.
We compare our technique with the one proposed by Reynolds and Kuncak [38],
which extends the SMT solver CHC4 with inductive reasoning.
Implementation. We have developed the AdtRem tool for ADT removal, which
is based on an implementation of Algorithm R in the VeriMAP system [8].
Benchmark Suite and Experiments. Our benchmark suite consists of 169 veriﬁ-
cation problems over inductively deﬁned data structures, such as lists, queues,
heaps, and trees, which have been adapted from the benchmark suite consid-
ered by Reynolds and Kuncak [38]. These problems come from benchmarks used
by various theorem provers: (i) 53 problems come from CLAM [23], (ii) 11 from
HipSpec [6], (iii) 63 from IsaPlanner [15,25], and (iv) 42 from Leon [41]. We have
performed the following experiments, whose results are summarized in Table 1 2.
(1) We have considered Reynolds and Kuncak’s dtt encoding of the veriﬁcation
problems, where natural numbers are represented using the built-in SMT type
Int, and we have discarded: (i) problems that do not use ADTs, and (ii) problems
that cannot be directly represented in Horn clause format. Since AdtRem does
not support higher order functions, nor user-provided lemmas, in order to make a
comparison between the two approaches on a level playing ﬁeld, we have replaced
higher order functions by suitable ﬁrst order instances and we have removed all
auxiliary lemmas from the input veriﬁcation problems. We have also replaced
the basic functions recursively deﬁned over natural numbers, such as the plus
and less-or-equal functions, by LIA constraints.
(2) Then, we have translated each veriﬁcation problem into a set, call it P,
of CHCs in the Prolog-like syntax supported by AdtRem by using a modiﬁed
version of the SMT-LIB parser of the ProB system [32]. We have run Eldarica and
Z3 3, which use no induction-based mechanism for handling ADTs, to check the
satisﬁability of P. Rows ‘Eldarica’ and ‘Z3’ show the number of solved problems,
that is, problems whose CHC encoding has been proved satisﬁable.
2 The tool and the benchmarks are available at https://fmlab.unich.it/adtrem/.
3 More speciﬁcally, Eldarica v2.0.1 and Z3 v4.8.0 with the Spacer engine [28].

Removing ADTs from CHCs Using Diﬀerence Predicates
97
(3) We have run algorithm R on P to produce a set T of CHCs without ADTs.
Row ‘R’ reports the number of problems for which Algorithm R terminates.
(4) We have converted T into the SMT-LIB format, and then we have run Eldar-
ica and Z3 for checking its satisﬁability. Rows ‘Eldarica noADT’ and ‘Z3 noADT’
report outside round parentheses the number of solved problems. There was
only one false positive (that is, a satisﬁable set P of clauses transformed into an
unsatisﬁable set T), which we have classiﬁed as an unsolved problem.
(5) In order to assess the improvements due to the use of the diﬀerential replace-
ment rule we have applied to P a modiﬁed version, call it R◦, of the ADT
removal algorithm R that does not introduce diﬀerence predicates, that is, the
Diﬀ-Introduce case of the Diﬀ-Deﬁne-Fold Procedure of Fig. 2 is never executed.
The number of problems for which R◦terminates and the number of solved
problems using Eldarica and Z3 are shown within round parentheses in rows
‘R’, ‘Eldarica noADT’, and ‘Z3noADT’, respectively.
(6) Finally, we have run the cvc4+ig conﬁguration of the CVC4 solver extended
with inductive reasoning [38] on the 169 problems in SMT-LIB format obtained
at Step (1). Row ‘CVC4+Ind’ reports the number of solved problems.
Table 1. Experimental results. For each problem we have set a timeout limit of 300
seconds. Experiments have been performed on an Intel Xeon CPU E5-2640 2.00 GHz
with 64GB RAM under CentOS.
CLAM HipSpec IsaPlanner
Leon
Total
Number of problems 53
11
63
42
169
Eldarica
0
2
4
9
15
Z3
6
0
2
10
18
R
(18) 36 (2) 4
(56) 59
(18) 30 (94) 129
Eldarica noADT
(18) 32 (2) 4
(56) 57
(18) 29 (94) 122
Z3 noADT
(18) 29 (2) 3
(55) 56
(18) 26 (93) 114
CVC4+Ind
17
5
37
15
74
Evaluation of Results. The results of our experiments show that ADT removal
considerably increases the eﬀectiveness of CHC solvers without inductive reason-
ing support. For instance, Eldarica is able to solve 15 problems out of 169, while
it solves 122 problems after the removal of ADTs. When using Z3, the improve-
ment is slightly lower, but still very considerable. Note also that, when the ADT
removal terminates (129 problems out of 169), the solvers are very eﬀective (95%
successful veriﬁcations for Eldarica). The improvements speciﬁcally due to the
use of the diﬀerence replacement rule are demonstrated by the increase of the
number of problems for which the ADT removal algorithm terminates (from 94
to 129), and of the number of problems solved (from 94 to 122, for Eldarica).
AdtRem compares favorably to CVC4 extended with induction (compare
rows ‘EldaricanoADT’ and ‘Z3noADT’ to row ‘CVC4+Ind’). Interestingly, the eﬀec-
tiveness of CVC4 may be increased if one extends the problem formalization with

98
E. De Angelis et al.
extra lemmas which may be used for proving the main conjecture. Indeed, CVC4
solves 100 problems when auxiliary lemmas are added, and 134 problems when,
in addition, it runs on the dti encoding, where natural numbers are represented
using both the built-in type Int and the ADT deﬁnition with the zero and suc-
cessor constructors. Our results show that in most cases AdtRem needs neither
those extra axioms nor that sophisticated encoding.
Finally, in Table 2 we report some problems solved by AdtRem with Eldarica
that are not solved by CVC4 with induction (using any encoding and auxiliary
lemmas), or vice versa. For details, see https://fmlab.unich.it/adtrem/.
Table 2. A comparison between AdtRem with Eldarica and CVC4 with induction.
Problem
Property proved by AdtRem and not by CVC4
CLAM goal6
∀x, y. len(rev(append(x, y))) = len(x) + len(y)
CLAM goal49
∀x. mem(x, sort(y)) ⇒mem(x, y)
IsaPlanner goal52
∀n, l. count(n, l) = count(n, rev(l))
IsaPlanner goal80
∀l. sorted(sort(l))
Leon heap-goal13
∀x, l. len(qheapsorta(x, l)) = hsize(x) + len(l)
Problem
Property proved by CVC4 and not by AdtRem
CLAM goal18
∀x, y. rev(append(rev(x), y)) = append(rev(y), x)
HipSpec rev-equiv-goal4 ∀x, y. qreva(qreva(x, y), nil) = qreva(y, x)
HipSpec rev-equiv-goal6 ∀x, y. append(qreva(x, y), z) = qreva(x, append(y, z))
7
Related Work and Conclusions
Inductive reasoning is supported, with diﬀerent degrees of human intervention,
by many theorem provers, such as ACL2 [27], CLAM [23], Isabelle [34], Hip-
Spec [6], Zeno [40], and PVS [35]. The combination of inductive reasoning and
SMT solving techniques has been exploited by many tools for program veriﬁca-
tion [30,36,38,41,43,44].
Leino [30] integrates inductive reasoning into the Dafny program veriﬁer
by implementing a simple strategy that rewrites user-deﬁned properties that
may beneﬁt from induction into proof obligation to be discharged by Z3. The
advantage of this technique is that it fully decouples inductive reasoning from
SMT solving. Hence, no extensions to the SMT solver are required.
In order to extend CVC4 with induction, Reynolds and Kuncak [38] also
consider the rewriting of formulas that may take advantage from inductive rea-
soning, but this is done dynamically, during the proof search. This approach
allows CVC4 to perform the rewritings lazily, whenever new formulas are gen-
erated during the proof search, and to use the partially solved conjecture, to
generate lemmas that may help in the proof of the initial conjecture.

Removing ADTs from CHCs Using Diﬀerence Predicates
99
The issue of generating suitable lemmas during inductive proofs has been also
addressed by Yang et al. [44] and implemented in AdtInd. In order to conjec-
ture new lemmas, their algorithm makes use of a syntax-guided synthesis strat-
egy driven by a grammar, which is dynamically generated from user-provided
templates and the function and predicate symbols encountered during the proof
search. The derived lemma conjectures are then checked by the SMT solver Z3.
In order to take full advantage of the eﬃciency of SMT solvers in checking
satisﬁability of quantiﬁer-free formulas over LIA, ADTs, and ﬁnite sets, the Leon
veriﬁcation system [41] implements an SMT-based solving algorithm to check the
satisﬁability of formulas involving recursively deﬁned ﬁrst-order functions. The
algorithm interleaves the unrolling of recursive functions and the SMT solving of
the formulas generated by the unrolling. Leon can be used to prove properties of
Scala programs with ADTs and integrates with the Scala compiler and the SMT
solver Z3. A reﬁned version of that algorithm, restricted to catamorphisms, has
been implemented into a solver-agnostic tool, called RADA [36].
In the context of CHCs, Unno et al. [43] have proposed a proof system that
combines inductive theorem proving with SMT solving. This approach uses Z3-
PDR [21] to discharge proof obligations generated by the proof system, and has
been applied to prove relational properties of OCaml programs.
The distinctive feature of the technique presented in this paper is that it
does not make use of any explicit inductive reasoning, but it follows a transfor-
mational approach. First, the problem of verifying the validity of a universally
quantiﬁed formula over ADTs is reduced to the problem of checking the satisﬁ-
ability of a set of CHCs. Then, this set of CHCs is transformed with the aim of
deriving a set of CHCs over basic types (i.e., integers) only, whose satisﬁability
implies the satisﬁability of the original set. In this way, the reasoning on ADTs
is separated from the reasoning on satisﬁability, which can be performed by spe-
cialized engines for CHCs on basic types (e.g. Eldarica [22] and Z3-Spacer [29]).
Some of the ideas presented here have been explored in [11,12], but there neither
formal results nor an automated strategy were presented.
A key success factor of our technique is the introduction of diﬀerence predi-
cates, which can be viewed as the transformational counterpart of lemma genera-
tion. Indeed, as shown in Sect. 6, the use of diﬀerence predicates greatly increases
the power of CHC solving with respect to previous techniques based on the trans-
formational approach, which do not use diﬀerence predicates [10].
As future work, we plan to apply our transformation-based veriﬁcation tech-
nique to more complex program properties, such as relational properties [9].
References
1. Gopalakrishnan, G., Qadeer, S. (eds.): CAV 2011. LNCS, vol. 6806. Springer, Hei-
delberg (2011). https://doi.org/10.1007/978-3-642-22110-1
2. Barrett, C., Tinelli, C.: Satisﬁability modulo theories. In: Clarke, E., Henzinger, T.,
Veith, H., Bloem, R. (eds.) Handbook of Model Checking, pp. 305–343. Springer,
Cham (2018). https://doi.org/10.1007/978-3-319-10575-8 11

100
E. De Angelis et al.
3. Bjørner, N., Gurﬁnkel, A., McMillan, K., Rybalchenko, A.: Horn Clause Solvers for
Program Veriﬁcation. In: Beklemishev, L.D., Blass, A., Dershowitz, N., Finkbeiner,
B., Schulte, W. (eds.) Fields of Logic and Computation II. LNCS, vol. 9300, pp.
24–51. Springer, Cham (2015). https://doi.org/10.1007/978-3-319-23534-9 2
4. Bundy, A.: The automation of proof by mathematical induction. In: Robinson, A.,
Voronkov, A. (eds.) Handbook of Automated Reasoning, vol. I, pp. 845–911. North
Holland (2001)
5. Cimatti, A., Griggio, A., Schaafsma, B.J., Sebastiani, R.: The MathSAT5 SMT
solver. In: Piterman, N., Smolka, S.A. (eds.) TACAS 2013. LNCS, vol. 7795, pp.
93–107. Springer, Heidelberg (2013). https://doi.org/10.1007/978-3-642-36742-7 7
6. Claessen, K., Johansson, M., Ros´en, D., Smallbone, N.: Automating inductive
proofs using theory exploration. In: Bonacina, M.P. (ed.) CADE 2013. LNCS
(LNAI), vol. 7898, pp. 392–406. Springer, Heidelberg (2013). https://doi.org/10.
1007/978-3-642-38574-2 27
7. Cousot, P., Halbwachs, N.: Automatic discovery of linear restraints among vari-
ables of a program. In: Proceedings of the Fifth ACM Symposium on Principles of
Programming Languages, POPL 1978, pp. 84–96. ACM (1978)
8. De Angelis, E., Fioravanti, F., Pettorossi, A., Proietti, M.: VeriMAP: a tool for
verifying programs through transformations. In: ´Abrah´am, E., Havelund, K. (eds.)
TACAS 2014. LNCS, vol. 8413, pp. 568–574. Springer, Heidelberg (2014). https://
doi.org/10.1007/978-3-642-54862-8 47. http://www.map.uniroma2.it/VeriMAP
9. De Angelis, E., Fioravanti, F., Pettorossi, A., Proietti, M.: Relational veriﬁcation
through Horn clause transformation. In: Rival, X. (ed.) SAS 2016. LNCS, vol.
9837, pp. 147–169. Springer, Heidelberg (2016). https://doi.org/10.1007/978-3-
662-53413-7 8
10. De Angelis, E., Fioravanti, F., Pettorossi, A., Proietti, M.: Solving Horn clauses
on inductive data types without induction. Theor. Pract. Logic Program. 18(3–4),
452–469 (2018). Special Issue on ICLP 2018
11. De Angelis, E., Fioravanti, F., Pettorossi, A., Proietti, M.: Lemma generation for
Horn clause satisﬁability: a preliminary study. In: Lisitsa, A., Nemytykh, A.P.
(eds.) Proceedings Seventh International Workshop on Veriﬁcation and Program
Transformation, VPT@Programming 2019, EPTCS, Genova, Italy, 2nd April 2019,
vol. 299, pp. 4–18 (2019)
12. De Angelis, E., Fioravanti, F., Pettorossi, A., Proietti, M.: Proving properties
of sorting programs: a case study in Horn clause veriﬁcation. In: De Angelis,
E., Fedyukovich, G., Tzevelekos, N., Ulbrich, M. (eds.) Proceedings of the Sixth
Workshop on Horn Clauses for Veriﬁcation and Synthesis and Third Workshop
on Program Equivalence and Relational Reasoning, HCVS/PERR@ETAPS 2019,
EPTCS, Prague, Czech Republic, 6–7 April 2019, vol. 296, pp. 48–75 (2019)
13. De Angelis, E., Fioravanti, F., Pettorossi, A., Proietti, M.: Removing algebraic
data types from constrained Horn clauses using diﬀerence predicates - Preliminary
version. CoRR (2020). http://arXiv.org/abs/2004.07749
14. de Moura, L., Bjørner, N.: Z3: an eﬃcient SMT solver. In: Ramakrishnan, C.R.,
Rehof, J. (eds.) TACAS 2008. LNCS, vol. 4963, pp. 337–340. Springer, Heidelberg
(2008). https://doi.org/10.1007/978-3-540-78800-3 24
15. Dixon, L., Fleuriot, J.: IsaPlanner: a prototype proof planner in Isabelle. In:
Baader, F. (ed.) CADE 2003. LNCS (LNAI), vol. 2741, pp. 279–283. Springer,
Heidelberg (2003). https://doi.org/10.1007/978-3-540-45085-6 22
16. Enderton, H.: A Mathematical Introduction to Logic. Academic Press, Cambridge
(1972)

Removing ADTs from CHCs Using Diﬀerence Predicates
101
17. Etalle, S., Gabbrielli, M.: Transformations of CLP modules. Theor. Comput. Sci.
166, 101–146 (1996)
18. Fioravanti, F., Pettorossi, A., Proietti, M.: Transformation rules for locally strat-
iﬁed constraint logic programs. In: Bruynooghe, M., Lau, K.-K. (eds.) Program
Development in Computational Logic. LNCS, vol. 3049, pp. 291–339. Springer,
Heidelberg (2004). https://doi.org/10.1007/978-3-540-25951-0 10
19. Fioravanti, F., Pettorossi, A., Proietti, M., Senni, V.: Generalization strategies
for the veriﬁcation of inﬁnite state systems. Theor. Pract. Logic Program. 13(2),
175–199 (2013). Special Issue on the 25th Annual GULP Conference
20. Grebenshchikov, S., Lopes, N.P., Popeea, C., Rybalchenko, A.: Synthesizing soft-
ware veriﬁers from proof rules. In: Proceedings of the ACM SIGPLAN Conference
on Programming Language Design and Implementation, PLDI 2012, pp. 405–416
(2012)
21. Hoder, K., Bjørner, N.: Generalized property directed reachability. In: Cimatti, A.,
Sebastiani, R. (eds.) SAT 2012. LNCS, vol. 7317, pp. 157–171. Springer, Heidelberg
(2012). https://doi.org/10.1007/978-3-642-31612-8 13
22. Hojjat, H., R¨ummer, P.: The ELDARICA Horn solver. In: Bjørner, N., Gurﬁnkel,
A. (eds.) 2018 Formal Methods in Computer Aided Design, FMCAD 2018, Austin,
TX, USA, 30 Oct–2 Nov 2018, pp. 1–7. IEEE (2018)
23. Ireland, A., Bundy, A.: Productive use of failure in inductive proof. J. Autom.
Reason. 16(1), 79–111 (1996)
24. Jaﬀar, J., Maher, M.: Constraint logic programming: a survey. J. Logic Program.
19(20), 503–581 (1994)
25. Johansson, M., Dixon, L., Bundy, A.: Case-analysis for rippling and inductive
proof. In: Kaufmann, M., Paulson, L.C. (eds.) ITP 2010. LNCS, vol. 6172, pp.
291–306. Springer, Heidelberg (2010). https://doi.org/10.1007/978-3-642-14052-
5 21
26. Kaﬂe, B., Gallagher, J.P., Morales, J.F.: Rahft: a tool for verifying Horn clauses
using abstract interpretation and ﬁnite tree automata. In: Chaudhuri, S., Farzan,
A. (eds.) CAV 2016. LNCS, vol. 9779, pp. 261–268. Springer, Cham (2016). https://
doi.org/10.1007/978-3-319-41528-4 14
27. Kaufmann, M., Manolios, P., Moore, J.S.: Computer-Aided Reasoning: An App-
roach. Kluwer Academic Publishers, Berlin (2000)
28. Komuravelli, A., Gurﬁnkel, A., Chaki, S.: SMT-based model checking for recursive
programs. In: Biere, A., Bloem, R. (eds.) CAV 2014. LNCS, vol. 8559, pp. 17–34.
Springer, Cham (2014). https://doi.org/10.1007/978-3-319-08867-9 2
29. Komuravelli, A., Gurﬁnkel, A., Chaki, S., Clarke, E.M.: Automatic abstraction in
SMT-based unbounded software model checking. In: Sharygina, N., Veith, H. (eds.)
CAV 2013. LNCS, vol. 8044, pp. 846–862. Springer, Heidelberg (2013). https://doi.
org/10.1007/978-3-642-39799-8 59
30. Leino, K.R.M.: Automating induction with an SMT solver. In: Kuncak, V.,
Rybalchenko, A. (eds.) VMCAI 2012. LNCS, vol. 7148, pp. 315–331. Springer,
Heidelberg (2012). https://doi.org/10.1007/978-3-642-27940-9 21
31. Leroy, X., Doligez, D., Frisch, A., Garrigue, J., R´emy, D., Vouillon, J.: The OCaml
system, Release 4.06. Documentation and user’s manual, Institut National de
Recherche en Informatique et en Automatique, France (2017)
32. Leuschel, M., Butler, M.: ProB: a model checker for B. In: Araki, K., Gnesi, S.,
Mandrioli, D. (eds.) FME 2003. LNCS, vol. 2805, pp. 855–874. Springer, Heidelberg
(2003). https://doi.org/10.1007/978-3-540-45236-2 46
33. Lloyd, J.W.: Foundations of Logic Programming, 2nd edn. Springer, Berlin (1987).
https://doi.org/10.1007/978-3-642-83189-8

102
E. De Angelis et al.
34. Nipkow, T., Wenzel, M., Paulson, L.C.: Isabelle/HOL: A Proof Assistant for
Higher-Order Logic. Springer, Heidelberg (2002)
35. Owre, S., Rushby, J.M., Shankar, N.: PVS: a prototype veriﬁcation system. In:
Kapur, D. (ed.) CADE 1992. LNCS, vol. 607, pp. 748–752. Springer, Heidelberg
(1992). https://doi.org/10.1007/3-540-55602-8 217
36. Pham, T.-H., Gacek, A., Whalen, M.W.: Reasoning about algebraic data types
with abstractions. J. Autom. Reason. 57(4), 281–318 (2016)
37. Rabin, M.O.: Decidable theories. In: Barwise, J. (ed.) Handbook of Mathematical
Logic, pp. 595–629. North-Holland, Amsterdam (1977)
38. Reynolds, A., Kuncak, V.: Induction for SMT solvers. In: D’Souza, D., Lal, A.,
Larsen, K.G. (eds.) VMCAI 2015. LNCS, vol. 8931, pp. 80–98. Springer, Heidelberg
(2015). https://doi.org/10.1007/978-3-662-46081-8 5
39. Seki, H.: On Inductive and coinductive proofs via unfold/fold transformations.
In: De Schreye, D. (ed.) LOPSTR 2009. LNCS, vol. 6037, pp. 82–96. Springer,
Heidelberg (2010). https://doi.org/10.1007/978-3-642-12592-8 7
40. Sonnex, W., Drossopoulou, S., Eisenbach, S.: Zeno: an automated prover for prop-
erties of recursive data structures. In: Flanagan, C., K¨onig, B. (eds.) TACAS 2012.
LNCS, vol. 7214, pp. 407–421. Springer, Heidelberg (2012). https://doi.org/10.
1007/978-3-642-28756-5 28
41. Suter, P., K¨oksal, A.S., Kuncak, V.: Satisﬁability modulo recursive programs. In:
Yahav, E. (ed.) SAS 2011. LNCS, vol. 6887, pp. 298–315. Springer, Heidelberg
(2011). https://doi.org/10.1007/978-3-642-23702-7 23
42. Tamaki, H., Sato, T.: A generalized correctness proof of the unfold/fold logic pro-
gram transformation. Technical Report 86–4, Ibaraki University, Japan (1986)
43. Unno, H., Torii, S., Sakamoto, H.: Automating induction for solving Horn clauses.
In: Majumdar, R., Kunˇcak, V. (eds.) CAV 2017. LNCS, vol. 10427, pp. 571–591.
Springer, Cham (2017). https://doi.org/10.1007/978-3-319-63390-9 30
44. Yang, W., Fedyukovich, G., Gupta, A.: Lemma synthesis for automating induction
over algebraic data types. In: Schiex, T., de Givry, S. (eds.) CP 2019. LNCS, vol.
11802, pp. 600–617. Springer, Cham (2019). https://doi.org/10.1007/978-3-030-
30048-7 35

Solving Bitvectors with MCSAT:
Explanations from Bits and Pieces
St´ephane Graham-Lengrand(B), Dejan Jovanovi´c, and Bruno Dutertre
SRI International, Menlo Park, USA
stephane.graham-lengrand@csl.sri.com
Abstract. We present a decision procedure for the theory of ﬁxed-
sized bitvectors in the MCSAT framework. MCSAT is an alternative
to CDCL(T) for SMT solving and can be seen as an extension of CDCL
to domains other than the Booleans. Our procedure uses BDDs to record
and update the sets of feasible values of bitvector variables. For explain-
ing conﬂicts and propagations, we develop specialized word-level inter-
polation for two common fragments of the theory. For full generality,
explaining conﬂicts outside of the covered fragments resorts to local bit-
blasting. The approach is implemented in the Yices 2 SMT solver and
we present experimental results.
1
Introduction
Model-constructing satisﬁability (MCSAT) [19,20,24] is an alternative to the
CDCL(T ) scheme [28] for Satisﬁability Modulo Theories (SMT). While CDCL(T )
interfaces a CDCL SAT solver [23] with black-box decision procedures, MCSAT
integrates ﬁrst-order reasoning into CDCL directly. Like CDCL, MCSAT alter-
nates between search and conﬂict analysis. In the search phase, MCSAT assigns
values to ﬁrst-order variables and propagates unit consequences of these assign-
ments. If a conﬂict occurs during search, e.g., when the domain of a ﬁrst-order
variable is empty, MCSAT enters conﬂict analysis and learns an explanation,
which is a symbolic representation of what was wrong with the assignments
causing the conﬂict. As in CDCL, the learned clause triggers backtracking from
which search can resume. Decision procedures based on MCSAT have demon-
strated strong performance in theories such as non-linear real [24] and integer
arithmetic [19]. These theories are relatively well-behaved and provide features
such as quantiﬁer elimination and interpolation—the building blocks of conﬂict
resolution in MCSAT.
We describe an MCSAT decision procedure for the theory of bitvectors (BV).
In contrast to arithmetic, the complexity of BV in terms of syntax and semantics,
combined with the lack of word-level interpolation and quantiﬁer elimination,
makes the development of BV decision procedures (MCSAT or not) very diﬃ-
cult. The state-of-the art BV decision procedures are all based on a “preprocess
and bitblast” pipeline [12,22,25]: they reduce the BV problems to a pure SAT
problem by reducing the word-level semantics to bit-level semantics. Exceptions
c
⃝Springer Nature Switzerland AG 2020
N. Peltier and V. Sofronie-Stokkermans (Eds.): IJCAR 2020, LNAI 12166, pp. 103–121, 2020.
https://doi.org/10.1007/978-3-030-51074-9_7

104
S. Graham-Lengrand et al.
to the bitblasting approach do exist, such as [4,16], which generally do not per-
form as well as bitblasting except on small classes of crafted examples, and the
MCSAT approach of [30], which we discuss below and in the conclusion.
An MCSAT decision procedure must provide two theory-speciﬁc reasoning
mechanisms.
First, the procedure must maintain a set of values that are feasible for each
variable. This set is updated during the search. It is used to propagate variable
values and to detect a conﬂict when the set becomes empty. Finding a suitable
representation for domains is a key step in integrating a theory into MCSAT. We
represent variable domains with Binary Decision Diagrams (BDDs) [5]. BDDs
can represent any set of bitvector values. By being canonical, they oﬀer a simple
mechanism to detect when a domain becomes a singleton—in which case MCSAT
can perform a theory propagation—and when a domain becomes empty–in which
case MCSAT enters conﬂict analysis. In short, BDDs oﬀer a generic mechanism
for proposing and propagating values, and for detecting conﬂicts. In contrast,
previous work by Zelji´c et al. [30] represents bitvector domains using intervals
and patterns, which cannot represent every set of bitvector values precisely; they
over-approximate the domains.
Second, once a conﬂict has been detected, the procedure must construct a
symbolic explanation of the conﬂict. This explanation must rule out the par-
tial assignment that caused the conﬂict, but it is desirable for explanations to
generalize and rule out larger parts of the search space. For this purpose, pre-
vious work [30] relied on incomplete abstraction techniques (replace a value by
an interval; extend a value into a larger set by leaving some bits unassigned),
and left open the idea of using interpolation to produce explanations. Instead
of aiming for a uniform, generic explanation mechanism, we take a modular
approach. We develop eﬃcient word-level explanation procedures for two useful
fragments of BV, based on interpolation. Our ﬁrst fragment includes bitvec-
tor equalities, extractions, and concatenations where word-level explanations
can be constructed through model-based variants of classic equality reasoning
techniques (e.g., [4,9,10]). Our second fragment is a subset of linear arithmetic
where explanations are constructed by interval reasoning in modular arithmetic.
When conﬂicts do not ﬁt into either fragment, we build an explanation by bit-
blasting and extracting an unsat core. Although this fallback produces theory
lemmas expressed at the bit-level, it is used only as a last resort. In addition, this
bitblasting-based procedure is local and limited to constraints that are relevant
to the current conﬂict; we do not apply bitblasting to the full problem.
Section 2, is an overview of MCSAT. It also presents the BDD approach and
general considerations for conﬂict explanation. Section 3 describes our interpola-
tion algorithm for equality with concatenation and extraction. Section 4 presents
our interpolation method for a fragment of linear bitvector arithmetic. Section 5
presents the normalization technique we apply to conﬂicts in the hope of express-

Solving Bitvectors with MCSAT: Explanations from Bits and Pieces
105
ing them in that bitvector arithmetic fragment. Section 6 presents an evaluation
of the approach, which we implemented in the Yices 2 solver [11].1
2
A General Scheme for Bitvectors
By BV, we denote the theory of quantiﬁer-free ﬁxed-sized bitvectors, a.k.a.
QF BV in SMT-LIB [1]. A ﬁrst-order term u of BV is sorted as either a Boolean
or a bitvector of a ﬁxed length (a.k.a. bitwidth), denoted |u|. Its set of variables
(a.k.a. uninterpreted constants) is denoted var(u). This paper only uses a few
BV operators. The concatenation of bitvector terms t and u is denoted t ◦u;
the binary predicates <u, ≤u denote unsigned comparisons, and <s, ≤s denote
signed comparisons. In such comparisons, both operands must have the same
bitwidth. If n is the bitwidth of u, and l and h are two integer indices such that
0 ≤l < h ≤n, then u[h:l], extracts h−l bits of u, namely the bits at indices
between l and h−1 (included). We write u[:l], u[h:], and u[l] as abbreviations
for u[n:l], u[h:0], and u[l+1:l], respectively. Our convention is to have bitvector
indices start from the right-hand side, so that bit 0 is the right-most bit and
0011[2:] is 11. We use standard notations for bitvector arithmetic, which coin-
cides with arithmetic modulo 2w where w is the bitwidth. We sometimes use
integer constants e.g., 0, 1, −1 for bitvectors when the bitwidth is clear. We use
the standard (quantiﬁer-free) notions of literal, clause, cube, and formula [29].
A model of a BV formula Φ is an assignment that gives a bitvector (resp.
Boolean) value to all bitvector (resp. Boolean) variables of Φ, in such a way that
Φ evaluates to true, under the standard interpretation of Boolean and bitvector
symbols. To simplify the presentation, we assume in this paper that there are
no Boolean variables, although they are supported in our implementation.
2.1
MCSAT Overview
MCSAT searches for a model of an input quantiﬁer-free formula by building
a partial assignment—maintained in a trail—and extends the concepts of unit
propagation and consistency to ﬁrst-order terms and literals [19,20,24]. Reason-
ing is implemented by theory-speciﬁc plugins, each of which has a partial view of
the trail. In the case of BV, the bitvector plugin sees in the trail an assignment
M of the form x1 →v1, . . . , xn →vn that gives values to bitvector variables,
and a set of bitvector literals L1, . . . , Lt, called constraints, that must be true
in the current trail. MCSAT and its bitvector plugin maintain the invariant that
none of the literals Li evaluates to false under M; either Li is true or some
variable of Li has no value in M. To maintain this invariant, they detect unit
inconsistencies: We say that literal Li is unit in y if y is the only unassigned
variable of Li, and that a trail is unit inconsistent if there is a variable y and a
subset {C1, . . . , Cm} of {L1, . . . , Lt}, called a conﬂict, such that every Cj is unit
1 This paper extends preliminary results presented at the SMT workshop [13,14] and
includes a full implementation and experimental evaluation.

106
S. Graham-Lengrand et al.
in y and the formula ∃y m
i = 1 Ci evaluates to false under M. In such a case, y
is called the conﬂict variable and C1, . . . , Cm are called the conﬂict literals.
When such a conﬂict is detected, the current assignment, or partial model, M
cannot be extended to a full model; some values assigned to x1, . . . , xn must be
revised. As in CDCL, MCSAT backtracks and updates the current assignment by
learning a new clause that explains the conﬂict. This new clause must not contain
other variables than x1, . . . , xn and it must rule out the current assignment. For
some theories, this conﬂict explanation can be built by quantiﬁer elimination.
More generally, we can build an explanation from an interpolant.
Deﬁnition 1 (Interpolant). A clause I is an interpolant2 for formula F at
model M assigning values to x1, . . . , xn, if (1) F ⇒I is valid (in BV), (2) The
variables in I are in {x1, . . . , xn} ∩var(F), and (3) I evaluates to false in M.
Given an interpolant I for the conjunction m
i = 1 Ci of the conﬂict literals (or
equivalently, for ∃y m
i = 1 Ci) at the current model M, the conﬂict explanation
is clause (m
i = 1 Ci) ⇒I. Our main goal is constructing such interpolants in BV.
2.2
BDD Representation and Conﬂict Detection
To detect conﬂicts, we must keep track of the set of feasible values for every
unassigned variable y. These sets are frequently updated during search so an
eﬃcient representation is critical. The following operations are needed:
– updating the set when a new constraint becomes unit in y,
– detecting when the set becomes empty,
– selecting a value from the set.
For BV, Zelji´c et al. [30] represent sets of feasible values using both inter-
vals and bit patterns. For example, the set deﬁned by the interval [0000, 0011]
and the pattern ???1 is the pair {0001, 0011} (i.e., all bitvectors in the interval
whose low-order bit is 1). This representation is lightweight and eﬃcient but it
is not precise. Some sets are not representable exactly. We use Binary Decision
Diagrams (BDD) [5] over the bits of y. The major advantage is that BDDs pro-
vide an exact implementation of any set of values for y. Updating sets of values
amounts to computing the conjunction of BDDs (i.e., set intersection). Checking
whether a set is empty and selecting a value in the set (if it is not), can be done
eﬃciently by, respectively, checking whether the BDD is false, and performing
a top-down traversal of the BDD data structure. There is a risk that the BDD
representation explodes but this risk is reduced in our context since each BDD
we build is for a single variable (and most variables do not have too many bits).
We use the CUDD package [8] to implement BDDs.
2 This is the same as the usual notion of (reverse) interpolant between formulas if we
see M as the formula FM deﬁned by (x1 ≃v1) ∧· · · ∧(xn ≃vn): the interpolant is
implied by F, it is inconsistent with FM, and its variables occur in both F and FM.

Solving Bitvectors with MCSAT: Explanations from Bits and Pieces
107
2.3
Baseline Conﬂict Explanation
Given a conﬂict as described previously, the clause (x1 ̸≃v1) ∨· · · ∨(xn ̸≃vn),
which is falsiﬁed by model M only, is an interpolant for m
i = 1 Ci at M according
to Deﬁnition 1. This gives the following trivial conﬂict explanation:
C1 ∧· · · ∧Cm ⇒(x1 ̸≃v1) ∨· · · ∨(xn ̸≃vn)
We seek to generalize model M with a formula that rules out bigger parts of the
search space than just M. A ﬁrst improvement is replacing the constraints by a
core C, that is, a minimal subset of {C1, . . . , Cn} that evaluates to false in M.3
To produce the interpolant I, we can bitblast the constraints C1, . . . , Cm
and solve the resulting SAT problem under the assumptions that each bit of
x1, . . . , xn is true or false as indicated by the values v1, . . . , vn. Since the SAT
problem encodes a conﬂict, the SAT solver will return an unsat core, from which
we can extract bits of v1, . . . , vn that contribute to unsatisﬁability. This gener-
alizes M by leaving some bits unassigned, as in [30].
This method is general. It works whatever the constraints C1, . . . , Cm, so we
use it as a default procedure. The bitblasting step focuses on constraints that are
unit in y, which typically leads to a much smaller SAT problem than bitblasting
the whole problem from the start. However, the bitblasting approach can still
be costly and it may produce weak explanations.
Example 1. Consider the constraints {x1 ̸≃x2, x1 ≃y, x2 ≃y} and the assign-
ment x1 →1001, x2 →0101. The bitblasting approach might produce explana-
tion (x1 ≃y ∧x2 ≃y) ⇒(x1[3] ⇒x2[3]). After backtracking, we might similarly
learn that (x2[3] ⇒x1[3]). In this way, it will take eight iterations to learn
enough information to represent the high-level explanation:
(x1 ≃y ∧x2 ≃y) ⇒x1 ≃x2.
A procedure that can produce (x1 ≃x2) directly is much more eﬃcient.
3
Equality, Concatenation, Extraction
Our ﬁrst specialized interpolation mechanism applies when constraints C =
{C1, . . . , Cm} belong to the following grammar:
Constraints
C ::= t ≃t | t ̸≃t
Terms
t
::= e | y[h:l] | t ◦t
where e ranges over any bitvector terms such that y ̸∈var(e). Without loss of
generality, we can assume that C is a core. We split C into a set of equalities
E = {ai ≃bi}i ∈E and a set of disequalities D = {ai ̸≃bi}i ∈D.
3 In our implementation, we construct C using the QuickXplain algorithm [21].

108
S. Graham-Lengrand et al.
Algorithm 1. E-graph with value management
1: function e graph(Es, M)
2:
Initialize(G)
▷each evaluable term or slice is its own component
3:
for t1 ≃t2 ∈Es do
4:
t′
1 ←rep(t1, G)
▷get representative for t1’s component
5:
t′
2 ←rep(t2, G)
▷get representative for t2’s component
6:
if y ̸∈var(t′
1) and y ̸∈var(t′
2) and [[t′
1]]M ̸= [[t′
2]]M then
7:
raise conﬂict(E ⇒t′
1 ≃t′
2)
▷D must be empty
8:
t3 ←select(t′
1, t′
2)
▷select representative for merged component
9:
G ←merge(t1, t2, t3, G)
▷merge the components with representative t3
10:
return G
Slicing. Our ﬁrst step rewrites C into an equivalent sliced form. This computes
the coarsest-base slicing [4,9] of equalities and disequalities in C. The goal of
this rewriting step is to split the variables into slices that can be treated as
independent terms. The terms in coarsest-base slicing are either of the form
y[h:l] (slices), or are evaluable terms e with y ̸∈var(e).
Example 2. Consider the constraints E = {x1[4:0] ≃x1[8:4], y[6:2] ≃y[4:0]} and
{y[4:0] ̸≃x1[8:4]} over variables y of length 6, and x1 of length 8. We cannot
treat y[6:2] and y[4:0] as independent terms because they overlap. To break
the overlap, we introduce slices: y[6:4], y[4:2], and y[2:0]. Equality y[6:2] ≃y[4:0]
is rewritten to (y[6:4] ≃y[4:2]) ∧(y[4:2] ≃y[2:0]). Disequality y[4:0] ̸≃x1[8:4] is
rewritten to (y[4:2] ̸≃x1[8:6]) ∨(y[2:0] ̸≃x1[6:4]). The ﬁnal result is
Es = { x1[4:2] ≃x1[8:6] , x1[2:0] ≃x1[6:4] , y[6:4] ≃y[4:2] , y[4:2] ≃y[2:0] },
Ds = { (y[4:2] ̸≃x1[8:6]) ∨(y[2:0] ̸≃x1[6:4]) }.
Explanations. After slicing, we obtain a set Es of equalities and a set Ds that
contains disjunctions of disequalities. We can treat each slice as a separate vari-
able, so the problem lies within the theory of equality on a ﬁnite domain.
We ﬁrst analyze the conﬂict with equality reasoning against the model, as
shown in Algorithm 1. We construct the E-graph G from Es [10], while also
taking into account the partial model M that triggered the conﬂict. The model
can evaluate terms e such that y ̸∈var(e) to values [[e]]M, and those can be the
source of the conﬂict. To use the model for evaluating terms, we maintain two
invariants during E-graph construction:
1. If a component contains an evaluable term c, then the representative of that
component is evaluable.
2. Two evaluable terms c1 and c2 in the same component must evaluate to the
same value, otherwise this is the source of the conﬂict.
The E-graph construction can detect and explain basic conﬂicts between the
equalities in E and the current assignment.
Example 3. Let r1, r2, r3 be bit ranges of the same width. Let E be such that
Es = {x1[r1] ≃y[r3], x2[r2] ≃y[r3]}, and let D = ∅. Consider the model M :=

Solving Bitvectors with MCSAT: Explanations from Bits and Pieces
109
Algorithm 2. Disequality conﬂict
1: function dis conflict(Ds, M, G)
2:
S ←∅
▷where we collect interface terms
3:
C0 ←∅
▷where we collect the disequalities that evaluate to false
4:
for C ∈Ds do
5:
Crep
M ←{rep(t1, G) ̸≃rep(t2, G) | (t1 ̸≃t2) ∈CM}
6:
if is empty(Cinterface) and is empty(Cfree) then
7:
raise conﬂict(E ∧D ⇒Crep
M)
8:
else
9:
C0 ←C0 ∨Crep
M
▷we collect the disequalities made false in the model
10:
for t1 ̸≃t2 ∈Cinterface with y ̸∈var(rep(t1, G)) do
11:
S ←S ∪{rep(t1, G)}
▷we collect the interface term
12:
C̸= ←{t1 ≃t2 | [[t1]]M ̸= [[t2]]M, t1, t2 ∈S}
13:
C= ←{t1 ̸≃t2 | [[t1]]M = [[t2]]M, t1 ̸= t2, t1, t2 ∈S}
14:
return E ∧D ⇒C0 ∨C̸= ∨C=
x1 →0 . . . 0, x2 →1 . . . 1. Then, e graph(Es, M) produces the conﬂict clause
E ⇒x1[r1] ≃x2[r2].
If the E-graph construction does not raise a conﬂict, then M is compatible
with the equalities in Es. Since C conﬂicts with M, the conﬂict explanation must
involve Ds. To obtain an explanation, we decompose each disjunct C ∈Ds into
(CEs ∨CM ∨Cinterface ∨Cfree) as follows.
– CEs contains disequalities t1 ̸≃t2 such that t1 and t2 have the same E-graph
representatives; such disequalities are false because of the equalities in Es.
– CM contains disequalities t1 ̸≃t2 such that t1 and t2 have distinct represen-
tatives t′
1 and t′
2 with [[t′
1]]M = [[t′
2]]M; these are false because of M.
– Cinterface contains disequalities t1 ̸≃t2 such that t1 and t2 have distinct repre-
sentatives t′
1 and t′
2, t′
1 is evaluable and t′
2 is a slice; we can still satisfy t1 ̸≃t2
by picking a good value for y; we say t′
1 is an interface term.
– Cfree contains disequalities t1 ̸≃t2 such that t1 and t2 have distinct slices as
representatives; we can still satisfy t1 ̸≃t2 by picking a good value for y.
The disjuncts in Ds take part in the conﬂict either when (i) one of the clauses in
Ds is false because Cinterface and Cfree are both empty; or (ii) the ﬁnite domains
are too small to satisfy the disequalities in Cinterface and Cfree, given the val-
ues assigned in M. In either case, we can produce a conﬂict explanation with
Algorithm 2.
In a type (i) conﬂict, the algorithm produces an interpolant Crep
M that is
derived from a single element of Ds. Because we assume that C is a core, a
type (i) conﬂict can happen only if Ds is a singleton. Here is how the algorithm
behaves on such a conﬂict:
Example 4. Let r1 and r2 be bit ranges of the same length, let r3, r4, r5 be bit
ranges of the same length. Assume Es contains
{ x1[r1] ≃y[r1] , x2[r2] ≃y[r2] , y[r3] ≃y[r5] , y[r4] ≃y[r5] },

110
S. Graham-Lengrand et al.
and assume Ds is the singleton { (y[r1] ̸≃y[r2] ∨y[r3] ̸≃y[r4]) }. Let M map x1
and x2 to 0 . . . 0 and assume y[r5] is the E-graph representative for component
{ y[r3], y[r4], y[r5] }.
The unique clause of Ds contains two disequalities:
– The ﬁrst one, y[r1] ̸≃y[r2], belongs to CM because the representatives of y[r1]
and y[r2], namely x1[r1] and x2[r2], both evaluate to 0 . . . 0.
– The second one, y[r3] ̸≃y[r4], belongs to CEs because the representatives of
y[r3] and y[r4] are both y[r5],
As Cinterface and Cfree are empty, Algorithm 2 outputs E ∧D ⇒x1[r1] ̸≃x2[r2].
For a conﬂict of type (ii), the equalities and disequalities that hold in M
between the interface terms make the slices of y require more values than there
exist. So the produced conﬂict clause includes (the negation of) all such equalities
and disequalities. An example can be given as follows:
Example 5. Assume E (and then Es) is empty and assume Ds is
{ x2[0] ̸≃x2[1] ∨y[0] ̸≃y[1] , x1[0] ̸≃y[0] , x1[1] ̸≃y[1] }
Let M map x1 and x2 to 00. Then dis conflict(Ds, M, G) behaves as follows:
– In the ﬁrst clause, call it C, the ﬁrst disequality is in CM, as the two sides
are in diﬀerent components but evaluate to the same value; so C0 becomes
{ x2[0] ̸≃x2[1] }; the second disequality features two slices and is thus in Cfree;
The clause is potentially satisﬁable and we move to the next clause.
– The second clause contains a single disequality that cannot be evaluated (since
y[0] is not evaluable in M). Term x1[0] is added to S. The clause is potentially
satisﬁable so we move to the next clause.
– The third clause of Ds is similar. It contains a single disequality that cannot
be evaluated. The interface term x1[1] is added to S.
Since all clauses of Ds have been processed, the conﬂict is of type (ii). Indeed,
y[0] must be diﬀerent from 0 because of the second clause, y[1] must also be
diﬀerent from 0 because of the third clause, but y[0] and y[1] must be diﬀerent
from each other because of the ﬁrst clause. Since both y[0] and y[1] have only one
bit, there are only two possible values for these two slices, so the three constrains
are in conﬂict. Algorithm 2 produces the conﬂict clause
D ⇒( x2[0] ̸≃x2[1] ∨x1[0] ̸≃x1[1] ).
The disequality x2[0] ̸≃x2[1] is necessary because, if it were true in M, we would
not have to satisfy y[0] ̸≃y[1] and therefore y ←11 would work. Disequality
x1[0] ̸≃x1[1] is also necessary because, if it were true in M, say with x1 ←01
(resp. x1 ←10), then y ←11 (resp. y ←00) would work.
Correctness of the method relies on the following lemma, whose proof can be
found in [15].
Lemma 1 (The produced clauses are interpolants).
1. If Algorithm 1 reaches line 7, t′
1 ≃t′
2 is an interpolant for E ∧D at M.
2. If Algorithm 2 reaches line 7, Crep
M is an interpolant for E ∧D at M.
3. If it reaches line 14, C0 ∨C̸= ∨C= is an interpolant for E ∧D at M.

Solving Bitvectors with MCSAT: Explanations from Bits and Pieces
111
Table 1. Creating the forbidden intervals
Atom a
Forbidden interval that a (resp. ¬a) speciﬁes for t
Ia
I¬a
Condition ca/c¬a
e1 + t ≤u e2 + t [ −e2 ; −e1[
[ −e1 ; −e2[
e1 ̸≃e2
1
[0 ; 0[
full
e1 ≃e2
2
e1 ≤u e2 + t
[ −e2 ; e1 −e2[
[e1 −e2 ; −e2[
e1 ̸≃0
3
[0 ; 0[
full
e1 ≃0
4
e1 + t ≤u e2
[e2 −e1 + 1 ; −e1[ [ −e1 ; e2 −e1 + 1[ e2 ̸≃−1
5
[0 ; 0[
full
e2 ≃−1
6
4
A Linear Arithmetic Fragment
Our second specialized explanation mechanism applies when constraints C =
{C1, . . . , Cm} belong to the following grammar:
Constraints
C ::= a
¬a
Atoms
a
::= e1 + t ≤u e2 + t
e1 ≤u e2 + t
e1 + t ≤u e2
Terms
t
::= y[h:]
t[:l]
t + e1
−t
0k ◦t
t ◦0k
where e1 and e2 range over evaluable bitvector terms (i.e., y ̸∈var(e1)∪var(e2)),
and 0k is 0 on k bits. We can represent variable y as the term y[|y|:]. This frag-
ment of bitvector arithmetic is linear in y and there can be only one occurrence
of y in terms. Constraints in Sect. 3 are then outside this fragment in general.
Let A be ∃y(C1 ∧· · · ∧Cm), and M be the partial model involved in the
conﬂict. The interpolant for A at model M is (roughly) produced as follows:
1. For each constraint Ci, 1 ≤i ≤m, featuring a (necessarily unique) lower-bits
extract y[wi:], we compute a condition cube ci satisﬁed by M and a forbidden
interval Ii of the form [li ; ui[, where li and ui are evaluable terms, such that
ci ⇒(Ci ⇔(y[wi:] /∈Ii)) is valid.
2. We group the resulting intervals (Ii)1 ≤i ≤m according to their bitwidths: if
Sw is the set of intervals forbidding values for y[w:], 1 ≤w ≤|y|, then under
condition m
i = 1 ci formula A is equivalent to ∃y(|y|
w = 1 ( y[w:] /∈
I ∈Sw I )).
3. We produce a series of constraints d1,. . . , dp that are satisﬁed by M and
that are inconsistent with |y|
w = 1 ( y[w:] /∈
I ∈Sw I ). The interpolant will
be (m
i = 1 ci ∧p
i = 1 di) ⇒⊥: it is implied by A, and evaluates to false in M.
4.1
Forbidden Intervals
An interval takes the form [l ; u[, where the lower bound l and upper bound u are
evaluable terms of some bitwidth w, with l included and u excluded. The notion
of interval used here is considered modulo 2w. We do not require l ≤u u so an
interval may “wrap around” in Z/2wZ. For instance, the interval [1111 ; 0001[
contains two bitvector values, namely, 1111 and 0000. If l and u evaluate to the
same value, then we consider [l ; u[ to be empty (as opposed to the full domain,

112
S. Graham-Lengrand et al.
Fig. 1. Transforming the forbidden intervals
which we denote by fullw or just full). Notation t ∈I stands for literal ⊤if I is
full and literal t−l <u u−l if I is [l ; u[. The value in model M of an evaluable
term e (resp. evaluable cube c, interval I) is denoted [[e]]M (resp. [[c]]M, [[I]]M).
Given a constraint C with unevaluable term t, we produce an interval IC of
forbidden values for t according to the rules of Table 1. A side condition literal
cC identiﬁes when the lower and upper bounds would coincide, in which case the
interval produced is either empty or full. For every row of the table, the formula
cC ⇒(C ⇔t /∈IC) is valid in BV. Given a partial model M, we convert C to
such an interval by selecting the row where [[cC]]M = true.
Example 6.
6.1 Assume C1 is literal ¬(x1 ≤u y) and M = {x1 →0000}. Then line 4 of
Table 1 applies, and IC1 is interval full with condition x1 ≃0.
6.2 Assume C1 is ¬(y ≃x1), C2 is (x1 ≤u x3 +y), C3 is ¬(y −x2 ≤u x3 +y), and
M = {x1 →1100, x2 →1101, x3 →0000}. Then by line 5, IC1 = [x1 ; x1+1[
with trivial condition (0 ̸≃−1), by line 3, IC2 = [−x3 ; x1−x3[ with condition
(x1 ̸≃0), and by line 1, IC3 = [x2 ; −x3[ with condition (−x2 ̸≃x3).
Given the supported grammar, term t contains a unique subterm of the form
y[w:]. We transform IC into an interval of forbidden values for y[w:] by applying
procedure forbid( t , IC , cC ) shown in Fig. 1, which proceeds by recursion on t.
Its speciﬁcation is given below, and correctness is proved by induction on t.

Solving Bitvectors with MCSAT: Explanations from Bits and Pieces
113
Fig. 2. Intervals collected from C1 ∧· · · ∧Cm
Lemma 2 (Correctness of forbidden intervals). Assuming cube c is true
in M, then forbid( t , I , c ) returns a triple (w, I′, c′) such that c′ is a cube that
is true in M, and both c′ ⇒c and c′ ⇒(t /∈I ⇔y[w:] /∈I′) are valid in BV.
Running forbid( tCi , ICi , cCi ) for all constraints Ci, 1 ≤i ≤m, produces a family
of triples (wi, I′
i, c′
i)1 ≤i ≤m such that, for each i, formula c′
i ⇒(Ci ⇔(y[wi:] /∈
I′
i)) is valid in BV and c′
i is true in M.
4.2
Interpolant
First, assume that one of the triples obtained above is of the form (w, full, c),
coming from constraint C. As the interval forbids the full domain of values for
y[w:], we produce conﬂict clause C ∧c ⇒⊥. This formula is an interpolant for
A at M. This is illustrated in Example 7.1.
Example 7.
7.1 In Example 6.1 where C1 is literal ¬(x1 ≤u y) and M = {x1 →0000}, the
interpolant for ¬(x1 ≤u y) at M is (x1 ≃0) ⇒⊥.
7.2 Example 6.2 does not contain a full interval. Model M satisﬁes the three
conditions c1 := (0 ̸≃−1), c2 := (x1 ̸≃0) and c3 := (−x2 ̸≃x3), and the
intervals I1 = [x1 ; x1+1[, I2 = [−x3 ; x1−x3[, and I3 = [x2 ; −x3[, evaluate
to [[I1]]M = [1100 ; 1101[, [[I2]]M = [0000 ; 1100[, and [[I3]]M = [1101 ; 0000[,
respectively. Note how 3
i = 1[[Ii]]M is the full domain.
Assume now that no interval is full (as in Example 7.2). We group the triples
(w, I, c) into diﬀerent layers characterized by their bitwidths w: I will henceforth
be called a w-interval, restricting the feasible values for y[w:], and cI denotes its
associated condition in the triple. Ordering the groups of intervals by decreasing
bitwidths w1 > w2 > · · · > wj, as shown in Fig. 2, Sj denotes the set of produced
wj-intervals. The properties satisﬁed by the triples entail that
A ∧(j
i = 1

I∈SicI) ⇒B
is valid, where B is ∃y j
i = 1(y[wi:] /∈
I∈Si I). And formula (j
i = 1

I∈Si cI) ⇒
B is false in M. To produce an interpolant, we replace B by a quantiﬁer-free
clause.
The simplest case is when there is only one bitwidth w = w1: the fact that
B is falsiﬁed by M means that 
I∈S1[[I]]M is the full domain Z/2wZ. Property
“
I∈S1 I is the full domain” is then expressed symbolically as a conjunction
of constraints in the bitvector language. To compute them, we ﬁrst extract a
sequence I1, . . . , Iq of intervals from the set S1, originating from a subset C of

114
S. Graham-Lengrand et al.
Algorithm 3. Producing the interpolant with multiple bitwidths
1: function cover((S1, . . . , Sj), M)
2:
output ←∅
▷output initialized with the empty set of constraints
3:
longest ←longest(S1, M)
▷longest interval identiﬁed
4:
baseline ←longest.upper
▷where to extend the coverage from
5:
while [[baseline]]M ̸∈[[longest]]M do
6:
if ∃I ∈S1, [[baseline]]M ∈[[I]]M then
7:
I ←furthest extend(baseline, S1, M)
8:
output ←output ∪{cI, baseline ∈I}
▷adding I’s condition and linking constraint
9:
baseline ←I.upper
▷updating the baseline for the next interval pick
10:
else
▷there is a hole in the coverage of Z/2w1Z by intervals in S1
11:
next ←next covered point(baseline, S1, M)
▷the hole is [baseline ; next[
12:
if [[next]]M −[[baseline]]M <u 2w2 then
13:
I ←[next[w2:] ; baseline[w2:][
▷it is projected on w2 bits and complemented
14:
output ←output ∪{next−baseline <u 2w2} ∪cover(((S2 ∪I), S3, . . . , Sj), M)
15:
baseline ←next
▷updating the baseline for the next interval pick
16:
else
▷intervals of bitwidths ≤w2 must forbid all values for y[w2:]
17:
return cover((S2, . . . , Sj), M)
▷S1 was not needed
18:
return output ∪{baseline ∈longest}
▷adding ﬁnal linking constraint
the original constraints (Ci)m
i=1, and such that the sequence [[I1]]M, . . . , [[Iq]]M
of concrete intervals leaves no “hole” between an interval of the sequence and
the next, and goes round the full circle of domain Z/2wZ: the sequence forms
a circular chain of linking intervals. This chain can be produced by a standard
coverage extraction algorithm (see, e.g., [15]). Formula B := ∃y(y[w:] /∈
I∈S1 I)
is then replaced by (q
i = 1 ui ∈Ii+1) ⇒⊥, where ui is the upper bound of Ii and
Iq+1 is I1. Each interval has its upper bound in the next interval (ui ∈Ii+1),
i.e., intervals do link up with each other. The conﬂict clause is then
(C ∧(q
i = 1 cIi) ∧(q
i = 1 ui ∈Ii+1)) ⇒⊥
Example 8. For Example 7.2, the coverage-extraction algorithm produces the
sequence I1, I3, I2, i.e., [x1 ; x1+1[, [x2 ; −x3[, [ −x3 ; x1−x3[. The linking con-
straints are then d3 := (x1+1) ∈I3, d2 := (−x3) ∈I2, and d1 := (x1−x3) ∈I1,
and the interpolant is d3 ∧d2 ∧d1 ⇒⊥.4
When several bitwidths are involved, the intervals must “complement each
other” at diﬀerent bitwidths so that no value for y is feasible. For a bitwidth
wi, the union of the wi-intervals in model M may not necessarily cover the full
domain (i.e., 
I∈Si[[I]]M may be diﬀerent from Z/2wiZ). The coverage can leave
“holes”, and values in that hole are ruled out by constraints of other bitwidths.
To produce the interpolant, we adapt the coverage-extraction algorithm into
Algorithm 3, which takes as input the sequence of sets (S1, . . . , Sj) as described
in Fig. 2, and produces the interpolant’s constraints d1, . . . , dp, collected in set
output. The algorithm proceeds in decreasing bitwidth order, starting with w1,
and calling itself recursively on smaller bitwidths to cover the holes that the
current layer leaves uncovered (termination of that recursion is thus trivial). For
every hole that 
I∈S1[[I]]M leaves uncovered, it must determine how intervals of
smaller bitwidths can cover it.
4 We omit c1, c2, c3 here, since they are subsumed by d1, d2, d3, respectively.

Solving Bitvectors with MCSAT: Explanations from Bits and Pieces
115
Fig. 3. Rewriting rules
Algorithm 3 relies on the following ingredients:
– longest(S, M) returns an interval among S whose concrete version [[I]]M
has maximal length;
– I.upper denotes the upper bound of an interval I;
– furthest extend(a, S, M) returns an interval I
∈
S
that furthest
extends a according to M (technically, an interval I that ≤u-maximizes
[[I.upper −a]]M among those intervals I such that [[a]]M ∈[[I]]M).
– If no interval in S covers a in M, next covered point(a, S, M) outputs
the lower bound l of an interval in S that ≤u-minimizes [[l −a]]M.
Algorithm 3 proceeds by successively moving a concrete bitvector value baseline
around the circle Z/2w1Z. The baseline is moved when a symbolic reason why it
is a forbidden value is found, in a while loop that ends when the baseline has gone
round the full circle. If there is at least one interval in S1 that covers baseline in
M (l. 6), the call to furthest extend(baseline, S1, M) succeeds, and output
is extended with condition cI and (baseline ∈I) (l. 8). If not, a hole has been
discovered, whose extent is given by next covered point(baseline, S1, M) (l.
11). If the hole is bigger than 2w2 (i.e., 2w2 ≤u [[next−baseline]]M), then the
intervals of layers w2 and smaller must rule out every possible value for y[w2:],
and the w1-intervals were not needed (l. 17). If on the contrary the hole is smaller
(i.e., [[next−baseline]]M <u 2w2), then the w1-interval [baseline ; next[ is projected
as a w2-interval I := [baseline[w2:] ; next[w2:][ that needs to be covered by the
intervals of bitwidth w2 and smaller. This is performed by a recursive call on
bitwidth w2 (l. 14); the fact that only hole I needs to be covered by the recursive
call, rather than the full domain Z/2w2Z, is implemented by adding to S2 in the
recursive call the complement [next[w2:] ; baseline[w2:][ of I. The result of the
recursive call is added to the output variable, as well as the fact that the hole
must be small. The ﬁnal interpolant is (
d ∈output d) ⇒⊥. An example of run
on a variant of Example 6.2 is given in [15].
5
Normalization
As implemented in Yices 2, MCSAT processes a conﬂict by ﬁrst computing the
conﬂict core with BDDs, and then normalizing the constraints using the rules
of Fig. 3. In the ﬁgure, u, u1 and u2 stand for any bitvector terms, ±-extk(u) is

116
S. Graham-Lengrand et al.
Fig. 4. Evaluation of the MCSAT solver and the eﬀect of diﬀerent explainer combina-
tions and propagation. Each curve shows the number of benchmarks that the solver
variant can solve against the time.
the sign-extension of u with k bits, and bvnot(u) is the bitwise negation of u.
The bottom left rule is applied with lower priority than the others (as upper-bits
extraction distributes over ◦but not over +) and only if exactly one of {u1, u2}
is evaluable (and not 0). In the implementation, u[|u|:0] is identiﬁed with u, ◦
is associative, and +, × are subject to ring normalization. This is helped by the
internal (ﬂattened) representation of concatenations and bitvector polynomials
in Yices 2. Normalization allows the specialized interpolation procedure to apply
at least to the following grammar:5
Atoms
a ::= e1 + t ⋖e2 + t
e1 ⋖e2 + t
e1 + t ⋖e2
e1 ⋖e2
Terms
t ::= t[h:l]
t + e1
−t
e1 ◦t
t ◦e1
±-extk(t)
where ⋖∈{≤u, <u, ≤s, <s, ≃}. Rewriting can often help further, by eliminating
occurrences of the conﬂict variable (thus making more subterms evaluable) and
increasing the chances that two unevaluable terms t1 and t2 become syntactically
equal in an atom e1+t1 ⋖e2+t2.6 Finally, we cache evaluable terms to avoid
recomputing conditions of the form y /∈var(e). These conditions are needed to
determine whether the specialized procedures apply to a given conﬂict core.
6
Experiments
We implemented our approach in the MCSAT solver within Yices 2 [11]. To
evaluate its eﬀectiveness, and the impact of the diﬀerent modules, we ran the
MCSAT solver with diﬀerent settings on the 41,547 QF BV benchmarks available
5 e1 ⋖e2 is accepted since it either constitutes the interpolant or it can be ignored.
6 For this reason we normalize evaluable subterms of, e.g., t1 and t2.

Solving Bitvectors with MCSAT: Explanations from Bits and Pieces
117
in the SMT-LIB library [1]. We used a three-minute timeout per instance. Each
curve in Fig. 4 shows the number of solved instances for each solver variant; all:
the procedures of Sects. 3 and 4, with the bitblasting baseline when these do
not apply; bb: only the bitblasting baseline; bb+eq: procedure of Sect. 3 plus the
baseline; bb+arith: procedure of Sect. 4 plus the baseline; all-prop is the same as
all but with no propagation of bitvector assignments during search. For reference,
we also included the version of the Yices 2 MCSAT solver that entered the 2019
SMT competition7, marked as smtcomp2019.
The solver combining all explainer modules solved 33,236 benchmarks before
timeout, 14,174 of which are solved by pure simpliﬁcation, and 19,062 of which
actually rely on MCSAT explanations. 14,313 of those are solved without ever
calling the default bitblasting baseline (only the dedicated explainers of Sects. 3
and 4 are used), while the other 4,749 instances are solved by a combination of
the three explainers.
The results show that both equality and arithmetic explainers contribute to
the eﬀectiveness of the overall solver, individually and combined. A bit more than
half of the problem instances involving MCSAT explanations are fully within the
scope of the two dedicated explainers. Of course these explainers are still useful
beyond that half, in combination with the bitblasting explainer. The results also
show that the eager MCSAT value propagation mechanism introduced in [19] is
important for eﬀective solving in practice.
For comparison, we also ran two solvers CDCL(T ) solvers based on bit-
blasting on the same benchmarks and with the same timeout. We picked
Yices 2 [11] (version 2.6.1) and Boolector [27] (version 3.2.0) and we used the
same backend SAT solver for both, namely CaDiCaL [6]. Yices 2 solved 40,962
instances and Boolector solved 40,763 instances. We found 794 instances in the
SMTLib benchmarks where our MCSAT solver was faster than Boolector by
more than 2 sec. The pspace/ndist* and pspace/shift1add* instances are
trivial for MCSAT (solved in less than 0.25 sec. each), while Boolector hit our
3-minute timeout on all ndist.a.* instances and all but 3 shift1add* ones.
The brummayerbiere4 instances are trivial for MCSAT (solved in less than 0.03
sec.) while Boolector ran out of memory in our experimentation (except for one
instance). Instances with a signiﬁcant runtime diﬀerence in favour of MCSAT are
among spear/openldap_v2.3.35/* and brummayerbiere/bitrev* (MCSAT is
systematically better), float/mult* (MCSAT is almost systematically better),
float/div*, asp/SchurNumbers/*, 20190311-bv-term-small-rw-Noetzli/*,
and Sage2/*. MCSAT is almost systematically faster on uclid/catchconv/*
and faster on more than half of spear/samba_v3.0.24/*.
Using an alternative MCSAT approach to bitvector solving, Zelji´c et
al. reported that their solver could solve 23704 benchmarks from a larger set
of 49971 instances with a larger timeout of 1200 s [30].8 We have not managed
7 https://smt-comp.github.io/2019/.
8 The additional 8424 benchmarks have since been deleted from the SMT-LIB library
as duplicates.

118
S. Graham-Lengrand et al.
to reproduce the results of Zelji´c’s solver on our Linux server for direct compar-
ison.
To debug the implementation of our explainers, every conﬂict explanation
that is produced when solving in debug mode is sent on-the-ﬂy to (non-MCSAT)
Yices 2, which checks the validity of the clause by bitblasting. In debug mode,
every normalization we perform with the rules of Sect. 5 is also sent to Yices 2
to prove the equality between the original term and the normalized term. Per-
formance benchmarking was only done after completing, without any red ﬂag, a
full run of MCSAT in debug mode on the 41,547 QF BV benchmarks instances.
7
Discussion and Future Work
The paper presents ongoing work on building an MCSAT solver for the theory of
bitvectors. We have presented two main ideas for the treatment of BV in MCSAT,
that go beyond the approach proposed by Zelji´c et al. [30].
First, by relying on BDDs for representing feasible sets, our design keeps the
main search mechanism of MCSAT generic and leaves fragment-speciﬁc mecha-
nisms to conﬂict explanation. The explanation mechanism is selected based on
the constraints involved in the conﬂict. BDDs are also used to minimize the con-
ﬂicts, which increases the chances that a dedicated explanation mechanism can
be applied. BDDs oﬀer a propagation mechanism that diﬀers from those in [30]
in that the justiﬁcation for a propagated assignment is computed lazily, only
when it is needed in conﬂict analysis. Computing the conﬂict core at that point
eﬀectively recovers justiﬁcation of the propagations.
Second, we propose explanation mechanisms for two fragments of the theory:
the core fragment of BV that includes equality, concatenation and extraction;
and a fragment of linear arithmetic. Compared to previous work on coarsest-base
slicing, such as [4], our work applies the slicing on the conﬂict constraints only,
rather than the whole problem. This should in general make the slices coarser,
which we expect to positively impact eﬃciency. Our work on explaining arith-
metic constraints is novel, notwithstanding the mechanisms studied by Janota
and Wintersteiger [17] that partly inspired our Table 1 but addressed a smaller
fragment of arithmetic outside of the context of MCSAT.
We have implemented the overall approach in the Yices 2 SMT solver. Exper-
iments show that the overall approach is eﬀective on practical benchmarks, with
all the proposed modules adding to the solver performance. MCSAT is not yet
competitive with bitblasting, but we are making progress. The main challenge
is devising eﬃcient word-level explanation mechanisms that can handle all or a
least a large fragment of BV. Finding high-level interpolants in BV is still an
open problem and our work on MCSAT shows progress for some fragments of
the bitvector theory. For MCSAT to truly compete with bitblasting, we will need
interpolation methods that cover larger classes of constraints.
A key step in that direction is to extend the bitvector arithmetic explainer
so that it handles multiplications by constants, then multiplication by evaluable
terms, and, ﬁnally, arbitrary multiplications. Deeper integration of fragment-
speciﬁc explainers could potentially help explaining hybrid conﬂicts that involve

Solving Bitvectors with MCSAT: Explanations from Bits and Pieces
119
constraints from diﬀerent fragments. To complement the explainers that we are
developing, we plan to further explore the connection between interpolant gen-
eration and the closely related domain of quantiﬁer elimination, particularly
those techniques by John and Chakraborty [18] for the bitvector theory. The
techniques by Niemetz et al. [26] for solving quantiﬁed bitvector problems using
invertibility conditions could also be useful for interpolant generation in MCSAT.
Future work also includes relating our approach to the report by Chihani,
Bobot, and Bardin [7], which aims at lifting the CDCL mechanisms to the word
level of bitvector reasoning, and therefore seems very close to MCSAT. Finally,
we plan to explore integrating our MCSAT treatment of bitvectors with other
components of SMT-solvers, whether in the context of MCSAT or in diﬀerent
architectures. An approach for this is the recent framework of Conﬂict-Driven
Satisﬁability (CDSAT) [2,3], which precisely aims at organizing collaboration
between generic theory modules.
Acknowledgments. The authors thank Aleksandar Zelji´c for fruitful discussions.
This material is based upon work supported in part by NSF grants 1528153 and
1816936, and by the Defense Advanced Research Project Agency (DARPA) and Space
and Naval Warfare Systems Center, Paciﬁc (SSC Paciﬁc) under Contract No. N66001-
18-C-4011. Any opinions, ﬁndings and conclusions or recommendations expressed in
this material are those of the author(s) and do not necessarily reﬂect the views of NSF,
DARPA, or SSC Paciﬁc.
References
1. Barrett, C., Stump, A., Tinelli, C.: The Satisﬁability Modulo Theories Library
(SMT-LIB) (2010). www.SMT-LIB.org
2. Bonacina, M.P., Graham-Lengrand, S., Shankar, N.: Conﬂict-driven satisﬁability
for theory combination: transition system and completeness. J. Autom. Reasoning
64(3), 579–609 (2019). https://doi.org/10.1007/s10817-018-09510-y
3. Bonacina, M.P., Graham-Lengrand, S., Shankar, N.: Satisﬁability modulo theories
and assignments. In: de Moura, L. (ed.) CADE 2017. LNCS (LNAI), vol. 10395,
pp. 42–59. Springer, Cham (2017). https://doi.org/10.1007/978-3-319-63046-5 4
4. Bruttomesso, R., Sharygina, N.: A scalable decision procedure for ﬁxed-width bit-
vectors. In: Proceedings of the 2009 International Conference on Computer-Aided
Design (ICCAD 2009), ICCAD 2009, pp. 13–20. ACM Press (2009). https://doi.
org/10.1145/1687399.1687403
5. Bryant, R.E.: Graph-based algorithms for boolean function manipulation. Comput.
IEEE Trans. 100(8), 677–691 (1986)
6. CaDiCaL Simpliﬁed Satisﬁability Solver. http://fmv.jku.at/cadical/
7. Chihani, Z., Bobot, F., Bardin, S.: CDCL-inspired Word-level Learning for
Bit-vector Constraint Solving, June 2017. https://hal.archives-ouvertes.fr/hal-
01531336, preprint
8. CUDD: the CU Decision Diagram package. https://github.com/ivmai/cudd
9. Cyrluk, D., M¨oller, O., Rueß, H.: An eﬃcient decision procedure for the theory
of ﬁxed-sized bit-vectors. In: Grumberg, O. (ed.) CAV 1997. LNCS, vol. 1254, pp.
60–71. Springer, Heidelberg (1997). https://doi.org/10.1007/3-540-63166-6 9

120
S. Graham-Lengrand et al.
10. Detlefs, D., Nelson, G., Saxe, J.B.: Simplify: a theorem prover for program check-
ing. J. ACM (JACM) 52(3), 365–473 (2005)
11. Dutertre, B.: Yices 2.2. In: Biere, A., Bloem, R. (eds.) CAV 2014. LNCS, vol. 8559,
pp. 737–744. Springer, Cham (2014). https://doi.org/10.1007/978-3-319-08867-
9 49
12. Damm, W., Hermanns, H. (eds.): CAV 2007. LNCS, vol. 4590. Springer, Heidelberg
(2007). https://doi.org/10.1007/978-3-540-73368-3
13. Graham-Lengrand, S., Jovanovi´c, D.: An MCSAT treatment of bit-vectors. In:
Brain, M., Hadarean, L. (eds.) 15th International Workshop on Satisﬁability Mod-
ulo Theories (SMT 2017), July 2017
14. Graham-Lengrand, S., Jovanovi´c, D.: Interpolating bit-vector arithmetic con-
straints in MCSAT. In: Sharygina, N., Hendrix, J. (eds.) 17th International Work-
shop on Satisﬁability Modulo Theories (SMT 2019), July 2019
15. Graham-Lengrand, S., Jovanovi´c, D., Dutertre, B.: Solving bitvectors with
MCSAT: explanations from bits and pieces (long version). Technical report, SRI
International (Apr 2020), https://arxiv.org/abs/2004.07940
16. Biere, A., Bloem, R. (eds.): CAV 2014. LNCS, vol. 8559. Springer, Cham (2014).
https://doi.org/10.1007/978-3-319-08867-9
17. Janota, M., Wintersteiger, C.M.: On intervals and bounds in bit-vector arithmetic.
In: King, T., Piskac, R. (eds.) Proceedings of the 14th International Workshop on
Satisﬁability Modulo Theories (SMT 2016). CEUR Workshop Proceedings, vol.
1617, pp. 81–84. CEUR-WS.org, July 2016. http://ceur-ws.org/Vol-1617/paper8.
pdf
18. John, A.K., Chakraborty, S.: A layered algorithm for quantiﬁer elimination from
linear modular constraints. Formal Methods Syst. Des. 49(3), 272–323 (2016).
https://doi.org/10.1007/s10703-016-0260-9
19. Jovanovi´c, D.: Solving nonlinear integer arithmetic with MCSAT. In: Bouajjani,
A., Monniaux, D. (eds.) VMCAI 2017. LNCS, vol. 10145, pp. 330–346. Springer,
Cham (2017). https://doi.org/10.1007/978-3-319-52234-0 18
20. Jovanovi´c, D., Barrett, C., de Moura, L.: The design and implementation of the
model constructing satisﬁability calculus. In: Proceedings of the 13th Interna-
tional Conference on Formal Methods In Computer-Aided Design (FMCAD 2013).
FMCAD Inc. October 2013
21. Junker, U.: Quickxplain: conﬂict detection for arbitrary constraint propagation
algorithms. In: IJCAI 2001 Workshop on Modelling and Solving Problems with
Constraints (2001)
22. Kroening, D., Strichman, O.: Decision Procedures - An Algorithmic Point of
View, Second Edition. Texts in Theoretical Computer Science. An EATCS Series.
Springer (2016). https://doi.org/10.1007/978-3-662-50497-0
23. Marques Silva, J., Lynce, I., Malik, S.: Conﬂict-driven clause learning SAT solvers.
In: Biere, A., Heule, M., Maaren, H.V., Walsh, T. (eds.) Handbook of Satisﬁability,
Frontiers in Artiﬁcial Intelligence and Applications, vol. 185, pp. 131–153. IOS
Press (2009)
24. de Moura, L., Jovanovi´c, D.: A model-constructing satisﬁability calculus. In: Gia-
cobazzi, R., Berdine, J., Mastroeni, I. (eds.) VMCAI 2013. LNCS, vol. 7737, pp.
1–12. Springer, Heidelberg (2013). https://doi.org/10.1007/978-3-642-35873-9 1
25. Niemetz, A., Preiner, M., Biere, A.: Boolector 2.0. J. Satisf. Boolean Model. Com-
put. 9(1), 53–58 (2014)

Solving Bitvectors with MCSAT: Explanations from Bits and Pieces
121
26. Niemetz, A., Preiner, M., Reynolds, A., Barrett, C., Tinelli, C.: Solving quantiﬁed
bit-vectors using invertibility conditions. In: Chockler, H., Weissenbacher, G. (eds.)
CAV 2018. LNCS, vol. 10982, pp. 236–255. Springer, Cham (2018). https://doi.
org/10.1007/978-3-319-96142-2 16
27. Niemetz, A., Preiner, M., Wolf, C., Biere, A.: Btor2, BtorMC and Boolector 3.0.
In: Chockler, H., Weissenbacher, G. (eds.) CAV 2018. LNCS, vol. 10981, pp. 587–
595. Springer, Cham (2018). https://doi.org/10.1007/978-3-319-96145-3 32
28. Nieuwenhuis, R., Oliveras, A., Tinelli, C.: Solving SAT and SAT modulo theories:
from an abstract Davis-Putnam-Logemann-Loveland procedure to DPLL (T). J.
ACM Press 53(6), 937–977 (2006). https://doi.org/10.1145/1217856.1217859
29. Robinson, J.A., Voronkov, A. (eds.): Handbook of Automated Reasoning (in 2
volumes). Elsevier and The MIT Press, Cambridge (2001)
30. Zelji´c, A., Wintersteiger, C.M., R¨ummer, P.: Deciding bit-vector formulas with
mcSAT. In: Creignou, N., Le Berre, D. (eds.) SAT 2016. LNCS, vol. 9710, pp.
249–266. Springer, Cham (2016). https://doi.org/10.1007/978-3-319-40970-2 16

Monadic Decomposition in Integer Linear
Arithmetic
Matthew Hague1(B)
, Anthony W. Lin2
, Philipp R¨ummer3
,
and Zhilin Wu4
1 Royal Holloway, University of London, Egham, UK
matthew.hague@rhul.ac.uk
2 TU Kaiserslautern, Kaiserslautern, Germany
3 Uppsala University, Uppsala, Sweden
4 State Key Laboratory of Computer Science, Institute of Software,
Chinese Academy of Sciences, Beijing, China
Abstract. Monadic decomposability is a notion of variable indepen-
dence, which asks whether a given formula in a ﬁrst-order theory is
expressible as a Boolean combination of monadic predicates in the theory.
Recently, Veanes et al. showed the usefulness of monadic decomposabil-
ity in the context of SMT (i.e. the input formula is quantiﬁer-free), and
found various interesting applications including string analysis. However,
checking monadic decomposability is undecidable in general. Decidabil-
ity for certain theories is known (e.g. Presburger Arithmetic, Tarski’s
Real-Closed Field), but there are very few results regarding their com-
putational complexity. In this paper, we study monadic decomposability
of integer linear arithmetic in the setting of SMT. We show that this deci-
sion problem is coNP-complete and, when monadically decomposable, a
formula admits a decomposition of exponential size in the worst case. We
provide a new application of our results to string constraint solving with
length constraints. We then extend our results to variadic decomposabil-
ity, where predicates could admit multiple free variables (in contrast to
monadic decomposability). Finally, we give an application to quantiﬁer
elimination in integer linear arithmetic where the variables in a block
of quantiﬁers, if independent, could be eliminated with an exponential
(instead of the standard doubly exponential) blow-up.
1
Introduction
A formula φ(¯x) in some theory L is monadically decomposable if it is L-equivalent
to a Boolean combination of monadic predicates in L, i.e., to a monadic decom-
position of φ. Monadic decomposability measures how tightly the free vari-
ables in φ are coupled. For example, x = y is not monadically decomposable
in any (ﬁnitary) logic over an inﬁnite domain, but x + y ≥2 can be decom-
posed, in Presburger arithmetic over natural numbers, since it can be written as
x ≥2 ∨(x ≥1 ∧y ≥1) ∨y ≥2.
Veanes et al. [24] initiated the study of monadic decomposability in the
setting of Satisﬁability Modulo Theories, wherein formulas are required to be
c
⃝Springer Nature Switzerland AG 2020
N. Peltier and V. Sofronie-Stokkermans (Eds.): IJCAR 2020, LNAI 12166, pp. 122–140, 2020.
https://doi.org/10.1007/978-3-030-51074-9_8

Monadic Decomposition in Integer Linear Arithmetic
123
quantiﬁer-free. Monadic decomposability has many applications, including sym-
bolic transducers [11] and string analysis [24]. Although the problem was shown
to be in general undecidable, a generic semi-algorithm for outputting monadic
decompositions (if decomposable) was provided. A termination check could in
fact be added if the input formula belongs to a theory for which monadic decom-
posability is decidable, e.g., linear arithmetic, Tarski’s Real-Closed Field, and the
theory of uninterpreted functions. Hitherto, not much is known about the com-
putational complexity of monadic decomposability problems for many ﬁrst-order
theories (in particular, quantiﬁer-free theories), and about practical algorithms.
This was an open problem raised by Veanes et al. in [24].
Monadic decomposability is intimately connected to the variable partition
problem, ﬁrst studied by Libkin [19] nearly 20 years ago. In particular, a monadic
decomposition gives rise to a partition of the free variables ¯x of a formula φ(¯x),
wherein each part consists of a single variable. More precisely, take a partition
Π = {Y1, . . . , Ym} of ¯x into sets Yi of variables, with linearizations yi. The
formula φ(¯x) is Π-decomposable (in some theory L) if it is L-equivalent to a
boolean combination of formulas of the form Δ(yi). As suggested in [19], such
variadic decompositions of φ(¯x) have potential applications in optimization of
database query processing and quantiﬁer elimination. The author gave a general
condition for the decidability of variable independence in ﬁrst-order theories.
This result is unfortunately not easily applicable in the SMT setting for at least
two reasons: (i) the full ﬁrst-order theory might be undecidable (e.g. theory
of uninterpreted functions), and (ii) even for a ﬁrst-order theory that admits
decidable monadic decompositions, the complexity of the algorithm obtained
from [19] could be too prohibitive for the quantiﬁer-free fragment. One example
that epitomizes (ii) is the problem of determining whether a given relation R ⊆
(Σ∗)k over strings represented by a regular transducer could be expressed as
a boolean combination of monadic predicates. The result of [19] would give a
double exponential-time algorithm for monadic decomposability, whereas it was
recently shown in [5] to be solvable in polynomial-time (resp. polynomial-space)
when the transducer is given as a deterministic (resp. nondeterministic) machine.
Contributions. First, we determine the complexity of deciding monadic decom-
posability and outputting monadic decompositions (if they exist) for the theory
of integer linear arithmetic in the setting of SMT. Our result is summarized in
Theorem 1.
Theorem 1 (Monadic Decomposability).
Given a quantifer-free formula
φ of Presburger Arithmetic, it is coNP-complete to decide if φ is monadically
decomposable. This is eﬃciently reducible to unsatisﬁability of quantiﬁer-free
Presburger formulas. Moreover, if a decomposition exists, it can be constructed
in exponential time.
We show a new application of monadic decomposability in integer linear arith-
metic for SMT over strings, which is currently a very active research area, e.g., see
[1–3,6,9,12,16,18,20,22,23]. One problem that makes string constraint solving

124
M. Hague et al.
diﬃcult is the presence of additional length constraints, which forces the lengths
of the strings in the solutions to satisfy certain linear arithmetic constraints.
Whereas satisﬁability of string equations with regular constraints is PSPACE-
complete (e.g. see [13,17]), it is a long-standing open problem [7,14] whether
word equations with length constraints are decidable. Length constraints are
omnipresent in Kaluza [22], arguably the ﬁrst serious string constraint bench-
marks obtained from real-world JavaScript applications. Using our monadic
decomposability solver, we show that 90% of the Kaluza benchmarks are in fact
in a decidable fragment of string constraints, since occurring length constraints
can be completely removed by means of decomposition.
Next we extend our result to variadic decomposability (cf. [19]).
Theorem 2 (Variadic Decomposability). It is coNP-complete to decide if φ
is Π-decomposable, given a quantifer-free formula φ(¯x) of Presburger Arithmetic
and a partition Π = {Y1, . . . , Yn} of ¯x. This is eﬃciently reducible to unsatisﬁa-
bility of quantiﬁer-free Presburger formulas. Moreover, if a decomposition exists,
it can be constructed in exponential time.
We show how this could be applied to quantiﬁer elimination. In particular, we
show that if a formula φ(¯y) = ∃¯x. ψ(¯x, ¯y), where ψ is quantiﬁer-free, is {X, Y }-
decomposable—where ¯x and ¯y are linearizations of the variables in X and Y —
then we can compute in exponential time a formula θ(¯y) such that ⟨N, +⟩|= θ ↔
φ, i.e., avoiding the standard double-exponential blow-up (cf. [25]).
Organization. Preliminaries are in Sect. 2. Results on monadic (resp. variadic)
decomposition are in Sect. 3 (resp. Sect. 4) and applications appear in Sect. 5.
2
Preliminaries
2.1
Presburger Syntax
In this paper we study the problem of monadic decomposition for formulas in lin-
ear integer arithmetic. All of our results are presented for Presburger arithmetic
over natural numbers, but they can be adapted easily to all integers.
Deﬁnition 1 (Fragments of Presburger Arithmetic). A formula φ of
Presburger arithmetic is a formula of the form Q1x1 · · · Qnxn. ψ where Qi ∈
{∀, ∃} and ψ is a quantiﬁer-free Presburger formula:
ψ :=

i
aixi ∼b | ax ≡k by | x ≡k c | φ1 ∧φ2 | ¬φ
where ai, a, b ∈Z, k, c ∈N with 0 ≤c < k, variables xi, x, y range over N, and
∼∈{≤, ≥}. The operator ≡k denotes equality modulo k, i.e., s ≡k t whenever
s −t is a multiple of k. Formulas of the shape 
i aixi ∼b, ax ≡k by, or x ≡k c
are called atoms.
Existential Presburger formulas are formulas of the form ∃x1, . . . , xn. ψ for
some quantiﬁer-free Presburger formula ψ. We let QF(N) (resp. ∃∗(N)) denote
the set of all quantiﬁer-free (resp., existential) Presburger formulas.

Monadic Decomposition in Integer Linear Arithmetic
125
Let x = (x1, . . . , xn) be a tuple of integer variables. We write f(x) = 
i aixi
for a linear sum over x. Let y = (y1, . . . , ym). By slight abuse of nota-
tion, we may also write φ(x, y) to denote a QF(N) formula over the variables
x1, . . . , xn, y1, . . . , ym.
2.2
Monadic Decomposability
A quantiﬁer-free formula φ is called monadic if every atom in φ contains at most
one variable, and it is called monadically decomposable if φ is equivalent to a
monadic formula φ′. In this case, φ′ is also called a decomposition of φ. For our
main results we use a slightly reﬁned notion of a formula being decomposable:
Deﬁnition 2 (Monadically Decomposable on x). Fix a logic L (e.g. QF(N)
or ∃∗(N)). We say a formula φ(x1, . . . , xn) in L is monadically decomposable
on xi whenever
φ(x1, . . . , xn) ≡

j
Δj(xi) ∧ψj(x1, . . . , xi−1, xi+1, . . . , xn)
for some formulas Δj and ψj in L.
It can be observed that a formula is monadically decomposable if and only
if it is monadically decomposable on all variables occurring in the formula (cf.
Lemma 1). We expand on this for the variadic case below.
We recall the following characterization of monadic decomposability for for-
mulas φ(x, y) with two free variables (cf. [5,8,19,24]), which holds regardless of
the theory under consideration. This can be extended easily to formulas with k
variables, but is not needed in this paper. Given a formula φ(x, y), deﬁne the
formula ∼as follows:
x ∼x′ := ∀y, y′. (φ(x, y) ∧φ(x′, y′) →(φ(x′, y) ∧φ(x, y′)))
Proposition 1. The relation ∼is an equivalence relation. Furthermore, φ(x, y)
is monadically decomposable iﬀ∼has a ﬁnite index (i.e. the number of
∼-equivalence classes is ﬁnite).
Using this proposition, it is easy to show that over a structure with an inﬁnite
domain (e.g. integer linear arithmetic) the formula x = y is not monadically
decomposable. As was noted already in [19], to check monadic decomposability
of a formula φ in Presburger Arithmetic in general, we may simply check if there
is an upper bound B on the smallest representation of every ∼-equivalence class,
i.e.,
∃B.∀x.∃xs. (xs ≤B ∧xs ∼x).
However, to derive tight complexity bounds for checking monadic decomposabil-
ity, this approach is problematic, since the above characterisation has multiple
quantiﬁer alternations. Using known results (e.g. [15]), one would only obtain an
upper bound in the weak exponential hierarchy [15], which only admits double-
exponential time algorithms.

126
M. Hague et al.
2.3
Variadic Decomposability
The notion of a variadic decomposition generalises monadic decomposition by
considering partitions of the occurring variables.
Deﬁnition 3 (Π-Decomposable). Fix a logic L (e.g. QF(N) or ∃∗(N)). Take
a formula φ(x1, . . . , xn) in L and a partition Π = {Y1, . . . , Ym} of x1, . . . , xn.
We say φ is Π-decomposable whenever
φ(x1, . . . , xn) ≡

i
Δ1
i (y1) ∧· · · ∧Δm
i (ym)
for some formulas Δj
i in L and linearizations yj of Yj.
Observe that a formula φ(x1, . . . , xn) is monadically decomposable on xi iﬀ
it is Π-decomposable with Π = {{xi}, {x1, . . . , xi−1, xi+1, . . . , xn}}. Moreover,
we say a formula φ over the set of variables X is variadic decomposable on Y
whenever it is Π-decomposable with Π = {Y, X \ Y }.
General Π-decompositions can be computed by decomposing on binary par-
titions {Y, X \ Y }, which is why we focus on this binary case in the rest of the
paper. We argue why this is the case below.
Let a formula φ and Π = {Y1, . . . , Ym} be given. We can ﬁrst decompose
separately on each {Yi, Y } where Y = Y1 ∪· · · ∪Yi−1 ∪Yi+1 ∪· · · ∪Ym. Using
the algorithm in Sect. 4 we obtain for each i a decomposition of a speciﬁc form:

j
Δi
j(yi) ∧φ

y1, . . . , yi−1, ci
j, yi+1, . . . , ym

.
Note, these decompositions can be performed independently using the algorithm
in Sect. 4 and the second conjunct of each disjunct is φ with yi replaced by
ﬁxed constants cj. Additionally, each Δi
j is polynomial in size and each ci
j can
be represented with polynomially many bits. We note also that our algorithm
ensures that each Δi
j is satisﬁable.
Given such decompositions, we can recursively decompose φ on Π. We ﬁrst
use the above decomposition for i = 1 and obtain

j
Δ1
j(y1) ∧φ

c1
j, y2, . . . , ym

.
Next, we use the decomposition for i = 2 to decompose the copies of φ in the
decomposition above. We obtain

j1
⎛
⎝Δ1
j1(y1) ∧

j2
Δ2
j2(y2) ∧φ

c1
j1, c2
j2, y3, . . . , ym

⎞
⎠.
This process repeats until all Yi have been considered. If φ is Π-
decomposable, we ﬁnd a decomposition. If φ is not Π-decomposable, then it

Monadic Decomposition in Integer Linear Arithmetic
127
would not be possible to do the independent decompositions for each i. Thus,
for Π = {Y1, . . . , Ym}, we can use variadic decompositions on Yi to compute
Π-decompositions.
The above algorithm runs in exponential time due both to the exponential
size of the decompositions and the branching caused by the disjuncts. If we are
only interested in whether a formula is Π-decomposable, it is enough to ask
whether it is decomposable on Yi for each i. In particular, a formula φ(¯x) is
monadically decomposable iﬀφ is decomposable for each variable y ∈¯x. Since
the complexity class coNP is closed under intersection, we obtain the following:
Lemma 1. A coNP upper bound for monadic decomposability on a given vari-
able y implies a coNP upper bound for monadic decomposability. Likewise, a
coNP upper bound for variadic decomposability on a given subset Y of variables
implies a coNP upper bound for Π-decomposability.
2.4
Example
Consider the formula φ(x, y, z) given by z = x + 2y ∧z < 5. This for-
mula is monadically decomposable, which means, it is Π-decomposable for
Π = {{x}, {y}, {z}}.
Our algorithm will ﬁrst take a decomposition on x and might obtain
4
i=0 Δ1
i (x)∧φ(i, y, z) where Δ1
i (x) = (x = i) and φ(i, y, z) = (z = i+2y)∧z < 5.
Next, we use a decomposition on y. For each φ(i, y, z) we substitute 2−⌈i
2 ⌉
j=0
y =
j ∧φ(i, j, z), and as the ﬁnal decomposition we get
4
i=0
2−⌈i
2 ⌉

j=0
x = i ∧y = j ∧z = i + 2j.
3
Monadic Decomposability
3.1
Lower Bounds
We ﬁrst show that unsatisﬁability of Boolean formulas can be reduced to monadic
decomposability of formulas with only two variables, directly implying coNP-
hardness:
Lemma 2 (coNP-Hardness). Deciding whether a formula φ(x, y) in QF(N)
is monadic decomposable is coNP-hard.
Proof. We reduce from unsatisﬁability of propositional formulas to monadic
decomposability of φ(x, y). Take a propositional formula S(x1, . . . , xn). Let
p1, . . . , pn be the ﬁrst n primes. Let ψ(x) be the formula obtained from S
by replacing each occurrence of xi by x ≡pi 0. Given an assignment ν :
{x1, . . . , xn} →{0, 1}, we let
Hν = {m ∈N | ∀1 ≤i ≤n. (m ≡pi 0 ↔ν(xi) = 1)}.

128
M. Hague et al.
Thanks to the Chinese Remainder Theorem, Hν is non-empty and periodic with
period p = n
i=1 pi, which implies that Hν is inﬁnite for every ν. We also have
that ν |= S iﬀ, for each n ∈Hν, ψ(n) is true.
Now deﬁne φ(x, y) = (ψ(x) ∧x = y). If S is unsatisﬁable, then ψ is unsat-
isﬁable and so it is decomposable. Conversely, if S can be satisﬁed by some
assignment ν, then φ(m, m) is true for all (inﬁnitely many) m ∈Hν. Since all
solutions to φ(x, y) imply that x = y, by Proposition 1 we have that φ is not
monadically decomposable.
⊓⊔
We next provide exponential lower bounds for decompositions in either dis-
junctive normal form (DNF) or conjunctive normal form (CNF). DNF has been
frequently used to represent monadic decompositions by previous papers (e.g.
[5,8,19]), and it is most suitable for applications in quantiﬁer elimination.
Lemma 3 (Size of Decomposition). There exists a family {φn(x, y)}n∈N of
formulas in QF(N) such that φn grows linearly in n, while the smallest decom-
position on x in DNF/CNF is exponential in n.
Proof. Consider the formulas φn(x, y) = (x + y ≤2n). Using a binary encoding
of constants, the size of the formulas is linear in n. We show that decompositions
in DNF/CNF must be exponential in size.
Disjunctive: Suppose ψn(x, y) = 
i ψx
i (x)∧ψy
i (y) is a monadic decomposition
in DNF. Each disjunct ψx
i (x)∧ψy
i (y), if it is satisﬁable at all, has an upper right
corner (xi, yi) such that ψx
i (xi)∧ψy
i (yi) holds, but ψx
i (x)∧ψy
i (y) ⇒x ≤xi ∧y ≤
yi. This immediately implies that exponentially many disjuncts are needed to
cover the exponentially many points on the line x + y = 2n.
Conjunctive: Suppose ψn(x, y) is a succinct monadic decomposition of φn in
CNF. Since ¬ψn(x, y) ≡2n+1 ≤x+y ≡(2n−x+1)+(2n−y) ≤2n, it follows that
¬ψn(2n−x+1, 2n−y) ≡(2n−(2n−x+1)+1)+(2n−(2n−y)) ≤2n ≡x+y ≤2n.
Therefore, ¬ψn(2n −x + 1, 2n −y) is a succinct decomposition of φn in DNF,
contradicting the lower bound for DNFs.
⊓⊔
3.2
Upper Bound
We prove Theorem 1. Following Lemma 1, it suﬃces to show that testing decom-
posability on a variable x is in coNP and that a decomposition can be computed
in exponential time. Assume without loss of generality that we have φ(x, y) where
y = (y1, . . . , yn), and that we are decomposing on the ﬁrst variable x.
We claim that φ is monadically decomposable on x iﬀ
∀x1, x2 ≥B.∀y. SameDiv(x1, x2, y) ⇒(φ(x1, y) ⇐⇒φ(x2, y))
where B is a bound exponential in the size of φ and SameDiv is a formula
asserting that x1 and x2 satisfy the same divisibility constraints. This bound is
computable in polynomial time and is described in Sect. 3.4. To deﬁne SameDiv,
let Divs be the set of all divisibility constraints az1 ≡k bz2 or z1 ≡k c appearing
(syntactically) in φ. Assume without loss of generality that x always appears on

Monadic Decomposition in Integer Linear Arithmetic
129
the left-hand side of a divisibility constraint (i.e., in the z1 position of az1 ≡k
bz2). We then deﬁne
SameDiv(x1, x2, y) =
⎛
⎜
⎜
⎝

ax≡kbz∈Divs
(ax1 ≡k bz) ⇐⇒(ax2 ≡k bz)
∧

x≡kc∈Divs
(x1 ≡k c) ⇐⇒(x2 ≡k c)
⎞
⎟
⎟
⎠.
We prove the claim in the following sections and simultaneously show how to
construct the decomposition. Once we have established the above, we can test
non-decomposability on x by checking
∃x1, x2 ≥B. ∃y. SameDiv(x1, x2, y) ∧φ(x1, y) ∧¬φ(x2, y)
which is decidable in NP. Thus we obtain a coNP decision procedure because
the above formula is polynomial in the size of φ.
Example. We consider some examples. First consider the formula x = y that
cannot be decomposed on x. Since there are no divisibility constraints, SameDiv
is simply true. It is straightforward to see that, ∀B.∃x1, x2 ≥B.∃y. true ∧x1 =
y ∧x2 ̸= y, for example by setting x1 = B, x2 = B + 1, and y = B.
Now consider the monadically decomposable formula
φ(x, y, z) = x + 2y ≥5 ∧z < 5 ∧x ≡2 y.
In this case SameDiv(x1, x2, y, z) = (x1 ≡2 y ⇐⇒x2 ≡2 y). We can verify
∀x1, x2 ≥B.∀y, z. SameDiv(x1, x2, y, z) ⇒(φ(x1, y, z) ⇐⇒φ(x2, y, z))
holds, as it will be the case that 5 < B and for all x > 5 the formula φ will hold
whenever x ≡2 y holds and z < 5. The precondition SameDiv ensures that the
if and only if holds. We will construct the decomposition in the next section.
Expanded Divisibility Constraints. Observe that divisibility constraints
are always decomposable. In particular, az1 ≡k bz2 is equivalent to a ﬁnite
disjunction of clauses z1 ≡k′ c ∧z2 ≡k′ c where k′ and c are bounded by a
multiple of a, b and k. The expansion is exponential in size, since the values
up to k′ have to be enumerated explicitly.
We deﬁne XDivs be the set of all constraints of the form x ≡k c′ where
0 ≤c′ < k and x ≡k c appears directly in φ or in the expansion of the divisibility
constraints of φ. This set will be used in the next sections.
3.3
Soundness
We show that if
∀x1, x2 ≥B.∀y. SameDiv(x1, x2, y) ⇒(φ(x1, y) ⇐⇒φ(x2, y))

130
M. Hague et al.
then φ is decomposable on x. We do this by constructing the decomposition.
Although there are doubly exponentially many subsets D ⊆XDivs, there
are only exponentially many maximal consistent subsets. We implicitly restrict
D to such subsets. This is because, for any k, there is no value of x such that
x ≡k c and x ≡k c′ both hold with c ̸= c′ but c, c′ ∈{0, . . . , k −1}. For any
maximal consistent set D ⊆XDivs, let cD be the smallest integer greater than
or equal to B satisfying all constraints in D. Note, since D is maximal, a value
that satisﬁes all constraints in D also does not satisfy an constraints not in D.
The number cD can be represented using polynomially many bits.
We can now decompose φ into
⎛
⎝
(x = 0 ∧φ(0, y))
∨· · · ∨
(x = B −1 ∧φ(B −1, y))
⎞
⎠∨

D⊆XDivs

x ≥B ∧

x≡kc∈D
x ≡k c ∧φ(cD, y)

.
This formula is exponential in the size of φ if D only ranges over the maximal
consistent subsets of XDivs. For values of x less than B, equivalence with the
original formula is immediate. For larger values, we use the fact that, from our
original assumption, for any values x1 and x2 that satisfy the same divisibility
constraints, we have φ(x1, y) iﬀφ(x2, y). Hence, we can substitute the values cD
in these cases.
Example. We return to φ(x, y, z) = x+2y ≥5∧z < 5∧x ≡2 y and compute the
decomposition on x. Assuming B is odd, the decomposition will be as follows.
In our presentation we slightly simplify the formula. Strictly speaking x ≡2 y
should be expanded to (x ≡2 0 ∧y ≡2 0) ∨(x ≡2 1 ∧y ≡2 1). We simplify these
to y ≡2 0 and y ≡2 1, respectively, when instantiated with concrete values of x.
(x = 0 ∧(0 + 2y ≥5 ∧z < 5 ∧y ≡2 0)) ∨
(x = 1 ∧(1 + 2y ≥5 ∧z < 5 ∧y ≡2 1))
∨· · · ∨
(x = B −1 ∧(B −1 + 2y ≥5 ∧z < 5 ∧y ≡2 0)) ∨
((x ≡2 0 ∧x ≥B) ∧(B + 1 + 2y ≥5 ∧z < 5 ∧y ≡2 0)) ∨
((x ≡2 1 ∧x ≥B) ∧(B + 2y ≥5 ∧z < 5 ∧y ≡2 1))
3.4
Completeness
We now show that every formula φ decomposable on x satisﬁes
∀x1, x2 ≥B.∀y. SameDiv(x1, x2, y) ⇒(φ(x1, y) ⇐⇒φ(x2, y)) .
We ﬁrst show that some B must exist. Once the existence has been established,
we can argue that it must be at most exponential in φ.
Existence of the Bound. If φ(x, y) is decomposable on x, then there is an
equivalent formula 
i Δi(x) ∧ψi(y). It is known that every formula Δ(x) is

Monadic Decomposition in Integer Linear Arithmetic
131
satisﬁed by a ﬁnite union of arithmetic progressions a+jb. Let B be larger than
the largest value of a in the arithmetic progressions satisfying the Δi(x).
We show when SameDiv(x1, x2, y) then φ(x1, y) iﬀφ(x2, y) for all values
x1, x2 ≥B and y. Assume towards a contradiction that we have values x1, x2 and
a tuple of values y such that SameDiv(x1, x2, y) and φ(x1, y), but not φ(x2, y).
Let k be the product of all k′ appearing in some divisibility constraint x ≡k′ c
in XDivs. We know that there is some disjunct of the monadic decomposition
such that Δ(x1) ∧ψ(y) holds. Moreover, let x1 belong to the arithmetic pro-
gression a + jb. Since x1 ≥B > a we know that Δ(x′
1) ∧ψ(y) also holds for
any x′
1 = x1 + j′bk. That is, we can pump x1 by adding a multiple of bk, while
staying in the same arithmetic progression and satisfying the same divisibility
constraints.
Similarly, let d be the product of all b appearing in the (ﬁnite number of)
arithmetic progressions that deﬁne the monadic decomposition of φ, limited to
disjuncts such that ψi(y) holds. Since φ(x2, y) does not hold, then φ(x′
2, y) also
does not hold for any x′
2 = x2 + jdk. This means that we can pump x2 staying
outside of the arithmetic progressions deﬁning permissible values of x for the
given values y, whilst additionally satisfying the same divisibility constraints.
Now, for each value of x′
1 satisfying φ(x′
1, y) we can consider the disjunctive
normal form of φ. By expanding the divisibility constraints, a disjunct becomes
a conjunction of terms of the form, where f represents some linear function on y,
1. ax + f(y) ≤c or ax + f(y) ≥c, or
2. yi ≡k′ c or x ≡k′ c.
Since there are inﬁnitely many x′
1, we can choose one disjunct satisﬁed by
inﬁnitely many x′
1. This means that for constraints of the form ax + f(y) ≤c or
ax+f(y) ≥c with a non-zero a, then a must be negative or positive respectively
(or zero). Otherwise, only a ﬁnite number of values of x would be permitted.
We know that x′
2 and y do not satisfy the disjunct. We argue that this is a
contradiction by considering each term in turn. Since there are inﬁnitely many
x′
2 we can assume without loss of generality that x′
2 > x′
1.
1. If ax + f(y) ≤c (resp. ax + f(y) ≥c) appears and is satisﬁed by x′
1, then a
must be negative or zero (resp. positive or zero) and x′
2 will also satisfy the
atom.
2. Atoms of the form yi ≡k′ c do not distinguish values of x and thus are satisﬁed
for both x′
1 and x′
2. We cannot have x′
1 ≡k′ c but not x′
2 ≡k′ c since x′
1 and
x′
2 satisfy the same divisibility constraints.
Thus, it cannot be the case that x′
1 satisﬁes the disjunct, while x′
2 does not.
This is our required contradiction. Hence, for all x1, x2 ≥B and y such that
SameDiv(x1, x2, y) it must be the case that φ(x1, y) iﬀφ(x2, y). We have thus
established the existence of a bound B.

132
M. Hague et al.
Size of the Bound. We now argue that this bound is exponential in the size
of φ, and can thus be encoded in a polynomial number of bits.
Consider the formula that is essentially the negation of our property.
χ(x1, x2, y) = SameDiv(x1, x2) ∧φ(x1, y) ∧¬φ(x2, y).
There is some computable bound B′ exponential in the size of χ (and thus φ)
such that, if there exists x1, x2 ≥B′ and some y such that χ(x1, x2, y) holds,
then there are inﬁnitely many x′
1 and x′
2 such that for some y′ we have that
χ(x′
1, x′
2, y′) holds. An argument for the existence of this bound is given in the
full version In short, we ﬁrst convert the formula above into a disjunction of
conjunctions of linear equalities, using a linear number of slack variables to
encode inequalities and divisibility constraints. Then, using a result of Chistikov
and Haase [10], we set B′ = 2dnm+3 where d is the number of bits needed to
encode the largest constant in the converted formula (polynomially related to
the size of the formula above), n is the maximum number of linear equalities in
any disjunct, and m is the number of variables (including slack variables).
Now, assume that the smallest B is larger than B′. That is
∀x1, x2 ≥B.∀y. SameDiv(x1, x2, y) ⇒(φ(x1, y) ⇐⇒φ(x2, y))
holds, but it does not hold that
∀x1, x2 ≥B′.∀y. SameDiv(x1, x2, y) ⇒(φ(x1, y) ⇐⇒φ(x2, y))
This implies there exists some x1, x2 ≥B′ and y such that χ(x1, x2, y) holds.
Thus, there are inﬁnitely many such x′
1 and x′
2, contradicting the fact that all
x′
1, x′
2 ≥B do not satisfy the property. Thus, we take B′ as the value of B. It
is computable in polynomial time, exponential in size, and representable in a
polynomial number of bits.
4
Variadic Decomposability
We consider decomposition along several variables instead of just one. In this
section, we assume without loss of generality that φ is given in positive normal
form and all (in)equalities rearranged into the form 
i aixi ≥b. We may use
negation ¬φ as a shorthand. We require this form because later we use the
set of all linear equations in the DNF of a formula. Since negation alters the
linear equations, it is more convenient to assume that negation has already been
eliminated.
4.1
Π-Decomposability
As described in Sect. 2.3, we reﬁne the notion of Π-decomposability to separate
only a single set Yi in Π = {Y1, . . . , Yn}. Without loss of generality, we assume
we are given a formula φ(x, y) and we separate the variables in x from y.
In particular, given a formula φ(x, y) we aim to decompose the formula into
φ(x, y) ≡
j
Δj(x) ∧ψj(y) for some QF(N) formulas Δi and ψi.

Monadic Decomposition in Integer Linear Arithmetic
133
4.2
Decomposition
We show that testing whether a given formula φ is variadic decomposable on x
is in coNP. This proves Theorem 2 as the coNP lower bound follows from the
monadic case.
Lemma 4 (Decomposing on x). Given a QF(N) formula φ(x, y) there is a
coNP algorithm to decide if φ is variadic decomposable on x. Moreover, if a
decomposition exists, it can be constructed in exponential-time and is exponential
in size.
Let F be the set of all f such that f(x) + g(y) ≥b is a linear inequality
appearing in φ. Our approach will divide the points of x into regions where all
points within a region can be paired with the same values of y to satisfy the
formula. These regions are given by a bound B. If f(x) is within the bound,
then two points x1 and x2 are in the same region if f(x1) = f(x2). If two points
are outside the bound, then by a pumping argument we can show that we have
φ(x1, y) iﬀφ(x2, y).
Let ˆr = (UB, EQ) be a partition of F into unbounded and bounded functions
(where EQ refers to equality being asserted over bounded functions as shown
below). Deﬁne for each ˆr = (UB, EQ)
Regionˆr(x1, x2) ≜
⎛
⎝
f∈EQ
f(x1) = f(x2)
⎞
⎠.
Note, this formula intentionally does not say anything about the unbounded
functions. This is important when we need to derive a bound—such a derivation
cannot use a pre-existing bound.
We also need to extend SameDiv to account for x1 and x2 being vectors.
This is a straightforward extension asserting that each variable in x1 satisﬁes
the same divisibility constraints as its counterpart in x2. Again, let Divs be the
set of all divisibility constraints az1 ≡k bz2 appearing (syntactically) in φ. Let xi,
x1
i and x2
i denote the ith variable of x, x1, and x2 respectively. Assume without
loss of generality that variables in x always either appear on the left-hand side
of a divisibility constraint (i.e. in the z1 position) or on both sides. Deﬁne
SameDiv(x1, x2, y) =

axi≡kbz∈Divs,
z̸=xj
⎛
⎝

ax1
i ≡k bz

⇐⇒

ax2
i ≡k bz

⎞
⎠∧

axi≡kbxj
∈
Divs
⎛
⎝

ax1
i ≡k bx1
j

⇐⇒

ax2
i ≡k bx2
j

⎞
⎠∧

xi≡kc
∈
Divs
⎛
⎝

x1
i ≡k c

⇐⇒

x2
i ≡k c

⎞
⎠.
Next, we introduce an operator for comparing a vector of variables with a
bound. For a ∈Z let abs(a) denote the absolute value of a. Given a bound B
and some ˆr = (UB, EQ) let
(x ≥ˆr B) ≜

f∈UB
abs(f(x)) ≥B ∧

f∈EQ
abs(f(x)) < B.

134
M. Hague et al.
We claim there is an exponential bound B such that φ is variadic decomposable
iﬀfor all ˆr we have
∀x1, x2 ≥ˆr B. ∀y.
⎛
⎝
Regionˆr(x1, x2)
∧
SameDiv(x1, x2, y)
⎞
⎠⇒
⎛
⎝
φ(x1, y)
⇐⇒
φ(x2, y)
⎞
⎠
(DC-ˆr)
Note, unsatisﬁability can be tested in NP. First guess ˆr, then guess x1, x2, y.
We prove soundness of the claim in the next section. Completeness is an
extension of the argument for the monadic case and is given in the full version. In
the monadic case, we were able to take some values of x1, x2 > B such that both
satisﬁed the same divisibility constraints, but one value satisﬁed the formula
while the other did not. Since these values were large, we derived an inﬁnite
number of such value pairs with increasing values. We then used these growing
solutions to show that it was impossible for the value of x1 to satisfy the formula,
while the value of x2 does not, as they were both beyond the distinguishing power
of the linear inequalities. The argument for the variadic case is similar, with the
values of x1 and x2 being replaced by the values of f(x1) and f(x2).
4.3
Soundness
Assume there is an exponential bound B such that for each ˆr, Equation DC-ˆr
holds. We show how to produce a decomposition.
As in the monadic case (Sect. 3.3), let XDivs be the set of all constraints of the
form xi ≡k c in the expansion of the divisibility constraints of φ. Observe again
that there are only exponentially many maximal consistent subsets D ⊆XDivs.
For each D ﬁx a vector of values cD that satisﬁes all constraints in D and is
encodable in a polynomial number of bits. Furthermore, we deﬁne
DivD(z) ≜

xi≡kc∈D
zi ≡k c.
For each ˆr and D we can deﬁne an equivalence relation over values of x such
that x ≥ˆr B and DivD(x).

x1 =D
ˆr x2

≜(x1 ≥ˆr B ∧x2 ≥ˆr B ∧Regionˆr(x1, x2) ∧DivD(x1) ∧DivD(x2)) .
Observe each equivalence relation has an exponential number of equivalence
classes depending on the values of the bounded f. Let CD
ˆr be a set of minimal
representatives from each equivalence class such that each representative is rep-
resentable in a polynomial number of bits. These can be computed by solving
an existential Presburger constraint for each set of values of the bounded f. In
particular, for each ˆr = (UB, EQ) and assignments abs(cf) < B for each f ∈EQ,
we select a solution to the equation
x ≥ˆr B ∧

f∈EQ
f(x) = cf ∧DivD(x)

Monadic Decomposition in Integer Linear Arithmetic
135
if such a solution exists. If no such solution exists, the assignment can be ignored.
The decomposition is

ˆr

D

c∈CD
ˆr
(x ≥ˆr B ∧Regionˆr(x, c) ∧DivD(x) ∧φ(c, y)) .
The correctness of this decomposition follows from the Equations DC-ˆr. For any
values cx and cy of x and y, ﬁrst assume φ(cx, cy) holds. Since there is some
disjunct in the decomposition for which it holds that cx ≥ˆr B ∧Regionˆr(cx, c) ∧
DivD(x) then, by applying Equation DC-ˆr we get φ(c, cy) as required. Con-
versely, if some disjunct of the decomposition holds, we can apply Equation DC-ˆr
and obtain φ(cx, cy).
5
Applications of Decomposition
5.1
Monadic Decomposition in String Solving
The development of eﬀective techniques for solving string constraints has
received a lot of attention over the last years, motivated by applications rang-
ing from program veriﬁcation [2,16] and security analysis [22,23] to the analysis
of access policies of cloud services [4]. Strings give rise to a rich theory that
may combine, depending on the studied fragment, (i) word equations, i.e., equa-
tions over the free monoid generated by some ﬁnite (but often large) alphabet,
(ii) regular expression constraints, (iii) transduction, i.e., constraints described
by ﬁnite-state automata with multiple tracks, (iv) conversion functions, e.g.
between integer variables and strings encoding numbers in binary or decimal
notation, (v) length constraints, i.e., arithmetic constraints on the length of
strings.
Table 1. Statistics about the Kaluza benchmarks [22]. It should be noted (and is well-
known [18]) that the categories “sat” and “unsat” do not (always) imply the status of
the benchmarks, they only represent the way the benchmarks were organised by the
Kaluza authors.
Folder
#Benchmarks Benchmarks
with str.len
Decomposition
checks
Decomposition
checks succeeded
sat/small
19804
2185
2183
2155
sat/big
1741
1318
1317
56
unsat/small 11365
3910
2919
2919
unsat/big
14374
13813
6786
3362
Total
47284
21226
13205
8492
The handling of length constraints has turned out to be particularly chal-
lenging in this context, both practically and theoretically. Even for the combi-
nation of word equations (or even just quadratic word equations) with length

136
M. Hague et al.
constraints, decidability of the (quantiﬁer-free) theory is a long-standing open
problem [21]. At the same time, length constraints are quite frequently used
in applications; they are needed, for instance, when encoding operations like
indexof or substring, or also when splitting a string into the parts separated by
some delimiter. In standard benchmark libraries for string constraints, like the
Kaluza set [22], benchmarks with length constraints occur in large numbers.
The notion of monadic decomposition is in this setting important, since
any monadic length constraint (in Presburger arithmetic) can be reduced to
a Boolean combination of regular expression constraints, and is therefore easier
to handle than the general case.
Proposition 2. Satisﬁability of a quantiﬁer-free formula φ = φeq ∧φregex ∧φlen
consisting of word equations, regular expression constraints, and monadically
decomposable length constraints is decidable.
Proof. Suppose w1, . . . , wn
are the string variables occurring in φ, and
|w1|, . . . , |wn| the terms representing their length. A decision procedure can ﬁrst
compute a monadic representation φ′
len of φlen over lengths |w1|, . . . , |wn|, and
then turn each atom Δ(|wi|) in φ′
len into an equivalent regular membership con-
straint wi ∈LΔ. This is possible because the Presburger formula Δ can be
represented as a semi-linear set, which can directly be translated to a regular
expression. Decidability follows from the decidability of word equations com-
bined with regular expression constraints [13].
⊓⊔
This motivates the use of monadic decomposition as a standard pre-
processing step in string solvers, transforming away those length constraints
that can be turned into monadic form. To evaluate the eﬀectiveness of such an
optimisation, we implemented the decomposition check deﬁned in Sect. 3.2, and
used it within the string SMT solver OSTRICH [9] to determine the number of
Kaluza benchmarks with monadic decomposable length constraints.1 The results
are summarised in Table 1:
– Of altogether 47 284 benchmarks, 21 226 contain the str.len function, and
therefore length constraints. This number was determined by a simple textual
analysis of the benchmarks.
– Running our decomposition check in OSTRICH, in 13 205 of the 21 226 cases
length constraints were found that could be analysed. The remaining 8 021
problems were proven unsatisﬁable without ever reaching the string theory
solver in OSTRICH, i.e., as a result of pre-processing the input formula, or
because Boolean reasoning discovered obvious inconsistencies in the problems.
– In 8 492 of the 13 205 cases, all analysed length constraints were found to be
monadically decomposable; 4 713 of the benchmarks contained length con-
straints that could not be decomposed.
1 Branch “modec” of https://github.com/uuveriﬁers/ostrich, which also contains
detailed logs of the experiments.

Monadic Decomposition in Integer Linear Arithmetic
137
This means that 42 571 of the Kaluza benchmarks (slightly more than 90%)
do in principle not require support for length constraints in a string solver,
either because there are no length constraints, or because length constraints can
be decomposed and then turned into regular expression constraints.
Even with a largely unoptimised implementation, the time required to check
whether length constraints can be decomposed was negligible in case of the
Kaluza benchmarks, with the longest check requiring 2.1 s (on an AMD Opteron
2220 SE machine). The maximum number of variables in a length constraint was
140.
5.2
Variadic Decomposition in Quantiﬁer Elimination
A second natural application of decomposition is quantiﬁer elimination, i.e., the
problem of deriving an equivalent quantiﬁer-free formula φ′ for a given formula φ
with quantiﬁers. In Presburger arithmetic, for a formula φ = ∃x1, . . . , xn. ψ with
n quantiﬁers but no quantiﬁer alternations, quantiﬁer elimination in the worst
case causes a doubly-exponential increase in formula size [25].
Variadic decomposition can be used to eliminate quantiﬁers with a smaller
worst-case increase in size, provided that the matrix of a quantiﬁer formula can
be decomposed. Suppose φ = ∃¯x. ψ(¯x, ¯y) is given and ψ is variadic decomposable
on ¯x, i.e.,
ψ(¯x, ¯y) ≡

j
Δj(¯x) ∧ψj(¯y)
This means that the existential quantiﬁers can be distributed over the disjunc-
tion, and their elimination turns into a simpler satisﬁability check:
∃¯x. ψ(¯x, ¯y) ≡

j
∃¯x. Δj(¯x) ∧ψj(¯y) ≡

j: Δj(¯x) is sat
ψj(¯y)
Universal quantiﬁers can be handled in a similar way by negating the matrix
ﬁrst.
Proposition 3. Take a formula φ(¯y) = ∃¯x. ψ(¯x, ¯y) in Presburger arithmetic
in which ψ is quantiﬁer-free and variadic decomposable on ¯x. Then there is
a quantiﬁer-free formula φ′(¯y) that is equivalent to φ and at most singly-
exponentially bigger than φ.
Checking whether a formula can be decomposed is therefore a simple optimi-
sation that can be added to any quantiﬁer elimination procedure for Presburger
arithmetic.
6
Conclusion and Future Work
We have shown that the monadic and variadic decomposability problem for
QF(N) is coNP-complete. Moreover, when a decomposition exists, it is at most
exponential in size and can be computed in exponential time. This formula size

138
M. Hague et al.
is tight for decompositions presented in either disjunctive or conjunctive normal
form.
We gave two applications of our results. The ﬁrst was in string constraint
solving. In program analysis, string constraints are often mixed with numerical
constraints on the lengths of the strings (for example, via the indexOf function).
Length constraints signiﬁcantly complicate the analysis of strings. However, if
the string constraints permit a monadic decomposition, they may be reduced
to regular constraints and thus eliminated. We analysed the well-known Kaluza
benchmarks and showed that less than 10% of the benchmarks contained length
constraints that could not be decomposed.
For the second application, we showed that the doubly exponential blow-up
caused by quantiﬁer elimination can be limited to a singly exponential blow
up whenever the formula is decomposable on the quantiﬁed variables. Thus,
variadic decomposition can form an optimisation step in a quantiﬁer elimination
algorithm.
Interesting problems are opened up by our results. It would be interesting to
study lower bounds for general boolean formulas. If smaller decompositions are
possible, they would be useful for applications in string solving.
Second, we may consider variadic decomposition where a partition Π is not
given as part of the input. Instead, one must check whether a Π-decomposition
exists for some non-trivial Π. This variant of the problem has a simple ΣP
2
algorithm that ﬁrst guesses some Π and then veriﬁes Π-decomposability. How-
ever, the only known lower bound is coNP, which follows the same argument
as monadic decomposability. A better algorithm would not improve the worst-
case complexity for our quantiﬁer elimination application, but it might provide
a way to quickly identify a subset of a block of quantiﬁers that can be eliminated
quickly with Π-decompositions.
Acknowledgments. We
thank
Christoph
Haase,
Leonid
Libkin,
and
Pascal
Bergstr¨aßer for their help during the preparation of this work. Matthew Hague is
supported by EPSRC [EP/T00021X/1]. Anthony Lin is supported by the European
Research Council (ERC) under the European Union’s Horizon 2020 research and
innovation programme (grant agreement no 759969), and by Max-Planck Fellowship.
Philipp R¨ummer is supported by the Swedish Research Council (VR) under grant
2018-04727, and by the Swedish Foundation for Strategic Research (SSF) under the
project WebSec (Ref. RIT17-0011). Zhilin Wu is partially supported by the NSFC
grant No. 61872340, Guangdong Science and Technology Department grant (No.
2018B010107004), and the INRIA-CAS joint research project VIP.
References
1. Abdulla, P.A., et al.: TRAU: SMT solver for string constraints. In: Formal Methods
in Computer Aided Design, FMCAD 2018 (2018)
2. Abdulla, P.A., et al.: String constraints for veriﬁcation. In: Biere, A., Bloem, R.
(eds.) CAV 2014. LNCS, vol. 8559, pp. 150–166. Springer, Cham (2014). https://
doi.org/10.1007/978-3-319-08867-9 10

Monadic Decomposition in Integer Linear Arithmetic
139
3. Amadini, R., Gange, G., Stuckey, P.J.: Sweep-based propagation for string con-
straint solving. In: Proceedings of the Thirty-Second AAAI Conference on Artiﬁ-
cial Intelligence (AAAI 2018), the 30th Innovative Applications of Artiﬁcial Intel-
ligence (IAAI 2018), and the 8th AAAI Symposium on Educational Advances
in Artiﬁcial Intelligence (EAAI 2018), New Orleans, Louisiana, USA, 2–7 Febru-
ary 2018, pp. 6557–6564 (2018). https://www.aaai.org/ocs/index.php/AAAI/
AAAI18/paper/view/16223
4. Backes, J., et al.: Semantic-based automated reasoning for AWS access policies
using SMT. In: Bjørner, N., Gurﬁnkel, A. (eds.) 2018 Formal Methods in Computer
Aided Design, FMCAD 2018, Austin, TX, USA, 30 October–2 November 2018, pp.
1–9. IEEE (2018). https://doi.org/10.23919/FMCAD.2018.8602994
5. Barcel´o, P., Hong, C., Le, X.B., Lin, A.W., Niskanen, R.: Monadic decomposability
of regular relations. In: 46th International Colloquium on Automata, Languages,
and Programming, ICALP 2019, Patras, Greece, pp. 103:1–103:14 (2019). https://
doi.org/10.4230/LIPIcs.ICALP.2019.103
6. Berzish, M., Ganesh, V., Zheng, Y.: Z3str3: a string solver with theory-aware
heuristics. In: 2017 Formal Methods in Computer Aided Design, FMCAD 2017,
Vienna, Austria, 2–6 October 2017, pp. 55–59. IEEE (2017). https://doi.org/10.
23919/FMCAD.2017.8102241
7. B¨uchi, J.R., Senger, S.: Deﬁnability in the existential theory of concatenation and
undecidable extensions of this theory. In: Mac, L.S., Siefkes, D. (eds.) The Collected
Works of J. Richard B¨uchi, pp. 671–683. Springer, Heidelberg (1990). https://doi.
org/10.1007/978-1-4613-8928-6 37
8. Carton, O., Choﬀrut, C., Grigorieﬀ, S.: Decision problems among the main sub-
families of rational relations. ITA 40(2), 255–275 (2006). https://doi.org/10.1051/
ita:2006005
9. Chen, T., Hague, M., Lin, A.W., R¨ummer, P., Wu, Z.: Decision procedures for
path feasibility of string-manipulating programs with complex operations. CoRR
abs/1811.03167 (2018). https://arxiv.org/abs/1811.03167
10. Chistikov, D., Haase, C.: The taming of the semi-linear set. In: Chatzigian-
nakis, I., Mitzenmacher, M., Rabani, Y., Sangiorgi, D. (eds.) 43rd International
Colloquium on Automata, Languages, and Programming (ICALP 2016). Leib-
niz International Proceedings in Informatics (LIPIcs), vol. 55, pp. 128:1–128:13.
Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik, Dagstuhl, Germany (2016).
https://doi.org/10.4230/LIPIcs.ICALP.2016.128. http://drops.dagstuhl.de/opus/
volltexte/2016/6263
11. D’Antoni, L., Veanes, M.: The power of symbolic automata and transducers. In:
Majumdar, R., Kunˇcak, V. (eds.) CAV 2017, Part 1. LNCS, vol. 10426, pp. 47–67.
Springer, Cham (2017). https://doi.org/10.1007/978-3-319-63387-9 3
12. Day, J.D., Ehlers, T., Kulczynski, M., Manea, F., Nowotka, D., Poulsen, D.B.: On
solving word equations using SAT. In: Filiot, E., Jungers, R., Potapov, I. (eds.)
RP 2019. LNCS, vol. 11674, pp. 93–106. Springer, Cham (2019). https://doi.org/
10.1007/978-3-030-30806-3 8
13. Diekert, V.: Makanin’s algorithm. In: Lothaire, M. (ed.) Algebraic Combinatorics
on Words, Encyclopedia of Mathematics and its Applications, vol. 90, chap.
12, pp. 387–442. Cambridge University Press (2002). https://doi.org/10.1017/
CBO9781107326019.013
14. Ganesh, V., Minnes, M., Solar-Lezama, A., Rinard, M.: Word equations with length
constraints: what’s decidable? In: Biere, A., Nahir, A., Vos, T. (eds.) HVC 2012.
LNCS, vol. 7857, pp. 209–226. Springer, Heidelberg (2013). https://doi.org/10.
1007/978-3-642-39611-3 21

140
M. Hague et al.
15. Haase, C.: Subclasses of presburger arithmetic and the weak EXP hierarchy. In:
Joint Meeting of the Twenty-Third EACSL Annual Conference on Computer Sci-
ence Logic (CSL) and the Twenty-Ninth Annual ACM/IEEE Symposium on Logic
in Computer Science (LICS), CSL-LICS 2014, Vienna, Austria, 14–18 July 2014,
pp. 47:1–47:10 (2014). https://doi.org/10.1145/2603088.2603092
16. Hojjat, H., R¨ummer, P., Shamakhi, A.: On strings in software model checking.
In: Lin, A.W. (ed.) APLAS 2019. LNCS, vol. 11893, pp. 19–30. Springer, Cham
(2019). https://doi.org/10.1007/978-3-030-34175-6 2
17. Jez, A.: Word equations in linear space. CoRR abs/1702.00736 (2017). http://
arxiv.org/abs/1702.00736
18. Liang, T., Reynolds, A., Tinelli, C., Barrett, C., Deters, M.: A DPLL(T) theory
solver for a theory of strings and regular expressions. In: Biere, A., Bloem, R. (eds.)
CAV 2014. LNCS, vol. 8559, pp. 646–662. Springer, Cham (2014). https://doi.org/
10.1007/978-3-319-08867-9 43
19. Libkin, L.: Variable independence for ﬁrst-order deﬁnable constraints. ACM Trans.
Comput. Log. 4(4), 431–451 (2003). https://doi.org/10.1145/937555.937557
20. Lin, A.W., Barcel´o, P.: String solving with word equations and transducers:
towards a logic for analysing mutation XSS. In: Proceedings of the 43rd Annual
ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages,
POPL 2016, pp. 123–136. Springer (2016). https://doi.org/10.1145/2837614.
2837641
21. Lin, A.W., Majumdar, R.: Quadratic word equations with length constraints,
counter systems, and presburger arithmetic with divisibility. In: Lahiri, S.K., Wang,
C. (eds.) ATVA 2018. LNCS, vol. 11138, pp. 352–369. Springer, Cham (2018).
https://doi.org/10.1007/978-3-030-01090-4 21
22. Saxena, P., Akhawe, D., Hanna, S., Mao, F., McCamant, S., Song, D.: A symbolic
execution framework for JavaScript. In: 31st IEEE Symposium on Security and
Privacy, S&P 2010, Berleley/Oakland, California, USA, 16–19 May 2010, pp. 513–
528. IEEE (2010). https://doi.org/10.1109/SP.2010.38
23. Trinh, M., Chu, D., Jaﬀar, J.: S3: a symbolic string solver for vulnerability detection
in web applications. In: Proceedings of the 2014 ACM SIGSAC Conference on
Computer and Communications Security, CCS 2014, pp. 1232–1243. ACM (2014).
https://doi.org/10.1145/2660267.2660372
24. Veanes, M., Bjørner, N., Nachmanson, L., Bereg, S.: Monadic decomposition. J.
ACM 64(2), 14:1–14:28 (2017). https://doi.org/10.1145/3040488
25. Weispfenning, V.: Complexity and uniformity of elimination in presburger arith-
metic. In: Proceedings of the 1997 International Symposium on Symbolic and Alge-
braic Computation, ISSAC 1997, Maui, Hawaii, USA, 21–23 July 1997, pp. 48–53
(1997)

Scalable Algorithms for Abduction via
Enumerative Syntax-Guided Synthesis
Andrew Reynolds1, Haniel Barbosa2, Daniel Larraz1, and Cesare Tinelli1(B)
1 Department of Computer Science, The University of Iowa, Iowa City, USA
cesare-tinelli@uiowa.edu
2 Department of Computer Science, Universidade Federal de Minas Gerais (UFMG),
Belo Horizonte, Brazil
Abstract. The abduction problem in logic asks whether there exists a
formula that is consistent with a given set of axioms and, together with
these axioms, suﬃces to entail a given goal. We propose an approach
for solving this problem that is based on syntax-guided enumeration.
For scalability, we use a novel procedure that incrementally constructs a
solution in disjunctive normal form that is built from enumerated formu-
las. The procedure can be conﬁgured to generate progressively weaker
and simpler solutions over the course of a run of the procedure. Our
approach is fully general and can be applied over any background logic
that is handled by the underlying SMT solver in our approach. Our
experiments show our approach compares favorably with other tools for
abductive reasoning.
1
Introduction
The abduction problem for theory T, a set of axioms A and goal G asks whether
there exists a formula ϕ such that: (i) A ∧ϕ is T-satisﬁable and (ii) A ∧
ϕ |=T G. In other words, it asks for a formula ϕ that is consistent with the
axioms and when added to it allows the goal to be proven. Ideally, ϕ should be
as weak as possible and typically, it is expected to satisfy additional syntactic
restrictions, such as, for instance, on its quantiﬁer preﬁx. Abductive reasoning
has gained a variety of applications recently, including extending knowledge bases
for failed veriﬁcation conditions [16] and invariant generation [17,20]. Despite the
usefulness of abductive reasoning, and the recent development of a few abductive
reasoners, such as GPiD [19] and Explain [15], general tools for automatic
abductive inference are not yet mainstream.
Independently from the research on abduction, many high-performance
general-purpose solvers for syntax-guided synthesis (SyGuS) have also been
developed in the past decade. These solvers have been applied successfully in
a number of domains, including the implementation of network protocols [36],
This work was partially supported by NSF grant #1656926 and DARPA grants
#N66001-18-C-4006 and #N66001-18-C-4012.
c
⃝Springer Nature Switzerland AG 2020
N. Peltier and V. Sofronie-Stokkermans (Eds.): IJCAR 2020, LNAI 12166, pp. 141–160, 2020.
https://doi.org/10.1007/978-3-030-51074-9_9

142
A. Reynolds et al.
data processing [22], and code optimization [29]. The performance and scal-
ability of SyGuS solvers has made considerable progress in recently years, as
demonstrated by an annual competition [4].
In this paper, we investigate scalable approaches to solving the abduction
problem using (enumerative) syntax-guided synthesis techniques. We impose no
requirements on the background theory T other than it must be supported by an
existing SMT solver and amenable to syntax-guided synthesis, as we explain in
more detail later. Our immediate goal is to leverage the power of syntax-guided
synthesis solvers. Our longer term goal is to standardize the interface for these
solvers for abduction problems and make them available to users of program
analysis and automated reasoning who would beneﬁt from high performance
automated reasoning systems for abduction.
Contributions
– We introduce a novel procedure for solving abduction problems using enu-
merative syntax-guided synthesis.
– We give an extension of the procedure that is capable of generating progres-
sively weaker solutions to a given abduction problem.
– We provide an implementation of these techniques in cvc4sy [31], a state-
of-the-art SyGuS solver implemented within the SMT solver cvc4 [8], and
discuss several experiments we designed to test its eﬀectiveness. We show
that it has compelling advantages with respect to to other approaches for
abduction including those implemented in Explain [15] and GPiD [19].
2
Preliminaries
We work in the context of many-sorted ﬁrst-order logic with equality (≃) and
assume the reader is familiar with the notions of signature, terms, and so on
(see, e.g., [21]). A theory is a pair T = (Σ, I) where Σ is a signature and I is
a non-empty class of Σ-interpretations, the models of T, that is closed under
variable reassignment (i.e., every Σ-interpretation that diﬀers from one in I only
in how it interprets the variables is also in I) and isomorphism. A Σ-formula ϕ
is T-satisﬁable (respectively, T-unsatisﬁable) if it is satisﬁed by some (resp., no)
interpretation in I. A satisfying interpretation for ϕ is a model ϕ. A formula ϕ
is valid in T (or T-valid), written |=T ϕ, if every model of T is a model of ϕ. We
write ϕ[x] for a tuple x of distinct variables to indicate that the free variables
of ϕ occur in x. Given ϕ[x], we write ϕ[t] to denote the result of replacing every
occurrence of every variable of x in ϕ with the corresponding term in the tuple
t. We write conjunctions of formulas as sets.
Syntax-Guided Synthesis (SyGuS). Syntax-guided synthesis [2] is a recent
paradigm for automated synthesis that combines semantic and syntactic restric-
tions on the space of solutions. Speciﬁcally, a SyGuS problem for a function f
in a theory T consists of

Scalable Algorithms for Abduction via Enumerative SyGuS
143
1. semantic restrictions, a speciﬁcation given by a (second-order) T-formula of
the form ∃f. ∀x. ϕ[f, x], and
2. syntactic restrictions on the solutions for f, given by a context-free gram-
mar R.
The grammar R is a triple (s0, S, R) where s0 is an initial symbol, S is a set
of symbols with s0 ∈S, and R is a set of production rules of the form s →t,
where s ∈S and t is a term built from the symbols in the signature of theory
T, free variables, and symbols from S. The rules deﬁne a rewrite relation over
such terms, also denoted by →, as expected. We say a term t is generated by
R if s0 →∗t where →∗is the reﬂexive-transitive closure of →and t does not
contain symbols from S. For example, the terms x, (x + x) and ((1 + x) + 1) are
all generated by the grammar R = (I, {I}, {I →x, I →1, I →(I + I)}). A solution
for the SyGuS problem for f is a lambda term λx.e of the same type as f such
that (i) ∀x. ϕ[λx.e, x] is T-valid and (ii) e is generated by R.
A number of recent approaches for the syntax-guided synthesis problem
exist that target speciﬁc classes of semantic and syntactic restrictions, including
programming-by-examples [22], single invocation conjectures [32], and pointwise
speciﬁcations [5,27]. General purpose methods for solving the syntax-guided
synthesis problem are generally based on enumerative counterexample-guided
inductive synthesis (CEGIS) [34,35]. Enumerative approach uses a grammar to
generate candidate solutions systematically based on some term ordering, typ-
ically term size (e.g., the number of non-nullary function applications in the
term). The generated candidate solutions are then tested for correctness using
a veriﬁcation oracle (typically an SMT solver). This process is accelerated by
the use of counterexamples for previously discarded candidates, i.e., valuations
for the input variables x, or points, that witness the failure of those candi-
dates to satisfy the speciﬁcation. Despite its simplicity, enumerative CEGIS is
the de-facto approach for solving the general class of SyGuS problems, as imple-
mented in a several recent tools, notably cvc4sy [31] and the enumerative solver
ESolver [3]. Its main downside remains scalability to cases where the required
solution is very large. As we will show in Sect. 4, we present a more scalable
procedure for the abduction problem that builds on top of enumerative CEGIS
and is capable of quickly ﬁnding (conjunctive) solutions.
3
The Abduction Problem
In general, the abduction problem for a set A of axioms and a goal G is the
problem of ﬁnding a formula S that is consistent with A and, together with A,
entails the goal. We reﬁne the problem by restricting it to ﬁrst-order logic and to
a given background theory T, and also considering syntactic restrictions on the
solution S. We refer to this as the syntax-restricted abduction problem, which
we formalize in the following deﬁnition.
Deﬁnition 1 (Abduction Problem).
The (syntax-restricted) abduction
problem for a theory T, a conjunction A[x] of axioms, a goal G[x] and a grammar

144
A. Reynolds et al.
R, where axioms and goal are ﬁrst-order formulas, is that of ﬁnding a ﬁrst-order
formula S[x] such that:
1. A ∧S |=T G,
2. A ∧S is T-satisﬁable, and
3. S is generated by grammar R.
In practice, as in SyGuS, syntactic restrictions on the solution space may be
used to capture user-requirements on the desired shape of a solution. They can
also be used as a mechanism for narrowing the search space to one where one
believes the solver is likely to ﬁnd a solution. Observe that the formulation of the
problem includes the case with no syntactic restriction as a trivial case of a gram-
mar that accepts all formulas in the signature of the theory T. In the abduction
solver we have developed for this work, the syntax restriction is optional. When
it is missing, a grammar generating the full language is constructed internally
automatically.
Syntax-restricted abduction bears a strong similarity to SyGuS.1 We exploit
this similarity by leveraging much of the technology we developed for SyGuS,
with the goal of achieving generality and scalability.
Normally, an abduction problem admits many solutions. Thus, it may be
useful to look for solutions that optimize certain criteria, such as generality with
respect to entailment in T, or minimality with respect to size or number of free
variables. Our evaluation contains several case studies where we explore this
aspect in detail.
Recent Applications. Abduction has a long history in logic and automatic
reasoning (see, e.g., [24]). More recently, it has found many useful applications
in program analysis. It has been used for identifying the possible facts a veriﬁ-
cation tool is missing to either discharge or validate a veriﬁcation condition [16],
inferring library speciﬁcations that are needed for verifying a client program [37],
and synthesizing speciﬁcations for multiple unknown procedures called from a
main program [1]. Other applications of abduction includes loop invariant gener-
ation [17,20], where it is used to iteratively strengthen candidate solutions until
they are inductive and strong enough to verify a program, and compositional
program veriﬁcation [25], where it is used for inferring not only loop invariants
but also preconditions required for the invariants to hold. Abductive inference
has also been applied to modular heap reasoning [12], and the synthesis of miss-
ing guards for memory safety [18].
4
Abduction via Enumerative Syntax-Guided Synthesis
In this section, we ﬁx a theory T and describe our approach for solving the abduc-
tion problem in T using enumerative syntax-guided synthesis. We ﬁrst present a
basic procedure for abduction in the following section, and then extend it to gen-
erate (conjunctive) solutions in a highly scalable manner. We then describe how
1 In fact, it could be readily recast as SyGuS, if one ignored Condition 2 in Deﬁnition 1.

Scalable Algorithms for Abduction via Enumerative SyGuS
145
Fig. 1. Basic procedure for the abduction problem for axioms A, goal G and grammar R.
either approach can be extended to be incremental so that it constructs progres-
sively logically weaker solutions over time. For simplicity, we restrict ourselves to
abduction problems where axioms, goals, and solutions are quantiﬁer-free. Note,
however, that the procedure can be used for abduction problems where these
components are quantiﬁed, as long the restrictions below (lifted to quantiﬁed
formulas) are satisﬁed.
Requirements on T . We assume that the T-satisﬁability of quantiﬁer-free
formulas is decidable. For each sort of T, we also assume a distinguished set of
variable-free terms of that sort which we call values (e.g., numerals and negated
numerals in the case of integer arithmetic) such that every T-satisﬁable formula
is satisﬁed by a valuation of its free variables to sort elements denoted by values.
Finally, we require the availability of a computable function Eval that takes a
ﬁrst-order formula ϕ[x] and a tuple p of values of the same length as x, and
returns ⊤if ϕ[p] is T-satisﬁable and ⊥otherwise. These restrictions are met by
most theories used in Satisﬁability Modulo Theories (SMT).
4.1
Enumerative Counterexample-Guided Inductive Synthesis for
Abduction
We start with a basic CEGIS-style synthesis procedure for solving the syntax-
restriction abduction problem where points that represent counterexamples for
candidate solutions are cached and used to discard subsequent candidates. The
procedure is presented in Fig. 1. It takes as input: axioms A, goal G and gram-
mar R, and maintains an internally set P of points that satisfy the axioms and
falsify the goal. On line 3, it invokes the stateful sub-procedure NextEnum(R)
which enumerates the formulas generated by grammar R based on enumerative
techniques used in SyGuS solvers. We will refer to the return formula c as the
current candidate solution. Then, using the (fast) evaluation function Eval, it
checks at line 4 that c is satisﬁed by none of the counterexample points in P. If
the check fails, the procedure discards c and loops back to line 3 because adding

146
A. Reynolds et al.
c to A would deﬁnitely be not enough to entail G. If the check succeeds, it also
checks, at line 5, whether c ∧A ∧¬G is T-satisﬁable. If so, it obtains a witness
point p for the satisﬁability, adds it to current set of points P on line 6, and
discards c; otherwise, it checks that c is consistent with A before returning it as
a possible solution.
Example 1. Let T be the theory of linear integer arithmetic with the usual sig-
nature. Let A be the set {y ⩾0}, let G be the set {x + y + z ⩾0}, and assume R
is a grammar generating all linear arithmetic atomic formulas over the variables
x, y, z. The results of the procedure are summarized in the table below. We pro-
vide, for each iteration, the candidate c generated by syntax-guided enumeration
on line 3, the Boolean value of the conditions on lines 4, 5 and 7 of the proce-
dure when applicable, and the point (x, y, z) added to P in when the condition
on line 5 evaluates to true. The last column speciﬁes the solution returned on
that iteration if any.
#
c
line 4
line 5
p ∈P
line 7 return
1
x ⩾0
true
true
(0, 0, −1)
2
x < 0
true
true
(−1, 0, 0)
3
y ⩾0
false
4
y < 0
true
false
false
5
z ⩾0
false
6
z < 0
false
7 x + y ⩾0
false
8 x + y < 0
false
9 x + z ⩾0
true
false
true
x + z ⩾0
On the ﬁrst iteration, the syntax-guided enumeration generates the formula x ⩾
0 as the candidate solution c. This fails to imply the goal, speciﬁcally, with
(x, y, z) = (0, 0, −1) the axioms and c are satisﬁed and the goal is falsiﬁed. The
second candidate fails for similar reasons for point (−1, 0, 0). The check on line
4 fails for ﬁve of the next six candidates, with the exception of the candidate
y < 0. This candidate is falsiﬁed by both points in P but it must be discarded
since it is inconsistent with the axioms (line 7). Finally, the candidate x + z ⩾0
generated on the ninth iteration passes all the tests and is returned as a solution
for this abduction problem.
⊓⊔
4.2
A Procedure for Abduction Based on Unsat Core Learning
This section extends the procedure from Fig. 1 with techniques that make it
scalable when the intended solution to the abduction problem is a conjunction
of formulas. The procedure is applicable to cases where the language generated
by grammar R is closed under conjunction. In essence, the procedure in this
section applies when s0 →s0 ∧s0 is a production rule in R where s0 is the
start symbol of R. However, it avoids enumerating conjunctive formulas directly,
preferring instead to generate them as sets of (non-conjunctive) formulas.

Scalable Algorithms for Abduction via Enumerative SyGuS
147
Fig. 2. Procedure for the abduction problem for A, G and R based on unsat core
learning.
This procedure is presented in Fig. 2. Similarly to the basic procedure from
the previous section, it maintains a set of points P that satisfy the axioms and
falsify the goal. Additionally, the new procedure maintains a set E of enumerated
formulas, and a set U of subsets of E that are inconsistent with the axioms. The
procedure modiﬁes to each of these three sets during the course of its run. Each
loop iteration attempts to construct a set C of formulas whose conjunction is
a solution to the abduction problem. This is in contrast to the basic procedure
from Fig. 1 which considers only individual formulas as candidate solutions.
To construct the candidate set C, the procedure uses a helper function
EnsureCexFalsify which ensures that (i) C is non-empty, (ii) the conjunction
of the formulas in C is falsiﬁed by each point in P and (iii) no subset of C occurs
in U. The ﬁrst condition is to ensure that the candidate is generated by the
grammar. The second condition ensures that C along with our axioms suﬃces to

148
A. Reynolds et al.
prove the goal. The third condition ensures that C is consistent with the axioms.
If we are able to successfully construct a candidate solution set C, then line
6 checks whether that candidate indeed suﬃces when added to the axioms to
show the goal. If it does not, we add a counterexample point to P; otherwise, we
construct a (ideally minimal) subset of Cmin of C that also suﬃces to show the
goal. This information can be readily computed by an SMT solver [10] with sup-
port respectively for model generation and for unsatisﬁable core generation [13],
two features common to most modern solvers, including cvc4. We then check
whether Cmin is consistent with our axioms. If it is consistent, we return it as
a solution to the abduction problem; if it is not, we add some subset of it to U
that is also inconsistent with the axioms, where again the subset can be com-
puted by an SMT solver with support for unsatisﬁable cores. Adding such subset
amounts to learning that subset should never be included in future candidate
solutions. To maintain the invariant that no subset of C occurs in U, we remove
one enumerated formula e ∈u from C on line 12. In the case where a point is
added to P (line 15) or when an unsat core is added to U (line 12), we run the
method EnsureCexFalsify starting from the current resultant set C. This will force
the procedure to try to construct a new candidate solution based on the set E.
When this strategy fails to construct a candidate, the inner loop terminates and
the next formula is added to E based on syntax-guided enumeration.
We now revisit Example 1. As demonstrated in this example, GetAbductUCL
is often capable of generating solutions to the abduction problem faster than the
one from Fig. 1, albeit those solutions may be logically stronger.
Example 2. We revisit Example 1, where A is the set {y ⩾0} and G is {x+y+z ⩾
0}. A run of the procedure from Fig. 2 is summarized in the table below. We list
iterations of the outer loop of the procedure (lines 2–18) in the ﬁrst column of this
table. For each iteration, we provide the formula that is added to our pool E (line
3), and the considered candidate set C upon a successful call to EnsureCexFalsify.
Notice that the inner loop of the procedure may consider multiple candidates C
for a single iteration of the outer loop. For each candidate, when applicable, we
give the result of the evaluation of the condition on line 6, the point p added to
P if that condition is false (line 15), the minimal candidate set Cmin constructed
on line 7, the evaluation of the condition on line 8, the set of formulas added to
our set of unsatisﬁable cores if that condition is false (line 12), and ﬁnally the
formula (if any) returned as a solution (line 9).
# e ∈E
C
line 6
p ∈P
Cmin line 8
u ∈U
return
1 x ⩾0
{x ⩾0}
false
(0, 0, −1)
2 x < 0
{x < 0}
false
(−1, 0, 0)
{x < 0, x ⩾0}
true
C
false
{x < 0, x ⩾0}
3 y ⩾0
4 y < 0
{y < 0}
true
C
false
{y < 0}
5 z ⩾0 {x ⩾0, z ⩾0}
true
C
true
x ⩾0 ∧z ⩾0

Scalable Algorithms for Abduction via Enumerative SyGuS
149
We assume the same ordered list of formulas enumerated from Fig. 1. On the
ﬁrst iteration, we add x ⩾0 to our pool of enumerated formulas E. The helper
function EnsureCexFalsify constructs the set C = {x ⩾0} since (vacuously) it is
true for all points in P. Similar to the ﬁrst iteration of Fig. 1, on line 6 we learn
that x ⩾0 does not suﬃce with our axioms to show the goal; a counterexample
point is (x, y, z) = (0, 0, −1) which is added to P. Afterwards, EnsureCexFalsify is
not capable of constructing another C since there are no other formulas in E. In
contrast to Fig. 1 which discards the formula x ⩾0 at this point, here it remains
in E and can be added as part of C in future iterations.
On the second iteration, we add x < 0 to our pool. We check the candidate set
C = {x < 0}, which fails to imply the goal for counterexample point (x, y, z) =
(−1, 0, 0). To construct the next candidate set C, we must ﬁnd an additional
formula from E that evaluates to false on this point (or otherwise we again
would fail to imply our goal). Indeed, x ⩾0 ∈E evaluates to false on this point,
and thus EnsureCexFalsify returns the set {x < 0, x ⩾0}. This set suﬃces to
prove the goal given the axioms, that is, the condition on line 6 succeeds; the
unsatisﬁable core Cmin computed for this query is the same as C. However, on line
8, we learn that this set is inconsistent with our axioms (in fact, the set by itself
is equivalent to false). On line 12, we add {x < 0, x ⩾0} to U. In other words,
we learn that any solution that contains both these formulas is inconsistent with
our axioms. Learning this subset will help prune later candidate solutions. The
procedure on this iteration proceeds by removing one of these formulas from
our candidate solution set C. Subsequently the helper function EnsureCexFalsify
cannot construct a new candidate subset due to {x < 0, x ⩾0} ∈U and since
no other formulas occur in E.
On the third iteration, y ⩾0 is added to our pool. However, no candidate
solution can be constructed, where notice that y ⩾0 evaluates to ⊤on both
points in P. On the fourth iteration, y < 0 is added to our pool and the candidate
solution set {y < 0} is constructed, where notice that this formula evaluates to
⊥on both points in P. This formula suﬃces to show the goal from the axioms,
but is however inconsistent with our axioms. Thus, {y < 0} is added to our set of
unsatisﬁable cores U. In other words, we have learned that no solution C should
include the formula y < 0 since it is alone inconsistent with our axioms.
On the ﬁfth iteration, z ⩾0 is added to our pool. The only viable candidate
that falsiﬁes all points in P and does not contain a subset from U is {x ⩾0, z ⩾
0}. This set is a solution to the abduction problem and so the formula x ⩾
0 ∧z ⩾0 is returned. Due to our assumption that R admits conjunctions, this
formula meets the syntax restrictions of our grammar. A run of this procedure
required the enumeration of only 5 formulas before ﬁnding a solution whereas
the basic one in Fig. 1 required 9.
⊓⊔
While the solution in the previous example x ⩾0 ∧z ⩾0 was found in
fewer iterations, notice that it is logically stronger than the solution x + z ⩾
0 produced in Example 1, since x ⩾0 ∧z ⩾0 entails x + z ⩾0 but not
vice versa. We remark that the main advantage of procedure Fig. 2 is that is
typically capable of generating any feasible solution to the abduction problem

150
A. Reynolds et al.
faster than the procedure from Fig. 1. This is especially the case if the only
solutions to the abduction problem consist of a large conjunction of literals of
small term size ℓ1 ∧. . . ∧ℓn. The basic procedure does not scale to this case, if
its enumeration is by formula size, since it will have to wait until the conjunction
above is enumerated as an individual formula.
Furthermore, we remark that procedure in this section can be conﬁgured to
have the same solution completeness guarantees as the basic procedure from
Fig. 1. In particular, our choice of e in the EnsureCexFalsify method chooses the
most recently enumerated formula when the candidate pool C is empty. Since
a single loop of the procedure is terminating and due to the above policy for
selection, the procedure will terminate in the worst case when the enumerated
pool E contains a formula that by itself is the solution to the synthesis conjecture.
4.3
Incremental Weakening for Abduction
The user may be interested in obtaining an abduction problem solution that
maximizes some criteria and is not necessarily the ﬁrst one discovered by (either
of) the procedures we have described so far. In this section, we describe an exten-
sion to our approach for abduction that maintains the advantage of returning
solutions quickly while still seeking to generate the best solution in the long run
according to metric such as logical weakness.
We observe that it is straightforward to extend our enumerative syntax-
guided approach to generate multiple solutions. We are interested, however, in
generating increasingly better solutions over time. We brieﬂy give an overview of
how the procedures of Fig. 1 and Fig. 2 can be extended in this way and discuss
a few relevant details of the extension. We focus on the problem of generating
the logically weakest solution to the abduction problem in this section.
Figure 3 presents an incremental procedure for generating (multiple) solu-
tions to a given abduction problem. The procedure requires that the language
restriction R admit disjunctive formulas which is the case, for instance, if
s0 →s0 ∨s0 is a production rule in R where s0 is again the start symbol.
It maintains a formula S that, when not ⊥, represents the logically weakest solu-
tion to the abduction problem known so far. In its main loop, on line 3, the
procedure calls one of the previous procedures for generating single solutions to
the abduction problem (written GetAbduct∗). Line 4 then checks whether a new
solution can be constructed that is logically weaker with respect to the axioms
than the current one. In particular, this is the case if C ∧A ∧¬S is T-satisﬁable,
which means that there is at least one point that satisﬁes the current candidate
but not the current solution S. In that case, the current solution S is updated to
S ∨C, which is by construction guaranteed to also be a solution to the abduc-
tion problem. If no such point can be found, then C is redundant with respect
to the current candidate solution since it does not generalize it. Optionally, the
procedure may learn a subset u of C that is also redundant with respect to the
current candidate solution. This subset can be learned as an unsatisﬁable core
when using the procedure GetAbductUCL as the sub-procedure on line 3.

Scalable Algorithms for Abduction via Enumerative SyGuS
151
Fig. 3. Incremental abduction procedure for axioms A, goal G and grammar R.
4.4
Implementation Details
We implemented the procedures above in the state-of-the-art SMT solver
cvc4 [8]. cvc4 incorporates a SyGuS solver, cvc4sy, implementing several
strategies for enumerative syntax-guided synthesis [31]. It accepts as input both
SMT problems written in the SMT-LIB version 2.6 format [9], and synthesis
problems written in the SyGuS version 2.0 format [30]. SMT-LIB version 2.6 is
a scripting language that allows one to assert a formula F to the solver with a
command of the form (assert F). The solver checks the satisﬁability the formu-
las asserted so far in response to the command (check-sat). We extended cvc4’s
SMT-LIB parser to support also commands of the form (get-abduct p G R) where
p is a symbol, the identiﬁer of the expected solution formula; G is a formula, the
goal of the abduction problem; and the optional R is a grammar expressed in
the SyGuS version 2.0 format. This command asks the solver to ﬁnd a formula
that is a solution to the abduction problem (A, G), where A, standing for the
set of axioms, consists of the conjunction of the currently asserted formulas. The
expected response from the solver is a deﬁnition of the form (deﬁne-fun p () Bool
S) where p is the identiﬁer provided in the ﬁrst argument of get-abduct and S is
a formula that solves the abduction problem.
Internally, invoking a get-abduct command causes a synthesis conjecture to be
constructed and passed to cvc4sy. The latter normally accepts conjectures of the
form ∃f. ∀x. ϕ[f, x] where ϕ is quantiﬁer-free. Thus, we must pass the abduction
problem in two parts: (i) the synthesis conjecture ∃P. ∀x. ¬(P(x)∧A∧¬G) where
x collects the free variables of A and of G,2 stating that the expected solution
P along with the axioms A must entail the goal G, and (ii) a side condition
∃x. P(x) ∧A stating that P must be consistent with the axioms. The synthesis
conjecture is of a form that can be readily handled by cvc4sy and processed
using its current techniques. We have modiﬁed it so that it considers the side
condition as well during solving, as described in Figs. 1 and 2.
2 We assume that all free symbols in A and G are variables.

152
A. Reynolds et al.
The procedure in Fig. 2 is implemented as a strategy on top of the basic
enumerative CEGIS loop of cvc4sy. We give some noteworthy implementation
details here. Firstly, we use a data structure for eﬃciently checking whether any
subset of C occurs in our set of unsatisﬁable cores U, which keeps the sets in U
in an index and is traversed dynamically as formulas are added to C. We chose
enumerated formulas on line 2 of EnsureCexFalsify by selecting ﬁrst the most
recently generated formula, and then a random one amongst those that meet
the criteria to be included in C. Finally, since the number of candidate solutions
can be exponential in the worst case for a given iteration of the inner loop of
this procedure, we use a heuristic where formulas cannot be added to C more
than once in the same iteration of the loop, making the number of candidate
sets tried on a given iteration linear in the size of E in the worst case.
5
Evaluation
We evaluated our approach3 in comparison with cvc4sy’s enumerative CEGIS, a
general purpose synthesis approach, as well as with GPiD [19] and Explain [15],
state-of-the-art solvers for similar abduction problems as the one deﬁned here. In
the comparison below, we refer to the basic procedure from Fig. 1 as cvc4sy+b
and the one from Fig. 2 as cvc4sy+u. Experiments ran on a cluster with Intel
E5-2637 v4 CPUs, Ubuntu 16.04. Each execution of a solver on a benchmark
was provisioned one core, 300 s and 8 GB RAM.
5.1
Benchmarks
Since abduction tools are generally focused on speciﬁc application domains, there
is no standard language or benchmark library for evaluation. Moreover, these
tools use abduction as part of a larger veriﬁcation toolchain. As here we did
not target a speciﬁc application but rather the abduction problem as a whole,
an evaluation with their benchmarks would require integrating our solver in the
tools as an alternative abduction engine. This was not feasible due to either
the source code not being available or the veriﬁcation and abduction engines
being too tightly coupled for us to use our solver as an alternative. Thus we had
to generate our own abduction benchmark sets. We did so using benchmarks
relevant for veriﬁcation from SMT-LIB [9], the standard test suite for SMT
solvers. We chose as a basis the SMT-LIB logics QF LIA, QF NIA, and QF SLIA
due to their relevance for veriﬁcation. For QF NIA, we focus on the benchmark
family VeryMax and on kaluza for QF SLIA. In QF LIA we excluded benchmark
families whose benchmarks explode in size without the let operator. This was
necessary to allow a comparison with Explain, whose parser does not fully
support let, on let-free benchmarks. We considered both benchmarks that were
(annotated as) satisﬁable and unsatisﬁable for generating abduction problems,
according to the following methodology.
3 Full material at http://cvc4.cs.stanford.edu/papers/abduction-sygus/.

Scalable Algorithms for Abduction via Enumerative SyGuS
153
Given a satisﬁable SMT-LIB problem ϕ = ψ1 ∧· · · ∧ψn4 in the theory T,
we see it as an encoding of a validity problem ψ1 ∧· · · ∧ψn−1 |= ¬ψn that
could not be proven. We consider the abduction problem where G is ¬ψn, A is
ψ1 ∧· · · ∧ψn−1, and R is a grammar that generates any quantiﬁer-free formula
in the language of T over the free variables of G and A. A solution S to this
problem allows the validity of ϕ to be proven, since ϕ ∧S is unsatisﬁable.
Given an unsatisﬁable SMT-LIB problem ϕ, let U = {ψ1, . . . , ψn} be a
minimal unsatisﬁable core for this formula, i.e. any conjunctive set U
{ψ}, for
some ψ ∈U, is satisﬁable. Let ψmax be U’s component with maximal size. We
will call ψmax the reference to the abduction problem. We consider the abduction
problem whose G is ¬ψG, for some ψG ∈U and ψG ̸= ψmax, whose axioms A are
U
{ψG, ψmax} and R as before is a grammar that generates any formula in the
language of T over the free variables of G and A. A solution S to this problem
allows proving the validity of U
{ψG, ψmax} |= ψG, since U
{ψmax} ∪{S}
is unsatisﬁable. Solving this abduction problem amounts to “completing” the
original unsatisﬁable core with the further restriction that this completion is at
least as weak as the reference, as well as consistent with all but one of the other
core components, seen as axioms for the abduction problem.
From satisﬁable SMT-LIB benchmarks we generated 2025 abduction prob-
lems in QF LIA, 12214 in QF NIA and 11954 in QF SLIA. For unsatisﬁable
benchmarks we were limited not only by the benchmark annotations but also by
being able to ﬁnd minimal unsatisﬁable cores. We used the Z3 SMT solver [14] to
generate minimal unsatisﬁable cores with a 120s timeout. Excluding benchmarks
whose cores had less than three assertions (so we could have axioms, a goal and
a reference), we ended up with 97 problems in QF LIA, 781 in QF NIA and 2546
in QF SLIA. We chose the reference as the component of the unsatisﬁable core
with maximal size and the goal as the last formula in the core (viewed as a list)
after the reference was removed.
Table 1. Comparison of abduction problems from originally SAT SMT-LIB bench-
marks.
Logic
#
cvc4sy+b
cvc4sy+u
Solved Unique Weaker Solved Unique Weaker
QF LIA
2025
721
261
183
594
134
2
QF SLIA 11954 10902
3
466
10980
81
0
QF NIA
12214
1492
171
671
1712
391
45
Total
26593 13329
435
1320
13628
606
47
5.2
Finding Missing Assumptions in SAT Benchmarks
In this section we evaluate how eﬀective cvc4sy+b and cvc4sy+u are in (i)
ﬁnding any solution to the abduction problem and (ii) ﬁnding logically weak
4 SMT-LIB problems are represented as sequences of assertions. Here we considered
each ψi as one of these assertions.

154
A. Reynolds et al.
solutions. The evaluation is done on the abduction problems produced from
satisﬁable SMT-LIB benchmarks as above. Results are summarized in Table 1.
The number of solved problems corresponds to the problems for which a given
cvc4 conﬁguration could ﬁnd a solution within 300s. cvc4sy+u solves a sig-
niﬁcant number of problems more than cvc4sy+b in all logics but QF LIA.
In both QF LIA and QF NIA we can see a signiﬁcant orthogonality between
both approaches. We attribute these both to the fragility of integer arithmetic
reasoning, where the underlying ground solver checking the consistency of can-
didate solutions is greatly impacted by the shape of the problems it is given.
Overall, the procedure in cvc4sy+u leads to a better success rate than the
basic procedure in cvc4sy+b. Solution strength was evaluated on commonly
solved problems considering the solutions produced according to the incremen-
tal procedures shown in Sect. 4.3, in which the overall solution is a disjunction
of individual solutions found over time. As expected, cvc4sy+u is able to solve
more problems but at the cost of often producing stronger (and bigger) solu-
tions than cvc4sy+b. This is particularly the case in QF SLIA and QF NIA,
in which cvc4sy+u both solves many more problems and often ﬁnds stronger
solutions.
5.3
Completing UNSAT Cores
Here we evaluate how eﬀective cvc4sy+b and cvc4sy+u are in solving the
abduction problem with the extra restriction of ﬁnding a solution that is at least
as weak as a given reference formula. We use the abduction problems produced
from unsatisﬁable SMT-LIB benchmarks following the methodology of Sect. 5.1
as the basis for this evaluation.
Table 2. Comparison of abduction problems from originally UNSAT SMT-LIB bench-
marks.
Logic
#
cvc4sy+b
cvc4sy+u
Solved Unique Solved Unique
QF LIA
97
6
0
6
0
QF SLIA 2546 2546
32
2514
0
QF NIA
781
86
49
41
4
Total
3424 2638
81
2561
4
The results are summarized in Table 2. cvc4sy+b signiﬁcantly outperforms
cvc4sy+u in QF SLIA, in which the references are very simple formulas (gener-
ally with size below 3), for which the specialized procedure of cvc4sy+u is not
necessary. Overall, as in the previous section when checking who ﬁnds the weak-
est solution, cvc4sy+b has as advantage over cvc4sy+u for ﬁnding solutions
as weak as the reference.

Scalable Algorithms for Abduction via Enumerative SyGuS
155
5.4
Comparison with Explain
Explain [15] is a tool for abductive inference based on quantiﬁer elimination. It
accepts as input a subset of SMT-LIB and we extended it to support abduction
problems as generated in Sect. 5.1. However, Explain imposes more restrictions
to their solutions, only producing those with a minimal number of variables
and for which every other solution with those variables is not stronger than it.
Their rationale is ﬁnding “simple” solutions, according to the above criteria,
which are more interesting to their applications. Since we do not apply these
restrictions in cvc4, nor is in the scope of this paper incorporating them into
our procedure, it should be noted that comparing cvc4 and Explain puts the
latter at a disadvantage. We considered satisﬁable SMT-LIB problems in the
QF LIA logic for our evaluation, as QF LIA is better supported by Explain
(Table 3).
Table 3. Comparison with Explain in 2025 abduction problems in QF LIA
Solved Unique Total time
cvc4sy+b 721
261
418849 s
cvc4sy+u 594
125
449424 s
Explain
33
0
532839 s
All problems solved by Explain are solved by cvc4sy+u. Of these 33 prob-
lems, cvc4sy+u, in incremental mode, ﬁnds a solution with the same minimal
number of variables as Explain for 25 of them. Of the 8 problems to which
it only ﬁnds solutions with more variables, in 4 of them the diﬀerence is of a
single variable. All other 4 are in the slacks benchmark family, which contains
crafted problems. A similar comparison occurs with cvc4sy+b. This shows that
even though cvc4 is not optimized to minimize the number of variables it its
solutions, it can still often ﬁnds solutions that are optimal (or close to optimal)
according to Explain’s criteria, while solving a much larger number of problems
with a fully general approach.
5.5
Comparison with GPiD
We also compared cvc4 with GPiD [19], a framework for generating impli-
cates, i.e. logical consequences of formulas. As Echenim et al. say in their paper,
negating the implicate of a satisﬁable formula ϕ yields the “missing hypothesis”
for making ϕ unsatisﬁable. Therefore GPiD solves a similar problem to that
of Sect. 5.2, diﬀering by they always considering an empty set of axioms and
the whole original formula as the goal. Given this similarity, we compare the
performance of GPiD in generating implicates for satisﬁable benchmarks and
of cvc4sy+b and cvc4sy+u in solving abduction problems generated from
those same benchmarks. We did not consider the benchmarks from the previous

156
A. Reynolds et al.
sections because we were not able to produce abduces, which are the syntac-
tic components GPiD uses to ﬁnd implicates, for other logics using the tools in
GPiD public repository5. Thus we restricted our analysis to 400 abduction prob-
lems produced, as per the methodology of Sect. 5.1, from satisﬁable QF UFLIA
benchmarks that were used in [19]. Note however that the cvc4 conﬁgurations
will require solutions to be consistent with all but the last assertion in the prob-
lems (which are the axioms in the respective abduction problem). Since that,
as far as we know, this is not a requirement in GPiD, eﬀectively cvc4sy+b
and cvc4sy+u are solving a harder problem than GPiD. We formulated the
abduction problem this way, rather than as with all assertions as goals, to avoid
trivializing the abduction problem, for which the negation of the goal would
always be a solution. Also note that the presence of uninterpreted functions in
the abduction problem requires solutions to be generated in a higher-order back-
ground logic, which cvc4 supports after a recent extension [7]. As in [19], we used
GPiD’s version with the Z3 backend. We present their results with (GPiD-1)
and without (GPiD) the restriction to limit the set of abduces to size 1.
Table 4. Comparison with GPiD on 400 abduction problems in the QF UFLIA logic.
Solved Unique Total time
cvc4sy+b 214
0
57290 s
cvc4sy+u 342
0
18735 s
GPiD
193
0
69 s
GPiD-1
398
54
1188 s
Results are summarized in Table 4. cvc4sy+u signiﬁcantly outperforms
cvc4sy+b, both in the number of problems solved and in total time, besides
being almost 20% faster on commonly solved problems. We also see that solu-
tion ﬁnding in GPiD is heavily dependent on which abduces are considered when
building solutions, as it solves almost all benchmarks when limited to abduces
of size 1 but barely half when unrestricted. It should also be noted that GPiD
takes pre-computed abduces, whose production time is not accounted for in the
evaluation. Despite this, cvc4sy+u is only on average 30% slower on commonly
solved problems than GPiD-1 and solves many more problems than GPiD. The
big variation of GPiD results in terms of what pre-determined set of candidates
can be used in the computation is a severe limitation of their tool. Similarly,
while the method proposed in [19] is theory agnostic, their tooling for producing
abduces imposes strong limitations on the usage of GPiD for theories other than
QF UFLIA.
5 At https://github.com/sellamiy/GPiD-Framework.

Scalable Algorithms for Abduction via Enumerative SyGuS
157
6
Related Work
The procedure introduced in Sect. 4.2 based on unsat core learning follows a
recent trend in enumerative syntax-guided synthesis solving that aims to improve
scalability by applying divide-and-conquer techniques, where candidate solutions
are built from smaller enumerated pieces rather than being directly enumerated.
While previous approaches, both for pointwise [5,27] and for unrestricted speciﬁ-
cations [6], have targeted general-purpose function synthesis, we specialize divide
and conquer for solving the abduction problem with a lean (see Sect. 4.4) and
eﬀective (see Sect. 5) procedure.
Abductive inference tools for the propositional case include the AbHS and
AbHS+ tools [26,33], based on SAT solvers [11] and hitting set procedures, and
the Hyper [23] tool, that includes a series of algorithmic improvements over the
former, and uses a MaxSAT solver for computing the hitting set. Like AbHS,
GetAbductUCL checks entailment and consistency using two separate calls to the
underlying solver, and uses its failures for the selection of new candidates. In
contrast, GetAbductUCL keeps this information in two dedicated data structures
rather than encoding it with an implicit hitting set. Another signiﬁcant diﬀer-
ence is that the set of hypotheses in the propositional case is ﬁxed and ﬁnite,
whereas in our setting it is generated dynamically from a grammar. More general
approaches, to which our work bears more resemblance and to which we provided
an experimental comparison in Sect. 5, are GPiD [19] and Explain [15]. GPiD
uses an oﬀ-the-shelf SMT solver as a black box to generate ground implicates.
It can be used with any theory supported by the underlying SMT solver, simi-
larly to our SyGuS-based approach. While we enumerate formulas that compose
the solution for the abduction problem GPiD’s authors use abducibles, which
are equalities and disequalities over the variables in the problem. They similarly
build candidates in a reﬁnement loop by combining abducibles according to con-
sistency checks performed by an underlying SMT solver. They use an order on
abducibles to guide the search, which is analogous to the enumeration order in
enumerative synthesis. Explain on the other hand is built on top of an SMT
solver for the theories of linear integer arithmetic and of equality with unin-
terpreted functions, although its abduction inference procedure in principle can
work with any theory that admits quantiﬁer elimination. The method imple-
mented in Explain is based on ﬁrst determining a subset of the variables in the
abduction problem and trying to build the weakest solution over these variables
via quantiﬁer elimination, while computing minimal satisfying assignments to
ensure that a found solution covers a minimal subset. This method, however, is
not complete, as it can miss solutions. The tool also allows the user to specify
costs for each variable, so that a given minimal set may be favored.
7
Conclusion
We have described approaches for solving the abduction problem using a modern
enumerative solver for syntax-guided synthesis. Our evaluation shows that proce-
dures based on enumerative CEGIS scale for several non-trivial abduction tasks,

158
A. Reynolds et al.
and have several compelling advantages with respect to other approaches like
those used in Explain and GPiD. In several cases, it suﬃces to use a basic pro-
cedure for enumerative CEGIS to generate solutions to abduction problems that
are optimal according to certain metrics. Moreover, the generation of feasible
solutions can be complemented and accelerated via a procedure for generating
conjunctions of enumerated formulas as shown in Fig. 2.
We believe that new abduction capabilities presented in this paper and imple-
mented in cvc4 will be useful in all the applications of abduction we describe
in Sect. 3. In addition, we see a number of promising applications in the context
of SMT itself. For example, we plan to use abduction to generate useful condi-
tional rewrite rules for SMT solvers. Many such rules are used internally by SMT
solvers to simplify their input formulas by (equivalence-preserving) term rewrit-
ing. The manual identiﬁcation and selection of good rewrite rules is a tedious
and error-prone process. Abduction can be used to generalize a recent approach
for the semi-automated development of rewrite rules [28] by synthesizing (most
general) conditions under which two terms are equivalent. This in turn can be
used to develop new solving strategies in the SMT solver based on those rewrite
rules.
References
1. Albarghouthi, A., Dillig, I., Gurﬁnkel, A.: Maximal speciﬁcation synthesis. ACM
SIGPLAN Not. 51(1), 789–801 (2016)
2. Alur, R., et al.: Syntax-guided synthesis. In: Formal Methods in Computer-Aided
Design (FMCAD), pp. 1–8. IEEE (2013)
3. Alur, R., ˇCern´y, P., Radhakrishna, A.: Synthesis through uniﬁcation. In: Kroening,
D., P˘as˘areanu, C.S. (eds.) CAV 2015. LNCS, vol. 9207, pp. 163–179. Springer,
Cham (2015). https://doi.org/10.1007/978-3-319-21668-3 10
4. Alur, R., Fisman, D., Singh, R., Solar-Lezama, A.: SyGuS-comp 2017: results and
analysis. In: Proceedings Sixth Workshop on Synthesis, SYNT@CAV 2017, Heidel-
berg, Germany, 22 July 2017, pp. 97–115 (2017)
5. Alur, R., Radhakrishna, A., Udupa, A.: Scaling enumerative program synthesis via
divide and conquer. In: Legay, A., Margaria, T. (eds.) TACAS 2017. LNCS, vol.
10205, pp. 319–336. Springer, Heidelberg (2017). https://doi.org/10.1007/978-3-
662-54577-5 18
6. Barbosa, H., Reynolds, A., Larraz, D., Tinelli, C.: Extending enumerative function
synthesis via SMT-driven classiﬁcation. In: Barrett, C.W., Yang, J. (eds.) Formal
Methods in Computer-Aided Design (FMCAD), pp. 212–220. IEEE (2019)
7. Barbosa, H., Reynolds, A., El Ouraoui, D., Tinelli, C., Barrett, C.: Extending SMT
solvers to higher-order logic. In: Fontaine, P. (ed.) CADE 2019. LNCS (LNAI),
vol. 11716, pp. 35–54. Springer, Cham (2019). https://doi.org/10.1007/978-3-030-
29436-6 3
8. Barrett, C., et al.: CVC4. In: Gopalakrishnan, G., Qadeer, S. (eds.) CAV 2011.
LNCS, vol. 6806, pp. 171–177. Springer, Heidelberg (2011). https://doi.org/10.
1007/978-3-642-22110-1 14
9. Barrett, C., Fontaine, P., Tinelli, C.: The SMT-LIB Standard: Version 2.6. Techni-
cal report, Department of Computer Science, The University of Iowa (2017). www.
SMT-LIB.org

Scalable Algorithms for Abduction via Enumerative SyGuS
159
10. Barrett, C., Sebastiani, R., Seshia, S., Tinelli, C.: Satisﬁability modulo theories
(chap. 26). In Biere, A., Heule, M.J.H., van Maaren, H., Walsh, T. (eds.) Handbook
of Satisﬁability. Frontiers in Artiﬁcial Intelligence and Applications, vol. 185, pp.
825–885. IOS Press (2009)
11. Biere, A., Heule, M., van Maaren, H., Walsh, T.: Handbook of Satisﬁability: Volume
185 Frontiers in Artiﬁcial Intelligence and Applications. IOS Press, Amsterdam
(2009)
12. Calcagno, C., Distefano, D., O’Hearn, P., Yang, H: Compositional shape analysis
by means of bi-abduction. In: Proceedings of the 36th Annual ACM SIGPLAN-
SIGACT Symposium on Principles of Programming Languages, pp. 289–300 (2009)
13. Cimatti, A., Griggio, A., Sebastiani, R.: Computing small unsatisﬁable cores in
satisﬁability modulo theories. J. Artif. Intell. Res. (JAIR) 40, 701–728 (2011)
14. de Moura, L., Bjørner, N.: Z3: an eﬃcient SMT solver. In: Ramakrishnan, C.R.,
Rehof, J. (eds.) TACAS 2008. LNCS, vol. 4963, pp. 337–340. Springer, Heidelberg
(2008). https://doi.org/10.1007/978-3-540-78800-3 24
15. Dillig, I., Dillig, T.: Explain: a tool for performing abductive inference. In: Shary-
gina, N., Veith, H. (eds.) CAV 2013. LNCS, vol. 8044, pp. 684–689. Springer,
Heidelberg (2013). https://doi.org/10.1007/978-3-642-39799-8 46
16. Dillig, I., Dillig, T., Aiken, A.: Automated error diagnosis using abductive infer-
ence. ACM SIGPLAN Not. 47(6), 181–192 (2012)
17. Dillig, I., Dillig, T., Li, B., McMillan, K.: Inductive invariant generation via abduc-
tive inference. ACM SIGPLAN Not. 48(10), 443–456 (2013)
18. Dillig, T., Dillig, I., Chaudhuri, S.: Optimal guard synthesis for memory safety. In:
Biere, A., Bloem, R. (eds.) CAV 2014. LNCS, vol. 8559, pp. 491–507. Springer,
Cham (2014). https://doi.org/10.1007/978-3-319-08867-9 32
19. Echenim, M., Peltier, N., Sellami, Y.: A generic framework for implicate generation
modulo theories. In: Galmiche, D., Schulz, S., Sebastiani, R. (eds.) IJCAR 2018.
LNCS (LNAI), vol. 10900, pp. 279–294. Springer, Cham (2018). https://doi.org/
10.1007/978-3-319-94205-6 19
20. Echenim, M., Peltier, N., Sellami, Y.: Ilinva: using abduction to generate loop
invariants. In: Herzig, A., Popescu, A. (eds.) FroCoS 2019. LNCS (LNAI), vol.
11715, pp. 77–93. Springer, Cham (2019). https://doi.org/10.1007/978-3-030-
29007-8 5
21. Enderton, H.B.: A Mathematical Introduction to Logic, 2nd edn. Academic Press,
Cambridge (2001)
22. Gulwani, S.: Programming by examples: applications, algorithms, and ambiguity
resolution. In: Olivetti, N., Tiwari, A. (eds.) IJCAR 2016. LNCS (LNAI), vol. 9706,
pp. 9–14. Springer, Cham (2016). https://doi.org/10.1007/978-3-319-40229-1 2
23. Ignatiev, A., Morgado, A., Marques-Silva, J.: Propositional abduction with implicit
hitting sets. In: ECAI 2016–22nd European Conference on Artiﬁcial Intelligence,
29 August–2 September 2016, The Hague, The Netherlands - Including Prestigious
Applications of Artiﬁcial Intelligence, PAIS 2016, pp. 1327–1335 (2016)
24. Josephson, J.R., Josephson, S.G. (eds.): Abductive Inference: Computation, Phi-
losophy, Technology. Cambridge University Press, Cambridge (1994)
25. Li, B., Dillig, I., Dillig, T., McMillan, K., Sagiv, M.: Synthesis of circular composi-
tional program proofs via abduction. In: Piterman, N., Smolka, S.A. (eds.) TACAS
2013. LNCS, vol. 7795, pp. 370–384. Springer, Heidelberg (2013). https://doi.org/
10.1007/978-3-642-36742-7 26
26. Moreno-Centeno, E., Karp, R.M.: The implicit hitting set approach to solve com-
binatorial optimization problems with an application to multigenome alignment.
Oper. Res. 61(2), 453–468 (2013)

160
A. Reynolds et al.
27. Neider, D., Saha, S., Madhusudan, P.: Synthesizing piece-wise functions by learning
classiﬁers. In: Chechik, M., Raskin, J.-F. (eds.) TACAS 2016. LNCS, vol. 9636, pp.
186–203. Springer, Heidelberg (2016). https://doi.org/10.1007/978-3-662-49674-
9 11
28. N¨otzli, A., et al.: Syntax-guided rewrite rule enumeration for SMT solvers. In:
Janota, M., Lynce, I. (eds.) SAT 2019. LNCS, vol. 11628, pp. 279–297. Springer,
Cham (2019). https://doi.org/10.1007/978-3-030-24258-9 20
29. Phothilimthana, P.M., Thakur, A., Bod´ık, R., Dhurjati, D.: Scaling up superopti-
mization. In: Proceedings of the Twenty-First International Conference on Archi-
tectural Support for Programming Languages and Operating Systems, ASPLOS
2016, Atlanta, GA, USA, 2–6 April 2016, pp. 297–310 (2016)
30. Raghothaman, M., Reynolds, A., Udupa, A.: The SyGuS language standard version
2.0 (2019)
31. Reynolds, A., Barbosa, H., N¨otzli, A., Barrett, C., Tinelli, C.: cvc4sy: smart and
fast term enumeration for syntax-guided synthesis. In: Dillig, I., Tasiran, S. (eds.)
CAV 2019. LNCS, vol. 11562, pp. 74–83. Springer, Cham (2019). https://doi.org/
10.1007/978-3-030-25543-5 5
32. Reynolds, A., Deters, M., Kuncak, V., Tinelli, C., Barrett, C.: Counterexample-
guided quantiﬁer instantiation for synthesis in SMT. In: Kroening, D., P˘as˘areanu,
C.S. (eds.) CAV 2015. LNCS, vol. 9207, pp. 198–216. Springer, Cham (2015).
https://doi.org/10.1007/978-3-319-21668-3 12
33. Saikko, P., Wallner, J.P., J¨arvisalo, M.: Implicit hitting set algorithms for reasoning
beyond NP. In: Principles of Knowledge Representation and Reasoning: Proceed-
ings of the Fifteenth International Conference, KR 2016, Cape Town, South Africa,
25–29 April 2016, pp. 104–113 (2016)
34. Solar-Lezama, A., Rabbah, R.M., Bod´ık, R., Ebcioglu, K.: Programming by sketch-
ing for bit-streaming programs. In: Sarkar, V., Hall, M.W. (eds.) Conference on
Programming Language Design and Implementation (PLDI), pp. 281–294. ACM
(2005)
35. Solar-Lezama, A., Tancau, L., Bod´ık, R., Seshia, S.A., Saraswat, V.A.: Combinato-
rial sketching for ﬁnite programs. In: Shen, J.P., Martonosi, M. (eds.) Architectural
Support for Programming Languages and Operating Systems (ASPLOS), pp. 404–
415. ACM (2006)
36. Udupa, A., Raghavan, A., Deshmukh, J.V., Mador-Haim, S., Martin, M.M.K.,
Alur, R.: TRANSIT: specifying protocols with concolic snippets. In: ACM SIG-
PLAN Conference on Programming Language Design and Implementation, PLDI
2013, Seattle, WA, USA, 16–19 June 2013, pp. 287–296 (2013)
37. Zhu, H., Dillig, T., Dillig, I.: Automated inference of library speciﬁcations for
source-sink property veriﬁcation. In: Shan, C. (ed.) APLAS 2013. LNCS, vol. 8301,
pp. 290–306. Springer, Cham (2013). https://doi.org/10.1007/978-3-319-03542-
0 21

Decision Procedures and Combination of
Theories

Deciding the Word Problem for Ground
Identities with Commutative
and Extensional Symbols
Franz Baader1(B) and Deepak Kapur2
1 Theoretical Computer Science, TU Dresden, Dresden, Germany
franz.baader@tu-dresden.de
2 Department of Computer Science, University of New Mexico, Albuquerque, USA
kapur@cs.unm.edu
Abstract. The word problem for a ﬁnite set of ground identities is
known to be decidable in polynomial time, and this is also the case if
some of the function symbols are assumed to be commutative. We show
that decidability in P is preserved if we also assume that certain function
symbols f are extensional in the sense that f(s1, . . . , sn) ≈f(t1, . . . , tn)
implies s1 ≈t1, . . . , sn ≈tn. In addition, we investigate a variant of
extensionality that is more appropriate for commutative function sym-
bols, but which raises the complexity of the word problem to coNP.
1
Introduction
One motivation for this work stems from Description Logic (DL) [1], where
constant symbols (called individual names) are used within knowledge bases
to denote objects or individuals in an application domain. If such objects are
composed of other objects, it makes sense to represent them as (ground) terms
rather than constants. For example, the couple consisting of individual a in the
ﬁrst component and individual b in the second component is more reasonably
represented by the term f(a, b) (where f is a binary function symbol denoting
the couple constructor) than by a third constant c that is unrelated to a and b.
In fact, if we have two couples, one consisting of a and b and the other of a′ and
b′, and we learn (by DL reasoning or from external sources) that a is equal to
a′ and b is equal to b′, then this automatically implies that f(a, b) is equal to
f(a′, b′), i.e., that this is one and the same couple, whereas we would not obtain
such a consequence if we had introduced constants c and c′ for the two couples.
If we use terms to represent objects, and can learn (e.g., by DL reasoning)
that two terms are supposed to be equal, we need to be able to decide which
other identities between terms can be derived from the given ones. Fortunately,
this problem (usually called the word problem for ground identities) is decid-
able in polynomial time. The standard approach for deciding this word problem
is congruence closure [3,5,10,12]. Basically, congruence closure starts with the
F. Baader—Partially supported by DFG, Grant 389792660, within TRR 248.
c
⃝Springer Nature Switzerland AG 2020
N. Peltier and V. Sofronie-Stokkermans (Eds.): IJCAR 2020, LNAI 12166, pp. 163–180, 2020.
https://doi.org/10.1007/978-3-030-51074-9_10

164
F. Baader and D. Kapur
given set of ground identities E, and then extends it using closure under reﬂexiv-
ity, symmetry, transitivity, and congruence. The set CC(E) obtained this way is
usually inﬁnite, and the main observation that yields decidability in polynomial
time is that one can restrict it to the subterms of E and the subterms of the
terms for which one wants to decide the word problem. An alternative approach
for deciding the word problem for ground identities is based on term rewriting.
Basically, in this approach one generates an appropriate canonical term rewrit-
ing system from E, and then decides whether two terms are equal modulo the
theory E by computing their canonical forms and checking whether they are
syntactically equal. This was implicit in [15], and made explicit in [7] (see also
[6,16] for other rewriting-based approaches).
In the motivating example from DL, but also in other settings where con-
gruence closure is employed (such as SMT [13,17]), it sometimes makes sense to
assume that certain function symbols satisfy additional properties that are not
expressible by ﬁnitely many ground identities. For example, one may want to
considered couples where the order of the components is irrelevant, which means
that the couple constructor function is commutative. Another interesting prop-
erty for (ordered) couples is extensionality: if two couples are equal then they
must have the same ﬁrst and second components, i.e., the couple constructor f
must satisfy the extensionality rule f(x, y) ≈f(x′, y′) ⇒x ≈x′ ∧y ≈y′. While
it is known that adding commutativity does not increase the complexity (see,
e.g., [5,8]), extensionality has, to the best of our knowledge, not been considered
in this context before. The problem with extensionality is that it allows us to
derive “small” identities from larger ones. Consequently, it is conceivable that
one ﬁrst needs to generate such large identities using congruence and applying
other rules, before one can get back to a smaller one through the application
of extensionality. Thus, it is not obvious that also with extensionality one can
restrict congruence closure to a ﬁnite set of terms determined by the input. Here,
we will tackle this problem using a rewriting-based approach. Our proofs imply
that, also with extensional symbols, proofs of identities that detour through
“large” terms can be replaced by proofs using only “small” terms, but it is not
clear how this could be shown directly without the rewriting-based approach.
In the next section, we show how the rewriting-based approach of [7] can
be extended such that it can also handle commutative symbols. In contrast to
approaches that deal with associative-commutative (AC) symbols [4,11] using
rewriting modulo AC, we treat commutativity by introducing an additional
rewrite system consisting of appropriately ordered ground instances of commu-
tativity. This sets the stage for our rewriting-based approach that works in the
presence of commutative symbols and extensional symbols presented in Sect. 4.
In this section, we do not consider symbols f that are both commutative and
extensional since extensionality as deﬁned until now is not appropriate for com-
mutative symbols: for arbitrary terms s, t, commutativity yields f(s, t) ≈f(t, s),
and thus extensionality implies s ≈t, which shows that the equational theory
becomes trivial. In Sect. 5, we introduce the notion of c-extensionality, which is
more appropriate for commutative symbols. Whereas the approaches developed

Ground Plus Commutative Plus Extensional
165
in Sects. 3 and 4 yield polynomial time decision procedures for the word problem,
c-extensionality makes the word problem coNP-complete.
Due to space constraints not all proofs can be given here. Detailed proofs
can be found in [2].
2
Preliminaries on Equational Theories and Term
Rewriting
We assume that the reader is familiar with basic notions and results regarding
equational theories, universal algebra, and term rewriting. Here, we brieﬂy recall
the most important notions and results and refer the readers to [3] for details.
We will keep as close as possible to the notation introduced in [3]. In particular,
we use ≈to denote identities between terms and = to denote syntactic equality.
Terms are built as usual from variables, constants, and function symbols. An
identity is a pair of terms (s, t), which we usually write as s ≈t. A ground term is
a term not containing variables and a ground identity is a pair of ground terms.
Given a set of identities E, the equational theory induced by E is deﬁned (seman-
tically) as ≈E := {s ≈t | every models of E is a model of s ≈t}. The notion of
model used here is the usual one from ﬁrst-order logic, where we assume that
identities are (implicitly) universally quantiﬁed. Since we consider signatures
consisting only of constant and function symbols, we call ﬁrst-order interpreta-
tions algebras.
Birkhoﬀ’s theorem provides us with an alternative characterization of ≈E
that is based on rewriting. A given set of identities E induces a binary relation
→E on terms. Basically, we have s →E t if there is an identity ℓ≈r in E such
that s contains a substitution instance σ(ℓ) of ℓas subterm, and t is obtained
from s by replacing this subterm with σ(r). Birkhoﬀ’s theorem says that ≈E
is identical to
∗↔E, where
∗↔E denotes the reﬂexive, transitive, and symmetric
closure of →E.
If →E is canonical (i.e., terminating and conﬂuent), then we have s
∗↔E t
iﬀs and t have the same canonical forms. The canonical form of a term s is
an irreducible term s such that s
∗→E s, where
∗→E denotes the reﬂexive and
transitive closure of →E and s is irreducible if there is no s′ with s →E s′.
Termination ensures that the canonical form exists and conﬂuence that it is
unique. The relation →E is conﬂuent if s
∗→E t1 and s
∗→E t2 imply that there
is a term t such that t1
∗→E t and t2
∗→E t. It is terminating if there is no inﬁnite
chain t0 →E t1 →E t2 →E · · · .
Termination can be proved using a so-called reduction order, which is a well-
founded order > on terms such that ℓ> r for all ℓ≈r ∈E implies s > t for all
terms s, t with s →E t. Since > is well-founded this then implies termination. If
→E is terminating, then conﬂuence can be tested by checking whether all critical
pairs of E are joinable. Basically, critical pairs (t1, t2) consider the most general
forks of the form s →E t1 and s →E t2 that are due to overlapping left-hand
sides of identities. Such a pair is joinable if there is a term t such that t1
∗→E t
and t2
∗→E t.

166
F. Baader and D. Kapur
Usually, when considering the relation →E, one calls E a term rewriting
system rather than a set of identities, and writes its elements (called rewrite
rules) as ℓ→r rather than ℓ≈r. From a formal point of view, however, both
rewrite rules and identities are pairs of terms. Given a set of such pairs, we can
view it as a set of identities or a term rewriting system, and thus the notions
introduced above apply to both.
3
Commutative Congruence Closure Based on Rewriting
Let Σ be a ﬁnite set of function symbols of arity ≥1 and C0 a ﬁnite set of
constant symbols. We denote the set of ground terms built using symbols from
Σ and C0 with G(Σ, C0). In the following, let E be a ﬁnite set of identities
s ≈t between terms s, t ∈G(Σ, C0), and ≈E the equational theory induced
by E on G(Σ, C0), deﬁned either semantically using algebras or (equivalently)
syntactically through rewriting [3].
It is well-known (see, e.g., [3], Lemma 4.3.3) that ≈E (viewed as a subset
of G(Σ, C0) × G(Σ, C0)) can be generated using congruence closure, i.e., by
exhaustively applying reﬂexivity, transitivity, symmetry, and congruence to E.
To be more precise, CC(E) is the smallest subset of G(Σ, C0) × G(Σ, C0) that
contains E and is closed under the following rules:
– if s ∈G(Σ, C0), then s ≈s ∈CC(E) (reﬂexivity);
– if s1 ≈s2, s2 ≈s3 ∈CC(E), then s1 ≈s3 ∈CC(E) (transitivity);
– if s1 ≈s2 ∈CC(E), then s2 ≈s1 ∈CC(E) (symmetry);
– if f is an n-ary function symbol and s1 ≈t1, . . . , sn ≈tn ∈CC(E), then
f(s1, . . . , sn) ≈f(t1, . . . , tn) ∈CC(E) (congruence).
The set CC(E) is usually inﬁnite. To obtain a decision procedure for the word
problem, one can show that it is suﬃcient to restrict the application of the
above rules to a ﬁnite subset of G(Σ, C0), which consists of the subterms of
terms occurring in E and of the subterms of the terms s0, t0 for which one wants
to decide whether s0 ≈E t0 holds or not (see, e.g., [3], Theorem 4.3.5).
This actually also works if one adds commutativity of some binary function
symbols to the theory. To be more precise, we assume that some of the binary
function symbols in Σ are commutative, i.e., there is a set of binary function
symbols Σc ⊆Σ whose elements we call commutative symbols. In addition to
the identities in E, we assume that the identities f(x, y) ≈f(y, x) are satisﬁed
for all function symbols f ∈Σc. From a semantic point of view, this means
that we consider algebras A that satisfy not only the identities in E, but also
commutativity for the symbols in Σc, i.e., for all f ∈Σc, and all elements a, b
of A we have that f A(a, b) = f A(b, a). Given s, t ∈G(Σ, C0), we say that s ≈t
follows from E w.r.t. the commutative symbols in Σc (written s ≈Σc
E t) if sA = tA
holds in all algebras that satisfy the identities in E and commutativity for the
symbols in Σc. The relation ≈Σc
E ⊆G(Σ, C0) × G(Σ, C0) can also be generated
by extending congruence closure by a commutativity rule.

Ground Plus Commutative Plus Extensional
167
To be more precise, CC Σc(E) is the smallest subset of G(Σ, C0) × G(Σ, C0)
that contains E and is closed under reﬂexivity, transitivity, symmetry, congru-
ence, and the following commutativity rule:
– if f ∈Σc and s, t ∈G(Σ, C0), then f(s, t) ≈f(t, s) ∈CC Σc(E)
(commutativity).
We call CC Σc(E) the commutative congruence closure of E. Using Birkhoﬀ’s
theorem, it is easy to see that CC Σc(E) coincides with ≈Σc
E
in the sense that
s ≈t ∈CC Σc(E) iﬀs ≈Σc
E
t (see Lemma 3.5.13 and Theorem 3.5.14 in [3]).
Again, it is not hard to show that the restriction of the commutative congruence
closure to a polynomially large set of terms determined by the input E, s0, t0 is
complete, which yields decidability of ≈Σc
E [5].
Here, we follow a diﬀerent approach, which is based on rewriting [7,8]. Let
S(E) denote the set of subterms of the terms occurring in E. In a ﬁrst step, we
introduce a new constant cs for every term s ∈S(E) \ C0. To simplify notation,
for a constant a ∈C0 we sometimes use ca to denote a. Let C1 be the set of new
constants introduced this way and C := C0 ∪C1. Given a term u ∈G(Σ, C), we
denote with u the term in G(Σ, C0) obtained from u by replacing the occurrences
of the constants cs ∈C1 in u with the corresponding terms s ∈S(E).
We ﬁx an arbitrary linear order > on C, which will be used to orient identities
between constants into rewrite rules. Note that this order does not take into
account which terms the constants correspond to, and thus we may well have
cs > cf(s).
The initial rewrite system R(E) induced by E consists of the following rules:
– If s ∈S(E) \ C0, then s is of the form f(s1, . . . , sn) for an n-ary function
symbol f and terms s1, . . . , sn for some n ≥1. For every such s we add the
rule f(cs1, . . . , csn) →cs to R(E).
– For every identity s ≈t ∈E we add cs →ct to R(E) if cs > ct, and ct →cs
if ct > cs.
Obviously, the cardinality of C1 is linear in the size of E, and R(E) can be
constructed in time linear in the size of E. From the above construction, it
follows that R(E) has two types of rules: constant rules of the form c →d for
c > d and function rules of the form f(c1, . . . , cn) →d.
Example 1. Consider E = {f(a, g(a)) ≈c, g(b) ≈h(a), a ≈b} with Σc = {f}.
It is easy to see that we have f(h(a), b) ≈Σc
E
c. Using our construction, we
ﬁrst introduce the new constants C1 = {cf(a,g(a)), cg(a), cg(b), ch(a)}. If we ﬁx the
linear order on C as cf(a,g(a)) > cg(a) > cg(b) > ch(a) > a > b > c, then we
obtain the following rewrite system: R(E) = {f(a, cg(a)) →cf(a,g(a)), g(a) →
cg(a), g(b) →cg(b), h(a) →ch(a), cf(a,g(a)) →c, cg(b) →ch(a), a →b}.
The following lemma is an easy consequence of the deﬁnition of R(E). The
ﬁrst part can be shown by a simple induction on the structure of s.
Lemma 1. For all terms s ∈S(E) we have s ≈R(E) cs. Consequently, u ≈R(E)
u and thus also u ≈Σc
R(E) u for all terms u ∈G(Σ, C).

168
F. Baader and D. Kapur
Using this lemma, we can show that the construction of R(E) is correct for
consequence w.r.t. commutative symbols in the following sense:
Lemma 2. Viewed as a set of identities, R(E) is a conservative extension of
E w.r.t. the commutative symbols in Σc, i.e., for all terms s0, t0 ∈G(Σ, C0) we
have s0 ≈Σc
E t0 iﬀs0 ≈Σc
R(E) t0.
In this lemma, we use commutativity of the elements of Σc as additional
identities. Our goal is, however, to deal both with the ground identities in E
and with commutativity by rewriting. For this reason, we consider the rewrite
system1
R(Σc) := {f(s, t) →f(t, s) | f ∈Σc, s, t ∈G(Σ, C), and s >lpo t},
(1)
where >lpo denotes the lexicographic path order (see Deﬁnition 5.4.12 in [3])
induced by a linear order on Σ ∪C that extends > on C, makes each function
symbol in Σ greater than each constant symbol in C, and linearly orders the
function symbols in an arbitrary way. Note that >lpo is then a linear order on
G(Σ, C) (see Exercise 5.20 in [3]). Consequently, for every pair of distinct terms
s, t ∈G(Σ, C), we have f(s, t) →f(t, s) ∈R(Σc) or f(t, s) →f(s, t) ∈R(Σc).
The term rewriting system R(E) ∪R(Σc) can easily be shown to terminate
using this order. In fact, >lpo is a reduction order, and we have ℓ>lpo r for
all rules ℓ→r ∈R(E) ∪R(Σc). However, in general R(E) ∪R(Σc) need not
be conﬂuent. For instance, in Example 1 we have the two rewrite sequences
g(a) →g(b) →cg(b) →ch(a) and g(a) →cg(a) w.r.t. R(E) ∪R(Σc), and the two
constants ch(a) and cg(a) are irreducible w.r.t. R(E) ∪R(Σc), but not equal.
We turn R(E)∪R(Σc) into a conﬂuent and terminating system by modifying
R(E) appropriately. We start with RΣc
0 (E) := R(E) and i := 0:
(a) Let RΣc
i (E)|con consist of the constant rules in RΣc
i (E). For every constant
c ∈C, consider
[c]i := {d ∈C | c ≈RΣc
i
(E)|con d},
and let e be the least element in [c]i w.r.t. the order >. We call e the
representative of c w.r.t. RΣc
i (E) and >. If c ̸= e, then add c →e to
RΣc
i+1(E).
(b) In all function rules in RΣc
i (E), replace each constant by its representative
w.r.t. RΣc
i (E) and >, and call the resulting set of function rules F Σc
i
(E).
Then, we distinguish two cases, depending on whether the function symbol
occurring in the rule is commutative or not.
(b1) Let f be an n-ary function symbol not belonging to Σc. For every term
f(c1, . . . , cn) occurring as the left-hand side of a rule in F Σc
i
(E), consider
all the rules f(c1, . . . , cn) →d1, . . . , f(c1, . . . , cn) →dk in F Σc
i
(E) with
this left-hand side. Let d be the least element w.r.t. > in {d1, . . . , dk}.
Add f(c1, . . . , cn) →d and dj →d for all j with dj ̸= d to RΣc
i+1(E).
1 Since this system is in general inﬁnite, we do not generate it explicitly. But we can
nevertheless apply the appropriate rule when encountering a commutative symbol
during rewriting by just ordering its arguments according to >lpo.

Ground Plus Commutative Plus Extensional
169
(b2) Let f be a binary function symbol belonging to Σc. For all pairs of con-
stant symbols c1, c2 such that f(c1, c2) or f(c2, c1) is the left-hand side
of a rule in F Σc
i
(E), consider the set of constant symbols {d1, . . . , dk}
occurring as right-hand sides of such rules, and let d be the least element
w.r.t. > in this set. Add dj →d for all j with dj ̸= d to RΣc
i+1(E). In
addition, if c2 >lpo c1, then add f(c1, c2) →d to RΣc
i+1(E), and otherwise
f(c2, c1) →d.
If at least one constant rule has been added in this step, then set i := i + 1
and continue with step (a). Otherwise, terminate with output RΣc(E) :=
RΣc
i+1(E).
Let us illustrate the construction of RΣc(E) using Example 1. In step (a),
the non-trivial equivalence classes are [a]0 = {a, b} with representative b,
[cf(a,g(a))] = {cf(a,g(a)), c} with representative c, and [cg(b)] = {cg(b), ch(a)} with
representative ch(a). Thus, a →b, cf(a,g(a)) →c, cg(b) →ch(a) are the constant
rule added to RΣc
1 (E). The function rules in F Σc
0 (E) are then f(b, cg(a)) →
c, g(b) →cg(a), g(b) →ch(a), h(b) →ch(a). For the two rules with left-hand side
g(b), we add cg(a) →ch(a) and g(b) →ch(a) to RΣc
1 (E). The rules with left-hand
sides diﬀerent from g(b) are moved unchanged from F Σc
0 (E) to RΣc
1 (E) since
their left-hand sides are unique. Thus, RΣc
1 (E) = {a →b, cf(a,g(a)) →c, cg(b) →
ch(a), cg(a) →ch(a), f(b, cg(a)) →c, g(b) →ch(a), h(b) →ch(a)}.
In the second iteration step, we now have the new non-trivial equivalence
class [cg(b)]1 = {cg(b), ch(a), cg(a)} with representative ch(a). The net eﬀect of
step (a) is, however, that the constant rules are moved unchanged from RΣc
1 (E)
to RΣc
2 (E). The function rules in F Σc
1 (E) are then f(b, ch(a)) →c, g(b) →
ch(a), h(b) →ch(a). Consequently, no constant rules are added in step (b), and the
construction terminates with output RΣc(E) = {a →b, cf(a,g(a)) →c, cg(b) →
ch(a), cg(a) →ch(a), f(b, ch(a)) →c, g(b) →ch(a), h(b) →ch(a)}.
Our goal is now to show that RΣc(E)∪R(Σc) provides us with a polynomial-
time decision procedure for the commutative word problem in E.
Lemma 3. The system RΣc(E) can be computed from R(E) in polynomial time,
and its construction is correct in the following sense: viewed as a set of identities,
RΣc(E) ∪R(Σc) is equivalent to R(E) with commutativity, i.e., for all terms
s, t ∈G(Σ, C) we have s ≈Σc
R(E) t iﬀs ≈
RΣc(E)∪R(Σc) t.
If we view RΣc(E) ∪R(Σc) as a term rewriting system, then we obtain the
following result.
Lemma 4. RΣc(E) ∪R(Σc) is canonical, i.e., terminating and conﬂuent.
Proof. Termination of the term rewriting system RΣc(E) ∪R(Σc) can be shown
as for R(E) ∪R(Σc), using the reduction order >lpo introduced in the deﬁnition
of R(Σc). Conﬂuence can thus be proved by showing that all non-trivial critical
pairs of this system can be joined (see [2] for details).
⊓⊔

170
F. Baader and D. Kapur
Since RΣc(E) ∪R(Σc) is canonical, each term s ∈G(Σ, C) has a unique
normal form (i.e., irreducible term reachable from s) w.r.t. RΣc(E) ∪R(Σc),
which we call the canonical form of s. We can thus use the system RΣc(E) ∪
R(Σc) to decide whether terms s, t are equivalent w.r.t. E and commutativity of
the symbols in Σc, i.e., whether s ≈t ∈CC Σc(E), by computing the canonical
forms of the terms s and t.
Theorem 1. Let s0, t0 ∈G(Σ, C0). Then we have s0 ≈t0 ∈CC Σc(E) iﬀs0
and t0 have the same canonical form w.r.t. RΣc(E) ∪R(Σc).
Consider the rewrite system RΣc(E) that we have computed (above Lemma 3)
from the set of ground identities E in Example 1, and recall that f(h(a), b) ≈Σc
E c.
The canonical form of c is clearly c, and the canonical form of f(h(a), b) can be
computed by the following rewrite sequence:
f(h(a), b) →R(Σc) f(b, h(a)) →
RΣc(E) f(b, h(b)) →
RΣc(E) f(b, ch(a)) →
RΣc(E) c.
Note that the construction of RΣc(E) is actually independent of the terms
s0, t0 for which we want to decide the word problem in E. This is in contrast
to approaches that restrict the construction of the congruence closure to the
subterms of E and the subterms of the terms s0, t0 for which one wants to
decide the word problem. This fact will turn out to be useful in the next section.
Since it is easy to show that reduction to canonical forms requires only a poly-
nomial number of rewrite steps, Theorem 1 thus yields the following complexity
result.
Corollary 1. The commutative word problem for ﬁnite sets of ground identities
is decidable in polynomial time, i.e., given a ﬁnite set of ground identities E ⊆
G(Σ, C0) × G(Σ, C0), a set Σc ⊆Σ of commutative symbols, and terms s0, t0 ∈
G(Σ, C0), we can decide in polynomial time whether s0 ≈Σc
E t0 holds or not.
This complexity result has been shown before in [5] and [8], but note that,
in these papers, detailed proofs are given for the case without commutativity,
and then it is only sketched how the respective approach can be extended to
accommodate commutativity. Like the approach in this paper, the one employed
in [8] is rewriting-based, but in contrast to ours it does not explicitly use the
rewrite system R(Σc).
4
Commutative Congruence Closure with Extensionality
Here, we additionally assume that some of the non-commutative2 function sym-
bols are extensional, i.e., there is a set of function symbols Σe ⊆Σ \ Σc whose
elements we call extensional symbols. In addition to the identities in E and
2 We will explain in the next section why the notion of extensionality introduced in
(2) below is not appropriate for commutative symbols.

Ground Plus Commutative Plus Extensional
171
commutativity for the symbols in Σc, we now assume that also the following
conditional identities are satisﬁed for every n-ary function symbol f ∈Σe:
f(x1, . . . , xn) ≈f(y1, . . . , yn) ⇒xi ≈yi for all i, 1 ≤i ≤n.
(2)
From a semantic point of view, this means that we now consider algebras A
that satisfy not only the identities in E and commutativity for the symbols in
Σc, but also extensionality for the symbols in Σe, i.e., for all f ∈Σe, all i, 1 ≤
i ≤n, and all elements a1, . . . , an, b1, . . . , bn of A we have that f A(a1, . . . , an) =
f A(b1, . . . , bn) implies ai = bi for all i, 1 ≤i ≤n. Let Σe
c = (Σc, Σe) and
s, t ∈G(Σ, C0). We say that s ≈t follows from E w.r.t. the commutative symbols
in Σc and the extensional symbols in Σe (written s ≈Σe
c
E
t) if sA = tA holds in
all algebras that satisfy the identities in E, commutativity for the symbols in
Σc, and extensionality for the symbols in Σe.
The relation ≈Σe
c
E
⊆G(Σ, C0) × G(Σ, C0) can also be generated using the
following extension of congruence closure by an extensionality rule. To be more
precise, CC Σe
c (E) is the smallest subset of G(Σ, C0) × G(Σ, C0) that contains
E and is closed under reﬂexivity, transitivity, symmetry, congruence, commuta-
tivity, and the following extensionality rule:
– if f ∈Σe is an n-ary function symbol, 1 ≤i ≤n, and f(s1, . . . , sn) ≈
f(t1, . . . , tn) ∈CC Σe
c (E), then si ≈ti ∈CC Σe
c (E) (extensionality).
Proposition 1. For all terms s, t ∈G(Σ, C0) we have s ≈Σe
c
E
t iﬀs ≈t ∈
CC Σe
c (E).
Proof. This proposition is an easy consequence of Theorem 54 in [18], which
(adapted to our setting) says that ≈Σe
c
E
is the least congruence containing E
that is invariant under applying commutativity and extensionality. Clearly, this
is exactly CC Σe
c (E).
⊓⊔
To obtain a decision procedure for ≈Σe
c
E , we extend the rewriting-based app-
roach from the previous section. Let the term rewriting system R(E) be deﬁned
as in Sect. 3.
Example 2. Consider E′ = {f(a, g(a)) ≈c, g(b) ≈h(a), g(a) ≈g(b)} with
Σc = {f} and Σe = {g}. It is easy to see that we have f(h(a), b) ≈Σe
c
E′ c.
Let the set C1 of new constants and the linear order on all constants be
deﬁned as in Example 1. Now, we obtain the following rewrite system: R(E′) =
{f(a, cg(a)) →cf(a,g(a)), g(a) →cg(a), g(b) →cg(b), h(a) →ch(a), cf(a,g(a)) →
c, cg(b) →ch(a), cg(a) →cg(b)}.
Lemma 5. The system R(E) is a conservative extension of E also w.r.t. the
commutative symbols in Σc and the extensional symbols in Σe, i.e., for all terms
s0, t0 ∈G(Σ, C0) we have s0 ≈Σe
c
E t0 iﬀs0 ≈Σe
c
R(E) t0.

172
F. Baader and D. Kapur
We extend the construction of the conﬂuent and terminating rewrite system
corresponding to R(E) by adding a third step that takes care of extensionality.
To be more precise, RΣe
c (E) is constructed by performing the following steps,
starting with RΣe
c
0 (E) := R(E) and i := 0:
(a) Let RΣe
c
i
(E)|con consist of the constant rules in RΣe
c
i
(E). For every constant
c ∈C, consider
[c]i := {d ∈C | c ≈R
Σec
i
(E)|con d},
and let e be the least element in [c]i w.r.t. the order >. We call e the
representative of c w.r.t. RΣe
c
i
(E) and >. If c ̸= e, then add c →e to
RΣe
c
i+1(E).
(b) In all function rules in RΣe
c
i
(E), replace each constant by its representative
w.r.t. RΣe
c
i
(E) and >, and call the resulting set of function rules F Σe
c
i
(E).
Then, we distinguish two cases, depending on whether the function symbol
occurring in the rule is commutative or not.
(b1) Let f be an n-ary function symbol not belonging to Σc. For every
term f(c1, . . . , cn) occurring as the left-hand side of a rule in F Σe
c
i
(E),
consider all the rules f(c1, . . . , cn) →d1, . . . , f(c1, . . . , cn) →dk in
F Σe
c
i
(E) with this left-hand side. Let d be the least element w.r.t. >
in {d1, . . . , dk}. Add f(c1, . . . , cn) →d and dj →d for all j with dj ̸= d
to RΣe
c
i+1(E).
(b2) Let f be a binary function symbol belonging to Σc. For all pairs of con-
stant symbols c1, c2 such that f(c1, c2) or f(c2, c1) is the left-hand side
of a rule in F Σe
c
i
(E), consider the set of constant symbols {d1, . . . , dk}
occurring as right-hand sides of such rules, and let d be the least element
w.r.t. > in this set. Add dj →d for all j with dj ̸= d to RΣe
c
i+1(E). In
addition, if c2 >lpo c1, then add f(c1, c2) →d to RΣe
c
i+1(E), and otherwise
f(c2, c1) →d.
If at least one constant rule has been added in this step, then set i := i + 1
and continue with step (a). Otherwise, continue with step (c).
(c) For all f ∈Σe, all pairs of distinct rules f(c1, . . . , cn) →d, f(c′
1, . . . , c′
n) →d
in F Σe
c
i
(E), and all i, 1 ≤i ≤n such that ci ̸= c′
i, add ci →c′
i to RΣe
c
i+1(E) if
ci > c′
i and otherwise add c′
i →ci to RΣe
c
i+1(E). If at least one constant rule
has been added in this step, then set i := i + 1 and continue with step (a).
Otherwise, terminate with output RΣe
c (E) := RΣe
c
i+1(E).
We illustrate the above construction using Example 2. In step (a), the non-
trivial equivalence classes are [cf(a,g(a))] = {cf(a,g(a)), c} with representative c
and [cg(b)] = {cg(a), cg(b), ch(a)} with representative ch(a). Thus, cf(a,g(a)) →
c, cg(a) →ch(a), cg(b) →ch(a) are the constant rules added to RΣe
c
1 (E′). The func-
tion rules in F Σe
c
0
(E′) are then f(a, ch(a)) →c, g(a) →ch(a), g(b) →ch(a), h(a) →
ch(a). Since these rules have unique left-hand sides, no constant rule is added in

Ground Plus Commutative Plus Extensional
173
step (b). Consequently, we proceed with step (c). Since g ∈Σe, the presence
of the rules g(a) →ch(a) and g(b) →ch(a) triggers the addition of a →b to
RΣe
c
1 (E′). The function rules in RΣe
c
1 (E′) are the ones in F Σe
c
0
(E′).
In the second iteration step, we now have the new non-trivial equivalence
class [a]1 = {a, b} with representative b. The net eﬀect of step (a) is, again,
that the constant rules are moved unchanged from RΣe
c
1 (E′) to RΣe
c
2 (E′). The
function rules in F Σe
c
1
(E′) are then f(b, ch(a)) →c, g(b) →ch(a), h(b) →ch(a).
Consequently, no new constant rules are added in steps (b) and (c), and the
construction terminates with output RΣe
c (E′) = {a →b, cf(a,g(a)) →c, cg(a) →
ch(a), cg(b) →ch(a), f(b, ch(a)) →c, g(b) →ch(a), h(b) →ch(a)}, which is identical
to the system RΣc(E) computed for the set of identity E of Example 1.
Our goal is now to show that RΣe
c (E) provides us with a polynomial-time
decision procedure for the extensional word problem in E, i.e., it allows us to
decide the relation ≈Σe
c
E . Let R(Σc) and >lpo be deﬁned as in (1).
Lemma 6. The system RΣe
c (E) can be computed from R(E) in polynomial time.
Viewed as a set of identities, RΣe
c (E) ∪R(Σc) is
– sound for commutative and extensional reasoning, i.e., for all rules s →t in
RΣe
c (E) ∪R(Σc) we have s ≈Σe
c
R(E) t, and
– complete for commutative reasoning, i.e., or all terms s, t ∈G(Σ, C) we have
that s ≈Σc
R(E) t implies s ≈
RΣec (E)∪R(Σc) t.
Lemma 7. Viewed as a term rewriting system, RΣe
c (E) ∪R(Σc) is canonical,
i.e., terminating and conﬂuent.
Intuitively, RΣe
c (E) extends RΣc(E) by additional rules relating constants
that are equated due to extensionality. However, to keep the system conﬂuent,
we need to re-apply the other steps once two constants have been equated.
Lemma 8. If s, t ∈G(Σ, C) have the same canonical forms w.r.t. RΣc(E) ∪
R(Σc), then they also have the same canonical forms w.r.t. RΣe
c (E) ∪R(Σc).
We are now ready to prove our main technical result, from which decidability
of the commutative and extensional word problem immediately follows.
Theorem 2. Let s, t ∈G(Σ, C0). Then we have s ≈t ∈CC Σe
c (E) iﬀs and t
have the same canonical form w.r.t. RΣe
c (E) ∪R(Σc).
Proof. Since the if-direction is easy to show, we concentrate here on the only-
if-direction. If s, t ∈G(Σ, C0) are such that s ≈t ∈CC Σe
c (E), then there is a
sequence of identities s1 ≈t1, s2 ≈t2, . . . , sk ≈tk such that sk = s, tk = t,
and for all i, 1 ≤i ≤k, the identity si ≈ti belongs to E or can be derived
from some of the identities sj ≈tj with j < i by apply reﬂexivity, transitivity,
symmetry, congruence, commutativity, or extensionality. We prove that s and t

174
F. Baader and D. Kapur
have the same canonical form w.r.t. RΣe
c (E)∪R(Σc) by induction on the number
of applications of the extensionality rule used when creating this sequence.
In the base case, no extensionality rule is used, and thus s ≈t ∈CC Σc(E).
By Theorem 1, s and t have the same canonical form w.r.t. RΣc(E) ∪R(Σc),
and thus by Lemma 8 also w.r.t. RΣe
c (E) ∪R(Σc).
In the step case, we consider the last identity sm ≈tm obtained by an
application of the extensionality rule. Then, by induction, we know that, for
each i, 1 ≤i < m, the terms si and ti have the same canonical form w.r.t.
RΣe
c (E) ∪R(Σc).
Now, consider the application of extensionality to an identity sℓ≈tℓ(ℓ< m)
that produced sm ≈tm. Thus, we have sℓ= f(g1, . . . , gn) and tℓ= f(h1, . . . , hn)
for some n-ary function symbol f ∈Σe, and extensionality generates the new
identity gμ ≈hμ for some μ, 1 ≤μ ≤n, such that sm = gμ and tm = hμ. For
ν = 1, . . . , n, let g′
ν be the canonical form of gν w.r.t. RΣe
c (E) ∪R(Σc), and h′
ν
the canonical form of hν w.r.t. RΣe
c (E) ∪R(Σc). We know that the canonical
forms of sℓand tℓw.r.t. RΣe
c (E)∪R(Σc) are identical, and these canonical forms
can be obtained by normalizing f(g′
1, . . . , g′
n) and f(h′
1, . . . , h′
n). Since the rules
of R(Σc) are not applicable to these terms due to the fact that f ̸∈Σc, there
are two possible cases for how the canonical forms of sℓand tℓcan look like:
1. sℓand tℓrespectively have the canonical forms f(g′
1, . . . , g′
n) and f(h′
1, . . . ,
h′
n), and thus the corresponding arguments are syntactically equal, i.e., g′
ν =
h′
ν for ν = 1, . . . , n. In this case, the identity sm ≈tm added by the application
of the extensionality rule satisﬁes sm ≈
RΣec (E)∪R(Σc) tm since we have sm =
gμ ≈
RΣec (E)∪R(Σc) g′
μ = h′
μ ≈
RΣec (E)∪R(Σc) hμ = tm.
2. sℓand tℓreduce to the same constant d. Then RΣe
c (E) must contain the rules
f(g′
1, . . . , g′
n) →d and f(h′
1, . . . , h′
n) →d. By the construction of RΣe
c (E),
we again have that g′
μ = h′
μ, i.e., the two terms are syntactically equal. In
fact, otherwise a new constant rule g′
μ →h′
μ or h′
μ →g′
μ would have been
added, and the construction would not have terminated yet. We thus have
again sm = gμ ≈
RΣec (E)∪R(Σc) g′
μ = h′
μ ≈
RΣec (E)∪R(Σc) hμ = tm.
Summing up, we have seen that we have si ≈
RΣec (E)∪R(Σc) ti for all i, 1 ≤i ≤m.
Since the identities sj ≈tj for m < j ≤k are generated from the identities
si ≈ti for i = 1, . . . , m and E using only reﬂexivity, transitivity, symmetry,
commutativity, and congruence, this implies that also these identities satisfy
sj ≈
RΣec (E)∪R(Σc) tj. In particular, we thus have sk ≈
RΣec (E)∪R(Σc) tk. Since
RΣe
c (E) ∪R(Σc) is canonical, this implies that sk = s and tk = t have the same
canonical form w.r.t. RΣe
c (E) ∪R(Σc).
⊓⊔
Recall that we have f(h(a), b) ≈Σe
c
E′ c for the set of identities E′ of Example 2.
We have already seen that these two terms rewrite to the same canonical form
w.r.t. RΣc(E) ∪R(Σc) = RΣe
c (E′) ∪R(Σc).
Again, it is easy to show that the decision procedure obtained by applying
Theorem 2 requires only polynomial time.

Ground Plus Commutative Plus Extensional
175
Corollary 2. The commutative and extensional word problem for ﬁnite sets of
ground identities is decidable in polynomial time, i.e., given a ﬁnite set of ground
identities E ⊆G(Σ, C0)×G(Σ, C0), ﬁnite sets Σc ⊆Σ of commutative and Σe ⊆
Σ \ Σc of non-commutative extensional symbols, and terms s0, t0 ∈G(Σ, C0),
we can decide in polynomial time whether s0 ≈Σe
c
E t0 holds or not.
We have mentioned in the introduction that it is unclear how this polynomi-
ality result could be obtained by a simple adaptation of the usual approach that
restricts congruence closure to a polynomially large set of subterms determined
by the input (informally called “small” terms in the following). The main prob-
lem is that one might have to generate identities between “large” terms before
one can get back to a desired identity between “small” terms using extensionality.
The question is now where our rewriting-based approach actually deals with this
problem. The answer is: in Case 1 of the case distinction in the proof of Theorem
2. In fact, there we consider a derived identity sℓ≈tℓsuch that the (syntactically
identical) canonical forms of sℓ= f(g1, . . . , gn) and tℓ= f(h1, . . . , hn) are not a
constant from C, but of the form f(g′
1, . . . , g′
n) = f(h′
1, . . . , h′
n). Basically, this
means that sℓand tℓare terms that are not equivalent modulo E to subterms of
terms occurring in E, since the latter terms have a constant representing them.
Thus, sℓ, tℓare “large” terms that potentially could cause a problem: an identity
between them has been derived, and now extensionality applied to this identity
yields a new identity gμ ≈hμ between smaller terms. Our induction proof shows
that this identity can nevertheless be derived from RΣe
c (E) ∪R(Σc), and thus
does not cause a problem.
5
Symbols that Are Commutative and Extensional
In the previous section, we have made the assumptions that the sets Σc and Σe
are disjoint, i.e., we did not consider extensionality for commutative symbols.
The reason is that the presence of a commutative and extensional symbol would
trivialize the equational theory. In fact, as already mentioned in the introduction,
if f is assumed to be commutative and extensional, then commutativity yields
f(s, t) ≈f(t, s) for all terms s, t ∈G(Σ, C0), and extensionality then s ≈t. This
shows that, in this case, the commutative and extensional congruence closure
would be G(Σ, C0) × G(Σ, C0), independently of E, and thus even for E = ∅.
In this section, we consider the following variant of extensionality for com-
mutative function symbols f, which we call c-extensionality:
f(x1, x2) ≈f(y1, y2) ⇒(x1 ≈y1 ∧x2 ≈y2) ∨(x1 ≈y2 ∧x2 ≈y1).
(3)
For example, if f is a commutative couple constructor, and two couples turn out
to be equal, then we want to infer that they consist of the same two persons,
independently of the order in which they were put into the constructor.
Unfortunately, adding such a rule makes the word problem coNP-hard, which
can be shown by a reduction from validity of propositional formulae.

176
F. Baader and D. Kapur
Proposition 2. In the presence of at least one commutative and c-extensional
symbol, the word problem for ﬁnite sets of ground identities is coNP-hard.
We prove this proposition by a reduction from validity of propositional for-
mulae. Thus, consider a propositional formula φ, and let p1, . . . , pn be the propo-
sitional variables occurring in φ. We take the constants 0 and 1, and for every
i, 1 ≤i ≤n, we view pi as a constant symbol, and add a second constant sym-
bol pi. In addition, we consider the function symbols f∨, f∧, f¬, f, and assume
that f is commutative and satisﬁes (3). We then consider ground identities that
axiomatize the truth tables for ∨, ∧, ¬, i.e.,
f∨(0, 0) ≈0, f∨(1, 0) ≈1, f∨(0, 1) ≈1, f∨(1, 1) ≈1,
f∧(0, 0) ≈0, f∧(1, 0) ≈0, f∧(0, 1) ≈0, f∧(1, 1) ≈1,
f¬(0) ≈1,
f¬(1) ≈0.
(4)
In addition, we consider, for every i, 1 ≤i ≤n, the identities f(pi, pi) ≈f(0, 1).
Let Eφ be the set of these ground identities, and let tφ be the term obtained
from φ by replacing the Boolean operations ∨, ∧, and ¬ by the corresponding
function symbols f∨, f∧, and f¬.
Proposition 2 is now an immediate consequence of the following lemma.
Lemma 9. The identity tφ ≈1 holds in every algebra satisfying Eφ together
with (3) and commutativity of f iﬀφ is valid.
To prove a complexity upper bound that matches the lower bound stated in
Proposition 2, we consider a ﬁnite signature Σ, a ﬁnite set of ground identities
E ⊆G(Σ, C0) × G(Σ, C0) as well as sets Σc ⊆Σ and Σe ⊆Σ of commutative
and extensional symbols, respectively, and assume that the non-commutative
extensional symbols in Σe \ Σc satisfy extensionality (2), whereas the commuta-
tive extensional symbols in Σe∩Σc satisfy c-extensionality (3). We want to show
that, in this setting, the problem of deciding, for given terms s0, t0 ∈G(Σ, C0),
whether s0 is not equivalent to t0 is in NP.
For this purpose, we employ a nondeterministic variant of our construction of
RΣe
c (E). In steps (a) and (b), this procedure works as described in the previous
section. For extensional symbols f ∈Σe \Σc, step (c) is also performed as in the
previous section. For an extensional symbol f ∈Σe ∩Σc, step (c) is modiﬁed
as follows: for all pairs of distinct rules f(c1, c2) →d, f(c′
1, c′
2) →d in F Σe
c
i
(E),
nondeterministically choose whether
– c1 and c′
1 as well as c2 and c′
2 are to be identiﬁed, or
– c1 and c′
2 as well as c2 and c′
1 are to be identiﬁed,
and then add the corresponding constant rules to RΣe
c
i+1(E) unless the respective
constants are already syntactically equal.
This nondeterministic algorithm has diﬀerent runs, depending on the choices
made in the nondeterministic part of step (c). But each run r produces a rewrite
system RΣe
c
r (E).

Ground Plus Commutative Plus Extensional
177
Example 3. We illustrate the nondeterministic construction using the identities
Eφ for φ = p∨¬p from our coNP-hardness proof. Then Eφ consists of the identi-
ties in (4) together with the identity f(p, p) ≈f(0, 1). Assuming an appropriate
order on the constants, the system R(Eφ) contains, among others, the rules
f∨(1, 0) →cf∨(1,0), cf∨(1,0) →1,
f∨(0, 1) →cf∨(0,1), cf∨(0,1) →1
f¬(0) →cf¬(0),
cf¬(0) →1,
f¬(1) →cf¬(1),
cf¬(1) →0
f(p, p) →cf(p,p),
f(1, 0) →cf(1,0), cf(p,p) →cf(1,0).
In step (a) and (b) of the construction, these rules are transformed into the form
f∨(1, 0) →1,
cf∨(1,0) →1,
f∨(0, 1) →1,
cf∨(0,1) →1
f¬(0) →1,
cf¬(0) →1,
f¬(1) →0,
cf¬(1) →0
f(p, p) →cf(1,0), f(1, 0) →cf(1,0), cf(p,p) →cf(1,0).
(5)
Since no new constant rule is added, the construction proceeds with step (c).
Due to the presence of the rules f(p, p) →cf(1,0) and f(1, 0) →cf(1,0) for
f ∈Σc ∩Σe, it now nondeterministically chooses between identifying p with 1
or with 0. In the ﬁrst case, the constant rules p →1, p →0 are added, and in
the second p →0, p →1 are added. In the next iteration, no new constant rules
are added, and thus the construction terminates. It has two runs r1 and r2. The
generated rewrite systems RΣe
c
r1 (E) and RΣe
c
r2 (E) share the rules in (5), but the
ﬁrst contains p →1 whereas the second contains p →0.
Coming back to the general case, as in the proofs of Lemma 6 and Lemma 7,
we can show the following for the rewrite systems RΣe
c
r (E).
Lemma 10. For every run r, the term rewriting system RΣe
c
r (E) is produced in
polynomial time, and the system RΣe
c
r (E) ∪R(Σc) is canonical.
Using the canonical rewrite systems RΣe
c
r (E) ∪R(Σc), we can now charac-
terize when an identity follows from E w.r.t. commutativity of the symbols in
Σc, extensionality of the symbols in Σe \Σc, and c-extensionality of the symbols
in Σe ∩Σc as follows.
Theorem 3. Let s0, t0 ∈G(Σ, C0). The identity s0 ≈t0 holds in every algebra
that satisﬁes E, commutativity for every f ∈Σc, extensionality for every f ∈
Σe \ Σc, and c-extensionality for every f ∈Σe ∩Σc iﬀs0, t0 have the same
canonical forms w.r.t. RΣe
c
r (E) ∪R(Σc) for every run r of the nondeterministic
construction.
The main ideas for how to deal with extensionality and c-extensionality in
the proof of this theorem are very similar to how extensionality was dealt with
in the proof of Theorem 2. As for all the other results stated without proof here,
a detailed proof can be found in [2]. Together with Proposition 2, Theorem 3
yields the following complexity results.

178
F. Baader and D. Kapur
Corollary 3. Consider a ﬁnite set of ground identities E
⊆G(Σ, C0) ×
G(Σ, C0) as well as sets Σc ⊆Σ and Σe ⊆Σ of commutative and extensional
symbols, respectively, and two terms s0, t0 ∈G(Σ, C0). The problem of deciding
whether the identity s0 ≈t0 holds in every algebra that satisﬁes E, commutativ-
ity for every f ∈Σc, extensionality for every f ∈Σe \ Σc, and c-extensionality
for every f ∈Σe ∩Σc is coNP-complete.
Coming back to Example 3, we note that φ = p ∨¬p is valid, and thus (by
Lemma 9), the identity f∨(p, f¬(p)) ≈1 holds in all algebra that satisfy Eφ and
interpret f as a commutative and c-extensional symbol. Using the rewrite system
generated by the run r1, we obtain the following rewrite sequence: f∨(p, f¬(p)) →
f∨(1, f¬(p)) →f∨(1, f¬(1)) →f∨(1, 0) →1. For the run r2, we obtain the
sequence f∨(p, f¬(p)) →f∨(0, f¬(p)) →f∨(0, f¬(0)) →f∨(0, 1) →1. Thus, for
both runs the terms f∨(p, f¬(p)) and 1 have the same canonical form 1.
6
Conclusion
We have shown, using a rewriting-based approach, that adding commutativity
and extensionality of certain function symbols to a ﬁnite set of ground iden-
tities leaves the complexity of the word problem in P. In contrast, adding c-
extensionality for commutative function symbols raises the complexity to coNP.
For classical congruence closure, it is well-known that it can actually be com-
puted in O(n log n) [12,13]. Since this complexity upper bound can also be
achieved using a rewriting-based approach [8,16], we believe that the approach
developed here can also be used to obtain an O(n log n) upper bound for the
word problem for ground identities in the presence of commutativity and exten-
sionality, as in Sect. 4, but this question was not in the focus here.
The rules specifying extensionality are simple kinds of Horn rules whose
atoms are identities. The question arises which other such Horn rules can be
added without increasing the complexity of the word problem. It is known that
allowing for associative-commutative (AC) symbols leaves the word problem for
ﬁnite sets of ground identities decidable [4,11]. It would be interesting to see what
happens if additionally (non-AC) extensional symbols are added. The approaches
employed in [4,11] are rewriting-based, but in contrast to our treatment of com-
mutativity, they use rewriting modulo AC. It is thus not clear whether the app-
roach developed in the present paper can be adapted to deal with AC symbols.
Regarding the application motivation from DL, it should be easy to extend
tableau-based algorithms for DLs to deal with individuals named by ground
terms and identities between these terms. Basically, the tableau algorithm then
works with the canonical forms of such terms, and if it identiﬁes two terms (e.g.,
when applying a tableau-rule dealing with number restrictions), then the rewrite
system and the canonical forms need to be updated. More challenging would be
a setting where rules are added to the knowledge base that generate new terms
if they ﬁnd a certain constellation in the knowledge base (e.g., a married couple,
for which the rule introduces a ground term denoting the couple and assertions

Ground Plus Commutative Plus Extensional
179
that link the couple with its components). In the context of ﬁrst-order logic and
modal logics, the combination of tableau-based reasoning and congruence closure
has respectively been investigated in [9] and [14].
Acknowledgements. The authors would like to thank the reviewers for their careful
reading of the paper and their useful comments, which helped to improve the presen-
tation of the paper. The ﬁrst author thanks Barbara Morawska for helpful discussions.
References
1. Baader, F., et al.: An Introduction to Description Logic. Cambridge University
Press, Cambridge (2017)
2. Baader, F., Kapur, D.: Deciding the word problem for ground identities with com-
mutative and extensional symbols. LTCS-Report 20-02, Chair of Automata The-
ory, Institute of Theoretical Computer Science, Technische Universität Dresden,
Dresden (2020). https://tu-dresden.de/inf/lat/reports#BaKa-LTCS-20-02
3. Baader, F., Nipkow, T.: Term Rewriting and All That. Cambridge University Press,
Cambridge (1998)
4. Bachmair, L., Ramakrishnan, I.V., Tiwari, A., Vigneron, L.: Congruence closure
modulo associativity and commutativity. In: Kirchner, H., Ringeissen, C. (eds.)
FroCoS 2000. LNCS (LNAI), vol. 1794, pp. 245–259. Springer, Heidelberg (2000).
https://doi.org/10.1007/10720084_16
5. Downey, P., Sethi, R., Tarjan, R.E.: Variations on the common subexpression prob-
lem. J. ACM 27(4), 758–771 (1980)
6. Gallier, J.: An algorithm for ﬁnding canonical sets of ground rewrite rules in poly-
nomial time. J. ACM 40(1), 1–16 (1993)
7. Kapur, D.: Shostak’s congruence closure as completion. In: Comon, H. (ed.) RTA
1997. LNCS, vol. 1232, pp. 23–37. Springer, Heidelberg (1997). https://doi.org/10.
1007/3-540-62950-5_59
8. Kapur, D.: Conditional congruence closure over uninterpreted and interpreted
symbols. J. Syst. Sci. Complex. 32(1), 317–355 (2019). https://doi.org/10.1007/
s11424-019-8377-8
9. Käuﬂ, T., Zabel, N.: The theorem prover of the program veriﬁer Tatzelwurm. In:
Stickel, M.E. (ed.) CADE 1990. LNCS, vol. 449, pp. 657–658. Springer, Heidelberg
(1990). https://doi.org/10.1007/3-540-52885-7_128
10. Kozen, D.: Complexity of ﬁnitely presented algebras. In: Proceedings of the 9th
ACM Symposium on Theory of Computing, pp. 164–177. ACM (1977)
11. Narendran, P., Rusinowitch, M.: Any ground associative-commutative theory has
a ﬁnite canonical system. In: Book, R.V. (ed.) RTA 1991. LNCS, vol. 488, pp. 423–
434. Springer, Heidelberg (1991). https://doi.org/10.1007/3-540-53904-2_115
12. Nelson, G., Oppen, D.C.: Fast decision procedures based on congruence closure. J.
ACM 27(4), 356–364 (1980)
13. Nieuwenhuis, R., Oliveras, A.: Fast congruence closure and extensions. Inf. Com-
put. 205(4), 557–580 (2007)
14. Schmidt, R.A., Waldmann, U.: Modal Tableau systems with blocking and congru-
ence closure. In: De Nivelle, H. (ed.) TABLEAUX 2015. LNCS (LNAI), vol. 9323,
pp. 38–53. Springer, Cham (2015). https://doi.org/10.1007/978-3-319-24312-2_4
15. Shostak, R.E.: An algorithm for reasoning about equality. Commun. ACM 21(7),
583–585 (1978)

180
F. Baader and D. Kapur
16. Snyder, W.: A fast algorithm for generating reduced ground rewriting systems from
a set of ground equations. J. Symb. Comput. 15(4), 415–450 (1993)
17. Stump, A., Barrett, C.W., Dill, D.L., Levitt, J.R.: A decision procedure for an
extensional theory of arrays. In: Proceedings of 16th Annual IEEE Symposium
on Logic in Computer Science (LICS 2001), pp. 29–37. IEEE Computer Society
(2001)
18. Wechler, W.: Universal Algebra for Computer Scientists. EATCS Monographs on
Theoretical Computer Science, vol. 25. Springer, Cham (1992). https://doi.org/10.
1007/978-3-642-76771-5

Combined Covers and Beth Deﬁnability
Diego Calvanese1, Silvio Ghilardi2, Alessandro Gianola1,3(B), Marco Montali1,
and Andrey Rivkin1
1 Faculty of Computer Science, Free University of Bozen-Bolzano, Bolzano, Italy
{calvanese,gianola,montali,rivkin}@inf.unibz.it
2 Dipartimento di Matematica, Universit`a degli Studi di Milano, Milan, Italy
silvio.ghilardi@unimi.it
3 CSE Department, University of California San Diego, San Diego, CA, USA
agianola@eng.ucsd.edu
Abstract. Uniform interpolants were largely studied in non-classical
propositional logics since the nineties, and their connection to model
completeness was pointed out in the literature. A successive parallel
research line inside the automated reasoning community investigated uni-
form quantiﬁer-free interpolants (sometimes referred to as “covers”) in
ﬁrst-order theories. In this paper, we investigate cover transfer to theory
combinations in the disjoint signatures case. We prove that, for con-
vex theories, cover algorithms can be transferred to theory combinations
under the same hypothesis needed to transfer quantiﬁer-free interpo-
lation (i.e., the equality interpolating property, aka strong amalgama-
tion property). The key feature of our algorithm relies on the exten-
sive usage of the Beth deﬁnability property for primitive fragments to
convert implicitly deﬁned variables into their explicitly deﬁning terms.
In the non-convex case, we show by a counterexample that cover may
not exist in the combined theories, even in case combined quantiﬁer-free
interpolants do exist.
1
Introduction
Uniform interpolants were originally studied in the context of non-classical logics,
starting from the pioneering work by Pitts [26]. We brieﬂy recall what uniform
interpolants are; we ﬁx a logic or a theory T and a suitable fragment (propo-
sitional, ﬁrst-order quantiﬁer-free, etc.) of its language L. Given an L-formula
φ(x, y) (here x, y are the variables occurring free in φ), a uniform interpolant
of φ (w.r.t. y) is a formula φ′(x) where only the x occur free, and that satisﬁes
the following two properties: (i) φ(x, y) ⊢T φ′(x); (ii) for any further L-formula
ψ(x, z) such that φ(x, y) ⊢T ψ(x, z), we have φ′(x) ⊢T ψ(x, z). Whenever uni-
form interpolants exist, one can compute an interpolant for an entailment like
φ(x, y) ⊢T ψ(x, z) in a way that is independent of ψ.
The existence of uniform interpolants is an exceptional phenomenon, which is
however not so infrequent; it has been extensively studied in non-classical logics
starting from the nineties, as witnessed by a large literature (a non-exhaustive
c
⃝Springer Nature Switzerland AG 2020
N. Peltier and V. Sofronie-Stokkermans (Eds.): IJCAR 2020, LNAI 12166, pp. 181–200, 2020.
https://doi.org/10.1007/978-3-030-51074-9_11

182
D. Calvanese et al.
list includes [1,11,17–19,23,28,31,32]). The main results from the above papers
are that uniform interpolants exist for intuitionistic logic and for some modal
systems (like the G¨odel-L¨ob system and the S4.Grz system); they do not exist
for instance in S4 and K4, whereas for the basic modal system K they exist for
the local consequence relation but not for the global consequence relation. The
connection between uniform interpolants and model completions (for equational
theories axiomatizing the varieties corresponding to propositional logics) was
ﬁrst stated in [20] and further developed in [17,23,31].
In the last decade, also the automated reasoning community developed an
increasing interest in uniform interpolants, with particular focus on quantiﬁer-
free fragments of ﬁrst-order theories. This is witnessed by various talks and
drafts by D. Kapur presented in many conferences and workshops (FloC 2010,
ISCAS 2013-14, SCS 2017 [22]), as well as by the paper [21] by Gulwani and
Musuvathi in ESOP 2008. In this last paper uniform interpolants were renamed
as covers, a terminology we shall adopt in this paper too. In these contributions,
examples of cover computations were supplied and also some algorithms were
sketched. The ﬁrst formal proof about existence of covers in EUF was however
published by the present authors only in [7]; such a proof was equipped with
powerful semantic tools (the Cover-by-Extensions Lemma 1 below) coming from
the connection to model-completeness, as well as with an algorithm relying on
a constrained variant of the Superposition Calculus (two simpler algorithms are
studied in [15]). The usefulness of covers in model checking was already stressed
in [21] and further motivated by our recent line of research on the veriﬁcation of
data-aware processes [5,6,8,9]. Notably, it is also operationally mirrored in the
MCMT [16] implementation since version 2.8. Covers (via quantiﬁer elimina-
tion in model completions and hierarchical reasoning) play an important role in
symbol elimination problems in theory extensions, as witnessed in the compre-
hensive paper [29] and in related papers (e.g., [25]) studying invariant synthesis
in model checking applications.
An important question suggested by the applications is the cover transfer
problem for combined theories: for instance, when modeling and verifying data-
aware processes, it is natural to consider the combination of diﬀerent theories,
such as the theories accounting for the read-write and read-only data storage of
the process as well as those for the elements stored therein [6–8]. Formally, the
cover transfer problem can be stated as follows: by supposing that covers exist in
theories T1, T2, under which conditions do they exist also in the combined theory
T1 ∪T2? In this paper we show that the answer is aﬃrmative in the disjoint
signatures convex case, using the same hypothesis (that is, the equality interpo-
lating condition) under which quantiﬁer-free interpolation transfers. Thus, for
convex theories we essentially obtain a necessary and suﬃcient condition, in the
precise sense captured by Theorem 6 below. We also prove that if convexity
fails, the non-convex equality interpolating property [2] may not be suﬃcient to
ensure the cover transfer property. As a witness for this, we show that EUF com-
bined with integer diﬀerence logic or with linear integer arithmetics constitutes
a counterexample.

Combined Covers and Beth Deﬁnability
183
The main tool employed in our combination result is the Beth deﬁnability the-
orem for primitive formulae (this theorem has been shown to be equivalent to
the equality interpolating condition in [2]). In order to design a combined cover
algorithm, we exploit the equivalence between implicit and explicit deﬁnability
that is supplied by the Beth theorem. Implicit deﬁnability is reformulated, via
covers for input theories, at the quantiﬁer-free level. Thus, the combined cover
algorithm guesses the implicitly deﬁnable variables, then eliminates them via
explicit deﬁnability, and ﬁnally uses the component-wise input cover algorithms
to eliminate the remaining (non implicitly deﬁnable) variables. The identiﬁca-
tion and the elimination of the implicitly deﬁned variables via explicitly deﬁning
terms is an essential step towards the correctness of the combined cover algo-
rithm: when computing a cover of a formula φ(x, y) (w.r.t. y), the variables x are
(non-eliminable) parameters, and those variables among the y that are implicitly
deﬁnable need to be discovered and treated in the same way as the parameters x.
Only after this preliminary step (Lemma 5 below), the input cover algorithms
can be suitably exploited (Proposition 1 below).
The combination result we obtain is quite strong, as it is a typical ‘black
box’ combination result: it applies not only to theories used in veriﬁcation (like
the combination of real arithmetics with EUF), but also in other contexts. For
instance, since the theory B of Boolean algebras satisﬁes our hypotheses (being
model completable and strongly amalgamable [14]), we get that uniform inter-
polants exist in the combination of B with EUF. The latter is the equational
theory algebraizing the basic non-normal classical modal logic system E from [27]
(extended to n-ary modalities). Notice that this result must be contrasted with
the case of many systems of Boolean algebras with operators where existence
of uniform interpolation fails [23] (recall that operators on a Boolean algebra
are not just arbitrary functions, but are required to be monotonic and also to
preserve either joins or meets in each coordinate).
As a last important comment on related work, it is worth mentioning that
Gulwani and Musuvathi in [21] also have a combined cover algorithm for convex,
signature disjoint theories. Their algorithm looks quite diﬀerent from ours; apart
from the fact that a full correctness and completeness proof for such an algorithm
has never been published, we underline that our algorithm is rooted on diﬀerent
hypotheses. In fact, we only need the equality interpolating condition and we
show that this hypothesis is not only suﬃcient, but also necessary for cover
transfer in convex theories; consequently, our result is formally stronger. The
equality interpolating condition was known to the authors of [21] (but not even
mentioned in their paper [21]): in fact, it was introduced by one of them some
years before [33]. The equality interpolating condition was then extended to the
non convex case in [2], where it was also semantically characterized via the strong
amalgamation property.
The paper is organized as follows: after some preliminaries in Sect. 2, the
crucial Covers-by-Extensions Lemma and the relationship between covers and
model completions from [7] are recalled in Sect. 3. In Sect. 4, we present some
preliminary results on interpolation and Beth deﬁnability that are instrumental

184
D. Calvanese et al.
to our machinery. After some useful facts about convex theories in Sect. 5, we
introduce the combined cover algorithms for the convex case and we prove its
correctness in Sect. 6; we also present a detailed example of application of the
combined algorithm in case of the combination of EUF with linear real arith-
metic, and we show that the equality interpolating condition is necessary (in
some sense) for combining covers. In Sect. 7 we exhibit a counteraxample to the
existence of combined covers in the non-convex case. Section 8 is devoted to the
conclusions and discussion of future work. The extended version of the current
paper with full proofs and details is available online in [4].
2
Preliminaries
We adopt the usual ﬁrst-order syntactic notions of signature, term, atom,
(ground) formula, and so on; our signatures are always ﬁnite or countable and
include equality. To avoid considering limit cases, we assume that signatures
always contain at least an individual constant. We compactly represent a tuple
⟨x1, . . . , xn⟩of variables as x. The notation t(x), φ(x) means that the term t, the
formula φ has free variables included in the tuple x. This tuple is assumed to be
formed by distinct variables, thus we underline that when we write e.g. φ(x, y),
we mean that the tuples x, y are made of distinct variables that are also disjoint
from each other.
A formula is said to be universal (resp., existential) if it has the form ∀x(φ(x))
(resp., ∃x(φ(x))), where φ is quantiﬁer-free. Formulae with no free variables are
called sentences. On the semantic side, we use the standard notion of Σ-structure
M and of truth of a formula in a Σ-structure under a free variables assignment.
The support of M is denoted as |M|. The interpretation of a (function, predi-
cate) symbol σ in M is denoted σM.
A Σ-theory T is a set of Σ-sentences; a model of T is a Σ-structure M where
all sentences in T are true. We use the standard notation T |= φ to say that φ
is true in all models of T for every assignment to the variables occurring free in
φ. We say that φ is T-satisﬁable iﬀthere is a model M of T and an assignment
to the variables occurring free in φ making φ true in M.
We now focus on the constraint satisﬁability problem and quantiﬁer elimina-
tion for a theory T. A Σ-formula φ is a Σ-constraint (or just a constraint) iﬀit is
a conjunction of literals. The constraint satisﬁability problem for T is the follow-
ing: we are given a constraint φ(x) and we are asked whether there exist a model
M of T and an assignment I to the free variables x such that M, I |= φ(x). A
theory T has quantiﬁer elimination iﬀfor every formula φ(x) in the signature
of T there is a quantiﬁer-free formula φ′(x) such that T |= φ(x) ↔φ′(x). Since
we are in a computational logic context, when we speak of quantiﬁer elimina-
tion, we assume that it is eﬀective, namely that it comes with an algorithm for
computing φ′ out of φ. It is well-known that quantiﬁer elimination holds in case
we can eliminate quantiﬁers from primitive formulae, i.e., formulae of the kind
∃y φ(x, y), with φ a constraint.
We recall also some further basic notions. Let Σ be a ﬁrst-order signature.
The signature obtained from Σ by adding to it a set a of new constants (i.e.,

Combined Covers and Beth Deﬁnability
185
0-ary function symbols) is denoted by Σa. Analogously, given a Σ-structure M,
the signature Σ can be expanded to a new signature Σ|M| := Σ ∪{¯a | a ∈|M|}
by adding a set of new constants ¯a (the name for a), one for each element a in
the support of M, with the convention that two distinct elements are denoted
by diﬀerent “name” constants. M can be expanded to a Σ|M|-structure M :=
(M, a)a∈|M| just interpreting the additional constants over the corresponding
elements. From now on, when the meaning is clear from the context, we will
freely use the notation M and M interchangeably: in particular, given a Σ-
structure M and a Σ-formula φ(x) with free variables that are all in x, we will
write, by abuse of notation, M |= φ(a) instead of M |= φ(¯a).
A Σ-homomorphism (or, simply, a homomorphism) between two Σ-structu-
res M and N is a map μ : |M| −→|N| among the support sets |M| of M and
|N| of N satisfying the condition (M |= ϕ
⇒
N |= ϕ) for all Σ|M|-atoms
ϕ (M is regarded as a Σ|M|-structure, by interpreting each additional constant
a ∈|M| into itself and N is regarded as a Σ|M|-structure by interpreting each
additional constant a ∈|M| into μ(a)). In case the last condition holds for all
Σ|M|-literals, the homomorphism μ is said to be an embedding and if it holds for
all ﬁrst order formulae, the embedding μ is said to be elementary. If μ : M −→N
is an embedding which is just the identity inclusion |M| ⊆|N|, we say that M
is a substructure of N or that N is an extension of M. Universal theories can be
characterized as those theories T having the property that if M |= T and N is
a substructure of M, then N |= T (see [10]). If M is a structure and X ⊆|M|,
then there is the smallest substructure of M including X in its support; this is
called the substructure generated by X. If X is the set of elements of a ﬁnite
tuple a, then the substructure generated by X has in its support precisely the
b ∈|M| such that M |= b = t(a) for some term t.
Let M be a Σ-structure. The diagram of M, written ΔΣ(M) (or just Δ(M)),
is the set of ground Σ|M|-literals that are true in M. An easy but important
result, called Robinson Diagram Lemma [10], says that, given any Σ-structure
N, the embeddings μ : M −→N are in bijective correspondence with expansions
of N to Σ|M|-structures which are models of ΔΣ(M). The expansions and the
embeddings are related in the obvious way: ¯a is interpreted as μ(a).
3
Covers and Model Completions
We report the notion of cover taken from [21] and also the basic results proved
in [7]. Fix a theory T and an existential formula ∃e φ(e, y); call a residue of
∃e φ(e, y) any quantiﬁer-free formula belonging to the set of quantiﬁer-free for-
mulae Res(∃e φ) = {θ(y, z) | T |= φ(e, y) →θ(y, z)}. A quantiﬁer-free formula
ψ(y) is said to be a T-cover (or, simply, a cover) of ∃e φ(e, y) iﬀψ(y) ∈Res(∃e φ)
and ψ(y) implies (modulo T) all the other formulae in Res(∃e φ). The follow-
ing “cover-by-extensions” Lemma [7] (to be widely used throughout the paper)
supplies a semantic counterpart to the notion of a cover:
Lemma 1 (Cover-by-Extensions). A formula ψ(y) is a T-cover of ∃e φ(e, y)
iﬀit satisﬁes the following two conditions:(i) T |= ∀y (∃e φ(e, y) →ψ(y)); (ii)

186
D. Calvanese et al.
for every model M of T, for every tuple of elements a from the support of M
such that M |= ψ(a) it is possible to ﬁnd another model N of T such that M
embeds into N and N |= ∃e φ(e, a).
◁
We underline that, since our language is at most countable, we can assume
that the models M, N from (ii) above are at most countable too, by a
L¨owenheim-Skolem argument.
We say that a theory T has uniform quantiﬁer-free interpolation iﬀevery
existential formula ∃e φ(e, y) (equivalently, every primitive formula ∃e φ(e, y))
has a T-cover.
It is clear that if T has uniform quantiﬁer-free interpolation, then it has ordi-
nary quantiﬁer-free interpolation [2], in the sense that if we have T |= φ(e, y) →
φ′(y, z) (for quantiﬁer-free formulae φ, φ′), then there is a quantiﬁer-free for-
mula θ(y) such that T |= φ(e, y) →θ(y) and T |= θ(y) →φ′(y, z). In fact,
if T has uniform quantiﬁer-free interpolation, then the interpolant θ is inde-
pendent on φ′ (the same θ(y) can be used as interpolant for all entailments
T |= φ(e, y) →φ′(y, z), varying φ′).
We say that a universal theory T has a model completion iﬀthere is a stronger
theory T ∗⊇T (still within the same signature Σ of T) such that (i) every Σ-
constraint that is satisﬁable in a model of T is satisﬁable in a model of T ∗;
(ii) T ∗eliminates quantiﬁers. Other equivalent deﬁnitions are possible [10]: for
instance, (i) is equivalent to the fact that T and T ∗prove the same universal
formulae or again to the fact that every model of T can be embedded into a
model of T ∗. We recall that the model completion, if it exists, is unique and that
its existence implies the quantiﬁer-free interpolation property for T [10] (the
latter can be seen directly or via the correspondence between quantiﬁer-free
interpolation and amalgamability, see [2]).
A close relationship between model completion and uniform interpolation
emerged in the area of propositional logic (see the book [17]) and can be for-
mulated roughly as follows. It is well-known that most propositional calculi, via
Lindembaum constructions, can be algebraized: the algebraic analogue of clas-
sical logic are Boolean algebras, the algebraic analogue of intuitionistic logic are
Heyting algebras, the algebraic analogue of modal calculi are suitable variaties
of modal agebras, etc. Under suitable hypotheses, it turns out that a proposi-
tional logic has uniform interpolation (for the global consequence relation) iﬀ
the equational theory axiomatizing the corresponding variety of algebras has a
model completion [17]. In the context of ﬁrst order theories, we prove an even
more direct connection:
Theorem 1. Suppose that T is a universal theory. Then T has a model com-
pletion T ∗iﬀT has uniform quantiﬁer-free interpolation. If this happens, T ∗
is axiomatized by the inﬁnitely many sentences ∀y (ψ(y) →∃e φ(e, y)), where
∃e φ(e, y) is a primitive formula and ψ is a cover of it.
◁
The proof (via Lemma 1, by iterating a chain construction) is in [9] (see also [3]).

Combined Covers and Beth Deﬁnability
187
4
Equality Interpolating Condition and Beth Deﬁnability
We report here some deﬁnitions and results we need concerning combined
quantiﬁer-free interpolation. Most deﬁnitions and result come from [2], but are
simpliﬁed here because we restrict them to the case of universal convex theories.
Further information on the semantic side is supplied in the extended version of
this paper [4].
A theory T is stably inﬁnite iﬀevery T-satisﬁable constraint is satisﬁable
in an inﬁnite model of T. The following Lemma comes from a compactness
argument (see [4] for a proof):
Lemma 2. If T is stably inﬁnite, then every ﬁnite or countable model M of T
can be embedded in a model N of T such that |N| \ |M| is countable.
◁
A theory T is convex iﬀfor every constraint δ, if T ⊢δ →n
i=1 xi = yi
then T ⊢δ →xi = yi holds for some i ∈{1, ..., n}. A convex theory T is
‘almost’ stably inﬁnite in the sense that it can be shown that every constraint
which is T-satisﬁable in a T-model whose support has at least two elements is
satisﬁable also in an inﬁnite T-model. The one-element model can be used to
build counterexamples, though: e.g., the theory of Boolean algebras is convex
(like any other universal Horn theory) but the constraint x = 0 ∧x = 1 is
only satisﬁable in the degenerate one-element Boolean algebra. Since we take
into account these limit cases, we do not assume that convexity implies stable
inﬁniteness.
Deﬁnition 1. A convex universal theory T is equality interpolating iﬀfor every
pair y1, y2 of variables and for every pair of constraints δ1(x, z1, y1), δ2(x, z2, y2)
such that
T ⊢δ1(x, z1, y1) ∧δ2(x, z2, y2) →y1 = y2
(1)
there exists a term t(x) such that
T ⊢δ1(x, z1, y1) ∧δ2(x, z2, y2) →y1 = t(x) ∧y2 = t(x).
(2)
◁
Theorem 2 [2,33]. Let T1 and T2 be two universal, convex, stably inﬁnite the-
ories over disjoint signatures Σ1 and Σ2. If both T1 and T2 are equality inter-
polating and have quantiﬁer-free interpolation property, then so does T1 ∪T2. ◁
There is a converse of the previous result; for a signature Σ, let us call
EUF(Σ) the pure equality theory over the signature Σ (this theory is equality
interpolating and has the quantiﬁer-free interpolation property).
Theorem 3 [2]. Let T be a stably inﬁnite, universal, convex theory admitting
quantiﬁer-free interpolation and let Σ be a signature disjoint from the signa-
ture of T containing at least a unary predicate symbol. Then, T ∪EUF(Σ) has
quantiﬁer-free interpolation iﬀT is equality interpolating.
◁

188
D. Calvanese et al.
In [2] the above deﬁnitions and results are extended to the non-convex case
and a long list of universal quantiﬁer-free interpolating and equality interpolating
theories is given. The list includes EUF(Σ), recursive data theories, as well as
linear arithmetics. For linear arithmetics (and fragments of its), it is essential to
make a very careful choice of the signature, see again [2] (especially Subsection
4.1) for details. All the above theories admit a model completion (which coincides
with the theory itself in case the theory admits quantiﬁer elimination).
The equality interpolating property in a theory T can be equivalently
characterized using Beth deﬁnability as follows. Consider a primitive formula
∃zφ(x, z, y) (here φ is a conjunction of literals); we say that ∃z φ(x, z, y) implic-
itly deﬁnes y in T iﬀthe formula
∀y ∀y′ (∃zφ(x, z, y) ∧∃zφ(x, z, y′) →y = y′)
(3)
is T-valid. We say that ∃zφ(x, z, y) explicitly deﬁnes y in T iﬀthere is a term
t(x) such that the formula
∀y (∃zφ(x, z, y) →y = t(x))
(4)
is T-valid.
For future use, we notice that, by trivial logical manipulations, the formu-
lae (3) and (4) are logically equivalent to
∀y∀z∀y′∀z′(φ(x, z, y) ∧φ(x, z′, y′) →y = y′).
(5)
and to
∀y∀z(φ(x, z, y) →y = t(x))
(6)
respectively (we shall use such equivalences without explicit mention).
We say that a theory T has the Beth deﬁnability property for primitive formu-
lae iﬀwhenever a primitive formula ∃z φ(x, z, y) implicitly deﬁnes the variable
y then it also explicitly deﬁnes it.
Theorem 4 [2]. A convex theory T having quantiﬁer-free interpolation is equal-
ity interpolating iﬀit has the Beth deﬁnability property for primitive formulae. ◁
Proof. We recall the easy proof of the left-to-right side (this is the only side we
need in this paper). Suppose that T is equality interpolating and that
T ⊢φ(x, z, y) ∧φ(x, z′, y′) →y = y′;
then there is a term t(x) such that
T ⊢φ(x, z, y) ∧φ(x, z′, y′) →y = t(x) ∧y′ = t(x).
Replacing z′, y′ by z, y via a substitution, we get precisely (6).
⊣

Combined Covers and Beth Deﬁnability
189
5
Convex Theories
We now collect some useful facts concerning convex theories. We ﬁx for this
section a convex, stably inﬁnite, equality interpolating universal theory T admit-
ting a model completion T ∗. We let Σ be the signature of T. We ﬁx also a
Σ-constraint φ(x, y), where we assume that y = y1, . . . , yn (recall that the tuple
x is disjoint from the tuple y according to our conventions from Sect. 2).
For i = 1, . . . , n, we let the formula ImplDefT
φ,yi(x) be the quantiﬁer-free
formula equivalent in T ∗to the formula
∀y ∀y′(φ(x, y) ∧φ(x, y′) →yi = y′
i)
(7)
where the y′ are renamed copies of the y. Notice that the variables occurring
free in φ are x, y, whereas only the x occur free in ImplDefT
φ,yi(x) (the variable
yi is among the y and does not occur free in ImplDefT
φ,yi(x)): these facts coming
from our notational conventions are crucial and should be kept in mind when
reading this and next section. The following semantic technical lemma is proved
in the extended version of this paper [4]:
Lemma 3. Suppose that we are given a model M of T and elements a from
the support of M such that M ̸|= ImplDefT
φ,yi(a) for all i = 1, . . . , n. Then
there exists an extension N of M such that for some b ∈|N| \ |M| we have
N |= φ(a, b).
◁
The following Lemma supplies terms which will be used as ingredients in our
combined covers algorithm:
Lemma 4. Let Li1(x) ∨· · · ∨Liki(x) be the disjunctive normal form (DNF) of
ImplDefT
φ,yi(x). Then, for every j = 1, . . . , ki, there is a Σ(x)-term tij(x) such
that
T ⊢Lij(x) ∧φ(x, y) →yi = tij.
(8)
As a consequence, a formula of the kind ImplDefT
φ,yi(x) ∧∃y (φ(x, y) ∧ψ) is
equivalent (modulo T) to the formula
ki

j=1
∃y (yi = tij ∧Lij(x) ∧φ(x, y) ∧ψ).
(9)
◁
Proof. We have that (
j Lij) ↔ImplDefT
φ,yi(x) is a tautology, hence from the
deﬁnition of ImplDefT
φ,yi(x), we have that
T ∗⊢Lij(x) →∀y ∀y′(φ(x, y) ∧φ(x, y′) →yi = y′
i);
however this formula is trivially equivalent to a universal formula (Lij does not
depend on y, y′), hence since T and T ∗prove the same universal formulae, we
get
T ⊢Lij(x) ∧φ(x, y) ∧φ(x, y′) →yi = y′
i.

190
D. Calvanese et al.
Using Beth deﬁnability property (Theorem 4), we get (8), as required, for some
terms tij(x). Finally, the second claim of the lemma follows from (8) by trivial
logical manipulations.
⊣
In all our concrete examples, the theory T has decidable quantiﬁer-free frag-
ment (namely it is decidable whether a quantiﬁer-free formula is a logical con-
sequence of T or not), thus the terms tij mentioned in Lemma 4 can be com-
puted just by enumerating all possible Σ(x)-terms: the computation terminates,
because the above proof shows that the appropriate terms always exist. How-
ever, this is terribly ineﬃcient and, from a practical point of view, one needs to
have at disposal dedicated algorithms to ﬁnd the required equality interpolating
terms. For some common theories (EUF, Lisp-structures, linear real arithmetic),
such algorithms are designed in [33]; in [2] [Lemma 4.3 and Theorem 4.4], the
algorithms for computing equality interpolating terms are connected to quanti-
ﬁer elimination algorithms in the case of universal theories admitting quantiﬁer
elimination. Still, an extensive investigation on te topic seems to be missed in
the SMT literature.
6
The Convex Combined Cover Algorithm
Let us now ﬁx two theories T1, T2 over disjoint signatures Σ1, Σ2. We assume
that both of them satisfy the assumptions from the previous section, meaning
that they are convex, stably inﬁnite, equality interpolating, universal and admit
model completions T ∗
1 , T ∗
2 respectively. We shall supply a cover algorithm for
T1 ∪T2 (thus proving that T1 ∪T2 has a model completion too).
We need to compute a cover for ∃e φ(x, e), where φ is a conjunction of Σ1∪Σ2-
literals. By applying rewriting puriﬁcation steps like
φ =⇒∃d (d = t ∧φ(d/t))
(where d is a fresh variable and t is a pure term, i.e. it is either a Σ1- or a
Σ2-term), we can assume that our formula φ is of the kind φ1 ∧φ2, where φ1 is
a Σ1-formula and φ2 is a Σ2-formula. Thus we need to compute a cover for a
formula of the kind
∃e (φ1(x, e) ∧φ2(x, e)),
(10)
where φi is a conjunction of Σi-literals (i = 1, 2). We also assume that both φ1
and φ2 contain the literals ei ̸= ej (for i ̸= j) as a conjunct: this can be achieved
by guessing a partition of the e and by replacing each ei with the representative
element of its equivalence class.
Remark 1. It is not clear whether this preliminary guessing step can be
avoided. In fact, Nelson-Oppen [24] combined satisﬁability for convex theories
does not need it; however, combining covers algorithms is a more complicated
problem than combining mere satisﬁability algorithms and for technical reasons
related to the correctness and completeness proofs below, we were forced to
introduce guessing at this step.
◁

Combined Covers and Beth Deﬁnability
191
To manipulate formulae, our algorithm employs acyclic explicit deﬁnitions as
follows. When we write ExplDef(z, x) (where z, x are tuples of distinct variables),
we mean any formula of the kind (let z := z1 . . . , zm)
m

i=1
zi = ti(z1, . . . , zi−1, x)
where the term ti is pure (i.e. it is a Σi-term) and only the variables z1, . . . , zi−1, x
can occur in it. When we assert a formula like ∃z (ExplDef(z, x) ∧ψ(z, x)), we
are in fact in the condition of recursively eliminating the variables z from it via
terms containing only the parameters x (the ‘explicit deﬁnitions’ zi = ti are in
fact arranged acyclically).
A working formula is a formula of the kind
∃z (ExplDef(z, x) ∧∃e (ψ1(x, z, e) ∧ψ2(x, z, e))),
(11)
where ψ1 is a conjunction of Σ1-literals and ψ2 is a conjunction of Σ2-literals.
The variables x are called parameters, the variables z are called deﬁned variables
and the variables e (truly) existential variables. The parameters do not change
during the execution of the algorithm. We assume that ψ1, ψ2 in a working
formula (11) always contain the literals ei ̸= ej (for distinct ei, ej from e) as a
conjunct.
In our starting formula (10), there are no deﬁned variables. However, if via
some syntactic check it happens that some of the existential variables can be rec-
ognized as deﬁned, then it is useful to display them as such (this observation may
avoid redundant cases - leading to inconsistent disjuncts - in the computations
below).
A working formula like (11) is said to be terminal iﬀfor every existential
variable ei ∈e we have that
T1 ⊢ψ1 →¬ImplDefT1
ψ1,ei(x, z) and T2 ⊢ψ2 →¬ImplDefT2
ψ2,ei(x, z).
(12)
Roughly speaking, we can say that in a terminal working formula, all variables
which are not parameters are either explicitly deﬁnable or recognized as not
implicitly deﬁnable by both theories; of course, a working formula with no exis-
tential variables is terminal.
Lemma 5. Every working formula is equivalent (modulo T1 ∪T2) to a disjunc-
tion of terminal working formulae.
◁
Proof. We only sketch the proof of this Lemma (see the extended version [4]
for full details), by describing the algorithm underlying it. To compute the
required terminal working formulae, it is suﬃcient to apply the following non-
deterministic procedure (the output is the disjunction of all possible outcomes).
The non-deterministic procedure applies one of the following alternatives.
(1) Update ψ1 by adding it a disjunct from the DNF of 
ei∈e ¬ImplDefT1
ψ1,ei
(x, z) and ψ2 by adding to it a disjunct from the DNF of 
ei∈e ¬
ImplDefT2
ψ1,ei(x, z);

192
D. Calvanese et al.
(2.i) Select ei ∈e and h ∈{1, 2}; then update ψh by adding to it a disjunct Lij
from the DNF of ImplDefTh
ψh,ei(x, z); the equality ei = tij (where tij is the
term mentioned in Lemma 4)1 is added to ExplDef(z, x); the variable ei
becomes in this way part of the deﬁned variables.
If alternative (1) is chosen, the procedure stops, otherwise it is recursively applied
again and again (we have one truly existential variable less after applying alter-
native (2.i), so we eventually terminate).
⊣
Thus we are left to the problem of computing a cover of a terminal working
formula; this problem is solved in the following proposition:
Proposition 1. A cover of a terminal working formula (11) can be obtained
just by unravelling the explicit deﬁnitions of the variables z from the formula
∃z (ExplDef(z, x) ∧θ1(x, z) ∧θ2(x, z))
(13)
where θ1(x, z) is the T1-cover of ∃eψ1(x, z, e) and θ2(x, z) is the T2-cover of
∃eψ2(x, z, e).
◁
Proof. In order to show that Formula (13) is the T1 ∪T2-cover of a terminal
working formula (11), we prove, by using the Cover-by-Extensions Lemma 1,
that, for every T1 ∪T2-model M, for every tuple a, c from |M| such that M |=
θ1(a, c)∧θ2(a, c) there is an extension N of M such that N is still a model of T1∪
T2 and N |= ∃e(ψ1(a, c, e)∧ψ2(a, c, e)). By a L¨owenheim-Skolem argument, since
our languages are countable, we can suppose that M is at most countable and
actually that it is countable by stable inﬁniteness of our theories, see Lemma 2
(the fact that T1 ∪T2 is stably inﬁnite in case both T1, T2 are such, comes from
the proof of Nelson-Oppen combination result, see [12,24,30]).
According to the conditions (12) and the deﬁnition of a cover (notice that
the formulae ¬ImplDefTh
ψh,ei(x, z) do not contain the e and are quantiﬁer-free)
we have that
T1 ⊢θ1 →¬ImplDefT1
ψ1,ei(x, z) and T2 ⊢θ2 →¬ImplDefT2
ψ2,ei(x, z)
(for every ei ∈e). Thus, since M ̸|= ImplDefT1
ψ1,ei(a, c) and M ̸|= ImplDefT2
ψ2,ei
(a, c) holds for every ei ∈e, we can apply Lemma 3 and conclude that there
exist a T1-model N1 and a T2-model N2 such that N1 |= ψ1(a, c, b1) and N2 |=
ψ2(a, c, b2) for tuples b1 ∈|N1| and b2 ∈|N2|, both disjoint from |M|. By a
L¨owenheim-Skolem argument, we can suppose that N1, N2 are countable and by
Lemma 2 even that they are both countable extensions of M.
The tuples b1 and b2 have equal length because the ψ1, ψ2 from our working
formulae entail ei ̸= ej, where ei, ej are diﬀerent existential variables. Thus there
is a bijection ι : |N1| →|N2| ﬁxing all elements in M and mapping component-
wise the b1 onto the b2. But this means that, exactly as it happens in the proof of
1 Lemma 4 is used taking as y the tuple e, as x the tuple x, z, as φ(x, y) the formula
ψh(x, z, e) and as ψ the formula ψ3−h.

Combined Covers and Beth Deﬁnability
193
the completeness of the Nelson-Oppen combination procedure, the Σ2-structure
on N2 can be moved back via ι−1 to |N1| in such a way that the Σ2-substructure
from M is ﬁxed and in such a way that the tuple b2 is mapped to the tuple b1.
In this way, N1 becomes a Σ1 ∪Σ2-structure which is a model of T1 ∪T2 and
which is such that N1 |= ψ1(a, c, b1) ∧ψ2(a, c, b1), as required.
⊣
From Lemma 5, Proposition 1 and Theorem 1, we immediately get
Theorem 5. Let T1, T2 be convex, stably inﬁnite, equality interpolating, univer-
sal theories over disjoint signatures admitting a model completion. Then T1 ∪T2
admits a model completion too. Covers in T1 ∪T2 can be eﬀectively computed as
shown above.
◁
Notice that the input cover algorithms in the above combined cover compu-
tation algorithm are used not only in the ﬁnal step described in Proposition 1,
but also every time we need to compute a formula ImplDefTh
ψh,ei(x, z): accord-
ing to its deﬁnition, this formula is obtained by eliminating quantiﬁers in T ∗
i
from (7) (this is done via a cover computation, reading ∀as ¬∃¬). In prac-
tice, implicit deﬁnability is not very frequent, so that in many concrete cases
ImplDefTh
ψh,ei(x, z) is trivially equivalent to ⊥(in such cases, Step (2.i) above
can obviously be disregarded).
An Example. We now analyze an example in detail. Our results apply for
instance to the case where T1 is EUF(Σ) and T2 is linear real arithmetic. We
recall that covers are computed in real arithmetic by quantiﬁer elimination,
whereas for EUF(Σ) one can apply the superposition-based algorithm from [7].
Let us show that the cover of
∃e1 · · · ∃e4
⎛
⎜
⎝
e1 = f(x1) ∧e2 = f(x2) ∧
∧f(e3) = e3 ∧f(e4) = x1 ∧
∧x1 + e1 ≤e3 ∧e3 ≤x2 + e2 ∧e4 = x2 + e3
⎞
⎟
⎠
(14)
is the following formula
[x2 = 0 ∧f(x1) = x1 ∧x1 ≤0 ∧x1 ≤f(0)] ∨
∨[x1 + f(x1) < x2 + f(x2) ∧x2 ̸= 0] ∨
∨
x2 ̸= 0 ∧x1 + f(x1) = x2 + f(x2) ∧f(2x2 + f(x2)) = x1 ∧
∧f(x1 + f(x1)) = x1 + f(x1)

(15)
Formula (14) is already puriﬁed. Notice also that the variables e1, e2 are in
fact already explicitly deﬁned (only e3, e4 are truly existential variables).
We ﬁrst make the partition guessing. There is no need to involve deﬁned vari-
ables into the partition guessing, hence we need to consider only two partitions;
they are described by the following formulae:
P1(e3, e4) ≡e3 ̸= e4
P2(e3, e4) ≡e3 = e4

194
D. Calvanese et al.
We ﬁrst analyze the case of P1. The formulae ψ1 and ψ2 to which we need to
apply exhaustively Step (1) and Step (2.i) of our algorithm are:
ψ1 ≡f(e3) = e3 ∧f(e4) = x1 ∧e3 ̸= e4
ψ2 ≡x1 + e1 ≤e3 ∧e3 ≤x2 + e2 ∧e4 = x2 + e3 ∧e3 ̸= e4
We ﬁrst compute the implicit deﬁnability formulae for the truly existential vari-
ables with respect to both T1 and T2.
– We ﬁrst consider ImplDefT1
ψ1,e3(x, z). Here we show that the cover of the nega-
tion of formula (7) is equivalent to ⊤(so that ImplDefT1
ψ1,e3(x, z) is equivalent
to ⊥). We must quantify over truly existential variables and their duplica-
tions, thus we need to compute the cover of
f(e′
3) = e′
3 ∧f(e3) = e3 ∧f(e′
4) = x1 ∧f(e4) = x1 ∧e3 ̸= e4 ∧e′
3 ̸= e′
4 ∧e′
3 ̸= e3
This is a saturated set according to the superposition based procedure of [7],
hence the result is ⊤, as claimed.
– The formula ImplDefT1
ψ1,e4(x, z) is also equivalent to ⊥, by the same argument
as above.
– To compute ImplDefT2
ψ2,e3(x, z) we use Fourier-Motzkin quantiﬁer elimina-
tion. We need to eliminate the variables e3, e′
3, e4, e′
4 (intended as existentially
quantiﬁed variables) from
x1 + e1 ≤e′
3 ≤x2 + e2 ∧x1 + e1 ≤e3 ≤x2 + e2 ∧e′
4 = x2 + e′
3∧
∧e4 = x2 + e3 ∧e3 ̸= e4 ∧e′
3 ̸= e′
4 ∧e′
3 ̸= e3.
This gives x1 + e1 ̸= x2 + e2 ∧x2 ̸= 0, so that ImplDefT2
ψ2,e3(x, z) is x1 + e1 =
x2 + e2 ∧x2 ̸= 0. The corresponding equality interpolating term for e3 is
x1 + e1.
– The formula ImplDefT2
ψ2,e4(x, z) is also equivalent to x1+e1 = x2+e2∧x2 ̸= 0
and the equality interpolating term for e4 is x1 + e1 + x2.
So, if we apply Step 1 we get
∃e1 · · · ∃e4
⎛
⎜
⎝
e1 = f(x1) ∧e2 = f(x2) ∧
∧f(e3) = e3 ∧f(e4) = x1 ∧e3 ̸= e4 ∧
∧x1 + e1 ≤e3 ∧e3 ≤x2 + e2 ∧e4 = x2 + e3 ∧x1 + e1 ̸= x2 + e2
⎞
⎟
⎠
(16)
(notice that the literal x2 ̸= 0 is entailed by ψ2, so we can simplify it to ⊤in
ImplDefT2
ψ2,e3(x, z) and ImplDefT2
ψ2,e4(x, z)). If we apply Step (2.i) (for i = 3), we
get (after removing implied equalities)
∃e1 · · · ∃e4
⎛
⎜
⎝
e1 = f(x1) ∧e2 = f(x2) ∧e3 = x1 + e1 ∧
∧f(e3) = e3 ∧f(e4) = x1 ∧e3 ̸= e4 ∧
∧e4 = x2 + e3 ∧x1 + e1 = x2 + e2
⎞
⎟
⎠
(17)

Combined Covers and Beth Deﬁnability
195
Step (2.i) (for i = 4) gives a formula logically equivalent to (17). Notice that (17)
is terminal too, because all existential variables are now explicitly deﬁned (this
is a lucky side-eﬀect of the fact that e3 has been moved to the deﬁned variables).
Thus the exhaustive application of Steps (1) and (2.i) is concluded.
Applying the ﬁnal step of Proposition 1 to (17) is quite easy: it is suﬃcient
to unravel the acyclic deﬁnitions. The result, after little simpliﬁcation, is
x2 ̸= 0 ∧x1 + f(x1) = x2 + f(x2) ∧
∧f(x2 + f(x1 + f(x1))) = x1 ∧f(x1 + f(x1)) = x1 + f(x1);
this can be further simpliﬁed to
x2 ̸= 0 ∧x1 + f(x1) = x2 + f(x2) ∧
∧f(2x2 + f(x2)) = x1 ∧f(x1 + f(x1)) = x1 + f(x1);
(18)
As to formula (16), we need to apply the ﬁnal cover computations mentioned
in Proposition 1. The formulae ψ1 and ψ2 are now
ψ′
1 ≡
f(e3) = e3 ∧f(e4) = x1 ∧e3 ̸= e4
ψ′
2 ≡
x1 + e1 ≤e3 ≤x2 + e2 ∧e4 = x2 + e3 ∧x1 + e1 ̸= x2 + e2 ∧e3 ̸= e4
The T1-cover of ψ′
1 is ⊤. For the T2-cover of ψ′
2, eliminating with Fourier-Motzkin
the variables e4 and e3, we get
x1 + e1 < x2 + e2 ∧x2 ̸= 0
which becomes
x1 + f(x1) < x2 + f(x2) ∧x2 ̸= 0
(19)
after unravelling the explicit deﬁnitions of e1, e2. Thus, the analysis of the case
of the partition P1 gives, as a result, the disjunction of (18) and (19).
We now analyze the case of P2. Before proceeding, we replace e4 with e3
(since P2 precisely asserts that these two variables coincide); our formulae ψ1
and ψ2 become
ψ′′
1 ≡f(e3) = e3 ∧f(e3) = x1
ψ′′
2 ≡x1 + e1 ≤e3 ∧e3 ≤x2 + e2 ∧0 = x2
From ψ′′
1 we deduce e3 = x1, thus we can move e3 to the explicitly deﬁned
variables (this avoids useless calculations: the implicit deﬁnability condition for
variables having an entailed explicit deﬁnition is obviously ⊤, so making case
split on it produces either tautological consequences or inconsistencies). In this
way we get the terminal working formula
∃e1 · · · ∃e3
⎛
⎜
⎝
e1 = f(x1) ∧e2 = f(x2) ∧e3 = x1
∧f(e3) = e3 ∧f(e3) = x1 ∧
∧x1 + e1 ≤e3 ∧e3 ≤x2 + e2 ∧0 = x2
⎞
⎟
⎠
(20)

196
D. Calvanese et al.
Unravelling the explicit deﬁnitions, we get (after exhaustive simpliﬁcations)
x2 = 0 ∧f(x1) = x1 ∧x1 ≤0 ∧x1 ≤f(0)
(21)
Now, the disjunction of (18),(19) and (21) is precisely the ﬁnal result (15)
claimed above. This concludes our detailed analysis of our example.
Notice that the example shows that combined cover computations may intro-
duce terms with arbitrary alternations of symbols from both theories (like
f(x2+f(x1+f(x1))) above). The point is that when a variable becomes explicitly
deﬁnable via a term in one of the theories, then using such additional variable
may in turn cause some other variables to become explicitly deﬁnable via terms
from the other theory, and so on and so forth; when ultimately the explicit deﬁ-
nitions are unraveled, highly nested terms arise with many symbol alternations
from both theories.
The Necessity of the Equality Interpolating Condition. The following
result shows that equality interpolating is a necessary condition for a transfer
result, in the sense that it is already required for minimal combinations with
signatures adding uninterpreted symbols:
Theorem 6. Let T be a convex, stably inﬁnite, universal theory admitting a
model completion and let Σ be a signature disjoint from the signature of T con-
taining at least a unary predicate symbol. Then T ∪EUF(Σ) admits a model
completion iﬀT is equality interpolating.
◁
Proof. The necessity can be shown by using the following argument. By Theo-
rem 1, T ∪EUF(Σ) has uniform quantiﬁer-free interpolation, hence also ordi-
nary quantiﬁer-free interpolation. We can now apply Theorem 3 and get that
T must be equality interpolating. Conversely, the suﬃciency comes from Theo-
rem 5 together with the fact that EUF(Σ) is trivially universal, convex, stably
inﬁnite, has a model completion [7] and is equality interpolating [2,33].
⊣
7
The Non-convex Case: A Counterexample
In this section, we show by giving a suitable counterexample that the convexity
hypothesis cannot be dropped from Theorems 5, 6. We make use of basic facts
about ultrapowers (see [10] for the essential information we need). We take as
T1 integer diﬀerence logic IDL, i.e. the theory of integer numbers under the
unary operations of successor and predecessor, the constant 0 and the strict
order relation <. This is stably inﬁnite, universal and has quantiﬁer elimination
(thus it coincides with its own model completion). It is not convex, but it satisﬁes
the equality interpolating condition, once the latter is suitably adjusted to non-
convex theories, see [2] for the related deﬁnition and all the above mentioned
facts.
As T2, we take EUF(Σf), where Σf has just one unary free function symbol
f (this f is supposed not to belong to the signature of T1).

Combined Covers and Beth Deﬁnability
197
Proposition 2. Let T1, T2 be as above; the formula
∃e (0 < e ∧e < x ∧f(e) = 0)
(22)
does not have a cover in T1 ∪T2.
◁
Proof. Suppose that (22) has a cover φ(x). This means (according to Cover-
by-Extensions Lemma 1) that for every model M of T1 ∪T2 and for every
element a ∈|M| such that M |= φ(a), there is an extension N of M such that
N |= ∃e (0 < e ∧e < a ∧f(e) = 0).
Consider the model M, so speciﬁed: the support of M is the set of the
integers, the symbols from the signature of T1 are interpreted in the standard
way and the symbol f is interpreted so that 0 is not in the image of f. Let
ak be the number k > 0 (it is an element from the support of M). Clearly it
is not possible to extend M so that ∃e (0 < e ∧e < ak ∧f(e) = 0) becomes
true: indeed, we know that all the elements in the interval (0, k) are deﬁnable
as iterated successors of 0 and, by using the axioms of IDL, no element can
be added between a number and its successor, hence this interval cannot be
enlarged in a superstructure. We conclude that M |= ¬φ(ak) for every k.
Consider now an ultrapower ΠDM of M modulo a non-principal ultraﬁlter
D and let a be the equivalence class of the tuple ⟨ak⟩k∈N; by the fundamental
Los theorem [10], ΠDM |= ¬φ(a). We claim that it is possible to extend ΠDM
to a superstructure N such that N |= ∃e (0 < e ∧e < a ∧f(e) = 0): this
would entail, by deﬁnition of cover, that ΠDM |= φ(a), contradiction. We now
show why the claim is true. Indeed, since ⟨ak⟩k∈N has arbitrarily big numbers as
its components, we have that, in ΠDM, a is bigger than all standard numbers.
Thus, if we take a further non-principal ultrapower N of ΠDM, it becomes
possible to change in it the evaluation of f(b) for some b < a and set it to 0 (in
fact, as it can be easily seen, there are elements b ∈|N| less than a but not in
the support of ΠDM).
⊣
The counterexample still applies when replacing integer diﬀerence logic with
linear integer arithmetics.
8
Conclusions and Future Work
In this paper we showed that covers (aka uniform interpolants) exist in the
combination of two convex universal theories over disjoint signatures in case they
exist in the component theories and in case the component theories also satisfy
the equality interpolating condition - this further condition is nevertheless needed
in order to transfer the existence of (ordinary) quantiﬁer-free interpolants. In
order to prove that, Beth deﬁnability property for primitive fragments turned
out to be the crucial ingredient to extensively employ. In case convexity fails,
we showed by a counterexample that covers might not exist anymore in the
combined theory. The last result raises the following research problem. Even if
in general covers do not exist for combination of non-convex theories, it would

198
D. Calvanese et al.
be interesting to see under what conditions one can decide whether a given cover
exists and, in the aﬃrmative case, to compute it.
Applications suggest a diﬀerent line of investigations. In database-driven ver-
iﬁcation [5,6,9] one uses only ‘tame’ theory combinations. A tame combination
of two multi-sorted theories T1, T2 in their respective signatures Σ1, Σ2 is char-
acterized as follows: the shared sorts can only be the codomain sort (and not a
domain sort) of a symbol from Σ1 other than an equality predicate. In other
words, if a relation or a function symbol has among its domain sorts a sort from
Σ1 ∩Σ2, then this symbol is from Σ2 (and not from Σ1, unless it is the equality
predicate). The key result for tame combination is that covers existence transfers
to a tame combination T1 ∪T2 in case covers exist in the two component theories
(the two component theories are only assumed to be stably inﬁnite with respect
to all shared sorts). Moreover, the transfer algorithm, in the case relevant for
applications to database driven veriﬁcation (where T2 is linear arithmetics and
T1 is a multi-sorted version of EUF(Σ) in a signature Σ containing only unary
function symbols and relations of any arity), is relatively well-behaved also from
the complexity viewpoint because its cost is dominated by the cost of the covers
computation in T2. These results on tame combinations are included in the larger
ArXiv version [4] of the present paper; an implementation in the beta version
2.9 of the model-checker MCMT is already available from the web site http://
users.mat.unimi.it/users/ghilardi/mcmt.
A ﬁnal future research line could consider cover transfer properties to non-
disjoint signatures combinations, analogously to similar results obtained in [13,
14] for the transfer of quantiﬁer-free interpolation.
References
1. B´ılkov´a, M.: Uniform interpolation and propositional quantiﬁers in modal logics.
Stud. Logica 85(1), 1–31 (2007)
2. Bruttomesso, R., Ghilardi, S., Ranise, S.: Quantiﬁer-free interpolation in combina-
tions of equality interpolating theories. ACM Trans. Comput. Log. 15(1), 5:1–5:34
(2014)
3. Calvanese, D., Ghilardi, S., Gianola, A., Montali, M., Rivkin, A.: Quantiﬁer
elimination for database driven veriﬁcation. Technical report arXiv:1806.09686,
arXiv.org (2018)
4. Calvanese, D., Ghilardi, S., Gianola, A., Montali, M., Rivkin, A.: Combined cov-
ers and Beth deﬁnability (extended version). Technical report arXiv:1911.07774,
arXiv.org (2019)
5. Calvanese, D., Ghilardi, S., Gianola, A., Montali, M., Rivkin, A.: Formal modeling
and SMT-based parameterized veriﬁcation of data-aware BPMN. In: Hildebrandt,
T., van Dongen, B.F., R¨oglinger, M., Mendling, J. (eds.) BPM 2019. LNCS, vol.
11675, pp. 157–175. Springer, Cham (2019). https://doi.org/10.1007/978-3-030-
26619-6 12
6. Calvanese, D., Ghilardi, S., Gianola, A., Montali, M., Rivkin, A.: From model com-
pleteness to veriﬁcation of data aware processes. In: Lutz, C., Sattler, U., Tinelli,
C., Turhan, A.-Y., Wolter, F. (eds.) Description Logic, Theory Combination, and
All That. LNCS, vol. 11560, pp. 212–239. Springer, Cham (2019). https://doi.org/
10.1007/978-3-030-22102-7 10

Combined Covers and Beth Deﬁnability
199
7. Calvanese, D., Ghilardi, S., Gianola, A., Montali, M., Rivkin, A.: Model complete-
ness, covers and superposition. In: Fontaine, P. (ed.) CADE 2019. LNCS (LNAI),
vol. 11716, pp. 142–160. Springer, Cham (2019). https://doi.org/10.1007/978-3-
030-29436-6 9
8. Calvanese, D., Ghilardi, S., Gianola, A., Montali, M., Rivkin, A.: Veriﬁcation of
data-aware processes: challenges and opportunities for automated reasoning. In:
Proceedings of ARCADE, EPTCS, vol. 311 (2019)
9. Calvanese, D., Ghilardi, S., Gianola, A., Montali, M., Rivkin, A.: SMT-based veriﬁ-
cation of data-aware processes: a model-theoretic approach. Math. Struct. Comput.
Sci. 30(3), 271–313 (2020)
10. Chang, C.-C., Keisler, J.H.: Model Theory, 3rd edn. North-Holland Publishing Co.,
Amsterdam (1990)
11. Ghilardi, S.: An algebraic theory of normal forms. Ann. Pure Appl. Logic 71(3),
189–245 (1995)
12. Ghilardi, S.: Model theoretic methods in combined constraint satisﬁability. J.
Autom. Reason. 33(3–4), 221–249 (2004)
13. Ghilardi, S., Gianola, A.: Interpolation, amalgamation and combination (The non-
disjoint signatures case). In: Dixon, C., Finger, M. (eds.) FroCoS 2017. LNCS
(LNAI), vol. 10483, pp. 316–332. Springer, Cham (2017). https://doi.org/10.1007/
978-3-319-66167-4 18
14. Ghilardi, S., Gianola, A.: Modularity results for interpolation, amalgamation and
superamalgamation. Ann. Pure Appl. Log. 169(8), 731–754 (2018)
15. Ghilardi, S., Gianola, A., Kapur, D.: Compactly representing uniform interpolants
for EUF using (conditional) DAGS. Technical report arXiv:2002.09784, arXiv.org
(2020)
16. Ghilardi, S., Ranise, S.: MCMT: a model checker modulo theories. In: Giesl, J.,
H¨ahnle, R. (eds.) IJCAR 2010. LNCS (LNAI), vol. 6173, pp. 22–29. Springer,
Heidelberg (2010). https://doi.org/10.1007/978-3-642-14203-1 3
17. Ghilardi, S., Zawadowski, M.: Sheaves, Games, and Model Completions: A Cat-
egorical Approach to Nonclassical Propositional Logics. Trends in Logic-Studia
Logica Library, vol. 14. Kluwer Academic Publishers, Dordrecht (2002)
18. Ghilardi, S., Zawadowski, M.W.: A sheaf representation and duality for ﬁnitely
presenting heyting algebras. J. Symb. Log. 60(3), 911–939 (1995)
19. Ghilardi, S., Zawadowski, M.W.: Undeﬁnability of propositional quantiﬁers in the
modal system S4. Stud. Logica 55(2), 259–271 (1995)
20. Ghilardi, S., Zawadowski, M.W.: Model completions, r-Heyting categories. Ann.
Pure Appl. Log. 88(1), 27–46 (1997)
21. Gulwani, S., Musuvathi, M.: Cover algorithms and their combination. In:
Drossopoulou, S. (ed.) ESOP 2008. LNCS, vol. 4960, pp. 193–207. Springer, Hei-
delberg (2008). https://doi.org/10.1007/978-3-540-78739-6 16
22. Kapur, D.: Nonlinear polynomials, interpolants and invariant generation for sys-
tem analysis. In: Proceedings of the 2nd International Workshop on Satisﬁability
Checking and Symbolic Computation Co-Located with ISSAC (2017)
23. Kowalski, T., Metcalfe, G.: Uniform interpolation and coherence. Ann. Pure Appl.
Log. 170(7), 825–841 (2019)
24. Nelson, G., Oppen, D.C.: Simpliﬁcation by cooperating decision procedures. ACM
Trans. Program. Lang. Syst. 1(2), 245–257 (1979)
25. Peuter, D., Sofronie-Stokkermans, V.: On invariant synthesis for parametric sys-
tems. In: Fontaine, P. (ed.) CADE 2019. LNCS (LNAI), vol. 11716, pp. 385–405.
Springer, Cham (2019). https://doi.org/10.1007/978-3-030-29436-6 23

200
D. Calvanese et al.
26. Pitts, A.M.: On an interpretation of second order quantiﬁcation in ﬁrst order intu-
itionistic propositional logic. J. Symb. Log. 57(1), 33–52 (1992)
27. Segerberg, K.: An Essay in Classical Modal Logic. Filosoﬁska Studier, vol. 13.
Uppsala Universitet (1971)
28. Shavrukov, V.: Subalgebras of diagonalizable algebras of theories containing arith-
metic. Dissertationes Mathematicae, CCCXXIII (1993)
29. Sofronie-Stokkermans, V.: On interpolation and symbol elimination in theory
extensions. Log. Methods Comput. Sci. 14(3), 1–41 (2018)
30. Tinelli, C., Harandi, M.: A new correctness proof of the Nelson-Oppen combination
procedure. In: Baader, F., Schulz, K.U. (eds.) Frontiers of Combining Systems.
ALS, vol. 3, pp. 103–119. Springer, Dordrecht (1996). https://doi.org/10.1007/
978-94-009-0349-4 5
31. van Gool, S.J., Metcalfe, G., Tsinakis, C.: Uniform interpolation and compact
congruences. Ann. Pure Appl. Log. 168(10), 1927–1948 (2017)
32. Visser, A.: Uniform interpolation and layered bisimulation. In H´ajek, P. (ed.) G¨odel
1996. Logical Foundations on Mathematics, Computer Science and Physics – Kurt
G¨odel’s Legacy. Springer, Heidelberg (1996)
33. Yorsh, G., Musuvathi, M.: A combination method for generating interpolants. In:
Nieuwenhuis, R. (ed.) CADE 2005. LNCS (LNAI), vol. 3632, pp. 353–368. Springer,
Heidelberg (2005). https://doi.org/10.1007/11532231 26

Deciding Simple Inﬁnity Axiom Sets
with One Binary Relation by Means
of Superpostulates
Timm Lampert1(B)
and Anderson Nakano2
1 Humboldt University Berlin, Unter den Linden 6, 10099 Berlin, Germany
lampertt@staff.hu-berlin.de
2 Pontif´ıcia Universidade Cat´olica, R. Monte Alegre, 984, S˜ao Paulo 05014-901, Brazil
alnakano@pucsp.br
Abstract. Modern logic engines widely fail to decide axiom sets that are
satisﬁable only in an inﬁnite domain. This paper speciﬁes an algorithm
that automatically generates a database of independent inﬁnity axiom
sets with fewer than 1000 characters. It starts with complete theories of
pure ﬁrst-order logic with only one binary relation (FOLR) and generates
further inﬁnity axiom sets S of FOLR with fewer than 1000 characters
such that no other inﬁnity axiom set with fewer than 1000 characters
exists in the database that implies S. We call the generated inﬁnity axiom
sets S “superpostulates”. Any formula that is derivable from (satisﬁable)
superpostulates is also satisﬁable. Thus far, we have generated a database
with 2346 inﬁnity superpostulates by running our algorithm. This paper
ends by identifying three practical uses of the algorithmic generation
of such a database: (i) for systematic investigations of inﬁnity axiom
sets, (ii) for deciding inﬁnity axiom sets and (iii) for the development of
saturation algorithms.
Keywords: First-order logic · Decision problem · Complete theories ·
Inﬁnity axioms · Reduction classes · Dyadic logic
1
Introduction
Modern logic engines for ﬁrst-order formulas (FOL), such as Vampire, CVC4,
Spass or i-Prover, are very powerful in proving theorems (or refutations) or
in deciding formulas with ﬁnite models (counter-models). For formulas that
have only inﬁnite models, however, these engines widely fail.1 One exception is
1 Exceptions include inﬁnity axiom sets that can be decided by saturation, such as the
theory of immediate successors as stated in FOL with identity (cf. (14) on p. 15). How-
ever, in regard to FOL without identity, inﬁnity axiom sets are only rarely solved, and
only a few of them are included in the Thousands of Problems for Theorem Provers
(TPTP) library, namely, problems SYO635+1 to SYO638+1. Among these problems,
only SYO638+1 can be solved by SPASS due to chaining rules, which directly apply
to the transitivity axioms. Furthermore, Inﬁnox decides the ﬁnite unsatisﬁability of
SY0638+1 only by virtue of the plain properties of the relation involved.
c
⃝Springer Nature Switzerland AG 2020
N. Peltier and V. Sofronie-Stokkermans (Eds.): IJCAR 2020, LNAI 12166, pp. 201–217, 2020.
https://doi.org/10.1007/978-3-030-51074-9_12

202
T. Lampert and A. Nakano
Inﬁnox, which decides the ﬁnite unsatisﬁability of some relevant TPTP prob-
lems due to speciﬁc model-theoretic principles (cf. [3]).2 In contrast to Inﬁnox,
we investigate how to generate hitherto unknown and intricate inﬁnity axiom
sets (superpostulates) of FOLR to make it possible to decide an inﬁnite number
of inﬁnity axiom sets related to these superpostulates. No engine is currently
able to solve most of the superpostulates we generate.
In this paper, we refer only to FOLR, i.e., FOL without function symbols, with-
out identity and with only one binary relation. Based on Herbrand’s reduction of
FOL to dyadic FOL, Kalmar has proven that FOLR is a reduction class (cf. [7]).
Boolos, Jeﬀrey and Burgess have deﬁned and proven the algorithm for reducing
FOL to FOLR in modern terms (cf. [2], Sect. 21.3). B¨orger, Gr¨adel and Gurevich
have speciﬁed prenex normal forms of FOLR-formulas with prenexes of the form
∀∃∀(= [∀∃∀, (0, 1)]-class) or of the form ∀∀∀∃(= [∀3∃, (0, 1)]-class) as the two
minimal classes of FOLR with inﬁnity axioms (cf. Theorem 6.5.4, p. 309) and have
presented the following two examples for the two classes (cf. [1], p. 307):
∀x∃y∀z(¬Rxx ∧Rxy ∧(Ryz →Rxz))
(1)
∀x∀y∀z∃u(¬Rxx ∧Rxu ∧(Rxy ∧Ryz →Rxz))
(2)
They also specify another formula with the prenex ∀∃∀(cf. [1], p. 33):
∀x∃u∀y(¬Rxx ∧Rxu ∧(Ryx →Ryu))
(3)
(1) and (3) are both derivable from (2). Thus, to decide that the latter is
satisﬁable, it is suﬃcient to decide that the former are likewise satisﬁable. This
method of deciding inﬁnity axioms can be extended by specifying inﬁnity axioms
that imply other inﬁnity axioms.
However, prenex normal forms do not provide a syntax that is suitable for
this task. Diﬀerent prenex normal forms can be converted into each other by
equivalence transformation. For example, formula (2), which is a member of the
minimal class [∀3∃, (0, 1)], can easily be converted into a member of the minimal
class [∀∃∀, (0, 1)], as shown in Table 1.
Table 1. Converting diﬀerent prenex normal forms into each other
No.
Formula
Strategy
(i)
∀x∀y∀z∃u(¬Rxx ∧Rxu ∧(Rxy ∧Ryz →Rxz))
(2)
(ii)
∀x¬Rxx ∧∀x∃uRxu ∧∀x∀y(¬Rxy ∨∀z(¬Ryz ∨Rxz))
Miniscoping
(iii)
∀x2¬Rx2x2 ∧∀x1∃y1Rx1y1 ∧∀x3∀x4(¬Rx3x4 ∨∀x5(¬Rx4x5 ∨Rx3x5))
Renaming
(iv)
∀x1∃y1∀x2∀x3∀x4∀x5(¬Rx2x2 ∧Rx1y1 ∧(¬Rx3x4 ∨(¬Rx4x5 ∨Rx3x5)))
Prenexing
2 A further exception with respect to the engines on the TPTP site is Decider. This
engine is able to identify inﬁnity axiom sets of pure FOL without identity due to
the implementation of a strong saturation algorithm that is not based on resolu-
tion. However, the complexity of the implemented algorithm is exponential, and the
Decider engine is not optimized for rapid decision making. Thus, it fails to decide
complex formulas within reasonable time and memory limits.

Superpostulates
203
Likewise, it may well be that a formula of the minimal class [∀∃∀, (0, 1)]
can be converted into a formula of a decidable class, e.g., with prenexes starting
with existential quantiﬁers followed by universal quantiﬁers (=∃∗∀∗, (0, 1)]-class).
Similarly, the distribution of bound variables in the matrix is not standardized in
prenex normal forms. Thus, many equivalent variations are possible. Therefore,
the syntax of prenex normal forms is not well suited for investigating the internal
properties and relations of inﬁnity axioms.
This is why we do not refer to inﬁnity axioms (and, thus, to prenex nor-
mal forms) in the following. Instead, we refer to the opposite of prenex normal
forms, namely, anti-prenex normal forms, in which the scopes of quantiﬁers are
minimized. We refer to inﬁnity axiom sets in terms of sets of anti-prenex normal
forms. To do so, we deﬁne primary formulas via negation normal forms (NNFs)
as follows:
Deﬁnition 1. A ﬁrst-order formula φ is primary (= in anti-prenex form) if φ
is an NNF and either
1. does not contain ∧or ∨or
2. contains ∧or ∨iﬀeither
(a) any conjunction of n conjuncts (n > 1) is preceded by a sequence of
existential quantiﬁers of minimal length 1, where all n conjuncts contain
each variable of the existential quantiﬁers in that sequence, or
(b) any disjunction of n disjuncts (n > 1) is preceded by a sequence of uni-
versal quantiﬁers of minimal length 1, where all n disjuncts contain each
variable of the universal quantiﬁers in that sequence.
Thus, lines (ii) and (iii) of Table 1 are conjunctions of three primary formulas,
while lines (i) and (iv) do not contain primary formulas. Any ﬁrst-order for-
mula can be converted into a disjunction of conjunctions of primary formulas
(=FOLDNF); cf. [8] for an algorithm for doing so. Since the satisﬁability of
a disjunction can be decided by deciding the satisﬁability of each disjunct, we
consider only conjunctions of primary formulas or, analogously, (ﬁnite) sets of
axioms (= theories) in anti-prenex form.
Deﬁnition 2. An axiom set is a ﬁnite set of primary formulas.
Deﬁnition 3. An inﬁnity axiom set is a ﬁnite set of primary formulas that is
satisﬁable only within an inﬁnite domain.
From here on, we will express all axiom sets of FOLR in standardized notation.
Deﬁnition 4. A set of primary formulas (axioms) of FOLR is in standardized
notation if
1. the binary predicate used is R and

204
T. Lampert and A. Nakano
2. for each primary formula (axiom) A,
– the m universal quantiﬁers of A are binding variables x1 to xm from left
to right and
– the n existential quantiﬁers of A are binding variables y1 to yn from left
to right.
Example 1. In standardized notation, formula (2) is converted into the following
inﬁnity axiom set:
∀x1¬Rx1x1, ∀x1∃y1Rx1y1, ∀x1∀x2(¬Rx1x2 ∨∀x3(¬Rx2x3 ∨Rx1x3))
(4)
This axiom set was the example used by Hilbert and Bernays to motivate
proof theory by showing that the consistency of (4) cannot be proven by ﬁnite
interpretations (“Methode der Aufweisung”; cf. [6], p. 10). The third axiom
expresses transitivity.
Formula (3) is equivalent to the following inﬁnity axiom set from [11], p. 183:
∀x1¬Rx1x1, ∀x1∃y1(Rx1y1 ∧∀x2(¬Rx2x1 ∨Rx2y1))
(5)
Like (1), (3) and (5) are derivable from (4). The following inﬁnity axiom set
(6) is taken from [2], p. 138:
∀x1∀x2(¬Rx1x2 ∨¬Rx2x1), ∀x1∃y1Rx1y1, ∀x1∀x2(¬Rx1x2 ∨∀x3(¬Rx2x3 ∨Rx1x3))
(6)
Like (2), this inﬁnity axiom set is equivalent to (4).
The formulas mentioned up to this point are examples of inﬁnity axiom sets
of FOLR given in the standard literature. They are all satisﬁable by interpreting
Rxy as x < y over Q. Thus, they are all derivable from the complete dense
linear order (DLO) axiom set without endpoints. In the following sections, we will
generate a system of DLO variants without identity, to which all of the mentioned
standard examples belong. We use the term “system of formulas” to refer to
a recursive set of formulas generated by rules that can be implemented by a
computer program. Our investigation may serve as a case study for investigating
systems of inﬁnity axiom sets of FOLR.
Our goal is the automated generation of a system of consistent inﬁnity axiom
sets S of a limited length L such that no other inﬁnity axiom sets S2 of length
≤L exist such that S2 implies S. We call these inﬁnity axiom sets S “superpostu-
lates”. Superpostulates enable the reduction of a large number of inﬁnity axiom
sets to a small number of inﬁnity axiom sets of limited length. The term and
the general idea of studying systems of superpostulates originated in the work
of Sheﬀer (cf. [12]). Sheﬀer, however, neither published his work nor developed
his logic project to a full extent. His student Langford made use of Sheﬀer’s
idea in [9]. Langford intended to prove that the axiom set for linear orderings is
a complete superpostulate. Urquhart (cf. [14], p. 44), however, objected to the
correctness of his proof and showed how to correct it. Langford proved similar
results for the theories of betweenness and cyclic order. All of his examples are
equivalent to formulas of decidable classes without existential quantiﬁers and
with identity, which have the ﬁnite model property (cf. [1], Theorem 6.5.1, p.

Superpostulates
205
307). To our knowledge, neither Langford nor Sheﬀer studied superpostulates in
terms of inﬁnity axiom sets.
The following sections investigate inﬁnity axiom sets related to the complete
DLO axiom set without endpoints. Section 2 introduces further terminology and
speciﬁes a superpostulate DLOR for FOLR, which is a DLO variant without
identity and without endpoints from which all of the mentioned inﬁnity axiom
sets follow. After having speciﬁed a complete inﬁnity axiom set, Sect. 3 speciﬁes
an inﬁnite sequence of inﬁnity axiom sets, each strictly implying the next and
all implied by (5). Section 4 then speciﬁes a system of superpostulates based on
DLOR; Sect. 5 refers to a system THSR based on the complete theory of immedi-
ate successors without identity. Finally, we conclude by identifying several prac-
tical uses of our method of algorithmic generation of systems of superpostulates
in Sect. 6.
2
Superpostulates
We ﬁrst introduce some terminology as a basis for deﬁning the term superpos-
tulate.
Deﬁnition 5. An axiom A is independent of a set of axioms S if A is not
implied by S and S ∧A is consistent.
Deﬁnition 6. An axiom set A is complete in FOLR if for any FOLR-formula
B, either B or ¬B follows from A.
Remark 1. Throughout this paper, we consider FOLR. This means that an axiom
set A is complete iﬀno FOLR-formula B is independent of A.
We measure the length of an axiom set by the number of characters it contains
in standardized notation.
Deﬁnition 7. An axiom set A of length ≤L is L-complete in FOLR if for any
FOLR-formula B of length ≤L, either B or ¬B follows from A.
Remark 2. Deﬁnition 7 implies that no formula B of length ≤L exists such that
A of length ≤L is strictly implied by B if A is L-complete.
Deﬁnition 8. An axiom set A is minimal if A does not contain any redundant
part that can be eliminated to result in a logically equivalent axiom set.
Deﬁnition 9. A superpostulate (SP) is a minimal and consistent set of inde-
pendent axioms that is L-complete.
It is desirable to deﬁne superpostulates as complete (and not merely L-complete)
theories. However, we abstain from doing so because we intend to deﬁne a prac-
tical algorithm for generating inﬁnity axiom sets. The standard of preserving
completeness, however, faces both practical and theoretical problems, as we will
show in Sect. 4. Thus, we merely presume that the generation of a system of
superpostulates is based on complete theories and aims for completeness rela-
tive to a length L of up to 1000 characters (cf. p. 11).

206
T. Lampert and A. Nakano
Deﬁnition 10. An inﬁnity superpostulate is a satisﬁable superpostulate that
has only inﬁnite models.
Any semi-decider is able to prove that a formula is implied by or inconsistent
with another formula or ﬁnite axiom set. Since these are the only two options
in the case of complete superpostulates, it is decidable whether a given formula
is implied by a complete superpostulate. Furthermore, all (ﬁnite) inﬁnity axiom
sets that are implied by a set of inﬁnity superpostulates are decidable.
From the following axiom set DLOR, all inﬁnity axiom sets of the standard
literature mentioned in Sect. 1 follow:
∀x1¬Rx1x1, ∀x1∃y1Rx1y1, ∀x1∃y1Ry1x1,
∀x1∀x2(¬Rx1x2 ∨∀x3(¬Rx2x3 ∨Rx1x3)),
∀x1∀x2(Rx1x2 ∨∀x3(Rx2x3 ∨¬Rx1x3)),
∀x1∀x2(¬Rx1x2 ∨∃y1(Rx1y1 ∧Ry1x2))
(7)
Axiom 1 (irreﬂexivity), Axioms 2 and 3 (no right endpoint and no left end-
point), Axiom 4 (transitivity) and Axiom 6 (density) are identical to axioms of
DLO without endpoints. However, DLO also contains trichotomy, i.e., the axiom
∀x∀y(Rxy∨Ryx∨x = y), which involves identity. The trichotomy axiom of DLO
is replaced by Axiom 5 (transitivity of ¬R) in DLOR.
Formula (7) contains all of the axioms of the inﬁnity axiom set given in
formula (4). Additionally, Axioms 3, 5 and 6 are added to those in (4). These
axioms are not essential for satisﬁability in an inﬁnite domain.
Deﬁnition 11. An axiom A of a superpostulate SP is essential if SP without
A is no longer an inﬁnity axiom.
The set of the essential axioms plus the negation of an inessential axiom is also
an inﬁnity axiom set, but it is not necessarily a complete one.
In the following, we prove that (7) is a complete superpostulate by proving
the relevant properties.
Theorem 1. (7) is an inﬁnity axiom set.
Proof. If (7) is satisﬁable, then it is satisﬁable only in an inﬁnite domain; this
is because it implies (4), which is known to be an inﬁnity axiom set. Thus,
it remains to be shown that (7) is satisﬁable. This is done by providing the
following model over Q for (7): ℑ(R) = {(x, y) | x < y}. It can be proven that
this interpretation is indeed a model by paraphrasing each axiom. We use the
common forms of the transitivity axioms (Axiom 4 and Axiom 5) and the density
axiom (Axiom 6) for convenience:

Superpostulates
207
Axiom 1
∀x¬Rxx
ℑ(Axiom 1)
All rational numbers (r.n.) are no less than (<) themselves (irreﬂexivity of <)
Axiom 2
∀x1∃y1Rx1y1
ℑ(Axiom 2)
For all r.n. x1, some r.n. y1 exists such that x1 < y1 (no right endpoint)
Axiom 3
∀x1∃y1Ry1x1
ℑ(Axiom 3)
For all r.n. x1, some r.n. y1 exists such that y1 < x1 (no left endpoint)
Axiom 4
∀x1∀x2∀x3(Rx1x2 ∧Rx2x3 →Rx1x3)
ℑ(Axiom 4)
For all r.n. x1, x2, and x3, if x1 < x2 and x2 < x3, then x1 < x3 (transitivity
of <)
Axiom 5
∀x1∀x2∀x3(¬Rx1x2 ∧¬Rx2x3 →¬Rx1x3)
ℑ(Axiom 5)
For all r.n. x1, x2, and x3, if x1 ≮x2 and x2 ≮x3, then x1 ≮x3 (transitivity
of ≮)
Axiom 6
∀x1∀x2(Rx1x2 →∃y1(Rx1y1 ∧Ry1x2))
ℑ(Axiom 6)
For all r.n. x1 and x2, if x1 < x2, then there exists an r.n. y1 such that
x1 < y1 and y1 < x2 (density)
Therefore, (7) is satisﬁable; this is so only in an inﬁnite domain, which means
that (7) is an inﬁnity axiom set.
⊓⊔
Theorem 2. (7) is minimal.
Proof. This is ensured by systematically producing the following:
1. all possible axiom sets A′ from (7) with one axiom eliminated (case 1a) or
with the conjunct in Axiom 6 eliminated (case 1b),
2. all possible axiom sets A′′ with one disjunct in Axioms 4 to 6 eliminated (case
2), and
3. all possible axiom sets A′′′ obtained by (α) reducing the universal quantiﬁers
in Axioms 4 to 6 and (β) replacing Axiom 2 and Axiom 3 with ∀x1Rx1x1 or
∃y1Ry1y1 (case 3).
In case 2 and case 3 (β), the resulting sets A′′ and A′′′ have been proven to
be inconsistent by Vampire in any arbitrary case; in case 3 (α), the resulting
axioms in A′′′ have been proven to be redundant by Vampire. In case 1a, the
results of eliminating Axiom 1 or Axiom 4 have been proven to be ﬁnite axiom
sets by Vampire. Therefore, the resulting axiom sets are not equivalent. Axioms
2, 3, 5 and 6 are not redundant (and, thus, cannot be eliminated) because they
can be proven to be independent axioms by replacing them with their negations,
which, in turn, results in inﬁnity axiom sets. This can be seen by referring to
the following interpretations:

208
T. Lampert and A. Nakano
Axiom 1 Axiom 2 Axiom 3 Axiom 4 Axiom 5 Axiom 6 ℑ
+
¬
+
+
+
+
x < y over Q+
+
+
¬
+
+
+
x < y over Q−
+
+
+
+
¬
+
x < y −1 over Q
+
+
+
+
+
¬
x < y over Z
The conjunct in Axiom 6 is not redundant because eliminating it results in
a redundant axiom, whereas Axiom 6 itself is not redundant.
⊓⊔
Theorem 3. (7) is complete.
Proof. We extend (7) by the following deﬁning axiom of x1 = x2:
∀x1∀x2(x1 = x2 ↔∀x3(Rx1x3 ↔Rx2x3))
(8)
The union of (7) and (8) is equivalent to DLO without endpoints (as proven by
Vampire). Thus, DLO without endpoints is a conservative extension of (7) and,
by conservativeness, (7) is complete if DLO is complete.3 Since DLO without
endpoints is known to be complete, (7) is also complete.
⊓⊔
We say that an inﬁnity axiom set B is weaker than A if B is strictly implied by
A, that is, A ⊢B, but B ⊢A does not hold. Before we consider how to generate
further superpostulates, let us ﬁrst consider the opposite: inﬁnity axiom sets
that are as weak as possible.
3
Weak Inﬁnity Axiom Sets
The set given in formula (5) is the weakest inﬁnity axiom set mentioned in the
standard literature. The following inﬁnite axiom sets, however, strictly follow
from (5):
∀x1∃y1(Rx1y1 ∧∀x2(¬Rx2x1 ∨Rx2y1) ∧¬Ry1y1)
(9)
∀x1∃y1(Rx1y1 ∧¬Ry1x1 ∧∀x2(¬Rx2x1 ∨Rx2y1))
(10)
Formula (9) strictly follows from (5), and (10), in turn, strictly follows from
(9). We ﬁrst prove that (10) is an inﬁnity axiom. We then show that it can
still be systematically weakened without losing the property of being an inﬁnity
axiom set.
Theorem 4. (10) is an inﬁnity axiom.
Proof. We ﬁrst show that there exists a denumerable model of (10) and then
show that (10) has no ﬁnite model.
We assume the natural numbers as the domain and interpret Rxy as x < y.
Then, it is true that for all natural numbers x1, there exists a number y1 such
3 For extensions by deﬁnitions, cf., e.g., [13], section 4.6.

Superpostulates
209
that x1 < y1 and y1 ≮x1, and that for all x2, if x2 < x1, then x2 < y1. Thus, a
denumerable model of (10) exists.
We prove that (10) has no ﬁnite model by contradiction. Suppose that there
is a ﬁnite model M of (10), where the elements of its domain are listed as
m1, . . . , mk. Let n1 = m1. In the following, we consider how to satisfy the three
conjuncts in the scope of ∀x1∃y1. By the ﬁrst conjunct, there must be some n in
the domain such that (n1, n) is in ℑ(R). Let n2 be the ﬁrst element for which this
is true. Thus, we have (n1, n2) ∈ℑ(R). By the second conjunct, we have that
(n2, n1) /∈ℑ(R). It follows that n1 ̸= n2 is in ℑ(R). Again, by the ﬁrst conjunct,
there must be some n such that (n2, n). Let n3 be the ﬁrst element for which
this is the case; thus, we have (n2, n3) ∈ℑ(R) and, by the second conjunct,
(n3, n2) /∈ℑ(R), and therefore, n2 ̸= n3. By the third conjunct, we have either
(n1, n2) /∈ℑ(R) or (n1, n3) ∈ℑ(R). Since we already have (n1, n2) ∈ℑ(R),
the second of these statements, i.e., (n1, n3) ∈ℑ(R), must be true. Then, by
the second conjunct, we have (n3, n1) /∈ℑ(R). Thus, we also have n1 ̸= n3
(in addition to n2 ̸= n3) since we have (n3, n1) /∈ℑ(R) and (n1, n3) ∈ℑ(R).
Continuing in this manner, we obtain n4, which is diﬀerent from all of n1, n2, and
n3; then, we obtain n5, which is diﬀerent from n1 to n4; and so on. However,
by the time we reach nk+1, we will have exceeded the number of elements of
the domain. Thus, our supposition that the domain of M is ﬁnite leads to a
contradiction. Therefore, (10) has no ﬁnite model.
⊓⊔
Formula (10) is a simple and weak inﬁnity axiom that no TPTP engine is
able to decide.4
Theorem 5. From (10), an inﬁnite number of strictly weaker inﬁnity axiom
sets follow.
Proof. We consider the iterative derivation of strictly weaker axioms from (10)
by applying the rule A ⊢B ∨A (∨I), as shown in Table 2.
Table 2. Inﬁnity axioms implied by (10)
No.
Axiom
Rule
(i)
∀x1∃y1(Rx1y1 ∧¬Ry1x1 ∧∀x2(¬Rx2x1 ∨Rx2y1))
(10)
(ii)
∀x1∃y1(Rx1y1 ∧¬Ry1x1 ∧∀x2(∀x3¬Rx3x2 ∨¬Rx2x1 ∨Rx2y1))
∨I
(iii)
∀x1∃y1(Rx1y1∧¬Ry1x1∧∀x2(∀x3(∀x4¬Rx4x3∨¬Rx3x2)∨¬Rx2x1∨Rx2y1))
∨I
(iv)
∀x1∃y1(Rx1y1 ∧¬Ry1x1 ∧∀x2(∀x3(∀x4(∀x5¬Rx5x4 ∨¬Rx4x3) ∨¬Rx3x2) ∨
¬Rx2x1 ∨Rx2y1))
∨I
...
...
...
Lines (ii), (iii), etc., follow from (10) with the application of nothing but ∨I,
which is a valid derivation rule. Each result is, in turn, a primary formula. To
verify that iteratively applying ∨I generates a sequence of weaker inﬁnity axiom
4 This can be checked on the site http://tptp.org/cgi-bin/SystemOnTPTP.

210
T. Lampert and A. Nakano
sets, one must determine how the proof of Theorem 4 still applies. Formula (10)
allows one to directly conclude that n1 ̸= n3 once n1 ̸= n2 and n2 ̸= n3 are
established. Applying ∨I once (cf. line (ii)), however, allows one to conclude
only that either (n1, n3) ∈ℑ(R) or (n4, n2) /∈ℑ(R) (where n4 is the element
introduced by ∀x3, which cannot be presumed to be either identical to or dif-
ferent from n1, n2, or n3). Due to the ﬁrst two conjuncts, n1 ̸= n3 follows from
(n1, n3) ∈ℑ(R), and n2 ̸= n4 follows from (n4, n2) /∈ℑ(R). Considering further
objects of the domain introduces further objects that must be diﬀerent from
all objects in one of the established alternative sets of objects. With each fur-
ther application of ∨I, further alternative sets arise. Thus, each axiom resulting
from ∨I establishes a wider range of objects that cannot be identical. Therefore,
each application allows for less powerful inferences concerning the non-identity
of the elements in pairs satisfying ℑ(R), and thus, each resulting axiom is strictly
weaker. Nevertheless, the resulting axioms still necessitate that at least one of
the newly considered objects (e.g., n3 and n4 in step 2 of the argument after
n1 ̸= n2 is established in step 1) in each step of the argument must be diﬀerent
from at least one of the already considered objects (e.g., n1 and n2 in step 1).
Therefore, the resulting axioms still cannot be satisﬁed under the assumption of
only a ﬁnite number of objects. Consequently, we generate an endless sequence
of increasingly weaker inﬁnity axioms.
⊓⊔
There are many further inﬁnite sequences of non-equivalent inﬁnity axiom
sets that are implied by (7). They all can be proven to be satisﬁable by proving
that they follow from (7).
4
A System of Superpostulates: The DLOR-System
We are searching for a system of inﬁnity superpostulates that is automatically
generated by transformation rules from a complete theory such as (7). The rules
speciﬁed in this section are a starting point for the general project of generating
systems of inﬁnity superpostulates from a given complete axiom set. We do not
claim that our third rule preserves completeness.
Deﬁnition 12. A DLOR-system is a system of inﬁnity superpostulates gener-
ated from (7).
Deﬁnition 13. An inﬁnity axiom set is a DLOR-variant if it is derivable from
a superpostulate of a DLOR-system.
The following two rules yield structurally similar inﬁnity axiom sets of FOLR.
They trivially preserve the property of being a complete inﬁnity axiom set in
the case of FOLR:
Rule 1: Exchange all negated literals for non-negated literals and all non-
negated literals for negated literals.

Superpostulates
211
Rule 2: Exchange the variables in positions 1 and 2 in all atomic propositional
functions.
These rules do not necessarily yield new (non-equivalent) inﬁnity axiom sets.
Rule 1, for example, converts Axiom 4 of superpostulate (7) into Axiom 5 and
vice versa. Rule 2 does not change Axiom 1, does convert Axiom 2 into Axiom
3 and vice versa, and yields equivalent axioms in the cases of Axioms 4 and 5.
In fact, applying these rules to our superpostulate (7) results in only one further
complete superpostulate:
∀x1Rx1x1, ∀x1∃y1¬Rx1y1, ∀x1∃y1¬Ry1x1,
∀x1∀x2(Rx1x2 ∨∀x3(Rx2x3 ∨¬Rx1x3)),
∀x1∀x2(¬Rx1x2 ∨∀x3(¬Rx2x3 ∨Rx1x3)),
∀x1∀x2(Rx1x2 ∨∃y1(¬Rx1y1 ∧¬Ry1x2))
(11)
The crucial problem in considering systems of superpostulates is specifying
rules beyond Rules 1 and 2. Replacing Axiom 2 or Axiom 3 with its negation
results in a complete variant of DLOR with endpoints.5 Thus, one might consider
a rule that replaces axioms with their negations. However, the case in which
completeness is preserved by simply replacing an axiom with its negation is
an exceptional one. This is true only if one cannot replace an axiom A of a
complete theory S with a strictly weaker axiom that is still independent of S
without A. For example, there is no primary formula X that is strictly implied
by ∀x1∃y1Rx1y1 (=Axiom 2) and is not implied by Axiom 1 and Axioms 3–6
of (7). The same does not hold, e.g., for Axiom 1. Therefore, simply replacing
Axiom 1 of (7) with its negation does not preserve completeness since ∃y1¬Ry1y1
could be added as an independent axiom.
To preserve completeness when replacing an axiom A of an axiom set S with
its negation, one must generate an axiom set S′ that is minimally weaker than
S and does not imply A.
Deﬁnition 14. An axiom set S2 is minimally weaker than an axiom set S1 if
S2 is strictly weaker than S1 and no intermediate axiom set S3 exists such that
S3 is strictly weaker than S1 and S2 is strictly weaker than S3.
Only adding the negation of an axiom A of S to S′, which is minimally weaker
than S, preserves completeness. However, the generation of a minimally weaker
axiom set S′ from a given axiom set S gives rise to intricate practical and
theoretical problems. We cannot even prove whether a minimal weaker axiom
set S′ exists for any axiom set S. The main theoretical and practical problem
is that one must specify some upper bound for an axiom set that preserves
completeness if one axiom is replaced with its negation. We leave it as an open
5 The same is not true in the case of negating Axiom 2 and Axiom 3. DLO with
both endpoints is complete only if one adds the axiom ∃y1∃y2y1 ̸= y2. Without this
further axiom, the remaining axioms have a trivial ﬁnite model in a domain with
only one object. We abstain from generating a DLOR-variant with left and right
endpoints.

212
T. Lampert and A. Nakano
question whether this problem is solvable and, if so, how it can be solved. Instead,
we conﬁne our DLOR-system to superpostulates of a certain length.
To do so, we adopt the following two restrictions:
Restriction 1: We set an upper bound for any considered axiom set with a
length L of 1000 characters.
Restriction 2: We make use of an incomplete calculus that does not signiﬁcantly
increase the length of formulas when generating implied formulas.
These restrictions are justiﬁed by the practical aim of generating a system
of inﬁnity superpostulates.
Due to our restrictions, the considered superpostulates are simple enough to
make use of the following assumption:
Assumption 1. If one of the following cases holds, this is proven by the stan-
dard casc-mode of Vampire within a time limit of 300 s:
1. a considered part of a considered axiom set is redundant,
2. a considered minimal axiom set is inconsistent, or
3. the considered axiom set is implied by another considered axiom set or implies
either (10) or (12).
In fact, Vampire is able to almost immediately identify these cases for the simple
axiom sets that we consider here in nearly all cases. By Assumption 1, we assume
that Vampire can solve the semi-decidable problems 1 to 3 for axiom sets with
fewer than 1000 characters to obtain a positive result within 300 s. In other
words, our algorithm runs Vampire with a time limit of 300 s, and if the problem
is not solved within this time limit, then we assume a negative solution. The
reliability of Assumption 1 is based on extensive experience.
To generate strictly weaker axioms A′ from an axiom A by applying an infer-
ence rule, we make use of an incomplete calculus that does not include rules for
the introduction of disjuncts (∨I) and conjuncts (∧I), which would increase the
length of axioms. For this reason, we cannot ensure the generation of minimally
weaker axiom sets and, thus, do not necessarily preserve completeness. Instead,
we consider only rules concerning quantiﬁers: one rule for changing the order of
quantiﬁers (QEx), two for universal quantiﬁer elimination (∀E1 and ∀E2) and
two for existential quantiﬁer introduction (∃I1 and ∃I2); cf. Table 3. The abbre-
viated notations for the rules specify only the relevant syntactic changes. For
example, ∃μ∀ν ⊢∀ν∃μ means that the order of two quantiﬁers is changed in a
primary formula. We tacitly presume that a sequence of universal (existential)
quantiﬁers is orderless; each of the quantiﬁers of such a sequence can be consid-
ered as either the leftmost or rightmost quantiﬁer of the sequence. μ/ν means
that μ is replaced with ν. In the case of ∀E1, ν may also be replaced with the
variable of a new existential quantiﬁer preceding the resulting axiom. ∀νϕ(ν, ν)
indicates that ν occurs more than once in the scope of ∀ν. ϕ(μ/ν, μ) means that
at least one occurrence of μ is replaced with ν. We tacitly presume that new
variables are used if a new quantiﬁer is introduced.

Superpostulates
213
Table 3. Quantiﬁer rules
QEx: ∃μ∀ν ⊢∀ν∃μ
∀E1: ∃μ∀νϕ(μ, ν) ⊢∃μϕ(μ, ν/μ)
∃I1: ∀νϕ(ν, ν) ⊢∀ν∃μϕ(ν, ν/μ)
∃I2: ∃μϕ(μ, μ) ⊢∃μ∃νϕ(μ/ν, μ)
∀E2: ∀μ∀νϕ(μ, ν, ν) ⊢∀μϕ(μ, ν/μ)
We establish the following rules for the application of the quantiﬁer rules.
∀E1 can be applied such that ∃μ is taken from an axiom that diﬀers from A after
having variables renamed and PN laws applied to the eﬀect that ∀ν is directly
to the right of ∃μ. Before ∀E1 can be applied to quantiﬁers of one and the same
axiom, PN laws must be applied to the eﬀect that existential quantiﬁers that are
separated by ∧from other existential quantiﬁers are pulled outwards. The same
holds for universal quantiﬁers that are separated by ∨from other universal quan-
tiﬁers. Thus, the numbers of quantiﬁers in a sequence of existential quantiﬁers
and a sequence of universal quantiﬁers from which ∃μ and ∀ν are to be chosen
are increased. ∀E2 and QEx can also be applied to universal quantiﬁers that
are separated by ∨; QEx additionally can be applied to existential quantiﬁers
that are separated by ∧from the universal quantiﬁer. In these cases, PN laws
are applied to exchange the order of the universal quantiﬁers and ∧or ∨. The
results of the application of these rules are converted into a minimized FOLDNF
in standardized notation (cf. p. 3). One application of a quantiﬁer rule results
in conversion from a set of primary formulas to a set of primary formulas. Thus,
each application comprises the application of equivalence rules to prepare for the
application of QEx, ∀E1 or ∀E2 and to convert the result into an FOLDNF. In
the case in which the resulting FOLDNF is, in fact, a disjunction, we consider
only disjuncts as axiom sets that are strictly implied by the axiom set prior to
the application of a quantiﬁer rule. Furthermore, we consider each conjunct of a
disjunct of an FOLDNF as a separate axiom.
Given an axiom set S that includes axiom A, we consider the totality of all
possible applications of the 5 quantiﬁer rules to A. In the case of ∀E1, this may
involve diﬀerent orders of the prior application of PN laws in addition to the
elimination of the universal quantiﬁed variable by variables bound by diﬀerent
existential quantiﬁers. QEx can be applied to all ∃μ and all ∀ν such that ∃μ
occurs in a sequence of existential quantiﬁers directly to the left of a sequence of
universal quantiﬁers containing ∀ν. ∃I1 and ∃I2 may replace variables in diﬀerent
positions. The totality of possible applications of QEx, ∀E1, ∀E2, ∃I1, and ∃I2
to A realizes all diﬀerent applications of the ﬁve rules.
Our method of generating a DLOR-system is based on all possible non-
redundant applications of the quantiﬁer rules starting from (7).
Deﬁnition 15. An application of a quantiﬁer rule to an axiom A of an axiom
set S is non-redundant if it results in a strictly weaker axiom set S′ and the
axiom A′ that replaces A in S is not implied by S′ without A′.
If, however, S′ implies S or if A′ is not independent in S′, then the application
of a quantiﬁer rule is redundant.

214
T. Lampert and A. Nakano
To ensure that all superpostulates are, in fact, inﬁnity axiom sets, we consider
only superpostulates that imply the very weak inﬁnity axiom set given in (10)
or its counterpart given in (12), as generated by Rule 2:
∀x1∃y1(Ry1x1 ∧¬Rx1y1 ∧∀x2(¬Rx1x2 ∨Ry1x2))
(12)
DLOR without endpoints, with a left endpoint, and with a right endpoint all
imply either (10) or (12).
By Assumption 1 and our quantiﬁer rules, it is possible to apply the following
algorithm to generate a set S∗of further superpostulates from a superpostulate
S.
Algorithm 1. Generate S∗from S, which includes A, as follows:
1. Set S∗= {}.
2. Specify the set A′ of all non-redundant applications of the quantiﬁer rules to
A.
3. Generate the powerset A′′ of A′.
4. Traverse the members A1, A2, . . . , Ak of A′′, beginning with the largest set
A′. Replace A in S with the members of Ai and denote the result by S′; if
Ai = {}, then S′ = S without A.
(a) Delete redundant axioms and redundant subexpressions in S′.
(b) If S′, ¬A is fewer than 1000 characters and consistent, then delete all
proper subsets of Ai in A′′.
(c) If ¬A is additionally independent of S′, then denote the conversion of the
resulting axiom set S′ ∪{¬A} set into standard notation by S′′.
(d) If S′′ implies (10) or (12), then append S′′ to S∗.
Remark 3. For simplicity, our implemented algorithm in fact considers only A′
(instead of its powerset A′′) and merely deletes redundant axioms (and no other
subexpressions) in S′.
Example 2. Applying Algorithm 1 to Axiom 1 of (7) results in a set S∗with the
following superpostulate as its only member:
∃y1Ry1y1, ∀x1∃y1¬Rx1y1, ∀x1∃y1¬Ry1x1,
∀x1∃y1(Rx1y1 ∧¬Ry1y1), ∀x1∃y1(Ry1x1 ∧¬Ry1y1),
(13)
Axioms 4, 5, and 6 of (7)
Note that the application of ∀E1 to replace x1 in Axiom 1 of (7) with y1 in
Axiom 6 is redundant. Set (13) can, in turn, serve as a starting point for the
application of Algorithm 1. Applying Algorithm 1 to Axiom 1 of (13) results in
(7) once again (consider the implied minimization strategies!).
Let us use S∗∗to denote the result of applying Algorithm 1 to all of the
axioms of S; S∗∗is the union of all results S∗. Furthermore, let us use ST to
denote the totality of the generated superpostulates. Finally, we use S∗∗∗to
denote the subset of S∗∗that contains only superpostulates that are not implied

Superpostulates
215
Fig. 1. Flowchart of Rule 3
by superpostulates in ST . Figure 1 provides a ﬂowchart of our general algorithm
(Rule 3).
Based on Algorithm 1, we generate a DLOR-system by means of the following
rule:
Rule 3: Starting from (7), generate inﬁnity superpostulates by iteratively
applying Algorithm 1 to all axioms of all generated superpostulates until
no further superpostulates are generated. Restriction: Delete superpostulates
if they are implied by previously generated superpostulates.
The restriction ensures that no other inﬁnity axiom set of ST strictly implies
S; this is the best we can do to generate L-complete inﬁnity axiom sets up to
a length of 1000 characters. Furthermore, the restriction avoids repetitions and
loops (cf. the application of Algorithm 1 to Axiom 1 of (13) in Example 2).
Remark 4. With our implementation of Rule 3, we have generated 732 super-
postulates in approximately 1000 h of run time. We have generated a database of
2196 inﬁnity superpostulates in TPTP format by applying Rules 1 to 3 starting
with DLOR. None of these superpostulates can be currently solved by any of the
TPTP-engines. Additionally, we have generated a database of 60 weak inﬁnity
DLOR-variants. 8 of them can be solved by SPASS within 60 s.
5
The THSR-System
A prominent alternative complete theory to start with is the theory of immediate
successors (THS), which can be deﬁned as follows in familiar notation:
∀x¬Rxx,
∀x∃y∀z(Rxz ↔z = y),
∃x∀y(∀z¬Rzy ↔y = x),
∀x(∃yRyx →∃y∀z(Rzx →z = y))
(14)
The following reduction THSR of THS to FOLR can be proven to be complete
by adding the deﬁning axiom ∀x∀y(x = y ↔∀z(Rxz ↔Ryz)∧∀z(Rzx ↔Rzy))
and proving the equivalence to (14):

216
T. Lampert and A. Nakano
∀x1¬Rx1x1, ∃y1∀x1¬Rx1y1, ∀x1∃y1Rx1y1,
∀x1∀x2(∀x3(¬Rx1x3 ∨¬Rx2x3) ∨∀x4(¬Rx4x1 ∨Rx4x2)),
(15)
∀x1∀x2(∀x3(¬Rx3x1 ∨¬Rx3x2) ∨∀x4(¬Rx1x4 ∨Rx2x4)),
∀x1∀x2((∀x3(¬Rx1x3 ∨Rx2x3) ∨∃y1Ry1x2) ∨∃y2Ry2x1)
Vampire is able to solve (14) by saturation in 300 s, but it cannot solve (15)
or variants thereof. Axioms 1, 2, and 3 of THSR are part of DLOR with a left
endpoint, and Axiom 6 follows from Axiom 5 of DLOR. Axioms 4 and 5 of THSR
contradict Axioms 1, 2, and 6 of DLOR. Axioms 4, 5, and 6 of DLOR contradict
Axioms 1, 2, and 4 of THSR.
Remark 5. Our algorithm has generated a database of 156 superpostulates from
(15) and terminated after approximately 200 h of run time. 32 of them are solved
by CVC4. None of the 156 superpostulates implies any of the superpostulates of
the DLOR-system or any of the weak DLOR-variants.
6
Conclusion
From a theoretical point of view, it would be desirable to generate a database
of complete inﬁnity axiom sets. However, this would involve the problem of
generating minimally weaker inﬁnity axiom sets that may be of arbitrary length.
Here, our intent is to provide a pragmatic algorithm that, in fact, generates
signiﬁcant inﬁnity axiom sets. Thus, we restrict the generated inﬁnity axiom sets
to a certain length. This restriction does not imply that the generated sets are
not complete theories; rather, it means only that we are unable to guarantee that
they are. Our algorithm starts with a complete theory of FOLR, such as DLOR or
THSR, and automatically generates further inﬁnity axiom sets S with fewer than
1000 characters such that, roughly speaking, no other inﬁnity axiom sets with
fewer than 1000 characters exist that strictly imply inﬁnity axiom sets S. We say
“roughly speaking” here because in addition to limiting the set length, we use
an incomplete calculus without inference rules that would signiﬁcantly increase
complexity, and we test the restriction only on the basis of already generated
entries in our database. We believe that the theoretical question concerning an
(unrestricted) algorithm that does, in fact, preserve completeness is an important
one, but we do not aim to answer it in this paper. Instead, our ﬁnal objective
is a practical algorithm, and we maintain that we have achieved this aim. Thus
far, we have generated more than 2250 superpostulates in 10 weeks of run time.
All of these superpostulates are hard problems.
Note that it is not trivial to automatically generate inﬁnity axiom sets. Inﬁn-
ity axiom sets (not to say systems of inﬁnity axiom sets) (i) are rarely encoun-
tered, (ii) can seldom be solved by logic engines, (iii) are hard problems even
when they are short in length, and (iv) have not been systematically studied;
moreover, (v) only a few of them exist in the TPTP library, and no systematic
archive of inﬁnity axiom sets is available. Therefore, we believe that our database
is of practical importance for (i) systematically studying inﬁnity axiom sets, (ii)

Superpostulates
217
deciding the satisﬁability of additional inﬁnity axiom sets that follow from our
database, and (iii) improving saturation algorithms by enabling the decidabil-
ity of inﬁnity axiom sets. In regard to (i), we are investigating the question
of whether starting our algorithm with diﬀerent complete FOLR theories will
yield identical superpostulates. MacPherson conjectured that any ω-categorical
axiom set has the strict order property (cf. [10] and [4] for detailed discussions).
Proofs of the property of being ω-categorical widely depend on identity. In light
of MacPherson’s conjecture, it would be interesting to study the limits of the
DLOR-system. We have also reduced Goldbach’s conjecture to FOLR and intend
to study its internal relations with other inﬁnity axiom sets in our still-growing
database. In regard to (ii), we have proven in Sect. 3 that an inﬁnite number
of additional inﬁnity axiom sets follow from our superpostulates. A database of
superpostulates will make it possible to decide the satisﬁability of inﬁnity axiom
sets that are implied by one of the entries of the database. In regard to (iii),
we are currently working on saturation algorithms that can cope with inﬁnity
axiom sets. We know from our work how important it is to test such algorithms
on a variety of intricate examples during development.
Acknowledgement. We thank the three anonymous referees for their comments and
GeoﬀSuttcliﬀe for checking our database with CVC4, E, Inﬁnox, SPASS and Vampire.
References
1. B¨orger, E., Gr¨adel, E., Gurevich, Y.: The classical decision problem. J. Logic Lang.
Inform. 8, 478–481 (2001). https://doi.org/10.1023/A:1008334715902
2. Boolos, G.S., Burgess, J.P., Jeﬀrey, R.C.: Computability and Logic, 4th edn. Cam-
bridge University Press, Cambridge (2003)
3. Claessen, K., Lillestr¨om, A.: Automated inference of ﬁnite unsatisﬁability. J.
Autom. Reason. 47, 111–132 (2011)
4. Djordjevic, M.: On ﬁrst-order sentences without ﬁnite models. J. Symb. Log. 69(2),
329–339 (2004)
5. Dreben, B., Goldfarb, W.D.: The Decision Problem. Solvable Classes of Quantiﬁ-
cational Formulas. Addison-Wesley, London (1979)
6. Hilbert, D., Bernays, P.: Grundlagen der Mathematik, vol. 1. Springer, Berlin
(1970)
7. Kalm´ar, L.: Zur¨uckf¨urung des Entscheidungsproblems auf den Fall von Formeln
mit einer einzigen, bin¨aren Funktions variablen. Compos. Math. 4, 137–144 (1936)
8. Lampert, T.: Minimizing disjunctive normal forms of pure ﬁrst-order logic. Log. J.
IGPL 25(3), 325–347 (2017)
9. Langford, C.: Analytic completeness of sets of postulates. Proc. London Math. Soc.
25, 115–142 (1926)
10. Macpherson, D.: Finite axiomatizability and theories with trivial algebraic closure.
Notre Dame J. Form. Log. 32(2), 188–192 (1991)
11. Quine, W.V.O.: Methods of Logic, 4th edn. Harvard UP, Cambridge (1982)
12. Sheﬀer, H.M.: The general theory of notational relativity. Typewritten manuscript
of 61 pp. Harvard Widener Library (1921)
13. Shoenﬁeld, J.R.: Mathematical Logic. Addison-Wesley, Reading (1967)
14. Urquhart, A.: Henry M. Sheﬀer and notational relativity. Hist. Philos. Log. 33(1),
33–47 (2012)

A Decision Procedure for String to Code
Point Conversion
Andrew Reynolds1
, Andres N¨otzli2(B)
, Clark Barrett2
,
and Cesare Tinelli1
1 Department of Computer Science, The University of Iowa, Iowa City, USA
2 Department of Computer Science, Stanford University, Stanford, USA
noetzli@cs.stanford.edu
Abstract. In text encoding standards such as Unicode, text strings are
sequences of code points, each of which can be represented as a natural
number. We present a decision procedure for a concatenation-free theory
of strings that includes length and a conversion function from strings
to integer code points. Furthermore, we show how many common string
operations, such as conversions between lowercase and uppercase, can
be naturally encoded using this conversion function. We describe our
implementation of this approach in the SMT solver CVC4, which con-
tains a high-performance string subsolver, and show that the use of a
native procedure for code points signiﬁcantly improves its performance
with respect to other state-of-the-art string solvers.
1
Introduction
String processing is an important part of many kinds of software. In particular,
strings often serve as a common representation for the exchange of data at inter-
faces between diﬀerent programs, between diﬀerent programming languages, and
between programs and users. At such interfaces, strings often represent values
of types other than strings, and developers have to be careful to sanitize and
parse those strings correctly. This is a challenging task, making the ability to
automatically reason about such software and interfaces appealing. Applications
of automated reasoning about strings include ﬁnding or proving the absence of
SQL injections and XSS vulnerabilities in web applications [27,30,33], reasoning
about access policies in cloud infrastructure [7], and generating database tables
from SQL queries for unit testing [31]. To make this type of automated reasoning
scalable, several approaches for reasoning natively about string constraints have
been proposed [3,4,11,20,21].
To reason about complex string operations such as conversions between
strings and numeric values, string solvers typically reduce these operations to
operations in some basic fragment of the theory of strings which they support
natively. The scalability of a string solver thus depends on the eﬃciency of the
This work was partially funded by Amazon Web Services.
c
⃝Springer Nature Switzerland AG 2020
N. Peltier and V. Sofronie-Stokkermans (Eds.): IJCAR 2020, LNAI 12166, pp. 218–237, 2020.
https://doi.org/10.1007/978-3-030-51074-9_13

A Decision Procedure for String to Code Point Conversion
219
reductions as well as the performance of the solver over the basic constraints.
In such approaches, the set of operations in the basic fragment of strings has
to be chosen carefully. If the set is too extensive, the implementation becomes
complex and its performance as well as its correctness may suﬀer as a result.
On the other hand, if the set is too restrictive, the reductions may become too
verbose or only approximate, also leading to suboptimal performance. In current
string solvers, basic constraints typically include only word equations (i.e., equal-
ities between concatenations of variables and constants) and length constraints.
Certain operations, however, such as conversions between strings and numeric
values, cannot be represented eﬃciently in this fragment because the encoding
requires reasoning by cases on the concrete characters that may occur in the
string values assigned to a string variable.
In this work, we investigate extending the set of basic operators supported in
a modern string solver to bridge the gap between character and integer domains.
We assume a ﬁnite character domain of some cardinality n and, similarly to the
Unicode standard, we assume a bijective mapping between its character set and
the ﬁrst n natural numbers which associates each character with a unique code
point. We introduce then a new string operator, code, from characters strings to
integers which can be used to encode the code point value of strings of length one
and, more generally, reason about the code point of any character in a string. We
propose an approach that involves extending a previous decision procedure with
native support for this operator, obtaining a new decision procedure which avoids
splitting on character values. Using the code operator, we can succinctly repre-
sent string operations including common string transducers, conversion between
strings and integers, lexicographic ordering on strings, and regular expression
membership constraints involving character ranges. We have implemented our
proposed decision procedure in the state-of-the-art SMT solver cvc4 as an exten-
sion of its decision procedure for word equations by Liang et al. [21]. We have
modiﬁed cvc4’s reductions to take advantage of code. Using benchmarks gen-
erated by the concolic execution of Python code, we show that our technique
provides signiﬁcant beneﬁts compared to doing case splitting on values.
To summarize, our contributions are as follows:
– We provide a decision procedure for a simple set of string operations contain-
ing length and a code point conversion function code, and prove its correctness.
We describe how it can be combined with existing procedures for other string
operators.
– We demonstrate how the code operator can be used in the reductions of
several classes of useful string constraints.
– We implement and evaluate our approach in cvc4, showing that it leads to
signiﬁcant performance gains with respect to the state of the art.
In the following, we discuss related work. We then describe a fragment of the
theory of strings in Sect. 2 that includes code. In Sect. 3, we provide a decision
procedure for this fragment, prove its correctness, and describe how it can be
integrated with existing decision procedures. Finally, we discuss applications

220
A. Reynolds et al.
Fig. 1. Functions in signature ΣAS. Str and Int denote strings and integers, respectively.
of reasoning about code points in Sect. 4 and evaluate our implementation in
Sect. 5.
Related Work. The study of the decidability of diﬀerent fragments of string
constraints has a long history. We know that solvability of word equations over
unbounded strings is decidable [23], whereas the addition of quantiﬁers makes
the problem undecidable [25]. The boundary between decidable and undecidable
fragments, however, remains unclear—a long-standing open question is whether
word equations combined with equalities over string lengths are decidable [14].
Adding extended string operators such as replace [13] or conversions between
strings and integers [18] leads to undecidability. Weakly chaining string con-
straints make up one decidable fragment. This fragment requires that the graph
of relational constraints appearing in the constraints only contains limited types
of cycles. It generalizes the straight-line fragment [22], which disallows equalities
between initialized string variables, and the acyclic fragment [5], which disallows
equalities involving multiple occurrences of a string variable and does not include
transducers.
In practice, string solvers have to deal with undecidable fragments or frag-
ments of unknown decidability, so current solvers for strings such as cvc4 [9],
z3 [16], z3str3 [11] and Trau [3] implement eﬃcient semi-decision procedures.
In this work, we present a decision procedure that can be combined modularly
with those procedures.
2
Preliminaries
We work in the context of many-sorted ﬁrst-order logic with equality and assume
the reader is familiar with the notions of signature, term, literal, (quantiﬁed)
formula, and free variable (see, e.g., [17]). We consider many-sorted signatures
Σ that contain an (inﬁx) logical symbol ≈for equality—which has type σ × σ
for all sorts σ in Σ and is always interpreted as the identity relation. A theory
is a pair T = (Σ, I) where Σ is a signature and I is a class of Σ-interpretations,
the models of T. A Σ-formula ϕ is satisﬁable (resp., unsatisﬁable) in T if it is
satisﬁed by some (resp., no) interpretation in I. Given a (set of) terms S, we write
T (S) to denote the set of subterms of S. By convention and unless otherwise
stated, we use letters x, y, z to denote variables and s, t to denote terms.
We consider a theory TAS of strings with length and code point functions,
with a signature ΣAS given in Fig. 1. We ﬁx a ﬁnite totally ordered set A of
characters as our alphabet and deﬁne TAS as a set of ΣAS-structures with universe
A∗(the set of all words over A) which diﬀer only on the value they assign to
variables. The signature includes the sorts Str and Int, interpreted as A∗and Z,

A Decision Procedure for String to Code Point Conversion
221
respectively. Figure 1 partitions the signature ΣAS into the subsignatures ΣA and
ΣS, as indicated. The ﬁrst includes the usual symbols of linear integer arithmetic,
interpreted as expected. We will write t1 ▷◁t2, with ▷◁∈{>, <, ⩽}, as syntactic
sugar for the equivalent inequality between t1 and t2 expressed using only ⩾.
The subsignature ΣS includes: all the words of A∗(including the empty word ϵ)
as constant symbols, or string constants, each interpreted as itself; a function
symbol len : Str →Int, interpreted as the word length function; and a code point
function whose semantics is deﬁned as follows.
Deﬁnition 1. Given
alphabet
A
and
its
associated
total
order
<,
let
(c0, . . . , cn−1) be the enumeration of A induced by < (with ci < ci+1 for all
i = 0 . . . , n −2). For each character ci in the enumeration, we refer to i as its
code point. The function symbol code : Str →Int is interpreted in TAS as the
unique code point function code such that:
1. for all words w ∈A1, code(w) is the code point of the (single) character of
w, and
2. for all other words w ∈A∗, code(w) is −1.
The code point function can be used in practice to reason about the code
point values of Unicode strings.1 We will see in Sect. 4 that this operator is very
useful for encoding constraints that occur in applications. We stress, however,
that the procedure presented in this paper is agnostic with respect to the concrete
alphabet A and its character ordering.
Note that we do not consider string concatenation in the signature above.
This omission is for the sake of modularity; also, procedures for word equations
have been addressed in a number of recent works [4,21]. In practice, our proce-
dure for string constraints involving code can be naturally combined with existing
procedures for a signature that includes string concatenation, as we discuss in
Sect. 3.1.
An atomic term is either a constant or a variable. A string term is either a
constant or one that contains function symbols from ΣS only. Notice that integer
constants are string terms. A string constraint is a (dis)equality between string
terms. An arithmetic constraint is an inequality or (dis)equality between linear
combinations of atomic and/or string terms with integer sort. Notice that the
equality code(x) ≈code(y) with variables x and y is both a string constraint
and an arithmetic constraint.
3
A Decision Procedure for String to Code Point
Conversion
In this section, we introduce a decision procedure for a fragment of string con-
straints involving code but not containing string concatenation. In particular,
we introduce a decision procedure for sets of (quantiﬁer-free) ΣAS-constraints
1 For technical details on Unicode see [28].

222
A. Reynolds et al.
for the signature introduced in Fig. 1. A key property of this procedure is that
it is able to reason about terms of the form code(x) without having to do case
splitting on concrete values for string x.
Following Liang et al. [21], we describe this procedure as a set of derivation
rules that modify conﬁgurations of the form ⟨A, S⟩, where A is a set of arithmetic
constraints, and S is a set of string constraints. At a high level, the procedure can
be understood as a cooperation between two subsolvers, an arithmetic subsolver
and a string subsolver, which handle these two sets respectively. Our procedure
assumes the following preconditions on ⟨A, S⟩and maintains them as an invariant
for all derived conﬁgurations:
1. A ∪S contains no terms of the form len(l) or code(l) for any string literal l.
2. For every string literal l ∈T (A ∪S), the set S contains x ≈l for some
variable x.
The above restrictions come with no loss of generality since terms of the form
len(l) and code(l) can be replaced by an equivalent (constant) integer, and fresh
variables can be introduced as necessary for the second requirement.
We present the rules of the procedure in two parts, given in Figs. 2 and 3. The
rules are given in guarded assignment form, where the top of the rule describes
the conditions under which the rule can be applied, and the bottom of the rule
either is unsat, or otherwise describes the resulting modiﬁcations to the com-
ponents of our conﬁguration. A rule may have multiple, alternative conclusions
separated by ∥. In the premises of the rules, we write S |= ϕ to denote that
S entails formula ϕ in the empty theory. This can be checked using a standard
algorithm for congruence closure, where string literals are treated as distinct
values; thus S |= l1 ̸≈l2 for any S, l1 ̸= l2. Observe that, for f ∈{len, code},
S |= f(x) ≈f(y) iﬀS |= x ≈y.
An application of a rule is redundant if it has a conclusion where each compo-
nent in the derived conﬁguration is a subset of the corresponding component in
the premise conﬁguration. A conﬁguration other than unsat is saturated if every
possible application of a derivation rule to it is redundant. A derivation tree is
a tree where each node is a conﬁguration whose children, if any, are obtained
by a non-redundant application of a rule of the calculus. A derivation tree is
closed if all of its leaves are unsat. We show later that a closed derivation tree
with root node ⟨A, S⟩is a proof that A ∪S is unsatisﬁable in TAS. In contrast, a
derivation tree with root node ⟨A, S⟩and a saturated leaf is a witness that A ∪S
is satisﬁable in TAS.
Figure 2 presents rules adapted from previous work [21,26] that model the
interaction between the string and arithmetic subsolvers. First, either subsolver
can report that the current set of constraints is unsatisﬁable by the rules A-Conf
or S-Conf. For the former, the entailment |=LIA can be checked by a standard
procedure for linear integer arithmetic. The rules A-Prop and S-Prop correspond
to a form of Nelson-Oppen-style theory combination between the two subsolvers.
In particular, each theory solver propagates entailed equalities between terms
of type Int. The next two rules ensure that length constraints are satisﬁed. In
particular, L-Intro ensures that the length of a term x is equal to the length of

A Decision Procedure for String to Code Point Conversion
223
Fig. 2. Core derivation rules.
Fig. 3. Code point derivation rules.
string literals x is equated to in S. We write (len(l))↓to denote the constant
integer corresponding to the result of evaluating the expression len(l). The rule
L-Valid has two conclusions. It ensures that either x is the empty string or the
value assigned to len(x) is positive. Finally, since our alphabet is ﬁnite, the rule
Card is used to determine when a length constraint is implied due to the number
of distinct terms of a given length. In particular, if there are n distinct variables
x1, . . . , xn whose length is the same, then either xi is equal to xj for some i ̸= j,
or their length must be large enough so that they each can be assigned a unique
string value. The lower bound on their length is determined by taking the ﬂoor of
the logarithm of n−1 base the cardinality of the alphabet, where this expression
denotes an integer constant.2
Figure 3 lists rules for reasoning about the code point function. In C-Intro, if a
string variable x is equal to a string literal of length one, we add to S an equality
between code(x) and the concrete value of the code point of l. An equality of
this form is also added via the rule C-Collapse if a string term x is equated in
S to a string literal and, in addition, code(x) occurs in S. Rule C-Valid splits
on whether an instance of code(x) from S is equal to a valid code point. The
left conclusion considers the case where the code point is −1, which means that
x must have a length diﬀerent from 1. The right conclusion considers the case
where the code point is between 0 and |A|−1, meaning that x is a one-character
string. Finally, rule C-Inj reﬂects the fact that code denotes an injective function
over the domain of strings of length 1. More precisely, it captures the fact that
2 In the degenerate case where the cardinality of the alphabet is one, we assume this
branch is omitted from the conclusion since logarithm base one is undeﬁned.

224
A. Reynolds et al.
for any pair of string values lx and ly for x and y respectively, one of the following
(non-necessarily disjoint) cases always holds: (i) lx has a length diﬀerent from 1,
(ii) lx has length 1 and diﬀers from ly, and has a diﬀerent code point from that
of ly, or (iii) lx and ly are the same.
We now demonstrate the procedure with a few simple examples. Recall that
we assume a ﬁxed alphabet A and write cn to denote a character from this
alphabet whose code point is some n between 0 and the cardinality of A minus
one.
Example 1. Let A0 be {len(x) > len(y), code(x) ≈code(y), code(x) ⩾0} and let
S0 be ∅. We can generate the following closed derivation tree with root ⟨A0, S0⟩.
At each node, we list the new constraint that is added to the conﬁguration at
that node. All the leaf nodes are derived by A-Conf (not shown in the tree).
⟨A, S⟩:= ⟨A0, S0⟩
code(x) ≈code(y) ∈S A-Prop
code(x) ≈−1 ∈A
unsat
code(x) ̸≈code(y) ∈A
unsat
x ≈y ∈S
len(x) ≈len(y) ∈A
unsat
S-Prop
C-Inj
First, since A |=LIA code(x) ≈code(y), we apply A-Prop which adds the equality
code(x) ≈code(y) to S. Subsequently, since code(x), code(y) ∈T (S), we apply
C-Inj which considers three cases. The ﬁrst two branches result in the arithmetic
component of our conﬁguration A being unsatisﬁable, and thus unsat may be
derived by A-Conf. In the third branch, we consider the case where x is equal
to y. We have that S entails that len(x) ≈len(y), and hence, by S-Prop, this
equality is added to A. Since len(x) > len(y) is already in A, we can derive unsat
in this branch by A-Conf as well. Since there is a closed derivation tree with root
⟨A0, S0⟩, A0 ∪S0 is unsatisﬁable in TAS.
⊓⊔
Example 2. Let A0 be {97 ⩽code(x) ⩽106} and let S0 be {x ̸≈y, x ̸≈z, y ≈
c97, z ≈c106}. We may obtain a derivation tree with root ⟨A0, S0⟩and a satu-
rated conﬁguration ⟨A, S⟩where A extends A0 with the constraints:
{code(x) ̸≈code(y), code(x) ̸≈code(z), code(y) ≈97, code(z) ≈106}
The constraints code(x) ̸≈code(y) and code(x) ̸≈code(z) may be obtained by
C-Inj, and code(y) ≈97 and code(z) ≈106 may be obtained by C-Intro. Since a
saturated conﬁguration exists in a derivation tree with root node ⟨A0, S0⟩, we
have that A0 ∪S0 is satisﬁable in TAS. As we show in Theorem 1 (below), a
model for A0 ∪S0 can be obtained by constructing an arbitrary model for A ∪S.
In particular, notice that due to our derived constraints, it must be the case that
code(x) is assigned a value in the range [98 . . . 105]. Indeed, a model M exists
for A0 ∪S0, where M(x) = ck, M(y) = c97, and M(z) = c106, for any k in
the range [98 . . . 105]. Note that we do not explicitly case split on the value of x.
Instead, as we later describe in Deﬁnition 2, our procedure assigns a value to x
based on the value that the arithmetic subsolver gives to code(x).
⊓⊔

A Decision Procedure for String to Code Point Conversion
225
Example 3. Let A0 be {48 ⩽code(x) < 58, len(x) < 1} and let S0 be ∅. We may
obtain the following closed derivation tree with root ⟨A0, S0⟩.
⟨A, S⟩:= ⟨A0, S0⟩
x ≈ϵ ∈S
code(x) ≈−1 ∈A
unsat
A-Conf
C-Collapse
len(x) > 0 ∈A
unsat
A-Conf
L-Valid
Since x is a string term from T (A ∪S), we apply L-Valid. The left branch considers
the case where x is empty. Since S |= x ≈ϵ, we apply C-Collapse which adds
code(x) ≈code(ϵ)↓= −1 to A. This makes A unsatisﬁable, and we can derive
unsat by A-Conf. In the right branch, we consider the case that len(x) > 0, which
results in a case where A is unsatisﬁable since len(x) < 1 ∈A. Thus, A0 ∪S0 is
unsatisﬁable in TAS.
⊓⊔
Example 4. Let A0 = {0 ⩽code(x) < len(x)} and S0 = ∅. We may obtain
a saturated conﬁguration ⟨A, S⟩where A extends A0 with {len(x) ≈1, 0 ⩽
code(x) < |A|}. These constraints are obtained by considering the right branch
of an application of C-Valid since code(x) ∈T (S) (after the trivial propagation
code(x) ≈code(x) by A-Prop). The only models for A ∪S are those where code(x)
is assigned the value for 0; hence the only models M for A ∪S (and hence A0 ∪S0)
are where M(x) = c0.
⊓⊔
We now discuss the formal properties of our calculus, proving that it is refutation-
sound, model-sound, and terminating for any set of ΣAS-constraints, and thus
yields a decision procedure. We also show that, for any saturated conﬁguration, it
is possible to construct a model for the input constraints based on the procedure
given in the following deﬁnition. In each step, we argue the well-formedness
of this construction. In the subsequent theorem, we show that the constructed
model indeed satisﬁes our input constraints.
Deﬁnition 2 (Model Construction). Let ⟨A, S⟩be a saturated conﬁguration.
Construct a model M for A ∪S based on the following steps.
1. Let U be the set of terms of the form len(x) or code(x) that occur in A. Let Z
be a model of A′, where A′ is the result of replacing in A each of its subterms
t ∈U with a fresh integer variable ut. Notice that Z exists, since A-Conf does
not apply to our conﬁguration, meaning that A (and hence A′) is satisﬁable
in LIA.
2. Construct M by assigning values to the variables in A ∪S in the following
order. Below, let S denote the congruence closure of S.3
(a) For all integer variables x, set M(x) = Z(x).
(b) For all string equivalence classes e ∈S that contain a string constant l
(including the case where l = ϵ), set M(y) = l for all variables y ∈e.
Notice that l is unique since S-Conf does not apply to our conﬁguration.
3 That is, the equivalence relation over T (S) such that s, t are in the same equivalence
class if and only if S |= s ≈t.

226
A. Reynolds et al.
(c) For all string equivalence classes e ∈S, such that Z(ulen(z)) = 1 and
code(z) ∈T (S) for some z ∈e, we let M(y) = ck for each variable y ∈e,
where k = Z(ucode(z)). Since C-Valid cannot be applied to our conﬁguration,
it must be the case that A′ contains the constraint 0 ⩽ucode(z) < |A|.
Since Z satisﬁes A′, the value of Z(ucode(z)) is guaranteed to be a valid
code point and thus ck is indeed a character in A.
(d) For all remaining unassigned string equivalence classes e ∈S, we have
that len(z) ∈T (A) for all variables z ∈e, since L-Valid cannot be applied
to our conﬁguration. We choose some l of length Z(ulen(z)), such that l is
not already assigned to any other string variable in M, and set M(y) = l
for all variables y ∈e. Since our conﬁguration is saturated with respect to
Card, we know that at least one such string literal exists: if the set of string
literals of length Z(ulen(z)) were each in the range of M, it would imply
that there are |A|Z(ulen(z)) + 1 distinct terms whose length is Z(ulen(z)),
in which case Card would require len(z) to be greater than the value of
⌊log|A| (|A|Z(ulen(z)) + 1 −1)⌋= Z(ulen(z)). However, this is not the case
since A is satisﬁable in LIA.
Theorem 1. Let M = A0 ∪S0 be a set of ΣAS-constraints where A0 are arith-
metic constraints and S0 are non-arithmetic constraints. The following state-
ments hold.
1. There is a closed derivation tree with root ⟨A0, S0⟩only if M is unsatisﬁable
in TAS.
2. There is a derivation tree with root ⟨A0, S0⟩containing a saturated conﬁgura-
tion only if M is satisﬁable in TAS.
3. All derivation trees with root ⟨A0, S0⟩are ﬁnite.
Proof. To show (1), assume there exists a model M of A0 ∪S0. It is straight-
forward to show that for every rule of the calculus, applying that rule to any
node ⟨A, S⟩results in a tree where at least one child ⟨A′, S′⟩is such that M also
satisﬁes A′ ∪S′. Thus, by induction on the size of the derivation tree, there
exists at least one terminal node that is not closed. Thus, if there exists a closed
derivation tree with root node ⟨A0, S0⟩, then it must be the case that no model
exists for A0 ∪S0, so M is unsatisﬁable in TAS.
To show (2), assume there exists a derivation tree with a saturated con-
ﬁguration ⟨A, S⟩. Let M be the model constructed based on the procedure in
Deﬁnition 2. Below, we argue that M is a model for A ∪S, which is a superset
of A0 ∪S0 and thus satisﬁes M. Let U and Z respectively be the set of terms
and model as computed in Step 1 of Deﬁnition 2. Below, we show that M is a
model for each constraint in A ∪S.
– To show M satisﬁes each constraint in A, we show Z(x) = M(x · σ) for all
integer variables x, where σ is the substitution {ut →t | ut ∈U}.
• Consider the case x = ulen(y) for some y, that is, x is a variable introduced
in Step 1 of Deﬁnition 2 for an application of a length term. If y was
assigned a value in Step 2(b) of Deﬁnition 2, then M(y) = l for some l

A Decision Procedure for String to Code Point Conversion
227
such that S |= y ≈l. Since L-Intro cannot be applied to our conﬁguration,
we have that len(y) ≈(len(l))↓∈A, and hence Z(ulen(y)) = Z(len(l)↓) =
M(len(y)). If y was assigned in Step 2(c) or 2(d), we have that M(y) = l
for some l whose length is Z(ulen(y)), and hence Z(ulen(y)) = M(len(y)).
• Consider the case x = ucode(y) for some y. If y was assigned in Step 2(b)
of Deﬁnition 2, then M(y) = l for some l such that S |= y ≈l. Since
C-Collapse cannot be applied to our conﬁguration, we have that code(y) ≈
(code(l))↓∈A and hence Z(ucode(y)) = M(code(l)↓) = M(code(y)). If y
was assigned in Step 2(c) or 2(d), we have that M(y) = l for some l whose
length is Z(ulen(y)). If it was assigned in Step 2(c), we have that l = ck for
k = Z(ucode(y)) and hence Z(ucode(y)) = M(code(y)). If y was assigned
in Step 2(d), we have that code(y) ∈T (S). Since y was not assigned in
Step 2(c), it must be the case that Z(ulen(y)) ̸= 1. Since C-Valid cannot be
applied, and since code(y) ∈T (S), we have, by its left conclusion, that
len(y) ̸≈1 and code(y) ≈−1 are in A. Due to the former constraint,
Z(ulen(y)) ̸= 1 and the length of l is not one, and thus M(code(y)) = −1.
Due to the latter constraint, Z(ucode(y)) = −1 as well.
• For all other x : Int, we have that Z(x) = M(x) by Step 2(a) of Deﬁni-
tion 2.
In all cases above, we have shown that Z(x) = M(t) where t = x · σ. Since
all free variables of A′ are of integer type and since Z is a model for A′, we
have that M satisﬁes A′ · σ = A.
– To show M satisﬁes the equalities between terms of type Int in S, since our
conﬁguration is saturated with respect to S-Prop, equalities between integer
terms are a subset of those in A, and since M satisﬁes A, it satisﬁes these
equalities as well. Furthermore, S′ does not contain disequalities between
terms of type Int by construction.
– To show M satisﬁes the equalities between terms of type Str in S, notice
that s ≈t ∈S′ implies that s and t reside in the same equivalence class of
e ∈S. By construction of M every variable in e is assigned the same value
and that value is the same value as the string literal in e if one exists. Thus
M(s) = M(t) for all terms s, t of type Str that reside in the same equivalence
class, and thus M satisﬁes s ≈t.
– To show that M satisﬁes the disequalities s ̸≈t between terms of string type
in S, it suﬃces to show that distinct values are assigned to variables in each
distinct equivalence class of S. Moreover, by assumption of the conﬁgurations,
each equivalence class of terms of type string has at least one variable in it.
Let x and y be variables residing in two distinct equivalence classes of S, and
without loss of generality, assume y was assigned after x in the construction
of M. We show M(x) ̸= M(y) in the following. If y was assigned in Step 2(d)
of Deﬁnition 2, then the statement holds since by construction, its value was
chosen to be distinct from the value of string variables in previous equivalence
classes, including the one containing x. If both x and y were assigned in Step
2(b), the statement holds since S-Conf does not apply. Otherwise, y must have
been assigned in Step 2(c) to a string literal of length one. If x was assigned
in Step 2(b) and S |= x ≈l for some string literal l not of length one, then x

228
A. Reynolds et al.
and y are assigned diﬀerent values trivially. Otherwise, x is assigned (either
by Step 2(b) or Step 2(c)) to a string of length one. Moreover, code(z) is a
term in S for some z such that S |= x ≈z: if x was assigned in Step 2(b), since
C-Intro cannot be applied we have code(x) ≈code(l)↓∈S; if x was assigned
in Step 2(c) it holds by construction. Since C-Inj cannot be applied, either
code(y) ≈−1 ∈A, code(z) ̸≈code(y) ∈A, or z ≈y ∈S. The ﬁrst case cannot
hold since M satisﬁes A, and thus M(code(y)) is not equal to −1. In the
second case, since M satisﬁes A, we have that M(code(z)) ̸= M(code(y)), and
hence, since code is injective over the domain of strings of length one, we have
that M(z) ̸= M(y). Since M(z) = M(x), it then follows that M(x) ̸= M(y).
The third case cannot hold since z and y are in distinct equivalence classes.
Thus, variables in distinct equivalence classes are assigned distinct values. All
disequalities s ̸≈t ∈S are such that s and t are in diﬀerent equivalence classes
since S-Conf cannot be applied. Thus, M satisﬁes s ̸≈t.
Thus, M satisﬁes all constraints in A ∪S and the part (2) of the theorem holds.
To show (3), it is enough to show that only ﬁnitely many constraints can
be generated by the rules of the calculus. Let T ∗be the (ﬁnite) set of terms
that includes T (A0 ∪S0) ∪{ϵ, −1} and contains len(x) and code(x) for all
variables x ∈T (A0 ∪S0) of type Str, and (len(l))↓and (code(l))↓for all string
literals l ∈T (A0 ∪S0). Let A∗be the set containing A0, equalities between
terms from T ∗of type Int, literals of the form len(x) > 0, 0 ⩽code(x) < |A|,
len(x) ̸≈1 for all variables x ∈T (A0 ∪S0) of type Str, and inequalities of the
form len(x) > log|A| (n −1) where n is any positive integer less than or equal
to the number of terms of type Str in T (A0 ∪S0). Let S∗be the set containing
S0 and equalities between string terms from T ∗. Notice that both A∗and S∗are
ﬁnite. By deﬁnition of the rules of our calculus, and by induction on the size of
the derivation tree, one can show that all derived conﬁgurations ⟨A, S⟩are such
that A ∪S is a subset of A∗∪S∗. Since no application of a derivation rule in
a tree is redundant, each node in the derivation tree contains at least one more
constraint from this set than its parent. Thus, the depth of any tree is bounded
by the cardinality of A∗∪S∗, and the statement holds.
⊓⊔
An immediate consequence of Theorem 1 is that any strategy for applying the
derivation rules in Figs. 2 and 3 is a decision procedure for ΣAS-constraints. We
stress that, thanks to the constructiveness of the proof of Part 2, the procedure
can also compute a satisfying assignment for the free variables of M when it halts
with a saturated conﬁguration.
3.1
Implementation in an SMT Solver
The procedure in this section can be integrated into the DPLL(T) solving archi-
tecture [24] used by modern SMT solvers such as cvc4. In the most basic version
of this architecture, given an arbitrary quantiﬁer-free ΣAS-formula, an incremen-
tal propositional SAT solver ﬁrst searches for a truth assignment for the literals
of this formula that satisﬁes the formula at the propositional level. If none can

A Decision Procedure for String to Code Point Conversion
229
Fig. 4. A sample of the extended string functions.
be found, the input is declared unsatisﬁable. Otherwise, the found assignment
is given as a set of ΣAS-literals to a theory solver that implements the calculus
above. If the solver ﬁnds a saturated conﬁguration, then the input is declared
satisﬁable. Otherwise, either a conﬂict clause or a lemma is asserted to the SAT
solver in the form of additional TAS-valid constraints and the process restarts
with the extended set of formulas.
We have integrated the procedure in cvc4. cvc4’s linear arithmetic subsolver
acts as the arithmetic subsolver of our procedure and reports a conﬂict clause
when the rule A-Conf is applied. Similarly, the string subsolver reports conﬂict
clauses when S-Conf is applied. The rules A-Prop and S-Prop are implemented
using the standard Nelson-Oppen theory combination mechanism. Rules with
multiple conclusions are implemented via the splitting-on-demand paradigm [10],
where the conclusions of the rule are sent as a disjunctive lemma to the SAT
solver. The remaining rules are implemented using a solver whose core data
structure implements congruence closure, where additional (dis)equalities are
added to this structure based on the speciﬁc rules of the calculus.
We remark that the procedure presented in this section can be naturally
combined with procedures for other kinds of string constraints. While the rules
we presented had premises of the form S |= s ≈t denoting entailment in the
empty theory, the procedure can be applied in the same manner for premises
S |=TS s ≈t for any extension TS of the core theory of strings. In practice, our
theory solver interleaves reasoning about code points with reasoning about other
string operators, e.g., string concatenation and regular expressions operators, via
the procedure by Liang et al. [21].
The derivation rules of the calculus are applied with consideration to com-
binations with the other subsolvers of cvc4. For the rules in Fig. 2, we follow
the strategy used by Liang et al., which applies L-Intro and L-Valid eagerly and
Card only after a conﬁguration is saturated with respect to all other rules. More-
over, since Card is very expensive, we split on equalities between string terms
(x1, . . . , xn in the premise of this rule) if some xi, xj such that neither xi ≈xj or
xi ̸≈xj is in our current set of assertions. Among the rules in Fig. 3, C-Valid and
C-Collapse are applied eagerly, the former when a term code(x) is registered with
the string subsolver, and the latter as soon as our congruence closure procedure
puts that term in the same equivalence class as a string literal. Rules C-Intro and
C-Inj are applied lazily, only after the arithmetic subsolver determines A is satis-
ﬁable in LIA and the string subsolver is ﬁnished computing the set of equalities
that are entailed by S.

230
A. Reynolds et al.
4
Applications
In this section, we describe how a number of common string functions can
be implemented eﬃciently using reductions involving the code function. Pre-
vious work has focused on eﬃcient techniques for handling extended string func-
tions, which include operators like substring (substr) and string replace (replace),
among others [26]. Here we consider the alphabet A to be the set of all Unicode
characters and interpret code as mapping one-character strings to the character’s
Unicode code point.
A few commonly used extended functions are listed in Fig. 4. In the following,
we say a string l is numeric if it is non-empty, all of its characters are in the range
"0" . . . "9", and it its has no leading zeroes, that is, it starts with "0" only if it has
length 1.4 At a high level, the semantics of the operators in Fig. 4 is the following.
First, substr(x, n, m) is interpreted as the maximal substring of x starting at
position n with length at most m, or the empty string if n is outside the interval
[0, |x| −1] or m is negative; to int(x) is the non-negative integer represented by
x in decimal notation if x is numeric, and is −1 otherwise; from int(n) is the
result of converting the value of n to its decimal notation if n is non-negative,
and is ϵ otherwise; x ⪯y holds if x is equal to y or precedes it lexicographically,
that is, in the lexicographic extension of the character ordering < introduced in
Deﬁnition 1; to upper(x) maps each lower case letter character from the Basic
Latin Unicode block (code points 97 to 122) in x to its uppercase version and
all the other characters to themselves. The inverse function to lower(x) is similar
except that it maps upper case letters (code points 65 to 90) to their lower case
version.
Note that our restriction to the Latin alphabet to upper(x) and to lower is
only for simplicity since case conversions for the entire Unicode alphabet depend
on the locale and follow complex rules. However, our deﬁnition and reduction
can be extended as needed depending on the application.
Generally speaking, current string solvers handle the additional functions
above using lazy reductions to a core language of string constraints. We say ρ is
a reduction predicate for an extended function f if ρ does not contain f and is
equivalent to λx, y. f(x) ≈y where x, y consist of distinct variables. All applica-
tions of f can be eliminated from a quantiﬁer-free formula ϕ by replacing their
occurrences with fresh variables y and conjoining ϕ with the appropriate appli-
cations of the reduction predicate. Reduction predicates are chosen so that their
dependencies are not circular (for instance, we do not use reduction predicates for
two functions that each introduce applications of the other). In practice, reduc-
tion predicates often may contain universally quantiﬁed formulas over (ﬁnite)
integer ranges, which can be handled via a ﬁnite model ﬁnding strategy that
incrementally sets upper bounds on the lengths of strings [26]. These reductions
often generate constraints that are both large and hard to reason about. Further-
4 Treatment of leading zeroes is slightly diﬀerent in the SMT-LIB theory of strings [29];
our implementation actually conforms to the SMT-LIB semantics. Here, we provide
an alternative semantics for simplicity since it admits a simpler reduction.

A Decision Procedure for String to Code Point Conversion
231
more, the reduction of certain extended functions cannot be expressed concisely.
For example, a reduction for the to upper(s) function naively requires splitting
on 26 cases to ensure that "a" is converted to "A", "b" to "B", and so on, for
each character in s. As part of this work, we have revisited these reductions and
incorporated the use of code. The new reduction predicates are more concise and
lead to signiﬁcant performance gains in practice as we demonstrate in Sect. 5.
Conversions to Lower/Upper Case. The equality to lower(s) ≈r is equiva-
lent to:
len(r) ≈len(s) ∧∀0⩽i<len(s). code(ri) ≈code(si) −ite(97 ⩽code(si) ⩽122, 32, 0)
where ri is substr(r, i, 1), si is substr(s, i, 1) and ite is the if-then-else operator.
Intuitively, the formula above states that the result of to upper(s) is a string r
of the same length as s such that for all positions i in s, the character at that
position has a code point that is 32 less than the character at the same position
in s if that character is a lowercase character; otherwise it has the same code
point. Similarly, the equality to lower(s) ≈r is equivalent to:
len(r) ≈len(s) ∧∀0⩽i<len(s). code(ri) ≈code(si) + ite(65 ⩽code(si) ⩽90, 32, 0)
More generally, the code operator allows us to concisely encode many common
string transducers, which have been studied in a number of recent works [6,19,
22].
String to Integer Conversion. The equality to int(s) ≈r is equivalent to:
(¬ϕis num
s
⇒r ≈−1) ∧(ϕis num
s
⇒(r ≈stis(len(s)) ∧ϕsti
s ))
where stis is an (uninterpreted) function of type Int →Int, ϕis num
s
is:
s ̸≈ϵ ∧∀0⩽i<len(s). ite(len(s) > 1 ∧i ≈0, 49, 48) ⩽code(si) ⩽57
si is substr(s, i, 1), and ϕsti
s is:
stis(0) ≈0 ∧∀0⩽i<len(s). stis(i + 1) ≈10 ∗stis(i) + code(si) −48
In the above reduction, the formula ϕis num
s
states that s is numeric. It must
be non-empty, and each of its characters must have a code point in the interval
[48, 57], which corresponds to the characters for digits "0" through "9". The term
ite(len(s) > 1 ∧i ≈0, 49, 48) insists that the code point of the ﬁrst index of s be
at least 49 to exclude the possibility that its ﬁrst character is "0" if the string
has length greater than 1.
For a numeric string s, the formula ϕsti
s ensures that for each non-zero position
i in s, the value of stis(i) is the result of converting the ﬁrst i characters in s
to the integer it denotes. The deﬁnition of ϕsti
s
ﬁrst constrains that stis(0) is
zero. Then, for each i ⩾0, the value of stis(i + 1) is determined by shifting the
previously considered characters to the left by a digits place (10 ∗stis(i)) and
adding the integer interpretation of the current character (code(si) −48). In the
end, the above formula ensures that the value of stis(len(s)) is equivalent to the

232
A. Reynolds et al.
overall value of to int(s), which is constrained to be equal to the result r in the
above reduction.
Given these deﬁnitions, it is straightforward to deﬁne the opposite reduc-
tion from integers to strings. The equality from int(n) ≈r is equivalent to the
following:
(n < 0 ⇒r ≈ϵ) ∧(n ⩾0 ⇒(ϕis num
r
∧n ≈stir(len(r)))) ∧ϕsti
r
By deﬁnition, from int maps negative integers to the empty string. For non-
negative integers, the above reduction states that the result of converting integer
n to a string is a string r that is a string representation of an integer (due to
ϕis num
r
), and moreover is such that stir for this string results in n. We additionally
insist that the formula constraining the semantics of this conversion (ϕsti
r ) holds.
In practice, these reductions are implemented by introducing a fresh unin-
terpreted function of type Int ⇒Int to represent stis for each string s. The
functions above are introduced during solving as needed for strings that occur
as arguments to to int or those that represent the result of from int according to
the above reduction.
Lexicographic Ordering. The (Boolean) equality (x ⪯y) ≈r is equivalent
to:
(x ≈y ⇒r ≈⊤) ∧(x ̸≈y ⇒∃k. ϕdiﬀ
k,x,y ∧r ≈code(xk) < code(yk))
where xk is substr(x, k, 1), yk is substr(y, k, 1), and ϕdiﬀ
k,x,y is:
xk ̸≈yk ∧substr(x, 0, k) ≈substr(y, 0, k)
Above, ϕdiﬀ
k,x,y states that x and y are diﬀerent and k is the ﬁrst position at which
they diﬀer. If x is a preﬁx of y or vice versa, then k is the length of the shorter
of the two.
The reduction above considers two cases. First, if x and y are the same string,
then x ⪯y is trivially true. If x and y are diﬀerent, then they must diﬀer at some
smallest position k. The value of r is equivalent to the comparison of code(xk)
and code(yk). This deﬁnition correctly handles cases when k refers to the end
position of x or y. If x is a strict preﬁx of y, k must be len(x), xk is the empty
string and hence code(xk) must be −1. In this case, r must be true since yk is
non-empty and hence the value of code(yk) is non-negative; indeed x ⪯y is true
when x is a preﬁx of y. Similarly, r must be false if y is a strict preﬁx of x; indeed
x ⪯y is false when y is a strict preﬁx of x.
Regular Expression Ranges. In practice, the theory of strings is often
extended with memberships constraints of the form x ∈R, where ∈is an inﬁx
binary predicate whose ﬁrst argument is a string and whose second argument
R is a regular expression denoting a sublanguage L(R) of A∗. This constraint
holds if x is a member of L(R).
The code operator can be used for regular expressions that occur often in
applications. In particular, the constraint x ∈range(cm, cn), where m ⩽n and
ci is is singleton string constant with code point i, states that x consists of

A Decision Procedure for String to Code Point Conversion
233
one character whose code point is in the interval [m, n]. This is equivalent to
n ⩽code(x) ∧code(x) ⩽m. Our implementation of regular expressions in cvc4
utilizes this as a rewrite rule on membership constraints since it can eliminate the
expensive computation of certain regular expression intersections. For example,
consider the following equivalent formulas:
x ∈range("A", "M") ∧x ∈range("J", "Z")
(1)
65 ⩽code(x) ⩽77 ∧74 ⩽code(x) ⩽90
(2)
A naive approach to regular expression solving may compute the intersection
of the two regular expressions above by explicitly splitting on characters in the
ranges of (1). Our approach instead reasons about the arithmetic constraints in
(2) and infers the constraint 74 ⩽code(x) ⩽77 without expensive case splits.
If the latter constraint persists in a saturated conﬁguration, our procedure will
then assign x a character in range("J", "M").
5
Evaluation
In this section, we evaluate whether our approach is practical and whether code
can enable more eﬃcient implementations of common string functions.5 As out-
lined in Sect. 3.1, we have implemented our approach in cvc4, which has a state-
of-the-art subsolver for the theory of strings with length and regular expressions.
We evaluated it on 21,573 benchmarks [1] originating from the concolic execu-
tion of Python code involving int() using Py-Conbyte [8,32]. The benchmarks
make extensive use of to int, from int and regular expression ranges. They are
divided into four sets, one for each solver used to generate the benchmarks (cvc4,
Trau [3], z3 [16], and z3str3).
We compare two conﬁgurations of cvc4 to show the impact of our approach:
A conﬁguration (cvc4+c) that uses the reductions from Sect. 4 and a conﬁgura-
tion (cvc4) that disables all code-derivations and uses reductions without code.
For regular expression ranges, cvc4 disables the rewrite to inequalities involving
code and uses its regular expression solver to process them. The reductions in
cvc4 use nested ite terms of the form ite(c = "9", 9, ite(c = "8", 8, . . .)), i.e., do
case splitting on the 10 concrete string values that correspond to valid digits,
instead of the code operator but keep the reductions the same otherwise. As a
point of reference, we also compare against z3 version 4.8.7, another state-of-the-
art string solver. We omit a comparison against z3str3 4.8.7 and z3-Trau 1.0 [2]
(the new version of Trau) because our experiments have shown that the current
versions are unsound.6
5 The implementation, the benchmarks, and the results are available at https://cvc4.
github.io/papers/ijcar2020-strings.
6 cvc4 and z3str3 disagreed on 498 benchmarks whereas cvc4 and z3-Trau dis-
agreed on 9. In all instances, z3str3 and z3-Trau answered that the benchmark is
unsatisﬁable but accepted cvc4’s model when we incorporated it as an additional
constraint to the benchmark.

234
A. Reynolds et al.
Benchmark Set
cvc4+c
cvc4
Z3
py-conbyte cvc4
sat
1344
1104
1187
unsat
8576
8547
8482
13
282
264
py-conbyte trauc
sat
1009
929
697
unsat
1424
1407
1428
13
110
321
py-conbyte z3seq
sat
1354
1126
1343
unsat
5864
5797
5719
35
330
191
py-conbyte z3str
sat
711
652
692
unsat
1227
1223
1223
3
66
26
Total
sat
4418
3811
3919
unsat
17091
16974
16852
64
788
802
Fig. 5. Number of solved problems per benchmark set and scatter plots comparing
the diﬀerent solvers and conﬁgurations on a log-log scale. Best results are in bold. All
benchmarks ran with a timeout of 300 s.
We ran our experiments on a cluster with Intel Xeon E5-2637 v4 CPUs
running Ubuntu 16.04 and allocated one CPU core, 8 GB of RAM, and 300 s for
each job.
Figure 5 summarizes the results of our experiments. The table lists the num-
ber of satisﬁable and unsatisﬁable answers as well as timeouts/memouts (×). z3
ran out of memory on a benchmark but had no other memouts. The ﬁgure shows
two scatter plots comparing the performance of cvc4+c and cvc4 and compar-
ing cvc4+c and z3. Conﬁguration cvc4 solves more unsatisﬁable benchmarks
than z3 and fewer satisﬁable ones, which suggests that cvc4 is a reasonable
baseline. Our new approach performs signiﬁcantly better than both cvc4 and
z3. Compared to cvc4, conﬁguration cvc4+c times out on an order of magnitude
fewer benchmarks (64 versus 788) and also improves performance on commonly
solved benchmarks, as the scatter plot indicates. While cvc4 performs worse
than z3 on satisﬁable benchmarks, cvc4+c performs signiﬁcantly better than
both on those benchmarks. The scatter plot indicates that z3 manages to solve a
subset of the benchmarks quickly. However, when z3 is not able to solve a bench-

A Decision Procedure for String to Code Point Conversion
235
mark quickly, it is unlikely that it solves it within our timeout. This results in
cvc4+c having signiﬁcantly fewer timeouts overall. The results indicate that our
new approach is practical and capable of improving the performance of state-of-
the-art solvers by enabling more eﬃcient encodings.
6
Conclusion
We have presented a decision procedure for a fragment of strings that includes
a string to code point conversion function. We have shown that models can be
generated for satisﬁable inputs, and that existing techniques for handling strings
in SMT solvers can be extended with this procedure. Due to its use for encoding
extended string functions, our implementation in cvc4 signiﬁcantly improves on
the state of the art for benchmarks involving string-to-integer conversions and
regular expression ranges.
In future work, we plan to extend cvc4 to solve new constraints of interest
to user applications. This includes instrumenting our string solver to be capable
of generating proofs based on the procedure described in this paper. Further
directions such as conﬁguring the solver to generate interpolants for constraints
in the theory of strings combined with linear arithmetic could also be explored.
Finally we conjecture that eﬃcient support for reasoning about string-to-code
conversions can be leveraged for further extensions, such as handling user-deﬁned
string transducers.
References
1. str int benchmarks (2019). https://github.com/plfm-iis/str int benchmarks
2. z3-Trau (2020). https://github.com/guluchen/z3/releases/tag/z3-trau
3. Abdulla, P.A., et al.: Flatten and conquer: a framework for eﬃcient analysis of
string constraints. In: Cohen and Vechev [15], pp. 602–617 (2017)
4. Abdulla, P.A., et al.: String constraints for veriﬁcation. In: Biere and Bloem [12],
pp. 150–166 (2014)
5. Abdulla, P.A., et al.: Norn: an SMT solver for string constraints. In: Kroening, D.,
P˘as˘areanu, C.S. (eds.) CAV 2015. LNCS, vol. 9206, pp. 462–469. Springer, Cham
(2015). https://doi.org/10.1007/978-3-319-21690-4 29
6. Abdulla, P.A., Atig, M.F., Diep, B.P., Hol´ık, L., Jank˚u, P.: Chain-free string con-
straints. In: Chen, Y.-F., Cheng, C.-H., Esparza, J. (eds.) ATVA 2019. LNCS, vol.
11781, pp. 277–293. Springer, Cham (2019). https://doi.org/10.1007/978-3-030-
31784-3 16
7. Backes, J., et al.: Semantic-based automated reasoning for AWS access policies
using SMT. In: Bjørner, N., Gurﬁnkel, A. (eds.) 2018 Formal Methods in Computer
Aided Design, FMCAD 2018, Austin, TX, USA, 30 October–2 November 2018, pp.
1–9. IEEE (2018)
8. Ball, T., Daniel, J.: Deconstructing dynamic symbolic execution. In: Irlbeck, M.,
Peled, D.A., Pretschner, A. (eds.) Dependable Software Systems Engineering, vol-
ume 40 of NATO Science for Peace and Security Series, D: Information and Com-
munication Security, pp. 26–41. IOS Press (2015)

236
A. Reynolds et al.
9. Barrett, C., et al.: CVC4. In: Gopalakrishnan, G., Qadeer, S. (eds.) CAV 2011.
LNCS, vol. 6806, pp. 171–177. Springer, Heidelberg (2011). https://doi.org/10.
1007/978-3-642-22110-1 14
10. Barrett, C., Nieuwenhuis, R., Oliveras, A., Tinelli, C.: Splitting on demand in
SAT modulo theories. In: Hermann, M., Voronkov, A. (eds.) LPAR 2006. LNCS
(LNAI), vol. 4246, pp. 512–526. Springer, Heidelberg (2006). https://doi.org/10.
1007/11916277 35
11. Berzish, M., Ganesh, V., Zheng, Y.: Z3str3: a string solver with theory-aware heuris-
tics. In: Stewart, D., Weissenbacher, G. (eds.) 2017 Formal Methods in Computer
Aided Design, FMCAD 2017, Vienna, Austria, 2–6 October 2017, pp. 55–59. IEEE
(2017)
12. Biere, A., Bloem, R. (eds.): CAV 2014. LNCS, vol. 8559. Springer, Cham (2014).
https://doi.org/10.1007/978-3-319-08867-9
13. Bjørner, N., Tillmann, N., Voronkov, A.: Path feasibility analysis for string-
manipulating programs. In: Kowalewski, S., Philippou, A. (eds.) TACAS 2009.
LNCS, vol. 5505, pp. 307–321. Springer, Heidelberg (2009). https://doi.org/10.
1007/978-3-642-00768-2 27
14. B¨uchi, J.R., Senger, S.: Deﬁnability in the existential theory of concatenation and
undecidable extensions of this theory. Math. Log. Q. 34(4), 337–342 (1988)
15. Cohen, A., Vechev, M.T. (eds.): Proceedings of the 38th ACM SIGPLAN Con-
ference on Programming Language Design and Implementation, PLDI 2017,
Barcelona, Spain, 18–23 June 2017. ACM (2017)
16. de Moura, L., Bjørner, N.: Z3: an eﬃcient SMT solver. In: Ramakrishnan, C.R.,
Rehof, J. (eds.) TACAS 2008. LNCS, vol. 4963, pp. 337–340. Springer, Heidelberg
(2008). https://doi.org/10.1007/978-3-540-78800-3 24
17. Enderton, H.B.: A mathematical Introduction to Logic, 2nd edn. Academic Press
(2001)
18. Ganesh, V., Berzish, M.: Undecidability of a theory of strings, linear arithmetic
over length, and string-number conversion. CoRR, abs/1605.09442 (2016)
19. Hu, Q., D’Antoni, L.: Automatic program inversion using symbolic transducers. In:
Cohen and Vechev [15], pp. 376–389 (2017)
20. Kiezun, A., Ganesh, V., Artzi, S., Guo, P.J., Hooimeijer, P., Ernst, M.D.: HAMPI:
a solver for word equations over strings, regular expressions, and context-free gram-
mars. ACM Trans. Softw. Eng. Methodol. 21(4), 25:1–25:28 (2012)
21. Liang, T., Reynolds, A., Tinelli, C., Barrett, C., Deters, M.: A DPLL(T) theory
solver for a theory of strings and regular expressions. In: Biere and Bloem [12], pp.
646–662 (2014)
22. Lin, A.W., Barcel´o, P.: String solving with word equations and transducers:
towards a logic for analysing mutation XSS. In: Bod´ık, R., Majumdar, R. (eds.)
Proceedings of the 43rd Annual ACM SIGPLAN-SIGACT Symposium on Prin-
ciples of Programming Languages, POPL 2016, St. Petersburg, FL, USA, 20–22
January 2016, pp. 123–136. ACM (2016)
23. Makanin, G.S.: The problem of solvability of equations in a free semigroup. Matem-
aticheskii Sbornik 145(2), 147–236 (1977)
24. Nieuwenhuis, R., Oliveras, A., Tinelli, C.: Solving SAT and SAT modulo theories:
from an abstract Davis-Putnam-Logemann-Loveland Procedure to DPLL(T). J.
ACM 53(6), 937–977 (2006)
25. Quine, W.V.O.: Concatenation as a basis for arithmetic. J. Symb. Log. 11(4),
105–114 (1946)

A Decision Procedure for String to Code Point Conversion
237
26. Reynolds, A., Woo, M., Barrett, C., Brumley, D., Liang, T., Tinelli, C.: Scaling
up DPLL(T) string solvers using context-dependent simpliﬁcation. In: Majumdar,
R., Kunˇcak, V. (eds.) CAV 2017. LNCS, vol. 10427, pp. 453–474. Springer, Cham
(2017). https://doi.org/10.1007/978-3-319-63390-9 24
27. Saxena, P., Akhawe, D., Hanna, S., Mao, F., McCamant, S., Song, D.: A symbolic
execution framework for Javascript. In: 31st IEEE Symposium on Security and
Privacy, S&P 2010, 16–19 May 2010, Berleley/Oakland, California, USA, pp. 513–
528. IEEE Computer Society (2010)
28. The Unicode Consortium. The Unicode Standard, Version 12.1.0 (2019). http://
www.unicode.org/versions/Unicode12.1.0/
29. Tinelli, C., Barrett, C., Fontaine, P.: Unicode Strings (2020). http://smtlib.cs.
uiowa.edu/theories-UnicodeStrings.shtml
30. Trinh, M., Chu, D., Jaﬀar, J.: S3: a symbolic string solver for vulnerability detection
in web applications. In: Ahn, G., Yung, M., Li, N. (eds.) Proceedings of the 2014
ACM SIGSAC Conference on Computer and Communications Security, Scottsdale,
AZ, USA, 3–7 November 2014, pp. 1232–1243. ACM (2014)
31. Veanes, M., Tillmann, N., de Halleux, J.: Qex: symbolic SQL query explorer. In:
Clarke, E.M., Voronkov, A. (eds.) LPAR 2010. LNCS (LNAI), vol. 6355, pp. 425–
446. Springer, Heidelberg (2010). https://doi.org/10.1007/978-3-642-17511-4 24
32. Wu, W.-C.: Py-Conbyte (2019). https://github.com/spencerwuwu/py-conbyte
33. Yu, F., Alkhalaf, M., Bultan, T.: Stranger: an automata-based string analy-
sis tool for PHP. In: Esparza, J., Majumdar, R. (eds.) TACAS 2010. LNCS, vol.
6015, pp. 154–157. Springer, Heidelberg (2010). https://doi.org/10.1007/978-3-
642-12002-2 13

Politeness for the Theory of Algebraic
Datatypes
Ying Sheng1
, Yoni Zohar1(B)
, Christophe Ringeissen2
, Jane Lange1
,
Pascal Fontaine2,3
, and Clark Barrett1
1 Stanford University, Stanford, USA
yoni206@gmail.com
2 Universit´e de Lorraine, CNRS, Inria, LORIA, 54000 Nancy, France
3 Universit´e de Li`ege, Li`ege, Belgium
Abstract. Algebraic datatypes, and among them lists and trees, have
attracted a lot of interest in automated reasoning and Satisﬁability Mod-
ulo Theories (SMT). Since its latest stable version, the SMT-LIB stan-
dard deﬁnes a theory of algebraic datatypes, which is currently supported
by several mainstream SMT solvers. In this paper, we study this particu-
lar theory of datatypes and prove that it is strongly polite, showing also
how it can be combined with other arbitrary disjoint theories using polite
combination. Our results cover both inductive and ﬁnite datatypes, as
well as their union. The combination method uses a new, simple, and nat-
ural notion of additivity, that enables deducing strong politeness from
(weak) politeness.
1
Introduction
Algebraic datatypes such as lists and trees are extremely common in many pro-
gramming languages. Reasoning about them is therefore crucial for modeling
and verifying programs. For this reason, various decision procedures for algebraic
datatypes have been, and continue to be developed and employed by formal rea-
soning tools such as theorem provers and Satisﬁability Modulo Theories (SMT)
solvers. For example, the general algorithm of [4] describes a decision procedure
for datatypes suitable for SMT solvers. Consistently with the SMT paradigm, [4]
leaves the combination of datatypes with other theories to general combination
methods, and focuses on parametric datatypes (or generic datatypes as they are
called in the programming languages community).
The traditional combination method of Nelson and Oppen [20] is applica-
ble for the combination of this theory with many other theories, as long as the
other theory is stably inﬁnite (a technical condition that intuitively amounts to
the ability to extend every model to an inﬁnite one). Some theories of inter-
est, however, are not stably inﬁnite, the most notable one being the theory of
This project was partially supported by a grant from the Defense Advanced Research
Projects Agency (N66001-18-C-4012), the Stanford CURIS program, and Jasmin
Blanchette’s European Research Council (ERC) starting grant Matryoshka (713999).
c
⃝Springer Nature Switzerland AG 2020
N. Peltier and V. Sofronie-Stokkermans (Eds.): IJCAR 2020, LNAI 12166, pp. 238–255, 2020.
https://doi.org/10.1007/978-3-030-51074-9_14

Politeness for the Theory of Algebraic Datatypes
239
ﬁxed-width bit-vectors, which is commonly used for modeling and verifying both
hardware and software. To be able to perform combinations with such theories,
a more general combination method was designed [21], which relies on polite the-
ories. Roughly speaking, a theory is polite if: (i) every model can be arbitrarily
enlarged; and (ii) there is a witness, a function that transforms any quantiﬁer-
free formula to an equivalent quantiﬁer-free formula such that if the original
formula is satisﬁable, the new formula is satisﬁable in a “minimal” interpreta-
tion. This notion was later strengthened to strongly polite theories [14], which
also account for possible arrangements of the variables in the formula. Strongly
polite theories can be combined with any other disjoint decidable theory, even
if that other theory is not stably inﬁnite. While strong politeness was already
proven for several useful theories (such as equality, arrays, sets, multisets [21]),
strong politeness of algebraic datatypes remained an unanswered question.
The main contribution of this paper is an aﬃrmative answer to this question.
We introduce a witness function that essentially “guesses” the right constructors
of variables without an explicit constructor in the formula. We show how to
“shrink” any model of a formula that is the output of this function into a minimal
model. The witness function, as well as the model-construction, can be used
by any SMT solver for the theory of datatypes that implements polite theory
combination. We introduce and use the notion of additive witnesses, which allows
us to prove politeness and conclude strong politeness. We further study the
theory of datatypes beyond politeness and extend a decision procedure for a
subset of this theory presented in [9] to support the full theory.
Related Work
The theory investigated in this paper is that of algebraic datatypes, as deﬁned
by the SMT-LIB 2 standard [3]. Detailed information on this theory, including
a decision procedure and related work, can be found in [4]. Later work extends
this procedure to handle shared selectors [23] and co-datatypes [22]. More recent
approaches for solving formulas about datatypes use, e.g., theorem provers [15],
variant satisﬁability [12,19], and reduction-based decision procedures [1,6,13].
In this paper, we focus on polite theory combination. Other combination
methods for non stably inﬁnite theories include shiny theories [27], gentle theo-
ries [11], and parametric theories [17]. The politeness property was introduced
in [21], and extends the stable inﬁniteness assumption initially used by Nelson
and Oppen. Polite theories can be combined `a la Nelson-Oppen with any arbi-
trary decidable theory. Later, a ﬂaw in the original deﬁnition of politeness was
found [14], and a corrected deﬁnition (here called strong politeness) was intro-
duced. Strongly polite theories were further studied in [8], where the authors
proved their equivalence with shiny theories.
More recently, it was proved [9] that a general family of datatype theories
extended with bridging functions is strongly polite. This includes the theories
of lists/trees with length/size functions. The authors also proved that a class of
axiomatizations of datatypes is strongly polite. In contrast, in this paper we focus
on standard interpretations, as deﬁned by the SMT-LIB 2 standard, without any

240
Y. Sheng et al.
size function, but including selectors and testers. One can notice that the theory
of standard lists without the length function, and more generally the theory of
ﬁnite trees without the size function, were not mentioned as polite in a recent
survey [7]. Actually, it was unclear to the authors of [7] whether these theories
are strongly polite. This is now clariﬁed in the current paper.
Outline
The paper is organized as follows. Section 2 provides the necessary notions from
ﬁrst-order logic and polite theories, and it introduces our working deﬁnition
of the theory of datatypes, which is based on SMT-LIB 2. Section 3 discusses
the diﬀerence between politeness and strong politeness, and introduces a useful
condition for their equivalence. Section 4 contains the main result of this paper,
namely that the theory of algebraic datatypes is strongly polite. Section 5 studies
various axiomatizations of the theory of datatypes, and relates them to polite-
ness. Section 6 concludes with directions for further research.
2
Preliminaries
2.1
Signatures and Structures
We brieﬂy review usual deﬁnitions of many-sorted ﬁrst-order logic with equality
(see [10,26] for more details). For any set S, an S-sorted set A is a function
from S to P(X) \ {∅} for some set X (i.e., A assigns a non-empty set to every
element of S), such that A(s) ∩A(s′) = ∅whenever s ̸= s′. We use As to
denote A(s) for every s ∈S, and call the elements of S sorts. When there is no
ambiguity, we sometimes treat sorted sets as sets (e.g., when writing expressions
like x ∈A). Given a set S (of sorts), the canonical S-sorted set, denoted [[S]],
satisﬁes [[S]]s = {s} for every s ∈S. A many-sorted signature Σ consists of a set
SΣ (of sorts), a set FΣ of function symbols, and a set PΣ of predicate symbols.
Function symbols have arities of the form σ1 × . . . × σn →σ, and predicate
symbols have arities of the form σ1 × . . . × σn, with σ1, . . . , σn, σ ∈SΣ. For each
sort σ ∈SΣ, PΣ includes an equality symbol =σ of arity σ × σ. We denote it by
= when σ is clear from context. Σ is called ﬁnite if SΣ, FΣ, and PΣ are ﬁnite.
We assume an underlying SΣ-sorted set of variables. Terms, formulas, and lit-
erals are deﬁned in the usual way. For a Σ-formula φ and a sort σ, we denote the
set of free variables in φ of sort σ by varsσ(φ). This notation naturally extends
to varsS(φ) when S is a set of sorts. A sentence is a formula without free vari-
ables. We denote by QF(Σ) the set of quantiﬁer-free formulas of Σ. A Σ-literal
is called ﬂat if it has one of the following forms: x = y, x ̸= y, x = f(x1, . . . , xn),
P(x1, . . . , xn), or ¬P(x1, . . . , xn) for some variables x, y, x1, . . . , xn and function
and predicate symbols f and P from Σ.
A Σ-structure is a many-sorted structure for Σ, without interpretation of
variables. It consists of a SΣ-sorted set A, and interpretations to the function
and predicate symbols of Σ. We further require that =σ is interpreted as the
identity relation over Aσ for every σ ∈SΣ. A Σ-interpretation A is an extension

Politeness for the Theory of Algebraic Datatypes
241
of a Σ-structure with interpretations to some set of variables. For any Σ-term
α, αA denotes the interpretation of α in A. When α is a set of Σ-terms, αA =

xA | x ∈α

. Similarly, σA, f A and P A denote the interpretation of σ, f and
P in A. Satisfaction is deﬁned as usual. A |= ϕ denotes that A satisﬁes ϕ.
A Σ-theory T is a class of Σ-structures. A Σ-interpretation whose variable-
free part is in T is called a T-interpretation. A Σ-formula φ is T-satisﬁable if
A |= φ for some T-interpretation A. Two formulas φ and ψ are T-equivalent
if they are satisﬁed by the same class of T-interpretations. Let Σ1 and Σ2 be
signatures, T1 a Σ1-theory, and T2 a Σ2-theory. The combination of T1 and T2,
denoted T1 ⊕T2, is the class of Σ1 ∪Σ2-structures A such that AΣ1 is in T1 and
AΣ2 is in T2, where AΣi is the restriction of A to Σi for i ∈{1, 2}.
2.2
The SMT-LIB 2 Theory of Datatypes
In this section we formally deﬁne the SMT-LIB 2 theory of algebraic datatypes.
The formalization is based on [3], but is adjusted to suit our investigation of
politeness.
Deﬁnition 1. Given a signature Σ, a set S ⊆SΣ and an S-sorted set A, the
set of Σ-trees over A of sort σ ∈SΣ is denoted by Tσ(Σ, A) and is inductively
deﬁned as follows:
– Tσ,0(Σ, A) = Aσ if σ ∈S and ∅otherwise.
– Tσ,i+1(Σ, A) = Tσ,i(Σ, A) ∪{c(t1, . . . , tn) | c : σ1 × . . . × σn →σ ∈FΣ, tj ∈
Tσj,i(Σ, A) for j = 1, . . . , n} for each i ≥0.
Then Tσ(Σ, A) = 
i≥0 Tσ,i(Σ, A). The depth of a Σ-tree over A is inductively
deﬁned by depth(a) = 0 for every a ∈A, depth(c) = 1 for every 0-ary function
symbol c ∈FΣ, and depth(c(t1, . . . , tn)) = 1 + max(depth(t1), . . . , depth(tn)) for
every n-ary function symbol c of Σ.
The idea behind Deﬁnition 1 is that Tσ(Σ, A) contains all ground σ-sorted
terms constructed from the elements of A (considered as constant symbols) and
the function symbols of Σ.
Example 1. Let Σ be a signature with two sorts, elem and struct, and whose
function symbols are b of arity struct, and c of arity (elem × struct ×
struct) →struct. Consider the {elem}-sorted set A = {a}. For the elem sort,
Telem(Σ, A) is the singleton A = {a} and the Σ-tree a is of depth 0. For the
struct sort, Tstruct(Σ, A) includes inﬁnitely many Σ-trees, such as b of depth
1, c(a, b, b) of depth 2, and c(a, c(a, b, b), b) of depth 3.
Deﬁnition 2. A ﬁnite signature Σ is called a datatypes signature if SΣ is the
disjoint union of two sets of sorts SΣ = ElemΣ ⊎StructΣ and FΣ is the
disjoint union of two sets of function symbols FΣ = COΣ ⊎SEΣ, such that
SEΣ = {sc,i : σ →σi | c ∈COΣ, c : σ1, . . . , σn →σ, 1 ≤i ≤n} and PΣ =
{isc : σ | c ∈COΣ, c : σ1, . . . , σn →σ}. We denote by Σ|CO the signature with

242
Y. Sheng et al.
the same sorts as Σ, no predicate symbols (except =σ for σ ∈SΣ), and whose
function symbols are COΣ. We further require the following well-foundedness
requirement: Tσ(Σ|CO, [[ElemΣ]]) ̸= ∅for any σ ∈StructΣ.
From now on, we omit the subscript Σ from the above notations (e.g., when
writing [[Elem]] rather than [[ElemΣ]], CO rather than COΣ) whenever Σ is
clear from the context. Notice that Deﬁnition 2 remains equivalent if we replace
[[Elem]] by any (non-empty) Elem-sorted set A. The set [[Elem]] has been
chosen since this minimal Elem-sorted set is suﬃcient.
In accordance with SMT-LIB 2, we call the elements of CO constructors, the
elements of SE selectors, and the elements of P testers. 0-ary constructors are
called nullary. In what follows, Σ denotes an arbitrary datatypes signature.
In the next example we review some common datatypes signatures.
Example 2. The signature Σlist has two sorts, elem and list. Its function sym-
bols are cons of arity (elem × list) →list, nil of arity list, car of arity
list →elem and cdr of arity list →list. Its predicate symbols are isnil and
iscons, both of arity list. It is a datatypes signature, with Elem = {elem},
Struct = {list}, CO = {nil, cons} and SE = {car, cdr}. It is often used to model
lisp-style linked lists. car represents the head of the list and cdr represents its
tail. nil represents the empty list. Σlist is well-founded as Tlist(Σlist |CO, [[Elem]])
includes nil.
The signature Σpair also has two sorts, elem and pair. Its function symbols
are pair of arity (elem × elem) →pair and first and second of arity pair →
elem. Its predicate symbol is ispair of arity pair. It is a datatypes signature, with
Elem = {elem}, Struct = {pair}, CO = {pair}, and SE = {first, second}. It
can be used to model ordered pairs, together with projection functions. It is well-
founded as Tpair(Σpair |CO, [[Elem]]) is not empty (as [[Elem]] is not empty).
The signature Σlp has three sorts, elem, pair and list. Its function sym-
bols are cons of arity (pair × list) →list, car of arity list →pair, as well
as nil, cdr, first, second with arities as above. Its predicate symbols are ispair,
iscons and nil, with arities as above. It can be used to model lists of ordered
pairs. Similarly to the above signatures, it is a datatypes signature.
Next, we distinguish between ﬁnite datatypes (e.g., records) and inductive
datatypes (e.g., lists).
Deﬁnition 3. A sort σ ∈Struct is ﬁnite if Tσ(Σ|CO, [[Elem]]) is ﬁnite, and is
called inductive otherwise.
We denote the set of inductive sorts in Σ by Ind(Σ) and the set of its ﬁnite
sorts by Fin(Σ). Note that if σ is inductive, then according to Deﬁnitions 1 and
3 we have that for any natural number i there exists a natural number i′ > i
such that Tσ,i′(Σ|CO, [[Elem]]) ̸= Tσ,i(Σ|CO, [[Elem]]). Further, for any natural
number d and every Elem-sorted set D there exists a natural number i′ such
that Tσ,i′(Σ|CO, D) contains an element whose depth is greater than d.
Example 3. list is inductive in Σlist and Σlp. pair is ﬁnite in Σpair and Σlp.

Politeness for the Theory of Algebraic Datatypes
243
Finally, we deﬁne datatypes structures and the theory of algebraic datatypes.
Deﬁnition 4. Let Σ be a datatypes signature and D an Elem-sorted set. A
Σ-structure A is said to be a datatypes Σ-structure generated by D if:
– σA = Tσ(Σ|CO, D) for every sort σ ∈SΣ,
– cA(t1, . . . , tn) = c(t1, . . . , tn) for every c ∈CO of arity (σ1 × . . . × σn) →σ
and t1 ∈σA
1 , . . . , tn ∈σA
n ,
– sA
c,i(c(t1, . . . , tn)) = ti for every c ∈CO of arity (σ1 × . . . × σn) →σ, t1 ∈
σA
1 , . . . , tn ∈σA
n and 1 ≤i ≤n,
– isA
c =

c(t1, . . . , tn) | t1 ∈σA
1 , . . . , tn ∈σA
n

for every c ∈CO of arity (σ1 ×
. . . × σn) →σ.
A is said to be a datatypes Σ-structure if it is a datatypes Σ-structure generated
by D for some Elem-sorted set D. The Σ-theory of datatypes, denoted TΣ is
the class of datatypes Σ-structures.
Notice that the interpretation of selector functions sc,i when applied to terms
that are constructed using a constructor diﬀerent than c is not ﬁxed and can be
set arbitrarily in datatypes structures, consistently with SMT-LIB 2.
Example 4. If A is a datatypes Σlist-structure then listA is the set of terms con-
structed from elemA and cons, plus nil. If elemA is the set of natural numbers,
then listA contains, e.g., nil, cons(1, nil), and cons(1, cons(1, cons(2, nil))).
These correspond to the lists [] (the empty list), [1] and [1, 1, 2], respectively.
If A is a datatypes Σpair-structure then pairA is the set of terms of the form
pair(a, b) with a, b ∈elemA. If elemA is again interpreted as the set of natural
numbers, pairA includes, for example, the terms pair(1, 1) and pair(1, 2), that
correspond to (1, 1) and (1, 2), respectively. Notice that in this case, pairA is an
inﬁnite set even though pair is a ﬁnite sort (in terms of Deﬁnition 3).
Datatypes Σlp-structures with the same interpretation for elem include the
terms nil, cons(pair(1, 1), nil), and cons(pair(1, 1), cons(pair(1, 2), nil)) in the
interpretation for list, that correspond to [], [(1, 1)] and [(1, 1), (1, 2)], respec-
tively. If we rename elem in the deﬁnition of Σlist to pair, we get that
TΣlp = TΣlist ⊕TΣpair .
2.3
Polite Theories
Given two theories T1 and T2, a combination method `a la Nelson-Oppen provides
a modular way to decide T1 ∪T2-satisﬁability problems using the satisﬁability
procedures known for T1 and T2. Assuming that T1 and T2 have disjoint signa-
tures is not suﬃcient to get a complete combination method for the satisﬁability
problem. The reason is that T1 and T2 may share sorts, and the equality symbol
on these shared sorts. To be complete, T1 and T2 must agree on the cardinality
of their respective models, and there must be an agreement between T1 and T2
on the interpretation of shared formulas built over the equality symbol. These
two requirements can be easily fulﬁlled, based on the following deﬁnitions:

244
Y. Sheng et al.
Deﬁnition 5 (Stable Inﬁniteness). Given a signature Σ and a set S ⊆SΣ,
we say that a Σ-theory T is stably inﬁnite with respect to S if every quantiﬁer-
free Σ-formula that is T-satisﬁable is also T-satisﬁable by a T-interpretation A
in which σA is inﬁnite for every σ ∈S.
Deﬁnition 6 (Arrangement). Let V be a ﬁnite set of variables whose sorts
are in S and {Vσ | σ ∈S} a partition of V such that Vσ is the set of vari-
ables of sort σ in V . We say that a formula δ is an arrangement of V if
δ = 
σ∈S(
(x,y)∈Eσ(x = y) ∧
(x,y)/∈Eσ(x ̸= y)), where Eσ is some equiva-
lence relation over Vσ for each σ ∈S.
Assume that both T1 and T2 are stably inﬁnite with disjoint signatures, and
let V be the ﬁnite set of variables shared by T1 and T2. Under this assumption,
T1 and T2 can agree on an inﬁnite cardinality, and guessing an arrangement of
V suﬃces to get an agreement on the interpretation of shared formulas.
In this paper we are interested in an asymmetric disjoint combination where
T1 and T2 are not both stably inﬁnite. In this scenario, one theory can be arbi-
trary. As a counterpart, the other theory must be more than stably inﬁnite: it
must be polite, meaning that it is always possible to increase the cardinality of
a model and to have a model whose cardinality is ﬁnite.
In the following we decompose the politeness deﬁnition from [14,21] in order
to distinguish between politeness and strong politeness (in terms of [8]) in var-
ious levels of the deﬁnition. In what follows, Σ is an arbitrary (many-sorted)
signature, S ⊆SΣ, and T is a Σ-theory.
Deﬁnition 7 (Smooth). The theory T
is
smooth w.r.t. S if for every
quantiﬁer-free formula φ, T-interpretation A that satisﬁes φ, and function κ
from S to the class of cardinals such that κ(σ) ≥
σA for every σ ∈S there
exists a Σ-interpretation A′ that satisﬁes φ with
σA′ = κ(σ) for every σ ∈S.
In deﬁnitions introduced above, as well as below, we often identify singletons
with their single elements when there is no ambiguity (e.g., when saying that a
theory is smooth w.r.t. a sort σ).
We now introduce some concepts in order to deﬁne ﬁnite witnessability. Let φ
be a quantiﬁer-free Σ-formula and A a Σ-interpretation. We say that A ﬁnitely
witnesses φ for T w.r.t. S (or, is a ﬁnite witness of φ for T w.r.t. S), if A is a
T-interpretation, A |= φ, and σA = varsσ(φ)A for every σ ∈S. We say that φ
is ﬁnitely witnessed for T w.r.t. S if it is either T-unsatisﬁable or it has a ﬁnite
witness for T w.r.t. S. φ is strongly ﬁnitely witnessed for T w.r.t. S if φ ∧δV is
ﬁnitely witnessed for T w.r.t. S for every arrangement δV of V , where V is any set
of variables whose sorts are in S. We say that a function wtn : QF(Σ) →QF(Σ)
is a (strong) witness for T w.r.t. S if for every φ ∈QF(Σ) we have that: 1. φ
and ∃−→
w .wtn(φ) are T-equivalent for −→
w = vars(wtn(φ))\vars(φ); and 2. wtn(φ)
is (strongly) ﬁnitely witnessed for T w.r.t. S.1
1 We note that in practice, the new variables in wtn(φ) are assumed to be fresh not
only with respect to φ, but also with respect to the formula from the second theory
being combined.

Politeness for the Theory of Algebraic Datatypes
245
Deﬁnition 8 (Finitely Witnessable). The theory T is (strongly) ﬁnitely wit-
nessable w.r.t. S if there exists a (strong) witness for T w.r.t. S which is com-
putable.
Deﬁnition 9 (Polite). T is called (strongly) polite w.r.t. S if it is smooth and
(strongly) ﬁnitely witnessable w.r.t. S.
Finally, we recall the following theorem from [14].
Theorem 1 ([14]). Let Σ1 and Σ2 be signatures and let S = SΣ1 ∩SΣ2. If T1
is a Σ1-theory strongly polite w.r.t. S1 ⊆SΣ1, T2 is a Σ2-theory strongly polite
w.r.t. S2 ⊆SΣ2, and S ⊆S2, then T1 ⊕T2 is strongly polite w.r.t. S1 ∪(S2 \ S).
3
Additive Witnesses
It was shown in [14] that politeness is not suﬃcient for the proof of the polite
combination method from [21]. Strong politeness was introduced to ﬁx the prob-
lem. It is unknown, however, whether there are theories that are polite but not
strongly polite. In this section we oﬀer a simple (yet useful) criterion for the
equivalence of the two notions. Throughout this section, unless stated other-
wise, Σ and S denote an arbitrary signature and a subset of its set of sorts, and
T, T1, T2 denote arbitrary Σ-theories.
The following example, which is based on [14] using notions of the current
paper, shows that the strong and non-strong witnesses are diﬀerent. Let Σ0 be
a signature with a single sort σ and no function or predicate symbols (except
=σ), T0 the Σ0-theory consisting of all Σ0-structures A with
σA ≥2, φ the
formula x = x ∧w = w, and δ the arrangement (x = w) of {x, w}. Then φ ∧δ is
T0-satisﬁable, but every interpretation A with σA = {x, w}A that satisﬁes it has
only one element in σA and so φ is not strongly ﬁnitely witnessed for T0 w.r.t. σ.
It is straightforward to show, however, that φ is ﬁnitely witnessed for T0 w.r.t.
σ. Moreover, the function wtn deﬁned by wtn(φ) = (φ ∧w1 = w1 ∧w2 = w2)
for fresh w1, w2 is a witness for T0 w.r.t. σ, but not a strong one. This does not
show, however, that T0 is not strongly polite. In fact, it is indeed strongly polite
since the function wtn′(φ) = φ ∧w1 ̸= w2 for fresh w1, w2 is a strong witness for
T0 w.r.t. σ.
We introduce the notion of additivity, which ensures that the witness is able
to “absorb” arrangements and thus lift politeness to strong politeness.
Deﬁnition 10 (Additivity). Let f : QF(Σ) →QF(Σ). We say that f is S-
additive for T if f(f(φ)∧ϕ) and f(φ)∧ϕ are T-equivalent and have the same set
of S-sorted variables for every φ, ϕ ∈QF(Σ), provided that ϕ is a conjunction
of ﬂat literals such that every term in ϕ is a variable whose sort is in S. When
T is clear from the context, we just say that f is S-additive. We say that T is
additively ﬁnitely witnessable w.r.t. S if there exists a witness for T w.r.t. S
which is both computable and S-additive. T is said to be additively polite w.r.t.
S if it is smooth and additively ﬁnitely witnessable w.r.t. S.

246
Y. Sheng et al.
Proposition 1. Let wtn be a witness for T w.r.t. S. If wtn is S-additive then
it is a strong witness for T w.r.t. S.2
Corollary 1. Suppose T is additively polite w.r.t. S. Then it is strongly polite
w.r.t. S.
The theory T0 from the example above is additively ﬁnitely witnessable w.r.t.
σ, even though wtn′ is not σ-additive. Indeed, it is possible to deﬁne a new
witness for T0 w.r.t. σ, say wtn′′, which is σ-additive. This function wtn′′ is
deﬁned by: wtn′′(φ) = wtn′(φ) if φ is a conjunction that includes some disequality
x ̸= y for some x, y. Otherwise, wtn′′(φ) = φ.
T0 is an existential theory: it consists of all the structures that satisfy an
existential sentence (in this case, ∃x, y . x ̸= y). The construction of wtn′′ can be
generalized to any existential theory. Such theories are also smooth w.r.t. any
set of sorts and so existential theories are additively polite.
The notion of additive witnesses is useful for proving that a polite theory is
strongly polite. In particular, the witnesses for the theories of equality, arrays,
sets and multisets from [21] are all additive, and so strong politeness of these
theories follows from their politeness. The same will hold later, when we conclude
strong politeness of theories of algebraic datatypes from their politeness.
4
Politeness for the SMT-LIB 2 Theory of Datatypes
Let Σ be a datatypes signature with SΣ = Elem ⊎Struct and FΣ = CO ⊎SE.
In this section, we prove that TΣ is strongly polite with respect to Elem. In
Sect. 4.1, we consider theories with only inductive sorts, and consider theories
with only ﬁnite sorts in Sect. 4.2. We combine them in Sect. 4.3, where arbi-
trary theories of datatypes are considered. This separation is only needed for
ﬁnite witnessability. For smoothness, however, it is straightforward to show that
the Elem domain of a given interpretation can always be augmented without
changing satisﬁability of quantiﬁer-free formulas.
Lemma 1. TΣ is smooth w.r.t. Elem.
Lemma 1 holds for any datatypes signature.
4.1
Inductive Datatypes
In this section, we assume that all sorts in Struct are inductive.
To prove ﬁnite witnessability, we now introduce an additive witness func-
tion. Following arguments from [21], it suﬃces to deﬁne the witness only for
conjunctions of ﬂat literals. A complete witness can then use the restricted one
by ﬁrst transforming the input formula to ﬂat DNF form and then creating a
2 Due to lack of space, some proofs have been omitted. They can be found in an
extended version at https://arxiv.org/abs/2004.04854.

Politeness for the Theory of Algebraic Datatypes
247
disjunction where each disjunct is the result of applying the witness on the corre-
sponding disjunct. Similarly, it suﬃces to show that wtn(φ) is ﬁnitely witnessed
for φ which is a conjunction of ﬂat literals. Essentially, our witness guesses pos-
sible constructors for variables whose constructors are not explicit in the input
formula.
Deﬁnition 11 (A Witness for TΣ). Let φ be a quantiﬁer-free conjunction of
ﬂat Σ-literals. wtni(φ) is obtained from φ by performing the following steps:
1. For any literal of the form y = sc,i(x) such that x = c(−→
u1, y, −→
u2) does not occur
in φ and x = d(−→
ud) does not occur in φ for any −→
u1, −→
u2, −→
ud, we conjunctively
add x = c(−→
u1, y, −→
u2) ∨(
d̸=c x = d(−→
ud)) with fresh −→
u1, −→
u2, −→
ud, where c and d
range over CO.
2. For any literal of the form isc(x) such that x = c(−→u ) does not occur in φ for
any −→u , we conjunctively add x = c(−→u ) with fresh −→u .
3. For any literal of the form ¬isc(x) such that x = d(−→
ud) does not occur in φ
for any d ̸= c and −→
ud, we conjunctively add 
d̸=c x = d(−→
ud), with fresh −→
ud.
4. For any sort σ ∈Elem such that φ does not include a variable of sort σ we
conjunctively add a literal x = x for a fresh variable x of sort σ.
Example 5. Let φ be the Σlist-formula y = cdr(x) ∧y′ = cdr(x) ∧iscons(y).
wtni(φ) is φ ∧(x = nil ∨x = cons(e, y)) ∧(x = nil ∨x = cons(e′, y′)) ∧y =
cons(e′′, z) ∧e′′′ = e′′′ where e, e′, e′′, e′′′, z are fresh.
In Deﬁnition 11, Item 1 guesses the constructor of the argument for the selec-
tor. Items 2 and 3 correspond to the semantics of testers. Item 4 is meant to
ensure that we can construct a ﬁnite witness with non-empty domains. The
requirement for absence of literals before adding literals or disjunctions to φ is
used to ensure additivity of wtni. And indeed:
Lemma 2. wtni is Elem-additive.
Further, it can be veriﬁed that:
Lemma 3. Let φ be a conjunction of ﬂat literals. φ and ∃−→
w . Γ are TΣ-
equivalent, where Γ = wtni(φ) and −→
w = vars(Γ) \ vars(φ).
The remainder of this section is dedicated to the proof of the following lemma:
Lemma 4 (Finite Witnessability).
Let φ be a conjunction of ﬂat literals.
Then, Γ = wtni(φ) is ﬁnitely witnessed for TΣ with respect to Elem.
Suppose that Γ is TΣ-satisﬁable, and let A be a satisfying TΣ-interpretation.
We deﬁne a TΣ-interpretation B as follows, and then show that B is a ﬁnite wit-
ness of Γ for TΣ w.r.t. Elem. First for every σ ∈Elem we set σB = varsσ(Γ)A,
and for every variable e ∈varsσ(Γ), we set eB = eA. The interpretations of
Struct-sorts, testers and constructors are uniquely determined by the theory. It
is left to deﬁne the interpretation of Struct-variables in B, as well as the inter-
pretation of the selectors (the interpretation of selectors is ﬁxed by the theory
only when applied to the “right” constructor). We do this in several steps:

248
Y. Sheng et al.
Step 1 – Simplifying Γ: since φ is a conjunction of ﬂat literals, Γ is a con-
junction whose conjuncts are either ﬂat literals or disjunctions of ﬂat literals
(introduced in Items 1 and 3 of Deﬁnition 11). Since A |= Γ, A satisﬁes exactly
one disjunct of each such disjunction. We can thus obtain a formula Γ1 from Γ
by replacing every disjunction with the disjunct that is satisﬁed by A. Notice
that A |= Γ1 and that it is a conjunction of ﬂat literals. Let Γ2 be obtained from
Γ1 by removing any literal of the form isc(x) and any literal of the form ¬isc(x).
Let Γ3 be obtained from Γ2 by removing any literal of the form x = sc,i(y). For
convenience, we denote Γ3 by Γ ′. Obviously, A |= Γ ′, and Γ ′ is a conjunction of
ﬂat literals without selectors and testers.
Step 2 – Working with Equivalence Classes: We would like to preserve
equalities between Struct-variables from A. To this end, we group all variables
in vars(Γ) to equivalence classes according to their interpretation in A. Let ≡A
denote an equivalence relation over vars(Γ) such that x ≡A y iﬀxA = yA.
We denote by [x] the equivalence class of x. Let α be an equivalence class, thus
αA =

xA | x ∈α

is a singleton. Identifying this singleton with its only element,
we have that αA denotes aA for an arbitrary element a of the equivalence class α.
Step 3 – Ordering Equivalence Classes: We would also like to preserve
disequalities between Struct-variables from A. Thus we introduce a relation ≺
over the equivalence classes, such that α ≺β if y = c(w1, . . . , wn) occurs as one
of the conjuncts in Γ ′ for some w1, . . . , wn and c such that wk ∈α for some
y ∈β, c ∈CO, and k. Call an equivalence class α nullary if A |= isc(x) for
some x ∈α and nullary constructor c. Call an equivalence class α minimal if
β ̸≺α for every β. Notice that each nullary equivalence class is minimal. The
relation ≺induces a directed acyclic graph (DAG), denoted G. The vertices are
the equivalence classes. Whenever α ≺β, we draw an edge from vertex α to β.
Step 4 – Interpretation of Equivalence Classes: We deﬁne αB for every
equivalence class α. Then, xB is simply deﬁned as [x]B, for every Struct-variable
x. The idea goes as follows. Nullary classes are assigned according to A. Other
minimal classes are assigned arbitrarily, but it is important to assign diﬀerent
classes to terms whose depths are far enough from each other to ensure that the
disequalities in A are preserved. Non-minimal classes are uniquely determined
after minimal ones are assigned. Formally, let m be the number of equivalence
classes, l the number of minimal equivalence classes, r the number of nullary
equivalence classes, and α1, . . . , αm a topological sort of G, such that all minimal
classes occur before all others, and the ﬁrst r classes are nullary. Let d be the
length of the longest path in G. We deﬁne αB
i by induction on i. In the deﬁnition,
we use BElem to denote the Elem-sorted set assigning σB to every σ ∈Elem.
1. If 0 < r and i ≤r then αi is a nullary class and so we set αB
i = αA
i .
2. If r < i ≤l then αi is minimal and not nullary. Let σ be the sort of
variables in αi. If σ ∈Elem, then all variables in the class have already
been deﬁned. Otherwise, σ ∈Struct. In this case, we deﬁne αB
i to be an
arbitrary element of Tσ(Σ|CO, BElem) that has depth strictly greater than
max

depth(αB
j ) | 0 < j < i

+ d (here max ∅= 0).

Politeness for the Theory of Algebraic Datatypes
249
3. If i > l then we set αB
i = c(βB
1 , . . . , βB
n ) for the unique equivalence classes
β1, . . . , βn ⊆{α1, . . . , αi−1} and c such that y = c(x1, . . . , xn) occurs in Γ ′
for some y ∈αi and x1 ∈β1, . . . , xn ∈βn.
Since Σ is a datatypes signature in which all Struct-sorts are inductive, the
second case of the deﬁnition is well-deﬁned. Further, the topological sort ensures
β1, . . . , βn exist, and the partition to equivalence classes ensures that they are
unique. Hence:
Lemma 5. αB
i is well-deﬁned.
Step 5 – Interpretation of Selectors: Let sc,i ∈SE for c : σ1×. . .×σn →σ,
1 ≤i ≤n and a ∈σB. If a ∈isB
c , we must have a = c(a1, . . . , an) for some
a1 ∈σB
1 , . . . , an ∈σB
n. We then set sB
c,i(a) = ai. Otherwise, we consider two
cases. If xB = a for some x ∈vars(Γ) such that y = sc,i(x) occurs in Γ2 for
some y, we set sB
c,i(a) = yB. Otherwise, sB
c,i(a) is set arbitrarily.
Example 6. Let Γ be the following Σlist-formula: x1 = cons(e1, x2) ∧x3 =
cons(e2, x4)∧x2 ̸= x4. Then Γ ′ = Γ. We have the following satisfying interpreta-
tion A: elemA = {1, 2, 3, 4}, eA
1 = 1, e2A = 2, x1A = [1, 2, 3], x2A = [2, 3], x3A =
[2, 2, 4], x4A = [2, 4]. The construction above yields the following interpretation
B: elemB = {1, 2}, e1B = 1, e2B = 2. For list-variables, we proceed as follows.
The equivalence classes of list-variables are [x1], [x2], [x3], [x4], with [x2] ≺[x1]
and [x4] ≺[x3]. The length of the longest path in G is 1. Assuming [x2] comes
before [x4] in the topological sort, xB
2 will get an arbitrary list over {1, 2} with
length greater than 1 (the depth of eB
2 plus the length of the longest path), say,
[1, 1, 1]. xB
4 will then get an arbitrary list of length greater than 4 (the depth of
xB
2 plus the length of the longest path). Thus we could have xB
4 = [1, 1, 1, 1, 1].
Then, xB
1 = [1, 1, 1, 1] and xB
3 = [2, 1, 1, 1, 1, 1].
Now that B is deﬁned, it is left to show that it is a ﬁnite witness of Γ for
TΣ w.r.t. Elem. By construction, σB = varsσ(Γ)B for every σ ∈Elem. B also
preserves the equalities and disequalities in A, and by considering every shape
of a literal in Γ ′ we can prove that B |= Γ ′. Our interpretation of the selectors
then ensures that:
Lemma 6. B |= Γ.
Lemma 6, together with the deﬁnition of the domains of B, gives us that B
is a ﬁnite witness of Γ for TΣ w.r.t. Elem, and so Lemma 4 is proven. As a
corollary of Lemmas 1, 2 and 4, strong politeness is obtained.
Theorem 2. If Σ is a datatypes signature and all sorts in StructΣ are induc-
tive, then TΣ is strongly polite w.r.t. ElemΣ.

250
Y. Sheng et al.
4.2
Finite Datatypes
In this section, we assume that all sorts in Struct are ﬁnite.
For ﬁnite witnessability, we deﬁne the following witness, that guesses the
construction of each Struct-variables until a ﬁxpoint is reached. For every
quantiﬁer-free conjunction of ﬂat Σ-literals φ, deﬁne the sequence φ0, φ1, . . .,
such that φ0 = φ, and for every i ≥0, φi+1 is obtained from φi by conjuncting
it with a disjunction 
c∈CO x = c(wc
1, . . . , wc
nc) for fresh wc
1, . . . , wc
nc, where x
is some arbitrary Struct-variable in φi such that there is no literal of the form
x = c(y1, . . . , yn) in φi for any constructor c and variables y1, . . . , yn, if such
x exists. Since Struct only has ﬁnite sorts, this sequence becomes constant at
some φk.
Deﬁnition 12 (A Witness for TΣ).
wtnf (φ) is φk for the minimal k such
that φk = φk+1.
Example 7. Let φ be the Σpair-formula x = first(y) ∧x′ = first(y′) ∧x ̸= x′.
wtnf (φ) is φ ∧y = pair(e1, e2) ∧y′ = pair(e3, e4).
Similarly to Sect. 4.1, we have:
Lemma 7. wtnf is Elem-additive.
Lemma 8. φ and ∃−→
w . wtnf (φ) are TΣ-equivalent, where −→
w = vars(wtnf (φ)) \
vars(φ).
We now prove the following lemma:
Lemma 9 (Finite Witnessability).
Let φ be a conjunction of ﬂat literals.
Then, wtnf (φ) is ﬁnitely witnessed for TΣ with respect to Elem.
Suppose Γ
= wtnf (φ) is TΣ-satisﬁable, and let A be a satisfying TΣ-
interpretation. We deﬁne a TΣ-interpretation B which is a ﬁnite witness of Γ for
TΣ w.r.t. Elem. We set σB = varsσ(Γ)A for every σ ∈Elem, eB = eA, for every
variable e ∈varsElem(Γ) and xB = xA for every variable x ∈varsStruct(Γ).
Selectors are also interpreted as they are interpreted in A. This is well-deﬁned:
for any Struct-variable x, every element in σA for σ ∈Elem that occurs in xA
has a corresponding variable e in Γ such that eA is that element. This holds by
the ﬁniteness of the sorts in Struct and the deﬁnition of wtnf . Further, for any
Struct-variable x such that sc,i(x) occurs in Γ, we must have that it occurs in
some literal of the form y = sc,i(x) of Γ. Similarly to the above, all elements
that occur in yA and xA have corresponding variables in Γ. Therefore, B |= Γ
is a trivial consequence of A |= Γ. By the deﬁnition of its domains, B is a ﬁnite
witness of Γ for TΣ w.r.t. Elem, and so Lemma 9 is proven. Then, by Lemmas 1
7 and 9 , strong politeness is obtained.
Theorem 3. If Σ is a datatypes signature and all sorts in StructΣ are ﬁnite,
then TΣ is strongly polite w.r.t. ElemΣ.

Politeness for the Theory of Algebraic Datatypes
251
4.3
Combining Finite and Inductive Datatypes
Now we consider the general case. Let Σ be a datatypes signature. We prove that
TΣ is strongly polite w.r.t. Elem. We show that there are datatypes signatures
Σ1, Σ2 ⊆Σ such that TΣ = TΣ1 ⊕TΣ2, and then use Theorem 1. In Σ1, inductive
sorts are excluded, while in Σ2, ﬁnite sorts are considered to be element sorts.
Formally, we set Σ1 as follows: where ElemΣ1 = ElemΣ and StructΣ1 =
Fin(Σ). FΣ1 = COΣ1 ⊎SEΣ1, where COΣ1 = {c : σ1 × . . . × σn →σ | c ∈
COΣ, σ ∈StructΣ1} and SEΣ1 and PΣ1 are the corresponding selectors and
testers. Notice that if σ is ﬁnite and c : σ1 × . . . × σn →σ is in COΣ, then σi
must be ﬁnite or in ElemΣ for every 1 ≤i ≤n. Next, we set Σ2 as follows:
SΣ2 = ElemΣ2 ⊎StructΣ2, where ElemΣ2 = ElemΣ ∪Fin(Σ) and StructΣ2 =
Ind(Σ). FΣ2 = COΣ2 ⊎SEΣ2, where COΣ2 = {c : σ2 × . . . × σn →σ | c ∈
COΣ, σ ∈StructΣ2} and SEΣ2 and PΣ2 are the corresponding selectors and
testers. Thus, TΣ = TΣ1 ⊕TΣ2. Now set S = ElemΣ ∪Fin(Σ), S1 = ElemΣ,
S2 = ElemΣ ∪Fin(Σ), T1 = TΣ1, and T2 = TΣ2.
By Theorem 3, T1 is strongly polite w.r.t. S1 and by Theorem 2, T2 is strongly
polite w.r.t. S2. By Theorem 1 we have:
Theorem 4. If Σ is a datatypes signature then TΣ is strongly polite w.r.t.
ElemΣ.
Remark 1. A concrete witness for TΣ in the general case, that we call wtnΣ,
is obtained by ﬁrst applying the witness from Deﬁnition 11 and then applying
the witness from Deﬁnition 12 on the literals that involve ﬁnite sorts. A direct
ﬁnite witnessability proof can be obtained by using the same arguments from
the proofs of Lemmas 4 and 9. This witness is simpler than the one produced in
the proof from [14] of Theorem 1, that involves puriﬁcation and arrangements. In
our case, we do not consider arrangements, but instead notice that the resulting
function is additive, and hence ensures strong ﬁnite witnessability.
5
Axiomatizations
In this section, we discuss the possible connections between the politeness of TΣ
and some axiomatizations of trees. We show how to get a reduction of any TΣ-
satisﬁability problem into a satisﬁability problem modulo an axiomatized theory
of trees. The latter can be decided using syntactic uniﬁcation.
Let Σ be a datatypes signature. The set TREE ∗
Σ of axioms is deﬁned as
the union of all the sets of axioms in Fig. 1 (where upper case letters denote
implicitly universally quantiﬁed variables). Let TREE Σ be the set obtained from
TREE ∗
Σ by dismissing Ext1 and Ext2. Note that because of Acyc, we have that
TREE Σ is inﬁnite (that is, consists of inﬁnitely many axioms) unless all sorts in
Struct are ﬁnite. TREE Σ is a generalization of the theory of Absolutely Free
Data Structures (AFDS) from [9] to many-sorted signatures with selectors and
testers. In what follows we identify TREE Σ (and TREE ∗
Σ) with the class of
structures that satisfy them when there is no ambiguity.

252
Y. Sheng et al.
(Inj)
{c(X1, . . . , Xn) = c(Y1, . . . , Yn)
n
i=1 Xi = Yi | c ∈CO}
(Dis)
{c(X1, . . . , Xn) ̸= d(Y1, . . . , Ym) | c, d ∈CO, c ̸= d}
(Proj)
{sc,i(c(X1, . . . , Xn)) = Xi | c ∈CO, i ∈[1, n]}
(Is1)
{isc(c(X1, . . . , Xn)) | c ∈CO}
(Is2)
{¬isc(d(X1, . . . , Xn)) | c, d ∈CO, c ̸= d}
(Acyc)
{X ̸= t[X] | t is a non-variable Σ|CO-term that contains X }
(Ext1)
{
c:σ1×...×σn
σ∈CO isc(X) | σ ∈Struct}
(Ext2)
{∃y . isc(X)
X = c( y ) | c ∈CO}
Fig. 1. Axioms for TREE Σ and TREE ∗
Σ
Proposition 2. Every TREE ∗
Σ-unsatisﬁable formula is TΣ-unsatisﬁable.
Remark 2. Along the lines of [1], a superposition calculus can be applied to get
a TREE Σ-satisﬁability procedure. Such a calculus has been used in [6,9] for
a theory of trees with selectors but no testers. To handle testers, one can use
a classical encoding of predicates into ﬁrst-order logic with equality, by repre-
senting an atom isc(x) as a ﬂat equality Isc(x) = T where Isc is now a unary
function symbol and T is a constant. Then, a superposition calculus dedicated
to TREE Σ can be obtained by extending the standard superposition calculus [1]
with some expansion rules, one for each axiom of TREE Σ [9]. For the axioms Is1
and Is2, the corresponding expansion rules are respectively x = c(x1, . . . , xn) ⊢
Isc(x) = T if c ∈CO, and x = d(x1, . . . , xn) ⊢Isc(x) ̸= T if c, d ∈CO, c ̸= d.
Further, consider the theory of ﬁnite trees deﬁned from TREE Σ by dismissing
Proj, Is1 and Is2. Being deﬁned by Horn clauses, it is convex. Further, it is a
Shostak theory [16,18,24] admitting a solver and a canonizer [9]. The solver is
given by a syntactic uniﬁcation algorithm [2] and the canonizer is the identity
function. The satisﬁability procedure built using the solver and the canonizer
can be applied to decide TREE Σ-satisﬁability problems containing Σ|CO-atoms.
The following result shows that any TΣ-satisﬁability problem can be reduced
to a TREE Σ-satisﬁability problem. This leads to a TΣ-satisﬁability procedure.
Proposition 3. Let Σ be a ﬁnite datatypes signature and ϕ any conjunction
of ﬂat Σ-literals including an arrangement over the variables in ϕ. Then, there
exists a Σ-formula ϕ′ such that:
1. ϕ and ∃−→
w . ϕ′ are TΣ-equivalent, where −→
w = vars(ϕ′)\vars(ϕ).
2. ϕ′ is TΣ-satisﬁable iﬀϕ′ is TREE Σ-satisﬁable.
Proposition 3 can be easily lifted to any conjunction of Σ-literals ϕ by ﬂat-
tening and then guessing all possible arrangements over the variables. Further,
∃−→
w . ϕ′ and ϕ are not only TΣ-equivalent but also TREE ∗
Σ-equivalent. As a
consequence, Proposition 3 also holds when stated using TREE ∗
Σ instead of TΣ.

Politeness for the Theory of Algebraic Datatypes
253
We conclude this section with a short discussion on the connection to Sect. 4.
Both the current section and Sect. 4 rely on two constructions: (i) A formula
transformation (wtnΣ in Sect. 4, ϕ →ϕ′ in the current section); and (ii) A small
model construction (ﬁnite witnessability in Sect. 4, equisatisﬁability between TΣ
and TREE in Proposition 3). While these constructions are similar in both sec-
tions, they are not the same. A nice feature of the constructions of Sect. 4 is
that they clearly separate between steps (i) and (ii). The witness is very sim-
ple, and amounts to adding to the input formula literals and disjunctions that
trivially follow from the original formula in TΣ. Then, the resulting formula is
post-processed in step (ii), according to a given satisfying interpretation. Hav-
ing a satisfying interpretation allows us to greatly simplify the formula, and the
simpliﬁed formula is useful for the model construction. In contrast, the satisfying
TREE Σ-interpretation that we start with in step (ii) of the current section is
not necessarily a TΣ-interpretation, which makes the approach of Sect. 4 incom-
patible, compared to the syntactic uniﬁcation approach that we employ here. For
that, some of the post-processing steps of Sect. 4 are employed in step (i) itself,
in order to eliminate all testers and as much selectors as possible. In addition,
a pre-processing is applied in order to include an arrangement. The constructed
interpretation ﬁnitely witnesses ϕ′ and so this technique can be used to produce
an alternative proof of strong politeness.
6
Conclusion
In this paper we have studied the theory of algebraic datatypes, as it is deﬁned
by the SMT-LIB 2 standard. Our investigation included both ﬁnite and inductive
datatypes. For this theory, we have proved that it is strongly polite, making it
amenable for combination with other theories by the polite combination method.
Our proofs used the notion of additive witnesses, also introduced in this paper.
We concluded by extending existing axiomatizations and a decision procedure
of trees to support this theory of datatypes.
There are several directions for further research that we plan to explore.
First, we plan to continue to prove that more important theories are strongly
polite, with an eye to recent extensions of the datatypes theory, namely datatypes
with shared selectors [23] and co-datatypes [22]. Second, we envision to further
investigate the possibility to prove politeness using superposition-based satisﬁa-
bility procedures. Third, we plan to study extensions of the theory of datatypes
corresponding to ﬁnite trees including function symbols with some equational
properties such as associativity and commutativity to model data structures such
as multisets [25]. We want to focus on the politeness of such extensions. Initial
work in that direction has been done in [5], that we plan to build on.
Acknowledgments. We are thankful to the anonymous reviewers for their comments.

254
Y. Sheng et al.
References
1. Armando, A., Bonacina, M.P., Ranise, S., Schulz, S.: New results on rewrite-based
satisﬁability procedures. ACM Trans. Comput. Log. 10(1), 4:1–4:51 (2009)
2. Baader, F., Snyder, W., Narendran, P., Schmidt-Schauß, M., Schulz, K.U.: Uni-
ﬁcation theory. In: Robinson, J.A., Voronkov, A. (eds.) Handbook of Automated
Reasoning (in 2 volumes), pp. 445–532. Elsevier and MIT Press (2001)
3. Barrett, C., Fontaine, P., Tinelli, C.: The SMT-LIB Standard: Version 2.6. Techni-
cal report, Department of Computer Science, The University of Iowa (2017). www.
SMT-LIB.org
4. Barrett, C.W., Shikanian, I., Tinelli, C.: An abstract decision procedure for a
theory of inductive data types. J. Satisﬁability Boolean Model. Comput. 3(1–2),
21–46 (2007)
5. Berthon, R., Ringeissen, C.: Satisﬁability modulo free data structures combined
with bridging functions. In: King, T., Piskac, R. (eds.) Proceedings of SMT@IJCAR
2016. CEUR Workshop Proceedings, vol. 1617, pp. 71–80. CEUR-WS.org (2016)
6. Bonacina, M.P., Echenim, M.: Rewrite-based satisﬁability procedures for recursive
data structures. Electron. Notes Theor. Comput. Sci. 174(8), 55–70 (2007)
7. Bonacina, M.P., Fontaine, P., Ringeissen, C., Tinelli, C.: Theory combination:
beyond equality sharing. In: Lutz, C., Sattler, U., Tinelli, C., Turhan, A.-Y.,
Wolter, F. (eds.) Description Logic, Theory Combination, and All That. LNCS,
vol. 11560, pp. 57–89. Springer, Cham (2019). https://doi.org/10.1007/978-3-030-
22102-7 3
8. Casal, F., Rasga, J.: Many-sorted equivalence of shiny and strongly polite theories.
J. Autom. Reasoning 60(2), 221–236 (2018)
9. Chocron, P., Fontaine, P., Ringeissen, C.: Politeness and combination methods for
theories with bridging functions. J. Autom. Reasoning 64(1), 97–134 (2020)
10. Enderton, H.B.: A Mathematical Introduction to Logic. Academic Press (2001)
11. Fontaine, P.: Combinations of theories for decidable fragments of ﬁrst-order
logic. In: Ghilardi, S., Sebastiani, R. (eds.) FroCoS 2009. LNCS (LNAI), vol.
5749, pp. 263–278. Springer, Heidelberg (2009). https://doi.org/10.1007/978-3-
642-04222-5 16
12. Guti´errez, R., Meseguer, J.: Variant-based decidable satisﬁability in initial algebras
with predicates. In: Fioravanti, F., Gallagher, J.P. (eds.) LOPSTR 2017. LNCS,
vol. 10855, pp. 306–322. Springer, Cham (2018). https://doi.org/10.1007/978-3-
319-94460-9 18
13. Hojjat, H., R¨ummer, P.: Deciding and interpolating algebraic data types by reduc-
tion. In: Jebelean, T., Negru, V., Petcu, D., Zaharie, D., Ida, T., Watt, S.M. (eds.)
19th International Symposium on Symbolic and Numeric Algorithms for Scien-
tiﬁc Computing, SYNASC 2017, Timisoara, Romania, 21–24 September 2017, pp.
145–152. IEEE Computer Society (2017)
14. Jovanovi´c, D., Barrett, C.: Polite theories revisited. In: Ferm¨uller, C.G., Voronkov,
A. (eds.) LPAR 2010. LNCS, vol. 6397, pp. 402–416. Springer, Heidelberg (2010).
https://doi.org/10.1007/978-3-642-16242-8 29
15. Kov´acs, L., Robillard, S., Voronkov, A.: Coming to terms with quantiﬁed reasoning.
In: Castagna, G., Gordon, A.D. (eds.) Proceedings of the 44th ACM SIGPLAN
Symposium on Principles of Programming Languages, POPL 2017, Paris, France,
18–20 January 2017, pp. 260–270. ACM (2017)
16. Krstic, S., Conchon, S.: Canonization for disjoint unions of theories. Inf. Comput.
199(1–2), 87–106 (2005)

Politeness for the Theory of Algebraic Datatypes
255
17. Krsti´c, S., Goel, A., Grundy, J., Tinelli, C.: Combined satisﬁability modulo para-
metric theories. In: Grumberg, O., Huth, M. (eds.) TACAS 2007. LNCS, vol.
4424, pp. 602–617. Springer, Heidelberg (2007). https://doi.org/10.1007/978-3-
540-71209-1 47
18. Manna, Z., Zarba, C.G.: Combining decision procedures. In: Aichernig, B.K.,
Maibaum, T. (eds.) Formal Methods at the Crossroads. From Panacea to Founda-
tional Support. LNCS, vol. 2757, pp. 381–422. Springer, Heidelberg (2003). https://
doi.org/10.1007/978-3-540-40007-3 24
19. Meseguer, J.: Variant-based satisﬁability in initial algebras. Sci. Comput. Program.
154, 3–41 (2018)
20. Nelson, G., Oppen, D.C.: Simpliﬁcation by cooperating decision procedures. ACM
Trans. Program. Lang. Syst. 1(2), 245–257 (1979)
21. Ranise, S., Ringeissen, C., Zarba, C.G.: combining data structures with nonsta-
bly inﬁnite theories using many-sorted logic. In: Gramlich, B. (ed.) FroCoS 2005.
LNCS, vol. 3717, pp. 48–64. Springer, Heidelberg (2005). https://doi.org/10.1007/
11559306 3. extended technical report is available at https://hal.inria.fr/inria-
00070335/
22. Reynolds, A., Blanchette, J.C.: A decision procedure for (co)datatypes in SMT
solvers. J. Autom. Reasoning 58(3), 341–362 (2017)
23. Reynolds, A., Viswanathan, A., Barbosa, H., Tinelli, C., Barrett, C.: Datatypes
with shared selectors. In: Galmiche, D., Schulz, S., Sebastiani, R. (eds.) IJCAR
2018. LNCS (LNAI), vol. 10900, pp. 591–608. Springer, Cham (2018). https://doi.
org/10.1007/978-3-319-94205-6 39
24. Shostak, R.E.: A practical decision procedure for arithmetic with function symbols.
J. ACM 26(2), 351–360 (1979)
25. Sofronie-Stokkermans, V.: Locality results for certain extensions of theories
with bridging functions. In: Schmidt, R.A. (ed.) CADE 2009. LNCS (LNAI), vol.
5663, pp. 67–83. Springer, Heidelberg (2009). https://doi.org/10.1007/978-3-642-
02959-2 5
26. Tinelli, C., Zarba, C.G.: Combining decision procedures for sorted theories. In:
Alferes, J.J., Leite, J. (eds.) JELIA 2004. LNCS (LNAI), vol. 3229, pp. 641–653.
Springer, Heidelberg (2004). https://doi.org/10.1007/978-3-540-30227-8 53
27. Tinelli, C., Zarba, C.G.: Combining nonstably inﬁnite theories. J. Autom. Reason-
ing 34(3), 209–238 (2005)

Superposition

A Knuth-Bendix-Like Ordering
for Orienting Combinator Equations
Ahmed Bhayat(B)
and Giles Reger(B)
University of Manchester, Manchester, UK
giles.reger@manchester.ac.uk
Abstract. We extend the graceful higher-order basic Knuth-Bendix
order (KBO) of Becker et al. to an ordering that orients combinator equa-
tions left-to-right. The resultant ordering is highly suited to parameter-
ising the ﬁrst-order superposition calculus when dealing with the theory
of higher-order logic, as it prevents inferences between the combinator
axioms. We prove a number of desirable properties about the ordering
including it having the subterm property for ground terms, being transi-
tive and being well-founded. The ordering fails to be a reduction ordering
as it lacks compatibility with certain contexts. We provide an intuition
of why this need not be an obstacle when using it to parameterise super-
position.
1
Introduction
There exists a wide range of methods for automated theorem proving in higher-
order logic. Some provers such as AgsyHOL [17], Satallax [10] and Leo-II [4]
implement dedicated higher-order proof calculi. A common approach, followed
by the Leo-III prover [21], is to use a co-operative architecture with a dedicated
higher-order prover working in conjunction with a ﬁrst-order prover. It has long
been part of theorem proving folklore that sound and complete translations
from higher-order to ﬁrst-order logic exist. Kerber [15] proves this result for a
higher-order logic that does not assume comprehension axioms (otherwise known
as applicative ﬁrst-order logic). Thus, translating higher-order problems to ﬁrst-
order logic and running ﬁrst-order provers on the translations is another method
of automated higher-order theorem proving. Variations of this method are widely
utilised by interactive theorem provers and their hammers such as Sledgehammer
[18] and the CoqHammer [11].
Almost all translations to ﬁrst-order logic translate λ-expressions using com-
binators. It is well known that the set of combinators S, K and I is suﬃcient
to translate any λ-expression. For purposes of completeness, these combina-
tors must be axiomatised: S⟨τ1, τ2, τ3⟩x y z = x z (y z), K⟨τ1, τ2⟩x y = x and
I⟨τ⟩x = x. If translating to a monomorphic logic a ﬁnite set of axioms cannot
achieve completeness.
However, till now, translation based methods have proven disappointing and
only achieved decent results with interactive theorem provers when the problems
c
⃝Springer Nature Switzerland AG 2020
N. Peltier and V. Sofronie-Stokkermans (Eds.): IJCAR 2020, LNAI 12166, pp. 259–277, 2020.
https://doi.org/10.1007/978-3-030-51074-9_15

260
A. Bhayat and G. Reger
are ﬁrst-order or nearly ﬁrst-order [22]. One major reason for this is that infer-
ences between combinator axioms can be hugely explosive. A common ﬁrst-order
proof calculus is superposition [19]. Consider a superposition inference from the
K axiom onto the right-hand of the S axiom. The result is S K y z = z. There is
little to restrict such inferences.
Superposition is parameterised by a simpliﬁcation ordering and inferences
are only carried out on the larger side of literals with respect to this ordering.
Inferences are not carried out at variables. Consider the S-, K- and I-axioms
given above. There can clearly be no uniﬁers between a subterm of the left
side of one axiom and the left side of another except at a variable. Thus, if
a simpliﬁcation ordering exists that orients the axioms left-to-right, inferences
amongst the axioms would be impossible.
Currently, no such simpliﬁcation ordering is known to exist and the authors
suspect that no such ordering can exist. Whilst there is a large body of work
on higher-order orderings, all either lack some property required for them to be
simpliﬁcation orderings or are unsuitable for orienting the combinator axioms.
Jouannaud and Rubio introduced a higher-order version of the recursive path
order called HORPO [14]. HORPO is compatible with β-reduction which sug-
gests that without much diﬃculty it could be modiﬁed to be compatible with
weak reduction. However, the ordering does not enjoy the subterm property, nor
is it transitive. Likewise, is the case for orderings based on HORPO such as
the computability path ordering [8] and the iterative HOIPO of Kop and Van
Raamsdonk [16]. More recently, a pair of orderings for λ-free higher-order terms
have been developed [2,7]. These orderings lack a speciﬁc monotonicity prop-
erty, but this does not prevent their use in superposition [3]. However, neither
ordering orients combinator axioms directly.
We investigate an extension of the graceful higher-order basic KBO >hb intro-
duced by Becker et al. [2]. Our new ordering, >ski, orients combinator equations
left-to-right. Thus, if it is used to parameterise a superposition calculus, there
can be no inferences among the axioms. The >ski ordering lacks full compatibility
with contexts which is normally a requirement for an ordering to parameterise
superposition. In particular, the ordering is not compatible with the so-called
unstable contexts. In separate work we show that this is not an obstacle to
achieving completeness [5].
A complete superposition calculus for HOL already exists [3]. This calculus
has the λ-calculus rather than combinatory logic as its underlying logic. It also
employs higher-order uniﬁcation. There appear to be two potential beneﬁts to
using a slightly modiﬁed ﬁrst-order superposition calculus parameterised by our
new ordering >ski over lambda superposition as developed in [3].
• A superposition calculus parameterised by >ski is far closer to standard ﬁrst-
order superposition than lambda superposition. Uniﬁcation is ﬁrst-order and
there is no need to deal with binders and bound variables. This allows the
re-use of the well-studied data-structures and algorithms used in ﬁrst-order
superposition [12,20].

Combinatory KBO
261
• As discussed further in the conclusion (Sect. 6), the >ski ordering allows the
comparison of a larger class of non-ground terms than the ordering used in
[3]. This results in fewer superposition inferences.
In Sect. 2, we provide the necessary preliminaries and then move on to the main
contributions of this paper which are:
• Two approaches extending the >hb ordering by ﬁrst comparing terms by the
length of the longest weak reduction from them. The approaches diﬀer in
the manner in which they compare non-ground terms. A useful trait for an
ordering that parameterises superposition is to be able to compare a large
class of non-ground terms since this reduces the number of inferences carried
out. The most powerful method of deﬁning a non-ground ordering ≻is to
semantically lift a ground ordering, i.e., to deﬁne t ≻s to hold iﬀtθ ≻sθ for
all grounding substitutions θ. Such an ordering in non-computable and both
our methods attempt to approximate it (Sect. 3).
• A set of proofs that the introduced >ski ordering enjoys the necessary proper-
ties required for its use within the superposition calculus (Sect. 4) and a set of
examples demonstrating how the ordering applies to certain terms (Sect. 5).
2
Preliminaries
Syntax of Types and Terms: We work in a polymorphic applicative ﬁrst-order
logic. Let Vty be a set of type variables and Σty be a set of type constructors
with ﬁxed arities. It is assumed that a binary type constructor →is present in
Σty which is written inﬁx. The set of types is deﬁned:
Polymorphic Types
τ ::= κ( τn ) | α | τ →τ
where α ∈Vty and κ ∈Σty
The notation tn is used to denote a tuple or list of types or terms depending
on the context. A type declaration is of the form Π α .σ where σ is a type and
all type variables in σ appear in α . Let Σ be a set of typed function symbols
and V a set of variables with associated types. It is assumed that Σ contains
the following function symbols, known as basic combinators:
S : Πατγ.(α →τ →γ) →(α →τ) →α →γ
I : Πα.α →α
C : Πατγ.(α →τ →γ) →τ →α →γ
K : Παγ.α →γ →α
B : Πατγ.(α →γ) →(τ →α) →τ →γ
The set of terms over Σ and V is deﬁned below. In what follows, type subscripts,
and at times even type arguments, are omitted.
Terms
T ::= x | f⟨τn ⟩| t1τ1→τ2t2τ1
where x ∈V , t1, t2 ∈T , f ∈Σ, f : Π αn .σ and τn are types

262
A. Bhayat and G. Reger
The type of the term f⟨τn ⟩is σ{ αn →τn }. Following [2], terms of the form
t1 t2 are called applications. Non-application terms are called heads. A term
can uniquely be decomposed into a head and n arguments. Let t = ζ t′n . Then
head(t) = ζ where ζ could be a variable or constant applied to possibly zero
type arguments. The symbol Cany denotes a member of {S, C, B, K, I}, whilst C3
denotes a member of {S, C, B}. These symbols are only used when the combina-
tor is assumed to have a full complement of arguments. Thus, in C3 tn , n ≥3
is assumed. The symbols x, y, z . . . are reserved for variables, c, d, f . . . for non-
combinator constants and ζ, ξ range over arbitrary symbols and, by an abuse of
notation, at times even terms. A term is ground if it contains no variables and
term ground if it contains no term variables.
Positions over Terms: For a term t, if t ∈V or t = f⟨τ ⟩, then pos(t) = {ϵ}
(type arguments have no position). If t = t1t2 then pos(t) = {ϵ} ∪{i.p | 1 ≤i ≤
2, p ∈pos(ti)}. Subterms at positions of the form p.1 are called preﬁx subterms
and subterms at positions of the form p.2 are known as ﬁrst-order subterms. A
position p is strictly above a position p′ (denoted p < p′) if ∃p′′.p′′ ̸= ϵ ∧p′ =
p.p′′. Positions p and p′ are incomparable (denoted p ∥p′) if neither p < p′ nor
p′ < p, nor p = p′. By |t|, the number of symbols occurring in t is denoted. By
vars#(t) the multiset of variables in t is denoted. The expression A ⊆B means
that either A is a subset of B or A is a submultiset of B depending on whether
A and B are sets or multisets.
Stable Subterms: We deﬁne a subset of ﬁrst-order subterms called stable sub-
terms. Let LPP(t, p) (LPP stands for Longest Proper Preﬁx) be a partial function
that takes a term t and a position p and returns the longest proper preﬁx p′ of
p such that head(t|p′) is not a partially applied combinator if such a position
exists. For a position p ∈pos(t), p is a stable position in t if LPP(t, p) is not
deﬁned or head(t|LPP(t,p)) is not a combinator. A stable subterm is a subterm
occurring at a stable position and is denoted t⟨⟨u⟩⟩p. We call t⟨⟨⟩⟩p a stable context
and drop the position where it is not relevant. For example, the subterm a is
not stable in f (S a b c), S (S a) b c (in both cases, head(t|LPP(t,p)) = S) and a c (a
is not a ﬁrst-order subterm), but is in g a b and f (S a) b. A subterm that is not
stable is known as an unstable subterm.
The notation t[u]p denotes an arbitrary subterm u of t that occurs at position
p and may be unstable. The notation t[u1, . . . , un] (or t[ un ]) denotes the term t
containing n non-overlapping subterms u1 to un. By u[]n, we refer to a context
with n non-overlapping holes. Whilst this resembles the notation for a term at
position n, ambiguity is avoided by never using n to denote a position or p to
denote a natural number.
Weak Reduction: Each combinator is deﬁned by its characteristic equation;
S x y z = x z (y z), C x y z = x z y, B x y z = x (y z), K x y = x and I x = x. A
term t weak-reduces to a term t′ in one step (denoted t −→w t′) if t = u[s]p and
there exists a combinator axiom l = r and substitution σ such that lσ = s and
t′ = u[rσ]p. The term lσ in t is called a weak redex or just redex. By −→∗
w, the
reﬂexive transitive closure of −→w is denoted. If term t weak-reduces to term

Combinatory KBO
263
t′ in n steps, we write t −→n
w t′. Further, if there exists a weak-reduction path
from a term t of length n, we say that t ∈nw. Weak-reduction is terminating
and conﬂuent as proved in [13]. By (t)↓w, we denote the term formed from t by
contracting its leftmost redex.
The length of the longest weak reduction from a term t is denoted ∥t∥. This
measure is one of the crucial features of the ordering investigated in this paper.
2.1
A Maximal Weak-Reduction Strategy
To show that the measure ∥∥is computable we provide a maximal weak-reduction
strategy and prove its maximality. The strategy is used in a number of proofs
later in the paper. It is in a sense equivalent to Barendregt’s ‘perpetual strat-
egy’ in the λ-calculus [1]. Our proof of its maximality follows the style of Van
Raamsdonk et al. [23] in their proof of the maximality of a particular β-reduction
strategy. We begin by proving the fundamental lemma of maximality for combi-
natory terms.
Lemma 1 (Fundamental Lemma of Maximality). ∥Cany tn ∥= ∥(Cany tn )
↓w ∥+ 1 + isK(Cany) × ∥t2∥where isK(Cany) = 1 if Cany = K and is 0 otherwise.
The lemma holds for n ≥3 if Cany ∈{S, C, B}, n ≥2 if Cany = K and n ≥1
otherwise.
Proof. Assume that Cany = K. Then any maximal reduction from K tn is of
the form: K t1 t2 . . . tn −→m
w K t′
1 t′
2 . . . t′
n −→w t′
1 t′
3 . . . t′
n −→m′
w
s where ∥s∥=
0, t1 −→m1
w
t′
1 . . . tn −→mn
w
t′
n, ∥t2∥= m2 and m = m1 + · · · + mn. Thus,
∥K tn ∥= n
i=1 mi + 1 + m′. There is another method of reducing K tn to s:
K t1 t2 . . . tn −→m2
w
K t1 t′
2 . . . tn
−→w
t1 t3 . . . tn
−→m−m2
w
t′
1 t′
3 . . . t′
n
−→m′
w
s
As the length of this reduction is the same as the previous reduction, it must be a
maximal reduction as well. Therefore we have that: ∥K t1 t2 . . . tn∥= m+m′+1 =
(m −m2 + m′) + m2 + 1 = ∥t1 t3 . . . tn∥+ ∥t2∥+ 1
Conversely, assume that Cany is not K. We prove that the formula holds
if Cany
=
S. The other cases are similar. If Cany
=
S, any maximal
reduction from S tn must be of the form: S t1 . . . tn −→m
w
S t′
1 . . . t′
n −→w
t′
1 t′
3 (t′
2 t′
3) t′
4 . . . t′
n −→m′
w
s where ∥s∥= 0, t1 −→m1
w
t′
1 . . . tn −→mn
w
t′
n and
m = m1 + · · · + mn. There is another method of reducing S tn to s:
S t1 . . . tn −→w
t1 t3 (t2 t3) t4 . . . tn
−→m+m3
w
t′
1 t′
3 (t′
2 t′
3) t′
4 . . . tn
−→m′
w
s
Thus, we have that ∥S tn ∥= m+m′+1 ≤m+m3+m′+1 = ∥(S tn )↓w ∥+1.
Since m + m′ + 1 is the length of the maximal reduction, equality must hold.

264
A. Bhayat and G. Reger
Lemma 2. Deﬁne a map F∞from T to T as follows:
F∞(t)
=
t
if ∥t∥= 0
F∞(ζ tn )
=
ζ t1 . . . ti−1 F∞(ti) ti+1 . . . tn
where ∥tj∥= 0 for 1 ≤j < i and ζ is not a
fully applied combinator
F∞(C3 tn )
=
(C3 tn )↓w
F∞(I t1 t2 . . . tn)
=
t1 t2 . . . tn
F∞(K t1 t2 . . . tn)
=

t1 t3 . . . tn
if ∥t2∥= 0
K t1 F∞(t2) . . . tn
otherwise
The reduction strategy F∞is maximal.
Proof. As the Lemma is not of direct relevance to the paper, its proof is relegated
to the accompanying technical report [6].
3
Term Order
First, Becker et al.’s [2] graceful higher-order basic KBO is presented as it is
utilised within our ordering. The presentation here diﬀers slightly from that in
[2] because we do not allow ordinal weightings and all function symbols have
ﬁnite arities. Furthermore, we do not allow the use of diﬀerent operators for the
comparison of tuples, but rather restrict the comparison of tuples to use only
the length-lexicographic extension of the base order. This is denoted ≫length lex
hb
.
The length-lexicographic extension ﬁrst compares the lengths of tuples and if
these are equal, carries out a lexicographic comparison. For this section, terms
are assumed to be untyped following the original presentation.
3.1
Graceful Higher-Order Basic KBO
Standard ﬁrst-order KBO ﬁrst compares the weights of terms, then compares
their head-symbols and ﬁnally compares arguments recursively. When work-
ing with higher-order terms, the head symbol may be a variable. To allow the
comparison of variable heads, a mapping ghd is introduced that maps variable
heads to members of Σ that could possibly instantiate the head. This mapping
respects arities if for any variable x, all members of ghd(x) have arities greater
or equal to that of x. The mapping can be extended to constant heads by taking
ghd(f) = {f}. A substitution σ respects the mapping ghd, if for all variables x,
ghd(xσ) ⊆ghd(x).
Let ≻be a total well-founded ordering or precedence on Σ. The precedence
≻is extended to arbitrary heads by deﬁning ζ ≻ξ iﬀ∀f ∈ghd(ζ) and ∀g ∈
ghd(ξ), f ≻g. Let w be a function from Σ to N that denotes the weight of a
function symbol and W a function from T to N denoting the weight of a term.
Let ε ∈N>0. For all constants c, w(c) ≥ε. The weight of a term is deﬁned
recursively:

Combinatory KBO
265
W (f) = w(f)
W (x) = ε
W (s t) = W (s) + W (t)
The graceful higher-order basic Knuth-Bendix order >hb is deﬁned induc-
tively as follows. Let t = ζ t and s = ξ s . Then t >hb s if vars#(s) ⊆vars#(t)
and any of the following are satisﬁed:
Z1 W (t) > W (s)
Z2 W (t) = W (s) and ζ ≻ξ
Z3 W (t) = W (s), ζ = ξ and t ≫length lex
hb
s
3.2
Combinator Orienting KBO
The combinator orienting KBO is the focus of this paper. It has the property
that all ground instances of combinator axioms are oriented by it left-to-right.
This is achieved by ﬁrst comparing terms by the length of the longest weak
reduction from the term and then using >hb. This simple approach runs into
problems with regards to stability under substitution, a crucial feature for any
ordering used in superposition.
Consider the terms t = f x a and s = x b. As the length of the maximum
reduction from both terms is 0, the terms would be compared using >hb resulting
in t ≻s as W (t) > W (s). Now, consider the substitution θ = {x →I}. Then,
∥sθ∥= 1 whilst ∥tθ∥= 0 resulting in sθ ≻tθ.
The easiest and most general way of obtaining an order which is stable under
substitution would be to restrict the deﬁnition of the combinator orienting KBO
to ground terms and then semantically lift it to non-ground terms as mentioned
in the introduction. However, the semantic lifting of the ground order is non-
computable and therefore useless for practical purposes. We therefore provide
two approaches to achieving an ordering that can compare non-ground terms
and is stable under substitution both of which approximate the semantic lifting.
Both require some conditions on the forms of terms that can be compared. The
ﬁrst is simpler, but more conservative than the second.
First, in the spirit of Bentkamp et al. [3], we provide a translation that
replaces “problematic” subterms of the terms to be compared with fresh
variables. With this approach, the simple variable condition of the standard
KBO, vars#(s) ⊆vars#(t), ensures stability. However, this approach is over-
constrained and prevents the comparison of terms such as t = x a and s = x b
despite the fact that for all substitutions θ, ∥tθ∥= ∥sθ∥. Therefore, we present a
second approach wherein no replacement of subterms occurs. This comes at the
expense of a far more complex variable condition. Roughly, the condition stip-
ulates that two terms are comparable if and only if the variables and relevant
combinators are in identical positions in each.
Approach 1. Because the >hb ordering is not deﬁned over typed terms, type
arguments are replaced by equivalent term arguments before comparison. The
translation ([]) from T to untyped terms is given below. First we deﬁne precisely
the subterms that require replacing by variables.

266
A. Bhayat and G. Reger
Deﬁnition 1 (Type-1 term). Consider a term t of the form Cany tn . If there
exists a position p such t|p is a variable, then t is a type-1 term.
Deﬁnition 2 (Type-2 term). A term x tn where n > 0 is a type-2 term.
The translation to untyped terms is deﬁned as follows. If t is a type variable
τ, then ([t]) = τ. If t = κ( σn ), then ([t]) = κ ([ σn ]). If t is a term variable x,
then ([t]) = x. If t is a type-1 or type-2 term, then ([t]) is a fresh variable xt. If
t = f⟨τn ⟩, then ([t]) = f ([ τn ]). Finally, if t = t1 t2, then ([t]) = ([t1])([t2]).
An untyped term t weak reduces to an untyped term t′ in one step if t = u[s]p
and there exists a combinator axiom l = r and substitution σ such that ([l])σ = s
and t′ = u[([r])σ]p. The aim of the ordering presented here is to parametrise the
superposition calculus. For this purpose, the property that for terms t and t′,
t −→w t′ =⇒t ≻t′, is desired. To this end, the following lemma is proved.
Lemma 3. For all term ground polymorphic terms t and t′, it is the case that
t −→w t′ ⇐⇒([t]) −→w ([t′]).
Proof. The =⇒direction can be proved by a straightforward induction on the
t. The opposite direction is proved by an induction on ([t]).
Corollary 1. A straightforward corollary of the above lemma is that for all
term-ground polymorphic terms t, ∥t∥= ∥([t])∥.
The combinator orienting Knuth-Bendix order (approach 1) >ski1 is deﬁned
as follows. For terms t and s, let t′ = ([t]) and s′ = ([s]). Then t >ski1 s if
vars#(s′) ⊆vars#(t′) and:
R1 ∥t′∥> ∥s′∥or,
R2 ∥t′∥= ∥s′∥and t′ >hb s′.
Approach 2. Using approach 1, terms t = y a and s = y b are incomparable.
Both are type-2 terms and therefore ([t]) = xt and ([s]) = xs. The variable con-
dition obviously fails to hold between xt and xs. Therefore, we consider another
approach which does not replace subterms with fresh variables. We introduce
a new translation  from T to untyped terms that merely replaces type argu-
ments with equivalent term arguments and does not aﬀect term arguments at all.
The simpler translation comes at the cost of a more complex variable condition.
Before the revised variable deﬁnition can be provided, some further terminology
requires introduction.
Deﬁnition 3 (Safe Combinator). Let Cany occur in t at position p and let
p′ be the shortest preﬁx of p such that head(t|p′) is a combinator and for all
positions p′′ between p and p′, head(t|p′′) is a combinator. Let p′′ be a preﬁx of
p of length one shorter than p′ if such a position exists and ϵ otherwise. Then
Cany is safe in t if t|p′ is ground and head(t|p′′) /∈V and unsafe otherwise.

Combinatory KBO
267
Intuitively, unsafe combinators are those that could aﬀect a variable on a
longest reduction path or could become applied to a subterm of a substitution.
For example, all combinators in the term S (K I) a x are unsafe because they
aﬀect x, whilst the combinator in f (I b) y is safe. The combinators in x (S I) a are
unsafe because they could potentially interact with a term substituted for x.
Deﬁnition 4. We say a subterm is top-level in a term t if it doesn’t appear
beneath an applied variable or fully applied combinator head in t.
Deﬁnition 5 (Safe). Let t1 and t2 be untyped terms. The predicate safe(t1, t2)
holds if for every position p in t2 such that t2|p = Cany tn and Cany (not necessar-
ily fully applied) is unsafe, then t1|p = Cany sn and for 1 ≤i ≤n, ∥si∥≥∥ti∥.
Further, for all p in pos(t2) such that t2|p = x tn , then t1|p = x sn and for
1 ≤i ≤n, ∥si∥≥∥ti∥.
The deﬁnition of safe ensures that if safe(t, s) and ∥t∥≥∥s∥, then ∥tσ∥≥
∥sσ∥for any substitution σ a result we prove in Lemma 13. Consider terms
t = x (I(I (I a))) b and s = x a (I (I b)). We have that ∥t∥= 3 > ∥s∥= 2. However,
it is not the case that safe(t, s) because the condition that ∥ti∥≥∥si∥for all i
is not met. ∥t2∥= ∥b∥= 0 < 2 = ∥I (I b)∥= ∥s2∥. Now consider the substitution
σ = {x →S c}. Because this substitution duplicates the second argument in s
and t, ∥tσ∥= 4 < ∥sσ∥= 5 showing the importance of the safe predicate in
ensuring stability.
We draw out some obvious consequences of the deﬁnition of safety. Firstly,
the predicate enjoys the subterm property in the following sense. If p is a position
deﬁned in terms t1 and t2, then safe(t1, t2) =⇒safe(t1|p, t2|p). Secondly, the
predicate is transitive; safe(t1, t2) ∧safe(t2, t3) =⇒safe(t1, t3).
There is a useful property that holds for non-ground terms t and s such that
safe(t, s).
Deﬁnition 6 (Semisafe). Let t and s be untyped terms. Let Cany sn be a term
that occurs in s at p such that all head symbols above Cany in s are combinators.
Then semisafe(t, s) if t|p = Cany tn and for 1 ≤i ≤n, ∥ti∥≥∥si∥.
It is clearly the case that (t not ground )∧(s not ground )∧safe(t, s)
=⇒
semisafe(t, s). The implication does not hold in the other direction. A useful
property of semisafe is that it is stable under head reduction. If for terms t and
s that reduce at their heads to t′ and s′ respective, we have semisafe(t, s), then
we have semisafe(t′, s′).
Variable Condition:
Let t′ = t and s′ = s for polymorphic terms t and s. Let A be the multiset
of all top-level, non-ground, ﬁrst-order subterms in s′ of the form x sn (n may
be 0) or Cany tn . Let B be a similarly deﬁned multiset of subterms of t′. Then,
var cond(t′, s′) holds if there exists an injective total function f from A to B
such that f only associates terms t1 and t2 if safe(t1, t2).

268
A. Bhayat and G. Reger
For example var cond(t, s) holds where t = f y (x a) and s = g (x b). In this
case A = {x b} and B = {y, x a}. There exists and injective total function from A
to B that matches the requirements by relating x b to x a. However, the variable
condition does not hold in either direction if t = f y (x a) and s = g (x (I b)). In
this case, x (I b) cannot be related to x a since the condition that ∥a∥≥∥I b∥is
not fulﬁlled.
We now deﬁne the combinator orienting Knuth-Bendix order (approach 2)
>ski. For terms t and s, let t′ = t and s′ = s′. Then t >ski s if var cond(t′, s′)
and:
R1 ∥t′∥> ∥s′∥or,
R2 ∥t′∥= ∥s′∥and t′ >hb s′.
Lemma 4. For all ground instances of combinator axioms l ≈r, we have
l >ski r.
Proof. Since for all ground instances of the axioms l ≈r, we have ∥l∥> ∥r∥, the
theorem follows by an application of R1.
It should be noted that for non-ground instances of an axiom l ≈r, we do
not necessarily have l >ski r since l and r may be incomparable. This is no
problem since the deﬁnition of >ski could easily be amended to have l >ski r by
deﬁnition if l ≈r is an instance of an axiom. Lemma 4 ensures that stability
under substitution would not be aﬀected by such an amendment.
4
Properties
Various properties of the order >ski are proved here. The proofs provided here
lack detail, the full proofs can be found in our report [6]. The proofs can easily be
modiﬁed to hold for the less powerful >ski1 ordering. In general, for an ordering
to parameterise a superposition calculus, it needs to be a simpliﬁcation ordering
[19]. That is, superposition is parameterised by an irreﬂexive, transitive, total
on ground-terms, compatible with contexts, stable under substitution and well-
founded binary relation. Compatibility with contexts can be relaxed at the cost
of extra inferences [3,5,9]. A desirable property to have in our case is coincidence
with ﬁrst-order KBO, since without this, the calculus would not behave on ﬁrst-
order problems as standard ﬁrst-order superposition would.
Theorem 1 (Irreﬂexivity). For all terms s, it is not the case that s >ski s.
Proof. Let s′ = s. It is obvious that ∥s′∥= ∥s′∥. Therefore s >ski s can only
be derived by rule R2. However, this is precluded by the irreﬂexivity of >hb.
Theorem 2 (Transitivity). For terms s, t and u, if s >ski t and t >ski u then
s >ski u.

Combinatory KBO
269
Proof. Let s′
= s, t′
= t and u′
= u. From var cond(s′, t′) and
var cond(t′, u′), var cond(s′, u′) by the deﬁnition of var cond and the appli-
cation of the transitivity of safe. If ∥s′∥> ∥t′∥or ∥t′∥> ∥u′∥then ∥s′∥> ∥u′∥
and s >ski u follows by an application of rule R1. Therefore, suppose that
∥s′∥= ∥t′∥= ∥u′∥. Then it must be the case that s′ >hb t′ and t′ >hb u′.
It follows from the transitivity of >hb that s′ >hb u′ and thus s >ski u.
Theorem 3 (Ground Totality). Let s and t be ground terms that are not
syntactically equal. Then either s >ski t or t >ski s.
Proof. Let s′ = s and t′ = t. If ∥s′∦= ∥t′∥then by R1 either s >ski t or
t >ski s. Otherwise, s′ and t′ are compared using >hb and either t′ >hb s′ or
s′ >hb t′ holds by the ground totality of >hb and the injectivity of .
Theorem 4 (Subterm Property for Ground Terms). If t and s are ground
and t is a proper subterm of s then s >ski t.
Proof. Let s′ = s and t′ = t. Since t is a subterm of s, t′ is a subterm of s′
and ∥s′∥≥∥t′∥because any weak reduction in t′ is also a weak reduction in s′.
If ∥s′∥> ∥t′∥, the theorem follows by an application of R1. Otherwise s′ and
t′ are compared using >hb and s′ >hb t′ holds by the subterm property of >hb.
Thus s >ski t.
Next, a series of lemmas are proved that are utilised in the proof of the
ordering’s compatibility with contexts and stability under substitution. We prove
two monotonicity properties Theorems 5 and 6. Both hold for non-ground terms,
but to show this, it is required to show that the variable condition holds between
terms u[t] and u[s] for t and s such that t >ski s. To avoid this complication, we
prove the Lemmas for ground terms which suﬃces for our purposes. To avoid
clutter, assume that terms mentioned in the statement of Lemmas 5–16 are all
untyped, formed by translating polymorphic terms.
Lemma 5. ∥ζ tn ∥= n
i=1 ∥ti∥if ζ is not a fully applied combinator.
Lemma 6. Let t = ζ tn . Then ∥t∥>n
i=1 ∥ti∥if ζ is a fully applied combinator.
Lemma 7. Let tn be terms such that for each ti, head(ti) /∈{I, K, B, C, S}. Let
t′n be terms with the same property. Moreover, let ∥ti∥≥∥t′
i∥for 1 ≤i ≤n. Let
s = u[ tn ] and s′ = u[ t′n ] where each ti and t′
i is at position pi in s and s′. If
the F∞redex in s is within ti for some i, then the F∞redex in s′ is within t′
i
unless t′
i is in normal form.
Proof. Proof is by induction on |s| + |s′|. If u has a hole at head position,
then s = f rm sm′′ and s′ = g vm′ s′
m′′ where t1 = f rm and t′
1 = g vm′ .
Assume that the F∞redex of s is in t1. Further, assume that ∥t′
1∥> 0.
Then, for some i in {1 . . . m′}, it must be the case that ∥vi∥> 0. Let j
be the smallest index such that ∥vj∥> 0. Then by the deﬁnition of F∞,
F∞(s′) = g v1 . . . vj−1 F∞(vj) vj+1 . . . vm′ s′
m′′ and the F∞redex of s′ is in t′
1.

270
A. Bhayat and G. Reger
Suppose that the F∞redex of s is not in t1. This can only be the case
if ∥t1∥= 0 in which case ∥t′
1∥= 0 as well. In this case, by the deﬁnition of
F∞, F∞(s) = f rm s1 . . . si−1 F∞(si) si+1 . . . s′′
m where ∥sj∥= 0 for 1 ≤j < i.
Without loss of generality, assume that the F∞redex of si occurs inside ti. Then
t′
i must be a subterm of s′
i. Assume that ∥t′
i∥> 0 and thus ∥s′
i∥> 0. Since for all
i, si and s′
i only diﬀer at positions where one contains a tj and the other contains
a t′
j and ∥ti∥≥∥t′
i∥for 1 ≤i ≤m′′, we have that ∥sj∥= 0 implies ∥s′
j∥= 0.
Thus, using the deﬁnition of F∞, F∞(s′) = g vm′ s′
1 . . . s′
i−1 F∞(s′
i) s′
i+1 . . . s′
m′′.
The induction hypothesis can be applied to si and s′
i to conclude that the F∞
redex of s′
i occurs inside t′
i. The lemma follows immediately.
If u does not have a hole at its head, then s = ζ sn and s′ = ζ s′n where ζ is
not a fully applied combinator other than K (if it was, the F∞redex would be
at the head).
If ζ is not a combinator, the proof follows by a similar induction to above.
Therefore, assume that ζ = K. It must be the case that ∥s2∥> 0 otherwise the
F∞redex in s would be at the head and not within a ti. By the deﬁnition of F∞,
F∞(s) = K s1 F∞(s2) s3 . . . sn. Let the F∞redex of s2 occur inside tj. Then t′
j
is a subterm of s′
2. If ∥t′
j∥> 0 then ∥s′
2∥> 0 and F∞(s′) = K s′
1 F∞(s′
2) s′
3 . . . s′
n.
By the induction hypothesis, the F∞redex of s′
2 occurs in t′
j.
Lemma 8. Let tn be terms such that for 1 ≤i ≤n, head(ti) /∈{I, K, B, C, S}.
Then for all contexts u[]n, if u[ tn ] −→w u′ then either:
1. ∃i.u′ = u[t1, . . . , ˆti, . . . , tn] where ti −→w ˆti or
2. u′ = ˆu{x1 →t1, . . . , xn →tn} where u[x1, . . . , xn] −→w ˆu
Proof. Let s = u[ tn ] and let p1, . . . pn be the positions of tn in s. Since s is
reducible, there must exist a p such that s|p is a redex.
If p > pi for some i, there exists a p′ ̸= ϵ such that p = pip′. Then, u[t1, . . . , ti,
. . . , tn]|pi = ti[Cany rn ]p′ −→w ti[(Cany rn ) ↓w]p′. Let ˆti = ti[(Cany rn ) ↓w]p′. We
thus have that ti −→w ˆti and thus u[t1, . . . , ti, . . . , tn] −→w u[t1, . . . , ˆti, . . . , tn].
It cannot be the case that p = pi for any i because head(ti) is not a com-
binator for any ti. In the case where p < pi or p ∥pi for all i, we have that
u[ tn ] = (u[ xn ])σ and u[ xn ]|p is a redex where σ = { xn →tn }. Let ˆu be
formed from u[ xn ] by reducing its redex at p. Then , s = u[ tn ] = (u[ xn ])σ −→w
ˆuσ = ˆu{x1 →t1 . . . xn →tn}
Lemma 9. Let tn be terms such that for each ti, head(ti) /∈{I, K, B, C, S}. Let
t′n be terms with the same property. Then:
1. If ∥ti∥= ∥t′
i∥for all i in {1, . . . , n}, then ∥u[ tn ]∥= ∥u[ t′n ]∥for all n holed
contexts u.
2. If ∥tj∥> ∥t′
j∥for some j ∈{1, . . . , n} and ∥ti∥≥∥t′
i∥for i ̸= j, then
∥u[ tn ]∥> ∥u[ t′n ]∥for all n holed contexts u.
Proof. Let p1, . . . , pn be the positions of the holes in u and let s = u[ tn ] and
s′ = u[ t′n ]. Proof is by induction on ∥s∥+ ∥s′∥. We prove part (1) ﬁrst:

Combinatory KBO
271
Assume that ∥u[ tn ]∥= 0. Then ∥ti∥= 0 for 1 ≤i ≤n. Now assume that
∥u[ t′n ]∦= 0. Then there must exist some position p such that s′|p is a redex.
We have that p ̸= pi for all pi as head(t′
i) /∈{I, K, B, C, S}. Assume p > pi for
some pi. But then, ∥t′
i∥> 0 which contradicts the fact that ∥ti∥= ∥t′
i∥for all
i. Therefore, for all pi either p < pi or p ∥pi. But then, if s′|p is a redex, so
must s|p be, contradicting the fact that ∥u[ tn ]∥= 0. Thus, we conclude that
∥u[ t′n ]∥= 0.
Assume that ∥u[ tn ]∥> 0. Let u′ = F∞(s). By Lemma 8 either u′ =
u[t1, . . . , ˆti, . . . , tn] where ti −→w ˆti for 1 ≤i ≤n or u′ = ˆu{ xn →tn }
where u[ xn ] −→w ˆu. In the ﬁrst case, by Lemma 7 and ∥ti∥= ∥t′
i∥we have
F∞(s′) = u′′ = u[t′
1, . . . , ˆt′
i, . . . , t′
n] where t′
i −→w ˆt′
i. By the induction hypothe-
sis ∥u′∥= ∥u′′∥and thus ∥s∥= ∥s′∥. In the second case, F∞(s′) = u′′ = ˆu{ xn →
t′n } where u[ xn ] −→w ˆu. Again, the induction hypothesis can be used to show
∥u′∥= ∥u′′∥and the theorem follows.
We now prove part (2); ∥u[ tn ]∥must be greater than 0. Again, let u′ = F∞(s)
and u′′ = F∞(s′). If u′ = u[t1, . . . , ˆti, . . . , tn] and ∥t′
i∦= 0, then by Lemma 7
u′′ = u[t′
1, . . . , ˆt′
i, . . . , t′
n] where t′
i −→w ˆt′
i unless ∥t′
i∥= 0 and the lemma follows
by the induction hypothesis.
If ∥t′
i∥= 0, consider terms u′ and s′. If ∥ˆti∥> 0 or ∥tj∥> ∥t′
j∥for some
j ̸= i, then the induction hypothesis can be used to show ∥u′∥> ∥s′∥and
therefore ∥s∥= ∥u′∥+ 1 > ∥s′∥. Otherwise, ∥tj∥= ∥t′
j∥for all j ̸= i and
∥ˆti∥= 0 = ∥t′
i∥. Part 1 of this lemma can be used to show that ∥u′∥= ∥s′∥and
thus ∥s∥= ∥u′∥+ 1 > ∥s′∥. If u′ = ˆu{ xn →tn }, then u′′ = ˆu{ xn →t′n } and
the lemma follows by the induction hypothesis.
Theorem 5 (Compatibility with Contexts).
For ground terms s and t,
such that head(s), head(t) /∈{I, K, B, C, S}, and s >ski t, then u[s] >ski u[t] for
all ground contexts u[].
Proof. Let s′ = s, t′ = t and u′ = u. By Lemma 9 Part 2, we have that
if ∥s′∥> ∥t′∥, then ∥u′[s′]∥> ∥u′[t′]∥. Thus, if s >ski t was derived by R1,
u[s] >ski u[t] follows by R1. Otherwise, s >ski t is derived by R2 and ∥s′∥= ∥t′∥.
By Lemma 9 Part 1, ∥u′[s′]∥= ∥u′[t′]∥follows. Thus, u′[s′] is compared with
u′[t′] by R2 and u[s] >ski u[t] by the compatibility with contexts of >hb.
Lemma 10. ∥s∥> ∥t∥=⇒∥u⟨⟨s⟩⟩∥> ∥u⟨⟨t⟩⟩∥and ∥s∥= ∥t∥=⇒∥u⟨⟨s⟩⟩∥=
∥u⟨⟨t⟩⟩∥.
Proof. Proceed by induction on the size of the context u. If u is the empty
context, both parts of the theorem hold trivially.
The inductive case is proved for the ﬁrst implication of the lemma ﬁrst. If u is
not the empty context, u⟨⟨s⟩⟩is of the form u′⟨⟨ζ t1 . . . ti−1, s, ti+1 . . . tn⟩⟩. By the
deﬁnition of a stable subterm ζ cannot be a fully applied combinator and thus
by Lemma 5 we have that ∥ζ t1 . . . ti−1, s, ti+1 . . . tn∥= n
j=1∧j̸=i ∥tj∥+ ∥s∥>
n
j=1∧j̸=i ∥tj∥+ ∥t∥= ∥ζ t1 . . . ti−1, t, ti+1 . . . tn∥. If ζ is not a combinator, then
∥u′⟨⟨ζ t1 . . . ti−1, s, ti+1 . . . tn⟩⟩∥> ∥u′⟨⟨ζ t1 . . . ti−1, t, ti+1 . . . tn⟩⟩∥follows from

272
A. Bhayat and G. Reger
Lemma 9 Part 2. Otherwise, ζ is a partially applied combinator and u′ is a smaller
stable context than u. The induction hypothesis can be used to conclude that
∥u′⟨⟨ζ t1 . . . ti−1, s, ti+1 . . . tn⟩⟩∥> ∥u′⟨⟨ζ t1 . . . ti−1, t, ti+1 . . . tn⟩⟩∥and thus that
∥u⟨⟨s⟩⟩∥> ∥u⟨⟨t⟩⟩∥. The proof of the inductive case for the second implication of
the lemma is almost identical.
Theorem 6 (Compatibility with Stable Contexts). For all stable ground
contexts u⟨⟨⟩⟩and ground terms s and t, if s >ski t then u⟨⟨s⟩⟩>ski u⟨⟨t⟩⟩.
Proof. If ∥s∥> ∥t∥then by Lemma 10, ∥u⟨⟨s⟩⟩∥> ∥u⟨⟨t⟩⟩∥holds and then by
an application of R1 we have u⟨⟨s⟩⟩>ski u⟨⟨t⟩⟩. Otherwise, if ∥s∥= ∥t∥, then by
Lemma 10 we have that ∥u⟨⟨s⟩⟩∥= ∥u⟨⟨t⟩⟩∥. Thus u⟨⟨s⟩⟩and u⟨⟨t⟩⟩are compared
using >hb. By the compatibility with contexts of >hb, u⟨⟨s⟩⟩ >hb u⟨⟨t⟩⟩ holds
and then by ofan application of R2 u⟨⟨s⟩⟩>ski u⟨⟨t⟩⟩is true.
We next prove stability under substitution. In order to prove this, it needs
to be shown that for untyped terms s and t and all substitutions σ:
1. var cond(s, t) implies var cond(sσ, tσ).
2. var cond(s, t) and ∥s∥≥∥t∥imply ∥sσ∥≥∥tσ∥
The ﬁrst is proved in Lemma 15. A slightly generalised version of (2) is proved
in Lemma 14. Lemmas 11–13 are helper lemmas used in the proof of the above
two properties.
Lemma 11. For a single hole context u⟨⟨⟩⟩such that the hole does not occur
below a fully applied combinator and any term t, ∥u⟨⟨t⟩⟩∥= ∥u⟨⟨⟩⟩∥+ ∥t∥.
Proof. Proof to be found in report.
Lemma 12. Let tn and sn be terms such that for n1 . . . nn ∈N and for 1 ≤
i ≤n, ∥ti∥≥∥si∥+ ni. Further, let t = t1 t2 . . . tn and s = s1 s2 . . . sn. Assume
that semisafe(t, s) holds. Then ∥t∥≥∥s∥+ n
i=1 ni.
Proof. Proof to be found in report.
Lemma 13. Let t and s be non-ground terms such that ∥t∥≥∥s∥+ m for
some m ∈N and safe(t, s). Then, for any substitution σ, ∥tσ∥≥∥sσ∥+ m and
safe(tσ, sσ).
Proof. Proof to be found in report.
Lemma 14. For terms t and s such that var cond(t, s) holds and ∥t∥≥∥s∥+n
for some n ∈N, for all substitutions σ, ∥tσ∥≥∥sσ∥+ n.
Proof. If s and t are ground, the theorem is trivial. If s is ground, then ∥tσ∥≥
∥t∥≥∥s∥+n. If s is not ground, then var cond(t, s) implies that t is not ground.
Therefore, assume that neither is ground. If head(s) (and therefore head(t) by the
variable condition) are fully applied combinators or variables, then var cond(t, s)

Combinatory KBO
273
implies safe(t, s) and Lemma 13 can be invoked to prove the lemma. Therefore,
assume that both have non-variable, non-fully applied combinator heads.
Let t = u⟨⟨tm ⟩⟩and s = u′⟨⟨sm ⟩⟩where sm are all the non-ground, top-
level, ﬁrst-order subterms of the form x args or Cany args in s. By the variable
condition, we have that there exists a total injective function respecting the
given conditions from the si to non-ground, top-level, ﬁrst-order subterms of t
of the form x args or Cany args . Let tm be the terms related to sm by this
function. Without loss of generality, assume that this function relates s1 to t1,
s2 to t2 and so on. For 1 ≤i ≤m, ∥ti∥= ∥si∥+ mi for mi ∈N. This follows
from the fact that since ti and si are both non-ground and safe(ti, si), we have
semisafe(ti, si) and can therefore invoke Lemma 12.
Let m′ = ∥u⟨⟨⟩⟩∥−∥u′⟨⟨⟩⟩∥. Note that m′ could be negative. By Lemma 11,
∥t∥= ∥u⟨⟨⟩⟩∥+m
i=1 ∥ti∥and ∥s∥= ∥u′⟨⟨⟩⟩∥+m
i=1 ∥si∥. Thus, ∥t∥= ∥s∥+m′ +
m
i=1 mi. Therefore, m′ + m
i=1 mi ≥n. Lemma 13 can be used to show that for
all i, ∥tiσ∥≥∥siσ∥+mi. Because u′⟨⟨⟩⟩is ground, it follows ∥uσ⟨⟨⟩⟩∥−∥u′σ⟨⟨⟩⟩∥≥
m′. To conclude the proof:
∥tσ∥
=
∥uσ⟨⟨tmσ ⟩⟩∥
=
∥uσ⟨⟨⟩⟩∥+ m
i=1 ∥tiσ∥
≥
∥u′σ⟨⟨⟩⟩∥+ m
i=1 ∥siσ∥+ m′ + m
i=1 mi
≥
∥u′σ⟨⟨⟩⟩∥+ m
i=1 ∥siσ∥+ n
=
∥sσ∥+ n
Lemma 15. For terms t and s such that var cond(t, s) holds and for all sub-
stitutions σ, var cond(tσ, sσ).
Proof. Let t = u⟨⟨tm ⟩⟩and s = u′⟨⟨sm ⟩⟩where sm are all the non-ground, top-
level, ﬁrst-order subterms of the form x args or Cany args in s. By the variable
condition, we have that there exists a total injective function respecting the given
conditions from the si to non-ground, top-level, ﬁrst-order subterms of t of the
form x args or Cany args . Let tm be the terms related to sm by this function.
Without loss of generality, assume that this function relates s1 to t1, s2 to t2
and so on. By the deﬁnition of the variable condition, we have that u′ must be
ground. This implies that any non-ground subterms of sσ must be subterms of
some siσ for 1 ≤i ≤m.
Assume that for some i and p ∈pos(siσ), siσ|p is a non-ground, top-level,
ﬁrst-order subterm of the form x args or Cany args . We show that tiσ|p is a
non-ground, top-level, ﬁrst-order subterm of tσ and safe(ti|p, si|p). This implies
the existence of a total, injective function from the multiset of non-ground,
top-level ﬁrst-order subterms in sσ to the like multiset of tσ in turn proving
var cond(tσ, sσ).
From Lemma 13, it can be shown that for 1 ≤i ≤m, safe(tiσ, siσ). By the
subterm property of safety, this implies that safe(tiσ|p, siσ|p).
To show that tiσ|p must be a non-ground, top-level, ﬁrst-order subterm in
tσ, it can be assumed that this not the case. This easily leads to a contradiction
with safe(tiσ, siσ).

274
A. Bhayat and G. Reger
Lemma 16. Let t be a polymorphic term and σ be a substitution. We deﬁne a
new substitution ρ such that the domain of ρ is dom(σ). Deﬁne yρ = yσ. For
all terms t, tσ = tρ.
Proof. Via a straightforward induction on t.
Theorem 7 (Stability under Substitution). If s >ski t then sσ >ski tσ for
all substitutions σ that respect the ghd mapping.
Proof. Let s′ = s and t′ = t. Let ρ be deﬁned as per Lemma 16. First, we show
that if R1 was used to derive s >ski t and thus ∥s′∥> ∥t′∥then ∥s′ρ∥> ∥t′ρ∥
and thus sσ >ski tσ because sσ = s′ρ and tσ = t′ρ.
From Lemma 15 and var cond(s′, t′), var cond(s′ρ, t′ρ) holds. Furthermore,
if ∥s′∥> ∥t′∥, then by Lemma 14 ∥s′ρ∥> ∥t′ρ∥and sσ >ski tσ by an application
of R1.
On the other hand, if ∥s′∥= ∥t′∥, then R2 was used to derive s >ski t. By
Lemma 14 ∥s′ρ∥≥∥t′ρ∥. If ∥s′ρ∥> ∥t′ρ∥, then this is the same as the former
case. Otherwise ∥s′ρ∥= ∥t′ρ∥and s′ρ and tρ are compared using R2. From the
stability under substitution of >hb, s′ρ >hb t′ρ follows and sσ >ski tσ can be
concluded.
Theorem 8 (Well-foundedness). There exists no inﬁnite descending chain
of comparisons s1 >ski s2 >ski s3 · · · .
Proof. Assume that such a chain exists. For each si >ski si+1 derived by R1,
we have that ∥si∥> ∥si+1∥. For each si >ski si+1 derived by R2, we have
that ∥si∥= ∥si+1∥. Therefore the number of times si >ski si+1 by R1 in the
inﬁnite chain must be ﬁnite and there must exist some m such that for all
n > m, sn >ski sn+1 by R2. Therefore, there exists an inﬁnite sequence of
>hb comparisons sm >hb sm+1 >hb sm+2 · · · . This contradicts the well-
foundedness of >hb.
Theorem 9 (Coincidence with First-Order KBO). Let >fo be the ﬁrst-
order KBO as described by Becker et al. in [2]. Assume that >ski and >fo are
parameterised by the same precedence ≻and that >fo always compares tuples
using the lexicographic extension operator. Then >ski and >fo always agree on
ﬁrst-order terms.
Proof. Let t′ = t and s′ = s. Since s and t are ﬁrst-order, ∥s′∥= 0 and
∥t′∥= 0. Thus, s′ and t′ will always be compared by >hb. Since >hb coincides
with >fo on ﬁrst-order terms, so does >ski.
5
Examples
To give a ﬂavour of how the ordering behaves, we provide a number of examples.
Example 1. Consider the terms (ignoring type arguments) t = S (K a) b c and
s = f c e. From the deﬁnition of the translation , we have that t = S (K a) b c
and s = f c e. Since ∥S (K a) b c∥= 2 and ∥f c e∥= 0, we have that t >ski s.

Combinatory KBO
275
Example 2. Consider the terms t = f (g b) e d and s = I a. Here s >ski t despite
the fact that s is syntactically smaller than t because s has a maximum reduction
of 1 as opposed to 0 of t.
Example 3. Consider terms t = f (I d) (S x a b) and s = g (S x (h d) b). The two
terms are comparable as the variable condition relates subterm S x (h d) b in s
to subterm S x a b in t. The unsafe combinator S and variable x are in the same
position in each subterm. As ∥t∥> ∥s∥, t >ski s.
Example 4. Consider terms t = f (I d) (S x a y) and s = g (S x (h y) b). This is
very similar to the previous example, but in this case the terms are incompa-
rable. Let s′ be a name for the subterm (S x (h y) b) in s and t′ a name for the
subterm (S x a y). The variable y occurs in diﬀerent positions in s′ and t′. There-
fore, s′ cannot be related to t by the variable condition and the two terms are
incomparable.
Example 5. Consider terms t = f (x (g (K I a b))) and s = h (I a) (x c). The vari-
able condition holds between t and s by relating (x (g (K I a b))) to (x c). The
combinator I in s is not unsafe and therefore does not need to be related to a
combinator in t.
Since ∥t∥= 2 > ∥s∥= 1, t >ski s. Intuitively, this is safe because a substitu-
tion for x in t can duplicate (g (K I a b)) whose maximum reduction length is 2
whilst a substitution for x in s can only duplicate c whose maximum reduction
length is 0.
6
Conclusion and Discussion
We have presented an ordering that orients all ground instances of S, C, B,
K and I axioms left-to-right. The ordering enjoys many other useful properties
such as stability under substitution, compatibility with stable contexts, ground
totality and transitivity. In as yet unpublished work, we have used this order-
ing to parameterise a complete superposition calculus for HOL [5]. Lack of full
compatibility with context has not been an obstacle. In the standard ﬁrst-order
proof of the completeness of superposition, compatibility with contexts is used
in model construction to rule out the need for superposition inferences beneath
variables [19]. Thus, by utilising >ski, some superposition is required beneath
variables. However, because terms with functional heads are compatible with all
contexts, such inference are quite restricted.
The >ski ordering presented here is able to compare non-ground terms that
cannot be compared by any ordering used to parameterise Bentkamp et al.’s
lambda superposition calculus [3]. They deﬁne terms to be β-equivalence classes.
Non-ground terms are compared using a quasiorder, ≿, such that t ≿s iﬀfor
all grounding substitutions θ, tθ ⪰sθ. Consider terms t = x a b and s = x b a
and grounding substitutions θ1 = x →λx y .f y x and θ2 = x →λx y .f x y. By
ground totality of ≻it must be the case that either f a b ≻f b a or f b a ≻f a b.
Without loss of generality assume the ﬁrst. Then, neither t ≿s nor s ≿t since
tθ1 = f b a ≺f a b = sθ1 and tθ2 = f a b ≻f b a = sθ2.

276
A. Bhayat and G. Reger
The >ski ordering allows weak reduction (or β-reduction) to be treated as
part of the superposition calculus. This allows terms t and t′ such that t −→+
w t′
(or t −→+
β t′) to be considered separate terms resulting in terms such as t and
s given above being comparable. Since ∥t∥= ∥s∥, t and s are compared using
>hb with stability under substitution ensured by the stability under substitution
of >hb.
Many of the deﬁnitions that have been provided here are conservative and
can be tightened to allow the comparison of a far larger class of non-ground terms
without losing stability under substitution. We provide an example of how the
deﬁnition of stable subterm could be reﬁned in our report [6]. In further work,
we hope to thoroughly explore such reﬁnements.
Acknowledgements. Thanks to Jasmin Blanchette, Alexander Bentkamp and Petar
Vukmirovi´c for many discussions on aspects of this research. We would also like to
thank reviewers of this paper, whose comments have done much to shape this paper.
The ﬁrst author thanks the family of James Elson for funding his research.
References
1. Barendregt, H.P.: The Lambda Calculus: Its Syntax and Semantics, 2nd edn. Else-
vier Science Publishers B.V., Amsterdam (1984)
2. Becker, H., Blanchette, J.C., Waldmann, U., Wand, D.: A transﬁnite Knuth–
Bendix order for lambda-free higher-order terms. In: de Moura, L. (ed.) CADE
2017. LNCS (LNAI), vol. 10395, pp. 432–453. Springer, Cham (2017). https://doi.
org/10.1007/978-3-319-63046-5 27
3. Bentkamp, A., Blanchette, J., Tourret, S., Vukmirovi´c, P., Waldmann, U.: Superpo-
sition with lambdas. In: Fontaine, P. (ed.) CADE 2019. LNCS (LNAI), vol. 11716,
pp. 55–73. Springer, Cham (2019). https://doi.org/10.1007/978-3-030-29436-6 4
4. Benzm¨uller, C., Sultana, N., Paulson, L.C., Theiß, F.: The higher-order prover Leo-
II. J. Autom. Reasoning 55(4), 389–404 (2015). https://doi.org/10.1007/s10817-
015-9348-y
5. Bhayat, A., Reger, G.: A combinator-based superposition calculus for higher-
order logic. In: The 10th International Joint Conference on Automated Reasoning
(IJCAR) (2020)
6. Bhayat, A., Reger, G.: A Knuth-Bendix-like ordering for orienting combinator
equations (technical report). Technical report, University of Mancester (2020).
https://easychair.org/publications/preprint open/rXSk
7. Blanchette, J.C., Waldmann, U., Wand, D.: A lambda-free higher-order recur-
sive path order. In: Esparza, J., Murawski, A.S. (eds.) FoSSaCS 2017. LNCS, vol.
10203, pp. 461–479. Springer, Heidelberg (2017). https://doi.org/10.1007/978-3-
662-54458-7 27
8. Blanqui, F., Jouannaud, J.-P., Rubio, A.: The computability path ordering: the end
of a quest. In: Kaminski, M., Martini, S. (eds.) CSL 2008. LNCS, vol. 5213, pp. 1–14.
Springer, Heidelberg (2008). https://doi.org/10.1007/978-3-540-87531-4 1
9. Boﬁll, M., Godoy, G., Nieuwenhuis, R., Rubio, A.: Paramodulation with non-
monotonic orderings. In: Proceedings - Symposium on Logic in Computer Science,
August 1999

Combinatory KBO
277
10. Brown, C.E.: Satallax: an automatic higher-order prover. In: Gramlich, B., Miller,
D., Sattler, U. (eds.) IJCAR 2012. LNCS (LNAI), vol. 7364, pp. 111–117. Springer,
Heidelberg (2012). https://doi.org/10.1007/978-3-642-31365-3 11
11. Czajka, L., Kaliszyk, C.: Hammer for Coq: automation for dependent type theory.
J. Autom. Reasoning 61(1), 423–453 (2018)
12. Graf, P.: Substitution tree indexing. In: Hsiang, J. (ed.) RTA 1995. LNCS, vol.
914, pp. 117–131. Springer, Heidelberg (1995). https://doi.org/10.1007/3-540-
59200-8 52
13. Hindley, J.R., Seldin, J.P.: Lambda-Calculus and Combinators: An Introduction,
2nd edn. Cambridge University Press, New York (2008)
14. Jouannaud, J.P., Rubio, A.: Polymorphic higher-order recursive path orderings. J.
ACM 54(1) (2007). https://doi.org/10.1145/1206035.1206037
15. Kerber, M.: How to prove higher order theorems in ﬁrst order logic. In: IJCAI, pp.
137–142, January 1991
16. Kop, C., van Raamsdonk, F.: A higher-order iterative path ordering. In: Cervesato,
I., Veith, H., Voronkov, A. (eds.) LPAR 2008. LNCS (LNAI), vol. 5330, pp. 697–
711. Springer, Heidelberg (2008). https://doi.org/10.1007/978-3-540-89439-1 48
17. Lindblad, F.: https://github.com/frelindb/agsyHOL. Accessed 25 Sept 2019
18. Meng, J., Paulson, L.C.: Translating higher-order clauses to ﬁrst-order clauses.
J. Autom. Reasoning 40(1), 35–60 (2008). https://doi.org/10.1007/s10817-007-
9085-y
19. Nieuwenhuis, R., Rubio, A.: Paramodulation-based theorem proving. In: Handbook
of Automated Reasoning, vol. 1, pp. 371–443. Elsevier Press and MIT press, August
2001. https://doi.org/10.1016/B978-044450813-3/50009-6
20. Sekar, R., Ramakrishnan, I., Voronkov, A.: Term indexing, chap. 26. In: Robinson,
A., Voronkov, A. (eds.) Handbook of Automated Reasoning, vol. II, pp. 1853–1964.
Elsevier Science (2001)
21. Steen, A.: Extensional paramodulation for higher-order logic and its eﬀective imple-
mentation Leo-III. Ph.D. thesis, Freie Universit¨at Berlin (2018)
22. Sultana, N., Blanchette, J.C., Paulson, L.C.: Leo-II and Satallax on the Sledge-
hammer test bench. J. Appl. Logic 11(1), 91–102 (2013). https://doi.org/10.1016/
j.jal.2012.12.002
23. van Raamsdonk, F., Severi, P., Sørensen, M., Xi, H.: Perpetual reductions in
lambda calculus. Inf. Comput. 149(2), 173–225 (1999). https://doi.org/10.1006/
inco.1998.2750

A Combinator-Based Superposition
Calculus for Higher-Order Logic
Ahmed Bhayat(B)
and Giles Reger(B)
University of Manchester, Manchester, UK
ahmed bhayat@hotmail.com, giles.reger@manchester.ac.uk
Abstract. We present a refutationally complete superposition calculus
for a version of higher-order logic based on the combinatory calculus.
We also introduce a novel method of dealing with extensionality. The
calculus was implemented in the Vampire theorem prover and we test
its performance against other leading higher-order provers. The results
suggest that the method is competitive.
1
Introduction
First-order superposition provers are often used to reason about problems in
extensional higher-order logic (HOL) [19,26]. Commonly, this is achieved by
translating the higher-order problem to ﬁrst-order logic (FOL) using combina-
tors. Such a strategy is sub-optimal as translations generally sacriﬁce complete-
ness and at times even soundness. In this paper, we provide a modiﬁcation of
ﬁrst-order superposition that is sound and complete for a combinatory version
of HOL. Moreover, it is graceful in the sense of that it coincides with standard
superposition on purely ﬁrst-order problems.
The work is complementary to the clausal λ-superposition calculus of Ben-
tkamp et al. [4]. Our approach appears to oﬀer two clear diﬀerences. Firstly, as
our calculus is based on the combinatory logic and ﬁrst-order uniﬁcation, it is
far closer to standard ﬁrst-order superposition. Therefore, it should be easier to
implement in state-of-the-art ﬁrst-order provers. Secondly, the >ski ordering that
we propose to parameterise our calculus with can compare more terms than can
be compared by the ordering presented in [4]. On the other hand, we suspect that
for problems requiring complex uniﬁers, our approach will not be competitive
with clausal λ-superposition.
Developing a complete and eﬃcient superposition calculus for a combinatory
version of HOL poses some diﬃculties. When working with a monomorphic logic
it is impossible to select a ﬁnite set of typed combinator axioms that can guaran-
tee completeness for a particular problem [12]. Secondly, using existing orderings,
combinator axioms can superpose among themselves, leading to a huge number
of consequences of the axioms. If the problem is ﬁrst-order, these consequences
can never interact with non-combinator clauses and are therefore useless.
We deal with both issues in the current work. To circumvent the ﬁrst issue, we
base our calculus on a polymorphic rather than monomorphic ﬁrst-order logic.
c
⃝Springer Nature Switzerland AG 2020
N. Peltier and V. Sofronie-Stokkermans (Eds.): IJCAR 2020, LNAI 12166, pp. 278–296, 2020.
https://doi.org/10.1007/978-3-030-51074-9_16

A Combinator-Based Superposition Calculus for Higher-Order Logic
279
The second issue can be dealt with by an ordering that orients combinator axioms
left-to-right. Consider the S-combinator axiom S x y z ≈x z (y z). Assume that
there exists a simpliﬁcation ordering ≻such that S x y z ≻x z (y z). Then, since
superposition is only carried out on the larger side of literals and not at variables,
there can be no inferences between the S-axiom and any other combinator axiom.
Indeed, in this case the axioms can be removed from the clause set altogether
and replaced by an inference rule (Sect. 7).
No ground-total simpliﬁcation ordering is known that is capable of orienting
all axioms for a complete set of combinators.1 The authors suspect that no such
simpliﬁcation ordering exists. Consider a KBO-like ordering. Since the variable
x appears twice on the right-hand side of the S-axiom and only once on the
left-hand side, the ordering would not be able to orient it. The same is the case
for any other combinator which duplicates its arguments.
In other related work [10], we have developed an ordering that enjoys most
of the properties of a simpliﬁcation ordering, but lacks full compatibility with
contexts. In particular, the ordering is not compatible with what we call unstable
contexts. We propose using such an ordering to parameterise the superposition
calculus. In the standard proof of the completeness of superposition, compati-
bility with contexts is used to rule out the need for superposition at or beneath
variables. As the ordering doesn’t enjoy full compatibility with contexts, limited
superposition at and below variables needs to be carried out. This is dealt with
by the addition of an extra inference rule to the standard rules of superposition,
which we call SubVarSup (Sect. 3).
By turning combinator axioms into rewrite rules, the calculus represents a
folding of higher-order uniﬁcation into the superposition calculus itself. Whilst
not as goal-directed as a dedicated higher-order uniﬁcation algorithm, it is still
far more goal-directed than using SK-style combinators in superposition provers
along with standard orderings. Consider the conjecture ∃z. ∀xy. z x y ≈f y x.
Bentkamp et al. ran an experiment and found that the E prover [25] running
on this conjecture supplemented with the S- and K-combinator axioms had to
perform 3756 inferences in order to ﬁnd a refutation [4]. Our calculus reduces
this number to 427 inferences. With the addition of rewrite rules for C-, B- and
I-combinators, the required inferences reduces to 18.
We consider likely that for problems requiring ‘simple’ uniﬁers, folding uniﬁ-
cation into superposition will be competitive with higher-order uniﬁcation whilst
providing the advantages that data structures and algorithms developed for ﬁrst-
order superposition can be re-used unchanged. The results of the empirical eval-
uation of our method can be found in Sect. 8.
2
The Logic
The logic we use is polymorphic applicative ﬁrst-order logic otherwise known as
λ-free (clausal) higher-order logic.
1 A complete set of combinators is a set of combinators whose members can be com-
posed to form a term extensionally equivalent to any given λ-term.

280
A. Bhayat and G. Reger
Syntax. Let Vty be a set of type variables and Σty be a set of type constructors
with ﬁxed arities. It is assumed that a binary type constructor →is present in
Σty which is written inﬁx. The set of types is deﬁned:
Polymorphic Types
τ:: = κ( τn ) | α | τ →τ
where α ∈Vty and κ ∈Σty
The notation tn is used to denote a tuple or list of types or terms depending
on the context. A type declaration is of the form Π αn . σ where σ is a type and
all type variables in σ appear in α . Let Σ be a set of typed function symbols
and V a set of variables with associated types. It is assumed that Σ contains
the following function symbols, known as basic combinators:
S : Πατγ. (α →τ →γ) →(α →τ) →α →γ
I : Πα. α →α
C : Πατγ. (α →τ →γ) →τ →α →γ
K : Παγ. α →γ →α
B : Πατγ. (α →γ) →(τ →α) →τ →γ
The intended semantics of the combinators is captured by the following com-
binator axioms:
S x y z
=
x z (y z)
I x
=
x
C x y z
=
x z y
K x y
=
x
B x y z
=
x (y z)
The set of terms over Σ and V is deﬁned below. In what follows, type sub-
scripts are generally omitted.
Terms
T :: = x | f⟨τn ⟩| tτ ′→τt′
τ ′
where f : Π αn . σ ∈Σ, x ∈V and t, t′ ∈T
The type of the term f⟨τn ⟩is σ{ αn →τn }. Terms of the form t1 t2 are called
applications. Non-application terms are called heads. A term can uniquely be
decomposed into a head and n arguments. Let t = ζ t′n . Then head(t) = ζ where
ζ could be a variable or constant applied to possibly zero type arguments. The
symbol Cany denotes an arbitrary combinator, whilst C3 denotes a member of
{S, C, B}. The S-, C- or B-combinators are fully applied if they have 3 or more
arguments. The K-combinator is fully applied if it has 2 or more arguments
and the I is fully applied if it has any arguments. The symbols Cany and C3
are only used if the symbols they represent are fully applied. Thus, in C3 tn ,
n ≥3 is assumed. The symbols x, y, z . . . are reserved for variables, c, d, f . . . for
non-combinator constants and ζ, ξ range over arbitrary function symbols and
variables and, by an abuse of notation, at times even terms. A head symbol that
is not a combinator applied to type arguments or a variable is called ﬁrst-order.
Positions over Terms: For a term t, if t ∈V or t = f⟨τ ⟩, then pos(t) = {ϵ}
(type arguments have no position). If t = t1 t2 then pos(t) = {ϵ} ∪{i.p | 1 ≤i ≤

A Combinator-Based Superposition Calculus for Higher-Order Logic
281
2, p ∈pos(ti)}. Subterms at positions of the form p.1 are called preﬁx subterms.
We deﬁne ﬁrst-order subterms inductively as follows. For any term t, t is a ﬁrst-
order subterm of itself. If t = ζ tn , where ζ is not a fully applied combinator,
then the ﬁrst-order subterms of each ti are also ﬁrst-order subterms of t. The
notation s⟨u⟩is to be read as u is a ﬁrst-order subterm of s. Note that this
deﬁnition is subtly diﬀerent to that in [10] since subterms underneath a fully
applied combinator are not considered to be ﬁrst-order.
Stable Subterms: Let LPP(t, p) be a partial function that takes a term t, a
position p and returns the longest proper preﬁx p′ of p such that head(t|p′) is not
a partially applied combinator if such a position exists. For a position p ∈pos(t),
p is a stable position in t if p is not a preﬁx position and either LPP(t, p) is not
deﬁned or head(t|LPP(t,p)) is not a variable or combinator. A stable subterm is
a subterm occurring at a stable position. For example, the subterm a is not
stable in f (S a b c), S (S a) b c (in both cases, head(t|LPP(t,p)) = S) and a c (a is
in a preﬁx position), but is in g a b and f (S a) b. A subterm that is not stable is
known as an unstable subterm.
The notation t[u] denotes an arbitrary subterm u of t. The notation
t[u1, . . . , un]n, at times given as t[ u ]n denotes that the the term t contains
n non-overlapping subterms u1 to un. By u[]n, we refer to a context with n
non-overlapping holes.
Weak Reduction: A term t weak-reduces to a term t′ in one step (denoted
t −→w t′) if t = u[s]p and there exists a combinator axiom l = r and substitution
σ such that lσ = s and t′ = u[rσ]p. The term lσ in t is called a weak redex or
just redex. By −→∗
w, the reﬂexive transitive closure of −→w is denoted. Weak-
reduction is terminating and conﬂuent as proved in [15]. By (t) ↓w, we denote
the term formed from t by contracting its leftmost redex.
Literals and Clauses: An equation s ≈t is an unordered pair of terms and
a literal is an equation or the negation of an equation represented s ̸≈t. Let
ax = l ≈r be a combinator axiom and xn be a tuple of variables not appearing
in ax. Then ax and l xn ≈r xn for all n are known as extended combinator
axioms. For example, I x1 x2 ≈x1 x2 is an extended combinator axiom. A clause
is a multiset of literals represented L1 ∨· · · ∨Ln where each Li is a literal.
Semantics. We follow Bentkamp et al. [6] closely in specifying the semantics.
An interpretation is a triple (U, E, J ) where U is a ground-type indexed family
of non-empty sets called universes and E is a family of functions Eτ,v : Uτ→v →
(Uτ →Uv). A type valuation ξ is a substitution that maps type variables to
ground types and whose domain is the set of all type variables. A type valuation
ξ is extended to a valuation by setting ξ(xτ) to be a member of U(τξ). An
interpretation function J maps a function symbol f : Π αn . σ and a tuple of
ground types τn to a member of U(σ{αi→τi}). An interpretation is extensional if
Eτ,v is injective for all τ, v and is standard if Eτ,v is bijective for all τ, v.
For an interpretation I = (U, E, J ) and a valuation ξ, a term is denoted
as follows: xξ
I = ξ(x), f⟨τ ⟩ξ
I = J (f,  τ ξ) and stξ
I = E(sξ
I)(tξ
I). An

282
A. Bhayat and G. Reger
equation s ≈t is true in an interpretation I with valuation function ξ if sξ
I
and tξ
I are the same object and is false otherwise. A disequation s ̸≈t is true
if s ≈t is false. A clause is true if one of its literals is true and a clause set is
true if every clause in the set is true. An interpretation I models a clause set N,
written I |= N, if N is true in I for all valuation functions ξ.
As Bentkamp et al. point out in [4] there is a subtlety relating to higher-order
models and choice. If, as is the case here, attention is not restricted to models that
satisfy the axiom of choice, naive skolemisation is unsound. One solution would
be to implement skolemisation with mandatory arguments as explained in [21].
However, the introduction of mandatory arguments considerably complicates
both the calculus and the implementation. Therefore, we resort to the same
‘trick’ as Bentkamp et al., namely, claiming completeness for our calculus with
respect to models as described above. This holds since we assume problems to be
clausiﬁed. Soundness is claimed for the implementation with respect to models
that satisfy the axiom of choice and completeness can be claimed if the axiom
of choice is added to the clause set.
3
The Calculus
The calculus is modeled after Bentkamp et al.’s intensional non-purifying cal-
culus [6]. The extensionality axiom can be added if extensionality is required.
The main diﬀerence between our calculus and that of [6] is that superposition
inferences are not allowed beneath fully applied combinators and an extra infer-
ence rule is added to deal with superposition beneath variables. We name the
calculus clausal combinatory-superposition.
Term Ordering. We also demand that clausal combinatory-superposition is
parameterised by a partial ordering ≻that is well-founded, total on ground
terms, stable under substitutions and has the subterm property and which orients
all instances of combinator axioms left-to-right. It is an open problem whether a
simpliﬁcation ordering enjoying this last property exists, but it appears unlikely.
However, for completeness, compatibility with stable contexts suﬃces. The >ski
ordering introduced in [10] orients all instances of combinator axioms left-to-
right and is compatible with stable contexts. It is not compatible with arbitrary
contexts. For terms t1 and t2 such that t1 >ski t2, it is not necessarily the case
that t1 u >ski t2 u or that S t1 a b >ski S t2 a b. We show that by not superposing
underneath fully applied combinators and carrying out some restricted superpo-
sition beneath variables, this lack of compatibility with arbitrary contexts can be
circumvented and does not lead to a loss of completeness. In a number of places
in the completeness proof, we assume the following conditions on the ordering
(satisﬁed by the >ski ordering). It may be possible to relax the conditions at the
expense of an increased number of inferences.
P1 For terms t, t′ such that t −→w t′, then t ≻t′
P2 For terms t, t′ such that t ≻t′ and head(t′) is ﬁrst-order, u[t] ≻u[t′]

A Combinator-Based Superposition Calculus for Higher-Order Logic
283
The ordering ≻is extended to literals and clauses using the multiset extension
as explained in [22].
Inference Rules. Clausal combinatory-superposition is further parameterised
by a selection function that maps a clause to a subset of its negative literals. Due
to the requirements of the completeness proof, if a term t = x sn>0 is a maximal
term in a clause C, then a literal containing x as a ﬁrst-order subterm may not
be selected. A literal L is σ-eligible in a clause C if it is selected or there are no
selected literals in C and Lσ is maximal in Cσ. If σ is the identity substitution
it is left implicit. In the latter case, it is strictly eligible if it is strictly maximal.
A variable x has a bad occurrence in a clause C if it occurs in C at an unstable
position. Occurrences of x in C at stable positions are good.
Conventions: Often a clause is written with a single distinguished literal
such as C′ ∨t ≈t′. In this case:
1. The distinguished literal is always σ-eligible for some σ.
2. The name of the clause is assumed to be the name of the remainder
without the dash.
3. If the clause is involved in an inference, the distinguished literal is the
literal that takes part.
Positive and negative superposition:
D′ ∨t ≈t′
C′ ∨[¬]s⟨u⟩≈s′
Sup
(C′ ∨D′ ∨[¬]s⟨t′⟩≈s′)σ
with the following side conditions:
1. The
variable
condition
(below)
holds
2. C is not an extended combinator
axiom;
3. σ = mgu(t, u);
4. tσ ̸⪯t′σ;
5. s⟨u⟩σ ̸⪯s′σ;
6. Cσ ̸⪯Dσ or D is an extended com-
binator axiom;
7. t ≈t′ is strictly σ-eligible in D;
8. [¬] s⟨u⟩≈s′ is σ-eligible in C, and
strictly σ-eligible if it is positive.
Deﬁnition 1. Let l = Cany xn and l ≈r be an extended combinator axiom. A
term v um is compatible with l ≈r if Cany = I and m = n or if Cany = K and
m ≥n −1 or if Cany ∈{B, C, S} and m ≥n −2.
Variable Condition: u /∈V . If u = x sn and D is an extended combinator
axiom, then D and u must be compatible.
Because the term ordering ≻is not compatible with unstable contexts, there
are instances when superposition beneath variables must be carried out. The
SubVarSup rule deals with this.

284
A. Bhayat and G. Reger
D′ ∨t ≈t′
C′ ∨[¬]s⟨y un ⟩≈s′
SubVarSup
(C′ ∨D′ ∨[¬]s⟨zt′ un ⟩≈s′)σ
with the following side conditions in addition to conditions 4 – 8 of Sup:
1. y has another occurrence bad in C;
2. z is a fresh variable;
3. σ = {y →z t};
4. t′ has a variable or combinator head;
5. n ≤1;
6. D is not an extended combinator
axiom.
The EqRes and EqFact inferences:
C′ ∨u ̸≈u′
EqRes
C′σ
C′ ∨u′ ≈v′ ∨u ≈v
EqFact
(C′ ∨v ̸≈v′ ∨u ≈v′)σ
For both inferences σ = mgu(u, u′). For EqRes, u ̸≈u′ is σ-eligible in the
premise. For EqFact, u′σ ̸⪯v′σ, uσ ̸⪯vσ, and u ≈v is σ-eligible in the premise.
In essence, the ArgCong inference allows superposition to take place at
preﬁx positions by ‘growing’ equalities to the necessary size.
C′ ∨s ≈s′
ArgCong
C′σ ∨(sσ)x ≈(s′σ)x
C′σ ∨(sσ) x2 ≈(s′σ) x2
C′σ ∨(sσ) x3 ≈(s′σ) x3
...
The literal s ≈s′ must be σ-eligible in C. Let s and s′ be of type α1 →· · · →
αm →β. If β is not a type variable, then σ is the identity substitution and
the inference has m conclusions. Otherwise, if β is a type variable, the inference
has an inﬁnite number of conclusions. In conclusions where n > m, σ is the
substitution that maps β to type τ1 →· · · →τn−m →β′ where β′ and each
τi are fresh type variables. In each conclusion, the xis are variables fresh for C.
Note that an ArgCong inference on a combinator axiom results in an extended
combinator axiom.
3.1
Extensionality
Clausal combinatory-superposition can be either intensional or extensional. If a
conjecture is proved by the intensional version of the calculus, it means that the
conjecture holds in all models of the axioms. On the other hand, if a conjecture
is proved by the extensional version, it means that the conjecture holds in all
extensional models (as deﬁned above). Practically, some domains naturally lend
themselves to intensional reasoning whilst other to extensional. For example,
when reasoning about programs, we may expect to treat diﬀerent programs as
diﬀerent entities even if they always produce the same output when provided the
same input. For the calculus to be extensional, we provide two possibilities. The

A Combinator-Based Superposition Calculus for Higher-Order Logic
285
ﬁrst is to add a polymorphic extensionality axiom. Let diﬀbe a polymorphic
symbol of type Πτ1, τ2. (τ1 →τ2) →(τ1 →τ2) →τ1. Then the extensionality
axiom can be given as:
x (diﬀ⟨τ1, τ2⟩x y)
̸≈
y (diﬀ⟨τ1, τ2⟩x y) ∨x ≈y
However, adding the extensionality axiom to a clause set can be explosive and
is not graceful. By any common ordering, the negative literal will be the larger
literal and therefore the literal involved in inferences. As it is not of functional
type it can unify with terms of atomic type including ﬁrst-order terms.
In order to circumvent this issue, we developed another method of dealing
with extensionality. Uniﬁcation is replaced by uniﬁcation with abstraction. Dur-
ing the uniﬁcation procedure, no attempt is made to unify pairs consisting of
terms of functional or variable type. Instead, if the remaining uniﬁcation pairs
can be solved successfully, such pairs are added to the resulting clause as neg-
ative constraint literals. This process works in conjunction with the negative
extensionality rule presented below.
C′ ∨s ̸≈s′
NegExt
(C′ ∨s (sk⟨α ⟩x ) ̸≈s′ (sk⟨α ⟩x ))σ
where s ̸≈s′ is σ-eligible in the premise, α and x are the free type and term
variable of the literal s ̸≈s′ and σ is the most general type uniﬁer that ensures
the well-typedness of the conclusion.
We motivate this second approach to extensionality with an example. Con-
sider the clause set:
g x ≈f x
h g ̸≈h f
equality resolution with abstraction on the second clause produces the clause
g ̸≈f. A NegExt inference on this clause results in g sk ̸≈f sk which can super-
pose with g x ≈f x to produce ⊥.
The uniﬁcation with abstraction procedure used here is very similar to that
introduced in [24]. Pseudocode for the algorithm can be found in Algorithm 1.
The inference rules other than ArgCong and SubVarSup must be modiﬁed to
utilise uniﬁcation with abstraction rather than standard uniﬁcation. We show the
updated superposition rule. The remaining rules can be modiﬁed along similar
lines.
C1 ∨t ≈t′
C2 ∨[¬]s⟨u⟩≈s′
Sup-wA
(C1 ∨C2 ∨D ∨[¬]s⟨t′⟩≈s′)σ
where D is the possibly empty set of negative literals returned by uniﬁcation.
Sup-wA shares all the side conditions of Sup given above. This method of
dealing with extensionality is not complete as shown in Appendix A of [9].

286
A. Bhayat and G. Reger
Algorithm 1. Uniﬁcation algorithm with constraints
function mguAbs(l, r)
let P be a set of uniﬁcation pairs; P := {⟨l, r⟩} , D be a set of disequalities;
D := ∅
let θ be a substitution; θ := {}
loop
if P is empty then return (θ, D), where D is the disjunction of literals in D
Select a pair ⟨s, t⟩in P and remove it from P
if s coincides with t then do nothing
else if s is a variable and s does not occur in t then θ := θ ◦{s →t};
P := P{s →t}
else if s is a variable and s occurs in t then fail
else if t is a variable then P := P ∪{⟨t, s⟩}
else if s and t have functional or variable type then D := D ∪{s ̸≈t}
else if s and t have diﬀerent head symbols then fail
else if s = f s1 . . . sn and t = f t1 . . . tn for some f then
P := P ∪{⟨s1, t1⟩, . . . , ⟨sn, tn⟩}
4
Examples
We provide some examples of how the calculus works. Some of the examples
utilised come from Bentkamp et al.’s paper [4] in order to allow a comparison
of the two methods. In all examples, it is assumed that the clause set has been
enriched with the combinator axioms.
Example 1. Consider the unsatisﬁable clause:
x a b ̸≈x b a
Superposing onto the left-hand side with the extended K axiom K x1 x2 x3 ≈
x1 x3 results in the clause x1 b ̸≈x1 a. Superposing onto the left-hand side of
this clause, this time with the standard K axiom adds the clause x ̸≈x from
which ⊥is derived by an EqRes inference.
Example 2. Consider the unsatisﬁable clause set where f a ≻c:
f a ≈c
h (y b)(y a) ̸≈h (g(f b))(g c)
A Sup inference between the B axiom B x1 x2 x3 ≈x1 (x2 x3) and the
subterm y b of the second clause adds the clause h(x1(x2 b))(x1(x2 a)) ̸≈
h(g(f b))(g c) to the set. By superposing onto the subterm x2 a of this clause
with the equation f a ≈c, we derive the clause h(x1(f b))(x1 c) ̸≈h(g(f b))(g c)
from which ⊥can be derived by an EqRes inference.
Example 3. Consider the unsatisﬁable clause set where f a ≻c. This example is
the combinatory equivalent of Bentkamp et al.’s Example 6.
f a ≈c
h (y (B g f) a) y ̸≈h (g c) I

A Combinator-Based Superposition Calculus for Higher-Order Logic
287
A Sup inference between the extended I axiom I x1 x2 ≈x1 x2 and the sub-
term y (B g f) a of the second clause adds the clause h (B g f a) I ̸≈h (g c) I to the
set. Superposing onto the subterm B g f a of this clause with the B axiom results
in the clause h (g (f a)) I ̸≈h (g c) I. Superposition onto the subterm f a with the
ﬁrst clause of the original set gives h (g c)) I ̸≈h (g c) I from which ⊥can be
derived via EqRes.
Note that in Examples 2 and 3, no use is made of SubVarSup even though
the analogous FluidSup rule in required in Bentkamp et al.’s calculus. We have
been unable to develop an example that requires the SubVarSup rule even
though it is required for the completeness result in Sect. 6.
5
Redundancy Criterion
In Sect. 6, we prove that the calculus is refutationally complete. The proof fol-
lows that of Bachmair and Ganzinger’s original proof of the completeness of
superposition [2], but is presented in the style of Bentkamp et al. [6] and Wald-
mann [31]. As is normal with such proofs, it utilises the concept of redundancy
to reduce the number of clauses that must be considered in the induction step
during the model construction process.
We deﬁne a weaker logic by an encoding ⌊⌋of ground terms into non-
applicative ﬁrst-order terms with ⌈⌉as its inverse. The encoding works by index-
ing each symbol with its type arguments and argument number. For example,
⌊f⌋= f0, ⌊f⟨τ ⟩a⌋= f τ
1 (a0). Terms with fully applied combinators as their head
symbols are translated to constants such that syntactically identical terms are
translated to the same constant. For example, ⌊S t1 t2 t3⌋= s0. The weaker logic
is known as the ﬂoor logic whilst the original logic is called the ceiling logic.
The encoding can be extended to literals and clauses in the obvious manner as
detailed in [5]. The function ⌈⌉is used to compare ﬂoor terms. More precisely,
for ﬂoor logic terms t and t′, t ≻t′ if ⌈t⌉≻⌈t′⌉. It is straightforward to show
that the order ≻on ﬂoor terms is compatible with all contexts, well-founded,
total on ground terms and has the subterm property.
The encoding serves a dual purpose. Firstly, as redundancy is deﬁned with
respect to the ﬂoor logic, it prevents the conclusion of all ArgCong from being
redundant. Secondly, subterms in the ﬂoor logic correspond to ﬁrst-order sub-
terms in the ceiling logic. This is of critical importance in the completeness
proof.
An inference is the ground instance of an inference I if it is equal to I after the
application of some grounding substitution θ to the premise(s) and conclusion
of I and the result is still an inference.
A ground ceiling clause C is redundant with respect to a set of ground ceiling
clauses N if ⌊C⌋is entailed by clauses in ⌊N⌋smaller than itself and the ﬂoor
of ground instances of extended combinator axioms in ⌊N⌋. An arbitrary ceiling
clause C is redundant to a set of ceiling clauses N if all its ground instances are
redundant with respect to GΣ(N), the set of all ground instances of clauses in
N. Red(N) is the set of all clauses redundant with respect to N.

288
A. Bhayat and G. Reger
For ground inferences other than ArgCong, an inference with right premise
C and conclusion E is redundant with respect to a set of clauses N if ⌊E⌋is
entailed by clauses in ⌊GΣ(N)⌋smaller than ⌊C⌋. A non-ground inference is
redundant if all its ground instances are redundant.
An ArgCong inference from a combinator axiom is redundant with respect
to a set of clauses N if its conclusion is in N. For any other ArgCong inference,
it is redundant with respect N if its premise is redundant with respect to N, or
its conclusion is in N or redundant with respect to N. A set N is saturated up to
redundancy if every inference with premises in N is redundant with respect to N.
6
Refutational Completeness
The proof of refutational completeness of clausal combinatory-superposition is
based on the completeness proof the λ-free HOL calculi presented in [6]. We
ﬁrst summarise their proof and then indicate the major places where our proof
diﬀers. A detailed version of the proof can be found in our technical report [9].
Let N be a set of clauses saturated to redundancy by one of the λ-free HOL
calculi and not containing ⊥. Then, Bentkmap et al. show that N must have a
model. This is done in stages, ﬁrst building a model R∞of ⌊GΣ(N)⌋and then
lifting this to a model of GΣ(N) and N. Most of the heavy work is in showing
R∞to be a model of ⌊GΣ(N)⌋. In the standard ﬁrst-order proof, superposition
is ruled out at or beneath variables. Consider a clause C containing a variable
x. Since the orderings that parameterise standard ﬁrst-order superposition are
compatible with contexts, we are guaranteed for terms t and t′ such that t ≻t′
and grounding substitution θ that Cθ[x →t] ≻Cθ[x →t′]. Then, the induction
hypotheses is used to show that Cθ[x →t′] is true in the candidate interpreta-
tion rendering superposition into x unnecessary. A similar argument works for
superposition below variables.
This argument does not work for the λ-free calculi since in their case the
ordering is not compatible with arguments. Consider the clause C = f x ≈
g x ∨x u ≈v. For terms t and t′ such that t ≻t′, it cannot be guaranteed that
Cθ[x →t] ≻Cθ[x →t′] since t′ u ≻t u is possible. Therefore, some superposition
has to take place at variables. Even in those cases where it can be ruled out,
the proof is more complex than the standard ﬁrst-order proof. Superposition
underneath variables can be ruled out entirely.
Returning to our calculus, we face a number of additional diﬃculties. Since
the ordering that we use is not compatible with arguments, but also not compati-
ble with stronger concept of unstable subterms, we cannot rule out superposition
beneath a variable. Consider, for example, the clause C = f x ≈g x ∨y x u ≈v
and the grounding substitution θ = {y →S a, x →K t}. Let θ′ be the same as θ,
but with x mapped to K t′. Even if t ≻t′, we do not necessarily have Cθ ≻Cθ′
since t occurs at an unstable position. Thus, some superposition below variables
must be carried out.
This introduces a new diﬃculty, namely, showing that superposition below a
variable is the ground instance of a SubVarSup rule. In some cases it may not be.

A Combinator-Based Superposition Calculus for Higher-Order Logic
289
Consider the clause C = x u ≈v and the grounding substitution θ = {x →f t a}.
A superposition inference from Cθ with conclusion C′ that replaces t with t′ is
not the ground instance of a SubVarSup from C. The only conclusion of a
SubVarSup from C is z t′ u ≈v. There is no instantiation of the z variable that
can make the term z t′ equal to f t′ a. However, mapping z to C f a results in a term
that is equal to f t′ a modulo the combinator axioms. Let C′′ = z t′ u ≈v{z →
C f a}. Since it is the set N that is saturated up to redundancy, we have that
all ground instances of the SubVarSup inference are redundant with respect
GΣ(N). For this to imply that C′ is redundant with respect to GΣ(N) requires
C′′ be rewritable to C′ using equations true in R⌊Cθ⌋(the partial interpretation
built from clauses smaller than Cθ). This requires that all ground instances of
combinator and extended combinator axioms be true in R⌊Cθ⌋for all clauses C.
This leads to probably the most novel aspect of our proof. We build our
candidate interpretations diﬀerently to the standard proof by ﬁrst adding rules
derived from combinator axioms. Let RECA be the set of rewrite rules formed
by turning the ﬂoor of all ground instances of combinator axioms into left right
rewrite rules. Then for all clauses C ∈⌊GΣ(N)⌋, RC is deﬁned to be RECA ∪
(
D≺C ED). In the detailed proof, we show that RC and R∞are still terminating
and conﬂuent.
In lifting R∞to be a model of GΣ(N), we face a further diﬃculty. This is
related to showing that for ceiling terms t, t′, u and u′, if ⌊t⌋ξ
R∞= ⌊t′⌋ξ
R∞
and ⌊u⌋ξ
R∞= ⌊u′⌋ξ
R∞, then ⌊t u⌋ξ
R∞= ⌊t′ u′⌋ξ
R∞. The diﬃculty arises
because t, t′ or both may be of the form C3 t1 t2. In such a case rewrites that can
be carried out from subterms of ⌊t⌋cannot be carried out from ⌊t u⌋because
t u has a fully applied combinator as its head and therefore is translated to a
constant in the ﬂoor logic. The fact that the ground instances of all combinator
axioms are true in R∞comes to the rescue. With these diﬃculties circumvented,
we can prove refutational completeness.
Theorem 1. For a set of clauses N saturated to redundancy by the above cal-
culus, N has a model iﬀit does not contain ⊥. Moreover, if N contains the
extensionality axiom, the model is extensional.
7
Removing Combinator Axioms
Next, we show that it is possible to replace the combinator axioms with a dedi-
cated inference rule. We name the inference Narrow. Unlike the other inference
rules, it works at preﬁx positions. We deﬁne nearly ﬁrst-order positions induc-
tively. For any term t, either t = ζ tn where ζ is not a fully applied combinator
or t = Cany tn . In the ﬁrst case, the nearly ﬁrst-order subterms of t are ζ ti for
0 ≤i ≤n and all the nearly ﬁrst-order subterms of the ti. In the second case,
the nearly ﬁrst-order subterms are Cany ti for 0 ≤i ≤n. The notation s⟨[u]⟩is
to be read as u is a nearly ﬁrst-order subterm of s. The Narrow inference:
C′ ∨[¬]s⟨[u]⟩≈s′
Narrow
(C′ ∨[¬]s⟨[r]⟩≈s′)σ
with the following side conditions:

290
A. Bhayat and G. Reger
1. u /∈V
2. Let l ≈r be a combinator axiom.
σ = mgu(l, u);
3. s⟨[u]⟩σ ̸≾s′σ;
4. [¬] s⟨[u]⟩≈s′ is σ-eligible in C, and
strictly σ-eligible if it is positive.
We show that any inference that can be carried out using an extended com-
binator axiom can be simulated with Narrow proving completeness. It is obvi-
ous that an EqRes or EqFact inference cannot have an extended combinator
axiom as its premise. By the SubVarSup side conditions, an extended combi-
nator axiom cannot be either of its premises. Thus we only need to show that
Sup inferences with extended combinator axioms can be simulated. Note that
an extended axiom can only be the left premise of a Sup inference. Consider the
following inference:
l ≈r
C′ ∨[¬]s⟨u⟩|p ≈s′
Sup
(C′ ∨[¬]s⟨r⟩≈s′)σ
Let l = S xn>3 . By the variable condition, we have that u = ζ tm where
n ≥m ≥n −2. If u = y tn−2 , then σ = {y →S x1 x2, x3 →t1, . . . , xn →
tn−2}. In this case rσ = (x1 x3 (x2 x3) x4 . . . xn)σ = x1 t1 (x2 t1) t2 . . . tn−2 and
the conclusion of the inference is (C′ ∨[¬]s⟨x1 t1 (x2 t1) t2 . . . tn−2⟩≈s′){y →
S x1 x2}. Now consider the following Narrow inference from C at the nearly
ﬁrst-order subterm y t1:
C′ ∨[¬]s⟨⟨[y t1]⟩t2 . . . tn⟩|p ≈s′
Narrow
(C′ ∨[¬]s⟨x1 t1 (x2 t1) t2 . . . tn−2⟩≈s′){y →S x1 x2}
As can be seen, the conclusion of the Sup inference is equivalent to that of
the Narrow inference up to variable naming. The same can be shown to be
the case where u = y tn−1 or u = y tn or u = S tn . Likewise, the same can be
shown to hold when the l ≈r is an extended B, C, K or I axiom.
8
Implementation and Evaluation
Clausal combinatory-superposition has been implemented in the Vampire theo-
rem prover [11,17]. The prover was ﬁrst extended to support polymorphism.
This turned out to be simpler than expected with types being turned into
terms and type equality checking changing to a uniﬁability (or matching) check.
Applicative terms are supported by the use of a polymorphic function app of
type Πα, β. (α →β) →α →β.
As the Sup, EqRes and EqFact inferences are identical to their ﬁrst-
order counterparts, these required no updating. The Narrow, SubVarSup and
ArgCong inferences had to be added to the implementation. Further, though
the NegExt inference is not required for completeness, empirical results suggest
that it is so useful, that it is permanently on in the implementation.

A Combinator-Based Superposition Calculus for Higher-Order Logic
291
The ArgCong inference implemented in Vampire does not match the rule
given in the calculus. The rule provided can have an inﬁnite number of conclu-
sions. In Vampire, we have implemented a version of ArgCong that appends a
single fresh variable to each side of the selected literal rather than a tuple and
therefore only has a single conclusion. This version matches what was originally
in the calculus. Shortly before the submission of this paper, it was discovered
that this leads to a subtle issue in the completeness proof and the inference was
changed to its current version. We expect to be able to revert to the previous
version and ﬁx the proof. As matters stand, Vampire contains a potential source
of incompleteness.
A greater challenge was posed by the implementation of the >ski ordering in
the prover. The ordering is based on the length of the longest weak-reduction
from a term. In order to increase the eﬃciency of calculating this quantity, we
implemented caching and lazy evaluation. For example, when inserting a term
of the form f t1 t2 into the term-sharing data structure, a check is made to see
if the maximum reduction lengths of t1 and t2 have already been calculated.
If they have, then the maximum reduction length of the term being inserted is
set to the sum of the maximum reduction lengths of t1 and t2. If not, it is left
unassigned and only calculated at the time it is required.
During the experimentation phase, it was realised that many redundant
clauses were being produced due to narrowing. For example, consider the
clause x a b ≈d ∨f x ≈a. Narrowing the ﬁrst literal with C-axiom results
in x′ b a ≈d ∨f (C x′) ≈a. A second narrow with the same axiom results in
x′′ a b ≈d ∨f (C (C x′′)) ≈a which is extensionally equivalent to ﬁrst clause and
therefore redundant. However, it will not be removed by subsumption since it is
only equivalent extensionally. To deal with this problem, we implemented some
rewrite rules that replace combinator terms with smaller extensionally equiva-
lent terms.2 For example, any term of the form C (C t) is rewritten to t. There
is no guarantee that these rewrites remove all such redundant clauses, but in
practice, they appear to help.
To implement uniﬁcation with abstraction, we reused the method introduced
in our previous work relating to the use of substitution trees as ﬁlters [8]. In our
current context, this involves replacing all subterms of functional or variable sort
with special symbols that unify with any term prior to inserting a term into the
substitution tree index.
To evaluate our implementation, we ran a number of versions of our prover
across two problem sets and compared their performance against that of some of
the leading higher-order provers. The ﬁrst problem set we tested on was the set
of all 592 monomorphic, higher-order problems from the TPTP problem library
[29] that do not contain ﬁrst-class boolean subterms. We restricted our attention
to monomorphic problems since some of the provers we used in our evaluation
do not support polymorphism. The second benchmark set was produced by the
Isabelle theorem prover’s Sledgehammer system. It contains 1253 benchmarks
kindly made available to us by the Matryoshka team and is called SH-λ fol-
2 Thanks to Petar Vukmirovi´c for suggesting and discussing this idea.

292
A. Bhayat and G. Reger
Table 1. Problems proved theorem or unsat
TPTP TH0 problems Sh-λ problems
Solved Uniques
Solved Uniques
Satallax 3.4
473
0
628
5
Leo-III 1.4
482
6
661
13
Vampire-THF 4.4
472
1
717
14
Vampire-csup-ninj
470
0 (1)
687
1 (2)
Vampire-csup-ax
469
0 (0)
680
0 (3)
Vampire-csup-abs
472
0 (0)
685
0 (0)
Vampire-csup-prag 475
1 (3)
628
0 (1)
Zipperposition 1.5
476
0
609
6
lowing their naming convention. All tests were run with a CPU time limit of
300. Experiments were performed on StarExec [28] nodes equipped with four
2.40 GHz Intel Xeon CPUs. Our experimental results are publicly available3.
To compare out current implementation against, we chose the Leo-III, 1.4,
Satallax 3.4, Zipperposition 1.5 and Vampire-THF 4.4 provers. These provers
achieved the top four spots in the 2019 CASC system competition. Vampire THF
4.4 was developed by the authors, but uses diﬀerent principles being based on
combinatory uniﬁcation. We compare the performance of these provers against
four variants of our current implementation. First, Vampire-csup-ax which imple-
ments clausal combinatory-superposition as described above and uses the exten-
sionality axiom. Second, Vampire-csup-abs which deals with extensionality via
uniﬁcation with abstraction. Third, Vampire-csup-ninj which incorporates an
inference to synthesise left-inverses for injective functions in a manner similar to
Leo-III [26, Section 4.2.5] and ﬁnally Vampire-csup-prag which introduces var-
ious heuristics to try and control the search space, though at the expense of
completeness. For example, it implements a heuristic that restricts the number
of narrow steps. It also switches oﬀthe SubVarSup rule which is never used
in a proof produced by the other variants of Vampire-csup. All four versions
are run on top of a ﬁrst-order portfolio of strategies. These strategies control
options such as the saturation algorithm used, which simpliﬁcation inferences
are switched on and so forth. The results of the experiments can be found sum-
marised in Table 1. In brackets, the number of uniques between Vampire-csup
versions is provided.
The closeness of the results on the TPTP benchmarks is striking. Out of the
592 benchmarks, 95 are known not to be theorems, leaving 497 problems that
could possibly be proved. All the provers are remarkably close to this number and
each other. Leo-III which is slightly ahead of the other provers, only manages
3 https://github.com/vprover/vampire publications/tree/master/experimental data/
IJCAR-2020-COMB-SUP.

A Combinator-Based Superposition Calculus for Higher-Order Logic
293
this through function synthesis which is not implemented in any of the other
provers.
It is disappointing that Vampire-csup performs worse than its predeces-
sor Vampire-THF 4.4 on Sledgehammer problems. We hypothesise that this is
related to the explosion in clauses created as a result of narrowing. Vampire-
csup-prag is supposed to control such an explosion, but actually performs worst
of all. This is likely due to the fact that it runs a number of lengthy strategies
aimed particularly at solving higher-order problems requiring complex uniﬁers.
Interestingly, the pragmatic version solved a diﬃculty rating 1.00 TPTP prob-
lem, namely, NUM829∧5.p.
9
Conclusion and Related Work
The combinatory superposition calculus presented here is amongst a small group
of complete proof calculi for higher-order logic. This group includes the RUE
resolution calculus of Benzm¨uller which has been implemented in the Leo-II
theorem prover [7]. The Satallax theorem prover implements a complete higher-
order tableaux calculus [13]. More recently, Bentkamp et al. have developed a
complete superposition calculus for clausal HOL [4]. As superposition is one
of the most successful calculi in ﬁrst-order theorem proving [22], their work
answered a signiﬁcant open question, namely, whether superposition could be
extended to higher-order logic.
Our work is closely related to theirs, and in some senses, the SubVar-
Sup rule of clausal combinatory-superposition mirrors the FluidSup rule of
clausal λ-superposition. However, there are some crucial diﬀerences. Arguably,
the side conditions on SubVarSup are tighter than those on FluidSup and some
problems such as the one in Example 3 can be solved by clausal combinatory-
superposition without the use of SubVarSup whilst requiring the use of Fluid-
Sup in clausal λ-superposition. Clausal λ-superposition is based on higher-order
uniﬁcation and λ-terms. Our calculus is based on (applicative) ﬁrst-order terms
and ﬁrst-order uniﬁcation and implementations can therefore reuse the well-
studied data structures and algorithms of ﬁrst-order theorem proving. On the
downside, narrowing terms with combinator axioms is still explosive and results
in redundant clauses. It is also never likely to be competitive with higher-order
uniﬁcation in ﬁnding complex uniﬁers. This is particularly the case with recent
improvements in higher-order uniﬁcation being reported [30].
Many other calculi for higher-order theorem proving have been devel-
oped, most of them incomplete. Amongst the early calculi to be devised are
Andrew’s mating calculus [1] and Miller’s expansion tree method [20] both
linked to tableaux proving. More recent additions include an ordered (incom-
plete) paramodulation calculus as implemented in the Leo-III prover [27] and a
higher-order sequent calculus implemented in the AgsyHOL prover [18]. In pre-
vious work, the current authors have extended ﬁrst-order superposition to use a
combinatory uniﬁcation algorithm [8]. Finally there is ongoing work to extend
SMT solving to higher-order logic [3].

294
A. Bhayat and G. Reger
There have also been many attempts to prove theorems in HOL by translating
to FOL. One of the pioneers in suggesting this approach was Kerber [16]. Since
his early work, it has become commonplace to combine a dedicated higher-
order theorem prover with a ﬁrst-order prover used to discharge ﬁrst-order proof
obligations. This is the approach taken by many interactive provers and their
associated hammers such as Sledgehammer [23] and CoqHammer [14]. It is also
the approach adopted by leading automated higher-order provers Leo-III and
Satallax.
In this paper we have presented a complete calculus for a polymorphic,
boolean-free, intensional, combinatory formulation of higher-order logic. For the
calculus to be extensional, an extensionality axiom can be added maintaining
completeness, but losing gracefulness. Alternatively, uniﬁcation can be turned
into uniﬁcation with abstraction maintaining gracefulness, but losing a com-
pleteness guarantee. Experimental results show an implementation of clausal
combinatory-superposition to be competitive with leading higher-order provers.
It remains to tune the implementation and calculus. We plan to further
investigate the use of heuristics in taming the explosion of clauses that result
from narrowing. the heuristics may lead to incompleteness. It would also be of
interest to investigate the use of heuristics or even machine learning to guide the
prover in selecting speciﬁc combinator axioms to narrow a particular clause with.
One of the advantages of our calculus is that it does not consider terms modulo
β- or weak-reduction. Therefore, theoretically, a larger class of terms should be
comparable by the non-ground order than is possible with a calculus that deals
with β- or weak-equivalence classes. It remains to implement a stricter version
of the >ski ordering and evaluate its usefulness.
As a next step, we plan to add support for booleans and choice to the calculus.
An appealing option for booleans is to extend the uniﬁcation with abstraction
approach currently used for functional extensionality. No attempt would be made
to solve uniﬁcation pairs consisting of boolean terms. Rather, these would be
added as negated bi-implications to the result which would then be re-clausiﬁed.
Finally, we feel that our calculus complements existing higher-order calculi
and presents a particularly attractive option for extending existing ﬁrst-order
superposition provers to dealing with HOL.
Acknowledgements. Thanks to Jasmin Blanchette, Alexander Bentkamp and Petar
Vukmirovi´c for many discussions on aspects of this research. We would also like to
thank Andrei Voronkov, Martin Riener and Michael Rawson. We are grateful to Visa
Nummelin for pointing out the incompleteness of uniﬁcation with abstraction and
providing the counterexample. Thanks is also due to the maintainers of StarExec and
the TPTP problem library both of which were invaluable to this research. The ﬁrst
author thanks the family of James Elson for funding his research.
References
1. Andrews, P.B.: On connections and higher-order logic. J. Autom. Reasoning 5(3),
257–291 (1989)

A Combinator-Based Superposition Calculus for Higher-Order Logic
295
2. Bachmair, L., Ganzinger, H.: Rewrite-based equational theorem proving with selec-
tion and simpliﬁcation. J. Logic Comput. 4(3), 217–247 (1994)
3. Barbosa, H., Reynolds, A., El Ouraoui, D., Tinelli, C., Barrett, C.: Extending SMT
solvers to higher-order logic. In: Fontaine, P. (ed.) CADE 2019. LNCS (LNAI),
vol. 11716, pp. 35–54. Springer, Cham (2019). https://doi.org/10.1007/978-3-030-
29436-6 3
4. Bentkamp, A., Blanchette, J., Tourret, S., Vukmirovi´c, P., Waldmann, U.: Superpo-
sition with lambdas. In: Fontaine, P. (ed.) CADE 2019. LNCS (LNAI), vol. 11716,
pp. 55–73. Springer, Cham (2019). https://doi.org/10.1007/978-3-030-29436-6 4
5. Bentkamp, A., Blanchette, J., Tourret, S., Vukmirovi´c, P., Waldmann, U.: Superpo-
sition with lambdas (technical report). Technical report (2019). http://matryoshka.
gforge.inria.fr/pubs/lamsup report.pdf
6. Bentkamp, A., Blanchette, J.C., Cruanes, S., Waldmann, U.: Superposition for
lambda-free higher-order logic. In: Galmiche, D., Schulz, S., Sebastiani, R. (eds.)
IJCAR 2018. LNCS (LNAI), vol. 10900, pp. 28–46. Springer, Cham (2018). https://
doi.org/10.1007/978-3-319-94205-6 3
7. Benzm¨uller, C., Sultana, N., Paulson, L.C., Theib, F.: The higher-order prover
Leo-II. J. Autom. Reasoning 55(4), 389–404 (2015)
8. Bhayat, A., Reger, G.: Restricted combinatory uniﬁcation. In: Fontaine, P. (ed.)
CADE 2019. LNCS (LNAI), vol. 11716, pp. 74–93. Springer, Cham (2019). https://
doi.org/10.1007/978-3-030-29436-6 5
9. Bhayat, A., Reger, G.: A combinator-based superposition calculus for higher-order
logic (technical report). Technical report, University of Mancester (2020). https://
easychair.org/publications/preprint open/66hZ
10. Bhayat, A., Reger, G.: A Knuth-Bendix-like ordering for orienting combinator
equations. In: The 10th International Joint Conference on Automated Reasoning
(IJCAR) (2020)
11. Bhayat, A., Reger, G.: A polymorphic vampire (short paper). In: The 10th Inter-
national Joint Conference on Automated Reasoning (IJCAR) (2020)
12. Bobot, F., Paskevich, A.: Expressing polymorphic types in a many-sorted language.
In: Tinelli, C., Sofronie-Stokkermans, V. (eds.) FroCoS 2011. LNCS (LNAI), vol.
6989, pp. 87–102. Springer, Heidelberg (2011). https://doi.org/10.1007/978-3-642-
24364-6 7
13. Brown, C.E.: Satallax: an automatic higher-order prover. In: Gramlich, B., Miller,
D., Sattler, U. (eds.) IJCAR 2012. LNCS (LNAI), vol. 7364, pp. 111–117. Springer,
Heidelberg (2012). https://doi.org/10.1007/978-3-642-31365-3 11
14. Czajka, L., Kaliszyk, C.: Hammer for Coq: automation for dependent type theory.
J. Autom. Reasoning 61(1), 423–453 (2018)
15. Hindley, J.R. Seldin, J.P.: Lambda-Calculus and Combinators: An Introduction,
2nd edn. Cambridge University Press, New York (2008)
16. Kerber, M.: How to prove higher order theorems in ﬁrst order logic, pp. 137–142,
January 1991
17. Kov´acs, L., Voronkov, A.: First-order theorem proving and Vampire. In: Shary-
gina, N., Veith, H. (eds.) CAV 2013. LNCS, vol. 8044, pp. 1–35. Springer, Heidel-
berg (2013). https://doi.org/10.1007/978-3-642-39799-8 1
18. Lindblad, F.: A focused sequent calculus for higher-order logic. In: Demri, S.,
Kapur, D., Weidenbach, C. (eds.) IJCAR 2014. LNCS (LNAI), vol. 8562, pp. 61–
75. Springer, Cham (2014). https://doi.org/10.1007/978-3-319-08587-6 5
19. Meng, J., Paulson, L.C.: Translating higher-order clauses to ﬁrst-order clauses. J.
Autom. Reasoning 40(1), 35–60 (2008)

296
A. Bhayat and G. Reger
20. Miller, D.A.: Proofs in higher-order logic. Ph.D. thesis, University of Pennsylvania
(1983)
21. Miller, D.A.: A compact representation of proofs. Stud. Logica 46(4), 347–370
(1987)
22. Nieuwenhuis, R., Rubio, A.: Paramodulation-based theorem proving, chap. 7. In:
Robinson, A., Voronkov, A. (eds.) Handbook of Automated Reasoning, vol. I, pp.
371–443. Elsevier Science (2001)
23. Paulsson, L.C., Blanchette, J.C.: Three years of experience with Sledgehammer, a
practical link between automatic and interactive theorem provers. In: IWIL-2010,
vol. 1 (2010)
24. Reger, G., Suda, M., Voronkov, A.: Uniﬁcation with abstraction and theory instan-
tiation in saturation-based reasoning. In: Beyer, D., Huisman, M. (eds.) TACAS
2018. LNCS, vol. 10805, pp. 3–22. Springer, Cham (2018). https://doi.org/10.1007/
978-3-319-89960-2 1
25. Schulz, S.: E – a Brainiac theorem prover. AI Commun. 15(2, 3), 111–126 (2002)
26. Steen, A.: Extensional paramodulation for higher-order logic and its eﬀective imple-
mentation Leo-III. Ph.D. thesis, Freie Universit¨at Berlin (2018)
27. Steen, A., Benzm¨uller, C.: The higher-order prover Leo-III. In: Galmiche, D.,
Schulz, S., Sebastiani, R. (eds.) IJCAR 2018. LNCS (LNAI), vol. 10900, pp. 108–
116. Springer, Cham (2018). https://doi.org/10.1007/978-3-319-94205-6 8
28. Stump, A., Sutcliﬀe, G., Tinelli, C.: StarExec, a cross community logic solving
service (2012). https://www.starexec.org
29. Sutcliﬀe, G.: The TPTP problem library and associated infrastructure, from CNF
to TH0, TPTP v6.4.0. J. Autom. Reasoning 59(4), 483–502 (2017)
30. Vukmirovi´c, P., Bentkamp, A., Nummelin, V.: Eﬃcient full higher-order uniﬁcation
(2019, unpublished). http://matryoshka.gforge.inria.fr/pubs/hounif paper.pdf
31. Waldmann, U.: Automated reasoning II. Lecture notes, Max-Planck-Institut f¨ur
Informatik (2016). http://resources.mpi-inf.mpg.de/departments/rg1/teaching/
autrea2-ss16/script-current.pdf

Subsumption Demodulation
in First-Order Theorem Proving
Bernhard Gleiss1, Laura Kov´acs1,2
, and Jakob Rath1(B)
1 TU Wien, Vienna, Austria
jakob.rath@tuwien.ac.at
2 Chalmers University of Technology, Gothenburg, Sweden
Abstract. Motivated by applications of ﬁrst-order theorem proving to
software analysis, we introduce a new inference rule, called subsumption
demodulation, to improve support for reasoning with conditional equal-
ities in superposition-based theorem proving. We show that subsump-
tion demodulation is a simpliﬁcation rule that does not require radi-
cal changes to the underlying superposition calculus. We implemented
subsumption demodulation in the theorem prover Vampire, by extend-
ing Vampire with a new clause index and adapting its multi-literal
matching component. Our experiments, using the TPTP and SMT-LIB
repositories, show that subsumption demodulation in Vampire can solve
many new problems that could so far not be solved by state-of-the-art
reasoners.
1
Introduction
For the eﬃciency of organizing proof search during saturation-based ﬁrst-order
theorem proving, simpliﬁcation rules are of critical importance. Simpliﬁcation
rules are inference rules that do not add new formulas to the search space, but
simplify formulas by deleting (redundant) clauses from the search space. As such,
simpliﬁcation rules reduce the size of the search space and are crucial in making
automated reasoning eﬃcient.
When reasoning about properties of ﬁrst-order logic with equality, one of
the most common simpliﬁcation rules is demodulation [10] for rewriting (and
hence simplifying) formulas using unit equalities l ≃r, where l, r are terms and
≃denotes equality. As a special case of superposition, demodulation is imple-
mented in ﬁrst-order provers such as E [14], Spass [21] and Vampire [10]. Recent
applications of superposition-based reasoning, for example to program analysis
and veriﬁcation [5], demand however new and eﬃcient extensions of demodula-
tion to reason about and simplify upon conditional equalities C →l ≃r, where
C is a ﬁrst-order formula. Such conditional equalities may, for example, encode
software properties expressed in a guarded command language, with C denoting
a guard (such as a loop condition) and l ≃r encoding equational properties over
program variables. We illustrate the need of considering generalized versions of
demodulation in the following example.
c
⃝Springer Nature Switzerland AG 2020
N. Peltier and V. Sofronie-Stokkermans (Eds.): IJCAR 2020, LNAI 12166, pp. 297–315, 2020.
https://doi.org/10.1007/978-3-030-51074-9_17

298
B. Gleiss et al.
Example 1. Consider the following formulas expressed in the ﬁrst-order theory
of integer linear arithmetic:
f(i) ≃g(i)
0 ≤i < n →P(f(i))
(1)
Here, i is an implicitly universally quantiﬁed logical variable of integer sort, and
n is an integer-valued constant. First-order reasoners will ﬁrst clausify formu-
las (1), deriving:
f(i) ≃g(i)
0 ≰i ∨i ≮n ∨P(f(i))
(2)
By applying demodulation over (2), the formula 0 ≰i ∨i ≮n ∨P(f(i)) is
rewritten1 using the unit equality f(i) ≃g(i), yielding the clause 0 ≰i ∨i ≮
n ∨P(g(i)). That is, 0 ≤i < n →P(g(i)) is derived from (1) by one application
of demodulation.
Let us now consider a slightly modiﬁed version of (1), as below:
0 ≤i < n →f(i) ≃g(i)
0 ≤i < n →P(f(i))
(3)
whose clausal representation is given by:
0 ≰i ∨i ≮n ∨f(i) ≃g(i)
0 ≰i ∨i ≮n ∨P(f(i))
(4)
It is again obvious that from (3) one can derive the formula 0 ≤i < n →
P(g(i)), or equivalently the clause:
0 ≰i ∨i ≮n ∨P(g(i))
(5)
Yet, one cannot anymore apply demodulation-based simpliﬁcation over (4) to
derive such a clause, as (4) contains no unit equality.
⊓⊔
In this paper we propose a generalized version of demodulation, called sub-
sumption demodulation, allowing to rewrite terms and simplify formulas using
rewriting based on conditional equalities, such as in (3). To do so, we extend
demodulation with subsumption, that is with deciding whether (an instance of
a) clause C is a submultiset of a clause D. In particular, the non-equality literals
of the conditional equality (i.e., the condition) need to subsume the unchanged
literals of the simpliﬁed clause. This way, subsumption demodulation can be
applied to non-unit clauses and is not restricted to have at least one premise
clause that is a unit equality. We show that subsumption demodulation is a sim-
pliﬁcation rule of the superposition framework (Sect. 4), allowing for example to
derive the clause (5) from (4) in one inference step. By properly adjusting clause
indexing and multi-literal matching in ﬁrst-order theorem provers, we provide an
eﬃcient implementation of subsumption demodulation in Vampire (Sect. 5) and
1 Assuming that g is simpler/smaller than f.

Subsumption Demodulation in First-Order Theorem Proving
299
evaluate our work against state-of-the-art reasoners, including E [14], Spass [21],
CVC4 [3] and Z3 [7] (Sect. 6).
Related Work. While several approaches generalize demodulation in superposi-
tion-based theorem proving, we argue that subsumption demodulation improves
existing methods either in terms of applicability and/or eﬃciency. The AVATAR
architecture of ﬁrst-order provers [19] splits general clauses into components with
disjoint sets of variables, potentially enabling demodulation inferences whenever
some of these components become unit equalities. Example 1 demonstrates that
subsumption demodulation applies in situations where AVATAR does not: in
each clause of (4), all literals share the variable i and hence none of the clauses
from (4) can be split using AVATAR. That is, AVATAR would not generate
unit equalities from (4), and therefore cannot apply demodulation over (4) to
derive (5).
The local rewriting approach of [20] requires rewriting equality literals to
be maximal2 in clauses. However, following [10], for eﬃciency reasons we con-
sider equality literals to be “smaller” than non-equality literals. In particular,
the equality literals of clauses (4) are “smaller” than the non-equality literals,
preventing thus the application of local rewriting in Example 1.
To the extent of our knowledge, the ordering restrictions on non-unit rewrit-
ing [20] do not ensure redundancy, and thus the rule is not a simpliﬁcation
inference rule. Subsumption demodulation includes all necessary conditions and
we prove it to be a simpliﬁcation rule. Furthermore, we show how the order-
ing restrictions can be simpliﬁed which enables an eﬃcient implementation, and
then explain how such an implementation can be realized.
We further note that the contextual rewriting rule of [1] is more general
than our rule of subsumption demodulation, and has been ﬁrst implemented
in the Saturate system [12]. Yet, eﬃciently automating contextual rewriting
is extremely challenging, while subsumption demodulation requires no radical
changes in the existing machinery of superposition provers (see Sect. 5).
To the best of our knowledge, except Spass [21] and Saturate, no other
state-of-the-art superposition provers implement variants of conditional rewrit-
ing. Subterm contextual rewriting [22] is a reﬁned notion of contextual rewriting
and is implemented in Spass. A major diﬀerence of subterm contextual rewriting
when compared to subsumption demodulation is that in subsumption demodu-
lation the discovery of the substitution is driven by the side conditions whereas
in subterm contextual rewriting the side conditions are evaluated by checking
the validity of certain implications by means of a reduction calculus. This reduc-
tion calculus recursively applies another restriction of contextual rewriting called
recursive contextual ground rewriting, among other standard reduction rules.
While subterm contextual rewriting is more general, we believe that the ben-
eﬁt of subsumption demodulation comes with its relatively easy and eﬃcient
integration within existing superposition reasoners, as evidenced also in Sect. 6.
2 w.r.t. clause ordering.

300
B. Gleiss et al.
Local contextual rewriting [9] is another reﬁnement of contextual rewriting
implemented in Spass. In our experiments it performed similarly to subterm
contextual rewriting.
Finally, we note that SMT-based reasoners also implement various methods
to eﬃciently handle conditional equalities [6,13]. Yet, the setting is very dif-
ferent as they rely on the DPLL(T) framework [8] rather than implementing
superposition.
Contributions. Summarizing, this paper brings the following contributions.
– To improve reasoning in the presence of conditional equalities, we introduce
the new inference rule subsumption demodulation, which generalizes demod-
ulation to non-unit equalities by combining demodulation and subsumption
(Sect. 4).
– Subsumption demodulation does not require radical changes to the under-
lying superposition calculus. We implemented subsumption demodulation in
the ﬁrst-order theorem prover Vampire, by extending Vampire with a new
clause index and adapting its multi-literal matching component (Sect. 5).
– We compared our work against state-of-the-art reasoners, using the TPTP
and SMT-LIB benchmark repositories. Our experiments show that subsump-
tion demodulation in Vampire can solve 11 ﬁrst-order problems that could
so far not be solved by any other state-of-the-art provers, including Vampire,
E, Spass, CVC4 and Z3 (Sect. 6).
2
Preliminaries
For simplicity, in what follows we consider standard ﬁrst-order logic with equal-
ity, where equality is denoted by ≃. We support all standard boolean connec-
tives and quantiﬁers in the language. Throughout the paper, we denote terms by
l, r, s, t, variables by x, y, constants by c, d, function symbols by f, g and predi-
cate symbols by P, Q, R, all possibly with indices. Further, we denote literals by
L and clauses by C, D, again possibly with indices. We write s ̸≃t to denote the
formula ¬s ≃t. A literal s ≃t is called an equality literal. We consider clauses as
multisets of literals and denote by ⊆M the subset relation among multisets. A
clause that only consists of one equality literal is called a unit equality.
An expression E is a term, literal, or clause. We write E[s] to mean an
expression E with a particular occurrence of a term s. A substitution, denoted
by σ, is any ﬁnite mapping of the form {x1 
→t1, . . . , xn 
→tn}, where n > 0.
Applying a substitution σ to an expression E yields another expression, denoted
by Eσ, by simultaneously replacing each xi by ti in E. We say that Eσ is an
instance of E. A uniﬁer of two expressions E1 and E2 is a substitution σ such
that E1σ = E2σ. If two expressions have a uniﬁer, they also have a most general
uniﬁer (mgu). A match of expression E1 to expression E2 is a substitution σ
such that E1σ = E2. Note that any match is a uniﬁer (assuming the sets of
variables in E1 and E2 are disjoint), but not vice-versa, as illustrated below.

Subsumption Demodulation in First-Order Theorem Proving
301
Example 2. Let E1 and E2 be the clauses Q(x, y)∨R(x, y) and Q(c, d)∨R(c, z),
respectively. The only possible match of Q(x, y) to Q(c, d) is σ1 = {x 
→c, y 
→
d}. On the other hand, the only possible match of R(x, y) to R(c, z) is σ2 =
{x 
→c, y 
→z}. As σ1 and σ2 are not the same, there is no match of E1 to
E2. Note however that E1 and E2 can be uniﬁed; for example, using σ3 = {x 
→
c, y 
→d, z 
→d}.
Superposition Inference System. We assume basic knowledge in ﬁrst-order
theorem proving and superposition reasoning [2,11]. We adopt the notations and
the inference system of superposition from [10]. We recall that ﬁrst-order provers
perform inferences on clauses using inference rules, where an inference is usually
written as:
C1
. . .
Cn
C
with n ≥0. The clauses C1, . . . , Cn are called the premises and C is the conclu-
sion of the inference above. An inference is sound if its conclusion is a logical
consequence of its premises. An inference rule is a set of (concrete) inferences
and an inference system is a set of inference rules. An inference system is sound
if all its inference rules are sound.
Modern ﬁrst-order theorem provers implement the superposition inference
system for ﬁrst-order logic with equality. This inference system is parametrized
by a simpliﬁcation ordering over terms and a literal selection function over
clauses. In what follows, we denote by ≻a simpliﬁcation ordering over terms,
that is ≻is a well-founded partial ordering satisfying the following three condi-
tions:
– stability under substitutions: if s ≻t, then sθ ≻tθ;
– monotonicity: if s ≻t, then l[s] ≻l[t];
– subterm property: s ≻t whenever t is a proper subterm of s.
The simpliﬁcation ordering ≻on terms can be extended to a simpliﬁcation order-
ing on literals and clauses, using a multiset extension of orderings. For simplicity,
the extension of ≻to literals and clauses will also be denoted by ≻. Whenever
E1 ≻E2, we say that E1 is bigger than E2 and E2 is smaller than E1 w.r.t. ≻.
We say that an equality literal s ≃t is oriented, if s ≻t or t ≻s. The literal
extension of ≻asserts that negative literals are always bigger than their pos-
itive counterparts. Moreover, if L1 ≻L2, where L1 and L2 are positive, then
¬L1 ≻L1 ≻¬L2 ≻L2. Finally, equality literals are set to be smaller than any
literal using a predicate diﬀerent than ≃.
A selection function selects at least one literal in every non-empty clause. In
what follows, selected literals in clauses will be underlined: when writing L ∨C,
we mean that (at least) L is selected in L ∨C. In what follows, we assume that
selection functions are well-behaved w.r.t. ≻: either a negative literal is selected
or all maximal literals w.r.t. ≻are selected.
In the sequel, we ﬁx a simpliﬁcation ordering ≻and a well-behaved selec-
tion function and consider the superposition inference system, denoted by Sup,

302
B. Gleiss et al.
parametrized by these two ingredients. The inference system Sup for ﬁrst-order
logic with equality consists of the inference rules of Fig. 1, and it is both sound
and refutationally complete. That is, if a set S of clauses is unsatisﬁable, then
the empty clause (that is, the always false formula) is derivable from S in Sup.
Resolution and Factoring
L ∨C1
¬L′ ∨C2
(C1 ∨C2)σ
L ∨L′ ∨C
(L ∨C)σ
where L is not an equality literal and σ = mgu(L, L′)
Superposition
s ≃t ∨C1
L[s′] ∨C2
(C1 ∨L[t] ∨C2)θ
s ≃t ∨C1
l[s′] ≃l′ ∨C2
(C1 ∨l[t] ≃l′ ∨C2)θ
s ≃t ∨C1
l[s′] ̸≃l′ ∨C2
(C1 ∨l[t] ̸≃l′ ∨C2)θ
where s′ not a variable, L is not an equality, θ = mgu(s, s′), tθ ̸≻sθ and l′θ ̸≻l[s′]θ
Equality Resolution and Equality Factoring
s ̸≃s′ ∨C
Cθ
s ≃t ∨s′ ≃t′ ∨C
(s ≃t ∨t ̸≃t′ ∨C)θ
where θ = mgu(s, s′), tθ ̸≻sθ and t′θ ̸≻tθ
Fig. 1. The superposition calculus Sup.
3
Superposition-Based Proof Search
We now overview the main ingredients in organizing proof search within ﬁrst-
order provers, using the superposition calculus. For details, we refer to [2,10,11].
Superposition-based provers use saturation algorithms: applying all possible
inferences of Sup in a certain order to the clauses in the search space until (i) no
more inferences can be applied or (ii) the empty clause has been derived. A simple
implementation of a saturation algorithm would however be very ineﬃcient as
applications of all possible inferences will quickly blow up the search space.
Saturation algorithms can however be made eﬃcient by exploiting a power-
ful concept of redundancy: deleting so-called redundant clauses from the search
space by preserving completeness of Sup. A clause C in a set S of clauses (i.e., in
the search space) is redundant in S, if there exist clauses C1, . . . , Cn in S, such
that C ≻Ci and C1, . . . , Cn ⊨C. That is, a clause C is redundant in S if it is a
logical consequence of clauses that are smaller than C w.r.t. ≻. It is known that

Subsumption Demodulation in First-Order Theorem Proving
303
redundant clauses can be removed from the search space without aﬀecting com-
pleteness of superposition-based proof search. For this reason, saturation-based
theorem provers, such as E, Spass and Vampire, not only generate new clauses
but also delete redundant clauses during proof search by using both generating
and simplifying inferences.
Simpliﬁcation Rules. A simplifying inference is an inference in which one
premise Ci becomes redundant after the addition of the conclusion C to the
search space, and hence Ci can be deleted. In what follows, we will denote deleted
clauses by drawing a line through them and refer to simplifying inferences as
simpliﬁcation rules. The premise Ci that becomes redundant is called the main
premise, whereas other premises are called side premises of the simpliﬁcation
rule. Intuitively, a simpliﬁcation rule simpliﬁes its main premise to its conclusion
by using additional knowledge from its side premises. Inferences that are not
simplifying are called generating, as they generate and add a new clause C to
the search space.
In saturation-based proof search, we distinguish between forward and back-
ward simpliﬁcations. During forward simpliﬁcation, a newly derived clause is
simpliﬁed using previously derived clauses as side clauses. Conversely, during
backward simpliﬁcation a newly derived clause is used as a side clause to sim-
plify previously derived clauses.
Demodulation. One example of a simpliﬁcation rule is demodulation, or also
called rewriting by unit equalities. Demodulation is the following inference rule:
l ≃r

L[t] ∨C
L[rσ] ∨C
where lσ = t, lσ ≻rσ and L[t] ∨C ≻(l ≃r)σ, for some substitution σ.
It is easy to see that demodulation is a simpliﬁcation rule. Moreover, demod-
ulation is a special case of a superposition inference where one premise of the
inference is deleted. However, unlike a superposition inference, demodulation is
not restricted to selected literals.
Example 3. Consider the clauses C1 = f(f(x)) ≃f(x) and C2 = P(f(f(c))) ∨
Q(d). Let σ be the substitution σ = {x 
→c}. By the subterm property of
≻, we have f(f(c)) ≻f(c). Further, as equality literals are smaller than non-
equality literals, we have P(f(f(c))) ∨Q(d) ≻f(f(c)) ≃f(c). We thus apply
demodulation and C2 is simpliﬁed into the clause C3 = P(f(c)) ∨Q(d):
f(f(x)) ≃f(x)
((((((((
(
P(f(f(c))) ∨Q(d)
P(f(c)) ∨Q(d)
⊓⊔
Deletion Rules. Even when simpliﬁcation rules are in use, deleting more/other
redundant clauses is still useful to keep the search space small. For this reason,
in addition to simplifying and generating rules, theorem provers also use deletion
rules: a deletion rule checks whether clauses in the search space are redundant

304
B. Gleiss et al.
due to the presence of other clauses in the search space, and removes redundant
clauses from the search space.
Given clauses C and D, we say C subsumes D if there is some substitution
σ such that Cσ is a submultiset of D, that is Cσ ⊆M D. Subsumption is the
deletion rule that removes subsumed clauses from the search space.
Example 4. Let C = P(x) ∨Q(f(x)) and D = P(f(c)) ∨P(g(c)) ∨Q(f(c)) ∨
Q(f(g(c))) ∨R(y) be clauses in the search space. Using σ = {x 
→g(c)}, it
is easy to see that C subsumes D, and hence D is deleted from the search
space.
⊓⊔
4
Subsumption Demodulation
In this section we introduce a new simpliﬁcation rule, called subsumption demod-
ulation, by extending demodulation to a simpliﬁcation rule over conditional
equalities. We do so by combining demodulation with subsumption checks to
ﬁnd simplifying applications of rewriting by non-unit (and hence conditional)
equalities.
4.1
Subsumption Demodulation for Conditional Rewriting
Our rule of subsumption demodulation is deﬁned below.
Deﬁnition 1 (Subsumption Demodulation).
Subsumption demodulation
is the inference rule:
l ≃r ∨C
L[t] ∨D
L[rσ] ∨D
(6)
where:
1. lσ = t,
2. Cσ ⊆M D,
3. lσ ≻rσ, and
4. L[t] ∨D ≻(l ≃r)σ ∨Cσ.
We call the equality l ≃r in the left premise of (6) the rewriting equality of
subsumption demodulation.
Intuitively, the side conditions 1 and 2 of Deﬁnition 1 ensure the soundness
of the rule: it is easy to see that if l ≃r ∨C and L[t]∨D are true, then L[rσ]∨D
also holds. We thus conclude:
Theorem 1 (Soundness). Subsumption demodulation is sound.
On the other hand, side conditions 3 and 4 of Deﬁnition 1 are vital to
ensure that subsumption demodulation is a simpliﬁcation rule (details follow
in Sect. 4.2).

Subsumption Demodulation in First-Order Theorem Proving
305
Detecting possible applications of subsumption demodulation involves (i)
selecting one equality of the side clause as rewriting equality and (ii) match-
ing each of the remaining literals, denoted C in (6), to some literal in the main
clause. Step (i) is similar to ﬁnding unit equalities in demodulation, whereas step
(ii) reduces to showing that C subsumes parts of the main premise. Informally
speaking, subsumption demodulation combines demodulation and subsumption,
as discussed in Sect. 5. Note that in step (ii), matching allows any instantiation of
C to Cσ via substitution σ; yet, we we do not unify the side and main premises
of subsumption demodulation, as illustrated later in Example 7. Furthermore,
we need to ﬁnd a term t in the unmatched part D \ Cσ of the main premise,
such that t can be rewritten according to the rewriting equality into rσ.
As the ordering ≻is partial, the conditions of Deﬁnition 1 must be checked a
posteriori, that is after subsumption demodulation has been applied with a ﬁxed
substitution. Note however that if l ≻r in the rewriting equality, then lσ ≻rσ
for any substitution, so checking the ordering a priori helps, as illustrated in the
following example.
Example 5. Let us consider the following two clauses:
C1 = f(g(x)) ≃g(x) ∨Q(x) ∨R(y)
C2 = P(f(g(c))) ∨Q(c) ∨Q(d) ∨R(f(g(d)))
By the subterm property of ≻, we conclude that f(g(x)) ≻g(x). Hence, the
rewriting equality, as well as any instance of it, is oriented.
Let σ be the substitution σ = {x 
→c, y 
→f(g(d))}. Due to the previ-
ous paragraph, we know f(g(c)) ≻g(c) As equality literals are smaller than
non-equality ones, we also conclude P(f(g(c))) ≻f(g(c)) ≃g(c). Thus, we have
P(f(g(c))) ∨Q(c) ∨Q(d) ∨R(f(g(d)))
≻
f(g(c)) ≃g(c) ∨Q(c) ∨R(f(g(d)))
and we can apply subsumption demodulation to C1 and C2, deriving clause
C3 = P(g(c)) ∨Q(c) ∨Q(d) ∨R(f(g(d))).
We note that demodulation cannot derive C3 from C1 and C2, as there is no
unit equality.
⊓⊔
Example 5 highlights limitations of demodulation when compared to sub-
sumption demodulation. We next illustrate diﬀerent possible applications of sub-
sumption demodulation using a ﬁxed side premise and diﬀerent main premises.
Example 6. Consider the clause C1 = f(g(x)) ≃g(y) ∨Q(x) ∨R(y). Only the
ﬁrst literal f(g(x)) ≃g(y) is a positive equality and as such eligible as rewriting
equality. Note that f(g(x)) and g(y) are incomparable w.r.t. ≻due to occur-
rences of diﬀerent variables, and hence whether f(g(x))σ ≻g(y)σ depends on
the chosen substitution σ.
(1) Consider the clause C2 = P(f(g(c))) ∨Q(c) ∨R(c) as the main premise.
With the substitution σ1 = {x 
→c, y 
→c}, we have f(g(x))σ1 ≻g(x)σ1
as f(g(c)) ≻g(c) due to the subterm property of ≻, enabling a possible
application of subsumption demodulation over C1 and C2.

306
B. Gleiss et al.
(2) Consider now C3 = P(g(f(g(c))))∨Q(c)∨R(f(g(c))) as the main premise and
the substitution σ2 = {x 
→c, y 
→f(g(c))}. We have g(y)σ2 ≻f(g(x))σ2,
as g(f(g(c)) ≻f(g(c)). The instance of the rewriting equality is oriented dif-
ferently in this case than in the previous one, enabling a possible application
of subsumption demodulation over C1 and C3.
(3) On the other hand, using the clause C4 = P(f(g(c))) ∨Q(c) ∨R(z) as the
main premise, the only substitution we can use is σ3 = {x 
→c, y 
→z}. The
corresponding instance of the rewriting equality is then f(g(c)) ≃g(z), which
cannot be oriented in general. Hence, subsumption demodulation cannot be
applied in this case, even though we can ﬁnd the matching term f(g(c))
in C4.
⊓⊔
As mentioned before, the substitution σ appearing in subsumption demodu-
lation can only be used to instantiate the side premise, but not for unifying side
and main premises, as we would not obtain a simpliﬁcation rule.
Example 7. Consider the clauses:
C1 = f(c) ≃c ∨Q(d)
C2 = P(f(c)) ∨Q(x)
As we cannot match Q(d) to Q(x) (although we could match Q(x) to Q(d)),
subsumption demodulation is not applicable with premises C1 and C2.
⊓⊔
4.2
Simpliﬁcation Using Subsumption Demodulation
Note that in the special case where C is the empty clause in (6), subsumption
demodulation reduces to demodulation and hence it is a simpliﬁcation rule. We
next show that this is the case in general:
Theorem 2 (Simpliﬁcation Rule).
Subsumption demodulation is a simpli-
ﬁcation rule and we have:
l ≃r ∨C

L[t] ∨D
L[rσ] ∨D
where:
1. lσ = t,
2. Cσ ⊆M D,
3. lσ ≻rσ, and
4. L[t] ∨D ≻(l ≃r)σ ∨Cσ.
Proof. Because of the second condition of the deﬁnition of subsumption demod-
ulation, L[t] ∨D is clearly a logical consequence of L[rσ] ∨D and l ≃r ∨C.
Moreover, from the fourth condition, we trivially have L[t] ∨D ≻(l ≃r)σ ∨Cσ.
It thus remains to show that L[rσ] ∨D is smaller than L[t] ∨D w.r.t. ≻. As

Subsumption Demodulation in First-Order Theorem Proving
307
t = lσ ≻rσ, the monotonicity property of ≻asserts that L[t] ≻L[rσ], and
hence L[t] ∨D ≻L[rσ] ∨D. This concludes that L[t] ∨D is redundant w.r.t. the
conclusion and left-most premise of subsumption demodulation.
⊓⊔
Example 8. By revisiting Example 5, Theorem 2 asserts that clause C2 is sim-
pliﬁed into C3, and subsumption demodulation deletes C2 from the search
space.
⊓⊔
4.3
Reﬁning Redundancy
The fourth condition deﬁning subsumption demodulation in Deﬁnition 1 is
required to ensure that the main premise of subsumption demodulation becomes
redundant. However, comparing clauses w.r.t. the ordering ≻is computationally
expensive; yet, not necessary for subsumption demodulation. Following the nota-
tion of Deﬁnition 1, let D′ such that D = Cσ∨D′. By properties of multiset order-
ings, the condition L[t]∨D ≻(l ≃r)σ∨Cσ is equivalent to L[t]∨D′ ≻(l ≃r)σ, as
the literals in Cσ occur on both sides of ≻. This means, to ensure the redundancy
of the main premise of subsumption demodulation, we only need to ensure that
there is a literal from L[t] ∨D such that this literal is bigger that the rewriting
equality.
Theorem 3 (Reﬁning Redundancy). The following conditions are equiva-
lent:
(R1) L[t] ∨D ≻(l ≃r)σ ∨Cσ
(R2) L[t] ∨D′ ≻(l ≃r)σ
As mentioned in Sect. 4.1, application of subsumption demodulation involves
checking that an ordering condition between premises holds (side condition 4 in
Deﬁnition 1). Theorem 3 asserts that we only need to ﬁnd a literal in L[t] ∨D′
that is bigger than the rewriting equality in order to ensure that the ordering
condition is fulﬁlled. In the next section we show that by re-using and prop-
erly changing the underlying machinery of ﬁrst-order provers for demodulation
and subsumption, subsumption demodulation can eﬃciently be implemented in
superposition-based proof search.
5
Subsumption Demodulation in Vampire
We implemented subsumption demodulation in the ﬁrst-order theorem prover
Vampire. Our implementation consists of about 5000 lines of C++ code and is
available at:
https://github.com/vprover/vampire/tree/subsumption-demodulation

308
B. Gleiss et al.
As for any simpliﬁcation rule, we implemented the forward and backward ver-
sions of subsumption demodulation separately. Our new Vampire options con-
trolling subsumption demodulation are fsd and bsd, both with possible val-
ues on and off, to respectively enable forward and backward subsumption
demodulation.
As discussed in Sect. 4, subsumption demodulation uses reasoning based on
a combination of demodulation and subsumption. Algorithm 1 details our imple-
mentation for forward subsumption demodulation. In a nutshell, given a clause
D as main premise, (forward) subsumption demodulation in Vampire consists
of the following main steps:
1. Retrieve candidate clauses C as side premises of subsumption demodulation
(line 1 of Algorithm 1). To this end, we design a new clause index with imper-
fect ﬁltering, by modifying the subsumption index in Vampire, as discussed
later in this section.
2. Prune candidate clauses by checking the conditions of subsumption demod-
ulation (lines 3–7 of Algorithm 1), in particular selecting a rewriting equality
and matching the remaining literals of the side premise to literals of the main
premise. After this, prune further by performing a posteriori checks for ori-
enting the rewriting equality E, and checking the redundancy of the given
main premise D. To do so, we revised multi-literal matching and redundancy
checking in Vampire (see later).
3. Build simpliﬁed clause by simplifying and deleting the (main) premise D
of subsumption demodulation using (forward) simpliﬁcation (line 8 of Algo-
rithm 1).
Our implementation of backward subsumption demodulation requires only
a few changes to Algorithm 1: (i) we use the input clause as side premise C
of backward subsumption demodulation and (ii) we retrieve candidate clauses
D as potential main premises of subsumption demodulation. Additionally, (iii)
instead of returning a single simpliﬁed clause D′, we record a replacement clause
for each candidate clause D where a simpliﬁcation was possible.
Clause Indexing for Subsumption Demodulation. We build upon the
indexing approach [15] used for subsumption in Vampire: the subsumption index
in Vampire stores and retrieves candidate clauses for subsumption. Each clause
is indexed by exactly one of its literals. In principle, any literal of the clause can
be chosen. In order to reduce the number of retrieved candidates, the best literal
is chosen in the sense that the chosen literal maximizes a certain heuristic (e.g.,
maximal weight). Since the subsumption index is not a perfect index (i.e., it
may retrieve non-subsumed clauses), additional checks on the retrieved clauses
are performed.
Using the subsumption index of Vampire as the clause index for forward sub-
sumption demodulation would however omit retrieving clauses (side premises) in
which the rewriting equality is chosen as key for the index, omitting this way a
possible application of subsumption demodulation. Hence, we need a new clause
index in which the best literal can be adjusted to be the rewriting equality. To

Subsumption Demodulation in First-Order Theorem Proving
309
Algorithm 1. Forward Subsumption Demodulation – FSD
Input
: Clause D, to be used as main premise
Output: Simpliﬁed clause D′ if (forward) subsumption demodulation is possible
// Retrieve candidate side premises
1 candidates := FSDIndex.Retrieve(D)
2 for each C ∈candidates do
3
while m = FindNextMLMatch(C, D) do
4
σ′ := m.GetSubstitution()
5
E := m.GetRewritingEquality()
// E is of the form l ≃r, for some terms l, r
6
if exists term t in D \ Cσ′ and substitution σ ⊇σ′ s.t. t = lσ then
7
if CheckOrderingConditions(D, E, t, σ) then
8
D′ := BuildSimpliﬁedClause(D, E, t, σ)
9
return D′
10
end
11
end
12
end
13 end
address this issue, we added a new clause index, called the forward subsumption
demodulation index (FSD index), to Vampire, as follows: we index potential
side premises either by their best literal (according to the heuristic), the second
best literal, or both. If the best literal in a clause C is a positive equality (i.e.,
a candidate rewriting equality) but the second best is not, C is indexed by the
second best literal, and vice versa. If both the best and second best literal are
positive equalities, C is indexed by both of them. Furthermore, because the FSD
index is exclusively used by forward subsumption demodulation, this index only
needs to keep track of clauses that contain at least one positive equality.
In the backward case, we can in fact reuse Vampire’s index for backward
subsumption. Instead we need to query the index by the best literal, the second
best literal, or both (as described in the previous paragraph).
Multi-literal Matching. Similarly to the subsumption index, our new sub-
sumption demodulation index is not a perfect index, that is it performs imperfect
ﬁltering for retrieving clauses. Therefore, additional post-checks are required on
the retrieved clauses. In our work, we devised a multi-literal matching approach
to:
– choose the rewriting equality among the literals of the side premise C, and
– check whether the remaining literals of C can be uniformly instantiated to
the literals of the main premise D of subsumption demodulation.
There are multiple ways to organize this process. A simple approach is to
(i) ﬁrst pick any equality of a side premise C as the rewriting equality of sub-
sumption demodulation, and then (ii) invoke the existing multi-literal matching
machinery of Vampire to match the remaining literals of C with a subset of

310
B. Gleiss et al.
literals of D. For the latter step (ii), the task is to ﬁnd a substitution σ such that
Cσ becomes a submultiset of the given clause D. If the choice of the rewriting
equality in step (i) turns out to be wrong, we backtrack. In our work, we revised
the existing multi-literal matching machinery of Vampire to a new multi-literal
matching approach for subsumption demodulation, by using the steps (i)-(ii)
and interleaving equality selection with matching.
We note that the substitution σ in step (ii) above is built in two stages: ﬁrst
we get a partial substitution σ′ from multi-literal matching and then (possibly)
extend σ′ to σ by matching term instances of the rewriting equality with terms
of D \ Cσ.
Example 9. Let D be the clause P(f(c, d))∨Q(c). Assume that our (FSD) clause
index retrieves the clause C = f(x, y) ≃y ∨Q(x) from the search space (line 1 of
Algorithm 1). We then invoke our multi-literal matcher (line 3 of Algorithm 1),
which matches the literal Q(x) of C to the literal Q(c) of D and selects the
equality literal f(x, y) ≃y of C as the rewriting equality for subsumption demod-
ulation over C and D. The matcher returns the choice of rewriting equality
and the partial substitution σ′ = {x 
→c}. We arrive at the ﬁnal substitu-
tion σ = {x 
→c, y 
→d} only when we match the instance f(x, y)σ′, that is
f(c, y), of the left-hand side of the rewriting equality to the literal f(c, d) of D.
Using σ, subsumption demodulation over C and D will derive P(d)∨Q(c), after
ensuring that D becomes redundant (line 8 of Algorithm 1).
⊓⊔
We further note that multi-literal matching is an NP-complete problem. Our
multi-literal matching problems may have more than one solution, with possi-
bly only some (or none) of them leading to successful applications of subsump-
tion demodulation. In our implementation, we examine all solutions retrieved
by multi-literal matching. We also experimented with limiting the number of
matches examined after multi-literal matching but did not observe relevant
improvements. Yet, our implementation in Vampire also supports an additional
option allowing the user to specify an upper bound on how many solutions of
multi-literal matching should be examined.
Redundancy Checking. To ensure redundancy of the main premise D after the
subsumption demodulation inference, we need to check two properties. First, the
instance Eσ of the rewriting equality E must be oriented. This is a simple order-
ing check. Second, the main premise D must be larger than the side premise C.
Thanks to Theorem 3, this latter condition is reduced to ﬁnding a literal among
the unmatched part of the main premise D that is bigger than the instance Eσ
of the rewriting equality E.
Example 10. In case of Example 9, the rewriting equality E is oriented and hence
Eσ is also oriented. Next, the literal P(f(c, d)) is bigger than Eσ, and hence D
is redundant w.r.t. C and D′.
⊓⊔

Subsumption Demodulation in First-Order Theorem Proving
311
6
Experiments
We evaluated our implementation of subsumption demodulation in Vampire
on the problems of the TPTP [17] (version 7.3.0) and SMT-LIB [4] (release
2019-05-06) repositories. All our experiments were carried out on the StarExec
cluster [16].
Benchmark Setup. From the 22,686 problems in the TPTP benchmark set,
Vampire can parse 18,232 problems.3 Out of these problems, we only used those
problems that involve equalities as subsumption demodulation is only applicable
in the presence of (at least one) equality. As such, we used 13,924 TPTP problems
in our experiments.
On the other hand, when using the SMT-LIB repository, we chose the bench-
marks from categories LIA, UF, UFDT, UFDTLIA, and UFLIA, as these bench-
marks involve reasoning with both theories and quantiﬁers and the background
theories are the theories that Vampire supports. These are 22,951 SMT-LIB
problems in total, of which 22,833 problems remain after removing those where
equality does not occur.
Comparative Experiments with Vampire. As a ﬁrst experimental study,
we compared the performance of subsumption demodulation in Vampire for
diﬀerent values of fsd and bsd, that is by using forward (FSD) and/or back-
ward (BSD) subsumption demodulation. To this end, we evaluated subsumption
demodulation using the CASC and SMTCOMP schedules of Vampire’s portfo-
lio mode. In order to test subsumption demodulation with the portfolio mode,
we added the options fsd and/or bsd to all strategies of Vampire. While the
resulting strategy schedules could potentially be further improved, it allowed us
to test FSD/BSD with a variety of strategies.
Table 1. Comparing Vampire with and without subsumption demodulation on TPTP,
using Vampire in portfolio mode.
Conﬁguration
Total
Solved New (SAT + UNSAT)
Vampire
13,924 9,923
–
Vampire, with FSD
13,924 9,757
20 (3 + 17)
Vampire, with BSD
13,924 9,797
14 (2 + 12)
Vampire, with FSD and BSD 13,924 9,734
30 (6 + 24)
Our results are summarized in Tables 1 and 2. The ﬁrst column of these tables
lists the Vampire version and conﬁguration, where Vampire refers to Vampire
in its portfolio mode (version 4.4). Lines 2–4 of these tables use our new Vam-
pire, that is our implementation of subsumption demodulation in Vampire. The
3 The other problems contain features, such as higher-order logic, that have not been
implemented in Vampire yet.

312
B. Gleiss et al.
Table 2. Comparing Vampire with and without subsumption demodulation on SMT-
LIB, using Vampire in portfolio mode.
Conﬁguration
Total
Solved New (SAT + UNSAT)
Vampire
22,833 13,705 –
Vampire, with FSD
22,833 13,620 55 (1 + 54)
Vampire, with BSD
22,833 13,632 48 (0 + 48)
Vampire, with FSD and BSD 22,833 13,607 76 (0 + 76)
column “Solved” reports, respectively, the total number of TPTP and SMT-LIB
problems solved by the considered Vampire conﬁgurations. Column “New” lists,
respectively, the number of TPTP and SMT-LIB problems solved by the version
with subsumption demodulation but not by the portfolio version of Vampire.
This column also indicates in parentheses how many of the solved problems were
satisﬁable/unsatisﬁable.
While in total the portfolio mode of Vampire can solve more problems, we
note that this comes at no surprise as the portfolio mode of Vampire is highly
tuned using the existing Vampire options. In our experiments, we were inter-
ested to see whether subsumption demodulation in Vampire can solve problems
that cannot be solved by the portfolio mode of Vampire. Such a result would
justify the existence of the new rule because the set of problems that Vampire
can solve in principle is increased. In future work, the portfolio mode should be
tuned by also taking into account subsumption demodulation, which then ideally
leads to an overall increase in performance. The columns “New” of Tables 1 and
2 give indeed practical evidence of the impact of subsumption demodulation:
there are 30 new TPTP problems and 76 SMT-LIB problems4 that the portfo-
lio version of Vampire cannot solve, but forward and backward subsumption
demodulation in Vampire can.
New Problems Solved Only by Subsumption Demodulation. Building
upon our results from Tables 1 and 2, we analysed how many new problems
subsumption demodulation in Vampire can solve when compared to other state-
of-the-art reasoners. To this end, we evaluated our work against the superposition
provers E (version 2.4) and Spass (version 3.9), as well as the SMT solvers
CVC4 (version 1.7) and Z3 (version 4.8.7). We note however, that when using
our 30 new problems from Table 1, we could not compare our results against Z3
as Z3 does not natively parse TPTP. On the other hand, when using our 76
new problems from Table 2, we only compared against CVC4 and Z3, as E and
Spass do not support the SMT-LIB syntax.
Table 3 summarizes our ﬁndings. First, 11 of our 30 “new” TPTP problems
can only be solved using forward and backward subsumption demodulation in
Vampire; none of the other systems were able solve these problems.
4 The list of these new problems is available at https://gist.github.com/JakobR/605a7b7db010
1259052e137ade54b32c.

Subsumption Demodulation in First-Order Theorem Proving
313
Table 3. Comparing Vampire with subsumption demodulation against other solvers,
using the “new” TPTP and SMT-LIB problems of Tables 1 and 2 and running Vampire
in portfolio mode.
Solver/conﬁguration
TPTP problems SMT-LIB problems
Baseline: Vampire, with FSD and BSD
30
76
E with --auto-schedule
14
–
Spass (default)
4
–
Spass (local contextual rewriting)
6
–
Spass (subterm contextual rewriting)
5
–
CVC4 (default)
7
66
Z3 (default)
–
49
Only solved by Vampire, with FSD and BSD 11
0
Second, while all our 76 “new” SMT-LIB problems can also be solved by
CVC4 and Z3 together, we note that out of these 76 problems there are 10
problems that CVC4 cannot solve, and similarly 27 problems that Z3 cannot
solve.
Comparative Experiments without AVATAR. Finally, we investigated the
eﬀect of subsumption demodulation in Vampire without AVATAR [19]. We used
the default mode of Vampire (that is, without using a portfolio approach) and
turned oﬀthe AVATAR setting. While this conﬁguration solves less problems
than the portfolio mode of Vampire, so far Vampire is the only superposition-
based theorem prover implementing AVATAR. Hence, evaluating subsumption
demodulation in Vampire without AVATAR is more relevant to other reasoners.
Further, as AVATAR may often split non-unit clauses into unit clauses, it may
potentially simulate applications of subsumption demodulation using demodula-
tion. Table 4 shows that this is indeed the case: with both fsd and bsd enabled,
subsumption demodulation in Vampire can prove 190 TPTP problems and 173
SMT-LIB examples that the default Vampire without AVATAR cannot solve.
Again, the column “New” denotes the number of problems solved by the respec-
tive conﬁguration but not by the default mode of Vampire without AVATAR.
Table 4. Comparing Vampire in default mode and without AVATAR, with and with-
out subsumption demodulation.
TPTP problems
SMT-LIB problems
Conﬁguration
Total
Solved New
(SAT + UNSAT)
Total
Solved New
(SAT + UNSAT)
Vampire
13,924 6,601
–
22,833 9,608
–
Vampire (FSD)
13,924 6,539
152 (13 + 139)
22,833 9,597
134 (1 + 133)
Vampire (BSD)
13,924 6,471
112 (12 + 100)
22,833 9,541
87 (0 + 87)
Vampire (FSD + BSD) 13,924 6,510
190 (15 + 175)
22,833 9,581
173 (1 + 172)

314
B. Gleiss et al.
7
Conclusion
We introduced the simplifying inference rule subsumption demodulation to
improve support for reasoning with conditional equalities in superposition-
based ﬁrst-order theorem proving. Subsumption demodulation revises existing
machineries of superposition provers and can therefore be eﬃciently integrated
in superposition reasoning. Still, the rule remains expensive and does not pay
oﬀfor all problems, leading to a decrease in total number of solved problems by
our implementation in Vampire. However, this is justiﬁed because subsumption
demodulation also solves many new examples that existing provers, including
ﬁrst-order and SMT solvers, cannot handle. Future work includes the design of
more sophisticated approaches for selecting rewriting equalities and improving
the imperfect ﬁltering of clauses indexes.
Acknowledgements. This work was funded by the ERC Starting Grant 2014 SYM-
CAR 639270, the ERC Proof of Concept Grant 2018 SYMELS 842066, the Wallenberg
Academy Fellowship 2014 TheProSE, the Austrian FWF research project W1255-N23,
the OMAA grant 101¨ou8, and the WWTF ICT15-193 grant.
References
1. Bachmair, L., Ganzinger, H.: Rewrite-based equational theorem proving with selec-
tion and simpliﬁcation. J. Logic Comput. 4(3), 217–247 (1994)
2. Bachmair, L., Ganzinger, H., McAllester, D.A., Lynch, C.: Resolution theorem
proving. In: Handbook of Automated Reasoning, pp. 19–99 (2001)
3. Barrett, C., et al.: CVC4. In: Gopalakrishnan, G., Qadeer, S. (eds.) CAV 2011.
LNCS, vol. 6806, pp. 171–177. Springer, Heidelberg (2011). https://doi.org/10.
1007/978-3-642-22110-1 14
4. Barrett, C., Fontaine, P., Tinelli, C.: The Satisﬁability Modulo Theories Library
(SMT-LIB) (2016). www.SMT-LIB.org
5. Barthe, G., Eilers, R., Georgiou, P., Gleiss, B., Kov´acs, L., Maﬀei, M.: Verifying
relational properties using trace logic. In: Proceedings of FMCAD, pp. 170–178
(2019)
6. Bjørner, N., Gurﬁnkel, A., McMillan, K.L., Rybalchenko, A.: Horn clause solvers
for program veriﬁcation. Fields Logic Comput. II, 24–51 (2015)
7. de Moura, L., Bjørner, N.: Z3: an eﬃcient SMT solver. In: Ramakrishnan, C.R.,
Rehof, J. (eds.) TACAS 2008. LNCS, vol. 4963, pp. 337–340. Springer, Heidelberg
(2008). https://doi.org/10.1007/978-3-540-78800-3 24
8. Ganzinger, H., Hagen, G., Nieuwenhuis, R., Oliveras, A., Tinelli, C.: DPLL(T):
fast decision procedures. In: Alur, R., Peled, D.A. (eds.) CAV 2004. LNCS, vol.
3114, pp. 175–188. Springer, Heidelberg (2004). https://doi.org/10.1007/978-3-
540-27813-9 14
9. Hillenbrand, T., Piskac, R., Waldmann, U., Weidenbach, C.: From search to com-
putation: redundancy criteria and simpliﬁcation at work. In: Voronkov, A., Wei-
denbach, C. (eds.) Programming Logics: Essays in Memory of Harald Ganzinger,
vol. 7797, pp. 169–193. Springer, Heidelberg (2013). https://doi.org/10.1007/978-
3-642-37651-1 7

Subsumption Demodulation in First-Order Theorem Proving
315
10. Kov´acs, L., Voronkov, A.: First-order theorem proving and Vampire. In:
Sharygina, N., Veith, H. (eds.) CAV 2013. LNCS, vol. 8044, pp. 1–35. Springer,
Heidelberg (2013). https://doi.org/10.1007/978-3-642-39799-8 1
11. Nieuwenhuis, R., Rubio, A.: Paramodulation-based theorem proving. In: Handbook
of Automated Reasoning, pp. 371–443 (2001)
12. Nivela, P., Nieuwenhuis, R.: Saturation of ﬁrst-order (constrained) clauses with the
saturate system. In: Kirchner, C. (ed.) Rewriting Techniques and Applications, vol.
690, pp. 436–440. Springer, Heidelberg (1993). https://doi.org/10.1007/978-3-662-
21551-7 33
13. Reynolds, A., Woo, M., Barrett, C., Brumley, D., Liang, T., Tinelli, C.: Scaling
up DPLL(T) string solvers using context-dependent simpliﬁcation. In: Majumdar,
R., Kunˇcak, V. (eds.) CAV 2017. LNCS, vol. 10427, pp. 453–474. Springer, Cham
(2017). https://doi.org/10.1007/978-3-319-63390-9 24
14. Schulz, S., Cruanes, S., Vukmirovi´c, P.: Faster, higher, stronger: E 2.3. In: Fontaine,
P. (ed.) CADE 2019. LNCS (LNAI), vol. 11716, pp. 495–507. Springer, Cham
(2019). https://doi.org/10.1007/978-3-030-29436-6 29
15. Sekar, R., Ramakrishnan, I.V., Voronkov, A.: Term indexing. In: Robinson, J.A.,
Voronkov, A. (eds.) Handbook of Automated Reasoning, pp. 1853–1964. Elsevier
Science Publishers B.V. (2001)
16. Stump, A., Sutcliﬀe, G., Tinelli, C.: StarExec: a cross-community infrastructure
for logic solving. In: Proceedings of IJCAR, pp. 367–373 (2014)
17. Sutcliﬀe, G.: The TPTP problem library and associated infrastructure. From CNF
to TH0, TPTP v6.4.0. J. Autom. Reasoning 59(4), 483–502 (2017)
18. Tange, O.: GNU Parallel 2018. Ole Tange (2018)
19. Voronkov, A.: AVATAR: the architecture for ﬁrst-order theorem provers. In: Biere,
A., Bloem, R. (eds.) CAV 2014. LNCS, vol. 8559, pp. 696–710. Springer, Cham
(2014). https://doi.org/10.1007/978-3-319-08867-9 46
20. Weidenbach, C.: Combining superposition, sorts and splitting. In: Handbook of
Automated Reasoning, pp. 1965–2013 (2001)
21. Weidenbach, C., Dimova, D., Fietzke, A., Kumar, R., Suda, M., Wischnewski,
P.: SPASS version 3.5. In: Schmidt, R.A. (ed.) CADE 2009. LNCS (LNAI), vol.
5663, pp. 140–145. Springer, Heidelberg (2009). https://doi.org/10.1007/978-3-
642-02959-2 10
22. Weidenbach, C., Wischnewski, P.: Contextual rewriting in SPASS. In: Proceedings
of PAAR (2008)

A Comprehensive Framework
for Saturation Theorem Proving
Uwe Waldmann1(B)
, Sophie Tourret1
, Simon Robillard2
,
and Jasmin Blanchette1,3,4
1 Max-Planck-Institut f¨ur Informatik, Saarland Informatics Campus,
Saarbr¨ucken, Germany
{uwe,stourret,jblanche}@mpi-inf.mpg.de
2 IMT Atlantique, Nantes, France
simon.robillard@imt-atlantique.fr
3 Vrije Universiteit Amsterdam, Amsterdam, The Netherlands
4 Universit´e de Lorraine, CNRS, Inria, LORIA, Nancy, France
Abstract. We present a framework for formal refutational completeness
proofs of abstract provers that implement saturation calculi, such as
ordered resolution or superposition. The framework relies on modular
extensions
of
lifted
redundancy
criteria.
It
allows
us
to
extend
redundancy criteria so that they cover subsumption, and also to model
entire prover architectures in such a way that the static refutational
completeness of a calculus immediately implies the dynamic refuta-
tional completeness of a prover implementing the calculus, for instance
within an Otter or DISCOUNT loop. Our framework is mechanized in
Isabelle/HOL.
1
Introduction
In their Handbook chapter [5, Sect. 4], Bachmair and Ganzinger remark that
“unfortunately, comparatively little eﬀort has been devoted to a formal analysis
of redundancy and other fundamental concepts of theorem proving strategies,
while more emphasis has been placed on investigating the refutational com-
pleteness of a variety of modiﬁcations of inference rules, such as resolution.” As
a remedy, they present an abstract framework for saturation up to redundancy.
Brieﬂy, theorem proving derivations take the form N0▷N1▷· · · , where N0 is the
initial clause set and each step either adds inferred clauses or deletes redundant
clauses. Given a suitable notion of fairness, the limit N∗of a fair derivation is
saturated up to redundancy. If the calculus is refutationally complete and N∗
does not contain the false clause ⊥, then N0 has a model.
Bachmair and Ganzinger also deﬁne a concrete prover, RP, based on a ﬁrst-
order ordered resolution calculus and the given clause procedure. However, like
all realistic resolution provers, RP implements subsumption deletion. This opera-
tion is not covered by the standard deﬁnition of redundancy, according to which
a clause C is redundant w.r.t. a clause set N if all its ground instances Cθ
c
⃝Springer Nature Switzerland AG 2020
N. Peltier and V. Sofronie-Stokkermans (Eds.): IJCAR 2020, LNAI 12166, pp. 316–334, 2020.
https://doi.org/10.1007/978-3-030-51074-9_18

A Comprehensive Framework for Saturation Theorem Proving
317
are entailed by strictly smaller ground instances of clauses belonging to N.
As a result, RP-derivations are not ▷-derivations, and the framework is not
applicable.
There are two ways to address this problem. In the Handbook, Bachmair and
Ganzinger start from scratch and prove the dynamic refutational completeness of
RP by relating nonground derivations to ground derivations. This proof, though,
turns out to be rather nonmodular—it refers simultaneously to properties of
the calculus, to properties of the prover, and to the fairness of the derivations.
Extending it to other calculi or prover architectures would be costly. As a result,
most authors stop after proving static refutational completeness of their calculi.
An alternative approach is to extend the redundancy criterion so that sub-
sumed clauses become redundant. As demonstrated by Bachmair and Ganzinger
in 1990 [3], this is possible by redeﬁning redundancy in terms of closures (C, θ)
instead of ground instances Cθ. We show that this approach can be generalized
and modularized: First, any redundancy criterion that is obtained by lifting a
ground criterion can be extended to a redundancy criterion that supports sub-
sumption without aﬀecting static refutational completeness (Sect. 3). Second, by
applying this property to labeled formulas, it becomes possible to give generic
completeness proofs for prover architectures in a straightforward way.
Most saturation provers implement a variant of the given clause procedure.
We present an abstract version of the procedure (Sect. 4) that can be reﬁned to
obtain an Otter [18] or DISCOUNT [1] loop and prove it refutationally complete.
We also present a generalization that decouples scheduling and computation of
inferences, to support orphan deletion [16,25] and dovetailing [9].
When these prover architectures are instantiated with a concrete saturation
calculus, the dynamic refutational completeness of the combination follows in a
modular way from the properties of the prover architecture and the static refuta-
tional completeness proof for the calculus. Thus, the framework is applicable to
a wide range of calculi, including ordered resolution [5], unfailing completion [2],
standard superposition [4], constraint superposition [19], theory superposition
[28], hierarchic superposition [7], and clausal λ-superposition [9].
Detailed proofs are included in a technical report [29], together with more
explanations, examples, and discussions. When Schlichtkrull, Blanchette, Tray-
tel, and Waldmann [24] mechanized Bachmair and Ganzinger’s chapter using
the Isabelle/HOL proof assistant [21], they found quite a few mistakes, including
one that compromised RP’s dynamic refutational completeness. This motivated
us to mechanize our framework as well (Sect. 5).
2
Preliminaries
Inferences and Redundancy. Let A be a set. An A-sequence is a ﬁnite
sequence (ai)k
i=0 = a0, a1, . . . , ak or an inﬁnite sequence (ai)∞
i=0 = a0, a1, . . .
with ai ∈A for all i. We write (ai)i≥0 or (ai)i for both ﬁnite and inﬁnite
sequences. Nonempty sequences can be split into a head a0 and a tail (ai)i≥1.
Given ▷⊆A × A, a ▷-derivation is a nonempty A-sequence such that ai ▷ai+1
for all i.

318
U. Waldmann et al.
A set F of formulas is a set with a nonempty subset F⊥⊆F. Elements of
F⊥represent false. Typically, F⊥:= {⊥}. In Sect. 4, diﬀerent elements of F⊥
will represent diﬀerent situations in which a contradiction has been derived.
A consequence relation |= over F is a relation |= ⊆P(F) × P(F) with the
following properties for all N1, N2, N3 ⊆F:
(C1) {⊥} |= N1 for every ⊥∈F⊥;
(C2) N2 ⊆N1 implies N1 |= N2;
(C3) if N1 |= {C} for every C ∈N2, then N1 |= N2;
(C4) if N1 |= N2 and N2 |= N3, then N1 |= N3.
Consequence relations are used to discuss soundness (and the addition of
formulas) and to discuss refutational completeness (and the deletion of formulas).
An example that requires this distinction is constraint superposition [19], where
one uses entailment w.r.t. the set of all ground instances, |≈, for soundness, but
entailment w.r.t. a subset of those instances, |=, for refutational completeness.
Some calculus-dependent argument is then necessary to show that refutational
completeness w.r.t. |= implies refutational completeness w.r.t. |≈.
An F-inference ι is a tuple (Cn, . . . , C0) ∈Fn+1, n ≥0. The formulas
Cn, . . . , C1 are called premises of ι; C0 is called the conclusion of ι, denoted
by concl(ι). An F-inference system Inf is a set of F-inferences. If N ⊆F, we
write Inf(N) for the set of all inferences in Inf whose premises are contained
in N, and Inf(N, M) := Inf(N ∪M) \ Inf(N \ M) for the set of all inferences in
Inf such that one premise is in M and the other premises are contained in N ∪M.
A redundancy criterion for an inference system Inf and a consequence rela-
tion |= is a pair Red = (Red I, Red F), where Red I : P(F) →P(Inf) and
Red F : P(F) →P(F) are mappings that satisfy the following conditions for
all N, N ′:
(R1) if N |= {⊥} for some ⊥∈F⊥, then N \ Red F(N) |= {⊥};
(R2) if N ⊆N ′, then Red F(N) ⊆RedF(N ′) and RedI(N) ⊆RedI(N ′);
(R3) if N ′ ⊆Red F(N), then Red F(N) ⊆RedF(N \ N ′) and Red I(N) ⊆
Red I(N \ N ′);
(R4) if ι ∈Inf and concl(ι) ∈N, then ι ∈RedI(N).
Inferences in Red I(N) and formulas in Red F(N) are called redundant w.r.t. N.1
Intuitively, (R1) states that deleting redundant formulas preserves inconsistency.
(R2) and (R3) state that formulas or inferences that are redundant w.r.t. a set N
remain redundant if arbitrary formulas are added to N or redundant formulas are
deleted from N. (R4) ensures that computing an inference makes it redundant.
We deﬁne the relation
such that
if and only
if
.
1 One can ﬁnd several slightly diﬀering deﬁnitions for redundancy criteria, fairness, and
saturation in the literature [5,7,28]. However, as shown in the technical report [29],
the diﬀerences are typically insigniﬁcant as far as static or dynamic refutational
completeness is concerned. Here we mostly follow Waldmann [28].

A Comprehensive Framework for Saturation Theorem Proving
319
Refutational Completeness. Let |= be a consequence relation, let Inf be an
inference system, and let Red be a redundancy criterion for |= and Inf.
A set N ⊆F is called saturated w.r.t. Inf and Red if Inf(N) ⊆RedI(N).
The pair (Inf, Red) is called statically refutationally complete w.r.t. |= if for
every saturated set N ⊆F such that N |= {⊥} for some ⊥∈F⊥, there exists a
⊥′ ∈F⊥such that ⊥′ ∈N.
Let (Ni)i be a P(F)-sequence. Its limit is the set N∗:= 
i

j≥i Nj. Its union
is the set N∞:= 
i Ni. A sequence is called fair if Inf(N∗) ⊆
i Red I(Ni). The
pair (Inf, Red) is called dynamically refutationally complete w.r.t. |= if for every
fair
derivation (Ni)i such that N0 |= {⊥} for some ⊥∈F⊥, we have
⊥′ ∈Ni for some i and some ⊥′ ∈F⊥. Properties (R1)–(R3) allow the passage
from a static set of formulas to a dynamic prover:
Lemma 1. (Inf, Red) is dynamically refutationally complete w.r.t. |= if and only
if it is statically refutationally complete w.r.t. |=.
Intersections of Redundancy Criteria. In the sequel, it will be useful to
deﬁne consequence relations and redundancy criteria as intersections of previ-
ously deﬁned consequence relations or redundancy criteria.
Let Q be an arbitrary set, and let (|=q)q∈Q be a Q-indexed family of con-
sequence relations over F. Then |=∩:= 
q∈Q |=q qualiﬁes as a consequence
relation. Moreover, let Inf be an inference system, and let (Red q)q∈Q be a
Q-indexed family of redundancy criteria, where each Red q = (Red q
I , Red q
F) is
a redundancy criterion for Inf and |=q. Let Red ∩
I (N) := 
q∈Q Red q
I (N) and
Red∩
F(N) := 
q∈Q Red q
F(N). Then Red ∩:= (Red ∩
I , Red ∩
F) qualiﬁes as a redun-
dancy criterion for |=∩and Inf.
Lemma 2. A set N ⊆F is saturated w.r.t. Inf and Red∩if and only if it is
saturated w.r.t. Inf and Redq for every q ∈Q.
Often, the consequence relations |=q agree for all q ∈Q. For calculi where they
disagree, such as constraint superposition [19], one can typically demonstrate the
static refutational completeness of (Inf, Red ∩) in the following form:
Lemma 3. If for every set N ⊆F that is saturated w.r.t. Inf and Red∩and
does not contain any ⊥′ ∈F⊥there exists some q ∈Q such that N ̸|=q {⊥} for
some ⊥∈F⊥, then (Inf, Red ∩) is statically refutationally complete w.r.t. |=∩.
3
Lifting
A standard approach for establishing the refutational completeness of a calculus is
to ﬁrst concentrate on the ground case and then lift the results to the nonground
case. In this section, we show how to perform this lifting abstractly, given a suitable
grounding function G. The function maps every formula C ∈F to a set G(C) of
formulas from a set of formulas G. Depending on the logic and the calculus, G(C)

320
U. Waldmann et al.
maybe,forexample,thesetofallgroundinstancesofC orasubsetthereof.Similarly,
G maps FInf-inferences to sets of GInf-inferences.
TherearecalculiwheresomeFInf-inferencesιdonothaveacounterpartinGInf,
such as the PosExt inferences of higher-order superposition calculi [9]. In these
cases, we set G(ι) = undef .
StandardLifting.GiventwosetsofformulasFandG,anF-inferencesystemFInf,
a G-inference system GInf, and a redundancy criterion Red for GInf, let G be a
function that maps every formula in F to a subset of G and every F-inference in
FInf to undef or to a subset of GInf. G is called a grounding function if
(G1) for every ⊥∈F⊥, ∅̸= G(⊥) ⊆G⊥;
(G2) for every C ∈F, if ⊥∈G(C) and ⊥∈G⊥then C ∈F⊥;
(G3) for every ι ∈FInf, if G(ι) ̸= undef , then G(ι) ⊆Red I(G(concl(ι))).
G is extended to sets N ⊆F by deﬁning G(N) := 
C∈N G(C). Analogously, for a
set I ⊆FInf, G(I) := 
ι∈I, G(ι)̸=undef G(ι).
Example 4. In standard superposition, F is the set of all universally quantiﬁed
ﬁrst-orderclausesoversomesignatureΣ,Gisthesetofallgroundﬁrst-orderclauses
over Σ, and G maps every clause C to the set of its ground instances Cθ and every
superposition inference ι to the set of its ground instances ιθ.
Let G be a grounding function from F and FInf to G and GInf, and let |= be a
consequence relation over G. We deﬁne the relation |=G ⊆P(F) × P(F) such that
N1 |=G N2 if and only if G(N1) |= G(N2). We call |=G the G-lifting of |=. It qualiﬁes
as a consequence relation over F and corresponds to Herbrand entailment. If Tarski
entailment (i.e., N1 |=T N2 if and only if any model of N1 is also a model of N2) is
desired,themismatchcanberepairedbyshowingthatthetwonotionsofentailment
are equivalent as far as refutations are concerned.
Let Red = (Red I, Red F) be a redundancy criterion for |= and GInf. We deﬁne
functions RedG
I : P(F) →P(FInf) and Red G
F : P(F) →P(F) by
ι ∈Red G
I (N) if and only if
G(ι) ̸= undef and G(ι) ⊆Red I(G(N))
or G(ι) = undef and G(concl(ι)) ⊆G(N) ∪RedF(G(N));
C ∈Red G
F(N) if and only if
G(C) ⊆Red F(G(N)).
We call Red G := (Red G
I , Red G
F) the G-lifting of Red. It qualiﬁes as a redundancy
criterion for |=G and FInf. We get the following folklore theorem:
Theorem 5. If (GInf, Red) is statically refutationally complete w.r.t. |=, and if we
have GInf(G(N)) ⊆G(FInf(N)) ∪Red I(G(N)) for every N ⊆F that is saturated
w.r.t. FInf and RedG, then (FInf, Red G) is statically refutationally complete w.r.t.
|=G.
Adding Tiebreaker Orderings. We now strengthen the G-lifting of redundancy
criteria to also support subsumption deletion. Let ⊐be a well-founded strict partial
ordering on F. We deﬁne RedG,⊐
F
: P(F) →P(F) as follows:

A Comprehensive Framework for Saturation Theorem Proving
321
C ∈Red G,⊐
F
(N) if and only if
for every D ∈G(C),
D ∈Red F(G(N)) or there exists C′ ∈N such that C ⊐C′and D ∈G(C′).
Notice how ⊐is used to break ties between C and C′, possibly making C redundant.
We call Red G,⊐:= (Red G
I , Red G,⊐
F
) the (G, ⊐)-lifting of Red. We get the previously
deﬁned Red G as a special case of Red G,⊐by setting ⊐:= ∅.
We obtain our ﬁrst main result:
Theorem 6. LetRed bearedundancycriterionfor|=andGInf,letG beagrounding
function from F and FInf to G and GInf, and let ⊐be a well-founded strict partial
ordering on F. Then the (G, ⊐)-lifting RedG,⊐of Red is a redundancy criterion for
|=G and FInf.
Observe that ⊐appears only in the second component of Red G,⊐
=
(Red G
I , Red G,⊐
F
) and that the deﬁnitions of a saturated set and of static refu-
tational completeness do not depend on the second component of a redun-
dancy criterion. The following lemmas are immediate consequences of these
observations:
Lemma 7. A set N ⊆F is saturated w.r.t. FInf and RedG,⊐if and only if it is sat-
urated w.r.t. FInf and RedG,∅.
Lemma 8. (FInf, Red G,⊐)isstaticallyrefutationallycompletew.r.t.|=G ifandonly
if (FInf, Red G,∅) is statically refutationally complete w.r.t. |=G.
Combining Lemmas 1 and 8, we obtain our second main result:
Theorem 9. LetRed bearedundancycriterionfor|=andGInf,letG beagrounding
function from F and FInf to G and GInf, and let ⊐be a well-founded strict partial
ordering on F. If (FInf, Red G,∅) is statically refutationally complete w.r.t. |=G, then
(FInf, Red G,⊐) is dynamically refutationally complete w.r.t. |=G.
Example 10. For resolution or superposition in standard ﬁrst-order logic, we can
deﬁnethesubsumptionquasi-ordering·≥onclausesbyC ·≥C′ ifandonlyifC = C′σ
for some substitution σ. The subsumption ordering ·> := ·≥\ ·≤is well founded. By
choosing ⊐:= ·>, we obtain a criterion RedG,⊐that includes standard redundancy
andalsosupportssubsumptiondeletion.Similarly,forproofcalculimoduloassocia-
tivity and commutativity, we can let C ·≥C′ be true if there exists a substitution σ
such that C equals C′σ up to the equational theory.
Example 11. Constraint superposition with ordering constraints [19] is an exam-
ple of a calculus where the subsumption ordering ·> is not well founded: A ground
instance of a constrained clause C [[K]] is a ground clause Cθ for which Kθ evaluates
totrue.Deﬁne·≥bystatingthatC [[K]] ·≥C′ [[K′]]ifandonlyifeverygroundinstance
of C [[K]] is a ground instance of C′ [[K′]], and deﬁne ·> := ·≥\ ·≤. If ≻is a simpliﬁca-
tion ordering, then P(x) [[ x ≺b ]] ·> P(x) [[ x ≺f(b) ]] ·> P(x) [[ x ≺f(f(b)) ]] ·> · · ·
is an inﬁnite chain.

322
U. Waldmann et al.
Example 12. For higher-order calculi such as higher-order resolution [17] and
clausal λ-superposition [9], subsumption is also not well founded, as witnessed by
the chain p x x ·> p (x a) (x b1) ·> p (x a a) (x b1 b2) ·> · · ·.
Even if the subsumption ordering for some logic is not well founded, as in the
two examples above, we can always deﬁne ⊐as the intersection of the subsumption
ordering and an appropriate ordering based on formula sizes or weights.
Conversely, the ⊐relation can be more general than subsumption. In Sect. 4,
we will use it to justify the movement of formulas between sets in the given clause
procedure.
Example 13. Forsomesuperposition-baseddecisionprocedures[6],onewouldlike
todeﬁne⊐asthereversesubsumptionordering ·<onﬁrst-orderclauses.Eventhough
·< is not well founded in general, it is well founded on {C ∈F | D ∈G(C)} for every
D ∈G. As shown in the technical report [29], our framework can be extended to
support this case by deﬁning Red G,⊐
F
using a G-indexed family (⊐D)D∈G of well-
founded strict partial orderings instead of a single ⊐.
Intersections of Liftings. The above results can be extended in a straightforward
waytointersectionsofliftedredundancycriteria.Asbefore,letFandGbetwosetsof
formulas,andletFInf beanF-inferencesystem.Inaddition,letQbeaset.Forevery
q ∈Q,let|=q beaconsequencerelationoverG,letGInf q beaG-inferencesystem,let
Red q bearedundancycriterionfor|=q andGInf q,andletGq beagroundingfunction
from F and FInf to G and GInf q. Let ⊐be a well-founded strict partial ordering on
F.
For each q ∈Q, we know by Theorem 6 that the (Gq, ∅)-lifting Red q,Gq,∅=
(Red q,Gq
I
, Red q,Gq,∅
F
) and the (Gq, ⊐)-lifting Red q,Gq,⊐= (Red q,Gq
I
, Red q,Gq,⊐
F
) of
Red q are redundancy criteria for |=q
Gq and FInf. Consequently, the intersections
Red ∩G,⊐:= (Red ∩G,⊐
I
, Red ∩G,⊐
F
) :=

q∈Q Red q,Gq
I
,

q∈Q Red q,Gq,∅
F

and
Red ∩G,⊐:= (Red ∩G,⊐
I
, Red ∩G,⊐
F
) :=

q∈Q Red q,Gq
I
,

q∈Q Red q,Gq,⊐
F

are redundancy criteria for |=∩
G := 
q∈Q |=q
Gq and FInf.
Theorem 14. If (GInf q, Red q) is statically refutationally complete w.r.t. |=q for
every q ∈Q, and if for every N ⊆F that is saturated w.r.t. FInf and Red∩G
there exists a q such that GInf q(Gq(N)) ⊆Gq(FInf(N)) ∪Red q
I (Gq(N)), then
(FInf, Red ∩G) is statically refutationally complete w.r.t. |=∩
G.
Lemma 15. A set N ⊆F is saturated w.r.t. FInf and Red∩G,⊐if and only if it is
saturated w.r.t. FInf and Red∩G.
Lemma 16. (FInf, Red ∩G,⊐) is statically refutationally complete w.r.t. |=∩
G if and
only if (FInf, Red ∩G) is statically refutationally complete w.r.t. |=∩
G.
Theorem 17. If (FInf, Red ∩G)isstaticallyrefutationallycompletew.r.t.|=∩
G,then
(FInf, Red ∩G,⊐) is dynamically refutationally complete w.r.t. |=∩
G.

A Comprehensive Framework for Saturation Theorem Proving
323
Example 18. Intersections of liftings are needed to support selection functions in
superposition[4].ThecalculusFInf isparameterizedbyafunctionfsel onthesetFof
ﬁrst-order clauses that selects a subset of the negative literals in each C ∈F. There
are several ways to extend fsel to a selection function gsel on the set G of ground
clauses such that for every D ∈G there exists some C ∈F such that D = Cθ and D
and C have corresponding selected literals. For every such gsel, |=gsel is ﬁrst-order
entailment, GInf gsel is the set of ground inferences satisfying gsel, and Red gsel is
the redundancy criterion for GInf gsel. The grounding function Ggsel maps C ∈F
to {Cθ ∈G | θ is a substitution} and ι ∈FInf to the set of ground instances
of ι in GInf gsel with corresponding literals selected in the premises. In the static
refutational completeness proof, only one gsel is needed, but this gsel is not known
during a derivation, so fairness must be guaranteed w.r.t. Redgsel,Ggsel
I
for every pos-
sible extension gsel of fsel. Thus, checking Red ∩G
I
amounts to a worst-case analysis,
where we must assume that every ground instance Cθ of a premise C ∈F inherits
the selection of C.
Example 19. Intersections of liftings are also necessary for constraint super-
position calculi [19]. Here the calculus FInf operates on the set F of ﬁrst-order
clauses with constraints. For a convergent rewrite system R, |=R is ﬁrst-order
entailment up to R on the set G of unconstrained ground clauses, GInf R is the
set of ground superposition inferences, and RedR is redundancy up to R. The
grounding function GR maps C [[K]] ∈F to {D ∈G | D = Cθ, Kθ =
true, xθ is R-irreducible for all x}2 and ι ∈FInf to the set of ground instances of
ι where the premises and conclusion of GR(ι) are the GR-ground instances of the
premises and conclusion of ι. In the static refutational completeness proof, only one
particular R is needed, but this R is not known during a derivation, so fairness must
be guaranteed w.r.t. Red R,GR
I
for every convergent rewrite system R.
Almost every redundancy criterion for a nonground inference system FInf that
can be found in the literature can be written as RedG,∅for some grounding function
G from F and FInf to G and GInf, and some redundancy criterion Red for GInf, or
as an intersection Red ∩G of such criteria. By Theorem 17, every static refutational
completenessresultforFInf andRed ∩G—whichdoesnotpermitthedeletionofsub-
sumedformulasduringarun—yieldsimmediatelyadynamicrefutationalcomplete-
nessresultforFInf andRed ∩G,⊐—whichpermitsthedeletionofsubsumedformulas
during a run, provided that they are larger w.r.t. ⊐.
Adding Labels. In practice, the ordering ⊐used in (G, ⊐)-lifting often depends on
meta-information about a formula, such as its age or the way in which it has been
processed so far during a derivation. To capture this meta-information, we extend
formulas and inference systems in a rather trivial way with labels. As before, let F
2 For a variable x that occurs only in positive literals x ≈t, the condition is slightly more
complicated.

324
U. Waldmann et al.
and G be two sets of formulas, let FInf be an F-inference system, let GInf be a G-
inference system, let |= ⊆P(G) × P(G) be a consequence relation over G, let Red
be a redundancy criterion for |= and GInf, and let G be a grounding function from
F and FInf to G and GInf.
Let L be a nonempty set of labels. Deﬁne FL := F × L and FL⊥:= F⊥× L.
Notice that there are at least as many false values in FL as there are labels in L. We
use M, N to denote labeled formula sets. Given a set N ⊆FL, let ⌊N⌋:= {C |
(C, l) ∈N } denote the set of formulas without their labels. We call an FL-inference
system FLInf a labeled version of FInf if it has the following properties:
(L1) for every inference (Cn, . . . , C0) ∈FInf and every tuple (l1, . . . , ln) ∈Ln,
there exists an l0 ∈L and an inference ((Cn, ln), . . . , (C0, l0)) ∈FLInf;
(L2) if ι = ((Cn, ln), . . . , (C0, l0)) is an inference in FLInf, then (Cn, . . . , C0) is an
inference in FInf, denoted by ⌊ι⌋.
LetFLInf bealabeledversionofFInf.DeﬁneGL byGL((C, l)) := G(C)forevery
(C, l) ∈FL and by GL(ι) := G(⌊ι⌋) for every ι ∈FLInf. It qualiﬁes as a grounding
function from FL and FLInf to G and GInf. Let |=GL be the GL-lifting of |=. Let
Red GL,∅be the (GL, ∅)-lifting of Red. The following lemmas are obvious:
Lemma 20. If a set N ⊆FL is saturated w.r.t. FLInf and RedGL,∅, then ⌊N⌋⊆F
is saturated w.r.t. FInf and RedG,∅.
Lemma 21. If (FInf, Red G,∅) is statically refutationally complete w.r.t. |=G, then
(FLInf, Red GL,∅) is statically refutationally complete w.r.t. |=GL.
Theextensiontointersectionsofredundancycriteriaisalsostraightforward.Let
F and G be two sets of formulas, and let FInf be an F-inference system. Let Q be
a set. For every q ∈Q, let |=q be a consequence relation over G, let GInf q be a G-
inference system, let Redq be a redundancy criterion for |=q and GInf q, and let Gq
be a grounding function from F and FInf to G and GInf q. Then for every q ∈Q,
the (Gq, ∅)-lifting Red q,Gq,∅is a redundancy criterion for the Gq-lifting |=q
Gq, and so
Red ∩G is a redundancy criterion for |=∩
G and FInf.
Now let L be a nonempty set of labels, and deﬁne FL, FL⊥, and FLInf as above.
Foreveryq ∈Q,deﬁnethefunctionGq
L byGq
L((C, l)) := Gq(C)forevery(C, l) ∈FL
andbyGq
L(ι) := Gq(⌊ι⌋)foreveryι ∈FLInf.Thenforeveryq ∈Q,the(Gq
L, ∅)-lifting
Red q,Gq
L = (Red
q,Gq
L
I
, Red
q,Gq
L,∅
F
) of Red q is a redundancy criterion for the Gq
L-lifting
|=q
Gq
L of |=q and FLInf, and so
Red ∩GL := (Red ∩GL
I
, Red ∩GL
F
) :=

q∈Q Red
q,Gq
L
I
,

q∈Q Red
q,Gq
L,∅
F

is a redundancy criterion for |=∩
GL := 
q∈Q |=q
Gq
L and FLInf.
Lemma 22. If a set N ⊆FL is saturated w.r.t. FLInf and Red∩GL, then ⌊N⌋⊆F
is saturated w.r.t. FInf and Red∩G.
Theorem 23. If (FInf, Red ∩G)isstaticallyrefutationallycompletew.r.t.|=∩
G,then
(FLInf, Red ∩GL) is statically refutationally complete w.r.t. |=∩
GL.

A Comprehensive Framework for Saturation Theorem Proving
325
4
Prover Architectures
We now use the above results to prove the refutational completeness of a popular
prover architecture: the given clause procedure [18]. The architecture is parame-
terized by an inference system and a redundancy criterion. A generalization of the
architecture decouples scheduling and computation of inferences.
GivenClauseProcedure.LetFandGbetwosetsofformulas,andletFInf bean
F-inference systemwithoutpremise-freeinferences.Let Qbeaset.For every q ∈Q,
let |=q be a consequence relation over G, let GInf q be a G-inference system, let Red q
be a redundancy criterion for |=q and GInf q, and let Gq be a grounding function
from F and FInf to G and GInf q. Assume (FInf, Red ∩G) is statically refutationally
completew.r.t.|=∩
G.Furthermore,letLbeanonemptysetoflabels,letFL := F×L,
and let the FL-inference system FLInf be a labeled version of FInf. By Theorem 23,
(FLInf, Red ∩GL) is statically refutationally complete w.r.t. |=∩
GL.
Let ·=beanequivalencerelationonF,let·≻beawell-foundedstrictpartialorder-
ing on F such that·≻is compatible with ·= (i.e., C ·≻D, C ·= C′, D ·= D′ implies
C′ ·≻D′), such that C ·= D implies Gq(C) = Gq(D) for all q ∈Q, and such that
C ·≻D implies Gq(C) ⊆Gq(D) for all q ∈Q. We deﬁne·⪰:= ·≻∪·=. In practice,
·= is typically α-renaming, ·≻is either the subsumption ordering ·> (provided it is
well founded) or some well-founded ordering included in ·>, and for every q ∈Q, Gq
mapseveryformulaC ∈FtothesetofgroundinstancesofC,possiblymodulosome
theory.
Let⊐⊐beawell-foundedstrictpartialorderingonL.Wedeﬁnetheordering⊐on
FL by (C, l) ⊐(C′, l′) if either C ·≻C′ or else C ·= C′ and l ⊐⊐l′. By Lemma 16, the
static refutational completeness of (FLInf, Red ∩GL) w.r.t. |=∩
GL implies the static
refutational completeness of (FLInf, Red ∩GL,⊐), which by Lemma 1 implies the
dynamic refutational completeness of (FLInf, Red ∩GL,⊐).
This result may look intimidating, so let us unroll it. The FL-inference system
FLInf is a labeled version of FInf, which means that we get an FLInf-inference by
ﬁrst omitting the labels of the premises (Cn, ln), . . . , (C1, l1), then performing an
FInf-inference (Cn, . . . , C0), and ﬁnally attaching an arbitrary label l0 to the con-
clusion C0. Since Gq
L diﬀers from Gq only by the omission of the labels and the ﬁrst
components of Red∩GL,⊐and Red∩GL agree, we get this result:
Lemma 24. An FLInf-inference ι is redundant w.r.t. Red∩GL,⊐and N if and only
if the underlying FInf-inference ⌊ι⌋is redundant w.r.t. Red∩G and ⌊N⌋.
Lemma 25. Let N
⊆FL, and let (C, l) be a labeled formula. Then (C, l) ∈
Red∩GL,⊐
F
(N) if (i) C ∈Red ∩G
F (⌊N⌋), or (ii) C ·≻C′ for some C′ ∈⌊N⌋, or
(iii) C ·⪰C′ for some (C′, l′) ∈N with l ⊐⊐l′.
The given clause procedure that lies at the heart of saturation provers can be
presented and studied abstractly. We assume that the set of labels L contains at
least two values, including a distinguished ⊐⊐-smallest value denoted by active, and
that the labeled version FLInf of FInf never assigns active to a conclusion.

326
U. Waldmann et al.
The state ofaprover is asetoflabeled formulas.Thelabelidentiﬁes towhichfor-
mulaseteachformulabelongs.Theactivelabelidentiﬁestheactiveformulasetfrom
the familiar given clause procedure. The other, unspeciﬁed formula sets are consid-
ered passive. Given a set N and a label l, we deﬁne the projection N↓l as consisting
only of the formulas labeled by l.
The given clause prover GC is deﬁned as the following transition system:
Process N ⊎M =⇒GC N ∪M′
where M ⊆Red ∩GL,⊐
F
(N ∪M′) and M′↓active = ∅
Infer N ⊎{(C, l)} =⇒GC N ∪{(C, active)} ∪M
where l ̸= active, M↓active = ∅, and
FInf(⌊N↓active⌋, {C}) ⊆Red∩G
I
(⌊N⌋∪{C} ∪⌊M⌋)
The Process rule covers most operations performed in a theorem prover. By
Lemma 25, this includes deleting Red∩G
F -redundant formulas with arbitrary labels
and adding formulas that make other formulas Red ∩G
F -redundant (i.e., simplifying
w.r.t.Red ∩G
F ),by(i);deletingformulasthatare·≻-subsumedbyotherformulaswith
arbitrary labels, by (ii); deleting formulas that are·⪰-subsumed by other formulas
with smaller labels, by (iii); and replacing the label of a formula by a smaller label
diﬀerent from active, also by (iii).
Infer is the only rule that puts a formula in the active set. It relabels a
passive formula C to active and ensures that all inferences between C and the
active formulas, including C itself, become redundant. Recall that by Lemma 24,
FLInf(N↓active, {(C, active)}) ⊆Red∩GL
I
(N ∪{(C, active)} ∪M) if and only if
FInf(⌊N↓active⌋, {C}) ⊆Red∩G
I
(⌊N⌋∪{C} ∪⌊M⌋). By property (R4), every
inference is redundant if its conclusion is contained in the set of formulas, and typ-
ically, inferences are in fact made redundant by adding their conclusions to any of
the passive sets. Then, ⌊M⌋equals concl(FInf(⌊N↓active⌋, {C})).
Since every =⇒GC-derivation is also a ▷Red∩GL,⊐-derivation and (FLInf,
Red ∩GL,⊐) is dynamically refutationally complete, it now suﬃces to show fairness
to prove the refutational completeness of GC.
Lemma 26. Let (Ni)i be a =⇒GC-derivation. If N0↓active = ∅and N∗↓l = ∅for all
l ̸= active, then (Ni)i is a fair ▷Red∩GL,⊐-derivation.
Theorem 27. Let (Ni)i be a =⇒GC-derivation, where N0↓active = ∅and N∗↓l = ∅
for all l ̸= active. If ⌊N0⌋|=∩
G {⊥} for some ⊥∈F⊥, then some Ni contains (⊥′, l)
for some ⊥′ ∈F⊥and l ∈L.
Example 28. The following Otter loop [18, Sect. 2.3.1] prover OL is an instance
of the given clause prover GC. This loop design is inspired by Weidenbach’s prover
without splitting from his Handbook chapter [30, Tables 4–6]. The prover’s state is
a ﬁve-tuple N | X | P | Y | A of formula sets. The N, P, and A sets store the new,
passive,andactiveformulas.TheX andY setsaresubsingletons(i.e.,setsofatmost
one element) that can store a chosen new or passive formula. Initial states are of the
form N | ∅| ∅| ∅| ∅.
ChooseN N ⊎{C} | ∅| P | ∅| A =⇒OL N | {C} | P | ∅| A

A Comprehensive Framework for Saturation Theorem Proving
327
DeleteFwd N | {C} | P | ∅| A =⇒OL N | ∅| P | ∅| A
if C ∈Red∩G
F (P ∪A) or C ·⪰C′ for some C′ ∈P ∪A
SimplifyFwd N | {C} | P | ∅| A =⇒OL N | {C′} | P | ∅| A
if C ∈Red∩G
F (P ∪A ∪{C′})
DeleteBwdP N | {C} | P ⊎{C′} | ∅| A =⇒OL N | {C} | P | ∅| A
if C′ ∈Red ∩G
F ({C}) or C′ ·≻C
SimplifyBwdP N | {C} | P ⊎{C′} | ∅| A =⇒OL N ∪{C′′} | {C} | P | ∅| A
if C′ ∈Red ∩G
F ({C, C′′})
DeleteBwdA N | {C} | P | ∅| A ⊎{C′} =⇒OL N | {C} | P | ∅| A
if C′ ∈Red ∩G
F ({C}) or C′ ·≻C
SimplifyBwdA N | {C} | P | ∅| A ⊎{C′} =⇒OL N ∪{C′′} | {C} | P | ∅| A
if C′ ∈Red ∩G
F ({C, C′′})
Transfer N | {C} | P | ∅| A =⇒OL N | ∅| P ∪{C} | ∅| A
ChooseP ∅| ∅| P ⊎{C} | ∅| A =⇒OL ∅| ∅| P | {C} | A
Infer ∅| ∅| P | {C} | A =⇒OL M | ∅| P | ∅| A ∪{C}
if FInf(A, {C}) ⊆Red ∩G
I
(A ∪{C} ∪M)
A reasonable strategy for applying the OL rules is presented below. It relies
on a well-founded ordering ≻on formulas to ensure that the backward simpli-
ﬁcation rules actually “simplify” their target, preventing nontermination of the
inner loop. It also assumes that FInf(N, {C}) is ﬁnite if N is ﬁnite. Brieﬂy, the
strategy corresponds to the regular expression

ChooseN;
SimplifyFwd∗;
(DeleteFwd | (DeleteBwdP∗; DeleteBwdA∗; SimplifyBwdP∗; Simpli-
fyBwdA∗; Transfer))
∗; (ChooseP; Infer)?∗, where ; denotes concatena-
tion and ∗and ? are given an eager semantics. Simpliﬁcations are applicable only
if the result is ≻-smaller than the original formula. Moreover, ChooseC always
chooses the oldest formula in N, and the choice of C in ChooseP must be fair.
The instantiation of GC relies on ﬁve labels l1 ⊐⊐· · · ⊐⊐l5 = active representing
N, X, P, Y, A. Let (Ni |Xi |Pi |Yi |Ai)i be a derivation following the strategy, where
N0 is ﬁnite and X0 = P0 = Y0 = A0 = ∅. We can show that N∗= X∗= P∗= Y∗= ∅.
Therefore, by Theorem 27, OL is dynamically refutationally complete.
In most calculi, Red is deﬁned in terms of some total and well-founded ordering
≻G on G. We can then deﬁne ≻so that C ≻C′ if the smallest element of Gq(C) is
greaterthanthesmallestelementofGq(C′)w.r.t.≻G,forsomearbitraryﬁxedq ∈Q.
This allows a wide range of simpliﬁcations typically implemented in superposition
provers. To ensure fairness when applying ChooseP, one approach is to use an N-
valued weight function that is strictly antimonotone in the age of the formula [22,
Sect.4].Anotheroptionistoalternatebetweenheuristicallychoosingnformulasand
taking the oldest formula [18, Sect. 2.3.1]. To guarantee soundness, we can require

328
U. Waldmann et al.
that the formulas added by simpliﬁcation andInfer are |≈-entailed by the formulas
in the state before the transition. This can be relaxed to consistency-preservation,
e.g., for calculi that perform skolemization.
Example 29. Bachmair and Ganzinger’s resolution prover RP [5, Sect. 4.3] is
another instance of GC. It embodies both a concrete prover architecture and a con-
crete inference system: ordered resolution with selection (O≻
S ). States are triples
N | P | O of ﬁnite clause sets. The instantiation relies on three labels l1 ⊐⊐l2 ⊐⊐
l3 = active. Subsumption can be supported as described in Example 10.
Delayed Inferences. An orphan is a passive formula that was generated by an
inference for which at least one premise is no longer active. The given clause prover
GC presented above is suﬃcient to describe a prover based on an Otter loop as well
as a basic DISCOUNT loop prover, but to describe a DISCOUNT loop prover with
orphan deletion, we need to decouple the scheduling of inferences and their compu-
tation. The same scheme can be used for inference systems that contain premise-
free inferences or that may generate inﬁnitely many conclusions from ﬁnitely many
premises. Yet another use of the scheme is to save memory: A delayed inference can
be stored more compactly than a new formula, as a tuple of premises together with
instructions on how to compute the conclusion.
The lazy given clause prover LGC generalizes GC. It is deﬁned as the following
transition system on pairs (T, N), where T (“to do”) is a set of inferences and N
is a set of labeled formulas. We use the same assumptions as for GC except that we
now permit premise-free inferences in FInf. Initially, T consists of all premise-free
inferences of FInf.
Process (T, N ⊎M) =⇒LGC (T, N ∪M′)
where M ⊆Red ∩GL,⊐
F
(N ∪M′) and M′↓active = ∅
ScheduleInfer (T, N ⊎{(C, l)}) =⇒LGC (T ∪T ′, N ∪{(C, active)})
where l ̸= active and T ′ = FInf(⌊N↓active⌋, {C})
ComputeInfer (T ⊎{ι}, N) =⇒LGC (T, N ∪M)
where M↓active = ∅and ι ∈Red∩G
I
(⌊N ∪M⌋)
DeleteOrphans (T ⊎T ′, N) =⇒LGC (T, N)
where T ′ ∩FInf(⌊N↓active⌋) = ∅
ScheduleInfer relabels a passive formula C to active and puts all inferences
between C and the active formulas, including C itself, into the set T. Compute-
Infer removes an inference from T and makes it redundant by adding appropriate
labeledformulastoN (typicallytheconclusionoftheinference).DeleteOrphans
can delete scheduled inferences from T if some of their premises have been deleted
from N↓active in the meantime. Note that the rule cannot delete premise-free infer-
ences, since the side condition is then vacuously false.
Abstractly,theT componentofthestateisasetofinferences(Cn, . . . , C0).Inan
actual implementation, it can be represented in diﬀerent ways: as a set of compactly
encoded recipes for computing the conclusion C0 from the premises (Cn, . . . , C1) as

A Comprehensive Framework for Saturation Theorem Proving
329
in Waldmeister [16], or as a set of explicit formulas C0 with information about their
parents (Cn, . . . , C1) as in E [25]. In the latter case, some presimpliﬁcations may be
performedonC0;thiscouldbemodeledmorefaithfullybydeﬁningT asasetofpairs
(ι, simp(C0)).
Lemma 30. If (Ti, Ni)i is a =⇒LGC-derivation, then (Ni)i is a ▷Red∩GL,⊐- deriva-
tion.
Lemma 31. Let (Ti, Ni)i be a =⇒LGC-derivation. If N0↓active = ∅, N∗↓l = ∅for all
l ̸= active, T0 is the set of all premise-free inferences of FInf, and T∗= ∅, then (Ni)i
is a fair ▷Red∩GL,⊐-derivation.
Theorem 32. Let (Ti, Ni)i be a =⇒LGC-derivation, where N0↓active = ∅, N∗↓l = ∅
for all l ̸= active, T0 is the set of all premise-free inferences of FInf, and T∗= ∅. If
⌊N0⌋|=∩
G {⊥} for some ⊥∈F⊥, then some Ni contains (⊥′, l) for some ⊥′ ∈F⊥
and l ∈L.
Example 33. ThefollowingDISCOUNTloop[1]proverDLisaninstanceofthelazy
givenclauseproverLGC.ThisloopdesignisinspiredbythedescriptionofE[25].The
prover’s state is a four-tuple T | P | Y | A, where T is a set of inferences and P, Y ,
A are sets of formulas. The T, P, and A sets correspond to the scheduled inferences,
the passive formulas, and the active formulas. The Y set is a subsingleton that can
store a chosen passive formula. Initial states have the form T | P | ∅| ∅, where T is
the set of all premise-free inferences of FInf.
ComputeInfer T ⊎{ι} | P | ∅| A =⇒DL T | P | {C} | A
if ι ∈Red ∩G
I
(A ∪{C})
ChooseP T | P ⊎{C} | ∅| A =⇒DL T | P | {C} | A
DeleteFwd T | P | {C} | A =⇒DL T | P | ∅| A
if C ∈Red∩G
F (A) or C ·⪰C′ for some C′ ∈A
SimplifyFwd T | P | {C} | A =⇒DL T | P | {C′} | A
if C ∈Red∩G
F (A ∪{C′})
DeleteBwd T | P | {C} | A ⊎{C′} =⇒DL T | P | {C} | A
if C′ ∈Red ∩G
F ({C}) or C′ ·≻C
SimplifyBwd T | P | {C} | A ⊎{C′} =⇒DL T | P ∪{C′′} | {C} | A
if C′ ∈Red ∩G
F ({C, C′′})
ScheduleInfer T | P | {C} | A =⇒DL T ∪T ′ | P | ∅| A ∪{C}
if T ′ = FInf(A, {C})
DeleteOrphans T ⊎T ′ | P | Y | A =⇒DL T | P | Y | A
if T ′ ∩FInf(A) = ∅
A reasonable strategy for applying the DL rules along the lines of that for
OL and with the same assumptions follows:

(ComputeInfer
|
ChooseP);

330
U. Waldmann et al.
SimplifyFwd∗; (DeleteFwd | (DeleteBwd∗; SimplifyBwd∗; Delete-
Orphans; ScheduleInfer))
∗.InComputeInfer,theﬁrstformulafromT ∪P,
organized as a single queue, is chosen. The instantiation of LGC relies on three labels
l1 ⊐⊐· · · ⊐⊐l3 = active corresponding to the sets P, Y, A.
Example 34. Higher-order uniﬁcation can give rise to inﬁnitely many incompa-
rable uniﬁers. As a result, in clausal λ-superposition [9], performing all inferences
between two clauses can lead to inﬁnitely many conclusions, which need to be
enumerated fairly. The Zipperposition prover [9] performs this enumeration in an
extended DISCOUNT loop. Another instance of inﬁnitary inferences is the n-ary
Acycl and Uniq rules of superposition with (co)datatypes [14].
Abstractly, a Zipperposition loop prover ZL operates on states T | P | Y |
A, where T is organized as a ﬁnite set of possibly inﬁnite sequences (ιi)i of infer-
ences, and P, Y, A are as in DL. The ChooseP, DeleteFwd, SimplifyFwd,
DeleteBwd, and SimplifyBwd rules are as in DL. The other rules follow:
ComputeInfer T ⊎{(ιi)i} | P | ∅| A =⇒ZL T ∪{(ιi)i≥1} | P ∪{C} | ∅| A
if ι0 ∈Red ∩G
I
(A ∪{C})
ScheduleInfer T | P | {C} | A =⇒ZL T ∪T ′ | P | ∅| A ∪{C}
if T ′ is a ﬁnite set of sequences (ιj
i)i of inferences such that the set of all ιj
i equals
FInf(A, {C})
DeleteOrphan T ⊎{(ιi)i} | P | Y | A =⇒ZL T | P | Y | A
if ιi /∈FInf(A) for all i
ComputeInferworksontheﬁrstelementofsequences.ScheduleInferadds
new sequences to T. Typically, these sequences store FInf(A, {C}), which may be
countably inﬁnite, in such a way that all inferences in one sequence have identical
premises and can be removed together by DeleteOrphan. To produce fair deriva-
tions, a prover needs to choose the sequence in ComputeInfer fairly and to choose
the formula in ChooseP fairly, thereby achieving dovetailing.
Example 35. The prover architectures described above can be instantiated with
saturation calculi that use a redundancy criterion obtained as an intersection of
lifted redundancy criteria. Most calculi are deﬁned in such a way that this require-
ment is obviously satisﬁed. The outlier is unfailing completion [2].
Although unfailing completion predates the introduction of Bachmair–
Ganzinger-style redundancy,itcanbeincorporatedintothatframeworkbydeﬁning
that formulas (i.e., rewrite rules and equations) and inferences (i.e., orientation and
criticalpaircomputation)areredundantifforeveryrewriteproofusingthatrewrite
rule,equation,orcriticalpeak,thereexistsasmallerrewriteproof.Therequirement
that the redundancy criterion must be obtained by lifting (which is necessary to
introduce the labeling) can then be trivially fulﬁlled by “self-lifting”—i.e., by deﬁn-
ing G := F and·≻:= ∅and by taking G as the function that maps every formula or
inference to the set of its α-renamings.

A Comprehensive Framework for Saturation Theorem Proving
331
5
Isabelle Development
The framework described in the previous sections has been formalized in Isabelle/
HOL[20,21],includingallthetheoremsandlemmasandtheproverarchitecturesGC
and LGC but excluding the examples. The Isabelle theory ﬁles are available in the
Archive of Formal Proofs [26]. The development is also part of the IsaFoL (Isabelle
Formalization of Logic) [12] eﬀort, which aims at developing a reusable computer-
checked library of results about automated reasoning.
The development relies heavily on Isabelle’s locales [8]. These are contexts that
ﬁx variables and make assumptions about these. With locales, the deﬁnitions and
lemmas look similar to how they are stated on paper, but the proofs often become
more complicated: Layers of locales may hide deﬁnitions, and often these need to be
manually unfolded before the desired lemma can be proved.
We chose to represent basic nonempty sets such as F and L by types. It relieved
us from having to thread through nonemptiness conditions. Moreover, objects
are automatically typed, meaning that lemmas could be stated without explicit
hypotheses that given objects are formulas, labels, or indices. On the other hand,
for sets such as F⊥and FInf that are subsets of other sets, it was natural to use
simply typed sets. Derivations, which are used to describe the dynamic behavior of
a calculus, are represented by the same lazy list codatatype [13] and auxiliary def-
initions that were used in the mechanization of the ordered resolution prover RP
(Example 29) by Schlichtkrull et al. [23,24].
The framework’s design and its mechanization were carried out largely in paral-
lel. This resulted in more work on the mechanization side, but it also helped shape
thetheoryitself.Inparticular,anattemptatverifyingRP inIsabelleusinganearlier
version of the framework made it clear that the theory was not general enough yet to
support selection functions (Example 18). In ongoing work, we are completing the
RP proof and are developing a veriﬁed superposition prover.
6
Conclusion
We presented a formal framework for saturation theorem proving inspired by
Bachmair and Ganzinger’s Handbook chapter [5]. Users can conveniently derive a
dynamic refutational completeness result for a concrete prover based on a statically
refutationally complete calculus. The key was to strengthen the standard redun-
dancy criterion so that all prover operations, including subsumption deletion, can
be justiﬁed by inference or redundancy. The framework is mechanized in Isabelle/
HOL, where it can be instantiated to verify concrete provers.
To employ the framework, the starting point is a statically complete satura-
tion calculus that can be expressed as the lifting (FInf, Red G) or (FInf, Red ∩G) of a
ground calculus (GInf, Red), where Red qualiﬁes as a redundancy criterion and G
qualiﬁes as a grounding function or grounding function family. The framework can
be used to derive two main results:
1. Afterdeﬁningawell-foundedordering⊐thatcapturessubsumption,invokeThe-
orem 17 to show (FInf, Red ∩G,⊐) dynamically complete.

332
U. Waldmann et al.
2. Based on the previous step, invoke Theorem 27 or 32 to derive the dynamic com-
pleteness of a prover architecture building on the given clause procedure, such as
the Otter loop, the DISCOUNT loop, or the Zipperposition loop.
Theframeworkcanalsohelpestablishthestaticcompletenessofthenongroundcal-
culus.Formanycalculi(withthenotableexceptionsofconstraintsuperpositionand
hierarchicsuperposition),Theorem5or14canbeusedtoliftthestaticcompleteness
of (GInf, Red) to (FInf, Red G) or (FInf, Red ∩G).
The main missing piece of the framework is a generic treatment of clause split-
ting. The only formal treatment of splitting we are aware of, by Fietzke and Wei-
denbach [15], hard-codes both the underlying calculus and the splitting strategy.
Voronkov’s AVATAR architecture [27] is more ﬂexible and yields impressive empir-
ical results, but it oﬀers no dynamic completeness guarantees.
Acknowledgment. We thank Alexander Bentkamp for discussions about prover archi-
tectures for higher-order logic and for feedback from instantiating the framework, Mathias
Fleury and Christian Sternagel for their help with the Isabelle development, and Robert
Lewis, Visa Nummelin, Dmitriy Traytel, and the anonymous reviewers for their comments
and suggestions. Blanchette’s research has received funding from the European Research
Council (ERC) under the European Union’s Horizon 2020 research and innovation pro-
gram (grant agreement No. 713999, Matryoshka). He also beneﬁted from the Netherlands
Organization for Scientiﬁc Research (NWO) Incidental Financial Support scheme and he
hasreceivedfundingfromtheNWOundertheVidiprogram(projectNo.016.Vidi.189.037,
Lean Forward).
References
1. Avenhaus, J., Denzinger, J., Fuchs, M.: DISCOUNT: a system for distributed equa-
tionaldeduction.In:Hsiang,J.(ed.)RTA1995.LNCS,vol.914,pp.397–402.Springer,
Heidelberg (1995). https://doi.org/10.1007/3-540-59200-8 72
2. Bachmair, L., Dershowitz, N., Plaisted, D.A.: Completion without failure. In: A¨ıt-
Kaci, H., Nivat, M. (eds.) Rewriting Techniques—Resolution of Equations in Alge-
braic Structures, vol. 2, pp. 1–30. Academic Press (1989)
3. Bachmair, L., Ganzinger, H.: On restrictions of ordered paramodulation with simpli-
ﬁcation. In: Stickel, M.E. (ed.) CADE 1990. LNCS, vol. 449, pp. 427–441. Springer,
Heidelberg (1990). https://doi.org/10.1007/3-540-52885-7 105
4. Bachmair, L., Ganzinger, H.: Rewrite-based equational theorem proving with selec-
tion and simpliﬁcation. J. Log. Comput. 4(3), 217–247 (1994)
5. Bachmair, L., Ganzinger, H.: Resolution theorem proving. In: Robinson, A.,
Voronkov, A. (eds.) Handbook of Automated Reasoning, vol. I, pp. 19–99. Elsevier
and MIT Press (2001)
6. Bachmair, L., Ganzinger, H., Waldmann, U.: Superposition with simpliﬁcation as a
decision procedure for the monadic class with equality. In: Gottlob, G., Leitsch, A.,
Mundici,D.(eds.)KGC1993.LNCS,vol.713,pp.83–96.Springer,Heidelberg(1993).
https://doi.org/10.1007/BFb0022557
7. Bachmair, L., Ganzinger, H., Waldmann, U.: Refutational theorem proving for hier-
archic ﬁrst-order theories. Appl. Algebra Eng. Commun. Comput. 5, 193–212 (1994)
8. Ballarin, C.: Locales: a module system for mathematical theories. J. Autom. Reason.
52(2), 123–153 (2014)

A Comprehensive Framework for Saturation Theorem Proving
333
9. Bentkamp, A., Blanchette, J., Tourret, S., Vukmirovi´c, P., Waldmann, U.: Superposi-
tion with lambdas. In: Fontaine, P. (ed.) CADE 2019. LNCS (LNAI), vol. 11716, pp.
55–73. Springer, Cham (2019). https://doi.org/10.1007/978-3-030-29436-6 4
10. Bentkamp, A., Blanchette, J.C., Cruanes, S., Waldmann, U.: Superposition for
lambda-free higher-order logic. In: Galmiche, D., Schulz, S., Sebastiani, R. (eds.)
IJCAR 2018. LNCS (LNAI), vol. 10900, pp. 28–46. Springer, Cham (2018). https://
doi.org/10.1007/978-3-319-94205-6 3
11. Bhayat, A., Reger, G.: A combinator-based superposition calculus for higher-order
logic. In: Peltier, N., Sofronie-Stokkermans, V. (eds.) IJCAR 2020. LNCS (LNAI).
Springer, Heidelberg (2020)
12. Blanchette, J.C.: Formalizing the metatheory of logical calculi and automatic provers
in Isabelle/HOL (invited talk). In: Mahboubi, A., Myreen, M.O. (eds.) CPP 2019, pp.
1–13. ACM (2019)
13. Blanchette, J.C., H¨olzl, J., Lochbihler, A., Panny, L., Popescu, A., Traytel, D.: Truly
modular (Co)datatypes for Isabelle/HOL. In: Klein, G., Gamboa, R. (eds.) ITP 2014.
LNCS, vol. 8558, pp. 93–110. Springer, Cham (2014). https://doi.org/10.1007/978-
3-319-08970-6 7
14. Blanchette, J.C., Peltier, N., Robillard, S.: Superposition with datatypes and
codatatypes. In: Galmiche, D., Schulz, S., Sebastiani, R. (eds.) IJCAR 2018. LNCS
(LNAI), vol. 10900, pp. 370–387. Springer, Cham (2018). https://doi.org/10.1007/
978-3-319-94205-6 25
15. Fietzke, A., Weidenbach, C.: Labelled splitting. Ann. Math. Artif. Intell. 55(1–2), 3–
34 (2009)
16. Hillenbrand, T., L¨ochner, B.: The next Waldmeister loop. In: Voronkov, A. (ed.)
CADE 2002. LNCS (LNAI), vol. 2392, pp. 486–500. Springer, Heidelberg (2002).
https://doi.org/10.1007/3-540-45620-1 38
17. Huet, G.P.: A mechanization of type theory. In: Nilsson, N.J. (ed.) IJCAI 1973, pp.
139–146. William Kaufmann (1973)
18. McCune, W., Wos, L.: Otter—the CADE-13 competition incarnations. J. Autom.
Reason. 18(2), 211–220 (1997)
19. Nieuwenhuis, R., Rubio, A.: Theorem proving with ordering and equality constrained
clauses. J. Symb. Comput. 19(4), 321–351 (1995)
20. Nipkow,T.,Klein,G.:ConcreteSemantics:WithIsabelle/HOL.Springer,Heidelberg
(2014). https://doi.org/10.1007/978-3-319-10542-0
21. Nipkow, T., Paulson, L.C., Wenzel, M.: Isabelle/HOL: A proof assistant for higher-
order logic. LNCS, vol. 2283. Springer, Heidelberg (2002). https://doi.org/10.1007/
3-540-45949-9
22. Schlichtkrull, A., Blanchette, J.C., Traytel, D.: A veriﬁed prover based on ordered res-
olution. In: Mahboubi, A., Myreen, M.O. (eds.) CPP 2019, pp. 152–165. ACM (2019)
23. Schlichtkrull,A.,Blanchette,J.C.,Traytel,D.,Waldmann,U.:FormalizationofBach-
mairandGanzinger’sorderedresolutionprover.ArchiveofFormalProofs2018(2018).
https://www.isa-afp.org/entries/Ordered Resolution Prover.html
24. Schlichtkrull,A.,Blanchette,J.C.,Traytel,D.,Waldmann,U.:FormalizingBachmair
and Ganzinger’s ordered resolution prover. In: Galmiche, D., Schulz, S., Sebastiani,
R. (eds.) IJCAR 2018. LNCS, vol. 10900, pp. 89–107. Springer, Heidelberg (2018).
https://doi.org/10.1007/978-3-319-94205-6 7
25. Schulz, S.: E—a brainiac theorem prover. AI Commun. 15(2–3), 111–126 (2002)
26. Tourret,S.:Acomprehensiveframeworkforsaturationtheoremproving.Arch.Formal
Proofs 2020 (2020). https://www.isa-afp.org/entries/Saturation Framework.shtml

334
U. Waldmann et al.
27. Voronkov,A.:AVATAR:thearchitectureforﬁrst-ordertheoremprovers.In:Biere,A.,
Bloem, R. (eds.) CAV 2014. LNCS, vol. 8559, pp. 696–710. Springer, Cham (2014).
https://doi.org/10.1007/978-3-319-08867-9 46
28. Waldmann, U.: Cancellative abelian monoids and related structures in refutational
theorem proving (part I). J. Symb. Comput. 33(6), 777–829 (2002)
29. Waldmann, U., Tourret, S., Robillard, S., Blanchette, J.: A comprehensive framework
for saturation theorem proving (technical report). Technical report (2020). http://
matryoshka.gforge.inria.fr/pubs/saturate report.pdf
30. Weidenbach, C.: Combining superposition, sorts and splitting. In: Robinson, A.,
Voronkov, A. (eds.) Handbook of Automated Reasoning, vol. II, pp. 1965–2013. Else-
vier and MIT Press (2001)

Proof Procedures

Possible Models Computation and
Revision – A Practical Approach
Peter Baumgartner1,2(B)
1 Data61/CSIRO, Canberra, Australia
Peter.Baumgartner@data61.csiro.au
2 The Australian National University, Canberra, Australia
Abstract. This paper describes a method of computing plausible states
of a system as a logical model. The problem of analyzing state-based sys-
tems as they evolve over time has been studied widely in the automated
reasoning community (and others). This paper proposes a speciﬁc app-
roach, one that is tailored to situational awareness applications. The
main contribution is a calculus for a novel speciﬁcation language that
is built around disjunctive logic programming under a possible models
semantics, stratiﬁcation in terms of event times, default negation, and a
model revision operator for dealing with incomplete or erroneous events
– a typical problem in realistic applications. The paper proves the cal-
culus correct wrt. a formal semantics of the speciﬁcation language and
it describes the calculus’ implementation via embedding in Scala. This
enables immediate access to rich data structures and external systems,
which is important in practice.
1
Introduction
This paper is concerned with logic-based modeling and automated reasoning
for estimating the current state of a system as it evolves over time. The main
motivation is situational awareness [12], which requires the ability to understand
and explain a system’s state, at any time, and at a level that matters to the user,
even if only partial or incorrect information about the external events leading
to that state is available. In a supply chain context, for example, one cannot
expect that events are reported correctly and in a timely manner. Sensors may
fail, transmission channels are laggy, reports exist only in paper form, not every
player is willing to share information, etc. Because of that, it is often impossible
to know with full certainty the actual state of the system. The paper addresses
this problem and proposes to instead derive a set of plausible candidate states as
an approximation of ground truth. The states may include consequences relevant
for situational awareness, e.g., that a shipment will be late. A human operator
may then make decisions or provide additional details, this way closing the loop.
The plausible candidate states are represented as models of a logical speciﬁ-
cation and a given a set of external timestamped events. The proposed modeling
This research is supported by the Science and Industry Endowment Fund.
c
⃝Springer Nature Switzerland AG 2020
N. Peltier and V. Sofronie-Stokkermans (Eds.): IJCAR 2020, LNAI 12166, pp. 337–355, 2020.
https://doi.org/10.1007/978-3-030-51074-9_19

338
P. Baumgartner
paradigm is logic programming, and models are computed in a bottom-up way. It
adopts notions of stratiﬁcation, default negation and a possible model semantics
for its (disjunctive) program rules. Stratiﬁcation is in terms of event time, with
increasing time horizons for anytime reasoning; default negation is needed to
reason in absence of information such as event reports; disjunctions are needed
to derive alternate candidate states. In order to deal with less-than-perfect event
data, the modeling language features a novel model revision operator that allows
the programmer to formulate conditions under which a model computation with
a corrected set of events should be attempted in an otherwise inconsistent state.
The following informal overview illustrates these features.
1.1
Main Ideas and Design Rationale
A model, or program, is comprised of a set of rules of the form head ←body.
The head can be a non-empty disjunction of atoms, or a fail head. The former
rules open the solution (models) space for a ﬁxed set of external events, while fail
head rules limit it and specify amended event sets for new solution attempts.
The body is a conjunction of atoms and negated (via “not”) conjunctions of
atoms. Negation is “default negation”, i.e., a closed world assumption is in place
for evaluating the latter. Rules may contain ﬁrst-order variables and must be
range restricted. This guarantees that only ground heads can be derived from
ground facts when a rule is evaluated in a bottom-up way. Our notion of range
restriction is somewhat non-standard, though, and permits extra variables inside
negation. These variables are implicitly existentially quantiﬁed (“not ∃x . . . ”).
We need, however, syntactic restrictions that enforce stratiﬁcation in terms
of “time”. This entails that rule evaluation does not depend on facts from the
future. In fact, this is a reasonable assumption for situational awareness, whose
any-time character requires to understand the current situation based on the
available information up to now only. Technically, every atom must have a ded-
icated “time” argument, a N-sorted variable, which, together, with earlier-than
constraints (via “<” or “≤”) enforces stratiﬁcation. The details of that will have
to wait until later (Deﬁnition 1). An example rule is
hungry(t, x) ∨thirsty(t, x) ←get up(t, x), not(t −6 ≤s, s ≤t, meal(s, x)) . (1)
It could say “if x gets up at time t and didn’t have a meal in 6 hours prior then x is
hungry or thirsty at t, or both”. A set of facts, say, {get up(8, bob), meal(12, bob)}
then entails hungry(8, bob) ∨thirsty(8, bob). Notice that in the relevant rule
instance the negated body element not(8 −6
≤
s, s
<
8, meal(s, bob))
is satisﬁed by the facts (using the closed-world assumption), as for the
only relevant meal-instance meal(12, bob) the arithmetic constraint is false.
The possible model semantics [26], which we adopt, interprets disjunctions
(also) inclusively. Each resulting case {hungry(8, bob)}, {thirsty(8, bob)} and
{hungry(8, bob), thirsty(8, bob)} together with the facts yields a possible model.
Stratiﬁcation in terms of time makes default negation with existentially quan-
tiﬁed variables possible; no need to look into the future. We impose a secondary

Possible Models Computation and Revision – A Practical Approach
339
kind of stratiﬁcation that also makes it more eﬃcient. It rests on distinguish-
ing two types of atoms: EDB atoms and IDB atoms (extensional/intensional
database, respectively). EDB atoms are for external events, the given facts, and
IDB atoms are for derived facts. A disjunctive head can contain IDB atoms
only. Now, an IDB atom within a negation has to be strictly earlier (<) than
the head, while an EDB atom within a negation can be non-strictly earlier (≤).
This make sure that a truth value for a negated expression cannot change later,
in the course of rule evaluation in increasing time order. The rule (1) above is
stratiﬁed if meal is EDB, otherwise “s ≤t” would have to be “s < t”. A rule
like hungry(t, x) ←get up(t, x), not hungry(t, x) cannot be stratiﬁed.
Rules with fail heads enable the programmer to specify when a (partial)
model candidate is unsatisﬁable and to say how to potentially ﬁx this situation.
A rule of the form fail() ←body without arguments to fail is a usual integrity
constraint, e.g.:
fail() ←hungry(t, x), eat(s, x), t −4 ≤s, s < t
(2)
The rule (2) rejects that x is both hungry and has eaten within the last 4 hours.
Together with rule (1) and the EDB {eat(7, bob), get up(8, bob)} one obtains the
sole possible model {eat(7, bob), get up(8, bob), thirsty(8, bob)}.
The second usage is of the form fail(+a, −b) ←body, where a and b are EDB
atoms with timestamps not in the future. When the rule body is satisﬁed, the
model computation restarts with a added and b removed from the EDB. For
example, the rule
fail(−eat(s, x)) ←get up(t, x), eat(s, x), t −1 ≤s, s < t
(3)
rejects a model candidate where x has eaten within one hour before getting up.
The rule correspondingly removes the eat event from the EDB and the model
computation restarts. One could further add
fail(−get up(t, x)) ←get up(t, x), eat(s, x), t −1 ≤s, s < t
(4)
to (1)–(3) as an alternative to ﬁx the problem. The principle is that as soon as
the earliest fail head is derived, the model candidate at that time is given up.
Then alternate model computations are started for all fail heads derivable for
that time. Later times are not considered. In the example, thus, both restarts
prescribed by rules (3) and (4) are tried.
1.2
Related Work and Novelty
Assigning models to logic programs as their intended meaning has been studied
for decades. We only mention the stable models semantics [14,18], its extension
for disjunctive programs [11,15], and the possible model semantics [26,27] as
the most relevant in the following discussion. Both ascribe meaning to a given
program in terms of minimal models, but diﬀer in the way disjunctive rule heads
are interpreted (exclusive vs. inclusive, respectively).

340
P. Baumgartner
Most reasoning tasks around stable models are rather complex, e.g., model
existence for propositional disjunctive programs is ΣP
2 -complete [10]. This com-
plexity translates into generate-and-test algorithms even without default nega-
tion. For instance, the tableau calculus in [22] for negation-free programs gener-
ates in its branches model candidates, whose minimality need to be tested by a
subsequent theorem prover call. Another need for generate-and-test algorithms
for stable models comes from default negation. In the general case, these algo-
rithms need to guess a stable model candidate and verify its minimality for a
certain negation-free program obtained by simpliﬁcation with this candidate, the
Gelfond-Lifschitz transformation.
In contrast, our approach avoids the intricacies of generate-and-test algo-
rithms. This is achieved by using the possible model semantics [26] and a spe-
ciﬁc concept of stratiﬁcation for dealing with default negation. The latter ﬁts in
the framework of local stratiﬁcation [25]. A similar concept of stratiﬁcation by
time has been employed for expressing greedy algorithms in Datalog [30]). The
usual stratiﬁed case, by predicates, but without quantiﬁcation within negation
was already been considered in [26].
As indicated in Sect. 1.1 our language features a fail operator for model revi-
sion. This feature is the one that possible stands out most among the other men-
tioned. A rule fail(−p(x)) ←q(x), p(x) applied to the facts {q(a), p(a)} derives
the model {q(a)}. This cannot be achieved without belief revision, as given facts
have to be satisﬁed.
Belief revision [1,24] is the process of changing beliefs to take into account
a new piece of information. It has also been studied extensively in a logic pro-
gramming context and in a general way. For instance, Schwind and Inoue [29]
consider the problem of revision by a program in a rather expressive setting,
generalized logic programs equipped with stable model semantics. The perhaps
closest approach to ours are the revision speciﬁcations of [19,20]. Revision pro-
grams generalize logic programming with stable model semantics by an explicit
deletion operator. Each revised model is obtained from the initial interpretation
by means of insertions and deletions speciﬁed by a Gelfond-Lifschitz type reduced
program. In that way, our approach is related, but simpler, as it revises only the
EDB and does not require a generate-and-test algorithm. On the other hand,
the semantics of our revisions takes timestamps into account, so that intended
revisions are only those that are derivable “now”.
The focus on the paper is not on situational awareness as such. We merely
mention that the problem has attracted interest from a logical perspective.
In earlier work [2] we proposed bottom-up model computation with a Hyper
Tableaux prover [4,23] as a component for data aggregation. In a related context
of conformance checking, the authors of [7] propose if-then rules for validating
process execution traces by means of a Prolog interpreter. Other approaches for
conformance checking include planning [9] and diagnosis of discrete dynamical
systems [8,21].
To sum up, the main novelty of our approach lies in the combination of
the possible model semantics with speciﬁc concepts of stratiﬁcation and model

Possible Models Computation and Revision – A Practical Approach
341
revision. The combination is designed to enable simple ﬁxpoint algorithms that
are sound and complete for a not too complicated declarative semantics. This is
the main theoretical contribution.
On the practical side we oﬀer a (publicly available) implementation of our
calculus, as a shallow embedding into the Scala programming language. Some-
what related, a shallow embedding into Scala has been used for monitoring event
streams over Allen’s temporal interval logic [17]. Yet, it is an uncommon imple-
mentation technique for automated reasoning systems. The practical advantages
are described in Sect. 6.
2
Preliminaries
We assume the reader is familiar with basic notions of ﬁrst-order logic and answer
set programming. See [6] and [13], respectively, for introductory texts.
A ﬁrst-order logic signature Σ = ΣP ⊎ΣF is comprised of predicate symbols
ΣP and function symbols ΣF of ﬁxed arities. We assume N ⊆ΣF , i.e., the natural
numbers are also constants of the logical language, and that ΣP contains the
arithmetic predicate symbols ΣN = {<, ≤, =, ̸=}. The ordinary predicate symbols
are ΣP \ΣN. Let X be a countably inﬁnite set of variables. Instead of introducing
a two-sorted signature we assume informally that all terms and formulas over Σ
and X are built in a sorted way.
The letters s and t usually stand for terms, x and y stand for variables, and p
and q for ordinary predicate symbols. We speak of ordinary atoms and arithmetic
atoms depending on whether the predicate symbol is ordinary or arithmetic,
respectively. For a set A of atoms let ord(A) be the set of all ordinary atoms in
A.
Intuitively, N represents timestamps (points in time), and < and ≤stand for
the strict and non-strict earlier-than relationships, respectively. We assume every
ordinary predicate symbol has arity ≥1 and its, say, ﬁrst argument ranges over N.
For any ordinary atom a = p(t1, . . . , tn) let time(a) = t1 be its timestamp. The
function symbols ΣF may contain arithmetic operations as needed to compute
with timestamps, but ΣF may not contain uninterpreted operators with N as
the result sort.
We assume the ordinary predicate symbols are partitioned as ΣP \ ΣN =
ΣEDB ⊎ΣIDB. The symbols in ΣEDB are called extensional database (EDB) pred-
icates, and the symbols in ΣIDB are the intensional database (IDB) predicates.
An EDB is a ﬁnite set of ground ΣEDB-atoms, and an IDB is a ﬁnite set of
ground ΣIDB-atoms. We may think of an EDB as a timestamped sequence of
external events, and an IDB as higher-level conclusions derived from that EDB.
Below we will exploit this distinction for computing models in a stratiﬁed way
and for deﬁning a model revision operator.
As usual, a substitution σ is a mapping from the variables to terms. A sub-
stitution is identiﬁed with its homomorphic extension to terms. Substitution
application is written postﬁx, i.e., we write tσ instead of σ(t). The domain of σ
is the set dom(σ) = {x ∈X | xσ ̸= x} and is always assumed to be ﬁnite.

342
P. Baumgartner
When z is a term, an atom, a sequence, or a set of those, let var(z) denote
the set of variables occurring in z. We say that z is ground if var(z) = ∅. A
substitution γ is a grounding substitution for z iﬀzγ is ground. In this case zγ
is also called a ground instance of z (via γ). Let gnd(z) denote the set of all
ground instances of z.
3
Stratiﬁed Programs
We are now in a position to deﬁne our main modeling tool, a variation on if-then
rules as popularized in the area of disjunctive logic programming.
A positive body is a list ⃗b = b1, . . . , bk of atoms with k ≥0. If k = 0 then ⃗b is
empty otherwise it is non-empty. (The list represents a conjunction.) A negative
body literal is an expression of the form not⃗b, where ⃗b is a non-empty positive
body. A body is a list b1, . . . , bk, not⃗bk+1, . . . , not⃗bn comprised of a (possibly
empty) positive body and (possibly zero) negative body literals. It is variable
free if var(b1, . . . , bk) = ∅. A head is one of the following:
(a) an ordinary head: a disjunction h1 ∨· · ·∨hm of IDB atoms, for some m ≥1,
or
(b) a fail head: an expression of the form fail(⃗e) where ⃗e = ±1 e1, . . . , ±k ek, for
some k ≥0, EDB atoms ei and ±i ∈{+, −}. If k = 0 then ⃗e is the empty
sequence ε, and fail(ϵ) is usually written as fail().
A rule consist of a head H and a body and is commonly written as an implication
H ←b1, . . . , bk, not⃗bk+1, . . . , not⃗bn .
(5)
By an ordinary rule (fail rule) we mean a rule with an ordinary head (fail head),
respectively. A fail set is a (possibly empty) set of ground fail heads.
Let r be a rule (5) and ⃗b = b1, . . . , bk its positive body. We say that r is
variable free iﬀvar(H) ∪var(⃗b) = ∅. This notion of variable-freeness is justiﬁed
by the fact that the extra variables var(⃗bi) \ var(⃗b) in the negative body literals
not⃗bi are implicitly existentially quantiﬁed, see Deﬁnition 5 below. We say that
r′ is a variable free instance of r via σ iﬀr′ = rσ is variable free and dom(σ) =
var(H) ∪var(⃗b). Notice that σ must not act on the extra variables as these are
shielded by quantiﬁcation.
A program is a set of rules. It is variable free if all of its rules are. Semantically,
every program R stands for the (possibly inﬁnite) variable free program vﬁnst(R)
that is obtained by taking all variable free instances of all rules in R.
The rules are to be evaluated in a bottom-up way. If a current model candi-
date satisﬁes a rule body then its head needs evaluation. An ordinary rule extends
the current model according to the possible model semantics as explained below
and a fail rule rejects the current model. If a fail rule’s head is fail() it acts like a
traditional rule with an empty head, as an integrity constraint. If the argument
list ⃗e is non-empty the fail rule “ﬁxes” the current EDB by adding (“+”) or
removing (“−”) EDB atoms and starting a new model computation.

Possible Models Computation and Revision – A Practical Approach
343
In order to admit eﬀective model computation our rules will be stratiﬁed.
Stratiﬁcation means range-restrictedness and other restrictions on variables and
negation.
Deﬁnition 1 (Stratiﬁed rule).
Let r be a rule (5) with positive body ⃗b =
b1, . . . , bk and y be a variable. The rule r is stratiﬁed wrt. y if there is a b ∈⃗b
such that time(b) = y and the following holds:
(i) var(⃗b) ⊆var(ord(⃗b)),
(ii) for every ordinary atom b ∈⃗b,
time(b) = y or time(b) = x and x ◁y ∈⃗b for some variable x and ◁∈{<
, ≤},
(iii) every negative body literal not⃗bk+1, . . . , not⃗bn is stratiﬁed, and
(iv) the head H is stratiﬁed.
In the above, a negative body literal not⃗bi is stratiﬁed if the following holds:
(i) var(⃗bi) \ var(⃗b) ⊆var(ord(⃗bi)),
(ii) for every EDB atom b ∈⃗bi,
time(b) = y or time(b) = x and x ◁y ∈⃗bi for some variable x and ◁∈{<
, ≤}, and
(iii) for every IDB atom b ∈⃗bi,
time(b) = x and x < y ∈⃗bi for some variable x.
The head H is stratiﬁed if the following holds:
(i) var(H) ⊆var(ord(⃗b)),
(ii) if H is an ordinary head h1 ∨· · ·∨hm then time(h1) = · · · = time(hm) = y.
(iii) if H is a fail head fail(⃗e) then for all ±e ∈⃗e, time(e) is an arithmetic
expression and time(e) ◁y ∈⃗b, for some ◁∈{<, ≤}.
A rule is stratiﬁed if it is stratiﬁed wrt. some variable y. A program is stratiﬁed
if each of its rules is stratiﬁed.1
Deﬁnition 1 expresses conditions on rules in terms of time-restrictedness and
range-restrictedness. The variable y stands for the latest of all timestamps among
all timestamps of the ordinary atoms in the rule body. This is made sure by
constraints x ◁y in the various parts of the deﬁnition where ◁∈{<, ≤}. More
precisely, ordinary atoms in the positive body are timestamped “≤”; ordinary
heads are timestamped “y” so that no literals timestamped in the past can be
inserted into the model (this would defy stratiﬁcation); and restarts can modify
only the past. For the ordinary atoms in negative body literals we distinguish
between EDB and IDB atoms. EDB atoms cannot be derived in heads of rules,
which aﬀords “≤”, whereas IDB atoms must be “<”.
1 Usually, stratiﬁcation is deﬁned as a property of the program as a whole, via its
call-graph.

344
P. Baumgartner
Fig. 1. Supply chain program. See Example 2 for explanations.
The remaining conditions force range-restrictedness. In the ﬁrst part, condi-
tion (i) says that every variable in a positive body atom appears also in some
ordinary positive atom; similarly for condition (i) for heads. Condition (i) for neg-
ative body atoms says that every extra variable in a negative body atom appears
also in some of its ordinary body atoms. Together these conditions make sure
that matching a rule’s ordinary atoms against a ground candidate model always
removes all variables. This way, all arithmetic expressions can be evaluated and
only ground heads can be derived.
Example 1 (Stratiﬁed rule).
Let ΣEDB = {p} and ΣIDB = {d}. The rule
d(x3, x1) ←x1 < x3, p(x1), p(x3), not (x1 < x2, x2 < x3, p(x2)) is stratiﬁed
wrt. x3 (= time(p(x3))). It collects in d the timestamps between consecutive
p-events. For example, given the set I = {p(2), p(4), p(7), p(13)}, the rule when
applied exhaustively derives d(4, 2), d(7, 4) and d(13, 7) but not, e.g, d(7, 2).
The extra variable x2 in its negative body literal is implicitly existentially
quantiﬁed.
⊓⊔

Possible Models Computation and Revision – A Practical Approach
345
Example 2 (Supply chain). The program in Fig. 1 illustrates a possible use of
our approach in a supply chain application. It is written in the concrete input
syntax of our implementation, the Fusemate system (Sect. 6).
The signature is ΣEDB = {Load, Unload, SameBatch} and ΣIDB = {In}. A
ﬂuent Load(time, obj, cont) expresses that at the given time an object obj is loaded
into a container cont, similarly for Unload. A ﬂuent In(time, obj, cont) says that
at the given time an object obj is inside the container cont.
With this interpretation, the rules (1) and (2) for the In relation should be
obvious. Rule (3) is a frame axiom for the In relation. That is, it states when an
In-ﬂuent carries over to the next timestamp: an object remains in a container if
neither it nor a container containing it is unloaded from the container. The body
atom Step(time, prev) holds true if prev is the most recent timestamp preceding
time. The Step relation is “built-in” into Fusemate for convenience.
Rule (4) ﬁxes the problem of a “missing” unloading event by inserting one
into the EDB at a speculated time (time+prev)/2. This rule will become clearer
in Example 3 below, where we discuss the program in conjunction with a concrete
EDB.
Rule (5) says that only items that are in a container can be unloaded in the
next step. Rule (6) demands loading prior to unloading. The other rules will also
be discussed below.
⊓⊔
4
Semantics
The possible model semantics [26,27] associates to a disjunctive program sets of
possible facts that might have been true in the actual world. (This is already a
good ﬁt for situational awareness.) We extend it to our stratiﬁed programs with
fail rules.
Let Th(ΣN) be the set of all ground time atoms that are true in the standard
model of natural number arithmetic. For a set A of ground ordinary atoms deﬁne
I(A) = A∪Th(ΣN) which represents the Herbrand Σ-interpretation that assigns
true to a ground atom a if and only if a ∈I(A).
Deﬁnition 2 (Rule semantics). A set A of ground ordinary atoms satisﬁes
a variable free body B = b1, . . . , bk, not⃗bk+1, . . . , not⃗bn, written as A |= B, if
{b1, . . . , bk} ⊆I(A) and (gnd(⃗bk+1) ∪· · · ∪gnd(⃗bn)) ∩I(A) = ∅.
The set A satisﬁes a variable free ordinary rule h1 ∨· · · ∨hm ←B if A |= B
entails {h1, . . . , hm}∩I(A) ̸= ∅. It is a model of a set R of variable free ordinary
rules, written as A |= R, iﬀit satisﬁes every rule in R.
The fail set of A and a set of variable free fail rules R is the set F = {fail(⃗e) |
there is a rule fail(⃗e) ←B ∈R such that A |= B}. This is written as A |=fail
R F.
Satisfaction of a variable free body according to Deﬁnition 2 is equivalent to
satisfaction of the ﬁrst-order logic formula b1 ∧· · · ∧bk ∧¬(∃⃗xk+1. ∧⃗bk+1) ∧
· · · ∧¬(∃⃗xn. ∧⃗bn) in the interpretation I(A), where ⃗xj = var(⃗bj), for all j =
k + 1, . . . , n. .

346
P. Baumgartner
Deﬁnition 2 is, in fact, somewhat more general than needed for deﬁning the
possible models semantics of logic programs. Possible models interpret disjunc-
tive heads inclusively, in all possible ways. This is expressed in the following
deﬁnition.
Deﬁnition 3 (Split program [26,27]).
Let R be a variable free program. A
split program of R is any program obtained from R by replacing every ordinary
rule h1 ∨· · · ∨hm ←B by the normal ordinary rules (called split rules) h ←B,
for every h ∈H, where H is some non-empty subset of {h1, . . . , hm}.
In [26,27], Sakama et al. deﬁne the possible model semantics (also) for disjunctive
programs without negation. Our stratiﬁed case admits a similar deﬁnition.
For any program R let R+ (R−) be the set of all ordinary (fail) rules of R,
respectively.
Deﬁnition 4 (Satisﬁcation of stratiﬁed programs). Let P be a stratiﬁed
program, F a fail set, E an EBD and I an IDB. We write (E, I) |=fail
P F if there
is a split program S of vﬁnst(P) such that all of the following hold:
(i) E ∪I |= S+
(E ∪I is a model of the ordinary rules)
(ii) E ∪J |= S+ for no J ⊊I
(I is minimal)
(iii) E ∪I |=fail
S−F
(F is the triggered fail heads)
If F = ∅then we say that (E, I) satiﬁes P and writte (E, I) |= P.
As an example (without time), if E = {p} and R = {q ∨r ←p, q ←r, s ←s}
then {p, q, s} |= R, but only ({p}, {q}) and ({p}, {r, q}) satisfy R in the sense of
Deﬁnition 4.
The purpose of a program P is to compute all extensions (E, I) of a given
EDB E that satisfy P. For failed such attempts, P also speciﬁes ways to revise
E, if any, as early as possible, leading to new tries. The following Deﬁnition 5
make this precise.
For a ground fail head fail(⃗e) and ground EDB E let upd(E,⃗e) be the EDB
obtained from E by ﬁrst adding all EDB atoms e such that ⃗e contains the
expression +e, and then deleting all all EDB atoms e such that ⃗e contains −e. For
any set A of ground ordinary atoms and t ∈N let A≤t = {a ∈A | time(a) ≤t};
analogously for A<t.
Deﬁnition 5 (Possible models of stratiﬁed programs). Let P be a strat-
iﬁed program and Einit an EDB. Let E be the smallest set of EDBs containing
Einit and satisfying, for all E ∈E, timestamps t in E, IDBs I and fail sets F:
If (E≤t, I) |=fail
P F and there is no J ⊆I and G ̸= ∅such that (E<t, J) |=fail
P G
then {upd(E,⃗e) | fail(⃗e) ∈F and ⃗e ̸= ε} ⊆E.
The set E is called the restart EDBs induced by P and Einit. Any pair (E, I)
such that E ∈E and (E, I) |= P is called a possible model of P and Einit, written
as (Einit, E, I) |= P. Let modsP (Einit) = {(E, I) | (Einit, E, I) |= P} be all such
possible models.

Possible Models Computation and Revision – A Practical Approach
347
The set E in Deﬁnition 5 contains a restart EDB E apart from Einit if and only
if E is obtained from some earliest time fail set F from another restart EDB
in E. This excludes fail sets that may otherwise be additionally derivable at a
later time. This was a design decision in support of “anytime” reasoning, for not
having to consider future events.
Example 3 (Supply chain, Example 2 continued). Consider the following EDB
Einit consisting of loading and unloading events:
SameBatch(10, Set(tomatoes, apples))
Load(40, container, ship)
Load(10, tomatoes, pallet)
Unload(60, apples, pallet)
Load(20, pallet, container)
The intuitive meaning of the Load atoms between times 10 and 40 should be
obvious. All what is reported at time 60 is that apples are unloaded from the
pallet. However, this is suspicious from a (practical) completeness and consis-
tency perspective. First, it can be alleged that some unloading events went under
unreported. Before an item (apples) can be unloaded from a pallet that was
loaded earlier into a container, the pallet needs to be unloaded from the con-
tainer ﬁrst, and that container must have been unloaded from the ship. Such
reports are missing. Second, loading of tomatoes does not go together well with
the unloading of apples later. This could be a reporting inconsistency or a report-
ing incompleteness if indeed apples were (also) loaded earlier.
All these plausible explanations are provided by (E1, I1), (E2, I2), and
(E3, I3), the three possible models of Einit and the program in Fig. 1. For space
reasons we list only their EDB components, which are as follows:
E1
Load(10, tomatoes, pallet)
Unload(45, container, ship)
Unload(50, pallet, container)
Unload(60, tomatoes, pallet)
E2
Load(10, tomatoes, pallet)
Load(10, apples, pallet)
Unload(45, container, ship)
Unload(50, pallet, container)
Unload(60, apples, pallet)
E3
Load(10, apples, pallet)
Unload(45, container, ship)
Unload(50, pallet, container)
Unload(60, apples, pallet)
In each of these models, the missing unloading events Unload(45, container, ship)
and Unload(50, pallet, container) are added by repeated application of rule (4).
Generally speaking, rule (4) inserts an Unload of the “containing container” the
object to be unloaded from is in. The rules (7) – (9) all ﬁx the “unloading
apples vs. loading tomatoes” problem. Rule (7) leads to (E1, I1), rule (8) leads
to (E2, I2), and rule (9) leads to (E3, I3). Each of these rules tests whether an
object (apples) is swappable with another object (tomatoes) for the purpose of
model revision, which is the case if the SameBatch relation says so. Notice that if
Einit had, say Unload(60, oranges, pallet) instead of Unload(60, apples, pallet) then
none of the rules (7) – (9) is applicable and no possible model exists.
⊓⊔
5
Model Computation
This section introduces our calculus for computing possible models of stratiﬁed
programs. It borrows some terminology from tableau calculi. A path p is a triple

348
P. Baumgartner
(E, I, t) where E is an EDB, I is an IDB and t ∈N is a timestamp. Intuitively, p
represents the interpretation I((E∪I)≤t). An initial path is of the form (E, ∅, 0).
A tableau is a ﬁnite set of paths.2
Let B = b1, . . . , bk, not⃗bk+1, . . . , not⃗bn be the body of a variable free
stratiﬁed rule and A a set of ground ordinary atoms. A substitution σ with
dom(σ) = var(b1, . . . , bk) is a body matcher for B on A, written as (B, σ) ⊑A,
if the following holds:
(i) {b1σ, . . . , bkσ} ⊆A ∪Th(ΣN), and
(ii) for no i = k + 1 . . . n there is a grounding substitution γ for ⃗biσ such that
⃗biσγ ⊆A ∪Th(ΣN).
Note 1 (Computing body matchers).
The deﬁnition of body matchers only
applies to bodies of stratiﬁed rules. It is easy to see that a body matcher σ,
if any exists, can be found by computing a simultaneous matching substitution
σ for the ordinary atoms among b1, . . . , bk to A. Similarly for the substitution
γ in condition (ii). Furthermore, stratiﬁcation guarantees that all arithmetic
atoms for testing conditions (i) and (ii) are necessarily ground and hence can be
evaluated.
⊓⊔
An inference rule is a schematic expression of the form p ⇒p1, . . . , pk where
p and pj are paths, for all 1 ≤j ≤k, where k ≥0. It means that the premise p
is to be replaced by the conclusions p1, . . . , pk. An inference is an instance of an
inference rule.
In the following, P is a stratiﬁed program and σ is a substitution such that
rσ is a variable free instance of a rule r that is clear from the context.
Ext: (E, I, t) ⇒(E, I ∪H1, t), . . . , (E, I ∪Hk, t)
if P contains an ordinary rule h1 ∨· · · ∨hk ←B such that
{H1, . . . , Hk} = {H | (B, σ) ⊑(E ∪I)≤t and ∅⊊H ⊆{h1σ, . . . , hkσ}}
Restart: (E, I, t) ⇒(upd(E,⃗e1), ∅, 0), . . . , (upd(E,⃗ek), ∅, 0)
if k ≥1 and {⃗e1, . . . ,⃗ek} = {⃗eσ | fail(⃗e) ←B ∈P, ⃗e ̸= ε and (B, σ) ⊑
(E ∪I)≤t}.
Fail: (E, I, t) ⇒
if P contains a rule fail() ←B and there is a σ such that (B, σ) ⊑(E ∪I)≤t.
Jump: (E, I, t) ⇒(E, I, s) if s is the least timestamp in E with t < s.
The Ext rule extends I to satisfy all split rules for each case H of some instance
of an ordinary rule in P whose body is satisﬁed by (E ∪I)≤t; Restart replaces
the current path with all initial paths as per the non-empty fail rules after
Ext is exhausted; Fail also terminates the current path but is to be applied
2 This terminology is inspired by visualizing a set of paths as a tableau in the usual
sense. For that, a path (E, I, t) leads to a branch whose nodes are labeled with
the atoms E ∪I and the branch as a whole is labeled with t. Moreover, the way
the calculus constructs these paths sets indeed corresponds to a typical tableau
construction. See, e.g., [3].

Possible Models Computation and Revision – A Practical Approach
349
only if Restart doesn’t; Jump advances the current time bound t. The following
formalizes this intuition.
An initial path (E, ∅, 0) is new wrt. a tableau T iﬀthere is no I and no t such
that (E, I, t) ∈T. Let Einit be an input EDB. A derivation D (from Einit and P)
is a sequence (T)i≥0 of tableaus D = (T0 = {(Einit, ∅, 0)}), T1, T2, . . . such that,
for all i ≥0, there is a selected path p ∈Ti and Ti+1 = (Ti \ {p}) ∪{p1, . . . , pk}
where:
(a) p ⇒p1, . . . , pk by Ext and {p1, . . . , pk} ̸⊆Ti,
(b) p
⇒
q1, . . . , qm by Restart and {p1, . . . , pk}
=
{p
∈
{q1, . . . , qm}
|
p is new wrt. Ti},
(c) p ⇒
by Fail and k = 0, or
(d) p ⇒p1 by Jump and k = 1.
In addition, the inference rules must be prioritized in this order. That is, if Ti+1 is
obtained from Ti by, say, case (c) , then there is no tableau that can be obtained
from Ti by case (a) or case (b) with the same selected path p; analogously for
the other cases.
The derivation D is exhausted if it is ﬁnite and no inference rule is applicable
to its ﬁnal tableau Tn, for no p ∈Tn. In this case the computed models of D is
the set M(D) = {(E, I) | (E, I, t) ∈Tn for some t ∈N}.
Figure 2 is a graphical illustration of a derivation and its computed models.
Fig. 2. Illustration of a hypothetical derivation. The root of each sub-tableau is
labeled with the EDB in that sub-derivation. The ﬁrst sub-tableau has two Restart
inferences, leading to the second and third sub-tableau, where E1 = upd(Einit,⃗e0
0),
E2 = upd(Einit,⃗e1
0). The isolated fail()s do not cause a Restart, they cause Fail. The
computed models are (Einit, I0
0), (Einit, I1
0), (E1, I0
1), etc.
Theorem 1 (Soundness and completeness). Assume a signature Σ without
k-ary function symbol, for k > 0. Let P be a stratiﬁed program and Einit an
EDB. Assume an exhausted derivation D from Einit and P. Then M(D) =
modsP (Einit).

350
P. Baumgartner
Proof. (Sketch) Let Tn be the ﬁnal tableau of D. For soundness, assume M(D) ̸=
∅and chose any (E, I) ∈M(D) arbitrary. That is, (E, I, t) ∈Tn, for some t. We
have to show (E, I) ∈modsP (Einit), equivalently (Einit, E, I) |= P.
The EDB E is either Einit or derived from Einit through, say, k > 0 interme-
diate EDBs by Restarts. By induction on k one can show that, on the semantic
side, E is a restart induced by P and Einit, i.e., E ∈E in Deﬁnition 5. This follows
from the deﬁnition of derivations. In particular, the earliest-time requirement in
Deﬁnition 5 is matched by prioritizing Restart over Fail and Jump.
With the EDB E traced down in E, it remains to prove (E, I) |= P.
With the stratiﬁcation of P (Deﬁnition 1) this is rather straightforward. Range-
restrictedness makes sure that only ground heads are derivable. The Ext inference
rule achieves on-the-ﬂy splitting and only for those variable-free instances of rules
whose body is satisﬁed, which are the only ones that count (details in [26,27]).
The requirements on EDB/IDB atoms in negative body literals (≤vs. <) in
Deﬁnition 1 entail that utilizing body matchers in derivations is correct wrt. the
rule semantics in Deﬁnition 2.
The timed setting requires a layered ﬁxpoint iteration. Stratiﬁcation makes
sure that stepwisely incrementing in derivations the time bound by Jump infer-
ences is all that is needed to comply with the “unstepped” possible model
semantics in Deﬁnitions 4 and 5. In particular, no ordinary rule can derive in
its head a conclusion with a timestamp earlier than the latest timestamp in
its body. This makes the derivability relation monotonic wrt. increasing time
stamps (usual stratiﬁcation by predicates is covered in [26]). Moreover, because
the given derivation is exhausted, no ﬁxpoint iteration can stop prematurely.
For completeness, assume modsP (Einit)
̸=
∅and chose any (E, I)
∈
modsP (Einit) arbitrary. We have to show (E, I) ∈M(D). The ﬁrst step is to
locate in D the sub-tableau with E at its root, by tracing E ∈E from Einit. The
next step then is to argue for the completeness of the sub-tableau construction
with that ﬁxed E, giving (E, I) ∈Tn. All that uses similar considerations as the
soundness proof above.
One important detail is that Ext is the highest-priority inference rule. This
makes sure that no model candidate is terminated too early, so that all possi-
ble branching out takes place. As a consequence, all possible fail heads for the
current time point will be derived. The requirement that there are no proper
(non-constant) function symbols make sure that the layered ﬁxpoint computa-
tion of derivations terminates and ﬁnds (E, I).
⊓⊔
6
Implementation
It is not too diﬃcult to translate the model computation calculus of Sect. 5 into
a proof procedure. Tableaux can be represented in a direct way, as a set of paths
(E, I, t). In terms of Fig. 2, the proof procedure can implement a one-branch-at-
a-time approach for one sub-tableau at a time, for space eﬃciency, embedded
in an adapted given-clause loop algorithm. Only the EDBs, not the full models,
need to be remembered to implement the “Progress” condition for derivations
(cf. Sect. 5).

Possible Models Computation and Revision – A Practical Approach
351
A concrete implementation could based on, e.g., hyper resolution with split-
ting [5] or hyper tableaux [3] calculi. Our implementation however is imple-
mented in an unusual way, by shallow embedding in Scala.3
Scala [28] is a modern high-level programming language that combines
object-oriented and functional programming styles. It has functions as ﬁrst-class
objects and supports user-deﬁnable pre-, post- and inﬁx syntax. With these
features, Scala is suitable as a host language for embedding domain-speciﬁc lan-
guages (DSLs). (See, e.g., [16] for a Scala DSL for runtime veriﬁcation.) In our
case, the logic program rules are nothing but partial functions, instantiating and
evaluating a rule body reduces to partial function deﬁnedness, and deriving a
rule head reduces to executing a partial function on a deﬁned point. An advan-
tage of the DSL approach is that is is easy to interface with external systems,
e.g., databases, in particular if they have a Java interface.
Moreover, it is easy to make the full Scala language and its associated data
structure libraries available for writing rules. (There is no theoretical problem
doing that as long as all Scala expressions are ground and, hence, can be eval-
uated.) For example, the EDB in Example 3 has the Scala-set forming term
Set(tomatoes, apples), and the rules (7) - (9) in Fig. 1 test in their last lines
membership in such sets by Scala expressions.
While EDBs are naturally written as Scala source code, logic program rules
are usually written in a (much) more convenient syntax and translated into the
required format by the Scala macro mechanism. See Listing 1.1 for an example.4
Listing 1.1. Sample EDB/IDB declarations and a rule. Some unimportant declarations left away.
1
2
3
4
5
6
In Listing 1.1, line 1 sets the concrete type for time to Int, the Scala integers.
A realistic application could use a rich time class like java.time.OﬀsetDataTime.
Lines 2, 3 and 4 deﬁne the EDB and IDB signature of the supply chain Example 2
by extension of the Scala classes EDBAtom and IDBAtom. The Load relation
says that the object obj was loaded into the container cont at time time. The
In relation says that obj was in container cont at time time. The time argument
must be named time, but all other arguments and their types can be freely
chosen. For simplicity we used strings, except for the SameBatch relation, which
has a set of strings for its objs parameter.
Line 6 deﬁnes a list with one rule that expresses the transitivity of the In
relation (rule (2) in Fig. 1). Line 5 is an annotation that tells the compiler to
3 Our implementation, the Fusemate system, is available at https://bitbucket.csiro.
au/users/bau050/repos/fusemate/.
4 Scala case classes are records in language agnostic terminology.

352
P. Baumgartner
expand the subsequent deﬁnition by a macro named rules. Indeed, without the
help of a macro the rule would not compile because of undeﬁned variables in the
rule. The macro expansion of the rule in line 6 is the Scala function in Listing
1.2.
Listing 1.2. Macro expansion of the transitivity rule.
1
2
3
4
The anonymous function in Listing 1.2 is passed in its formal parameter I a
set of atoms which will always be the “current interpretation” (E ∪I)≤t where
p = (E, I, t) is the current path. The set I is needed for evaluating negative
body literals and is not relevant for this example. The function returns a partial
function in the form of a case expression. The pattern in the case expression are
the ordinary atom of the positive body literals. These are the ones that need
to be matched to the atoms of (E ∪I)≤t for rule evaluation (see Note 1). The
matching is done by applying the partial function to all tuples (of the proper
arity) of elements from the current interpretation. If the application succeeds,
i.e., if the case pattern match succeeds and the additional if -condition is satis-
ﬁed, the partial function body (to the right of =>) is executed, which results in
an instantiated head. If the head is disjunctive, all non-empty subsets are taken.
This gives all split programs (cf. Deﬁnition 3) on the ﬂy. The resulting sets are
collected in one sweep for each rule and are candidates for extending the current
path. fail-rules are processed in a similar way.
Notice that the pattern of a case expression needs to be linear, hence
the renaming apart of pattern variables and the obvious equalities in the if -
condition. Notice also that substitutions are not explicitly represented, they are
hidden in the Scala runtime system.
It remains to be explained how arithmetic atoms and negative body literals
are macro-expanded. By way of example, consider, say, rule (7) in Fig. 1. Its
arithmetic condition t < time is simply conjoined to the if-condition of the rule’s
case expression. An if-expression like if((b contains obj) and (b contains o)) is
a backdoor for adding arbitrary Boolean-valued Scala code to that condition
(“contains” belongs to the Scala library and tests set membership). Recall that
all variables in arithmetic atoms will always be ground instantiated by matching
and hence can be evaluated. This must be be extended to Scala conditions. In
the implementation, any such free variable would be detected as a compile time
error.
The body literal not(load(t, obj, cont), t < time) is expanded into a partial fun-
ction
.
It is structurally the same as the one in Listing 1.1 except that the binding
of the variables obj and cont in the surrounding context need to respected, giv-
ing the stated equalities. Now, the if-condition of the case expression of the
surrounding rule (i.e., rule 7) is conjoined with a Scala expression for testing

Possible Models Computation and Revision – A Practical Approach
353
that the partial function does not return Abort, for no tuple of elements from
the set I explained above.
The implementation supports some more features not further discussed here:
rules can be deﬁned locally within case classes; literals – possibly negated atoms –
can be used anywhere instead of atoms; a “strong fail” head operator terminates
a model candidate without restarts, e.g., for classical negation: a and ¬a together
is unﬁxable; and Scala conditions can access the interpretation (E ∪I)≤t for,
e.g., concise data aggregation.
7
Conclusions
This paper presented a novel calculus and implementation for situational aware-
ness applications. The approach is meant to be practical in three ways: ﬁrst,
realistic situational awareness requires being able to reason with incomplete
or erroneous data. Moreover, “anytime” reasoning is needed, meaning that a
model can be derived, rejected or repaired at any current time. Our approach
supports these needs with a (disjunctive) logic programming framework with
timed predicates, stratiﬁed negation and a novel model revision operator. Sec-
ond, thanks to implementation on top of Scala, it is trivial to attach arbitrary
Scala code and Java libraries. (It would not be diﬃcult to extend the calculus
respectively.) For instance, reading in XML data and making them available as
terms (Scala case classes) is easy. Third, we strived for a “cheap” model compu-
tation procedure that makes do without additional generate-and-test needs. As
such, it is perhaps more adequately seen as study in pushing bottom-up ﬁrst-
order logic model computation technology rather than slimmed down answer-set
programming.
As for future work, one interesting idea is to add probabilities to the picture,
say, in the way ProbLog extends Prolog. This is obviously useful because, e.g.,
some explanations (models) or repairs (restarts) are more likely than others.
Another idea is to view the model computation as runtime veriﬁcation. This view
suggests that (probabilistic) linear temporal logic could serve as an additional
useful high-level speciﬁcation language component.
Acknowledgements. I thank the reviewers for their constructive comments. Yuzhou
Chen discovered an error in an earlier version of the paper.
References
1. Alchourr`on, C.E., G¨ardenfors, P., Makinson, D.: On the logic of theory change:
partial meet contraction and revision functions. J. Symb. Logic 50, 510–530 (1985)
2. Baader, F., et al.: A novel architecture for situation awareness systems. In: Giese,
M., Waaler, A. (eds.) TABLEAUX 2009. LNCS (LNAI), vol. 5607, pp. 77–92.
Springer, Heidelberg (2009). https://doi.org/10.1007/978-3-642-02716-1 7
3. Baumgartner, P., Furbach, U., Niemel¨a, I.: Hyper tableaux. In: Alferes, J.J.,
Pereira, L.M., Orlowska, E. (eds.) JELIA 1996. LNCS, vol. 1126, pp. 1–17. Springer,
Heidelberg (1996). https://doi.org/10.1007/3-540-61630-6 1

354
P. Baumgartner
4. Baumgartner, P., Furbach, U., Pelzer, B.: Hyper tableaux with equality. In: Pfen-
ning, F. (ed.) CADE 2007. LNCS (LNAI), vol. 4603, pp. 492–507. Springer, Hei-
delberg (2007). https://doi.org/10.1007/978-3-540-73595-3 36
5. Baumgartner, P., Schmidt, R.: Blocking and other enhancements for bottom-up
model generation methods. J. Autom. Reason. 64, 197–251 (2019). https://doi.
org/10.1007/s10817-019-09515-1
6. Bradley, A., Manna, Z.: The Calculus of Computation. Springer, Heidelberg (2007).
https://doi.org/10.1007/978-3-540-74113-8
7. Chesani, F., Mello, P., Montali, M., Riguzzi, F., Sebastianis, M., Storari, S.: Check-
ing compliance of execution traces to business rules. In: Ardagna, D., Mecella, M.,
Yang, J. (eds.) BPM 2008. LNBIP, vol. 17, pp. 134–145. Springer, Heidelberg
(2009). https://doi.org/10.1007/978-3-642-00328-8 13
8. Cordier, M., Thi´ebaux, S.: Event-based diagnosis for evolutive systems. In: Pro-
ceedings of 5th International Workshop on Principles of Diagnosis, pp. 64–69 (1994)
9. De Giacomo, G., Maggi, F.M., Marrella, A., Sardina, S.: Computing trace align-
ment against declarative process models through planning. In: Proceedings of 26th
International Conference on Automated Planning and Scheduling (ICAPS) (2016)
10. Eiter, T., Gottlob, G.: Complexity results for disjunctive logic programming and
application to nonmonotonic logics. In: Proceedings of the 1993 International Sym-
posium on Logic Programming, ILPS 1993, pp. 266–278. MIT Press, Cambridge
(1993)
11. Eiter, T., Gottlob, G., Mannila, H.: Disjunctive datalog. ACM Trans. Database
Syst. 22, 364–418 (2001)
12. Endsley, M.: Toward a theory of situation awareness in dynamic systems. Hum.
Factors J.: J. Hum. Factors Ergon. Soc. 37(1), 32–64 (1995)
13. Gelfond, M.: Answer sets. In: van Harmelen, F., Lifschitz, V., Porter, B. (eds.)
Handbook of Knowledge Representation, Foundations of Artiﬁcial Intelligence, vol.
3, pp. 285–316. Elsevier, Amsterdam (2008)
14. Gelfond, M., Lifschitz, V.: The stable model semantics for logic programming. In:
Kowalski, R., Bowen, K. (eds.) Proceedings of the 5th International Conference on
Logic Programming, Seattle, pp. 1070–1080 (1988)
15. Gelfond, M., Lifschitz, V.: Classical negation in logic programs and disjunc-
tive databases. New Gener. Comput. 9, 365–385 (1991). https://doi.org/10.1007/
BF03037169
16. Havelund, K., Joshi, R.: Modeling rover communication using hierarchical state
machines with scala. In: Tonetta, S., Schoitsch, E., Bitsch, F. (eds.) SAFECOMP
2017. LNCS, vol. 10489, pp. 447–461. Springer, Cham (2017). https://doi.org/10.
1007/978-3-319-66284-8 38
17. Kauﬀman, S., Havelund, K., Joshi, R.: nfer – a notation and system for inferring
event stream abstractions. In: Falcone, Y., S´anchez, C. (eds.) RV 2016. LNCS, vol.
10012, pp. 235–250. Springer, Cham (2016). https://doi.org/10.1007/978-3-319-
46982-9 15
18. Lifschitz, V.: Action languages, answer sets, and planning. In: Apt, K.R., Marek,
V.W., Truszczynski, M., Warren, D.S. (eds.) The Logic Programming Paradigm:
A 25-Year Perspective, pp. 357–373. Springer, Heidelberg (1999). https://doi.org/
10.1007/978-3-642-60085-2 16
19. Marek, V.W., Truszczy´nski, M.: Revision speciﬁcations by means of programs. In:
MacNish, C., Pearce, D., Pereira, L.M. (eds.) JELIA 1994. LNCS, vol. 838, pp.
122–136. Springer, Heidelberg (1994). https://doi.org/10.1007/BFb0021968
20. Marek, V.W., Truszczy´nski, M.: Revision programming. Theoret. Comput. Sci.
190(2), 241–277 (1998)

Possible Models Computation and Revision – A Practical Approach
355
21. McIlraith, S.: Toward a theory of diagnosis, testing and repair. In: Proceedings of
5th International Workshop on Principles of Diagnosis, pp. 185–192 (1994)
22. Niemel¨a, I.: A tableau calculus for minimal model reasoning. In: Miglioli, P.,
Moscato, U., Mundici, D., Ornaghi, M. (eds.) TABLEAUX 1996. LNCS, vol. 1071,
pp. 278–294. Springer, Heidelberg (1996). https://doi.org/10.1007/3-540-61208-
4 18
23. Pelzer, B., Wernhard, C.: System description: E- KRHyper. In: Pfenning, F. (ed.)
CADE 2007. LNCS (LNAI), vol. 4603, pp. 508–513. Springer, Heidelberg (2007).
https://doi.org/10.1007/978-3-540-73595-3 37
24. Peppas, P.: Chapter 8 belief revision. In: van Harmelen, F., Lifschitz, V., Porter,
B. (eds.) Handbook of Knowledge Representation, Foundations of Artiﬁcial Intel-
ligence, vol. 3, pp. 317–359. Elsevier, Amsterdam (2008)
25. Przymusinski, T.C.: Chapter 5 - on the declarative semantics of deductive
databases and logic programs. In: Minker, J. (ed.) Foundations of Deductive
Databases and Logic Programming, pp. 193–216. Morgan Kaufmann, Burlington
(1988)
26. Sakama, C.: Possible model semantics for disjunctive databases. In: Kim, W.,
Nicholas, J.M., Nishio, S. (eds.) Proceedings First International Conference on
Deductive and Object-Oriented Databases (DOOD-89), pp. 337–351. Elsevier
(1990)
27. Sakama, C., Inoue, K.: An alternative approach to the semantics of disjunctive
logic programs and deductive databases. J. Autom. Reason. 13, 145–172 (1994).
https://doi.org/10.1007/BF00881915
28. The Scala Programming Language. https://www.scala-lang.org
29. Schwind, N., Inoue, K.: Characterization of logic program revision as an extension
of propositional revision. Theory Pract. Logic Program. 16(1), 111–138 (2016)
30. Zaniolo, C.: Expressing and supporting eﬃciently greedy algorithms as locally
stratiﬁed logic programs. Technical Communications of ICLP 2015 1433 (2015)

SGGS Decision Procedures
Maria Paola Bonacina and Sarah Winkler(B)
Universit`a degli Studi di Verona, Verona, Italy
{mariapaola.bonacina,sarahmaria.winkler}@univr.it
Abstract. SGGS (Semantically-Guided Goal-Sensitive reasoning) is a
conﬂict-driven ﬁrst-order theorem-proving method which is refutation-
ally complete and model complete in the limit. These features make it
attractive as a basis for decision procedures. In this paper we show that
SGGS decides the stratiﬁed fragment which generalizes EPR, the PVD
fragment, and a new fragment that we dub restrained. The new class has
the small model property, as the size of SGGS-generated models can be
upper-bounded, and is also decided by hyperresolution and ordered reso-
lution. We report on experiments with a termination tool implementing
a restrainedness test, and with an SGGS prototype named Koala.
1
Introduction
Many applications of automated reasoning require to combine the decidability of
satisﬁability with an expressive logic. Since ﬁrst-order theorem proving is only
semidecidable, the quest for decidable fragments of ﬁrst-order logic is key in
advancing the ﬁeld, and many classes of formulæ were shown decidable. Without
claiming completeness (see [15,18,22] for surveys), we mention: the Bernays-
Sch¨onﬁnkel class, also known as EPR for eﬀectively propositional [3,8,19,37,39];
the Ackermann class [2,23]; the monadic class with and without equality [2,
5,23]; the positive variable dominated (PVD) fragment [17]; the two-variable
fragment (FO2) [21]; the guarded fragment [4,20]; the modal fragment [9,31],
which is included in the EPR [22], FO2 [32], and guarded [4] fragments; and the
stratiﬁed fragment [1,25,36], which generalizes EPR. However, many theorem
proving problems from the practice fall in none of these classes.
Example 1. Problem HWV036-2 from TPTP 7.3.0 [41] speciﬁes a full-adder in
51 clauses, including for instance:
¬andok(x) ∨¬1(in1(x)) ∨¬1(in2(x)) ∨1(out1(x)),
¬lor(x) ∨orok(x) ∨error(x),
¬halfadd(x) ∨connection(in1(x), in1(or1(x))),
¬fulladd(x) ∨halfadd(h1(x)).
This set is satisﬁable, which means that termination of a theorem prover is
a priori not guaranteed. However, it is neither EPR (∃∗∀∗ϕ formulæ with ϕ
quantiﬁer- and function-free), nor Ackermann (∃∗∀∃∗ϕ formulæ with ϕ as above)
nor FO2 (only two variables, no functions), nor monadic (only unary predicates,
no functions). One can also check that it is neither guarded nor stratiﬁed.
This research was funded in part by grant “Ricerca di base 2017” of the Universit`a
degli Studi di Verona. Authors are listed alphabetically.
c
⃝Springer Nature Switzerland AG 2020
N. Peltier and V. Sofronie-Stokkermans (Eds.): IJCAR 2020, LNAI 12166, pp. 356–374, 2020.
https://doi.org/10.1007/978-3-030-51074-9_20

SGGS Decision Procedures
357
As refutational completeness guarantees termination on unsatisﬁable inputs,
if one can prove termination of an inference system on satisﬁable inputs in a
certain class, any strategy given by that inference system and a fair search
plan is a decision procedure for satisﬁability in that class. Here we consider
Semantically-Guided Goal-Sensitive reasoning (SGGS) [13,14], that is a refuta-
tionally complete instance-based theorem-proving method especially suitable for
decision procedures: SGGS is model-based (it searches for a model by building
candidates), semantically guided (the search is guided by a ﬁxed initial interpre-
tation), conﬂict-driven (it applies inferences such as resolution only to explain
conﬂicts), proof conﬂuent (it never needs to undo inferences), and model com-
plete in the limit (if the input is satisﬁable, the limit of the derivation represents
a model), so that model generation is guaranteed if termination is.
This paper shows that SGGS decides the stratiﬁed fragment [1], which
includes EPR and ﬁnds application in veriﬁcation [1,25,36], and the PVD frag-
ment. Then, we discover a new decidable class named the restrained fragment,
and show that SGGS, ordered resolution, and hyperresolution all decide it. Since
it is possible to compute bounds on the size of SGGS-generated models, this
new class enjoys the small model property. We give a suﬃcient condition for
membership in the restrained fragment that can be tested automatically by
termination tools for rewriting. The relevance of this new class is evaluated
empirically by applying this test to problems in TPTP. For instance, the axiom-
atization in Example 1, as well as all the TPTP problems including it, turn
out to be restrained. We also summarize the outcomes of experiments with an
SGGS prototype, named Koala, built reusing code from Konstantin Korovin’s
iProver [24,26].
The paper is structured as follows. Since the stratiﬁed fragment has sorts,
Sect. 2 presents SGGS for a language with sorts. Section 3 shows that SGGS
decides the stratiﬁed fragment. In Sect. 4 we deﬁne the restrained fragment,
establish the small model property, and prove that SGGS decides both this class
and PVD. Ordered resolution and hyperresolution also decide restrained sets
(Sect. 5). The experimental results are reported in Sect. 6, and Sect. 7 concludes
the paper.
2
Preliminaries: SGGS for Many-Sorted Logic
Let S be a set of clauses in many-sorted logic with non-empty sorts (there is a
ground term for every sort). We use a, b for constants, P, Q for predicates, f, g for
functions, w, x, y, z for variables, t, u for terms, L, M for literals, at(L) for L’s
atom, C, D for clauses, Var(C) for the set of variables in C, α, σ for substitutions,
I, J for interpretations, and we extend the at notation to sets of literals, clauses,
and sets of clauses. C+ and C−are the disjunctions of the positive and negative
literals in C, respectively; C is positive if C = C+ and negative if C = C−.
In SGGS, a clause C may have a constraint A, written A ▷C. An atomic
SGGS constraint is true, false, t ≡u, and top(t) = f, where ≡is syntactic iden-
tity, and top(t) is the top symbol of term t. The negation, conjunction, and

358
M. P. Bonacina and S. Winkler
disjunction of constraints is a constraint. Constraints in standard form are true,
false, and conjunctions of distinct atomic constraints x ̸≡y and top(x) ̸= f.
Substitutions are sort-preserving (xσ has the same sort as x) so that instantia-
tion respects sorts. The set Gr(A ▷C) of constrained ground instances (cgi) of
A▷C is the set of ground instances of C that satisfy A. Literals A▷L and B▷M
intersect if at(Gr(A ▷L)) ∩at(Gr(B ▷M)) ̸= ∅, and are disjoint otherwise.
Example 2. In a signature with sorts {s1, s2} and symbols a: s1, b: s2, f : s1
→s2, and P ⊆s2 × s2, the only term of sort s1 is a, and b and f(a) are the only
terms of sort s2. Thus, Gr(P(x, y)) = {P(b, b), P(f(a), b), P(b, f(a)), P(f(a), f(a))}.
For P(f(x), y), with x: s1 and y: s2, constraint top(x) ̸= a is unsatisﬁable, while
top(y) ̸= a is valid. Then, top(x) ̸= a▷P(f(x), y) is equivalent to false▷P(f(x), y)
and has no cgi’s, while top(y) ̸= a ▷P(f(x), y) is equivalent to true ▷P(f(x), y),
or simply P(f(x), y), and has all cgi’s, namely P(f(a), b) and P(f(a), f(a)).
SGGS is semantically guided by an initial interpretation I: unless I |= S,
SGGS seeks a model of S, by building candidate partial interpretations diﬀerent
from I, and using I as default to complete them. If the empty clause ⊥arises in
the process, unsatisﬁability is reported. If I is the all-negative interpretation I−
that makes all negative literals true, SGGS tries to discover which positive literals
need to be true to satisfy S, and dually if I is the all-positive interpretation I+.
While I can be any Herbrand interpretation, I+ and I−suﬃce in this paper.
SGGS works with a trail of clauses Γ = A1 ▷C1[L1], . . . , An ▷Cn[Ln], where
C[L] means that literal L ∈C is selected in C. The length of Γ and its preﬁx
of length j are denoted |Γ| and Γ|j, respectively. An SGGS-trail Γ represents
a partial interpretation Ip(Γ): if Γ is empty, denoted ε, Ip(Γ) = ∅; otherwise,
Ip(Γ)=Ip(Γ|n−1)∪pcgi(An ▷Ln, Γ), where pcgi abbreviates proper constrained
ground instances. A pcgi of An ▷Cn[Ln] is a cgi C[L] that is not satisﬁed by
Ip(Γ|n−1) (i.e., Ip(Γ|n−1) ∩C[L] = ∅) and can be satisﬁed by adding L as ¬L ̸∈
Ip(Γ|n−1). For the selected literal, pcgi(An ▷Ln, Γ) = {L :
C[L] ∈pcgi(An ▷
Cn[Ln], Γ)}. Ip(Γ) is completed into an interpretation I[Γ] by consulting I for
the truth value of any literal undeﬁned in Ip(Γ).
A literal L is uniformly false in an interpretation J, if all L′∈Gr(L) are false
in J. Then, L is said to be I-false if it is uniformly false in I, and I-true if it is
true in I. SGGS requires that if a clause in Γ has I-false literals, one is selected,
so as to diﬀerentiate I[Γ] from I. A clause whose literals are all I-true is an
I-all-true clause, and only in such a clause an I-true literal is selected.
A conﬂict clause is one whose literals are all uniformly false in I[Γ]. SGGS
ensures that every I-all-true clause C[L] in Γ is either a conﬂict clause or the
justiﬁcation of its selected literal L, meaning that all literals of C[L] except L
are uniformly false in I[Γ], so that L must be true in I[Γ] to satisfy C[L]. In the
latter case C[L] is in the disjoint preﬁx of Γ, denoted dp(Γ), which is the longest
preﬁx such that pcgi(A ▷C[L], Γ) = Gr(A ▷C[L]) for all its clauses A ▷C[L].
An SGGS-derivation is a series of trails Γ0 ⊢Γ1 ⊢. . . Γj ⊢. . ., where Γ0 = ε,
and ∀j, j > 0, an SGGS-inference generates Γj from Γj−1 and S. If ⊥̸∈Γ

SGGS Decision Procedures
359
and I[Γ] ̸|= S, SGGS has two ways to make progress. If Γ = dp(Γ), the trail is
in order, but I[Γ] ̸|= C′ for some C′ ∈Gr(C) and C ∈S. Then, SGGS applies
SGGS-extension to generate from C and Γ a clause A ▷E, such that E is an
instance of C and C′ ∈Gr(A▷E). If Γ ̸= dp(Γ), the trail needs repair: either there
is a conﬂict, or there are intersections between selected literals to be removed
by SGGS-splitting. The SGGS-extension rules specialize the SGGS-extension
scheme ([14, Def. 12]) of which we give here the instance for I based on sign:
Deﬁnition 1. Given input clause set S and trail Γ, if there is a clause C ∈S
such that for all its I-true literals L1, . . . , Ln (n ≥0) there are clauses B1 ▷
D1[M1], . . . , Bn ▷Dn[Mn] in dp(Γ), such that literals M1, . . . , Mn are I-false,
and ∀j, 1 ⩽j ⩽n, Ljα = ¬Mjα with simultaneous most general uniﬁer (mgu) α,
then SGGS-extension adds A ▷E = (n
j=1 Bjα) ▷Cα to Γ.
SGGS-splitting decomposes a clause into instances to isolate and remove
intersections between literals, and it is the only rule that introduces constraints.
Splitting a ground clause is trivial and never done. Let A▷C[L] be a clause where
A is satisﬁable. Roughly speaking (see [14, Sect. 3.2]), a partition of A▷C[L] is a
set {Ai ▷Ci[Li]}n
i=1 such that Gr(A ▷C) = n
i=1{Gr(Ai ▷Ci)} and the literals
Ai ▷Li are pairwise disjoint. Adding predicate symbol Q ⊆s1 × s2 to Exam-
ple 2, a partition of [P(f(x), y)]∨Q(x, y) is {[P(f(x), b)]∨Q(x, b), [P(f(x), f(a))]∨
Q(x, f(a))}. Given trail clauses A ▷C[L] and B ▷D[M], a splitting of C by D,
denoted split(C, D), is a partition of A ▷C[L] such that at(Gr(Aj ▷Lj)) for
some j is the intersection of A▷L and B ▷M, and all other Ai ▷Li are disjoint
from B ▷M. SGGS-splitting replaces A ▷C[L] with split(C, D). Not all clauses
in split(C, D) need to be kept for completeness (see [14, Sect. 4.2]).
Clause An ▷Cn[Ln] is disposable, if Ip(Γ|n−1) |= An ▷Cn[Ln], and SGGS-
deletion removes all disposable clauses from the trail. The following example
shows that SGGS halts, if applied to the EPR set used to show that another
semantically-guided method, hyperresolution, cannot decide EPR.
Example 3. The set S consists of four clauses ([18, Ex. 4.8] and [15, Ex. 3.17]):
P(x, x, a)
(i),
P(x, y, w) ∨P(y, z, w) ∨¬P(x, z, w)
(ii),
¬P(x, x, b)
(iii),
P(x, z, w) ∨¬P(x, y, w) ∨¬P(y, z, w)
(iv).
SGGS with all-negative initial interpretation I−yields the following derivation:
Γ0 : ε ⊢Γ1 : [P(x, x, a)]
extend (i)
⊢Γ2 : [P(x, x, a)], P(x, y, a) ∨[P(y, x, a)] ∨¬P(x, x, a)
extend (ii)
⊢Γ3 : [P(x, x, a)], P(x, x, a) ∨[P(x, x, a)] ∨¬P(x, x, a),
y ̸= x ▷P(x, y, a) ∨[P(y, x, a)] ∨¬P(x, x, a)
split
⊢Γ4 : [P(x, x, a)],
y ̸= x ▷P(x, y, a) ∨[P(y, x, a)] ∨¬P(x, x, a)
delete
Since I−̸|= P(x, x, a), SGGS-extension puts it on the trail. As I[Γ1] satisﬁes
P(x, x, a), but no other positive literal, I[Γ1] ̸|= (ii). Thus, SGGS-extension uni-
ﬁes the third literal of clause (ii) with [P(x, x, a)] on the trail, producing Γ2,

360
M. P. Bonacina and S. Winkler
where an I−-false (i.e., positive) literal is selected in the added clause (choos-
ing the other makes no diﬀerence). As the selected literals intersect, the sec-
ond clause gets split, yielding Γ3. The second clause in Γ3 is disposable and
SGGS-deletion removes it. Since I[Γ4] |= S, the derivation halts reporting satis-
ﬁable. In contrast, hyperresolution generates inﬁnitely many clauses of the form
P(x1, x2, a) ∨P(x2, x3, a) ∨· · · ∨P(xn−1, xn, a) ∨P(xn, x1, a).
If a clause A ▷C[L] added by SGGS-extension is in conﬂict with I[Γ] and
C[L] contains I-false literals, SGGS-resolution explains the conﬂict by resolving
A ▷C[L] with a justiﬁcation in dp(Γ): the resolvent is also a conﬂict clause that
replaces A ▷C[L]. As SGGS-extension (see [14, Def. 19]) ensures that all I-false
literals of a conﬂict clause can be resolved away, conﬂict explanation generates
either ⊥or an I-all-true conﬂict clause B ▷D[M]. The conﬂict represented by
B ▷D[M] is solved by moving (SGGS-move) B ▷D[M] to the left of the clause
in dp(Γ) whose selected literal makes M uniformly false in I[Γ]: the eﬀect is to
ﬂip M from being uniformly false to being an implied literal.
Fairness of an SGGS-derivation involves several properties: SGGS-deletion
and other clause removals are applied eagerly; trivial splitting is avoided; progress
is made whenever possible; every SGGS-extension generating a conﬂict clause is
bundled with explanation and conﬂict-solving inferences to eliminate the conﬂict
before new extensions occur; and inferences applying to shorter preﬁxes of the
trail are never neglected in favor of others applying to longer preﬁxes (see [14,
Defs. 32, 37, and 39]). The limit of a fair derivation Γ0 ⊢Γ1 ⊢. . . Γj ⊢Γj+1 ⊢. . .
is the longest trail Γ∞such that ∀i, i ⩽|Γ∞|, there is an ni such that ∀j,
j ⩾ni, if |Γj| ≥i then Γj|i is equivalent to Γ∞|i (see [14, Def. 50]). In words, all
preﬁxes of the trail stabilize eventually. Both derivation and Γ∞may be inﬁnite,
but if the derivation halts at stage k, Γ∞= Γk. The following results employ an
SGGS-suitable (i.e., total and extending the size ordering, hence well-founded)
ordering on ground atoms (see [14, Def. 16]) and a convergence ordering >c on
SGGS-trails (see [14, Def. 46]):
– Completeness: For all input clause sets S, initial interpretations I, and fair
SGGS-derivations, if S is satisﬁable, I[Γ∞] |= S, and if S is unsatisﬁable,
⊥∈Γk for some k (see [14, Thm. 9 and 11]).
– Descending chain theorem: A fair SGGS-derivation forms a descending chain
Γ0 >c Γ1 >c . . . >c Γj >c Γj+1 . . . (see [14, Thm. 8]).
– Finiteness of descending chains of length-bounded trails: A chain Γ0 >c Γ1 >c
. . . Γj >c Γj+1 . . . where ∀j, j ≥0, |Γj| ⩽n, for some n ≥0, is ﬁnite (see [14,
Thm. 6 and Cor. 2]).
The gist of this paper is to ﬁnd fragments where the length of SGGS-trails is
bounded so that termination of fair derivations is guaranteed.
3
SGGS Decides the Stratiﬁed Fragment
A way to ensure termination is to restrict an inference engine to produce only
terms or atoms from a ﬁnite set B, usually called a basis. For SGGS, let B be a
ﬁnite subset of the Herbrand base A of the input clause set S.

SGGS Decision Procedures
361
Deﬁnition 2. An SGGS-trail Γ = A1 ▷C1[L1], . . . , An ▷Cn[Ln] is in B if for
all i, 1 ⩽i ⩽n, at(Gr(Ai ▷Ci)) ⊆B.
An SGGS-derivation is in B if all its trails are.
Lemma 1. If a fair SGGS-derivation Γ0 ⊢Γ1 ⊢. . . Γj ⊢Γj+1 ⊢. . . is in a ﬁnite
basis B, then for all j, j ⩾0, |Γj| ⩽|B|+1, and if the derivation halts at stage
k, k ⩾0, then |Γk| ⩽|B|.
Proof. SGGS cannot do worse than generating a ground trail where every atom
in B appears selected with either sign: any trail with non-ground clauses will be
shorter, because a non-ground clause covers many (possibly inﬁnitely many)
ground instances. By fairness, if the trail contains an intersection given by
clauses C[L] and D[L], or C[L] and D[¬L] with L ∈B, the clause on the right is
either deleted eagerly by SGGS-deletion, or replaced with a resolvent by SGGS-
resolution before SGGS-extension applies. Thus, there can be at most one such
intersection, and the ﬁrst claim follows. The second claim holds, because the
intersection is removed by fairness prior to termination.
⊓⊔
By the descending chain theorem and the ﬁniteness of descending chains of
length-bounded trails, the following general result follows:
Theorem 1. A fair SGGS-derivation in a ﬁnite basis is ﬁnite.
In order to apply this result, we need to ﬁnd fragments that admit a ﬁnite
basis. We begin with the stratiﬁed fragment. A signature is stratiﬁed, if there is
a well-founded ordering <s on sorts, and for all functions f : s1 × · · · × sn →s, it
holds that s <s si for all 1 ⩽i ⩽n [1,25,36]. Thus, there are no cycles over sorts
when applying functions. The signature from Example 2 is stratiﬁed with order-
ing s1 >s s2. If a sentence over a stratiﬁed signature belongs to the ∃∗∀∗frag-
ment, Skolemization only introduces constants and preserves stratiﬁcation [25].
If there is only one sort, this fragment reduces to EPR, because stratiﬁcation over
a single sort implies that there are no function symbols. However, also stratiﬁed
sentences with a preﬁx other than ∃∗∀∗can yield stratiﬁed clauses [33].
Example 4. Assume a stratiﬁed signature with sorts s1 and s2 such that s1 <s s2,
and symbols f : s1→s2, and P ⊆s2 × s2. The Skolemization of ∀x∃y. P(f(x), y)
preserves stratiﬁcation, as clause P(f(x), g(x)) with Skolem symbol g: s1→s2 is
still stratiﬁed. On the other hand, the Skolemization of ∀x∃y. P(f(y), x) yields
P(f(g(x)), x) with Skolem symbol g: s2→s1, so that stratiﬁcation is lost.
Given a set S of clauses whose signature is stratiﬁed, or stratiﬁed clause
set for short, the Herbrand universe H and the Herbrand base A are ﬁnite,
because stratiﬁcation prevents building terms of unbounded depth [1]. Therefore,
it suﬃces to pick A itself as the ﬁnite basis for Theorem 1.
Theorem 2. Any fair SGGS-derivation from a stratiﬁed clause set S halts, is
a refutation if S is unsatisﬁable, and constructs a model of S if S is satisﬁable.

362
M. P. Bonacina and S. Winkler
However, SGGS-derivations can get exponentially long.
Example 5. Consider the following clause set Sk describing a k-digits binary
counter [38, Def. 2.4.10]. Let Q be a predicate symbol of arity k, and for all i,
1 ⩽i ⩽k, let 0i, 1i, and xi be i-tuples of 0’s, 1’s, and distinct variables x1, . . . , xi,
respectively. Sk consists of the k + 2 clauses, for 1 ⩽m ⩽k,
C0 : Q(0k)
Cm : ¬Q(xm, 0, 1k−m−1) ∨Q(xm, 1, 0k−m−1)
Ck+1 : ¬Q(1k)
so that it is in EPR. Guided by I−, SGGS generates a derivation
Γ0 : ε ⊢Γ1 : [Q(0k)]
extend (C0)
⊢Γ2 : . . . , ¬Q(0k) ∨[Q(0k−1, 1)]
extend (Ck−1)
⊢Γ3 : . . . , ¬Q(0k−1, 1) ∨[Q(0k−2, 1, 0)]
extend (Ck−2)
⊢Γ4 : . . . , ¬Q(0k−2, 1, 0) ∨[Q(0k−2, 1, 1)]
extend (Ck−1)
. . .
. . .
⊢Γ2k−1 : . . . , ¬Q(1k−2, 0, 1) ∨[Q(1k−1, 0)]
extend (Ck−2)
⊢Γ2k : . . . , ¬Q(1k−1, 0) ∨[Q(1k)]
extend (Ck−1)
⊢Γ2k+1 : . . . , ¬Q(1k−1, 0) ∨[Q(1k)], [¬Q(1k)] extend (Ck+1)
that simulates binary counting by adding a clause in each of these 2k+1 steps till
a conﬂict emerges. Then it takes another 2k+1 steps to detect unsatisﬁability:
⊢Γ2k+2 : . . . , [¬Q(1k)], ¬Q(1k−1, 0) ∨[Q(1k)]
move
⊢Γ2k+3 : . . . , [¬Q(1k)], [¬Q(1k−1, 0)]
resolve
⊢Γ2k+2 : . . . , [¬Q(1k−1, 0)], ¬Q(1k−2, 0, 1) ∨[Q(1k−1, 0)], [¬Q(1k)]
move
⊢Γ2k+3 : . . . , [¬Q(1k−1, 0)], [¬Q(1k−2, 0, 1)], [¬Q(1k)]
resolve
. . .
. . .
⊢Γ2k+2 : [¬Q(0k)], [Q(0k)], . . .
move
⊢Γ2k+2+1 : ⊥, . . .
resolve
Similar to positive resolution1 [38, Thm. 2.4.12] or SCL [19], SGGS behaves expo-
nentially, whereas resolution oﬀers a refutation in 2k+1 steps [38, Thm. 2.4.11],
which shows that in EPR ground resolution (same as positive resolution for Sk)
can do exponentially worse than resolution [34]. Encoding Sk in propositional
logic requires exponentially many clauses, as each clause Cm, 1 ⩽m ⩽k, has 2m
ground instances that need to be modeled as distinct propositional clauses. This
indicates that generating instances is not good for this example.
4
SGGS Decides the Restrained Fragment
We begin with the notion of ground-preserving clause, which is convenient for
sign-based semantic guidance.
1 Every resolution step has a positive parent.

SGGS Decision Procedures
363
Deﬁnition 3. A clause C is positively ground-preserving if Var(C) ⊆Var(C−),
and negatively ground-preserving if Var(C) ⊆Var(C+). A set of clauses is pos-
itively/negatively ground-preserving if all its clauses are, and ground-preserving
if it is positively or negatively ground-preserving.
For example, ¬P(x, y, z) ∨Q(y) ∨Q(f(z)) and ¬Q(x) ∨¬Q(y) are positively
ground-preserving, while the clauses in Example 5 are both positively and neg-
atively ground-preserving. We say that I is suitable for a ground-preserving set
S if either I is I−and S is positively ground-preserving, or I is I+ and S is
negatively ground-preserving. If S is positively ground-preserving, its positive
clauses are ground, positive hyperresolution only generates ground clauses ([12],
Lem. 3), and similarly for the negative variant. We show that this also holds for
SGGS.
Lemma 2. If the input clause set S is ground-preserving and the initial inter-
pretation is suitable for S, any fair SGGS-derivation from S is ground.
Proof. We consider S positively ground-preserving and I−(for the dual case one
exchanges the signs). The proof is by induction on the length n of the derivation.
The base case (n = 0) is vacuously true. The induction hypothesis is that the
claim holds for a derivation of length n producing trail Γ. Let Γ ⊢Γ ′ be the
(n+1)-th step. Since Γ is ground, Γ ⊢Γ ′ cannot be a splitting step, because any
splitting of a ground clause yields the clause itself, and fairness excludes such
trivial splittings ([14, Defs. 32, 47, and 49]). If Γ ⊢Γ ′ is an SGGS-resolution
step, it is a ground resolution step, and also Γ ′ is ground. If Γ ⊢Γ ′ is an
SGGS-extension step, it adds an instance Cα of a clause C ∈S, where α is the
simultaneous mgu of all I−-true (i.e., negative) literals L1, . . . , Ln in C with
as many I−-false (i.e., positive) selected literals M1, . . . , Mn in Γ (see Def. 1).
Since Γ is ground by induction hypothesis, the clauses containing M1, . . . , Mn
are ground and do not have constraints. Thus, L1α, . . . , Lnα are also ground.
The I−-false literals of Cα are ground, because C is positively ground-preserving
(i.e., Var(C) ⊆Var(C−)), so that all its variables get grounded by α. Hence Cα
and Γ ′ are ground.
⊓⊔
The next example illustrates Lemma 2 and gives the intuition for
restrainedness.
Example 6. Assume a positively ground-preserving set S which includes:
P(s10(0), s9(0))
(i),
¬P(s(s(x)), y) ∨P(x, s(y))
(ii),
¬P(s(0), 0)
(iii),
and I−is the initial interpretation. SGGS starts with an extension that puts
the positive clause P(10, 9) on the trail, where we write n for sn(0). Subsequent
extensions unify the negative literal in clause (ii) with some positive ground
literal on the trail, so that new literals in added clauses are positive:
Γ0 : ε ⊢Γ1 : [P(10, 9)]
⊢Γ2 : [P(10, 9)], ¬P(10, 9) ∨[P(8, 10)]
⊢Γ3 : [P(10, 9)], ¬P(10, 9) ∨[P(8, 10)], ¬P(8, 10) ∨[P(6, 11)].

364
M. P. Bonacina and S. Winkler
The positive literals have decreasing number of symbols, matching the fact that
P(s(s(x)), y) ≻P(x, s(y)) in (ii) for ≻any lexicographic path ordering (LPO).
This suggests to strengthen ground-preservingness with an ordering (unre-
lated to the SGGS-suitable ordering of Sect. 2) to get a ﬁnite basis.
Deﬁnition 4. A quasi-ordering ⪰on terms and atoms is restraining, if (i) it
is stable under substitution, (ii) the strict ordering ≻= ⪰\ ⪯is well-founded,
and (iii) the equivalence ≈= ⪰∩⪯has ﬁnite equivalence classes.
Note that Condition (i) implies that ≻and ≈are stable under substitution.
From now on, ⪰is a restraining quasi-ordering. Let AS be the set of ground
atoms occurring in S, and A⪯
S the subset of the Herbrand base A of ground
atoms upper-bounded by AS, so A⪯
S = {L : L ∈A, ∃M ∈AS with M ⪰L}.
By Conditions (ii) and (iii) in Deﬁnition 4, A⪯
S is ﬁnite and thus can serve as
basis.
Deﬁnition 5. A clause C is (strictly) positively restrained if it is positively
ground preserving, and for all non-ground literals L ∈C+ there is a literal
M ∈C−such that at(M) ⪰at(L) (at(M) ≻at(L)). A set of clauses is positively
restrained if all its clauses are.
Negatively restrained clauses and clause sets are deﬁned similarly, and a set
of clauses is restrained if it is positively or negatively restrained. The set in
Example 6 is strictly positively restrained. We see next the role of the quasi-
ordering.
Example 7. Problem PLA030-1 in TPTP is neither stratiﬁed, nor monadic, nor
guarded. Its clause diﬀer(x, y)∨¬diﬀer(y, x) cannot be shown strictly restrained.
Let ≻acrpo be an AC-compatible [40] recursive path ordering with diﬀer as an AC-
symbol, meaning associative-commutative. The quasi-ordering ⪰acrpo, built from
≻acrpo and the AC-equivalence ≈AC that has ﬁnite equivalence classes, satisﬁes
diﬀer(x, y)⪰acrpodiﬀer(y, x), and shows that PLA030-1 is negatively restrained.
Restrainedness is undecidable in general, but decidable for ﬁxed, suitable
orderings. If S is restrained, a fair SGGS-derivation will be in A⪯
S .
Lemma 3. If the input clause set S is restrained and the initial interpretation
is suitable for S, any fair SGGS-derivation from S is in A⪯
S .
Proof. We consider S positively ground-preserving and I−(for the dual case one
exchanges the signs). Since the set is restrained hence ground-preserving, the
derivation is ground by Lemma 2 (†). The proof is by induction on the length
n of the derivation, and it follows the same pattern as that of Lemma 2. Let
Γ ⊢Γ ′ be the (n+1)-th step. By induction hypothesis, Γ is in A⪯
S . If Γ ⊢Γ ′ is
an SGGS-resolution step, it is a ground resolution step which does not generate
new atoms, and also Γ ′ is in A⪯
S . If Γ ⊢Γ ′ is an SGGS-extension step, it adds
an instance Cα of a clause C ∈S, where α is the simultaneous mgu of all I−-true

SGGS Decision Procedures
365
(i.e., negative) literals ¬L1, . . . , ¬Ln in C with as many I−-false (i.e., positive)
selected literals M1, . . . , Mn in Γ. The literals M1, . . . , Mn are ground by (†),
and by induction hypothesis they are in A⪯
S . We have to show at(Cα) ⊆A⪯
S .
– For the negative literals ¬L1α, . . . , ¬Lnα we have Liα = Miα = Mi ∈A⪯
S .
– Let L be a literal in C+. If L is ground, then Lα = L ∈AS ⊆A⪯
S . If L
is not ground, by positive restrainedness there exists a ¬Li, 1 ⩽i ⩽n, such
that Li ⪰L. By stability, Liα ⪰Lα. Since for all i, 1 ⩽i ⩽n, Mi ∈A⪯
S and
Mi = Miα = Liα ⪰Lα, we have Lα ∈A⪯
S .
⊓⊔
Therefore, as A⪯
S is ﬁnite, Theorem 1 applies.
Theorem 3. Any fair SGGS-derivation from a restrained clause set S with a
suitable initial interpretation for S halts, is a refutation if S is unsatisﬁable, and
constructs a model of S if S is satisﬁable.
Restrainedness also makes it possible to derive an upper bound on the cardi-
nality of a single-sorted model, deﬁned as the cardinality of its domain. Let H⪯
S
be the set H⪯
S = {t : t is a strict subterm of L for some L ∈A⪯
S }.
Theorem 4. A restrained satisﬁable clause set S has a model of cardinality at
most |H⪯
S | + 1 that can be extracted from the limit of any fair SGGS-derivation
from S with suitable initial interpretation I.
Proof. By Theorem 3 the derivation halts with some trail Γ, and by Lemmas 2
and 3, Γ contains only ground clauses whose atoms are in A⪯
S . Since SGGS is
model complete, I[Γ] |= S. Consider the following interpretation J with domain
H⪯
S ⊎{u}, where u is a new constant symbol: for every constant symbol c we
set cJ = c if c ∈H⪯
S , and cJ = u otherwise. For every n-ary (n ⩾1) func-
tion symbol f, we set f J(t1, . . . , tn) = f(tJ
1 , . . . , tJ
n) if f(t1, . . . , tn) ∈H⪯
S , and
f J(t1, . . . , tn) = u otherwise. For every predicate symbol P, (t1, . . . , tn) ∈P J if
I[Γ] |= P(t1, . . . , tn). Note that J is well-deﬁned because if f(t1, . . . , tn) ∈H⪯
S
then t1, . . . , tn are also, hence all terms are interpreted in H⪯
S ⊎{u}. As J agrees
with I[Γ] on all atoms, J |= S, and it has cardinality |H⪯
S |+1 by construction. ⊓⊔
Therefore the restrained fragment also enjoys the small model property.
Example 8. The satisﬁable clause set S (PUZ054-1 in TPTP) extending
Example 6:
P(s10(0), s9(0)),
¬P(s(s(x)), y) ∨P(x, s(y)),
¬P(x, s(s(y))) ∨P(x, s(y)),
¬P(s(0), 0),
¬P(s(x), s(y)) ∨P(s(x), y),
is neither in EPR, nor in FO2, nor in the monadic class. However, it can be shown
strictly positively restrained by a Knuth-Bendix ordering (KBO) ≻with empty
precedence and weights w(P) = 0 and w0 = w(s) = w(0) = 1, where w0 is the
weight of variables. The largest atom in AS = {P(s10(0), s9(0)), P(s(0), 0)} has
weight w(P(s10(0), s9(0))) = 21. No atom L of the form P(sn(0), sm(0)) in A⪯
S can

366
M. P. Bonacina and S. Winkler
have a subterm sk(0) with k > 19, because otherwise w(L) > w(P(s10(0), s9(0))).
Therefore, we have H⪯
S = {si(0) : 0 ⩽i ⩽19} and there is a model of cardinality
at most 21 by Theorem 4.
We next consider PVD [17]. Let depth(C) be the maximum depth of an atom
in clause C, and depthx(C) the maximum occurrence depth in C of x ∈Var(C).
Deﬁnition 6. A clause set S is in PVD if every clause C ∈S is positively
ground-preserving and ∀x ∈V ar(C+) it holds that depthx(C+) ⩽depthx(C−).
Let Ad be the subset of the Herbrand base A containing all ground atoms whose
depth does not exceed d.
Lemma 4. If the input clause set S is in PVD and has maximal depth d then
any fair SGGS-derivation from S using I−is in Ad.
Proof. Since S is in PVD and hence ground-preserving, by Lemma 2 the deriva-
tion is ground (†). The proof is by induction on the length n of the derivation,
and follows that of Lemma 3 except for the extension case in the inductive step.
Let Γ ⊢Γ ′ be an SGGS-extension step as described in the proof of Lemma 3.
The positive literals M1, . . . , Mn are ground by (†) and by induction hypothesis
in Ad. Since Liα = Miα = Mi for all i, 1 ⩽i ⩽n, the atoms Liα in C−α are
ground and in Ad. Let L be a literal in C+. Since S is in PVD, every variable
in L occurs at a lower or equal depth in some Li (1 ⩽i ⩽n). Therefore, Lα is
ground and its depth cannot exceed that of Liα, so that Lα ∈Ad.
⊓⊔
Since Ad is a ﬁnite basis, termination follows from Theorem 1.
Theorem 5. Any fair SGGS-derivation using I−from a PVD set S halts, is a
refutation if S is unsatisﬁable, and constructs a model of S if S is satisﬁable.
5
Ordered Resolution Decides the Restrained Fragment
In this section we work with the positively restrained fragment; the case for
the negatively restrained one is symmetric. Let > be a stable and well-founded
ordering on literals, such that positive literals are maximal only in positive
clauses, so that ordered resolution is positive ordered resolution. In this way,
the ordering embeds the suitable sign-based semantic guidance for the positively
restrained fragment. The ordering > could be the extension of the restraining
ordering ≻(see Deﬁnition 5) to literals, but does not have to be. We use the
following notations [18]: Res>(S) denotes the set of ordered resolvents gener-
ated from parents in S; R0
>(S) = S, Rk+1
>
(S) = Rk
>(S) ∪Res>(Rk
>(S)), and
R∗
>(S) = 
k⩾0 Rk
>(S). We begin with an auxiliary lemma.
Lemma 5. If S is positively restrained, then for all C ∈R∗
>(S), for all L ∈C+
either (i) L ∈A⪯
S , or (ii) at(M) ⪰at(L) for some M ∈C−.

SGGS Decision Procedures
367
Proof. The proof is by induction on the stage k of the construction of R∗
>(S).
For k = 0, the clauses in R0
>(S) = S satisfy the claim by the deﬁnitions of
restrainedness, AS, and A⪯
S . The induction hypothesis is that all clauses in
Rk
>(S) satisfy the claim. For the inductive step, let Cσ ∨Dσ be a resolvent
in Res>(Rk
>(S)) generated from parents ¬L ∨C and L′ ∨D, where ¬Lσ and
L′σ are >-maximal literals, and Lσ = L′σ for mgu σ. The clause L′ ∨D must
be positive, otherwise L′σ cannot be >-maximal. Since L′ ∨D ∈Rk
>(S), by
induction hypothesis at(L′ ∨D) ⊆A⪯
S (†), which means L′ ∨D is ground,
(L′ ∨D)σ = L′ ∨D, and all atoms in Dσ are in A⪯
S . For the positive literals in
Cσ, let Mσ be one of them, so M ∈C+. Since ¬L∨C is in Rk
>(S), by induction
hypothesis, either (i) M ∈A⪯
S , or (ii) M ′ ⪰M for some negative literal ¬M ′
in ¬L ∨C. In case (i), M is ground, Mσ = M, and Mσ ∈A⪯
S . In case (ii), if
¬M ′ occurs in C then ¬M ′σ ∈Cσ, and M ′σ ⪰Mσ holds by stability, so that
the claim holds. Otherwise, ¬M ′ is the resolved-upon literal ¬L with Lσ = L′σ.
Thus, L = M ′ ⪰M, which implies Lσ ⪰Mσ by stability, and since L′ is ground,
L′σ = L′. By (†), L′ ∈A⪯
S . Since L′ = L′σ = Lσ ⪰Mσ, we have Mσ ∈A⪯
S by
deﬁnition of A⪯
S .
⊓⊔
Theorem 6. Any fair ordered resolution derivation using > from a restrained
clause set S terminates, and is a refutation if S is unsatisﬁable.
Proof. Let S be positively restrained. We show that R∗
>(S) is ﬁnite. The claim
then follows from refutational completeness of ordered resolution. We ﬁrst show
the following (†): for every ordered resolvent Cσ ∨Dσ in R∗
>(S) from parents
¬L ∨C and L′ ∨D and mgu σ, L′ ∨D is ground and positive, and Cσ ∨Dσ has
strictly fewer negative literals than ¬L ∨C. Indeed, by the deﬁnition of ordered
resolution, ¬Lσ and L′σ are >-maximal in (¬L∨C)σ and (L′∨D)σ, respectively.
The clause L′ ∨D must be positive, otherwise L′σ cannot be >-maximal. By
Lemma 5, every positive literal in a positive clause is in A⪯
S , and therefore it is
ground. Thus, in the resolvent Cσ ∨Dσ the literals in Dσ are positive, hence
Cσ ∨Dσ has fewer negative literals than ¬L ∨C.
Now suppose that R∗
>(S) is inﬁnite. Observation (†) reveals that the number
of negative literals decreases with every resolution step. Hence, if R∗
>(S) is inﬁ-
nite, it must contain inﬁnitely many positive clauses. By Lemma 5, these positive
clauses are ground, and all their atoms are in the ﬁnite basis A⪯
S . As repeated
literals in ground clauses disappear by merging, the number of ground clauses
that can be built from A⪯
S is ﬁnite. This contradicts R∗
>(S) being inﬁnite.
⊓⊔
This result extends to other positive resolution strategies.
Corollary 1. Hyperresolution and >-ordered resolution with negative selection
decide the positively restrained fragment.
The next example shows that SGGS can be exponentially more eﬃcient than
these saturation-based resolution strategies because it is model-based.

368
M. P. Bonacina and S. Winkler
Example 9. Consider the following parametric clause set Sn consisting of n + 1
clauses, using i+1-ary predicates Pi and constants ci, for all i, 0 ⩽i ⩽n:
P0(c0) ∨P0(c1) ∨· · · ∨P0(cn)
(C0),
¬P0(x1) ∨P1(x1, c0) ∨P1(x1, c1) ∨· · · ∨P1(x1, cn)
(C1),
¬P1(x1, x2) ∨P2(x1, x2, c0) ∨· · · ∨P2(x1, x2, cn)
(C2),
. . .
. . .
¬Pn−1(x1, . . . , xn) ∨Pn(x1, . . . , xn, c0) ∨· · · ∨Pn(x1, . . . , xn, cn)
(Cn).
The set Sn is positively restrained by an LPO with precedence P0 > · · · >
Pn > ci for all i, 0⩽i⩽n. SGGS with I−simply selects one positive literal per
clause and detects satisﬁability after n + 1 SGGS-extension steps, producing
for instance the model where P0(c0), P1(c0, c0), . . . , Pn(c0, . . . , c0) are true and
all other atoms are false. A saturation by any of the above positive resolution
strategies produces exponentially many clauses, because for all i, 0⩽i⩽n, all n
positive literals in Ci unify with the negative literal in Ci+1, generating ni+1
positive clauses, so that the clause count is given by n
k=1 nk.
6
Experiments
We begin by reducing positive restrainedness (the negative case is similar) to ter-
mination of rewrite systems, so that termination tools can yield a partial practi-
cal test. Since restrainedness employs a quasi-ordering, we consider unsorted re-
write systems R and E, and rewriting modulo →R/E deﬁned by ↔∗
E ◦→R ◦↔∗
E.
Deﬁnition 7. Given a clause set S, a pair of rewrite systems (RS, ES) is positi-
vely restraining for S if for all clauses C ∈S and all non-ground literals L ∈C+,
there is a rule at(M) →at(L) in RS ∪ES for some literal M ∈C−.
Let (RS, ES) be positively restraining for S. For instance, for Example 8,
RS will consist of P(s(s(x)), y) →P(x, s(y)), P(x, s(s(y))) →P(x, s(y)), and
P(s(x), s(y)) →P(s(x), y), while ES = ∅. A common situation is that ES has
permutative rules, as in Example 7 where diﬀer(x, y) →diﬀer(y, x) will be in ES.
Lemma 6. (1) If →RS is terminating and ES = ∅, clause set S is strictly posi-
tively restrained. (2) If →RS/ES is terminating, Var(t) = Var(u) for all t →u
in ES, and ↔∗
E has ﬁnite equivalence classes, S is positively restrained.
Proof. (1) Since →RS is terminating, for all t →u in RS, Var(u) ⊆Var(t),
so that S is positively ground-preserving. S is strictly positively restrained by
the quasi-ordering →∗
RS: indeed, →∗
RS is stable, →+
RS is well-founded, and the
equivalence classes of →∗
RS ∩
∗
RS←, which is identity, are ﬁnite. (2) Since →RS/ES
is terminating, for all t →u in RS, Var(u) ⊆Var(t), and for all t →u in ES,
Var(u) = Var(t) by hypothesis, so that S is positively ground-preserving. S
is positively restrained by →∗
RS/ES: indeed, →∗
RS/ES is stable, →+
RS/ES is well-
founded, and the equivalence classes of ↔∗
E are ﬁnite by hypothesis.
⊓⊔

SGGS Decision Procedures
369
For the experiments, given a clause set S, a script named StoR generates
rewrite systems RS and ES that can be fed to a termination tool. The source
of clause sets is TPTP 7.2.0 and the termination tool is TTT2 [27]. All problems
in the FOF category are transformed into conjunctive normal form, excluding
those with equality (whether sets with equality can be restrained is a topic for
future work). Besides 1,539 inputs where either StoR or TTT2 timed out, out of
the remaining 3,462 problems, TTT2 found 313 restrained ones. For those still
undetermined, we tested whether it is suﬃcient to ﬂip the sign of all literals with
a certain predicate to get a restrained problem, which succeeded in 36 cases, for
a total of 349 restrained problems. Of these, 277 are positively restrained, 181
negatively restrained, and 109 are both; 74 are ground, 232 are PVD, 277 are
stratiﬁed, 252 are EPR, 169 are monadic, 204 are FO2, 209 are guarded, but 43
problems do not fall in any of these classes, and therefore, to the best of our
knowledge, they are proved decidable here for the ﬁrst time. The average TPTP
rating of the 349 problems is 0.1, and that of the 43 problems is 0.015, where
0.1 means that the problem can be solved by most provers. However, the group
of 349 includes hard problems such as instances of the binary counter problem
in Example 5 (MSC015-1.n), and Rubik’s cube problems (e.g., PUZ052-1). For
example, MSC015-1.030 is restrained and has rating 1.00, that is, no theorem
prover could solve it so far within the timeout allowed in the CASC competition.
Example 10. Problem HWV036-2 (cf. Example 1) is a set of axioms with no
ground atoms, so that H⪯
S is empty, and the model constructed according to The-
orem 4 is trivial. Several other problems combine this set with ground clauses to
prove theorems from those axioms. For example, HWV008-2.002 adds 23 ground
clauses. As we found a terminating positively restraining rewrite system for
HWV008-2.002, this problem, as well as HWV036-2, is strictly restrained.
An SGGS prototype named Koala was built reusing code for basic data
structures, term indexing, and type inference from iProver [24,26]. In Koala, the
SGGS-trail is represented as a list of constrained clauses, with constraints main-
tained in standardized form (see Sect. 2 and [14, Sect. 7]), and selected literals
stored in a discrimination tree, since SGGS-extension requires to ﬁnd selected
literals that unify with literals in an input clause (cf. Deﬁnition 1). Koala takes
sorts into account when checking satisﬁability of constraints (e.g., Example 2),
and implements a fair search plan which ensures that all derivations are fair (see
Sect. 2). The SGGS-suitable ordering is a KBO with built-in precedence, w0 = 1,
and weight 1 for all function and predicate symbols in order to extend the size
ordering. Koala also sorts by this ordering the clauses in a splitting, according
to the SGGS notion of preferred clause in a splitting [14, Def. 22]).
Koala picks either I−or I+ as initial interpretation, based on whether the
problem is positively or negatively ground-preserving, which overapproximates
positively or negatively restrained. Koala implements the above mentioned sign-
ﬂipping test to obtain more ground-preserving sets. In order to recognize strati-
ﬁed input problems, one can compute the sort dependency graph and check for
acyclicity [25], or let Koala apply type inference. For all experiments with Koala,

370
M. P. Bonacina and S. Winkler
the time-out was 300 sec of wall clock time. Out of the above 349 restrained
problems, Koala shows that 50 are satisﬁable and 283 unsatisﬁable. Of 351 PVD
problems, not including the 232 ones in the set of 349 restrained problems, Koala
discovers 232 unsatisﬁable and 76 satisﬁable instances. Of the 1,246 stratiﬁed
problems found in the FOF category by the above acyclicity test, Koala solves
643 unsatisﬁable and 277 satisﬁable instances. The list of the 349 restrained sets
with their properties and restraining rewrite systems, as well as detailed results
of the experiments with Koala are available.2
7
Discussion
SGGS [13,14] is an attractive candidate for decision procedures, because it is
conﬂict-driven and it builds models. In this paper, we showed that if the gen-
erated clauses are in a ﬁnite basis, SGGS is guaranteed to terminate, and we
instantiated this result to yield SGGS decision procedures for the stratiﬁed frag-
ment, PVD, and the newly introduced restrained fragment, all without equality.
While also Inst-Gen [24] and the model evolution calculus (MEC) [7] decide
the stratiﬁed fragment [25], this is not the case for the restrained fragment: for
instance, if Inst-Gen uses an unfortunate literal selection in Example 8, it does
not halt. SGGS avoids this phenomenon thanks to semantic guidance. Since
MEC starts with I+ as candidate model, it may not terminate on satisﬁable
negatively restrained sets such as Example 7. Indeed, E-Darwin [6] does not halt
on this example that Koala solves in a few seconds.
However, it is generally diﬃcult to tame a refutationally complete ﬁrst-order
inference system to yield decision procedures. For instance, SGGS does not halt
given the following set of clauses that belongs to several decidable fragments.3
Example 11. The following set is in the Ackermann, monadic, and FO2 classes:
P(0)
(i)
P(x) ∨P(f(x))
(ii)
¬P(x) ∨¬P(f(x))
(iii),
where membership in the Ackermann class stems from the Skolemization of
∃v∀x∃y.P(v)∧(P(x)∨P(y))∧(¬P(x)∨¬P(y)). It has the ﬁnite model property,
as witnessed by model I with domain {0, 1}, fI(x) = 1−x, 0I = 0, and PI = {0}.
With I−, SGGS performs an inﬁnite series of non-conﬂicting extensions:
ε ⊢[P(0)]
extend (i)
⊢[P(0)], ¬P(0) ∨[¬P(f(0))]
extend (iii)
⊢[P(0)], ¬P(0) ∨[¬P(f(0))], P(f(0)) ∨[P(f(f(0)))]
extend (ii)
⊢. . .
It can be shown that SGGS does not terminate with I+ either. Inst-Gen may
terminate if appropriate candidate models are constructed, but not in general.
Ordered resolution terminates if the ordering satisﬁes ¬P(f(x)) > ¬P(x).
2 http://profs.scienze.univr.it/winkler/sggsdp/.
3 Renate A. Schmidt and Marco Voigt suggested this example to the ﬁrst author.

SGGS Decision Procedures
371
The stratiﬁed fragment was presented as the ﬁrst of three fragments of
increasing expressivity, named St0, St1, and St2, and suitable to capture
Alloy speciﬁcations [1]. If the generic predicate symbols P of Example 4 is
replaced with equality for sort s2, the second sentence of Example 4 becomes
∀x∃y. f(y) ≃x, which states that x is in the image of f: this formula separates
St0 and St1, since it is allowed in St1, but not in St0. The LISBQ logic for the
veriﬁcation of unbounded data structures [29] can be translated into St0 with
equality [1].
The restrained fragment is deﬁned starting from a notion of ground-preser-
vingness, which is convenient for positive or negative strategies. Negative ground-
preservingness was used for Horn theories with equality [28] and [10, Sect. 5.2].
Positive ground-preservingness, also named range restrictedness [15], is implicit
in PVD and recent extensions [30]. Unlike these settings, the restrained fragment
does not limit literal depth. Positive ground-preservingness was used to show that
DPLL(Γ +T ), where Γ is an inference system including hyperresolution, super-
position with negative selection, and simpliﬁcation, decides essentially ﬁnite
(only one monadic function f with ﬁnite range) and positively ground-preserving
axiomatizations, provided that speculative axioms f j(x) ≃f k(x) (j > k) are tried
for increasing values of j and k ([12], Thm. 7 and Lem. 3). Simpliﬁcation applies
the speculative axioms to limit the depth of generated terms. Without this fea-
ture, it is not surprising that SGGS does not halt.
Example 12. Consider a positively ground-preserving variant of Example 11:
P(0)
(i)
¬P(x) ∨P(f(f(x)))
(ii)
¬P(x) ∨¬P(f(x))
(iii)
The ﬁnite model property implies that it is essentially ﬁnite. SGGS terminates
with neither I−nor I+ as initial interpretation. With I−it generates:
ε ⊢[P(0)]
extend (i)
⊢[P(0)], ¬P(0) ∨[P(f2(0))]
extend (ii)
⊢[P(0)], ¬P(0) ∨[P(f2(0))], ¬P(f2(0)) ∨[P(f4(0))]
extend (ii)
⊢. . .
Positive hyperresolution generates the series {P(f2k(0))}k≥0, which is essentially
the same behavior as SGGS. DPLL(Γ +T ) tries f(x) ≃x, detects a conﬂict,
backtracks, tries f2(x) ≃x, and halts reporting satisﬁability.
Ensuring termination by restricting new (i.e., non-input) terms to come from
a ﬁnite basis is common for conﬂict-driven decision procedures [11,16]. A major
direction for future work towards decision procedures for richer languages is the
integration of SGGS with CDSAT [11], in order to endow SGGS with equality
and CDSAT with quantiﬁers. Speculative inferences and initial interpretations
not based on sign are additional leads. For the restrained fragment, one may con-
sider its relations with other decidable fragments and its relevance to applications
beyond TPTP. While positive clauses in a restrained set are ground, one may

372
M. P. Bonacina and S. Winkler
study the decidability of sets where positive clauses are not necessarily ground,
but admit a restraining rewrite system (Deﬁnition 7) such that narrowing halts.
Techniques to detect the termination of narrowing are known [35].
Although Koala is only a prototype, the experiments show potential and
allow us to identify critical issues for the performance of an SGGS prover. For
example, instance generation by SGGS-extension is a bottleneck for problems
with many input clauses, and forms of caching should be considered to avoid
repeating computations. Further development of Koala and more experiments
may contribute to discover classes of ﬁrst-order problems where the conﬂict-
driven style of SGGS reasoning is especially rewarding.
References
1. Abadi, A., Rabinovich, A., Sagiv, M.: Decidable fragments of many-sorted logic.
J. Symb. Comput. 45(2), 153–172 (2010)
2. Ackermann, W.: Solvable Cases of the Decision Problem. North Holland, Amster-
dam (1954)
3. Alagi, G., Weidenbach, C.: NRCL - a model building approach to the Bernays-
Sch¨onﬁnkel fragment. In: Lutz, C., Ranise, S. (eds.) FroCoS 2015. LNCS (LNAI),
vol. 9322, pp. 69–84. Springer, Cham (2015). https://doi.org/10.1007/978-3-319-
24246-0 5
4. Andr´eka, H., van Benthem, J., Nemeti, I.: Modal logics and bounded fragments of
predicate logic. J. Phil. Logic 27(3), 217–274 (1998)
5. Bachmair, L., Ganzinger, H., Waldmann, U.: Superposition with simpliﬁcation as
a decision procedure for the monadic class with equality. In: Gottlob, G., Leitsch,
A., Mundici, D. (eds.) KGC 1993. LNCS, vol. 713, pp. 83–96. Springer, Heidelberg
(1993). https://doi.org/10.1007/BFb0022557
6. Baumgartner, P., Fuchs, A., Tinelli, C.: Implementing the model evolution calculus.
Int. J. Artif. Intell. Tools 15(1), 21–52 (2006)
7. Baumgartner, P., Tinelli, C.: The model evolution calculus as a ﬁrst-order DPLL
method. Artif. Intell. 172(4–5), 591–632 (2008)
8. Bernays, P., Sch¨onﬁnkel, M.: Zum Entscheidungsproblem der mathematischen
Logik. Math. Annalen 99, 342–372 (1928)
9. Blackburn, P., de Rijke, M., Venema, Y.: Modal Logic. Cambridge University Press,
Cambridge (2001)
10. Bonacina, M.P., Dershowitz, N.: Canonical ground Horn theories. In: Voronkov, A.,
Weidenbach, C. (eds.) Programming Logics. LNCS, vol. 7797, pp. 35–71. Springer,
Heidelberg (2013). https://doi.org/10.1007/978-3-642-37651-1 3
11. Bonacina, M.P., Graham-Lengrand, S., Shankar, N.: Conﬂict-driven satisﬁability
for theory combination: transition system and completeness. J. Autom. Reason.
64(3), 579–609 (2020)
12. Bonacina, M.P., Lynch, C.A., de Moura, L.: On deciding satisﬁability by theorem
proving with speculative inferences. J. Autom. Reason. 47(2), 161–189 (2011)
13. Bonacina, M.P., Plaisted, D.A.: Semantically-guided goal-sensitive reasoning:
model representation. J. Autom. Reason. 56(2), 113–141 (2016)
14. Bonacina, M.P., Plaisted, D.A.: Semantically-guided goal-sensitive reasoning: infer-
ence system and completeness. J. Autom. Reason. 59(2), 165–218 (2017)
15. Caferra, R., Leitsch, A., Peltier, N.: Automated Model Building. Kluwer, Alphen
aan den Rijn (2004)

SGGS Decision Procedures
373
16. de Moura, L., Jovanovi´c, D.: A model-constructing satisﬁability calculus. In: Gia-
cobazzi, R., Berdine, J., Mastroeni, I. (eds.) VMCAI 2013. LNCS, vol. 7737, pp.
1–12. Springer, Heidelberg (2013). https://doi.org/10.1007/978-3-642-35873-9 1
17. Ferm¨uller, C., Leitsch, A.: Model building by resolution. In: B¨orger, E., J¨ager, G.,
Kleine B¨uning, H., Martini, S., Richter, M.M. (eds.) CSL 1992. LNCS, vol. 702, pp.
134–148. Springer, Heidelberg (1993). https://doi.org/10.1007/3-540-56992-8 10
18. Ferm¨uller, C., Leitsch, A., Hustadt, U., Tammet, T.: Resolution decision proce-
dures. In: Handbook of Automated Reasoning, pp. 1791–1849. Elsevier and MIT
Press (2001)
19. Fiori, A., Weidenbach, C.: SCL clause learning from simple models. In: Fontaine, P.
(ed.) CADE 2019. LNCS (LNAI), vol. 11716, pp. 233-249. Springer, Cham (2019).
https://doi.org/10.1007/978-3-030-29436-6 14
20. Ganzinger, H., de Nivelle, H.: A superposition decision procedure for the guarded
fragment with equality. In: Proceedings of LICS-14. IEEE Computer Society Press
(1999)
21. Gr¨adel, E., Kolaitis, P., Vardi, M.: On the decision problem for two-variable ﬁrst-
order logic. Bull. Symb. Log. 3, 53–69 (1997)
22. Hustadt, U., Schmidt, R.A., Georgieva, L.: A survey of decidable ﬁrst-order frag-
ments and description logics. J. Relat. Methods Comput. Sci. 1, 251–276 (2004)
23. Joyner Jr., W.H.: Resolution strategies as decision procedures. J. ACM 23(3),
398–417 (1976)
24. Korovin, K.: Inst-Gen – a modular approach to instantiation-based automated
reasoning. In: Voronkov, A., Weidenbach, C. (eds.) Programming Logics. LNCS,
vol. 7797, pp. 239–270. Springer, Heidelberg (2013). https://doi.org/10.1007/978-
3-642-37651-1 10
25. Korovin, K.: Non-cyclic sorts for ﬁrst-order satisﬁability. In: Fontaine, P., Ringeis-
sen, C., Schmidt, R.A. (eds.) FroCoS 2013. LNCS (LNAI), vol. 8152, pp. 214–228.
Springer, Heidelberg (2013). https://doi.org/10.1007/978-3-642-40885-4 15
26. Korovin, K.: iProver – an instantiation-based theorem prover for ﬁrst-order logic
(v2.8) (2018). http://www.cs.man.ac.uk/korovink/iprover/index.html
27. Korp, M., Sternagel, C., Zankl, H., Middeldorp, A.: Tyrolean Termination Tool 2.
In: Treinen, R. (ed.) RTA 2009. LNCS, vol. 5595, pp. 295–304. Springer, Heidelberg
(2009). https://doi.org/10.1007/978-3-642-02348-4 21
28. Kounalis, E., Rusinowitch, M.: On word problems in Horn theories. J. Symb. Com-
put. 11(1–2), 113–128 (1991)
29. Lahiri, S., Qadeer, S.: Back to the future: revisiting precise program veriﬁcation
using SMT solvers. In: Proceedings of POPL-31, pp. 171–182. ACM (2004)
30. Lamotte-Schubert, M., Weidenbach, C.: BDI: a new decidable clause class. J. Log.
Comput. 27(2), 441–468 (2017)
31. Lewis, C.I., Langford, C.H.: Symbolic Logic. Dover Publications Inc., Mineola
(1932)
32. Lutz, C., Sattler, U., Wolter, F.: Modal logic and the two-variable fragment. In:
Fribourg, L. (ed.) CSL 2001. LNCS, vol. 2142, pp. 247–261. Springer, Heidelberg
(2001). https://doi.org/10.1007/3-540-44802-0 18
33. McMillan, K.: Developing distributed protocols with Ivy. Slides (2019). http://
vmcaischool19.tecnico.ulisboa.pt/
34. Navarro, J.A., Voronkov, A.: Proof systems for eﬀectively propositional logic. In:
Armando, A., Baumgartner, P., Dowek, G. (eds.) IJCAR 2008. LNCS (LNAI),
vol. 5195, pp. 426–440. Springer, Heidelberg (2008). https://doi.org/10.1007/978-
3-540-71070-7 36

374
M. P. Bonacina and S. Winkler
35. Nishida, N., Vidal, G.: Termination of narrowing via termination of rewriting.
Appl. Algebr. Eng. Commun. 21(3), 177–225 (2010)
36. Padon, O., McMillan, K., Panda, A., Sagiv, M., Shoham, S.: Ivy: Safety veriﬁcation
by interactive generalization. In: SIGPLAN Not. vol. 51, no. 6, pp. 614–630 (2016)
37. Piskac, R., de Moura, L., Bjørner, N.: Deciding eﬀectively propositional logic using
DPLL and substitution sets. J. Autom. Reason. 44(4), 401–424 (2010)
38. Plaisted, D.A., Zhu, Y.: The Eﬃciency of Theorem Proving Strategies. Friedr.
Vieweg, Hamburg (1997)
39. Ramsey, F.: On a problem in formal logic. Proc. Lond. Math. Soc. 30, 264–286
(1930)
40. Rubio, A.: A fully syntactic AC-RPO. Inform. Comput. 178(2), 515–533 (2002)
41. Sutcliﬀe, G.: The TPTP problem library and associated infrastructure from CNF
to TH0, TPTP v6.4.0. J. Autom. Reason. 59(4), 483–502 (2017)

Integrating Induction and Coinduction
via Closure Operators and Proof Cycles
Liron Cohen1
and Reuben N. S. Rowe2(B)
1 Department of Computer Science, Ben-Gurion University, Beersheba, Israel
cliron@cs.bgu.ac.il
2 Department of Computer Science, Royal Holloway University of London,
Egham, UK
reuben.rowe@rhul.ac.uk
Abstract. Coinductive reasoning about inﬁnitary data structures has
many applications in computer science. Nonetheless developing natu-
ral proof systems (especially ones amenable to automation) for reason-
ing about coinductive data remains a challenge. This paper presents a
minimal, generic formal framework that uniformly captures applicable
(i.e. ﬁnitary) forms of inductive and coinductive reasoning in an intuitive
manner. The logic extends transitive closure logic, a general purpose logic
for inductive reasoning based on the transitive closure operator, with
a dual ‘co-closure’ operator that similarly captures applicable coinduc-
tive reasoning in a natural, eﬀective manner. We develop a sound and
complete non-well-founded proof system for the extended logic, whose
cyclic subsystem provides the basis for an eﬀective system for automated
inductive and coinductive reasoning. To demonstrate the adequacy of the
framework we show that it captures the canonical coinductive data type:
streams.
1
Introduction
The principle of induction is used widely in computer science for reason-
ing about data types such as numbers or lists. The lesser-known principle
of coinduction is used for reasoning about coinductive data types, which
are data structures containing non-well-founded elements, e.g. inﬁnite streams
or trees [7,25,27,32,35,37,44,46,48]. A duality between the two principles is
observed when formulating them within an algebraic, or categorical, frame-
work [49]. However, such formulation does not account well for the way these
principles are commonly used in deduction, where there is a mismatch in how
they are usually applied.
Due to this tension between the abstract theory of coalgebras and its
implementation in formal frameworks [41], coinductive reasoning is generally
not fully and naturally incorporated into major proof assistants (e.g. Coq [7],
Nuprl [20], Agda [8], Idris [9] and Dafny [36]). Even in notable exceptions such
as
[33,36,38,44] the combination of induction and coinduction is not intu-
itively accounted for. The standard approach in such formalisations is to deﬁne
c
⃝Springer Nature Switzerland AG 2020
N. Peltier and V. Sofronie-Stokkermans (Eds.): IJCAR 2020, LNAI 12166, pp. 375–394, 2020.
https://doi.org/10.1007/978-3-030-51074-9_21

376
L. Cohen and R. N. S. Rowe
inductive data with constructors and coinductive data with destructors, or obser-
vations [1]. In this paper we propose an alternative approach to formally integrat-
ing induction and coinduction that clearly reveals the duality between the two
principles. Our approach has the advantage that the same signature is shared
for both inductive and coinductive data, making certain aspects of the rela-
tionship between the two principles more apparent. To achieve this, we extend
and combine two powerful frameworks: semantically we follow the approach of
transitive closure logic, a generic logic for expressing inductive structures [3,14–
16,31,39,51]; for deduction, we adopt non-well-founded proof theory [2,5,10–
12,17–19,23,24,26,50,55]. This combination captures the intuitive dynamics of
inductive and coinductive reasoning, reﬂecting how these principles are under-
stood and applied in practice.
Transitive closure (RTC) logic minimally extends ﬁrst-order logic by adding
a single, intuitive notion: an operator, RTC, for forming the (reﬂexive) tran-
sitive closures of an arbitrary formula (more precisely, of the binary relation
induced by the formula). This operator alone is suﬃcient for capturing all ﬁni-
tary induction schemes within a single, uniﬁed language (unlike other systems
that are a priori parametrized by a set of inductive deﬁnitions [12,40,42,58]).
Transitive closures arise as least ﬁxed points of certain composition operators. In
this paper we extend RTC logic with the semantically dual notion: an operator,
RTC op, for forming greatest ﬁxed points of these same composition operators.1
We call these transitive co-closures, and show that they are equally as intuitive.
Just as transitive closure captures induction, we show that transitive co-closure
facilitates coinductive deﬁnitions and reasoning.
Non-well-founded proof theory formalises the inﬁnite-descent style of induc-
tion. It enables a separation between local steps of deductive inference and global
well-foundedness arguments (i.e. induction), which are encoded in traces of for-
mulas through possibly inﬁnite derivations. A major beneﬁt of these systems
is that inductive invariants do not need to be explicit. On the other hand,
existing approaches for combining induction and coinduction rely on making
(co)invariants explicit within proofs [4,30,59]. In previous work, a non-well-
founded proof system for RTC logic was developed [17,18]. In this paper, we
show that the meaning of the transitive co-closure operator can be captured
proof-theoretically using inference rules having the exact same structure, with
soundness now requiring inﬁnite ascent (i.e. showing productivity) rather than
descent. What obtains is a proof system in which induction and coinduction are
smoothly integrated, and which very clearly highlights their similarities. Their
diﬀerences are also thrown into relief, consisting in the way formulas are traced
in a proof derivation. Speciﬁcally, traces of RTC formulas show that certain
inﬁnite paths cannot exist (induction is well-founded), while traces of RTC op
formulas show that other inﬁnite paths must exist (coinduction is productive).
To demonstrate that our system naturally captures patterns of mixed induc-
tive/coinductive reasoning, we formalise one of the most well-known examples
of a coinductive data type: streams. In particular, we consider two illustrative
1 The notation RTC op comes from the categorical notion of the opposite (dual)
category.

Integrating Induction and Coinduction via Closure Operators
377
examples: transitivity of the lexicographic ordering on streams; and transitivity
of the substream relation. Both are known to be hard to prove. Our system han-
dles these without recourse to general ﬁxpoint operators or algebraic structures.
The transitive (co-)closure framework is contained in the ﬁrst-order mu-
calculus [43], but oﬀers several advantages. The concept of transitive (co-)closure
is intuitively simpler than that of general ﬁxed-point operators, and does not
require any syntactic restrictions to ensure monotonicity. Our framework is also
related, but complementary to logic programming with coinductive interpreta-
tions [52,53] and its coalgebraic semantics [34]. Logic programs, built from Horn
clauses, have a ﬁxed intended domain (viz. Herbrand universes), and the seman-
tics of mixing inductive and coinductive interpretations is subtle. Our frame-
work, on the other hand, uses a general syntax that can freely mix closures and
co-closures, and its semantics considers all ﬁrst-order models. Furthermore, the
notion of proof in our setting is more general than the (semantic) notion of proof
in logic programming, in which, for instance, there is no analogous concept of
global trace condition.
Outline. Section 2 presents the syntax and semantics of the extended logic, RTcC.
Section 3 describes how streams and their properties can be expressed in RTcC.
Section 4 presents non-well-founded proof systems for RTcC, showing soundness
and completeness. Section 5 then illustrates how the examples of Sect. 3 are for-
malised in this system. Section 6 concludes with directions for future work.
2
RTcC Logic: Syntax and Semantics
Transitive closure (RTC) logic [3,15] extends the language of ﬁrst-order logic
with a predicate-forming operator, RTC, for denoting the (reﬂexive) transitive
closures of (binary) relations. In this section we extend RTC logic into what we
call transitive (co-)closure (RTcC) logic, by adding a single transitive co-closure
operator, RTC op. Roughly speaking, whilst the RTC operator denotes the set of
all pairs that are related via a ﬁnite chain (or path), the RTC op operator gives
the set of all pairs that are ‘related’ via a possibly inﬁnite chain. In Sect. 3 we
show that this allows capturing coinductive deﬁnitions and reasoning.
For simplicity of presentation we assume (as is standard practice) a desig-
nated equality symbol. Note also that we use the reﬂexive transitive closure;
however the reﬂexive and non-reﬂexive forms are equivalent in the presence of
equality.
Deﬁnition 1 (RTcC Formulas). Let s, t and P range over the terms and pred-
icate symbols, respectively, of a ﬁrst-order signature Σ. The language LRTcC (of
formulas over Σ) is given by the following grammar:
ϕ, ψ ::=s = t | P(t1, . . . , tn) | ¬ϕ | ∀x . ϕ | ∃x . ϕ | ϕ ∧ψ | ϕ ∨ψ | ϕ →ψ |
(RTC x,y ϕ)(s, t) | (RTC op
x,y ϕ)(s, t)
where the variables x and y in the formulas (RTC x,y ϕ)(s, t) and (RTC op
x,y ϕ)(s, t)
must be distinct and are bound in the subformula ϕ, referred to as the body.

378
L. Cohen and R. N. S. Rowe
The semantics of formulas is an extension of the standard semantics of ﬁrst-
order logic. We write M and ν to denote a ﬁrst-order structure over a (non-
empty) domain D and a valuation of variables in D, respectively. We denote by
ν[x1 := dn, . . . , xn := dn] the valuation that maps xi to di for each i and behaves
as ν otherwise. We write ϕ {t1/x1, . . . , tn/xn} for the result of simultaneously
substituting each ti for the free occurrences of xi in ϕ. We use (di)i≤n to denote a
non-empty sequence of elements d1, . . . , dn; and (di)i>0 for a (countably) inﬁnite
sequence of elements d1, d2, . . .. We use ≡to denote syntactic equality.
Deﬁnition 2 (Semantics).
Let M be a structure for LRTcC, and ν a valua-
tion in M. The satisfaction relation M, ν |= ϕ extends the standard satisfaction
relation of classical ﬁrst-order logic with the following clauses:
M, ν |= (RTC x,y ϕ)(s, t) ⇔
∃(di)i≤n . d1 = ν(s) ∧dn = ν(t) ∧∀i < n . M, ν[x := di, y := di+1] |= ϕ
M, ν |= (RTC op
x,y ϕ)(s, t) ⇔
∃(di)i>0 . d1 = ν(s) ∧∀i > 0 . di = ν(t) ∨M, ν[x := di, y := di+1] |= ϕ
Intuitively, the formula (RTC x,y ϕ)(s, t) asserts that there is a (possibly empty)
ﬁnite ϕ-path from s to t. The formula (RTC op
x,y ϕ)(s, t) asserts that either there
is a (possibly empty) ﬁnite ϕ-path from s to t, or an inﬁnite ϕ-path starting
at s.
We can connect these closure operators to the general theory of ﬁxed points,
with (RTC x,y ϕ) and (RTC op
x,y ϕ) denoting, respectively, the least and greatest
ﬁxed points of a certain operator on binary relations.
Deﬁnition 3 (Composition Operator). Given a binary relation X, we def-
ine an operator ΨX on binary relations, which post-composes its input with X,
by: ΨX(R) = X ∪(X ◦R) = {(a, c) | (a, c) ∈X ∨∃b . (a, b) ∈X ∧(b, c) ∈R}.
Notice that the set of all binary relations (over some given domain) forms a
complete lattice under the subset ordering ⊆. Moreover, composition operators
ΨX are monotone w.r.t. ⊆. Thus we have the following standard results, from
the Knaster–Tarski theorem. For any binary relation X, the least ﬁxed point
lfp(ΨX) of ΨX is given by lfp(ΨX) = {R | ΨX(R) ⊆R}, i.e. the intersection of
all its preﬁxed points. Dually, the greatest ﬁxed point gfp(ΨX) of ΨX is given
by the union of all its postﬁxed points, i.e. gfp(ΨX) = {R | R ⊆ΨX(R)}.
Via the usual notion of formula deﬁnability, RTC and RTC op are easily seen
to be ﬁxed point operators. For a model M and valuation ν, denote the binary
relation deﬁned by a formula ϕ with respect to x and y by [[ϕ]]M,ν
x,y
= {(a, b) |
M, ν[x := a, y := b] |= ϕ}.
Proposition 1. The following hold.
(i) M, ν |= (RTC x,y ϕ)(s, t) iﬀν(s) = ν(t) or (ν(s), ν(t)) ∈lfp(Ψ[[ϕ]]M,ν
x,y ).
(ii) M, ν |= (RTC op
x,y ϕ)(s, t) iﬀν(s) = ν(t) or (ν(s), ν(t)) ∈gfp(Ψ[[ϕ]]M,ν
x,y ).

Integrating Induction and Coinduction via Closure Operators
379
Note that labelling the co-closure ‘transitive’ is justiﬁed since, for any model M,
valuation ν, and formula ϕ, the relation gfp(Ψ[[ϕ]]M,ν
x,y ) is indeed transitive.
The RTC op operator enjoys dualisations of properties governing the transi-
tive closure operator (see, e.g., [16, Proposition 3]) that are either symmetrical,
or involve the ﬁrst component. This is because the semantics of the RTC op has
an embedded asymmetry between the arguments. Reasoning about closures is
based on decomposition into one step and the remaining path. For RTC, this
decomposition can be done in both directions, but for RTC op it can only be
done in one direction.
Proposition 2. The following formulas, connecting the two operators, are valid.
i) (RTC x,y ϕ)(s, t) →(RTC op
x,y ϕ)(s, t)
ii) ¬(RTC x,y ¬ϕ)(s, t) →(RTC op
x,y ϕ)(s, t)
iii) ¬(RTC op
x,y ¬ϕ)(s, t) →(RTC x,y ϕ)(s, t)
iv) ((RTC op
x,y ϕ)(s, t) ∧∃z.¬(RTC op
x,y ϕ)(s, z)) →(RTC x,y ϕ)(s, t)
v) ((RTC op
x,y ϕ)(s, t) ∧¬(RTC op
x,y ϕ ∧y ̸= t)(s, t)) →(RTC x,y ϕ)(s, t)
Note that the converse of these properties do not hold in general, thus they do
not provide characterisations of one operator in terms of the other. A counter-
example for the converses of (ii) and (iii) can be obtained by taking ϕ to be
x = y. Then, for any domain D, the formulas (RTC x,y ¬ϕ), (RTC op
x,y ϕ), and
(RTC op
x,y ¬ϕ) all denote the full binary relation D×D, while (RTC x,y ϕ) denotes
the identity relation on D.
3
Streams in RTcC Logic
This section demonstrates the adequacy of RTcC logic for formalising and rea-
soning about coinductive data types. As claimed by Rutten: “streams are the
best known example of a ﬁnal coalgebra and oﬀer a perfect playground for the use
of coinduction, both for deﬁnitions and for proofs.” [47]. Hence, in this section
and Sect. 5 we illustrate that RTcC logic naturally captures the stream data type
(see, e.g., [29,48]).
3.1
The Stream Datatype
We formalise streams as inﬁnite lists, using a signature consisting of the standard
list constructors: the constant nil and the (inﬁx) binary function symbol ‘::’,
traditionally referred to as ‘cons’. These are axiomatized by:
nil = e :: σ ⇒
(1)
e :: σ = e′ :: σ′ ⇒e = e′
(2)
e :: σ = e′ :: σ′ ⇒σ = σ′ (3)
Note that for simplicity of presentation we have not speciﬁed that the ele-
ments of possibly inﬁnite lists should be any particular sort (e.g. numbers). Thus,
the theory of streams we formulate here is generic in this respect. To refer specif-
ically to streams over a particular domain, we could use a multisorted signature

380
L. Cohen and R. N. S. Rowe
containing a Base sort, in addition to the sort List∞of possibly inﬁnite lists, with
nil a constant of type List∞and :: a function of type Base × List∞−→List∞.
Nonetheless, we do use the following conventions for formalising streams in this
section and in Sect. 5. For variables and terms ranging over Base we use a, b, c, . . .
and e, e′, . . . , respectively; and for variables and terms ranging over possibly inﬁ-
nite lists we use x, y, z, . . . and σ, σ′, . . . , respectively.
The (graphs of) the standard head (hd) and tail (tl) functions are deﬁnable2
by hd(σ) = e
def
:= ∃x.σ = e :: x and tl(σ) = σ′
def
:= ∃a.σ = a :: σ′. Finite and
possibly inﬁnite lists can be deﬁned by using the transitive closure and co-closure
operators, respectively, as follows.
List(σ)
def
:= (RTC x,y tl(x) = y)(σ, nil)
List∞(σ)
def
:= (RTC op
x,y tl(x) = y)(σ, nil)
Roughly speaking, these formulas assert that we can perform some number of
successive tail decompositions of the term σ. For the RTC formula, this decom-
position must reach the second component, nil, in a ﬁnite number of steps. For
the RTC op formula, on the other hand, the decomposition is not required to
reach nil but, in case it does not, must be able to continue indeﬁnitely.
To deﬁne the notion of a necessarily inﬁnite list (i.e. a stream), we specify
in the body that, at each step, the decomposition of the stream cannot actually
reach nil (abbreviating ¬(s = t) by s ̸= t). Moreover, since we are using reﬂexive
forms of the operators we must also stipulate that nil itself is not a stream.
Stream(σ)
def
:= (RTC op
x,y tl(x) = y ∧y ̸= nil)(σ, nil) ∧σ ̸= nil
This technique—of specifying that a single step cannot reach nil and then
taking nil to be the terminating case in the RTC op formula—is a general method
we will use in order to restrict attention to the inﬁnite portion in the induced
semantics of an RTC op formula. To this end, we deﬁne the following notation.
ϕinf
x,y(σ)
def
:= (RTC op
x,y (ϕ ∧y ̸= nil))(σ, nil) ∧σ ̸= nil
3.2
Relations and Operations on Streams
We next show that RTcC also naturally captures properties of streams. Using the
RTC operator we can (inductively) deﬁne the extension relation ◁on possibly
inﬁnite lists as follows:
σ ◁σ′ def
:= (RTC x,y tl(x) = y)(σ, σ′)
This asserts that σ extends σ′, i.e. that σ is obtained from σ′ by prepending
some ﬁnite sequence of elements to σ′. Equivalently, σ′ is obtained by some
ﬁnite number of tail decompositions from σ: that is, σ′ is a suﬃx of σ.
2 Although hd(σ) and tl(σ) could have been deﬁned as terms using Russell’s
ι
operator,
we opted for the above deﬁnition for simplicity of the proof theory.

Integrating Induction and Coinduction via Closure Operators
381
We next formalise some standard predicates.
Contains(e, σ)
def
:= ∃x . σ ◁x ∧hd(x) = e
Const(e, σ)
def
:= (x = e :: y)
inf
x,y(σ)
Const→
∞(σ)
def
:= ∃x . σ ◁x ∧∃a . Const(a, x)
Contains(e, ·) deﬁnes the possibly inﬁnite lists that contain the element denoted
by e; Const(e, ·) deﬁnes the constant stream consisting of the element denoted
by e; and Const→
∞deﬁnes streams that are eventually constant.
We next consider how (functional) relations on streams can be formalised
in RTcC, using some illustrative examples. To capture these we need to use
ordered pairs. For this, we use the notation ⟨u, v⟩for u :: (v :: nil),3 then abbreviate
(RTC w,w′ ∃u, u′, v, v′ . w = ⟨u, v⟩∧w′ = ⟨u′, v′⟩∧ϕ) by (RTC ⟨u,v⟩,⟨u′,v′⟩ϕ) (and
similarly for RTC op formulas), and also write ϕinf
⟨x1,x2⟩,⟨y1,y2⟩(⟨σ, σ′⟩) to stand for
(RTC op
⟨x1,x2⟩,⟨y1,y2⟩(ϕ ∧y1 ̸= nil ∧y2 ̸= nil))(⟨σ, σ′⟩, ⟨nil, nil⟩) ∧σ ̸= nil ∧σ′ ̸= nil.
Append and Periodicity. With ordered pairs, we can inductively deﬁne (the
graph of) the function that appends a possibly inﬁnite list to a ﬁnite list.
σ1⌢σ2 = σ3
def
:=
(RTC ⟨x1,x2⟩,⟨y1,y2⟩∃a . x1 = a :: y1 ∧x2 = a :: y2)(⟨σ1, σ3⟩, ⟨nil, σ2⟩)
We remark that the formulas σ ◁σ′ and ∃z . z⌢σ′ = σ are equivalent. To deﬁne
this as a function requires also proofs that the deﬁned relation is total and
functional. However, this is generally straightforward when the body formula is
deterministic, as is the case in all the examples we present here. Other standard
operations on streams, such as element-wise operations, are also deﬁnable in
RTcC as (functional) relations. For example, assuming a unary function ⊕, we
can coinductively deﬁne its elementwise extension to streams ⊕∞as follows.
⊕∞(σ) = σ′ def
:= (∃a . x1 = a :: y1 ∧x2 = ⊕(a) :: y2)
inf
⟨x1,x2⟩,⟨y1,y2⟩(⟨σ, σ′⟩)
As an example of mixing induction and coinduction, we can express a predicate
coinductively deﬁning the periodic streams using the append function.
Periodic(σ)
def
:= ∃z . z ̸= nil ∧(z⌢y = x)
inf
x,y(σ)
Lexicographic Ordering. The lexicographic order on streams extends point-
wise an order on the underlying elements. Thus, we assume a binary relation
symbol ≤with the standard axiomatisation of a (non-strict) partial order.
⇒e ≤e
e ≤e′, e′ ≤e′′ ⇒e ≤e′′
e ≤e′, e′ ≤e ⇒e = e′
3 Here we use the fact that ‘::’ behaves as a pairing function. In other languages one
might need to add a function ⟨·, ·⟩, and (axiomatically) restrict the semantics to
structures that interpret it as a pairing function. Note that incorporating pairs is
equivalent to taking 2n-ary operators RTC n and RTC op
n for every n ≥1.

382
L. Cohen and R. N. S. Rowe
The lexicographic ordering relation ≤ℓis captured as follows, where we use e < e′
as an abbreviation for e ≤e′ ∧e ̸= e′.
σ ≤ℓσ′ def
:= (RTC op
⟨x1,x2⟩,⟨y1,y2⟩ψℓ)(⟨σ, σ′⟩, ⟨nil, nil⟩)
where ψℓ≡∃a, b, z1, z2 . x1 = a :: z1 ∧x2 = b :: z2 ∧
Stream(z1) ∧Stream(z2) ∧(a < b ∨(a = b ∧z1 = y1 ∧z2 = y2))
The semantics of the RTC op operator require an inﬁnite sequence of pairs such
that, until ⟨nil, nil⟩is reached, each two consecutive pairs are related by ψℓ. This
formula states that if the heads of the lists in the ﬁrst pair are equal, the next
pair of lists in the inﬁnite sequence is their two tails, thus the lexicographic
relation must also hold of them. Otherwise, if the head of the ﬁrst is less than
that of the second, nothing is required of the tails, i.e. they may be any streams.
Substreams. We consider one stream to be a substream of another if the latter
contains every element of the former in the same order (although it may con-
tain other elements too). Equivalently, the latter is obtained by inserting some
(possibly inﬁnite) number of ﬁnite sequences of elements in between those of the
former. This description makes it clearer that deﬁning this relation involves mix-
ing (or, rather, nesting) induction and coinduction. We formalise the substream
relation, ≽using the inductive extension relation ◁to capture the inserted ﬁnite
sequences, wrapping it within a coinductive deﬁnition using the RTC op operator.
σ ≽σ′ def
:= ψ≽
inf
⟨x1,x2⟩,⟨y1,y2⟩(⟨σ, σ′⟩)
where ψ≽≡∃a . x1 ◁a :: y1 ∧x2 = a :: y2
On examination, one can observe that this relation is transitive. However, prov-
ing this is non-trivial and, unsurprisingly, involves applying both induction and
coinduction. In Sect. 5, we give a proof of the transitivity of ≽in RTcC. This
relation was also considered at length in [6, §5.1.3] where it is formalised in
terms of selectors, which form streams by picking out certain elements from
other streams. The treatment in [6] requires some heavy (coalgebraic) metathe-
ory. While our proof in Sect. 5 requires some (fairly obvious) lemmas, the basic
structure of the (co)inductive reasoning required is made plain by the cycles in
the proof. Furthermore, the RTcC presentation seems to enable a more intu-
itive understanding of the nature of the coinductive deﬁnitions and principles
involved.
4
Proof Theory
We now present a non-well-founded proof system for RTcC, which extends (an
equivalent of) the non-well-founded proof system considered in [17,18] for tran-
sitive closure logic (i.e. the RTC-fragment of RTcC).

Integrating Induction and Coinduction via Closure Operators
383
Γ ⇒Δ, (RTC x,y ϕ)(s, s)
(4)
Γ ⇒Δ, ϕ {s/x, r/y}
Γ ⇒Δ, (RTC x,y ϕ)(r, t)
Γ ⇒Δ, (RTC x,y ϕ)(s, t)
(5)
Γ, s = t ⇒Δ
Γ, ϕ {s/x, z/y}, (RTC x,y ϕ)(z, t) ⇒Δ
(†)
Γ, (RTC x,y ϕ)(s, t) ⇒Δ
(6)
Γ ⇒Δ, (RTC op
x,y ϕ)(s, s)
(7)
Γ ⇒Δ, ϕ {s/x, r/y}
Γ ⇒Δ, (RTC op
x,y ϕ)(r, t)
Γ ⇒Δ, (RTC op
x,y ϕ)(s, t)
(8)
Γ, s = t ⇒Δ
Γ, ϕ {s/x, z/y} , (RTC op
x,y ϕ)(z, t) ⇒Δ
(‡)
Γ, (RTC op
x,y ϕ)(s, t) ⇒Δ
(9)
where: (†) z ̸∈fv(Γ, Δ, (RTC x,y ϕ)(s, t)); and (‡) z ̸∈fv(Γ, Δ, (RTC op
x,y ϕ)(s, t)).
Fig. 1. Proof rules of RTcC∞
G
4.1
A Non-well-Founded Proof System
In non-well-founded proof systems, e.g. [2,5,10–12,23,24,50], proofs are allowed
to be inﬁnite, i.e. non-well-founded trees, but they are subject to the restriction
that every inﬁnite path in the proof admits some inﬁnite progress, witnessed by
tracing terms or formulas. The inﬁnitary proof system for RTcC logic is deﬁned
as an extension of LK=, the sequent calculus for classical ﬁrst-order logic with
equality and substitution [28,56].4 Sequents are expressions of the form Γ ⇒Δ,
for ﬁnite sets of formulas Γ and Δ. We abbreviate Γ, Δ and Γ, ϕ by Γ ∪Δ and
Γ ∪{ϕ}, respectively, and write fv(Γ) for the set of free variables of the formulas
in Γ. A sequent Γ ⇒Δ is valid if and only if the formula 
ϕ∈Γ ϕ →
ψ∈Δ ψ is.
Deﬁnition 4 (RTcC∞
G ). The proof system RTcC∞
G is obtained by adding to LK=
the proof rules given in Fig. 1.
Rules (6), and (8) are the unfolding rules for the two operators that represent
the induction and coinduction principles in the system, respectively. The proof
rules for both operators have exactly the same form, and so the reader may
wonder what it is, then, that distinguishes the behaviour of the two operators.
The diﬀerence proceeds from the way the decomposition of the corresponding
formulas is traced in the non-well-founded proof system. For induction, RTC
4 Unlike in the original system, here we take LK= to include the substitution rule.

384
L. Cohen and R. N. S. Rowe
formulas on the left-hand side of the sequents are traced through Rule (6);
for coinduction, RTC op formulas on the right-hand side of sequents are traced
through Rule (8).
Deﬁnition 5 (RTcC∞
G Pre-proofs). An RTcC∞
G pre-proof is a rooted, possi-
bly non-well-founded (i.e. inﬁnite) derivation tree constructed using the RTcC∞
G
proof rules. A path in a pre-proof is a possibly inﬁnite sequence S0, S1, . . . (, Sn)
of sequents with S0 the root of the proof, and Si+1 a premise of Si for each i < n.
We adopt the usual proof-theoretic notions of formula occurrence and sub-
occurrence, and of ancestry between formulas [13]. A formula occurrence is called
a proper formula if it is not a sub-occurrence of any formula.
Deﬁnition 6 ((Co-)Traces). A trace (resp. co-trace) is a possibly inﬁnite
sequence τ1, τ2, . . . (, τn) of proper RTC (resp. RTC op) formula occurrences in
the left-hand (resp, right-hand) side of sequents in a pre-proof such that τi+1 is
an immediate ancestor of τi for each i > 0. If the trace (resp. co-trace) contains
an inﬁnite number of formula occurrences that are principal for instances of Rule
(6) (resp. Rule (8)), then we say that it is inﬁnitely progressing.
As usual in non-well-founded proof theory, we use the notion of (co-)trace to
deﬁne a global trace condition, distinguishing certain ‘valid’ pre-proofs.
Deﬁnition 7 (RTcC∞
G Proofs). An RTcC∞
G proof is a pre-proof in which every
inﬁnite path has a tail followed by an inﬁnitely progressing (co-)trace.
In general, one cannot reason eﬀectively about inﬁnite proofs, as found in
RTcC∞
G . In order to do so our attention has to be restricted to those proof trees
which are ﬁnitely representable. That is, the regular inﬁnite proof trees, con-
taining only ﬁnitely many distinct subtrees. They can be speciﬁed as systems of
recursive equations or, alternatively, as cyclic graphs [22]. One way of formalis-
ing such proof graphs is as standard proof trees containing open nodes (called
buds), to each of which is assigned a syntactically equal internal node of the
proof (called a companion). The restriction to cyclic proofs provides the basis
for an eﬀective system for automated inductive and coinductive reasoning. The
system RTcC∞
G can naturally be restricted to a cyclic proof system for RTcC
logic as follows.
Deﬁnition 8 (Cyclic Proofs). The cyclic proof system RTcCω
G for RTcC logic
is the subsystem of RTcC∞
G comprising of all and only the ﬁnite and regular inﬁ-
nite proofs (i.e. proofs that can be represented as ﬁnite, possibly cyclic, graphs).5
It is decidable whether a cyclic pre-proof satisﬁes the global trace condition,
using a construction involving an inclusion between B¨uchi automata [10,54].
However since this requires complementing B¨uchi automata (a PSPACE proce-
dure), RTcCω
G is not a proof system in the Cook-Reckhow sense [21]. Notwith-
standing, checking the trace condition for cyclic proofs found in practice is not
prohibitive [45,57].
5 Note that in [17,18] RTCω
G denoted the full inﬁnitary system for the RTC-fragment.

Integrating Induction and Coinduction via Closure Operators
385
(7)
⇒(RTC op
x,y ϕ)(u, u)
(Eq)
u = v ⇒(RTC op
x,y ϕ)(u, v)
.
.
.
.
.
.
.
(Ax)
ϕ {u/x, w/y}, (RTC x,y ϕ)(w, v) ⇒ϕ {u/x, w/y}
.
.
.
.
(RTC x,y ϕ)(u, v)
⇒
(RTC op
x,y ϕ)(u, v)
(Subst)
(RTC x,y ϕ)(w, v)
⇒
(RTC op
x,y ϕ)(w, v)
(Wk)
ϕ {u/x, w/y}, (RTC x,y ϕ)(w, v)
⇒
(RTC op
x,y ϕ)(w, v)
∗
(8)
ϕ {u/x, w/y}, (RTC x,y ϕ)(w, v)
† ⇒
(RTC op
x,y ϕ)(u, v)
(6)
(RTC x,y ϕ)(u, v)
⇒
(RTC op
x,y ϕ)(u, v)
Fig. 2. Proof in RTcCω
G of (RTC x,y ϕ)(u, v) ⇒(RTC op
x,y ϕ)(u, v)
Although RTcC∞
G is complete (cf. Theorem 2 below) RTcCω
G is not, since
arithmetic can be encoded in RTcC logic and the set of RTcCω
G proofs is recur-
sively enumerable.6 Nonetheless, RTcCω
G is adequate for RTcC logic in the sense
that it suﬃces for proving the standard properties of the operators, as in, e.g.,
Proposition 2.
Example 1. Figure 2 demonstrates an RTcCω
G proof that the transitive closure
is contained within the transitive co-closure. Notice that the proof has a single
cycle, and thus a single inﬁnite path. Following this path, there is both a trace
(consisting of the highlighted RTC formulas, on the left-hand side of sequents)
which progresses on traversing Rule (6) (marked †), and a co-trace (consisting
of the highlighted RTC op forumlas, on the right-hand side of sequents), which
progresses on traversing Rule (8) (marked ∗). Thus, Fig. 2 can be seen both as
a proof by induction and a proof by coinduction. It exempliﬁes how naturally
such reasoning can be captured within RTcCω
G.
A salient feature of non-well-founded proof systems, including this one, is
that (co)induction invariants need not be mentioned explicitly, but instead are
encoded in the cycles of a proof. This facilitates the automation of such reasoning,
as the invariants may be interactively constructed during a proof-search process.
4.2
Soundness
To show soundness, i.e. that all derived sequents are valid, we establish that
the inﬁnitely progressing (co-)traces in proofs preclude the existence of counter-
models. By local soundness of the proof rules, any given counter-model for a
sequent derived by a proof identiﬁes an inﬁnite path in the proof consisting of
6 The RTC-fragment of RTcCω
G was shown complete for a Henkin-style semantics [17].

386
L. Cohen and R. N. S. Rowe
invalid sequents. However, the presence of a (co-)trace along this path entails a
contradiction (and so conclude that no counter-models exist). From a trace, one
may infer the existence of an inﬁnitely descending chain of natural numbers. This
relies on a notion of (well-founded) measure for RTC formulas, viz. the measure
of φ ≡(RTC x,y ϕ)(s, t) with respect to a given model M and valuation ν—
denoted by δφ(M, ν)—is deﬁned to be the minimum number of ϕ-steps needed to
connect ν(s) and ν(t) in M. Conversely, from a co-trace beginning with a formula
(RTC op
x,y ϕ)(s, t) one can construct an inﬁnite sequence of ϕ-steps beginning at
s, i.e. a witness that the counter-model does in fact satisfy (RTC op
x,y ϕ)(s, t).
The key property needed for soundness of the proof system is the following
strong form of local soundness for the proof rules.
Proposition 3 (Trace Local Soundness). Let M be a model and ν a valu-
ation that invalidate the conclusion of an instance of an RTcC∞
G inference rule;
then there exists a valuation ν′ that invalidates some premise of the inference
rule such that the following hold.
1. If (τ, τ ′) is a trace following the path from the conclusion to the invalid
premise, then δτ ′(M, ν′) ≤δτ(M, ν); moreover δτ ′(M, ν′) < δτ(M, ν) if the
rule is an instance of (6) and τ is the principal formula.
2. If (τ, τ ′) is a co-trace following the path from the conclusion to the invalid
premise, with τ ≡(RTC op
x,y ϕ)(s, t) and τ ′ ≡(RTC op
x,y ϕ′)(r, t′), then: (a)
M, ν[x := d, y := d′] |= ϕ if and only if M, ν′[x := d, y := d′] |= ϕ′, for all ele-
ments d and d′ in M; and (b) M, ν′ |= ϕ {s/x, r/y} if τ is the principal formula
of an instance of (8), and ν(s) = ν′(r) otherwise.
The global soundness of the proof system then follows.
Theorem 1 (Soundness of RTcC∞
G ). Sequents derivable in RTcC∞
G are valid.
Proof. Take a proof deriving Γ ⇒Δ. Suppose, for contradiction, that there is
a model M and valuation ν1 invalidating Γ ⇒Δ. Then by Proposition 3 there
exists an inﬁnite path of sequents (Si)i>0 in the proof and an inﬁnite sequence
of valuations (νi)i>0 such that M and νi invalidate Si for each i > 0. Since the
proof must satisfy the global trace condition, this inﬁnite path has a tail (Si)i>k
followed by an inﬁnitely progressing (co-)trace (τ i)i>0.
– If (τ i)i>0 is a trace, Proposition 3 implies an inﬁnitely descending chain of
natural numbers: δτ1(Mk+1, νk+1) ≤δτ2(Mk+2, νk+2) ≤. . .
– If (τ i)i>0 is a co-trace, with τ1 ≡(RTC op
x,y ϕ)(s, t) and M, νk+1 ̸|= τ1, then
Proposition 3 entails that there is an inﬁnite sequence of terms t0, t1, t2, . . .
with s ≡t0 such that M, νk+1[x:=νk+1(tj), y:=νk+1(tj+1)] |= ϕ for each
j ≥0. That is, it follows from Deﬁnition 2 that M, νk+1 |= (RTC op
x,y ϕ)(s, t).
In both cases we have a contradiction, so conclude that Γ ⇒Δ is valid.
⊓⊔
Since every RTcCω
G proof is also an RTcC∞
G proof, soundness of RTcCω
G is an
immediate corollary.
Corollary 1. A sequent Γ ⇒Δ is valid if there is an RTcCω
G proof deriving it.

Integrating Induction and Coinduction via Closure Operators
387
4.3
Completeness
The completeness proof for RTcC∞
G is obtained by extending the completeness
proof of the RTC-fragment of RTcC∞
G found in [17,18], which, in turn, follows a
standard technique used in e.g. [12]. We next outline the core of the proof, full
details can be found in the appendix.
Roughly speaking, for a given sequent Γ ⇒Δ one constructs a ‘search
tree’ which corresponds to an exhaustive search strategy for a cut-free proof
for the sequent. Search trees are, by construction, recursive and cut-free. In
case the search tree is not an RTcC∞
G proof (and there are no open nodes) it
must contain some untraceable inﬁnite branch, i.e. one that does not satisfy
the global trace condition. We then collect the formulas occurring along such
an untraceable branch to construct a (possibly inﬁnite) ‘sequent’, Γω ⇒Δω
(called a limit sequent), and construct the Herbrand model Mω of open terms
quotiented by the equalities it contains. That is, taking ∼to be the smallest
congruence on terms such that s ∼t whenever s = t ∈Γω, the elements of
Mω are ∼-equivalence classes and every k-ary relation symbol q is interpreted
as {([t1], . . . , [tk]) | q(t1, . . . , tk) ∈Γω} (here [t] denotes the ∼-equivalence class
containing t). This model, together with the valuation νω deﬁned by νω(x) = [x]
for all variables x, can be shown to invalidate the sequent Γ ⇒Δ. The com-
pleteness result therefore follows.
Theorem 2 (Completeness). All valid sequents are derivable in RTcC∞
G .
Proof. Given any sequent S, if some search tree for S is not an RTcC∞
G proof then
it has an untraceable branch, and the model Mω and valuation νω constructed
from the corresponding limit sequent invalidate S. Thus if S is valid, then the
search tree is a recursive RTcC∞
G proof deriving S.
⊓⊔
We obtain admissibility of cut for the full inﬁnitary system as the search tree,
by construction, is cut-free. Since the construction of the search tree does not
necessarily produce RTcCω
G pre-proofs, we do not obtain a regular completeness
result using this technique.
Corollary 2 (Cut admissibility). Cut is admissible in RTcC∞
G .
5
Proving Properties of Streams
We now demonstrate how (co)inductive reasoning about streams and their prop-
erties is formalised in the cyclic fragment of the proof system presented above.
For the sake of clarity, in the derivations below we elide detailed applications of
the proof rules (including the axioms for list constructors), instead indicating
the principal rules involved at each step. We also elide (using ‘. . .’) formulas in
sequents that are not relevant to the local reasoning at that point.
Transitivity of Lexicographic Ordering. Fig. 3 outlines the main structure
of an RTcCω
G proof deriving the sequent x ≤ℓy, y ≤ℓz ⇒x ≤ℓz, where x, y,

388
L. Cohen and R. N. S. Rowe
(∃R)/(Ax)
···
a < c, . . . ⇒ψℓ{x/x1, z/x2, nil/y1, nil/y2}
(7)
. . . ⇒nil ≤ℓnil
(8)
a < c, . . . ⇒x ≤ℓz
×3
.
.
.
.
.
.
.
(∃R)/(Ax)
···
a = c, . . . ⇒ψℓ{x/x1, z/x2, x′/y1, z′/y2}
x ≤ℓy, y ≤ℓz ⇒
x ≤ℓz
(Wk)/(Subst)
x′ ≤ℓy′, y′ ≤ℓz′, . . . ⇒
x′ ≤ℓz′
∗
(8)
a = c, . . . , x′ ≤ℓy′, y′ ≤ℓz′ ⇒
x ≤ℓz
(∨L)
x = a :: x′, y = b :: y′, z = c :: z′, a < b ∨(a = b ∧x′ = x′′ ∧y′ = y′′
1 ),
b < c ∨(b = c ∧y′ = y′′
2 ∧z′ = z′′), x′′ ≤ℓy′′
1 , y′′
2 ≤ℓz′′, . . . ⇒
x ≤ℓz
(2), (3)
Uℓ(x, y, x′, y′), Uℓ(y, z, y′′, z′) ⇒
x ≤ℓz
(a) Sub-proof containing non-trivial cases.
.
.
.
.
.
.
.
.
.
.
.
.
(Ax)
x ≤ℓz ⇒x ≤ℓz
(Eq)
x = y = nil, y ≤ℓz ⇒x ≤ℓz
(∃R)/(Ax)
···
(8)
Uℓ(x, y, x′, y′) ⇒x ≤ℓw
(Eq)
Uℓ(x, y, x′, y′), w = z = nil ⇒x ≤ℓz
.
.
.
.
.
.
Figure 3a
···
Uℓ(x, y, x′, y′), Uℓ(y, z, y′′, z′) ⇒
x ≤ℓz
Uℓ(x, y, x′, y′), y ≤ℓz ⇒
x ≤ℓz
(9)
z ≤ℓy, y ≤ℓz ⇒
x ≤ℓz
(b) Root of the proof, with trivial cases.
Fig. 3. High-level structure of an RTcCω
G proof of transitivity of ≤ℓ.
and z are distinct variables. All other variables in Fig. 3 are freshly introduced.
Uℓ(σ1, σ2, σ′
1, σ′
2) abbreviates the set {ψℓ{σ1/x1, σ2/x2, σ′
1/y1, σ′
2/y2} , σ′
1 ≤ℓσ′
2}
(i.e. the result of unfolding the step case of the formula σ1 ≤ℓσ2 using σ′
1
and σ′
2 as the intermediate terms).
The proof begins by unfolding the deﬁnitions of x ≤ℓy and y ≤ℓz, shown
in Fig. 3b. The interesting part is the sub-proof shown in Fig. 3a, when each of
the lists is not nil. Here, we perform case splits on the relationship between the
head elements a, b, and c. For the case a = c, i.e. the heads are equal, when
unfolding the formula x ≤ℓz on the right-hand side, we instantiate the second
components of the RTC op formula to be the tails of the streams, x′ and z′. In the
left-hand premise we must show ψℓ{x/x1, z/x2, x′/y1, z′/y2}, which can be done by
matching with formulas already present in the sequent. The right-hand premise
must derive x′ ≤ℓz′, i.e. the tails are lexicographically related. This is where we
apply the coinduction principle, by renaming the variables and forming a cycle in

Integrating Induction and Coinduction via Closure Operators
389
(Ax)
···
. . . ⇒ψ≽{x/x1, z/x2, x′/y1, z′/y2}
x ≽y, y ≽z ⇒
x ≽z
(Wk)/(Subst)
x′ ≽y′′, y′′ ≽z′, . . . ⇒
x′ ≽z′
∗
(8)
x ◁b :: x′, z = b :: z′, x′ ≽y′′, y′′ ≽z′, . . . ⇒
x ≽z
.
.
.
.
.
.
.
.
.
.
x′ ≽y′, y′ ◁b :: y′′
⇒∃x′′ . x′ ◁b :: x′′ ∧x′′ ≽y′′
.
.
.
x ◁a :: x′ , x′ ◁b :: x′′ ⇒x ◁b :: x′′
...
.
.
.
(Ax)
···
. . . ⇒ψ≽{x/x1, z/x2, x′/y1, z′/y2}
x ≽y, y ≽z ⇒
x ≽z
(Wk)/(Subst)
x′′ ≽y′′, y′′ ≽z′, . . . ⇒
x′ ≽z′
∗
(8)
x ◁b :: x′′, z = b :: z′, x′′ ≽y′′, y′′ ≽z′, . . . ⇒
x ≽z
(Cut)
x ◁a :: x′, x′ ◁b :: x′′, . . . ⇒
x ≽z
(Cut)
x′ ≽y′, y′ ◁b :: y′′, . . . ⇒
x ≽z
(6)
U≽(x, y, x′, y′), U≽(y, z, y′′, z′) ⇒
x ≽z
···
(6), sim. fig. 3b
x ≽y, y ≽z ⇒
x ≽z
Fig. 4. High-level structure of an RTcCω
G proof of transitivity of ≽.
the proof back to the root. This does indeed produce a proof, since we can form
a co-trace by following the formulas x ≤ℓz, . . . , x′ ≤ℓz′ on the right-hand side
of sequents along this cycle. This co-trace progresses as it traverses the instance
of Rule (8) each time around the cycle (marked ∗).
Transitivity of the Substream Relation. Fig. 4 outlines the structure of an
RTcCω
G proof of the sequent x ≽y, y ≽z ⇒x ≽z, for distinct variables x, y, and
z. As above, other variables are freshly introduced, and we use U≽(σ1, σ2, σ′
1, σ′
2)
to denote the set {ψ≽{σ1/x1, σ2/x2, σ′
1/y1, σ′
2/y2}, σ′
1 ≽σ′
2} (i.e. the result of unfold-
ing the step-case of the formula σ1 ≽σ2 using σ′
1 and σ′
2 as the intermediate
terms).
The reﬂexive cases are handled similarly to the previous example. Again,
the work is in proving the step cases. After unfolding both x ≽y and y ≽z,
we obtain x′ ≽y′ and y′′ ≽z′, as part of U≽(x, y, x′, y′) and U≽(y, z, y′′, z′),
respectively. We also have (for fresh variables a and b) that: (i) x ◁a :: x′; (ii)
y = a :: y′ (y′ is the immediate tail of y); (iii) y ◁b :: y′′ (y′′ is some tail of y);
and (iv) z = b :: z′ (z′ is the immediate tail of z). Ultimately, we are looking
to obtain x ◁b :: x′′ and x′′ ≽y′′ (for some tail x′′), so that we can unfold the
formula x ≽z on the right-hand side to obtain x′′ ≽z′ and thus be able to form
a (coinductive) cycle.

390
L. Cohen and R. N. S. Rowe
The application of Rule (6) shown in Fig. 4 performs a case-split on the
formula y ◁b :: y′′. The left-hand branch handles the case that y′′ is, in fact, the
immediate tail of y; thus y′ = y′′ and a = b, and so we can substitute b and
y′′ in place of a and y′, respectively, and take x′′ to be x′. In the right-hand
branch, corresponding to the case that y′′ is not the immediate tail of y, we
obtain y′ ◁b :: y′′ from the case-split. Then we apply two lemmas; namely: (i) if
x′ ≽y′ and y′ ◁b :: y′′, then there is some x′′ such that x′ ◁b :: x′′ and x′′ ≽y′′;
and (ii) if x ◁a :: x′ and x′ ◁b :: x′′, then x ◁b :: x′′ (a form of transitivity for
the extends relation). For space reasons we do not show the structure of the
sub-proofs deriving these, however, as marked in the ﬁgure, we note that they
are both carried out by induction on the ◁relation.
In summary the proof contains two (inductive) sub-proofs, each validated
by inﬁnitely progressing inductive traces, and also two overlapping outer cycles.
Inﬁnite paths following these outer cycles have co-traces consisting of the high-
lighted formulas in Fig. 4, which progress inﬁnitely often as they traverse the
instances of Rule (8) (marked ∗).
6
Conclusion and Future Work
This paper presented a new framework that extends the well-known, powerful
transitive closure logic with a dual transitive co-closure operator. An inﬁnitary
proof system for the logic was developed and shown to be sound and com-
plete. Its cyclic subsystem was shown to be powerful enough for reasoning over
streams, and in particular automating combinations of inductive and coinductive
arguments.
Much remains to be done to fully develop the new logic and its proof the-
ory, and to study its implications. Although we have shown that our framework
captures many interesting properties of the canonical coinductive data type,
streams, a primary task for future research is to formally characterise its ability
to capture ﬁnitary coinductive deﬁnitions in general. In particular, it seems plau-
sible that RTcC is a good candidate setting in which to look for characterisations
that complement and bridge existing results for coinductive data in automata
theory and coalgebra. That is, it may potentially mirror (and also perhaps even
replace) the role that monadic second order logic plays for (ω-)regular languages.
Another important research task is to further develop the structural proof
theory of the systems RTcC∞
G and RTcCω
G in order to describe the natural process
and dynamics of inductive and coinductive reasoning. This includes properties
such as cut elimination, admissibility of rules, regular forms for proofs, focussing,
and proof search strategies. For example, syntactic cut elimination for non-well-
founded systems has been studied extensively in the context of linear logic [5,26].
The basic approach would seem to work for RTcC, however, one expects that
cut-elimination will not preserve regularity.
Through the proofs-as-programs paradigm (a.k.a. the Curry-Howard corre-
spondence) our proof-theoretic synthesis of induction and coinduction has a num-
ber of applications that invite further investigation. Namely, our framework pro-
vides a general setting for verifying program correctness against speciﬁcations

Integrating Induction and Coinduction via Closure Operators
391
of coinductive (safety) and inductive (liveness) properties. Implementing proof-
search procedures can lead to automation, as well as correct-by-construction
synthesis of programs operating on (co)inductive data. Finally, grounding proof
assistants in our framework will provide a robust, proof-theoretic basis for mech-
anistic coinductive reasoning.
Acknowledgements. We are grateful to Alexandra Silva for valuable coinductive
reasoning examples, and Juriaan Rot for helpful comments and pointers. We also extend
thanks to the anonymous reviewers for their questions and comments.
References
1. Abel, A., Pientka, B.: Well-founded recursion with copatterns and sized types. J.
Funct. Program. 26, e2 (2016). https://doi.org/10.1017/S0956796816000022
2. Afshari, B., Leigh, G.E.: Cut-free completeness for modal mu-calculus. In: Proceed-
ings of the 32nd Annual ACM/IEEE Symposium on Logic in Computer Science
(LICS 2017), Reykjavik, Iceland, 20–23 June 2017, pp. 1–12 (2017). https://doi.
org/10.1109/LICS.2017.8005088
3. Avron, A.: Transitive closure and the mechanization of mathematics. In: Kamared-
dine, F.D. (ed.) Thirty Five Years of Automating Mathematics, Applied Logic
Series. APLS 2013, vol. 28, pp. 149–171. Springer, Netherlands (2003). https://
doi.org/10.1007/978-94-017-0253-9 7
4. Baelde, D.: Least and greatest ﬁxed points in linear logic. ACM Trans. Comput.
Log. 13(1), 2:1–2:44 (2012). https://doi.org/10.1145/2071368.2071370
5. Baelde, D., Doumane, A., Saurin, A.: Inﬁnitary proof theory: the multiplicative
additive case. In: Proceedings of the 25th EACSL Annual Conference on Computer
Science Logic (CSL 2016), 29 August–1 September 2016, Marseille, France, pp.
42:1–42:17 (2016). https://doi.org/10.4230/LIPIcs.CSL.2016.42
6. Basold, H.: Mixed Inductive-Coinductive Reasoning Types, Programs and Logic.
Ph.D. thesis, Radboud University (2018). https://hdl.handle.net/2066/190323
7. Bertot, Y., Casteran, P.: Interactive Theorem Proving and Program Development.
Springer, Heidelberg (2004). https://doi.org/10.1007/978-3-662-07964-5
8. Bove, A., Dybjer, P., Norell, U.: A brief overview of Agda – a functional language
with dependent types. In: Berghofer, S., Nipkow, T., Urban, C., Wenzel, M. (eds.)
TPHOLs 2009. LNCS, vol. 5674, pp. 73–78. Springer, Heidelberg (2009). https://
doi.org/10.1007/978-3-642-03359-9 6
9. Brady, E.: Idris, a general-purpose dependently typed programming language:
design and implementation. J. Funct. Program. 23, 552–593 (2013). https://doi.
org/10.1017/S095679681300018X
10. Brotherston, J.: Formalised inductive reasoning in the logic of bunched implica-
tions. In: Nielson, H.R., Fil´e, G. (eds.) SAS 2007. LNCS, vol. 4634, pp. 87–103.
Springer, Heidelberg (2007). https://doi.org/10.1007/978-3-540-74061-2 6
11. Brotherston, J., Bornat, R., Calcagno, C.: Cyclic proofs of program termination
in separation logic. In: Proceedings of the 35th ACM SIGPLAN-SIGACT Sympo-
sium on Principles of Programming Languages (POPL 2008), pp. 101–112 (2008).
https://doi.org/10.1145/1328438.1328453
12. Brotherston, J., Simpson, A.: Sequent calculi for induction and inﬁnite descent. J.
Log. Comput. 21(6), 1177–1216 (2010). https://doi.org/10.1093/logcom/exq052

392
L. Cohen and R. N. S. Rowe
13. Buss, S.R.: Handbook of proof theory. In: Studies in Logic and the Foundations of
Mathematics. Elsevier Science (1998)
14. Cohen, L.: Completeness for ancestral logic via a computationally-meaningful
semantics. In: Schmidt, R.A., Nalon, C. (eds.) TABLEAUX 2017. LNCS (LNAI),
vol. 10501, pp. 247–260. Springer, Cham (2017). https://doi.org/10.1007/978-3-
319-66902-1 15
15. Cohen, L., Avron, A.: Ancestral logic: a proof theoretical study. In: Kohlenbach,
U., Barcel´o, P., de Queiroz, R. (eds.) WoLLIC 2014. LNCS, vol. 8652, pp. 137–151.
Springer, Heidelberg (2014). https://doi.org/10.1007/978-3-662-44145-9 10
16. Cohen, L., Avron, A.: The middle ground-ancestral logic. Synthese, 1–23 (2015).
https://doi.org/10.1007/s11229-015-0784-3
17. Cohen, L., Rowe, R.N.S.: Uniform inductive reasoning in transitive closure logic
via inﬁnite descent. In: Proceedings of the 27th EACSL Annual Conference on
Computer Science Logic (CSL 2018), 4–7 September 2018, Birmingham, UK, pp.
16:1–16:17 (2018). https://doi.org/10.4230/LIPIcs.CSL.2018.16
18. Cohen, L., Rowe, R.N.S.: Non-well-founded proof theory of transitive closure logic.
Trans. Comput. Logic (2020, to appear). https://arxiv.org/pdf/1802.00756.pdf
19. Cohen, L., Rowe, R.N.S., Zohar, Y.: Towards automated reasoning in Her-
brand structures. J. Log. Comput. 29(5), 693–721 (2019). https://doi.org/10.1093/
logcom/exz011
20. Constable, R.L., et al.: Implementing Mathematics with the Nuprl Proof Develop-
ment System. Prentice-Hall Inc, Upper Saddle River (1986)
21. Cook, S.A., Reckhow, R.A.: The relative eﬃciency of propositional proof systems.
J. Symbolic Log. 44(1), 36–50 (1979). https://doi.org/10.2307/2273702
22. Courcelle, B.: Fundamental properties of inﬁnite trees. Theor. Comput. Sci. 25,
95–169 (1983). https://doi.org/10.1016/0304-3975(83)90059-2
23. Das, A., Pous, D.: Non-Wellfounded Proof Theory for (Kleene+Action)(Algebras+
Lattices). In: Proceedings of the 27th EACSL Annual Conference on Computer
Science Logic (CSL 2018), pp. 19:1–19:18 (2018). https://doi.org/10.4230/LIPIcs.
CSL.2018.19
24. Doumane, A.: Constructive completeness for the linear-time μ-calculus. In: Pro-
ceedings of the 32nd Annual ACM/IEEE Symposium on Logic in Computer Science
(LICS 2017), pp. 1–12 (2017). https://doi.org/10.1109/LICS.2017.8005075
25. Endrullis, J., Hansen, H., Hendriks, D., Polonsky, A., Silva, A.: A coinductive
framework for inﬁnitary rewriting and equational reasoning. In: 26th International
Conference on Rewriting Techniques and Applications (RTA 2015), vol. 36, pp.
143–159 (2015). https://doi.org/10.4230/LIPIcs.RTA.2015.143
26. Fortier, J., Santocanale, L.: Cuts for circular proofs: semantics and cut-elimination.
In: Rocca, S.R.D. (ed.) Computer Science Logic 2013 (CSL 2013). Leibniz Inter-
national Proceedings in Informatics (LIPIcs), vol. 23, pp. 248–262. Dagstuhl, Ger-
many (2013). https://doi.org/10.4230/LIPIcs.CSL.2013.248
27. Gapeyev, V., Levin, M.Y., Pierce, B.C.: Recursive subtyping revealed. J. Funct.
Program. 12(6), 511–548 (2002). https://doi.org/10.1017/S0956796802004318
28. Gentzen, G.: Untersuchungen ¨uber das Logische Schließen. I. Mathematische
Zeitschrift 39(1), 176–210 (1935). https://doi.org/10.1007/BF01201353
29. Hansen, H.H., Kupke, C., Rutten, J.: Stream diﬀerential equations: speciﬁcation
formats and solution methods. In: Logical Methods in Computer Science, vol. 13(1),
February 2017. https://doi.org/10.23638/LMCS-13(1:3)2017
30. Heath, Q., Miller, D.: A proof theory for model checking. J. Autom. Reasoning
63(4), 857–885 (2019). https://doi.org/10.1007/s10817-018-9475-3

Integrating Induction and Coinduction via Closure Operators
393
31. Immerman, N.: Languages that capture complexity classes. SIAM J. Comput.
16(4), 760–778 (1987). https://doi.org/10.1137/0216051
32. Jacobs, B., Rutten, J.: A tutorial on (co) algebras and (co) induction. Bull. Eur.
Assoc. Theor. Comput. Sci. 62, 222–259 (1997)
33. Jeannin, J.B., Kozen, D., Silva, A.: CoCaml: functional programming with regular
coinductive types. Fundamenta Informaticae 150, 347–377 (2017). https://doi.org/
10.3233/FI-2017-1473
34. Komendantskaya, E., Power, J.: Coalgebraic semantics for derivations in logic pro-
gramming. In: Corradini, A., Klin, B., Cˆırstea, C. (eds.) CALCO 2011. LNCS,
vol. 6859, pp. 268–282. Springer, Heidelberg (2011). https://doi.org/10.1007/978-
3-642-22944-2 19
35. Kozen, D., Silva, A.: Practical coinduction. Math. Struct. Comput. Sci. 27(7),
1132–1152 (2017). https://doi.org/10.1017/S0960129515000493
36. Leino, R., Moskal, M.: Co-induction simply: automatic co-inductive proofs
in a program veriﬁer. Technical report MSR-TR-2013-49, Microsoft Research,
July 2013. https://www.microsoft.com/en-us/research/publication/co-induction-
simply-automatic-co-inductive-proofs-in-a-program-veriﬁer/
37. Leroy, X., Grall, H.: Coinductive big-step operational semantics. Inf. Comput.
207(2), 284–304 (2009). https://doi.org/10.1016/j.ic.2007.12.004
38. Lucanu, D., Ro¸su, G.: CIRC: a circular coinductive prover. In: Mossakowski, T.,
Montanari, U., Haveraaen, M. (eds.) CALCO 2007. LNCS, vol. 4624, pp. 372–378.
Springer, Heidelberg (2007). https://doi.org/10.1007/978-3-540-73859-6 25
39. Martin, R.M.: A homogeneous system for formal logic. J. Symbolic Log. 8(1), 1–23
(1943). https://doi.org/10.2307/2267976
40. Martin-L¨of, P.: Hauptsatz for the intuitionistic theory of iterated inductive def-
initions. In: Fenstad, J.E. (ed.) Proceedings of the Second Scandinavian Logic
Symposium, Studies in Logic and the Foundations of Mathematics, vol. 63, pp.
179–216. Elsevier (1971). https://doi.org/10.1016/S0049-237X(08)70847-4
41. McBride, C.: Let’s see how things unfold: reconciling the inﬁnite with the inten-
sional (extended abstract). In: Kurz, A., Lenisa, M., Tarlecki, A. (eds.) CALCO
2009. LNCS, vol. 5728, pp. 113–126. Springer, Heidelberg (2009). https://doi.org/
10.1007/978-3-642-03741-2 9
42. McDowell, R., Miller, D.: Cut-elimination for a logic with deﬁnitions and induction.
Theor. Comput. Sci. 232(1–2), 91–119 (2000). https://doi.org/10.1016/S0304-
3975(99)00171-1
43. Park, D.M.R.: Finiteness is mu-ineﬀable. Theor. Comput. Sci. 3(2), 173–181
(1976). https://doi.org/10.1016/0304-3975(76)90022-0
44. Ro¸su, G., Lucanu, D.: Circular coinduction: a proof theoretical foundation. In:
Kurz, A., Lenisa, M., Tarlecki, A. (eds.) CALCO 2009. LNCS, vol. 5728, pp. 127–
144. Springer, Heidelberg (2009). https://doi.org/10.1007/978-3-642-03741-2 10
45. Rowe, R.N.S., Brotherston, J.: Automatic cyclic termination proofs for recursive
procedures in separation logic. In: Proceedings of the 6th ACM SIGPLAN Confer-
ence on Certiﬁed Programs and Proofs (CPP 2017), Paris, France, 16–17 January
2017, pp. 53–65 (2017). https://doi.org/10.1145/3018610.3018623
46. Rutten, J.: Universal coalgebra: a theory of systems. Theor. Comput. Sci. 249(1),
3–80 (2000)
47. Rutten, J.: On Streams and Coinduction (2002). https://homepages.cwi.nl/∼janr/
papers/ﬁles-of-papers/CRM.pdf
48. Rutten, J.: The Method of Coalgebra: Exercises in Coinduction. CWI, Amsterdam
(2019)

394
L. Cohen and R. N. S. Rowe
49. Sangiorgi, D., Rutten, J.: Advanced Topics in Bisimulation and Coinduction, 1st
edn. Cambridge University Press, Cambridge (2011)
50. Santocanale, L.: A calculus of circular proofs and its categorical semantics. In:
Nielsen, M., Engberg, U. (eds.) FoSSaCS 2002. LNCS, vol. 2303, pp. 357–371.
Springer, Heidelberg (2002). https://doi.org/10.1007/3-540-45931-6 25
51. Shapiro, S.: Foundations Without Foundationalism : A Case for Second-order
Logic. Clarendon Press, Oxford Logic Guides (1991)
52. Simon, L., Bansal, A., Mallya, A., Gupta, G.: Co-logic programming: extending
logic programming with coinduction. In: Arge, L., Cachin, C., Jurdzi´nski, T., Tar-
lecki, A. (eds.) ICALP 2007. LNCS, vol. 4596, pp. 472–483. Springer, Heidelberg
(2007). https://doi.org/10.1007/978-3-540-73420-8 42
53. Simon, L., Mallya, A., Bansal, A., Gupta, G.: Coinductive logic programming.
In: Etalle, S., Truszczy´nski, M. (eds.) ICLP 2006. LNCS, vol. 4079, pp. 330–345.
Springer, Heidelberg (2006). https://doi.org/10.1007/11799573 25
54. Simpson, A.: Cyclic arithmetic is equivalent to peano arithmetic. In: Esparza, J.,
Murawski, A.S. (eds.) FoSSaCS 2017. LNCS, vol. 10203, pp. 283–300. Springer,
Heidelberg (2017). https://doi.org/10.1007/978-3-662-54458-7 17
55. Sprenger, C., Dam, M.: On the structure of inductive reasoning: circular and tree-
shaped proofs in the μcalculus. In: Gordon, A.D. (ed.) FoSSaCS 2003. LNCS,
vol. 2620, pp. 425–440. Springer, Heidelberg (2003). https://doi.org/10.1007/3-
540-36576-1 27
56. Takeuti, G.: Proof Theory. Dover Books on Mathematics. Dover Publications,
Incorporated, New York (2013)
57. Tellez, G., Brotherston, J.: Automatically verifying temporal properties of pointer
programs with cyclic proof. In: de Moura, L. (ed.) CADE 2017. LNCS (LNAI),
vol. 10395, pp. 491–508. Springer, Cham (2017). https://doi.org/10.1007/978-3-
319-63046-5 30
58. Tiu, A.: A Logical Framework For Reasoning About Logical Speciﬁcations. Ph.D.
thesis, Penn. State University (2004)
59. Tiu, A., Momigliano, A.: Cut elimination for a logic with induction and co-
induction. J. Appl. Log. 10(4), 330–367 (2012). https://doi.org/10.1016/j.jal.2012.
07.007

Logic-Independent Proof Search
in Logical Frameworks
(Short Paper)
Michael Kohlhase1, Florian Rabe1, Claudio Sacerdoti Coen2,
and Jan Frederik Schaefer1(B)
1 Computer Science, FAU Erlangen-N¨urnberg, Erlangen, Germany
jan.frederik.schaefer@fau.de
2 Department of Computer Science and Engineering,
Universit`a di Bologna, Bologna, Italy
Abstract. Logical frameworks like LF allow to specify the syntax and
(natural deduction) inference rules for syntax/proof-checking a wide vari-
ety of logical systems. A crucial feature that is missing for prototyping
logics is a way to specify basic proof automation. We try to alleviate this
problem by generating λProlog (ELPI) inference predicates from logic
speciﬁcations and controlling them by logic-independent helper predi-
cates that encapsulate the prover characteristics. We show the feasibility
of the approach with three experiments: We directly automate ND cal-
culi, we generate tableau theorem provers and model generators.
1
Introduction and Related Work
Logical frameworks like LF [HHP93] and λProlog [Mil] enable prototyping and
analyzing logical systems, using high-level declarative logic deﬁnitions based
on higher-order abstract syntax. Building theorem provers automatically from
declarative logic deﬁnitions has been a long-standing research goal. But cur-
rently, logical framework-induced fully logic-independent proof support is gen-
erally limited to proof checking and simple search.
Competitive proof support, on the other hand, is either highly optimized for
very speciﬁc logics, most importantly propositional logics or untyped ﬁrst-order
logic. Generic approaches have so far been successful for logics without bind-
ing (and thus quantiﬁers) such as the tableaux prover generation in MetTeL2
[TSK]. Logical frameworks shine when applied to logics with binding, for which
specifying syntax and calculus is substantially more diﬃcult. However, while the
Isabelle framework was designed as such a generic prover [Pau93], it is nowadays
primarily used for one speciﬁc logic (Isabelle/HOL). On the other hand, there
has been an explosion of logical systems, often domain-speciﬁc, experimental, or
The authors gratefully acknowledge project support by German Research Council
(DFG) grants KO 2428/13-1 and RA-1872/3-1 OAF as well as EU Horizon 2020 grant
ERI 676541 OpenDreamKit.
c
⃝Springer Nature Switzerland AG 2020
N. Peltier and V. Sofronie-Stokkermans (Eds.): IJCAR 2020, LNAI 12166, pp. 395–401, 2020.
https://doi.org/10.1007/978-3-030-51074-9_22

396
M. Kohlhase et al.
otherwise restricted to small user communities that cannot sustain the develop-
ment of a practical theorem prover. To gain theorem proving support for such
logics, proof obligations can be shipped to existing provers via one of the TPTP
languages, or the logics may be deﬁned as DSLs inside existing provers as is com-
monly done using Coq [Coq15], Isabelle [Pau94], or Leo [Ben+08]. If applicable,
these approaches are very successful. But they are also limited by the language
and proof strategy of the host system, which can preclude fully exploring the
design space for logics and prover.
We investigate this question by combining the advantages of two logi-
cal frameworks: To deﬁne logics, we use the implementation of LF in Mmt
[Rab17,Rab18]. Mmt is optimized for specifying and prototyping logics, provid-
ing in particular type reconstruction, module system, and graphical user inter-
face. Then we generate ELPI theorem provers from these logic deﬁnitions. ELPI
[SCT15] is an extension of λProlog with constraint programming via user-deﬁned
rules, macros, type abbreviations, optional polymorphic typing and more. ELPI is
optimized for fast execution of logical algorithms such as type inference, uniﬁca-
tion, or proof search, and it allows prototyping such systems much more rapidly
than traditional imperative or functional languages. Both Mmt and ELPI were
designed to be ﬂexible and easy to integrate with other systems. Our approach is
logic-independent and applicable to any logic deﬁned in LF. Concretely, we eval-
uate our systems by generating provers for the highly modular suite of logic def-
initions in the LATIN atlas [Cod+11], which includes e.g. ﬁrst- and higher-order
and modal logics and various dependent type theories. These logic deﬁnitions
can be found at [LATIN] and the generated ELPI provers in [GEP].
We follow the approach proposed by Miller et al. in the ProofCert project
[CMR13] but generalize it to non-focused logics. The key idea is to translate each
rule R of the deduction system to an ELPI clause for the provability predicate,
whose premises correspond to the premises of R. The provability predicate has
an additional argument that represents a proof certiﬁcate and each clause has a
new premise that is a predicate, called its helper predicate, that relates the proof
certiﬁcates of the premises to the one of the conclusion. Following [CMR13], the
deﬁnitions of the certiﬁcates and the helper predicates are initially left open, and
by providing diﬀerent instances we can implement diﬀerent theorem provers. In
the simplest case, the additional premise acts as a guard that determines if and
when the theorem prover should use a rule. It can also suggest which formulas to
use during proof search when the rule is not analytic. This allows implementing
strategies such as iterative deepening or backchaining. Alternatively, the helper
predicates can be used to track information in order to return information such
as the found proof. These can be combined modularly with minimal additional
work, e.g., to return the proof term found by a backchaining prover or to run a
second prover on a branch where the ﬁrst one failed.
[CMR13] and later works by the authors use focusing as a preliminary require-
ment. While focusing was the source of inspiration of the whole technique and
brings great beneﬁts by reduction of search space, we show here that focusing
is not really a requirement and that we can achieve comparable reductions in
search space by designing certiﬁcates that impose the two phases of proof search

Logic-Independent Proof Search in Logical Frameworks
397
in focused calculi a posteriori. By not using focusing, our work makes the app-
roach much more accessible as focusing is largely unknown in many communities
(e.g., in linguistics). Moreover, [CMR13] was motivated by applications to one
speciﬁc logic: classical/intuitionist focused ﬁrst-order logic. While it is clear in
theory that the same methodology can be applied to other logics, in practice we
need to build tools to test and benchmark the approach in those logics. Here
the possibility of generating many diﬀerent provers for LF-deﬁned logics in a
completely uniform way is an important novelty of our work.
This paper is a short version of [Koh+20] which contains additional details.
2
Natural Deduction Provers
Logic Deﬁnitions in MMT/LF. While our approach is motivated by and appli-
cable to very complex logics, including e.g. dependent type theories, it is easier
to present our ideas by using a very simple running example. Concretely, we will
use the language features of conjunction and untyped universal quantiﬁcation.
Their formalized syntax is shown below relative to base theories for propositions
and terms.
Props = {prop : type}
Terms = {term : type}
Conj = {include Props, and : prop →prop →prop}
Univ = {include Props, include Terms, univ : (term →prop) →prop}
Below we extend these with the respective natural deduction rules relative
to a theory ND that introduces a judgment to map a proposition F : prop to the
type ded F of proofs of F:
ND = {include Props, judg ded : prop →type}
ConjND = {include {ND, Conj}, andEl : ΠA,B:propded (A ∧B) →ded A, . . .}
For brevity, we only give some of the rules and use the usual notations for
the constants and and univ. Note that judg tags ded as a judgment: while this
is irrelevant for LF type checking, it allows our theorem provers to distinguish
the data from the judgment level. Concretely, type declarations are divided into
data types (such as prop and term) and judgments (such as ded). And term
declarations are divided, by looking at their return type, into data constructors
(such as and and univ) and rules (such as andEl and univE).
Generating ELPI Provers. Our LF-based formalizations of logics deﬁne the well-
formed proofs, but implementations of LF usually do not oﬀer proof search
control that would allow for automation. Therefore, we systematically translate
every LF theory into an ELPI ﬁle. ELPI is similarly expressive as LF so that a
naive approach could simply translate the andEl rule to the ELPI statement
ded A
:−ded (and A B)
Note how the Π-bound variables of the LF rule (which correspond to implicit
arguments that LF implementations reconstruct) simply become free variables

398
M. Kohlhase et al.
for ELPI’s Prolog engine to instantiate. However, this would not yield a use-
ful theorem prover at all — instead, the depth-ﬁrst search behavior of Prolog
would easily lead to divergence. Therefore, to control proof search, we introduce
additional arguments as follows:
– An n-ary judgment like ded becomes a (1+n)-ary predicate in ELPI. The new
argument, called a proof certiﬁcate, can record information about the choice
of rules and arguments to be used during proof search.
– A rule r with n premises (i.e., with n arguments remaining after discarding
the implicit ones) becomes an ELPI statement with 1 + n premises.
The additional premise is a predicate named rhelp, i.e., we introduce one helper
predicate for each rule; it receives all certiﬁcates and formulas of the rule as
input. A typical use for rhelp is to disable or enable r according to the certiﬁcate
provided in the input and, if enabled, extract from this certiﬁcate those for the
recursive calls. An alternative use is to synthesize a certiﬁcate in output from
the output certiﬁcates of the recursive calls, which allows recording a successful
proof search attempt. This is possible because λProlog is based on relations,
and it is not ﬁxed a priori which arguments are to be used as input and which
as output. The two uses can even be combined by providing half-instantiated
certiﬁcates in input that become fully instantiated in output.
For our running example, the following ELPI rule is generated along with a
number of helper predicates:
ded X2 F
:−help/andEl X2 F G X1, ded X1 (and F G).
Iterative Deepening. Iterative deepening is a very simple mechanism to control
the proof search and avoid divergence. Here the certiﬁcate simply contains an
integer indicating the remaining search depth. A top-level loop (not shown here)
just repeats proof search with increasing initial depth. Due to its simplicity, we
can easily generate the necessary helper predicate automatically:
help/andEl (idcert X3) F G (idcert X2)
:−X3 > 0, X2 is X3 −1.
Backchaining. Here, the idea is to be more cautious when trying to use non-
analytic elimination rules like andEl, whose premises contain a sub-formula not
present in the conclusion. To avoid wrongly guessing these, Miller [CMR13]
employs a focused logic where forward and backward search steps are alternated.
We reproduce a similar behavior for our simpler unfocused logic by programming
the helper to trigger forward reasoning search and by automatically generating
forward reasoning clauses for some of our rules:
help/andEl (bccert X3) F G (bccert (bc/fwdLocked X2))
:−bc/val X3 X4, X4 > 0, X2 is X4 −1, bc/fwdable (and F G).
bc/fwdable
:−ded/hyp
T, bc/aux T A.
Here we use two predicates that are deﬁned once and for all, i.e., logic-
independently: bc/fwdable (and F G) asks for a forward reasoning proof of
(and F G); and ded/hyp
T recovers an available hypothesis T.

Logic-Independent Proof Search in Logical Frameworks
399
Finally, bc/aux T A proves A from T using forward reasoning steps. Its
deﬁnition picks up on annotations in LF that mark forward rules, and if andEl
is annotated accordingly, we automatically generate the forward-reasoning clause
below, which says that X5 is provable from (and F G) if it is provable from F:
bc/aux (and F G) X5
:−bc/aux F X5.
3
Tableau Provers
Logic Deﬁnitions in MMT/LF. The formalizations of the tableau rules for our
running example are given below. The general idea is to represent each branch
of a tableau as an LF context; the unary judgments 1 A and 0 A represent the
presence of the signed formula A on the branch, and the judgment ⊥represents
its closability. Thus, the type 0 A →⊥represents that A can be proved. For
example, the rule and0 below states: if 0 (A ∧B) is on a branch, then that
branch can be closed if the two branches extending it with 0 A resp. 0 B can.
Tab = {include Props, judg 1 : prop →type, judg 0 : prop →type,
judg ⊥: type, close : ΠA:prop1 A →0 A →⊥}
ConjTab = {include Tab, include Conj,
and0 : ΠA,B:prop 0 (A ∧B) →(0 A →⊥) →(0 B →⊥) →⊥, . . .}
UnivTab = {. . . , forall1 : ΠP ΠX:term 1(∀P) →(1(PX) →⊥) →⊥}
Generating ELPI Provers. We use the same principle to generate ELPI state-
ments, i.e., every LF-judgment receives an additional argument and every LF-
rule an additional premise.
To generate a tableau prover, we use the additional arguments to track the
current branch. This allows recording how often a rule has been applied in order
to prioritize rule applications. For ﬁrst-order logic, this is only needed to allow
applying the relevant quantiﬁer rules more than once.
For theorem proving, branches that are abandoned when the depth-limit is
reached represent failed proof attempts. But we can just as well use the prover
as a model generator: here we modify the helper predicates in such a way
that abandoning an unclosed branch returns that branch. Thus, the overall run
returns the list of open branches, i.e., the potential models.
Note that the ND theorem prover from Sect. 2 is strong enough to prove the
tableau rules admissible for the logics we experimented with. If this holds up, it
makes prototyping proof support for logic experiments much more convenient.
Application to Natural Language Understanding. Part of the motivation for the
work reported here was to add an inference component — a tableau machine for
natural language pragmatics — to our Grammatical/Logical Framework [KS19],
which combines syntactic processing via the LF-based GF [Ran04] with Mmt
to obtain an NL interpretation pipeline that is parametric in the target logic.
A variant of the model generator discussed above—where we extend the helper

400
M. Kohlhase et al.
predicate to choose a currently preferred branch when a resource bound in satu-
ration is reached—yields an NL understanding framework that combines anaphor
resolution, world-knowledge, and belief revision as in [KK03] with support for
changing and experimenting with the target logic. A demo of the envisioned
system can be found at [GD].
4
Conclusion and Future Work
We have revisited the question of generating theorem provers from declara-
tive logic deﬁnitions in logical frameworks. We believe that, after studying such
frameworks for the last few decades, the community has now understood them
well enough and implemented them maturely enough to have a serious chance
at succeeding. The resulting provers will never be competitive with existing
state-of-the-art provers optimized for a particular logic, but the expressivity and
ﬂexibility of these frameworks allows building practically relevant proof support
for logics that would otherwise have no support at all.
Our infrastructure already scales well to large collections of logics and multi-
ple prover strategies, and we have already used it successfully to rapidly proto-
type a theorem prover in a concrete natural language understanding application.
In the future, we will develop stronger proof strategies, in particular better sup-
port for equational reasoning and advanced type systems. We will also integrate
the ELPI-based theorem provers as a backend for Mmt/LF in order to provide
both automated and interactive proof support directly in the graphical user
interface. A key question will be how the customization of the theorem prover
can be integrated with the logic deﬁnitions (as we already did by annotating
forward rules) without losing the declarative ﬂavor of LF.
References
[Ben+08] Benzm¨uller, C., Paulson, L.C., Theiss, F., Fietzke, A.: LEO-II - A Cooper-
ative Automatic Theorem Prover for Classical Higher-Order Logic (System
Description). In: Armando, A., Baumgartner, P., Dowek, G. (eds.) IJCAR
2008. LNCS (LNAI), vol. 5195, pp. 162–170. Springer, Heidelberg (2008).
https://doi.org/10.1007/978-3-540-71070-7 14
[CMR13] Chihani, Z., Miller, D., Renaud, F.: Checking foundational proof certiﬁcates
for ﬁrst-order logic. In: Blanchette, J., Urban, J. (eds.) Proof Exchange for
Theorem Proving. EasyChair, pp. 58–66 (2013)
[Cod+11] Codescu, M., Horozal, F., Kohlhase, M., Mossakowski, T., Rabe, F.: Project
Abstract: Logic Atlas and Integrator (LATIN). In: Davenport, J.H., Farmer,
W.M., Urban, J., Rabe, F. (eds.) CICM 2011. LNCS (LNAI), vol. 6824, pp.
289–291. Springer, Heidelberg (2011). https://doi.org/10.1007/978-3-642-
22673-1 24
[Coq15] Coq Development Team: The Coq proof assistant: reference manual. Tech-
nical report INRIA (2015)
[GD] GLIF Demo. https://gl.kwarc.info/COMMA/glif-demo-ijcar-2020. Acces-
sed 27 Jan 2020

Logic-Independent Proof Search in Logical Frameworks
401
[GEP] Generated ELPI Provers. https://gl.mathhub.info/MMT/LATIN2/tree/
devel/elpi. Accessed 26 Jan 2020
[HHP93] Harper, R., Honsell, F., Plotkin, G.: A framework for deﬁning logics. J.
Assoc. Comput. Mach. 40(1), 143–184 (1993)
[KK03] Kohlhase, M., Koller, A.: Resource-adaptive model generation as a per-
formance model. Log. J. IGPL 11(4), 435–456 (2003). http://jigpal.
oxfordjournals.org/cgi/content/abstract/11/4/435
[Koh+20] Kohlhase, M., et al.: Logic-independent proof search in logical frame-
works (extended report). Extended Report of Conference Submission (2020).
https://kwarc.info/kohlhase/submit/mmtelpi.pdf
[KS19] Kohlhase, M., Schaefer, J.F.: GF + MMT = GLF - from language to seman-
tics through LF. In: Miller, D., Scagnetto, I., (eds.) Proceedings of the
Fourteenth Workshop on Logical Frameworks and Meta-Languages: Theory
and Practice (LFMTP 2019). Electronic Proceedings in Theoretical Com-
puter Science (EPTCS), vol. 307, pp. 24–39 (2019). https://doi.org/10.4204/
EPTCS.307.4
[LATIN] LATIN2 - Logic Atlas Version 2. https://gl.mathhub.info/MMT/LATIN2.
Accessed 02 June 2017
[Mil] Miller,
D:
λProlog.
http://www.lix.polytechnique.fr/Labo/Dale.Miller/
lProlog/
[Pau93] Paulson, L.C.: Isabelle: The Next 700 Theorem Provers. In: arXiv CoRR
cs.LO/9301106 (1993). https://arxiv.org/abs/cs/9301106
[Pau94] Paulson, L.C. (ed.): Isabelle: A Generic Theorem Prover. LNCS, vol. 828.
Springer, Heidelberg (1994). https://doi.org/10.1007/BFb0030541
[Rab17] Rabe, F.: How to identify, translate, and combine logics? J. Log. Comput.
27(6), 1753–1798 (2017)
[Rab18] Rabe, F.: A modular type reconstruction algorithm. ACM Trans. Comput.
Log. 19(4), 1–43 (2018)
[Ran04] Ranta, A.: Grammatical framework, a type-theoretical grammar formalism.
J. Funct. Programm. 14(2), 145–189 (2004)
[SCT15] Coen, C.S., Tassi, E.: The ELPI system (2015). https://github.com/LPC
IC/elpi
[TSK] Tishkovsky, D., Schmidt, R.A., Khodadadi, M.: MetTeL2: towards a tableau
prover generation platform. In: Proceedings of the Third Workshop on Prac-
tical As- pects of Automated Reasoning (PAAR-2012), p. 149 (2012)

Layered Clause Selection for Theory
Reasoning
(Short Paper)
Bernhard Gleiss1(B) and Martin Suda2
1 TU Wien, Vienna, Austria
bgleiss@forsyte.at
2 Czech Technical University in Prague, Prague, Czech Republic
Abstract. Explicit theory axioms are added by a saturation-based the-
orem prover as one of the techniques for supporting theory reasoning.
While simple and eﬀective, adding theory axioms can also pollute the
search space with many irrelevant consequences. As a result, the prover
often gets lost in parts of the search space where the chance to ﬁnd a
proof is low. In this paper, we describe a new strategy for controlling the
amount of reasoning with explicit theory axioms. The strategy reﬁnes a
recently proposed two-layer-queue clause selection and combines it with
a heuristic measure of the amount of theory reasoning in the derivation
of a clause. We implemented the new strategy in the automatic theorem
prover Vampire and present an evaluation showing that our work dra-
matically improves the state-of-the-art clause-selection strategy in the
presence of theory axioms.
1
Introduction
Thanks to recent advances, saturation-based theorem provers are increasingly
used to reason about problems requiring quantiﬁed theory-reasoning [4,6]. One
of the standard techniques to enable such reasoning is to automatically add
ﬁrst-order axiomatisations of theories detected in the input [14,18]. For exam-
ple, (incomplete) axiomatisations of integer and real arithmetic or McCarthy’s
axioms of the theory of arrays [15] are routinely used. While this simple tech-
nique is often eﬀective, we observed (see also [21]) two problems inherent to
the solution: First, explicit axioms blow up the search space in the sense that a
huge amount of consequences can additionally be generated. This happens since
theory axioms are often repeatedly combined with certain clauses or among
themselves, eﬀectively creating cyclic patterns in the derivation. Most of these
consequences would immediately be classiﬁed as practically useless by humans.
Second, many of the resulting consequences have small weight. This has the
unfortunate eﬀect that the age-weight clause selection heuristic [16], predomi-
nantly used by saturation-based theorem provers for guiding the exploration of
the search-space, often selects these theory-focused consequences. This way the
prover is getting lost in parts of the search space where the chance of ﬁnding a
proof is low.
c
⃝Springer Nature Switzerland AG 2020
N. Peltier and V. Sofronie-Stokkermans (Eds.): IJCAR 2020, LNAI 12166, pp. 402–409, 2020.
https://doi.org/10.1007/978-3-030-51074-9_23

Layered Clause Selection for Theory Reasoning
403
In this paper, we propose to limit the exploration of theory-focused conse-
quences by extending clause selection to take into account the amount of the-
ory reasoning in the derivation of a clause. Our solution consists of two parts.
First, we propose an eﬃciently computable feature of clauses, which we call th-
distance, that measures the amount of theory reasoning in the derivation of a
clause (Sect. 3). Second, we turn to the general problem of incorporating a feature
to a clause selection strategy. There has been an ongoing interest in this prob-
lem [24,25,28]. We take inspiration from the layered clause selection approach
presented in [28] and introduce the reﬁned notion of multi-split queues, which
present a principled solution to the incorporation problem (Sect. 2). We ﬁnally
obtain a clause selection strategy for theory reasoning by instantiating multi-
split queues with the feature th-distance. We implemented the resulting clause
selection in the state-of-the-art saturation-based theorem prover Vampire [14],
and evaluate its beneﬁts on a relevant subset of the smt-lib benchmark (Sect. 4).
Related Work. There are diﬀerent approaches to adding support for theory
reasoning to saturation-based theorem provers, either by extending the prover’s
inference system with dedicated inference rules [2,10,12,13] or using even more
fundamental design changes [1,7,20,22]. While such solutions can result in very
eﬃcient reasoning procedures, their development is incredibly challenging and
their implementation is a huge eﬀort. As a result, only a few theories are covered
by such approaches, in contrast to our technique, which applies to arbitrary the-
ories. In particular, our technique can be used by non-experts on custom theory-
domains coming from applications for which no dedicated solution exists. Our
work has similar motivation to [21], where the authors use the set-of-support
strategy [30] to limit the amount of reasoning performed with pure theory conse-
quences. However, unlike our technique, they do not impose any limit on clauses
whose derivation contains at least one non-theory-axiom.
Contributions. The summarized contributions of this paper are:
– A new approach for building clause selection strategies from clause features,
based on multi-split queues.
– A new clause selection strategy for theory reasoning based on the instantiation
of multi-split queues with the th-distance-feature measuring the amount of
theory reasoning in the derivation of a clause. Our solution applies to arbitrary
theories and does not require fundamental changes to the implementation of
clause selection.
– An implementation of the introduced clause selection strategy in the state-
of-the-art theorem prover Vampire.
– An experimental evaluation conﬁrming the eﬀectiveness of the technique, by
improving on the existing heuristics by up to 37 % on a relevant set of bench-
marks.

404
B. Gleiss and M. Suda
2
Layered Clause Selection
We assume the reader to be familiar with the saturation-based theorem proving
technology (see, e.g. [3,17]) and, in particular, with clause selection, the pro-
cedure for deciding, at each iteration of a saturation algorithm, which of the
currently passive clauses to next select for activation, i.e. for participation in
inferences with the previously activated clauses. To agree on terminology, we
start this section by recalling clause selection by age and weight. We then move
on to explaining layered clause selection.
The two most important features of a clause for clause selection are 1) its
age, typically implemented using an ever-increasing “date of birth” timestamp,
and 2) weight, which refers to the number of symbols occurring in the clause. A
theorem prover prefers to select clauses that are old, which implicitly corresponds
to a breadth-ﬁrst search strategy, and clauses that are light, which is a form of
best-ﬁrst search (clauses with few symbols are cheaper to process, tend to be
stronger simpliﬁers, and are intuitively closer to the ultimate target, the empty
clause). In practice, the best performance is achieved by combining these two
criteria [16,25]. This is achieved by storing the passive clauses in two queues,
one sorted by age and the other by weight, and setting a ratio to specify how
the selection alternates between picking from these two queues.
Layered Selection. In the system description of GKC [28], Tammet describes
an idea of using two layers of queues to organise clause selection. The ﬁrst layer
relies on the just-described combination of selection by age and weight. In the
second layer, clauses are split into disjoint groups using a certain property (e.g.,
“being derived from the goal or not” could deﬁne two groups), each group is rep-
resented by two sub-queues of the ﬁrst layer, and the decision from which group
to select the next clause is dictated by a new second-layer ratio. Although Tam-
met does not expand much on the insights behind using the layered approach,
he reports it highly beneﬁcial for the performance of GKC. In our understand-
ing, the additional layer (in principle, there could be more than two) provides a
clean way of incorporating into clause selection a new notion of what a preferred
clause should be, without a priori disturbing the already established and tuned
primary approach, such as selection by age and weight.1
Our preliminary experiments with the idea (instantiated with the derived-
from-the-goal property) found it useful, but not as powerful as other goal-
directed heuristics in Vampire. In particular, ﬁnding a universally good ratio
between the “good” clauses and the “bad” ones seemed hard. What we propose
here instead (and what also led in our experiment to a greater performance gain)
is to instead organise the clauses into groups with “good” ones and “all”. Here
the second group contains all the passive clauses and essentially represents a
fallback to the original single-layer strategy. The advantage of this new take on
layered selection is that a bad clause is only selected if 1) it is time to try a bad
1 A known alternative [25] is to adapt the formula for computing weight to include a
term for penalising bad clauses and still rely on selection by age and this new reﬁned
notion of weight. (See also the non-goal weight coeﬃcient in [27].).

Layered Clause Selection for Theory Reasoning
405
clause according to the second-layer ratio and 2) the best bad clause is also the
current overall best according to the age-weight perspective. This makes picking
a good second-layer ratio much easier. In particular, one can “smoothly” move
(by changing the second-layer ratio) from a high preference for the “all” second-
layer queue towards selecting more “good” clauses without necessarily having to
select any “bad” ones.
Multi-split Queues. We propose multi-split queues to realize layered selection
with second layer groups deﬁned by a real-valued clause feature.
Deﬁnition 1. Let μ be a real-valued clause evaluation feature such that prefer-
able clauses have low value of μ(C). Let the cutoﬀs c1, . . . , ck be monotonically
increasing real numbers with ck = ∞, and let the ratio r1 : . . . : rk be a list of
positive integer values. These together determine a layered selection scheme with
k groups Ci = {C|μ(C) ≤ci} for i = 1, . . . , k, such that we select from the i-th
group with a frequency ri/(Σk
j=1rj).
It is easy to see that multi-split queues generalise the binary “good” vs “all”
arrangement, since, thanks to monotonicity of the cutoﬀs, we have Ci ⊆Ci+1.
Moreover, since ck = ∞, Ck will contain all the passive clauses.
3
Theory Part
In this section, we instantiate the idea of multi-split queues from Sect. 2 with a
concrete clause evaluation feature, which measures the amount of theory reason-
ing in the derivation of a clause. We assume that the initial clauses given to the
saturation algorithm, which we simply refer to as axioms, consists of non-theory
axioms obtained by classifying the input problem and theory axioms added to
facilitate theory reasoning.
We start by deﬁning the fraction of theory reasoning in the derivation of a
general clause. This relies on counting the number of theory axioms, resp. the
number of all axioms, in the derivation-tree using running sums.
Deﬁnition 2. For a theory axiom C, deﬁne both thAx(C) and allAx(C) as
1. For a non-theory axiom C, deﬁne thAx(C) as 0 and allAx(C) as 1. For a
derived clause C with parent clauses C1, . . . , Cn, deﬁne thAx(C) as 
i thAx(Ci)
and allAx(C) as 
i allAx(Ci). Finally, we set frac(C) := thAx(C)/allAx(C).
Assume now that for a given problem we expect (based on domain knowledge
and experience) the fraction of theory reasoning in the ﬁnal refutation frac(⊥) to
be at most 1/d, for a positive integer d. Our clause evaluation feature th-distance
measures how much frac(C) exceeds the expected “maximally allowed” fraction
1/d. More precisely, th-distance counts the number of non-theory axioms which
the derivation of C would additionally need to contain to achieve a ratio 1/d.
Deﬁnition 3. The th-distance : Clauses →N is deﬁned as
th −distance(C) := max(thAx(C) · d −allAx(C), 0).

406
B. Gleiss and M. Suda
Our heuristic is based on the idea that a clause with small th-distance is
more likely to contribute to the refutation than a clause with high th-distance.
We therefore want to ensure that clause selection focuses on selecting clauses C
with a low value th-distance(C). We realize this with the multi-split queues (see
Sect. 2), instantiating the clause evaluation feature μ by th-distance, resulting
in a second layer clause selection strategy with parameters d, c1, . . . , ck and
r1 : . . . : rk.
4
Experiments
We implemented the heuristic described in Sect. 3 in Vampire (version 4.4).
Our newly added implementation consists of about 900 lines of C++ code and
is compatible with both the LRS saturation algorithm [23] and Avatar [29].
For evaluation, we used the following subset of the most recent version (as of
January 2020) of SMTLIB [5]: We took all the problems from the sub-logics that
contain quantiﬁcation and theories, such as LIA, LRA, NRA, ALIA, UFDT, . . .
except for those requiring bit-vector (BV) or ﬂoating-point (FP) reasoning, cur-
rently not supported by Vampire. Subsequently, we excluded problems known
to be satisﬁable and those that were provable using Vampire’s default strategy
in 10 s either without adding theory axioms or while performing clause selection
by age only. This way, we obtained 20 795 problems.2
Table 1. Comparing clause selection strategies on Vampire’s default conﬁguration.
Strategy
d-value Cutoﬀs
Ratio
Refuted Δbase Δbase%
default
–
–
–
886
0
0.0
layered2 10
23, ∞
33:8
1112
226
25.5
layered3
7
0, 30, ∞
16:8:1
1170
284
32.1
layered4
8
16, 41, 59, ∞84:9:2:2 1176
290
32.7
As a ﬁrst experiment, we compared the number of problems solved in 10 s
by the default strategy3 and its various extensions by multi-split queues deﬁned
in Sect. 3.4 The d-value, cutoﬀs and ratio values for the heuristic were selected
by educated guessing and randomised hill-climbing. Table 1 lists results of the
best obtained conﬁgurations. It can be seen that already with two second layer
queues a substantial improvement of 25.5% over the default is achieved. More-
over, while it is increasingly more diﬃcult to choose good values for the many
parameters deﬁning a conﬁguration with multiple queues, their use further sig-
niﬁcantly improves the number of problems solved.
2 A list of the selected problems along with other information needed to reproduce
our experiments can be found at https://git.io/JvqhP.
3 The default strategy uses Avatar [29], the LRS saturation algorithm [23] and an
age-weight ratio of 1:1.
4 The experiment was run on our local server with Intel Xeon 2.3 GHz processors.

Layered Clause Selection for Theory Reasoning
407
Table 2. Comparing clause selection strategies on Vampire’s portfolio conﬁguration.
Strategy
d-value Cutoﬀs
Ratio
Refuted Uniques
SMTCOMP2019
–
–
–
5479
194
SMTCOMP2019+layered4 8
16, 41, 59, ∞84:9:2:2 5629
344
In a second experiment,5 we ran Vampire’s strategy schedule for SMTCOMP
2019 [11] on our problems and also the same schedule additionally imposing the
most successful second-layer clause selection scheme layered4 from the ﬁrst
experiment. The time limit was 500 s per problem. Table 2 shows the results.
We can see that the version with second-layer queues improved over the
standard schedule by 150 solved problems. This is a very signiﬁcant result, sug-
gesting the achieved control of theory reasoning is incredibly helpful. Moreover,
one should keep in mind that strategies in a schedule are carefully selected to
complement each other and even locally good changes in the strategies often
destroy this complementarity (cf., e.g., [19,21]). In our case, however, we achieve
an improvement despite this looming negative eﬀect. Finally, it is very likely
that a new schedule, constructed while taking our new technique into account,
will be able to additionally cover some of the 194 problems currently only solved
by the unaltered schedule.
5
Conclusion
We introduced a new clause selection heuristic for reasoning in the presence of
explicit theory axioms. The heuristic is based on the combination of multi-split
queues and a new clause-feature measuring the amount of theory reasoning in the
derivation of a clause. Our experiments show that the new heuristic signiﬁcantly
improves the existing state-of-the-art clause selection strategy. As future work,
we want to extend layered clause selection with new clause-features and combine
it with the machine-learning-based approach in the style of ENIGMA [8].
References
1. Althaus, E., Kruglov, E., Weidenbach, C.: Superposition modulo linear arithmetic
SUP(LA). In: Ghilardi, S., Sebastiani, R. (eds.) FroCoS 2009. LNCS (LNAI), vol.
5749, pp. 84–99. Springer, Heidelberg (2009). https://doi.org/10.1007/978-3-642-
04222-5 5
2. Bachmair, L., Ganzinger, H.: Ordered chaining calculi for ﬁrst-order theories of
transitive relations. J. ACM 45(6), 1007–1049 (1998)
3. Bachmair, L., Ganzinger, H., McAllester, D.A., Lynch, C.: Resolution theorem
proving. In: Robinson, J.A., Voronkov, A. (eds.) Handbook of Automated Reason-
ing, vol. 2, pp. 19–99. Elsevier and MIT Press, Cambridge (2001)
5 The second experiment was run on the StarExec cluster [26] with 2.4 GHz processors.

408
B. Gleiss and M. Suda
4. Backes, J., et al.: Reachability analysis for AWS-based networks. In: Dillig, I.,
Tasiran, S. (eds.) CAV 2019. LNCS, vol. 11562, pp. 231–241. Springer, Cham
(2019). https://doi.org/10.1007/978-3-030-25543-5 14
5. Barrett, C., Fontaine, P., Tinelli, C.: The Satisﬁability Modulo Theories Library
(SMT-LIB) (2016). www.SMT-LIB.org
6. Barthe, G., Eilers, R., Georgiou, P., Gleiss, B., Kovcs, L., Maﬀei, M.: Verifying
relational properties using trace logic. In: 2019 Formal Methods in Computer Aided
Design (FMCAD), pp. 170–178, October 2019
7. Baumgartner, P., Waldmann, U.: Hierarchic superposition with weak abstraction.
In: Bonacina, M.P. (ed.) CADE 2013. LNCS (LNAI), vol. 7898, pp. 39–57. Springer,
Heidelberg (2013). https://doi.org/10.1007/978-3-642-38574-2 3
8. Chvalovsk´y, K., Jakubuv, J., Suda, M., Urban, J.: ENIGMA-NG: eﬃcient neural
and gradient-boosted inference guidance for E. In: Fontaine [9], pp. 197–215
9. Fontaine, P. (ed.): CADE 2019. LNCS (LNAI), vol. 11716. Springer, Cham (2019).
https://doi.org/10.1007/978-3-030-29436-6
10. Gupta, A., Kov´acs, L., Kragl, B., Voronkov, A.: Extensional crisis and proving
identity. In: Cassez, F., Raskin, J.-F. (eds.) ATVA 2014. LNCS, vol. 8837, pp.
185–200. Springer, Cham (2014). https://doi.org/10.1007/978-3-319-11936-6 14
11. Hadarean, L., Hyvarinen, A., Niemetz, A., Reger, G.: 14th International Satisﬁabil-
ity Modulo Theories Competition (SMT-COMP 2019) (2019). https://smt-comp.
github.io/2019/
12. Kotelnikov, E., Kov´acs, L., Reger, G., Voronkov, A.: The vampire and the FOOL.
In: Avigad, J., Chlipala, A. (eds.) Proceedings of the 5th ACM SIGPLAN Confer-
ence on Certiﬁed Programs and Proofs, Saint Petersburg, FL, USA, 20–22 January
2016, pp. 37–48. ACM (2016)
13. Kov´acs, L., Robillard, S., Voronkov, A.: Coming to terms with quantiﬁed reasoning.
In: Proceedings of the 44th ACM SIGPLAN Symposium on Principles of Program-
ming Languages. POPL 2017, New York, NY, USA, pp. 260–270. Association for
Computing Machinery (2017)
14. Kov´acs, L., Voronkov, A.: First-order theorem proving and Vampire. In: Shary-
gina, N., Veith, H. (eds.) CAV 2013. LNCS, vol. 8044, pp. 1–35. Springer, Heidel-
berg (2013). https://doi.org/10.1007/978-3-642-39799-8 1
15. Mccarthy, J.: Towards a mathematical science of computation. In: IFIP Congress,
pp. 21–28. North-Holland (1962)
16. McCune, W.: Otter 3.0 reference manual and guide. Technical report ANL-94/6,
Argonne National Laboratory (1994)
17. Overbeek, R.A.: A new class of automated theorem-proving algorithms. J. ACM
21(2), 191–200 (1974)
18. Prevosto, V., Waldmann, U.: SPASS+T. In: Sutcliﬀe, G., Schmidt, R., Schulz, S.
(eds.) Proceedings of the FLoC 2006 Workshop on Empirically Successful Com-
puterized Reasoning, 3rd International Joint Conference on Automated Reasoning,
number 192 in CEUR Workshop Proceedings, pp. 19–33 (2006)
19. Rawson, M., Reger, G.: Old or heavy? Decaying gracefully with age/weight shapes.
In: Fontaine [9], pp. 462–476
20. Reger, G., Bjorner, N., Suda, M., Voronkov, A.: AVATAR modulo theories. In:
Benzm¨uller, C., Sutcliﬀe, G., Rojas, R. (eds.) GCAI 2016. 2nd Global Conference
on Artiﬁcial Intelligence, EPiC Series in Computing, vol. 41, pp. 39–52. EasyChair
(2016)

Layered Clause Selection for Theory Reasoning
409
21. Reger, G., Suda, M.: Set of support for theory reasoning. In: Eiter, T., Sands,
D., Sutcliﬀe, G., Voronkov, A. (eds.) IWIL@LPAR 2017 Workshop and LPAR-21
Short Presentations, Maun, Botswana, 7–12 May 2017, vol. 1. Kalpa Publications
in Computing. EasyChair (2017)
22. Reger, G., Suda, M., Voronkov, A.: Uniﬁcation with abstraction and theory instan-
tiation in saturation-based reasoning. In: Beyer, D., Huisman, M. (eds.) TACAS
2018. LNCS, vol. 10805, pp. 3–22. Springer, Cham (2018). https://doi.org/10.1007/
978-3-319-89960-2 1
23. Riazanov, A., Voronkov, A.: Limited resource strategy in resolution theorem prov-
ing. J. Symb. Comput. 36(1–2), 101–115 (2003)
24. Schulz, S., Cruanes, S., Vukmirovic, P.: Faster, higher, stronger: E 2.3. In Fontaine
[9], pp. 495–507
25. Schulz, S., M¨ohrmann, M.: Performance of clause selection heuristics for saturation-
based theorem proving. In: Olivetti, N., Tiwari, A. (eds.) IJCAR 2016. LNCS
(LNAI), vol. 9706, pp. 330–345. Springer, Cham (2016). https://doi.org/10.1007/
978-3-319-40229-1 23
26. Stump, A., Sutcliﬀe, G., Tinelli, C.: StarExec, a cross community logic solving
service (2012). https://www.starexec.org
27. Suda, M.: Aiming for the goal with SInE. In: Kovacs, L., Voronkov, A. (eds.)
Vampire 2018 and Vampire 2019. The 5th and 6th Vampire Workshops. EPiC
Series in Computing, vol. 71, pp. 38–44. EasyChair (2020)
28. Tammet, T.: GKC: a reasoning system for large knowledge bases. In: Fontaine [9],
pp. 538–549
29. Voronkov, A.: AVATAR: the architecture for ﬁrst-order theorem provers. In: Biere,
A., Bloem, R. (eds.) CAV 2014. LNCS, vol. 8559, pp. 696–710. Springer, Cham
(2014). https://doi.org/10.1007/978-3-319-08867-9 46
30. Wos, L., Robinson, G.A., Carson, D.F.: Eﬃciency and completeness of the set of
support strategy in theorem proving. J. ACM 12(4), 536–541 (1965)

Non Classical Logics

Description Logics with Concrete
Domains and General Concept Inclusions
Revisited
Franz Baader(B) and Jakub Rydval
Institute of Theoretical Computer Science, TU Dresden, Dresden, Germany
{franz.baader,jakub.rydval}@tu-dresden.de
Abstract. Concrete domains have been introduced in the area of
Description Logic to enable reference to concrete objects (such as num-
bers) and predeﬁned predicates on these objects (such as numerical
comparisons) when deﬁning concepts. Unfortunately, in the presence of
general concept inclusions (GCIs), which are supported by all modern
DL systems, adding concrete domains may easily lead to undecidability.
One contribution of this paper is to strengthen the existing undecidabil-
ity results further by showing that concrete domains even weaker than
the ones considered in the previous proofs may cause undecidability. To
regain decidability in the presence of GCIs, quite strong restrictions, in
sum called ω-admissibility, need to be imposed on the concrete domain.
On the one hand, we generalize the notion of ω-admissibility from con-
crete domains with only binary predicates to concrete domains with pred-
icates of arbitrary arity. On the other hand, we relate ω-admissibility
to well-known notions from model theory. In particular, we show that
ﬁnitely bounded, homogeneous structures yield ω-admissible concrete
domains. This allows us to show ω-admissibility of concrete domains
using existing results from model theory.
Keywords: Description logic · Concrete domains · GCIs ·
ω-admissibility · Homogeneity · Finite boundedness · Decidability ·
Constraint satisfaction
1
Introduction
Description Logics (DLs) [3,7] are a well-investigated family of logic-based
knowledge representation languages, which are frequently used to formalize
ontologies for application domains such as the Semantic Web [27] or biology and
medicine [26]. To deﬁne the important notions of such an application domain
as formal concepts, DLs state necessary and suﬃcient conditions for an individ-
ual to belong to a concept. These conditions can be Boolean combinations of
atomic properties required for the individual (expressed by concept names) or
Supported by DFG GRK 1763 (QuantLA) and TRR 248 (cpec, grant 389792660).
c
⃝Springer Nature Switzerland AG 2020
N. Peltier and V. Sofronie-Stokkermans (Eds.): IJCAR 2020, LNAI 12166, pp. 413–431, 2020.
https://doi.org/10.1007/978-3-030-51074-9_24

414
F. Baader and J. Rydval
properties that refer to relationships with other individuals and their properties
(expressed as role restrictions). For example, the concept of a father that has
only daughters can be formalized by the concept description
C := ¬Female ⊓∃child.Human ⊓∀child.Female,
which uses the concept names Female and Human and the role name child as well
as the concept constructors negation (¬), conjunction (⊓), existential restriction
(∃r.D), and value restriction (∀r.D). The GCIs
Human ⊑∀child.Human and ∃child.Human ⊑Human
say that humans have only human children, and they are the only ones that can
have human children.
DL systems provide their users with reasoning services that allow them to
derive implicit knowledge from the explicitly represented one. In our example,
the above GCIs imply that elements of our concept C also belong to the con-
cept D := Human ⊓∀child.Human, i.e., C is subsumed by D w.r.t. these GCIs.
A speciﬁc DL is determined by which kind of concept constructors are avail-
able. A major goal of DL research was and still is to ﬁnd a good compromise
between expressiveness and the complexity of reasoning, i.e., to locate DLs that
are expressive enough for interesting applications, but still have inference prob-
lems (like subsumption) that are decidable and preferably of a low complexity.
For the DL ALC, in which all the concept descriptions used in the above example
can be expressed, the subsumption problem w.r.t. GCIs is ExpTime-complete [7].
Classical DLs like ALC cannot refer to concrete objects and predeﬁned rela-
tions over these objects when deﬁning concepts. For example, a constraint stating
that parents are strictly older than their children cannot be expressed in ALC.
To overcome this deﬁcit, a scheme for integrating certain well-behaved concrete
domains, called admissible, into ALC was introduced in [4], and it was shown
that this integration leaves the relevant inference problems (such as subsump-
tion) decidable. Basically, admissibility requires that the set of predicates of the
concrete domain is closed under negation and that the constraint satisfaction
problem (CSP) for the concrete domain is decidable. However, in this setting,
GCIs were not considered since they were not a standard feature of DLs then,1
though a combination of concrete domains and GCIs would be useful in many
applications. For example, using the syntax employed in [33] and also in the
present paper, the above constraint regarding the age of parents and their chil-
dren could be expressed by the GCI Human ⊓∃child age, age.[>] ⊑⊥, which
says that there cannot be a human whose age is smaller than the age of one of
his or her children. Here ⊥is the bottom concept, which is always interpreted
as the empty set, age is a concrete feature that maps from the abstract domain
populating concepts into the concrete domain of natural numbers, and > is the
usual greater predicate on the natural numbers.
1 Actually, they were introduced (with a diﬀerent name) at about the same time as
concrete domains [2,38].

DLs with Concrete Domains and GCIs
415
A ﬁrst indication that concrete domains might be harmful for decidability
was given in [6], where it was shown that adding transitive closure of roles to
ALC(R), the extension of ALC by an admissible concrete domain R based on
real arithmetics, renders the subsumption problem undecidable. The proof of
this result uses a reduction from the Post Correspondence Problem (PCP). It
was shown in [31] that this proof can be adapted to the case where transitive
closure of roles is replaced by GCIs, and it actually works for considerably weaker
concrete domain, such as the rational numbers Q or the natural numbers N with
a unary predicate =0 for equality with zero, a binary equality predicate =, and a
unary predicate +1 for incrementation. In [7] it is shown, by a reduction from the
halting problem of two-register machines, that undecidability even holds without
binary equality. In the present paper, we will improve on this result by showing
that, even if =0 is removed as well, undecidability still holds, and that the same
is true if we replace +1 with +.
To regain decidability, one can either impose syntactic restriction on how
the DL can interact with the concrete domain [22,36]. The main idea is here to
disallow paths (such as child age in our example), which has the eﬀect that con-
crete domain predicates cannot compare properties (such as the age) of diﬀerent
individuals. The other option is to impose stronger restrictions than admissi-
bility on the concrete domain. After ﬁrst positive results for speciﬁc concrete
domains (e.g., a concrete domain over the rational numbers with order and
equality [30,32]), the notion of ω-admissible concrete domains was introduced in
[33], and it was shown (by designing a tableau-based decision procedure) that
integrating such a concrete domain into ALC leaves reasoning decidable also
in the presence of GCIs. In the present paper, we generalize the notion of ω-
admissibility and the decidability result from concrete domains with only binary
predicates as in [33] to concrete domains with predicates of arbitrary arity. But
the main contribution of this paper is to show that there is a close relationship
between ω-admissibility and well-known notions from model theory. In particu-
lar, we show that ﬁnitely bounded, homogeneous structures yield ω-admissible
concrete domains. This allows us to locate new ω-admissible concrete domains
using existing results from model theory. Due to space constraints, we cannot
prove all our results in detail here. Complete proofs and more examples of ω-
admissible concrete domains can be found in [8].
2
Preliminaries
We write [n] for the set {1, . . . , n}. Given a set A, the diagonal on A is deﬁned as
the binary relation △A := {(a, a) | a ∈A}. The kernel of a mapping f : A →B,
denoted by ker f, is the equivalence relation {(a, a′) ∈A × A | f(a) = f(a′)}.
From a mathematical point of view, concrete domains are relational struc-
tures. A (relational) signature τ is a set of predicate symbols, each with an asso-
ciated natural number called its arity. A relational τ-structure A consists of
a set A (the domain) together with relations RA ⊆Ak for each k-ary symbol
R ∈τ. We often describe structures by listing their domain and relations, e.g.,

416
F. Baader and J. Rydval
we write Q = (Q; <) for the relational structure whose domain is the set of
rational numbers Q, and which has the usual smaller relation < on Q as its only
relation.2 An expansion of the τ-structure A is a σ-structure B with A = B,
τ ⊆σ, and RB = RA for each relation symbol R ∈τ. Conversely, we call A a
reduct of B.
One possibility to obtain an expansion of a τ-structure is to use formulas
of ﬁrst-order logic (FO) over the signature τ to deﬁne new predicates, where a
formula with k free variables deﬁnes a k-ary predicate in the obvious way. We
assume that equality = as well as the symbol false for falsity is always avail-
able. Thus, atomic formulas are of the form false, xi = xj, and R(x1, . . . , xk)
for some k-ary R ∈τ and variables x1, . . . , xk. The FO theory of a structure
is the set of all FO sentences that are true in the structure. In addition to full
FO, we also use standard fragments of FO such as the existential positive (∃+),
the quantiﬁer-free (qf), and the primitive positive (pp) fragment. The existential
positive fragment consists of formulas built using conjunction, disjunction, and
existential quantiﬁcation only. The quantiﬁer-free fragment consists of Boolean
combinations of atomic formulas, and the primitive positive fragment of existen-
tially quantiﬁed conjunctions of atomic formulas. Let Σ be a set of FO formulas
and D a structure. We say that a relation over D has a Σ deﬁnition in D if it
is of the form {t ∈Dk | D |= φ(t)} for some φ ∈Σ. We refer to this relation by
φD. For example, the formula y < x ∨x = y is an existential positive formula
and, interpreted in the structure Q, it clearly deﬁnes the binary relation ≥on
Q. This shows that ≥is ∃+ deﬁnable in Q. An example of a pp formula is the
formula ∃y. x = y, which deﬁnes the unary relation interpreted as the whole
domain Q.
A homomorphism h: A →B for τ-structures A, B is a mapping h: A →
B that preserves each relation of A, i.e., (a1, . . . , ak) ∈RA for some k-ary
relation symbol R ∈τ implies (h(a1), . . . , h(ak)) ∈RB. We write A →B if
A homomorphically maps to B and A ̸→B otherwise. We say that A and B
are homomorphically equivalent if A →B and B →A. An endomorphism is a
homomorphism from a structure to itself. By an embedding we mean an injective
homomorphism that additionally satisﬁes the only if direction in the deﬁnition
of a homomorphism, i.e., it also preserves the complements of relations. We
write A →B if A embeds into B. A substructure of A is a structure B over
B ⊆A such that the natural inclusion map i: A →B is an embedding. We
call A an extension of B. An isomorphism is a surjective embedding. We say
that two structures A and B are isomorphic and write A ∼= B if there exists an
isomorphism from A to B. An automorphism is an isomorphism from a structure
into itself.
The deﬁnition of admissibility of a concrete domain in [4] requires that the
constraint satisfaction problem for this structure is decidable. Let D be a struc-
ture with a ﬁnite relational signature τ. The constraint satisfaction problem of
D, short CSP(D), is the following decision problem:
2 By a slight abuse of notation, we use < instead of <Q to denote also the interpreta-
tion of the predicate symbol < in Q.

DLs with Concrete Domains and GCIs
417
INPUT: A ﬁnite τ-structure A.
QUESTION: Does A homomorphically map to D?
Formally, CSP(D) is the class of all ﬁnite τ-structures that homomorphically
map to D. We call D the template of CSP(D). A solution for an instance A of
the CSP is a homomorphism h: A →D.
It is easy to see that deciding whether a CSP instance admits a solution
amounts to evaluating a pp sentences in the template and vice versa [10]. For
example, verifying whether the structure A = ({a1, a2, a3}; <A) with <A :=
{(a1, a2), (a2, a3), (a3, a1)} homomorphically maps into Q is the same as checking
whether the pp sentence ∃x1.∃x2.∃x3.(x1 < x2 ∧x2 < x3 ∧x3 < x1) is true in Q.
The CSP for Q is in P since a structure A = (A, <A) can homomorphically
be mapped into Q iﬀit does not contain a <-cycle, i.e., there are is no n ≥1 and
elements a0, . . . , an−1 ∈A such that a0 <A . . . <A an−1 <A a0. Testing whether
such a cycle exists can be done in logarithmic space since it requires solving the
reachability problem in a directed graph (digraph). In the example above, we
obviously have a cycle, and thus this instance of CSP(Q) has no solution.
The deﬁnition of admissibility in [4] actually also requires that the predicates
are closed under negation and that there is a predicate for the whole domain.
We have already seen that the negation ≥of < is ∃+ deﬁnable in Q and that the
predicate for the whole domain is pp deﬁnable. The negation of this predicate
has the pp deﬁnition x < x. The following lemma implies that the expansion of
Q by these predicates still has a decidable CSP.3
Lemma 1 ([10]).
Let C, D be structures over the same domain with ﬁnite
signatures. If the relations of C have a pp deﬁnition in D, then CSP(C) ≤PTime
CSP(D); if they have an ∃+ deﬁnition in D, then CSP(C) ≤NPTime CSP(D).
3
DLs with Concrete Domains
As in [4] and [33], we extend the well-known DL ALC with concrete domains.
We assume that the reader is familiar with the syntax and semantics of ALC
(as, e.g., deﬁned in [7]), and thus only show how both need to be extended to
accommodate a concrete domain D. In the general deﬁnition, we allow reference
to Σ deﬁnable predicates for a fragment Σ of FO rather than just the elements
of τ. For technical reasons, we must, however, restrict the arities of deﬁnable
predicates by a ﬁxed upper bound d. Given a τ-structure D with ﬁnite relational
signature τ, a set Σ of FO formulas over the signature τ, and a bound d ≥1 on
the arity of the Σ-deﬁnable predicates, we obtain a DL ALCd
Σ(D), which extends
ALC as follows.
From a syntactic point of view, we assume that the set of role names NR
contains a set of functional roles NfR ⊆NR, and that in addition we have a set of
3 The lemma actually only yields an NP decision procedure for this CSP, but it is
easy to see that the above polynomial-time cycle-checking algorithm can be adapted
such that it also works for the expanded structure.

418
F. Baader and J. Rydval
feature names NF, which provide the connection between the abstract and the
concrete domain. A path is of the form r f or f where r ∈NR and f ∈NF. In
our example in the introduction, age is a feature name and child age is a path.
The DL ALCd
Σ(D) extends ALC with two new concept constructors:
∃p1, . . . , pk. [φ(x1, . . . , xk)]
and
∀p1, . . . , pk. [φ(x1, . . . , xk)],
where k ≤d, p1, . . . , pk are paths, and φ(x1, . . . , xk) is a formula in Σ with k
free variables, deﬁning a k-ary predicate on D. As usual, a TBox is deﬁned to
be a ﬁnite set of GCIs C ⊑D, where C, D are ALCd
Σ(D) concept descriptions.
Regarding the semantics, we consider interpretations I = (ΔI, ·I), consisting
of a non-empty set ΔI and an interpretation function ·I, which interprets concept
names A as subsets AI of ΔI and role names r as binary relations rI on ΔI, with
the restriction that rI is functional for r ∈NfR, i.e., (d, e) ∈rI and (d, e′) ∈rI
imply e = e′. In addition, features f ∈NF are interpreted as functional binary
relations f I ⊆ΔI × D. We extend the interpretation function to paths of the
form p = r f by setting
pI = {(d, d′) | there is d′′ ∈ΔI such that (d, d′′) ∈rI and (d′′, d′) ∈f I}.
For the concept constructors of ALC, the extension of the interpretation func-
tion to complex concepts is deﬁned in the usual way. The new concrete domain
constructors are interpreted as follows:
(∃p1, . . . , pk. [φ(x1, . . . , xk)])I = {d ∈ΔI | there exist d1, . . . , dk ∈D such that
(d, di) ∈pI
i for all i ∈[k] and D |= φ(d1, . . . , dk)},
(∀p1, . . . , pk. [φ(x1, . . . , xk)])I = {d ∈ΔI | for all d1, . . . , dk ∈D we have that
(d, di) ∈pI
i for all i ∈[k] implies D |= φ(d1, . . . , dk)}.
As usual, an interpretation I is a model a TBox T if it satisﬁes all the GCIs in
T , where I satisﬁes the GCI C ⊑D if CI ⊆DI holds. The ALCd
Σ(D) concept
description C is satisﬁable w.r.t. T if there is a model I of T such that CI ̸= ∅.
Since all Boolean operators are available in ALCd
Σ(D), the subsumption problem
mentioned in the introduction and the satisﬁability problem are inter-reducible
in polynomial time [7].
As a convention, we write ALC(D) instead of ALCd
Σ(D) if d is the maximal
arity of the predicates in τ and Σ consists of all atomic τ-formulas not using the
equality predicate.
3.1
Undecidable DLs with Concrete Domains
We show by a reduction from the halting problem of two-register machines that
concept satisﬁability in ALC(D) is undecidable if D is a structure with domain
Q, Z, or N whose only predicate is the binary predicate +1, which is interpreted
as incrementation (i.e., it consists of the tuples (m, m + 1) for numbers m in the
respective domain).
Our proof is an adaptation of the undecidability proof in [7] to the case where
no zero test =0 is available.4 A two-register machine (2RM) is a pair (Q, P)
4 A similar trick for zero test elimination is used in the proof of Proposition 1 in [16].

DLs with Concrete Domains and GCIs
419
with states Q = {q0, . . . , qℓ} and a sequence of instructions P = I0, . . . , Iℓ−1.
By deﬁnition, q0 is the initial state and qℓthe halting state. In state qi (i <
ℓ) the instruction Ii must be applied. Instructions come in two varieties. An
incrementation instruction is of the form I = +(p, q) where p ∈{1, 2} is the
register number and q is a state. This instruction increments (the content of)
register p and then goes to state q. A decrementation instruction is of the form
I = −(p, q, q′) where p ∈{1, 2} and q, q′ are states. This instruction decrements
register p and goes to state q if the content of register p is not zero; otherwise, it
leaves register p as it is and goes to state q′. It is well-known that the problem
of deciding whether a given 2RM halts on input (0, 0) is undecidable [35].
Proposition 1. If D is (Q, +1), (Z, +1), or (N, +1), then concept satisﬁability
in ALC(D) w.r.t. TBoxes is undecidable.
Proof. Let (Q, P) be an arbitrary 2RM. We deﬁne a concept C and a TBox
T in such a way that every model of T in which C is non-empty represents
the computation of (Q, P) on the input (0, 0). For every state qi we introduce
a concept name Qi. We also introduce two concept names Z1, Z2 to indicate
a positive zero test for the ﬁrst and second register, respectively. In addition,
we introduce a functional role g ∈NfR representing the transitions between
conﬁgurations of the 2RM. For p ∈{1, 2}, we have features rp ∈NF representing
the content of register p. However, since our concrete domain does not have
the predicate =0, we cannot enforce that, in our representation of the initial
conﬁguration, r1 and r2 have value zero. What we can ensure, though, is that
their value is the same number, which we can store in a concrete feature z ∈NF.
The idea is now that register p of the machine actually contain the value of rp
oﬀset with the value of z. We also need auxiliary concrete features s1, s2, s ∈NF,
which respectively refer to the successor values of r1, r2, z. They are needed to
express equality using +1.
The following GCI ensures that the elements of C represent the initial con-
ﬁguration together with appropriate values for the auxiliary features:
C ⊑Q0 ⊓∃r1, s1. [+1] ⊓∃r2, s2. [+1] ⊓∃z, s1. [+1] ⊓∃z, s2. [+1] ⊓∃z, s. [+1].
Next, the GCI ⊤⊑∃gz, s. [+1] ⊓∃gz, gs. [+1] guarantees that the value z of an
individual carries over to its g-successor. We denote the second value in {1, 2}
beside p by p, i.e., p = 3−p. To enforce that the incrementation instructions are
executed correctly, for every instruction Ii = +(p, qj), we include in T the GCI
Qi ⊑∃rp, grp. [+1] ⊓∃grp, sp. [+1] ⊓∃sp, gsp. [+1] ⊓∃rp, gsp. [+1] ⊓∃g. Qj
The GCIs Zp ⊑∃z, sp. [+1], ∃z, sp. [+1] ⊑Zp ensure that Zp represents a positive
zero test for register p. Note that, for individuals for which values for s, z, sp, rp
are deﬁned, the negation of Zp is semantically equivalent to a negative zero test
for register p. To enforce that decrementation is executed correctly, for every
instruction Ii = −(p, qj, qk), we include in T the GCIs
Qi ⊓Zp ⊑∃grp, sp. [+1] ⊓∃grp, sp. [+1] ⊓∃rp, gsp. [+1] ⊓∃rp, gsp. [+1] ⊓∃g. Qk,
Qi ⊓¬Zp ⊑∃grp, rp. [+1] ⊓∃grp, sp. [+1] ⊓∃gsp, sp. [+1] ⊓∃rp, gsp. [+1] ⊓∃g. Qj.

420
F. Baader and J. Rydval
Finally, we include the GCI Qℓ⊑⊥, which states that the halting state is never
reached. It is now easy to see that the computation of (Q, P) on (0, 0) does not
reach the halting state iﬀC is satisﬁable w.r.t. T .
⊓⊔
Note that, even though our proof of Proposition 1 uses a functional role g to
represent the transitions between conﬁgurations of a given two-register machine,
the reduction also works if g is assumed to be an arbitrary role. One simply
must use additional universal quantiﬁcation to ensure that all the g-successors
of an individual behave the same (i.e., for every existential quantiﬁcation in the
current proof we add the corresponding universal quantiﬁcation).
It turns out that undecidability also holds if we use the ternary predicate +
rather than the binary predicate +1. Intuitively, with + we can easily test for
0 since m is 0 iﬀm + m = m. Instead of incrementation by 1, we can then use
addition of a ﬁxed non-zero number (see [8] for a detailed proof).
Proposition 2. If D is (Q, +), (Z, +), or (N, +), then concept satisﬁability in
ALC(D) w.r.t. TBoxes is undecidable.
3.2
ω-admissible Concrete Domains
To regain decidability in the presence of GCIs and concrete domains, the notion
of ω-admissible concrete domains was introduced in [33]. We generalize this
notion and the decidability result from concrete domains with only binary pred-
icates as in [33] to concrete domains with predicates of arbitrary arity.
We say that a countable structure D has homomorphism compactness if, for
every countable structure B, it holds that B →D iﬀA →D for every ﬁnite
structure A with A →B. A relational τ-structure D satisﬁes
(JE) if, for some d ≥2,  
RD  R ∈τ, RD ⊆Dk
= Dk if k ≤d and ∅else;
(PD) if RD ∩R′D = ∅for all pairwise distinct R, R′ ∈τ;
(JD) if {RD  R ∈τ, RD ⊆△D} = △D.
Here JE stands for “jointly exhaustive,” PD for “pairwise disjoint,” and JD for
“jointly diagonal.” Note that JD was not considered in [33]. We include it here
since it makes the comparison with known notions from model theory easier. In
addition, all the ω-admissible concrete domains considered in [33] satisfy JD.
A relational τ-structure D is a patchwork if it is JDJEPD, and for all ﬁnite
JEPD τ-structures A, B1, B2 with e1 : A →B1, e2 : A →B2, B1 →D and
B2 →D, there exist f1 : B1 →D and f2 : B2 →D with f1 ◦e1 = f2 ◦e2.
Deﬁnition 1. The relational structure D is ω-admissible if CSP(D) is decid-
able, D has homomorphism compactness, and D is a patchwork.
The idea is now that one can use disjunctions of atomic formulas of the same
arity within concrete domain restrictions. We refer to the set of all FO τ-formulas
of the form R1(x1, . . . , xk)∨· · ·∨Rm(x1, . . . , xk) for R1, . . . , Rm k-ary predicates
in τ by∨+. The following theorem is proved in [8] by extending the tableau-based
decision procedure given in [33] to our more general deﬁnition of ω-admissibility.
Note that the proof of correctness of this procedure does not depend on JD.

DLs with Concrete Domains and GCIs
421
Theorem 1. Let D be an ω-admissible τ-structure with at most d-ary rela-
tions for some d ≥2. Then concept satisﬁability in ALCd
∨+(D) w.r.t. TBoxes is
decidable.
The main motivation for the deﬁnition of ω-admissible concrete domains in
[33] was that they can capture qualitative calculi of time and space. In particular,
it was shown in [33] that Allen’s interval logic [1] as well as the region connection
calculus RCC8 [37] can be represented as ω-admissible concrete domains. To
the best of our knowledge, no other ω-admissible concrete domains have been
exhibited in the literature since then.
4
A Model-Theoretic Approach Towards ω-admissibility
In this section, we introduce several model-theoretic properties of relational
structures and show their connection to ω-admissibility. This allows us to for-
mulate suﬃcient conditions for ω-admissibility using well-know notions from
model theory, and thus to employ existing model-theoretic results to ﬁnd new
ω-admissible concrete domains.
ω-categoricity. We start with introducing ω-categoricity since it gives us homo-
morphism compactness “for free.” A structure is ω-categorical if its ﬁrst-order
theory has exactly one countable model up to isomorphism. For example, it
is well-known that Q is, up to isomorphism, the only countable dense linear
order without lower or upper bound. This result, which clearly implies that Q
is ω-categorical, is due to Cantor.
For every structure A, the set of all its automorphisms forms a permutation
group, which we denote by Aut(A) (see Theorem 1.2.1 in [25]). Every relation
with an FO deﬁnition in A is easily seen to be preserved by Aut(A). For ω-
categorical structures, the other direction holds as well.
Theorem 2 (Engeler, Ryll-Nardzewski, Svenonius [25]). For a countably
inﬁnite structure A with a countable signature, the following are equivalent:
1. A is ω-categorical.
2. For every k ≥1, only ﬁnitely many k-ary relations are FO deﬁnable in A.
3. Every relation over A preserved by Aut(A) is FO deﬁnable in A.
The following corollary to this theorem establishes the ﬁrst link between model
theory and ω-admissibility.
Corollary 1 (Lemma 3.1.5 in [10]). Every ω-categorical structure has homo-
morphism compactness.
In order to obtain JDJEPD, we replace the original relations of a given
ω-categorical τ-structure A with appropriate ﬁrst-order deﬁnable ones, using
the results of Theorem 2. The orbit of a tuple (a1, . . . , ak) ∈Ak under the
natural action of Aut(A) on Ak is the set

g(a1), . . . , g(an)

| g ∈Aut(A)

.
By Theorem 2, the set of all at most k-ary relations FO deﬁnable in A is ﬁnite

422
F. Baader and J. Rydval
for every k ∈N. Since every such set is closed under intersections, it contains
ﬁnitely many minimal non-empty relations. Since every relation over A that
is preserved by all automorphisms of A is FO deﬁnable in A, these minimal
elements are precisely the orbits of tuples over A under the natural action of
Aut(A).
Deﬁnition 2. For a given arity bound d ≥2, the d-reduct of the ω-categorical τ-
structure A, denoted by A⩽d, is the relational structure over A whose relations
are all orbits of at most d-ary tuples over A under Aut(A). We denote the
signature of A⩽d by τ ⩽d.
It is easy to see that A⩽d is JDJEPD, and that every at most d-ary relation
over A FO deﬁnable in A can be obtained as a disjunction of atomic formu-
las built using the symbols in τ ⩽d. As an example, consider the ω-categorical
structure Q. The orbits of k-tuples of elements of Q can be deﬁned by quantiﬁer-
free formulas that are conjunctions of atoms of the form xi = xj or xi < xj. For
example, the orbit of the tuple (2, 3, 2, 5) consists of all tuples (q1, q2, q3, q4) ∈Q4
that satisfy the formula x1 < x2 ∧x1 = x3 ∧x2 < x4 if xi is replaced by qi for
i = 1, . . . , 4. The FO deﬁnable k-ary relations in Q are obtained as unions of
these orbits, where the deﬁning formula is then the disjunction of the formulas
deﬁning the respective orbits. Since these formulas are quantiﬁer-free, this also
shows that Q admits quantiﬁer elimination. Recall that a τ-structure admits
quantiﬁer elimination if for every FO τ-formula there exists a quantiﬁer-free
(qf) τ-formula that deﬁnes the same relation over this structure.
Homogeneity. To obtain the patchwork property, we restrict the attention to
homogeneous structures. A structure A is homogeneous if every isomorphism
between ﬁnite substructures of A extends to an automorphism of A.
Theorem 3 ([25]).
A countable relational structure with a ﬁnite signature is
homogeneous iﬀit is ω-categorical and admits quantiﬁer elimination.
Since Q is ω-categorical and admits quantiﬁer elimination, it is thus homo-
geneous. This can, however, also easily be shown directly without using the
theorem. In fact, given ﬁnite substructures B and C of Q and an isomorphism
between them, we know that B consists of ﬁnitely many elements p1, . . . , pn
and C of the same number of elements q1, . . . , qn such that p1 < . . . < pn,
q1 < . . . < qn, and the isomorphism maps pi to qi (for i = 1, . . . , n). It is now
easy to see that < is also a dense linear order without lower or upper bound on
the sets {p | p < p1} and {q | q < q1}, and thus there is an order isomorphism
between these sets. The same is true for the pairs of sets {p | pi < p < pi+1} and
{q | qi < q < qi+1}, and for the pair {p | pn < p} and {q | q < qn}. Using the
isomorphisms between these pairs, we can clearly put together an isomorphisms
from Q to Q that extends the original isomorphism from B to C.
Countable homogeneous structures can be obtained as Fra¨ıss´e limits of amal-
gamation classes. A class K of relational τ-structures has the amalgamation prop-
erty (AP) if, for every A, B1, B2 ∈K with e1 : A →B1 and e2 : A →B2 there

DLs with Concrete Domains and GCIs
423
exists C ∈K with f1 : B1 →C and f2 : B2 →C such that f1 ◦e1 = f2 ◦e2.
A class K of ﬁnite relational structures with a countable signature τ is called
an amalgamation class if it has AP, is closed under applying isomorphisms and
taking substructures, and contains only countably many structures up to iso-
morphism. We denote by Age (A) the class of all ﬁnite structures that embed
into the structure A.
Theorem 4 (Fra¨ıss´e [25]).
Let K be an amalgamation class of τ-structures.
Then there exists a homogeneous countable τ-structure A with Age (A) = K.
The structure A is unique up to isomorphism and referred to as the Fra¨ıss´e
limit of K. Conversely, Age (A) for a countable homogeneous structure A with
a countable signature is an amalgamation class.
For our running example Q = (Q, <), we have that Age (Q) consists of all
ﬁnite linear orders, and thus by Fra¨ıss´e’s theorem this class of structures is an
amalgamation class. In addition, Q is the Fra¨ıss´e limit of this class. Proposition 3
below shows that there is a close connection between AP and the patchwork
property. Its proof uses the following lemma, whose proof can be found in [8].
Lemma 2. Let A, B be two JEPD τ-structures, such that B is JD, and f : A →
B a homomorphism. Then f preserves the complements of all relations of A and
ker f = {RA | R ∈τ and RB ⊆△B}.
Proposition 3. Let D be a JDJEPD τ-structure. Then D is a patchwork iﬀ
Age (D) has AP.
Proof. For simpliﬁcation purposes, every statement indexed by i is suppose to
hold for both i ∈{1, 2}. First, suppose that Age (D) has AP. Let A, B1, B2 be
ﬁnite JEPD τ-structures with ei : A →Bi and hi : Bi →D. We must show
that there exist fi : Bi →D with f1 ◦e1 = f2 ◦e2. Let A1 and A2 be the
substructures of D on (h1 ◦e1)(A) and (h2 ◦e2)(A), respectively. Clearly both
A1 and A2 are JDJEPD, because they are substructures of D. Due to Lemma 2,
we have A1 ∼= A2, because both h1 ◦e1 and h2 ◦e2 preserve the complements of
all relations of A and ker h1 ◦e1 = {RA | R ∈τ and RD ⊆△D} = ker h2 ◦e2.
However, what we want is an isomorphism that commutes with h1 ◦e1 and
h2 ◦e2. Consider the map g: A1 →A2 given by g

(h1 ◦e1)(a)
 := (h2 ◦e2)(a).
It is well deﬁned, because ker h1 ◦e1 = ker h2 ◦e2. Now, for every R ∈τ and
((h1◦e1)(a1), . . . , (h1◦e1)(ak)) ∈R 
A1, we have (a1, . . . , ak) ∈RA, because h1◦e1
preserves the complements of all relations of A due to Lemma 2. But this implies
((h2 ◦e2)(a1), . . . , (h2 ◦e2)(ak)) ∈R 
A2, because h2 ◦e2 is a homomorphism. By
Lemma 2, g preserves the complements of all relations of A1 and
ker g =
	 
R

A1  R ∈τ and R

A2 ⊆△
A2

= △
A1.
Hence g is an isomorphism that additionally satisﬁes g ◦h1 ◦e1 = h2 ◦e2.
Let B1 and B2 be the substructures of D on h1(B1) and h2(B2), respectively.
Now consider the inclusions ei : Ai →Bi. Since Age (D) has AP, there exists

424
F. Baader and J. Rydval
C ∈Age (D) together with fi : Bi →C and e: C →D such that f1 ◦e1 =
f2 ◦e2 ◦g. We deﬁne the homomorphisms fi : Bi →D by fi := e ◦fi ◦hi. Then,
for every a ∈A,
f1 ◦e1(a) = e ◦f1 ◦h1 ◦e1(a) = e ◦f1 ◦e1 ◦h1 ◦e1(a)
= e ◦f2 ◦e2 ◦g ◦h1 ◦e1(a) = e ◦f2 ◦e2 ◦h2 ◦e2(a)
= e ◦f2 ◦h2 ◦e2(a) = f2 ◦e2(a).
Hence D is a patchwork. For the other direction, suppose that D is a patchwork.
Let A, B1, B2 be ﬁnite τ-structures with ei : A →Bi and hi : Bi →D. Since B1
and B2 are isomorphic to substructures of D, they are clearly JEPD. Thus, as
D is a patchwork, there exist homomorphisms fi : Bi →D with f1 ◦e1 = f2 ◦e2.
By Lemma 2, the fi preserve the complements of all relations of Bi, and
ker fi =
	 
RBi  R ∈τ and RD ⊆△D

= ker hi = △Bi.
This means that fi are embeddings. We obtain AP for Age (D) by choosing C
to be the substructure of D on f2(B1) ∪f1(B2).
⊓⊔
Recall that, to obtain JDJEPD, we actually need to take the d-reduct of
a given ω-categorical structure, rather than the structure itself. Fortunately,
homogeneity transfers from D to D⩽d (see [8] for the proof).
Lemma 3. Let D be a countable homogeneous structure with a ﬁnite relational
signature τ. Then D⩽d is homogeneous for every d that exceeds or is equal to
the maximal arity of the symbols from τ.
Finite Boundedness. The only property of ω-admissible structures we have
not yet considered in this section is the decidability of the CSP. One possi-
bility to achieve this is to consider ﬁnitely bounded structures. For a class N
of τ-structures, we denote by Forbe(N) the class of all ﬁnite τ structures not
embedding any member of N. We say that a structure A is ﬁnitely bounded if
its signature is ﬁnite and Age (A) = Forbe(N) for a ﬁnite N [14]. Note that A is
ﬁnitely bounded iﬀthere exists a universal FO sentence Φ(A) s.t. B ∈Age (A)
iﬀB |= Φ(A) [8].
The structure Q is ﬁnitely bounded. To show this, we can use the set N
consisting of the four structures depicted in Fig. 1: the self loop, the 2-cycle,
the 3-cycle, and two isolated vertices. We must show that Age (Q) = Forbe(N).
Clearly, none of the structures in N embeds into a linear order, which shows
Age (Q) ⊆Forbe(N). Conversely, assume that A is an element of Forbe(N).
We must show that <A is a linear order. Since N contains the self loop, we
have (a, a) ̸∈<A for all a ∈A, which shows that <A is irreﬂexive. For distinct
elements a, b ∈A, we must have a <A b or b <A a since otherwise the structure
consisting of two isolated vertices could be embedded into A. This shows that any
two distinct elements are comparable w.r.t. <A. To show that <A is transitive,

DLs with Concrete Domains and GCIs
425
<
<
>
<
>
>
Fig. 1. A set of four forbidden substructures for Q = (Q; <).
assume that a <A b and b <A c holds. Since the 2-cycle does not embed into A,
a and c must be distinct, and are thus comparable. We cannot have c <A a since
then we could embed the 3-cycle into A. Consequently, we must have a <A c,
which proves transitivity. This show that A is a linear order. As formula Φ(Q)
we can take the conjunction of the usual axioms deﬁning linear orders.
Finitely bounded structures are interesting since their CSP and their ﬁrst-
order theory are decidable. The ﬁrst result can, e.g., be found in [13] (Theorem 4)
and the second result is stated in [28,29] (see [8] for a detailed proof).
Proposition 4. Let D be ﬁnitely bounded homogeneous structure with |D| > 1.
Then CSP(D) is decidable in NP and the FO theory of D is PSpace-complete.
The following proposition, whose proof can be found in [8], implies that
Proposition 4 applies not only to a given ﬁnitely bounded homogeneous structure
D, but also to its d-reduct D⩽d.
Proposition 5. Let A be a ﬁnitely bounded homogeneous structure and B a
structure with the same domain and ﬁnitely many relations that are FO deﬁnable
in A. Then B is a reduct of a ﬁnitely bounded homogeneous structure.
We are now ready to formulate our ﬁrst suﬃcient condition for ω-
admissibility.
Theorem 5. Let D be a ﬁnitely bounded homogeneous relational structure with
at most d-ary relations for some d ≥2. Then D⩽d is ω-admissible.
Proof. It follows directly from its deﬁnition that D⩽d is JEPD. Since d ≥2, it is
clearly also JD. By Lemma 3, D⩽d is homogeneous and ω-categorical. Thus D
has homomorphism compactness by Corollary 1. By Theorem 4, Age (D⩽d) has
AP. Thus D⩽d is a patchwork by Proposition 3. By Proposition 5, Lemma 1,
and Proposition 4, CSP(D⩽d) is in NP. Hence D⩽d is ω-admissible.
⊓⊔
This theorem, together with Theorem 1, immediately yields decidability for
ALCd
∨+(D⩽d). The following corollary shows that we can even allow for arbi-
trary FO deﬁnable relations with arity bounded by d in the concrete domain.
Corollary 2. Let D be a ﬁnitely bounded homogeneous relational structure with
at most d-ary relations for some d ≥2. Then concept satisﬁability in ALCd
FO(D)
w.r.t. TBoxes is decidable.

426
F. Baader and J. Rydval
The idea for proving this result is to reduce concept satisﬁability in ALCd
FO(D) to
concept satisﬁability in ALCd
∨+(D⩽d). We know that every at most d-ary relation
over D FO deﬁnable in D can be obtained as a disjunction of atomic formulas
built using the signature of D⩽d. What still needs to be shown is that, given
a ﬁrst-order formula in the signature of D with at most d free variables, this
disjunction can eﬀectively be computed (see [8] for how this can be proved).
Cores. Finally, we consider the situation where we have a homogeneous rela-
tional structure D with ﬁnitely many at most d-ary relations that is not ﬁnitely
bounded, but which we know (by some other means) to have a decidable CSP.
In this situation, we can show decidability for ALCd
∃+(D) under one additional
assumption. A structure D is called a core if every endomorphism from D to
itself is an embedding. It was shown in [9] that, if D is a homogeneous core,
then the orbits of tuples over D under Aut(D) are pp deﬁnable in D. As an
easy consequence of this result, we obtain our second suﬃcient condition for
ω-admissibility (see [8] for the proof).
Theorem 6. Let D be a homogeneous relational structure with ﬁnitely many at
most d-ary relations for some d ≥2 that is a core and has a decidable CSP.
Then D⩽d is ω-admissible.
By showing that concept satisﬁability in ALCd
∃+(D) can eﬀectively be
reduced to concept satisﬁability in ALCd
∨+(D⩽d), we obtain the following decid-
ability result (see [8] for the proof).
Corollary 3. Let D be a homogeneous relational structure with ﬁnitely many
at most d-ary relations for some d ≥2 that is a core and has a decidable CSP.
Then concept satisﬁability in ALCd
∃+(D) w.r.t. TBoxes is decidable.
5
Application and Discussion
In this section, we discuss how the results of Sect. 4 can be used to obtain speciﬁc
ω-admissible concrete domains. But let us ﬁrst start with a caveat.
Finiteness of Signature Matters. In Corollary 2 and Corollary 3, the signature
of the structure D is required to be ﬁnite. This restriction is needed to obtain
decidability. For instance, the expansion of the structure (Z; +1) from Sect. 3.1
by all relations +k = {(m, n) ∈Z2 | m + k = n} for k ∈Z is homogeneous, and
satisﬁability of ﬁnite conjunctions of constraints is decidable in this structure.
However, we have seen in Proposition 1 that reasoning with (Z; +1) as a concrete
domain w.r.t. TBoxes is undecidable.
(Un)decidability of the Conditions. If one intends to use Theorem 5 to obtain
an ω-admissible concrete domain, one could start with selecting a ﬁnite set N
of bounds, i.e., forbidden τ-substructures, for a ﬁnite signature τ. The ques-
tion is then whether N really induces a ﬁnitely bounded structure, i.e., whether

DLs with Concrete Domains and GCIs
427
there is a τ-structure D such that Age (D) = Forbe(N). This question is in gen-
eral undecidable. In fact, it is shown in [17] that the joint embedding property
(JEP) is undecidable for classes of structures that are deﬁnable by ﬁnitely many
bounds. In addition, it is known that a class of structures deﬁnable by ﬁnitely
many bounds has JEP iﬀthis class is the age of some countably inﬁnite struc-
ture (see [25], Theorem 6.1.1). However, if one restricts the attention to binary
signatures, then it is decidable whether a class of the form Forbe(N) has AP
[12]. If this is the case, then the Fra¨ıss´e limit D of Forbe(N) is a ﬁnitely bounded
homogeneous structure satisfying Age (D) = Forbe(N) by Theorem 4.
Reproducing Known Results. The examples for ω-admissible concrete domains
given in [33] were RCC8 and Allen’s interval algebra, for which the patchwork
property is proved “by hand” in [33]. Given Theorem 5, we obtain these results
as a consequence of known results from model theory. It was shown in [15]
that RCC8 has a representation by a homogeneous structure R with a ﬁnite
relational signature (Theorem 2 in [15]). Since Age (R) has a ﬁnite universal
axiomatization (Deﬁnition 3 in [15]), R is ﬁnitely bounded. For Allen’s interval
algebra, it was shown in [24] that it has a representation by a homogeneous
structure A with a ﬁnite relational signature. Since Age (A) has a ﬁnite universal
axiomatization, A is ﬁnitely bounded. Our running example Q = (Q, <) also
satisﬁes the preconditions of Theorem 5, and thus Corollary 2 yields decidability
of ALCd
FO(Q) with TBoxes. For Q extended just with >, ≤, ≥, =, ̸=, decidability
was proved in [30], using an automata-based procedure. Our results show that
there is also a tableau-based decision procedure for this logic.
Expansions, Disjoint Unions, and Products. When modelling concepts in a DL
with concrete domain D, it is often useful to be able to refer to speciﬁc elements
d of the domain, i.e., to have unary predicate symbols =d that are interpreted
as {d}. We can show that the class of reducts of ﬁnitely bounded homogeneous
structures is closed under expansion by ﬁnitely many such relations [8].
It would also be useful to be able to refer to predicates of diﬀerent concrete
domains (say RCC8 and Allen) when deﬁning concepts. In [5], it was shown that
admissible concrete domains are closed under disjoint union. We can prove the
corresponding result for ﬁnitely bounded homogeneous structures [8]. Using dis-
joint union to refer to several concrete domain works well if the paths employed
in concrete domain constructors contain only functional roles, which is the case
considered in [5]. However, if we allow for non-functional roles in paths, then
using disjoint union is not appropriate. In general, if R, R′ are two binary rela-
tions over D and r ∈NR\NfR, then the situation where an individual x has an
r-successor y with features related through both R and R′ cannot be described
using the disjoint union of (D; R) and (D; R′) as a concrete domain (see [8] for
details).
To overcome this problem, we propose to use the so-called full product [10].
Let A1, . . . , Ak be ﬁnitely many structures with disjoint relational signatures
τ1, . . . , τk. The full product of A1, . . . , Ak, denoted by A1 ⊠· · · ⊠Ak, has as
its domain the Cartesian product A := A1 × · · · × Ak and as its signature the

428
F. Baader and J. Rydval
union of the signatures τi. For a ∈A1 × · · · × Ak and i ∈[k], we denote the ith
component of the tuple a by a[i]. The relations are deﬁned as follows:
RA1×···×Ak := {(a1, . . . , an) ∈An | (a1[i], . . . , an[i]) ∈RAi}
for every i ∈[k] and every n-ary R ∈τi. We show in [8] that the full product
preserves homogeneity and ﬁnite boundedness, and thus the prerequisites for
Theorem 5 and Corollary 2 are preserved under building the full product.
Proposition 6. Let A1, . . . , Ak be ﬁnitely bounded homogeneous structures with
disjoint relational signatures τ1, . . . , τk such that, for i ∈[k], τi contains the
symbol =i, which is deﬁned in Ai as △Ai. Then A1 ⊠· · · ⊠Ak is a ﬁnitely
bounded homogeneous structure.
Together with Proposition 4 this also yields a general complexity result for
combinations of constraints over several ﬁnitely bounded homogeneous tem-
plates. Such combinations were previously considered in the literature in special
cases; for example, for RCC8 and Allen [21].
Henson Digraphs. A directed graph is a tournament if every two distinct vertices
in it are connected by exactly one directed edge. In [23], Henson proved that there
are uncountably many homogeneous directed graphs by showing that, for any
set N of ﬁnite tournaments (plus the loop and the 2-cycle) such that no member
of N is embeddable into any other member of N, Forbe(N) is an amalgamation
class whose Fra¨ıss´e limit is a homogeneous directed graph. Furthermore, the
Fra¨ıss´e limits for two distinct sets of such tournaments are distinct as well. In
the literature, such directed graphs are often called Henson digraphs [34]. If G
is a Henson digraph, then Age (G) = CSP(G).5 Clearly, only countably many
Henson digraphs can have a decidable CSP. Beside the ﬁnitely bounded ones (see
Proposition 4), there is an interesting example constructed using the inﬁnite set
of non-isomorphic tournaments from Henson’s original proof of uncountability.
Consider the tournaments T1, T2, . . . with domains [2], [3], . . . such that the edge
relation of Tn consists of the edges (i, j) for every j = i + 1 with 0 ≤i ≤n,
(0, n + 1), and (j, i) for every j > i + 1 with (i, j) ̸= (0, n + 1). It was shown in
[11] that the CSP of the Henson digraph corresponding to N := {T1, T2, . . .} is
coNP-complete. This digraph is a homogeneous core, and its CSP is decidable.
Thus, it satisﬁes the requirements of Corollary 3. However, it is clearly not ﬁnitely
bounded, and thus does not satisfy the requirements of Corollary 2. Conversely,
it is known that the random graph is ﬁnitely bounded and homogeneous [25], but
it is not a core [9]. This shows that the class of structures covered by Corollary 3
is incomparable with the one covered by Corollary 2.
6
Conclusion
We have shown that ω-admissibility, which was introduced in the DL community
to obtain decidable extensions of DLs by concrete domains, is closely related to
5 One direction is obvious, the other holds because homomorphisms between directed
graphs cannot contract any edges.

DLs with Concrete Domains and GCIs
429
well-known notions from model theory. Given the fact that a large number of
homogeneous structures are known from the literature [34] and that homoge-
neous and ﬁnitely bounded structures play an important rˆole in the CSP com-
munity, we believe that our work will turn out to be useful for locating new
ω-admissible concrete domains.
This is not the ﬁrst model-theoretic description of a suﬃcient condition for
decidability of reasoning in DLs with concrete domains in the presence of TBoxes.
The existence of homomorphism is deﬁnable (EHD) property was used in [19] to
obtain decidability results for DLs with concrete domains. However, the way the
concrete domain is integrated into the DL in [19] is diﬀerent from the classical
one employed by us and used in all other papers on DLs with concrete domains.
In [19], constraints are always placed along a linear path stemming from a single
individual, which is rather similar to the use of constraints in temporal logics
[18,20]. In contrast, in the classical setting of DLs with concrete domains, one
can compare feature values of siblings of an individual.
References
1. Allen, J.F.: Maintaining knowledge about temporal intervals. Commun. ACM
26(11), 832–843 (1983)
2. Baader, F., B¨urckert, H.J., Hollunder, B., Nutt, W., Siekmann, J.H.: Concept log-
ics. In: Lloyd, J.W. (ed.) Computational Logics. ESPRIT Basic Research Series, pp.
177–201. Springer, Heidelberg (1990). https://doi.org/10.1007/978-3-642-76274-
1 10
3. Baader, F., Calvanese, D., McGuinness, D., Nardi, D., Patel-Schneider, P.F. (eds.):
The Description Logic Handbook: Theory, Implementation, and Applications.
Cambridge University Press, Cambridge (2003)
4. Baader, F., Hanschke, P.: A schema for integrating concrete domains into concept
languages. In: Proceedings of the 12th International Joint Conference on Artiﬁcial
Intelligence (IJCAI 1991), pp. 452–457 (1991). Long version available as [5]
5. Baader, F., Hanschke, P.: A scheme for integrating concrete domains into
concept languages. Technical report RR-91-10, Deutsches Forschungszentrum
f¨ur K¨unstliche Intelligenz (DFKI) (1991). https://lat.inf.tu-dresden.de/research/
reports/1991/DFKI-RR-91-10.pdf
6. Baader, F., Hanschke, P.: Extensions of concept languages for a mechanical engi-
neering application. In: J¨urgen Ohlbach, H. (ed.) GWAI 1992. LNCS, vol. 671, pp.
132–143. Springer, Heidelberg (1993). https://doi.org/10.1007/BFb0018999
7. Baader, F., Horrocks, I., Lutz, C., Sattler, U.: An Introduction to Description
Logic. Cambridge University Press, Cambridge (2017)
8. Baader, F., Rydval, J.: Using model-theory to ﬁnd ω-admissible concrete domains.
LTCS-Report 20-01, Chair of Automata Theory, Institute of Theoretical Computer
Science, Technische Universit¨at Dresden, Dresden, Germany (2020). https://tu-
dresden.de/inf/lat/reports#BaRy-LTCS-20-01
9. Bodirsky, M.: The core of a countably categorical structure. In: Diekert, V.,
Durand, B. (eds.) STACS 2005. LNCS, vol. 3404, pp. 110–120. Springer, Heidelberg
(2005). https://doi.org/10.1007/978-3-540-31856-9 9
10. Bodirsky, M.: Complexity classiﬁcation in inﬁnite-domain constraint satisfaction.
Habilitation thesis, Universit´e Diderot - Paris 7 (2012). https://arxiv.org/pdf/
1201.0856.pdf

430
F. Baader and J. Rydval
11. Bodirsky, M., Grohe, M.: Non-dichotomies in constraint satisfaction complexity.
In: Aceto, L., Damg˚ard, I., Goldberg, L.A., Halld´orsson, M.M., Ing´olfsd´ottir, A.,
Walukiewicz, I. (eds.) ICALP 2008. LNCS, vol. 5126, pp. 184–196. Springer, Hei-
delberg (2008). https://doi.org/10.1007/978-3-540-70583-3 16
12. Bodirsky, M., Kn¨auer, S., Starke, F.: AMSNP: a tame fragment of existential
second-order logic. In: Proceedings of the 16th Conference on Computability in
Europe - Beyond the Horizon of Computability (CiE 2020). Lecture Notes in Com-
puter Science. Springer (2020). https://arxiv.org/abs/2001.08190.pdf
13. Bodirsky, M., Mottet, A.: Reducts of ﬁnitely bounded homogeneous structures,
and lifting tractability from ﬁnite-domain constraint satisfaction. In: Proceedings
of the 31st Annual ACM/IEEE Symposium on Logic in Computer Science (LICS
2016), pp. 623–632. ACM/IEEE (2016)
14. Bodirsky, M., Neˇsetˇril, J.: Constraint satisfaction with countable homogeneous
templates. J. Logic Comput. 16(3), 359–373 (2006)
15. Bodirsky, M., W¨olﬂ, S.: RCC8 is polynomial on networks of bounded treewidth. In:
Proceedings of the 22nd International Joint Conference on Artiﬁcial Intelligence
(IJCAI 2011). IJCAI/AAAI (2011)
16. Boja´nczyk, M., Segouﬁn, L., Toru´nczyk, S.: Veriﬁcation of database-driven systems
via amalgamation. In: Proceedings of the 32nd ACM SIGMOD-SIGACT-SIGART
Symposium on Principles of Database Systems (PODS 2013), pp. 63–74. ACM
(2013)
17. Braunfeld, S.: The undecidability of joint embedding and joint homomorphism
for hereditary graph classes. Discrete Math. Theoret. Comput. Sci. 21(2) (2019).
https://arxiv.org/pdf/1903.11932.pdf
18. Carapelle, C., Feng, S., Kartzow, A., Lohrey, M.: Satisﬁability of ECTL∗with local
tree constraints. Theory Comput. Syst. 61(2), 689–720 (2017). https://doi.org/10.
1007/s00224-016-9724-y
19. Carapelle, C., Turhan, A.Y.: Description logics reasoning wrt general TBoxes is
decidable for concrete domains with the EHD-property. In: Proceedings of the 22nd
European Conference on Artiﬁcial Intelligence (ECAI 2016), pp. 1440–1448. IOS
Press (2016)
20. Demri, S., D’Souza, D.: An automata-theoretic approach to constraint LTL. Inf.
Comput. 205(3), 380–415 (2007)
21. Gerevini, A., Nebel, B.: Qualitative spatio-temporal reasoning with RCC-8 and
Allen’s interval calculus: Computational complexity. In: Proceedings of the 15th
European Conference on Artiﬁcial Intelligence (ECAI 2002), pp. 312–316. IOS
Press (2002)
22. Haarslev, V., M¨oller, R., Wessel, M.: The description logic ALCNHR+ extended
with concrete domains: a practically motivated approach. In: Gor´e, R., Leitsch, A.,
Nipkow, T. (eds.) IJCAR 2001. LNCS, vol. 2083, pp. 29–44. Springer, Heidelberg
(2001). https://doi.org/10.1007/3-540-45744-5 4
23. Henson, C.W.: Countable homogeneous relational structures and ℵ0-categorical
theories. J. Symb. Logic 37(3), 494–500 (1972)
24. Hirsch, R.: Relation algebras of intervals. Artif. Intell. 83(2), 267–295 (1996)
25. Hodges, W.: A Shorter Model Theory. Cambridge University Press, Cambridge
(1997)
26. Hoehndorf, R., Schoﬁeld, P.N., Gkoutos, G.V.: The role of ontologies in biological
and biomedical research: a functional perspective. Brief. Bioinform. 16(6), 1069–
1080 (2015)
27. Horrocks, I., Patel-Schneider, P.F., van Harmelen, F.: From SHIQ and RDF to
OWL: the making of a web ontology language. J. Web Semant. 1(1), 7–26 (2003)

DLs with Concrete Domains and GCIs
431
28. Klin, B., Lasota, S., Ochremiak, J., Torunczyk, S.: Homomorphism problems for
ﬁrst-order deﬁnable structures. In: 36th IARCS Annual Conference on Foundations
of Software Technology and Theoretical Computer Science (FSTTCS 2016). Schloss
Dagstuhl-Leibniz-Zentrum f¨ur Informatik (2016)
29. Kopczy´nski, E., Toru´nczyk, S.: LOIS: an application of SMT solvers. In: King, T.,
Piskac, R. (eds.) Proceedings of the 14th International Workshop on Satisﬁability
Modulo Theories (SMT@IJCAR 2016). CEUR Workshop Proceedings, vol. 1617,
pp. 51–60. CEUR-WS.org (2016)
30. Lutz, C.: Interval-based temporal reasoning with general TBoxes. In: Nebel, B.
(ed.) Proceedings of the 17th International Joint Conference on Artiﬁcial Intelli-
gence (IJCAI 2001), pp. 89–94. Morgan Kaufmann, San Mateo (2001)
31. Lutz, C.: NExpTime-complete description logics with concrete domains. In: Gor´e,
R., Leitsch, A., Nipkow, T. (eds.) IJCAR 2001. LNCS, vol. 2083, pp. 45–60.
Springer, Heidelberg (2001). https://doi.org/10.1007/3-540-45744-5 5
32. Lutz, C.: Adding numbers to the SHIQ description logic–ﬁrst results. In: Proceed-
ings of the 8th International Conference on Principles of Knowledge Representation
and Reasoning (KR 2002), pp. 191–202. Morgan Kaufmann, Los Altos (2002)
33. Lutz, C., Milicic, M.: A tableau algorithm for description logics with concrete
domains and general TBoxes. J. Autom. Reason. 38(1–3), 227–259 (2007). https://
doi.org/10.1007/s10817-006-9049-7
34. Macpherson, D.: A survey of homogeneous structures. Discrete Math. 311(15),
1599–1634 (2011)
35. Minsky, M.L.: Computation: Finite and Inﬁnite Machines. Prentice-Hall, Engle-
wood Cliﬀs (1967)
36. Pan, J.Z., Horrocks, I.: Reasoning in the SHOQ(Dn) description logic. In: Hor-
rocks, I., Tessaris, S. (eds.) Proceedings of the 2002 Description Logic Workshop
(DL 2002). CEUR Workshop Proceedings, vol. 53. CEUR-WS.org (2002)
37. Randell, D.A., Cui, Z., Cohn, A.G.: A spatial logic based on regions and connection.
In: Proceedings of the 3rd International Conference on the Principles of Knowledge
Representation and Reasoning (KR 1992), pp. 165–176. Morgan Kaufmann, Los
Altos (1992)
38. Schild, K.: A correspondence theory for terminological logics: preliminary report.
In: Mylopoulos, J., Reiter, R. (eds.) Proceedings of the 12th International Joint
Conference on Artiﬁcial Intelligence (IJCAI 1991), pp. 466–471. Morgan Kaufmann
(1991)

A Formally Veriﬁed, Optimized Monitor
for Metric First-Order Dynamic Logic
David Basin, Thibault Dardinier, Lukas Heimes, Srđan Krstić(B)
,
Martin Raszyk(B)
, Joshua Schneider(B)
, and Dmitriy Traytel(B)
Institute of Information Security, Department of Computer Science,
ETH Zürich, Zurich, Switzerland
{srdan.krstic,martin.raszyk,joshua.schneider,traytel}@inf.ethz.ch
Abstract. Runtime monitors for rich speciﬁcation languages are sophis-
ticated algorithms, especially when they are heavily optimized. To gain
trust in them and safely explore the space of possible optimizations, it
is important to verify the monitors themselves. We describe the devel-
opment and correctness proof in Isabelle/HOL of a monitor for metric
ﬁrst-order dynamic logic. This monitor signiﬁcantly extends previous
work on formally veriﬁed monitors by supporting aggregations, regular
expressions (the dynamic part), and optimizations including multi-way
joins adopted from databases and a new sliding window algorithm.
1
Introduction
As the complexity of IT systems increases, so does the complexity and impor-
tance of their veriﬁcation. Research in runtime veriﬁcation (RV) has developed
well-established formal techniques that can often be applied more easily than
traditional formal methods such as model checking. RV is based on dynamic
analysis, trading oﬀcompleteness for eﬃciency. It is mechanized using monitors,
which are algorithms that search sequences of events, either oﬄine from log ﬁles
or online, for patterns indicating faults.
Monitors must be trusted when they are used as veriﬁers. This trust can be
justiﬁed by checking the monitors themselves for correctness [16,17,31,36,41,
42,44,45,49]. Recently, a simpliﬁed version of the algorithm used in the Mon-
Poly tool [8,9] has been formalized and proved correct in Isabelle/HOL [45]
(Sect. 2). MonPoly and its formal counterpart, called VeriMon, are both mon-
itors for metric ﬁrst-order temporal logic (MFOTL). However, VeriMon only
supports a restricted fragment of this logic and lacks many optimizations that
are necessary for an acceptable and competitive performance.
We present a formally veriﬁed monitor, VeriMon+, that substantially extends
and improves VeriMon. VeriMon+ closes all expressiveness gaps between Mon-
Poly and VeriMon. It supports aggregation operators like sum and average [7]
similar to those found in database query languages, arbitrary negations of closed
formulas, the unbounded  (Next) operator, and constraints involving terms
(e.g., P(x) ∧y = x + 2). Due to space limitations, our focus (Sect. 3) will be
c
⃝Springer Nature Switzerland AG 2020
N. Peltier and V. Sofronie-Stokkermans (Eds.): IJCAR 2020, LNAI 12166, pp. 432–453, 2020.
https://doi.org/10.1007/978-3-030-51074-9_25

A Formally Veriﬁed, Optimized Monitor for MFODL
433
primarily on aggregations, our largest addition. Moreover, VeriMon+ exceeds
MFOTL in expressiveness by featuring a signiﬁcantly richer speciﬁcation lan-
guage, metric ﬁrst-order dynamic logic (MFODL). To our knowledge, it is the
ﬁrst monitor for MFODL with past and bounded future operators (Sect. 4).
This logic combines MFOTL with regular expressions, similar to linear dynamic
logic [22] but enriched with metric constraints, aggregations, and ﬁrst-order
quantiﬁcation.
We have also implemented and proved correct several new optimizations.
First, to speed up the evaluation of conjunctions, we integrated an eﬃcient algo-
rithm for multi-way joins [38,39], which we generalized to include anti-joins
(Sect. 5). Second, we developed a specialized sliding window algorithm to eval-
uate the Since and Until operators more eﬃciently (Sect. 6). VeriMon+ is exe-
cutable via the generation of OCaml code from Isabelle. To this end, we aug-
mented the code generation setup for IEEE ﬂoating point numbers in OCaml [50]
with a linear ordering, which is needed for eﬃcient set and mapping data struc-
tures.
The result of our eﬀorts is both a veriﬁed monitor and a tool for evaluating
unveriﬁed monitors. Since MFODL is extremely expressive, this gives us very
wide scope. For example, we discovered previously unknown bugs in MonPoly
via diﬀerential testing (Sect. 7), extending a previous case study [45]. As this
experience suggests, and we ﬁrmly believe, formal veriﬁcation is the most reliable
way to obtain correct, optimized monitors.
In sum, our main contribution is a veriﬁed monitor for MFODL with aggre-
gations, a highly expressive speciﬁcation language that combines regular expres-
sions and ﬁrst-order temporal logic. Our monitor includes optimizations that
are novel in the context of ﬁrst-order monitoring. Our formalization is publicly
available [20,21].
Related Work. We refer to a recent book [4] for an introduction to runtime
veriﬁcation. The main families of speciﬁcation languages in this domain are
extensions of LTL [13,26,46], automata [3], stream expressions [19], and rule
systems [23]. We combine two expressive temporal logics and their corresponding
monitoring algorithms. MFOTL, implemented in MonPoly [7–9], supports ﬁrst-
order quantiﬁcation over parametrized events, but it cannot express all regular
patterns. Metric dynamic logic (MDL), implemented in Aerial [12], supports
regular expressions, but it is not ﬁrst-order. VeriMon+ is based on VeriMon [45],
which only supports a fragment of MFOTL and is ineﬃcient (Sect. 7). We refer
to [45, Section 1] for an overview of related monitor formalizations. Relational
database systems have been formalized by Malecha et al. [33] and by Benzaken
et al. [15]. These works use binary joins only, which are not worst-case optimal.
Another eﬃcient ﬁrst-order monitor, DejaVu [25], supports past-only ﬁrst-
order temporal logic. It uses binary decision diagrams (BDDs) and does not
restrict the use of negation, unlike MonPoly, which uses ﬁnite tables. DejaVu’s
performance is incomparable to MonPoly’s and it is unclear whether multi-way
joins can improve conjunctions of BDDs. Aerial and VeriMon+ evaluate regular
expressions using derivatives [2,18], which also have been used for timed regular

434
D. Basin et al.
Fig. 1. Syntax and semantics of MFOTL as presented in [45], with additions in gray
expressions [47]. Quantiﬁed regular expressions [1,34] extend regular expressions
with data and aggregations. They can be evaluated eﬃciently, but can neither
express metric constraints nor future modalities directly.
2
A Veriﬁed Monitor for Metric First-Order Temporal
Logic
VeriMon [45] is a formally veriﬁed monitor for a large fragment of MFOTL [8].
The monitor takes an MFOTL formula, which may be open, and incrementally
processes an inﬁnite stream of time-stamped events. It outputs for every stream
position the set of variable assignments that satisfy the formula. Thus, the mon-
itor can be used to extract data from the stream. Typically, one is interested
in the violations of a property speciﬁed as an MFOTL formula, which can be
obtained by monitoring the negated formula.
We give an overview of MFOTL and VeriMon. We also cover some of the
smaller additions in our new monitor, VeriMon+, highlighted in gray. For read-
ability, we liberally use abbreviations and symbolic notation, departing mildly
from Isabelle’s syntax.
Figure 1 shows MFOTL’s syntax and semantics. Events have a name (string)
and a list of parameters of type data. In VeriMon+, data is a disjoint union
of integers, double-precision ﬂoats, and strings. Multiple events are grouped
together into a database (db) if they are considered to occur simultaneously.
We call an inﬁnite stream of databases, augmented with their corresponding
time-stamps, an event stream or trace. Time-stamps (ts) are modeled as natural
numbers (nat). We write T σ i to denote the time-stamp of the ith database
Γ σ i of the event stream σ. The predicate trace expresses that the time-stamps

A Formally Veriﬁed, Optimized Monitor for MFODL
435
are monotone, i.e., T σ i ≤T σ (i+1) for all i ≥0, and always eventually strictly
increasing, i.e., ∀t. ∃i. t < T σ i. Consecutive time-points i can have the same
time-stamp.
Terms and formulas are represented by the datatypes trm and frm, respec-
tively. Our formalization uses de Bruijn indices for free and bound variables
(constructor V). In examples, we prefer the standard named syntax (and omit
V). The type I models nonempty, possibly unbounded intervals over nat. We
write n ∈I I for n’s membership in I, and [a, b] for the unique interval satisfying
n ∈I [a, b] iﬀa ≤n ≤b. The right bound b is of type enat, i.e., either a natural
number or inﬁnity ∞for an unbounded interval.
The functions etrm and sat (Fig. 1) deﬁne MFOTL’s semantics. Both take a
variable assignment v, which is a list of type data list whose ith element v ! i
is the value assigned to the variable with index i. The function etrm evaluates
terms under a given assignment. The expression sat σ v i ϕ is true iﬀthe formula
ϕ is satisﬁed by v at time-point i in the trace σ. VeriMon+ adds arithmetic
operators and type conversions to terms, as well as the predicates ≺and ⪯.
Their semantics on data is lifted from the corresponding operations on integers,
ﬂoats, and strings, whenever they are meaningful. The ordering ≤on data is
total: strings are compared lexicographically and Int i < Flt f < Str s.
VeriMon computes sets of satisfactions (i.e., satisfying assignments) by recur-
sion over the formula’s structure. It represents these sets as ﬁnite tables, to which
it applies standard relational operations such as the natural join (▷◁) and union.
Tables are sets of tuples, which are lists of optional data values; missing values
are denoted by ⊥. This representation allows us to use tuples with the same
length across subformulas with diﬀerent free variables. The predicate wf_tuple
deﬁnes the well-formed tuples for a given length n and a set of variables V . We
also refer to V as the columns of a tuple (or table).
deﬁnition wf_tuple :: nat ⇒nat set ⇒tuple ⇒bool where
wf_tuple n V v = (length v = n ∧(∀x < n. v ! x = ⊥←→x /∈V ))
The set of satisfactions may be inﬁnite. VeriMon supports only a fragment of
MFOTL for which all computed tables are ﬁnite. The predicate safe (omitted)
deﬁnes the monitorable fragment [45]. It accepts only certain combinations of
operators and constrains the free variables of subformulas. Also, the intervals of
all U operators must be bounded.
VeriMon’s interface consists of two functions init :: frm ⇒mstate and step ::
db × ts ⇒mstate ⇒(nat × table) list × mstate. The former initializes the
monitor’s state, and the latter updates it with a new time-stamped database
to report any new satisfactions. We require that satisfactions are reported for
every time-point and in order. Note that a formula containing a future operator
such as U cannot necessarily be evaluated at time-point i after observing the ith
database. Therefore, the output for several time-points may become available at
once, so step returns a list of pairs of time-points and tables.
We describe the evaluation of αS[a, b]β in more detail. This formula is equiva-
lent to the disjunction of αS[c, c] β for all c such that a ≤c ≤b. Suppose that the
most recent time-point is i with time-stamp τ. The monitor’s state for α S[a, b] β

436
D. Basin et al.
Fig. 2. Simpliﬁed state of a Since operator and its update
consists of a list of tables Tc with the satisfactions of α S[c, c] β at time-point i,
along with the corresponding time-stamps τ −c. VeriMon also stores the satis-
factions Tc (and time-stamps) for 0 ≤c < a, which are not yet in the interval.
Figure 2 (left) depicts a state, where we assume for simplicity that we store a
table for every time-stamp between τ −b and τ. (In reality, time-stamps not in
the trace do not have a corresponding entry in this list.) The state is updated
for every new time-point with time-stamp τ ′, for which we already know the
satisfactions Rα and Rβ of the subformulas α and β. In Fig. 2, we distinguish
whether τ ′ equals τ (otherwise τ ′ > τ by monotonicity). The update consists
of three steps: (1) remove tables that fall out of the interval; (2) evaluate the
conjunction of each remaining table with Rα using a relational join; and (3) add
the new tuples from Rβ, either by inserting them into the most recent table T0
or by adding a new table, depending on whether τ ′ equals τ. Finally, we take
the union of all tables within the interval to obtain the satisfactions of α S[a,b] β.
We summarize VeriMon’s correctness, which we also prove for VeriMon+.
It relates the monitor’s implementation to its speciﬁcation verdicts :: frm ⇒
(db ×ts) list ⇒(nat ×tuple) set, which deﬁnes the expected output on a stream
preﬁx. The ﬁrst result shows that verdicts characterizes an MFOTL monitor,
where preﬁx π σ means that π is a preﬁx of σ, and map the v converts v to an
assignment by mapping ⊥to an unspeciﬁed value.
Lemma 1 ([45], Lemma 2). Suppose that safe ϕ is true. Then, verdicts ϕ is
sound and eventually complete, i.e., for all preﬁxes π of trace σ, time-points i,
and tuples v,
(a) (i, v) ∈verdicts ϕ π −→sat σ (map the v) i ϕ, and
(b) i < length π ∧wf_tuple (nfv ϕ) (fv ϕ) v ∧(∀σ′. preﬁx π σ′ −→sat σ′ (map
the v) i ϕ) −→(∃π′. preﬁx π′ σ ∧(i, v) ∈verdicts ϕ π′).
Above, nfv ϕ is the smallest number larger than all free variables of ϕ, written
fv ϕ. The next result establishes the implementation’s correctness using the state
invariant wf_mstate :: frm ⇒(db × ts) list ⇒mstate ⇒bool (omitted). Let set
convert lists into sets, last_ts π be the last time-stamp in π, and π1 @ π2 be the
concatenation of π1 and π2.

A Formally Veriﬁed, Optimized Monitor for MFODL
437
Theorem 1 ([45], Theorem 1). The initialization init establishes the invariant
and the update step preserves the invariant and its output can be described in
terms of verdicts:
(a) If safe ϕ, then wf_mstate ϕ [ ] (init ϕ).
(b) Let step (db, τ) mst = (A, mst′). If wf_mstate ϕ π mst and last_ts π ≤τ,
then ((i, V ) ∈set A. {(i, v) | v ∈V }) = verdicts ϕ (π @ [(db, τ)]) −
verdicts ϕ π and wf_mstate ϕ (π @ [(db, τ)]) mst′.
3
Aggregations
Basin et al. [7] extended MFOTL with a generic aggregation operator. This oper-
ator was inspired by the group-by clause and aggregation functions of SQL. It
ﬁrst partitions the satisfying assignments of its subformula into groups, and then
computes a summary value, such as count, sum, or average, for each group. We
formalized the aggregation operator’s semantics, added an evaluation algorithm
to VeriMon+, and proved its correctness.
Consider the formula s ←Sum x; x. P(g, x). The aggregation operator s ←
Sum x; x has four parameters: a result variable (s), the aggregation type (Sum),
an aggregation term (ﬁrst x), and a list of variables that are bound by the
operator and thus excluded from grouping (second x). When evaluated, the
above formula yields a set of tuples (s, g). There is one such tuple for every
value of g with at least one P event that has g’s value as its ﬁrst parameter. The
values of g partition the satisfactions of P(g, x) into groups. For every group,
the sum over the values of x in that group is assigned to the variable s.
We added the constructor nat ←agg_op trm; nat. frm to frm. Consider the
instance y ←Ω t; b. ϕ. The operator binds b variables simultaneously in the
formula ϕ and in the term t, over which we aggregate. In examples, we list the
bound variables explicitly instead of writing the number b. The remaining free
variables (possibly none) of ϕ are used for grouping. The variable y receives the
result of the aggregation operation Ω = (ω, d), where ω is one of Cnt (count),
Min, Max, Sum, Avg (average), or Med (median). The default value d, which
we usually omit, determines the result for empty groups (e.g., 0 for Cnt). The
formula’s free variables are those of ϕ excluding the b bound variables, plus y.
Figure 3 shows the semantics of the aggregation operator y ←Ω t; b. ϕ.
The assignment v determines both a group and a candidate value v ! y for the
aggregation’s result on that group. The sat function checks whether the value
is correct. First, it computes the set M, which encodes a multiset in the form
of pairs (x, c), where c is x’s multiplicity. This multiset contains the values of
the term t under all assignments z @ v that satisfy ϕ, where z is an assignment
to the bound variables. The expression card∞Z stands for the cardinality of Z
when it is ﬁnite, and ∞otherwise. Then, sat compares v ! y to the result of the
aggregation operation Ω on M, which is given by agg_op Ω M (omitted).
We extended the safe predicate with suﬃcient conditions that describe when
the aggregation formula y ←Ω t; b. ϕ has ﬁnitely many satisfactions. We require
that ϕ satisﬁes safe, that the variable y is not free in ϕ excluding the b bound
variables, and that all bound variables and the variables in t occur free in ϕ. We

438
D. Basin et al.
Fig. 3. Semantics and evaluation of the aggregation operator
adopted the convention [7] that an aggregation formula is not satisﬁed when M is
empty, unless all free variables of ϕ are bound by the operator. Otherwise, there
would be inﬁnitely many groups (and hence, satisfactions) with the aggregate
value agg_op Ω {}, assuming that ϕ is safe.
Figure 3 also deﬁnes eval_agg, which evaluates the aggregation operator. It
takes a table R with ϕ’s satisfactions, and returns a table with the aggregation
operator’s satisfactions. The ﬁrst argument n controls the length of the tuples
in the tables (Sect. 2). The argument g0 speciﬁes whether all free variables of ϕ
are bound by the operator. The remaining arguments y, Ω, b, and t are those of
the operator. We write f ‘ X for the image of X under f.
In eval_agg, we ﬁrst check whether g0 ∧R = {} is true to handle the special
case mentioned above. (The expression singleton_table n y a is a table with a
single tuple of length n that assigns a to variable y.) Otherwise, we compute the
aggregate value separately for each group k. The set of groups is obtained by
discarding the ﬁrst b values of each tuple in R. To every group k, we apply the
lambda-term to augment the tuple with the aggregate value. The set G contains
all tuples in the group. Note that these tuples extend k with assignments to the
b bound variables. Then, we compute the image of G under the term t, which
is evaluated by meval_trm :: trm ⇒tuple ⇒data (omitted). Finally, we obtain
the multiset M by counting how many tuples in G map to each value in the
image.
4
Regular Expressions
VeriMon+ extends VeriMon’s language by generalizing MFOTL’s temporal oper-
ators to regular expressions. The resulting metric ﬁrst-order dynamic logic
(MFODL) can be seen [24, §3.16] as the “supremum” (in the sense of combin-
ing features) of metric dynamic logic (MDL) [12] and MFOTL [8]. Peycheva’s
master’s thesis [40] develops a monitor for past-only MFODL. We give the ﬁrst
formal deﬁnition of MFODL with past and future operators. We also deﬁne a
fragment whose formulas can be evaluated using ﬁnite relations (Sect. 4.1). This
fragment guides our evaluation algorithm’s design (Sect. 4.2).
Figure 4 (left) deﬁnes the syntax and semantics of our variant of regular
expressions. The type re is parametrized by a type variable 'a, which is used

A Formally Veriﬁed, Optimized Monitor for MFODL
439
Fig. 4. Syntax and semantics of MFODL (left) and conversion of MFOTL into MFODL
(right)
in the _? constructor. The semantics is given by match and assigns to each
expression a binary relation (⊗) on natural numbers. Intuitively, a pair (i, j) is
in the relation assigned to r when r matches the portion of a trace from i to
j. The trace notion is abstracted away in match via the argument test, which
indicates whether a parameter of type 'a may advance past a given point.
In more detail, the wildcard operator ⋆k matches all pairs (i, j), where j =
i + k; we write ⋆for the useful special case ⋆1. The test x? only matches pairs of
the form (i, i) that pass test i x. The semantics of alternation (+) as union (∪),
concatenation ( · ) as relation composition (•), and Kleene star (_∗) as reﬂexive-
transitive closure (_∗) is standard.
Figure 4 (left) also shows frm’s extension with two constructors that use
regular expressions. The regular expression’s parameter nests a recursive occur-
rence of frm, i.e., our regular expressions’ leaves are formulas, which in turn may
further nest regular expressions, and so on. MDL’s syntax is often presented as
a mutually recursive datatype [12]. Our nested formulation is beneﬁcial because
it lets us formalize regular expressions independently, for use in diﬀerent appli-
cations (e.g., monitors for MDL and MFODL).
In terms of their semantics, the two new operators naturally generalize the
SI and UI operators. The past match operator
I r is satisﬁed at i if there
is an earlier time-point j subject to the same temporal constraint I as in the
satisfaction of SI and moreover the regular expression r matches from j to i. For
the future match operator
I r, the situation is symmetric with the existentially
quantiﬁed j being a future time-point. In both cases, the test parameter of match
is recursively instantiated with the satisfaction predicate sat.
We can embed MFOTL into MFODL by expressing the temporal operators
using semantically equivalent formulas built from regular expressions (Fig. 4,
right). Thus, we could in principle remove the operators , S, , and U from
frm and use regular expressions instead. We prefer to keep these operators in
frm as this allows us to optimize their evaluation in a way that is not available
for the more general match operators (Sect. 6).

440
D. Basin et al.
Fig. 5. Safety conditions for MFODL
Fig. 6. Simpliﬁed state of a past match operator and its update
We conclude MFODL’s introduction with an example. Many systems for user
authentication follow a policy like: “A user should not be able to authenticate
after entering the wrong password three times in a row within the last 10 min-
utes.” We write ✗(u) for the event “User u entered the wrong password” and ✓(u)
for “User u has successfully authenticated.” Additionally, we abbreviate ϕ? · ⋆
by ϕ. (This abbreviation is only used when ϕ appears in a regular expression
position, e.g., as an argument of ·). Then the formula
✓(u) ∧
[0,600]

✗(u) · (¬✓(u))∗· ✗(u) · (¬✓(u))∗· ✗(u) · (¬✓(u))∗
expresses this policy’s violations: its satisfying assignments are precisely the users
that successfully authenticate after entering wrong credentials for three times in
the last 600 seconds, without intermediate successful authentications. We can
express this property in MFOTL using three nested S operators, one for each of
the ✗(u) subformulas. Yet, it is unclear which intervals to put as arguments to S
beyond the fact that they should sum up to 600. The rather impractical solution
exploits that there are only ﬁnitely many ways to split the intervals due to their
bounds being natural numbers and constructs the disjunction of all possible
splits (180 901 in this case). MFODL remediates this infeasible construction.

A Formally Veriﬁed, Optimized Monitor for MFODL
441
4.1
Finitely Evaluable Regular Expressions
Following MonPoly’s design [8], VeriMon+ represents all sets of satisfying assign-
ments with ﬁnite tables. The databases occurring in the trace are all ﬁnite, yet
their combination may not be. Therefore, MonPoly and VeriMon+ work with
syntactic restrictions that ensure that all sets that arise are ﬁnite. For example,
negation must occur under a conjunction α ∧¬β, where the free variables of β,
written fv β, are contained in those of α. We say that ¬β is guarded by α and
compute α∧¬β as the anti-join (▷) of the corresponding tables. For disjunctions
α ∨β, we must have fv α = fv β. Similar restrictions also apply to temporal
operators: to evaluate α SI β and α UI β we require fv α ⊆fv β.
We derive a new suﬃcient criterion for match operators to have ﬁnitely many
satisfying assignments. To develop some intuition, we ﬁrst consider several exam-
ples that result in inﬁnite tables. The ﬁrst example is any expression with a
Kleene star as the topmost operator. The formula ϕ =
[0,b] (r∗) is satisﬁed at
all points i for all assignments v (regardless of r’s free variables) since 0 ∈I I
and any (i, i) matches r∗. Thus, when we evaluate ϕ at i, we can choose i as
the witness for the existential quantiﬁer in the deﬁnition of sat. It follows that
Kleene stars must be guarded by a ﬁnite table.
The union of two ﬁnite tables is ﬁnite only if the tables have the same columns
(assuming an inﬁnite domain data). This explains the requirement for the sub-
formulas of ∨to have the same variables, but a similar requirement is needed
for the + of regular expressions. Perhaps more surprisingly, concatenation can
also hide a union: consider ϕ =
[0,b] (r · s∗) and assume that s matches (j, i)
for some j < i. By the semantics of concatenation, we can split the satisfactions
of ϕ into those that use s∗’s matching pair (i, i) (i.e., the satisfying assignments
of
I r at i) and those that do not. To combine these assignments it seems nec-
essary to take the union of the satisfaction of ϕ and
I r, which in turn requires
these formulas to have the same free variables, or equivalently fv s ⊆fv r (over-
loading notation to apply fv to regular expression). The future match operator
behaves symmetrically, requiring the side condition fv r ⊆fv s for
[0,b] (r∗· s).
MonPoly also allows the left subformula of S and U to be negated: (¬α)
SI β
and
(¬α) UI β.
Hence,
we
should
support
the
MFODL
vari-
ants
I (β? · (⋆· (¬α)?)∗) and
I (((¬α)? · ⋆)∗· β?), but also generalize these
patterns to ﬂexibly support negated tests.
Our solution to these issues comprises the predicates shown in Fig. 5. The
safe predicate on regular expressions is parametrized by two ﬂags: context distin-
guishing whether the expression occurs under a past or a future match operator
and mode determining whether the tests may be negated and other safety con-
ditions relaxed. The most interesting cases are those for concatenation. There,
in addition to the fv side conditions, only one argument is checked recursively in
the same mode as the overall expression. The other argument is checked using
the lax mode, in which side conditions are skipped, except for the requirement
that (possibly negated) formulas under the test operators are safe. The context
parameter dictates which argument keeps, and which changes, the mode.

442
D. Basin et al.
Fig. 7. The core evaluation functions for MFODL
4.2
Evaluation Algorithm
The evaluation algorithm’s structure for the past match operator
I r (Fig. 6)
closely resembles the evaluation of α SI β (Fig. 2). What is diﬀerent is the data
that is stored for each time-stamp and the way we update it. For S, each stored
table Tc corresponds to the satisfactions of α S[c, c] β. For
I r, each Tc is a
mapping from a regular expression s to the table denoting the satisfactions
of
[c, c] s. (We represent mappings here by plain functions for readability.)
Clearly, this mapping’s domain must be ﬁnite. We restrict it to the ﬁnite set
Δ(r) of right partial derivatives [2,12] of the overall regular expression r, which
correspond to the states of a non-deterministic automaton that matches r from
right to left.
Partial derivatives allow us to extend satisfactions of
[c, c] s for s ∈Δ(r)
at time-point i to satisfactions of
[c+(τi+1−τi), c+(τi+1−τi)] s for s ∈Δ(r) at
time-point i + 1. The Since operator’s counterpart of this extension is the join
with Rα, the new satisfactions of α, which is performed for all Tcs for every
update. Here, the extension function δR inputs a function R assigning the new
satisfactions for all tests occurring in r (possibly with a negation stripped) and
updates the mapping Tc. It is deﬁned as δR T = (λs. δ id R T s) where δ is
deﬁned recursively on the structure of regular expressions as shown in Fig. 7.
The ﬁrst parameter of δ uses continuation passing style. It builds up a regular
expression context that we use when evaluating the leaves. It is thus guaranteed
that if we apply δ to any regular expression s ∈Δ(r), all calls to T will apply T
to some s′ ∈Δ(r).
The function δ uses the recursive function εlax in its deﬁnition. This function
computes the assignments that give rise to matches of the form (i, i) under the
assumption that a guard (in form of the table X) is given. For δ, the recursive
call acts as εlax’s guard.
The function εstrict is used to update the state with satisfying assignments at
the newly added time-point (Fig. 6). It is only speciﬁed for expressions satisfying

A Formally Veriﬁed, Optimized Monitor for MFODL
443
Fig. 8. Multi-way join algorithm
safe past strict and uses εlax for subexpressions that only satisfy safe past lax.
The recursive structure of εstrict and εlax follows the one of safe past. We write
εn
R = (λr. εstrict n R r) and use ∪to denote the pointwise union of mappings
in Fig. 6. The Since operator’s counterpart of this update is the addition of the
satisfactions for the subformula β (Fig. 2).
The above description just sketches our evaluation algorithm and our formal-
ization provides full details. Our proofs establish the monitor’s overall correct-
ness, which amounts to the same statement as Theorem 1 but now covers the
syntax and semantics extended with the match operators (and aggregations). In
particular, the formalization also includes the future match operators for which
the evaluation uses similar ideas (partial derivatives), but in a symmetric fashion
following the deﬁnition of safe future.
5
Multi-way Join
The natural join ▷◁is a central operation in ﬁrst-order monitors. Not only is
it used to evaluate conjunctions; temporal operators also crucially rely on it.
Despite this operation’s importance, both MonPoly and VeriMon naively com-
pute A ▷◁B as nested unions:  v ∈A.  w ∈B. ⌈join1 (v, w)⌉, where join1
joins two tuples v and w if possible, and ⌈_⌉converts the optional result into a
set. In this section, we describe a recent development from database theory that
we formalize and extend to optimize the computation of joins.
Ngo et al. [37] and Veldhuizen [48] have developed worst-case optimal multi-
way join algorithms that compute the natural join of multiple tables. Here,
optimality means that the algorithm never constructs an intermediate result
that is larger than the maximum size of all input tables and the overall output.
This strictly improves over any evaluation plan using binary joins: There are
tables A, B, and C such that the size of A ▷◁B ▷◁C is linear in |A| = |B| = |C|,
but any plan constructs a quadratic intermediate result from the binary join it

444
D. Basin et al.
evaluates ﬁrst [39, Fig. 2]. The key idea of the multi-way join is to build the
result table column-wise, adding one or more columns at a time, while taking all
tables that refer to the currently added columns into account. All intermediate
results are restrictions of the overall result to the processed columns, and thus
not larger than the overall result.
Figure 8 shows our formalization of the multi-way join algorithm following
Ngo et al.’s uniﬁed presentation [39] but generalizing it to support anti-joins ▷;
these additions are highlighted in gray. A query is a set of atables, i.e., tables
annotated with the columns (represented by nat) they have. The main function,
generic_join, takes as input a set of columns V and two queries Qpos and Qneg.
It computes the multi-way join of Qpos while subtracting the tuples of tables in
Qneg. For example, generic_join {a, b, c, d} {({a, b}, A), ({b, c}, B), ({c, d}, C)}
{({d}, D), ({a, c}, E)} computes A ▷◁B ▷◁C ▷D ▷E.
The algorithm proceeds by recursion on V . The base case in which V is empty
or a singleton set is evaluated directly using intersections and unions. We ﬁrst
describe the recursive structure of the original algorithm [39], obtained by ignor-
ing the highlighted anti-join additions in Fig. 8. The algorithm is parametrized by
the getIJ function, which partitions V into two nonempty sets I and J that each
determine the number of columns and the order in which they are added. Ngo
et al. [39] show how diﬀerent multi-way join algorithms [37,48] can be obtained
by using speciﬁc instances of getIJ. We use a heuristic to pick ﬁrst the column i
that maximizes the number of tuples in Qpos it aﬀects (by setting I = {i}). The
partitioning only aﬀects performance, not correctness.
Once I and J are ﬁxed, the algorithm constructs a reduced query QI
pos by
focusing on tables that have a column in I. Furthermore, it restricts their columns
to I via the overloaded notation _ ↓I, which denotes the restriction of tuples
(by setting the optional data values for columns outside I to ⊥[45]), annotated
tables, and queries (Fig. 8).
Next, QI
pos is evaluated recursively, yielding table AI with columns I. We
now consider tables that have a column in J. This yields a second reduced query
QJ
pos, which is, however, not restricted to J. Keeping the columns I in QJ
pos
allows us to focus on tuples in QJ
pos that match some t ∈AI, i.e., coincide with t
for all values in columns I. The function extend performs this matching. For each
tuple t ∈AI, it creates the query extend J QJ
pos (I, t) consisting of tables from
QJ
pos restricted to t-matching tuples (in database terminology this is a semi-join)
further restricted to columns J. These queries are again solved recursively, each
resulting in a table At with columns J. The ﬁnal step consists of merging the
tuples t with At. Since t and A have disjoint columns I and J, the function call
join1 (v, t) will return some result (which we extract via the) for all v ∈A.
We extend the algorithm to support anti-joins by introducing a second query
Qneg, which we think of as being negated. It is not possible to split Qneg’s tables
column-wise. Instead, our generalization processes tables with columns U from
Qneg once the positive query has accumulated a superset of U as its columns.
This is an improvement over the naive strategy of computing Qpos ﬁrst and only
then removing tuples from it.

A Formally Veriﬁed, Optimized Monitor for MFODL
445
The correctness of generic_join relies on several side conditions, e.g., no input
table may have zero columns and V must be the union of the columns in the
positive query. A wrapper function mwjoin takes care of these corner cases, e.g.,
by computing V from Qpos and Qneg. We omit mwjoin’s straightforward deﬁni-
tion, but show its correctness property (which only diﬀers from generic_join’s
correctness by having fewer assumptions):
Qpos ̸= {} ∧(∀(V, A) ∈Qpos ∪Qneg. (∀v ∈A. wf_tuple n V v) ∧(∀x ∈V. x < n)) −→
z ∈mwjoin Qpos Qneg ←→wf_tuple n ((V, _) ∈Qpos. V ) z ∧
(∀(V, A) ∈Qpos. z ↓V ∈A) ∧(∀(U, B) ∈Qneg. z ↓U /∈B)
In words: whenever Qpos is nonempty and all tables in Qpos and Qneg ﬁt their
declared columns, a tuple z belongs to the output of mwjoin iﬀit has the correct
columns and matches all positive tables from Qpos and does not match any
negative ones from Qneg.
The multi-way join algorithm is integrated in VeriMon+ by adding a new
constructor Ands :: frm list ⇒frm to the formula datatype. At least one of the
subformulas of Ands must be non-negated, and the columns of the negative sub-
formulas must be a subset of the positive ones. Since MonPoly’s parser, which
we reuse in VeriMon+, generates formulas with binary conjunctions, we have
deﬁned a semantics-preserving preprocessing function convert_multiway (omit-
ted), which rewrites nested binary conjunctions into Ands.
6
Sliding Window Algorithm
To evaluate the temporal operators S and U, VeriMon computes the union of
tables that are associated with time-stamps within the operator’s interval. These
sets of time-stamps often overlap between consecutive monitor steps. The sliding
window algorithm (SWA) [10] is an eﬃcient algorithm for combining the elements
of overlapping sequences with an associative operator. It improves over the naive
approach that recomputes the combination (here, the union) from scratch for
every sequence. MonPoly uses SWA for the special cases ♦Iβ = TT SI β and
♦Iβ = TT UI β, where TT = ∃x. x ≈x. However, SWA was not designed for the
evaluation of arbitrary S and U operators. For these, the tables in the sequence
must be joined with the left subformula’s results in every monitor step. In a
separate work [27,28], we formally veriﬁed SWA’s functional correctness (but
not its optimality) and extended it with a join operation to support arbitrary S
and U operators.
SWA is overly general: it supports any associative operator, not just the
union of tables. We conjecture that the generic SWA algorithm is not optimal
in the special case needed for S and U. To optimize the evaluation of the S and
U operators in VeriMon+, we abstracted the individual steps of their evaluation
in one locale for each of them (Sect. 6.1). We then instantiated the locales with
specialized sliding window algorithms (Sect. 6.2). Due to space limitations, we
only describe the optimization for the Since operator here.

446
D. Basin et al.
Fig. 9. The locale for evaluating the Since operator (assumptions omitted)
6.1
Integration into the Monitor
Recall the evaluation of Since in VeriMon (Sect. 2). First, VeriMon updates the
operator’s state with a new time-stamp τ ′ and the satisfactions Rα and Rβ for
the subformulas α and β. Second, it evaluates the state to obtain the satisfactions
for α SI β.
Let mslist denote the type of the S operator’s state in VeriMon. In Veri-
Mon+, we deﬁne a locale msaux that abstracts the update and evaluation of an
optimized state 'msaux and relates the optimized state to VeriMon’s original
mslist state (Fig. 9). We provide additional constant arguments for evaluating
the S operator in a record args. It consists of the S operator’s interval, the argu-
ments to wf_tuple characterizing the satisfactions of the two subformulas, and a
Boolean value denoting whether the left subformula occurs negated. The pred-
icate valid_msaux relates an optimized state to VeriMon’s state with respect
to the given args and a current time-stamp. The function init_msaux returns
an initial optimized state. The next three functions add_new_ts, join_msaux,
and add_new_table correspond to the three steps in which VeriMon’s state is
updated (Sect. 2), except that now they act on the optimized state. Finally,
result_msaux evaluates the optimized state to obtain the satisfactions of the S
operator at the current time-point. The omitted locale assumptions state that
all operations preserve valid_msaux and that result_msaux returns the union
computed on any VeriMon state related by valid_msaux.
6.2
The Specialized Algorithm
VeriMon’s state for the S operator consists of a list of tables Tc with the satisfac-
tions of formulas α S[c, c] β, along with the corresponding time-stamps. VeriMon
stores the satisfactions Tc (and time-stamps) for all c that do not exceed the
interval’s upper bound.
In our optimized state, we partition the list of tables Tc into a list data_prev
for time-stamps that are not yet in the interval and a list data_in for time-stamps
that are already in the interval. The state also contains a mapping tuple_in that
assigns to each tuple occurring in some table Tc in the interval the latest time-
stamp in the interval for which this tuple occurs in the respective table. Finally,
the state contains a mapping tuple_since that assigns to each tuple occurring
in some table Tc in the entire state the earliest time-stamp for which this tuple
occurs in the respective table. (For eﬃciency, we delete tuples from tuple_since

A Formally Veriﬁed, Optimized Monitor for MFODL
447
ts db
step
data_prev
data_in
tuple_in tuple_since
init_msaux
[]
[]
{}
{}
1
{Q(a),Q(b),
Q(c)}
add_new_table [(1,{a,b,c})] []
{}
{a →1,b →1,
c →1}
2
{P(b),P(c)}
join_msaux
[(1,{a,b,c})] []
{}
{b →1,c →1}
add_new_table [(1,{a,b,c}),
(2,{})]
[]
{}
{b →1,c →1}
3
{P(b),P(c),
Q(a),Q(b)}
add_new_ts
[(2,{})]
[(1,{a,b,c})] {b →1,
c →1}
{b →1,c →1}
add_new_table [(2,{}),
(3,{a,b})]
[(1,{a,b,c})] {b →1,
c →1}
{a →3,b →1,
c →1}
7
{P(a)}
add_new_ts
[]
[(3,{a,b})]
{a →3,
b →3}
{a →3,b →1,
c →1}
join_msaux
[]
[(3,{a,b})]
{a →3} {a →3}
add_new_table [(7,{})]
[(3,{a,b})]
{a →3} {a →3}
Fig. 10. An example of updating the optimized state for the formula P(x) S[2,4] Q(x)
lazily, i.e., only at deﬁned garbage collection points, such that the mapping may
even contain tuples from some Tc that already has fallen out of the interval.)
The state is initialized via init_msaux to consist of empty lists and empty
mappings. The function add_new_ts drops tables from data_in that fall out of
the interval based on a newly received time-stamp. It also removes those tuples
from tuple_in whose latest occurrence (which is stored in this mapping) has
fallen out of the interval. Then it moves tables that newly enter the interval
from data_prev to data_in, and updates the tuples from these moved tables
in tuple_in to the most recent time-stamp τ for which they now occur in the
interval, but only if tuple_since maps the tuple to a time-stamp that is at most
τ.
The function join_msaux only modiﬁes the mappings tuple_since and
tuple_in by removing tuples that are not matched by any tuple in the given
table Rα. The function add_new_table appends the new table Rβ to data_prev
(or directly data_in, if 0 ∈I I), adds the tuples from Rβ that were not in
tuple_since to that mapping, and, if 0 ∈I I, updates the tuples from Rβ in the
mapping tuple_in to the current time-stamp. Finally, result_msaux returns the
keys of the mapping tuple_in, in particular without computing any unions. In
other words, tuple_in contains precisely the tuples that are in the interval and
have not been removed by joins. Crucially, and unlike in VeriMon’s state, the join
operation does not change the tables Tc in our optimized state. This function-
ality is implemented more eﬃciently by ﬁltering the two mappings tuple_since
and tuple_in.
Example. Figure 10 shows how the optimized state for the formula P(x) S[2,4]
Q(x) is updated. In total, four time-points are processed. The ﬁrst two columns
show the time-stamp and database for each time-point. The other columns show

448
D. Basin et al.
the state after applying the step named in the third column. Each step cor-
responds to a function in the msaux locale. The satisfactions {}, {}, {b, c}, {a}
returned by result_msaux can be read oﬀfrom the mapping tuple_in after each
time-point’s last step. We omit steps that do not change the state.
The ﬁrst row shows the initial state. For the ﬁrst time-point, the steps
add_new_ts with time-stamp 1 and join_msaux with the table {} (as there are
no P events) do not change the initial state. Then, add_new_table appends the
table {a, b, c} with the parameters of the Q events to data_prev (as 0 ̸∈I [2, 4])
and adds its elements to tuple_since.
For the second time-point, VeriMon+ applies add_new_ts with time-stamp
2. Again this step has no eﬀect: data_prev’s ﬁrst entry is not moved to data_in
as the diﬀerence 2−1 to the current time-stamp is not in [2, 4]. Next, join_msaux
with the table {b, c} (from the P events) removes a from tuple_since, but not
from data_prev. Finally, add_new_table appends the table {} (as there are no
Q events) to data_prev.
For the third time-point, add_new_ts moves data_prev’s ﬁrst entry to
data_in because the time-stamp diﬀerence 3−1 is in [2, 4]. The values b, c of that
entry are added to tuple_in because tuple_since maps them to a time-stamp ≤1.
Note that a is not added, as it is not contained in tuple_since. The join_msaux
step with the table {b, c} does not change the state. The add_new_table step
appends {a, b} to data_prev. Now, a is added to tuple_since, whereas b is already
contained in tuple_since and its value is not updated.
When the fourth time-point is processed, the ﬁrst two observed time-stamps
fall out of the interval and add_new_ts discards their entries from data_prev
and data_in, and their values from tuple_in but not from tuple_since. As before,
the last table {a, b} in data_prev is moved to data_in and its elements are
added to tuple_in. As time has progressed by more than the upper bound of
the interval [2, 4], join_msaux triggers garbage collection, which removes the
key c from tuple_since. The join operation further removes b from tuple_in and
tuple_since. Finally, add_new_table appends {} to data_prev.
7
Evaluation
We perform two kinds of experiments. First, we carry out diﬀerential testing [35]
of VeriMon+ against three (unveriﬁed) state-of-the-art monitors: MonPoly [9],
Aerial [11], and Hydra [43]. Second, we compare VeriMon+’s performance to
these monitors on representative formulas. VeriMon+ reuses MonPoly’s log and
formulas parsers and user interface. The veriﬁed monitor’s code extracted from
Isabelle is integrated with these unveriﬁed components in about 170 lines of
unveriﬁed OCaml code. Our implementation and our experiments are avail-
able [5]. Of the above monitors, only VeriMon+ supports full MFODL. MonPoly
supports a monitorable fragment of MFOTL with bounded future operators and
aggregations. Aerial and Hydra support the propositional fragment of MFODL.
Diﬀerential Testing. To validate the results produced by unveriﬁed monitors,
we generate random stream preﬁxes and formulas, invoke the monitors, and

A Formally Veriﬁed, Optimized Monitor for MFODL
449
compare their results to VeriMon+’s. For this purpose, we developed a random
stream and formula generator. It takes as parameters the formula size (in terms of
number of operators) and the number of free variables that occur in the formula.
The generator can be conﬁgured to generate formulas within the fragments of
MFODL supported by the diﬀerent monitors we evaluate.
Our tests uncovered several classes of inputs where MonPoly’s output devi-
ated from VeriMon+’s. Here, we show one example and refer to our extended
report [6] for a comprehensive overview. Namely, formulas of the form m ←
Ω x; x. ♦Iα, where fv α = {x, y}, Ω ∈{Min, Max}, and 0 /∈I I, were evalu-
ated in MonPoly using a specialized algorithm, which incorrectly updated the
satisfactions of α when they fell out of the interval I.
Aerial’s and Hydra’s output mostly coincided with VeriMon+’s. However,
we noticed that Hydra’s output is not as eager as it could be at the end of the
stream preﬁx. For example,
[1,1] (⋆· (TT ∨
[1,1] ⋆)?) is satisﬁed at time-point
0 of the preﬁx ({}, 0), ({}, 1) due to the existence of time-point 1, where TT can
be evaluated. The subformula
[1,1] ⋆cannot be evaluated at time-point 1. This
prevents Hydra from outputting this verdict at 0.
Performance Evaluation. To assess VeriMon+’s performance, we selected four
formulas, shown in Fig. 11, which exercise the optimizations (multi-way join and
sliding window) and the language features (aggregations and regular expressions)
we have introduced. The formula Star(N) is derived from the star conjunctive
query, commonly used as a benchmark for joins [14]. We use it to evaluate our
multi-way join (for N = 10) and sliding window (for N = 30) implementations.
The formula Top is a commonly used aggregation query, which computes the
most frequently occurring value of the event P’s second parameter. Finally, Alt
checks if events P and Q alternate over the last 10 time units.
We generate random stream preﬁxes with a time span of 60 time units con-
taining events P, Q, and R, each with two integer parameters sampled uniformly
at random from the set {1, 2, . . . , 109}. Our stream generator is parametrized by
the event rate (i.e., by the number of events with the same time-stamp). Since
VeriMon+ reuses MonPoly’s formula and log parsing infrastructure, there is an
additional (conceptually unnecessary) overhead caused by converting the data
structures to match the appropriate interfaces. In cases where the monitoring
task is easy, this becomes the bottleneck and MonPoly performs better than
VeriMon+. To make the monitoring task diﬃcult for Star(10), we sample the
value of the ﬁrst parameter of each event (the common variable x) using the
Zipf distribution. Thus, some parameter values occur frequently. This results in
large intermediate tables, which are problematic for binary joins.
Figure 11 shows that VeriMon+ outperforms MonPoly on the Star(N) for-
mulas. The results conﬁrm the feasibility of monitoring aggregations and regular
expressions with VeriMon+. Specialized algorithms remain more performant on
problems in their domain.

450
D. Basin et al.
Formula
Star(10)
Star(30)
Top
Alt
Monitor MonPoly VeriMon+ VeriMon MonPoly VeriMon+ VeriMon MonPoly VeriMon+ Aerial Hydra VeriMon+
Event rate
50
0.0/6.2
0.1/9.3
0.2/9.5
0.1/6.4
0.2/12.0
3.0/12.4
0.2/8.9
6.0/10.4 0.3/5.8 0.2/3.2
2.1/8.6
100
0.1/7.0
0.2/12.0
0.9/16.5
0.1/7.0
0.3/13.4
10.7/24.2
0.3/10.0 29.9/12.6 0.4/5.8 0.2/3.2
3.4/8.6
200
0.5/9.1
0.3/12.1
6.2/47.4
0.4/9.2
0.7/18.7
50.1/48.5
0.9/9.9
to
0.6/6.0 0.2/3.1
7.2/8.8
500
6.0/9.8
1.3/16.3
so
2.5/12.9
1.9/32.5
so
2.9/13.5
to
1.1/6.3 0.2/3.2 18.0/8.7
1000 38.0/12.8
2.5/22.2
so
11.6/17.9
5.2/58.0
so
10.9/22.4
to
1.7/6.4 0.3/3.1 34.1/8.8
2000
to
5.9/36.5
so
to
11.9/106.7
so
22.0/34.2
to
3.1/6.3 0.4/3.2
to
4000
to
15.0/65.0
so
to
22.8/206.0
so
50.8/62.1
to
5.0/6.5 0.6/3.3
to
Fig. 11. Time (s)/memory (MB) usage of the monitors (to = timeout of 60s, so =
stack overﬂow)
8
Conclusion
We have presented a veriﬁed monitor, competitive with the state-of-the-art, for
the expressive speciﬁcation language metric ﬁrst-order dynamic logic. Our for-
malization comprises roughly 15 000 lines of Isabelle code, distributed over the
four features we presented: regular expressions (2 000), terms and aggregations
(750), multi-way join (3 300), and the sliding window algorithm (3 000). Isabelle
extracts a 7 500 line OCaml program from our formalization. This code includes
eﬃcient libraries representing sets and mappings via red–black trees introduced
transparently into the formalization via the Containers framework [32]. We also
use and extend a formalization of IEEE ﬂoating point numbers [50].
We have made additional contributions from the algorithmic perspective.
Our monitor is the ﬁrst monitoring algorithm for MFODL with aggregations.
Moreover, our specialized sliding window algorithm improves over the existing
generic algorithm [10]. Our usage of multi-way joins in the context of ﬁrst-order
monitoring is also novel, as is our extension of the multi-way join algorithm
to handle anti-joins. It would be interesting to investigate the optimality of
this extension and further consider a multi-way-like evaluation of an arbitrary
Boolean combination of ﬁnite tables.
Our focus was on extending the veriﬁed monitor’s speciﬁcation language and
improving its algorithms. As next steps, we plan to further improve performance
by reﬁning our algorithms to imperative data structures following Lammich’s
methodology [29,30].
Acknowledgment. We thank the anonymous IJCAR reviewers for their helpful com-
ments. This research is supported by the US Air Force grant “Monitoring at Any Cost”
(FA9550-17-1-0306) and by the Swiss National Science Foundation grant “Big Data
Monitoring” (167162). The authors are listed in alphabetical order.
References
1. Alur, R., Fisman, D., Raghothaman, M.: Regular programming for quantitative
properties of data streams. In: Thiemann, P. (ed.) ESOP 2016. LNCS, vol. 9632,
pp. 15–40. Springer, Heidelberg (2016). https://doi.org/10.1007/978-3-662-49498-
1_2

A Formally Veriﬁed, Optimized Monitor for MFODL
451
2. Antimirov, V.M.: Partial derivatives of regular expressions and ﬁnite automaton
constructions. Theoret. Comput. Sci. 155(2), 291–319 (1996). https://doi.org/10.
1016/0304-3975(95)00182-4
3. Barringer, H., Falcone, Y., Havelund, K., Reger, G., Rydeheard, D.: Quantiﬁed
event automata: towards expressive and eﬃcient runtime monitors. In: Gian-
nakopoulou, D., Méry, D. (eds.) FM 2012. LNCS, vol. 7436, pp. 68–84. Springer,
Heidelberg (2012). https://doi.org/10.1007/978-3-642-32759-9_9
4. Bartocci, E., Falcone, Y. (eds.): Lectures on Runtime Veriﬁcation - Introductory
and Advanced Topics. LNCS, vol. 10457. Springer, Cham (2018). https://doi.org/
10.1007/978-3-319-75632-5
5. Basin, D., et al.: VeriMon+: implementation and case study associated with this
paper (2020). https://bitbucket.org/jshs/monpoly/downloads/verimonplus.zip
6. Basin, D., et al.: A formally veriﬁed, optimized monitor for metric ﬁrst-
order dynamic logic (extended report) (2020). https://people.inf.ethz.ch/trayteld/
papers/ijcar20-verimonplus/verimonplus_report.pdf
7. Basin, D., Klaedtke, F., Marinovic, S., Zălinescu, E.: Monitoring of temporal
ﬁrst-order properties with aggregations. Form Methods Syst. Des. 46(3), 262–285
(2015). https://doi.org/10.1007/s10703-015-0222-7
8. Basin, D., Klaedtke, F., Müller, S., Zălinescu, E.: Monitoring metric ﬁrst-order
temporal properties. J. ACM 62(2), 15:1–15:45 (2015). https://doi.org/10.1145/
2699444
9. Basin, D., Klaedtke, F., Zălinescu, E.: The MonPoly monitoring tool. In: Reger,
G., Havelund, K. (eds.) RV-CuBES 2017. Kalpa Publications in Computing, vol.
3, pp. 19–28. EasyChair (2017)
10. Basin, D., Klaedtke, F., Zălinescu, E.: Greedily computing associative aggregations
on sliding windows. Inf. Process. Lett. 115(2), 186–192 (2015). https://doi.org/10.
1016/j.ipl.2014.09.009
11. Basin, D., Krstić, S., Traytel, D.: AERIAL: almost event-rate independent algo-
rithms for monitoring metric regular properties. In: Reger, G., Havelund, K. (eds.)
RV-CuBES 2017. Kalpa Publications in Computing, vol. 3, pp. 29–36. EasyChair
(2017)
12. Basin, D., Bhatt, B.N., Krstić, S., Traytel, D.: Almost event-rate independent
monitoring. Form. Methods Syst. Des. 54(3), 449–478 (2019). https://doi.org/10.
1007/s10703-018-00328-3
13. Bauer, A., Küster, J.-C., Vegliach, G.: From propositional to ﬁrst-order monitoring.
In: Legay, A., Bensalem, S. (eds.) RV 2013. LNCS, vol. 8174, pp. 59–75. Springer,
Heidelberg (2013). https://doi.org/10.1007/978-3-642-40787-1_4
14. Beame, P., Koutris, P., Suciu, D.: Communication steps for parallel query process-
ing. J. ACM 64(6), 40:1–40:58 (2017). https://doi.org/10.1145/3125644
15. Benzaken, V., Contejean, É., Keller, C., Martins, E.: A Coq formalisation of SQL’s
execution engines. In: Avigad, J., Mahboubi, A. (eds.) ITP 2018. LNCS, vol. 10895,
pp. 88–107. Springer, Cham (2018). https://doi.org/10.1007/978-3-319-94821-8_6
16. Blech, J.O., Falcone, Y., Becker, K.: Towards certiﬁed runtime veriﬁcation. In:
Aoki, T., Taguchi, K. (eds.) ICFEM 2012. LNCS, vol. 7635, pp. 494–509. Springer,
Heidelberg (2012). https://doi.org/10.1007/978-3-642-34281-3_34
17. Bohrer, B., Tan, Y.K., Mitsch, S., Myreen, M.O., Platzer, A.: VeriPhy: veriﬁed
controller executables from veriﬁed cyber-physical system models. In: Foster, J.S.,
Grossman, D. (eds.) PLDI 2018, pp. 617–630. ACM (2018). https://doi.org/10.
1145/3192366.3192406
18. Brzozowski, J.A.: Derivatives of regular expressions. J. ACM 11(4), 481–494
(1964). https://doi.org/10.1145/321239.321249

452
D. Basin et al.
19. D’Angelo, B., et al.: LOLA: runtime monitoring of synchronous systems. In:
TIME 2005, pp. 166–174. IEEE Computer Society (2005). https://doi.org/10.1109/
TIME.2005.26
20. Dardinier, T.: Formalization of multiway-join algorithms. Archive of Formal Proofs
(2019). https://isa-afp.org/entries/Generic_Join.html
21. Dardinier, T., Heimes, L., Raszyk, M., Schneider, J., Traytel, D.: Formalization of
an optimized monitoring algorithm for metric ﬁrst-order dynamic logic with aggre-
gations. Archive of Formal Proofs (2020). https://isa-afp.org/entries/MFODL_
Monitor_Optimized.html
22. De Giacomo, G., Vardi, M.Y.: Linear temporal logic and linear dynamic logic on
ﬁnite traces. In: Rossi, F. (ed.) IJCAI 2013, pp. 854–860. IJCAI/AAAI (2013)
23. Havelund, K.: Rule-based runtime veriﬁcation revisited. STTT 17(2), 143–170
(2015). https://doi.org/10.1007/s10009-014-0309-2
24. Havelund, K., Leucker, M., Reger, G., Stolz, V.: A shared challenge in behavioural
speciﬁcation (Dagstuhl Seminar 17462). Dagstuhl Rep. 7(11), 59–85 (2017).
https://doi.org/10.4230/DagRep.7.11.59
25. Havelund, K., Peled, D.: Eﬃcient runtime veriﬁcation of ﬁrst-order temporal prop-
erties. In: Gallardo, M.M., Merino, P. (eds.) SPIN 2018. LNCS, vol. 10869, pp.
26–47. Springer, Cham (2018). https://doi.org/10.1007/978-3-319-94111-0_2
26. Havelund, K., Roşu, G.: Synthesizing monitors for safety properties. In: Katoen,
J.-P., Stevens, P. (eds.) TACAS 2002. LNCS, vol. 2280, pp. 342–356. Springer,
Heidelberg (2002). https://doi.org/10.1007/3-540-46002-0_24
27. Heimes, L.: Extending and optimizing a veriﬁed monitor for metric ﬁrst-order
temporal logic. Bachelor’s thesis, Department of Computer Science, ETH Zürich
(2019)
28. Heimes, L., Schneider, J., Traytel, D.: Formalization of an algorithm for greedily
computing associative aggregations on sliding windows. Archive of Formal Proofs
(2020). https://isa-afp.org/entries/Sliding_Window_Algorithm.html
29. Lammich, P.: Generating veriﬁed LLVM from Isabelle/HOL. In: Harrison, J.,
O’Leary, J., Tolmach, A. (eds.) ITP 2019. LIPIcs, vol. 141, pp. 22:1–22:19.
Schloss Dagstuhl - Leibniz-Zentrum für Informatik (2019). https://doi.org/10.
4230/LIPIcs.ITP.2019.22
30. Lammich, P.: Reﬁnement to imperative HOL. J. Autom. Reasoning 62(4), 481–503
(2019). https://doi.org/10.1007/s10817-017-9437-1
31. Laurent, J., Goodloe, A., Pike, L.: Assuring the guardians. In: Bartocci, E., Majum-
dar, R. (eds.) RV 2015. LNCS, vol. 9333, pp. 87–101. Springer, Cham (2015).
https://doi.org/10.1007/978-3-319-23820-3_6
32. Lochbihler, A.: Light-weight containers for Isabelle: eﬃcient, extensible, nestable.
In: Blazy, S., Paulin-Mohring, C., Pichardie, D. (eds.) ITP 2013. LNCS, vol.
7998, pp. 116–132. Springer, Heidelberg (2013). https://doi.org/10.1007/978-3-
642-39634-2_11
33. Malecha, J.G., Morrisett, G., Shinnar, A., Wisnesky, R.: Toward a veriﬁed rela-
tional database management system. In: Hermenegildo, M.V., Palsberg, J. (eds.)
POPL 2010, pp. 237–248. ACM (2010. https://doi.org/10.1145/1706299.1706329
34. Mamouras, K., Raghothaman, M., Alur, R., Ives, Z.G., Khanna, S.: StreamQRE:
modular speciﬁcation and eﬃcient evaluation of quantitative queries over streaming
data. In: Cohen, A., Vechev, M.T. (eds.) PLDI 2017, pp. 693–708. ACM (2017).
https://doi.org/10.1145/3062341.3062369
35. McKeeman, W.M.: Diﬀerential testing for software. Digit. Tech. J. 10(1), 100–107
(1998)

A Formally Veriﬁed, Optimized Monitor for MFODL
453
36. Mitsch, S., Platzer, A.: ModelPlex: veriﬁed runtime validation of veriﬁed cyber-
physical system models. Form. Methods Syst. Des. 49(1–2), 33–74 (2016). https://
doi.org/10.1007/s10703-016-0241-z
37. Ngo, H.Q., Porat, E., Ré, C., Rudra, A.: Worst-case optimal join algorithms:
[extended abstract]. In: Benedikt, M., Krötzsch, M., Lenzerini, M. (eds.) PODS
2012, pp. 37–48. ACM (2012). https://doi.org/10.1145/2213556.2213565
38. Ngo, H.Q., Porat, E., Ré, C., Rudra, A.: Worst-case optimal join algorithms. J.
ACM 65(3), 16:1–16:40 (2018). https://doi.org/10.1145/3180143
39. Ngo, H.Q., Ré, C., Rudra, A.: Skew strikes back: new developments in the theory
of join algorithms. SIGMOD Rec. 42(4), 5–16 (2013). https://doi.org/10.1145/
2590989.2590991
40. Peycheva, G.: Real-time veriﬁcation of datacenter security policies via online log
analysis. Master’s thesis, ETH Zürich (2018)
41. Pike, L., Niller, S., Wegmann, N.: Runtime veriﬁcation for ultra-critical systems.
In: Khurshid, S., Sen, K. (eds.) RV 2011. LNCS, vol. 7186, pp. 310–324. Springer,
Heidelberg (2012). https://doi.org/10.1007/978-3-642-29860-8_23
42. Pike, L., Wegmann, N., Niller, S., Goodloe, A.: Experience report: a do-it-yourself
high-assurance compiler. In: Thiemann, P., Findler, R.B. (eds.) ICFP 2012, pp.
335–340. ACM (2012). https://doi.org/10.1145/2364527.2364553
43. Raszyk, M., Basin, D., Krstić, S., Traytel, D.: Multi-head monitoring of metric
temporal logic. In: Chen, Y.-F., Cheng, C.-H., Esparza, J. (eds.) ATVA 2019.
LNCS, vol. 11781, pp. 151–170. Springer, Cham (2019). https://doi.org/10.1007/
978-3-030-31784-3_9
44. Rizaldi, A., et al.: Formalising and monitoring traﬃc rules for autonomous vehi-
cles in Isabelle/HOL. In: Polikarpova, N., Schneider, S. (eds.) IFM 2017. LNCS,
vol. 10510, pp. 50–66. Springer, Cham (2017). https://doi.org/10.1007/978-3-319-
66845-1_4
45. Schneider, J., Basin, D., Krstić, S., Traytel, D.: A formally veriﬁed monitor for
metric ﬁrst-order temporal logic. In: Finkbeiner, B., Mariani, L. (eds.) RV 2019.
LNCS, vol. 11757, pp. 310–328. Springer, Cham (2019). https://doi.org/10.1007/
978-3-030-32079-9_18
46. Thati, P., Rosu, G.: Monitoring algorithms for metric temporal logic speciﬁcations.
Electron. Notes Theoret. Comput. Sci. 113, 145–162 (2005). https://doi.org/10.
1016/j.entcs.2004.01.029
47. Ulus, D.: Montre: a tool for monitoring timed regular expressions. In: Majumdar,
R., Kunčak, V. (eds.) CAV 2017. LNCS, vol. 10426, pp. 329–335. Springer, Cham
(2017). https://doi.org/10.1007/978-3-319-63387-9_16
48. Veldhuizen, T.L.: Triejoin: a simple, worst-case optimal join algorithm. In:
Schweikardt, N., Christophides, V., Leroy, V. (eds.) ICDT 2014, pp. 96–106. Open-
Proceedings.org (2014). https://doi.org/10.5441/002/icdt.2014.13
49. Völlinger, K.: Verifying the output of a distributed algorithm using certiﬁcation.
In: Lahiri, S., Reger, G. (eds.) RV 2017. LNCS, vol. 10548, pp. 424–430. Springer,
Cham (2017). https://doi.org/10.1007/978-3-319-67531-2_29
50. Yu, L.: A formal model of IEEE ﬂoating point arithmetic. Archive of Formal Proofs
(2013). https://isa-afp.org/entries/IEEE_Floating_Point.html

Constructive Hybrid Games
Rose Bohrer1(B)
and Andr´e Platzer1,2
1 Computer Science Department, Carnegie Mellon University, Pittsburgh, USA
aplatzer@cs.cmu.edu
2 Fakult¨at f¨ur Informatik, Technische Universit¨at M¨unchen, Munich, Germany
Abstract. Hybrid games combine discrete, continuous, and adversarial
dynamics. Diﬀerential game logic (dGL) enables proving (classical) exis-
tence of winning strategies. We introduce constructive diﬀerential game
logic (CdGL) for hybrid games, where proofs that a player can win the
game correspond to computable winning strategies. This constitutes the
logical foundation for synthesis of correct control and monitoring code
for safety-critical cyber-physical systems. Our contributions include novel
semantics as well as soundness and consistency.
Keywords: Game logic · Constructive logic · Hybrid games ·
Dependent types
1
Introduction
Diﬀerential Game Logic (dGL) provides a calculus for proving the (classical) exis-
tence of winning strategies for hybrid games [42], whose mixed discrete, continu-
ous, and adversarial dynamics are compelling models for cyber-physical systems
(CPSs). Classical existence does not necessarily imply that the resulting winning
strategies are computable, however. To overcome this challenge, this paper intro-
duces Constructive Diﬀerential Game Logic (CdGL) with a Curry-Howard corre-
spondence: constructive proofs for constructive hybrid games correspond to pro-
grams implementing their winning strategies. We develop a new type-theoretic
semantics which elucidates this correspondence and an operational semantics
which describes the execution of strategies. Besides its theoretical appeal, this
Curry-Howard interpretation provides the foundation for proof-driven synthe-
sis methods, which excel at synthesizing expressive classes of games for which
synthesis and correctness require interactive proof. Hybrid games are a com-
pelling domain for proof-based synthesis both because many CPS applications
are safety-critical or even life-critical, such as transportation systems, energy
systems, and medical devices and because the combination of discrete, contin-
uous, and adversarial dynamics makes veriﬁcation and synthesis undecidable in
both theory and practice. Our example model and proof, while short, lay the
groundwork for future case studies.
This research was sponsored by the AFOSR under grant number FA9550-16-1-0288
and the Alexander von Humboldt Foundation. The ﬁrst author was also funded by an
NDSEG Fellowship.
c
⃝The Author(s) 2020
N. Peltier and V. Sofronie-Stokkermans (Eds.): IJCAR 2020, LNAI 12166, pp. 454–473, 2020.
https://doi.org/10.1007/978-3-030-51074-9_26

Constructive Hybrid Games
455
Challenges and Contributions. In addition to dGL [42], we build directly on
Constructive Game Logic (CGL) [9] for discrete games. Compared to CGL, we
target a domain with readily-available practical applications (hybrid games),
and introduce new type-theoretic and operational semantics which complement
the realizability semantics of CGL while making Curry-Howard particularly clear
and providing a simple notion of strategy execution. We overcome the following
challenges in the process:
– Our semantics must carefully capture the meaning of constructive hybrid
game strategies, including strategies for diﬀerential equations (ODEs).
– Soundness must be justiﬁed constructively. We adapt previous arguments to
use constructive analysis [6,12] by appealing to constructive formalizations
of ODEs [17,34]. This adaptation to our new semantics makes it possible to
simplify statements of some standard lemmas.
– We study 1D driving control as an example, which demonstrates the strengths
of both games and constructivity. Games and constructivity both introduce
uncertainties: A player is uncertain how their opponent will play, while con-
structive real-number comparisons are never sure of exact equality. These
uncertainties demand more nuanced proof invariants, but these nuances
improve our ﬁdelity to real systems.
These contributions are of likely interest to several communities. Other con-
structive program logics could reuse our semantic approach. Our example uses
reach-avoid proofs for hybrid games, a powerful, under-explored [48] approach.
2
Related Work
We discuss related works on games, constructive logic, and hybrid systems.
Games in Logic. Propositional GL was introduced by Parikh [39]. GL is a pro-
gram logic in the spirit of Hoare calculi [26] or especially dynamic logics (DL) [47]:
modalities capture the eﬀect of game execution. GLs are unique in their clear del-
egation of strategy to the proof language rather than the model language, allow-
ing succinct game speciﬁcations with sophisticated winning strategies. Succinct
speciﬁcations are important: speciﬁcations are trusted because proving the wrong
theorem would not ensure correctness. Relatives without this separation include
SL [14], ATL [2], CATL [27], SDGL [23], structured strategies [49], DEL [3,5,56],
evidence logic [4], and Angelic Hoare Logic [35].
Constructive Modal Logics. We are interested in the semantics of games, thus we
review constructive modal semantics generally. This should not be confused with
game semantics [1], which give a semantics to programs in terms of games. The
main semantic approaches for constructive modal logics are intuitionistic Kripke
semantics [58] and realizability semantics [32,38]. CGL [9] used a realizability
semantics which operate on a state, reminiscent of state in Kripke semantics,
whereas we interpret CdGL formulas into type theory.

456
R. Bohrer and A. Platzer
Modal Curry-Howard is relatively little-studied, and each author has their
own emphasis. Explicit proof terms are considered for CGL [9] and a small frag-
ment thereof [30]. Others [13,18,59] focus on intuitionistic semantics for their
logics, fragments of CGL. Our semantics should be of interest for these frag-
ments. We omit proof terms for space. CdGL proof terms would extend CGL proof
terms [9] with a constructive version of existing classical ODE proof terms [8].
Propositional modal logic [37] has been interpreted as a type system.
Hybrid Systems Synthesis. Hybrid games synthesis is one motivation of this
work. Synthesis of hybrid systems (1-player games) is an active area. The unique
strength of proof-based synthesis is expressiveness: it can synthesize every prov-
able system. CdGL proofs support ﬁrst-order regular games with ﬁrst-order (e.g.,
semi-algebraic) initial and goal regions. While synthesis and proof are both unde-
cidable, interactive proof for undecidable logics is well-understood. The Mod-
elPlex [36] synthesizer for CdGL’s classical systems predecessor dL [44] recently
added [11] proof-based synthesis to improve expressiveness. CdGL aims to pro-
vide a computational foundation for a more systematic proof-based synthesizer
in the more general context of games.
Fully automatic synthesis, in contrast, restricts itself to small fragments
in order to sidestep undecidability. Studied classes include rectangular hybrid
games [25], switching systems [52], linear systems with polyhedral sets [31,52],
and discrete abstractions [20,21]. A well-known [55] systems synthesis app-
roach translates speciﬁcations into ﬁnite-alternation games. Arbitrary ﬁrst-order
games are our source rather than target language. Their approach is only known
to terminate for simpler classes [50,51].
3
Constructive Hybrid Games
Hybrid games in CdGL are 2-player, zero-sum, and perfect-information, where
continuous subgames are ordinary diﬀerential equations (ODEs) whose duration
is chosen by a player. Hybrid games should not be confused with diﬀerential
games which compete continuously [29,43]. The players considered in this paper
are Angel and Demon where the player currently controlling choices is always
called Angel, while the player waiting to play is always called Demon. For any
game α and formula φ, the modal formula ⟨α⟩φ says Angel can play α to ensure
postcondition φ, while [α]φ says Demon can play α to ensure postcondition φ.
These generalize safety and liveness modalities from DL. Dual games αd, unique
to GLs, take turns by switching the Angel and Demon roles in game α. The Curry-
Howard interpretation of a proof of a CdGL modality ⟨α⟩φ or [α]φ is a program
which performs each player’s winning strategy. Games can have several winning
strategies, each corresponding to a diﬀerent proof and a diﬀerent program.
3.1
Syntax of CdGL
We introduce the language of CdGL with three classes of expressions e: terms f, g,
games α, β, and formulas φ, ψ. We characterize terms semantically for the sake

Constructive Hybrid Games
457
of generality: a shallow embedding of CdGL inside a proof assistant might use
the host language for terms. For games and formulas, we ﬁnd it more convenient
to explicitly and syntactically deﬁne a closed language.
A (scalar) semantic term is a function from states to reals, which are under-
stood constructively `a la Bishop [6,12]. We use Bishop-style real analysis because
it preserves many classical intuitions (e.g., uncountability) about R while ensur-
ing computability. Type-2 [57] computability requires that all functions on real
numbers are computable to arbitrary precision if represented as streams of bits,
yet computability does not require that variables range over only computable
reals. It is a theorem [57] that all such computable functions are continuous, but
not always Lipschitz-continuous nor diﬀerentiable.
We introduce commonly used term constructs, which are not exhaustive
because the language of terms is open. The simplest terms are game variables
x, y ∈V where V is the (at most countable) set of variable identiﬁers. The game
variables, which are mutable, contain the state of the game, which is globally
scoped. For every base game variable x there is a primed counterpart x′ whose
purpose within an ODE is to track the time derivative of x. Real-valued terms
f, g are simply type-2 computable functions, usually from states to reals. It is
occasionally useful for f to return a tuple of reals, which are computable when
every component is computable. Since terms are functions, operators are com-
binators: f + g is a function which sums the results of f and g.
Deﬁnition 1 (Terms). A term f, g is any computable function over the game
state. The following constructs appear in this paper:
f, g ::=
· · · | c | x | f + g | f · g | f/g | min(f, g) | max(f, g) | (f)′
where c ∈R is a real literal, x a game variable, f + g a sum, f · g a product, and
f/g is real division of f by g. Divisors g are assumed to be nonzero. Minimum
and maximum of terms f and g are written min(f, g) and max(f, g). Any dif-
ferentiable term f has a deﬁnable (Sect. 4.2) spatial diﬀerential term (f)′, which
agrees with the time derivative within an ODE.
CdGL is constructive, so Angel strategies make choices computably. Until his
turn, Demon just observes Angel’s choices, and does not care whether Angel
made them computably. We discuss game-playing informally here, then formally
in Sect. 4. In red are the ODE and dual games, which respectively distinguish
hybrid games from discrete games and games from systems.
Deﬁnition 2 (Games). The set of games α, β is deﬁned recursively as such:
α, β ::= ?φ | x := f | x := ∗| x′ = f & ψ | α ∪β | α; β | α∗| αd
The test game ?φ, is a no-op if Angel proves φ, else Demon wins by default
since Angel “broke the rules”. A deterministic assignment x := f updates game
variable x to the value of term f. Nondeterministic assignments x := ∗ask Angel
to compute the new value of x : R, i.e., Angel’s strategy for x := ∗is a term whose
value is assigned to x. The ODE game x′ = f & ψ evolves ODE x′ = f for dura-
tion d ≥0 chosen by Angel such that Angel proves the domain constraint formula

458
R. Bohrer and A. Platzer
ψ is true throughout. We require that term f is eﬀectively-locally-Lipschitz on
domain ψ, meaning that at every state satisfying ψ, a neighborhood and coef-
ﬁcient L can be constructed such that L is a Lipschitz constant of f in the
neighborhood. Eﬀective local Lipschitz continuity guarantees unique solutions
exist by constructive Picard-Lindel¨of [34]. ODEs are explicit-form, so no primed
variable y′ for y ∈V is mentioned in f or ψ. Systems of ODEs are supported, we
present single equations for readability. In the choice game α ∪β, Angel chooses
whether to play game α or game β. In the sequential composition game α; β,
game α is played ﬁrst, then β from the resulting state. In the repetition game
α∗, Angel chooses after each repetition of α whether to continue playing, but
must not repeat α inﬁnitely. The exact number of repetitions is not known in
advance, because it may depend on Demon’s reactions. In the dual game αd,
Angel takes the Demon role and vice-versa while playing α. Demon strategies
“wait” until a dual game αd is encountered, then play an Angelic strategy for
α. We parenthesize games with braces {α} when necessary.
Deﬁnition 3 (CdGL Formulas). The CdGL formulas φ (also ψ) are:
φ ::= ⟨α⟩φ | [α]φ | f ∼g
Above, f ∼g is a comparison formula for ∼∈{≤, <, =, ̸=, >, ≥}. The deﬁn-
ing formulas of CdGL (and GL) are the modalities ⟨α⟩φ and [α]φ. These mean
that Angel or Demon respectively have a constructive strategy to play hybrid
game α and prove postcondition φ. We do not develop modalities for existence
of classical strategies because those cannot be synthesized to executable code.
Standard connectives are deﬁned from games and comparisons. Verum (tt)
is deﬁned 1 > 0 and falsum (ff) is 0 > 1. Conjunction φ ∧ψ is deﬁned ⟨?φ⟩ψ,
disjunction φ∨ψ is deﬁned ⟨?φ ∪?ψ⟩tt, and implication φ →ψ is deﬁned [?φ]ψ.
Real quantiﬁers ∀x φ and ∃x φ are deﬁned [x := ∗]φ and ⟨x := ∗⟩φ, respectively. As
usual, equivalence φ ↔ψ reduces to (φ →ψ) ∧(ψ →φ), negation ¬φ is deﬁned
as φ →ff, and inequality is deﬁned by f ̸= g ≡¬(f = g). Semantics and proof
rules are needed only for core constructs, but we use derived constructs when
they improve readability. Keep these deﬁnitions in mind, because the semantics
and rules for some game connectives mirror ﬁrst-order connectives.
For convenience, we also write derived operators where Demon is given con-
trol of a single choice before returning control to Angel. The Demonic choice
α ∩β, deﬁned {αd ∪βd}d, says Demon chooses which branch to take, but Angel
controls the subgames. Demonic repetition α× is deﬁned likewise by {{αd}∗}d.
We write φ y
x (likewise for α and f) for the renaming of variable x for y and
vice versa in formula φ, and write φf
x for the result of substitution of term f for
game variable x in φ, if the substitution is admissible (Deﬁnition 12 on page 14).
3.2
Example Game
We give an example game and theorem statements, proven in [10]. Automotive
systems are a major class of CPS. As a simple indicative example we consider

Constructive Hybrid Games
459
time-triggered 1-dimensional driving with adversarial timing. For maximum time
T between control cycles, we let Demon choose any duration in [0, T]. When
we need to prohibit pathological “Zeno” behaviors while keeping constraints
realistic, we can further restrict t ∈[T/2, T].
We write x for the current position of the car, v for its velocity, a for the
acceleration, A > 0 for the maximum positive acceleration, and B > 0 for the
maximum braking rate. We assume x = v = 0 initially to simplify arithmetic.
In time-triggered control, the controller runs at least once every T > 0 time
units. Time and physics are continuous, T gives an upper bound on how often
the controller runs. Local clock t marks the current time within the current
timestep, then resets at each step. The control game (ctrl) says Angel can pick
any acceleration a that is physically achievable (−B ≤a ≤A). The clock t is
then reinitialized to 0. The plant game (plant) says Demon can evolve physics for
duration t ∈[0, T] such that v ≥0 throughout, then returns control to Angel.
Typical theorems in DLs and GLs are safety and liveness: are unsafe states
always avoided and are desirable goals eventually reached? Safety and liveness
of the 1D system has been proved previously: safe driving (safety) never goes
past goal g, while live driving eventually reaches g (liveness).
pre ≡T > 0 ∧A > 0 ∧B > 0 ∧v = 0 ∧x = 0
post ≡(g = x ∧v = 0)
ctrl ≡a := ∗; ? −B ≤a ≤A; t := 0
plant ≡{t′ = 1, x′ = v, v′ = a & t ≤T ∧v ≥0}d
safety ≡pre →⟨(ctrl; plant)×⟩x ≤g
liveness ≡pre →⟨(ctrl; plant; {?t ≥T/2}d)
∗⟩x ≥g
Liveness theorem liveness requires a lower time bound ({?t ≥T/2}d) to rule
out Zeno strategies where Demon “cheats” by exponentially decreasing durations
to essentially freeze the progress of time. The limit t ≥T/2 is chosen for sim-
plicity. Safety theorem safety omits this constraint because even Zeno behaviors
are safe.
Safety and liveness theorems, if designed carelessly, have trivial solutions
including but not limited to Zeno behaviors. It is safe to remain at x = 0 and is
live to maintain a = A, but not vice-versa. In contrast to DLs, GLs easily express
the requirement that the same strategy is both safe and live: we must remain
safe while reaching the goal. We use this reach-avoid speciﬁcation because it is
immune to trivial solutions. We give a new reach-avoid result for 1D driving.
Example 4 (Reach-avoid). The following is provable in dGL and CdGL:
reachAvoid ≡pre →⟨{ctrl; plant; ?x ≤g; {?t ≥T/2}d}
∗⟩post
Angel reaches g = x ∧v = 0 while safely avoiding states where x ≤g does
not hold. Angel is safe at every iteration for every time t ∈[0, T], thus safe
throughout the game. The (dual) test ?t ≥T/2 appears second, allowing Demon
to win if Angel violates safety during t < T/2.

460
R. Bohrer and A. Platzer
Fig. 1. Safe driving envelope (Color ﬁgure
online)
1D driving is well-studied for
classical systems, but the construc-
tive reach-avoid proof [10] is subtle.
The proof constructs an envelope
of safe upper and live lower bounds
on velocity as a function of position
(Fig. 1). The blue point indicates
where Angel must begin to brake
to ensure time-triggered safety. It
is surprising that Angel can achieve
postcondition g = x ∧v = 0, given that trichotomy (f < g ∨f = g ∨f > g)
is constructively invalid. The key [10] is that comparison terms min(f, g) and
max(f, g) are exact in Type 2 computability where bits of min and max may be
computed lazily. Our exact result encourages us that constructivity is not overly
burdensome in practice. When decidable comparisons (f < g + δ ∨f > g) are
needed, the alternative is a weaker guarantee g −ε ≤x ≤g for parameter ε > 0.
This relaxation is often enough to make the theorem provable, and reﬂects the
fact that real agents only expect to reach their goal within ﬁnite precision.
4
Type-Theoretic Semantics
In this section, we deﬁne the semantics of hybrid games and game formulas in
type theory. We start with assumptions on the underlying type theory.
4.1
Type Theory Assumptions
We assume a Calculus of Inductive and Coinductive Constructions (CIC)-like
type theory [15,16,54] with polymorphism and dependency. We write M for
terms and Δ ⊢M : τ to say M has type τ in CIC context Δ. We assume
ﬁrst-class (indexed [19]) inductive and coinductive types. We write τ for type
families and κ for kinds: type families inhabited by other type families. Induc-
tive type families are written μt : κ. τ, which denotes the smallest solution ty of
kind κ to the ﬁxed-point equation ty = τ ty
t . Coinductive type families are writ-
ten ρt : κ. τ, which denotes the largest solution ty of kind κ to the ﬁxed-point
equation ty = τ ty
t . Type-expression τ must be monotone in t so smallest and
largest solutions exist by Knaster-Tarski [24, Thm. 1.12]. Proof assistants like
Coq reject deﬁnitions where monotonicity requires nontrivial proof; we did not
mechanize our proofs because they use such deﬁnitions.
We use one predicative universe which we write T and Coq writes Type 0.
Predicativity is an important assumption because our semantic deﬁnition is a
large elimination, a feature known to interact dangerously with impredicativity.
We write Πx : τ1. τ2 for a dependent function type with argument named x of
type τ1 and where return type τ2 may mention x. We write Σx : τ1. τ2 for a
dependent pair type with left component named x of type τ1 and right compo-
nent of type τ2, possibly mentioning x. These specialize to the simple function

Constructive Hybrid Games
461
τ1 ⇒τ2 and product types τ1 * τ2 respectively when x is not mentioned in τ2.
Lambdas (λx : τ. M) inhabit dependent function types. Pairs (M, N) inhabit
dependent pair types. Application is M N. Let-binding unpacks pairs, whose
left and right projection are πLM and πRM. We write τ1 + τ2 for a disjoint
union inhabited by ℓ· M and r · M, and write case A of p ⇒B | q ⇒C for its
case analysis.
We assume a real number type R and a Euclidean state type S. The positive
real numbers are written R>0, nonnegative reals R≥0. We assume scalar and
vector sums, products, inverses, and units. States s, t support operations s x
and set s x v which respectively retrieve the value of variable x in s : S or
update it to v. The usual axioms of setters and getters [22] are satisﬁed. We
write s for the distinguished variable of type S representing the current state.
We will ﬁnd it useful to consider the semantics of an expression both at current
state s and at states s, t deﬁned in terms of s (e.g., set s x 5).
4.2
Semantics of CdGL
Terms f, g are type-theoretic functions of type S ⇒R. We will need diﬀerential
terms (f)′, a deﬁnable term construct when f is diﬀerentiable. Not every term
f need be diﬀerentiable, so we give a virtual deﬁnition, deﬁning when (f)′ is
equal to some term g. If (f)′ does not exist, then (f)′ = g is not provable. We
deﬁne the (total) diﬀerential as the Euclidean dot product (·) of the gradient
(variable name: ∇) with s′, which is the vector of values s x′ assigned to primed
variables x′. To show that ∇is the gradient, we deﬁne the gradient as a limit,
which we express in (ε, δ) style. In this deﬁnition, f and g are scalar-valued, and
the minus symbol is used for both scalar and vector diﬀerence.
((f)′ s = g s) ≡Σ∇: R|s′|. (g s = ∇·s′)*Πε : R>0. Σδ : R>0. Πr : S.
(∥r −s∥< δ) ⇒|f r −f s −∇·(r −s)| ≤ε∥r −s∥
For practical proofs, a library of standard rules for automatic, syntactic dif-
ferentiation of common arithmetic operations [7] could be proven.
The interpretation ⌜φ⌝: S ⇒T of formula φ is a predicate over states. A
predicate of kind S ⇒T is also understood as a region, e.g., ⌜φ⌝is the region
containing states where φ is provable. A CdGL context Γ is interpreted over a
uniform state term s : S where s : S ⊢s : S, i.e., s usually mentions s. We deﬁne
⌜Γ ⌝(s) to be the CIC context containing s : S and ⌜φ⌝s for each φ ∈Γ. The
sequent (Γ ⊢φ) is valid if there exists M where ⌜Γ ⌝(s) ⊢M : (⌜φ⌝s). Formula
φ is valid iﬀsequent (· ⊢φ) is valid. That is, a valid formula is provable in every
state with a common proof term M. The witness may inspect the state, but
must do so constructively. Formula semantics employ the Angelic and Demonic
semantics of games, which determine how to win a game α whose postcondition
is φ. We write ⟨⟨α⟩⟩: (S ⇒T) ⇒(S ⇒T) for the Angelic semantics of α and
[[α]] : (S ⇒T) ⇒(S ⇒T) for its Demonic semantics.

462
R. Bohrer and A. Platzer
Deﬁnition 5 (Formula semantics). Angel and Demon strategies for a hybrid
game α with goal region P are inhabitants of ⟨⟨α⟩⟩P and [[α]] P, respectively.
⌜[α]φ⌝s = [[α]] ⌜φ⌝s
⌜⟨α⟩φ⌝s = ⟨⟨α⟩⟩⌜φ⌝s
⌜f ∼g⌝s = ((f s) ∼(g s))
Modality ⟨α⟩φ is provable in s when ⟨⟨α⟩⟩⌜φ⌝s is inhabited so Angel has an
α strategy from s to reach region ⌜φ⌝on which φ is provable. Modality [α]φ is
provable in s when [[α]] ⌜φ⌝s is inhabited so Demon has an α strategy from s to
reach region ⌜φ⌝on which φ is provable. For ∼∈{≤, <, =, ̸=, >, ≥}, the values
of f and g are compared at state s in f ∼g. The game and formula semantics
are simultaneously inductive. In each case, the connectives which deﬁne ⟨⟨α⟩⟩
and [[α]] are duals, because [α]φ and ⟨α⟩φ are dual. Below, P refers to the goal
region of the game and s to the initial state.
Deﬁnition 6 (Angel semantics). We deﬁne ⟨⟨α⟩⟩: (S ⇒T) ⇒(S ⇒T)
inductively (by a large elimination) on α:
⟨⟨?ψ⟩⟩P s = ⌜ψ⌝s * P s
⟨⟨x := f⟩⟩P s = P (set s x (f s))
⟨⟨x := ∗⟩⟩P s = Σv : R. P (set s x v)
⟨⟨α ∪β⟩⟩P s = ⟨⟨α⟩⟩P s + ⟨⟨β⟩⟩P s
⟨⟨α; β⟩⟩P s = ⟨⟨α⟩⟩(⟨⟨β⟩⟩P) s
⟨⟨αd⟩⟩P s = [[α]] P s
⟨⟨x′ = f & ψ⟩⟩P s = Σd : R≥0. Σsol : [0, d] ⇒R.
(sol, s, d ⊨x′ = f)
* (Πt : [0, d]. ⌜ψ⌝(set s x (sol t)))
* P (set s (x, x′)
(sol d, f (set s x (sol d))))
⟨⟨α∗⟩⟩P s =

μτ ′ : (S ⇒T). λt : S. (P t ⇒τ ′ t) + (⟨⟨α⟩⟩τ ′ t ⇒τ ′ t)

s
Angel wins ⟨?ψ⟩P by proving both ψ and P at s. Angel wins the determin-
istic assignment x := f by performing the assignment, then proving P. Angel
wins nondeterministic assignment x := ∗by constructively choosing a value v to
assign, then proving P. Angel wins α ∪β by choosing between playing α or β,
then winning that game. Angel wins α; β if she wins α with the postcondition
of winning β. Angel wins αd if she wins α in the Demon role. Angel wins ODE
game x′ = f & ψ by choosing some solution sol of some duration d which sat-
isﬁes the ODE and domain constraint throughout and the postcondition φ at
time d. While top-level postconditions rarely mention x′, intermediate invariant
steps do, thus x and x′ both are updated in the postcondition. The construct
(sol, s, d ⊨x′ = f), saying sol solves x′ = f from state s for time d, is deﬁned:
(sol, s, d ⊨x′ = f) ≡

(s x = sol 0) * Πr : [0, d]. ((sol)′ r = f (set s x (sol r)))

Note that variable sol stands for a function of the host theory, all of which
are computable and therefore continuous. When (sol, s, d ⊨x′ = f) holds, sol
is also continuously diﬀerentiable. Constructive Picard-Lindel¨of [34] constructs
a solution for every eﬀectively-locally-Lipschitz ODEs, which need not have a
closed form. The proof calculus we introduce in Sect. 5 includes both solution-
based proof rules, which are useful for ODEs with simple closed forms, and
invariant-based rules, which enable proof even when closed forms do not exist.

Constructive Hybrid Games
463
Angel strategies for α∗are inductively deﬁned: either choose to stop the loop
and prove P now, else play a round of α before repeating inductively. By Knaster-
Tarski [24, Thm. 1.12], this least ﬁxed point exists because the interpretation of
a game is monotone in its postcondition (Lemma 7).
Lemma 7 (Monotonicity). Let P, Q : S ⇒T. If s : S, P s ⊢M : Q s then
there exists a term N such that s : S, [[α]] P s ⊢N : [[α]] Q s
Deﬁnition 8 (Demon semantics). We deﬁne [[α]] : (S ⇒T) ⇒(S ⇒T)
inductively (by a large elimination) on α:
[[?ψ]] P s = ⌜ψ⌝s ⇒P s
[[x := f]] P s = P (set s x (f s))
[[x := ∗]] P s = Πv : R. P (set s x v)
[[α ∪β]] P s = [[α]] P s * [[β]] P s
[[α; β]] P s = [[α]] ([[β]] P) s
[[αd]] P s = ⟨⟨α⟩⟩P s
[[x′ = f & ψ]] P s = Πd : R≥0. Πsol : [0, d] ⇒R.
(sol, s, d ⊨x′ = f)
⇒(Πt : [0, d]. ⌜ψ⌝(set s x (sol t)))
⇒P (set s (x, x′)
(sol d, f (set s x (sol d))))
[[α∗]] P s =

ρτ ′ : (S ⇒T). λt : S. (τ ′ t ⇒[[α]] τ ′ t) * (τ ′ t ⇒P t)

s
Demon wins [?ψ]P by proving P under assumption ψ, which Angel must pro-
vide (Sect. 7). Demon’s deterministic assignment is identical to Angel’s. Demon
wins x := ∗by proving ψ for every choice of x. Demon wins α ∪β with a pair of
winning strategies. Demon wins α; β by winning α with a postcondition of win-
ning β. Demon wins αd if he can win α after switching roles with Angel. Demon
wins x′ = f & ψ if for an arbitrary duration and arbitrary solution which satisfy
the domain constraint, he can prove the postcondition. Demon wins [α∗]P if he
can prove P no matter how many times Angel makes him play α. Demon repe-
tition strategies are coinductive using some invariant τ ′. When Angel decides to
stop the loop, Demon responds by proving P from τ ′. Whenever Angel chooses
to continue, Demon proves that τ ′ is preserved. Greatest ﬁxed points exist by
Knaster-Tarski [24, Thm. 1.12] using Lemma 7.
It is worth comparing the Angelic and Demonic semantics of x := ∗. An Angel
strategy says how to compute x. A Demon strategy simply accepts x ∈R as its
input, even uncomputable numbers. This is because Angel strategies supply a
computable real while Demon acts with computable outputs given real inputs. In
general, each strategy is constructive but permits its opponent to play classically.
In the cyber-physical setting, the opponent is indeed rarely a computer.
5
Proof Calculus
To enable direct syntactic proof, we give a natural deduction-style system for
CdGL. We write Γ = ψ1, . . . , ψn for a context of formulas and Γ ⊢φ for the
natural-deduction sequent with conclusion φ and context Γ. We begin with rules
shared by CGL [9] and CdGL, then give the ODE rules. We write Γ y
x for the

464
R. Bohrer and A. Platzer
renaming of game variable x to y and vice versa in context Γ. Likewise Γ f
x is the
substitution of term f for game variable x. To avoid repetition, we write ⟨[α]⟩φ
to indicate that the same rule applies for ⟨α⟩φ and [α]φ. These rules write [⟨α⟩]φ
for the dual of ⟨[α]⟩φ. We write FV(e), BV(α), and MBV(α) for the free variables
of expression e, bound variables of game α, and must-bound variables of game α
respectively, i.e., variables which might inﬂuence the meaning of an expression,
might be modiﬁed during game execution, or are written during every execution.
([∪]I) Γ ⊢[α]φ
Γ ⊢[β]φ
Γ ⊢[α ∪β]φ
(⟨∪⟩I1)
Γ ⊢⟨α⟩φ
Γ ⊢⟨α ∪β⟩φ
(⟨?⟩I) Γ ⊢φ
Γ ⊢ψ
Γ ⊢⟨?φ⟩ψ
([?]I)
Γ, φ ⊢ψ
Γ ⊢[?φ]ψ
([∪]E1) Γ ⊢[α ∪β]φ
Γ ⊢[α]φ
(⟨∪⟩I2)
Γ ⊢⟨β⟩φ
Γ ⊢⟨α ∪β⟩φ
(⟨?⟩E1) Γ ⊢⟨?φ⟩ψ
Γ ⊢φ
([?]E) Γ ⊢[?φ]ψ
Γ ⊢φ
Γ ⊢ψ
([∪]E2) Γ ⊢[α ∪β]φ
Γ ⊢[β]φ
(hyp) Γ, φ ⊢φ
(⟨?⟩E2) Γ ⊢⟨?φ⟩ψ
Γ ⊢ψ
(⟨∪⟩E) Γ ⊢⟨α ∪β⟩φ
Γ, ⟨α⟩φ ⊢ψ
Γ, ⟨β⟩φ ⊢ψ
Γ ⊢ψ
Fig. 2. CdGL proof calculus: propositional game rules
Figure 2 gives the propositional game rules. Rule [?]E is modus ponens and
[?]I is implication introduction because [?φ]ψ is implication. Angelic choices are
disjunctions introduced by ⟨∪⟩I1 and ⟨∪⟩I2 and case-analyzed by ⟨∪⟩E. Angelic
tests and Demonic choices are conjunctions introduced by ⟨?⟩I and [∪]I, elimi-
nated by ⟨?⟩E1, ⟨?⟩E2, [∪]E1, and [∪]E2. Rule hyp applies an assumption.
([:∗]I)
Γ y
x ⊢φ
Γ ⊢[x := ∗]φ
(⟨:∗⟩I) Γ ⊢⟨x := f⟩φ
Γ ⊢⟨x := ∗⟩φ
(⟨[;]⟩I) Γ ⊢⟨[α]⟩⟨[β]⟩φ
Γ ⊢⟨[α; β]⟩φ
(⟨[:=]⟩I) Γ y
x, x = f y
x ⊢φ
Γ ⊢⟨[x := f]⟩φ
([:∗]E) Γ ⊢[x := ∗]φ
Γ ⊢φf
x
(⟨:∗⟩E) Γ ⊢⟨x := ∗⟩φ
Γ ⊢∀x (φ →ψ)
Γ ⊢ψ
(x /∈FV(ψ))
(M)
Γ ⊢⟨[α]⟩φ
Γ
y
BV(α), φ ⊢ψ
Γ ⊢⟨[α]⟩ψ
(⟨[d]⟩I) Γ ⊢[⟨α⟩]φ
Γ ⊢⟨[αd]⟩φ
Fig. 3. CdGL proof calculus: ﬁrst-order games (y fresh, f computable, φf
x admissible)
Figure 3 covers assignment, choice, sequencing, duals, and monotonicity.
Angelic games have injectors (⟨∗⟩S, ⟨∗⟩G) and case analysis (⟨∗⟩E). Duality
switches players by switching modalities. Sequential games
are decomposed
as nested modalities.

Constructive Hybrid Games
465
Monotonicity (M) is Lemma 7 in rule form. The second premiss writes Γ
y
BV(α)
to indicate that the bound variables of α must be freshly renamed in Γ for
soundness. Rule M is used for generalization because all GLs are subnormal,
lacking axiom K (modal modus ponens) and necessitation. Common uses include
concise right-to-left symbolic execution proofs and, in combination with
,
Hoare-style sequential composition reasoning.
Nondeterministic assignments quantify over real-valued game variables.
Assignments
remember the initial value of x in fresh variable y (Γ y
x) for
sake of completeness, then provide an assumption that x has been assigned to f.
Skolemization [: ∗]I bound-renames x to y in Γ, written Γ y
x. Specialization [: ∗]E
instantiates x to a term f by substituting φf
x. Existentials are introduced by giv-
ing a witness f in ⟨:∗⟩I. Herbrandization ⟨:∗⟩E unpacks existentials, soundness
requires x is not free in ψ.
(⟨∗⟩E) Γ ⊢⟨α∗⟩φ
Γ, φ ⊢ψ
Γ, ⟨α⟩⟨α∗⟩φ ⊢ψ
Γ ⊢ψ
([∗]E)
Γ ⊢[α∗]φ
Γ ⊢φ ∧[α][α∗]φ
(⟨∗⟩S)
Γ ⊢φ
Γ ⊢⟨α∗⟩φ
([∗]R) Γ ⊢φ ∧[α][α∗]φ
Γ ⊢[α∗]φ
(⟨∗⟩G) Γ ⊢⟨α⟩⟨α∗⟩φ
Γ ⊢⟨α∗⟩φ
(loop) Γ ⊢J
J ⊢[α]J
J ⊢φ
Γ ⊢[α∗]φ
(FP) Γ ⊢⟨α∗⟩φ
φ ⊢ψ
⟨α⟩ψ ⊢ψ
Γ ⊢ψ
(⟨∗⟩I)
Γ ⊢ϕ
ϕ, 0 ≽M ⊢φ
ϕ, (M ≻0 ∧M0 = M) ⊢⟨α⟩(ϕ ∧M0 ≻M)
Γ ⊢⟨α∗⟩φ
Fig. 4. CdGL proof calculus: loops (M0 fresh)
Figure 4 provides rules for repetitions. In rule ⟨∗⟩I, M indicates an arbitrary
termination metric where ≻and ≽denote strict and nonstrict comparison in an
arbitrary (eﬀectively) well-founded [28] partial order. Metavariable 0 represents
a terminal value at which iteration stops; we will choose 0 = 0 in our example,
but 0 need not be 0 in general. M0 is a fresh variable which remembers M.
Angel plays α∗by repeating an α strategy which always decreases the termina-
tion metric. Angel maintains a formula ϕ throughout, and stops once 0 ≽M.
The postcondition need only follow from termination condition 0 ≽M and con-
vergence formula ϕ. Simple real comparisons x ≥y are not well-founded, but
inﬂated comparisons like x ≥y+1 are. Well-founded metrics ensure convergence
in ﬁnitely (but often unboundedly) many iterations. In the simplest case, M is a
real-valued term. Generalizing M to tuples enables, e.g., lexicographic termina-
tion metrics. For example, the metric in the proof of Example 4 is the distance
to the goal, which must decrease by some minimum amount each iteration.
Repetition games can be folded and unfolded ([∗]E, [∗]R). Rule FP says ⟨α∗⟩φ
is a least pre-ﬁxed-point. It works backwards: ﬁrst show ψ holds after α∗, then

466
R. Bohrer and A. Platzer
preserve ψ when each iteration is unwound. Rule loop is the repetition invariant
rule. Demonic repetition is eliminated by [∗]E.
Like any ﬁrst-order program logic, CdGL proofs contain ﬁrst-order reason-
ing at the leaves. Decidability of constructive real arithmetic is an open prob-
lem [33], so ﬁrst-order facts are proven manually in practice. Our semantics
embed CdGL into type theory; we defer ﬁrst-order arithmetic proving to the
host theory. Even eﬀectively-well-founded ≽need not have decidable guards
(0 ≻M ∨M ≽0) since exact comparisons are not computable [6]. We may
not be able to distinguish M = 0 from very small positive values of M, leading
to one unnecessary loop iteration, after which M is certainly 0 and the loop
terminates. Comparison up to ε > 0 is decidable [12] (f > g ∨(f < g + ε)).
(DC) Γ ⊢[x′=f & ψ]R
Γ ⊢[x′=f & ψ ∧R]φ
Γ ⊢[x′=f & ψ]φ
(DG) Γ ⊢∃y [x′=f, y′ = a(x)y + b(x) & ψ]φ
Γ ⊢[x′=f & ψ]φ
(DI) Γ ⊢φ
Γ ⊢∀x (ψ →[x′ := f](φ)′)
Γ ⊢[x′=f & ψ]φ
(DW) Γ ⊢∀x ∀x′ (ψ →φ)
Γ ⊢[x′=f & ψ]φ
(DV)
ψ, h ≥g ⊢φ
Γ ⊢d > 0 ∧ε > 0 ∧h −g ≥−dε
Γ ⊢⟨t := 0; {t′=1, x′=f & ψ}⟩t ≥d
Γ ⊢[x′=f]((h)′ −(g)′) ≥ε
Γ ⊢⟨x′=f & ψ⟩φ
(bsolve) Γ ⊢∀t : R≥0 ((∀r : [0, t] [t := r; x := sln]ψ) →[x := sln; x′ := f]φ)
Γ ⊢[x′=f & ψ]φ
(dsolve) Γ ⊢∃t : R≥0 ((∀r : [0, t] ⟨t := r; x := sln⟩ψ) ∧⟨x := sln; x′ := f⟩φ)
Γ ⊢⟨x′=f & ψ⟩φ
Fig. 5. CdGL proof calculus: ODEs. In bsolve and dsolve, sln solves x′ = f globally, t
and r fresh, x′ /∈FV(φ)
Figure 5 gives the ODE rules, which are a constructive version of those from
dGL [42]. For nilpotent ODEs such as the plant of Example 4, reasoning via
solutions is possible. Since CdGL supports nonlinear ODEs which often do not
have closed-form solutions, we provide invariant-based rules, which are com-
plete [46] for invariants of polynomial ODEs. Diﬀerential induction DI [41] says
φ is an invariant of an ODE if it holds initially and if its diﬀerential formula [41]
(φ)′ holds throughout, for example (f ≥g)′ ≡((f)′ ≥(g)′). Soundness of DI
requires diﬀerentiability, and (φ)′ is not provable when φ mentions nondiﬀeren-
tiable terms. Diﬀerential cut DC proves R invariant, then adds it to the domain
constraint. Diﬀerential weakening DW says that if φ follows from the domain
constraint, it holds throughout the ODE. Diﬀerential ghosts DG permit us to
augment an ODE system with a fresh dimension y, which enables [46] proofs of
otherwise unprovable properties. We restrict the right-hand side of y to be linear
in y and (uniformly) continuous in x because soundness requires that ghosting y′
does not change the duration of an ODE. A linear right-hand side is guaranteed
to be Lipschitz on the whole existence interval of equation x′ = f, thus ensuring
an unchanged duration by (constructive) Picard-Lindel¨of [34]. Diﬀerential vari-
ants [41,53] DV is an Angelic counterpart to DI. The schema parameters d and

Constructive Hybrid Games
467
ε must not mention x, x′, t, t′. To show that f eventually exceeds g, ﬁrst choose
a duration d and a suﬃciently high minimum rate ε at which h −g will change.
Prove that h −g decreases at rate at least ε and that the ODE has a solution of
duration d satisfying constraint ψ. Thus at time d, both h ≥g and its provable
consequents hold. Rules bsolve and dsolve assume as a side condition that sln is
the unique solution of x′ = f on domain ψ. They are convenient for ODEs with
simple solutions, while invariant reasoning supports complicated ODEs.
6
Theory: Soundness
Following constructive counterparts of classical soundness proofs for dGL, we
prove that the CdGL proof calculus is sound: provable formulas are true in the
CIC semantics. For the sake of space, we give statements and some outlines here,
reporting all proofs and lemmas elsewhere [10]. Similar lemmas have been used
to prove soundness of dGL [45], but our new semantics lead to simpler statements
for Lemmas 10 and 11. The coincidence property for terms is not proved but
assumed, since we inherit a semantic treatment of terms from the host theory.
Let s y
x be s with the values of x and y swapped. Let sf
x be set s x (f s). Deﬁned
CIC term s
V= t ↔*x∈V (s x = t x) says s and t agree on all x ∈V .
Lemma 9 (Uniform renaming). Let M y
x rename x and y in proof term M.
– If ⌜Γ ⌝(s) ⊢M : (⌜φ⌝s) then ⌜Γ y
x
⌝(s y
x) ⊢M y
x : (⌜φ y
x
⌝s y
x).
Lemma 10 (Coincidence). Assume s
V= t where V ⊇FV(Γ) ∪FV(φ).
– If ⌜Γ ⌝(s) ⊢M : (⌜φ⌝s) then exists N such that ⌜Γ ⌝(t) ⊢N : (⌜φ⌝t).
Lemma 11 (Bound eﬀect). Let P : S ⇒T and let V ⊆BV(α)∁, the com-
plement of bound variables of α.
– There exists M such that ⌜Γ ⌝(s) ⊢M : (⟨⟨α⟩⟩P s) iﬀthere exists N such that
⌜Γ ⌝(s) ⊢N : (⟨⟨α⟩⟩(λt. P t * s
V= t) s).
– There exists M such that ⌜Γ ⌝(s) ⊢M : ([[α]] P s) iﬀthere exists N such that
⌜Γ ⌝(s) ⊢N : ([[α]] (λt. P t * s
V= t) s).
Deﬁnition 12 (Term substitution admissibility [40, Def. 6]). For a for-
mula φ, (likewise for context Γ, term f, and game α) we say φf
x is admissible if
x never appears free in φ under a binder of {x} ∪FV(f).
Lemma 13 (Term substitution). Let M f
x substitute f for x in proof term
M. Let Γ f
x and φf
x be admissible.
– If ⌜Γ ⌝(sf
x) ⊢M : (⌜φ⌝sf
x) then
⌜Γ f
x
⌝(s) ⊢M f
x : (
⌜φf
x
⌝s).
The converse implication also holds, though its witness is not necessarily M.

468
R. Bohrer and A. Platzer
Soundness of the proof calculus follows from the lemmas, and soundness of
the ODE rules employing several known results from constructive analysis.
Theorem 14 (Soundness). If Γ ⊢M : φ holds, then sequent (Γ ⊢φ) is valid.
As a special case, if · ⊢M : φ holds, then formula φ is valid.
Proof Sketch. By induction on the derivation. The assignment case holds by
Lemma 13 and Lemma 9. Lemma 10 and Lemma 11 are applied when main-
taining truth of a formula across changing state. The equality and inequality
cases of DI and DV employ the constructive mean-value theorem [10, Thm. 21],
which has been formalized, e.g., in Coq [17]. Rules DW, bsolve, and dsolve follow
from the semantics of ODEs. Rule DC uses the fact that preﬁxes of solutions
are solutions. Rule DG uses constructive Picard-Lindel¨of [34], which constitutes
an algorithm for arbitrarily approximating the solution of any Lipschitz ODE,
with a convergence rate depending on its Lipschitz constant.
⊓⊔
We have shown that every provable formula is true in the type-theoretic seman-
tics. Because the soundness proof is constructive, it amounts to an extraction
algorithm from CdGL into type theory: for each CdGL proof, there exists a pro-
gram in type theory which inhabits the corresponding type of the semantics.
7
Theory: Extraction and Execution
Another perspective on constructivity is that provable properties must have
witnesses. We show Existential and Disjunction properties providing witnesses
for existentials and disjunctions. For modal formulas ⟨α⟩φ and [α]φ we show
proofs can be used as winning strategies: a big-step operational semantics play
allows playing strategies against each other to extract a proof that their goals
hold in some ﬁnal state t. Our presentation is more concise than deﬁning the
language, semantics, and properties of strategies, while providing key insights.
Lemma 15. (Existential Property). Let s : S. If ⌜Γ ⌝(s) ⊢M : (⌜∃x φ⌝s)
then there exist terms f : R and N such that ⌜Γ ⌝(s) ⊢N : (
⌜φf
x
⌝s).
Lemma 16. (Disjunction Property). If ⌜Γ ⌝(s) ⊢M : (⌜φ ∨ψ⌝s) then there
exists a proof term N such that ⌜Γ ⌝(s) ⊢N : (⌜φ⌝s) or ⌜Γ ⌝(s) ⊢N : (⌜ψ⌝s).
The proofs follow their counterparts in type theory. The Disjunction Property
considers truth at a speciﬁc state. Validity of φ ∨ψ does not imply validity of
either φ or ψ. For example, x < 1 ∨x > 0 is valid, but its disjuncts are not.
Function play below gives a big-step semantics: Angel and Demon strategies
as and ds for respective goals φ and ψ in game α suﬃce to construct a ﬁnal
state t satisfying both. By parametricity, t was found by playing α, because play
cannot inspect P and Q, thus can only prove them via as and ds.
play : Πα : Game. ΠP, Q : (S ⇒T). Πs : S.

⟨⟨α⟩⟩P s ⇒[[α]] Q s ⇒Σt : S. P t * Q t


Constructive Hybrid Games
469
Applications of play are written playα s as ds (P and Q implicit). Game consis-
tency (Corollary 17) is by play and consistency of type theory. Note that αd is
played by swapping the Angel and Demon strategies in α.
playx:=f s as ds = (let t = set s x (f s) in (t, (as t, ds t)))
playx:=∗s as ds = let t = set s x πLas in (t, (πRas, ds πLas))
playx′=f & ψ s as ds = let (d, sol, solves, c, p) = as s in
(set s x (sol d), (p, ds d sol solves c))
play?φ s as ds = (s, (πRas, ds (πLas)))
playα∪β s as ds = case (as s) of
as′ ⇒playα s as′ (πLds)
| as′ ⇒playβ s as′ (πRds)
playα;β s as ds = (let (t, (as′, ds′)) = playα s as ds in playβ t as′ ds′)
playα∗s as ds = case (as s) of
as′ ⇒(s, (as′, πLds))
| as′ ⇒let (t, (as′′, ds′′)) = playα s as′ (πRds) in
playα∗t as′′ ds′′
playαd s as ds = playα s ds as
Corollary 17. (Consistency). It is never the case that both ⌜⟨α⟩φ⌝s and
⌜[α]¬φ⌝s are inhabited.
Proof. Suppose as : ⌜⟨α⟩φ⌝s and ds : ⌜[α]¬φ⌝s, then πR(playα s as ds) : ⊥,
contradicting consistency of type theory.
⊓⊔
The play semantics show how strategies can be executed. Consistency is a
theorem which ought to hold in any GL and thus helps validate our semantics.
8
Conclusion and Future Work
We extended Constructive Game Logic CGL to CdGL for constructive hybrid
games. We contributed new semantics. We presented a natural deduction proof
calculus for CdGL and used it to prove reach-avoid correctness of 1D driving with
adversarial timing. We showed soundness and constructivity results.
The next step is to implement a proof checker, game interpreter, and syn-
thesis tool for CdGL. Function play is the high-level interpreter algorithm, while
synthesis would commit to one Angel strategy and allow black-box Demon imple-
mentations for an external environment. Angel strategies are positive and are
synthesized by extracting witnesses from each introduction rule. Demonic invari-
ants and test conditions describe allowed observable behaviors. Demon strategies
are negative and characterized by observable behaviors, so it suﬃces to monitor
their compliance with invariants and test conditions extracted from the proof.

470
R. Bohrer and A. Platzer
Acknowledgements. We thank Jon Sterling for suggestions regarding our choice of
type theory and for references to the literature. We thank the anonymous reviewers for
their helpful feedback.
References
1. Abramsky, S., Jagadeesan, R., Malacaria, P.: Full abstraction for PCF. Inf. Com-
put. 163(2), 409–470 (2000). https://doi.org/10.1006/inco.2000.2930
2. Alur, R., Henzinger, T.A., Kupferman, O.: Alternating-time temporal logic. J.
ACM 49(5), 672–713 (2002). https://doi.org/10.1145/585265.585270
3. Benthem, J.: Logic of strategies: what and how? In: van Benthem, J., Ghosh, S.,
Verbrugge, R. (eds.) Models of Strategic Reasoning. LNCS, vol. 8972, pp. 321–332.
Springer, Heidelberg (2015). https://doi.org/10.1007/978-3-662-48540-8 10
4. van Benthem, J., Pacuit, E.: Dynamic logics of evidence-based beliefs. Stud. Logica.
99(1–3), 61–92 (2011). https://doi.org/10.1007/s11225-011-9347-x
5. van Benthem, J., Pacuit, E., Roy, O.: Toward a theory of play: a logical perspective
on games and interaction. Games (2011). https://doi.org/10.3390/g2010052
6. Bishop, E.: Foundations of Constructive Analysis. McGraw-Hill, New York (1967)
7. Bohrer, R., Fern´andez, M., Platzer, A.: dLι: deﬁnite descriptions in diﬀerential
dynamic logic. In: Fontaine, P. (ed.) CADE 2019. LNCS (LNAI), vol. 11716, pp.
94–110. Springer, Cham (2019). https://doi.org/10.1007/978-3-030-29436-6 6
8. Bohrer, R., Platzer, A.: Toward structured proofs for dynamic logics. CoRR
abs/1908.05535 (2019), http://arxiv.org/abs/1908.05535
9. Bohrer, R., Platzer, A.: Constructive game logic. ESOP 2020. LNCS, vol. 12075,
pp. 84–111. Springer, Cham (2020). https://doi.org/10.1007/978-3-030-44914-8 4
10. Bohrer, R., Platzer, A.: Constructive hybrid games. CoRR abs/2002.02536 (2020),
https://arxiv.org/abs/2002.02536
11. Bohrer, R., Tan, Y.K., Mitsch, S., Myreen, M.O., Platzer, A.: VeriPhy: veriﬁed con-
troller executables from veriﬁed cyber-physical system models. In: Grossman, D.
(ed.) PLDI, pp. 617–630. ACM (2018). https://doi.org/10.1145/3192366.3192406
12. Bridges, D.S., Vita, L.S.: Techniques of Constructive Analysis. Springer, New York
(2007). https://doi.org/10.1007/978-0-387-38147-3
13. Celani, S.A.: A fragment of intuitionistic dynamic logic. Fundam. Inform. 46(3),
187–197 (2001). http://content.iospress.com/articles/fundamenta-informaticae/
ﬁ46-3-01
14. Chatterjee, K., Henzinger, T.A., Piterman, N.: Strategy logic. In: Caires, L., Vas-
concelos, V.T. (eds.) CONCUR 2007. LNCS, vol. 4703, pp. 59–73. Springer, Hei-
delberg (2007). https://doi.org/10.1007/978-3-540-74407-8 5
15. Coquand, T., Huet, G.P.: The calculus of constructions. Inf. Comput. 76(2/3),
95–120 (1988). https://doi.org/10.1016/0890-5401(88)90005-3
16. Coquand, T., Paulin, C.: Inductively deﬁned types. In: Martin-L¨of, P., Mints,
G. (eds.) COLOG 1988. LNCS, vol. 417, pp. 50–66. Springer, Heidelberg (1990).
https://doi.org/10.1007/3-540-52335-9 47
17. Cruz-Filipe, L., Geuvers, H., Wiedijk, F.: C-CoRN, the constructive Coq repository
at Nijmegen. In: Asperti, A., Bancerek, G., Trybulec, A. (eds.) MKM 2004. LNCS,
vol. 3119, pp. 88–103. Springer, Heidelberg (2004). https://doi.org/10.1007/978-
3-540-27818-4 7
18. Degen, J., Werner, J.: Towards intuitionistic dynamic logic. Log. Log. Philos. 15(4),
305–324 (2006). https://doi.org/10.12775/LLP.2006.018

Constructive Hybrid Games
471
19. Dybjer, P.: Inductive families. Formal Asp. Comput. 6(4), 440–465 (1994). https://
doi.org/10.1007/BF01211308
20. Filippidis, I., Dathathri, S., Livingston, S.C., Ozay, N., Murray, R.M.: Control
design for hybrid systems with TuLiP: the temporal logic planning toolbox. In:
Conference on Control Applications, pp. 1030–1041. IEEE (2016). https://doi.org/
10.1109/CCA.2016.7587949
21. Finucane, C., Jing, G., Kress-Gazit, H.: LTLMoP: experimenting with language,
temporal logic and robot control. In: IROS, pp. 1988–1993. IEEE (2010). https://
doi.org/10.1109/IROS.2010.5650371
22. Foster, J.N.: Bidirectional programming languages. Technical report MS-CIS-10-
08, Department of Computer & Information Science, University of Pennsylvania,
Philadelphia, PA, March 2010
23. Ghosh, S.: Strategies made explicit in dynamic game logic. In: Workshop on Logic
and Intelligent Interaction at ESSLLI, pp. 74–81 (2008)
24. Harel, D., Kozen, D., Tiuryn, J.: Dynamic Logic. MIT Press, Cambridge (2000)
25. Henzinger, T.A., Horowitz, B., Majumdar, R.: Rectangular hybrid games. In:
Baeten, J.C.M., Mauw, S. (eds.) CONCUR 1999. LNCS, vol. 1664, pp. 320–335.
Springer, Heidelberg (1999). https://doi.org/10.1007/3-540-48320-9 23
26. Hoare, C.A.R.: An axiomatic basis for computer programming. Commun. ACM
12(10), 576–580 (1969). https://doi.org/10.1145/363235.363259
27. van der Hoek, W., Jamroga, W., Wooldridge, M.J.: A logic for strategic reasoning.
In: Dignum, F., Dignum, V., Koenig, S., Kraus, S., Singh, M.P., Wooldridge, M.J.
(eds.) AAMAS. ACM (2005). https://doi.org/10.1145/1082473.1082497
28. Hofmann, M., van Oosten, J., Streicher, T.: Well-foundedness in realizability. Arch.
Math. Log. 45(7), 795–805 (2006). https://doi.org/10.1007/s00153-006-0003-5
29. Isaacs, R.: Diﬀerential Games: A Mathematical Theory with Applications to
Warfare and Pursuit, Control and Optimization. Series in Applied Mathematics
(SIAM), Wiley, New York (1965)
30. Kamide, N.: Strong normalization of program-indexed lambda calculus. Bull. Sect.
Log. Univ. L´od´z 39(1–2), 65–78 (2010)
31. Kloetzer, M., Belta, C.: A fully automated framework for control of linear systems
from temporal logic speciﬁcations. IEEE Trans. Automat. Control 53(1), 287–297
(2008). https://doi.org/10.1109/TAC.2007.914952
32. Lipton, J.: Constructive Kripke semantics and realizability. In: Moschovakis, Y.
(ed.) Logic From Computer Science, pp. 319–357. Springer, New York (1992).
https://doi.org/10.1007/978-1-4612-2822-6 13
33. Lombardi, H., Mahboubi, A.: Th´eories g´eom´etriques pour l’alg`ebre des nombres
r´eels. Contemp. Math. 697, 239–264 (2017)
34. Makarov, E., Spitters, B.: The Picard algorithm for ordinary diﬀerential equations
in Coq. In: Blazy, S., Paulin-Mohring, C., Pichardie, D. (eds.) ITP 2013. LNCS,
vol. 7998, pp. 463–468. Springer, Heidelberg (2013). https://doi.org/10.1007/978-
3-642-39634-2 34
35. Mamouras, K.: Synthesis of strategies using the Hoare logic of angelic and demonic
nondeterminism. Log. Methods Comput. Sci. 12(3), 1–41 (2016). https://doi.org/
10.2168/LMCS-12(3:6)2016
36. Mitsch, S., Platzer, A.: ModelPlex: veriﬁed runtime validation of veriﬁed cyber-
physical system models. Form. Methods Syst. Des. 49(1), 33–74 (2016). https://
doi.org/10.1007/s10703-016-0241-z

472
R. Bohrer and A. Platzer
37. Murphy VII, T., Crary, K., Harper, R., Pfenning, F.: A symmetric modal lambda
calculus for distributed computing. In: LICS. IEEE (2004), https://doi.org/10.
1109/LICS.2004.1319623
38. van Oosten, J.: Realizability: a historical essay. Math. Structures Comput. Sci.
12(3), 239–263 (2002). https://doi.org/10.1017/S0960129502003626
39. Parikh, R.: Propositional game logic. In: FOCS, pp. 195–200. IEEE (1983). https://
doi.org/10.1109/SFCS.1983.47
40. Platzer, A.: Diﬀerential dynamic logic for hybrid systems. J. Autom. Reas. 41(2),
143–189 (2008). https://doi.org/10.1007/s10817-008-9103-8
41. Platzer, A.: Diﬀerential-algebraic dynamic logic for diﬀerential-algebraic programs.
J. Log. Comput. 20(1), 309–352 (2010). https://doi.org/10.1093/logcom/exn070
42. Platzer, A.: Diﬀerential game logic. ACM Trans. Comput. Log. 17(1), 1:1-1:51
(2015). https://doi.org/10.1145/2817824
43. Platzer, A.: Diﬀerential hybrid games. ACM Trans. Comput. Log. 18(3), 19:1-19:44
(2017). https://doi.org/10.1145/3091123
44. Platzer, A.: Logical Foundations of Cyber-Physical Systems. Springer, Cham
(2018). https://doi.org/10.1007/978-3-319-63588-0
45. Platzer, A.: Uniform substitution for diﬀerential game logic. In: Galmiche, D.,
Schulz, S., Sebastiani, R. (eds.) IJCAR 2018. LNCS (LNAI), vol. 10900, pp. 211–
227. Springer, Cham (2018). https://doi.org/10.1007/978-3-319-94205-6 15
46. Platzer, A., Tan, Y.K.: Diﬀerential equation invariance axiomatization. J. ACM
67, 1 (2020). https://doi.org/10.1145/3380825
47. Pratt, V.R.: Semantical considerations on Floyd-Hoare logic. In: FOCS, pp. 109–
121. IEEE (1976). https://doi.org/10.1109/SFCS.1976.27
48. Quesel, J.-D., Platzer, A.: Playing hybrid games with KeYmaera. In: Gramlich, B.,
Miller, D., Sattler, U. (eds.) IJCAR 2012. LNCS (LNAI), vol. 7364, pp. 439–453.
Springer, Heidelberg (2012). https://doi.org/10.1007/978-3-642-31365-3 34
49. Ramanujam, R., Simon, S.E.: Dynamic logic on games with structured strategies.
In: Brewka, G., Lang, J. (eds.) Knowledge Representation, pp. 49–58. AAAI Press
(2008). http://www.aaai.org/Library/KR/2008/kr08-006.php
50. Shakernia, O., Pappas, G.J., Sastry, S.: Semi-decidable synthesis for triangular
hybrid systems. In: Di Benedetto, M.D., Sangiovanni-Vincentelli, A. (eds.) HSCC
2001. LNCS, vol. 2034, pp. 487–500. Springer, Heidelberg (2001). https://doi.org/
10.1007/3-540-45351-2 39
51. Shakernia, O., Pappas, G.J., Sastry, S.: Decidable controller synthesis for classes of
linear systems. In: Lynch, N., Krogh, B.H. (eds.) HSCC 2000. LNCS, vol. 1790, pp.
407–420. Springer, Heidelberg (2000). https://doi.org/10.1007/3-540-46430-1 34
52. Taly, A., Tiwari, A.: Switching logic synthesis for reachability. In: Carloni, L.P.,
Tripakis, S. (eds.) EMSOFT, pp. 19–28. ACM (2010). https://doi.org/10.1145/
1879021.1879025
53. Tan, Y.K., Platzer, A.: An axiomatic approach to liveness for diﬀerential equations.
In: ter Beek, M.H., McIver, A., Oliveira, J.N. (eds.) FM 2019. LNCS, vol. 11800, pp.
371–388. Springer, Cham (2019). https://doi.org/10.1007/978-3-030-30942-8 23
54. The Coq development team: The Coq proof assistant reference manual (2019).
https://coq.inria.fr/
55. Tomlin, C.J., Lygeros, J., Sastry, S.S.: A game theoretic approach to controller
design for hybrid systems. Proc. IEEE 88(7), 949–970 (2000)
56. Van Benthem, J.: Games in dynamic-epistemic logic. Bull. Econ. Res. 53(4), 219–
248 (2001)

Constructive Hybrid Games
473
57. Weihrauch, K.: Computable Analysis - An Introduction. Texts in Theoretical Com-
puter Science, Springer, Heidelberg (2000). https://doi.org/10.1007/978-3-642-
56999-9
58. Wijesekera, D.: Constructive modal logics I. Ann. Pure Appl. Log. 50(3), 271–301
(1990). https://doi.org/10.1016/0168-0072(90)90059-B
59. Wijesekera, D., Nerode, A.: Tableaux for constructive concurrent dynamic logic.
Ann. Pure Appl. Log. 135(1–3), 1–72 (2005). https://doi.org/10.1016/j.apal.2004.
12.001
Open Access This chapter is licensed under the terms of the Creative Commons
Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/),
which permits use, sharing, adaptation, distribution and reproduction in any medium
or format, as long as you give appropriate credit to the original author(s) and the
source, provide a link to the Creative Commons license and indicate if changes were
made.
The images or other third party material in this chapter are included in the
chapter’s Creative Commons license, unless indicated otherwise in a credit line to the
material. If material is not included in the chapter’s Creative Commons license and
your intended use is not permitted by statutory regulation or exceeds the permitted
use, you will need to obtain permission directly from the copyright holder.

Formalizing a Seligman-Style Tableau
System for Hybrid Logic
(Short Paper)
Asta Halkjær From1(B)
, Patrick Blackburn2
, and Jørgen Villadsen1
1 Technical University of Denmark, Kongens Lyngby, Denmark
ahfrom@dtu.dk
2 Roskilde University, Roskilde, Denmark
Abstract. Hybrid logic is modal logic enriched with names for worlds.
We formalize soundness and completeness proofs for a Seligman-style
tableau system for hybrid logic in the proof assistant Isabelle/HOL. The
formalization shows how to lift certain rule restrictions, thereby simpli-
fying the original un-formalized proof. Moreover, the completeness proof
we formalize is synthetic which suggests we can extend this work to prove
a wider range of results about hybrid logic.
Keywords: Isabelle/HOL · Hybrid logic · Soundness · Completeness
1
Introduction
Hybrid logic extends ordinary modal logic with nominals, a special sort of propo-
sitional symbol true at exactly one world. Nominals, and the satisfaction oper-
ators they give rise to, make hybrid logic well-suited for diﬀerent applications
ranging from temporal logic [4] to epistemic logics for social networks [22]. The
description logics underlying the Web Ontology Language and applications in
biomedical informatics [16] can also be seen as forms of hybrid logic [2].
ST is a sound and complete tableau system for hybrid logic. It is known to
terminate when ﬁve restrictions are imposed on the rules, and one key rule is split
into three cases [5]. Two completeness proofs exist for ST, a synthetic one that
does not cover the rule restrictions [17] and a complex translation-based proof
that does [5]. In this paper we modify ST and three of its restrictions slightly, and
use the proof assistant Isabelle/HOL to show that we can lift these restrictions
by (a) formally proving the admissibility of their unrestricted versions, and (b)
formalizing a synthetic completeness proof for the modiﬁed calculus.
Isabelle is a generic proof assistant and Isabelle/HOL is its instance based
on higher-order logic [20]. Proof assistants like Isabelle provide tools to express
mathematical statements and proofs in a formal language that can be mechan-
ically veriﬁed; all proofs presented here have been checked in this manner. The
full formalization, 4396 lines, is available in the Archive of Formal Proofs which
keeps refereed submissions up to date with the current Isabelle version [13].
The formalization was developed for the ﬁrst author’s MSc thesis [15]. We chose
Isabelle/HOL because it is the proof assistant we know best.
c
⃝Springer Nature Switzerland AG 2020
N. Peltier and V. Sofronie-Stokkermans (Eds.): IJCAR 2020, LNAI 12166, pp. 474–481, 2020.
https://doi.org/10.1007/978-3-030-51074-9_27

Formalizing a Seligman-Style Tableau System for Hybrid Logic
475
2
Syntax and Semantics
The well-formed formulas of basic hybrid logic are deﬁned as follow. We use the
letter x for propositional symbols and i, a and b for nominals.
φ, ψ ::= x | i | ¬φ | φ ∨ψ | ♦φ | @iφ
The language is interpreted on Kripke models M, consisting of a frame (W, R)
and a valuation of propositional symbols V . Here W is a non-empty set of worlds
and R is a binary accessibility relation between them. To interpret the nominals
we use an assignment g mapping nominals to elements of W; if g(i) = w then
we say that nominal i denotes w. Formula satisﬁability is deﬁned as follows:
M, g, w |= x
iﬀ
w ∈V (x)
M, g, w |= i
iﬀ
g(i) = w
M, g, w |= ¬φ
iﬀ
M, g, w ̸|= φ
M, g, w |= φ ∨ψ
iﬀ
M, g, w |= φ or M, g, w |= ψ
M, g, w |= ♦φ
iﬀ
for some w′, wRw′ and M, g, w′ |= φ
M, g, w |= @iφ
iﬀ
M, g, g(i) |= φ
An expression of the form @iφ is called a satisfaction statement, and such state-
ments are true iﬀφ is true at the world denoted by nominal i. Note two important
special cases: @ia says that the nominals i and a denote the same world, and
@i♦b says that the world denoted by i has access to the world denoted by b.
3
A Seligman-Style Tableau System
Many proof systems for hybrid logic exist; see Blackburn et al. [5] for discussion.
These typically work by manipulating only formulas preﬁxed by satisfaction
operators, which gives a global ﬂavour to proofs, however the tableau system
we formalize here manipulates arbitrary formulas. It is an adaptation of system
ST, due to Blackburn et al. [5], which was inspired by Jeremy Seligman’s local
natural deduction and sequent calculus systems for hybrid logic [23,24].
The tableau rules are based on a subdivision of tableau branches into blocks.
Each pair of blocks is separated by a horizontal line and the ﬁrst formula on each
block is a nominal dubbed the opening nominal. The intuition is that the formu-
las on a block are true in the world denoted by its opening nominal. We assume
that the initial block, like the rest, is always named (this is our ﬁrst modiﬁcation
of the original ST system). This assumption simpliﬁes the formalization, as we
can now model all blocks as lists of formulas paired with an opening nominal,
and a branch as a list of blocks. If Θ is a branch and φ occurs on an i-block
in Θ then we say that φ occurs at i in Θ. We occasionally refer to the opening
nominal of a block as its name or type.
The rules are given in Fig. 1: the ﬁrst three are propositional, the three below
are for working with the blocks, and the four to the right apply to the hybrid
logic connectives. The input to the rule is given above the vertical line and the

476
A. H. From et al.
output below it. Above every input formula, we write the opening nominal of
the block it occurs on. Similarly, the opening nominal of the output block is the
ﬁrst thing below the line. If the opening nominals match, then the output block
may be the same as an input block. In the formalization we model the rules as
an inductively deﬁned set of branches that have closing extensions.
a
φ ∨ψ
a
/
\
φ
ψ
a
¬(φ ∨ψ)
a
|
¬φ
¬ψ
a
¬¬φ
a
|
φ
a
♦φ
a
|
♦i
@iφ
a
a
¬♦φ
♦i
a
|
¬@iφ
(∨)
(¬∨)
(¬¬)
(♦)1
(¬♦)
|
i
b
b
a
i
φ
i
a
|
φ
i
i
φ
¬φ
b
@aφ
a
|
φ
b
¬@aφ
a
|
¬φ
GoTo2
Nom
Closing
(@)
(¬@)
1 i is fresh, φ is not a nominal.
2 i is not fresh.
Fig. 1. Tableau rules.
Consider the (¬¬) rule: if ¬¬φ occurs on an a-block and the current block is
an a-block, then φ is a legal extension of the branch. For the Nom rule, nominal i
occurs at both a and b, so they must denote the same world and copying φ from
a b-block to the current a-block is legal. Here we also diﬀer from the original
ST: we do not require the shared nominal i to occur on the current block as
this would be a problem for our Strengthening Lemma in Sect. 4. The GoTo rule
allows us to change perspective from one world to another by starting a new
block with an opening nominal that already occurs somewhere on the branch.
A branch closes if the same formula occurs on the same type of block both
positively and negatively, and a tableau closes if all its branches do. If a closed
tableau can be obtained starting from the branch Θ, then we write ⊢Θ. If Θ
is a branch and the current block has opening nominal a, then we write the
extension of Θ by φ as φ −a Θ to resemble the extensions in Fig. 1.
The original ST has ﬁve restrictions, called R1-R5 [5]. Restriction R3 is
unnecessary in our system since it applies to an omitted rule that names the

Formalizing a Seligman-Style Tableau System for Hybrid Logic
477
initial block. Restriction R4 forbids applying GoTo twice in a row and formal-
izing it is left for future work. Here are our adaptations of the three remaining
restrictions:
R1 The output of a rule must include a formula new to the current block type.
R2 The (♦) rule can only be applied to input ♦φ on an a-block if it is not already
witnessed on a.
R5 (@) and (¬@) can only be applied to premises i and @iφ (¬@iφ) when the
current block is an i-block.
The formula φ is new to a in Θ if φ does not occur at a in Θ. A formula ♦φ
is witnessed at a in Θ if for some witnessing nominal i, both ♦i and @iφ occur
at a in Θ. The original R2 restriction states that the (♦) rule cannot be applied
twice to the same formula occurrence, but formalizing this would require keeping
track of previous rule applications. We already keep track of the branch so we
prefer the R2 presented here. Our version of the @-rules already satisfy the R5
restriction.
4
Main Results
Theorem 1 (Soundness). If ⊢Θ where Θ consists of just ¬φ on an i-block
and i does not occur in φ, then φ is valid.
Proof. Similar to the original soundness proof by Blackburn et al. [5].
⊓⊔
The following lemma allows us to derive rules unrestricted by R1:
Lemma 1 (Strengthening). If an extension is not new then it is redundant.
That is, if ⊢φ −a Θ and φ occurs at a in Θ then ⊢Θ.
Proof. The existing φ can be used as rule input in place of the extension.
⊓⊔
To lift R2 we use the following substitution lemma where φσ and Θσ are
obtained from φ and Θ, respectively, by replacing every nominal i with σ(i).
Lemma 2 (Substitution). Let σ be a substitution function whose domain and
codomain coincide. If ⊢Θ then ⊢Θσ.
Proof. By induction on the derivation of ⊢Θ for arbitrary σ. In the (♦) case we
assume that ♦φ occurs at a in Θ and need to derive ⊢Θσ from ⊢(@iφ −a ♦i −a
Θ)σ′ where i is some nominal fresh to Θ and we get to pick σ′.
By assumption, ♦φ is unwitnessed at a in Θ but since the substitution can
collapse formulas, ♦(φσ) may be witnessed in Θσ by some witnessing nominal j.
In this case, where restriction R2 prevents us from applying the (♦) rule, we let
σ′ = σ(i := j) in the induction hypothesis. Since i occurs only in the extension
the rest of the branch is unaﬀected by this choice, Θ(σ(i := j)) = Θσ, but now
the extension occurs elsewhere at a and the Nom rule justiﬁes it.
⊓⊔

478
A. H. From et al.
Lemma 3 (Unrestricted (♦)). If ♦φ occurs at a in Θ, i is fresh and φ is not
a nominal then we can derive ⊢Θ from a witnessing extension ⊢@iφ−a♦i−aΘ.
Proof. If ♦φ is already witnessed at a in Θ then use Lemma 2 to make i coincide
with the existing witnessing nominal and justify the extension by Nom.
⊓⊔
If Θ consists of blocks B1, B2, . . . , Bn, let Blocks(Θ) = {B1, B2, . . . , Bn}.
The substitution lemma allows us to prove the following:
Lemma 4 (Branch structure). Given inﬁnitely many nominals, we can add,
contract and rearrange blocks: If ⊢Θ and Blocks(Θ) ⊆Blocks(Θ′) then ⊢Θ′.
Proof. By induction on the derivation of ⊢Θ for arbitrary Θ′.
⊓⊔
Lemma 5 (Unrestricted (@) (and (¬@))). If ⊢φ−a Θ, @iφ occurs at b in Θ
and i occurs at a then ⊢Θ.
Proof. Figure 2 shows the derivation where each new branch to the right is known
by Lemma 4 to still have a closing extension.
⊓⊔
b
a
@iφ
i
a
|
φ
⇝
b
a
@iφ
i
i
a
φ
a
|
φ
Nom
⇝
b
a
@iφ
i
a
|
φ
i
GoTo
a
Nom
φ
(@)
Fig. 2. Deriving the unrestricted (@) rule.
Theorem 2 (Completeness). If φ is valid then ⊢Θ where Θ consists of a
single block with φ on it.
Proof. Essentially a modiﬁcation of the proof for ST by Jørgensen et al. [17],
since our system is similar, and we have proved we can lift our restrictions.
⊓⊔
We remark that the completeness proof is an example of what are known
as synthetic approaches to completeness [11,25], which involve reasoning about
maximal consistent sets and their properties. However the completeness proof
for ST distinguishes itself by using maximal sets of entire blocks rather than
plain formulas. One component of the proof is a deﬁnition of when such a set
of blocks is a Hintikka set and thus satisﬁable [17]. In the formalization [13] we
precisely formulate this deﬁnition in the formal language of Isabelle/HOL and
in doing so we discovered a shortcoming in the deﬁnition given by Jørgensen et
al. Essentially, their requirement on propositional symbols fails to take so-called
equivalence of nominals into account, making their model valuation incompatible
with their model existence result.

Formalizing a Seligman-Style Tableau System for Hybrid Logic
479
5
Related Work
Linker formalizes in Isabelle/HOL a semantic embedding of a spatio-temporal
multi-modal logic designed for reasoning about motorway traﬃc which includes a
hybrid logic-inspired at-operator [18]. Linker and Hilscher give a sound labelled
natural deduction proof system for a version of the logic without the hybrid
extension [19]. Doczkal and Smolka formalize hybrid logic with nominals but no
special operators in constructive type theory using the proof assistant Coq. They
do not deﬁne a proof system but give algorithmic proofs of small model theo-
rems and computational decidability of satisﬁability, validity, and equivalence of
formulas [10]. The present work appears to be the ﬁrst proof system for hybrid
logic with a formalized completeness proof.
Formalizations of completeness proofs in Isabelle exist for, among others, a
tableau system and a one-sided sequent calculus for ﬁrst-order logic [14], a nat-
ural deduction system for ﬁrst-order logic [3], a Hilbert system for epistemic
logic [12], and the ﬁrst-order resolution calculus [21]. Blanchette et al. give
abstract proofs of soundness and completeness that can be instantiated for a
range of Gentzen and tableau systems for various ﬂavors of ﬁrst-order logic [7].
Moreover, Blanchette gives an overview of the formalized metatheory of various
logical calculi and automatic provers in Isabelle [6].
6
Future Work
We are currently working on restricting the GoTo and Nom rules to ensure ter-
mination; previous (un-formalized) work has shown via translation to and from
a diﬀerent system that completeness can be preserved and that the resulting
system is terminating [5]. We would like to show termination directly via a
decreasing length argument in the style of Bolander and Blackburn’s work on an
internalized labelled tableau system [8]. Given a sound, complete and terminat-
ing system we want to verify an algorithm based on it and use it as a decision
procedure for basic hybrid logic. Moreover, as the completeness proof that we
formalized is based on reasoning about maximal consistent sets and their proper-
ties, it should be possible to extend it to other key results for hybrid logic which
have been proved by similar forms of reasoning, notably interpolation results [1].
7
Conclusion
We have presented a tableau system for basic hybrid logic whose soundness and
completeness has been formalized in Isabelle/HOL. Moreover, we have shown
how to lift certain restrictions on the rules so that an existing completeness
proof could be formalized and applied. The fact that the completeness proof we
formalized is a synthetic proof suggests that it can be extended to a number of
other key results for hybrid logic that can be found in the literature.

480
A. H. From et al.
References
1. Areces, C., Blackburn, P., Marx, M.: Hybrid logics: characterization, interpolation
and complexity. J. Symb. Logic 66(3), 977–1010 (2001)
2. Areces, C.E.: Logic engineering: the case of description and hybrid logics. Ph.D.
thesis, Institute for Logic, Language and Computation, Amsterdam, The Nether-
lands (2000)
3. Berghofer, S.: First-Order Logic According to Fitting. Archive of Formal Proofs,
August 2007. http://isa-afp.org/entries/FOL-Fitting.html. Formal proof develop-
ment
4. Blackburn, P.: Representation, reasoning, and relational structures: a hybrid logic
manifesto. Logic J. IGPL 8(3), 339–365 (2000). https://doi.org/10.1093/jigpal/8.
3.339
5. Blackburn, P., Bolander, T., Bra¨uner, T., Jørgensen, K.F.: Completeness and ter-
mination for a Seligman-style tableau system. J. Logic Comput. 27(1), 81–107
(2017). https://doi.org/10.1093/logcom/exv052
6. Blanchette, J.C.: Formalizing the metatheory of logical calculi and automatic
provers in Isabelle/HOL (invited talk). In: Mahboubi, A., Myreen, M.O. (eds.)
Proceedings of the 8th ACM SIGPLAN International Conference on Certiﬁed Pro-
grams and Proofs. CPP 2019. pp. 1–13. ACM (2019)
7. Blanchette, J.C., Popescu, A., Traytel, D.: Soundness and completeness proofs by
coinductive methods. J. Autom. Reasoning 58(1), 149–179 (2016). https://doi.org/
10.1007/s10817-016-9391-3
8. Bolander, T., Blackburn, P.: Termination for hybrid tableaus. J. Logic Comput.
17(3), 517–554 (2007). https://doi.org/10.1093/logcom/exm014
9. Bra¨uner, T.: Hybrid Logic and its Proof-Theory. Springer, Dordrecht (2010)
10. Doczkal, C., Smolka, G.: Constructive formalization of hybrid logic with eventual-
ities. In: Jouannaud, J.-P., Shao, Z. (eds.) CPP 2011. LNCS, vol. 7086, pp. 5–20.
Springer, Heidelberg (2011). https://doi.org/10.1007/978-3-642-25379-9 3
11. Fitting, M.: Proof Methods for Modal and Intuitionistic Logics, vol. 169. Springer,
Heidelberg (1983). https://doi.org/10.1007/978-94-017-2794-5
12. From, A.H.: Epistemic Logic. Archive of Formal Proofs, October 2018. http://isa-
afp.org/entries/Epistemic Logic.html. Formal proof development
13. From, A.H.: Formalizing a Seligman-Style Tableau System for Hybrid Logic.
Archive of Formal Proofs, December 2019. http://isa-afp.org/entries/Hybrid
Logic.html. Formal proof development
14. From, A.H.: A Sequent Calculus for First-Order Logic. Archive of Formal Proofs,
July 2019. http://isa-afp.org/entries/FOL Seq Calc1.html. Formal proof develop-
ment
15. From, A.H.: Hybrid logic. Master’s thesis, Technical University of Denmark, Jan-
uary 2020
16. Horrocks, I., Glimm, B., Sattler, U.: Hybrid logics and ontology languages. Elec-
tron. Notes Theoret. Comput. Sci. 174(6), 3–14 (2007). https://doi.org/10.1016/
j.entcs.2006.11.022
17. Jørgensen, K.F., Blackburn, P., Bolander, T., Bra¨uner, T.: Synthetic completeness
proofs for Seligman-style tableau systems. In: Proceedings of the 11th Conference
on Advances in Modal Logic, held in Budapest, Hungary, 30 August–2 September
2016, vol. 11, pp. 302–321 (2016)
18. Linker, S.: Hybrid Multi-Lane Spatial Logic. Archive of Formal Proofs, Novem-
ber 2017. http://isa-afp.org/entries/Hybrid Multi Lane Spatial Logic.html. For-
mal proof development

Formalizing a Seligman-Style Tableau System for Hybrid Logic
481
19. Linker, S., Hilscher, M.: Proof theory of a multi-lane spatial logic. Log. Methods
Comput. Sci. 11(3) (2015). https://doi.org/10.2168/LMCS-11(3:4)2015
20. Nipkow, T., Wenzel, M., Paulson, L.C. (eds.): Isabelle/HOL - A Proof Assistant
for Higher-Order Logic. LNCS, vol. 2283. Springer, Heidelberg (2002). https://doi.
org/10.1007/3-540-45949-9
21. Schlichtkrull, A.: The Resolution Calculus for First-Order Logic. Archive of Formal
Proofs, June 2016. http://isa-afp.org/entries/Resolution FOL.html. Formal proof
development
22. Seligman, J., Liu, F., Girard, P.: Facebook and the epistemic logic of friendship. In:
Schipper, B.C. (ed.) Proceedings of the 14th Conference on Theoretical Aspects of
Rationality and Knowledge (TARK 2013), Chennai, India, pp. 229–238 (2013)
23. Seligman, J.: The logic of correct description. In: de Rijke, M. (ed.) Advances in
Intensional Logic. Applied Logic Series, vol. 7, pp. 107–135. Springer, Dordrecht
(1997). https://doi.org/10.1007/978-94-015-8879-9 5
24. Seligman, J.: Internalization: the case of hybrid logics. J. Logic Comput. 11(5),
671–689 (2001). https://doi.org/10.1093/logcom/11.5.671
25. Smullyan, R.M.: First-Order Logic. Dover Publications, New York (1995)

NP Reasoning in the Monotone
µ-Calculus
Daniel Hausmann(B) and Lutz Schr¨oder(B)
Friedrich-Alexander-Universit¨at Erlangen-N¨urnberg, Erlangen, Germany
{daniel.hausmann,lutz.schroeder}@fau.de
Abstract. Satisﬁability checking for monotone modal logic is known
to be (only) NP-complete. We show that this remains true when the
logic is extended with alternation-free ﬁxpoint operators as well as the
universal modality; the resulting logic – the alternation-free monotone
μ-calculus with the universal modality – contains both concurrent propo-
sitional dynamic logic (CPDL) and the alternation-free fragment of game
logic as fragments. We obtain our result from a characterization of satis-
ﬁability by means of B¨uchi games with polynomially many Eloise nodes.
1
Introduction
Monotone modal logic diﬀers from normal modal logics (such as K [4], equiv-
alent to the standard description logic ALC [1]) by giving up distribution of
conjunction over the box modality, but retaining monotonicity of the modalities.
Its semantics is based on (monotone) neighbourhood models instead of Kripke
models. Monotone modalities have been variously used as epistemic operators
that restrict the combination of knowledge by epistemic agents [27]; as next-step
modalities in the evolution of concurrent systems, e.g. in concurrent proposi-
tional dynamic logic (CPDL) [24]; and as game modalities in systems where
one transition step is determined by moves of two players, e.g. in Parikh’s game
logic [7,12,20,23]. The monotonicity condition suﬃces to enable formation of
ﬁxpoints; one thus obtains the monotone μ-calculus [8], which contains both
CPDL and game logic as fragments (indeed, the recent proof of completeness of
game logic [7] is based on embedding game logic into the monotone μ-calculus).
While many modal logics (including K/ALC) have PSpace-complete satis-
ﬁability problems in the absence of ﬁxpoints, it is known that satisﬁability in
monotone modal logic is only NP-complete [27] (the lowest possible complexity
given that the logic has the full set of Boolean connectives). In the present paper,
we show that the low complexity is preserved under two extensions that usu-
ally cause the complexity to rise from PSpace-complete to ExpTime-complete:
Adding the universal modality (equivalently global axioms or, in description logic
parlance, a general TBox) and alternation-free ﬁxpoints; that is, we show that
satisﬁability checking in the alternation-free fragment of the monotone μ-calculus
with the universal modality [8] is only NP-complete. This logic subsumes both
CPDL and the alternation-free fragment of game logic [23]. Thus, our results
c
⃝Springer Nature Switzerland AG 2020
N. Peltier and V. Sofronie-Stokkermans (Eds.): IJCAR 2020, LNAI 12166, pp. 482–499, 2020.
https://doi.org/10.1007/978-3-030-51074-9_28

NP Reasoning in the Monotone μ-Calculus
483
imply that satisﬁability checking in these logics is only NP-complete (the best
previously known upper bound being ExpTime in both cases [20,23,24]); for
comparison, standard propositional dynamic logic (PDL) and in fact already the
extension of K with the universal modality are ExpTime-hard. (Our results thus
seemingly contradict previous results on ExpTime-completeness of CPDL. How-
ever, these results rely on embedding standard PDL into CPDL, which requires
changing the semantics of CPDL to interpret atomic programs as sequential
programs, i.e. as relations rather than neighbourhood systems [24].) Our results
are based on a variation of the game-theoretic approach to μ-calculi [19]. Speciﬁ-
cally, we reduce satisﬁability checking to the computation of winning regions in a
satisﬁability game that has exponentially many Abelard-nodes but only polyno-
mially many Eloise-nodes, so that history-free winning strategies for Eloise have
polynomial size. From this approach we also derive a polynomial model property.
Organization.
We recall basics on ﬁxpoints and games in Sect. 2, and the
syntax and semantics of the monotone μ-calculus in Sect. 3. We discuss a key
technical tool, formula tracking, in Sect. 4. We adapt the standard tableaux sys-
tem to the monotone μ-calculus in Sect. 5. In Sect. 6, we establish our main
results using a game characterization of satisﬁability.
2
Notation and Preliminaries
We ﬁx basic concepts and notation on ﬁxpoints and games.
Fixpoints.
Let U be a set; we write P(U) for the powerset of U. Let f :
P(U) →P(U) be a monotone function, i.e. f(A) ⊆f(B) whenever A ⊆B ⊆U.
By the Knaster-Tarski ﬁxpoint theorem, the greatest (GFP) and least (LFP)
ﬁxpoints of f are given by
LFP f = {V ⊆U | f(V ) ⊆V }
GFP f = {V ⊆U | V ⊆f(V )},
and are thus also the least preﬁxpoint (f(V ) ⊆V ) and greatest postﬁxpoint
(V ⊆f(V )) of f, respectively. For V ⊆U and n ∈N, we deﬁne f n(V ) as
expected by f 0(V ) = V and f n+1(V ) = f(f n(V )). If U is ﬁnite, then LFP f =
f |U|(∅) and GFP f = f |U|(U) by Kleene’s ﬁxpoint theorem.
Inﬁnite Words and Games.
We denote the sets of ﬁnite and inﬁnite
sequences of elements of a set U by U ∗and U ω, respectively. We often view
sequences τ = u1, u2, . . . in U ∗(or U ω) as partial (total) functions τ : N ⇀U,
writing τ(i) = ui. We write
Inf(τ) = {u ∈U | ∀i ∈N. ∃j > i. τ(j) = u}
for the set of elements of U that occur inﬁnitely often in τ.
A B¨uchi game G = (V, E, v0, F) consists of a set V of nodes, partitioned
into the sets V∃of Eloise-nodes and V∀of Abelard-nodes, a set E ⊆V × V
of moves, an initial node v0, and a set F ⊆V of accepting nodes. We write

484
D. Hausmann and L. Schr¨oder
E(v) = {v′ | (v, v′) ∈E}. For simplicity, assume that v0 ∈V∃and that the game
is alternating, i.e. E(v) ⊆V∀for all v ∈V∃, and E(v) ⊆V∃for all v ∈V∀(our
games will have this shape). A play of G is a sequence τ = v0, v1, . . . of nodes
such that (vi, vi+1) ∈E for all i ≥0 and τ is either inﬁnite or ends in a node
without outgoing moves. Eloise wins a play τ if and only if τ is ﬁnite and ends in
an Abelard-node or τ is inﬁnite and Inf(τ)∩F ̸= ∅, that is, τ inﬁnitely often visits
an accepting node. A history-free Eloise-strategy is a partial function s : V∃⇀V
such that s(v0) is deﬁned, and whenever s(v) is deﬁned, then (v, s(v)) ∈E and
s(v′) is deﬁned for all v′ ∈E(s(v)). A play v0, v1, . . . is an s-play if vi+1 = s(vi)
whenever vi ∈V∃. We say that s is a winning strategy if Eloise wins every s-play,
and that Eloise wins G if there is a winning strategy for Eloise. B¨uchi games
are history-free determined, i.e. in every B¨uchi game, one of the players has a
history-free winning strategy [17].
3
The Monotone µ-Calculus
We proceed to recall the syntax and semantics of the monotone μ-calculus.
Syntax. We ﬁx countably inﬁnite sets P, A and V of atoms, atomic programs
and (ﬁxpoint) variables, respectively; we assume that P is closed under duals
(i.e. atomic negation), i.e. p ∈P implies p ∈P, where p = p. Formulae of the
monotone μ-calculus (in negation normal form) are then deﬁned by the grammar
ψ, φ := ⊥| ⊤| p | ψ ∧φ | ψ ∨φ | ⟨a⟩ψ | [a]ψ | X | ηX.ψ
where p ∈P, a ∈A, X ∈V; throughout, we use η ∈{μ, ν} to denote extremal
ﬁxpoints. As usual, μ and ν are understood as taking least and greatest ﬁxpoints,
respectively, and bind their variables, giving rise to the standard notion of free
variable in a formula ψ. We write FV(ψ) for the set of free variables in ψ, and
say that ψ is closed if FV(ψ) = ∅. Negation ¬ is not included but can be deﬁned
by taking negation normal forms as usual, with ¬p = p. We refer to formulae of
the shape [a]φ or ⟨a⟩φ as ( a-)modal literals. As indicated in the introduction, the
modalities [a], ⟨a⟩have been equipped with various readings, recalled in more
detail in Example 7.
Given a closed formula ψ, the closure cl(ψ) of ψ is deﬁned to be the least set
of formulae that contains ψ and satisﬁes the following closure properties:
if ψ1 ∧ψ2 ∈cl(ψ) or ψ1 ∨ψ2 ∈cl(ψ), then {ψ1, ψ2} ⊆cl(ψ),
if ⟨a⟩ψ1 ∈cl(ψ) or [a]ψ1 ∈cl(ψ), then ψ1 ∈cl(ψ),
if ηX.ψ1 ∈cl(ψ), then ψ1[ηX.ψ1/X] ∈cl(ψ),
where ψ1[ηX.ψ1/X] denotes the formula that is obtained from ψ1 by replacing
every free occurrence of X in ψ1 with ηX.ψ1. Note that all formulae in cl(ψ)
are closed. We deﬁne the size |ψ| of ψ as |ψ| = |cl(ψ)|. A formula ψ is guarded
if whenever ηX. φ ∈cl(ψ), then all free occurrences of X in φ are under the
scope of at least one modal operator. We generally restrict to guarded formulae;

NP Reasoning in the Monotone μ-Calculus
485
see however Remark 1. A closed formula ψ is clean if all ﬁxpoint variables in ψ
are bound by exactly one ﬁxpoint operator. Then θ(X) denotes the subformula
ηX.φ that binds X in ψ, and X is a least (greatest) ﬁxpoint variable if η = μ
(η = ν). We deﬁne a partial order ≥μ on the least ﬁxpoint variables in ρ1 and ρ0
by X ≥μ Y iﬀθ(Y ) is a subformula of θ(X) and θ(Y ) is not in the scope of a
greatest ﬁxpoint operator within θ(X) (i.e. there is no greatest ﬁxpoint operator
between μX and μY ). The index idx(X) of such a ﬁxpoint variable X is
idx(X) = |{Y ∈V | Y ≥μ X}|,
For a subformula φ of ψ, we write idx(φ) = max{idx(X) | X ∈FV(φ)}. We
denote by θ∗(φ0) the closed formula that is obtained from a subformula φ0 of ψ
by repeatedly replacing free variables X with θ(X). Formally, we deﬁne θ∗(φ0)
as φ|FV(φ0)|, where φi+1 is deﬁned inductively from φi. If φi is closed, then put
φi+1 = φi. Otherwise, pick the variable Xi ∈FV(φi) with the greatest index and
put φi+1 = φi[θ(Xi)/Xi]. Then idx(φi+1) < idx(φi), so θ∗(φ0) really is closed;
moreover, one can show that θ∗(φ0) ∈cl(ψ). A clean formula is alternation-free
if none of its subformulae contains both a free least and a free greatest ﬁxpoint
variable. Finally, ψ is irredundant if X ∈FV(φ) whenever ηX.φ ∈cl(ψ).
Remark 1. We have deﬁned the size of formulae as the cardinality of their clo-
sure, implying a very compact representation [3]. Our upper complexity bounds
thus become stronger, i.e. they hold even for this small measure of input size.
Moreover, the restriction to guarded formulae is then without loss of general-
ity, since one has a guardedness transformation that transforms formulae into
equivalent guarded ones, with only polynomial blowup of the closure [3].
Semantics. The monotone μ-calculus is interpreted over neighbourhood models
(or epistemic structures [27]) F = (W, N, I) where N : A×W →2(2W ) assigns to
each atomic program a and each state w a set N(a, w) ⊆2W of a-neighbourhoods
of w, and I : P →2W interprets propositional atoms such that I(p) = W \ I(p)
for p ∈P (by 2, we denote the set {⊥, ⊤} of Boolean truth values, and 2W is
the set of maps W →2, which is in bijection with the powerset P(W)). Given
such an F, each formula ψ is assigned an extension ψσ ⊆W that additionally
depends on a valuation σ : V →2W , and is inductively deﬁned by
pσ = I(p)
Xσ = σ(X)
ψ ∧φσ = ψσ ∩φσ
ψ ∨φσ = ψσ ∪φσ
⟨a⟩ψσ = {w ∈W | ∃S ∈N(a, w). S ⊆ψσ}
μX.ψσ = LFPψX
σ
[a]ψσ = {w ∈W | ∀S ∈N(a, w). S ∩ψσ ̸= ∅}
νX.ψσ = GFPψX
σ
where, for U ⊆W and ﬁxpoint variables X, Y
∈V, we put ψX
σ (U) =
ψσ[X→U], (σ[X →U])(X) = U and (σ[X →U])(Y ) = σ(Y ) if X ̸= Y .
We omit the dependence on F in the notation [[φ]]σ, and when necessary clarify
the underlying neighbourhood model by phrases such as ‘in F’. If ψ is closed,
then its extension does not depend on the valuation, so we just write ψ. A

486
D. Hausmann and L. Schr¨oder
closed formula ψ is satisﬁable if there is a neighbourhood model F such that
ψ ̸= ∅in F; in this case, we also say that ψ is satisﬁable over F. Given a set
Ψ of closed formulae, we write Ψ = 
ψ∈Ψψ. An (inﬁnite) path through a
neighbourhood model (W, N, I) is a sequence x0, x1, . . . of states xi ∈W such
that for all i ≥0, there are a ∈A and S ∈N(a, xi) such that xi+1 ∈S.
The soundness direction of our game characterization will rely on the follow-
ing immediate property of the semantics, which may be seen as soundness of a
modal tableau rule [5].
Lemma 2
[27, Proposition 3.8]. If [a]φ ∧⟨a⟩ψ is satisﬁable over a neighbour-
hood model F, then φ ∧ψ is also satisﬁable over F.
Remark 3. The dual box and diamond operators [a] and ⟨a⟩are completely
symmetric, and indeed the notation is not uniform in the literature. Our use of
[a] and ⟨a⟩is generally in agreement with work on game logic [20] and CPDL [24];
in work on monotone modal logics and the monotone μ-calculus, the roles of box
and diamond are often interchanged [8,26,27].
Remark 4. The semantics may equivalently be presented in terms of monotone
neighbourhood models, where the set of a-neighbourhoods of a state is required
to be upwards closed under subset inclusion [8,12,20,23]. In this semantics, the
interpretation of ⟨a⟩φ simpliﬁes to just requiring that the extension of φ is an
a-neighbourhood of the current state. We opt for the variant where upwards
closure is instead incorporated into the interpretation of the modalities, so as
to avoid having to distinguish between monotone neighbourhood models and
their representation as upwards closures of (plain) neighbourhood models, e.g.
in small model theorems.
We further extend the expressiveness of the logic (see also Remark 9) by adding
global assumptions or equivalently the universal modality:
Deﬁnition 5 (Global assumptions). Given a closed formula φ, a φ-model is
a neighbourhood model F = (W, N, I) in which [[φ]] = W. A formula ψ is φ-
satisﬁable if ψ is satisﬁable over some φ-model; in this context, we refer to φ as
the global assumption, and to the problem of deciding whether ψ is φ-satisﬁable
as satisﬁability checking under global assumptions.
We also deﬁne an extension of the monotone μ-calculus, the monotone μ-calculus
with the universal modality, by adding two alternatives
· · · | [∀]φ | [∃]φ
to the grammar, in both alternatives restricting φ to be closed. The deﬁnition
of the semantics over a neighbourhood model (W, N, I) and valuation σ is cor-
respondingly extended by [[[∀]φ]]σ = W if [[φ]]σ = W, and [[[∀]φ]]σ = ∅otherwise;
dually, [[[∃]φ]]σ = W if [[φ]]σ ̸= ∅, and [[[∃]φ]]σ = ∅otherwise. That is, [∀]φ says
that φ holds in all states of the model, and [∃]φ that φ holds in some state.

NP Reasoning in the Monotone μ-Calculus
487
Remark 6. In description logic, global assumptions are typically called (gen-
eral) TBoxes or terminologies [1]. For many next-step modal logics (i.e. modal
logics without ﬁxpoint operators), satisﬁability checking becomes harder under
global assumptions. A typical case is the standard modal logic K (corresponding
to the description logic ALC), in which (plain) satisﬁability checking is PSpace-
complete [15] while satisﬁability checking under global assumptions is ExpTime-
complete [6,9]. Our results show that such an increase in complexity does not
happen for monotone modalities.
For purposes of satisﬁability checking, the universal modality and global
assumptions are mutually reducible in a standard manner, where the non-trivial
direction (from the universal modality to global assumptions) is by guessing
beforehand which subformulae [∀]φ, [∃]φ hold (see also [11]).
Example 7. 1. In epistemic logic, neighbourhood models have been termed
epistemic structures [27]. In this context, the a ∈A are thought of as agents,
the a-neighbourhoods of a state w are the facts known to agent a in w, and
correspondingly the reading of [a]φ is ‘a knows φ’. The use of (monotone)
neighbourhood models and the ensuing failure of normality imply that agents
can still weaken facts that they know but are not in general able to combine
them, i.e. knowing φ and knowing ψ does not entail knowing φ ∧ψ [27].
2. In concurrent propositional dynamic logic (CPDL), a-neighbourhoods of a
state are understood as sets of states that can be reached concurrently, while
the choice between several a-neighbourhoods of a state models sequential non-
determinism. CPDL indexes modalities over composite programs α, formed
using tests ?φ and the standard operations of propositional dynamic logic
(PDL) (union ∪, sequential composition ‘;’, and Kleene star (−)∗) and addi-
tionally intersection ∩. It forms a sublogic of Parikh’s game logic, recalled
next, and thus in particular translates into the monotone μ-calculus.
As indicated in the introduction, CPDL satisﬁability checking has been
shown to be ExpTime-complete [24], seemingly contradicting our results
(Corollary 25). Note however that the interpretation of atomic programs in
CPDL, originally deﬁned in terms of neighbourhood systems [24, p. 453],
is, for purposes of the ExpTime-hardness proof, explicitly changed to rela-
tions [24, pp. 458–459]; ExpTime-hardness then immediately follows since
PDL becomes a sublogic of CPDL [24, p. 461]. Our NP bound applies to the
original semantics.
3. Game logic [20,23] extends CPDL by a further operator on programs, dualiza-
tion (−)d, and reinterprets programs as games between two players Angel and
Demon; in this view, dualization just corresponds to swapping the roles of the
players. In comparison to CPDL, the main eﬀect of dualization is that one
obtains an additional demonic iteration operator (−)×, distinguished from
standard iteration (−)∗by letting Demon choose whether or not to continue
the iteration. A game logic formula is alternation-free if it does not contain
nested occurrences (unless separated by a test) of (−)× within (−)∗or vice
versa [23].

488
D. Hausmann and L. Schr¨oder
Enqvist et al. [7] give a translation of game logic into the monotone μ-
calculus that is quite similar to Pratt’s [25] translation of PDL into the stan-
dard μ-calculus. The translation (−)♯is deﬁned by commutation with all
Boolean connectives and by
(⟨γ⟩φ)♯= τγ(φ♯),
in mutual recursion with a function τγ that translates the eﬀect of applying
⟨γ⟩into the monotone μ-calculus. (Boxes [γ] can be replaced with ⟨γd⟩). We
refrain from repeating the full deﬁnition of τγ by recursion over γ; some key
clauses are
τγ∩δ(ψ)=τγ(ψ)∧τδ(ψ)
τγ∗(ψ)=μX. (ψ∨τγ(X))
τγ×(ψ) = νY. (ψ∧τγ(Y ))
where in the last two clauses, X and Y are chosen as fresh variables (for read-
ability, we gloss over a more precise treatment of this point given in [7]). The
ﬁrst clause (and a similar one for ∪) appear at ﬁrst sight to cause exponential
blowup but recall that we measure the size of formulae by the cardinality
of their closure; in this measure, there is in fact no blowup. The translated
formula ψ♯need not be guarded as the clauses for ∗and × can introduce
unguarded ﬁxpoint variables; as mentioned in Remark 1, we can however
apply the guardedness transformation, with only quadratic blowup of the
closure [3].
Under this translation, the alternation-free fragment of game logic ends
up in the (guarded) alternation-free fragment of the monotone μ-calculus.
For later use, we note
Lemma 8. The monotone μ-calculus with the universal modality has the ﬁnite
model property.
Proof (sketch). We reduce to global assumptions as per Remark 6, and proceed
by straightforward adaptation of the translations of monotone modal logic [14]
and game logic [23] into the relational μ-calculus, thus inheriting the ﬁnite model
property [2]. This translation is based on turning neighbourhoods into additional
states, connected to their elements via a fresh relation e. Then, e.g., the monotone
modality [a] (in our notation, cf. Remark 3) is translated into [a]⟨e⟩(relational
modalities). Moreover, we translate a global assumption φ into a formula saying
that all reachable states satisfy φ, expressed in the μ-calculus in a standard
fashion.
⊓⊔
Remark 9. In the relational μ-calculus, we can encode a modality ⊠‘in all
reachable states’, generalizating the AG operator from CTL. As already indicated
in the proof of Lemma 8, this modality allows for a straightforward reduction
of satisﬁability under global assumptions to plain satisﬁablity in the relational
μ-calculus: A formula ψ is satisﬁable under the global assumption φ iﬀψ ∧⊠φ
is satisﬁable, where ‘if’ is shown by restricting the model to reachable states. To

NP Reasoning in the Monotone μ-Calculus
489
motivate separate consideration of the universal modality, we brieﬂy argue why
an analogous reduction does not work in the monotone μ-calculus.
It is not immediately clear what reachability would mean on neighbourhood
models. We can however equivalently rephrase the deﬁnition of ⊠in the relational
case to let ⊠φ mean ‘the present state is contained in a submodel in which
every state satisﬁes φ’, where as usual a submodel of a relational model C with
state set W is a model C′ with state set W ′ ⊆W such that the graph of
the inclusion W ′ →W is a bisimulation from C′ to C. We thus refer to ⊠
as the submodel modality. This notion transfers to neighbourhood models using
a standard notion of bisimulation: A monotone bisimulation [21,22] between
neighbourhood models (W1, N1, I1), (W2, N2, I2) is a relation S ⊆W1 ×W2 such
that whenever (x, y) ∈S, then
– for all a ∈A and A ∈N1(a, x), there is B ∈N2(a, y) such that for all v ∈B,
there is u ∈A such that (u, v) ∈S,
– for all a ∈A and B ∈N2(a, y), there is A ∈N1(a, x) such that for all u ∈A,
there is a v ∈B such that (u, v) ∈S,
– for all p ∈P, x ∈I1(p) if and only if y ∈I2(p).
We then deﬁne a submodel of a neighbourhood model F = (W, N, I) to be a
neigbourhood model F ′ = (W ′, N ′, I′) such that W ′ ⊆W and the graph of the
inclusion W ′ →W is a monotone bisimulation between F ′ and F. If (x, y) ∈S
for some monotone bisimulation S, then x and y satisfy the same formulae
in the monotone μ-calculus [8]. It follows that the submodel modality ⊠on
neighbourhood models, deﬁned verbatim as in the relational case, allows for
the same reduction of satisﬁability under global assumptions as the relational
submodel modality.
However, the submodel modality fails to be expressible in the monotone
μ-calculus, as seen by the following example. Let F1 = (W1, N1, I1), F2 =
(W2, N2, I2) be the neighbourhood models given by W1 = {x1, u1, v11, v12},
W2 = {x2, u2, v2}, N1(a, x1) = {{u1, v11}, {v11, v12}}, N2(a, x2) = {{v2}},
N1(b, y) = N2(b, z) = ∅for (b, y) ̸= (a, x1), (b, z) ̸= (a, x2), I1(p) =
{x1, v11, v12}, I2(p) = {x2, v2}, and I1(q) = I2(q) = ∅for q ̸= p. Then
S = {(x1, x2), (u1, u2), (v11, v2), (v12, v2)} is a monotone bisimulation, so x1
and x2 satisfy the same formulae in the monotone μ-calculus. However, x2 satis-
ﬁes ⊠p because x2 is contained in a submodel with set {x2, v2} of states, while x1
does not satisfy ⊠p.
4
Formula Tracking
A basic problem in the construction of models in ﬁxpoint logics is to avoid inﬁnite
unfolding of least ﬁxpoints, also known as inﬁnite deferral. To this end, we use
a tracking function to follow formulae along paths in prospective models. For
unrestricted μ-calculi, inﬁnite unfolding of least ﬁxpoints is typically detected
by means of a parity condition on tracked formulae. However, since we restrict
to alternation-free formulae, we can instead use a B¨uchi condition to detect (and

490
D. Hausmann and L. Schr¨oder
then reject) such inﬁnite unfoldings by sequential focussing on sets of formulae
in the spirit of focus games [16].
We ﬁx closed, clean, irredundant formulae ρ1 and ρ0, aiming to check ρ0-
satisﬁability of ρ1; we also require both ρ1 and ρ0 to be alternation-free. We put
cl = cl(ρ1) ∪cl(ρ0) and n = |cl|.
Next we formalize our notion of deferred formulae that originate from least
ﬁxpoints; these are the formulae for which inﬁnite unfolding has to be avoided.
Deﬁnition 10 (Deferrals). A formula φ ∈cl(ψ) is a deferral if there is a
subformula χ of ψ such that FV(χ) contains a least ﬁxpoint variable and φ =
θ∗(χ). We put dfr = {φ ∈cl | φ is a deferral}.
Since we assume formulas to be alternation-free, no formula νX. φ is a deferral.
Example 11. For ψ = μX. [b]⊤∨⟨a⟩X, the formula [b]⊤∨⟨a⟩ψ = ([b]⊤∨
⟨a⟩X)[ψ/X] ∈cl(ψ) is a deferral since X is a least ﬁxpoint variable and occurs
free in [b]⊤∨⟨a⟩X; the formula [b]⊤on the other hand is not a deferral since it
cannot be obtained from a subformula of ψ by replacing least ﬁxpoint variables
with their binding ﬁxpoint formulae.
We proceed to deﬁne a tracking function that nondeterministically tracks defer-
rals along paths in neighbourhood models.
Deﬁnition 12 (Tracking function). We deﬁne an alphabet Σ = Σp ∪Σm for
traversing the closure, separating propositional and modal traversal, by
Σp = {(φ0 ∧φ1, 0), (φ0 ∨φ1, b), (ηX.φ0, 0) |
φ0, φ1 ∈cl, η ∈{μ, ν}, X ∈V, b ∈{0, 1}}
Σm = {(⟨a⟩φ0, [a]φ1) | φ0, φ1 ∈cl, a ∈A}.
The tracking function δ : dfr × Σ →P(dfr) is deﬁned by δ(foc, (χ, b)) = {foc}
for foc ∈dfr and (χ, b) ∈Σp with χ ̸= foc, and by
δ(foc, (foc, b)) =
⎧
⎪
⎨
⎪
⎩
{φ0, φ1} ∩dfr
foc = φ0 ∧φ1
{φb} ∩dfr
foc = φ0 ∨φ1
{φ0[θ(X)/X]}
foc = μX. φ0
δ(foc, (⟨a⟩φ0, [a]φ1)) =

{φ0}
foc = ⟨a⟩φ0
{φ1}
foc = [a]φ1,
noting that μX. φ0 ∈dfr implies φ0[θ(X)/X] ∈dfr. We extend δ to sets Foc ⊆dfr
by putting δ(Foc, l) = 
foc∈Foc δ(foc, l) for l ∈Σ. We also extend δ to words in
the obvious way; e.g. we have δ(foc, w) = [a]foc for
foc = μX. (p∨(⟨b⟩p∧[a]X))
w = (foc, 0), (p∨(⟨b⟩p∧[a]foc), 1), (⟨b⟩p∧[a]foc, 0).

NP Reasoning in the Monotone μ-Calculus
491
Remark 13. Formula tracking can be modularized in an elegant way by using
tracking automata to accept exactly the paths that contain inﬁnite deferral of
some least ﬁxpoint formula. While tracking automata for the full μ-calculus are in
general nondeterministic parity automata [10], the automata for alternation-free
formulae are nondeterministic co-B¨uchi automata. Indeed, our tracking func-
tion δ can be seen as the transition function of a nondeterministic co-B¨uchi
automaton with set dfr of states and alphabet Σ in which all states are accept-
ing. Since models have to be constructed from paths that do not contain inﬁnite
deferral of some least ﬁxpoint formula, the tracking automata have to be comple-
mented in an intermediate step (which is complicated by nondeterminism) when
checking satisﬁability. In our setting, the complemented automata are determin-
istic B¨uchi automata obtained using a variant of the Miyano-Hayashi construc-
tion [18] similarly as in our previous work on ExpTime satisﬁability checking
in alternation-free coalgebraic μ-calculi [13]; for brevity, this determinization
procedure remains implicit in our satisﬁability games (see Deﬁnition 20 below).
To establish our game characterization of satisﬁability, we combine the tracking
function δ with a function for propositional transformation of formula sets guided
by letters from Σp, embodied in the function γ deﬁned next.
Deﬁnition 14 (Propositional transformation). We deﬁne γ : P(cl)×Σp →
P(cl) on Γ ⊆cl, (χ, b) ∈Σp by γ(Γ, (χ, b)) = Γ if χ /∈Γ, and
γ(Γ, (χ, b)) = (Γ \ χ) ∪
⎧
⎪
⎨
⎪
⎩
{φb}
χ = φ0 ∨φ1
{φ0, φ1}
χ = φ0 ∧φ1
{φ0[ηX. φ0/X]}
χ = ηX. φ0
if χ ∈Γ. We extend γ to words over Σp in the obvious way.
Example 15. Take φ
=
χ ∨νX. (ψ1 ∧ψ2) and w
=
(χ ∨νX. (ψ1 ∧
ψ2), 1), (νX.(ψ1 ∧ψ2), 0), ((ψ1 ∧ψ2)[θ(X)/X], 1). The letter (χ∨νX. (ψ1 ∧ψ2), 1)
picks the right disjunct and (νX. (ψ1 ∧ψ2), 0) passes through the ﬁxpoint oper-
ator νX to reach ψ1 ∧ψ2; the letter (ψ1 ∧ψ2, 1) picks both conjuncts. Thus,
γ({φ}, w) = {ψ1[θ(X)/X], ψ2[θ(X)/X]}.
5
Tableaux
As a stepping stone between neighbourhood models and satisﬁability games, we
now introduce tableaux which are built using a variant of the standard tableau
rules [10] (each consisting of one premise and a possibly empty set of conclu-
sions), where the modal rule (⟨a⟩) reﬂects Lemma 2, taking into account the
global assumption ρ0:
(⊥)
Γ, ⊥
()
Γ, p, p
(∧)
Γ, φ1 ∧φ2
Γ, φ1, φ2
(∨)
Γ, φ1 ∨φ2
Γ, φ1
Γ, φ2
(⟨a⟩)
Γ, ⟨a⟩φ1, [a]φ2
φ1, φ2, ρ0
(η)
Γ, ηX. φ
Γ, φ[ηX. φ/X]

492
D. Hausmann and L. Schr¨oder
(for a ∈A, p ∈P); we usually write rule instances with premise Γ and conclu-
sion Θ = Γ1, . . . , Γn (n ≤2) inline as (Γ/Θ). Looking back at Sect. 4, we see
that letters in Σ designate rule applications, e.g. the letter (⟨a⟩φ1, [a]φ2) ∈Σm
indicates application of (⟨a⟩) to formulae ⟨a⟩φ1, [a]φ2.
Deﬁnition 16 (Tableaux). Let states denote the set of (formal) states, i.e.
sets Γ ⊆cl such that ⊥/∈Γ, {p, p} ̸⊆Γ for all p ∈P, and such Γ does not
contain formulae that contain top-level occurrences of the operators ∧, ∨, or ηX.
A pre-tableau is a directed graph (W, L), consisting of a ﬁnite set W of nodes
labelled with subsets of cl by a labeling function l : W →P(cl), and of a relation
L ⊆W × W such that for all nodes v ∈W with label l(v) = Γ ∈states and all
applications (Γ/Γ1, . . . , Γn) of a tableau rule to Γ, there is an edge (v, w) ∈L
such that l(w) = Γi for some 1 ≤i ≤n. For nodes with label l(v) = Γ /∈states,
we require that there is exactly one node w ∈W such that (v, w) ∈L; then we
demand that there is an application (Γ/Γ1, . . . , Γn) of a non-modal rule to Γ such
that l(w) = Γi for some 1 ≤i ≤n. Finite or inﬁnite words over Σ then encode
sequences of rule applications (and choices of conclusions for disjunctions). That
is, given a starting node v, they encode branches with root v, i.e. (ﬁnite or
inﬁnite) paths through (W, L) that start at v.
Deferrals are tracked along rule applications by means of the function δ. E.g.
for a deferral φ0 ∨φ1, the letter l = (φ0 ∨φ1, 0) identiﬁes application of (∨)
to {φ0 ∨φ1}, and the choice of the left disjunct; then φ0 ∨φ1 is tracked from
a node with label Γ ∪{φ0 ∨φ1} to a successor node with label Γ ∪{φ0} if
φ0 ∈δ(φ0 ∨φ1, l), that is, if φ0 is a deferral. A trace (of φ0) along a branch with
root v (whose label contains φ0) encoded by a word w is a (ﬁnite or inﬁnite)
sequence t = φ0, φ1 . . . of formulae such that φi+1 ∈δ(φi, w(i)). A tableau is a
ﬁnite pre-tableau in which all traces are ﬁnite, and a tableau is a tableau for ρ1
if some node label contains ρ1.
Given a tableau (W, L) and a node v ∈W, let tab(v) (for tableau timeout)
denote the least number m such that for all formulae φ in l(v) and all branches
of (W, L) that are rooted at v, all traces of φ along the branch have length at
most m; such an m always exists by the deﬁnition of tableaux.
To link models and tableaux, we next deﬁne an inductive measure on unfolding
of least ﬁxpoint formulae in models.
Deﬁnition 17 (Extension under timeouts). Let k be the greatest index of
any least ﬁxpoint variable in ψ, and let (W, N, I) be a ﬁnite neighbourhood
model. Then a timeout is a vector m = (m1, . . . , mk) of natural numbers mi ≤
|W|. For 1 ≤i ≤k such that mi > 0, we put m@i = (m1, . . . , mi−1, mi −
1, |W|, . . . , |W|). Then m >l m@i, where >l denotes lexicographic ordering. For
φ ∈cl, we inductively deﬁne the extension φm under timeout m by φm = φ

NP Reasoning in the Monotone μ-Calculus
493
for φ /∈dfr and, for φ ∈dfr, by
ψ0 ∧ψ1m = ψ0m ∩ψ1m
ψ0 ∨ψ1m = ψ0m ∪ψ1m
⟨a⟩ψm = {w ∈W | ∃S ∈N(a, w). S ⊆ψm}
[a]ψm = {w ∈W | ∀S ∈N(a, w). S ∩ψm ̸= ∅}
μX.ψm =

∅
midx(X) = 0
ψ[μX.ψ/X]m@idx(X)
midx(X) > 0
using the lexicographic ordering on (m, φ) as the termination measure; crucially,
unfolding least ﬁxpoints reduces the timeout. We extend this deﬁnition to sets
of formulae by Ψm = 
φ∈Ψφm for Ψ ⊆cl.
Lemma 18. In ﬁnite neighbourhood models, we have that for all ψ ∈cl there is
some timeout m such that
ψ ⊆ψm.
Proof. Let W be the set of states. Since ψm is deﬁned like ψ in all cases
but one, we only need to consider the inductive case where ψ = μX.ψ0 ∈cl. We
show that for all subformulae φ of ψ0, for all timeout vectors m = (m1, . . . , mk)
such that mi = |W| for all i such that φ does not contain a free variable with
index at least i, we have
φσ(m) ⊆θ∗(φ)m,
(1)
where σ(m) maps each Y ∈FV(φ) to (φ1Y
σ(m))midx(Y )(∅) where θ(Y ) = μY. φ1
(this is a recursive deﬁnition of σ(m) since the value of σ(m) for Y is overwritten
in (φ1Y
σ(m))midx(Y )(∅) so that this set depends only on values mi such that
i < idx(Y )). This shows that the claimed property μX. ψ0 ⊆μX. ψ0m holds
for m = (|W|, . . . , |W|): By Kleene’s ﬁxpoint theorem,
μX.ψ0 = (ψ0X)|W |(∅) = ψ0[X→(ψ0X)|W |−1(∅)] = ψ0σ(m@idx(X))
⊆θ∗(ψ0)m@idx(X) = μX.ψ0m,
using (1) in the second-to-last step.
The proof of (1) is by induction on φ; we do only the non-trivial cases. If
θ∗(φ) /∈dfr, then φ is closed (since ψ0 does not contain a free greatest ﬁxpoint
variable and since φ is not in the scope of a greatest ﬁxpoint operator within
ψ0, that is, no free greatest ﬁxpoint variable is introduced during the induction).
Hence we have φσ(m) = φ and θ∗(φ)m = θ∗(φ) = φ so that we are
done. If φ = Y , then Y σ(m) = (φ1Y
σ(m))midx(Y )(∅) where θ(Y ) = μY. φ1.
If midx(Y ) = 0, then (φ1Y
σ(m))midx(Y )(∅) = ∅so there is nothing to show. If

494
D. Hausmann and L. Schr¨oder
midx(Y ) > 0, then
(φ1Y
σ(m))midx(Y )(∅) = φ1Y
σ(m)((φ1Y
σ(m))midx(Y )−1(∅))
= φ1(σ(m))[Y →(φ1Y
σ )
midx(Y )−1(∅)]
⊆θ∗(φ1)m@idx(Y )
= θ∗(φ1[μY. φ1/Y ])m@idx(Y )
= θ∗(μY. φ1)m = θ∗(Y )m,
where the inclusion is by the inductive hypothesis. If φ = μY. φ1, then
μY. φ1σ(m) = (φ1Y
σ(m))|W |(∅) = φ1Y
σ(m)((φ1Y
σ(m))|W |−1(∅))
= φ1(σ(m))[Y →(φ1Y )|W |−1(∅)]
⊆θ∗(φ1)m@idx(Y )
= θ∗(φ1[θ(Y )/Y ])m@idx(Y )
= θ∗(μY. φ1)m,
where the ﬁrst equality is by Kleene’s ﬁxpoint theorem and the inclusion is by
the inductive hypothesis since midx(Y ) = |W| (hence m@idx(Y )idx(Y ) = |W| −1)
by assumption as μY. φ1 does not contain a free variable with index at least
idx(Y ).
⊓⊔
To check satisﬁability it suﬃces to decide whether a tableau exists:
Theorem 19. There is a tableau for ρ1 if and only if ρ1 is ρ0-satisﬁable.
Proof. Let ρ1 be ρ0-satisﬁable. Then there is a ﬁnite neighbourhood model
(W, N, I) such that W ⊆ρ0 and W ∩ρ1 ̸= ∅by Lemma 8. We deﬁne a tableau
over the set V = {(x, Ψ, w) ∈W × P(cl) × Σ∗
p | x ∈Ψ, u(Ψ) ≤3n −|w|}, where
u(Ψ) denotes the sum of the numbers of unguarded operators in formulae from
Ψ. Let (x, Ψ, w) ∈V . For Ψ /∈states, u(Ψ) ≤3n −|w|, we pick some φ ∈Ψ, dis-
tinguishing cases. If φ = φ0∧φ1 or φ = ηX. φ0, then we put b = 0. If φ = φ0∨φ1,
then let m be the least timeout such that x ∈φm (such m exists by Lemma 18).
Then there is some b′ ∈{0, 1} such that x ∈φb′m and we put b = b′. In any
case, we put l = (φ, b) and add an edge from (x, Ψ, w) to (x, γ(Ψ, l), (w, l)) to
L, having u(γ(Ψ, l)) = u(Ψ) −1 and hence u(γ(Ψ, l)) ≤3n −|w, l|. Since we are
interested in the nodes that can be reached from nodes (x, Ψ, ϵ) with |Ψ| ≤3
and since each formula in Ψ contains at most n unguarded operators, we indeed
only have to construct nodes (x, Ψ ′, w) with u(Ψ) ≤3n −|w| before reaching
state labeled nodes. For Ψ ∈states and {⟨a⟩φ0, [a]φ1} ⊆Ψ, let m be the least
timeout such that x ∈{⟨a⟩φ0, [a]φ1}m (again, such m exists by Lemma 18).
Then there is S ∈N(a, x) such that S ⊆φ0m ∩ρ0 and S ∩φ1m ̸= ∅. Pick
y ∈φ0m ∩φ1m ∩ρ0 and add an edge from (x, Ψ, w) to (y, {φ0, φ1, ρ0}, ϵ) to
L, having u({φ0, φ1, ρ0}) ≤3n −|ϵ|. Deﬁne the label l(x, Ψ, w) of nodes (x, Ψ, w)
to be just Ψ. Then the structure (V, L) indeed is a tableau for ρ1: Since there
is z ∈W ∩ρ1, we have (z, {ρ1}, ϵ) ∈V , that is, ρ1 is contained in the label

NP Reasoning in the Monotone μ-Calculus
495
of some node in (V, L). The requirements of tableaux for matching rule appli-
cations are satisﬁed by construction of L. It remains to show that each trace in
(V, L) is ﬁnite. So let τ = φ0, φ1, . . . be a trace of some formula φ0 along some
branch (encoded by a word w) that is rooted at some node v = (x, Ψ, w′) ∈V
such that φ0 ∈Ψ. Let m be the least timeout such that x ∈φ0m (again, such
m exists by Lemma 18). For each i, we have φi+1 ∈δ(φi, w(i)) and there are
(xi, Ψi, wi) ∈V and mi such that φi ∈Ψi and xi ∈φimi. We have chosen
disjuncts and modal successors in a minimal fashion, that is, in such a way that
we always have mi+1 ≤l mi. Since traces can be inﬁnite only if they contain
inﬁnitely many unfolding steps for some least ﬁxpoint, τ is ﬁnite or some least
ﬁxpoint is unfolded in it inﬁnitely often. In the former case, we are done. In the
latter case, each unfolding of a least ﬁxpoint by Deﬁnition 17 reduces some digit
of the timeout so that we have an inﬁnite decreasing chain mi1 >l mi2 >l . . .
with ij+1 > ij for all j, which is a contradiction to <l being a well-order.
For the converse direction, let (V, L) be a tableau for ρ1 labeled by some
function l : V →P(cl). We construct a model (W, N, I) over the set
W = {x ∈V | l(x) ∈states}.
For p ∈P, put I(p) = {x ∈W | p ∈l(x)}; since (V, L) is a tableau,
the rules (⊥) and () do not match the label of any node, so we have
I(p) = W \ I(p), as required. Let x ∈W and a ∈A. If l(x) contains no
a-box literal, then we put N(a, x) = {∅}. If l(x) contains some a-box literal
but no a-diamond literal, then we put N(a, x) = ∅. Otherwise, let the a-
modalities in l(x) be exactly ⟨a⟩χ1, . . . , ⟨a⟩χo, [a]ψ1, . . . , [a]ψm. We put N(a, x) =
{{y1,1, . . . , y1,m}, . . . , {yo,1, . . . , yo,m}}, where, for 1 ≤i ≤o, 1 ≤j ≤m, the
state yi,j is picked minimally with respect to tab among all nodes z such that
(x, z) ∈L and {χi, ψj, ρ0} = l(z); such yi,j with minimal tableau timeout always
exists since (V, L) is a tableau. It remains to show that (W, N, I) is a ρ0-model
for ρ1. We put
	
[[φ]] = {x ∈W | l(x) ⊢PL φ}
for φ ∈cl, where ⊢PL denotes propositional entailment (modal literals [a]φ, ⟨a⟩φ
are regarded as propositional atoms and ηX. φ and φ[ηX. φ/X] entail each other).
Since we have W ⊆
[[ρ0]] and W ∩
[[ρ1]] ̸= ∅by deﬁnition of (W, N, I) and tableaux,
it suﬃces to show that we have 	
[[φ]] ⊆φ for all φ ∈cl. The proof of this is by
induction on φ, using coinduction in the case for greatest ﬁxpoint formulae and
a further induction on tableau timeouts in the case for least ﬁxpoint formulae.
⊓⊔
6
Satisﬁability Games
We now deﬁne a game characterizing ρ0-satisﬁability of ρ1. Player Eloise tries to
establish the existence of a tableau for ρ1 using only polynomially many support-
ing points for her reasoning. To this end, sequences of propositional reasoning
steps are contracted into single Eloise-moves. Crucially, the limited branching of

496
D. Hausmann and L. Schr¨oder
monotone modalities (and the ensuing limited need for tracking of deferrals) has
a restricting eﬀect on the nondeterminism that the game needs to take care of.
Deﬁnition 20 (Satisﬁability games). We put U = {Ψ ⊆cl | 1 ≤|Ψ| ≤
2} (note |U| ≤n2) and Q = {Foc ⊆dfr | |Foc| ≤2}. The ρ0-satisﬁability
game for ρ1 is the B¨uchi game G = (V, E, v0, F) with set V = V∃∪V∀of
nodes (with the union made disjoint by markers omitted in the notation) where
V∃= {(Ψ, Foc) ∈U × Q | Foc ⊆Ψ} and V∀= states × Q, with initial node
v0 = ({ρ1}, ∅) ∈V∃, and with set F = {(Ψ, Foc) ∈V∃| Foc = ∅} of accepting
nodes. The set E of moves is deﬁned by
E(Ψ, Foc) = {(γ(Ψ ∪{ρ0}, w), δ(Foc, w)) ∈states × Q | w ∈(Σp)∗, |w| ≤3n}
E(Γ, Foc) = { ({φ0, φ1}, Foc′) ∈U × Q | {⟨a⟩φ0, [a]φ1} ⊆Γ,
if Foc ̸= ∅, then Foc′ = δ(Foc, (⟨a⟩φ0, [a]φ1)),
if Foc = ∅, then Foc′ = {φ0, φ1} ∩dfr }
for (Ψ, Foc) ∈V∃, (Γ, Foc) ∈V∀.
Thus, Eloise steers the propositional evolution of formula sets into formal states,
keeping track of the focussed formulae, while Abelard picks an application of
Lemma 2, and resets the focus set after it is ﬁnished, i.e. becomes ∅; Eloise wins
plays in which the focus set is ﬁnished inﬁnitely often.
Remark 21. It is crucial that while the game has exponentially many Abelard-
nodes, there are only polynomially many Eloise-nodes. In fact, all Eloise-nodes
(Ψ, Foc) have Foc ⊆Ψ, so the game has at most 4|U| ≤4n2 Eloise-nodes.
Next we prove the correctness of our satisﬁability games.
Theorem 22. There is a tableau for ρ1 if and only if Eloise wins G.
Proof. Let s be a winning strategy for Eloise in G with which she wins every
node in her winning region win∃. For v = (Ψ, Foc) ∈win∃, we let ws(v) denote a
ﬁxed propositional word such that s(Ψ, Foc) = (γ(Ψ, ws(v)), δ(Foc, ws(v))), that
is, a witness word for the move of Eloise that s prescribes at v. We construct a
tableau (W, L) over the set
W = {(γ(Ψ ∪{ρ0}, w′), δ(Foc, w′)) ∈P(cl) × P(dfr) |
(Ψ, Foc) ∈win∃, w′ is a preﬁx of ws(Ψ,Foc)}.
We deﬁne the label l(Φ, Foc′) of nodes from (Φ, Foc′) ∈W to be just Φ. Let
(Φ, Foc′) = (γ(Ψ ∪{ρ0}, w′), δ(Foc, w′)) ∈W. If w′ is a proper preﬁx of ws(Ψ,Foc),
then we have Φ /∈states; let l ∈Σp be the letter such that (w′, l) is a preﬁx of
ws(Ψ,Foc) and add the pair ((Φ, Foc′), (γ(Φ, l), δ(Foc′, l))) to L. If w′ = ws(Ψ,Foc),
then we have Φ ∈states. For {⟨a⟩φ, [a]χ} ⊆Φ, we distinguish cases. If Foc′ = ∅,
then put Foc′′ = {φ, χ} ∩dfr; otherwise, put Foc′′ = δ(Foc′, (⟨a⟩φ, [a]χ)). Then
add the pair ((Φ, Foc′), ({φ, χ, ρ0}, Foc′′)) to L, having ({φ, χ}, Foc′′) ∈win∃.

NP Reasoning in the Monotone μ-Calculus
497
It remains to show that all traces in (W, L) are ﬁnite. So let τ = φ0, φ1, . . .
be a trace along some branch (encoded by a word w) that is rooted at some
node (Φ, Foc′) ∈W such that φ0 ∈Φ. By construction, this branch gives
rise to an s-play (Ψ0, Foc0), (Γ0, Foc′
0), (Ψ1, Foc1), (Γ1, Foc′
1), . . . that starts at
(Ψ0, Foc0) = (Ψ, Foc). Let i be the least position such that Foc′
i = ∅(i exists
because s is a winning strategy). Since the φj are tracked along rule appli-
cations, we have φi ∈Γi; hence φi+1 ∈Foci+1. Let i′ be the least position
greater than i such that Foc′
i′ = ∅(again, i′ exists because s is a winning strat-
egy). Between (Ψi+1, Foci+1) and (Γi′, Foc′
i′), all formulae from Foci+1 (including
φi+1) are transformed to a non-deferral by the formula manipulations encoded
in w. In particular, the trace τ ends between node(Ψi, Foci) and node(Ψi′, Foci′),
and hence is ﬁnite.
For the converse direction, let (W, L) be tableau for ρ1, labeled with l :
W →P(cl). We extract a strategy s for Eloise in G. A game node (Ψ, Foc) is
realized if there is v ∈W such that Ψ ⊆l(v); then we say that v realizes the
game node. For all realized game nodes (Ψ, Foc), we pick a realizing tableau
node v(Ψ, Foc) that is minimal with respect to tab among the tableau nodes
that realize (Ψ, Foc). Then we construct a propositional word w = l0, l1, . . .
as follows, starting with Ψ0 = Ψ ∪{ρ0} and v0 = v(Ψ, Foc). For i ≥0, pick
some non-modal letter li = (φ, b) such that φ ∈Ψi, b ∈{0, 1} and such that
l(vi+1) = γ(Ψi, li) where vi+1 ∈W is the node such that (vi, vi+1) ∈L. Such
a letter li exists since (W, L) is a tableau. By guardedness of ﬁxpoint variables,
this process will eventually terminate with a word w = l0, l1, . . . , lm such that
m ≤3n, since Ψ0 contains at most three formulae and each formula contains
at most n unguarded operators. Put s(Ψ, Foc) = (γ(Ψ ∪{ρ0}, w), δ(Foc, w)),
having γ(Ψ ∪{ρ0}, w) = l(vm). It remains to show that s is a winning strategy.
So let τ = (Ψ0, Foc0), (Γ0, Foc′
0), (Ψ1, Foc1), (Γ1, Foc′
1), . . . be an s-play, where
Ψ0 = {ρ1} and Foc0 = ∅. It suﬃces to show that for all i such that Foci ̸= ∅,
there is j ≥i such that Focj = ∅. So let Foci ̸= ∅and let wi be the word that
is constructed in the play from (Ψi, Foci) on. Since (W, L) is a tableau and since
s has been constructed using realizing nodes with minimal tableau timeouts, all
traces of formulae from Foci along the branch that is encoded by wi are ﬁnite.
Let j be the least number such that all such traces have ended after 2j further
moves from (Ψi, Foci). Then we have Foc′
i+j = ∅, as required.
⊓⊔
Corollary 23. Every satisﬁable formula of size n in the alternation-free mono-
tone μ-calculus with the universal modality has a model of size at most 4n2.
Corollary 24. The satisﬁability checking problem for the alternation-free
monotone μ-calculus with the universal modality is in NP (hence NP-complete).
Proof. Guess a winning strategy s for Eloise in G and verify that s is a winning
strategy. Veriﬁcation can be done in polynomial time since the structure obtained
from G by imposing s is of polynomial size and since the admissibility of single
moves can be checked in polynomial time.
⊓⊔
By the translations recalled in Example 7, we obtain moreover

498
D. Hausmann and L. Schr¨oder
Corollary 25. Satisﬁability-checking in concurrent propositional dynamic logic
CPDL and in the alternation-free fragment of game logic is in NP (hence NP-
complete).
7
Conclusion
We have shown that satisﬁability checking in the alternation-free fragment of
the monotone μ-calculus with the universal modality is only NP-complete, even
when formula size is measured as the cardinality of the closure. Via straight-
forward translations (which have only quadratic blow-up under the mentioned
measure of formula size), it follows that both concurrent propositional dynamic
logic (CPDL) and the alternation-free fragment of game logic are also only NP-
complete under their original semantics, i.e. with atomic programs interpreted as
neighbourhood structures (they become ExpTime-complete when atomic pro-
grams are interpreted as relations). We leave as an open problem whether the
upper bound NP extends to the full monotone μ-calculus, for which the best
known upper bound thus remains ExpTime, by results on the coalgebraic μ-
calculus [5], or alternatively by the translation into the relational μ-calculus
that we give in the proof of the ﬁnite model property (Lemma 8).
References
1. Baader, F., Calvanese, D., McGuinness, D.L., Nardi, D., Patel-Schneider, P.F.
(eds.): The Description Logic Handbook. Cambridge University Press, Cambridge
(2003)
2. Bradﬁeld, J., Stirling, C.: Modal μ-calculi. In: Handbook of Modal Logic, pp. 721–
756. Elsevier (2006)
3. Bruse, F., Friedmann, O., Lange, M.: On guarded transformation in the modal
μ-calculus. Log. J. IGPL 23(2), 194–216 (2015)
4. Chellas, B.F.: Modal Logic. Cambridge University Press, Cambridge (1980)
5. Cˆırstea, C., Kupke, C., Pattinson, D.: EXPTIME tableaux for the coalgebraic μ-
calculus. Log. Methods Comput. Sci. 7(3:03), 1–33 (2011)
6. Donini, F.: Complexity of reasoning. In: Baader, F., et al. [1], pp. 96–136
7. Enqvist, S., Hansen, H.H., Kupke, C., Marti, J., Venema, Y.: Completeness for
game logic. In: Logic in Computer Science (LICS 2019), pp. 1–13. IEEE (2019)
8. Enqvist, S., Seifan, F., Venema, Y.: Monadic second-order logic and bisimulation
invariance for coalgebras. In: Logic in Computer Science (LICS 2015), pp. 353–365.
IEEE (2015)
9. Fischer, M., Ladner, R.: Propositional dynamic logic of regular programs. J. Com-
put. Syst. Sci. 18, 194–211 (1979)
10. Friedmann, O., Lange, M.: Deciding the unguarded modal μ-calculus. J. Appl.
Non-Classical Log. 23, 353–371 (2013)
11. Goranko, V., Passy, S.: Using the universal modality: gains and questions. J. Log.
Comput. 2, 5–30 (1992)
12. Hansen, H., Kupke, C.: A coalgebraic perspective on monotone modal logic. In:
Ad´amek, J., Milius, S. (eds.) Coalgebraic Methods in Computer Science (CMCS
2004) of ENTCS, vol. 106, pp. 121–143. Elsevier (2004)

NP Reasoning in the Monotone μ-Calculus
499
13. Hausmann, D., Schr¨oder, L., Egger, C.: Global caching for the alternation-free
coalgebraic μ-calculus. In: Concurrency Theory (CONCUR 2016) of LIPIcs, vol.
59, pp. 34:1–34:15. Schloss Dagstuhl - Leibniz-Zentrum f¨ur Informatik (2016)
14. Kracht, M., Wolter, F.: Normal monomodal logics can simulate all others. J. Symb.
Log. 64(1), 99–138 (1999)
15. Ladner, R.: The computational complexity of provability in systems of modal
propositional logic. SIAM J. Comput. 6, 467–480 (1977)
16. Lange, M., Stirling, C.: Focus games for satisﬁability and completeness of temporal
logic. In: Logic in Computer Science (LICS 2001), pp. 357–365. IEEE Computer
Society (2001)
17. Mazala, R.: Inﬁnite games. In: Gr¨adel, E., Thomas, W., Wilke, T. (eds.) Automata
Logics, and Inﬁnite Games. LNCS, vol. 2500, pp. 23–38. Springer, Heidelberg
(2002). https://doi.org/10.1007/3-540-36387-4 2
18. Miyano, S., Hayashi, T.: Alternating ﬁnite automata on ω-words. Theory Comput.
Sci. 32, 321–330 (1984)
19. Niwinski, D., Walukiewicz, I.: Games for the μ-calculus. Theory Comput. Sci. 163,
99–116 (1996)
20. Parikh, R.: The logic of games and its applications. Ann. Discrete Math. 24, 111–
140 (1985)
21. Pauly,
M.:
Bisimulation
for
general
non-normal
modal
logic.
Unpublished
manuscript (1999)
22. Pauly, M.: Logic for social software. Ph.D. thesis, Universiteit van Amsterdam
(2001)
23. Pauly, M., Parikh, R.: Game logic - an overview. Stud. Log. 75(2), 165–182 (2003)
24. Peleg, D.: Concurrent dynamic logic. J. ACM 34, 450–479 (1987)
25. Pratt, V.: A decidable mu-calculus: preliminary report. In: Foundations of Com-
puter Science (FOCS 1981), pp. 421–427. IEEE Computer Society (1981)
26. van Benthem, J., Bezhanishvili, N., Enqvist, S.: A propositional dynamic logic for
instantial neighborhood semantics. Stud. Log. 107(4), 719–751 (2019)
27. Vardi, M.: On the complexity of epistemic reasoning. In: Logic in Computer Science
(LICS 1989), pp. 243–252. IEEE Computer Society (1989)

Soft Subexponentials and Multiplexing
Max Kanovich1,2, Stepan Kuznetsov3,2(B), Vivek Nigam5,4,
and Andre Scedrov6,2
1 University College London, London, UK
m.kanovich@ucl.ac.uk
2 National Research University Higher School of Economics, Moscow, Russia
3 Steklov Mathematical Institute of RAS, Moscow, Russia
sk@mi.ras.ru
4 Federal University of Para´ıba, Jo˜ao Pessoa, Brazil
nigam@fortiss.org
5 fortiss GmbH, Munich, Germany
6 University of Pennsylvania, Philadelphia, USA
scedrov@math.upenn.edu
Abstract. Linear logic and its reﬁnements have been used as a spec-
iﬁcation language for a number of deductive systems. This has been
accomplished by carefully studying the structural restrictions of lin-
ear logic modalities. Examples of such reﬁnements are subexponentials,
light linear logic, and soft linear logic. We bring together these reﬁne-
ments of linear logic in a non-commutative setting. We introduce a non-
commutative substructural system with subexponential modalities con-
trolled by a minimalistic set of rules. Namely, we disallow the contrac-
tion and weakening rules for the exponential modality and introduce two
primitive subexponentials. One of the subexponentials allows the mul-
tiplexing rule in the style of soft linear logic and light linear logic. The
second subexponential provides the exchange rule. For this system, we
construct a sequent calculus, establish cut elimination, and also provide
a complete focused proof system. We illustrate the expressive power of
this system by simulating Turing computations and categorial grammar
parsing for compound sentences. Using the former, we prove undecidabil-
ity results. The new system employs Lambek’s non-emptiness restriction,
which is incompatible with the standard (sub)exponential setting. Lam-
bek’s restriction is crucial for applications in linguistics: without this
restriction, categorial grammars incorrectly mark some ungrammatical
phrases as being correct.
1
Introduction
For the speciﬁcation of deductive systems, linear logic [4,5], and a number of
reﬁnements of linear logic have been proposed, such as commutative [23,25]
and non-commutative [11,12] subexponentials, light linear logic [7], soft linear
logic [16], and easy linear logic [10]. The key diﬀerence between these reﬁne-
ments is their treatment of the linear logic exponentials, !, ?. These reﬁnements
c
⃝Springer Nature Switzerland AG 2020
N. Peltier and V. Sofronie-Stokkermans (Eds.): IJCAR 2020, LNAI 12166, pp. 500–517, 2020.
https://doi.org/10.1007/978-3-030-51074-9_29

Soft Subexponentials and Multiplexing
501
allow, e.g., a ﬁner control on the structural rules, i.e., weakening, contraction
and exchange rules, and how exponentials aﬀect the sequent antecedent. For
example [12], we proposed a logical framework with commutative and non-
commutative subexponentials, applying it for applications in type-logical gram-
mar. In particular, we demonstrated that this logical framework can be used
to “type” correctly sentences that were not able before with previous logical
frameworks, such as Lambek calculus.
However, as we have shown recently, our logical framework in [12] is incom-
patible with the important Lambek’s non-emptiness property [15]. This prop-
erty, which requires all antecedents to be non-empty, is motivated by linguistic
applications of Lambek-like calculi. Namely, it prevents the system from rec-
ognizing (“typing”) incorrectly formed sentences as grammatically correct. We
discuss these linguistic issues in detail in Sect. 3. The lack of Lambek’s restric-
tion means that the logical framework proposed in our previous work is too
expressive, typing incorrectly sentences.
To address this problem, we propose a new non-commutative proof system,
called SLLM, that admits Lambek’s non-emptiness condition and at the same
time is expressive enough to type correctly sentences in our previous work. This
system is also still capable of modelling computational processes, as we show in
Sect. 5.1 on the example of Turing computations.
In particular, SLLM takes inspiration from the following reﬁnements of linear
logic: subexponentials, by allowing two types of subexponentials, ! and ∇; soft
linear logic, which contributes a version of the multiplexing rule, !L, shown below
to the left; and light linear logic, which contributes the two right subexponentials
rules, !R, ∇R, shown below to the right.
Γ, F, . . . , F, Δ →G
Γ, !F, Δ →G
!L
F →G
!F →!G !R
F →G
∇F →∇G ∇R
In our version of the system, the premise of the rule !L does not allow zero
instances of F. Hence, ! is a relevant subexponential as discussed in [12].
This rule is used to type sentences correctly, while the rules !R and ∇R are
used to maintain Lambek’s condition. SLLM contains, therefore, soft subexpo-
nentials and multiplexing.
Our main contributions are summarized below:
– Admissibility of Cut Rule: We introduce the proof system SLLM in Sect. 2.
We also prove that it has basic properties, namely admissibility of Cut Rule
and the substitution property. The challenge is to ensure a reasonable balance
between the expressive power of systems and complexity of their implemen-
tation, and in particular, to circumvent the diﬃculties caused by linear logic
contraction and weakening rules.
– Lambek’s Non-Emptiness Condition: We demonstrate in Sect. 3 that
SLLM (and thus also SLLMF) admits Lambek’s non-emptiness condition. This
means that SLLM cannot be used to “type” incorrect sentences.
We demonstrate this by means of some examples.

502
M. Kanovich et al.
– Focused Proof System: We introduce in Sect. 4 a focused proof system
(SLLMF) proving that it is sound and complete with respect to SLLM. The
focused proof system diﬀers from the focused proof system in our previous
work [12] by allowing a subexponential that can contract, but not weaken
nor be exchanged. Such subexponentials were not allowed in the proof system
introduced in [12]. A key insight is to keep track on when a formula necessarily
has to be introduced in a branch and when not.
– Complexity: We investigate in Sect. 5 the complexity of SLLM. We ﬁrst
demonstrate that the provability problem for SLLM is undecidable in general
and identify a fragment for which it is decidable.
Finally, in Sect. 6, we conclude by pointing to related and future work.
Table 1. Lambek Calculus: A non-commutative version of ILL
A →A I
Φ →A
Σ1, B, Σ2 →C
Σ1, Φ, A \ B, Σ2 →C
\L
A, Σ →B
Σ →A \ B \R (Σ is not empty)
Φ →A
Σ1, B, Σ2 →C
Σ1, B / A, Φ, Σ2 →C
/L
Σ, A →B
Σ →B / A /R (Σ is not empty)
Σ1, A, B, Σ2 →C
Σ1, A · B, Σ2 →C ·L
Σ1 →A
Σ2 →B
Σ1, Σ2 →A · B
·R
Table 2. SLLM: Lambek calculus with multiplexing.
A →A I
Φ →A
Σ1, B, Σ2 →C
Σ1, Φ, A \ B, Σ2 →C
\L
A, Σ →B
Σ →A \ B \R (Σ is not empty)
Φ →A
Σ1, B, Σ2 →C
Σ1, B / A, Φ, Σ2 →C
/L
Σ, A →B
Σ →B / A /R (Σ is not empty)
Σ1, A, B, Σ2 →C
Σ1, A · B, Σ2 →C ·L
Σ1 →A
Σ2 →B
Σ1, Σ2 →A · B
·R
Γ1,
k times



A, A, . . . , A, Γ2 →C
Γ1, !A, Γ2 →C
!L (k ≥1)
A →C
!A →!C !R
Γ1, A, Γ2 →C
Γ1, ∇A, Γ2 →C ∇L
A →C
∇A →∇C ∇R
Γ1, Γ2, ∇A, Γ3 →C
Γ1, ∇A, Γ2, Γ3 →C
and
Γ1, ∇A, Γ2, Γ3 →C
Γ1, Γ2, ∇A, Γ3 →C ∇E

Soft Subexponentials and Multiplexing
503
2
The Non-commutative System SLLM with Multiplexing
As the non-commutative source, we take the Lambek calculus [17], Table 1, the
well-known fundamental system in linguistic foundations.
The proof system introduced here, SLLM, extends the Lambek calculus by
adding two new connectives (subexponentials) ! and ∇and their rules in Table 2.
Drawing inspiration from commutative logics such as linear logic [6], light lin-
ear logic [7], soft linear logic [16], and easy linear logic [10], here we introduce our
primitive non-commutative modalities !A and ∇A controlled by a minimalistic
set of rules.
Multiplexing Rule (local):
Γ1,
k times



A, A, . . . , A, Γ2 →C
Γ1, !A, Γ2 →C
!L (k ≥1)
(1)
Informally,
!A
stands
for:
“any
positive number
of
copies
of
A
at the same position”.
Remark 1. In contrast to soft linear logic and light linear logic, where Weakening
is one of the necessary ingredients, here we exclude the Weakening case: (k = 0).
Unlike Contraction rule that can be recursively reused, and !A keeps the
subexponential (it is introduced by Dereliction), our Multiplexing can only be
used once with all copies provided immediately at the same time in one go,
and the subexponentials get removed. Thus, if one wishes to reuse Multiplexing
further in the proof, nested subexponentials would be needed (like !!A for two
levels of Multiplexing).
The second subexponential, ∇A, provides the exchange rule.
Exchange Rule (non-local):
Γ1, Γ2, ∇A, Γ3 →C
Γ1, ∇A, Γ2, Γ3 →C
and
Γ1, ∇A, Γ2, Γ3 →C
Γ1, Γ2, ∇A, Γ3 →C ∇E
(2)
and Dereliction Rule (local):
Γ1, A, Γ2 →C
Γ1, ∇A, Γ2 →C ∇L
(3)
“∇A can be thought of as storing a missing candidate A in a ﬁxed local storage,
with the ability to deliver A to the right place at the appropriate time.”
Remark 2. Notwithstanding that the traditional Promotion rule
Γ →C
!Γ →!C is
accepted in linear logic as well as in soft linear logic, we conﬁne ourselves to
the restricted light linear logic promotion
A →C
!A →!C , in order to guarantee cut
admissibility for the non-commutative SLLM (cf. [11]). E.g., with Γ →C
!Γ →!C the
sequent
!B, !(B\C) →(C · C)
(4)

504
M. Kanovich et al.
is derivable by cut
B, B \ C →C
!B, !(B \ C) →!C
!C →C · C
!B, !(B \ C) →C · C
Cut
but, to ﬁnalize a cut-free proof for (4), we need commutativity:
B →B
B, C, B \ C →C · C
[ok with C, B, B \ C →C · C]
B, B, B \ C, B \ C →C · C
B, B, !(B \ C) →C · C
!L
!B, !(B \ C) →C · C
!L
The following theorem states that SLLM enjoys cut admissibility and the
substitution property.
Theorem 1.
(a) The calculus SLLM enjoys admissibility of the Cut Rule:
Π →A
Γ1, A, Γ2 →C
Γ1, Π, Γ2 →C
Cut
(5)
(b) Given an atomic p, let the sequent Γ(p) →C(p) be derivable in the calculus.
Then for any formula B,
Γ(B) →C(B) is also derivable in the calculus.
Here by Γ(B) and C(B) we denote the result of replacing all occurrences
of p by B in Γ(p) and C(p), resp.
Proof.
(a) By reductions. E.g.,
B →A
!B →!A !R
Γ1,
k times



A, A, . . . , A, Γ2 →C
Γ1, !A, Γ2 →C
!L
Γ1, !B, Γ2 →C
Cut
is reduced to
B →A
Γ1, A, A, . . . , A, Γ2 →C
Γ1, B, B, . . . , B, Γ2 →C
Cut (k times)
Γ1, !B, Γ2 →C
!L
(b) By induction.

Soft Subexponentials and Multiplexing
505
3
Linguistic Motivations
In this Section we illustrate how (and why) our modalities provide parsing com-
plex and compound sentences in a natural language.
We start with standard examples, which go back to Lambek [17]; for in-depth
discussion of linguistic matters we refer to standard textbooks [3,19,21].
The sentence “Bob sent the letter yesterday” is grammatical, and the follow-
ing “type” speciﬁcation is provable in Lambek calculus, Table 1.
N, (N \ S) / N, N, V \ V →S
Here the ‘syntactical type’ N stands for nouns “the letter” and “Bob”, and
((N\S)/N), i.e., (V/N), for the transitive verb “sent”, and (V \V ) for the verb
modiﬁer “yesterday”, where V = (N\S). The whole sentence is of type S.
Lambek’s non-emptiness restriction is important for correctness of Lambek’s
approach to modelling natural language syntax. Without this restriction Lambek
grammars overgenerate, that is, recognize ungrammatical phrases as if they were
correct. The standard example [19, § 2.5] is as follows: “very interesting book” is
a correct noun phrase, established by the following derivation:
(N / N) /(N / N), N / N, N →N.
The sequent above is derivable in the Lambek calculus. Without Lambek’s
restriction, however, one can also derive
(N / N) /(N / N), N →N
(since →N / N is derivable with an empty antecedent). This eﬀect is unwanted,
since the corresponding phrase “very book” is ungrammatical. Thus, Lambek’s
non-emptiness restriction is a highly desired property for linguistic applications.
Fortunately, SLLM enjoys Lambek’s non-emptiness property. That is:
Theorem 2. The calculus SLLM provides Lambek’s non-emptiness restriction:
If a sequent Γ →C is derivable in the calculus then the list of formulas Γ is not
empty.
Proof. The crucial point is that, in the absence of Weakening, !A never happens
to produce the empty list.
⊓⊔
Theorems 1 and 2 show how our new system SLLM resolves the issues dis-
cussed in [15] for the case of the Lambek calculus extended with a full-power
exponential in linear logic style. Namely, as we show in that article, no reason-
able extension of the Lambek calculus with the exponential modality can have
the three properties simultaneously:
– cut elimination;
– substitution;
– Lambek’s restriction.

506
M. Kanovich et al.
Moreover, as also shown in [15], for the one-variable fragment the same happens
to the relevant subexponential, which allows contraction and permutation, but
not weakening. Our new system overcomes these issues by reﬁning the rules for
subexponentials.
Now let us show how one can use subexponentials of SLLM to model
more complicated sentences. Our analysis shares much with that of Morrill
and Valent´ın [22]. Unlike ours, the systems in [22] do not enjoy Lambek’s
restriction.
(1) The noun phrase: “the letter that Bob sent yesterday,” is grammatical, so
that its “type” speciﬁcation (6) should be provable in Lambek calculus or
alike:
N, ((N \ N) / S′), N, (V / N), (V \ V ) →N
(6)
Here ((N \ N) / S′ stands for a subordinating connective “that”.
As a type for the whole dependent clause,
“Bob sent yesterday,′′
we take some S′, not a full S, because the direct object, “the letter”, is
missing.
Our solution reﬁnes the approach of [2,20]. We mark the missing item, the
direct object “the letter” of type N, by a speciﬁc formula ∇N stored at a ﬁxed
local position and by means of Exchange Rule (2) and Dereliction Rule (3)
deliver the missing N to the right place with providing
N, (V / N), N, (V \ V ) →S,
which is the type speciﬁcation for the sentence “Bob sent the letter yesterday ”
completed with the direct object, “the letter”.
By taking S′ = (∇N\S), we can prove (6):
N, V / N, N, V \ V →S
↙“Bob sent the letter yesterday”
∇N, N, V / N, V \ V →S ∇L, ∇E
N, V / N, V \ V →S′
N, N \ N →N
N, (N \ N) / S′, N, V / N, V \ V →N
Remark 3. If we allowed Weakening, we would prove the ungrammatical
“the letter that Bob sent the letter yesterday,”
(2) The noun phrase: “the letter that Bob sent without reading” is grammatical
despite two missing items: the direct object to “sent” and the direct object
to “reading”, resp. Hence, the corresponding “type” speciﬁcation (7) should
be provable in Lambek calculus or alike:
N, ((N \ N) / S′′), Δ1, Δ2 →N
(7)

Soft Subexponentials and Multiplexing
507
Here N stands for “the letter”, and ((N \ N) / S′′) for “that”, and some Δ1
for “Bob sent”, and Δ2 for “without reading”.
As a type for the whole dependent clause,
“Bob sent
without reading
′′
we have to take some S′′, not a full S, to respect the fact that this time two
items are missing: the direct objects to “sent” and “reading”, resp.
To justify “the letter that Bob sent
without reading ” with its (7), in addition
to the ∇-rules, we invoke Multiplexing Rule (1).
The correlation between the full S and S′′ with its multiple holes is given by:
S′′ = ((!∇N) \ S)
(8)
As compared with the previous case of S′ representing one missing item ∇N,
the S′′ here is dealing with !∇N which provides two copies of ∇N to represent
two missing items.
Then the proof for (7) is as:
Δ1, N, Δ2, N →S
↙“Bob sent the letter without reading it”
∇N, ∇N, Δ1, Δ2 →S ∇L, ∇E
!∇N, Δ1, Δ2 →S
!L
Δ1, Δ2 →S′′
N, N \ N →N
N, (N \ N) / S′′, Δ1, Δ2 →N
4
Focused Proof System
This section introduces a sound and complete focused proof system SLLMF for
SLLM. Focusing [1] is a discipline on proofs that reduces proof search space. We
take an intermediate step, by ﬁrst introducing the proof system SLLM# that
handles the non-determinism caused by the multiplexing rule.
4.1
Handling Local Contraction
For bottom-up proof-search, the multiplexing rule
Γ, F, . . . , F, Δ →G
Γ, !F, Δ →G
has a great deal of don’t know non-determinism as one has to decide how many
copies of F appears in its premise. This decision aﬀects provability as each
formula has to be necessarily be used in the proof, i.e., they cannot be weakened.
To address this problem, we take a lazy approach by using two new connec-
tives ♯∗and ♯+. The formula ♯∗F denotes zero or more local copies of F, and
♯+F one of more copies of F. We construct the proof system SLLM# from SLLM

508
M. Kanovich et al.
as follows. It contains all rules of SLLM, except the rule !L, which is replaced by
the following rules
Γ, ♯+F, Δ
Γ, !F, Δ →G
Γ, F, ♯∗F, Δ →G
Γ, ♯+F, Δ →G
Γ, F, Δ →G
Γ, ♯∗F, Δ →G
Γ, Δ →G
Γ, ♯∗F, Δ →G
Notice that there is no need for explicit contraction and only ♯∗allows for
weakening. We accommodate contraction into the introduction rules, namely,
by modifying the rules where there is context splitting, such as in the rules \L.
In particular, one should decide in which branch a formula C is necessarily be
used. This is accomplished by using adequately ♯+F and ♯∗F. For example, some
rules for \L are shown below:
♯∗C, Γ2 →F
Γ1, ♯+C, G, Γ3 →H
Γ1, ♯+C, Γ2, F \ G, Γ3 →H
♯+C, Γ2 →F
Γ1, ♯∗C, G, Γ3 →H
Γ1, ♯+C, Γ2, F \ G, Γ3 →H
♯∗C, Γ2 →F
Γ1, ♯∗C, G, Γ3 →H
Γ1, ♯∗C, Γ2, F \ G, Γ3 →H
In the ﬁrst rule, the C is necessarily used in the right premise, while in the
left premise one can chose to use C or not. SLLM# contains similar symmetric
rules where C is necessarily moved to the left premise. Also it contains the
corresponding rules for /L.
SLLM# also include the following more reﬁned right-introduction rules for !
and ∇. where Γ ∗
1 , Γ ∗
2 are lists containing only formulas of the form ♯∗H.
F →G
Γ ∗
1 , !F, Γ ∗
2 →!G
F →G
Γ ∗
1 , ∇F, Γ ∗
2 →∇G
Notice how the decision that all formulas in Γ1, Γ2 represent zero copies is made
in the rules above.
Theorem 3. Let Γ, G be a list of formulas not containing ♯∗nor ♯+. A sequent
Γ →G is provable in the SLLM if and only if it is provable in SLLM#.
Completeness follows from straightforward proof by induction on the size
of proofs. One needs to slightly generalize the inductive hypothesis. Soundness
follows from the fact that contractions are local and can be permuted below
every other rule.
4.2
Focused Proof System
First proposed by Andreoli [1] for Linear Logic, focused proof systems reduce
proof search space by distinguishing rules which have don’t know non-
determinism, classiﬁed as positive, from rules which have don’t care non-
determinism, classiﬁed as negative. We classify the rules ·R, \L, /L as positive

Soft Subexponentials and Multiplexing
509
and the rules ·L, \R, /R as negative. Non-atomic formulas of the form F · G are
classiﬁed as positive, ∇F, ♯∗F, ♯+F and !F are classiﬁed as modal formulas, and
formulas of the form F \ G and F / G as negative.
The focused proof system manipulates the following types of sequents, where
Γ1 and Γ2 are possibly empty lists of non-positive formulas, Θ is multiset of
formulas, and NN is a non-negative formula. Intitively, Θ will contain all formulas
of the form ∇F. As they allow exchange rule, their collection can be treated as
a multiset. Γ1, Γ2 contain formulas that cannot be introduced by negative rules.
– Negative Phase: Θ : [Γ1], ⇑Δ, [Γ2] →[NN] and Θ : [Γ1], ⇑Δ, [Γ2] →⇑F.
Intuitively, the formulas in Δ and F are eagerly introduced whenever they
negative rules are applicable, as one does not lose completeness in doing so.
– Border: Θ : [Γ1] →[NN]. These are sequents for which no negative rule can
be applied. At this moment, one has to decide on a formula starting a positive
phase.
– Positive Phase: Θ : [Γ1], ⇓F, [Γ2] →[NN] and Θ : [Γ1] →⇓F, where only
the formula F is focused on.
The focused proof system SLLMF is composed by the rules in Figs. 1 and 2.
Intuitively, reaction rules RL1, RL2, RR and negative phase rules are applied until
no more rules are applicable. Then a decision rule is applied which focuses on
one formula. One needs, however, to be careful on whether the focused formula’s
main connective is ♯+ or ♯∗. If it is the former, then we have committed to one
copy of the formula and therefore, it can be modiﬁed to be ♯∗, while the latter
does not change.
The number of rules in Fig. 2 simply reﬂects the diﬀerent cases emerging
due to the presence or not of formulas whose main connective is ♯+ or ♯∗. For
example, \L2 considers the case when the splitting of the context occurs exactly
on a formula ♯+C. In this case, the decision is to commit to use a copy of C in
the right-premise, thus containing ♯+C, while ♯∗C on the left-premise. The other
rules follow the same reasoning.
Finally, notice that in the rules I, !R and ∇R the context may contain formu-
las with main connective ♯∗in their conclusion, but not in their premise. This
illustrates the lazy decision of how many copies of a formula are needed.
Theorem 4. A sequent Γ →G is provable in SLLM# if and only if the sequent
· : ⇑Γ →⇑G is provable in SLLMF.
The proof of this theorem follows the same ideas as detailed in [26] and [12].
Corollary 1. Let Γ, G be a list of formulas not containing ♯∗nor ♯+. A sequent
Γ →G is provable in SLLM if and only if the sequent · : ⇑Γ →⇑G is provable
in SLLMF.
Remark 4. The focused proof system introduced above enables the use of more
sophisticated search mechanisms. For example, lazy methods can reduce the
non-determinism caused by the great number of introduction rules caused by
managing ♯∗, ♯+, e.g., Fig. 2.

510
M. Kanovich et al.
· : [Γ ∗
1 , A, Γ ∗
2 ] →⇓A I
· : [Γ ∗
1 , ♯+/∗A, Γ ∗
2 ] →⇓A
I
Θ1 : [Γ1] →⇓F
Θ2 : [Γ1] →⇓G
Θ1, Θ2 : [Γ1, Γ2] →⇓F · G
·R
· : ⇑F →⇑G
· : [Γ ∗
1 , !F, Γ ∗
2 ] →⇓!G !R
· : ⇑F →⇑G
F : [Γ ∗] →⇓∇G ∇R
Θ : [Γ1], ⇑F, G, Δ, [Γ2] →R
Θ : [Γ1], ⇑F · G, Δ, [Γ2] →R
·L
Θ : ⇑F, [Γ] →⇑G
Θ : [Γ] →⇑F \ G \R
Θ : [Γ], ⇑G →⇑F
Θ : [Γ] →⇑F / G /R
Θ : [Γ1], ⇑N ∇
P , [Γ2] →[G]
Θ : [Γ1], ⇓N ∇
P , [Γ2] →[G]
RL
Θ : [Γ] →⇑NNA
Θ : [Γ] →⇓NNA RR
Θ : [Γ1, NP ], ⇑Δ, [Γ2] →R
Θ : [Γ1], ⇑NP , Δ, [Γ2] →R ⇑L1
Θ, F : [Γ1], ⇑Δ, [Γ2] →R
Θ : [Γ1], ⇑∇F, Δ, [Γ2] →R ⇑L2
Θ : [Γ] →[NN]
Θ : [Γ] →⇑NN RR
[Γ] →⇓G
[Γ] →[G] DR
Θ : [Γ1], ⇓NP , [Γ2] →[G]
Θ : [Γ1, NP , Γ2] →[G]
D1
Θ : [Γ1], ⇓F, [Γ2] →[G]
Θ, F : [Γ1, Γ2] →[G]
D2
Θ : [Γ1, ♯+F, Γ2] →[G]
Θ : [Γ1, !F, Γ2] →[G]
D3
Θ : [Γ1, ♯∗F], ⇓F, [Γ2] →[G]
Θ : [Γ1, ♯+/∗F, Γ2] →[G]
D4
Fig. 1. Focused proof system for SLLM#. NN represent a non-negative formula. NNA
represent a non-atomic, non-negative formula. NP represents a non-positive formula
whose main connective is not ∇. N ∇
P represents a non-positive formula. We use R for
both ⇑G or [NN]. We use ♯+/∗F for both ♯+F and ♯∗F. We use Γ ∗for a possibly
empty list of formulas of the form ♯∗F.
5
Complexity
In this section, we investigate the complexity of SLLM. In particular, we show
that it is undecidable in general, by encoding Turing computations. This encod-
ing also illustrate how the focused proof system SLLMF reduces non-determinism.
We then identify decidable fragments for SLLM.
5.1
Encoding Turing Computations in SLLM
Any Turing instruction I is encoded by !∇AI with an appropriate AI.
E.g., an instruction I : qξ →q′ηR
if in state q looking at symbol ξ, replace it by η, move the tape head one
cell to the right, and go into state q′,
is encoded by !∇Ai where
Ai = [(qi · ξi) \(ηi · q′
i)]

Soft Subexponentials and Multiplexing
511
Θ1 : [Γ2], →⇓F
Θ2 : [Γ1], ⇓G, [Γ3] →[H]
Θ1, Θ2 : [Γ1, Γ2], ⇓F \ G, [Γ3] →[H]
\⋆
L1
Θ1 : [♯∗C, Γ2], →⇓F
Θ2 : [Γ1, ♯+C], ⇓G, [Γ3] →[H]
Θ1, Θ2 : [Γ1, ♯+C, Γ2], ⇓F \ G, [Γ3] →[H]
\L2
Θ1 : [♯+C, Γ2], →⇓F
Θ2 : [Γ1, ♯∗C], ⇓G, [Γ3] →[H]
Θ1, Θ2 : [Γ1, ♯+C, Γ2], ⇓F \ G, [Γ3] →[H]
\L3
Θ1 : [♯∗C, Γ2], →⇓F
Θ2 : [Γ1, ♯∗C], ⇓G, [Γ3] →[H]
Θ1, Θ2 : [Γ1, ♯∗C, Γ2], ⇓F \ G, [Γ3] →[H]
\L4
Θ1 : [Γ2], →⇓G
Θ2 : [Γ1], ⇓F, [Γ3] →[H]
Θ1, Θ2 : [Γ1], ⇓F / G, [Γ2, Γ3] →[H]
/†
L1
Θ1 : [Γ2, ♯∗C], →⇓G
Θ2 : [Γ1], ⇓F, [♯+C, Γ3] →[H]
Θ1, Θ2 : [Γ1], ⇓F / G, [Γ2, ♯+C, Γ3] →[H]
/L2
Θ1 : [Γ2, ♯+C], →⇓G
Θ2 : [Γ1], ⇓F, [♯∗C, Γ3] →[H]
Θ1, Θ2 : [Γ1], ⇓F / G, [Γ2, ♯+C, Γ3] →[H]
/L3
Θ1 : [Γ2, ♯∗C], →⇓G
Θ2 : [Γ1], ⇓F, [♯∗C, Γ3] →[H]
Θ1, Θ2 : [Γ1], ⇓F / G, [Γ2, ♯∗C, Γ3] →[H]
/L4
Fig. 2. Focused left introduction rules for / and \ . The proviso ⋆in \L1 states that
the left-most formula of Γ2 or the right-most formula of Γ1 are not of the form ♯∗F nor
♯+F. The proviso † in /L1 is states that the right-most formula of Γ2 or the left-most
formula of Γ3 are not of the form ♯∗F nor ♯+F.
Let M lead from an initial conﬁguration, represented as B1 · q1 · ξ · B2, to the
ﬁnal conﬁguration q0 (the tape is empty).
We demonstrate how focusing improves search with the encoding of Turing
computations. For example, an instruction that write to the tape and moves to
the right has the form: Ai = [(qi · ξi) \(ηi · q′
i)] while the Turing Machine (TM)
conﬁguration is encoded as in the sequent context: [B1, q1, ξ, B2] speciﬁes A TM
at state q1 looking at the symbol ξ in the tape B1, ξ, B2.
Assuming that A1, . . . , An is used at least once, !∇A1, . . . , !∇An speciﬁes
the behavior of the TM. This becomes transparent with focusing. The following
focused derivation illustrate how a copy of an instruction encoding, below A1,
can be made available to be used. Recall that the one has to look at the derivation
from bottom-up.
A1 : [♯∗∇A1, . . . , !∇An, B1, q1, ξ, B2] →[q0]
· : ⇓∇A1, [♯∗∇A1, . . . , !∇An, B1, q1, ξ, B2] →[q0] RL, ⇑2
· : [♯+∇A1, . . . , !∇An, B1, q1, ξ, B2] →[q0]
D4
· : [!∇A1, . . . , !∇An, B1, q1, ξ1, B2] →[q0] D3

512
M. Kanovich et al.
Notice that A1 is placed in the Θ context. This means that it can be moved at
any place. Also notice that since one copy of A1 is used, ♯+∇A1 is replaced by
♯∗∇A1.
The following derivation continues from the premise of the derivation above
by focusing on A1, A = ♯∗∇A1, . . . , !∇An:
· : [q1] →⇓q1 I
· : [η1] →⇓η1 I
· : [q1, ξ] →⇓q1 · ξ1
·R
· : [A, B1, η1 · q′
1, B2] →[q0]
· : [A, B1], ⇑η1 · q′
1, [B2] →[q0] ·L, 2× ⇑L1
· : [A, B1], ⇓η1 · q′
1, [B2] →[q0] RL
· : [A, B1, q1, ξ], ⇓(q1 · ξ1) \(η1 · q′
1), [B2] →[q0]
\L
A1 : [A, B1, q1, ξ, B2] →[q0]
D4
Notice that the resulting premise (at the right of the tree) speciﬁes exactly the
TM tape resulting from executing the instruction speciﬁed by A1.
Moreover, notice that for the rule D4, there are many options on where
exactly to use A1. However, it will only work if done as above. This is because
otherwise it would not be possible to apply the initial rules as to the left of the
derivation above. This reduces considerably the non-determinism involved for
proof search.
Finally, once the ﬁnal conﬁguration q0 (our Turing machine is responsible for
garbage collection, so the conﬁguration is just q0, with no symbols on the tape)
is reached, one ﬁnishes the proof:
· : [♯∗A1, . . . , ♯∗An, q0] →⇓q0 I
· : [♯∗A1, . . . , ♯∗An, q0] →[q0] DR
The focusing discipline guarantees that the structure of the proof described
above is the only one available. Therefore our encoding soundly and faith-
fully encodes Turing computations, resulting in the following theorem. However,
notice additionally, that due to the non-determinism due to the ! left introduc-
tion, the encoding of Turing computations is not on the level of derivations, but
on the level of proofs, following the terminology in [24].
The absence of Weakening seems to reduce the expressive power of our sys-
tem SLLM in the case where not all instructions might have been applied within
a particular computation, see Remark 5. However, we are still able to get a
strong positive statement.
Theorem 5. We establish a strong correspondence between Turing computa-
tions and focused derivations in SLLM.
Namely, given a subset {I1, I2, . . . , Im} of Turing instructions, the following
two statements are equivalent:
(a) The deterministic Turing machine M leads from an initial conﬁguration,
represented as B1 · q1 · ξ · B2, to the ﬁnal conﬁguration q0 so that I1, I2, . . . ,
Im are only those instructions that have been actually applied in the given
Turing computation.

Soft Subexponentials and Multiplexing
513
(b) A sequent of the following speciﬁed form is derivable in SLLM.
!∇AI1, !∇AI2, . . . , !∇AIm, B1 · q1 · ξ · B2 ⊢q0
(9)
Corollary 2. The derivability problem for SLLM is undecidable.
Proof. Assume the contrary: a decision algorithm α decides whether any sequent
in SLLM is derivable or not. In particular, for any Turing machine M and any
initial conﬁguration of M, α decides whether any sequent of the form (9) is
derivable or not, where B1 · q1 · ξ · B2 represents the initial conﬁguration.
Then for each of the subsets
{I1, I2, . . . , Im} of the instructions of M, we
apply α to the corresponding sequent of the form (9).
If all results are negative then we can conclude that M does not terminate
on the initial conﬁguration, represented as B1 · q1 · ξ · B2.
Otherwise, M does terminate.
Since the halting problem for Turing machines is undecidable, we conclude
that α is impossible.
⊓⊔
Remark 5. The traditional approach to machine-based undecidability is to
establish a one-to-one correspondence between machine computations and
derivations in the system under consideration. Namely, given a Turing/Minsky
machine M, we encode M as a formula codeM, the ‘product’ of all CI repre-
senting instructions I. The ‘product’ ranges over all M’s instructions. Then we
have to establish that, whatever initial conﬁguration W we take, a sequent of
the form
codeM, W ⊢q0
(10)
is decidable in the logical system at hand iﬀM terminates on the initial conﬁg-
uration represented as W. Here one and the same codeM is used for each of the
initials W.
Now, suppose that M terminates on each of the initial conﬁgurations W1
and W2 so that some instruction represented by C is used within the computation
starting with W1, but it is not used within the computation starting with W2.
Then, because of W1, C takes an active part within codeM. On the other hand, C
becomes redundant and must be ‘ignored’ within the derivation for the sequent
codeM, W2 ⊢q0. To cope with the problem, it would be enough, e.g., to assume
Weakening in the system under consideration, which is not a case here.
The novelty of our approach to the machine-based undecidability is to cir-
cumvent the problem caused by the absence of Weakening, by changing the order
of quantiﬁers.
Instead of codeM, one and the same for all W, we introduce a ﬁnite number
of candidates of the form (9) with ranging over subsets {I1, I2, . . . , Im} of the
instructions of M.
According to Theorem 5, for any initial conﬁguration W, there exists a
sequent, say codeM,W , W ⊢q0, chosen from the ﬁnite set of candidates of the
form (9) such that the terminated computation starting from W corresponds to
an SLLM derivation for the chosen codeM,W , W ⊢q0. See also Corollary 2.
⊓⊔

514
M. Kanovich et al.
5.2
Decidable Fragments: Syntactically Deﬁned
An advantage of our approach is that, unlike the contraction rule, we can syn-
tactically control the multiplexing to provide decidable fragments still suﬃcient
for applications.
Theorem 6. If we bound k in the multiplexing rule in the calculus SLLM with
a ﬁxed constant k0, such a fragment becomes decidable.
Proof Sketch. Each application of Multiplexing of the form:
Γ1,
k times



A, A, . . . , A, Γ2 →C
Γ1, !A, Γ2 →C
!L (1 ≤k ≤k0)
multiplies the number of formulas with the factor k0, which provides an upper
bound on the size S of the sequents involved:
S = O(S0 · kn
0 )
here S0 is the size of the input, and n is a bound on the nesting depth of
the !-formulas. It suﬃces to apply a non-deterministic decision procedure but
generally on the sequents of exponential size.
⊓⊔
Theorem 7. In the case where we bound k in the multiplexing rule in the cal-
culus SLLM with a ﬁxed constant k0, and, in addition, we bound the depth of
nesting of !A, we get NP-completeness.
Remark 6. In fact Theorem 7 gives NP-procedures for parsing complex and com-
pound sentences in many practically important cases.
Remark 7. The strong lower bound is given by the following.
The !-free fragment that invokes only one implication, (A\B), and ∇A is still
NP-complete.
6
Concluding Remarks
In this paper we have introduced SLLM, a non-commutative intuitionistic linear
logic system with two subexponentials. One subexponential implements permu-
tation and the other one obeys the multiplexing rule, which is a weaker, miniature
version of contraction. Our system was inspired by subexponentials [23], linear
logic [6], light linear logic [7], soft linear logic [16], and easy linear logic [10].
We have also provided a complete focused proof system for our calcu-
lus SLLM. We have illustrated the expressive power of the focused system by
modelling computational processes.
The general aim is to develop more reﬁned and eﬃcient procedures for the
miniature versions of non-commutative systems, e.g., Lambek calculus and its

Soft Subexponentials and Multiplexing
515
extensions, based on the multiplexing rule. We aim to ensure a reasonable bal-
ance between the expressive power of the formal systems in question and the
complexity of their algorithmic implementation. The calculus SLLM, with mul-
tiplexing instead of contraction, provides simultaneously three properties: cut
elimination, substitution, and Lambek’s restriction.
One particular advantage of our system SLLM to systems in our previous
work [12,15] is the fact that it naturally incorporates Lambek’s non-emptiness
restriction, which is incompatible with stronger systems involving contrac-
tion [15]. Lambek’s non-emptiness restriction plays a crucial role in applications
of substructural (Lambek-style) calculi in formal linguistics (type-logical gram-
mars). Indeed, overcoming this impossibility result is one of our main motivations
in looking for a system that would satisfy cut-elimination, substitution, and the
Lambek’s restriction. The new system proposed in this paper is our proposed
solution to this subtle and challenging problem.
Moreover, there is no direct way of reducing the undecidability result in this
paper, say, to the undecidability results from our previous papers [11,14,15] by
a logical translation or representation of the logical systems. Since a number of
those systems are also undecidable, there are of course Turing reductions both
ways to the system in this paper. However, the Turing reductions factor through
the new representation of Turing machines introduced in this paper. That is, the
undecidability result in this paper is a new result.
Also the focused proof system proposed here has innovations to the
paper [12]. For example, our proof system contains relevant subexponentials
that do not allow contraction, something that was not addressed in [12]. Indeed
such subexponentials have also been left out of other focused proof systems, e.g.,
in the papers [23,25].
Another advantage of this approach is that, unlike the contraction rule, we
can syntactically control the multiplexing to provide feasible fragments still suf-
ﬁcient for linguistic applications.
As future work, we plan to investigate how to extend the systems proposed
in this paper with additives. In particular, the proposed focused proof system
using the introduced connectives ♯∗, ♯+ may have to be extended in order to
support additives. This is because it seems problematic, with these connectives,
to provide that the same number of copies of a contractable formula are used in
both premises when introducing an additive connective.
Financial Support. The work of Max Kanovich was partially supported by
EPSRC Programme Grant EP/R006865/1: “Interface Reasoning for Interact-
ing Systems (IRIS).” The work of Andre Scedrov and Stepan Kuznetsov was
prepared within the framework of the HSE University Basic Research Program
and partially funded by the Russian Academic Excellence Project ‘5–100’. The
work of Stepan Kuznetsov was also partially supported by grant MK-430.2019.1
of the President of Russia, by the Young Russian Mathematics Award, and by
the Russian Foundation for Basic Research grant 20-01-00435. Vivek Nigam’s
participation in this project has received funding from the European Union’s

516
M. Kanovich et al.
Horizon 2020 research and innovation programme under grant agreement No
830892. Vivek Nigam is also partially supported by CNPq grant 303909/2018-8.
References
1. Andreoli, J.-M.: Logic programming with focusing proofs in linear logic. J. Log.
Comput. 2(3), 297–347 (1992)
2. Barry, G., Hepple, M., Leslie, N., Morrill, G.: Proof ﬁgures and structural operators
for categorial grammar. In: Proceedings of the Fifth Conference of the European
Chapter of the Association for Computational Linguistics, Berlin (1991)
3. Carpenter, B.: Type-Logical Semantics. MIT Press, Cambridge (1998)
4. Cervesato, I., Pfenning, F.: A linear logic framework. In: Proceedings of the
Eleventh Annual IEEE Symposium on Logic in Computer Science, New Brunswick,
New Jersey, pp. 264–275. IEEE Computer Society Press, July 1996
5. Cervesato, I., Pfenning, F.: A linear logical framework. Inform. Comput. 179(1),
19–75 (2002)
6. Girard, J.-Y.: Linear logic. Theor. Comput. Sci. 50(1), 1–101 (1987)
7. Girard, J.-Y.: Light linear logic. Inform. Comput. 143(2), 175–204 (1998)
8. Kanovich, M.I.: Horn programming in linear logic is NP-complete. In: Proceedings
of the Seventh Annual IEEE Symposium on Logic in Computer Science , pp. 200–
210 (1992)
9. Kanovich, M.I., Okada, M., Scedrov, A.: Phase semantics for light linear logic.
Theor. Comput. Sci. 294(3), 525–549 (2003)
10. Kanovich, M.I.: Light linear logics with controlled weakening: expressibility, con-
ﬂuent strong normalization. Ann. Pure Appl. Logic 163(7), 854–874 (2012)
11. Kanovich, M., Kuznetsov, S., Nigam, V., Scedrov, A.: Subexponentials in non-
commutative linear logic. Math. Struct. Comput. Sci. 29(8), 1217–1249 (2019)
12. Kanovich, M., Kuznetsov, S., Nigam, V., Scedrov, A.: A logical framework with
commutative and non-commutative subexponentials. In: Galmiche, D., Schulz, S.,
Sebastiani, R. (eds.) IJCAR 2018. LNCS (LNAI), vol. 10900, pp. 228–245. Springer,
Cham (2018). https://doi.org/10.1007/978-3-319-94205-6 16
13. Kanovich, M., Kuznetsov, S., Scedrov, A.: L-models and R-models for Lambek cal-
culus enriched with additives and the multiplicative unit. In: Iemhoﬀ, R., Moortgat,
M., de Queiroz, R. (eds.) WoLLIC 2019. LNCS, vol. 11541, pp. 373–391. Springer,
Heidelberg (2019). https://doi.org/10.1007/978-3-662-59533-6 23
14. Kanovich, M., Kuznetsov, S., Scedrov, A.: Undecidability of the Lambek calcu-
lus with a relevant modality. In: Foret, A., Morrill, G., Muskens, R., Osswald,
R., Pogodalla, S. (eds.) FG 2015-2016. LNCS, vol. 9804, pp. 240–256. Springer,
Heidelberg (2016). https://doi.org/10.1007/978-3-662-53042-9 14
15. Kanovich, M., Kuznetsov, S., Scedrov, A.: Reconciling Lambek’s restriction, cut-
elimination, and substitution in the presence of exponential modalities. J. Logic
Comput. 30(1), 239–256 (2020)
16. Lafont, Y.: Soft linear logic and polynomial time. Theor. Comput. Sci. 318(1–2),
163–180 (2004)
17. Lambek, J.: The mathematics of sentence structure. Amer. Math. Monthly 65(3),
154–170 (1958)
18. Lincoln, P., Mitchell, J.C., Scedrov, A., Shankar, N.: Decision problems for propo-
sitional linear logic. Ann. Pure Appl. Logic 56(1–3), 239–311 (1992)

Soft Subexponentials and Multiplexing
517
19. Moot, R., Retor´e, C.: The Logic of Categorial Grammars. A Deductive Account of
Natural Language Syntax and Semantics. LNCS vol. 6850. Springer, Cham (2012).
https://doi.org/10.1007/978-3-642-31555-8
20. Moortgat, M.: Constants of grammatical reasoning. In: Bouma, G., Hinrichs, E.,
Kruijﬀ, G.-J., Oehrle, R. (eds.) Constraints and Resources in Natural Language
Syntax and Semantics, pp. 195–219. CSLI Publications, Stanford (1999)
21. Morrill, G.: Categorial Grammar: Logical Syntax, Semantics, and Processing.
Oxford University Press, Oxford (2011)
22. Morrill, G., Valent´ın, O.: Computational coverage of TLG: nonlinearity. In:
Kanazawa, M., Moss, L., de Paiva, V. (eds.) Proceedings of NLCS 2015. Third
Workshop on Natural Language and Computer Science, EPiC, vol. 32, pp. 51–63.
EasyChair (2015)
23. Nigam, N., Miller, D.: Algorithmic speciﬁcations in linear logic with subexpo-
nentials. In: Proceedings of the 11th ACM Sigplan Conference on Principles and
Practice of Declarative Programming, pp. 129–140 (2009)
24. Nigam, V., Miller, D.: A framework for proof systems. J. Automated Reason. 45(2),
157–188 (2010)
25. Nigam, V., Pimentel, E., Reis, G.: An extended framework for specifying and
reasoning about proof systems. J. Logic Comput. 26(2), 539–576 (2016)
26. Saurin, A.: Une ´etude logique du contrˆole. Ph.D. Thesis (2008)
27. Yetter, D.N.: Quantales and (noncommutative) linear logic. J. Symb. Log. 55(1),
41–64 (1990)

Mechanised Modal Model Theory
Yiming Xu1(B) and Michael Norrish2,1(B)
1 Australian National University, Canberra, Australia
u5943321@anu.edu.au
2 Data61, CSIRO, Canberra, Australia
michael.norrish@data61.csiro.au
Abstract. In this paper, we discuss the mechanisation of some funda-
mental propositional modal model theory. The focus on models is novel:
previous work in mechanisations of modal logic have centered on proof
systems and applications in model-checking. We have mechanised a num-
ber of fundamental results from the ﬁrst two chapters of a standard text-
book (Blackburn et al. [1]). Among others, one important result, the Van
Benthem characterisation theorem, characterises the connection between
modal logic and ﬁrst order logic. This latter captures the desired satu-
ration property of ultraproduct models on countably incomplete ultra-
ﬁlters.
1
Introduction
The theory of modal logic has long been a fruitful area when it comes to mechani-
sation. The proof systems are appealing, and the applications in model-checking
are of clear real-world interest. It helps also that the subject domain (proof cal-
culi and automata) are well-suited to “standard” theorem-proving technology
(rule inductions and interesting data types).
There has been much less work on the model theory behind modal logic;
indeed even in ﬁrst order logic, most developments concern themselves only with
model theory inasmuch as it is required to show completeness of an accompany-
ing proof system. As our experience demonstrates, it is also clear that modern
theorem-proving systems are not necessarily so well-suited to the mathematics
behind model theory. Harrison [5] complained in 1998 that the very notion of
validity is awkward to capture in HOL, and our own work shows up further
failings in simple type theory.
Nonetheless, there is much interesting mathematics to be found even in the
early chapters of a standard text such as Blackburn et al. [1]. The fact that
mechanising only as far as [1, Chapter 2] requires what we believe to be the
ﬁrst mechanisation of the notion of ultraproduct (ultimately leading to Lo´s’s
theorems and other results), is a strong suggestion that we are exploring novel
mathematical ground for interactive theorem-proving systems.
Contributions. This paper presents the ﬁrst mechanised proofs of a number of
basic results from the ﬁrst two chapters of Blackburn et al. [1] (e.g., bounded
c
⃝Springer Nature Switzerland AG 2020
N. Peltier and V. Sofronie-Stokkermans (Eds.): IJCAR 2020, LNAI 12166, pp. 518–533, 2020.
https://doi.org/10.1007/978-3-030-51074-9_30

Mechanised Modal Model Theory
519
morphisms, bisimulations and the ﬁnite model property via selection), as well
as
– Two versions of Lo´s’s theorem on the saturation of ultraproduct models;
– modal equivalence as bisimilarity between ultraﬁlter extensions; and
– a close approximation of Van Benthem’s Characterisation Theorem.
We also discuss where HOL’s simple type theory lets us down: some standard
results (including the best possible statement of Van Benthem’s Characterisation
Theorem) seem impossible to prove in our setting.
HOL4 Notation. All of our theorems have been pretty-printed to LaTEX from
the HOL theory ﬁles. We hope that most of the basic syntax is easy to follow. In
a few places we use CHOICE s to denote the arbitrary choice of an element from
set s (appealing to the Axiom of Choice). The power-set of a set s is written
P s. In a number of places, we use HOL’s “itself” type to allow us to explicitly
mention a type via a term. The type α itself has just one value inhabiting it
for any given choice of α; that value is written (:α).
Source Availability. Our HOL4 sources are available from GitHub at
https://github.com/u5943321/Modal-Logic
The sources build under HOL4 commit with SHA 03829d8986f.
2
Syntax, Semantics and the Standard Translation
In our mechanisation, we consider the basic modal language, in which the only
primitive modal operator is the ‘♦’. A modal formula is either of form Vm p, where
p is of type num, enumerating all the possible variable symbols, a disjunction
DISJ φ ψ (pretty-printed to φ ∨m ψ in most places), the falsity ⊥m, a negation
¬m φ, or, ﬁnally, of the form ♦φ. We deﬁne a data type called form to represent
the formulas of this modal language.
Deﬁnition 1. [1, Deﬁnition 1.9]
form = Vm num | DISJ form form | ⊥m | ¬m form | ♦form
If we wanted to consider modal operators with any arity, we should change the
last constructor of modal formulas so it takes two parameters: a natural number
indexing the modal operator, and a list of modal formulas. This would in turn
require a well-formedness predicate to be deﬁned over formulas to make sure
that modalities were applied to the right number of arguments.
A model where these formulas can be interpreted consists of a frame and a
valuation, where a β frame is a β-set with a relation on it, and a model adds
valuations for the variables present at each world:
Deﬁnition 2. [1, Deﬁnition 1.19]
β frame = ⟨⟨world : β →bool; rel : β →β →bool⟩⟩
β model = ⟨⟨frame : β frame; valt : num →β →bool⟩⟩

520
Y. Xu and M. Norrish
In the rest of the paper, the ﬁeld M .valt of a model M will be called the valuation,
and M W , M R and M V are used to denote the world set, the relation, and the
valuation of M respectively. The interpretation of modal formulas on a model is
given by the predicate satisfaction. We read ‘M , w ⊩φ’ as ‘φ is satisﬁed at the
world w in M ’.
Deﬁnition 3. [1, Deﬁnition 1.20]
M , w ⊩Vm p
def=
w ∈M W ∧w ∈M V p
M , w ⊩⊥m
def=
F
M , w ⊩¬m φ
def=
w ∈M W ∧M , w ̸⊩φ
M , w ⊩(φ1 ∨m φ2)
def=
M , w ⊩φ1 ∨M , w ⊩φ2
M , w ⊩♦φ
def=
w ∈M W ∧∃v. M R w v ∧v ∈M W ∧M , v ⊩φ
By requiring w ∈M W in various clauses above, we ensure that models’ world
sets must be non-empty if they are to satisfy any formulas.
Two worlds w1 ∈M1W
and w2 ∈M2W
are modal equivalent
(written
M1, w1 ↭M2,w2) if they satisfy the same set of modal formulas. If φ1, φ2 are
modal formulas, we say they are equivalent over β models (written φ1 ≡(:β) φ2)
if they are satisﬁed in the same worlds in every model:
Deﬁnition 4 (Notions of equivalence).
M , w ↭M ′, w ′
def= ∀φ. M , w ⊩φ
⇐⇒
M ′, w ′ ⊩φ
(φ1 : form) ≡(:β) (φ2 : form)
def=
∀(M : β model) (w : β). M , w ⊩φ1
⇐⇒
M , w ⊩φ2
We cannot omit the type parameter (:β) in the deﬁnition, as there would oth-
erwise be a type, namely the type of the underlying set of the models we are
talking about, that only appears on the right-hand side but not on the left-
hand side of the deﬁnition. HOL forbids such deﬁnitions for soundness reasons.
Also, HOL does not permit quantiﬁcation over types, so it is impossible to write
∀μ. φ1 ≡μ φ2, with μ a type. Therefore, this deﬁnition is not exactly encoding
the equivalence in the usual sense: when we mention equivalence of formulas in
usual mathematical language, we are implicitly referring to the class of all mod-
els, but the constraint here bans us from talking about all models of all possible
types at once.
A modal formula can be translated into a ﬁrst-order formula via the standard
translation. To mechanise this translation, we build on Harrison’s construction
of ﬁrst-order logic [5]. The ﬁrst-order connectives are decorated with an f. A ﬁrst
order model M is a set M .Dom with interpretation of function symbols M .Fun
and predicate symbols M .Pred. A valuation σ of M is a function that maps all
the natural numbers into the domain of M . If a ﬁrst-order formula φ is satisﬁed
in a ﬁrst-order model M with σ a valuation assigning free variables of φ elements
in the domain of M , we write M , σ ⊨φ.
For a modal formula φ, STx φ is the standard translation of φ using x as the
only free variable that may occur:

Mechanised Modal Model Theory
521
Deﬁnition 5. [1, Deﬁnition 2.45 (Standard Translation)]
STx (Vm p)
def=
Pf p (Vf x)
STx ⊥m
def=
⊥f
STx (¬m φ)
def=
¬f (STx φ)
STx (φ ∨m ψ)
def=
STx φ ∨f STx ψ
STx (♦φ)
def=
∃f (x + 1) (Rf (Vf x) (Vf (x + 1)) ∧f STx + 1 φ)
As one would expect, we translate ♦φ into an existential formula. To ensure we
use a fresh variable, we use x + 1 as our new variable symbol in this clause. The
standard translation gives a ﬁrst-order reformulation of satisfaction of modal
formulas:
Proposition 1. [1, Theorem 2.47 (i)]
⊢M , w ⊩φ
⇐⇒
mm2folm M , (λ n. w) ⊨STx φ
Here mm2folm is the function that turns a modal model into a ﬁrst-order model,
deﬁned as:
mm2folm M
def=
⟨⟨Dom := M W ; Fun := (λ n l. CHOICE M W );
Pred :=
(λ p zs.
case zs of
[ ] ⇒F
| [w1] ⇒w1 ∈M W ∧M V p w1
| [w1; w2] ⇒p = 0 ∧M R w1 w2 ∧w1 ∈M W ∧w2 ∈M W
| w1 :: w2 :: w3 :: ws ⇒F)⟩⟩
That is, the model obtained by converting a modal model M has domain M W ,
maps every term Fnf f l into an arbitrary world, maps each propositional letter to
distinct predicates on worlds, and uses one binary predicate (the “0th predicate”)
to encode the frame relation.
3
Basic Results
We discuss some highlights of mechanised results from Blackburn et al. [1, §2.1–
§2.3] below.
3.1
Tree-Like Property
A tree-like model is a model whose underlying frame is a tree. If Tr, a frame, is
also a tree with root r, we write tree Tr r:

522
Y. Xu and M. Norrish
Deﬁnition 6. [1, Deﬁnition 1.7]
tree Tr r
def=
r ∈Tr.world ∧(∀w. w ∈Tr.world ⇒Tr.rel |Tr.world
∗r w) ∧
(∀w. w ∈Tr.world ⇒¬Tr.rel w r) ∧
∀w. w ∈Tr.world ∧w ̸= r ⇒∃!w0. w0 ∈Tr.world ∧Tr.rel w0 w
The tree-like property says each satisﬁable modal formula can be satisﬁed in a
tree-like model:
Proposition 2. [1, Proposition 2.15]
⊢(M : β model), (w : β) ⊩(φ : form) ⇒
∃(M ′ : β list model) (r : β list). tree M ′.frame r ∧M ′, r ⊩φ
The world set of the tree-like model constructed from M is a set of lists of worlds
in M (such lists are eﬀectively paths from the root to various positions within
the tree). Thus, passing to a tree-like model does not preserve the model type.
The tree-like lemma is used to prove the ﬁnite model property via selection
afterwards.
3.2
Bisimulation
Though apparently verbose, the deﬁnition of bisimulation in HOL is straightfor-
ward.
Deﬁnition 7. [1, Deﬁnition 2.16 (Bisimulations)]
M1
Z
 M2
def=
∀w1 w2.
w1 ∈M W
1
∧w2 ∈M W
2
∧Z w1 w2 ⇒
(∀p. M1, w1 ⊩Vm p
⇐⇒
M2, w2 ⊩Vm p) ∧
(∀v1.
v1 ∈M W
1
∧M R
1 w1 v1 ⇒
∃v2. v2 ∈M W
2
∧Z v1 v2 ∧M R
2 w2 v2) ∧
∀v2.
v2 ∈M W
2
∧M R
2 w2 v2 ⇒
∃v1. v1 ∈M W
1
∧Z v1 v2 ∧M R
1 w1 v1
M , w  M ′, w ′
def= ∃Z. M
Z
 M ′ ∧w ∈M W ∧w ′ ∈M ′W ∧Z w w ′
It is trivial to prove by induction that bisimilar worlds are modal equivalent. As
the most signiﬁcant theorem on the basic theory of bisimulations, we proved the
Hennessy-Milner theorem, which states that modal equivalence and bisimulation
on image-ﬁnite models are the same thing. An image-ﬁnite model is a model
where every world can only be related to ﬁnitely many worlds. In HOL, we get:

Mechanised Modal Model Theory
523
Theorem 1. [1, Theorem 2.24 (Hennessy-Milner Theorem)]
⊢image-ﬁnite M1 ∧image-ﬁnite M2 ∧w1 ∈M W
1
∧w2 ∈M W
2
⇒
(M1, w1 ↭M2, w2
⇐⇒
M1, w1  M2, w2)
Bisimulation is an interesting topic in modal logic. Three other signiﬁcant
theorems on bisimulations (including an approximation of Van Benthem Char-
acterisation theorem) are discussed later.
3.3
Finite Model Property
There are two classical approaches to constructing ﬁnite models using model
theory, namely via selection and via ﬁltration. The complicated one is the former:
Given M1, w1 ⊩φ, where φ has degree k, we can construct M2, M3, M4 and M5
consecutively, such that M5 is the ﬁnite model we want, where:
– M2 is the tree-like model obtained from Proposition 2 with root w2 such that
M2, w2 ⊩φ.
– M3 is the restriction of M2 to height k.
– M4 is obtained from M3 by modifying the valuation into λ p v. if p
∈
prop-letters φ then M3
V p v else F, where prop-letters φ is the set of all
propositional letters used by φ.
The construction of M5 requires a lemma:
Lemma 1. [1, Proposition 2.29]
⊢FINITE (Φ : num →bool) ∧INFINITE U(:β) ⇒
∀(n : num). FINITE { φ | DEG φ ≤n ∧prop-letters φ ⊆Φ } /≡(:β)
The proof of Lemma 1 further relies on the following fact: Given a set A of
modal-formulas that is ﬁnite up to equivalence, if we combine the elements of
A using only connectives other than ♦, then we get only ﬁnitely many non-
equivalent formulas. To show this, we prove that there is an injection from the
set of equivalence classes of such combinations to a ﬁnite set. For the antecedent
of Lemma 1, we require the assumption that the universe of β is inﬁnite since we
rely on the fact that two modal formulas ♦φ1 and ♦φ2 are equivalent if and only
if φ1 and φ2 are equivalent. This would be easy to prove in set theory. However,
in simple type theory, the proof of φ1 ≡(:β) φ2 iﬀ♦φ1 ≡(:β) ♦φ2 requires us
(in the right-to-left direction) to be able to construct a model with a new world
inserted, something only sure to be possible if the β universe is inﬁnite. As the
construction used Proposition 2, we change the type of the model by passing to
a ﬁnite model via selection:
Theorem 2. [1, Theorem 2.34 (Finite model property, via selection)]
⊢(M1 : β model), (w1 : β) ⊩(φ : form) ⇒
∃(M : β list model) (v : β list). FINITE M W ∧v ∈M W ∧M , v ⊩φ

524
Y. Xu and M. Norrish
We also mechanised the ﬁltration approach, but omit the details for lack of
space. The advantage of ﬁltration is that the resulting ﬁnite model is over worlds
of the same type as in the starting model.
All the results proved above can be captured using num models everywhere.
If one takes β to be num (or any inﬁnite type) in Theorem 2, one can also exploit
the fact that numbers and lists of numbers have the same cardinality to derive
a ﬁnite model result that preserves the “input type”.
4
Mechanising Ultraﬁlters and Ultraproducts
A number of results in Blackburn et al. [1, §2.5–§2.7] rely on theorems about
ultraﬁlters and ultraproducts.
4.1
Ultraﬁlters
Given a non-empty set J, a set L ⊆
P J is a ﬁlter if it contains J itself, is
closed under binary intersection, and is closed upward.
Deﬁnition 8. [1, Deﬁnition A.12 (Filters)]
ﬁlter L J
def=
J ̸= ∅∧L ⊆P J ∧J ∈L ∧
(∀X Y . X ∈L ∧Y ∈L ⇒X ∩Y ∈L) ∧
∀X Z. X ∈L ∧X ⊆Z ∧Z ⊆J ⇒Z ∈L
We call L a proper ﬁlter if L is not the whole power set. An ultraﬁlter is a ﬁlter
U such that for every X ⊆J, exactly one of X or J \ X is in U. Intuitively,
subsets X ⊆J in an ultraﬁlter U are considered as ‘large’ subsets of J.
The ultraﬁlter theorem states that every proper ﬁlter is contained in an
ultraﬁlter:
Theorem 3. [1, Fact A.14, ﬁrst half]
⊢proper-ﬁlter L J ⇒∃U . ultraﬁlter U J ∧L ⊆U
(The proof uses Zorn’s Lemma.)
A subset A of the power set on J has ﬁnite intersection property if once
we take the intersection of a ﬁnite, nonempty family in A, the resultant set is
nonempty.
Deﬁnition 9. [1, Deﬁnition A.13 (Finite Intersection Property)]
⊢FIP A J
⇐⇒
A ⊆P J ∧∀B. B ⊆A ∧FINITE B ∧B ̸= ∅⇒ B ̸= ∅
As a corollary of ultraﬁlter theorem, a set with ﬁnite intersection property is
contained in an ultraﬁlter.

Mechanised Modal Model Theory
525
4.2
Ultraproducts
The notion of ultraproducts is deﬁned for sets, modal models, and ﬁrst-order
models.
Ultraproduct of Sets. A family of sets indexed by J is a function As in HOL.
For j
∈J, As j is the set indexed by j. Given a family As indexed by a non-
empty set J such that each As j is non-empty, the ultraproduct of As is deﬁned
as a quotient of the cartesian product of the family.
Deﬁnition 10. [1, Page 495 (Cartesian product)]
Cart-prod J As
def= { f | ∀j. j ∈J ⇒f j ∈As j }
If U is an ultraﬁlter on J, for two functions f, g in the Cartesian product
Cart-prod J As, we say f and g are U-equivalent (notation: f ∼As
U g) if the set
{ j | j
∈J ∧f j
= g j } (where the values of f and g agree) is in U. The
ultraproduct of As modulo U is the quotient of Cart-prod J As by ∼As
U .
Deﬁnition 11. [1, Deﬁnition 2.69 (Ultraproduct of Sets)]
ultraproduct U J As
def= Cart-prod J As/ ∼As
U
We write fU to denote the equivalence class that f belongs to. In the case where
As j = A for all j ∈J, the ultraproduct is called the ultrapower of A modulo
U.
Ultraproduct for Modal Models. Given a family M s of modal models
indexed by J and an ultraﬁlter U on J, the ultraproduct model of M s mod-
ulo U (notation: ΠU M s) is described as follows:
– The world set is the ultraproduct of world sets of M s modulo U.
– Two equivalence classes fU, gU of functions are related in ΠU M s iﬀthere
exist f0 ∈fU, g0 ∈gU, such that { j ∈J | (M s j)R (f0 j) (g0 j) } is in U.
– A propositional letter p is satisﬁed at fU in ΠU M s iﬀthere exists f0 ∈fU
such that { j | j ∈J ∧f0 j ∈(M s j)V p } is in U.
Deﬁnition 12. [1, Deﬁnition 2.70 (Ultraproduct of Modal Models)]
ΠU M s
def=
⟨⟨frame :=
⟨⟨world := ultraproduct U J (λ j. (M s j)W );
rel :=
(λ fU gU .
∃f0 g0.
f0 ∈fU ∧g0 ∈gU ∧
{ j | j ∈J ∧(M s j)R (f0 j) (g0 j) } ∈U )⟩⟩;
valt :=
(λ p fU . ∃f0. f0 ∈fU ∧{ j | j ∈J ∧f0 j ∈(M s j)V p } ∈U )⟩⟩

526
Y. Xu and M. Norrish
As ∼A
U is an equivalence relation, if one element in an equivalence class
satisﬁes the required condition, then all the elements in the equivalence class
will satisfy the condition. Therefore, if we replace all the existential quantiﬁers
with universal quantiﬁers in the above deﬁnition, the construction is still valid,
and will give the same model as the current deﬁnition.
The critical result we need about ultraproducts of modal models is a modal
version of the fundamental theorem of ultraproducts, also known as Lo´s’s theo-
rem.
Theorem 4 (Lo´s’s theorem, Modal version).
⊢ultraﬁlter U J ∧fU ∈(ΠU M s)W ⇒
(ΠU M s, fU ⊩φ
⇐⇒
∃f0. f0 ∈fU ∧{ j | j ∈J ∧M s j, f0 j ⊩φ } ∈U )
According to our intuition about ultraﬁlters, we can gloss this theorem to mean
that the ultraproduct of a family of modal models satisﬁes a modal formula if
and only if ‘most of’ the models in the family satisfy the formula. Though it is
possible to derive this result from Lo´s’s theorem of ﬁrst-order models using the
standard translation, our proof is direct, by structural induction on φ.
Ultraproducts for First-Order Models. Given a family M s of ﬁrst-order
models indexed by J and an ultraﬁlter U on J, the ultraproduct model of M s
modulo U (notation: fΠU M s) is given by:
– The domain is the ultraproduct of the domains of M s over U on J.
– A function named by symbol (natural number) n sends a list zs of equiv-
alence classes to the equivalence class of a function that sends j ∈J to
(M s j).Fun n l, where the k-th member of the list l is a representative of the
k-th member (which is an equivalence class) of zs.
– A predicate named by symbol p will hold for a list zs of equivalence classes if
and only if when we have a list zr, where k-th member is a representative of
the k-th member of zs, the set of elements j ∈J such that (M s j).Pred p zr
is in U.
Deﬁnition 13. [1, Deﬁnition A.18 (Ultraproduct of First-Order Models)]
fΠU M s
def=
⟨⟨Dom := ultraproduct U J (λ j. (M s j).Dom);
Fun :=
(λ n zs.
{ y |
(∀j. j ∈J ⇒y j ∈(M s j).Dom) ∧
{ j | j ∈J ∧y j = Fun-component M s n zs j } ∈U } );
Pred := (λ p zs. { j | j ∈J ∧Pred-component M s p zs j } ∈U )⟩⟩

Mechanised Modal Model Theory
527
Here we ﬁx the representative of each equivalence class fU to be CHOICE fU .
Therefore, as described above, the functions Fun-component and Pred-component
are:
Fun-component M s n fs i
def= (M s i).Fun n (MAP (λ fU . CHOICE fU i) fs)
Pred-component M s p zs i
def= (M s i).Pred p (MAP (λ fU . CHOICE fU i) zs)
The semantic behavior of ultraproduct models is characterised by Lo´s’s theo-
rem: for the ultraproduct of a family M s of ﬁrst-order models over an ultraﬁlter
U on J, a formula φ is satisﬁed under a valuation σ if and only if the set
indexing the models M s j in the family where φ is true under the valuation
λ v. CHOICE (σ v) j is in the ultraﬁlter U.
Theorem 5. [1, Theorem A.19 (Lo´s’s theorem)]
⊢ultraﬁlter U J ∧valuation (fΠU M s) σ ∧
(∀j. j ∈J ⇒wﬀm (M s j)) ⇒
(fΠU M s, σ ⊨φ
⇐⇒
{ j | j ∈J ∧M s j, (λ v. CHOICE (σ v) j) ⊨φ } ∈U )
where wﬀm M means the functions of M never map a list out of the domain of
M .
5
Ultraﬁlter Extensions
The ﬁrst application of the theory of ultraﬁlters above is to construct the ultraﬁl-
ter extension of a model, which has the nice property of being modally saturated
(m-saturated hereafter). To deﬁne m-saturation, we give the following three def-
initions (the ﬁrst two are called ﬁnitely satisﬁable, satisﬁable) consecutively:
Deﬁnition 14. [1, Deﬁnition 2.53]
satisﬁable-in Σ X M
def=
X ⊆M W ∧∃w. w ∈X ∧∀φ. φ ∈Σ ⇒M , w ⊩φ
ﬁn-satisﬁable-in Σ X M
def= ∀S. S ⊆Σ ∧FINITE S ⇒satisﬁable-in S X M
m-sat M
def=
∀w Σ.
w ∈M W ∧ﬁn-satisﬁable-in Σ { v | v ∈M W ∧M R w v } M ⇒
satisﬁable-in Σ { v | v ∈M W ∧M R w v } M
For m-saturated models, bisimulation and modal equivalence coincide:
Proposition 3. [1, Proposition 2.54]
⊢m-sat M1 ∧m-sat M2 ∧w1 ∈M W
1
∧w2 ∈M W
2
⇒
(M1, w1 ↭M2, w2
⇐⇒
M1, w1  M2, w2)

528
Y. Xu and M. Norrish
Given a model M and a set X of worlds of M , the set of worlds that ‘can see’
X (notation: M♦(X )) is the set of worlds w of M such that there exists some
v ∈X such that M R w v. We deﬁne the ultraﬁlter extension ueM of M as:
– The world set is the set of all ultraﬁlters on M W .
– Two ultraﬁlters u, v on M are related in the ultraﬁlter extension of M if for
every X ∈v, the set of worlds that can see X is in u.
– A propositional letter p to be satisﬁed at an ultraﬁlter v if and only if the set
of worlds in M which satisﬁes p is in v.
In HOL:
Deﬁnition 15. [1, Deﬁnition 2.57 (Ultraﬁlter Extension)]
ueM
def=
⟨⟨frame :=
⟨⟨world := { u | ultraﬁlter u M W } ;
rel :=
(λ u v.
ultraﬁlter u M W ∧ultraﬁlter v M W ∧
∀X . X ∈v ⇒M♦(X ) ∈u)⟩⟩;
valt := (λ p v. ultraﬁlter v M W ∧{ w | w ∈M W ∧M V p w } ∈v)⟩⟩
Using the ultraﬁlter theorem and some basic properties about ultraﬁlters, we
derive:
Proposition 4. [1, Proposition 2.59 (i)]
⊢ultraﬁlter u M W ⇒
({ w | w ∈M W ∧M , w ⊩φ } ∈u
⇐⇒
ueM , u ⊩φ)
In particular, every world w ∈M W is embedded as the principal ﬁlter πM W
w
on
M W generated by w in the ultraﬁlter extension or M . Also, the above leads to
the proof of the fact that the ultraﬁlter extension of every model is m-saturated.
The m-saturatedness of ultraﬁlter extensions together with Proposition 3 imme-
diately gives the central result about ultraﬁlter extension: bisimilarity of worlds
in a model M can be characterised as bisimilarity in ueM .
Theorem 6. [1, Proposition 2.62]
⊢w1 ∈M W
1
∧w2 ∈M W
2
⇒
(M1, w1 ↭M2, w2
⇐⇒
ueM1, πM W
1
w1
 ueM2, πM W
2
w2
)
6
Countable Saturatedness of Ultrapower Models
Given a ﬁrst-order model M with no information about interpretation of its
function symbols, we can expand the model M by adding an interpretation of
some function symbols. For our purpose, we are only interested in adding the

Mechanised Modal Model Theory
529
interpretation of ﬁnitely many nullary function symbols, also called constants.
We write expand M A f to denote the model that is the result of adding each
element in A to M as a new constant. Further, the function f is a bijection
between {0, · · · , n−1} and A, which is assumed to be ﬁnite, so that each nullary
function symbol c will be interpreted as f c in M ′.
Deﬁnition 16. [1, Deﬁnition A.9 (Expansion)]
expand M A f
def=
⟨⟨Dom := M .Dom;
Fun :=
(λ c l. if c < CARD A ∧l = [ ] then f c else CHOICE M .Dom);
Pred := M .Pred⟩⟩
As is apparent from the deﬁnition, the only diﬀerence between a model and its
expansion is the interpretation of function symbols.
A set Σ of ﬁrst-order formulas is called consistent with a model M if for
every ﬁnite subset Σ0 ⊆Σ, there exists a valuation of M such that all elements
of Σ0 are satisﬁed, in this case, we write consistent M Σ. A set Γ of ﬁrst-order
formula is an x-type if for each formula in Γ, the only free variable that may
contain is x. In this case, we write ‘ftype x Γ’ in HOL. If Γ is an x-type, when
evaluating formulas in Γ, the valuations will only control where the only free
variable x goes to. We say Γ is realised in M if there is an element w in the
domain of M such that M , (λ v. w) ⊨φ for all φ ∈Γ. In this case, we write
‘frealises M x Γ’ in HOL. Let M be a model and n be a natural number. If
for every A
⊆
M .Dom with |A| < n and every f : N →M .Dom, the model
expand M A f realises every x-type Γ that is consistent with expand M A f , then
we say M is n-saturated. In HOL:
Deﬁnition 17. [1, Deﬁnition 2.63 (n-Saturated)]
n-saturated M n
def=
∀A Γ x f .
IMAGE f U(:num) ⊆M .Dom ∧FINITE A ∧CARD A ≤n ∧
A ⊆M .Dom ∧BIJ f (count (CARD A)) A ∧
(∀φ. φ ∈Γ ⇒form-functions φ ⊆{ (c, 0) | c < CARD A } ) ∧
ftype x Γ ∧consistent (expand M A f ) Γ ⇒
frealises (expand M A f ) x Γ
We say M is countably saturated if M is n-saturated for every natural number
n. The ultimate goal is to prove a lemma to be used in the proof of Van Benthem
characterisation theorem: For a family of non-empty models, their ultraproduct
on a countably incomplete ultraﬁlter is countably saturated.
Lemma 2. [1, Lemma 2.73]
⊢(∀j. j ∈J ⇒(M s j)W ̸= ∅) ∧countably-incomplete U J ⇒
countably-saturated (mm2folm (ΠU M s))

530
Y. Xu and M. Norrish
Here a countably incomplete ultraﬁlter is an ultraﬁlter that contains a countably
inﬁnite family that intersects to the empty set. We prove in HOL that such
ultraﬁlters do exist using Theorem 3. The above theorem is not simply a direct
consequence of Lo´s’s theorem: that result is about ultraproducts of ﬁrst-order
models, and it says nothing about expansion. But to prove Lemma 2, we must
prove a statement for an expanded ﬁrst-order model, and this ﬁrst order model
is itself obtained by converting a ultraproduct of modal models.
To deal with this issue, the key observation is that constants are nothing
more than forcing some symbols to be sent to some points in a model under
every valuation, hence rather than use nullary function symbols, we ﬁx a set of
variable letters, each corresponding to a function symbol, and only consider the
valuations that send these variable letters to certain ﬁxed points. With this idea,
we can remove all the constants in a formula, and hence change our scope from
an expanded model back to the unexpanded model. To get rid of the constants
{0, · · · , n−1}, we replace every Vf m with Vf (m + n), and replace every constant
Fnf c [ ] by Vf c. This operation is done by the function shift-form which takes
a natural number (the number of constants we want to remove), and a ﬁrst-
order formula (where the only function symbols may appear are the constants
0, · · · , n−1). Since 0, · · · , n−1 in a shifted formula are now designed to be sent
to ﬁxed places f 0, · · · , f (n−1), it does not make sense to assign these variable
symbols anywhere else. Therefore, to talk about evaluation of shifted formula,
the ﬁrst thing is to make sure that the valuations we are considering send the
variables which actually denote constants to the right place. Hence we shift the
valuations accordingly, and then prove that a formula is satisﬁed on an expanded
model is satisﬁed under a valuation if and only if the shifted formula is satisﬁed
under the shifted valuation. Also, we prove that ‘taking the ultraproduct ﬁrst-
order model commutes with the convertion from modal to a ﬁrst-order model on
certain formulas’, in the sense that the resulting models satisﬁes the same ﬁrst-
order formulas without function symbols. By putting these two results together,
we prove Lemma 2 using the proof in Chang and Keisler [3].
7
Van Benthem’s Characterisation Theorem
Note that the standard translation of any modal formula can only contain unary
predicate symbols which correspond to propositional letters, one binary predicate
symbol which corresponds to the relation, and no function symbols. A ﬁrst-order
formula which only uses these symbols is called an L1
τ-formula. An L1
τ-formula
which contains only one free variable is called invariant under bisimulation if for
all models M and N with w ∈M W and v ∈N W , if there exists a bisimulation
relation between M and N relating w and v, then φ holds at w if and only if it
holds at v when both M and N are viewed as ﬁrst-order models.

Mechanised Modal Model Theory
531
Deﬁnition 18. [1, Deﬁnition 2.67 (Invariant for Bisimulations)]
invar4bisim (x : num) (:α) (:β) (φ : folform)
def=
FV φ ⊆{ x } ∧L1
τ φ ∧
∀(M : α model) (N : β model) (v : β) (w : α).
M , w  N , v ⇒
(mm2folm M , (λ (x : num). w) ⊨φ
⇐⇒
mm2folm N , (λ (x : num). v) ⊨φ)
Because of the same problem we met when deﬁning equivalence of formulas,
the type parameters are necessary here. However, although it is possible to prove
theorems for diﬀerent types α and β in the above deﬁnition, in the theorems to
come, we will only consider the case where α and β are the same.
The Van Benthem characterisation theorem says an L1
τ formula with at most
one free variable x is invariant under bisimulation precisely when it is equiva-
lent to the standard translation of some modal formula at x. It is immediate
from Proposition 1 that every such formula which is equivalent to a standard
translation is invariant for bisimulation. We cannot prove it as an ‘if and only
if’ statement, since according to the proofs in [1], we can only prove the two
directions separately as:
Proposition 5. [1, Theorem 2.68, as two separate directions]
⊢FV δ ⊆{ x } ∧L1
τ δ ∧δ
f≡(:α) STx φ ⇒invar4bisim x (:α) (:α) δ
⊢INFINITE U(:α) ∧
invar4bisim x (:(num →α) →bool) (:(num →α) →bool) δ ⇒
∃φ. δ
f≡(:α) STx φ
which cannot be put together into a double implication. To see the reason: given
an L1
τ-formula φ with no more then one free variable, by the second theorem
above, if φ is invariant under bisimulation for models with (num →α) →bool-
worlds, then φ is equivalent to a standard translation on a model with α-worlds.
However, if we want to prove the converse of this statement, we need to start
with the assumption that φ is equivalent to a standard translation on models
with α-worlds, and prove that φ is invariant for bisimulation for models with
(num
→
α)
→
bool-worlds. But by the ﬁrst theorem above, we can only
conclude φ is invariant for bisimulation for models of type α. The point is that
it is not the fact that all our desired operations can be taken within a type. In
particular, we cannot take ultraproducts of models and preserve cardinalities.
The cardinality of the type universe of (num →α) →bool is too large to
be embedded into α, so we cannot just ﬁx the ‘base type’ to be α and get an
‘if and only if’ statement-we cannot derive φ is invariant for bisimulation for
models with (num →α) →bool-worlds from the fact that φ is invariant for
bisimulation for models with α-worlds. If we could quantify over types (as we
could in a theorem prover based on dependent type theory), then we can could
deﬁne ‘invariant under bisimulation for models of every type’, and hence prove
the original statement of Van Benthem characterisation theorem.

532
Y. Xu and M. Norrish
For the proof of the two theorems above, the ﬁrst one is immediate from
Proposition 1, and the second one requires another critical lemma saying ‘modal
equivalence between two worlds implies bisimilarity of the two worlds when
embedded in some other models’. More precisely, if two worlds w
∈
M W
and v ∈N W are modal equivalent, then we can ﬁnd an ultraﬁlter U on J such
that in ultrapower models of M and N on U respectively, there is a bisimulation
between the worlds corresponding to w and v.
Theorem 7. [1, Theorem 2.74, one direction]
⊢w ∈M W ∧v ∈N W ∧(∀φ. M , w ⊩φ
⇐⇒
N , v ⊩φ) ⇒
∃U J.
ultraﬁlter U J ∧
ΠU (λ j. M ), { f | (λ j. w) ∼worlds (λ j. M)
U
f }  ΠU (λ j. N ), { g |
(λ j. v) ∼worlds (λ j. N)
U
g }
The proof of the above relies on Lemma 2.
8
Conclusion
To summarise, we have mechanised all of the results (appearing as propositions,
lemmas and theorems) in the ﬁrst two chapters in Blackburn et al. [1] that can
be captured by the HOL logic, and which are about the basic modal language.
The exceptions are:
– The result in Sect. 2.6 about ‘deﬁnability’, which requires a deﬁnition of the
‘models closed under taking ultraproducts’. Simple type theory cannot cap-
ture such large sets.
– The result about ‘safety’ in Sect. 2.7 is a result about the PDL language, which
has inﬁnitely many modal operators. For the moment, we have restricted our
attention to the basic modal language, with only ♦(and the derived □).
The two characterisation theorems from Blackburn et al. [1], namely Theorem
2.68 (Van Benthem’s Characterisation Theorem) and Theorem 2.78, are the only
two mechanised theorem such that translating the ‘if and only if’ statements
from set theory into simple theory does not yield an ‘if and only if’ statement.
Blackburn et al.’s proof of Theorem 2.78 has the same pattern as Van Benthem’s
Characterisation Theorem (discussed earlier), and is less complicated.
For each of the mechanised deﬁnitions and results, we write the statement in
HOL to be as close as possible to the original statement in [1]. We believe that
this makes it as easy as possible for people who are interested in mechanising
other results in [1] to continue with our work as a starting point. The work on
ultraproducts up to Lo´s’s theorem is independent of our work on modal model
theory, and should be generally useful in other model-theoretic applications.

Mechanised Modal Model Theory
533
8.1
Related Work
We believe that we are the ﬁrst to mechanise the bulk of the results in this
paper. Of course, much work has been done in this and similar areas. For exam-
ple, de Wind’s thesis [7] is a notable early mechanisation of modal logic, mainly
focusing on proving the validity of modal formulas via natural deduction. Of
similar vintage is Harrison’s mechanisation of foundational results about ﬁrst
order model theory [5], in particular compactness. We used this mechanisation
directly in our own work. A great deal of work has also been done in the mech-
anisation of ﬁrst order proof theory, such as the recent pearl by Blanchette et
al. [2], showing completeness in elegant fashion.
The connections between modal logic and process algebra are well-understood
and there has been a great deal of mechanised work on the operational theory
of such (co-)algebraic systems, starting at least as far back as Nesi [6]. Our
proof of the Hennessy-Milner theorem (Theorem 1) is a gesture in this direction,
but Van Benthem’s theorem is much deeper and uses bisimulations as a tool to
understanding the connection between modal and ﬁrst order logics, rather than
as a connection to process algebras.
Mechanised work with ultraﬁlters began with Fleuriot’s use of them to mech-
anise non-standard analysis [4]. We are unaware of any previous mechanised use
of ultraproducts or ultrapowers.
References
1. Blackburn, P., de Rijke, M., Venema, Y.: Modal Logic. Cambridge University Press,
Cambridge (2001)
2. Blanchette, J.C., Popescu, A., Traytel, D.: Uniﬁed classical logic completeness. In:
Demri, S., Kapur, D., Weidenbach, C. (eds.) IJCAR 2014. LNCS (LNAI), vol. 8562,
pp. 46–60. Springer, Cham (2014). https://doi.org/10.1007/978-3-319-08587-6 4
3. Chang, C.C., Keisler, H.J.: Model Theory. North Holland, Amsterdam (1990)
4. Fleuriot, J.: A Combination of Geometry Theorem Proving and Nonstandard Anal-
ysis with Application to Newton’s Principia. Springer, London (2001). https://doi.
org/10.1007/978-0-85729-329-9
5. Harrison, J.: Formalizing basic ﬁrst order model theory. In: Grundy, J., Newey, M.
(eds.) TPHOLs 1998. LNCS, vol. 1479, pp. 153–170. Springer, Heidelberg (1998).
https://doi.org/10.1007/BFb0055135
6. Nesi, M.: Mechanising a modal logic for value-passing agents in HOL. Elec-
tron. Notes Theor. Comput. Sci. 5, 31–46 (1996). https://doi.org/10.1016/S1571-
0661(05)80682-6
7. de Wind, P.: Modal Logic in Coq. Master’s thesis, Vrije Universiteit (2001)

Author Index
Affeldt, Reynald
II-3
Aitken, Dave
II-464
Allamigeon, Xavier
II-185
Baader, Franz
I-163, I-413
Baanen, Anne
II-21
Baranowski, Marek
I-13
Barbosa, Haniel
I-141
Barnett, Lee A.
I-32
Barrett, Clark
I-218, I-238
Basin, David
I-432
Baumgartner, Peter
I-337
Bhayat, Ahmed
I-259, I-278, II-361
Biere, Armin
I-32
Blackburn, Patrick
I-474
Blanchette, Jasmin
I-316
Blazy, Sandrine
II-324
Bohrer, Rose
I-454
Bonacina, Maria Paola
I-356
Brakensiek, Joshua
I-48
Bray, Matt
II-464
Bride, Hadrien
II-369
Brown, Chad E.
II-489
Cai, Cheng-Hao
II-369
Calvanese, Diego
I-181
Cerna, David
I-32
Chew, Leroy
I-66
Chlipala, Adam
II-119
Chvalovský, Karel
II-448
Clymo, Judith
I-66
Cohen, Cyril
II-3
Cohen, Liron
I-375
Cruanes, Simon
II-464
Czajka, Łukasz
II-28
Dalmonte, Tiziano
II-378
Dardinier, Thibault
I-432
De Angelis, Emanuele
I-83
de Moura, Leonardo
II-167
de Vilhena, Paulo Emílio
II-204
Delaware, Benjamin
II-119
Dong, Jin Song
II-369
Duarte, André
II-388
Dutertre, Bruno
I-103
Eßmann, Robin
II-291
Fioravanti, Fabio
I-83
Fontaine, Pascal
I-238
From, Asta Halkjær
I-474
Fürer, Basil
II-58
Ghilardi, Silvio
I-181
Gianola, Alessandro
I-181
Girlando, Marianna
II-398
Gleiss, Bernhard
I-297, I-402
Gligoric, Milos
II-97
Goertzel, Zarathustra Amadeus
II-408
Gore, Rajeev
II-369
Graham-Lengrand, Stéphane
I-103
Gross, Jason
II-119
Gunther, Emmanuel
II-221
Gutiérrez, Raúl
II-416, II-436
Guttmann, Walter
II-236
Hague, Matthew
I-122
Hales, Thomas
II-254
Hashim, Mohammed
II-480
Hausmann, Daniel
I-482
He, Shaobo
I-13
Heimes, Lukas
I-432
Heule, Marijn
I-48
Hóu, Zhé
II-369
Ignatovich, Denis
II-464
Jakubův, Jan
II-448
Jovanović, Dejan
I-103
Kagan, Elijah
II-464
Kanishev, Kostya
II-464
Kanovich, Max
I-500
Kapur, Deepak
I-163
Katz, Ricardo D.
II-185
Kerjean, Marie
II-3

Kirst, Dominik
II-79
Kohlhase, Michael
I-395
Korovin, Konstantin
II-388
Kovács, Laura
I-297
Krstić, Srđan
I-432
Kuznetsov, Stepan
I-500
Lammich, Peter
II-307
Lampert, Timm
I-201
Lange, Jane
I-238
Larchey-Wendling, Dominique
II-79
Larraz, Daniel
I-141
Léchenet, Jean-Christophe
II-324
Lechner, Mathias
I-13
Li, Junyi Jessy
II-97
Lin, Anthony W.
I-122
Lochbihler, Andreas
II-58
Lucas, Salvador
II-416, II-436
Mackey, John
I-48
Maclean, Ewen
II-464
Mahboubi, Assia
II-3
Mahony, Brendan
II-369
Marić, Filip
II-270
McCarthy, Jim
II-369
Mometto, Nicola
II-464
Montali, Marco
I-181
Naeem, Zan
II-480
Nakano, Anderson
I-201
Narváez, David
I-48
Nguyen, Thanh Son
I-13
Nie, Pengyu
II-97
Nigam, Vivek
I-500
Nipkow, Tobias
II-291, II-341
Norrish, Michael
I-518
Nötzli, Andres
I-218
Olivetti, Nicola
II-378
Olšák, Miroslav
II-448
Pagano, Miguel
II-221
Palmskog, Karl
II-97
Passmore, Grant
II-464
Paulson, Lawrence C.
II-204
Pease, Adam
II-158, II-472
Pettorossi, Alberto
I-83
Pichardie, David
II-324
Piotrowski, Bartosz
II-448
Piskac, Ruzica
I-3
Pit-Claudel, Clément
II-119
Platzer, André
I-454
Pozzato, Gian Luca
II-378
Proietti, Maurizio
I-83
Rabe, Florian
I-395
Rakamarić, Zvonimir
I-13
Raszyk, Martin
I-432
Rath, Jakob
I-297
Rau, Martin
II-341
Raya, Rodrigo
II-254
Reger, Giles
I-259, I-278, II-361
Reis, Giselle
II-480
Reynolds, Andrew
I-141, I-218
Ringeissen, Christophe
I-238
Rivkin, Andrey
I-181
Robillard, Simon
I-316, II-291
Rouhling, Damien
II-3
Rowe, Reuben N. S.
I-375
Rümmer, Philipp
I-122
Rydval, Jakub
I-413
Sacerdoti Coen, Claudio
I-395
Sakaguchi, Kazuhiko
II-3, II-138
Sánchez Terraf, Pedro
II-221
Scedrov, Andre
I-500
Schaefer, Jan Frederik
I-395
Schneider, Joshua
I-432, II-58
Schröder, Lutz
I-482
Schulz, Stephan
II-158
Sheng, Ying
I-238
Straßburger, Lutz
II-398
Strub, Pierre-Yves
II-185
Suda, Martin
I-402, II-448
Tinelli, Cesare
I-141, I-218
Tourret, Sophie
I-316
Traytel, Dmitriy
I-432, II-58
Ullrich, Sebastian
II-167
Urban, Josef
II-448, II-489
Villadsen, Jørgen
I-474
536
Author Index

Waldmann, Uwe
I-316
Wang, Peng
II-119
Winkler, Sarah
I-356
Wu, Zhilin
I-122
Xu, Yiming
I-518
Zohar, Yoni
I-238
Zombori, Zsolt
II-489
Author Index
537

