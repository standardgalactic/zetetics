Cognitive Robotics 3 (2023) 45–53 
Contents lists available at ScienceDirect 
Cognitive Robotics 
journal homepage: http://www.k eaipublishing.com/en/journals/cogniti ve-robotics/ 
Lightweight YOLOv5 model based small target detection in power 
engineering 
Ping Luo, Xinsheng Zhang ∗ , Yongzhong Wan 
Nanjing Electricity Supply Industry General Corp., 333 Hanzhongmen Street, Gulou District, Nanjing, 2100000 
a r t i c l e 
i n f o 
Keywords: 
YOLOv5 
Small target detection 
Power engineering 
Light-weight model 
a b s t r a c t 
Deep learning architectures have yielded a signiﬁcant leap in target detection performance. How- 
ever, the high cost of deep learning impedes real-world applications, especially for UAV and UGV 
platforms. Moreover, detecting small targets is still of lower accuracy in contrast to the large 
ones. Aiming to comprehensively handle these two issues, a novel SP-CBAM-YOLOv5 architec- 
ture is proposed. The main novelty of our hybrid model lies in the cooperation of the attention 
mechanism and the typical YOLOv5 architecture, which can largely improve the performance of 
the small target detection. Moreover, the depth convolution and knowledge distillation are jointly 
introduced for lightening the model architecture. To evaluate the performance of our proposed 
SP-CBAM-YOLOv5, we built a novel dataset containing challenging scenes of power engineer- 
ing. Experimental results on this benchmark demonstrate that our proposed SP-CBAM-YOLOv5 
achieves a competitive performance in contrast to the other YOLO architectures. Besides, our 
lightweight YOLOv5 has more than 70% decrease of parameters. Moreover, the ablation study is 
conducted to demonstrate the compact architecture of SP-CBAM-YOLOv5. 
1. Introduction 
Power engineering is commonly conducted in wild scenes where intelligent monitoring platforms (e.g. the Unmanned Aerial 
Vehicles (UAVs) and ﬁxed cameras) are required to guarantee engineering safety [ 1 , 2 ]. Target detection plays an important role in 
intelligent monitoring platforms, which aims to identify the location of multiple targets. However, target detection in wide scenes 
is challenging, which can be explained with two points. First, the targets in the wild scenes are of small sizes due to the long-range 
shot. Second, power engineering scenes are characterized by complicated background and noises. Aiming to solve these problems, 
deep learning architectures have been developed to detect small targets in complicated scenes, which have yielded a signiﬁcant 
leap in target detection performance. However, when dealing with real-world tasks, the trade-oﬀbetween detection accuracy and 
eﬃciency should be taken into consideration. In this aspect, current deep learning architectures have some limitations, since they 
commonly require large computational and storage resources. For example, CNNs have demonstrated excellent performance in two- 
stage detection, whereas they should be conducted on powerful platforms and they are not suited to real-time applications [3] . 
Aiming to solve this problem, the YOLO (you only look once) networks have been established, which can eﬃciently obtain real-time 
detection results [4] . However, powerful resources are necessary for training YOLO networks. As the result, none of existing methods 
can nicely cater for small target detection in power engineering. Nevertheless, challenges are increased by the small target size, low 
resolution and cluster background. In practice, the tradeoﬀbetween model accuracy and computational cost should be taken into 
consideration. Many industrial applications require that the computational models should be deployed on mobile or edge devices, 
such that lightweight and low-cost devices are required to achieve cheaper and faster data processing. 
∗ Corresponding author. Tel.: + 86 15251867359. 
E-mail address: 857792731@qq.com (X. Zhang) . 
https://doi.org/10.1016/j.cogr.2023.03.002 
Received 16 January 2023; Received in revised form 29 March 2023; Accepted 30 March 2023 
Available online 31 March 2023 
2667-2413/© 2023 The Authors. Publishing Services by Elsevier B.V. on behalf of KeAi Communications Co. Ltd. This is an open access article 
under the CC BY-NC-ND license ( http://creativecommons.org/licenses/by-nc-nd/4.0/ ) 

P. Luo, X. Zhang and Y. Wan 
Cognitive Robotics 3 (2023) 45–53 
UAV platforms have been widely utilized in wild practice, which have a wide ﬁeld size of vision and thus provide an aerial view 
to the site of power engineering. However, the UAV monitoring is diﬃcult, which can be explained as follows: 
i Small dataset —current datasets for training detection model are collected in common scenes, whereas no existing dataset can be 
applied in the target detection in power engineering. 
ii Small target —typical targets in power engineering include the constructors and engineering equipment, which are of the small 
size in the aerial view. This problem signiﬁcantly degenerates target detection accuracy. 
iii Low load —UAV platforms cannot carry heavy hardware resources, while the target detection is necessarily conducted in wild 
scenes. A lightweight calculation model is required for remote detection. 
Aiming to solve above problems, this paper proposes a novel updated YOLO architecture, which can reach a good tradeoﬀbetween 
the detection accuracy and cost. The contributions of our proposed architecture are three-folded: 
i A novel dataset is built for target detection in the power engineering project, which contains various targets and wide scenes. 
ii A attention mechanism is introduced to YOLO architectures, which can signiﬁcantly increase the accuracy of small target detection. 
iii A knowledge distillation strategy is introduced to train the teacher and student networks, the lightweight student network is 
applied in practice. 
In general, our proposed model achieves an excellent performance speciﬁc to the small target detection task in power engineering 
projects. The major aim of this study is to obtain promising small target detection results at a low cost. To the best of our acknowledge, 
the hybrid model proposed in this paper is the ﬁrst that employing this type of hybrid model to detect small targets in power 
engineering scenes. Our proposed model will attract interests of the engineers, since it can solve practical problems in real-world 
power engineering projects. The rest of the paper is organized as follows. Section 2 reviews related works for deep learning-based 
target detection. The proposed method is described in Section 3 . Section 4 includes the experimental comparison and discussion. The 
summary and conclusion are presented in Section 5 . 
2. Related works 
There are generally two groups of deep learning-based target detection methods. The ﬁrst group is based on the double-stage 
framework, and the second group achieves detection with single-stage frameworks. The double-stage method can obtain better results 
in term of detection accuracy, whereas their architectures are much complicated and can hardly suit real-time tasks. By contrast, the 
single-stage architectures are more eﬃcient, but their accuracy is mostly worse than the former one. 
2.1. Double-stage framework for target detection 
Pioneering double-stage deep learning frameworks are mostly based on the classic CNN (convolutional neural network). For ex- 
ample, the backbone of R-CNN is introduced and updated for target detection [5] . Due to the high computational cost of CNNs, 
further eﬀorts are mainly dedicated to improving detection eﬃciency. In this aspect, typical methods include the Spatial Pyramid 
Pooling Net (SPPNet) and Fast R-CNN models. The key advantage of these two models lies in that they just input samples to the 
learning networks once, while other candidate frames are processed on a certain layer. As the result, model iterations number are 
reduced, thus the detection process is accelerated [6] . Aiming to shorten the selective search time during detection, the traditional 
candidate frame generation methods are replaced with the Region Proposal Network (RPN) to achieve an end-to-end model train- 
ing, which is called Faster R-CNN [7] . A Region based Fully Convolutional Network (RFCN) is proposed to maintain the translation 
invariance after ROI pooling by using position-sensitive score maps [8] . In contrast to previous detection networks, the advantage 
of RFCN lies in that it reduces the number of layers after ROI pooling. Moreover, RON, DSSD and TridentNet [9–11] are developed 
for single-stage detection frameworks. In addition to the eﬀorts for updating model architectures, there are also studies focusing 
on the model optimization. Typical samples include the Non-Maximum Suppression (NMSnon) algorithm, soft-NMS and softer-NMS 
[ 12 , 13 ]. Although above methods have won many successes in detection accuracy or eﬃciency, small-size target detection is still 
a common challenge for current models. The main reason lies in the weak feature representation of the small targets and distur- 
bance from the complicated background. A large convolutional scale may overlook the detailed features, while a small convolutional 
scale may signiﬁcantly reduce the model eﬃciency and introduce background noises. This problem motivates our study in this 
paper. 
2.2. Single-stage framework for target detection 
Although many eﬀorts have been dedicated to accelerating double-stage frameworks, their complicated backbones degrade their 
performance in real-time applications. Aiming to achieve the end-to-end detection process, various eﬀorts have been dedicated to 
designing diﬀerent types of single-stage frameworks. In general, the merit of the single-stage frameworks is their advantage in compu- 
tational eﬃciency and compact architecture. A typical end-to-end target detection architecture is based on the YOLO architectures, 
which simultaneously achieves the candidate frame representation, classiﬁcation and regression in the same depth without any 
skips or branches. From previous records, the detection speed is signiﬁcantly increased from 4fps in Fast-CNN to 45fps in YOLO 
architectures [14] . Aiming to reach an optimal tradeoﬀbetween accuracy and eﬃciency, the SSD (single-shot multibox detector) 
46 

P. Luo, X. Zhang and Y. Wan 
Cognitive Robotics 3 (2023) 45–53 
is proposed to achieve a single-stage detection [15] . The main contribution of SSD lies in that it improves the performance of the 
lightweight single-stage framework in the term of detection accuracy. Aiming to improve the detection accuracy of the YOLOv1, 
YOLOv2 is established, which introduces the residual network, Darknet-53, Feature Pyramid Networks into the YOLO architecture 
[16] . Further improvements on the YOLO architecture are realized by employing a multi-scale fusion strategy, which motivates 
the development of the YOLOv3 architecture. From previous experiments, YOLOv3 achieves the equivalent performance with 1/3 
time cost in contrast to SSD [17] . Model pruning is a feasible strategy to improve detection eﬃciency. For example, an automated 
iterative procedure of incremental model pruning is achieved by 4 steps which are applied in the classic YOLO networks. Aiming 
to improve the YOLOv3 architecture, the Spatial Pyramid Pooling module (SPP) is added at the top of the architecture, such that 
model pruning is achieved. From previous results, the lightest edition of YOLOv3, i.e., SlimYOLOv3-SPP3, obtains a performance 
with 18% relative drop of the accuracy but leads to 80% faster [18] . The YOLO-based architecture is one of the most eﬃcient 
target detection architectures, however they can hardly detect targets which are of the small size, especially in complicated wild 
scenes. 
2.3. Small target detection 
Small target detection is a hotspot in recent study, since increasing long-range shot devices have been deployed in various moni- 
toring platforms. Aiming to increase the sensitivity of the YOLO networks, many works reduce the receptive ﬁeld of the YOLOv3 
and add the L2 regularization term to avoid the overﬁtting problem [19] . Experiments on the DOTA dataset demonstrate that 
the accuracy of this updated model is up to 3% in contrast to YOLOv3. However, its time cost is relatively increased due to the 
larger model scale. This study demonstrates that the 52 × 52 receptive ﬁeld suites the small target detection. As the result, the 
YOLO-E network is built with 52 × 52 and 104 × 104 receptive ﬁelds [20] . Moreover, a two-way residual sub-module is added 
to reduce the network depth. YOLO-E achieves a signiﬁcantly better performance with a minor increase of the time cost. Consid- 
ering the superior performance of the VGG16 model, a cascaded detector is composed by two CNNs, which realizes the detection 
by target identifying and locating [21] . As mentioned above, another challenge of small target detection in the wild scenes lies 
in the cluttered background. This problem is worsened by the low-resolution of the aerial imagery. A typical solution to handle 
this issue is to combine the super-resolution and detection methods. For example, the Joint Super-Resolution and Vehicle Detec- 
tion Network (Joint-SRVDNet) is built by connecting a multi-scale MsGAN for image super-resolution and a YOLOv3 for small 
target detection [22] . This work demonstrates that cooperative deep learning can extract more informative features and detect 
more targets in low-resolution aerial images. However, the drawback of the hybrid model lies in its high model scale, which 
signiﬁcantly reduces the detection eﬃciency. A more compact architecture is desired for small target detection. Moreover, it is 
desired to consider the tradeoﬀbetween the model accuracy and eﬃciency when detecting small targets. A large convolutional 
scale may reduce the computational cost, while the detection sensitivity is also degraded as well. A small scale signiﬁcantly can 
improve the detection accuracy, but this strategy may increase the computational complexity, thus slowing down the detection 
process. 
3. Lightweight YOLOv5 
Considering the tradeoﬀbetween the accuracy and eﬃciency in target detection, this study selects the lightweight YOLOv5 as the 
backbone [24] . YOLOv5 is the newest development of the YOLO network which realizes high detection accuracy with fast inference 
speed. Another advantage of YOLOv5 can be explained from its lightweight model scale which is almost 90% lighter than YOLOv5. 
Therefore, the YOLOv5 model is adaptive to this study, since the accuracy, eﬃciency and the lightweight scale are essential to 
the target detection in power engineering projects. This work aims to further improve the performance of YOLOv5 in small target 
detection. 
In general, the architecture of the YOLOv5 model is shown in Fig. 1 . On the whole, the architecture of the YOLOv5 network is 
comprised of four parts: input, backbone, neck and head modules: 
i Input: it engages in preprocessing the input images. For various applications, the input images are diﬀerent in the image size 
due to diﬀerent imaging properties. Hence, it is desired to make input images same in the image size. In this aspect, YOLOv5 
utilizes an adaptive image resizing which can lead to 37% faster of detection speed. Moreover, YOLOv5 introduces the anchor 
box predeﬁnition in the learning architecture, which makes the model more compact. At last, the Mosaic sample enhancement 
method is applied in this stage to improve the target detection accuracy, 
ii Backbone: it is characterized by the connection of the CSPDarknet and Focus networks. In CSPDarknet, CSP and Resnet are 
combined to improve the feature representability of the interest target. Moreover, the Focus network further concatenates the 
vectors from multiple channels into features which are of the size 304 × 304 × 32. 
iii Neck: it is built by connecting SPP and PANet modules. In function, the advantage of the SPP module is that it can achieve a 
ﬁxed-size mapping while ignoring the size of the input images, which can signiﬁcantly improve the eﬃciency of target detection 
process. PAN can realize multi-scale feature fusion by using the top-down and bottom-up pyramid architecture, which can present 
the appearance and the location information of the target, thus improving the representability of the target. 
iv Head: CIoU Loss function is introduced in the head module to measure the diﬀerence between the true value and the predicted 
values 
47 

P. Luo, X. Zhang and Y. Wan 
Cognitive Robotics 3 (2023) 45–53 
Fig. 1. YOLOv5 architecture [23] . 
Fig. 2. Feature extraction by convolutional calculations. 
3.2. Attention mechanism based YOLOv5 updating 
Feature extraction is commonly realized by layerwise convolutional calculations in the deep learning architecture. However, during 
successive convolutions the feature of small targets is increasingly degenerated, whereas the background features are prominent due 
to the large size and saliency of the false targets. A typical sample can be seen in Fig. 2 . From this result, we can ﬁnd that the feature 
of the small target is confused with that of the background after the convolutional calculations, whereas the background targets 
are falsely highlighted. It means that the small targets will be falsely removed in the target detection results. Aiming to solve this 
problem, the attention mechanism is introduced to make our proposed model emphasize on the small targets. In principle, the attention 
mechanism enhances the distinctiveness of the importance between diﬀerent feature channels, thus it can make the network model 
focus on the features which are important for speciﬁc tasks. Moreover, the backward updating process in the attention mechanism 
provides evidence for model pruning. 
In our study, the Convolutional Block Attention Module (CBAM) is exploited to realize the attention mechanism [25] . The reasons 
for selecting CBAM can be explained with three points. First, CBAM is an eﬀective attention module for feed-forward convolutional 
neural networks, which can simultaneously achieve the channel and spatial attention with a low computational cost. Second, CBAM 
is a general module, it can be easily integrated into any convolutional architectures seamlessly. Third, CBAM training is of negligible 
overheads and is end-to-end trainable along with the network backbone. In general, CBAM has two sequential sub-modules: channel 
and spatial attention modules. In our architecture, the layerwise feature map is adaptively reﬁned through CBAM at every convolu- 
tional block of deep networks. In speciﬁc, given an intermediate feature map, CBAM generates the attention maps with two separate 
dimensions, i.e., the channel and spatial dimensions. The feature reﬁnement results are generated by multiplying the attention maps 
with the intermediate feature map. In CBAM, The inter-spatial relationship of features is utilized to generate a spatial attention map 
which is complementary to the channel attention. The spatial attention map is processed with the global max pooling and global 
average pooling, which generates a new feature map. Later, the channel importance is adaptively determined by the multilayer 
feedforward perceptron (MLP) model in neural networks. The learned channel importance is normalized by the sigmoid activation 
function. Consequently, the 1D channel attention map can be summarized as: 
𝑀 𝑐 ( 𝐹 ) = 𝜎( 𝑀 𝐿𝑃 ( 𝐴𝑣𝑔𝑃 𝑜𝑜𝑙 ( 𝐹 ) ) + 𝑀 𝐿𝑃 ( 𝑀 𝑎𝑥𝑃 𝑜𝑜𝑙 ( 𝐹 ) ) ) 
= 𝜎
(
𝑊 1 
(
𝑊 0 
(
𝐹 𝑐 
𝑎𝑣𝑔 
))
+ 𝑊 1 
(𝑊 0 
(𝐹 𝑐 
𝑚𝑎𝑥 
)))
, 
(1) 
where 𝜎denotes the sigmoid function. It is noticed that the MLP weights, W 0 and W 1 are shared for the inputs and ReLU activation 
function is followed by W 0 . Given an input image, the channel and spatial attention modules jointly generate comprehensive attention, 
which focuses on the category and location of targets. In practice, these two modules can be connected with a parallel or sequential 
structure, and the sequential structure arrangement provides better performance. Two feature maps are concatenated and convolved 
48 

P. Luo, X. Zhang and Y. Wan 
Cognitive Robotics 3 (2023) 45–53 
Fig. 3. Depthwise separable convolutions, (a) classic convolutional, (b) depth convolution, (c) pointwise convolution. 
by convolution calculations, generating the 2D spatial attention map which can be calculated as: 
𝑀 𝑠 ( 𝐹 ) = 𝜎( 𝑓 (7×7) ([ 𝐴𝑣𝑔𝑃 𝑜𝑜𝑙( 𝐹 ); 𝑀𝑎𝑥𝑃 𝑜𝑜𝑙( 𝐹 )])) 
= 𝜎( 𝑓 (7×7) ([ 𝐹 𝑠 
𝑎𝑣𝑔 ; 𝐹 𝑠 
max ])) , 
(2) 
where 𝑓 (7×7) is a convolution operation with the ﬁlter size of 7 × 7. 
3.3. Model lightening 
A lightweight model is required for small target detection in power engineering projects since the applications are mostly operated 
on embedded devices. In this study, two strategies are employed to lighten the network architecture, the ﬁrst is MobileNet and 
Knowledge Distillation is the second [ 26 , 27 ]. The core of the MobileNet lies in that it replaces Separable Convolutions with Depthwise 
Separable convolutions which is realized by jointly using channel-wise convolution and the linear integration [27] . In speciﬁc, 
depthwise separable convolution can use the 3 × 3 convolution kernel with only one layer thickness, which slides on the input tensor 
by layers. After each convolutional calculation, an output channel can be generated. After each convolution, the layer thickness is 
adjusted by 1 × 1 pointwise convolution. 
Fig. 3 (a) presents the standard convolutional calculations. Given a feature map 𝐹 , kernel 𝐾and the output 𝐺, the complexity of 
the standard convolutional calculations can be presented as: 
𝑆 = 𝐷 𝐾 × 𝐷 𝐾 × 𝑀 × 𝑁 × 𝐷 𝐹 × 𝐷 𝐹 
(3) 
Fig. 3 (b) and (c) present the depth convolution and pointwise convolution. The output features of the depth convolution and 
the pointwise convolution are 𝐺 ′ and 𝐺 ′′. As the result, the total complexity of the depth convolution and pointwise convolution 
calculations can be presented as: 
⌢ 
𝑆 = (𝐷 𝐺 ′ × 𝐷 𝐺 ′ × 𝑀 + 𝑀 × 𝑁 ) × 𝐷 𝐺 ′′ × 𝐷 𝐺 ′′
(4) 
Comparing 
⌢ 
𝑆 and 𝑆, we can ﬁnd that the Depthwise Separable convolutions leads to 75% reduce in term of calculation complexity, 
compared to the original convolutional calculation. 
The second strategy for model lightening is conducted by Knowledge Distillation. As a classic type of model compression and 
acceleration strategy, knowledge distillation is to learn a small-scale model (student model) from a large model (teacher model). Recall 
that the MobileNet is used to lighten the teacher model, thus the Knowledge Distillation can further reduce the model parameters. 
In knowledge distillation, knowledge property, distillation strategies and the teacher-student architecture are the key for student 
learning. Inspired by the idea of the multi-stage distillation, the Hinton KD and Feature Kd architectures are combined to build an 
updated network for Knowledge Distillation. In theory, this architecture is eﬃcient to process response-based knowledge which refers 
to the neural output of the teacher model. The main idea is to directly simulate the ﬁnal decision of the teacher model. This type of 
response-based knowledge distillation is simple and feasible for model compression. The pretrained teacher network generates features 
which provide guidance to the student network. The student network is optimized by minimizing the feature variance obtained by 
the student and teacher networks. In the intermediary layers, we update the Feature KD by the ResNet, which is called Res-Feature 
KD. The architecture of the Res-Feature KD is shown in Fig. 4 . The core idea of the Res-Feature KD model is that the current layer of 
the student network is updated by the Knowledge Distillation from the output of both the current and next layers. The downsampling 
operation is taken advantage in the current layer of the teacher network which can improve the feature representability of the student 
network. Generally, the KD loss for feature-based Knowledge Distillation can be formulated as: 
𝐿 (𝑓 𝑡 ( 𝑥 ) , 𝑓 𝑠 ( 𝑥 ) ) = 𝓁 𝐹 
(𝜙𝑡 ( 𝑓 𝑡 ( 𝑥 )) , 𝜙𝑠 ( 𝑓 𝑠 ( 𝑥 )) ), 
(5) 
where 𝑓 𝑡 ( 𝑥 ) and 𝑓 𝑠 ( 𝑥 ) are the output of the intermediate layers of teacher and student models respectively, 𝜙𝑡 ( 𝑓 𝑡 ( 𝑥 )) and 𝜙𝑠 ( 𝑓 𝑠 ( 𝑥 )) are 
the transformation functions which are used to align the feature maps generated by the teacher and student models. 𝓁 𝐹 ( ⋅) is designed 
to measure the feature diﬀerence between the teacher and student models. The optimized result of KD loss can achieve a feature 
assignment of feature maps between the teacher and student models, such that we can replace the large-scale student model with the 
small student model. 
49 

P. Luo, X. Zhang and Y. Wan 
Cognitive Robotics 3 (2023) 45–53 
Fig. 4. Res-Feature KD. 
Fig. 5. Synthetic maps. 
Table 1 
Model parameter. 
Initial learning rate 
0.01 
Final learning rate 
0.003 
Training iteration 
300 
Attenuation coeﬃcient 
0.0004 
Batch-size 
8 
Momentum factor 
0.943 
4. Experimental results and analysis 
4.1. Dataset 
Aiming to evaluate the performance of our proposed model, we build a novel dataset to stimulate the extreme conditions of power 
engineering. In addition to the data collected in practice, we increase samples by synthetic maps. In speciﬁc, the Grabcut model and 
image fusion method are employed to generate synthetic maps. The small targets segmented from common power engineering scenes 
are fused with complicated backgrounds. A typical sample is shown in Fig. 5 . As the result, our data set includes 1300 original images. 
Aiming to enhance the dataset, we produce variants of the dataset images horizontally and vertically. Also, we add noises to original 
images and generate more samples by the mix-up method which mixes samples by image interpolation. As the result, our dataset 
includes 2300 samples. The Labelimg tool is taken advantage to label the data samples, which records the location, width and length 
of the targets. 2000 labeled samples are included in the training subset and the 300 samples are used to evaluate the performance of 
diﬀerent methods. 
4.2. Experimental comparison 
The SGD algorithm is used to optimize deep learning models, and the CIOU Loss is used to measure the training cost. Details of 
the parameter setting are shown in Table 1 . The parameters (Params), MAP(Mean Average Precision), FLOPs and the testing time 
(Time) are recorded to evaluate the performance of diﬀerent methods. 
We ﬁrst evaluate the contribution of the CBAM-based attention mechanism and then the performance of the lightweight architec- 
ture is demonstrated. Fig. 6 presents samples of the small target detection in challenging conditions by the YOLOv5 (ﬁrst row) and 
SP-CBAM-YOLOv5s (second row). From the results, we can ﬁnd that SP-CBAM-YOLOv5s is more sensitive to the small targets, and all 
targets are correctly detected, whereas some small targets are missed in the results obtained by YOLOv5. Hence, we can conclude that 
SP-CBAM-YOLOv5s outperforms YOLOv5 in term of detection accuracy. The quantitative results are shown in Table 2 which com- 
pares the results obtained by the YOLOv3, YOLOv4, YOLOv5s and SP-CBAM-YOLOv5s. From these results, our SP-CBAM-YOLOv5s 
wins the best in MAP. However, we can ﬁnd that the model scale of the SP-CBAM-YOLOv5s model is larger than that of the YOLOv5s 
model which results in the increase of the running time. The main reason lies in the added CBAM module. 
Our lightweight model is evaluated in terms of detection accuracy and eﬃciency. Here, CBAM-YOLOv5s is employed to build the 
teacher network. The student network is constructed by replacing the classic convolutional calculation with the depth convolutional 
50 

P. Luo, X. Zhang and Y. Wan 
Cognitive Robotics 3 (2023) 45–53 
Fig. 6. Small target detection by YOLOv5(ﬁrst row) and SP-CBAM-YOLOv5(second row). 
Table 2 
Performance comparison. 
Params 
FLOPs 
MAP(%) 
Time(ms) 
SP-CBAM-YOLOv5s 
8.57 
7.21 
90.6 
9.5 
YOLOv5s 
7.27 
6.38 
87.3 
6.8 
YOLOv4 
56.3 
47.8 
89.0 
78 
YOLOv3 
48.6 
32.9 
83.4 
62 
Fig. 7. Small target detection by the student network (ﬁrst row) and teacher network (second row). 
calculation, which is called SP-CBAM-YOLOv5. The Knowledge Distillation method presented in Section 3.2 is used to train the 
student model. The qualitative results obtained by the teacher network (second row) and student network (ﬁrst row) are shown in 
Fig. 7 . From these results, the performance of the student network is generally equivalent to that given by the teacher network. The 
quantitative evaluation result demonstrates that the MAP obtained by SP-CBAM-YOLOv5 reaches 91%. By contrast, the SP-CBAM- 
YOLOv5 is signiﬁcantly lighter than the CBAM-YOLOv5s. In speciﬁc, the time cost of the SP-YOLOv5s for handling a single frame is 
5.3ms which is almost half of that of the CBAM-YOLOv5s. 
4.3. Ablation study 
We leverage four metrics (Params, MAP Weight(M) and Time) to quantitatively demonstrate the eﬀect of diﬀerent modules in our 
SP-CBAM-YOLOv5, that are CBAM-YOLOv5, SP-YOLOv5 and YOLOv5s. To investigate the eﬀect of these three modules, we replace 
diﬀerent modules in each ablation test. In addition to our dataset, the common FBMS dataset is used for performance evaluations, 
which includes 59 sequences, 30 of which were used as a validation set [28] . Table 3 records the average score on our dataset and 
FBMS dataset. It comprehensively compares diﬀerent architectures, from which four points can be drawn. First, we can see an increase 
of detection accuracy when the FBMS dataset is added for experimental evaluations. The reason is that FBMS dataset includes larger- 
size targets in contrast to our dataset. Detecting targets from FBMS is easier than our dataset. Second, the cooperation of SP, CBAM 
and YOLOv5 can improve the small target detection results, since there is an obvious increase in the term of MAP in contrast to the 
ablation models. It is acknowledged that the hybrid model may increase the parameter number of the target detection model, this 
51 

P. Luo, X. Zhang and Y. Wan 
Cognitive Robotics 3 (2023) 45–53 
Table 3 
Performance comparison. 
Params 
MAP(%) 
Weight(M) 
Time(ms) 
SP-CBAM-YOLOv5s 
8.57 
91.5 
15.8 
9.5 
CBAM-YOLOv5s 
12.73 
92.0 
29.3 
20.7 
SP-YOLOv5s 
2.03 
87.2 
4.3 
6.1 
YOLOv5s 
7.43 
88.5 
14.1 
6.8 
eﬀort is desired due to the notable performance improvement by using our hybrid model. Third, removing CBAM causes signiﬁcant 
performance degeneration. Although CBAM has a relatively large number of model parameters, this module is desired for small target 
detection due to its contribution to improving performance. Fourth, Knowledge Distillation contributes much to lightening the model 
weight, since we can see an obvious weight reduction when the teacher model is replaced with our student models. Combining SP, 
CBAM and YOLOv5 can achieve the best performance with a slight increase of the parameter number, which can nicely cater for the 
applications of power engineering. 
5. Conclusions 
This paper proposes a novel SP-CBAM-YOLOv5 to achieve small target detection in wild power engineering scenes. The main 
novelties of this model lie in the introduction of the attention mechanism and knowledge distillation strategy into the YOLOv5 
model. These two novelties help our model achieves an optimal tradeoﬀbetween eﬀect and eﬃciency, which is crucial for real- 
world applications. Diﬀerent from common scenes, this paper focuses on the special scenes of power engineering and intends to 
detect small-size targets from complicated backgrounds. Aiming to demonstrate the performance of our proposed SP-CBAM-YOLOv5, 
a novel dataset is built by combining real-world images and synthetic samples. Experimental results demonstrate that our method 
provides competitive performance in contrast to the state-of-the-art methods. 
Many studies have demonstrated that employing the attention mechanism can signiﬁcantly improve deep learning performance. 
Like the feature fusion method, the key of the attention mechanism is to determine the importance of diﬀerent features. Most current 
strategies achieve the attention mechanism by data-driven training. However, none of the existing studies has discussed the conver- 
gency of the attention model while the theoretical analysis is insuﬃcient in this ﬁeld. In future work, our eﬀorts will focus on the 
development of the attention mechanism and try to improve the interpretability of the attention-based deep learning architectures. 
Declaration of Competing Interest 
The authors declare that they have no known competing ﬁnancial interests or personal relationships that could have appeared to 
inﬂuence the work reported in this paper. 
References 
[1] J. Zhao, et al., Power system dynamic state estimation: motivations, deﬁnitions, methodologies, and future work, IEEE Trans. Power Syst. 34 (4) (2019) 3188–
3198 July, doi: 10.1109/TPWRS.2019.2894769 . 
[2] Z. Bayasgalan, A. Sukhbaatar, U. Tudevdagva, W. Hardt, Top inspection and monitoring of HV power line towers damage by UAV, in: 2021 International 
Conference on Technology and Policy in Energy and Electric Power (ICT-PEP), 2021, pp. 248–252, doi: 10.1109/ICT-PEP53949.2021.9601091 . 
[3] X. Chen, N. Su, Y. Huang, J. Guan, False-alarm-controllable radar detection for marine target based on multi features fusion via CNNs, IEEE Sens. J. 21 (7) (2021) 
9099–9111 1 April1, doi: 10.1109/JSEN.2021.3054744 . 
[4] M.B. Ullah, CPU Based YOLO: a real time object detection algorithm, in: 2020 IEEE Region 10 Symposium (TENSYMP), 2020, pp. 552–555, doi: 10.1109/TEN- 
SYMP50017.2020.9230778 . 
[5] D. Wang, K. Shang, H. Wu and C. Wang, "Decoupled R-CNN: sensitivity-speciﬁc detector for higher accurate localization," IEEE Trans. Circuits Syst. Video 
Technol., doi: 10.1109/TCSVT.2022.3167114 . 
[6] F. Meslet-Millet, E. Chaput, S. Mouysset, SPPNet: an approach for real-time encrypted traﬃc classiﬁcation using deep learning, in: 2021 IEEE Global Communi- 
cations Conference, 2021, pp. 1–6, doi: 10.1109/GLOBECOM46510.2021.9686037 . 
[7] Y. Li, S. Zhang, W.-Q. Wang, A lightweight faster R-CNN for ship detection in SAR images, IEEE Geosci. Remote Sens. Lett. 19 (2022) 1–5 Art no. 4006105, 
doi: 10.1109/LGRS.2020.3038901 . 
[8] X. Zhou, K. Yang, R. Duan, Deep learning based on striation images for underwater and surface target classiﬁcation, IEEE Signal Process Lett. 26 (9) (2019) 
1378–1382 Sept., doi: 10.1109/LSP.2019.2919102 . 
[9] T. Kong, F. Sun, A. Yao, H. Liu, M. Lu, Y. Chen, RON: reverse connection with objectness prior networks for object detection, in: 2017 IEEE Conference on 
Computer Vision and Pattern Recognition (CVPR), 2017, pp. 5244–5252, doi: 10.1109/CVPR.2017.557 . 
[10] R. Araki, T. Onishi, T. Hirakawa, T. Yamashita, H. Fujiyoshi, MT-DSSD: deconvolutional single shot detector using multi task learning for object de- 
tection, Segmentation, and Grasping Detection, in: 2020 IEEE International Conference on Robotics and Automation (ICRA), 2020, pp. 10487–10493, 
doi: 10.1109/ICRA40945.2020.9197251 . 
[11] Y. Li, Y. Chen, N. Wang, Z.-X. Zhang, Scale-aware trident networks for object detection, in: 2019 IEEE/CVF International Conference on Computer Vision (ICCV), 
2019, pp. 6053–6062, doi: 10.1109/ICCV.2019.00615 . 
[12] A. Abdelmutalab, C. Wang, Pedestrian detection using MB-CSP model and boosted identity aware non-maximum suppression, IEEE Transactions on Intelligent 
Transportation Systems, 2022, doi: 10.1109/TITS.2022.3196854 . 
[13] J. Li et al., "Monocular 3D Object detection based on depth guided local convolution for smart payment in D2D systems," IEEE Internet Things J., doi: 
10.1109/JIOT.2021.3128440 . 
[14] W.-Y. Hsu, W.-Y. Lin, Ratio-and-scale-aware YOLO for pedestrian detection, IEEE Trans. Image Process. 30 (2021) 934–947, doi: 10.1109/TIP.2020.3039574 . 
[15] X. Lu, J. Ji, Z. Xing, Q. Miao, Attention and feature fusion SSD for remote sensing object detection, IEEE Trans. Instrum. Meas. 70 (2021) 1–9 Art no. 5501309, 
doi: 10.1109/TIM.2021.3052575 . 
52 

P. Luo, X. Zhang and Y. Wan 
Cognitive Robotics 3 (2023) 45–53 
[16] D. Sadykova, D. Pernebayeva, M. Bagheri, A. James, IN-YOLO: real-time detection of outdoor high voltage insulators using UAV imaging, IEEE Trans. Power 
Delivery 35 (3) (2020) 1599–1601 June, doi: 10.1109/TPWRD.2019.2944741 . 
[17] D. Tian, et al., SA-YOLOv3: an eﬃcient and accurate Object detector using self-attention mechanism for autonomous driving, IEEE Trans. Intell. Transp. Syst. 23 
(5) (2022) 4099–4110 May, doi: 10.1109/TITS.2020.3041278 . 
[18] P. Zhang, Y. Zhong, X. Li, SlimYOLOv3: narrower, faster and better for real-time UAV Applications, in: 2019 IEEE/CVF International Conference on Computer 
Vision Workshop (ICCVW), 2019, pp. 37–45, doi: 10.1109/ICCVW.2019.00011 . 
[19] K. Ye, et al., Research on small target detection algorithm based on improved yolov3, in: 2020 5th International Conference on Mechanical, Control and Computer 
Engineering (ICMCCE), 2020, pp. 1467–1470, doi: 10.1109/ICMCCE51767.2020.00321 . 
[20] Z.-Z. Wang, K. Xie, X.-Y. Zhang, H.-Q. Chen, C. Wen, J.-B. He, Small-object detection based on YOLO and dense block via image super-resolution, IEEE Access 9 
(2021) 56416–56429, doi: 10.1109/ACCESS.2021.3072211 . 
[21] C. Yan, K. Wu, C. Zhang, A new anchor-labeling method for oriented text detection using dense detection framework, IEEE Signal Process Lett. 25 (9) (2018) 
1295–1299 Sept., doi: 10.1109/LSP.2018.2852954 . 
[22] M. Mostofa, S.N. Ferdous, B.S. Riggan, N.M. Nasrabadi, Joint-SRVDNet: joint super resolution and vehicle detection network, IEEE Access 8 (2020) 82306–82319, 
doi: 10.1109/ACCESS.2020.2990870 . 
[23] D. Jiang. A complete explanation of yolov3 & yolov4 & yolov5 & yolox core basic knowledge of Yolo series, Zhihu, https://zhuanlan.zhihu.com/p/143747206 , 
(2020c, May 27). 
[24] Y. Jiang, COTS recognition and detection based on Improved YOLO v5 model, in: 2022 7th International Conference on Intelligent Computing and Signal 
Processing (ICSP), 2022, pp. 830–833, doi: 10.1109/ICSP54964.2022.9778430 . 
[25] R. Li, X. Wang, J. Wang, Y. Song, L. Lei, SAR Target recognition based on eﬃcient fully convolutional attention block CNN, IEEE Geosci. Remote Sens. Lett. 19 
(2022) 1–5 Art no. 4005905, doi: 10.1109/LGRS.2020.3037256 . 
[26] J. Tang, X. Peng, X. Chen, B. Luo, An improved mobilenet-SSD approach for face detection, in: 2021 40th Chinese Control Conference (CCC), 2021, pp. 8072–8076, 
doi: 10.23919/CCC52363.2021.9549245 . 
[27] Y. Yang, et al., Adaptive knowledge distillation for Lightweight remote sensing object detectors optimizing, IEEE Trans. Geosci. Remote Sens. 60 (2022) 1–15 
Art no. 5623715, doi: 10.1109/TGRS.2022.3175213 . 
[28] T. Brox, J. Malik, Object segmentation by long term analysis of point trajectories, in: European Conference on Computer Vision, 2010, pp. 282–295 . 
53 

