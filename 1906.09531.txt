Bias Correction of Learned Generative Models using
Likelihood-Free Importance Weighting
Aditya Grover1, Jiaming Song1, Alekh Agarwal2, Kenneth Tran2,
Ashish Kapoor2, Eric Horvitz2, Stefano Ermon1
1Stanford University, 2Microsoft Research, Redmond
Abstract
A learned generative model often produces biased statistics relative to the under-
lying data distribution. A standard technique to correct this bias is importance
sampling, where samples from the model are weighted by the likelihood ratio
under model and true distributions. When the likelihood ratio is unknown, it can
be estimated by training a probabilistic classiﬁer to distinguish samples from the
two distributions. We employ this likelihood-free importance weighting method
to correct for the bias in generative models. We ﬁnd that this technique consis-
tently improves standard goodness-of-ﬁt metrics for evaluating the sample quality
of state-of-the-art deep generative models, suggesting reduced bias. Finally, we
demonstrate its utility on representative applications in a) data augmentation for
classiﬁcation using generative adversarial networks, and b) model-based policy
evaluation using off-policy data.
1
Introduction
Learning generative models of complex environments from high-dimensional observations is a long-
standing challenge in machine learning. Once learned, these models are used to draw inferences and
to plan future actions. For example, in data augmentation, samples from a learned model are used to
enrich a dataset for supervised learning [1]. In model-based off-policy policy evaluation (henceforth
MBOPE), a learned dynamics model is used to simulate and evaluate a target policy without real-world
deployment [2], which is especially valuable for risk-sensitive applications [3]. In spite of the recent
successes of deep generative models, existing theoretical results show that learning distributions in an
unbiased manner is either impossible or has prohibitive sample complexity [4, 5]. Consequently, the
models used in practice are inherently biased,1 and can lead to misleading downstream inferences.
In order to address this issue, we start from the observation that many typical uses of generative
models involve computing expectations under the model. For instance, in MBOPE, we seek to ﬁnd
the expected return of a policy under a trajectory distribution deﬁned by this policy and a learned
dynamics model. A classical recipe for correcting the bias in expectations, when samples from
a different distribution than the ground truth are available, is to importance weight the samples
according to the likelihood ratio [6]. If the importance weights were exact, the resulting estimates are
unbiased. But in practice, the likelihood ratio is unknown and needs to be estimated since the true
data distribution is unknown and even the model likelihood is intractable or ill-deﬁned for many deep
generative models, e.g., variational autoencoders [7] and generative adversarial networks [8].
Our proposed solution to estimate the importance weights is to train a calibrated, probabilistic
classiﬁer to distinguish samples from the data distribution and the generative model. As shown in
prior work, the output of such classiﬁers can be used to extract density ratios [9]. Appealingly, this
estimation procedure is likelihood-free since it only requires samples from the two distributions.
1We call a generative model biased if it produces biased statistics relative to the true data distribution.
33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.
arXiv:1906.09531v2  [stat.ML]  3 Nov 2019

Together, the generative model and the importance weighting function (speciﬁed via a binary classiﬁer)
induce a new unnormalized distribution. While exact density estimation and sampling from this
induced distribution is intractable, we can derive a particle based approximation which permits
efﬁcient sampling via resampling based methods. We derive conditions on the quality of the weighting
function such that the induced distribution provably improves the ﬁt to the the data distribution.
Empirically, we evaluate our bias reduction framework on three main sets of experiments. First, we
consider goodness-of-ﬁt metrics for evaluating sample quality metrics of a likelihood-based and a
likelihood-free state-of-the-art (SOTA) model on the CIFAR-10 dataset. All these metrics are deﬁned
as Monte Carlo estimates from the generated samples. By importance weighting samples, we observe
a bias reduction of 23.35% and 13.48% averaged across commonly used sample quality metrics on
PixelCNN++ [10] and SNGAN [11] models respectively.
Next, we demonstrate the utility of our approach on the task of data augmentation for multi-class
classiﬁcation on the Omniglot dataset [12]. We show that, while naively extending the model with
samples from a data augmentation, a generative adversarial network [1] is not very effective for multi-
class classiﬁcation, we can improve classiﬁcation accuracy from 66.03% to 68.18% by importance
weighting the contributions of each augmented data point.
Finally, we demonstrate bias reduction for MBOPE [13]. A typical MBOPE approach is to ﬁrst
estimate a generative model of the dynamics using off-policy data and then evaluate the policy via
Monte Carlo [2, 14]. Again, we observe that correcting the bias of the estimated dynamics model via
importance weighting reduces RMSE for MBOPE by 50.25% on 3 MuJoCo environments [15].
2
Preliminaries
Notation. Unless explicitly stated otherwise, we assume that probability distributions admit abso-
lutely continuous densities on a suitable reference measure. We use uppercase notation X, Y, Z to
denote random variables and lowercase notation x, y, z to denote speciﬁc values in the corresponding
sample spaces X, Y, Z. We use boldface for multivariate random variables and their vector values.
Background. Consider a ﬁnite dataset Dtrain of instances x drawn i.i.d. from a ﬁxed (unknown)
distribution pdata. Given Dtrain, the goal of generative modeling is to learn a distribution pθ to
approximate pdata. Here, θ denotes the model parameters, e.g. weights in a neural network for deep
generative models. The parameters can be learned via maximum likelihood estimation (MLE) as in
the case of autoregressive models [16], normalizing ﬂows [17], and variational autoencoders [7, 18],
or via adversarial training e.g., using generative adversarial networks [8, 19] and variants.
Monte Carlo Evaluation We are interested in use cases where the goal is to evaluate or optimize
expectations of functions under some distribution p (either equal or close to the data distribution
pdata). Assuming access to samples from p as well some generative model pθ, one extreme is to
evaluate the sample average using the samples from p alone. However, this ignores the availability of
pθ, through which we have a virtually unlimited access of generated samples ignoring computational
constraints and hence, could improve the accuracy of our estimates when pθ is close to p. We begin
by presenting a direct motivating use case of data augmentation using generative models for training
classiﬁers which generalize better.
Example Use Case: Sufﬁcient labeled training data for learning classiﬁcation and regression system
is often expensive to obtain or susceptible to noise. Data augmentation seeks to overcome this
shortcoming by artiﬁcially injecting new datapoints into the training set. These new datapoints are
derived from an existing labeled dataset, either by manual transformations (e.g., rotations, ﬂips for
images), or alternatively, learned via a generative model [1, 20].
Consider a supervised learning task over a labeled dataset Dcl. The dataset consists of feature and
label pairs (x, y), each of which is assumed to be sampled independently from a data distribution
pdata(x, y) deﬁned over X × Y. Further, let Y ⊆Rk. In order to learn a classiﬁer fψ : X →Rk
with parameters ψ, we minimize the expectation of a loss ℓ: Y × Rk →R over the dataset Dcl:
Epdata(x,y)[ℓ(y, fψ(x))] ≈
1
|Dcl|
X
(x,y)∼Dcl
ℓ(y, fψ(x)).
(1)
2

E.g., ℓcould be the cross-entropy loss. A generative model for the task of data augmentation learns a
joint distribution pθ(x, y). Several algorithmic variants exist for learning the model’s joint distribution
and we defer the speciﬁcs to the experiments section. Once the generative model is learned, it can be
used to optimize the expected classiﬁcation loss in Eq. (1) under a mixture distribution of empirical
data distributions and generative model distributions given as:
pmix(x, y) = mpdata(x, y) + (1 −m)pθ(x, y)
(2)
for a suitable choice of the mixture weights m ∈[0, 1]. Notice that, while the eventual task here
is optimization, reliably evaluating the expected loss of a candidate parameter ψ is an important
ingredient. We focus on this basic question ﬁrst in advance of leveraging the solution for data
augmentation. Further, even if evaluating the expectation once is easy, optimization requires us to do
repeated evaluation (for different values of ψ) which is signiﬁcantly more challenging. Also observe
that the distribution p under which we seek expectations is same as pdata here, and we rely on the
generalization of pθ to generate transformations of an instance in the dataset which are not explicitly
present, but plausibly observed in other, similar instances [21].
3
Likelihood-Free Importance Weighting
Whenever the distribution p, under which we seek expectations, differs from pθ, model-based
estimates exhibit bias. In this section, we start out by formalizing bias for Monte Carlo expectations
and subsequently propose a bias reduction strategy based on likelihood-free importance weighting
(LFIW). We are interested in evaluating expectations of a class of functions of interest f ∈F w.r.t.
the distribution p. For any given f : X →R, we have Ex∼p[f(x)] =
R
p(x)f(x)dx.
Given access to samples from a generative model pθ, if we knew the densities for both p and pθ,
then a classical scheme to evaluate expectations under p using samples from pθ is to use importance
sampling [6]. We reweight each sample from pθ according to its likelihood ratio under p and pθ and
compute a weighted average of the function f over these samples.
Ex∼p[f(x)] = Ex∼pθ
 p(x)
pθ(x)f(x)

≈1
T
T
X
i=1
w(xi)f(xi)
(3)
where w(xi) := p(xi)/pθ(xi) is the importance weight for xi ∼pθ. The validity of this procedure
is subject to the use of a proposal pθ such that for all x ∈X where pθ(x) = 0, we also have
f(x)p(x) = 0.2
To apply this technique to reduce the bias of a generative sampler pθ w.r.t. p, we require knowledge
of the importance weights w(x) for any x ∼pθ. However, we typically only have a sampling access
to p via ﬁnite datasets. For instance, in the data augmentation example above, where p = pdata, the
unknown distribution used to learn pθ. Hence we need a scheme to learn the weights w(x), using
samples from p and pθ, which is the problem we tackle next.In order to do this, we consider a binary
classiﬁcation problem over X × Y where Y = {0, 1} and the joint distribution is denoted as q(x, y).
Let γ = q(y=0)
q(y=1) > 0 denote any ﬁxed odds ratio. To specify the joint q(x, y), we additionally need
the conditional q(x|y) which we deﬁne as follows:
q(x|y) =
pθ(x) if y = 0
p(x) otherwise.
(4)
Since we only assume sample access to p and pθ(x), our strategy would be to estimate the conditional
above via learning a probabilistic binary classiﬁer. To train the classiﬁer, we only require datasets
of samples from pθ(x) and p(x) and estimate γ to be the ratio of the size of two datasets. Let
cφ : X →[0, 1] denote the probability assigned by the classiﬁer with parameters φ to a sample x
belonging to the positive class y = 1. As shown in prior work [9, 22], if cφ is Bayes optimal, then
the importance weights can be obtained via this classiﬁer as:
wφ(x) = p(x)
pθ(x) = γ
cφ(x)
1 −cφ(x).
(5)
2A stronger sufﬁcient, but not necessary condition that is independent of f, states that the proposal pθ is
valid if it has a support larger than p, i.e., for all x ∈X, pθ(x) = 0 implies p(x) = 0.
3

(a) Setup
(b) n = 50
(c) n = 100
(d) n = 1000
Figure 1: Importance Weight Estimation using Probabilistic Classiﬁers. (a) A univariate Gaussian
(blue) is ﬁt to samples from a mixture of two Gaussians (red). (b-d) Estimated class probabilities
(with 95% conﬁdence intervals based on 1000 bootstraps) for varying number of points n, where n is
the number of points used for training the generative model and multilayer perceptron.
In practice, we do not have access to a Bayes optimal classiﬁer and hence, the estimated importance
weights will not be exact. Consequently, we can hope to reduce the bias as opposed to eliminating it
entirely. Hence, our default LFIW estimator is given as:
Ex∼p[f(x)] ≈1
T
T
X
i=1
ˆwφ(xi)f(xi)
(6)
where ˆwφ(xi) = γ
cφ(xi)
1−cφ(xi) is the importance weight for xi ∼pθ estimated via cφ(x).
Practical Considerations. Besides imperfections in the classiﬁer, the quality of a generative model
also dictates the efﬁcacy of importance weighting. For example, images generated by deep generative
models often possess distinct artifacts which can be exploited by the classiﬁer to give highly-conﬁdent
predictions [23, 24]. This could lead to very small importance weights for some generated images,
and consequently greater relative variance in the importance weights across the Monte Carlo batch.
Below, we present some practical variants of LFIW estimator to offset this challenge.
1. Self-normalization: The self-normalized LFIW estimator for Monte Carlo evaluation normalizes
the importance weights across a sampled batch:
Ex∼p[f(x)] ≈
T
X
i=1
ˆwφ(xi)
PT
j=1 ˆwφ(xj)
f(xi) where xi ∼pθ.
(7)
2. Flattening: The ﬂattened LFIW estimator interpolates between the uniform importance weights
and the default LFIW weights via a power scaling parameter α ≥0:
Ex∼p[f(x)] ≈1
T
T
X
i=1
ˆwφ(xi)αf(xi) where xi ∼pθ.
(8)
For α = 0, there is no bias correction, and α = 1 returns the default estimator in Eq. (6). For
intermediate values of α, we can trade-off bias reduction with any undesirable variance introduced.
3. Clipping: The clipped LFIW estimator speciﬁes a lower bound β ≥0 on the importance weights:
Ex∼p[f(x)] ≈1
T
T
X
i=1
max( ˆwφ(xi), β)f(xi) where xi ∼pθ.
(9)
When β = 0, we recover the default LFIW estimator in Eq. (6). Finally, we note that these estimators
are not exclusive and can be combined e.g., ﬂattened or clipped weights can be normalized.
Conﬁdence intervals. Since we have real and generated data coming from a ﬁnite dataset and
parametric model respectively, we propose a combination of empirical and parametric bootstraps to
derive conﬁdence intervals around the estimated importance weights. See Appendix A for details.
Synthetic experiment. We visually illustrate our importance weighting approach in a toy experiment
(Figure 1a). We are given a ﬁnite set of samples drawn from a mixture of two Gaussians (red). The
model family is a unimodal Gaussian, illustrating mismatch due to a parametric model. The mean
4

Algorithm 1 SIR for the Importance Resampled Generative Model pθ,φ
Input: Generative Model pθ, Importance Weight Estimator ˆwφ, budget T
1: Sample x1, x2, . . . , xT independently from pθ
2: Estimate importance weights ˆw(x1), ˆw(x2), . . . , ˆw(xT )
3: Compute ˆZ ←PT
t=1 ˆw(xt)
4: Sample j ∼Categorical

ˆ
w(x1)
ˆ
Z
, ˆ
w(x2)
ˆ
Z
, . . . , ˆ
w(xT )
ˆ
Z

5: return xj
and variance of the model are estimated by the empirical means and variances of the observed data.
Using estimated model parameters, we then draw samples from the model (blue).
In Figure 1b, we show the probability assigned by a binary classiﬁer to a point to be from true data
distribution. Here, the classiﬁer is a single hidden-layer multi-layer perceptron. The classiﬁer is not
Bayes optimal, which can be seen by the gaps between the optimal probabilities curve (black) and the
estimated class probability curve (green). However, as we increase the number of real and generated
examples n in Figures 1c-d, the classiﬁer approaches optimality. Furthermore, even its uncertainty
shrinks with increasing data, as expected. In summary, this experiment demonstrates how a binary
classiﬁer can mitigate this bias due to a mismatched generative model.
4
Importance Resampled Generative Modeling
In the previous section, we described a procedure to augment any base generative model pθ with
an importance weighting estimator ˆwφ for debiased Monte Carlo evaluation. Here, we will use this
augmentation to induce an importance resampled generative model with density pθ,φ given as:
pθ,φ(x) ∝pθ(x) ˆwφ(x)
(10)
where the partition function is expressed as Zθ,φ =
R
pθ(x) ˆwφ(x)dx = Epθ[ ˆwφ(x)].
Density Estimation. Exact density estimation requires a handle on the density of the base model pθ
(typically intractable for models such as VAEs and GANs) and estimates of the partition function.
Exactly computing the partition function is intractable. If pθ permits fast sampling and importance
weights are estimated via LFIW (requiring only a forward pass through the classiﬁer network),
we can obtain unbiased estimates via a Monte Carlo average, i.e., Zθ,φ ≈1
T
PT
i=1 ˆwφ(xi) where
xi ∼pθ. To reduce the variance, a potentially large number of samples are required. Since samples
are obtained independently, the terms in the Monte Carlo average can be evaluated in parallel.
Sampling-Importance-Resampling. While exact sampling from pθ,φ is intractable, we can instead
perform sample from a particle-based approximation to pθ,φ via sampling-importance-resampling [25,
26] (SIR). We deﬁne the SIR approximation to pθ,φ via the following density:
pSIR
θ,φ (x; T) := Ex2,x3,...,xT ∼pθ
"
ˆwφ(x)
ˆwφ(x) + PT
i=2 ˆwφ(xi)
pθ(x)
#
(11)
where T > 0 denotes the number of independent samples (or “particles"). For any ﬁnite T, sampling
from pSIR
θ,φ is tractable, as summarized in Algorithm 1. Moreover, any expectation w.r.t. the SIR
approximation to the induced distribution can be evaluated in closed-form using the self-normalized
LFIW estimator (Eq. 7). In the limit of T →∞, we recover the induced distribution pθ,φ:
lim
T →∞pSIR
θ,φ (x; T) = pθ,φ(x)
∀x
(12)
Next, we analyze conditions under which the resampled density pθ,φ provably improves the model ﬁt
to pdata. In order to do so, we further assume that pdata is absolutely continuous w.r.t. pθ and pθ,φ.
We deﬁne the change in KL via the importance resampled density as:
∆(pdata, pθ, pθ,φ) := DKL(pdata, pθ,φ) −DKL(pdata, pθ).
(13)
Substituting Eq. 10 in Eq. 13, we can simplify the above quantity as:
∆(pdata, pθ, pθ,φ) = Ex∼pdata[−log(pθ(x) ˆwφ(x)) + log Zθ,φ + log pθ(x)]
(14)
= Ex∼pdata[log ˆwφ(x)] −log Ex∼pθ[ ˆwφ(x)].
(15)
5

Table 1: Goodness-of-ﬁt evaluation on CIFAR-10 dataset for PixelCNN++ and SNGAN. Standard
errors computed over 10 runs. Higher IS is better. Lower FID and KID scores are better.
Model
Evaluation
IS (↑)
FID (↓)
KID (↓)
-
Reference
11.09 ± 0.1263
5.20 ± 0.0533
0.008 ± 0.0004
PixelCNN++
Default (no debiasing)
5.16 ± 0.0117
58.70 ± 0.0506
0.196 ± 0.0001
LFIW
6.68 ± 0.0773
55.83 ± 0.9695
0.126 ± 0.0009
SNGAN
Default (no debiasing)
8.33± 0.0280
20.40 ± 0.0747
0.094 ± 0.0002
LFIW
8.57 ± 0.0325
17.29 ± 0.0698
0.073 ±0.0004
The above expression provides a necessary and sufﬁcient condition for any positive real valued
function (such as the LFIW classiﬁer in Section 3) to improve the KL divergence ﬁt to the underlying
data distribution. In practice, an unbiased estimate of the LHS can be obtained via Monte Carlo
averaging of log- importance weights based on Dtrain. The empirical estimate for the RHS is however
biased.3 To remedy this shortcoming, we consider the following necessary but insufﬁcient condition.
Proposition 1. If ∆(pdata, pθ, pθ,φ) ≥0, then the following conditions hold:
Ex∼pdata[ ˆwφ(x)] ≥Ex∼pθ[ ˆwφ(x)],
(16)
Ex∼pdata[log ˆwφ(x)] ≥Ex∼pθ[log ˆwφ(x)].
(17)
The conditions in Eq. 16 and Eq. 17 follow directly via Jensen’s inequality applied to the LHS and
RHS of Eq. 15 respectively. Here, we note that estimates for the expectations in Eqs. 16-17 based on
Monte Carlo averaging of (log-) importance weights are unbiased.
5
Application Use Cases
In all our experiments, the binary classiﬁer for estimating the importance weights was a calibrated
deep neural network trained to minimize the cross-entropy loss. The self-normalized LFIW in Eq. (7)
worked best. Additional analysis on the estimators and experiment details are in Appendices B and C.
5.1
Goodness-of-ﬁt testing
In the ﬁrst set of experiments, we highlight the beneﬁts of importance weighting for a debiased
evaluation of three popularly used sample quality metrics viz. Inception Scores (IS) [27], Frechet
Inception Distance (FID) [28], and Kernel Inception Distance (KID) [29]. All these scores can be
formally expressed as empirical expectations with respect to the model. For all these metrics, we can
simulate the population level unbiased case as a “reference score" wherein we artiﬁcially set both the
real and generated sets of samples used for evaluation as ﬁnite, disjoint sets derived from pdata.
We evaluate the three metrics for two state-of-the-art models trained on the CIFAR-10 dataset viz.
an autoregressive model PixelCNN++ [10] learned via maximum likelihood estimation and a latent
variable model SNGAN [11] learned via adversarial training. For evaluating each metric, we draw
10,000 samples from the model. In Table 1, we report the metrics with and without the LFIW bias
correction. The consistent debiased evaluation of these metrics via self-normalized LFIW suggest
that the SIR approximation to the importance resampled distribution (Eq. 11) is a better ﬁt to pdata.
5.2
Data Augmentation for Multi-Class Classiﬁcation
We consider data augmentation via Data Augmentation Generative Adversarial Networks (DA-
GAN) [1]. While DAGAN was motivated by and evaluated for the task of meta-learning, it can also
be applied for multi-class classiﬁcation scenarios, which is the setting we consider here. We trained a
DAGAN on the Omniglot dataset of handwritten characters [12]. The DAGAN training procedure is
described in the Appendix. The dataset is particularly relevant because it contains 1600+ classes but
only 20 examples from each class and hence, could potentially beneﬁt from augmented data.
3If ˆZ is an unbiased estimator for Z, then log ˆZ is a biased estimator for log Z via Jensen’s inequality.
6

(a)
(b)
(c)
(d)
(e)
(f)
Figure 2: Qualitative evaluation of importance weighting for data augmentation. (a-f) Top row shows
held-out data samples from a speciﬁc class in Omniglot. Bottom row shows generated samples from
the same class ranked in decreasing order of importance weights.
Table 2: Classiﬁcation accuracy on the Omniglot dataset. Standard errors computed over 5 runs.
Dataset
Dcl
Dg
Dg w/ LFIW
Dcl + Dg
Dcl + Dg w/ LFIW
Accuracy
0.6603 ± 0.0012
0.4431 ± 0.0054
0.4481 ± 0.0056
0.6600 ± 0.0040
0.6818 ± 0.0022
Once the model has been trained, it can be used for data augmentation in many ways. In particular, we
consider ablation baselines that use various combinations of the real training data Dcl and generated
data Dg for training a downstream classiﬁer. When the generated data Dg is used, we can either
use the data directly with uniform weighting for all training points, or choose to importance weight
(LFIW) the contributions of the individual training points to the overall loss. The results are shown in
Table 2. While generated data (Dg) alone cannot be used to obtain competitive performance relative
to the real data (Dcl) on this task as expected, the bias it introduces for evaluation and subsequent
optimization overshadows even the naive data augmentation (Dcl + Dg). In contrast, we can obtain
signiﬁcant improvements by importance weighting the generated points (Dcl + Dg w/ LFIW).
Qualitatively, we can observe the effect of importance weighting in Figure 2. Here, we show true
and generated samples for 6 randomly choosen classes (a-f) in the Omniglot dataset. The generated
samples are ranked in decreasing order of the importance weights. There is no way to formally test
the validity of such rankings and this criteria can also prefer points which have high density under
pdata but are unlikely under pθ since we are looking at ratios. Visual inspection suggests that the
classiﬁer is able to appropriately downweight poorer samples, as shown in Figure 2 (a, b, c, d - bottom
right). There are also failure modes, such as the lowest ranked generated images in Figure 2 (e, f -
bottom right) where the classiﬁer weights reasonable generated samples poorly relative to others.
This could be due to particular artifacts such as a tiny disconnected blurry speck in Figure 2 (e -
bottom right) which could be more revealing to a classiﬁer distinguishing real and generated data.
5.3
Model-based Off-policy Policy Evaluation
So far, we have seen use cases where the generative model was trained on data from the same
distribution we wish to use for Monte Carlo evaluation. We can extend our debiasing framework to
more involved settings when the generative model is a building block for specifying the full data
generation process, e.g., trajectory data generated via a dynamics model along with an agent policy.
In particular, we consider the setting of off-policy policy evaluation (OPE), where the goal is to
evaluate policies using experiences collected from a different policy. Formally, let (S, A, r, P, η, T)
denote an (undiscounted) Markov decision process with state space S, action space A, reward
function r, transition P, initial state distribution η and horizon T. Assume πe : S × A →[0, 1]
is a known policy that we wish to evaluate. The probability of generating a certain trajectory
τ = {s0, a0, s1, a1, ..., sT , aT } of length T with policy πe and transition P is given as:
p⋆(τ) = η(s0)
T −1
Y
t=0
πe(at|st)P(st+1|st, at).
(18)
The return on a trajectory R(τ) is the sum of the rewards across the state, action pairs in τ: R(τ) =
PT
t=1 r(st, at), where we assume a known reward function r.
7

Table 3: Off-policy policy evaluation on MuJoCo tasks. Standard error is over 10 Monte Carlo
estimates where each estimate contains 100 randomly sampled trajectories.
Environment
v(πe) (Ground truth)
˜v(πe)
ˆv(πe) (w/ LFIW)
ˆv80(πe) (w/ LFIW)
Swimmer
36.7 ± 0.1
100.4 ± 3.2
25.7 ± 3.1
47.6 ± 4.8
HalfCheetah
241.7 ± 3.56
204.0 ± 0.8
217.8 ± 4.0
219.1 ± 1.6
HumanoidStandup
14170 ± 53
8417 ± 28
9372 ± 375
9221 ± 381
0
20
40
60
80
100
H
20
40
60
| (v)|
Swimmer
0
20
40
60
80
100
H
20
30
| (v)|
HalfCheetah
0
20
40
60
80
100
H
4500
5000
5500
| (v)|
HumanoidStandup
Figure 3: Estimation error δ(v) = v(πe) −ˆvH(πe) for different values of H (minimum 0, maximum
100). Shaded area denotes standard error over different random seeds.
We are interested in the value of a policy deﬁned as v(πe) = Eτ∼p∗(τ) [R(τ)]. Evaluating πe requires
the (unknown) transition dynamics P. The dynamics model is a conditional generative model of
the next states st+1 conditioned on the previous state-action pair (st, at). If we have access to
historical logged data Dτ of trajectories τ = {s0, a0, s1, a1, . . . , } from some behavioral policy
πb : S × A →[0, 1], then we can use this off-policy data to train a dynamics model Pθ(st+1|st, at).
The policy πe can then be evaluated under this learned dynamics model as ˜v(πe) = Eτ∼˜p(τ)[R(τ)],
where ˜p uses Pθ instead of the true dynamics in Eq. (18).
However, the trajectories sampled with Pθ could signiﬁcantly deviate from samples from P due to
compounding errors [30]. In order to correct for this bias, we can use likelihood-free importance
weighting on entire trajectories of data. The binary classiﬁer c(st, at, st+1) for estimating the
importance weights in this case distinguishes between triples of true and generated transitions.
For any true triple (st, at, st+1) extracted from the off-policy data, the corresponding generated
triple (st, at,ˆst+1) only differs in the ﬁnal transition state, i.e., ˆst+1 ∼Pθ(ˆst+1|st, at). Such a
classiﬁer allows us to obtain the importance weights ˆw(st, at,ˆst+1) for every predicted state transition
(st, at,ˆst+1). The importance weights for the trajectory τ can be derived from the importance weights
of these individual transitions as:
p⋆(τ)
˜p(τ) =
QT −1
t=0 P(st+1|st, at)
QT −1
t=0 Pθ(st+1|st, at)
=
T −1
Y
t=0
P(st+1|st, at)
Pθ(st+1|st, at) ≈
T −1
Y
t=0
ˆw(st, at,ˆst+1).
(19)
Our ﬁnal LFIW estimator is given as:
ˆv(πe) = Eτ∼˜p(τ)
"T −1
Y
t=0
ˆw(st, at,ˆst+1) · R(τ)
#
.
(20)
We consider three continuous control tasks in the MuJoCo simulator [15] from OpenAI gym [31]
(in increasing number of state dimensions): Swimmer, HalfCheetah and HumanoidStandup. High
dimensional state spaces makes it challenging to learning a reliable dynamics model in these environ-
ments. We train behavioral and evaluation policies using Proximal Policy Optimization [32] with
different hyperparameters for the two policies. The dataset collected via trajectories from the behavior
policy are used train a ensemble neural network dynamics model. We the use the trained dynamics
model to evaluate ˜v(πe) and its IW version ˆv(πe), and compare them with the ground truth returns
v(πe). Each estimation is averaged over a set of 100 trajectories with horizon T = 100. Speciﬁcally,
for ˆv(πe), we also average the estimation over 10 classiﬁer instances trained with different random
seeds on different trajectories. We further consider performing IW over only the ﬁrst H steps, and
use uniform weights for the remainder, which we denote as ˆvH(πe). This allow us to interpolate
between ˜v(πe) ≡ˆv0(πe) and ˆv(πe) ≡ˆvT (πe). Finally, as in the other experiments, we used the
self-normalized variant (Eq. (7)) of the importance weighted estimator in Eq. (20).
We compare the policy evaluations under different environments in Table 3. These results show that
the rewards estimated with the trained dynamics model differ from the ground truth by a large margin.
8

By importance weighting the trajectories, we obtain much more accurate policy evaluations. As
expected, we also see that while LFIW leads to higher returns on average, the imbalance in trajectory
importance weights due to the multiplicative weights of the state-action pairs can lead to higher
variance in the importance weighted returns. In Figure 3, we demonstrate that policy evaluation
becomes more accurate as more timesteps are used for LFIW evaluations, until around 80 −100
timesteps and thus empirically validates the beneﬁts of importance weighting using a classiﬁer. Given
that our estimates have a large variance, it would be worthwhile to compose our approach with
other variance reduction techniques such as (weighted) doubly robust estimation in future work [33],
as well as incorporate these estimates within a framework such as MAGIC to further blend with
model-free OPE [14]. In Appendix C.5.1, we also consider a stepwise LFIW estimator for MBOPE
which applies importance weighting at the level of every decision as opposed to entire trajectories.
Overall. Across all our experiments, we observe that importance weighting the generated samples
leads to uniformly better results, whether in terms of evaluating the quality of samples, or their utility
in downstream tasks. Since the technique is a black-box wrapper around any generative model, we
expect this to beneﬁt a diverse set of tasks in follow-up works.
However, there is also some caution to be exercised with these techniques as evident from the results
of Table 1. Note that in this table, the conﬁdence intervals (computed using the reported standard
errors) around the model scores after importance weighting still do not contain the reference scores
obtained from the true model. This would not have been the case if our debiased estimator was
completely unbiased and this observation reiterates our earlier claim that LFIW is reducing bias,
as opposed to completely eliminating it. Indeed, when such a mismatch is observed, it is a good
diagnostic to either learn more powerful classiﬁers to better approximate the Bayes optimum, or ﬁnd
additional data from pdata in case the generative model fails the full support assumption.
6
Related Work & Discussion
Density ratios enjoy widespread use across machine learning e.g., for handling covariate shifts,
class imbalance etc. [9, 34]. In generative modeling, estimating these ratios via binary classiﬁers
is frequently used for deﬁning learning objectives and two sample tests [19, 35, 35–41]. In partic-
ular, such classiﬁers have been used to deﬁne learning frameworks such as generative adversarial
networks [8, 42], likelihood-free Approximate Bayesian Computation (ABC) [43] and earlier work
in unsupervised-as-supervised learning [44] and noise contrastive estimation [43] among others.
Recently, [45] used importance weighting to reweigh datapoints based on differences in training
and test data distributions i.e., dataset bias. The key difference is that these works are explicitly
interested in learning the parameters of a generative model. In contrast, we use the binary classiﬁer
for estimating importance weights to correct for the model bias of any ﬁxed generative model.
Recent concurrent works [46–48] use MCMC and rejection sampling to explicitly transform or reject
the generated samples. These methods require extra computation beyond training a classiﬁer, in
rejecting the samples or running Markov chains to convergence, unlike the proposed importance
weighting strategy. For many model-based Monte Carlo evaluation usecases (e.g., data augmentation,
MBOPE), this extra computation is unnecessary. If samples or density estimates are explicitly needed
from the induced resampled distribution, we presented a particle-based approximation to the induced
density where the number of particles is a tunable knob allowing for trading statistical accuracy with
computational efﬁciency. Finally, we note resampling based techniques have been extensively studied
in the context of improving variational approximations for latent variable generative models [49–52].
7
Conclusion
We identiﬁed bias with respect to a target data distribution as a fundamental challenge restricting the
use of deep generative models as proposal distributions for Monte Carlo evaluation. We proposed a
bias correction framework based on importance sampling. The importance weights are learned in a
likelihood-free fashion via a binary classiﬁer. Empirically, we ﬁnd the bias correction to be useful
across a surprising variety of tasks including goodness-of-ﬁt sample quality tests, data augmentation,
and model-based off-policy policy evaluation. The ability to characterize the bias of a deep generative
model is an important step towards using these models to guide decisions in high-stakes applications
under uncertainty [53, 54], such as healthcare [55–57] and robust anomaly detection [58, 59].
9

Acknowledgments
This project was initiated when AG was an intern at Microsoft Research. We are thankful to Daniel
Levy, Rui Shu, Yang Song, and members of the Reinforcement Learning, Deep Learning, and
Adaptive Systems and Interaction groups at Microsoft Research for helpful discussions and comments
on early drafts. This research was supported by NSF (#1651565, #1522054, #1733686), ONR,
AFOSR (FA9550-19-1-0024), and FLI.
References
[1] Antreas Antoniou, Amos Storkey, and Harrison Edwards. Data augmentation generative
adversarial networks. arXiv preprint arXiv:1711.04340, 2017.
[2] Shie Mannor, Duncan Simester, Peng Sun, and John N Tsitsiklis. Bias and variance approxima-
tion in value function estimates. Management Science, 53(2):308–322, 2007.
[3] Philip S Thomas. Safe reinforcement learning. PhD thesis, University of Massachusetts
Libraries, 2015.
[4] Murray Rosenblatt. Remarks on some nonparametric estimates of a density function. The
Annals of Mathematical Statistics, pages 832–837, 1956.
[5] Sanjeev Arora, Andrej Risteski, and Yi Zhang. Do gans learn the distribution? some theory and
empirics. In International Conference on Learning Representations, 2018.
[6] Daniel G Horvitz and Donovan J Thompson. A generalization of sampling without replacement
from a ﬁnite universe. Journal of the American statistical Association, 1952.
[7] Diederik P Kingma and Max Welling.
Auto-encoding variational bayes.
arXiv preprint
arXiv:1312.6114, 2013.
[8] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil
Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural
Information Processing Systems, 2014.
[9] Masashi Sugiyama, Taiji Suzuki, and Takafumi Kanamori. Density ratio estimation in machine
learning. Cambridge University Press, 2012.
[10] Tim Salimans, Andrej Karpathy, Xi Chen, and Diederik P Kingma. Pixelcnn++: Improving the
pixelcnn with discretized logistic mixture likelihood and other modiﬁcations. arXiv preprint
arXiv:1701.05517, 2017.
[11] Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization
for generative adversarial networks. arXiv preprint arXiv:1802.05957, 2018.
[12] Brenden M Lake, Ruslan Salakhutdinov, and Joshua B Tenenbaum. Human-level concept
learning through probabilistic program induction. Science, 350(6266):1332–1338, 2015.
[13] Doina Precup, Richard S. Sutton, and Satinder P. Singh. Eligibility traces for off-policy policy
evaluation. In International Conference on Machine Learning, 2000.
[14] Philip Thomas and Emma Brunskill. Data-efﬁcient off-policy policy evaluation for reinforce-
ment learning. In International Conference on Machine Learning, 2016.
[15] Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based
control. In International Conference on Intelligent Robots and Systems. IEEE, 2012.
[16] Benigno Uria, Marc-Alexandre Côté, Karol Gregor, Iain Murray, and Hugo Larochelle. Neural
autoregressive distribution estimation. The Journal of Machine Learning Research, 17(1):
7184–7220, 2016.
[17] Laurent Dinh, David Krueger, and Yoshua Bengio. Nice: Non-linear independent components
estimation. arXiv preprint arXiv:1410.8516, 2014.
10

[18] Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation
and approximate inference in deep generative models. arXiv preprint arXiv:1401.4082, 2014.
[19] Shakir Mohamed and Balaji Lakshminarayanan. Learning in implicit generative models. arXiv
preprint arXiv:1610.03483, 2016.
[20] Alexander J Ratner, Henry Ehrenberg, Zeshan Hussain, Jared Dunnmon, and Christopher Ré.
Learning to compose domain-speciﬁc transformations for data augmentation. In Advances in
Neural Information Processing Systems, 2017.
[21] Shengjia Zhao, Hongyu Ren, Arianna Yuan, Jiaming Song, Noah Goodman, and Stefano Ermon.
Bias and generalization in deep generative models: An empirical study. In Advances in Neural
Information Processing Systems, 2018.
[22] Aditya Grover and Stefano Ermon. Boosted generative models. In AAAI Conference on Artiﬁcial
Intelligence, 2018.
[23] Augustus Odena, Vincent Dumoulin, and Chris Olah.
Deconvolution and checkerboard
artifacts. Distill, 2016. doi: 10.23915/distill.00003. URL http://distill.pub/2016/
deconv-checkerboard.
[24] Augustus Odena. Open questions about generative adversarial networks. Distill, 4(4):e18, 2019.
[25] Jun S Liu and Rong Chen. Sequential monte carlo methods for dynamic systems. Journal of
the American statistical association, 93(443):1032–1044, 1998.
[26] Arnaud Doucet, Simon Godsill, and Christophe Andrieu. On sequential monte carlo sampling
methods for bayesian ﬁltering. Statistics and computing, 10(3):197–208, 2000.
[27] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.
Improved techniques for training gans. In Advances in Neural Information Processing Systems,
pages 2234–2242, 2016.
[28] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.
Gans trained by a two time-scale update rule converge to a local nash equilibrium. In Advances
in Neural Information Processing Systems, pages 6626–6637, 2017.
[29] Mikołaj Bi´nkowski, Dougal J Sutherland, Michael Arbel, and Arthur Gretton. Demystifying
mmd gans. arXiv preprint arXiv:1801.01401, 2018.
[30] Stéphane Ross and Drew Bagnell. Efﬁcient reductions for imitation learning. In International
Conference on Artiﬁcial Intelligence and Statistics, 2010.
[31] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang,
and Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.
[32] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal
policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
[33] Mehrdad Farajtabar, Yinlam Chow, and Mohammad Ghavamzadeh. More robust doubly robust
off-policy evaluation. In International Conference on Machine Learning, 2018.
[34] Jonathon Byrd and Zachary C Lipton. What is the effect of importance weighting in deep
learning? arXiv preprint arXiv:1812.03372, 2018.
[35] Mihaela Rosca, Balaji Lakshminarayanan, David Warde-Farley, and Shakir Mohamed. Vari-
ational approaches for auto-encoding generative adversarial networks.
arXiv preprint
arXiv:1706.04987, 2017.
[36] Arthur Gretton, Karsten M Borgwardt, Malte Rasch, Bernhard Schölkopf, and Alex J Smola.
A kernel method for the two-sample-problem. In Advances in Neural Information Processing
Systems, 2007.
11

[37] Samuel R Bowman, Luke Vilnis, Oriol Vinyals, Andrew M Dai, Rafal Jozefowicz, and Samy
Bengio. Generating sentences from a continuous space. arXiv preprint arXiv:1511.06349,
2015.
[38] David Lopez-Paz and Maxime Oquab. Revisiting classiﬁer two-sample tests. arXiv preprint
arXiv:1610.06545, 2016.
[39] Ivo Danihelka, Balaji Lakshminarayanan, Benigno Uria, Daan Wierstra, and Peter Dayan.
Comparison of maximum likelihood and gan-based training of real nvps. arXiv preprint
arXiv:1705.05263, 2017.
[40] Daniel Jiwoong Im, He Ma, Graham Taylor, and Kristin Branson. Quantitatively evaluating
gans with divergences proposed for training. arXiv preprint arXiv:1803.01045, 2018.
[41] Ishaan Gulrajani, Colin Raffel, and Luke Metz. Towards gan benchmarks which require
generalization. In International Conference on Learning Representations, 2019.
[42] Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-gan: Training generative neural sam-
plers using variational divergence minimization. In Advances in Neural Information Processing
Systems, pages 271–279, 2016.
[43] Michael U Gutmann and Aapo Hyvärinen. Noise-contrastive estimation of unnormalized
statistical models, with applications to natural image statistics. Journal of Machine Learning
Research, 13(Feb):307–361, 2012.
[44] Jerome Friedman, Trevor Hastie, and Robert Tibshirani. The elements of statistical learning,
volume 1. Springer series in statistics New York, NY, USA:, 2001.
[45] Maurice Diesendruck, Ethan R Elenberg, Rajat Sen, Guy W Cole, Sanjay Shakkottai,
and Sinead A Williamson.
Importance weighted generative networks.
arXiv preprint
arXiv:1806.02512, 2018.
[46] Ryan Turner, Jane Hung, Yunus Saatci, and Jason Yosinski. Metropolis-hastings generative
adversarial networks. arXiv preprint arXiv:1811.11357, 2018.
[47] Samaneh Azadi, Catherine Olsson, Trevor Darrell, Ian Goodfellow, and Augustus Odena.
Discriminator rejection sampling. arXiv preprint arXiv:1810.06758, 2018.
[48] Chenyang Tao, Liqun Chen, Ricardo Henao, Jianfeng Feng, and Lawrence Carin. Chi-square
generative adversarial network. In International Conference on Machine Learning, 2018.
[49] Yuri Burda, Roger Grosse, and Ruslan Salakhutdinov. Importance weighted autoencoders.
arXiv preprint arXiv:1509.00519, 2015.
[50] Tim Salimans, Diederik Kingma, and Max Welling. Markov chain monte carlo and variational
inference: Bridging the gap. In International Conference on Machine Learning, 2015.
[51] Christian A Naesseth, Scott W Linderman, Rajesh Ranganath, and David M Blei. Variational
sequential monte carlo. arXiv preprint arXiv:1705.11140, 2017.
[52] Aditya Grover, Ramki Gummadi, Miguel Lazaro-Gredilla, Dale Schuurmans, and Stefano
Ermon. Variational rejection sampling. In International Conference on Artiﬁcial Intelligence
and Statistics, 2018.
[53] Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing model
uncertainty in deep learning. In International Conference on Machine Learning, 2016.
[54] Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable
predictive uncertainty estimation using deep ensembles. In Advances in Neural Information
Processing Systems, 2017.
[55] Matthieu Komorowski, A Gordon, LA Celi, and A Faisal. A markov decision process to suggest
optimal treatment of severe infections in intensive care. In Neural Information Processing
Systems Workshop on Machine Learning for Health, 2016.
12

[56] Zhengyuan Zhou, Daniel Miller, Neal Master, David Scheinker, Nicholas Bambos, and Peter
Glynn. Detecting inaccurate predictions of pediatric surgical durations. In International
Conference on Data Science and Advanced Analytics, 2016.
[57] Aniruddh Raghu, Matthieu Komorowski, Leo Anthony Celi, Peter Szolovits, and Marzyeh
Ghassemi. Continuous state-space models for optimal sepsis treatment-a deep reinforcement
learning approach. arXiv preprint arXiv:1705.08422, 2017.
[58] Eric Nalisnick, Akihiro Matsukawa, Yee Whye Teh, Dilan Gorur, and Balaji Lakshminarayanan.
Do deep generative models know what they don’t know? arXiv preprint arXiv:1810.09136,
2018.
[59] Hyunsun Choi and Eric Jang. Generative ensembles for robust anomaly detection. arXiv
preprint arXiv:1810.01392, 2018.
[60] Bradley Efron and Robert J Tibshirani. An introduction to the bootstrap. CRC press, 1994.
[61] Alexandru Niculescu-Mizil and Rich Caruana. Predicting good probabilities with supervised
learning. In International Conference on Machine learning, 2005.
[62] Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural
networks. In International Conference on Machine Learning, 2017.
[63] Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu
Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al. Tensorﬂow: a system for
large-scale machine learning. In Operating Systems Design and Implementation, 2016.
[64] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethink-
ing the inception architecture for computer vision. In IEEE conference on Computer Vision and
Pattern Recognition, 2016.
[65] Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito,
Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in
pytorch. 2017.
[66] Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, et al. Matching networks
for one shot learning. In Advances in Neural Information Processing Systems, 2016.
[67] Prafulla Dhariwal, Christopher Hesse, Oleg Klimov, Alex Nichol, Matthias Plappert, Alec
Radford, John Schulman, Szymon Sidor, and Yuhuai Wu. Openai baselines. GitHub, GitHub
repository, 2017.
[68] Prajit Ramachandran, Barret Zoph, and Quoc V Le. Searching for activation functions. arXiv
preprint arXiv:1710.05941, 2017.
13

Appendices
A
Conﬁdence Intervals via Bootstrap
Bootstrap is a widely-used tool in statistics for deriving conﬁdence intervals by ﬁtting ensembles of
models on resampled data points. If the dataset is ﬁnite e.g., Dtrain, then the bootstrapped dataset
is obtained via random sampling with replacement and conﬁdence intervals are estimated via the
empirical bootstrap. For a parametric model generating the dataset e.g., pθ, a fresh bootstrapped
dataset is resampled from the model and conﬁdence intervals are estimated via the parametric
bootstrap. See [60] for a detailed review. In training a binary classiﬁer, we can estimate the
conﬁdence intervals by retraining the classiﬁer on a fresh sample of points from pθ and a resampling
of the training dataset Dtrain (with replacement). Repeating this process over multiple runs and then
taking a suitable quantile gives us the corresponding conﬁdence intervals.
B
Bias-Variance of Different LFIW estimators
As discussed in Section 3, bias reduction using LFIW can suffer from issues where the importance
weights are too small due to highly conﬁdent predictions of the binary classiﬁer. Across a batch of
Monte Carlo samples, this can increase the corresponding variance. Inspired from the importance
sampling literature, we proposed additional mechanisms to mitigate this additional variance at the
cost of reduced debiasing in Eqs. (7-9). We now look at the empirical bias-variance trade-off of these
different estimators via a simple experiment below.
Our setup follows the goodness-of-ﬁt testing experiments in Section 5. The statistics we choose to
estimate is simply are the 2048 activations of the preﬁnal layer of the Inception Network, averaged
across the test set of 10, 000 samples of CIFAR-10.
That is, the true statistics s = {s1, s2, · · · , s2048} are given by:
sj =
1
|Dtest|
X
x∈Dtest
aj(x)
(21)
where aj is the j-th preﬁnal layer activation of the Inception Network. Note that set of statistics s is
ﬁxed (computed once on the test set).
To estimate these statistics, we will use different estimators. For example, the default estimator
involving no reweighting is given as:
ˆsj = 1
T
T
X
i=1
aj(x)
(22)
where x ∼pθ.
Note that ˆsj is a random variable since it depends on the T samples drawn from pθ. Similar to
Eq. (22), other variants of the LFIW estimators proposed in Section 3 can be derived using Eqs. (7-9).
For any LFIW estimate ˆsj, we can use the standard decomposition of the expected mean-squared
error into terms corresponding to the (squared) bias and variance as shown below.
E[(sj −ˆsj)2] = s2
j −2sjE[ ˆsj] + E[ ˆsj]2
(23)
= s2
j −2sjE[ ˆsj] + (E[ ˆsj])2 + E[ ˆsj
2] −(E[ ˆsj])2
(24)
= (sj −E[ ˆsj])2
|
{z
}
Bias2
+ E[ ˆsj
2] −(E[ ˆsj])2
|
{z
}
Variance
.
(25)
In Table 4, we report the bias and variance terms of the estimators averaged over 10 draws of
T = 10, 0000 samples and further averaging over all 2048 statistics corresponding to s. We observe
that self-normalization performs consistently well and is the best or second best in terms of bias and
MSE in all cases. The ﬂattened estimator with no debiasing (corresponding to α = 0) has lower bias
and higher variance than the self-normalized estimator. Amongst the ﬂattening estimators, lower
14

Table 4: Bias-variance analysis for PixelCNN++ and SNGAN when T = 10, 000. Standard errors
over the absolute values of bias and variance evaluations are computed over the 2048 activation
statistics. Lower absolute values of bias, lower variance, and lower MSE is better.
Model
Evaluation
|Bias| (↓)
Variance (↓)
MSE (↓)
PixelCNN++
Self-norm
0.0240 ± 0.0014
0.0002935 ± 7.22e-06
0.0046 ± 0.00031
Flattening (α = 0)
0.0330 ± 0.0023
9.1e-06 ± 2.6e-07
0.0116 ± 0.00093
Flattening (α = 0.25)
0.1042 ± 0.0018
5.1e-06 ± 1.5e-07
0.0175 ± 0.00138
Flattening (α = 0.5)
0.1545 ± 0.0022
8.4e-06 ± 3.7e-07
0.0335 ± 0.00246
Flattening (α = 0.75)
0.1626 ± 0.0022
3.19e-05 ± 2e-06
0.0364 ± 0.00259
Flattening (α = 1.0)
0.1359 ± 0.0018
0.0002344 ± 1.619e-05
0.0257 ± 0.00175
Clipping (β = 0.001)
0.1359 ± 0.0018
0.0002344 ± 1.619e-05
0.0257 ± 0.00175
Clipping (β = 0.01)
0.1357 ± 0.0018
0.0002343 ± 1.618e-05
0.0256 ± 0.00175
Clipping (β = 0.1)
0.1233 ± 0.0017
0.000234 ± 1.611e-05
0.0215 ± 0.00149
Clipping (β = 1.0)
0.1255 ± 0.0030
0.0002429 ± 1.606e-05
0.0340 ± 0.00230
SNGAN
Self-norm
0.0178 ± 0.0008
1.98e-05 ± 5.9e-07
0.0016 ± 0.00023
Flattening (α = 0)
0.0257 ± 0.0010
9.1e-06 ± 2.3e-07
0.0026 ± 0.00027
Flattening (α = 0.25)
0.0096 ± 0.0007
8.4e-06 ± 3.1e-07
0.0011 ± 8e-05
Flattening (α = 0.5)
0.0295 ± 0.0006
1.15e-05 ± 6.4e-07
0.0017 ± 0.00011
Flattening (α = 0.75)
0.0361 ± 0.0006
1.93e-05 ± 1.39e-06
0.002 ± 0.00012
Flattening (α = 1.0)
0.0297 ± 0.0005
3.76e-05 ± 3.08e-06
0.0015 ± 7e-05
Clipping (β = 0.001)
0.0297 ± 0.0005
3.76e-05 ± 3.08e-06
0.0015 ± 7e-05
Clipping (β = 0.01)
0.0297 ± 0.0005
3.76e-05 ± 3.08e-06
0.0015 ± 7e-05
Clipping (β = 0.1)
0.0296 ± 0.0005
3.76e-05 ± 3.08e-06
0.0015 ± 7e-05
Clipping (β = 1.0)
0.1002 ± 0.0018
3.03e-05 ± 2.18e-06
0.0170 ± 0.00171
values of α seem to provide the best bias-variance trade-off. The clipped estimators do not perform
well in this setting, with lower values of β slightly preferable over larger values. We repeat the same
experiment with T = 5, 000 samples and report the results in Table 5. While the variance increases
as expected (by almost an order of magnitude), the estimator bias remains roughly the same.
Table 5: Bias-variance analysis for PixelCNN++ and SNGAN when T = 5, 000. Standard errors over
the absolute values of bias and variance evaluations are computed over the 2048 activation statistics.
Lower absolute values of bias, lower variance, and lower MSE is better.
Model
Evaluation
|Bias| (↓)
Variance (↓)
MSE (↓)
PixelCNN++
Self-norm
0.023 ± 0.0014
0.0005086 ± 1.317e-05
0.0049 ± 0.00033
Flattening (α = 0)
0.0330 ± 0.0023
1.65e-05 ± 4.6e-07
0.0116 ± 0.00093
Flattening (α = 0.25)
0.1038 ± 0.0018
9.5e-06 ± 3e-07
0.0174 ± 0.00137
Flattening (α = 0.5)
0.1539 ± 0.0022
1.74e-05 ± 8e-07
0.0332 ± 0.00244
Flattening (α = 0.75)
0.1620 ± 0.0022
6.24e-05 ± 3.83e-06
0.0362 ± 0.00256
Flattening (α = 1.0)
0.1360 ± 0.0018
0.0003856 ± 2.615e-05
0.0258 ± 0.00174
Clipping (β = 0.001)
0.1360 ± 0.0018
0.0003856 ± 2.615e-05
0.0258 ± 0.00174
Clipping (β = 0.01)
0.1358 ± 0.0018
0.0003856 ± 2.615e-05
0.0257 ± 0.00173
Clipping (β = 0.1)
0.1234 ± 0.0017
0.0003851 ± 2.599e-05
0.0217 ± 0.00148
Clipping (β = 1.0)
0.1250 ± 0.0030
0.0003821 ± 2.376e-05
0.0341 ± 0.00232
SNGAN
Self-norm
0.0176 ± 0.0008
3.88e-05 ± 9.6e-07
0.0016 ± 0.00022
Flattening (α = 0)
0.0256 ± 0.0010
1.71e-05 ± 4.3e-07
0.0027 ± 0.00027
Flattening (α = 0.25)
0.0099 ± 0.0007
1.44e-05 ± 3.7e-07
0.0011 ± 8e-05
Flattening (α = 0.5)
0.0298 ± 0.0006
1.62e-05 ± 5.3e-07
0.0017 ± 0.00012
Flattening (α = 0.75)
0.0366 ± 0.0006
2.38e-05 ± 1.11e-06
0.0021 ± 0.00012
Flattening (α = 1.0)
0.0302 ± 0.0005
4.56e-05 ± 2.8e-06
0.0015 ± 7e-05
Clipping (β = 0.001)
0.0302 ± 0.0005
4.56e-05 ± 2.8e-06
0.0015 ± 7e-05
Clipping (β = 0.01)
0.0302 ± 0.0005
4.56e-05 ± 2.8e-06
0.0015 ± 7e-05
Clipping (β = 0.1)
0.0302 ± 0.0005
4.56e-05 ± 2.81e-06
0.0015 ± 7e-05
Clipping (β = 1.0)
0.1001 ± 0.0018
5.19e-05 ± 2.81e-06
0.0170 ± 0.0017
15

C
Additional Experimental Details
C.1
Calibration
Figure 4: Calibration of classiﬁers for density ratio estimation.
We found in all our cases that the binary classiﬁers used for training the model were highly calibrated
by default and did not require any further recalibration. See for instance the calibration of the binary
classiﬁer used for goodness-of-ﬁt experiments in Figure 4. We performed the analysis on a held-out
set of real and generated samples and used 10 bins for computing calibration statistics.
We believe the default calibration behavior is largely due to the fact that our binary classiﬁers
distinguishing real and fake data do not require very complex neural networks architectures and
training tricks that lead to miscalibration for multi-class classiﬁcation. As shown in [61], shallow
networks are well-calibrated and [62] further argue that a major reason for miscalibration is the use
of a softmax loss typical for multi-class problems.
C.2
Synthetic experiment
The classiﬁer used in this case is a multi-layer perceptron with a single hidden layer of 100 units and
has been trained to minimize the cross-entropy loss by ﬁrst order optimization methods. The dataset
used for training the classiﬁer consists of an equal number of samples (denoted as n in Figure 1)
drawn from the generative model and the data distribution.
C.3
Goodness-of-ﬁt testing
We used the Tensorﬂow implementation of Inception Network [63] to ensure the sample quality
metrics are comparable with prior work. For a semantic evaluation of difference in sample quality,
this test is performed in the feature space of a pretrained classiﬁer, such as the preﬁnal activations of
the Inception Net [64]. For example, the Inception score for a generative model pθ given a classiﬁer
d(·) can be expressed as:
IS = exp(Ex∼pθ[KL(d(y|x), d(y))]).
The FID score is another metric which unlike the Inception score also takes into account real data
from pdata. Mathematically, the FID between sets S and R sampled from distributions pθ and pdata
respectively, is deﬁned as:
FID(S, R) = ∥µS −µR∥2
2 + Tr(ΣS + ΣR −2
p
ΣSΣR)
where (µS, ΣS) and (µR, ΣR) are the empirical means and covariances computed based on S and R
respectively. Here, S and R are sets of datapoints from pθ and pdata. In a similar vein, KID compares
statistics between samples in a feature space deﬁned via a combination of kernels and a pretrained
classiﬁer. The standard kernel used is a radial-basis function kernel with a ﬁxed bandwidth of 1. As
desired, the score is optimized when the data and model distributions match.
We used the open-sourced model implementations of PixelCNN++ [27] and SNGAN [11]. Following
the observation by [38], we found that training a binary classiﬁer on top of the feature space of any
16

pretrained image classiﬁer was useful for removing the low-level artifacts in the generated images in
classifying an image as real or fake. We hence learned a multi-layer perceptron (with a single hidden
layer of 1000 units) on top of the 2048 dimensional feature space of the Inception Network. Learning
was done using the Adam optimizer with the default hyperparameters with a learning rate of 0.001
and a batch size of 64. We observed relatively fast convergence for training the binary classiﬁer (in
less than 20 epochs) on both PixelCNN++ and SNGAN generated data and the best validation set
accuracy across the ﬁrst 20 epochs was used for ﬁnal model selection.
C.4
Data Augmentation
Our codebase was implemented using the PyTorch library [65]. We built on top of the open-source
implementation of DAGAN4 [1].
A DAGAN learns to augment data by training a conditional generative model Gθ : X × Z →X
based on a training dataset Dcl. This dataset is same as the one we used for training the generative
model and the binary classiﬁer for density ratio estimation. The generative model is learned via a
minimax game with a critic. For any conditioning datapoint xi ∈Dtrain and noise vector z ∼p(z),
the critic learns to distinguish the generated data Gθ(xi, z) paired along with xi against another pair
(xi, xj). Here, the point xj is chosen such that the points xi and xj have the same label in Dcl, i.e.,
yi = yj. Hence, the critic learns to classify pairs of (real, real) and (real, generated) points while
encouraging the generated points to be of the same class as the point being conditioned on. For
the generated data, the label y is assumed to be the same as the class of the point that was used for
generating the data. We refer the reader to [1] for further details.
Given a DAGAN model, we additionally require training a binary classiﬁer for estimating importance
weights and a multi-class classiﬁer for subsequent classiﬁcation. The architecture for both these
use cases follows prior work in meta learning on Omniglot [66]. We train the DAGAN on the 1200
classes reserved for training in prior works. For each class, we consider a 15/5/5 split of the 20
examples for training, validation, and testing. Except for the ﬁnal output layer, the architecture
consists of 4 blocks of 3x3 convolutions and 64 ﬁlters, followed by batch normalization [64], a ReLU
non-linearity and 2x2 max pooling. Learning was done for 100 epochs using the Adam optimizer
with default parameters and a learning rate of 0.001 with a batch size of 32.
C.5
Model-based Off-policy Policy Evaluation
For this set of experiments, we used Tensorﬂow [63] and OpenAI baselines5 [67]. We evaluate over
three envionments viz. Swimmer, HalfCheetah, and HumanoidStandup (Figure 5. Both HalfCheetah
and Swimmer rewards the agent for gaining higher horizontal velocity; HumanoidStandup rewards the
agent for gaining more height via standing up. In all three environments, the initial state distributions
are obtained via adding small random perturbation around a certain state. The dimensions for state
and action spaces are shown in Table 6.
(a) Swimmer
(b) HalfCheetah
(c) HumanoidStandup
Figure 5: Environments in OPE experiments.
Our policy network has two fully connected layers with 64 neurons and tanh activations for each
layer, where as our transition model / classiﬁer has three hidden layers of 500 neurons with swish
activations [68]. We obtain our evaluation policy by training with PPO for 1M timesteps, and our
behavior policy by training with PPO for 500k timesteps. Then we train the dynamics model Pθ for
4https://github.com/AntreasAntoniou/DAGAN.git
5https://github.com/openai/baselines.git
17

Table 6: Statistics for the environments.
Environment
State dimensionality
# Action dimensionality
Swimmer
8
2
HalfCheetah
17
6
HumanoidStandup
376
17
Table 7: Off-policy policy evaluation on MuJoCo tasks. Standard error is over 10 Monte Carlo
estimates where each estimate contains 100 randomly sampled trajectories. Here, we perform
stepwise LFIW over transition triplets.
Environment
v(πe) (Ground truth)
˜v(πe)
ˆv(πe) (w/ LFIW)
ˆv80(πe) (w/ LFIW)
Swimmer
36.7 ± 0.1
100.4 ± 3.2
19.4 ± 4.3
48.3 ± 4.0
HalfCheetah
241.7 ± 3.6
204.0 ± 0.8
229.1 ± 4.9
214.9 ± 3.9
HumanoidStandup
14170 ± 5.3
8417 ± 28
10612 ± 794
9950 ± 640
100k iterations with a batch size of 128. Our classiﬁer is trained for 10k iterations with a batch size
of 250, where we concatenate (st, at, st+1) into a single vector.
0
20
40
60
80
100
H
20
40
60
| (v)|
Swimmer
0
20
40
60
80
100
H
20
40
| (v)|
HalfCheetah
0
20
40
60
80
100
H
4000
6000
| (v)|
HumanoidStandup
Figure 6: Estimation error δ(v) = v(πe) −ˆvH(πe) for different values of H (minimum 0, maximum
100). Shaded area denotes standard error over different random seeds; each seed uses 100 sampled
trajectories. Here, we use LFIW over transition triplets.
C.5.1
Stepwise LFIW
Here, we consider performing LFIW over the transition triplets, where each transition triplet
(st, at, st+1) is assigned its own importance weight. This is in contrast to assigning a single impor-
tance weight for the entire trajectory, obtained by multiplying the importance weights of all transitions
in the trajectory. The importance weight for a transition triplet is deﬁned as:
p⋆(st, at, st+1)
˜p(st, at, st+1) ≈ˆw(st, at, st+1),
(26)
so the corresponding LFIW estimator is given as
ˆv(πe) = Eτ∼˜p(τ)
"T −1
X
t=0
ˆw(st, at,ˆst+1) · r(st, at)
#
.
(27)
We describe this as the “stepwise" LFIW approach for off-policy policy evaluation. We perform
self-normalization over the weights of each triplet.
From the results in Table 7 and Figure 6, stepwise LFIW also reduces bias for OPE compared to
without LFIW. Compared to the “trajectory based" LFIW described in Eq. (20), the stepwise estimator
has slightly higher variance and weaker performance for H = 20, 40, but outperforms the trajectory
level estimators when H = 100 on HalfCheetah and HumanoidStandup environments.
18

