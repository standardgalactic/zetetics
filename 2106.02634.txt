Light Field Networks: Neural Scene Representations
with Single-Evaluation Rendering
Vincent Sitzmann1,∗
sitzmann@mit.edu
Semon Rezchikov2,∗
skr@math.columbia.edu
William T. Freeman1,3
billf@mit.edu
Joshua B. Tenenbaum1,4,5
jbt@mit.edu
Frédo Durand1
fredo@mit.edu
1MIT CSAIL
2Columbia University
3 IAFI
4MIT BCS
5 CBMM
vsitzmann.github.io/lfns/
Abstract
Inferring representations of 3D scenes from 2D observations is a fundamental prob-
lem of computer graphics, computer vision, and artiﬁcial intelligence. Emerging
3D-structured neural scene representations are a promising approach to 3D scene
understanding. In this work, we propose a novel neural scene representation, Light
Field Networks or LFNs, which represent both geometry and appearance of the
underlying 3D scene in a 360-degree, four-dimensional light ﬁeld parameterized
via a neural network. Rendering a ray from an LFN requires only a single network
evaluation, as opposed to hundreds of evaluations per ray for ray-marching or
volumetric based renderers in 3D-structured neural scene representations. In the
setting of simple scenes, we leverage meta-learning to learn a prior over LFNs
that enables multi-view consistent light ﬁeld reconstruction from as little as a
single image observation. This results in dramatic reductions in time and memory
complexity, and enables real-time rendering. The cost of storing a 360-degree
light ﬁeld via an LFN is two orders of magnitude lower than conventional methods
such as the Lumigraph. Utilizing the analytical differentiability of neural implicit
representations and a novel parameterization of light space, we further demonstrate
the extraction of sparse depth maps from LFNs.
1
Introduction
A fundamental problem across computer graphics, computer vision, and artiﬁcial intelligence is to
infer a representation of a scene’s 3D shape and appearance given impoverished observations such
as 2D images of the scene. Recent contributions have advanced the state of the art for this problem
signiﬁcantly. First, neural implicit representations have enabled efﬁcient representation of local 3D
scene properties by mapping a 3D coordinate to local properties of the 3D scene at that coordinate [1–
6]. Second, differentiable neural renderers allow for the inference of these representations given only
2D image observations [3, 4]. Finally, leveraging meta-learning approaches such as hypernetworks or
gradient-based meta-learning has enabled the learning of distributions of 3D scenes, and therefore
reconstruction given only a single image observation [3]. This has enabled a number of applications,
such as novel view synthesis [7, 3, 6], 3D reconstruction [5, 3] semantic segmentation [8, 9], and
SLAM [10]. However, 3D-structured neural scene representations come with a major limitation:
Their rendering is prohibitively expensive, on the order of tens of seconds for a single 256 × 256
image for state-of-the-art approaches. In particular, parameterizing the scene in 3D space necessitates
∗These authors contributed equally to this work.
35th Conference on Neural Information Processing Systems (NeurIPS 2021), Sydney, Australia.
arXiv:2106.02634v2  [cs.CV]  18 Jan 2022

the discovery of surfaces along camera rays during rendering. This can be solved either by encoding
geometry as a level set of an occupancy or signed distance function, or via volumetric rendering,
which solves an alpha-compositing problem along each ray. Either approach, however, requires tens
or even hundreds of evaluations of the 3D neural scene representation in order to render a single
camera ray.
We propose a novel neural scene representation, dubbed Light Field Networks or LFNs. Instead of
encoding a scene in 3D space, Light Field Networks encode a scene by directly mapping an oriented
camera ray in the four dimensional space of light rays to the radiance observed by that ray. This
obviates the need to query opacity and RGB at 3D locations along a ray or to ray-march towards the
level set of a signed distance function, speeding up rendering by three orders of magnitude compared
to volumetric methods. In addition to directly encoding appearance, we demonstrate that LFNs
encode information about scene geometry in their derivatives. Utilizing the unique ﬂexibility of neural
ﬁeld representations, we introduce the use of Plücker coordinates to parameterize 360-degree light
ﬁelds, which allow for storage of a-priori unbounded scenes and admit a simple expression for the
depth as an analytical function of an LFN. Using this relationship, we demonstrate the computation
of geometry in the form of sparse depth maps. While 3D-structured neural scene representations
are multi-view consistent by design, parameterizing a scene in light space does not come with this
guarantee: the additional degree of freedom enables rays that view the same 3D point to change
appearance across viewpoints. For the setting of simple scenes, we demonstrate that this challenge
can be overcome by learning a prior over 4D light ﬁelds in a meta-learning framework. We benchmark
with current state-of-the-art approaches for single-shot novel view synthesis, and demonstrate that
LFNs compare favorably with globally conditioned 3D-structured representations, while accelerating
rendering and reducing memory consumption by orders of magnitude.
In summary, we make the following contributions:
1. We propose Light Field Networks (LFNs), a novel neural scene representation that directly
parameterizes the light ﬁeld of a 3D scene via a neural network, enabling real-time rendering
and vast reduction in memory utilization.
2. We demonstrate that we may leverage 6-dimensional Plücker coordinates as a parameteri-
zation of light ﬁelds, despite their apparent overparameterization of the 4D space of rays,
thereby enabling continuous, 360-degree light ﬁelds.
3. By embedding LFNs in a meta-learning framework, we demonstrate light ﬁeld reconstruction
and novel view synthesis of simple scenes from sparse 2D image supervision only.
4. We demonstrate that inferred LFNs encode both appearance and geometry of the underlying
3D scenes by extracting sparse depth maps from the derivatives of LFNs, leveraging their
analytical differentiability.
Scope.
The proposed method is currently constrained to the reconstruction of simple scenes, such
as single objects and simple room-scale scenes, in line with recent work on learning generative
models in this regime [3, 11].
2
Related Work
Neural Scene Representations and Neural Rendering.
A large body of work addresses the
question of inferring feature representations of 3D scenes useful to downstream tasks across graphics,
vision, and machine learning. Models without 3D structure suffer from poor data efﬁciency [12, 13].
Voxel grids [14–20] offer 3D structure, but scale poorly with spatial resolution. Inspired by neural
implicit representations of 3D geometry [1, 2], recent work has proposed to encode properties
of 3D scenes as neural ﬁelds (also implicit- or coordinate-based representations, see [21] for an
overview), neural networks that map 3D coordinates to local properties of the 3D scene at these
coordinates. Using differentiable rendering, these models can be learned from image observations
only [3, 4, 22, 11]. Reconstruction from sparse observations can be achieved by learning priors
over the space of neural ﬁelds [3, 5, 11, 23–25] or by conditioning of the neural ﬁeld on local
features [6, 26, 27]. Differentiable rendering of such 3D-structured neural scene representations is
exceptionally computationally intensive, requiring hundreds of evaluations of the neural representation
per ray, with tens of thousands to millions of rays per image. Some recent work seeks to accelerate
test-time rendering, but either does not admit generalization [28–30], or does not alleviate the cost of
2

Figure 1: Overview. We propose Light Field Networks or LFNs, which encode the full 360-degree
light ﬁeld of a 3D scene in the weights of a fully connected neural network Φφ (with weights φ
conditioned on a latent code z) that maps an oriented ray r to the radiance c observed by that ray.
Rendering an LFN Φφ only requires evaluating the underlying MLP once per ray, in contrast to
3D-structured neural scene representations Φ3D such as SRNs [3], NeRF [4], or DVR [5] that require
hundreds of evaluations per ray. We leverage meta-learning to learn a multi-view consistent space of
LFNs. Once trained, this enables reconstruction of a 360-degree light ﬁeld and subsequent real-time
novel view synthesis of simple scenes from only a single observation.
rendering at training/inference time [31–33]. With Light Field Networks, we propose to leverage 360-
degree light ﬁelds as neural scene representations. We introduce a novel neural ﬁeld parameterization
of 360-degree light ﬁelds, infer light ﬁelds via meta-learning from as few as a single 2D image
observation, and demonstrate that LFNs encode both scene geometry and appearance.
Light ﬁelds and their reconstruction.
Light ﬁelds have a rich history as a scene representation
in both computer vision and computer graphics. Adelson et al. [34] introduced the 5D plenoptic
function as a uniﬁed representation of information in the early visual system [35]. Levoy et al. [36]
and, concurrently, Gortler et al. [37] introduced light ﬁelds in computer graphics as a 4D sampled
scene representation for fast image-based rendering. Light ﬁelds have since enjoyed popularity as a
representation for novel view synthesis [38] and computational photography, e.g. [39]. Light ﬁelds
enable direct rendering of novel views by simply extracting a 2D slice of the 4D light ﬁeld. However,
they tend to incur signiﬁcant storage cost, and since they rely on two-plane parameterizations, they
make it hard to achieve a full 360-degree representation without concatenating multiple light ﬁelds.
A signiﬁcant amount of prior work addresses reconstruction of fronto-parallel light ﬁelds via hand-
crafted priors, such as sparsity in the Fourier or shearlet domains [40–42]. With the advent of deep
learning, approaches to light ﬁeld reconstruction that leverage convolutional neural networks to
in-paint or extrapolate light ﬁelds from sparse views have been proposed [43, 7, 44], but similarly
only support fronto-parallel novel view synthesis. We are instead interested in light ﬁelds as a
representation of 3D appearance and geometry that enables efﬁcient inference of and reasoning about
the properties of the full underlying scene.
3
Background: 3D-structured Neural Scene Representations
Recent progress in neural scene representation and rendering has been driven by two key innovations.
The ﬁrst are neural ﬁelds, often also referred to as neural implicit- or coordinate-based scene
representations Φ3D [3, 4], which model a scene as a continuous function, parameterized as an MLP
which maps a 3D coordinate to a representation v of whatever is at that 3D coordinate:
Φ3D : R3 →Rn,
x 7→Φ3D(x) = v.
(1)
The second is a differentiable renderer m, which, given a ray r in R3, and the representation Φ3D,
computes the value of the color c of the scene when viewed along r:
m(r, Φ3D) = c(r) ∈R3.
(2)
Existing rendering methods broadly fall into two categories: sphere-tracing-based renderers [3, 45,
5, 46] and volumetric renderers [19, 4]. These methods require on the order of tens or hundreds of
3

evaluations of the values of Φ3D along a ray r to compute c(r). This leads to extraordinarily large
memory and time complexity of rendering. As training requires error backpropagation through the
renderer, this impacts both training and test time.
4
The Light Field Network Scene Representation
We propose to represent a scene as a 360-degree neural light ﬁeld, a function parameterized by an
MLP Φφ with parameters φ that directly maps the 4D space L of oriented rays to their observed
radiance:
Φφ : L →R3, r 7→Φφ(r) = c(r).
(3)
A light ﬁeld completely characterizes the ﬂow of light through unobstructed space in a static scene
with ﬁxed illumination. Light ﬁelds have the unique property that rendering is achieved by a single
evaluation of Φ per light ray, i.e., no ray-casting is required. Moreover, while the light ﬁeld only
encodes appearance explicitly, its derivatives encode geometry information about the underlying
3D scene [47, 34, 35]. This makes many methods to extract 3D geometry from light ﬁelds possible
[48–51], and we demonstrate efﬁcient recovery of sparse depth maps from LFNs below.
4.1
Implicit representations for 360 degree light ﬁelds
To fully represent a 3D scene requires a parameterization of all light rays in space. Conventional light
ﬁeld methods are constrained to leverage minimal parameterizations of the 4D space of rays, due
to the high memory requirements of discretely sampled high-dimensional spaces. In contrast, our
use of neural ﬁeld representations allows us to freely choose a continuous parameterization that is
mathematically convenient. In particular, we propose to leverage the 6D Plücker parameterization of
the space of light rays L for LFNs. The Plücker coordinates (see [52] for an excellent overview) of a
ray r through a point p in a normalized direction d are
r = (d, m) ∈R6 where m = p × d, for
d ∈S2, p ∈R3.
(4)
where × denotes the cross product. While Plücker coordinates are a-priori 6-tuples of real numbers,
the coordinates of any ray lie on a curved 4-dimensional subspace L. Plücker coordinates uniformly
represent all oriented rays in space without singular directions or special cases. Intuitively, a
general ray r together with the origin deﬁne a plane, and m is a normal vector to the plane with
its magnitude capturing the distance from the ray to the origin; if m = 0 then the ray passes
through the origin and is deﬁned by its direction d. This is in contrast to conventional light ﬁeld
parameterizations: Fronto-parallel two-plane or cylindrical parameterizations cannot represent the
full 360-degree light ﬁeld of a scene [36, 53]. Cubical two-plane arrangements [37, 38] are not
continuous, complicating the parameterization via a neural implicit representation. In contrast to the
two-sphere parameterization [54], Plücker coordinates do not require that scenes are bounded in size
and do not require spherical trigonometry.
The parameterization via a neural ﬁeld enables compact storage of a 4D light ﬁeld that can be sampled
at arbitrary resolutions, while non-neural representations are resolution-limited. Neural ﬁelds further
allow the analytical computation of derivatives. This enables the efﬁcient computation of sparse
depth maps, where prior representations of light ﬁelds require ﬁnite-differences approximations of
the gradient [48–50].
Rendering LFNs.
To render an image given an LFN, one computes the Plücker coordinates ru,v
of the camera rays at each u, v pixel coordinate in the image according to Equation 4. Speciﬁcally,
given the extrinsic E =

R|t

∈SE(3) and intrinsic K ∈R3×3 camera matrices [55] of a camera,
one may retrieve the Plücker coordinates of the ray ru,v at pixel coordinate u, v as:
ru,v = (du,v, t × du,v)/∥du,v∥, where du,v = RK−1  u
v
1

+ t,
(5)
where we use the world-to-camera convention for the extrinsic camera parameters. Rendering then
amounts to a single evaluation of the LFN Φ for each ray, cu,v = Φ(ru,v). For notational convenience,
we introduce a rendering function
ΘΦ
E,K : Rℓ→RH×W ×3
(6)
which renders an LFN Φφ with parameters φ ∈Rℓwhen viewed from a camera with extrinsic and
intrinsic parameters (E, K) into an image.
4

Figure 2: Given a 3D scene (left) and a light ray r (blue), we can slice the scene along a 2D plane
containing the ray (light blue), yielding a 2D scene (center). The light ﬁeld of all rays in a 2D plane,
the Epipolar Plane Image (EPI) c(s, t) (right). can be analytically computed from our 360-degree
LFN. The family of rays Lp (green) going through a point p on an object in the scene deﬁnes a
straight line in the EPI. See below for further discussion of EPI geometry.
4.2
The geometry of Light Field Networks
We will now analyze the properties of LFNs representing Lambertian 3D scenes, and illustrate
how the geometry of the underlying 3D scene is encoded. We will ﬁrst derive an expression that
establishes a relationship between LFNs and the classic two-plane parameterization of the light ﬁeld.
Subsequently, we will derive an expression for the depth of a ray in terms of the local color gradient
of the light ﬁeld, therefore allowing us to efﬁciently extract sparse depth maps from the light ﬁeld
at any camera pose via analytical differentiation of the neural implicit representation. Please see
Figure 2 for an overview.
Locally linear slices of the light ﬁeld.
We derive here a local parametrization that will allow us to
work with an LFN as if it were a conventional 2-plane light ﬁeld. Given a ray r in Plücker coordinates,
we pick two points x, x′ ∈R3 along this ray. We then ﬁnd a normalized direction d ∈S2 not
parallel to the ray direction - a canonical choice is a direction orthogonal to the ray direction. We
may now parameterize two parallel lines a(s) = x + sd and b(t) = x′ + td that give rise to a local
two-plane basis of the light ﬁeld with ray coordinates s and t. r intersects these lines at the two-plane
coordinates (s, t) = (0, 0). This choice of local basis now assigns the two-plane coordinates (s, t) to
the ray r from a(s) to b(t). In Figure 2, we illustrate this process on a simple 2D scene.
Epipolar Plane Images and their geometry.
The Plücker coordinates (see Eq. 4) enable us to
extract a 2D slice from an LFN ﬁeld by varying (s, t) and sampling Φ on the Plücker coordinates of
the rays parametrized pairs of points on the lines a(s) and b(t):
c(s, t) = Φ (r(s, t)) , where r(s, t) = −−−−−→
a(s)b(t) =
 b(t) −a(s)
∥b(t) −a(s)∥, a(s) × b(t)
∥b(t) −a(s)∥

.
(7)
The image of this 2D slice c(s, t) is well-known in the light ﬁeld literature as an Epipolar Plane
Image (EPI) [47]. EPIs carry rich information about the geometry of the underlying 3D scene.
For example, consider a point p on the surface of an object in the scene; please see Figure 2 for
a diagram. A point p ∈R2 has a 1-dimensional family of rays going through the point, which
correspond to a (green) line Lp in the EPI. In a Lambertian scene, all rays that meet in this point
and that are not occluded by other objects must observe the same color. Therefore, the light ﬁeld
is constant along this line. As one travels along Lp, rotating through the family of rays through p,
one eventually reaches a (magenta) tangent ray τ to the object. At a tangent ray, the value of the
EPI ceases to be constant, and the light ﬁeld changes its color to whatever is disoccluded by the
object at this tangent ray. Because objects of different depth undergo differing amounts of parallax,
EPIs
RGB
Gradients
Depths
the slope of the segment of Lp along which c
is constant determines the 3D coordinates of p.
Finally, by observing that we may extract EPIs
from any perspective, it is clear that an LFN
encodes the full 3D geometry of the underlying
scene. Intuitively, this may also be seen by con-
5

sidering that one could render out all possible perspectives of the underlying scene, and solve a classic
multi-view stereo problem to retrieve the shape.
Extracting depth maps from LFNs.
A correctly inferred light ﬁeld necessarily contains accurate
3D geometry information, although the geometry is encoded in a nontrivial way. To extract 3D
geometry from an LFN, we utilize the property of the 2-plane parameterization that the light ﬁeld is
constant on segments Lp, the slopes of which determine p. In the supplemental material, we derive
Proposition 1. For a Lambertian scene, the distance d along r = −−−−−→
a(s)b(t) from a(s) to the point p
on the object is
d(r) = D
∂tc(s, t)
∂sc(s, t) + ∂tc(s, t).
(8)
where a(s) and b(t) are as above, c(s, t) is deﬁned by (7), D is the distance between the lines a(t)
and b(t). Thus p = a(s) + d(r) b(t)−a(s)
∥b(t)−a(s)∥, and ∂x denotes the partial derivative by variable x.
This result yields meaningful depth estimates wherever the derivatives of the light ﬁelds are nonzero
along the ray. In practice, we sample several rays in a small (s, t) neighborhood of the ray r
and declare depth estimates as invalid if the gradients have high variance-please see the code for
implementation details. This occurs when r hits the object at a point where the surface color is
changing, or when r is a tangent ray. We note that there is a wealth of prior art that could be used to
extend this approach to extract dense depth maps [48–51].
4.3
Meta-learning with conditional Light Field Networks
We consider a dataset D consisting of N 3D scenes
Si = {(Ij, Ej, Kj)}K
j=1 ∈RH×W ×3 × SE(3) × R3×3, i = 1 . . . N
(9)
with K images Ij of each scene taken with cameras with extrinsic parameters Ej and intrinsic param-
eters Kj [55]. Each scene is completely described by the parameters φi ∈Rℓof its corresponding
light ﬁeld MLP Φi = Φφi.
Meta-learning and multi-view consistency.
In the case of 3D-structured neural scene representa-
tions, ray-marching or volumetric rendering naturally ensure multi-view consistency of the recon-
structed 3D scene representation. In contrast, a general 4D function Φ : L →R3 is not multi-view
consistent, as most such functions are not the light ﬁelds of any 3D scene. We propose to overcome
this challenge by learning a prior over the space of light ﬁelds. As we will demonstrate, this prior
can also be used to reconstruct an LFN from a single 2D image observation. In this paradigm,
differentiable ray-casting is a method to force the light ﬁeld of a scene to be multi-view consistent,
while we instead impose multi-view consistency by learning a prior over light ﬁelds.
Meta-learning framework.
We propose to represent each 3D scene Si by its own latent vector
zi ∈Rk. Generalizing to new scenes amounts to learning a prior over the space of light ﬁelds that is
concentrated on the manifold of multi-view consistent light ﬁelds of natural scenes. To represent this
latent manifold, we utilize a hypernetwork [56, 3]. The hypernetwork is a function, represented as an
MLP
Ψ : Rk →Rℓ, Ψψ(zi) = φi
(10)
with parameters ψ which sends the latent code zi of the i-th scene to the parameters of the corre-
sponding LFN.
Several reasonable approaches exist to obtain latent codes zi. One may leverage a convolutional-
or transformer-based image encoder, directly inferring the latent from an image [11, 5], or utilize
gradient-based meta-learning [23]. Here, we follow an auto-decoder framework [1, 3] to ﬁnd the
latent codes zi, but note that LFNs are in no way constrained to this approach. We do not claim
that this particular meta-learning method will out-perform other forms of conditioning, such as
gradient-based meta-learning [57, 23] or FILM conditioning [58], but perform a comparison to a
conditioning-by-concatenation approach in the appendix. We assume that the latent vectors have
6

Local Light Field
3D Light Field Slice
Depth
RGB
Depth
RGB
s
t
y
Novel Views
Novel Views
Figure 3: 360-degree light ﬁeld parameterization. Top left: 3D slice in the style of the Lumi-
graph [37] of an LFN of a room-scale scene. Bottom left: nearby views of a car as well as vertical
(right, red) and horizontal (top, green) Epipolar Plane Images, sampled from an LFN. Reconstructed
from training set, 50 and 15 views, respectively. Right: LFNs enable rendering from arbitrary,
360-degree camera perspectives as well as sparse depth map extraction from only a single sample per
ray. Please see the supplemental video for more qualitative results.
a Gaussian prior with zero mean and a diagonal covariance matrix. At training time, we jointly
optimize the latent parameters zi together with the hypernetwork parameters ψ using the objective
arg min
{zi},ψ
X
i
X
j
∥ΘΦ
Ej,Kj(Ψψ(zi)) −Ij∥2
2 + λlat∥zi∥2
2.
(11)
Here the ΘΦ is the rendering function (Equation 6), the ﬁrst term is an ℓ2 loss penalizing the light
ﬁelds that disagree with the observed images, and the second term enforces the prior over the latent
variables. We solve Equation 11 using gradient descent. At test time, we freeze the parameters of
the hypernetwork and reconstruct the light ﬁeld for a new scene S given a single observation of the
scene {(I, E, K)} by optimizing, using gradient descent, the latent variable zS of the scene, such
that the reconstructed light ﬁeld ΦΨψ(zS) best matches the given observation of the scene:
zS = arg min
z
∥ΘΦ
E,K (Ψψ(z)) −I)∥2
2 + λlat∥z∥2
2.
(12)
Global vs. local conditioning
The proposed meta-learning framework globally conditions an LFN
on a single latent variable z. Recent work instead leverages local conditioning, where a neural ﬁeld is
conditioned on local features extracted from a context image [26, 6, 27]. In particular, the recently
proposed pixelNeRF [6] has achieved impressive results on few-shot novel view synthesis. As we
will see, the current formulation of LFNs does not outperform pixelNeRF. We note, however, that
local conditioning methods solve a different problem. Rather than learning a prior over classes of
objects, local conditioning methods learn priors over patches, answering the question “How does
this image patch look like from a different perspective?”. As a result, this approach does not learn
a latent space of neural scene representations. Rather, scene context is required to be available at
test time to reason about the underlying 3D scene, and the representation is not compact: the size of
the conditioning grows with the number of context observations. In contrast, globally conditioned
methods [3, 11, 1, 2] ﬁrst infer a global representation that is invariant to the number of context views
and subsequently discard the observations. However, local conditioning enables better generalization
due to the shift-equivariance of convolutional neural networks. An equivalent to local conditioning in
light ﬁelds is non-obvious, and an exciting direction for future work.
7

Input
DVR
SRNs
Ours
GT
Input
DVR
SRNs
Ours
GT
Input
DVR
SRNs
Ours
GT
Figure 4: Category-agnostic single-shot reconstruction. We train a single model on the 13 largest
Shapenet classes. LFNs consistently outperform global conditioning baselines. Baseline results
courtesy of the pixelNeRF [6] authors. Please see the supplemental video for more qualitative results.
Table 1: Single-shot multi-class reconstruction results. We benchmark LFNs with SRNs [3] and
DVR [5] on the task of single-shot (auto-decoding with a single view), multi-class reconstruction
of the 13 largest ShapeNet [59] classes. LFNs signiﬁcantly outperform DVR and SRNs on almost
all classes, on average by more than 1dB, while only requiring a single network evaluation per ray.
Note that DVR requires additional supervision in the form of foreground-background masks. Means
across classes are weighted equally.
plane
bench
cbnt.
car
chair
disp.
lamp
spkr.
riﬂe
sofa
table
phone
boat
mean
↑PSNR
DVR [5]
25.29
22.64
24.47
23.95
19.91
20.86
23.27
20.78
23.44
23.35
21.53
24.18
25.09
22.70
SRN [3]
26.62
22.20
23.42
24.40
21.85
19.07
22.17
21.04
24.95
23.65
22.45
20.87
25.86
23.28
LFN
29.95
23.21
25.91
28.04
22.94
20.64
24.56
22.54
27.50
25.15
24.58
22.21
27.16
24.95
↑SSIM
DVR [5]
0.905
0.866
0.877
0.909
0.787
0.814
0.849
0.798
0.916
0.868
0.840
0.892
0.902
0.860
SRN [3]
0.901
0.837
0.831
0.897
0.814
0.744
0.801
0.779
0.913
0.851
0.828
0.811
0.898
0.849
LFN
0.932
0.855
0.871
0.943
0.835
0.786
0.844
0.808
0.939
0.874
0.868
0.844
0.907
0.870
5
Experiments
We demonstrate the efﬁcacy of LFNs by reconstructing 360-degree light ﬁelds of a variety of simple
3D scenes. In all experiments, we parameterize LFNs via a 6-layer ReLU MLP, and the hypernetwork
as a 3-layer ReLU MLP, both with layer normalization. We solve all optimization problems using the
ADAM solver with a step size of 10−4. Please ﬁnd more results, as well as precise hyperparameter,
implementation, and dataset details, in the supplemental document and video.
Reconstructing appearance and geometry of single-object and room-scale light ﬁelds.
We
demonstrate that LFN can parameterize 360-degree light ﬁelds of both single-object ShapeNet [59]
objects and simple, room-scale environments. We train LFNs on the ShapeNet “cars” dataset with 50
observations per object from [3], as well as on simple room-scale environments as proposed in [13].
Subsequently, we evaluate the ability of LFNs to generate novel views of the underlying 3D scenes.
Please see Figure 3 for qualitative results. LFNs succeed in parameterizing the 360-degree light
ﬁeld, enabling novel view synthesis at real-time frame-rates (see supplemental video). We further
demonstrate that LFNs encode scene geometry by presenting Epipolar Plane Images and leveraging
the relationship derived in Equation 8 to infer sparse depth maps. We highlight that both rendering
and depth map extraction do not require ray-casting, with only a single evaluation of the network or
the network and its gradient respectively.
Multi-class single-view reconstruction.
Following [5, 6], we benchmark LFNs with recent global
conditioning methods on the task of single-view reconstruction and novel view synthesis of the
13 largest ShapeNet categories. We follow the same evaluation protocol as [60] and train a single
model across all categories. See Figure 4 for qualitative and Table 1 for quantitative baseline compar-
isons. We signiﬁcantly outperform both Differentiable Volumetric Rendering (DVR) [5] and Scene
Representation Networks (SRNs) [3] on all but two classes by an average of 1dB, while requiring
more than an order of magnitude fewer network evaluations per ray. Qualitatively, we ﬁnd that the
reconstructions from LFNs are often crisper than those of either Scene Representation Networks or
DVR. Note that DVR requires additional ground-truth foreground-background segmentation masks.
Class-speciﬁc single-view reconstruction.
We benchmark LFNs on single-shot reconstruction on
the Shapenet “cars” and “chairs” classes as proposed in SRNs [3]. See Figure 5 for qualitative and
quantitative results. We report performance better than SRNs in PSRN and on par in terms of SSIM
8

Input
Ours
GT
SRNs
Input
Ours
GT
SRNs
Chairs
Cars
SRNs [3]
22.89 / 0.89
22.25 / 0.89
LFN
22.26 / 0.90
22.42 / 0.89
Figure 5: Class-speciﬁc single-shot reconstruction. LFN performs approximately on par with
SRNs [3] in the single-class single-shot reconstruction case, while requiring an order of magnitude
fewer network evaluations, memory, and rendering time. Quantitative results report (PSNR, SSIM).
Input
Ours
GT
pixelNeRF
Input
Ours
GT
pixelNeRF
Class-speciﬁc
Multi-Class
LFN
22.34 / 0.90
24.95 / 0.87
pixelNeRF
23.45 / 0.91
26.80 / 0.91
Figure 6: Global vs. local conditioning. The locally conditioned pixelNeRF outperforms LFNs in
single-shot reconstruction, though LFNs require three orders of magnitude less rendering time and
memory. Qualitatively, for many objects LFNs are on par with pixelNERF (left), but confuse the
object class on others (right). Quantitative results report (PSNR, SSIM). ‘Class-speciﬁc’ refers to
average over cars and chairs as in Fig. 5 while ’multi-class’ refers to the average over all ShapeNet
classes as in Fig. 1.
on the “cars” class, and worse in PSNR but better in terms of SSIM on the “chairs” class, while
requiring an order of magnitude fewer network evaluations and rendering in real-time. We attribute
the drop in performance compared to multi-class reconstruction to the smaller dataset size, causing
multi-view inconsistency.
Global vs. local conditioning and comparison to pixelNeRF [6].
We investigate the role of
global conditioning, where a single latent is inferred to describe the whole scene [3], to local condi-
tioning, where latents are inferred per-pixel in a 2D image and leveraged to locally condition a neural
implicit representation [26, 27, 6]. We benchmark with the recently proposed pixelNeRF [6]. As
noted above (see Section 4.3), local conditioning does not infer a compact neural scene representation
of the scene. Nevertheless, we provide the comparison here for completeness. See Figure 6 for
qualitative and quantitative results. On average, LFNs perform 1dB worse than pixelNeRF in the
single-class case, and 2dB worse in the multi-class setting.
Real-time rendering and storage cost.
See Table 2 for a quantitative comparison of the rendering
complexity of LFN compared with that of volumetric and ray-marching based neural renderers [3, 45,
19, 4, 6]. All clock times were collected for rendering 256 × 256 images on an NVIDIA RTX 6000
GPU. We further compare the cost of storing a single LFN with the cost of storing a conventional
light ﬁeld. With approximately 400k parameters, a single LFN requires around 1.6 MB of storage,
compared to 146 MB required for storing a 360-degree light ﬁeld at a resolution of 256×256×17×17
in the six-plane Lumigraph conﬁguration.
Multi-view consistency as a function of training set size.
We investigate how multi-view con-
sistency scales with the amount of data that the prior is trained on. Please ﬁnd this analysis in the
supplementary material.
Table 2: Comparison of rendering complexity. LFNs require three orders of magnitude less
compute than volumetric rendering based approaches, and admit real-time rendering. Please see
Table of supplemental material for comparison on larger images based on data in []
LFNs
SRNs [3]
pixelNeRF [6]
# evaluations per ray
1
11
192
clock time for 256 × 256 image (ms)
2.1
120
30e3
9

Method
Cars
Chairs
SRNs
0.071
0.20
LFN
0.059
0.130
Figure 7: Geometry reconstruction with LFNs. By backprojecting extracted depth into 3D, LFNs
enable reconstruction of 3D pointclouds (left). For rays were depth is valid, LFNs achieve lower
mean L1 depth error than Scene Representation Networks (right).
Overﬁtting of single 3D scenes.
We investigate overﬁtting a single 3D scene with a Light Field
Network with positional encodings / sinusoidal activations [24, 61]. Please ﬁnd this analysis in the
supplementary material.
Evaluation of Reconstructed Geometry.
We investigate the quality of the geometry that can be
computed from an LFN via Eq. 8. For every sample in the class-speciﬁc single-shot reconstruction
experiment, we extract its per-view sparse depth map. We then backproject depth maps from four
views into 3D to reconstruct a point cloud, and benchmark mean L1 error on valid depth estimates with
Scene Representation Networks [3]. Fig. 7 displays qualitative and quantitative results. Qualitatively,
point clouds succeed in capturing ﬁne detail such as the armrests of chairs. Quantitatively, LFNs
outperform SRNs on both cars and chairs. We note that LFNs have a slight advantage in this
comparison, as we can only benchmark on the sparse depth values, for which LFNs have high
conﬁdence. This includes occlusion boundaries, which are areas where the sphere-tracing based
SRNs incurs high error, as it is forced to take smaller and smaller steps and may not reach the surface.
We highlight that we do not claim that the proposed method is competetive with methods designed for
geometry reconstruction in particular, but that we only report this to demonstrate that the proposed
method is capable to extract valid depth estimates from an LFN.
Limitations.
First, as every existing light ﬁeld approach,
LFNs store only one color per oriented ray, which makes
rendering views from cameras placed in between occlud-
ing objects challenging, even if the information may still be stored in the light ﬁeld. Second, though
we outperform globally-conditioned methods, we currently do not outperform the locally condi-
tioned pixelNeRF. Finally, as opposed to 3D-structured representations, LFNs do not enforce strict
multi-view consistency, and may be inconsistent in the case of small datasets.
6
Discussion and Conclusion
We have proposed Light Field Networks, a novel neural scene representation that directly parameter-
izes the full 360-degree, 4D light ﬁeld of a 3D scene via a neural implicit representation. This enables
both real-time neural rendering with a single evaluation of the neural scene representation per ray, as
well as sparse depth map extraction without ray-casting. Light Field Networks outperform globally
conditioned baselines in single-shot novel view synthesis, while being three orders of magnitude
faster and less memory-intensive than current volumetric rendering approaches. Exciting avenues
for future work include combining LFNs with local conditioning, which would enable stronger
out-of-distribution generalization, studying the learning of non-Lambertian scenes, and enabling
camera placement in obstructed 3D space. With this work, we make important contributions to
the emerging ﬁelds of neural rendering and neural scene representations, with exciting applications
across computer vision, computer graphics, and robotics.
Societal Impacts.
Potential improvements extending our work on few-observation novel view
synthesis could enable abuse by decreasing the cost of non-consensual impersonations. We refer the
reader to a recent review of neural rendering [22] for an in-depth discussion of this topic.
Acknowledgements and Disclosure of Funding
This work is supported by the NSF under Cooperative Agreement PHY-2019786 (The NSF AI Institute
for Artiﬁcial Intelligence and Fundamental Interactions, http://iaifi.org/), ONR under 1015
10

G TA243/N00014-16-1-2007 (Understanding Scenes and Events through Joint Parsing, Cognitive
Reasoning and Lifelong Learning), Mitsubishi under 026455-00001 (Building World Models from
some data through analysis by synthesis), DARPA under CW3031624 (Transfer, Augmentation and
Automatic Learning with Less Labels), as well as the Singapore DSTA under DST00OECI20300823
(New Representations for Vision). We thank Andrea Tagliasacchi, Tomasz Malisiewicz, Prafull
Sharma, Ludwig Schubert, Kevin Smith, Bernhard Egger, Christian Richardt, Manuel Rey Area,
and Jürgen and Susanne Sitzmann for interesting discussions and feedback, and Alex Yu for kindly
sharing the outputs of pixelNeRF and baselines with us.
References
[1] Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, and Steven Lovegrove. Deepsdf:
Learning continuous signed distance functions for shape representation. In Proc. CVPR, 2019.
[2] Lars Mescheder, Michael Oechsle, Michael Niemeyer, Sebastian Nowozin, and Andreas Geiger. Occupancy
networks: Learning 3d reconstruction in function space. In Proc. CVPR, 2019.
[3] Vincent Sitzmann, Michael Zollhöfer, and Gordon Wetzstein. Scene representation networks: Continuous
3d-structure-aware neural scene representations. In Proc. NeurIPS 2019, 2019.
[4] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng.
Nerf: Representing scenes as neural radiance ﬁelds for view synthesis. In Proc. ECCV, 2020.
[5] Michael Niemeyer, Lars Mescheder, Michael Oechsle, and Andreas Geiger. Differentiable volumetric
rendering: Learning implicit 3d representations without 3d supervision. In Proc. CVPR, 2020.
[6] Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa. pixelnerf: Neural radiance ﬁelds from one or
few images. Proc. CVPR, 2020.
[7] Ben Mildenhall, Pratul P Srinivasan, Rodrigo Ortiz-Cayon, Nima Khademi Kalantari, Ravi Ramamoorthi,
Ren Ng, and Abhishek Kar. Local light ﬁeld fusion: Practical view synthesis with prescriptive sampling
guidelines. ACM Transactions on Graphics (TOG), 38(4):1–14, 2019.
[8] Shuaifeng Zhi, Tristan Laidlow, Stefan Leutenegger, and Andrew J Davison. In-place scene labelling and
understanding with implicit scene representation. arXiv preprint arXiv:2103.15875, 2021.
[9] A. Kohli, V. Sitzmann, and G. Wetzstein. Semantic Implicit Neural Scene Representations with Semi-
supervised Training. In Proc. 3DV, 2020.
[10] Edgar Sucar, Shikun Liu, Joseph Ortiz, and Andrew J Davison. imap: Implicit mapping and positioning in
real-time. arXiv preprint arXiv:2103.12352, 2021.
[11] Adam R Kosiorek, Heiko Strathmann, Daniel Zoran, Pol Moreno, Rosalia Schneider, Soˇna Mokrá,
and Danilo J Rezende.
Nerf-vae: A geometry aware 3d scene generative model.
arXiv preprint
arXiv:2104.00587, 2021.
[12] Daniel E Worrall, Stephan J Garbin, Daniyar Turmukhambetov, and Gabriel J Brostow. Interpretable
transformations with encoder-decoder networks. In Proc. ICCV, volume 4, 2017.
[13] SM Ali Eslami, Danilo Jimenez Rezende, Frederic Besse, Fabio Viola, Ari S Morcos, Marta Garnelo,
Avraham Ruderman, Andrei A Rusu, Ivo Danihelka, Karol Gregor, et al. Neural scene representation and
rendering. Science, 360(6394):1204–1210, 2018.
[14] Vincent Sitzmann, Justus Thies, Felix Heide, Matthias Nießner, Gordon Wetzstein, and Michael Zollhöfer.
Deepvoxels: Learning persistent 3d feature embeddings. In Proc. CVPR, 2019.
[15] Hsiao-Yu Fish Tung, Ricson Cheng, and Katerina Fragkiadaki. Learning spatial common sense with
geometry-aware recurrent networks. Proc. CVPR, 2019.
[16] Thu H Nguyen-Phuoc, Chuan Li, Stephen Balaban, and Yongliang Yang. Rendernet: A deep convolutional
network for differentiable rendering from 3d shapes. In Proc. NIPS. 2018.
[17] Jun-Yan Zhu, Zhoutong Zhang, Chengkai Zhang, Jiajun Wu, Antonio Torralba, Josh Tenenbaum, and Bill
Freeman. Visual object networks: image generation with disentangled 3d representations. In Proc. NIPS,
pages 118–129, 2018.
[18] Thu Nguyen-Phuoc, Chuan Li, Lucas Theis, Christian Richardt, and Yong-Liang Yang. Hologan: Unsuper-
vised learning of 3d representations from natural images. In Proc. ICCV, 2019.
[19] Stephen Lombardi, Tomas Simon, Jason Saragih, Gabriel Schwartz, Andreas Lehrmann, and Yaser Sheikh.
Neural volumes: Learning dynamic renderable volumes from images. arXiv preprint arXiv:1906.07751,
2019.
[20] Danilo Jimenez Rezende, S. M. Ali Eslami, Shakir Mohamed, Peter Battaglia, Max Jaderberg, and Nicolas
Heess. Unsupervised learning of 3d structure from images. In Proc. NIPS. 2016.
11

[21] Yiheng Xie, Towaki Takikawa, Shunsuke Saito, Or Litany, Shiqin Yan, Numair Khan, Federico Tombari,
James Tompkin, Vincent Sitzmann, and Srinath Sridhar. Neural ﬁelds in visual computing and beyond. In
arXiv, 2021.
[22] Ayush Tewari, Ohad Fried, Justus Thies, Vincent Sitzmann, Stephen Lombardi, Kalyan Sunkavalli, Ricardo
Martin-Brualla, Tomas Simon, Jason Saragih, Matthias Nießner, et al. State of the art on neural rendering.
In Computer Graphics Forum, volume 39, pages 701–727. Wiley Online Library, 2020.
[23] Vincent Sitzmann, Eric R Chan, Richard Tucker, Noah Snavely, and Gordon Wetzstein. Metasdf: Meta-
learning signed distance functions. Proc. NeurIPS, 2020.
[24] Vincent Sitzmann, Julien N.P. Martel, Alexander W. Bergman, David B. Lindell, and Gordon Wetzstein.
Implicit neural representations with periodic activation functions. In Proc. NeurIPS, 2020.
[25] Konstantinos Rematas, Ricardo Martin-Brualla, and Vittorio Ferrari. Sharf: Shape-conditioned radiance
ﬁelds from a single view. In Proc. ICML, 2021.
[26] Shunsuke Saito, Zeng Huang, Ryota Natsume, Shigeo Morishima, Angjoo Kanazawa, and Hao Li. Pifu:
Pixel-aligned implicit function for high-resolution clothed human digitization. In Proc. ICCV, pages
2304–2314, 2019.
[27] Alex Trevithick and Bo Yang. Grf: Learning a general radiance ﬁeld for 3d scene representation and
rendering. arXiv preprint arXiv:2010.04595, 2020.
[28] Christian Reiser, Songyou Peng, Yiyi Liao, and Andreas Geiger. Kilonerf: Speeding up neural radiance
ﬁelds with thousands of tiny mlps. arXiv preprint arXiv:2103.13744, 2021.
[29] Alex Yu, Ruilong Li, Matthew Tancik, Hao Li, Ren Ng, and Angjoo Kanazawa. Plenoctrees for real-time
rendering of neural radiance ﬁelds. arXiv preprint arXiv:2103.14024, 2021.
[30] Lingjie Liu, Jiatao Gu, Kyaw Zaw Lin, Tat-Seng Chua, and Christian Theobalt. Neural sparse voxel ﬁelds.
Proc. NeurIPS, 2020.
[31] David B Lindell, Julien NP Martel, and Gordon Wetzstein. Autoint: Automatic integration for fast neural
volume rendering. Proc. CVPR, 2020.
[32] Thomas Neff, Pascal Stadlbauer, Mathias Parger, Andreas Kurz, Chakravarty R Alla Chaitanya, Anton
Kaplanyan, and Markus Steinberger. Donerf: Towards real-time rendering of neural radiance ﬁelds using
depth oracle networks. arXiv e-prints, pages arXiv–2103, 2021.
[33] Petr Kellnhofer, Lars Jebe, Andrew Jones, Ryan Spicer, Kari Pulli, and Gordon Wetzstein. Neural
lumigraph rendering. Proc. CVPR, 2021.
[34] Edward H Adelson, James R Bergen, et al. The plenoptic function and the elements of early vision,
volume 2. Vision and Modeling Group, Media Laboratory, Massachusetts Institute of Technology, 1991.
[35] Edward H Adelson and John YA Wang. Single lens stereo with a plenoptic camera. IEEE transactions on
pattern analysis and machine intelligence, 14(2):99–106, 1992.
[36] Marc Levoy and Pat Hanrahan. Light ﬁeld rendering. In Proc. SIGGRAPH, 1996.
[37] Steven J Gortler, Radek Grzeszczuk, Richard Szeliski, and Michael F Cohen. The lumigraph. In Proc.
SIGGRAPH, 1996.
[38] Chris Buehler, Michael Bosse, Leonard McMillan, Steven Gortler, and Michael Cohen. Unstructured
lumigraph rendering. In Proceedings of the 28th annual conference on Computer graphics and interactive
techniques, pages 425–432, 2001.
[39] Ren Ng et al. Digital light ﬁeld photography, volume 7. stanford university Stanford, 2006.
[40] Suren Vagharshakyan, Robert Bregovic, and Atanas Gotchev. Light ﬁeld reconstruction using shearlet
transform. Proc. PAMI, 40(1):133–147, 2017.
[41] Lixin Shi, Haitham Hassanieh, Abe Davis, Dina Katabi, and Fredo Durand. Light ﬁeld reconstruction
using sparsity in the continuous fourier domain. ACM Transactions on Graphics (TOG), 34(1):1–13, 2014.
[42] Anat Levin and Fredo Durand. Linear view synthesis using a dimensionality gap light ﬁeld prior. In Proc.
CVPR, pages 1831–1838. IEEE, 2010.
[43] Nima Khademi Kalantari, Ting-Chun Wang, and Ravi Ramamoorthi. Learning-based view synthesis for
light ﬁeld cameras. ACM Trans. Graph. (SIGGRAPH Asia), 35(6):193, 2016.
[44] Mojtaba Bemana, Karol Myszkowski, Hans-Peter Seidel, and Tobias Ritschel. X-ﬁelds: Implicit neural
view-, light- and time-image interpolation. Proc. SIGGRAPH Asia 2020, 39(6), 2020.
[45] Lior Yariv, Yoni Kasten, Dror Moran, Meirav Galun, Matan Atzmon, Basri Ronen, and Yaron Lipman.
Multiview neural surface reconstruction by disentangling geometry and appearance. Proc. NeurIPS, 2020.
[46] Qiangeng Xu, Weiyue Wang, Duygu Ceylan, Radomír Mech, and Ulrich Neumann. DISN: deep implicit
surface network for high-quality single-view 3d reconstruction. In Proc. NIPS, 2019.
12

[47] Robert C Bolles, H Harlyn Baker, and David H Marimont. Epipolar-plane image analysis: An approach to
determining structure from motion. Proc. IJCV, 1(1):7–55, 1987.
[48] Don Dansereau and Len Bruton. Gradient-based depth estimation from 4d light ﬁelds. In Proc. IEEE
International Symposium on Circuits and Systems, volume 3, pages III–549. IEEE, 2004.
[49] Ivana Tosic and Kathrin Berkner. Light ﬁeld scale-depth space transform for dense depth estimation.
In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pages
435–442, 2014.
[50] Changil Kim, Henning Zimmer, Yael Pritch, Alexander Sorkine-Hornung, and Markus H Gross. Scene
reconstruction from high spatio-angular resolution light ﬁelds. ACM Transactions on Graphics (TOG),
32(4):73–1, 2013.
[51] Lipeng Si and Qing Wang. Dense depth-map estimation and geometry inference from light ﬁelds via global
optimization. In Asian Conference on Computer Vision, pages 83–98. Springer, 2016.
[52] Yan-Bin Jia.
Plücker coordinates for lines in the space.
Problem Solver Techniques
for
Applied
Computer
Science,
Com-S-477/577
Course
Handout.
Iowa
State
University,
https://faculty.sites.iastate.edu/jia/ﬁles/inline-ﬁles/plucker-coordinates.pdf, 2020.
[53] Bernd Krolla, Maximilian Diebold, Bastian Goldlücke, and Didier Stricker. Spherical light ﬁelds. In Proc.
BMVC, 2014.
[54] Insung Ihm, Sanghoon Park, and Rae Kyoung Lee. Rendering of spherical light ﬁelds. In Proceedings The
Fifth Paciﬁc Conference on Computer Graphics and Applications, pages 59–68, 1997.
[55] Richard Hartley and Andrew Zisserman. Multiple View Geometry in Computer Vision. Cambridge
University Press, 2nd edition, 2003.
[56] David Ha, Andrew Dai, and Quoc V Le. Hypernetworks. In Proc. ICLR, 2017.
[57] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of
deep networks. In Proceedings of the 34th International Conference on Machine Learning - Volume 70,
ICML’17, page 1126–1135. JMLR.org, 2017.
[58] Ethan Perez, Florian Strub, Harm de Vries, Vincent Dumoulin, and Aaron C. Courville. Film: Visual
reasoning with a general conditioning layer. In AAAI, 2018.
[59] Angel X Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio
Savarese, Manolis Savva, Shuran Song, Hao Su, et al. Shapenet: An information-rich 3d model repository.
arXiv preprint arXiv:1512.03012, 2015.
[60] Hiroharu Kato, Yoshitaka Ushiku, and Tatsuya Harada. Neural 3d mesh renderer. In Proc. CVPR, pages
3907–3916, 2018.
[61] Matthew Tancik, Pratul P Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Raghavan, Utkarsh
Singhal, Ravi Ramamoorthi, Jonathan T Barron, and Ren Ng. Fourier features let networks learn high
frequency functions in low dimensional domains. In Proc. NeurIPS, 2020.
13

Light Field Networks: Neural Scene Representations
with Single-Evaluation Rendering
–Supplementary Material–
Vincent Sitzmann1,∗
sitzmann@mit.edu
Semon Rezchikov2,∗
skr@math.columbia.edu
William T. Freeman1,3
billf@mit.edu
Joshua B. Tenenbaum1,4,5
jbt@mit.edu
Frédo Durand1
fredo@mit.edu
1MIT CSAIL
2Columbia University
3 IAFI
4MIT BCS
5 CBMM
vsitzmann.github.io/lfns/
Contents
1
Additional results on the local two-plane parameterization
2
2
Reproducibility
3
2.1
Hardware . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3
2.2
Architecture Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3
2.3
360-degree light ﬁeld reconstruction experiments (Figure 5)
. . . . . . . . . . . .
3
2.4
Multi-class single shot reconstruction
. . . . . . . . . . . . . . . . . . . . . . . .
3
2.5
Single-class single shot reconstruction . . . . . . . . . . . . . . . . . . . . . . . .
4
3
Additional Results
4
4
References
6
∗These authors contributed equally to this work.
35th Conference on Neural Information Processing Systems (NeurIPS 2021), Sydney, Australia.
14

1
Additional results on the local two-plane parameterization
In this section, we derive Proposition 1 of the paper. We recall here that the Plucker coordinates
parameterize the space L of oriented lines in R3 as the set of tuples
L = {(d, w) | d, w ∈R3; d · w = 0, ∥d∥= 1}.
(1)
The line through x in direction d (with ∥d∥= 1), i.e. the line x, x + d, has normalized Plücker
coordinates
(d, x × (x + d)) = (d, x × d).
If the direction vector d is not normalized, we can still compute Plücker coordinates as above; to
normalize the coordinates we must apply the normalization operator
N(d, w) = (d, w)/∥d∥.
(2)
Thus the ray −−−−−→
a(s)b(t) has (unnnormalized) Plücker coordinates
v(s, t) = (b −a, a × b).
(3)
Proof of Proposition 1. Given two coplanar lines
a(s) = x + sd and b(t) = x′ + td
(4)
we write
c(s, t) = Φ
 b −a
∥b −a∥, a × b
∥b −a∥

= Φ(N(v(s, t))
(5)
for the color of the ray ℓ= −−−−−→
a(s)b(t) described by the LFN Φ. Suppose ℓcaptures the color of a point
p in a 3D scene. For a Lambertian scene, if the ray is not a tangent ray, c(s, t) should be constant
near ℓalong the line in the (s, t) plane corresponding to the family of rays through p. Therefore, the
gradient ∇s,tc(s, t) is orthogonal to this line in the (s, t) plane.
Write J for the matrix of rotation by 90 degrees, i.e. [0, −1; 1, 0]. Thus at non-tangent lines,
J∇s,tc(s, t) will point along the family of rays through p.
We write D for the distance between the lines a and b; so D = ∥x′ −x∥for a, b as in (4). Let a0, b0
be the closest points to p on a(s) and b(t), respectively. Write a1 = a(s) and b1 = b(t) for the
intersection of ℓwith a, b; then a nearby ray on the pencil of rays through p is given by ℓ′ = −−→
a′
1b′
1
for some a′
1 = a(s −δs), b′
1 = b(t + δt). We have similar triangles △pa0a1, △pa0a′
1, △pb0b1,
and △pb0b′
1. Since the (s, t) coordinates of ℓ′ differ from the (s, t) coordinates of ℓby a multiple
of J∇s,tc(s, t), the similarity relationships between the triangles above imply that
δs
δt = −(J∇c)s
(J∇c)t
=
d
D −d.
This simpliﬁes to
∂tc
∂sc =
d
D −d.
Rearranging, we have
d = D
∂tc
∂sc + ∂tc.
(6)
Let δ be the distance from a(s) to p; then δ =
√
s2 + d2.
Let’s now describe how to get an estimate of a point generating the color of a ray in Plücker
coordinates. Say we have (normalized) Plücker coordinates (d, w). Write x = d × w; this is the
closest point on the line ℓ(d,w) to the origin. Now choose auxiliary d′ with ∥d∥= 1. (Ideally d is
215

not close to d′.) Write x′ = x + Dd for some positive number D. Then we consider the two-plane
parametrization through
a(s) = x + sd′, b(t) = x′ + td′
(7)
Deine c via (5) where we deﬁne v as in (3) using a, b as in (7). The line −−−−−→
x, x + d = ℓ(d,w) intersects
a(s) at x = a(0) and intersects b(t) at x′ = x + Dd = b(0). Compute ∂sc, ∂tc — a procedure that
involves computing analytical derivatives of the neural network Φ — then compute d(s, t) via (6),
and then
p = x + δ(0, 0)d.
2
Reproducibility
In the following, we provide all the details necessary to reproduce the experiments outlined in the
paper. All code and datasets will be made publicly available.
2.1
Hardware
Each model was separately trained on a single NVIDIA RTX 6000 GPU with 24 GB of memory.
Overall, we used up to 4 GPUs in parallel.
2.2
Architecture Details
All LFNs are implemented as six-layer fully connected neural networks with ReLU nonlinearities
and 256 hidden units per layer. Before each layer, we leverage layer normalization without afﬁne
transforms, i.e., no additional parameters are introduced. All hypernetworks are implemented as
three-layer fully connected neural networks with ReLU nonlinearities and layer normalization with
afﬁne transform. The hidden layer size of the hypernetworks is 256. Thus, the parameter ℓin Equation
10 of the main text is ∼400, 000. The size of latent codes z is 256.
2.3
360-degree light ﬁeld reconstruction experiments (Figure 5)
Cars dataset.
We use the dataset proposed by Sitzmann et al. [9],
hosted by the
authors
of
pixelNeRF
[11]
under
https://drive.google.com/drive/folders/
1PsT3uKwqHHD2bEEHkIXB99AlIjtmrEiR.
Rooms dataset.
Using the assets provided by the authors of GQN [2], hosted under
https://github.com/deepmind/lab/tree/master/assets/textures/map/lab_games
and a modiﬁed version of the Blender Shapenet rendering script hosted under https://github.
com/vsitzmann/shapenet_renderer/blob/master/shapenet_spherical_renderer.py,
we rendered 10, 000 rooms. The rooms have a sidelength of 7. Cameras are placed exclusively in the
center 2 × 2 square of the room. Objects are placed exclusively in the outer 1.5 perimeter of the
room, such that the camera is only placed in unobstructed free space. Colors and types of objects are
chosen at random. Every room features between 1 and 5 objects. We render 30 observations per
room.
Experiment Details.
We train separate models for the “cars” and “rooms” classes. We train in two
resolution stages. First, at a resolution of 64 × 64 and a batch size of 300, then, at a batch size of 75
at a resolution of 128 × 128 until convergence for a total of approx. 3 days, both using the ADAM
optimizer with a step size of 10−4. We choose the parameter λlat as 1e2. At test time, we freeze
the network parameters, initialize all latent codes to the prior mean (i.e., all zeros), and optimize the
latent codes until convergence. Hyperparameters were discovered via unstructured search.
2.4
Multi-class single shot reconstruction
Dataset.
We use the dataset proposed by Kato et al. [3], available for download un-
der
https://s3.eu-central-1.amazonaws.com/avg-projects/differentiable_
316

volumetric_rendering/data/NMR_Dataset.zip (hosted by the authors of Differentiable
Volumetric Rendering [5]).
Experiment Details.
We train a single model on all 13 classes. We train until convergence (approx.
two days) at a resolution of 64 × 64 and a batch size of 300 until convergence for a total of approx. 3
days using the ADAM optimizer with a step size of 10−4. We choose the parameter λlat as 1e2. At
test time, we freeze the network parameters, initialize all latent codes to the prior mean (i.e., all zeros),
and optimize the latent codes until convergence. Leveraging an aggressive learning rate schedule,
auto-decoding converges within 200 iterations or approximately 1 second for a batch of 300 shapes.
Hyperparameters were discovered via unstructured search.
2.5
Single-class single shot reconstruction
Dataset.
We
use
the
dataset
proposed
by
Sitzmann
et
al.
[9],
hosted
by
the
au-
thors
of
pixelNeRF
[11]
under
https://drive.google.com/drive/folders/
1PsT3uKwqHHD2bEEHkIXB99AlIjtmrEiR.
Experiment Details.
We train separate models for the “cars” and “chairs” classes. We train in
two resolution stages. First, at a resolution of 64 × 64 and a batch size of 300 for 200k steps, then,
at a batch size of 75 at a resolution of 128 × 128 until convergence (total approx. 3 days), both
using the ADAM optimizer with a step size of 10−4. We choose the parameter λlat as 1e2. At
test time, we freeze the network parameters, initialize all latent codes to the prior mean (i.e., all
zeros), and optimize the latent codes until convergence, as in the multi-class single-shot experiment.
Hyperparameters were discovered via unstructured search.
3
Additional Results
In ﬁg. 1 we show novel views of training set objects together with extracted epipolar plane images
and depth maps. Please see the supplemental video for extensive qualitative results.
In Table 1 we show additional results comparing rendering complexity of LFNs to other existing
methods, for larger, 800 × 800 pixel images.
Comparison to conditioning-by-concatenation.
We performed an ablation study comparing hy-
pernetworks to the simpler alternative of conditioning via concatenation. We parameterized the
LFN identically to the MLPs in pixelNeRF and DVR: A 5-layer, fully-connected residual MLP
with ReLU activations and 512 hidden units. The latent code was directly concatenated to the input
ray coordinate. The remaining experimental settings were identical to those in Section 2.4 above.
Both methods were trained for approximately 3 days on a single RTX 6000 GPU. In this setting,
conditioning via concatenation yields signiﬁcantly worse performance, as shown in Table 2. This is
in line with the insight from Pi-GAN [1], where FILM-conditioning [6], which is an intermediate
between a hypernetwork and conditioning via concatenation, was found to perform signiﬁcantly better
than conditioning via concatenation. Similarly, Facebook AI has found the hypernetwork-conditioned
SRN to outperform concatenation-conditioned DVR and NeRF alternatives (see minute 16:30 of
video associated to [7]). We further note that conditioning via concatenation is a special case of
a hypernetwork, namely a single linear layer outputting the biases of the ﬁrst LFN layer - see for
instance appendix of [8] for a proof.
We note that we do not claim that the proposed method of conditioning is superior to any other
method of conditioning. Rather, the proposed LFN framework is compatible with any conditioning
method. We merely argue that to learn multi-view consistent light ﬁelds, we require some form of
meta-learning, and see the analysis of the optimal conditioning method as an avenue for future work.
Ablating overﬁtting vs. generalization.
To validate our claim that meta-learning on a dataset of
3D shapes enables consistent novel view synthesis on test objects by learning a multi-view consistency
prior in light-ﬁelds, we benchmarked PSNR of novel views on 10 objects in the 10 set, reconstructed
from a single observation using models trained on 1, 10, 100, 1k, and up to 2500 shapes, both in the
single-class and multi-class settings. Experimental settings are as in Sections 2.4 and 2.5. Please
417

see Table 3 for results. The trend in PSNR is consistent improvement, suggesting that additional
training-set objects would likely improve performance further.
We further note that meta-learning across a dataset of 3D scenes serves two distinct roles: learning a
space of multi-view consistent light ﬁelds, and learning a prior over 3D scenes to enable few-shot
reconstruction. To better disentangle these two roles, we ran a second experiment. For each instance
in the training set, which was drawn from the ShapeNet chair class, we randomly picked 5 views,
such that on average, almost every surface point of each chair has been observed, but not every
oriented ray. I.e., while the 3D surfaces of the chairs are densely sampled, their light ﬁelds are not.
We trained LFNs on 10, 100, 1000, and all (4.5k) chair instances, including the 10 chairs previously
mentioned, and evaluated view synthesis performance in PSNR on a held-out set of novel views for
each of these 10 chairs. This tests only the ﬁrst property of the meta-learning approach - i.e., how the
meta-learning learns multi-view consistency as an abstract property - without entangling it with the
capability to perform few-shot reconstruction. Experimental parameters were as in 2.5. Please see
Table 4 for results. The performance similarly improves logarithmically.
EPIs
Rendered Image
Sparse Depth Map
Rendered Image
Sparse Depth Map
EPIs
Figure 1: Novel views of cars and rooms with Epipolar Plane Images and depth maps.
Table 1: Comparison of rendering complexity, continued. Data for all architectures aside from
LFN taken from [7]; see 17:00 of associated video.
LFNs
SRNs [9]
DVR [5]
IDR [10]
NeRF [4]
clock time for 800 × 800 image (ms)
20.5
1e3
9e4
1e5
2e4
Table 2: Comparison of meta-learning approach. We compared conditioning via a hypernetwork,
used in the paper, to conditining-by-concatenation (CvC below) in the 13-class ShapeNet dataset. All
results are PSNR in dB.
Method
plane
bench
cbnt.
car
chair
disp.
lamp
spkr.
riﬂe
sofa
table
phone
boat
mean
Hypernetwork
29.95
23.21
25.91
28.04
22.94
20.64
24.56
22.54
27.50
25.15
24.58
22.21
27.16
24.95
CvC
24.08
19.43
21.01
23.57
19.81
17.47
19.05
19.40
22.24
20.85
19.81
17.64
24.02
20.64
Overﬁtting a densely sampled 3D scene.
We demonstrate that LFNs can in principle represent
high-frequency information. We ﬁt an LFN to all the views of the “Fern” scene from the NeRF [4]
dataset, which consists of photos of a real-world scene captured with a DSLR camera. We add
positional encodings to the plucker embedding, following NeRF. Please see qualitative results in
Fig 2, as well as the supplemental video. LFNs succeed in reproducing all context images perfectly,
proving that in principle, an LFN is capable of parameterizing realistic, 4d high-frequency content.
As expected, rendering out the intermediate views leads to basically random images, please see this
video, as there is no mechanism to enforce multi-view consistency, in contrast to the experiments
in the paper, where this prior was learnt using meta-learning. We note that fundamentally, there are
518

Table 3: Multi-view consistency vs. training set size. As we scale the size of the training set, the
multi-view consistency prior improves consistently, both in the single-class and multi-class setting.
All results are PSNR in dB. Top row denotes number of object instances in the traninig set. The cars
dataset contains approximately 2500 objects, while the NMR dataset contains approximately 2000
objects per class.
Dataset
1
10
100
1k
all
Cars (Single-class)
16.21
18.98
20.76
23.09
23.56
NMR (Multi-Class)
18.03
19.77
20.82
23.87
24.95
Table 4: Disentangling multi-view consistency vs. object prior. Columns indicate number of
instances of chair ShapeNet class. Each LFN was trained on 5 random views of each instance and
evaluated on 10 novel views of a ﬁxed subset of 10 training set chairs.
10
100
1k
all
PSNR (dB) on novel views
15.88
16.83
19.12
22.15
Figure 2: Overﬁtting a single 3D scene. We overﬁt an LFN on the context views of the “Fern” scene
from NeRF [4]. Context views are reproduced almost perfectly, demonstrating that an LFN can in
principle ﬁt high-frequency detail (left). Rendering out intermediate views with unobserved rays fails,
as no multi-view consistency prior is enforced (right).
two different regimes of interest. (1) The overﬁtting regime, where a neural scene representation
is ﬁt to a 3D scene that is completely observed, i.e., every surface point is observed enough times
to enable triangulation. (2) The prior-based reconstruction regime, where we aim to reconstruct a
3D scene given incomplete observations. We note that problem (2) is of great interest in the ﬁeld
of artiﬁcial intelligence, where we regularly infer neural scene representations from incomplete
observations. LFNs are immediately applicable to this regime, opening avenues of future work in
scene understanding, scene decomposition, reinforcement learning (where a tractable inverse graphics
model is of critical importance), etc.
4
References
[1] E. R. Chan, M. Monteiro, P. Kellnhofer, J. Wu, and G. Wetzstein. pi-gan: Periodic implicit generative
adversarial networks for 3d-aware image synthesis. Proc. CVPR, 2020.
[2] S. A. Eslami, D. J. Rezende, F. Besse, F. Viola, A. S. Morcos, M. Garnelo, A. Ruderman, A. A. Rusu,
I. Danihelka, K. Gregor, et al. Neural scene representation and rendering. Science, 360(6394):1204–1210,
2018.
[3] H. Kato, Y. Ushiku, and T. Harada. Neural 3d mesh renderer. In Proc. CVPR, pages 3907–3916, 2018.
[4] B. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ramamoorthi, and R. Ng. Nerf: Representing
scenes as neural radiance ﬁelds for view synthesis. In Proc. ECCV, 2020.
619

[5] M. Niemeyer, L. Mescheder, M. Oechsle, and A. Geiger. Differentiable volumetric rendering: Learning
implicit 3d representations without 3d supervision. In Proc. CVPR, 2020.
[6] E. Perez, F. Strub, H. de Vries, V. Dumoulin, and A. C. Courville. Film: Visual reasoning with a general
conditioning layer. In AAAI, 2018.
[7] J. Reizenstein, R. Shapovalov, P. Henzler, L. Sbordone, P. Labatut, and D. Novotny. Common objects
in 3d: Large-scale learning and evaluation of real-life 3d category reconstruction. In Proc. ICCV, pages
10901–10911, 2021.
[8] V. Sitzmann, E. R. Chan, R. Tucker, N. Snavely, and G. Wetzstein. Metasdf: Meta-learning signed distance
functions. Proc. NeurIPS, 2020.
[9] V. Sitzmann, M. Zollhöfer, and G. Wetzstein. Scene representation networks: Continuous 3d-structure-
aware neural scene representations. In Proc. NeurIPS 2019, 2019.
[10] L. Yariv, Y. Kasten, D. Moran, M. Galun, M. Atzmon, B. Ronen, and Y. Lipman. Multiview neural surface
reconstruction by disentangling geometry and appearance. Proc. NeurIPS, 2020.
[11] A. Yu, V. Ye, M. Tancik, and A. Kanazawa. pixelnerf: Neural radiance ﬁelds from one or few images.
Proc. CVPR, 2020.
720

