arXiv:2004.12488v2  [cs.LG]  17 Nov 2020
Order preserving
hierarchical agglomerative clustering
Daniel Bakkelund
daniel.bakkelund@ifi.uio.no
Abstract
We present a method for hierarchical clustering of directed acyclic graphs and other
strictly partially ordered data that preserves the data structure. In particular, if we have
a < b in the original data and denote their respective clusters by [a] and [b], we get [a] < [b]
in the produced clustering. The clustering uses standard linkage functions, such as single-
and complete linkage, and is a generalisation of hierarchical clustering of non-ordered sets.
To achieve this, we deï¬ne the output from running hierarchical clustering algorithms
on strictly ordered data to be partial dendrograms; sub-trees of classical dendrograms with
several connected components. We then construct an embedding of partial dendrograms
over a set into the family of ultrametrics over the same set. An optimal hierarchical clus-
tering is now deï¬ned as follows: Given a collection of partial dendrograms, the optimal
clustering is the partial dendrogram corresponding to the ultrametric closest to the ori-
ginal dissimilarity measure, measured in the p-norm. Thus, the method is a combination
of classical hierarchical clustering and ultrametric ï¬tting.
Keywords
Hierarchical clustering
Â·
Order preserving
clustering
Â·
Partial
dendrogram Â· Unsupervised classiï¬cation Â· Ultrametric ï¬tting Â· Acyclic partition
1
Introduction
Clustering is one of the oldest and most frequently used techniques for exploratory data analysis
and unsupervised classiï¬cation. The toolbox contains a large variety of methods and algorithms,
spanning from the initial, but still popular ideas of k-means (Macqueen, 1967) and hierarchical
clustering (Johnson, 1967), to more recent methods, such as density- and model based cluster-
ing (Kriegel et al., 2011; Fraley and Raftery, 2002), and semi-supervised methods (Basu et al.,
2008), plus a large list of variants. All these methods have one thing in common: they try
to extract hidden structure from the data, and make it visible to the analyst. But they also
share another feature: if the analysed data is already endowed with some form of structure, the
structure is lost in the clustering process; the clustering does not try to retain the structure.
This in spite of the fact that strictly partially ordered data, such as directed acyclic graphs,
rooted trees and linear orders, are types of data that is more and more commonly analysed by
practitioners.
In this paper, we show how to extend hierarchical clustering to relational data in a way that
preserves the relations. In particular, if the input is a set X equipped with a strict order relation
<, and if a, b âˆˆX, we ensure that if a < b then we will have [a] <â€² [b] after clustering, where [a]
and [b] are the respective clusters of a and b, and <â€² is an order relation on the clusters naturally
induced by <.
1

1.1
Motivating real-world use case
The motivation for our method comes from an industry database of machine parts that are
arranged in part-of relations: parts are registered as sub-parts of other parts. For historical
reasons, there have been incidents of copy-paste of machine designs, and the copies have been
given entirely new identiï¬ers with no links to the original design. In hindsight, there is a wish
to identify these equivalent machine parts, but telling them apart is hard. Also, the metadata
that is available has a tendency of displaying high similarity between a part and its sub-parts,
leading to â€œvertical clusteringâ€ in the data.
Since the motivation is to identify equivalent machinery with the aim of replacing one piece
of machinery with an equivalent part, and since a part and its sub-parts by no means can be
interchanged, it is essential to maintain this parent-child relationship. Moreover, since a part
and its sub-part are never equivalent, this is a strict order relation. The set of all machine parts
thus makes up a strictly partially ordered set.
By preserving these relations in the clustering process, we can eliminate the errors due to
close resemblance between the part and the sub-part, resulting in improved over all quality of
the clustering.
This is but one concrete example of a real world problem where the method we present
performs signiï¬cantly better than standard methods that disregard the structure, but it is
possible to imagine several other cases for which we have not yet had the opportunity to test
our methodology. We will only mention two here; citation networks and time series:
Citation networks are partial orders, where the order is deï¬ned by the citations.
If we
perform order preserving clustering in the above sense on citation networks, the clusters will
contain related research, and the clusters will be ordered according to appearance relative other
related research. This diï¬€ers from clustering with regards to time: when clustering with time
as a parameter, you have to choose, implicitly or explicitly, a time interval for each cluster.
When the citation graph is used for ordering, the clusters will contain research that occurred
in parallel, citing similar sources, and being cited by similar sources, regardless to whether they
occurred in some particular time interval.
Time series are totally ordered sets of events, making a family of time series a partially
ordered set of events. If you want to do correlations across the time series, but the time stamps
cannot be used for this purpose (diï¬€erent time zones, errors in time stamps, drift in time stamps,
etc.), the method we present in this article allows you to produce slices across all or some of the
time series while keeping the order intact, suggesting which time stamps in one series correlate
to time stamps in the other series.
1.2
Problem overview
This section presents the research problem on a high level. All terms and concepts used in this
section will be properly deï¬ned in the main text.
1.2.1
Hierarchical clustering at a glance
From a birdâ€™s-eye view, one may describe hierarchical agglomerative clustering as follows: A
clustering of a set X is a partitioning of X into disjoint subsets called clusters. Given a set X
together with a notion of (dis-)similarity between the elements of X, a hierarchical agglomerative
clustering can be obtained as follows:
1. Start by placing each element of X in a separate cluster.
2

2. Pick the two clusters that are most similar according to the (dis-)similarity measure, and
combine them into one cluster by taking their union.
3. If all elements of X are in the same cluster, we are done. Otherwise, go to Step 2 and
continue.
The result from this process is a dendrogram; a tree structure showing the sequence of the
clustering process. Figure 1 shows a dendrogram over the set X = {a, b, c, d, e}. The elements
of X are the leaf nodes of the dendrogram, and, starting at the bottom, the horizontal bars
indicate which elements are joined at which step in the process. The numbers on the y-axis
indicate at which dissimilarity level the diï¬€erent clusters were formed.
2.0
4.5
8.0
10.0
a
b
c
d
e
Figure 1: A dendrogram over the set X = {a, b, c, d, e}.
The set of dendrograms over a ï¬nite set is in a bijective correspondence with the set of
ultrametrics over the same set (Carlsson and MÂ´emoli, 2010).
An ultrametric is a particular
type of metric. From Figure 1, we deï¬ne the ultrametric distance between two elements to
be the minimal height you have to ascend to in order to traverse from one element to the
other. For example, the ultrametric distance between elements c and e is 8.0. Dendrograms and
ultrametrics play a central role in our theory.
1.2.2
Introducing a strict order relation on X
Given a set X = {a, b, c, d} where a < b and c < d, we can use arrows to denote the order
relation, thinking of X as a directed acyclic graph with two connected components. If we want
to produce a hierarchical clustering of X as described above, while at the same time maintaining
the order relation, our options are depicted in the Hasse digram in Figure 2.
ac
bd
a
bc
d
c
ad
b
ac
b
d
a
c
bd
a
b
c
d
Figure 2: Possible hierarchical clusterings over the set X = {a, b, c, d} with a < b and c < d.
Adjacent elements indicate clusters.
Each path in this diagram, starting at the bottom and advancing upwards, represents a
hierarchical clustering. But, since we are required to preserve the order relation, we cannot merge
any more elements than what we see here. This means that we will never obtain dendrograms
like the one in Figure 1, that joins at the top when all elements are placed in a single cluster.
3

Rather, the output of hierarchical agglomerative clustering would take the form of the partial
dendrograms of Figure 3.
1
2
b
c
a
d
a
bc
d
1
2
a
d
b
c
c
ad
b
1
2
a
c
b
d
ac
bd
Figure 3: Partial dendrograms over the set X = {a, b, c, d} with a < b and c < d. Each partial
dendrogram corresponds to a path in Figure 2 starting at the bottom and advancing upwards
to the ordered set depicted below the dendrogram.
Moreover, consider the situation where both (a, d) and (a, c) are pairs of minimal dissimilar-
ity. Being mutual exclusive merges, choosing to merge one over the other leads to very diï¬€erent
solutions, so we need a method that tells us which of the candidates is the better.
1.3
Outline of our method and contributions
In this section, we provide a high level view of our method, together with our main contributions
and results.
1.3.1
An embedding of partial dendrograms into ultrametrics
When we apply hierarchical agglomerative clustering to strictly partially ordered data, the
output is partial dendrograms. One of the contributions of this paper is an embedding of partial
dendrograms over a set into the family of ultrametrics over the same set. Due to the bijective
correspondence between ultrametrics and dendrograms, this also provides an embedding from
partial dendrograms into dendrograms.
This embedding is of obvious theoretical interest, since it parallels the bijective correspond-
ence between dendrograms over a set and ultrametrics over the same set. The embedding lifts
to the bijection, tying the theories for hierarchical agglomerative clustering for ordered and
non-ordered sets together.
But the embedding has value beyond the purely theoretical. The ï¬rst is that ultrametrics
(or metrics) are more suitable as tools for mathematics than are partial dendrograms. Secondly,
substantial work has been conducted since Jardine and Sibson (1971) studying hierarchical clus-
tering in the light of the correspondence between dendrograms and ultrametrics. Also, several
optimisation based methods for hierarchical clustering use the clustering structure, in terms of
dendrograms, as input to the objective function. Our embedding makes it possible to beneï¬t
from and participate in all of this. We have therefore laid down extra eï¬€ort, making sure that
this mapping is indeed an embedding (that is, an injective map), and not just any map.
But more importantly, we deï¬ne our objective function as follows: from a selected list of
candidate partial dendrograms, the partial dendrogram representing the best hierarchical clus-
tering is the one that corresponds to the ultrametric that is closest to the original dissimilarity
measure when measured in the p-norm. This makes our model a variant of ultrametric ï¬tting.
4

1.3.2
Optimised hierarchical agglomerative clustering
Consider the case of a non ordered set X equipped with a dissimilarity measure. A family of
candidate hierarchical clusterings are deï¬ned as follows: By running the algorithm for classical
hierarchical clustering from Section 1.2.1, we consider every output a candidate solution. In
particular, if the algorithm faces two or more equally valid pairs for merging, we simply merge
those pairs in every possible order. This generates one candidate solution for each permutation
of those connections.
Given the collection of candidate solutions, we deï¬ne the optimised hierarchical agglomerative
clustering to be the dendrogram among the candidate solutions corresponding to the ultrametric
closest to the original dissimilarity measure when measured in the p-norm.
Due to the way the candidates are generated, this deï¬nition is permutation invariant; it
is not inï¬‚uenced by the order in which the elements of the set are enumerated.
The main
characteristics of optimised hierarchical clustering may be summarised as follows:
1. Optimised hierarchical clustering is permutation invariant
2. Classical hierarchical clustering with single linkage is identical to optimised hierarchical
clustering with single linkage
3. Optimised hierarchical clustering with complete linkage is NP-hard
As we demonstrate in relation to the proof of Item 3, optimised hierarchical clustering with
complete linkage is able to detect maximal cliques in graphs. This is a powerful property, and
we take this as part of the evidence that our choice of objective function is reasonable. In some
sense, we may say that this is a fulï¬llment of the idea behind complete linkage.
1.3.3
Order preserving optimised hierarchical agglomerative clustering
Our main result: order preserving hierarchical agglomerative clustering for strictly partially
ordered sets extends the above model for non-ordered clustering.
We copy the process for
candidate generation, with the modiï¬cation that we ensure that every merge leads to an order
preserving clustering. Our set of candidates now consists of partial dendrograms. The optimal
partial dendrogram is deï¬ned as the partial dendrogram corresponding to the ultrametric being
closest to the original dissimilarity measure when measured in the p-norm. This comparison is
possible due to our embedding of partial dendrograms into ultrametrics.
In this way, we end up with order preserving hierarchical clustering that is permutation
invariant. For hierarchical clustering of strictly ordered sets, permutation invariance is key: As
described in Section 1.2.2, diï¬€erent choices of merges may lead to very diï¬€erent results. Being
dependent on the enumeration order of the underlying set could be disastrous.
The complexity class of order preserving hierarchical agglomerative clustering remains an
open question. At the time of writing, the authors have neither managed to ï¬nd a proof of
NP-completeness, nor found any eï¬ƒcient algorithm for solving the clustering problem.
1.3.4
Polynomial time approximation
Since no eï¬ƒcient algorithm currently exists, we present a method of approximation that can be
computed in polynomial time. The approximation method is based on random sampling from
candidate solutions, and therefore allows for parallel processing. We demonstrate the approx-
imation method on synthetic data generated as random directed acyclic graphs and random
dissimilarity measures.
5

We report the eï¬ƒcacy of the approximation in terms of convergence plots, and when we have
a reference clustering available, we also compute the correlation between the approximation and
the reference clustering in terms of the adjusted Rand index. We also present a method for
computing the Rand index for the induced order relation relative to the reference order relation,
which be believe is a ï¬rst of its kind.
The approximation converges very rapidly on the demonstration datasets, indicating that
just a few samples are required to obtain close to optimal solutions.
1.3.5
Contributions
Our main contribution is the theory and algorithms for order preserving hierarchical agglomer-
ative clustering for strict posets. Further contributions we wish to highlight are:
â€¢ Optimised hierarchical agglomerative clustering for non-ordered sets; a hierarchical clus-
tering methodology very close to classical hierarchical clustering, but that is permutation
invariant.
â€¢ A general framework for order preserving ultrametric ï¬tting of strict partial orders and
directed acyclic graphs equipped with a dissimilarity measure.
â€¢ A polynomial time approximation scheme for order preserving hierarchical agglomerative
clustering
â€¢ A novel method for comparison of induced order relations over a set based on the adjusted
Rand index.
1.4
Related work
Hierarchical agglomerative clustering is described in a plethora of books and articles, and we
shall not try to give a comprehensive account of the material here. For an introduction to the
subject, refer to (Jain and Dubes, 1988, Â§3.2).
1.4.1
Clustering of ordered data
There are quite a few articles presenting clustering of orders, placing themselves in one of two
categories.
The ï¬rst is clustering of sets where the (dis)similarity measure is replaced by information
about whether one pair of elements is more similar than another pair of elements, for example
based on user preferences. This is sometimes referred to as comparison based clustering. See the
recent article by Ghoshdastidar et al. (2019) for an example and references. In this category, we
also ï¬nd the works of Janowitz (2010), providing a wholly order theoretic description of hier-
archical clustering, including the case where the dissimilarity measure is replaced by a partially
ordered set.
The second variant is to partition a family of ordered sets so that similarly ordered sets are
associated with each other. Examples include the paper by Kamishima and Fujiki (2003), where
they develop a variation of k-means, called k-oâ€²means, for clustering preference data, each list
of preferences being a totally ordered set. Other examples in this category include clustering of
times series, identifying which times series are alike ( Luczak, 2016).
Our method diï¬€ers from all of these examples in that we cluster elements inside one ordered
set through the use of a (dis)similarity measure, while maintaining the original orders of ele-
ments.
6

1.4.2
Clustering to detect order
Another variant is the detection of order relations in data through clustering: In (Carlsson et al.,
2014), it is demonstrated how hierarchical agglomerative quasi-clustering can be used to deduce
a partial order of â€œnet ï¬‚owâ€ from an asymmetric network.
In this category, it is also worth mentioning dynamic time warping. This is a method for
aligning time series, and can be considered as clustering across two time series that is indeed
order preserving. See ( Luczak, 2016) for further references on this.
1.4.3
Acyclic graph partitioning problems
The problem of order preserving hierarchical agglomerative clustering can be said to belong to
the family of acyclic graph partitioning problems (Herrmann et al., 2017). If we consider the
strict partial order to be a directed acyclic graph (DAG), the task is to partition the vertices
into groups so that the groups together with the arrows still makes up a DAG.
Graph partitioning has received a substantial attention from researchers, especially within
computer science, over the last 50 years. Two important ï¬elds of application of this theory are
VLSI and parallel execution.
For VLSI, short for Very Large Scale Integration, the problem can be formulated as follows:
Given a set of micro processors, the wires that connect them, and a set of circuit boards, how
do you best place the processors on the circuit boards in order to optimise a given objective
function? Typically, a part of the objective function is to minimise the wire length. But other
features may also be part of the optimisation, such as the amount or volume of traï¬ƒc between
certain processors etc. (Markov et al., 2015)
For parallel processing, the input data is a set of tasks to be executed.
The tasks are
organised as a DAG, where predecessors must be executed before descendants. Given a ï¬nite
number of processors, the problem is to group the tasks so that they can be run group-wise
on a processor, or running groups in parallel on diï¬€erent processors, in order to execute all
tasks as quickly as possible. Typically additional information available is memory requirements,
expected execution times for the tasks, etc. (BuluÂ¸c et al., 2016)
It is not diï¬ƒcult to understand why both areas have received attention, being essential
in the development of modern computers. The development of theory and methods has been
both successful and abundant, and a large array of techniques are available, both academic and
commercially.
Although both problems do indeed perform clustering of strict partial orders, their solutions
are not directly transferable to exploratory data analysis. Mostly because they have very speciï¬c
constraints and objectives originating from their respective problem domains.
The method we propose in this paper has as input a strict partial order (equivalently; a DAG)
together with an arbitrary dissimilarity measure. We then use the classical linkage functions
single-, average-, and complete linkage to suggest clusterings of the vertices from the input
dataset, while preserving the original order relation.
Our method therefore places itself ï¬rmly in the family of acyclic graph partitioning meth-
odologies, but with diï¬€erent motivation, objective and solution, compared to existing methods.
1.4.4
Hierarchical clustering as an optimisation problem
Several publications aim at solving hierarchical clustering in terms of optimisation. However,
due to the procedural nature of classical hierarchical clustering, combined with the linkage
functions, pinning down an objective function may be an impossible task.
Especially since
classical hierarchical clustering is not even well deï¬ned for complete linkage in the presence
7

of tied connections. This leads to a general abandonment of linkage functions in optimisation
based hierarchical clustering.
Quite commonly, optimisation based hierarchical clustering is done in terms of ultrametric ï¬t-
ting. That is, it aims to ï¬nd an ultrametric that is as close to the original dissimilarity measure as
possible, perhaps adding some additional constraints (Gilpin et al., 2013; Chierchia and Perret,
2019). It is well known that solving single linkage hierarchical clustering is equivalent to ï¬nd-
ing the so called maximal sub-dominant ultrametric. That is; the ultrametric that is pointwise
maximal among all ultrametrics not exceeding the original dissimilarity (Rammal et al., 1986).
But for the other linkage functions, there is no equivalent result.
Optimisation based hierarchical clustering therefore generally present alternative deï¬nitions
of hierarchical clustering. Quite often based on objective functions that originate from some par-
ticular domain. Exceptions from this are, for example, Wardâ€™s method (Ward, 1963), where the
topology of the clusters are the focus of the objective, and also the recent addition by Dasgupta
(2016), where the optimisation aims towards topological properties of the generated dendrogram.
Although our method is, eventually, based on ultrametric ï¬tting, we optimise over a very
particular set of dendrograms. Namely the dendrograms that can be generated through classical
hierarchical clustering with linkage functions. It is therefore reasonable to claim that our method
places itself between classical hierarchical clustering and optimised models.
1.4.5
Clustering with constraints
A signiï¬cant amount of research has been devoted to the topic of clustering with constraints
in the form of pairwise must-link or cannot-link constraints, often in addition to other con-
straints, such as minimal- and maximal distance constraints, and so on. Some work as also
been done on hierarchical agglomerative clustering with constraints, starting with the works
of Davidson and Ravi (2005). For a thorough treatment of constrained clustering, see (Basu et al.,
2008).
Order preserving clustering (as well as acyclic partitioning) can be seen as a particular version
of constrained clustering, where the constraint is a directed, transitive cannot-link constraint. A
type of constraint that is not found in the constrained clustering literature.
1.5
Organisation of the remainder of this paper
Section 2 provides the necessary background material. We start by recalling strict and non-strict
partial order relations and equivalence relations. Thereafter, we revisit classical hierarchical
agglomerative clustering, recalling central concepts such as dissimilarity measures, ultrametrics
and dendrograms.
In Section 3, we develop optimised hierarchical agglomerative clustering for non-ordered
sets. This is a permutation invariant clustering model that is tailored especially to ï¬t into our
framework for agglomerative clustering of ordered sets.
In Section 4, we tackle the problem of order preservation during clustering: We deï¬ne what
we mean by order preservation, and classify exactly the clusterings that are order preserving.
We also provide concise necessary and suï¬ƒcient conditions for an hierarchical agglomerative
clustering algorithm to be order preserving.
In Section 5, we deï¬ne partial dendrograms, and develop the embedding of partial dendro-
grams over an ordered set into the family of ultrametrics over the same set.
Our main results are given in Section 6, where we generalise the clustering model provided
in Section 3 to order preserving hierarchical agglomerative clustering for strict partial orders.
Section 7 provides a polynomial time approximation scheme for the clustering method, and
demonstrate the eï¬ƒcacy of the approximation on synthetic data. Section 8 closes the article
8

with some concluding remarks, and a list of future work topics.
2
Background
In this section we recall basic background material.
We start by recollecting the required
order-theoretical tools together with equivalence relations, before recalling classical hierarchical
clustering.
2.1
Relations
Deï¬nition 1. A relation R on a set X is a subset R âŠ†X Ã— X, and we say that x and y are
related if (x, y) âˆˆR. The short hand notation aRb is equivalent to writing (a, b) âˆˆR.
2.1.1
Strict and non-strict partial orders
A strict partial order on a set X is a relation S on X that is irreï¬‚exive and transitive. Recall
that, an irreï¬‚exive and transitive relation is also anti-symmetric. A strictly partially ordered
set, or a strict poset, is a pair (X, S), where X is a set and S is a strict partial order on X.
We commonly denote a strict partial order by the symbol <.
On the other hand a partial order on X is a relation P on X that is reï¬‚exive, asymmetric
and transitive, and the pair (X, P) is called a partially ordered set, or a poset. The usual
notation for a partial order is â‰¤.
We shall just refer to strict and non-strict partial orders as orders, unless there is any need
for disambiguation: If R is an order on X, we say that a, b âˆˆX are comparable if either
(a, b) âˆˆR or (b, a) âˆˆR. And, if every pair of elements in X are comparable, we call X totally
ordered. A totally ordered subset of an ordered set is called a chain, and a subset where no
two elements are comparable is called an antichain. We denote non-comparability by aâŠ¥b.
That is, for any elements a, b in an antichain, we have aâŠ¥b.
A cycle in a relation E is a sequence in E on the form (a, b1), (b1, b2), . . . , (bn, a).
The
transitive closure of E is the minimal set E for which the following holds: If there is a
sequence of pairs (a1, a2), (a2, a3), . . . , (anâˆ’1, an) in E, then (a1, an) âˆˆE.
Let (X, E) be an ordered set. An element x0 âˆˆX is a minimal element if there is no
element y âˆˆX âˆ’{x0} for which (y, x0) âˆˆE. Dually, y0 is a maximal element if there is no
x âˆˆX âˆ’{y0} for which (y0, x) âˆˆE. If (X, E) has a unique minimal element, then this is called
the bottom element or the least element, and a unique maximal element is called the top
element or the greatest element.
Finally, a map f : (X, <X) â†’(Y, <Y ) is order preserving if a <X b â‡’f(a) <Y f(b), and
if f is a set isomorphism (that is, a bijection) for which f âˆ’1 is also order preserving, we say that
f is an order isomorphism, and that the sets (X, <X) and (Y, <Y ) are order isomorphic,
writing (X, <X) â‰ˆ(Y, <Y ).
2.1.2
Partitions and equivalence relations
A partition of X is a collection of disjoint subsets of X, the union of which is X. That is;
a clustering of X is a partition of X. The family of all partitions of X, denoted P(X), has a
natural partial order deï¬ned by partition-reï¬nement: If A = {Ai}i and B = {Bj}j are partitions
of X, we say that A is a reï¬nement of B, writing A â‹B, if, for every Ai âˆˆA there exists a
Bj âˆˆB such that Ai âŠ†Bj. The sets of a partition are referred to as blocks.
Partitions are intimately related to the concept of equivalence relations: An equivalence
relation is a relation R on X that is reï¬‚exive, symmetric and transitive. Let the family of all
9

equivalence relations over a set X be denoted by R(X). If R âˆˆR(X) and (x, y) âˆˆR, we say
that x and y are equivalent, writing x âˆ¼y. The maximal set of elements equivalent to x âˆˆX
is called the equivalence class of x, and is denoted [x]. R(X) is also partially ordered, but
by subset inclusion: that is, for R, S âˆˆR(X), we say that R is less than or equal to S if and
only if R âŠ†S .
The quotient of X modulo R, denoted X/R, is the set of equivalence classes of X under R.
Notice that [x] is an element of X/R, but a subset of X.
Since the equivalence classes are subsets of X that together cover X, X/R is a partition of X.
Indeed, the family of partitions of X is in a one-to-one correspondence with the equivalence
relations of X: If A âˆˆP(X), then there exists a unique R âˆˆR(X) for which A = X/R.
Moreover, the correspondence is order preserving; for A = X/R and B = X/S , we have
A â‹B â‡”R âŠ†S .
Both P(X) and R(X) have top- and bottom elements: The least element of P(X) is the
singleton partition S(X), where each element is in an equivalence class by itself: S(X) =
{{x} | x âˆˆX}. The singleton partition corresponds to the diagonal equivalence relation, given
by âˆ†(X) = {(x, x) | x âˆˆX}, which is the least element of R(X). The greatest element of P(X)
is the trivial partition, {X}, corresponding to the equivalence relation X Ã—X, where all element
are equivalent. That is
S(X) = X/âˆ†(X)
and
{X} = X/(X Ã— X).
If A and B are partitions of X with A being a reï¬nement of B, we say that A is ï¬ner than
B, and that B is coarser than A. We use the exact same terminology for the corresponding
equivalence relations A , B âˆˆR(X), having A âŠ†B.
For a subset A âŠ†X, let the notation X/A denote the partition of X where all of A is one
equivalence class, and the rest of X remains as singletons. Formally, this corresponds to the
equivalence relation RA = âˆ†(X) âˆª(A Ã— A). And ï¬nally, the quotient map corresponding to
an equivalence relation R âˆˆR(X) is the unique map qR : X â†’X/R deï¬ned as qR(x) = [x].
That is, qR sends each element to its equivalence class.
Deï¬nition 2. A clustering of a set X is a partition of X, and a hierarchical clustering is
a chain in P(X) containing both the bottom and top elements. A cluster in a clustering is a
block in the partition.
Alternatively, a clustering of X is an equivalence relation R âˆˆR(X), and a hierarchical
clustering is a chain in R(X) containing both the bottom- and top elements of R(X). A cluster
is, then, an equivalence class in X/R.
We will refer to clusters as equivalence classes, clusters or blocks depending on the context,
all terms being frequently used in clustering literature.
Example 1. For the three-element space X = {a, b, c}, the lattice of partitions takes the form
of the below Hasse diagram.
{{a, b, c}}
{{a, b}, {c}}
{{a, c}, {b}}
{{a}, {b, c}}
{{a}, {b}, {c}}
P(X) :
The elements in bold make up a chain in P(X) that contains both the bottom- and top elements,
and therefore also constitutes a hierarchical clustering of the elements in X.
10

2.2
Classical hierarchical clustering
In this section, we recall classical hierarchical clustering in terms of Jardine and Sibson (1971).
Our theory builds directly on the theory for classical hierarchical clustering, so we provide a
fair bit of detail: We start by recalling the formal deï¬nition of a dendrogram, before recalling
dissimilarity measures and ultrametrics.
Thereafter, we recall linkage functions, and at the
end of the section, we tie all the concepts together and provide a description of the classical
hierarchical agglomerative clustering algorithm.
2.2.1
Dendrograms
Hierarchical clustering outputs dendrograms. But the graphical tree structure depicted in Fig-
ure 1 is not well-suited for formal deduction. In this section, we recall the deï¬nition of dendro-
gram due to Jardine and Sibson (1971) for this purpose. For the remainder of the text, let R+
denote the non-negative reals.
Deï¬nition 3. Let R+ be equipped with the usual total order â‰¤, and let P(X) be partially
ordered by partition reï¬nement. A dendrogram over a ï¬nite set X is an order preserving map
Î¸ : R+ â†’P(X) for which
D1. Î¸(0) = S(X), the least element of P(X).
D2. âˆƒt0 > 0 s.t. Î¸(t0) = {X}, the greatest element of P(X),
D3. âˆ€t âˆˆR+ âˆƒÎµ > 0 s.t. Î¸(t) = Î¸(t + Îµ).
Axiom D3 ensures that the dendrogram is piecewise constant on intervals on the form [t, tâ€²),
as illustrated in the following example.
Example 2. Below, we see a graphical dendrogram over the set X = {a, b, c, d, e} on the left
hand side, and the corresponding deï¬nition of Î¸ : R+ â†’P(X) on the right.
2.0
4.5
8.0
10.0
a
b
c
d
e
Î¸(x) =
ï£±
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£²
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£³

{a}{b}{c}{d}{e}
	
for x âˆˆ[0.0, 2.0)

{a}{b}{c, d}{e}
	
for x âˆˆ[2.0, 4.5)

{a, b}{c, d}{e}
	
for x âˆˆ[4.5, 8.0)

{a, b}{c, d, e}
	
for x âˆˆ[8.0, 10.0)

{a, b, c, d, e}
	
for x âˆˆ[10.0, âˆ)
We will use the term dendrogram to denote both the graphical and the functional repres-
entation. If im(Î¸) = {Bi}n
i=0, we assume that the enumeration is compatible with the order
relation on P(X); in other words, that {Bi}n
i=0 is a chain in P(X). We denote the family of
all dendrograms over X by D(X).
2.2.2
Dissimilarity measures and ultrametrics
As pointed out in Section 1.2.1, in order to produce a hierarchical agglomerative clustering, we
need a notion of (dis-)similarity, or â€œdistanceâ€ between elements: A dissimilarity measure on
a set X is a function d : X Ã— X â†’R+, satisfying
d1. âˆ€x âˆˆX : d(x, x) = 0,
d2. âˆ€x, y âˆˆX : d(x, y) = d(y, x).
11

If d additionally satisï¬es
d3. âˆ€x, y, z âˆˆX : d(x, z) â‰¤max{d(x, y), d(y, z)},
we call d an ultrametric. The pair (X, d) is correspondingly called a dissimilarity space or
an ultrametric space. The family of all dissimilarity measures over X is denoted by M(X),
and the family of all ultrametrics by U(X).
Example 3 (Ultrametric). Property d3 is referred to as the ultrametric inequality, and is
a strengthening of the usual triangle inequality. In an ultrametric space (X, u), every triple of
points is arranged in an isosceles triangle: Let a, b, c âˆˆX, and let the pair a, b be of minimal
distance such that u(a, b) â‰¤min{u(a, c), u(b, c)}. The ultrametric inequality gives us
u(a, c) â‰¤max{u(a, b), u(b, c)} = u(b, c)
u(b, c) â‰¤max{u(b, a), u(a, c)} = u(a, c)

â‡”u(a, c) = u(b, c).
Ultrametrics show up in many diï¬€erent contexts, such as p-Adic number theory (Holly, 2001),
inï¬nite trees (Hughes, 2004), numerical taxonomy (Sneath and Sokal, 1973) and also within
physics (Rammal et al., 1986), just to cite a few. For hierarchical clustering, ultrametrics are
relevant because the dendrograms over a set are in a bijective relation to the ultrametrics over
the same set (Carlsson and MÂ´emoli, 2010).
We shall also need the following terms, which apply to any dissimilarity space: The diameter
of (X, d) is given by the maximal inter-point distance:
diam(X, d) = max{ d(x, y) | x, y âˆˆX }.
And the separation of (X, d) is the minimal inter point distance:
sep(X, d) = min{ d(x, y) | x, y âˆˆX âˆ§x Ì¸= y }.
2.2.3
Classical hierarchical clustering
Before we deï¬ne classical hierarchical clustering, we need to recall linkage functions. Our deï¬n-
ition follows the lines of the deï¬nition found in (Carlsson and MÂ´emoli, 2010):
Deï¬nition 4. Given a set X, a family L of linkage functions on X is a set of maps
â„“Q : Q Ã— Q Ã— M(X) âˆ’â†’R+
for Q âˆˆP(X)
so that for each partition Q âˆˆP(X) and dissimilarity measure d âˆˆM(X), the map â„“Q(âˆ’, âˆ’, d) :
Q Ã— Q â†’R+ is a dissimilarity measure on Q.
Let (X, d) be a dissimilarity space with Q âˆˆP(X), and let p, q âˆˆQ. We will commit to the
following abuse of notation: for a family of linkage functions L, we will write L(p, q, d) for the
dissimilarity between p and q assigned by the unique dissimilarity measure â„“Q(âˆ’, âˆ’, d). We will,
also somewhat misleading, refer to the family L as a linkage function.
Deï¬nition 5. For a dissimilarity space (X, d), let Q âˆˆP(X) and recall that p, q âˆˆQ are subsets
of X, since they are blocks of Q. The classical linkage functions are deï¬ned as follows:
Single linkage
:
SL(p, q, d) = minxâˆˆp minyâˆˆq d(x, y).
Complete linkage
:
CL(p, q, d) = maxxâˆˆp maxyâˆˆq d(x, y).
Average linkage
:
AL(p, q, d) =
P
xâˆˆp
P
yâˆˆq d(x, y)
|p| Â· |q|
.
12

Deï¬nition 6 (Classical HC). Given a dissimilarity space (X, d) and a linkage function L, if
we follow the procedure outlined in Section 1.2.1, using L as the â€œnotion of dissimilarityâ€, the
result is a chain of partitions {Qi}|X|âˆ’1
i=1
together with the dissimilarities {Ïi}|X|âˆ’1
i=1
at which the
partitions were formed. The sequence of pairs Q = {(Qi, Ïi)}|X|âˆ’1
i=1
maps to a dendrogram Î¸Q
as follows:
Î¸Q(x) = Qmax{iâˆˆN | Ïiâ‰¤x}.
(1)
We deï¬ne a classical hierarchical clustering of (X, d) using L to be a dendrogram
HCL(X, d) = Î¸Q
obtained through this procedure.
Remark 7. Notice that (1) maps {(Qi, Ïi)}|X|âˆ’1
i=1
to a dendrogram if and only if
sep(Qi, L) â‰¤sep(Qi+1, L)
for 0 â‰¤i < |X| âˆ’1.
(2)
Otherwise, the Ïi will not make up a monotone sequence, and the resulting function Î¸Q will not
be an order preserving map. Although all of SL, AL and CL satisfy (2), it is fully possible to
deï¬ne linkage functions that do not.
At any point during the clustering process, if we encounter a partition Q âˆˆP(X) with two
distinct pairs of elements (p1, q1), (p2, q2) âˆˆQ Ã— Q for which
L(p1, q1, d) = L(p2, q2, d) = sep(Q, L),
we say that the two connections are tied, since they are both eligible candidates for the next
merge.
It is well known that HCSL is invariant with respect to the order of resolution of
ties (Jardine and Sibson, 1971), a property referred to as being permutation invariant, since
the order of enumeration of elements will not aï¬€ect the output of the clustering process. On
the other hand, both HCAL and HCCL are sensitive to enumeration order.
2.2.4
Dendrograms and ultrametrics
It has been long known that dendrograms map to ultrametrics (Jardine and Sibson, 1971):
Î¨X : D(X) âˆ’â†’U(X).
In (Carlsson and MÂ´emoli, 2010) the map Î¨X is shown to be a bijection. If Î¸ âˆˆD(X), the map
is deï¬ned as
Î¨X(Î¸)(x, y) = min{ t âˆˆR+ | âˆƒB âˆˆÎ¸(t) : x, y âˆˆB }.
(3)
That is, the ultrametric distance is the least real number t for which Î¸ maps to a partition
where x and y are in the same block.
The minimisation is well deï¬ned due to Axiom D3.
The ultrametric can be read from the diagrammatic representation of the dendrogram as the
minimum height you have to ascend to in order to traverse from one element to the other
following the paths in the tree.
3
Optimised hierarchical clustering
In this section we devise a permutation invariant version of hierarchical clustering based on the
classical deï¬nition. We do this in two steps: First, we produce a family of candidate dendrograms
13

based on occurrences of tied connections. Then we pick the dendrogram that closest resembles
the initial dissimilarity measure as the best solution.
To tackle the problem of tied connections, consider the procedure outlined in Section 1.2.1.
If we resolve tied connections by picking a random minimal dissimilarity pair, the way the
procedure is speciï¬ed, HCL becomes a non-deterministic algorithm; it may produce diï¬€erent
dendrograms for the same input in the presence of ties, depending on which tied pair is selected.
And, moreover, it is capable of producing any dendrogram than can be produced by any tie
resolution order:
Deï¬nition 8. Given a dissimilarity space (X, d) and a linkage function L, let DL(X, d) be the
set of all possible outputs from HCL(X, d).
Recall that a dissimilarity measure d over a ï¬nite set X can be described as an |X| Ã— |X|
real matrix [di,j]. Hence, we can compute the p-norm of a dissimilarity measure, and for an
ultrametric u âˆˆU(X), we can compute the pointwise diï¬€erence
âˆ¥u âˆ’dâˆ¥p =
p
s X
x,yâˆˆX
|u(x, y) âˆ’d(x, y)|p.
(4)
We suggest the following deï¬nition, recalling the deï¬nition of Î¨X (3):
Deï¬nition 9. Given a dissimilarity space (X, d) and a linkage function L, the optimised
hierarchical agglomerative clustering over (X, d) using L is given by
HCL
opt(X, d) =
arg min
Î¸âˆˆDL(X,d)
âˆ¥Î¨X(Î¸) âˆ’dâˆ¥p.
(5)
That is; among all dendrograms that can be generated by HCL(X, d), optimised hierarch-
ical agglomerative clustering picks the dendrogram that is closest to the original dissimilarity
measure. In the tradition of ultrametric ï¬tting, this is the right choice of candidate.
Since DL(X, d) contains all dendrograms generated over all possible permutations of enu-
merations of X, the below theorem follows directly from Deï¬nition 9:
Theorem 10. HCL
opt is permutation invariant. That is, the order of enumeration of the ele-
ments of the set X does not aï¬€ect the output from HCL
opt(X, d).
Also, since SL is permutation invariant, we always have
DSL(X, d)
 = 1, yielding
Theorem 11. HCSL
opt(X, d) = HCSL(X, d).
Since HCAL and HCCL are not permutation invariant, we have no corresponding result in
these cases. For complete linkage, however, we have the following theorem. First, notice that
due to the deï¬nition of complete linkage (Deï¬nition 5), if Î¸ is a solution to HCCL
opt(X, d) and
u = Î¨X(Î¸) is the corresponding ultrametric, then
u(x, y) â‰¥d(x, y)
âˆ€x, y âˆˆX.
Hence, in the case of complete linkage we can reformulate (5) as follows:
HCCL
opt(X, d) =
arg min
Î¸âˆˆDCL(X,d)
âˆ¥Î¨X(Î¸)âˆ¥p.
(6)
To see why this is the case, notice that if u, uâ€² âˆˆM(X) and both d â‰¤u and d â‰¤uâ€² pointwise,
then we can produce two non-negative functions Î´, Î´â€² on X Ã—X so that u = d+Î´ and uâ€² = d+Î´â€².
In particular, we have u âˆ’d = Î´, from which we deduce
âˆ¥u âˆ’dâˆ¥p â‰¤âˆ¥uâ€² âˆ’dâˆ¥â‡”âˆ¥Î´âˆ¥p â‰¤âˆ¥Î´â€²âˆ¥p â‡”âˆ¥d + Î´âˆ¥p â‰¤âˆ¥d + Î´â€²âˆ¥p â‡”âˆ¥uâˆ¥p â‰¤âˆ¥uâ€²âˆ¥p.
14

Theorem 12. Solving HCCL
opt(X, d) is NP-hard.
Proof. Let G = (V, E) be an undirected graph with vertices V and edges E âŠ†V Ã— V . Recall
the clique problem: Given a positive integer K < |V |, is there a clique in G of size at least K?
Equivalently: is there a set V â€² âŠ†V with |V â€²| â‰¥K for which V â€² Ã— V â€² âŠ†E? This is a known
NP-hard problem (Karp, 1972).
To reduce clique to HCCL
opt, deï¬ne a dissimilarity measure on V as follows:
d(v, vâ€²) =
(
1
if (v, vâ€²) âˆˆE,
2
otherwise.
(7)
Then (V, d) is a dissimilarity space. Let Î¸ be a solution of HCCL
opt(V, d), and set d = Î¨V (Î¸).
An intrinsic property of CL is that if two blocks p, q âˆˆQi are merged, then
âˆ€v, vâ€² âˆˆp âˆªq : d(v, vâ€²) â‰¤CL(p, q, d).
And since we have d(v, vâ€²) = 1 â‡”(v, vâ€²) âˆˆE, it means that for a subset V â€² âŠ†V , we have that
âˆ€v, vâ€²âˆˆV â€² : d(v, vâ€²) = 1 â‡”V â€² is a clique in G.
(8)
It follows that a largest possible cluster at proximity level 1 is a maximal clique in G.
We claim that minimising the norm is equivalent to producing a maximal cluster at proximity
level 1: Let d be the |V | Ã— |V | distance matrix [di,j]. Due to the deï¬nition of CL, we have
d(v, vâ€²) âˆˆ{0, 1, 2}. If Î¸(1) = {Vi}s
i=1, then these are exactly the blocks that are subsets of
cliques, so each Vi contributes with |Vi|(|Vi| âˆ’1) ones in [di,j].
Having more ones reduces the norm of d.
Let Vj be of maximal cardinality in {Vi}s
i=1.
Assume ï¬rst that Vj has at least two elements more than the next to largest block, and let
|Vj| = P.
Removing one element from Vj reduces the number of ones in the dissimilarity matrix by
P(P âˆ’1)âˆ’(P âˆ’1)(P âˆ’2) = 2(P âˆ’1). Let the next to largest block have Q elements. Transferring
the element to this block then increases the number of ones by (Q+1)Qâˆ’Q(Qâˆ’1) = 2Q. Since
Q < P âˆ’1, this means that the total number of ones is reduced by moving an element from the
largest block to any of the smaller blocks. Hence, achieving the largest possible number of ones
implies maximising the size of the largest block.
If now, Vj only has one element more than the next to largest block, moving an element as
above corresponds to keeping the number of ones. Since each Vi for 1 â‰¤i â‰¤s is a subset of
a clique in G, the maximal number of ones is achieved by producing a block Vj that contains
exactly a maximal clique of G.
Therefore, if I{1}(x) is the indicator function for the set {1}, the size of a maximal clique in
G can be computed as
max
1â‰¤iâ‰¤|V |
n |V |
X
j=1
I{1}
 di,j
o
,
counting the maximal number of row-wise ones in [di,j] in O(N 2) time. We therefore conclude
that HCCL
opt is NP-hard.
The computational hardness of HCCL
opt is directly connected to the presence of
tied connections: every encounter of n tied connections leads to n! new can-
didate solutions.
Since neither HCAL
opt is permutation invariant, the authors strongly believe that this is also
NP-hard, although that remains to be proven.
15

Remark 13. We cannot, in general, expect the mapping Î¸ 7â†’âˆ¥Î¨X(Î¸) âˆ’dâˆ¥p to be injective,
meaning that the answer to (5) may not be unique. Now, HCL, by construction, and HCSL
opt, by
Theorem 11, have unique solutions for every input (X, d). But both HCAL
opt and HCCL
opt may have
more than one solution, each solution being optimal.
Given a set X, denote the power set of X by P(X). We shall consider HCL
opt(X, âˆ’) to be
the function
HCL
opt(X, âˆ’) : M(X) âˆ’â†’P(D(X)),
mapping a dissimilarity measure over X to a set of dendrograms over X.
3.1
Other permutation invariant solutions
Carlsson and MÂ´emoli (2010) oï¬€er an alternative approach to permutation invariant hierarchical
agglomerative clustering. In their solution, when they face a set of tied connections, they merge
all tied the pairs in one operation, resulting in permutation invariance.
In the case of order preserving clustering, a family of tied connections can contain several
mutually exclusive merges due to the order relation. Using the method of Carlsson and MÂ´emoli
leads to a problem of ï¬guring which blocks of tied connections to merge together, and in which
combinations and order. This leads to a combinatorial explosion of alternatives. The method we
have suggested is utterly simple, no doubt, but it is designed to circumvent this very problem.
4
Order preserving clustering
In this section, we determine what it means for an equivalence relation to be order preserving
with regards to a strict partial order. Some of the material presented here is already known,
and can be found in articles on acyclic graph partitioning, for example (Herrmann et al., 2017).
In most of these works, one is usually content by stating that the resultant graph shall be
acyclic. We proceed further to establish precise conditions that are necessary and suï¬ƒcient for
a hierarchical agglomerative clustering algorithm to be order preserving.
4.1
Order preserving equivalence relations
Having established what we mean by a clustering (Deï¬nition 2), we can start the discussion of
what constitutes an order preserving clustering of a strict poset (X, <). If R is an equivalence
relation on X with quotient map q : X â†’X/R, we have already established, in Section 1.1,
that we require
âˆ€x, yâˆˆX : x < y â‡’q(x) <â€² q(y).
That is, we are looking for a particular class of equivalence relations; namely
those that preserve the structure of the strict partial orderâ€”in other words,
the equivalence relations for which the quotient map is order preserving.
Given an ordered set (X, E), there is a particular relation induced on the quotient set X/R
for any equivalence relation R âˆˆR(X) (Blyth, 2005, Â§3.1):
Deï¬nition 14. Given an ordered set (X, E) and an equivalence relation R âˆˆR(X), ï¬rst deï¬ne
the relation S0 on X by
([a], [b]) âˆˆS0 â‡”âˆƒx, y âˆˆX : a âˆ¼x âˆ§b âˆ¼y âˆ§(x, y) âˆˆE.
(9)
The transitive closure of S0 is called the relation on X/R induced by E. We denote this
relation by S.
16

Example 4. An instructive illustration of what the relation S0 looks like for an ordered set
(X, <) under the equivalence relation R is that of an R-fence (Blyth, 2005), or just fence, for
short:
b1
b2
bnâˆ’1
bn
Â· Â· Â·
a1
a2
anâˆ’1
an
Triple lines represent equivalences under R, and the arrows represent the order on (X, <). The
fence illustrates visually how one can traverse from a1 to bn along arrows and through equivalence
classes in X/R, and in that case we say that the fence links b1 to an. The induced relation
S has the property that (a, b) âˆˆS if there exists an R-fence in X linking a to b.
Recall that a cycle in a relation R is a sequence of pairs starting and ending with the same
element: (a, b1), (b1, b2), . . . , (bn, a).
Theorem 15. Let (X, E) be a strict poset, R âˆˆR(X), and let S be the relation on X/R induced
by E. Then the following statements are equivalent:
1. S is a strict partial order on X/R;
2. There are no cycles in S0;
3. qR : (X, E) âˆ’â†’(X/R, S) is order preserving.
Proof. From the deï¬nition of strict posets, they contain no cycles, so 1 â‡’2. Since a non-cyclic
set is irreï¬‚exive, and since S is transitive by construction, 2 â‡’1.
Let qR be order preserving.
Notice that if S0 is the set deï¬ned in (9), we have S0 =
qR Ã— qR(E). In particular, for all x, y âˆˆX for which (x, y) âˆˆE, we have ([x], [y]) âˆˆS0. Assume
that S is not a strict order. Then there is a cycle in S0; that is there are x, y âˆˆX for which
(x, y) âˆˆE, but ([y], [x]) âˆˆS0 also. This yields
âˆƒaâ€², bâ€² âˆˆX : aâ€² âˆ¼x âˆ§bâ€² âˆ¼y âˆ§(bâ€², aâ€²) âˆˆE.
But, since ([x], [y]) âˆˆS0, we also have
âˆƒa, b âˆˆX : a âˆ¼x âˆ§b âˆ¼y âˆ§(a, b) âˆˆE.
This yields a âˆ¼aâ€² and b âˆ¼bâ€², so we have
 qR(a), qR(b)

âˆˆS0 âˆ§qR(b) = qR(bâ€²) âˆ§
 qR(bâ€²), qR(aâ€²)

âˆˆS0.
But, since we have both qR(a) = qR(aâ€²) and (a, b) âˆˆE, this contradicts the fact that qR is
order preserving, so our assumption that both ([x], [y]) and ([y], [x]) are elements of S0 must be
wrong. Hence, if qR is order preserving, there are no cycles in S0, and S is a strict partial order
on X/R. This shows that 3 â‡’1.
Finally, let S be a strict partial order, and assume that qR is not order preserving. Then,
there exists x, y âˆˆX where (x, y) âˆˆE and for which at least one of ([x], [y]) Ì¸âˆˆS or ([y], [x]) âˆˆS
holds. Now, ([x], [y]) âˆˆS by Deï¬nition 14. Therefore, ([y], [x]) âˆˆS implies that S has a cycle,
contradicting the fact that S is a strict partial order.
17

Deï¬nition 16. Let (X, E) be an ordered set. An equivalence relation R âˆˆR(X) is regular if
there exists an order on X/R for which the quotient map is order preserving. We denote the
set of all regular equivalence relations over an ordered set (X, <) by R(X, <). Likewise,
the family of all regular partitions of (X, <) is denoted P(X, <).
In general, for an ordered set (X, <) and a regular equivalence relation R âˆˆR(X, <), we
will denote the induced order relation by <â€².
4.2
The structure of regular equivalence relations
In this section, we establish a suï¬ƒcient and necessary condition for an agglomerative clustering
algorithm to be order preserving. To help in the proof, we employ the concept of crowns (Blyth,
2005):
If (X, <) is a strict poset and the induced order on X/R contains a cycle, then this corres-
ponds to the existence of an R-crown:
b1
b2
bnâˆ’1
bn
Â· Â· Â·
a1
a2
anâˆ’1
an.
That is; the R-crown is a â€œcircularâ€ R-fence.
Recall that, if A âŠ†X, X/A denotes the quotient for which the quotient map qA : X â†’X/A
sends all of A to a point, and is the identity otherwise. That is, for every x, y âˆˆX, we have
qA(x) = qA(y) â‡”x, y âˆˆA.
Theorem 17. If A âŠ†X for a strictly ordered set (X, <), the quotient map qA : X â†’X/A is
order preserving if and only if A is an antichain in (X, <).
Proof. If A is not an antichain, then X/A places comparable elements in the same equivalence
class, so qA is not order preserving.
Assume A is an antichain. If qA is not order preserving, then there is a cycle in (X/A, <â€²),
and since we have only one non-singleton equivalence class, there must exist a crown on the
form
b
A
c
But this means we have a, aâ€² âˆˆA for which b < a and aâ€² < c, but since c < b, this implies
aâ€² < a, contradicting the fact that A is an antichain.
Since a composition of order preserving maps is order preserving, this also applies to a
composition of quotient maps for a chain of regular equivalence relations R1 âŠ†Â· Â· Â· âŠ†Rn.
Combining this with Theorem 17, we have the following:
A clustering of a strictly ordered set will be order preserving if it can be pro-
duced as a sequence of pairwise merges of non-comparable elements.
We close the section with an observation about the family of all hierarchical clusterings over
a strict poset:
18

Theorem 18. For a strictly ordered set (X, <), the set P(X, <) of regular partitions over (X, <)
has S(X) as its least element. Unless < is the empty order, there is no greatest element.
Proof. S(X) is always a regular partition, so S(X) âˆˆP(X, <). And since S(X) is a reï¬nement
of every partition of X, S(X) is the least element of P(X, <).
If the order relation is not empty, then there are at least two elements that are comparable,
and, according to Theorem 17, they cannot be in the same equivalence class. Hence, there is no
greatest element.
The situation of Theorem 18 is depicted in Figure 2, and has already been discussed in
Section 1.2.2: In the case of tied connections that represent mutually exclusive merges, choosing
to merge one connection over the other may lead to very diï¬€erent results. We therefore need a
strategy to select one of these solutions over the others. This will be the main focus of Sections 5
and 6.
5
Partial dendrograms
In this section, we construct the embedding of partial dendrograms into ultrametrics. Let an
ordered dissimilarity space be denoted by (X, <, d). We generally assume that the order
relation is non-empty, meaning that there are comparable elements in (X, <).
Recall that
P(X, <) is the set of all regular partitions over (X, <); that is, the partitions for which the
quotient map is order preserving.
Example 5. Recall the Hasse diagram of regular partitions of the set X = {a, b, c, d} equipped
with the order relation a < b, c < d depicted in Figure 2. Assume that we equip X with the
dissimilarity measure
dX :
b
c
d
a
2.0
1.0
1.3
b
1.0
1.5
c
2.0
.
(10)
For the usual agglomerative hierarchical clustering algorithms, the maximal chains in P(X, <)
will correspond to the partial dendrograms in Figure 4.
1
2
b
c
a
d
a
bc
d
1
2
a
d
b
c
c
ad
b
1
2
a
c
b
d
ac
bd
Figure 4: The partial dendrograms corresponding to the maximal chains in the Hasse diagram
of Figure 2.
The levels in the dendrograms are given by the dissimilarity dX in (10). The
corresponding maximal partitions are displayed below each dendrogram.
A signiï¬cant diï¬€erence from non-ordered sets is that there is no greatest element {X} in
P(X, <).
As a consequence, there are no dendrograms Î¸ : R+ â†’P(X, <), so we
have to provide an alternative:
19

Deï¬nition 19. Let R+ be equipped with the usual total order â‰¤, and let P(X, <) be partially
ordered by partition reï¬nement. A partial dendrogram over (X, <) is an order preserving
map Î¸ : R+ â†’P(X, <) satisfying
P1. Î¸(0) = S(X), the least element of P(X, <).
P2. âˆ€t âˆˆR+ âˆƒÎµ > 0 s.t. Î¸(t) = Î¸(t + Îµ).
We will let Î¸(âˆ) denote the maximal partition in the image of Î¸, and denote the family of
all partial dendrograms over (X, <) by PD(X, <).
Remark 20. If the order relation is non-empty, we have PD(X, <) âˆ©D(X) = âˆ….
The only diï¬€erence between a partial dendrogram, and the deï¬nition of dendrogram given
in Deï¬nition 3, is that we do not any longer require a greatest element to be in the image of Î¸.
However, since P(X, <) is ï¬nite, a partial dendrogram Î¸ âˆˆPD(X, <) is eventually constant;
that is, there exists a positive real number t0 for which
t â‰¥t0 â‡’Î¸(t) = Î¸(âˆ).
Partial dendrograms are clearly a generalisation of dendrograms. To distinguish between
the two, we will occasionally refer to the non-partial dendrograms as complete dendrograms.
As before, we will use the term partial dendrogram to address both the diagrammatic- and
functional representations.
Example 6. The partial dendrogram Î¸ : R+ â†’P(X, <) corresponding to the right hand side
diagram in Figure 4 is deï¬ned as follows:
1
2
a
c
b
d
ac
bd
Î¸(x) =
ï£±
ï£´
ï£²
ï£´
ï£³

{a}, {b}, {c}, {d}
	
for x âˆˆ[0.0, 1.0)

{a, c}, {b}, {d}
	
for x âˆˆ[1.0, 1.5)

{a, c}, {b, d}
	
for x âˆˆ[1.5, âˆ)
5.1
Mapping partial dendrograms to dendrograms
We will now demonstrate, on a high level, how we can construct an ultrametric from a partial
dendrogram in a well deï¬ned manner. Looking at the partial dendrograms of Example 5, each
connected component in the partial dendrograms is a complete dendrogram over its leaf nodes.
Since complete dendrograms map to ultrametrics, each connected component gives rise to an
ultrametric on the subset of X constituted by its leaf nodes:
Example 7. Recalling that any singleton {x} is a trivial ultrametric space with ultrametric
d{x} : (x, x) 7â†’0, the center diagram of Figure 4 provides the following ultrametrics for the
subsets {a, d}, {b} and {c} of X:
d{a,d}(x, y) =
(
0
if x = y,
1.3
otherwise
d{b} = 0
d{c} = 0.
For a disjoint family of ultrametric spaces we have the following result:
20

Lemma 21. Given a family of bounded, disjoint ultrametric spaces {(Xj, dj)}n
j=1 together with
a positive real number K â‰¥maxj {diam(Xj, dj)}, the map
dâˆª:
[
Xj Ã—
[
Xj âˆ’â†’R+
given by
dâˆª(x, y) =
(
dj(x, y)
if âˆƒj : x, yâˆˆXj,
K
otherwise
is an ultrametric on S
j Xj.
Proof. To prove that the ultrametric inequality holds, we start by showing that dâˆª1,2 is an
ultrametric on the restriction to the disjoint union X1 âˆªX2: Let x, y âˆˆX1 and z âˆˆX2, and
choose a positive K â‰¥max{diam(X1, d1), diam(X2, d2)}. We now have
dâˆª1,2(x, z) = K
dâˆª1,2(x, y) = d1(x, y)
dâˆª1,2(y, z) = K.
This means that every triple of points are either already contained in an ultrametric space, or
they make up an isosceles triangle. In both cases, the ultrametric inequality holds, according to
the observation in Example 3.
By induction, we can now prove that
 (X1 âˆªX2) âˆªX3), dâˆª1,2,3

is an ultrametric space, and
so on, until all the (Xj, dj) are included.
Restricting K to be strictly positive makes the above construction work even when each
(Xj, dj) is a trivial ultrametric space, in which case dâˆªbecomes the discrete metric on S
j X.
Turning back to partial dendrograms, assume Î¸ âˆˆPD(X, <), and that the partition Î¸(âˆ) is
given by B = {Bj}m
j=1. That is; the number of connected components in the partial dendrogram
of Î¸ is m: one connected component for each block in the coarsest partition.
Let dX|Bj be the restriction of dX to Bj Ã— Bj, and, likewise, let <|Bj be the order relation
induced on Bj by <.
Each space (Bj, < |Bj, dX|Bj) is an ordered dissimilarity subspace of
(X, <, dX), and each Bj has a complete dendrogram over all of its points.
Since complete
dendrograms correspond to ultrametrics, each connected component in the partial dendrogram
of X corresponds to an ultrametric space (Bj, du
j ). Furthermore, since the {(Bj, du
j )}m
i=1 make up
a disjoint family of ultrametric spaces covering X, we can use Lemma 21 to deï¬ne an ultrametric
on all of X as follows:
Pick a K â‰¥maxj{diam(Bj, du
j )}, and deï¬ne uÎ¸ : X Ã— X â†’R+ by
uÎ¸(x, y) =
(
du
j (x, y)
if âˆƒj : x, yâˆˆBj,
K
otherwise.
(11)
Example 8. An illustration of how this construction turns out in the case of the partial dendro-
grams of Example 5 is provided in Figure 5:
1
2
b
c
a
d
a
bc
d
1
2
a
d
b
c
c
ad
b
1
2
a
c
b
d
ac
bd
Figure 5: â€œCompletedâ€ dendrograms corresponding to the partial dendrograms of Example 5,
using K = 2.0. The completions are marked by the dashed lines.
21

5.2
Ultrametric completions
We will now formalise the above construction in terms of a function from partial dendrograms to
complete dendrograms. We will also present necessary and suï¬ƒcient conditions for this function
to be injective. Injectivity is not strictly required for the theory to work, but it signiï¬cantly
increases the discriminative power of the theory. Without injectivity, the families of optimal
solutions may include clusterings of rather appalling quality. An example is provided towards
the end of the section.
We deï¬ne the diameter of a partial dendrogram Î¸ to be the number
diam(Î¸) = sup{x âˆˆR+ | Î¸(x) Ì¸= Î¸(âˆ)}.
Deï¬nition 22. Given an ordered space (X, <) and a positive real number Îµ, we deï¬ne the
ultrametric completion on Îµ to be the map UÎµ : PD(X, <) âˆ’â†’U(X) for which
UÎµ : Î¸ 7â†’uÎ¸,
where uÎ¸ is deï¬ned as in (11), setting K = diam(Î¸) + Îµ.
From the discussion in Section 5.1, we know that this is well-deï¬ned, but we want a concrete
function describing the mapping. We already have the map Î¨X : D(X) âˆ’â†’U(X) from (3),
mapping dendrograms to ultrametrics. We therefore seek a map
ÎºÎµ : PD(X, <) âˆ’â†’D(X)
making the following diagram commute:
D(X)
U(X)
PD(X, <)
Î¨X
ÎºÎµ
UÎµ
.
(12)
Seeing that ÎºÎµ must map partial dendrograms to complete dendrograms, a quick glance at
Figure 5 suggests the following deï¬nition:
ÎºÎµ(Î¸)(x) =
(
Î¸(x)
for x < diam(Î¸) + Îµ
{X}
otherwise.
It is straightforward to check that ÎºÎµ(Î¸) is a complete dendrogram.
Theorem 23. Î¨X â—¦ÎºÎµ = UÎµ. That is; the diagram in (12) commutes.
Proof. Assume ï¬rst that Î¸ âˆˆPD(X, <) is a proper partial dendrogram, and that im(Î¸) =
{Bi}n
i=0. Let the coarsest partition in the image of Î¸ be given by Bn = {Bj}m
j=1. That is, each
block Bj corresponds to a connected component in the partial dendrogram. Pick a block B âˆˆBn
and assume x, y âˆˆB.
If
k = min{ i âˆˆN | âˆƒBâ€² âˆˆBi : B = Bâ€² },
then Bk is the ï¬nest partition containing all of B in one block. Since B âŠ†X, the partitions
BB
i
= { B âˆ©Bâ€² | Bâ€² âˆˆBi }
for 1 â‰¤i â‰¤k
22

constitute a chain in P(B) containing both S(B) and {B}. Hence, we can construct a complete
dendrogram over B by deï¬ning
Î¸B(x) = { B âˆ©Bâ€² | Bâ€² âˆˆÎ¸(x) }.
(13)
This is exactly the complete dendrogram corresponding to the connected component of the tree
over X having the elements of B as leaf nodes. By Deï¬nition 22,
x, y âˆˆB â‡’UÎµ(Î¸)(x, y) = Î¨B(Î¸B)(x, y).
(14)
Due to (13), we have
x, y âˆˆB â‡’(âˆƒB âˆˆÎ¸B(x) : x, y âˆˆB â‡”âˆƒBâ€² âˆˆÎ¸(x) : x, y âˆˆBâ€²)
â‡’min{ t âˆˆR+ | âˆƒB âˆˆÎ¸B(t) : x, y âˆˆB }
= min{ t âˆˆR+ | âˆƒBâ€² âˆˆÎ¸(t) : x, y âˆˆBâ€² }.
Hence, by the deï¬nition of Î¨X in (3) we conclude that
x, y âˆˆB â‡’Î¨B(Î¸B)(x, y) = (Î¨X â—¦ÎºÎµ)(Î¸)(x, y).
Combining this with (14), we get that whenever x, y âˆˆB, we have Î¨X â—¦ÎºÎµ = UÎµ.
On the other side, let x âˆˆBi and y âˆˆBj with i Ì¸= j. By deï¬nition, we have UÎµ(Î¸)(x, y) =
diam(Î¸) + Îµ. And, since there is no block in Î¸(âˆ) containing both x and y, we ï¬nd that the
minimal partition in im(ÎºÎµ(Î¸)) containing x and y in one block is {X}. But this means that
Î¨X(ÎºÎµ(Î¸))(x, y) = diam(Î¸) + Îµ, so Î¨X â—¦ÎºÎµ = UÎµ holds in this case too.
Finally, if Î¸ is a complete dendrogram, we have ÎºÎµ(Î¸) = Î¸, so Î¨X â—¦ÎºÎµ(Î¸) = Î¨X(Î¸). But
since Î¸(âˆ) = {X}, it follows that UÎµ(Î¸) maps exactly to the ultrametric over X deï¬ned by
Î¨X(Î¸).
Theorem 24. Given a strict poset (X, <) with a non-empty order relation and a positive real
number Îµ, the map
UÎµ : PD(X, <) âˆ’â†’U(X)
is injective.
Proof. Since UÎµ = Î¨K â—¦ÎºÎµ and Î¨X is a bijection, injectivity follows if ÎºÎµ is injective. Assume
that ÎºÎµ(Î¸) = ÎºÎµ(Î¸â€²). Then, for every x < diam(Î¸) + Îµ, we have
ÎºÎµ(Î¸)(x) = ÎºÎµ(Î¸â€²)(x) â‡”Î¸(x) = Î¸â€²(x).
Example 9. If Îµ is not chosen to be strictly positive, the map UÎµ will not necessarily be
injective. Let Îµ = 0 and consider the two partial dendrograms
1
a
b
c
d
1
a
b
c
d
Both are mapped to the following dendrogram via Îº0:
23

1
a
b
c
d
This demonstrates what we mean by reduced discriminative power in the case of a non-injective
completion. Since the partial dendrograms exhibit distinctively diï¬€erent information, it is de-
sirable that the methodology distinguishes between them.
6
Hierarchical clustering of ordered sets
We are now ready to embark on the speciï¬cation of order preserving hierarchical clustering
of ordered sets. We do this by extending our notion of optimised hierarchical clustering from
Section 3.
For an ordered set (X, <), recall that non-comparability of a, b âˆˆX is denoted aâŠ¥b. We
introduce the non-comparable separation of (X, <, d), deï¬ned as
sepâŠ¥(X, <, d) =
min
x,yâˆˆX{ d(x, y) | x Ì¸= y âˆ§xâŠ¥y }.
Consider the following modiï¬cation of classical hierarchical clustering. The only diï¬€erence is
that for each iteration, we check that there are elements that actually can be merged while
preserving the order relation; according to Theorem 17, merging a pair of non-comparable
elements produces a regular quotient. Recall that S(X) denotes the singleton partition of X.
Let (X, <, d) be given together with a linkage function L.
1. Start by setting Q0 = S(X), and endow Q0 with the induced order relation <0.
2. Among the pairs of non-comparable clusters, pick a pair of minimal dissimilarity according
to L, and combine them into one cluster by taking their union.
3. Endow the new clustering with the induced order relation.
4. If all elements of X are in the same cluster, or if all clusters are comparable, we are done.
Otherwise, go to Step 2 and continue.
The procedure results in a chain of ordered partitions {(Qi, <i)}m
i=0 together with the dis-
similarities {Ïi}m
i=0 at which the partitions where formed.
And the sequence of pairs Q =
{(Qi, Ïi)}m
i=0 maps to a partial dendrogram through (1) if the following lemma is satisï¬ed:
Lemma 25. The sequence of pairs {(Qi, Ïi)}m
i=0 maps to a partial dendrogram through applic-
ation of (1) if and only if
sepâŠ¥(Qi, <i, L) â‰¤sepâŠ¥(Qi+1, <i+1, L).
Since the singleton partition Q0 maps to a partial dendrogram, the algorithm will produce a
partial dendrogram for any ordered dissimilarity space, and since there can be at most |X| âˆ’1
merges, the procedure always terminates.
As for classical hierarchical clustering, the procedure is non-deterministic in the sense that
given a set of tied pairs, we select a random pair for the next merge. Hence, the procedure is
able to produce all possible partial dendrograms for all possible tie resolution strategies:
24

Deï¬nition 26. Given an ordered dissimilarity space (X, <, d) and a linkage function L, let the
set of all possible outputs from the above procedure be denoted by DL(X, <, d).
The set DL(X, <, d) diï¬€ers from DL(X, d) in two important ways:
â€¢ DL(X, <, d) contains partial dendrograms, not dendrograms
â€¢ The cardinality of DL(X, <, d) is at least that of DL(X, d), and often higher, due to
mutually exclusive merges and the â€œdead endsâ€ in P(X, <) (see Figure 2).
Even for single linkage we have
DSL(X, <, d)
 > 1 if there are mutually exclusive tied connec-
tions.
In the spirit of optimised hierarchical clustering, we suggest the following deï¬nition, employ-
ing the ultrametric completion UÎµ from Deï¬nition 22:
Deï¬nition 27. Given an ordered dissimilarity space (X, <, d), together with a linkage func-
tion L, let Îµ > 0. An order preserving hierarchical agglomerative clustering using L
and Îµ is given by
HC<L
opt,Îµ(X, <, d) =
arg min
Î¸âˆˆDL(X,<,d)
âˆ¥UÎµ(Î¸) âˆ’dâˆ¥p.
(15)
Our next result shows that if we remove the order relation, then optimised clustering and
order preserving clustering coincide.
Keep in mind that a dissimilarity space is an ordered
dissimilarity space with an empty order relation; that is, (X, d) = (X, âˆ…, d).
Theorem 28. If the order relation is empty, then order preserving optimised hierarchical clus-
tering and optimised hierarchical clustering coincide:
HC<L
opt,Îµ(X, âˆ…, d) = HCL
opt(X, d).
Proof. First, notice that
âˆ€(Q, <Q) âˆˆP(X, âˆ…) : sepâŠ¥(Q, <Q, L) = sep(Q, L),
where <Q denotes the (trivial) induced order. Hence, we have DL(X, âˆ…, d) = DL(X, d). Since
UÎµ|D(X) = Î¨X, the result follows.
6.1
On the choice of Îµ
In HC<L
opt,Îµ(X, <, d) we identify the elements from DL(X, <, d) that are closest to the dissimilarity
measure d when measured in the p-norm. Since UÎµ : PD(X, <) â†’U(X) is injective, UÎµ induces
a relation âª¯d,Îµ on PD(X, <) deï¬ned by
Î¸ âª¯d,Îµ Î¸â€² â‡”âˆ¥UÎµ(Î¸) âˆ’dâˆ¥p â‰¤âˆ¥UÎµ(Î¸â€²) âˆ’dâˆ¥p,
and the optimisation ï¬nds the minimal elements under this order.
The choice of Îµ may aï¬€ect the ordering of dendrograms under âª¯d,Îµ. We will show this by
providing an alternative formula for âˆ¥u âˆ’dâˆ¥p that better expresses the eï¬€ect of the choice of Îµ
in the ultrametric completion: Assume that Î¸ is a partial dendrogram over (X, <), and let
Î¸(âˆ) = {Bi}m
i=1. Furthermore, let ni = |Bi| for 1 â‰¤i â‰¤m be the cardinalities of the blocks,
and let the corresponding ultrametric be given by u = UÎµ(Î¸). The sum in the standard formula
25

for âˆ¥u âˆ’dâˆ¥p given by (4) can be split in two: the intra-block diï¬€erences and the inter-block
diï¬€erences. The intra-block diï¬€erences are independent of Îµ, and are given by
Î± =
m
X
i=1
X
x,yâˆˆBi
|u(x, y) âˆ’d(x, y)|p.
(16)
On the other hand, for every inter-block pair (x, y), we have u(x, y) = diam(Î¸) + Îµ, so the
inter-block diï¬€erences, that are dependent on Îµ, can be computed as
Î²Îµ =
X
(x,y)âˆˆBiÃ—Bj
iÌ¸=j
|diam(Î¸) + Îµ âˆ’d(x, y)|p.
(17)
We can now write âˆ¥u âˆ’dâˆ¥p =
pâˆšÎ± + Î²Îµ. If we think of u as an approximation of d, and saying
that |X| = N, the mean p-th error of this approximation can be expressed as a function of Îµ:
Ed(Îµ|Î¸, p) =
1
N âˆ¥u âˆ’dâˆ¥p
p =
Î±
N +
1
N
X
(x,y)âˆˆBi Ã—Bj
iÌ¸=j
|diam(Î¸) + Îµ âˆ’d(x, y)|p.
Notice that the minimisation in (15) exactly identiï¬es the partial dendrograms
of minimal mean p-th error.
Moreover, X being ï¬nite means that d is bounded, so Ed(Îµ|Î¸, p) â†’âˆwhen Îµ â†’âˆ. Hence,
Ed(Îµ|Î¸, p) has at least one global minimum on [0, âˆ).
Theorem 29. Diï¬€erent choices of Îµ may result in diï¬€erent orders on the dendrograms.
Proof. Let Î¸1 be a partial dendrogram over (X, <, d), and let {Îµ1}n
i=1 be the set of strictly
positive global minima of Ed(Îµ|Î¸1, p), assuming n â‰¥1.
Then Î¸1 is a minimal element in
(PD(X, <), âª¯Îµi,p) for 1 â‰¤i â‰¤n. Assume that there exists a partial dendrogram Î¸2 âˆˆPD(X, <)
that is not minimal in (PD(X, <), âª¯Îµi,p) for any of the Îµi, so that Î¸1 â‰ºÎµi,p Î¸2. Assume fur-
ther that there is an Îµâ€² > 0 that is a global minimum of Ed(Îµ|Î¸2, p) so that Î¸2 is minimal in
(PD(X, <), âª¯Îµâ€²,p). Since Îµâ€² Ì¸= Îµi for 1 â‰¤i â‰¤n, we get Î¸2 â‰ºÎµâ€²,p Î¸1.
The question now is which value to pick for Îµ.
From the formula for Ed(Îµ|Î¸, p), we see
that when Îµ becomes large, the inter-block diï¬€erences dominate the approximation
error. For increasing Îµ, having low error eventually equals having few inter-block
pairs. Alternatively: the intra-block diï¬€erences have insigniï¬cant inï¬‚uence on the
approximation error for large Îµ.
The eï¬€ect of this is that, when Îµ increases, the partial dendrograms close to d will be
those that have a low number of inter-block pairs, regardless of the quality of the intra-block
approximations of d. From the standpoint of ultrametric ï¬tting, this is intuitively wrong. Also,
it will lead to clusterings where as many elements as possible are placed in one large cluster,
since this is the most eï¬€ective method for reducing the number of inter-block pairs.
On the other side, a low value of Îµ will move the weight towards improving the approximation
of the intra-block distances. Again from the standpoint of ultrametric ï¬tting, this is the right
thing to do. Also, since the inter-block distances are between non-mergeable pairs, one may
claim that these diï¬€erences should be given less attention in the ultrametric ï¬tting.
This points towards selecting a low value for Îµ. In the process of choosing, we have the
following result at our aid:
26

Theorem 30. For any ï¬nite ordered dissimilarity space (X, <, d) and linkage function L, there
exists an Îµ0 > 0 for which
Îµ, Îµâ€² âˆˆ(0, Îµ0) â‡’
 DL(X, <, d), âª¯d,Îµ) â‰ˆ
 DL(X, <, d), âª¯d,Îµâ€²).
That is; all Îµ âˆˆ(0, Îµ0) induce the same order on the partial dendrograms.
Proof. Since X is ï¬nite, DL(X, <, d) is also ï¬nite. And according to Ed(Îµ|Î¸, p), if the cardinality
of DL(X, <, d) is n, there are at most pn positive values of Îµ that are distinct global minima
of partial dendrograms in DL(X, <, d). But this means there is a ï¬nite set of Îµ for which the
order on (DL(X, <), âª¯Îµ,p) changes. And since all these values are strictly positive, they have a
strictly positive lower bound.
It is, of course, possible to play with diï¬€erent values of Îµ to obtain diï¬€erent results. But,
since Îµ+diam(Î¸) is an upper bound of the partial dendrogram to begin with, we generally advise
as follows:
We suggest to use a value of Îµ that is as small as possible.
6.2
Idempotency of HC<L
opt,Îµ
A detailed axiomatic analysis along the lines of for example Ackerman and Ben-David (2016) is
beyond the scope of this paper, and is considered for future work. We still include a proof of
idempotency of HC<L
opt,Îµ, since this is an essential property of classical hierarchical clustering.
A function f is idempotent if f â—¦f = f. For classical hierarchical clustering, the set of
ultrametrics U(X) âŠ†M(X) over a set X are ï¬xed points under the map
(Î¨X â—¦HCL(X, âˆ’)) : M(X) â†’U(X).
In particular, if u = Î¨X â—¦HCL(X, d) for some d âˆˆM(X), since u âˆˆU(X), this yields u =
Î¨X â—¦HCL(X, u). This property is what Jardine and Sibson (1971) refers to as appropriateness,
being the ï¬rst of a set of conditions expected fulï¬lled by clustering methods.
This property does necessarily depend on the linkage function. We say that L is a convex
linkage function if we always have
SL(p, q, d) â‰¤L(p, q, d) â‰¤CL(p, q, d).
Now, if u is an ultrametric on X, the ultrametric inequality yields
u(a, b) = sep(X, u) â‡’âˆ€c âˆˆX : u(a, c) = u(b, c).
So if L is a convex linkage function and u(a, b) = sep(X, u), we have
L({a, b}, {c}) = L({a}, {c}) = L({b}, {c})
âˆ€c Ì¸= a, b.
This is to say that a convex linkage function preserves the structure of the original ultrametric
when minimal dissimilarity elements are merged. As a result, we get that DL(X, u) contains
exactly one element, namely the dendrogram that corresponds to the ultrametric.
Theorem 31. For a convex linkage function L, an ultrametric u âˆˆU(X) and Î¸ = Î¨âˆ’1
X (u), we
have HCL
opt(X, u) = {Î¸}.
27

Hence, all of U(X) are ï¬xed points under Î¨X â—¦HCL
opt(X, âˆ’) whenever L is convex.
For ordered spaces, the case is diï¬€erent. It is easy to construct an ordered ultrametric space
(X, <, u) for which u(a, b) = sep(X, u) and a < b, in which case the ultrametric cannot be
reproduced. Hence, all of U(X) cannot be ï¬xed points under UÎµ â—¦HC<L
opt,Îµ(X, <, âˆ’), but the
mapping is still idempotent:
Theorem 32 (Idempotency). For an ordered dissimilarity space (X, <, d) and a convex linkage
function L, if Î¸ âˆˆHC<L
opt,Îµ(X, <, d) and UÎµ(Î¸) = u, then HC<L
opt,Îµ(X, <, u) = {Î¸}.
Proof. Let Î¸(âˆ) = {Bi}m
i=1. Then each Bi is an antichain in (X, <), so we have
âˆ€x, y âˆˆBi : sep(Bi, u|Bi) = sepâŠ¥(Bi, u|Bi)
for 1 â‰¤i â‰¤m.
Since Îµ > 0, we also have
x, y âˆˆBi â‡’u(x, y) < diam(X, u)
for 1 â‰¤i â‰¤m.
And, lastly, since every pair of comparable elements are in pairwise diï¬€erent blocks, we have
x < y âˆ¨y < x â‡’u(x, y) = diam(X, u).
Now, since L is convex, based on the discussion before Theorem 31, the intra-block structure
of every block will be preserved. And, since every inter-block dissimilarity is accompanied by
comparability across blocks, the procedure for generation of DL(X, <, d) will exactly reproduce
the intra block structure of all blocks and then halt. Hence, DL(X, <, d) = {Î¸}.
6.3
On the computational complexity of HC<L
opt,Îµ
Due to Theorem 28, HC<CL
opt,Îµ is NP-complete. The complexity classes of HC<SL
opt,Îµ and HC<AL
opt,Îµ are
not known. The problems share similarity with several known NP-complete problems, but no
viable reduction has been found so far.
Typical similar problems, to mention just a few, are acyclic partition, and precedence con-
strained scheduling (Garey and Johnson, 1979), and also oriented circuit free graph coloring (Culus and Demange,
2006). The similarity lies in the fact that HC<L
opt,Îµ produces an acyclic partition of the type that
is asked for, but the mentioned problems put constraints on the number of blocks, or the sizes
of the blocks, in the partition. As of now, it is not obvious to us how to do this for HC<L
opt,Îµ.
The main tool for manipulation of the algorithm is the dissimilarity measure, and that does not
allow us to constrain this aspect of the partition directly.
7
Polynomial time approximation
This section describes a polynomial time approximation scheme for HC<L
opt,Îµ, and provides a
demonstration of the eï¬ƒcacy of the approximation on synthetic random data. We start by
describing the approximation scheme, before introducing the random data model and how we
can measure the quality of the approximation. We then proceed by presenting the performance
of the approximation scheme on families of random datasets in terms of plots, displaying the
convergence towards the optimum as a function of the number of random samples.
28

7.1
The approximation model
Recall the set DL(X, <, d) of partial dendrograms over (X, <, d) from Deï¬nition 26. The al-
gorithm for producing a random element of DL(X, <, d) is described at the beginning of Sec-
tion 6; the key is to pick a random pair for merging whenever we encounter a set of tied
connections.
Our approximation model now becomes very simple: we generate a family of random partial
dendrograms over (X, <, d), and choose the one with the best ultrametric ï¬t:
Deï¬nition 33. Let (X, <, d) be given, and let N be a positive integer.
For any random
selection of N partial dendrograms {Î¸i}i from DL(X, <, d), an N-fold approximation of
HC<L
opt,Îµ(X, <, d) is a partial dendrogram Î¸ âˆˆ{Î¸i}i minimising âˆ¥UÎµ(Î¸) âˆ’dâˆ¥p.
7.1.1
Running time complexity
As mentioned above, the algorithm for producing one random partial dendrogram is described
at the beginning of Section 6.
Assume that |X| = n. In the worst case, we may have to check
 n
2

pairs to ï¬nd one that is
not comparable, and the test for aâŠ¥b has complexity O(n2), leading to a complexity of O(n4)
of ï¬nding a mergeable pair. Since there are up to n âˆ’1 merges, the worst case estimate of the
running time complexity for producing one element in DL(X, <, d) is O(n5).
As a function of the order density.
A part of this estimate is the number of comparability
tests we have to perform in order to ï¬nd a mergeable pair. For a sparse order relation, we
may have to test signiï¬cantly less than
 n
2

pairs before ï¬nding a mergeable pair: if K is the
expected number of test we have to do, the resulting complexity of ï¬nding a mergeable pair
becomes O(Kn2). This yields a total expected algorithmic complexity of O(Kn3). Notice that
limKâ†’1 O
 Kn3
= O(n3), which is the running time complexity of classical HC. So, if the
order relation is sparse, we can generally expect the algorithm to execute signiï¬cantly faster
than the worst case estimate.
Parallel execution.
When producing an N-fold approximation, the N random partial dendro-
grams can be generated in up to N parallel processes, reducing the computational time of the
approximation. For the required number of dendrograms to obtain a good approximation, please
see the below demonstration.
7.2
Random ordered dissimilarity spaces
This section gives a concise description of the random model used to generate ordered dissimil-
arity spaces. The model consists of two parts: the random order and the random dissimilarity
measure.
7.2.1
Random partial order
A partial order is equivalent to a transitively closed directed acyclic graph, and since any acyclic
graph has a unique transitive closure, we can use any random model for directed acyclic graphs
to generate random partial orders. We choose to use the classical ErdËos-RÂ´enyi random graph
model (BollobÂ´as, 2001). Recall that a directed acyclic graph on n vertices is a binary n Ã— n
adjacency matrix that is permutation similar a strictly upper triangular matrix; that is, there
exists a permutation that, when applied on both rows and columns of one matrix, transforms it
29

into the other. Let this family of nÃ—n matrices be denoted by A(n). For a number p âˆˆ[0, 1], the
sub-family A(n, p) âŠ†A(n) is deï¬ned as follows: for A âˆˆA(n), let Aâ€² be strictly upper triangular
and permutation similar to A. Then each entry above the diagonal of Aâ€² is 1 with probability
p. The sought partial order is the transitive closure of this graph; we denote the corresponding
set of transitively closed directed acyclic graphs by A(n, p).
7.2.2
Random dissimilarity measure
If |X| = n, a dissimilarity measure over X with no tied connections consists of
 n
2

distinct
values. Hence, any permutation of the sequence {1, . . . ,
 n
2

} is a non-tied random dissimilarity
measure over X.
To generate tied connections, let t â‰¥1 be the expected number of ties per level. That is,
for each unique value in the dissimilarity measure, that value is expected to have multiplicity t.
In the case where t does not divide
 n
2

, we resolve this by setting the multiplicity of the largest
dissimilarity to
  n
2

mod t

.
We write D(n, t) to denote the family of random dissimilarity measures over sets of n elements
with an expected number of t ties per level, in the above sense.
Deï¬nition 34. Given positive integers n and t, together with p âˆˆ[0, 1], the family of random
ordered dissimilarity spaces generated by (n, p, t) is given by
O(n, p, t) = A(n, p) Ã— D(n, t).
7.3
A measure of cluster quality
A large body of literature exists on the topic of comparing clusterings (see for instance (Vinh et al.,
2010) for a brief review). In our demonstration, we use the rather popular adjusted Rand in-
dex (Hubert and Arabie, 1985) to measure the ability of the approximation in ï¬nding a decent
partition, comparing against the optimal result.
Less work is done on this type of comparison for partial orders and directed acyclic graphs.
We suggest to use a modiï¬ed version of the adjusted Rand index for this purpose too, based
on an adaptation of the Rand index used for network analysis (Hoï¬€man et al., 2015). For an
introduction to the Rand index, and also to some of the versions of the adjusted Rand index,
see (Rand, 1971; Hubert and Arabie, 1985; Gates and Ahn, 2017).
7.3.1
Adjusted Rand index for partition quality
We use the adjusted Rand index (ARI) to compute the eï¬ƒcacy of the approximation in ï¬nding
a partition close to the optimum, when we have an optimal solution to compare against. This
corresponds to what Gates and Ahn (2017) refers to as a one sided Rand index, since one of
the partitions are given as a reference, whereas the other is drawn from some distribution. In
the below demonstration, we assume that the approximating partition is drawn from the set of
all partitions over X under the uniform distribution.
7.3.2
Adjusted Rand index for induced order relations
When comparing induced orders on partitions over a set, unless the partitions coincide, it is not
obvious which blocks in one partition correspond to which blocks in the other. To overcome this
problem, we base our measurements on the base space projection:
30

Deï¬nition 35. For an ordered set (X, E) and a partition Q of X with induced order Eâ€², the
base space projection of (Q, Eâ€²) onto X is the order relation EQ on X deï¬ned as
(x, y) âˆˆEQ â‡”([x], [y]) âˆˆEâ€².
This allows us to compare the induced orders in terms of diï¬€erent orders on X. Notice that if
the induced order Eâ€² is a [strict] partial order on Q, then EQ is a [strict] partial order on X.
Hoï¬€man et al. (2015) demonstrate that the adjusted Rand index can be used to detect
missing links in networks. The concept relies on the fact that a network link and a link in
an equivalence relation are not that diï¬€erent: Both networks and equivalence relations are
special classes of relations, and the Rand index simply counts the number of coincidences and
mismatches between the two. A necessary criterion for this way of thinking about the Rand
index requires that the compared networks have known and ï¬xed labels, so that we know which
vertices map to which vertices (Warrens, 2008).
We can extend on this method, to develop an order relation similarity measure. Let A and
B be the adjacency matrices of two base space projections, and let Ai denote the i-th row of A,
and likewise for Bi. If âŸ¨a, bâŸ©is the inner product of a and b, we deï¬ne
ai = âŸ¨Ai, BiâŸ©
ci = âŸ¨Ai, 1 âˆ’BiâŸ©
bi = âŸ¨1 âˆ’Ai, BiâŸ©
di = âŸ¨1 âˆ’Ai, 1 âˆ’BiâŸ©.
Here, ai is the number of common direct descendants of i in both relations, bi is the number of
descendants of i found in A but not in B, ci is the number of descendants of i in B but not in
A, while di counts the common non-descendants of i in the two relations.
Notice that we compare the i-th row in A only to the i-row in B since these rows corres-
pond to the same element in the set X. This diï¬€ers from the computation of the Rand index
in (Hoï¬€man et al., 2015), where aij, bij, cij and dij are computed for 1 â‰¤i, j â‰¤n. We may now
compute the adjusted order Rand index (Â¯oARI) for the pair of relations as
Â¯oARI(A, B) = 1
n
n
X
i=1
2(aidi âˆ’bici)
(ai + bi)(bi + di) + (ai + ci)(ci + di).
(18)
7.3.3
Normalised ultrametric ï¬t
A natural choice of quality measure is to report the ultrametric ï¬t âˆ¥UÎµ(Î¸) âˆ’dâˆ¥p of the obtained
partial dendrogram Î¸, especially if we can compare it to the ultrametric ï¬t of the optimal
solution. The scale of the ultrametric ï¬t depends heavily on both the size of the space and the
order of the norm, so we choose to normalise. Also, we invert the normalised value, so that the
optimal ï¬t has a value of 1, and a worst possible ï¬t has value 0. This makes it easy to compare
the convergence of the ultrametric ï¬t to the convergence of the ARI and Â¯oARI.
Deï¬nition 36. Given a set of partial dendrograms {Î¸i} over (X, <, d), let their respective
ultrametric ï¬ts be given by Î´i = âˆ¥UÎµ(Î¸i) âˆ’dâˆ¥p. The normalised ultrametric ï¬t are the
corresponding values
Ë†Î´i = 1 âˆ’
Î´i âˆ’mini{Î´i}
maxi{Î´i} âˆ’mini{Î´i}.
In the presence of a reference solution, we substitute mini{Î´i} with the ultrametric ï¬t of the
reference.
31

7.3.4
ARI vs ultrametric ï¬t
We could measure the quality of the clustering in terms of the ultrametric ï¬t alone. However,
we do not know the distribution of the ultrametric ï¬t, so as a stand-alone value, this provides
little information. Using ARI and Â¯oARI to measure the quality of the clustering gives us a
measure that is, at least, adjusted for chance.
On the other hand, the ARI and Â¯oARI only compare the ï¬nal partition and order relative
the reference, and not the full hierarchy. The reference partition can be reached through diï¬€erent
sequences of merges, and neither AL nor CL are invariant in this respect. The hierarchy and
the merge threshold values are reï¬‚ected in the ultrametric, and deviations in the hierarchy will
therefore be reported by the ultrametric ï¬t.
7.4
Demonstration
This section demonstrates the eï¬ƒcacy of the polynomial time approximation of HC<L
opt,Îµ. The
demonstration is split in two: In the ï¬rst part, we demonstrate the eï¬ƒcacy of the approximation
relative a known optimal solution.
In the second part, we demonstrate the convergence of
ultrametric ï¬t for larger spaces with much larger numbers of tied connections; spaces for which
the optimal algorithm does not terminate within any reasonable time.
For each parameter combination, a set of 30 random ordered dissimilarity spaces are gen-
erated. For each space, the optimal solution is found, and 100 approximations are generated
according to the prescribed procedure.
We then bootstrap the approximations to generate
N-fold approximations for diï¬€erent N.
7.4.1
Results
We present the results in terms of convergence plots, showing the eï¬ƒcacy of the approximation
as a function of the sample size.
For the results where a reference solution is available, the plots contain three curves:
E(ARI)
- The expected adjusted Rand index of the approximated partition.
E(Â¯oARI) - The expected adjusted Rand index of the approximated induced order.
norm.fit - The mean of the normalised ï¬t.
For the results where no reference solution is available, we present the distribution of the
normalised ï¬t.
The results are presented in Figures 6, 7, 8 and 10 on pages 33, 34, 34 and 35, respectively.
The parameter settings corresponding to the ï¬gures are given in Table 1 for easy reference, and
are also repeated in the ï¬gure text.
The parameters have been chosen to illustrate how the algorithm behaviour changes with
changing expected number of ties, changing link probability in the random partial order, and
choice of linkage function.
In general, a large expected number of tied connections requres
larger sample size for the approximation, while a more dense order relation (higher value of p
in O(n, p, t)) seems to require a smaller sample compared to a more sparse relation. Regarding
choice of linkage function, the approximation is very eï¬ƒcient for both SL and AL, while CL
shows a signiï¬cant degeneration in approximation for larger numbers of tied connections.
32

n
L
p
t
has reference
Figure 6
200
SL, AL, CL
0.01, 0.02, 0.05
5
yes
Figure 7
200
SL, AL, CL
0.05
3, 7
yes
Figure 8
500
SL, AL, CL
0.01
10, 50, 100
no
Figure 9
500
SL, AL, CL
0.05
50, 100
no
Figure 10
500
SL, AL, CL
0.10
100
no
Table 1: Parameter settings for the demonstrations. The right-most column indicates whether
the reference clustering is available or not.
SL, p = 0.01
2
4
6
8
10
0.6
0.8
1
1
AL, p = 0.01
2
4
6
8
10
0.6
0.8
1
1
CL, p = 0.01
2
4
6
8
10
0.6
0.8
1
1
SL, p = 0.02
2
4
6
8
10
0.6
0.8
1
1
AL, p = 0.02
2
4
6
8
10
0.6
0.8
1
1
CL, p = 0.02
2
4
6
8
10
0.6
0.8
1
1
SL, p = 0.05
2
4
6
8
10
0.6
0.8
1
1
AL, p = 0.05
2
4
6
8
10
0.6
0.8
1
1
CL, p = 0.05
2
4
6
8
10
0.6
0.8
1
1
E(ARI);
E(Â¯oARI);
norm.fit
Figure 6: Eï¬ƒcacy for n = 200 and t = 5 with p âˆˆ{0.01, 0.02, 0.05}. The ï¬rst axis is the size of
the drawn sample.
33

SL, t = 3
2
4
6
8
10
0.6
0.8
1
1
AL, t = 3
2
4
6
8
10
0.6
0.8
1
1
CL, t = 3
2
4
6
8
10
0.6
0.8
1
1
SL, t = 7
2
4
6
8
10
0.6
0.8
1
1
AL, t = 7
2
4
6
8
10
0.6
0.8
1
1
CL, t = 7
2
4
6
8
10
0.6
0.8
1
1
E(ARI);
E(Â¯oARI);
norm.fit
Figure 7: Eï¬ƒcacy for n = 200 and p = 0.05 with t âˆˆ{3, 7}. The ï¬rst axis is the size of the
drawn sample. The plots for t = 5 can be found in the bottom row of Figure 6.
SL, t = 10
2
4
6
8
10
0.6
0.8
1
1
AL, t = 10
2
4
6
8
10
0.6
0.8
1
1
CL, t = 10
2
4
6
8
10
0.6
0.8
1
1
SL, t = 50
2
4
6
8
10
0.6
0.8
1
1
AL, t = 50
10
20
0.6
0.8
1
1
CL, t = 50
20
40
60
80
0.6
0.8
1
1
SL, t = 100
2
4
6
8
10
0.6
0.8
1
1
AL, t = 100
10
20
0.6
0.8
1
1
CL, t = 100
20
40
60
80
0.6
0.8
1
1
[Q1 âˆ’1.5IQR, Q3 + 1.5IQR];
[Q1, Q3];
median
Figure 8: Polynomial approximation rate for n = 500, P = 0.01 and t âˆˆ{10, 20, 40}. The ï¬rst
axis is the size of the drawn sample.
34

SL, t = 50
2
4
6
8
10
0.6
0.8
1
1
AL, t = 50
2
4
6
8
10
0.6
0.8
1
1
CL, t = 50
2
4
6
8
10
0.6
0.8
1
1
SL, t = 100
2
4
6
8
10
0.6
0.8
1
1
AL, t = 100
2
4
6
8
10
0.6
0.8
1
1
CL, t = 100
2
4
6
8
10
0.6
0.8
1
1
[Q1 âˆ’1.5IQR, Q3 + 1.5IQR];
[Q1, Q3];
median
Figure 9: Polynomial approximation rate for n = 500, p = 0.05 and t âˆˆ{50, 100}. The ï¬rst axis
is the size of the drawn sample.
SL, t = 100
2
4
6
8
10
0.6
0.8
1
1
AL, t = 100
2
4
6
8
10
0.6
0.8
1
1
CL, t = 100
2
4
6
8
10
0.6
0.8
1
1
[Q1 âˆ’1.5IQR, Q3 + 1.5IQR];
[Q1, Q3];
median
Figure 10: Polynomial approximation rate for n = 500, p = 0.10 and t = 100. The ï¬rst axis is
the size of the drawn sample.
35

7.4.2
First conclusions
The ï¬rst thing that strikes the eye is that the approximations converge very rapidly. Even for
moderately sized spaces (âˆ¼500 elements), it appears to be suï¬ƒcient with 20 samples for SL and
AL, and for smaller spaces (âˆ¼200 elements), even fewer samples are required. We also notice
that there is a strong correlation between the ARI, Â¯oARI and normalised ï¬t.
For the part of the demonstration where we have no reference clustering, we cannot know for
sure whether the best reported ï¬t is also optimal. However, from the convergent behaviour of
the data, and the strong correlation between optimality and normalised ï¬t in Figures 6 and 7,
this points in the direction of convergence to the true optimum.
Only CL displays convergence issues, indicating that if one wishes to use CL for large spaces
or large numbers of tied connections, it may be wise to do so in conjunction with convergence
tests.
On the other hand, since SL is independent on tie resolution order, every sequence of merges
ending in the same maximal partition will produce the same partial dendrogram. This explains
why the convergence rate of SL is less aï¬€ected by the expected number of tied connections than,
say, CL.
On the very rapid convergence for high link probability
The convergence rate is very
high in some of the plots of Figures 9 and 10. The authors believe this is due the high probability
of two random elements being comparable (high p in O(n, p, t)), since a dense relation leads to
fewer candidate solutions. This in contrast to the larger set of candidates for a more sparse
relation, such as in Figure 8.
On the other hand, as we can see in Figures 8 and 9, keeping p ï¬xed and increasing the
number of tied connections, and thereby the number of possible branch points, causes a slower
convergence rate.
Comparing against classical HC.
For each reference clustering, we also computed the ARI
of the solution produced by classical HC1. To do this, we ran classical HC on the dissimilarity
measure, extracted the partition having the same number of blocks as the reference clustering,
and computed the ARI of this partition relative the reference clustering.
However, most ARI values for classical HC are too low to even show up in the above plots,
with 75% of the samples having an ARI of less than 0.31, and no observations of an ARI of
more than 0.58. This is presumably due to the fact that classical HC is unable to take into
account the constraints provided by the order relation, or said diï¬€erently: the family of possible
solutions for HC is a proper superset of DL(X, <, d).
7.5
Reference implementation
The implementation that was used to generate the above data is available as open source 2.
8
Summing up
We have established order preserving hierarchical agglomerative clustering for strictly partially
ordered sets. The clustering uses classical linkage functions such as single-, average-, and com-
plete linkage. We have shown that the clustering is idempotent and permutation invariant.
1python 3.7.7 and scipy.cluster.hierarchy version 1.3.2
2https://bitbucket.org/Bakkelund/ophac/wiki/Home
36

The output of order preserving hierarchical agglomerative clustering are partial dendrograms;
sub-trees of classical dendrograms, the diï¬€erence being that partial dendrograms have several
connected components. We have shown that the family of partial dendrograms over a set embed
into the family of dendrograms over the set.
When applying the theory to non-ordered sets, we see that we have a new theory for hier-
archical agglomerative clustering that is very close to the classical theory.
But, diï¬€erently
from classical hierarchical clustering, our theory is permutation invariant. We have shown that
for single linkage, our theory coincide with classical hierarchical clustering, while for complete
linkage, the clustering problem becomes NP-hard. However, the computational complexity is
directly linked to the number of tied connections, and in the absence of tied connections, the
theories coincide again.
We also present a polynomial approximation scheme for the clustering theory, and demon-
strate its convergence properties and eï¬ƒcacy on randomly generated ordered sets and randomly
generated dissimilarity measures. The approximation model demonstrates fast convergence to-
wards the optimal solution for both single- and average linkage in all tests.
8.1
Future work topics
We suggest the following future work topics:
8.1.1
Complexity
The complexity classes of order preserving hierarchical agglomerative clustering for SL and AL
remain to be established (see Section 6.3).
8.1.2
Order versus dissimilarity
The order relation has a signiï¬cant eï¬€ect on the output from the clustering process: If the
dissimilarity measure starts out by associating â€œwrongâ€ elements, the induced order may exclude
future merges of elements correctly belonging together. Also, if the order relation erroneously
identiï¬es elements as comparable, this may prevent elements that belong together from be
merged, regardless of the quality of the dissimilarity measure.
Together, these observations call for a need to â€œloosen upâ€ the stringent nature of the or-
der relation, or to allow to balance the merge-aï¬ƒnity of the dissimilarity measure against the
prohibitions of the order relation.
Acknowledgments.
I would like to thank Henrik Forssell, Department of Informatics (Uni-
versity of Oslo), and Gudmund Hermansen, Department of Mathematics (University of Oslo),
for their comments, questions and discussions greatly improving the exposition of this work.
References
M.
Ackerman
and
S.
Ben-David.
A
characterization
of
linkage-based
hierarchical
clustering.
Journal
of
Machine
Learning
Research,
17(231):1â€“17,
2016.
URL
http://jmlr.org/papers/v17/11-198.html.
S. Basu, I. Davidson, and K. Wagstaï¬€. Constrained Clustering: Advances in Algorithms, Theory,
and Applications. Chapman & Hall/CRC, 1 edition, 2008. ISBN 1584889969, 9781584889960.
37

T. Blyth. Lattices and Ordered Algebraic Structures. Universitext. Springer London, 2005. ISBN
9781852339050. URL https://www.springer.com/gp/book/9781852339050.
B. BollobÂ´as. Random Graphs. Cambridge Studies in Advanced Mathematics. Cambridge Uni-
versity Press, 2 edition, 2001. doi: 10.1017/CBO9780511814068.
A. BuluÂ¸c, H. Meyerhenke, I. Safro, P. Sanders, and C. Schulz.
Recent Advances in Graph
Partitioning, pages 117â€“158. Springer International Publishing, Cham, 2016. ISBN 978-3-319-
49487-6. URL https://link.springer.com/chapter/10.1007/978-3-319-49487-6_4.
G. Carlsson and F. MÂ´emoli. Characterization, stability and convergence of hierarchical clus-
tering methods.
J. Mach. Learn. Res., 11:1425â€“1470, Aug. 2010. ISSN 1532-4435. URL
http://www.jmlr.org/papers/v11/carlsson10a.html.
G. Carlsson, F. MÂ´emoli, A. Ribeiro, and S. Segarra.
Hierarchical quasi-clustering meth-
ods for asymmetric networks.
In E. P. Xing and T. Jebara, editors, Proceedings of the
31st International Conference on Machine Learning, volume 32 of Proceedings of Ma-
chine Learning Research, pages 352â€“360, Bejing, China, 22â€“24 Jun 2014. PMLR.
URL
http://proceedings.mlr.press/v32/carlsson14.html.
G. Chierchia and B. Perret. Ultrametric ï¬tting by gradient descent. In H. Wallach, H. Larochelle,
A. Beygelzimer, F. d'AlchÂ´e-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Inform-
ation Processing Systems, volume 32, pages 3181â€“3192. Curran Associates, Inc., 2019. URL
https://proceedings.neurips.cc/paper/2019/file/b865367fc4c0845c0682bd466e6ebf4c-Paper.pdf.
J.-F. Culus and M. Demange. Oriented coloring: Complexity and approximation. In J. Wie-
dermann, G. Tel, J. PokornÂ´y, M. BielikovÂ´a, and J. Ë‡Stuller, editors, SOFSEM 2006: Theory
and Practice of Computer Science, pages 226â€“236, Berlin, Heidelberg, 2006. Springer Berlin
Heidelberg.
S. Dasgupta. A cost function for similarity-based hierarchical clustering. In Proceedings of the
Forty-Eighth Annual ACM Symposium on Theory of Computing, STOC â€™16, pages 118â€“127,
New York, NY, USA, 2016. Association for Computing Machinery. ISBN 9781450341325. doi:
10.1145/2897518.2897527. URL https://dl.acm.org/doi/10.1145/2897518.2897527.
I. Davidson and S. S. Ravi. Agglomerative hierarchical clustering with constraints: Theoretical
and empirical results.
In A. M. Jorge, L. Torgo, P. Brazdil, R. Camacho, and J. Gama,
editors, Knowledge Discovery in Databases: PKDD 2005, pages 59â€“70, Berlin, Heidelberg,
2005. Springer Berlin Heidelberg.
C. Fraley and A. E. Raftery. Model-based clustering, discriminant analysis, and density es-
timation.
Journal of the American Statistical Association, 97(458):611â€“631, 2002.
doi:
10.1198/016214502760047131. URL https://doi.org/10.1198/016214502760047131.
M. R. Garey and D. S. Johnson.
Computers and Intractability:
A Guide to the The-
ory of NP-Completeness.
W. H. Freeman & Co., USA, 1979.
ISBN 0716710447.
URL
https://dl.acm.org/doi/book/10.5555/578533.
A.
J.
Gates
and
Y.-Y.
Ahn.
The
impact
of
random
models
on
clustering
similarity.
Journal
of
Machine
Learning
Research,
18(87):1â€“28,
2017.
URL
http://jmlr.org/papers/v18/17-039.html.
38

D.
Ghoshdastidar,
M.
Perrot,
and
U.
von
Luxburg.
Foundations
of
comparison-
based
hierarchical
clustering.
In
H.
Wallach,
H.
Larochelle,
A.
Beygelzimer,
F.
d'AlchÂ´e-Buc,
E.
Fox,
and
R.
Garnett,
editors,
Advances
in
Neural
Informa-
tion Processing Systems 32, pages 7456â€“7466. Curran Associates, Inc., 2019.
URL
http://papers.nips.cc/paper/8964-foundations-of-comparison-based-hierarchical-clustering.pdf.
S.
Gilpin,
S.
Nijssen,
and
I.
Davidson.
Formalizing
hierarchical
clustering
as
in-
teger
linear
programming.
In
Proceedings
of
the
Twenty-Seventh
AAAI
Confer-
ence on Artiï¬cial Intelligence,
AAAIâ€™13,
pages 372â€“378. AAAI Press,
2013.
URL
https://www.aaai.org/ocs/index.php/AAAI/AAAI13/paper/view/6440.
J. Herrmann, J. Kho, B. UÂ¸car, K. Kaya, and Â¨U. V. CÂ¸atalyÂ¨urek. Acyclic partitioning of large
directed acyclic graphs. In 2017 17th IEEE/ACM International Symposium on Cluster, Cloud
and Grid Computing (CCGRID), pages 371â€“380, May 2017. doi: 10.1109/CCGRID.2017.101.
URL https://hal.inria.fr/hal-01744603.
M.
Hoï¬€man,
D.
Steinley,
and
M.
J.
Brusco.
A
note
on
using
the
adjus-
ted
rand
index
for
link
prediction
in
networks.
Social
Networks,
42:72
â€“
79,
2015.
ISSN
0378-8733.
doi:
https://doi.org/10.1016/j.socnet.2015.03.002.
URL
http://www.sciencedirect.com/science/article/pii/S0378873315000210.
J. E. Holly.
Pictures of ultrametric spaces, the p-adic numbers, and valued ï¬elds.
The
American Mathematical Monthly, 108(8):721â€“728, 2001.
ISSN 00029890, 19300972. URL
http://www.jstor.org/stable/2695615.
L. Hubert and P. Arabie. Comparing partitions. Journal of Classiï¬cation, pages 193â€“218, 1985.
B. Hughes. Trees and ultrametric spaces: a categorical equivalence. Advances in Mathematics,
189(1):148 â€“ 191, 2004. URL https://doi.org/10.1016/j.aim.2003.11.008.
A. K. Jain and R. C. Dubes. Algorithms for Clustering Data. Prentice-Hall, Inc., Upper Saddle
River, NJ, USA, 1988. ISBN 0-13-022278-X.
M. F. Janowitz. Ordinal and Relational Clustering. WORLD SCIENTIFIC, 2010. doi: 10.1142/
7449. URL https://www.worldscientific.com/doi/abs/10.1142/7449.
N. Jardine and R. Sibson. Mathematical Taxonomy. Wiley series in probability and mathematical
statistics. Wiley, 1971. ISBN 9780471440505.
S. C. Johnson. Hierarchical clustering schemes.
Psychometrika, 32(3):241â€“254, 1967. URL
https://link.springer.com/article/10.1007/BF02289588.
T. Kamishima and J. Fujiki. Clustering orders. In G. Grieser, Y. Tanaka, and A. Yamamoto, ed-
itors, Discovery Science, pages 194â€“207, Berlin, Heidelberg, 2003. Springer Berlin Heidelberg.
ISBN 978-3-540-39644-4. URL https://doi.org/10.1007/978-3-540-39644-4_17.
R. M. Karp.
Reducibility among Combinatorial Problems, pages 85â€“103.
Springer US,
Boston, MA, 1972.
ISBN 978-1-4684-2001-2.
doi:
10.1007/978-1-4684-2001-2 9.
URL
https://doi.org/10.1007/978-1-4684-2001-2_9.
H.-P. Kriegel, P. KrÂ¨oger, J. Sander, and A. Zimek.
Density-based clustering.
WIREs
Data Mining and Knowledge Discovery, 1(3):231â€“240, 2011. doi: 10.1002/widm.30. URL
https://onlinelibrary.wiley.com/doi/abs/10.1002/widm.30.
39

M.
 Luczak.
Hierarchical
clustering
of
time
series
data
with
parametric
derivat-
ive
dynamic
time
warping.
Expert
Systems
with
Applications,
62:116
â€“
130,
2016.
ISSN
0957-4174.
doi:
https://doi.org/10.1016/j.eswa.2016.06.012.
URL
http://www.sciencedirect.com/science/article/pii/S0957417416302937.
J. Macqueen. Some methods for classiï¬cation and analysis of multivariate observations. In In
5-th Berkeley Symposium on Mathematical Statistics and Probability, pages 281â€“297, 1967.
URL https://projecteuclid.org/euclid.bsmsp/1200512992.
I. L. Markov,
J. Hu,
and M. Kim.
Progress and challenges in vlsi placement re-
search.
Proceedings of the IEEE, 103(11):1985â€“2003, Nov 2015.
ISSN 1558-2256.
URL
https://ieeexplore.ieee.org/document/7295553.
R. Rammal, G. Toulouse, and M. A. Virasoro.
Ultrametricity for physicists.
Rev.
Mod.
Phys.,
58:765â€“788,
Jul
1986.
doi:
10.1103/RevModPhys.58.765.
URL
https://link.aps.org/doi/10.1103/RevModPhys.58.765.
W. M. Rand.
Objective criteria for the evaluation of clustering methods.
Journal of
the American Statistical Association, 66(336):846â€“850, 1971.
ISSN 01621459.
URL
http://www.jstor.org/stable/2284239.
P. Sneath and R. Sokal.
Numerical Taxonomy: The Principles and Practice of Numerical
Classiï¬cation. A Series of books in biology. W. H. Freeman, 1973. ISBN 9780716706977.
N. X. Vinh, J. Epps, and J. Bailey. Information theoretic measures for clusterings comparison:
Variants, properties, normalization and correction for chance. Journal of Machine Learning
Research, 11(95):2837â€“2854, 2010. URL http://jmlr.org/papers/v11/vinh10a.html.
J. H. Ward.
Hierarchical grouping to optimize an objective function.
Journal of
the American Statistical Association, 58(301):236â€“244, 1963.
ISSN 01621459.
URL
http://www.jstor.org/stable/2282967.
M. J. Warrens. On the equivalence of cohenâ€™s kappa and the hubert-arabie adjusted rand index.
Journal of Classiï¬cation, pages 177â€“183, 2008.
40

