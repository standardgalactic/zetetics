A
Abelian Hidden Subgroup Problem
Michele Mosca
Canadian Institute for Advanced Research,
Toronto, ON, Canada
Combinatorics and Optimization/Institute for
Quantum Computing, University of Waterloo,
Waterloo, ON, Canada
Perimeter Institute for Theoretical Physics,
Waterloo, ON, Canada
Keywords
Abelian hidden subgroup problem; Abelian sta-
bilizer problem; Quantum algorithms; Quantum
complexity; Quantum computing
Years and Authors of Summarized
Original Work
1995; Kitaev
2008; Mosca
Problem Deﬁnition
The Abelian hidden subgroup problem is the
problem of ﬁnding generators for a subgroup K
of an Abelian group G, where this subgroup is
deﬁned implicitly by a function f W G ! X,
for some ﬁnite set X. In particular, f has the
property that f .v/ D f .w/ if and only if the
cosets (we are assuming additive notation for the
group operation here.) vCK and wCK are equal.
In other words, f is constant on the cosets of the
subgroup K and distinct on each coset.
It is assumed that the group G is ﬁnitely
generated and that the elements of G and X have
unique binary encodings. The binary assumption
is only for convenience, but it is important to
have unique encodings (e.g., in [22] Watrous
uses a quantum state as the unique encoding of
group elements). When using variables g and h
(possibly with subscripts), multiplicative notation
is used for the group operations. Variables x and
y (possibly with subscripts) will denote integers
with addition. The boldface versions x and y will
denote tuples of integers or binary strings.
By assumption, there is computational means
of computing the function f , typically a circuit or
“black box” that maps the encoding of a value g
to the encoding of f .g/. The theory of reversible
computation implies that one can turn a circuit
for computing f .g/ into a reversible circuit for
computing f .g/ with a modest increase in the
size of the circuit. Thus, it will be assumed that
there is a reversible circuit or black box that
maps .g; z/ 7! .g; z ˚ f .g//, where ˚ denotes
the bit-wise XOR (sum modulo 2), and z is any
binary string of the same length as the encoding
of f .g/.
Quantum
mechanics
implies
that
any
reversible gate can be extended linearly to a
unitary operation that can be implemented in
the model of quantum computation. Thus, it
is assumed that there is a quantum circuit or
© Springer Science+Business Media New York 2016
M.-Y. Kao (ed.), Encyclopedia of Algorithms,
DOI 10.1007/978-1-4939-2864-4

2
Abelian Hidden Subgroup Problem
black box that implements the unitary map
Uf W j gi j zi 7! j gi j z ˚ f .g/i.
Although special cases of this problem have
been considered in classical computer science,
the general formulation as the hidden subgroup
problem seems to have appeared in the context of
quantum computing, since it neatly encapsulates
a family of black-box problems for which quan-
tum algorithms offer an exponential speedup (in
terms of query complexity) over classical algo-
rithms. For some explicit problems (i.e., where
the black box is replaced with a speciﬁc function,
such as exponentiation modulo N ), there is a
conjectured exponential speedup.
Abelian Hidden Subgroup Problem:
Input: Elements g1; g2; : : : ; gn 2 G that gen-
erate the Abelian group G. A black box that
implements Uf
W j m1; m2; : : : ; mni j yi 7!
j m1; m2; : : : ; mni j f .g/ ˚ yi where g
D
gm1
1 gm2
2
: : : gmn
n
and K is the hidden subgroup
corresponding to f .
Output: Elements h1; h2; : : : ; hl 2 G that gen-
erate K.
Here we use multiplicative notation for the
group G in order to be consistent with Kitaev’s
formulation of the Abelian stabilizer problem.
Most of the applications of interest typically use
additive notation for the group G.
It is hard to trace the precise origin of this
general formulation of the problem, which simul-
taneously generalizes “Simon’s problem” [20],
the order-ﬁnding problem (which is the quantum
part of the quantum factoring algorithm [18]), and
the discrete logarithm problem.
One of the earliest generalizations of Simon’s
problem, order-ﬁnding problem, and discrete log-
arithm problem, which captures the essence of the
Abelian hidden subgroup problem, is the Abelian
stabilizer problem which was solved by Kitaev
using a quantum algorithm in his 1995 paper [14]
(and also appears in [15,16]).
Let G be a group acting on a ﬁnite set X.
That is, each element of G acts as a map from
X to X in such a way that for any two elements
g; h 2 G, g.h.´// D .gh/.´/ for all ´ 2 X. For
a particular element ´ 2 X, the set of elements
that ﬁx ´ (i.e., the elements g 2 G such that
g.´/ D ´) form a subgroup. This subgroup is
called the stabilizer of ´ in G, denoted StG.´/.
Abelian Stabilizer Problem:
Input: Elements g1; g2; : : : ; gn
2
G
that
generate the group G. An element ´ 2 X.
A black box that implements U.G;X/
W
j m1; m2; : : : ; mni j ´i 7! j m1; m2; : : : ; mni
jg.´/i where g D gm1
1 gm2
2
: : : gmn
n .
Output: Elements h1; h2; : : : ; hl 2 G that gen-
erate StG.´/.
Let f´ denote the function from G to X that
maps g 2 G to g.´/. One can implement Uf´
using U.G;X/. The hidden subgroup correspond-
ing to f´ is StG.´/. Thus, the Abelian stabilizer
problem is a special case of the Abelian hidden
subgroup problem.
One of the subtle differences (discussed in
Appendix 6 of [12]) between the above for-
mulation of the Abelian stabilizer problem and
the Abelian hidden subgroup problem is that
Kitaev’s formulation gives a black box that for
any g; h 2 G maps j m1; : : : ; mni j f´.h/i 7!
j m1; : : : ; mni j f´.hg/i, where g D gm1
1 gm2
2
: : :
gmn
n . The algorithm given by Kitaev is essentially
one that estimates eigenvalues of shift operations
of the form j f´.h/i 7! j f´.hg/i. In general,
these shift operators are not explicitly needed,
and it sufﬁces to be able to compute a map of the
form j yi 7! j f´.h/ ˚ yi for any binary string y.
Generalizations of this form have been known
since shortly after Shor presented his factoring
and discrete logarithm algorithms (e.g., [23]
presents the hidden subgroup problem for a large
class of ﬁnite Abelian groups or more generally
in [11] for ﬁnite Abelian groups presented as a
product of ﬁnite cyclic groups. In [17] the natural
Abelian hidden subgroup algorithm is related to
eigenvalue estimation.)
Other problems which can be formulated in
this way include:
Deutsch’s Problem:
Input: A black box that implements Uf
W
j xi j bi 7! j xi j b ˚ f .x/i, for some function
f that maps Z2 D f0; 1g to f0; 1g.

Abelian Hidden Subgroup Problem
3
A
Output: “constant” if f .0/ D f .1/ and “bal-
anced” if f .0/ ¤ f .1/.
Note that f .x/ D f .y/ if and only if x  y 2
K, where K is either f0g or Z2 D f0; 1g. If K D
f0g, then f is 11 or “balanced,” and if K D Z2,
then f is constant [5,6].
Simon’s Problem:
Input: A black box that implements Uf
W
j xi j bi 7! j xi j b ˚ f .x/i for some function
f from Zn
2 to some set X (which is assumed to
consist of binary strings of some ﬁxed length)
with the property that f .x/ D f .y/ if and
only if x  y 2 K D f0; sg for some s 2 Zn
2.
Output: The “hidden” string s.
The decision version allows K D f0g and
asks whether K is trivial. Simon [20] presents
an efﬁcient algorithm for solving this problem
and an exponential lower bound on the query
complexity. The solution to the Abelian hidden
subgroup problem is a generalization of Simon’s
algorithm.
Key Results
Theorem (ASP) There exists a quantum algo-
rithm that, given an instance of the Abelian stabi-
lizer problem, makes n C O.1/ queries to U.G;X/
and uses poly.n/ other elementary quantum and
classical operations, with probability at least 2
3
output values h1; h2; : : : ; hl such that StG.´/ D
hh1i ˚ hh2i ˚    hhli.
Kitaev ﬁrst solved this problem (with a
slightly
higher
query
complexity,
because
his eigenvalue estimation procedure was not
optimal). An eigenvalue estimation procedure
based
on
the
Quantum
Fourier
Transform
achieves the n C O.1/ query complexity [5].
Theorem (AHSP) There exists a quantum algo-
rithm that, given an instance of the Abelian hid-
den subgroup problem, makes nCO.1/ queries to
Uf and uses poly.n/ other elementary quantum
and classical operations, with probability at least
2
3 output values h1; h2; : : : ; hl such that K D
hh1i ˚ hh2i ˚    hhli.
In some cases, the success probability can
be made 1 with the same complexity, and in
general the success probability can be made
1   using n C O.log.1=// queries and
poly.n; log.1=// other elementary quantum and
classical operations.
Applications
Most of these applications in fact were known
before the Abelian stabilizer problem or hidden
subgroup problem were formulated.
Finding the order of an element in a group:
Let a be an element of a group H (which does
not need to be Abelian), and let r be the smallest
positive integer so that ar D 1.
Consider the function f from G D Z to the
group H where f .x/ D ax for some element a of
H. Then f .x/ D f .y/ if and only if x y 2 rZ.
The hidden subgroup is K D rZ and a generator
for K gives the order r of a.
Finding the period of a periodic function:
Consider a function f from G D Z to a set X
with the property that for some positive integer r,
we have f .x/ D f .y/ if and only if x  y 2 rZ.
The hidden subgroup of f is K D rZ and a
generator for K gives the period r.
Order ﬁnding is a special case of period ﬁnd-
ing and was also solved by Shor’s algorithm
[18].
Discrete Logarithms: Let a be an element of a
group H (which does not need to be Abelian),
with ar
D
1, and suppose b
D
ak from
some unknown k. The integer k is called the
discrete logarithm of b to the base a. Consider
the function f from G D Zr  Zr to H satis-
fying f .x1; x2/ D ax1bx2. Then f .x1; x2/ D
f .y1; y2/ if and only if .x1; x2/  .y1; y2/ 2
f.t; tk/; t
D
0; 1; : : : ; r  1g which is the
subgroup h.1; k/i of Zr  Zr. Thus, ﬁnding a
generator for the hidden subgroup K will give

4
Abelian Hidden Subgroup Problem
the discrete logarithm k. Note that this algorithm
works for H equal to the multiplicative group of
a ﬁnite ﬁeld, or the additive group of points on an
elliptic curve, which are groups that are used in
public-key cryptography.
Recently, Childs and Ivanyos [3] presented
an efﬁcient quantum algorithm for ﬁnding dis-
crete logarithms in semigroups. Their algo-
rithm makes use of the quantum algorithms for
period ﬁnding and discrete logarithms as subrou-
tines.
Hidden Linear Functions: Let  be some per-
mutation of ZN for some integer N . Let h be a
function from G D Z  Z to ZN , h.x; y/ D
x C ay mod N . Let f D  ı h. The hidden
subgroup of f is h.a; 1/i. Boneh and Lipton
[1] showed that even if the linear structure of h
is hidden (by ), one can efﬁciently recover the
parameter a with a quantum algorithm.
Self-Shift-Equivalent
Polynomials: Given
a
polynomial P in l variables X1; X2; : : : ; Xl over
Fq, the function f that maps .a1; a2; : : : ; al/ 2
Fl
q to P.X1  a1; X2  a2; : : : ; Xl  al/ is
constant on cosets of a subgroup K of Fl
q. This
subgroup K is the set of shift-self-equivalences
of the polynomial P . Grigoriev [10] showed how
to compute this subgroup.
Decomposition
of
a
Finitely
Generated
Abelian Group: Let G be a group with a unique
binary representation for each element of G, and
assume that the group operation, and recognizing
if a binary string represents an element of G or
not, can be done efﬁciently.
Given a set of generators g1; g2; : : : ; gn for a
group G, output a set of elements h1; h2; : : : ; hl,
l

n, from the group G such that G
D
hg1i ˚ hg2i ˚ : : : ˚ hgli. Such a generating set
can be found efﬁciently [2] from generators of
the hidden subgroup of the function that maps
.m1; m2; : : : ; mn/ 7! gm1
1 gm2
2
: : : gmn
n .
This simple algorithm directly leads to an
algorithm for computing the class group and
class number of a quadratic number ﬁeld, as
pointed out by Watrous [22] in his paper that
shows how to compute the order of solvable
groups. Computing the class group of a more
general number ﬁeld is a much more difﬁcult
task: this and related problems have been suc-
cessfully tackled in a series of elegant work
summarized in ▷Quantum Algorithms for Class
Group of a Number Field.
Such a decomposition of Abelian groups was
also applied by Friedl, Ivanyos, and Santha [9] to
test if a ﬁnite set with a binary operation is an
Abelian group, by Kedlaya [13] to compute the
zeta function of a genus g curve over a ﬁnite ﬁeld
Fq in time polynomial in g and q, and by Childs,
Jao, and Soukharev [4] in order to construct
elliptic curve isogenies in subexponential time.
Discussion: What About Non-Abelian
Groups?
The great success of quantum algorithms for
solving the Abelian hidden subgroup problem
leads to the natural question of whether it
can solve the hidden subgroup problem for
non-Abelian groups. It has been shown that
a polynomial number of queries sufﬁce [8];
however, in general there is no bound on
the overall computational complexity (which
includes other elementary quantum or classical
operations).
This question has been studied by many re-
searchers, and efﬁcient quantum algorithms can
be found for some non-Abelian groups. However,
at present, there is no efﬁcient algorithm for
most non-Abelian groups. For example, solving
the HSP for the symmetric group would directly
solve the graph automorphism problem.
Cross-References
▷Quantum Algorithm for Factoring
▷Quantum Algorithm for Solving Pell’s Equa-
tion
▷Quantum Algorithms for Class Group of a
Number Field
Recommended Reading
1. Boneh D, Lipton R (1995) Quantum cryptanal-
ysis
of
hidden
linear
functions
(extended
ab-
stract). In: Proceedings of 15th Annual International
Cryptology Conference (CRYPTO’95), Santa Bar-
bara, pp 424–437

Abstract Voronoi Diagrams
5
A
2. Cheung K, Mosca M (2001) Decomposing ﬁnite
Abelian groups. Quantum Inf Comput 1(2):26–32
3. Childs AM, Ivanyos G (2014) Quantum computation
of discrete logarithms in semigroups. J Math Cryptol
8(4):405–416
4. Childs AM, Jao D, Soukharev V (2010) Constructing
elliptic curve isogenies in quantum subexponential
time. preprint. arXiv:1012.4019
5. Cleve R, Ekert A, Macchiavello C, Mosca M (1998)
Quantum algorithms revisited. Proc R Soc Lond A
454:339–354
6. Deutsch D (1985) Quantum theory, the Church-
Turing principle and the universal quantum computer.
Proc R Soc Lond A 400:97–117
7. Deutsch D, Jozsa R (1992) Rapid solutions of prob-
lems by quantum computation. Proc R Soc Lond Ser
A 439:553–558
8. Ettinger M, Høyer P, Knill E (2004) The quantum
query complexity of the hidden subgroup problem is
polynomial. Inf Process Lett 91:43–48
9. Friedl K, Ivanyos G, Santha M (2005) Efﬁcient
testing of groups. In: Proceedings of the 37th An-
nual ACM Symposium on Theory of Computing
(STOC’05), Baltimore, pp 157–166
10. Grigoriev
D
(1997)
Testing
shift-equivalence
of
polynomials
by
deterministic,
probabilistic
and quantum machines. Theor Comput Sci 180:
217–228
11. Høyer P (1999) Conjugated operators in quantum
algorithms. Phys Rev A 59(5):3280–3289
12. Kaye P, Laﬂamme R, Mosca M (2007) An intro-
duction to quantum computation. Oxford University
Press, Oxford, UK
13. Kedlaya KS (2006) Quantum computation of zeta
functions of curves. Comput Complex 15:1–19
14. Kitaev A (1995) Quantum measurements and the
Abelian stabilizer problem. quant-ph/9511026
15. Kitaev A (1996) Quantum measurements and the
Abelian stabilizer problem. In: Electronic Collo-
quium on Computational Complexity (ECCC), vol 3.
http://eccc.hpi-web.de/report/1996/003/
16. Kitaev A. Yu (1997) Quantum computations: al-
gorithms and error correction. Russ Math Surv
52(6):1191–1249
17. Mosca M, Ekert A (1998) The hidden subgroup
problem and eigenvalue estimation on a quantum
computer. In: Proceedings 1st NASA International
Conference
on
Quantum
Computing
&
Quan-
tum Communications, Palm Springs. Lecture notes
in computer science, vol 1509. Springer, Berlin,
pp 174–188
18. Shor P (1994) Algorithms for quantum computation:
discrete logarithms and factoring. In: Proceedings
of the 35th Annual Symposium on Foundations
of
Computer
Science
(FOCS’94),
Santa
Fe,
pp 124–134
19. Shor P (1997) Polynomial-time algorithms for prime
factorization and discrete logarithms on a quantum
computer. SIAM J Comput 26:1484–1509
20. Simon D (1994) On the power of quantum compu-
tation. In: Proceedings of the 35th IEEE Symposium
on the Foundations of Computer Science (FOCS’94),
Santa Fe, pp 116–123
21. Simon D (1997) On the power of quantum computa-
tion. SIAM J Comput 26:1474–1483
22. Watrous J (2000) Quantum algorithms for solvable
groups. In: Proceedings of the 33rd ACM Sympo-
sium on Theory of Computing (STOC’00), Portland,
pp 60–67
23. Vazirani U (1997) Berkeley lecture notes. Fall. Lec-
ture 8. http://www.cs.berkeley.edu/~vazirani/qc.html
Abstract Voronoi Diagrams
Rolf Klein
Institute for Computer Science, University of
Bonn, Bonn, Germany
Keywords
Abstract Voronoi diagram; Bisector; Computa-
tional geometry; Metric; Voronoi diagram
Years and Authors of Summarized
Original Work
2009; Klein, Langetepe, Nilforoushan
Problem Deﬁnition
Concrete Voronoi diagrams are usually deﬁned
for a set S of sites p that exert inﬂuence over
the points ´ of a surrounding space M. Often, in-
ﬂuence is measured by distance functions dp.´/
that are associated with the sites. For each p, its
Voronoi region is given by
VR.p; S/
D f ´2MI dp.´/<dq.´/ for all q 2S n fpgg;
and the Voronoi diagram V.S/ of S is the decom-
position of M into Voronoi regions; compare the
entry ▷Voronoi Diagrams and Delaunay Trian-
gulations of this Encyclopedia.
Quite different Voronoi diagrams result de-
pending on the particular choices of space, sites,
and distance measures; see Fig. 1. A great num-
ber of other types of Voronoi diagrams can be
found in the monographs [1] and [14]. In each

6
Abstract Voronoi Diagrams
Abstract Voronoi
Diagrams, Fig. 1 Voronoi
diagrams of points in the
Euclidean and Manhattan
metric and of disks (or
additively weighted points)
in the Euclidean plane
case, one wants to quickly compute the Voronoi
diagram, because it contains a lot of distance in-
formation about the sites. However, the classical
algorithms for the standard case of point sites in
the Euclidean plane do not apply to more general
situations.
To free us from designing individual algo-
rithms for each and every special case, we would
like to ﬁnd a unifying concept that provides struc-
tural results and efﬁcient algorithms for gener-
alized Voronoi diagrams. One possible approach
studied in [5,6] is to construct the lower envelope
of the 3-dimensional graphs of the distance func-
tions dp.´/, whose projection to the XY -plane
equals the Voronoi diagram.
Key Results
A different approach is given by abstract Voronoi
diagrams that are not based on the notions of sites
and distance measures (as their deﬁnitions vary
anyway). Instead, AVDs are built from bisecting
curves as primary objects [7].
Let S D fp; q; r; : : :g be a set of n indices, and
for p 6D q 2 S, let J.p; q/ D J.q; p/ denote an
unbounded curve that bisects the plane into two
unbounded open domains D.p; q/ and D.q; p/.
We require that each J.p; q/ is mapped to a
closed Jordan curve through the north pole, under
stereographic projection to the sphere. Now we
deﬁne Voronoi regions by
VR.p; S/ WD
\
q2Snfpg
D.p; q/
and the abstract Voronoi diagram by
V.S/ WD R2 n
[
p2S
VR.p; S/:
The system J of the curves J.p; q/ is called
admissible if the following axioms are fulﬁlled
for every subset T of S of size three.
A1. Each Voronoi region VR.p; T / is pathwise
connected.
A2. Each point of R2 lies in the closure of a
Voronoi region VR.p; T /.
These combinatorial properties should not be
too hard to check in a concrete situation because
only triplets of sites need to be inspected. Yet,
they form a strong foundation, as was shown
in [8]. The following fact is crucial for the proof
of Theorem 1. It also shows that AVDs can be
seen as lower envelopes of surfaces in dimen-
sion 3.
Lemma 1 For
all
p; q; r
in
S,
we
have
D.p; q/ \ D.q; r/  D.p; r/. Consequently,
for each point ´ 2 R2 not contained in any curve
of J, the relation
p <´ q W, ´ 2 D.p; q/
is an ordering of the sites in S at ´.
Theorem 1 If J is admissible, then axioms A1
and A2 hold for all subsets T of S. Moreover, the
abstract Voronoi diagram V.S/ is a planar graph
of size O.n/.

Abstract Voronoi Diagrams
7
A
Of the classical algorithm for constructing
Voronoi diagrams, the randomized incremental
construction method works best for abstract
Voronoi diagrams [8,10].
Theorem 2 If J is admissible, then V.S/ can be
constructed in an expected number of O.n log n/
many steps and in expected linear space.
Here basic operations like computing an intersec-
tion of two bisecting curves are counted as one
step.
Applications
To show that a concrete type of Voronoi diagram
is under the roof of abstract Voronoi diagrams,
one needs to prove that its bisector system is
admissible.
Let d be a metric in the plane that enjoys
the following properties. Each d-disk contains a
Euclidean disk and vice versa; for any two points
a; b, there exists a point c different from a and b
such that d.a; b/ D d.a; c/ C d.c; b/ holds; for
any two points a; b, their metric bisector
Bd.a; b/ D
˚
´ 2 R2I d.a; ´/ D d.b; ´/

is itself a curve that maps to a closed Jordan curve
through the north pole by stereographic projec-
tion to the sphere, or, in case Bd.p; q/ contains 2-
dimensional pieces, its boundary consists of two
such curves.
The ﬁrst two properties ensure that any two
points can be connected by a d-straight path
along which d-distances add up. The third
condition ensures that we can choose from
Bd.p; q/ suitable bisecting curves. Let us call
metric d very nice if also a fourth condition is
fulﬁlled. Given three points a; p0; p1, there exist
d-straight paths from a to p0 and from a to
p1 that have only point a in common, or each
d-straight path from a to pi contains p1i for
i D 0 or i D 1. All convex distance functions
(gauges) are very nice.
Theorem 3 Very nice metrics have admissible
point bisector curves.
Other applications of AVDs include points with
additive weights, both the regular and the Haus-
dorff Voronoi diagram of disjoint convex sites
with respect to a convex distance function, and
some types of city Voronoi diagrams; see [1] for
further details.
Generalizations
How to dynamize abstract Voronoi diagrams
has been studied in [12]. Special cases of 3-
dimensional abstract Voronoi diagrams have been
discussed in [11]; they include all convex distance
functions whose unit spheres are ellipsoids.
It is well known that for the vertices of a
convex polygon, the Voronoi diagram can be
constructed in linear time. This result has been
generalized to AVDs in [9] and [4]. In [3] the
path-connectedness of abstract Voronoi regions
(axiom A1) has been relaxed. If a region of three
sites can have up to s connected components, the
abstract Voronoi diagram can still be constructed
in expected time O.s2n Pn
j D3 mj =j /, where mj
denotes the average number of faces per region
in any subdiagram of j sites from S.
In an order-k Voronoi diagram, all points of
space M are placed in one region that shares the
same k nearest sites in S. For k D n1, this con-
cept has been generalized to furthest site abstract
Voronoi diagrams in [13]. Here the furthest (or
inverse) region of p 2 S is the intersection of all
domains D.q; p/, where q 2 Snfpg. If all regular
Voronoi regions are nonempty, then the furthest
site AVD is a tree of size O.n/, even though some
regions may be disconnected.
General order-k abstract Voronoi diagrams
have been studied in [2]. If all regular Voronoi
regions are nonempty and if bisecting curves
are in general position, a tight upper complexity
bound of 2k.nk/ can be shown. Fortunately, the
nonemptiness of the regular regions need only be
tested for all subsets of S of size 4.

8
Active Learning – Modern Learning Theory
Cross-References
▷Voronoi Diagrams and Delaunay Triangula-
tions
Recommended Reading
1. Aurenhammer F, Klein R, Lee DT (2013) Voronoi
diagrams and Delaunay triangulations. World Scien-
tiﬁc, Singapore
2. Bohler C, Cheilaris P, Klein R, Liu CH, Pa-
padopoulou E, Zavershynskyi M (2013) On the com-
plexity of higher order abstract Voronoi diagrams. In:
Proceedings of the 40th international colloquium on
automata languages and programming, Riga. Lecture
notes in computer science, vol 7965, pp 208–219
3. Bohler C, Klein R (2013) Abstract Voronoi dia-
grams with disconnected regions. In: Proceedings of
the 24th international symposium on algorithms and
computation, Hong Kong. Lecture notes in computer
science, vol 8283, pp 306–316
4. Bohler C, Klein R, Liu CH (2014) Forest-like abstract
Voronoi diagrams in linear time. In: 26th Canadian
conference on computational geometry, Halifax
5. Boissonnat JD, Wormser C, Yvinec M (2006)
Curved Voronoi diagrams. In: Boissonnat JD, Teil-
laud M (eds) Effective computational geometry for
curves and surfaces. Mathematics and visualization.
Springer, Berlin/New York, pp 67–116
6. Edelsbrunner H, Seidel R (1986) Voronoi diagrams
and arrangements. Discret Comput Geom 1:387–421
7. Klein R (1989) Concrete and abstract Voronoi dia-
grams. Lecture notes in computer science, vol 400.
Springer, Berlin/New York
8. Klein R, Langetepe E, Nilforoushan Z (2009) Ab-
stract Voronoi diagrams revisited. Comput Geom
Theory Appl 42(9):885–902
9. Klein R, Lingas A (1994) Hamiltonian abstract
Voronoi diagrams in linear time. In: Proceedings
of the 5th international symposium on algorithms
and computation, Beijing. Lecture notes in computer
science, vol 834, pp 11–19
10. Klein R, Mehlhorn K, Meiser S (1993) Random-
ized incremental construction of abstract Voronoi
diagrams. Comput Geom Theory Appl 3:157–184
11. Lê NM (1995) Randomized incremental construc-
tion of simple abstract Voronoi diagrams in 3-space.
In: Proceedings of the 10th international confer-
ence on fundamentals of computation theory, Dres-
den. Lecture notes in computer science, vol 965,
pp 333–342
12. Malinauskas KK (2008) Dynamic construction of
abstract Voronoi diagrams. J Math Sci 154(2):
214–222
13. Mehlhorn K, Meiser S, Rasch R (2009) Furthest site
abstract Voronoi diagrams. Int J Comput Geom Appl
11:583–616
14. Okabe A, Boots B, Sugihara K, Chiu SN (2000)
Spatial tessellations: concepts and applications of
Voronoi diagrams. Wiley, Chichester
Active Learning – Modern Learning
Theory
Maria-Florina Balcan and Ruth Urner
Department of Machine Learning, Carnegie
Mellon University, Pittsburgh, PA, USA
Keywords
Active
learning; Computational
complexity;
Learning theory; Sample complexity
Years and Authors of Summarized
Original Work
2006; Balcan, Beygelzimer, Langford
2007; Balcan, Broder, Zhang
2007; Hanneke
2013; Urner, Wulff, Ben-David
2014; Awashti, Balcan, Long
Problem Deﬁnition
Most classic machine learning methods depend
on the assumption that humans can annotate all
the data available for training. However, many
modern machine learning applications (including
image and video classiﬁcation, protein sequence
classiﬁcation, and speech processing) have mas-
sive amounts of unannotated or unlabeled data.
As a consequence, there has been tremendous in-
terest both in machine learning and its application
areas in designing algorithms that most efﬁciently
utilize the available data while minimizing the
need for human intervention. An extensively used
and studied technique is active learning, where
the algorithm is presented with a large pool of
unlabeled examples (such as all images available
on the web) and can interactively ask for the

Active Learning – Modern Learning Theory
9
A
labels of examples of its own choosing from the
pool, with the goal to drastically reduce labeling
effort.
Formal Setup
We
consider
classiﬁcation
problems
(such
as classifying images by who is in them or
classifying emails as spam or not), where the goal
is to predict a label y based on its corresponding
input vector x. In the standard machine learning
formulation, we assume that the data points
.x; y/ are drawn from an unknown underlying
distribution DXY over X  Y ; X is called the
feature (instance) space and Y D f0; 1g is the
label space. The goal is to output a hypothesis
function h of small error (or small 0=1 loss),
where err.h/
D
P.x;y/DXY Œh.x/
¤
y:
In the passive learning setting, the learning
algorithm is given a set of labeled examples
.x1; y1/; : : : ; .xm; ym/ drawn i.i.d. from DXY
and the goal is to output a hypothesis of small
error by using only a polynomial number of
labeled examples. In the realizable case [10]
(PAC learning), we assume that the true label of
any example is determined by a deterministic
function of the features (the so-called target
function) that belongs to a known concept class
C (e.g., the class of linear separators, decision
trees, etc.). In the agnostic case [10, 13], we do
not make the assumption that there is a perfect
classiﬁer in C, but instead we aim to compete
with the best function in C (i.e., we aim to
identify a classiﬁer whose error is not much
worse than opt.C/, the error of the best classiﬁer
in C). Both in the realizable and agnostic settings,
there is a well-developed theory of Sample
Complexity [13], quantifying in terms of the so-
called VC-dimension (a measure of complexity of
a concept class) how many training examples we
need in order to be conﬁdent that a rule that does
well on training data is a good rule for future data
as well.
In the active learning setting, a set of labeled
examples .x1; y1/; : : : ; .xm; ym/ is also drawn
i.i.d. from DXY ; the learning algorithm is per-
mitted direct access to the sequence of xi values
(unlabeled data points), but has to make a label
request to obtain the label yi of example xi.
The hope is that we can output a classiﬁer of
small error by using many fewer label requests
than in passive learning by actively directing the
queries to informative examples (while keeping
the number of unlabeled examples polynomial).
It has been long known that, in the realiz-
able case, active learning can sometimes provide
an exponential improvement in label complexity
over passive learning. The canonical example [6]
is learning threshold classiﬁers (X D Œ0; 1 and
C D f1Œ0;a j a 2 Œ0; 1g). Here we can actively
learn with only
QO.log.1=// label requests by
using a simple binary search-like algorithm as
follows: we ﬁrst draw N D
QO..1=/ log.1=ı//
unlabeled examples, then do binary search to
ﬁnd the transition from label 1 to label 0, and
with only O.log.N // queries we can correctly
infer the labels of all our examples; we ﬁnally
output a classiﬁer from C consistent with all
the inferred labels. By standard VC-dimension
based bounds for supervised learning [13], we are
guaranteed to output an -accurate classiﬁer. On
the other hand, for passive learning, we provably
need ˝.1=/ labels to output a classiﬁer of error
at most  with constant probability, yielding the
exponential reduction in label complexity.
Key Results
While in the simple threshold concept class
described above active learning always provides
huge improvements over passive learning, things
are more delicate in more general scenarios.
In particular, both in the realizable and in
the agnostic case, it has been shown that for
more general concept spaces, in the worst case
over all data-generating distributions, the label
complexity of active learning equals that of
passive learning. Thus, much of the literature was
focused on identifying non-worst case, natural
conditions about the relationship between the
data distribution and the target, under which
active learning provides improvements over
passive. Below, we discuss three approaches,
under which active learning has been shown to
reduce the label complexity: disagreement-based

10
Active Learning – Modern Learning Theory
techniques, margin-based techniques and cluster-
based techniques.
Disagreement-Based Active Learning
Disagreement-based active learning was the
ﬁrst method to demonstrate the feasibility of
agnostic active learning for general concept
classes. The general algorithmic framework
of disagreement-based active learning in the
presence of noise was introduced with the A2
algorithm
by
Balcan et al. [2].
Subsequently,
several
researchers
have
proposed
related
disagreement-based algorithms with improved
sample complexity, e.g., [5,8,11].
At a high level, A2 operates in rounds. It
maintains a set of candidate classiﬁers from the
concept class C and in each round queries labels
aiming to efﬁciently reduce this set to only few
high-quality candidates. More precisely, in round
i, A2 considers the set of surviving classiﬁers
Ci  C, and asks for the labels of a few random
points that fall in the region of disagreement of
Ci. Formally, the region of disagreement of a set
of classiﬁers Ci is DIS.Ci/ D fx 2 X j 9f; g 2
Ci W f .x/ ¤ g.x/g. Based on these queried la-
bels from DIS.Ci/, to obtain CiC1, the algorithm
then throws out hypotheses that are suboptimal.
The key ingredient is that A2 only throws out
hypotheses, for which it is statistically conﬁdent
that they are suboptimal.
Balcan et al. [2] show that A2 provides
exponential improvements in the label sample
complexity
in
terms
of
the
1=-parameter
when the noise rate  is sufﬁciently small,
both for learning thresholds and for learning
homogeneous linear separators in Rd, one
of the most widely used and studied classes
in machine learning. Following up on this,
Hanneke [9] provided a generic analysis of the
A2 algorithm that applies to any concept class.
This analysis quantiﬁes the label complexity
of A2 in terms of the so-called disagreement
coefﬁcient of the class C. The disagreement
coefﬁcient is a distribution-dependent sample
complexity measure that quantiﬁes how fast the
region of disagreement of the set of classiﬁers
at distance r of the optimal classiﬁer collapses
as a function r. In particular, [9] showed that
the label complexity of the A2 algorithm is
O

2
2
2 C1

.d log.1=/Clog.1=ı// log.1=/

,
where  is the best error rate of a classiﬁer
in C, d is the VC-dimension of C, and  is
the disagreement-coefﬁcient. As an example,
for homogeneous linear separators, we have
 D .
p
d/ under uniform marginal over the
unit ball. Here, the disagreement-based analysis
yields a label complexity of QO

d 2 2
2 log.1=/

in the agnostic case and QO

d 3=2 log.1=/

in the
realizable case.
Margin-Based Active Learning
While the disagreement-based active learning
line
of
work
provided
the
ﬁrst
general
understanding of the sample complexity beneﬁts
with active learning for arbitrary concept classes,
it suffers from two main drawbacks: (1) methods
and analyses developed in this context are often
suboptimal in terms of label complexity, since
they take a conservative approach and query even
points on which there is only a small amount of
uncertainty, (2) the methods are computationally
inefﬁcient.
Margin-based
active
learning
is
a technique that overcomes both the above
drawbacks for learning homogeneous linear
separators under log-concave distributions. The
technique was ﬁrst introduced by Balcan et al. [3]
and further developed by Balcan et al. [4], and
Awasthi et al. [1].
At a high level, like disagreement-based meth-
ods, the margin-based active learning algorithm
operates in rounds, in which a number of labels
are queried in some subspace of the domain
and a set of candidate classiﬁers for the next
round is identiﬁed. The crucial idea to reduce
the label complexity is to design a more ag-
gressive querying strategy by carefully choosing
where to query instead of querying in all of
the current disagreement region. Concretely, in
round k the algorithm has a current hypothesis
wk, and the set of candidate classiﬁers for the
next round consists of all homogeneous halfs-
paces that lie in a ball of radius rk around wk
(in terms of their angle with wk). The algorithm
then queries points for labels near the decision
boundary of wk; that is, it only queries points

Active Learning – Modern Learning Theory
11
A
wk
γk
rk
wk+1
+
+
+
−
+
+
+
+
+
−
+
−
−
+
−
−
−
−
+
−
−
Active Learning – Modern Learning Theory, Fig. 1
The margin-based active learning algorithm after iteration
k. The algorithm samples points within margin k of the
current weight vector wk and then minimizes the hinge
loss over this sample subject to the constraint that the new
weight vector wkC1 is within distance rk from wk
that are within a margin k of wk; see Fig. 1. To
obtain wkC1, the algorithm ﬁnds a loss minimizer
among the current set of candidates with respect
to the queried examples of round k. In the realiz-
able case, this is done by 0=1-loss minimization.
In the presence of noise, to obtain a compu-
tationally efﬁcient procedure, the margin-based
technique minimizes a convex surrogate loss.
Balcan et al. [3] and Balcan and Long [4]
showed that by localizing aggressively, namely
by setting the margin parameter to k D 	. 1
2k /,
one can actively learn with only QO.d log.1=//
label requests in the realizable case, when the
underlying distribution is isotropic log-concave.
A key idea of their analysis is to decompose,
in round k, the error of a candidate classiﬁer
w as its error outside margin k of the current
separator plus its error inside margin k, and
to prove that for the above parameters, a small
constant error inside the margin sufﬁces to re-
duce the overall error by a constant factor. For
the constant error inside the margin only .d/
labels need to be queried, and since in each
round the overall error gets reduced by a con-
stant factor, O.log.1=// rounds sufﬁce to reduce
the error to , yielding the label complexity of
QO.d log.1=//. Passive learning here provably
requires ˝.d=/ labeled examples. Thus, the
dependence on 1= is exponentially improved,
but without increasing the dependence on d (as
in the disagreement-based method for this case,
see above).
Building on this work, [1] gave the ﬁrst
polynomial-time active learning algorithm for
learning linear separators to error  in the
presence of agnostic noise (of rate O./) when
the underlying distribution is an isotropic log-
concave distribution in Rd. They proposed to
use a normalized hinge loss minimization (with
normalization factor 
k) for selecting the next
classiﬁer wkC1 in round k. Awasthi et al. [1]
show that by setting the parameters appropriately
(namely, 
k D 	.1=2k/ and rk D 	.1=2k/),
the algorithm again achieves error  using only
O.log.1=// rounds, with O.d 2/ label requests
per round. This yields a query complexity of
poly.d; log 1=/. The key ingredient for the
analysis of this computationally efﬁcient version
in the noisy setting is proving that by constraining
the search for wkC1 to vectors within a ball of
radius rk around wk, the hinge-loss acts as a
sufﬁciently faithful proxy for the 0=1-loss.
A recent work [14] proposes an elegant gener-
alization of [3,4] to more general concept spaces
and shows an analysis that is always tighter than
disagreement-based active learning (though their
results are not computationally efﬁcient).
Cluster-Based Active Learning
The methods described above (disagreement-
based and margin-based active learning) use
active label queries to efﬁciently identify a clas-
siﬁer from the concept class C with low error. An
alternative approach to agnostic active learning is
to design active querying methods that efﬁciently
ﬁnd a (approximately) correct labeling of the
unlabeled input sample. Here, “correct labeling”
refers to the hidden labels yi in the sample
.x1; y1/; : : : ; .xm; ym/
from
the
distribution
DXY
(as deﬁned in the formal setup section).
The so labeled sample can then be used as input to
a passive learning algorithm to learn an arbitrary
concept class.
Cluster-based active learning is a method
for the latter approach and was introduced
by Dasgupta and Hsu [7]. The idea is to use

12
Active Learning – Modern Learning Theory
a hierarchical clustering (cluster tree) of the
unlabeled data, and check the clusters for label
homogeneity by starting at the root of the tree (the
whole data set) and working towards the leaves
(single data points). The label homogeneity of a
cluster is estimated by choosing data points for
label query uniformly at random from the cluster.
If a cluster is considered label homogeneous
(with sufﬁciently high conﬁdence), all remaining
unlabeled points in that cluster are labeled with
the majority label. If a cluster is detected to be
label heterogeneous, it is split into its children
in the cluster tree and processed later. The key
insight in [7] is that since the cluster tree is ﬁxed
before any labels were seen, the induced labeled
subsample of a child cluster can be considered
a sample that was chosen uniformly at random
from the points in that child-cluster. Thus, the
algorithm can reuse labels from the parent cluster
without introducing any sampling bias. The label
efﬁciency of this paradigm crucially depends
on the quality of input hierarchical clustering.
Intuitively, if the cluster tree has a small pruning
with label homogeneous clusters, the procedure
will make only few label queries.
Urner et al. [12] proved label complexity re-
ductions with this paradigm under a distributional
assumption. They analyze a version (PLAL) of
the above paradigm that uses hierarchical clus-
terings induced by spatial trees on the domain
Œ0; 1d and provide label query bounds in terms of
the Probabilistic Lipschitzness of the underlying
data-generating distribution. Probabilistic Lips-
chitzness quantiﬁes a marginal-label relatedness
in the sense of close points being likely to have
the same label. For a distribution with determin-
istic labels (PrŒY D 1 j X D x 2 f0; 1g for all
x), the Probabilistic Lipschitzness is a function 
that bounds, as a function of , the mass of points
x for which both labels 0 and 1 occur in the ball
B.x/.
Urner et al. [12] show that, independently of
the any data assumptions, (with probability 1ı)
PLAL labels a .1/-fraction of the input points
correctly. They further show that using PLAL as
a preprocedure, if the data-generating distribution
has deterministic labels and its Probabilistic Lip-
schitzness is bounded by ./ D n for some
n 2 N, then classes C of bounded VC-dimension
on domain X
D Œ0; 1d can be learned with
QO
 1

 nC2d
nCd

many labels, while any passive
proper learner (i.e., a passive learner that outputs
a function from C) requires to see ˝.1=2/ many
labels. Further, [12] show that PLAL can be used
to reduce the number of labels needed for nearest
neighbor classiﬁcation (i.e., labeling a test point
by the label of its nearest point in the sample)
from ˝
 1

1C d1
n

to QO
 1

1C
d2
n.nCd/

.
Cross-References
▷PAC Learning
Recommended Reading
1. Awasthi P, Balcan M-F, Long PM (2014) The power
of localization for efﬁciently learning linear separa-
tors with noise. In: Proceedings of the 46th annual
symposium on the theory of computing (STOC), New
York
2. Balcan MF, Beygelzimer A, Langford J (2006) Ag-
nostic active learning. In: Proceedings of the 23rd in-
ternational conference on machine learning (ICML),
Pittsburgh
3. Balcan M-F, Broder A, Zhang T (2007)
Margin
based active learning.
In: Proceedings of the 20th
annual conference on computational learning theory
(COLT), San Diego
4. Balcan M-F, Long PM (2013)
Active and passive
learning of linear separators under log-concave dis-
tributions. In: Proceedings of the 26th conference on
learning theory (COLT), Princeton
5. Beygelzimer A, Hsu D, Langford J, Zhang T (2010)
Agnostic active learning without constraints.
In:
Advances in neural information processing systems
(NIPS), Vancouver
6. Cohn D, Atlas L, Ladner R (1994) Improving gen-
eralization with active learning. In: Proceedings of
the 11th international conference on machine learning
(ICML), New Brunswick
7. Dasgupta S, Hsu D (2008)
Hierarchical sampling
for active learning. In: Proceedings of the 25th in-
ternational conference on machine learning (ICML),
Helsinki
8. Dasgupta S, Hsu DJ, Monteleoni C (2007) A general
agnostic active learning algorithm.
In: Advances
in neural information processing systems (NIPS),
Vancouver
9. Hanneke S (2007) A bound on the label complexity
of agnostic active learning.
In: Proceedings of the

Active Self-Assembly and Molecular Robotics with Nubots
13
A
24th international conference on machine learning
(ICML), Corvallis
10. Kearns MJ, Vazirani UV (1994) An introduction to
computational learning theory. MIT, Cambridge
11. Koltchinskii V (2010) Rademacher complexities and
bounding the excess risk in active learning. J Mach
Learn 11:2457–2485
12. Urner R, Wullf S, Ben-David S (2013) Plal: cluster-
based active learning.
In: Proceedings of the 26th
conference on learning theory (COLT), Princeton
13. Vapnik VN (1998) Statistical learning theory. Wiley,
New York
14. Zhang C, Chaudhuri K (2014) Beyond disagreement-
based agnostic active learning. In: Advances in neural
information processing systems (NIPS), Montreal
Active Self-Assembly and Molecular
Robotics with Nubots
Damien Woods
Computer Science, California Institute of
Technology, Pasadena, CA, USA
Keywords
Molecular robotics; Rigid-body motion; Self-
assembly
Years and Authors of Summarized
Original Work
2013;
Woods,
Chen,
Goodfriend,
Dabby,
Winfree, Yin
2013; Chen, Xin, Woods
2014; Chen, Doty, Holden, Thachuk, Woods,
Yang
Problem Deﬁnition
In the theory of molecular-scale self-assembly,
large numbers of simple interacting components
are
designed
to
come
together
to
build
complicated shapes and patterns. Many models
of self-assembly, such as the abstract Tile
Assembly Model [6], are cellular automata-like
crystal growth models. Indeed such models have
given rise to a rich theory of self-assembly as
described elsewhere in this encyclopedia. In
biological organisms we frequently see much
more sophisticated growth processes, where self-
assembly is combined with active molecular
components that change internal state and even
molecular motors that have the ability to push
and pull large structures around. Molecular
engineers are now beginning to design and build
molecular-scale DNA motors and active self-
assembly systems [2]. We wish to understand,
at a high level of abstraction, the ultimate
computational capabilities and limitations of
such molecular-scale rearrangement and growth.
The nubot model, put forward in [8], is akin
to an asynchronous nondeterministic cellular
automaton augmented with nonlocal rigid-body
movement. Unit-sized monomers are placed
on a 2D triangular grid. Monomers undergo
state changes, appear, and disappear using local
rules, as shown in Fig. 1. However, there is
also a nonlocal aspect to the model: rigid-body
movement that comes in two forms, movement
rules and random agitations.
A movement rule r, consisting of a pair of
monomer states A; B and two unit vectors, is a
programmatic way to specify unit-distance trans-
lation of a set of monomers in one step. See Fig. 2
for an example. If A and B are in a prescribed
orientation, one is nondeterministically chosen
to move unit distance in a prescribed direction.
The rule r is applied in a rigid-body fashion:
roughly speaking, if A is to move right, it pushes
anything immediately to its right and pulls any
monomers that are bound to its left which in turn
push and pull other monomers, all in one step.
The rule may not be applicable if it is blocked
(i.e., if movement of A would force B to also
move), which is analogous to the fact that an
arm cannot push its own shoulder. The other,
somewhat related, form of movement is called
agitation: at every point in time, every monomer
on the grid may move unit distance in any of the
six directions, at unit rate for each (monomer,
direction) pair. An agitating monomer will push
or pull any monomers that it is adjacent to, in a
way that preserves rigid-body structure and all in

14
Active Self-Assembly and Molecular Robotics with Nubots
1
1
(0,0)
x
y
w
(1,0)
(2,0)
(0,1)
(0,2)
(1,1)
p
p + y
p + w
p + x
p - x
p - w
p - y
a
b
Change states
1
1
2
3
Make a flexible bond
1
1
1
1
1
1
Break a rigid bond
2
3
1
1
Change a rigid bond to a flexible bond 
and change states
1
2
1
1
Position change in the w direction
w
Base
Arm
1
1
Appearance
b
1
a
Disappearance
1
A
B
A
B
1
2
1
1
Position change in the -w direction
-w
Base
Arm
A
B
A
B
 r1
 r2
 r3
 r4
 r5
 r6
 r7
 r7
Active Self-Assembly and Molecular Robotics with
Nubots, Fig. 1 Overview of the nubot model. (a) A
nubot conﬁguration showing a single nubot monomer
on the triangular grid. (b) Examples of nubot monomer
rules. Rules r1–r6 are local cellular automaton-like rules,
whereas r7 effects a nonlocal movement that may translate
other monomers as shown in Fig. 2. Monomers contin-
uously undergo agitation, as shown in Fig. 3. A ﬂexible
bond is depicted as an empty red circle and a rigid bond is
depicted as a solid red disk (from [8])
1
2
1
2
1
2
(0,0)
a
b
(0,0)
1
2
2
(0,0)
c
d
e
1
1
2
(0,0)
Active Self-Assembly and Molecular Robotics with
Nubots, Fig. 2 Movement rule. (a) Initial conﬁguration.
(b) Movement rule with one of two results depending on
the choice of arm or base. (c) Result if the monomer with
state 2 is the arm or (d) monomer with state 1 is the arm.
The shaded monomers are the movable set. The affect on
rigid (ﬁlled red disks), ﬂexible (hollow red circles), and
null bonds is shown. (e) A conﬁguration for which the
movement rule is blocked: movement of 1 or 2 would
force the other to move; hence the rule is not applicable
(from [3])

Active Self-Assembly and Molecular Robotics with Nubots
15
A
1
2
(0,0)
2
(0,0)
2
1
1
1
2
1
2
Active Self-Assembly and Molecular Robotics with
Nubots, Fig. 3 Example agitations. Starting from the
centre conﬁguration, there are 48 possible agitations (8
monomers, 6 directions each) each with equal probability.
The right conﬁguration is the result of agitation of the
monomer in state 2 in direction !, the left is the result
of the agitation of the monomer in state 1 in direction  .
The shaded monomers are the agitation set – monomers
that are moved by the agitation (from [4])
one step as shown in Fig. 3. Unlike movement,
agitations are never blocked. Rules are applied
asynchronously and in parallel. Taking its time
model from stochastic chemical kinetics, a nubot
system evolves as a continuous time Markov
process.
For intuition, we describe motion in terms of
pushing and pulling. However movement and ag-
itation are actually intended to model a nanoscale
environment with diffusion, Brownian motion,
convection, turbulent ﬂow, cytoplasmic stream-
ing, and other uncontrolled inputs of energy that
interact monomers in all directions, moving large
molecular assemblies in a random fashion (i.e.,
agitation) and allowing motors to simply latch
and unlatch large assemblies into position (i.e.,
the movement rule).
Key Results
Assembling simple structures, namely, lines and
squares, has proven to be a fruitful way to explore
the power of the nubot model for a few reasons.
Firstly, it helps us develop a number of tech-
niques and intuitions for the model. Secondly,
lines and squares get used again and again in
more general results that show the full power of
the model. Thirdly, the efﬁciency of assembling
simple shapes has been a de facto benchmark
problem for a number of self assembly models
(although this benchmark often does not give the
full story). In a variety of models, such as the
abstract Tile Assembly Model, cellular automata,
and some robotics models, it takes time ˝.n/ to
assemble a length n line. In the nubot model this
is achieved in merely O.log n/ expected time and
O.log n/ states.
Theorem 1 ([8]) For each n 2 N, there is a
set of nubot rules N line
n
such that starting from
a single monomer N line
n
assembles a length n
line in O.log n/ expected time, n  2 space, and
O.log n/ states.
One can trade time for states by giving a slightly
slower method with fewer states:
Theorem 2 ([3]) There is a set of nubot rules
N line such that for each n 2 N, from a line of
O.log n/ “binary” monomers (each in state 0 or
1), N line assembles a length n line in O.log2 n/
expected time, n  O.1/ space, and O.1/
states.
An n  n square can be built by growing a
horizontal line and then n vertical lines, showing
that assembly of squares with nubots is exponen-
tially faster than the 	.n/ expected time seen in
the abstract Tile Assembly Model [1]:
Theorem 3 ([8]) For each n 2 N, there is a
set of nubot rules N square
n
such that starting from
a single monomer, N square
n
assembles a n  n
square in O.log n/ expected time, n  n space,
and O.log n/ states.
The results above, and all of those in [3, 8],
crucially make use of the rigid-body movement
rule: the ability for a single monomer to control
the movement of large objects quickly and at
a time and place of the programmer’s choos-
ing. However, in a molecular-scale environment,
molecular motion is happening in a largely un-
controlled and fundamentally random manner,
all of the time. The agitation nubot model does

16
Active Self-Assembly and Molecular Robotics with Nubots
not have the movement rule, but instead permits
such uncontrolled random agitation (movement).
Although this form of movement is challenging
to control in a precise manner, the following
result shows we can use it to achieve sublinear
expected time growth of a length n line in only
O.n/ space:
Theorem 4 ([4]) There is a set of nubot rules
Nline, such that 8n 2 N, starting from a line of
blog2 ncC1 monomers, each in state 0 or 1, Nline
in the agitation nubot model assembles an n  1
line in O.n1=3 log n/ expected time, n  5 space,
and O.1/ monomer states.
For a square we can do much better, achieving
polylogarithmic expected time:
Theorem 5 ([4]) There is a set of nubot rules
Nsquare, such that 8n
2
N, starting from
a line of blog2 nc C 1 monomers, each in
state 0 or 1, Nsquare in the agitation nubot
model assembles an n  n square in O.log2 n/
expected time, n  n space, and O.1/ monomer
states.
This section concludes with three results
on
general-purpose
computation
and
shape
construction with the nubot model. First we
have a computability-theoretic result: any ﬁnite
computable connected shape can be quickly self-
assembled.
Theorem 6 ([8]) An arbitrary connected com-
putable 2D shape of size  pn  pn can be
assembled in expected time O.log2 n C t.jnj//
using O.s C log n/ states. Here, t.jnj/ is the time
required for a program-size s Turing machine to
compute, given a pixel index as a binary string
of length jnj D blog2 nc C 1, whether or not the
pixel is present in the shape.
For
complicated
computable
shapes
the
construction for Theorem 6 necessarily requires
computation workspace outside of the shape’s
bounding box. The next result is of a more
resource-bounded style and, roughly speaking,
states that 2D patterns with efﬁciently com-
putable pixel colors can be assembled using
nubots in merely polylogarithmic expected time
while staying inside the pattern’s bounding box.
Theorem 7 ([8]) An arbitrary ﬁnite computable
2D pattern of size  n  n, where n D 2p; p 2
N, with pixels whose color is computable on
a polynomial time O.jnj`/ (inputs are binary
strings of length jnj D O.log n/), linear space
O.jnj/, program-size s Turing machine, can be
assembled in expected time O.log`C1 n/, with
O.s C log n/ monomer states and without grow-
ing outside the pattern borders.
The results cited so far can be used to compare
the nubot model to other models of self-assembly
and tell us that nubots build shapes and patterns
in a fast parallel manner. The next result quan-
tiﬁes this parallelism in terms of a well-known
parallel model from computational complexity
theory: NC is the class of problems solved by
uniform polylogarithmic depth and polynomial-
size Boolean circuits.
Theorem 8 ([3]) For each language L 2 NC,
there is a set of nubot rules NL that decides L in
polylogarithmic expected time, constant number
of monomer states, and polynomial space in the
length of the input string of binary monomers
(in state 0 or 1). The output is a single binary
monomer.
This result stands in contrast to sequential ma-
chines like Turing machines, that cannot read all
of an n-bit input string in polylogarithmic time,
and “somewhat parallel” models like cellular au-
tomata and the abstract Tile Assembly Model,
that cannot have all of n bits inﬂuence a single
output bit decision in polylogarithmic time [5].
Thus, adding the nubot rigid-body movement
primitive to an asynchronous nondeterministic
cellular automaton drastically increases its paral-
lel processing abilities.
Open Problems
Some future research directions are discussed
here and in [3, 4, 8]. It remains as future work
to look at other topics such as fault tolerance,
self-healing, dynamical tasks, or systems that
continuously respond to the environment.

Active Self-Assembly and Molecular Robotics with Nubots
17
A
The Complexity of Assembling Lines
Theorem 1 states that a line can be grown in
expected time O.log n/, space O.n/  O.1/,
and O.log n/ states, and Theorem 2 trades time
for states to get expected time O.log2 n/, space
O.n/  O.1/, and O.1/ states. What is the com-
plexity (expected time  states) of assembling a
line in the nubot model? Is it possible to meet
the lower bound of expected time  states D
˝.log n/? In this problem, the input should be a
set of monomers with space  states D O.log n/.
Computational Power
Theorem 8 gives a lower bound on the compu-
tational complexity of the nubot model. What is
the exact power of polylogarithmic expected time
nubots? The answer may differ on whether we
begin from a small collection of monomers (as
in Theorem 8) or a large prebuilt structure. One
challenge, for the upper bound, involves ﬁnding
better Turing machine space, or circuit depth,
bounds on computing multiple applications of the
movable set on a large nubots grid.
Synchronization and Composition of
Nubot Algorithms
Synchronization is a method to quickly send
signals using nonlocal rigid-body motion [3, 8].
The nubot model is asynchronous, but synchro-
nization can be used to set discrete stages, or
checkpoints, during a complicated construction.
This in turn facilitates composition of nubot al-
gorithms (run algorithm 1, synchronize, run al-
gorithm 2, synchronize, etc.) and many of the
results cited here use it for exactly that reason.
However, synchronization-less constructions of-
ten exhibit a kind of independence where growth
proceeds everywhere in parallel, without waiting
on signals from distant components. Such sys-
tems are highly distributed, easy to analyze, and
perhaps more amenable to laboratory implemen-
tation. Intuitively, this seems like the right way
to program molecules. The proof of Theorem 7
does not use synchronization which shows that
without it a very general class of (efﬁciently)
computable patterns can be grown and indeed
the proof gives methods to compose nubot al-
gorithms without resorting to synchronization.
It remains as future work to formalize both this
notion of synchronization-less “independence”
and what we mean by “composition” of nubot
algorithms. What conditions are necessary and
sufﬁcient for composition of nubot algorithms?
What classes of shapes and patterns can be as-
sembled using without synchronization or other
forms of rapid long-range communication?
Agitation Versus the Movement Rule
Is it possible to simulate the movement rule using
agitation? More formally, is it the case that for
each nubot program N, there is an agitation
nubot program AN , that acts just like N but with
some m  m scale-up in space, and a k factor
slowdown in time, where m and k are (constants)
independent of N and its input? As motivation,
note that every self-assembled molecular-scale
structure was made under conditions where ran-
dom jiggling of monomers is a dominant source
of movement! Our question asks if we can pro-
grammably exploit this random molecular motion
to build structures quicker than without it.
Intrinsic Universality and Simulation
Is the nubot model intrinsically universal? Specif-
ically, does there exist a set of monomer rules U ,
such that any nubot system N can be simulated
by “seeding” U with a suitable initial conﬁgura-
tion? Here the simulation should have a spatial
scale factor m that is a function of the number of
states in the simulated system N. Is the agitation
nubot model intrinsically universal? Our hope
would be that simulation could be used to tease
apart the power of different notions of movement
(e.g., to understand if nubot-style movement is
weaker or stronger than other notions of robotic
movement), in the way it has been used to char-
acterize and separate the power of other self-
assembly models [7].
Brownian Nubots
With nubots, under agitation, or multiple parallel
movement rules, larger objects move faster. This
is intended to model an environment with uncon-
trolled and rapid ﬂuid ﬂows. But in Brownian
motion, larger objects move slower: what is the
power of nubots with such a rate model, for

18
Adaptive Partitions
example, with rate equal to object size? Although
assembly in such a model may be slower than
with the usual model, many of the same program-
ming principles should apply, and indeed it will
still be possible to assemble objects in a parallel
distributed fashion.
Cross-References
▷Combinatorial Optimization and Veriﬁcation in
Self-Assembly
▷Intrinsic Universality in Self-Assembly
▷Patterned Self-Assembly Tile Set Synthesis
▷Randomized Self-Assembly
▷Robustness in Self-Assembly
▷Self-Assembly at Temperature 1
▷Self-Assembly of Fractals
▷Self-Assembly of Squares and Scaled Shapes
▷Self-Assembly with General Shaped Tiles
▷Temperature Programming in Self-Assembly
Acknowledgments A warm thanks to all of my coau-
thors on this topic and especially to Erik Winfree and
Chris Thachuk for their helpful comments. The author
is supported by NSF grants 0832824, 1317694, CCF-
1219274, and CCF-1162589.
Recommended Reading
1. Adleman LM, Cheng Q, Goel A, Huang MD (2001)
Running time and program size for self-assembled
squares. In: STOC 2001: proceedings of the 33rd
annual ACM symposium on theory of computing,
Hersonissos. ACM, pp 740–748
2. Bath J, Turberﬁeld A (2007) DNA nanomachines. Nat
Nanotechnol 2:275–284
3. Chen M, Xin D, Woods D (2013) Parallel computation
using active self-assembly. In: DNA19: the 19th inter-
national conference on DNA computing and molecular
programming. LNCS, vol 8141. Springer, pp 16–30.
arxiv preprint arXiv:1405.0527
4. Chen HL, Doty D, Holden D, Thachuk C, Woods
D, Yang CT (2014) Fast algorithmic self-assembly of
simple shapes using random agitation. In: DNA20: the
20th international conference on DNA computing and
molecular programming. LNCS, vol 8727. Springer,
pp 20–36. arxiv preprint: arXiv:1409.4828
5. Keenan A, Schweller R, Sherman M, Zhong X
(2014) Fast arithmetic in algorithmic self-assembly.
In: UCNC: the 13th international conference on
unconventional computation and natural computation.
LNCS, vol 8553. Springer, pp 242–253. arxiv preprint
arXiv:1303.2416 [cs.DS]
6. Winfree E (1998) Algorithmic self-assembly of DNA.
PhD thesis, California Institute of Technology
7. Woods
D
(2015)
Intrinsic
universality
and
the
computational
power
of
self-assembly.
Philos
Trans R Soc A: Math Phys Eng Sci 373(2046).
doi:10.1098/rsta.2014.0214,
ISBN:1471-2962,
ISSN:1364-503X
8. Woods D, Chen HL, Goodfriend S, Dabby N, Winfree
E, Yin P (2013) Active self-assembly of algorith-
mic shapes and patterns in polylogarithmic time. In:
ITCS’13: proceedings of the 4th conference on inno-
vations in theoretical computer science. ACM, pp 353–
354. Full version: arXiv:1301.2626 [cs.DS]
Adaptive Partitions
Ping Deng1, Weili Wu1;4;5, Eugene
Shragowitz2, and Ding-Zhu Du1;3
1Department of Computer Science, The
University of Texas at Dallas, Richardson,
TX, USA
2Department of Computer Science and
Engineering, University of Minnesota,
Minneapolis, MN, USA
3Computer Science, University of Minnesota,
Minneapolis, MN, USA
4College of Computer Science and Technology,
Taiyuan University of Technology, Taiyuan,
Shanxi Province, China
5Department of Computer Science, California
State University, Los Angeles, CA, USA
Keywords
Technique for constructing approximation
Years and Authors of Summarized
Original Work
1986; Du, Pan, Shing

Adaptive Partitions
19
A
Problem Deﬁnition
Adaptive partition is one of major techniques
to
design
polynomial-time
approximation
algorithms, especially polynomial-time approx-
imation schemes for geometric optimization
problems. The framework of this technique
is to put the input data into a rectangle and
partition this rectangle into smaller rectangles
by a sequence of cuts so that the problem is also
partitioned into smaller ones. Associated with
each adaptive partition, a feasible solution can be
constructed recursively from solutions in smallest
rectangles to bigger rectangles. With dynamic
programming, an optimal adaptive partition is
computed in polynomial time.
Historical Note
The adaptive partition was ﬁrst introduced to
the design of an approximation algorithm by Du
et al. [4] with a guillotine cut while they studied
the minimum edge-length rectangular partition
(MELRP) problem. They found that if the par-
tition is performed by a sequence of guillotine
cuts, then an optimal solution can be computed
in polynomial time with dynamic programming.
Moreover, this optimal solution can be used as a
pretty good approximation solution for the origi-
nal rectangular partition problem. Both Arora [1]
and Mitchell et al. [12,15] found that the cut does
not need to be completely guillotine. In other
words, the dynamic programming can still run
in polynomial time if subproblems have some
relations but the number of relations is small.
As the number of relations goes up, the approxi-
mation solution obtained approaches the optimal
one, while the run time, of course, goes up. They
also found that this technique can be applied to
many geometric optimization problems to obtain
polynomial-time approximation schemes.
Key Results
The MELRP was proposed by Lingas et al. [10]
as follows: Given a rectilinear polygon possibly
with some rectangular holes, partition it into
rectangles with minimum total edge length. Each
hole may be degenerated into a line segment or a
point.
There are several applications mentioned in
[10] for the background of the problem: process
control (stock cutting), automatic layout systems
for integrated circuit (channel deﬁnition), and
architecture (internal partitioning into ofﬁces).
The minimum edge-length partition is a natural
goal for these problems since there is a cer-
tain amount of waste (e.g., sawdust) or expense
incurred (e.g., for dividing walls in the ofﬁce)
which is proportional to the sum of edge lengths
drawn. For very large-scale integration (VLSI)
design, this criterion is used in the MIT place-
ment and interconnect (PI) system to divide the
routing region up into channels – one ﬁnds that
this produces large “natural-looking” channels
with a minimum of channel-to-channel interac-
tion to consider.
They showed that while the MELRP in general
is nondeterministic polynomial-time (NP)-hard,
it can be solved in time O.n4/ in the hole-
free case, where n is the number of vertices
in the input rectilinear polygon. The polynomial
algorithm is essentially a dynamic programming
based on the fact that there always exists an opti-
mal solution satisfying the property that every cut
line passes through a vertex of the input polygon
or holes (namely, every maximal cut segment is
incident to a vertex of input or holes).
A naive idea to design an approximation al-
gorithm for the general case is to use a forest
connecting all holes to the boundary and then to
solve the resulting hole-free case in O.n4/ time.
With this idea, Lingas [9] gave the ﬁrst constant-
bounded approximation; its performance ratio
is 41.
Motivated by a work of Du et al. [6] on
application of dynamic programming to opti-
mal routing trees, Du et al. [4] initiated an idea
of adaptive partition. They used a sequence of
guillotine cuts to do rectangular partition; each
guillotine cut breaks a connected area into at least
two parts. With dynamic programming, they were
able to show that a minimum-length guillotine
rectangular partition (i.e., one with minimum
total length among all guillotine partitions) can

20
Adaptive Partitions
be computed in O.n5/ time. Therefore, they
suggested using the minimum-length guillotine
rectangular partition to approximate the MELRP
and tried to analyze the performance ratio. Un-
fortunately, they failed to get a constant ratio in
general and only obtained an upper bound of 2 for
the performance ratio in an NP-hard special case
[7]. In this special case, the input is a rectangle
with some points inside. Those points are holes.
The following is a simple version of the proof
obtained by Du et al. [5].
Theorem 1 The minimum-length guillotine rect-
angular partition is an approximation with per-
formance ratio 2 for the MELRP.
Proof Consider a rectangular partition P . Let
proj x (P ) denote the total length of segments on
a horizontal line covered by vertical projection of
the partition P .
A rectangular partition is said to be covered
by a guillotine partition if each segment in the
rectangular partition is covered by a guillotine
cut of the latter. Let guil(P ) denote the minimum
length of the guillotine partition covering P and
length(P ) denote the total length of rectangular
partition P . It will be proved by induction on the
number k of segments in P that
guil.P /  2  length.P /  projx.P /:
For k D 1, one has guil.P / D length.P /. If the
segment is horizontal, then one has projx.P / D
length.P / and hence
guil.P / D 2  length.P /  projx.P / :
If the segment is vertical, then projx.P / D 0 and
hence
guil.P / < 2  length.P /  projx.P /:
Now, consider k  2. Suppose that the initial
rectangle has each vertical edge of length a and
each horizontal edge of length b. Consider two
cases.
Case 1.
There exists a vertical segment s having
length greater than or equal to 0:5a. Apply a
guillotine cut along this segment s. Then the
remainder of P is divided into two parts, P1
and P2, which form rectangular partition of two
resulting small rectangles, respectively. By induc-
tion hypothesis,
guil.Pi/  2  length.Pi/  projx.Pi/
for i D 1; 2. Note that
guil .P /  guil .P1/ C guil .P2/ C a;
length .P / D length .P1/ C length .P2/
C length .s/ ;
projx .P / D projx .P1/ C projx .P2/ :
Therefore,
guil.P /  2  length.P /  projx.P /:
Case 2.
No vertical segment in P has length
greater than or equal to 0:5a. Choose a horizontal
guillotine cut which partitions the rectangle into
two equal parts. Let P1 and P2 denote rectangle
partitions of the two parts, obtained from P . By
induction hypothesis,
guil.Pi/  2  length.Pi/  projx.Pi/
for i D 1; 2. Note that
guil.P / D guil.P1/ C guil.P2/ C b;
length.P /  length.P1/ C length.P2/;
projx.P / D projx.P1/ D projx.P2/ D b:
Therefore,
guil.P /  2  length.P /  projx.P / :
Gonzalez and Zheng [8] improved this upper
bound to 1.75 and conjectured that the perfor-
mance ratio in this case is 1.5.
ut

Adaptive Partitions
21
A
Applications
In 1996, Arora [1] and Mitchell et al. [12,14,15]
found that the cut does not necessarily have
to be completely guillotine in order to have a
polynomial-time computable optimal solution for
such a sequence of cuts. Of course, the number
of connections left by an incomplete guillotine
cut should be limited. While Mitchell et al. de-
veloped the m-guillotine subdivision technique,
Arora employed a “portal” technique. They also
found that their techniques can be used for not
only the MELRP, but also for many geometric
optimization problems [1–3,12–15].
Open Problems
One current important submicron step of tech-
nology evolution in electronics interconnects has
become the dominating factor in determining
VLSI performance and reliability. Historically
a problem of interconnects design in VLSI has
been very tightly intertwined with the classi-
cal problem in computational geometry: Steiner
minimum tree generation. Some essential char-
acteristics of VLSI are roughly proportional to
the length of the interconnects. Such character-
istics include chip area, yield, power consump-
tion, reliability, and timing. For example, the
area occupied by interconnects is proportional
to their combined length and directly impacts
the chip size. Larger chip size results in re-
duction of yield and increase in manufacturing
cost. The costs of other components required for
manufacturing also increase with the increase of
the wire length. From the performance angle,
longer interconnects cause an increase in power
dissipation, degradation of timing, and other un-
desirable consequences. That is why ﬁnding the
minimum length of interconnects consistent with
other goals and constraints is such an important
problem at this stage of VLSI technology.
The combined length of the interconnects on
a chip is the sum of the lengths of individual
signal nets. Each signal net is a set of electrically
connected terminals, where one terminal acts
as a driver and other terminals are receivers of
electrical signals. Historically, for the purpose
of ﬁnding an optimal conﬁguration of intercon-
nects, terminals were considered as points on the
plane, and a routing problem for individual nets
was formulated as a classical Steiner minimum
tree problem. For a variety of reasons, VLSI
technology implements only rectilinear wiring
on the set of parallel planes, and, consequently,
with few exceptions, only a rectilinear version of
the Steiner tree is being considered in the VLSI
domain. This problem is known as the RSMT.
Further progress in VLSI technology resulted
in more factors than just length of interconnects
gaining importance in selection of routing topolo-
gies. For example, the presence of obstacles led
to reexamination of techniques used in studies of
the rectilinear Steiner tree, since many classical
techniques do not work in this new environment.
To clarify the statement made above, we will
consider the construction of a rectilinear Steiner
minimum tree in the presence of obstacles.
Let us start with a rectilinear plane with ob-
stacles deﬁned as rectilinear polygons. Given n
points on the plane, the objective is to ﬁnd the
shortest rectilinear Steiner tree that interconnects
them. One already knows that a polynomial-time
approximation scheme for RSMT without obsta-
cles exists and can be constructed by adaptive
partition with application of either the portal or
the m-guillotine subdivision technique. However,
both the m-guillotine cut and the portal tech-
niques do not work in the case that obstacles ex-
ist. The portal technique is not applicable because
obstacles may block the movement of the line that
crosses the cut at a portal. The m-guillotine cut
could not be constructed either, because obstacles
may break down the cut segment that makes the
Steiner tree connected.
In spite of the facts stated above, the RSMT
with obstacles may still have polynomial-time
approximation schemes. Strong evidence was
given by Min et al. [11]. They constructed a
polynomial-time approximation scheme for the
problem with obstacles under the condition that
the ratio of the longest edge and the shortest edge
of the minimum spanning tree is bounded by a
constant. This design is based on the classical
nonadaptive partition approach. All of the above

22
Additive Spanners
make us believe that a new adaptive technique
can be found for the case with obstacles.
Cross-References
▷Metric TSP
▷Rectilinear Steiner Tree
▷Steiner Trees
Recommended Reading
1. Arora S (1996) Polynomial-time approximation
schemes for Euclidean TSP and other geometric
problems. In: Proceedings of the 37th IEEE sympo-
sium on foundations of computer science, pp 2–12
2. Arora S (1997) Nearly linear time approximation
schemes for Euclidean TSP and other geometric
problems. In: Proceedings of the 38th IEEE sympo-
sium on foundations of computer science, pp 554–
563
3. Arora S (1998) Polynomial-time approximation
schemes for Euclidean TSP and other geometric
problems. J ACM 45:753–782
4. Du D-Z, Pan, L-Q, Shing, M-T (1986) Minimum
edge length guillotine rectangular partition. Technical
report 0241886, Math. Sci. Res. Inst., Univ. Califor-
nia, Berkeley
5. Du D-Z, Hsu DF, Xu K-J (1987) Bounds on guillotine
ratio. Congressus Numerantium 58:313–318
6. Du DZ, Hwang FK, Shing MT, Witbold T (1988)
Optimal routing trees. IEEE Trans Circuits 35:1335–
1337
7. Gonzalez T, Zheng SQ (1985) Bounds for partition-
ing rectilinear polygons. In: Proceedings of the 1st
symposium on computational geometry
8. Gonzalez T, Zheng SQ (1989) Improved bounds for
rectangular and guillotine partitions. J Symb Comput
7:591–610
9. Lingas A (1983) Heuristics for minimum edge
length rectangular partitions of rectilinear ﬁgures.
In: Proceedings of the 6th GI-conference, Dortmund.
Springer
10. Lingas A, Pinter RY, Rivest RL, Shamir A (1982)
Minimum edge length partitioning of rectilinear poly-
gons. In: Proceedings of the 20th Allerton conference
on communication, control, and computing, Illinois
11. Min M, Huang SC-H, Liu J, Shragowitz E, Wu W,
Zhao Y, Zhao Y (2003) An approximation scheme for
the rectilinear Steiner minimum tree in presence of
obstructions. Fields Inst Commun 37:155–164
12. Mitchell JSB (1996) Guillotine subdivisions approx-
imate polygonal subdivisions: a simple new method
for the geometric k-MST problem. In: Proceedings
of the 7th ACM-SIAM symposium on discrete algo-
rithms, pp 402–408
13. Mitchell JSB (1997) Guillotine subdivisions ap-
proximate polygonal subdivisions: part III – faster
polynomial-time approximation scheme for geomet-
ric network optimization, manuscript, State Univer-
sity of New York, Stony Brook
14. Mitchell JSB (1999) Guillotine subdivisions approx-
imate polygonal subdivisions: part II – a simple
polynomial-time approximation scheme for geomet-
ric k-MST, TSP, and related problem. SIAM J Com-
put 29(2):515–544
15. Mitchell JSB, Blum A, Chalasani P, Vempala S
(1999) A constant-factor approximation algorithm for
the geometric k-MST problem in the plane. SIAM J
Comput 28(3):771–781
Additive Spanners
Shiri Chechik
Department of Computer Science, Tel Aviv
University, Tel Aviv, Israel
Keywords
Approximate shortest paths; Spanners; Stretch
factor
Years and Authors of Summarized
Original Work
2013; Chechik
Problem Deﬁnition
A spanner is a subgraph of a given graph that
faithfully preserves the pairwise distances of that
graph. Formally, an .˛; ˇ/ spanner of a graph
G D .V; E/ is a subgraph H of G such that
for any pair of nodes x; y, dist.x; y; H/  ˛ 
dist.x; y; G/ C ˇ, where dist.x; y; H 0/ for a
subgraph H 0 is the distance of the shortest path
from s to t in H 0. We say that the spanner is
additive if ˛ D 1, and if in addition ˇ D O.1/,
we say that the spanner is purely additive. If
ˇ D 0, we say that the spanner is multiplicative;
otherwise, we say that the spanner is mixed.

Additive Spanners
23
A
Key Results
This
section
presents
a
survey
on
span-
ners with a special focus on additive span-
ners.
Graph spanners were ﬁrst introduced in
[12,13] in the late 1980s and have been
extensively studied since then.
Spanners are used as a key ingredient in many
distributed applications, e.g., synchronizers [13],
compact routing schemes [6, 14, 17], broadcast-
ing [11], etc.
Much of the work on spanners considers mul-
tiplicative spanners. A well-known theorem on
multiplicative spanners is that one can efﬁciently
construct a .2k  1; 0/ spanner with O

n1C1=k
edges [2]. Based in the girth conjecture of Erd˝os
[10], this size-stretch ratio is conjectured to be
optimal.
The problem of additive spanners was also
extensively studied, but yet several key questions
remain open. The girth conjecture does not con-
tradict the existence of .1; 2k2/ spanners of size
O

n1C1=k
, or in fact any .˛; ˇ/ spanners of size
O

n1C1=k
such that ˛Cˇ D 2k 1 with ˛  1
and ˇ > 0.
The ﬁrst construction for purely additive
spanners was introduced by Aingworth et al. [1].
It was shown in [1] how to efﬁciently construct a
.1; 2/ spanner, or a 2-additive spanner for short,
with O

n3=2
edges (see also [8, 9, 16, 19] for
further follow-up). Later, Baswana et al. [3, 4]
presented an efﬁcient construction for 6-additive
spanners with O

n4=3
edges. Woodruff [21]
further presented another construction for 6-
additive spanners with
QO

n4=3
edges with
improved construction time. Chechik [7] recently
presented a new algorithm for .1; 4/-additive
spanners with QO

n7=5
edges. These are the only
three constructions known for purely additive
spanners. Interestingly, Woodruff [20] presented
lower bounds for additive spanners that match the
girth conjecture bounds. These lower bounds do
not rely on the correctness of the conjecture.
More precisely, Woodruff [20] showed the
existence of graphs for which any spanner of size
O

k1n1C1=k
must have an additive stretch of
at least 2k  1.
In the absence of additional purely additive
spanners, attempts were made to seek sparser
spanners with nonconstant additive stretch. Bol-
lobás et al. [5] showed how to efﬁciently con-
struct a

1; n12ı
spanner with O

21=ın1Cı
edges for any ı > 0. Later, Baswana et al. in [3,4]
improved this additive stretch to

1; n13ı
, and
in addition, Pettie [15] improved the stretch to

1; n9=167ı=8
(the latter is better than the former
for every ı < 7=34).
Chechik [7] recently further improved the
stretch for a speciﬁc range of ı. More speciﬁcally,
Chechik presented a construction for additive
spanners with QO

n1Cı
edges and QO

n1=23ı=2
additive stretch for any 3=17  ı < 1=3. Namely,
[7] decreased the stretch for this range to the root
of the best previously known additive stretch.
Sublinear additive spanners, that is, spanners
with additive stretch that is sublinear in the
distances, were also studied. Thorup and Zwick
[19] showed a construction of a O

kn1C1=k
size spanner such that for every pair of nodes s
and t, the additive stretch is O

d 11=k C 2k
,
where d D dist.s; t/ is the distance between
s and t. This was later improved by Pettie
[15]
who
presented
an
efﬁcient
spanner
construction with O

kn
1C
.3=4/k2
72.3=4/k2

size
and O

kd 11=k C kk
additive stretch, where
d D dist.s; t/. Speciﬁcally, for k D 2, the size of
the spanner is O

n6=5
and the additive stretch
is O
p
d

.
Chechik [7] slightly improved the size of
Pettie’s [15] sublinear additive spanner with
additive stretch O.
p
d/ from O

n1C1=5
to
QO

n1C3=17
.
Open Problems
A major open problem in the area of additive
spanners is on the existence of purely additive
spanners with O

n1Cı
for any ı
>
0. In
particular, proving or disproving the existence of
a spanner of size O

n4=3
for some constant
 with constant or even polylog additive stretch
would be a major breakthrough.

24
Adwords Pricing
Recommended Reading
1. Aingworth D, Chekuri C, Indyk P, Motwani R
(1999) Fast estimation of diameter and shortest paths
(without matrix multiplication). SIAM J Comput
28(4):1167–1181
2. Althöfer I, Das G, Dobkin D, Joseph D, Soares
J (1993) On sparse spanners of weighted graphs.
Discret Comput Geom 9:81–100
3. Baswana S, Kavitha T, Mehlhorn K, Pettie S (2005)
New constructions of .˛; ˇ/-spanners and purely
additive spanners. In: Proceedings of the 16th sym-
posium on discrete algorithms (SODA), Vancouver,
pp 672–681
4. Baswana S, Kavitha T, Mehlhorn K, Pettie S (2010)
Additive spanners and .˛; ˇ/-spanners. ACM Trans
Algorithm 7:1–26. A.5
5. Bollobás B, Coppersmith D, Elkin M (2003) Sparse
distance preservers and additive spanners. In: Pro-
ceedings of the 14th ACM-SIAM symposium on
discrete algorithms (SODA), Baltimore, pp 414–423
6. Chechik S (2013) Compact routing schemes with
improved stretch. In: Proceedings of the 32nd ACM
symposium on principles of distributed computing
(PODC), Montreal, pp 33–41
7. Chechik S (2013) New additive spanners. In: Pro-
ceedings of the 24th symposium on discrete algo-
rithms (SODA), New Orleans, pp 498–512
8. Dor D, Halperin S, Zwick U (2000) All-pairs almost
shortest paths. SIAM J Comput 29(5):1740–1759
9. Elkin M, Peleg D (2004) .1 C ; ˇ/-spanner con-
structions for general graphs. SIAM J Comput
33(3):608–631
10. Erd˝os P (1964) Extremal problems in graph theory.
In: Theory of graphs and its applications. Methuen,
London, pp 29–36
11. Farley AM, Proskurowski A, Zappala D, Windisch
K (2004) Spanners and message distribution in net-
works. Discret Appl Math 137(2):159–171
12. Peleg D, Schäffer AA (1989) Graph spanners. J
Graph Theory 13:99–116
13. Peleg D, Ullman JD (1989) An optimal synchronizer
for the hypercube. SIAM J Comput 18(4):740–747
14. Peleg D, Upfal E (1989) A trade-off between
space and efﬁciency for routing tables. J ACM
36(3):510–530
15. Pettie S (2009) Low distortion spanners. ACM Trans
Algorithms 6(1):1–22
16. Roditty L, Thorup M, Zwick U (2005) Deterministic
constructions of approximate distance oracles and
spanners. In: Proceedings of the 32nd international
colloquium on automata, languages and program-
ming (ICALP), Lisbon, pp 261–272
17. Thorup M, Zwick U (2001) Compact routing
schemes. In: Proceedings of the 13th ACM sym-
posium on parallel algorithms and architectures
(SPAA), Heraklion, pp 1–10
18. Thorup M, Zwick U (2005) Approximate distance
oracles. J ACM 52(1):1–24
19. Thorup M, Zwick U (2006) Spanners and emulators
with sublinear distance errors. In: Proceedings of the
17th ACM-SIAM symposium on discrete algorithms
(SODA), Miami, pp 802–809
20. Woodruff DP (2006) Lower bounds for additive span-
ners, emulators, and more. In: Proceedings of the 47th
IEEE symposium on foundations of computer science
(FOCS), Berkeley, pp 389–398
21. Woodruff DP (2010) Additive spanners in nearly
quadratic time. In: Proceedings of the 37th interna-
tional colloquium on automata, languages and pro-
gramming (ICALP), Bordeaux, pp 463–474
Adwords Pricing
Tian-Ming Bu
Software Engineering Institute, East China
Normal University, Shanghai, China
Keywords
Adword
auction;
Convergence;
Dynamics;
Equilibrium; Mechanism design
Years and Authors of Summarized
Original Work
2007; Bu, Deng, Qi
Problem Deﬁnition
The model studied here is the same as that which
is ﬁrst presented in [10] by Varian. For some
keyword, N
D f1; 2; : : : ; N g advertisers bid
K D f1; 2; : : : ; Kg advertisement slots (K < N )
which will be displayed on the search result page
from top to bottom. The higher the advertisement
is positioned, the more conspicuous it is and the
more clicks it receives. Thus for any two slots
k1; k2 2 K, if k1 < k2, then slot k1’s click-
through rate (CTR) ck1 is larger than ck2. That
is, c1 > c2 >    > cK, from top to bottom,
respectively. Moreover, each bidder i 2 N has
privately known information, vi, which repre-
sents the expected return of per click to bidder i.

Adwords Pricing
25
A
According to each bidder i’s submitted bid bi,
the auctioneer then decides how to distribute the
advertisement slots among the bidders and how
much they should pay for per click. In particular,
the auctioneer ﬁrst sorts the bidders in decreasing
order according to their submitted bids. Then the
highest slot is allocated to the ﬁrst bidder, the
second highest slot is allocated to the second
bidder, and so on. The last N  K bidders would
lose and get nothing. Finally, each winner would
be charged on a per-click basis for the next bid in
the descending bid queue. The losers would pay
nothing.
Let bk denote the kth highest bid in the de-
scending bid queue and vk the true value of the
kth bidder in the descending queue. Thus if bid-
der i got slot k, i’s payment would be bkC1  ck.
Otherwise, his payment would be zero. Hence,
for any bidder i 2 N, if i were on slot k 2 K,
his/her utility (payoff) could be represented as
ui
k D .vi  bkC1/  ck
Unlike one-round sealed-bid auctions where
each bidder has only one chance to bid, the
adword auction allows bidders to change their
bids any time. Once bids are changed, the
system
refreshes
the
ranking
automatically
and instantaneously. Accordingly, all bidders’
payment and utility are also recalculated. As a
result, other bidders could then have an incentive
to change their bids to increase their utility
and so on.
Deﬁnition 1 (Adword Pricing)
INPUT: The CTR for each slot, each bidder’s
expected return per click on his/her advertising
OUTPUT: The stable states of this auction and
whether any of these stable states can be
reached from any initial states
Key Results
Let b represent the bid vector .b1; b2; : : : ; bN /.
8i
2
N, Oi.b/ denotes bidder i’s place
in the descending bid queue. Let bi
D
.b1; : : : ; bi1; biC1; : : : ; bN / denote the bids
of all other bidders except i. Mi.bi/ returns a
set deﬁned as
Mi.bi/ D arg
max
bi 2Œ0;vi
n
ui
Oi .bi ;bi/
o
(1)
Deﬁnition 2 (Forward-Looking Best-Response
Function) Given bi, suppose Oi.Mi.bi/;
bi/
D
k, then bidder i’s forward-looking
response function Fi.bi/ is deﬁned as
Fi.bi/
D
(
vi 
ck
ck1 .vi  bkC1/
2  k  K
vi
k D 1 or k > K
(2)
Deﬁnition 3 (Forwarding-Looking
Equilibr-
ium) A forward-looking best-response function-
based equilibrium is a strategy proﬁle Ob such that
8i 2 N; Obi 2 Fi.Obi/
Deﬁnition 4 (Output Truthful (Kao et al.,
2006, Output truthful versus input truthful:
a new concept for algorithmic mechanism
design, unpublished) [7]) For any instance
of
adword
auction
and
the
corresponding
equilibrium set E, if 8e 2 E and 8i 2 N,
Oi.e/
D
Oi.v1; : : : ; vN /, then the adword
auction is output truthful on E.
Theorem 1 An adword auction is output truthful
on Eforwardlooking.
Corollary 1 An Adword auction has a unique
forward-looking Nash equilibrium.
Corollary 2 Any bidder’s payment under the
forward-looking Nash equilibrium is equal to
his/her payment under the VCG mechanism for
the auction.
Corollary 3 For adword auctions, the auction-
eer’s revenue in a forward-looking Nash equilib-
rium is equal to his/her revenue under the VCG
mechanism for the auction.
Deﬁnition 5 (Simultaneous
Readjustment
Scheme) In
a
simultaneous
readjustment
scheme, all bidders participating in the auction

26
Adwords Pricing
will use forward-looking best-response function
F to update their current bids simultaneously,
which turns the current stage into a new stage.
Then based on the new stage, all bidders may
update their bids again.
Theorem 2 An adword auction may not always
converge to a forward-looking Nash equilibrium
under the simultaneous readjustment scheme
even when the number of slots is 3. But the
protocol converges when the number of slots is 2.
Deﬁnition 6 (Round-Robin
Readjustment
Scheme) In
the
round-robin
readjustment
scheme, bidders update their biddings one after
the other, according to the order of the bidder’s
number or the order of the slots.
Theorem 3 An adword auction may not always
converge to a forward-looking Nash equilibrium
under the round-robin readjustment scheme even
when the number of slots is 4. But the protocol
converges when the number of slots is 2 or 3.
1 Readjustment Scheme: Lowest-First(K, j, b1, b2,· · ·, bN)
1: if (j= 0) then
2:      exit
3: end if
4: Let i be the ID of the bidder whose current bid is bj (and equivalently, bi).
5: Let h = Oi(Mi(b−i), b−i).
6: Let F i(b−i) be the best response function value for Bidder i.
7: Re-sort the bid sequence. (So h is the slot of the new bid F i(b−i) of Bidder i.)
8: if (h < j) then
9:      call Lowest-First (K,j,b1,b2,···,bN),
10: else
11:      call Lowest-First(K,h−1,b1,b2,···,bN)
12: end if
Theorem 4 Adword auctions converge to a
forward-looking Nash equilibrium in ﬁnite steps
with a lowest-ﬁrst adjustment scheme.
Theorem 5 Adword
auctions
converge
to
a
forward-looking
Nash
equilibrium
with
probability 1 under a randomized readjustment
scheme.
Applications
Online adword auctions are the fastest growing
form of advertising. Many search engine compa-
nies such as Google and Yahoo! make huge prof-
its on this kind of auction. Because advertisers
can change their bids anytime, such auctions can
reduce the advertisers’ risk. Further, because the
advertisement is only displayed to those people
who are really interested in it, such auctions can
reduce the advertisers’ investment and increase
their return on investment.
For the same model, Varian [10] focuses on
a subset of Nash equilibria, called Symmetric
Nash Equilibria, which can be formulated nicely
and dealt with easily. Edelman et al. [8] study
locally envy-free equilibria, where no player can
improve his/her payoff by exchanging bids with
the player ranked one position above him/her.
Coincidently, locally envy-free equilibrium is
equal to symmetric Nash equilibrium proposed
in [10]. Further, the revenue under the forward-
looking Nash equilibrium is the same as the
lower bound under Varian’s symmetric Nash
equilibria and the lower bound under Edelman
et al.’s locally envy-free equilibria. In [6], Cary
et al. also study the dynamic model’s equilibria
and convergence based on the balanced bidding
strategy which is actually the same as the
forward-looking best-response function in [4].
Cary et al. explore the convergence properties
under two models, a synchronous model which
is the same as simultaneous readjustment scheme
in [4] and an asynchronous model which is

Algorithm DC-TREE for k-Servers on Trees
27
A
the same as randomized readjustment scheme
in [4].
In addition, there are other models for adword
auctions. Abrams [1] and Bu et al. [5] study
the model under which each bidder could submit
his/her daily budget, even the maximum number
of clicks per day, in addition to the price per
click. Both [9] and [3] study bidders’ behavior
of bidding on several keywords. Aggarwal et al.
[2] studies the model where the advertiser not
only submits a bid but additionally submits which
positions he/she is going to bid for.
Open Problems
The speed of convergence still remains open.
Does the dynamic model converge in polynomial
time under randomized readjustment scheme?
Even more, are there other readjustment scheme
that converge in polynomial time?
Cross-References
▷Multiple Unit Auctions with Budget Constraint
▷Position Auction
Recommended Reading
1. Abrams Z (2006) Revenue maximization when bid-
ders have budgets. In: Proceedings of the seven-
teenth annual ACM-SIAM symposium on discrete
algorithms (SODA-06), Miami, pp 1074–1082
2. Aggarwal G, Feldman J, Muthukrishnan S (2006)
Bidding to the top: VCG and equilibria of position-
based auctions. In: Proceedings of the 4th inter-
national workshop on approximation and online
algorithms (WAOA-2006), Zurich, pp 15–28
3. Borgs C, Chayes J, Etesami O, Immorlica N, Jain
K, Mahdian M (2006) Bid optimization in online ad-
vertisement auctions. In: 2nd workshop on sponsored
search auctions (SSA2006), in conjunction with the
ACM conference on electronic commerce (EC-06),
Ann Arbor
4. Bu TM, Deng X, Qi Q (2008) Forward looking nash
equilibrium for keyword auction. Inf Process Lett
105(2):41–46
5. Bu TM, Qi Q, Sun AW (2008) Unconditional compet-
itive auctions with copy and budget constraints. Theor
Comput Sci 393(1–3):1–13
6. Cary M, Das A, Edelman B, Giotis I, Heimerl
K, Karlin AR, Mathieu C, Schwarz M (2007)
Greedy bidding strategies for keyword auctions.
In:
Proceedings
of
the
8th
ACM
conference
on electronic commerce (EC-2007), San Diego,
pp 262–271
7. Chen X, Deng X, Liu BJ (2006) On incentive
compatible competitive selection protocol. In: Pro-
ceedings of the 12th annual international computing
and combinatorics conference (COCOON06), Taipei,
pp 13–22
8. Edelman B, Ostrovsky M, Schwarz M (2007) Internet
advertising and the generalized second price auction:
selling billions of dollars worth of keywords. Am
Econ Rev 97(1):242–259
9. Kitts B, Leblanc B (2004) Optimal bidding on key-
word auctions. Electron Mark Spec Issue Innov Auc-
tion Mark 14(3):186–201
10. Varian HR (2007) Position auctions. Int J Ind Organ
25(6):1163–1178
Algorithm DC-TREE for k-Servers
on Trees
Marek Chrobak
Computer Science, University of California,
Riverside, CA, USA
Keywords
Competitive analysis; K-server problem; On-line
algorithms; Trees
Years and Authors of Summarized
Original Work
1991; Chrobak, Larmore
Problem Deﬁnition
In the k-Server Problem, one wishes to schedule
the movement of k-servers in a metric space M,
in response to a sequence % D r1; r2; : : : ; rn of
requests, where ri 2 M for each i. Initially, all
the servers are located at some initial conﬁgu-
ration X0  M of k points. After each request
ri is issued, one of the k-servers must move

28
Algorithm DC-TREE for k-Servers on Trees
to ri. A schedule speciﬁes which server moves
to each request. The cost of a schedule is the
total distance traveled by the servers, and our
objective is to ﬁnd a schedule with minimum
cost.
In the online version of the k-Server Problem,
the decision as to which server to move to each
request ri must be made before the next request
riC1 is issued. In other words, the choice of this
server is a function of requests r1; r2; : : : ; ri. It
is quite easy to see that in this online scenario, it
is not possible to compute an optimal schedule
for each request sequence, raising the question
of how to measure the accuracy of such online
algorithms. A standard approach to doing this
is based on competitive analysis. If A is an
online k-server algorithm denote by costA.%/
the cost of the schedule produced by A on a
request sequence %, and by opt.%/ the cost of
the optimal schedule. A is called R-competitive
if costA.%/  R  opt.%/ C B, where B is
a constant that may depend on M and X0. The
smallest such R is called the competitive ratio
of A.
The k-Server Problem was introduced by
Manasse, McGeoch, and Sleator [7, 8], who
proved that there is no online R-competitive
algorithm for R
<
k, for any metric space
with at least k C 1 points. They also gave a 2-
competitive algorithm for k D 2 and formulated
what is now known as the k-Server Conjecture,
which postulates that there exists a k-competitive
online algorithm for all k. Koutsoupias and
Papadimitriou [5, 6] proved that the so-called
Work-Function Algorithm has competitive ratio
at most 2k  1, which to date remains the best
upper bound known.
Efforts to prove the k-Server Conjecture led to
discoveries of k-competitive algorithms for some
restricted classes of metric spaces, including Al-
gorithm DC-TREE for trees [3] presented in this
entry. (See [1, 2, 4] for other examples.) A tree
is a metric space deﬁned by a connected acyclic
graph whose edges are treated as line segments
of arbitrary positive lengths. This metric space
includes both the tree’s vertices and the points on
the edges, and the distances are measured along
the (unique) shortest paths.
Key Results
Let T be a tree, as deﬁned above. Given the
current server conﬁguration S D fs1; : : : ; skg,
where sj denotes the location of server j , and a
request point r, the algorithm will move several
servers, with one of them ending up on r. For
two points x; y 2 T, let Œx; y be the unique path
from x to y in T. A server j is called active if
there is no other server in Œsj ; r 
˚
sj

, and j is
the minimum-index server located on sj (the last
condition is needed only to break ties).
Algorithm DC-TREE. On a request r, move all
active servers, continuously and with the same
speed, towards r, until one of them reaches the
request. Note that during this process some active
servers may become inactive, in which case they
halt. Clearly, the server that will arrive at r is the
one that was closest to r at the time when r was
issued.
More formally, denoting by sj the variable
representing the current position of server j , the
algorithm serves r as follows:
while sj ¤ r for all j do
let ı D 1
2 mini<j
˚
d.si; sj / C d.si; r/
d.sj ; r/

move each active server sj by distance
ı towards r
The example below shows how DC-TREE
serves a request r (Fig. 1).
The competitive analysis of Algorithm DC-
TREE is based on a potential argument. The cost
of Algorithm DC-TREE is compared to that of
an adversary who serves the requests with her
own servers. Denoting by A the conﬁguration of
the adversary servers at a given step, deﬁne the
potential by ˆ D k  D.S; A/ C P
i<j d.si; sj /,
where D.S; A/ is the cost of the minimum match-
ing between S and A. At each step, the adversary
ﬁrst moves one of her servers to r. In this sub-
step the potential increases by at most k times the
increase of the adversary’s cost. Then, Algorithm
DC-TREE serves the request. One can show that
then the sum of ˆ and DC-TREE’s cost does not
increase. These two facts, by amortization over

Algorithm DC-TREE for k-Servers on Trees
29
A
y
s4
s2
s1
r
x
y
r = s3
s1
s2
s4
x
s3
Algorithm DC-TREE for k-Servers on Trees, Fig. 1
Algorithm DC-TREE serving a request on r. The conﬁg-
uration before r is issued is on the left; the conﬁguration
after the service is completed is on the right. At ﬁrst, all
servers are active. When server 3 reaches point x, server 1
becomes inactive. When server 3 reaches point y, server
2 becomes inactive
the whole request sequence, imply the following
result [3]:
Theorem 1 ([3]) Algorithm
DC-TREE
is
k-
competitive on trees.
Applications
The k-Server Problem is an abstraction of various
scheduling problems, including emergency crew
scheduling, caching in multilevel memory sys-
tems, or scheduling head movement in 2-headed
disks. Nevertheless, due to its abstract nature, the
k-server problem is mainly of theoretical interest.
Algorithm DC-TREE can be applied to other
spaces by “embedding” them into trees. For ex-
ample, a uniform metric space (with all distances
equal 1) can be represented by a star with arms
of length 1=2, and thus Algorithm DC-TREE
can be applied to those spaces. This also im-
mediately gives a k-competitive algorithm for
the caching problem, where the objective is to
manage a two-level memory system consisting
of a large main memory and a cache that can
store up to k memory items. If an item is in the
cache, it can be accessed at cost 0, otherwise it
costs 1 to read it from the main memory. This
caching problem can be thought of as the k-server
problem in a uniform metric space where the
server positions represent the items residing in
the cache. This idea can be extended further to the
weighted caching [4], which is a generalization of
the caching problem where different items may
have different costs. In fact, if one can embed
a metric space M into a tree with distortion
bounded by ı, then Algorithm DC-TREE yields
a ık-competitive algorithm for M.
Open Problems
The k-Server Conjecture – whether there is a k-
competitive algorithm for k-servers in any metric
space – remains open. It would be of interest
to prove it for some natural special cases, for
example the plane, either with the Euclidean or
Manhattan metric. (A k-competitive algorithm
for the Manhattan plane for k D 2; 3 servers is
known [1], but not for k  4).
Very little is known about online randomized
algorithms for k-servers. In fact, even for k D 2
it is not known if there is a randomized algorithm
with competitive ratio smaller than 2.
Cross-References
▷Deterministic Searching on the Line
▷Generalized Two-Server Problem
▷Metrical Task Systems
▷Online Paging and Caching
▷Work-Function Algorithm for k-Servers
Recommended Reading
1. Bein W, Chrobak M, Larmore LL (2002) The 3-
server problem in the plane. Theor Comput Sci 287:
387–391
2. Borodin A, El-Yaniv R (1998) Online computation
and competitive analysis. Cambridge University Press,
Cambridge
3. Chrobak M, Larmore LL (1991) An optimal online
algorithm for k servers on trees. SIAM J Comput
20:144–148
4. Chrobak M, Karloff H, Payne TH, Vishwanathan S
(1991) New results on server problems. SIAM J Dis-
cret Math 4:172–181

30
Algorithmic Cooling
5. Koutsoupias E, Papadimitriou C (1994) On the
k-server conjecture. In: Proceedings of the 26th
symposium on theory of computing (STOC). ACM,
Montreal, pp 507–511
6. Koutsoupias E, Papadimitriou C (1995) On the k-
server conjecture. J ACM 42:971–983
7. Manasse M, McGeoch LA, Sleator D (1988) Competi-
tive algorithms for online problems. In: Proceedings of
the 20th symposium on theory of computing (STOC).
ACM, Chicago, pp 322–333
8. Manasse M, McGeoch LA, Sleator D (1990) Com-
petitive algorithms for server problems. J Algorithms
11:208–230
Algorithmic Cooling
Tal Mor
Department of Computer Science, Technion –
Israel Institute of Technology, Haifa, Israel
Keywords
Cooling; Data compression; Nuclear magnetic
resonance; Quantum computing; Spin cooling;
State initialization
Years and Authors of Summarized
Original Work
1999; Schulman, Vazirani
2002;
Boykin,
Mor,
Roychowdhury,
Vatan,
Vrijen
Problem Deﬁnition
The fusion of concepts taken from the ﬁelds
of quantum computation, data compression, and
thermodynamics has recently yielded novel algo-
rithms that resolve problems in nuclear magnetic
resonance and potentially in other areas as well,
algorithms that “cool down” physical systems.
•
A leading candidate technology for the con-
struction of quantum computers is nuclear
magnetic resonance (NMR). This technology
has the advantage of being well established
for other purposes, such as chemistry and
medicine. Hence, it does not require new and
exotic equipment, in contrast to ion traps and
optical lattices, to name a few. However, when
using standard NMR techniques, (not only for
quantum computing purposes) one has to live
with the fact that the state can only be ini-
tialized in a very noisy manner: The particles’
spins point in mostly random directions, with
only a tiny bias towards the desired state.
The key idea of Schulman and Vazi-
rani [27] is to combine the tools of both data
compression and quantum computation, to
suggest a scalable state initialization process,
a “molecular-scale heat engine.” Based on
Schulman and Vazirani’s method, Boykin,
Mor, Roychowdhury, Vatan, and Vrijen [4]
then developed a new process, “heat-bath
algorithmic cooling,” to signiﬁcantly improve
the state initialization process, by opening
the system to the environment. Strikingly,
this offered a way to put to good use
the
phenomenon
of
decoherence,
which
is usually considered to be the villain in
quantum computation. These two methods are
now sometimes called “closed-system” (or
“reversible”), algorithmic cooling, and “open-
system” algorithmic cooling, respectively.
•
The far-reaching consequence of this research
lies in the possibility of reaching beyond
the
potential
implementation
of
remote-
future
quantum
computing
devices.
An
efﬁcient technique to generate ensembles of
spins that are highly polarized by external
magnetic ﬁelds is considered to be a Holy
Grail
in
NMR
spectroscopy.
Spin-half
nuclei have steady-state polarization biases
that increase inversely with temperature;
therefore, spins exhibiting polarization biases
above their thermal-equilibrium biases are
considered cool. Such cooled spins present an
improved signal-to-noise ratio if used in NMR
spectroscopy or imaging.
Existing
spin-cooling
techniques
are
limited in their efﬁciency and usefulness.
Algorithmic cooling is a promising new
spin-cooling approach that employs data
compression
methods
in
open
systems.

Algorithmic Cooling
31
A
It reduces the entropy of spins to a point
far beyond Shannon’s entropy bound on
reversible
entropy
manipulations,
thus
increasing their polarization biases. As a
result, it is conceivable that the open-system
algorithmic
cooling
technique
could
be
harnessed to improve on current uses of NMR
in areas such as chemistry, material science,
and even medicine, since NMR is at the basis
of MRI – magnetic resonance imaging.
Basic Concepts
Loss-Less In-Place Data Compression
Given a bit string of length n, such that the
probability distribution is known and far enough
from the uniform distribution, one can use data
compression to generate a shorter string, say
of m bits, such that the entropy of each bit
is much closer to one. As a simple example,
consider a four-bit string which is distributed as
follows: p0001 D p0010 D p0100 D p1000 D
1=4, with pi the probability of the string i. The
probability of any other string value is exactly
zero, so the probabilities sum up to one. Then,
the bit string can be compressed, via a lossy
compression algorithm, into a 2-bit string that
holds the binary description of the location of “1”
in the above four strings. One can also envision
a similar process that generates an output which
is of the same length n as the input, but such
that the entropy is compressed via a loss-less, in-
place, data compression into the last two bits. For
instance, logical gates that operate on the bits can
perform the permutation 0001 ! 0000, 0010 !
0001, 0100 ! 0010, and 1000 ! 0011, while
the other input strings transform to output strings
in which the two most signiﬁcant bits are not
zero; for instance, 1100 ! 1010. One can easily
see that the entropy is now fully concentrated on
the two least signiﬁcant bits, which are useful in
data compression, while the two most signiﬁcant
bits have zero entropy.
In order to gain some intuition about the de-
sign of logical gates that perform entropy manip-
ulations, one can look at a closely related scenario
which was ﬁrst considered by von Neumann.
He showed a method to extract fair coin ﬂips,
given a biased coin; he suggested taking a pair of
biased coin ﬂips, with results a and b, and using
the value of a conditioned on a ¤ b. A simple
calculation shows that a D 0 and a D 1 are now
obtained with equal probabilities, and therefore,
the entropy of coin a is increased in this case to 1.
The opposite case, the probability distribution of
a given that a D b, results in a highly determined
coin ﬂip, namely, a (conditioned) coin ﬂip with
a higher bias or lower entropy. A gate that ﬂips
the value of b if (and only if) a D 1 is called a
controlled NOT gate. If after applying such a gate
b D 1 is obtained, this means that a ¤ b prior to
the gate operation; thus, now the entropy of a is
1. If, on the other hand, after applying such a gate
b D 0 is obtained, this means that a D b prior to
the gate operation; thus, the entropy of a is now
lower than its initial value.
Spin Temperature, Polarization Bias, and
Effective Cooling
In physics, two-level systems, namely, systems
that possess only binary values, are useful in
many ways. Often it is important to initialize such
systems to a pure state “0” or to a probability
distribution which is as close as possible to a pure
state “0.” In these physical two-level systems,
a data compression process that brings some of
them closer to a pure state can be considered as
“cooling.” For quantum two-level systems, there
is a simple connection between temperature, en-
tropy, and population probability. The population
difference between these two levels is known as
the polarization bias, . Consider a single spin-
half particle – for instance, a hydrogen nucleus
– in a constant magnetic ﬁeld. At equilibrium
with a thermal heat-bath, the probability of this
spin to be up or down (i.e., parallel or antipar-
allel to the ﬁeld direction) is given by p" D
1C
2
and p#
D
1
2 . The entropy H of the
spin is H.single  bit/ D H.1=2 C =2/ with
H.P / 	 P log2 P  .1  P / log2.1  P /
measured in bits. The two pure states of a spin-
half nucleus are commonly written as j"i 	“0”
and j#i 	“1”; the ji notation will be clariﬁed
elsewhere. (Quantum Computing entries in this
encyclopedia.) The polarization bias of the spin at

32
Algorithmic Cooling
thermal equilibrium is given by  D p" p#. For
such a physical System, the bias is obtained via
a quantum statistical mechanics argument,  D
tanh

„B
2KBT

, where „ is Planck’s constant, B
is the magnetic ﬁeld,  is the particle-dependent
gyromagnetic constant,(This constant, , is thus
responsible for the difference in equilibrium po-
larization bias [e.g., a hydrogen nucleus is 4
times more polarized than a carbon isotope 13C
nucleus, but about 103 less polarized than an
electron spin].) KB is Boltzman’s coefﬁcient, and
T is the thermal heat-bath temperature. For high
temperatures or small biases  
„B
2KBT ; thus, the
bias is inversely proportional to the temperature.
Typical values of  for spin-half nuclei at room
temperature (and magnetic ﬁeld of 10 T) are
105–106, and therefore, most of the analysis
here is done under the assumption that  
1. The spin temperature at equilibrium is thus
T D
Const
 , and its (Shannon) entropy is H D
1  .2= ln 4/.
A spin temperature out of thermal equilibrium
is still deﬁned via the same formulas. Therefore,
when a system is moved away from thermal
equilibrium, achieving a greater polarization bias
is equivalent to cooling the spins, without cool-
ing the system, and to decreasing their entropy.
The process of increasing the bias (reducing the
entropy) without increasing the temperature of
the thermal bath is known as “effective cooling.”
After a typical period of time, termed the ther-
malization time or relaxation time, the bias will
gradually revert to its thermal equilibrium value;
yet during this process, typically in the order
of seconds, the effectively cooled spin may be
used for various purposes as described in section
“Applications.”
Consider a molecule that contains n adjacent
spin-half nuclei arranged in a line; these form
the bits of the string. These spins are initially
at thermal equilibrium due to their interaction
with the environment. At room temperature, the
bits at thermal equilibrium are not correlated to
their neighbors on the same string: more pre-
cisely, the correlation is very small and can be
ignored. Furthermore, in a liquid state one can
also neglect the interaction between strings (be-
tween molecules). It is convenient to write the
probability distribution of a single spin at thermal
equilibrium using the “density-matrix” notation
 D
p" 0
0 p#

D
.1 C /=2
0
0
.1  /=2

;
(1)
since these two-level systems are of a quantum
nature (namely, these are quantum bits – qubits)
and, in general, can also have states other than
just a classical probability distribution over “0”
and “1.” The classical case will now be consid-
ered, where  contains only diagonal elements,
and these describe a conventional probability
distribution. At thermal equilibrium, the state of
n D 2 uncorrelated qubits that have the same po-
larization bias is described by the density matrix
fnD2g
init
D ˝, where ˝ means tensor product.
The probability of the state “00,” for instance, is
then .1 C /=2  .1 C /=2 D .1 C /2=4, etc.
Similarly, the initial state of an n-qubit system of
this type, at thermal equilibrium, is
fng
init D  ˝  ˝    ˝ :
(2)
This state represents a thermal probability distri-
bution, such that the probability of the classical
state “000. . . 0” is P000:::0 D .1 C 0/n=2n, etc.
In reality, the initial bias is not the same on each
qubit,(Furthermore, individual addressing of each
spin during the algorithm requires a slightly dif-
ferent bias for each.) but as long as the differences
between these biases are small (e.g., all qubits
are of the same nucleus), these differences can be
ignored in a discussion of an idealized scenario.
Key Results
Molecular-Scale Heat Engines
Schulman and Vazirani (SV) [27] identiﬁed the
importance of in-place loss-less data compression
and of the low-entropy bits created in that pro-
cess: physical two-level systems (e.g., spin-half
nuclei) may be similarly cooled by data com-
pression algorithms. SV analyzed the cooling
of such a system using various tools of data
compression. A loss-less compression of an n-bit
binary string distributed according to the thermal

Algorithmic Cooling
33
A
equilibrium distribution, Eq. 2, is readily ana-
lyzed using information-theoretical tools: In an
ideal compression scheme (not necessarily real-
izable), with sufﬁciently large n, all randomness
– and hence all the entropy – of the bit string
is transferred to n  m bits; the remaining m
bits are thus left, with extremely high probability,
at a known deterministic state, say the string
“000. . . 0.” The entropy H of the entire system
is H.system/ D nH.single  bit/ D nH.1=2 C
=2/. Any compression scheme cannot decrease
this entropy; hence, Shannon’s source coding
entropy bound yields m  nŒ1  H.1=2 C =2/.
A simple leading-order calculation shows that m
is bounded by (approximately)
2
2 ln 2n for small
values of the initial bias . Therefore, with typical
  105, molecules containing an order of
magnitude of 1010 spins are required to cool a
single spin close to zero temperature.
Conventional methods for NMR quantum
computing
are
based
on
unscalable
state
initialization schemes [7, 14] (e.g., the “pseudo-
pure-state” approach) in which the signal-to-
noise ratio falls exponentially with n, the number
of
spins.
Consequently,
these
methods
are
deemed inappropriate for future NMR quantum
computers. SV [27] were ﬁrst to employ tools
of information theory to address the scaling
problem; they presented a compression scheme
in which the number of cooled spins scales
well (namely, a constant times n). SV also
demonstrated a scheme approaching Shannon’s
entropy bound, for very large n. They provided
detailed analyses of three cooling algorithms,
each useful for a different regime of  values.
Some ideas of SV were already explored a
few years earlier by Sørensen [29], a physical
chemist who analyzed effective cooling of spins.
He considered the entropy of several spin systems
and the limits imposed on cooling these systems
by polarization transfer and more general polar-
ization manipulations. Furthermore, he consid-
ered spin-cooling processes in which only unitary
operations were used, wherein unitary matrices
are applied to the density matrices; such oper-
ations are realizable, at least from a conceptual
point of view. Sørensen derived a stricter bound
on unitary cooling, which today bears his name.
Yet, unlike SV, he did not infer the connection
to data compression or advocate compression
algorithms.
SV named their concept “molecular-scale heat
engine.” When combined with conventional po-
larization transfer (which is partially similar to
a SWAP gate between two qubits), the term
“reversible polarization compression (RPC)” is
more descriptive.
Heat-Bath Algorithmic Cooling
The next signiﬁcant development came when
Boykin, Mor, Roychowdhury, Vatan, and Vrijen
(hereinafter referred to as BMRVV), invented a
new spin-cooling technique, which they named
Algorithmic cooling [4] or more speciﬁcally heat-
bath algorithmic cooling in which the use of
controlled interactions with a heat bath enhances
the cooling techniques much further. Algorith-
mic cooling (AC) expands the effective cooling
techniques by exploiting entropy manipulations
in open systems. It combines RPC steps (When
the entire process is RPC, namely, any of the
processes that follow SV ideas, one can refer to
it as reversible AC or closed-system AC, rather
than as RPC.) with fast relaxation (namely, ther-
malization) of the hotter spins, as a way of pump-
ing entropy outside the system and cooling the
system much beyond Shannon’s entropy bound.
In order to pump entropy out of the system, AC
employs regular spins (here called computation
spins) together with rapidly relaxing spins. The
latter are auxiliary spins that return to their ther-
mal equilibrium state very rapidly. These spins
have been termed “reset spins,” or, equivalently,
reset bits. The controlled interactions with the
heat bath are generated by polarization transfer
or by standard algorithmic techniques (of data
compression) that transfer the entropy onto the
reset spins which then lose this excess entropy
into the environment.
The ratio Rrelaxtimes, between the relaxation
time of the computation spins and the relaxation
time of the reset spins, must satisfy Rrelaxtimes 
1. This condition is vital if one wishes to perform
many cooling steps on the system to obtain sig-
niﬁcant cooling.

34
Algorithmic Cooling
In a pure information-theoretical point of
view, it is legitimate to assume that the only
restriction on ideal RPC steps is Shannon’s
entropy bound; then the equivalent of Shannon’s
entropy bound, when an ideal open-system AC is
used, is that all computation spins can be cooled
down to zero temperature, that is to  D 1.
Proof: repeat the following till the entropy of
all computation spins is exactly zero: (i) push
entropy from computation spins into reset spins
and (ii) let the reset spins cool back to room
temperature. Clearly, each application of step
(i), except the last one, pushes the same amount
of entropy onto the reset spins, and then this
entropy is removed from the system in step (ii).
Of course, a realistic scenario must take other
parameters into account such as ﬁnite relaxation-
time ratios, realistic environment, and physical
operations on the spins. Once this is done, cooling
to zero temperature is no longer attainable. While
ﬁnite relaxation times and a realistic environment
are system dependent, the constraint of using
physical operations is conceptual.
BMRVV therefore pursued an algorithm that
follows some physical rules; it is performed by
unitary operations and reset steps and still bypass
Shannon’s entropy bound, by far. The BMRVV
cooling algorithm obtains signiﬁcant cooling be-
yond that entropy bound by making use of very
long molecules bearing hundreds or even thou-
sands of spins, because its analysis relies on the
law of large numbers.
Practicable Algorithmic Cooling
The concept of algorithmic cooling then led
to
practicable
algorithms
[13]
for
cooling
small molecules. In order to see the impact of
practicable algorithmic cooling, it is best to use a
different variant of the entropy bound. Consider
a system containing n spin-half particles with
total entropy higher than n  1, so that there
is no way to cool even one spin to zero
temperature. In this case, the entropy bound
is a result of the compression of the entropy
into n  1 fully random spins, so that the
remaining entropy on the last spin is minimal.
The entropy of the remaining single spin satisﬁes
H.single/  1  n2= ln 4; thus, at most, its
polarization can be improved to
ﬁnal  pn :
(3)
The
practicable
algorithmic
cooling
(PAC),
suggested
by
Fernandez,
Lloyd,
Mor,
and
Roychowdhury in [13], indicated potential for
a near-future application to NMR spectroscopy.
In particular, it presented an algorithm named
PAC2 which uses any (odd) number of spins n,
such that one of them is a reset spin, and .n  1/
are computation spins. PAC2 cools the spins such
that the coldest one can (approximately) reach
a bias ampliﬁcation by a factor of .3=2/.n1/=2.
The approximation is valid as long as the ﬁnal
bias .3=2/.n1/=2  is much smaller than 1.
Otherwise, a more precise treatment must be
done. This proves an exponential advantage
of AC over the best possible reversible AC,
as these reversible cooling techniques, e.g.,
of [27, 29], are limited to improve the bias by
no more than a factor of pn. PAC can be applied
for small n (e.g., in the range of 10–20), and
therefore, it is potentially suitable for near-
future applications [9, 13, 19] in chemical and
biomedical usages of NMR spectroscopy.
It is important to note that in typical scenarios,
the initial polarization bias of a reset spin is
higher than that of a computation spin. In this
case, the bias ampliﬁcation factor of .3=2/.n1/=2
is relative to the larger bias, that of the reset
spin.
Exhaustive Algorithmic Cooling
Next, AC was analyzed, wherein the cooling
steps (reset and RPC) are repeated an arbitrary
number of times. This is actually an idealization
where an unbounded number of reset and logic
steps can be applied without error or decoher-
ence, while the computation qubits do not lose
their polarization biases. Fernandez [12] consid-
ered two computation spins and a single reset
spin (the least signiﬁcant bit, namely, the qubit
at the right in the tensor-product density-matrix
notation) and analyzed optimal cooling of this
system. By repeating the reset and compression

Algorithmic Cooling
35
A
exhaustively, he realized that the bound on the
ﬁnal biases of the three spins is approximately
f2; 1; 1g in units of , the polarization bias of the
reset spin.
Mor and Weinstein generalized this analysis
further and found that n  1 computation spins
and a single reset spin can be cooled (approx-
imately) to biases according to the Fibonacci
series: f: : : 34; 21; 13; 8; 5; 3; 2; 1; 1g. The com-
putation spin that is further away from the reset
spin can be cooled up to the relevant Fibonacci
number Fn. That approximation is valid as long
as the largest term times  is still much smaller
than 1. Schulman then suggested the “partner
pairing algorithm” (PPA) and proved the optimal-
ity of the PPA among all classical and quantum
algorithms. These two algorithms, the Fibonacci
AC and the PPA, led to two joint papers [25,26],
where upper and lower bounds on AC were also
obtained. The PPA is deﬁned as follows: repeat
these two steps until cooling sufﬁciently close to
the limit: (a) RESET, applied to a reset spin in a
system containing n  1 computation spins and
a single (the LSB) reset spin, and (b) SORT, a
permutation that sorts the 2n diagonal elements of
the density matrix by decreasing order, so that the
MSB spin becomes the coldest. Two important
theorems proven in [26] are:
(1) Lower bound: When 2n  1 (namely, for
long enough molecules), Theorem 3 in [26]
promises that nlog.1=/ cold qubits can be
extracted. This case is relevant for scalable
NMR quantum computing.
(2) Upper bound: Section 4.2 in [26] proves
the following theorem: No algorithmic cool-
ing method can increase the probability of
any basis state to above minf2ne2n; 1g,
wherein the initial conﬁguration is the com-
pletely mixed state (the same is true if the
initial state is a thermal state).
More
recently,
Elias,
Fernandez,
Mor,
and
Weinstein [9] analyzed more closely the case
of n
<
15 (at room temperature), where
the coldest spin (at all stages) still has a
polarization bias much smaller than 1. This
case is most relevant for near-future applications
in NMR spectroscopy. They generalized the
Fibonacci-AC to algorithms yielding higher-
term Fibonacci series, such as the tribonacci
(also
known
as
3-term
Fibonacci
series),
f: : : 81; 44; 24; 13; 7; 4; 2; 1; 1g, etc. The ultimate
limit
of
these
multi-term
Fibonacci
series
is obtained when each term in the series is
the sum of all previous terms. The resulting
series
is
precisely
the
exponential
series
f: : : 128; 64; 32; 16; 8; 4; 2; 1; 1g, so the coldest
spin is cooled by a factor of 2n2. Furthermore,
a leading-order analysis of the upper bound
mentioned above (Section 4.2 in Ref. [26]) shows
that no spin can be cooled beyond a factor of
2n1; see Corollary 1 in [9].
Other Results
For several other theoretical results dealing with
relevant algorithms and with the connection to
thermodynamics, see [11, 15, 17, 21]. For several
popular “News and Views” discussions of AC in
Nature, see [18,22,24].
Applications
The two major far-future and near-future appli-
cations are already described in section “Prob-
lem Deﬁnition.” It is important to add here that
although the speciﬁc algorithms analyzed so far
for AC are usually classical, their practical im-
plementation via an NMR spectrometer must be
done through analysis of universal quantum com-
putation, using the speciﬁc gates allowed in such
systems. Therefore, AC could yield the ﬁrst near-
future application of quantum computing devices.
AC may also be useful for cooling various
other physical systems; for several examples (the-
oretical and experimental), see [2,16,28,30,31],
since state initialization is a common problem in
physics in general and in quantum computation
in particular.
Open Problems
A main open problem in practical AC is
technological;
can
the
ratio
of
relaxation

36
Algorithmic Cooling
times be increased so that many cooling steps
may be applied onto relevant NMR systems?
Other methods, for instance, a spin-diffusion
mechanism [3,23], may also be useful for various
applications.
Another interesting open problem is whether
the ideas developed during the design of AC can
also lead to applications in classical information
theory.
Last but not least, in the context of building
scalable quantum computers, it is interesting to
study if AC can become a practical tool for ad-
vancing the non-conventional model of quantum
computing called the one pure qubit (or one clean
qubit) model as suggested in [1, 8] and to study
if AC can be useful for designing fault-tolerant
quantum computers as suggested in [20].
Experimental Results
Various ideas of AC had already led to several
experiments using 3–4 qubit quantum computing
devices in NMR (AC used in other systems was
mentioned earlier in section “Applications”):
(1) An experiment [6] that implemented a single
RPC step.
(2) Two experiments [5, 10] in which entropy-
conservation bounds (which apply in any
closed system) were bypassed. The second
one [10] was done on bio-molecules – amino
acids.
(3) A full AC experiment [3] that includes the
initialization of three carbon nuclei to the bias
of a hydrogen spin, followed by a single com-
pression step on these three carbons. This
work was later on extended also to multi-
cycle AC [23].
Cross-References
Quantum computing entries such as ▷Quantum
Algorithm for Factoring, ▷Quantum Algorithm
for the Parity Problem and ▷Quantum Key
Distribution. Data compression entries such as
▷Dictionary-Based Data Compression.
Recommended Reading
1. Ambainis A, Schulman LJ, Vazirani U (2006)
Computing with highly mixed states. J ACM 53:
507–531
2. Bakr WS, Preiss PM, Tai ME, Ma R, Simon J, Greiner
M (2011) Orbital excitation blockade and algorithmic
cooling in quantum gases. Nature 480:500–503
3. Baugh J, Moussa O, Ryan CA, Nayak A, Laﬂamme R
(2005) Experimental implementation of heat-bath al-
gorithmic cooling using solid-state nuclear magnetic
resonance. Nature 438:470–473
4. Boykin PO, Mor T, Roychowdhury V, Vatan F,
Vrijen R (2002) Algorithmic cooling and scalable
NMR quantum computers. Proc Natl Acad Sci USA
99:3388–3393
5. Brassard G, Elias Y, Fernandez JM, Gilboa H, Jones
JA, Mor T, Weinstein Y, Xiao L (2005) Experimental
heat-bath cooling of spins. Submitted to Euro Phys
Lett PLUS. See also arXiv:0511156 [quant-ph]. See
a much improved version in arXiv:1404.6885 [quant-
ph], 2014
6. Chang DE, Vandersypen LMK, Steffen M (2001)
NMR implementation of a building block for scal-
able quantum computation. Chem Phys Lett 338:
337–344
7. Cory DG, Fahmy AF, Havel TF (1997) Ensemble
quantum computing by NMR spectroscopy. Proc Natl
Acad Sci USA 94:1634–1639
8. Datta A, Flammia ST, Caves CM (2005) Entan-
glement and the power of one qubit. Phys Rev A
72:042316
9. Elias Y, Fernandez JM, Mor T, Weinstein Y (2007)
Optimal algorithmic cooling of spins. Isr J Chem
46:371–391
10. Elias Y, Gilboa H, Mor T, Weinstein Y (2011) Heat-
bath cooling of spins in two amino acids. Chem Phys
Lett 517:126–131
11. Elias Y, Mor T, Weinstein Y (2011) Semiopti-
mal practicable algorithmic cooling. Phys Rev A
83:042340
12. Fernandez
JM
(2004)
De
computatione
quan-
tica. Ph.D. Dissertation, University of Montreal,
Montreal
13. Fernandez JM, Lloyd S, Mor T, Roychowdhury V
(2004) Practicable algorithmic cooling of spins. Int
J Quantum Inf 2:461–477
14. Gershenfeld NA, Chuang IL (1997) Bulk spin-
resonance quantum computation. Science 275:350–
356
15. Kaye P (2007) Cooling algorithms based on the 3-bit
majority. Quantum Inf Proc 6:295–322
16. Ladd TD, Goldman JR, Yamaguchi F, Yamamoto Y,
Abe E, Itoh KM (2002) All-Silicon quantum com-
puter. Phys Rev Lett 89:017901
17. Linden N, Popescu P, Skrzypczyk P (2010) How
small can thermal machines be? The smallest possible
refrigerator. Phys Rev Lett 105:130401

Algorithmic Mechanism Design
37
A
18. Lloyd S (2014) Quantum optics: cool computation,
hot bits. Nat Photon 8:90–91
19. Mor T, Roychowdhury V, Lloyd S, Fernandez JM,
Weinstein Y (2005) Algorithmic cooling. US patent
No. 6,873,154
20. Paz-Silva GA, Brennen GK, Twamley J (2010) Fault
tolerance with noisy and slow measurements and
preparation. Phys Rev Lett 105:100501
21. Rempp F, Michel M, Mahler G (2007) Cyclic cooling
algorithm. Phys Rev A 76:032325
22. Renner R (2012) Thermodynamics: the fridge gate.
Nature 482:164–165
23. Ryan CA, Moussa O, Baugh J, Laﬂamme R (2008)
Spin based heat engine: demonstration of multi-
ple rounds of algorithmic cooling. Phys Rev Lett
100:140501
24. Schulman LJ (2005) Quantum computing: a bit chilly.
Nature 438:431–432
25. Schulman LJ, Mor T, Weinstein Y (2005) Physical
limits of heat-bath algorithmic cooling. Phys Rev Lett
94:120501
26. Schulman LJ, Mor T, Weinstein Y (2007) Physical
limits of heat-bath algorithmic cooling. SIAM J Com-
put (SICOMP) 36:1729–1747
27. Schulman LJ, Vazirani U (1999) Molecular scale
heat engines and scalable quantum computation. In:
Proceedings of the 31st ACM STOC (Symposium on
Theory of Computing), Atlanta, pp 322–329
28. Simmons S, Brown RM, Riemann H, Abrosimov
NV, Becker P, Pohl HJ, Thewalt MLW, Itoh KM,
Morton JJL (2011) Entanglement in a solid-state spin
ensemble. Nature 470:69–72
29. Sørensen OW (1989) Polarization transfer experi-
ments in high-resolution NMR spectroscopy. Prog
Nucl Magn Reson Spectrosc 21:503–569
30. Twamley J (2003) Quantum-cellular-automata quan-
tum computing with endohedral fullerenes. Phys Rev
A 67:052318
31. Xu JS, Yung MH, Xu XY, Boixo S, Zhou ZW, Li
CF, Aspuru-Guzik A, Guo GC (2014) Demon-like
algorithmic quantum cooling and its realization with
quantum optics. Nat Photonics 8:113–118
Algorithmic Mechanism Design
Ron Lavi
Faculty of Industrial Engineering and
Management, Technion, Haifa, Israel
Years and Authors of Summarized
Original Work
1999; Nisan, Ronen
Problem Deﬁnition
Mechanism design is a sub-ﬁeld of economics
and game theory that studies the construction
of social mechanisms in the presence of selﬁsh
agents. The nature of the agents dictates a basic
contrast between the social planner, that aims
to reach a socially desirable outcome, and the
agents, that care only about their own private
utility. The underlying question is how to incen-
tivize the agents to cooperate, in order to reach
the desirable social outcomes.
In the Internet era, where computers act and
interact on behalf of selﬁsh entities, the connec-
tion of the above to algorithmic design suggests
itself: suppose that the input to an algorithm is
kept by selﬁsh agents, who aim to maximize their
own utility. How can one design the algorithm so
that the agents will ﬁnd it in their best interest
to cooperate, and a close-to-optimal outcome
will be outputted? This is different than clas-
sic distributed computing models, where agents
are either “good” (meaning obedient) or “bad”
(meaning faulty, or malicious, depending on the
context). Here, no such partition is possible. It is
simply assumed that all agents are utility maxi-
mizers. To illustrate this, let us describe a moti-
vating example:
A Motivating Example: Shortest Paths
Given a weighted graph, the goal is to ﬁnd
a shortest path (with respect to the edge weights)
between a given source and target nodes. Each
edge is controlled by a selﬁsh entity, and the
weight of the edge, we is private information
of that edge. If an edge is chosen by the
algorithm to be included in the shortest path,
it will incur a cost which is minus its weight
(the cost of communication). Payments to the
edges are allowed, and the total utility of an edge
that participates in the shortest path and gets
a payment pe is assumed to be ue D pe  we.
Notice that the shortest path is with respect to the
true weights of the agents, although these are not
known to the designer.
Assuming that each edge will act in order
to maximize its utility, how can one choose the

38
Algorithmic Mechanism Design
path and the payments? One option is to ignore
the strategic issue all together, ask the edges
to simply report their weights, and compute the
shortest path. In this case, however, an edge
dislikes being selected, and will therefore prefer
to report a very high weight (much higher than
its true weight) in order to decrease the chances
of being selected. Another option is to pay each
selected edge its reported weight, or its reported
weight plus a small ﬁxed “bonus”. However in
such a case all edges will report lower weights,
as being selected will imply a positive gain.
Although this example is written in an
algorithmic language, it is actually a mechanism
design problem, and the solution, which is now
a classic, was suggested in the 1970’s. The
chapter continues as follows: First, the abstract
formulation for such problems is given, the
classic solution from economics is described, and
its advantages and disadvantages for algorithmic
purposes are discussed. The next section then
describes
the
new
results
that
algorithmic
mechanism design offers.
Abstract Formulation
The framework consists of a set A of alternatives,
or outcomes, and n players, or agents. Each
player i has a valuation function viW A ! <
that assigns a value to each possible alternative.
This valuation function belongs to a domain
Vi
of all possible valuation functions. Let
V D V1      Vn,
and
Vi D Q
j ¤i Vj .
Observe that this generalizes the shortest path
example of above: A is all the possible s  t
paths in the given graph, ve(a) for some path
a 2 A is either we (if e 2 a) or zero.
A social choice function f W V ! A assigns
a socially desirable alternative to any given
proﬁle of players’ valuations. This parallels the
notion of an algorithm. A mechanism is a tuple
M D .f; p1; : : : ; pn/, where f is a social choice
function, and piW V ! < (for i D 1; : : : ; n) is the
price charged from player i. The interpretation is
that the social planner asks the players to reveal
their true valuations, chooses the alternative
according to f as if the players have indeed
acted truthfully, and in addition rewards/punishes
the players with the prices. These prices should
induce “truthfulness” in the following strong
sense: no matter what the other players declare,
it is always in the best interest of player i to
reveal her true valuation, as this will maximize
her utility. Formally, this translates to:
Deﬁnition 1 (Truthfulness) M is “truthful” (in
dominant strategies) if, for any player i, any pro-
ﬁle of valuations of the other players vi 2 Vi,
and any two valuations of player ivi; v0
i 2 Vi,
vi.a/  pi.vi; vi/  vi.b/  pi.v0
i; vi/
where f .vi; vi/ D a and f .v0
i; vi/ D b.
Truthfulness is quite strong: a player need not
know anything about the other players, even not
that they are rational, and still determine the best
strategy for her. Quite remarkably, there exists
a truthful mechanism, even under the current
level of abstraction. This mechanism suits all
problem domains, where the social goal is to
maximize the “social welfare”:
Deﬁnition
2
(Social
welfare
maximiza-
tion) A
social
choice
function
f W V ! A
maximizes
the
social
welfare
if
f .v/
2
argmaxa2A
P
i vi.a/, for any v 2 V:
Notice that the social goal in the shortest path
domain is indeed welfare maximization, and, in
general, this is a natural and important economic
goal. Quite remarkably, there exists a general
technique to construct truthful mechanisms that
implement this goal:
Theorem 1 (Vickrey–Clarke–Groves (VCG))
Fix any alternatives set A and any domain V,
and suppose that f W V ! A maximizes the social
welfare. Then there exist prices p such that the
mechanism (f, p) is truthful.
This gives “for free” a solution to the shortest
path problem, and to many other algorithmic
problems. The great advantage of the VCG
scheme is its generality: it suits all problem
domains. The disadvantage, however, is that
the
method
is
tailored
to
social
welfare
maximization. This turns out to be restrictive,

Algorithmic Mechanism Design
39
A
especially for algorithmic and computational
settings, due to several reasons: (i) different
algorithmic goals: the algorithmic literature
considers a variety of goals, including many
that cannot be translated to welfare maximiza-
tion. VCG does not help us in such cases.
(ii) computational complexity: even if the goal is
welfare maximization, in many settings achieving
exactly the optimum is computationally hard.
The CS discipline usually overcomes this by
using approximation algorithms, but VCG will
not work with such algorithm – reaching exact
optimality is a necessary requirement of VCG.
(iii) different algorithmic models: common CS
models change “the basic setup”, hence cause
unexpected difﬁculties when one tries to use
VCG (for example, an online model, where the
input is revealed over time; this is common
in CS, but changes the implicit setting that
VCG requires). This is true even if welfare
maximization is still the goal.
Answering any one of these difﬁculties re-
quires the design of a non-VCG mechanism.
What analysis tools should be used for this pur-
pose? In economics and classic mechanism de-
sign, average-case analysis, that relies on the
knowledge of the underlying distribution, is the
standard. Computer science, on the other hand,
usually prefers to avoid strong distributional as-
sumptions, and to use worst-case analysis. This
difference is another cause to the uniqueness
of the answers provided by algorithmic mecha-
nism design. Some of the new results that have
emerged as a consequence of this integration
between Computer Science and Economics is
next described. Many other research topics that
use the tools of algorithmic mechanism design
are described in the entries on Adword Pric-
ing, Competitive Auctions, False Name Proof
Auctions, Generalized Vickrey Auction, Incen-
tive Compatible Ranking, Mechanism for One
Parameter Agents Single Buyer/Seller, Multiple
Item Auctions, Position Auctions, and Truthful
Multicast.
There are two different but closely related
research topics that should be mentioned in the
context of this entry. The ﬁrst is the line of works
that studies the “price of anarchy” of a given
system. These works analyze existing systems,
trying to quantify the loss of social efﬁciency
due to the selﬁsh nature of the participants, while
the approach of algorithmic mechanism design
is to understand how new systems should be
designed. For more details on this topic the reader
is referred to the entry on Price of Anarchy.
The second topic regards the algorithmic study
of various equilibria computation. These works
bring computational aspects into economics and
game theory, as they ask what equilibria notions
are reasonable to assume, if one requires com-
putational efﬁciency, while the works described
here bring game theory and economics into com-
puter science and algorithmic theory, as they ask
what algorithms are reasonable to design, if one
requires the resilience to selﬁsh behavior. For
more details on this topic the reader is referred
(for example) to the entry on Algorithms for
Nash Equilibrium and to the entry on General
Equilibrium.
Key Results
Problem Domain 1: Job Scheduling
Job scheduling is a classic algorithmic setting: n
jobs are to be assigned to m machines, where job
j requires processing time pij on machine i. In the
game-theoretic setting, it is assumed that each
machine i is a selﬁsh entity, that incurs a cost pij
from processing job j. Note that the payments
in this setting (and in general) may be negative,
offsetting such costs. A popular algorithmic goal
is to assign jobs to machines in order to minimize
the “makespan”: maxi
P
j is assigned to i pij . This
is different than welfare maximization, which
translates in this setting to the minimization of
P
i
P
j is assigned to i pij , further illustrating the
problem of different algorithmic goals. Thus the
VCG scheme cannot be used, and new methods
must be developed.
Results for this problem domain depend on the
speciﬁc assumptions about the structure of the
processing time vectors. In the related machines
case, pij D pj =si for any i j, where the pj’s are
public knowledge, and the only secret parameter
of player i is its speed, si.

40
Algorithmic Mechanism Design
Theorem
2
([3,
22]) For
job
scheduling
on related machines, there exists a truthful
exponential-time mechanism that obtains the
optimal makespan, and a truthful polynomial-
time mechanism that obtains a 3-approximation
to the optimal makespan.
More details on this result are given in the entry
on Mechanism for One Parameter Agents Sin-
gle Buyer. The bottom line conclusion is that,
although the social goal is different than welfare
maximization, there still exists a truthful mech-
anism for this goal. A non-trivial approximation
guarantee is achieved, even under the additional
requirement of computational efﬁciency. How-
ever, this guarantee does not match the best pos-
sible without the truthfulness requirement, since
in this case a PTAS is known.
Open Question 1 Is there a truthful PTAS for
makespan minimization in related machines?
If the number of machines is ﬁxed then [2] give
such a truthful PTAS.
The above picture completely changes in the
move to the more general case of unrelated ma-
chines, where the pij’s are allowed to be arbitrary:
Theorem 3 ([13, 30]) Any truthful scheduling
mechanism for unrelated machines cannot ap-
proximate the optimal makespan by a factor bet-
ter than 1 C
p
2 (for deterministic mechanisms)
and 2  1=m (for randomized mechanisms).
Note that this holds regardless of computational
considerations. In this case, switching from
welfare maximization to makespan minimization
results
in
a
strong
impossibility.
On
the
possibilities side, virtually nothing (!) is known.
The VCG mechanism (which minimizes the total
social cost) is an m-approximation of the optimal
makespan [32], and, in fact, nothing better is
currently known:
Open Question 2 What is the best possible ap-
proximation for truthful makespan minimization
in unrelated machines?
What caused the switch from “mostly possi-
bilities” to “mostly impossibilities”? Related
machines
is
a
single-dimensional
domain
(players hold only one secret number), for
which truthfulness is characterized by a simple
monotonicity
condition,
that
leaves
ample
ﬂexibility for algorithmic design. Unrelated
machines, on the other hand, are a multi-
dimensional
domain,
and
the
algorithmic
conditions implied by truthfulness in such a case
are harder to work with. It is still unclear
whether these conditions imply real mathematical
impossibilities, or perhaps just pose harder
obstacles that can be in principle solved. One
multi-dimensional scheduling domain for which
possibility results are known is the case where
pij 2 fLj ; Hj g, where the “low” ’s and “high”
’s are ﬁxed and known. This case generalizes
the classic multi-dimensional model of restricted
machines (pij 2 fpj ; 1g), and admits a truthful
3-approximation [27].
Problem Domain 2: Digital Goods
and Revenue Maximization
In the E-commerce era, a new kind of “digital
goods” have evolved: goods with no marginal
production cost, or, in other words, goods with
unlimited supply. One example is songs being
sold on the Internet. There is a sunk cost of
producing the song, but after that, additional
electronic copies incur no additional cost. How
should such items be sold? One possibility is
to conduct an auction. An auction is a one-
sided market, where a monopolistic entity (the
auctioneer) wishes to sell one or more items to
a set of buyers.
In this setting, each buyer has a privately
known value for obtaining one copy of the good.
Welfare maximization simply implies the allo-
cation of one good to every buyer, but a more
interesting question is the question of revenue
maximization. How should the auctioneer design
the auction in order to maximize his proﬁt? Stan-
dard tools from the study of revenue-maximizing
auctions (This model was not explicitly studied
in classic auction theory, but standard results
from there can be easily adjusted to this setting.)
suggest to simply declare a price-per-buyer, de-
termined by the probability distribution of the
buyer’s value, and make a take-it-or-leave-it offer.

Algorithmic Mechanism Design
41
A
However, such a mechanism needs to know the
underlying distribution. Algorithmic mechanism
design suggests an alternative, worst-case result,
in the spirit of CS-type models and analysis.
Suppose that the auctioneer is required to
sell all items in the same price, as is the case
for many “real-life” monopolists, and denote by
F.Ev/ the maximal revenue from a ﬁxed-price sale
to bidders with values Ev D v1; : : : vn, assuming
that all values are known. Reordering indexes so
that v1  v2      vn, let F.Ev/ D maxi i  vi.
The problem is, of-course, that in fact nothing
about the values is known. Therefore, a truthful
auction that extracts the players’ values is in
place. Can such an auction obtain a proﬁt that is
a constant fraction of F.Ev/, for any Ev (i.e., in the
worst case)? Unfortunately, the answer is prov-
ably no [17]. The proof makes use of situations
where the entire proﬁt comes from the highest
bidder. Since there is no potential for competition
among bidders, a truthful auction cannot force
this single bidder to reveal her value.
Luckily, a small relaxation in the optimality
criteria signiﬁcantly helps. Speciﬁcally, denote
by F .2/.Ev/ D maxi2 i  vi (i.e., the benchmark
is the auction that sells to at least two buyers).
Theorem 4 ([17, 20]) There exists a truthful
randomized auction that obtains an expected rev-
enue of at least F .2/=3:25, even in the worst-
case. On the other hand, no truthful auction
can approximate F .2/ within a factor better than
2.42.
Several interesting formats of distribution-free
revenue-maximizing auctions have been consid-
ered in the literature. The common building block
in all of them is the random partitioning of the
set of buyers to random subsets, analyzing each
set separately, and using the results on the other
sets. Each auction utilizes a different analysis on
the two subsets, which yields slightly different
approximation guarantees. Aggarwal et al. [1]
describe an elegant method to derandomize these
type of auctions, while losing another factor of
4 in the approximation. More details on this
problem domain can be found in the entry on
Competitive Auctions.
Problem Domain 3: Combinatorial
Auctions
Combinatorial auctions (CAs) are a central
model with theoretical importance and practical
relevance.
It
generalizes
many
theoretical
algorithmic settings, like job scheduling and
network routing, and is evident in many real-
life situations. This new model has various pure
computational aspects, and, additionally, exhibits
interesting game theoretic challenges. While each
aspect is important on its own, obviously only
the integration of the two provides an acceptable
solution.
A combinatorial auction is a multi-item auc-
tion in which players are interested in bundles
of items. Such a valuation structure can repre-
sent substitutabilities among items, complemen-
tarities among items, or a combination of both.
More formally, m items () are to be allocated
to n players. Players value subsets of items,
and vi(S) denotes i’s value of a bundle S  ˝.
Valuations additionally satisfy: (i) monotonicity,
i.e., vi.S/  vi.T / for S  T , and (ii) normal-
ization, i.e., vi.;/ D 0. The literature has mostly
considered the goal of maximizing the social
welfare: ﬁnd an allocation .S1; : : : ; Sn/ that max-
imizes P
i vi.Si/.
Since a general valuation has size exponential
in n and m, the representation issue must be taken
into account. Two models are usually considered
(see [11] for more details). In the bidding lan-
guages model, the bid of a player represents his
valuation is a concise way. For this model it is
NP-hard to approximate the social welfare within
a ratio of ˝.m1=2/, for any  > 0 (if “single-
minded” bids are allowed; the exact deﬁnition
is given below). In the query access model, the
mechanism iteratively queries the players in the
course of computation. For this model, any al-
gorithm with polynomial communication cannot
obtain an approximation ratio of ˝.m1=2/ for
any  > 0. These bounds are tight, as there exist
a deterministic pm-approximation with polyno-
mial computation and communication. Thus, for
the general valuation structure, the computational
status by itself is well-understood.
The basic incentives issue is again well-
understood: VCG obtains truthfulness. Since

42
Algorithmic Mechanism Design
VCG requires the exact optimum, which is NP-
hard to compute, the two considerations therefore
clash, when attempting to use classic techniques.
Algorithmic mechanism design aims to develop
new techniques, to integrate these two desirable
aspects.
The ﬁrst positive result for this integration
challenge was given by [29], for the special case
of “single-minded bidders”: each bidder, i, is
interested in a speciﬁc bundle Si, for a value vi
(any bundle that contains Si is worth vi, and other
bundles have zero value). Both vi; Si are private
to the player i.
Theorem 5 ([29]) There exists a truthful and
polynomial-time
deterministic
combinatorial
auction for single-minded bidders, which obtains
a
pm-approximation to the optimal social
welfare.
A possible generalization of the basic model
is to assume that each item has B copies, and
each player still desires at most one copy from
each item. This is termed “multi-unit CA”. As B
grows, the integrality constraint of the problem
reduces, and so one could hope for better solu-
tions. Indeed, the next result exploits this idea:
Theorem 6 ([7]) There exists a truthful and
polynomial-time deterministic multi-unit CA,
for B  3 copies of each item, that obtains
O.B  m1=.B2//-approximation to the optimal
social welfare.
This auction copes with the representation issue
(since general valuations are assumed) by access-
ing the valuations through a “demand oracle”:
given per-item prices fpxgx2˝, specify a bundle
S that maximizes vi.S/  P
x2S px.
Two main drawbacks of this auction motivate
further research on the issue. First, as B gets
larger it is reasonable to expect the approxi-
mation to approach 1 (indeed polynomial-time
algorithms with such an approximation guarantee
do exist). However here the approximation ratio
does not decrease below O.log m/ (this ratio is
achieved for B D O.log m/). Second, this auc-
tion does not provide a solution to the original
setting, where B D 1, and, in general for small
B’s the approximation factor is rather high. One
way to cope with these problems is to introduce
randomness:
Theorem 7 ([26]) There exists a truthful-in-
expectation and polynomial-time randomized
multi-unit CA, for any B  1 copies of each item,
that obtains O.m1=.BC1//-approximation to the
optimal social welfare.
Thus, by allowing randomness, the gap from
the standard computational status is being com-
pletely closed. The deﬁnition of truthfulness-in-
expectation is the natural extension of truthful-
ness to a randomized environment: the expected
utility of a player is maximized by being truthful.
However, this notion is strictly weaker than
the deterministic notion, as this implicitly implies
that players care only about the expectation of
their utility (and not, for example, about the
variance). This is termed “the risk-neutrality”
assumption in the economics literature. An in-
termediate notion for randomized mechanisms is
that of “universal truthfulness”: the mechanism
is truthful given any ﬁxed result of the coin toss.
Here, risk-neutrality is no longer needed. Dobzin-
ski et al. [15] give a universally truthful CA for
B D 1 that obtains an O.pm/-approximation.
Universally truthful mechanisms are still weaker
than deterministic truthful mechanisms, due to
two reasons: (i) It is not clear how to actually cre-
ate the correct and exact probability distribution
with a deterministic computer. The situation here
is different than in “regular” algorithmic settings,
where various derandomization techniques can
be employed, since these in general does not
carry through the truthfulness property. (ii) Even
if a natural randomness source exists, one cannot
improve the quality of the actual output by re-
peating the computation several times (using the
the law of large numbers). Such a repetition will
again destroy truthfulness. Thus, exactly because
the game-theoretic issues are being considered
in parallel to the computational ones, the impor-
tance of determinism increases.
Open Question 3 What is the best-possible
approximation
ratio
that
deterministic
and

Algorithmic Mechanism Design
43
A
truthful combinatorial auctions can obtain, in
polynomial-time?
There are many valuation classes, that restrict the
possible valuations to some reasonable format
(see [28] for more details). For example, sub-
additive valuations are such that, for any two
bundles S; T;  ˝, v.S [ T /  v.S/ C v.T /.
Such
classes
exhibit
much
better
approx-
imation
guarantees,
e.g.,
for
sub-additive
valuation a polynomial-time 2-approximation
is known [16]. However, no polynomial-time
truthful
mechanism
(be
it
randomized,
or
deterministic) with a constant approximation
ratio, is known for any of these classes.
Open Question 4 Does there exist polynomial-
time truthful constant-factor approximations for
special cases of CAs that are NP-hard?
Revenue maximization in CAs is of-course
another
important
goal.
This
topic
is
still
mostly unexplored, with few exceptions. The
mechanism [7] obtains the same guarantees
with respect to the optimal revenue. Improved
approximations exist for multi-unit auctions
(where all items are identical) with budget
constrained players [12], and for unlimited-
supply CAs with single-minded bidders [6].
The
topic of Combinatorial
Auctions is
discussed also in the entry on Multiple Item
Auctions.
Problem Domain 4: Online Auctions
In the classic CS setting of “online computa-
tion”, the input to an algorithm is not revealed
all at once, before the computation begins, but
gradually, over time (for a detailed discussion
see the many entries on online problems in this
book). This structure suits the auction world,
especially in the new electronic environments.
What happens when players arrive over time, and
the auctioneer must make decisions facing only
a subset of the players at any given time?
The integration of online settings, worst-
case analysis, and auction theory, was suggested
by [24]. They considered the case where players
arrive one at a time, and the auctioneer must
provide an answer to each player as it arrives,
without knowing the future bids. There are
k identical items, and each bidder may have
a distinct value for every possible quantity of the
item. These values are assumed to be marginally
decreasing, where each marginal value lies in
the interval Œv; Nv. The private information of
a bidder includes both her valuation function,
and her arrival time, and so a truthful auction
need to incentivize the players to arrive on time
(and not later on), and to reveal their true values.
The most interesting result in this setting is for
a large k, so that in fact there is a continuum of
items:
Theorem 8 ([24]) There exists a truthful on-
line auction that simultaneously approximates,
within a factor of O.log.Nv=v//, the optimal of-
ﬂine welfare, and the ofﬂine revenue of VCG. Fur-
thermore, no truthful online auction can obtain
a better approximation ratio to either one of these
criteria (separately).
This auction has the interesting property of being
a “posted price” auction. Each bidder is not re-
quired to reveal his valuation function, but, rather,
he is given a price for each possible quantity, and
then simply reports the desired quantity under
these prices.
Ideas from this construction were later used
by [10] to construct two-sided online auction
markets, where multiple sellers and buyers arrive
online.
This approximation ratio can be dramatically
improved, to be a constant, 4, if one assumes
that (i) there is only one item, and (ii) player
values are i.i.d from some ﬁxed distribution.
No
a–priori
knowledge
of
this
distribution
is needed, as neither the mechanism nor the
players are required to make any use of it.
This work, [19], analyzes this by making an
interesting connection to the class of “secretary
problems”.
A general method to convert online algorithms
to online mechanisms is given by [4]. This is
done for one item auctions, and, more generally,
for one parameter domains. This method is com-
petitive both with respect to the welfare and the
revenue.

44
Algorithmic Mechanism Design
The revenue that the online auction of The-
orem 8 manages to raise is competitive only
with respect to VCG’s revenue, which may be
far from optimal. A parallel line of works is
concerned with revenue maximizing auctions. To
achieve good results, two assumptions need to be
made: (i) there exists an unlimited supply of items
(and recall from section “Problem Domain 2:
Digital Goods and Revenue Maximization” that
F(v) is the ofﬂine optimal monopolistic ﬁxed-
price revenue), and (ii) players cannot lie about
their arrival time, only about their value. This
last assumption is very strong, but apparently
needed. Such auctions are termed here “value-
truthful”, indicating that “time-truthfulness” is
missing.
Theorem 9 ([9]) For any  > 0, there exists
a value-truthful online auction, for the unlimited
supply case, with expected revenue of at least
.F.v//=.1 C /  O.h=2/.
The
construction
exploits
principles
from
learning theory in an elegant way. Posted
price auctions for this case are also possible,
in which case the additive loss increases to
O.h log log h/. Hajiaghayi et al. [19] consider
fully-truthful
online
auctions
for
revenue
maximization,
but
manage
to
obtain
only
very high (although ﬁxed) competitive ratios.
Constructing fully-truthful online auctions with
a close-to-optimal revenue remains an open
question. Another interesting open question
involves
multi-dimensional
valuations.
The
work [24] remains the only work for players
that may demand multiple items. However
their competitive guarantees are quite high,
and achieving better approximation guarantees
(especially with respect to the revenue) is
a challenging task.
Advanced Issues
Monotonicity
What is the general way for designing a truthful
mechanism? The straight-forward way is to
check, for a given social choice function f,
whether truthful prices exist. If not, try to “ﬁx”
f. It turns out, however, that there exists a more
structured way, an algorithmic condition that
will imply the existence of truthful prices. Such
a condition shifts the designer back to the familiar
territory of algorithmic design. Luckily, such
a condition do exist, and is best described in the
abstract social choice setting of section “Problem
Deﬁnition”:
Deﬁnition 3 ([8, 23)] A social choice function
f W V ! A is “weakly monotone” (W-MON) if
for any i, vi 2 Vi, and any vi; v0
i 2 Vi, the fol-
lowing holds. Suppose that f .vi; vi/ D a, and
f .v0
i; vi/ D b. Then v0
i.b/  vi.b/  v0
i.a/ 
vi.a/.
In words, this condition states the following.
Suppose that player i changes her declaration
from vi to v0
i, and this causes the social choice
to change from a to b. Then it must be the case
that i’s value for b has increased in the transition
from vi to v0
i no-less than i’s value for a.
Theorem 10 ([35]) Fix a social choice function
f W V ! A, where V is convex, and A is ﬁnite.
Then there exist prices p such that M D .f; p/ is
truthful if and only if f is weakly monotone.
Furthermore, given a weakly monotone f, there
exists an explicit way to determine the appropri-
ate prices p (see [18] for details).
Thus, the designer should aim for weakly
monotone algorithms, and need not worry about
actual prices. But how difﬁcult is this? For single-
dimensional domains, it turns out that W-MON
leaves ample ﬂexibility for the algorithm de-
signer. Consider for example the case where ev-
ery alternative has a value of either 0 (the player
“loses”) or some vi 2 < (the player “wins” and
obtains a value vi). In such a case, it is not hard
to show that W-MON reduces to the following
monotonicity condition: if a player wins with vi,
and increases her value to v0
i > vi (while vi
remains ﬁxed), then she must win with v0
i as
well. Furthermore, in such a case, the price of
a winning player must be set to the inﬁmum over
all winning values.

Algorithmic Mechanism Design
45
A
Impossibilities of truthful design
It
is
fairly simple
to
construct
algorithms
that
satisfy W-MON for
single-dimensional
domains, and a variety of positive results were
obtained for such domains, in classic mechanism
design, as well as in algorithmic mechanism
design. But how hard is it to satisfy W-MON
for multi-dimensional domains? This question
is yet unclear, and seems to be one of the
challenges of algorithmic mechanism design.
The contrast between single-dimensionality and
multi-dimensionality appears in all problem
domains that were surveyed here, and seems to
reﬂect some inherent difﬁculty that is not exactly
understood yet. Given a social choice function
f, call f implementable (in dominant strategies)
if there exist prices p such that M D .f; p/ is
truthful. The basic question is then what forms of
social choice functions are implementable.
As detailed in the beginning, the welfare max-
imizing social choice function is implementable.
This speciﬁc function can be slightly generalized
to allow weights, in the following way: ﬁx some
non-negative real constants fwign
iD1 (not all are
zero) and faga2A, and choose an alternative
that maximizes the weighted social welfare, i.e.,
f .v/ 2 argmaxa2A
P
i wivi.a/ C a. This class
of functions is sometimes termed “afﬁne maxi-
mizers”. It turns out that these functions are also
implementable, with prices similar in spirit to
VCG. In the context of the above characterization
question, one sharp result stands out:
Theorem 11 ([34]) Fix a social choice function
f W V ! A, such that (i) A is ﬁnite, jAj  3, and
f is onto A, and (ii) Vi D <A for all i. Then f
is implementable (in dominant strategies) if and
only if it is an afﬁne maximizer.
The domain V that satisﬁes Vi D <A for all i
is term an “unrestricted domain”. The theorem
states that, if the domain is unrestricted, at least
three alternatives are chosen, and the set A of
alternatives is ﬁnite, then nothing besides afﬁne
maximizers can be implemented!
However, the assumption that the domain is
unrestricted is very restrictive. All the above
example domains exhibit some basic combina-
torial structure, and are therefore restricted in
some way. And as discussed above, for many
restricted domains the theorem is simply not
true. So what is the possibilities – impossibilities
border? As mentioned above, this is an unsolved
challenge. Lavi, Mu’alem, and Nisan [23] ex-
plore this question for Combinatorial Auctions
and similar restricted domains, and reach partial
answers. For example:
Theorem 12 ([23]) Any truthful combinatorial
auction or multi-unit auction among two players,
that must always allocate all items, and that
approximates the welfare by a factor better than
2, must be an afﬁne maximizer.
Of-course, this is far from being a complete
answer. What happens if there are more than two
players? And what happens if it is possible to
“throw away” part of the items? These questions,
and the more general and abstract characteriza-
tion question, are all still open.
Alternative solution concepts
In light of the conclusions of the previous section,
a natural thought would be to re-examine the
solution concept that is being used. Truthfulness
relies on the strong concept of dominant strate-
gies: for each player there is a unique strategy that
maximizes her utility, no matter what the other
players are doing. This is very strong, but it ﬁts
very well the worst-case way of thinking in CS.
What other solution concepts can be used? As de-
scribed above, randomization, and truthfulness-
in-expectation, can help. A related concept, again
for randomized mechanisms, is truthfulness with
high probability. Another direction is to consider
mechanisms where players cannot improve their
utility too much by deviating from the truth-
telling strategy [21].
Algorithm designers do not care so much
about actually reaching an equilibrium point, or
ﬁnding out what will the players play – the major
concern is to guarantee the optimality of the so-
lution, taking into account the strategic behavior
of the players. Indeed, one way of doing this is to
guarantee a good equilibrium point. But there is

46
Algorithmic Mechanism Design
no reason to rule out mechanisms where several
acceptable strategic choices for the players exist,
provided that the approximation will be achieved
in each of these choices.
As a ﬁrst attempt, one is tempted to simply
let the players try and improve the basic result
by allowing them to lie. However, this can cause
unexpected dynamics, as each player chooses her
lies under some assumptions about the lies of the
others, etc. etc. To avoid such an unpredictable
situation, it is important to insist on using rigor-
ous game theoretic reasoning to explain exactly
why the outcome will be satisfactory.
The work [31] suggests the notion of “feasibly
dominant” strategies, where players reveal the
possible lies they consider, and the mechanism
takes this into account. By assuming that the
players are computationally bounded, one can
show that, instead of actually “lying”, the players
will prefer to reveal their true types plus all the
lies they might consider. In such a case, since
the mechanism has obtained the true types of
the players, a close-to-optimal outcome will be
guaranteed.
Another deﬁnition tries to capture the initial
intuition by using the classic game-theoretic no-
tion of undominated strategies:
Deﬁnition 4 ([5]) A mechanism M is an “al-
gorithmic implementation of a c-approximation
(in undominated strategies)” if there exists a set
of strategies, D, such that (i) M obtains a c-
approximation for any combination of strategies
from D, in polynomial time, and (ii) For any
strategy not in D, there exists a strategy in D
that weakly dominates it, and this transition is
polynomial-time computable.
By the second condition, it is reasonable to as-
sume that a player will indeed play some strategy
in D, and, by the ﬁrst condition, it does not
matter what tuple of strategies in D will actually
be chosen, as any of these will provide the ap-
proximation. This transfers some of the burden
from the game-theoretic design to the algorithmic
design, since now a guarantee on the approxi-
mation should bu provided for a larger range of
strategies. Babaioff et al. [5] exploit this notion to
design a deterministic CA for multi-dimensional
players that achieves a close-to-optimal approxi-
mation guarantee. A similar-in-spirit notion, al-
though a weaker one, is the notion of “Set-
Nash” [25].
Applications
One of the popular examples to a “real-life” com-
binatorial auction is the spectrum auction that the
US government conducts, in order to sell spec-
trum licenses. Typical bids reﬂect values for dif-
ferent spectrum ranges, to accommodate different
geographical and physical needs, where different
spectrum ranges may complement or substitute
one another. The US government invests research
efforts in order to determine the best format for
such an auction, and auction theory is heavily
exploited. Interestingly, the US law guides the
authorities to allocate these spectrum ranges in
a way that will maximize the social welfare, thus
providing a good example for the usefulness of
this goal.
Adword auctions are another new and fast-
growing application of auction theory in general,
and of the new algorithmic auctions in particular.
These are auctions that determine the advertise-
ments that web-search engines place close to the
search results they show, after the user submits
her search keywords. The interested companies
compete, for every given keyword, on the right to
place their ad on the results’ page, and this turns
out to be the main source of income for com-
panies like Google. Several entries in this book
touch on this topic in more details, including
the entries on Adwords Pricing and on Position
Auctions.
A third example to a possible application, in
the meanwhile implemented only in the academic
research labs, is the application of algorithmic
mechanism design to pricing and congestion con-
trol in communication networks. The existing
ﬁxed pricing scheme has many disadvantages,
both with respect to the needs of efﬁciently allo-
cating the available resources, and with respect to
the new opportunities of the Internet companies
to raise more revenue due to speciﬁc types of

Algorithmic Mechanism Design
47
A
trafﬁc. Theory suggests solutions to both of these
problems.
Cross-References
▷Adwords Pricing
▷Competitive Auction
▷False-Name-Proof Auction
▷Generalized Vickrey Auction
▷Incentive Compatible Selection
▷Position Auction
▷Truthful Multicast
Recommended Reading
The topics presented here are detailed in the
textbook [33]. Section “Problem Deﬁnition” is
based on the paper [32], that also coined the term
“algorithmic mechanism design”. The book [14]
covers the various aspects of combinatorial auc-
tions.
1. Aggarwal G, Fiat A, Goldberg A, Immorlica N,
Sudan M (2005) Derandomization of auctions. In:
Proceedings of the 37th ACM symposium on theory
of computing (STOC’05)
2. Andelman N, Azar Y, Sorani M (2005) Truthful
approximation mechanisms for scheduling selﬁsh re-
lated machines. In: Proceedings of the 22nd interna-
tional symposium on theoretical aspects of computer
science (STACS), pp 69–82
3. Archer A, Tardos É (2001) Truthful mechanisms for
one-parameter agents. In: Proceedings of the 42nd an-
nual symposium on foundations of computer science
(FOCS), pp 482–491
4. Awerbuch B, Azar Y, Meyerson A (2003) Reducing
truth-telling online mechanisms to online optimiza-
tion. In: Proceedings of the 35th ACM symposium on
theory of computing (STOC’03)
5. Babaioff M, Lavi R, Pavlov E (2006) Single-value
combinatorial auctions and implementation in un-
dominated strategies. In: Proceedings of the 17th
symposium on discrete algorithms (SODA)
6. Balcan M, Blum A, Hartline J, Mansour Y (2005)
Mechanism design via machine learning. In: Proceed-
ings of the 46th annual symposium on foundations of
computer science (FOCS’05)
7. Bartal Y, Gonen R, Nisan N (2003) Incentive compat-
ible multiunit combinatorial auctions. In: Proceedings
of the 9th conference on theoretical aspects of ratio-
nality and knowledge (TARK’03)
8. Bikhchandani S, Chatterjee S, Lavi R, Mu’alem A,
Nisan N, Sen A (2006) Weak monotonicity character-
izes deterministic dominant-strategy implementation.
Econometrica 74:1109–1132
9. Blum A, Hartline J (2005) Near-optimal online auc-
tions. In: Proceedings of the 16th symposium on
discrete algorithms (SODA)
10. Blum
A,
Sandholm
T,
Zinkevich
M
(2006)
Online algorithms for market clearing. J ACM
53(5):845–879
11. Blumrosen L, Nisan N (2005) On the computational
power of iterative auctions. In: Proceedings of the 7th
ACM conference on electronic commerce (EC’05)
12. Borgs C, Chayes J, Immorlica N, Mahdian M, Saberi
A (2005) Multi-unit auctions with budget-constrained
bidders. In: Proceedings of the 7th ACM conference
on electronic commerce (EC’05)
13. Christodoulou G, Koutsoupias E, Vidali A (2007)
A lower bound for scheduling mechanisms. In: Pro-
ceedings of the 18th symposium on discrete algo-
rithms (SODA)
14. Cramton P, Shoham Y, Steinberg R (2005) Combina-
torial auctions. MIT
15. Dobzinski S, Nisan N, Schapira M (2006) Truthful
randomized mechanisms for combinatorial auctions.
In: Proceedings of the 38th ACM symposium on
theory of computing (STOC’06)
16. Feige U (2006) On maximizing welfare when utility
functions are subadditive. In: Proceedings of the 38th
ACM symposium on theory of computing (STOC’06)
17. Goldberg A, Hartline J, Karlin A, Saks M, Wright
A (2006) Competitive auctions. Games Econ Behav
55(2):242–269
18. Gui H, Muller R, Vohra RV (2004) Character-
izing dominant strategy mechanisms with multi-
dimensional types. Working paper
19. Hajiaghayi M, Kleinberg R, Parkes D (2004) Adap-
tive limited-supply online auctions. In: Proceedings
of the 6th ACM conference on electronic commerce
(EC’04)
20. Hartline J, McGrew R (2005) From optimal limited to
unlimited supply auctions. In: Proceedings of the 7th
ACM conference on electronic commerce (EC’05)
21. Kothari A, Parkes D, Suri S (2005) Approximately-
strategy proof and tractable multi-unit auctions. Decis
Support Syst 39:105–121
22. Kovács A (2005) Fast monotone 3-approximation
algorithm for scheduling related machines. In: Pro-
ceedings of the 13th annual European symposium on
algorithms (ESA), pp 616–627
23. Lavi R, Mu’alem A, Nisan N (2003) Towards a char-
acterization of truthful combinatorial auctions. In:
Proceedings of the 44rd annual symposium on foun-
dations of computer science (FOCS’03)
24. Lavi R, Nisan N (2004) Competitive analysis of
incentive compatible on-line auctions. Theor Comput
Sci 310:159–180
25. Lavi R, Nisan N (2005) Online ascending auctions for
gradually expiring items. In: Proceedings of the 16th
symposium on discrete algorithms (SODA)

48
Algorithms for Combining Rooted Triplets into a Galled Phylogenetic Network
26. Lavi R, Swamy C (2005) Truthful and near-optimal
mechanism design via linear programming. In:
Proceedings
of
the
46th
annual
symposium
on
foundations
of
computer
science
(FOCS),
pp 595–604
27. Lavi R, Swamy C (2007) Truthful mechanism design
for multi-dimensional scheduling via cycle mono-
tonicity. Working paper
28. Lehmann B, Lehmann D, Nisan N (2006) Combi-
natorial auctions with decreasing marginal utilities.
Games Econ Behav 55(2):270–296
29. Lehmann D, O’Callaghan L, Shoham Y (2002) Truth
revelation in approximately efﬁcient combinatorial
auctions. J ACM 49(5):577–602
30. Mu’alem A, Schapira M (2007) Setting lower bounds
on truthfulness. In: Proceedings of the 18th sympo-
sium on discrete algorithms (SODA)
31. Nisan N, Ronen A (2000) Computationally feasible
VCG mechanisms. In: Proceedings of the 2nd ACM
conference on electronic commerce (EC’00)
32. Nisan N, Ronen A (2001) Algorithmic mechanism
design. Games Econ Behav 35:166–196
33. Nisan N, Roughgarden T, Tardos E, Vazirani V
(2007) Algorithmic game theory. Cambridge Univer-
sity Press (expected to appear)
34. Roberts K (1979) The characterization of imple-
mentable choice rules. In: Laffont JJ (ed) Aggrega-
tion and revelation of preferences. North-Holland,
pp 321–349
35. Saks M, Yu L (2005) Weak monotonicity sufﬁces
for truthfulness on convex domains. In: Proceedings
of the 6th ACM conference on electronic commerce
(ACM-EC), pp 286–293
Algorithms for Combining Rooted
Triplets into a Galled Phylogenetic
Network
Jesper Jansson1 and Wing-Kin Sung2
1Laboratory of Mathematical Bioinformatics,
Institute for Chemical Research, Kyoto
University, Gokasho, Uji, Kyoto, Japan
2Department of Computer Science, National
University of Singapore, Singapore, Singapore
Keywords
Dense set; Galled phylogenetic network; Phy-
logenetic tree; Polynomial-time approximation
algorithm; Rooted triplet
Years and Authors of Summarized
Original Work
2006; Jansson, Sung
2006; Jansson, Nguyen, Sung
2006; He, Huynh, Jansson, Sung
2010; Byrka, Gawrychowski, Huber, Kelk
2011; van Iersel, Kelk
Problem Deﬁnition
A phylogenetic tree is a binary, rooted, unordered
tree whose leaves are distinctly labeled. A phylo-
genetic network is a generalization of a phyloge-
netic tree formally deﬁned as a rooted, connected,
directed acyclic graph in which (1) each node has
outdegree at most 2; (2) each node has indegree 1
or 2, except the root node which has indegree 0;
(3) no node has both indegree 1 and outdegree 1;
and (4) all nodes with outdegree 0 are labeled
by elements from a ﬁnite set L in such a way
that no two nodes are assigned the same label.
Nodes of outdegree 0 are referred to as leaves and
are identiﬁed with their corresponding elements
in L. Nodes with indegree 2 are called reticula-
tion nodes. For any phylogenetic network N , let
U.N / be the undirected graph obtained from N
by replacing each directed edge by an undirected
edge. N is said to be a galled phylogenetic
network (galled network, for short) if all cycles
in U.N / are node-disjoint. Galled networks are
also known in the literature as topologies with
independent recombination events [15], galled-
trees [6], and level-1 phylogenetic networks [2,5,
7,9,10,14].
A phylogenetic tree with exactly three leaves
is called a rooted triplet. The unique rooted triplet
on a leaf set fx; y; ´g in which the lowest com-
mon ancestor of x and y is a proper descendant
of the lowest common ancestor of x and ´ (or
equivalently, where the lowest common ances-
tor of x and y is a proper descendant of the
lowest common ancestor of y and ´) is denoted
by xyj´. For any phylogenetic network N , the
rooted triplet xyj´ is said to be consistent with N
if N contains three leaves labeled by x, y, and ´

Algorithms for Combining Rooted Triplets into a Galled Phylogenetic Network
49
A
Algorithms for
Combining Rooted
Triplets into a Galled
Phylogenetic Network,
Fig. 1 A dense set T D
fabjc; abjd; cdja; bcjdg
of rooted triplets with leaf
set fa; b; c; dg and a
galled phylogenetic
network that is consistent
with T . Note that this
solution is not unique
b
c
c
a
d
d
b
c
a
d
a
b
c
a
b
d
as well as two internal vertices w and ´ such that
there are four directed paths of nonzero length
from w to a, from w to b, from ´ to w, and
from ´ to c that are vertex-disjoint except for in
the vertices w and ´. A set T of rooted triplets is
consistent with N if every rooted triplet in T is
consistent with N . See Fig. 1 for an example.
Denote the set of leaves in any phylogenetic
network N by .N /, and for any set T
of
rooted triplets, deﬁne .T /
D
S
ti 2T .ti/.
A set T of rooted triplets is dense if for each
fx; y; ´g  .T /, at least one of the three
possible rooted triplets xyj´, x´jy, and y´jx
belongs to T . Observe that if T is dense, then
jT j D 	.j.T /j3/. Jansson and Sung introduced
the following problem in [10].
Problem 1 Given a set T of rooted triplets, out-
put a galled network N with .N / D .T / such
that N and T are consistent, if such a network
exists; otherwise, output null.
A natural optimization version of Problem 1
is:
Problem 2 Given a set T of rooted triplets, out-
put a galled network N with .N / D .T / that
is consistent with the maximum possible number
of rooted triplets belonging to T .
A generalization of Problem 1 studied by He
et al. in [8] involves forbidden rooted triplets and
is deﬁned as follows.
Problem 3 Given two sets T and F of rooted
triplets, output a galled network N with .N / D
.T / [ .F/ such that (1) N and T are consis-
tent and (2) N is not consistent with any rooted
triplet belonging to F; if no such network exists,
output null.
Below, we write L D .T / and n D jLj.
Key Results
As shown in [11], Problem 1 can be solved in
(optimal) O.jT j/ D O.n3/ time for dense inputs:
Theorem 1 ([11]) Given
any
dense
set
T
of rooted triplets with leaf set L, a galled
network consistent with T (if one exists) can
be constructed in O.n3/ time, where n D jLj.
The algorithm referred to in Theorem 1 was
extended by van Iersel and Kelk [14] as follows.
Theorem 2 ([14]) Given any dense set T
of
rooted triplets with leaf set L, a galled network
consistent with T (if one exists) that contains
as few reticulation nodes as possible can be
constructed in O.n5/ time, where n D jLj.
For the more general case of nondense inputs,
Problem 1 becomes harder:
Theorem 3 ([11]) The problem of determining if
there exists a galled network that is consistent

50
Algorithms for Combining Rooted Triplets into a Galled Phylogenetic Network
with an input nondense set T of rooted triplets
is NP-hard.
Since not all sets of rooted triplets are con-
sistent with a galled network, it is of interest to
consider Problem 2. It follows from Theorem 3
that Problem 2 is also NP-hard for nondense
inputs, and this motivates polynomial-time ap-
proximation algorithms. Say that an algorithm
for Problem 2 is an f -approximation algorithm
if it always returns a galled network N such
that N.T /
jT j
 f , where N.T / is the number of
rooted triplets in T that are consistent with N .
Deﬁne the nonlinear recurrence relation S.n/ D
max1an
˚a
3

C2
a
2

.na/Ca
na
2

CS.n
a/

for n > 0 and S.0/ D 0. It was shown in [4]
that limn!1
S.n/
3.n
3/ D
2.
p
31/
3

 0:488033 : : :
and that
S.n/
3.n
3/ >
2.
p
31/
3

 0:488033 : : : for
all n > 2. The following theorem was proved by
Byrka et al. in [2].
Theorem 4 ([2]) There exists an S.n/
3.n
3/-approxim-
ation algorithm for Problem 2 that runs in
O.n3 C njT j/ time.
A matching negative bound is:
Theorem 5 ([11]) For any f > limn!1
S.n/
3.n
3/,
there exists a set T of rooted triplets such that
no galled network can be consistent with at least
a factor of f of the rooted triplets in T . (Thus,
no f -approximation algorithm for Problem 2 is
possible.)
For Problem 3, Theorem 3 immediately im-
plies NP-hardness by taking F D ;. The follow-
ing positive result is known for the optimization
version of Problem 3.
Theorem 6 ([8]) There exists an O.jLj2jT j.jT j
CjFj//-time algorithm for inferring a galled net-
work N that guarantees jN.T /j  jN.F/j

5
12  .jT j  jFj/, where L D .T / [ .F/.
Finally, we remark that the analogous version
of Problem 1 of inferring a phylogenetic tree con-
sistent with all the rooted triplets in an input set
(when such a tree exists) can be solved in poly-
nomial time with a classical algorithm by Aho
et al. [1] from 1981. Similarly, for Problem 2, to
infer a phylogenetic tree consistent with as many
rooted triplets from an input set of rooted triplets
as possible is NP-hard and admits a polynomial-
time 1=3-approximation algorithm, which is op-
timal in the sense that there exist certain inputs
for which no tree can achieve a factor larger than
1=3. See, e.g., [3] for a survey of known results
about maximizing rooted triplet consistency for
trees. On the other hand, more complex network
structures such as the level-k phylogenetic net-
works [5] permit a higher percentage of the input
rooted triplets to be embedded; in the extreme
case, if there are no restrictions on the reticula-
tion nodes at all, then a sorting network-based
construction yields a phylogenetic network that
is trivially consistent with every rooted triplet
over L [10]. A number of efﬁcient algorithms
for combining rooted triplets into higher level
networks have been developed; see, e.g., [2,7,14]
for further details and references.
Applications
Phylogenetic networks are used by scientists to
describe evolutionary relationships that do not
ﬁt the traditional models in which evolution is
assumed to be treelike. Evolutionary events such
as horizontal gene transfer or hybrid speciation
(often referred to as recombination events) which
suggest convergence between objects cannot be
represented in a single tree but can be modeled in
a phylogenetic network as internal nodes having
more than one parent (i.e., reticulation nodes).
The phylogenetic network is a relatively new tool,
and various fast and reliable methods for con-
structing and comparing phylogenetic networks
are currently being developed.
Galled networks form an important class
of phylogenetic networks. They have attracted
special attention in the literature [5, 6, 15] due
to their biological signiﬁcance (see [6]) and
their simple, almost treelike, structure. When
the number of recombination events is limited
and most of the recombination events have
occurred recently, a galled network may sufﬁce
to accurately describe the evolutionary process
under study [6]. The motivation behind the

Algorithms for Combining Rooted Triplets into a Galled Phylogenetic Network
51
A
rooted triplet approach taken here is that a highly
accurate tree for each cardinality-three subset of
the leaf set can be obtained through maximum
likelihood-based methods or Sibley-Ahlquist-
style
DNA-DNA
hybridization
experiments
(see [13]). The algorithms mentioned above can
be used as the merging step in a divide-and-
conquer approach for constructing phylogenetic
networks
analogous
to
the
quartet
method
paradigm for inferring unrooted phylogenetic
trees [12] and other supertree methods. We
consider dense input sets in particular because
this case can be solved in polynomial time.
Open Problems
The approximation factor given in Theorem 4
is expressed in terms of the number of rooted
triplets in the input T , and Theorem 5 shows that
it cannot be improved. However, if one measures
the quality of the approximation in terms of a
galled network NOP T that is consistent with
the maximum possible number of rooted triplets
from T , Theorem 4 can be far from optimal. An
open problem is to determine the polynomial-
time approximability and inapproximability of
Problem 2 when the approximation ratio is de-
ﬁned as
N.T /
NOP T .T / instead of N.T /
jT j .
Another research direction is to develop ﬁxed-
parameter polynomial-time algorithms for Prob-
lem 1. The level of the constructed network, the
number of allowed reticulation nodes, or some
measure of the density of the input set of rooted
triplet might be suitable parameters.
URLs to Code and Data Sets
A Java implementation of the algorithm for
Problem 1 referred to in Theorem 2 (coded by
its authors [14]) is available at http://skelk.sdf-
eu.org/marlon.html. See also http://skelk.sdf-eu.
org/lev1athan/ for a Java implementation of a
polynomial-time heuristic described in [9] for
Problem 2.
Cross-References
▷Directed Perfect Phylogeny (Binary Charac-
ters)
▷Distance-Based
Phylogeny
Reconstruction
(Fast-Converging)
▷Distance-Based
Phylogeny
Reconstruction:
Safety and Edge Radius
▷Perfect
Phylogeny
(Bounded
Number
of
States)
▷Phylogenetic Tree Construction from a Dis-
tance Matrix
Acknowledgments JJ was funded by the Hakubi Project
at Kyoto University and KAKENHI grant number
26330014.
Recommended Reading
1. Aho AV, Sagiv Y, Szymanski TG, Ullman JD (1981)
Inferring a tree from lowest common ancestors with
an application to the optimization of relational ex-
pressions. SIAM J Comput 10(3):405–421
2. Byrka J, Gawrychowski P, Huber KT, Kelk S (2010)
Worst-case optimal approximation algorithms for
maximizing triplet consistency within phylogenetic
networks. J Discret Algorithms 8(1):65–75
3. Byrka J, Guillemot S, Jansson J (2010) New results
on optimizing rooted triplets consistency.
Discret
Appl Math 158(11):1136–1147
4. Chao K-M, Chu A-C, Jansson J, Lemence RS,
Mancheron A (2012)
Asymptotic limits of a new
type of maximization recurrence with an application
to bioinformatics. In: Proceedings of the 9th annual
conference on theory and applications of models of
computation (TAMC 2012), Beijing, pp 177–188
5. Choy C, Jansson J, Sadakane K, Sung W-K (2005)
Computing the maximum agreement of phylogenetic
networks. Theor Comput Sci 335(1):93–107
6. Gusﬁeld D, Eddhu S, Langley C (2003)
Efﬁcient
reconstruction of phylogenetic networks with con-
strained recombination. In: Proceedings of computa-
tional systems bioinformatics (CSB2003), Stanford,
pp 363–374
7. Habib M, To T-H (2012) Constructing a minimum
phylogenetic network from a dense triplet set.
J
Bioinform Comput Biol 10(5):1250013
8. He Y-J, Huynh TND, Jansson J, Sung W-K (2006) In-
ferring phylogenetic relationships avoiding forbidden
rooted triplets. J Bioinform Comput Biol 4(1):59–74
9. Huber KT, van Iersel L, Kelk S, Suchecki R (2011)
A practical algorithm for reconstructing level-1 phy-
logenetic networks. IEEE/ACM Trans Comput Biol
Bioinform 8(3):635–649

52
All Pairs Shortest Paths in Sparse Graphs
10. Jansson J, Sung W-K (2006)
Inferring a level-1
phylogenetic network from a dense set of rooted
triplets. Theor Comput Sci 363(1):60–68
11. Jansson J, Nguyen NB, Sung W-K (2006) Algorithms
for combining rooted triplets into a galled phyloge-
netic network. SIAM J Comput 35(5):1098–1121
12. Jiang T, Kearney P, Li M (2001)
A polynomial
time approximation scheme for inferring evolutionary
trees from quartet topologies and its application.
SIAM J Comput 30(6):1942–1961
13. Kannan S, Lawler E, Warnow T (1996) Determining
the evolutionary tree using experiments. J Algorithms
21(1):26–50
14. van Iersel L, Kelk S (2011) Constructing the simplest
possible phylogenetic network from triplets. Algo-
rithmica 60(2):207–235
15. Wang L, Zhang K, Zhang L (2001) Perfect phyloge-
netic networks with recombination. J Comput Biol
8(1):69–78
All Pairs Shortest Paths in Sparse
Graphs
Seth Pettie
Electrical Engineering and Computer Science
(EECS) Department, University of Michigan,
Ann Arbor, MI, USA
Keywords
Quickest route; Shortest route
Years and Authors of Summarized
Original Work
2004; Pettie
Problem Deﬁnition
Given a communications network or road net-
work, one of the most natural algorithmic ques-
tions is how to determine the shortest path from
one point to another. The all pairs shortest path
problem (APSP) is, given a directed graph G D
.V; E; l/, to determine the distance and shortest
path between every pair of vertices, where jV j D
n; jEj D m, and l W E ! R is the edge length
(or weight) function. The output is in the form
of two n  n matrices: D.u; v/ is the distance
from u to v and S.u; v/ D w if (u; w) is the
ﬁrst edge on a shortest path from u to v. The
APSP problem is often contrasted with the point-
to-point and single source (SSSP) shortest path
problems. They ask for, respectively, the shortest
path from a given source vertex to a given target
vertex and all shortest paths from a given source
vertex.
Deﬁnition of Distance
If ` assigns only non-negative edge lengths then
the deﬁnition of distance is clear: D.u; v/ is the
length of the minimum length path from u to v,
where the length of a path is the total length of
its constituent edges. However, if ` can assign
negative lengths then there are several sensible
notations of distance that depend on how negative
length cycles are handled. Suppose that a cycle
C has negative length and that u; v 2 V are
such that C is reachable from u and v reachable
from C. Because C can be traversed an arbitrary
number of times when traveling from u to v, there
is no shortest path from u to v using a ﬁnite
number of edges. It is sometimes assumed a priori
that G has no negative length cycles; however it
is cleaner to deﬁne D.u; v/ D 1 if there is no
ﬁnite shortest path. If D.u; v/ is deﬁned to be the
length of the shortest simple path (no repetition of
vertices) then the problem becomes NP-hard. (If
all edges have length 1 then D.u; v/ D .n1/
if and only if G contains a Hamiltonian path [7]
from u to v.) One could also deﬁne distance to be
the length of the shortest path without repetition
of edges.
Classic Algorithms
The Bellman-Ford algorithm solves SSSP in
O(mn) time and under the assumption that edge
lengths are non-negative, Dijkstra’s algorithm
solves it in O.m C n log n/ time. There is a
well known O(mn)-time shortest path preserving
transformation that replaces any length function
with a non-negative length function. Using
this transformation and n runs of Dijkstra’s
algorithm gives an APSP algorithm running in
O.mn C n2 log n/ D O.n3/ time. The Floyd-
Warshall algorithm computes APSP in a more

All Pairs Shortest Paths in Sparse Graphs
53
A
direct manner, in O.n3/ time. Refer to [4] for
a description of these algorithms. It is known
that APSP on complete graphs is asymptotically
equivalent to (min, C) matrix multiplication
[1], which can be computed by a non-uniform
algorithm
that
performs
O.n2:5/
numerical
operations [6].
Integer-Weighted Graphs
Much recent work on shortest paths assume
that edge lengths are integers in the range
fC; : : : ; Cg or f0; : : : ; Cg. One line of research
reduces APSP to a series of standard matrix
multiplications. These algorithms are limited in
their applicability because their running times
scale linearly with C. There are faster SSSP
algorithms for both non-negative edge lengths
and arbitrary edge lengths. The former exploit
the power of RAMs to sort in o.n log n/ time
and the latter are based on the scaling technique.
See Zwick [20] for a survey of shortest path
algorithms up to 2001.
Key Results
Pettie’s APSP algorithm [12] adapts the hier-
archy approach of Thorup [16] (designed for
undirected, integer-weighted graphs) to general
real-weighted directed graphs. Theorem 1 is the
ﬁrst improvement over the O.mnCn2 log n/ time
bound of Dijkstra’s algorithm on arbitrary real-
weighted graphs.
Theorem 1 Given
a
real-weighted
directed
graph, all pairs shortest paths can be solved
in O.mn C n2 log log n/ time.
This algorithm achieves a logarithmic speedup
through a trio of new techniques. The ﬁrst is to
exploit the necessary similarity between the SSSP
trees emanating from nearby vertices. The second
is a method for computing discrete approximate
distances in real-weighted graphs. The third is
a new hierarchy-type SSSP algorithm that runs
in O.m C n log log n/ time when given suitably
accurate approximate distances.
Theorem 1 should be contrasted with the time
bounds of other hierarchy-type APSP algorithms
[11,14,16].
Theorem 2 ([14], 2005) Given a real-weighted
undirected graph, APSP can be solved in
O.mn log ˛.m; n// time.
Theorem 3 ([16], 1999) Given an undirected
graph G.V; E; l/, where ` assigns integer edge
lengths in the range f  2w1; : : : ; 2w1  1g,
APSP can be solved in O.mn/ time on a RAM
with w-bit word length.
Theorem 4 ([13], 2002) Given a real-weighted
directed
graph,
APSP
can
be
solved
in
polynomial time by an algorithm that performs
O.mn log ˛.m; n// numerical operations, where
˛ is the inverse-Ackermann function.
A secondary result of [12, 14] is that no
hierarchy-type
shortest
path
algorithm
can
improve on the O.m C n log n/ running time
of Dijkstra’s algorithm.
Theorem 5 Let G be an input graph such
that the ratio of the maximum to minimum
edge length is r. Any hierarchy-type SSSP
algorithm performs .mCmin fn log n; n log rg/
numerical operations if G
is directed and
.m C min fn log n; n log log rg/
if
G
is
undirected.
Applications
Shortest paths appear as a subproblem in other
graph optimization problems; the minimum
weight perfect matching, minimum cost ﬂow,
and minimum mean-cycle problems are some
examples. A well known commercial application
of shortest path algorithms is ﬁnding efﬁcient
routes on road networks; see, for example,
Google Maps, MapQuest, or Yahoo Maps.
Open Problems
The longest standing open shortest path problems
are to improve the SSSP algorithms of Dijkstra’s
and Bellman-Ford on real-weighted graphs.

54
All Pairs Shortest Paths in Sparse Graphs
Problem 1 Is there an o(mn) time SSSP or point-
to-point shortest path algorithm for arbitrarily
weighted graphs?
Problem 2 Is there an O.m/ C o.n log n/ time
SSSP algorithm for directed, non-negatively
weighted graphs? For undirected graphs?
A partial answer to Problem 2 appears in
[14], which considers undirected graphs. Perhaps
the most surprising open problem is whether
there is any (asymptotic) difference between the
complexities of the all pairs, single source, and
point-to-point shortest path problems on arbitrar-
ily weighted graphs.
Problem 3 Is point-to-point shortest paths eas-
ier than all pairs shortest paths on arbitrarily
weighted graphs?
Problem 4 Is there a truly subcubic APSP al-
gorithm, i.e., one running in time O.n3/? In
a recent breakthrough on this problem, Williams
[19] gave a new APSP algorithm running in
n3=2‚.p
log n= log log n/ time. Vassilevska Williams
and Williams [17] proved that a truly subcubic
algorithm for APSP would imply truly subcubic
algorithms for other graph problems.
Experimental Results
See [5, 8, 15] for recent experiments on SSSP
algorithms. On sparse graphs the best APSP al-
gorithms use repeated application of an SSSP
algorithm, possibly with some precomputation
[15]. On dense graphs cache-efﬁciency becomes
a major issue. See [18] for a cache conscious
implementation of the Floyd-Warshall algorithm.
The trend in recent years is to construct a lin-
ear space data structure that can quickly answer
exact or approximate point-to-point shortest path
queries; see [2,6,9,10].
Data Sets
See [5] for a number of U.S. and European road
networks.
URL to Code
See [5].
Cross-References
▷All Pairs Shortest Paths via Matrix Multiplica-
tion
▷Single-Source Shortest Paths
Recommended Reading
1. Aho AV, Hopcroft JE, Ullman JD (1975) The de-
sign and analysis of computer algorithms. Addison-
Wesley, Reading
2. Bast H, Funke S, Matijevic D, Sanders P, Schultes
D (2007) In transit to constant shortest-path queries
in road networks. In: Proceedings of the 9th work-
shop on algorithm engineering and experiments
(ALENEX), San Francisco
3. Chan T (2007) More algorithms for all-pairs shortest
paths in weighted graphs. In: Proceedings of the 39th
ACM symposium on theory of computing (STOC),
San Diego, pp 590–598
4. Cormen TH, Leiserson CE, Rivest RL, Stein C (2001)
Introduction to algorithms. MIT, Cambridge
5. Demetrescu C, Goldberg AV, Johnson D (2006) 9th
DIMACS implementation challenge – shortest paths.
http://www.dis.uniroma1.it/~challenge9/
6. Fredman ML (1976) New bounds on the complex-
ity of the shortest path problem. SIAM J Comput
5(1):83–89
7. Garey MR, Johnson DS (1979) Computers and in-
tractability: a guide to NP-completeness. Freeman,
San Francisco
8. Goldberg AV (2001) Shortest path algorithms: engi-
neering aspects. In: Proceedings of the 12th inter-
national symposium on algorithms and computation
(ISAAC), Christchurch. LNCS, vol 2223. Springer,
Berlin, pp 502–513
9. Goldberg AV, Kaplan H, Werneck R (2006) Reach for
A*: efﬁcient point-to-point shortest path algorithms.
In: Proceedings of the 8th workshop on algorithm
engineering and experiments (ALENEX), Miami
10. Knopp S, Sanders P, Schultes D, Schulz F, Wagner
D (2007) Computing many-to-many shortest paths
using highway hierarchies. In: Proceedings of the 9th
workshop on algorithm engineering and experiments
(ALENEX), New Orleans
11. Pettie S (2002) On the comparison-addition com-
plexity of all-pairs shortest paths. In: Proceedings of
the 13th international symposium on algorithms and
computation (ISAAC), Vancouver, pp 32–43

All Pairs Shortest Paths via Matrix Multiplication
55
A
12. Pettie S (2004) A new approach to all-pairs shortest
paths on real-weighted graphs. Theor Comput Sci
312(1):47–74
13. Pettie S, Ramachandran V (2002) Minimizing ran-
domness in minimum spanning tree, parallel connec-
tivity and set maxima algorithms. In: Proceedings of
the 13th ACM-SIAM symposium on discrete algo-
rithms (SODA), San Francisco, pp 713–722
14. Pettie S, Ramachandran V (2005) A shortest path
algorithm for real-weighted undirected graphs. SIAM
J Comput 34(6):1398–1431
15. Pettie S, Ramachandran V, Sridhar S (2002) Experi-
mental evaluation of a new shortest path algorithm.
In: Proceedings of the 4th workshop on algorithm
engineering and experiments (ALENEX), San Fran-
cisco, pp 126–142
16. Thorup M (1999) Undirected single-source shortest
paths with positive integer weights in linear time. J
ACM 46(3):362–394
17. Vassilevska Williams V, Williams R (2010) Subcubic
equivalences between path, matrix, and triangle prob-
lems. In Proceedings of the 51st IEEE symposium
on foundations of computer science (FOCS), Los
Alamitos, pp 645–654
18. Venkataraman G, Sahni S, Mukhopadhyaya S (2003)
A blocked all-pairs shortest paths algorithm. J Exp
Algorithms 8
19. Williams R (2014) Faster all-pairs shortest paths via
circuit complexity. In Proceedings of the 46th ACM
symposium on theory of computing (STOC), New
York, pp 664–673
20. Zwick U (2001) Exact and approximate distances
in graphs – a survey. In: Proceedings of the 9th
European symposium on algorithms (ESA), Aarhus,
pp 33–48. See updated version at http://www.cs.tau.
ac.il/~zwick/
All Pairs Shortest Paths via Matrix
Multiplication
Tadao Takaoka
Department of Computer Science and Software
Engineering, University of Canterbury,
Christchurch, New Zealand
Keywords
Algorithm
analysis; All
pairs
shortest
path
problem; Bridging set; Matrix multiplication;
Shortest path problem; Two-phase algorithm;
Witness
Years and Authors of Summarized
Original Work
2002; Zwick
Problem Deﬁnition
The all pairs shortest path (APSP) problem is
to compute shortest paths between all pairs of
vertices of a directed graph with nonnegative real
numbers as edge costs. Focus is given on shortest
distances between vertices, as shortest paths can
be obtained with a slight increase of cost. Classi-
cally, the APSP problem can be solved in cubic
time of O.n3/. The problem here is to achieve
a sub-cubic time for a graph with small integer
costs.
A directed graph is given by G D .V; E/,
where V D f1; : : : ; ng, the set of vertices, and E
is the set of edges. The cost of edge .i; j / 2 E is
denoted by dij . The .n; n/-matrix D is one whose
.i; j / element is dij . It is assumed for simplicity
that dij > 0 and dii D 0 for all i ¤ j . If
there is no edge from i to j , let dij D 1. The
cost, or distance, of a path is the sum of costs of
the edges in the path. The length of a path is the
number of edges in the path. The shortest distance
from vertex i to vertex j is the minimum cost
over all paths from i to j , denoted by d 
ij . Let
D D fd 
ij g. The value of n is called the size of
the matrices.
Let A and B are .n; n/-matrices. The three
products are deﬁned using the elements of A and
B as follows: (1) Ordinary matrix product over a
ring C D AB (2) Boolean matrix product C D
A  B (3) Distance matrix product C D A  B,
where
.1/ cij D
n
X
kD1
aikbkj ;
.2/ cij D
n
_
kD1
aik ^ bkj ;
.3/ cij D min
1knfaik C bkj g:
The matrix C is called a product in each case; the
computational process is called multiplication,
such as distance matrix multiplication. In those

56
All Pairs Shortest Paths via Matrix Multiplication
three cases, k changes through the entire set
f1; : : :; ng. A partial matrix product of A and
B is deﬁned by taking k in a subset I of V .
In other words, a partial product is obtained
by multiplying a vertically rectangular matrix,
A.; I/, whose columns are extracted from A
corresponding to the set I, and similarly a hor-
izontally rectangular matrix, B.I; /, extracted
from B with rows corresponding to I. Intuitively,
I is the set of check points k, when going from i
to j .
The best algorithm [11] computes (1) in
O.n!/ time, where ! D 2:373. This was recently
achieved as improvement from ! D 2:376 in [4]
after more than two decades of interval. We
use ! D 2:376 to describe Zwick’s result in
this article. Three decimal points are carried
throughout this article. To compute (2), Boolean
values 0 and 1 in A and B can be regarded
as integers and use the algorithm for (1), and
convert nonzero elements in the resulting matrix
to 1. Therefore, this complexity is O.n!/. The
witnesses of (2) are given in the witness matrix
W D fwij g where wij D k for some k such that
aik ^ bkj D 1. If there is no such k, wij D 0.
The witness matrix W D fwij g for (3) is deﬁned
by wij D k that gives the minimum to cij . If
there is an algorithm for (3) with T .n/ time,
ignoring a polylog factor of n, the APSP problem
can be solved in QO.T .n// time by the repeated
squaring method, described as the repeated use
of D D  D O.log n/ times.
The deﬁnition here of computing shortest
paths is to give a witness matrix of size n by
which a shortest path from i to j can be given
in O.`/ time where ` is the length of the path.
More speciﬁcally, if wij
D k in the witness
matrix W D fwij g, it means that the path from
i to j goes through k. Therefore, a recursive
function path.i; j / is deﬁned by .path.i; k/, k,
path.k; j // if path.i; j / D k > 0 and nil if
path.i; j / D 0, where a path is deﬁned by a list
of vertices excluding endpoints. In the following
sections, k is recorded in wij whenever k is found
such that a path from i to j is modiﬁed or newly
set up by paths from i to k and from k to j .
Preceding results are introduced as a framework
for the key results.
Alon-Galil-Margalit Algorithm
The algorithm by Alon, Galil, and Margalit [1]
is reviewed. Let the costs of edges of the given
graph be ones. Let D.`/ be the `-th approximate
matrix for D deﬁned by d .`/
ij
D d 
ij if d 
ij  `,
and d .`/
ij
D 1 otherwise. Let A be the adjacency
matrix of G, that is, aij D 1 if there is an edge
.i; j /, and aij D 0, otherwise. Let aii D 1 for
all i. The algorithm consists of two phases. In the
ﬁrst phase, D.`/ is computed for ` D 1; : : :; r,
by checking the .i; j / element of A` D fa`
ij g.
Note that if a`
ij
D 1, there is a path from i
to j of length ` or less. Since Boolean matrix
multiplication can be computed in O.n!/ time,
the computing time of this part is O.rn!/.
In the second phase, the algorithm computes
D.`/ for ` D r,
˙ 3
2r
	
,
l
3
2
˙ 3
2r
	m
; : : : ; n0 by
repeated squaring, where n0 is the smallest integer
in this sequence of ` such that `  n. Let
Ti˛
D fj jd .`/
ij
D ˛g and Ii
D Ti˛ such
that jTi˛j is minimum for d`=2e  ˛  `.
The key observation in the second phase is that
it is only needed to check k in Ii whose size
is not larger than 2n=`, since the correct dis-
tances between `C1 and d3`=2e can be obtained
as the sum d .`/
ik C d .`/
kj
for some k satisfying
d`=2e

d .`/
ik

`. The meaning of Ii is
similar to I for partial products except that I
varies for each i. Hence, the computing time of
one squaring is O.n3=`/. Thus, the time of the
second phase is given with N D dlog3=2 n=re
by O
PN
sD1 n3=..3=2/sr/

D O.n3=r/. Bal-
ancing the two phases with rn! D n3=r yields
O.n.!C3/=2/ D O.n2:688/ time for the algorithm
with r D O.n.3!/=2/.
Witnesses can be kept in the ﬁrst phase in
time polylog of n by the method in [2]. The
maintenance of witnesses in the second phase is
straightforward.
When a directed graph G whose edge costs are
integers between 1 and M is given, where M is a
positive integer, the graph G can be expanded to
G0 by creating up to M  1 new vertices for each
vertex and replacing each edge by up to M edges
with unit cost. Obviously, the problem for G can
be solved by applying the above algorithm to G0,

All Pairs Shortest Paths via Matrix Multiplication
57
A
which takes O

.Mn/.!C3/=2
time. This time is
sub-cubic when M < n0:116. The maintenance
of witnesses has an extra polylog factor in each
case.
For undirected graphs with unit edge costs,
QO.n!/ time is known in Seidel [9].
Takaoka Algorithm
When the edge costs are bounded by a positive in-
teger M, a better algorithm can be designed than
in the above as shown in Takaoka [10]. Romani’s
algorithm [7] for distance matrix multiplication is
reviewed brieﬂy.
Let A and B be .n; m/ and .m; n/ distance
matrices whose elements are bounded by M or
inﬁnite. Let the diagonal elements be 0. A and
B are converted into A0 and B0 where a0
ij D
.m C 1/Maij , if aij ¤ 1, 0, otherwise, and
b0
ij D .m C 1/Mbij , if bij ¤ 1, 0, otherwise.
Let C 0 D A0B0 be the product by ordinary
matrix multiplication and C D A  B be that
by distance matrix multiplication. Then, it holds
that
c0
ij D
m
X
kD1
.m C 1/2M.aikCbkj /;
cij D 2M  blogmC1 c0
ij c:
This distance matrix multiplication is called
.n; m/-Romani.
In
this
section,
the
above
multiplication is used with square matrices, that
is, .n; n/-Romani is used. In the next section, the
case where m < n is dealt with.
C can be computed with O.n!/ arithmetic
operations on integers up to .n C 1/M. Since
these values can be expressed by O.M log n/
bits and Schönhage and Strassen’s algorithm
[8]
for
multiplying
k-bit
numbers
takes
O.k log k log log k/ bit operations, C can be
computed in O.n!M log n log.M log n/ log log
.M log n// time, or QO.Mn!/ time.
The ﬁrst phase is replaced by the one based on
.n; n/-Romani and the second phase is modiﬁed
based on path lengths, not distances.
Note that the bound M is replaced by `M
in the distance matrix multiplication in the ﬁrst
phase. Ignoring polylog factors, the time for the
ﬁrst phase is given by
QO.n!r2M/. It is as-
sumed that M is O.nk/ for some constant k.
Balancing this complexity with that of second
phase, O.n3=r/, yields the total computing time
of
QO.n.6C!/=3M 1=3/ with the choice of r D
O.n.3!/=3M 1=3/. The value of M can be al-
most O.n0:624/ to keep the complexity within
sub-cubic.
Key Results
Zwick improved the Alon-Galil-Margalit algo-
rithm in several ways. The most notable is an im-
provement of the time for the APSP problem with
unit edge costs from O.n2:688/ to O.n2:575/. The
main accelerating engine in Alon-Galil-Margalit
[1] was the fast Boolean matrix multiplication
and that in Takaoka [10] was the fast distance ma-
trix multiplication by Romani, both powered by
the fast matrix multiplication of square matrices.
In this section, the engine is the fast distance
matrix multiplication by Romani powered by the
fast matrix multiplication of rectangular matrices
given by Coppersmith [3] and Huang and Pan
[5]. Suppose the product of .n; m/ matrix and
.m; n/ matrix can be computed with O.n!.1;;1//
arithmetic operations, where m D n with 0 
  1. Several facts such as O.n!.1;1;1// D
O.n2:376/ and O.n!.1;0:294;1// D
QO.n2/ are
known. To compute the product of .n; n/ square
matrices, n1 matrix multiplications are needed,
resulting in O.n!.1;;1/C1/ time, which is re-
formulated as O.n2C/, where  satisﬁes the
equation !.1; ; 1/ D 2 C 1. Also, the upper
bound of !.1; ; 1/ is given by
!.1; ; 1/ D 2; if 0    ˛
!.1; ; 1/ D 2 C .!  2/
.  ˛/=.1  ˛/; if ˛    1
The best known value for , when [12] was
published, was  D 0:575, derived from the
above formulae, ˛ > 0:294 and ! < 2:376.
So, the time becomes O.n2:575/, which is not as

58
All Pairs Shortest Paths via Matrix Multiplication
good as O.n2:376/. Thus, we use the algorithm
for rectangular matrices in the following.
The above algorithm for rectangular matrix
multiplication
is
incorporated
into
.n; m/-
Romani with m
D
n and M
D
nt, and
the computing time of
QO.Mn!.1;;1//. The
next step is how to incorporate .n; m/-Romani
into the APSP algorithm. The ﬁrst algorithm
is a monophase algorithm based on repeated
squaring, similar to the second phase of the
algorithm in [1]. To take advantage of rectangular
matrices
in
.n; m/-Romani,
the
following
deﬁnition of the bridging set is needed, which
plays the role of the set I in the partial distance
matrix product in section “Problem Deﬁnition.”
Let ı.i; j / be the shortest distance from i to j ,
and .i; j / be the minimum length of all shortest
paths from i to j . A subset I of V is an `-
bridging set if it satisﬁes the condition that if
.i; j /  `, there exists k 2 I such that ı.i; j / D
ı.i; k/ C ı.k; j /. I is a strong `-bridging set if it
satisﬁes the condition that if .i; j /  `, there
exists k 2 I such that ı.i; j / D ı.i; k/ C ı.k; j /
and .i; j / D .i; k/ C .k; j /. Note that those
two sets are the same for a graph with unit edge
costs.
Note that if .2=3/`  .i; j /  ` and I is
a strong `=3-bridging set, there is a k 2 I such
that ı.i; j / D ı.i; k/ C ı.k; j / and .i; j / D
.i; k/ C .k; j /. With this property of strong
bridging sets, .n; m/-Romani can be used for the
APSP problem in the following way. By repeated
squaring in a similar way to Alon-Galil-Margalit,
the algorithm computes D.`/ for ` D 1,
˙ 3
2
	
,
l
3
2
˙ 3
2
	m
; : : : ; n0, where n0 is the ﬁrst value of
` that exceeds n, using various types of set I
described below. To compute the bridging set, the
algorithm maintains the witness matrix with extra
polylog factor in the complexity. In [12], there are
three ways for selecting the set I. Let jIj D nr
for some r such that 0  r  1.
1. Select 9n ln n=` vertices for In from V at
random. In this case, it can be shown that
the algorithm solves the APSP problem with
high probability, i.e., with 1  1=nc for some
constant c
>
0, which can be shown to
be 3. In other words, I is a strong `=3-
bridging set with high probability. The time T
is dominated by .n; m/-Romani. It holds that
T
D
QO.`Mn!.1;r;1//, since the magnitude
of matrix elements can be up to `M. Since
m D O.n ln n=`/ D nr, it holds that ` D
QO.n1r/, and thus T D O.Mn1rn!.1;r;1//.
When M
D 1, this bound on r is  D
0:575, and thus T D O.n2:575/. When M D
nt  1, the time becomes O.n2C.t//, where
t  3  ! D 0:624 and  D .t/ satisﬁes
!.1; ; 1/ D 1C2t. It is determined from
the best known !.1; ; 1/ and the value of t.
As the result is correct with high probability,
this is a randomized algorithm.
2. Consider the case of unit edge costs here.
In (1), the computation of witnesses is an
extra thing, i.e., not necessary if only shortest
distances are needed. To achieve the same
complexity in the sense of an exact algorithm,
not a randomized one, the computation of
witnesses is essential. As mentioned earlier,
maintenance of witnesses, that is, matrix W ,
can be done with an extra polylog factor,
meaning the analysis can be focused on Ro-
mani within the
QO-notation. Speciﬁcally, I
is selected as an `=3-bridging set, which is
strong with unit edge costs. To compute I
as an O.`/-bridging set, obtain the vertices
on the shortest path from i to j for each i
and j using the witness matrix W in O.`/
time. After obtaining those n2 sets spending
O.`n2/ time, it is shown in [12] how to obtain
a O.`/-bridging set of O.n ln n=`/ size within
the same time complexity. The process of
obtaining the bridging set must stop at ` D
n1=2 as the process is too expensive beyond
this point, and thus, the same bridging set is
used beyond this point. The time before this
point is the same as that in (1) and that after
this point is QO.n2:5/. Thus, this is a two-phase
algorithm.
3. When edge costs are positive and bounded
by M D nt > 0, a similar procedure can
be used to compute an O.`/-bridging set of
O.n ln n=`/ size in
QO.`n2/ time. Using the
bridging set, the APSP problem can be solved
in QO.n2C.t// time in a similar way to (1).

All-Distances Sketches
59
A
The result can be generalized into the case
where edge costs are between M and M
within the same time complexity by modifying
the procedure for computing an `-bridging set,
provided there is no negative cycle. The details
are shown in [12].
Applications
The eccentricity of a vertex v of a graph is the
greatest distance from v to any other vertices. The
diameter of a graph is the greatest eccentricity of
any vertices. In other words, the diameter is the
greatest distance between any pair of vertices. If
the corresponding APSP problem is solved, the
maximum element of the resulting matrix is the
diameter.
Open Problems
Recently, LeGall [6] discovered an algorithm
for
multiplying
rectangular
matrices
with
!.1; 0:530; 1/ < 2:060, which gives the upper
bound  < 0:530. This improves the complexity
of APSP with unit edge costs from O.n2:575/
by Zwick to O.n2:530/ in the same framework
as that of Zwick in this article. Two major
challenges are stated here among others. The
ﬁrst is to improve the complexity of QO.n2:530/
for the APSP with unit edge costs.
The other is to improve the bound of M <
O.n0:624/ for the complexity of the APSP with
integer costs up to M to be sub-cubic.
Cross-References
▷All Pairs Shortest Paths in Sparse Graphs
Recommended Reading
1. Alon N, Galil Z, Margalit O (1991) On the exponent
of the all pairs shortest path problem. In: Proceedings
of the 32th IEEE FOCS, San Juan, pp 569–575. Also
JCSS 54 (1997), pp 255–262
2. Alon N, Galil Z, Margalit O, Naor M (1992) Wit-
nesses for Boolean matrix multiplication and for
shortest paths. In: Proceedings of the 33th IEEE
FOCS, Pittsburgh, pp 417–426
3. Coppersmith D (1997) Rectangular matrix multipli-
cation revisited. J Complex 13:42–49
4. Coppersmith D, Winograd S (1990) Matrix multipli-
cation via arithmetic progressions. J Symb Comput
9:251–280
5. Huang X, Pan VY (1998) Fast rectangular matrix
multiplications and applications. J Complex 14:257–
299
6. Le Gall F (2012) Faster algorithms for rectangular
matrix multiplication. In: Proceedings of the 53rd
FOCS, New Brunswick, pp 514–523
7. Romani F (1980) Shortest-path problem is not harder
than matrix multiplications. Inf Proc Lett 11:134–136
8. Schönhage A, Strassen V (1971) Schnelle Multiplika-
tion Großer Zahlen. Computing 7:281–292
9. Seidel R (1992) On the all-pairs-shortest-path prob-
lem. In: Proceedings of the 24th ACM STOC, Victo-
ria, pp 745–749. Also JCSS 51 (1995), pp 400–403
10. Takaoka T (1998) Sub-cubic time algorithms for the
all pairs shortest path problem. Algorithmica 20:309–
318
11. Williams VV (2012) Multiplying matrices faster than
Coppersmith-Winograd. In: Proceedings of the 44th
symposium on theory of computing, ACM STOC,
New York, pp 887–898
12. Zwick U (2002) All pairs shortest paths using bridg-
ing sets and rectangular matrix multiplication. J ACM
49(3):289–317
All-Distances Sketches
Edith Cohen
Tel Aviv University, Tel Aviv, Israel
Stanford University, Stanford, CA, USA
Keywords
Distance distribution; Distinct counting; Graph
analysis; Inﬂuence; Nodes similarity; Summary
structures
Years and Authors of Summarized
Original Work
1997, 2014; Cohen
2002; Palmer, Gibbons, Faloutsos
2004, 2007; Cohen, Kaplan

60
All-Distances Sketches
Problem Deﬁnition
All-distances sketches (The term least element
lists was used in [3]; the terms MV/D lists and
Neighborhood summaries were used in [6].) are
randomized summary structures of the distance
relations of nodes in a graph. The graph can
be directed or undirected, and edges can have
uniform or general nonnegative weights.
Preprocessing Cost
A set of sketches, ADS.v/ for each node v,
can be computed efﬁciently, using a near-linear
number of edge traversals. The sketch sizes are
well concentrated, with logarithmic dependence
on the total number of nodes.
Supported Queries
The sketches support approximate distance-based
queries, which include:
•
Distance distribution: The query speciﬁes a
node v and value d
 0 and returns the
cardinality jNd.v/j of the d-neighborhood of
v Nd.v/ D fu j dvu  dg, where duv
is the shortest path distance from u to v.
We are interested in estimating jNd.v/j from
ADS.v/.
A related property is the effective diameter
of the graph, which is a quantile of the dis-
tance distribution of all node pairs; we are in-
terested in computing an estimate efﬁciently.
•
Closeness
centrality
(distance-decaying)
is
deﬁned
for
a
node
v,
a
monotone
nonincreasing function ˛.x/

0 (where
˛.C1/ 	 0), and a nonnegative function
ˇ.u/  0:
C˛;ˇ.v/ D
X
u
˛.dvu/ˇ.u/ :
(1)
The function ˛ speciﬁes the decay of rele-
vance with distance and the function ˇ weighs
nodes based on metadata to focus on a topic or
property of relevance. Neighborhood cardinal-
ity is a special case obtained using ˇ 	 1 and
˛.x/ D 1 if x  d and ˛.x/ D 0 otherwise.
The number of reachable nodes from v is
obtained using ˛.x/ 	 1. Also studied were
exponential decay ˛.x/ D 2x with distance
[10], the (inverse) harmonic mean of distances
˛.x/ D 1=x [1, 14], and general decay func-
tions [4,6]. We would like to estimate C˛;ˇ.v/
from ADS.v/.
•
Closeness similarity [8] relates a pair of nodes
based on the similarity of their distance rela-
tions to all other nodes.
SIM
˛;ˇ .v; u/ D
P
j ˛.maxfdu;j ; dv;j g/ˇ.j /
P
j ˛.minfdu;j ; dv;j g/ˇ.j / :
(2)
We would like to estimate SIM˛;ˇ.v; u/ 2
Œ0; 1 from ADS.v/ and ADS.u/.
•
Timed inﬂuence of a seed set S of nodes
depends on the set of distances from S to other
nodes. Intuitively, when edge lengths model
transition times, the distance is the “elapsed
time” needed to reach the node from S. Inﬂu-
ence is higher when distances are shorter:
INF
˛;ˇ .S/ D
X
j
˛.min
i2S dij /ˇ.j / :
(3)
We would like to estimate INF˛;ˇ.S/ from the
sketches fADS.v/ j v 2 Sg.
•
Approximate distance oracles: For two nodes
v; u, use ADS.u/ and ADS.v/ to estimate du;v.
Key Results
We provide a precise deﬁnition of ADSs,
overview algorithms for scalable computation,
and ﬁnally discuss estimators.
Deﬁnition
The ADS of a node v, ADS.v/, is a set of node ID
and distance pairs .u; dvu/. The included nodes
are a sample of the nodes reachable from v.
The sampling is such that the inclusion proba-
bility of a node is inversely proportional to its
Dijkstra rank (nearest neighbor rank). That is, the
probability that the ith closest node is sampled is
proportional to 1=i.
The ADSs are deﬁned with respect to random
mappings/permutations r of the set of all nodes

All-Distances Sketches
61
A
and come in three ﬂavors: bottom-k, k-mins, and
k-partition. The integer parameter k determines
a tradeoff between the sketch size and computa-
tion time on one hand and the information and
estimate quality on the other. For simplicity, we
assume here that distances dvu are unique for dif-
ferent u (using tie breaking). We use the notation
˚<u.v/ for the set of nodes that are closer to
node v than node u and vu D 1 C j˚<u.v/j
for the Dijkstra rank of u with respect to v (u is
the vu closest node from v). For a set N and
a numeric function r W N , the function krth.N /
returns the kth smallest value in the range of r on
N . If jN j < k, then we deﬁne krth.N / to be the
supremum of the range of r.
A bottom-k ADS [3,7] is deﬁned with respect to
a single random permutation r. ADS.v/ includes
a node u if and only if the rank r.u/ is one of the
k smallest ranks among nodes that are at least as
close to v:
u 2 ADS.v/ ” r.u/ < krth.˚<u.v//:
(4)
A k-partition ADS (implicit in) [2] is deﬁned
with respect to a random partition BUCKET W
V ! Œk of the nodes to k subsets and a random
permutation r. ADS.v/ includes u if and only if
u has the smallest rank among nodes in its bucket
that are at least as close to v.
u 2 ADS.v/ ” r.u/
< min

r.h/ j BUCKET.h/ D BUCKET.u/
^ h 2 ˚<u.v/

:
A k-mins ADS [3, 15] is k bottom-1 ADSs, de-
ﬁned with respect to k independent permutations.
It is often convenient to specify the ranks
r.j / and (for k-partition ADSs) the bucket
BUCKET.j / using random hash functions, so they
are readily available from the node ID. The same
randomization is used for all nodes, which results
in the sketches being coordinated. This means
that a node sampled in one sketch is more likely
to be included in other sketches. Coordination is
an artifact of scalable computation of the sketches
but also facilitates more accurate similarity and
inﬂuence queries.
Relation to MINHASH Sketches
All-distances sketches are related to MINHASH
sketches: ADS.v/ is the union of the MINHASH
sketches of the neighborhoods Nd.v/, for all
possible values of d. We explain how a MIN-
HASH sketch of a neighborhood Nd.v/ can be
obtained from ADS.v/. From a k-mins ADS,
we obtain a k-mins MINHASH sketch of Nd.v/,
which includes for each of the k permutations r
the value x  minu2Nd .v/ r.u/. Note that x is the
minimum rank of a node of distance at most d in
the respective bottom-1 ADS deﬁned for r. The
k minimum rank values x.t/ t 2 Œk we obtain
from the different permutations are the k-mins
MINHASH sketch of Nd.v/. We now consider
obtaining a bottom-k MINHASH sketch of Nd.v/
from a bottom-k ADS.v/. The MINHASH sketch
of Nd.v/ includes the k nodes of minimum rank
in Nd.v/, which are also the k nodes of mini-
mum rank in ADS.v/ within distance at most d.
Finally, a k-partition MINHASH sketch of Nd.v/
is similarly obtained from a k-partition ADS by
taking, for each bucket i 2 Œk, the smallest rank
value of a node in bucket i that is in Nd.v/. This
is also the smallest value in ADS.v/ over nodes
in bucket i that have distance at most d from v.
Direction
For directed graphs, inﬂuence, centrality, and
closeness similarity queries can be deﬁned with
respect to either forward or reversed distances.
Accordingly, we can separately consider for each
node v the forward ADS and the backward ADS
of each node, which are deﬁned respectively
using forward or reverse paths from v.
Node Weights
All the ADS ﬂavors can be extended to be with
respect to speciﬁed node weights ˇ.v/  0 [3,4].
This makes queries formulated with respect to the
weights more efﬁcient.
Algorithms
There are two meta-approaches for scalable
computation of an ADS set. The ﬁrst approach,

62
All-Distances Sketches
PRUNEDDIJKSTRA
(Algorithm
1),
performs
pruned
applications
of
Dijkstra’s
single-
source shortest paths algorithm (BFS when
unweighted) [3, 7]. The second approach, DP
(implicit in [2, 15]), applies to unweighted
graphs and is based on dynamic programming
or Bellman-Ford shortest paths computation.
LOCALUPDATES (Algorithm 2) [4] extends DP
to weighted graphs. LOCALUPDATES is node-
centric and is appropriate for MapReduce or
similar platforms, but can incur more overhead
than PRUNEDDIJKSTRA. The algorithms are
presented
for
bottom-k
sketches,
but
both
approaches can be easily adopted to work with
all three ADS ﬂavors.
Algorithm 1: ADS set for G via PRUNED-
DIJKSTRA
1 for u by increasing r.u/ do
2
Perform Dijkstra from u on GT (the transpose
graph)
3
foreach scanned node v do
4
if jf.x; y/ 2 ADS.u/ j y < dvugj D k
then
5
prune Dijkstra at v
6
else
7
ADS.v/  ADS.v/[f.r.u/; dvu/g
Both PRUNEDDIJKSTRA and DP can be
performed in O.km log n/ time (on unweighted
graphs) on a single processor in main memory,
where n and m are the number of nodes and
edges in the graph. These algorithms maintain a
partial ADS for each node, as entries of node
ID and distance pairs. ADS.v/ is initialized
with the pair .v; 0/. The basic operation we
use is edge relaxation: when relaxing .v; u/,
ADS.v/ is updated using ADS.u/. For bottom-k,
the relaxation modiﬁes ADS.v/ when ADS.u/
contains a node i such that r.i/ is smaller than
the kth smallest rank among nodes in ADS.v/
with distance at most dui C wvu from v. Both
PRUNEDDIJKSTRA and DP perform relaxations
in an order which guarantees that inserted entries
are part of the ﬁnal ADS, that is, there are no
other nodes that are both closer and have lower
rank: PRUNEDDIJKSTRA iterates over all nodes
in increasing rank, runs Dijkstra’s algorithm from
the node on the transpose graph, and prunes at
nodes when the ADS is not updated. DP performs
iterations, where in each iteration, all edges
.v; u/; such that ADS.v/ was updated in the
previous step, are relaxed. Therefore, entries are
inserted by increasing distance.
Algorithm 2: ADS set for G via LOCALUP-
DATES
// Initialization
1 for u do
2
ADS.u/  f.r.u/; 0/g
// Propagate updates .r; d/ at node
u
3 if .r; d/ is added to ADS.u/ then
4
foreach y j .u; y/ 2 G do
5
send .r; d C w.u; y// to y
// Process update .r; d/ received at
u
6 if node u receives .r; d/ then
7
if r < kth
xf.x; y/ 2 ADS.u/ j y < dg then
8
ADS.u/  ADS.u/ [ f.r.v/; d/g
// Clean-up ADS.u/
9
for entries .x; y/ 2 ADS.u/ j y > d by
increasing y do
10
if x > kth
hf.h; ´/ 2 ADS.u/ j ´ < yg
then
11
ADS.u/  ADS.u/ n .x; y/
Estimation
Distance Distribution
Neighborhood cardinality queries for a node v,
and d

0 can be estimated with a small
relative error from ADS.v/. The generic estima-
tor extracts a MINHASH sketch of the neighbor-
hood Nd.v/ from ADS.v/ and applies a MIN-
HASH cardinality estimator to this sketch. This
approach was used in [3,7,15]. A nearly optimal
estimator, the Historic Inverse Probability (HIP)
estimator [4], has a factor 2 improvement in
variance by using all information in the ADS
instead of just the MINHASH sketch. HIP works
by considering for each entry .u; d/ in the sketch,
the HIP threshold probability, which is the prob-

All-Distances Sketches
63
A
ability, under randomly drawn rank for the node
u, but ﬁxing ranks of all other nodes, that the
entry is included in the sketch. The entry then
obtains an adjusted weight that is the inverse
of the HIP threshold probability. Neighborhood
cardinality can be estimated by the sum of the
adjusted weights of ADS entries that fall in the
neighborhood.
Closeness Centrality (Distance-Decaying)
C˛;ˇ.v/ can be estimated from ADS.v/ with a
small relative error when the set of ADSs is
computed with respect to ˇ. Estimators using
MINHASH sketches were given in [6]. The tighter
HIP estimator in [4] simply sums, over entries
.u; d/ 2 ADS.v/, the product of the adjusted
weight of the entry and ˛.d/ˇ.u/.
Closeness Similarity and Inﬂuence
When the sketches are computed with respect to
ˇ, the closeness similarity of two nodes u and
v can be estimated from ADS.u/ and ADS.v/
within a small additive  [8]. The inﬂuence of a
set of nodes S can be estimated from fADS.v/ j
v 2 Sg to within a small relative error [9]. These
estimators are instances of the L estimator [5]
applied with the HIP inclusion probabilities [4].
Approximate Distance Oracles
An upper bound on the distance of two nodes u; v
can be computed from ADS.u/ and ADS.v/ [8].
This is done by looking at the minimum, over
nodes h that are in the intersection ADS.u/ \
ADS.v/, of duh C dhv. When the graph is undi-
rected, the oracle has worst-case quality guaran-
tees that match the distance oracle of [16] (oracle
time can be improved by looking only at nodes in
the few ADS entries that correspond to k D 1).
We note that observed quality in practice (using
the full oracle) tends to have a small relative
error [8].
Applications
Massive graphs, with billions of edges, are preva-
lent and model web graphs and social networks.
Centralities, similarities, inﬂuence, and distances
are basic data analysis tasks on these graphs.
ADSs are a powerful tool for scalable analysis of
very large graphs.
Extensions
An ADS can be viewed as a MINHASH sketch
constructed from a stream, where all updates are
recorded. This means that the HIP estimator [4]
can be applied for distinct counting on streams,
obtaining improved performance over estimators
applied to the MINHASH sketch alone [11,12].
In a graph context, ADS.v/ is a recording of
all updates to a MINHASH sketch obtained by
sweeping through nodes in increasing distance
from v. More generally, we can construct ADS
for other settings and apply the same estimation
machinery. One example is Euclidean distances
[6, 13]. Another example is constructing a com-
bined ADS of multiple graphs for the application
of timed inﬂuence oracles [9].
Cross-References
▷All-Distances Sketches can be viewed as an
extension of ▷Min-Hash Sketches. They are also
▷Coordinated Sampling.
Recommended Reading
1. Boldi P, Vigna S (2014) Axioms for centrality. Inter-
net Math 10:222–262
2. Boldi P, Rosa M, Vigna S (2011) HyperANF: ap-
proximating the neighbourhood function of very large
graphs on a budget. In: WWW, Hyderabad
3. Cohen E (1997) Size-estimation framework with ap-
plications to transitive closure and reachability. J
Comput Syst Sci 55:441–453
4. Cohen E (2014) All-distances sketches, revisited: HIP
estimators for massive graphs analysis. In: PODS.
ACM. http://arxiv.org/abs/1306.3284
5. Cohen E (2014) Estimation for monotone sam-
pling: competitiveness and customization. In: PODC.
ACM. http://arxiv.org/abs/1212.0243, full version
http://arxiv.org/abs/1212.0243
6. Cohen E, Kaplan H (2007) Spatially-decaying ag-
gregation over a network: model and algorithms.
J Comput Syst Sci 73:265–288. Full version of a
SIGMOD 2004 paper
7. Cohen E, Kaplan H (2007) Summarizing data using
bottom-k sketches. In: PODC, Portland. ACM

64
Alternate Parameterizations
8. Cohen E, Delling D, Fuchs F, Goldberg A, Gold-
szmidt M, Werneck R (2013) Scalable similarity
estimation in social networks: closeness, node labels,
and random edge lengths. In: COSN, Boston. ACM
9. Cohen E, Delling D, Pajor T, Werneck RF (2014)
Timed inﬂuence: computation and maximization.
Manuscript. ArXiv 1410.6976
10. Dangalchev C (2006) Residual closeness in networks.
Phisica A 365:556–564
11. Flajolet P, Martin GN (1985) Probabilistic counting
algorithms for data base applications. J Comput Syst
Sci 31:182–209
12. Flajolet P, Fusy E, Gandouet O, Meunier F (2007)
Hyperloglog: the analysis of a near-optimal cardinal-
ity estimation algorithm. In: Analysis of algorithms
(AOFA), Juan des Pins
13. Guibas LJ, Knuth DE, Sharir M (1992) Randomized
incremental construction of Delaunay and Voronoi
diagrams. Algorithmica 7:381–413
14. Opsahl T, Agneessens F, Skvoretz J (2010) Node
centrality
in
weighted
networks:
generalizing
degree
and
shortest
paths.
Soc
Netw
32.
http://toreopsahl.com/2010/03/20/
15. Palmer CR, Gibbons PB, Faloutsos C (2002) ANF:
a fast and scalable tool for data mining in massive
graphs. In: KDD, Edmonton
16. Thorup M, Zwick U (2001) Approximate dis-
tance oracles. In: Proceedings of the 33th annual
ACM symposium on theory of computing, Crete,
pp 183–192
Alternate Parameterizations
Neeldhara Misra
Department of Computer Science and
Automation, Indian Institute of Science,
Bangalore, India
Keywords
Above
guarantee;
Complexity
ecology
of
parameters; Dual parameters; Structural parame-
terization
Years and Authors of Summarized
Original Work
2013; Fellows, Jansen, Rosamond
2014;
Lokshtanov,
Narayanaswamy,
Raman,
Ramanujan, Saurabh
2014; Marx, Pilipczuk
Problem Deﬁnition
A parameterized problem is a language L 
˙  N. Such a problem is said to be ﬁxed-
parameter tractable if there is an algorithm that
decides if .x; k/
2
L in time f .k/jXjO.1/.
For attacking an intractable problem within the
multivariate algorithmic framework, a necessary
ﬁrst step is to identify some reasonable param-
eters. The relevance of an FPT algorithm will
depend on the quality of the choice of parameters.
The ﬁrst objective is of a practical concern: the
choice of parameter should not “cheat,” that is,
it should be a choice that leads to tractability in
the context of instances that are relevant to real-
world applications. On the other hand, the param-
eter should also lend a perspective that is useful
to the algorithm designer, usually by provid-
ing additional structural insights, thereby making
an otherwise unwieldy problem manageable. Fi-
nally, the parameter itself should be accessible,
in the sense that it should either typically accom-
pany the input or be easy to compute from the
input.
For a combinatorial optimization problem, the
size of the desired solution is a natural param-
eter. For a minimization problem, it is usually
reasonable to assume that this parameter is also
small in practice. For a maximization problem,
the dual parameter, which is the difference from
the best possible upper bound on the optimum,
is also a natural choice. For example, consider
the problem of satisfying at least k clauses of
a CNF formula. Here, the standard parameter
would be k, while the dual parameter would be
.m  k/: in other words, can we satisfy all but k
clauses in the formula? In the rest of this section,
we broadly describe the other possibilities for
parameters.
Structural Parameterizations
Structural parameters are a considered attempt at
acknowledging that various aspects of an instance
inﬂuence its complexity. A classic example is
ML-type checking [11], which is an NP-complete
problem but can be resolved in time O.2k/nO.1/,
where k is the maximum nesting depth of the

Alternate Parameterizations
65
A
input program. Fortunately, nesting depths of
most programs are no more than four or ﬁve; the
algorithm proposed is entirely adequate for real-
world instances.
Since every problem context is inherently
suggestive of several possible parameters, we
are only able to describe a few illustrative
examples. In the context of graph problems,
width parameters such as treewidth, cliquewidth,
and rankwidth have enjoyed immense success.
The notion of treewidth is particularly popular
because of a number of real-world instances
that are known to exhibit small treewidth, and
on the other hand, the theoretical foundations
of algorithms on graphs of bounded treewidth
are extremely well established and actively
developed
(see,
e.g.,
[3,
Chapter
7]).
An
analogous notion for treewidth for directed
graphs
remains
elusive,
although
several
proposals with varying merits exist in the
literature [7].
Special graph classes, such as interval graphs,
chordal graphs, planar graphs, and so on,
have been extensively studied, and most hard
problems turn out to be tractable on these
classes. For an arbitrary graph, one might hope
that the tractability carries over if the graph
is “close enough” to being, say, a chordal
graph. An increasingly popular program involves
considering
distance-to-C
parameterizations,
where C is a class of graphs on which the
problem of interest is easily solvable. For
instance, we might let k be the size of a smallest
subset of vertices whose removal makes the
input graph a member of C. Other measures
of closeness, using operations like addition,
removal, or contraction of edges, are also
frequently considered.
In the context of satisﬁability and constraint
satisfaction also, the notion of distance from
tractable subclasses has garnered much attention
in recent times. This is formalized by the notion
of backdoors, which are subsets of variables
whose “removal” makes the formula tractable
(for instance, one of the Schaefer classes). Much
work has been done with backdoors as parame-
ters for determining satisﬁability, and we refer the
reader to [9].
Above or Below Guarantee
Parameterizations
Consider the standard parameterization of VER-
TEX COVER: given a graph G D .V; E/ on n
vertices, decide whether G has a vertex cover
of size at most k. The best-known algorithm for
vertex cover is due to Chen et al. [1] and runs
in time O.1:2852k C kn/. Observe that if G
has a matching of size , then any vertex cover
also has size at least . In particular, if G has a
perfect matching, then all vertex covers of G have
˝.n/ edges, and even the FPT algorithms for the
standard parameter will be obliged to spend time
that is exponential in n.
Mahajan and Raman [13] consider the follow-
ing alternative parameterization: does G have a
vertex cover of size at most  C k? Note that the
parameter k here is the size of the vertex cover
above the matching size. Since the matching size
is a guaranteed lower bound on the vertex cover
size, this problem is referred to as the ABOVE
GUARANTEE VERTEX COVER problem. Just as
one can parameterize above guaranteed values,
one can consider parameterizations below guar-
anteed values. A classic example is the follow-
ing variant of VERTEX COVER: given a planar
graph G D .V; E/ on n vertices and an integer
parameter k, does G admit a vertex cover of size
b3n=4c  k?
Key Results
One of the earliest attempts at parameterizing by
the size of the vertex cover was made in [4].
Various graph layout problems were considered,
where it turned out that a small vertex cover led
to a very convenient structure for formulating
a linear program. For many of these problems,
it is not known if they are FPT parameterized
by treewidth, and some are hard even on graphs
of bounded treewidth (indeed, BANDWIDTH is
NP-hard even on trees). This justiﬁes the need
for a stronger structural parameter, and in these
examples, vertex cover turned out to be a very
fruitful parameterization.
These examples led to a broader theme,
namely, the “complexity ecology of parameters”

66
Alternate Parameterizations
program, which was proposed in [5]. The
theoretical foundations of this program were
further established and surveyed in [6]. An
immediate concern is that of how one formalizes
the structural parameterization in question. Along
the lines of [6], we distinguish the following
possible objectives from the formalization:
1. The complexity of verifying and then exploit-
ing a bounded parameter value
2. The complexity of exploiting structure that
is guaranteed, but not given explicitly (a
“promise” problem)
3. The complexity of exploiting structure that is
explicitly provided along with the input, as a
“witness”
The ﬁrst setting is the most general but also
the most computationally restrictive, as it puts
the burden of discovering the structure also on
the algorithm. This deﬁnition makes the study
of parameters like bandwidth and cliquewidth
prohibitive, as these are hard to determine even in
the parameterized framework. On the other hand,
while the other two notions are increasingly re-
laxed, the premise of a promise or the availability
of witnesses in real-world witnesses remains a
concern. We point the reader to [6] for a detailed
discussion of the precise formalisms and their
respective merits and trade-offs.
One of the major theoretical themes with al-
ternate parameterizations is the exercise of iden-
tifying meta-theorems that explain the inﬂuence
of the parameter over a large class of prob-
lems, usually speciﬁed in an appropriate logic.
A cornerstone result of this kind is Courcelle’s
theorem, establishing that a problem express-
ible in Monadic Second Order Logic is FPT
when parameterized by treewidth and the size
of the formula [2]. Several generalizations of
Courcelle’s theorem have since been proposed,
and many of them are surveyed in [10]. More
recent work also establishes a similar result in
the context of kernelization and parameters like
vertex cover [8].
There is a rich literature that evidences the
growing consideration of alternate parameter-
izations for optimization problems in varied
contexts. As a concluding example, we turn
our attention to [14], which serves to illustrate
the scale at which it is possible to execute
an exercise in understanding a question from
several perspectives. Given two graphs H and
G, the SUBGRAPH
ISOMORPHISM
problem
asks if H is isomorphic to a subgraph of G.
In [14], a framework is developed involving
ten relevant parameters for each of H and G
(such as treewidth, maximum degree, number of
components, and so on). The generic question
addressed in this work is if the problem admits
an algorithm with running time:
f1.p1; p2; : : : ; p`/  nf2.p`C1;:::;pk/;
where each of p1; : : : ; pk is one of the ten pa-
rameters depending only on H or G. We refer
the reader to Figure 1 in [14] for a concise tab-
ulation of the results. Notably, all combinations
of questions (the number of which runs into the
billions) are answered by a set of 28 of positive
and negative results.
There are many examples of problems that
are parameterized away from guaranteed bounds.
We note that parameterizing vertex cover above
the LP optimum has attracted considerable inter-
est because a number of fundamental problems
including Above Guarantee Vertex Cover, Odd
Cycle Transversal, Split Vertex Deletion, and
Almost 2-SAT reduce to this problem. Indeed, for
many of these problems, the fastest algorithms at
the time of this writing are obtained by reducing
these problems to vertex cover parameterized
above the LP optimum [12].
Open Problems
We direct the reader to the excellent survey [6] for
several open problems concerning speciﬁc com-
binations of parameters for particular problems.
In an applied context, an interesting possibility is
to investigate if parameters can be learned from
large samples of data.

Alternative Performance Measures in Online Algorithms
67
A
Cross-References
▷Kernelization, Constraint Satisfaction Prob-
lems Parameterized above Average
▷Kernelization, Max-Cut Above Tight Bounds
▷Kernelization, MaxLin Above Average
▷Kernelization, Permutation CSPs Parameter-
ized above Average
▷LP Based Parameterized Algorithms
▷Parameterization
in
Computational
Social
Choice
▷Parameterized SAT
▷Treewidth of Graphs
Recommended Reading
1. Chen J, Kanj IA, Jia W (2001) Vertex cover: further
observations and further improvements. J Algorithms
41(2):280–301
2. Courcelle B (1990) The monadic second-order logic
of graphs I: recognizable sets of ﬁnite graphs. Inf
Comput 85:12–75
3. Cygan M, Fomin FV, Kowalik L, Lokshtanov D,
Marx D, Pilipczuk M, Pilipczuk M, Saurabh S (2015)
Parameterized algorithms. Springer, Cham. http://
www.springer.com/us/book/9783319212746
4. Fellows MR, Lokshtanov D, Misra N, Rosamond FA,
Saurabh S (2008) Graph layout problems parameter-
ized by vertex cover. In: 19th international sympo-
sium on algorithms and computation (ISAAC). Lec-
ture notes in computer science, vol 5369. Springer,
Berlin, pp 294–305
5. Fellows M, Lokshtanov D, Misra N, Mnich M, Rosa-
mond F, Saurabh S (2009) The complexity ecol-
ogy of parameters: an illustration using bounded
max leaf number. ACM Trans Comput Syst 45:822–
848
6. Fellows MR, Jansen BMP, Rosamond FA (2013)
Towards fully multivariate algorithmics: parameter
ecology and the deconstruction of computational
complexity. Eur J Comb 34(3):541–566
7. Ganian R, Hlinený P, Kneis J, Meister D, Obdrzálek
J, Rossmanith P, Sikdar S (2010) Are there any
good digraph width measures? In: IPEC, vol 6478.
Springer, Berlin, pp 135–146
8. Ganian R, Slivovsky F, Szeider S (2013) Meta-
kernelization with structural parameters. In: 38th in-
ternational symposium on mathematical foundations
of computer science, MFCS 2013. Lecture notes in
computer science, vol 8087. Springer, Heidelberg,
pp 457–468
9. Gaspers S, Szeider S (2012) Backdoors to satisfac-
tion. In: Bodlaender HL, Downey R, Fomin FV, Marx
D (eds) The multivariate algorithmic revolution and
beyond. Lecture notes in computer science, vol 7370.
Springer, Berlin/Heidelberg, pp 287–317
10. Grohe M, Kreutzer S (2011) Methods for algo-
rithmic meta theorems. In: Grohe M, Makowsky J
(eds) Model theoretic methods in ﬁnite combina-
torics. Contemporary mathematics, vol 558. Ameri-
can Mathematical Society, Providence, pp 181–206
11. Henglein F, Mairson HG (1991) The complexity of
type inference for higher-order typed lambda calculi.
J Funct Program 4:119–130
12. Lokshtanov D, Narayanaswamy NS, Raman V, Ra-
manujan MS, Saurabh S (2014) Faster parameterized
algorithms using linear programming. ACM Trans
Algorithms 11(2):15:1–15:31
13. Mahajan M, Raman V (1999) Parameterizing above
guaranteed values: maxsat and maxcut. J Algorithms
31(2):335–354
14. Marx D, Pilipczuk M (2014) Everything you always
wanted to know about the parameterized complexity
of subgraph isomorphism (but were afraid to ask). In:
31st international symposium on theoretical aspects
of computer science (STACS), Lyon, pp 542–553
Alternative Performance Measures
in Online Algorithms
Alejandro López-Ortiz
David R. Cheriton School of Computer Science,
University of Waterloo, Waterloo, ON, Canada
Keywords
Bijective
analysis; Diffuse
adversary; Loose
competitiveness;
Relative
interval
analysis;
Relative worst-order ratio; Smoothed analysis
Years and Authors of Summarized
Original Work
2000; Koutsoupias, Papadimitriou
2005; Dorrigiv, López-Ortiz
Problem Deﬁnition
While the competitive ratio [19] is the most
common metric in online algorithm analysis and
it has led to a vast amount of knowledge in the
ﬁeld, there are numerous known applications in

68
Alternative Performance Measures in Online Algorithms
which the competitive ratio produces unsatisfac-
tory results. Far too often, it leads to unrealisti-
cally pessimistic measures including the failure to
distinguish between algorithms that have vastly
differing performance under any practical char-
acterization in practice. Because of this there,
has been extensive research in alternatives to the
competitive ratio, with a renewed effort in the
period from 2005 to the present date.
The competitive ratio metric can be derived
from the observation that an online algorithm, in
essence, computes a partial solution to a prob-
lem using incomplete information. Then, it is
only natural to quantify the performance drop
due to this absence of information. That is, we
compare the quality of the solution obtained by
the online algorithm with the one computed in
the presence of full information, namely, that of
the ofﬂine optimal OPT, in the worst case. More
formally,
Deﬁnition 1 An online algorithm A is said to
have (asymptotic) competitive ratio c if A./ 
c  OPT./Cb for all input sequences  and ﬁxed
constants b and c.
The early literature considered only algorithms
with constant competitive ratio, and all others
are termed as algorithms with unbounded com-
petitive ratio. However, it is easy to extend this
deﬁnition to a C.n/-competitive algorithm as
follows:
Deﬁnition 2 An online algorithm A is said
to have (asymptotic) competitive ratio C.n/ if
A./  C.n/  OPT./ C b for all  and a ﬁxed
constant b. When b D 0, C.n/ is termed the
absolute competitive ratio.
A natural expectation would be that the per-
formance of OPT reﬂects both knowledge of the
future and the inherent structure of the speciﬁc
instance being solved, and hence, an online algo-
rithm with optimal competitive ratio must handle
most if not all instances in an efﬁcient manner.
Unfortunately, for most problems, the worst-case
nature of the competitive ratio leads to algorithms
of varying degrees of sophistication having the
same equally bad competitive ratio. As a con-
sequence the competitive ratio leads to “equiva-
lence” for online algorithms with vastly differing
performance in practice.
In the next sections we discuss the main al-
ternatives to and reﬁnements of the competitive
ratio and highlight their relative beneﬁts and
drawbacks.
Key Results
Relative Worst-Order Ratio
The relative worst-order ratio [8, 10, 11] com-
bines some desirable properties of two earlier
measures, namely, the max/max ratio [6] and
the random order ratio [15]. Using this measure
we can directly compare two online algorithms.
Informally, for a given sequence, it considers the
worst-case ordering of that sequence for each
algorithm and compares their behavior as a ratio
on these orderings. Then it ﬁnds among all se-
quences (not just reorderings) the one that max-
imizes the ratio above in the worst-case perfor-
mance.
Let A and B be online algorithms for an
online minimization problem and let A.I/
be the cost of A on an input sequence I
D
.i1; i2; : : : ; in/. Denote by I
the sequence
obtained by applying a permutation  to I,
i.e., I
D
.i1; : : : ; in/. Deﬁne AW .I/
D
min A.I/.
Deﬁnition 3 ([11]) Let S1.c/ and S2.c/ be the
statements about algorithms A and B deﬁned in
the following way:
S1.c/ W There exists a constant b such that
AW .I/  c  BW .I/ C b for all I.
S2.c/ W There exists a constant b such that
AW .I/  c  BW .I/  b for all I.
The relative worst-order ratio WRA;B of an on-
line algorithm A to algorithm B is deﬁned if
S1.1/ or S2.1/ holds. In this case A and B
are said to be comparable. If S1.1/ holds, then
WRA;B D supfrjS2.r/g, and if S2.r/ holds, then
WRA;B D inffrjS1.r/g:
WRA;B can be used to compare the qualities
of A and B. If WRA;B D 1, then these two

Alternative Performance Measures in Online Algorithms
69
A
algorithms have the same quality with respect
to this measure. The magnitude of difference
between WRA;B and 1 reﬂects the difference
between the behavior of the two algorithms. For
a minimization problem, A is better than B with
respect to this measure if WRA;B
<
1 and
vice versa. Boyar and Favrholdt showed that the
relative worst-order ratio is transitive [8].
Note that we can also compare the online
algorithm A to an optimal ofﬂine algorithm OPT.
The worst-order ratio of A is deﬁned as WRA D
WRA;OPT. For some problems, OPT is the same
for all order of requests on a given input se-
quence, and hence, the worst-order ratio is the
same as the competitive ratio. However, for other
problems such as paging the order does matter for
OPT.
In [10], three online algorithms (FIRST-FIT,
BEST-FIT, and WORST-FIT) for two variants of
the seat reservation problem [9] are compared
using the relative worst-order ratio. The relative
worst-order ratio when applied to paging algo-
rithms can be used to differentiate LRU which
is strictly better than FWF with respect to the
worst-order ratio, while they have the same com-
petitive ratio [11]. Similarly, [11] proposes a new
paging algorithm, retrospective LRU (RLRU),
and shows that it is better than LRU under this
measure while not under the competitive ratio.
Loose Competitiveness
Loose competitiveness was ﬁrst proposed in [22]
and later modiﬁed in [25]. It attempts to obtain
a more realistic measure by observing that ﬁrst,
in many real online problems, we can ignore
those input sequences on which the online al-
gorithm incurs a cost less than a certain thresh-
old and, second, many online problems have a
second resource parameter (e.g., size of cache,
number of servers) and the input sequences are
independent of these parameters. In contrast, in
competitive analysis, the adversary can select
sequences tailored against those parameters. For
example, for caching the worst-case input with
competitive ratio k can only be constructed by
the adversary if it is aware of the size k of
the cache. However, in practice the competitive
ratios of many online paging algorithms have
been observed to be constant [25], i.e., indepen-
dent of k.
In loose competitiveness we consider an ad-
versary that is oblivious to the parameter by
requiring it to give a sequence that is bad for most
values of the parameter rather than just a speciﬁc
bad value of the parameter. Let Ak.I/ denote the
cost of an algorithm A on an input sequence I,
when the parameter of the problem is k.
Deﬁnition 4 ([25]) An algorithm A is .; ı/-
loosely c-competitive if, for any input sequence
I and for any n, at least .1  ı/ n of the
values k
2
f1; 2; : : : ; ng satisfy Ak.I/

maxfc  OPTk.I/;  jIjg:
Therefore, we ignore the input sequences I which
cost less than  jIj. Also we require the algorithm
to be good for at least .1  ı/ fraction of the
possible parameters. For each online problem, we
can select the appropriate constants  and ı. The
following result shows that by this modiﬁcation
of the competitive analysis, we can obtain paging
algorithms with constant performance ratios.
Theorem 1 ([25]) Every k-competitive paging
algorithm is .; ı/-loosely c-competitive for any
0 < ; ı < 1, and c D .e=ı/ ln.e=/, where e is
the base of the natural logarithm.
Diffuse Adversary Model
The diffuse adversary model [16] tries to reﬁne
the competitive ratio by restricting the set of legal
input sequences. In the diffuse adversary model,
the input is generated according to a distribution
belonging to a member of a class  of distribu-
tions.
Deﬁnition 5 Let A be an online algorithm for
a minimization problem and let  be a class of
distributions for the input sequences. Then A is
c-competitive against , if there exists a constant
b, such that EI2DA.I/  cEI2DOPT.I/Cb; for
every distribution D 2 , where A.I/ denotes
the cost of A on the input sequence I and the
expectations are taken over sequences that are
picked according to D.
In other words, for a given algorithm A, the
adversary selects the distribution D in  that

70
Alternative Performance Measures in Online Algorithms
leads to its worst-case performance in that family.
If  is highly restrictive, then A knows more
about the distribution of input sequences and the
power of adversary is more constrained. When
 contains all possible distributions, then the
competitive analysis against  is the same as the
standard competitive ratio.
Computing the actual competitive ratio of both
deterministic and randomized paging algorithms
against  is studied in [23, 24]. An estimation
of the optimal competitive ratio for several algo-
rithms (such as LRU and FIFO) within a factor
of 2 is given. Also it is observed that around
the threshold  
 1=k, the best competitive
ratios against  are 	.ln k/. The competitive
ratios rapidly become constant for values of 
less than the threshold. For  D !.1=k/, i.e.,
values greater than the threshold, the competitive
ratio rapidly tends to 	.k/ for deterministic al-
gorithms while it remains unchanged for random-
ized algorithms.
Note that we can also model locality of refer-
ence using the diffuse adversary model by consid-
ering only those distributions that are consistent
with distributions obeying a locality of reference
principle. In particular Dorrigiv et al. showed
that for the list update problem MTF is optimal
in expected cost under any probability distribu-
tion that has locality of reference monotonicity,
i.e., a recently accessed item has equal or larger
probability of being accessed than a less recently
accessed item [14].
Bijective Analysis
Bijective analysis and average analysis [3] build
upon the framework of locality of reference by
[1]. These models directly compare two online
algorithms without appealing to the concept of
the ofﬂine “optimal” cost. In addition, these mea-
sures do not evaluate the performance of the
algorithm on a single “worst-case” request, but
instead use the cost that the algorithm incurs
on each and all request sequences. Informally,
bijective analysis aims to pair input sequences
for two algorithms A and B using a bijection
in such a way that the cost of A on input 
is no more than the cost of B on the image
of , for all request sequences  of the same
length. In this case, intuitively, A is no worse
than B. On the other hand, average analysis
compares the average cost of the two algorithms
over all request sequences of the same length.
For an online algorithm A and an input sequence
, let A./ be the cost incurred by A on .
Denote by In the set of all input sequences of
length n.
We say that an online algorithm A is no worse
than an online algorithm B according to bijective
analysis if there exists an integer n0  1 so that
for each n  n0, there is a bijection b W In $ In
satisfying A./  B.b.// for each  2 In. We
denote this by A b B.
We say that an online algorithm A is no worse
than an online algorithm B according to average
analysis if there exists an integer n0  1 so that
for each n  n0, P
I2In A.I/  P
I2In B.I/.
We denote this by A a B.
Under both bijective analysis and average
analysis alone, all lazy algorithms (including
LRU and FIFO, but not FWF) are in fact strongly
equivalent. This is evidence of an inherent
difﬁculty to separate these algorithms in any
general unrestricted setting. Their superiority
is seemingly derived from the well-known
observation that input sequences for paging
and several other problems show locality of
reference [12,13]. This means that when a page is
requested, it is more likely to be requested in the
near future. Therefore, several models for paging
with locality of reference have been proposed.
Hence, the need to combine bijective analysis
with an assumption of locality of reference model
such as concave analysis. In this model a request
sequence has high locality of reference if the
number of distinct pages in a window of size n
is small.
Using this measure Angelopoulos et al. [3]
show that LRU is never outperformed in any pos-
sible subpartition on the request sequence space
induced by concave analysis, while it always
outperforms any other paging algorithm in at
least one subpartition of the sequence space. This
result proves separation between LRU and all
other algorithms and provides theoretical back-
ing to the observation that LRU is preferable in
practice. This is the ﬁrst deterministic theoretical

Alternative Performance Measures in Online Algorithms
71
A
model to provide full separation between LRU
and all other algorithms. Recently this result was
strengthened by Angelopolous and Schweitzer
[2] where they showed that the separation also
holds under the stricter bijective analysis (as
opposed to average analysis) using the concave
analysis framework.
Smoothed Competitiveness
Some algorithms that have very bad worst-case
performance behave very well in practice. One of
the most famous examples is the simplex method.
This algorithm has a very good performance in
practice but it has exponential worst-case running
time. Average case analysis of algorithms can
somehow explain this behavior, but sometimes
there is no basis to the assumption that the inputs
to an algorithm are random.
Smoothed analysis of algorithms [21] tries to
explain this intriguing behavior without assum-
ing anything about the distribution of the input
instances. In this model, we randomly perturb
(smoothen) the input instances according to a
probability distribution f and then analyze the
behavior of the algorithm on these perturbed
(smoothed) instances. For each input instance LI,
we compute the neighborhood N. LI/ of LI which
contains the set of all perturbed instances that
can be obtained from LI. Then we compute the
expected running time of the algorithm over all
perturbed instances in this neighborhood. The
smoothed complexity of the algorithm is the
maximum of this expected running time over all
the input instances. Intuitively, an algorithm with
a bad worst-case performance can have a good
smoothed performance if its worst-case instances
are isolated. Spielman and Teng show [21] that
the simplex algorithm has polynomial smoothed
complexity. Several other results are known about
the smoothed complexity of the algorithms [4, 7,
18,20].
Becchetti et al. [5] introduced smoothed
competitive analysis which mirrors competitive
analysis
except
that
we
consider
the
cost
of
the
algorithm
on
randomly
perturbed
adversarial sequences. As in the analysis of
the
randomized
online
algorithms,
we
can
have either an
oblivious adversary or an
adaptive adversary. The smoothed competitive
ratio of an online algorithm A for a mini-
mization problem can be formally deﬁned as
follows.
Deﬁnition 6 ([5]) The smoothed competitive ra-
tio of an algorithm A is deﬁned as
c D sup
LI
EI N. LI/

A.I/
OP T .I/

;
where the supremum is taken over all input in-
stances LI and the expectation is taken over all
instances I that are obtainable by smoothening
the input instance LI according to f in the neigh-
borhood N. LI/.
In [5], they use the smoothed competitive ratio
to analyze the MULTI-LEVEL FEEDBACK(MLF)
algorithm for processor scheduling in a time-
sharing multitasking operating system. This al-
gorithm has very good practical performance,
but its competitive ratio is very bad and obtains
strictly better ratios using the smooth competitive
analysis than with the competitive ratio.
Search Ratio
The search ratio belongs to the family of mea-
sures in which the ofﬂine OPT is weakened. It is
deﬁned only for the speciﬁc case of geometric
searches in an unknown terrain for a target of
unknown position. Recall that the competitive
ratio compares against an all-knowing OPT; in-
deed, for geometric searches in the competitive
ratio framework, the OPT is simply a shortest
path algorithm, while the online search algorithm
has intricate methods for searching. The search
ratio instead considers the case where OPT knows
the terrain but not the position of the target.
That is, the search ratio compares two search
algorithms, albeit one more powerful than the
other. By comparing two instances of like objects,
the search ratio can be argued to be a more mean-
ingful measure of the quality of an online search
algorithm. Koutsopias et al. show that searching
in trees results the same large competitive ratio
regardless of the search strategy, yet under the
search ratio framework, certain algorithms are far
superior to others [17].

72
Amortized Analysis on Enumeration Algorithms
Cross-References
▷Online Paging and Caching
Recommended Reading
1. Albers S, Favrholdt LM, Giel O (2005) On paging
with locality of reference. JCSS 70(2): 145–175
2. Angelopoulos S, Schweitzer P (2009) Paging and list
update under bijective analysis. In: Proceedings of the
twentieth annual ACM-SIAM symposium on discrete
algorithms (SODA 2009), New York, 4–6 Jan 2009,
pp 1136–1145
3. Angelopoulos S, Dorrigiv R, López-Ortiz A (2007)
On the separation and equivalence of paging strate-
gies. In: Proceedings of the eighteenth annual ACM-
SIAM symposium on discrete algorithms (SODA
2007), New Orleans, 7–9 Jan 2007, pp 229–237
4. Banderier C, Mehlhorn K, Beier R (2003) Smoothed
analysis of three combinatorial problems. In: Pro-
ceedings of the 28th international symposium on
mathematical foundations of computer science 2003
(MFCS 2003), Bratislava, 25–29 Aug 2003, vol 2747,
pp 198–207
5. Becchetti L, Leonardi S, Marchetti-Spaccamela A,
Schafer G, Vredeveld T (2003) Average case and
smoothed competitive analysis of the multi-level
feedback algorithm. In: IEEE (ed) Proceedings of the
44th symposium on foundations of computer science
(FOCS 2003), Cambridge, 11–14 Oct 2003, pp 462–
471
6. Ben-David S, Borodin A (1994) A new measure for
the study of on-line algorithms. Algorithmica 11:73–
91
7. Blum A, Dunagan J (2002) Smoothed analysis of
the perceptron algorithm for linear programming.
In: Proceedings of the thirteenth annual ACM-SIAM
symposium on discrete algorithms, San Francisco,
6–8 Jan 2002, pp 905–914
8. Boyar J, Favrholdt LM (2003) The relative worst or-
der ratio for on-line algorithms. In: Proceedings of the
5th Italian conference on algorithms and complexity
(CIAC 2003), Rome, 28–30 May 2003,
9. Boyar J, Larsen KS (1999) The seat reservation
problem. Algorithmica 25(4):403–417
10. Boyar J, Medvedev P (2004) The relative worst order
ratio applied to seat reservation. In: Proceedings of
the 9th Scandinavian workshop on algorithm theory
(SWAT 2004), Humlebaek, 8–10 July 2004
11. Boyar J, Favrholdt LM, Larsen KS (2005) The rela-
tive worst order ratio applied to paging. In: Proceed-
ings of the sixteenth annual ACM-SIAM symposium
on discrete algorithms (SODA 2005), Vancouver, 23–
25 Jan 2005. ACM, pp 718–727
12. Denning PJ (1968) The working set model for pro-
gram behaviour. CACM 11(5):323–333
13. Denning PJ (1980) Working sets past and present.
IEEE Trans Softw Eng SE-6(1):64–84
14. Dorrigiv R, López-Ortiz A (2012) List update with
probabilistic locality of reference. Inf Process Lett
112(13):540–543
15. Kenyon C (1996) Best-ﬁt bin-packing with random
order. In: Proceedings of the seventh annual ACM-
SIAM symposium on discrete algorithms, Atlanta,
28–30 Jan 1996, pp 359–364
16. Koutsoupias E, Papadimitriou C (2000) Beyond com-
petitive analysis. SIAM J Comput 30: 300–317
17. Koutsoupias E, Papadimitriou C, Yannakakis M
(1996) Searching a ﬁxed graph. In: Proceedings
of the 23rd international colloquium on automata,
languages and programming (ICALP96), Paderborn,
8–12 July 1996, vol 1099, pp 280–289
18. Manthey B, Reischuk R (2005) Smoothed analysis
of the height of binary search trees. Schriftenreihe
der
Institute
für
Informatik
und
Mathematik
A-05-17,
Universität
zu
Lübeck,
Lübeck.
http://www.tcs.uni-luebeck.de/pages/manthey/publica
tions/SearchTrees-SIIM-A-05-17.pdf
19. Sleator DD, Tarjan RE (1985) Amortized efﬁciency
of list update and paging rules. Commun ACM
28:202–208
20. Spielman DA, Teng SH (2003) Smoothed analysis of
termination of linear programming algorithms. Math
Program 97(1–2):375–404
21. Spielman DA, Teng SH (2004) Smoothed analysis of
algorithms: why the simplex algorithm usually takes
polynomial time. J ACM 51(3):385–463
22. Young NE (1994) The k-server dual and loose com-
petitiveness for paging. Algorithmica 11(6):525–541
23. Young NE (1998) Bounding the diffuse adversary. In:
Proceedings of the ninth annual ACM-SIAM sym-
posium on discrete algorithms, San Francisco, 25–27
Jan 1998, pp 420–425
24. Young NE (2000) On-line paging against adversari-
ally biased random inputs. J Algorithms 37(1):218–
235
25. Young NE (2002) On-line ﬁle caching. Algorithmica
33(3):371–383
Amortized Analysis on Enumeration
Algorithms
Takeaki Uno
National Institute of Informatics, Chiyoda,
Tokyo, Japan
Keywords
Amortized analysis; Delay; Enumeration tree;
Recursion

Amortized Analysis on Enumeration Algorithms
73
A
Years and Authors of Summarized
Original Work
1998; Uno
Problem Deﬁnition
Let A be an enumeration algorithm. Suppose that
A is a recursive type algorithm, i.e., composed
of a subroutine that recursively calls itself several
times (or none). Thus, the recursion structure of
the algorithm forms a tree. We call the subroutine
or the execution of the subroutine an iteration.
We here assume that an iteration does not include
the computation done in the recursive calls gen-
erated by itself. We regard a series of subroutines
of different types as an iteration if they form a
nested recursion. We simply write the set of all
iterations of an execution of A by X.
When an iteration X recursively calls an iter-
ation Y , X is called the parent of Y , and Y is
called a child of X. The root iteration is that with
no parent. For non-root iteration X, its parent is
unique and is denoted by P.X/. The set of the
children of X is denoted by C.X/. The parent-
child relation between iterations forms a tree
structure called a recursion tree, or an enumer-
ation tree. An iteration is called a leaf iteration if
it has no child and an inner iteration otherwise.
For iteration X, an upper bound of the exe-
cution time (the number of operations) of X is
denoted by T .X/. Here we exclude the computa-
tion for the output process from the computation
time. We remind that T .X/ is the time for local
execution time and thus does not include the
computation time in the recursive calls generated
by X. For example, when T .X/ D O.n2/, T .X/
is written as cn2 for some constant c. T  is
the maximum T .X/ among all leaf iterations X.
Here, T  can be either constant or a polynomial
of the input size. If X is an inner iteration, let
T .X/ D P
Y 2C.X/ T .Y /.
Key Results
We explain methods to amortize the computation
time of iterations that only requires a local
condition and give simple algorithms which
achieves
nontrivial
time
complexity.
On
enumeration algorithms, it is very hard to grasp
the global structures of the computation and the
recursion tree that is coming from the hardness
of estimating the number of iterations in a
branch. Instead of that, we approach from local
amortization from parent and children. When
we go deep in a recursion tree, the number of
iterations tends to increase exponentially, and the
size of the input of each iteration often decreases
on the other hand. Motivated by this observation,
we amortize the computation time by moving
the computation time of each iteration to its
children from the top to bottom, so that the long
computation time on upper levels is diffused.
Amortization by Children
Suppose that each iteration X takes O..jC.X/jC
1/T / time. Note that this implies that a leaf itera-
tion takes O.T / time. Then, the total computation
time of the algorithm is O.T P
X2X jC.X/j C
1/ D O.T .jXj C P
X2X jC.X/j/ D O.T jXj/,
since any iteration is a child of at most one
iteration. Hence, an iteration takes O.T / time
on average. Let us see an example on the fol-
lowing algorithm for enumerating all subsets of
f1; : : : ; ng.
We can conﬁrm that the algorithm correctly
enumerates all subsets without duplications, and
an iteration X takes O.jC.X/j/ time, except
for the output process. Without amortization, the
time complexity is O.n/ for each iteration, but
the above amortization reduces it to O.1/. Note
that the output process is shortened by outputting
each subset by the difference from the previously
output subset, and by this the accumulated com-
putation time for output process is also bounded
by O.1/ for each subset. This amortization tech-
nique is common in many algorithms. Further,
in the enumeration of spanning trees, the time
Algorithm EnumSubset (S; x):
1 output S
2 for i WD x C 1 to n; call EnumSubset
(S [ fig; i C 1)

74
Amortized Analysis on Enumeration Algorithms
complexity is amortized by not only the chil-
dren but also the grandchildren [3]. More so-
phisticated amortization is used in [1, 2] for path
connecting given two vertices and subtrees of
size k.
Push-Out Amortization
When the computation time of an iteration X
is not proportional jC.X/j, the above amorti-
zation does not work. In such cases, push-out
amortization [4–6] can work. We amortize the
computation time by charging the computation
time of iterations near by the root of the recursion
to those in bottom levels, by recursively moving
the computation time from an iteration to its
children from top to down. The move is done in
the following push-out rule.
Push-out
rule
(PO
rule):
Suppose
that
iteration X receives a computation time of
S.X/ from its parent; thus X has computation
time of S.X/ C T .X/ in total. Then, we ﬁx
ˇ
˛1.jC.X/jC1/T  of the computation time to X
and charge (push-out) the remaining computation
time of quantity S.X/ C T .X/ 
ˇ
˛1.jC.X/j C
1/T  to its children. Each child Z of X receives
computation time proportional to T .Z/, that
is, S.Z/ D .S.X/ C T .X/ 
ˇ
˛1.jC.X/j C
1/T / T .Z/
T .X/.
After the moves in this rule from the top to
bottom of the recursion tree, each inner itera-
tion has O..jC.X/j C 1/T / computation time,
thus O.T / time per iteration. Moreover, when
the following push-out condition holds for any
non-leaf iteration X, each leaf iteration receives
computation time of O.T / from its parent; thus
the computation time per iteration is bounded by
O.T /. Suppose that ˛ > 1 and ˇ  0 are two
constants.
Push-Out Condition (PO Condition)
T .X/  ˛T .X/  ˇ.jC.X/j C 1/T 
Intuitively, this means that T .X/  ˛T .X/ holds
after the assignment of the computation time of
˛ˇ.jC.X/jC1/T  to children and the remaining
to itself, the inequation. Thus, the computation
time of one level of recursion intuitively increases
as the depth, unless there are not so many leaf it-
erations. These suggest that the total computation
time spent by middle-level iterations is relatively
short compared to that by leaf iterations.
Theorem 1 If any inner iteration of an enu-
meration algorithm satisﬁes PO condition, the
amortized computation time of an iteration is
O.T /.
Proof We state by induction that when we charge
computation time with PO rule, from the root
iteration to the leaf iterations, each iteration X
satisﬁes S.X/  T .X/=.˛  1/. The root it-
eration satisﬁes this condition. Suppose that an
iteration X satisﬁes it. Then, for any child Z of
X, we have
S.Z/ D .S.X/ C T .X/ 
ˇ
˛  1.jC.X/j C 1/T / T .Z/
T .X/
 .T .X/=.˛  1/ C T .X/ 
ˇ
˛  1.jC.X/j C 1/T / T .Z/
T .X/
D ˛T .X/  ˇ.jC.X/j C 1/T 
T .X/
 T .Z/
˛  1:
Therefore, any leaf iteration receives O.T / time
from its parent, and the statement holds.
Since PO condition is satisﬁed, T .X/

˛T .X/  ˇ.jC.X/j C 1/T . Thus,
˛T .X/  ˇ.jC.X/j C 1/T 
T .X/
T .Z/
˛  1  T .Z/
˛  1:ut
Matching Enumeration
Let us see an example of designing algorithms
so that push-out amortization does work. The
problem is the enumeration of matchings in an
undirected graph G D .V; E/. A matching is an
edge set M  E such that any vertex is incident
to at most one edge in M. A straightforward

Amortized Analysis on Enumeration Algorithms
75
A
way to enumerate all matchings is to choose an
edge e and enumerate matchings including e and
enumerate matchings not including e, recursively.
This algorithm yields the time complexity of
O.jV j/ for each matching.
We here consider another way for the enumer-
ation. We choose a vertex v of the maximum de-
gree and partition the problem into enumeration
of matchings including e1, matchings including
e2, : : :, matchings including ek, and matchings
including none of e1; : : : ; ek. Here e1; : : : ; ek are
the edges incident to v. Since any matching has
at most one edge incident to v, this algorithm
is complete and makes no duplication. The algo-
rithm is described as follows. Note that G n fvg
denotes the graph obtained by removing vertex v
and edges incident to v from G.
Algorithm
EnumMatching
(G D .V; E/;
M):
1 if E D ; then output M; return
2 choose a vertex v having the maximum degree in G
3 call EnumMatching (G n fvg; M)
4 for each edge e D .v; u/, call EnumMatching
(G n fu; vg; M [ feg)
G n fu; vg is obtained from G n fu0; vg in
O.d.u/ C d.u0// time, where d.u/ and d.u0/ are
the degrees of u and u0, respectively. From this,
the computation time in step 4 is bounded by the
sum of degrees of all vertices adjacent to v. Here
T .X/ D cjEj for some c, except for the output
process. Note that jEj is the number of edges in
the graph given to X.
The input graph of the child generated in step
3 has jEj  d.v/ edges and that in step 4 has
jEj d.v/ d.u/ C1 edges. Thus, when d.v/ <
jEj=4, we have T .X/ D c..jEjd.v//C.jEj
d.v/  d.u/ C 1//  1:25cjEj. When d.v/ 
jEj=4, jC.X/j  jEj=4. Thus, PO condition
holds by setting ˛ D 1:25 and choosing a certain
ˇ. The output process can be shorten as the subset
enumeration.
Theorem 2 Matchings of a graph can be enu-
merated in O.1/ time for each matching.
Elimination Ordering
An elimination ordering is a sequence of
elements obtained by iteratively removing an
element from an object G with keeping a property
satisﬁed, until the object will be empty. Examples
are perfect elimination ordering and perfect
sequence. The former is the removal sequence
of simplicial vertices from a chordal graph, and
the latter is the removal sequence of cliques from
a connected chordal graph. Elimination orderings
can be enumerated by a simple algorithm as
follows.
Algorithm EnumElim (G; S):
1 if G D ; then output S; return
2 for each element e of G that can be removed, call
EnumElim (G n feg; S [ feg)
Here we assume that T .X/
D
poly.jGj/
except for output process. The decision problem
of removing an element from G is naturally
considered to be solved in O.poly.jGj// time;
thus this assumption is natural.
Theorem 3 If any G of size larger than some
constant c has at least two removable elements,
elimination orderings are enumerated in O.1/
time for each.
Proof The statement means that each iteration
has at least two children, if its computation time is
not constant. For sufﬁciently large constant ı, we
always have poly.jGj/  2poly.jGj  1/ for any
jGj > ı. This implies that PO condition always
holds for these iterations.
ut
Recommended Reading
1. Birmele E, Ferreira RA, Grossi R, Marino A, Pisanti N,
Rizzi R, Sacomoto G (2013) Optimal listing of cycles
and st-paths in undirected graphs. SODA 2013:1884–
1896
2. Ferreira RA, Grossi R, Rizzi R (2011) Output-sensitive
listing of bounded-size trees in undirected graphs. ESA
2011:275–286
3. Shioura A, Tamura A, Uno T (1997) An optimal
algorithm for scanning all spanning trees of undirected
graphs. SIAM J Comput 26:678–692

76
AMS Sketch
4. Uno T (1998) New approach for speeding up enumer-
ation algorithms. LNCS 1533:287–296
5. Uno T (1999) A new approach for speeding up enu-
meration algorithms and its application for matroid
bases. LNCS 1627:349–359
6. Uno T (2014) A new approach to efﬁcient enumeration
by push-out amortization. arXiv:1407.3857
AMS Sketch
Graham Cormode
Department of Computer Science, University of
Warwick, Coventry, UK
Keywords
Euclidean norm; Second-moment estimation;
Sketch; Streaming algorithms
Years and Authors of Summarized
Original Work
1996; Alon, Matias, Szegedy
Problem Deﬁnition
Streaming algorithms aim to summarize a large
volume of data into a compact summary, by main-
taining a data structure that can be incrementally
modiﬁed as updates are observed. They allow the
approximation of particular quantities. The AMS
sketch is focused on approximating the sum of
squared entries of a vector deﬁned by a stream of
updates. This quantity is naturally related to the
Euclidean norm of the vector and so has many
applications in high-dimensional geometry and in
data mining and machine learning settings that
use vector representations of data.
The data structure maintains a linear projec-
tion of the stream (modeled as a vector) with
a number of randomly chosen vectors. These
random vectors are deﬁned implicitly by sim-
ple hash functions, and so do not have to be
stored explicitly. Varying the size of the sketch
changes the accuracy guarantees on the resulting
estimation. The fact that the summary is a linear
projection means that it can be updated ﬂexibly,
and sketches can be combined by addition or
subtraction, yielding sketches corresponding to
the addition and subtraction of the underlying
vectors.
Key Results
The AMS sketch was ﬁrst proposed by Alon,
Matias, and Szegedy in 1996 [1]. Several re-
ﬁnements or variants have subsequently appeared
in the literature, for example, in the work of
Thorup and Zhang [4]. The version presented
here works by using hashing to map each update
to one of t counters rather than taking the average
of t repetitions of an “atomic” sketch, as was
originally proposed. This hash-based variation is
often referred to as the “fast AMS” summary.
Data Structure Description
The AMS summary maintains an array of counts
which are updated with each arriving item. It
gives an estimate of the `2-norm of the vector
v that is induced by the sequence of updates.
The estimate is formed by computing the norm of
each row and taking the median of all rows. Given
parameters " and ı, the summary uses space
O.1="2 log 1=ı/ and guarantees with probability
of at least 1  ı that its estimate is within relative
"-error of the true `2-norm, kvk2.
Initially, v is taken to be the zero vector. A
stream of updates modiﬁes v by specifying an
index i to which an update w is applied, setting
vi  vi C w. The update weights w can be
positive or negative.
The AMS summary is represented as a com-
pact array C of d  t counters, arranged as d
rows of length t. In each row j , a hash func-
tion hj maps the input domain U uniformly to
f1; 2; : : : tg. A second hash function gj maps
elements from U uniformly onto f1; C1g. For
the analysis to hold, we require that gj is four-
wise independent. That is, over the random choice
of gj from the set of all possible hash functions,
the probability that any four distinct items from

AMS Sketch
77
A
the domain that get mapped to f1; C1g4 is
uniform: each of the 16 possible outcomes is
equally likely. This can be achieved by using
polynomial hash functions of the form gj .x/ D
2..ax3 C bx2 C cx C d mod p/ mod 2/  1,
with parameters a; b; c; d chosen uniformly from
the prime ﬁeld p.
The sketch is initialized by picking the hash
functions to be used and initializing the array of
counters to all zeros. For each update operation
to index i with weight w (which can be either
positive or negative), the item is mapped to an
entry in each row based on the hash functions
h and the update applied to the corresponding
counter, multiplied by the corresponding value
of g. That is, for each 1  j  d, hj .i/ is
computed, and the quantity wgj .i/ is added to
entry CŒj; hj .i/ in the sketch array. Processing
each update therefore takes time O.d/, since
each hash function evaluation takes constant
time.
The sketch allows an estimate of kvk2
2, the
squared Euclidean norm of v, to be obtained.
This is found by taking the sums of the squares
of the rows of the sketch and in turn ﬁnding
the median of these sums. That is, for row j ,
it computes Pt
kD1 CŒj; k2 as an estimate and
takes the median of these d estimates. The query
time is linear in the size of the sketch, O.td/, as
is the time to initialize a new sketch. Meanwhile,
update operations take time O.d/.
The analysis of the algorithm follows by con-
sidering the produced estimate as a random vari-
able. The random variable can be shown to be
correct in expectation: its expectation is the de-
sired quantity, kvk2
2. This can be seen by expand-
ing the expression of the estimator. The resulting
expression has terms P
i v2
i but also terms of the
form vivj for i ¤ j . However, these “unwanted
terms” are multiplied by either C1 or 1 with
equal probability, depending on the choice of the
hash function g. Therefore, their expectation is
zero, leaving only kvk2. To show that it is likely
to fall close to its expectation, we also analyze
the variance of the estimator and use Chebyshev’s
inequality to argue that with constant probabil-
ity, each estimate is close to the desired value.
Then, taking the median of sufﬁcient repetitions
ampliﬁes this constant probability to be close to
certainty.
This analysis shows that the estimate is be-
tween .1  "/kvk2
2 and .1 C "/kvk2
2. Taking the
square root of the estimate gives a result that
is between .1  "/1=2kvk2 and .1 C "/1=2kvk2,
which means it is between .1  "=2/kvk2 and
.1 C "=2/kvk2.
Note that since the updates to the AMS sketch
can be positive or negative, it can be used to mea-
sure the Euclidean distance between two vectors
v and u: we can build an AMS sketch of v and
one of u and merge them together by adding the
sketches. Note also that a sketch of u can be
obtained from a sketch of u by negating all the
counter values.
Applications
The sketch can also be applied to estimate the
inner product between a pair of vectors. A similar
analysis shows that the inner product of cor-
responding rows of two sketches (formed with
the same parameters and using the same hash
functions) is an unbiased estimator for the inner
product of the vectors. This use of the sum-
mary to estimate the inner product of vectors
was described in a follow-up work by Alon,
Matias, Gibbons, and Szegedy [2], and the anal-
ysis was similarly generalized to the fast version
by Cormode and Garofalakis [3]. The ability to
capture norms and inner products in Euclidean
space means that these sketches have found many
applications in settings where there are high-
dimensional vectors, such as machine learning
and data mining.
URLs to Code and Data Sets
Sample implementations are widely available in
a variety of languages.
C code is given by the MassDAL code
bank:
http://www.cs.rutgers.edu/~muthu/
massdalcode-index.html.

78
Analyzing Cache Behaviour in Multicore Architectures
C++ code given by Marios Hadjieleftheriou
is
available
at
http://hadjieleftheriou.com/
sketches/index.html.
Cross-References
▷Count-Min Sketch
Recommended Reading
1. Alon N, Matias Y, Szegedy M (1996) The space com-
plexity of approximating the frequency moments. In:
ACM symposium on theory of computing, Philadel-
phia, pp 20–29
2. Alon N, Gibbons P, Matias Y, Szegedy M (1999)
Tracking join and self-join sizes in limited storage.
In: ACM principles of database systems, New York,
pp 10–20
3. Cormode G, Garofalakis M (2005) Sketching streams
through the net: distributed approximate query track-
ing. In: International conference on very large data
bases, Trondheim
4. Thorup M, Zhang Y (2004) Tabulation based 4-
universal hashing with applications to second moment
estimation. In: ACM-SIAM symposium on discrete
algorithms, New Orleans
Analyzing Cache Behaviour
in Multicore Architectures
Alejandro López-Ortiz1 and Alejandro Salinger2
1David R. Cheriton School of
Computer Science, University of Waterloo,
Waterloo, ON, Canada
2Department of Computer Science, Saarland
University, Saarbücken, Germany
Keywords
Cache; Chip multiprocessor; Multicore; Online
algorithms; Paging
Years and Authors of Summarized
Original Work
2010; Hassidim
2012; López-Ortiz, Salinger
Problem Deﬁnition
Multicore processors are commonly equipped
with one or more levels of cache memory, some
of which are shared among two or more cores.
Multiple cores compete for the use of shared
caches for fast access to their program’s data,
with the cache usage patterns of a program
running on one core, possibly affecting the cache
performance of programs running on other cores.
Paging
The management of data across the various levels
of the memory hierarchy of modern computers is
abstracted by the paging problem. Paging models
a two-level memory system with a small and fast
memory – known as cache – and a large and
slow memory. Data is transferred between the
two levels of memory in units known as pages.
The input to the problem is a sequence of page
requests that must be made available in cache
as they are requested. If the currently requested
page is already present in the cache, then this
is known as a hit. Otherwise a fault occurs, and
the requested page must be brought from slow
memory to cache, possibly requiring the eviction
of a page currently residing in the cache. An
algorithm for this problem must decide, upon
each request that results in a fault with a full
cache, which page to evict in order to minimize
the number of faults. Since the decision of which
page to evict must be taken without information
of future requests, paging is an online problem.
The most popular framework to analyze the
performance of online algorithms is competitive
analysis [10]: an algorithm A for a minimization
problem is said to be c-competitive if its cost
is at most c times that of an optimal algorithm
that knows the input in advance. Formally, let
A.r/ and OPT.r/ denote the costs of A and the
optimal algorithm OPT on an input r. Then A
is c-competitive if for all inputs r, A.r/  c 
OPT.r/ C ˇ, where ˇ is a constant that does not
depend on r. The inﬁmum of all such values c is
known as A’s competitive ratio.
Traditional
paging
algorithms,
like
least
recently used (LRU), evict the page currently

Analyzing Cache Behaviour in Multicore Architectures
79
A
in cache that was least recently accessed, or
ﬁrst-in-ﬁrst-out (FIFO), evict the page currently
in the cache that was brought in the earliest,
have an optimal competitive ratio equal to the
cache size. Other optimal eviction policies are
ﬂush-when-full (FWF) and Clock (see [2] for
deﬁnitions).
Paging in Multicore Caches
The paging problem described above can be ex-
tended to model several programs running simul-
taneously with a shared cache. For a multicore
system with p cores sharing one cache, the mul-
ticore paging problem consists of a set r of p
request sequences r1; : : : rp to be served with one
shared cache of size k pages. At any timestep,
at most p requests from different processors can
arrive and must be served in parallel. A paging
algorithm must decide which pages to evict when
a fault occurs on a full cache.
The general model we consider for this prob-
lem was proposed by Hassidim [6]. This model
deﬁnes the fetching time 
 of a page as the
ratio between a cache miss and a cache hit. A
sequence of requests that suffers a page fault
must wait 
 timesteps for the page to be fetched
into the cache, while other sequences that in-
cur hits can continue to be served. In addition,
paging algorithms can decide on the schedule of
request sequences, choosing to serve a subset of
the sequences and delay others. In this problem,
the goal of a paging algorithm is to minimize
the makespan. López-Ortiz and Salinger [8] pro-
posed a slightly different model in which paging
algorithms are not allowed to make schedul-
ing decisions and must serve requests as they
arrive. Furthermore, instead of minimizing the
makespan, they propose two different goals: min-
imize the number of faults and decide if each
of the sequences can be served with a num-
ber of faults below a given threshold. We con-
sider both these settings here and the following
problems:
Deﬁnition 1 (Min-Makespan) Given a set r of
request sequences r1; : : : ; rp to be served with a
cache of size k, minimize the timestep at which
the last request among all sequences is served.
Deﬁnition 2 (Min-Faults) Given a set r of re-
quests r1; : : : ; rp to be served with a cache of
size k, minimize the total number of faults when
serving r.
Deﬁnition 3 (Partial-Individual-Faults) Given
a set r of requests r1; : : : ; rp to be served with a
cache of size k, a timestep t, and a bound bi for
each sequence, decide whether r can be served
such that at time t the number of faults on ri is at
most bi for all 1  i  p.
Key Results
Online Paging
For both the models of Hassidim and López-Ortiz
and Salinger ,no online algorithm has been shown
to be competitive, while traditional algorithms
that are competitive in the classic paging setting
are not competitive in the multicore setting. Has-
sidim shows that LRU and FIFO have a com-
petitive ratio in the Min-Makespan problem of
	.
/, which is the worst possible for any online
algorithm in this problem.
In the following, k is the size of the shared
cache of an online algorithm, and h is the size of
the shared cache of the optimal ofﬂine.
Theorem 1 ([6]) For any ˛ > 1, the competitive
ratio of LRU (or FIFO) is 	.
=˛/, when h D
k=˛. In particular, if we give LRU a constant
factor resource augmentation, the ratio is 	.
/.
There is a setting with this ratio with just d˛e C 1
cores.
The bad competitive ratio stems from the abil-
ity of the ofﬂine algorithm to schedule sequences
one after the other one so that each sequence can
use the entire cache. Meanwhile, LRU or FIFO
will try to serve all sequences simultaneously, not
having enough cache to satisfy the demands of
any sequence. A similar result is shown in [8] for
the Min-Faults problem, even in the case in which
the optimal ofﬂine cannot explicitly schedule the
input sequences. In this case, given a set of
request sequences that alternate periods of high
and low cache demand, the optimal ofﬂine algo-
rithm can delay some sequences through faults in

80
Analyzing Cache Behaviour in Multicore Architectures
order to align periods of high demands of some
sequences with periods of low demands of others
and with a total cache demand below capacity.
As in the previous lower bound, traditional on-
line algorithms will strive to serve all sequences
simultaneously, incurring only faults in periods of
high demand.
Theorem 2 ([8]) Let A be any of LRU, FIFO,
Clock, or FWF, let p  4, let n be the total
length of request sequences, and assume 
 > 1.
The competitive ratio of A is at least ˝.
p
n
=k/
when the optimal ofﬂine’s cache is h  k=2 C
3p=2. If A has no resource augmentation, the
competitive ratio is at least ˝.
p
n
p=k/.
These results give light about the characteris-
tics required by online policies to achieve better
competitive ratios. López-Ortiz and Salinger an-
alyzed paging algorithms for Min-Faults , sepa-
rating the cache partition and the eviction policy
aspects. They deﬁned partitioned strategies as
those that give a portion of the cache to each
core and serve the request sequences with a given
eviction policy exclusively with the given part of
the cache. The partition can be static or dynamic.
They also deﬁne shared strategies as those in
which all requests are served with one eviction
policy using a global cache. The policies consid-
ered in Theorems 1 and 2 above are examples of
shared strategies.
If a cache partition is determined externally by
a scheduler or operating system, then traditional
eviction policies can achieve a good performance
when compared to the optimal eviction policy
with the same partition. More formally,
Theorem 3 Let A be any marking or conserva-
tive paging algorithm and B be any dynamically
conservative algorithm [9] (these classes include
LRU, FIFO, and Clock). Let S and D be any
static and dynamic partition functions and let
OPTs and OPTd denote the optimal eviction
policies given S and D, respectively. Then, for
all inputs r, A.r/  k  OPTs.r/ and B.r/ 
pk  OPTd.r/.
The result above relies on a result by Peserico
[9] which states that dynamically conservative
policies are k-competitive when the size of the
cache varies throughout the execution of the
cache instance.
When considering a strategy as a partition plus
eviction policy, it should not be a surprise that
a strategy involving a static partition cannot be
competitive. In fact, even a dynamic partition that
does not change the sizes of the parts assigned
to its cores often enough cannot be competitive.
There are sequences for which the optimal static
partition with the optimal paging policy in each
part can incur a number of faults that is arbitrarily
large compared to an online shared strategy using
LRU. A similar result applies to dynamic parti-
tions that change a sublinear number of times.
These results suggest that in order to be com-
petitive, an online strategy needs to be either
shared or partitioned with a partition that changes
often.
Ofﬂine Paging
We now consider the ofﬂine multicore paging
problem. Hassidim shows that Min-Makespan is
NP-hard for k D p=3 and [7] extends it to arbi-
trary k and p. In the model without scheduling
of [8], Partial-Individual-Faults, a variant of the
fault minimization problem, is also shown to be
NP-hard. It is not known, however, whether Min-
Faults is NP-hard as well. Interestingly, Partial-
Individual-Faults remains NP-hard when 
 D 1
(and hence a fault does not delay the affected
sequence with respect to other sequences). In
contrast, in this case, Min-Faults can be solved
simply by evicting the page that will be requested
furthest in the future, as in classic paging. On
the positive side, the following property holds for
both Min-Makespan and Min-Faults (on disjoint
sequences).
Theorem 4 There exist optimal algorithms for
Min-Makespan and Min-Faults that, upon a fault,
evict the page that is the furthest in the future for
some sequence.
This result implies that multicore paging re-
duces to determining the optimal dynamic par-
tition of the cache: upon a fault, the part of the
cache of one sequence is reduced (unless this
sequence is the same as the one which incurred
the fault), and the page whose next request is

Analyzing Cache Behaviour in Multicore Architectures
81
A
furthest is the future in this sequence should be
evicted.
Finally, in the special case of a constant num-
ber of processors p and constant delay 
, Min-
Makespan admits a polynomial time approxi-
mation scheme (PTAS), while Min-Faults and
Partial-Individual-Faults admit exact polynomial
time algorithms.
Theorem 5 ([6]) There exists an algorithm that,
given an instance of Min-Makespan with optimal
makespan m, returns a solution with makespan
.1 C /m. The running time is exponential on p,

, and 1=.
Theorem 6 ([8]) An instance of Min-Faults with
p requests of total length n, with p D O.1/ and

 D O.1/ can be solved in time O.nkCp
p/.
Theorem 7 ([8]) An
instance
of
Partial-
Individual-Faults with p requests of total length
n, with p D O.1/ and 
 D O.1/, can be solved
in time O.nkC2pC1
pC1/.
Other Models
Paging with multiple sequences with a shared
cache has also been studied in other models
[1,3–5], even prior to multicores. In these models,
request sequences may be interleaved; however,
only one request is served at a time and all
sequences must wait upon a fault affecting one
sequence.
In the application-controlled model of Cao
et al. [3], each process has full knowledge of its
request sequence, while the ofﬂine algorithm also
knows the interleaving of requests. As opposed to
the models in [6, 8], the interleaving is ﬁxed and
does not depend on the decisions of algorithms.
It has been shown that for p sequences and a
cache of size k, no online deterministic algorithm
can have a competitive ratio better than p C 1
in the case where sequences are disjoint [1] and
p
2 log

4.kC1/
3p

otherwise [7]. On the other hand,
there exist algorithms with competitive ratios
2.p C 1/ [1, 3] and maxf10; p C 1g [7] for the
disjoint case, and 2p.ln.ek=p/ C 1/ [7] for the
shared case.
Open Problems
Open problems in multicore paging are ﬁnding
competitive
online
algorithms,
determining
the exact complexity of Min-Faults, obtaining
approximation algorithms for Min-Makespan for
a wider range of parameters, and obtaining faster
exact ofﬂine algorithms for Min-Faults and
Partial-Individual-Faults. Another challenge in
multicore paging is concerned with modeling
the right features of the multicore architecture
while enabling the development of meaningful
algorithms.
Factors
to
consider
are
cache
coherence, limited parallelism in other shared
resources (such as bus bandwidth), different
cache associativities, and others.
Cross-References
▷Online Paging and Caching
Recommended Reading
1. Barve RD, Grove EF, Vitter JS (2000) Application-
controlled paging for a shared cache. SIAM J Comput
29:1290–1303
2. Borodin A, El-Yaniv R (1998) Online computa-
tion and competitive analysis. Cambridge University
Press, New York
3. Cao P, Felten EW, Li K (1994) Application-controlled
ﬁle caching policies. In: Proceedings of the USENIX
summer 1994 technical conference – volume 1
(USTC’94), Boston, pp 171–182
4. Feuerstein E, Strejilevich de Loma A (2002) On-line
multi-threaded paging. Algorithmica 32(1):36–60
5. Fiat A, Karlin AR (1995) Randomized and multi-
pointer paging with locality of reference. In: Proceed-
ings of the 27th annual ACM symposium on theory of
computing (STOC’95), Las Vegas. ACM, pp 626–634
6. Hassidim A (2010) Cache replacement policies for
multicore processors. In: Yao ACC (ed) Innovations
in computer science (ICS 2010), Tsinghua University,
Beijing, 2010, pp 501–509
7. Katti AK, Ramachandran V (2012) Competitive
cache
replacement
strategies
for
shared
cache
environments. In: International parallel and dis-
tributed processing symposium (IPDPS’12), Shang-
hai, pp 215–226
8. López-Ortiz A, Salinger A (2012) Paging for multi-
core shared caches. In: Proceedings of the 3rd
innovations in theoretical computer science confer-
ence (ITCS’12), Cambridge. ACM, pp 113–127

82
Analyzing Cache Misses
9. Peserico E (2013) Elastic paging. In: SIGMETRICS,
Pittsburgh. ACM, pp 349–350
10. Sleator DD, Tarjan RE (1985) Amortized efﬁciency
of list update and paging rules. Commun ACM
28(2):202–208
Analyzing Cache Misses
Naila Rahman
University of Hertfordshire, Hertfordshire, UK
Keywords
Cache analysis
Years and Authors of Summarized
Original Work
2003; Mehlhorn, Sanders
Problem Deﬁnition
The
problem
considered
here
is
multiple
sequence access via cache memory. Consider
the following pattern of memory accesses. k
sequences of data, which are stored in disjoint
arrays and have a total length of N , are accessed
as follows:
for t WD 1 to N do
select a sequence si 2 f1; : : : kg
work on the current element of sequence si
advance sequence si to the next element.
The aim is to obtain exact (not just asymp-
totic) closed form upper and lower bounds for
this problem. Concurrent accesses to multiple
sequences of data are ubiquitous in algorithms.
Some examples of algorithms which use this
paradigm are distribution sorting, k-way merg-
ing, priority queues, permuting, and FFT. This
entry summarizes the analyses of this problem in
[5,8].
Caches, Models, and Cache Analysis
Modern computers have hierarchical memory
which consists of registers, one or more levels
of caches, main memory, and external memory
devices such as disks and tapes. Memory size
increases, but the speed decreases with distance
from the CPU. Hierarchical memory is designed
to improve the performance of algorithms by
exploiting temporal and spatial locality in data
accesses.
Caches are modeled as follows. A cache has
m blocks each of which holds B data elements.
The capacity of the cache is M D mB. Data is
transferred between one level of cache and the
next larger and slower memory in blocks of B
elements. A cache is organized as s D m=a sets
where each set consists of a blocks. Memory at
address xB, referred to as memory block x, can
only be placed in a block in set x mod s. If
a D 1, the cache is said to be direct mapped, and
if a D s, it is said to be fully associative.
If memory block x is accessed and it is not in
cache, then a cache miss occurs, and the data in
memory block x is brought into cache, incurring
a performance penalty. In order to accommodate
block x, it is assumed that the least recently used
(LRU) or the ﬁrst used (FIFO) block from the
cache set x mod s is evicted, and this is referred
to as the replacement strategy. Note that a block
may be evicted from a set, even though there may
be unoccupied blocks in other sets.
Cache analysis is performed for the number of
cache misses for a problem with N data elements.
To read or write N data elements, an algorithm
must incur .N=B/ cache misses. These are the
compulsory or ﬁrst reference misses. In the multi-
ple sequence access via cache memory problem,
for given values of M and B, one aim is to ﬁnd
the largest k such that there are O.N=B/ cache
misses for the N data accesses. It is interesting
to analyze cache misses for the important case of
direct mapped cache and for the general case of
set-associative caches.
A large number of algorithms have been
designed on the external memory model [11],
and these algorithms optimize the number of
data transfers between main memory and disk.
It seems natural to exploit these algorithms

Analyzing Cache Misses
83
A
to minimize cache misses, but due to the
limited associativity of caches, this is not
straightforward. In the external memory model,
data transfers are under programmer control,
and the multiple sequence access problem has
a trivial solution. The algorithm simply chooses
k  Me=Be, where Be is the block size and
Me is the capacity of the main memory in the
external memory model. For k  Me=Be, there
are O.N=Be/ accesses to external memory. Since
caches are hardware controlled, the problem
becomes nontrivial. For example, consider the
case where the starting addresses of k > a equal
length sequences map to the ith element of the
same set, and the sequences are accessed in a
round-robin fashion. On a cache with an LRU or
FIFO replacement strategy, all sequence accesses
will result in a cache miss. Such pathological
cases can be overcome by randomizing the
starting addresses of the sequences.
Related Problems
A very closely related problem is where accesses
to the sequences are interleaved with accesses
to a small working array. This occurs in ap-
plications such as distribution sorting or matrix
multiplication.
Caches can emulate external memory with an
optimal replacement policy [3,10]; however, this
requires some constant factor more memory.
Since the emulation techniques are software
controlled
and
require
modiﬁcation
to
the
algorithm, rather than selection of parameters,
they work well for fairly simple algorithms
[6].
Key Results
Theorem 1 ([5]) Given an a-way set-associative
cache with m cache blocks, s D m=a cache
sets, cache blocks size B, and LRU or FIFO
replacement strategy. Let Ua denote the expected
number of cache misses in any schedule of
N
sequential accesses to k sequences with
starting addresses that are at least .a C 1/-wise
independent:
U1  k C N
B

1 C .B  1/ k
m

;
(1)
U1  N
B

1 C .B  1/
k  1
m C k  1

;
(2)
Ua  k C N
B

1 C .B  1/
k˛
m
a
C
1
m=.k˛/  1 C k  1
s  1

(3)
for k  m
˛ ;
Ua  k C N
B

1 C .B  1/
kˇ
m
a
C
1
m=.kˇ/  1

(4)
for k  m
2˛ ;
Ua  N
B

1 C .B  1/Ptail

k  1; 1
s ; a

 kM;
(5)
Ua  N
B
 
1 C .B  1/
.k  a/˛
m
a 
1  1
s
k!
 kM;
(6)

84
Analyzing Cache Misses
where ’ D ’.a/ D a=.aŠ/1=a, Ptail.n; p; a/ D
P
ia
n
i

pi.1p/ni is the cumulative binomial
probability, and “ WD 1 C ’.daxe/ where x D
x.a/ D inff0 < ´ < 1 W ´ C ´=’.da´e/ D
1g.
Here 1  ’ < e and “.1/ D 2, “.1/ D
1 C e 
 3:71. This analysis assumes that an ad-
versary schedules the accesses to the sequences.
For the lower bound, the adversary initially ad-
vances sequence si for i D 1 : : : k by Xi el-
ements, where the Xi is chosen uniformly and
independently from f0; M  1g. The adversary
then accesses the sequences in a round-robin
manner.
The k in the upper bound accounts for a
possible extra block that may be accessed due
to randomization of the starting addresses. The
kM term in the lower bound accounts for the
fact that cache misses cannot be counted when
the adversary initially winds forwards the se-
quences.
The bounds are of the form pN C c, where c
does not depend on N and p is called the cache
miss probability. Letting r D k=m, the ratio
between the number of sequences and the number
of cache blocks, the bounds for the cache miss
probabilities in Theorem 1 become [5]
p1  .1=B/.1 C .B  1/r/;
(7)
p1  .1=B/

1 C .B  1/
r
1 C r

;
(8)
pa  .1=B/.1 C .B  1/.r˛/a C r˛ C ar/
for r  1
˛ ;
(9)
pa  .1=B/.1 C B  1/.rˇ/a C rˇ for r  1
2ˇ
(10)
pa.1=B/
 
1 C .B  1/.r˛/a

1  1
s
k!
:
(11)
The 1/B term accounts for the compulsory or
ﬁrst reference miss, which must be incurred in
order to read a block of data from a sequence.
The remaining terms account for conﬂict misses,
which occur when a block of data is evicted from
cache before all its elements have been scanned.
Conﬂict misses can be reduced by restricting
the number of sequences. As r approaches zero,
the cache miss probabilities approach 1/B. In
general, inequality (4) states that the number of
cache misses is O.N=B/ if r  1=.2“/ and
.B  1/.r“/a D O.1/. Both of these condi-
tions are satisﬁed if k  m= max.B1=a; 2“/. So,
there are O.N=B/ cache misses provided k D
O.m=B1=a/.
The analysis shows that for a direct-mapped
cache, where a D 1, the upper bound is a factor
of r C 1 above the lower bound. For a  2,
the upper bounds and lower bounds are close if
.1  1=s/k 
 and .˛ C ˛/r  1, and both these
conditions are satisﬁed if k  s.
Rahman and Raman [8] obtain closer up-
per and lower bounds for average case cache
misses assuming the sequences are accessed uni-
formly randomly on a direct-mapped cache. Sen
and Chatterjee [10] also obtain upper and lower
bounds assuming the sequences are randomly
accessed. Ladner, Fix, and LaMarca have ana-
lyzed the problem on direct-mapped caches on
the independent reference model [4].
Multiple Sequence Access with Additional
Working Set
As stated earlier in many applications, accesses
to sequences are interleaved with accesses to an
additional data structure, a working set, which
determines how a sequence element is to be
treated. Assuming that the working set has size
at most sB and is stored in contiguous memory
locations, the following is an upper bound on the
number of cache misses:
Theorem 2 ([5]) Let Ua denote the bound on the
number of cache misses in Theorem 1 and deﬁne
U0 D N . With the working set occupying w
conﬂict-free memory blocks, the expected number
of cache misses arising in the N accesses to the
sequence data, and any number of accesses to the
working set, is bounded by w C .1  w=s/Ua C
2.w=s/Ua1.

Analyzing Cache Misses
85
A
On a direct-mapped cache, for i D 1; : : : ; k, if
sequence i is accessed with probability pi inde-
pendently of all previous accesses and is followed
by an access to element i of the working set, then
the following are upper and lower bounds for the
number of cache misses:
Theorem 3 ([8]) In a direct-mapped cache with
m cache blocks, each of B elements, if sequence
i, for i D; : : : ; k, is accessed with probability
pi and block j of the working set, for j
D
; : : : ; k=B, is accessed with probability Pj , then
the expected number of cache misses in N se-
quence accesses is at most N.ps C pw/ C k.1 C
1=B/, where
ps  1
B C
k
mB C B  1
mB
k
X
iD1
0
@
k=B
X
j D1
piPj
pi C Pj
C B  1
B
k
X
j D1
pipj
pi C pj
1
A ;
pw 
k
B2m C B  1
mB
k=B
X
iD1
k
X
j D1
Pipj
Pipj
:
Theorem 4 ([8]) In a direct-mapped cache with
m cache blocks each of B elements, if sequence
i, for i D 1; : : : ; k, is accessed with probability
pi  1=m, then the expected number of cache
misses in N sequence accesses is at least Nps C
k, where
ps  1
B C k.2m  k/
2m2
C k.k  3m/
2Bm2

1
2Bm 
k
2B2m C B.k  m/ C 2m  3k
Bm2
k
X
iD1
k
X
j D1
.pi/2
pi C pj
C .B  1/2
B3m2
k
X
iD1
pi
2
4
k
X
j D1
pi.1  pi  pj /
.pi C pj /2
 B  1
2
k
X
j D1
k
X
lD1
pi
pi C pj C pl  pj pl
3
5  O.eB/:
The lower bound ignores the interaction with
the working set, since this can only increase the
number of cache misses.
In Theorems 3 and 4, ps is the probability
of a cache miss for a sequence access, and in
Theorem 3, pw is the probability of a cache miss
for an accesses to the working set.
If the sequences are accessed uniformly ran-
domly, then using Theorems 3 and 4, the ratio
between the upper and lower bound is 3=.3  r/,
where r D k=m. So for uniformly random data,
the lower bound is within a factor of about 3=2 of
the upper bound when k  m and is much closer
when k  m.
Applications
Numerous algorithms have been developed on the
external memory model which access multiple
sequences of data, such as merge sort, distribution
sort, priority queues, and radix sorting. These
analyses are important as they allow initial pa-
rameter choices to be made for cache memory
algorithms.
Open Problems
The analyses assume that the starting addresses
of the sequences are randomized, and current ap-
proaches to allocating random starting addresses
waste a lot of virtual address space [5]. An open
problem is to ﬁnd a good online scheme to ran-
domize the starting addresses of arbitrary length
sequences.
Experimental Results
The cache model is a powerful abstraction of real
caches; however, modern computer architectures
have complex internal memory hierarchies, with

86
Applications of Geometric Spanner Networks
registers, multiple levels of caches, and transla-
tion lookaside buffers (TLB). Cache miss penal-
ties are not of the same magnitude as the cost
of disk accesses, so an algorithm may perform
better by allowing conﬂict misses to increase in
order to reduce computation costs and compul-
sory misses, by reducing the number of passes
over the data. This means that in practice, cache
analysis is used to choose an initial value of k
which is then ﬁne-tuned for the platform and
algorithm [1,2,6,7,9,12,13].
For distribution sorting, in [6], a heuristic was
considered for selecting k, and equations for
approximate cache misses were obtained. These
equations were shown to be very accurate in
practice.
Cross-References
▷Cache-Oblivious Model
▷Cache-Oblivious Sorting
▷External Sorting and Permuting
▷I/O-Model
Recommended Reading
1. Bertasi P, Bressan M, Peserico E (2011) Psort, yet
another fast stable sorting software. ACM J Exp
Algorithmics 16:Article 2.4
2. Bingmann T, Sanders P (2013) Parallel string sample
sort. In: Proceedings of the 21st European symposium
on algorithms (ESA’13), Sophia Antipolis. Springer,
pp 169–180
3. Frigo M, Leiserson CE, Prokop H, Ramachandran S
(1999) Cache-oblivious algorithms. In: Proceedings
of the 40th annual symposium on foundations of com-
puter science (FOCS’99), New York. IEEE Computer
Society, Washington, DC, pp 285–297
4. Ladner RE, Fix JD, LaMarca A (1999) Cache perfor-
mance analysis of traversals and random accesses. In:
Proceedings of the 10th annual ACM-SIAM sympo-
sium on discrete algorithms (SODA’99), Baltimore.
Society for Industrial and Applied Mathematics,
Philadelphia, pp 613–622
5. Mehlhorn K, Sanders P (2003) Scanning multiple
sequences via cache memory. Algorithmica 35:75–93
6. Rahman N, Raman R (2000) Analysing cache effects
in distribution sorting. ACM J Exp Algorithmics
5:Article 14
7. Rahman N, Raman R (2001) Adapting radix sort
to the memory hierarchy. ACM J Exp Algorithmics
6:Article 7
8. Rahman N, Raman R (2007) Cache analysis of
non-uniform distribution sorting algorithms. http://
www.citebase.org/abstract?id=oai:arXiv.org:0706.
2839. Accessed 13 Aug 2007 Preliminary version in:
Proceedings of the 8th annual European symposium
on
algorithms
(ESA’00),
Saarbrücken.
Lecture
notes in computer science, vol 1879. Springer,
Berlin/Heidelberg, pp 380–391 (2000)
9. Sanders P (2000) Fast priority queues for cached
memory. ACM J Exp Algorithmics 5:Article 7
10. Sen S, Chatterjee S (2000) Towards a theory of cache-
efﬁcient algorithms. In: Proceedings of the 11th an-
nual ACM-SIAM symposium on discrete algorithms
(SODA’00), San Francisco. Society for Industrial and
Applied Mathematics, pp 829–838
11. Vitter JS (2001) External memory algorithms and
data structures: dealing with massive data. ACM
Comput Surv 33, 209–271
12. Wassenberg J, Sanders P (2011) Engineering a Multi-
core Radix Sort, In: Proceedings of the 17th inter-
national conference, Euro-Par (2) 2011, Bordeaux.
Springer, pp 160–169
13. Wickremesinghe R, Arge L, Chase JS, Vitter JS
(2002) Efﬁcient sorting using registers and caches.
ACM J Exp Algorithmics 7:9
Applications of Geometric Spanner
Networks
Joachim Gudmundsson1;2, Giri Narasimhan3;4,
and Michiel Smid5
1DMiST, National ICT Australia Ltd,
Alexandria, Australia
2School of Information Technologies, University
of Sydney, Sydney, NSW, Australia
3Department of Computer Science, Florida
International University, Miami, FL, USA
4School of Computing and Information
Sciences, Florida International University,
Miami, FL, USA
5School of Computer Science, Carleton
University, Ottawa, ON, Canada
Keywords
Approximation
algorithms;
Cluster
graphs;
Dilation;
Distance
oracles;
Shortest
paths;
Spanners

Applications of Geometric Spanner Networks
87
A
Years and Authors of Summarized
Original Work
2002; Gudmundsson, Levcopoulos, Narasimhan,
Smid
2005; Gudmundsson, Narasimhan, Smid
2008; Gudmundsson, Levcopoulos, Narasimhan,
Smid
Problem Deﬁnition
Given a geometric graph in d-dimensional space,
it is useful to preprocess it so that distance
queries, exact or approximate, can be answered
efﬁciently. Algorithms that can report distance
queries in constant time are also referred to as
“distance oracles.” With unlimited preprocessing
time and space, it is clear that exact distance
oracles can be easily designed. This entry sheds
light on the design of approximate distance
oracles with limited preprocessing time and space
for the family of geometric graphs with constant
dilation.
Notation and Deﬁnitions
If p and q are points in Rd, then the notation jpqj
is used to denote the Euclidean distance between
p and q; the notation ıG.p; q/ is used to denote
the Euclidean length of a shortest path between
p and q in a geometric network G. Given a
constant t > 1, a graph G with vertex set S is
a t-spanner for S if ıG.p; q/  tjpqj for any
two points p and q of S. A t-spanner network
is said to have dilation (or stretch factor) t. A
.1 C "/-approximate shortest path between p and
q is deﬁned to be any path in G between p and
q having length , where ıG.p; q/   
.1C"/ıG.p; q/. For a comprehensive overview of
geometric spanners, see the book by Narasimhan
and Smid [14].
All networks considered in this entry are sim-
ple and undirected. The model of computation
used is the traditional algebraic computation tree
model with the added power of indirect address-
ing. In particular, the algorithms presented here
do not use the non-algebraic ﬂoor function as a
unit-time operation. The problem is formalized
below.
Problem 1 (Distance Oracle) Given an arbi-
trary real constant 
>
0, and a geometric
graph G in d-dimensional Euclidean space with
constant dilation t, design a data structure that
answers .1C/-approximate shortest path length
queries in constant time.
The data structure can also be applied to
solve several other problems. These include (a)
the problem of reporting approximate distance
queries between vertices in a planar polygonal
domain with “rounded” obstacles, (b) query
versions of closest pair problems, and (c)
the efﬁcient computation of the approximate
dilations of geometric graphs.
Survey of Related Research
The design of efﬁcient data structures for
answering distance queries for general (non-
geometric) networks was considered by Thorup
and Zwick [17] (unweighted general graphs),
Baswanna and Sen [3] (weighted general graphs,
i.e., arbitrary metrics), and Arikati et al. [2] and
Thorup [16] (weighted planar graphs).
For the geometric case, variants of the problem
have been considered in a number of papers (for
a recent paper, see, e.g., Chen et al. [5]). Work
on the approximate version of these variants can
also be found in many articles (for a recent
paper, see, e.g., Agarwal et al. [1]). The focus
of this entry is the results reported in the work
of Gudmundsson et al. [10–13]. Similar results
on distance oracles were proved subsequently for
unit disk graphs [7]. Practical implementations of
distance oracles in geometric networks have also
been investigated [15].
Key Results
The main result of this entry is the existence
of approximate distance oracle data structures
for geometric networks with constant dilation
(see Theorem 4 below). As preprocessing, the
network is “pruned” so that it only has a linear
number of edges. The data structure consists of

88
Applications of Geometric Spanner Networks
a series of “cluster graphs” of increasing coarse-
ness, each of which helps answer approximate
queries for pairs of points with interpoint dis-
tances of different scales. In order to pinpoint
the appropriate cluster graph to search in for a
given query, the data structure uses the bucketing
tool described below. The idea of using cluster
graphs to speed up geometric algorithms was ﬁrst
introduced by Das and Narasimhan [6] and later
used by Gudmundsson et al. [9] to design an
efﬁcient algorithm to compute .1 C "/-spanners.
Similar ideas were explored by Gao et al. [8] for
applications to the design of mobile networks.
Pruning
If the input geometric network has a superlinear
number of edges, then the preprocessing step
for the distance oracle data structure involves
efﬁciently “pruning” the network so that it has
only a linear number of edges. The pruning may
result in a small increase of the dilation of the
spanner. The following theorem was proved by
Gudmundsson et al. [12].
Theorem 1 Let t
> 1 and "0 > 0 be real
constants. Let S be a set of n points in Rd,
and let G D .S; E/ be a t-spanner for S with
m edges. There exists an algorithm to compute
in O.m C n log n/ time, a .1 C "0/-spanner
of G having O.n/ edges and whose weight is
O.wt.MST .S///.
The pruning step requires the following technical
theorem proved by Gudmundsson et al. [12].
Theorem 2 Let S be a set of n points in Rd, and
let c  7 be an integer constant. In O.n log n/
time, it is possible to compute a data structure
D.S/ consisting of:
1. A sequence L1; L2; : : : ; L` of real numbers,
where ` D O.n/, and
2. A sequence S1; S2; : : : ; S` of subsets of S such
that P`
iD1 jSij D O.n/,
such that the following holds. For any two distinct
points p and q of S, it is possible to compute in
O.1/ time an index i with 1  i  ` and two
points x and y in Si such that (a) Li=ncC1 
jxyj < Li and (b) both jpxj and jqyj are less
than jxyj=nc2.
Despite its technical nature, the above theorem
is of fundamental importance to this work. In
particular, it helps to deal with networks where
the interpoint distances are not conﬁned to a
polynomial range, i.e., there are pairs of points
that are very close to each other and very far from
each other.
Bucketing
Since the model of computation assumed here
does not allow the use of ﬂoor functions, an
important component of the algorithm is a “buck-
eting tool” that allows (after appropriate prepro-
cessing) constant-time computation of a quantity
referred to as BINDEX, which is deﬁned to be the
ﬂoor of the logarithm of the interpoint distance
between any pair of input points.
Theorem 3 Let S be a set of n points in Rd that
are contained in the hypercube .0; nk/d, for some
positive integer constant k, and let " be a positive
real constant. The set S can be preprocessed
in O.n log n/ time into a data structure of size
O.n/, such that for any two points p and q of
S, with jpqj  1, it is possible to compute
in constant time the quantity BINDEX".p; q/ D
blog1C" jpqjc:
The constant-time computation mentioned in
Theorem 3 is achieved by reducing the prob-
lem to one of answering least common ancestor
queries for pairs of nodes in a tree, a problem for
which constant-time solutions were devised most
recently by Bender and Farach-Colton [4].
Main Results
Using the bucketing and the pruning tools, and
using the algorithms described by Gudmundsson
et al. [13], the following theorem can be proved.
Theorem 4 Let t
> 1 and " > 0 be real
constants. Let S be a set of n points in Rd, and let
G D .S; E/ be a t-spanner for S with m edges.
The graph G can be preprocessed into a data
structure of size O.n log n/ in time O.mn log n/,

Applications of Geometric Spanner Networks
89
A
such that for any pair of query points p; q 2 S,
it is possible to compute a .1 C "/-approximation
of the shortest path distance in G between p and
q in O.1/ time. Note that all the big-Oh notations
hide constants that depend on d, t, and ".
Additionally, if the traditional algebraic model
of computation (without indirect addressing) is
assumed, the following weaker result can be
proved.
Theorem 5 Let S be a set of n points in Rd,
and let G D .S; E/ be a t-spanner for S, for
some real constant t
> 1, having m edges.
Assuming the algebraic model of computation,
in O.m log log n C n log2 n/ time, it is possible
to preprocess G into a data structure of size
O.n log n/, such that for any two points p and q
in S, a .1C"/-approximation of the shortest-path
distance in G between p and q can be computed
in O.log log n/ time.
Applications
As mentioned earlier, the data structure described
above can be applied to several other problems.
The ﬁrst application deals with reporting distance
queries for a planar domain with polygonal ob-
stacles. The domain is further constrained to be
t-rounded, which means that the length of the
shortest obstacle-avoiding path between any two
points in the input point set is at most t times the
Euclidean distance between them. In other words,
the visibility graph is required to be a t-spanner
for the input point set.
Theorem 6 Let F be a t-rounded collection of
polygonal obstacles in the plane of total com-
plexity n, where t is a positive constant. One can
preprocess F in O.n log n/ time into a data struc-
ture of size O.n log n/ that can answer obstacle-
avoiding .1C"/-approximate shortest path length
queries in time O.log n/. If the query points are
vertices of F, then the queries can be answered
in O.1/ time.
The next application of the distance oracle data
structure includes query versions of closest pair
problems, where the queries are conﬁned to spec-
iﬁed subset(s) of the input set.
Theorem 7 Let G D .S; E/ be a geometric
graph on n points and m edges, such that G is
a t-spanner for S, for some constant t > 1. One
can preprocess G in time O.m C n log n/ into a
data structure of size O.n log n/ such that given
a query subset S0 of S, a .1 C "/-approximate
closest pair in S0 (where distances are measured
in G) can be computed in time O.jS0j log jS0j/.
Theorem 8 Let G D .S; E/ be a geometric
graph on n points and m edges, such that G is
a t-spanner for S, for some constant t > 1. One
can preprocess G in time O.m C n log n/ into a
data structure of size O.n log n/ such that given
two disjoint query subsets X and Y of S, a .1 C
"/-approximate bichromatic closest pair (where
distances are measured in G) can be computed in
time O..jXj C jY j/ log.jXj C jY j//.
The last application of the distance oracle data
structure includes the efﬁcient computation of the
approximate dilations of geometric graphs.
Theorem 9 Given a geometric graph on n ver-
tices with m edges, and given a constant C that
is an upper bound on the dilation t of G, it is
possible to compute a .1 C "/-approximation to t
in time O.m C n log n/.
Open Problems
Two open problems remain unanswered:
1. Improve the space utilization of the distance
oracle data structure from O.n log n/ to O.n/.
2. Extend the approximate distance oracle data
structure to report not only the approximate
distance but also the approximate shortest path
between the given query points.
Cross-References
▷Geometric Spanners
▷Sparse Graph Spanners

90
Approximate Dictionaries
Recommended Reading
1. Agarwal PK, Har-Peled S, Karia M (2000) Comput-
ing approximate shortest paths on convex polytopes.
In: Proceedings of the 16th ACM symposium on
computational geometry, Hong Kong, pp 270–279
2. Arikati S, Chen DZ, Chew LP, Das G, Smid M, Zaro-
liagis CD (1996) Planar spanners and approximate
shortest path queries among obstacles in the plane.
In: Proceedings of the 4th annual european sym-
posium on algorithms, Barcelona. Lecture notes in
computer science, vol 1136. Springer, Berlin, pp 514–
528
3. Baswana S, Sen S (2004) Approximate distance
oracles for unweighted graphs in
Q
O.n2/ time. In:
Proceedings of the 15th ACM-SIAM symposium on
discrete algorithms, Philadelphia, pp 271–280
4. Bender MA, Farach-Colton M (2000) The LCA prob-
lem revisited. In: Proceedings of the 4th Latin Amer-
ican symposium on theoretical informatics, Punta del
Este. Lecture notes in computer science, vol 1776.
Springer, Berlin, pp 88–94
5. Chen DZ, Daescu O, Klenk KS (2001) On geomet-
ric path query problems. Int J Comput Geom Appl
11:617–645
6. Das G, Narasimhan G (1997) A fast algorithm for
constructing sparse Euclidean spanners. Int J Comput
Geom Appl 7:297–315
7. Gao J, Zhang L (2005) Well-separated pair decompo-
sition for the unit-disk graph metric and its applica-
tions. SIAM J Comput 35(1):151–169
8. Gao J, Guibas LJ, Hershberger J, Zhang L, Zhu
A (2003) Discrete mobile centers. Discret Comput
Geom 30:45–63
9. Gudmundsson J, Levcopoulos C, Narasimhan G
(2002) Fast greedy algorithms for constructing sparse
geometric spanners. SIAM J Comput 31:1479–1500
10. Gudmundsson J, Levcopoulos C, Narasimhan G,
Smid M (2002) Approximate distance oracles for
geometric graphs. In: Proceedings of the 13th ACM-
SIAM symposium on discrete algorithms, San Fran-
cisco, pp 828–837
11. Gudmundsson J, Levcopoulos C, Narasimhan G,
Smid M (2002) Approximate distance oracles revis-
ited. In: Proceedings of the 13th international sympo-
sium on algorithms and computation, Osaka. Lecture
notes in computer science, vol 2518. Springer, Berlin,
pp 357–368
12. Gudmundsson J, Narasimhan G, Smid M (2005) Fast
pruning of geometric spanners. In: Proceedings of the
22nd symposium on theoretical aspects of computer
science, Stuttgart. Lecture notes in computer science,
vol 3404. Springer, Berlin, pp 508–520
13. Gudmundsson J, Levcopoulos C, Narasimhan G,
Smid M (2008) Approximate distance oracles for ge-
ometric spanners. ACM Trans Algorithms 4(1):article
10
14. Narasimhan G, Smid M (2007) Geometric spanner
networks. Cambridge University, Cambridge
15. Sankaranarayanan J, Samet H (2010) Query process-
ing using distance oracles for spatial networks. IEEE
Trans Knowl Data Eng 22(8):1158–1175
16. Thorup M (2004) Compact oracles for reachability
and approximate distances in planar digraphs. J ACM
51:993–1024
17. Thorup M, Zwick U (2001) Approximate distance
oracles. In: Proceedings of the 33rd annual ACM
symposium on the theory of computing, Crete,
pp 183–192
Approximate Dictionaries
Venkatesh Srinivasan
Department of Computer Science, University of
Victoria, Victoria, BC, Canada
Keywords
Cell probe model; Data structures; Static mem-
bership
Years and Authors of Summarized
Original Work
2002;
Buhrman,
Miltersen,
Radhakrishnan,
Venkatesh
Problem Deﬁnition
The Problem and the Model
A static data structure problem consists of a set
of data D, a set of queries Q, a set of answers A,
and a function f W D  Q ! A. The goal is to
store the data succinctly, so that any query can be
answered with only a few probes to the data struc-
ture. Static membership is a well-studied problem
in data structure design [2,6,9,10,16,17,23].
Deﬁnition 1 (Static Membership) In the static
membership problem, one is given a subset
S of at most n keys from a universe U
D
f1; 2; : : : ; mg. The task is to store S so that
queries of the form “Is u in S?” can be answered
by making few accesses to the memory.

Approximate Dictionaries
91
A
A natural and general model for studying any
data structure problem is the cell probe model
proposed by Yao [23].
Deﬁnition 2 (Cell Probe Model) An .s; w; t/
cell probe scheme for a static data structure
problem f W D  Q ! A has two components: a
storage scheme and a query scheme. The storage
scheme stores the data d 2 D as a Table T Œd of
s cells, each cell of word size w bits. The storage
scheme is deterministic. Given a query q 2 Q,
the query scheme computes f .d; q/ by making
at most t probes to T Œd, where each probe reads
one cell at a time, and the probes can be adaptive.
In a deterministic cell probe scheme, the query
scheme is deterministic. In a randomized cell
probe scheme, the query scheme is randomized
and is allowed to err with a small probability.
Buhrman et al. [3] study the complexity of
the static membership problem in the bitprobe
model. The bitprobe model is a variant of the
cell probe model in which each cell holds just a
single bit. In other words, the word size w is 1.
Thus, in this model, the query algorithm is given
bitwise access to the data structure. The study of
the membership problem in the bitprobe model
was initiated by Minsky and Papert in their book
Perceptrons [16]. However, they were interested
in average-case upper bounds for this problem,
while this work studies worst-case bounds for the
membership problem.
Observe that if a scheme is required to
store sets of size at most n, then it must use
at least dlog P
in
m
i

e number of bits. If
n  m1	.1/, this implies that the scheme must
store .n log m/ bits (and therefore use .n/
cells). The goal in [3] is to obtain a scheme that
answers queries, uses only constant number of
bitprobes, and at the same time uses a table of
O.n log m/ bits.
Related Work
The static membership problem has been well
studied in the cell probe model, where each cell
is capable of holding one element of the universe.
That is, w D O.log m/. In a seminal paper,
Fredman, Komlós, and Szemerédi [10] proposed
a scheme for the static membership problem in
the cell probe model with word size O.log m/
that used a constant number of probes and a table
of size O.n/. This scheme will be referred to as
the FKS scheme. Thus, up to constant factors,
the FKS scheme uses optimal space and number
of cell probes. In fact, Fiat et al. [9], Brodnik
and Munro [2], and Pagh [17] obtain schemes
that use space (in bits) that is within a small
additive term of dlog P
in
m
i

e and yet answer
queries by reading at most a constant number
of cells. Despite all these fundamental results
for the membership problem in the cell probe
model, very little was known about the bitprobe
complexity of static membership prior to the
work in [3].
Key Results
Buhrman et al. investigate the complexity of the
static membership problem in the bitprobe model.
They study
•
Two-sided error randomized schemes that are
allowed to err on positive instances as well as
negative instances (i.e., these schemes can say
“No” with a small probability when the query
element u is in the set S and “Yes” when it is
not).
•
One-sided error randomized schemes where
the errors are restricted to negative instances
alone (i.e., these schemes never say “No”
when the query element u is in the set S);
•
Deterministic schemes in which no errors are
allowed.
The main techniques used in [3] are based
on 2-colorings of special set systems that are
related to r-cover-free family of sets considered
in [5, 7, 11]. The reader is referred to [3] for
further details.
Randomized Schemes with Two-Sided
Error
The main result in [3] shows that there are ran-
domized schemes that use just one bitprobe and

92
Approximate Dictionaries
yet use space close to the information theoretic
lower bound of .n log m/ bits.
Theorem 1 For any 0 <  
1
4, there is a
scheme for storing subsets S of size at most n
of a universe of size m using O

n
2 log m

bits
so that any membership query “Is u 2 S?” can
be answered with error probability at most  by a
randomized algorithm which probes the memory
at just one location determined by its coin tosses
and the query element u.
Note that randomization is allowed only in the
query algorithm. It is still the case that for each
set S, there is exactly one associated data struc-
ture T .S/. It can be shown that deterministic
schemes that answer queries using a single bit-
probe need m bits of storage (see the remarks
following Theorem 4). Theorem 1 shows that, by
allowing randomization, this bound (for constant
) can be reduced to O.n log m/ bits. This space
is within a constant factor of the information
theoretic bound for n sufﬁciently small. Yet,
the randomized scheme answers queries using a
single bitprobe.
Unfortunately, the construction above does not
permit us to have sub-constant error probability
and still use optimal space. Is it possible to im-
prove the result of Theorem 1 further and design
such a scheme? Buhrman et al. [3] shows that this
is not possible: if  is made sub-constant, then the
scheme must use more than n log m space.
Theorem 2 Suppose
n
m1=3
  
1
4. Then,
any two-sided -error randomized scheme which
answers queries using one bitprobe must use
space 

n
 log.1=/ log m

.
Randomized Schemes with One-Sided
Error
Is it possible to have any savings in space if the
query scheme is expected to make only one-sided
errors? The following result shows it is possible
if the error is allowed only on negative instances.
Theorem 3 For any 0 <  
1
4, there is a
scheme for storing subsets S of size at most n of
a universe of size m using O
 n

2 log m

bits
so that any membership query “Is u 2 S?” can
be answered with error probability at most  by
a randomized algorithm which makes a single
bitprobe to the data structure. Furthermore, if
u 2 S, the probability of error is 0.
Though this scheme does not operate with op-
timal space, it still uses signiﬁcantly less space
than a bitvector. However, the dependence on n is
quadratic, unlike in the two-sided scheme where
it was linear. Buhrman et al. [3] shows that this
scheme is essentially optimal: there is necessarily
a quadratic dependence on n
 for any scheme with
one-sided error.
Theorem 4 Suppose
n
m1=3   
1
4. Consider
the static membership problem for sets S of
size at most n from a universe of size m. Then,
any scheme with one-sided error  that answers
queries using at most one bitprobe must use


n2
2 log.n=/ log m

bits of storage.
Remark 1 One could also consider one-probe
one-sided error schemes that only make errors on
positive instances. That is, no error is made for
query elements not in the set S. In this case, [3]
shows that randomness does not help at all: such
a scheme must use m bits of storage.
The following result shows that the space
requirement can be reduced further in one-sided
error schemes if more probes are allowed.
Theorem 5 Suppose 0
<
ı
<
1. There is
a randomized scheme with one-sided error nı
that solves the static membership problem using
O

n1Cı log m

bits of storage and O
 1
ı

bit-
probes.
Deterministic Schemes
In contrast to randomized schemes, Buhrman
et al. show that deterministic schemes exhibit a
time-space tradeoff behavior.
Theorem 6 Suppose
a
deterministic
scheme
stores subsets of size n from a universe of
size m using s bits of storage and answers
membership queries with t bitprobes to memory.
Then,
m
n

 maxint
2s
i

.
This tradeoff result has an interesting conse-
quence. Recall that the FKS hashing scheme is

Approximate Dictionaries
93
A
a data structure for storing sets of size at most
n from a universe of size m using O.n log m/
bits, so that membership queries can be answered
using O.log m/ bitprobes. As a corollary of the
tradeoff result, [3] show that the FKS scheme
makes an optimal number of bitprobes, within a
constant factor, for this amount of space.
Corollary 1 Let  > 0; c  1 be any constants.
There is a constant ı > 0 so that the following
holds. Let n  m1 and let a scheme for storing
sets of size at most n of a universe of size m
as data structures of at most cn log m bits be
given. Then, any deterministic algorithm answer-
ing membership queries using this structure must
make at least ı log m bitprobes in the worst case.
From Theorem 6, it also follows that any
deterministic scheme that answers queries using t
bitprobes must use space at least 

tm1=tn11=t
in the worst case. The ﬁnal result shows the
existence of schemes which almost match the
lower bound.
Theorem 7 1. There is a nonadaptive scheme
that stores sets of size at most n from a
universe of size m using O

ntm
2
tC1

bits and
answers queries using 2t C 1 bitprobes. This
scheme is non-explicit.
2. There is an explicit adaptive scheme that
stores sets of size at most n from a universe
of size m using O

m1=tn log m

bits and
answers queries using O.log nClog log m/Ct
bitprobes.
Power of Few Bitprobes
In this section, we highlight some of recent re-
sults for this problem subsequent to [3] and en-
courage the reader to read the corresponding
references for more details. Most of these results
focus on the power of deterministic schemes with
a small number of bitprobes.
Let S.m; n; t/ denote the minimum number of
bits of storage needed by a deterministic scheme
that answers queries using t (adaptive) bitprobes.
In [3], it was shown that S.m; n; 1/ D m and
S.m; n; 5/ D o.m/ for n D o

m1=3
(Theo-
rem 7, Part 1). This leads us to a natural question:
Is S.m; n; t/ D o.m/ for t = 2, 3, and 4 and under
what conditions on n?
Initial progress for the case t D 2 was made
by [18] who considered the simplest case: n D 2.
They showed that S.m; 2; 2/
D
O

m2=3
.
It was later shown in [19] that S.m; 2; 2/ D


m4=7
. The upper bound result of [18] was
improved upon by the authors of [1] who showed
that S.m; n; 2/
D
o.m/ if n
D
o.log m/.
Interestingly, a matching lower bound was shown
recently in [12]: S.m; n; 2/ D o.m/ only if
n D o.log m/.
Strong upper bounds were obtained by [1] for
the case t D 3 and t D 4. They showed that
S.m; n; 3/ D o.m/ whenever n D o.m/. They
also showed that S.m; n; 4/ D o.m/ for n D
o.m/ even if the four bitprobes are nonadaptive.
Recently, it was shown in [14] that S.m; 2; 3/ 
7m2=5. This work focuses on explicit schemes for
n D 2 and t  3.
Finally, we end with two remarks. Our prob-
lem for the case n D ‚.m/ has been studied by
Viola [22]. A recent result of Chen, Grigorescu,
and de Wolf [4] studies our problem in the pres-
ence of adversarial noise.
Applications
The results in [3] have interesting connections to
questions in coding theory and communication
complexity. In the framework of coding theory,
the results in [3] can be viewed as constructing
locally decodable source codes, analogous to the
locally decodable channel codes of [13]. The-
orems 1–4 can also be viewed as giving tight
bounds for the following communication com-
plexity problem (as pointed out in [15]): Alice
gets u 2 f1; : : : ; mg, Bob gets S  f1; : : : ; mg of
size at most n, and Alice sends a single message
to Bob after which Bob announces whether u 2
S. See [3] for further details.
Recommended Reading
1. Alon N and Feige U (2009) On the power of two,
three and four probes. In: Proceedings of SODA’09,
New York, pp 346–354

94
Approximate Distance Oracles with Improved Query Time
2. Brodnik A, Munro JI (1994) Membership in constant
time and minimum space. In: Algorithms ESA’94:
second annual European symposium, Utrecht. Lec-
ture notes in computer science, vol 855, pp 72–81. Fi-
nal version: Membership in constant time and almost-
minimum space. SIAM J Comput 28(5):1627–1640
(1999)
3. Buhrman
H,
Miltersen
PB,
Radhakrishnan
J,
Venkatesh S (2002) Are bitvectors optimal? SIAM J
Comput 31(6):1723–1744
4. Chen V, Grigorescu E, de Wolf R (2013)
Error-
correcting data structures. SIAM J Comput 42(1):84–
111
5. Dyachkov AG, Rykov VV (1982)
Bounds on the
length of disjunctive codes.
Problemy Peredachi
Informatsii 18(3):7–13 [Russian]
6. Elias P, Flower RA (1975) The complexity of some
simple retrieval problems.
J Assoc Comput Mach
22:367–379
7. Erd˝os P, Frankl P, Füredi Z (1985) Families of ﬁnite
sets in which no set is covered by the union of r
others. Isr J Math 51:79–89
8. Fiat A, Naor M (1993) Implicit O.1/ probe search.
SIAM J Comput 22:1–10
9. Fiat A, Naor M, Schmidt JP, Siegel A (1992) Non-
oblivious hashing. J Assoc Comput Mach 31:764–
782
10. Fredman ML, Komlós J, Szemerédi E (1984) Storing
a sparse table with O.1/ worst case access time. J
Assoc Comput Mach 31(3):538–544
11. Füredi Z (1996) On r-cover-free families. J Comb
Theory Ser A 73:172–173
12. Garg M, Radhakrishnan J (2015)
Set membership
with a few bit probes. In: Proceedings of SODA’15,
San Diego, pp 776–784
13. Katz J, Trevisan L (2000) On the efﬁciency of local
decoding procedures for error-correcting codes. In:
Proceedings of STOC’00, Portland, pp 80–86
14. Lewenstein M, Munro JI, Nicholson PK, Raman V
(2014) Improved explicit data structures in the bit-
probe model. In: Proceedings of ESA’14, Wroclaw,
pp 630–641
15. Miltersen PB, Nisan N, Safra S, Wigderson A (1998)
On data structures and asymmetric communication
complexity. J Comput Syst Sci 57:37–49
16. Minsky M, Papert S (1969)
Perceptrons.
MIT,
Cambridge
17. Pagh R (1999) Low redundancy in static dictionaries
with O.1/ lookup time. In: Proceedings of ICALP
’99, Prague. Lecture notes in computer science, vol
1644, pp 595–604
18. Radhakrishnan J, Raman V, Rao SS (2001) Explicit
deterministic constructions for membership in the
bitprobe model. In: Proceedings of ESA’01, Aarhus,
pp 290–299
19. Radhakrishnan J, Shah S, Shannigrahi S (2010) Data
structures for storing small sets in the bitprobe model.
In: Proceedings of ESA’10, Liverpool, pp 159–
170
20. Ruszinkó M (1984) On the upper bound of the size of
r-cover-free families. J Comb Theory Ser A 66:302–
310
21. Ta-Shma A (2002)
Explicit one-probe storing
schemes using universal extractors. Inf Process Lett
83(5):267–274
22. Viola E (2012) Bit-probe lower bounds for succinct
data structures. SIAM J Comput 41(6):1593–1604
23. Yao ACC (1981) Should tables be sorted? J Assoc
Comput Mach 28(3):615–628
Approximate Distance Oracles with
Improved Query Time
Christian Wulff-Nilsen
Department of Computer Science, University of
Copenhagen, Copenhagen, Denmark
Keywords
Approximate distance oracle; Graphs; Query
time; Shortest paths
Years and Authors of Summarized
Original Work
2013; Wulff-Nilsen
Problem Deﬁnition
This problem is concerned with obtaining a com-
pact data structure capable of efﬁciently reporting
approximate shortest path distance queries in
a given undirected edge-weighted graph G D
.V; E/. If the query time is independent (or nearly
independent) of the size of G, we refer to the
data structure as an approximate distance oracle
for G. For vertices u and v in G, we denote by
dG.u; v/ the shortest path distance between u and
v in G. For a given stretch parameter ı  1, we
call the oracle ı-approximate if for all vertices u
and v in G, dG.u; v/  QdG.u; v/  ıdG.u; v/,
where QdG.u; v/ is the output of the query for u
and v. Hence, we allow estimates to be stretched
by a factor up to ı but not shrunk.

Approximate Distance Oracles with Improved Query Time
95
A
Key Results
A major result in the area of distance oracles is
due to Thorup and Zwick [4]. They gave, for
every integer k  1, a .2k  1/-approximate
distance oracle of size O

kn1C1=k
and query
time O.k/, where n is the number of vertices of
the graph. This is constant query time when k
is constant. Corresponding approximate shortest
paths can be reported in time proportional to
their length. Mendel and Naor [3] asked the
question of whether query time can be improved
to a universal constant (independent also of k)
while keeping both size and stretch small. They
obtained O

n1C1=k
size and O.1/ query time
at the cost of a constant-factor increase in stretch
to 128k. Unlike the oracle of Thorup and Zwick,
Mendel and Naor’s oracle is not path reporting.
In [5], it is shown how to improve the query
time of Thorup-Zwick to O.log k/ without
increasing space or stretch. This is done while
keeping essentially the same data structure but
applying binary instead of linear search in so-
called bunch structures that were introduced by
Thorup and Zwick [4]; the formal deﬁnition
will be given below. Furthermore, it is shown
in [5] how to improve the stretch of the oracle of
Mendel and Naor to .2 C /k for an arbitrarily
small constant  > 0 while keeping query time
constant (bounded by 1=). This improvement is
obtained without an increase in space except for
large values of k close to log n (only values of k
less than log n are interesting since the Mendel-
Naor oracle has optimal O.n/ space and O.1/
query time for larger values). Below, we sketch
the main ideas in the improvement of Thorup-
Zwick and of Mendel-Naor, respectively.
Oracle with O.log k/ Query Time
The oracle of Thorup and Zwick keeps a hier-
archy of sets of sampled vertices V D A0 
A1  A2 : : :  Ak
D ;, where for i
D
1; : : : ; k  1, Ai is obtained by picking each
element of Ai1 independently with probability
n1=k. Deﬁne pi.u/ as the vertex in Ai closest
to u. The oracle precomputes and stores for each
vertex u 2 V the bunch Bu, deﬁned as
Bu D
k1
[
iD0
˚
v 2 Ai n AiC1jdG.u; v/
< dG.u; piC1.u//

:
See Fig. 1 for an illustration of a bunch. The
distance dG.u; v/ for each v 2 Bu is precomputed
as well.
Now, to answer a query for a vertex pair
.u; v/, the oracle performs a linear search through
bunches Bu and Bv. Pseudocode is given in
Fig. 2. It is clear that query time is O.k/, and it
can be shown that the estimate output in line 6
has stretch 2k  1.
p2(u)
p1(u)
u = p0(u)
Approximate Distance Oracles with Improved Query
Time, Fig. 1 A bunch Bu in a complete Euclidean graph
with k D 3. Black vertices belong to A0, grey vertices to
A1, and white vertices to A2. Line segments connect u to
vertices of Bu
Algorithm distk(u, v)
1. w ← p0(u); j ←0
2. while w ∈ Bv
3.    j ← j + 1
4.
(u, v) ← (v, u)
5.    w ← pj(u)
6. return dG(w, u) + dG(w, v)
Approximate Distance Oracles with Improved Query
Time, Fig. 2 Answering a distance query, starting at
sample level i

96
Approximate Distance Oracles with Improved Query Time
We can improve query time to O.log k/
by instead doing binary search in the bunch
structures. A crucial property of the Thorup-
Zwick oracle is that every time the test in line
2 succeeds, dG.u; pj .u// increases by at most
dG.u; v/, and this is sufﬁcient to prove 2k  1
stretch. In particular, if the test succeeds two
times in a row, dG.u; pj C2.u//dG.u; pj .u// 
2dG.u; v/, where j is even. If we can check that
dG.u; pj 0C2.u//  dG.u; pj 0.u//  2dG.u; v/
for all smaller even indices j 0, we may start the
query algorithm at index j instead of index 0.
Since we would like to apply binary search, pick
j to be (roughly) k=2. It sufﬁces to check only
one inequality, namely, the one with the largest
value dG.u; pj 0C2.u//  dG.u; pj 0.u//. Note
that this value depends only on u and k, so we
can precompute the index j 0 with this largest
value. In the query phase, we can check in O.1/
time whether dG.u; pj 0C2.u//dG.u; pj 0.u// 
2dG.u; v/. If the test succeeds, we can start the
query at j , and hence, we can recurse on indices
between j and k  1. Conversely, if the test fails,
it means that the test in line 2 fails for either j 0 or
j 0 C1. Hence, the query algorithm of Thorup and
Zwick terminates no later than at index j 0 C 1,
and we can recurse on indices between 0 and
j 0 C 1. In both cases, the number of remaining
indices is reduced by a factor of at least 2. Since
each recursive call takes O.1/ time, we thus
achieve O.log k/ query time.
Since the improved oracle is very similar to the
Thorup-Zwick oracle, it is path reporting, i.e., it
can report approximate paths in time proportional
to their length.
Oracle with Constant Query Time
The second oracle in [5] can be viewed as a
hybrid between the oracles of Thorup-Zwick and
of Mendel-Naor. An initial estimate is obtained
by querying the Mendel-Naor oracle. This esti-
mate has stretch at most 128k, and it is reﬁned
in subsequent iterations until the desired stretch
.2C/k is obtained. In each iteration, the current
estimate is reduced by a small constant factor
greater than 1 (depending on ). Note that after
a constant number of iterations, the estimate will
be below the desired stretch, but it needs to be
ensured that it is not below the shortest path
distance.
In each iteration, the hybrid algorithm at-
tempts to start the Thorup-Zwick query algorithm
at a step corresponding to this estimate. If this can
be achieved, only a constant number of steps of
this query algorithm need to be executed before
the desired stretch is obtained. Conversely, if
the hybrid algorithm fails to access the Thorup-
Zwick oracle in any iteration, then by a prop-
erty of the bunch structures, it is shown that
the current estimate is not below the shortest
path distance. Hence, the desired stretch is again
obtained.
An important property of the Mendel-Naor
oracle needed above is that the set dMN of all
different values the oracle can output has size
bounded by O

n1C1=k
. This is used in the
hybrid algorithm as follows. In a preprocessing
step, values from dMN are ordered in a list L
together with additional values corresponding to
the intermediate estimates that the hybrid algo-
rithm can consider in an iteration. Updating the
estimate in each iteration then corresponds to a
linear traversal of part of L. Next, each vertex
pi of each bunch structure Bu of the Thorup-
Zwick oracle is associated with the value in the
list closest to dG.u; pi/. For each element of
L, a hash table is kept for the bunch vertices
associated with that element. It can be shown that
this way of linking the oracle of Thorup-Zwick
and Mendel-Naor achieves the desired.
Applications
The practical need for efﬁcient algorithms to
answer the shortest path (distance) queries in
graphs has increased signiﬁcantly over the years,
in large part due to emerging GPS navigation
technology and other route planning software.
Classical algorithms like Dijkstra do not scale
well as they may need to explore the entire graph
just to answer a single query. As road maps are
typically of considerable size, obtaining compact
distance oracles has received a great deal of
attention from the research community.

Approximate Matching
97
A
Open Problems
A widely believed girth conjecture of Erd˝os [2]
implies that an oracle with stretch 2k  1, size
O

n1C1=k
, and query time O.1/ would be op-
timal. Obtaining such an oracle (preferably one
that is path reporting) is a main open problem in
the area. Some progress has recently been made:
Chechik [1] gives an oracle (not path reporting)
with stretch 2k  1, size O

kn1C1=k
, and O.1/
query time.
Recommended Reading
1. Chechik S (2014) Approximate distance oracles with
constant query time. In: STOC, New York, pp 654–663
2. Erd˝os P (1964) Extremal problems in graph theory.
In: Theory of graphs and its applications (Proceedings
of the symposium on smolenice, 1963). Czechoslovak
Academy of Sciences, Prague, pp 29–36
3. Mendel M, Naor A (2007) Ramsey partitions and
proximity data structures. J Eur Math Soc 9(2):253–
275. See also FOCS’06
4. Thorup M, Zwick U (2005) Approximate distance
oracles. J Assoc Comput Mach 52:1–24
5. Wulff-Nilsen C (2013) Approximate distance oracles
with improved query time. In: Proceedings of the
24th ACM-SIAM symposium on discrete algorithms
(SODA), New Orleans, pp 202–208
Approximate Matching
Ran Duan
Institute for Interdisciplinary Information
Sciences, Tsinghua University, Beijing, China
Keywords
Approximation algorithms; Maximum matching
Years and Authors of Summarized
Original Work
1973; Hopcroft, Karp
1980; Micali, Vazirani
2014; Duan, Pettie
Problem Deﬁnition
In graph theory, a matching in a graph is a
set of edges without common vertices, while a
perfect matching is one in which all vertices
are associated with matching edges. In a graph
G D .V; E; w/, where V is the set of vertices,
E is the set of edges, and w W E ! R is
the edge weight function, the maximum matching
problem determines the matching M in G which
maximizes w.M/ D P
e2M w.e/. Note that a
maximum matching is not necessarily perfect.
The maximum cardinality matching (MCM) prob-
lem means the maximum matching problem for
w.e/ D 1 for all edges. Otherwise, it is called the
maximum weight matching (MWM).
Algorithms for Exact MWM
Although the maximum matching problem has
been studied for decades, the computational com-
plexity of ﬁnding an optimal matching remains
quite open. Most algorithms for graph matchings
use the concept of augmenting paths. An alternat-
ing path (or cycle) is one whose edges alternate
between M and EnM. An alternating path P is
augmenting if P begins and ends at free vertices,
that is, M ˚P
def
D .MnP /[.P nM/ is a matching
with cardinality jM ˚ P j D jMj C 1. Therefore,
the basic algorithm ﬁnds the maximum cardinal-
ity matching by ﬁnding an augmenting path in the
graph and adding it the matching each time, until
no more augmenting paths exist. The running
time for the basic algorithm will be O.mn/ where
m D jEj and n D jV j. The major improvement
over this for bipartite graphs is the Hopcroft-Karp
algorithm [10]. It ﬁnds a maximal set of vertex
disjoint shortest augmenting paths in each step
and shows that the length of shortest augmenting
paths will increase each time. The running time
of the Hopcroft-Karp algorithm is O.mpn/. Its
corresponding algorithm for general graphs is
given by Micali and Vazirani [14].
For the maximum weight matching (MWM)
and maximum weight perfect matching (MWPM),
the most classical algorithm is the Hungarian
algorithm [12] for bipartite graphs and the
Edmonds algorithm for general graphs [6, 7].

98
Approximate Matching
For fast implementations, Gabow and Tarjan [8]
gave
bit-scaling
algorithms
for
MWM
in
bipartite graphs running in O.mpn log.nN//
time, where the edge weights are integers
in
ŒN; : : : ; N .
Then,
they
also
gave
its
corresponding algorithm for general graphs [9].
Extending [15], Sankowski [18] gave an O.Nn!/
MWM algorithm for bipartite graphs (here, ! 
2:373 denotes the exponential of the complexity
of fast matrix multiplication (FMM) [2, 20]),
while Huang and Kavitha [11] obtained a similar
time bound for general graphs. We can see these
time complexities are still far from linear, which
shows the importance of fast approximation
algorithms.
Approximate Matching
Let a ı-MWM be a matching whose weight is
at least a ı fraction of the maximum weight
matching, where 0 < ı  1, and let ı-MCM be
deﬁned analogously.
It is well known that the greedy algorithm –
iteratively chooses the maximum weight edge not
incident to previously chosen edges – produces
a 1
2-MWM. A straightforward implementation of
this algorithm takes O.m log n/ time. Preis [3,17]
gave a 1
2-MWM algorithm running in linear time.
Vinkemeier and Hougardy [19] and Pettie and
Sanders [16] proposed several
 2
3  

-MWM al-
gorithms (see also [13]) running in O.m log 1/
time; each is based on iteratively improving a
matching by identifying sets of short weight-
augmenting paths and cycles.
Key Results
Approximate Maximum Cardinality
Matching
In fact, the Hopcroft-Karp algorithm [10] for bi-
partite graphs and Micali-Vazirani [14] algorithm
for general graphs both imply a .1  /-MCM
algorithm in O.1m/ time. We can search for a
maximal set of vertex disjoint shortest augment-
ing paths for k steps, and the matching obtained
is a

1 
1
kC1

-MCM.
Theorem 1 ([10,14]) In a general graph G, the
.1  /-MCM algorithm can be found in time
O.1m/.
Approximate Maximum Weighted
Matching
In 2014, Duan and Pettie [5] give the ﬁrst .1/-
MWM algorithm for arbitrary weighted graphs
whose running time is linear. In particular, we
show that such a matching can be found in
O.m1 log 1/ time, improving a preliminary
result of O.m2 log3 n/ running time by the
authors in 2010 [4]. This result leaves little room
for improvement. The main results are given in
the following two theorems:
Theorem 2 ([5]) In a general graph G with inte-
ger edge weights between Œ0; N , a .1/-MWM
can be computed in time O.m1 log N /.
Theorem 3 ([5]) In a general graph G with real
edge weights, a .1  /-MWM can be computed
in time O.m1 log 1/.
Unlike previous algorithms of approximation
ratios of 1=2 [3, 17] or 2=3 [16, 19], the new
algorithm does not ﬁnd weight-augmenting paths
and cycles directly, but follows a primal-dual
relaxation on the linear programming formulation
of MWM. This relaxed complementary slack-
ness approach relaxes the constraint of the dual
variables by a small amount, so that the itera-
tive process of the dual problem will converge
to an approximate solution much more quickly.
While it takes O.pn/ iterations of augmenting
to achieve a perfect matching, we proved that we
only need O.log N=/ iterations to achieve a .1
/-approximation. Also, we make the relaxation
“dynamic” by shrinking the relaxation when the
dual variables decrease by one half, so that ﬁnally
the relaxation is at most  times the edge weight
on each matching edge and very small on each
nonmatching edge, which gives an approximate
solution.
Applications
Graph matching is a fundamental combinatorial
problem that has a wide range of applications in

Approximate Regular Expression Matching
99
A
many ﬁelds, and it can also be building blocks
of other algorithms, such as the Christoﬁdes al-
gorithm [1] for approximate traveling salesman
problem. The approximate algorithm for maxi-
mum weight matching described above has linear
running time, much faster than the Hungarian
algorithm [12] and Edmonds [6,7] algorithm. It is
also much simpler than the Gabow-Tarjan scaling
algorithms [8,9] of QO.mpn/ running time. Thus,
it has a great impact both in theory and in real-
world applications.
Cross-References
▷Assignment Problem
▷Maximum Matching
Recommended Reading
1. Christoﬁdes N, GROUP CMUPPMSR (1976) Worst-
case analysis of a new heuristic for the trav-
elling salesman problem. Defense Technical In-
formation Center, http://books.google.de/books?id=
2A7eygAACAAJ
2. Coppersmith D, Winograd T (1987) Matrix multipli-
cation via arithmetic progressions. In: Proceedings of
19th ACM symposium on the theory of computing
(STOC), New York, pp 1–6
3. Drake D, Hougardy S (2003) A simple approximation
algorithm for the weighted matching problem. Inf
Process Lett 85:211–213
4. Duan R, Pettie S (2010) Approximating maximum
weight matching in near-linear time. In: Proceedings
51st IEEE symposium on foundations of computer
science (FOCS), Las Vegas, pp 673–682
5. Duan R, Pettie S (2014) Linear-time approximation
for maximum weight matching. J ACM 61(1):1:1–
1:23.
doi:10.1145/2529989,
http://doi.acm.org/10.
1145/2529989
6. Edmonds J (1965) Maximum matching and a polyhe-
dron with 0; 1-vertices. J Res Nat Bur Stand Sect B
69B:125–130
7. Edmonds J (1965) Paths, trees, and ﬂowers. Can J
Math 17:449–467
8. Gabow HN, Tarjan RE (1989) Faster scaling al-
gorithms for network problems. SIAM J Comput
18(5):1013–1036
9. Gabow HN, Tarjan RE (1991) Faster scaling algo-
rithms for general graph-matching problems. J ACM
38(4):815–853
10. Hopcroft JE, Karp RM (1973) An n5=2 algorithm
for maximum matchings in bipartite graphs. SIAM
J Comput 2:225–231
11. Huang CC, Kavitha T (2012) Efﬁcient algorithms for
maximum weight matchings in general graphs with
small edge weights. In: Proceedings of the twenty-
third annual ACM-SIAM symposium on discrete al-
gorithms, SODA’12, Kyoto. SIAM, pp 1400–1412.
http://dl.acm.org/citation.cfm?id=2095116.2095226
12. Kuhn HW (1955) The Hungarian method for the
assignment problem. Nav Res Logist Q 2:83–97
13. Mestre J (2006) Greedy in approximation algorithms.
In: Proceedings of the 14th conference on annual Eu-
ropean symposium, Zurich, vol 14. Springer, London,
pp 528–539. doi:10.1007/11841036_48, http://portal.
acm.org/citation.cfm?id=1276191.1276239
14. Micali S, Vazirani V (1980) An O.
p
jV j  jEj/
algorithm for ﬁnding maximum matching in general
graphs. In: Proceedings of 21st IEEE symposium on
foundations of computer science (FOCS), Syracuse,
pp 17–27
15. Mucha M, Sankowski P (2004) Maximum match-
ings via Gaussian elimination. In: Proceedings of
45th symposium on foundations of computer science
(FOCS), Rome, pp 248–255
16. Pettie S, Sanders P (2004) A simpler linear time
2=3   approximation to maximum weight match-
ing. Inf Process Lett 91(6):271–276
17. Preis R (1999) Linear time 1=2-approximation al-
gorithm for maximum weighted matching in general
graphs. In: Proceedings of 16th symposium on the-
oretical aspects of computer science (STACS), Trier.
LNCS, vol 1563, pp 259–269
18. Sankowski P (2006) Weighted bipartite matching in
matrix multiplication time. In: Proceedings of 33rd
international symposium on automata, languages, and
programming (ICALP), Venice, pp 274–285
19. Vinkemeier DED, Hougardy S (2005) A linear-time
approximation algorithm for weighted matchings in
graphs. ACM Trans Algorithms 1(1):107–122
20. Williams VV (2012) Multiplying matrices faster
than Coppersmith-Winograd. In: Proceedings of
the 44th symposium on theory of computing,
STOC’12, New York. ACM, New York, pp 887–
898. doi:10.1145/2213977.2214056, http://doi.acm.
org/10.1145/2213977.2214056
Approximate Regular Expression
Matching
Gonzalo Navarro
Department of Computer Science, University of
Chile, Santiago, Chile
Keywords
Regular expression matching allowing errors or
differences

100
Approximate Regular Expression Matching
Years and Authors of Summarized
Original Work
1995; Wu, Manber, Myers
Problem Deﬁnition
Given a text string T = t1t2 : : : tn and a regular
expression R of length m denoting language,
L.R/ over an alphabet † of size , and given a
distance function among strings d and a threshold
k, the approximate regular expression matching
(AREM) problem is to ﬁnd all the text positions
that ﬁnish a so-called approximate occurrence of
R in T , that is, compute the set fj; 9i; 1;  i 
j; 9P
2 L.R/; d.P; ti; : : : ; tj /  kg T; R,
and k are given together, whereas the algorithm
can be tailored for a speciﬁc d.
This entry focuses on the so-called weighted
edit distance, which is the minimum sum of
weights of a sequence of operations converting
one string into the other. The operations are inser-
tions, deletions, and substitutions of characters.
The weights are positive real values associated
to each operation and characters involved. The
weight of deleting a character c is written w.c !
"/, that of inserting c is written w."
! c/,
and that of substituting c by c ¤ c0 is written
w.c !c0). It is assumed w.c ! c/ = 0 for
all c
2
† [ " and the triangle inequality,
that is, w.x ! y/ C w.y ! ´/  w.x ! ´/ for
any x; y; ´; 2 † [ f"g. As the distance may be
asymmetric, it is also ﬁxed that d.A,B/ is the
cost of converting A into B. For simplicity and
practicality, m D o.n/ is assumed in this entry.
Key Results
The most versatile solution to the problem [3] is
based on a graph model of the distance computa-
tion process. Assume the regular expression R is
converted into a nondeterministic ﬁnite automa-
ton (NFA) with O.m/ states and transitions using
Thompson’s method [8]. Take this automaton
as a directed graph G.V ,E/ where edges are
labeled by elements in † [ f"g. A directed and
weighted graph G is built to solve the AREM
problem. G is formed by putting n + 1 copies
of G; G0; G1; : : : ; Gn and connecting them with
weights so that the distance computation reduces
to ﬁnding shortest paths in G.
More formally, the nodes of G are {vi, v 2 V ,
0 i  n}, so that vi is the copy of node v 2 V in
graph Gi . For each edge u
c! v in E, c 2 † [ ",
the following edges are added to graph G:
ui ! vi;
with weight w.c ! " /;
0  i  n:
ui ! ui C 1;
with weight w." ! ti C 1/;
0  i  n:
ui ! vi C 1;
with weight w.c ! ti C 1/;
0  i  n:
Assume for simplicity that G has initial state s
and a unique ﬁnal state f (this can always be
arranged). As deﬁned, the shortest path in G from
s0 to fn gives the smallest distance between T
and a string in L.R/. In order to adapt the graph
to the AREM problem, the weights of the edges
between si and si C 1 are modiﬁed to be zero.
Then, the AREM problem is reduced to com-
puting shortest paths. It is not hard to see that G
can be topologically sorted so that all the paths
to nodes in Gi are computed before all those to
Gi C 1. This way, it is not hard to solve this short-
est path problem in O(mnlog m/ time and O.m/
space. Actually, if one restricts the problem to the
particular case of network expressions, which are
regular expressions without Kleene closure, then
G has no loops and the shortest path computation
can be done in O(mn) time, and even better on
average [2].
The most delicate part in achieving O(mn)
time for general regular expressions [3] is to
prove that, given the types of loops that arise in
the NFAs of regular expressions, it is possible to
compute the distances correctly within each Gi
by (a/ computing them in a topological order of
Gi without considering the back edges introduced
by Kleene closures, (b/ updating path costs by
using the back edges once, and (c/ updating path

Approximate Regular Expression Matching
101
A
costs once more in topological order ignoring
back edges again.
Theorem 1 (Myers and Miller [3]) There exists
an O(mn) worst-case time solution to the AREM
problem under weighted edit distance.
It is possible to do better when the weights
are integer-valued, by exploiting the unit-cost
RAM model through a four-Russian technique
[10]. The idea is as follows. Take a small subex-
pression of R, which produces an NFA that will
translate into a small subgraph of each Gi . At
the time of propagating path costs within this
automaton, there will be a counter associated to
each node (telling the current shortest path from
s0/. This counter can be reduced to a number in
[0, k + 1], where k + 1 means “more than k.”
If the small NFA has r states, rdlog 2.k + 2)e
bits are needed to fully describe the counters
of the corresponding subgraph of Gi. Moreover,
given an initial set of values for the counters,
it is possible to precompute all the propagation
that will occur within the same subgraph of Gi,
in a table having 2rdlog2.k C 2/e entries, one per
possible conﬁguration of counters. It is sufﬁcient
that r
<
˛log k C 2n for some ˛
< 1 to
make the construction and storage cost of those
tables o.n/. With the help of those tables, all the
propagation within the subgraph can be carried
out in constant time. Similarly, the propagation
of costs to the same subgraph at Gi C 1 can also
be precomputed in tables, as it depends only on
the current counters in Gi and on text character
ti C 1, for which there are only  alternatives.
Now, take all the subtrees of R of maximum
size not exceeding r and preprocess them with
the technique above. Convert each such subtree
into a leaf in R labeled by a special character
aA, associated to the corresponding small NFA
A. Unless there are consecutive Kleene closures
in R, which can be simpliﬁed as R   = R  ,
the size of R after this transformation is O.m=r/.
Call R0 the transformed regular expression. One
essentially applies the technique of Theorem 1 to
R0, taking care of how to deal with the special
leaves that correspond to small NFAs. Those
leaves are converted by Thompson’s construction
into two nodes linked by an edge labeled aA.
When the path cost propagation process reaches
the source node of an edge labeled aA with cost
c, one must update the counter of the initial state
of NFA A to c (or k + 1 if c > k/. One then
uses the four-Russians table to do all the cost
propagation within A in constant time and ﬁnally
obtain, at the counter of the ﬁnal state of A, the
new value for the target node of the edge labeled
aA in the top-level NFA. Therefore, all the edges
(normal and special) of the top-level NFA can be
traversed in constant time, so the costs at Gi can
be obtained in O.mn=r/ time using Theorem 1.
Now one propagates the costs to Gi C 1, using the
four-Russians tables to obtain the current counter
values of each subgraph A in Gi C 1.
Theorem 2 (Wu et al. [10]) There exists an
O.n + mn/log k C 2n/ worst-case time solution to
the AREM problem under weighted edit distance
if the weights are integer numbers.
Applications
The problem has applications in computational
biology, to ﬁnd certain types of motifs in DNA
and protein sequences. See [1] for a more de-
tailed discussion. In particular, PROSITE pat-
terns are limited regular expressions rather pop-
ular to search protein sequences. PROSITE pat-
terns can be searched for with faster algorithms in
practice [7]. The same occurs with other classes
of complex patterns [6] and network expressions
[2].
Open Problems
The
worst-case
complexity
of
the
AREM
problem
is
not
fully
understood.
It
is
of
course .n/, which has been achieved for
mlog(k + 2) = O(log n/, but it is not known
how much can this be improved.
Experimental Results
Some experiments are reported in [5]. For small
m and k, and assuming all the weights are 1

102
Approximate String Matching
(except w.c ! c/ = 0), bit-parallel algorithms
of worst-case complexity O.kn.m=log n/2/ [4,9]
are the fastest (the second is able to skip some text
characters, depending on R/. For arbitrary inte-
ger weights, the best choice is a more complex
bit-parallel algorithm [5] or the four-Russians
based one [10] for larger m and k. The original
algorithm [3] is slower, but it is the only one
supporting arbitrary weights.
URL to Code
A
recent
and
powerful
software
package
implementing AREM is TRE (http://laurikari.
net/tre),
which
supports
edit
distance
with
different costs for each type of operation.
Older packages offering efﬁcient AREM are
agrep [9] (https://github.com/Wikinaut/agrep) for
simpliﬁed weight choices and nrgrep [4] (http://
www.dcc.uchile.cl/~gnavarro/software).
Cross-References
▷Approximate String Matching is a simpliﬁca-
tion of this problem, and the relation between
graph G here and matrix C there should be
apparent.
▷Regular Expression Matching is the simpliﬁed
case where exact matching with strings in L.R/
is sought.
Recommended Reading
1. Gusﬁeld D (1997) Algorithms on strings, trees and
sequences. Cambridge University Press, Cambridge
2. Myers EW (1996) Approximate matching of network
expressions with spacers. J Comput Biol 3(1):33–51
3. Myers EW, Miller W (1989) Approximate matching
of regular expressions. Bull Math Biol 51:7–37
4. Navarro G (2001) Nr-grep: a fast and ﬂexible pattern
matching tool. Softw Pract Exp 31:1265–1312
5. Navarro G (2004) Approximate regular expression
searching with arbitrary integer weights. Nord J Com-
put 11(4):356–373
6. Navarro G, Rafﬁnot M (2002) Flexible pattern match-
ing in strings – practical on-line search algorithms for
texts and biological sequences. Cambridge University
Press, Cambridge
7. Navarro G, Rafﬁnot M (2003) Fast and simple char-
acter classes and bounded gaps pattern matching,
with applications to protein searching. J Comput Biol
10(6):903–923
8. Thompson K (1968) Regular expression search algo-
rithm. Commun ACM 11(6):419–422
9. Wu S, Manber U (1992) Fast text searching allowing
errors. Commun ACM 35(10):83–91
10. Wu S, Manber U, Myers EW (1995) A subquadratic
algorithm for approximate regular expression match-
ing. J Algorithms 19(3):346–360
Approximate String Matching
Gonzalo Navarro
Department of Computer Science, University of
Chile, Santiago, Chile
Keywords
Inexact string matching; Semiglobal or semilocal
sequence similarity; String matching allowing
errors or differences
Years and Authors of Summarized
Original Work
1980; Sellers
1989; Landau, Vishkin
1999; Myers
2003; Crochemore, Landau, Ziv-Ukelson
2004; Fredriksson, Navarro
Problem Deﬁnition
Given a text string T D t1t2 : : : tn and a pattern
string P D p1p2 : : : pm, both being sequences
over an alphabet † of size , and given a distance
function among strings d and a threshold k, the
approximate string matching (ASM) problem is to
ﬁnd all the text positions that ﬁnish the so-called
approximate occurrence of P in T , that is, com-
pute the set fj; 9i; 1  i  j; d.P; ti : : : tj / 
kg. In the sequential version of the problem, T; P ,

Approximate String Matching
103
A
and k are given together, whereas the algorithm
can be tailored for a speciﬁc d.
The solutions to the problem vary widely de-
pending on the distance d used. This entry fo-
cuses on a very popular one, called Levenshtein
distance or edit distance, deﬁned as the minimum
number of character insertions, deletions, and
substitutions necessary to convert one string into
the other. It will also pay some attention to other
common variants such as indel distance, where
only insertions and deletions are permitted and
is the dual of the longest common subsequence
lcs .d.A; B/ D jAj C jBj  2  lcs.A; B//, and
Hamming distance, where only substitutions are
permitted.
A popular generalization of all the above is
the weighted edit distance, where the operations
are given positive real-valued weights and the
distance is the minimum sum of weights of a
sequence of operations converting one string into
the other. The weight of deleting a character c
is written w.c
!
"/, that of inserting c is
written w." ! c/, and that of substituting c
by c0 ¤ c is written w.c ! c0/. It is assumed
w.c ! c/ D 0 and the triangle inequality, that
is, w.x ! y/ C w.y ! ´/  w.x ! ´/ for any
x; y; ´; 2 P [f"g. As the distance may now be
asymmetric, it is ﬁxed that d.A; B/ is the cost
of converting A into B. Of course, any result for
weighted edit distance applies to edit, Hamming,
and indel distances (collectively termed unit-cost
edit distances) as well, but other reductions are
not immediate.
Both worst- and average-case complexity are
considered. For the latter, one assumes that pat-
tern and text are randomly generated by choosing
each character uniformly and independently from
†. For simplicity and practicality, m D o.n/ is
assumed in this entry.
Key Results
The most ancient and versatile solution to
the problem [13] builds over the process of
computing weighted edit distance. Let A D
a1a2 : : : am and B D b1b2 : : : bn be two strings.
Let CŒ0 : : : m; 0 : : : n be a matrix such that
CŒi; j D d.a1 : : : ai; b1 : : : bj /. Then, it holds
CŒ0; 0 D 0 and
CŒi; j D min.CŒi  1; j 
C w.ai ! "/; CŒi; j  1 C w." ! bj /;
CŒi  1; j  1 C w.ai ! bj //;
where CŒi; 1 D CŒ1; j  D 1 is assumed.
This matrix is computed in O.mn/ time and
d.A; B/ D CŒm; n. In order to solve the approx-
imate string matching problem, one takes A D P
and B D T and sets CŒ0; j D 0 for all j , so that
the above formula is used only for i > 0.
Theorem 1 (Sellers 1980 [13]) There exists an
O.mn/ worst-case time solution to the ASM
problem under weighted edit distance.
The space is O.m/ if one realizes that C can
be computed column-wise and only column j 1
is necessary to compute column j . As explained,
this immediately implies that searching under
unit-cost edit distances can be done in O.mn/
time as well. In those cases, it is quite easy to
compute only part of matrix C so as to achieve
O.kn/ average-time algorithms [14].
Yet, there exist algorithms with lower worst-
case complexity for weighted edit distance. By
applying a Ziv-Lempel parsing to P and T ,
it is possible to identify regions of matrix C
corresponding to substrings of P and T that
can be computed from other previous regions
corresponding to similar substrings of P and T
[5].
Theorem 2 (Crochemore
et
al.
2003
[5])
There exists an O.n C mn= log n/ worst-
case time solution to the ASM problem under
weighted edit distance. Moreover, the time is
O.n C mnh= log n/, where 0  h  log  is the
entropy of T .
This result is very general, also holding for
computing weighted edit distance and local sim-
ilarity (see section on “Applications”). For the
case of edit distance and exploiting the unit-cost
RAM model, it is possible to do better. On one
hand, one can apply a four-Russian technique:
All the possible blocks (submatrices of C) of

104
Approximate String Matching
size t  t, for t D O.log n/, are precomputed,
and matrix C is computed block-wise [9]. On
the other hand, one can represent each cell in
matrix C using a constant number of bits (as it
can differ from neighboring cells by ˙1) so as to
store and process several cells at once in a single
machine word [10]. This latter technique is called
bit-parallelism and assumes a machine word of
‚.log n/ bits.
Theorem 3 (Masek
and
Paterson
1980
[9]; Myers 1999 [10]) There exist O.n C
mn=.log n/2/ and O.n C mn= log n/ worst-
case time solutions to the ASM problem under
edit distance.
Both complexities are retained for indel dis-
tance, yet not for Hamming distance.
For unit-cost edit distances, the complexity
can depend on k rather than on m, as k < m for
the problem to be nontrivial, and usually k is a
small fraction of m (or even k D o.m//. A classic
technique [8] computes matrix C by processing
in constant time diagonals CŒi C d; j C d; 0 
d  s, along which cell values do not change.
This is possible by preprocessing the sufﬁx trees
of T and P for lowest common ancestor queries.
Theorem 4 (Landau and Vishkin 1989 [8])
There exists an O.kn/ worst-case time solution
to
the
ASM
problem
under
unit-cost
edit
distances.
Other solutions exist which are better for small
k, achieving time O.n.1 C k4=m// [4]. For
the case of Hamming distance, one can achieve
improved results using convolutions [1].
Theorem 5 (Amir et al. 2004 [1]) There exist
O.n
p
k log k/ and O.n.1Ck3=m/ log k/ worst-
case time solution to the ASM problem under
Hamming distance.
The last result for edit distance [4] achieves
O.n/ time if k is small enough (k D O.m1=4//. It
is also possible to achieve O.n/ time on unit-cost
edit distances at the expense of an exponential
additive term on m or k: The number of different
columns in C is independent of n, so the transi-
tion from every possible column to the next can
be precomputed as a ﬁnite-state machine.
Theorem 6 (Ukkonen 1985 [14]) There exists
an O.nCm min.3m; m.2m/k// worst-case time
solution to the ASM problem under edit distance.
Similar results apply for Hamming and in-
del distance, where the exponential term reduces
slightly according to the particularities of the
distances.
The worst-case complexity of the ASM prob-
lem is of course .n/, but it is not known if this
can be attained for any m and k. Yet, the average-
case complexity of the problem is known.
Theorem 7 (Chang and Marr 1994 [3]) The
average-case complexity of the ASM problem is
‚.n.k C log m/=m/ under unit-cost edit dis-
tances.
It is not hard to prove the lower bound as
an extension to Yao’s bound for exact string
matching [15]. The lower bound was reached in
the same paper [3], for k=m < 1=3  O

1=p

.
This was improved later to k=m
<
1=2 
O

1=p

[6] using a slightly different idea. The
approach is to precompute the minimum distance
to match every possible text substring (block) of
length O.log m/ inside P . Then, a text window
is scanned backwards, block-wise, adding up
those minimum precomputed distances. If they
exceed k before scanning all the window, then
no occurrence of P with k errors can contain the
scanned blocks, and the window can be safely slid
over the scanned blocks, advancing in T . This
is an example of a ﬁltration algorithm, which
discards most text areas and applies an ASM
algorithm only over those areas that cannot be
discarded.
Theorem 8 (Fredriksson and Navarro 2004
[6]) There exists an optimal-on-average solution
to the ASM problem under edit distance, for any
k=m  1e=p
2e=p D 1=2  O

1=p

.
The result applies verbatim to indel distance.
The same complexity is achieved for Hamming
distance, yet the limit on k=m improves to 1 
1 / . Note that, when the limit k=m is reached,
the average complexity is already ‚.n/. It is not
clear up to which k=m limit could one achieve
linear time on average.

Approximate String Matching
105
A
Applications
The problem has many applications in compu-
tational biology (to compare DNA and protein
sequences, recovering from experimental errors,
so as to spot mutations or predict similarity of
structure or function), text retrieval (to recover
from spelling, typing, or automatic recognition
errors), signal processing (to recover from trans-
mission and distortion errors), and several others.
See a survey [11] for a more detailed discussion.
Many extensions of the ASM problem exist,
particularly in computational biology. For exam-
ple, it is possible to substitute whole substrings
by others (called generalized edit distance), swap
characters in the strings (string matching with
swaps or transpositions), reverse substrings (re-
versal distance), have variable costs for inser-
tions/deletions when they are grouped (similarity
with gap penalties), and look for any pair of
substrings of both strings that are sufﬁciently
similar (local similarity). See, for example, Gus-
ﬁeld’s book [7], where many related problems are
discussed.
Open Problems
The worst-case complexity of the problem is not
fully understood. For unit-cost edit distances, it is
‚.n/ if m D O.min.log n; .log n/2// or k D
O.min.m1=4; logm n//. For weighted edit dis-
tance, the complexity is ‚.n/ if m D O.log n/.
It is also unknown up to which k=m value can one
achieve O.n/ average time; up to now this has
been achieved up to k=m D 1=2  O

1=p

.
Experimental Results
A thorough survey on the subject [11] presents
extensive experiments. Nowadays, the fastest al-
gorithms for edit distance are in practice ﬁltra-
tion algorithms [6,12] combined with bit-parallel
algorithms to verify the candidate areas [2, 10].
Those ﬁltration algorithms work well for small
enough k=m; otherwise, the bit-parallel algo-
rithms should be used stand-alone. Filtration al-
gorithms are easily extended to handle multiple
patterns searched simultaneously.
URL to Code
Well-known packages offering efﬁcient ASM
are
agrep
(https://github.com/Wikinaut/agrep)
and nrgrep (http://www.dcc.uchile.cl/~gnavarro/
software).
Cross-References
▷Approximate Regular Expression Matching is
the more complex case where P can be a
regular expression
▷Indexed Approximate String Matching refers to
the case where the text can be preprocessed
▷String Matching is the simpliﬁed version where
no errors are permitted
Recommended Reading
1. Amir A, Lewenstein M, Porat E (2004) Faster al-
gorithms for string matching with k mismatches.
J Algorithms 50(2):257–275
2. Baeza-Yates R, Navarro G (1999) Faster approximate
string matching. Algorithmica 23(2):127–158
3. Chang W, Marr T (1994) Approximate string match-
ing and local similarity. In: Proceedings of the 5th
annual symposium on combinatorial pattern match-
ing (CPM’94), Asilomar. LNCS, vol 807. Springer,
Berlin, pp 259–273
4. Cole R, Hariharan R (2002) Approximate string
matching: a simpler faster algorithm. SIAM J Comput
31(6):1761–1782
5. Crochemore M, Landau G, Ziv-Ukelson M (2003)
A subquadratic sequence alignment algorithm for
unrestricted scoring matrices. SIAM J Comput
32(6):1654–1673
6. Fredriksson K, Navarro G (2004) Average-optimal
single and multiple approximate string matching.
ACM J Exp Algorithms 9(1.4)
7. Gusﬁeld D (1997) Algorithms on strings, trees and
sequences. Cambridge University Press, Cambridge
8. Landau G, Vishkin U (1989) Fast parallel and serial
approximate string matching. J Algorithms 10:157–
169
9. Masek W, Paterson M (1980) A faster algorithm for
computing string edit distances. J Comput Syst Sci
20:18–31

106
Approximate Tandem Repeats
10. Myers G (1999) A fast bit-vector algorithm for
approximate string matching based on dynamic
progamming. J ACM 46(3):395–415
11. Navarro G (2001) A guided tour to approximate string
matching. ACM Comput Surv 33(1):31–88
12. Navarro G, Baeza-Yates R (1999) Very fast and sim-
ple approximate string matching. Inf Proc Lett 72:65–
70
13. Sellers P (1980) The theory and computation of evo-
lutionary distances: pattern recognition. J Algorithms
1:359–373
14. Ukkonen E (1985) Finding approximate patterns in
strings. J Algorithms 6:132–137
15. Yao A (1979) The complexity of pattern matching for
a random string. SIAM J Comput 8:368–387
Approximate Tandem Repeats
Gregory Kucherov1 and Dina Sokol2
1CNRS/LIGM, Université Paris-Est,
Marne-la-Vallée, France
2Department of Computer and Information
Science, Brooklyn College of CUNY, Brooklyn,
NY, USA
Keywords
Approximate periodicities; Approximate repeti-
tions
Years and Authors of Summarized
Original Work
2001; Landau, Schmidt, Sokol
2003; Kolpakov, Kucherov
Problem Deﬁnition
Identiﬁcation of periodic structures in words
(variants of which are known as tandem repeats,
repetitions, powers, or runs) is a fundamental
algorithmic
task
(see
entry
▷Squares
and
Repetitions). In many practical applications, such
as DNA sequence analysis, considered repetitions
admit a certain variation between copies of the
repeated pattern. In other words, repetitions under
interest are approximate tandem repeats and not
necessarily exact repeats only.
The simplest instance of an approximate tan-
dem repeat is an approximate square. An approx-
imate square in a word w is a subword uv, where
u and v are within a given distance k accord-
ing to some distance measure between words,
such as Hamming distance or edit (also called
Levenshtein) distance. There are several ways
to deﬁne approximate tandem repeats as succes-
sions of approximate squares, i.e., to generalize
to the approximate case the notion of arbitrary
periodicity (see entry ▷Squares and Repetitions).
In this entry, we discuss three different deﬁnitions
of approximate tandem repeats. The ﬁrst two are
built upon the Hamming distance measure, and
the third one is built upon the edit distance.
Let h.,) denote the Hamming distance be-
tween two words of equal length.
Deﬁnition 1 A word rŒ1: : :n is called a K-
repetition of period p, p  n=2, iff h.rŒ1: : :n 
p; rŒp C 1: : :n/  K.
Equivalently, a word rŒ1: : :n is a K-repetition
of period p, if the number of mismatches, i.e., the
number of i such that rŒi ¤ rŒi C p, is at most
K. For example, ataa atta ctta ct is a 2-repetition
of period 4. atc atc atc atg atg atg atg atg is a
1-repetition of period 3, but atc atc atc att atc atc
atc att is not.
Deﬁnition 2 A word rŒ1: : :n is called a K-run,
of period p, p  n=2, iff for every i 2 Œ1: : :n 
2p C 1, we have h.rŒi: : :i C p  1; rŒi C p; i C
2p  1/  K.
A K-run can be seen as a sequence of approx-
imate squares uv such that juj D jvj D p and u
and v differ by at most K mismatches. The total
number of mismatches in a K-run is not bounded.
Let ed(,) denote the edit distance between
two strings.
Deﬁnition 3 A word r is a K-edit repeat if it can
be partitioned into consecutive subwords, r D
v0w1w2 : : : w`v00, `  2, such that
ed.v0; w0
1/C
`1
X
iD1
ed.wi; wiC1/Ced.w00
`; v00/K;

Approximate Tandem Repeats
107
A
where w0
1 is some sufﬁx of w1 and w00
` is some
preﬁx of w`.
A K-edit repeat is a sequence of “evolving”
copies of a pattern such that there are at most
K insertions, deletions, and mismatches, overall,
between all consecutive copies of the repeat. For
example, the word r = caagct cagct ccgct is a 2-
edit repeat.
When looking for tandem repeats occurring in
a word, it is natural to consider maximal repeats.
Those are the repeats extended to the right and
left as much as possible provided that the corre-
sponding deﬁnition is still veriﬁed. Note that the
notion of maximality applies to K-repetitions, to
K-runs, and to K-edit repeats.
Under
the
Hamming
distance,
K-runs
provide the weakest “reasonable” deﬁnition of
approximate tandem repeats, since it requires
that every square it contains cannot contain
more than K mismatch errors, which seems
to be a minimal reasonable requirement. On
the other hand, K-repetition is the strongest
such notion as it limits by K the total number
of mismatches. This provides an additional
justiﬁcation
that
ﬁnding
these
two
types
of repeats is important as they “embrace”
other intermediate types of repeats. Several
intermediate deﬁnitions have been discussed in
[9, Section 5].
In general, each K-repetition is a part of a
K-run of the same period, and every K-run is
the union of all K-repetitions it contains. Ob-
serve that a K-run can contain as many as a
linear number of K-repetitions with the same
period. For example, the word (000 100)n of
length 6n is a 1-run of period 3, which contains
.2n  1/ 1-repetitions. In general, a K-run r
contains .s  K C 1/ K-repetitions of the same
period, where s is the number of mismatches
in r.
Example 1 The following Fibonacci word con-
tains three 3-runs of period 6. They are shown in
regular font, in positions aligned with their oc-
currences. Two of them are identical and contain
each four 3-repetitions, shown in italic for the
ﬁrst run only. The third run is a 3-repetition in
itself.
010010
100100
101001
010010
010100
1001
10010
100100
101001
10010
100100
10
0010
100100
101
10
100100
10100
0
100100
101001
1001
010010
010100
1
10
010100
1001
Key Results
Given a word w of length n and an integer K, it
is possible to ﬁnd all K-runs, K-repetitions, and
K-edit repeats within w in the following time and
space bounds:
K -runs can be found in time O.nK log K CS/
(S the output size) and working space O.n/
[9].
K -repetitions can be found in time O.nK
log K C S/ and working space O.n/ [9].
K
-edit
repeats
can
be
found
in
time
O.nK log K log.n=K/ C S/ and working
space O.n C K2/ [14,19].
All three algorithms are based on similar
algorithmic tools that generalize corresponding
techniques for the exact case [4, 15, 16] (see
[10] for a systematic presentation). The ﬁrst
basic tool is a generalization of the longest
extension functions [16] that, in the case of
Hamming
distance,
can
be
exempliﬁed
as
follows. Given a word w, we want to compute,
for each position p and each k

K, the
quantity max fj jh.wŒ1: : :j ; wŒp: : :p C j 
1/

kg. Computing all those values can
be
done
in
time
O(nK)
using
a
method
based on the sufﬁx tree and the computation
of
the
lowest
common
ancestor
described
in [7].
The second tool is the Lempel-Ziv factoriza-
tion used in the well-known compression method.
Different variants of the Lempel-Ziv factorization
of a word can be computed in linear time [7,18].
The algorithm for computing K-repetitions
from [9] can be seen as a direct generalization of

108
Approximate Tandem Repeats
the algorithm for computing maximal repetitions
(runs) in the exact case [8, 15]. Although based
on the same basic tools and ideas, the algorithm
[9] for computing K-runs is much more involved
and uses a complex “bootstrapping” technique for
assembling runs from smaller parts.
The algorithm for ﬁnding the K-edit repeats
uses both the recursive framework and the idea
of the longest extension functions of [16]. The
longest common extensions, in this case, allow
up to K edit operations. Efﬁcient methods for
computing these extensions are based upon a
combination of the results of [12] and [13]. The
K-edit repeats are derived by combining the
longest common extensions computed in the for-
ward direction with those computed in the reverse
direction.
Applications
Tandemly repeated patterns in DNA sequences
are involved in various biological functions and
are used in different practical applications.
Tandem repeats are known to be involved in
regulatory mechanisms, e.g., to act as binding
sites for regulatory proteins. Tandem repeats have
been shown to be associated with recombina-
tion hotspots in higher organisms. In bacteria,
a correlation has been observed between certain
tandem repeats and virulence and pathogenicity
genes.
Tandem repeats are responsible for a number
of inherited diseases, especially those involving
the central nervous system. Fragile X syndrome,
Kennedy disease, myotonic dystrophy, and Hunt-
ington’s disease are among the diseases that have
been associated with triplet repeats.
Examples of different genetic studies illustrat-
ing abovementioned biological roles of tandem
repeats can be found in introductive sections
of [1, 6, 11]. Even more than just genomic el-
ements associated with various biological func-
tions, tandem repeats have been established to be
a fundamental mutational mechanism in genome
evolution [17].
A major practical application of short tandem
repeats is based on the interindividual variability
in copy number of certain repeats occurring at a
single locus. This feature makes tandem repeats a
convenient tool for genetic proﬁling of individ-
uals. The latter, in turn, is applied to pedigree
analysis and establishing phylogenetic relation-
ships between species, as well as to forensic
medicine [3].
Open Problems
The deﬁnition of K-edit repeats is similar to
that of K-repetitions (for the Hamming distance
case). It would be interesting to consider other
deﬁnitions of maximal repeats over the edit dis-
tance. For example, a deﬁnition similar to the K-
run would allow up to K edits between each pair
of neighboring periods in the repeat. Other possi-
ble deﬁnitions would allow K errors between any
pair of copies of a repeat, or between all pairs
of copies, or between some consensus and each
copy.
In general, a weighted edit distance scheme is
necessary for biological applications. Known al-
gorithms for tandem repeats based on a weighted
edit distance scheme are not feasible, and thus,
only heuristics are currently used.
URL to Code
The algorithms described in this entry have
been implemented for DNA sequences and
are publicly available. The Hamming distance
algorithms (K-runs and K-repetitions) are part
of the mreps software package, available at
http://mreps.univ-mlv.fr/ [11]. The K-edit repeat
software, TRED, is available at http://tandem.sci.
brooklyn.cuny.edu/ [19]. The implementations of
the algorithms are coupled with postprocessing
ﬁlters, necessary due to the nature of biological
sequences.
In practice, software based on heuristic and
statistical methods is largely used. Among them,
TRF (http://tandem.bu.edu/trf/trf.html) [1] is the
most popular program used by the bioinformatics
community. Other programs include ATRHunter
(http://bioinfo.cs.technion.ac.il/atrhunter/)
[20]

Approximating Fixation Probabilities in the Generalized Moran Process
109
A
and
TandemSWAN
(http://favorov.bioinfolab.
net/swan/)
[2].
STAR
(http://atgc.lirmm.fr/
star/) [5] is another software, based on an
information-theoretic approach, for computing
approximate tandem repeats of a prespeciﬁed
pattern.
Cross-References
▷Squares and Repetitions
Acknowledgments This work was supported in part by
the National Science Foundation Grant DB&I 0542751.
Recommended Reading
1. Benson G (1999) Tandem repeats ﬁnder: a program to
analyze DNA sequences. Nucleic Acids Res 27:573–
580
2. Boeva VA, Régnier M, Makeev VJ (2004) SWAN:
searching for highly divergent tandem repeats in
DNA sequences with the evaluation of their statistical
signiﬁcance. In: Proceedings of JOBIM 2004, Mon-
treal, p 40
3. Butler JM (2001) Forensic DNA typing: biology and
technology behind STR markers. Academic Press,
San Diego
4. Crochemore M (1983) Recherche linéaire d’un carré
dans un mot. C R Acad Sci Paris Sér I Math 296:781–
784
5. Delgrange O, Rivals E (2004) STAR – an algorithm
to search for tandem approximate repeats. Bioinfor-
matics 20:2812–2820
6. Gelfand Y, Rodriguez A, Benson G (2007) TRDB
– the tandem repeats database. Nucleic Acids Res
35(suppl. 1):D80–D87
7. Gusﬁeld D (1997) Algorithms on strings, trees,
and sequences. Cambridge University Press, Cam-
bridge/New York
8. Kolpakov R, Kucherov G (1999) Finding maximal
repetitions in a word in linear time. In: 40th sympo-
sium foundations of computer science (FOCS), New
York, pp 596–604. IEEE Computer Society Press
9. Kolpakov R, Kucherov G (2003) Finding approx-
imate repetitions under Hamming distance. Theor
Comput Sci 33(1):135–156
10. Kolpakov R, Kucherov G (2005) Identiﬁcation of
periodic structures in words. In: Berstel J, Perrin D
(eds) Applied combinatorics on words. Encyclopedia
of mathematics and its applications. Lothaire books,
vol 104, pp 430–477. Cambridge University Press,
Cambridge
11. Kolpakov R, Bana G, Kucherov G (2003) mreps:
efﬁcient and ﬂexible detection of tandem repeats in
DNA. Nucleic Acids Res 31(13):3672–3678
12. Landau GM, Vishkin U (1988) Fast string matching
with k differences. J Comput Syst Sci 37(1):63–78
13. Landau GM, Myers EW, Schmidt JP (1998) In-
cremental
string
comparison.
SIAM
J
Comput
27(2):557–582
14. Landau GM, Schmidt JP, Sokol D (2001) An algo-
rithm for approximate tandem repeats. J Comput Biol
8:1–18
15. Main M (1989) Detecting leftmost maximal periodic-
ities. Discret Appl Math 25:145–153
16. Main M, Lorentz R (1984) An O.nlog n) algorithm
for ﬁnding all repetitions in a string. J Algorithms
5(3):422–432
17. Messer PW, Arndt PF (2007) The majority of recent
short DNA insertions in the human genome are tan-
dem duplications. Mol Biol Evol 24(5):1190–1197
18. Rodeh M, Pratt V, Even S (1981) Linear algorithm
for data compression via string matching. J Assoc
Comput Mach 28(1):16–24
19. Sokol D, Benson G, Tojeira J (2006) Tandem repeats
over the edit distance. Bioinformatics 23(2):e30–e35
20. Wexler Y, Yakhini Z, Kashi Y, Geiger D (2005)
Finding approximate tandem repeats in genomic
sequences. J Comput Biol 12(7):928–942
Approximating Fixation
Probabilities in the Generalized
Moran Process
George B. Mertzios
School of Engineering and Computing Sciences,
Durham University, Durham, UK
Keywords
Approximation algorithm; Evolutionary dyna-
mics; Fixation probability; Markov-chain Monte
Carlo; Moran process
Years and Authors of Summarized
Original Work
2014; Diaz, Goldberg, Mertzios, Richerby, Serna,
Spirakis
Problem Deﬁnition
Population and evolutionary dynamics have been
extensively studied, usually with the assumption
that the evolving population has no spatial struc-
ture. One of the main models in this area is
the Moran process [17]. The initial population

110
Approximating Fixation Probabilities in the Generalized Moran Process
contains a single “mutant” with ﬁtness r > 0,
with all other individuals having ﬁtness 1. At
each step of the process, an individual is chosen
at random, with probability proportional to its
ﬁtness. This individual reproduces, replacing a
second individual, chosen uniformly at random,
with a copy of itself.
Lieberman, Hauert, and Nowak introduced a
generalization of the Moran process, where the
members of the population are placed on the
vertices of a connected graph which is, in gen-
eral, directed [13, 19]. In this model, the initial
population again consists of a single mutant of
ﬁtness r > 0 placed on a vertex chosen uniformly
at random, with each other vertex occupied by a
nonmutant with ﬁtness 1. The individual that will
reproduce is chosen as before, but now one of its
neighbors is randomly selected for replacement,
either uniformly or according to a weighting of
the edges. The original Moran process can be re-
covered by taking the graph to be an unweighted
clique.
Several similar models describing particle in-
teractions have been studied previously, including
the SIR and SIS epidemic models [8, Chapter 21],
the voter model, the antivoter model, and the
exclusion process [1,7,14]. Related models, such
as the decreasing cascade model [12, 18], have
been studied in the context of inﬂuence propa-
gation in social networks and other models have
been considered for dynamic monopolies [2].
However, these models do not consider different
ﬁtnesses for the individuals.
In general, the Moran process on a ﬁnite, con-
nected, directed graph may end with all vertices
occupied by mutants or with no vertex occupied
by a mutant – these cases are referred to as ﬁxa-
tion and extinction, respectively – or the process
may continue forever. However, for undirected
graphs and strongly connected digraphs, the pro-
cess terminates almost surely, either at ﬁxation
or extinction. At the other extreme, in a directed
graph with two sources, neither ﬁxation nor ex-
tinction is possible. In this work we consider
ﬁnite undirected graphs. The ﬁxation probability
for a mutant of ﬁtness r in a graph G is the
probability that ﬁxation is reached and is denoted
fG;r.
Key Results
The ﬁxation probability can be determined by
standard Markov chain techniques. However, do-
ing so for a general graph on n vertices requires
solving a set of 2n linear equations, which is not
computationally feasible, even numerically. As
a result, most prior work on computing ﬁxation
probabilities in the generalized Moran process
has either been restricted to small graphs [6] or
graph classes where a high degree of symme-
try reduces the size of the set of equations –
for example, paths, cycles, stars, and complete
graphs [3–5] – or has concentrated on ﬁnding
graph classes that either encourage or suppress
the spread of the mutants [13,16].
Because of the apparent intractability of exact
computation, we turn to approximation. Using
a potential function argument, we show that,
with high probability, the Moran process on an
undirected graph of order n reaches absorption
(either ﬁxation or extinction) within O.n6/ steps
if r D 1 and O.n4/ and O.n3/ steps when r > 1
and r < 1, respectively. Taylor et al. [20] studied
absorption times for variants of the generalized
Moran process, but, in our setting, their results
only apply to the process on regular graphs,
where it is equivalent to a biased random walk
on a line with absorbing barriers. The absorption
time analysis of Broom et al. [3] is also restricted
to cliques, cycles, and stars. In contrast to this
earlier work, our results apply to all connected
undirected graphs.
Our bound on the absorption time, along with
polynomial upper and lower bounds for the ﬁx-
ation probability, allows the estimation of the
ﬁxation and extinction probabilities by Monte
Carlo techniques. Speciﬁcally, we give a fully
polynomial randomized approximation scheme
(FPRAS) for these quantities. An FPRAS for a
function f .X/ is a polynomial-time randomized
algorithm g that, given input X and an error
bound ", satisﬁes .1  "/f .X/ 6 g.X/ 6 .1 C
"/f .X/ with probability at least 3
4 and runs in
time polynomial in the length of X and 1
" [11].
For the case r < 1, there is no polynomial
lower bound on the ﬁxation probability so only
the extinction probability can be approximated

Approximating Fixation Probabilities in the Generalized Moran Process
111
A
by this technique. Note that, when f
 1,
computing 1  f to within a factor of 1 ˙ " does
not imply computing f to within the same factor.
Bounding the Fixation Probability
In the next two lemmas, we provide polynomial
upper and lower bounds for the ﬁxation proba-
bility of an arbitrary undirected graph G. Note
that the lower bound of Lemma 1 holds only
for r
> 1. Indeed, for example, the ﬁxation
probability of the complete graph Kn is given by
fKn;r D .1  1
r /=.1 
1
rn / [13, 19], which is
exponentially small for any r < 1.
Lemma 1 Let G D .V; E/ be an undirected
graph with n vertices. Then fG;r >
1
n for any
r > 1.
Lemma 2 Let G D .V; E/ be an undirected
graph with n vertices. Then fG;r 6 1 
1
nCr for
any r > 0.
Bounding the Absorption Time
In this section, we show that the Moran process
on a connected graph G of order n is expected
to reach absorption in a polynomial number of
steps. To do this, we use the potential function
given by
.S/ D
X
x2S
1
deg x
for any state S  V.G/ and we write .G/ for
.V.G//. Note that 1 < .G/ < n and that
.fxg/ D 1= deg x 6 1 for any vertex x 2 V .
First, we show that the potential strictly in-
creases in expectation when r > 1 and strictly
decreases in expectation when r < 1.
Lemma 3 Let .Xi/i>0 be a Moran process on a
graph G D .V; E/ and let ;  S  V. If r > 1,
then
EŒ.XiC1/.Xi/ j Xi D S >

1  1
r

 1
n3 ;
with equality if and only if r D 1. For r < 1,
EŒ.XiC1/  .Xi/ j Xi D S < r  1
n3
:
To bound the expected absorption time, we
use martingale techniques. It is well known how
to bound the expected absorption time using a
potential function that decreases in expectation
until absorption. This has been made explicit by
Hajek [9] and we use the following formulation
based on that of He and Yao [10]. The proof is
essentially theirs but is modiﬁed to give a slightly
stronger result.
Theorem 1 Let .Yi/i>0 be a Markov chain with
state space ˝, where Y0 is chosen from some set
I  ˝. If there are constants k1; k2 > 0 and a
nonnegative function  W ˝ ! R such that
•
 .S/ D 0 for some S 2 ˝,
•
 .S/ 6 k1 for all S 2 I and
•
EŒ .Yi/   .YiC1/ j Yi D S > k2 for all
i > 0 and all S with  .S/ > 0,
then EŒ
 6 k1=k2, where 
 D min fi W  .Yi/ D
0g.
Using Theorem 1, we can prove the following
upper bounds for the absorption time 
 in the
cases where r < 1 and r > 1, respectively.
Theorem 2 Let G D .V; E/ be a graph of order
n. For r < 1 and any S  V , the absorption time

 of the Moran process on G satisﬁes
EŒ
 j X0 D S 6
1
1  r n3.S/ :
Theorem 3 Let G D .V; E/ be a graph of order
n. For r > 1 and any S  V , the absorption time

 of the Moran process on G satisﬁes
EŒ
 j X0 D S 6
r
r  1n3
.G/  .S/

6
r
r  1n4:
The case r
D
1 is more complicated as
Lemma 3 shows that the expectation is constant.
However, this allows us to use standard martin-
gale techniques and the proof of the following
is partly adapted from the proof of Lemma 3.4
in [15].

112
Approximating Fixation Probabilities in the Generalized Moran Process
Theorem 4 The expected absorption time for the
Moran process .Xi/i>0 with r D 1 on a graph
G D .V; E/ is at most n4..G/2  EŒ.X0/2/.
Approximation Algorithms
We
now
have
all
the
components
needed
to present our fully polynomial randomized
approximation schemes (FPRAS) for the problem
of computing the ﬁxation probability of a graph,
where r > 1, and for computing the extinction
probability for all r > 0. In the following two
theorems, we give algorithms whose running
times are polynomial in n, r, and
1
" . For the
algorithms to run in time polynomial in the
length of the input and thus meet the deﬁnition of
FPRAS, r must be encoded in unary.
Theorem 5 There is an FPRAS for MORAN FIX-
ATION, for r > 1.
Proof (sketch) The algorithm is as follows. If
r D 1 then we return 1
n. Otherwise, we simulate
the Moran process on G for T D d 8r
r1Nn4e
steps, N D d 1
2"2n2 ln 16e times and compute
the proportion of simulations that reached ﬁxa-
tion. If any simulation has not reached absorption
(ﬁxation or extinction) after T steps, we abort and
immediately return an error value.
Note that each transition of the Moran process
can be simulated in O.1/ time. Maintaining ar-
rays of the mutant and nonmutant vertices allows
the reproducing vertex to be chosen in constant
time, and storing a list of each vertex’s neigh-
bors allows the same for the vertex where the
offspring is sent. Therefore, the total running time
is O.NT / steps, which is polynomial in n and 1
" ,
as required.
For i 2 f1; : : : ; N g, let Xi D 1 if the ith
simulation of the Moran process reaches ﬁxation
and Xi D 0 otherwise. Assuming all simulation
runs reach absorption, the output of the algorithm
is p D 1
N
P
i Xi.
ut
Note that this technique fails for disadvan-
tageous mutants (r < 1) because there is no
analogue of Lemma 1 giving a polynomial lower
bound on fG;r. As such, an exponential number
of simulations may be required to achieve the
desired error probability. However, we can give
an FPRAS for the extinction probability for all
r > 0. Although the extinction probability is just
1fG;r, there is no contradiction because a small
relative error in 1  fG;r does not translate into a
small relative error in fG;r when fG;r is, itself,
small.
Theorem 6 There is an FPRAS for MORAN EX-
TINCTION for all r > 0.
Proof (sketch) The algorithm and its correctness
proof are essential as in the previous theorem. If
r D 1, we return 1  1
n. Otherwise, we run N D
d 1
2"2.r C n/2 ln 16e simulations of the Moran
process on G for T .r/ steps each, where
T .r/ D
(
d 8r
r1Nn4e
if r > 1
d
8
1r Nn3e
if r < 1.
If any simulation has not reached absorption
within T .r/ steps, we return an error value; oth-
erwise, we return the proportion p of simulations
that reached extinction.
ut
It remains open whether other techniques
could lead to an FPRAS for MORAN FIXATION
when r < 1.
Recommended Reading
1. Aldous DJ, Fill JA (2002) Reversible Markov
chains and random walks on graphs. Monograph
in preparation. Available at http://www.stat.berkeley.
edu/aldous/RWG/book.html
2. Berger E (2001) Dynamic monopolies of constant
size. J Comb Theory Ser B 83:191–200
3. Broom M, Hadjichrysanthou C, Rychtáˇr J (2010)
Evolutionary games on graphs and the speed of the
evolutionary process. Proc R Soc A 466(2117):1327–
1346
4. Broom M, Hadjichrysanthou C, Rychtáˇr J (2010) Two
results on evolutionary processes on general non-
directed graphs. Proc R Soc A 466(2121):2795–2798
5. Broom M, Rychtáˇr J (2008) An analysis of the ﬁxa-
tion probability of a mutant on special classes of non-
directed graphs. Proc R Soc A 464(2098):2609–2627
6. Broom M, Rychtáˇr J, Stadler B (2009) Evolutionary
dynamics on small order graphs. J Interdiscip Math
12:129–140
7. Durrett R (1988) Lecture notes on particle systems
and percolation. Wadsworth Publishing Company,
Paciﬁc Grove
8. Easley D, Kleinberg J (2010) Networks, crowds, and
markets: reasoning about a highly connected world.
Cambridge University Press, New York

Approximating Metric Spaces by Tree Metrics
113
A
9. Hajek B (1982) Hitting-time and occupation-time
bounds implied by drift analysis with applications.
Adv Appl Probab 14(3):502–525
10. He J, Yao X (2001) Drift analysis and average time
complexity of evolutionary algorithms. Artif Intell
127:57–85
11. Karp RM, Luby M (1983) Monte-Carlo algorithms
for enumeration and reliability problems. In: Pro-
ceedings of 24th annual IEEE symposium on founda-
tions of computer science (FOCS), Tucson, pp 56–64
12. Kempel D, Kleinberg J, Tardos E (2005) Inﬂuential
nodes in a diffusion model for social networks. In:
Proceedings of the 32nd international colloquium
on automata, languages and programming (ICALP),
Lisbon. Lecture notes in computer science, vol 3580,
pp 1127–1138. Springer
13. Lieberman E, Hauert C, Nowak MA (2005) Evolu-
tionary dynamics on graphs. Nature 433:312–316
14. Liggett TM (1985) Interacting particle systems.
Springer, New York
15. Luby M, Randall D, Sinclair A (2001) Markov chain
algorithms for planar lattice structures. SIAM J Com-
put 31(1):167–192
16. Mertzios GB, Nikoletseas S, Raptopoulos C, Spirakis
PG (2013) Natural models for evolution on networks.
Theor Comput Sci 477:76–95
17. Moran PAP (1958) Random processes in genetics.
Proc Camb Philos Soc 54(1):60–71
18. Mossel E, Roch S (2007) On the submodularity of
inﬂuence in social networks. In: Proceedings of the
39th annual ACM symposium on theory of comput-
ing (STOC), San Diego, pp 128–134
19. Nowak MA (2006) Evolutionary dynamics: exploring
the equations of life. Harvard University Press, Cam-
bridge
20. Taylor C, Iwasa Y, Nowak MA (2006) A symmetry of
ﬁxation times in evolutionary dynamics. J Theor Biol
243(2):245–251
Approximating Metric Spaces
by Tree Metrics
Jittat Fakcharoenphol1, Satish Rao2, and
Kunal Talwar3
1Department of Computer Engineering,
Kasetsart University, Bangkok, Thailand
2Department of Computer Science, University of
California, Berkeley, CA, USA
3Microsoft Research, Silicon Valley Campus,
Mountain View, CA, USA
Keywords
Embedding general metrics into tree metrics
Years and Authors of Summarized
Original Work
1996; Bartal, Fakcharoenphol, Rao, Talwar
2004; Bartal, Fakcharoenphol, Rao, Talwar
Problem Deﬁnition
This problem is to construct a random tree metric
that probabilistically approximates a given arbi-
trary metric well. A solution to this problem is
useful as the ﬁrst step for numerous approxima-
tion algorithms because usually solving problems
on trees is easier than on general graphs. It
also ﬁnds applications in on-line and distributed
computation.
It is known that tree metrics approximate
general metrics badly, e.g., given a cycle Cn
with n nodes, any tree metric approximating this
graph metric has distortion ˝.n/ [17]. However,
Karp [15] noticed that a random spanning tree
of Cn approximates the distances between any
two nodes in Cn well in expectation. Alon,
Karp, Peleg, and West [1] then proved a bound
of
exp.O.
p
log n log log n//
on
an
average
distortion for approximating any graph metric
with its spanning tree.
Bartal [2] formally deﬁned the notion of prob-
abilistic approximation.
Notations
A graph G D .V; E/ with an assignment of non-
negative weights to the edges of G deﬁnes a met-
ric space .V; dG/ where for each pair u; v 2 V ,
dG.u; v/ is the shortest path distance between
u and v in G. A metric (V, d) is a tree metric
if there exists some tree T D .V 0; E0/ such that
V  V 0 and for all u; v 2 V , dT .u; v/ D d.u; v/.
The metric (V, d) is also called a metric induced
by T.
Given a metric (V, d), a distribution D
over tree metrics over V ˛-probabilistically
approximates d if every tree metric dT 2 D,
dT .u; v/  d.u; v/
and
EdT 2DŒdT .u; v/

˛  d.u; v/, for every u; v 2 V . The quantity ’ is
referred to as the distortion of the approximation.

114
Approximating Metric Spaces by Tree Metrics
Although the deﬁnition of probabilistic ap-
proximation uses a distribution D over tree met-
rics, one is interested in a procedure that con-
structs a random tree metric distributed according
to D, i.e., an algorithm that produces a random
tree metric that probabilistically approximates
a given metric. The problem can be formally
stated as follows.
Problem (APPROX-TREE)
INPUT: a metric (V, d)
OUTPUT: a tree metric .V; dT / sampled from
a distribution
D over tree metrics that ’-
probabilistically approximates (V, d).
Bartal then deﬁned a class of tree metrics, called
hierarchically well-separated trees (HST), as fol-
lows. A k-hierarchically well-separated tree (k-
HST) is a rooted weighted tree satisfying two
properties: the edge weight from any node to
each of its children is the same, and the edge
weights along any path from the root to a leaf
are decreasing by a factor of at least k. These
properties are important to many approximation
algorithms.
Bartal showed that any metric on n points
can be probabilistically approximated by a set
of
k-HST’s
with
O.log2 n/
distortion,
an
improvement
from
exp.O.
p
log n log log n//
in [1]. Later Bartal [3], following the same
approach as in Seymour’s analysis on the
Feedback Arc Set problem [18], improved the
distortion down to O.log n log log n/. Using
a rounding procedure of Calinescu, Karloff, and
Rabani [5], Fakcharoenphol, Rao, and Talwar [9]
devised
an
algorithm
that,
in
expectation,
produces a tree with O.log n/ distortion. This
bound is tight up to a constant factor.
Key Results
A tree metric is closely related to graph decom-
position. The randomized rounding procedure
of Calinescu, Karloff, and Rabani [5] for the
0-extension problem decomposes a graph into
pieces with bounded diameter, cutting each edge
with probability proportional to its length and
a ratio between the numbers of nodes at certain
distances. Fakcharoenphol, Rao, and Talwar [9]
used the CKR rounding procedure to decompose
the graph recursively and obtained the following
theorem.
Theorem 1 Given an n-point metric (V, d), there
exists a randomized algorithm, which runs in
time O(n2), that samples a tree metric from the
distribution D over tree metrics that O.log n/-
probabilistically approximates (V, d). The tree is
also a 2-HST.
The bound in Theorem 1 is tight, as Alon et al. [1]
proved the bound of an ˝.log n/ distortion when
(V, d) is induced by a grid graph. Also note that it
is known (as folklore) that even embedding a line
metric onto a 2-HST requires distortion ˝.log n/.
If the tree is required to be a k-HST, one
can apply the result of Bartal, Charikar, and
Raz [4] which states that any 2-HST can be
O.k= log k/-probabilistically approximated by
k-HST, to obtain an expected distortion of
O.k log n= log k/.
Finding a distribution of tree metrics that
probabilistically approximates a given metric
has a dual problem that is to ﬁnd a single
tree T with small average weighted stretch.
More speciﬁcally, given weight cuv on edges,
ﬁnd a tree metric dT such that for all u; v 2
VdT .u; v/  d.u; v/ and P
u;v2V cuv dT .u; v/ 
˛ P
u;v2V cuv  d.u; v/.
Charikar, Chekuri, Goel, Guha, and Plotkin [6]
showed how to ﬁnd a distribution of O.n log n/
tree metrics that ’-probabilistically approximates
a given metric, provided that one can solve the
dual problem. The algorithm in Theorem 1 can
be derandomized by the method of conditional
expectation to ﬁnd the required tree metric
with ˛ D O.log n/. Another algorithm based on
modiﬁed region growing techniques is presented
in [9], and independently by Bartal.
Theorem 2 Given an n-point metric (V, d), there
exists a polynomial-time deterministic algorithm
that ﬁnds a distribution D over O.n log n/ tree
metrics that O.log n/-probabilistically approxi-
mates (V, d).

Approximating Metric Spaces by Tree Metrics
115
A
Note that the tree output by the algorithm con-
tains Steiner nodes, however Gupta [10] showed
how to ﬁnd another tree metric without Steiner
nodes while preserving all distances within a con-
stant factor.
Applications
Metric approximation by random trees has ap-
plications in on-line and distributed computation,
since randomization works well against oblivious
adversaries, and trees are easy to work with and
maintain. Alon et al. [1] ﬁrst used tree embedding
to give a competitive algorithm for the k-server
problem. Bartal [3] noted a few problems in his
paper: metrical task system, distributed paging,
distributed k-server problem, distributed queuing,
and mobile user.
After the paper by Bartal in 1996, numerous
applications in approximation algorithms have
been found. Many approximation algorithms
work for problems on tree metrics or HST
metrics. By approximating general metrics with
these metrics, one can turn them into algorithms
for general metrics, while, usually, losing only
a factor of O.log n/ in the approximation factors.
Sample problems are metric labeling, buy-at-bulk
network design, and group Steiner trees. Recent
applications include an approximation algorithm
to the Unique Games [12], information network
design [13], and oblivious network design [11].
The SIGACT News article [8] is a review of
the metric approximation by tree metrics with
more detailed discussion on developments and
techniques. See also [3, 9], for other applications.
Open Problems
Given a metric induced by a graph, some applica-
tion, e.g., solving a certain class of linear systems,
does not only require a tree metric, but a tree
metric induced by a spanning tree of the graph.
Elkin, Emek, Spielman, and Teng [7] gave an
algorithm for ﬁnding a spanning tree with average
distortion of O.log2 n log log n/. It remains open
if this bound is tight.
Cross-References
▷Metrical Task Systems
▷Sparse Graph Spanners
Recommended Reading
1. Alon N, Karp RM, Peleg D, West D (1995) A graph-
theoretic game and its application to the k-server
problem. SIAM J Comput 24:78–100
2. Bartal Y (1996) Probabilistic approximation of metric
spaces and its algorithmic applications. In: FOCS
’96: proceedings of the 37th annual symposium on
foundations of computer science, Washington, DC.
IEEE Computer Society, pp 184–193
3. Bartal Y (1998) On approximating arbitrary metrices
by tree metrics. In: STOC ’98: proceedings of the
thirtieth annual ACM symposium on theory of com-
puting. ACM Press, New York, pp 161–168
4. Bartal Y, Charikar M, Raz D (2001) Approximating
min-sum k-clustering in metric spaces. In: STOC ’01:
proceedings of the thirtythird annual ACM sympo-
sium on theory of computing. ACM Press, New York,
pp 11–20
5. Calinescu G, Karloff H, Rabani Y (2001) Approx-
imation algorithms for the 0-extension problem. In:
SODA ’01: proceedings of the twelfth annual ACM-
SIAM symposium on Discrete algorithms. Society
for Industrial and Applied Mathematics, Philadel-
phia, pp 8–16
6. Charikar M, Chekuri C, Goel A, Guha S (1998)
Rounding via trees: deterministic approximation al-
gorithms for group Steiner trees and k-median. In:
STOC ’98: proceedings of the thirtieth annual ACM
symposium on theory of computing. ACM Press,
New York, pp 114–123
7. Elkin M, Emek Y, Spielman DA, Teng S-H (2005)
Lower-stretch spanning trees. In: STOC ’05: proceed-
ings of the thirty-seventh annual ACM symposium on
theory of computing. ACM Press, New York, pp 494–
503
8. Fakcharoenphol J, Rao S, Talwar K (2004) Ap-
proximating metrics by tree metrics. SIGACT News
35:60–70
9. Fakcharoenphol J, Rao S, Talwar K (2004) A tight
bound on approximating arbitrary metrics by tree
metrics. J Comput Syst Sci 69:485–497
10. Gupta A (2001) Steiner points in tree metrics don’t
(really) help. In: SODA ’01: proceedings of the
twelfth annual ACM-SIAM symposium on discrete
algorithms. Society for Industrial and Applied Math-
ematics, Philadelphia, pp 220–227
11. Gupta A, Hajiaghayi MT, Räcke H (2006) Oblivious
network design. In: SODA ’06: proceedings of the
seventeenth annual ACM-SIAM symposium on dis-
crete algorithm. ACM Press, New York, pp 970–979

116
Approximating the Diameter
12. Gupta A, Talwar K (2006) Approximating unique
games. In: SODA ’06: proceedings of the sev-
enteenth annual ACM-SIAM symposium on dis-
crete algorithm, New York. ACM Press, New York,
pp 99–106
13. Hayrapetyan A, Swamy C, Tardos É (2005) Network
design for information networks. In: SODA ’05: pro-
ceedings of the sixteenth annual ACM-SIAM sym-
posium on discrete algorithms. Society for Industrial
and Applied Mathematics, Philadelphia, pp 933–942
14. Indyk P, Matousek J (2004) Low-distortion em-
beddings of ﬁnite metric spaces. In: Goodman JE,
O’Rourke J (eds) Handbook of discrete and computa-
tional geometry. Chapman&Hall/CRC, Boca Raton,
chap. 8
15. Karp R (1989) A 2k-competitive algorithm for the
circle. Manuscript
16. Matousek J (2002) Lectures on discrete geometry.
Springer, New York
17. Rabinovich Y, Raz R (1998) Lower bounds on the dis-
tortion of embedding ﬁnite metric spaces in graphs.
Discret Comput Geom 19:79–94
18. Seymour PD (1995) Packing directed circuits frac-
tionally. Combinatorica 15:281–288
Approximating the Diameter
Liam Roditty
Department of Computer Science, Bar-Ilan
University, Ramat-Gan, Israel
Keywords
Diameter; Graph algorithms; Shortest paths
Years and Authors of Summarized
Original Work
1999; Aingworth, Chekuri, Indyk, Motwani
2013; Roditty, Vassilevska Williams
2014; Chechik, Larkin, Roditty, Schoenebeck,
Tarjan, Vassilevska Williams
Problem Deﬁnition
The diameter of a graph is the largest distance be-
tween its vertices. Closely related to the diameter
is the radius of the graph. The center of a graph is
a vertex that minimizes the maximum distance to
all other nodes, and the radius is the distance from
the center to the node furthest from it. Being able
to compute the diameter, center, and radius of a
graph efﬁciently has become an increasingly im-
portant problem in the analysis of large networks
[11]. For general weighted graphs the only known
way to compute the exact diameter and radius
is by solving the all-pairs shortest paths problem
(APSP). Therefore, a natural question is whether
it is possible to get faster diameter and radius
algorithms by settling for an approximation. For
a graph G with diameter D, a c-approximation
of D is a value
OD such that
OD 2 ŒD=c; D.
The question is whether a c-approximation can
be computed in sub-cubic time.
Key Results
For sparse directed or undirected unweighted
graphs, the best-known algorithm (ignoring poly-
logarithmic factors) for APSP, diameter, and ra-
dius does breadth-ﬁrst search (BFS) from every
node and hence runs in O.mn/ time, where m
is the number of edges in the graph. For dense
directed unweighted graphs, it is possible to com-
pute both the diameter and the radius using fast
matrix multiplication (this is folklore; for a recent
simple algorithm, see [5]), thus obtaining QO.n!/
time algorithms, where ! < 2:38 is the matrix
multiplication exponent [4, 9, 10] and n is the
number of nodes in the graph.
A 2-approximation for both the diameter and
the radius of an undirected graph can be obtained
in O.m C n/ time using BFS from an arbitrary
node. For APSP, Dor et al. [6] show that any
(2  )-approximation algorithm in unweighted
undirected graphs running in T .n/ time would
imply an O.T .n// time algorithm for Boolean
matrix multiplication (BMM). Hence a priori it
could be that (2  )-approximating the diameter
and radius of a graph may also require solving
BMM.
Aingworth et al. [1] showed that this is not
the case by presenting a sub-cubic (2  )-
approximation algorithm for the diameter in both
directed and undirected graphs that does not
use fast matrix multiplication. Their algorithm

Approximating the Partition Function of Two-Spin Systems
117
A
computes in
QO.mpn C n2/ time an estimate
OD such that
OD 2 Œb2D=3c; D. Berman and
Kasiviswanathan [2] showed that for the radius
problem the approach of Aingworth et al. can be
used to obtain in QO.mpn C n2/ time an estimate
Or that satisﬁes r 2 ŒOr; 3=2r, where r is the radius
of the graph. For weighted graphs the algorithm
of Aingworth et al. [1] guarantees that the
estimate OD satisﬁes OD 2 Œb 2
3 Dc.M 1/; D,
where M is the maximum edge weight in the
graph.
Roditty and Vassilevska Williams [8] gave
a Las Vegas algorithm running in expected
QO.mpn/ time that has the same approximation
guarantee as Aingworth et al. for the diameter
and the radius. They also showed that obtaining
a . 3
2  /-approximation algorithm running
in O.n2ı/ time in sparse undirected and
unweighted graphs for constant ; ı > 0 would
be difﬁcult, as it would imply a fast algorithm
for CNF Satisﬁability, violating the widely
believed Strong Exponential Time Hypothesis
of Impagliazzo, Paturi, and Zane [7].
Chechik et al. [3] showed that it is possible
to remove the additive error while still keeping
the running time (in terms of n) subquadratic for
sparse graphs. They present two deterministic al-
gorithms with 3
2-approximation for the diameter,
one running in QO.m
3
2 / time and one running in
QO.mn
3
2 / time.
Open Problems
The main open problem is to understand the
relation between the diameter computation and
the APSP problem. Is there a truly sub-cubic time
algorithm for computing the exact diameter or
can we show sub-cubic equivalence between the
exact diameter computation and APSP problem?
Another important open problem is to ﬁnd an
algorithm that distinguishes between graphs of
diameter two to graphs of diameter three in sub-
cubic time. Alternatively, can we show that it
is sub-cubic equivalent to the problem of exact
diameter?
Recommended Reading
1. Aingworth D, Chekuri C, Indyk P, Motwani R
(1999) Fast estimation of diameter and shortest paths
(without matrix multiplication). SIAM J Comput
28(4):1167–1181
2. Berman P, Kasiviswanathan SP (2007) Faster approx-
imation of distances in graphs. In: Proceedings of the
WADS, Halifax, pp 541–552
3. Chechik S, Larkin D, Roditty L, Schoenebeck G,
Tarjan RE, Williams VV (2014) Better approximation
algorithms for the graph diameter. In: SODA, Port-
land, pp 1041–1052
4. Coppersmith D, Winograd S (1990) Matrix multipli-
cation via arithmetic progressions. J Symb Comput
9(3):251–280
5. Cygan M, Gabow HN, Sankowski P (2012) Algorith-
mic applications of Baur-strassen’s theorem: shortest
cycles, diameter and matchings. In: Proceedings of
the FOCS, New Brunswick
6. Dor D, Halperin S, Zwick U (2000) All-pairs almost
shortest paths. SIAM J Comput 29(5):1740–1759
7. Impagliazzo R, Paturi R, Zane F (2001) Which
problems have strongly exponential complexity? J
Comput Syst Sci 63(4):512–530
8. Roditty L, Vassilevska Williams V (2013) Fast ap-
proximation algorithms for the diameter and ra-
dius of sparse graphs. In: Proceedings of the 45th
annual ACM symposium on theory of computing,
STOC ’13, Palo Alto. ACM, New York, pp 515–
524. doi:10.1145/2488608.2488673, http://doi.acm.
org/10.1145/2488608.2488673
9. Stothers A (2010) On the complexity of matrix mul-
tiplication. PhD thesis, University of Edinburgh
10. Vassilevska Williams V (2012, to appear) Multiply-
ing matrices faster than Coppersmith-Winograd. In:
Proceedings of the STOC, New York
11. Watts DJ, Strogatz SH (1998) Collective dynamics of
‘small-world’ networks. Nature 393:440–442
Approximating the Partition
Function of Two-Spin Systems
Pinyan Lu1 and Yitong Yin2
1Microsoft Research Asia, Shanghai, China
2Nanjing University, Jiangsu, Nanjing, Gulou,
China
Keywords
Approximate counting; Partition function; Two-
state spin systems

118
Approximating the Partition Function of Two-Spin Systems
Years and Authors of Summarized
Original Work
1993; Jerrum, Sinclair
2003; Goldberg, Jerrum, Paterson
2006; Weitz
2012; Sinclair, Srivastava, Thurley
2013; Li, Lu, Yin
2015; Sinclair, Srivastava, Štefankoviˇc, Yin
Problem Deﬁnition
Spin systems are well-studied objects in statisti-
cal physics and applied probability. An instance
of a spin system is an undirected graph G D
.V; E/ of n vertices. A conﬁguration of a two-
state spin system, or simply just two-spin system
on G, is an assignment  W V ! f0; 1g of two
spin states “0” and “1” (sometimes called “”
and “C” or seen as two colors) to the vertices
of G. Let A D
A0;0 A0;1
A1;0 A1;1

be a nonnegative
symmetric matrix which speciﬁes the local inter-
actions between adjacent vertices and b D
b0
b1

a nonnegative vector which speciﬁes preferences
of individual vertices over the two spin states. For
each conﬁguration  2 f0; 1gV , its weight is then
given by the following product:
w./ D
Y
fu;vg2E
A.u/;.v/
Y
v2V
b.v/:
The partition function ZA;b.G/ of a two-spin sys-
tem on G is deﬁned to be the following exponen-
tial summation over all possible conﬁgurations:
ZA;b.G/ D
X
2f0;1gV
w./:
Up to normalization, A and b can be described
by three parameters, so that one can assume that
A D
ˇ 1
1 

and b D

1

, where ˇ;   0
are the edge activities and  > 0 is the external
ﬁeld. Since the roles of the two spin states are
symmetric, it can be further assumed that ˇ 
 without loss of generality. Therefore, a two-
spin system is completely speciﬁed by the three
parameters .ˇ; ; / where it holds that 0  ˇ 
 and  > 0. The resulting partition function is
written as Z.ˇ;;/.G/ D ZA;b.G/ and as Z.G/
for short if the parameters are clear from the
context.
The two-spin systems are classiﬁed according
to their parameters into two families with dis-
tinct physical and computational properties: the
ferromagnetic two-spin systems (ˇ
> 1) in
which neighbors favor agreeing spin states and
the antiferromagnetic two-spin systems (ˇ < 1)
in which neighbors favor disagreeing spin states.
Two-spin systems with ˇ D 1 are trivial in
both physical and computational senses and thus
are usually not considered. The model of two-
spin systems covers some of the most exten-
sively studied statistical physics models as spe-
cial cases, as well as being accepted in computer
science as a framework for counting problems,
for examples:
•
When ˇ D 0, 
D 1, and  D 1, the
Z.ˇ;;/.G/ gives the number of independent
sets (or vertex covers) of G.
•
When ˇ D 0 and  D 1, the Z.ˇ;;/.G/ is the
partition function of the hardcore model with
fugacity  on G.
•
When ˇ D , the Z.ˇ;;/.G/ is the partition
function of the Ising model with edge activity
ˇ and external ﬁeld  on G.
Given a set of parameters .ˇ; ; /, the
computational problem
TWO-SPIN.ˇ; ; / is
the problem of computing the value of the
partition function Z.ˇ;;/.G/ when the graph
G is given as input. This problem is known to
be #P-hard except for the trivial cases where
ˇ D 1 or ˇ D  D 0 [1]. Therefore, the
main focus here is the efﬁcient approximation
algorithms for
TWO-SPIN.ˇ; ; /. Formally,
a fully polynomial-time approximation scheme
(FPTAS) is an algorithm which takes G and
any " > 0 as input and outputs a number OZ
satisfying Z.G/ exp."/ 
OZ  Z.G/ exp."/
within time polynomial in n and 1="; and a fully

Approximating the Partition Function of Two-Spin Systems
119
A
polynomial-time
randomized
approximation
scheme (FPRAS) is its randomized relaxation
in which randomness is allowed and the above
accuracy of approximation is required to be
satisﬁed with high probability.
For many important two-spin systems (e.g., in-
dependent sets, antiferromagnetic Ising model), it
is NP-hard to approximate the partition function
on graphs of unbounded degrees. In these cases,
the problem is further reﬁned to consider the
approximation algorithms for TWO-SPIN.ˇ; ; /
on graphs with bounded maximum degree. In
addition, in order to study the approximation
algorithms on graphs which has bounded average
degree or on special classes of lattice graphs, the
approximation of partition function is studied on
classes of graphs with bounded connective con-
stant, a natural and well-studied notion of average
degree originated from statistical physics.
Therefore, the main problem of interest is to
characterize the regimes of parameters .ˇ; ; /
for which there exist efﬁcient approximation al-
gorithms for TWO-SPIN.ˇ; ; / on classes of
graphs with bounded maximum degree max, or
on classes of graphs with bounded connective
constant , or on all graphs.
Key Results
Given a two-spin system on graph G D .V; E/, a
natural probability distribution  over all conﬁg-
urations  2 f0; 1gV , called the Gibbs measure,
can be deﬁned by ./ D
w./
Z.G/, where w./ D
Q
fu;vg2E Au;v
Q
v2V bv is the weight of 
and the normalizing factor Z.G/ is the partition
function.
The Gibbs measure deﬁnes a marginal distri-
bution at each vertex. Suppose that a conﬁgura-
tion  is sampled according to the Gibbs measure
. Let pv denote the probability of vertex v
having spin state “0” in ; and for a ﬁxed con-
ﬁguration 

 2 f0; 1g
 partially speciﬁed over
vertices in   V , let p
v
denote the probability
of vertex v having spin state “0” conditioning on
that the conﬁguration of vertices in  in  is as
speciﬁed by 

.
The marginal probability plays a key role in
computing the partition function. Indeed, the
marginal probability p
v
itself is a quantity
of main interest in many applications such as
probabilistic inference. In addition, due to the
standard procedure of self-reduction, an FPTAS
for the partition function Z.G/ can be obtained if
the value of p
v
can be approximately computed
with an additive error " in time polynomial
in both n and 1=". This reduces the problem
of approximating the partition function (with
multiplicative
errors)
to
approximating
the
marginal probability (with additive errors), which
is achieved either by rapidly mixing random
walks or by recursions exhibiting a decay of
correlation.
Ferromagnetic Two-Spin Systems
For
the
ferromagnetic
case,
the
problem
TWO-SPIN.ˇ; ; / is considered for ˇ > 1
and without loss of generality for ˇ  .
In a seminal work [3], Jerrum and Sinclair
gave an FPRAS for approximately computing
the partition function of the ferromagnetic Ising
model, which is the TWO-SPIN.ˇ; ; / problem
with ˇ D  > 1.
The algorithm uses the Markov chain Monte
Carlo (MCMC) method; however very interest-
ingly, it does not directly apply the random walk
over conﬁgurations of two-spin system since such
random walk might have a slow mixing time.
Instead, it ﬁrst transforms the two-spin system
into conﬁgurations of the so-called “subgraphs
world”: each such conﬁguration is a subgraph of
G. A random walk over the subgraph conﬁgura-
tions is applied and proved to be rapidly mixing
for computing the new partition function deﬁned
over subgraphs, which is shown to be equal to the
partition function Z.G/ of the two-spin system.
This equivalence is due to that this transformation
between the “spins world” and the “subgraphs
world” is actually a holographic transformation,
which is guaranteed to preserve the value of the
partition function.
The result of [3] can be stated as the following
theorem.

120
Approximating the Partition Function of Two-Spin Systems
Theorem 1 If ˇ D  > 1 and  > 0, then there
is an FPRAS for TWO-SPIN.ˇ; ; /.
The algorithm actually works for a stronger
setting where the external ﬁelds are local (vertices
have different external ﬁelds) as long as the ex-
ternal ﬁelds are homogeneous (all have the same
preference over spin states).
For the two-spin system with general ˇ and
, one can translate it to the Ising model where
ˇ D  by delegating the effect of the general ˇ; 
to the degree-dependent effective external ﬁelds.
This extends the FPRAS for the ferromagnetic
Ising model to certain regime of ferromagnetic
two-spin systems, stated as follows.
Theorem 2 ([2, 6]) If ˇ
<
, ˇ
>
1,
and   =ˇ, then there is an FPRAS for
TWO-SPIN.ˇ; ; /.
If one is restricted to the deterministic algo-
rithms for approximating the partition function,
then a deterministic FPTAS is known for a strictly
smaller regime, implicitly stated in the following
theorem.
Theorem 3 ([7]) There is a continuous mono-
tonically
increasing
function
 ./
deﬁned
on Œ1; C1/ satisfying (1)  .1/
D
1, (2)
1
<
 ./
<
 for all 
>
1, and (3)
lim!C1
 ./

D 1, such that there is an FPTAS
for TWO-SPIN.ˇ; ; / if ˇ > 1, ˇ   ./,
and   1.
This deterministic FPTAS uses the same holo-
graphic transformation from two-spin systems
to the “subgraphs world” as in [3], and it ap-
proximately computes the marginal probability
deﬁned in the subgraphs world by a recursion.
The accuracy of the approximation is guaranteed
by the decay of correlation. This technique is
more extensively and successfully used for the
antiferromagnetic two-spin systems.
On the other hand, assuming certain complex-
ity assumptions, it is unlikely that for every ferro-
magnetic two-spin system its partition function is
easy to approximate.
Theorem 4 ([6]) For any ˇ <  with ˇ > 1,
there is a 0 such that TWO-SPIN.ˇ; ; / is
#BIS-hard for all   0.
Antiferromagnetic Two-Spin Systems
For the antiferromagnetic case, the problem
TWO-SPIN.ˇ; ; / is considered for ˇ < 1
and without loss of generality for ˇ  .
In [2], a heatbath random walk over spin
conﬁgurations is applied to obtain an FPRAS for
TWO-SPIN.ˇ; ; / for a regime of antiferromag-
netic two-spin systems.
The regime of antiferromagnetic two-spin sys-
tems whose partition function is efﬁciently ap-
proximable is characterized by the uniqueness
condition.
Given parameters .ˇ; ; / and d  1, the tree
recursion f .x/ is given by
f .x/ D 
ˇx C 1
x C 
d
:
(1)
For antiferromagnetic .ˇ; ; /, the function f .x/
is decreasing in x; thus, there is a unique positive
ﬁxed point Ox satisfying Ox D f . Ox/. Consider the
absolute derivative of f .x/ at the ﬁxed point:
ˇˇf 0. Ox/
ˇˇ D
d.1  ˇ/ Ox
.ˇ Ox C 1/. Ox C /:
Deﬁnition 1 Let 0

ˇ

, ˇ
<
1,
and
d

1.
The
uniqueness
condition
UNIQUE.ˇ; ; ; d/ is satisﬁed if jf 0. Ox/j < 1;
and the condition NON-UNIQUE.ˇ; ; ; d/ is
satisﬁed if jf 0. Ox/j > 1.
The condition UNIQUE.ˇ; ; ; d/ holds if
and only if the dynamical system (1) converges
to its unique ﬁxed point Ox at an exponential rate.
The name uniqueness condition is due to that
UNIQUE.ˇ; ; ; d/ implies the uniqueness of the
Gibbs measure of two-spin system of parameters
.ˇ; ; / on the Bethe lattice (i.e., the inﬁnite
d-regular tree) and NON-UNIQUE.ˇ; ; ; d/
implies that there are more than one such
measures (Fig. 1).
Efﬁcient
approximation
algorithms
for
TWO-SPIN.ˇ; ; / are discovered for special
cases of antiferromagnetic two-spin systems
within the uniqueness regime, including the
hardcore model [12], the antiferromagnetic Ising
model [8], and the antiferromagnetic two-spin

Approximating the Partition Function of Two-Spin Systems
121
A
Approximating the Partition Function of Two-Spin Systems, Fig. 1 The regime of .ˇ; / for which the unique-
ness condition UNIQUE.ˇ; ; ; d/ holds for  D 1 and for all integer d  1
systems without external ﬁeld [4], and ﬁnally for
all antiferromagnetic two-spin systems within the
uniqueness regime [5].
Theorem 5 ([5]) For 0  ˇ   and ˇ <
1, there is an FPTAS for TWO-SPIN.ˇ; ; /
on graphs of maximum degree at most max if
UNIQUE.ˇ; ; ; d/ holds for all integer 1 
d  max  1.
This algorithmic result for graphs of bounded
maximum degree can be extended to graphs of
unbounded degrees.
Theorem 6 ([4, 5]) For 0  ˇ   and ˇ <
1, there is an FPTAS for TWO-SPIN.ˇ; ; / if
UNIQUE.ˇ; ; ; d/ holds for all integer d  1.
All these algorithms follow the framework
introduced by Weitz in his seminal work [12].
In this framework, the marginal probability p
v
is computed by applying the tree recursion (1)
on the tree of self-avoiding walks, (In fact, (1)
is the recursion for the ratio p
v =.1  p
v / of
marginal probabilities.) which enumerates all
paths originated from vertex v. Then, a decay
of correlation, also called the spatial mixing
property, is veriﬁed, so that a truncated recursion
tree of polynomial size is sufﬁcient to provide
the required accuracy for the estimation of the
marginal probability. For graphs of unbounded
degrees, a stronger notion of decay of correlation,
called the computationally efﬁcient correlation
decay [4], is veriﬁed to enforce the same cost and
accuracy even when the branching number of the
recursion tree is unbounded.
On the other hand, for antiferromagnetic two-
spin systems in the nonuniqueness regime, the
partition function is hard to approximate.
Theorem 7 ([11]) Let 0  ˇ   and ˇ < 1.
For any max  3, unless NP D RP , there
does not exist an FPRAS for TWO-SPIN.ˇ; ; /
on graphs of maximum degree at most max if
NON-UNIQUE.ˇ; ; ; d/ holds for some integer
1  d  max  1.

122
Approximating the Partition Function of Two-Spin Systems
Altogether, this gives a complete classiﬁcation
of the approximability of partition function of
antiferromagnetic two-spin systems except for
the uniqueness threshold.
Algorithms for Graphs with Bounded
Connective Constant
The connective constant is a natural and well-
studied notion of the average degree of a graph,
which, roughly speaking, measures the growth
rate of the number of self-avoiding walks in
the graph as their length grows. As a quantity
originated from statistical physics, the connec-
tive constant has been especially well studied
for various inﬁnite regular lattices. In order to
suit the algorithmic applications, the deﬁnition
of connective constant was extended in [9] to
families of ﬁnite graphs.
Given a vertex v in a graph G, let N.v; l/ de-
note the number of self-avoiding walks of length
` in G which start at v.
Deﬁnition 2 ([9]) Let G be a family of ﬁnite
graphs. The connective constant of G is at most
 if there exist constants a and c such that for
any graph G D .V; E/ in G and any vertex v
in G, it holds that P`
iD1 N.v; i/  c` for all
`  a log jV j.
The connective constant has a natural interpre-
tation as the “average arity” of the tree of self-
avoiding walks.
For certain antiferromagnetic two-spin sys-
tems, it is possible to establish the desirable decay
of correlation on the tree of self-avoiding walks
with bounded average arity instead of maximum
arity, and hence the arity d in the uniqueness
condition UNIQUE.ˇ; ; ; d/ can be replaced
with the connective constant . The algorithmic
implication of this is stated as the following
theorem.
Theorem 8 ([10]) For
the
following
two
cases:
•
(The hardcore model) ˇ D 0 and  D 1;
•
(The antiferromagnetic Ising model with zero
ﬁeld) ˇ D  < 1 and  D 1;
there exists an FPTAS for TWO-SPIN.ˇ; ; /
on graphs of connective constant at most  if
UNIQUE.ˇ; ; ; / holds.
For the two-spin systems considered by this the-
orem, it holds that UNIQUE.ˇ; ; ; / implies
UNIQUE.ˇ; ; ; d/ for all 1  d  .
The connective constant of a graph of max-
imum degree max is at most max  1, but
the connective constant of a family of graphs
can be much smaller than this crude bound. For
example, though the maximum degree of a graph
drawn from the Erdös-Rényi model G.n; d=n/
is 	.log n= log log n/ with high probability, the
connective constant of such a graph is at most
d.1 C "/ with high probability for any ﬁxed
" > 0. Therefore, for the considered two-spin
systems, the algorithm in Theorem 8 works on
strictly more general families of graphs than that
of Theorem 5.
Cross-References
▷Complexity Dichotomies for Counting Graph
Homomorphisms
Recommended Reading
1. Bulatov AA, Grohe M (2005) The complexity of
partition functions. Theor Comput Sci 348(2–3):148–
186
2. Goldberg LA, Jerrum M, Paterson M (2003) The
computational complexity of two-state spin systems.
Random Struct Algorithms 23(2):133–154
3. Jerrum M, Sinclair A (1993) Polynomial-time ap-
proximation algorithms for the ising model. SIAM J
Comput 22(5):1087–1116
4. Li L, Lu P, Yin Y (2012) Approximate counting via
correlation decay in spin systems. In: Proceedings of
SODA, Kyoto, pp 922–940
5. Li L, Lu P, Yin Y (2013) Correlation decay up
to uniqueness in spin systems. In: Proceedings of
SODA, New Orleans, pp 67–84
6. Liu J, Lu P, Zhang C (2014) The complexity of
ferromagnetic two-spin systems with external ﬁelds.
In: Proceedings of RANDOM, Barcelona. To appear
7. Lu P, Wang M, Zhang C (2014) Fptas for weighted
ﬁbonacci gates and its applications. In: Esparza J,
Fraigniaud P, Husfeldt T, Koutsoupias E (eds) ICALP
(1), Copenhagen. Lecture Notes in Computer Sci-
ence, vol 8572. Springer, pp 787–799

Approximation Schemes for Bin Packing
123
A
8. Sinclair A, Srivastava P, Thurley M (2012) Approx-
imation algorithms for two-state anti-ferromagnetic
spin systems on bounded degree graphs. In: Proceed-
ings of SODA, Kyoto, pp 941–953
9. Sinclair A, Srivastava P, Yin Y (2013) Spatial mix-
ing and approximation algorithms for graphs with
bounded connective constant. In: Proceedings of
FOCS, Berkeley, pp 300–309
10. Sinclair A, Srivastava P, Štefankoviˇc D, Yin Y (2015)
Spatial mixing and the connective constant: Optimal
bounds. In: Proceedings of SODA, San Diego. To
appear
11. Sly A, Sun N (2012) The computational hardness of
counting in two-spin models on d-regular graphs. In:
Proceedings of FOCS, New Brunswick, pp 361–369
12. Weitz D (2006) Counting independent sets up to
the tree threshold. In: Proceedings of STOC, Seattle,
pp 140–149
Approximation Schemes for Bin
Packing
Nikhil Bansal
Eindhoven University of Technology,
Eindhoven, The Netherlands
Keywords
Cutting-stock problem
Years and Authors of Summarized
Original Work
1982; Karmarker, Karp
Problem Deﬁnition
In the bin-packing problem, the input consists
of a collection of items speciﬁed by their sizes.
There are also identical bins, which without loss
of generality can be assumed to be of size 1, and
the goal is to pack these items using the minimum
possible number of bins.
Bin packing is a classic optimization problem,
and hundreds of its variants have been deﬁned
and studied under various settings such as av-
erage case analysis, worst-case off-line analysis,
and worst-case online analysis. This note consid-
ers the most basic variant mentioned above under
the off line model where all the items are given
in advance. The problem is easily seen to be NP-
hard by a reduction from the partition problem.
In fact, this reduction implies that unless P = NP,
it is impossible to determine in polynomial time
whether the items can be packed into two bins or
whether they need three bins.
Notations
The input to the bin-packing problem is a set of n
items I speciﬁed by their sizes s1; : : : ; sn, where
each si is a real number in the range (0,1]. A
subset of items S  I can be packed feasibly in a
bin if the total size of items in S is at most 1. The
goal is to pack all items in I into the minimum
number of bins. Let OPT.I/ denote the value of
the optimum solution and Size(I) the total size of
all items in I. Clearly, OPT.I/  d Size.I/e.
Strictly speaking, the problem does not admit
a polynomial-time algorithm with an approxi-
mation guarantee better than 3/2. Interestingly,
however, this does not rule out an algorithm that
requires, say, OPT.I/ C 1 bins (unlike other
optimization problems, making several copies of
a small hard instance to obtain a larger hard in-
stance does not work for bin packing). It is more
meaningful to consider approximation guarantees
in an asymptotic sense. An algorithm is called an
asymptotic  approximation if the number of bins
required by it is   OPT.I/ C O(1).
Key Results
During the 1960s and 1970s, several algorithms
with constant factor asymptotic and absolute ap-
proximation guarantees and very efﬁcient run-
ning times were designed (see [1] for a survey). A
breakthrough was achieved in 1981 by de la Vega
and Lueker [3], who gave the ﬁrst polynomial-
time asymptotic approximation scheme.
Theorem 1 ([3]) Given any arbitrary parameter
 > 0, there is an algorithm that uses .1 C
/OPT.I/ C O.1/ bins to pack I. The running

124
Approximation Schemes for Bin Packing
time of this algorithm is O.n log n/ C .1 C
/O.1=/.
The main insight of de la Vega and Lueker
[3] was to give a technique for approximating
the original instance by a simpler instance where
large items have only O(1) distinct sizes. Their
idea was simple. First, it sufﬁces to restrict at-
tention to large items, say, with size greater than
". These can be called Ib. Given an (almost)
optimum packing of Ib, consider the solution
obtained by greedily ﬁlling up the bins with
remaining small items, opening new bins only
if needed. Indeed, if no new bins are needed,
then the solution is still almost optimum since the
packing for Ib was almost optimum. If additional
bins are needed, then each bin, except possibly
one, must be ﬁlled to an extent .1  /, which
gives a packing using Size.I/=.1  / C 1 
OPT.I/=.1  / C 1 bins. So it sufﬁces to focus
on solving Ib almost optimally. To do this, the
authors show how to obtain another instance
I 0 with the following properties. First, I 0 has
only O.1=2/ distinct sizes, and second, I 0 is an
approximation of Ib in the sense that OPT.Ib/ 
OPT.I 0/, and moreover, any solution of I 0 im-
plies another solution of Ib using O.  OPT.I//
additional bins. As I 0 has only 1=2 distinct item
sizes, and any bin can obtain at most 1= such
items, there are at most O

1=21= ways to
pack a bin. Thus, I 0 can be solved optimally by
exhaustive enumeration (or more efﬁciently using
an integer programming formulation described
below).
Later, Karmarkar, and Karp [4] proved a sub-
stantially stronger guarantee.
Theorem 2 ([4]) Given an instance I, there is
an algorithm that produces a packing of I using
OPT.I/ C O.log2 OPT.I// bins. The running
time of this algorithm is O.n8/.
Observe that this guarantee is signiﬁcantly
stronger than that of [3] as the additive term
is O.log2OPT/ as opposed to o .  OPT/. Their
algorithm also uses the ideas of reducing the
number of distinct item sizes and ignoring small
items, but in a much more reﬁned way. In par-
ticular, instead of obtaining a rounded instance
in a single step, their algorithm consists of a
logarithmic number of steps where in each step
they round the instance “mildly” and then solve it
partially.
The starting point is an exponentially large
linear programming (LP) relaxation of the prob-
lem commonly referred to as the conﬁguration
LP. Here, there is a variable xS corresponding
to each subset of items S that can be packed
feasibly in a bin. The objective is to minimize
P
S
xS subject to the constraint that for each item
i, the sum of xS over all subsets S that contain
i is at least 1. Clearly, this is a relaxation as
setting xS D 1 for each set S corresponding to a
bin in the optimum solution is a feasible integral
solution to the LP. Even though this formulation
has exponential size, the separation problem for
the dual is a knapsack problem, and hence the LP
can be solved in polynomial time to any accuracy
(in particular within an accuracy of 1) using the
ellipsoid method. Such a solution is called a
fractional packing. Observe that if there are ni
items each of size exactly si, then the constraints
corresponding to i can be “combined” to obtain
the following LP:
min P
S
xS
s.t.
P
S
aS;ixS  ni
8item sizes i
xS  0
8 feasible sets S:
Here, aS;i is the number of items of size si in the
feasible S. Let q.I/ denote the number of distinct
sizes in I. The number of nontrivial constraints
in LP is equal to q.I/, which implies that there is
a basic optimal solution to this LP that has only
q.I/ variables set nonintegrally. Karmarkar and
Karp exploit this observation in a very clever way.
The following lemma describes the main idea.
Lemma 1 Given any instance J , suppose there
is an algorithmic rounding procedure to obtain
another instance J 0 such that J 0 has Size.J /=2
distinct item sizes and J and J 0 are related in the
following sense: given any fractional packing of
J using ` bins gives a fractional packing of J 0
with at most ` bins, and given any packing of J 0
using `0 bins gives a packing of J using `0 C c

Approximation Schemes for Bin Packing
125
A
bins, where c is some ﬁxed parameter. Then, J
can be packed using OPT.J / C c  log.OPT.J //
bins.
Proof Let I0 D I and let I1 be the instance
obtained by applying the rounding procedure to
I0. By the property of the rounding procedure,
OPT.I/  OPT.I1/Cc and LP.I1/  LP.I/. As
I1 has Size.I0/=2 distinct sizes, the LP solution
for I1 has at most Size.I0/=2 fractionally set vari-
ables. Remove the items packed integrally in the
LP solution, and consider the residual instance
I 0
1. Note that Size.I 0
1/  Size.I0/=2. Now, again
apply the rounding procedure to I 0
1 to obtain I2
and solve the LP for I2. Again, this solution has
at most Size.I 0
1/=2  Size.I0/=4 fractionally
set variables and OPT.I 0
1/  OPT.I2/ C c and
LP.I2/  LP.I 0
1/. The above process is repeated
for a few steps. At each step, the size of the
residual instance decreases by a factor of at least
two, and the number of bins required to pack I0
increases by additive c. After log.Size.I0//.
log.OPT.I/// steps, the residual instance has size
O(1) and can be packed into O(1) additional
bins.
ut
It remains to describe the rounding procedure.
Consider the items in nondecreasing order s1 
s2      sn and group them as follows.
Add items to current group until its size ﬁrst
exceeds 2. At this point, close the group and start
a new group. Let G1; : : : ; Gk denote the groups
formed and let ni D jGij, setting n0 D 0 for
convenience. Deﬁne I 0 as the instance obtained
by rounding the size of ni1 largest items in Gi
to the size of the largest item in Gi for i D
1; : : : ; k. The procedure satisﬁes the properties of
Lemma 1 with c D O.log nk/ (left as an exercise
to the reader). To prove Theorem 2, it sufﬁces to
show that nk D O.Size.I//. This is done easily
by ignoring all items smaller than 1=Size.I/ and
ﬁlling them in only in the end (as in the algorithm
of de la Vega and Lueker).
In the case when the item sizes are not too
small, the following corollary is obtained.
Corollary 1 If all the item sizes are at least
ı, it is easily seen that c
D
O.log1=ı/,
and the above algorithm implies a guarantee
of OPT C O.log.1=ı/  log OPT/, which is
OPT C O.log OPT/ if ı is a constant.
Recently, Rothvoss gave the ﬁrst
improve-
ment to the result of Karmarkar and Karp and im-
prove their additive guarantee from O.log2Opt/
to O(log Opt log log Opt). His algorithm also
uses the conﬁguration LP solution and is based on
several new ideas and recent developments. First
is the connection of bin packing to a problem in
discrepancy theory known as the k-permutation
problem. Second are the recently developed al-
gorithmic approaches for addressing discrepancy
minimization problems.
In addition to these, a key idea in Rothvoss’
algorithm is to glue several small items contained
in a conﬁguration into a new large item. For more
details, we refer the reader to [5].
Applications
The bin-packing problem is directly motivated
from practice and has many natural applications
such as packing items into boxes subject to
weight constraints, packing ﬁles into CDs,
packing
television
commercials
into
station
breaks, and so on. It is widely studied in
operations research and computer science. Other
applications include the so-called cutting-stock
problems where some material such as cloth or
lumber is given in blocks of standard size from
which items of certain speciﬁed size must be
cut. Several variations of bin packing, such as
generalizations to higher dimensions, imposing
additional constraints on the algorithm and
different optimization criteria, have also been
extensively studied. The reader is referred to
[1,2] for excellent surveys.
Open Problems
Except for the NP-hardness, no other hardness
results are known, and it is possible that a
polynomial-time algorithm with guarantee OPT
C 1 exists for the problem. Resolving this is a key
open question. A promising approach seems to
be via the conﬁguration LP (considered above).

126
Approximation Schemes for Geometric Network Optimization Problems
In fact, no instance is known for which the
additive gap between the optimum conﬁguration
LP solution and the optimum integral solution
is more than 1. It would be very interesting to
design an instance that has an additive integrality
gap of two or more.
Cross-References
▷Bin Packing
▷Knapsack
Recommended Reading
1. Coffman EG, Garey MR, Johnson DS (1996) Ap-
proximation algorithms for bin packing: a survey. In:
Hochbaum D (ed) Approximation algorithms for NP-
hard problems. PWS, Boston, pp 46–93
2. Csirik J, Woeginger G (1998) On-line packing and
covering problems. In: Fiat A, Woeginger G (eds)
Online algorithms: the state of the art. LNCS, vol 1442.
Springer, Berlin, pp 147–177
3. Fernandez de la Vega W, Lueker G (1981) Bin packing
can be solved within 1C" in linear time. Combinator-
ica 1:349–355
4. Karmarkar N, Karp RM (1982) An efﬁcient approx-
imation scheme for the one-dimensional bin-packing
problem. In: Proceedings of the 23rd IEEE symposium
on foundations of computer science (FOCS), Chicago,
pp 312–320
5. Rothvoss T (2013) Approximating bin packing withing
O(log OPT log log OPT) bins. In: Proceedings of the
54th IEEE symposium on foundations of computer
science (FOCS), Berkeley, pp 20–29
Approximation Schemes for
Geometric Network Optimization
Problems
Joseph S.B. Mitchell
Department of Applied Mathematics and
Statistics, Stony Brook University, Stony Brook,
NY, USA
Keywords
Approximation
algorithms;
Computational
geometry; Geometric networks; Optimization;
Polynomial-time approximation scheme; Travel-
ing salesperson problem
Years and Authors of Summarized
Original Work
1998; Arora
1999: Mitchell
1998; Rao, Smith
Problem Deﬁnition
Geometric network optimization is the problem
of computing a network in a geometric space
(e.g.,
the
Euclidean
plane),
based
on
an
input of geometric data (e.g., points, disks,
polygons/polyhedra) that is optimal according
to an objective function that typically involves
geometric measures, such as Euclidean length,
perhaps in addition to combinatorial metrics,
such as the number of edges in the network.
The desired network is required to have certain
properties, such as being connected (or k-
connected), having a speciﬁc topology (e.g.,
forming a path/cycle), spanning at least a certain
number of input objects, etc.
One of the most widely studied optimization
problems is the traveling salesperson problem
(TSP): given a set S of n sites (e.g., cities), and
distances between each pair of sites, determine a
route or tour of minimum length that visits every
member of S. The (symmetric) TSP is often for-
mulated in terms of a graph optimization problem
on an edge-weighted complete graph Kn, and the
goal is to determine a Hamiltonian cycle (a cycle
visiting each vertex exactly once), or a tour, of
minimum total weight. In geometric settings, the
sites are often points in the plane with distances
measured according to the Euclidean metric.
The TSP is known to be NP-complete in
graphs and NP-hard in the Euclidean plane. Many
methods of combinatorial optimization, as well
as heuristics, have been developed and applied
successfully to solving to optimality instances of
TSP; see Cook [7]. Our focus here is on provable
approximation algorithms.

Approximation Schemes for Geometric Network Optimization Problems
127
A
In the context of the TSP, a minimization prob-
lem, a c-approximation algorithm is an algorithm
guaranteed to yield a solution whose objective
function value (length) is guaranteed to be at most
c times that of an optimal solution. A polynomial-
time approximation scheme (PTAS) is a family of
c-approximation algorithms, with c D 1 C ", that
runs in polynomial (in input size) time for any
ﬁxed " > 0. A quasi-polynomial-time approx-
imation scheme (QPTAS) is an approximation
scheme, with factor c D 1 C " for any ﬁxed
" > 0, whose running time is quasi-polynomial,
2O..log n/C /, for some C.
In the Euclidean Steiner minimum spanning
tree (SMST) problem, the objective is to compute
a minimum total length tree that spans all of the
input points S, allowing nodes of the tree to be
at points of the Euclidean space other than S
(such points are known as Steiner points). The
Euclidean SMST is known to be NP-hard, even
in the plane.
Key Results
A simple 2-approximation algorithm for TSP
follows from a “doubling” of a minimum
spanning
tree,
assuming
that
the
distances
obey the triangle inequality. By augmenting the
minimum spanning tree with a minimum-weight
matching on the odd-degree nodes of the tree,
Christoﬁdes obtained a 1.5-approximation for
TSP with triangle inequality. This is the currently
best-known approximation for general metric
spaces; an outstanding open conjecture is that a
4/3-approximation (or better) may be possible. It
is known that the TSP in a general metric space
is APX-complete, implying that, unless P D NP,
no PTAS exists, in general.
Research has shown that “geometry helps” in
network optimization problems. Geometric struc-
ture has played a key role in solving combinato-
rial optimization problems. There are problems
that are NP-hard in their abstract generality, yet
are solvable exactly in polynomial time in geo-
metric settings (e.g., maximum TSP in polyhe-
dral metrics), and there are problems for which
we have substantially better, or more efﬁcient,
approximation algorithms in geometric settings
(e.g., TSP).
As shown by Arora [1] and Mitchell [10] in
papers originally appearing in 1996, geometric
instances of TSP and SMST have special
structure that allows for the existence of a
PTAS. Arora [1] gives a randomized algorithm
that, with probability 1/2, yields a .1 C "/-
approximate tour in time n.log n/O.
p
d="/d1
in Euclidean d-space. Rao and Smith [14]
obtain a deterministic algorithm with running
time
2.d="/O.d/n C .d="/O.d/n log n.
This
O.n log n/ bound (for ﬁxed d; ") matches the
˝.n log n/ lower bound in the decision tree
bound. In the real RAM model, with atomic
ﬂoor or mod function, Bartal and Gottlieb [3]
give
a
randomized
linear-time
PTAS
that,
with probability 1  eO.n1=3d /, computes a
.1 C "/-approximation to an optimal tour in time
2.d="/O.d/n. The exponential dependence on d
in the PTAS bounds is essentially best possible,
since Trevisan has shown that if d  log n, it is
NP-hard to obtain a .1 C "/-approximation.
A key insight of Rao and Smith is the applica-
tion of the concept of “spanners” to the approx-
imation schemes. A connected subgraph G of
the complete Euclidean graph, joining every pair
of points in S (within Euclidean d-dimensional
space), is said to be a t-spanner for S if all points
of S are nodes of G and, for any points u; v 2 S,
the length of a shortest path in G from u to v is
at most t times the Euclidean distance, d2.u; v/.
It is known that for any point set S and t > 1,
t-spanners exist and can be calculated in time
O.n log n/, with the property that the t-spanner
is lightweight, meaning that the sum of its edge
lengths is at most a constant factor (depending on
d and t) greater than the Euclidean length of a
minimum spanning tree on S.
Overview of Methods
The PTAS techniques are based on structure the-
orems showing that an optimal solution can be
“rounded” to a “nearby” solution, of length at
most a factor .1 C "/ longer, that falls within a
special class of recursively “nice” solutions for
which optimization via dynamic programming

128
Approximation Schemes for Geometric Network Optimization Problems
can be done efﬁciently, because the interface
between adjacent subproblems is “small” com-
binatorially. Arora’s algorithm [1] is random-
ized, as is that of Rao and Smith [14]; both
can be derandomized. The m-guillotine method
(Mitchell [10]) is directly a deterministic method;
however, the proof of its structure theorem is
effectively an averaging argument.
Arora’s Dissection Method
Arora [1, 2] gives a method based on geomet-
ric dissection using a quadtree (or its octtree
analogue in d dimensions). On the boundary
of each quadtree square are m equally spaced
points (“portals”); a portal-respecting tour is one
that crosses the boundaries of squares only at
portals. Using an averaging argument based on
a randomly shifted quadtree that contains the
bounding square of S, Arora proves structure
theorems, the simplest of which shows that, when
m > .log n/=, the expected length of a shortest
portal-respecting tour, T , is at most .1 C / times
the length of an optimal tour. Within a quadtree
square, T consists of at most m disjoint paths that
together visit all sites within the square. Since
the interface data specifying a subproblem has
size 2O.m/, dynamic programming computes a
shortest portal-respecting tour in time 2.O.m/ per
quadtree square, for overall time 2O..log n/=/ D
nO.1=/. An improved, near-linear (randomized)
running time is obtained via a stronger structure
theorem, based on “.m; k/-light” tours, which
are portal respecting and enter/leave each square
at most k times (with k D O.1="/). Rao and
Smith’s improvement uses the observation that it
sufﬁces to restrict the algorithm to use edges of a
.1 C "/-spanner.
The m-Guillotine Method
The m-guillotine method of Mitchell [10] is
based on the notion of an m-guillotine structure.
A geometric graph G in the plane has the m-
guillotine structure if the following holds: either
(1) G has O.m/ edges or (2) there exists a cut by
an axis-parallel line ` such that the intersection
of ` with the edge set of G has O.m/ connected
components and the subgraphs of G on each side
of the cut recursively also have the m-guillotine
structure. The m-guillotine structure is deﬁned in
dimensions d > 2 as well, using hyperplane cuts
orthogonal to the coordinate axes.
The m-guillotine structure theorem in 2 di-
mensions states that, for any positive integer m,
a set E of straight line segments in the plane is
either m-guillotine already or is “close” to being
m-guillotine, in that there exists a superset, Em 
E that has m-guillotine structure, where Em is
obtained from E by adding a set of axis-parallel
segments (bridges, or m-spans) of total length at
most O."jEj/. The proof uses a simple charging
scheme.
The m-guillotine method originally (1996)
yielded a PTAS for TSP and related problems in
the plane, with running time nO.1=/; this was
improved (1997) to nO.1/. With the injection
of the idea of Rao and Smith [14] to employ
spanners, the m-guillotine method yields a
simple, deterministic O.n log n/ time PTAS for
TSP and related problems in ﬁxed dimension
d  2. The steps are the following: (a) construct
(in O.n log n/ time) a spanner, T ; (b) compute
its m-guillotine superset, Tm, by standard sweep
techniques (in time O.n log n/); and (c) use
dynamic programming (time O.n/) applied to
the recursive tree associated with Tm, to optimize
over spanning subgraphs of Tm.
Generalizations to Other Metrics
The PTAS techniques described above have
been signiﬁcantly extended to variants of the
Euclidean TSP. While we do not expect that a
PTAS exists for general metric spaces (because
of APX-hardness), the methods can be extended
to a very broad class of “geometric” metrics
known as doubling metrics, or metric spaces of
bounded doubling dimension. A metric space X
is said to have doubling constant cd if any ball of
radius r can be covered by cd balls of radius r=2;
the logarithm of cd is the doubling dimension of
X. Euclidean d-space has doubling dimension
O.d/. Bartal, Gottlieb, and Krauthgamer [4]
have given a PTAS for TSP in doubling metrics,
improving on a prior QPTAS.
For the discrete metric space induced by an
edge-weighted planar graph, the TSP has a linear-
time PTAS. The subset TSP for edge-weighted

Approximation Schemes for Geometric Network Optimization Problems
129
A
planar graphs, in which there is a subset S  V
of the vertex set V that must be visited, also has
an efﬁcient (O.n log n/ time) PTAS; this implies
a PTAS for the geodesic metric for TSP on a set
S of sites in a polygonal domain in the plane,
with distances given by the (Euclidean) lengths
of geodesic shortest paths between pairs of
sites.
Applications to Network Optimization
The approximation schemes we describe above
have been applied to numerous geometric net-
work optimization problems, including the list
below. We do not give references for most of the
results summarized below; see the surveys [2,11,
12] and Har-Peled [9].
1. A PTAS for the Euclidean Steiner minimum
spanning tree (SMST) problem.
2. A PTAS for the Euclidean minimum Steiner
forest problem, in which one is to compute
a minimum-weight forest whose connected
components (Steiner trees) span given (dis-
joint) subsets S1; : : : ; SK  S of the sites,
allowing Steiner points.
3. A PTAS for computing a minimum-weight
k-connected spanning graph of S in Eu-
clidean d-space.
4. A PTAS for the k-median problem, in which
one is to determine k centers, among S,
in order to minimize the sum of the dis-
tances from the sites S to their nearest center
points.
5. A PTAS for the minimum latency problem
(MLP), also known as the deliveryman prob-
lem or the traveling repairman problem, in
which one is to compute a tour on S that
minimizes the sum of the “latencies” of all
points, where the latency of a point p is the
length of the tour from a given starting point
to the point p. The PTAS of Sitters [15]
runs in time nO.1=/, improving the prior
QPTAS.
6. A PTAS for the k-TSP (and k-MST), in
which one is to compute a shortest tour (tree)
spanning at least k of the n sites of S.
7. A QPTAS for degree-bounded spanning trees
in the plane.
8. A QPTAS for the capacitated vehicle routing
problem (VRP) [8], in which one is to com-
pute a minimum-length collection of tours,
each visiting at most k sites of S. A PTAS is
known for some values of k.
9. A PTAS for the orienteering problem, in
which the goal is to maximize the number
of sites visited by a length-bounded tour
[6].
10. A
PTAS
for
TSP
with
Neighborhoods
(TSPN), in which each site of the input set S
is a connected region of d-space (rather than
a point), and the goal is to compute a tour that
visits each region. The TSPN has a PTAS for
regions in the plane that are “fat” and are
weakly disjoint (no point lies in more than
a constant number of regions) [13]. Chan
and Elbassioni [5] give a QPTAS for fat,
weakly disjoint regions in doubling metrics.
For TSPN with disconnected regions, the
problem is that of the “group TSP” (also
known as “generalized TSP” or “one-of-a-
set TSP”), which, in general metrics, is much
harder than TSP; even in the Euclidean plane,
the problem is NP-hard to approximate to
within any constant factor for ﬁnite point
sets and is NP-hard to approximate better
than a ﬁxed constant for visiting point
pairs.
11. A PTAS for the milling and lawnmowing
problems, in which one is to compute a
shortest path/tour for a speciﬁed cutter so
that all points of a given region R in the
plane is swept over by the cutter head
while keeping the cutter fully within the
region R (milling), or allowing the cutter
to sweep over points outside of region R
(lawnmowing).
12. A PTAS for computing a minimum-length
cycle that separates a given set of “red”
points from a given set of “blue” points in
the Euclidean plane.
13. A QPTAS for the minimum-weight trian-
gulation (MWT) problem of computing
a triangulation of the planar point set S
in order to minimize the sum of the edge
lengths. The MWT has been shown to be
NP-hard.

130
Approximation Schemes for Geometric Network Optimization Problems
14. A PTAS for the minimum-weight Steiner
convex partition problem in the plane, in
which one is to compute an embedded planar
straight-line graph with convex faces whose
vertex set contains the input set S.
Open Problems
A prominent open problem in approximation al-
gorithms for network optimization is to deter-
mine if approximations better than factor 3/2
can be achieved for the TSP in general metric
spaces.
Speciﬁc open problems for geometric network
optimization problems include:
1. Is there a PTAS for minimum-weight trian-
gulation (MWT) in the plane? (A QPTAS is
known.)
2. Is there a PTAS for capacitated vehicle rout-
ing, for all k?
3. Is there a PTAS for Euclidean minimum span-
ning trees of bounded degree (3 or 4)? (A
QPTAS is known for degree-3 trees.)
4. Is there a PTAS for TSP with Neighborhoods
(TSPN) for connected disjoint regions in the
plane?
5. Is there a PTAS for computing a minimum-
weight t-spanner of a set of points in a Eu-
clidean space?
Finally, can PTAS techniques be implemented
to be competitive with other practical methods
for
TSP
or
related
network
optimization
problems?
Cross-References
▷Applications of Geometric Spanner Networks
▷Euclidean Traveling Salesman Problem
▷Metric TSP
▷Minimum Geometric Spanning Trees
▷Minimum k-Connected Geometric Networks
▷Steiner Trees
Recommended Reading
1. Arora S (1998) Polynomial time approximation
schemes for Euclidean traveling salesman and other
geometric problems. J ACM 45(5):753–782
2. Arora S (2007) Approximation algorithms for geo-
metric TSP. In: Gutin G, Punnen AP (eds) The travel-
ing salesman problem and its variations. Combinato-
rial Optimization, vol 12. Springer, New York/Berlin,
pp 207–221
3. Bartal Y, Gottlieb LA (2013) A linear time approx-
imation scheme for Euclidean TSP. In: Proceedings
of the 54th IEEE Foundations of Computer Science
(FOCS). IEEE, Piscataway, pp 698–706
4. Bartal Y, Gottlieb LA, Krauthgamer R (2012) The
traveling salesman problem: low-dimensionality im-
plies a polynomial time approximation scheme. In:
Proceedings of the 44th ACM Symposium on Theory
of Computing. ACM, New York, pp 663–672
5. Chan TH, Elbassioni KM (2011) A QPTAS for TSP
with fat weakly disjoint neighborhoods in doubling
metrics. Discret Comput Geom 46(4):704–723
6. Chen K, Har-Peled S (2008) The Euclidean orienteer-
ing problem revisited. SIAM J Comput 38(1):385–
397
7. Cook W (2011) In pursuit of the travelling salesman:
mathematics at the limits of computation. Princeton
University Press, Princeton
8. Das A, Mathieu C (2015) A quasipolynomial time
approximation scheme for Euclidean capacitated ve-
hicle routing. Algorithmica 73(1):115–142
9. Har-Peled S (2011) Geometric approximation algo-
rithms, vol 173. American Mathematical Society,
Providence
10. Mitchell JSB (1999) Guillotine subdivisions approx-
imate polygonal subdivisions: a simple polynomial-
time approximation scheme for geometric tsp, k-mst,
and related problems. SIAM J Comput 28(4):1298–
1309
11. Mitchell JSB (2000) Geometric shortest paths and
network optimization. In: Sack JR, Urrutia J (eds)
Handbook of Computational Geometry. Elsevier Sci-
ence, North-Holland/Amsterdam, pp 633–701
12. Mitchell JSB (2004) Shortest paths and networks. In:
Goodman JE, O’Rourke J (eds) Handbook of Discrete
and Computational Geometry, 2nd edn. Chapman &
Hall/CRC, Boca Raton, chap 27, pp 607–641
13. Mitchell JSB (2007) A PTAS for TSP with neighbor-
hoods among fat regions in the plane. In: Proceedings
of the 18th ACM-SIAM Symposium on Discrete
Algorithms, New Orleans, pp 11–18
14. Rao SB, Smith WD (1998) Approximating geometri-
cal graphs via spanners and banyans. In: Proceedings
of the 30th ACM Symposium on Theory of Comput-
ing. ACM, New York, pp 540–550
15. Sitters R (2014) Polynomial time approximation
schemes for the traveling repairman and other min-
imum latency problems. In: Proceedings of the 25th
ACM-SIAM Symposium on Discrete Algorithms,
Portland, pp 604–616

Approximation Schemes for Makespan Minimization
131
A
Approximation Schemes for
Makespan Minimization
Asaf Levin
Faculty of Industrial Engineering and
Management, The Technion, Haifa, Israel
Keywords
Approximation scheme; Load balancing; Parallel
machine scheduling
Years and Authors of Summarized
Original Work
1987, 1988; Hochbaum, Shmoys
Problem Deﬁnition
Non-preemptive makespan minimization on m
uniformly related machines is deﬁned as follows.
We are given a set M D f1; 2; : : : ; mg of m
machines where each machine i has a speed si
such that si > 0. In addition we are given a set
of jobs J D f1; 2; : : : ; ng, where each job j has
a positive size pj and all jobs are available for
processing at time 0. The jobs need to be parti-
tioned into m subsets S1; : : : ; Sm, with Si being
the subset of jobs assigned to machine i, and each
such (ordered) partition is a feasible solution to
the problem. Processing job j on machine i takes
pj
si time units. For such a solution (also known
as a schedule), we let Li D .P
j 2Si pj /=si be
the completion time or load of machine i. The
work of machine i is Wi D P
j 2Si pj D Li  si,
that is, the total size of the jobs assigned to
i. The makespan of the schedule is deﬁned as
maxfL1; L2; : : : ; Lmg, and the goal is to ﬁnd a
schedule that minimizes the makespan. We also
consider the problem on identical machines, that
is, the special case of the above problem in which
si D 1 for all i (in this special case, the work
and the load of a given machine are always the
same).
Key Results
A
PTAS
(polynomial-time
approximation
scheme)
is
a
family
of
polynomial-time
algorithms such that for all  > 0, the family
has an algorithm such that for every instance of
the makespan minimization problem, it returns
a feasible solution whose makespan is at most
1 C  times the makespan of an optimal solution
to the same instance. Without loss of generality,
we can assume that  < 1
5.
The Dual Approximation Framework and
Common Preprocessing Steps
Using a guessing step of the optimal makespan,
and scaling the sizes of all jobs by the value of
the optimal makespan, we can assume that the
optimal makespan is in the interval Œ1; 1 C / and
it sufﬁces to construct a feasible solution whose
makespan is at most 1 C c for a constant c (then
scaling  before applying the algorithm will give
the claimed result). This assumption can be made
since we can ﬁnd in polynomial time two values
LB and UB such that the optimal makespan is
in the interval ŒLB; UB and UB
LB is at most some
constant (or even at most an exponential function
of the length of the binary encoding of the input),
then using a constant (or polynomial) number
of iterations, we can ﬁnd the minimum integer
power of 1 C  for which the algorithm below
will succeed to ﬁnd a schedule with makespan at
most 1 C c times the optimal makespan. This
approach is referred to as the dual approximation
method [7,8].
From now on, we assume that the optimal
makespan is in the interval Œ1; 1 C /. The next
step is to round up the size of each job to the
next integer power of 1 C  and to round down
the speed of each machine to the next integer
power of 1 C . That is, the rounded size of job
j is p0
j D .1 C /dlog1C pj e and the rounded
speed of machine i is s0
i D .1 C /blog1C si c.
Note that this rounding does not decrease the
makespan of any feasible solution and increase
the optimal makespan by a multiplicative factor
of at most .1 C /2. Thus, in the new instance
that we call the rounded instance, the makespan

132
Approximation Schemes for Makespan Minimization
of an optimal solution is in the interval Œ1; .1 C
/3/. We observe that if the original instance to
the makespan minimization problem was for the
special case of identical machines, so does the
rounded instance. The next steps differ between
the PTAS for identical machines and its general-
ization for related machines.
The Case of Identical Machines
We deﬁne a job to be small if its rounded size
is at most , and otherwise it is large. The large
jobs instance is the instance we obtain from the
rounded instance by removing all small jobs. The
ﬁrst observation is that it is sufﬁcient to design
an algorithm for ﬁnding a feasible solution to the
large jobs instance whose makespan is at most
1 C c where c  5 is some constant. This is
so, because we can apply this algorithm on the
large jobs instance and obtain a schedule of these
large jobs. Later, we add to the schedule the small
jobs one by one using the list scheduling heuristic
[5]. In the analysis, there are two cases. In the
ﬁrst one, adding the small jobs did not increase
the makespan of the resulting schedule, and in
this case our claim regarding the makespan of
the output of the algorithm clearly holds. In the
second case, the makespan increased by adding
the small jobs, and we consider the last iteration
in which such increase happened. In that last
iteration, the load of one machine was increased
by the size of the job assigned in this iteration,
that is by at most , and before this iteration
its load was smaller than
P
j p0
j
m
 .1 C /3,
where the inequality holds because the makespan
of a feasible solution cannot be smaller than the
average load of the machines. The claim now
follows using .1 C /3 C   1 C 5 as  < 1
5.
The large jobs instance has a compact
representation. There are m identical machines
and jobs of at most O.log1C
1
 / distinct sizes.
Note that each machine has at most 2
 large jobs
assigned to it (in any solution with makespan
smaller than 2), and thus there are a constant
number of different schedules of one machine
when we consider jobs of the same size as
identical jobs. A schedule of one machine in
a solution to the large jobs instance is called the
conﬁguration of the machine. Now, we can either
perform a dynamic programming algorithm that
assigns large jobs to one machine after the other
while recalling in each step the number of jobs
of each size that still need to be assigned (as
done in [7]) or use an integer program of ﬁxed
dimension [9] to solve the problem of assigning
all large jobs to conﬁgurations of machines while
having at most m machines in the solution and
allowing only conﬁgurations corresponding to
machines with load at most .1C/3 as suggested
by Shmoys (see [6]).
We refer to [1, 2], and [10] for PTASs for
other load balancing problems on identical
machines.
The Case of Related Machines
Here, we still would like to consider separately
the large jobs and the small jobs; however, a given
job j can be large for one machine and small
for another machine (it may even be too large
for other machines, that is, processing it on such
machine may take a period of time that is longer
than the makespan of an optimal solution). Thus,
for a given job j , we say that it is
huge for
machine i if pj
si > .1C/3, it is large for machine
i if  <
pj
si
 .1 C /3, and otherwise it is
small for machine i. A conﬁguration of machine
i is the number of large jobs of each rounded
size that are assigned to machine i (observe
that similarly to the case of identical machines,
the number of sizes of large jobs for a given
machine is a constant) as well as approximate
value of the total size of small jobs assigned to
machine i, that is a value i such that the total
size of small jobs assigned to machine i is in
the interval

.i  1/  1
si ; i  1
si
i
. Note that
the vector of conﬁgurations of machines deﬁnes
some information about the schedule, but it does
not give a one-to-one assignment of small jobs to
the machines.
Once again, [8] suggested to use dynamic
programming for assigning jobs to the machines
by traversing the machines from the slowest one
to the fastest one and, for each machine, decide
the number of large jobs of each size as well as
an approximate value of the total size of small

Approximation Schemes for Planar Graph Problems
133
A
jobs (for that machine) assigned to it. That is, the
dynamic programming decides the conﬁguration
of each machine. To do so, it needs to recall the
number of large jobs (with respect to the current
machine) that are still not assigned, as well as
the total size of small jobs (with respect to the
current machine) that are still not assigned (this
total size is a rounded value). At a postprocessing
step, the jobs assigned as large jobs by the solu-
tion for the dynamic programming are scheduled
accordingly, while the other jobs are assigned as
small jobs as follows.
We assign the small jobs to the machines while
traversing the machines from slowest to fastest
and assigning the small jobs from the smallest
to largest. At each time we consider the current
machine i and the preﬁx of unassigned small jobs
that are small with respect to the current machine.
Denote by i the value of this parameter accord-
ing to the solution of the dynamic programming.
Due to the successive rounding of the total size of
unassigned small jobs, we will allow assignments
of a slightly larger total size of small jobs to
machine i. So we will assign the small jobs one
by one as long as their total size is at most
.i C4/
si
. If at some point there are no further
unassigned small jobs that are small for machine
i, we move to the next machine, otherwise we
assign machine i small jobs (for machine i) of
total size of at least
.i C3/
si
. This sufﬁces to
guarantee the feasibility of the resulting solution
(i.e., all jobs are assigned) while increasing the
makespan only by a small amount.
We refer to [3] and [4] for PTASs for other
load balancing problems on related machines.
Cross-References
▷Robust Scheduling Algorithms
▷Vector Scheduling Problems
Recommended Reading
1. Alon N, Azar Y, Woeginger GJ, Yadid T (1997) Ap-
proximation schemes for scheduling. In: Proceedings
of
the
8th
symposium
on
discrete
algorithms
(SODA), New Orleans, USA pp 493–500
2. Alon N, Azar Y, Woeginger GJ, Yadid T (1998)
Approximation schemes for scheduling on parallel
machines. J Sched 1(1):55–66
3. Azar Y, Epstein L (1998) Approximation schemes
for covering and scheduling on related machines.
In: Proceedings of the 1st international workshop
on approximation algorithms for combinatorial opti-
mization (APPROX), Aalborg, Denmark pp 39–47
4. Epstein L, Sgall J (2004) Approximation schemes for
scheduling on uniformly related and identical parallel
machines. Algorithmica 39(1):43–57
5. Graham RL (1966) Bounds for certain multiprocess-
ing anomalies. Bell Syst Tech J 45(9):1563–1581
6. Hochbaum DS (1997) Various notions of approxi-
mations: Good, better, best and more. In: Hochbaum
DS (ed) Approximation algorithms, PWS Publishing
Company, Boston
7. Hochbaum DS, Shmoys DB (1987) Using dual ap-
proximation algorithms for scheduling problems: the-
oretical and practical results. J ACM 34(1):144–162
8. Hochbaum DS, Shmoys DB (1988) A polynomial
approximation scheme for scheduling on uniform
processors: using the dual approximation approach.
SIAM J Comput 17(3):539–551
9. Lenstra HW Jr (1983) Integer programming with a
ﬁxed number of variables. Math Oper Res 8(4):538–
548
10. Woeginger GJ (1997) A polynomial-time approxima-
tion scheme for maximizing the minimum machine
completion time. Oper Res Lett 20(4):149–154
Approximation Schemes for Planar
Graph Problems
Mohammad Taghi Hajiaghayi1 and
Erik D. Demaine3
1Department of Computer Science, University of
Maryland, College Park, MD, USA
2MIT Computer Science and Artiﬁcial
Intelligence Laboratory, Cambridge, MA, USA
Keywords
Approximation algorithms in planar graphs;
Baker’s approach; Lipton-Tarjan approach
Years and Authors of Summarized
Original Work
1983; Baker
1994; Baker

134
Approximation Schemes for Planar Graph Problems
Problem Deﬁnition
Many NP-hard graph problems become easier to
approximate on planar graphs and their general-
izations. (A graph is planar if it can be drawn
in the plane (or the sphere) without crossings.
For deﬁnitions of other related graph classes, see
the entry on ▷Bidimensionality (2004; Demaine,
Fomin, Hajiaghayi, Thilikos).) For example, a
maximum independent set asks to ﬁnd a maxi-
mum subset of vertices in a graph that induce
no edges. This problem is inapproximable in
general graphs within a factor of n1 for any
 > 0 unless NP D ZPP (and inapproximable
within n1=2 unless P D NP), while for planar
graphs, there is a 4-approximation (or simple 5-
approximation) by taking the largest color class
in a vertex 4-coloring (or 5-coloring). Another
is minimum dominating set, where the goal is
to ﬁnd a minimum subset of vertices such that
every vertex is either in or adjacent to the subset.
This problem is inapproximable in general graphs
within  log n for some  > 0 unless P D NP,
but as we will see, for planar graphs, the problem
admits a polynomial-time approximation scheme
(PTAS): a collection of .1 C /-approximation
algorithms for all  > 0.
There are two main general approaches to
designing PTASs for problems on planar graphs
and their generalizations: the separator approach
and the Baker approach.
Lipton and Tarjan [15, 16] introduced the ﬁrst
approach, which is based on planar separators.
The ﬁrst step in this approach is to ﬁnd a separator
of O.pn/ vertices or edges, where n is the size
of the graph, whose removal splits the graph into
two or more pieces each of which is a constant
fraction smaller than the original graph. Then,
recurse in each piece, building a recursion tree of
separators, and stop when the pieces have some
constant size such as 1=. The problem can be
solved on these pieces by brute force, and then it
remains to combine the solutions up the recursion
tree. The induced error can often be bounded in
terms of the total size of all separators, which
in turn can be bounded by  n. If the optimal
solution is at least some constant factor times n,
this approach often leads to a PTAS.
There are two limitations to this planar-
separator approach. First, it requires that the
optimal solution be at least some constant factor
times n; otherwise, the cost incurred by the
separators can be far larger than the desired
optimal solution. Such a bound is possible in
some problems after some graph pruning (linear
kernelization), e.g., independent set, vertex cover,
and forms of the traveling salesman problem.
But, for example, Grohe [12] states that the
dominating set is a problem “to which the
technique based on the separator theorem does
not apply.” Second, the approximation algorithms
resulting
from
planar
separators
are
often
impractical because of large constant factors. For
example, to achieve an approximation ratio of
just 2, the base case requires exhaustive solution
of graphs of up to 22;400 vertices.
Baker [1] introduced her approach to address
the second limitation, but it also addresses the
ﬁrst limitation to a certain extent. This approach
is based on decomposition into overlapping sub-
graphs of bounded outerplanarity, as described in
the next section.
Key Results
Baker’s original result [1] is a PTAS for a
maximum independent set (as deﬁned above)
on planar graphs, as well as the following list
of problems on planar graphs: maximum tile
salvage, partition into triangles, maximum H-
matching, minimum vertex cover, minimum
dominating set, and minimum edge-dominating
set.
Baker’s approach starts with a planar embed-
ding of the planar graph. Then it divides vertices
into layers by iteratively removing vertices on
the outer face of the graph: layer j consists of
the vertices removed at the j th iteration. If one
now removes the layers congruent to i modulo
k, for any choice of i, the graph separates into
connected components each with at most k con-
secutive layers, and hence the graph becomes k-
outerplanar. Many NP-complete problems can be
solved on k-outerplanar graphs for ﬁxed k using
dynamic programming (in particular, such graphs

Approximation Schemes for Planar Graph Problems
135
A
have bounded treewidth). Baker’s approximation
algorithm computes these optimal solutions for
each choice i of the congruence class of layers
to remove and returns the best solution among
these k solutions. The key argument for maxi-
mization problems considers the optimal solution
to the full graph and argues that the removal of
one of the k congruence classes of layers must
remove at most a 1=k fraction of the optimal
solution, so the returned solution must be within
a 1 C 1=k factor of optimal. A more delicate
argument handles minimization problems as well.
For many problems, such as maximum indepen-
dent set, minimum dominating set, and minimum
vertex cover, Baker’s approach obtains a (1 C )-
approximation algorithms with a running time of
2O.1=/nO.1/ on planar graphs.
Eppstein [10] generalized Baker’s approach
to a broader class of graphs called graphs of
bounded local treewidth, i.e., where the treewidth
of the subgraph induced by the set of vertices at a
distance of at most r from any vertex is bounded
above by some function f .r/ independent of n.
The main differences in Eppstein’s approach are
replacing the concept of bounded outerplanarity
with the concept of bounded treewidth, where
dynamic programming can still solve many prob-
lems, and labeling layers according to a sim-
ple breadth-ﬁrst search. This approach has led
to PTASs for hereditary maximization problems
such as maximum independent set and maximum
clique, maximum triangle matching, maximum
H-matching, maximum tile salvage, minimum
vertex cover, minimum dominating set, mini-
mum edge-dominating set, minimum color sum,
and subgraph isomorphism for a ﬁxed pattern
[6,8,10]. Frick and Grohe [11] also developed a
general framework for deciding any property ex-
pressible in ﬁrst-order logic in graphs of bounded
local treewidth.
The foundation of these results is Eppstein’s
characterization of minor-closed families of
graphs
with
bounded
local
treewidth
[10].
Speciﬁcally, he showed that a minor-closed
family has bounded local treewidth if and only
if it excludes some apex graph, a graph with
a vertex whose removal leaves a planar graph.
Unfortunately, the initial proof of this result
brought Eppstein’s approach back to the realm
of impracticality, because his bound on local
treewidth in a general apex-minor-free graph is
doubly exponential in r W 22O.r/. Fortunately,
this bound could be improved to 2O.r/ [3] and
even the optimal O.r/ [4]. The latter bound
restores Baker’s 2O.1=/nO.1/ running time for
(1 C )-approximation algorithms, now for all
apex-minor-free graphs.
Another way to view the necessary decom-
position of Baker’s and Eppstein’s approaches
is that the vertices or edges of the graph can
be split into any number k of pieces such that
deleting any one of the pieces results in a graph of
bounded treewidth (where the bound depends on
k). Such decompositions in fact exist for arbitrary
graphs excluding any ﬁxed minor H [9], and
they can be found in polynomial time [6]. This
approach generalizes the Baker-Eppstein PTASs
described above to handle general H-minor-free
graphs.
This decomposition approach is effectively
limited
to
deletion-closed
problems,
whose
optimal solution only improves when deleting
edges or vertices from the graph. Another
decomposition
approach
targets
contraction-
closed
problems,
whose
optimal
solution
only improves when contracting edges. These
problems include classic problems such as
dominating set and its variations, the traveling
salesman problem, subset TSP, minimum Steiner
tree, and minimum-weight c-edge-connected
submultigraph. PTASs have been obtained for
these problems in planar graphs [2, 13, 14] and
in bounded-genus graphs [7] by showing that the
edges can be decomposed into any number k of
pieces such that contracting any one piece results
in a bounded-treewidth graph (where the bound
depends on k).
Applications
Most applications of Baker’s approach have been
limited to optimization problems arising from
“local” properties (such as those deﬁnable in ﬁrst-
order logic). Intuitively, such local properties can
be decided by locally checking every constant-

136
Approximation Schemes for Planar Graph Problems
size neighborhood. In [5], Baker’s approach
is generalized to obtain PTASs for nonlocal
problems, in particular, connected dominating
set. This generalization requires the use of
two different techniques. The ﬁrst technique
is to use an "-fraction of a constant-factor (or
even logarithmic-factor) approximation to the
problem as a “backbone” for achieving the
needed nonlocal property. The second technique
is to use subproblems that overlap by ‚.log n/
layers instead of the usual ‚ (1) in Baker’s
approach.
Despite this advance in applying Baker’s
approach to more general problems, the planar-
separator approach can still handle some different
problems.
Recall,
though,
that
the
planar-
separator approach was limited to problems
in which the optimal solution is at least some
constant factor times n. This limitation has
been overcome for a wide range of problems
[5], in particular obtaining a PTAS for feedback
vertex set, to which neither Baker’s approach nor
the planar-separator approach could previously
apply. This result is based on evenly dividing the
optimum solution instead of the whole graph,
using a relation between treewidth and the
optimal solution value to bound the treewidth
of the graph, thus obtaining an O.
p
OPT/
separator instead of an O.pn/ separator. The
O.
p
OPT/ bound on treewidth follows from
the bidimensionality theory described in the
entry on ▷Bidimensionality (2004; Demaine,
Fomin, Hajiaghayi, Thilikos). We can divide
the optimum solution into roughly even pieces,
without knowing the optimum solution, by using
existing constant-factor (or even logarithmic-
factor) approximations for the problem. At the
base of the recursion, pieces no longer have
bounded size but do have bounded treewidth, so
fast ﬁxed-parameter algorithms can be used to
construct optimal solutions.
Open Problems
An intriguing direction for future research is
to build a general theory for PTASs of subset
problems. Although PTASs for subset TSP and
Steiner tree have recently been obtained for
planar graphs [2, 14], there remain several open
problems of this kind, such as subset feedback
vertex set, group Steiner tree, and directed Steiner
tree.
Another instructive problem is to understand
the extent to which Baker’s approach can be
applied to nonlocal problems. Again there is an
example of how to modify the approach to handle
the nonlocal problem of connected dominating
set [5], but, for example, the only known PTAS
for feedback vertex set in planar graphs follows
the separator approach.
Cross-References
▷Bidimensionality
▷Separators in Graphs
▷Treewidth of Graphs
Recommended Reading
1. Baker BS (1994) Approximation algorithms for NP-
complete problems on planar graphs. J Assoc Comput
Mach 41(1):153–180
2. Borradaile G, Kenyon-Mathieu C, Klein PN (2007)
A polynomial-time approximation scheme for Steiner
tree in planar graphs. In: Proceedings of the 18th an-
nual ACM-SIAM symposium on discrete algorithms
(SODA’07), New Orleans
3. Demaine ED, Hajiaghayi M (2004) Diameter and
treewidth in minor-closed graph families, revisited.
Algorithmica 40(3):211–215
4. Demaine ED, Hajiaghayi M (2004) Equivalence
of local treewidth and linear local treewidth and
its algorithmic applications. In: Proceedings of the
15th ACM-SIAM symposium on discrete algorithms
(SODA’04), New Orleans, pp 833–842
5. Demaine ED, Hajiaghayi M (2005) Bidimension-
ality: new connections between FPT algorithms
and PTASs. In: Proceedings of the 16th annual
ACM-SIAM symposium on discrete algorithms
(SODA’05), Vancouver, pp 590–601
6. Demaine ED, Hajiaghayi M, Kawarabayashi K-I
(2005) Algorithmic graph minor theory: decomposi-
tion, approximation, and coloring. In: Proceedings of
the 46th annual IEEE symposium on foundations of
computer science, Pittsburgh, pp 637–646
7. Demaine ED, Hajiaghayi M, Mohar B (2007) Ap-
proximation algorithms via contraction decomposi-
tion. In: Proceedings of the 18th annual ACM-SIAM
symposium on discrete algorithms (SODA’07), New
Orleans, 7–9 Jan 2007, pp 278–287

Approximations of Bimatrix Nash Equilibria
137
A
8. Demaine ED, Hajiaghayi M, Nishimura N, Ragde
P, Thilikos DM (2004) Approximation algorithms
for classes of graphs excluding single-crossing
graphs as minors. J Comput Syst Sci 69(2):166–
195
9. DeVos M, Ding G, Oporowski B, Sanders DP, Reed
B, Seymour P, Vertigan D (2004) Excluding any
graph as a minor allows a low tree-width 2coloring.
J Comb Theory Ser B 91(1):25–41
10. Eppstein D (2000) Diameter and treewidth in minor-
closed graph families. Algorithmica 27(3–4):275–
291
11. Frick M, Grohe M (2001) Deciding ﬁrst-order prop-
erties of locally tree-decomposable structures. J ACM
48(6):1184–1206
12. Grohe M (2003) Local tree-width, excluded mi-
nors, and approximation algorithms. Combinatorica
23(4):613–632
13. Klein PN (2005) A linear-time approximation scheme
for TSP for planar weighted graphs. In: Proceedings
of the 46th IEEE symposium on foundations of com-
puter science, Pittsburgh, pp 146–155
14. Klein PN (2006) A subset spanner for planar graphs,
with application to subset TSP. In: Proceedings of
the 38th ACM symposium on theory of computing,
Seattle, pp 749–756
15. Lipton RJ, Tarjan RE (1979) A separator theorem for
planar graphs. SIAM J Appl Math 36(2):177–189
16. Lipton RJ, Tarjan RE (1980) Applications of a planar
separator theorem. SIAM J Comput 9(3):615–627
Approximations of Bimatrix Nash
Equilibria
Paul (Pavlos) Spirakis
Computer Engineering and Informatics,
Research and Academic Computer Technology
Institute, Patras University, Patras, Greece
Computer Science, University of Liverpool,
Liverpool, UK
Computer Technology Institute (CTI), Patras,
Greece
Keywords
-Nash equilibria; -Well-supported Nash equi-
libria
Years and Authors of Summarized
Original Work
2003; Lipton, Markakis, Mehta
2006; Daskalaskis, Mehta, Papadimitriou
2006; Kontogiannis, Panagopoulou, Spirakis
Problem Deﬁnition
Nash [15] introduced the concept of Nash
equilibria in noncooperative games and proved
that any game possesses at least one such
equilibrium.
A
well-known
algorithm
for
computing a Nash equilibrium of a 2-player
game is the Lemke-Howson algorithm [13];
however, it has exponential worst-case running
time in the number of available pure strategies
[18].
Daskalakis et al. [5] showed that the problem
of computing a Nash equilibrium in a game with
4 or more players is PPAD-complete; this result
was later extended to games with 3 players [8].
Eventually, Chen and Deng [3] proved that the
problem is PPAD-complete for 2-player games as
well.
This fact emerged the computation of approx-
imate Nash equilibria. There are several versions
of approximate Nash equilibria that have been
deﬁned in the literature; however, the focus of
this entry is on the notions of -Nash equilibrium
and -well-supported Nash equilibrium. An -
Nash equilibrium is a strategy proﬁle such that
no deviating player could achieve a payoff higher
than the one that the speciﬁc proﬁle gives her,
plus . A stronger notion of approximate Nash
equilibria is the -well-supported Nash equilib-
ria; these are strategy proﬁles such that each
player plays only approximately best-response
pure strategies with nonzero probability. These
are additive notions of approximate equilibria;
the problem of computing approximate equilibria
of bimatrix games using a relative notion of
approximation is known to be PPAD-hard even
for constant approximations.
Notation
For a n  1 vector x, denote by x1; : : : ; xn the
components of x and by xT the transpose of x.
Denote by ei the column vector with 1 at the
ith coordinate and 0 elsewhere. For an n  m
matrix A, denote aij the element in the i-th row

138
Approximations of Bimatrix Nash Equilibria
and j -th column of A. Let Pn be the set of
all probability vectors in n dimensions: Pn D

´ 2 Rn
0 W
nP
iD1
´i D 1

.
Bimatrix Games
Bimatrix games [16] are a special case of 2-
player games such that the payoff functions can
be described by two real n  m matrices A and
B. The n rows of A; B represent the action set
of the ﬁrst player (the row player), and the m
columns represent the action set of the second
player (the column player). Then, when the row
player chooses action i and the column player
chooses action j , the former gets payoff aij ,
while the latter gets payoff bij . Based on this,
bimatrix games are denoted by  D hA; Bi.
A strategy for a player is any probability
distribution on his/her set of actions. Therefore,
a strategy for the row player can be expressed
as a probability vector x 2 Pn, while a strategy
for the column player can be expressed as a
probability vector y 2 Pm. Each extreme point
ei
2 Pn.ej
2 Pm/ that corresponds to the
strategy assigning probability 1 to the i-th row
(j -th column) is called a pure strategy for the
row (column) player. A strategy proﬁle .x; y/ is
a combination of (mixed in general) strategies,
one for each player. In a given strategy proﬁle
.x; y/, the players get expected payoffs xT Ay
(row player) and xT By (column player).
If both payoff matrices belong to Œ0; 1mn,
then the game is called a [0,1]-bimatrix (or else,
positively normalized) game. The special case of
bimatrix games in which all elements of the ma-
trices belong to f0; 1g is called a f0; 1g-bimatrix
(or else, win-lose) game. A bimatrix game hA; Bi
is called zero sum if B D A.
Approximate Nash Equilibria
Deﬁnition 1 (-Nash equilibrium) For any  >
0, a strategy proﬁle .x; y/ is an -Nash equilib-
rium for the n  m bimatrix game  D hA; Bi
if
1. For all pure strategies i 2 f1; : : : ; ng of the
row player, eT
i Ay  xT Ay C .
2. For all pure strategies j 2 f1; : : : ; mg of the
column player, xT Bej  xT By C .
Deﬁnition 2 (-well-supported Nash equilib-
rium) For any  > 0, a strategy proﬁle .x; y/
is an -well-supported Nash equilibrium for the
n  m bimatrix game  D hA; Bi if
1. For all pure strategies i 2 f1; : : : ; ng of the
row player,
xi > 0 ) eT
i Ay  eT
k Ay 8k 2 f1; : : : ; ng
2. For all pure strategies j 2 f1; : : : ; mg of the
column player,
yi > 0 ) xT Bej  xT Bek
  8k 2 f1; : : : ; mg:
Note that both notions of approximate equilibria
are deﬁned with respect to an additive error term
. Although (exact) Nash equilibria are known
not to be affected by any positive scaling, it is
important to mention that approximate notions of
Nash equilibria are indeed affected. Therefore,
the commonly used assumption in the literature
when referring to approximate Nash equilibria is
that the bimatrix game is positively normalized,
and this assumption is adopted in the present
entry.
Key Results
The work of Althöfer [1] shows that, for any
probability vector p, there exists a probability
vector OP with logarithmic supports, so that for a
ﬁxed matrix C, max
j
jpT Cej  OpT Cej j  , for
any constant  > 0. Exploiting this fact, the work
of Lipton, Markakis, and Mehta [14] shows that,
for any bimatrix game and for any constant  >
0, there exists an -Nash equilibrium with only
logarithmic support (in the number n of available
pure strategies). Consider a bimatrix game  D
hA; Bi, and let .x; y/ be a Nash equilibrium for
. Fix a positive integer k and form a multiset S1
by sampling k times from the set of pure strate-

Approximations of Bimatrix Nash Equilibria
139
A
gies of the row player, independently at random
according to the distribution x. Similarly, form a
multiset S2 by sampling k times from set of pure
strategies of the column player according to y.
Let Ox be the mixed strategy for the row player that
assigns probability 1=k to each member of S1
and 0 to all other pure strategies, and let Oy be the
mixed strategy for the column player that assigns
probability 1=k to each member of S2 and 0 to
all other pure strategies. Then, Ox and Oy are called
k-uniform [14], and the following holds:
Theorem 1 ([14]) For any Nash equilibrium
.x; y/ of a n  n bimatrix game and for every
 > 0, there exists, for every k  .12 ln n/=2, a
pair of k-uniform strategies Ox; Oy such that . Ox; Oy/
is an -Nash equilibrium.
This result directly yields a quasi-polynomial
.nO.ln n// algorithm for computing such an
approximate equilibrium. Moreover, as pointed
out in [1], no algorithm that examines supports
smaller
than
about
ln n
can
achieve
an
approximation better than 1=4.
Theorem 2 ([4]) The problem of computing a
1=n‚.1/-Nash equilibrium of a n  n bimatrix
game is PPAD-complete.
Theorem 2 asserts that, unless PPAD  P ,
there exists no fully polynomial time approxima-
tion scheme for computing equilibria in bimatrix
games. However, this does not rule out the ex-
istence of a polynomial approximation scheme
for computing an -Nash equilibrium when  is
an absolute constant, or even when  D ‚.1 
=poly.ln n//. Furthermore, as observed in [4], if
the problem of ﬁnding an -Nash equilibrium
were PPAD-complete when  is an absolute con-
stant, then, due to Theorem 1, all PPAD problems
would be solved in quasi-polynomial time, which
is unlikely to be the case.
Two concurrent and independent works [6,
11] were the ﬁrst to make progress in provid-
ing -Nash equilibria and -well-supported Nash
equilibria for bimatrix games and some constant
0 <  < 1. In particular, the work of Kontogian-
nis, Panagopoulou, and Spirakis [11] proposes
a simple linear-time algorithm for computing a
3=4-Nash equilibrium for any bimatrix game:
Theorem 3 ([11]) Consider any n  m bimatrix
game  D hA; Bi, and let ai1;j1 D maxi;j aij
and bi2;j 2 D maxi;j bij . Then the pair of strate-
gies . Ox; Oy/ where Oxi1 D Oxi2 D Oyj1 D Oyj2 D 1=2
is a 3=4-Nash equilibrium for .
The above technique can be extended so as to
obtain a parametrized, stronger approximation:
Theorem 4 ([11]) Consider a n  m bimatrix
game  D hA; Bi. Let 1 .2 / be the mini-
mum, among all Nash equilibria of , expected
payoff for the row (column) player, and let  D
maxf1 ; 2 g. Then, there exists a .2 C /=4-
Nash equilibrium that can be computed in time
polynomial in n and m.
The work of Daskalakis, Mehta, and Papadim-
itriou [6] provides a simple algorithm for com-
puting a 1=2-Nash equilibrium: Pick an arbitrary
row for the row player, say row i. Let j
D
arg max 0
j b0
ij . Let k D arg max 0
ka0
kj . Thus, j is
a best-response column for the column player to
the row i, and k is a best-response row for the row
player to the column j . Let Ox D 1=2ei C 1=2ek
and Oy D ej , i.e., the row player plays row i
or row k with probability 1=2 each, while the
column player plays column j with probability
1. Then:
Theorem 5 ([6]) The strategy proﬁle . Ox; Oy/ is a
1=2-Nash equilibrium.
A polynomial construction (based on linear pro-
gramming) of a 0.38-Nash equilibrium is pre-
sented in [7].
For the more demanding notion of well-
supported
approximate
Nash
equilibrium,
Daskalakis,
Mehta,
and
Papadimitriou
[6]
propose an algorithm, which, under a quite
interesting
and
plausible
graph
theoretic
conjecture, constructs in polynomial time a 5=6-
well-supported Nash equilibrium. However, the
status of this conjecture is still unknown. In [6],
it is also shown how to transform a [0,1]-bimatrix
game to a f0; 1g-bimatrix game of the same size,
so that each -well-supported Nash equilibrium
of the resulting game is .1C/=2-well-supported
Nash equilibrium of the original game.

140
Approximations of Bimatrix Nash Equilibria
An algorithm given by Kontogiannis and
Spirakis computes a 2=3-well-supported Nash
equilibrium in polynomial time [12]. Their
methodology for attacking the problem is based
on the solvability of zero-sum bimatrix games
(via its connection to linear programming) and
provides a 0.5-well-supported Nash equilibrium
for win-lose games and a 2=3-well-supported
Nash equilibrium for normalized games. In [9], a
polynomial-time algorithm computes an "-well-
supported Nash equilibrium with " < 2=3, by
extending the 2=3-algorithm of Kontogiannis
and Spirakis. In particular, it is shown that either
the strategies generated by their algorithm can
be tweaked to improve the approximation or
that we can ﬁnd a sub-game that resembles
matching pennies, which again leads to a better
approximation. This allows to construct a (2/3-
0.004735)-well-supported Nash equilibrium in
polynomial time.
Two new results improved the approximation
status of -Nash equilibria:
Theorem 6 ([2]) There is a polynomial time al-
gorithm, based on linear programming, that pro-
vides an 0.36392-Nash equilibrium.
The second result below, due to Tsaknakis
and Spirakis, is the best till now. Based on local
search, it establishes that any local minimum of
a very natural map in the space of pairs of mixed
strategies or its dual point in a certain minimax
problem used for ﬁnding the local minimum
constitutes a 0.3393-Nash equilibrium.
Theorem 7 ([19]) There exists a polynomial
time algorithm, based on the stationary points of
a natural optimization problem, that provides an
0.3393-Nash Equilibrium.
In [20], it is shown that the problem of com-
puting a Nash equilibrium for 2-person games
can be polynomially reduced to an indeﬁnite
quadratic programming problem involving the
spectrum of the adjacency matrix of a strongly
connected directed graph on n vertices, where n
is the total number of players’ strategies. Based
on that, a new method is presented for com-
puting approximate equilibria, and it is shown
that its complexity is a function of the average
spectral energy of the underlying graph. The
implications of the strong connectedness prop-
erties on the energy and on the complexity of
the method are discussed, and certain classes of
graphs are described for which the method is a
polynomial time approximation scheme (PTAS).
The worst- case complexity is bounded by a
subexponential function in the total number of
strategies n.
Kannan and Theobald [10] investigate a
hierarchy of bimatrix games hA; Bi which
results from restricting the rank of the matrix
A C B to be of ﬁxed rank at most k. They
propose
a
new
model
of
-approximation
for games of rank k and, using results from
quadratic optimization, show that approximate
Nash
equilibria
of
constant
rank
games
can
be
computed
deterministically
in
time
polynomial in 1=. Moreover, [10] provides a
randomized approximation algorithm for certain
quadratic optimization problems, which yields
a randomized approximation algorithm for the
Nash equilibrium problem. This randomized
algorithm has similar time complexity as the
deterministic one, but it has the possibility
of ﬁnding an exact solution in polynomial
time if a conjecture is valid. Finally, they
present a polynomial time algorithm for relative
approximation (with respect to the payoffs in an
equilibrium) provided that the matrix A C B has
a nonnegative decomposition.
Applications
Noncooperative
game
theory
and
its
main
solution concept, i.e., the Nash equilibrium,
have been extensively used to understand the
phenomena
observed
when
decision-makers
interact and have been applied in many diverse
academic ﬁelds, such as biology, economics,
sociology,
and
artiﬁcial
intelligence.
Since
however the computation of a Nash equilibrium
is in general PPAD-complete, it is important to
provide efﬁcient algorithms for approximating
a Nash equilibrium; the algorithms discussed
in this entry are a ﬁrst step towards this
direction.

Arbitrage in Frictional Foreign Exchange Market
141
A
Recommended Reading
1. Althöfer I (1994) On sparse approximations to ran-
domized strategies and convex combinations. Linear
Algebr Appl 199:339–355
2. Bosse H, Byrka J, Markakis E (2007) New algorithms
for approximate Nash equilibria in bimatrix games.
In: Proceedings of the 3rd international workshop on
internet and network economics (WINE 2007), San
Diego, 12–14 Dec 2007. Lecture notes in computer
science
3. Chen X, Deng X (2005) Settling the complexity of 2-
player Nash-equilibrium. In: Proceedings of the 47th
annual IEEE symposium on foundations of computer
science (FOCS’06), Berkeley, 21–24 Oct 2005
4. Chen X, Deng X, Teng S-H (2006) Computing Nash
equilibria: approximation and smoothed complexity.
In: Proceedings of the 47th annual IEEE sympo-
sium on foundations of computer science (FOCS’06),
Berkeley, 21–24 Oct 2006
5. Daskalakis C, Goldberg P, Papadimitriou C (2006)
The complexity of computing a Nash equilibrium. In:
Proceedings of the 38th annual ACM symposium on
theory of computing (STOC’06), Seattle, 21–23 May
2006, pp 71–78
6. Daskalakis C, Mehta A, Papadimitriou C (2006) A
note on approximate Nash equilibria. In: Proceedings
of the 2nd workshop on internet and network eco-
nomics (WINE’06), Patras, 15–17 Dec 2006, pp 297–
306
7. Daskalakis C, Mehta A, Papadimitriou C (2007)
Progress in approximate Nash equilibrium. In: Pro-
ceedings of the 8th ACM conference on electronic
commerce (EC07), San Diego, 11–15 June 2007
8. Daskalakis C, Papadimitriou C (2005) Three-player
games are hard. In: Electronic colloquium on compu-
tational complexity (ECCC TR 05-139)
9. Fearnley J, Goldberg PW, Savani R, Bjerre Sørensen
T (2012) Approximate well-supported Nash equi-
libria below two-thirds. In: SAGT 2012, Barcelona,
pp 108–119
10. Kannan R, Theobald T (2007) Games of ﬁxed rank:
a hierarchy of bimatrix games. In: Proceedings of the
ACM-SIAM symposium on discrete algorithms, New
Orleans, 7–9 Jan 2007
11. Kontogiannis S, Panagopoulou PN, Spirakis PG
(2006) Polynomial algorithms for approximating
Nash equilibria of bimatrix games. In: Proceedings of
the 2nd workshop on internet and network economics
(WINE’06), Patras, 15–17 Dec 2006, pp 286–296
12. Kontogiannis S, Spirakis PG (2010) Well supported
approximate equilibria in bimatrix games. Algorith-
mica 57(4):653–667
13. Lemke CE, Howson JT (1964) Equilibrium points of
bimatrix games. J Soc Indust Appl Math 12:413–423
14. Lipton RJ, Markakis E, Mehta A (2003) Playing
large games using simple startegies. In: Proceedings
of the 4th ACM conference on electronic commerce
(EC’03), San Diego, 9–13 June 2003, pp 36–41
15. Nash J (1951) Noncooperative games. Ann Math
54:289–295
16. von Neumann J, Morgenstern O (1944) Theory of
games and economic behavior. Princeton University
Press, Princeton
17. Papadimitriou CH (1991) On inefﬁcient proofs of ex-
istence and complexity classes. In: Proceedings of the
4th Czechoslovakian symposium on combinatorics
1990, Prachatice
18. Savani R, von Stengel B (2004) Exponentially many
steps for ﬁnding a Nash equilibrium in a bimatrix
game. In: Proceedings of the 45th annual IEEE
symposium on foundations of computer science
(FOCS’04), Rome, 17–19 Oct 2004, pp 258–267
19. Tsaknakis H, Spirakis P (2007) An optimization ap-
proach for approximate Nash equilibria. In: Proceed-
ings of the 3rd international workshop on internet and
network economics (WINE 2007). Lecture notes in
computer science. Also in J Internet Math 5(4):365–
382 (2008)
20. Tsaknakis H, Spirakis PG (2010) Practical and ef-
ﬁcient approximations of Nash equilibria for win-
lose games based on graph spectra. In: WINE 2010,
Stanford, pp 378–390
Arbitrage in Frictional Foreign
Exchange Market
Mao-cheng Cai1 and Xiaotie Deng2;3
1Chinese Academy of Sciences, Institute of
Systems Science, Beijing, China
2AIMS Laboratory (Algorithms-Agents-Data on
Internet, Market, and Social Networks),
Department of Computer Science and
Engineering, Shanghai Jiao Tong University,
Shanghai, China
3Department of Computer Science, City
University of Hong Kong, Hong Kong, China
Keywords
Arbitrage;
Complexity;
Foreign
exchange;
Market
Years and Authors of Summarized
Original Work
2003; Cai, Deng

142
Arbitrage in Frictional Foreign Exchange Market
Problem Deﬁnition
The simultaneous purchase and sale of the same
securities, commodities, or foreign exchange in
order to proﬁt from a differential in the price.
This usually takes place on different exchanges
or marketplaces and is also known as a “riskless
proﬁt.”
Arbitrage is, arguably, the most fundamental
concept in ﬁnance. It is a state of the variables
of ﬁnancial instruments such that a riskless proﬁt
can be made, which is generally believed not
in existence. The economist’s argument for its
nonexistence is that active investment agents will
exploit any arbitrage opportunity in a ﬁnancial
market and thus will deplete it as soon as it
may arise. Naturally, the speed at which such
an arbitrage opportunity can be located and be
taken advantage of is important for the proﬁt-
seeking investigators, which falls in the realm
of analysis of algorithms and computational
complexity.
The identiﬁcation of arbitrage states is, at fric-
tionless foreign exchange market (a theoretical
trading environment where all costs and restraints
associated with transactions are nonexistent), not
difﬁcult at all and can be reduced to existence
of arbitrage on three currencies (see [11]). In
reality, friction does exist. Because of friction,
it is possible that there exist arbitrage opportu-
nities in the market but difﬁcult to ﬁnd it and to
exploit it to eliminate it. Experimental results in
foreign exchange markets showed that arbitrage
does exist in reality. Examination of data from 10
markets over a 12-day period by Mavrides [11]
revealed that a signiﬁcant arbitrage opportunity
exists. Some opportunities were observed to be
persistent for a long time. The problem becomes
worse at forward and future markets (in which
future contracts in commodities are traded) cou-
pled with covered interest rates, as observed by
Abeysekera and Turtle [1] and Clinton [4]. An
obvious interpretation is that the arbitrage oppor-
tunity was not immediately identiﬁed because of
information asymmetry in the market. However,
that is not the only factor. Both the time necessary
to collect the market information (so that an
arbitrage opportunity would be identiﬁed) and the
time people (or computer programs) need to ﬁnd
the arbitrage transactions are important factors
for eliminating arbitrage opportunities.
The computational complexity in identifying
arbitrage, the level in difﬁculty measured by
arithmetic operations, is different in different
models of exchange systems. Therefore, to
approximate an ideal exchange market, models
with lower complexities should be preferred to
those with higher complexities.
To model an exchange system, consider n
foreign currencies: N D f1; 2; : : : ; ng. For each
ordered pair .i; j /, one may change one unit of
currency i to rij units of currency j . Rate rij
is the exchange rate from i to j . In an ideal
market, the exchange rate holds for any amount
that is exchanged. An arbitrage opportunity is a
set of exchanges between pairs of currencies such
that the net balance for each involved currency
is nonnegative and there is at least one currency
for which the net balance is positive. Under ideal
market conditions, there is no arbitrage if and
only if there is no arbitrage among any three
currencies (see [11]).
Various types of friction can be easily mod-
eled in such a system. Bid-offer spread may
be expressed in the present mathematical for-
mat as rij rji
< 1 for some i; j
2 N . In
addition, usually the traded amount is required
to be in multiples of a ﬁxed integer amount,
hundreds, thousands, or millions. Moreover, dif-
ferent traders may bid or offer at different rates,
and each for a limited amount. A more general
model to describe these market imperfections will
include, for pairs i ¤ j 2 N , lij different rates
rk
ij of exchanges from currency i to j up to bk
ij
units of currency i, k D 1; : : : ; lij , where lij
is the number of different exchange rates from
currency i to j .
A currency exchange market can be repre-
sented by a digraph G D .V; E/ with vertex set
V and arc set E such that each vertex i 2 V
represents currency i and each arc ak
ij
2 E
represents the currency exchange relation from i
to j with rate rk
ij and bound bk
ij . Note that parallel
arcs may occur for different exchange rates. Such
a digraph is called an exchange digraph. Let x D
.xk
ij / denote a currency exchange vector (Fig. 1).

Arbitrage in Frictional Foreign Exchange Market
143
A
Arbitrage in Frictional Foreign Exchange Market,
Fig. 1 Digraph G1
Problem 1 The existence of arbitrage in a fric-
tional exchange market can be formulated as
follows:
X
j ¤i
lji
X
kD1
brk
jixk
jic
X
j ¤i
lij
X
kD1
xk
ij  0; i D 1; : : : ; n;
(1)
at least one strict inequality holds
0  xk
ij  bk
ij ; 1  k  lij ; 1  i ¤ j  n;
(2)
xk
ij is integer; 1  k  lij ; 1  i ¤ j  n:
(3)
Note that the ﬁrst term in the right-hand side
of (1) is the revenue at currency i by selling other
currencies and the second term is the expense at
currency i by buying other currencies.
The corresponding optimization problem is
Problem 2 The maximum arbitrage problem in
a frictional foreign exchange market with bid-ask
spreads, bound, and integrality constraints is the
following integer linear programming .P /:
maximize
n
X
iD1
wi
X
j ¤i
0
@
lji
X
kD1
brk
jixk
jic 
lij
X
kD1
xk
ij
1
A
subject to
X
j ¤i
0
@
lji
X
kD1
brk
jixk
jic 
lij
X
kD1
xk
ij
1
A 0; i D1; : : : ; n;
(4)
0  xk
ij  bk
ij ; 1  k  lij ; 1  i ¤ j  n;
(5)
xk
ij is integer; 1  k  lij ; 1  i ¤ j  n;
(6)
where wi  0 is a given weight for currency
i; i D 1; 2; : : : ; n, with at least one wi > 0.
Finally, consider another
Problem 3 In order to eliminate arbitrage, how
many transactions and arcs in a exchange digraph
have to be used for the currency exchange sys-
tem?
Key Results
A decision problem is called nondeterministic
polynomial (NP for short) if its solution (if one
exists) can be guessed and veriﬁed in polynomial
time; nondeterministic means that no particular
rule is followed to make the guess. If a problem
is NP and all other NP problems are polynomial-
time reducible to it, the problem is NP-complete.
And a problem is called NP-hard if every other
problem in NP is polynomial-time reducible to it
(Fig. 2).
Theorem 1 It is NP-complete to determine
whether there exists arbitrage in a frictional
foreign exchange market with bid-ask spreads,
bound, and integrality constraints even if all
lij D 1.
Then, a further inapproximability result is ob-
tained.
Theorem 2 There exists ﬁxed  > 0 such that
approximating .P / within a factor of n is NP-
hard even for any of the following two special
cases:
.P1/ all lij D 1 and wi D 1.
.P2/ all lij D 1 and all but one wi D 0.

144
Arbitrage in Frictional Foreign Exchange Market
Arbitrage in Frictional Foreign Exchange Market, Fig. 2 Digraph G2
Now, consider two polynomially solvable spe-
cial cases when the number of currencies is con-
stant or the exchange digraph is star shaped (a
digraph is star shaped if all arcs have a common
vertex).
Theorem 3 There are polynomial time algo-
rithms for .P / when the number of currencies is
constant.
Theorem 4 It is polynomially solvable to ﬁnd
the maximum revenue at the center currency of
arbitrage in a frictional foreign exchange mar-
ket with bid-ask spread, bound, and integrality
constraints when the exchange digraph is star
shaped.
However, if the exchange digraph is the coa-
lescence of a star-shaped exchange digraph and
its copy, shown by Digraph G1, then the problem
becomes NP-complete.
Theorem 5 It is NP-complete to decide whether
there exists arbitrage in a frictional foreign ex-
change market with bid-ask spreads, bound, and
integrality constraints even if its exchange di-
graph is coalescent.
Finally, an answer to Problem 3 is as follows
Theorem 6 There is an exchange digraph of
order n such that at least bn=2cdn=2e  1 trans-
actions and at least n2=4 C n  3 arcs are in
need to bring the system back to non-arbitrage
states.
For instance, consider the currency exchange
market corresponding to digraph G2 D .V; E/,
where the number of currencies is n D jV j, p D
bn=c, and K D n2.
Set
C D faij 2 Ej1  i  p; p C 1  j  ng
[ fa1.pC1/gnfa.pC1/1g [ fai.i1/j2  i  pg
[ fai.iC1/jp C 1  i  n  1g:
Then, jCj D bn=2cdn=2e C n  2 D jEj= >
n2=4 C n  3. It follows easily from the rates and
bounds that each arc in C has to be used to elim-
inate arbitrage. And bn=2cdn=2e  1 transactions
corresponding to faij 2 Ej1  i  p; p C 1 
j  ngnfa.pC1/1g are in need to bring the system
back to non-arbitrage states.
Applications
The present results show that different foreign
exchange systems exhibit quite different compu-
tational complexities. They may shed new light
on how monetary system models are adopted and
evolved in reality. In addition, it provides with
a computational complexity point of view to the
understanding of the now fast growing Internet
electronic exchange markets.

Arithmetic Coding for Data Compression
145
A
Open Problems
The dynamic models involving in both spot mar-
kets (in which goods are sold for cash and de-
livered immediately) and futures markets are the
most interesting ones. To develop good approxi-
mation algorithms for such general models would
be important. In addition, it is also important to
identify special market models for which poly-
nomial time algorithms are possible even with
future markets. Another interesting paradox in
this line of study is why friction constraints that
make arbitrage difﬁcult are not always eliminated
in reality.
Recommended Reading
1. Abeysekera SP, Turtle HJ (1995) Long-run relations
in exchange markets: a test of covered interest parity.
J Financ Res 18(4):431–447
2. Ausiello G, Crescenzi P, Gambosi G, Kann V,
Marchetti-Spaccamela A, Protasi M (1999) Com-
plexity and approximation: combinatorial optimiza-
tion problems and their approximability properties.
Springer, Berlin
3. Cai M, Deng X (2003) Approximation and com-
putation of arbitrage in frictional foreign exchange
market. Electron Notes Theor Comput Sci 78:1–10
4. Clinton K (1988) Transactions costs and covered
interest arbitrage: theory and evidence. J Polit Econ
96(2):358–370
5. Deng X, Papadimitriou C (1994) On the complexity
of cooperative game solution concepts. Math Oper
Res 19(2):257–266
6. Deng X, Li ZF, Wang S (2002) Computational com-
plexity of arbitrage in frictional security market. Int J
Found Comput Sci 13(5):681–684
7. Deng X, Papadimitriou C, Safra S (2003) On the
complexity of price equilibria. J Comput Syst Sci
67(2):311–324
8. Garey MR, Johnson DS (1979) Computers and
intractability:
a
guide
of
the
theory
of
NP-
completeness. Freeman, San Francisco
9. Jones CK (2001) A network model for foreign ex-
change arbitrage, hedging and speculation. Int J
Theor Appl Finance 4(6):837–852
10. Lenstra HW Jr (1983) Integer programming with a
ﬁxed number of variables. Math Oper Res 8(4):538–
548
11. Mavrides M (1992) Triangular arbitrage in the
foreign exchange market – inefﬁciencies, technol-
ogy and investment opportunities. Quorum Books,
London
12. Megiddo N (1978) Computational complexity and the
game theory approach to cost allocation for a tree.
Math Oper Res 3:189–196
13. Mundell RA (1981) Gold would serve into the 21st
century. Wall Str J, Sep 30, p 33
14. Mundell
RA
(2000)
Currency
areas,
exchange
rate systems, and international monetary reform,
paper delivered at Universidad del CEMA, Buenos
Aires.
http://www.robertmundell.net/pdf/Currency.
Accessed 17 Apr 2000
15. Zhang S, Xu C, Deng X (2002) Dynamic arbitrage-
free asset pricing with proportional transaction costs.
Math Finance 12(1):89–97
Arithmetic Coding for Data
Compression
Paul G. Howard1 and Jeffrey Scott Vitter2
1Akamai Technologies, Cambridge, MA, USA
2University of Kansas, Lawrence, KS, USA
Keywords
Entropy coding; Statistical data compression
Years and Authors of Summarized
Original Work
1994; Howard, Vitter
Problem Deﬁnition
Often it is desirable to encode a sequence of
data efﬁciently to minimize the number of bits
required to transmit or store the sequence. The
sequence may be a ﬁle or message consisting of
symbols (or letters or characters) taken from a
ﬁxed input alphabet, but more generally the se-
quence can be thought of as consisting of events,
each taken from its own input set. Statistical data
compression is concerned with encoding the data
in a way that makes use of probability estimates
of the events. Lossless compression has the prop-
erty that the input sequence can be reconstructed
exactly from the encoded sequence. Arithmetic

146
Arithmetic Coding for Data Compression
Arithmetic Coding for Data Compression, Table 1 Comparison of codes for Huffman coding, Hu-Tucker coding,
and arithmetic coding for a sample 5-symbol alphabet
Symbol e k
Prob.
Huffman
Hu-Tucker
Arithmetic
p k
log 2pk
Code
Length
Code
Length
Length
a
0.04
4.644
1111
4
000
3
4.644
b
0.18
2.474
110
3
001
3
2.474
c
0.43
1.218
0
1
01
2
1.218
d
0.15
2.737
1110
4
10
2
2.737
e
0.20
2.322
10
2
11
2
2.322
coding is a nearly optimal statistical coding tech-
nique that can produce a lossless encoding.
Problem (statistical data compression) INPUT:
A sequence of m events a1, a2; : : : ; am. The ith
event ai is taken from a set of n distinct pos-
sible events ei;1, ei;2; : : : ; ei;n, with an accurate
assessment of the probability distribution Pi of
the events. The distributions Pi need not be the
same for each event ai.
OUTPUT: A succinct encoding of the events
that can be decoded to recover exactly the origi-
nal sequence of events.
The goal is to achieve optimal or near-optimal
encoding length. Shannon [10] proved that the
smallest possible expected number of bits needed
to encode the ith event is the entropy of Pi,
denoted by
H.Pi/ D
n
X
k D 1
pi;k log2 pi;k
where pi;k is the probability that ek occurs as
the ith event. An optimal code outputs – log 2p
bits to encode an event whose probability of
occurrence is p.
The well-known Huffman codes [6] are opti-
mal only among preﬁx (or instantaneous) codes,
that is, those in which the encoding of one event
can be decoded before encoding has begun for
the next event. Hu-Tucker codes are preﬁx codes
similar to Huffman codes and are derived using a
similar algorithm, with the added constraint that
coded messages preserve the ordering of original
messages.
When an instantaneous code is not needed,
as is often the case, arithmetic coding provides
a number of beneﬁts, primarily by relaxing the
constraint that the code lengths must be integers:
(1) The code length is optimal (log 2p bits for
an event with probability p/, even when prob-
abilities are not integer powers of 1
2. (2) There
is no loss of coding efﬁciency even for events
with probability close to 1. (3) It is trivial to
handle probability distributions that change from
event to event. (4) The input message to output
message ordering correspondence of Hu-Tucker
coding can be obtained with minimal extra effort.
As an example, consider a 5-symbol input
alphabet. Symbol probabilities, codes, and code
lengths are given in Table 1.
The average code length is 2.13 bits per in-
put symbol for the Huffman code, 2.22 bits per
symbol for the Hu-Tucker code, and 2.03 bits per
symbol for arithmetic coding.
Key Results
In theory, arithmetic codes assign one “code-
word” to each possible input sequence. The code-
words consist of half-open subintervals of the
half-open unit interval [0,1) and are expressed by
specifying enough bits to distinguish the subinter-
val corresponding to the actual sequence from all
other possible subintervals. Shorter codes corre-
spond to larger subintervals and thus more prob-
able input sequences. In practice, the subinterval
is reﬁned incrementally using the probabilities of
the individual events, with bits being output as
soon as they are known. Arithmetic codes almost
always give better compression than preﬁx codes,
but they lack the direct correspondence between

Arithmetic Coding for Data Compression
147
A
the events in the input sequence and bits or groups
of bits in the coded output ﬁle.
The algorithm for encoding a ﬁle using arith-
metic coding works conceptually as follows:
1. The “current interval” [L; H) is initialized to
[0,1).
2. For each event in the ﬁle, two steps are per-
formed:
(a) Subdivide the current interval into subinter-
vals, one for each possible event. The size
of an event’s subinterval is proportional to
the estimated probability that the event will
be the next event in the ﬁle, according to
the model of the input.
(b) Select the subinterval corresponding to the
event that actually occurs next and make it
the new current interval.
3. Output enough bits to distinguish the ﬁnal
current interval from all other possible ﬁnal
intervals.
The length of the ﬁnal subinterval is clearly
equal to the product of the probabilities of the
individual events, which is the probability p of
the particular overall sequence of events. It can
be shown that b
 log 2pc + 2 bits are enough
to distinguish the ﬁle from all other possible
ﬁles.
For ﬁnite-length ﬁles, it is necessary to in-
dicate the end of the ﬁle. In arithmetic coding,
this can be done easily by introducing a special
low-probability event that can be injected into
the input stream at any point. This adds only
O(log m) bits to the encoded length of an m-
symbol ﬁle.
In step 2, one needs to compute only the
subinterval corresponding to the event ai that
actually occurs. To do this, it is convenient to
use two “cumulative” probabilities: the cumula-
tive probability PC
D
i  1
P
k D 1
pk and the next-
cumulative probability PN
D
PC C pi
D
i  1
P
k D 1
pk. The new subinterval is [L C PC .H 
L/,L + PN .H

L//. The need to maintain
and supply cumulative probabilities requires the
model to have a sophisticated data structure, such
as that of Moffat [7], especially when many more
than two events are possible.
Modeling
The
goal
of
modeling
for
statistical
data
compression is to provide probability infor-
mation to the coder. The modeling process
consists of structural and probability estimation
components; each may be adaptive (starting
from a neutral model, gradually build up the
structure and probabilities based on the events
encountered), semi-adaptive (specify an initial
model that describes the events to be encountered
in the data and then modify the model during
coding so that it describes only the events yet to
be coded), or static (specify an initial model and
use it without modiﬁcation during coding).
In addition there are two strategies for prob-
ability estimation. The ﬁrst is to estimate each
event’s probability individually based on its fre-
quency within the input sequence. The second is
to estimate the probabilities collectively, assum-
ing a probability distribution of a particular form
and estimating the parameters of the distribution,
either directly or indirectly. For direct estimation,
the data can yield an estimate of the parameter
(the variance, for instance). For indirect esti-
mation [4], one can start with a small number
of possible distributions and compute the code
length that would be obtained with each; the one
with the smallest code length is selected. This
method is very general and can be used even
for distributions from different families, without
common parameters.
Arithmetic coding is often applied to text com-
pression. The events are the symbols in the text
ﬁle, and the model consists of the probabilities
of the symbols considered in some context. The
simplest model uses the overall frequencies of
the symbols in the ﬁle as the probabilities; this
is a zero-order Markov model, and its entropy
is denoted H0. The probabilities can be esti-
mated adaptively starting with counts of 1 for all
symbols and incrementing after each symbol is
coded, or the symbol counts can be coded before
coding the ﬁle itself and either modiﬁed during
coding (a decrementing semi-adaptive code) or
left unchanged (a static code). In all cases, the

148
Arithmetic Coding for Data Compression
code length is independent of the order of the
symbols in the ﬁle.
Theorem 1 For all input ﬁles, the code length
LA of an adaptive code with initial 1-weights is
the same as the code length LSD of the semi-
adaptive decrementing code plus the code length
LM of the input model encoded assuming that
all symbol distributions are equally likely. This
code length is less than LS = mH0 + LM, the
code length of a static code with the same input
model. In other words, LA = LSD + LM
<
mH0 + LM = LS.
It is possible to obtain considerably better
text compression by using higher-order Markov
models. Cleary and Witten [2] were the ﬁrst to
do this with their PPM method. PPM requires
adaptive modeling and coding of probabilities
close to 1 and makes heavy use of arithmetic
coding.
Implementation Issues
Incremental Output
The basic implementation of arithmetic coding
described above has two major difﬁculties: the
shrinking current interval requires the use of
high-precision arithmetic, and no output is pro-
duced until the entire ﬁle has been read. The
most straightforward solution to both of these
problems is to output each leading bit as soon
as it is known and then to double the length
of the current interval so that it reﬂects only
the unknown part of the ﬁnal interval. Witten,
Neal, and Cleary [11] add a clever mechanism
for preventing the current interval from shrinking
too much when the endpoints are close to
1
2
but straddle
1
2. In that case, one does not yet
know the next output bit, but whatever it is, the
following bit will have the opposite value; one
can merely keep track of that fact and expand
the current interval symmetrically about 1
2. This
follow-on procedure may be repeated any number
of times, so the current interval size is always
strictly longer than 1
4.
Before [11] other mechanisms for incremental
transmission and ﬁxed precision arithmetic were
developed through the years by a number of
researchers beginning with Pasco [8]. The bit-
stufﬁng idea of Langdon and others at IBM [9]
that limits the propagation of carries in the ad-
ditions serves a function similar to that of the
follow-on procedure described above.
Use of Integer Arithmetic
In practice, the arithmetic can be done by storing
the endpoints of the current interval as sufﬁ-
ciently large integers rather than in ﬂoating point
or exact rational numbers. Instead of starting
with the real interval [0,1), start with the integer
interval [0,N ), N invariably being a power of 2.
The subdivision process involves selecting non-
overlapping integer intervals (of length at least
1) with lengths approximately proportional to the
counts.
Limited-Precision Arithmetic Coding
Arithmetic coding as it is usually implemented is
slow because of the multiplications (and in some
implementations, divisions) required in subdivid-
ing the current interval according to the prob-
ability information. Since small errors in prob-
ability estimates cause very small increases in
code length, introducing approximations into the
arithmetic coding process in a controlled way
can improve coding speed without signiﬁcantly
degrading compression performance. In the Q-
Coder work at IBM [9], the time-consuming mul-
tiplications are replaced by additions and shifts,
and low-order bits are ignored.
Howard and Vitter [3] describe a different
approach to approximate arithmetic coding. The
fractional bits characteristic of arithmetic coding
are stored as state information in the coder. The
idea, called quasi-arithmetic coding, is to reduce
the number of possible states and replace arith-
metic operations by table lookups; the lookup
tables can be precomputed.
The number of possible states (after applying
the interval expansion procedure) of an arithmetic
coder using the integer interval [0,N / is 3N
2/16. The obvious way to reduce the number of
states in order to make lookup tables practicable
is to reduce N . Binary quasi-arithmetic coding
causes an insigniﬁcant increase in the code length
compared with pure arithmetic coding.

Arithmetic Coding for Data Compression
149
A
Theorem 2 In a quasi-arithmetic coder based
on full interval [0,N /, using correct probability
estimates, and excluding very large and very
small probabilities, the number of bits per input
event by which the average code length obtained
by the quasi-arithmetic coder exceeds that of an
exact arithmetic coder is at most
4
ln 2

log2
2
e ln 2
 1
N C O
 1
N 2


 0:497
N
C O
 1
N 2

;
and the fraction by which the average code length
obtained by the quasi-arithmetic coder exceeds
that of an exact arithmetic coder is at most

log2
2
e ln 2

1
log2 N C O

1
.log N/2


0:0861
log2 N C O

1
.log N/2

:
General-purpose algorithms for parallel encod-
ing and decoding using both Huffman and quasi-
arithmetic coding are given in [5].
Applications
Arithmetic coding can be used in most applica-
tions of data compression. Its main usefulness is
in obtaining maximum compression in conjunc-
tion with an adaptive model or when the probabil-
ity of one event is close to 1. Arithmetic coding
has been used heavily in text compression. It has
also been used in image compression in the JPEG
international standards for image compression
and is an essential part of the JBIG international
standards for bilevel image compression. Many
fast implementations of arithmetic coding, espe-
cially for a two-symbol alphabet, are covered by
patents; considerable effort has been expended in
adjusting the basic algorithm to avoid infringing
those patents.
Open Problems
The technical problems with arithmetic coding it-
self have been completely solved. The remaining
unresolved issues are concerned with modeling,
in which the issue is how to decompose an input
data set into a sequence of events, so that the set
of events possible at each point in the data set can
be described by a probability distribution suitable
for input into the coder. The modeling issues are
entirely application-speciﬁc.
Experimental Results
Some experimental results for the Calgary and
Canterbury corpora are summarized in a report
by Arnold and Bell [1].
Data Sets
Among the most widely used data sets suitable
for
research
in
arithmetic
coding
are
the
Calgary Corpus and Canterbury Corpus (corpus.
canterbury.ac.nz) and the Pizza&Chili Corpus
(pizzachili.dcc.uchile.cl
or
http://pizzachili.di.
unipi.it).
URL to Code
A number of implementations of arithmetic cod-
ing are available on The Data Compression Re-
source on the Internet, www.data-compression.
info/Algorithms/AC/.
Cross-References
▷Burrows-Wheeler Transform
▷Huffman Coding
Recommended Reading
1. Arnold R, Bell T (1997) A corpus for the evaluation
of lossless compression algorithms. In: Proceedings
of the IEEE data compression conference, Snowbird,
Mar 1997, pp 201–210

150
Assignment Problem
2. Cleary JG, Witten IH (1984) Data compression using
adaptive coding and partial string matching. IEEE
Trans Commun COM-32:396–402
3. Howard PG, Vitter JS (1992) Practical implementa-
tions of arithmetic coding. In: Storer JA (ed) Images
and text compression. Kluwer Academic, Norwell
4. Howard PG, Vitter JS (1993) Fast and efﬁcient loss-
less image compression. In: Proceedings of the IEEE
data compression conference, Snowbird, Mar 1993,
pp 351–360
5. Howard PG, Vitter JS (1996) Parallel lossless image
compression using Huffman and arithmetic coding.
Inf Process Lett 59:65–73
6. Huffman DA (1952) A method for the construction
of minimum redundancy codes. Proc Inst Radio Eng
40:1098–1101
7. Moffat A (1999) An improved data structure for cu-
mulative probability tables. Softw Pract Exp 29:647–
659
8. Pasco R (1976) Source coding algorithms for fast data
compression. Ph.D. thesis, Stanford University
9. Pennebaker WB, Mitchell JL, Langdon GG, Arps RB
(1988) An overview of the basic principles of the Q-
coder adaptive binary arithmetic coder. IBM J Res
Dev 32:717–726
10. Shannon CE (1948) A mathematical theory of com-
munication. Bell Syst Tech J 27:398–403
11. Witten IH, Neal RM, Cleary JG (1987) Arithmetic
coding for data compression. Commun ACM 30:520–
540
Assignment Problem
Samir Khuller
Computer Science Department, University of
Maryland, College Park, MD, USA
Keywords
Weighted bipartite matching
Years and Authors of Summarized
Original Work
1955; Kuhn
1957; Munkres
Problem Deﬁnition
Assume
that
a
complete
bipartite
graph,
G.X; Y; X  Y /, with weights w.x; y/ assigned
to every edge (x; y) is given. A matching M
is a subset of edges so that no two edges in
M have a common vertex. A perfect matching
is one in which all the nodes are matched.
Assume that jXj D jY j D n. The weighted
matching problem is to ﬁnd a matching with the
greatest total weight, where w .M/ D P
e2M
w .e/.
Since G is a complete bipartite graph, it has a
perfect matching. An algorithm that solves the
weighted matching problem is due to Kuhn [4]
and Munkres [6]. Assume that all edge weights
are non-negative.
Key Results
Deﬁne a feasible vertex labeling ` as a mapping
from the set of vertices in G to the reals, where
` .x/ C ` .y/  w .x; y/ :
Call `.x/ the label of vertex x. It is easy to
compute a feasible vertex labeling as follows:
8y 2 Y ` .y/ D 0
and
8x 2 X
` .x/ D max
y2Y w .x; y/ :
Deﬁne the equality subgraph, G`, to be the
spanning subgraph of G, which includes all ver-
tices of G but only those edges (x; y) that have
weights such that
w .x; y/ D ` .x/ C ` .y/ :
The connection between equality subgraphs and
maximum-weighted matchings is provided by the
following theorem:
Theorem 1 If the equality subgraph, G`, has a
perfect matching, M , then M  is a maximum-
weighted matching in G.
In fact, note that the sum of the labels is an
upper bound on the weight of the maximum-
weighted perfect matching. The algorithm even-
tually ﬁnds a matching and a feasible labeling

Assignment Problem
151
A
such that the weight of the matching is equal to
the sum of all the labels.
High-Level Description
The above theorem is the basis of an algorithm
for ﬁnding a maximum-weighted matching in a
complete bipartite graph. Starting with a feasible
labeling, compute the equality subgraph, and then
ﬁnd a maximum matching in this subgraph (here,
one can ignore weights on edges). If the matching
found is perfect, the process is done. If it is not
perfect, more edges are added to the equality
subgraph by revising the vertex labels. After
adding edges to the equality subgraph, either the
size of the matching goes up (an augmenting path
is found) or the Hungarian tree continues to grow.
(This is the structure of explored edges when one
starts BFS simultaneously from all free nodes
in S. When one reaches a matched node in T ,
one only explores the matched edge; however, all
edges incident to nodes in S are explored.) In
the former case, the phase terminates, and a new
phase starts (since the matching size has gone
up). In the latter case, the Hungarian tree grows
by adding new nodes to it, and clearly, this cannot
happen more than n times.
Let S be the set of free nodes in X. Grow
Hungarian trees from each node in S. Let T be
the nodes in Y encountered in the search for an
augmenting path from nodes in S. Add all nodes
from X that are encountered in the search to S.
Note the following about this algorithm:
S D XnS:
T D Y nT:
jSj > jT j :
There are no edges from S to T since this would
imply that one did not grow the Hungarian trees
completely. As the Hungarian trees are grown in
G`, alternate nodes in the search are placed into
S and T . To revise the labels, take the labels in S,
and start decreasing them uniformly (say, by ),
and at the same time, increase the labels in T by
. This ensures that the edges from S to T do not
leave the equality subgraph (Fig. 1).
Assignment Problem, Fig. 1 Sets S and T as main-
tained by the algorithm
As the labels in S are decreased, edges (in G)
from S to T will potentially enter the equality
subgraph, G`. As we increase , at some point in
time, an edge enters the equality subgraph. This
is when one stops and updates the Hungarian tree.
If the node from T added to T is matched to a
node in S, both these nodes are moved to S and
T , which yields a larger Hungarian tree. If the
node from T is free, an augmenting path is found,
and the phase is complete. One phase consists of
those steps taken between increases in the size of
the matching. There are at most n phases, where
n is the number of vertices in G (since in each
phase the size of the matching increases by 1).
Within each phase, the size of the Hungarian tree
is increased at most n times. It is clear that in
O.n2/ time, one can ﬁgure out which edge from
S to T is the ﬁrst to enter the equality subgraph
(one simply scans all the edges). This yields an
O.n4/ bound on the total running time. How to
implement it in O.n3/ time is now shown.

152
Asynchronous Consensus Impossibility
More Efﬁcient Implementation
Deﬁne the slack of an edge as follows:
slack .x; y/ D ` .x/ C ` .y/  w .x; y/ :
Then,
 D
min
x2S;y2T
slack .x; y/ :
Naively, the calculation of  requires O.n2/ time.
For every vertex y 2 T , keep track of the edge
with the smallest slack, i.e.,
slack Œy D min
x2S slack .x; y/ :
The computation of slack[y] (for all y 2 T )
requires O.n2/ time at the start of a phase. As the
phase progresses, it is easy to update all the slack
values in O.n/ time since all of them change by
the same amount (the labels of the vertices in S
are going down uniformly). Whenever a node u
is moved from S to S, one must recompute the
slacks of the nodes in T , requiring O.n/ time.
But a node can be moved from S to S at most n
times.
Thus, each phase can be implemented in
O.n2/ time. Since there are n phases, this gives
a running time of O.n3/. For sparse graphs,
there is a way to implement the algorithm in
O.n.m C n log n// time using min cost ﬂows
[1], where m is the number of edges.
Applications
There are numerous applications of biparitite
matching, for example, scheduling unit-length
jobs with integer release times and deadlines even
with time-dependent penalties.
Open Problems
Obtaining a linear, or close to linear, time algo-
rithm.
Recommended Reading
Several books on combinatorial optimization de-
scribe algorithms for weighted bipartite matching
(see [2,5]). See also Gabow’s paper [3].
1. Ahuja R, Magnanti T, Orlin J (1993) Network ﬂows:
theory, algorithms and applications. Prentice Hall, En-
glewood Cliffs
2. Cook W, Cunningham W, Pulleyblank W, Schrijver A
(1998) Combinatorial Optimization. Wiley, New York
3. Gabow H (1990) Data structures for weighted match-
ing and nearest common ancestors with linking. In:
Symposium on discrete algorithms, San Francisco,
pp 434–443
4. Kuhn H (1955) The Hungarian method for the assign-
ment problem. Naval Res Logist Q 2:83–97
5. Lawler E (1976) Combinatorial optimization: net-
works and matroids. Holt, Rinehart and Winston, New
York
6. Munkres J (1957) Algorithms for the assignment and
transportation problems. J Soc Ind Appl Math 5:32–38
Asynchronous Consensus
Impossibility
Maurice Herlihy
Department of Computer Science,
Brown University, Providence, RI, USA
Keywords
Agreement; Wait-free consensus
Years and Authors of Summarized
Original Work
1985; Fischer, Lynch, Paterson
Problem Deﬁnition
Consider a distributed system consisting of a set
of processes that communicate by sending and
receiving messages. The network is a multiset of
messages, where each message is addressed to
some process. A process is a state machine that
can take three kinds of steps.

Asynchronous Consensus Impossibility
153
A
•
In a send step, a process places a message in
the network.
•
In a receive step, a process A either reads
and removes from the network a message
addressed to A, or it reads a distinguished
null value, leaving the network unchanged.
If a message addressed to A is placed in the
network, and if A subsequently performs an
inﬁnite number of receive steps, then A will
eventually receive that message.
•
In a computation state, a process changes
state without communicating with any other
process.
Processes are asynchronous: there is no bound on
their relative speeds. Processes can crash: they
can simply halt and take no more steps. This
article considers executions in which at most one
process crashes.
In the consensus problem, each process starts
with a private input value, communicates with the
others, and then halts with a decision value. These
values must satisfy the following properties:
•
Agreement: all processes’ decision values
must agree.
•
Validity: every decision value must be some
process’ input.
•
Termination: every non-fault process must de-
cide in a ﬁnite number of steps.
Fischer, Lynch, and Paterson showed that there
is no protocol that solves consensus in any asyn-
chronous message-passing system where even
a single process can fail. This result is one of
the most inﬂuential results in Distributed Com-
puting, laying the foundations for a number of
subsequent research efforts.
Terminology
Without loss of generality, one can restrict atten-
tion to binary consensus, where the inputs are
0 or 1. A protocol state consists of the states of the
processes and the multiset of messages in transit
in the network. An initial state is a protocol state
before any process has moved, and a ﬁnal state is
a protocol state after all processes have ﬁnished.
The decision value of any ﬁnal state is the value
decided by all processes in that state.
Any terminating protocol’s set of possible
states forms a tree, where each node represents
a
possible
protocol
state,
and
each
edge
represents a possible step by some process.
Because the protocol must terminate, the tree is
ﬁnite. Each leaf node represents a ﬁnal protocol
state with decision value either 0 or 1.
A bivalent protocol state is one in which the
eventual decision value is not yet ﬁxed. From any
bivalent state, there is an execution in which the
eventual decision value is 0, and another in which
it is 1. A univalent protocol state is one in which
the outcome is ﬁxed. Every execution starting
from a univalent state decides the same value.
A 1-valent protocol state is univalent with even-
tual decision value 1, and similarly for a 0-valent
state.
A protocol state is critical if
•
it is bivalent, and
•
if any process takes a step, the protocol state
becomes univalent.
Key Results
Lemma 1 Every consensus protocol has a biva-
lent initial state.
Proof Assume, by way of contradiction, that
there exists a consensus protocol for (n C 1)
threads A0;    ; An in which every initial state
is univalent. Let si be the initial state where pro-
cesses Ai;    ; An have input 0 and A0; : : : ; Ai1
have input 1. Clearly, s0 is 0-valent: all processes
have input 0, so all must decide 0 by the validity
condition. If si is 0-valent, so is siC1. These states
differ only in the input to process Ai W 0 in si,
and 1 in siC1. Any execution starting from si in
which Ai halts before taking any steps is indistin-
guishable from an execution starting from siC1
in which Ai halts before taking any steps. Since
processes must decide 0 in the ﬁrst execution,
they must decide 1 in the second. Since there

154
Asynchronous Consensus Impossibility
is one execution starting from siC1 that decides
0, and since siC1 is univalent by hypothesis,
siC1 is 0-valent. It follows that the state snC1, in
which all processes start with input 1, is 0-valent,
a contradiction.

Lemma 2 Every consensus protocol has a criti-
cal state.
Proof by contradiction. By Lemma 1, the proto-
col has a bivalent initial state. Start the protocol
in this state. Repeatedly choose a process whose
next step leaves the protocol in a bivalent state,
and let that process take a step. Either the protocol
runs forever, violating the termination condition,
or the protocol eventually enters a critical state.
Theorem 1 There is no consensus protocol for
an asynchronous message-passing system where
a single process can crash.
Proof Assume by way of contradiction that such
a protocol exists. Run the protocol until it reaches
a critical state s. There must be two processes A
and B such that A’s next step carries the protocol
to a 0-valent state, and B’s next step carries the
protocol to a 1-valent state.
Starting from s, let sA be the state reached if
A takes the ﬁrst step, sB if B takes the ﬁrst step,
sAB if A takes a step followed by B, and so on.
States sA and sAB are 0-valent, while sB and sBA
are 1-valent. The rest is a case analysis.
Of all the possible pairs of steps A and B could
be about to execute, most of them commute: states
sAB and sBA are identical, which is a contradiction
because they have different valences.
The only pair of steps that do not commute
occurs when A is about to send a message to B
(or vice versa). Let sAB be the state resulting if
A sends a message to B and B then receives it,
and let sBA be the state resulting if B receives
a different message (or null) and then A sends
its message to B. Note that every process other
than B has the same local state in sAB and sBA.
Consider an execution starting from sAB in which
every process other than B takes steps in round-
robin order. Because sAB is 0-valent, they will
eventually decide 0. Next, consider an execution
starting from sBA in which every process other
than B takes steps in round-robin order. Because
sBA is 1-valent, they will eventually decide 1. But
all processes other than B have the same local
states at the end of each execution, so they cannot
decide different values, a contradiction.

In the proof of this theorem, and in the proofs
of the preceding lemmas, we construct scenarios
where at most a single process is delayed. As a re-
sult, this impossibility result holds for any system
where a single process can fail undetectably.
Applications
The consensus problem is a key tool for un-
derstanding the power of various asynchronous
models of computation.
Open Problems
There are many open problems concerning the
solvability of consensus in other models, or with
restrictions on inputs.
Related Work
The original paper by Fischer, Lynch, and Pater-
son [8] is still a model of clarity.
Many researchers have examined alternative
models of computation in which consensus can
be solved. Dolev, Dwork, and Stockmeyer [5]
examine a variety of alternative message-passing
models, identifying the precise assumptions
needed to make consensus possible. Dwork,
Lynch, and Stockmeyer [6] derive upper and
lower bounds for a semi-synchronous model
where there is an upper and lower bound on
message delivery time. Ben-Or [1] showed that
introducing
randomization
makes
consensus
possible in an asynchronous message-passing
system. Chandra and Toueg [3] showed that
consensus becomes possible if in the presence
of an oracle that can (unreliably) detect when
a process has crashed. Each of the papers cited

Atomic Broadcast
155
A
here has inspired many follow-up papers. A good
place to start is the excellent survey by Fich and
Ruppert [7].
A protocol is wait-free if it tolerates failures
by all but one of the participants. A concur-
rent object implementation is linearizable if each
method call seems to take effect instantaneously
at some point between the method’s invocation
and response. Herlihy [9] showed that shared-
memory objects can each be assigned a consensus
number, which is the maximum number of pro-
cesses for which there exists a wait-free consen-
sus protocol using a combination of read-write
memory and the objects in question. Consensus
numbers induce an inﬁnite hierarchy on objects,
where (simplifying somewhat) higher objects are
more powerful than lower objects. In a system of
n or more concurrent processes, it is impossible
to construct a lock-free implementation of an
object with consensus number n from an object
with a lower consensus number. On the other
hand, any object with consensus number n is
universal in a system of n or fewer processes: it
can be used to construct a wait-free linearizable
implementation of any object.
In 1990, Chaudhuri [4] introduced the k-set
agreement problem (sometimes called k-set con-
sensus, which generalizes consensus by allowing
k or fewer distinct decision values to be chosen.
In particular, 1-set agreement is consensus. The
question whether k-set agreement can be solved
in asynchronous message-passing models was
open for several years, until three independent
groups [2, 10, 11] showed that no protocol exists.
Cross-References
▷Linearizability
▷Topology Approach in Distributed Computing
Recommended Reading
1. Ben-Or M (1983) Another advantage of free choice
(extended abstract): completely asynchronous agree-
ment protocols. In: PODC ’83: proceedings of the
second annual ACM symposium on principles of dis-
tributed computing. ACM Press, New York, pp 27–30
2. Borowsky E, Gafni E (1993) Generalized FLP
impossibility result for t-resilient asynchronous com-
putations. In: Proceedings of the 1993 ACM sympo-
sium on theory of computing, May 1993, pp 206–
215
3. Chandra TD, Toueg S (1996) Unreliable failure
detectors for reliable distributed systems. J ACM
43(2):225–267
4. Chaudhuri S (1990) Agreement is harder than con-
sensus: set consensus problems in totally asyn-
chronous systems. In: Proceedings of the ninth annual
ACM symposium on principles of distributed com-
puting, Aug 1990, pp 311–324
5. Chandhuri S (1993) More choices allow more faults:
set consensus problems in totally asynchronous sys-
tems. Inf Comput 105(1):132–158
6. Dwork C, Lynch N, Stockmeyer L (1988) Consen-
sus in the presence of partial synchrony. J ACM
35(2):288–323
7. Fich F, Ruppert E (2003) Hundreds of impossibility
results for distributed computing. Distrib Comput
16(2–3):121–163
8. Fischer M, Lynch N, Paterson M (1985) Impossibility
of distributed consensus with one faulty process. J
ACM 32(2):374–382
9. Herlihy M (1991) Wait-free synchronization. ACM
Trans Program Lang Syst (TOPLAS) 13(1):124–149
10. Herlihy M, Shavit N (1999) The topological structure
of asynchronous computability. J ACM 46(6):858–
923
11. Saks ME, Zaharoglou F (2000) Wait-free k-set agree-
ment is impossible: the topology of public knowl-
edge. SIAM J Comput 29(5):1449–1483
Atomic Broadcast
Xavier Défago
School of Information Science, Japan Advanced
Institute of Science and Technology (JAIST),
Ishikawa, Japan
Keywords
Atomic multicast; Total order broadcast; Total
order multicast
Years and Authors of Summarized
Original Work
1995; Cristian, Aghili, Strong, Dolev

156
Atomic Broadcast
Problem Deﬁnition
The problem is concerned with allowing a set
of processes to concurrently broadcast messages
while ensuring that all destinations consistently
deliver them in the exact same sequence, in spite
of the possible presence of a number of faulty
processes.
The work of Cristian, Aghili, Strong, and
Dolev [7] considers the problem of atomic broad-
cast in a system with approximately synchronized
clocks and bounded transmission and processing
delays. They present successive extensions of an
algorithm to tolerate a bounded number of omis-
sion, timing, or Byzantine failures, respectively.
Related Work
The work presented in this entry originally ap-
peared as a widely distributed conference contri-
bution [6], over a decade before being published
in a journal [7], at which time the work was well-
known in the research community. Since there
was no signiﬁcant change in the algorithms, the
historical context considered here is hence with
respect to the earlier version.
Lamport [11] proposed one of the ﬁrst pub-
lished algorithms to solve the problem of order-
ing broadcast messages in a distributed systems.
That algorithm, presented as the core of a mutual
exclusion algorithm, operates in a fully asyn-
chronous system (i.e., a system in which there
are no bounds on processor speed or commu-
nication delays), but does not tolerate failures.
Although the algorithms presented here rely on
physical clocks rather than Lamport’s logical
clocks, the principle used for ordering messages
is essentially the same: message carry a times-
tamp of their sending time; messages are deliv-
ered in increasing order of the timestamp, using
the sending processor name for messages with
equal timestamps.
At roughly the same period as the initial publi-
cation of the work of Cristian et al. [6], Chang and
Maxemchuck [3] proposed an atomic broadcast
protocol based on a token passing protocol, and
tolerant to crash failures of processors. Also,
Carr [1] proposed the Tandem global update pro-
tocol, tolerant to crash failures of processors.
Cristian [5] later proposed an extension to
the omission-tolerant algorithm presented here,
under the assumption that the communication
system consists of f C 1 independent broad-
cast channels (where f is the maximal number
of faulty processors). Compared with the more
general protocol presented here, its extension
generates considerably fewer messages.
Since the work of Cristian, Aghili, Strong,
and Dolev [7], much has been published on the
problem of atomic broadcast (and its numerous
variants). For further reading, Défago, Schiper,
and Urbán [8] surveyed more than sixty different
algorithms to solve the problem, classifying them
into ﬁve different classes and twelve variants.
That survey also reviews many alternative deﬁ-
nitions and references about two hundred articles
related to this subject. This is still a very active
research area, with many new results being pub-
lished each year.
Hadzilacos and Toueg [10] provide a system-
atic classiﬁcation of speciﬁcations for variants of
atomic broadcast as well as other broadcast prob-
lems, such as reliable broadcast, FIFO broadcast,
or causal broadcast.
Chandra and Toueg [2] proved the equiva-
lence between atomic broadcast and the con-
sensus problem. Thus, any application solved
by a consensus can also be solved by atomic
broadcast and vice-versa. Similarly, impossibil-
ity results apply equally to both problems. For
instance, it is well-known that consensus, thus
atomic broadcast, cannot be solved deterministi-
cally in an asynchronous system with the pres-
ence of a faulty process [9].
Notations and Assumptions
The system G consists of n distributed processors
and m point-to-point communication links. A link
does not necessarily exists between every pair
of processors, but it is assumed that the com-
munication network remains connected even in
the face of faults (whether processors or links).
All processors have distinct names and there
exists a total order on them (e.g., lexicographic
order).

Atomic Broadcast
157
A
A component (link or processor) is said to
be correct if its behavior is consistent with
its speciﬁcation, and faulty otherwise. The
paper considers three classes of component
failures, namely, omission, timing, and Byzantine
failures.
•
An omission failure occurs when the faulty
component fails to provide the speciﬁed out-
put (e.g., loss of a message).
•
A timing failure occurs when the faulty com-
ponent omits a speciﬁed output, or provides it
either too early or too late.
•
A Byzantine failure [12] occurs when the com-
ponent does not behave according to its spec-
iﬁcation, for instance, by providing output
different from the one speciﬁed. In particular,
the paper considers authentication-detectable
Byzantine failures, that is, ones that are de-
tectable using a message authentication pro-
tocol, such as error correction codes or digital
signatures.
Each processor p has access to a local clock Cp
with the properties that (1) two separate clock
readings yield different values, and (2) clocks are
"-synchronized, meaning that, at any real time t,
the deviation in readings of the clocks of any two
processors p and q is at most ".
In addition, transmission and processing de-
lays, as measured on the clock of a correct pro-
cessor, are bounded by a known constant •. This
bound accounts not only for delays in transmis-
sion and processing, but also for delays due to
scheduling, overload, clock drift or adjustments.
This is called a synchronous system model.
The diffusion time dı is the time necessary
to propagate information to all correct processes,
in a surviving network of diameter d with the
presence of a most   processor failures and œ link
failures.
Problem Deﬁnition
The problem of atomic broadcast is deﬁned in
a synchronous system model as a broadcast prim-
itive which satisﬁes the following three proper-
ties: atomicity, order, and termination.
Problem 1 (Atomic broadcast)
INPUT: A stream of messages broadcast by
n concurrent processors, some of which may be
faulty.
OUTPUT: The messages delivered in sequence,
with the following properties:
1. Atomicity: if any correct processor delivers
an update at time U on its clock, then that
update was initiated by some processor and is
delivered by each correct processor at time U
on its clock.
2. Order: all updates delivered by correct proces-
sors are delivered in the same order by each
correct processor.
3. Termination: every update whose broadcast is
initiated by a correct processor at time T on its
clock is delivered at all correct processors at
time T C  on their clock.
Nowadays,
problem
deﬁnitions
for
atomic
broadcast that do not explicitly refer to physical
time are often preferred. Many variants of time-
free deﬁnitions are reviewed by Hadzilacos and
Toueg [10] and Défago et al. [8]. One such
alternate deﬁnition is presented below, with
the terminology adapted to the context of this
entry.
Problem 2 (Total order broadcast)
INPUT: A stream of messages broadcast by n con-
current processors, some of which may be faulty.
OUTPUT: The messages delivered in sequence,
with the following properties:
1. Validity: if a correct processor broadcasts
a message m, then it eventually delivers m.
2. Uniform agreement: if a processor delivers
a message m, then all correct processors even-
tually deliver m.
3. Uniform integrity: for any message m, every
processor delivers m at most once, and only
if m was previously broadcast by its sending
processor.
4. Gap-free uniform total order: if some pro-
cessor delivers message m0 after message m,
then a processor delivers m0 only after it has
delivered m.

158
Atomic Broadcast
Key Results
The paper presents three algorithms for solving
the problem of atomic broadcast, each under an
increasingly demanding failure model, namely,
omission, timing, and Byzantine failures. Each
protocol is actually an extension of the previous
one.
All three protocols are based on a clas-
sical
ﬂooding,
or
information
diffusion,
algorithm
[14].
Every
message
carries
its
initiation timestamp T, the name of the initiating
processor s, and an update . A message is then
uniquely identiﬁed by (s, T). Then, the basic
protocol is simple. Each processor logs every
message it receives until it is delivered. When it
receives a message that was never seen before,
it forwards that message to all other neighbor
processors.
Atomic Broadcast for Omission Failures
The ﬁrst atomic broadcast protocol, supporting
omission
failures,
considers
a
termination
time o as follows.
o D ı C dı C " :
(1)
The delivery deadline T C o is the time by
which a processor can be sure that it has received
copies of every message with timestamp T (or
earlier) that could have been received by some
correct process.
The protocol then works as follows. When
a processor initiates an atomic broadcast, it prop-
agates that message, similar to the diffusion al-
gorithm described above. The main exception is
that every message received after the local clock
exceeds the delivery deadline of that message,
is discarded. Then, at local time T C o, a pro-
cessor delivers all messages timestamped with T,
in order of the name of the sending processor.
Finally, it discards all copies of the messages
from its logs.
Atomic Broadcast for Timing Failures
The second protocol extends the ﬁrst one by in-
troducing a hop count (i.e., a counter incremented
each time a message is relayed) to the messages.
With this information, each relaying processor
can determine when a message is timely, that is,
if a message timestamped T with hop count h is
received at time U then the following condition
must hold.
T  h" < U < T C h.ı C "/ :
(2)
Before relaying a message, each processor checks
the acceptance test above and discard the message
if it does not satisfy it. The termination time t of
the protocol for timing failures is as follows.
t D .ı C "/ C dı C " :
(3)
The authors point out that discarding early mes-
sages is not necessary for correctness, but ensures
that correct processors keep messages in their log
for a bounded amount of time.
Atomic Broadcast for Byzantine Failures
Given some text, every processor is assumed to
be able to generate a signature for it, that cannot
be faked by other processors. Furthermore, every
processor knows the name of every other proces-
sors in the network, and has the ability to verify
the authenticity of their signature.
Under the above assumptions, the third pro-
tocol extends the second one by adding signa-
tures to the messages. To prevent a Byzantine
processor (or link) from tampering with the hop
count, a message is co-signed by every processor
that relays it. For instance, a message signed by
k processors p1; : : : ; pk is as follows.
.relayed; : : :.relayed; .ﬁrst; T;; p1; s1/; p2; s2/ ;
: : : pk; sk/
Where ¢ is the update, T the timestamp, p1 the
message source, and si the signature generated
by processor pi. Any message for which one of
the signature cannot be authenticated is simply
discarded. Also, if several updates initiated by
the same processor p carry the same timestamp,
this indicates that p is faulty and the correspond-
ing updates are discarded. The remainder of the
protocol is the same as the second one, where

Atomic Broadcast
159
A
the number of hops is given by the number of
signatures. The termination time b is also as
follows.
b D .ı C "/ C dı C " :
(4)
The authors insist however that, in this case, the
transmission time • must be considerably larger
than in the previous case, since it must account
for the time spent in generating and verifying the
digital signatures; usually a costly operation.
Bounds
In addition to the three protocols presented above
and their correctness, Cristian et al. [7] prove the
following two lower bounds on the termination
time of atomic broadcast protocols.
Theorem 1 If the communication network G
requires x steps, then any atomic broadcast pro-
tocol tolerant of up to  processor and  link
omission failures has a termination time of at
least xı C ".
Theorem 2 Any atomic broadcast protocol for
a Hamiltonian network with n processors that tol-
erate n  2 authentication-detectable Byzantine
processor failures cannot have a termination time
smaller than .n  1/.ı C "/.
Applications
The main motivation for considering this problem
is its use as the cornerstone for ensuring fault-
tolerance through process replication. In particu-
lar, the authors consider a synchronous replicated
storage, which they deﬁne as a distributed and
resilient storage system that displays the same
content at every correct physical processor at any
clock time. Using atomic broadcast to deliver
updates ensures that all updates are applied at
all correct processors in the same order. Thus,
provided that the replicas are initially consis-
tent, they will remain consistent. This technique,
called state-machine replication [11, 13] or also
active replication, is widely used in practice as
a means of supporting fault-tolerance in dis-
tributed systems.
In contrast, Cristian et al. [7] consider atomic
broadcast in a synchronous system with bounded
transmission and processing delays. Their work
was motivated by the implementation of a highly-
available replicated storage system, with tightly
coupled processors running a real-time operating
system.
Atomic broadcast has been used as a support
for the replication of running processes in real-
time systems or, with the problem reformulated
to isolate explicit timing requirements, has also
been used as a support for fault-tolerance and
replication in many group communication toolk-
its (see survey of Chockler et al. [4]).
In
addition,
atomic
broadcast
has
been
used for the replication of database systems,
as a means to reduce the synchronization
between the replicas. Wiesmann and Schiper [15]
have compared different database replication
and transaction processing approaches based
on
atomic
broadcast,
showing
interesting
performance gains.
Cross-References
▷Asynchronous Consensus Impossibility
▷Causal Order, Logical Clocks, State Machine
Replication
▷Clock Synchronization
▷Failure Detectors
Recommended Reading
1. Carr R (1985) The Tandem global update protocol.
Tandem Syst Rev 1:74–85
2. Chandra TD, Toueg S (1996) Unreliable failure
detectors for reliable distributed systems. J ACM
43:225–267
3. Chang J-M, Maxemchuk NF (1984) Reliable broad-
cast protocols. ACM Trans Comput Syst 2:251–273
4. Chockler G, Keidar I, Vitenberg R (2001) Group
communication
speciﬁcations:
a
comprehensive
study. ACM Comput Surv 33:427–469
5. Cristian F (1990) Synchronous atomic broadcast
for redundant broadcast channels. Real-Time Syst
2:195–212

160
Attribute-Efﬁcient Learning
6. Cristian F, Aghili H, Strong R, Dolev D (1985)
Atomic broadcast: from simple message diffusion to
Byzantine agreement. In: Proceedings of the 15th
international symposium on fault-tolerant computing
(FTCS-15), Ann Arbor, June 1985. IEEE Computer
Society Press, pp 200—206
7. Cristian F, Aghili H, Strong R, Dolev D (1995)
Atomic broadcast: from simple message diffusion to
Byzantine agreement. Inform Comput 118:158–179
8. Défago X, Schiper A, Urbán P (2004) Total order
broadcast and multicast algorithms: taxonomy and
survey. ACM Comput Surv 36:372–421
9. Fischer MJ, Lynch NA, Paterson MS (1985) Im-
possibility of distributed consensus with one faulty
process. J ACM 32:374–382
10. Hadzilacos V, Toueg S (1993) Fault-tolerant broad-
casts
and
related
problems.
In:
Mullender
S
(ed) Distributed systems, 2nd edn. ACM Press
Books/Addison-Wesley, pp 97–146, Extended ver-
sion appeared as Cornell Univ. TR 94-1425
11. Lamport L (1978) Time, clocks, and the ordering
of events in a distributed system. Commun ACM
21:558–565
12. Lamport L, Shostak R, Pease M (1982) The Byzan-
tine generals problem. ACM Trans Prog Lang Syst
4:382–401
13. Schneider FB (1990) Implementing fault-tolerant ser-
vices using the state machine approach: a tutorial.
ACM Comput Surv 22:299–319
14. Segall A (1983) Distributed network protocols. IEEE
Trans Inf Theory 29:23–35
15. Wiesmann M, Schiper A (2005) Comparison of
database replication techniques based on total order
broadcast. IEEE Trans Knowl Data Eng 17:551–
566
Attribute-Efﬁcient Learning
Jyrki Kivinen
Department of Computer Science, University of
Helsinki, Helsinki, Finland
Keywords
Learning with irrelevant attributes
Years and Authors of Summarized
Original Work
1987; Littlestone
Problem Deﬁnition
Given here is a basic formulation using the online
mistake-bound model, which was used by Little-
stone [9] in his seminal work.
Fix a class C of Boolean functions over n
variables. To start a learning scenario, a target
function f 2 C is chosen but not revealed to the
learning algorithm. Learning then proceeds in a
sequence of trials. At trial t, an input xt 2 f0; 1gn
is ﬁrst given to the learning algorithm. The learn-
ing algorithm then produces its prediction Oyt,
which is its guess as to the unknown value f.xt/.
The correct value yt D f.xt/ is then revealed to
the learner. If yt ¤ Oyt, the learning algorithm
made a mistake. The learning algorithm learns C
with mistake-bound m, if the number of mistakes
never exceeds m, no matter how many trials are
made and how f and x1; x2; : : : are chosen.
Variable
(or
attribute)
Xi
is
relevant
for function f
W
f0; 1gn
!
f0; 1g if
f .x1; : : : ; xi; : : : ; xn/
¤
f .x1; : : : ; 1

xi; : : : ; xn/ holds for some Ex 2 f0; 1gn. Suppose
now that for some k  n, every function f 2 C
has at most k relevant variables. It is said that
a learning algorithm learns class C attribute-
efﬁciently, if it learns C with a mistake-bound
polynomial in k and log n. Additionally, the
computation time for each trial is usually required
to be polynomial in n.
Key Results
The main part of current research of attribute-
efﬁcient
learning
stems
from
Littlestone’s
Winnow algorithm [9]. The basic version of
Winnow maintains a weight vector wt
D
.wt;1; : : : ; wt;n/ 2 Rn. The prediction for input
xt 2 f0; 1gn is given by
Oyt D sign
 n
X
iD1
wt;ixt;i  
!
where  is a parameter of the algorithm. Initially
w1 D .1; : : : ; 1/, and after trial t, each compo-
nent wt; i is updated according to

Attribute-Efﬁcient Learning
161
A
wtC1;i D
8
<
:
˛wt;i if yt D 1; Oyt D 0 and xt;i D 1
wt;i=˛ if yt D 0; Oyt D 1 and xt;i D 1
wt;i
otherwise
(1)
where ˛ > 1 is a learning rate parameter.
Littlestone’s basic result is that with a suitable
choice of  and ˛, Winnow learns the class
of monotone k-literal disjunctions with mistake-
bound O.k log n/. Since the algorithm changes
its weights only when a mistake occurs, this
bound also guarantees that the weights remain
small enough for computation times to remain
polynomial in n. With simple transformations,
Winnow also yields attribute-efﬁcient learning
algorithms for general disjunctions and conjunc-
tions. Various subclasses of DNF formulas and
decision lists [8] can be learned, too.
Winnow is quite robust against noise, i.e.,
errors in input data. This is extremely impor-
tant for practical applications. Remove now the
assumption about a target function f
2
C
satisfying yt D f.xt/ for all t. Deﬁne attribute
error of a pair .x; y/ with respect to a function
f as the minimum Hamming distance between
x and x0 such that f .x0/ D y. The attribute
error of a sequence of trials with respect to f
is the sum of attribute errors of the individual
pairs .xt; yt/. Assuming the sequence of trials
has attribute error at most A with respect to some
k-literal disjunction, Auer and Warmuth [1] show
that Winnow makes O.ACk log n/ mistakes. The
noisy scenario can also be analyzed in terms of
hinge loss [5].
The update rule (1) has served as a model
for a whole family of multiplicative update al-
gorithms. For example, Kivinen and Warmuth
[7] introduce the exponentiated gradient algo-
rithm, which is essentially Winnow modiﬁed for
continuous-valued prediction, and show how it
can be motivated by a relative entropy minimiza-
tion principle.
Consider a function class C where each func-
tion can be encoded using O.p.k/ log n/ bits
for some polynomial p. An example would be
Boolean formulas with k relevant variables, when
the size of the formula is restricted to p.k/
ignoring the size taken by the variables. The
cardinality of C is then jCj D 2O.p.k/ log n/. The
classical halving algorithm (see [9] for discussion
and references) learns any class consisting of
m Boolean functions with mistake-bound log2 m
and would thus provide an attribute-efﬁcient al-
gorithm for such a class C. However, the running
time would not be polynomial. Another serious
drawback would be that the halving algorithm
does not tolerate any noise. Interestingly, a mul-
tiplicative update similar to (1) has been used
in Littlestone and Warmuth’s weighted majority
algorithm [10], and also Vovk’s aggregating al-
gorithm [14], to produce a noise-tolerant general-
ization of the halving algorithm.
Attribute-efﬁcient learning has also been stud-
ied in other learning models than the mistake-
bound model, such as Probably Approximately
Correct learning [4], learning with uniform dis-
tribution [12], and learning with membership
queries [3]. The idea has been further developed
into learning with a potentially inﬁnite number of
attributes [2].
Applications
Attribute-efﬁcient algorithms for simple function
classes have a potentially interesting application
as a component in learning more complex func-
tion classes. For example, any monotone k-term
DNF formula over variables x1; : : : ; xn can be
represented as a monotone k-literal disjunction
over 2n variables ´A, where ´A is deﬁned as
´A
D
Q
i2A
xi for A  f1; : : : ; ng. Running
Winnow with the transformed inputs ´ 2 f0; 1g2n
would give a mistake-bound O.k log 2n/
D
O(kn). Unfortunately the running time would
be linear in 2n, at least for a naive implemen-
tation. Khardon et al. [6] provide discouraging
computational hardness results for this potential
application.
Online learning algorithms have a natural ap-
plication domain in signal processing. In this
setting, the sender emits a true signal yt at time
t, for t D 1; 2; 3; : : :. At some later time .t C d/,
a receiver receives a signal ´t, which is a sum

162
Automated Search Tree Generation
of the original signal yt and various echoes of
earlier signals yt0, t0 < t, all distorted by random
noise. The task is to recover the true signal yt
based on received signals ´t; ´t1; : : : ; ´tl over
some time window l. Currently attribute-efﬁcient
algorithms are not used for such tasks, but see
[11] for preliminary results.
Attribute-efﬁcient
learning
algorithms
are
similar in spirit to statistical methods that ﬁnd
sparse models. In particular, statistical algorithms
that use L1 regularization are closely related
to multiplicative algorithms such as winnow
and exponentiated gradient. In contrast, more
classical L2 regularization leads to algorithms
that are not attribute-efﬁcient [13].
Cross-References
▷Learning DNF Formulas
Recommended Reading
1. Auer P, Warmuth MK (1998) Tracking the best dis-
junction. Mach Learn 32(2):127–150
2. Blum A, Hellerstein L, Littlestone N (1995) Learning
in the presence of ﬁnitely or inﬁnitely many irrelevant
attributes. J Comput Syst Sci 50(1):32–40
3. Bshouty N, Hellerstein L (1998) Attribute-efﬁcient
learning in query and mistake-bound models. J Com-
put Syst Sci 56(3):310–319
4. Dhagat A, Hellerstein L (1994) PAC learning with ir-
relevant attributes. In: Proceedings of the 35th Annual
Symposium on Foundations of Computer Science,
Santa Fe. IEEE Computer Society, Los Alamitos,
pp 64–74
5. Gentile C, Warmuth MK (1999) Linear hinge loss
and average margin. In: Kearns MJ, Solla SA, Cohn
DA (eds) Advances in Neural Information Processing
Systems, vol 11. MIT, Cambridge, pp 225–231
6. Khardon R, Roth D, Servedio RA (2005) Efﬁciency
versus convergence of boolean kernels for on-line
learning algorithms. J Artif Intell Res 24:341–356
7. Kivinen J, Warmuth MK (1997) Exponentiated gra-
dient versus gradient descent for linear predictors. Inf
Comput 132(1):1–64
8. Klivans AR, Servedio RA (2006) Toward attribute
efﬁcient learning of decision lists and parities. J Mach
Learn Res 7:587–602
9. Littlestone N (1988) Learning quickly when irrele-
vant attributes abound: a new linear threshold algo-
rithm. Mach Learn 2(4):285–318
10. Littlestone
N,
Warmuth
MK
(1994)
The
weighted majority algorithm. Inf Comput 108(2):
212–261
11. Martin RK, Sethares WA, Williamson RC, Johnson
CR Jr (2002) Exploiting sparsity in adaptive ﬁlters.
IEEE Trans Signal Process 50(8):1883–1894
12. Mossel E, O’Donnell R, Servedio RA (2004) Learn-
ing functions of k relevant variables. J Comput Syst
Sci 69(3):421–434
13. Ng AY (2004) Feature selection, L1 vs. L2 regu-
larization, and rotational invariance. In: Greiner R,
Schuurmans D (eds) Proceedings of the 21st Interna-
tional Conference on Machine Learning, Banff. The
International Machine Learning Society, Princeton,
pp 615–622
14. Vovk V (1990) Aggregating strategies. In: Fulk M,
Case J (eds) Proceedings of the 3rd Annual Work-
shop on Computational Learning Theory, Rochester.
Morgan Kaufmann, San Mateo, pp 371–383
Automated Search Tree Generation
Falk Hüffner
Department of Math and Computer Science,
University of Jena, Jena, Germany
Keywords
Automated proofs of upper bounds on the run-
ning time of splitting algorithms
Years and Authors of Summarized
Original Work
2004; Gramm, Guo, Hüffner, Niedermeier
Problem Deﬁnition
This problem is concerned with the automated
development and analysis of search tree algo-
rithms. Search tree algorithms are a popular way
to ﬁnd optimal solutions to NP-complete prob-
lems. (For ease of presentation, only decision
problems are considered; adaption to optimiza-
tion problems is straightforward.) The idea is
to recursively solve several smaller instances in
such a way that at least one branch is a yes-

Automated Search Tree Generation
163
A
instance if and only if the original instance is.
Typically, this is done by trying all possibilities to
contribute to a solution certiﬁcate for a small part
of the input, yielding a small local modiﬁcation
of the instance in each branch.
For
example,
consider
the
NP-complete
CLUSTER EDITING problem: can a given graph
be modiﬁed by adding or deleting up to k edges
such that the resulting graph is a cluster graph,
that is, a graph that is a disjoint union of cliques?
To give a search tree algorithm for CLUSTER
EDITING, one can use the fact that cluster graphs
are exactly the graphs that do not contain a P3
(a path of 3 vertices) as an induced subgraph. One
can thus solve CLUSTER EDITING by ﬁnding
a P3 and splitting it into 3 branches: delete the
ﬁrst edge, delete the second edge, or add the
missing edge. By this characterization, whenever
there is no P3 found, one already has a cluster
graph. The original instance has a solution with
k modiﬁcations if and only if at least one of the
branches has a solution with k  1 modiﬁcations.
Analysis
For NP-complete problems, the running time of
a search tree algorithm only depends on the size
of the search tree up to a polynomial factor ,
which depends on the number of branches and the
reduction in size of each branch. If the algorithm
solves a problem of size s and calls itself recur-
sively for problems of sizes s  d1; : : : ; s  di,
then .d1; : : : ; di/ is called the branching vector
of this recursion. It is known that the size of the
search tree is then O(’s), where the branching
number ’ is the only positive real root of the
characteristic polynomial
´d  ´dd1      ´ddi ;
(1)
where d D maxfd1; : : : ; dig. For the simple
CLUSTER EDITING search tree algorithm and the
size measure k, the branching vector is (1, 1, 1)
and the branching number is 3, meaning that the
running time is up to a polynomial factor O(3k).
Case Distinction
Often, one can obtain better running times by
distinguishing a number of cases of instances,
and giving a specialized branching for each case.
The overall running time is then determined by
the branching number of the worst case. Several
publications obtain such algorithms by hand (e.g.,
a search tree of size O(2.27k) for CLUSTER EDIT-
ING [4]); the topic of this work is how to automate
this. That is, the problem is the following:
Problem 1 (Fast Search Tree Algorithm)
INPUT: An NP-hard problem P and a size mea-
sure s(I) of an instance I of P where instances I
with s.I/ D 0 can be solved in polynomial time.
OUTPUT: A partition of the instance set of P into
cases, and for each case a branching such that the
maximum branching number over all branchings
is as small as possible.
Note that this problem deﬁnition is somewhat
vague; in particular, to be useful, the case an
instance belongs to must be recognizable quickly.
It is also not clear whether an optimal search
tree algorithm exists; conceivably, the branching
number can be continuously reduced by increas-
ingly complicated case distinctions.
Key Results
Gramm et al. [3] describe a method to obtain fast
search tree algorithms for CLUSTER EDITING
and related problems, where the size measure is
the number of editing operations k. To get a case
distinction, a number of subgraphs are enumer-
ated such that each instance is known to contain at
least one of these subgraphs. It is next described
how to obtain a branching for a particular case.
A standard way of systematically obtaining
specialized branchings for instance cases is to use
a combination of basic branching and data re-
duction rules. Basic branching is typically a very
simple branching technique, and data reduction
rules replace an instance with a smaller, solution-
equivalent instance in polynomial time. Applying
this to CLUSTER EDITING ﬁrst requires a small
modiﬁcation of the problem: one considers an
annotated version, where an edge can be marked
as permanent and a non-edge can be marked as
forbidden. Any such annotated vertex pair cannot

164
Automated Search Tree Generation
Automated Search Tree
Generation, Fig. 1
Branching for a CLUSTER
EDITING case using only
basic branching on vertex
pairs (double circles), and
applications of the
reduction rules (asterisks).
Permanent edges are
marked bold, forbidden
edges dashed. The numbers
next to the subgraphs state
the change of the problem
size k. The branching
vector is (1, 2, 3, 3, 2),
corresponding to a search
tree size of O(2.27k)
0
0
1
1
1
2
2
3
3
a
b
d
c
be edited anymore. For a pair of vertices, the
basic branching then branches into two cases:
permanent or forbidden (one of these options will
require an editing operation). The reduction rules
are: if two permanent edges are adjacent, the third
edge of the triangle they induce must also be
permanent; and if a permanent and a forbidden
edge are adjacent, the third edge of the triangle
they induce must be forbidden.
Figure 1 shows an example branching derived
in this way.
Using a reﬁned method of searching the
space for all possible cases and to distinguish
all branchings for a case, Gramm et al. [3] derive
a number of search tree algorithms for graph
modiﬁcation problems.
Applications
Gramm et al. [3] apply the automated genera-
tion of search tree algorithms to several graph
modiﬁcation problems (see also Table 1). Fur-
ther, Hüffner [5] demonstrates an application of
DOMINATING SET on graphs with maximum
degree 4, where the size measure is the size of
the dominating set.
Fedin and Kulikov [2] examine variants of
SAT; however, their framework is limited in that
it only proves upper bounds for a ﬁxed algorithm
instead of generating algorithms.
Skjernaa [6] also presents results on variants
of SAT. His framework does not require user-
provided data reduction rules, but determines
reductions automatically.
Open Problems
The analysis of search tree algorithms can be
much improved by describing the “size” of an
instance by more than one variable, resulting in
multivariate recurrences [1]. It is open to intro-
duce this technique into an automation frame-
work.
It has frequently been reported that better
running time bounds obtained by distinguish-
ing a large number of cases do not necessarily
speed up, but in fact can slow down, a program.
A careful investigation of the tradeoffs involved
and a corresponding adaption of the automation
frameworks is an open task.
Experimental Results
Gramm et al. [3] and Hüffner [5] report search
tree sizes for several NP-complete problems. Fur-
ther, Fedin and Kulikov [2] and Skjernaa [6]

Automated Search Tree Generation
165
A
Automated
Search
Tree
Generation,
Table
1
Summary of search tree sizes where automation gave
improvements. “Known” is the size of the best previously
published “hand-made” search tree. For the satisﬁability
problems, m is the number of clauses and l is the length
of the formula
Problem
Trivial
Known New
CLUSTER EDITING
3
2.27
1.92 [3]
CLUSTER DELETION
2
1.77
1.53 [3]
CLUSTER VERTEX
DELETION
3
2.27
2.26 [3]
BOUNDED DEGREE
DOMINATING SET
4
3.71 [5]
X3SAT, size measure m
3
1.1939
1.1586 [6]
(n, 3)-MAXSAT, size
measure m
2
1.341
1.2366 [2]
(n, 3)-MAXSAT, size
measure l
2
1.1058
1.0983 [2]
report on variants of satisﬁability. Table 1 sum-
marizes the results.
Cross-References
▷Vertex Cover Search Trees
Acknowledgments Partially supported by the Deutsche
Forschungsgemeinschaft, Emmy Noether research group
PIAF (ﬁxed-parameter algorithms), NI 369/4.
Recommended Reading
1. Eppstein D (2004) Quasiconvex analysis of backtrack-
ing algorithms. In: Proceedings of the 15th SODA.
ACM/SIAM, pp 788–797
2. Fedin SS, Kulikov AS (2006) Automated proofs of up-
per bounds on the running time of splitting algorithms.
J Math Sci 134:2383–2391. Improved results at http://
logic.pdmi.ras.ru/~kulikov/autoproofs.html
3. Gramm J, Guo J, Hüffner F, Niedermeier R (2004)
Automated generation of search tree algorithms
for hard graph modiﬁcation problems. Algorithmica
39:321–347
4. Gramm J, Guo J, Hüffner F, Niedermeier R (2005)
Graph-modeled
data
clustering:
exact
algorithms
for clique generation. Theor Comput Syst 38:373–
392
5. Hüffner
F
(2003)
Graph
modiﬁcation
problems
and automated search tree generation. Diplomarbeit,
Wilhelm-Schickard-Institut für Informatik, Universitüt
Tübingen
6. Skjernaa B (2004) Exact algorithms for variants of
satisﬁability and colouring problems. PhD thesis,
Department of Computer Science, University of
Aarhus

