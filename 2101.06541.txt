SceneGen: Learning to Generate Realistic Trafﬁc Scenes
Shuhan Tan1,2*
Kelvin Wong1,3∗
Shenlong Wang1,3
Sivabalan Manivasagam1,3
Mengye Ren1,3
Raquel Urtasun1,3
1Uber Advanced Technologies Group
2Sun Yat-Sen University
3University of Toronto
tanshh@mail2.sysu.edu.cn
{kelvinwong,slwang,manivasagam,mren,urtasun}@cs.toronto.edu
Abstract
We consider the problem of generating realistic trafﬁc
scenes automatically. Existing methods typically insert ac-
tors into the scene according to a set of hand-crafted heuris-
tics and are limited in their ability to model the true com-
plexity and diversity of real trafﬁc scenes, thus inducing a
content gap between synthesized trafﬁc scenes versus real
ones. As a result, existing simulators lack the ﬁdelity neces-
sary to train and test self-driving vehicles. To address this
limitation, we present SceneGen—a neural autoregressive
model of trafﬁc scenes that eschews the need for rules and
heuristics. In particular, given the ego-vehicle state and a
high deﬁnition map of surrounding area, SceneGen inserts
actors of various classes into the scene and synthesizes their
sizes, orientations, and velocities. We demonstrate on two
large-scale datasets SceneGen’s ability to faithfully model
distributions of real trafﬁc scenes. Moreover, we show that
SceneGen coupled with sensor simulation can be used to
train perception models that generalize to the real world.
1. Introduction
The ability to simulate realistic trafﬁc scenarios is an
important milestone on the path towards safe and scalable
self-driving. It enables us to build rich virtual environments
in which we can improve our self-driving vehicles (SDVs)
and verify their safety and performance [9, 31, 32, 53].
This goal, however, is challenging to achieve. As a ﬁrst
step, most large-scale self-driving programs simulate pre-
recorded scenarios captured in the real world [32] or em-
ploy teams of test engineers to design new scenarios [9, 31].
Although this approach can yield realistic simulations, it is
ultimately not scalable. This motivates the search for a way
to generate realistic trafﬁc scenarios automatically.
More concretely, we are interested in generating the lay-
out of actors in a trafﬁc scene given the SDV’s current
state and a high deﬁnition map (HD map) of the surround-
*Indicates equal contribution. Work done at Uber ATG.
Figure 1: Given the SDV’s state and an HD map, SceneGen
autoregressively inserts actors onto the map to compose a
realistic trafﬁc scene. The ego SDV is shown in red; vehi-
cles in blue; pedestrians in orange; and bicyclists in green.
ing area.
We call this task trafﬁc scene generation (see
Fig. 1). Here, each actor is parameterized by a class label,
a bird’s eye view bounding box, and a velocity vector. Our
lightweight scene parameterization is popular among exist-
ing self-driving simulation stacks and can be readily used in
downstream modules; e.g., to simulate LiDAR [9, 10, 32].
A popular approach to trafﬁc scene generation is to use
procedural models to insert actors into the scene accord-
ing to a set of rules [55, 31, 9, 37]. These rules encode
reasonable heuristics such as “pedestrians should stay on
the sidewalk” or “vehicles should drive along lane center-
lines”, and their parameters can be manually tuned to give
reasonable results. Still, these simplistic heuristics cannot
fully capture the complexity and diversity of real world
trafﬁc scenes, thus inducing a content gap between syn-
thesized trafﬁc scenes and real ones [26]. Moreover, this
approach requires signiﬁcant time and expertise to design
good heuristics and tune their parameters.
To address these issues, recent methods use machine
1
arXiv:2101.06541v1  [cs.CV]  16 Jan 2021

learning techniques to automatically tune model parame-
ters [52, 51, 24, 26, 8]. These methods improve the realism
and scalability of trafﬁc scene generation. However, they
remain limited by their underlying hand-crafted heuristics
and priors; e.g., pre-deﬁned scene grammars or assumptions
about road topologies. As a result, they lack the capacity
to model the true complexity and diversity of real trafﬁc
scenes and, by extension, the ﬁdelity necessary to train and
test SDVs in simulation. Alternatively, we can use a simple
data-driven approach by sampling from map-speciﬁc em-
pirical distributions [10]. But this cannot generalize to new
maps and may yield scene-inconsistent samples.
In this paper, we propose SceneGen—a trafﬁc scene gen-
eration model that eschews the need for hand-crafted rules
and heuristics. Our approach is inspired by recent successes
in deep generative modeling that have shown remarkable
results in estimating distributions of a variety of data, with-
out requiring complex rules and heuristics; e.g., handwrit-
ing [18], images [49], text [39], etc. Speciﬁcally, SceneGen
is a neural autoregressive model that, given the SDV’s cur-
rent state and an HD map of the surrounding area, sequen-
tially inserts actors into the scene—mimicking the process
by which humans do this as well. As a result, we can sam-
ple realistic trafﬁc scenes from SceneGen and compute the
likelihood of existing ones as well.
We evaluate SceneGen on two large-scale self-driving
datasets. The results show that SceneGen can better esti-
mate the distribution over real trafﬁc scenes than compet-
ing baselines and generate more realistic samples as well.
Furthermore, we show that SceneGen coupled with sensor
simulation can generate realistic labeled data to train per-
ception models that generalize to the real world. With Sce-
neGen, we take an important step towards developing SDVs
safely and scalably through large-scale simulation. We hope
our work here inspires more research along this direction so
that one day this goal will become a reality.
2. Related Work
Trafﬁc simulation:
The study of trafﬁc simulation can be
traced back to at least the 1950s with Gerlough’s disserta-
tion on simulating freeway trafﬁc ﬂow [16]. Since then, var-
ious trafﬁc models have been used for simulation. Macro-
scopic models simulate entire populations of vehicles in
the aggregate [30, 40] to study “macroscopic” properties of
trafﬁc ﬂow, such as trafﬁc density and average velocity. In
contrast, microscopic models simulate the behavior of each
individual vehicle over time by assuming a car-following
model [36, 6, 34, 13, 17, 1, 44]. These models improve
simulation ﬁdelity considerably but at the cost of compu-
tational efﬁciency. Microscopic trafﬁc models have been
included in popular software packages such as SUMO [31],
CORSIM [35], VISSIM [11], and MITSIM [55].
Recently, trafﬁc simulation has found new applications
in testing and training the autonomy stack of SDVs. How-
ever, existing simulators do not satisfy the level of realism
necessary to properly test SDVs [52].
For example, the
CARLA simulator [9] spawns actors at pre-determined lo-
cations and uses a lane-following controller to simulate the
vehicle behaviors over time. This approach is too simplistic
and so it induces a sim2real content gap [26]. Therefore,
in this paper, we study how to generate snapshots of trafﬁc
scenes that mimic the realism and diversity of real ones.
Trafﬁc scene generation:
While much of the research
into microscopic trafﬁc simulation have focused on mod-
eling actors’ behaviors, an equally important yet underex-
plored problem is how to generate realistic snapshots of
trafﬁc scenes.
These snapshots have many applications;
e.g., to initialize trafﬁc simulations [52] or to generate la-
beled data for training perception models [26]. A popular
approach is to procedurally insert actors into the scene ac-
cording to a set of rules [55, 31, 9, 37]. These rules encode
reasonable heuristics such as “pedestrians should stay on
the sidewalk” and “vehicles should drive along lane center-
lines”, and their parameters can be manually tuned to give
reasonable results. For example, SUMO [31] inserts ve-
hicles into lanes based on minimum headway requirements
and initializes their speeds according to a Gaussian distribu-
tion [52]. Unfortunately, it is difﬁcult to scale this approach
to new environments since tuning these heuristics require
signiﬁcant time and expertise.
An alternative approach is to learn a probabilistic distri-
bution over trafﬁc scenes from which we can sample new
scenes [52, 51, 24, 10, 14, 15, 57]. For example, Wheeler et
al. [52] propose a Bayesian network to model a joint dis-
tribution over trafﬁc scenes in straight multi-lane highways.
This approach was extended to model inter-lane dependen-
cies [51] and generalized to handle a four-way intersec-
tion [24]. These models are trained to mimic a real distri-
bution over trafﬁc scenes. However, they consider a limited
set of road topologies only and assume that actors follow
reference paths in the map. As a result, they are difﬁcult
to generalize to real urban scenes, where road topologies
and actor behaviors are considerably more complex; e.g.,
pedestrians do not follow reference paths in general.
Recent advances in deep learning have enabled a more
ﬂexible approach to learn a distribution over trafﬁc scenes.
In particular, MetaSim [26] augments the probabilistic
scene graph of Prakash et al. [37] with a graph neural
network. By modifying the scene graph’s node attributes,
MetaSim reduces the content gap between synthesized im-
ages versus real ones, without manual tuning. MetaSim2 [8]
extends this idea by learning to sample the scene graph as
well. Unfortunately, these approaches are still limited by
their hand-crafted scene grammar which, for example, con-
strains vehicles to lane centerlines. We aim to develop a
2

Sample
Box
Class
Velocity
Position
ci
pi
bi
vi
Traffic Scene
Step 1
Autoregressive Traffic Scene Generation (Sec. 3.1)
Probabilistic Actor Model (Sec. 3.2)
Traffic Scene
Step 2
Traffic Scene
Step 3
Sample
Stop
New Actor
ConvLSTM
ConvLSTM
ConvLSTM
ConvLSTM
ConvLSTM
ConvLSTM
Figure 2: Overview of our approach. Given the ego SDV’s state and an HD map of the surrounding area, SceneGen generates
a trafﬁc scene by inserting actors one at a time (Sec. 3.1). We model each actor ai ∈A probabilistically, as a product over
distributions of its class ci ∈C, position pi ∈R2, bounding box bi ∈B, and velocity vi ∈R2 (Sec. 3.2).
more general method that avoids requiring these heuristics.
Autoregressive models:
Autoregressive models factorize
a joint distribution over n-dimensions into a product of con-
ditional distributions p(x) = Qn
i=1 p(xi|x<i). Each condi-
tional distribution is then approximated with a parameter-
ized function [12, 2, 45, 46, 47]. Recently, neural autore-
gressive models have found tremendous success in mod-
eling a variety of data, including handwriting [18], im-
ages [49], audio [48], text [39], sketches [20], graphs [29],
3D meshes [33], indoor scenes [50] and image scene lay-
outs [25]. These models are particularly popular since they
can factorize a complex joint distribution into a product
of much simpler conditional distributions. Moreover, they
generally admit a tractable likelihood, which can be used
for likelihood-based training, uncovering interesting/outlier
examples, etc. Inspired by these advances, we exploit au-
toregressive models for trafﬁc scene generation as well.
3. Trafﬁc Scene Generation
Our goal is to learn a distribution over trafﬁc scenes from
which we can sample new examples and evaluate the likeli-
hood of existing ones. In particular, given the SDV a0 ∈A
and an HD map m ∈M, we aim to estimate the joint dis-
tribution over other actors in the scene {a1, . . . , an} ⊂A,
p(a1, . . . , an|m, a0)
(1)
The HD map m ∈M is a collection of polygons and
polylines that provide semantic priors for a region of inter-
est around the SDV; e.g., lane boundaries, drivable areas,
trafﬁc light states. These priors provide important contex-
tual information about the scene and allow us to generate
actors that are consistent with the underlying road topology.
We parameterize each actor ai ∈A with an eight-
dimensional random variable containing its class label ci ∈
C, its bird’s eye view location (xi, yi) ∈R2, its bounding
box bi ∈B1, and its velocity vi ∈R2. Each bounding box
bi ∈B is a 3-tuple consisting of the bounding box’s size
(wi, li) ∈R2
>0 and heading angle θi ∈[0, 2π). In our ex-
periments, C consists of three classes: vehicles, pedestrians,
and bicyclists. See Fig. 1 for an example.
Modeling Eq. 1 is a challenging task since the actors in
a given scene are highly correlated among themselves and
with the map, and the number of actors in the scene is ran-
dom as well. We aim to model Eq. 1 such that our model
is easy to sample from and the resulting samples reﬂect
the complexity and diversity of real trafﬁc scenes. Our ap-
proach is to autoregressively factorize Eq. 1 into a product
of conditional distributions. This yields a natural generation
process that sequentially inserts actors into the scene one at
a time. See Fig. 2 for an overview of our approach.
In the following, we ﬁrst describe our autoregressive fac-
torization of Eq. 1 and how we model this with a recurrent
neural network (Sec. 3.1). Then, in Sec. 3.2, we describe
how SceneGen generates a new actor at each step of the
generation process. Finally, in Sec. 3.3, we discuss how we
train and sample from SceneGen.
3.1. The Autoregressive Generation Process
Given the SDV a0 ∈A and an HD map m ∈M, our
goal is to estimate a conditional distribution over the ac-
tors in the scene {a1, . . . , an} ⊂A. As we alluded to
earlier, modeling this conditional distribution is challenging
since the actors in a given scene are highly correlated among
themselves and with the map, and the number of actors in
the scene is random. Inspired by the recent successes of
neural autoregressive models [18, 49, 39], we propose to au-
toregressively factorize p(a1, . . . , an|m, a0) into a product
1Pedestrians are not represented by bounding boxes. They are repre-
sented by a single point indicating their center of gravity.
3

Figure 3: Trafﬁc scenes generated by SceneGen conditioned on HD maps from ATG4D (top) and Argoverse (bottom).
of simpler conditional distributions. This factorization sim-
pliﬁes the task of modeling the complex joint distribution
p(a1, . . . , an|m, a0) and results in a model with a tractable
likelihood. Moreover, it yields a natural generation process
that mimics how a human might perform this task as well.
In order to perform this factorization, we assume a ﬁxed
canonical ordering over the sequence of actors a1, . . . , an,
p(a1, . . . , an|m, a0) = p(a1|m, a0)
n
Y
i=1
p(ai|a<i, m, a0)
(2)
where a<i = {a1, . . . , ai−1} is the set of actors up to and
including the i−1-th actor in canonical order. In our exper-
iments, we choose a left-to-right, top-to-bottom order based
on each actor’s position in bird’s eye view coordinates. We
found that this intuitive ordering works well in practice.
Since the number of actors per scene is random, we in-
troduce a stopping token ⊥to indicate the end of our se-
quential generation process. In practice, we treat ⊥as an
auxillary actor that, when generated, ends the generation
process. Therefore, for simplicity of notation, we assume
that the last actor an is always the stopping token ⊥.
Model architecture:
Our model uses a recurrent neural
network to capture the long-range dependencies across our
autoregressive generation process. The basis of our model is
the ConvLSTM architecture [42]—an extension of the clas-
sic LSTM architecture [22] to spatial data—and the input
to our model at the i-th generation step is a bird’s eye view
multi-channel image encoding the SDV a0, the HD map m,
and the actors generated so far {a1, . . . , ai−1}.
For the i-th step of the generation process: Let x(i) ∈
RC×H×W denote the multi-channel image, where C is the
number of feature channels and H × W is the size of the
image grid. Given the previous hidden and cell states h(i−1)
and c(i−1), the new hidden and cell states are given by:
h(i), c(i) = ConvLSTM(x(i), h(i−1), c(i−1); w)
(3)
f (i) = CNNb(h(i); w)
(4)
where ConvLSTM is a two-layer ConvLSTM, CNNb is a
ﬁve-layer convolutional neural network (CNN) that extract
backbone features, and w are the neural network parame-
ters. The features f (i) summarize the generated scene so
far a<i, a0, and m, and we use f (i) to predict the con-
ditional distribution p(ai|a<i, m, a0), which we describe
next. See our appendix for details.
3.2. A Probabilistic Model of Actors
Having speciﬁed the generation process, we now turn
our attention to modeling each actor probabilistically. As
discussed earlier, each actor ai ∈A is parameterized by
its class label ci ∈C, location (xi, yi) ∈R2, oriented
bounding box bi ∈B and velocity vi ∈R2.
To cap-
ture the dependencies between these attributes, we factorize
p(ai|a<i, m, a0) as follows:
p(ai) = p(ci)p(xi, yi|ci)p(bi|ci, xi, yi)p(vi|ci, xi, yi, bi)
(5)
where we dropped the condition on a<i, m, and a0 to sim-
plify notation. Thus, the distribution over an actor’s location
is conditional on its class; its bounding box is conditional
on its class and location; and its velocity is conditional on
its class, location, and bounding box. Note that if ai is the
stopping token ⊥, we do not model its location, bounding
box, and velocity. Instead, we have p(ai) = p(ci), where
ci is the auxillary class c⊥.
4

SceneGen
MetaSim
Lane Graph
LayoutVAE
Argo
Figure 4: Qualitative comparison of trafﬁc scenes generated by SceneGen and various baselines.
Class:
To model a distribution over an actor’s class, we
use a categorical distribution whose support is the set of
classes C∪{c⊥} and whose parameters πc are predicted by
a neural network:
πc = MLPc(avg-pool(f (i)); w)
(6)
ci ∼Categorical(πc)
(7)
where avg-pool: RC×H×W →RC is average pooling over
the spatial dimensions and MLPc is a three-layer multi-
layer perceptron (MLP) with softmax activations.
Location:
We apply uniform quantization to the actor’s
position and model the quantized values using a categorical
distribution. The support of this distribution is the set of
H × W quantized bins within our region of interest and
its parameters πloc are predicted by a class-speciﬁc CNN.
This approach allows the model to express highly multi-
modal distributions without making assumptions about the
distribution’s shape [49]. To recover continuous values, we
assume a uniform distribution within each quantization bin.
Let k denote an index into one of the H × W quantized
bins, and suppose ⌊pk⌋∈R2 (resp., ⌈pk⌉∈R2) is the
minimum (resp., maximum) continuous coordinates in the
k-th bin. We model p(xi, yi|ci) as follows:
πloc = CNNloc(f (i); ci, w)
(8)
k ∼Categorical(πloc)
(9)
(xi, yi) ∼Uniform(⌊pk⌋, ⌈pk⌉)
(10)
where CNNloc(·; ci, w) is a CNN with softmax activa-
tions for the class ci. During inference, we mask and re-
normalize πloc such that quantized bins with invalid posi-
tions according to our canonical ordering have zero proba-
bility mass. Note that we do not mask during training since
this resulted in worse performance.
After sampling the actor’s location (xi, yi) ∈R2, we
extract a feature vector f (i)
xi,yi ∈RC by spatially indexing
into the k-th bin of f (i). This feature vector captures local
information at (xi, yi) and is used to subsequently predict
the actor’s bounding box and velocity.
Bounding box:
An actor’s bounding box bi ∈B con-
sists of its width and height (wi, li) ∈R2
>0 and its heading
θi ∈[0, 2π). We model the distributions over each of these
independently. For an actor’s bounding box size, we use a
mixture of K bivariate log-normal distributions:
[πbox, µbox, Σbox] = MLPbox(f (i)
xi,yi; ci, w)
(11)
k ∼Categorical(πbox)
(12)
(wi, li) ∼LogNormal(µbox,k, Σbox,k) (13)
where πbox are mixture weights, each µbox,k ∈R2 and
Σbox,k ∈S2
+ parameterize a component log-normal distri-
bution, and MLPbox(·; ci, w) is a three-layer MLP for the
class ci. This parameterization allows our model to natu-
rally capture the multi-modality of actor sizes in real world
data; e.g., the size of sedans versus trucks.
Similarly, we model an actor’s heading angle with a mix-
ture of K Von-Mises distributions:
[πθ, µθ, κθ] = MLPθ(f (i)
xi,yi; ci, w)
(14)
k ∼Categorical(πθ)
(15)
θi ∼VonMises(µθ,k, κθ,k)
(16)
where πθ are mixture weights, each µθ,k ∈[0, 2π) and
κθ,k > 0 parameterize a component Von-Mises distribu-
5

ATG4D
Argoverse
Method
NLL
Features
Class
Size
Speed
Heading
NLL
Features
Class
Size
Speed
Heading
Prob. Grammar
-
0.20
0.24
0.46
0.34
0.31
-
0.38
0.26
0.41
0.57
0.38
MetaSim
-
0.12
0.24
0.45
0.35
0.15
-
0.18
0.26
0.50
0.52
0.18
Procedural
-
0.38
0.24
0.17
0.34
0.07
-
0.58
0.26
0.23
0.59
0.17
Lane Graph
-
0.17
0.24
0.30
0.32
0.16
-
0.11
0.26
0.31
0.32
0.28
LayoutVAE
210.80
0.15
0.12
0.18
0.33
0.29
200.78
0.25
0.11
0.21
0.41
0.29
SceneGen
59.86
0.11
0.20
0.06
0.27
0.08
67.11
0.14
0.21
0.17
0.17
0.21
Table 1: Negative log-likelihood (NLL) and maximum mean discrepency (MMD) results on ATG4D and Argoverse. NLL is
reported in nats, averaged across all scenes in the test set. MMD is computed between distributions of features extracted by
a motion forecasting model and various scene statistics (see main text for description). For all metrics, lower is better.
tion, and MLPθ(·; ci, w) is a three-layer MLP for the class
ci. The Von-Mises distribution is a close approximation of a
normal distribution wrapped around the unit circle [38] and
has the probability density function
p(θ|µ, κ) = eκ cos(θ−µ)
2πI0(κ)
(17)
where I0 is the modiﬁed Bessel function of order 0. We use
a mixture of Von-Mises distributions to capture the multi-
modality of headings in real world data; e.g., a vehicle can
go straight or turn at an intersection. To sample from a mix-
ture of Von-Mises distributions, we sample a component k
from a categorical distribution and then sample θ from the
Von-Mises distribution of the k-th component [3].
Velocity:
We parameterize the actor’s velocity vi ∈R2
as vi = (si cos ωi, si sin ωi), where si ∈R≥0 is its speed
and ωi ∈[0, 2π) is its direction. Note that this parameter-
ization is not unique since ωi can take any value in [0, 2π)
when vi = 0. Therefore, we model the actor’s velocity as a
mixture model where one of the K ≥2 components corre-
sponds to vi = 0. More concretely, we have
πv = MLPv(f (i)
xi,yi; ci, w)
(18)
k ∼Categorical(πv)
(19)
where for k > 0, we have vi = (si cos ωi, si sin ωi), with
[µs, σs] = MLPs(f (i)
xi,yi; ci, w)
(20)
[µω, κω] = MLPω(f (i)
xi,yi; ci, w)
(21)
si ∼LogNormal(µs,k, σs,k)
(22)
ωi ∼VonMises(µω,k, κω,k)
(23)
and for k = 0, we have vi = 0. As before, we use three-
layer MLPs to predict the parameters of each distribution.
For vehicles and bicyclists, we parameterize ωi ∈[0, 2π)
as an offset relative to the actor’s heading θi ∈[0, 2π). This
is equivalent to parameterizing their velocities with a bicy-
cle model [43], which we found improves sample quality.
3.3. Learning and Inference
Sampling:
Pure sampling from deep autoregressive mod-
els can lead to degenerate examples due to their “unreali-
able long tails” [23]. Therefore, we adopt a sampling strat-
egy inspired by nucleus sampling [23]. Speciﬁcally, at each
generation step, we sample from each of SceneGen’s out-
put distributions M times and keep the most likely sample.
We found this to help avoid degenerate trafﬁc scenes while
maintaining sample diversity. Furthermore, we reject vehi-
cles and bicyclists whose bounding boxes collide with those
of the actors sampled so far.
Training:
We train our model to maximize the log-
likelihood of real trafﬁc scenes in our training dataset:
w⋆= arg max
w
N
X
i=1
log p(ai,1, . . . , ai,n|mi, ai,0; w)
(24)
where w are the neural network parameters and N is the
number of samples in our training set. In practice, we use
the Adam optimizer [27] to minimize the average negative
log-likelihood over mini-batches. We use teacher forcing
and backpropagation-through-time to train through the gen-
eration process, up to a ﬁxed window as memory allows.
4. Experiments
We evaluate SceneGen on two self-driving datasets: Ar-
goverse [7] and ATG4D [54]. Our results show that Sce-
neGen can generate more realistic trafﬁc scenes than the
competing methods (Sec. 4.3). We also demonstrate how
SceneGen with sensor simulation can be used to train per-
ception models that generalize to the real world (Sec. 4.4).
4.1. Datasets
ATG4D:
ATG4D [54] is a large-scale dataset collected by
a ﬂeet of SDVs in cities across North America. It consists
6

# Mixtures
Scene
Vehicle
Pedestrian
Bicyclist
1
125.97
7.26
10.36
9.16
3
68.22
2.64
8.52
7.34
5
64.05
2.35
8.27
7.22
10
59.86
1.94
8.32
6.90
Table 2: Ablation of the number of mixture components in
ATG4D. Scene NLL is averaged across scenes and NLL per
class is the average NLL per actor of that class.
L
DA
C
TL
Scene
Veh.
Ped.
Bic.
93.73
4.90
8.85
7.17
✓
63.33
2.12
8.69
7.10
✓
✓
57.66
1.73
8.40
6.84
✓
✓
✓
57.96
1.77
8.32
6.61
✓
✓
✓
✓
59.86
1.94
8.32
6.90
Table 3: Ablation of map in ATG4D (in NLL). L is lanes;
DA drivable areas; C crosswalks; and TL trafﬁc lights.
of 5500 25-seconds logs which we split into a training set of
5000 and an evaluation set of 500. Each log is subsampled
at 10Hz to yield 250 trafﬁc scenes, and each scene is an-
notated with bounding boxes for vehicles, pedestrians, and
bicyclists. Each log also provides HD maps that encode lane
boundaries, drivable areas, and crosswalks as polygons, and
lane centerlines as polylines. Each lane segment is anno-
tated with attributes such as its type (car vs. bike), turn di-
rection, boundary colors, and trafﬁc light state.
In our experiments, we subdivide the training set into
two splits of 4000 and 1000 logs respectively. We use the
ﬁrst split to train the trafﬁc scene generation models and the
second split to train the perception models in Sec. 4.4.
Argoverse:
Argoverse [7] consists of two datasets col-
lected by a ﬂeet of SDVs in Pittsburgh and Miami. We use
the Argoverse 3D Tracking dataset which contains track an-
notations for 65 training logs and 24 validation logs. Each
log is subsampled at 10Hz to yield 13,122 training scenes
and 5015 validation scenes. As in ATG4D, Argoverse pro-
vides HD maps annotated with drivable areas and lane seg-
ment centerlines and their attributes; e.g., turn direction.
However, Argoverse does not provide crosswalk polygons,
lane types, lane boundary colors, and trafﬁc lights.
4.2. Experiment Setup
Baselines:
Our ﬁrst set of baselines is inspired by recent
work on probabilistic scene grammars and graphs [37, 26,
8]. In particular, we design a probabilistic grammar of traf-
ﬁc scenes (Prob. Grammar) such that actors are randomly
placed onto lane segments using a hand-crafted prior [37].
Sampling from this grammar yields a scene graph, and our
next baseline (MetaSim) uses a graph neural network to
transform the attributes of each actor in the scene graph.
Our implementation follows Kar et al. [26], except we use a
training algorithm that is supervised with heuristically gen-
erated ground truth scene graphs.2
Our next set of baselines is inspired by methods that rea-
son directly about the road topology of the scene [52, 51,
24, 32]. Given a lane graph of the scene, Procedural uses
a set of rules to place actors such that they follow lane cen-
terlines, maintain a minimum clearance to leading actors,
etc. Each actor’s bounding box is sampled from a Gaussian
KDE ﬁtted to the training dataset [5] and velocities are set
to satisfy speed limits and a time gap between successive
actors on the lane graph. Similar to MetaSim, we also con-
sider a learning-based version of Procedural that uses a lane
graph neural network [28] to transform each actor’s posi-
tion, bounding box, and velocity (Lane Graph).
Since the HD maps in ATG4D and Argoverse do not
provide reference paths for pedestrians, the aforementioned
baselines cannot generate pedestrians.3 Therefore, we also
compare against LayoutVAE [25]—a variational autoen-
coder for image layouts that we adapt for trafﬁc scene gen-
eration. We modify LayoutVAE to condition on HD maps
and output oriented bounding boxes and velocities for ac-
tors of every class. Please see our appendix for details.
Metrics:
Our ﬁrst metric measures the negative log-
likelihood (NLL) of real trafﬁc scenes from the evalua-
tion distribution, measured in nats.
NLL is a standard
metric to compare generative models with tractable likeli-
hoods. However, as many of our baselines do not have like-
lihoods, we compute a sample-based metric as well: maxi-
mum mean discrepancy (MMD) [19]. For two distributions
p and q, MMD measures a distance between p and q as
MMD2(p, q) = Ex,x′∼p[k(x, x′)]
+Ey,y′∼q[k(y, y′)] −2Ex∼p,y∼q[k(x, y)]
(25)
for some kernel k. Following [56, 29], we compute MMD
using Gaussian kernels with the total variation distance to
compare distributions of scene statistics between generated
and real trafﬁc scenes. Our scene statistics measure the dis-
tribution of classes, bounding box sizes, speeds, and head-
ing angles (relative to the SDV) in each scene. To peer into
the global properties of the trafﬁc scene, we also compute
MMD in the feature space of a pre-trained motion forecast-
ing model that takes a rasterized image of the scene as in-
put [53]. This is akin to the popular IS [41], FID [21], and
KID [4] metrics for evaluating generative models, except
we use a feature extractor trained on trafﬁc scenes. Please
see our appendix for details.
2We were unable to train MetaSim using their unsupervised losses.
3In Argoverse, these baselines generate vehicles only since bike lanes
are not given. This highlights the challenge of designing good heuristics.
7

Figure 5: ATG4D scene with a trafﬁc violation.
Additional details:
Each trafﬁc scene is a 80m × 80m
region of interest centered on the ego SDV. By default, Sce-
neGen uses K = 10 mixture components and conditions on
all available map elements for each dataset. We train Sce-
neGen using the Adam optimizer [27] with a learning rate
of 1e−4 and a batch size of 16, until convergence. When
sampling each actor’s position, heading, and velocity, we
sample M = 10 times and keep the most likely sample.
4.3. Results
Quantitative results:
Tab. 1 summarizes the NLL and
MMD results for ATG4D and Argoverse. Overall, Scene-
Gen achieves the best results across both datasets, demon-
strating that it can better model real trafﬁc scenes and
synthesize realistic examples as well.
Interestingly, all
learning-based methods outperform the hand-tuned base-
lines with respect to MMD on deep features—a testament
to the difﬁculty of designing good heuristics.
Qualitative results:
Fig. 3 visualizes samples generated
by SceneGen on ATG4D and Argoverse. Fig. 4 compares
trafﬁc scenes generated by SceneGen and various base-
lines. Although MetaSim and Lane Graph generate rea-
sonable scenes, they are limited by their underlying heuris-
tics; e.g., actors follow lane centerlines. LayoutVAE gener-
ates a greater variety of actors; however, the model does
not position actors on the map accurately, rendering the
overall scene unrealistic. In contrast, SceneGen’s samples
reﬂects the complexity of real trafﬁc scenes much better.
That said, SceneGen occassionally generates near-collision
scenes that are plausible but unlikely; e.g., Fig. 3 top-right.
Ablation studies:
In Tab. 2, we sweep over the number of
mixture components used to parameterize distributions of
bounding boxes and velocities. We see that increasing the
number of components consistently lowers NLL, reﬂecting
the need to model the multi-modality of real trafﬁc scenes.
We also ablate the input map to SceneGen: starting from
an unconditional model, we progressively add lanes, driv-
able areas, crosswalks, and trafﬁc light states. From Tab. 3,
we see that using more map elements generally improves
NLL. Surprisingly, incorporating trafﬁc lights slightly de-
grades performance, which we conjecture is due to infre-
quent trafﬁc light observations in ATG4D.
Vehicle
Pedestrian
Bicyclist
Method
0.5
0.7
0.3
0.5
0.3
0.5
Prob. Gram.
81.1
66.6
-
-
11.2
10.6
MetaSim
76.3
63.3
-
-
8.2
7.5
Procedural
80.2
63.0
-
-
6.5
3.8
Lane Graph
82.9
71.7
-
-
7.6
6.9
LayoutVAE
85.9
76.3
49.3
41.8
18.4
16.4
SceneGen
90.4
82.4
58.1
48.7
19.6
17.9
Real Scenes
93.7
86.7
69.3
61.6
29.2
25.9
Table 4: Detection AP on real ATG4D scenes.
Figure 6: Outputs of detector trained with SceneGen scenes.
Discovering interesting scenes:
We use SceneGen to
ﬁnd unlikely scenes in ATG4D by searching for scenes with
the highest NLL, normalized by the number of actors. Fig. 5
shows an example of a trafﬁc violation found via this pro-
cedure; the violating actor has an NLL of 21.28.
4.4. Sim2Real Evaluation
Our next experiment demonstrates that SceneGen cou-
pled with sensor simulation can generate realistic labeled
data for training perception models. For each method under
evaluation, we generate 250,000 trafﬁc scenes conditioned
on the SDV and HD map in each frame of the 1000 held-out
logs in ATG4D. Next, we use LiDARsim [32] to simulate
the LiDAR point cloud corresponding to each scene. Fi-
nally, we train a 3D object detector [54] using the simulated
LiDAR and evaluate its performance on real scenes and Li-
DAR in ATG4D.
From Tab. 4, we see that SceneGen’s trafﬁc scenes ex-
hibit the lowest sim2real gap. Here, Real Scenes is sim-
ulated LiDAR from ground truth placements. This reaf-
ﬁrms our claim that the underlying rules and priors used in
MetaSim and Lane Graph induce a content gap. By eschew-
ing these heuristics altogether, SceneGen learns to generate
signiﬁcantly more realistic trafﬁc scenes. Intriguingly, Lay-
outVAE performs competitively despite struggling to posi-
tion actors on the map. We conjecture that this is because
LayoutVAE captures the diversity of actor classes, sizes,
headings, etc. well. However, by accurately modeling ac-
tor positions as well, SceneGen further reduces the sim2real
gap, as compared to ground truth trafﬁc scenes.
8

5. Conclusion
We have presented SceneGen—a neural autoregressive
model of trafﬁc scenes from which we can sample new ex-
amples as well as evaluate the likelihood of existing ones.
Unlike prior methods, SceneGen eschews the need for rules
or heuristics, making it a more ﬂexible and scalable ap-
proach for modeling the complexity and diversity of real
world trafﬁc scenes. As a result, SceneGen is able to gen-
erate realistic trafﬁc scenes, thus taking an important step
towards safe and scalable self-driving.
References
[1] M. Bando, K. Hasebe, A. Nakayama, A. Shibata, and Y.
Sugiyama. Dynamical model of trafﬁc congestion and nu-
merical simulation. Physical Review E, 1995. 2
[2] Yoshua Bengio and Samy Bengio.
Modeling high-
dimensional discrete data with multi-layer neural networks.
In NeurIPS, 1999. 3
[3] Donald Best and Nicholas Fisher.
Efﬁcient simulation of
the von mises distribution. Journal of the Royal Statistical
Society. Series C. Applied Statistics, 1979. 6
[4] Mikolaj Binkowski, Dougal J. Sutherland, Michael Arbel,
and Arthur Gretton. Demystifying MMD gans. In ICLR,
2018. 7
[5] Christopher M. Bishop. Pattern Recognition and Machine
Learning. 2006. 7
[6] Robert E. Chandler, Robert Herman, and Elliott W. Montroll.
Trafﬁc dynamics: Studies in car following. In Operations
Research, 1958. 2
[7] Ming-Fang Chang, John Lambert, Patsorn Sangkloy, Jag-
jeet Singh, Slawomir Bak, Andrew Hartnett, De Wang, Peter
Carr, Simon Lucey, Deva Ramanan, and James Hays. Argo-
verse: 3d tracking and forecasting with rich maps. In CVPR,
2019. 6, 7
[8] Jeevan Devaranjan, Amlan Kar, and Sanja Fidler.
Meta-
sim2: Unsupervised learning of scene structure for synthetic
data generation. 2020. 2, 7
[9] Alexey Dosovitskiy, Germ´an Ros, Felipe Codevilla, Anto-
nio M. L´opez, and Vladlen Koltun. CARLA: an open urban
driving simulator. In CoRL, 2017. 1, 2
[10] Jin Fang, Dingfu Zhou, Feilong Yan, Tongtong Zhao, Feihu
Zhang, Yu Ma, Liang Wang, and Ruigang Yang. Augmented
lidar simulator for autonomous driving, 2019. 1, 2
[11] Martin Fellendorf. Vissim: A microscopic simulation tool to
evaluate actuated signal control including bus priority. 1994.
2
[12] Brendan J. Frey, Geoffrey E. Hinton, and Peter Dayan. Does
the wake-sleep algorithm produce good density estimators?
In NeurIPS, 1995. 3
[13] Denos C. Gazis, Robert Herman, and Richard W. Rothery.
Nonlinear follow-the-leader models of trafﬁc ﬂow. In Oper-
ations Research, 1961. 2
[14] Andreas Geiger, Martin Lauer, and Raquel Urtasun. A gener-
ative model for 3d urban scene understanding from movable
platforms. In CVPR, 2011. 2
[15] Andreas Geiger, Christian Wojek, and Raquel Urtasun. Joint
3d estimation of objects and scene layout. In NeurIPS, 2011.
2
[16] Daniel Gerlough.
Simulation of Freeway Trafﬁc on a
General-purpose Discrete Variable Computer. 1955. 2
[17] Peter Gipps. Computer program multsim for simulating out-
put from vehicle detectors on a multi-lane signal-controlled
road. 1976. 2
[18] Alex Graves.
Generating sequences with recurrent neural
networks. CoRR, 2013. 2, 3
[19] Arthur Gretton, Karsten M. Borgwardt, Malte J. Rasch,
Bernhard Sch¨olkopf, and Alexander J. Smola. A kernel two-
sample test. JMLR, 2012. 7
[20] David Ha and Douglas Eck.
A neural representation of
sketch drawings. In ICLR, 2018. 3
[21] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,
Bernhard Nessler, and Sepp Hochreiter. Gans trained by a
two time-scale update rule converge to a local nash equilib-
rium. In NeurIPS, 2017. 7
[22] Sepp Hochreiter and J¨urgen Schmidhuber. Long short-term
memory. Neural Computation, 1997. 4
[23] Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin
Choi. The curious case of neural text degeneration. In ICLR,
2020. 6
[24] Stefan Jesenski, Jan Erik Stellet, Florian A. Schiegg, and
J. Marius Z¨ollner. Generation of scenes in intersections for
the validation of highly automated driving functions. In IV,
2019. 2, 7
[25] Akash Abdu Jyothi, Thibaut Durand, Jiawei He, Leonid Si-
gal, and Greg Mori. Layoutvae: Stochastic scene layout gen-
eration from a label set. In ICCV, 2019. 3, 7
[26] Amlan Kar, Aayush Prakash, Ming-Yu Liu, Eric Cameracci,
Justin Yuan, Matt Rusiniak, David Acuna, Antonio Torralba,
and Sanja Fidler. Meta-sim: Learning to generate synthetic
datasets. In ICCV, 2019. 1, 2, 7
[27] Diederik P. Kingma and Jimmy Ba. Adam: A method for
stochastic optimization. In ICLR, 2015. 6, 8
[28] Ming Liang, Bin Yang, Rui Hu, Yun Chen, Renjie Liao, Song
Feng, and Raquel Urtasun. Learning lane graph representa-
tions for motion forecasting. In ECCV, 2020. 7
[29] Renjie Liao,
Yujia Li,
Yang Song,
Shenlong Wang,
William L. Hamilton, David Duvenaud, Raquel Urtasun, and
Richard S. Zemel. Efﬁcient graph generation with graph re-
current attention networks. In NeurIPS, 2019. 3, 7
[30] Michael James Lighthill and Gerald Beresford Whitham. On
kinematic waves. II. A theory of trafﬁc ﬂow on long crowded
roads. In Royal Society of London. Series A, Mathematical
and Physical Sciences, 1955. 2
[31] Pablo ´Alvarez L´opez, Michael Behrisch, Laura Bieker-Walz,
Jakob Erdmann, Yun-Pang Fl¨otter¨od, Robert Hilbrich, Leon-
hard L¨ucken, Johannes Rummel, Peter Wagner, and Eva-
marie WieBner. Microscopic trafﬁc simulation using SUMO.
In ITSC, 2018. 1, 2
[32] Sivabalan Manivasagam, Shenlong Wang, Kelvin Wong,
Wenyuan Zeng, Mikita Sazanovich, Shuhan Tan, Bin Yang,
Wei-Chiu Ma, and Raquel Urtasun. Lidarsim: Realistic lidar
simulation by leveraging the real world. In CVPR, 2020. 1,
7, 8
9

[33] Charlie Nash, Yaroslav Ganin, S. M. Ali Eslami, and Pe-
ter W. Battaglia.
Polygen: An autoregressive generative
model of 3d meshes. ICML, 2020. 3
[34] G. F. Newell. Nonlinear effects in the dynamics of car fol-
lowing. Operations Research, 1961. 2
[35] Larry Owen, Yunlong Zhang, Lei Rao, and Gene Mchale.
Trafﬁc ﬂow simulation using corsim.
Winter Simulation
Conference, 2001. 2
[36] Louis A. Pipes. An operational analysis of trafﬁc dynamics.
In Journal of Applied Physics, 1953. 2
[37] Aayush Prakash, Shaad Boochoon, Mark Brophy, David
Acuna, Eric Cameracci, Gavriel State, Omer Shapira, and
Stan Birchﬁeld. Structured domain randomization: Bridg-
ing the reality gap by context-aware synthetic data. In ICRA,
2019. 1, 2, 7
[38] Sergey Prokudin, Peter V. Gehler, and Sebastian Nowozin.
Deep directional statistics: Pose estimation with uncertainty
quantiﬁcation. In ECCV, 2018. 6
[39] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario
Amodei, and Ilya Sutskever. Language models are unsuper-
vised multitask learners. 2018. 2, 3
[40] Paul Richards. Shock waves on the highway. In Operations
Research, 1956. 2
[41] Tim Salimans, Ian J. Goodfellow, Wojciech Zaremba, Vicki
Cheung, Alec Radford, and Xi Chen. Improved techniques
for training gans. In NeurIPS, 2016. 7
[42] Xingjian Shi, Zhourong Chen, Hao Wang, Dit-Yan Yeung,
Wai-Kin Wong, and Wang-chun Woo. Convolutional LSTM
network:
A machine learning approach for precipitation
nowcasting. In NeurIPS, 2015. 4
[43] Saied Taheri.
An investigation and design of slip control
braking systems integrated with four wheel steering. 1990. 6
[44] Martin Treiber, Ansgar Hennecke, and Dirk Helbing. Con-
gested trafﬁc states in empirical observations and micro-
scopic simulations. In Automatisierungstechnik, 2000. 2
[45] Benigno Uria, Iain Murray, and Hugo Larochelle. NADE:
the real-valued neural autoregressive density-estimator.
CoRR, 2013. 3
[46] Benigno Uria, Iain Murray, and Hugo Larochelle. RNADE:
the real-valued neural autoregressive density-estimator. In
NeurIPS, 2013. 3
[47] Benigno Uria, Iain Murray, and Hugo Larochelle. A deep
and tractable density estimator. In ICML, 2014. 3
[48] A¨aron van den Oord, Sander Dieleman, Heiga Zen, Karen
Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner,
Andrew W. Senior, and Koray Kavukcuoglu. Wavenet: A
generative model for raw audio. In ISCA, 2016. 3
[49] A¨aron van den Oord,
Nal Kalchbrenner,
and Koray
Kavukcuoglu. Pixel recurrent neural networks. In ICML,
2016. 2, 3, 5
[50] Kai Wang, Manolis Savva, Angel X. Chang, and Daniel
Ritchie. Deep convolutional priors for indoor scene synthe-
sis. TOG, 2018. 3
[51] Tim Allan Wheeler and Mykel J. Kochenderfer. Factor graph
scene distributions for automotive safety analysis. In ITSC,
2016. 2, 7
[52] Tim Allan Wheeler, Mykel J. Kochenderfer, and Philipp
Robbel. Initial scene conﬁgurations for highway trafﬁc prop-
agation. In ITSC, 2015. 2, 7
[53] Kelvin Wong, Qiang Zhang, Ming Liang, Bin Yang, Renjie
Liao, Abbas Sadat, and Raquel Urtasun. Testing the safety
of self-driving vehicles by simulating perception and predic-
tion. ECCV, 2020. 1, 7
[54] Bin Yang, Ming Liang, and Raquel Urtasun. HDNET: ex-
ploiting HD maps for 3d object detection. In CoRL, 2018. 6,
8
[55] Qi Yang and Haris N. Koutsopoulos. A microscopic traf-
ﬁc simulator for evaluation of dynamic trafﬁc management
systems. Transportation Research Part C: Emerging Tech-
nologies, 1996. 1, 2
[56] Jiaxuan You, Rex Ying, Xiang Ren, William L. Hamilton,
and Jure Leskovec. Graphrnn: Generating realistic graphs
with deep auto-regressive models. In ICML, 2018. 7
[57] Hongyi Zhang, Andreas Geiger, and Raquel Urtasun. Under-
standing high-level semantics by modeling trafﬁc patterns. In
ICCV, 2013. 2
10

Supplementary Materials
SceneGen: Learning to Generate Realistic Trafﬁc Scenes
Shuhan Tan1,2*
Kelvin Wong1,3∗
Shenlong Wang1,3
Sivabalan Manivasagam1,3
Mengye Ren1,3
Raquel Urtasun1,3
1Uber Advanced Technologies Group
2Sun Yat-Sen University
3University of Toronto
tanshh@mail2.sysu.edu.cn
{kelvinwong,slwang,manivasagam,mren,urtasun}@cs.toronto.edu
Abstract
In our supplementary materials, we detail SceneGen’s model architecture and training procedure (Sec. 1). Additionally,
we provide additional experiment details in Sec. 2 and additional experiment results in Sec. 3. In Sec. 4, we exhibit an
extensive array of qualitative results that demonstrate the realism and diversity of the trafﬁc scenes generated by SceneGen.
1. Additional Model Details
1.1. Input Representation
At each step of the generation process, SceneGen takes a bird’s eye view multi-channel image encoding the HD map m,
the SDV a0, and the actors generated so far {a1, . . . , ai−1}. The image emcompasses an 80m × 80m region of interest
centered on the SDV a0 and has a resolution of 0.25m per pixel, yielding a 320 × 320 image. The HD map is rasterized into
a multi-channel image describing all available map elements in each dataset. For ATG4D, our multi-channel image consists
of: lane polygons (straight vehicle lanes, dedicated right vehicle lanes, dedicated left vehicle lanes, dedicated bus lanes,
and dedicated bike lanes); lane centerlines and dividers (allowed to cross, forbidden to cross, and maybe allowed to cross);
lane segments (straight vehicle lanes, dedicated right vehicle lanes, and dedicated left vehicle lanes); drivable area and road
polygons; and crosswalk polygons. In addition, we also encode each lane segment’s trafﬁc light state (green, yellow, red,
ﬂashing yellow, ﬂashing red, and unknown), speed limit, and orientation as ﬁlled lane polygons. Note that orientation angles
are encoded in their Biternion representations θ = (cos θ, sin θ) [16]. In aggregate, this yields a 24-channel image.
Argoverse provides a more limited set of map elements. Here, our multi-channel image consists of: lane polygons; lane
centerlines (all lanes, left turn lanes, right turn lanes, intersection lanes, and trafﬁc-controlled lanes); lane orientations (in
Biternion representation); and drivable area polygons. In aggregate, this yields a 9-channel image.
To encode the actors a0, a1, . . ., we rasterize their bounding boxes onto a collection of binary occupancy images [1],
one for each class; i.e., SDV, vehicles, pedestrians, and bicyclists. Furthermore, we encode their headings and velocities by
rasterizing their bounding boxes onto a ﬁve-channel image, ﬁlled with their respective speed, direction, and heading. As
before, direction and heading angles are encoded in their Biternion representations. See Fig. 1 for an example.
1.2. Model Architecture
The basis of our model is the ConvLSTM architecture [19]. Let x(i) ∈RC×H×W denote the input multi-channel image
at the i-th step of the generation process. Given the previous hidden and cell states h(i−1) and c(i−1), the new hidden states
h(i), cell states c(i), and backbone features f (i) are given by:
h(i), c(i) = ConvLSTM(x(i), h(i−1), c(i−1); w)
(1)
f (i) = CNNb(h(i); w)
(2)
*Indicates equal contribution. Work done at Uber ATG.
1
arXiv:2101.06541v1  [cs.CV]  16 Jan 2021

Actor Occupancies
Lane Polygons
Drivable Areas + Roads
Crosswalks
Lane Segments
Lane Centerline + 
Dividers
Traffic Light States
Lane Orientation + 
Speed Limits
Actor Heading + 
Velocity
Figure 1: The input multi-channel image to SceneGen for ATG4D.
Here, ConvLSTM is a two-layer ConvLSTM with 5 × 5 convolution kernels and 32 hidden channels, and CNNb is a
ﬁve-layer convolutional neural network (CNN) with 32 feature channels per layer. Each convolution layer consists of a 3 × 3
convolution kernel, Group Normalization [23], and ReLU activations. The backbone features f (i) summarize the generated
scene so far and are given as input to the subsequent actor modules, which we detail next.
Class:
We predict the class categorical distribution parameters πc ∈∆|C| as follows1:
πc = MLPc(avg-pool(f (i)); w)
(3)
where avg-pool: RC×H×W →RC is average pooling over the spatial dimensions and MLPc is a three-layer multi-layer
perceptron (MLP) with 32 feature channels per hidden layer, ReLU activations, and softmax outputs.
Location:
We apply uniform quantization to each actor’s position and model the quantized values with a categorical distri-
bution. Our quantization resolution is 0.25m, which we found sufﬁcient to generate realistic trafﬁc scenes while balancing
memory efﬁciency. To predict the parameters πloc ∈∆H×W −1, we use a three-layer CNN with 32 feature channels per
hidden layer. Each hidden convolution layer consists of a 3 × 3 convolution kernel, Group Normalization [23], and ReLU
activations. The output convolution layer uses a 1 × 1 kernel with softmax activations. Note that we use separate CNN
weights for each class in C; i.e., vehicles, pedestrians, and bicyclists.
Bounding box:
An actor’s bounding box bi ∈B consists of its width and height (wi, li) ∈R2
>0 and its heading θi ∈[0, 2π).
We model the distribution over bounding box sizes with a mixture of K bivariate log-normal distributions whose parameters
are predicted by a three-layer MLP (with the same architecture as described earlier):
[πbox, µbox, Σbox] = MLPbox(f (i)
xi,yi; ci, w)
(4)
where πbox ∈∆K−1 are mixture weights and each µbox,k ∈R2 and Σbox,k ∈S2
+ parameterize a component log-normal
distribution. To enforce the constraint that each Σ ∈S2
+, MLPbox predicts a variance term σ2 ∈R2
>0 (in log-scale) and a
1We use ∆n = {(x0, x1, . . . , xn) ∈Rn+1| P
i xi = 1 and xi ≥0 for all i} to denote the n-simplex.
2

correlation term ρ ∈[−1, 1] (using tanh) such that:
Σ =
 σ2
1
ρσ1σ2
ρσ1σ2
σ2
2

∈S2
+
(5)
Similarly, we model the distribution over heading angles with a mixture of K Von-Mises distributions whose parameters
are predicted by another three-layer MLP:
[πθ, µθ, κθ] = MLPθ(f (i)
xi,yi; ci, w)
(6)
where πθ ∈∆K−1 are mixture weights and each µθ,k ∈[0, 2π) and κθ,k > 0 parameterize a component Von-Mises
distribution. Following Prokudin et al. [16], we parameterize each µ with its Biternion representation µ = (cos µ, sin µ) and
each κ is predicted in log-scale. Note that we use separate MLP weights for each class in C whose actors are represented by
bounding boxes; i.e., vehicles and bicyclists. Pedestrians are represented by their center of gravity only (i.e., location).
Velocity:
Each of MLPv, MLPs, and MLPω is a three-layer MLP with the same architecture as described above. We
parameterize the mixture of K Von-Mises distributions for directions ω just as we parameterize the distribution of headings.
As before, we use separate MLP weights for each class in C.
1.3. Training Details
We train our model to maximize the log-likelihood of real trafﬁc scenes in our training dataset:
w⋆= arg max
w
N
X
i=1
log p(ai,1, . . . , ai,n|mi, ai,0; w)
(7)
where w are the neural network parameters and N is the number of samples in our training set. We use teacher forcing and
backpropagation-through-time to train through the generation process, up to a ﬁxed window as memory allows. On a Nvidia
Quadro RTX 5000 with 16GB of GPU memory, we train through 25 generation steps with batch size of 1 per GPU. We use
PyTorch [14] and Horovod [18] to distribute the training process over 16 GPUs with a total batch size of 16. During training,
we also randomly rotate each trafﬁc scene with θ ∈[0, 2π).
Note that each summand log p(a1, . . . , an|m; w) can be decomposed into a sum of the log-likelihoods for each actors;
namely, we have
log p(ai|ξi) = log p(ci|ξi)
|
{z
}
class
+ log p(xi, yi|ci, ξi)
|
{z
}
location
+ log p(bi|ci, xi, yi, ξi)
|
{z
}
bounding box
+ log p(vi|ci, xi, yi, bi, ξi)
|
{z
}
velocity
(8)
where ξi encapsulates the conditions on a<i, m, and a0, to simplify notation. Therefore, the ﬁrst summand log p(ci|ξi) is
the (negative) cross-entropy loss between the predicted parameters πc and the ground truth class ci ∈C ∪{⊥}. We describe
the remaining summands in detail next.
Location:
The second summand log p(xi, yi|ci, ξi) measures the log-likelihood the actor’s location (xi, yi) ∈R2. As
discussed earlier, we uniformly quantize each actor’s location and parameterize it with a categorical distribution. Therefore,
log p(xi, yi|ci, ξi) is the (negative) cross-entropy loss between the predicted parameters πloc and the actor’s ground truth
quantized location. To address the signiﬁcant imbalance of positive versus negative locations here, we use online negative
hard mining. Speciﬁcally, we normalize πloc over the hardest 10,000 locations only (including the positive location), and
compute log p(xi, yi|ci, ξi) based this restricted categorical distribution instead.
Bounding box:
The third summand log p(bi|ci, xi, yi, ξi) is a sum of the log-likelihoods of the actor’s bounding box size
(wi, li) ∈R2 and heading θi ∈[0, 2π):
log p(bi|ci, xi, yi, ξi) = log p(wi, li|ci, xi, yi, ξi) + log p(θi|ci, xi, yi, ξi)
(9)
3

Since we model bounding box size with a mixture of K bivariate log-normal distributions, we have
log p(wi, li|ci, xi, yi, ξi) = log
K
X
k=1
πk
1
2πσk,1σk,2
p
1 −ρ2
k
e
−
1
2(1−ρ2
k)
 log wi−µk,1
σk,1
2
+
 log li−µk,2
σk,2
2
+2ρk
 log wi−µk,1
σk,1
 log li−µk,2
σk,2

(10)
where π ∈∆K−1 are mixture weights and each µk ∈R2, σk ∈R2
>0, and ρk ∈[−1, 1] parameterize a component bivariate
log-normal distribution.
Similarly, since we model heading angles with a mixture of K Von-Mises distributions, we have
log p(θi|ci, xi, yi, ξi) = log
K
X
k=1
πk
eκk cos(θi−µk)
2πI0(κk)
(11)
where π ∈∆K−1 are mixture weights and each µk ∈[0, 2π) and κk > 0 parameterize a component Von-Mises distribution.
Velocity:
The fourth summand log p(vi|ci, xi, yi, bi, ξi) is the log-likelihood of the actor’s velocity vi ∈R2, which we
parameterize as vi = (si cos ωi, si sin ωi) where si ∈R≥0 is its speed and ωi ∈[0, 2π) is its direction. Recall that we model
the distribution over an actor’s velocity as a mixture model where one of the K ≥2 components corresponds to vi = 0.
Therefore, for vi = 0, we have
log p(vi|ci, xi, yi, bi, ξi) = log π1
(12)
and for vi > 0, we have
log p(vi|ci, xi, yi, bi, ξi) = log
K
X
k=2
πk
1
σs,k
√
2π e
−1
2
 log si−µs,k
σs,k
2
|
{z
}
speed
× eκω,k cos(ωi−µω,k)
2πI0(κω,k)
|
{z
}
direction
(13)
where π ∈∆K−1 are mixture weights, each µs,k ∈R and σs,k > 0 parameterize a component log-normal distribution for
speed si, and each µω,k ∈[0, 2π) and κω,k > 0 parameterize a component Von-Mises distribution for direction ωi.
2. Additional Experiment Details
2.1. Baselines
Prob. Grammar:
Our Prob. Grammar baseline is inspired by recent work on probabilistic scene grammars [15, 9, 3].
Here, trafﬁc scenes are composed by placing actors onto lane segments in the HD map, and initializing their classes, sizes,
headings, velocities according to a hand-crafted prior. In our experiments, we use the following scene grammar:
⟨Scene⟩→⟨Lanes⟩
(14)
⟨Lanes⟩→⟨Lane⟩⟨Lanes⟩|ϵ
(15)
⟨Lane⟩→⟨Actors⟩
(16)
⟨Actors⟩→⟨Actor⟩⟨Actors⟩|ϵ
(17)
where Actor and ϵ are terminal symbols. Sampling from this scene grammar yields a scene graph, which deﬁnes the scene
structure—where lane segments are and which actors are positioned on top of them—and scene parameters—the attributes
of each lane segment and actor. In our setting, we are given the lane nodes (and the SDV actor’s node) of the scene graph as
a condition, and our goal is to insert/modify the actor nodes.
Drawing inspiration from MetaSim’s probabilistic scene grammar [9], we ﬁrst uniformly sample the maximum number
of actors per lane segment and then place them along the lane centerline, with a random clearance between successive actors
drawn from the exponential distribution. The class of each actor is determined by the lane segment under consideration (i.e.,
car lane vs. bike lane); its lateral offset from the lane centerline is given by uniform noise; its bounding box size is sampled
from a uniform distribution; its heading and the direction of its velocity is given by the direction of the lane segment plus
some uniform noise; and its speed is the minimum of a sample from a uniform distribution and the lane segment’s speed
limit. The parameters of every distribution are tuned by hand.
4

ATG4D
Argoverse
Method
Size
Speed
Heading
Size
Speed
Heading
Prob. Grammar
0.49
0.42
0.30
0.41
0.57
0.38
MetaSim
0.49
0.33
0.14
0.50
0.53
0.18
Procedural
0.15
0.41
0.07
0.23
0.59
0.17
Lane Graph
0.33
0.28
0.16
0.31
0.34
0.38
LayoutVAE
0.16
0.40
0.29
0.21
0.46
0.29
SceneGen
0.06
0.19
0.08
0.15
0.20
0.22
Table 1: Vehicle-only maximum mean discrepency (MMD) results on ATG4D and Argoverse.
MetaSim:
Our next baseline (MetaSim) uses a graph neural network (GNN) to transform the attributes of each actor
node in the given scene graph. We use the implementation of Kar et al. [9] for this purpose. Speciﬁcally, given a scene
graph drawn from Prob. Grammar, MetaSim deterministically modiﬁes each actor’s distance along its lane centerline, lateral
offset, bounding box size, heading, and velocity. The inputs to MetaSim is a scene graph where each node’s features are
its attributes (normalized between 0 and 1 based on their respective minimum/maximum values under the prior), and the
outputs of MetaSim are each node’s new attributes (again normalized between 0 and 1). We use the GNN architecture of
Kar et al. [9]: a three-layer GNN encoder with 32 →64 →128 features channels and a three-layer GNN decoder with
128 →64 →32 feature channels. Additionally, we use linear layers to encode and decode the per-node attributes.
Note that we train MetaSim using a supervised algorithm with heuristically generated ground truth scene graphs. In
particular, given a real trafﬁc scene, we ﬁrst associate each actor to a lane segment; if this is not possible, the actor is not
included in the scene graph. Next, we modify the attributes of each actor according to Prob. Grammar’s prior. Finally, this
modiﬁed scene graph is given as input to MetaSim, and we train MetaSim to transform the modiﬁed attributes back to their
original ones. In our setting, this training process was both faster and more stable than the original unsupervised algorithm.
Procedural:
Our Procedural baseline is inspired by methods that operate directly on the road topology of the trafﬁc
scene [21, 20, 7, 13]. Speciﬁcally, given a lane graph of the scene [10], Procedural uses a set of rules to place actors
onto lane centerlines. First, we determine a set of valid routes traversing the entire lane graph. Each valid route is a sequence
of successive lane centerlines along which actors can traverse without violating trafﬁc rules; e.g., running red lights, merging
onto an oncoming lane, etc. Next, we place actors onto each route such that successive actors maintain a random clearance
(drawn from an exponential distribution) and no two actors collide. Each actor’s bounding box size is sampled form a Gaus-
sian KDE ﬁtted to the training dataset, and its heading is determined by the tangent vector along its lane centerline at its
location. Finally, we initialize the speed of each actor such that successive actors maintain a random time gap (drawn from
an exponential distribution). Procedural is similar to the heuristics underlying [21, 20, 7] but generalized to handle arbitrary
road topologies. Similar to Prob. Grammar, Procedural can generate only vehicles and bicyclists since existing HD maps do
not provide sidewalks. We believe this limitation highlights the difﬁculty of using a heuristics-based approach.
Lane Graph:
Inspired by MetaSim, we also consider a learning-based version of Procedural. Speciﬁcally, given a trafﬁc
scene generated by Procedural, we use a lane graph neural network to transform the attributes of each actor; i.e., location,
bounding box size, heading, and velocity. Our lane graph neural network follows the design of the state-of-the-art motion
forecasting model by Liang et al. [10]. It consists of MapNet for extracting map topology features and four fusion modules:
actor-to-lane, lane-to-lane, lane-to-actor, and actor-to-actor. We train Lane Graph using heuristically generated ground truth,
as in our MetaSim baseline.
LayoutVAE:
Our implementation of LayoutVAE largely follows that of Jyothi et al. [8]. To adapt LayoutVAE to trafﬁc
scene generation, we ﬁrst augment the original model with an additional CNN to extract map features. In particular, given a
bird’s eye view multi-channel image of the HD map, we use the backbone architecture of Liang et al. [11] to extract multi-
scale map features, which we subsequently average-pool into a feature vector. This is then given to LayoutVAE as input in
place of the label set encoding used in the original setting2. Our second modiﬁcation enables LayoutVAE to output oriented
2The label set in our setting is ﬁxed to be vehicles, pedestrians, and bicyclists.
5

M = 1
M = 10
M = 20
Figure 2: Trafﬁc scenes generated by SceneGen using M = 1, 10, 20 sample proposals for ATG4D.
bounding boxes and velocities. Speciﬁcally, we replace the spherical quadrivariate Gaussian distribution of its BBoxVAE
with a bivariate Gaussian distribution for location, a bivariate log-normal distribution for bounding box size, and a bivariate
Gaussian distribution for velocity. To evaluate the log-likelihood of a scene, we use Monte-Carlo approximation with 1000
samples from the conditional prior [8].
2.2. MMD Metrics
To complement our likelihood-based metric, we compute a sample-based metric as well: maximum mean discrepancy
(MMD) [4]. As we discussed in the main text, MMD measures a distance between two distributions p and q as
MMD2(p, q) = Ex,x′∼p[k(x, x′)] + Ey,y′∼q[k(y, y′)] −2Ex∼p,y∼q[k(x, y)]
(18)
for some kernel k. Following [24, 12], we compute MMD using Gaussian kernels (with bandwidth σ = 1) with the total
variation distance to compare scene statistics between generated and real trafﬁc scenes. In particular, we ﬁrst sample a set P
of real trafﬁc scenes from the evaluation dataset. Conditioned the SDV state and HD map of the scenes in P, we generate a
set Q of synthetic scenes using the method under evaluation. Then, we approximate MMD as:
MMD2(p, q) ≈
1
|P|2
X
x∈P
X
x′∈P
k(x, x′) +
1
|Q|2
X
y∈Q
X
y′∈Q
k(y, y′) −
2
|P||Q|
X
x∈P
X
y∈Q
k(x, y)
(19)
Our scene statistics measure the distribution of classes, bounding box sizes (in m2), speeds (in m/s), and heading angles
(relative to that of the SDV) for each scene. Empty scenes are discarded since these scene statistics are undeﬁned. Since
MMD is expensive to compute, in ATG4D, we form P by sampling the evaluation dataset by every 25th scene, yielding
approximately 5000 scenes. We compute MMD over the full Argoverse validation set as it contains 5015 scenes only.
We also compute MMD in the feature space of a pre-trained motion forecasting model. This is similar to some popular
metrics for evaluating generative models such IS [17], FID [5], and KID [2], except we use a motion forecasting model as our
feature extractor. Here, our motion forecasting model takes a bird’s eye view multi-channel image of the actors in the scene
and regresses the future locations of each actor over the next 3 seconds in 0.5s increments. We use the actor rasterization
procedure described in Sec. 1.1 and the model architecture from [22], and we train the model using 4000 training log from
the ATG4D training set. To obtain a feature vector summarizing the scene, we average pool the model’s backbone features
along its spatial dimensions. Then, to compute MMD, we use the RBF kernel with bandwidth σ = 1.
6

M
Features
Class
Size
Speed
Heading
1
0.13
0.05
0.05
0.10
0.10
10
0.11
0.20
0.06
0.33
0.08
20
0.11
0.30
0.07
0.41
0.08
Table 2: Analysis of the number of sample proposals M on ATG4D. The reported numbers are the MMD metrics computed
between distributions of features extracted by a motion forecasting model and various scene statistics (see main text).
3. Additional Experiment Results
3.1. Vehicle MMD Metrics
In Tab. 1, we report vehicle-only MMD metrics for ATG4D and Argoverse. Speciﬁcally, we compute scene statistics for
generated and real trafﬁc scenes using vehicle actors only. As before, scenes with no vehicles are discarded during evaluation.
This allows for an alternative comparison that controls for the class most easily handled by heuristics; i.e., vehicles. Overall,
we see that SceneGen still achieves the best results among the competing methods. This result reafﬁrms our claim that
heuristics-based methods are insufﬁcient to model the full complexity and diversity of real world trafﬁc scenes.
3.2. Sampling Strategy Analysis
As discussed in the main text, SceneGen uses a sampling strategy inspired by nucleus sampling [6]. Speciﬁcally, at
each generation step, we sample each of SceneGen’s position, heading, and velocity distributions M times and return the
most likely sample as output. In Tab. 2 and Fig. 2, we analyze the effects of using different numbers of sample proposals
M = 1, 10, 20. We see that using M > 1 decreases MMD on deep features, indicating that scene-level realism is improved.
This improvement is even more evident in Fig. 2, where we see vehicles disregarding the rules of trafﬁc when M = 1. With
more ﬁne-grained tuning of M, we expect to see improvements in the actor-level statistics as well; i.e., class, size, and speed.
4. Additional Qualitative Results
In Fig. 3 and 4, we present an array of additional qualitative results for ATG4D and Argoverse respectively. Here, we
compare trafﬁc scenes generated by SceneGen, MetaSim, Lane Graph, and LayoutVAE. From these visualizations, we see
that SceneGen generates trafﬁc scenes that best reﬂect the complexity and diversity of real world trafﬁc scenes. For example,
in the second-to-last row of Fig. 3, we show a trafﬁc scene generated by SceneGen in which a vehicle performs a three-point
turn. In the bottom row of Fig. 3, we also show a scene in which two bicyclists perform an left turn using the car lane. These
scenes highlight SceneGen’s ability to model rare but plausible trafﬁc scenes that could occur in the real world.
In Fig. 5 and 6, we also showcase the diversity of trafﬁc scenes that SceneGen is able to generate. Each row in the ﬁgures
show four samples from our model when given the same SDV state and HD map as inputs. From these visualizations, we
see that SceneGen captures the multi-modality of real world trafﬁc scenes well. For example, the top row of Fig. 5 shows
four trafﬁc scenes generated for a four-way intersection. Here, we see samples in which pedestrians cross the intersection,
vehicles perform an unprotected left turn, and a large bus goes straight.
Finally, in Fig. 7, we visualize the quantized location heatmaps for steps t = 0, 5, 10, 15, 20 of the generation process.
Each row shows the categorical distribution from which we sample the next actor’s location. From these visualizations, we see
that SceneGen is able to model the distribution over actor locations (and the corresponding uncertainties) quite precisely. For
example, the distribution over vehicle locations are concentrated around lane centerlines and the distribution over pedestrian
locations are diffused over crosswalks and sidewalks.
7

SceneGen
MetaSim
Lane Graph
LayoutVAE
Figure 3: Qualitative comparison of trafﬁc scenes generated by SceneGen and various baselines on ATG4D. The ego SDV
is shown in red; vehicles in blue; pedestrians in orange; and bicyclists in green. We visualize lane segments and drivable
surfaces in light grey and crosswalks in dark grey.
8

SceneGen
MetaSim
Lane Graph
LayoutVAE
Figure 4: Qualitative comparison of trafﬁc scenes generated by SceneGen and various baselines on Argoverse. The ego SDV
is shown in red; vehicles in blue; pedestrians in orange; and bicyclists in green. We visualize lane segments in light grey.
9

Sample 1
Sample 2
Sample 3
Sample 4
Figure 5: Trafﬁc scenes generated by SceneGen on ATG4D. The trafﬁc scenes in each row are generated from the same SDV
state and HD map inputs. Each trafﬁc scene is a distinct sample drawn from our model.
Sample 1
Sample 2
Sample 3
Sample 4
Figure 6: Trafﬁc scenes generated by SceneGen on Argoverse. The trafﬁc scenes in each row are generated from the same
SDV state and HD map inputs. Each trafﬁc scene is a distinct sample drawn from our model.
10

Generated Traffic Scene
S
Figure 7: Trafﬁc scenes generated by SceneGen on ATG4D (ﬁrst two columns) and Argoverse (last two columns). We
visualize the quantized location heatmap for steps t = 0, 5, 10, 15, 20 of the generation process. Each column represents the
generation process for one trafﬁc scene. Bright yellow means higher likelihood.
11

References
[1] Mayank Bansal, Alex Krizhevsky, and Abhijit S. Ogale. Chauffeurnet: Learning to drive by imitating the best and synthesizing the
worst. In RSS, 2019. 1
[2] Mikolaj Binkowski, Dougal J. Sutherland, Michael Arbel, and Arthur Gretton. Demystifying MMD gans. In ICLR, 2018. 6
[3] Jeevan Devaranjan, Amlan Kar, and Sanja Fidler. Meta-sim2: Unsupervised learning of scene structure for synthetic data generation.
2020. 4
[4] Arthur Gretton, Karsten M. Borgwardt, Malte J. Rasch, Bernhard Sch¨olkopf, and Alexander J. Smola. A kernel two-sample test.
JMLR, 2012. 6
[5] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale
update rule converge to a local nash equilibrium. In NeurIPS, 2017. 6
[6] Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text degeneration. In ICLR, 2020. 7
[7] Stefan Jesenski, Jan Erik Stellet, Florian A. Schiegg, and J. Marius Z¨ollner. Generation of scenes in intersections for the validation
of highly automated driving functions. In IV, 2019. 5
[8] Akash Abdu Jyothi, Thibaut Durand, Jiawei He, Leonid Sigal, and Greg Mori. Layoutvae: Stochastic scene layout generation from
a label set. In ICCV, 2019. 5, 6
[9] Amlan Kar, Aayush Prakash, Ming-Yu Liu, Eric Cameracci, Justin Yuan, Matt Rusiniak, David Acuna, Antonio Torralba, and Sanja
Fidler. Meta-sim: Learning to generate synthetic datasets. In ICCV, 2019. 4, 5
[10] Ming Liang, Bin Yang, Rui Hu, Yun Chen, Renjie Liao, Song Feng, and Raquel Urtasun. Learning lane graph representations for
motion forecasting. In ECCV, 2020. 5
[11] Ming Liang, Bin Yang, Wenyuan Zeng, Yun Chen, Rui Hu, Sergio Casas, and Raquel Urtasun. Pnpnet: End-to-end perception and
prediction with tracking in the loop. In CVPR, 2020. 5
[12] Renjie Liao, Yujia Li, Yang Song, Shenlong Wang, William L. Hamilton, David Duvenaud, Raquel Urtasun, and Richard S. Zemel.
Efﬁcient graph generation with graph recurrent attention networks. In NeurIPS, 2019. 6
[13] Sivabalan Manivasagam, Shenlong Wang, Kelvin Wong, Wenyuan Zeng, Mikita Sazanovich, Shuhan Tan, Bin Yang, Wei-Chiu Ma,
and Raquel Urtasun. Lidarsim: Realistic lidar simulation by leveraging the real world. In CVPR, 2020. 5
[14] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia
Gimelshein, Luca Antiga, Alban Desmaison, Andreas K¨opf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank
Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep
learning library. In NeurIPS, 2019. 3
[15] Aayush Prakash, Shaad Boochoon, Mark Brophy, David Acuna, Eric Cameracci, Gavriel State, Omer Shapira, and Stan Birchﬁeld.
Structured domain randomization: Bridging the reality gap by context-aware synthetic data. In ICRA, 2019. 4
[16] Sergey Prokudin, Peter V. Gehler, and Sebastian Nowozin. Deep directional statistics: Pose estimation with uncertainty quantiﬁcation.
In ECCV, 2018. 1, 3
[17] Tim Salimans, Ian J. Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training
gans. In NeurIPS, 2016. 6
[18] Alexander Sergeev and Mike Del Balso. Horovod: fast and easy distributed deep learning in tensorﬂow. arXiv, 2018. 3
[19] Xingjian Shi, Zhourong Chen, Hao Wang, Dit-Yan Yeung, Wai-Kin Wong, and Wang-chun Woo. Convolutional LSTM network: A
machine learning approach for precipitation nowcasting. In NeurIPS, 2015. 1
[20] Tim Allan Wheeler and Mykel J. Kochenderfer. Factor graph scene distributions for automotive safety analysis. In ITSC, 2016. 5
[21] Tim Allan Wheeler, Mykel J. Kochenderfer, and Philipp Robbel. Initial scene conﬁgurations for highway trafﬁc propagation. In
ITSC, 2015. 5
[22] Kelvin Wong, Qiang Zhang, Ming Liang, Bin Yang, Renjie Liao, Abbas Sadat, and Raquel Urtasun. Testing the safety of self-driving
vehicles by simulating perception and prediction. ECCV, 2020. 6
[23] Yuxin Wu and Kaiming He. Group normalization. In ECCV, 2018. 2
[24] Jiaxuan You, Rex Ying, Xiang Ren, William L. Hamilton, and Jure Leskovec. Graphrnn: Generating realistic graphs with deep
auto-regressive models. In ICML, 2018. 6
12

