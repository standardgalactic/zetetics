Towards Self-Assembling Artificial Neural Networks through
Neural Developmental Programs
Elias Najarro, Shyam Sudhakaran, & Sebastian Risi
IT University of Copenhagen, Denmark
{enaj, shsu, sebr}@itu.dk
Abstract
Biological nervous systems are created in a fundamentally
different way than current artificial neural networks. Despite
its impressive results in a variety of different domains, deep
learning often requires considerable engineering effort to de-
sign high-performing neural architectures. By contrast, bio-
logical nervous systems are grown through a dynamic self-
organizing process. In this paper, we take initial steps toward
neural networks that grow through a developmental process
that mirrors key properties of embryonic development in bi-
ological organisms. The growth process is guided by another
neural network, which we call a Neural Developmental Pro-
gram (NDP) and which operates through local communica-
tion alone. We investigate the role of neural growth on dif-
ferent machine learning benchmarks and different optimiza-
tion methods (evolutionary training, online RL, offline RL,
and supervised learning). Additionally, we highlight future
research directions and opportunities enabled by having self-
organization driving the growth of neural networks.
Introduction
The study of neural networks has been a topic of great inter-
est in the field of artificial intelligence due to their ability to
perform complex computations with remarkable efficiency.
However, despite significant advancements in the develop-
ment of neural networks, the majority of them lack the abil-
ity to self-organize, grow, and adapt to new situations in the
same way that biological neurons do. Instead, their struc-
ture is often hand-designed, and learning in these systems is
restricted to the optimization of connection weights.
Biological networks on the other hand, self-assemble and
grow from an initial single cell. Additionally, the amount
of information it takes to specify the wiring of a sophisti-
cated biological brain directly is far greater than the infor-
mation stored in the genome (Breedlove and Watson, 2013).
Instead of storing a specific configuration of synapses, the
genome encodes a much smaller number of rules that gov-
ern how to grow a brain through a local and self-organizing
process (Zador, 2019). For example, the 100 trillion neural
connections in the human brain are encoded by only around
30 thousand active genes. This outstanding compression has
also been called the “genomic bottleneck” (Zador, 2019),
and neuroscience suggests that this limited capacity has a
regularizing effect that results in wiring and plasticity rules
that generalize well.
In this paper, we take first steps in investigating the role
of developmental and self-organizing algorithms in growing
neural networks instead of manually designing them, which
is an underrepresented research area (Gruau, 1992; Nolfi
et al., 1994; Kow Aliw et al., 2014; Miller, 2014). Even
simple models of development such as cellular automata
demonstrate that growth (i.e. unfolding of information over
time) can be crucial to determining the final state of a sys-
tem, which can not directly be calculated (Wolfram, 1984).
The grand vision is to create a system in which neurons self-
assemble, grow, and adapt, based on the task at hand.
Towards this goal, we present a graph neural network type
of encoding, in which the growth of a policy network (i.e. the
neural network controlling the actions of an agent) is con-
trolled by another network running in each neuron, which
we call a Neural Developmental Program (NDP).
The
NDP takes as input information from the connected neu-
rons in the policy network and decides if a neuron should
replicate and how each connection in the network should
set its weight. Starting from a single neuron, the approach
grows a functional policy network, solely based on the lo-
cal communication of neurons. Our approach is different
from methods like NEAT (Stanley and Miikkulainen, 2002)
that grow neural networks during evolution, by growing net-
works during the lifetime of the agent. While not imple-
mented in the current NDP version, this will ultimately al-
low the neural network of the agents to be shaped based on
their experience and environment.
While indirect genome-to-phenotype encodings such as
CPPN-based approaches (Stanley, 2007) or Hypernetworks
(Ha et al., 2016) have had great success, they often pur-
posely abstracted away development and the process of self-
organizational growth. However, in nature, these abilities
seem essential in enabling the remarkable robustness to per-
turbations and unexpected changes (Ha and Tang, 2021;
Risi, 2021).
Allowing each neuron in an artificial neu-
ral network to act as an autonomous agent in a decentral-
arXiv:2307.08197v1  [cs.NE]  17 Jul 2023

Graph state     updated via local
information aggregation
Graph growth
 New developmental cycle 
Edge weights updated
Graph at developmental step
Graph convolution
Weight prediction
MLP
Growth MLP
Neural Developmental Program 
Figure 1: Neural Developmental Program approach for growing neural network. Each node state s is represented as an
embedding vector. During the information aggregation phase, the graph propagates each node state s to its neighbors for n
steps. Based on the updated nodes embedding ˆst+n, the replication model —implemented as a MLP— determines which nodes
will grow new nodes. Finally, if the target network is not unweighted, another MLP estimates the edge weights of each pair
of nodes based on their concatenated embeddings; otherwise, edges are assigned a unitary weight. The resulting network is
evaluated through an objective function, i.e. solving a task or having certain topological properties. The NDP is a distributed
model that operates purely on local information.
ized way similar to their biological counterpart (Hiesinger,
2018), could enable our AI methods to overcome some of
their current limitations in terms of robustness.
Since the space of possible NDP representations is
large, we explore two different representations and differ-
ent training methods such as evolutionary and gradient-
descent based. While lacking state-of-the-art performance,
our method can learn to grow networks and policies that can
perform competitively, opening up interesting future work in
growing and developmental deep neural networks. Our goal
is to inspire researchers to explore the potential of NDP-
like methods as a new paradigm for self-assembling artifi-
cial neural networks. Overall, this work represents a step to-
wards the development of more biologically inspired devel-
opmental encodings, which have the potential to overcome
some of the limitations of current deep learning approaches.
Background and Related Work
Indirect encodings
Indirect encodings are inspired by the biological process of
mapping a compact genotype to a larger phenotype and have
been primarily studied in the context of neuroevolution (Flo-
reano et al., 2008) (i.e. evolving neural networks) but more
recently were also optimized through gradient-descent based
approaches (Ha et al., 2016). In indirect encodings, the de-
scription of the solution is compressed, allowing informa-
tion to be reused and the final solution to contain more com-
ponents than the description itself. Even before the success
of deep RL, these methods enabled solving challenging car
navigation tasks from pixels alone (Koutn´ık et al., 2013).
A
highly
influential
indirect
encoding
is
Hyper-
NEAT (Stanley et al., 2009). HyperNEAT employs an in-
direct encoding called compositional pattern producing net-
works (CPPNs) that abstracts away the process of growth
and instead describes the connectivity of a neural network
through a function of its geometry; an extension called
Evolvable-substrate HyperNEAT (Risi and Stanley, 2012)
allowed neural architectures to be discovered automatically
but did not involve any self-organizing process. More re-
cently, Hypernetworks (Ha et al., 2016) demonstrated that
networks generating the weights of another network can also
be trained end-to-end through gradient descent. Hypernet-
works have been shown to be able to generate competitive
convolutional neural networks (CNNs) (Zhmoginov et al.,
2022) and recurrent neural networks (RNNs) in a variety of
tasks while using a smaller set of trainable parameters. How-
ever, neither HyperNEAT nor Hypernetworks make use of
the process of development over time, which can increase
the evolvability of artificial agents (Kriegman et al., 2017;
Bongard, 2011) and is an important ingredient of biological
systems (Hiesinger, 2021).
Developmental Encodings
Developmental encodings are
a particular family of indirect encodings.
They are ab-
stractions of the developmental process that allowed nature
to produce complex artifacts through a process of growth
and local interactions between cells, ranging from the low
level of cell chemistry simulations to high-level grammati-
cal rewrite systems (Stanley and Miikkulainen, 2003), and
neurogenesis approaches (Miller, 2022; Maile et al., 2022;
Tran et al., 2022; Huang et al., 2023).
Approaches with

neural networks that can grow are a largely under-explored
area (Gruau, 1992; Nolfi et al., 1994; Kow Aliw et al., 2014;
Miller, 2014; ric, 2014) because these algorithms are either
not expressive enough or not efficiently searchable.
Recently, cellular automata (CA) had a resurgence of in-
terest as a model of biological development. CA are a class
of computational models whose outcomes emerge from the
local interactions of simple rules. Introduced by Neumann
et al. (1966) as a part of his quest to build a self-replicating
machine or universal constructor, a CA consist of a lattice
of computing cells that iteratively update their states based
exclusively on their own state and the states of their local
neighbors. On a classical CA, each cell’s state is represented
by an integer and adjacent cells are considered neighbors.
Critically, the update rule of a cellular automaton is identi-
cal for all the cells.
Neural cellular automata (NCA) differ from classical cel-
lular automata (CA) models by replacing the CA update
function with an optimized neural network (Mordvintsev
et al., 2020; Nichele et al., 2017). Recently, this approach
has been extended to grow complex 3D entities such as cas-
tles, apartment blocks, and trees in a video game environ-
ment (Sudhakaran et al., 2021).
A recent method called HyperNCA, extends NCA to grow
a 3D pattern, which is then mapped to the weight matrix
of a policy network (Najarro et al., 2022). While working
well in different reinforcement learning tasks, the mapping
from the grown 3D pattern to policy weight matrix did not
take the topology of the network into account. Instead of a
grid-like HyperNCA approach, the method presented in this
paper extends NCA to directly operate on the policy graph
itself and should thus also allow more flexibility in the types
of architectures that can be grown.
Distribution-fitting approaches to evolving graphs
Previous work has explored the emerging topologies of the
different growth processes (Albert and Barab´asi, 2000) and
shown that they can reproduce real networks by fitting the
parameters of the distribution from which new nodes are
sampled. In contrast to our method, the growth processes in
previous network-theory approaches do not depend on the
internal state of the graph, and therefore do not make use
of the developmental aspect of the network to achieve the
target topological properties.
Approach: Growing Neural Networks through
Neural Developmental Programs
This section presents the two different Neural Developmen-
tal Program instantiations we are exploring in this paper:
(1) an evolution-based NDP and (2) a differentiable ver-
sion trained with gradient descent-based.
While an evo-
lutionary version allows us to more easily explore differ-
ent architectures without having to worry about their dif-
ferentiability, gradient descent-based architectures can of-
ten be more sample efficient, allow scaling to higher dimen-
sions, and enable approaches such as offline reinforcement
learning. Code implementations will be available soon at:
https://github.com/enajx/NDP.
Evolutionary-based NDP
The NDP consists of a series of developmental cycles ap-
plied to an initial seeding graph; our experiments always use
a seeding graph consisting of a single node or a minimal
network connecting the neural network’s inputs directly to
its outputs. Each of the nodes of the graph has an internal
state represented as n−dimensional latent vector whose val-
ues are updated during the developmental process through
local communication. The node state-vectors —or embed-
dings— encode the cells’ states and are used by the NDP
to determine which nodes will duplicate to make new nodes.
Similarly to how most cells in biological organisms con-
tain the same program in the form of DNA, each node’s
growth and the synaptic weights are controlled by a copy
of the same NDP, resulting in a distributed self-organized
process that incentives the reuse of information.
An
overview of the approach is shown in Fig. 1.
The NDP architecture consists of a Multilayer Percep-
tron (MLP) —acting as a Graph Cellular Automata (GNCA)
(Grattarola et al., 2021)— which updates the node embed-
dings after each message-passing step during the develop-
mental phase.
Subsequently, a replication model in the
form of a second MLP queries each node state and predicts
whether a new node should be added; if so, a new node is
connected to the parent node and its immediate neighbors.
Finally, if the target network is weighted, a third MLP deter-
mines the edge weights based on the concatenation of each
pair of node embeddings. The grown network can now be
evaluated on the task at hand by assigning a subset of nodes
as the input nodes and another subset as the output nodes. In
our case, we select the first —and last— rows of the adja-
cency matrix representing the network to act as input — and
output— nodes, respectively. During evaluation the activa-
tions of the nodes are scalars (that is, R1 instead of the Rn
vectors used during development), and all node activations
are initialized to zero.
We refer to the NDP as the set of these MLPs which are
identical for each cell in the policy network; in order to keep
the number of parameters of the NDP low, the reported
experiments make use of small MLPs with a single hidden
layer. However, it’s worth noticing that because the NDP is
a distributed model (i.e. the same models are being applied
to every node), the number of parameters is constant with re-
spect to the size of the graph in which it operates. Therefore,
any neural network of arbitrary size or architecture could be
used, provided that it was deemed necessary to grow a more
complex graph. The NDP’s neural networks can be trained
with any black-box optimization algorithm to satisfy any ob-
jective function. In this paper, we demonstrate how the ap-

Algorithm
1:
Neural
Developmental
Program
NDP: non-differentiable version
Input: Replication model R, Graph Cellular
Automata GNCA, Weight update model W,
number of developmental cycles C, pruning
threshold P, number of training generations
T , training hyper-parameters Ω
Output: Developmental program producing graph G
that minimise/maximise F
1 Co-evolve or sample random embedding EN=0 for
the initial node N0 of G;
2 for generation in T do
3
for developmental cycle in C do
4
Compute network diameter D;
5
Propagate nodes states EN via graph
convolution D steps;
6
Replication model R determines nodes in
growing state;
7
New nodes are added to each of the growing
nodes and their immediate neighbors;
8
New nodes’ embeddings are defined as the
mean embedding of their parent nodes.
9
if weighted network then
10
Weight update model W updates
connectivity for each pair of nodes based
on their concatenated embeddings;
11
if pruning then
12
Edges with weights below pruning
threshold P are removed;
13
Evaluate objective F of grown graph G;
14
Use F(G) to guide optimisation;
proach allows to grow neural networks capable of solving
reinforcement learning and classification tasks, or exhibiting
some topological properties such as small-worldness. The
pseudocode of the approach is detailed in Algorithm 1.
Gradient-based NDP
The gradient-based growth process is similar to the evolu-
tionary approach, albeit with some additional constraints
due to its requirement of complete differentiability in the
process. In contrast to the evolutionary approach, the grown
networks are exclusively feedforward networks, where in-
formation is iteratively transmitted through message pass-
ing via the topologically sorted nodes. Each node has a bias
value, and an activation that is applied to when the incoming
nodes’ information is aggregated, similar to the node behav-
ior in e.g. NEAT (Stanley and Miikkulainen, 2002).
Like in the evolutionary approach, cells’ states are rep-
resented as vectors. However, instead of all values of each
cell’s state vector being treated as intractable black-boxes
for the network to store information, here each first and sec-
Algorithm
2:
Neural
Developmental
Program
NDP: differentiable version
Input: Developmental model D, number of
developmental cycles ND, number of training
iterations T , replication model R, edge
prediction model E, reinforcement learning
algorithm RL
Output: Developmental program producing graph G
that maximises reward F
1 Initialize trainable node embeddings Ninit of G;
2 for training iteration in T do
3
for i in ND do
4
1. Get new node embeddings Ni via Neural
Message Passing D(Ninitial)
5
2. Sample parent node Pi using replication
channel probabilities in node embeddings
Ni
6
3. Create replicated child node with
replication model Ci = R(Pi) and connect
an incoming edge from Pi to Ci. Connect an
outgoing edge from Ci to a random node at
the further on in the network (layers deeper
in the network).
7
4. Add child node to the network
8
5. Predict edge weights using Edge
prediction model E for each edge in the
network, using pairs of source and
destination node embeddings
9
Collect trajectories with a rollout using grown
graph G;
10
Update parameters via RL algorithm RL
ond elements of the vectors have pre-defined roles: the first
element represents the bias value and the second encodes
the activation. The remaining encode hidden states for each
cell, capturing temporal information used to guide the de-
velopmental process. These cells are passed into a message-
passing network, implemented as a graph Convolution (Kipf
and Welling, 2016), where a neighborhood of cell embed-
dings, characterized as being 1 edge away are linearly pro-
jected and added to create new embeddings. In addition to
using a message passing network to update nodes, we use
a trainable small 2-layer MLP, with tanh activation, to pre-
dict edge weights using pairs of (source, destination) nodes.
The message-passing network is composed of a GraphConv
layer that outputs vectors of size 32, followed by a Tanh ac-
tivation, followed by a linear layer that maps the size vectors
to the original cell embeddings.
In order to add more nodes, we treat a channel in each cell
as a replication probability, which is used to sample cells
and connect to a random node further into the network. This
process happens every other growth step.

Detailed replication process: (1) A replication network,
implemented as a separate graph convolution, is applied on
cells to output replication probabilities. (2) A cell is sam-
pled from these probabilities and is passed into a perturba-
tion network to get a new cell, and an incoming edge is con-
nected from the parent to the new cell. An outgoing edge is
connected to a random child (found further in the network).
After the new node is added to the network, we update the
edges using an MLP that takes in incoming node and outgo-
ing node pairs and outputs a new edge weight.
We initialize the starting network by fully connecting
trainable input cell embeddings and output cell embeddings,
which we found to work better for our gradient-based NDP
than starting with a single node. For example, if input = 4
and output = 2, then each input node will be connected to an
output node, resulting in 4×2 initial edges. The pseudocode
of this approach is detailed in Algorithm 2.
Experiments
We test the NDP approach on generating networks for clas-
sification tasks such as MNIST, boolean gates such as XOR
gate and reinforcement learning tasks with both continuous
and discrete action spaces. The RL tasks include CartPole,
a classical control task with non-linear dynamics, and Lu-
narLander, a discrete task where the goal is to smoothly land
on a procedurally generated terrain. We also tackle an offline
RL task using Behavioral Cloning (BC) (Torabi et al., 2018),
which is HalfCheetah, a canine-like robot task.
The different environments are shown in Fig. 2. The Cart-
Pole environment provides observations with 4 dimensions,
LunarLander has 8, MNIST has 64, and HalfCheetah has
17.
CartPole has 2 actions, LunarLander has 4 actions,
MNIST has 10 classes to predict, and HalfCheetah has a con-
tinuous action of dimension 7.
Evolutionary Optimization Details
We use CMA-ES — Covariance Matrix Adaptation Evolu-
tion Strategy — (Hansen and Ostermeier, 1996), a black-box
population-based optimisation algorithm to train the NDP.
Evolutionary strategies (ES) algorithms have been shown
to be capable of finding high-performing solutions for re-
inforcement learning tasks, both directly optimising the pol-
icy weights (Salimans et al., 2017), and with developmental
encodings (Najarro et al., 2022). Black-box methods such
as CMA-ES have the advantage of not requiring to compute
gradients and being easily parallelizable.
Experiments have been run on a single machine with a
AMD Ryzen Threadripper 3990X CPU with 64 cores. We
choose a population size of 512, and an initial variance of
0.1. Finally, we employ an early stopping method which
stops and resets training runs that show poor performance
after a few hundred generations.
Differentiable Optimization Details
We use the Pytorch Geometric library to implement our
NDP as well as our grown networks, enabling us to back-
propagate a loss using predictions from the grown networks
all the way back to the parameters of the NDP. We are also
able to leverage GPUs for the forward and backward passes.
Experiments have been run on a single machine with a
NVIDIA 2080ti GPU. We use the Adam Optimizer (Kingma
and Ba, 2014) to optimize the NDP’s trainable parameters.
Online RL with PPO
Because we can take advantage
of backpropagation with the differentiable NDP approach,
we can utilize reinforcement learning algorithms, specifi-
cally Proximal Policy Optimization (PPO) (Schulman et al.,
2017), to grow optimal policies. Implementations of PPO
typically use separate networks / shared networks for critic
and actor heads. In our implementations, we simply treat
the critic output as a separate node that is initially fully con-
nected to all the input nodes. In our implementation, we use
a learning rate of 0.0005, an entropy coefficient of 0.001,
and optimize for 30 steps after each rollout is collected. We
train for 10,000 rollouts and record the average reward from
10 episodes in Tables 1a and 1b.
Offline RL with Behavioral Cloning
In Offline RL, in-
stead of gathering data from an environment, we only have
access to a dataset of trajectories. This setting is challeng-
ing but also easier because we can avoid the noisy training
process of Online RL. In our approach, we utilize Behav-
ioral Cloning (BC) to optimize our NDP to grow high-
performing policies. We use a learning rate of 0.0001 and
a batch size of 32 observations. We train for 10000 rollouts
and record the average reward from 10 episodes in Table 2b.
Supervised Learning
We evaluate the supervised learn-
ing capabilities of our differentiable NDP with the classic
MNIST task, where a small (8×8) image is classified as a
digit between 0-9. We use a learning rate of 0.001 and a
batch size of 32 observations. We train for 10000 iterations
and record the test accuracy in Table 2a.
Evolutionary-based training results
Growing networks for XOR gate. The XOR gate is a clas-
sic problem, which interests reside in its non-linearly sep-
arable nature. While the NDP has in fact more param-
eters than necessary to solve the simple XOR task (which
can be solved with a neural network with one hidden node),
it serves as the initial test that networks with multiple lay-
ers can be grown with the evolutionary NDP.
Indeed,
we find that a simple NDP with 14 trainable parameters
can grow an undirected graph capable of solving the XOR
gate (Fig. 4, left).
The resulting graph has 4 nodes and
7 weighted edges. Graph layouts are generated using the

(a) CartPole
(b) LunarLander
(c) HalfCheetah
(d) 8x8 MNIST Example
Figure 2: Test domains in this paper include both reinforcement and supervised tasks.
0
25
50
75
100
125
150
175
200
Generation
0
1
2
3
4
Reward
XOR task
0
250
500
750
1000
1250
1500
1750
2000
Generation
100
50
0
50
100
150
Lander task
0
2
4
6
8
Generation
0
100
200
300
400
500
600
CartPole task
Figure 3: Fitness during evolution on the XOR, LunarLan-
der and CartPole tasks. Gray areas show standard deviation
at each generation of the CMA-ES sampled solutions.
Fruchterman-Reingold force-directed algorithm.
Training
curves are shown in Fig. 3.
Growing policy network for RL tasks. We trained a
NDP with 162 parameters for the CartPole tasks, in which
it has to grow a policy network controlling the force applied
to a pole to keep it balanced. It grew an undirected network
with 10 nodes and 33 weighted edges that reached a reward
of 500±0 over 100 rollouts (Fig. 3). This score is considered
as solving the task. The growth process of the network from
a single node to the final policy can be seen in Fig. 5.
Similarly, we trained a NDP to grow a network policy to
solve the Lunar Lander control task (Fig. 4, right). In this
case, a NDP with 868 trainable parameters grew an undi-
rected network policy with 16 nodes and 78 weighted edges.
Over 100 rollouts, the mean reward obtained is 116 ± 124.
Although the resulting policy controller could solve the task
in many of the rollouts, the stochasticity of the environment
(e.g. changing the landing surface at each instantiation) re-
sulted in a high reward variance. This means that the grown
network did not quite reach the 200 reward over 100 rollouts
score that is considered as the task’s solving criterion.
Growing small-world topologies. Real networks found
in nature such as biological neural networks, ecological net-
works or social networks are not simple random networks
but rather their graphs have very specific topological prop-
erties. In order to analyze the ability of the neural devel-
opmental program to grow complex graph motifs, we train
the NDP to grow a small-world network (Watts and Stro-
gatz, 1998). A small-world network is characterised by a
small average shortest path length but a relatively large clus-
XOR gate
Lander
Figure 4: Grown networks solving XOR gate task (left), and
Lunar Lander task (right). Red nodes behave as sensory neu-
rons, white nodes are interneurons, and blue ones are the
output neurons that determine the actions of the cartpole.
tering coefficient. We can quantify this with two metrics
σ = C/Cr
L/Lr and ω = Lr
L −C
Cl , where C and L are respec-
tively the average clustering coefficient and average short-
est path length of the network. Cr and Lr are respectively
the average clustering coefficient and average shortest path
length of an equivalent random graph. A graph is commonly
classified as small-world if σ > 1 or, equivalently, ω ≈0.
We show that NDP technique can grow a graph with
small-world coefficients are σ ≈1.27 and ω ≈−1.11−16,
hence satisfying the small-worldness criteria. An example
network is shown in Fig. 6. While these results are promis-
ing, further experiments are required —notably on bigger
graphs— to investigate with solid statistical significance the
potential of the method to grow graphs with arbitrary topo-
logical properties.
Gradient-Based Results
We evaluate the differentiable NDP by comparing models
that are trained and tested on different numbers of growth
steps (”Growth Steps” column in Table 1). It seems that for
most tasks, after a certain number of growth steps, the grown
network’s performance can deteriorate, as policies do not
really benefit from more complexity. This could also be due
to the simplicity constraint of grown architectures, making
it unable to take advantage of new nodes as the networks get
larger. Automatically learning when to stop growing will be

Initial node
1st growth cycle
2nd growth cycle
3rd growth cycle
4th growth cycle
Figure 5: Developmental growth of the network capable of solving the Cart Pole balancing task. The network starts as a single
node and grows to a network of size 2, 4, 5, and finally 10 neurons and 33 weighted edges after 4 growth cycles. Red nodes
behave as sensory neurons, white nodes as interneurons, and blue ones act as the output neurons that determine the actions of
the cartpole. The vector displayed above the neurons are the node embeddings which represent the state of each node during
the growth process.
Growth Steps
Rew.
Network Size
1
123 ± 11.3
7
12
257 ± 6.9
13
24
334 ± 4.4
19
48
500 ± 3.1
31
64
500 ± 1.7
39
128
500 ± 2.8
71
(a) CartPole Results. Number of NDP parameters: 11223.
Growth Steps
Rew.
Network Size
1
24 ± 7.3
13
12
68 ± 8.1
19
24
100 ± 8.7
25
48
130 ± 3.4
37
64
112 ± 5.8
45
128
110 ± 6.4
77
(b) LunarLander Results. Number of NDP parameters: 11445.
Table 1: Differentiable NDP: Online RL Results. Mean reward is calculated over 10 test episodes after 10,000 rollouts were
collected. Networks are trained with a specific number of growth steps, as shown in the “growth steps” column.
Figure 6: Grown Small-World network. In this experiment,
the NDP seeks to maximise the coefficients used to capture
the small-worldness criteria of a graph. Unlike the networks
grown to act as policies, this network is unweighted.
an important addition to the NDP approach.
The differentiable NDP reaches comparable perfor-
mance to the evolutionary-based version on CartPole (Ta-
ble 1a) and LunarLander (Table 1b). An example of the
growth steps for the LunarLander tasks is shown in Figure 7.
In the offline RL setting, the NDP is able to get a mean re-
ward of 29 on the HalfCheetah task (Table 2b), which is
satisfactory but lower compared to the performance of 43.1
achieved by behavioral cloning in (Chen et al., 2021).
We are also able to scale up to larger networks for tasks
like MNIST, which uses an 8×8 image and reaches a test
accuracy of 91% (Table 2a). The sequence of an MNIST
network growing is shown in Figure 8.
Discussion and Future Work
We introduced the idea of a neural developmental program
and two instantiations of it, one evolutionary-based and one
gradient descent. We showed the feasibility of the approach
in continuous control problems and in growing topologies
with particular properties such as small-worldness. While
the approach is able to solve these simple domains, many
future work directions remain to be explored.
For example, the current NDP version does not in-
clude any activity-dependent growth, i.e. it will grow the
same network independently of the incoming activations that
the agent receives during its lifetime.
However, biologi-
cal nervous systems often rely on both activity and activity-
independent growth; activity-dependent neural development
enables biological systems to shape their nervous system de-
pending on the environment. Similar mechanisms also form
the basis for the formation of new memories and learning.
In the future, we will extend the NDP with the ability to
also incorporate activity-dependent and reward-modulated
growth and adaptation.
While a developmental encoding offers the possibility to
encode large networks with a much smaller genotype, the

Growth Steps
Test Acc.
Network Size
1
13 ± 8.1
74
12
33 ± 6.3
80
24
78 ± 4.7
86
48
93 ± 2.9
98
64
91 ± 1.4
106
(a) MNIST Results. Number of NDP parameters: 13739.
Growth Steps
Rew.
Network Size
1
13 ± 2.2
24
12
17 ± 2.6
30
24
23 ± 2.3
36
48
25 ± 1.7
48
64
29 ± 1.1
56
(b) HalfCheetah Results. Number of NDP parameters: 11889.
Table 2: Differentiable NDP: Supervised Learning and Offline RL tasks. Test accuracy for MNIST calculated after 10,000
epochs. Mean reward for HalfCheetah calculated over 10 test episodes after 10000 epochs.
Initial seeding graph.
13 Nodes.
16 growth steps.
21 Nodes.
32 growth steps.
29 Nodes.
48 growth steps.
37 Nodes.
64 growth steps.
45 Nodes.
Figure 7: Differentiable NDP: Lunar Lander Network policy growth over 64 steps. Red nodes are input nodes, blue nodes
are output nodes, white nodes are hidden nodes.
Initial seeding graph.
74 Nodes.
16 growth steps.
82 Nodes.
32 growth steps.
90 Nodes.
48 growth steps.
98 Nodes.
64 growth steps.
106 Nodes.
Figure 8: Differentiable NDP: MNIST network growth over 64 steps.
NDP in this paper are in fact often larger than the result-
ing policy networks. However, by running the developmen-
tal process longer, it is certainly possible to ultimately grow
policy networks with a larger number of parameters than the
underlying NDP. However, as the results in this paper sug-
gest, growing larger policy networks than necessary for the
tasks can have detrimental effects (Table 1b), so it will be
important to also learn when to stop growing. The exact in-
terplay between genome size, developmental steps, and task
performance constitutes important future work.
We will additionally extend the approach to more com-
plex domains and study in more detail the effects of growth
and self-organization on the type of neural networks that
evolution is able to discover. NDPs offer a unifying prin-
ciple that has the potential to capture many of the proper-
ties that are important for biological intelligence to strive
(Versace et al., 2018; Kudithipudi et al., 2022). While in-
nate structures in biological nervous systems have greatly in-
spired AI approaches (e.g. convolutional architectures being
the most prominent), how evolution discovered such wiring
diagrams and how they are grown through a genomic bottle-
neck are questions rarely addressed. In the future, NDPs
could consolidate a different pathway for training neural net-
works and lead to new methodologies for developing AI sys-
tems, beyond training and fine-tuning.
Acknowledgments
This project was supported by a GoodAI Research Award
and a European Research Council (ERC) grant (GA no.
101045094, project ”GROW-AI”)

References
(2014). Evolving Morphologies with CPPN-NEAT and a Dynamic
Substrate, volume ALIFE 14: The Fourteenth International
Conference on the Synthesis and Simulation of Living Sys-
tems.
Albert, R. and Barab´asi, A.-L. (2000).
Topology of Evolving
Networks: Local Events and Universality. Phys. Rev. Lett.,
85(24):5234–5237.
Bongard, J. (2011). Morphological change in machines accelerates
the evolution of robust behavior. Proceedings of the National
Academy of Sciences, 108(4):1234–1239.
Breedlove, S. M. and Watson, N. V. (2013). Biological psychol-
ogy: An introduction to behavioral, cognitive, and clinical
neuroscience. Sinauer Associates.
Chen, L., Lu, K., Rajeswaran, A., Lee, K., Grover, A., Laskin,
M., Abbeel, P., Srinivas, A., and Mordatch, I. (2021). Deci-
sion transformer: Reinforcement learning via sequence mod-
eling. Advances in neural information processing systems,
34:15084–15097.
Floreano, D., D¨urr, P., and Mattiussi, C. (2008). Neuroevolution:
from architectures to learning.
Evolutionary Intelligence,
1(1):47–62.
Grattarola, D., Livi, L., and Alippi, C. (2021). Learning Graph
Cellular Automata. arXiv.
Gruau, F. (1992).
Genetic synthesis of boolean neural net-
works with a cell rewriting developmental process.
In
Combinations of Genetic Algorithms and Neural Networks,
1992., COGANN-92. International Workshop on, pages 55–
74. IEEE.
Ha, D., Dai, A., and Le, Q. V. (2016).
Hypernetworks.
arXiv
preprint arXiv:1609.09106.
Ha, D. and Tang, Y. (2021).
Collective intelligence for deep
learning: A survey of recent developments. arXiv preprint
arXiv:2111.14377.
Hansen, N. and Ostermeier, A. (1996).
Adapting arbitrary nor-
mal mutation distributions in evolution strategies: the covari-
ance matrix adaptation. In Proceedings of IEEE International
Conference on Evolutionary Computation, pages 312–317.
IEEE.
Hiesinger, P. R. (2018). The Self-Assembling Brain: Contributions
to Morphogenetic Engineering from a Self-Organizing Neu-
ral Network in the Insect Brain. MIT Press, pages 202–203.
Hiesinger, P. R. (2021).
The Self-Assembling Brain.
Princeton
University Press, Princeton, NJ, USA.
Huang, S., Fang, H., Mahmood, K., Lei, B., Xu, N., Lei, B., Sun,
Y., Xu, D., Wen, W., and Ding, C. (2023).
Neurogenesis
Dynamics-inspired Spiking Neural Network Training Accel-
eration. arXiv.
Kingma, D. P. and Ba, J. (2014). Adam: A method for stochastic
optimization.
Kipf, T. N. and Welling, M. (2016). Semi-supervised classification
with graph convolutional networks.
Koutn´ık, J., Cuccu, G., Schmidhuber, J., and Gomez, F. (2013).
Evolving large-scale neural networks for vision-based rein-
forcement learning. In Proceeding of the fifteenth annual con-
ference on Genetic and evolutionary computation conference,
pages 1061–1068. ACM.
Kow Aliw, T., Bredeche, N., and Dours At, R. (2014). Growing
Adaptive Machines. Springer.
Kriegman, S., Cheney, N., Corucci, F., and Bongard, J. C. (2017).
A minimal developmental model can increase evolvability in
soft robots. In Proceedings of the Genetic and Evolutionary
Computation Conference, pages 131–138.
Kudithipudi, D., Aguilar-Simon, M., Babb, J., Bazhenov, M.,
Blackiston, D., Bongard, J., Brna, A. P., Chakravarthi Raja,
S., Cheney, N., Clune, J., et al. (2022). Biological underpin-
nings for lifelong learning machines. Nature Machine Intel-
ligence, 4(3):196–210.
Maile, K., Rachelson, E., Luga, H., and Wilson, D. G. (2022).
When, where, and how to add new neurons to ANNs. arXiv.
Miller, J. F. (2014). Neuro-centric and holocentric approaches to
the evolution of developmental neural networks. In Growing
Adaptive Machines, pages 227–249. Springer.
Miller, J. F. (2022). Designing Multiple ANNs with Evolutionary
Development: Activity Dependence. In Genetic Program-
ming Theory and Practice XVIII, pages 165–180. Springer,
Singapore.
Mordvintsev, A., Randazzo, E., Niklasson, E., and Levin,
M. (2020).
Growing neural cellular automata.
Distill.
https://distill.pub/2020/growing-ca.
Najarro, E., Sudhakaran, S., Glanois, C., and Risi, S. (2022).
HyperNCA: Growing Developmental Networks with Neural
Cellular Automata. arXiv.
Neumann, J., Burks, A. W., et al. (1966).
Theory of self-
reproducing automata, volume 1102024. University of Illi-
nois press Urbana.
Nichele, S., Ose, M. B., Risi, S., and Tufte, G. (2017). CA-NEAT:
Evolved compositional pattern producing networks for cellu-
lar automata morphogenesis and replication. IEEE Transac-
tions on Cognitive and Developmental Systems.
Nolfi, S., Miglino, O., and Parisi, D. (1994). Phenotypic plasticity
in evolving neural networks. In From Perception to Action
Conference, 1994., Proceedings, pages 146–157. IEEE.
Risi, S. (2021).
The future of artificial intelligence is self-
organizing and self-assembling. sebastianrisi.com.
Risi, S. and Stanley, K. O. (2012). An enhanced hypercube-based
encoding for evolving the placement, density, and connectiv-
ity of neurons. Artificial life, 18(4):331–363.
Salimans, T., Ho, J., Chen, X., Sidor, S., and Sutskever, I. (2017).
Evolution Strategies as a Scalable Alternative to Reinforce-
ment Learning. arXiv.
Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov,
O. (2017). Proximal policy optimization algorithms.

Stanley, K. O. (2007). Compositional pattern producing networks:
A novel abstraction of development. Genetic programming
and evolvable machines, 8(2):131–162.
Stanley, K. O., D’Ambrosio, D. B., and Gauci, J. (2009).
A
hypercube-based encoding for evolving large-scale neural
networks. Artificial life, 15(2):185–212.
Stanley, K. O. and Miikkulainen, R. (2002). Evolving neural net-
works through augmenting topologies. Evolutionary compu-
tation, 10(2):99–127.
Stanley, K. O. and Miikkulainen, R. (2003). A taxonomy for artifi-
cial embryogeny. Artificial Life, 9(2):93–130.
Sudhakaran, S., Grbic, D., Li, S., Katona, A., Najarro, E., Glanois,
C., and Risi, S. (2021).
Growing 3d artefacts and func-
tional machines with neural cellular automata. arXiv preprint
arXiv:2103.08737.
Torabi, F., Warnell, G., and Stone, P. (2018). Behavioral cloning
from observation.
Tran, L. M., Santoro, A., Liu, L., Josselyn, S. A., Richards,
B. A., and Frankland, P. W. (2022).
Adult neurogenesis
acts as a neural regularizer. Proc. Natl. Acad. Sci. U.S.A.,
119(45):e2206704119.
Versace, E., Martinho-Truswell, A., Kacelnik, A., and Vallortigara,
G. (2018). Priors in animal and artificial intelligence: Where
does learning begin? Trends in cognitive sciences.
Watts, D. J. and Strogatz, S. H. (1998). Collective dynamics of
‘small-world’ networks. Nature, 393:440–442.
Wolfram, S. (1984). Universality and complexity in cellular au-
tomata. Physica D: Nonlinear Phenomena, 10(1-2):1–35.
Zador, A. M. (2019). A critique of pure learning and what arti-
ficial neural networks can learn from animal brains. Nature
communications, 10(1):1–7.
Zhmoginov, A., Sandler, M., and Vladymyrov, M. (2022). Hy-
pertransformer: Model generation for supervised and semi-
supervised few-shot learning.

